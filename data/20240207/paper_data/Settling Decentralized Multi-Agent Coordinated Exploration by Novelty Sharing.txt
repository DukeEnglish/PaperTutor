Settling Decentralized Multi-Agent Coordinated Exploration by Novelty Sharing
HaobinJiang1,ZiluoDing1,2,ZongqingLu1†
1SchoolofComputerScience,PekingUniversity
2BeijingAcademyofArtificialIntelligence
{haobin.jiang, ziluo, zongqing.lu}@pku.edu.cn
Abstract elty and global state novelty, which makes the exploration
merelybasedonlocalnoveltyhighlyunreliable(Wangetal.
Exploration in decentralized cooperative multi-agent rein-
2019a; Iqbal and Sha 2019). Fortunately, communication
forcementlearningfacestwochallenges.Oneisthatthenov-
canhelpeasepartialobservabilitybyprovidingextrainfor-
eltyofglobalstatesisunavailable,whilethenoveltyoflocal
mation about other agents (Jiang and Lu 2018; Das et al.
observationsisbiased.Theotherishowagentscanexplore
in a coordinated way. To address these challenges, we pro- 2019;Wangetal.2019b;Ding,Huang,andLu2020).How-
pose MACE, a simple yet effective multi-agent coordinated ever, unlimited communication may incur too much com-
exploration method. By communicating only local novelty, munication overhead, and it can indeed transform the de-
agents can take into account other agents’ local novelty to centralized setting into a centralized setting. Therefore, we
approximatetheglobalnovelty.Further,wenewlyintroduce resorttodecentralizedlearningwithlimitedcommunication
weightedmutualinformationtomeasuretheinfluenceofone toaddresssuchproblems.
agent’sactiononotheragents’accumulatednovelty.Wecon-
In addition to the challenge of novelty measurement, in
vertitasanintrinsicrewardinhindsighttoencourageagents
cooperativetasks,agentsmustalsoacquiretheabilitytoco-
toexertmoreinfluenceonotheragents’explorationandboost
ordinate with each other to explore and achieve the final
coordinated exploration. Empirically, we show that MACE
achievessuperiorperformanceinthreemulti-agentenviron- goal. Ideally, the optimal exploration strategy should con-
mentswithsparserewards. siderothers’observationsandactions.Previouswork(Wang
etal.2019a;IqbalandSha2019;Liuetal.2021)findsthat
independent exploration is not efficient and redundant ex-
1 Introduction
ploration occurs. By coordination in exploration, we mean
Recent progress in decentralized learning theories and al- agents help other agents to achieve novel observations or
gorithms for multi-agent reinforcement learning (MARL) reach novel states together through cooperation. In other
(Zhang et al. 2018; de Witt et al. 2020; Jin et al. 2021; words, an agent should be encouraged when its action en-
Daskalakis,Golowich,andZhang2022;JiangandLu2022) ablesotheragentstoreachmorenovelobservations.
makes it feasible to learn high-performant policies in a de-
In this paper, we propose a simple yet effective Multi-
centralizedwayforcooperativemulti-agenttasks.However,
Agent Coordinated Exploration method, namely MACE.
one critical issue remains, i.e., how to enable agents to ef-
MACE introduces a novelty-based intrinsic reward and a
fectivelyexploreinacoordinatedwayundersuchalearning
hindsight-based intrinsic reward to enable coordinated ex-
paradigm, especially for sparse-reward tasks where the en-
plorationindecentralizedcooperativetasks.Withinthecon-
vironmentrarelyprovidesrewards.
finesoflimitedcommunication,agentsonlysharetheirlocal
One of the most popular exploration schemes in the
novelties (merely a floating point number) during training.
single-agentsettingisnovelty-basedexploration(Bellemare
Eachagentleveragesthissharedinformationtoapproximate
et al. 2016; Pathak et al. 2017; Burda et al. 2018b; Zhang
theglobalnovelty,whichservesasthenovelty-basedintrin-
et al. 2021b), where the agent is encouraged by well-
sicreward.Thisapproachaimstobridgethegapbetweenthe
designedintrinsicrewardtovisitnovelstatesitrarelysees.
localnoveltyandtheglobalnovelty.Moreover,weencour-
However, things could be different when migrating to de-
age agents to exert more influence on others’ explorations
centralizedmulti-agentsettings,whichleadstoanunsolved
throughthehindsight-basedintrinsicreward,therebyboost-
problem:asonlythelocalobservationinsteadoftheglobal
ing coordinated exploration. To this end, we measure the
state is available, how should each agent measure the nov-
weighted mutual information (Guiasu 1977; Schaffernicht
eltyoftheglobalstate?
andGross2011)betweentheactionoftheagentandtheac-
Indecentralizedsettings,partialobservabilityexpandsthe
cumulated novelty obtained thereafter by others given the
discrepancy between each agent’s local observation nov-
localobservation.Thehighertheweightedmutualinforma-
tion value, the higher the hindsight-based intrinsic reward
†Correspondingauthor
theagentreceives.
Copyright©2024,AssociationfortheAdvancementofArtificial
Intelligence(www.aaai.org).Allrightsreserved. We evaluate MACE in three multi-agent environments:
4202
beF
3
]AM.sc[
1v79020.2042:viXraGridWorld, Overcooked (Carroll et al. 2019), and SMAC otheragents(Section3.2)andconvertsitintothehindsight-
(Samvelyanetal.2019).Alltasksintheseenvironmentsare based intrinsic reward (Section 3.3). The weighted sum of
sparse-rewardandhardtoexplore.Theexperimentalresults these two parts is the final intrinsic reward used in MACE
verifytheeffectivenessofMACE.Throughablationstudies, (Section3.4).
weshowthatboththeapproximationtoglobalnoveltyand
theencouragementtoinfluenceotheragents’explorationare 3.1 ApproximationtoGlobalNovelty
indispensable in decentralized multi-agent exploration, and In decentralized training, if we only take into account the
our newly employed weighted mutual information works individual exploration of agent i, ui = novelty(oi ) can
t t+1
significantlybetterthannormalmutualinformation. serveastheintrinsicrewardtoencourageagentitotakeac-
tionstowardsobservationsitseldomvisits.Whentheobser-
2 Preliminary vation space is discrete and small, such as the 2-dimension
grid (x,y), we could directly record the number of times
Decentralized learning. We consider an N-agent Markov
each observation that agent i has visited before and define
decisionprocess(MDP)M = {S,O,A,P,R,γ}.Here,S
novelty(o)=1/n(o)wheren(o)denotesthevisitcounts.If
representsthestatespacewhileAisthejointactionspace;
theobservationspacebecomeslargeorcontinuous,methods
thetransitionprobabilityisdefinedbyP(s′|s,a).Indecen-
designed for high-dimensional input such as pseudo-count
tralizedlearning,eachagenthasonlyaccesstoitsownlocal
(Bellemareetal.2016),ICM(Pathaketal.2017),andRND
observationo ∈ O ,ratherthantheglobalstate,andlearns
i i (Burdaetal.2018b)couldbeusedtomeasurethenovelty.
an independent policy π i to maximize the shared reward However, ui only measures the local novelty of agent i.
defined by R together with other agents. Notably, decen- t
In the multi-agent environment, given the discrepancy be-
tralizedlearningismorepracticalthancentralizedlearning, tween the local novelty and the global novelty, ui may not
owingtoitsbetterscalability,privacy,andsecurity(Zhang, t
be able to provide accurate and sufficient information for
Yang,andBasar2019).
exploration.Forexample,weconsideratwo-agentenviron-
Limitedcommunication.Weallowagentstocommunicate mentwhereattimestept,agent1isinanobservationwith
duringthetrainingphase.However,inordertoenhancethe lowlocalnovelty,andagent2isinanobservationwithhigh
practicality and adhere closely to the decentralized setting, local novelty. From the global perspective, the two agents
we impose constraints on the bandwidth of the communi- are in a novel state. However, from agent 1’s perspective,
cationchanneltoreducecommunicationoverhead(Foerster it thinks that the observation is not novel and gives itself a
etal.2016;Kimetal.2018;Wangetal.2020a).Specifically, lowintrinsicreward,preventingitfromfurtherexploringthe
themessagesentbyoneagentateachstepisconfinedtoa currentnovelstate.
floating point number. In this paper, we set agents to com- Duetothedecentralizedsetting,theglobalnoveltyisnot
municate their local novelties through this limited channel. availabletoeachagent.Therefore,weneedamoreappropri-
Communicationisnotallowedduringexecution. ateintrinsicrewardtermthanui t tonarrowthegapwiththe
globalnovelty.Thankstothelimitedcommunication,agents
Notethatthissettingdiffersfromcentralizedtrainingand
canexchangetheirlocalnoveltyui witheachotherateach
decentralizedexecution(CTDE)(Loweetal.2017;Foerster t
timestept.Weproposeaheuristicthatusesthesummation
etal.2018;Rashidetal.2018),whereagentscanuseunlim-
ofallagents’localnoveltyasanapproximationtotheglobal
itedextrainformationtoeasetraining,suchasotheragents’
noveltyandasthenovelty-basedintrinsicreward:
observations and actions, or a centralized value function.
Besides, our setting is not identical to fully decentralized ri (oi,ai)=(cid:88) uj. (1)
learning(Tan1997;deWittetal.2020;JiangandLu2022), nov t t t
j
where communication is forbidden. On top of the fully de-
Withtheintroductionofotheragents’novelty,wecanavoid
centralized learning algorithm, we will show that adding
theaforementioneddilemma.
communicationofnoveltyduringtrainingcanenablecoor-
Weadmitthat(1)stilldeviatesfromtheglobalnoveltyin
dinatedexplorationofagentstosolvesparse-rewardtasks.
some cases. For example, agent 1 and agent 2 are in low-
novelty observations while the global state is novel, which
3 Methodology
occurs when agent 1 and agent 2 seldom visit current ob-
Inthissection,wepresentMACEaddressingthechallenges servations simultaneously. Nevertheless, the gap with the
indecentralizedmulti-agentexploration.MACEfollowsthe global novelty cannot be closed entirely due to the limited
lineofintrinsicallymotivatedexploration(Yangetal.2021) information,andexperimentalresultsproveempiricallythat
that designs intrinsic rewards r and trains agents via the (1) works better than the local novelty ui (Section 5). One
int t
shapedrewardr = r +r ,wherer denotestheex- may argue that an alternative is to use the maximum of all
s ext int ext
trinsicrewardgivenbytheenvironment.MACEadoptsthe agents’noveltyastheintrinsicreward,butourempiricalre-
following two parts to design intrinsic rewards: 1) To ob- sultshowsthat(1)worksbetter(AppendixD.1).
tain a more reliable novelty estimate as the novelty-based
3.2 InfluenceonOtherAgents’Exploration
intrinsic reward, MACE uses the summation of all agents’
novelty to approximate the global novelty (Section 3.1). 2) To boost coordinated exploration in multi-agent environ-
Toboostcoordinatedexploration,MACEfurtherquantifies ments, each agent should consider its influence on other
theinfluenceofagentsontheaccumulatedfuturenoveltyof agents’explorationsothatitcouldfindsomecriticalstatesTable1:Actionsandrewardprobabilitiesoftwoillustrative
states.
0.3
2.0
state1 state2
0.2
act rewardprobability act rewardprobability
1.0
p(r=1|a 1)=0.1 p(r=1|a 1)=0.1 0.1
state 1
a p(r=5|a )=0.8 a p(r=5|a )=0.8
1 1 1 1 state 2
p(r=9|a 1)=0.1 p(r=9|a 1)=0.1 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
p(r=1|a )=0.8 p(r=1|a )=0.1
2 2 p(a 1) p(a 1)
a p(r=5|a )=0.1 a p(r=5|a )=0.1
2 2 2 2 (a) MI (b) WMI
p(r=9|a )=0.1 p(r=9|a )=0.8
2 2
Figure 1: (a) Mutual information (MI) between action and
rewardinstate1andstate2.(b)Weightedmutualinforma-
(Yang et al. 2021). Critical states here mean that in these tion(WMI)betweenactionandrewardinstate1andstate2.
states, the action taken by one agent affects other agents’
exploration progress, e.g., one agent steps on a switch and
thus opens a door that blocks another agent’s way. So en- z tj givenagenti’sobservationoi t:
c tho eu mrag lein arg na tg oe cn ots opto ere ax tepl eo fr fe ecth tie vs ee lyc .r Witi eca fil rs st ta dte is scw uso sul hd owhe tlp o ωI(cid:16) Ai t;Z tj|oi t(cid:1) =
quantifyoneagent’sinfluenceonotheragents’exploration. (cid:34) (cid:35)
p(ai,zj|oi)
Supposetherearetwoagents,agentiandagentj,inthe E ω(ai,zj)log t t t ,
environment. To estimate agent i’s influence on agent j’s ai t,z tj|oi t t t p(ai t|oi t)p(z tj|oi t)
exploration in a specific observation, we could use mutual
whereω(·,·)denotestheweightplacedonthepairofaiand
information, a common measure used in MARL (Li et al. t
2022),toquantifythedependencebetweenagenti’saction zj. By introducing weights, pairs of ai and zj would have
t t t
ai andagentj’saccumulatednoveltyzj = (cid:80) γt′−tuj different informativeness. We set ω(ai,zj) = zj, mean-
t t t′=t t′ t t t
givenagenti’sobservationoi: ingthatrelationalmappingsbetweenai andhigherzj carry
t t t
more significance than others. Then, the final measure we
(cid:34) (cid:35) usecanbewrittenas:
(cid:16) (cid:17) p(ai,zj|oi)
I Ai;Zj|oi =E log t t t . (cid:34) (cid:35)
t t t ai t,z tj|oi t p(ai t|oi t)p(z tj|oi t) ωI(cid:16) Ai;Zj|oi(cid:17) =E zjlog p(ai t,z tj|oi t) .
t t t ai t,z tj|oi
t
t p(ai|oi)p(zj|oi)
t t t t
Hereweuseagentj’saccumulatednoveltyzj insteadofits (2)
t
immediatenoveltyuj tomeasurethelong-termdependence. To illustrate how it works, Figure 1(b) shows weighted
t mutual information of state 1 and state 2 with different
However, mutual information ignores the magnitude of
p(a ), where we can observe that the weighted mutual in-
agent j’s accumulated novelty z tj. So it would give similar form1 ation of state 2 is always higher than that of state 1,
measurementsforobservationoi 1whereai tisrelatedtosome consistent with what we expected. To summarize, (2) eval-
low-valuezj,andobservationoi whereaiisrelatedtosome uatesanobservationoi basednotonlyonwhetheragenti’s
t 2 t t
high-valuezj.Toillustratethestatementintuitively,wede- actionhasaninfluenceonagentj’sexploration,butalsoon
t
visetwostateswithtwoactionsandthreedifferentrewards, whether agent i’s action could lead to a high accumulated
describedinTable1.Actionandrewardherecanbeseenas noveltyofagentj.
ai andzj respectively.AsshowninFigure1(a),withdiffer-
t t 3.3 IntrinsicRewardinHindsight
ent p(a ), state 1 and state 2 always keep the same mutual
1
information. But state 2 is more critical to coordinated ex- To encourage each agent i to visit observations with high
plorationbecausetheaction(a )takenbyagentiinstate2 weightedmutualinformation,wedefineanintrinsicreward
2
canleadagentj tohighaccumulatednovelty(r = 9)more ri ofitsobservationoi as:
wmi t
likely. Although agent i’s action in state 1 has an influence
(cid:88)
onagentj’saccumulatednoveltytothesameextentasthat ri (oi)= ri→j(oi), (3)
wmi t wmi t
instate2measuredbymutualinformation,itismorelikely j̸=i
to result in lower accumulated novelty (r = 1 or r = 5). ri→j(oi)=ωI(Ai;Zj|oi). (4)
Thereforeweneedamoreeffectivemeasuretoestimatethe wmi t t t t
influence of agent i’s action ai t on agent j’s accumulated ri→j denotes the intrinsic reward given to agent i corre-
noveltyzj,whiletakingintoaccountthemagnitudeofzj. spw om ni
dingtoitsinfluenceonagentj’sexplorationmeasured
t t
To this end, we newly introduce weighted mutual infor- byweightedmutualinformation.Agenti’sintrinsicreward
mation (Guiasu 1977; Schaffernicht and Gross 2011) be- ri isthesummationofallri→j,representingitstotalin-
wmi wmi
tweenagenti’sactionai andagentj’saccumulatednovelty fluenceonotheragents’exploration.However,itisnontriv-
tial to compute ri→j according to (2), because it is an ex- Algorithm1:MACEforeachagenti
wmi
pectation over all actions and accumulated novelty. So we
fore=1toM do
decomposetheintrinsicreward(4)ontoeachaction:
fort=1toT do
(cid:34) p(ai,zj|oi) (cid:35) Takeactionai t ∼πi(·|oi t)
ri→j(oi,ai)=E zjlog t t t . (5) Observerewardr andnextobservationoi
wmi t t z tj|oi t,ai t t p(ai t|oi t)p(z tj|oi t) Computelocalnoe vx et ltyui =novelty(oi )t+1
t t+1
Further, we can continue to decompose (5) and obtain a Sendui tootheragents
t
hindsight-basedintrinsicreward: Receive{uj} fromotheragents
t j̸=i
p(ai,zj|oi) Collect(oi t,ai t,r ext,{uj t}N j=1,oi t+1)intobuffer
ri→j(oi,ai,zj)=zjlog t t t . (6) endfor
hin t t t t p(ai t|oi t)p(z tj|oi t) Computez tj =(cid:80)T t′=tγt′−tuj
t′
Herep(ai t|oi t)isthecurrentpolicyπi(ai t|oi t)ofagenti.With Updateestimateddistributionp(ai t|oi t,z tj)
Bayesianrulep(ai|oi,zj) = p(ai t,z tj|oi t),wecanrewritethe Updatepolicyπiusingshapedreward(10)withIPPO
t t t p(zj|oi) endfor
t t
logarithmictermin(6)andhave:
p(ai|oi,zj)
ri→j(oi,ai,zj)=zjlog t t t , (7) followsthedistributionoverzj whichisdeterminedbythe
hin t t t t πi(ai|oi) t
t t current policies of all agents. So our proposed intrinsic re-
(cid:88)
ri (oi,ai,{zj} )= ri→j(oi,ai,zj), (8) ward is more suitable for on-policy reinforcement learning
hin t t t j̸=i hin t t t
algorithms.
j̸=i
whicharetheformsofthehindsight-basedintrinsicreward 3.4 MACE
weuseinthepaper.Theterm‘hindsight’reflectsthediffer-
We combine the novelty-based intrinsic reward (1) and the
encebetween(6)to(8)andnormalrewardfunctions,where
hindsight-based intrinsic reward (8) to get the final shaped
theformeruseinformationobtainedinfuture,i.e.,agentj’s
reward:
accumulatednoveltyzj,whichisnotavailableuntiltheend
t
oftheepisode. ri(oi,ai,{zj} )
s t t t j̸=i
The logarithmic term in (7) is the pointwise mutual in-
= r +ri (oi,ai)+λri (oi,ai,{zj} )
formationbetweenai andzj.Pointwisemutualinformation ext nov t t hin t t t j̸=i
t t
measures the association between two random variables, (cid:88) (cid:88) p(ai|oi,zj)
= r + uj +λ zjlog t t t , (10)
commonly used in natural language processing (NLP). ext t t πi(ai|oi)
Therefore,(7)couldbeinterpretedasencouragingactionas- j j̸=i t t
sociativewiththehighaccumulatednoveltyofagentj.Ifan
where λ is a hyperparameter that denotes the weight of
ai co-occurswithahighzj attimesteptbutthereisnoas- thehindsight-basedintrinsicreward.Inotherwords,λcon-
t t
sociationbetweenthem,thelogarithmictermin(7)willbe trols the weight between encouraging agents to visit glob-
aroundzeroandagentiwillnotreceiveahighintrinsicre- ally novel states and encouraging agents to influence other
ward,despitethehighzj. agents’ exploration. Since the calculation of the hindsight-
t
Note that the hindsight-based intrinsic rewards keep the basedintrinsicrewardrequireson-policysamples,weusein-
followingrelationshipwiththeoriginalri→j(oi): dependentPPO(IPPO)(Schulmanetal.2017;deWittetal.
wmi t
2020)asthebaseRLalgorithmandtraineachagentiwith
(cid:104) (cid:105)
ri→j(oi)=E ri→j(oi,ai) shaped reward (10). Algorithm 1 summarizes our method
wmi t ai|oi wmi t t
t t from the perspective of an individual agent i. To guarantee
(cid:104) (cid:105)
=E ri→j(oi,ai,zj) , (9) scalability, we also propose a scalable hindsight-based in-
ai,zj|oi hin t t t
t t t trinsic reward using weighted mutual information between
thus using ri→j(oi,ai,zj) could be regarded as a Monte theagent’sactionandthesummationofallotheragents’ac-
hin t t t
Carlomethodforestimatingri→j(oi). cumulatednovelty,describedinAppendixE.
wmi t
To calculate ri→j(oi,ai,zj) at each timestep t, agent
hin t t t 4 RelatedWork
i needs agent j’s accumulated novelty zj and the poste-
riordistributionp(ai|oi,zj).Theformerist
computedatthe
Single-agent exploration. Advanced RL algorithms have
t t t beendevelopedtoimproveexploration.Providingtheagent
end of the episode by accumulating agent j’s novelty uj
t with a manually designed intrinsic reward has been proven
thatagentiobtainedthroughcommunication.Notethatthis
effective in environments with sparse rewards like Mon-
doesnotrequireadditionalcommunication,andeachagent
tezuma’sRevenge(Brockmanetal.2016).Typically,thein-
stillcommunicatesonlylocalnoveltyateachtimestep.The
trinsic reward is set to be the novelty of the state, e.g., the
latter could be estimated from trajectory samples. To ful-
inverse of the visit count: r (s) = novelty(s) = 1/n(s),
fill (9), samples used to estimate p(ai t|oi t,z tj) and compute to encourage the agent to tain kt e action towards states it sel-
ri→j(oi,ai,zj)shouldbeon-policybecausetheexpectation dom visits. However, states in real problems are usually
hin t t thigh-dimensional,meaningthatn(s)isimpossibletocount 2021; Li et al. 2021), communication (Wang et al. 2020a),
in most cases. Count-based methods solve this problem by andexploration(Wangetal.2019a;Mahajanetal.2019).In
introducing pseudo-count (Bellemare et al. 2016) or hash- practice,optimizingmutualinformationcanbeachievedby
ing to discretize states (Tang et al. 2017). Other methods transforming it into an intrinsic reward that is added to the
measurenoveltyfromdifferentperspectives,includingpre- environmentalreward(Jaquesetal.2019;Wangetal.2019a;
dictionerroroftransitionmodel(Pathaketal.2017;Burda JiangandLu2021;Lietal.2021,2022)orbyusingitasa
etal.2018a;Kimetal.2019),predictionerrorofstatefea- regularizer in the overall optimization objective (Mahajan
tures(Burdaetal.2018b),policydiscrepancy(Flet-Berliac etal.2019;Wangetal.2020b;Kimetal.2020;Wangetal.
et al. 2020), state marginal matching (Lee et al. 2019), de- 2020a;Caoetal.2021).MACEoptimizesavariantofmu-
viation of policy cover (Zhang et al. 2021a), uncertainty tualinformation,namelyweightedmutualinformation,and
(Houthooft et al. 2016; Pathak, Gandhi, and Gupta 2019), ensures that it is tractable in decentralized learning. In Ap-
and TD error of random reward (Ramesh et al. 2022). Re- pendixA,weprovideadetailedcomparisonbetweenthese
centworkplacesanepisodicrestrictiononintrinsicreward, algorithmsandMACE.
wheretheintrinsicrewardobtainedbyanagentvisitingare-
Decentralized multi-agent reinforcement learning. By
peatedstatewithinanepisodewillbereduced(Badiaetal.
virtueoftheadvantagesofdecentralizedlearning,e.g.,easy
2019; Raileanu and Rockta¨schel 2019; Zhang et al. 2021b;
toimplement,betterscalability,andmorerobust(Jiangand
Henaffetal.2022).
Lu 2022), decentralized learning has attracted much atten-
Multi-agent exploration. Exploration in multi-agent envi- tionfromtheMARLcommunity.Theconvergenceofdecen-
ronmentsrequiresintrinsicrewardthatisdifferentfromthat tralized learning was theoretically studied for cooperative
in single-agent environments. Iqbal and Sha (2019) pro- gamesinnetworkedsettings(Zhangetal.2018)andforfully
posedseveraltypesofintrinsicrewardwhichtakeintocon- decentralized(withoutcommunication)stochasticgamesin
siderationthenoveltyofagenti’sobservationfromtheper- tabular cases (Jin et al. 2021; Daskalakis, Golowich, and
spectiveofagentj.EITI/EDTI(Wangetal.2019a)focuses Zhang 2022), laying the theoretical foundation for decen-
on encouraging the agent to states or observations where tralized learning. de Witt et al. (2020); Papoudakis et al.
the agent influences other agents’ transition or value func- (2021)showedthepromisingempiricalperformanceoffully
tion. EMC (Zheng et al. 2021) uses the summation of the decentralized algorithms including IPPO and independent
prediction errors of local Q-functions as the shared intrin- Q-learning (IQL) (Tan 1993) in several cooperative multi-
sicreward.MAVEN(Mahajanetal.2019)improvesmulti- agentbenchmarks.Recently,JiangandLu(2022)proposed
agentexplorationbymaximizingthemutualinformationbe- I2Q, a practical fully decentralized algorithm based on Q-
tween the trajectory and a latent variable, by which agents learning for cooperative tasks, and proved the convergence
areencouragedtovisitdiversetrajectories.CMAE(Liuetal. of the optimal joint policy, yet limited to deterministic en-
2021)combinesthegoal-basedmethodwithastatespacedi- vironments. However, the existing work does not take into
mensionselectiontechniquetoadapttotheexponentiallyin- consideration coordinated exploration and simply uses ϵ-
creasedstatespace.MASER(Jeonetal.2022)selectsgoals greedyorsamplingfromthestochasticpolicyatindividual
fromtheobservationspaceinsteadofthestatespace. agents.Wetakeastepfurthertoconsiderdecentralizedco-
However,thesemethodsfollowtheCTDEsetting,where ordinatedexplorationandthusenabledecentralizedlearning
unlimited extra information can be used to ease training. algorithms to solve sparse-reward tasks. As discussed be-
SomeuseQMIX(Rashidetal.2018)astheirbackbone(Ma- fore,ourproposedhindsight-basedintrinsicrewardismore
hajan et al. 2019; Zheng et al. 2021; Liu et al. 2021; Jeon suitable for on-policy algorithms, thus we currently build
etal.2022),whileothersrequireagentstosharetheirlocal MACEonIPPO.CombiningMACEwithoff-policydecen-
observations and actions (Iqbal and Sha 2019; Wang et al. tralizedalgorithmslikeIQLorI2Qisleftasfuturework.
2019a). In contrast, MACE is built on top of decentralized
learning algorithms and requires neither a centralized Q- 5 Experiments
functionlikeQMIXnorthecommunicationofobservations
In experiments, we evaluate MACE in three environments:
andactionsbetweenagents.Itonlyneedstopassafloating
pointnumber,i.e.,thelocalnovelty,betweenagents,result- GridWorld, Overcooked (Carroll et al. 2019), and SMAC
(Samvelyan et al. 2019). We set all environments sparse-
inginmuchlesscommunicationoverheadthanthemethods
reward.Sinceweconsiderdecentralizedlearning,agentsin
mentionedabove.Thus,comparisonwiththesemulti-agent
theexperimentsdonotsharetheirparametersandlearnin-
explorationmethodsisoutofthefocusofthispaper.
dependently,followingexistingwork(JiangandLu2022).
Mutual information in MARL. Mutual information is a
widelyusedmathematicaltoolinMARL.Itservesasamea- 5.1 Setup
sure of correlation between variables, and maximizing or
minimizing it can be used as an auxiliary task to improve GridWorld. We design three tasks in GridWorld includ-
theperformanceofMARLalgorithms.Thesealgorithmsop- ing Pass, SecretRoom, and MultiRoom. Pass and
timizemutualinformationbetweendifferentvariablestoad- SecretRoom reference tasks in Wang et al. (2019a) and
dressvariousaspectsofMARLproblems,includingcoordi- Liuetal.(2021).InMultiRoom,thetaskextendstothree
nation(Jaquesetal.2019;Kimetal.2020;Caoetal.2021; agents.Thegoalofalltasksisthatallagentsenterthetarget
Lietal.2022),diversity(Mahajanetal.2019;JiangandLu roomshowninFigure2.Agent2 Switch 2 Agent2 Switch 2 Agent1 Agent3 Switch 2
Agent1 Agent1 Door 1 Agent2 Door 1
TargetRoom Door 4
Switch 3 Switch 3
Door 1 Door 2 Door 2
TargetRoom
Switch 4 Door 5
Switch 1 Door 3 Door 3 (a) Base (b) Narrow (c) Large
TargetRoom Switch 1 Switch 1 Switch 4
(a) Pass (b) SecretRoom (c) MultiRoom Figure3:Overcooked:(a)Base.(b)Narrow.(c)Large.
Figure 2: GridWorld: (a) Pass. (b) SecretRoom. (c)
MultiRoom. game.In3mand8m,agentsalsoreceivea+10rewardwhen
one enemy dies so as to ease the task. In 8m, we use the
scalable hindsight-based intrinsic reward described in Ap-
Pass:Therearetwoagentsinthe30×30grid.Door1will pendixE.
openwhenanyswitchisoccupied.Toachievethegoal,one
Implementation. For all tasks, we implement PPO lever-
agentneedstoreachswitch1toopendoor1sothattheother
aging GRU (Cho et al. 2014) as the policy and critic func-
agent can enter the target room, then the latter agent needs
tion.InGridWorld,giventhattheobservationspaceissmall
toreachswitch2tolettheformeragentcomein.
and discrete, we use the inverse of visit counts as the nov-
SecretRoom: There are two agents in the 30×30 grid.
elty measurement and use a table to record each observa-
Doorkwillopenwhenswitchk+1isoccupiedandalldoors
tion’svisitcount.Also,weuseatabletorecordrecentdis-
willopenwhenswitch1isoccupied.Agentsneedtotakethe cretized accumulated novelty zj and corresponding obser-
samestepsasthatinPasstofinishthetask.SecretRoom t
vation oi and action ai. Then we can estimate the poste-
is harder than Pass because there are three rooms on the t t
rior distribution p(ai|oi,zj) from the table. In Overcooked
righttoexplorebutonlyoneroomisthetarget. t t t
and SMAC, we use RND (Burda et al. 2018b) as the nov-
MultiRoom:Therearethreeagentsinthe30×30grid.In
elty measurement and use an MLP to fit the posterior dis-
detail, door 1 will open when switch 1 is occupied; door tribution p(ai|oi,zj) via supervised learning. More details
3 will open when switch 2 is occupied; door 2 will open t t t
about the novelty measurement, the estimation of the pos-
when switch 4 is occupied; door 4 and door 5 will open
teriordistribution,andthehyperparametersareavailablein
when switch 3 is occupied. More complicated coordinated
AppendixC.
explorationisrequiredamongthethreeagents.Aelaborated
description of the solution to this task is provided in Ap-
5.2 GridWorld
pendixB.1.
We first verify the effectiveness of MACE in promoting
Theepisodeendswhenallagentsareinthetargetroom,
coordinated exploration by ablation studies. We compare
andeachagentreceivesa+100reward.Eachagentobserves
MACEwiththefollowingmethods:
itsownlocation(x,y)andtheopenstatesofdoors.Morede-
tails about this environment are available in Appendix B.1. • IPPO+r loc:Agentsaretrainedwithr +ui,meaning
ext t
These GridWorld tasks serve as didactic examples because thattheyonlytakeintoconsiderationthelocalnovelty.
thecriticalstatesinwhichexplorationofagentsinteractwith • IPPO+r nov:Agentsaretrainedwithr +ri ,mean-
eachotherareobvious,namelytheswitchlocations. ext nov
ingthattheyexploreviaapproximatedglobalnovelty.
Overcooked.WedesignthreetasksinOvercooked(Carroll • IPPO+r hin:Agentsaretrainedwithr +ui+λri ,
ext t hin
etal.2019):Base,Narrow,andLarge.Alltaskscontain meaningthattheyexplorevialocalnoveltyandinfluence
two agents, separated by an impassable kitchen counter as on other agents’ exploration. λ here keeps the same as
showninFigure3.Therefore,thetwoagentsmustcooperate thatusedinMACE.
to complete the task. The left agent has access to tomatoes
The results are shown in Figure 4. Each curve shows
and the serving area (the gray patch in Figure 3), and the
the mean reward of several runs with different random
rightagenthasaccesstodishesandthepot.Agentsneedto
seeds (5 runs in Pass, 8 runs in SecretRoom and
put one tomato into the pot, cook it, put the resulting soup
MultiRoom) and shaded regions indicate standard error.
intoadish,andserveitinorderbypassingitemsthroughthe
IPPO+r loc is unable to solve any task because the local
counter.Whenthesoupisserved,theepisodeends,andeach
noveltyisunreliableandinsufficientforcoordinatedexplo-
agentreceivesa+100reward.ComparedtoBase,Narrow
ration. IPPO+r nov performs better than IPPO+r loc, indi-
limitstheareawhereitemscanbepassedtoonlythemiddle
cating that taking into account the local novelty of other
of the counter, and Large increases the size of the entire
agentstoapproximatetheglobalnoveltyishelpfulforcoor-
environment.Moredetailsaboutthisenvironmentareavail-
dinated exploration. MACE achieves the best performance
ableinAppendixB.2.
on all three tasks, suggesting that the hindsight-based in-
SMAC. We use three maps in SMAC (Samvelyan et al. trinsic reward can further boost coordinated exploration by
2019) 2.4.10: 2m vs 1z, 3m, and 8m, customized to be finding the critical states where the agent influences other
sparse-reward.Agentsreceivea+200rewardiftheywinthe agents’ exploration. This can also be evidenced by the fact100 MACE 100 MACE 100 MACE
80 IPPO + r_hin 80 IPPO + r_hin 80 IPPO + r_hin
60 IPPO + r_nov 60 IPPO + r_nov 60 IPPO + r_nov
IPPO + r_loc IPPO + r_loc IPPO + r_loc
40 40 40
20 20 20
0 0 0
0 1 2 3 4 0 2 4 6 8 0 2 4 6 8
Steps (1e7) Steps (1e7) Steps (1e7)
(a) Pass (b) SecretRoom (c) MultiRoom
Figure4:LearningcurvesofMACEcomparedwithIPPO+r loc,IPPO+r nov,andIPPO+r hinonthreeGridWorldtasks:(a)
Pass,(b)SecretRoom,and(c)MultiRoom.
100 MACE 100 MACE 100 MACE
80 MACE-MI 80 MACE-MI 80 MACE-MI
MACE-Z MACE-Z MACE-Z
60 60 60
40 40 40
20 20 20
0 0 0
0 1 2 3 4 0 2 4 6 8 0 2 4 6 8
Steps (1e7) Steps (1e7) Steps (1e7)
(a) Pass (b) SecretRoom (c) MultiRoom
Figure 5: Learning curves of MACE compared with MACE-MI and MACE-Z on three GridWorld tasks: (a) Pass, (b)
SecretRoom,and(c)MultiRoom.
that IPPO+r hin achieves higher rewards than IPPO+r loc.
We also conduct a parameter study on λ, available in Ap- .029
.200
pendix D.3. A comparison with additional baselines, such
asIPPO,canbefoundinAppendixD.4.
.025 .150
Reward visualization. To further illustrate how the intrin-
sic rewards work, we visualize the novelty-based intrinsic
.021 .100
rewardandthehindsight-basedintrinsicrewardofagent1in
theleftroominPass,averagingover700to1000PPOup-
.050
dates.Thecriticalstatesconsistofoneagentsteppingonone .017
switch because it will open the middle door and allow the
otheragenttoenterthetargetroomandexplore.Asshown (a) novelty-based (b) hindsight-based
in Figure 6(a), agent 1 earns higher novelty-based intrinsic
rewardsatthebottomoftheleftroomthanatthetop.This
Figure6:Visualizationoftheaveraged(a)novelty-basedin-
is because, after agent 1 steps on switch 1 and agent 2 en-
trinsic reward and (b) hindsight-based intrinsic reward re-
ters the target room successfully, agent 1 will receive high
ceivedbyagent1atdifferentpositions.Themapsonlyshow
localnoveltyu2 fromagent2inthefollowingtimesteps.It
theintrinsicrewardsintheleftroominPass.Theredcross
is noticeable that the hindsight-based intrinsic reward can
highlightsthelocationofswitch1.
locate the critical states more accurately. As shown in Fig-
ure6(b),agent1earnsthehighesthindsight-basedintrinsic
rewardaroundswitch1,becausethesepositionscorrespond
to high weighted mutual information: If agent 1 moves to- hindsight-based intrinsic reward with the logarithmic term.
wardsswitch1,itislikelyforagent1toeventuallystepon
Wenameit‘MI’becausetheexpectationofthistermequals
switch1andallowagent2toenterthetargetroomwithhigh themutualinformationbetweenai andzj givenoi.There-
novelty;Ifagent1movesawayfromswitch1,agent2may t t t
sultsinFigure5showthatMACE-MIislesseffectivethan
beblockedfromthetargetroom.
MACEinalltasks,validatingourclaiminSection3.2that
weightedmutualinformationisamoreeffectivemeasureof
Ablation.Thehindsight-basedintrinsicreward(7)consists
of two parts: zj, the accumulated novelty of agent j, and a the influence on other agent’s exploration than mutual in-
t formation.MACE-Z,replacingthehindsight-basedintrinsic
logarithmictermlogp π(a i(i t a|o i ti t |o,z i ttj )).Wetesttheeffectivenessof rewardwithz tj,alsoperformsworsethanMACE,indicating
the two parts separately to verify that none of them alone thatutilizingotheragents’accumulatednoveltyastheintrin-
leadstoMACE’shighperformance.MACE-MIreplacesthe sicreward,regardlessofwhetheritisrelatedtotheagent’s
draweR
egarevA
draweR
egarevA
draweR
egarevA
draweR
egarevA
draweR
egarevA
draweR
egarevA100 MACE 100 MACE 100 MACE
80 IPPO + r_hin 80 IPPO + r_hin 80 IPPO + r_hin
60 IPPO + r_nov 60 IPPO + r_nov 60 IPPO + r_nov
IPPO + r_loc IPPO + r_loc IPPO + r_loc
40 40 40
20 20 20
0 0 0
0.00 0.25 0.50 0.75 1.00 0.0 0.5 1.0 1.5 2.0 0 1 2 3 4
Steps (1e7) Steps (1e7) Steps (1e7)
(a) Base (b) Narrow (c) Large
Figure7:LearningcurvesofMACEcomparedwithIPPO+r loc,IPPO+r nov,andIPPO+r hinonthreeOvercookedtasks:(a)
Base,(b)Narrow,and(c)Large.
1.0 MACE 1.0 MACE 0.8 MACE
0.8 IPPO + r_hin 0.8 IPPO + r_hin IPPO + r_hin 0.6
0.6 IPPO + r_nov 0.6 IPPO + r_nov IPPO + r_nov
IPPO + r_loc IPPO + r_loc 0.4 IPPO + r_loc
0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5 0 2 4 6 8 10
Steps (1e6) Steps (1e6) Steps (1e6)
(a) 2m vs 1z (b) 3m (c) 8m
Figure 8: Learning curves of MACE compared with IPPO+r loc, IPPO+r nov, and IPPO+r hin on three SMAC maps: (a)
2m vs 1z,(b)3m,and(c)8m.
ownactions,isineffective. 6 Conclusion
We propose MACE to enable multi-agent coordinated ex-
5.3 Overcooked ploration in decentralized learning with limited communi-
cation. MACE uses a novelty-based intrinsic reward and a
We then evaluate the performance of MACE in Over-
hindsight-based intrinsic reward to guide exploration. The
cooked(Carrolletal.2019)andcompareitwithIPPO+r loc,
formerisdevisedtonarrowthegapbetweenthelocalnov-
IPPO+r nov,andIPPO+r hin.TheresultsareshowninFig-
elty and the unavailable global novelty. The latter is de-
ure 7. Each curve shows the mean reward of 8 runs with
signedtofindthecriticalstateswhereoneagent’sactionin-
different random seeds, and shaded regions indicate stan-
fluences other agents’ exploration, measured by the newly
dard error. MACE outperforms others, proving that MACE
introduced weighted mutual information metric. Through
also works in the high-dimensional state space where the
empirical evaluation, we demonstrate the effectiveness of
novelty is calculated via RND (Burda et al. 2018b) and
MACEinavarietyofsparse-rewardmulti-agenttaskswhich
the posterior distribution p(ai|oi,zj) is learned via super-
t t t need agents to explore cooperatively. In addition, we ac-
vised learning. We also observe that IPPO+r nov outper-
knowledge that certain limitations exist, such as the need
forms IPPO+r hin in all tasks, especially in Narrow, sug-
forafully-connectedcommunicationnetworktosharenov-
gesting that the novelty-based intrinsic reward may play a
elty among agents. Future work could explore ways to fur-
morecriticalroleinsuchcomplicatedtasks.
therreducethenumberofnecessaryconnectionsbesidesthe
bandwidthofcommunicationchannels.
5.4 SMAC
Acknowledgments
We further examine MACE in more complex SMAC (Car-
roll et al. 2019) tasks and compare it with IPPO+r loc, This work was supported by NSF China under grant
IPPO+r nov,andIPPO+r hin.TheresultsareshowninFig- 62250068.Theauthorswouldliketothanktheanonymous
ure8forthethreemaps:2m vs 1z,3m,and8m.Eachcurve reviewersfortheirvaluablecomments.
shows the mean reward of 8 runs with different random
seeds, and shaded regions indicate standard error. MACE
References
learnsfasterorachievesahigherwinratethantheotherthree
methods,bywhichweverifytheeffectivenessofMACEon Badia,A.P.;Sprechmann,P.;Vitvitskyi,A.;Guo,D.;Piot,
sparse-rewardtasksinsuchahigh-dimensionalcomplexen- B.;Kapturowski,S.;Tieleman,O.;Arjovsky,M.;Pritzel,A.;
vironment. In addition, we demonstrate that MACE could Bolt,A.;etal.2019. NeverGiveUp:LearningDirectedEx-
also work on some SMAC tasks with the dense reward, as plorationStrategies. InInternationalConferenceonLearn-
showninAppendixD.5. ingRepresentations.
draweR
egarevA
etar
niw
tseT
draweR
egarevA
etar
niw
tseT
draweR
egarevA
etar
niw
tseTBellemare, M.; Srinivasan, S.; Ostrovski, G.; Schaul, T.; Houthooft,R.;Chen,X.;Duan,Y.;Schulman,J.;DeTurck,
Saxton,D.;andMunos,R.2016. Unifyingcount-basedex- F.;andAbbeel,P.2016.Vime:Variationalinformationmax-
plorationandintrinsicmotivation.Advancesinneuralinfor- imizing exploration. Advances in neural information pro-
mationprocessingsystems,29. cessingsystems,29.
Brockman, G.; Cheung, V.; Pettersson, L.; Schneider, J.; Iqbal, S.; and Sha, F. 2019. Coordinated exploration via
Schulman, J.; Tang, J.; and Zaremba, W. 2016. OpenAI intrinsic rewards for multi-agent reinforcement learning.
Gym. arXivpreprintarXiv:1606.01540. arXivpreprintarXiv:1905.12127.
Burda, Y.; Edwards, H.; Pathak, D.; Storkey, A.; Darrell, Jaques,N.;Lazaridou,A.;Hughes,E.;Gulcehre,C.;Ortega,
T.;andEfros,A.A.2018a. Large-scalestudyofcuriosity- P.;Strouse,D.;Leibo,J.Z.;andDeFreitas,N.2019. Social
drivenlearning. arXivpreprintarXiv:1808.04355. InfluenceasIntrinsicMotivationforMulti-AgentDeepRe-
Burda,Y.;Edwards,H.;Storkey,A.;andKlimov,O.2018b. inforcementLearning. InInternationalConferenceonMa-
Explorationbyrandomnetworkdistillation. arXivpreprint chineLearning(ICML).
arXiv:1810.12894. Jeon, J.; Kim, W.; Jung, W.; and Sung, Y. 2022. Maser:
Cao, J.; Yuan, L.; Wang, J.; Zhang, S.; Zhang, C.; Yu, Y.; Multi-agentreinforcementlearningwithsubgoalsgenerated
andZhan,D.-C.2021. Linda:Multi-agentlocalinformation fromexperiencereplaybuffer. InInternationalConference
decompositionforawarenessofteammates. arXivpreprint onMachineLearning,10041–10052.PMLR.
arXiv:2109.12508. Jiang, J.; and Lu, Z. 2018. Learning Attentional Commu-
Carroll, M.; Shah, R.; Ho, M. K.; Griffiths, T.; Seshia, S.; nicationforMulti-AgentCooperation. AdvancesinNeural
Abbeel, P.; and Dragan, A. 2019. On the utility of learn- InformationProcessingSystems(NeurIPS).
ing about humans for human-ai coordination. Advances in Jiang,J.;andLu,Z.2021. Theemergenceofindividuality.
neuralinformationprocessingsystems,32. In International Conference on Machine Learning, 4992–
Cho, K.; Van Merrie¨nboer, B.; Bahdanau, D.; and Ben- 5001.PMLR.
gio, Y. 2014. On the properties of neural machine Jiang, J.; and Lu, Z. 2022. I2Q: A Fully Decentralized Q-
translation: Encoder-decoder approaches. arXiv preprint Learning Algorithm. In Advances in Neural Information
arXiv:1409.1259. ProcessingSystems(NeurIPS).
Das,A.;Gervet,T.;Romoff,J.;Batra,D.;Parikh,D.;Rab- Jin,C.;Liu,Q.;Wang,Y.;andYu,T.2021. V-Learning-A
bat,M.;andPineau,J.2019. Tarmac:Targetedmulti-agent Simple, Efficient, Decentralized Algorithm for Multiagent
communication. In International Conference on Machine RL. arXivpreprintarXiv:2110.14555.
Learning,1538–1546.PMLR.
Kim,D.;Moon,S.;Hostallero,D.;Kang,W.J.;Lee,T.;Son,
Daskalakis, C.; Golowich, N.; and Zhang, K. 2022. The K.;andYi,Y.2018. LearningtoScheduleCommunication
Complexity of Markov Equilibrium in Stochastic Games. in Multi-agent Reinforcement Learning. In International
arXivpreprintarXiv:2204.03991. ConferenceonLearningRepresentations.
de Witt, C. S.; Gupta, T.; Makoviichuk, D.; Makoviychuk, Kim, H.; Kim, J.; Jeong, Y.; Levine, S.; and Song, H. O.
V.;Torr,P.H.;Sun,M.;andWhiteson,S.2020. Isindepen- 2019. EMI: Exploration with Mutual Information. In In-
dentlearningallyouneedinthestarcraftmulti-agentchal- ternational Conference on Machine Learning, 3360–3369.
lenge? arXivpreprintarXiv:2011.09533. PMLR.
Ding, Z.; Huang, T.; and Lu, Z. 2020. Learning individ- Kim, W.; Jung, W.; Cho, M.; and Sung, Y. 2020. A max-
ually inferred communication for multi-agent cooperation. imum mutual information framework for multi-agent rein-
Advances in Neural Information Processing Systems, 33: forcementlearning. arXivpreprintarXiv:2006.02732.
22069–22079.
Lee, L.; Eysenbach, B.; Parisotto, E.; Xing, E.; Levine, S.;
Flet-Berliac,Y.;Ferret,J.;Pietquin,O.;Preux,P.;andGeist, and Salakhutdinov, R. 2019. Efficient exploration via state
M. 2020. Adversarially Guided Actor-Critic. In Interna- marginalmatching. arXivpreprintarXiv:1906.05274.
tionalConferenceonLearningRepresentations.
Li,C.;Wang,T.;Wu,C.;Zhao,Q.;Yang,J.;andZhang,C.
Foerster, J.; Assael, I. A.; De Freitas, N.; and Whiteson, S. 2021. Celebratingdiversityinsharedmulti-agentreinforce-
2016. Learning to communicate with deep multi-agent re- mentlearning. AdvancesinNeuralInformationProcessing
inforcementlearning. Advancesinneuralinformationpro- Systems,34:3991–4002.
cessingsystems,29.
Li, P.; Tang, H.; Yang, T.; Hao, X.; Sang, T.; Zheng, Y.;
Foerster, J. N.; Farquhar, G.; Afouras, T.; Nardelli, N.; and Hao,J.;Taylor,M.E.;Tao,W.;andWang,Z.2022. PMIC:
Whiteson,S.2018. CounterfactualMulti-AgentPolicyGra- Improving Multi-Agent Reinforcement Learning with Pro-
dients.InAAAIConferenceonArtificialIntelligence(AAAI). gressiveMutualInformationCollaboration.InInternational
Guiasu, S. 1977. Information Theory with Applications. ConferenceonMachineLearning,12979–12997.PMLR.
NewYork,NY:McGraw-Hill.
Liu,I.-J.;Jain,U.;Yeh,R.A.;andSchwing,A.2021. Co-
Henaff, M.; Raileanu, R.; Jiang, M.; and Rockta¨schel, T. operative exploration for multi-agent deep reinforcement
2022. Exploration via Elliptical Episodic Bonuses. In Ad- learning. In International Conference on Machine Learn-
vancesinNeuralInformationProcessingSystems. ing,6826–6836.PMLR.Lowe,R.;Wu,Y.I.;Tamar,A.;Harb,J.;PieterAbbeel,O.; Wang, T.; Dong, H.; Lesser, V.; and Zhang, C. 2020b.
and Mordatch, I. 2017. Multi-agent actor-critic for mixed ROMA: Multi-Agent Reinforcement Learning with Emer-
cooperative-competitive environments. Advances in neural gentRoles. InInternationalConferenceonMachineLearn-
informationprocessingsystems,30. ing,9876–9886.PMLR.
Mahajan, A.; Rashid, T.; Samvelyan, M.; and Whiteson, S. Wang,T.;Wang,J.;Wu,Y.;andZhang,C.2019a.Influence-
2019.Maven:Multi-agentvariationalexploration.Advances Based Multi-Agent Exploration. In International Confer-
inNeuralInformationProcessingSystems,32. enceonLearningRepresentations.
Papoudakis, G.; Christianos, F.; Scha¨fer, L.; and Al- Wang,T.;Wang,J.;Zheng,C.;andZhang,C.2019b.Learn-
brecht, S. V. 2021. Benchmarking Multi-Agent Deep Re- ingNearlyDecomposableValueFunctionsViaCommunica-
inforcement Learning Algorithms in Cooperative Tasks. tionMinimization.InInternationalConferenceonLearning
arXiv:2006.07869. Representations.
Pathak,D.;Agrawal,P.;Efros,A.A.;andDarrell,T.2017. Yang, T.; Tang, H.; Bai, C.; Liu, J.; Hao, J.; Meng, Z.; and
Curiosity-driven exploration by self-supervised prediction. Liu,P.2021. Explorationindeepreinforcementlearning:a
In International conference on machine learning, 2778– comprehensivesurvey. arXivpreprintarXiv:2109.06668.
2787.PMLR.
Yu,C.;Velu,A.;Vinitsky,E.;Wang,Y.;Bayen,A.;andWu,
Pathak,D.;Gandhi,D.;andGupta,A.2019.Self-supervised
Y.2021. Thesurprisingeffectivenessofmappoincoopera-
exploration via disagreement. In International conference tive,multi-agentgames. arXivpreprintarXiv:2103.01955.
onmachinelearning,5062–5071.PMLR.
Zhang,K.;Yang,Z.;andBasar,T.2019. Multi-AgentRein-
Raileanu, R.; and Rockta¨schel, T. 2019. RIDE: Rewarding
forcementLearning:ASelectiveOverviewofTheoriesand
Impact-Driven Exploration for Procedurally-Generated En-
Algorithms. arXivpreprintarXiv:1911.10635.
vironments. InInternationalConferenceonLearningRep-
Zhang,K.;Yang,Z.;Liu,H.;Zhang,T.;andBas¸ar.,T.2018.
resentations.
Fully Decentralized Multi-Agent Reinforcement Learning
Ramesh, A.; Kirsch, L.; van Steenkiste, S.; and Schmidhu-
with Networked Agents. In International Conference on
ber,J.2022.ExploringthroughRandomCuriositywithGen-
MachineLearning(ICML).
eralValueFunctions. arXivpreprintarXiv:2211.10282.
Zhang, T.; Rashidinejad, P.; Jiao, J.; Tian, Y.; Gonzalez,
Rashid,T.;Samvelyan,M.;Schroeder,C.;Farquhar,G.;Fo-
J. E.; and Russell, S. 2021a. Made: Exploration via maxi-
erster, J.; and Whiteson, S. 2018. Qmix: Monotonic value
mizingdeviationfromexploredregions.AdvancesinNeural
function factorisation for deep multi-agent reinforcement
InformationProcessingSystems,34:9663–9680.
learning. InInternationalconferenceonmachinelearning,
Zhang, T.; Xu, H.; Wang, X.; Wu, Y.; Keutzer, K.; Gonza-
4295–4304.PMLR.
lez,J.E.;andTian,Y.2021b.Noveld:Asimpleyeteffective
Samvelyan,M.;Rashid,T.;SchroederdeWitt,C.;Farquhar,
explorationcriterion. AdvancesinNeuralInformationPro-
G.; Nardelli, N.; Rudner, T. G.; Hung, C.-M.; Torr, P. H.;
cessingSystems,34:25217–25230.
Foerster, J.; and Whiteson, S. 2019. The StarCraft Multi-
Agent Challenge. In Proceedings of the 18th International Zheng, L.; Chen, J.; Wang, J.; He, J.; Hu, Y.; Chen, Y.;
ConferenceonAutonomousAgentsandMultiAgentSystems, Fan, C.; Gao, Y.; and Zhang, C. 2021. Episodic multi-
2186–2188. agent reinforcement learning with curiosity-driven explo-
ration.AdvancesinNeuralInformationProcessingSystems,
Schaffernicht,E.;andGross,H.-M.2011. Weightedmutual
34:3757–3769.
information for feature selection. In International Confer-
enceonArtificialNeuralNetworks,181–188.Springer.
Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and
Klimov,O.2017. Proximalpolicyoptimizationalgorithms.
arXivpreprintarXiv:1707.06347.
Tan,M.1993.Multi-agentreinforcementlearning:Indepen-
dentvs.cooperativeagents. InInternationalConferenceon
MachineLearning(ICML).
Tan,M.1997. Multi-AgentReinforcementLearning:Inde-
pendent versus Cooperative Agents. In International Con-
ferenceonMachineLearning.
Tang,H.;Houthooft,R.;Foote,D.;Stooke,A.;XiChen,O.;
Duan, Y.; Schulman, J.; DeTurck, F.; and Abbeel, P. 2017.
# exploration: A study of count-based exploration for deep
reinforcementlearning.Advancesinneuralinformationpro-
cessingsystems,30.
Wang,R.;He,X.;Yu,R.;Qiu,W.;An,B.;andRabinovich,
Z. 2020a. Learning efficient multi-agent communication:
Aninformationbottleneckapproach. InInternationalCon-
ferenceonMachineLearning,9908–9918.PMLR.A MutualInformationinMARL
Mutualinformationhasbeenwidelyusedinmulti-agentreinforcementlearning.TheseMARLalgorithmsdifferintermsofthe
variablesusedtocomputemutualinformation,thepurposeforwhichmutualinformationisused,andtheforminwhichmutual
informationisemployed.WeprovideasummaryofMARLalgorithmsthatincorporatemutualinformationinTable2.
Table2:MutualInformationinMARL.
Method MutualInformation Purpose Meaning
SI(Jaquesetal.2019)* I(aj;ai|s ) coordination correlation between agent i’s action
t t t
andagentj’action
EITI(Wangetal.2019a)* I(sj ;si,ai|sj,aj) exploration correlation between agent i’s action
t+1 t t t t
andagentj’nextstate
MAVEN(Mahajanetal.2019)† I(τ;z) exploration/ correlation between joint trajectory
diversity andlatentvariable
ROMA(Wangetal.2020b)† I(ρi;τi |oi) diversity correlation between agent i’s trajec-
t t−1 t
toryanditsrole
MMI(Kimetal.2020)† I(πi(·|s );πj(·|s )) coordination correlation between agent i’s policy
t t
andagentj’spolicy
NDVF(Wangetal.2020a)† I(aj;mij|τj,m(−i)j) communication correlation between agent j’s action
t t t t
andmessageintendedfromagentito
agentj
EOI(JiangandLu2021)* I(oi;i) diversity correlationbetweenagenti’sobserva-
t
tionanditsindex
CDS(Lietal.2021)* I(τi;i) diversity correlation between agent i’s trajec-
T
toryanditsindex
LINDA(Caoetal.2021)† I(ci;τj|τi) coordination correlation between agent j’s repre-
j
sentation from agent i and agent j’s
trajectory
PMIC(Lietal.2022)* I(s ;a ) coordination correlationbetweenstateandjointac-
t t
tion
MACE(ours)* ωI(ai;zj|oi) exploration correlation between agent i’s action
t t t
and agent j’s accumulated novelty,
taking into account the magnitude of
agentj’saccumulatednovelty
* Mutualinformationisemployedasanintrinsicreward.
† Mutualinformationisemployedasaregularizer.
B EnvironmentDetails
B.1 GridWorld
Thedoor-switchrulesinMultiRoomareasfollows:
• Door1willopenwhenswitch1isoccupied.
• Door3willopenwhenswitch2isoccupied.
• Door2willopenwhenswitch4isoccupied.
• Door4anddoor5willopenwhenswitch3isoccupied.
Toachievethegoal,agentsneedtotakethefollowingstepsinorder:
• Oneagentreachesswitch1andletsanotheragententertheroomcontainingswitch2throughdoor1.
• Oneagentreachesswitch2andletsanotheragententertheroomcontainingswitch4throughdoor3.
• Oneagentreachesswitch4andletsanotheragententerthetargetroomthroughdoor2.Agent1 Switch 2
Agent3
Door 1
Agent2
Door 4
Switch 3
Door 2
TargetRoom
Door 5
Door 3
Switch 1 Switch 4
Figure9:GridWorldenvironment:MultiRoom
• Oneagentreachesswitch3andletstheothertwoagentsenterthetargetroomthroughdoor4ordoor5.
In GridWorld, each agent can observe its own location (x,y) and the open states of doors indicated by 0 (closed) and 1
(open).Sothedimensionsofobservationspacesinthethreetasksare3,5,and7respectively.Theactionspacecontainsfour
actions,includingmoveup,movedown,moveleft,andmoveright.Themaximumepisodelengthissetto300.
B.2 Overcooked
Alltaskscontaintwoagents,separatedbyanimpassablekitchencounterasshowninFigure10.Therefore,thetwoagentsmust
cooperatetocompletethetask.Theleftagenthasaccesstotomatoesandtheservingarea(thegraypatchinFigure10),andthe
rightagenthasaccesstodishesandthepot.Agentsneedtoputonetomatointothepot,cookit,puttheresultingsoupintoa
dish,andserveitinorderbypassingitemsthroughthecounter.Weusetheopen-sourceOvercookedenvironment1 ofCarroll
etal.(2019)(MITLicense).Wemakesomemodificationstotheoriginalenvironment,including:
1. We restrict the observation ranges of agents. In the original environment, each agent could observe objects regardless of
thedistance.Toincreasethebiasbetweentheglobalnoveltyandthelocalnovelty,weaddanoptiontosettheobservation
rangeoftheagent.Itemsoutsidetheobservationrangearetreatedasnon-existent.Forexample,inourtasks,wecouldset
theleftagenttonotbeabletoobservethedishesandthepotintherightroom.
2. Weremovethecookingtimeofthesouptoeasethetask.Intheoriginalenvironment,ittakes20timestepstocookasoup.
Wesetthecosttimeto0,i.e.,thesoupiscookedimmediatelyaftertheagentinteractswiththepot.
3. Wesettheepisodetoendwithonesuccessfulservinginsteadofafterafixedtimestep.Intheoriginalenvironment,agents
needtoservethecorrectsoupintherecipeasmanyaspossibleinafixed-lengthepisode.Inourtasks,theonlysoupinthe
recipeconsistsofonetomato,andtheepisodeendsimmediatelywhenthecorrectsoupisserved.
We use the featurized state provided by the environment as the observation and add the observation range restriction. The
observationspaceinourtaskscontains38dimensions.InBaseandNarrow,werestricteachagenttoobserveonlytheitems
initsroomoronthemiddlecounter,asshowninFigure10.Wedonotrestrictagents’observationrangesinLarge,because
thepreliminaryexperimentshowsLargeisdifficulttolearnwiththerestrictedobservationrange.Theactionspacecontains
sixactions,includingmoveup,movedown,moveleft,moveright,interact,andstay.Themaximumepisodelengthissetto300.
(a) Base (b) Narrow
Figure10:Observationrangesin(a)Baseand(b)Narrow.Thebluedashedboxrepresentstheobservationrangeoftheleft
agent,andthegreendashedboxrepresentstheobservationrangeoftherightagent.
1https://github.com/HumanCompatibleAI/overcooked aiB.3 StarCraftMulti-AgentChallenge
StarCraftMulti-AgentChallenge(SMAC)(Samvelyanetal.2019)isacommonlyusedcooperativemulti-agentreinforcement
learningenvironment.Weusetheopen-sourceSMACenvironment2(MITLicense).Weselectthreemaps:2m vs 1z,3m,and
8m.In2m vs 1z,twoMarinesneedtodefeatanenemyZealot.In3m,threeMarinesneedtodefeatthreeenemyMarines.In
8m,eightMarinesneedtodefeateightMarines.
C ImplementationDetails
C.1 NoveltyEstimation
InGridWorld,thenoveltyofanobservationoisdefinedas:
1
novelty(o)=10· ,
(cid:112)
n(x ,y )
o o
wheren(x ,y )isthevisitcountofthecoordinate(x,y)ino.Eachagenthasa30×30tabletorecorditsvisitcountsofall
o o
coordinates.Eachcellinthetablerecordsthevisitcountofthecorrespondingcoordinate.
In Overcooked (Carroll et al. 2019) and SMAC (Samvelyan et al. 2019), we use RND (Burda et al. 2018b) to compute
novelty.Thenoveltyisdefinedas:
novelty(o)=(cid:13) (cid:13)f(o;θ)−f¯(o)(cid:13)
(cid:13) . (11)
2
f¯(·)isafixedandrandomlygeneratedMLP(targetnetwork),andf(·;θ)isanMLPthatistrainedtominimize(11)(predictor
network).Duetotraining,themoretimesanobservationoisvisited,thesmallerthedifferencebetweentheoutputvectorsof
thesetwoMLPsonthisobservation.ThenetworkarchitecturesandoptimizersettingsareavailableinTable3.Thepredictor
networkistrainedtogetherwithIPPO,sootherhyperparametersincludingbuffersize,numberofmini-batch,andepocharethe
sameasthoseinIPPO(AppendixC.3).Wedonotuserewardandobservationnormalization.
Table3:HyperparametersforRNDnetwork.
Hyperparameter Value
hiddenlayersofthetargetnetwork [64,64]
hiddenlayersofthepredictornetwork [64,64,64,64]
outputsize 32
optimizer Adam
optimizerepsilon 1e-5
learningrate 3e-4
C.2 PosteriorDistributionEstimation
InGridWorld,weuseacount-basedmethodtoestimatetheposteriordistributionp(ai|oi,zj):
t t t
n(ai,oi,zj)
pˆ(ai|oi,zj)= t t t , (12)
t t t n(oi,zj)
t t
where n(·) is the visit count of each (ai,oi,zj) pair and (oi,zj) pair. Each agent i uses a table to record the visit counts of
(ai,oi,zj)pairswhicharerelatedtoagentj.n(oi,zj)iscomputedvia(cid:80) n(ai,oi,zj).SupposethereareN agentsinthe
ai
environment,eachagentrequiresN −1tables.
Touseatabletosavethevisitcountsofall(ai,oi,zj)pairs,weneedtodiscretizezj.However,discretizationisfacedwith
aproblem:noveltyuj willkeepdecliningwithsampling,resultinginacontinuousdeclineofzj,sozj cannotbedividedinto
binsbysettingfixedboundaries.Tosolvethisproblem,wefirstdiscretizethenoveltyusingpercentile.Inmoredetail,aftereach
samplingiscompleted,wecalculatethe20th,40th,60th,and80thpercentilesofalluj inthissamplingasbinedges.Theneach
uj isdividedintoonebinandrelabelledas0.1,0.3,0.5,0.7,or0.9.Wedenotetherelabellednoveltyasu˜j andtheaccumulated
relabellednoveltyasz˜j,wherez˜j = (cid:80) γt′−tu˜j.Thescaleofz˜j isstable,i.e.,between0.1/(1−γ)to0.9/(1−γ).We
t t′=t t′
replacezj withz˜j anddiscretizeitintoK bins.
Intheory,tofulfill(9),weneedon-policysamplestocalculate(12),meaningthatn(·)onlycounts(ai,oi,zj)pairsincurrent
PPOsampling.However,wefindthatincludingsomeoff-policydata,i.e.,thesamplescollectedinpreviousPPOsampling,to
calculate(12)canimprovetheoverallperformanceofthealgorithmbyvariance-biastradeoff(AppendixD.2).Therefore,we
recordthesamplesfromthepreviouswtimesofPPOsamplingandusethemtocalculate(12).
2https://github.com/oxwhirl/smacGiventhemethodsdescribedabove,thesizeofatablerelatedtoagentj isw×|A|×|S|×K.Pass,SecretRoomand
MultiRoomshareacommon|A|=4.|S|ofthreetasksis30×30×2,30×30×23,and30×30×25,respectively.Inour
experiments,wesetw =10andK =30.
InOvercooked(Carrolletal.2019)andSMAC(Samvelyanetal.2019),weuseanMLPf (·|o,z)toestimateeachposterior
p
distribution p(ai|oi,zj) via supervised learning. f takes as input oi and zj and outputs a predicted distribution of action.
t t t p
Then f is trained to minimize the cross entropy between the predicted distribution and true action. Given that there are
p
N −1otheragentsintheenvironment,eachagentrequiresN −1f s,eachcorrespondingtooneotheragentj.Thecommon
p
hyperparameters used in Overcooked and SMAC are available in Table 4. Like GridWorld, we use another buffer containing
previous samples to train these MLPs. In Overcooked, the buffer size is 1e5 for Base and Narrow, and 2e5 for Large. In
SMAC,thebuffersizeis5e4for2m vs 1zand1e4for3mand8m.
Table4:Commonhyperparametersforf inOvercookedandSMAC.
p
Hyperparameter Value
hiddenlayers [64,64]
optimizer Adam
optimizerepsilon 1e-5
learningrate 3e-4
epoch 40
nummini-batch 1
C.3 IPPOHyperparameters
Table5describesthecommonhyperparametersforIPPOacrossalltasks.ThemeaningofthehyperparametersfollowsYuetal.
(2021).Table6showsthespecifichyperparametersusedineachtask,includingparametersthatcontrolthesamplingprocedure
andλthatcontrolstheweightofthehindsight-basedintrinsicreward.
Table5:CommonhyperparametersforIPPOacrossalltasks.
CommonHyperparameter Value
GRUhiddenlayers [64]
fchiddenlayers [64,64]
recurrentdatachunklength 10
gradientclipnorm 10.0
gaelambda 0.95
gamma 0.99
valueloss huberloss
huberdelta 10.0
nummini-batch 1
optimizer Adam
optimizerepsilon 1e-5
actorlearningrate 7e-4
criticlearningrate 7e-4
epoch 10
activation ReLU
entropycoef 0.05
PPOclip 0.2
networkinitialization orthogonal
gain 0.01
D MoreExperimentalResults
D.1 Summationv.s.MaximumofLocalNovelty
WecomparetheperformanceofIPPOtrainedwithr +(cid:80) uj(denotedasIPPO+sum u)andIPPOtrainedwithr +max uj
ext j t ext j t
(denotedasIPPO+max u)onPass.TheresultinFigure11showsthatIPPO+sum uworksbetterthanIPPO+max u.Therefore,Table6:SpecifichyperparametersforIPPOineachtask.
GridWorld Overcooked SMAC
Hyperparameter
Pass SR* MR* Base Narrow Large 2m vs 1z 3m 8m
numenvs 128 128 128 32 32 32 8 8 8
bufferlength 300 300 300 300 300 300 600 600 600
λin(10) 0.01 0.01 0.01 0.1 0.1 0.1 1.0 0.005 0.1
* SRandMRareshortforSecretRoomandMultiRoomrespectively.
wechoosetousethesummationofagents’localnoveltyasthenovelty-basedintrinsicrewardinMACEinsteadofthemaximum
ofagents’localnovelty.
100
IPPO + sum_u
80 IPPO + max_u
60
40
20
0
0 1 2 3 4 5
Steps (1e7)
Figure11:Learningcurvesusingsummation/maximumoflocalnoveltyonPass.
D.2 MACEwithDifferentw
Figure12showstheperformanceofMACEwithdifferentwincluding1,10,and50,onthreeGridWorldtasks.Wecanobserve
thatMACEwithw = 10performsbestacrossthreetasks.MACEwithw = 1,whichestimate(12)withon-policysamples,
performs worse than MACE with w = 10 on SecretRoom and MultiRoom. MACE with w = 50 performs worse than
MACEwithw =10onPassandSecretRoom.Theseresultsshow:Ontheonehand,usingon-policydatatoestimate(12)
isunbiased,butitmaycausealargevarianceduetothesmallamountofdata.Ontheotherhand,estimating(12)bycombining
off-policy data with on-policy data could increase the amount of data and reduce the variance, but it is biased. Therefore, w
controlsthebalancebetweenbiasandvariance.
100 MACE w=50 100 MACE w=50 100 MACE w=50
80 MACE w=10 80 MACE w=10 80 MACE w=10
MACE w=1 MACE w=1 MACE w=1
60 60 60
40 40 40
20 20 20
0 0 0
0 1 2 3 4 0 2 4 6 8 0 2 4 6 8
Steps (1e7) Steps (1e7) Steps (1e7)
(a) Pass (b) SecretRoom (c) MultiRoom
Figure 12: Learning curves of MACE with different w on three GridWorld tasks: (a) Pass, (b) SecretRoom, and (c)
MultiRoom.
D.3 MACEwithDifferentλ
Figure13showstheperformanceofMACEwithdifferentλincluding0.1,0.01,and0.001,onthreeGridWorldtasks.MACE
withλ=0.01outperformsMACEwithλ=0.1andMACEwithλ=0.001significantlyacrossalltasks.Therefore,λ=0.01
can maintain a good balance between the novelty-based intrinsic reward encouraging the agent to globally novel states and
the hindsight-based intrinsic reward encouraging the agent to influence other agents’ exploration. As shown in Table 6, in
Overcooked,λissettobe0.1acrossalltasks.However,inSMAC,MACEhasadifferentλfor2m vs 1z,3mand8m.The
mainreasonisthateachmapinSMACmayrequireadifferentstrategytowinthegame.Coordinatedexplorationislikelyto
playamoreimportantrolein2m vs 1zthanin3mand8m.Inmoredetail,thewinningstrategyin2m vs 1zisalternatingfire,
wheretheMarinechasedbyZealotkeepsrunningaway,andtheotherMarinefiresatZealot.Soagentsinthistaskrequirehighly
draweR
egarevA
draweR
egarevA
draweR
egarevA
draweR
egarevAcoordinatedexploration.Incontrast,explorationofagentsaremoreindependentin3mand8m,wheretheagent’sbehaviordoes
notaffectotheragentsfiringatenemies.WecanalsoobservethisdifferencefromFigure8:IPPOwiththelocalnoveltyreaches
around37.5%winningrate(3outof8seedsreach100%winningrate)in3m,butcannotlearnatallin2m vs 1z,verifyingthat
explorationismoreindependentin3mthanin2m vs 1z.RegardingthepoorperformanceofIPPO+r locin8m,wespeculate
thatthereasonmaybetheincreasinggapbetweenthelocalnoveltyandtheglobalnoveltywiththeincreasingnumberofagents.
Besides,theinteractionbetweenagentsbecomesmorecomplicatedin8m,makingcoordinationmoreimportantandrequiring
ahigherλthan3m.
100 MACE =0.1 100 MACE =0.1 100 MACE =0.1
80 MACE =0.01 80 MACE =0.01 80 MACE =0.01
MACE =0.001 MACE =0.001 MACE =0.001
60 60 60
40 40 40
20 20 20
0 0 0
0 1 2 3 4 0 2 4 6 8 0 2 4 6 8
Steps (1e7) Steps (1e7) Steps (1e7)
(a) Pass (b) SecretRoom (c) MultiRoom
Figure 13: Learning curves of MACE with different λ on three GridWorld tasks: (a) Pass, (b) SecretRoom, and (c)
MultiRoom.
D.4 ComparisonwithMoreBaselines
We compared MACE with some additional baselines on three GridWorld tasks. These baselines include: a) IPPO: the basic
decentralizedRLalgorithmweusedinthispaper;b)MAPPO(Yuetal.2021):apopularsimple-yet-effectiveCTDEmethod;
c) MASER (Jeon et al. 2022): a recent SOTA centralized multi-agent exploration method. The results shown in Figure 14
indicate that none of IPPO, MAPPO, and MASER succeeds on GridWorld tasks. IPPO and MAPPO fail as a result of the
sparserewardsignalandtheabsenceofmotivationtoexplore.Tooursurprise,MASERalsoshowsineffective.Wespeculate
thatitsineffectivenesscouldbeattributedtothemethoditselfanditsdefaulthyperparameters,asweusedherefollowingthe
provided code3, which appears to be tailored specifically for SMAC tasks. CMAE (Liu et al. 2021) is another contemporary
centralizedmulti-agentexplorationmethod.Figure1inCMAEpaperpresentsitsperformanceonPassandSecretRoom.
CMAE proves better sample efficiency than MACE on these two tasks due to its off-policy learning, tabular Q-function, and
accesstotheglobalstate.
100 MACE 100 MACE 100 MACE
80 MAPPO 80 MAPPO 80 MAPPO
IPPO IPPO IPPO
60 60 60
MASER MASER MASER
40 40 40
20 20 20
0 0 0
0 1 2 3 4 0 2 4 6 8 0 2 4 6 8
Steps (1e7) Steps (1e7) Steps (1e7)
(a) Pass (b) SecretRoom (c) MultiRoom
Figure 14: Learning curves of MACE compared with IPPO, MAPPO, and MASER on three GridWorld tasks: (a) Pass, (b)
SecretRoom,and(c)MultiRoom.
WealsocarriedoutanexperimenttocompareMACEwiththesebaselinesonthe2m vs 1zwithsparsereward.Theresult
averagedover8seeds,ispresentedinFigure15(a),whichshowsthatMACEachievesahigherwinratethanMASER,indicating
that our proposed intrinsic rewards are more successful in encouraging cooperative exploration on 2m vs 1z. MAPPO and
IPPOalsofailtolearnavalidcooperativepolicyonthismapduetosparse-reward.
D.5 DenseReward
To verify whether MACE can work well on hard tasks with dense reward, we carried out an experiment to compare MACE
withIPPOonahardSMACmap,3s vs 5z,withnormalreward.Asthenormalrewardisscaledtoamaximumof20(default
inSMAC),weintroduceanewhyperparameterβ toscaleourintrinsicrewardsusedinEquation(10):
(cid:88)
ri =r +β(ri +λ ri→j) (13)
s ext nov hin
j̸=i
3https://github.com/Jiwonjeon9603/MASER
draweR
egarevA
draweR
egarevA
draweR
egarevA
draweR
egarevA
draweR
egarevA
draweR
egarevAWesetβ to0.1andλto0.01,whilekeepingotherhyperparametersthesameasthoseusedin3m.Theresult,averagedover
8seeds,isshowninFigure15(b).MACEoutperformsIPPOsignificantly,provingthatMACEalsoworkswellonhardtasks
withdensereward.Furthermore,thewinrateofMACEisslightlyhigherthanthatofourablationIPPO+r nov,indicatingthe
effectivenessofusinghindsight-basedintrinsicrewardstofacilitatecoordinatedexplorationinnormaldenserewardtasks.
1.0 MACE 0.6 MACE 100 MACE
0.8 MAPPO 0.5 IPPO + r_nov 80 IPPO + r_hin
0.6 IPPO 0.4 IPPO 60 MACE-s
MASER 0.3 IPPO + r_hin-s
0.4 40
0.2
0.2 0.1 20
0.0 0.0 0
0 1 2 3 4 0 1 2 3 4 0 2 4 6 8
Steps (1e6) Steps (1e6) Steps (1e6)
(a) 2m vs 1z (b) 3s vs 5z (c) MultiRoom
Figure15:(a)LearningcurvesofMACEcomparedwithMAPPO,MASER,andIPPOon2m vs 1zwithsparsereward.(b)
Learning curves of MACE compared with IPPO+r nov, and IPPO on 3s vs 5z with dense reward. (c) Learning curves of
MACEcomparedwithMACE-s,IPPO+r hin,andIPPO+r hin-sonMultiRoom.
E Scalability
Accordingtothedefinitionofthehindsight-basedintrinsicreward(7)andtheestimationofposteriordistributionp(ai|oi,zj)
t t t
described in Appendix C.2, each agent requires N −1 repeated modules (tables in GridWorld, and f s in Overcooked and
p
SMAC)ifthereareN agentsintheenvironment.Eachmoduledescribesaconditionaldistributionoftheagent’sactiononone
other agent’s accumulated novelty and its own observation. Therefore, the size of each agent’s model increases linearly with
thenumberofagentsintheenvironment.ToguaranteethescalabilityofMACEwhenextendingtomoreagents,weproposea
scalablehindsight-basedintrinsicrewardri .Insteadofweightedmutualinformationbetweenagenti’sactionai andagent
hin−s t
j’saccumulatednoveltyzjgivenagenti’sobservationoi,weuseweightedmutualinformationbetweenaiandz−i =(cid:80) zj,
t t t t j̸=i t
thesummationofallotheragents’zj,givenoi:
t t
ωI(cid:0) Ai;Z−i|oi(cid:1) =E (cid:20) z−ilog p(ai t,z t−i|oi t) (cid:21) . (14)
t t t ai t,z t−i|oi
t
t p(ai|oi)p(z−i|oi)
t t t t
Then we can define the intrinsic reward ri (oi) = ωI(cid:0) Ai;Z−i|oi(cid:1) , decompose it, and get the scalable hindsight-based
wmi t t t t
intrinsicreward:
p(ai|oi,z−i)
ri (oi,ai,{zj} )=z−ilog t t t . (15)
hin−s t t t j̸=i t πi(ai|oi)
t t
Comparedto(7),(15)reducescomputationoverhead,becauseeachagentrequiresonlyonemoduletorepresenttheconditional
distributionoftheagent’sactiononthesummationofallotheragent’saccumulatednoveltyanditsownobservation.
Wetestthescalablehindsight-basedintrinsicrewardonMultiRoom,wherez−i isthesummationoftheothertwoagents’
t
accumulated novelty. Figure 15(c) shows the performance of MACE, IPPO+r hin, MACE-s, and IPPO+r hin-s, in which
MACE-s and IPPO+r hin-s replace ri in MACE and IPPO+r hin with ri respectively. The learning curves of MACE
hin hin−s
andMACE-sareclose,verifyingtheeffectivenessofthescalablehindsight-basedintrinsicreward.IPPO+r hinperformsbetter
thanIPPO+r hin-s,suggestingthatri maybemoreaccurateandeffectivethanri alone.
hin hin−s
etar
niw
tseT
etar
niw
tseT
draweR
egarevA