Minimum Description Length and Generalization
Guarantees for Representation Learning
MiladSefidgaran∤, AbdellatifZaidi:∤, PiotrKrasnowski∤
∤ ParisResearchCenter,HuaweiTechnologiesFrance :Universite´GustaveEiffel,France
{milad.sefidgaran2,piotr.g.krasnowski}@huawei.com, abdellatif.zaidi@univ-eiffel.fr
Abstract
Amajorchallengeindesigningefficientstatisticalsupervisedlearningalgorithmsis
findingrepresentationsthatperformwellnotonlyonavailabletrainingsamplesbutalso
onunseendata.Whilethestudyofrepresentationlearninghasspurredmuchinterest,
mostexistingsuchapproachesareheuristic;andverylittleisknownabouttheoretical
generalizationguarantees. Forexample, theinformationbottleneckmethodseeksa
goodgeneralizationbyfindingaminimaldescriptionoftheinputthatismaximally
informativeaboutthelabelvariable,whereminimalityandinformativenessareboth
measuredbyShannon’smutualinformation.
Inthispaper,weestablishacompressibilityframeworkthatallowsustoderiveupper
bounds on the generalization error of a representation learning algorithm in terms
of the “Minimum Description Length” (MDL) of the labels or the latent variables
(representations). Rather than the mutual information between the encoder’s input
andtherepresentation,whichisoftenbelievedtoreflectthealgorithm’sgeneralization
capabilityintherelatedliteraturebutinfact,fallsshortofdoingso,ournewbounds
involvethe“multi-letter”relativeentropybetweenthedistributionoftherepresentations
(orlabels)ofthetrainingandtestsetsandafixedprior.Inparticular,thesenewbounds
reflectthestructureoftheencoderandarenotvacuousfordeterministicalgorithms.
Ourcompressibilityapproach,whichisinformation-theoreticinnature,buildsupon
thatofBlum-LangfordforPAC-MDLboundsandintroducestwoessentialingredients:
block-codingandlossy-compression. Thelatterallowsourapproachtosubsumethe
so-calledgeometricalcompressibilityasaspecialcase. Tothebestknowledgeofthe
authors,theestablishedgeneralizationboundsarethefirstoftheirkindforInformation
Bottleneck(IB)typeencodersandrepresentationlearning.Finally,wepartlyexploitthe
theoreticalresultsbyintroducinganewdata-dependentprior.Numericalsimulations
illustratetheadvantagesofwell-chosensuchpriorsoverclassicalpriorsusedinIB.
1 Introduction
Akeyperformanceindicatorofstochasticlearningalgorithmsistheircapabilitytogeneralize,i.e.,perform
equallywellontrainingandunseendata.However,designinglearningalgorithmswithgoodgeneralization
guaranteesremainsamajorchallenge.Apopularapproachinvolveslearninganencoderpartandadecoder
part.Theencoderaimsatgeneratingasuitablerepresentationoftheinput(referredtoas“latentvariable”)
byextractingrelevantfeaturesfromtheinputdata.Thedecoderaimsatoptimizingtheperformanceofthe
learningtaskforthegiventrainingdataset,knownasempiricalriskminimization(ERM),basedonlyonthe
learnedlatentvariables.ThisapproachisgroundedontheideathatperformingERMonthelatentvariable
(insteadoftheinputitself)preventsoverfitting.
InformationBottleneck.Severalapproacheshaveattemptedtoformalizetheconceptofa“goodrepresen-
tation”[Shamiretal.,2010,Alemietal.,2017,VanDenOordetal.,2017,Duboisetal.,2020].Perhaps,
themostprominentistheinformationbottleneck(IB)methodwhichwasfirstintroducedin[Tishbyetal.,
2000]andthenextendedinseveraldirections[Shamiretal.,2010,Alemietal.,2017,AguerriandZaidi,
2019,Kolchinskyetal.,2019,Fischer,2020,Rodr´ıguezGa´lvezetal.,2020,Kleinmanetal.,2022].The
37thConferenceonNeuralInformationProcessingSystems(NeurIPS2023).
4202
beF
5
]LM.tats[
1v45230.2042:viXraIBapproachwasdeemedusefultoanalyzedeepneuralnetworks[Shwartz-ZivandTishby,2017,Zaidi
etal.,2020,GoldfeldandPolyanskiy,2020,Geiger,2021]andguidetheirtrainingprocess.Forinstance,
insupervisedlearningtaskstheIBseeksrepresentationsthatcapture“minimum”informationaboutthe
inputdata(shootsforgoodgeneralizationpower)whileproviding“maximum”informationaboutthelabel
(shootsforsmallempiricalrisk),wheretheamountsofcapturedandprovidedinformationaremeasured
usingShannonmutualinformation.Moreprecisely,denotingbyY thelabelvariableandbyX theinput
data,theIBlatentvariableU ischosensoastomaximizethemutualinformationIpU;Yqwhileminimizing
themutualinformationIpU;Xq.Equivalently,thiscanbeformulatedasmaximizingtheLagrangecost
IpU;Yq´βIpU;Xq,
whereβ ě0designatesaLagrangemultiplier.LetpXm,Umqdenoteavector,orblock,ofmindependent
andidenticallydistributed(i.i.d.)realizationspX i,U iq,i“1,...,m,ofdiscretevariablespX,Uq„P X,U.
FixadistributionP
Uˆ
suchP
X,Uˆ
“P X,U.Essentiallybymeansoftherate-distortiontheoreticcovering
lemma[CoverandThomas,2006],itiseasytoseethatonecangetasuitabledescriptionUˆ ofX using
only IpU;Xq bits per symbol. Precisely, generating a codebook of roughly lm « 2mIpU;Xq vectors
uˆmrjsPUm,j “1,...,m,alldrawni.i.d. accordingtoP U,thecoveringlemmastatesthatforlargem
thereexistsatleastoneindexjforwhichtheempiricaldistributionofpXm,uˆmrjsqisarbitrarycloseto
P
X,U
insomesuitablesense.Thatis,theempiricaldistributionsofpXm,UmqandpXm,uˆmrjsqareclose.
Hence,an“equivalent”versionUˆmofUmcanbedescribedwithroughlymIpU;Xqbits.Intuitively,this
makesaconnectionbetweenthemutualinformationIpU;XqandtheconceptofMinimalDescription
Length(MDL)[Rissanen,1978,Gru¨nwaldetal.,2005]whenoneconsidersMDLofthelatentvariables,
notthatofmodelparameters[Veraetal.,2018,Zaidietal.,2020]. Moreover,itisnowrelativelywell
knownthatthereexistsaconnectionbetweenthegeneralizationerrorofalearningmodelandtheMDL
oftheparametersofthatmodel,see,e.g., [Blumeretal.,1987,BlierandOllivier,2018,Gru¨nwaldand
Roos,2019].Anotablework[BlumandLangford,2003]hasconsideredMDLofthepredictedlabelsofa
super-sampleoftrainingandtestdata.
CriticstoIB.TheaforementionedconnectionsperhapsformedabeliefthatIpU;Xqiscloselyrelatedto
thegeneralizationperformanceofrepresentationlearningalgorithms.Thisbelief,however,iscontroversial
andconflictingevidencehasbeenreported[GeigerandKoch,2019].Forinstance,usingthetermIpU;Xq
asaregularizerhasbeencriticizedforfourmainreasons[Kolchinskyetal.,2018,RodriguezGalvez,2019,
AmjadandGeiger,2019,Duboisetal.,2020]:i.Thefewexistingupperboundsonthegeneralizationerror
whichinvolve(amongotherterms)themutualinformationIpU;Xqreportedin[Veraetal.,2018]andthe
veryrecentandconcurrentwork[Kawaguchietal.,2023]arenotconvincing. Forexample,thebound
of[Veraetal.,2018]holdsonlywhenthealphabetsoftheinputandlatentspaces´arefinit¯e; aand,inthatcase,
itstatesthatforanyδą0,withprobability1´δitholdsthat:genp `S,W Lqď ˘O log npnq IpU;Xq`C δ,
?
wheregenpS,WqisthegeneralizationerrorofmodelW,C
δ
:“O |U| n ,|U|isthesizeofthelatent
spaceandnisthesizeofthetrainingdataset. Forreasonablesetups,however,thetermC dominates
δ
and their bound becomes vacuous [Rodriguez Galvez, 2019, Lyu et al., 2023]. The bound reported
in[Kawaguchietal.,2023]sufferssimilarshortcomings–Forfurtherdetailsonthis,seeAppendixB.2.
ii. Experimental evidence shows dependence of the generalization error on the so-called geometrical
compressionratherthanonIpU;Xq[GeigerandKoch,2019]. Geometricalcompressionoccurswhen
thelatentvariablesareconcentratedaroundalimitednumberofclusters. See[GeigerandKoch,2019,
Fig.2]foravisualrepresentation. iii. IpU;Xqisinvarianttobijection; and,assuch,itdoesnotfavor
learningalgorithms/representationswithsimpledecisionboundariesanddoesnotreflectthe“structure”or
“simplicity”oftheencoder/decoder.Pleasereferto[AmjadandGeiger,2019,Section4.3]foradetailed
discussionandexamples.iv.Finally,fordeterministicalgorithmsIpU;Xqcantakelargeoreveninfinite
values,especiallyforcontinuousvariablesorhigh-dimensionaldata,hencelimitingtheusefulnessofIB
[Kolchinskyetal.,2018,AmjadandGeiger,2019].
MDL/Compressibility. Several works have studied MDL/compressibility to establish generalization
bounds.Inthiscontext,keyisthe“Occam’srazor”principle[LittlestoneandWarmuth,1986,Blumeretal.,
1987,BlumandLangford,2003]which,e.g.,forbinaryclassificationtasks,statesthatifthelabelsofthe
trainingdataS “tZ 1,...,Znu,Z
i
“pX i,Y iqpredictedbyamodelW canbedescribedbyk!nnumber
ofbits,thenthelearnedmodelW hasagoodgeneralizationperformance.Thereexistthreecloselyrelated
linesofworkthatusedifferentmeanstodescribethedatasetlabels: i. Thefirstdescribesthepredicted
labelsviathehypothesis(parametermodels). Itincludestheworksof[Blumeretal.,1987,Aroraetal.,
2018,Suzukietal.,2020,Hsuetal.,2021,Barsbeyetal.,2021,Sefidgaranetal.,2022].Recently,itwas
shownin[Sefidgaranetal.,2022,SefidgaranandZaidi,2023]thatinformation-theoreticbounds[Russo
2andZou,2016,XuandRaginsky,2017,SteinkeandZakynthinou,2020],PAC-Bayesbounds[McAllester,
1998]andintrinsicdimension-basedapproaches[S¸ims¸eklietal.,2020]alsofallintothiscategory. The
proofusesso-called“fixed-size”[Sefidgaranetal.,2022]and“variable-size”[SefidgaranandZaidi,2023]
compressibilityframeworksintroducedtherein.ii.Thesecondformulatestheminimaldescriptionoflabels
asfollows:ifalearningalgorithmcanpredictalllabelsofthetrainingdataSbyusingonlysamplesfroma
smallsubsetofS,thenthelearningalgorithmisguaranteedtogeneralizewell.Thissecondapproachwas
initiatedby[LittlestoneandWarmuth,1986]andfollowedbyothersincluding[HannekeandKontorovich,
2019,Hannekeetal.,2019,Bousquetetal.,2020,HannekeandKontorovich,2021,Hannekeetal.,2020,
Cohen and Kontorovich, 2022]. iii. The third, initiated by [Blum and Langford, 2003], deals directly
withthecompressionofthepredictedlabels. Thisapproachisshowntoberelatedtotheprevioustwo
lines,andalsotoPAC-BayesandVC-dimension-basedresults.Moreover,itistheclosesttotheproblem
ofestablishingboundsonthegeneralizationerrorofrepresentationlearningalgorithmsintermsofthe
compressibilityofthelatentvariables.ThisapproachisdetailedinAppendixB.1.
Inthiswork,weaimatunderstandingthetheoryofrepresentationlearningthrougharigorousinvestigation
of the connection between the MDL (or compressibility) of the latent variable and the generalization
performanceofrepresentationlearningalgorithms. Indoingso,wefirststudythesingle-stepprediction
model of Fig. 1a; and, then, we leverage the developed tools to study the two-step (encoder-decoder)
predictionmodelofFig.1b.WealsoexaminetheclaimedrelationshipbetweenMDLandIpU;Xq.
Contributions.Specifically,themaincontributionsofthisworkareasfollows.
• ForthepredictionmodelofFig. 1a,inspiredby[BlumandLangford,2003]weestablishaninformation-
theoreticframeworkthatallowsustomeasurethecompressibility(MDL)ofthepredictedlabelsinterms
ofanewobjectwhichistheKL-divergenceofavectoroflabelsandanyarbitrarysymmetricprior.For
aproperchoiceofprior,thisnewmeasurereducestothesample-wisemutualinformationandhence
inheritstheassociatedproperties.However,unlikethesample-wisemutualinformation,itcanalsoreflect
thestructure/simplicityofthelearningalgorithm. Furthermore,byextendingtheframeworkto“lossy
compressibility”,thisnewmeasuredoesnotbecomevacuouswhenoneconsiderscontinuousvariables
insteadofdiscretelabels.Moreover,itsubsumesgeometricalcompressibilityasaspecialcase.
• Wealsoestablishbothin-expectationandtailgeneralizationboundsintermsofthiscompressibility
measureofthepredictedlabels.Inasimplecase,thein-expectationboundcanbecastintotheform
c
2ˆMDL(PredictedLabels)
.
n
Thetailboundinvolvesasimilarexpression.ThisgeneralizationboundeasilyrecoverstheVC-dimension
boundasaspecialcase;and,hence,italsoshowshowtheintroducedcompressibilitymeasuredepends
onthecomplexityofthehypothesisclass.
• Ourresultsmakeaconnectionbetweenthecompressibilityframeworkof[BlumandLangford,2003]
andthefunctionalconditionalmutualinformation(f-CMI)of[Harutyunyanetal.,2021,Hellstro¨mand
Durisi,2022],whichitselfisanextensionoftheCMI-frameworkdevelopedby[SteinkeandZakynthinou,
2020]. Inparticular,thisconnectionshowshowthef-CMIframeworkcanbeleveragedtostudythe
compressibilityofthepredictedlabels.
• Fortheencoder-decoderpredictionmodelofFig.1b, theframeworkisextendednon-triviallytothe
caseinwhichinsteadofthecompressibilityofthepredictedlabelsitisthecompressibilityofthelatent
variablewhichisconsidered. Theresultsforthispredictionmodel,whichareourmainresultsinthis
paperasgiveninSection3,aregeneralizationboundsfortherepresentationlearningalgorithmsinterms
ofthecomplexityofthelatentvariable. Theestablishedin-expectationandtailboundsholdforany
decoder;and,fortheK-classclassificationtaskforexample,taketheform
c
2ˆMDL(LatentVariables)`K`2
2 .
n
These bounds appear to be the first of their kind for the studied representation learning setup. In
part,theirutilityliesinthat: i)theyreflectthestructureoftheencoderclass,ii)theydonotbecome
vacuousfordeterministicencoderswithcontinuouslatentspaceandiii)theycanexplainthegeometrical
compressibilityphenomenon.Thus,ourframeworkrevealsthatthe“joint”MDLofthelatentspaceis
morerelatedtotheencoderstructure,ratherthanthemutualinformationIpU;Xq.
• Finally,ourresultssuggestthatdata-dependentpriorscanbeusedinplaceofthedata-independentprior
ofthepopularvariationalIB(VIB)method.Weconductexperimentsthatillustratetheadvantagebrought
upbyproperchoicesofdata-dependentpriorsoverVIB.
3training training
S W S W=(We ,Wd )
Y X Ŷ Y X U Ŷ
(a)Single-steppredictionmodel. (b)Two-steppredictionmodel.
Figure1: Consideredlearningframeworks.
OurresultsalsoopenupseveralfuturedirectionsasdiscussedinAppendixC.4.
Notation.Randomvariables,theirrealizations,andtheiralphabetsaredenotedrespectivelybyupper-case
letters,lower-caseletters,andCalligraphyfonts,e.g.,X,x,andX. Theirdistributionsandexpectations
aredenotedbyP
X
andErXs.Foreaseofpresentation,whenX isadiscreteset,P
X
isaprobabilitymass
function.Otherwise,P
X
isaprobabilitydensityfunction.AcollectionofnrandomvariablespX 1,...,Xnq
isdenotedbyXnorX. Weusethenotationtx ium i“1todenoteasequenceofmrealornaturalnumbers.
Thesett1,...,Ku,K PN,isdenotedbyrKs.Finally,R`denotesthesetofnon-negativerealnumbers.
Ourresultsareexpressedintermsofinformation-theoreticfunctions.FortwodistributionsP andQ,the
Kullback–Leibler(KL)divergenceisdefinedasD KLpP}Qq:“E PrlogpP{QqsifP !Q,and8otherwise.
ThemutualinformationbetweentworandomvariablesX,Y „P
X,Y
withmarginalsP
X
andP
Y
isdefined
as IpX;Yq :“ D KLpP X,Y}P XP Yq. The reader is referred to [Cover and Thomas, 2006, Csisza´r and
Ko¨rner,2011]forfurtherinformation.
Ourresultsinthispaperwillbeexpressedintermsofsymmetricconditionalpriors.Thefollowingdefinition
formalizesthreetypesofsymmetry.
Definition 1 (Symmetric Priors). For U,V „ P U,V, let pU2n,V2nq be vectors composed of 2n
i.i.d. instances pU i,V iq „ P U,V, i P r2ns. For any permutation π: r2ns Ñ r2ns, denote U π2n :“
pU πp1q,...,U πp2nqqandV π2n :“pV πp1q,...,V πp2nqq.
• Type-Isymmetry: Definetype-Ipermutationsasthesetofpermutationsπ: r2ns Ñ r2nshavingthe
propertythatforanyiPrns,thesetstπpiq,πpi`nquandti,i`nuareequal. Theconditionalprior
QpU2n|V2nqhastype-IsymmetryifQpU π2n|V π2nqisinvarianttoarbitrarytype-Ipermutations. This
definitionfirstappearedin[Audibert,2004](called“almostexchangeableprior”therein)andwasused
fortheCMIframeworkin[Grunwaldetal.,2021].
• Type-IIsymmetry:TheconditionalpriorQpU2n|V2nqhastype-IIsymmetryifQpU π2n|V π2nqisinvariant
toanyarbitrarypermutationsπ: r2nsÑr2ns.
• Type-IIIsymmetry:ForarandomvariableZ „P Z|U2n,V2n,theconditionalpriorQpU2n|V2n,Zqhas
type-IIIsymmetryifQpU π2n|V2n,Zqisinvarianttoanypermutationπ: r2nsÑr2nshavingtheproperty
thatV
i
“V πpiqforeveryiPr2ns.
Throughout,iftheunderlyingpU2n,V2nqisclearfromthecontext,foreaseofthenotationthecorrespond-
ingsetsofType-IandType-IIpriorswillbedenotedsimplyasQ andQ respectively.
i ii
Problemsetup.Unlessindicatedotherwise,weconsidertheK-classclassificationsetup.LetZ “pX,Yq
besomeinputdatatakingvalueovertheinputspaceZ “X ˆY accordingtoanunknowndistributionµ.
WecallX PX thefeaturesofthedata,andY PY itslabel,whereY “rKs.Weassumeatrainingdataset
S “tZ 1,...,Znu„µbn “:P S,composedofni.i.d.samplesZ
i
“pX i,Y iqoftheinputdata,isavailable.
WedenotethefeaturesandlabelsofSbyX:“Xn „µbnandY:“Yn „µbn,respectively.Weoften
X Y
usealsoaghostortestdatasetS1 “ tZ 11,...,Z n1u „ µbn “: P S1,whereZ i1 “ pX i1,Y i1q. Similarly,we
denotethefeaturesandlabelsofS1byX1 :“X1n „µbnandY1 :“Y1n „µbn,respectively.
X Y
2 Generalizationboundsintermsofpredictedlabelcomplexity
Inthissection,weformulateacompressibilityframeworkthatallowsustoderiveupperboundsonthe
generalizationerrorofarepresentationlearningalgorithm. Ourproposedframeworkcanbeseenasa
suitablegeneralizationoftheframeworkofBlumandLangford[BlumandLangford,2003],inwhichthe
generalizationencompasseslossycompressionofthepredictedlabelsandwhichexploitsblock-coding.
However,unlikeintheoriginalframeworkbasedona(compression)gamebetweentwoagentsAliceand
Bob(seeAppendixB.1fordetails),thisworkadoptsarate-distortiontheoreticperspective.
4Foreaseofexposition,theresultsarepresentedforclassificationproblemsandthe0-1lossfunction,but
theycanbeextendedtriviallytoanycontinuousY andboundedlossfunction.ConsiderthesetupinFig.1a.
LetA: Zn ÑW beapossiblystochasticlearningalgorithm.Thatis,foragivenS “pZ 1,...,ZnqPZn
thealgorithmpicksahypothesisormodelW “ApSqPW. Also,lettheinducedjointdistributionover
SˆW bedenotedasP S,W;andtheinducedconditionaldistributionoverW givenSbedenotedasP W|S.
Foreveryinputdataz“px,yqPZ,everychoiceofhypothesiswPW inducesaconditionaldistribution
P Yˆ|X,WpYˆ|x,wqonYˆ“Y.Forconvenience,weusethefollowinghandynotations:
ź
P Yˆb |n X,Wpyˆ|x,wq“: źiPrnsP Yˆ|X,Wpyˆ i|x i,wq,
P Yˆb |n X,Wpyˆ1|x1,wq“: źiPrns´P Yˆ|X,Wpyˆ i1|x1 i,wq,
¯
P Yˆb |2 Xn ,Wpyˆ,yˆ1|x,x1,wq“:
iPrns
P Yˆ|X,Wpyˆ i|x i,wqP Yˆ|X,Wpyˆ i1|x1 i,wq .
Thequalityofthepredictionismeasuredbythelossfunctionℓ: ZˆW Ñr0,1sgivenby
ℓpz,wq:“E
Yˆ„P
Yˆ|X,WpYˆ|x,wqr1 ty‰Yˆus, (1)
where 1 stands for the indicatřor function. The associated empirical and population risks for this loss
are defined as Lˆps,wq :“ n1 iPrnsℓpz i,wq and Lpwq :“ E Z„µrℓpZ,wqs, respectively. Finally, the
generalizationerrorisdefinedasgenps,wq:“Lpwq´Lˆps,wq.
2.1 Compressibilityframework
Now,weintroducebrieflythejointcompressionofablock ofthepredictedlabels. Furtherdetailscan
befoundinAppendixC.1. Considermi.i.d. pairsoftrainandtestdatasetsS
j
:“ pZ j,1,...,Z j,nqand
S j1 :“ pZ j1 ,1,...,Z j1 ,nq, where Z j,i “ pX j,i,Y j,iq and Z j1 ,i “ pX j1 ,i,Y j1 ,iq. Let Sm “ pS 1,...,Smq,
S1m “pS 11,...,S m1 qandWm :“pW 1,...,Wmq,whereW
j
„P Wj|Sj.Denotethepredictedlabelsusing
modelW j forinputsX j,i andX j1 ,i asYˆ j,i andYˆ j1 ,i,forj P rmsandi P rns. Moreover,letZ2 jn P Z2n
denotearearrangementoftheelementsofpS j,S j1qinawaythatmakesitindistinguishablewhetheragiven
samplezisfromS orS1. Inthefollowingtwosubsections,weinvestigatetwosuchrearrangementsof
j j
agivenpS,S1qasZ2n. Then,forthecollectionpSm,S1mq,eachpairpS j,S j1qforj P rmsisrearranged
independentlyandthematrixofallrearrangementsisdenotedbyZ2mn. Finally,givensomeZ2mn,let
denotetherearrangedversionsofpYmn,Y1mnqandpYˆmn,Yˆ1mnqrespectivelybyY2mnandYˆ2mn.
Ourapproachisbasedonstudyingthecompressibilityoftherearrangedmodel-predictedlabelsYˆ2mn,
fromaninformation-theoreticpointofview.Therationaleisasfollows:sincethe(rearranged)predicted-
labelsvectorYˆ2mnagreesmostlywiththetruelabelsY2mnonthedatasetSm,theninaccordancewith
“Occam’sRazor”theorem[LittlestoneandWarmuth,1986,Blumeretal.,1987]themodelW isguaranteed
togeneralizewellifYˆ2mn canbedescribedusingonlyafewbits(ornats). Weleveragesourcecoding
argumentstomeasurethecompressibilityofYˆ2mn.Inourblock-codingrate-distortiontheoreticframework,
acompressionrateRPR`issaidtobeachievableifthereexistsacompressioncodebookofsize«emR
(fixedapriori)whichcoversthespacespannedbythemodel-predictedlabelsYˆ2mnwithhighprobability.
Thatis,ifRisachievablethenYˆ2mn canbedescribedusingR P R` nats. Formally,Risachievable
if there exists a sequence of label books tYˆ mumPN, with Yˆ m :“ tyˆrrs,r P rlmsu Ď Y2mn, lm P N,
yˆrrs “ pyˆ 1rrs,...,yˆmrrsqandyˆ jrrs “ pyˆ j,1rrs,...,yˆ j,2nrrsq P Y2n, suchthat: i. lm ď emR andii.
thereexistasequencetδmumPNforwhichlimmÑ8δm “0suchthatwithprobabilityatleastp1´δmq
overthechoicesofSm,S1monecanfindatleastoneindexrPrlmswhoseassociatedyˆrrsequalsYˆ2mn.
AsexplainedfurtherinAppendixC.1.3,thisfixed-sizecompressibilityframework,whichissuitableto
upperboundtheexpectationofthegeneralizationerror,canbeextendedtovariable-sizecompressibilityina
waythatissimilarto[SefidgaranandZaidi,2023]inordertoupperboundthegeneralizationerrorwithhigh
probability.Wenoticethat,inessence,thecoreideaofthecompressibilityframeworkofBlum-Langford,
whichwerecallinAppendixB.1,canbeseenasaone-shotcounterpartofourapproach. Asdiscussed
in[Sefidgaranetal.,2022,SefidgaranandZaidi,2023],intheone-shotcaseonedealswiththe“worst
case”scenario;andthisresultsinboundsthatinvolvecombinatorialterms[BlumandLangford,2003].
Incontrast,ourinformation-theoreticframeworkallowsustoboundthecompressionrateRintermsofa
simplernewquantity:therelativeentropyofthejointconditionalPpYˆn,Yˆ1n|Yn,Y1nqanda(symmetric)
conditionalpriorQoverYˆ2ngivenY2n.
5OurapproachtoestablishingaboundonthecompressibilityofYˆ2mnintermsofinformation-theoretic
measuresstartingfromthecombinatorialapproachof[BlumandLangford,2003]andbyusingblock-coding
essentiallyconsistsinasuitablecombinationofthefollowingkeyproofsteps: (i)introduceandusethe
symmetriesofDefinition1torearrangepSm,S1mq,(ii)generatethecodewordsusingsymmetricpriors
and(iii)analyzetheminimumcodebooksizeneededto“cover”reliablyYˆ2mn,essentiallybyuseofthe
coveringlemma. Asshownin[Sefidgaranetal.,2022,SefidgaranandZaidi,2023]andalsoourproofs,
thelasttwoingredientscanbemergedviatheDonsker-Varadhan’svariationalrepresentationlemma.The
applicationofthislemmaisreminiscentofinformation-theoreticworksonthegeneralizationerrorsuch
as[RussoandZou,2016,XuandRaginsky,2017,SteinkeandZakynthinou,2020].
2.2 Generalizationboundsusingtype-Isymmetricpriors
OnewaytorearrangeindistinguishablypS,S1qasZ2nisasfollows:LetJ“pJ 1,...,Jnqbeavectorof
ni.i.d. Bernoullip1 2qrandomvariablesJ
i
Pti,i`nu,iPrns. ForeveryiPrns,lettherandomvariable
J ic P ti,i`nubedefinedsuchthatJ ic “ i`nifJ i “ iandJ ic “ iifJ i “ i`n. Also,letthevector
Jc “ pJ 1c,...,J ncq. Fori P rns, welettherandomvariablesZ Ji andZ J ic dedefinedasZ Ji “ Z i and
Z Jc “Z i1. ObservethatthevectorZ2n “pZ 1,...,Z 2nqisaJ-dependentrandomre-arrangementofthe
i
samplesofthetrainingandghostdatasetsSandS1.WithoutknowledgeofJeveryelementofthevector
Z2nhasequallikelihoodtobefromSorS1.Asimilarconstructionwasusedinthecontextoftheanalysis
ofRademachercomplexityandtheCMIof[SteinkeandZakynthinou,2020]. WeuseType-Isymmetric
priors,blockcoding,andinformation-theoreticcoveringargumentsinordertoanalyzethespacespanned
bytherandomvectorZ2n.Thisyieldsthegeneralizationboundstatedinthenexttheoremwhoseproofis
deferredtoAppendixE.2.
Theorem1. i. LetQ ibaethesetoftype-IsymmetricconditionalpriorsonpYˆ,Yˆ1qgivenpY,Y1q.Then,
E S,WrgenpS,Wqsď 2R{n,with
Rď Qi Pn Qf iE Y,Y1” D KL´ E X1,X,W” P Yˆb |2 Xn ,WpYˆ,Yˆ1|X,X1,Wqı› › ›Q¯ı “I´ J;Yˆ2nˇ ˇ Y2n¯ (2)
whereY,Y1 „µb2nandX1,X,W „P P .Alsothemutualinformationiscalculatedwith
Y X1|Y1 X,W|Y
respecttothejointdistr ”ibutionP J,Yˆ2n,Y2n “Bernp1{2 ıqbnµb Y2nP Yˆ J2n,Yˆ J2 cnˇ ˇ Y J2nY J2 cn andthelatterterm
isdefinedasE
X1,X,W
P Yˆb |2 Xn ,WpYˆ J2n,Yˆ J2 cn|X,X1,Wq inwhichX1,X,W „P
X1|Y J2
cnP
X,W|Y
J2n.
ii. ForanyδPR`andanyconditionaltype-IsymmetricpriorQonpYˆ,Yˆ1qgivenpX,Y,X1,Y1q,with
probabilityatleastp1´δqoverchoicesofS,S1,W „P S1P S,W,itholdsthat1
d
ˆ ˆ › ˙ ˙
LˆpS1,Wq´LˆpS,Wqď 2n4
´1
D
KL
P Yˆb |2 Xn ,WpYˆ,Yˆ1|X,X1,Wq› › ›Q `logp? 2n{δq .
A couple of remarks are in order. The part i of the result of Theorem 1 can be understood as being
someformofthef-CMIof[Harutyunyanetal.,2021]inwhichthefunctionf representsthepredicted
labels–infact,ourresultisslightlystrongercomparatively,sinceinsteadofconditioningonZ2nasinthe
f-CMIof[Harutyunyanetal.,2021]onehereconditionsonlyonY2n.Incidentally,theresultalsounveils
anappealingconnectionbetweenanextensionofthecompressibilityapproachof[BlumandLangford,
2003](inthisextensiononeneedstoconsiderType-Isymmetricpriors)andCMIandf-CMI[Steinkeand
Zakynthinou,2020,Harutyunyanetal.,2021,Hellstro¨mandDurisi,2022].However,fortype-IIandtype-III
symmetries,usedinthecomingsections,ourframeworkgoesbeyondtheCMIframework.Thetype-Ipriors
havebeenalsousedin[Grunwaldetal.,2021]toestablish(fast-rate)tailboundsfortheCMIframework.
Duetothedifferencebetweentheconsideredsetups,theirresultsarenotdirectlycomparablewithours.
Wealsonotethattheaboveresult(aswellassomethatwillfollow)arekeptsimpleforreasonsofclarity
oftheexpositionbuttheycanbeextendedstraightforwardly,e.g.,bymřeansofanyorcombinationsof:i)
movingE
Y,Y1
outsidethesquareroot,ii)usingthatErgenps,wqs“ n1 iPrnsErgenpz i,wqsandapplying
theboundforeveryErgenpz i,wqsasin[Buetal.,2020,Harutyunyanetal.,2021,Rodr´ıguez-Ga´lvezetal.,
2021],iii)extendingtheresultsinawaythatisessentiallysimilartofore-CMI[SteinkeandZakynthinou,
1Thereadermaynoticetheabsenceofa“disintegratedbound”here,asopposedtotheclassicalsingle-draw
PACBayesbound[Catoni,2007].Thisisduetothechoiceofthelossfunction,whichincludesanexpectation
term.
62020, Hellstro¨m and Durisi, 2022], and iav) establishing tail bound on genpS,Wq by noting that with
probabilityp1´δq,LˆpS1,WqěLpWq´ logp1{δq{p2nq(similartoTheorem5).
“ ‰
Next,observethatforanyQ:“E
X,X1|Y,Y1
Q 1pYˆ,Yˆ1|X,X1,Y,Y1q ,whereQ 1pYˆ,Yˆ1|X,X1,Y,Y1q
isanarbitrarytype-Isymmetricprior,andbyusingtheJensen’sinequality,wehave
” ` › ˘ı
RHSof(2)ďE
S,S1,W„PS,WP S1
D
KL
P Yˆb |2 Xn ,WpYˆn,Yˆ1n |Xn,X1n,Wq› Q
1
. (3)
RelationtoMutualInformation. AparticularchoiceoftheconditionalpriorQ intheRHSof(3)is
1
Qb2nforsomepriorQdefinedoverY.Forthisspecialchoice,theRHSof(3)isgivenby
“ ` ˘‰ “ ` ˘‰
nE S,WE
X„µˆX|S
D
KL
P Yˆ|X,WpYˆ|X,Wq}Q `nE
X1,W„µXPW
D
KL
P Yˆ|X,WpYˆ1|X1,Wq}Q ,
(4)
whereµˆ istheempiricaldistributionofX inthedatasetS.Moreover,bychoosingQasthemarginal
X|S
distributionofYˆ underP P ,itiseasytoseethatthefirsttermofthesumoftheRHSof(4)
Yˆ|X,W S,W
coincideswithnIˆpX;Yˆqforthatchoice,whereIˆp¨;¨qstandsforthe“empiricalmutualinformation”as
computedfromtheavailablesamples.Thus,thecontributionofthistermtotheboundongeneralization
errordoesnotnecessarilyvanishasnÑ8(unlesstheempiricalmutualinformationitselfissmall). In
fact,asalreadyobservedin[GeigerandKoch,2019],thereexistmodelswhichgeneralizewellbuthave
non-smallmutual-informationIˆpX;Yˆq. Thisinstantiatesthatmutual-informationtypeboundsmayfall
shortofexplainingtruegeneralizationcapability,anobservationwhichwasalreadymadein[Geigerand
Koch,2019].TheboundonthegeneralizationerrorofourTheorem1,whichisprovablytighter(see(3)),
thenpossiblyremediesthisissue.
InwhatfollowswefurtherinvestigatetherelationshipoftheresultofourTheorem1,whichisbasedon
KL-divergence,toVC-dimensionandmutualinformationtypeboundsonthegeneralizationerror.
Relation to VC-dimension Suppose that the VC-dimension of the hypothesis class is d. Then, using
theSauer–Shelahlemma[Sauer,1972,Shelah,1972]wegetthatgivenanyXandX1 onecanhaveat
mostp2en{dqddistinctlabels.BylettingtheconditionalpriorQ 1beauniformdistributionoverallsuch
possiblepredictions, itiseasilyseenthattheKLdivergencetermofourTheorem1isupperbounded
bydlogp2en{dq. ThismeansthattheresultofourTheorem1recoversandpossiblyimprovesoverthe
VC-dimensionbound.
Structureand“simplicity”ofthelearningalgorithm.Letusconsiderasimpleexample.Supposethat
X “ r0,1s Ă Randletτ P p0,1qbeaparameter. Then,considerthesetofdeterministicclassifiersw
asfollows: P Yˆ|X,Wpyˆ|x,wq“1if(yˆ“1andxěτ)or(yˆ“0andxăτ);andP Yˆ|X,Wpyˆ|x,wq“0
otherwise. ItiswellknownthattheVC-dimensionofthislearningclassisd “ 1. Then, byrecalling
theaforementionedrelationtothaeVCdimension,weobtainthatourTheorem1yieldsaboundonthe
generalizationerrorwhichisOp logpnq{nq. Inparticular,itiseasytoseethatthisboundvanishesas
nÑ8. ThisisinsharpcontrastwiththemutualinformationtermIˆpX;Yˆqwhichdoesnotnecessarily
vanishforlargen. Forexample,ifthepairpX,YqissuchthatY “ 1iffX ě τ˚ forsomeτ˚ P p0,1q,
thentheratiobetweenRHSof(4)andnconverges(forlargen)toavaluewhichisatleastHpYq(because
IˆpX;YˆqÑIpX;YqasnÑ8and,inthisexample,IpX;Yq“HpYq).
Theaboveexampleindicatesthatusingmutualinformationasaregularizerforlearningalgorithms(asis
thecaseinIB)mayfailtofindmodelsthatgeneralizewellandhaveacomparativelysimplestructure.In
fact,usingadifferentapproach,itwasalreadyobservedin[AmjadandGeiger,2019]and[Duboisetal.,
2020]thatusingthemutualinformationtermasacomplexitymeasuredoesnotreflectthe“structure”ofthe
learningalgorithmandconsideringitasaregularizermightnotfavor“simple”learningalgorithms.This
suggeststhatmutualinformationregularizerintheIBapproachmaybereplacedbytheKLdivergence
termoftheRHSof(3)(whenthelatentvariablesareconsideredinsteadofpredictions)whichdoesreflect
thecomplexityoftheencoder’sstructure.AsinvestigatedandshowninSection3,inadditiontofavoring
encodersofsimplerstructure,ourapproachprovidestheoreticalguaranteesofthegeneralizationerror.
2.2.1 Lossycompressiblity
Theaboveapproachandresultscanbeextendedtoanyboundedlossfunctionandcontinuousvariables
Y andYˆ (seethenextsectionwherecontinuouslatentvariablesarestudied).Intheregimeofcontinuous
variables,astandardresultofrate-distortiontheorystipulatesthatanylosslessencodingofYˆnandYˆ1n
mayrequireaninfinitenumberofbits,makingthegeneralizationboundsvacuous.Thisispreciselywhy
7Shannon’smutualinformationfailsasaregularizerindeterministiclearningalgorithmswithcontinuous
alphabetvariables.Ontheotherhand,ourapproachcanbeeasilyextendedtoincludethelossycompression
ofthelabels(seeAppendixC.1.2;inthesamespiritasdonein[Sefidgaranetal.,2022]forthehypothesis
compression.Thefollowingresultstatesalossyin-expectationboundandalossytailboundisreportedin
AppendixA.
Theorem2. LetQ ibethesetoftype-IsymmetricconditionalpriorsonpYˆ,Yˆ1qgivenpY,Y1q.Then,for
anyϵPR,E S,WrgenpS,Wqsisupperboundedby
d
„ ˆ ” ı› ˙ȷ
1 ›
Pi Wn ˆf |SQi Pn Qf
i
nE
Y,Y1
D
KL
E
X1,X,Wˆ
P Yˆb |2 Xn ,WˆpYˆ,Yˆ1|X,X1,Wˆq ›Q `ϵ,
whereY,Y “1 „ µb Y2n, X1,X,Wˆ „ P ‰X1|Y1P X,Wˆ|Y, andthefirstinfimumisoverallP Wˆ|S satisfying
E
PS,WP Wˆ|S
genpS,Wq´genpS,Wˆq ďϵ.
A proo“f of this th‰eorem follows by an easy combination of the distortion criterion with the bound on
E
S,Wˆ
genpS,Wˆq obtained by application of Theorem 1 to the compressed model Wˆ (not W). This
simpletrick,whichisrelatedconceptuallytolossysourcecoding,preventstheKL-divergencetermfrom
takingverylarge(infinite)valuesforcontinuousalphabetvariables.Moreover,thelossycompressibility
alsooffersaninterpretationofthegeometricalcompressibilityconceptthatwasobservedtoberelatedto
thegeneralizationperformancein[GeigerandKoch,2019].
2.3 Generalizationboundsusingtype-IIsymmetricpriors
Inthissection,weconsideranotherwaytorearrangeindistinguishablypS,S1qasZ2n whichisinspired
by[BlumandLangford,2003]. LetT “ tT 1,...,Tnubearandomsetobtainedbydrawingrandomly,
withoutreplacement,nindicesfromthesett1,...,2nu.NotethatherethecomponentsofTarepossibly
dependentstatistically. LetTc “t1,...,2nuzT“tT 1c,...,T ncubethecomplementsetofT. Also,for
everyiPrnsletpZ Ti,Zc Tiq“pZ i,Z i1q.Weusetype-IIsymmetricpriortocoversuchrearrangedvectors.
Forconvenienceletthefunctionh D: r0,1sˆr0,1sÑr0,2sbedefinedas
´ ¯
x`x1
h Dpx,x1q:“2h
b 2
´h bpxq´h bpx1q, (5)
where h bpxq :“ ´xlog 2pxq´p1´xqlog 2p1´xq. Note that h Dpx,x1q{2 equals the Jensen-Shannon
divergencebetweentwobinaryBernoullidistributionswithparametersxandx1.Thereaderisreferredto
AppendixAforadiscussionoftherelationofthisfunctionwiththecombinatorialtermusedin[Blumand
Langford,2003].Thefunctionh Dp¨,¨qhasthefollowinginterestingproperties,provedinAppendixE.3.
Lemma1. @px,x1qPr0,1sˆr0,1s,wehave:(i) h Dpx,x1qěpx´x1q2,(ii)h Dpx,0qěx,(iii)h Dpx,x1q
isincreasingwithrespecttoxintherangerx1,1s,and(iv)h Dpx,x1qisconvexwithrespecttobothinputs.
Now,westatethemainresultofthissection.
Theorem3. LetQ iibethesetoftype-IIsymmetricpriorsonpYˆ,Yˆ1qgivenpY,Y1q.Then,forně10,
´ “ ‰ “ ‰¯
nh D E W LpWq ,E S,W LˆpS,Wq ď
Qi Pn Qf iiE Y,Y1” D KL´ E X1,X,W” P Yˆb |2 Xn ,WpYˆ,Yˆ1|X,X1,Wqı› › ›Q¯ı `logpnq“I´ T;Yˆ2nˇ ˇ Y2n¯ `logpnq,
whereY,Y1 „µb2nandX1,X,W „P P . Alsothemutualinformationiscalculatedwith
Y X1|Y1 X,W|Y
respecttoth ”ejointdistributionP T,Yˆ2n,Y2nı“P Tµb Y2nP
Yˆ T2n,Yˆ T2n
cˇ ˇ
Y T2nY T2n c
andthelattertermisdefined
asE
X1,X,W
P Yˆb |2 Xn ,WpYˆ T2n,Yˆ T2n c|X,X1,Wq inwhichX1,X,W „P
X1|Y T2n
cP
X,W|Y
T2n.
TheproofofTheorem3isdeferredtoAppendixE.4.Also,asimilartailboundisprovidedinAppendixA.
Usingthepart(i)ofLemma1,itcanbeseenthatifthevalueoftheKLdivergencetermislargerthan
logpnqthentheboundofTheorem3istighterthanthatTheorem1. Also,usingpart(ii)ofLemma1it
isseenthatiftheerroronthetrainingsetiszero(asettingreferredtoas“realizablecase”in[Blumand
Langford,2003]),ourTheorem3yieldsaboundonthegeneralizationerrorwhichisOp1{nq.
83 Generalizationboundsintermsoflatentvariablecomplexity
Thegeneralizationboundsoftheprevioussectionareparticularlyusefulinthefollowingsense: ifthe
learningalgorithmis“simple”enoughtoproducealow“relativeentropy”sequenceoflabelsfortraining
andtestsets, thenthealgorithmgeneralizeswell. However, theyhaveadownsidethattheycannotbe
useddirectlyastheyareintheoptimization,sinceminimizingthoseboundsmayresultinsolutionswith
largeempiricalrisk.Inthissection,weextendtheresultsoftheprevioussectiontosettingsinwhichthe
processingissplitintotwoparts:anencoderpartthatproducesafamilyofrepresentationsthathavethe
propertytogeneralizewell(developedaccordingtotheguidelinesoftheprevioussection)andadecoder
partthatselectsamongthatfamilyonerepresentationthatminimizestheempiricalrisk.Assuchthegoal
oftheencoderistoguaranteeasmallgeneralizationerrorandthatofthedecoderistoguaranteeasmall
empiricalrisk.Thisprocedure,whichissimilartotheInformationBottleneckmethod,aimsatfindinga
goodbalancebetweengeneralizingwelltounseendataandminimizingtheriskofthetrainingdata.
LetW “pWe,W dq,whereWeandW darethehypotheses(ormodels)usedbytheencoderandthedecoder,
respectively,asillustratedinFig.1b.Also,letU denotetheoutputoftheencoder,whichwillbereferredto
hereafterinterchangeablyas“representation”or“latentvariable”.Theencoderproducestherepresentation
U accordingtotheconditionalP .ThedecoderproducesanestimateYˆ ofthetruelabelY according
totheconditionalP Yˆ|U,Wd.WecU o|X ns, iW dee rastochasticlearningalgorithmA: Zn ÑWwhichpicksamodel
W “pWe,W dqPW :“WeˆW daccordingtoP W|S. Letalossfunctionℓ: ZˆW Ñr0,1sbegiven.
Foramodelw“pwe,w dqthequalityofthepredictionofYˆ isevaluatedas
” ı
ℓpz,wq:“E
Yˆ„P
Yˆ|X,WpYˆ|x,wqr1 ty‰Yˆus:“E U„PU|X,WepU|x,weqE
Yˆ„P Yˆ|U,WdpYˆ|U,wdq
1
ty‰Yˆu
.
Empirical risk, population risk, and generalization error are defined similarly to the previous section.
Accordingtotheabovemotivation,weareinterestedinestablishingaboundonthegeneralizationerrorof
thealgorithmAthatdependsonlyonthepartWeofthemodelW.Inaccordancewithourcompressibility
frameworkofSection2suchboundwouldthendependonthecomplexityofthelatentvariableU.However,
here,theencoderpartWeandthedecoderpartW darebothtrainedusingthedatasetS. Thus,theymay
be statistically dependent in general and therefore the “rearrangement” ideas using type-I and type-II
symmetriesdonotworkhere.Toelaborateonthisabitmore,recallthatforexamplefortype-Isymmetry,
torearrangepY,YˆqandpY1,Yˆ1qinanindistinguishablemanner,werandomlyshufflethepositionsof
thepairspY i,Yˆ iqandpY i1,Yˆ i1q. Ifwenowconsiderusingthetype-Isymmetryforthenewsetup, then
shufflingofthepairspY i,U i,Yˆ iqandpY i1,U i1,Yˆ i1q,changesthedatasetSandthusthedistributionsP
W|S
and P . Hence, covering the resulting rearranged sequence would unavoidably depend on both
Yˆ|Wd,U
encoderanddecoderparts. Toovercomethisissue,weusetype-IIIsymmetry,whereweleavethetrain
andghostdatasetsuntouchedandrandomly“swap”onlythoselatentvariablesandtheircorresponding
predictions,i.e.,pU i,Yˆ iqandpU j1,Yˆ j1q,thatareassociatedwiththetrainandghostsampleswiththesame
label,i.e.,ifY
i
“Y j1.Thefollowingresultisthemaintheoremofthissectionandthepaperderivedusing
thetype-IIIsymmetricpriors.AformalproofofthisresultcanbefoundinAppendixE.5.
Theorem4(GeneralizationBoundforRepresentationLearningAlgorithms). ConsideraK-classification
learningta´sk. LetQbeatype-I ˇIIsymmetricconditionalprioro¯verU2ngivenX2n,Y2nandWˆ e PWe,
ˇ
namely,Q pu πp1q,...,u πp2nqqpx 1,...,x 2nq,py 1,...,y 2nq,wˆe remainsthesameforallpermutations
π: r2nsÞÑr2nsthatpreservesthelabel,i.e.,y
πpiq
“y iforiPr2ns.Then,
g
f „ ˆ › ˙ȷ
f ›
E
S,WrgenpS,Wqsďinf2f e2E
S,S1,Wˆ e
D
KL
P Ub |2 Xn
,Wˆ
epU n,U1|X,X1,Wˆ eq›Q `K`2
`ϵ,
where S,S1,Wˆ e „ P S,W“ˆ eP S1 and the infimum i ‰s over all Markov kernels P Wˆ e|S such that for Wˆ “
pWˆ e,W dq,E
PS,WP Wˆe|S
genpS,Wq´genpS,Wˆq ďϵ.
TheboundofTheorem4onthegeneralizationerrorofrepresentationlearning,andtherelatedIB-encoder,
is to the best of our knowledge the first of its kind and significance. In fact, while few works have
alreadyinvestigatedthegeneralizationerrorofIB[Shamiretal.,2010,Veraetal.,2018,Kawaguchietal.,
2023], for the special case of discrete variables, their bounds, which in part are expressed in terms of
theempiricalmutualinformationIˆpU;Xq, appeartobevacuousformostsettingsasalreadyobserved
in[RodriguezGalvez,2019,GeigerandKoch,2019,AmjadandGeiger,2019,Duboisetal.,2020](see
9introductionandAppendixB.2formoredetails).Furthermore,asalsoextensivelydiscussedin[Amjadand
Geiger,2019,GeigerandKoch,2019,Duboisetal.,2020],mutualinformationmaynotbeagoodindicator
ofthegeneralizationerror.Suchaspects,includinghowmutualinformationfailstoreflectthe“simplicity”
or“structure”oftheencoder,arediscussedinmoredetailintheprevioussection.Analternateboundonthe
generalizationerrorofrepresentationlearningappearedin[Duboisetal.,2020].However,theirboundonly
holdsforencodersthatfindtheoptimalrepresentationinasensedefinedtherein.
Now,fewremarksontheresultofTheorem4areinorder. First,notethatthegeneralizationboundof
thistheoremdependsonlyontheencoderandmorepreciselyonthecomplexityofthelatentspaceU;
and,so,thisboundisvalidforanychoicedecoder.2 Next,similartotheboundsoftheTheorems1-5,the
KL-divergencetermofTheorem4explicitlytakesintoaccountthe“structure”and“simplicity”ofthe
encoder,and,therefore,itresolvesoneofthemajorissuesoftheIBmethod [AmjadandGeiger,2019,
GeigerandKoch,2019,Duboisetal.,2020].
Moreover,theresultofTheorem4suggeststhattheconsideredpriorcoulddependonthedataandmodel.
Thisenablesalargerclassofchoicesfortheprior.Examplesinclude(i)SymmetricjointlyGaussianpriors,
(ii)priorsthatdependonthecategory(label),i.e.,thepriorforacategoryk PrKsateachoptimization
iterationcouldpossiblydependonsomestatisticsofthelatentvariablesofalltrainingandtestsamples
havinglabelk,and(iii)priorsthatsteerlatentvariablestowardsomepre-defined“constellations”inthe
latentspace,dependingontheirlabel. Onthisaspect,notethatallowingthepriortodependonlabels
enablesaconnectionwiththe“conditionalinformation-bottleneck”of[Fischer,2020].
AnotherimportantpropertyofourboundofTheorem4isthat,unliketheempiricalmutualinformation
termof[Shamiretal.,2010,Veraetal.,2018,Kawaguchietal.,2023],3 itdoesnotbecomeinfinitefor
deterministicencoderswithcontinuousinput-output. Moreover, ourapproachwhichisbasedonlossy
compressionprovidesaninterpretationofthegeometriccompressionof[GeigerandKoch,2019,Goldfeld
et al., 2019] where latent variables are concentrated around some constellation points.4 We hasten to
mentionthat“lossycompression”hereshouldnotbeconfusedwithapproachesthataddnoiseafterthe
encoder.Themaindifferenceisthatinthoseapproachesthenoisyrepresentationsarepassedtothedecoder;
while here, the “noisy” representations are used only to estimate lossy compressibility. These “noisy”
representationscanbeachievedbyeitheraddingsmall“noise”tothemodelparametersorthelatentvariable.
NotethatbyincreasingthenoiseleveltheϵterminTheorem4increaseswhiletheKL-divergenceterm
potentiallydecreases.Inpractice,asuitabletrade-offbetweenthetwoeffectscanbefoundbytreatingthe
amountofaddednoiseasahyper-parametertooptimize.
Finally,asimilartailboundonthegeneralizationerrorhasbeenestablishedinAppendixA.
4 Experiments
Inthissection,weillustrateourresultsviasomeexperiments.Formoredetailandotherexperiments,the
readerisreferredtoAppendixD.
OurmainTheorems4and7suggestthatfortherepresent´ationlearningsetupofFig.1bthegen ›era¯lization
erroriscontrolledessentiallybythedivergencetermD
KL
P Ub |2 Xn ,WepU,U1|X,X1,Y,Y1,Weq› Q where
Qisatype-IIIsymmetricprior.Inasense,thisalsomeansthat,withaproperdata-dependentchoiceofthe
prior,theusageoftheaforementioneddivergencetermasaregularizerpossiblyoffersbettergeneralization
guarantees(relative,e.g.,totheconventionaldata-independentpriorofVIB).Inwhatfollows,wepropose
anewfamilyofdata-dependentpriorsthatappeartobettercapturethe“simplicity”or“structure”ofthe
encoder.WealsocomparetheassociatedaccuracywiththatofferedbythefixedpriorofVIB.
More precisely, in VIB the prior Q factorizes as a product of 2n scalar standard Gaussian priors, i.e.,
Q “ Qb2n,whereQ “ Np0m,Imq,0m P Rm isthezerovectorandIm isthemˆmidentitymatrix.
Inourlosslessapproach,whichweherecoinasLosslessCategory-DeśpendentVIB(CDVIB),theprior
Qstillfactorizesasaproductof2nscalarGaussianpriors,i.e.,Q “ iPr2nsQ i,butwiththreemajor
differences: i. EachQ
i
canbechosenfromasetofM ˆK priors–M priorsforeachlabel. ii. Unlike
VIB,eachofM ˆKpriorscandependonsomestatisticsofpS,S1qandalsoonthelabelofeachsample,
2Insomecases,theboundmaybeloose,however,e.g.,withadecoderw thatproducesanestimateYˆ
d
independentlyoftheobtainedrepresentationU.
3Thelossycompressiontrick,however,canalsobeappliedforresultsof[Shamiretal.,2010,Veraetal.,
2018,Kawaguchietal.,2023].
4ThereaderisreferredtoAppendixC.3forasimpleexampleofgeometriccompression.
10iii. UnlikeVIB,wherethepriorisfixed,hereQis‘learned”duringthetrainingphase. Tothisend,the
meanandvarianceofthescalarpriorsareupdatedaftereachtrainingiterationusingamovingaverage
withsomesmallcoefficient,allowingthelatentspacetobetteradapttothestructureoftheencoderandthe
data.Takingthemovingaveragealsohasanotherrole,whichisto“partially”reproducetheeffectofthe
“ghostdata”(whichcomesfromthetestdatasetthatisusuallyunavailableduringtraining).InlossyCDVIB,
similarpriorsareconsideredbutover“noisy”versionsofthelatentvariables,i.e.,pUˆ,Uˆ1q.Notethatwhile
thenoisyversionsareconsideredfortheregularizer,thedecoderreceivesasinputpU,U1q.
WeconsiderCIFAR10[Krizhevskyetal.,2009]imageclassificationusingasmallCNN-basedencoder
andalineardecoder.TheresultsshowninFig.2indicatethatthemodeltrainedusingourpriorsachieves
better(„2.5%)performanceintermsofbothgeneralizationerrorandpopulationrisk.Thissuggeststhat
ourpriorshelptofindabetterrepresentationthanthestandardVIBprior.
(a)Accuracy. (b)Log-likelihood.
Figure2: Accuracyduringthetestphaseofourtwo-steppredictionmodeltrainedusingthestandard
VIBpriorandour“lossless”CDVIBand“lossy”CDVIBpriorscomputedforM “5. Thevalues
areaveragedover5runs. Thegraphsaredisplayedtogetherwith95%bootstrapconfidenceintervals.
5 Acknowledgement
Theauthorswouldliketothanktheanonymousreviewersfortheirmanyinsightfulcommentsandsugges-
tions.Inparticular,bypointingouttheconnectionbetweenourresultsandthef-CMIliterature.
References
OhadShamir,SivanSabato,andNaftaliTishby.Learningandgeneralizationwiththeinformationbottleneck.
TheoreticalComputerScience,411(29-30):2696–2711,2010.
Alexander A. Alemi, Ian Fischer, Joshua V. Dillon, and Kevin Murphy. Deep variational information
bottleneck.InInternationalConferenceonLearningRepresentations,2017.URLhttps://openreview.
net/forum?id=HyxQzBceg.
AaronVanDenOord,OriolVinyals,etal. Neuraldiscreterepresentationlearning. Advancesinneural
informationprocessingsystems,30,2017.
YannDubois,DouweKiela,DavidJSchwab,andRamakrishnaVedantam.Learningoptimalrepresentations
withthedecodableinformationbottleneck. AdvancesinNeuralInformationProcessingSystems,33:
18674–18690,2020.
NaftaliTishby,FernandoCPereira,andWilliamBialek.Theinformationbottleneckmethod.arXivpreprint
physics/0004057,2000.
InakiEstellaAguerriandAbdellatifZaidi.Distributedvariationalrepresentationlearning.IEEEtransactions
onpatternanalysisandmachineintelligence,43(1):120–138,2019.
ArtemyKolchinsky,BrendanDTracey,andDavidHWolpert. Nonlinearinformationbottleneck. Entropy,
21(12):1181,2019.
IanFischer. Theconditionalentropybottleneck. Entropy,22(9):999,2020.
11BorjaRodr´ıguezGa´lvez,RagnarThobaben,andMikaelSkoglund. Theconvexinformationbottleneck
lagrangian. Entropy,22(1):98,2020.
MichaelKleinman,AlessandroAchille,StefanoSoatto,andJonathanKao. Gacs-kornercommoninforma-
tionvariationalautoencoder. arXivpreprintarXiv:2205.12239,2022.
RavidShwartz-ZivandNaftaliTishby. Openingtheblackboxofdeepneuralnetworksviainformation.
arXivpreprintarXiv:1703.00810,2017.
AbdellatifZaidi, In˜akiEstella-Aguerri, andShlomoShamai. Ontheinformationbottleneckproblems:
Models,connections,applicationsandinformationtheoreticviews. Entropy,22(2):151,2020.
ZivGoldfeldandYuryPolyanskiy. Theinformationbottleneckproblemanditsapplicationsinmachine
learning. IEEEJournalonSelectedAreasinInformationTheory,1(1):19–38,2020.
BernhardCGeiger.Oninformationplaneanalysesofneuralnetworkclassifiers–areview.IEEETransactions
onNeuralNetworksandLearningSystems,2021.
Thomas M. Cover and Joy A. Thomas. Elements of information theory (2. ed.). Wiley, 2006. ISBN
978-0-471-24195-9.
JormaRissanen. Modelingbyshortestdatadescription. Automatica,14(5):465–471,1978.
PeterDGru¨nwald,InJaeMyung,andMarkAPitt. Advancesinminimumdescriptionlength:Theoryand
applications. MITpress,2005.
Mat´ıVera,PabloPiantanida,andLeonardoReyVega. Theroleoftheinformationbottleneckinrepresenta-
tionlearning. In2018IEEEInternationalSymposiumonInformationTheory(ISIT),pages1580–1584,
2018. doi:10.1109/ISIT.2018.8437679.
AnselmBlumer,AndrzejEhrenfeucht,DavidHaussler,andManfredKWarmuth. Occam’srazor. Informa-
tionprocessingletters,24(6):377–380,1987.
Le´onardBlierandYannOllivier. Thedescriptionlengthofdeeplearningmodels. AdvancesinNeural
InformationProcessingSystems,31,2018.
PeterGru¨nwaldandTeemuRoos. Minimumdescriptionlengthrevisited. Internationaljournalofmathe-
maticsforindustry,11(01):1930001,2019.
AvrimBlumandJohnLangford. Pac-mdlbounds. InLearningTheoryandKernelMachines:16thAnnual
ConferenceonLearningTheoryand7thKernelWorkshop,COLT/Kernel2003,Washington,DC,USA,
August24-27,2003.Proceedings,pages344–357.Springer,2003.
Bernhard C Geiger and Tobias Koch. On the information dimension of stochastic processes. IEEE
transactionsoninformationtheory,65(10):6496–6518,2019.
Artemy Kolchinsky, Brendan D Tracey, and Steven Van Kuyk. Caveats for information bottleneck in
deterministicscenarios. arXivpreprintarXiv:1808.07593,2018.
Borja Rodriguez Galvez. The information bottleneck: Connections to other problems, learning and
explorationoftheibcurve,2019.
RanaAliAmjadandBernhardCGeiger. Learningrepresentationsforneuralnetwork-basedclassifica-
tionusingtheinformationbottleneckprinciple. IEEEtransactionsonpatternanalysisandmachine
intelligence,42(9):2225–2239,2019.
KenjiKawaguchi,ZhunDeng,XuJi,andJiaoyangHuang. Howdoesinformationbottleneckhelpdeep
learning? InAndreasKrause,EmmaBrunskill,KyunghyunCho,BarbaraEngelhardt,SivanSabato,
andJonathanScarlett,editors,Proceedingsofthe40thInternationalConferenceonMachineLearning,
volume202ofProceedingsofMachineLearningResearch,pages16049–16096.PMLR,23–29Jul2023.
YilinLyu,XinLiu,MingyangSong,XinyueWang,YaxinPeng,TieyongZeng,andLipingJing. Recogniz-
ableinformationbottleneck. arXivpreprintarXiv:2304.14618,2023.
NickLittlestoneandManfredWarmuth. Relatingdatacompressionandlearnability. Citeseer,1986.
12SanjeevArora,RongGe,BehnamNeyshabur,andYiZhang. Strongergeneralizationboundsfordeepnets
viaacompressionapproach. InInternationalConferenceonMachineLearning,pages254–263.PMLR,
2018.
Taiji Suzuki, Hiroshi Abe, Tomoya Murata, Shingo Horiuchi, Kotaro Ito, Tokuma Wachi, So Hirai,
MasatoshiYukishima,andTomoakiNishimura. Spectralpruning:Compressingdeepneuralnetworksvia
spectralanalysisanditsgeneralizationerror. InInternationalJointConferenceonArtificialIntelligence,
pages2839–2846,2020.
DanielHsu,ZiweiJi,MatusTelgarsky,andLanWang. Generalizationboundsviadistillation. InInterna-
tionalConferenceonLearningRepresentations,2021.
MelihBarsbey,MiladSefidgaran,MuratAErdogdu,Gae¨lRichard,andUmutS¸ims¸ekli. Heavytailsin
SGDandcompressibilityofoverparametrizedneuralnetworks. InThirty-FifthConferenceonNeural
InformationProcessingSystems,2021.
MiladSefidgaran,AminGohari,GaelRichard,andUmutSimsekli. Rate-distortiontheoreticgeneralization
boundsforstochasticlearningalgorithms. InConferenceonLearningTheory,pages4416–4463.PMLR,
2022.
MiladSefidgaranandAbdellatifZaidi. Data-dependentgeneralizationboundsviavariable-sizecompress-
ibility. arXivpreprintarXiv:2303.05369,2023.
DanielRussoandJamesZou. Controllingbiasinadaptivedataanalysisusinginformationtheory. InArthur
GrettonandChristianC.Robert,editors,Proceedingsofthe19thInternationalConferenceonArtificial
IntelligenceandStatistics,volume51ofProceedingsofMachineLearningResearch,pages1232–1240,
Cadiz,Spain,09–11May2016.PMLR.
AolinXuandMaximRaginsky. Information-theoreticanalysisofgeneralizationcapabilityoflearning
algorithms. AdvancesinNeuralInformationProcessingSystems,30,2017.
ThomasSteinkeandLydiaZakynthinou.Reasoningaboutgeneralizationviaconditionalmutualinformation.
InJacobAbernethyandShivaniAgarwal,editors,ProceedingsofThirtyThirdConferenceonLearning
Theory,volume125ofProceedingsofMachineLearningResearch,pages3437–3452.PMLR,09–12Jul
2020.
DavidAMcAllester. Somepac-bayesiantheorems. InProceedingsoftheeleventhannualconferenceon
Computationallearningtheory,pages230–234,1998.
UmutS¸ims¸ekli,OzanSener,GeorgeDeligiannidis,andMuratAErdogdu. Hausdorffdimension,heavy
tails,andgeneralizationinneuralnetworks. InH.Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,
andH.Lin,editors,AdvancesinNeuralInformationProcessingSystems,volume33,pages5138–5151.
CurranAssociates,Inc.,2020.
SteveHannekeandAryehKontorovich.Asharplowerboundforagnosticlearningwithsamplecompression
schemes. InAlgorithmicLearningTheory,pages489–505.PMLR,2019.
SteveHanneke,AryehKontorovich,andMenachemSadigurschi. Samplecompressionforreal-valued
learners. InAlgorithmicLearningTheory,pages466–488.PMLR,2019.
OlivierBousquet,SteveHanneke,ShayMoran,andNikitaZhivotovskiy. Properlearning,hellynumber,
andanoptimalsvmbound. InConferenceonLearningTheory,pages582–609.PMLR,2020.
SteveHannekeandAryehKontorovich. Stablesamplecompressionschemes: Newapplicationsandan
optimalsvmmarginbound. InAlgorithmicLearningTheory,pages697–721.PMLR,2021.
SteveHanneke,AryehKontorovich,SivanSabato,andRoiWeiss. Universalbayesconsistencyinmetric
spaces. In2020InformationTheoryandApplicationsWorkshop(ITA),pages1–33.IEEE,2020.
DanTsirCohenandAryehKontorovich. Learningwithmetriclosses. InConferenceonLearningTheory,
pages662–700.PMLR,2022.
HrayrHarutyunyan,MaximRaginsky,GregVerSteeg,andAramGalstyan. Information-theoreticgeneral-
izationboundsforblack-boxlearningalgorithms. AdvancesinNeuralInformationProcessingSystems,
34,2021.
13FredrikHellstro¨mandGiuseppeDurisi. Anewfamilyofgeneralizationboundsusingsamplewiseevaluated
cmi. AdvancesinNeuralInformationProcessingSystems,35:10108–10121,2022.
ImreCsisza´randJa´nosKo¨rner. InformationTheory:CodingTheoremsforDiscreteMemorylessSystems.
CambridgeUniversityPress,2edition,2011.
Jean-YvesAudibert. PAC-Bayesianstatisticallearningtheory. PhDthesis,Universite´ParisVI,,2004.
PeterGrunwald,ThomasSteinke,andLydiaZakynthinou. Pac-bayes,mac-bayesandconditionalmutual
information:Fastrateboundsthathandlegeneralvcclasses. InConferenceonLearningTheory,pages
2217–2247.PMLR,2021.
OlivierCatoni. Pac-bayesiansupervisedclassification. LectureNotes-MonographSeries.IMS,1277,2007.
YuhengBu,ShaofengZou,andVenugopalV.Veeravalli. Tighteningmutualinformation-basedboundson
generalizationerror. IEEEJournalonSelectedAreasinInformationTheory,1(1):121–130,May2020.
ISSN2641-8770.
BorjaRodr´ıguez-Ga´lvez, Germa´nBassi, RagnarThobaben, andMikaelSkoglund. Onrandomsubset
generalizationerrorboundsandthestochasticgradientlangevindynamicsalgorithm. In2020IEEE
InformationTheoryWorkshop(ITW),pages1–5.IEEE,2021.
NorbertSauer.Onthedensityoffamiliesofsets.JournalofCombinatorialTheory,SeriesA,13(1):145–147,
1972. ISSN0097-3165.
SaharonShelah.Acombinatorialproblem;stabilityandorderformodelsandtheoriesininfinitarylanguages.
PacificJournalofMathematics,41(1):247–261,1972.
ZivGoldfeld,EwoutVanDenBerg,KristjanGreenewald,IgorMelnyk,NamNguyen,BrianKingsbury,
andYuryPolyanskiy. Estimatinginformationflowindeepneuralnetworks. InKamalikaChaudhuriand
RuslanSalakhutdinov,editors,Proceedingsofthe36thInternationalConferenceonMachineLearning,
volume97ofProceedingsofMachineLearningResearch,pages2299–2308.PMLR,09–15Jun2019.
AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages. Toronto,
ON,Canada,2009.
BehnamNeyshabur,SrinadhBhojanapalli,andNathanSrebro. Apac-bayesianapproachtospectrally-
normalizedmarginboundsforneuralnetworks,2018.
DorTsur,BasharHuleihel,andHaimPermuter.Ratedistortionviaconstrainedestimatedmutualinformation
minimization. In2023IEEEInternationalSymposiumonInformationTheory(ISIT),pages695–700,
2023. doi:10.1109/ISIT54713.2023.10206867.
MohamedIshmaelBelghazi,AristideBaratin,SaiRajeswar,SherjilOzair,YoshuaBengio,AaronCourville,
andRDevonHjelm. Mine: mutualinformationneuralestimation. arXivpreprintarXiv:1801.04062,
2018.
CheukTingLiandAbbasElGamal. Strongfunctionalrepresentationlemmaandapplicationstocoding
theorems. IEEETransactionsonInformationTheory,64(11):6967–6978,2018.
NaderHBshoutyandHannaMazzawi. Exactlearningcomposedclasseswithasmallnumberofmistakes.
InLearningTheory: 19thAnnualConferenceonLearningTheory,COLT2006,Pittsburgh,PA,USA,
June22-25,2006.Proceedings19,pages199–213.Springer,2006.
DiederikPKingmaandMaxWelling. Auto-encodingvariationalbayes. ICLR,2014.
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,TrevorKilleen,
ZemingLin,NataliaGimelshein,LucaAntiga,etal. Pytorch:Animperativestyle,high-performance
deeplearninglibrary. Advancesinneuralinformationprocessingsystems,32,2019.
XavierGlorotandYoshuaBengio.Understandingthedifficultyoftrainingdeepfeedforwardneuralnetworks.
InProceedingsofthethirteenthinternationalconferenceonartificialintelligenceandstatistics,pages
249–256.JMLRWorkshopandConferenceProceedings,2010.
14DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. In3rdInternational
ConferenceonLearningRepresentations,ICLR2015,SanDiego,CA,USA,May7-9,2015,Conference
TrackProceedings,2015.
RobertGGallager. Informationtheoryandreliablecommunication,volume588. Springer,1968.
LucDevroye. Theequivalenceofweak,strongandcompleteconvergenceinl1forkerneldensityestimates.
TheAnnalsofStatistics,pages896–904,1983.
MartinJ.Wainwright. High-DimensionalStatistics:ANon-AsymptoticViewpoint. CambridgeSeriesinSta-
tisticalandProbabilisticMathematics.CambridgeUniversityPress,2019. doi:10.1017/9781108627771.
15Appendices
Theappendicesareorganizedasfollows.
• AppendixAcontainsfurthertheoreticalresults.Specifically,
– inAppendixA.1twomoretailboundsonthegeneralizationerrorintermsofthepredicted
labelcomplexityarepresented.
– in Appendix A.2 a tail bound on the generalization error in terms of the latent variable
complexityispresented.
• WediscusssomeoftherelatedworksinmoredetailinAppendixB.Inparticular,
– inAppendixB.1,werecallthecompressibilityframeworkofBlumandLangford[Blumand
Langford,2003].
– inAppendixB.2,wediscusstheresultsof[Kawaguchietal.,2023].
• InAppendixC,wediscussvarioustopicsthatcouldnotbesufficientlyaddressedinthepaper,due
tolackofspace.Moreprecisely,
– wedefinevariousnotionsofcompressibility, includinglossless, lossy, andvariable-size
compressibilityinAppendixC.1,
– wepresentanintuitiononthefunctionh Dp¨,¨q,usedinSection2,inAppendixC.2,
– wegiveasimpleexampleofgeometricalcompression,anditsrelationtolossycompressibil-
ity,inAppendixC.3,
– andfinallywepresenttheconclusionofourworktogetherwithsomeinterestingfuture
directionsinAppendixC.4.
• AppendixDcontainsthedetailsoftheexperimentspresentedinSection4.
– AppendixD.1recallsthestandardVIBobjectivefunction,
– AppendixD.2definesafamilyoflosslessobjectivefunctionsusingdata-dependentpriors,
– AppendixD.3definesafamilyoflossyobjectivefunctionsusingdata-dependentpriors,
– AppendixD.4detailsthetrainingandtestingdatasetsusedinexperiments,
– AppendixD.5detailsthemodelarchitectureusedinexperiments,
– AppendixD.6providestrainingdetails,
– AppendixD.7presentsanddiscussesthenumericalresults.
• AppendixEcontainsthedeferredproofsofallourtheoreticalresults.Moreprecisely,
– theintuitionbehindourprooftechniquesispresentedinAppendixE.1,
– AppendixE.2containstheproofofTheorem1,
– AppendixE.3containstheproofofLemma1,
– AppendixE.4containstheproofofTheorem3,
– AppendixE.5containstheproofofTheorem4,
– AppendixE.6containstheproofofTheorem5,
– AppendixE.7containstheproofofTheorem6,
– AppendixE.8containstheproofofTheorem7,
– AppendixE.9containstheproofofLemma3.
A Additionaltheoreticalresults
Inthissection,wepresentseveraltailboundsonthegeneralizationerrorintermsofthecomplexityofthe
predictedlabelsorthelatentvariables.
A.1 Tailboundsonthegeneralizationerrorintermsofpredictedlabelcomplexity
First,westartbystatingalossytailboundonthegeneralizationerrorintermsofthecomplexityofthetail
bound.
16Theorem5. Considerthelearningframeworkdefinedabove.LetQbeanyfixedtype-Isymmetricprior
onpYˆ,Yˆ1qthatcoulddependonpX,Y,X1,Y1q.ThenforanyϵPRandδPR`withprobabilityatleast
p1´δqoverchoiceofSandS1,wehavethatE W„PW|SrgenpS,Wqsisupperboundedby
c d ˆ „ ˆ › ˙ȷ ˙
log 2p2 n{δq `inf 2n4
´1
E
Wˆ„P Wˆ|S
D
KL
P Yˆb |2 Xn ,WˆpYˆ,Yˆ1|X,X1,Wˆq› › ›Q `logp? 8n{δq `ϵ,
wheretheinfimumisoverallP thatsatisfy
Wˆ|S
”ˇ´ ¯ ´ ¯ˇı
ˇ ˇ
E PW|SP Wˆ|S ˇ LˆpS1,Wq´LˆpS,Wq ´ LˆpS1,Wˆq´LˆpS,Wˆq ˇ ďϵ{2. (6)
ThetheoremisprovedinAppendixE.6.Intheaboveresult,itiseasytoreplacetheexpectationwithrespect
toP withanyarbitraryexpectations,asitiscommoninPAC-Bayesboundsandusedforexamplein
W|S
[Neyshaburetal.,2018].
Next,westateatailboundintermsofthefunctionh Dp¨,¨q,definedin(5).
Theorem6. ConsiderthelearningframeworkofSection2.LetQbeanyfixedtype-IIsymmetricprioron
pYˆ,Yˆ1qthatcoulddependonpX,Y,X1,Y1q.Then,foranyδPR`,withprobabilityatleastp1´δqover
choicesofpS,S1,Wq„P S1P S,W,itholdsthat
´ ¯ ˆ › › ˙
nh
D
LˆpS1,Wq,LˆpS,Wq ďD
KL
P Yˆb |2 Xn ,WpYˆ,Yˆ1|X,X1,Wq› ›Q `logpn{δq.
ThistheoremisprovedinAppendixE.7.Inparticular,whenLˆpS,Wq“0,usingLemma1concludethat,
thisresultyieldsthatwithprobabilityatleastp1´δq,
ˆ › ˙
›
LˆpS1,WqďD
KL
P Yˆb |2 Xn ,WpYˆ,Yˆ1|X,X1,Wq› ›Q `logpn{δq
.
n
A.2 Tailboundsonthegeneralizationerrorintermsoflatentvariablecomplexity
Inthissection,wepresentatailgeneralizationboundforthetwo-steplearningsetupofSection3that
highlightstherelevanceoflatentvariablecompressibilitytogeneralizationerror.
Theorem 7. Consider a K-classification learning task with the above defined framework. Let
Q´be a type-III symm ˇetric conditional prior over U¯2n given X2n,Y2n and We P We, namely,
ˇ
Q pu πp1q,...,u πp2nqqpx 1,...,x 2nq,py 1,...,y 2nq,w remainsthesameforallpermutationsπ: r2nsÞÑ
r2nsthatpreservesthelabel,i.e.,y
πpiq
“y iforiPrns.Then,foranyλPR`,
i. withprobabilityatleastp1´δqoverchoicesofpS,S1,Wq„P S1P S,W,
´ › ¯
›
LˆpS1,Wq´LˆpS,Wqď
D
KL
P Ub |2 Xn ,WepU,U1|X,X1,Weq›Q `pK`2q{2`logp1{δq
`
2λ
,
λ n
ii. withprobabilityatleastp1´δqoverchoicesofpS,S1,Wq„P S1P S,W,
´ › ¯
› c
genpS,WqďD
KL
P Ub |2 Xn ,WepU,U1|X,X1,Weq›Q `pK`2q{2`logp2{δq
`
2λ
`
logp2{δq
.
λ n n
NotethatthesecondpartofthetheoremcanbeeasilyderivedfromthefirstpartusingHoeffding’sinequality.
WeprovidetheproofofthefirstpartinAppendixE.8.
B Relatedworks
Inthissection,wediscusssomeoftherelatedworksinmoredetail.
17B.1 PAC-MDLframeworkofBlumandLangford
Here,werecallthePAC-MDLframeworkof[BlumandLangford,2003]which,therein,wasintroducedin
theformofa(compression)gamebetweentwoagents,AliceandBob.Alicehasaccesstobothalabeled
training set S “ pXn,Ynq, consisting ofn labeled samples, and a testset S1 “ X1n, consistingof n
unlabeledsamples,alldrawnindependentlyfromµ.5 Bobhasavailablejustthetestsetandtheunlabeled
versionofthetrainingset,i.e.,pXn,X1nq,orderedinsomepredefined(e.g.,lexicographic)orderknownto
bothagents.HereafterwedenotetheorderedvectoroflabeledandunlabeledsamplesasπpXn,X1nq.It
isassumedthatbyobservingthevectorπpXn,X1nqBobcannotknowwhichsamplesofitarefromthe
trainingsetandwhicharefromthetestset. ThegoalofAliceistocommunicatethelabelsπpYn,Y1nq
toBobusingasfewbitsaspossible. 6 Tothisend,letE: ZnˆXn Ñ t0,1u˚ beamapping(encoder)
usedbyAliceandσ:“EpS,X1nqthestringtransmittedtoBob.ThegoalofBobistoguessthelabelsof
bothsetsXnandX1n;anddoessobyrunningadecodermappingD: X2nˆt0,1u˚ ÑY2n.Thatis,Bob
formsanestimateořfthetruelabelsasDpπpXn,X1nq,σq.Inthiscontext,theempiricalrřiskismeasured
asLˆpσ,s,s1q:“ n1 iPrns1 tyi‰yˆiuandaproxytestriskismeasuredasLpσ,s,s1q:“ n1 iPrns1
ty i1‰yˆ
i1u.
Essentially,thePAC-MDLboundof[BlumandLangford,2003]canbeseenasageneralizedversionof
Occam’s-Razortheorem[LittlestoneandWarmuth,1986,Blumeretal.,1987]whichstatesthatifonecan
explain(orencode)thelabelsofasetofntrainingsamplesbyahypothesisthatcanbedescribedusing
only|σ|!nbitsthenthisguaranteesthatthishypothesisgeneralizeswellforunseensamples.
Theorem8([BlumandLangford,2003,Theorem6]). ForanypriordistributionQpσqofσandanyδą0,
withprobabilityatleast1´δoverthechoicesofS,S1 „P SP S1,wehave:
´ ¯
@σ: nLpσ,S,S1qďbmax n,Lˆpσ,S,S1q,Qpσqδ , (7)
where
´ ¯
a
bmax n, ,δ :“maxtb: Bucketpn,a,bqěδu, (8)
n
and
` ˘` ˘
ÿ n n
Bucketpn,a,bq:“ c `a`b˘´c . (9)
2n
cPrb,a`bs a`b
Theresultismademoreexplicitfortheso-calledrealizablecase,i.e.,whenLˆpσ,S,S1q“0.Forthiscase,
wehave[BlumandLangford,2003,Corollary3]
´ L ¯
P @σ: Lˆpσ,S,S1qą0orLpσ,S,S1qďp|σ|`log p1{δqq n ą1´δ,
2
where|σ|denotesthesizeinbitsofthestringσusingsomeapriorifixedcodewords. Theresultclearly
showsthatifthestringσcanbesentusingfewbitsthenthealgorithmgeneralizeswell.Also,theapproach
canbeusedinordertoestablishsimilarresultsfortheVC-dimensionandPAC-Bayesbounds.
Weemphasizethatourconstructionisinsharpcontrastwithanadaptationoftheabove-describedapproach
ofBlumandLangford[BlumandLangford,2003]inwhichonewouldrevealbothlabeledandunlabeled
samplespSm,X1mnqtoAliceandonlyunlabeledsamplespXmn,X1mnqtoBob(bothinapredefined
order).We,however,inourproposedapproachinSection2.1,revealthevectorZ2mn,containingpSm,S1mq
inapredefinedordertoboth.Furthermore,weemphasizethatthegoalinourapproachisnottorecoverthe
labelsY2mn,butthepredictionsYˆ2mnasproducedbythepickedmodelsorhypothesesWm.
Onanothernote,weremarkthattheinterpretationofR,whichappearedinthesizeofthecodebookin
ourapproach,i.e.,|Yˆ m|ďemR,correspondstotheaveragenumberofbits(perdatasetS)thatisneeded
tosendacompressedversionofthestringσof[BlumandLangford,2003]. Assuch,forthefixed-size
codebookexplainedinSection2.1andwhen|σ|isconstant,Rď|σ|.Similarrelationsholdingeneral,by
consideringthevariable-sizecodebook.
5Forsimplicity,weassumeherethatthetrainingandtestsetsSandS1haveidenticalsizes,i.e.,|S|“|S1|;
butalltheresultsthatwillfollowextendeasilytothecaseof|S|‰|S1|.
6NotethatAlicedoesnotknowthetruelabelsY1nofthetestsamplesX1nandcanonlyestimatethemas
Yˆ1n.
18B.2 Onthegeneralizationboundsof[Kawaguchietal.,2023]
Aftertheinitialsubmissionofourwork,anotherwork[Kawaguchietal.,2023]appearedonarXiv,accepted
atICML2023,thatprovidesanupperboundonthegeneralizationerrorofrepresentationlearningalgorithms.
Asclaimed,thisresultjustifiesthebenefitsoftheinformationbottleneckprinciple,byrelatingIBtothe
generalizationerror.Theresultsareprovidedforthemulti-layerneuralnetworksandfordifferentchoices
oflatentvariablescorrespondingtotheoutputofdifferentlayers.Here,toadapttheresultstothesetupof
thispaper,weonlyconsidertheoutputoftheencoderlayerasthelatentvariableandadaptcorrespondingly
thenotationsoftheresultsin[Kawaguchietal.,2023]toournotations.
Withtheadaptednotations,[Kawaguchietal.,2023]claimstoboundthegeneralizationerrorofarepresen-
tationlearningalgorithm“roughlyby
˜c ¸
IpX;U|Yq`1
O˜ .”
n
However,firstly,sincethisboundisintermsofthemutualinformationfunction,thecriticsontherelation
ofmutualinformationandgeneralizationerror,discussedin[Kolchinskyetal.,2018,RodriguezGalvez,
2019,AmjadandGeiger,2019,Duboisetal.,2020]andprovidedalsointhe“CriticstoIB”sectionofthe
introduction(Section1),arevalidfortheresultsof[Kawaguchietal.,2023],aswell.Moreimportantly,
wecouldnotconcludethereportedorder-wisebehaviorfrom[Kawaguchietal.,2023,Theorem2].Using
thenotationsofourwork,theirresultstatesthatwithprobabilityatleast1´δovertrainingdataS,the
generalizationerrorisboundedby
d
G 3 pIpX;U|Yq`IpWe;Sqqlnp2q`Gˆ 2 ` ?G 1,
n n
wheretheconstantsG ,Gˆ ,andG aredefinedin[Kawaguchietal.,2023,AppendixE.1]andclaimed
1 2 3
thatGˆ
2
„O˜p1qandG
3
„O˜p1q,asnÑ8.However,wewereunabletoresolvethefollowingconcerns
regardingthisbound.
i. Firstly,itisnotclearhowIpWe;SqisconsideredtobehaveasO˜p1q,whennÑ8.Indeed,the
sizeofdatasetSandthelearningalgorithmP changeasnÑ8.
We|S
ii. Secondly,byreferringtothedefinitionsoftheconstantsin[Kawaguchietal.,2023,AppendixE.1],
itcanbeeasilyverifiedthatGˆ
2
“C 1`pHpU|X,Yq`HpWe|Sqqlnp2q,forsomenon-negative
constantC .Hence,theboundcanbere-writtenas
1
c
G 3 pHpU|Yq`HpWeqqlnp2q`C 1 ` ?G 1.
n n
Thus,theboundisintermsofHpU|Yq`HpWeq,andnotIpX;U|Yq`IpWe;Sq.
iii. Lastly,thetermG 3,definedin[Kawaguchietal.,2023,AppendixE.1],iscomposedofT
Y
PN
elements. By[Kawaguchietal.,2023,Lemma2],T
Y
behavesroughlyasO˜p2HpU|Yqq. Using
[Kawaguchi et al., 2023, Lemma 2], it is shown that G is bounded. However, it seems that
3
thisisonlyshownbyassumingthattheT termsofG satisfycertainconditionsrelatedtothe
Y 3
decreasingrateoftheirorderedvalues.Thisassumptionhowever,maynotholdingeneral,and
henceG 3maybehaveasO˜pTyq«O˜p2HpU|Yqq,whichthenbecomesthedominantterminthe
bound.
C Furtherclarificationsanddiscussions
Inthissection, weexplainourcompressibilityframeworkinmoredetail. Moreover, wepresentsome
intuitionsonthefunctionh Dp¨,¨qandgiveanexampleofhowlossycompressionisrelatedtogeometrical
compression.Finally,wediscusssomepotentialfutureworks.
C.1 Compressibilityframework
Inthissection,weproposevariousnotionsofthecompressibilityusedinSection2.
19C.1.1 Losslessfixed-sizecompressibility
We start by recalling the needed elements for joint compression of a block of the predicted labels.
Consider m i.i.d. pairs of train and test datasets S j :“ pZ j,1,...,Z j,nq and S j1 :“ pZ j1 ,1,...,Z j1 ,nq,
whereZ j,i “ pX j,i,Y j,iqandZ j1 ,i “ pX j1 ,i,Y j1 ,iq. LetSm “ pS 1,...,Smq, S1m “ pS 11,...,S m1 qand
Wm :“ pW 1,...,Wmq,whereW j „ P Wj|Sj. ItisimportanttonotethatthemodelorhypothesisW j
ischosenbasedonthedatasetS onlyandtheintroductionofthe(ghost)datasetS1 ishereonlyforthe
j j
sake of the analysis, similarly as it was done in the derivation of the PAC-MDL bound of [Blum and
Langford,2003]orinRademachersamplecomplexityortheconditionalmutualinformationof[Steinke
andZakynthinou,2020].DenotethepredictedlabelsusingmodelW forinputsX andX1 asYˆ and
i i,j j,i j,i
Yˆ1 ,forj PrmsandiPrns.
j,i
Moreover,letZ2mn P Z2mn denotearearrangementoftheelementsofpSm,S1mqsuchthatwhilethe
entiresetsSmandS1mareknownbyhavingthevectorZ2mn,buttheyarearrangedinanorderthatone
cannot distinguish whether a given sample z is from Sm or S1m. Accordingly, denote the rearranged
versionsofpYmn,Y1mnqandpYˆmn,Yˆ1mnqrespectivelybyY2mnandYˆ2mn.InSections2.2and2.3,we
presentedtwomethodsforsuchrearrangement.Asmentionedbefore,inbothmethods,therearrangement
ofpSm,S1mqasZ2mn “ tZ j,iu
jPrms,iPrns
isdoneintwosteps: First, foreachj P rms, werearrange
indistinguishably pS j,S j1q as Zqj2n, and then we concatenated the m resulting tZ2 jnu
jPrms
to achieve
Z2mn.Now,weexplainhowtorearrangeindistinguishablyagivenpS,S1qasZ2nusingtwomethods:
• Section2.2: LetJ “ pJ 1,...,Jnqbeavectorofni.i.d. Bernoullip1 2qrandomvariablesJ i P
ti,i`nu,iPrns.DenotebyJ“pJ 1c,...,J ncqthevectorofthecomplementarychoicesinJ,i.e.,
J iYJ ic “ti,i`nu.Now,foreachiPrns,letpZ Ji,Z
J
icqtobeequaltopZ i,Z i1q.
• Section2.3:LetT“tT 1,...,Tnu,bearandomsetobtainedbypickinguniformlynindicesfrom
t1,...,2nu,withoutreplacement.NotethatincontrasttoJwhichhadi.i.d.components,herethe
componentsofTaredependent. letTc “t1,...,2nuzTbethecomplementofT,havingthe
elementsTc “tT 1c,...,T ncu.Now,foreachiPrns,letpZ Ti,Zc Tiq“pZ i,Z i1q.
Basedontheabove,hereafterwestudythecompressibilityofthesortedmodel-predictedlabelsYˆ2mn,
fromaninformation-theoreticpointofview. Therationaleisthat,inaccordancewith“Occam’sRazor”
theorem [Littlestoneand Warmuth,1986, Blumer et al.,1987], since the (rearranged) predicted-labels
vectorYˆ2mnagreesmostlywiththetruelabelsY2mnonthedatasetSm,ifYˆ2mncanbedescribedusing
onlyafewbits,thisguaranteesthatthemodelW generalizeswell. Asitwillbeshownfromtheresult
thatwillfollow,insteadofthesizeofthemessageneededtobesentintheBlum-Langfordapproach(see
AppendixB.1),anewquantityemergesinourworkasameasureofthecompressibilityofYˆ2mn: the
relativeentropyofthejointconditionalPpYˆn,Yˆ1n|Yn,Y1nqanda(symmetric)conditionalpriorQover
Yˆ2ngivenY2n.DependingonthewaywerearrangepSm,S1mq,thetype-Iortype-IIsymmetricpriorsare
neededtobeapplied.
Let R P R`. In our block-coding rate-distortion theoretic framework, R is said to be achievable if
thereexistsacompressioncodebookofsize« emR,fixedapriori,whichcoversthespacespannedby
themodel-predictedlabelsYˆ2mn withhighprobability. Specifically,ifthereexistsasequenceoflabel
books tYˆ mumPN, where Yˆ m :“ tyˆrrs,r P rlmsu Ď Y2mn, lm P N, yˆrrs “ pyˆ 1rrs,...,yˆmrrsq and
yˆ jrrs“pyˆ j,1rrs,...,yˆ j,2nrrsqPY2nsuchthat:
(i) lm ďemR,
(ii) thereexistasequencetδmumPN forwhichlimmÑ8δm “ 0andwithprobabilityatleastp1´δmq
overthechoicesofSm,S1m,andJorT,onecanfindatleastoneindexrPrlmswhoseassociated
yˆrrsexactlyequalsYˆ2mn.
The above framework, explained in Section 2.1, is called fixed-size compressibility, as the size of the
codebook does not depend on a given dataset. This type of compressibility is useful for establishing
“data-independent”bounds,7e.g.,boundsontheexpectationofthegeneralizationerror.Thisisalsocalled
lossless,sincewelookforacodewordthatisexactlyequaltoYˆ2mn.
7Here,weemphasizethatbydata-dependentboundswerefertoboundsthatdependontheparticularsample
oftheinputdataathand,ratherthanforexamplejustonthedistributionofthedatawhichisunknown.
20C.1.2 Lossyfixed-sizecompressibility
As already stated, the above approach and results can be extended to any bounded loss function and
continuousvariablesY andYˆ (seeforexampleSection3wherecontinuouslatentvariablesarestudied).
Inthiscase,astandardresultofrate-distortiontheorystatesthattocoverlosslesslyandreliablyYˆ2mn,R
shouldbeinfinity,whichmakestheframeworkandresultingboundsvacuous.However,theapproachcan
beeasilyextendedtoincludethelossycompressionofthelabels;inthesamespiritasdonein[Sefidgaran
etal.,2022]forthehypothesiscompression.Moreprecisely,foragivendistortionthresholdϵPR,onecan
considerthesamewayofcodebookgeneration,butwiththefollowingconditions:
(i) lm ďemR,
(ii) thereexistasequencetδmumPN forwhichlimmÑ8δm “ 0andwithprobabilityatleastp1´δmq
overthechoicesofSm,S1m,andJorT,onecanfindatleastoneindexrPrlmswhoseassociated
yˆrrssatisfies:
ÿ ˆˆ ˙ ´ ¯˙
1
mn
jPrms,iPrns
1 tYj,tc i‰Yˆ j,tc iu´1 tYj,ti‰Yˆ j,tiu ´ 1 tYj,tc i‰yˆ j,tc irrsu´1 tYj,ti‰yˆj,tirrsu ăϵ,
whereitisassumedthatthesetofindicestpj,t iqu
jPrms,iPrns
andtpj,tc iqu
jPrms,iPrns
arethesetsof
iŤndicesintherearrangedsequencethatbelongtothetrainingandghostdatasets,respectively.Here,
iPrnspt iYtc iq“t1,...,2nu.
Inthecaseoflossycompressibility,theblock-codingtechniquebringsanotheradvantageinadditiontothe
advantagesdiscussedforthelosslesscase:itallowstoconsidertheaveragedistortioncriterion,insteadof
worst-casedistortioncriterion.Pleasereferto[Sefidgaranetal.,2022]forfurtherdiscussiononthis.
C.1.3 Variable-sizecompressibility
Intheframeworkoftheprevioussections,foragivenmthesizeoftheusedcodebookemRisfixed.Hence,
theboundsderivedusingsuchaframeworkcannotdependonaparticulartrainingdatasetathand. In
particular,whileabovementionedframeworksareusefultoestablishboundsontheexpectationofthe
generalizationerrortheyarenotappropriateforestablishingdata-dependenttailbounds.Toovercomethis
issue,in[SefidgaranandZaidi,2023]a“variable-size”compressibilityisproposedinwhichthesearchfor
asuitablecoveringyˆrrsisamongadata-dependentpartofthecodebook,nottheentirecodebook.Precisely,
thesearchisamongthefirstemR pS,S1,Wq elementsofthecodebook,whereR pS,S1,Wqisadata-dependent
term. Forexample,forPart.iiofTheorem1R istheKL-divergencetermofthegeneralization
pS,S1,Wq
bound.
C.2 Intuitionaboutthefunctionh
D
In Section 2.3, we have provided the bound on the generalization error in terms of the function
h Dpx;x1q: r0,1sˆr0,1sÑr0,2s,definedas:
´ ¯
x`x1
h Dpx,x1q:“2h
b 2
´h bpxq´h bpx1q,
whereh bpxq:“´xlog 2pxq´p1´xqlog 2p1´xq.Asmentioned, 1 2h Dpx,x1qisequaltotheJensen-Shannon
divergencebetweentwobinaryBernoullidistributionswithparametersxandx1.
Inthissection,weprovideanintuitionaboutthisfunction,byshowingitsrelationwiththecombinatorial
termthatappearedin[BlumandLangford,2003].Recallthatthemainresultof[BlumandLangford,2003],
i.e.,Theorem6therein(re-statedintheAppendixB.1),isestablishedintermsofbmaxpn,a{n,δqdefinedas
follows:
´ ¯
a
bmax n, ,δ :“maxtb: Bucketpn,a,bqěδu,
n
where
` ˘` ˘
ÿ n n
Bucketpn,a,bq:“ c `a`b˘´c ,
2n
cPrb,a`bs a`b
whererb,a`bsĂNdenotestheintegerintervalandcPN.
21Now,theintuitionaboutthefunctionh Dpx,x1qisasfollows:byStirling’sformula,wehavethat
` ˘` ˘
mn mn
mt `ma`mb˘´mt ÝÑe´mnhDp nt,a` nb´tq (10)
2mn
ma`mb
asmÑ8.Hence,forlarge(infinite)valuesofmwehavethatthefunctionBucketpmn,ma,mbqof[Blum
andLangford,2003]isdominatedby
max
e´mnhDp nt,a` nb´tq
, (11)
tP b,a`b
(cid:74) (cid:75)
where b,a`b ĂRdenotestherealintervalandtPR.Furthermoreandasaconsequence
(cid:74) (cid:75) ´ ¯ " ˆ ˙ *
m1
bmax mn,
na
,δm ÝÑmax b: tPm b,ain
`b
nh
D
nt ,a` nb´t
ďlogp1{δq , (12)
(cid:74) (cid:75)
asmÑ8.
Itcanbeobservedthatbyconsideringblock-codingandlettingmÑ8,theintractablecombinatorialterms
appearedin[BlumandLangford,2003]canbeexpressedintermsofthefunctionh Dp¨,¨q.
C.3 Onrelationbetweenlossycompressibilityandgeometriccompression
The experimental studies suggest the existence of a relation between generalization performance and
geometrical compression [Geiger and Koch, 2019]. Geometrical compression occurs when the latent
variablesareconcentratedaroundalimitednumberofclusters. Pleasereferto[GeigerandKoch,2019,
Fig. 2] for a visual representation. As mentioned both in Section 3, lossy compression provides an
interpretationofthegeometriccompressionof[GeigerandKoch,2019,Goldfeldetal.,2019]wherelatent
variablesareconcentratedaroundsomeconstellationpoints.Inthissection,weprovideasimpleexample.
Ingeometricalcompression,thelatentvariablesU „P ofX aredistinctbutconcentratedarounda
U|X
few“centers”.Hence,whileinthiscasethe“losslesscompression”capturedbyIpU;Xqbecomeslarge(or
eveninfinite),the“lossycompression”capturedbytherate-distortiontermmaybesmall,asthescattered
latentvariablesaroundthecenterscanbeseenaslossyversionsofthemappingsfromX tooneofthe
centerswithsomesmalldistortion. Asimpleexampleisasfollows: supposeX Pr0,1s,andifX ă0.5,
thenU “´1`X{5,andifX ą0.5,thenU “1`X{5.InthiscasewhileIpU;Xq“8,asimplelossy
mappingP
Uˆ|X
withaveragedistortionlessthan0.05canbefoundsuchthatIpUˆ;Xq“1.
C.4 Conclusionandfuturedirections
Inthispaper,inspiredby[BlumandLangford,2003],wedevelopedacompressibilityframeworkthatwe
usedtoestablishvariousboundsonthegeneralizationerrorofthestochasticlearningalgorithms. The
boundsareexpressedintermsofthenewlydefinednotionof“minimumdescriptionlength”(MDL)of
thepredictedlabelsorthelatentvariables. Thenewnotionisexpressedintermsofa“symmetric”prior,
wherethe“symmetry”isdefinedandusedinthreedifferentways.Thetype-Iandtype-IIsymmetriesare
usefultoestablishtheboundsintermsofMDLofthepredictedlabels.Theformerone,inparticular,shows
aclearconnectionbetweentheseeminglydifferentapproachesof[BlumandLangford,2003]andCMI
[SteinkeandZakynthinou,2020,Harutyunyanetal.,2021].Thetype-IIIsymmetryisusedtoderivethe
mainresultofthispaperwhichisaboundintermsofMDLoflatentvariables.Theresultsofthelastsection
suggestthatthegeneralizationerrorinrepresentationlearningisrelatedtothenewlydefinedMDLoflatent
variables.Unlikemutualinformation,whichcapturesthe“informationleakage”,thenewnotionofMDL
capturesthesimplicityandstructureoftheencoder.Theseinsightsarethenpartlyexploitedtoproposenew
regularizersintermsofnew“data-dependent”priors.Theperformedsimulationsshowtheadvantageof
thesepriorsovertheclassicalonesusedinVIB.
Our proposed framework and obtained results also open up several future research directions. In the
following,wediscusssomeofthosedirections.
• InParti.ofTheorem1,weproposedatailbound,whichunliketheclassicalPAC-Bayesboundwitha
singledrawfromtheposterior[Catoni,2007],doesnotcontaina“disintegrated”term. Asexplained,
thisisduetothechoiceofthelossfunction,whichinherentlycontainsanexpectationterm.Itwouldbe
interestingtoconsideralossfunctionthatcapturesthe“one-shot”predictionoftheperformanceandto
developadisintegratedboundforsuchalossfunction.
22• Oneofthecontributionsofthisworkhasbeentoestablish“rate-distortiontheoretic”boundsbyusinga
lossycompressibilityframework.Forexample,theestablishedboundinTheorem4,containsaninfimum
overall“compressedalgorithms”P thatsatisfysomedistortioncriterion. Notethatthisimplies
Wˆ e|S
thatthisboundholdsfor“any”choiceofeligibleP ,andthus,takingtheinfimumtoobtaina“valid
Wˆ e|S
bound”isnotnecessarilyrequired. Thus,anysimpletechniquethataddsnoiseorappliesparameter
quantizationcanbeconsidered. However, besidesthesegeneralapproaches, aninterestingdirection
ishowtofindan“optimal”P . Perhaps,theapproachtakenin[Tsuretal.,2023],whichusesa
Wˆ e|S
combinationoftheMINEestimator[Belghazietal.,2018]andthe“SFRL”[LiandElGamal,2018]
couldbeadaptedtooursetup.
• Anotherpotentiallyvaluabledirectionwouldbetocomputetheboundsanalyticallyinsimplesetups,
suchastwo-layerneuralnetworkswithGaussiandata,andcomparetheresultswiththeexistingresults
suchas[Veraetal.,2018].Also,itisinstructivetostudynumericallythegeometryandMDLofthelatent
variablesfordifferentnetworkarchitectures,e.g.,FCNversusCNN.Thismayleadtoanewunderstanding
ofhowandwhysomearchitecturesproduce“better”representations.
• Inspiredbyourresults,wehaveproposedsomesimple“data-dependent”priors. Whiletheproposed
priorsshowimprovementsovertheclassicalpriors,theyarelimitedtopriorsthatcanbefactorizedas
productsofsomeGaussiandistributions.Thus,theyonly“partially”capturethejointcompressibilityof
thelatentvariables(andhencetheencoderstructure).Weimaginethatmore“appropriate”priorscanbe
proposedthatcapturebetterthestructureoftheencoder.
• Inthispaper,wepartiallydiscusstheintuitionbehindtheimportanceofthe“structure”oftheencoder.
However,theexpliciteffectofthe“structure”onthegeometryoflatentvariablesandpredictionsneedsto
beinvestigated,aspartiallyshownin[BshoutyandMazzawi,2006].
• Finally,weemphasizethatinrepresentationlearning,oneisinterestedinextractinggoodrepresentations
thataresuitableintermsofgeneralizationerrorformultiplelearningtasks,simultaneously.Itwouldbe
interestingtoextendourresultstosuchmorerealisticsettings.
D Detailsoftheexperiments
Inthissection,wepresentthedetailsoftheexperimentspresentedinSection4.
D.1 VIBobjectivefunction
Thetraditionalinformationbottleneck(IB)approach[Tishbyetal.,2000,Shamiretal.,2010]fortraining
representation learning models, and particularly its variational implementation (VIB) by [Alemi et al.,
2017],considersafixeddata-independentpriorQ“Qb2n.Then,forsomeLagrangemultiplierβ ą0,the
VIBapproachminimizes
ÿn ! ” ı ” ı)
1
n βD KL P U|X,WepU i|x i,Weq}Q ´ E Ui„PU|X,WepUi|xi,Weq logP Yˆ|U,Wdpy i|U i,W dq ,
i“1
usingthereparametrizationtrickof[KingmaandWelling,2014].Ascan´benoticed,thefirstterm,which›ac¯ts
›
asaregularizer,onlytakesintoaccountthetrainingdatasetpartofD
KL
P Ub |2 Xn ,WepU,U1|X,X1,Weq›Q ,
asthetestsetisnotavailable. ThesecondtermattemptstomaximizetherelevanceofU forprediction.
Intuitively,itseekstofindthebestdecoderamongthepossiblechoices,i.e.,theonethatminimizesthe
empiricalrisk.
ApopularchoiceforQisthemulti-dimensionalstandardGaussiandistributionQ“Np0m,Imq,where
Imistheidentitymatrix,misthedimensionofthelatentvariableU,and0m PRmistheallzerovector.
Intheoriginalimplementationof[Alemietal.,2017],foreachsamplex,theencodergeneratesthemean
µx “pµ x,1,...,µx,mqPRmandvarianceσ x2 “pσ x2 ,1,...,σ x2 ,mqPRmofthelatentvariableU. Then,
we let P U|X,WepU|x,Weq “ Npµx,diagpσ x2qq, where diagpσ x2q P Rmˆm denotes a diagonal matrix
whosediagonalelementsaredenotedbythevectorσ2.Thismeansthatthelatentvariablefortheinputxis
x
generatedaccordingtoU „Npµx,diagpσ x2qq.Hence,theobjectivefunctiontominimizebecomes
ÿb ! ” ı ” ı)
1
b βD KL Npµxi,diagpσ x2 iqq}Np0m,Imq ´ E Ui„PU|X,WepUi|xi,Weq logP Yˆ|U,Wdpy i|U i,W dq ,
i“1
(13)
23wherebisthesizeofamini-batchoftrainingsamplesz 1,...,z b,z
i
“px i,y iq.Moreover,(13)isrepeated
iterativelyovermultiplemini-batchesuntiltheconvergenceoftherepresentationlearningmodel.
D.2 LosslessCDVIBobjectivefunction
Wedescribehereanotherlearningapproachwhich,unliketheVIB,usesadata-dependentprior.Thisprior,
coinedCaśtegory-DependentVIB(CDVIB)isagainfactorizedasaproductof2nscalarGaussianpriors,
i.e.,Q“ iPr2nsQ i.EachofthesescalarpriorsQ iischosenamongoneoftheKˆM Gaussianpriors
(centers)–M priorspereachlabel.Moreprecisely,foreachlabelkPrKs,weconsiderM PNpriors
ˆ ˆ ˙˙
Qptq
“N
µptq,diag σptq2
, rPrMs,kPrKs,
k,r k,r k,r
definedoverRm,wherethesuperscripttPNrepresentstheoptimizationiteration.
UnlikeintheVIBapproach,themeanandvarianceofthescalarpriorsareupdatedduringthetraining
process.Firstly,thevectorsµptq andσptq2
areinitializedrespectivelyasm-dimensionalvectorsofzeros
k,r k,r
andones.Next,theseinitializedvectorsareupdatedateachiterationusingaproceduredefinedbelow.
Foranysamplez“px,yqwithlabely“kandforanyiterationtě1,letrPrMsdenotetheindexofthe
category-dependentprior(center)whichistheclosesttoNpµx,diagpσ x2qqintermsoftheKL-divergence
„ ˆ ˙ȷ
r zptq “argminD
KL
Npµx,diagpσ x2 qq}N µp kt ,q r,σp kt ,q r2 .
rPrMs
Supposethatthepickedmini-batchatiterationtisBt “tz 1,...,z bu.ForeachkPrKsandrPrMs,let
! )
I kpt ,q r “ z i: iPrbs,y i “k,r zpt iq “r ,
andletb
k,r
“|I k,r|.Now,ifb
k,r
‰0,updateµp kt ,q
r
andσp kt ,q
r
as
ÿ
µp kt ,q
r
:“p1´αb k,rqµp kt ,´ r1q `α µxi,
g
ziPIk,r
f ÿ
σ kpt ,q
r,j
:“f ep1´αb k,rqσp kt ,´ r,1 jq2 `α σ x2 i,j, j “1,...,m,
ziPIk,r
whereαPr0,1sisacoefficientthatsmoothenstheevolutionofthemeanandvariance,andalsoimplicitly
takesintoaccounttheeffectoftheghostdatasetS1 appearingintheboundsofTheorems4and7. The
optimalvalueofα,amongothers,dependsonthemini-batchsizebandthenumberofcentersM.8
Finally,theconsideredobjectivefunctionatiterationtintheLosslessCDVIBapproachis
ÿb " „ ´ ¯ ˆ ˙ȷ
1 b
i“1
βD KL N µxi,diagpσ x2 iq }N µp yt iq ,r ”zpt iq,diagpσp yt iq ,rzpt iq2 q
ı*
´ E
Ui„PU|X,WepUi|xi,Weq
logP Yˆ|U,Wdpy i|U i,W dq . (14)
D.3 LossyCDVIBobjectivefunction
Inspired by the lossy compression and bounds introduced in our work, we consider the MDL of the
“perturbed”latentvariable,whilepassingtheun-perturbedlatentvariabletothedecoder.Moreprecisely,as
before,weconsidertheloglossforevaluationoftherelevanceofU inthedecoder,i.e.,
” ı
E
Ui„PU|X,WepUi|xi,Weq
logP Yˆ|U,Wdpy i|U i,W dq .
Fortheregularizer,wefirstconsidertheperturbedU as
Uˆ “U `Z
2
“µ
X
`Z 2`σ XZ
1
“Uˆ 1`Uˆ 2, (15)
8ItcanbeshownthatthischoiceofpriorforM “1satisfiesthetype-IIIsymmetrypropertyandforM ą1
isanapproximationofapriorthatsatisfiessuchasymmetry.
24whereZ 1andZ 2areindependentlydrawnfromthesamedistributionNp0m,Imq.Notethatwechose
Uˆ
1
:“µ
X
`Z 2, Uˆ
2
:“σ XZ 1. (16)
Hence,givenpX,Weq,Uˆ
1
„Npµ X,ImqisindependentfromUˆ
2
„Np0m,diagpσ X2 qq.Letusdefinetwo
setsofpriorsQ 1 :“ tQ 1,k,ru kPrKs,rPrMs overUˆ 1 andQ 2 :“ tQ 2,k,ru kPrKs,rPrMs overUˆ 2. Next, for
eachi P rbs,weselecttwopriorsQ 1,i P Q 1 andQ 2,i P Q 2,inamannerthatwillbecomeclearinthe
following.DenotetheinducedpriorforUˆ “Uˆ 1`Uˆ 2,whereUˆ
1
„Q 1,iandUˆ
2
„Q 2,ibyQ i.Then,the
KLdive „rgenceofP Uˆ|X,WepUˆ|X ȷ,WeqandQ i,canbeupperboundedby
D
KL
P Uˆ|X,WepUˆ|X,Weq}Q
i
„ ȷ „ ȷ
ďD
KL
P
Uˆ
1|X,WepUˆ 1|X,Weq}Q
1,i
`D
KL
P
Uˆ
2|X,WepUˆ 2|X,Weq}Q
2,i
„ ȷ „ ȷ
“D
KL
Npµ X,Imq}Q
1,i
`D
KL
Np0m,diagpσ X2 qq}Q
2,i
. (17)
Weusethisupperboundforourregularizer. Tomakethingsmoreformal, foreachlabelk P rKs, we
consider2M priors,M PN,
´ ¯
Q 1pt ,q
k,r
“N µp kt ,q r,Im , rPrMs,kPrKs,
ˆ ˆ ˙˙
Qp 2t ,q
k,r
“N 0m,diag σp kt ,q r2 , rPrMs,kPrKs,
definedoverRm,wherethesuperscripttPNrepresentstheoptimizationiteration.
SimilartolosslessCDVIB,firstly,thevectorsµptq andσptq2
areinitializedrespectivelyasm-dimensional
k,r k,r
vectorsofzerosandones. Next,theseinitializedvectorsareupdatedateachiterationusingaprocedure
definedbelow.
Foranysamplez “px,yqwithlabely “kandforanyiterationtě1,letr PrMsdenotetheindexof
thecategory-dependentprior(center)whichhasthesmallestdistancetopQptq ,Qptq
qinasensethat
1,k,r 2,k,r
minimizestheRHSof(17).Inotherwords,rptqisequalto
z
" „ ´ ¯ȷ „ ˆ ˆ ˙˙ȷ*
argmin D
KL
Npµx,Imq}N µ kpt ,q r,Im `D
KL
Np0m,diagpσ x2 qq}N 0m,diag σp kt ,q r2 .
rPrMs
Then,themeanandvariancesofthepriorsareupdatedexactlysimilarlytoLosslessCDVIB.Forcomplete-
ness,werepeatthisprocedurehere.Supposethatthepickedmini-batchatiterationtisBt “tz 1,...,z bu.
ForeachkPrKsandrPrMs,let
! )
I kpt ,q r “ z i: iPrbs,y i “k,r zpt iq “r ,
andletb
k,r
“|I k,r|.Now,ifb
k,r
‰0,updateµp kt ,q
r
andσp kt ,q
r
as
ÿ
µp kt ,q
r
:“p1´αb k,rqµp kt ,´ r1q `α µxi,
g
ziPIk,r
f ÿ
σ kpt ,q
r,j
:“f ep1´αb k,rqσp kt ,´ r,1 jq2 `α σ x2 i,j, j “1,...,m,
ziPIk,r
whereαPr0,1sisacoefficientthatsmoothenstheevolutionofthemeanandvariance,andalsoimplicitly
takesintoaccounttheeffectoftheghostdatasetS1 appearingintheboundsofTheorems4and7. The
optimalvalueofα,amongothers,dependsonthemini-batchsizebandthenumberofcentersM.
Finally,theconsideredobjectivefunctionatiterationtintheLossyCDVIBapproachis
ÿb " „ ´ ˙ı „ ´ ´ ¯¯ȷ
1 b
i“1
βD KL Npµxi,Imq}N µ ypt ”iq ,rzpt iq,Im `βD KL Np0 ım *,diagpσ x2 iqq}N 0m,diag σp yt iq ,rzpt iq2
´ E
Ui„PU|X,WepUi|xi,Weq
logP Yˆ|U,Wdpy i|U i,W dq . (18)
25D.4 Datasets
Intheexperiments,weusedCIFAR10[Krizhevskyetal.,2009].Thefulldatasetwassplitintoatrainingset
with50,000labeledimagesandavalidationsetwith10,000labeledimages,allofthemofsize32ˆ32ˆ3.
Theinputimageswerescaledtohavemostofthevaluesbetween0and1beforebeingfedtothenetwork.
D.5 Architecturedetails
ThemodelarchitectureconsideredinourexperimentsisdetailedinTable1.Theencoderpartofourtwo-step
predictionmodelisaconvolutionalnetworkconsistingoffourconvolutionallayersfollowedbytwolinear
layers.Weusemax-poolingandaLeakyReLUactivationfunctionwithanegativeslopecoefficientequal
to0.1. Theencodertakesasinputre-scaledimagesandproducesparametersµx andvarianceσ x2 ofthe
latentvariableofdimensionm“64.Thelatentsamplesaregeneratedusingthereparameterizationtrickof
[KingmaandWelling,2014].Next,theproducedlatentsamplesareprocessedbyadecoderconsistingof
onelinearlayerwithasoftmaxactivationfunction.Thedecoderoutputsasoftclassprediction.
Similarly,asin[Duboisetal.,2020],ourevaluatedencoderiscomplexenoughinordertomakeitclose
to“auniversalfunctionapproximator”. Ontheotherhand,weuseasimpledecoderasin[Alemietal.,
2017]inordertoreducespuriousregularizationintroducedbythehighdecoder’scomplexityandhenceto
highlightthebenefitsofourregularizerintermsofgeneralizationperformance.
Table1: Themodelarchitectureusedinexperiments. Theconvolutionallayersareparametrized
respectivelybythenumberofinputchannels,thenumberofoutputchannels,andthefiltersize. The
linearlayersaredefinedbytheirinputandoutputsizes.
Encoder Encodercont’d Encodercont’d
Number Layer Number Layer Number Layer
1 Conv2D(3,8,5) 6 Conv2D(16,16,3) 11 LeakyReLU(0.1)
2 Conv2D(3,8,5) 7 LeakyReLU(0.1) 12 Linear(256,128)
3 LeakyReLU(0.1) 8 MaxPool(2,2) Decoder
4 MaxPool(2,2) 9 Flatten 1 Linear(64,10)
5 Conv2D(8,16,3) 10 Linear(1024,256) 2 Softmax
D.6 Implementationandtrainingdetails
OurpredictionmodelwastrainedusingPyTorch[Paszkeetal.,2019]andaGPUTeslaP100withCUDA
11.0.AllweightswereinitializedusingthedefaultPyTorchXavierinitializationscheme[GlorotandBengio,
2010]withallbiasesinitializedtozero.TheAdamoptimizer[KingmaandBa,2015](β
1
“0.5,β
2
“0.999)
wasusedwithaninitiallearningrateof10´4andanexponentialdecayof0.97.Thebatchsizewasequalto
128throughoutthewholeexperiment.Thecodeusedintheexperimentsisavailableathttps://github.
com/PiotrKrasnowski/MDL_and_Generalization_Guarantees_for_Representation_Learning.
Duringthetrainingphase,wejointlytrainedtheencoderandthedecoderpartsfor200epochsusingeither
thestandardGaussianpriorofthetraditionalVIBobjectivefunctionorourCDVIBobjectivefunctions.As
in[Alemietal.,2017],wegeneratedonelatentsampleperimageduringtrainingand12samplesduring
testing.
D.7 Numericalfindings
Figure3displaysthetrainingandtestperformanceofourmodelforawiderangeofparametersβ.Itcould
benoticedthatthebesttestaccuracywithourLossyCDVIBobjectivefunctionis65%(forβ “ 0.01),
whichisabout2.5%betterthanthebesttestaccuracyforVIB(62.5%forβ “ 0.005). Inthecaseof
LosslessCDVIB,thebestachievedtestaccuracyis64%forβ “1e´5.
E Proofs
E.1 Generalprooftechniques
Mostofourproofscontaintwomainsteps.
26(a)Accuracy. (b)Log-likelihood.
Figure3: Testandtrainperformancesofourtwo-steppredictionmodeltrainedusingthestandard
VIBprior,the"lossless"CDVIBprior,andthe"lossy"CDVIBprior,bothwithM “5. Theplots
showtheaverageover5runsand95%bootstrapconfidenceintervals.
Technicallyspeaking,thefirststepusesDonsker-Varadhan’svariationalrepresentationlemma,tochange
themeasure.Asanexample,inproofofTheorem1,thefirststepgives
λE S,WrgenpS ˆ,Wqs
” ı› › ˙
ďD KL µb Y2nE S1,S,W P Yˆb |2 Xn ,WpYˆn,Yˆ1n |Xn,X1n,Wq › ›µb Y2nQpYˆn,Yˆ i1n |Yn,Y1n q
„ ř ´ ¯ȷ
`logE Yn,Y1n,Yˆn,Yˆ1n„µb2nQpYˆn,Yˆ1n|Yn,Y1nq
enλ
iPrns
1
tYi1‰Yˆ
i1u´1
tYi‰Yˆiu ,
Y i
thatchangesthemeasurefrom
” ı
µb Y2nE
S1,S,W
P Yˆb |2 Xn ,WpYˆn,Yˆ1n |Xn,X1n,Wq ,
to
µb2nQpYˆn,Yˆ1n |Yn,Y1n q.
Y
Thishasaparticularmeaningandintuitioninourfixed-sizecompressibilityapproach.Indeed,onecanshow
thatthereexistsaproperseque ˆnceofcompressionbook ›s,withsize|Yˆ m|ďemR,w ˙here
›
R“D KL µb Y2nP Yˆn,Yˆ1n|Yn,Y1n› ›µb Y2nQpYˆn,Yˆ i1n |Yn,Y1n q .
Thismeansthatbythisstep,wefixasuitablesequenceofcompressionbookssuchthatwithhighprobability
(thatgoesto1asmÑ8),onecanfindthesequenceofpredictedlabelsinthiscodebook.UsingDonsker-
Varadhan’schangeofmeasureisashortcuttothis, aspreviouslyexplainedin[Sefidgaranetal.,2022,
AppendixB.1.] inthecontextofhypothesiscompression. Then,weconsidertheunionboundoverall
elementsofthiscodebookinthenextstep.Fortailbounds,similarinterpretationshold,butthistimewith
variable-sizecompressibilitynotion,asshownin[SefidgaranandZaidi,2023],inthecontextofhypothesis
compression.
The second step can be seen as bounding the generalization error for every element of such code-
book. This step is achieved in quite different manners for the proofs of different results of the paper,
as follows in the coming sections. Note that in this step, the predicted labels have distributions ac-
cording to the prior QpYˆn,Yˆ1n|Yn,Y1nq, rather than the one induced by the learning algorithm, i.e.,
i
Pb2n pYˆn,Yˆ1n|Xn,X1n,Wq.
Yˆ|X,W
E.2 ProofofTheorem1
E.2.1 Parti.
Proof. Wefirstshowthat
Qi Pn Qf iE Y,Y1” D KL´ E X1,X,W” P Yˆb |2 Xn ,WpYˆ,Yˆ1|X,X1,Wqı› › ›Q¯ı “I´ J;Yˆ2nˇ ˇ Y2n¯ . (19)
27LetQ1,bethesetofconditionalpriorsQ1thatcanbewrittenas
i
´ ˇ ¯ ” ´ ˇ ¯ı
Q1 Yˆ,Yˆ1ˇ Y,Y1 “E
J
Q1
1
Yˆ J2n,Yˆ J2 cnˇ Y J2n,Y J2 cn , (20)
for some arbitrary distribution Q1. Here Y2n and Yˆ2n are the concatenations of the vectors pY,Y1q
1
and pYˆ,Yˆ „1q, respectively. It is easyȷto verify that Q i “ Q1 i. Hence, bydenoting PpYˆ,Yˆ1|Y,Y1q :“
E
X1,X,W
P Yˆb |2 Xn ,WpYˆ,Yˆ1|X,X1,Wq wecanwrite
” ´ › ¯ı
›
LHS“ inf E
Y,Y1
D
KL
PpYˆ,Yˆ1|Y,Y1q›Q
QPQi
” ´ › ¯ı
›
“ inf E Y,Y1 D KL PpYˆ,Yˆ1|Y,Y1q›Q1
Q1PQ1
“ inf i E Y2nE J” D KL´ P´ Yˆ J2n,Yˆ J2 cnˇ ˇ Y J2n,Y J2 cn¯› › ›Q1¯ı
Q1PQ1
“infE Yi 2nE J” D KL´ P´ Yˆ J2n,Yˆ J2 cnˇ ˇ Y J2n,Y J2 cn¯› › ›E J” Q1 1´ Yˆ J2n,Yˆ J2 cnˇ ˇ Y J2n,Y J2 cn¯ı¯ı
Q1
´1 ˇ ¯
“I J;Yˆ2nˇ Y2n .
Thiscompletestheproofofshowingtheequalityin(2).
Now,weproceedtoprovetheupperbound. LetQbeanyfixedtype-Isymmetricconditionalprioron
pYˆn,Yˆ1nqgivenpYn,Y1nq.Weshowthat
g
f „ ˆ „ ȷ› ˙ȷ
f ›
E S,WrgenpS,Wqs
ďf e2E Yn,Y1n D KL E X1n,Xn,W P Yˆb n|2 Xn ,WpYˆn,Yˆ1n|Xn,X1n,Wq › ›Q
, (21)
whereYn,Y1n „µb Y2nandX1n,Xn,W „P X1n|Y1nP Xn,W|Yn.Foreaseofnotation,denote
” ı
P Yˆn,Yˆ1n|Yn,Y1n :“E X1n,Xn,W P Yˆb |2 Xn ,WpYˆn,Yˆ1n |Xn,X1n,Wq ,
whereagainX1n,Xn,W „P X1n|Y1nP Xn,W|Yn.Westarttheproofbyapplyingthechangeofmeasure
usingDonsker-Varadhan’svariationalrepresentation(steppaqbelow):
λE S,WrgenpS,Wqs
» fi
ÿ ´ ¯
“E
S,S1,W,Yˆn,Yˆ1n„P S1PS,WP Yˆb |2 Xn
,WpYˆn,Yˆ1n|Xn,X1n,Wq– nλ
iPrns
1
tY i1‰Yˆ
i1u´1
tYi‰Yˆ iu
fl
» fi
ÿ ´ ¯
“E
Yn,Y1n,Yˆn,Yˆ1n„µ Yb2nP
Yˆn,Yˆ1n|Yn,Y1n– nλ 1
tY i1‰Yˆ
i1u´1
tYi‰Yˆ iu
fl
iPrns
ˆ › ˙
p ďaq D KL µb Y2nP Yˆn,Yˆ1n|Yn,Y1n› › ›µb Y2nQpYˆn,Yˆ i1n |Yn,Y1n q
„ ř ´ ¯ȷ
`logE Yn,Y1n,Yˆn,Yˆ1n„µb2nQpYˆn,Yˆ1n|Yn,Y1nq
enλ
iPrns
1
tYi1‰Yˆ
i1u´1
tYi‰Yˆiu . (22)
Y i
Now, we bound the second term. Consider the notation Y i,j P Y2n and Yˆ i,j P Y2n for i P rns and
j P t1,2u. Denotejc :“ 1 tj“1u`1. Furthermore,foreveryi P rns,letK i bearandomvariablethat
takesvaluesuniformlyovert1,2u—ThevariablestK iuareassumedtobemutuallyindependent.Letthe
complementvariableK icbeequalto2ifK
i,j
“1and1otherwise.
» ř ´ ¯fi
logE –
enλ
iPrns
1
tYi1‰Yˆ
i1u´1
tYi‰Yˆiu fl
Yn,Y1n,Yˆn,Yˆ1n„µb2nQpYˆn,Yˆ1n|Yn,Y1nq
Y i
» ˆ ˙fi
ř
p “aq logE –
enλ
iPrns
1 tYKi,i‰YˆKi,iu´1
tYKic,i‰Yˆ Kic,iu fl
Ynˆ2,Yˆnˆ2,Kn„Pb2nQpYˆnˆ2|Ynˆ2qUnifp1,2qbn
Y
28» ˆ ˙fi
ř
“logE E –
enλ
iPrns
1 tYKi,i‰YˆKi,iu´1
tYKic,i‰Yˆ Kic,iu fl
Ynˆ2,Yˆnˆ2„Pb2nQpYˆnˆ2|Ynˆ2q Kn„Unifp1,2qbn
Y
˜ ¸
n
pbq eλ{n`e´λ{n
ďlog
2
λ2
ď , (23)
2n
wherepaqisconcludedbysymmetryofQandpbqbyinequality
ex`e´x ďex2{2.
Combiningthiswith
2
(22)yield
ˆ › ˙
›
E S,WrgenpS,Wqsď λ1 D KL µb Y2nP Yˆn,Yˆ1n|Yn,Y1n› ›µb Y2nQpYˆn,Yˆ i1n |Yn,Y1n q ` 2λ n.
Letting
d
ˆ › ˙
›
λ:“ 2nD
KL
µb Y2nP Yˆn,Yˆ1n|Yn,Y1n› ›µb Y2nQpYˆn,Yˆ i1n|Yn,Y1nq ,
completestheproof.
E.2.2 Partii.
Proof. TheproofofthispropositionfollowssimilarlyasproofofTheorem5(withϵ“0),andavoided
forbrevitay. NotethatsimilartoTheorem5,bynotingthatwithprobabilityatleastp1´δq,LˆpS1,Wqě
LpWq´ logp1{δq{p2nq,onecanalsoestablishatailboundongenpS,Wq.
E.3 ProofofLemma1
Proof. Partsi.andii.canbee´asilyver¯ifiednum `erica ˘lly.ToshowPartiii.,takethederivativewithrespect
tox. Thisderivative,i.e.,log 2´x´x1 ´log 1´x ,isalwaysnon-negativefor1ąxąx1. ForPartiv.
x`x1 x
thesecondpartialderivativeofh Dpx,x1qwithrespecttoxisequalto xp11
´xq
´ px`x1qp2
2´x´x1q
whichis
alwayspositivefor0ďx,x1 ď1.Finally,weshowtheconvexitywithrespecttobothvariablesxandx1
simultaneously.TheHessianofthefunctionh Dpx,x1qequals
« ff
1 ´ 2 ´ 2
H “ xp1´xq px`x1qp2´x´x1q px`x1qp2´x´x1q . (24)
´ 2 1 ´ 2
px`x1qp2´x´x1q x1p1´x1q px`x1qp2´x´x1q
Weshowtheeigenvaluesofthissymmetricmatrixisalwaysnon-negative,andhenceH ispositivesemi-
definite.Thiscompletestheproof.
Solving
ˇ« ffˇ
ˇ ˇ λ´ 1 ` 2 2 ˇ ˇ
|λI 2´H|“ˇ
ˇ
xp1´xq 2px`x1qp2´x´x1q
λ´
1px`x1 `qp2´x´x1q
2
ˇ ˇ“0, (25)
px`x1qp2´x´x1q x1p1´x1q px`x1qp2´x´x1q
reducestosolving
ˆ ˙
1 1 1
pλ`aq2 ´ ` pλ`aq` ´a2 “0, (26)
xp1´xq x1p1´x1q xp1´xqx1p1´x1q
wherea:“ 2 .Therootsofthisequationare
px`x1qp2´x´x1q
c
´ ¯ ´ ¯
2
1 ` 1 ˘ 1 ´ 1 `4a2
xp1´xq x1p1´x1q xp1´xq x1p1´x1q
λ“ ´a. (27)
2
Itisstraightforwardtoverifythatbothrootsarealwaysnon-negative,whichcompletestheproof.
29E.4 ProofofTheorem3
Proof. Wefirstshowthat
Qi Pn Qf iiE Y,Y1” D KL´ E X1,X,W” P Yˆb |2 Xn ,WpYˆ,Yˆ1|X,X1,Wqı› › ›Q¯ı “I´ T;Yˆ2nˇ ˇ Y2n¯ . (28)
LetQ1 ,bethesetofconditionalpriorsQ1thatcanbewrittenas
ii
´ ˇ ¯ ” ´ ˇ ¯ı
Q1 Yˆ,Yˆ1ˇ Y,Y1 “E
T
Q1
1
Yˆ T2n,Yˆ T2n cˇ Y T2n,Y T2n
c
, (29)
forsomearbitrary(andnotnecessarilysymmetricdistributionQ1.HereY2nandYˆ2naretheconcatenations
1
ofthevectorspY,Y1qandpYˆ „,Yˆ1q,respectively.ItiseasyȷtoverifythatQ
ii
“Q1 ii.Hence,bydenoting
PpYˆ,Yˆ1|Y,Y1q:“E
X1,X,W
P Yˆb |2 Xn ,WpYˆ,Yˆ1|X,X1,Wq wecanwrite
” ´ › ¯ı
›
LHS“ inf E
Y,Y1
D
KL
PpYˆ,Yˆ1|Y,Y1q›Q
QPQii
” ´ › ¯ı
›
“ inf E Y,Y1 D KL PpYˆ,Yˆ1|Y,Y1q›Q1
Q1PQ1
“ infii E Y2nE T” D KL´ P´ Yˆ T2n,Yˆ T2n cˇ ˇ Y T2n,Y T2n c¯› › ›Q1¯ı
Q1PQ1
“infE Yii 2nE T” D KL´ P´ Yˆ T2n,Yˆ T2n cˇ ˇ Y T2n,Y T2n c¯› › ›E T” Q1 1´ Yˆ T2n,Yˆ T2n cˇ ˇ Y T2n,Y T2n c¯ı¯ı
Q1
´1 ˇ ¯
“I T;Yˆ2nˇ Y2n .
Thiscompletestheproofofshowingtheequalityin(28).
´ ” ı¯
Nowweproceedtoshowtheupperboundonnh
D
E WrLpWqs,E
S,W
LˆpS,Wq . LetQbeanyfixed
type-IIsymmetricconditionalprioronpYˆn,Yˆ1nqgivenpYn,Y1nq.Weshowthatforně10,
´ ” ı¯
nh
D
E WrLpWqs,E
S,W
LˆpS,Wq
„ ˆ ” ı› › ˙ȷ
ďE Yn,Y1n D KL E S1,S,W P Yˆb |2 Xn ,WpYˆn,Yˆ1n |Xn,X1n,Wq › ›Q `logpnq,
whereYn,Y1n „µb Y2nandS1,S,W „P S1|Y1nP S,W|Yn.
Foreaseofnotation,denote
” ı
P
Yˆn,Yˆ1n|Yn,Y1n
:“E
S1,S,W
P Yˆb |2 Xn ,WpYˆn,Yˆ1n |Xn,X1n,Wq ,
whereS1,S,W „P S1|Y1nP S,W|Yn.Wehave
´ ” ı¯
nh
D
E WrLpWqs,E
S,W
LˆpS,Wq
” ´ ¯ı
ďnE
P S1PS,W«
h
D
ˆ
LˆpS1,Wq,LˆpS,Wq
„ ȷ „ ȷ˙ff
ÿ ÿ
1 1
“nE
S1,S,W
h
D
E
P Yˆb |n X,WpYˆ1n|X1n,Wq n
iPrns1
tY i1‰Yˆ i1u
,E
P Yˆb |n X,WpYˆn|Xn,Wq n
iPrns1
tYi‰Yˆ iu
« ˆ ˙ff
ÿ ÿ
1 1
ďnE
P S1PS,WP Yˆb |2 Xn ,WpYˆn,Yˆ1n|Xn,X1n,Wq
h
D n
iPrns1
tY i1‰Yˆ
i1u,
n
iPrns1
tYi‰Yˆ iu
ˆ › ˙
›
ďD KL µb Y2nP Yˆn,Yˆ1n|Yn,Y1n› ›µb Y2nQpYˆn,Yˆ1n |Yn,Y1n qq
„ ´ ř ř ¯ȷ
`logE
Yn,Y1n,Yˆn,Yˆ1n„µb2nQpYˆn,Yˆ1n|Yn,Y1nq
enhD n1 iPrns1 tYi1‰Yˆ i1u, n1 iPrns1 tYi‰Yˆiu . (30)
Y i
Now, we compute the last term, which does not depend on W anymore. Suppose that Unifp2nq is a
distribution that picks uniformly n indices among indices 2n indices, i.e., the probability of each one
is p21 nq. WedenotesuchindicesbyT “ pT 1,...,Tnq,andthecorrespondingdistributionbyUnifp2nq.
n
30For a vector Y2n of length 2n, we denote the elements corresponding to n indices picked by T as
Y T2n “pY T2 1n,...,Y T2 nnq. WedenotebyTc “pT 1c,...,T ncqtheotherremainingnelementsin1,...,2n
thatarenotpickedbyT.WedenoteYTn,c
“pY Tc,...,Y Tcq.Then,
1 n
„ ´ ř ř ¯ȷ
logE
Yn,Y1n,Yˆn,Yˆ1n„µb2nQpYˆn,Yˆ1n|Yn,Y1nq
enhD n1 iPrns1 tYi1‰Yˆ i1u, n1 iPrns1 tYi‰Yˆiu
Y i » ˆ ˙fi
ř ř
“logE Y2n,Yˆ2n,T,„µb2nQpYˆ2n|Y2nqUnifp2nq– enhD n1 iPrns1 tYTic‰Yˆ Ticu, n1 iPrns1 tYTi‰YˆTiu fl .
(31)
ř
LetV bearandomvariableindicatingV :“ iPr2ns1
tYi‰Yˆ
iuinthesequenceY2n. Then,weconsider
differentcasesforV andshowthat
» ˆ ˙fi
ř ř
E T„Unifp2nq– enhD n1 iPrns1 tYTic‰Yˆ Ticu, n1 iPrns1 tYTi‰YˆTiu fl ďn, (32)
forně10.Thiscompletestheproof.
Weusethefollowinglemmarepeatedlyintherestoftheproof.
Lemma2([Gallager,1968,Exercise5.8.a]). Forj ě1andn´j ě1,wherej,nPN,
˜ ¸
c c
n n n
ď e´nhbpj{nq ď . (33)
8jpn´jq j 2πjpn´jq
Now,
i.IfV Pr1,ns:
` ˘` ˘ ` ˘` ˘
ÿV n n Vÿ´1 n n
enhDpj{n,pV´jq{nq j`V˘´j “`2 ˘ ` enhDpj{n,pV´jq{nq j`V˘´j
2n 2n 2n
j“0 V V j“1 V
a
paq 2
Vÿ´1
nVp2n´Vq
ď` ˘ ` a
2n π jpn´jqpV ´jqpn´V `jq
V j“1
ˆ ˙
pbq 2 1
a Vÿ´1
1 1
ď` ˘ ` nVp2n´Vq `
2n 2π jpn´jq pV ´jqpn´V `jq
V j“1
a Vÿ´1
2 1 1
“` ˘ ` nVp2n´Vq
2n π jpn´jq
V j“1
ˆ ˙
a Vÿ´1
2 1 1 1
“` ˘ ` nVp2n´Vq `
2n nπ j n´j
V j“1
pcq 2 2 ? nÿ´1 1
ď` ˘ ` n3
2n nπ j
V j“1
?
2 2 n
ď ` plogpn´1q`0.58`1{p2n´2qq
2n π
pdq
ďn,
wherepaqisdeducedusingLemma2,pbqisduetoinequality ?1 ď 1 ` 1 foranyx,yą0,pcqbythe
xy 2x 2y
upperboundontheHarmonicseries,andpdqholdsforně2.
ii.IfV Prn`1,2n´1s:
` ˘` ˘
ÿn n n
enhDpj{n,pV´jq{nq j`V˘´j
2n
j“V´n V
31` ˘ ` ˘` ˘
n nÿ´1 n n
“2enhDpn{n,pV´nq{nq V`´n˘
`
enhDpj{n,pV´jq{nq j`V˘´j
2n 2n
˜
V j“V´n`1
¸
V
?
paq 2V n
ď2max max a ,enhDp1,1{nq` ˘
VPrn`2,2n´1s πpV ´nq 2n
n`1
a
Vÿ´1
nVp2n´Vq
` a
π jpn´jqpV ´jqpn´V `jq
j“1
c ˆ ˙
pbq pn`2q 1
a nÿ´1
1 1
ď2 ` nVp2n´Vq `
π 2π jpn´jq pV ´jqpn´V `jq
j“V´n`1
c
a nÿ´1
pn`2q 1 1
“2 ` nVp2n´Vq
π π jpn´jq
j“V´n`1
c ˆ ˙
a nÿ´1
pn`2q 1 1 1
“2 ` nVp2n´Vq `
π nπ j n´j
j“V´n`1
c
pcq pn`2q 2 ? nÿ´1 1
ď2 ` n3
π nπ j
j“1
c
?
pn`2q 2 n
ď2 ` plogpn´1q`0.58`1{p2n´2qq
π π
pdq
ďn,
wherepaqisdeducedusingLemma2,pbqisduetoinequality ?1 ď 1 ` 1 foranyx,y ą 0andby
xy 2x 2y
verifyingthefirsttermforV “n`1numerically,pcqbytheupperboundontheHarmonicseries,andpdq
holdsforně10.
iii.IfV “2n:Inthiscaseř n enhDpj{n,pV´jq{nqpn jqp Vn ´jq “1.
j“V´n p2nq
V
E.5 ProofofTheorem4
LetQbe´aconditionalpriorove ˇrU2ngivenX2n,Y2nandWˆ e P¯We,whichissymmetricinthefollowing
ˇ
sense: Q pu πp1q,...,u πp2nqqpx 1,...,x 2nq,py 1,...,y 2nq,wˆe remains the same for all permutations
π: r2ns ÞÑ r2nsthatpreservesthelabel, i.e., y
πpiq
“ y i. Then, weshowthatfortheK-classification
learningtask,
c
2B`K`2
E S,WrgenpS,Wqsď2
n
`ϵ,
where
” ´ › ¯ı
›
B :“infE
S,S1,Wˆ e„P S,WˆeP S1
D
KL
P Ub |2 Xn
,Wˆ
epUn,U1n |Xn,X1n,Wˆ eq›Q ,
andtheinfimumisoverallP
Wˆ e|S
suchthat ”forWˆ “pWˆ e,W dq,
ı
E
PS,WP Wˆe|S
genpS,Wq´genpS,Wˆq ďϵ.
Proof. WeprovethetheoremforWˆ “W andϵ“0.Thegeneralresultfollowsbythedistortioncriterion
andapplyingthetheoremongenpS,Wˆq.WestarttheproofsimilartotheproofofTheorem1;butwitha
differentexpansionoftheprobabilitydistribution.
λE S,WrgenpS,Wqs
» fi
ÿ ´ ¯
“E
S,S1,W,Un,U1n,Yˆn,Yˆ1n„P
S1PS,Wν1– nλ 1
tY i1‰Yˆ
i1u´1
tYi‰Yˆ iu
fl
iPrns
ˆ › ˙
›
ďD
KL
P S1P S,WP Ub |2 Xn ,WepUn,U1n |Xn,X1n,Weq› ›P S1P S,WQpUn,U1n |S,S1,Wq
32„ ř ´ ¯ȷ
`logE
S,S1,W,Un,U1n,Yˆn,Yˆ1n„P S1PS,Wν2
enλ
iPrns
1
tYi1‰Yˆ
i1u´1
tYi‰Yˆiu , (34)
where
ν
1
:“P Ub |2 Xn ,WepUn,U1n |Xn,X1n,WeqP Yˆb |2 Un ,WdpYˆn,Yˆ1n |Un,U1n,W dq,
ν
2
:“QpUn,U1n |S,S1,WqP Yˆb |2 Un ,WdpYˆn,Yˆ1n |Un,U1n,W dq.
Now,weboundthelastterm.Let
´ ¯ ” ı
P Q Yˆn,Yˆ1n |S,S1,W :“E pUn,U1nq„QpUn,U1n|S,S1,Wq P Yˆb |2 Un ,WdpYˆn,Yˆ1n |Un,U1n,W dq . (35)
Then,thelasttermin(34)canbewrittenas
„ ř ´ ¯ȷ
logE
S,S1,W,Yˆn,Yˆ1n„P S1PS,WPQpYˆn,Yˆ1n|S,S1,Wq
enλ
iPrns
1 tYi1‰Yˆ1,iu´1
tYi‰Yˆ2,iu .
Not ´e that since we h ¯ave P Yˆ|U,Wd,X,Y “ P Yˆ|U,Wd, it can be easily verified that the distribution
P
Q
Yˆn,Yˆ1n|S,S1,W issymmetricwithrespecttoallpermutationsπthatpreservethelabelsofY.
Foreachs,s1,letf: rnsÑrns,iPrnsandk: rnsÑrns,iPrnsbepermutationsofindicesofsands1,
respectively,whereY
fi
“Y k1
i
foriďT andY
fi
‰Y k1
i
foriąT,andwhereT equalsn´ n 2}pˆs´pˆ s1}1.
Here,pˆs andpˆs areempiricaldistributionsofY insands1,respectively. Wedenotepyˆn,yˆ1nqbyyˆ2ˆn,
where@iPrns,yˆ
1,i
“yˆ i1 andyˆ
2,i
“yˆ i.
ConsiderthebinaryrandomvariableK itakingvalueasp2,f iqorp1,k iqwithprobability1{2(independent
ofotherj ‰i).DenotethecomplementarychoiceasK ic,i.e.,K ic “p2,f iqiffK
i
“p1,k iq.
Then,duetotheparticularsymmetryofQ,andbyusingshorthandnotation
´ ¯
P:“P S1P S,WP
Q
Yˆn,Yˆ1n |S,S1,W ,
wehave
» ˆ ˙fi
ř
E S,S1,W,Yˆ2ˆn„P–
enλ
iPrns
1
tYk1 i‰Yˆ
1,kiu´1
tYfi‰Yˆ 2,fiu fl
»$ ˆ ˙,$ ˆ ˙,fi
“E
S,S1,W,Yˆ2ˆn„P–& %źT enλ 1
tYk1 i‰Yˆ
1,kiu´1
tYfi‰Yˆ 2,fiu
. -&
%
źn enλ 1
tYk1 i‰Yˆ
1,kiu´1
tYfi‰Yˆ 2,fiu
.
-fl
i“1 i“T`1
»$ ˆ ˙, fi
ďE S,S1,W,Yˆ2ˆn„P–& %źT enλ 1 tYi‰Yˆ 1,kiu´1 tYfi‰Yˆ 2,fiu . -eλpn n´Tqfl
i“1
»$ ˆ ˙, fi
“E S,S1,W,Yˆ2ˆn,Kn„PUnifpp2,fiq,p1,kiqqbn–& %źT enλ 1 tYi‰YˆKiu´1 tYfi‰Yˆ Kicu . -eλpn n´Tqfl
i“1
»˜ ¸ fi
T
ďE
S,S1,W„P
S1PS,W–
eλ{n` 2e´λ{n
eλpn n´Tqfl
„ ȷ
ďE
S,S1,W„P S1PS,W
eλ 2n2T 2eλpn n´Tq
” ı
p ďaq eλ 2n2 ˆˆE
S,S1,„P S1PS,W
eλ
2
˙}pˆS´pˆ S1}1
λ2 K`2 3λ2
ďexp ` `
2n 2 2n
ˆ ˙
2λ2 K`2
ďexp ` ,
n 2
33wherepaqisdeducedfromLemma3,conditionedthatλ{nă1.36.
a
Now,combiningthiswith(34),andlettingλ“λ˚ “ np2B`K`2q{4,theexpectationofgeneraliza-
tionerrorisupperboundedby
2B`K`2 2λ
E S,WrgenpS,Wqs“
c 2λ
`
n
p2B`K`2q
ď2 ,
n
b
ifλ˚{nă1.36.Notethatif1.36ăλ˚{n“ p2B`K`2q,then
4n
c
p2B`K`2q
2 “4pλ˚{nqą1.
n
Sincegeneralizationerrorisalwaysboundedby1,hence,thisboundalwaysholds.
Lemma3. SupposethatYnandY1nare2ni.i.d.instancesofY „pPrKs.Then,ifλ{nă0.68,then,
” ı
logE Yn,Y1n eλ}pˆYn´pˆ Y1n}1 ď
K 2`2
`
6 nλ2
.
ThelemmaisprovedinAppendixE.9usingresultsof[Devroye,1983].
E.6 ProofofTheorem5
LetQbeanyfixedsymmetricprioronpYˆn,Yˆ1nqthatcoulddependonpXn,Yn,X1n,Y1nq. Weshow
thatforanyϵ P Randδ P R` withprobabilityatleastp1´δqoverchoicesofS andS1,wehavethat
E W„QrgenpS,Wqsisupperboundedby
c
logp2{δq
`infg f f f eE
Wˆ„P
Wˆ|S„ D KLˆ P Yˆb |2 Xn ,WˆpYˆn,Yˆ1n|Xn,X1n,Wˆq› › › ›Q˙ȷ `logp? 8n{δq
`ϵ, (36)
2n p2n´1q{4
wheretheinfimumisoverallP thatsatisfy
Wˆ|S
”ˇ´ ¯ ´ ¯ˇı
ˇ ˇ
E PW|SP Wˆ|S ˇ LˆpS1,Wq´LˆpS,Wq ´ LˆpS1,Wˆq´LˆpS,Wˆq ˇ ďϵ{2. (37)
Proof. ConsideradistributionP Wˆ|S thatsatisfies(37).Denoteλ˚ “ 2n 4´1 and
g
f „ ˆ › ˙ȷ
f › ?
∆pS,Qq:“f eE
Wˆ„P
Wˆ|S
D
KL
P Yˆb |2 Xn ,WˆpYˆn,Yˆ1n|Xn,X1n,Wˆq› ›Q `logp 8n{δq
`ϵ.
λ˚
Furthermore,weusetheshorthandnotationspˆ:“Pb2n pYˆn,Yˆ1n|Xn,X1n,Wˆq.Then,
Yˆ|X,Wˆ
˜ c ¸
logp2{δq
P
S„µbn
E W„PW|SrgenpS,Wqsą
2n
`∆pS,Qq
´ ” ı ¯
paq
ďP
pS,S1q„µb2n
E
W„PW|S
LˆpS1,Wq´LˆpS,Wq ą∆pS,Qq `δ{2
ˆ „ ´ ¯ ȷ ˙
pbq 2
ďP
pS,S1q„µb2n
E
W„PW|S
λ˚ LˆpS1,Wq´LˆpS,Wq ąλ˚∆pS,Qq2 `δ{2
ˆ „ ´ ¯ ȷ ˙
pcq 2
ďP pS,S1q„µb2n ¨E Wˆ„P
Wˆ|S
λ˚ LˆpS1,W »ˆ ¨q´LˆpS,Wˆq ąλ˚∆pS,Qq2 ˛` fiδ{2
˛
ÿ ´ ¯ 2
“P pS,S1q„µb2n˚ ˝λ˚E
Wˆ,Yˆn,Yˆ1n„P
Wˆ|Spˆ— –˝ n1 1
tY i1‰Yˆ
i1u´1
tYi‰Yˆ iu
‚ffi flěλ˚∆pS,Qq2‹ ‚
iPrns
`δ{2
34˜ ˆ › ˙
p ďdq P
pS,S1q„µb2n
D
KL
P Wˆ|Sρˆ› › ›P Wˆ|SQ (38)
„ ´ ´ ¯¯ ȷ ¸
2
`logE Yˆn,Yˆ1n„Q exp λ˚ LˆpS1,Y1n q´LˆpS,Yn q ěλ˚∆pS,Qq2
`δ{2
˜ « ff
p ďeq
P pS,S1q„µb2n logE Yˆn,Yˆ1n„Q
eλ˚´ n1 ř iPrns´ 1
tYi1‰Yˆ
i1u´1 tYi‰Yˆiu¯¯ 2
ě
« ff ¸
logE
S,S1,Yˆn,Yˆ1n„PSP S1Q
eλ˚´ n1 ř iPrns´ 1
tYi1‰Yˆ
i1u´1 tYi‰Yˆiu¯¯ 2
`logp2{δq
`δ{2
pfq
ďδ, (39)
wherepaqholdsbyHoeffdinginequalityandusingthefactthatforarbitraryrandomvariablesU,V and
constantsa,bPR,PpU `V ąa`bqďPpU ąaq`PpV ąbq,pbqbyapplyingtheJenseninequalityon
theconvexfunctionfpxq“x2,pcqbyusingthedistortionfunction(37)andsincethelossisboundedby
one,pdqbyusingtheDonsker-Varadhan’svariationalrepresentationlemma,peqisshowninthefollowing,
andpfqbyMarkovinequality.
Hence,itremainstoshowthesteppeq.Tothisend,itissufficientupperbound
« ff
logE
S,S1,Yˆn,Yˆ1n„PSP S1Q
eλ˚´ n1 ř iPrns´ 1
tYi1‰Yˆ
i1u´1 tYi‰Yˆiu¯¯ 2
?
bylogp 2nq.Notethatthistermsequals
« ff
logE
Yn,Y1n,Yˆn,Yˆ1n„P Yb2nQ1
eλ˚´ n1 ř iPrns´ 1
tYi1‰Yˆ
i1u´1 tYi‰Yˆiu¯¯ 2
,
” ı
whereQ 1isequaltoE
PXn|YnP X1n|Y1n
QpYˆn,Yˆ i1n|Xn,Yn,X1n,Y1nq .Now,
» ˜ ¸ fi
ř ´ ¯ 2
E
— –eλ˚ n1
iPrns
1 tYi1‰Yˆ i1u´1 tYi‰Yˆiu ffi
fl
Yn,Y1n,Yˆn,Yˆ1n„µ Yb2nQ1pYˆn,Yˆ i1n|Yn,Y1nq
» ˜ ˆ ˙¸ fi
ř 2
p “aqE — –eλ˚ n1 iPrns 1 tYKi,i‰YˆKi,iu´1 tYKic,i‰Yˆ Kic,iu ffi fl
Ynˆ2,Yˆnˆ2,Kn„P Yb2nQ1pYˆnˆ2|Ynˆ2qUnifp1,2qbn
» ˜ ˆ ˙¸ fi
ř 2
“E E — –eλ˚ n1 iPrns 1 tYKi,i‰YˆKi,iu´1 tYKic,i‰Yˆ Kic,iu ffi fl
Ynˆ2,Yˆnˆ2„P Yb2nQ1pYˆnˆ2|Ynˆ2q Kn„Unifp1,2qbn
pbq?
ď 2n, (40)
wherepaqisconcludedbysymmetryofQ 1andpbqisdeducedsince
ˆ ˙
ÿ
1
n
iPrns
1
tYKi,i‰Yˆ
Ki,iu´1
tYKic,i‰Yˆ Kic,iu
,
?
is1{ n-subgaussianprocessandhence
» ˆ ˆ ˙˙ fi
ř 2
E Kn„Unifp1,2qbn– e n1 iPrns 1 tYKi,i‰YˆKi,iu´1 tYKic,i‰Yˆ Kic,iu {p4{p2n´1qqfl ď? 2n,
dueto[Wainwright,2019,Theorem2.6.IV.].Thiscompletestheproof.
35E.7 ProofofTheorem6
LetQbeanyfixedsymmetricprioronpYˆn,Yˆ1nqthatcoulddependonpXn,Yn,X1n,Y1nq.Then,forany
δPR`,weshowthatwithprobabilityatleastp1´δqoverchoiceofpS,S1,Wq„P S1P S,W,
´ ¯ ˆ › › ˙
nh
D
LˆpS1,Wq,LˆpS,Wq ďD
KL
P Yˆb |2 Xn ,WpYˆn,Yˆ1n |Xn,X1n,Wq› ›Q `logpn{δq.
Proof. TheproofisacombinationofproofsofTheorems3and5. DenotetheRHSoftheboundinthe
theoremas∆pS,S1,Wq.
First,notethat
´ ¯
nh
D
LˆpS1,Wq,LˆpS,Wq
¨ » fi » fi˛
ÿ ÿ
“nh D˝E
Yˆ1n„P Yˆb |n
X,WpYˆ1n|X1n,Wq– n1 iPrns1
tY i1‰Yˆ
i1ufl ,E
Yˆn„P Yˆb |n
X,WpYˆn|Xn,Wq– n1 iPrns1
tYi‰Yˆ
iufl‚
» ¨ ˛fi
ÿ ÿ
ďnE
Yˆn,Yˆ1n„P Yˆb |2 Xn
,WpYˆn,Yˆ1n|Xn,X1n,Wq– h D˝ n1 iPrns1
tY i1‰Yˆ
i1u, n1 iPrns1
tYi‰Yˆ
iu‚fl
ˆ › ˙
›
ďD KL P Yˆb |2 Xn ,WpYˆn,Yˆ1n |Xn,X1n,Wq› ›QpYˆn,Yˆ1n |Xn,Yn,X1n,Y1n q
„ ´ ř ř ¯ȷ
`logE
Yˆn,Yˆ1n„QpYˆn,Yˆ1n|Xn,Yn,X1n,Y1nq
enhD n1 iPrns1 tYi1‰Yˆ i1u, n1 iPrns1 tYi‰Yˆiu . (41)
i
Hence,
´ ´ ¯ ¯
P
S,S1,W
nh
D
LˆpS1,Wq,LˆpS,Wq ą∆pS,S1,Wq
ˆ „ ´ ř ř ¯ȷ ˙
p ďaq P
Yn,Y1n„µb2n
logE
Yˆn,Yˆ1n„Q
enhD n1 iPrns1 tYi1‰Yˆ i1u, n1 iPrns1 tYi‰Yˆiu ąlogpn{δq
ˆ „ ´ ř ř ¯ȷ
p ďbq P Yn,Y1n„µb2n logE Yˆn,Yˆ1n„Q enhD n1 iPrns1 tYi1‰Yˆ i1u, n1 iPrns1 tYi‰Yˆiu ą
„ ´ ř ř ¯ȷ ˙
logE
Yn,Y1n,Yˆn,Yˆ1n„µb2nQ
enhD n1 iPrns1 tYi1‰Yˆ i1u, n1 iPrns1 tYi‰Yˆiu `logp1{δq
Y
pcq
ďδ,
wherepaqfollowsby(41),pbqholdssince
„ ´ ř ř ¯ȷ
logE
Yn,Y1n,Yˆn,Yˆ1n„µb2nQ
enhD n1 iPrns1 tYi1‰Yˆ i1u, n1 iPrns1 tYi‰Yˆiu ďlogpnq,
Y
bytheproofofTheorem3,andpcqisderivedusingMarkovinequality.Thiscompletestheproof.
E.8 ProofofTheorem7
Le´t Q be a type-III sy ˇmmetric conditional prior ove¯r U2n given X2n,Y2n and We P We, namely,
ˇ
Q pu πp1q,...,u πp2nqqpx 1,...,x 2nq,py 1,...,y 2nq,w remainsthesameforallpermutationsπ: r2nsÞÑ
r2ns that preserves the label, i.e., y πpiq “ y i for i P rns. Then, for any λ P R`, we show that for
K-classificationlearningtask,withprobabilityatleastp1´δqoverpS,S1,Wq„P S1P S,W,
´ › ¯
›
LˆpS1,Wq´LˆpS,Wqď
D
KL
P Ub |2 Xn ,WepUn,U1n|Xn,X1n,Weq›Q `pK`2q{2`logp1{δq
`
2λ
.
λ n
ThepoofofthesecondpartfollowstriviallyusingHoeffding’sinequality.
Proof. Note that whenever λ{n ě 1.36, the bound trivially holds. Hence, assume that λ{n ă 1.36.
Denote the RHS of the bound of Part i. as ∆pS,S1,Wq. We use the shorthand notation p
1
:“
P Ub |2 Xn ,WepUn,U1n|Xn,X1n,Weqandp
2
:“P Yˆb |2 Un ,WdpYˆn,Yˆ1n|Un,U1n,W dq.
36NotethatpS,S1,Wq„P S1P S,W.Now,
´ ¯
P
S,S1,W
LˆpS1,Wq´LˆpS,Wqą∆pS,S1,Wq
¨ » fi ˛
ÿ ´ ¯
“P S,S1,W˝ λE p1p2– n1 1
tY i1‰Yˆ
i1u´1
tYi‰Yˆ iu
fl ěλ∆pS,S1,Wq‚
iPrns
˜ ˆ › ˙
p ďaq P
S,S1,W
D
KL
p 1p 2› › ›Qp
2
(42)
¸
” ´ ´ ¯¯ı
`logE Qp2 exp λ LˆpS1,Y1n q´LˆpS,Yn q ěλ∆pS,S1,Wq
˜ „ ř ´ ¯ȷ
p ďcq
P S,S1,W logE Qp2
enλ
iPrns
1
tYi1‰Yˆ
i1u´1
tYi‰Yˆiu ě
„ ř ´ ¯ȷ ¸
logE
P S1PS,WQp2
enλ
iPrns
1
tYi1‰Yˆ
i1u´1
tYi‰Yˆiu `logp1{δq
pdq
ďδ, (43)
wherepaqbyusingtheDonsker-Varadhan’svariationalrepresentationlemma,pbqsinceduetoproofof
Theorem4(inAppendixE.5),
logE P
S1PS,WQp2„ enλř iPrns´ 1
tYi1‰Yˆ
i1u´1 tYi‰Yˆiu¯ȷ
ď
2 nλ2
`
K 2`2
,
whenλ{nă1.36,andpdqbyMarkovinequality.Thiscompletestheproof.
E.9 ProofofLemma3
Proof. Theprooftakesitsmainelementsfrom[Devroye,1983]. LetY ,Y ,...andY1,Y1,...bei.i.d.
1 2 1 2
realizationsofY „pPrKs.DenoteP Ypkq“p
k
forkPrKs.LetN andN1betwoindependentPoisson
randomvariableswithmeann.LetU iandU i1,iPrKs,benumberofoccurrencesofvaluekamongYrns
andYr1ns,respectively. Moreover,letV i andV i1,i P rKs,benumberofoccurrencesofvaluekamong
YrNsandYr1N1s,respectively.Asshownin[Devr“oye‰,1983],V 1,...,V
K
andV 11,...,V K1 areindependent
PoissonrandomvariableswithmeansErV ks“E V k1 “np k,forkPrKs.Inaddition,pU 1,...,U Kqand
pU 11,...,U K1 qaretwoindependentmultinomialpn,p 1,...,p Kqrandomvectors.Then,
” ı ” ř ř ı
E eλ}pˆYn´pˆ Y1n}1 ďE enλp K i“1|Ui´Vi´pU i1´V i1q|q enλp K i“1|Vi´V i1|q
c
” ı ” ı
p ďaq E e2 nλpř K i“1|Ui´Vi´pU i1´V i1q|q E e2 nλpř K i“1|Vi´V i1|q
c
” ı ” ı
ř ř
ď E e2 nλp K i“1|Ui´Vi|`|U i1´V i1|q E e2 nλp K i“1|Vi´V i1|q
c
” ı ” ı
ř
ď E e2 nλp|N´n|`|N1´n|q E e2 nλp K i“1|Vi´V i1|q , (44)
wherepaqisduetoCauchy-Schwarzinequality.
” ı
RecallthatforrandomvariableXhavingthePoissondistributionwithmeanη,wehaveE etX “eηpet´1q.
Now,
” ı ” ı ” ı
E e2 nλp|N´n|`|N1´n|q “E e2 nλp|N´n|q E e2 nλp|N1´n|q
” ı ” ı
ďE e2 nλpN´nq`e2 nλpn´Nq E e2 nλpN1´nq `e2 nλpn´N1q
ď4e2npe2 nλ ´1´2 nλq,
(45)
37and
” ř ı źK ´ ” ı¯
E e2 nλp K i“1|Vi´V i1|q ď E e2 nλpV i1´Viq `e2 nλpVˆ i1´V i1q
i“1
“2K
źK
enpipe2 nλ ´1`e´2 nλ
´1q
i“1
“2Kenpe2 nλ ´1`e´2 nλ
´1q, (46)
Combining(44),(45),and(45)yield
” ı ˆ ˙
logE eλ}pˆS´pˆ S1}1 p ďaqK`2 `n e2 nλ ´1´ 2λ ` 1 e2 nλ ` 1 e´2 nλ ´1
2 n 2 2
˜ ˆ ˙ ˆ ˙ ¸
2 2
K`2 1 2λ 7 2λ
“ `n `
2 1.2 n 12 n
K`2 6λ2
ď ` ,
2 n
wherepaqholdsduetotwoinequalitiesthatcanbeverifiednumerically:i)forxă1.36,ex ă1`x` x2
1.2
andii)forxă1.3, ex`e´x ă1` 7 x2.
2 12
38