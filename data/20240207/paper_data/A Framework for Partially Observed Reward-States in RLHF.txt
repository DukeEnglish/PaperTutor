A Framework for Partially Observed Reward-States in
RLHF
Chinmaya Kausik1, Mirco Mutti2, Aldo Pacchiano3,˚, Ambuj Tewari1,˚
1Universityof Michigan, 2Technion, 3Broad Instituteof MIT and Harvard & Boston University
˚Joint senior-authorship
Abstract
Thestudyof reinforcement learning from human feedback (RLHF)has gained prominence
in recent years due to its role in the development of LLMs. Neuroscience research shows
that human responses to stimuli are known to depend on partially-observed “internal states.”
Unfortunately current models of RLHF do not take take this into consideration. Moreover
most RLHFmodels do not account for intermediate feedback, which is gaining importance in
empiricalworkandcanhelpimprovebothsamplecomplexityandalignment. Toaddressthese
limitations, we model RLHF as reinforcement learning with partially observed reward-states
(PORRL).WeshowreductionsfromthethetwodominantformsofhumanfeedbackinRLHF–
cardinalandduelingfeedbacktoPORRL.Forcardinalfeedback,wedevelopgenericstatistically
efficientalgorithmsandinstantiatethemtopresentPOR-UCRLandPOR-UCBVI.Fordueling
feedback,weshowthatanaivereductiontocardinalfeedbackfailstoachievesublineardueling
regret. Wethenpresentthefirstexplicitreductionthatconvertsguaranteesforcardinalregret
to dueling regret. We show that our models and guarantees in both settings generalize and
extendexistingones. Finally,weidentifyarecursivestructureonourmodelthatcouldimprove
the statistical and computational tractability of PORRL, giving examples from past work on
RLHFas well as learning perfect reward machines, which PORRL subsumes.
1 Introduction
Asautomatedsystemsbecomemoreubiquitous,theneedtounderstandhowtoaligntheirobjectives
with the needs of humans that interact with them has become increasingly important (Christian,
2020; Hendrycks et al., 2021; Leike et al., 2018; Ji et al., 2023). The explosive success of large
languagemodels (LLMs) andtheir trainingmethodologieshas spurreda flurry ofresearchworkon
how to successfully distill training objectives from human data and guide these systems towards
behaviors that are aligned with human expectations (Bai et al., 2022; Ouyang et al., 2022). Some
of the most successful algorithms for training, fine-tuning language models used in technologies
such as ChatGPT (Achiam et al., 2023) rely on an RLHF pipeline.
Thestudyofhowtofindapolicythatmaximizesanobjectivedefinedintermsofhumanlabeled
datainRLdomainsisknownasRLHForreinforcementlearningfromhumanfeedback(Christiano et al.,
2017; Wirth et al., 2017). Some of the methods for this entail learning a reward function from hu-
man data, and then using the learnt reward function as an input to a traditional reinforcement
learning algorithm such as PPO (Schulman et al., 2017). Although there are other methods (such
1
4202
beF
5
]GL.sc[
1v28230.2042:viXraasDPO(Rafailov et al.,2023))thatdonotexplicitlyusetraditionalreinforcementlearningtotrain
LLMs, the language of rewards and reinforcement learning is crucial in deriving the DPO objec-
tive. Additionally, while there has been effort towards moving past simple reward-based feedback
models (Wu et al., 2023; Swamy et al., 2024), reward-basedRLHF remains dominant in practice.
Gainingaproperunderstandingofthereward-basedRLHFparadigmisimportantfordesigning
better methods for aligning to human feedback. This is not just important for training LLMs,
but can also be used in other human-facing technologies such as robotics (Christiano et al., 2017;
Brown et al., 2019; Shin et al., 2023) and recommender systems (Xue et al., 2022).
There exist two dominant kinds of feedback in reward-based RLHF, cardinal and dueling feed-
back, also known as once-per-episode and preference feedback, respectively. Cardinal feedback
requiresthe human labeler to providea binary labelovera single trajectoryofinteractionbetween
the agent and the environment Efroni et al. (2021); Chatterji et al. (2021). In its simplest form,
dueling feedback requires the human to specify which one of two trajectories they prefer. Dueling
feedback combined with RL algorithms have been successfully used to train LLMs (Ziegler et al.,
2019; Stiennon et al., 2020; Askell et al., 2021; Bai et al., 2022; Ouyang et al., 2022).
We observe that current models of reward-based RLHF assume a very specific model of non-
Markovian rewards. Modeling rewards as non-Markovian is natural, since human responses to
stimuli are known to be affected by partially-observed and evolving “internal states” Flavell et al.
(2022). For example, when a human reads a piece of text (possibly generated by an LLM), their
assessmentmay oscillate betweenopposing sentiments in different parts ofthe text. Unfortunately,
current models do not explicitly incorporate such “internal states” that affect rewards, and are
limited to a specific linear model of rewards.
Additionally,currentmodelsassumethatfeedbackisreceivedonlyonceattheendofanepisode
or pair of episodes. In many applications such as motion Lee et al. (2021) and mathematical
reasoningUesato et al.(2022), correctlyincorporatingintermediate or“snippet-level” feedbackcan
speed up learning as well as account for human curricula. Thus, we aim to address the following
questions:
• How do we generalize current models of RLHF to a richer model explicitly incorporating
“internal states” and intermediate feedback?
• What algorithms give us low regret on the cardinal feedback in our setting?
• How can we use these ideas to get algorithms achieving low dueling regret?
• What are some general structural assumptions on our models that can improve computational
and statistical tractability?
In this work,we propose a frameworkfor RLHF that explicitly incorporatesthe “internalstate”
involved in human feedback. We provide optimistic algorithms for cardinal feedback and show
that dueling feedback can be reduced to optimistic algorithms for cardinal feedback. We further
show that existing RLHF models such as once-per-episode and dueling feedback are sub-cases
of this expanded framework. Perhaps surprisingly, in some cases the once-per-episode and dueling
feedbackcanbethoughtofasimplicitly modelinganinternalstateofthehumanlabelerthroughout
the trajectory.1 We finally discuss how assuming a recursive structure on the internal states can
improve statistical and computational tractability, highlighting two special cases.
1The trajectory features φpτq in Chatterji etal. (2021); Pacchianoetal. (2021) can be thought of as internal
states at the end of a trajectory. As noted above, however, the authors assume a linear feedback model over φpτq
anddonothandleintermediate feedback.
21.1 Contributions
• PORRL as general reward-based RLHF: We introduce a model of RL with partially ob-
served reward-states (PORRL) that generalizes current RLHF models RLHF to incorporating
“internal states” and model intermediate feedback. We hope that our algorithms serve as a tem-
plate for adapting algorithms from traditional RL and existing work in RLHF to PORRL.
• Optimism for cardinal PORRL: We describe generic optimistic algorithms for cardinal feed-
backinPORRLundergeneralfunctionapproximation forthe“internalstates” thataffectrewards.
We then instantiate them to present POR-UCRL and POR-UCBVI, which achieve regret of the
form O poly H,S,A d d ?T under minimal assumptions.2 Our results recovers
pp p q` h h C,h q q
those ofChatterji et al.(2021) asa specialcase,who canbe interpretedasworkingwith a linear
ř a
functiorn class for internal states.
• ReductionfromduelingtocardinalPORRL:Weshowthatanaïveblackboxreductionfrom
dueling to cardinal PORRL always fails. We design a whitebox reduction from dueling PORRL
to a large class of optimistic algorithms for cardinal PORRL. To the best of our knowledge, we
present the first explicit reduction that converts guarantees for cardinal regret to dueling regret
in RL. Previous reductions have only considered sample complexity and the cardinal regret of
the better of the dueling policies. Dueling regret is important in online scenarios, where both
options presented to the human must be good.
• Recursive structure on internal states: We argue that having a recursive structure on
internalstatescanimprovestatisticalandcomputationalperformance. Weshowthatourcardinal
regretguaranteescan be as bad as ?AHT for a class of perfect rewardmachines with stochastic
feedback,whichformaspecialcaseofourframework. However,aspecializedalgorithmleveraging
the recursive structure can achieve AHlog T cardinal regret.
2p q
1.2 Related Work
RLHF with cardinal and dueling feedback. RL with human preferences has a long his-
tory(Akrour et al.,2012;Busa-Fekete and Hüllermeier,2014; Sadigh et al.,2017). It hasbeensuc-
cessfullyusedindisparatedomainssuchasrobotics,gamesandlargelanguagemodels. Efroni et al.
(2021); Chatterji et al. (2021) theoretically study the problem of learning from cardinal feedback.
Theoretical guarantees for utility-based preferential feedback can be found in (Novoseller et al.,
2020; Saha et al., 2023; Chen et al., 2022b; Zhan et al., 2023). The non-Markovian nature of the
optimal policy under these models of RLHF contributes greatlyto why the problemis harder than
traditional RL.
Internal states and intermediate feedback Thereisevidenceinneuroscienceresearchtoindi-
cate that humanresponses to stimuli areaffected by “‘internalstates’— partially hidden variables
that profoundly shape perception, cognition, and action” (see (Flavell et al., 2022)). Despite not
explicitlyrecognizingthephenomenonofhumaninternalstates,severalworksinRLHFincorporate
richerformsoffeedback. Forexample, (Wu et al.,2023)considershumanlabelingoversub-sections
of the text. In work on process supervision (Uesato et al., 2022; Lightman et al., 2023), humans
give feedback on intermediate steps. Motivated by these, our work is a first attempt at laying
the groundwork for a theoretical treatment of internal human states and intermediate feedback in
RLHF, using partially observed reward-states.
2dh istheeluderdimensionofarelevant functionclassanddC,h isitscovering dimension.
3Partial observability in RL The problem of partial observability in RL is not new. Al-
though learning in POMDPs Åström (1965) is known to be statistically intractable in gen-
eralKrishnamurthy et al.(2016);Jin et al.(2020),aflurryofrecentworkshavestudied POMDPs
under various structural assumptions Du et al. (2019); Liu et al. (2022a,b); Golowich et al. (2022);
Zhan et al. (2022); Cai et al. (2022); Chen et al. (2022a, 2023); Wang et al. (2023a); Zhong et al.
(2023). Our model is distinct from POMDPs since our results do not require the latent state
evolution to be Markovian,but assumes Markoviantransitions for observed states.
2 RL with Partially Observed Reward-States (PORRL)
Notation. We consider an episodic reinforcement learning setting where a learner interacts with
an MDP S,A,O,P where S is the state space, A its action space, O its observation space,
H its episp ode lengthq and P its transition dynamics. During time-step h H H of an
p
P Ă r s
episode, we make observations o O associated but possibly different to rewards r . A tra-
h h
P
jectory τ s ,a , ,s ,a is a sequence of states and actions, and we use the notation
1 1 H H
“ p ¨¨¨ q
τ h s ,a , ,s ,a to denote the subtrajectory of τ of length h. We denote the space
1 1 h h
r s “ p ¨¨¨ q
of trajectories as Γ and the space of sub-trajectories of length h as Γ .
h
We first intuitively describe how internal states should be incorporated into rewards with an
example, and use this to formally define cardinal PORRL and dueling PORRL.
2.1 Internal Reward-States and Cardinal PORRL
Consider the following model for how internal states could affect rewards, keeping in mind the
example of a human interacting with a language model. In this example, an action is a token,
the state is the text so far, and reward is some score representing the human’s satisfaction which
induces stochastic feedback. The internal states could be the human’s emotional reaction to the
text (happy,frustrated, amused,etc), or numbers in 0,1 encoding a confidence levelthatthe text
r s
is progressing towards a coherent response.
Anagentgoesthroughasequenceofstatesandactions,duetowhichthesystem(i.e. thehuman)
progresses through internal states u U, representing for example an emotion or confidence level.
P
WemodelthisbyassumingthatforthesetΓu ofhistoriesthatincludeinternalstates,definedby
h´1
τu h 1 : s ,u ,a h´1 thereisalatent-stategeneratorw :Γu S A ∆ U sothatthe
r ´ s “tp l l l qul“1 h h´1ˆ ˆ Ñ p q
human’s internal reward-stateu is sampled from the distribution defined by w τu h 1 ,s ,a .
h h h h
p r ´ s q
Thehuman’ssatisfactionr attime h shouldthenbe a functionofthe currentstate andaction,
h
but also the current internal state. This can be modeled as r s ,u ,a .
h h h h
p q
Thehumanthenemitsfeedbacko dependingontherewardr andperhapsalsothecurrentstate
h h
and action s ,a , where the latter two are the current text and token in our example. Typically,
h h
o will be 0,1 feedback reflecting whether the human says that they are satisfied or not. In
h
t u
general,this could be stochastic. For example,this couldbe Ber σ r for some function σ . So,
h h h
p p qq
o e r forsome distributione r . This leads to the generaldefinitionbelow,where wehave
h h h h h
„ p q p q
introduced new objects H ,U,w,e not seen in traditional RL:
p
Definition 2.1. A PORMDP M with cardinal feedback is a tuple S,A,U,P,H ,r,w,e , where:
p
p q
• S,A are fully observable states and actions, P s,a is a Markovian transition matrix, s S
1
is an initial state.3 p¨ | q P
3Recallthatchoosingaformalstates1 toserveasaplaceholderinitialstateisnotrestrictive.
4• H H is a set of timesteps where reward and feedback is obtained with size H p.
p p
Ăr s | |“
• U is a set of unobserved internal reward-states and r “tr h uhPH p so that r h :S ˆU ˆA ÑR is
the reward function at time h.
• w “tw h uhPH p so that w h :Γu h´1ˆS ˆA Ñ∆ pU q are internal reward-state functions that map
s,a,u histories to distributions over U.
p q
• mei es aa nt σupl re te fh ou rh aPH fup ns co tit oh nat σth :e Rfeedb Ra .c 4ko h „e h pr h qforanη h-subgaussiandistributione h with
h h h
p q Ñ
Inoursetting,ahistory-dependentpolicyπinteractswiththePORMDPMtoobserveahistory
τ h 1 Γ andtakesanactiona π τ h 1 ,s . Wethendonotobservetherewardr ,but
h´1 h h h
r ´ sP „ p r ´ s q
instead receive an observation o e r . Choose and fix some subclass Π of history-dependent
h h h
„ p q
policies. The total expected reward of a policy π Π is given by
P
V w M,π : E τu„Pw,π r h s h,u h,a h
p q “ » p qfi
h ÿPH p
– fl
Since the states u are never revealed, a learner interacting with M can only use τ h to make
r s
decisions. In this light, we show in Lemma A.1 that the information of w can be captured by
h
stochastic functions g : Γ ∆ U that marginalize w over the sequence u ,...u . That is,
h h h 1 h´1
given an s,a history τ h , wÑ e cap n dq efine5 g τ h u τ h . Now define
h h
p q r s p r sq„ | r s
V g pM,π q: “E τ„Pπ
»
E uh„ghpτrhsq rr h ps h,u h,a h
qsfi
h ÿPH p
– fl
Lemma A.1 then states that V M,π V M,π . So, for the purposes of value functions, M
w g
is fully specified by
S,A,U,P,Hp ,r,gq ,e“ andp hencq
eforth we replace w with g. Denote the value
p
functionV M,π byp V M,π . Definetheq optimalpolicyasπ : argmax V M,π . Consideran
g p q p q ‹ “ πPΠ p q
algorithmproducingasequenceofpolicies π ,...π Π, whereπ ischosenonly usingtrajectories
1 T t
P
τ t generated by π T . We measure the performance of such an algorithm by its cardinal
t i ui“1 t i ui“1
regret:
T
Regret T V M,π V M,π
‹ t
p q“ p q´ p q
t“1
ÿ
We make a few observations about our setting:
• It subsumes traditional RL by setting U , H H , O R, r s , ,a r s ,a
p h h h h h h
“ t‹u “ r s “ p ‹ q “ p q
and e r δ for all h. The setting also subsumes Efroni et al. (2021) by instead set-
h
p
h
q „
rh
ting H H . However, most traditional RL algorithms output (possibly time-dependent)
p
Markovia“ nt polu icies, and we show in Lemma A.2 that there are PORMDPs where any such
algorithmwillhavelinearregret. Thisemphasizestheimportanceofmodelinginternalreward-
states for alignment.
4ThisincludestheexampleofBernoullifeedbackinRLHF.
5Namely,defineghpτrhsqtobetheconditionalexpectation (viewedasarandomvariable)oftherandomvariable
whppτrh´1s,u1,...uh´1q,sh,ahq,conditioned onτrhs.
5• It subsumes the once-per-episode feedback model ofChatterji et al.(2021). Indeed, let H
p
H ,U R,O 0,1 ,letg τ betheDirac-δ distributionoverφ τ Jw,letr s,u,a “ u
H H
t u “ “t u p q p q p q“
and e r Ber µ r , where µ is the softmax function.
H
p q“ p p qq
• Our setting is strictly different from POMDPs . It is not a special case, since g might not
h
be a distribution generated by Markovian transitions s,u,a s1,u1. It is not more general,
sinces,a,s1 transitionsareMarkovianinoursetting.6 EvenfoÑ rmorespecializedPORMDPs
, modeling them as POMDPs can be wasteful, see Appendix A.3 for discussion.
2.2 Dueling PORRL
We now define dueling PORRL, where the motivation is the same as cardinal PORRL, except
the human is now presented with two trajectories and they provide feedback based on the pair
of trajectories. In most cases, this involves indicating a 0-1 preference between trajectories. We
then define the performance metric in this setting as dueling regret, which crucially needs both
trajectories presented for feedback to eventually become good, not just one of them.
Definition 2.2. A PORMDP M with dueling feedback is a tuple S,A,U,P,H ,r,w,e , where
p
everything is identical to a PORMDP with cardinal feedback, exp cept that every episoq de now
involves running two trajectories τ and τ that produce rewards r and r for h H , and
1 2 h,1 h,2 p
P
feedback is distributed as o e r r .
h h h,1 h,2
„ p ´ q
InduelingPORRL,weplayaduel byrunningtwopolicies π ,π Π Πinparalleltoobtain
1 2
p qP ˆ
t nr oa tje oc bt so er rie vs edpτ .1 T,τ h2
eq
a dn efid nr ie tc ioe niv se of fee Vdb Mac ,k πto ah
nu
dhPH πp. aA rega thin e, sn ao mte et ah sat inth ce arr de iw na ar ld Ps Oof Rt Rh Le .p Io fli wci ees pa lar ye
‹
p q
T duels π ,π ,... π ,π according to an algorithm, then we aim to minimize the dueling
1,1 2,1 1,T 2,T
p q p q
regret of this algorithm, which is given by
T V M,π V M,π
Regret T V M,π p 1,t q` p 1,t q
Dp q“ p ‹ q´ 2
t“1
ÿ
Remark 2.3. Note that dueling PORRL subsumes the setting of Pacchiano et al. (2021), which in
turn subsumes the feedback models of RLHF Wang et al. (2023b). However, crucially, Wang et al.
(2023b) measure performance using sample complexity and cardinalregret,not dueling regret. Du-
eling regret is important in online scenarios, when both options presented to the human must be
good.
2.3 Deterministic Internal Reward-State Functions
We now instantiatethe PORRL model ina statisticallytractablescenariothatstill subsumes most
existing work on RLHF. In this setting we assume that the internal reward-state functions g are
h
deterministic, and the feedback is emitted according to a Bernoulli distribution depending on the
reward. We will work under this assumption for Sections 3 and 4.
6This is reasonable since inthe language model example, given the text s and a new token a, the next state s1
ismerelythenewtext. Similararguments canbemadeformathematicalreasoningsettingsaswellasmanymotion
tasks.
6Assumption 2.4. We work in a known class P of transition kernels P. Let the reward function
r :S U A R be known for all h and r B. Let g be deterministic (but unknown) and
h h h
ˆ ˆ Ñ | |ď
belongingtoa knownclassof“decoderfunctions” G . LetO 0,1 andlete onlydependonthe
h h
“t u
rewards. For dueling feedback, let e r ,r be η -subgaussianwith mean σ r r . Also
h h,1 h,2 h h 1,h 2,h
assume that σ and σ´1 are Lipschitzp with Lipq schitz constants κ and κ respp ectiv´ ely. Cq all the
h h 1,h 2,h
resulting class of PORMDPs M.
Remark 2.5. NotethatthisstillsubsumesallsettingsdiscussedinSection2.1andRemark2.3. Note
thatthesoftmaxanditsinverseareLipschitzoverboundedsets,andthattheBernoullidistribution
is 1-subgaussian.
We also define a function class induced by r and G .
h h
Definition 2.6. Let us then consider the decoder-induced function classes F given by
h
F : f :Γ R g G s.t.
h h h h h
“ Ñ D P
! ˇ
f τ h r ˇs ,g τ h 1 ,a , τ
h p r sq“ h pˇ h h p r ´ sq h q @
)
Also define F : “ hPH pF h so that we have elements f “ tf h uhPH p P F. Finally, we define
V P,f,π to be the value function for policy π under transition model P and decoder-induced
p q ś
function f. Note that V pP,f,π q“E τPPπ hPH pf h pτ rh
sq
.
” ı
In the case of Chatterji et al. (2021),řF F φ τ Jw w W , where φ τ is an
H
“ “ t p q | } } ď u p q
arbitrary known function satisfying φ τ B for all τ.
} p q}ď
Further Notation. The space of P matrices that we work in is denoted P. We will denote the
spaceof PORMDPs MthatweworkinbyM. Wedenote the1 δ probabilityconfidencesetsfor
´
F inDefinition2.6byC F δ . ThecorrespondingconfidencesetsforF h fromthedefinitionareC h δ .
TheconfidencesetsforPp arq edenotedbyC P pδ q. LetN F h,B T, }¨}8 betheℓ 8 coveringnumberp oq f
F . Define d : log N F ,B, , then β δ O η d log 1 δ for β δ defined
h C,h “ p h T }¨}8 q h,t p q`“ p h C˘,h p { qq h,t p q
later.
` ˘ a
3 Optimistic Algorithms for Cardinal PORRL
We now describe two kinds of generic optimistic algorithms in our setting, and later instantiate
themwithspecificconfidencesetsandbonusestogetPOR-UCRLandPOR-UCBVI.Thesegeneric
descriptions will be used to describe the reduction from dueling to cardinal PORRL in Section 4.
Generic Confidence-Set Optimism. We presentatemplate to getregretbounds fora generic
optimistic algorithm using confidence sets, defined as follows. The algorithm is determined by
confidence sets C M D,δ C P D,δ C F D,δ based on a dataset D. Maintaining a running
p q “ p q ˆ p q
dataset D , at each step t, we run π given by
t t
π ,M : argmax V P,f,π
t t
“ πPΠ,MPCMpD t,δq p q
r
7We obtain a trajectory τ t
„
Pπ ‹t and append it to D t to get D t`1, recompute confidence sets
C M D t`1,δ , and continue. This algorithm is formally presented in Appendix B.1. We now make
p q
the following assumption about our confidence sets. It essentially controls the effect of shrinking
confidence sets for P and F on the value. Showing this assumption is the core of proving regret
bounds for any instantiation of this generic algorithm. We will see later that it is satisfied by the
confidence sets for POR-UCRL.
Assumption 3.1 (ControllingValueErrorduetoConfidenceSets). ForatransitionkernelP and
‹
function f , consider any sequence of policies π and datasets D that contain τ t generated
under pPπ ‹t‹ ,f‹ q. We require that M ‹
P
C M pD t,δt
q
for all t with pt robability 1 ´t δ {i 1u 6i“ . 1 We require
that there exist problem dependent functions C M,T,δ and C M,T,δ so that for arbitrary
P F
sequences P t C P D t,δ , fˆt C F D t,δ and ft p F, theq followingp hold wiq th probability 1 δ 2
P p q P p q P ´ {
each.
T
V P ,ft,π V P ,ft,π O C M,T,δ
t t ‹ t P
ˇ p q´ p qˇ“ p p qq
ˇt ÿ“1 ˇ
ˇ ˇ T ˇ ˇ r
ˇ V P ,ft,π V P ,f ,π ˇ O C M,T,δ
‹ t ‹ ‹ t F
ˇ p q´ p qˇ“ p p qq
ˇt ÿ“1 ˇ
ˇ ˇ ˇ ˇ r
Theorem 3.2 (Regretˇ for Confidence-Set Optimism).ˇ Under Assumption 3.1, any generic opti-
mistic algorithm using confidence sets C M D,δ satisfies the regret bound
p q
Regret T O C M,T,δ C M,T,δ
P F
p q“ p p q` p qq
GenericBonus-BasedOptimism Wrepresentatemplatetogetregretboundsforagenericopti-
misticalgorithmusingbonuses,definedasfollows. SuchanalgorithmreliesonbD P,π,δ ,bD P,π,δ
P F
p q p q
that depend on a policy π, transition kernel P and dataset D. It also relies on estimates Pˆ D
and fˆ D that depend on D. Maintaining a running dataset D t, at each step t, we run π t :
argmax πPΠV pPˆ D t,fˆ D t,π q, where V pPˆ D t,fˆ D t,π
q
is given by: “
r V pPˆ D t,fˆ D t,rπ q`bD Ft pPˆ D t,π,δ q`z pBp qbD Pt pPˆ D t,π,δ
q
wherez isdefinedbelow. Weobtainatrajectoryτ t „Pπ ‹t andappendittoD t togetD t`1,compute
new bonuses and estimates, and continue. This algorithm is formally presented in Appendix B.2.
We now make the following assumptionabout our bonuses. Showing this assumption is the core of
proving regret bounds for any instantiation of this generic algorithm. We will see later that it is
satisfied by the bonuses for POR-UCBVI.
Assumption3.3(ControllingValueErrorviaBonuses). ForatransitionkernelP andfunctionf ,
‹ ‹
Wco ens ri ed qe ur ia reny ths ae tqu foe rnc se eqo uf ep no cl ei sci Pe ˆs Dπ
t
aa nn dd fˆd Dat aa nse dts anD
yt
st eh qa ut ec no cn eta fi tn tτ
Fi u
,t
i t“ h1
eg fe on lle or wat ined
g
u hn old de .r pPπ ‹t,f‹ q.
t t P
• Bounding effect of error in F: With probability 1 δ 32, for any P and uniformly over all
p tho ali tcies Tπ, b|DV tpP P,f ,ˆ D πt ,, δπ q´ OV p CP,f M‹,π ,q T| ,ď
δ
bD F wt ip tP h,π p, roδ´ q baa bn{ id litt yh 1ere i δs 3a 2function C F pM,T,δ q so
t“1 F p ‹ t q“ p F p qq ´ {
ř
r
8• BoundingeffectoferrorinP: Foranyfunctionµ:Γ RboundedbyD,thereisafunction
H
Ñ
z D D so that the following holds uniformly over all policies π with probability 1 δ 32.
p qě ´ {
E τ„pPˆ Dtqπµ pτ q´E τ„Pπ ‹µ pτ qďz pD qbD Pt pP ‹,π,δ
q
The statement also holds if we switch P ‹ and Pˆ D t. Additionally, the statement holds for a
suitableD ifwereplaceE τ„Pπµ τ withbP P,π,δ orbF P,π,δ .7 Finally,thereisafunction
C M,T,δ so that T bD t Pp ,q π ,δ Op C Mq ,T,δp withq probability 1 δ 32.
P p q t“1 P p ‹ t q“ p P p qq ´ {
ř
Theorem 3.4 (Regret for Bonus-BasedOptimisr m). Under Assumption 3.3, with z D z D
1
p q“ p q`
z 2D z 2z D , any generic optimistic algorithm using bonuses satisfies
p q` p p qq
Regret T O C M,T,δ z Bp C M,T,δ
F 1 P
p q“ p p q` p q p qq
3.1 POR-UCRL and PORr-UCBVI
We now set the stage for our optimistic algorithms, POR-UCRL and POR-UCBVI. While we use
least squares loss here, which works for all subgaussian feedback distributions, we can used spe-
cialized losses like cross-entropyloss for Bernoulli distributions and produce analogous guarantees.
Given a dataset of the first t trajectory samples τ t and an index h H , we consider the
t i ui“1 P r s
following least squares objective to estimate f:
t
ft`1 argmin σ f τ h oi 2
h “ fhPF
h i“1
h p h p i r sqq´ h
ÿ` ˘
p
Simple least squares guarantees imply the lemma below.
Lemma 3.5 (Concentration for σ f ). Define
h
˝
t
MSE f ,f1 : σ f τ h σ f1 τ h 2
h,t p h hq “ h p h p r sqq´ h p hp r sqq
i“1
ÿ` ˘
Then f‹ simultaneously satisfies the following for all h,t with probability 1 δ 32.
h ´ {
δ
MSE f‹,fpt`1q β¯
h,t p h h qď h,t 2t2H
ˆ ˙
p
where for with α
h,t
:
“
tB`tηh Tlog pδt
q, we define
N F ,B,
β¯ δ 4 η2log h T }¨}8 α
h,t p q“ g f h ˜ ` δ ˘¸` h,t
f
e
7ThiswouldinstantlyholdwithD“BpifbFpP,π,δq:“E τ„PπbFpτ,δqforsometrajectory levelbonusbFpτ,δq,
andsimilarlyforP.
9POR-UCRL. We instantiate the generic optimistic algorithm using confidence sets by defining
C M D t,δ : C Pt δ C Ft δ using C Pt δ ,C Ft δ defined below. We name the resulting algorithm
PORp -UCq RL“ . Firsp t,q lˆ et Ct`p1q δ Cpt`q1 δ p wq ith Ct`1 δ defined below:
F p q“ h h p q h p q
Ct`1 δ : ś f F MSE f‹,fpt`1q β δ
h p q “ h P h h,t p h h qď h,t p q
! ˇ )
where β h,t pδ
q
:
“
β¯ h,t 2tδ 2H . We also useˇ ˇ the MLE epstimate for P after t episodes to define
Pˆt p¨|s,a q:
“
N Ntp ts p, sa ,a,s q1q.`Now˘for ζ pn,δ q“2 Slogp2q`log 2pn npn`1qSA{δq, define C Pt pδ
q
as below:
b
P P s,a Pˆ s,a ζ N s,a ,δ s,a
t 1 t
} p¨| q´ p¨| q} ď p p q q@
! ˇ )
ˇ
Theorem3.6 (POR-UCRLˇ Regret). UnderAssumption2.4, POR-UCRLsatisfiesAssumption3.1
anditsregretRegret T isboundedbythefollowingwithprobabilityatleast1 δ,ignoringpolynomial
p q ´
terms independent of T.
O BpS?HA κ d β δ ?T
2,h h h,T
¨¨ ` p q˛ ˛
h ÿPH p b
r˝˝ ‚ ‚
where d dim F ,B .
h “ E h T
` ˘
POR-UCBVI. We now instantiate the generic optimistic algorithm using bonuses by defining
therequiredestimatesandbonuses. TheresultingalgorithmisPOR-UCBVI.Setfˆ D : fˆt defined
a sub po ev re s, cl re ipt tPˆ DD t b: “
y
jPˆ ut stb te .t Nhe owM fL orE δ¯e :stimat δe o ,f wth ee detr fia nn es :ition matrix. For bonuses, wt e“ replace the
t “ SHAH
γ τ h ,δ : max f τ h f1 τ h
h,t p r s q “ fh,f h1PC ht pδ¯
q
h p r sq´ hp r sq
bt F P,π,δ : min 2Bp,E τ„Pπ γ h,t τ h ,δ
p q “ ¨ » p r s qfi˛
h ÿPH p
˝ H´–1 fl‚
bt P P,π,δ : min 4,E τ„Pπ ξt s h,a h,δ
p q “ ˜ « p qff¸
h“1
ÿ
for ξt s,a,δ H`S (precise definition in Appendix D). We can obtain computationally
p q « Ntps,aq
tractablebonusesbγ τ,δ undermanymodels. InChatterji et al.(2021),itisgivenbysup φ τ J w
H,t
p q
w,w1PWt
p q p ´
w1 for an ellipsoid confidence set Wt defined by them. This is clearly computable.
q
Theorem 3.7 (POR-UCBVI Regret). Under Assumption 2.4, POR-UCBVI satisfies Assump-
tion 3.3 and its regret Regret T is bounded by the following with probability at least 1 δ, ignoring
p q ´
polynomial terms independent of T.
O BpC H,S,A κ d β δ ?T
2,h h h,T
¨¨ p q` p q˛ ˛
h ÿPH p b
r˝˝ ‚ ‚
10where C H,S,A : p H?SA S?HA . Also note that d β δ O η Hd d in our
h h,T h h C,h
p q “ p ` q p q “
case.
a ` a ˘
r
Remark 3.8. Note that in Chatterji et al. (2021), d ,d 0 for all h H and d d d.
h C,h H C,H
“ ă “ “
We thus recover their guarantees up to an extra ?H in d?HT.
4 Reducing Dueling To Cardinal PORRL
The dueling and cardinal feedback models are intimately related. It is thus tempting to use al-
gorithms for cardinal PORRL to solve dueling PORRL. However, we detail why the "obvious"
reductionfromduelingfeedbackto cardinalfeedback fails. This bothdemonstratesthe hardnessof
the problem and motivates our reduction.
4.1 The Naive Reduction Always Fails
Consider a modified PORMDP M with S : S S, A : A A, P : P P, where we run
“ ˆ “ ˆ “ b
the pair of policies π : π ,π and obtain observations based on the decoder-induced function
1 2
f τ h ,τ h : f τ“ hp fq τ h . Consider the space of all such PORMDPs induced by
Mhp , a1 nr ds de2 nr ots eq it“ byh Mp 1 .r Ss iq n´ ce ch ap rd2 ir nas lq feedback in M exactly corresponds to dueling feedback in
M, it is tempting to restrict to searching over Π Π and run any algorithm for cardinal PORRL
on this modified PORMDP M to achieve low duˆ eling regret.
Thisfailsbecausethefeedbackmodelandregretmetricarefundamentallynon-alignedindueling
feedback, unlike in cardinal feedback. While the agent receives dueling feedback over the duel for
π ,π , dueling regret is instead concerned with duels for π ,π and π ,π . Running an
1,t 2,t ‹ 1,t ‹ 2,t
p q p q p q
algorithmforcardinalPORRLonthemodifiedMDPwillmaximizetheduelingfeedback itself. This
is achieved by playing one good and one really bad policy, unlike the two good policies needed for
low dueling regret. We formalize this in Lemma A.3, showing that the naive reduction leads to
lineardueling regretforany PORMDP andany cardinalPORRL algorithmwithsublinearregret.
4.2 Reducing Dueling to Optimistic Cardinal PORRL
Thenaivereductionfailsbecausemaximizingduelingfeedbackcanleadtobadpoliciesbeingplayed.
In this subsection, we presenta white-box reductionwhere we ensure that we only play potentially
goodpolicies for both π and π . We detail here how we can obtain analgorithmfor the dueling
1,t 2,t
feedback problem from any optimistic algorithm for cardinal PORRL. The key insight is to use
confidencesetsfromcardinalPORRLtosearchforπ andπ onlyamongpoliciesthatboth have
1,t 2,t
a chance of being optimal. Then one plays the most uncertain duel among all possible choices for
π and π . This generalizes and abstracts out ideas in Pacchiano et al. (2021), which presents
1,t 2,t
a specific algorithm to achieve low dueling regret in their model. We present the reduction to
optimism over confidence sets, the version for bonuses is in Appendix E.2. Define
V M,π,π1 V M,π V M,π1
D
p q“ p q´ p q
We compute the confidence sets C P D,δ as the image of C P D,δ under P P. We compute
C F pD,δ q by treating to h uhPH p as cap rdinaq l feedback in M. Ap s anq example, fÞÑ or POR-UCRL, we
perform a least squares fit for f and use Lemma 3.5 to define our confidence sets again.
11Algorithm 1 Reduction from Dueling to Cardinal Confidence-Set Optimism
1: Input Known reward function tr h uH h“1, method to compute C M pD,δ qØC P pD,δ qˆC F pD,δ q.
2: Initialize dataset D 1 , C M D 1,δ : P F
Ðtu p q “ ˆ
3: for t 1,...,T do
“
4: Compute good set Π t {Valid π ‹ candidates}
Π : π Π M C D ,δ s.t.
t M t
“ P D P p q
! ˇ
V M,ˇπ,π 0 π Π
p ˇ 1 qě @ 1 P
)
5: Pick π 1,t,π 2,t given by {Most uncertain duel}
p q
argmax max V M,π,π1 V M1 ,π,π1
D D
π,π1PΠt M,M1 PC MpD t,δq p q´ p q
H
6: Collect trajectories τ i,t “ pst i,h,at i,hqq h“1 along with feedback to h uhPH p by sampling from
Pπi,t for i 1,2. ! )
‹
“
7: Append the data to D t to get D t`1.
8: Compute C P D t`1,δ , C F D t`1,δ .
p q p q
9: end for
Theorem 4.1 (ReductionfromDuelingtoConfidence-Set-BasedOptimism). If the confidencesets
C M pD t,δ
q
satisfy Assumption 3.1, then the dueling regret Regret DpT
q
of Algorithm 1 is given by
Regret T O C M,T,δ C M,T,δ
Dp q“ p P p q` F p qq
Remark 4.2. While the C
F
term dependrs on M instead of M, we show in Lemma E.1 that
dim F ,ε 9dim F ,ε 2 ,andsimilarresultscanbeshownforothernotionsofdimension. So,
E h E h
p qď p { q
we can often instantly import guarantees for C M,T,δ when computing C M,T,δ , since C
F F F
p q p q
oftendepends onsuchdimensionsoffunctionclasses. SeeCorollariesE.2 and E.4 forinstantiating
this using POR-UCRL and POR-UCBVI.
Proof Idea. Whilethedetailsarecomplex,thecorepunchlineissimpleandmotivatesouralgorithm
design cleanly. For ease of notation, let us also use the sets C M D t,δ given by the pre-image of
C M D t,δ under the map M M. We first recall that by Assup mptioq n 3.1, M ‹ C M D t,δ and
p q ÞÑ P p q
so π Π for all t with probability 1 δ 16. Now, since π Π for i 1,2, there exists some
‹ t i,t t
M i,t P C M D t,δ fori 1,2sothatV D´M i{ ,t,π,π 1,t V M i,t,πP V M i,t“ ,π i,t 0forallπ. Now
P p q “ p q“ p q´ p qď
12note that 2Regret T is given by the following:
Dp q
T 2
V M ,π ,π
D ‹ ‹ i,t
p q
t“1i“1
ÿ ÿ
T 2 2
V M ,π ,π V M ,π ,π V M ,π ,π
D ‹ ‹ i,t D i,t ‹ i,t D i,t ‹ i,t
“ p q´ p q` p q
t ÿ“1”i ÿ“1 i ÿ“1 ı
2 T
piq
V M ,π ,π V M ,π ,π
D ‹ ‹ i,t D i,t ‹ i,t
ď p q´ p q
i“1t“1
ÿ ÿ“ ‰
T
piiq
2U π ,π
t 1,2 1,2
ď p q
t“1
ÿ
where U
t
pπ,π1
q
:
“
maxM,M1PCMpD
t,δq
V
D
pM,π,π1 q´V
D
pM1 ,π,π1
q
is the uncertainty in the duel
π,π1 at time t. Inequality i holds”by the argument about M i,tıabove. Inequality ii holds by
p q p q p q
definition of π ,π as the most uncertain duel at time t. This is the only point where we use
1,t 2,t
p q
the details of the algorithm. We can now make clever arguments using Assumption 3.1 to bound
the sum of these uncertainties over confidence sets C M D t,δ .
p q
5 PORRL with Recursive Reward-State Dynamics
In previous sections, we show how the PORMDP model allows to handle cardinal and dueling
feedback through optimistic algorithms that lead to provable regret guarantees in those settings.
Notably, we considered an extremely general model in which the distribution of u , determined
h
by g : Γ ∆ U , need not be related to the distribution of u . In relevant applications,
h h h´1
Ñ p q
the dynamics of the internal state may be recursive in nature. Even in the example of a human
interacting with a language model, it is not unreasonable to assume that their internal states
transition in a Markovian fashion. Using the knowledge of such recursive dynamics in algorithm
designcanimprovebothstatisticalandcomputationalefficiency. We canformalizethis bydefining
a sub-class of PORMDPs .
Definition 5.1. A PORMDP M with recursive dynamics is a PORMDP where grec : ∆ U
h p qˆ
S A ∆ U , h H , is a recursivereward-statefunction suchthat g τ h grec g τ h
ˆ Ñ p q @ P p h p r sq„ h p h´1 p r ´
1 ,s ,a .
h´1 h´1
sq q
Computational Efficiency. OneexistingexampleofaPORMDP withrecursivedynamicsis
themodelofRLwithonce-per-episode-feedbackundersum-decomposability,definedinChatterji et al.
(2021). Their UCBVI algorithm is not computationally efficient in general, and the core rea-
son is that planning under non-Markovian feedback can be hard. They show that assuming
sum-decomposability of trajectory features allows them to design a dynamic-programming based
planning procedure, making their algorithm computationally efficient. In their setting, u
1
“
φ s ,a Jw and u u φ s ,a Jw for some unknown w.
1 1 1 h h´1 h h h
p q “ ` p q
Statistical Efficiency. Consider the problem of learning perfect reward machines, studied by
Icarte et al. (2019). A perfect reward machine is an MDP augmented with reward states u U
P
that transition according to u δ u ,L s ,a ,s for some δ and L. Consider reward
h u h´1 h´1 h´1 h u
“ p p qq
13functions r s ,u ,a , and let feedback be produced by adding subgaussian noise. We subsume
h h h h
p q
this setting,butshowinLemma A.4thatusingthe recursivestructureofaperfectrewardmachine
can lead to an exponential improvement in regret guarantees.
6 Conclusions and Future Work
In this work, we have introduced PORMDPs and their analysis as a way to better model human
internal states in RLHF. We have introduced several statistically efficient algorithms for handling
partially observed reward-states, and have shown that existing models for once-per-episode and
duelingfeedbackaresubsumedbythisnew framework. We believethatapartfromouralgorithmic
contributions,amajortakeawayofthisworkistheconceptualintroductionofPORRL.Wehopeto
lay the groundwork for further theoretical understanding of the statistical limits of learning good
policies when interacting with stateful agents such as humans. We also think that this framework
can be useful in improving our understanding of partial feedback mechanisms, such as process
supervision and fine-grained feedback.
14References
Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Al-
tenschmidt, J., Altman, S., Anadkat, S., et al. (2023). Gpt-4 technical report. arXiv preprint
arXiv:2303.08774.
Akrour, R., Schoenauer, M., and Sebag, M. (2012). April: Active preference learning-based rein-
forcement learning. In European Conerence on Machine Learning and Knowledge Discovery in
Databases.
Askell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann,
B.,DasSarma,N.,etal.(2021). Agenerallanguageassistantasalaboratoryforalignment. arXiv
preprint arXiv:2112.00861.
Åström, K. J. (1965). Optimal control of Markov processes with incomplete state information.
Journal of Mathematical Analysis and Applications, 10(1):174–205.
Bai, Y., Jones, A., Ndousse, K., Askell, A., Chen, A., DasSarma, N., Drain, D., Fort, S., Ganguli,
D., Henighan, T., et al. (2022). Training a helpful and harmless assistant with reinforcement
learning from human feedback. arXiv preprint arXiv:2204.05862.
Brown, D., Goo, W., Nagarajan, P., and Niekum, S. (2019). Extrapolating beyond suboptimal
demonstrationsviainversereinforcementlearningfromobservations. InInternationalConference
on Machine Learning.
Busa-Fekete, R. and Hüllermeier, E. (2014). A survey of preference-based online learning with
bandit algorithms. In Algorithmic Learning Theory.
Cai, Q., Yang, Z., and Wang, Z. (2022). Reinforcement learning from partial observation: Linear
functionapproximationwithprovablesampleefficiency. In International Conference on Machine
Learning.
Chan, J., Pacchiano, A., Tripuraneni, N., Song, Y. S., Bartlett, P., and Jordan, M. I. (2021).
Parallelizing contextual linear bandits. arXiv preprint arXiv:2105.10590.
Chatterji, N., Pacchiano, A., Bartlett, P., and Jordan, M. (2021). On the theory of reinforcement
learningwith once-per-episodefeedback. In Advances in Neural Information Processing Systems.
Chen, F., Bai, Y., and Mei, S. (2022a). Partially observable rl with b-stability: Unified struc-
tural condition and sharp sample-efficient algorithms. In International Conference on Learning
Representations.
Chen, F., Wang, H., Xiong,C., Mei, S., andBai,Y. (2023). Lowerbounds forlearningin revealing
pomdps. In International Conference on Machine Learning.
Chen, X., Zhong, H., Yang, Z., Wang, Z., and Wang, L. (2022b). Human-in-the-loop: Provably
efficient preference-based reinforcement learning with general function approximation. In Inter-
national Conference on Machine Learning.
Christian,B.(2020). The alignment problem: Machine learning and human values. WW Norton&
Company.
15Christiano, P. F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D. (2017). Deep
reinforcement learning from human preferences. In Advances in Neural Information Processing
Systems.
Du, S., Krishnamurthy, A., Jiang, N., Agarwal, A., Dudik, M., and Langford, J. (2019). Provably
efficient rl with rich observations via latent state decoding. In International Conference on
Machine Learning.
Efroni, Y., Merlis, N., andMannor,S. (2021). Reinforcementlearningwith trajectoryfeedback. In
AAAI conference on artificial intelligence.
Flavell, S. W., Gogolla, N., Lovett-Barron, M., and Zelikowsky, M. (2022). The emergence and
influence of internal states. Neuron.
Golowich,N., Moitra,A., andRohatgi,D. (2022). Learninginobservablepomdps, withoutcompu-
tationally intractable oracles. In Advances in Neural Information Processing Systems.
Hendrycks,D.,Carlini,N.,Schulman,J.,andSteinhardt,J.(2021). Unsolvedproblemsinmlsafety.
arXiv preprint arXiv:2109.13916.
Icarte,R.T.,Waldie,E.,Klassen,T.,Valenzano,R.,Castro,M.,andMcIlraith,S.(2019).Learning
rewardmachinesforpartiallyobservablereinforcementlearning. InAdvances in Neural Informa-
tion Processing Systems.
Ji,J.,Qiu,T.,Chen,B.,Zhang,B.,Lou,H.,Wang,K.,Duan,Y.,He,Z.,Zhou,J.,Zhang,Z.,etal.
(2023). Ai alignment: A comprehensive survey. arXiv preprint arXiv:2310.19852.
Jin,C.,Kakade,S.,Krishnamurthy,A.,andLiu,Q.(2020). Sample-efficientreinforcementlearning
of undercomplete pomdps. In Advances in Neural Information Processing Systems.
Jin, C., Liu, Q., and Miryoosefi, S. (2021). Bellman eluder dimension: New rich classes of rl prob-
lems, and sample-efficient algorithms. In Advances in Neural Information Processing Systems.
Krishnamurthy, A., Agarwal, A., and Langford, J. (2016). Pac reinforcement learning with rich
observations. In Advances in Neural Information Processing Systems.
Lee,K.,Smith,L.,andAbbeel,P.(2021).Pebble: Feedback-efficientinteractivereinforcementlearn-
ing via relabeling experience and unsupervised pre-training. arXiv preprint arXiv:2106.05091.
Leike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S. (2018). Scalable agent
alignment via reward modeling: a researchdirection. arXiv preprint arXiv:1811.07871.
Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J.,
Sutskever, I., and Cobbe, K. (2023). Let’s verify step by step. arXiv preprint arXiv:2305.20050.
Liu,Q.,Chung,A.,Szepesvári,C.,andJin,C.(2022a). Whenispartiallyobservablereinforcement
learning not scary? In Conference on Learning Theory.
Liu, Q., Netrapalli, P., Szepesvari, C., and Jin, C. (2022b). Optimistic mle–a generic model-based
algorithm for partially observable sequential decision making. arXiv preprint arXiv:2209.14997.
16Novoseller, E., Wei, Y., Sui, Y., Yue, Y., and Burdick, J. (2020). Dueling posterior sampling for
preference-based reinforcement learning. In Conference on Uncertainty in Artificial Intelligence.
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S.,
Slama, K., Ray, A., et al. (2022). Training language models to follow instructions with human
feedback. In Advances in Neural Information Processing Systems.
Pacchiano, A., Saha, A., and Lee, J. (2021). Dueling rl: reinforcement learning with trajectory
preferences. arXiv preprint arXiv:2111.04850.
Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., and Finn, C. (2023). Di-
rect preference optimization: Your language model is secretly a reward model. arXiv preprint
arXiv:2305.18290.
Russo, D. and Van Roy, B. (2013). Eluder dimension and the sample complexity of optimistic
exploration. InBurges,C.,Bottou,L.,Welling,M.,Ghahramani,Z.,andWeinberger,K.,editors,
Advances in Neural Information Processing Systems.
Sadigh,D., Dragan,A. D., Sastry,S., andSeshia, S.A. (2017). Active preference-basedlearningof
reward functions. Technical report, EECS Berkeley.
Saha, A., Pacchiano, A., and Lee, J. (2023). Dueling rl: Reinforcement learning with trajectory
preferences. In International Conference on Artificial Intelligence and Statistics.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy
optimization algorithms. arXiv preprint arXiv:1707.06347.
Shin,D.,Dragan,A.D.,andBrown,D.S.(2023). Benchmarksandalgorithmsforofflinepreference-
based reward learning. arXiv preprint arXiv:2301.01392.
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D., Lowe, R., Voss, C., Radford, A., Amodei, D., and
Christiano, P. F. (2020). Learning to summarize with human feedback. In Advances in Neural
Information Processing Systems.
Swamy,G.,Dann,C.,Kidambi,R.,Wu,Z.S.,andAgarwal,A.(2024). Aminimaximalistapproach
to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056.
Szepesvári, C. (2023). Lecture notes in theoretical foundations of reinforcement learning: Lecture
23, tabular mdps. https://rltheory.github.io/lecture-notes/online-rl/lec23/.
Uesato, J., Kushman, N., Kumar, R., Song, F., Siegel, N., Wang, L., Creswell, A., Irving, G., and
Higgins,I.(2022). Solvingmathwordproblemswithprocess-andoutcome-basedfeedback. arXiv
preprint arXiv:2211.14275.
Wang,L.,Cai,Q.,Yang,Z.,andWang,Z.(2023a). Representtocontrolpartiallyobservedsystems:
Representationlearningwithprovablesampleefficiency. InInternationalConference on Learning
Representations.
Wang, Y., Liu, Q., and Jin, C. (2023b). Is rlhf more difficult than standard rl? arXiv preprint
arXiv:2306.14111.
17Wirth, C., Akrour, R., Neumann, G., Fürnkranz, J., et al. (2017). A survey of preference-based
reinforcement learning methods. Journal of Machine Learning Research, 18(136):1–46.
Wu, Z., Hu, Y., Shi, W., Dziri, N., Suhr, A., Ammanabrolu, P., Smith, N. A., Ostendorf, M.,
andHajishirzi, H.(2023). Fine-grainedhumanfeedback givesbetter rewardsfor languagemodel
training. arXiv preprint arXiv:2306.01693.
Xue,W.,Cai,Q.,Xue,Z.,Sun,S.,Liu,S.,Zheng,D.,Jiang,P.,Gai,K.,andAn,B.(2022).Prefrec:
Recommendersystemswithhumanpreferencesforreinforcinglong-termuserengagement. arXiv
preprint arXiv:2212.02779.
Zhan, W., Uehara, M., Sun, W., and Lee, J. D. (2022). Pac reinforcement learning for predictive
state representations. In International Conference on Learning Representations.
Zhan, W., Uehara, M., Sun, W., and Lee, J. D. (2023). How to query human feedback efficiently
in rl? arXiv preprint arXiv:2305.18505.
Zhong, H., Xiong, W., Zheng, S., Wang, L., Wang, Z., Yang, Z., and Zhang, T. (2023). Gec: A
unified framework for interactive decision making in mdp, pomdp, and beyond. arXiv preprint
arXiv:2211.01962.
Ziegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D., Christiano, P.,
and Irving, G. (2019). Fine-tuning language models from human preferences. arXiv preprint
arXiv:1909.08593.
18A Lemmas and Discussion
A.1 Relation between V and V
w g
Lemma A.1 (Replacing w with g). For any history-dependent policy π that selects an action
a π τ h 1 ,s , the following holds for any M.
h h
„ p r ´ s q
V M,π V M,π
w g
p q“ p q
Proof. By a slight abuse of notation, the following chain of equalities holds. Here, i holds since
p q
r s ,u ,a is a function of s ,u ,a . Equation ii holds since we have already conditioned on
h h h h h h h
p q p q
τ h , which includes s ,a . Equation iii holds by the definition of g τ h as the conditional
h h h
r s p q p r sq
distribution of u given τ h .
h
r s
V w M,π E τu„Pw,π r h s h,u h,a h
p q“ » p qfi
h ÿPH p
E τu„– Pw,π r h s h,u h,a h fl
“ r p qs
h ÿPH p
E τurhs„Pw,π r h s h,u h,a h
“ r p qs
h ÿPH p
E τrhs„Pπ E τurhs„Pw,π r h s h,u h,a h τ h
“ r p qs r s
h ÿPH p ” ˇ ı
ˇ
p “iq E τrhs„Pπ E uh,sh,ah„Pw,π rr h ps h,u h,a hˇ
qs
τ rh
s
h ÿPH p ” ˇ ı
ˇ
p “iiq E τrhs„Pπ E uh„Pw,π rr h ps h,u h,a h
qs
τ rh sˇ
h ÿPH p ” ˇ ı
ˇ
pi “iiq E τrhs„Pπ E uh„ghpτrhsq rr h ps h,u h,aˇ h
qs
h ÿPH p
“ ‰
“
E τ„Pπ E uh„ghpτrhsqrr h ps h,u h,a h
qs
h ÿPH p “ ‰
“E τ„Pπ
»
E uh„ghpτrhsqrr h ps h,u h,a h
qsfi
h ÿPH p
V M,π– fl
g
“ p q
A.2 Ignoring Internal Reward-States is Bad for Alignment
WedefinetraditionalRLmethodsasthosethatoutputpossiblytime-dependentMarkovianpolicies.
In this section, we provide a toy example showing that there is a PORMDP with good sublinear
regret guarantees where any time-dependent Markovian policy has value bounded away from the
maximumvalue. This meansthat traditionalRL methods willalwaysincur linearregret. We hope
that this illustrates that RL methods that ignore internal reward-states can be bad for alignment.
19Lemma A.2 (Markovianpolicies are not enough). There is a PORMDP where POR-UCRL and
POR-UCBVI achieve poly H,S,A ?T regret, but any Markovian policy is at least 1-suboptimal.
p q 4
Proof. ConsideraPORMDP Minthe settingofChatterji et al.(2021)describedinSection2and
set S s ,s , A a ,a and w 1 R. Let the transition matrix be P s1 s,a 1 for all
“ t 1 2 u “ t 1 2 u “ P p | q “ 2
s,a,s1. Let the starting state always be s .
1
Consider the set T of all trajectories that have a until s appears, and then only have a .
2 2 1
Choose φ τ 1 τ T . The best non-Markovian policy π can follow this rule and achieve
‹
φ τ 1p forq a“ ll τp PPπ‹.q Thus, max πPΠV M,π 1, where Π is given by all history-dependent
p q “ „ p q “
policies.
Ontheotherhand,consideraMarkovianbutpotentiallytime-dependentpolicyπ. Ifπ a 0,
2
p q“
thenitsvalueiszero. Ifπ a 0,thenconditionedontheeventthats appearsfirstattimestep1,
2 2
p qą
the expected total rewardis at most π a 1 π a . Conditioned on the event that s appears
1 2 2 2 2
p qp ´ p qq
first at timestep 2, the expected total reward is at most π a π a . Conditioned on seeing s at
1 2 2 2 2
p q p q
or after h 3, the expected total reward is certainly at most 1. Using these crude inequalities, we
“
can bound the expected reward of a Markovianpolicy π by
π a 1 π a π a π a H 1 π a 2 π a 1 1 1 3
1 2 2 2 1 2 2 2 1 1 2 2
p qp ´ p qq p q p q p qp ´ p qq
2 ` 4 ` 2h ď 4 ` 4 ď 2 ` 4 “ 4
h“3
ÿ
This means that the value of any time-dependent Markovian policy is at most 3 and so any time-
4
dependent Markovianpolicy is at least 1-suboptimal.
4
RecallthatwedefinedtraditionalRLalgorithmsasthosethatoutput(possiblytime-dependent)
Markovianpolicies. Clearly, any traditional RL algorithm in this sense will have at least T regret,
4
which is linear regret.
A.3 Comparing POMDPs and PORMDPs
ContinuingthediscussioninSection2.1,notethatanaiveapproachwillinvolvemakingthehidden
state the entire history τ h 1 , which creates a hidden state space of size exponentialin H. Even
a different attempt to mor de´ l PsORMDPs using POMDPs will still have to ensure that there are
at least SU states while the number of observable states is S. This would be the overcomplete
POMDP setting, which is relatively understudied. For example, the work of Liu et al. (2022a)
gives the first sample efficient algorithm for overcomplete POMDPs , and they assume that the
reward is only a function of observed states. We crucially require the reward to depend on the
unobserved states as well.
Finally, note the following example: consider the cardinal feedback setting of Chatterji et al.
(2021), which is detailed in Section 2.1. Now choose a random vector in Rd for each trajectory τ
anddefineφ τ tobethisvector. Apriori,thereisnoclearwaytomodelthisasaPOMDPwithout
p q
carrying the entire history in the latent state until the very last stage, when it is replaced by φ τ
p q
or φ τ Jw. However, the corresponding Eluder dimension is d, giving us ?dHT regret. However,
anyp atq tempt to view this as a POMDP and explicitly hold the history in the latent state will lead
to a state space exponentialin H. So, even a PORMDP with deterministic internal reward-states
and a very simple definition of the internal reward-state and reward model can have trouble being
cast as a POMDP .
20In the very special case of a recursive structure with deterministic dynamics, we can let S U
be the total state space, let S be the observable states and we can cast this as a POMDP wˆ ith
transitions P s ,u s ,u ,a P s s ,a 1 u grec u ,s ,a . This is only
slightlymoreh gp enh e` r1 althh` an1 a| ph erfeh ctrh eq wa“ rdmp ah c` h1 in| eah ssoch iq atp edh t` o1 a“POh MDp Ph ,ih nthh aqq
titstransitions
are Markovianbut time-dependent, not stationary.
A.4 The Naive Reduction from Dueling to Cardinal PORRL Fails
Lemma A.3 (NaiveReductionLowerBound). Using any algorithm for cardinal PORRLwith sub-
linear cardinal regret on M¯ with policy class Π1 : Π Πto get a sequence π ,π ... π ,π
1,1 2,1 1,T 2,T
leads to linear dueling regret for M whenever al“ l poliˆ cies π do not have thp e same vaq lue Vp M,π . q
p q
Proof. Define π : argmax V M,π and let π : argmin V M,π . Then note that
‹ “ πPΠ p q min “ πPΠ p q
max V M,π,π1 maxV M,π V M,π1
D
π,π1PΠ p q“ π,π1 p q´ p q
max V M,π V M,π1
“π,π1PΠ p q´ p q
maxV M,π max V M,π1
“ πPΠ p q`π1PΠ ´ p q
maxV M,π minV“ M,π1 ‰
“ πPΠ p q´π1PΠ p q
V M,π V M,π
‹ min
“ p q´ p q
UnderthenaivereductioninSection4,acardinalPORRLalgorithmisusedtomaximizedueling
feedback. If the algorithm has sublinear cardinal regret, then it will produce duels π ,π ,t
1,t 2,t
p q “
1 T, satisfying
Ñ
T
max V M,π,π1 V M,π ,π o T
D D 1,t 2,t
π,π1PΠ p q´ p q“ p q
t“1
ÿ
From above, this means that
T
V M,π V M,π V M,π V M,π o T
‹ 1,t 2,t min
r p q´ p qs`r p q´ p qs“ p q
t“1
ÿ
Now note that by definition of π and π , both terms are positive. This is the key point. We
‹ min
thus have
T
V M,π V M,π o T
‹ 1,t
p q´ p q“ p q
t“1
ÿ
T
V M,π V M,π o T
2,t min
p q´ p q“ p q
t“1
ÿ
21This means that for dueling regret Regret T , we have the following.
Dp q
T
Regret T V M,π V M,π V M,π V M,π
Dp q“ r p ‹ q´ p 1,t qs`r p ‹ q´ p 2,t qs
t“1
ÿ
T
V M,π V M,π V M,π V M,π
‹ 1,t ‹ min
“ r p q´ p qs`r p q´ p qs
t“1
ÿ
T
V M,π V M,π
min 2,t
` r p q´ p qs
t“1
ÿ
o T T V M,π V M,π
‹ min
“ p q` r p q´ p qs
Θ T
“ p q
Wherethelastlineholdssinceallpoliciesπ donothavethesamevalueV M,π ,andsoV M,π
‹
V M,π 0. p q p q´
min
p qą
A.5 Gap in Guarantees for a Family of Reward Machines with Stochastic
Feedback
LemmaA.4(GapinGuaranteesforRewardMachines). ThereisafamilyRMofrewardmachines
that induces a corresponding family of decoder-induces functions F so that:
• The Eluderdimension d of F is AH, giving an unsatisfying POR-UCRLand POR-UCBVI
H H
guarantee exceeding ?AHT.
• A specialized algorithm levergaing the recursive structure achieves AHlog T regret
2p q
Proof. We define a family of perfect reward machines with reward states U 0,1,...H,H 1
“ t ` u
and the system always starts in state 0. Let S and A is a finite set. The observed state
“ t‹u
always transitions from to with probability 1. Also, we define r ,u,a Ber
1pu‰0q
for
‹ ‹ h p‹ q “ 2
all reward machines in the family. That is, the reward is 0 in state 0 and Ber 1 ´2 other¯wise.
p { q
Finally, we parametrize the family using a sequence of actions of length H. That is, for every
sequence a ,...a , we define a reward machine in the family as follows. h
ah
h 1 and any
1 H
p q ÝÑ `
other transition takes u to 0 for any u. This family of reward machines is then in bijection with
sequences a ,...a AH.
1 h
Nownop te thatfoq rP anysuchrewardmachine, f τ 11 τ a ,...a . Thatis, a ,...a
H p q“ 2 p “p 1 H qq p 1 H q
servesas a combinationlock and any other actiononthe way canleadto the sink state u 0 with
“
0 reward. We know (see Russo and Van Roy (2013), for example) that if a function class contains
D indicators, then its Eluder dimension is at least D. So, the Eluder dimension of F is at least
H
D and the first item in our claim follows from Theorems 3.6 and 3.7.
We nowdescribe analgorithmto learnthe rewardmachine by explicitly leveragingits recursive
structure. First, take all actions K times and estimate the averagerewardof each action. There is
only one action that can lead to a non-zero feedback signal, namely a (which is unknown). The
1
probability of getting a zero signal from a on all the K tries is 1 . In the event that this does
1 2K
not happen, we have determined a . If all rewards are zero, stop and quit and just play a random
1
sequence of actions. Now we define our procedure recursively as follows, indexed by timestep h. If
atanyindexh,allrewardsatharezero,stopandquitandstartplayingarandomactionsequences.
22Otherwise, play the inferred action sequences a ,...a and then play each action K times at the
1 h
h 1 th step.
p ` q
Let us analyse the expected regretof this algorithm. The probability of quitting is bounded by
H using a union bound. In that case, we will incur linear regret. If we don’t quit, then for the
2K
first AKH episodes, we incur regret of 1 2 at each episode, following which we incur zero regret.
{
The final expected regret is bounded by
AHK T H
Regret T
p qď 2 ` 2 2K
ˆ ˙
We can pick K : log T . This gives us that
“ 2p q
Regret T O AHlog T
p q“ p 2p qq
23B Proofs for General Optimistic Algorithms for Cardinal PORRL
B.1 Generic Confidence-Set Optimism
We first formally describe a generic optimistic algorithm using confidence sets.
Algorithm 2 Generic Confidence-Set Optimism
1: Input Known reward function tr h uH h“1, known model class M induced by known probability
transition kernel class P and known decoder-induced function class F, confidence level δ.
2: Initialize dataset D 1 and C M D 1,δ M.
Ðtu p qÐ
3: for t 1,...,T do
“
4: Compute the optimistic history dependent policy,
π ,M argmax V M,π
t t
“ π, MPCMpD t,δq p q
r
5: Collect trajectory τ t “tpst h,at hquH h“1 and feedback to h uhPH p by sampling from Pπ ‹t with true
decoder-induced function f .
‹
6: Update D t`1 D t τ t and compute new confidence set C M D t`1,δ .
Ð Yt u p q
7: end for
Theorem 3.2 (Regret for Confidence-Set Optimism). Under Assumption 3.1, any generic opti-
mistic algorithm using confidence sets C M D,δ satisfies the regret bound
p q
Regret T O C M,T,δ C M,T,δ
P F
p q“ p p q` p qq
Proof. Let M
t
be given by P
t
and ft. Nrote the following inequalities, where i holds with proba-
p q
bility 1 by the optimistic definition of π .
t
r r r
T
Regret T V P ,f‹,π V P ,f‹,π
‹ ‹ ‹ t
p q“ p q´ p q
t“1
ÿ
T
piq
V P ,ft,π V P ,f‹,π
t t ‹ t
ď p q´ p q
t“1
ÿ
T r r
V P ,ft,π V P ,ft,π V P ,ft,π V P ,f‹,π
t t ‹ t ‹ t ‹ t
ď p q´ p q` p q´ p q
t“1
ÿ pIq pIIq
r r r r
looooooooooooooooomooooooooooooooooon looooooooooooooooomooooooooooooooooon
WenowapplyAssumption3.1tobound I and II . WecanusetheassumptionsinceD contains
t
p q p q
trajectories tτ i ut i“1 generated by Pπ ‹t, ft P C F pD t,δ q Ă F and Pt P C F pD t,δ q. This immediately
gives us that with probability 1 δ
´
r r
Regret T O C M,T,δ C M,T,δ
F P
p q“ p p q` p qq
as desired. r
24B.2 Generic Bonus-Based Optimism
Algorithm 3 Generic Bonus-BasedOptimism
1: DIn ,p bu ot nuK sn fo uw nn ctir oe nw sa brd Df Pu ,n πct ,i δon as nt dr h buDH h“ P1 ,, πm ,δet ,ho cod nE fis dt ep nD ceq lt eo vee lst δimate Pˆ D and fˆ D from dataset
F P
p q p q
2: Initialize D 1 Ðtu, initialize fˆD1,Pˆ D1 arbitrarily.
3: for t 1,...,T do
“
4: Compute optimistic history dependent policy,
π t “arg πmaxV pPˆ D t,fˆ D t,π q`bD Ft pPˆ D t,π,δ q`z pBp qpbD Pt pPˆ D t,π,δ
qq
5: Collect trajectory τ t “tpst h,at hquH h“1 and feedback to h uhPH p by sampling from Pπ ‹t,f‹.
6: Obtain new estimates fˆD t`1,PˆD t`1 Est D t`1 and compute new bonus functions
b FD t`1 fˆD t`1, ,δ ,bD Pt`1 PˆD t`1, ,δ . Ð p q
p ¨ q p ¨ q
7: end for
Theorem 3.4 (Regret for Bonus-BasedOptimism). Under Assumption 3.3, with z D z D
1
p q“ p q`
z 2D z 2z D , any generic optimistic algorithm using bonuses satisfies
p q` p p qq
Regret T O C M,T,δ z Bp C M,T,δ
F 1 P
p q“ p p q` p q p qq
fP ˆ Dro to Pf. FN io st ce omth pa ut tw edec ua sn inu gs De tA as ns dum Pˆ Dpt trio isn c3 o. m3 ps uin tc ee dD ust inco gn Dta ti .n As lt sr oaj ne oc tt eor ti he as tt Wτ i u Lt i“ O1 Gg ,e bn D Pe tr pa Pte ,d π,b δy qP ďπ ‹t 2,
alwaysholdssincewecanotherwiseclipitat2andourassumptionwillstillhold. Similarly,WLOG
bD t P,π,δ 2z Bp , otherwise we can clip it at 1 and our assumption will still hold. Now note
F
p q ď p q
25the following inequalities.
T
Regret T V P ,f‹,π V P ,f‹,π
‹ ‹ ‹ t
p q“ p q´ p q
t“1
ÿ
T
p ďiq V pPˆ D t,f‹,π ‹ q`z pBp qpbD Pt pPˆ D t,π ‹,δ qq´V pP ‹,f‹,π t
q
t“1
ÿ
T
p ďiiq V pPˆ D t,fˆ D t,π ‹ q`bD Ft pPˆ D t,π ‹,δ q`z pBp qpbD Pt pPˆ D t,π ‹,δ qq´V pP ‹,f‹,π t
q
t“1
ÿ
T
pi ďiiq
V pPˆ D t,fˆ D t,π t q`bD Ft pPˆ D t,π t,δ q`z pBp qpbD Pt pPˆ D t,π t,δ qq´V pP ‹,f‹,π t
q
t“1
ÿ
T
“
V pPˆ D t,fˆ D t,π ‹ q´V pP ‹,f‹,π t q`bD Ft pPˆ D t,π t,δ q`z pBp qpbD Pt pPˆ D t,π t,δ
qq
t“1
ÿ
T
“
V pPˆ D t,fˆ D t,π ‹ q´V pPˆ D t,f‹,π ‹ q`V pPˆ D t,f‹,π ‹ q´V pP ‹,f‹,π t
q
t“1
ÿ
T
`
bD Ft pPˆ D t,π t,δ q`z pBp qpbD Pt pPˆ D t,π t,δ
qq
t“1
ÿ
Here,inequality i holdswithprobability1 δ 16bythesecondpointinAssumption3.3. Inequality
p q ´ {
ii holdswithprobability1 δ 16by the firstpointinAssumption3.3. Inequality iii holdswith
p q ´ { p q
probability 1 by the optimistic definition of π . Continuing, we have
t
T
Regret pT qp ďivq 2 bD Ft pPˆ D t,π t,δ q`z pBp qpbD Pt pPˆ D t,π t,δ
qq
t“1
ÿ
T
pvq
2 bD t P ,π ,δ 2z Bp bD t P ,π ,δ z Bp bD t P ,π ,δ 2z z Bp bD t P ,π ,δ
F ‹ t P ‹ t P ‹ t P ‹ t
ď p q` p qp p qq` p qp p qq` p p qqp p qq
t“1
ÿ
T
O bD t P ,π ,δ z Bp bD t P ,π ,δ
F ‹ t 1 P ‹ t
“ ˜ p q` p qp p qq¸
t“1
ÿ
Here, inequality iv holds with probability 1 δ 8 by a union bound overthe first and the second
p q ´ {
point in Assumption 3.3. Finally, inequality v holds with probability 1 δ 4 by a union bound
p q ´ {
over four applications of the second point of Assumption 3.3. Finally, we use a union bound over
both points of Assumption 3.3 to conclude that with probability 1 δ 8
´ {
T
O bD t P ,π ,δ z Bp bD t P ,π ,δ O C M,T,δ z Bp C M,T,δ
F ‹ t 1 P ‹ t F 1 P
˜ p q` p qp p qq¸“ p p q` p q p qq
t“1
ÿ
By taking a union bound over the events of all inequalrities above, we have that with probability
1 δ
´
Regret T O C M,T,δ z Bp C M,T,δ
F 1 P
p q“ p p q` p q p qq
as desired.
r
26C Details and Proofs for Cardinal POR-UCRL
Recallthatgivenadatasetofthefirstttrajectorysamples τ t andanindexh H ,weconsider
t i ui“1 Pr s
the following least squares objective to estimate f:
t
ft`1 argmin σ f τ h oi 2
h “ fhPF
h i“1
h p h p i r sqq´ h
ÿ` ˘
Simple least squares guarantepes imply the lemma below.
Lemma C.1 (Concentration for σ f ). Define
h
˝
t
MSE f ,f1 : σ f τ h σ f1 τ h 2
h,t p h hq “ h p h p r sqq´ h p hp r sqq
i“1
ÿ` ˘
Then f‹ simultaneously satisfies the following for all h,t with probability 1 δ 32.
h ´ {
δ
MSE f‹,fpt`1q β¯
h,t p h h qď h,t 2t2H
ˆ ˙
where for with α
h,t
:
“
tB`tηh Tlog pδt
q, we
definp
e
N F ,B,
β¯ δ 4 η2log h T }¨}8 α
h,t p q“ g f h ˜ ` δ ˘¸` h,t
f
e
Proof. We apply Lemma 6 in Chan et al. (2021) and the last statement in its proof to each h
separately with the function class in the lemma set to σ f f F , P 1, x x τ h
h h h h t,p t,1 t
t ˝ | P u “ “ “ r s
and misspecification ε 0 (decoupled from the Eluder dimension’s ε). We also note that ot are
“ h
η -subgaussian samples with mean σ f τ h . This gives us that each of event indexed by h,t
h h h
below holds with probability at least
1p p δr s .qq
´
2t2H
t σ f τ h σ f1 τ h 2 β¯ δ
h p h p i r sqq´ h p hp i r sqq ď h,t 2t2H
i“1 ˆ ˙
ÿ` ˘
So, the events all simultaneously hold with probability at least 1 δ by a union bound.
´
Recall the definition of our confidence sets below.
Confidence Sets for POR-UCRL. We instantiate the generic optimistic algorithm using con-
fidence sets by defining C M D t,δ : C Pt δ C Ft δ as our confidence sets below. We name the
p q “ p qˆ p q
resulting algorithm POR-UCRL. We use the data from trajectories τ t to build the confidence
sets Ct`1 δ Ct`1 δ with Ct`1 δ defined below, where β δt :i ui β“ ¯1 δ .
F p q“ h h p q h p q h,t p q “ h,t 2t2H
ś Ct`1 δ : f F MSE f‹,fpt`1q β δ ` ˘
h p q “ h P h h,t p h h qď h,t p q
! ˇ )
We also use the MLE estimate for P afteˇr t episodespto define Pˆt s,a : Ntps,a,s1q. Now for
ˇ p¨| q “ Ntps,aq
ζ n,δ 2 Slogp2q`logpnpn`1qSA{δq, define Ct δ as below:
p q“ 2n P p q
b
P P s,a Pˆ s,a ζ N s,a ,δ s,a
t 1 t
} p¨| q´ p¨| q} ď p p q q@
! ˇ )
ˇ
ˇ
27Confidence Sets for POR-UCRL in case P is known. For known-model UCRL, the confi-
‹
dence sets Ct δ are still as above, but Ct δ : P
F P ‹
p q p q “t u
C.1 Showing that Assumption 3.1 is Satisfied
C.1.1 Bounding Reward Model Deviations
sL ae tim sfm yina gC f.2 (Bo Bun td hi an tg iR ndew uca erd vaM luo ede ful nD ce tv ioia nt sio Vns) P. ,fC ,o πns .id Fe or rd aec no yde sr e- qi un ed nu cc eed offu pn oc lt ii co ien ss πtf ,h u ih fP tH hp
e
h t
| | ď p q
confidence C Ft pδ
q
is generated using data τ
i
„Pπ ‹i,i “1 Ñt and ft PC Ft pδ
q
is an arbitrary sequence
of functions, then we have the following with probability 1 δ 4.
´ {
r
T
V P ,ft,π V P ,f‹,π O Bp T log T δ Bκ d κ d β δ T
‹ t ‹ t 2,h h 2,h h h,T
ˇ ˇt ÿ“1 p q´ p qˇ ˇď ¨ a p { q` h ÿPH p ` h ÿPH p b p q ˛
ˇ ˇ r ˇ ˇ ˝ ‚
Pˇ roof. ˇ
T T H H
V pP ‹,ft,π t q´V pP ‹,f‹,π t
q“
E τ„Pπ ‹t
«
f ht pτ rh sqff´E τ„Pπ ‹t
«
f h‹ pτ rh
sqff
t“1 t“1 h“1 h“1
ÿ ÿ ÿ ÿ
r T r
“
E τ„Pπ ‹t
»
f ht pτ rh sqfi´E τ„Pπ ‹t
»
f h‹ pτ rh
sqfi
t ÿ“1 h ÿPH p h ÿPH p
– r fl – fl
T
“
E τ„Pπ ‹t
»
f ht pτ rh sq´f h‹ pτ rh
sqfi
t ÿ“1 h ÿPH p
– r fl
T
ft τ h f‹ τ h X X
“ » hp t r sq´ hp t r sq` 1,t ` 2,t fi
t ÿ“1 h ÿPH p
– r fl
T
piiq
ft τ h f‹ τ h O Bp T log T δ
ď » hp t r sq´ hp t r sqfi` p { q
t ÿ“1 h ÿPH p ´ a ¯
– r fl
where
X 1,t : “E τ„Pπ
»
f ht pτ rh
sqfi´»
f ht pτ t rh
sqfi
h ÿPH p h ÿPH p
– r fl – r fl
X 2,t :
“»
f h‹ pτ t rh sqfi´E τ„Pπ
»
f h‹ pτ rh
sqfi
h ÿPH p h ÿPH p
– fl – fl
Inequality i followsby the definitionofπ andft – that is,by optimism. Inequality ii holds
p q t h p q
withprobabilityatleast1 δ sinceX andX arebothmartingaleswithrespecttothefiltration
1,t 2,t
G
t
given by the data of t´ rajectories tτ
s
ut s´ “1 1. Also,r |X
1,t
|, |X
2,t
| ď
Bp. We can thus apply the
Azuma-Hoeffding inequality twice to obtain inequality ii .
p q
28Continuing, note the following.
T
V P ,ft,π V P ,f‹,π
‹ t ‹ t
p q´ p q
t“1
ÿ
H r
ft τ h f‹ τ h Bp T log T δ
ď« hp t r sq´ hp t r sqff` p { q
h“1
ÿ a
H
κ σ ft τ h κ σ f‹ τ h Bp T log T δ
ď« 2,h h p hp t r sqq´ 2,h h p hp t r sqqff` p { q
h“1
ÿ a
T
κ max σ f τ h σ f1 τ h Bp T log T δ
ď 2,h
t ÿ“1h ÿPH
pfh,f h1PW htpδq h p h p t r sqq´ h p hp t r sqq`
a
p { q
“:γ¯h,tpτtrhs,δq
Tloooooooooooooooooooooooooomoooooooooooooooooooooooooon
κ γ¯ τ h ,δ Bp T log T δ
2,h h,t t
“ h ÿPH p« t ÿ“1 p r s qff`
a
p { q
The sum of these maximum uncertainty evaluations can be upper bounded using the Eluder
dimension. The inequality below holds by applying Lemma 3 in (Chan et al., 2021) for each h
separately,with the function classin the lemma set to σ f f F , P 1,x x τ h
h h h h t,p t,1 t
t ˝ | P u “ “ “ r s
and misspecification ε 0 (decoupled from the Eluder dimension’s ε). We also recall that ot are
“ h
η -subgaussian samples with mean σ f τ h . We obtain
h h h t
p p r sqq
T
γ¯ τ h ,δ O Bd d β δ T
h,t t h h h,T
p r s qď ` p q
t ÿ“1 ˆ b ˙
Where d dim F ,B is the Eluder dimension of F and β δ β¯ δ . Therefore,
h “ E h T h h,T p q“ h,t 2t2H
we have our result.
` ˘ ` ˘
T
V P ,ft,π V P ,f‹,π O Bκ d κ d β δ T Bp T log T δ
‹ t ‹ t 2,h h 2,h h h,T
p q´ p qď ¨ ` p q ` p { q˛
t ÿ“1 h ÿPH p h ÿPH p b a
r ˝ ‚
Note that this entire argument can be repeated with f and ft switched, by the symmetry of the
‹
definition of γ¯ τ h ,δ and the fact that the negative of a martingale is also a martingale.
h,t t
p r s q
r
C.1.2 Bounding Probability Model Deviations
Lemma C.3 (Bounding Probability Model Deviations). Consider an arbitrary sequence of func-
tions ft F satisfying f B that induce value functions V P,f,π . For any sequence of policies
h
P | |ď p q
π t, if the confidence C Pt pδ
q
is generated using data that includes τ
i
„Pπ ‹i,i “1 Ñt and P
t
PC Pt pδ
q
is an arbitrary sequence of transition structures, then we have the following with probability 1 δ 4.
´ {
r
T
V P ,ft,π V P ,ft,π O c Bp?SAHT c BpHSA Bp pTlog T δ
t t ‹ t δ δ
ˇ p q´ p qˇď ` ` p { q
ˇt ÿ“1 ˇ ´ a ¯
whereˇ ˇc : 8r Slog 2 log HTSAˇ ˇδ .
ˇ δ “ p q` p {ˇ q
a
29Proof. We first show the following.
Lemma C.4.
T T H
V P ,ft,π V P ,ft,π 2pζ N st,at ,δ O Bp pT log T δ
p t t q´ p ‹ t qď p t p h hq q` p { q
t ÿ“1 t ÿ“1h ÿ“1 ´ a ¯
r
Recall that we denote the Bellman operator by Tπ where Tπf E Pf. Momentarily define
a„π
“
the following for τ τ ,s ,τ1 , where τ is an arbitrary trajectory of length l 1 H, and s
l´1 l l´1 l
“p q ´ ď
is an arbitrary state.
H
V lt ,P pτ l´1,s l q: “E τ1„Pπt
«
f ht pτ rh
sqff
h“l
ÿ
r
“E τ1„Pπt
»
f ht pτ rh
sqfi
hPH ÿp,hěl
– r fl
Sointhedefinitionabove,thefirstl 1observationsinτ comefromτ whiletherestaregenerated
l´1
by the input P starting with state s´ . Note that V P,ft,π Vt ,s . Also note that by the
l
p
t
q “
1,P
pH
1
q
Bellman equation, we have the following.
V lt
,P
pτ
t
rl ´1 s,s
l
q“E
a„πt
f lt pτ
t
rl
sq
`E
a„πt
E s1„Pp¨|sl,aqV lt
`1,P
ppτ
t
rl ´1 s,s,a q,s1
qq
“E a„πt“f lt pτ t rl sq‰ `E a„πt“P p¨|s l,a qJV lt `1,P ppτ t rl ´1 s,s l,a q,
¨q
‰
Now use τ to set τ : τ l “ 1 and‰ define the“ following. ‰
t l´1 t
“ r ´ s
∆t s : Vt τ l 1 ,s V τ l 1 ,s
lp l q “ l,P ‹p t r ´ s l q´ l,P tp t r ´ s l q
Note that r
∆t s V P ,ft,π V P ,ft,π (1)
1p 1 q“ p t t q´ p ‹ t q
The computation above then gives us the following.
r
∆t st E P st,a JVt τ l 1 ,st,a , E P st,a JVt τ l 1 ,st,a ,
lp lq“ a„πt t p¨| l q l`1,P tpp t r ´ s l q ¨qq ´ a„πt ‹ p¨| l q l`1,P ‹pp t r ´ s l q ¨qq
P s”t,at JVt τ l , P st,at JVtı τ l ,“ Y Z ‰
“ t p¨| lr lq l`1,P tp t r sr ¨q´ ‹ p¨| l lq l`1,P ‹p t r s ¨q` l,t ` l,t
where Y l,trand Z
l,t
are stochrastic processes defined below.
Y : P st,a JVt τ l , E P st,a JVt τ l 1 ,st,a ,
l,t “ ‹ p¨| l q l`1,P ‹p t r s ¨q´ a„πt ‹ p¨| l q l`1,P ‹pp t r ´ s l q ¨q
Z : E P st,a JVt τ l 1“,st,a , P st,a JVt τ l , ‰
l,t “ a„πt t p¨| l q l`1,P tpp t r ´ s l q ¨q ´ t p¨| l q l`1,P tp t r s ¨qq
” ı
Consider the filtrationrG induced byrthe data of τ t´1 τrl 1 st . Srince at π and
τ l 1 ,st,at τ l ,l, wt e get that E Y G tEs u Zs“1 GY t r ´ 0.sY Sot , olu ne can seel t„ hatt both
p t r ´ s l lq “ t r s r l,t | l,t s “ r l,t | l,t s “
30processes are martingales over G . Also note that Y , Z p. We thus have that
l,t l,t l,t
| | | |ď
∆t st P st,at P st,at Vt τ l ,
lp lq“ t p¨| l lq´ ‹ p¨| l lq l`1,P tp t r s ¨qq
” ı
r `P ‹ p¨|st l,at lqJ V lt `1,P tpτ t rl s, ¨qqr ´V lt `1,P ‹pτ t rl s, ¨q `Y l,t `Z l,t
” ı
“
P
t
p¨|st l,at lq´P
‹
p¨|st l,rat
lq
V lt
`1,P
tpτ
t
rl s, ¨qq`P
‹
p¨|st l,at lqJ∆t l`1ps1 q`Y
l,t
`Z
l,t
” ı
“ P rt p¨|st l,at lq´P ‹ p¨|st l,at lq V lt `1,Pr tpτ t rl s, ¨qq`E s1„P ‹p¨|st l,at lq ∆t l`1ps1 q `Y l,t `Z l,t
“” P rt p¨|st l,at lq´P ‹ p¨|st l,at lqı V lt `1,Pr tpτ t rl s, ¨qq`∆t l`1pst l`1q`“U l,t `Y l,t‰ `Z l,t
” ı
where r r
U l,t : “E s1„P ‹p¨|st l,at lq ∆t l`1ps1 q ´∆t l`1pst l`1q
Consider the filtration G¯ defined by the data o“f τ t´1‰ τ l . Clearly, U is a martingale over
G¯ . Also note that U l,t p To conclude, we havt e ts hus a“ t1Y t r s l,t
l,t l,t
| |ď
∆t st ∆t st P st,at P st,at Vt τ l , U Y Z
lp lq´ l`1p l`1q“ t p¨| l lq´ ‹ p¨| l lq l`1,P tp t r s ¨qq` l,t ` l,t ` l,t
” ı
Using a telescoping sum over lrfor a fixed t and equation 1, wre get that for any t, the following
holds.
V P ,ft,π V P ,ft,π
t t ‹ t
p q´ p q
∆t s
“r 1p 1 q
H
P st,at P st,at Vt τ l , U Y Z
“ t p¨| l lq´ ‹ p¨| l lq l`1,P tp t r s ¨qq` l,t ` l,t ` l,t
l ÿ“1” ı
r r
V P ,ft,π V P ,ft,π
t t ‹ t
p q´ p q
H
r p P st,at P st,at U Y Z
ď t p¨| l lq´ ‹ p¨| l lq 1` l,t ` l,t ` l,t
l ÿ“1 › ›
H ›r ›
› ›
Bp P st,at Pˆ st,at Bp P st,at Pˆ st,at U Y Z
ď t p¨| l lq´ t p¨| l lq 1` ‹ p¨| l lq´ t p¨| l lq 1` l,t ` l,t ` l,t
l ÿ“1 › › › ›
›r › › ›
› › › ›
Untilnow,allstatements haveheldwithprobability1anddidnotuse anyfacts aboutP . Now,
t
note the following well known concentration lemma. See, for example, Szepesvári (2023).
r
Lemma C.5. For ζ n,δ 8 Slogp2q`logpnpn`1qSA{δq and
p q“ 2n
b
Ct δ P P s,a Pˆ s,a ζ N s,a ,δ s,a
P t 1 t
p q“ } p¨| q´ p¨| q} ď p p q q@
! ˇ )
the true model P‹ C Pt δ for alˇ ˇl t 1 with probability at least 1 δ 32.
P p q ě ´ {
31Applying the lemma twice and applying a union bound imply that the following holds with
probability 1 δ 8.
´ {
T
V P ,ft,π V P ,ft,π
t t ‹ t
p q´ p q
t“1
ÿ
piq H r
2Bpζ N st,at ,δ U Y Z
ď p t p l lq q` l,t ` l,t ` l,t
l“1
ÿ
T H T
2Bpζ N st,at ,δ U Y Z
“ p t p h hq q`» h,t ` h,t ` h,t fi
t ÿ“1h ÿ“1 t ÿ“1h ÿPH p
T H – fl
piiq
2Bpζ N st,at ,δ O Bp pT log T δ
ď p t p h hq q` p { q
t ÿ“1h ÿ“1 ´ a ¯
Note that inequality i is subtle since we could have used more data than that from τ ,i 1 t
i
p q “ Ñ
to construct Ct . The inequality still holds since ζ n,δ is decreasing in n. Also, inequality ii
P
p q p q
holds by the Azuma-Hoeffding inequality.
Now note that the whole argument above can be repeated with P and P switched, since the
‹ t
negative of a martingale is also a martingale. So, we have that with probability 1 δ 4
´ {
r
T T H
V P ,ft,π V P ,ft,π 2Bpζ N st,at ,δ O Bp pTlog T δ
ˇ p t t q´ p ‹ t qˇď p t p h hq q` p { q
ˇt ÿ“1 ˇ t ÿ“1h ÿ“1 ´ a ¯
ˇ r ˇ
ˇ ˇ
Finˇally, we need the following easyˇ lemma, proved in Szepesvári (2023).
Lemma C.6. Let c : 8 Slog 2 log HTSA δ . Then the following holds almost surely.
δ
“ p q` p { q
a
T H
2Bpζ N st,at ,δ c Bp?SAHT c BpHSA
p t p h hq qď δ ` δ
t“1h“1
ÿ ÿ
This establishes our claim.
C.2 Putting It All Together
Theorem3.6 (POR-UCRLRegret). UnderAssumption2.4, POR-UCRLsatisfiesAssumption3.1
anditsregretRegret T isboundedbythefollowingwithprobabilityatleast1 δ,ignoringpolynomial
p q ´
terms independent of T.
O BpS?HA κ d β δ ?T
2,h h h,T
¨¨ ` p q˛ ˛
h ÿPH p b
r˝˝ ‚ ‚
where d dim F ,B .
h “ E h T
` ˘
32Proof. We can now combine Lemmas 3.5and C.5 to conclude that M ‹ C M D t,δ for all t with
P p q
probability1 δ 16. WecannowcombinethisobservationwithLemmasC.2andC.3toobservethat
´ {
Assumption 3.1 is satisfied by POR-UCRL. By Theorem 3.2, the following holds with probability
1 δ.
´
Regret T O c Bp?SAHT c BpHSA Bκ d κ d β δ T
δ δ 2,h h 2,h h h,T
p q“ ¨ ` ` ` p q ˛
h ÿPH p h ÿPH p b
˝ ‚
where c 8 Slog 2 log HTSA δ , d dim F ,B is the Eluder dimension of F and
δ “ p q` p { q h “ E h T h
β δ β δ . This is because all the terms dependent on p get absorbed by the first term
h,T p q“ h,t 2at2H ` ˘
in our expression below. We thus have our result.
` ˘
We further refine it by ignoring terms independent of T to get
Regret T O BpS?AHT κ d β δ T
2,h h h,T
p q“ ¨ ` p q ˛
h ÿPH p b
r˝ ‚
Also note the following theorem.
Theorem C.7 (POR-UCRL Regretif P is Known). If we know the transition matrix P in POR-
‹ ‹
UCRL, then our regret is given by the following with probability 1 δ, ignoring polynomial terms
´
independent of T.
Regret T O Bp κ d β δ ?T
2,h h h,T
p q“ ` p q
ˆˆ b ˙ ˙
Proof. We can now use Lemmas C.2 anrd the fact that Ct δ is always a singleton to observe that
P
p q
Assumption 3.1 is satisfied by this version of POR-UCRL as well. By Theorem 3.2, the following
holds with probability 1 δ.
´
Regret T O Bp?T Bκ d κ d β δ T
2,h h 2,h h h,T
p q“ ¨ ` ` p q ˛
h ÿPH p h ÿPH p b
r˝ ‚
We further refine it by ignoring terms independent of T to get
Regret T O Bp κ d β δ ?T
2,h h h,T
p q“ ¨¨ ` p q˛ ˛
h ÿPH p b
r˝˝ ‚ ‚
33D Details and Proofs for Cardinal POR-UCBVI
WenowdescribehowweinstantiatePOR-UCBVIfromagenericoptimisticalgorithmusingbonuses.
Given a dataset of the first t trajectory samples τ t and an index h H , we consider the
t i ui“1 P r s
following:
Estimates for POR-UCBVI:
t
ft`1 argmin σ f τ h oi 2
h “ fhPF
h i“1
p h p i r sqq´ h
ÿ` ˘
p
We also use the MLE estimate for P after t episodes to define Pˆt s,a : Ntps,a,s1q. Now for
p¨| q “ Ntps,aq
ζ n,δ 2 Slogp2q`logpnpn`1qSA{δq, define Ct δ as below:
p q“ 2n P p q
b
P P s,a Pˆ s,a ζ N s,a ,δ s,a
t 1 t
} p¨| q´ p¨| q} ď p p q q@
! ˇ )
Recall the definition of oˇur bonus below.
ˇ
Bonusesfor POR-UCBVI. Recallthatsimpleleastsquaresguaranteesimplythelemmabelow.
Lemma D.1 (Concentration for σ f ). Define
h
˝
t
MSE f ,f1 : σ f τ h σ f1 τ h 2
h,t p h hq “ h p h p r sqq´ h p hp r sqq
i“1
ÿ` ˘
Then f‹ simultaneously satisfies the following for all h,t with probability 1 δ 32.
h ´ {
δ
MSE f‹,fpt`1q β¯
h,t p h h qď h,t 2t2H
ˆ ˙
p
where for with α
h,t
:
“
tB`tηh Tlog pδt
q, we define
N F ,B,
β¯ δ 4 η2log h T }¨}8 α
h,t p q“ g f h ˜ ` δ ˘¸` h,t
f
e
We use the data from trajectories τ t to build the confidence sets Ct`1 δ Ct`1 δ
with Ct`1 δ defined below, where β t δ i u :i“1 β¯ δ . F p q “ h h p q
h p q h,t p q “ h,t 2t2H ś
` ˘
Ct`1 δ : f F MSE f‹,fpt`1q β δ
h p q “ h P h h,t p h h qď h,t p q
! ˇ )
We first define a trajectory dependent boˇ ˇnus term beplow, with δ¯: δ
“ HSHAH
γ τ h ,δ max f τ h f1 τ h
h,t p r s q“ fh,f h1PC ht pδ¯
q
h p r sq´ hp r sq
34Note that according to the definition of β, this does not create any exponential dependence in
the confidence intervals used to define Ct`1.
h
δ
β 16 log N F ,α, αtB αtηHlog tHSA δ
h,t 16SHAH “ p p p h }¨}8 qq` ` p { qq
ˆ ˙
It follows by a union bound over all trajectory segments and all timesteps t that with probability
at least 1 δ 16 and for any trajectory τ and t 1,h H ,
p
´ { ě P
f‹ τ h fˆt τ h γ τ h ,δ (2)
| hp r sq´ hp r sq|ď h,t p r s q
Remark D.2. In the case of many popular function classes F, like the linear class F τ
H
“ t ÞÑ
φ τ Jw w W , we can compute γ τ h ,δ quite easily. In this case γ is given by
h,t H,t
p q |} }ď u p r s q
w,s wu 1Pp Wtφ pτ qJ pw ´w1 q“}φ pτ q}Vt}w,s wu 1Pp Wt}w ´w1 }V t´1
for a suitable quadratic form V .
t
γ τ h ,δ induces a trajectory-dependent bonus, given by
h,t
p r s q
bt τ,δ : γ τ h ,δ
F h,t
p q “ p r s q
h ÿPH p
This in turn induces a policy-level bonus (which depends on the transition kernel), given by:
bt F pP,π,δ q: “E τ„Pπ
‹
bt F pτ,δ
q
“E τ„Pπ
»
γ h,t pτ rh s,δ
qfi
“ ‰ h ÿPH p
– fl
Let us define a term ξt s,a,δ that will be used to define the probability bonus.
p q
Hlog 6HSA Slog 8t2H2 log 32t2N s,a δ
ξt s,a,δ : min 2,4 p q` p q` p t p q{ q
p q “ ˜ d 2N t s,a ¸
p q
This induces a trajectory-dependent bonus, given by
H´1
bt τ,δ : ξt s ,a ,δ
P h h
p q “ p q
h“1
ÿ
This induces a policy-level bonus (which depends on the transition kernel), given by:
H´1
bt P pP,π,δ q: “min 4,E τ„Pπ
‹
bt F pτ,δ
q
“min ˜4,E τ„Pπ
«
ξt ps h,a h,δ
qff¸
h“1
` “ ‰˘ ÿ
EstimatesandBonusesincaseP isknown. Ifpˆrob isinsteadknown,keepfˆtandbt P,π,δ
‹ ‹ F
the same as above, but set pˆrob : P and bt P,π,δ : 0 for all t. p q
t ‹ P
“ p q “
35D.1 Showing that Assumption 3.3 is Satisfied
D.1.1 Bounding effect of error in F
LemmaD.3 (BoundingfˆtValueError). Given anyP, withfˆt computedusingdatafrom τ t
Pπ ‹i for any sequence of policies π i using least squares, the following holds with probabilityt 1i ´ui“ δ1 {1„ 6
uniformly over all π.
V P,fˆt,π V P,f‹,π bt P,π,δ
F
| p q´ p q|ď p q
Proof. Recall that with probability at least 1 δ 16, the following holds for any trajectory τ and
´ {
any t 1,h H .
p
ě P
f‹ τ h fˆt τ h γ τ h ,δ (3)
| hp r sq´ hp r sq|ď h,t p r s q
Nownote the followinginequalities,where i holds withprobability1 δ 16uniformlyoverall
p q ´ {
policies due to inequality 3 above.
V pP,fˆt,π q´V pP,f‹,π q“E τ„Pπ
»
fˆ ht pτ rh sq´f h‹ pτ rh
sqfi
h ÿPH p
– fl
piq
E τ„Pπ γ h,t τ h ,δ
ď » p r s qfi
h ÿPH p
bt P,π–,δ fl
F
“ p q
Lemma D.4 (Bounding Sum of F Bonuses). The following holds with probability 1.
T
bt P ,π ,δ O Bκ d κ d β δ T Bp T log T δ
F ‹ t 2 h 2 h h,T
p q“ ¨ ` p q ` p { q˛
t ÿ“1 h ÿPH p h ÿPH p b a
r˝ ‚
Proof. First note the following inequality, which hold with probability 1 δ 16 by the Azuma-
´ {
Hoeffding inequality.
T T
bt F pP ‹,π t,δ
q“
E τ„Pπ ‹t
»
γ t,h pτ rh s,δ
qfi
t ÿ“1 t ÿ“1 h ÿPH p
T – fl
γ τ h ,δ O Bp T log T δ
t,h
ď p r s q` p { q
t ÿ“1h ÿPH p ´ a ¯
Now apply Lemma 3 in (Chan et al., 2021) for each h separately,with the function class in the
lemmasetto σ f f F ,P 1,x x τ h andmisspecificationε 0(decoupledfrom
h h h t,p t,1 t
t ˝ | P u “ “ “ r s “
36the Eluder dimension’s ε). We also note that ot are η-subgaussiansamples with mean σ f τ h .
h p h p r sqq
We obtain
T
max σ f τ h σ f1 τ h O Bd d β δ T
t
ÿ“1fh,f h1PC htpδq p h p r sqq´ p hp r sqqď
ˆ
h `
b
h h,T p q
˙
where d dim F ,B is the Eluder dimension of F and β δ β¯ δ . Since the
h “ E h T h h,T p q “ 2t2H
Lipschitz constant of σ´1 is κ , we have that the following holds with probability 1.
2
` ˘ ` ˘
T
γ τ h ,δ O Bκ d κ d β δ T
t,h 2 h 2 h h,T
p r s qď ` p q
t ÿ“1 ˆ b ˙
This implies that the following holds with probability 1 δ 16.
´ {
T T
bt P ,π ,δ γ τ h ,δ O Bp T log T δ
F ‹ t t,h
p qď p r s q` p { q
t ÿ“1 t ÿ“1h ÿPH p ´ a ¯
O Bκ d κ d β δ T Bp T log T δ
2 h 2 h h,T
“ ¨ ` p q ` p { q˛
h ÿPH p h ÿPH p b a
r˝ ‚
D.1.2 Bounding effect of error in P
We now restate Lemma B.2 of Chatterji et al. (2021) in our notation.
Lemma D.5 (Change of Measure Inequality). For any function µ of trajectories bounded by D,
if Pˆt is computed from data that includes trajectories tτ i „Pπ ‹i ut i“1 for any sequence of policies π i,
then the following holds uniformly over all policies π with probability 1 δ 16.
´ {
E τ„P ‹rµ pτ qs´E τ„Pˆ trµ pτ qsď2D log pD qbt P pPˆ t,π,δ
q
The same statement holds if we switch the roles of P ana d Pˆ on both sides.
t
Proof. For the order of P and Pˆ in the statement, the following follows from Lemma B.2 of
t
Chatterji et al. (2021) with η D and ε 1. We pull the additive log D in the square root
“ “
t2
p q
outside to fit our assumption’s phrasing.
1
E τ„P ‹rµ pτ qs´E τ„Pˆ trµ pτ qsďD log pD qbt P pPˆ t,π,δ q` t2 ď2D log pD qbt P pPˆ t,π,δ q
a a
The only subtlety is that more data than that from τ t could have been used to compute Pˆt.
The proof still follows since cP Pˆt,π,D is decreasingt ini u ti h“ e1 counts N
t
s,a .
Finally,ifweswitchPandPˆp onbothq sides,wecanfollowtheproofop fLeq
mmaB.2verbatimwith
t
P and Pˆ switched everywhere, except for the martingale argument. There, instead of switching
t
the two transition kernels, we negate the martingale to get our desired result. This exception is
because we still need the expectation to be over the true transition kernel P for the stochastic
‹
process defined to be a martingale.
37Lemma D.6 (Bounding Pˆ Value Error). Consider any sequence of functions ft that induce value
t
functions V P,ft,π . For any sequence of policies π t, if the estimates Pˆt, bonuses bP and costs cP
are generatep d usingq data including that of τ
i
„
Pπ ‹i,i
“
1
Ñ
t, then the following holds uniformly
over t and over all policies with probability 1 δ 16.
´ {
V Pˆ ,ft,π V P ,ft,π Bp log Bp bt P ,π,δ
t t ‹ t P ‹
p q´ p q“ p qp p qq
The statement also holds if we switch Pˆ and P . a
t ‹
Proof. Note the following two inequalities that immediately follow from Lemma D.5
V P ,ft,π V Pˆ ,ft,π Bp log Bp bt Pˆ ,π,δ
‹ t t t P t
p q´ p qď p qp p qq
V Pˆ t,ft,π
t
V P ‹,ft,π
t
Bpalog Bp bt
P
P ‹,π,δ
p q´ p qď p qp p qq
a
Our result follows immediately.
LemmaD.7(BoundingSumofP Bonuses). Thefollowing holds withprobability 1 δ 16whenever
the data used to compute bt P,π,δ includes the data of trajectories τ ,t 1 t.´ {
P i
p q “ Ñ
T
bt P ,π ,δ O SAc¯ c¯ ?HSAT
P ‹ t δ δ
p q“ `
t ÿ“1 ´ ¯
r
where
Hlog 6HSA Slog 8t2H2 log 32t2N s,a δ
c¯ δ : “4 p q` p 2 q` p T p q{ q
c
This means that for any s,a, ξt s,a,δ 2 until N s,a c¯δ
p q“ t p qě 2
Proof. First note that by the definition of the bonus and the Azuma-Hoeffding inequality, we have
the following.
T T
bt
P
P ‹,π t,δ E τ„Pπbt
P
τ,δ
p qď p q
t“1 t“1
ÿ ÿ
T
bt τ ,δ O 4 T log T δ
P t
ď p q` p p { qq
t“1
ÿ a
T H´1
ξt st,at,δ O 4 T log T δ
“ p h h q` p p { qq
t“1h“1
ÿ ÿ a
Nownotethatthefirstinequalityholdsevenifmoredatabeyondthatof τ t isusedtocompute
t i ui“1
38ξt s,a,δ , since ξt s,a,δ is decreasing in N s,a .
t
p q p q p q
T H´1 T H´1 c¯
ξt st,at,δ SAc¯ δ
t ÿ“1h ÿ“1 p h h qď δ ` t ÿ“1h ÿ“1 N t psp htq,ap htq
q
b NTps,aq
1
SAc¯ c¯
δ δ
ď ` ?l
ps,aqPSˆA l“1
ÿ ÿ
SAc¯ 2c¯ N s,a
δ δ T
ď ` p q
ps,aqPSˆA
ÿ a
SAc¯ 2c¯ SA N s,a
δ δ T
ď ` p q
d ps,aqPSˆA
ÿ
O SAc¯ c¯ ?SATH
δ δ
“ `
´ ¯
This concludes our proof, since 1 O ?HSA
“ p q
D.1.3 Putting everything togetrher
Theorem 3.7 (POR-UCBVI Regret). Under Assumption 2.4, POR-UCBVI satisfies Assump-
tion 3.3 and its regret Regret T is bounded by the following with probability at least 1 δ, ignoring
p q ´
polynomial terms independent of T.
O BpC H,S,A κ d β δ ?T
2,h h h,T
¨¨ p q` p q˛ ˛
h ÿPH p b
r˝˝ ‚ ‚
where C H,S,A : p H?SA S?HA . Also note that d β δ O η Hd d in our
h h,T h h C,h
p q “ p ` q p q “
case.
a ` a ˘
r
Proof. NotethatbyLemmasD.3, D.4, D.5, D.6, D.7,Assumption3.3issatisfiedbyPOR-UCBVI.
Using Theorem 3.4 and Lemmas D.4 and D.7, we have the following.
Regret T O Bκ d BpHSAc¯ κ d β δ T Bp T log T δ c¯ BpH?SAT
2 h δ 2 h h,T δ
p q“ ¨ ` ` p q ` p { q` ˛
hP ÿcHp h ÿPH p b a
˝ ‚
We further refine it grouping terms and ignoring terms independent of T, and also noting that
c¯ O ?H ?S
δ
“ p ` q
r
Regret T O κ d β δ Bp H?SA S?HA ?T
2 h h,T
p q“ ¨¨ p q` p ` q˛ ˛
h ÿPH p b
r˝˝ ‚ ‚
O BpC H,S,A d β δ ?T
h h,T
“ ¨¨ p q` p q˛ ˛
h ÿPH pb
˝˝ ‚ ‚
39where C H,S,A,p : Bp H?SA S?HA . Finally, recall that β δ O Hd .
h,T C,h
p q “ p ` q p q“ p q
a
r
We also have the following theorem, in the same vein as Theorem C.7
TheoremD.8(RegretforPOR-UCBVIifP isKnown). theoremWhenP isknown,POR-UCBVI
‹ ‹
that sets Pˆ : P and bt P,πδ : 0 for all t 1 still satisfies Assumption 3.3 and its regret
t ‹ P
“ p q “ ě
Regret T is bounded by the following with probability at least 1 δ, ignoring terms independent of
p q ´
T.
O Bp d β δ ?T
h h,T
¨¨ ` p q˛ ˛
h ÿPH pb
r˝˝ ‚ ‚
where d dim F ,B .
h “ E h T
Proof. Note that by`Lemma˘s D.3 and D.4, Assumption 3.3 is satisfied by POR-UCBVI. Using
Lemma D.4, we have the following.
Regret T O Bκ d κ d β δ T Bp T log T δ
2 h 2 h h,T
p q“ ¨ ` p q ` p { q˛
hP ÿcHp h ÿPH p b a
˝ ‚
We further refine it grouping terms and ignoring terms independent of T, and also noting that
c¯ O ?H ?S
δ
“ p ` q
r
Regret T O κ d β δ Bp ?T
2 h h,T
p q“ ¨¨ p q` ˛ ˛
h ÿPH p b
r˝˝ ‚ ‚
O Bp d β δ ?T
h h,T
“ ¨¨ ` p q˛ ˛
h ÿPH pb
˝˝ ‚ ‚
40E Proofs for Duelling Feedback
E.1 Proof for Reduction to Confidence-Set Optimism
Theorem 4.1 (ReductionfromDuelingtoConfidence-Set-BasedOptimism). If the confidencesets
C M pD t,δ
q
satisfy Assumption 3.1, then the dueling regret Regret DpT
q
of Algorithm 1 is given by
Regret T O C M,T,δ C M,T,δ
Dp q“ p P p q` F p qq
Proof. Foreaseofnotation,letususethersetsC M D t,δ givenbythepre-imageofC M D t,δ under
the map M M from Section 4. We first recall tp hat Mq ‹ C M D t,δ and so π ‹ Π t p for allq t with
probability 1ÞÑ δ 16. Recall that the value of a duel π,π1P undp er moq del M isP denoted by
´ { p q Ø
V M,π,π1 : V M,π V M,π1 V P,f,π V P,g,π1
D
p q “ p q´ p q“ p q´ p q
We overloadnotation and use the natural maps P,f M M to define
p qØ ÞÑ
V M,π,π1 : V M,π,π1
D D
p q “ p q
For ease of notation, we will then work with C M D t,δ in this proof until we can. Since π i,t
Π t for i 1,2, there exists some M i,t C M D t,δp forq i 1,2 so that V D M i,t,π,π 1,t P 0
for all π.“ Note that dueling regret is givP en bep low. q Inequali“ ty i is by definitp ion of M ,q sď ince
i,t
V M ,π ,π 0 for i 1,2. Inequality ii holds by definitiop nq of π ,π .
D i,t ‹ i,t 1,t 2,t
p qď “ p q
T 2
Regret T V M ,π ,π
Dp q“ D p ‹ ‹ i,t q
t“1i“1
ÿ ÿ
T 2 2
V M ,π ,π V M ,π ,π V M ,π ,π
D ‹ ‹ i,t D i,t ‹ i,t D i,t ‹ i,t
“ p q´ p q` p q
t ÿ“1”i ÿ“1 i ÿ“1 ı
2 T
piq
V M ,π ,π V M ,π ,π
D ‹ ‹ i,t D i,t ‹ i,t
ď r p q´ p qs
i“1t“1
ÿ ÿ
T
piiq
2 max V M,π ,π V M1,π ,π
D 1,t 2,t D 1,t 2,t
ď
t“1
M,M1PCMpD t,δq p q´ p q
ÿ “ ‰
41Continuing, we have
T
Regret T 2 max V M,π ,π V M1,π ,π
Dp qď
t“1
M,M1PCMpD t,δq D p 1,t 2,t q´ D p 1,t 2,t q
ÿ “ ‰
T
2 max V M,π ,π V M ,π ,π V M ,π ,π
D 1,t 2,t D ‹ 1,t 2,t D ‹ 1,t 2,t
“
t
ÿ“1M,M1PCMpD t,δq
”
p q´ p q` p q
V M1,π ,π
D 1,t 2,t
´ p q
T ı
2 max V M,π ,π V M ,π ,π
D 1,t 2,t D ‹ 1,t 2,t
ď t“1M,M1PCMpD t,δqr p q´ p qs`
ÿ
max V M ,π ,π V M1,π ,π
D ‹ 1,t 2,t D 1,t 2,t
M,M1PCMpD t,δq p q´ p q
T “ ‰
2 V M ,π ,π V M ,π ,π V M ,π ,π V M1,π ,π
“ D p t 1,t 2,t q´ D p ‹ 1,t 2,t q ` D p ‹ 1,t 2,t q´ D p t 1,t 2,t q
t ÿ“1” ı ” ı
r r
where M and M1 are the respective maximisers. It suffices to analyseonly one of the terms, as
t t
a consequence of the symmetry of Assumption 3.1.
We canrnow usre the fact that M is described by P,f to analyse the first term, letting M
t
p q Ø
P ,ft .
t
p q
r
T
r r
V M ,π ,π V M ,π ,π
D t 1,t 2,t D ‹ 1,t 2,t
p q´ p q
t ÿ“1” ı
T r
2 V P ,ft,π ,π V P ,f‹,π ,π
D t 1,t 2,t D ‹ 1,t 2,t
“ p q´ p q
t ÿ“1” ı
T r r
2 V P ,ft,π ,π V P ,ft,π ,π V P ,ft,π ,π V P ,f‹,π ,π
D t 1,t 2,t D ‹ 1,t 2,t D ‹ 1,t 2,t D ‹ 1,t 2,t
ď p q´ p q ` p q´ p q
t ÿ“1” ı ” ı
T r r r r
2 V P ,ft,π ,π V P ,ft,π ,π V P ,ft,π ,π V P ,f‹,π ,π
D t 1,t 2,t D ‹ 1,t 2,t D ‹ 1,t 2,t D ‹ 1,t 2,t
ď p q´ p q ` p q´ p q
t ÿ“1” ı ” ı
T r r r r
piq 2 V P ,ft,π V P ,ft,π V P ,ft,π V P ,ft,π
t 1,t ‹ 1,t t 2,t ‹ 2,t
“ p q´ p q ´ p q´ p q
t ÿ“1” ı ” ı
V rP ,rft,π ,π Vr P ,f‹,π ,πr r r
D ‹ 1,t 2,t D ‹ 1,t 2,t
` p q´ p q
T” ı
piiq 2 V P ,frt,π V P ,ft,π V P ,ft,π V P ,ft,π
t 1,t ‹ 1,t t 2,t ‹ 2,t
“ p q´ p q ´ p q´ p q
t ÿ“1” ı ” ı
V Pr rP ,ft , π ,π r V P P ,rf‹ ,rπ ,π r
‹ ‹ 1,t 2,t ‹ ‹ 1,t 2,t
` p b p qq´ p b p qq
” ı
Where i holdsbythedefinitionofV andV,and ii holdsintheproductMDPM oncewedefine
D ‹
ft τ ,p τq h : ft τ h ft τ h and recallp thq at P P P . Now, we can immediately
hpp 1 2 qr sq “ hp 1 r sq´ hp 2 r sq ‹ “ ‹ b ‹
apply Assumption 3.1 to the last line in two different ways. For the first two terms, we apply the
r r
42firstpointintheassumptiontoeachundercardinalfeedbackforMDPM ,notingthatthedatasets
‹
D contain trajectories from π as well as π . For the last term, we apply the second point in
t 1,t 2,t
the assumption under cardinal feedback for the MDP P ,f‹ .
‹
p q
This gives us that with probability 1 δ,
´
Regret T O C M,T,δ C M,T,δ
P F
p q“ p p q` p qq
r
We have the following lemma, which is an immediate consequence of
Lemma E.1. dim F ,ε 9dim F ,ε 2
E h E h
p qď p { q
Proof. Let d dim F ,ε . Pick the ε1 so that there is a sequence of d pairs τ ,j 1 d of
h E h h j h
“ p q “ Ñ
lengthhtrajectories,whereeachoneisε1-independentofitspredecessors. Notethatτ τ ,τ .
j 1,j 2,j
“p q
We now inductively build a sequence i so that each τ is ε1 2-independent of its predecessors.
j ij,j
{
Pick the first i arbitrarily. Now assume that we have built the sequence until index j k.
1
“
Also, by definition of this sequence, there exist f ,f1 , we have k f τ f1 τ 2 ε1 but
j j j“1p jp j q´ jp j qq ď
|f k`1pτ
j
q´f1 k`1pτ
j
q|ěε1. Since a2 `b2 ď2 pa `b q2, we have tbhařt
k k
f τ f1 τ 2 f τ f1 τ 2 f τ f1 τ 2
g p j p ij,j q´ jp ij,j qq ďg p j p ij,j q´ jp ij,j qq `p j p 3´ij,j q´ jp 3´ij,j qq
fj“1 fj“1
fÿ fÿ
e e
k
2 f τ f1 τ 2 ?2ε1
ďg p jp j q´ jp j qq ď
fj“1
fÿ
e
Additionally, since
f τ f1 τ f τ f1 τ f τ f1 τ ε1
| k`1 p 1,k`1 q´ k`1p 1,k`1 q|`| k`1 p 2,k`1 q´ k`1p 2,k`1 q|ě| k`1p j q´ k`1p j q|ě
. So, there is an i so that
k`1
f τ f1 τ ε1 2
| k`1 p ik`1,k`1 q´ k`1p ik`1,k`1 q|ě {
So, we have a sequence x : τ and a sequence of pairs of functions f ,f1 so that for any
j “ ij,j j j
1 k d , k f x f1 x 2 2 ε1 2 but f x f1 x ε1 2. This implies
ď ď h j“1p j p j q´ jp j qq ď p q | k`1 p k`1 q´ k`1p k`1 q| ě {
the following. Inequality i holds by Proposition 43 of Jin et al. (2021) upon setting β 2 ε1 2
ř p q “ p q
and setting the proposition’s ε to ε1 2. Inequality ii holds since ε1 2 ε 2.
{ p q { ě {
dh
d 1 f x f1 x ε1 2
h “ p| j p j q´ jp j q|ě { q
j“1
ÿ
piq 2 ε1 2
p q 1 dim F ,ε 2
ď ε1 2 2 ` E p h { q
ˆp { q ˙
9dim F ,ε1 2
E h
“ p { q
9dim F ,ε 2
E h
ď p { q
This establishes our claim.
43We have the following immediate corollary of Theorem 3.2, Theorem C.7 and Lemma E.1.
Corollary E.2. By using POR-UCRL as the algorithm in the dueling reduction in Algorithm 1,
we can get a bound on the dueling regret given by
Regret T O BpS?HA κ d¯ β δ ?T
Dp q“ ¨¨ ` 2 h h,T p q˛ ˛
h ÿPH p b
r˝˝ ‚ ‚
where d¯ dim F , B
h “ E h 2T
` ˘
44E.2 Reduction to Bonus-Based Optimism
We define the reduction using the algorithm below.
Algorithm 4 Reduction from Dueling to Cardinal Bonus-BasedOptimism
1: I Dn ,p bu ot nuK sn fo uw ncn tir oe nw sa br Dd f Pu ,n πc ,ti δon at nr dh u bH h D“1 P, ,πm ,e δth ,o cd onE fis dt ep nD cq et lo eve es lt δim
.
ate Pˆ D and f D from dataset
Fp q P p q
2: Initialize dataset D 1
Ðtu
3: for t 1,...,T do
“
4: Compute good set Π t {Valid π ‹ candidates}
Π t :
“
π PΠV D ppPˆ D t,f D tq,π,π 1 q`b F pPˆ D t, pπ,π 1 q,δ
q
! ˇ ˇ ˇ `z pBp qbP pPˆ D t,π,δ q`z pBp qbP pPˆ D t,π 1,δ qě0, @π 1 PΠ
)
5: Pick π 1,t,π 2,t given by {Most uncertain duel}
p q
a πr ,g π1m PΠa txb F pPˆ D t, pπ,π1 q,δ q`z pBp qbP pPˆ D t,π,δ q`z pBp qbP pPˆ D t,π 1,δ
q
H
6: Collect trajectories τ t,i “ pst h,i,at h,iqq h“1 along with feedback to h uhPH p by sampling from
Pπi,t for i 1,2. ! )
‹
“
7: Append the data to D t to get D t`1, update estimates and bonuses.
8: end for
Theorem E.3 (Reduction from Dueling to Bonus-BasedOptimism). If the bonuses and estimates
usedinAlgorithm4satisfyAssumption3.1,thenwithprobability1 δ,theduelingregretRegret T
´ Dp q
of Algorithm 4 is given by
Regret T O C M,T,δ C M,T,δ
Dp q“ p P p q` F p qq
Proof. Recall that the value of a duel π, rπ1 under model M M P,f is denoted by
p q Ø Øp q
V M,π,π1 : V M,π V M,π1 V P,f,π V P,g,π1
D
p q “ p q´ p q“ p q´ p q
We overloadnotation and use the natural bijection M M to define
Ø
V M,π,π1 : V M,π,π1
D D
p q “ p q
For ease of notation in the proof, we often work with an arbitrary pre-image fˆ D of f D under
the map f f. A careful read will confirm that this does not affect the correctness of any of the
ÞÑ
statements. First note that π Π for all T with probability 1 δ 16 since the following hold
‹ t
P ´ {
45uniformly over all π Π
1
P
´V D ppPˆ D t,fˆ D tq,π ‹,π 1 q“V pPˆ D t,fˆ D t,π 1 q´V pPˆ D t,fˆ D t,π ‹
q
“
V pPˆ D t,fˆ D t,π 1 q´V pP ‹,fˆ D t,π 1
q ´
V pP ‹,fˆ D t,π 1 q´V pPˆ D t,fˆ D t,π 1
q
” V P ,f‹,π V P ,f‹,π ı ” ı
‹ 1 ‹ ‹
` p q´ p q
V P ,f ,π ,π V P ,f‹ ,π ,π
D ‹ D 1 ‹ D ‹ 1 ‹
` pp tq q´ pp q q
ďz pBp qbP pPˆ D t,π ‹,δ q`z pBp qbP pPˆ D t,π 1,δ
q
0
`
`b F pPˆ D t, pπ ‹,π 1 q,δ
q`
“b F pPˆ D t, pπ ‹,π 1 q,δ q`z pBp qbP pPˆ D t,π ‹,δ q`z pBp qbP pPˆ D t,π 1,δ
q
where the inequality holds by Assumption 3.3 and the optimality of π in the true model. Note let
‹
Mˆ t be the model given by Pˆ D t,fˆ D t and let M t be the corresponding model in M. We make the
following abbreviation:
b M pM t, pπ,π1 q,δ q: “b F pPˆ D t, pπ,π1 q,δ q`z pBp qbP pPˆ D t,π,δ q`z pBp qbP pPˆ D t,π1,δ
q
T 2
Regret T V M ,π ,π
Dp q“ D p ‹ ‹ i,t q
t“1i“1
ÿ ÿ
T 2
V M ,π ,π V Mˆ ,π ,π b M , π ,π ,δ
D ‹ ‹ i,t D t ‹ i,t M t ‹ i,t
“ p q´ p q` p p q q
t ÿ“1”i ÿ“1
2
V Mˆ ,π ,π b M , π ,π ,δ
D t ‹ i,t M t ‹ i,t
` p q´ p p q q
i ÿ“1 ı
2 T
piq
V M ,π ,π V Mˆ ,π ,π b M , π ,π ,δ
D ‹ ‹ i,t D t ‹ i,t M t ‹ i,t
ď p q´ p q` p p q q
i ÿ“1t ÿ“1” ı
Inequality i holds since V Mˆ ,π ,π V Mˆ ,π ,π , and π Π for i 1,2 implies
D t ‹ i,t D t i,t ‹ i,t t
p q p q“´ p q P “
that
V D pMˆ t,π i,t,π ‹ q`b F pPˆ D t, pπ ‹,π 1 q,δ q`z pBp qbP pPˆ D t,π ‹,δ q`z pBp qbP pPˆ D t,π 1,δ qě0
Now note that the following holds uniformly over all timesteps t with probability 1 3δ 8 for
´ {
46i 1,2 simultaneously using Assumption 3.3 multiple times and applying a union bound.
“
V D pM ‹,π ‹,π i,t q´V D pMˆ t,π ‹,π i,t q“V D ppP ‹,f‹ q,π ‹,π i,t q´V D ppPˆ D t,f D tq,π ‹,π i,t
q
“V D ppP ‹,f‹ q,π ‹,π i,t q´V D ppPˆ D t,f‹ q,π ‹,π i,t
q
`V D ppPˆ D t,f‹ q,π ‹,π i,t q´V D ppPˆ D t,f D tq,π ‹,π i,t
q
“V pP ‹,f‹ ,π ‹ q´V pPˆ D t,f‹ ,π ‹ q`V pPˆ D t,f‹ ,π i,t q´V pP ‹,f‹ ,π i,t
q
`V D ppPˆ D t,f‹ q,π ‹,π i,t q´V D ppPˆ D t,f D tq,π ‹,π i,t
q
“z pBp qbP pPˆ D t,π ‹,δ q`z pBp qbP pPˆ D t,π i,t,δ
q
`b F pPˆ D t, pπ ‹,π i,t q,δ
q
b M , π ,π ,δ
M t ‹ i,t
“ p p q q
So, with probability 1 3δ 16, we have that
´ {
T 2
Regret T b M , π ,π ,δ
Dp qď M p t p ‹ i,t q q
t“1i“1
ÿ ÿ
T
piq
2 b M , π ,π ,δ
M t 1,t 2,t
ď p p q q
t“1
ÿ
T
“2 z pBp qbP pPˆ D t,π 1,t,δ q`z pBp qbP pPˆ D t,π 2,t,δ q`b F pPˆ D t, pπ 1,t,π 2,t q,δ
q
t“1
ÿ
T
piiq
O z
1
Bp bP P ‹,π 1,t,δ z
1
Bp bP P ‹,π 2,t,δ b
F
P ‹, π 1,t,π
2,t
,δ
ď ˜ p p q p q` p q p q` p p q q¸
t“1
ÿ
r
where inequality i holds since π ,π argmax b M , π,π1 ,δ and inequality ii
p q p
1,t 2,t
q “
π,π1PΠt M
p
t
p q q p q
holds with probability 1 3δ 8 by 6 applications of the change of measure inequality in Assump-
´ {
tion 3.3.
Now,wecanusethefactthatAssumption3.3issatisfiedagaintoconcludethatwithprobability
1 δ 32.
´ {
T
z
1
Bp bP P ‹,π 1,t,δ z
1
Bp bP P ‹,π 2,t,δ b
F
P ‹, π 1,t,π
2,t
,δ O C
P
M,T,δ C
F
M,T,δ
p p q p q` p q p q` p p q q“ p p q` p qq
t“1
ÿ
r
Taking a union bound over all inequalities stated so far, we have the following with probability
1 δ
´
Regret T O C M,T,δ C M,T,δ
Dp q“ p P p q` F p qq
as desired. r
Again, the following corollary is immediate from Theorem 3.4, Theorem D.8 and Lemma E.1.
47Corollary E.4. By using POR-UCBVI as the algorithm in the dueling reduction in Algorithm 4,
we can get a bound on the dueling regret given by
Regret T O BpS?HA BpH?SA κ d¯ β δ ?T
Dp q“ ¨¨ ` ` 2 h h,T p q˛ ˛
h ÿPH p b
r˝˝ ‚ ‚
where d¯ dim F , B
h “ E h 2T
` ˘
48