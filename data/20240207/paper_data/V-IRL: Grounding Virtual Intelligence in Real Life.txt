V-IRL: Grounding Virtual Intelligence in Real Life
JihanYang1* RunyuDing1 EllisBrown2 XiaojuanQi1 SainingXie2
1TheUniversityofHongKong 2NewYorkUniversity
https://virl-platform.github.io
SanFrancisco NewYork London Paris Tokyo Melbourne
‚ÄúFrom star-ratings to
‚ÄúBuilding the future, one your dining!‚Äù ‚ÄúNew city, new chapter,
blueprint at a time!‚Äù ‚ÄúSeamless schedules, endless curiosity!‚Äù
signature service!‚Äù
Yourperfecthome
awaitshere!
‚ÄúBeep beep! Clean ‚ÄúDash, deliver, done!‚Äù ‚ÄúLost in wonder.
street ahead!‚Äù Show me the way?‚Äù
Buenos Aires Rio de Janeiro Lagos Mumbai HongKong
Figure1. V-IRLagentsleveragereal-worldgeospatialinformationandstreetviewimagerytonavigateurbanterrains, executecomplex
tasks, and interact in real-time scenarios. From recommending relevant destinations to assessing city infrastructure to collaboratively
giving&followingverbaldirections‚ÄîwedevelopagentsthatillustrateV-IRL‚Äôscurrentcapabilities,flexibility,andutility. Aboveallelse,
wepresentaflexibleplatformforresearcherstoharnessabundantdatafromacrosstheglobetocreateandtestdiverseautonomousagents.
Abstract 1.Introduction
There is a sensory gulf between the Earth that humans
Theadventoflargelanguagemodels(LLMs)hasbreathed
inhabit and the digital realms in which modern AI agents
newlifeintoautonomousagentresearchbyofferingauni-
arecreated. TodevelopAIagentsthatcansense,think,and
versalinterfacefordiversecapabilities,rangingfrombasic
act as flexibly as humans in real-world settings, it is im-
reasoning to complex planning and tool use [72]. While
perative to bridge the realism gap between the digital and
these developments are promising, most of these agents
physical worlds. How can we embody agents in an envi-
remain confined to text-based environments or simplis-
ronmentasrichanddiverseastheoneweinhabit,without
tic simulations. Visual components in existing agents are
theconstraintsimposedbyrealhardwareandcontrol? To-
either rudimentary‚Äîsuch as simulated tabletop environ-
wardsthisend,weintroduceV-IRL:aplatformthatenables
ments[11,28]‚Äîorrelyonabstractedrepresentationsusing
agents to scalably interact with the real world in a virtual
ground-truthAPIs[27,67]. Furthermore, theprevalentvi-
yet realistic environment. Our platform serves as a play-
sualmodelsemployedbytheseagentsaretrainedonphoto-
ground for developing agents that can accomplish various
genic, object-centricInternetimages, whichfailtocapture
practicaltasksandasavasttestbedformeasuringprogress
theunpredictabilityanddiversityofreal-worldscenes.
in capabilities spanning perception, decision-making, and
This paper aims to bridge this gap between AI agents
interactionwithreal-worlddataacrosstheentireglobe.
and the sensory world by grounding them in rich, real-
worldenvironments‚Äîacrucialsteptowardsdevelopingau-
*WorkconductedduringavisittoNYU. tonomousagentsthatcaneffectivelyoperateinreal-lifesce-
1
4202
beF
5
]IA.sc[
1v01330.2042:viXranarios.OurnovelsettingforAIagentsnecessitatesrichsen- sues of scalability and real-world utility. In contrast, the
sorygroundingandperception: virtualembodimentwithin new wave of LLM-driven agents overcomes these chal-
citiesaroundtheglobeusingrealvisualandgeospatialdata. lenges with text as a universal interface, enabling natural
Tothisend,weintroduceV-IRL,aversatileplatformfor human interaction and adaptability to various tasks [49,
buildingandtestingvirtualagentswithinthisnovelvirtual- 62, 63, 68, 77]. Moreover, these models equip agents
real-world setting. V-IRL harnesses the power of map- with complex capabilities, such as tool use and collabora-
pingandstreetviewdata,enablingagentstonavigatereal- tion [26, 35, 50, 55, 67, 71, 84]. Yet a critical limitation
world locations, access up-to-date information about their persists:theagentsinthisnewwaveareentirelytext-based,
surroundings,andperformpracticaltasks. Withgeospatial devoid of any tangible connection to the visual or sensory
coordinatesatitscore,V-IRLisflexibleandextensible,inte- aspectsoftherealworld.
gratingwitharbitrarygeospatialplatformsandAPIs.More-
Embodied AI. Embodied AI studies intelligent agents &
over, V-IRL opens up a vast sea of visual data, allowing a
robotsperceivingandinteractingwiththeirenvironment. A
simpleandextensiblewayforresearcherstoevaluatevision
significantchallengeinthisfieldistheacquisitionoflarge
modelsonrealisticdatadistributions.
quantitiesofrealisticdata.Consequently,robotsareprimar-
WedemonstratetheversatilityandadaptabilityofV-IRL
ily trained in simulated environments [12, 46, 54, 73, 74]
by developing a series of diverse exemplar agents, each
to develop skills such as navigation [4, 5, 13] and manip-
solving a unique and practical task. As these agents hinge
ulation [25, 79]. Recent advancements in LLMs [2, 6, 66]
uponfoundationallanguageandvisionmodels,itiscritical
haveenabledembodiedagentstoperformlong-horizonand
toevaluatethesemodelswithinthissettingandtheirimpact
open-endtasksingame-engines[27,28,39,45,60]orhu-
onagentperformance. Weleveragethevastdataavailable
man rooms [10, 11, 19, 29, 38]. However, the diversity of
through our platform to develop global scale benchmarks
tasks and data is still too narrow and simplistic to enable
measuring the performance of underlying vision models
themtooperateflexiblyindiversereal-worldenvironments.
onimagesfromdiversegeographicandculturalcontexts‚Äî
evaluating their adaptability to shifting environmental, ar- Open-World Computer Vision. Motivated by the suc-
chitectural, and language-specific elements. Furthermore, cess of vision-language models [3, 8, 51, 80] pre-trained
we evaluate the contributions of models to agent perfor- on large-scale web-crawled data [16, 32, 56, 61, 65, 75],
mance on challenging tasks. Our results illustrate the po- open-world computer vision has received increasing atten-
tentialofV-IRLinbridgingthegapbetweenvirtualagents tion in recent years [23, 33, 34, 37, 47, 76, 82]. However,
and visually rich real-world environments, paving the way images and benchmarks sourced from the Internet [7, 18,
forfutureresearchinthisdirection. 21,31,33,53]areunavoidablybiasedtowardsspecificdis-
Insummary,ourcontributionsare: tributions rather than truly reflecting the real world [52].
‚Ä¢ V-IRL:anopen-sourceplatformforbuildingandtesting Because they are trained and evaluated entirely on Inter-
agents in a real-world setting that necessitates rich sen- netdata,existing‚Äúopen-world‚Äùmodelsareeffectivelymore
sory grounding and perception‚Äîembodiment using real open-Internetthanopen-world.
geospatialdataandstreet-viewimagery.
3.VirtualIntelligenceinRealLife
‚Ä¢ Developmentofdiverseexemplaragentsthatshowcase
theplatform‚Äôsversatilityandadaptability. TodemonstratetheversatilityofV-IRL,weuseittoinstan-
‚Ä¢ Globalbenchmarksmeasuringtheperformanceoffoun- tiateseveralexemplaragentsinourvirtualreal-worldenvi-
dational language and vision models (1) in isolation us- ronment. Inthissection,weengagetheseagentswithtasks
ingourplatform‚Äôsreal-worlddataand(2)onend-to-end thathighlightvariouscapabilitiesofourplatform.InSec.4,
agent performance in challenging tasks. In addition, we wediscussthetechnicaldetailsofourplatformandhowit
discusstherobustnessof‚Äúopen-world‚Äùvisionmodels enablesagentstointeractwiththerealworld.
toreal-worlddatafromacrosstheglobe. For illustration, we give V-IRL agents character meta-
We are excited to see how the research community will data, includingan8-bitavatar, aname, ashortbio, andan
leverage V-IRL to develop and test agents that can under- intention they are trying to accomplish. More concretely,
standandinteractwiththerealworld. agentsaredefinedbypipelinesthatusethischaractermeta-
dataalongwithourplatform‚ÄôsAPIandpretrainedmodelsto
2.RelatedWork addresscomplextasks(seeSec.4).Hereweprovideahigh-
leveloverviewofthetasks,highlighttheV-IRLcapabilities
Here,wegroundV-IRLtothreestreamsofresearch.
theyrequire,andvisualizetheagentssolvingthem.
AIAgents. Agentsareautonomousentitiescapableofper- We highlight the specific V-IRL capabilities being em-
ceivingtheirenvironmentandactingtoachievegoals[69]. ployed throughout using tags and corresponding colored
Historically,agentdevelopmenthasleveragedsymbolicand underlines: Map ‚Üí action, LLM ‚Üí reasoning,
reinforcementlearning methods[9,30, 48], whichface is- Vision ‚Üíperception,& Colab ‚Üícollaboration.
2
VC
VNE
LOC
ML3.1.EarthboundAgents Aria searches for possible
V-IRL agents inhabit virtual representations of realPecrsoniatlizieed Rsating: 8üëç Perso Â§∏na Áà∂liz ÁÇ∏ed ‰∏≤ Ra Kt win ag F: o8 oüëç d restaurantsnearby. Shethen
gar ro au pn hd icth ce oog rl do ib ne a. teA st cth oe rrc eo spre ono df it nh gi PD es ro ss otT nr oo ar lie o zs edTp pMa Rq e aur ox tie ic ne r ai gia n :s n2.5e tüëésnPetr oT |sao Gan r nrtta einl eiiez ne ro wyd ti C cR hhnaa f Vt √©i ein l‚Äì l Sg aa g: TB e7 a A Er. r5 Rüëç Te argÂ§∏ De tÁà∂ ee hpÁÇ∏ Fo‰∏≤ r ‚Äôie sdK - w Ska e F wo eo rd s PD eo rss oT no ar lo izs eT da M Rq e au x te i ic nr ai ga n :P 2er .5so üëéT |n Gaa rl ri t ez ine eed nr wyR ia C ct hain f V√©g i : l‚Äì la S7 gB. T5 ea Arüëç RTDeep Fried Skewers s toyn mth ae ks eiz fies nap lu rb ecli oc mr mev ei ne dw as -
surface. Using these coordinates, V-IRL allows virtual tions via GPT-4. As Peng
agents to ground themselves in the real world using maps, is new to the city and orig-
inallyfromSichuan,sherecommendsaspicyChinesejoint
streetviewimagery,informationaboutnearbydestinations,
KwaFoodDeepFriedSkewerstogivehimatasteofhome.
andadditionaldatafromarbitrarygeospatialAPIs.
PenghiresVivektohelphimfindanapartmentinEastVil-
RouteOptimizer Map
lage,JerseyCity,orLongIslandCityfor$1k‚Äì$3kpermonth
closetoagym,supermarket,andpublictransit...
Name:Peng Age:21 Loc:NYC
Bio: OriginallyfromChengdu,Sichuan,PengisastudentatPKU.Hejust
arrivedforasemesterabroadatNYC,andiscouchsurfinguntilhegetssettled. Vivek uses real es-
Intention: Pengneedstovisitfivelocationsaroundthecity:hisUniversity Recommendations RentalInformation
CardCenter,ResidenceHall,ResearchCenter,Library,andStudentCenter. Personalizedrating:7.5/10 üëç "address": 42-18 28th St, Unit tate APIs to find po-
Task: Givenastartingaddressandalistofwaypoints,planthe T a ac nh c de esa asp ta o gr yt sm mupe ,n er wt m his a icrw hke el t al s- ll , io gpc nua sbte li wd c it tw r hait nh Ps epe noa grs t 'y s, 1 ‚Äù ‚Äùr s2 qeE n ft, t "N " ::
4
e $ 5w 2
0
9Y ,0 "o b4r e,k d", t rN y opY oe
m
1 ": s1 A "1 :p 0 0a1 ,r , t ment, tential apartments in
shortestroutetoallwaypointsandthenfollowitonstreetview. r ne oq tu bi ere cm ose tn -et fs f. ecH tio vw ee fv orer a, st th ue denpr t.icemay "bathrooms": 1, Peng‚Äôsdesiredregions
Takeaway: V-IRLinstantiatesagentswithrealgeospatialinfor- Personalizedrating:8/10üëç "address": 37-14 32nd St, Unit and price range. For
Theapartmentiswell-locatedneara 508, New York, NY 11101,
mation,andenablesusefultaskslikerouteoptimization. s P n su te e ap n a te rg iobr 's nm y,l ma if br e ak us ye ttyt atl fa e h fn e. e cd M tlag u hcy l it k sm ip c, ol oew f mh b a muic s uch l ts o ea t s .al ei tg i sn o uns bsw wai at rh ye ‚Äù ‚Äù "br s aqen tft ht "" r:: o8 $ o01 m09 , s8 " " :6 b 1, e , d"t ry op oe m": s A ":p 1a ,r tment, e sea ac rh chc ea sndi id tsate p, roh xe imre --
gP ee tn dg on ce ue md es nt to ssv ii gs nit edse fv oe rra rel gl io sc tra at ti io on ns at shr ao vu ig sh ito inu gt t sh tue dc ei nty t..to . P T b au rhe eser , es se s sso u et bn a nwt ta e ia al y li laz fosce t rk ad Ps ti eor n nna e gsa 't s,ri rb an ey n qg d us: iu rg2 p ey/ me m1 r em s0 n,a tw srüëé .k he it cs h, " J $ 5 1a e ,2 7d r "6 1 yd s ,4 er e "3e ay bs r, e Cs " d bt" i: yt r
u
y o1 p il, o5 e tN m ""5 :: J 1sAW "0 9p :a 7 90as 3 2r ,h t "0 ,i bmn 2 ag e, t t n‚Äù ho t rrn , oe ‚Äùn oS s mtt q", f : s t "" :: i ct ay ret so ath be oup tl .ace Ss ynP te hn eg -
sizingthesefactors,Vivekprovidesaholisticratingandac-
Leveraging Geolocation & Mapping capabilities, Peng
companyingreasoningusingGPT-4. Histoprecommenda-
saves 7 minutes by walking along the shortest path as op-
tionisacost-effective1bedroomapartmentfor$1986/mo,
posed to in order waypoint visitation as shown in Fig. 2.
whichisclosetoasupermarket,2busstations,andagym.
3 3 4
3.3.VisuallyGroundedAgents
2 4 END Although language-driven agents can address some real-
1 2 3 1 world tasks using external tools, their reliance solely on
END START END START START text-based information limits their applicability to tasks
4 1 2 wherevisualgroundingisrequired.Incontrast,realsensory
42.7 min,3.1km 36.0min,2.6 km 9.7min,2.9km
inputisintegraltomanydailyhumanactivities‚Äîallowinga
Sequentialrouting Shortestpathrouting Shortestpathrouting
deepconnectiontoandunderstandingoftheworldaround
Figure2.FindingtheshortestpathforPengtotraveltofiveplaces.
us. Agents can leverage street view imagery through the
3.2.Language-DrivenAgents
V-IRL platform to visually ground themselves in the real
To tackle more complex tasks, we follow the pattern of world‚Äîopeningupawiderangeofperception-driventasks.
language-driven agents [72]. LLMs enable agents to flex-
UrbanAssistanceRobot Map Vision iblyreason,plan,anduseexternaltools&APIs.
PlaceRecommender Map LLM Name:RX-399 Age:Unk. Loc:HK/NYC
Bio: Thisurbanrobot‚Äôsadvancedobjectdetection,localization,andnaviga-
tionaltelemetrysystemsallowittoperformperceptivetasksinbusycitystreets.
Name:Aria Age:26 Loc:NYC Intention: Reportthelocationsoftrashbinstothesanitationdept.
B alwio ay: slA ook3r ind gy fe oa rr ng er wad pu la at ce esst tu od te rn yt ,w anh do slo hv ae rs esto het rry fan ve ow ritr ees st pa ou tr sa on nts. heS rh be lois g! Task: Travelalongaspecifiedrouteanddetectinstancesofaspec-
Intention: PickoutalunchspotthatPengmightlike. ifiedobject(e.g.,trashbins,hydrants,benches,etc.).
Name:Vivek Age:35 Loc:NYC Takeaway: V-IRLagentscanuseperceptiveinputtounderstand
Bio: Atech-savvyestateagentwhocombineshislocalknowledgewithonline andinteractwiththeirenvironment.
toolslikeZillowtofindtheperfecthomesforhisclientsinthebustlingcity.
Intention: HelpPengfindaplacetoliveforthesemester.
RX-399isastate-of-the-artrobotagentwithadvancednavi-
Task: Givenspecificlocation,background,andintention,synthe-
gationandsensingcapabilities. Itsmanufacturerisrunning
sizereviewsofnearbybusinessestoprovidearecommendation.
apilotprogramwithsanitationdepartmentsinHongKong
Takeaway: V-IRLexposesrichreal-worldinformationtoagents andNewYorkCitytoassessitsreadinessforgarbageduty...
thattheycanuseforreal-worldtasks.
RX-399navigatesalongpre-definedcityroutes,taggingall
Pengisstarvingforsomelunchbutdoesn‚Äôtknowwhereto
trash bins using its open-world detector and geolocation
eat...Luckily,hemetanicegradstudentAriaduringhiser-
module as depicted in Fig. 4. RX-399 can actively adjust
randswhomightbeabletohelphimfindagoodspot...
its camera pose to the optimal view for each potential ob-
3
VNE
VNE
ML
VNE VCTrashbin Trashbin Trashbin Trashbin Hydrant
Hydrant Bench Bench
Figure3.Imani‚Äôsvisualizationoftrashbins,firehydrants,&parkbenchesinNYC‚ÄôsCentralParkusingdatacollectedbyRX-399.
jectthankstoourinteractiveembodiedenvironmentandthe UrbanPlanner Map Vision
sensor-rich visual input. During the pilot in Hong Kong,
RX-399 locates eight trash bins, correctly identifying five Name:Imani Age:42 Loc:NYC
butoverlookingone. InNewYork, itaccuratelydetectsall Bio: Asustainableurbandevelopmentgraduate,Imaniispassionateabout
maintainingaharmoniousbalancebetweennatureandurbanecosystems.
fivetrashbinsbutmistakenlyreportstwomailboxes. Intention: UseRX-399tocollectfirst-persondataforherstudies.
Task: Recordthelocationofallinstancesofanyspecifiedobjects
HongKong (e.g.,trashbins,hydrants,benches,etc.) inaspecifiedregion.
Takeaway: V-IRLenablesrealisticopen-worldapplicationsre-
quiringvastgeospatialandfirst-personvisualinformation.
Imanineedstoanalyzethedistributionoftrashbins,firehy-
drants, andparkbenchesinNewYork‚ÄôsCentralParkfora
projectwiththeNYCParks&Recreationdepartment...
NYC
Imani sets routes spanning Central Park and objects of in-
terestforRX-399,whotraversestheroutesandrecordsall
detected instances. After RX-399 finishes its route, Imani
analyzes the collected data at different levels of detail. As
depictedinFig.3,thecoarsestlevelshowsgeneraldistribu-
tionsoftrashbins,hydrants,andbenchesinthepark. Imani
Figure4.PortionsofRX-399‚ÄôssystemrecordsinHKandNYC.
can also zoom in to specific regions, where lighter colors
RX-399 can avoid double-counting previously seen ob- represent positions with more unique instances identified.
jects by using feature matching to check for duplicates ThefollowingtablepresentsRX-399‚Äôscountingreport:
amongpriordetections(seeFig.5).
Category Trash Bin Fire Hydrant Park Bench‚àó
Count 1059 727 1015
Table1.RX-399‚ÄôscountingreportinCentralPark,NewYorkCity.
(‚àóNote:contiguousbenchescountedasoneinstance).
By retrieving geotagged sensory-rich data within RX-399,
Imanicanalsoinspectthedetectionresultsforeachobject
Figure5. RX-399avoidsdouble-countingtrashcansbyidentify- to help her verify the reliability of RX-399‚Äôs reports as il-
ingduplicatesacrossdifferentviewpointsusingfeaturematching.
lustratedbythebottomlevelinFig.3.
4
VNE VC3.4.CollaborativeAgents
IntentionalExplorer Map LLM Vision
Humans often work together to solve complex real-world
Name:Hiro Age:22 Loc:HK
Bio: Aseasonedtraveler,Hirothrivesinunknownterritories.Heenjoysget- tasks. Thiscollaborationpromotesefficiencyandeffective-
tinglostinnewplacesinsteadoffollowingthetravelguide.
Intention: Hiroislookingforanauthenticlunchspotthatisnottoospicy. nessbydecomposingacomplextaskintosimplersub-tasks,
Task: Exploreonfoot(instreetview)lookingforadestinationthat allowing each to be handled by an expert in its domain.
fulfillsacertainintention(e.g.,lunch,shopping,etc.) Grounded in the world via our platform, V-IRL agents can
Takeaway: Agentscanutilizevisualdetectors,VLMsandLLMs leveragegeospatialdataandstreetviewimagerytocollab-
toiterativelyperceive,decide,andinteractintheenvironment. oratewithotheragentsaswellaswithhumanusers.
HiroisstartinganewjourneyinHongKong. Hedecidesto
explorewithoutaspecificdestinationinmind,lookingfora
3.4.1 Agent-AgentCollaboration
goodlocallunchspotwithfoodthat‚Äôsnottoospicy...
Aswithpreviousagents, collaborativeagentsaredesigned
As depicted in Fig. 6, starting at , Hiro walks down the for specific tasks; however, they can handle objectives be-
street and encounters the first intersection. Thanks to the yondtheirexpertisethroughcollaborationwitheachother.
interactiveandsensory-richenvironment,hecanadjusthis
pose to fetch street views for each possible path. Using
Tourist Map LLM Vision Colab
VQAontheseviews,hedecidestoturnleft:
Name:Ling Age:25 Loc:NYC/SF/HK
Residentialbuildingsontheleftroadindicatecozyand Bio: LingisaspiritedtravelerfromTaipeiwhoisalwayseagertoexplorenew
citiesandcultures.Sheisunafraidofaskinglocalsforhelpwhenshe‚Äôslost!
family-runlocalfood...Abetterchoicethantheothers! Intention: NYC:findgiftsforfriendsbackhome;gotoafamousrestaurant.
SF:findastoretorepairabrokeniPhone.HK:trysomeauthenticlocalfood.
Then,afterexploringforablock,heencountersthesecond
Task: (i)AskanearbyLocalagentfordirectionstoaspecificloca-
intersectionwherehelooksaroundanddecidestoturnright:
tion. TheLocalswillpreviewtherouteonthemapandinstreetview
andthenprovidewalkingdirectionsinnaturallanguage,mentioning
Looksliketherearesomelocalfoodspotsthisway... majorintersectionsandlandmarks.
Afterafewsteps,Hirofinds‚ÄúAOneChineseNoodlesÈòø‰∏Ä (ii)Followthesedirectionsinstreetview,andiflost,ask
anotherLocalagentforassistance.
Ë±¨ÊâíÈÖ∏Ëæ£Á±≥Á∑ö‚Äùusinghisopen-worlddetector.Heretrieves
Takeaway: Agentscancollaboratetosolvecomplextasksthat
information, ratings, and reviews for the restaurant using
arebeyondtheirindividualexpertise.
our platform, which connects street views to places. Hiro
ultimatelydecidestopassonitandkeepexploringbecause:
Lingtravelstocitiesaroundtheworld.Sheseeksoutauthen-
Mostreviewsmentionthespicyporkchopnoodles...
ticexperiencesandisalwaysunafraidtoaskforhelpfrom
Finally, at the end of the block , Hiro discovers another Localswhenevershefindsherselflost...
lunch spot called ‚ÄúXintianfa Êñ∞Â§©Áôº‚Äù. He decides to dine
thereafterreadingnumerousonlinereviewspraisingitsau-
AfterobtainingroutedescriptionsfromLocals, Lingstarts
thenticcuisineanddiversemenu.
herjourney‚ÄîasshowninFig.7. Groundedinourembod-
ied platform, Ling can adjust her pose and identify visual
[EXPLORATION STARTS]
‚ÄúLet‚Äòs grab a bite to eat, I‚Äôm hungry.‚Äù
landmarks along the streets using open-world recognition
andhermap. Correctlyrecognizingtheselandmarkshelps
GPT-4tomakecorrectdecisionsaboutwheretochangedi-
[ACTION1] rection,moveforward,andstop,asseeninthetoptwoNew
‚ÄúIshouldturnleft.‚Äù
York City cases in Fig. 7. The success of these decisions
made by GPT-4 relies on the real-sensory input for visual
[ACTION2] groundingandtheinteractiveenvironmentfromV-IRL.
‚ÄúIshouldturnright.‚Äù
AOne Nevertheless,Lingmayoccasionallyfailtofindthedes-
[EXPLORATIONENDS] Èòø‚ºÄË±¨ÊâíÈÖ∏Ëæ£‚Ω∂Á∂´
‚ÄúLet‚Äôs dinehere!‚Äù ChineseNoodles [ACTION3] tination.InthebottomleftSanFranciscoexampleinFig.7,
‚ÄúI‚Äôllpass.Keepexploring.‚Äù
restaurant Ling passes by the Apple Store because only its stainless
restaurant steelwallisvisiblefromherviewpoint. Inthebottomright
Êñ∞Â§©Áôº
ChineseTakeout Hong Kong example, Ling mistakes another restaurant for
her destination and stops prematurely. Fortunately, when
shemakesthesemistakes,LingcanaskanotherLocalagent
Figure6.VisualizationforHiro‚ÄôslunchexplorationinHK. for new directions and start another round of navigation,
whicheventuallyleadshertothedestination.
5
VNE ML VC
VNE ML VC LOCNYC NYC
Where‚Äôs a good place to buy N Any good burger spots N
somegifts for my friends? around here?
START
The MoMA Design Store is START
an excellent option!Let me Famous Ben‚Äôs Pizza
tell you how to get there:‚≠ê
Black Tap is worth checking out!
I'll tell you how to get there:‚≠ê
MoMA Design Store
Gi5 Shop
Arrived!
Aritzia
Women‚Äôs clothing store
Black Tap Cra6 Burgers
I‚Äôve found the
& Beer ‚ÄìSoHo
MoMAStore!
American -$$
‚≠êFirst, turn left to face northwest and walk a short distance until you reach the ‚≠êFirst, turn right to face southeast and walk a short distance until you reach
next intersection, with Prince St in front of you. Next, make a sharp left turnto the next intersection. You should see Famous Ben‚Äôs Pizza on your left.Next, turn
head southwest. Continue straight for a while until you reach the next rightto head southwest. Continue walking for a while until the next
intersection, where you'll see Aritziaon your left front. Finally, make a sharp left intersection. JanovicPaint & Decorating Center SoHo should be behind you on
turn to head southeast and walk a bit further. You will find your destination, the your right.Finally, turn rightagain to face northwest and walk just a bit further.
MoMA Design Store, on your left. Your destination, Black Tap Craft Burgers & Beer -SoHo, will be on your left.
San Francisco HongKong
Oh no, my iPhone START N
Ka Hing
screen just cracked! I‚ÄôvefoundKa
ÂòâËààÈ§êÂª≥
Hing,finally!
There‚Äòs an Apple Store just No, you should keep
Starbucks a short walk from here.I walkingtothewest.üí¨
can guide you there:‚≠ê
Is Ka Hing There are two restaurants
Restaurant here? on my left. I‚Äôm guessing
one of them is Ka Hing?
Apple Union Square
Electronics Store Hmm. Idon‚ÄôtseetheApple
Store on my right. Maybe
Ahh, I see the Apple it‚Äôs a bit further ahead?
Store on my left now! McDonald‚Äôs
I still can‚Äôt find the Apple È∫•Áï∂Âãû START
Store. Maybe I should find Fast Food $
You just went past the some more help‚Ä¶
Apple Store. You need Is there a local
to turn around.üí¨ I eat at Ka Hing restaurant you‚Äôd
every day!‚≠ê recommend?
‚≠êTry1:First, turn around and head west.Continuestraightuntilyoureachthe ‚≠êTry1:First, turn to southand walk until you reach the intersection. You will
intersection,with aStarbuckson your left.Next, turn left to head south. Walk a notice the McDonaldon your left front.Then, take a right turn, and continue
bit further, and your destination, Apple Union Square, will be on your right. walking west. Proceed until you see Ka Hing Restaurant on your left-hand side.
üí¨Try2:Turn around and head north. Walk straight for a short distance until üí¨Try2:Facing west, walk a short distance until you spot Ka Hing Restauranton
you reach the intersection. You will see the Apple Union Square,on your left. your left.
Figure7.LingandLocalcollaborationexamples.TrajectoriesinredandgreenmeanLing‚Äôsfirstandsecondattempts,respectively.
6
NInteractiveConcierge
76 Energy Hunger Pain Joy Stress Sadness Budget
42 3
8
Morning walk in the Washington Square Park. 09:00 -10:00 80% 20% 0% 30% 50% 0% $120
5 Travel from Washington Square Parkto The Cloisters. 10:00 -10:52 90% 25% 0% 45% 40% 0% $120
ExploreThe Cloistersand the surrounding Fort Tryon Park. 10:52-12:30 85% 30% 0% 60% 30% 0% $117.25
Having food inJochy‚ÄòsCafenearFort Tryon Park. 12:30-13:30 75% 45% 0% 80% 25% 0% $92.25
Travel from Jochy‚ÄòsCafeto the Little Red Lighthouse. 14:00 -14:28 95% 15% 0% 95% 20% 0% $72.25
VisitLittle Red Lighthouseand enjoy views of the Hudson River. 14:28-16:00 90% 15% 0% 100% 15% 0% $69.5
Travel from the Little Red Lighthouseto Wave Hill. 16:00-17:06 80% 25% 0% 100% 10% 0% $69.5
1START Explore the gardens and art exhibitions at Wave Hill. 17:06-19:00 75% 35% 0% 100% 5% 0% $64.5
Travel fromWave Hillto the Riverdaleneighborhood. 19:00-19:13 65% 45% 0% 100% 0% 0% $54.5
Having food in Floridita Restaurant,Riverdale. 19:13-20:00 63% 50% 0% 100% 0% 0% $52.5
Travel from Floridita Restaurant in Riverdaleto theuniversity. 20:00-20:40 83% 20% 0% 100% 0% 0% $22.5
73% 30% 0% 100% 0% 1% $19.75
Figure8.ThePerfectDayItinerary:CraftedbyDiego,ouriterativeconciergeagent,thisscheduleismeticulouslytailored,accountingfor
yourmentalandphysicalwell-beingandbudgetvariationsasyourdayunfolds.
Geo:[40.8649162,-73.9311561] Geo:[40.8647205, -73.9325163] Geo:[40.8653388, -73.9322499] Geo:[40.8609142,-73.9324818] Geo:[40.8642401,-73.9325958]
üëçRating :7.5 üëçRating:7.2 üëçRating :6.5 üëéRating:4.2 üëéRating:3.5
Figure9.DiegotraversesregionsofinteresttofinSdinsgcelenipcalsoscraetisounltsstoaddtoyouritinerary.
3.4.2 Human-AgentCollaboration 5
6
08:00 -9:00
Morning walk at The High Line.
Groundedinthesameenvironmentwehumansinhabit, V- 09:15-10:00
Breakfast at a local caf√© in Chelsea.
IRLagentscancollaboratewithandassistrealhumanusers.
Subway ride from Chelseato Green-Wood Cemetery. 10:00-10:30
InteractiveConcierge Map LLM Vision Colab Explore Green-Wood Cemetery. 10:30-12:30
12:45-13:45
Lunch at a quiet restaurant in Brooklyn.
Name:Diego Age:62 Loc:NYC
Bio: Diegoisanexpertconciergeatahotel.He‚Äôsamasteratcreatingintri- Visit the Brooklyn Botanic Garden. 14:00-15:30
c Ia nt te ei ntin te ir oa nri :esa Pn ld anp pro ev ri sd oi nn ag liv zea dlu aa nb dle plo raca ctl ia cad lv ii tc ie n.
eraryforcustomer! Travel from Brooklyn Botanic Gardento Wave Hill.
15:30-16:00 12
16:00-18:00
Task: Givenauser‚Äôslocation,background,andintentionforaday, Stroll around Wave Hill. START
planafullitinerarybalancingtheirmental/physicalstate&budget. Dinner at a cozy restaurant in Riverdale. 18:15-19:00
Takeaway: V-IRLagentscancollaboratewithuserstosolve Travelback to theuniversity. 19:00-20:00
complextasksthatrequireunderstandingtheuser‚Äôsinternalstate. 4
3
As a university student in NYC, you are excited to spend a
dayexploringlesser-knownandtranquilplaces.Yourfriend Figure10.AnungroundedLLM-onlyconciergeagent‚Äôsitinerary.
recommendedDiego, whoisknownforhisprofessionalism
inplanningpracticalandpersonalizeditineraries.
tanceandtraveltimebetweenlocationswithoutaccesstoV-
IRL,resultinginanimpracticalitinerary.Forexample,lack-
AsdepictedinFig.8,Diego‚Äôsitineraryistailoredtoyour
ing real geospatial information, the ungrounded concierge
(theuser‚Äôs)needs. Diegonotonlyconsidersyourphysical
allocates only 30 minutes for travel between the ‚ÄúBrook-
andmentalinteroceptionstatus,budgetforeachactivity,but
lynBotanicGarden‚Äùand‚ÄúWaveHill‚ÄùintheBronx, which
alsoanticipatesyourstatuschangesandcostwhenyoufol-
actuallyrequires60‚Äì100minutes*. Thehallucinatedtravel
low each event. He is able to take into account real travel
timesoverlookgeospatialrealitiesandresultinaplanwith
times from the V-IRL platform and select suitable destina-
excessivelydistantdestinations.
tionsbycollaboratingwithanotherrecommendationagent.
Also,asshowninFig.11,youcaninterveneinDiego‚Äôs
In contrast, Fig. 10 shows that a simpler ‚Äúungrounded‚Äù
LLM-onlyconciergeagentisunabletoconsidertherealdis- *(perGoogleMapshttps://maps.app.goo.gl/SW1r5GSx3ZVo7BTr7).
7
VNE ML VC LOCplanning process by adjusting your interoceptive status or
Background Intention Interoceptive State
byprovidingverbalfeedback. Inresponse,Diegopromptly
reviseshisoriginalplantoaccommodateyourdemands,and
re-estimatesyourstatechangesafterhisrevision. Location Biography Goal Task Mental Physical
Visit Little Red Lighthouseand enjoy views of the Hudson River. 14:28-16:00 Energy Hunger ‚Ä¶ Stress SadnessBudget Perception Reasoning Action Collaboration
OriginalPlan: 16:00-17:06 80% 25% 10% 0% $69.5
Travel from the Little Red Lighthouseto Wave Hill.
HumanIntervention(Option1): ‚Ä¶
Adjustinginteroceptivestates 40% 80% 10% 0% $69.5 Computer Vision Language Model
RevisedPlan1: 16:00-16:30 ‚Ä¶
E bra et a a kt aB nu du n ren fi reC so hf mfe ee nn te .arLittleRedLighthousefor a short 70% 20% 0% 0% $49.5 O Rep ce on g-W nio tir ol nd Localization MF ae ta ct hu ir ne g VQA Too Ul & se API Hu Im nta en ra / c tA iog nent
HumanIntervention(Option2): ‚ÄúOh no, I totally forgot! There's an assignment due tonight
Providingverbalfeedback and I need to change my plans immediately to get it done!‚Äù
RevisedPlan2: 16:00-17:20 ‚Ä¶
Travel from the Little Red Lighthouseback to theuniversity.
70% 35% 30% 5% $66.75 Environment
Figure11.Diegoadaptsoriginalplantosuituser‚Äôsintervention.
St Ir mee at g V erie yw Geolocation Movement Mapping Pla Sc ee a I rn cf ho &
Finally, usingV-IRL‚ÄôsstreetviewsandMap, Diegocan
traverse regions of interest scouting for potential scenic
viewpoints for you to visit as shown in Fig. 9. He uses
Figure12.HierarchicalV-IRLarchitecturedescribedinSec.4.
VQA to rate and assess each captured view, and adds the
highest-ratedlocationstoyouritinerary.
4.2.1 Environment(Action)
4.SystemFundamentals Environment components are responsible for ground-
ingagentsintheworldaroundthem: providinganavigable
This section introduces our system‚Äôs core: a platform de- representationofrealcities(seeSec.3.1). Geographicco-
signed for perception-driven agents that transforms real- ordinatesserveasthelinkbetweentheworldandourvirtual
world cities around the world into a vast virtual play- representationofit. LeveragingtheGoogleMapsPlatform
ground where agents can be constructed to solve practi- (GMP)[24],V-IRLenablesagentstoaccessstreetviewim-
cal tasks. At its heart, V-IRL is comprised of a hier- agery, query valid movements, retrieve information about
archical architecture (see Fig. 12). The platform lies at nearbylocations,andplanroutes. Asthesecoordinatesand
thefoundation‚Äîprovidingtheunderlyingcomponentsand locationinformationareboundtotherealworld, theyalso
infrastructure for agents to employ. Higher level capa- provideanaturalinterfacewithexternaltoolsthatleverage
bilities of Perception, Reasoning, Action, and geolocation‚ÄîsuchasrealestateAPIs(seeSec.3.2). Tech-
Collaboration emergefromtheplatform‚Äôscomponents. nicaldesignsofenvironmentaredetailedinAppendixC.
Finally, agentsleveragethesecapabilitiesanduser-defined
metadataintask-specificroutinestosolvetasks.
4.2.2 Vision(Perception)
4.1.AgentDefinition
Perception components enable agents to process the In our system, agent behavior is shaped by user-defined
sensory-rich data provided by the environment, especially
metadata, including a background, an intended goal, and
aninteroceptivestate.Thebackgroundprovidesthecontext street view imagery. Pretrained localization models [37]
giveagentsaprecisespatialunderstandingoftheirenviron-
necessary to instantiate the agent in the real world (loca-
ment.ThisallowsRX-399toidentifyandcountinstancesof
tion),andtoguideitsreasoninganddecision-making(biog-
raphy). Intentionsoutlineagents‚Äôpurposewithintheenvi- objects,andHirotopickoutspecificbusinessestolookup
ronment. Anagent‚Äôsinteroceptivestatereflectsitsinternal withtheGMP(Sec.3.3). Whilelocalizationmodelsallow
for precise interaction with perceptive input, open-world
mentalandphysicalstatus‚Äîvaryingovertimeandinfluenc-
recognitionmodels[51]aremoregeneral,andallowagents
ingitsbehavior. ThisnovelconceptiscrucialtoAIagents
todetectawiderrangeofobjectsintheirfieldofview(e.g.,
forenhancingcollaborationwithhumans(seeSec.3.4.2).
Tourist searches for the Apple Store). Pretrained feature
Concretely,agentsaredevelopedbywritingtask-specific
matching models [40] provide an understanding of conti-
run() routines that leverage the various components of
nuityacrossviewsofthesamelocation, andenableagents
ourplatformandtheagent‚Äôsmetadatatosolvetasks.
toidentify&deduplicateinstancesofthesameobjectfrom
different viewpoints (Sec. 3.3). Multimodal models with
4.2.PlatformComponents
VQA & Captioning capabilities [36] bridge the perceptual
Next, we delve into the platform components, which pro- world with natural language, and are essential for integra-
vide the infrastructure to instantiate capabilities, execute tionwithreasoning(Sec.3.3).
agentactions,andgroundagentsintherealworld.
8
LOC
VC ML VNE
VNE
VC
tnegA
seitilibapaC
mroftalPWorking PlanningIterations Human-Agent Human
Memory Interaction Background
Supervisor
Intention Budget
Audit ReviseApprove
Interoceptive State
RevisingLoop Interoceptive
Estimator Agent-Agent
Collaboration
Hierarchical Local Agents
Coordinator
Interactive Information
Concierge Retrieval Environment
Figure13.ArchitectureoverviewofinteractiveconciergeagentDiego(Sec.3.4.2).SeepipelinedescriptioninSec.4.4.
4.2.3 Language(Reasoning&Collaboration) The crucial final step involves a supervisor
module, which reviews (‚Äúaudits‚Äù) the incoming ac-
Reasoning componentsallowdecisionmakingbasedon
tivity in light of the current user status, remain-
information from perception and the environment. LLMs
ing budget, and potential interactions (exemplified in
suchasGPT-4[2]andLlama2[66]interfaceacrossvarious
Fig. 11). If the supervisor deems the plan unsuit-
APIs (Sec. 3.2), transforming environmental data and per-
able, it initiates revisions. The revised plan is then
ceptual outputs into actionable insights. They also enable
looped back to the hierarchical coordinator
Collaboration between agents or with humans through
and interoceptive estimator for reliability, fol-
natural language (Sec. 3.4) Custom prompts facilitate this
lowed by another review from the supervisor (see
interaction(seeSec.4.4).
the revising loop in Fig. 13). This iterative pro-
4.3.V-IRLCapabilities cess between the hierarchical coordinator, the
interoceptive estimator, and the supervisor
Ourplatform‚Äôscomponentscanbeflexiblycombinedtoex- continuesuntilthesupervisorapprovestheactivityand
hibitavastarrayofcapabilities.InSec.3,wepresentagents addsittoitsworking memory.
thatexhibitincreasinglycomplexbehaviors,eachrequiring
After finalizing an activity, Diego proceeds to plan the
more components of the platform. From simple combina-
subsequentactivitybyrepeatingthisprocessuntiltheday‚Äôs
tions, like the Route Optimizer (Sec. 3.1), to more com-
itineraryiscomplete.
plexarrangements,liketheTourist(Sec.3.4.1),oursystem
showcases the versatility and potential of the V-IRL plat-
5.V-IRLBenchmarks
form to be applied to various real-world scenarios. Next,
we perform a high-level case study of how V-IRL‚Äôs com-
Intheprevioussections,weillustratetheprimarybenefitof
ponentsarecombinedtocreateourmostcomplexagent;in the V-IRL platform: seamless access to first-person street-
Appendix D, we delve deeper into the low-level platform
viewimageryanddescriptiveinformationaboutreal-world
detailsthatunderpincreatingaV-IRLagent.
citiesacrosstheglobe. Thisscalablesourceoftrulyopen-
world data can be harnessed to test core component mod-
4.4. High-Level System Case Study: Interactive
elsandagentcapabilities. WeproposethreeV-IRLbench-
Concierge‚ÄúDiego‚Äù
marks: two evaluating vision-language models on open-
BystudyingDiego(Sec.3.4.2),weillustratehowourplat- world vision tasks (Secs. 5.2 and 5.3), and one evaluating
form‚Äôscomponentsarecombinedtocreatecomplexagents. end-to-end agent performance (Sec. 5.4). Benchmark de-
Behind Diego‚Äôs proficiency in developing itineraries is tailsareinAppendixE.
his iterative planning pipeline (depicted in Fig. 13). The
process begins with Diego creating an initial draft plan 5.1.AutomatedDataandAnnotationCollection
for the first activity using GPT-4, taking into account the
To allow our V-IRL benchmarks to scale globally, we de-
user‚Äôs biography, requirements, and previous activities in
velop an automatic data/annotation construction pipeline
working memory. This draft is then meticulously refined.
instead of crawling and manually annotating limited data.
First, a hierarchical coordination module re-
This allows models to be conveniently tested worldwide,
trieves real transportation time and asks a recommenda-
providedthereisaccesstoGoogleStreetViews[24].
tion agent for dining recommendations. Subsequently, an
interoceptive estimation module evaluates the RegionSelection.Thoughourbenchmarkisfeasibleacross
effect of the proposed activity on the user‚Äôs mental/physi- allregionscoveredbytheGMP,weselect14districtsacross
calstateandbudget. 12citiesfrom6continentstoensurecoverageofadiverse
9
ML
LOCdata distribution while keeping inference costs affordable. baseline,CLIP(w/GLIPproposal),whichinvolvesreclas-
ThedetailedlocationsoftheseregionsarelistedinTab.2. sifyingthecategoriesofGLIPproposalswithCLIP[51].
Place Types. We collect place information in each re-
gionforall96placestypesannotatedbyGMP‚Ä†.OurV-IRL Evaluation. We evaluate the models based on localiza-
tion recall, which is quantified as Ntp , where N and
place: localization, recognition and VQA benchmarks are Ntp+Nfn tp
builtuponallorpartoftheseplacetypes. N fnrepresentsthenumberofcorrectlylocalizedplacesand
missedplaces,respectively.
VisionandPlaceDataCollection. Withineachregion,we
collectgeolocationswithavailablestreetviews,placeinfor-
Matching between Object Proposals and Places. As
mation,andplace-centricimages. DataCleaning. Though
mentionedin Sec.5.1,wedonotannotateboundingboxes
scalable,automateddatacollectioncanintroducenoisedue
forplacesoneachpotentialstreetviewimage. Suchhuman
totheabsenceofhumansupervision.Tothisend,wedesign
annotationdivergesfromourinitialmotivationofproviding
three automatic data cleaning strategies: i) distance-based plug-and-playandsensor-rich(V-IRL)benchmarks. Toas-
filteringtoexcludeplacesnoteasilyvisiblefromanystreet
signgroundtruthforeachobjectproposalinthisscenario,
views due to their distance; ii) human-review filtering to
wedevelopasimplematchingstrategytoassignobjectpro-
remove ‚Äúzombie‚Äù places with no reviews which might no
posalsfromstreetviewobjectdetectionstonearbyplaces.
longerbevalidorrelevant;andiii)CLIP-basedfilteringto
retain only place-centric images with a high CLIP likeli- As illustrated in Fig. 14, we first project the bounding
hoodofbeingstorefronts. boxofeachobjectproposalontoafrustuminthe3Dspace,
subjecttoaradius. Wethendetermineifanynearbyplaces
Continent City District fall within this frustum and radius. If any nearby place is
Johannesburg Rosebank found, the closest one is assigned as the ground truth for
Africa
Lagos Surulere the object proposal. Otherwise, the object proposal is re-
Mumbai Khar gardedasafalsepositive. Whenmultipleplacesareinside
NewDelhi LajpatNagar thefrustum,weconsiderthenearestoneasthegroundtruth
Asia
HongKong PrinceEdward since it would likely block the others in the image. This
Tokyo Shinjuku process is also used in Intentional Explorer agent Hiro to
Melbourne CBD parseobjectproposalsonimagetoplaceinformation.
Australia
Melbourne SouthBank
Milan Brera
Europe
London OxfordSt
NewYorkCity Chinatown,Manhattan
NorthAmerica NewYorkCity SoHo,Manhattan
SanFrancisco UnionSquare
SouthAmerica BuenosAires Monserrat
Table2.RegionlistforglobalV-IRLbenchmarks.
5.2.V-IRLPlace: Localization
Figure14.Matchingbetween2Dobjectproposalandstreetplace.
Every day, humans traverse cities, moving between varied
places to fulfill a range of goals, like the Intentional Ex-
Results. Tab. 3 shows that open-world detectors like
ploreragent(Sec.3.3).Weassesstheperformanceofvision
GroundingDINO[43],Owl-ViT[47]andGLIP[37]arebi-
modelsontheeverydayhumanactivityoflocalizingplaces
ased towards certain place types such as school, cafe,
usingstreetviewimageryandassociatedplacedata.
and convenience store, respectively. In contrast,
Setups.WemodifyRX-399(Sec.3.3)totraversepolygonal CLIP (w/ GLIP proposal) can identify a broader spectrum
areaswhilelocalizing&identifying20typesofplaces. We ofplacetypes.Thisismainlycausedbythecategorybiasin
subsample28polygonalareasfromthe14districts. objectdetectiondatasetswithalimitedvocabulary. Hence,
even if detectors like Owl-ViT are initialized with CLIP,
BenchmarkedModels.Weevaluatethreeprominentopen-
their vocabulary space narrows down due to fine-tuning.
worlddetectionmodels: GroundingDINO[43], GLIP[37]
These results suggest that cascading category-agnostic ob-
and Owl-ViT [47]. We also implement a straightforward
ject proposals to zero-shot recognizers appears promising
‚Ä†https://developers.google.com/maps/documentation/places/web- for‚Äúreal‚Äùopen-worldlocalization‚Äîespeciallyforlesscom-
service/supported_types/#table1 moncategoriesinobjectdetectiondatasets.
10PlaceTypes AR10AR20
GroundingDINO[43] 0.0 0.0 0.0 0.0 0.0 7.8 0.0 0.0 16.8 0.0 2.5 1.2
Owl-ViT[47] 0.0 58.0 0.0 0.0 6.4 1.6 0.9 0.0 0.0 0.0 6.7 4.4
GLIP[37] 24.6 0.0 19.2 0.0 0.0 0.0 16.6 0.0 0.0 0.0 6.0 3.7
CLIP[51](w/GLIPproposal)58.5 8.8 28.841.233.623.013.025.0 0.0 14.5 24.6 20.1
Table3. BenchmarkresultsonV-IRLPlaceLocalization. AR10
andAR20denoteaveragerecallonsubsampled10andall20place
categories,respectively.FullresultsinAppendixE.1.
5.3.V-IRLPlace: RecognitionandVQA
IncontrasttothechallengingV-IRLplacelocalizationtask
using street view imagery alone, in real life, humans can
recognizebusinessesbytakingacloser,place-centriclook.
Weassessexistingvisionmodelsinthismannerontwoper-
Figure 16. Top row: examples of street view imagery. Bottom
ceptiontasksbasedonplace-centricimages: i)recognizing
row:correspondingplace-centricimages.
specificplacetypes;ii)identifyinghumanintentionsviaVi-
sionQuestionAnswering(VQA),dubbed‚ÄúintentionVQA‚Äù. s/vehicles. In contrast, place-centric images, drawn from
the Google Place database¬ß, are taken on foot and focus
Setups. Forrecognition,weassess10open-worldrecogni-
morecloselyonthespecificplace‚Äîprovidingamorecon-
tionmodelsonidentifyingaplace‚Äôstype(from96options)
centratedview.
usingplace-centricimages(seeTab.4).ForintentionVQA,
we evaluate 8 multi-modal large language models (MM- Evaluation. We adopt mean accuracy (mAcc) to evaluate
LLM) to determine viable human intentions from a four- bothplacerecognitionandVQAtasks. ForplaceVQA,we
option multiple-choice. The V-IRL Place VQA process is follow MMBench [44] to conduct circular evaluation and
illustratedinFig.15,wherethecandidateandtruechoices GPT-assistedanswerparsing.
aregeneratedbyGPT-4[2]giventheplacetypesandplace
Model #Param mAcc(%)
namescorrespondingtotheimage.
V-IRLPlaceRecognition
CLIP[51] ViT-B/32 151M 18.2
CLIP[51] ViT-L/14 428M 37.2
CLIP[51] ViT-L/14@336px 428M 41.3
OpenCLIP[16] ViT-B/32 151M 21.2
OpenCLIP[16] ViT-L/14 428M 31.0
Eva-02-CLIP[64] ViT-B/16 150M 19.5
Eva-02-CLIP[64] ViT-L/14 428M 34.2
Eva-02-CLIP[64] ViT-L/14@336px 428M 40.7
SigLIP[81] ViT-B/16 203M 29.5
SigLIP[81] ViT-L/16@384px 652M 37.3
V-IRLPlaceVQA
MiniGPT-4[83] Vicuna-13B-v0 14.0B 3.9
mPLUG-Owl[78] LLaMA-7B 7.2B 5.5
Shikra[15] Vicuna-7B 7.2B 10.9
Question:Which human intentions can be accomplished here? BLIP-2[36] FlanT5XXL 12.1B 69.6
Choices: A. Learning how to cook authentic Australian food. InstructBLIP[17] FlanT5XXL 12.0B 68.0
B. Applying for a reduction on parking fines. LLaVA[42] Vicuna-13B-v1.3 13.4B 23.5
C. Reporting a crime or lost property.
LLaVA-1.5[41] Vicuna-7B-v1.5 7.2B 60.1
D. Attending a yoga session.
LLaVA-1.5[41] Vicuna-13B-v1.5 13.4B 61.9
Figure15.ExampleofV-IRLPlaceVQAprocess.
Table4.BenchmarkresultsonV-IRLPlacerecognitionandV-IRL
Place-centricImagesvs.StreetViewImages. Incontrast PlaceVQA. Green indicatesincreasedresolutionmodels,while
tothestreetviewimageryutilizedintheV-IRLPlacelocal- Blue denotesmodelparameterscaling.
ization benchmark, the V-IRL Place recognition and VQA
benchmarksuseplace-centricimages. Toillustratethedis- Results. Tab. 4 shows that CLIP (L/14@336px) outper-
tinctionbetweentheseimagetypes,wepresentexamplesin formseventhebiggestversionofEva-02-CLIPandSigLIP
Fig.16. Thefigureshowsthatstreetviewimages,sourced intheV-IRLrecognitiontask,highlightingthehigh-quality
fromtheGoogleStreetViewdatabase‚Ä°,aretakenfromthe datausedtotrainCLIP[51]. Thebottomofthetableshows
street and encompass a broader view of the surroundings, thatBLIP2[36],InstructBLIP[17],andLLaVA-1.5[41]ex-
includingmultiplebuildingsandpossibleoccludingobject- celatintentionVQA,whereasothersstruggle. Wenotethat
‚Ä°https://developers.google.com/maps/documentation/streetview/request- ¬ßhttps://developers.google.com/maps/documentation/places/web-
streetview service/photos
11Recognition Localization
Start Intersection Stop
SF NYC Lo Mnd ilo an n N Me uw m D be al ih HiT Kokyo SF NYC Lo Mnd ilo an n N Me uw m D be al ih HiT Kokyo Method Success Reac Arr Reac Arr Reac
Lagos Lagos
Oracle(NoVision) 1.0 1.0 1.0 1.0 1.0 1.0
Bs. AiresJohannesburg Melbourne Bs. AiresJohannesburg Melbourne
CLIP(B/32)[51] 0.22 1.0 0.86 0.84 0.83 0.22
CLIP(L/14@336px)[51] 0.44 0.83 0.73 0.94 0.67 0.44
EVA-02-CLIP(BigE/14-plus)[64] 0.39 0.89 0.77 0.94 0.72 0.39
Acc. Acc.
EVA-02-CLIP(L/14@336px)[64] 0.22 1.0 0.82 0.83 0.78 0.22
0.3 0.35 0.4 0 0.02 0.04 0.06 0.08 0.1
Visual Question Answering English Speakers LLaVA-1.5-13B [41] 0.11 0.61 0.55 1.0 0.56 0.11
London PP-OCR [20](+GPT3.5) 0.28 0.89 0.73 0.94 0.72 0.28
SF NYC Milan New DelhiTokyo
MumbaiHK
Lagos Table5.ResultsonV-IRLVLN-mini.WetestvariousCLIP-based
Bs. AiresJohannesburg Melbourne models, MMLLM,and OCR modelwithGPTpostprocessing.
25metersofthedestination. Inaddition,asnavigationsuc-
Acc. % Eng. Speakers cessismainlyinfluencedbytheagent‚Äôsactionsatkeyposi-
0.38 0.4 0.42 0.44 0.46 20 40 60 80 100
tions(i.e.,startpositions,intersectionsandstoppositions),
Figure17.City-levelvisualizationofV-IRLbenchmarkresults. we also evaluate the arrival ratio (Arr) and reaction accu-
racy (Reac) for each route. Arr denotes the percentage of
these three top-performing MM-LLMs provide consistent
key positions reached, while Reac measures the accuracy
answers in the circular evaluation, while others frequently
oftheagent‚Äôsactionpredictionsatthesekeypositions. To
failduetoinconsistentselections. Moreover,visionmodels
saveGPT-4resources, wemainlycomparevisionmodules
perform better on intention VQA over place-type recogni-
ona10%mini-setcomprising18routesfrom9regions.See
tion,suggestingdirectpromptsabouthumanintentioncould
AppendixE.3forfull-setresultswithCLIPandOracle.
be more effective for intention-driven tasks. We provide
place-typeperspectiveanalysisinAppendixE.2. Results. Table 5 shows that, with oracle landmark in-
formation, powerful LLMs can impressively comprehend
5.4.V-IRLVision-LanguageNavigation navigation instructions and thus make accurate decisions.
However, when relying on vision models to fetch land-
As discussed in Sec. 3.3, Intentional Explorer and Tourist
markinformationfromstreetviews,thesuccessratedrops
agentsrequirecoordinationbetweenvisionmodelsandlan-
dramatically‚Äîsuggesting that the perception of vision
guage models to accomplish vision-language tasks. To in-
models is noisy and misguides LLMs‚Äô decision-making.
vestigate the effect of various models on end-to-end agent
Amongtheserecognizers,largervariantsofCLIP[51]and
performance,wedevelopanembodiedtaskthatjointlytests
EVA-02-CLIP[64]performbetter,highlightingthebenefits
vision and language models: Vision-Language Navigation
of model scaling. LLaVA-1.5 [41] shows inferior perfor-
(VLN).InVLN,agentsnavigatetoadesireddestinationby
mancewithCLIP(L/14@336px)asitsvisionencoder,pos-
followingtextualdirectionsusingonlyrawstreetviews.
siblyduetothealignmenttax[2]introducedduringinstruc-
Setup. WeadopttheTouristimplementationfromSec.3.4
tiontuning. Further, PP-OCR[20](+GPT-3.5)achievesa
andswapitsrecognitioncomponentwiththevariousbench-
28%successrate,signifyingthatOCRiscrucialforvisual
marked models. These models are used to identify visual
landmarkrecognition.
landmarksduringnavigation. Subsequently,GPT-4[2]pre-
dicts the next action according to the recognition results. 5.5.GeographicDiversity
NavigationinstructionsaregeneratedusingtheLocalagent.
Spanning12citiesacrosstheglobe,ourV-IRLbenchmarks
Recent work VELMA [59] attempts to enhance VLN by
provide an opportunity to analyze the inherent model bi-
leveraging LLMs on existing datasets [14, 58]. In con-
ases across different regions. As depicted in Fig. 17, vi-
trast, our V-IRL VLN benchmark evaluates vision models
sion models demonstrate subpar performance on all three
andtheircoordinationwithlanguagemodelsacrossaglobal
benchmarktasksinLagos,Tokyo,HongKong,andBuenos
datascale. SeemoredetailsinAppendixE.3.
Aires.VisionmodelsmightstruggleinLagosduetoitsnon-
Benchmarked methods. Four approaches are evaluated traditionalstreetviewsrelativetomoredevelopedcities(see
to recognize landmarks during navigation: (i) Oracle that street views in Fig. 1). For cities like Tokyo, Hong Kong,
searches nearby landmarks with GMP [24]; (ii) Zero-shot andBuenosAires,anintriguingobservationistheirprimary
recognizersCLIP[51]&EVA-CLIP[64];(iii)Multi-modal use of non-English languages in street views, as shown in
LLMLLaVA-1.5[41]; (iv)AnOCRmodel[20]toextract Fig.17bottomright¬∂ andFig.1. Thissuggeststhatexist-
text in street views followed by GPT answer parsing. Im- ing vision models may face challenges when deployed in
plementationdetailsareprovidedinAppendixE.3. non-English-dominantcountries.
Evaluation. Weprimarilymeasurenavigationsuccessrate ¬∂Source: https://en.wikipedia.org/wiki/List_of_countries_by_English-
(Success),definingsuccessasthenavigatorstoppingwithin speaking_population
126.Discussion: Ethics&Privacy References
Our platform serves as a tool for AI development and as a [1] Image geo-localization based on multiplenearest neighbor
crucibleforethicaldiscourseandpreparation. AsAIisin- featurematchingusinggeneralizedgraphs.TPAMI,2014.13
evitablybeingintegratedintosociety‚Äîe.g.,viaaugmented [2] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ah-
reality wearables, spatial computing platforms, or mobile mad,IlgeAkkaya,FlorenciaLeoniAleman,DiogoAlmeida,
robots navigating city streets‚Äîit is imperative to confront JankoAltenschmidt, SamAltman, ShyamalAnadkat, etal.
GPT-4 technical report. arXiv preprint arXiv:2303.08774,
anddiscussethicalandprivacyconcernsnow. Unlikethese
2023. 2,9,11,12,18
impendingreal-timesystems,thedataaccessedbyV-IRLis
[3] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine
‚Äústale‚Äù and preprocessed‚Äîproviding a controlled environ-
Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,
menttostudytheseconcerns.
KatherineMillican, MalcolmReynolds, etal. Flamingo: a
Notably, V-IRL exclusively utilizes preexisting, read-
visual language model for few-shot learning. In NeurIPS,
ily available APIs; it does not capture or make available
2022. 2
any previously inaccessible data. Our primary source of
[4] Peter Anderson, Angel Chang, Devendra Singh Chaplot,
street-viewimagery,GoogleMaps[24],issubjecttomajor
Alexey Dosovitskiy, Saurabh Gupta, Vladlen Koltun, Jana
privacy-protection measures, including blurring faces and
Kosecka,JitendraMalik,RoozbehMottaghi,ManolisSavva,
license plates [22]. Moreover, V-IRL complies with the etal. Onevaluationofembodiednavigationagents. arXiv
Google Maps Platform license||, similarly to notable exist- preprintarXiv:1807.06757,2018. 2
ingworksthatalsoleverageGoogle‚Äôsstreetviews[1,14]. [5] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark
We believe V-IRL is an invaluable tool for researching Johnson, Niko S√ºnderhauf, Ian Reid, Stephen Gould, and
bias. As discussed in Sec. 5.5, V-IRL‚Äôs global scale pro- AntonVanDenHengel. Vision-and-LanguageNavigation:
vides a lens to study linguistic, cultural, and other geo- Interpretingvisually-groundednavigationinstructionsinreal
graphicbiasesinherentinmodels. ByusingV-IRLtostudy environments. InCVPR,2018. 2
such questions, we aim to preemptively tackle the ethi- [6] Rohan Anil, Andrew M Dai, Orhan Firat, Melvin John-
cal dilemmas that will arise with deploying real-time sys- son, Dmitry Lepikhin, Alexandre Passos, Siamak Shakeri,
tems rather than being blindsided by them. We hope our EmanuelTaropa,PaigeBailey,ZhifengChen,etal. PaLM2
technicalreport. arXivpreprintarXiv:2305.10403,2023. 2
work helps spur proactive discussion of future challenges
throughoutthecommunity. [7] YukiMAsano,ChristianRupprecht,AndrewZisserman,and
AndreaVedaldi. PASS:Animagenetreplacementforself-
7.Conclusion supervisedpretrainingwithouthumans. InNeurIPS,2021.
2
Inthiswork,weintroduceV-IRL,anopen-sourceplatform [8] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,Yusuf
designedtobridgethesensorygapbetweenthedigitaland Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton,
physicalworlds,enablingAIagentstointeractwiththereal SamirGadre,ShioriSagawa,etal.OpenFlamingo:Anopen-
worldinavirtualyetrealisticenvironment.ThroughV-IRL, source framework for training large autoregressive vision-
language models. arXiv preprint arXiv:2308.01390, 2023.
agentscandeveloprichsensorygroundingandperception,
2
utilizing real geospatial data and street-view imagery. We
[9] Christopher Berner, Greg Brockman, Brooke Chan, Vicki
demonstrate the platform‚Äôs versatility by creating diverse
Cheung, Przemys≈Çaw DeÀõbiak, Christy Dennison, David
exemplaragentsanddevelopingbenchmarksmeasuringthe
Farhi, Quirin Fischer, Shariq Hashme, Chris Hesse, et al.
performanceoffoundationallanguageandvisionmodelson
Dota2withlargescaledeepreinforcementlearning. arXiv
open-worldvisualdatafromacrosstheglobe.
preprintarXiv:1912.06680,2019. 2
ThisplatformopensnewavenuesforadvancingAIcapa-
[10] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen
bilitiesinperception,decision-making,andreal-worlddata
Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding,
interaction. As spatial computing and robotic systems be-
DannyDriess,AvinavaDubey,ChelseaFinn,PeteFlorence,
comeincreasinglyprevalent,thedemandforandpossibili- ChuyuanFu,MontseGonzalezArenas,KeerthanaGopalakr-
tiesofAIagentswillonlygrow.Frompersonalassistantsto ishnan, Kehang Han, Karol Hausman, Alex Herzog, Jas-
practical applications like urban planning to life-changing mine Hsu, Brian Ichter, Alex Irpan, Nikhil Joshi, Ryan
toolsforthevisuallyimpaired,wehopeV-IRLhelpsusher Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal,
inaneweraofperceptuallygroundedagents. Lisa Lee, Tsang-Wei Edward Lee, Sergey Levine, Yao Lu,
Henryk Michalewski, Igor Mordatch, Karl Pertsch, Kan-
ishkaRao,KristaReymann,MichaelRyoo,GreciaSalazar,
Pannag Sanketi, Pierre Sermanet, Jaspiar Singh, Anikait
Singh,RaduSoricut,HuongTran,VincentVanhoucke,Quan
Vuong,AyzaanWahid,StefanWelker,PaulWohlhart,Jialin
Wu, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu,
||https://cloud.google.com/maps-platform/terms andBriannaZitkovich. RT-2: Vision-language-actionmod-
13elstransferwebknowledgetoroboticcontrol.arXivpreprint [24] Google Map Team. Google Map Platform. https://
arXiv:2307.15818,2023. 2 mapsplatform.google.com/. 8,9,12,13
[11] Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol [25] Shixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey
Hausman,AlexanderHerzog,DanielHo,JulianIbarz,Alex Levine. Deep reinforcement learning for robotic manipu-
Irpan,EricJang,RyanJulian,etal. DoAsICan,NotAsI lationwithasynchronousoff-policyupdates.InICRA,2017.
Say: Groundinglanguageinroboticaffordances. InCoRL, 2
2023. 1,2 [26] SiruiHong,MingchenZhuge,JonathanChen,XiawuZheng,
[12] AngelChang,AngelaDai,ThomasFunkhouser,MaciejHal- Yuheng Cheng, Ceyao Zhang, Jinlin Wang, Zili Wang,
ber,MatthiasNiessner,ManolisSavva,ShuranSong,Andy Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu
Zeng,andYindaZhang.Matterport3D:Learningfromrgb-d Ran,LingfengXiao,ChenglinWu,andJ√ºrgenSchmidhuber.
datainindoorenvironments. In3DV,2017. 2 MetaGPT:MetaProgrammingforAMulti-AgentCollabora-
[13] DevendraSinghChaplot,DhirajPrakashchandGandhi,Ab- tiveFramework. InICLR,2023. 2
hinavGupta,andRussRSalakhutdinov.Objectgoalnaviga- [27] Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor
tionusinggoal-orientedsemanticexploration. InNeurIPS, Mordatch. Language Models As Zero-Shot Planners: Ex-
2020. 2 tracting actionable knowledge for embodied agents. In
ICML,2022. 1,2
[14] HowardChen,AlaneSuhr,DipendraMisra,NoahSnavely,
[28] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky
andYoavArtzi. TOUCHDOWN:Naturallanguagenaviga-
Liang, PeteFlorence, AndyZeng, JonathanTompson, Igor
tionandspatialreasoninginvisualstreetenvironments. In
Mordatch, YevgenChebotar, etal. InnerMonologue: Em-
CVPR,2019. 12,13
bodiedreasoningthroughplanningwithlanguagemodels.In
[15] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang,
CoRL,2022. 1,2
Feng Zhu, and Rui Zhao. Shikra: Unleashing multi-
[29] Wenlong Huang, Chen Wang, Ruohan Zhang, Yunzhu Li,
modal llm‚Äôs referential dialogue magic. arXiv preprint
JiajunWu,andLiFei-Fei. VoxPoser: Composable3dvalue
arXiv:2306.15195,2023. 11
mapsforroboticmanipulationwithlanguagemodels. arXiv
[16] MehdiCherti,RomainBeaumont,RossWightman,Mitchell
preprintarXiv:2307.05973,2023. 2
Wortsman,GabrielIlharco,CadeGordon,ChristophSchuh-
[30] HeinrichK√ºttler,NantasNardelli,AlexanderMiller,Roberta
mann,LudwigSchmidt,andJeniaJitsev.Reproduciblescal-
Raileanu, Marco Selvatici, Edward Grefenstette, and Tim
inglawsforcontrastivelanguage-imagelearning. InCVPR,
Rockt√§schel.Thenethacklearningenvironment.InNeurIPS,
2023. 2,11
2020. 2
[17] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat
[31] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Ste-
Fung, and Steven Hoi. InstructBLIP: Towards General-
fan Popov, Matteo Malloci, Alexander Kolesnikov, et al.
purpose Vision-Language Models with Instruction Tuning.
TheOpenImagesDatasetV4: Unifiedimageclassification,
InNeurIPS,2023. 11,20
object detection, and visual relationship detection at scale.
[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
IJCV,2020. 2
andLiFei-Fei. ImageNet: Alarge-scalehierarchicalimage
[32] HugoLauren√ßon,LucileSaulnier,L√©oTronchon,StasBek-
database. InCVPR,2009. 2
man,AmanpreetSingh,AntonLozhkov,ThomasWang,Sid-
[19] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey
dharthKaramcheti,AlexanderMRush,DouweKiela,etal.
Lynch,AakankshaChowdhery,BrianIchter,AyzaanWahid,
OBELICS:Anopenweb-scalefiltereddatasetofinterleaved
Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong image-textdocuments. InNeurIPS,2023. 2
Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duck-
[33] Alexander C Li, Ellis Brown, Alexei A Efros, and Deepak
worth,SergeyLevine,VincentVanhoucke,KarolHausman,
Pathak. Internetexplorer: Targetedrepresentationlearning
Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch,
ontheopenweb. InInternationalConferenceonMachine
andPeteFlorence.PaLM-E:AnEmbodiedMultimodalLan-
Learning,pages19385‚Äì19406.PMLR,2023. 2
guageModel. InICML,2023. 2
[34] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
[20] YDu,CLi,RGuo,XYin,WLiu,JZhou,YBai,ZYu,Y Koltun, and Rene Ranftl. Language-driven semantic seg-
Yang,QDang,etal. PP-OCR:Apracticalultralightweight mentation. InICLR,2022. 2
ocr system. arxiv 2020. arXiv preprint arXiv:2009.09941, [35] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani,
2020. 12,23 DmitriiKhizbullin, andBernardGhanem. CAMEL:Com-
[21] Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland, municativeagentsfor"mind"explorationoflargelanguage
and Dhruv Mahajan. Adaptive methods for real-world do- modelsociety. InNeurIPS,2023. 2
maingeneralization. InCVPR,2021. 2 [36] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
[22] AndreaFrome,GermanCheung,AhmadAbdulkader,Marco BLIP-2: bootstrapping language-image pre-training with
Zennaro, Bo Wu, Alessandro Bissacco, Hartwig Adam, frozenimageencodersandlargelanguagemodels.InICML,
HartmutNeven,andLucVincent. Large-scaleprivacypro- 2023. 8,11
tectioningooglestreetview. InICCV,2009. 13 [37] LiunianHaroldLi,PengchuanZhang,HaotianZhang,Jian-
[23] GolnazGhiasi,XiuyeGu,YinCui,andTsung-YiLin. Scal- wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
ing open-vocabulary image segmentation with image-level Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded
labels. InECCV,2022. 2 language-imagepre-training. InCVPR,2022. 2,8,10,11
14[38] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol [52] Vikram V Ramaswamy, Sing Yu Lin, Dora Zhao,
Hausman,BrianIchter,PeteFlorence,andAndyZeng.Code Aaron Bryan Adcock, Laurens van der Maaten, Deepti
asPolicies:Languagemodelprogramsforembodiedcontrol. Ghadiyaram, and Olga Russakovsky. GeoDE: a geograph-
InICRA,2023. 2 ically diverse evaluation dataset for object recognition. In
[39] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco NeurIPS,2023. 2
Pavone, and Jeannette Bohg. Text2Motion: From natu- [53] WilliamAGaviriaRojas, SudnyaDiamos, KeertanRanjan
ral language instructions to feasible plans. arXiv preprint Kini, David Kanter, Vijay Janapa Reddi, and Cody Cole-
arXiv:2303.12153,2023. 2 man. The Dollar Street Dataset: Images representing the
[40] PhilippLindenberger,Paul-EdouardSarlin,andMarcPolle- geographic and socioeconomic diversity of the world. In
feys. LightGlue:LocalFeatureMatchingatLightSpeed. In NeurIPS,2022. 2
ICCV,2023. 8 [54] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets,
Yili Zhao, Erik Wijmans, Bhavana Jain, Julian Straub, Jia
[41] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
Liu,VladlenKoltun,JitendraMalik,etal. Habitat: Aplat-
Lee. Improved baselines with visual instruction tuning.
formforembodiedairesearch. InICCV,2019. 2
arXiv:2310.03744,2023. 11,12,23
[55] Timo Schick, Jane Dwivedi-Yu, Roberto Dess√¨, Roberta
[42] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Can-
Visualinstructiontuning. InNeurIPS,2023. 11,20
cedda,andThomasScialom. Toolformer:Languagemodels
[43] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao
canteachthemselvestousetools. InNeurIPS,2023. 2
Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu,Jun
[56] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Zhu,etal. GroundingDINO:Marryingdinowithgrounded
Cade W Gordon, Ross Wightman, Mehdi Cherti, Theo
pre-training for open-set object detection. arXiv preprint
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
arXiv:2303.05499,2023. 10,11
man,PatrickSchramowski,SrivatsaRKundurthy,Katherine
[44] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,Songyang
Crowson,LudwigSchmidt,RobertKaczmarczyk,andJenia
Zhang,WangboZhao,YikeYuan,JiaqiWang,ConghuiHe,
Jitsev. LAION-5b: Anopenlarge-scaledatasetfortraining
ZiweiLiu,etal. MMBench:IsYourMulti-modalModelan
nextgenerationimage-textmodels. InNeurIPS,2022. 2
All-aroundPlayer? arXivpreprintarXiv:2307.06281,2023.
[57] JohnSchulman, BarretZoph, ChristinaKim, JacobHilton,
11
JacobMenick, JiayiWeng, JuanFelipeCeronUribe, Liam
[45] ZeyiLiu,ArpitBahety,andShuranSong. REFLECT:Sum- Fedus,LukeMetz,MichaelPokorny,etal. ChatGPT:Opti-
marizingrobotexperiencesforfailureexplanationandcor- mizinglanguagemodelsfordialogue. OpenAIblog, 2022.
rection. InCoRL,2023. 2 23
[46] Viktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, [58] RaphaelSchumannandStefanRiezler.Generatinglandmark
Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, navigation instructions from maps as a graph-to-text prob-
Nikita Rudin, Arthur Allshire, Ankur Handa, et al. Isaac lem. InACL,2020. 12,18
Gym: High performance gpu-based physics simulation for
[59] RaphaelSchumann,WanrongZhu,WeixiFeng,Tsu-JuiFu,
robotlearning. InNeurIPS,2021. 2
StefanRiezler,andWilliamYangWang. VELMA:Verbal-
[47] MatthiasMinderer,AlexeyGritsenko,AustinStone,Maxim ization embodiment of llm agents for vision and language
Neumann,DirkWeissenborn,AlexeyDosovitskiy,Aravindh navigationinstreetview. arXivpreprintarXiv:2307.06082,
Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran 2023. 12,18,23
Shen,etal. SimpleOpen-VocabularyObjectDetectionwith [60] Hao Shao, Yuxuan Hu, Letian Wang, Steven L Waslander,
VisionTransformers. InECCV,2022. 2,10,11 Yu Liu, and Hongsheng Li. LMDrive: Closed-loop end-
[48] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex to-end driving with large language models. arXiv preprint
Graves, Ioannis Antonoglou, Daan Wierstra, and Martin arXiv:2312.07488,2023. 2
Riedmiller. Playingatariwithdeepreinforcementlearning. [61] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
arXivpreprintarXiv:1312.5602,2013. 2 Soricut. ConceptualCaptions: Acleaned,hypernymed,im-
[49] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, agealt-textdatasetforautomaticimagecaptioning. InACL,
LongOuyang,ChristinaKim,ChristopherHesse,Shantanu 2018. 2
Jain, Vineet Kosaraju, William Saunders, et al. WebGPT: [62] Noah Shinn, Beck Labash, and Ashwin Gopinath. Reflex-
Browser-assistedquestion-answeringwithhumanfeedback. ion: anautonomousagentwithdynamicmemoryandself-
arXivpreprintarXiv:2112.09332,2021. 2 reflection. InNeurIPS,2023. 2
[50] Joon Sung Park, Joseph O‚ÄôBrien, Carrie Jun Cai, Mered- [63] SignificantGravitas.AutoGPT.https://github.com/
ith Ringel Morris, Percy Liang, and Michael S Bernstein. Significant-Gravitas/AutoGPT,2023. 2
Generative Agents: Interactive simulacra of human behav- [64] QuanSun,YuxinFang,LedellWu,XinlongWang,andYue
ior. InUIST,2023. 2 Cao.EVA-CLIP:ImprovedTrainingTechniquesforCLIPat
[51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Scale. arXivpreprintarXiv:2303.15389,2023. 11,12
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, [65] Bart Thomee, David A Shamma, Gerald Friedland, Ben-
AmandaAskell,PamelaMishkin,JackClark,etal. Learn- jaminElizalde,KarlNi,DouglasPoland,DamianBorth,and
ingtransferablevisualmodelsfromnaturallanguagesuper- Li-JiaLi.YFCC100M:Thenewdatainmultimediaresearch.
vision. InICML,2021. 2,8,10,11,12 CommunicationsoftheACM,2016. 2
15[66] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, [80] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Boxin Li, Chunyuan Li, et al. Florence: A new
LLAMA 2: Open foundation and fine-tuned chat models. foundation model for computer vision. arXiv preprint
arXivpreprintarXiv:2307.09288,2023. 2,9 arXiv:2111.11432,2021. 2
[67] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, [81] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and
ChaoweiXiao,YukeZhu,LinxiFan,andAnimaAnandku- LucasBeyer. Sigmoidlossforlanguageimagepre-training.
mar. Voyager: An open-ended embodied agent with large arXivpreprintarXiv:2303.15343,2023. 11
language models. arXiv preprint arXiv:2305.16291, 2023. [82] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp
1,2 Kr√§henb√ºhl, and Ishan Misra. Detecting twenty-thousand
[68] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten classesusingimage-levelsupervision. InECCV,2022. 2
Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. [83] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
Chain-of-thought prompting elicits reasoning in large lan- hamedElhoseiny. MiniGPT-4: EnhancingVision-Language
guagemodels. InNeurIPS,2022. 2 Understanding with Advanced Large Language Models.
[69] Michael Wooldridge and Nicholas R Jennings. Intelligent arXivpreprintarXiv:2304.10592,2023. 11
Agents: Theory and practice. The knowledge engineering [84] XizhouZhu, YuntaoChen, HaoTian, ChenxinTao, Weijie
review,1995. 2 Su,ChenyuYang,GaoHuang,BinLi,LeweiLu,Xiaogang
[70] Penghao Wu and Saining Xie. V*: Guided Visual Search Wang, Yu Qiao, Zhaoxiang Zhang, and Jifeng Dai. Ghost
asaCoreMechanisminMultimodalLLMs. arXivpreprint in the Minecraft: Generally capable agents for open-world
arXiv:2312.14135,2023. 17 environments via large language models with text-based
[71] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, knowledgeandmemory. arXivpreprintarXiv:2305.17144,
ShaokunZhang,ErkangZhu,BeibinLi,LiJiang,Xiaoyun 2023. 2
Zhang, and Chi Wang. AutoGen: Enabling next-gen llm
applicationsviamulti-agentconversationframework. arXiv
preprintarXiv:2308.08155,2023. 2
[72] ZhihengXi,WenxiangChen,XinGuo,WeiHe,YiwenDing,
BoyangHong,MingZhang,JunzheWang,SenjieJin,Enyu
Zhou,etal. Theriseandpotentialoflargelanguagemodel
based agents: A survey. arXiv preprint arXiv:2309.07864,
2023. 1,3
[73] FeiXia,AmirRZamir,ZhiyangHe,AlexanderSax,Jitendra
Malik,andSilvioSavarese.GibsonEnv:Real-worldpercep-
tionforembodiedagents. InCVPR,2018. 2
[74] Fanbo Xiang, Yuzhe Qin, Kaichun Mo, Yikuan Xia, Hao
Zhu,FangchenLiu,MinghuaLiu,HanxiaoJiang,YifuYuan,
HeWang,etal.SAPIEN:Asimulatedpart-basedinteractive
environment. InCVPR,2020. 2
[75] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang,
RussellHowes,VasuSharma,Shang-WenLi,GargiGhosh,
LukeZettlemoyer,andChristophFeichtenhofer. Demystify-
ingclipdata. arXivpreprintarXiv:2309.16671,2023. 2
[76] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon,
ThomasBreuel,JanKautz,andXiaolongWang. GroupViT:
Semantic segmentation emerges from text supervision. In
CVPR,2022. 2
[77] ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,
Karthik Narasimhan, and Yuan Cao. ReAct: Synergizing
reasoningandactinginlanguagemodels. InICLR,2023. 2
[78] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,
Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi,
Yaya Shi, et al. mPLUG-Owl: Modularization empowers
large language models with multimodality. arXiv preprint
arXiv:2304.14178,2023. 11
[79] SriramYenamandra,ArunRamachandran,KarmeshYadav,
AustinWang,MukulKhanna,TheophileGervet,Tsung-Yen
Yang, Vidhi Jain, Alexander William Clegg, John Turner,
etal. HomeRobot: Open-vocabularymobilemanipulation.
InCoRL,2023. 2
16A.AppendixOutline B.3.Vivek: EstateAgent
Inthesesupplementarymaterials,weprovideadditionalde- The process employed by Vivek is similar to that of Aria,
tailsforourV-IRLplatform,including: asbotharedesignedtorecommendplaces. However,Vivek
showcasestheversatilityofourV-IRLplatformbydemon-
‚Ä¢ DesignsbehindV-IRLAgents(AppendixB);
stratinghowitcanseamlesslyintegrateawiderangeofre-
‚Ä¢ Technical details and challenges in the V-IRL environ-
alisticinformationbeyondtheGoogleMapsPlatform,with
ment(AppendixC).
a standardized definition of geographic coordinates. This
‚Ä¢ Alow-levelcasestudyofIntentionalExploreragentHiro,
capability enables the creation of even more sophisticated
delvingintoimplementationdetailsofoursystemsuchas
andintriguingagents.
LLMprompts(AppendixD);
‚Ä¢ More detailed setups and results for our V-IRL bench-
B.4.RX-399: UrbanAssistanceRobot
marks(AppendixE).
Different from previous example agents, RX-399 intro-
ducesvisualperceptioncapabilitiessuchasopen-worldde-
B.TechnicalDetailsofV-IRLAgents
tection and feature matching. There are two fundamental
systems inside it ‚Äì navigation and perception. In terms
In Sec. 3, our discussion mainly focuses on the innovative
ofnavigation,RX-399canautomaticallynavigatefromthe
capabilities and behaviors of V-IRL agents empowered by
currentpositiontothepre-defineddestinationstepbystep.
ourplatform.Weavoidin-depthdiscussionsabouttechnical
This navigation process is elaborated in Appendix C.2.2,
detailsinthemainpaperduetotheconcernofreadability.
andthus,willnotbeextensivelydiscussedhere.
In this section, we go through our main technical designs
When it comes to its perception system, RX-399 is de-
foreachagent. Morecomprehensivetechnicalimplementa-
signed to simulate human visual perception by capturing
tionsareavailableinourreleasedcode.
streetviewswithina90-degreehorizontalangletobothits
leftandright.Foreachcapturedview,anopen-worlddetec-
B.1.Peng: RouteOptimizer
tionprocessisconducted. Leveragingtheinteractivecapa-
bilities of our environment, we further propose an active
Peng is designed to showcase the utilization of real geo-
detection strategy to dynamically adjust the agent‚Äôs ego-
graphic coordinates within our platform. By processing
pose and focal length according to the scale and position
a sequence of real addresses, Peng calculates the shortest
of potential objects. This significantly improves its per-
pathfortraversingthemusingvariousmodesoftransporta-
formance as illustrated in Tab. 6. In the future, more ad-
tion, such as walking, driving, and bicycling, among oth-
vanced approaches such as visual search [70] could also
ers. This capability is powered by the mapping module
beconsidered. Inthesubsequentde-duplicationprocedure,
described in Appendix C.3. After that, Peng proceeds to
whichaimstoavoiddouble-countingobjectsacrossdiffer-
navigate through the destinations along the predetermined
ent viewpoints, we have tried a few strategies including
path,employingthepointnavigationprocedureoutlinedin
measuring with multi-view geometry, object tracking, and
AppendixC.2.2.
feature matching. We choose feature matching because of
itsaccuracyandefficiency.
B.2.Aria: PlaceRecommender
City HongKong NewYorkCity
Aria leverages the realistic place information provided by w/activedetection 0.63/0.83 0.71/1.00
ourPlaceInfo&Searchmodule(seeAppendixC.4)toen- w/oactivedetection 0.10/0.33 0.30/0.60
hanceLLMs‚Äôreasoningcapabilityinthegeographicaspect.
Table 6. RX-399 detection performance with or without active
Specifically, Aria evaluates Peng‚Äôs intention to determine
detectionmanner.Metricsareaccuracy/recall.
the suitable type of place and searches all possible places
in the vicinity. For each searched place, Aria considers its
B.5.Imani: UrbanPlanner
reviewsanduserratingsfromGoogletosummarizeaplace
overview. Subsequently,wecustomizepromptsforAriato ThevisualperceptionsystemofImanimirrorsthatofRX-
amalgamate Peng‚Äôs biography, intentions, and the summa- 399. The primary distinction between them lies in their
rizedplaceoverviewstorateeachplacebetween0and10, navigationsystems. Imanipossessesthecapabilitytoplan
accompaniedbyjustifications. a navigation route in the given polygonal region, enabling
Without such technical designs, LLMs could recom- RX-399totraversethatregion. Thisfunctionalityisnamed
mendsomeplacesthatareeithertoodistantorpermanently ‚Äúregionnavigation‚ÄùandelaboratedinAppendixC.2.2. Ad-
closed. This issue arises because LLMs struggle to accu- ditionally, within the Imani agent, we develop a heatmap
ratelyunderstandgeospatialrelationshipsandoftendepend visualizationtooltovisualizeandverifythedatacollected
onoutdateddatabases. byRX-399(seeFig.3).
17B.6.Hiro: IntentionalExplorer using GPT-4(V) [2], where it receives a rating between 0
and10alongwithexplanatoryreasons.
Hiro is a representative agent equipped with geographical,
perceptual,andreasoningabilities,toaddressadailyhuman
C.TechnicalDetailsofEnvironment
task: randomly exploring to find a suitable restaurant. In
thisregard,wehavededicatedaseparatesectiontoofferan InSec.4.2.1,weprovideanoverviewofoursystem‚Äôsenvi-
in-depthcasestudy,includingthedetailedmethodologyand ronment,whichgroundsagentsinreallife. Here,wedelve
promptsinAppendixD. into the technical designs beyond mere leveraging Google
MapPlatformsystemcalls. Concreteimplementationscan
B.7.Ling: Tourist
befoundinouropen-sourcedcode.
Our vision language navigation pipeline of Ling is sim-
C.1.Geolocation&StreetViewImagery
ilar to [59], leveraging vision models, the map, and
LLMs. At each position, we start by capturing eight AtthecoreofV-IRLliesitsinnovativeuseofsensor-richen-
street views around the agent, corresponding to front, vironment,includingstreetviewimageryandgeolocations.
left front,left,left behind,behind,right Theyenableagentstogathersurroundingplaceandvision
behind,rightandright front. Visionmodelsuse information.
thesestreetviewstoidentifylandmarksmentionedinroute
Geolocation. AgentsintheV-IRLplatforminhabitvirtual
descriptions,whicharethenverbalizedaslandmarkobser-
representationsofrealcitiesaroundtheglobe. Atthecore
vations. Also,intersectioninformationisretrievedfromthe
of this representation are geographic coordinates (i.e. ge-
movertoformulateanintersectionobservation. LLMsplay
olocation) corresponding to points on the Earth‚Äôs surface.
acrucialroleinprocessinglandmark&intersectionobser-
Theinitialgeolocationofeachagentisspecifiedbyits‚ÄúLo-
vations along with the agent‚Äôs previous working history to
cation‚Äù configuration, as illustrated in Fig. 12. Whenever
determinethenextaction. Aftereachaction,currentobser-
agentsrequireaccesstosurroundinginformation(e.g.street
vationsandactionsarestoredintotheagent‚Äôsworkinghis-
views, placesormaps), geolocationservesasacrucialpa-
tory. Thisauto-regressiveprocesscontinuesuntiltheagent
rameterforqueryingtherelatedGoogleMapAPIs.
decidestostop.
Streetviewimagery. GoogleMapPlatformspecifieseach
B.8.LocalAgent street view imagery with multiple key parameters: geolo-
cation, heading (the horizontal angle ranging from 0‚ó¶ to
The primary mission of the Local agent is to generate
360‚ó¶),pitch(averticalanglerangingfrom-90‚ó¶to90‚ó¶),and
human-likeandeasilyfollowablenavigationinstructionson
FieldofView(FOV,rangingfrom20‚àº120). It‚Äôsnotewor-
a global scale (refer to 3.4.1). This task is known as navi-
thy that adjusting the FOV here is similar to changing the
gation instruction generation [58]. Contrary to most exist-
camera‚Äôsfocallength,ratherthansimplyzoominginonan
ing research, which depends on human-annotated data for
image, which ensures that image resolution remains high,
limited geographic areas, our ‚ÄúLocal‚Äù agent automatically
even as the FOV decreases to a low value. By modifying
selects suitable landmarks taking account into real-world
the heading, pitch, and FOV, we can simulate the human
places and generates human-like route descriptions using
sensory process of adjusting one‚Äôs pose and concentrating
LLMsacrosstheglobe. Remarkably,itachievesthiswith-
onaspecificarea.
outtheneedforanytrainingdata,relyingsolelyonourtai-
loredpromptsandafewin-contextexamples.Theeffective- Alignmentbetweenstreetviewimageryandgeolocation.
nessofitsgeneratedinstructionshasbeenverifiedthrough Withinoursensor-richplatform,afundamentalchallengeis
collaboration with ‚ÄùLing‚Äù. To the best of our knowledge, toensureagentsarepositionedatgeolocationswherestreet
this is a first in the field. There are massive technical de- viewimageryisavailable. Toaddressthisissue,wedesign
tails on selecting easily noticeable landmarks and prompt a custom operation named ‚Äúrelocate‚Äù. Specifically, when
engineering,whichareavailableinourreleasedcode. an agent is initialized at a location lacking street view im-
agery, the ‚Äúrelocate‚Äù operation will identify and transition
B.9.Diego: InteractiveConcierge
theagenttothenearestviablegeolocationwherestreetview
InSec.4.4,wehavealreadypresentedthetechnicaldesigns dataisavailable.Noticethat,thisoperationisindispensable
of Diego‚Äôs itinerary. Here, we detail how Diego can find toourplatform,asthepositionswithavailablestreetviews
scenic locations as shown in Fig. 9. For any given des- are relatively sparse in comparison to the vast continuous
tination, such as ‚ÄúFort Tryon Park‚Äù, Diego will sample a spaceofallpossiblecoordinates.
rectangle region around it and traverse all navigable posi-
C.2.Movement
tions within it. At each position, he will capture a photo-
graph(i.e.streetviewimagery)usingpre-definedheadings, Enabling agents to move along city streets is a core func-
pitches,andFOVs. Eachphotographwillthenbeevaluated tionality of our platform, allowing interaction between
18agents and the real world. Whenever an agent needs to vicinity of the agent and employing the ‚Äúrelocate‚Äù opera-
move,thismodulepowersallrelatedprocesses,fromroute tion to sift through these locations, identifying those that
planning and direction selection to the continuous update arenavigable. Whilethismethodoffersamorecomprehen-
oftheagent‚Äôsgeolocationduringitsmoving. SinceGoogle siveviewofnavigablepositions,itismarkedlymoretime-
MapPlatformdoesnotprovideAPIstoaccessnearbynavi- consuming than the web-based approach due to the exten-
gablepositionsanddirections,thedesignofthismovement sivenumberofGoogleMapsAPIcallsrequired.
moduleisasignificanttechnicalchallengeandasubstantial
Inourpracticalapplications,wedesignaheuristicstrat-
contributionfromourteam. Wediscussitslow-levelimple-
egythatcombinesweb-basedcontrollingandgrid-basedre-
mentations in Appendix C.2.1 and the enabled high-level
location. This hybrid approach aims to balance the trade-
actionsinAppendixC.2.2.
offs between the speed and the completeness of navigable
position data, optimizing our agents‚Äô capabilities and effi-
C.2.1 Mover ciencyinreal-worldscenarios.
Movebycontrollingthewebinterface. Astraightforward
solutionistolettheagentcontrolthewebfront-endGoogle
Street View to select moving directions and move. Never- C.2.2 Navigator
theless,therearethreekeychallengesforthissolution:
(i) How can Python-implemented agents control the Here,weintroducethehigh-levelactionofagentspowered
movement via the interaction to the webpage? We use a by the mover ‚Äì navigation. Unlike the mover, which con-
PythonpackageSelenium**tolocatewebelementsrespon- centratesonenablingagentmobilityintheenvironment,the
sible for movement. After determining a movement direc- focushereshiftstodeterminingthedirectionofmovement.
tion, theagentusesSeleniumtosimulateaclickactionon Inourplatform,wegroupdifferentnavigatorsaccordingto
thewebelementcorrespondingtothechosendirection. theirusagesintofourtypes:
(ii)Howcantheagentacquirethenecessaryinforma-
(i) Point navigator is designed to tackle navigation
tiontodecidemovingdirection? Althoughagentscanac-
tasks that clearly define single or multiple destinations
cessallpotentialmovementdirectionsfromwebelements,
(represented in addresses or geolocations). This naviga-
they cannot identify these directions without prior knowl-
tor employs the route planning function described in Ap-
edgeofwhateachrepresents. Wefindthatthe‚Äútransform‚Äù
pendix C.3 to obtain a series of key positions for naviga-
attributeinthewebelementcorrespondingtoeachdirection
tion. Ateachlocation,theagentutilizesagreedyalgorithm
canbeleveragedtocalculatetheirrepresentedheadingan-
to select the most optimal direction towards the next key
gles. Theheadinganglealsoallowsustocollectstreetview
position that has not yet been reached. Exemplary agents,
imagery for each movement direction. Agent‚Äôs movement
such as ‚ÄúPeng‚Äù, ‚ÄúRX-399‚Äù and ‚ÄúLocal‚Äù, use this type of
decision-makingisthenbasedontheseheadinganglesand
navigatorintheirimplementation.
thevisualdatafromstreetviewimagery.
(ii)Regionnavigatoristailoredforagentslike‚ÄúImani‚Äù
(iii) How to track the agent‚Äôs geolocation along its
and ‚ÄúDiego‚Äù, who need to traverse every position within a
movement? To accomplish this, we customize a webpage
polygonalregion.Thisnavigatorfirstemploysagridsearch
elementtodisplaythegeolocationofthecurrentstreetview
combinedwithour‚Äúrelocate‚Äùoperationtoidentifyallnavi-
panorama. As the agents move and trigger updates to the
gablepositionswithinthespecifiedregion. Subsequently,it
streetviewpanorama,thiscustomizedelementconcurrently
adoptsaheuristicalgorithmdesignedtosolvethetraveling
refreshestoreflectthenewgeolocation.ByusingSelenium,
salesman problem, planning an efficient order for visiting
wecanthenextractthisupdatedgeolocationdata,enabling
these positions. The agents‚Äô task is to simply follow this
continuoustrackingoftheagent‚Äôsgeolocationchanges.
predeterminedroute,visitingeachnavigablepositioninthe
Move by grid-based relocating. In our test of the above plannedorder.
web-based mover, a critical limitation emerged: the web-
(iii) Vision-language navigator is specifically devel-
embedded Google Street View panoramas display only a
opedforthetouristagent‚ÄúLing‚Äù,aswellasfortaskswithin
subsetofnavigabledirections. Thisconstraintsignificantly
the V-IRL vision-language navigation benchmark. Its pri-
restricts our agents‚Äô mobility, often preventing them from
mary function is to guide the agent in selecting a proper
successfullynavigatingtotheirintendeddestinationsdueto
direction based on navigation instructions. The detailed
theincompletecoverageofpotentialroutes.
pipelineispresentedinAppendixB.7.
To overcome this obstacle, we develop an alternative
(iv) Intention navigator is utilized in intentional ex-
method: a grid-based relocating mover. This approach
plorer agent ‚ÄùHiro‚Äú to select the most suitable direction
involves performing a grid search for geolocations in the
thatalignswiththeagent‚Äôsspecificintentions. Thedetailed
**https://www.selenium.dev/ methodologyandpromptaredetailedinAppendixD.2.
19C.3.Mapping [Role]
You are PlaceSuggesterGPT, an expert
The mapping module in our environment is designed to
in recommending types of places
equipagentswithfunctionalitiessuchasrouteplanningand based on user-specified intentions.
transportation time estimation. It mainly utilizes the ‚ÄúDi-
rectionsAPI‚Äù‚Ä†‚Ä† fromtheGoogleMapPlatformtofacilitate
[Task Description]
these capabilities. Given the complex nature of this API‚Äôs
Given a user-specified intention,
interface, our principal focus has been on parsing its out- determine the type of "place"
putandadaptingitintovarioususer-friendlyinterfacesfor one should seek to fulfill the
agents. intention. Your response should
be in the following JSON format:
C.4.PlaceInfo&Search {"place": "Desired Place Type"}
Place Info & Search module hosts another important in-
[Example]
formation source in our platform beyond the visual street
Input:"Intention: <buy a book>"
view imagery, enabling agents to interact with real-world
Output:{"place": "bookstore"}
‚Äúplaces‚Äù. Itprovidesvariousattributesofplaces,including
type,name,location,imagery,reviews,etc. Inthismodule,
[Input]
our technical efforts are primarily focused on understand-
Intention: <{agent_intention}>
ing,comparing,andintegratingthemostsuitablefunctions
fromthevastarrayofGoogleMapsPlatformAPIsrelated [Output]
to place information and nearby place searches. Addition- Your recommended place type based on
ally, we devise some post-processing strategies to identify the user-specified intention, in the
and eliminate invalid or conflicting data sources from the required JSON format:
GoogleMapsPlatform.
Anotheressentialcapabilityenabledbythismoduleisto Usingthispromptwiththeintention
associateobjectproposals instreetviewimageryand their
Hiroishungryandlookingforaplacewherehe
correspondingplacesintherealcity. Thisfunctionisvital
cantrysomegoodlocalfood. Hecannothandle
to enhance the reality of our platform by connecting street
spicyfood.
viewandgeolocation. Italsopowersthe‚ÄúHiro‚Äùagentand
the evaluation of the V-IRL Place localization benchmark. returnstheresult
TheimplementationisdetailedinSec.5.2.
{"place": "restaurant"}.
D.Low-LevelSystemCaseStudy:
Theidentifiedplacetype(here,restaurant)isextracted
IntentionalExplorer‚ÄúHiro‚Äù
andsetasthetargetcategoryforHiro‚Äôsopen-worlddetector
duringhisexploration.
This section delves deeper into the low-level implemen-
tation details of the Intentional Explorer agent ‚ÄúHiro‚Äù D.2.RoadSelection
(Sec.3.3),focusingonthepromptsutilizedtointeractwith
various parts of our system. Concretely, we present the Whenever Hiro is at a crossroads, he determines the best
prompts in four subparts: identifying a type of place to roadtofollowusinghismulti-modalLLMandGPT-4. The
searchusingtheuser-definedintention(AppendixD.1),se- primarygoaloftheroadselectionprocessistoidentifythe
lectingappropriateroads(AppendixD.2),summarizingre- roadmostlikelytoleadtothedesiredplacetypethataligns
views of places (Appendix D.3), and making action deci- with Hiro‚Äôs intention. First, Hiro fetches the street view
sions (Appendix D.4). These four components jointly en- towards each potential road using the V-IRL environment.
able Hiro to explore in our interactive embodied environ- Then he utilizes his multi-modal LLM (such as Instruct-
mentdrivenbyhisinitialintention. BLIP [17] or LLaVA [42]) to generate captions for each
roadusingthefollowingprompt:
D.1.IntentiontoPlaceType
I am looking for a {place_type}.
Startingwithauser-definedagentintention,Hirofirstdeter- Please detail information that might
minesthetypeofplacethatcouldfulfillthisintentionusing be helpful for me along this road:
GPT-4andthefollowingprompt:
Captionsforeachroadarethenformattedinthestyleof
‚Ä†‚Ä†https : / / developers . google . com / maps /
documentation/directions {road_idx}: {road_description}
20and concatenated to form all_road_descriptions. [Task Description]
These road captions, along with Hiro‚Äôs user-defined inten- Given a set of road images, select
tion,arethenfedintoGPT-4todeterminethemostpromis- the best road from the crossroad.
ingroadtofollowusingthefollowingprompt: Your answer must be in the following
JSON format:
[Role] {"idx": "Selected road index (start
You are PathSelectorGPT, an expert by 0)", "reason": "Justification
in choosing the optimal road from for your selection"}
multiple candidates based on a
user-specified intention. [Input]
User Intention: <{agent_intention}>
[Task Description]
Given an intention, the road
[Output]
previously traveled, and
Please answer with the road index
descriptions of available candidate
and the reasoning behind your
roads, select the best road from the
selection, in the required JSON
crossroad. Your response must be in
format:
the following JSON format:
{"idx": "Selected road index",
"reason": "Justification for your
An example road selection response for the first crossroad
selection"}
selectioninFig.6isasfollows:
[Example]
‚Äúidx‚Äù: ‚Äú0‚Äù, ‚Äúreason‚Äù: ‚ÄúChoosing road 0 promises a
For the intention "find a grocery
genuinetasteoflocalcuisineinalesscommercializedset-
store", the road previously traveled
ting. It‚Äôslikelytohavefamily-runeaterieswhereIcanre-
as "1", and with candidates "2:
questnon-spicydishesandsavorauthenticflavors.Thisroad
Leads to residential area, 3: Leads
offersatranquildiningatmosphere,allowingforamoreen-
to a shopping district", the output
gagedandleisurelyculinaryexplorationamongthelocals.‚Äù
might be: {"idx": "3", "reason":
"Road 3 leads to a shopping district
D.3.SummarizePlaceReviews
which is more likely to have a
grocery store."} WhenHirodiscoversaplaceinthestreetviewimagery,he
[Input] retrievesitscorrespondingnameandGooglereviewsfrom
User Intention: <{agent_intention}> the V-IRL environment. There is a complex algorithm be-
Road Descriptions: hindprojectingthe2Dboxonstreetviewimagerytoacon-
{all_road_descriptions} crete place in the real world, which is detailed in ‚Äúmatch-
Previously Traveled Road: Road ingbetweenobjectproposalandplaces‚ÄùofSec.5.2. After
{from_road_idx}
Hiroobtainstheseplacereviews,hesummarizestheminto
[Output] aplaceoverview(toaidindecision-making)usingthefol-
Your chosen road index and the lowingprompt:
reasoning behind your selection,
[Role]
in the required JSON format:
You are SummarizeGPT, skilled at
condensing multiple reviews into a
We design such a two-stage captioning and decision-
concise overview of a location.
making pipeline for road selection because Multi-modal
LLMs cannot process multiple images simultaneously.
However,withtherecentadvancementsofGPT-4V,itmay [Task Description]
Given multiple reviews with ratings,
bepossibletoperformroadselectionusingseveralroadim-
craft a brief overview of the place.
ageswithasinglepromptatonce. Empiricalfindingssug-
Your response should be in the
gest that GPT-4V yields more reasonable choices with the
following JSON format:
followingprompt:
{"summarization": "Concise
description (limited to 80 words)"}
[Role]
You are PathSelectorGPT, an expert
in choosing the optimal road from [Example]
multiple road images according to a For reviews "Great ambiance but
user-specified intention. average food (Rating: 3)" and
"Loved the decor, food could be
21better (Rating: 3.5)", the output User Intention: <{intention}>
might be: Place Overview: <{place_intro}>
{"summarization": "The place
boasts great ambiance and decor,
[Output]
but the food quality receives mixed
Your chosen action and the rationale
reviews."}
behind your decision in the
prescribed JSON format:
[Input]
Reviews: {all_reviews}
Hiro‚Äôs exploration will continue if he decides to
continue() and will terminate if he opts for
[Output] enter_place().
Your concise overview (max 80 words)
based on the provided reviews, in E.V-IRLBenchmarks: Details
the prescribed JSON format:
E.1.V-IRLPlaces: Localization(Details)
D.4.ActionDecision
All category results. Due to the page limit of the main
After obtaining the overview of the identified place, Hiro paper,weonlypresenttheresultsof10categoriesinTab.3.
decides to visit the place or keep exploration using GPT-4 Here, we present the place recall for all 20 categories in
andthefollowingprompt: Fig.18.
[Role]
You are ActionSelectorGPT, Grounding DINO Owl-ViT GLIP CLIP (w/ GLIP proposal)
proficient in choosing the most
convenience store
appropriate action based on a cafe
clothing store
user‚Äôs background, intention, and park
an overview of a place. bank
pharmacy
lodging
book store
[Task Description] restaurant
jewelry store
Evaluate the provided user library
laundry
background, intention, and place school
overview to select the most suitable bakery
hospital
action from the list. Your response supermarket
bar
should be in the following JSON gym
format: spa
movie theater
{"action": "Selected Action", 0 20 40 60
"reason": "Justification for your
Recall
choice"} Figure18.RecallsinV-IRLPlacelocalization
Possible actions:
Exampleillustrations. Tofacilitatetheunderstandingof
- enter_place(): Enter the
designated place. V-IRL Place localization benchmark, we present some ex-
- continue(): Continue searching amplesofCLIP(w/GLIPproposals)inFig.21.
for another appropriate place.
E.2.V-IRLPlaces: RecognitionandVQA(Details)
[Example] Placetypesperformanceforrecognition. InFigure19,
For the background "loves historical wepresenttheaveragedaccuracyforeachplacetypeacross
sites", intention "discover local 10benchmarkedvisionmodels. Thesizeandthex-axispo-
history", and place overview
sition of each bubble correspond to the number of places
"This is a 200-year-old preserved
within each type. A clear trend emerges: accuracy tends
mansion", the output might be:
to correlate with the frequency. Common categories such
"action": "enter_place()",
as clothing store, cafe exhibit higher accuracy,
"reason": "The historical mansion
whereasvisionmodelsoftenstrugglewithinfrequentplace
aligns with the user‚Äôs interest in
historical sites."
typeslikebowling alleyormosque.
[Input] Place types performance for VQA. The place types
User Background: <{background}> performance of the V-IRL place VQA in Fig. 20 further
22100 can be identified with a high
degree of confidence?
bicycle store
The VQA options include all potential landmarks men-
75
florist clothing store tioned in the route description, along with a ‚ÄúNone of
movie rental departmecnat rs dtoeraeler boocko nsvtoerneienc be a s kt eojerr sywe heolery s stotorree above‚Äù choice. The model‚Äôs response to this question is
50 casino movingg a cfus on mset para atil no h ynome locp ke smt ds h itt ra ho urr m gde swe ta opal ror eeds ets lotivofl ffi eurq i ccerru ayeno rit r p u
r
es h rmt e pao aerr se m ie a t rol le a
r
r etc c ea aat y kr lto em en aa si rwc tt
a
s ag
t
y eas
b
t l hb alo e aoe gr r ne ma ey ku net
c
y
gy
s oa ocl do a sn f e
store
store t nh ie zFn edop ra tePr xs Pe t-d O usa C is nRt gh [e P2 P0la ] -n O(d + Cm G Ra Pr [k T 2-o 03b ].s 5 fe o)r , rv wa et e ai co fi hn r. s st tre ex et tra vc it ea wll ir mec ao gg e-
.
25 0bowlinge e l ae m lc lb etr a yic si sa yn hcb inou mossdus epur ts cl h utt oqep a o mnuema t ui dbpoi s pn aern e lt i rreme p yr oc a smalr ciycr v ho e ew s ovc ti oa eeh ls r op tih nhpoh aealy rars yc kti l ea o i cbrr t ah rr a ree ermrn yat upa css hil s te uot l iuo n rr cmac s h tg aa u rt ae cn l ro a ncgi ug n oso rh c u iuiv tst e n ne h sttc i tr r ta voa il as n anu eg stu sv m tt gbd rp ier hep osae e in ole t ni cl ln tan ca pa ar yttm lt yig pu wi os oeg ina n yns t sfnydr f ep cgik mcr c drha ye y mep oot a coar tllok li rng lodginh gairb a car re restaurant T m teh xae trn k a, ninG dtP lh aT i ns- d3 s m. t5 r ae[ re5 kt7 v n] i aed mwe et ie .mrm agin ee ,s jot ih ne tlp yr ce ose nn sic de ero if ngea tc hh eOla Cnd R-
1 10 100 1000
Fullsetresults. Apartfromthemini-setresultspresented
Place count
inSec.5.4,wealsoprovidethefullsetresultsofOracleand
Figure19. Category-wiseaccuracyandnumbersforV-IRLPlace
CLIP(L/14@336px)inTab.7. TheOracleresults,interest-
Recognitionbenchmark.
ingly,donotachievea100%successrate,duetoincorrect
decisions made by the LLM at stop positions. This is evi-
denced by the high arrival ratio and low reaction accuracy
verifies the correlation between accuracy and frequency
atstoppositions. Empirically,weobservethattheLLMoc-
fromahumanintentionperspective. Thetop-10categories
casionallydecidestokeepmoving,despitecleardestination
are closely aligned with the most common human activ-
indicationsintheobservations.
ities, purchasing and dining. In contrast, the bottom-10
When we substitute the map in oracle with the CLIP
placetypesrelatetoplacesthatarelessfrequentlyencoun-
modeltogatherlandmarkobservationsfromstreetviewim-
tered and serve a more diverse purpose, such as mosque,
agery, we observe a significant drop in the success rate,
plumberandembassy.
due to the inevitable model prediction errors. To improve
60 the success rate in VLN, we can focus on two important
Top 10 place types
50 Bottom 10 place types factors: (i) designing better vision models; (ii) develop-
40
ingLLMsandprompttechniquesthatarerobusttovision-
30
20 relatednoise.Especially,ourempiricalfindingssuggestthat
10 sophisticated prompt designs significantly improve the ro-
0 bicycle sto dr ee pb aa rtke mr ey nt sst uo pr ee r sm ha o cr p ok p ne it vn eg nim ea nll ce store clfl oo tri hist ng store c saf he oe store insul ra anw gcy oee
v
r ea rg ne sn mc eey cnt
o
nof dfi ac rye s hc ih no do ul temple mos cq ou ue rth bo ou wls ie ng alle ey mbassy plumber bu Mstn ete hs os dofLLMstovisualob Ss tae rr tva It nio ten rsn eco ti is oe n.
Stop
Place Type
Success Reac Arr Reac Arr Reac
Figure 20. Top-10 and bottom-10 place types averaged on four
Oracle(NoVision) 0.88 1.0 0.95 0.99 0.96 0.88
visionmodelsofV-IRLPlaceVQA.
CLIP(L/14@336px) 0.22 0.84 0.66 0.90 0.61 0.22
E.3.V-IRLVision-LanguageNavigation(Details)
Table7.ResultsofV-IRLVLN-full.
Navigationpipeline. AsmentionedinAppendixB.7,our
VLN pipeline is similar to [59], however, our benchmark
offersgreaterscalabilitythroughtheworldwideV-IRLplat-
formandanautomateddatacollectionpipeline,asopposed
tothemanualannotationofaspecificregion. Furthermore,
ourbenchmarkemphasizestheanalysisofthevisioncom-
ponentintheVLNpipeline,asopposedto[59],whichaims
to enhance performance on existing VLN datasets using
LLMs.
Implementation Details. Here, we introduce the imple-
mentationdetailsforLLaVA-1.5[41]andPP-OCR[20](+
GPT-3.5). ForLLaVA-1.5[41],wetransformthelandmark
recognitiontasktoamultiplechoiceVQAproblem,asking
Which of the following landmarks
23
)%001(
ycaruccA
)%(
ycaruccAFigure21.SamplesofV-IRLPlacelocalizationusingCLIP(w/GLIPproposals).
24