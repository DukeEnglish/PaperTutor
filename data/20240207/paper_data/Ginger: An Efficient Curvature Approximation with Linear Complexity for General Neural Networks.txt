GINGER: An Efficient Curvature Approximation with
Linear Complexity for General Neural Networks
YongchangHao♠ YanshuaiCao♢ LiliMou♠♣
yongcha1@ualberta.ca yanshuai.cao@borealisai.com doublepower.mou@gmail.com
♠AlbertaMachineIntelligenceInstitute(Amii)
Dept. ComputingScience,UniversityofAlberta
♢BorealisAI ♣CanadaCIFARAIChair
ABSTRACT
Second-order optimization approaches like the generalized Gauss-Newton method are
consideredmorepowerfulastheyutilizethecurvatureinformationoftheobjectivefunction
withpreconditioningmatrices. Albeitofferingtemptingtheoreticalbenefits,theyarenot
easilyapplicabletomoderndeeplearning. Themajorreasonisduetothequadraticmemory
and cubic time complexity to compute the inverse of the matrix. These requirements
are infeasible even with state-of-the-art hardware. In this work, we propose GINGER,
an eigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our
method enjoys efficient linear memory and time complexity for each iteration. Instead
of approximating the conditioning matrix, we directly maintain its inverse to make the
approximationmoreaccurate. WeprovidetheconvergenceresultofGINGERfornon-convex
objectives. Ourexperimentsondifferenttaskswithdifferentmodelarchitecturesverifythe
effectivenessofourmethod.1
1 Introduction
Second-orderoptimizationmethodsareusuallymorepowerfulbyconsideringthecurvatureinformation
oftheobjectivefunctionwithpreconditioningmatrices. However, suchmethodsareimpracticaldueto
theprohibitivememoryandtimecost. Specifically,foraneuralnetworkwithdparameters,thefull-matrix
preconditioningrequiresquadraticmemorytostoreandcubictimeforinverseateachiteration. Considera
Transformer(Vaswanietal.,2017)modelwith80Mparameters,thefull-matrixpreconditioningrequires3
petabytesmemorysolelytostorethematrix,nottomentionthecomputationtimetoobtainitsinverse.
Inpractice,deeplearningmodelsareusuallytrainedwiththediagonalapproximationofsuchmatrices,
suchasAdaGrad(Duchietal.,2011)anditsvariants(Hintonetal.,2012;KingmaandBa,2015;Liuetal.,
2023). Thesemethodsonlyrequirelinearmemoryandlineartimecomplexitybyusingtheelement-wise
inverseofthepreconditioningmatrix. However,thediagonalapproximationover-simplifiesthecurvature
informationbecauseitignorestheoff-diagonalelementsthatcontainthecorrelationbetweenparameters.
Therearenumerousattemptstoapproximatethefull-matrixpreconditioningwithaffordablememoryand
timecomplexity. Forinstance,K-FAC(MartensandGrosse,2015;GrosseandMartens,2016;Martensetal.,
2018) uses the Kronecker-factored approximation to reconstruct the preconditioning matrix. However,
such approximation is limited to specific model architectures like feed-forward neural networks (FFNs)
orconvolutionalneuralnetworks(CNNs). Moreimportantly,thecomplexityissuper-linearinthemodel
size,makingitimpracticaltonowadayslargemodels. Recently,Heetal.(2022)proposedaquasi-natural
gradient(QNG)methodthatapproximatesthefull-matrixpreconditioningbyfactorizingitintotheproduct
ofmultiplesimplematrices. ThisapproximationallowstheQNGmethodtoachievelinearmemoryandtime
complexity. However,wediscussinObservation1thatthisapproximationtendstobeinaccurate,leadingto
aworseapproximation.
1Ourcodeispubliclyavailableathttps://github.com/MANGA-UOFA/Ginger.
4202
beF
5
]GL.sc[
1v59230.2042:viXraInthiswork,weproposeGINGER,anewderivationtoapproximatethepreconditioningmatrixwithout
factorization. GINGERenjoysthesamelinearmemoryandtimecomplexityasQNG,butwithamoreaccurate
approximation. WeprovidetheconvergenceresultofGINGERfornon-convexobjectives. Empirically,we
showtheeffectivenessofGINGERacrossdifferenttasksandmodelarchitectures.
2 Approach
2.1 Background: generalizedGauss–Newtonandnaturalgradientmethods
Inthecontextofmachinelearning,weusuallymodelaconditionaldistributionbydefining
p (y|x) :=r(y|f(θ;x)), (1)
θ
where f is a function on the input x with some model parameter θ, and r(y|z) is a distribution in the
exponentialfamily(e.g., softmax). Theparameter θ ∈ Rd istrainedbymaximumlikelihoodestimation
(MLE),foragivendatasetD = {(x,y )}m . Thisisequivalenttominimizingthenegativelog-likelihood:
i i i=1
1 ∑ 1 ∑
L(θ) := L(θ;x,y) = [−logp (y|x)], (2)
|D| |D| θ
(x,y)∈D (x,y)∈D
whereL(θ)andL(θ;x,y)arethelossesforthewholedatasetandforasample,respectively.
Second-orderoptimizationmethodsNocedalandWright(1999)areappealingforsolvingtheoptimization
problemabovebecausetheyoftenenjoyfasterconvergencebyutilizingthecurvatureinformation. Specifi-
cally,Newton’smethod,awell-knownsecond-orderapproach,updatestheparameterswiththefollowing
rule:
(cid:16) (cid:17)−1
θ t+1 ← θ t−η t ∇2L(θ t) ∇L(θ t), (3)
whereη >0isthelearningrateand∇2L(θ )isthesecond-orderderivative,knownastheHessianmatrix.
t t
ThegeneralizedGauss–Newtonmethod. ThestandardNewton’smethodmaynotworkwellfornon-
convexoptimization,becausethepreconditioningmatrixmaynotbepositivesemi-definite. Ortegaand
Rheinboldt(2000)showthattheHessianmatrixcanbedecomposedas
(cid:34) (cid:35)
∇2L(θ) = 1 ∑ ∂f(θ;x)⊤ ∂2L(θ;x,y)∂f(θ;x) +∑c ∂2f(i)(θ;x)∂L(θ;x,y) , (4)
|D| ∂θ ∂f(θ;x)2 ∂θ ∂θ2 ∂f(i)(θ;x)
(x,y)∈D i=1
where ∂f(θ) istheJacobianmatrixof f(θ),and f(i)(θ)referstotheithelementofthefunction f(θ)in(1).
∂θ
Inpractice,thesecondterminsidethesummationisfoundtobelessimportantthanthefirstone(Sankar
etal.,2021). ThisfindingresultsinthefollowingbiasedapproximationoftheHessianmatrixby
1 ∑ ∂f(θ;x)⊤ ∂2L(θ;x,y)∂f(θ;x)
G := , (5)
|D| ∂θ ∂f(θ;x)2 ∂θ
(x,y)∈D
whereGisnamedthegeneralizedGauss–Newton(GGN)matrix(OrtegaandRheinboldt,2000;Schraudolph,
2002).
Theconnectiontonaturalgradient. Inoursettingswherer(y|z)isintheexponentialfamily,thematrixin
themiddlecanberewrittenas
∂2L(θ;x,y) (cid:20) ∂logr(yˆ|f(θ;x))∂logr(yˆ|f(θ;x))⊤(cid:21)
= E , (6)
∂f(θ;x)2
yˆ∼r(·|f(θ;x))
∂f(θ;x) ∂f(θ;x)
whichisamatrixindependentofthetargetlabely. Putting(6)into(5),wehave
(cid:20) (cid:21)
G = 1 ∑ E ∇ logp (yˆ|x)∇ logp (yˆ|x)⊤ . (7)
|D| θ θ θ θ
(x,·)∈Dyˆ∼pθ(·|x)
2ThelastequationiscommonlynamedtheFisherinformationmatrix(Fisher,1920)inthecontextofmachine
learning. Thisconnectionhasbeenestablishedinpreviousliterature(Martens,2020). Itrevealsasimple
waytoapproximatetheHessianmatrixbysolelyusingthefirst-orderderivativeswhenr(y|f(θ;x))isin
theexponentialfamily. Thisconditionactuallyholdsinmanyimportantapplications,suchaslanguage
models(Vaswanietal.,2017),imageclassifiers(Heetal.,2016),anddiffusionmodels(Hoetal.,2020). We
henceleveragetheconnectionandonlyconsidertheformofGgivenbyEquation(7)inthispaper.
Stochasticnaturalgradientdescent. ThecomputationoftheexactGGNmatrixGisusuallynotfeasible
because the dataset may contain an enormous number of data points. A remedy to this is maintaining
anexponentialmovingaverageofG atiterationstept ∈ N. Thiscanbecomputediterativelyusingthe
t
followingrule:
G
t
← αG t−1+(1−α)d td⊤
t
, (8)
whereforsimplicitywedefine
1 ∑
d t := (cid:112)
|B |
∇ θtlogp θt(yˆ|x). (9)
t (x,·)∈B
t
Here,B isamini-batchsampledfromthedataset,andyˆisasampledpredictiondrawnfrom p foreach
t θt
inputx. Thedecayrateα ∈ (0,1)controlsthestrengthofthemovingaverage,andG isaninitialization
0
usuallysetasanidentitymatrix. Ifθ isfixed,itiseasytoverifythatthisestimationinexpectationconverges
t
totheexactGwhent → ∞.
Although this seems to solve the problem of large datasets, the memory complexity for storing G is at
t
least O(d2). Even worse, the pseudoinverse of G takes O(d3) time. Preferably, both time and memory
t
complexitiesshouldnotexceedlineartomaketheGGNmethodfeasibleformodernlargeneuralnetworks.
2.2 Quasi-naturalgradientmethod
Recently,Heetal.(2022)proposedanovelmethod,calledquasi-naturalgradient(QNG),thatconstructsthe
GGNmatrixinlinearspaceandtime. TheprocedurefirstfactorizesG
t
= A t+1A⊤ t+1,sinceG
t
shouldalways
bepositivesemi-definite(PSD).Theruleforupdating A isgivenby:
t
√ √
A t = A 1K 1K 2...K t−1 = ( αI+β 1q 1q 1⊤)...( αI+β t−1q t−1q⊤ t−1), (10)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=:K1 =:Kt−1
√
where A isagainsettoidentity,andwedefineq := A−1d andβ = 1 ((cid:112) α+(1−α)∥q ∥2− α). Itis
1 t t t t ∥qt∥2 t
theneasytoshowthat
G
t
= A tK tK t⊤ A⊤
t
= αG t−1+(1−α)d td⊤
t
, (11)
whichrecoverstheformoftheexponentialmovingaveragedefinedinEquation(8).
However,itisimpossibletostoreallK matricesfromthebeginning. Thus,QNGintuitivelymaintainsthe
t
lastτmatricesandestimateeach A as
t
√ √
A t ≈ Aˆ t := A 1Kˆ t−τKˆ t−τ+1...Kˆ t−1 = ( αI+β t−τqˆ t−τqˆ⊤ t−τ)...( αI+β t−1qˆ t−1qˆ⊤ t−1), (12)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=:Kˆ
t−τ
=:Kˆ
t−1
whereKˆ dependsonqˆ ,whichinturndependsontruncated Aˆ byqˆ := Aˆ−1d .
t t t t t t
Giventhederivationabove,westateourobservationasfollows.
Observation 1. The QNG in He et al. (2022) essentially approximates the GGN matrix with the form G =
t
αmin(τ,t)I+Q Q⊤ forsomeQ ∈Rd×2min(τ,t).
t t t
Thiscanbeseenbyunrollingtheconstructionof Aˆ andlookingintoeachmultiplicationofKˆ matrices:
t
√ √
( αI+β t−1q t−1q⊤ t−1)( αI+β tq tq⊤
t
) (13)
(cid:16)√ (cid:17) √
=αI+q t−1 αβ t−1+(β t−1β tq⊤ t−1q t)q t + αβ tq tq⊤ t . (14)
3Itisobviousthattherankofthesumofthelasttwotermsisatmosttwo. Byrepeatingthemultiplicationτ
times,wehaveA −ατ/2I atmostofrankτ,implyingthatG −ατI = A A⊤−ατI hasatmostrank2τ.
t t t t
Wearguethatthis practicedoesnotcapturethemostuseful information in G , astheoptimallow-rank
t
approximation Q Q⊤ is given by the spectral decomposition or singular value decomposition. We thus
t t
propose a QNG variant to maintain the significant low-rank approximation in an online fashion while
keepingthespaceandtimecomplexitieslinearinthenumberofmodelparameters.
2.3 Ourapproach: GINGER
Motivatedbytheaboveobservation,weproposeanovelQNGvariant,calledGINGER,thatdirectlymodels
adampedGGNmatrixintheformof
G = γI+U diag(σ)U⊤ , (15)
t,γ t t t
where t istheupdatestepand γ isthedampingstrength. ThesecondtermU diag(σ)U⊤ isalow-rank
t t t
approximationoftheGGNmatrix(8),whereU ∈Rd×τ isasemi-orthogonalmatrix,and0⪯ σ ∈Rτ isthe
t t
vectorofeigenvaluessortedinthedescendingorder,forarankofτ.
OurapproachgeneralizesHeetal.(2022)’sQNGformaswealsodecomposetheGGNmatrixinadiagonal
pluslow-rankform. However,wedirectlymodelthelow-rankpartbyspectraldecomposition,whereasthe
diagonaliscontrolledbyadampinghyperparameterγ.Inthisway,wecanmodeltheoptimalapproximation
ofthelow-rankapproximationbytheEckart–YoungtheoremEckartandYoung(1936).
Querying the update direction. Assuming the matrix G is already known, we are interested in the
t,γ
updatedirectionG−1gforanyvectorg ∈Rd,suchasgbeingthegradientoflosswrttheparameters. This
t,γ
canbeobtainedthroughtheWoodburymatrixidentity:
G−1g =(γI+U diag(σ)U⊤)−1g (16)
t,γ t t t
=(γ−1I−U (γ2I+γdiag(σ))−1diag(σ)U⊤)g,
(17)
t t t t
(cid:124) (cid:123)(cid:122) (cid:125)
Kt,γ
whereK isadiagonalmatrixthatcanbecomputedinO(τ)time(recallσ ∈Rτ). Specifically,wehavethe
t,γ t
followingrelationship
(i)
K(i,i) = σ t (18)
t,γ γ2+γσ(i)
t
betweentheithelementsinK andσ.
t,γ t
Bycomputingtheresultfromtherighttotheleft,wecanobtaintheupdatedirectioninO(dτ)time.
Updaterules. AssumingG t−1,γ isalreadyconstructedandthenewgradientisd t,wewouldliketouse
themovingaveragetoupdatetheundampedGGNapproximation,i.e.,thesecondtermofEquation(15).
Withoutrestrictingitsrank(indicatedbyatilde),wehave
G˜ t− ,γ1 =(γI+αG t−1,0+(1−α)d td⊤
t
)−1 (EMA)
=(αG t−1,γ/α+(1−α)d td⊤
t
)−1 (19)
=α−1G−1 −β h h⊤ (Sherman–Morrison)
t−1,γ/a t t t
=γ−1I−(U t−1(α−1K t−1,γ/a)U t⊤ −1+β th th⊤
t
), (20)
whereh := G−1 d andβ := α−1 areobtainedbytheSherman–Morrisonformula. Equation(20)
t t,γ/a t t α+(1−α)h⊤
t
dt
expandsG−1
bytheWoodburymatrixidentity,similartoEquation(17).
t−1,γ/α
To maintaina low-rank approximation withrank τ mimic thebehavior of
G˜−1,
we would liketo find a
t,γ
matrixU diag(σ)U⊤ suchthattheerror
t t t
ϵ(U ,σ) :=∥G˜−1−(γI+U diag(σ)U⊤)−1∥ (21)
t t t,γ t t t 2
=∥U t−1(α−1K t−1,γ/a)U t⊤ −1+β th th⊤
t
−U tK t,γU t⊤∥
2
(22)
4isminimized.
WeobservethatU K U⊤ hasarankofatmostτ,sotheoptimalsolutionisgivenbythetruncatedSVDof
t t,γ t
U t−1(α−1K t−1,γ/a)U t⊤ −1+β th th⊤
t
. TheefficientcomputationoftheSVDisdeferredtothenextsubsection.
AfterobtainingK ,wewillfindanewσ forourGGNapproximationinEquation(15). Thiscanbedoneby
t t
matchingthediagonalofK withEquation(18),whichyields
t
γ2K(i,i)
σ(i) = t,γ (23)
t 1−γK(i,i)
t,γ
foranyi ∈ {1,...,τ}.
Notethatσ isguaranteedtobenon-negative. ThiscanbeshownthroughEquation(20)bynoticingthatG˜
t t,γ
ispositivedefiniteduetoEMA,whichimplies:
0⪯U t−1(α−1K t−1,γ/a)U t⊤ −1+β th th⊤
t
≺ γ−1I (24)
foranyiterationstept.Here,matrixcomparisonsA ⪯ BandA ≺ BmeanthatB−Aispositivesemi-definite
andpositivedefinite,respectively.
EfficientSVD. WenowturntotheefficientcomputationoftheSVDofU t−1(α−1K t−1,γ/a)U t⊤ −1+β th th⊤
t
.By
Equation(18),weknowthatα−1K t−1,γ/a isasorteddiagonalmatrixandU t−1issemi-orthogonal;therefore,
the first term itself is in the SVD form. Observing β h h⊤ is rank-1, we can efficiently compute the new
t t t
SVD.Specifically,weusetheapproachinBrand(2006),butourcalculationwillbesimplified,asourSVDis
essentiallyeigendecompositionbecausethematrixispositivesemi-definite.
Wefirstrewritetheupdateinthecompactform:
U t−1(α−1K t−1,γ/a)U t⊤ −1+β th th⊤ t = ( U t−1 h t )(cid:18) α−1K t 0−1,γ/a β0 (cid:19) ( U t−1 h t )⊤ . (25)
t
Wenoticethat( U t−1 h t )canbefactorizedas
(cid:18) I U⊤ h (cid:19)
( U t−1 p t )
0
t− r1 t , (26)
t
where r t = ∥h t −U t−1U t⊤ −1h t∥ and p t = (h t −U t−1U t⊤ −1h t)/r t. In this way, ( U t−1 p t ) is a semi-
orthogonalmatrix.
Therefore,wehavenewfactorization
⊤
( U t−1 p t )C t( U t−1 p t ) , (27)
whereC isdefinedas
t
(cid:18) I U t⊤ −1h t (cid:19)(cid:18) α−1K t−1,γ/a 0 (cid:19)(cid:18) I U t⊤ −1h t (cid:19)⊤ (28)
0 r t 0 β t 0 r t
withashapeof(τ+1)×(τ+1).
WithO(τ3)time,wecanobtaintheSVDofC =VK′V⊤. Itiseasytoseethat
t
U t′ K′ U t′⊤ =U t−1(α−1K t−1,γ/a)U t⊤ −1+βh th⊤
t
, (29)
whereU t′ = ( U t−1 p t )V issemi-orthogonalandK′ isadiagonalmatrix. Wethusconcludethat(U t′,K′)
istheSVDofU t−1(α−1K t−1,γ/a)U t⊤ −1+β th th⊤
t
.
NotethatU′ ∈ Rd×(τ+1) and K′ ∈ Rτ+1 nowhaveonemoredimensionthanthepreviousiteration. We
t t
hencetruncatethelastcolumnofU′andthelastelementofK′toobtainU ∈Rd×τandK ∈Rτ,respectively.
t t t t
ThetruncatedU isstillsemi-orthogonalandK isstilldiagonal. Finally,weuseEquation(23)totranslateK
t t t
backtoσ.
t
ThetotalcomputationoftheprocesstakesO(dτ2+τ3)timeandO(dτ+τ2)space. Underatypicalchoice
√
ofτwhereτ ≪ d,wecansimplifythecomplexitiesasO(dτ2)andO(dτ)fortimeandspace,respectively.
√
Therefore,weconcludethatthealgorithmislineartothenumberofparametersdwhenτ ≪ d.
TheoverallalgorithmissummarizedinAlgorithm1.
5Algorithm1:Ourapproach: GINGER
Input:Decayrateα,dampingfactorγ,rankτ
Input:Initialparametersθ
0
defr1u(U,K,d):
/* returns the SVD of UKU⊤+dd⊤ */
U,K←SVD(UKU⊤+dd⊤) ▷ fast version
returnU(1:τ,1:τ),K(1:τ) ▷ O(τ2(d+τ))
defdrt(U,σ,g):
/* calculates the update direction (γI+Udiag(σ)U⊤)−1g */
Σ←diag(σ) ▷ O(τ)
Kγ ←(γ2I+γΣ)−1Σ ▷ O(τ)
g′ ←UKγU⊤g ▷ O(dτ)
returnγ−1g−g′ ▷ O(d)
defupd(U,σ,d):
/* updates U and σ with d */
h←drt(U,ασ,d) ▷ O(dτ)
Σ←diag(σ) ▷ O(τ)
K ←((γ/α)2I+(γ/α)Σ)−1Σ ▷ O(τ)
γ/α
β← α−1−1 ▷ O(d)
α+(1−α)h⊤d
U,K←r1u(U,K ,(cid:112) βh) ▷ O(τ2(d+τ))
γ/a
σ←γ2K(1−γK)† ▷ O(τ)
returnU,σ
/* Initialization */
U ←randomsemi-orthogonalmatrix
0
σ ←0
0
fort←0...T−1do
/* Update the optimizer state first */
Learningratescheduleηt
dt ←Equation(9)
U t+1,σ t+1 ←upd(Ut,σ,dt) ▷ O(τ2(d+τ))
/* Update parameters */
gt ←drt(U t+1,σ t+1,∇L(θt)),(U t+1,σ t+1)
θ t+1 ←θt−ηtgt
t←t+1
returnθ
T
3 Theoreticalanalyses
Weincludetheconvergenceproofofourmethodtoshowthesanityofourmethod. Beforewepresentthe
theoreticalanalyses,wefirstshowthefollowinglemmathatboundstheeigenvaluesofthepreconditioning
matrixG .
t,γ
Wemakethefollowingassumptionstoobtaintheconvergenceguarantee,wherethefirstthreearestandard
inthestochasticoptimizationanalysisBottouetal.(2018). Thelastassumptionisespeciallyforourmethod,
whichessentiallyassumesthegradientisfinite.
Assumption1(Lipschitzgradient). Thegradientofthelossfunction∇ LisL-Lipschitzcontinuouswith
θ
L >0. Thismeansthatwehave∥∇L(θ )−∇L(θ )∥ ≤ L∥θ −θ ∥forallθ ,θ ∈Rd.
1 2 1 2 1 2
Assumption2(Boundedsquaredgradient). Thereexistsaconstant M >0suchthatforallt ≥1,
g
E [∥∇ L(θ ;x,y )∥2] ≤ M2. (30)
θ t i i g
(xi,yi)
Assumption3(Learningrateschedule). Thelearningrateschedulesatisfiesthefollowingconditions:
∞ ∞
∑ η = ∞ and ∑ η2 < ∞. (31)
t t
t=1 t=1
6Assumption4(Bounded∥d ∥). Thereexistsaconstant M >0toboundthenormofd inEquation(9):
t d t
∥d ∥ ≤ M (32)
t d
forallt ≥1.
Undertheseassumptions,weobtainthefollowinglemmas:
Lemma1. TheapproximationG−1hasboundedeigenvaluesforallt ≥0. Specifically,wehave0< λ (G−1) ≤
t,γ min t,γ
λ (G−1) = γ−1wheneverτ < d.
max t,γ
Proof. Seethesupplementarymaterial.
Lemma2. Wehave
L
η tλ min(G t− ,γ1)∥∇L(θ t)∥2 ≤E[L(θ t)−L(θ t+1)]+ 2η t2(M g/γ)2
forallt ≥0.
Proof. Seethesupplementarymaterial.
UsingLemma2,wehavethefollowingconvergenceresult:
Theorem1. Undertheaboveassumptions,wehave
lim inf ∥∇L(θ )∥2 =0. (33)
t
T→∞t=0,...,T
Proof. UsingLemma2,wehave
T ∑−1
η tλ min(G t− ,γ1)∥∇L(θ t)∥2
≤T ∑−1(cid:18)
E[L(θ t)−L(θ t+1)]+
L
2η t2(M
g/γ)2(cid:19)
(34)
t=0 t=0
=L(θ )−E[L(θ )]+
L
(M
/γ)2T ∑−1
η2. (35)
0 T 2 g t
t=0
Thenwehave
inf λ (G−1)∥∇L(θ )∥2 ≤
1 T ∑−1
η λ (G−1)∥∇L(θ )∥2 (36)
t=0,...,T−1 min t,γ t ∑ tT =− 01η
t t=0
t min t,γ t
≤L(θ 0)−E[L(θ T)]
+
L(M g/γ)2 T ∑−1
η2. (37)
∑T−1η 2 ∑T−1η t
t=0 t t=0 t t=0
NoticethatL(θ )−E[L(θ )]isupper-bounded,becausethelossfunctionListypicallylower-bounded,
0 T
whichisthecaseourMLEforexponentialfamilydefinedinEquation(2). Sincelim T→∞∑ tT =− 01η t2 < ∞and
lim T→∞∑ tT =− 01η
t
= ∞,wehave
lim inf λ (G )∥∇L(θ )∥2 =0. (38)
min t,γ t
T→∞t=0,...,T
WealsonoticethatAssumption4guaranteestheeigenvaluesofthemovingaverageinEquation(20)isupper-
bounded,whichlower-boundstheeigenvaluesofitsinversewithsomepositivenumber. Inotherwords,we
haveλ min(G t− ,γ1) ≥ ϵforsomeϵ >0. Wecombineitwith(37),concludinginf t=0,...∥∇L(θ t)∥2 →0.
Weprovidethetheoreticalanalysisheretoshowthesanityofourapproximation. Itisworthnotingthatthe
convergenceresultholdsregardlessoftheconvexityoftheobjective.
7Table1: TheresultsontheCIFAR-100dataset. Weusegreenandredtohighlightthebetterandtheworse
results,respectively.
ResNet-18 ResNet-50
Optimizers
Acc Lval Lˆtrain FLOPs Mem Acc Lval Lˆtrain FLOPs Mem
10−6 10−6
Momentum 71.89 1.320 10.47 2.21e9 3.15 74.42 1.276 11.47 1.13e10 5.92
Adam 71.61 1.249 9.57 2.36e9 3.16 71.59 1.241 60.65 1.16e10 6.50
QNG(τ=1) 71.74 1.374 23.83 3.56e9 3.20 74.30 1.290 32.29 1.80e10 6.56
QNG(τ=2) 72.30 1.347 23.97 3.70e9 3.25 73.92 1.255 42.12 1.83e10 6.78
QNG(τ=4) 72.32 1.323 27.02 3.97e9 3.41 74.51 1.287 12.53 1.89e10 7.30
QNG(τ=8) 71.98 1.318 15.91 4.51e9 3.59 75.12 1.311 6.98 2.00e10 7.66
QNG(τ=16) 71.76 1.290 13.60 5.59e9 3.92 74.53 1.254 6.35 2.23e10 8.65
QNG(τ=32) 71.95 1.321 18.08 7.74e9 4.60 74.27 1.267 9.37 2.68e10 10.66
GINGER(τ=1) 72.04 1.265 117.8 3.77e9 3.19 74.54 1.269 37.07 1.84e10 6.60
GINGER(τ=2) 72.66 1.270 22.78 3.80e9 3.25 74.99 1.245 11.86 1.85e10 6.66
GINGER(τ=4) 72.90 1.263 13.37 4.08e9 3.33 75.83 1.249 3.90 1.91e10 6.91
GINGER(τ=8) 72.89 1.242 11.18 4.66e9 3.56 75.73 1.255 4.26 2.03e10 7.36
GINGER(τ=16) 73.17 1.236 6.41 5.82e9 3.90 75.53 1.202 3.82 2.28e10 8.20
GINGER(τ=32) 73.15 1.210 9.88 8.16e9 4.61 75.48 1.216 5.27 2.77e10 10.17
Table2: TheresultsontheXSUMdataset. WeusethesamecolorschemeasinTable1. However,weomitthe
backgroundcolorforallbaselinesastheyarenotcomparablewithAdam.
LoRA Full
Optimizers
R1/R2/RL Pˆtrain FLOPs Mem R1/R2/RL Pˆtrain FLOPs Mem
Adam 31.06/9.09/24.10 10.236 5.61e9 2.67 32.61/10.40/25.48 3.747 5.42e9 2.06
Momentum 27.39/6.64/20.81 12.566 5.61e9 2.67 29.90/8.28/23.07 4.831 4.60e9 1.98
QNG(τ=1) 29.01/7.63/22.32 11.775 5.62e9 2.67 29.96/8.34/23.11 4.938 8.60e9 3.45
QNG(τ=2) 28.99/7.64/22.28 11.658 5.63e9 2.68 29.94/8.39/23.17 4.778 9.05e9 3.75
QNG(τ=4) 28.95/7.74/22.42 11.646 5.66e9 2.69 29.98/8.30/23.13 4.826 1.22e10 4.66
QNG(τ=8) 29.01/7.74/22.42 11.822 5.74e9 2.71 29.84/8.29/23.13 4.860 2.21e10 5.89
QNG(τ=16) 28.98/7.67/22.29 11.717 6.05e9 2.74 29.85/8.37/23.13 4.870 5.06e10 9.31
QNG(τ=32) 29.00/7.66/22.28 11.752 7.15e9 2.77 29.91/8.34/23.05 4.879 9.76e10 16.51
GINGER(τ=1) 31.03/9.03/24.03 10.278 5.63e9 2.67 32.73/10.51/25.64 4.212 1.00e10 3.43
GINGER(τ=2) 31.14/9.07/24.11 10.298 5.64e9 2.68 32.93/10.68/25.85 3.532 1.10e10 3.67
GINGER(τ=4) 31.07/9.10/24.10 10.268 5.68e9 2.68 32.85/10.71/25.85 3.618 1.43e10 4.35
GINGER(τ=8) 31.07/9.10/24.10 10.196 5.77e9 2.70 32.81/10.60/25.72 3.564 2.41e10 6.48
GINGER(τ=16) 30.93/8.96/24.04 10.176 6.08e9 2.74 32.88/10.60/25.78 3.370 5.51e10 9.77
GINGER(τ=32) 31.03/9.10/24.08 10.044 7.18e9 2.77 32.82/10.66/25.75 3.418 16.33e10 18.04
4 Experiments
Inthissection,weconductexperimentsondifferenttasksandmodelarchitecturestoverifytheeffectiveness
ofGINGER. Fortaskselection,weconsiderimageclassificationandconditionallanguagemodeling,which
aretwosymbolicbenchmarksindeeplearning. Forthebaselines,weonlyconsiderthemethodsthatareable
toachievelinearmemoryandtimecomplexity,whichincludethefirst-ordermethodsandthequasi-second-
order methods. For the former, we consider the standard momentum method and the well-established
AdamoptimizerKingmaandBa(2015). Forthelatter,weconsiderthequasi-naturalgradient(QNG)method
recentlyproposedbyHeetal.(2022).
4.1 Imageclassification
Dataset. WeusetheCIFAR-100(Krizhevskyetal.,2009)imageclassificationdataset,whichcontains50K
trainingand10Ktestimages. Eachdatapointisa32×32RGBimagebelongingtooneofthe100classes.
Models. Weconsiderpopularconvolutionalneuralnet(CNN)-basedarchitecturesforimageclassification,
namely,ResNet-18(11Mparameters)andResNet-50(24Mparameters)(Heetal.,2016).
8Trainingdetails. WeusethestandarddataaugmentationandnormalizationfortrainingHeetal.(2016).
Wesetthecoefficientsoffirstmomentandsecondmomentas0.9and0.99,respectively,foralloptimizers.
Wetunethelearningrateinthesetof{1,5}×10{−1,−2,−3,−4} foralloptimizersonasubsetofthevalidation
set. Aftertuning,wefixthelearningrateforeachoptimizeracrossallvariationsofit. Toruleouttheeffect
oflearningrateschedulingandweightdecay,wedonotusetheminourexperiments. Wetrainallmodels
for200epochswithabatchsizeof128.
Evaluationmetrics. WereportthebestvalidationaccuracyandthecorrespondingevaluatinglossLval. To
getmoreinsightsintothetrainingprocess,wealsoreporttheminimumtrainingloss(scaledby10−6 for
readability)onsampledbatches. Inaddition,wecalculatefloatingpointoperationsperiteration(FLOPs)
andthepeakmemoryusagewiththeJAXprofilingtool(Bradburyetal.,2018).
Results. ThemainresultsaresummarizedinTable1. WecanseethatGINGERachievesthebestvalidation
accuracyonbothResNet-18andResNet-50.Inaddition,GINGERachievesthebesttraininglossonResNet-18
andthesecond-besttraininglossonResNet-50. TheseresultsindicatethatGINGERisabletoachievebetter
generalizationperformancethanotheroptimizers. IntermsofFLOPsandmemoryusage,GINGERinevitably
requiresmoreFLOPsandmemorythanthefirst-ordermethodslikeMomentumorAdam. However,itisstill
abletoachievelinearmemoryandtimecomplexity,whichismuchmoreefficientthanthequasi-second-order
methods.
Aninterestingobservationisthatwhenτgrowslarger,theperformanceofGINGERgenerallyincreases. This
isbecausealargerτleadstoamoreaccurateapproximationofthepreconditioningmatrix. However,the
performancesaturateswhenτislargeenough,asthegeneralizedGauss–Newtonmatrixheavilydepends
ontheleadingeigenvalues,corroboratingfindingsinpriorwork(Feinbergetal.,2023).
Althoughalargerτgenerallyyieldsbetterapproximation,italsoleadstomoreFLOPs(quadraticincrease)
√
and memory (linear increase). As mentioned in Section 2, however, we typically have τ ≪ d, so our
approachdoesnotaddtothecomplexitymuchcomparedwithotherpartsofthelearningalgorithm,suchas
forwardandbackwardpropagation.
4.2 Conditionallanguagemodeling
Languagemodelingisanotherwell-establishedtaskindeeplearning,withthetremendoussuccessoflarge
languagemodelslikeGPT-3(Brownetal.,2020). Inthispaper,wespecificallyconsiderconditionallanguage
modeling,namely,textsummarization,asitiseasiertoevaluate.
Dataset. WeusetheXSUMdataset(Narayanetal.,2018)inourexperiments. XSUMisasummarization
datasetthatcontains204Ktrainingsamples,11Kvalidationsamples,and11Ktestsamples. Eachsampleisa
newsarticlewithasummary. Thetaskistogenerateasummaryofthearticle.
Models. WeusethestandardTransformerasourmodelarchitecture. Specifically,weloadapre-trained
T5-smallmodel(Raffeletal.,2020)andfine-tuneitwiththeXSUMdataset. Themodelhasaround60M
parametersintotal.
Inadditiontothestandardfull-parameterfine-tuning,wealsoconsiderthesettingwhereonlylow-rank
adapters(LoRA)(Huetal.,2022)arefine-tuned. Ithasgainedincreasingattentionrecentlybecauseitisable
toachievecomparableperformancewithmuchfewerparameters,makingitanidealchoiceforfine-tuning
largelanguagemodels.
Trainingdetails. Foreachsample,wefirsttokenizethesourceandtargetsentenceswiththeT5tokenizer.
Wethentruncatethesourceto512tokensandthetargetto128tokens.
Mostofthehyper-parametersaretunedinthesamewayasintheimageclassificationtask. Inaddition,we
settherankofeachattentionadapteras8fortheLoRAsetting. Wetrainallmodelsfor1epochwithabatch
sizeof4.
Evaluation metrics. We report the best rouges scores (Lin, 2004), including ROUGE-1, ROUGE-2, and
ROUGE-Lindividually. Thesescoresrepresenttheoverlapbetweenthegeneratedsummaryandtheground-
truthsummary. Theyarewidelyusedinsummarizationtasks. WealsoreportthetrainingperplexityPˆtrain
9andtheevaluatinglossLval. Similartoimageclassification,wealsoreportFLOPsandthepeakmemory
usageforeachoptimizer.
Results. For the full-parameter fine-tuning setting, there is a clear trend that GINGER achieves better
performancethanotheroptimizers. Especially,thelargerτleadstolowertraininglossduringthetraining
process. Further,thelowerlosstranslatestobetterrougesscoresingeneral. ThisindicatesthatGINGERis
abletomaintainareasonablegeneralizationability.
FortheLoRAfine-tuningsetting, GINGER alsoachievesamarginallybetterperformance. However,the
performance of GINGER is not as good as the full-parameter fine-tuning setting. We hypothesize that
thisisbecausetheLoRAweightsaregenerallyeasiertooptimizewiththeirlow-rankstructure,making
thecurvatureinformationlessimportant. Nevertheless,weargueinthiscasethatGINGERisagnosticto
architecturalmodificationsorgradientcalculationmethods,makingitamoregeneraloptimizer.
4.3 Analyses.
Sensitive of τ and α. We conduct experiments to analyze the sensitivity of τ and α on the image clas-
sificationtask. WeuseResNet-18asthemodelarchitectureandCIFAR-100asthedataset. Alltheother
hyper-parametersarethesameasinthemainexperiments. Wereportthebestvalidationaccuracyandthe
minimumtraininglossinTable3andTable4,respectively.
Table3: Validationaccuracy Table4: Trainingloss.
α\τ 2 4 8 α\τ 2 4 8
0.9 73.24 73.35 73.15 0.9 14.82 6.03 10.62
0.99 72.66 72.90 72.89 0.99 22.78 13.37 11.18
0.999 72.73 73.14 72.57 0.999 52.98 11.92 17.55
From the results, we can see that the performance of GINGER is not sensitive to τ and α. In fact, the
performanceofGINGERcanbehigherthandefaultsettingswhenτandαaretunedproperly. Thisindicates
thatGINGERisrobusttothehyper-parameters.
5 Discussion
Therehasbeenalonghistoryofapproximatingsecond-orderoptimizationmethods. Themostpopularones
areBFGS(NocedalandWright,1999)andL-BFGS(LiuandNocedal,1989),whichapproximatetheinverse
oftheHessianmatrixwithreasonablylargememoryandtimecomplexity. However,theyarenotsuitable
fornon-convexfunctionswithnon-PSDHessianmatrices.
ThegeneralizedGauss-NewtonmethodasanapproximationoftheHessianmatrixisguaranteedtohavethe
PSDpreconditioning. However,materializingtheexactGauss-Newtonmethodisnotpracticalformodern
deeplearning. Martensetal.(2010),whichusestheconjugategradientmethodtosolvethelinearsystem
withtheHessian-vectorproduct. However,thememoryandtimecomplexityoftheHessian-freemethodare
stilltoohighformoderndeeplearning.
Tofurtherreducetheexcessivememoryandtimecomplexity,MartensandGrosse(2015)proposedKFAC,
whichapproximatestheFisherinformationmatrixwithKroneckerfactors. However,itisrestrictedtocertain
modelarchitectures. Moreover,thetimecomplexityofKFACtakesO(n3)foreachlayerwithahiddensizeof
n. ThistranslatestoatleastO(d1.5)forthetotalparametersizedofthemodel,makingitstrictlysuperlinear
inmodelsize.
Recently,Heetal.(2022)proposedaquasi-naturalgradientmethod,whichapproximatestheFisherinfor-
mationmatrixwithlinearcomplexity. Asdiscussedinthispaper,weshowthatthequasi-naturalgradient
methodisequivalenttoanidentitymatrixplusalow-rankmatrix. However,thelow-rankmatrixmightnot
capturethecurvatureinformationwell,whichleadstoanon-informativepreconditioner. Incontrast,our
methoddirectlyminimizesthenormdifferencebetweentheinversesofthenextEMAandapproximation.
Asconfirmedbyexperiments,ourmethodismoreeffectivethanthequasi-naturalgradientmethod.
InsteadofthegeneralizedGauss-Newtonmatrix,itisalsopossibletouse(∑t [g g⊤])1/2asG ,knownas
i=1 i i t
Adagrad(Duchietal.,2011).Thefull-matrixAdagradrequiresquadraticmemoryandcubictime,notscalable
10tolargemodels. Toreducethecomplexity,Guptaetal.(2018)proposedShampoo,whichapproximatesthe
full-matrixAdagradwithKroneckerfactors. However,thetimecomplexityisO(d1.5),whichdoesnotscale
welltolargemodels.
6 Conclusion
Summary. Inthiswork,weproposeGINGER,anefficientcurvatureapproximationwithlinearcomplexity
forgeneralneuralnetworks. Specifically,itisbasedontheeigendecompositionoftheinversegeneralized
Gauss-Newtonmatrix.WeshowconvergenceofGINGERfornon-convexobjectives.Experimentsondifferent
taskswithdifferentmodelarchitecturesverifytheeffectivenessofourmethod.
Future directions. In this work, we build the convergence proof of GINGER to show the sanity of our
method. However, we only show its benefits empirically and do not attempt to obtain the asymptotic
convergencerates. Thisisbecausetheytypicallyrequireadditionalassumptionsofthelossfunction. We
√
leavethisdirectiontofuturework. Inaddition,thetimecomplexityisO(dτ2)forτ ≪ d,whichmaygrow
quadraticallyinτ. WehopetoreducethecomplexitytoO(dτ)tomakeitscalabletolargemodels.
Acknowledgements
This research was supported in part by Natural Sciences and Engineering Research Council of Canada
(NSERC) under Grant No. RGPIN2020-04465, the Amii Fellow Program, the Canada CIFAR AI Chair
Program,theDigitalResearchAllianceofCanada(alliancecan.ca),andaMitacsAccelerate(Cluster)grant.
References
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,
andIlliaPolosukhin. Attentionisallyouneed. NIPS,2017. URLhttps://proceedings.neurips.cc/
paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html.
JohnDuchi,EladHazan,andYoramSinger.Adaptivesubgradientmethodsforonlinelearningandstochastic
optimization. JMLR,12(61):2121–2159,2011. URLhttps://jmlr.org/papers/v12/duchi11a.html.
GeoffreyHinton,NitishSrivastava,andKevinSwersky. Neuralnetworksformachinelearninglecture6a
overviewofmini-batchgradientdescent. Citedon,14(8):2,2012. URLhttps://www.cs.toronto.edu/
~tijmen/csc321/slides/lecture_slides_lec6.pdf.
DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InYoshuaBengioand
YannLeCun,editors,ICLR,2015. URLhttps://arxiv.org/abs/1412.6980.
Hong Liu, Zhiyuan Li, David Hall, Percy Liang, and Tengyu Ma. Sophia: A scalable stochastic second-
order optimizer for language model pre-training. arXiv preprint arXiv:2305.14342, 2023. URL https:
//arxiv.org/abs/2305.14342.
James Martens and Roger Grosse. Optimizing neural networks with kronecker-factored approximate
curvature. InICML,pages2408–2417,2015. URLhttp://proceedings.mlr.press/v37/martens15.html.
RogerGrosseandJamesMartens. Akronecker-factoredapproximatefishermatrixforconvolutionlayers. In
ICML,pages573–582,2016. URLhttp://proceedings.mlr.press/v48/grosse16.html.
JamesMartens,JimmyBa,andMattJohnson. Kronecker-factoredcurvatureapproximationsforrecurrent
neuralnetworks. InICLR,2018. URLhttps://openreview.net/forum?id=HyMTkQZAb.
XiaoyuHe,ZibinZheng,YurenZhou,andChuanChen. QNG:Aquasi-naturalgradientmethodforlarge-
scalestatisticallearning. SIAMJournalonOptimization,32(1):228–255,2022. URLhttps://epubs.siam.
org/doi/10.1137/20M1376753.
JorgeNocedalandStephenJWright. Numericaloptimization. Springer, 1999. URLhttps://doi.org/10.
1007/b98874.
JamesMOrtegaandWernerCRheinboldt. Iterativesolutionofnonlinearequationsinseveralvariables. SIAM,
2000. URLhttps://doi.org/10.1137/1.9780898719468.
AdepuRaviSankar,YashKhasbage,RahulVigneswaran,andVineethNBalasubramanian. Adeeperlookat
thehessianeigenspectrumofdeepneuralnetworksanditsapplicationstoregularization. InAAAI,pages
9481–9488,2021. URLhttps://ojs.aaai.org/index.php/AAAI/article/view/17383.
11NicolNSchraudolph. Fastcurvaturematrix-vectorproductsforsecond-ordergradientdescent. Neural
computation,14(7):1723–1738,2002. URLhttps://doi.org/10.1162/08997660260028683.
R.A.Fisher. Amathematicalexaminationofthemethodsofdeterminingtheaccuracyofobservationbythe
meanerror,andbythemeansquareerror. MonthlyNoticesoftheRoyalAstronomicalSociety,80(8):758–770,
1920. URLhttps://doi.org/10.1093/mnras/80.8.758.
JamesMartens. Newinsightsandperspectivesonthenaturalgradientmethod. JMLR,21(146):1–76,2020.
URLhttps://jmlr.org/papers/v21/17-678.html.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecognition.
InCVPR,pages770–778,2016. URLhttps://openaccess.thecvf.com/content_cvpr_2016/html/He_
Deep_Residual_Learning_CVPR_2016_paper.html.
Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models.
NeurIPS, pages 6840–6851, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/
4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html.
CarlEckartandGaleYoung. Theapproximationofonematrixbyanotheroflowerrank. Psychometrika,1(3):
211–218,1936. URLhttps://doi.org/10.1007/BF02288367.
MatthewBrand. Fastlow-rankmodificationsofthethinsingularvaluedecomposition. Linearalgebraandits
applications,415(1):20–30,2006. URLhttps://doi.org/10.1016/j.laa.2005.07.021.
LéonBottou,FrankECurtis,andJorgeNocedal. Optimizationmethodsforlarge-scalemachinelearning.
SiamReview,60(2):223–311,2018. URLhttps://epubs.siam.org/doi/abs/10.1137/16M1080173.
AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiplelayersoffeaturesfromtinyimages. Technical
report,2009. URLhttps://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,
GeorgeNecula,AdamPaszke,JakeVanderPlas,SkyeWanderman-Milne,andQiaoZhang. JAX:compos-
abletransformationsofPython+NumPyprograms,2018.
VladimirFeinberg,XinyiChen,Y.JenniferSun,RohanAnil,andEladHazan. Sketchy: Memory-efficient
adaptive regularization with frequent directions. In NeurIPS, 2023. URL https://openreview.net/
forum?id=DeZst6dKyi.
TomBrown, BenjaminMann, NickRyder, MelanieSubbiah, JaredDKaplan, PrafullaDhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-
shot learners. NeurIPS, pages 1877–1901, 2020. URL https://papers.nips.cc/paper/2020/hash/
1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
ShashiNarayan,ShayB.Cohen,andMirellaLapata. Don’tgivemethedetails,justthesummary! Topic-
aware convolutional neural networks for extreme summarization. EMNLP, 2018. URL https://www.
aclweb.org/anthology/D18-1206.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.
JMLR,21(1):5485–5551,2020. URLhttp://jmlr.org/papers/v21/20-074.html.
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. URL https:
//openreview.net/forum?id=nZeVKeeFYf9.
Chin-YewLin. ROUGE:Apackageforautomaticevaluationofsummaries. InTextSummarizationBranches
Out,pages74–81,July2004. URLhttps://www.aclweb.org/anthology/W04-1013.
DongCLiuandJorgeNocedal.Onthelimitedmemorybfgsmethodforlargescaleoptimization.Mathematical
programming,45(1-3):503–528,1989. URLhttps://doi.org/10.1007/BF01589116.
James Martens et al. Deep learning via Hessian-free optimization. In ICML, pages 735–742, 2010. URL
https://dl.acm.org/doi/10.5555/3104322.3104416.
VineetGupta,TomerKoren,andYoramSinger. Shampoo: Preconditionedstochastictensoroptimization. In
ICML,pages1842–1850,2018. URLhttp://proceedings.mlr.press/v80/gupta18a.html.
12A Proofs
Inthissection,weprovideproofsforthelemmas.
A.1 ProofofLemma1
Lemma1. TheapproximationG−1hasboundedeigenvaluesforallt ≥0. Specifically,wehave0< λ (G−1) ≤
t,γ min t,γ
λ (G−1) = γ−1wheneverτ < d.
max t,γ
Proof. Fort =0,wehaveλ (G ) = λ (G ) = γ−1byinitialization.
max 0,γ min 0,γ
Fort >1,werestateEquation(24)here:
0⪯U t−1(α−1K t−1,γ/a)U t⊤ −1+β th th⊤
t
≺ γ−1I (39)
Thetop-τeigenvaluesofthismatrixarealsotheeigenvaluesofU K U⊤,whichisatruncatedSVD.
t t,γ t
ByEquation(17),wehave
τ
G−1 = γ−1I−U K U⊤ = γ−1I−∑ κ u u⊤ , (40)
t,γ t t,γ t i i i
i=1
whereu istheithcolumnofU andκ istheithelementonthediagonalofK . Wethushave
i t i t,γ
(cid:110) (cid:111)
λ (G−1) = max x⊤ G−1x (41)
max t,γ t,γ
∥x∥=1
(cid:110) (cid:111) (cid:40) (cid:18) τ (cid:19) (cid:41)
≤ max x⊤(γ−1I)x + max x⊤ −∑ κ u u⊤ x (42)
i i i
∥x∥=1 ∥x∥=1
i=1
(cid:40) (cid:41)
τ
= γ−1− min ∑ κ (x⊤ u )2 (43)
i i
∥x∥=1
i=1
= γ−1−0, (44)
wherethelastequalityholdsbecausewecanalwaysfindavectorxsuchthatx⊤u =0foralli ∈ [τ]given
i
τ < d.
Also,
(cid:16) (cid:17)
λ (G−1) = λ γ−1I−(U K U⊤) (45)
min t,γ min t t,γ t
≥ γ−1−λ (U K U⊤) (46)
max t t,γ t
>0. (47)
Here,weapplyWeyl’stheoremoneigenvaluestoobtainthefirstinequality.
A.2 ProofofLemma2
Lemma2. Wehave
L
η tλ min(G t− ,γ1)∥∇L(θ t)∥2 ≤E[L(θ t)−L(θ t+1)]+ 2η t2(M g/γ)2
forallt ≥0.
13Proof. Define∆
t
= θ t+1−θ
t
= −η tG t− ,γ1∇L(θ t;x t,y t). Wehave
(cid:90) 1
L(θ t+1)−L(θ t)−∇L(θ t)⊤∆
t
= (∇L(θ t+ρ∆ t)−∇L(θ t))⊤∆ tdρ (48)
0
(cid:90) 1
≤ ∥∇L(θ +ρ∆ )−∇L(θ )∥∥∆ ∥dρ (49)
t t t t
0
(cid:90) 1
≤ Lρ∥∆ ∥∥∆ ∥dρ (50)
t t
0
L
= ∥∆ ∥2. (51)
t
2
Noticethat
E[∆ ] =E[−η G−1∇L(θ ;x ,y )] (52)
t t t,γ t t t
=−η G−1E[∇L(θ ;x ,y )] (53)
t t,γ t t t
=−η G−1∇L(θ ) (54)
t t,γ t
and
E[∥∆ ∥2] =E[η2∥G−1∇L(θ ;x ,y )∥2] (55)
t t t,γ t t t
≤η2∥G−1∥2E[∥∇L(θ ;x ,y )∥2] (56)
t t,γ 2 t t t
≤η2(M /γ)2, (57)
t g
where||isthelargesteigvenvalueofG.Wefurtherhave
L
E[L(θ t+1)−L(θ t)] ≤∇L(θ t)⊤E[∆ t]+ E[∥∆ t∥2] (58)
2
L
≤−η ∇L(θ )⊤ G−1∇L(θ )+ η2(M /γ)2 (59)
t t t,γ t 2 t g
L
≤−η λ (G−1)∥∇L(θ )∥2+ η2(M /γ)2, (60)
t min t,γ t 2 t g
whichcompletestheproof.
14