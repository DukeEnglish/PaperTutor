Fast Peer Adaptation with Context-aware Exploration
LongMa12 YuanfeiWang12 FangweiZhong12 Song-ChunZhu12 YizhouWang1
Abstract
Fastadaptingtounknownpeers(partnersorop-
ponents) with different strategies is a key chal-
lenge in multi-agent games. To do so, it is cru-
cial for the agent to efficiently probe and iden-
tifythepeer’sstrategy,asthisistheprerequisite
forcarryingoutthebestresponseinadaptation. Figure1.Anexampleoffastpeeradaptationwithexperiencesfrom
However, it is difficult to explore the strategies onlineinteraction,whereamotheremploysherpriorexperiences
of unknown peers, especially when the games withherbabyascontextualcuestodeterminetheappropriateitem
are partially observable and have a long hori- toofferandfurtherexplorethebaby.Intheinitialencounter,hav-
zon. In this paper, we propose a peer identifi- ingobservedthebaby’sdisinterestinthemilkbottle,themother
infers that the baby is not hungry and suggests a toy as an al-
cationreward,whichrewardsthelearningagent
ternative. Despite theinitialunfavorableresponse totheteddy
based on how well it can identify the behavior
bear, thereisadiscernibleimprovementinthebaby’sreaction,
pattern of the peer over the historical context,
ultimatelyleadingthemothertosuccessfullychooseatoycarin
such as the observation over multiple episodes.
theirthirdinteraction.
Thisrewardmotivatestheagenttolearnacontext-
aware policy for effective exploration and fast agentstoefficientlyprobeandidentifythepeer’sstrategy
adaptation, i.e., to actively seek and collect in- andrespondwiththeoptimalstrategiesaccordingly. Thisis
formative feedback from peers when uncertain essentialforachievingsuccessfulcooperationorexploita-
abouttheirpoliciesandtoexploitthecontextto tion in various domains. For example, in board and card
perform the best response when confident. We games(Brown&Sandholm,2019;Silveretal.,2017;Hu
evaluateourmethodondiversetestbedsthatin- etal.,2020),agentsneedtoadjusttotheskillandstyleof
volvecompetitive(KuhnPoker),cooperative(PO- theiropponentsorteammates,suchasbluffing,coordina-
Overcooked),ormixed(Predator-Prey-W)games tion, or signaling. In DOTA (Berner et al., 2019), agents
withpeeragents.Wedemonstratethatourmethod needtocopewiththedynamicstrategiesandtacticsofthe
inducesmoreactiveexplorationbehavior,achiev- enemyteam,suchascounter-picking,ganking,orpushing.
ing faster adaptation and better outcomes than
Previousworksoverlookedtheroleofefficientexploration
existingmethods. Morevividexamplesareavail-
inobtaininginformativefeedbackforfastpeeradaptation.
able on the project page: https://sites.
Theymainlyfocusedonopponentmodeling(Heetal.,2016;
google.com/view/peer-adaptation
Raileanuetal.,2018;Wangetal.,2022;Papoudakisetal.,
2021;Yuetal.,2022;Albrecht&Stone,2018),assuming
1.Introduction thattheegoagentscanreadilyobservetheothers’behav-
iorsandinferthebestresponseaccordingly. However,this
Fastadaptationtodiversepeersisakeyabilityforsocial assumption may not hold in partially observable environ-
agents, who often face unknown peers with different co- ments,whicharemorerealisticandchallengingformulti-
operative or competitive strategies in multi-agent games. agent games. Therefore, the ego agent needs to learn an
Thus,theagents’performancehingesonhowquicklyand exploratorypolicythatcanactivelyinducethegamestates
effectivelytheycanadapttothesepeers. Thisrequiresthe thatrevealthepeers’preferencesandstrategies.
*Equal contribution 1Peking University, Beijing, China Learningtoexploreinpeeradaptationischallenging,asitin-
2NationalKeyLaboratoryofGeneralArtificialIntelligence,BI- volvestradingoffshort-termandlong-termoutcomesunder
GAI, Beijing, China. Correspondence to: Fangwei Zhong
limited context and without explicit reward signals. Fig-
<zfw@pku.edu.cn>.
ure.1showsanexampleofexploringthebaby’spreference
Preliminarywork.UnderreviewbytheInternationalConference whensoothingacryingbaby. Likewise,incardgames,play-
onMachineLearning(ICML).Donotdistribute. ersmaycallbluffstolearnopponents’tendencies,risking
1
4202
beF
4
]IA.sc[
1v86420.2042:viXraFastPeerAdaptationwithContext-awareExploration
Peer
Identification 
−  − 
Episode …  ,   , 
1

Context
Encoder   − 

Episode …
n Ego Agent Environment Peer Agents ∼
Peer
New Time Step
Context Pool

Figure2.IllustrationofPACE.Theegoagent(left)istrainedagainstadiversepoolo(  fp,e  ers)(right)duringtraining. Conditionedon
thepastepisodes,theegoagentproposesnewactionstoexplorethepeerorexploitthebestresponse.Thepeeridentificationobjective
backpropagatestothecontextencoderandgeneratesexplorationrewardforthepolicytomaximizemutualinformation.
short-termlossesbutenhancinglong-termstrategy. Simi- the peer identification reward to address the insufficient
larly,inDOTA,humanplayersuseexplorationbehaviors explorationproblem. 2)Weintroduceapracticalcontext-
suchaswardingtoobtainenemystates,sacrificingtimeand awarepolicylearningmethodthatoptimizescross-episode
resourcesbutenablingcounter-strategy. taskrewardswithexplorationrewardsagainstadiversepool
ofpeers,promotingexploration-exploitationbalance. 3)We
Motivated by this, we propose a fast Peer Adaptation
empiricallyvalidateourmethodinacompetitivecardgame
method with Context-aware Exploration (PACE) that en-
(Kuhn Poker), a cooperative game (PO-Overcooked) and
courages context-aware exploration of peer agents while
amixedenvironment(Predator-Prey-W),showingthatour
optimizingtotalreturnsovermultipleepisodesofinterac-
context-awarepolicycanquicklyadapttounknownpeers
tionduringadaptation. Weintroducepeeridentificationas
andachievehighperformance.
anauxiliarytaskthatfacilitatestherecognitionofthepeers’
strategiesbasedontheobservedcontext. Duringtraining,
2.RelatedWork
a peer identifier is trained to estimate a representation of
FastAdaptation. FastAdaptationincludesadaptationto
thecurrenttrainingpeerpolicygiventhecurrentinteraction
newenvironments(tasks)(Finnetal.,2017;Raileanuetal.,
context. Withtheidentification,apeeridentificationreward
2020;Zuoetal.,2019;Laskinetal.,2022;Luoetal.,2022)
isusedtoguidethelearningoftheegopolicy,promoting
andnewagents(Stoneetal.,2010;Ravula,2019;Rakelly
exploratorybehaviorsandtheemergenceofinformativecon-
etal.,2019;Zhuetal.,2021;Zhongetal.,2019;2021;Rah-
textsaboutpeerpolicies. Theegopolicyiscontext-aware
manetal.,2021;Yanetal.,2023).Inthispaper,weconsider
and trained to optimize a multi-episode return objective,
the problem of fast adaptation to unknown agents. Meta-
strikinganoverallexploration-exploitationbalanceacross
learningmethods(Al-Shedivatetal.,2018;Kimetal.,2021)
multipleepisodesofonlineadaptation. Thecontextencoder
computemeta-multiagentpolicygradientduringinteraction
issharedbetweentheegopolicyandtheauxiliarytaskto
toadaptthepolicyaccordingly. Bayesianinference(Zint-
providebetterpeerrepresentationsforpolicylearning.
grafetal.,2021)hasbeeninvestigatedforupdatingthebe-
We conduct experiments in a competitive environment liefaboutotheragentstoeffectivelyrespondtothem. Some
(KuhnPoker),acooperativeenvironment(PO-Overcooked), methods(Zhangetal.,2023;Guetal.,2021)utilizeValue
andamixedenvironmentwithbothcooperativeandcompet- Decompositionwithteammatecontextmodelingtoachieve
itivepeers(Predator-Prey-W).WeshowthatPACEadapts onlineadaptation. Modelingofotheragentscanhelpim-
faster and achieves higher returns than existing methods provetheadaptationtothem(Heetal.,2016;Raileanuetal.,
whenfacingunknownopponentsorcollaborators. Infurther 2018;Wangetal.,2022;Papoudakisetal.,2021;Yuetal.,
ablationstudies,weanalyzetheeffectsoftheproposedaux- 2022;Albrecht&Stone,2018;Fuetal.,2022;Xieetal.,
iliarytaskandthecorrespondingintrinsicreward. At-SNE 2021). However, the previous methods assume that the
visualizationofthelatentembeddingsshowsthatthePACE contextsaboutpeersareeasytoobtain,whilewefocuson
agentquicklydistinguishesbetweenthepeerpolicies. partiallyobservablegamesthatrequireactiveexplorationto
collectthecontextsaboutpeers. Ourmethodleveragesthe
Insummary,ourmaincontributionsarethree-fold. 1)We
auxiliarytasktogenerateanintrinsicrewardtoboostthe
investigatethepeeradaptationproblemindetailandpropose
explorationstrategylearning.
2
……
……FastPeerAdaptationwithContext-awareExploration
LearningtoExplore. Explorationisalong-studiedprob- 3.2.Context-awarePolicy
lem in single-agent and multi-agent reinforcement learn-
Tooptimizeobjective(Eq.1),theegoagentneedstolever-
ing(Haoetal.,2023). Itiscrucialtosufficientlyexplorethe
ageitslocaltrajectoriesofobservationsandactions,referred
taskspacetofindanoptimalpolicy,butlong-horizonand
to as context, to infer the type and mind (e.g., belief, in-
sparserewardpropertiesmayhingeeffectiveexploration.To
tention,desire)ofitspeeragentsandtakeappropriateac-
overcomethedifficulties,severalworks(Pathaketal.,2017;
tions. Formally,wedenotethecontextfortheegoagentas
Burdaetal.,2018;Zhangetal.,2021;Badiaetal.,2020)
C1 = {{o1 ,a1 }Tn }N ,consistingoftheegoagent’s
introducevariousintrinsicrewardsmeasuringcuriosityor n,t n,t t=1 n=1
localobservationsandactions. T isthelengthofthen-th
dynamicerrortoboostexplorationinsingle-agentreinforce- n
episode. ThecontextalsoincludesthecurrentepisodeN,
mentlearning. Furthermore,thereareseveralattemptsat
which may be incomplete; in this case, T is the current
solvingthemulti-agentexplorationproblem. Maven(Ma- N
numberofstepsinepisodeN.
hajanetal.,2019)proposesasharedlatentspaceforhier-
archicalpolicybasedonthevaluedecompositionmethod ThenumberofepisodesinthecontextC1mayvary,andso
forbetterexploration. CMAE(Liuetal.,2021)introduces dothelengthsoftheepisodesinasinglecontext. Webuild
a shared common goal selected from restricted space to acontextencoderχparameterizedbyθtoencodecontexts
promote cooperative exploration. Some works (Iqbal & withvaryingsizestoafixed-lengthvectorz1 ∈Rm:
Sha,2019;Zhengetal.,2021;Jaquesetal.,2019)alsotry
to extend the intrinsic reward to multi-agent exploration.
However, all of these works focus on how to effectively (cid:32)
1
(cid:88)N
1
(cid:88)Tn (cid:33)
explore during the learning process to obtain an optimal z1 :=χ θ(C1)=g θ N T f θ(o1 n,t,a1 n,t) (2)
n
policy, whereas we focus on efficient exploration during n=1 t=1
onlineinteractionwithpeerstoidentifytheirpolicies.
where f : R|O1| × R|A1| → Rm,g : Rm → Rm are
θ θ
3.Method MLPs, χ(∅) := 0. With the context encoded by χ, a
context-aware policy π (a|o,χ (C)) is trained to maxi-
3.1.ProblemFormulation θ θ
mizethediscountedreinforcementlearning(RL)objective:
We formulate the underlying game of peer adap-
tation as a finite-horizon partially observable maximize
E[(cid:88)Neps (cid:88)Tn
γt′ r1 ] (3)
stochastic game (POSG) (Hansen et al., 2004) n,t
n=1t=1
⟨I,S,b0,{A },{O },P,{R }⟩, where I is the fi-
i i i
nitesetofallagentsintheenvironment,S isthestatespace, whereγ ∈(0,1)isthediscountfactor,t′ :=t+(cid:80)n−1 T
n′=1 n′
b0 ∈ ∆ S is the initial state distribution, A i,O i,R i is the is the cumulative number of time steps until time step t
action space, observation space, and reward function for of episode n. This objective is amenable to standard RL
agenti,andP isthetransitionfunctionP(s′|s,a). Every algorithms by concatenating N episodes together and
eps
episodeofthegameterminatesinfinitelymanytimesteps computing the total returns, similar to the multi-episode
(finite-horizon). Followingpriorwork(Fuetal.,2022;Gu returnobjectiveusedin(Xieetal.,2021).
et al., 2021), we consider the case with a single adaptive
Theencoderandthepolicyarejointlyparameterizedand
agentπ (egoagent)andmpeeragentsψ. Ourgoalisto
optimizedinanend-to-endmannerusingPPO(Schulman
learnapolicyπ thatcanadapttovariouscombinationsof
θ
etal.,2017).Byend-to-endtrainingofbothcomponents,the
peeragentsfortheegoagent.
encoderχ istrainedtoextractthecharacteristicsofpeers
θ
In peer adaptation, the ego agent will repeatedly interact from trajectories sampled by the current policy π . This
θ
withpeeragentsoverN epsepisodesofPOSGtomaximize eliminatesthedistributionmismatchpotentiallyencountered
thecumulativereturn.Denotingtheegoagentasagent1,we by some prior works (Fu et al., 2022) that separate the
definetheobjectiveofpeeradaptationoverN epsepisodes learningofagentmodelingandpolicy.
asthecumulativereturnoverallepisodes:
3.3.Context-awareExplorationwithPeerIdentification
(cid:88)Neps (cid:88)Tn
maximize E[ r1 ] (1)
n,t Foradaptationtounknownpeers,theegoagentmustfirst
n=1t=1 infercertaincharacteristicsofthepeeragents,e.g.strategies
wherer1 istherewardfortheegoagentattimesteptof andpreferences,tocarryoutthebestresponse. However,
n,t
episoden,T isthelengthofepisoden. Ourmulti-episode theinferenceprocesscanbehinderedinpartiallyobserv-
n
objectiveposessignificantchallengestolearningadaptation ableenvironments,asthebehaviorsofpeeragentsmaynot
policy,asalonghorizoncoupledwithpartialobservability always be revealed in the ego agent’s observation. Peer
makesitdramaticallyhardertoexploreandexploit. adaptationthusrequiresasophisticatedexplorationstrategy,
3FastPeerAdaptationwithContext-awareExploration
whichishardtolearndirectlyfromtheoriginaltaskreward. lossforapproximatingtheposteriordistributionisgivenby
Forexample,inthePO-Overcookedenvironment,theego
agentcanobservethepeeragent’spolicybygoingacrossthe  
m
h ioo rri cz ao nn nta ol tw yia el ll din at no yth tae sklo rw ewer ar ro do im m. mH eo dw iae tev le yr ., Ath sis ab re eh sua lv t-
,
L aux(θ)=E ψ,C1 m1 (cid:88) CE(h θ(χ θ(C1)) j,i j) (5)
j=1
thelearningpolicywillquicklyconvergeduringtheinitial
stages of training to the local optimum strategy, i.e., not
visitingthelowerroom.Thisproblemisfurtherexacerbated whereh θ(χ θ(C1)) j istheposteriorcategoricaldistribution
bytherequirementofadaptationacrossmultipleepisodes. fortheindexofpeeragentj,CE isthecross-entropyloss.
Toproperlyadapttothepeers,theegoagentmayneedto Optimizing L aux requires sampling paired peers and con-
performanexplorationbehaviorfirsttoactivelycollectin- texts(ψ,C1). WecollectthecontextsgeneratedduringRL
formativecontextsforreasoning,identifythecharacteristics traininginabuffertoprovidedataforL aux,sotheidentifier
ofthepeersbasedontheobservedcontext,andthencarry alwaysstayson-policy. SeeSection3.4fortrainingdetails.
outaspecificresponsestrategy. Failureinanyofthesesteps
AfterestimatingP(ψ|C1),formaximizingthemutualin-
wouldleadtoanoverallfailureanddisruptthelearningof
formationobjective(Eq.4),weaddanexplorationreward
othersteps. Asaresult,itisextremelydifficulttolearnsuch
basedontheobjective:
strategieswithoutanyincentiveforexploration.
Toovercomethisissue,weproposethemaximizationofthe m
1 (cid:88)
followingmutualinformationobjective: re := h (χ (C1)) (6)
m θ θ j,ij
j=1
I(ψ,C1) whichistheestimatedposteriorprobabilitiesoftheactual
= H(ψ)−H(ψ|C1) (4) peer agents, re ∈ [0,1]. We directly use the probability
= H(ψ)+E[logP(ψ|C1)] insteadofthelogversionsincetheprobabilityisbounded.
For encouraging the balance between exploring the peer
agentsandexploitingtoachievetasksuccess,thefinalre-
wheretherandomnessistakenoverchoiceofpeeragents wardfortheegoagentiscomputedasr1 =r+c·re,where
ψ,thepoliciesoftheegoagentπ andthepeeragentsψ,
θ ristheoriginaltaskrewardandcisacoefficient. During
andtheenvironmentdynamics. Intuitively, thisobjective
training,welinearlydecayscfromc to0inM environ-
init
encouragestheegoagenttoactinawaysuchthatitscon-
mentsteps.Theexplorationrewardisnotusedduringonline
textcontainsenoughsignalstoidentifythepoliciesofthe
adaptation,astheground-truthpeeridentitiesareunknown.
peeragents,leadingtoactiveexplorationbehaviors. After
SeeAppendixC.1,C.2fordetails.
thoroughlyprobingthepeeragents,theegoagentcanthen
adaptitspolicytotakethebestresponse. Duringtraining,
3.4.TrainingStrategy
we utilize a diverse peer pool Ψ from which peer agent
policiesψaresampled. Forthisconcretepeerdistribution Wepresentourtrainingalgorithmthatenablestheegoagent
Ψ,H(ψ)isaconstantwithrespecttoparametersθ. to adapt to peers with markedly different strategies. To
accomplishthis,weassumethatadiversepeerdistribution
Now, tocomputeP(ψ|C1)andmaximizeit, wepropose
isavailablefortraining,fromwhichwedrawseveralpolicies
peeridentificationasanauxiliarytask.Thistaskservestwo
asthetrainingpeerpoolΨ.Theegoagentistrainedtoadapt
purposes: theestimationofP(ψ|C1)yieldsabettercon-
toallthepeersinthetrainingpeerpoolΨ.
textrepresentation,whichisusefulfordownstreampolicy
learning;theestimatedP(ψ|C1)canbeaddedasanexplo- SeeAlgorithm1forpseudocode. Duringtraining,wemain-
rationrewardtomaximizeEq.4andpromoteexploratory tainanenvironment, acontext, andacurrentobservation
behaviors.Wetrainanidentifierh :Rm →∆ toproduce foreverytupleofpeeragentsψinthetrainingpeerpoolΨ
θ ψ
theposteriordistributionofpeeragentsh (χ (C1))given (Line2-3). Thecontextreceivesanewobservation-action
θ θ
contextC1. Theidentifierisjointlyparameterizedwiththe pairateverytimestepduringpolicyrollout(Line11)and
contextencoderχ .Thetrainingofidentifierh dependson getsclearedafterreachingN episodes(Line13). During
θ θ eps
theparameterizationofpeerpoliciesψ,whichcanberule- policyrollouts,theintrinsicexplorationrewardisgenerated
basedorparameterizedbyaneuralnetwork, etc. Forthe andaddedtothetaskreward(Line9). Aftercollectinga
generalcasewithoutanyknowledgeorassumptionabout batch of data, we update the actor and critic for RL and
thepeerpoolΨ,weparameterizeψ asatupleofindexes thecontextencoder(Line18). TheRLlossesandL are
aux
(i ,i ,...,i ), where ψ ∈ Ψ = {ψ ,ψ ,...,ψ } is computedwiththesamemini-batchandaddedtogetherfor
1 2 m ij 1 2 |Ψ|
thepolicyofthej-thpeeragent. Inthiscase,thetraining optimization(AppendixC.2).
4FastPeerAdaptationwithContext-awareExploration
P1 (Ego Agent) P2 (Peer Agent) Terminal Node (with payout)
Watchtower 
 
Ego
Agent
Predator
Prey
 Obstacle 
 

(a)KuhnPoker (b)PO-Overcooked (c)Predator-Prey-W
Figure3.IllustrationsofKuhnPoker(a),PO-Overcooked(b),andPredator-Prey-W(c).In(a),thehandofthepeeragentisonlyrevealed
atshowdowns(bluediamondnodes);in(b),themaskedgrayareaindicatestheunobservedareatotheegoagent(theagentintheleft
room);in(c),theegopredatorcanonlyhavefullobservabilityduringcontactwiththewatchtowers(bluecircles).
Algorithm1TrainingProcedureofPACE andtheintrinsicrewardinfluencelearningandadaptation?
Require: TrainingpeerpoolΨ,contextsizeN 6)WhatdoesPACElearninitslatentspace?
ctx
1: Randomlyinitializeθ,t←0
2: ∀ψ ∈Ψ,C1 ←∅{Initializetrainingcontexts} 4.1.ExperimentSetup
ψ
3: Initialize|Ψ|environmentsandresettogeto1 0forallψ To demonstrate the validity of PACE, we conduct online
4: whileMaximumtrainingstepnotreacheddo adaptation experiments in three commonly used environ-
5: D ←∅{Initializecurrenttrainingbatch} ments in the field of MARL: Kuhn Poker (Kuhn, 1950),
6: whileCurrentbatchsizenotreacheddo PO-Overcooked(Carrolletal.,2019), andPredator-Prey-
7: forallψ ∈Ψdo W(Loweetal.,2017). Theenvironmentsencompassawide
8: Steptheenvwitha1 t ∼ π θ(a|o1 t,χ θ(C ψ1))and spectrumofscenarios,characterizedbydiverseaspectssuch
a−
t
1 ∼ψ,obtainr t,o1
t+1
ascooperative,competitive,andmixedsettings;partialob-
9: Computer t1usingEq.6andr t1 =r t+c·r te servability;multiplepeeragents;aswellasshortandlong
10: Put(C1,ψ,o1,a1,r1)intoD time horizons. We use the average episodic rewards or
ψ t t t
11: UpdateC1 with(o1,a1) successratesoverN episodesastheevaluationmetric.
ψ t t eps
12: ifC1 containsN completeepisodesthen
ψ eps KuhnPoker(Kuhn,1950). Thisisasimplifiedtwo-player
13: C ψ1 ←∅ poker game involving a deck of three cards and at most
14: endif threerounds. Thegametreesforeverypossibleassignment
15: endfor ofhandcardsareshowninFig.3a. Bothplayersplacean
16: t←t+1 ante of 1 before any action, and each player is dealt one
17: endwhile privatecardinitially. TheleftcardisforP1andtheright
18: UpdateθwithPPOlossandL auxusingD cardisforP2inFig.3a. Theplayers,P1andP2,taketurns
19: endwhile todecidewhethertoBet(Call)orCheck(Fold). Thegame
endswhenoneoftheplayersunilaterallyfoldsandforfeits
thepottotheotherplayer. Ifneitheroftheplayersfolds,the
gameendsinashowdown(Fig.3a,bluediamondnodes),
4.Experiments
wherethehandsoftheplayersarerevealedtoeachotherto
In this section, we conduct experiments in Kuhn Poker decidethewinner. Inthispaper,theegoagentisP1while
(Competitive),PO-Overcooked(Cooperative),andPredator- thepeeragentplaysP2. Thepeeragentpoolisgenerated
Prey-W(Mixed)toanswerthefollowingquestions: 1)How similarlyasin(Southeyetal.,2009),whichparameterizes
well can PACE exploit the opponent in the competitive theP2policyspacewithtwoparameters.
setting? 2) How well can PACE adapt to the partner in
Partially Observable Overcooked (PO-Overcooked).
thecooperativesetting? 3)HowwellcanPACEperformin
Overcooked(Carrolletal.,2019)isacollaborativecooking
thepresenceofbothkindsofpeers? 4)CanPACEadaptto
gamewhereagents,actingaschefs,worktogethertocom-
peerswithsuddenchanges? 5)Howdopeeridentification
5FastPeerAdaptationwithContext-awareExploration
0.075 1.0
00 .. 00 25 50 0.8 P G L L LA I I Ie A A LC n IM ME e Xralist 33 .. 50 P G L L LA I I Ie A A LC n IM ME e Xralist
0.000 0.6 G OS raC cU le G OS raC cU le
4.0
0.025 0.4
PACE
0.050 Generalist 4.5
LIAM 0.2
LIAMX
0.075 LILI
GSCU
Oracle 0.0 5.0
10 20 30 40 50 60 70 80 90 100 1 2 3 4 5 1 2 3 4 5
Evaluation Episodes Evaluation Episodes Evaluation Episodes
(a)KuhnPoker (b)PO-Overcooked (c)Predator-Prey
Figure4.TheonlineadaptationresultsonKuhnPoker(a),PO-Overcooked(b),andPredator-Prey-W(c).PACEcontinuouslyimproves
overthewholeonlineadaptationprocess,outperformingbaselinesinallenvironments.Inparticular,PACEistheonlyagentcapableof
adaptationinthePO-Overcookedenvironment.Oracledenotesthebestresponsesdesignedseparatelyforeverypeerinthetestpool.
pleteaseriesofsub-tasksandservedishes. Toaddtothe ingcontactbutyieldnoimmediaterewards. Toconstruct
challengeandpromotediversepolicybehaviors,weprovide diverse behaviors, prey policies in the peer pool are rule-
apartiallyobservablemulti-recipeversionofOvercooked basedpolicieswithdifferentpreferencesonthesequences
basedon(Charakornetal.,2023),showninFig3b. Oursce- toreachlandmarks,whileeverypredatorpolicyprefersto
nariofeaturesatotalof6kindsofingredientsand9recipes. chasespecificprey.
The game environment includes a series of counters that
Wechoosethefollowingbaselinestovalidatetheadapta-
divide the room, compelling the agents to collaborate by
tionandpeermodelingcapabilityofPACE.1)GSCU(Fu
passingobjects,includingingredientsandplates,overthe
et al., 2022) trains a conditional policy conditioned on a
counter. Theegoagentistheleftagentinchargeofmaking
pre-trainedopponentmodel. Notably,GSCUassumesthe
disheswhilethepeeragentdeliversthemattheright. There
availabilityofpeerobservationsandactionsaftertheendof
isalsoahorizontalwallacrosstheroom,blockingtheline
anepisode,whichisnotnecessaryforPACE.2)LIAM(Pa-
ofsightoftheagents,forcingthemtomoveacrossthewall
poudakisetal.,2021)modelstheopponent’sobservation
toseetheotherside. InFig.3b, theleftfigureshowsthe
andactionasanauxiliarytaskunderpartiallyobservable
egoagentintheupperroomandcan’tseetheobjectsinthe
settings. 3)LIAMXisavariantofLIAMwithcross-episode
lowerroom,unlessitgoesacrossthehorizontalwallasin
contexts. 4)LILI (Xieetal.,2021)modelsthetransitions
therightfigure.
observedbytheegoagentusingthelastepisode,implicitly
Togenerateadiversepeerpool,weadoptarule-basedap- encodingtheopponentasenvironmentdynamicsfortheego
proachbyconstructingacollectionofpeerpoliciesasthe agent. 5)Generalistisaplainrecurrentpolicywithaccess
rightagent,eachrepresentingaspecificpreferenceforin- tocross-episodecontexts.
gredientsandrecipes. Therule-basedagentsaredesigned
totakeandserveonlytheingredientsanddishesthatmatch 4.2.HowwellcanPACEexploittheopponentinthe
theirpreferences. Existingapproaches(Strouseetal.,2021; competitivesetting?
Charakornetal.,2023;Lupuetal.,2021)primarilyemploy
InKuhnPoker,werandomlysample40P2policiesfrom
ReinforcementLearningalgorithmsinconjunctionwithdi-
theparameterizedpolicyspaceasthetrainingpool. Wealso
versityobjectivestotrainpoliciesthatexhibitawiderange
sampleanother10differentpoliciesforonlineadaptation
of behaviors. In this work, we leverage these preference-
testing. ThistestingprocedurespansN =100episodes.
basedpoliciestocapturemorehuman-likebehaviorwithin eps
Theaveragerewardsevery10episodesduringtheonline
thegame(AppendixB.2).
adaptationarepresentedinFigure4a. Standarddeviations
Predator-Prey with Watchtowers (Predator-Prey-W). arereportedover3trainingseeds. Theaveragerewardsover
Weemploythepredator-preyscenarioinMulti-agentParti- all100episodesarereportedintheAppendix(Table3).
cleEnvironment(Loweetal.,2017)withbothcollaborative
As is shown in Figure 4a, PACE continuously improves
and competitive elements. To increase the difficulty of
its rewards and outperforms the baselines. This demon-
exploration,weintroducepartialobservabilityandwatch-
stratestheeffectivenessandefficiencyofthelearnedpolicy
towerstotheenvironment. Whiletheobservationoftheego
in adapting to the unknown opponents. In particular, we
agentisrestrictedtoasmallradiusaroundtheagent(Fig.3c,
notethatthePACEagentalsoexplicitlyoptstoexplorethe
blackdottedcircle),itmaychoosetovisitthewatchtowers
peer’sstrategybyusingamoreaggressivestrategyforthe
aroundthemap,whichrestorefullobservabilityforitdur-
first10episodes. Duringthistime,thegameentersshow-
6
draweR etaR sseccuS draweRFastPeerAdaptationwithContext-awareExploration
downatahigherrateof∼ 0.64forthePACEagent,soit
Table1.TheaveragesuccessratesofPACEandLIAMXwhen
getstoseethepeer’shandmorefrequentlyandobtainsmore
adaptingtosuddenlychangedpeersinPO-Overcooked.PACE(*)
informationaboutthepeer’sstrategy. Whilethisleadsto
isthesuccessratewithstationarypeersforreference.
certainshort-termlossesinrewards,theoverallperformance
isgreatlyimprovedto0.047,whilethebestbaselinefetches
PACE(*) PACE Generalist LIAMX
0.027(Tab.3). Incomparison,theshowdownrateforLILI
0.553±0.029 0.435±0.027 0.120±0.007 0.116±0.004
holdssteadyat∼0.60overallofthe100episodesofinter-
action. GSCUalsoimprovesalongtheonlineinteractions
but fails to reach a satisfying level of rewards within the
testing time horizon, due to a low starting point. During Predator-Prey-W,theegoagentisapredatorseekingtocol-
onlineadaptation,GSCUmayattimesusea“conservative laboratewithapeerpredatorandcatchtwopeerprey. To
policy"thatistheNashEquilibrium(NE)policyinKuhn achievethis,theegoagentmustfirstdeterminethetrajec-
Poker. ThisNEpolicydoesnotactivelytrytoexploitits toriesofthepreyandwhichpreythepeerpredatorprefers,
opponent,leadingtoarelativelyunsatisfactoryperformance thentracktheotherprey. Eachofthesestepsishardina
atfirst. Otherbaselinesmainlyfluctuatearoundtheinitial partiallyobservableenvironment,whichrestrictstheobser-
performance, showing that it is hard to make use of the vationofeachagenttoasmallradiusaroundtheagent. To
contextandexplorepeerstrategieswithoutproperguidance. mitigatethisrestriction,theegoagentcanvisitwatchtow-
ersaroundthecornersofthemaptogainfullobservability
4.3.HowwellcanPACEadapttothepartnerinthe
duringcontactwiththewatchtowers. Wesample16combi-
cooperativesetting?
nationsoftrainingpreyandpredatorpoliciesasthetraining
ThePO-Overcookedenvironmentposesasignificantchal- pooland24separatecombinationswithunseenpreypolicies
lengeforadaptationincoordination. Tocookameal,the foronlineadaptation.
ego agent in the left room has to work together with the
WereporttheonlineadaptationperformanceoverN =5
peeragentintherightroom. Notethattheagentcannot eps
episodes in Figure 4c and average performance in Table
gototheotherroom,forcingthecollaboration.Thecontext
3. Standarddeviationsarereportedover3trainingseeds.
consists of a small number of episodes (N = 5), each
eps PACEachieveshigherrewardsthanallthebaselinesover
lastingfordozensofsteps. However,asthepeeragentonly
thewholeadaptationprocedure. Theonlybaselinecapable
touches ingredients and dishes within its preference and
of some adaptation is LIAMX, demonstrating that cross-
ignoreseverythingelse,mostofthecontextcontainslittle
episodecontextswithauxiliaryobjectivescanindeedim-
informationaboutitstruepreference. Thisrequirestheego
proveperformance. However,LIAMXstillunderperforms
agenttoactivelyperformexploratoryactions. Wesample
PACE,astheobjectiveofLIAMXissimplytosupervise
18 policies from the training pool and another 9 policies
the encoder without altering policies, while PACE gener-
fromthetestingpool.
atesadditionalrewardstomaximizethemutualinformation
Figure4bshowsthatPACEistheonlyagentthatcanadapt objective in Eq. 4. GSCU similarly fails to adapt as in
tothepeerinthePO-Overcookedenvironment,achieving PO-Overcooked. Thesmallernumberofepisodesinboth
anaveragesuccessrateof0.553,whileallofthebaselines environmentsmayalsocontributetotheunderperformance
failwithsuccessratesofaround0.1. Standarddeviations ofGSCU,asGSCUrunsvariationalinferenceandadjusts
arereportedover3trainingseeds. Theaveragerewardsare itsgreedypolicyonlyonceaftereachepisode.
reported in Table 3 in the Appendix. Specifically, while
4.5.CanPACEadapttopeerswithsuddenchanges?
GSCUcanadapttoitspeersandkeepimprovinginKuhn
Poker,itfailsinPO-Overcookedduetothelackofaneffec- Inadditiontoadaptingtostationarypeers,inthissection,
tiveexplorationstrategy. TheconservativepolicyofGSCU wedemonstratethatPACEcandetectandadapttochanges
inPO-Overcookedisageneralistpolicythatrarelyserves inpeerpoliciesbyconductingasudden-changeexperiment
thepreferreddish. Consequently, thepeerstandsstillfor inPO-Overcooked. Over10adaptationepisodes, weuse
mostofthetime,revealinglittleinformationforGSCUto a single tuple of peer agents in the first 5 episodes, then
model. The results also demonstrate that the context en- switchtoanothertupleofpeeragentsforthelast5episodes.
coderχ canefficientlysummarizelong-termcontextsand However,thetimingofthepeerswitchisunknowntothe
θ
captureonlytheusefulportion. egoagent. Todetectthechangeinpeerstrategies,weusea
heuristicsbasedontherewardobservedbytheegoagent;
4.4.HowwellcanPACEperforminthepresenceof
seeAppendixC.3fordetails. Theegoagentclearsitscon-
bothkindsofpeers?
textandrestartsexplorationafterdetectingsuchchanges.As
WefurthervalidatetheperformanceofPACEinPredator- showninTable1,thePACEagentsuccessfullydetectsthe
Prey-W with both cooperative and competitive peers. In peerchangeandadaptsaccordinglywithsmallperformance
degradationcomparedtothestaticcase. Thebaselineslearn
7FastPeerAdaptationwithContext-awareExploration
seen that at the beginning of adaptation, both PACE and
Table2.TheaveragesuccessratesofPACEandablationsinPO-
LILIdonotknowthepreferencesofthepeeragents,asthe
Overcooked.
context is empty. This is indicated by the light cluster in
PACE PACE-reward PACE-reward-aux
themiddleofFig.5aandatthetopofFig.5b. However,
0.553±0.029 0.173±0.016 0.143±0.020 withtheaccumulationofmoreinteractioncontexts,PACE
successfullyprobesthepeersanddistinguishesbetweenthe
40 60 contextsofdifferentpeers,asthelatentsfirstsplitintothree
broadclusters,thenintoninesmallclusterscorresponding
20 40
toeachtestpeer. Incontrast,theLILIlatentsscatteraround
20
0 the space with no discernible pattern, indicating that the
0
encoderisunawareofthepeer’sidentity.
20 20
40 5.Conclusion
40
60
40 20 0 20 40 40 20 0 20 40 Inthispaper,weproposefastpeeradaptationwithcontext-
(a)PACE (b)LILI awareexploration(PACE),amethodfortrainingagentsthat
exploreandadapttounknownpeeragentsefficiently. Au-
Figure5.Thet-SNEplotofthelatentembeddingsproducedby tonomousagentsintherealworldobserveandadapttothe
thePACE(a)andLILI(b)encoderinPO-Overcooked.Eachcolor
behaviorsoftheirpeers,whethertofacilitatecooperationor
indicatesaspecifictestingpeer,whiletheshadesofcolordenote
exploitation,namelypeeradaptation. Toachievethisgoal,
thetimeorderduringadaptation.
agentsneedtostriketherightbalancebetweenexploration
andexploitation,recognizingthepeerpoliciesbeforetaking
policiesthatareunresponsivetopeerpolicies. Asaresult, theappropriateresponse. PACEleveragespeeridentifica-
thebaselinesalsofailtoadaptinthesudden-changesetting, tionasanauxiliarytasktoguidecontextencoderlearning
achievingconsistentlylowsuccessrates. andgenerateexplorationrewardsfortheagent. Withadi-
versepooloftrainingpeers,PACEtrainsacontext-aware
policy to maximize both the original task reward and the
4.6.Howdotheauxiliarytaskandrewardinfluence
exploration reward. The policy explores the peers when
learningandadaptation?
it is uncertain about the best response, and exploits the
Hereweperformablationexperimentstoexaminetheeffect bestresponseotherwise. WeconductexperimentsinKuhn
oftheauxiliarytaskandrewardonthetrainingprocessand Poker,PO-Overcooked,andPredator-Prey-W,threepopular
finalconvergence.Table2containstheaveragesuccessrates MARLenvironmentscoveringawiderangeofproperties.
overtheonlineadaptationprocedureforPACEandablations ExperimentalresultsconfirmthatthePACEagentcaneffi-
inPO-Overcooked. PACE-reward-auxisPACEwithneither cientlyexplorethepeersandadaptitspolicybasedonthe
theauxiliarytasknortheexplorationreward,whilePACE- context,achievinggoodperformanceinvariousscenarios.
reward ablates the reward and retains the task. It can be
LimitationandFutureWork.Therearecertainlimitations
observedthattheperformancedropsseverelyfrom0.553
to PACE. The algorithm requires a diverse peer pool for
to 0.173 after removing the auxiliary reward, indicating
training,whichisessentialfortraininganeffectiveadaptive
that the reward is critical for performance. Without the
agent. Generatingmorecomplexandsophisticatedstrate-
explorationreward,thepolicyquicklyconvergestothelocal
gies for the training peer pool could further enhance the
optimumofnotvisitingthelowerroom. Furtherremoving
performanceofPACE.Currently,ourexperimentsinvolve
theauxiliarytaskalsohurtsperformance. Incomparison,
the same number of agents in testing as in training. Ad-
the full PACE agent visits the lower room about once on
dressingscenarioswherethenumberofagentsmaychange
average in an online adaptation procedure, as shown in
betweenepisodesisanotherimportantfuturedirection. Al-
Figure6intheAppendix.
ternative auxiliary tasks may also benefit policy learning.
Moreover,extendingourapproachtoembodiedAIscenar-
4.7.WhatdoesPACElearninitslatentspace?
ios,wheretheagentshavetoperceiveandactincomplex3D
WevisualizethelatentspaceofPACEandcompareitwith environments(Qiuetal.,2017),couldopenupnewpossibil-
baselinesbycollectingthegeneratedcontextembeddings itiesandchallengesforonlinepeeradaptation. Furthermore,
duringonlineadaptationandprojectingtheminto2dimen- althoughakeygoalofmulti-agentlearningistobuildagents
sions using t-SNE. Fig. 5 is a visualization of latent em- thatcaninteractwithhumans,weusenohumanpeersinthis
beddingsgeneratedbytheencoderofPACEandLILI(Xie work. Animportantdirectionistoconducthumanstudiesto
et al., 2021) in PO-Overcooked. Each color corresponds evaluatehowPACEagentscaninteractwithhumanpeersin
tooneofthe9testingpeers,whiletheshadesofcolorde- varioussettings. Thiswouldrequireaddressingissuessuch
notethechronologicalorderduringadaptation. Itcanbe ashumanfactors,ethicalconsiderations,anduserfeedback.
8FastPeerAdaptationwithContext-awareExploration
BroaderImpact Etel,E.andSlaughter,V. Theoryofmindandpeercooper-
ationintwoplaycontexts. JournalofAppliedDevelop-
Peer adaptation is a fundamental problem in the area of
mentalPsychology,60:87–95,2019.
multi-agent reinforcement learning, as collaboration and
competitionareeverywhereintherealworld. Researchin Finn,C.,Abbeel,P.,andLevine,S. Model-agnosticmeta-
thisareafacilitatestheseinteractionsforautonomousagents learningforfastadaptationofdeepnetworks. InInterna-
inthefutureandimprovestheirefficiency. PACEcanalso tionalconferenceonmachinelearning,pp.1126–1135.
helpwiththeinteractionsbetweenhumansandautonomous PMLR,2017.
agents,likehomecarerobotsadaptingtotheneedsofthe
Fu,H.,Tian,Y.,Yu,H.,Liu,W.,Wu,S.,Xiong,J.,Wen,
elderly. However,likeothermachinelearningtechniques,
Y., Li, K., Xing, J., Fu, Q., et al. Greedy when sure
PACE faces the risk of misuse, especially in competitive
and conservative when uncertain about the opponents.
settings. Forexample,maliciousactorsmayuseittoengage
In International Conference on Machine Learning, pp.
inillegalactivitiesandcircumventlawenforcement. Active
6829–6848.PMLR,2022.
mitigationeffortsarerequiredbeforePACEcanbedeployed
intherealworld. Gu,P.,Zhao,M.,Hao,J.,andAn,B. Onlineadhocteam-
workunderpartialobservability. InInternationalConfer-
References enceonLearningRepresentations,2021.
Al-Shedivat,M.,Bansal,T.,Burda,Y.,Sutskever,I.,Mor- Hansen,E.A.,Bernstein,D.S.,andZilberstein,S.Dynamic
datch,I.,andAbbeel,P. Continuousadaptationviameta- programmingforpartiallyobservablestochasticgames.
learninginnonstationaryandcompetitiveenvironments. InAAAI,volume4,pp.709–715,2004.
InInternationalConferenceonLearningRepresentations,
Hao, J., Yang, T., Tang, H., Bai, C., Liu, J., Meng, Z.,
2018.
Liu,P.,andWang,Z. Explorationindeepreinforcement
learning: Fromsingle-agenttomultiagentdomain. IEEE
Albrecht,S.V.andStone,P. Autonomousagentsmodelling
TransactionsonNeuralNetworksandLearningSystems,
otheragents:Acomprehensivesurveyandopenproblems.
2023.
ArtificialIntelligence,258:66–95,2018.
He, H., Boyd-Graber, J., Kwok, K., and Daumé III, H.
Badia,A.P.,Sprechmann,P.,Vitvitskyi,A.,Guo,D.,Piot,
Opponentmodelingindeepreinforcementlearning. In
B.,Kapturowski,S.,Tieleman,O.,Arjovsky,M.,Pritzel,
Internationalconferenceonmachinelearning,pp.1804–
A., Bolt, A., et al. Never give up: Learning directed
1813.PMLR,2016.
explorationstrategies. arXivpreprintarXiv:2002.06038,
2020. Hu,H.,Lerer,A.,Peysakhovich,A.,andFoerster,J. “other-
play”forzero-shotcoordination. InInternationalCon-
Berner,C.,Brockman,G.,Chan,B.,Cheung,V.,De˛biak,P.,
ference on Machine Learning, pp. 4399–4410. PMLR,
Dennison,C.,Farhi,D.,Fischer,Q.,Hashme,S.,Hesse,
2020.
C., et al. Dota 2 with large scale deep reinforcement
learning. arXivpreprintarXiv:1912.06680,2019. Iqbal,S.andSha,F. Coordinatedexplorationviaintrinsic
rewards for multi-agent reinforcement learning. arXiv
Brown,N.andSandholm,T. Superhumanaiformultiplayer
preprintarXiv:1905.12127,2019.
poker. Science,365(6456):885–890,2019. doi: 10.1126/
science.aay2400. URL https://www.science. Jaques,N.,Lazaridou,A.,Hughes,E.,Gulcehre,C.,Ortega,
org/doi/abs/10.1126/science.aay2400. P., Strouse, D., Leibo, J.Z., andDeFreitas, N. Social
influence as intrinsic motivation for multi-agent deep
Burda, Y., Edwards, H., Storkey, A., andKlimov, O. Ex- reinforcementlearning. InInternationalconferenceon
plorationbyrandomnetworkdistillation. arXivpreprint machinelearning,pp.3040–3049.PMLR,2019.
arXiv:1810.12894,2018.
Kim,D.K.,Liu,M.,Riemer,M.D.,Sun,C.,Abdulhai,M.,
Carroll,M.,Shah,R.,Ho,M.K.,Griffiths,T.,Seshia,S., Habibi,G.,Lopez-Cot,S.,Tesauro,G.,andHow,J. A
Abbeel, P., and Dragan, A. On the utility of learning policygradientalgorithmforlearningtolearninmultia-
abouthumansforhuman-aicoordination. Advancesin gentreinforcementlearning. InInternationalConference
neuralinformationprocessingsystems,32,2019. onMachineLearning,pp.5541–5550.PMLR,2021.
Charakorn,R.,Manoonpong,P.,andDilokthanakul,N.Gen- Kostrikov, I. Pytorch implementations
eratingdiversecooperativeagentsbylearningincompat- of reinforcement learning algorithms.
iblepolicies. InInternationalConferenceonLearning https://github.com/ikostrikov/
Representations,2023. pytorch-a2c-ppo-acktr-gail,2018.
9FastPeerAdaptationwithContext-awareExploration
Kuhn,H.W. Asimplifiedtwo-personpoker. Contributions Raileanu,R.,Goldstein,M.,Szlam,A.,andFergus,R. Fast
totheTheoryofGames,1:97–103,1950. adaptationviapolicy-dynamicsvaluefunctions. InInter-
nationalconferenceonMachineLearning.PMLR,2020.
Laskin, M., Wang, L., Oh, J., Parisotto, E., Spencer, S.,
Steigerwald, R., Strouse, D., Hansen, S., Filos, A., Rakelly,K.,Zhou,A.,Finn,C.,Levine,S.,andQuillen,D.
Brooks,E.,etal. In-contextreinforcementlearningwith Efficientoff-policymeta-reinforcementlearningviaprob-
algorithmdistillation. arXivpreprintarXiv:2210.14215, abilisticcontextvariables. InInternationalconferenceon
2022. machinelearning,pp.5331–5340.PMLR,2019.
Liu,I.-J.,Jain,U.,Yeh,R.A.,andSchwing,A. Cooperative Ravula,M.C.R.Ad-hocteamworkwithbehavior-switching
explorationformulti-agentdeepreinforcementlearning. agents. InInternationalJointConferencesonArtificial
In International Conference on Machine Learning, pp. Intelligence,2019.
6826–6836.PMLR,2021.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Lowe,R.,Wu,Y.,Tamar,A.,Harb,J.,Abbeel,P.,andMor-
Klimov, O. Proximal policy optimization algorithms.
datch,I. Multi-agentactor-criticformixedcooperative-
arXivpreprintarXiv:1707.06347,2017.
competitiveenvironments. NeuralInformationProcess-
ingSystems(NIPS),2017. Sher,I.,Koenig,M.,andRustichini,A. Children’sstrategic
theoryofmind. ProceedingsoftheNationalAcademyof
Luo,F.-M.,Jiang,S.,Yu,Y.,Zhang,Z.,andZhang,Y.-F.
Sciences,111(37):13307–13312,2014.
Adapttoenvironmentsuddenchangesbylearningacon-
textsensitivepolicy. InProceedingsoftheAAAIConfer- Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou,
enceonArtificialIntelligence,volume36,pp.7637–7646, I.,Huang,A.,Guez,A.,Hubert,T.,Baker,L.,Lai,M.,
2022. Bolton, A., et al. Mastering the game of go without
humanknowledge. Nature,550(7676):354–359,2017.
Lupu, A., Cui, B., Hu, H., andFoerster, J. Trajectorydi-
versityforzero-shotcoordination. InInternationalcon-
Southey, F., Hoehn, B., and Holte, R. C. Effective short-
ference on Machine Learning, pp. 7204–7213. PMLR,
termopponentexploitationinsimplifiedpoker. Machine
2021.
Learning,74:159–189,2009.
Mahajan,A.,Rashid,T.,Samvelyan,M.,andWhiteson,S.
Stone,P.,Kaminka,G.,Kraus,S.,andRosenschein,J. Ad
Maven: Multi-agentvariationalexploration. Advancesin
hocautonomousagentteams: Collaborationwithoutpre-
neuralinformationprocessingsystems,32,2019.
coordination. InProceedingsoftheAAAIConferenceon
Papoudakis,G.,Christianos,F.,andAlbrecht,S.Agentmod- ArtificialIntelligence,volume24,pp.1504–1509,2010.
ellingunderpartialobservabilityfordeepreinforcement
Strouse,D.,McKee,K.,Botvinick,M.,Hughes,E.,andEv-
learning. Advances in Neural Information Processing
erett,R. Collaboratingwithhumanswithouthumandata.
Systems,34:19210–19222,2021.
AdvancesinNeuralInformationProcessingSystems,34:
Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. 14502–14515,2021.
Curiosity-driven exploration by self-supervised predic-
Wang,Y.,Zhong,F.,Xu,J.,andWang,Y. Tom2c: Target-
tion. InInternationalconferenceonmachinelearning,
oriented multi-agent communication and cooperation
pp.2778–2787.PMLR,2017.
with theory of mind. In International Conference on
Qiu,W.,Zhong,F.,Zhang,Y.,Qiao,S.,Xiao,Z.,Kim,T.S., LearningRepresentations,2022.
and Wang, Y. Unrealcv: Virtual worlds for computer
vision. In Proceedings of the 25th ACM International Xie, A., Losey, D., Tolsma, R., Finn, C., and Sadigh, D.
ConferenceonMultimedia,pp.1221–1224,2017. Learninglatentrepresentationstoinfluencemulti-agent
interaction. InConferenceonrobotlearning, pp.575–
Rahman,M.A.,Hopner,N.,Christianos,F.,andAlbrecht, 588.PMLR,2021.
S.V. Towardsopenadhocteamworkusinggraph-based
policylearning. InInternationalConferenceonMachine Yan,X.,Guo,J.,Lou,X.,Wang,J.,Zhang,H.,andDu,Y.
Learning,pp.8776–8786.PMLR,2021. Anefficientend-to-endtrainingapproachforzero-shot
human-aicoordination. InNeurIPS2023.2023.
Raileanu,R.,Denton,E.,Szlam,A.,andFergus,R. Mod-
elingothersusingoneselfinmulti-agentreinforcement Yu,X.,Jiang,J.,Zhang,W.,Jiang,H.,andLu,Z. Model-
learning. InInternationalconferenceonmachinelearn- basedopponentmodeling. AdvancesinNeuralInforma-
ing,pp.4257–4266.PMLR,2018. tionProcessingSystems,35:28208–28221,2022.
10FastPeerAdaptationwithContext-awareExploration
Zhang,T.,Xu,H.,Wang,X.,Wu,Y.,Keutzer,K.,Gonza-
lez, J.E., andTian, Y. Noveld: Asimpleyeteffective
exploration criterion. Advances in Neural Information
ProcessingSystems,34:25217–25230,2021.
Zhang,Z.,Yuan,L.,Li,L.,Xue,K.,Jia,C.,Guan,C.,Qian,
C.,andYu,Y.Fastteammateadaptationinthepresenceof
suddenpolicychange. arXivpreprintarXiv:2305.05911,
2023.
Zheng, L., Chen, J., Wang, J., He, J., Hu, Y., Chen, Y.,
Fan, C., Gao, Y., and Zhang, C. Episodic multi-agent
reinforcementlearningwithcuriosity-drivenexploration.
AdvancesinNeuralInformationProcessingSystems,34:
3757–3769,2021.
Zhong, F., Sun, P., Luo, W., Yan, T., and Wang, Y. Ad-
vat+:Anasymmetricduelingmechanismforlearningand
understandingvisualactivetracking. IEEEtransactions
onpatternanalysisandmachineintelligence,43(5):1467–
1482,2019.
Zhong, F., Sun, P., Luo, W., Yan, T., and Wang, Y. To-
wardsdistraction-robustactivevisualtracking. InInter-
nationalConferenceonMachineLearning,pp.12782–
12792.PMLR,2021.
Zhu,H.,Neubig,G.,andBisk,Y.Few-shotlanguagecoordi-
nationbymodelingtheoryofmind. InInternationalCon-
ferenceonMachineLearning,pp.12901–12911.PMLR,
2021.
Zintgraf,L.,Devlin,S.,Ciosek,K.,Whiteson,S.,andHof-
mann,K. Deepinteractivebayesianreinforcementlearn-
ingviameta-learning. arXivpreprintarXiv:2101.03864,
2021.
Zuo,Y.,Qiu,W.,Xie,L.,Zhong,F.,Wang,Y.,andYuille,
A. L. Craves: Controlling robotic arm with a vision-
basedeconomicsystem.InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,
pp.4214–4223,2019.
11FastPeerAdaptationwithContext-awareExploration
Table3.TheaverageepisodicperformanceofPACEandbaselines.TheOraclebestresponseperformanceisshownforreference.
KuhnPoker PO-Overcooked Predator-Prey-W
Methods
(Reward) (Successrate) (Reward)
Oracle 0.077 1.0 -2.82
PACE 0.047±0.004 0.553±0.029 -3.93±0.04
Generalist 0.012±0.016 0.111±0.016 -4.73±0.03
LIAM 0.024±0.001 0.054±0.027 -4.62±0.02
LIAMX 0.027±0.008 0.099±0.009 -4.34±0.08
LILI 0.025±0.001 0.090±0.007 -4.71±0.09
GSCU -0.041±0.021 0.100±0.005 -4.72±0.05
PACE PACE
0.6 PACE-reward 2.5 PACE-reward
PACE-reward-aux PACE-reward-aux
0.5 2.0
0.4
1.5
0.3
1.0
0.2
0.5
0.1
0.0 0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0
Training Steps 1e7 Training Steps 1e7
Figure6.TrainingcurvesofPACEandablationsonPO-Overcooked.PACEhasahighersuccessrate(a)andexploresthelowerroom
aboutonceperadaptationprocedure(b),whileablationsfailtoexploreandcan’tcooperateproperly.
A.EnvironmentDetails
A.1.KuhnPoker
KuhnPokerisasimplifiedtwo-player(P1&P2)pokergame(Kuhn,1950). Thegameinvolvesadeckofthreeplaying
cards,whereeachplayerisdealtonecard. Thecardsareranked(fromlowesttohighest)Jack,Queen,King. Thereareno
suitsinKuhnpoker,onlyranks. Andtheactionisrestrictedtobetandpass,differentfromNoLimitTexasHold’em,which
supportsmulti-roundraising. Thegameproceedsasfollows:
1. Bothplayersputoneante(chip)intothepot.
2. Eachplayerisdealtwithonecardfromthedeck. Theremainingcardisunseenbybothplayers.
3. Afterthedeal,P1isthefirsttotakeaction,choosingtobet1chiporpass.
• IfP1choosestobet,thenP2canbet(callP1’sbetandgameendsinashowdown)orpass(foldandforfeitthepot).
• IfP1choosestopass,thenP2canpass(checkandgameendsinashowdown)orbet.
• IfP2betsafterP1passes,P1shouldchoosetobet(callP2’sbetandgameendsinashowdown)orpass(foldand
forfeitthepot).
Inthispaper,wefocusonlearningtheadaptationstrategyofP1againstP2,andthepeerplaysasP2. Followingthestrategy
simplificationapproachintroducedbySoutheyetal.(Southeyetal.,2009),weeliminateobviouslydominatedpoliciesfor
P2. Forexample,P2neverbetswithQueenafterP1checks,becauseP1willalwaysfoldwithJackandalwayscallwith
King. Thewholesimplifiedgametreecanbefoundinthepaper(Southeyetal.,2009). Thissimplificationallowsusto
parameterizetheP2policyusingtwoparameters(ξ,η)withintherangeof0to1. ηistheprobabilityofbettingQueenafter
P1bets. ξistheprobabilityofbettingJackafterP1passes. Consequently,theentirepolicyspaceofP2canbedividedinto
sixsections,eachcorrespondingtothebestresponsefromP1.
12
etaR
sseccuS
mooR
rewoL
ot
stisiV
#FastPeerAdaptationwithContext-awareExploration
ObservationSpace. Theagentsinthegameobserveastaterepresentedbya13-dimensionalvector,consistingof3one-hot
vectors. Thefirstone-hotvectorisa7-dimensionalrepresentationofthecurrentstageinthegametree. Thesecondand
thirdone-hotvectors,eachof3dimensions,representthehandcardoftheegoplayerandtheopponent. Ifithasnotcometo
ashowdownstage,theopponenthandisalwaysrepresentedbyaall-zerovector.
ActionSpace. Asstatedabove,eachplayercanonlychoosetobetorpass,sotheactionspaceisadiscretespacewith2
actions.
Reward. Therewardisnotdirectlydeterminedbythepotitself. Instead,itiscalculatedbasedonthechipspresentinthe
potminusthechipscontributedbythewinner. Fortheloser,therewardisthenegativevalueofthewinner’sreward. Ifthe
gameendsinashowdown,theplayerholdingthehighest-rankcardwinsthepot. Ifnoplayerbets,thenthepotis2,sothe
rewardis±1. Otherwise,thepotis4(oneplayerbetsandtheotheronebetsthereafter),sotherewardis±2. Ifthegame
endsduetooneplayerforfeitingthepot,thentheotherplayerwinsthepotof3chips,sotherewardis±1.
A.2.PO-Overcooked
PO-Overcookedisacollaborativecookinggamewhereplayerstakeontherolesofchefsworkingtogethertocomplete
varioussub-tasksandservedishes(Carrolletal.,2019). Inthispaper,weintroduceamorecomplexMulti-Recipeversion,
whichbuildsuponthemodificationsbyCharakornetal.(Charakornetal.,2023). Specifically,weaddtwoextraingredients,
potatoandbroccoli,andcorrespondinglymorerecipestoincreasethechallengeandencouragediversepolicybehaviors.
Thegamescenarioinvolvesatotalof6ingredients(Tomato,Onion,Carrot,Lettuce,Potato,andBroccoli)and9recipes.
Notably,thegameenvironmentfeaturesacounterthatdividestheroom,necessitatingcollaborationbetweenchefsasthey
passobjectssuchasingredientsandplatesbackandforthoverthecounter. Toserveadish,thenecessaryingredientsshould
befirsttakentothecutboardandchopped. Afteralltherequiredingredientsarechoppedandputintoaplate,thedishneed
tobecarriedtothedeliversquaretofinishthetask. Furthermore,weaddpartialobservabilitytothegame,separatingthe
gamescenehorizontallyintoanupperroomandalowerroom. Eachagentcanonlyseeobjectsinthesameroomasitself.
Observation Space. The observation is a 105-dimensional vector. It consists of multiple features, including position,
direction,holdingobjects,frontobjectsandsoon. Thereisaflagforeachrelevantobjectthatindicatesitsvisibilityto
accountforpartialobservation.
ActionSpace. Eachagentcanchoosefromadiscretespacewith6actions: moveleft/right/up/down,interact(withobjects)
andno-op(takenoaction).
Reward. PO-Overcookedisafullycooperativegame,soallagentssharethereward. Therearethreetypesofrewards
inthegame. Thefirstisinteractreward. Eachagentreceivearewardof0.5ifanobjectisinteractedbyanagent. Note
thatrepeatedinteractionwithasameobjectdonotaccumulateadditionalrewards. Thesecondisprogressreward. Each
agentreceivesarewardof1.0whenthestateofarecipeprogresses. Forexample,ifachoppedcarrotisplacedintoaplate,
transitioningtherecipestatefrom"choppedcarrot"to"carrotplate,"eachagentisrewarded. Thethirdiscompletereward.
Whenadishsatisfyingarecipeisservedtothedeliversquare,eachagentreceivesarewardof10.0.
A.3.Predator-Prey-W
Hereweintroduceanenvironmentwithmultiplepeers,wheresomepeerscooperatewiththeegoagentandotherscompete
withit. Weuseamodifiedversionofthepredator-preyscenariofromtheMulti-agentParticleEnvironment(MPE)(Lowe
etal.,2017)commonlyusedintheMARLliterature. AsillustratedinFig.3c,theenvironmentfeaturestwopredators(red
circles,thedarkeroneofwhichistheegoagent),twopreys(greencircles),andmultiplelandmarks(greyandbluecircles).
Thepredatorsaretaskedwithchasingthepreywhilethepreyescapefromthepredators. Furthermore,thepredatorsare
requiredtocollaboratesuchthatallofthepreyarecoveredbypredators(seetheRewardsectionbelow). Eachepisodelasts
foratmost40steps. Ifallthepreyshavebeentouchedbypredators,theepisodeterminatesimmediately.
Tomakethetaskharder,weadditionallyintroducepartialobservabilityandfourwatchtowers(bluecircles,cornersofthe
figure). Theegoagentcanonlyobserveagentsandlandmarkswithinitsobservationradius,whichissetto0.2throughout
theexperiments. Theegoagentmaychoosetonavigatetothewatchtowerforfullobservability. Duringitscontactwithany
ofthewatchtowers,theegoagentcanobservealltheagentsandlandmarksintheenvironment.
Observationspace. Theobservationspaceisa37-dimensionalvector,consistingofthepositionsandvelocitiesofthe
agentsandthepositionsofthelandmarks. Anadditional0/1signisaddedforeverylandmarkandeveryagentotherthan
13FastPeerAdaptationwithContext-awareExploration
theegoagenttoindicateiftheentityiscurrentlyvisiblebytheegoagent. Invisibleentitieshavethesign,positions,and
velocitiessetto0. Allpositions,excludingthatoftheegoagent,arerelativetotheegoagent.
Actionspace. WeusethediscreteactionspaceofMPE,with5actionscorrespondingtomovingleft/right/up/downand
standingstill.
Reward. Thepredatorsshareacommonrewardthatencouragesthemtocollaborateandcoverallthepreys. Specifically,
denoteAasthesetofallpredatorsandBasthesetofallpreys,therewardforpredatorsateachtimestepisgivenas
(cid:88)
−c mind(a,b)
a∈A
b∈B
wheredistheEuclideandistancefunction,c=0.1. Intuitively,thisrewardallowsthepredatorstodivideandconquersuch
thatforeverypreythereisapredatornearby.
B.PeerPoolGeneration
InPACEpipeline,weneedtofirstcollectadiversepeerpoolΨ,whichcontainsrepresentativebehaviorsoftherealpeer
distribution. Currentmethods(Strouseetal.,2021;Charakornetal.,2023;Lupuetal.,2021)mainlyuseRLalgorithms
withdiversityobjectivestotrainpoliciesthatexhibitvariousbehaviors. Inthispaper,however,wegenerateacollectionof
rule-basedpolicies. ItisbecausetheP2policyinKuhnPokercanbeparameterizedbytwoprobabilities(ξ,η). Andwe
believethepreference-basedpoliciesinPO-OvercookedandPredator-Prey-Wcapturemorehuman-likebehaviorswithin
thegame. Thedetailsoftherule-basedpolicypoolarelistedbelow.
B.1.KuhnPoker
Aswementionedabove,weeliminatethedominatedstrategiesforP2. Therefore,P2policycanbedeterminedbytwo
factors: ηandξ. ηistheprobabilityofbettingwithQueenafterP1bets. ξistheprobabilityofbettingwithJackafterP1
passes.
Inthisway,wecaneasilygenerateasmanyP2policiesaswewantbyrandomlysamplingξandη. Inthispaper,wesample
40P2policiesfortrainingand10P2policiesfortesting.
B.2.PO-Overcooked
Inthispaper,wegeneratepreference-basedpeeragentsthatpossessindividualpreferencesforspecificrecipes. Forinstance,
eachpeeragentmayhaveapreferenceforarecipesuchasTomato&OnionSalad. Thesepeeragentsareconsistently
positionedontherightsideofthekitchenandinteractexclusivelywithingredientsanddishesthatalignwiththeirpreferred
recipe. For instance, a Tomato & Onion Salad peer agent focuses on sub-tasks related to handling Tomato and Onion
ingredients(choppedorfresh)ordeliveringdishesthatexclusivelycontainthesetwoingredients.
Ateachtime-step,theagentevaluateswhetheritscurrentsub-taskiscompletedornot. Ifthesub-taskremainsunfinished,
theagentdeterminestheshortestpathtothetargetpositionandnavigatesaccordingly. Ontheotherhand,ifthesub-taskis
completed,theagentsamplesanewsub-taskfromitspreferredsetofsub-tasks.
Inaddition,therearetwoparametersthatcontrolmorefine-grainedstrategies. P istheprobabilityofmovingright/left
nav
insteadofup/downwhentherearemultipleshortestpaths. P istheprobabilityofchoosingarandomactioninsteadofthe
act
optimalactionforthecurrentsub-task. Forexample,supposethepeeristryingtoputtheTomatoontothecounter. With
probabilityP ,itrandomlychoosesanactionfromtheactionspace. Withprobability1−P ,itchoosestheoptimal
act act
action(navigateorinteract).
Webelievesuchrule-basedagentsexhibitbehaviorsthataremorehuman-likethanself-playagentstrainedbyRLalgorithms.
First,cognitionstudies(Etel&Slaughter,2019;Sheretal.,2014)suggestthathumansindeedactbasedonintentionsand
desires. Furthermore,self-playagentsoftenhavearbitraryconventions(Huetal.,2020). Inovercooked,suchconventions
maybeputting/takingingredientsandplatesatacertaincounter,andrefusestointeractwithobjectsatdifferentlocations.
However,theseself-playconventionsrarelyappearinhumanbehaviors. Asaresult,preference-basedpolicyisabetter
choice.
Theovercookedscenariointhispaperconsistsof9recipes. TherearealsotwoparametersP andP thatcontrolmore
nav act
14FastPeerAdaptationwithContext-awareExploration
Algorithm2OnlineAdaptationProcedure
Require: Onlinepeerψ,adaptationhorizonN ,parametersθ
ctx
output Averageepisodicreturn
1: C1 ←∅,R←0,t←0
2: Resettheenvironmentandgeto1
0
3: whileN episodesnotreacheddo
eps
4: Steptheenvironmentwitha1
t
∼π θ(a|o1 t,χ θ(C1))anda−
t
1 ∼ψ,obtaintaskrewardr tandnextobservationo1
t+1
5: UpdateC1with(o1,a1)
t t
6: UpdateR←R+r t,t←t+1
7: ifTimetoswitchpeeragentsthen
8: Resampleψ
9: endif
10: ifCriterionforclearingthecontextismetthen
11: C1 ←∅
12: endif
13: endwhile
14: Return R
Neps
fine-grainedstrategies. Whengeneratinganewpeerpolicy,wefirstuniformlysampleitspreferredrecipefromthe9recipes,
andthenrandomlysampleP andP . Thetrainingpeerpoolcontains18policiesandthetestingpeerpoolcontains9
nav act
policies.
B.3.Predator-Prey-W
Forthepredatorpeer,wedesignpoliciesthathaveapreferencetowardsaspecificprey. Thepredatorpeerwillalwayschase
thepreferredpreyunderfullobservation.
Forthepreypeers,weconstruct8differentpatterns(Fig. 3c,dottedlines,⃝1-⃝8),whereeachpreypeermovesbackand
forthalongapreferredpath. Wedividethesetofpathsintoatrainset(bluedottedlines,⃝1-⃝4)andatestset(reddotted
lines,⃝5-⃝8). Thefinaltrainpeerpoolisgeneratedbysamplingdifferentcombinationsof1predatorpeerand2trainprey
peers,whilethetestpeerpoolsamplescombinationsof1predatorpeerand2testpreypeers. Asaresult,duringonline
adaptation, the policies of all prey peers are unseen to the ego agent. We sample 16 combinations for training and 24
combinationsfortesting.
C.AlgorithmDetails
C.1.OnlineAdaptationDetails
WepresentthepesudocodefortheonlineadaptationprocedureinAlgorithm2. Theonlineadaptationprocedurecomputes
the total returnof thetask rewardwithout theexploration reward (Line6). Inthe sudden-changepeerexperiment, the
onlinepeermaychange(Line8)andthecontextmaybecleared(Line11)duringadaptation,butthetwoeventstakeplace
separately,andtheagentisunawareofthepeerchangeexceptthroughtheproceduredescribedinAppendixC.3.
C.2.TrainingDetails
ThegeneraltrainingpipelineofLILI,LIAM,LIAMX,andGeneralistissimilartoPACEinAlgorithm1. Thedifference
betweenthesemethodsandPACEisthattheyhavesomedifferentauxiliarytasksanddonothavetheexplorationreward
usedinPACE.ThetrainingprocedureofGSCUisquitedifferentfromothermethods,whichcanbefoundintheoriginal
paper.
Forallbaselinesandablations,weusePPO(Schulmanetal.,2017;Kostrikov,2018)astheRLtrainingalgorithm. Table
4, 5, and 6 list the hyperparameters related to architectures and PPO training for Kuhn Poker, PO-Overcooked, and
Predator-Prey-W,respectively.
WekeeptheoriginalhyperparametersforGSCUonKuhnPoker. ForKuhnPoker,thetrainingbudgetforallalgorithms
15FastPeerAdaptationwithContext-awareExploration
Table4. HyperparametersforallthealgorithmsintheKuhnPokerenvironment.
Algorithms
ParameterName
PACE Generalist LILI LIAM(X) GSCU
LearningRate 2e-4 2e-4 2e-4 2e-4 5e-4
PPOClipϵ 0.2 0.2 0.2 0.2 0.2
EntropyCoefficient 5e-4 5e-4 5e-4 5e-4 0.01
γ 0.99 0.99 0.99 0.99 0.99
GAEλ 0.95 0.95 0.95 0.95 0.95
BatchSize 80000 80000 80000 80000 1000
#UpdateEpochs 15 15 15 15 5
#MiniBatches 12 12 12 12 30
GradientClipping(L2) 2.0 2.0 2.0 2.0 0.5
ActivationFunction ReLU ReLU ReLU ReLU ReLU
Actor/CriticHiddenDims [128,128] [128,128] [128,128] [128,128] [128,128]
f HiddenDims [64,64] N/A N/A N/A N/A
θ
g HiddenDims [64] N/A N/A N/A N/A
θ
Table5. HyperparametersforallthealgorithmsinthePO-Overcookedenvironment.
Algorithms
ParameterName
PACE Generalist LILI LIAM(X) GSCU
LearningRate 1e-3 1e-3 1e-3 1e-3 7e-4
PPOClipϵ 0.2 0.2 0.2 0.2 0.2
EntropyCoefficient 0.03 0.03 0.03 0.03 0.01
γ 0.99 0.99 0.99 0.99 0.99
GAEλ 0.95 0.95 0.95 0.95 0.95
BatchSize 72000 72000 72000 72000 2500
#UpdateEpochs 4 4 4 4 8
#MiniBatches 18 18 18 18 2
GradientClipping(L2) 15.0 15.0 15.0 15.0 0.5
ActivationFunction ReLU ReLU ReLU ReLU Tanh
Actor/CriticHiddenDims [128,128] [128,128] [128,128] [128,128] [6464]
f HiddenDims [128,128] N/A N/A N/A N/A
θ
g HiddenDims [128] N/A N/A N/A N/A
θ
exceptGSCUis5millionsteps,whileforGSCUtheembeddinglearningtakes1millionepisodesandconditionalRLtakes
1millionepisodes. ForPO-Overcooked,thetrainingbudgetforallalgorithmsexceptGSCUis30millionsteps,whilefor
GSCUtheembeddinglearningtakes2millionstepsandtheconditionalRLtakes30millionsteps. ForPredator-Prey-W,
thetrainingbudgetforallalgorithmsincludingGSCUis15millionsteps. TheembeddinglearningforGSCUtakesan
additional2millionsteps. ThetrainingofPACEtakes∼12hourswith∼80processesonasingleTitanXpGPU.Boththe
PACEactorπ (a|o,χ (C))andcritictakeconcatenatedobservationandencoderoutputastheinput.
θ θ
ForalgorithmsusingRNN,includingGeneralist,LIAM,andLIAMX,theRNNisimplementedasasingle-layerGRUwith
128hiddenunits. TheRNNistrainedusingback-propagationthroughtime(BPTT)withgradientsdetachedevery20steps.
ActorandcriticsharethesameRNN,aswellasthehiddenlayersbeforetheRNN.
Formethodswithauxiliarytasks,thereisanadditionallossaccompanyingthemainRLloss,computedusingthesame
mini-batchasusedinRLtraining. ForPACE,theauxiliarylossisusedwithaweight1.0. ForLIAM,theauxiliaryloss
isusedwithweight1.0forbothactionandobservationprediction. ForLILI,thecontextisthelastepisode,asspecified
in(Xieetal.,2021). Theauxiliarylossisusedwithweight1.0forbothrewardandnextobservationprediction.
ThecoefficientforPACE’sexplorationrewarddecaysfromc to0inM steps. c =0.2forPO-Overcooked,0.01for
init init
16FastPeerAdaptationwithContext-awareExploration
Table6. HyperparametersforallthealgorithmsinthePredator-Prey-Wenvironment.
Algorithms
ParameterName
PACE Generalist LILI LIAM(X) GSCU
LearningRate 1e-3 1e-3 1e-3 1e-3 5e-4
PPOClipϵ 0.2 0.2 0.2 0.2 0.2
EntropyCoefficient 0.03 0.03 0.03 0.03 0.01
γ 0.99 0.99 0.99 0.99 0.99
GAEλ 0.95 0.95 0.95 0.95 0.95
BatchSize 64000 64000 64000 64000 2500
#UpdateEpochs 4 4 4 4 8
#MiniBatches 16 16 16 16 2
GradientClipping(L2) 15.0 15.0 15.0 15.0 0.5
ActivationFunction ReLU ReLU ReLU ReLU Tanh
Actor/CriticHiddenDims [128,128] [128,128] [128,128] [128,128] [6464]
f HiddenDims [128,128] N/A N/A N/A N/A
θ
g HiddenDims [128] N/A N/A N/A N/A
θ
1.0
PACE
Generalist
0.8 LIAMX
Oracle
0.6
0.4
0.2
0.0
1 2 3 4 5 6 7 8 9 10
Evaluation Episodes
Figure7.ResultsofpeerswithsuddenchangesinPO-Overcooked. Afterthepeerchangeinepisode6,PACEdetectsthechangeand
clearsthecontext,allowingperformancetorecover,whilebaselinesfailconsistently.
KuhnPoker,and0.1forPredator-Prey-W,whileM =2.5∗107forPO-Overcooked,4∗106forKuhnPoker,and1.5∗107
forPredator-Prey-W.Wechoosethesehyperparameterssuchthattheexplorationrewardhasaninitialscalesimilartothat
ofthetaskreward,anddecaysto0neartheendofthetraining. Additionally,wewarmupthecontextencoderforM
w
stepsusingtheauxiliarylossonlywithoutRLloss. M =106forPO-Overcooked,105forKuhnPoker,and5∗105for
w
Predator-Prey-W.
C.3.AdaptingtotheSudden-changePeer
Wepresentmoredetailsofthesudden-changepeerexperimentinsection4.5. WeruntheexperimentinPO-Overcooked
for10episodes. Theegoagentneedstocollaboratewithtwodifferentpeersinthefirst5episodesandthelast5episodes
withoutknowingwhenthepeerchangetakesplace. WhilethePACEagentsaretrainedagainststaticpeersthatneverchange,
weusedropsintheevaluationmetricasanindicatorofpeerchangesandclearthecontextwhensuchchangestakeplace.
Formally,denotetheevaluationmetricforepisodeiasR ,thenwedetectapeerchangeatiiff
i
17
etaR
sseccuSFastPeerAdaptationwithContext-awareExploration
3.0
0.99 0.99
0.6 0.9 0.9
2.5
0.999 0.999
0.5
2.0
0.4
1.5
0.3
1.0
0.2
0.1 0.5
0.0 0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0
Training Steps 1e7 Training Steps 1e7
(a)SuccessRate (b)NumberofVisitstotheLowerRoom
Figure8.Trainingsuccessrates(a)andnumberofvisitstothelowerroom(b)forthediscountfactorablationsinPO-Overcooked.Our
discountfactorofchoice,γ =0.99,performsbest.γ =0.9isshort-sightedanddoesnotexhibitexplorationbehaviors,whileγ =0.999
introducestraininginstabilities.
PACE 2.75 PACE
0.6 PACE-tf 2.50 PACE-tf
0.5 2.25
0.4 2.00
1.75
0.3
1.50
0.2
1.25
0.1 1.00
0.0 0.75
0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0
Training Steps 1e7 Training Steps 1e7
(a)SuccessRate (b)PeerIdentificationLoss
Figure9.Trainingsuccessrates(a)andpeeridentificationloss(b)fortheTransformerencoderablationsinPO-Overcooked.PACE-tfis
R <c ∗maxR +(1−c )∗minR
i th j th j
j≤i j≤i
wherec ∈[0,1]isathresholdcoefficient. c =1isthemostaggressive(clearscontextwheneverperformancedrops)
th th
andc =0isthemostconservative(neverclearsthecontext). Wesetc h=0.8inthisexperiment.
th t
ThesuccessratecurveduringtheadaptationprocedureispresentedinFig. 7. PACEsuccessfullyrecoversafterthechange
ofpeeragentinepisode6,whilebaselinesconsistentlyunderperform.
D.AdditionalAblations
D.1.ImpactofDiscountFactor
We conduct an ablation in PO-Overcooked for the impact of the discount factor. The discount factor γ defines the RL
objective3,whereasmallγ putsmoreemphasisonshort-termreturnsandalargeγ approximatestheundiscountedreturn
betterbuthashighervariance. Fig. 8aand8bpresentthetrainingsuccessratesandnumberofvisitstothelowerroom. The
numberofvisitstothelowerroomreflectstheexploratorytendenciesoftheagentsincethisactiononlyrevealsinformation
(behaviorsofthepeeragentinthelowerroom)totheagentinsteadofgeneratingimminentenvironmentrewards. Asseen
18
etaR
sseccuS
etaR
sseccuS
mooR
rewoL
ot
stisiV
#
ssoL
DI
reePFastPeerAdaptationwithContext-awareExploration
inFig. 8,γ = 0.9leadstoashort-sightedpolicythatquicklyconvergestoalocalminimum,eveninthepresenceofan
explorationreward. Alargerγ =0.999retainstheexplorationbehaviorbutleadstounstabletraining.
D.2.ImpactofTransformerEncoder
Wedesignedabi-leveltransformerencodertocomparewiththeMLPencoderweusedinourprimaryexperiments. We
constructtwotransformerencodersforencodingepisode-levelinformationandcontext-levelinformation, respectively.
Formally,theencodingforepisodenis
z1 :=Ence((o1 ,a1 ),(o1 ,a1 ),...,(o1 ,a1 ))
n n,1 n,1 n,2 n,2 n,Tn n,Tn
whiletheencodingforthewholecontextC is
z1 :=Encc(z1,z1,...,z1 )
1 2 N
where Ence,Encc are transformer encoders for the episode- and context-level, followed by average pooling over the
sequencedimension. WeapplylearnablepositionalembeddingandstandardregularizationtechniqueslikeDropoutto
theencoder. Weusehiddendimensionandfeed-forwarddimensionof128, Dropoutcoefficientof0.5, 4headsforthe
multi-headattention,andasinglelayeroftransformerencoderineachofEnceandEncc.
AsshowninFig. 9,PACE-tfisgreatlyoutperformedbythestandardPACEMLPencoder. Thereasonforthisunderperfor-
mance,wehypothesize,isthatthetransformerencodermayoverfitthecurrentRLtrainingbatchonthepeeridentification
task. InFig. 9b,itcanbeseenthatthepeerIDlossofPACE-tfdropsfasterthanPACEinitially,butslowlyincreasesafter
that,untilanabruptchangeinthemiddleofthetraining. Incontrast,thelossforPACEdropsconsistentlyalongthetraining
procedure. Itisworthnotingthatourpeeridentificationtaskandexplorationrewardarenotlimitedtoaspecifickindof
encoderarchitecture,andthatwechooseMLPwithaveragepoolingbasedonempiricalperformance.
19