4D Gaussian Splatting:
Towards Efficient Novel View Synthesis for Dynamic Scenes
YuanxingDuan1* FangyinWei2* QiyuDai1,4 YuhangHe1 WenzhengChen3† BaoquanChen1,4†
1PekingUniversity 2PrincetonUniversity 3NVIDIA 4NationalKeyLabofGeneralAI,China
DynamicVideoInput TheProposedMethod HyperReel[2] EvaluationonPSNRvs.FPS
Figure1.Wepresent4DGaussianSplatting,anovelapproachthattakesvideosasinputandsynthesizesdynamicnovel-viewimageswith
finedetails.Theproposedmethodoutperformspriorartsinrenderingqualityandachievesupto277FPSinferencespeedonanRTX3090
GPUatPlenopticDataset[33].
Abstract 1.Introduction
We consider the problem of novel view synthesis (NVS) Reconstructing 3D scenes from 2D images and synthesiz-
fordynamicscenes.Recentneuralapproacheshaveaccom- ing their appearance from novel views has been a long-
plished exceptional NVS results for static 3D scenes, but standinggoalincomputervisionandgraphics. Thistaskis
extensions to 4D time-varying scenes remain non-trivial. pivotal in numerous industrial applications including film,
Prior efforts often encode dynamics by learning a canon- gaming, and VR/AR, where there is a substantial demand
icalspaceplusimplicitorexplicitdeformationfields,which for high-speed, photo-realistic rendering effects. The task
struggle in challenging scenarios like sudden movements divergesintotwodifferentscenetypes: staticsceneswhere
or capturing high-fidelity renderings. In this paper, we objects are still across all images [4, 24, 27, 37] and dy-
introduce 4D Gaussian Splatting (4DGS), a novel method namicsceneswherescenecontentsexhibittemporalvaria-
thatrepresentsdynamicsceneswithanisotropic4DXYZT tions [11, 33, 40, 43, 57]. While the former has witnessed
Gaussians, inspired by the success of 3D Gaussian Splat- significantprogressrecently,efficientandaccurateNVSfor
ting in static scenes [26]. We model dynamics at each dynamic scenes remains challenging due to the complexi-
timestamp by temporally slicing the 4D Gaussians, which tiesintroducedbythetemporaldimensionanddiversemo-
naturally compose dynamic 3D Gaussians and can be tionpatterns.
seamlessly projected into images. As an explicit spatial-
A variety of methods have been proposed to tackle the
temporalrepresentation, 4DGSdemonstratespowerfulca-
challenges posed by dynamic NVS. A series of methods
pabilities for modeling complicated dynamics and fine de-
modelthe3Dsceneanditsdynamicsjointly[14,21].How-
tails—especiallyforsceneswithabruptmotions.Wefurther
ever,thesemethodsoftenfallshortinpreservingfinedetails
implementourtemporalslicingandsplattingtechniquesin
intheNVSrenderingsduetothecomplexitycausedbythe
ahighlyoptimizedCUDAaccelerationframework,achiev-
highlyentangledspatialandtemporaldimensions. Alterna-
ing real-time inference rendering speeds of up to 277 FPS
tively,manyexistingtechniques[15,39,40,43,51]decou-
on an RTX 3090 GPU and 583 FPS on an RTX 4090
pledynamicscenesbylearningastaticcanonicalspaceand
GPU.Rigorousevaluationsonsceneswithdiversemotions
then predicting a deformation field to account for the tem-
showcasethesuperiorefficiencyandeffectivenessof4DGS,
poral variations. Nonetheless, this paradigm struggles in
whichconsistentlyoutperformsexistingmethodsbothquan-
capturing complex dynamics such as objects appearing or
titativelyandqualitatively.
disappearingsuddenly. Moreimportantly,prevailingmeth-
odsondynamicNVSmostlybuilduponvolumetricrender-
ing[37]whichrequiresdensesamplingonmillionsofrays.
*Equalcontribution
†Co-correspondingauthors As a consequence, these methods typically cannot support
1
4202
beF
5
]VC.sc[
1v70330.2042:viXrareal-timerenderingspeedevenforstaticscenes[33,47].
Recently, 3D Gaussian Splatting (3DGS) [26] has
emerged as a powerful tool for efficient NVS of static
scenes. By explicitly modeling the scene with 3D Gaus-
sianellipsoidsandemployingfastrasterizationtechnique,it
achievesphoto-realisticNVSinrealtime. Inspiredbythis,
weproposetoliftGaussiansfrom3Dto4Dandprovidea
novelspatial-temporalrepresentationthatenablesNVSfor
morechallengingdynamicscenes.
Our key observation is that 3D scene dynamics at each
timestamp can be viewed as 4D spatial-temporal Gaussian
ellipsoids sliced with different time queries. Fig. 2 illus-
trates a simplified XYT case: the dynamics in 2D XY
space at time T is equivalent to building 3D XYT Gaus-
i
sians and slicing by the t = T plane. Analogously, we
i
extend 3D Gaussians to 4D XYZT space to model dy- Figure2. ASimplified2DIllustrationoftheProposedTempo-
namic3Dscenes.Thetemporallysliced4DGaussianscom- ralSlicing. (a)wemodel2Ddynamicswith3DXYT ellipsoids
pose 3D Gaussians that can be seamlessly projected to 2D andslicethemwithdifferenttimequeries. (b)Thesliced3Del-
screensviafastrasterization,inheritingbothexquisiteren- lipsoidsform2Ddynamicellipsesateachtimestamp.
dering effects and high speed characteristic from 3DGS.
Moreover,extendingtheprune-splitmechanisminthetem- dented 583 FPS on an RTX 4090 GPU and 277 FPS on
poral dimension makes 4D Gaussians particularly suitable an RTX 3090 GPU. We conduct extensive experiments on
for representing complex dynamics, including abrupt ap- two datasets spanning a wide range of settings and mo-
pearancesordisappearances. tion patterns, including monocular videos [43] and multi-
cameravideos[33].Quantitativeandqualitativeevaluations
Itisnon-trivialtolift3DGaussiansinto4Dspace,where
demonstrate the distinct advantages over preceding meth-
tremendous challenges exist in the design of the 4D rota-
ods,includingthenewstate-of-the-artrenderingqualityand
tion,slicing,aswellasthejointspatial-temporaloptimiza-
speed.
tion scheme. We draw inspiration from geometric algebra
andcarefullychoose4Drotor[6]torepresent4Drotation,
2.RelatedWork
which is a spatial-temporal separable rotation representa-
tion. Notably, rotorrepresentationaccommodatesboth3D Inthissection,wemainlyreviewoptimization-basednovel
and 4D rotation: when the temporal dimension is set to view synthesis (NVS) methods, where the input is a set of
zero, it becomes equivalent to a quaternion and can rep- posedimagesandtheoutputisnewappearancefromnovel
resent3Dspatialrotationaswell. Suchadaptabilitygrants viewpoints. We first describe NVS for static scenes, then
ourmethodtheflexibilitytomodelbothdynamicandstatic talkaboutitsdynamicextensions. Lastly,wediscussrecent
scenes. In other words, 4DGS is a generalizable form of Gaussian-basedNVSmethods.
3DGS:whenclosingthetemporaldimension,our4DGSre- Static Novel View Synthesis Previous approaches for-
ducesto3DGS. malize light-field [30] or lumigraph [7, 22] that generate
Weenhancetheoptimizationstrategiesin3DGSandin- novel-view images by interpolating from existing views,
troduce two new regularization terms to stabilize and im- which require densely captured images in order to acquire
prove the dynamic reconstruction. We first propose an en- realistic renderings. Other classical methods exploit geo-
tropylossthatpushestheopacityofGaussianstowardsei- metric proxies such as mesh and volume to reproject and
theroneorzero,whichproveseffectivetoremove“floaters” blendcontentsfromsourceimagesontonovelviews.Mesh-
in our experiments. We further introduce a novel 4D con- basedmethods[13,44,49,53,55]representthesceneswith
sistency loss to regularize the motion of Gaussian points surfacesthatsupportefficientrenderingbutarehardtoop-
andyieldmoreconsistentdynamicsreconstruction. Exper- timize. Volume-basedmethodsusevoxelgrids[29,42,45]
imentsshowthatbothtermsnotablyimprovetherendering or multi-plane images (MPIs) [17, 36, 48, 63], which pro-
quality. videdelicaterenderingeffectsbutarememory-inefficientor
WhileexistingGaussian-basedmethods[35,56,60,61] limitedtosmallviewchanges. Recently,thetrendofNVS
are mostly based on PyTorch [41], we further develop a wasspearheadedbyNeuralRadianceFields(NeRFs)[37],
highlyoptimizedCUDAframeworkwithcarefulengineer- which achieves photo-realistic rendering quality. Since
ingdesignsforfasttrainingandinferencespeed.Ourframe- then,aseriesofeffortshaveemergedtoimprovethetrain-
worksupportsrendering1352×1014videosatanunprece- ing speed [10, 18, 38], enhance rendering quality [3, 52],
2or extend to more challenging scenarios such as reflection real-time rasterization in Sec. 3.2.2. Finally, we introduce
andrefraction[5,28,59]. Still,mostofthesemethodsrely ourdynamicoptimizationstrategiesinSec.3.3.
onvolumerenderingthatrequiressamplingmillionsofrays
3.1.Preliminaryof3DGaussianSplatting
andhindersreal-timerendering[3,10,37].
Dynamic Novel View Synthesis This poses new chal- 3DGaussianSplatting(3DGS)[26]hasdemonstratedreal-
lenges due to the temporal variations in the input images. time, state-of-the-art rendering quality on static scenes. It
Traditional methods estimate varying geometries such as models a scene with a dense cluster of N anisotropic 3D
surfaces [12, 31] and depth maps [25, 64] to account for Gaussianellipsoids. EachGaussianisrepresentedbyafull
dynamics. InspiredbyNeRFs, recentworktypicallymod- 3DcovariancematrixΣanditscenterpositionµ:
els dynamic scenes with neural representations. A branch
of methods implicitly model the dynamics by adding a
G(x)=e− 21(x−µ)TΣ−1(x−µ). (1)
temporal input or latent code [14, 21]. Another line of To ensure a valid positive semi-definite covariance matrix
works [15, 39, 40, 43, 51] explicitly model deformation duringoptimization,Σisdecomposedintothescalingma-
fields that map 3D points at arbitrary timestamp into a trixSandtherotationmatrixRtocharacterizethegeome-
canonical space. Other techniques explore decomposing tryofa3DGaussianellipsoid:
a scene into static and dynamic components [47], using
Σ=RSSTRT, (2)
keyframestoreduceredundancy[2,33],estimatingaflow
field [23, 34, 50], or exploiting 4D grid-based representa- where S = diag(s ,s ,s ) ∈ R3 and R ∈ SO(3) are
x y z
tions[8,15,19,20,32,46,54],etc. Thecommonissuesof stored as a 3D vector and quaternion, respectively. Be-
dynamicscenemodelingarethecomplexitiesintroducedby yondµ,SandR,eachGaussianmaintainsadditionallearn-
thespatial-temporalentanglement,aswellastheadditional able parameters including opacity o ∈ (0,1) and spher-
memoryandtimecostcausedbythetemporaldimension. ical harmonic (SH) coefficients in Rk representing view-
Gaussian-Based NVS The recent seminal work [1, 26, dependent colors (k is related to SH order). During op-
58, 62] models static scenes with Gaussians whose posi- timization, 3DGS adaptively controls the Gaussian distri-
tions and appearance are learned through a differentiable bution by splitting and cloning Gaussians in regions with
splatting-based renderer. Particularly, 3D Gaussian Splat- largeview-spacepositionalgradients,aswellastheculling
ting (3DGS) [26] achieves impressive real-time rendering ofGaussiansthatexhibitnear-transparency.
thanks to its Gaussian split/clone operations and the fast Efficientrenderingandparameteroptimizationin3DGS
splatting-basedrenderingtechnique. Ourworkdrawsinspi- are powered by the differentiable tile-based rasterizer.
rationfrom3DGSbutlifts3DGaussiansinto4Dspaceand Firstly, 3D Gaussians are projected to 2D space by com-
focusesondynamicscenes. Severalconcurrentworksalso putingthecameraspacecovariancematrixΣ′:
extend 3DGS to model dynamics. Deformable3DGS [35]
Σ′ =JVΣVTJT, (3)
directly learns the temporal motion and rotation of each
3D Gaussian along time, which makes it suitable for dy- whereJistheJacobianmatrixoftheaffineapproximation
namic tracking applications. Similarly, [56, 60] utilize oftheprojectiontransformation,andVistheextrinsiccam-
MLPs to predict the temporal movement. However, it is eramatrix. Then,thecolorofeachpixelontheimageplane
challenging for these methods to represent dynamic con- iscalculatedbyblendingGaussianssortedbytheirdepths:
tentsthatsuddenlyappearordisappear. Similartous, Re-
N i−1
alTime4DGS [61] also leverages 4D Gaussian representa- C =(cid:88) c α (cid:89) (1−α ), (4)
i i j
tiontomodel3Ddynamics. Theychooseadual-quaternion
i=1 j=1
based4Drotationformulationthatislessinterpretableand
wherec isthecolorofthei-th3DGaussianG ,α =o G′
lacks the spatial-temporal separable property compared to i i i i i
witho andG′ astheopacityand2DprojectionofG , re-
rotor-baserepresentation. Moreover,wefurtherinvestigate i i i
spectively. Pleasereferto3DGS[26]formoredetails.
better spatial-temporal optimization strategies and develop
a high-performance framework that achieves much higher 3.2.4DGaussianSplatting
renderingspeedandbetterrenderingquality.
We now discuss our 4D Gaussian Splatting (4DGS) algo-
rithm. Specifically, wemodelthe4DGaussianwithrotor-
3.Method
basedrotationrepresentation(Sec.3.2.1)andslicethetime
In this section, we first review the 3D Gaussian Splatting dimensiontogeneratedynamic3DGaussiansateachtimes-
(3DGS)method[26]inSec.3.1. Wethendescribeour4G tamp(Sec.3.2.2).The3DGaussianellipsoidsslicedateach
GaussianSplattingalgorithminSec.3.2,wherewepresent timestamp can be efficiently rasterized onto a 2D image
rotor-based 4D Gaussian representation in Sec. 3.2.1 and plane for real-time and high-fidelity rendering of dynamic
discuss the temporal slicing technique for differentiable scenes.
3Differentiable Operation Flow
Initialization Temporal Slicing
Rasterization Gradient Flow
t 𝒕 = 𝟔.𝟒𝟎 𝒔 𝒕 = 𝟔.𝟔𝟕 𝒔 𝒕 = 𝟔.𝟒𝟎 𝒔 𝒕 = 𝟔.𝟒𝟎 𝒔
Projection
z
𝒕 = 𝟔.𝟔𝟕 𝒔 𝒕 = 𝟔.𝟔𝟕 𝒔
y
Adaptive
x Density Control
4D Gaussians Initialization Rotor-Based 4D Gaussians 2D Rendering Ground Truth Images
Figure3. FrameworkOverview. Afterinitialization,wefirsttemporallyslicethe4DGaussianswhosespatio-temporalmovementsare
modeledwithrotors.Thedynamicssuchastheflickeringflamesnaturallyevolvethroughtime,evenwithinashortperiodoflessthan0.3
second. Thesliced3DGaussiansarethenprojectedonto2Dusingdifferentiablerasterization. Thegradientsfromimagedifferenceloss
areback-propagatedandguidetheadaptivedensitycontrolof4DGaussians.
3.2.1 Rotor-Based4DGaussianRepresentation Ourrotor-based4DGaussianoffersawell-defined,inter-
pretable rotation representation: the first four components
Analogous to 3D Gaussians, a 4D Gaussian can be ex-
encode the 3D spatial rotation, and the last four compo-
pressedwiththe4Dcenterpositionµ =(µ ,µ ,µ ,µ )
4D x y z t nents define spatio-temporal rotation, i.e., spatial transla-
andthe4DcovariancematrixΣ as
4D tion. Inparticular,settingthelastfourcomponentstozero
effectivelyreducesrtoaquaternionfor3Dspatialrotation,
G 4D(x)=e−1 2(x−µ4D)TΣ− 4D1(x−µ4D). (5)
thereby enabling our framework to model both static and
The covariance Σ can be further factorized into the 4D dynamicscenes. Fig.4presentsanexamplewhereourre-
4D
scalingS andthe4DrotationR as sultonastatic3Dscenematchesthatoforiginal3DGS[26].
4D 4D
Alternatively,concurrentwork[61]alsomodelsdynamic
Σ =R S ST RT . (6) scenes with 4D Gaussian. However, they represent 4D ro-
4D 4D 4D 4D 4D
tationwithtwoentangledisoclinicquaternions[9]. Asare-
While modeling S 4D = diag(s x,s y,s z,s t) is straightfor- sult,theirspatialandtemporalrotationsaretightlycoupled,
ward,seekingaproper4DrotationrepresentationforR 4D anditisunclearhowtoconstrainandnormalizethisalter-
isachallengingproblem. nativerotationrepresentationduringoptimizationtomodel
Ingeometricalgebra,rotationsofhigh-dimensionalvec- static3Dscenes.
tors can be described using rotors [6]. Motivated by this,
weintroduce4Drotorstocharacterizethe4Drotations. A
3.2.2 Temporally-Sliced4DGaussiansSplatting
4Drotorriscomposedof8componentsbasedonasetof
basis: Wenowdescribehowtoslice4DGaussiansinto3Dspace.
Given that Σ and its inverse Σ−1 are both symmetric
4D 4D
r=s+b e +b e +b e +b e matrices,wedefine
01 01 02 02 03 03 12 12
+b 13e 13+b 23e 23+pe 0123, (7) (cid:18) U V(cid:19) (cid:18) A M(cid:19)
Σ = andΣ−1 = , (9)
4D VT W 4D MT Z
where e = e ∧e ∧e ∧e , and e = e ∧e is
0123 0 1 2 3 ij i j
theouterproductbetween4Daxise (i ∈ {0,1,2,3})that where both U and A are 3 × 3 matrices. Then, given a
i
formtheorthonormalbasisin4DEuclideanspace. There- time t, the projected 3D Gaussian is obtained as (detailed
fore, a 4D rotation can be determined by 8 coefficients derivationintheSupplementaryMaterial):
(s,b ,b ,b ,b ,b ,b ,p).
A0 n1 alo0 g2 ous03 to1 q2 ua1 te3 rni2 o3
n, a rotor r can also be con-
G 3D(x,t)=e− 21λ(t−µt)2 e−1 2[x−µ(t)]TΣ− 3D1[x−µ(t)], (10)
verted into a 4D rotation matrix R with proper normal-
4D where
izationfunctionF androtor-matrixmappingfunction
norm
F . Wecarefullyderiveanumericallystablenormaliza-
λ=W−1,
map
tionmethodforrotortransformationandprovidedetailsof VVT
thetwofunctionsinSupplementaryMaterial: Σ 3D =A−1 =U− W , (11)
V
R
4D
=F map(F norm(r)). (8) µ(t)=(µ x,µ y,µ z)T +(t−µ t) W.
4beclosetotheobjectsurface,andinmostcasesitsopacity
shouldbeclosetoone. Therefore,weencouragetheopac-
itytobeclosetooneorclosetozerobyaddinganentropy
loss, and by default Gaussians with near-zero opacity will
beprunedduringtraining:
(a) 3DGS (PSNR 27.01) (b) Ours (PSNR 27.07)
N
1 (cid:88)
L = −o log(o ). (12)
Figure4. Modeling3DStaticScenes.Ourrotor-basedrepresen- entropy N i i
tationenablesboth3Dstaticand4Ddynamicscenemodeling.Our i=1
frameworkmatchestheresultsof3DGS[26]onstatic3Dscenes. WefindthatL helpscondenseGaussianpointsand
entropy
filternoisyfloaters,whichisveryusefulwhentrainingwith
sparseviews.
Compared with Eq. 1 in the original 3DGS [26], the
sliced 3D Gaussian in Eq. 10 contains a temporal decay
3.3.2 4DConsistencyLoss
term e−1 2λ(t−µt)2. As t goes by, a Gaussian point first ap-
pearsoncetissufficientlyclosetoitstemporalpositionµ t Intuitively,nearbyGaussiansin4Dspaceshouldhavesim-
andstartstogrow. Itreachesthepeakopacitywhent=µ t. ilarmotions. Wefurtherregularizethe4DGaussianpoints
Afterthat,the3DGaussiangraduallyshrinksindensityun- byaddingthe4Dspatial-temporalconsistencyloss. Recall
til vanishing when t is sufficiently far from µ t. By con- whenslicinga4DGaussianatagiventimet,aspeedterm
trollingthetemporalpositionandscalingfactor, 4DGaus- sisderived. Thus,giventhei-thGaussianpoint,wegather
siancanrepresentchallengingdynamics,e.g.,motionsthat K nearest4DpointsfromitsneighbourspaceΩ andregu-
i
suddenly appear or disappear. During rendering, we filter larizetheirmotionstobeconsistent:
pointsthataretoofarfromcurrenttime, wherethethresh-
oldforvisibility 21λ(t−µ t)2isempiricallysetto16.
N
(cid:12) (cid:12)(cid:12)
(cid:12)
(cid:12) (cid:12)(cid:12)
(cid:12)
Moreover, the sliced 3D Gaussian exhibits a new mo- 1 (cid:88)(cid:12)(cid:12) 1 (cid:88) (cid:12)(cid:12)
tion term of (t − µ t)V/W added to the center position L consistent4D = N (cid:12) (cid:12)(cid:12) (cid:12)s i− K s j(cid:12) (cid:12)(cid:12) (cid:12) . (13)
(µ ,µ ,µ )T. In theory, linear movement of a 3D Gaus- i=1(cid:12)(cid:12) j∈Ωi (cid:12)(cid:12) 1
x y z
sianemergesfromour4Dslicingoperation. Weassumein For 4D Gaussians, 4D distance is a better metric for point
a small time interval, all motions can be approximated by similarity than 3D distance, because points that are neigh-
linear motions, and more complex non-linear cases can be borsin3Ddonotnecessarilyfollowthesamemotions,e.g.,
representedbycombinationofmultipleGaussians. Further, when they belong to two objects with different motions.
V/Windicatesthemotionspeedinthecurrenttimestamp. Notethatcalculating4Dnearestneighborsisuniquelyand
Therefore,bymodelingascenewithourframework,wecan naturallyenabledinour4Drepresentation,whichcannotbe
acquirespeedfieldforfree. Wevisualizetheopticalflowin exploitedbydeformation-basedmethods[56]. Webalance
Fig.8. thedifferentscalesofeachdimensionbydividingwiththe
Finally, following 3DGS [26], we project the sliced 3D correspondingspacialandtemporalscenescales.
Gaussianstothe2Dimageplaneindepthorderandperform
thefastdifferentiablerasterizationtoobtainthefinalimage.
3.3.3 TotalLoss
We implement rotor representation and slicing in a high-
performance CUDA framework and achieve much higher Wefolloworiginal3DGS[26]andaddL 1andSSIMlosses
renderingspeedcomparedtoPyTorchimplementation. betweentherenderedimagesandgroundtruthimages. Our
finallossisdefinedas:
3.3.OptimizationSchema
Whenlifting3DGaussiansinto4Dspace,theincreaseddi- L=(1−λ )L +λ L +λ L +λ L .
1 1 1 ssim 2 entropy 3 consistent4D
mension extends the freedom of Gaussian points. There-
(14)
fore,weintroducetworegularizationtermstohelpstabilize
thetrainingprocess: entropylossand4Dconsistencyloss.
3.3.4 OptimizationFramework
We implement two versions of our method: one using
3.3.1 EntropyLoss
PyTorch for fast development and one highly-optimized
SimilartoNeRFs,eachGaussianpointhasalearnableopac- equivalent in C++ and CUDA for fast training and infer-
itytermo andthevolumerenderingformulaisappliedto ence. Compared to the PyTorch version, our CUDA ac-
i
compositethefinalcolor. Ideally, aGaussianpointshould celeration allows to render at an unprecedented 583 FPS
5at 1352×1014 resolution on a single NVIDIA RTX 4090 Table 1. Evaluation on Plenoptic Video Dataset. We com-
GPU.Further,ourCUDAframeworkalsoacceleratestrain- pare our method with both NeRF-based an Gaussian-based ap-
proaches. Our method significantly outperforms baselines on
ingby16.6x.Forbenchmarkingwithbaselines,wealsotest
PSNR and inference speed. *:Only tested on the scene Flame
ourframeworkonanRTX3090GPUandachieve277FPS,
Salmon.**:Trainedon8GPUs.†:Resultsfrompaper.
whichsignificantlyoutperformscurrentstateoftheart(114
FPS[61]).
ID Method PSNR↑SSIM↑LPIPS↓ Train↓ FPS↑
a DyNeRF[33]*† 29.58 - 0.08 1344h** 0.015
4.Experiments b StreamRF[32] 28.16 0.85 0.31 79min 8.50
c HyperReel[2] 30.36 0.92 0.17 9h 2.00
4.1.Datasets d NeRFPlayer[47]† 30.69 - 0.11 6h 0.05
e K-Planes[19] 30.73 0.93 0.07 190min 0.10
We evaluate our method on two commonly used datasets f MixVoxels[54] 30.85 0.96 0.21 91min 16.70
that are representative of various challenges in dynamic g Deformable4DGS[56] 28.42 0.92 0.17 72min 24.30
h RealTime4DGS[61] 29.95 0.92 0.16 8h 81.31
scene modeling. Plenoptic Video Dataset [33] contains
i Ours 31.62 0.94 0.14 60min 277.47
real-world multi-view videos of 6 scenes. It includes
abrupt motion as well as transparent and reflective ma-
terials. Following prior work [33], we use resolution of 4.4.Results
1352×1014. D-NeRF Dataset [43] contains one-second
monocularvideosfor8syntheticscenes. Weuseresolution 4.4.1 EvaluationonPlenopticVideoDataset
of400×400followingstandardpractice[43].
As detailed in Tab. 1, prior work struggles with trade-offs
between rendering speed and quality, due to the slow vol-
4.2.ImplementationDetails
ume rendering (a-f), time cost of querying neural network
Initialization.Weuniformlysample100,000pointsina4D components(c, d, g), orthespatial-temporalentanglement
boundingboxasGaussianmeans.ForPlenopticdataset,we (a,c,g). Ourmethod,however,exhibitssignificantadvan-
initialize with colored COLMAP [16] reconstruction. 3D tages. Foremost, itmarkedlyoutperformsexistingworkin
scalesaresetasthedistancetothenearestneighbor. Rotors rendering high-resolution videos (1352×1014) at 277 FPS
areinitializedwith(1,0,0,0,0,0,0,0)equivalentofstatic onanNVIDIARTX3090GPU,over10xfasterthanNeRF-
identitytransformation. basedmethods(a-f)andover2xfasterthanGaussian-based
Training. UsingAdamoptimizer,wetrainfor20,000steps methods(g,h). Moreover,ourmethodachievesthehighest
withbatchsize3onD-NeRFdatasetand30,000stepswith PSNR of 31.62 (vs. the previous best 30.85) with a short
batch size 2 on Plenoptic dataset. Densification gradient averagetrainingtimeof60min.
thresholdsare5e−5and2e−4forD-NeRFandPlenoptic As presented in Fig. 5, our method promotes a clearer
datasets, respectively. We set λ = 0.2,λ = 0.01,λ = and more detailed reconstruction of dynamic regions over
1 2 3
0.05,andK = 8,exceptthatλ = 0forPlenopticdataset baselines. For all four scenes, the proposed approach re-
2
since its videos contain a large number of transparent ob- constructshigher-qualityhumanheadsthatmovefrequently
jects. Learning rates, densification, pruning, and opacity andcontainhigh-frequencydetails.Asmagnifiedinthefirst
resetsettingsallfollow[26]. threescenes,baselinesmayproduceblurryartifactsforthe
handregionswithfastmotions. Incomparison,ourmethod
CUDA Acceleration. We implemented the forward and
yieldsthesharpestrenderingsforthesameregions.
backward functions of 4D rotors to 4D rotation matrices
and4DGaussianslicinginourcustomizedCUDAtraining
framework. The duplication and pruning of the 4D Gaus- 4.4.2 EvaluationonD-NeRFDataset
sians are also performed by CUDA, ensuring a low GPU
Monocular video NVS is particularly challenging due to
memoryusage.
sparseinputviews. AssummarizedinTab.2,[60]achieves
thehighestPSNRsinceitdirectlytracksthedeformationof
4.3.Baselines
3D Gaussian points, which perfectly aligns with D-NeRF
We compare with both NeRF-based and concurrent dataset. Otherwise,ourmethodachievesthebestrendering
Gaussian-based methods on the two datasets. Most com- qualityamongalltheothermethods,atarenderingspeedof
pared methods have released official codebase, in which 1258FPS(8xfasterthanthepreviousbest). Moreover,the
case we run the code as is and report the obtained num- trainingonlytakesaround5mininourfastimplementation.
bersfornovelviewrenderingquality,trainingtime,andin- Fig.6showcaseshowthisworksurpassesbaselinesinre-
ference speed. Otherwise, we copy the results from their ducingfloatersandenhancingreconstruction. Forinstance,
papers. All the numbers reported in the tables are bench- the blade of the Lego bulldozer is now more defined. In
markedonanNVIDIARTX3090GPU. Jumping Jacks, our method generates fingers with crisper
6HyperReel MixVoxels RealTime4DGS Ours GroundTruth
Figure5. QualitativeComparisononPlenopticVideoDataset. Comparedwithpreviousstateofthearts,ourmethodrecoversfiner
detailsofdynamicregions,e.g.,themagnifiedhumanfaceandhand. Ourmethodalsoproducessharperrenderingofstaticregions,e.g.,
theoutdoorbackgroundsaroundthehumanheadinsceneFlameSalmonandthezoomed-inhookinsceneCookSpinach.
7Lego JumpingJacks StandUp T-Rex Hook
Figure6. QualitativeComparisonofOurMethodsandBaselinesonD-NeRFDataset. WecomparewithbothNeRF-based(TiNeu-
Vox [15])and Gaussian-based(Deformable4DGS [56]and RealTime4DGS [61]) methods. As revealedin thehighlighted regions, our
methodisabletoreconstructsceneswithlessnoisesandmoredetails,e.g.,thebladeoftheLegobulldozer,thecuffsandhandsinJumping
JacksandHook,theteethofT-Rex,andthehelmetandfacialdetailsinStandUp.
shapes and eliminates artifacts as observed in baseline re- inginclearerfingers.
sults, e.g., floaters beside the cuff in row 3. In Stand Up,
4.5.AblationStudies
thepatternsonthehelmetandfacialfeaturesaremorepro-
nounced in our results. The missing teeth details in base- Tab.3ablatestheeffectivenessofindividualdesignsinour
line results are recovered in ours. Additionally, noises in methodonthechallengingD-NeRFdataset.
Hook’shandaremitigatedbytheproposedmethod,result- Entropy Loss. As shown in Tab. 3 (b), adding entropy
8
xoVueNiT
SGD4elbamrofeD
SGD4emiTlaeR
sruO
hturTdnuorGLego Hook T-Rex HellWarrior BouncingBalls
Figure7. AblationStudyonD-NeRFDataset.Eachrowaddsadifferentcomponent(left)tothesettingoftherowaboveit.Comparing
thescenesbetweenfirsttworows,wecanseethatEntropyLosshelpsreducefloaters. Furthermore,bothKNNLossandBatchTraining
contributetosharpeningappearancedetails,e.g.,thehandsofHookandtheheadofT-Rex,aswellasreconstructingbettergeometry,e.g.,
thebladeofLego,thefeetofHellWarrior,andtheshapesofBouncingBalls.
losssignificantlyreducesthenumberofpointsbytheorder measuredinPSNRandSSIM.Theeffectofentropylossis
of one while maintaining the overall rendering quality as clearlyrevealedinFig.7. Forexample,thefloatersaround
9
gnitteSlanigirO
ssoLyportnE+
ssoLNNK+
gniniarThctaB+
hturTdnuorGTable2. EvaluationonD-NeRFDataset. Wecomparewithre-
centNeRF-basedandGaussian-basedmethods. Ourmethodout-
performsbaselinesonbothPSNRandrenderingspeedbyalarge
margin.
Method PSNR↑SSIM↑LPIPS↓ Train↓ FPS↑
D-NeRF[43] 29.17 0.95 0.07 24h 0.09
TiNeuVox[15] 32.87 0.97 0.04 28min 1.60
K-Planes[19] 31.07 0.97 0.02 54min 1.20
Deformable3DGS[60] 39.31 0.99 0.01 26min 1.78
w/o4DConsistencyL.w/4DConsistencyL. GroundTruth Deformable4DGS[56] 32.99 0.97 0.05 13min 83.00
RealTime4DGS[61] 32.71 0.97 0.03 10min 372.09
Figure8. OpticalFlowVisualization. Ourmethodnaturallyde- Ours 34.26 0.97 0.03 5min 1257.63
rivesaspeedfield.Wecompute3DmotionsforallGaussianpoints
and visualize the rendered 2D optical flow. Adding 4D Consis-
Table 3. Ablation of 4DGS on D-NeRF Dataset. We validate
tencylosssignificantlyreducesnoisesandimprovestheaccuracy
threedesignsonrenderingqualityandnumberofpoints(inmil-
andsmoothnessofopticalflow.
lions): (b)Entropy(withcross-entropyloss), (c)KNN(with4D
KNNConsistencyloss),and(Full)Batch(withbatchtraining).
the scene Lego, Hook, and Bouncing Balls in row 1 have
ID AblationItems D-NeRF
beencompletelyremovedinrow2. Thisdemonstratesthat Entropy KNN Batch PSNR↑ SSIM↑ #Point(M)↓
theentropylosshelpsimposestrongconstraintsonthe4D a 31.53 0.96 1.7e5
Gaussian point distributionduring optimization. However, b ✓ 31.50 0.97 5.4e4
c ✓ ✓ 31.91 0.97 3.0e5
wealsofindthatitresultsinPSNRdegradationinPlenoptic
Full ✓ ✓ ✓ 33.06 0.98 1.4e5
dataset. We believe this is because Plenoptic dataset pro-
videsdenseviewsandcontainsalotoftransparentobjects.
Therefore, we recommend the addition of entropy loss for
struction quality, we observe that, due to the increased di-
opaquesurfacesandsparseviews.
mensions, 4D Gaussians are hard to constrain and cause
4D Consistency Loss. Originally, the states of neigh-
artifacts such as floaters and inconsistent motions. While
boringGaussiansin4Dspacecanchangefreely,whichin-
entropy loss and 4D consistency loss help mitigate these
creases the difficulty of optimization and the redundancy
issues,artifactsstillexist.Futuredirectionsincludeexploit-
ofthemodel. However, theapplicationof4DConsistency
ing4DGaussiansfordownstreamtaskssuchastrackingand
lossenforceslocalconsistencyacrossbothspatialandtem-
dynamicscenegeneration.
poraldimensions.ThisisconfirmedinFig.7,Tab.3(c)and
Fig. 8 where 4D Consistency loss helps recover consistent
motions,addmoredetails,andimproverenderingquality.
BatchTraining. Batchtraininghelpsreducethegradi-
ent noise and stabilize training. Tab. 3 (Full) shows that
batch training further improves the rendering quality over
References
setting (c). For sparse view settings such as monocular
videos,batchtrainingalsohelpsimprovethegeometrycon- [1] JadAbou-Chakra,FerasDayoub,andNikoSu¨nderhauf.Par-
sistency by jointly optimizing over multiple views, as evi- ticlenerf: Particlebasedencodingforonlineneuralradiance
dencedinFig.7,e.g.,feetinHellWarrior. fieldsindynamicscenes. arXivpreprintarXiv:2211.04041,
2022. 3
5.Conclusions [2] BenjaminAttal,Jia-BinHuang,ChristianRichardt,Michael
Zollhoefer, JohannesKopf, MatthewO’Toole, andChangil
In this work, we propose 4D Gaussian Splatting, a novel Kim. Hyperreel: High-fidelity 6-dof video with ray-
conditioned sampling. In Proceedings of the IEEE/CVF
approachthatenableshigh-quality4Ddynamicscenemod-
Conference on Computer Vision and Pattern Recognition,
eling. Ourmethodoutperformspriorartsbyalargemargin
pages16610–16620,2023. 1,3,6
andachievesanunprecedented583FPSrenderingspeedon
[3] JonathanTBarron,BenMildenhall,MatthewTancik,Peter
an RTX 4090 GPU. Moreover, this is a unified framework
Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan.
forboth3Dstaticand4Ddynamicreconstruction. Wewill
Mip-nerf: Amultiscalerepresentationforanti-aliasingneu-
release the code to the community to facilitate related re-
ralradiancefields. InProceedingsoftheIEEE/CVFInter-
search. nationalConferenceonComputerVision,pages5855–5864,
While we have already achieved state-of-the-art recon- 2021. 2,3
10[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P on Computer Vision and Pattern Recognition, pages 2367–
Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded 2376,2019. 2
anti-aliased neural radiance fields. In Proceedings of the [18] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong
IEEE/CVF Conference on Computer Vision and Pattern Chen, BenjaminRecht, andAngjooKanazawa. Plenoxels:
Recognition,pages5470–5479,2022. 1 Radiancefieldswithoutneuralnetworks. InProceedingsof
[5] MojtabaBemana,KarolMyszkowski,JeppeRevallFrisvad, theIEEE/CVFConferenceonComputerVisionandPattern
Hans-Peter Seidel, and Tobias Ritschel. Eikonal fields for Recognition,pages5501–5510,2022. 2
refractivenovel-viewsynthesis. InACMSIGGRAPH2022 [19] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk
ConferenceProceedings,pages1–9,2022. 3 Warburg,BenjaminRecht,andAngjooKanazawa.K-planes:
[6] MarcTenBosch.N-dimensionalrigidbodydynamics.ACM Explicit radiance fields in space, time, and appearance. In
TransactionsonGraphics(TOG),39(4):55–1,2020. 2,4 ProceedingsoftheIEEE/CVFConferenceonComputerVi-
[7] Chris Buehler, Michael Bosse, Leonard McMillan, Steven sionandPatternRecognition,pages12479–12488,2023. 3,
Gortler, and Michael Cohen. Unstructured lumigraph ren- 6,10,18
dering.ACMTransactionsonGraphics(Proc.SIGGRAPH), [20] Wanshui Gan, Hongbin Xu, Yi Huang, Shifeng Chen, and
2001. 2 Naoto Yokoya. V4d: Voxel for 4d novel view synthesis.
[8] AngCaoandJustinJohnson. Hexplane: Afastrepresenta- IEEE Transactions on Visualization and Computer Graph-
tion for dynamic scenes. In Proceedings of the IEEE/CVF ics,2023. 3
Conference on Computer Vision and Pattern Recognition,
[21] ChenGao,AyushSaraf,JohannesKopf,andJia-BinHuang.
pages130–141,2023. 3
Dynamicviewsynthesisfromdynamicmonocularvideo. In
[9] ArthurCayley.ThecollectedmathematicalpapersofArthur ProceedingsoftheIEEE/CVFInternationalConferenceon
Cayley. UniversityofMichiganLibrary,1894. 4 ComputerVision,pages5712–5721,2021. 1,3
[10] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and
[22] StevenJ.Gortler,RadekGrzeszczuk,RichardSzeliski,and
Hao Su. Tensorf: Tensorial radiance fields. In European
MichaelF.Cohen. Lightfieldrendering. ACMTransactions
Conference on Computer Vision, pages 333–350. Springer,
onGraphics(Proc.SIGGRAPH),1996. 2
2022. 2,3
[23] XiangGuo,JiadaiSun,YuchaoDai,GuanyingChen,Xiao-
[11] WeiCheng, RuixiangChen, SimingFan, WanqiYin, Keyu
qingYe,XiaoTan,ErruiDing,YumengZhang,andJingdong
Chen,ZhongangCai,JingboWang,YangGao,Zhengming
Wang. Forward flow for novel view synthesis of dynamic
Yu, Zhengyu Lin, et al. Dna-rendering: A diverse neural
scenes. InProceedingsoftheIEEE/CVFInternationalCon-
actorrepositoryforhigh-fidelityhuman-centricrendering.In
ferenceonComputerVision,pages16022–16033,2023. 3
ProceedingsoftheIEEE/CVFInternationalConferenceon
[24] PeterHedman,JulienPhilip,TruePrice,Jan-MichaelFrahm,
ComputerVision,pages19982–19993,2023. 1
GeorgeDrettakis, andGabrielBrostow. Deepblendingfor
[12] AlvaroCollet,MingChuang,PatSweeney,DonGillett,Den-
free-viewpoint image-based rendering. ACM Transactions
nis Evseev, David Calabrese, Hugues Hoppe, Adam Kirk,
onGraphics(ToG),37(6):1–15,2018. 1
andSteveSullivan. High-qualitystreamablefree-viewpoint
[25] TakeoKanade,PeterRander,andPJNarayanan. Virtualized
video. ACM Transactions on Graphics (ToG), 34(4):1–13,
reality: Constructingvirtualworldsfromrealscenes. IEEE
2015. 3
multimedia,4(1):34–47,1997. 3
[13] PaulEDebevec,CamilloJTaylor,andJitendraMalik.Mod-
[26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimku¨hler,
elingandrenderingarchitecturefromphotographs:Ahybrid
and George Drettakis. 3d gaussian splatting for real-time
geometry-andimage-basedapproach. ACMTransactionson
radiance field rendering. ACM Transactions on Graphics
Graphics(Proc.SIGGRAPH),1996. 2
(ToG),42(4):1–14,2023. 1,2,3,4,5,6,18
[14] Yilun Du, Yinan Zhang, Hong-Xing Yu, Joshua B Tenen-
[27] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen
baum, and Jiajun Wu. Neural radiance flow for 4d view
Koltun.Tanksandtemples:Benchmarkinglarge-scalescene
synthesis and video processing. In 2021 IEEE/CVF In-
reconstruction. ACM Transactions on Graphics (ToG), 36
ternational Conference on Computer Vision (ICCV), pages
(4):1–13,2017. 1
14304–14314.IEEEComputerSociety,2021. 1,3
[15] Jiemin Fang, Taoran Yi, Xinggang Wang, Lingxi Xie, Xi- [28] Georgios Kopanas, Thomas Leimku¨hler, Gilles Rainer,
aopengZhang,WenyuLiu,MatthiasNießner,andQiTian. Cle´mentJambon,andGeorgeDrettakis. Neuralpointcata-
Fast dynamic radiance fields with time-aware neural vox- causticsfornovel-viewsynthesisofreflections.ACMTrans-
els.InSIGGRAPHAsia2022ConferencePapers,pages1–9, actionsonGraphics(TOG),41(6):1–15,2022. 3
2022. 1,3,8,10,18 [29] KiriakosNKutulakosandStevenMSeitz.Atheoryofshape
[16] Alex Fisher, Ricardo Cannizzaro, Madeleine Cochrane, byspacecarving. Internationaljournalofcomputervision,
Chatura Nagahawatte, and Jennifer L Palmer. Colmap: 38:199–218,2000. 2
A memory-efficient occupancy grid mapping framework. [30] MarcLevoyandPatHanrahan. Lightfieldrendering. ACM
RoboticsandAutonomousSystems,142:103755,2021. 6 TransactionsonGraphics(Proc.SIGGRAPH),1996. 2
[17] JohnFlynn, MichaelBroxton, PaulDebevec, MatthewDu- [31] Hao Li, Linjie Luo, Daniel Vlasic, Pieter Peers, Jovan
Vall, Graham Fyffe, Ryan Overbeck, Noah Snavely, and Popovic´, Mark Pauly, and Szymon Rusinkiewicz. Tempo-
RichardTucker.Deepview:Viewsynthesiswithlearnedgra- rallycoherentcompletionofdynamicshapes. ACMTrans-
dientdescent. InProceedingsoftheIEEE/CVFConference actionsonGraphics(TOG),31(1):1–11,2012. 3
11[32] Lingzhi Li, Zhen Shen, Zhongshu Wang, Li Shen, and [44] GernotRieglerandVladlenKoltun. Freeviewsynthesis. In
Ping Tan. Streaming radiance fields for 3d video synthe- EuropeanConferenceonComputerVision,pages623–640,
sis.AdvancesinNeuralInformationProcessingSystems,35: 2020. 2
13485–13498,2022. 3,6 [45] Steven M Seitz and Charles R Dyer. Photorealistic scene
[33] Tianye Li, Mira Slavcheva, Michael Zollhoefer, Simon reconstruction by voxel coloring. International Journal of
Green, Christoph Lassner, Changil Kim, Tanner Schmidt, ComputerVision,35:151–173,1999. 2
Steven Lovegrove, Michael Goesele, Richard Newcombe, [46] Ruizhi Shao, Zerong Zheng, Hanzhang Tu, Boning Liu,
etal. Neural3dvideosynthesisfrommulti-viewvideo. In HongwenZhang,andYebinLiu. Tensor4d: Efficientneural
ProceedingsoftheIEEE/CVFConferenceonComputerVi- 4d decomposition for high-fidelity dynamic reconstruction
sionandPatternRecognition,pages5521–5531,2022. 1,2, andrendering. InProceedingsoftheIEEE/CVFConference
3,6,18,19 onComputerVisionandPatternRecognition,pages16632–
[34] ZhengqiLi,SimonNiklaus,NoahSnavely,andOliverWang. 16642,2023. 3
Neuralsceneflowfieldsforspace-timeviewsynthesisofdy-
[47] LiangchenSong,AnpeiChen,ZhongLi,ZhangChen,Lele
namicscenes. InProceedingsoftheIEEE/CVFConference
Chen, Junsong Yuan, Yi Xu, and Andreas Geiger. Nerf-
on Computer Vision and Pattern Recognition, pages 6498–
player: Astreamabledynamicscenerepresentationwithde-
6508,2021. 3
composedneuralradiancefields.IEEETransactionsonVisu-
[35] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and alization and Computer Graphics, 29(5):2732–2742, 2023.
Deva Ramanan. Dynamic 3d gaussians: Tracking 2,3,6
by persistent dynamic view synthesis. arXiv preprint
[48] Pratul P Srinivasan, Richard Tucker, Jonathan T Barron,
arXiv:2308.09713,2023. 2,3
RaviRamamoorthi,RenNg,andNoahSnavely.Pushingthe
[36] BenMildenhall, PratulPSrinivasan, RodrigoOrtiz-Cayon,
boundariesofviewextrapolationwithmultiplaneimages.In
NimaKhademiKalantari,RaviRamamoorthi,RenNg,and
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
AbhishekKar. Locallightfieldfusion: Practicalviewsyn-
sionandPatternRecognition,pages175–184,2019. 2
thesiswithprescriptivesamplingguidelines. ACMTransac-
[49] JustusThies,MichaelZollho¨fer,andMatthiasNießner. De-
tionsonGraphics(TOG),38(4):1–14,2019. 2
ferred neural rendering: Image synthesis using neural tex-
[37] B Mildenhall, PP Srinivasan, M Tancik, JT Barron, R Ra-
tures. Acm Transactions on Graphics (TOG), 38(4):1–12,
mamoorthi,andRNg. Nerf: Representingscenesasneural
2019. 2
radiancefieldsforviewsynthesis. InEuropeanconference
[50] Fengrui Tian, Shaoyi Du, and Yueqi Duan. Monon-
oncomputervision,2020. 1,2,3
erf: Learning a generalizable dynamic radiance field from
[38] ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexan-
monocularvideos.InProceedingsoftheIEEE/CVFInterna-
der Keller. Instant neural graphics primitives with a mul-
tionalConferenceonComputerVision,pages17903–17913,
tiresolutionhashencoding. ACMTransactionsonGraphics
2023. 3
(ToG),41(4):1–15,2022. 2
[51] EdgarTretschk,AyushTewari,VladislavGolyanik,Michael
[39] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien
Zollho¨fer,ChristophLassner,andChristianTheobalt. Non-
Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo
rigidneuralradiancefields: Reconstructionandnovelview
Martin-Brualla. Nerfies: Deformableneuralradiancefields.
synthesis of a dynamic scene from monocular video. In
In Proceedings of the IEEE/CVF International Conference
ProceedingsoftheIEEE/CVFInternationalConferenceon
onComputerVision,pages5865–5874,2021. 1,3
ComputerVision,pages12959–12970,2021. 1,3
[40] KeunhongPark, UtkarshSinha, PeterHedman, JonathanT
Barron, Sofien Bouaziz, Dan B Goldman, Ricardo Martin- [52] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,
Brualla, and Steven M Seitz. Hypernerf: a higher- JonathanTBarron,andPratulPSrinivasan.Ref-nerf:Struc-
dimensionalrepresentationfortopologicallyvaryingneural turedview-dependentappearanceforneuralradiancefields.
radiancefields. ACMTransactionsonGraphics(TOG),40 In2022IEEE/CVFConferenceonComputerVisionandPat-
(6):1–12,2021. 1,3 ternRecognition(CVPR),pages5481–5490.IEEE,2022. 2
[41] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, [53] MichaelWaechter,NilsMoehrle,andMichaelGoesele. Let
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming therebecolor!large-scaletexturingof3dreconstructions.In
Lin,NataliaGimelshein,LucaAntiga,etal.Pytorch:Anim- EuropeanConferenceonComputerVision,pages836–850.
perativestyle, high-performancedeeplearninglibrary. Ad- Springer,2014. 2
vancesinneuralinformationprocessingsystems,32,2019. [54] Feng Wang, Sinan Tan, Xinghang Li, Zeyue Tian, Yafei
2 Song,andHuapingLiu. Mixedneuralvoxelsforfastmulti-
[42] EricPennerandLiZhang. Soft3dreconstructionforview viewvideosynthesis. InProceedingsoftheIEEE/CVFIn-
synthesis. ACMTransactionsonGraphics(TOG),36(6):1– ternational Conference on Computer Vision, pages 19706–
11,2017. 2 19716,2023. 3,6
[43] Albert Pumarola, Enric Corona, Gerard Pons-Moll, and [55] DanielNWood,DanielIAzuma,KenAldinger,BrianCur-
Francesc Moreno-Noguer. D-nerf: Neural radiance fields less,TomDuchamp,DavidHSalesin,andWernerStuetzle.
fordynamicscenes. InProceedingsoftheIEEE/CVFCon- Surfacelightfieldsfor3dphotography.InSeminalGraphics
ferenceonComputerVisionandPatternRecognition,pages Papers: PushingtheBoundaries,Volume2,pages487–496.
10318–10327,2021. 1,2,3,6,10,18 2023. 2
12[56] GuanjunWu,TaoranYi,JieminFang,LingxiXie,Xiaopeng
Zhang,WeiWei,WenyuLiu,QiTian,andXinggangWang.
4dgaussiansplattingforreal-timedynamicscenerendering.
arXivpreprintarXiv:2310.08528,2023. 2,3,5,6,8,10,18
[57] Minye Wu, Yuehao Wang, Qiang Hu, and Jingyi Yu.
Multi-view neural human rendering. In Proceedings of
theIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages1682–1691,2020. 1
[58] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin
Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf:
Point-based neural radiance fields. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages5438–5448,2022. 3
[59] ZhiwenYan,ChenLi,andGimHeeLee.Nerf-ds:Neuralra-
diancefieldsfordynamicspecularobjects.InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),pages8285–8295,2023. 3
[60] Ziyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing
Zhang, and Xiaogang Jin. Deformable 3d gaussians for
high-fidelitymonoculardynamicscenereconstruction.arXiv
preprintarXiv:2309.13101,2023. 2,3,6,10,18,19
[61] Zeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li
Zhang. Real-time photorealistic dynamic scene represen-
tation and rendering with 4d gaussian splatting. In Inter-
national Conference on Learning Representations (ICLR),
2024. 2,3,4,6,8,10,18,19
[62] Qiang Zhang, Seung-Hwan Baek, Szymon Rusinkiewicz,
andFelixHeide. Differentiablepoint-basedradiancefields
forefficientviewsynthesis. InSIGGRAPHAsia2022Con-
ferencePapers,pages1–12,2022. 3
[63] TinghuiZhou, RichardTucker, JohnFlynn, GrahamFyffe,
andNoahSnavely.Stereomagnification.ACMTransactions
onGraphics,37(4):1–12,2018. 2
[64] CLawrenceZitnick,SingBingKang,MatthewUyttendaele,
Simon Winder, and Richard Szeliski. High-quality video
view interpolation using a layered representation. ACM
transactionsongraphics(TOG),23(3):600–608,2004. 3
13A.Detailsof4DGaussianSplatting
The4Drotorrisconstructedfromacombinationof8componentsbasedonasetofbasis:
r=s+b e +b e +b e +b e +b e +b e +pe , (15)
01 01 02 02 03 03 12 12 13 13 23 23 0123
wheree = e ∧e representstheouterproductbetween4Daxise (i ∈ {0,1,2,3})thatdefinestheorthonormalbasisin
ij i j i
the4DEuclideanspace,ande =e ∧e ∧e ∧e istheouterproductofall4Daxise (i∈{0,1,2,3}).
0123 0 1 2 3 i
A.1.4DRotorNormalization
Toensurethatrrepresentsavalid4Drotation,itmustbenormalizedwith
rr† =1, (16)
wherer†istheconjugateofr:
r† =s−b e −b e −b e −b e −b e −b e +pe . (17)
01 01 02 02 03 03 12 12 13 13 23 23 0123
ByintegratingEq.16,weget
rr† = (−2b b +2b b −2b b +2ps)e ∧e ∧e ∧e +
01 23 02 13 03 12 0 1 2 3
(cid:0) b2 +b2 +b2 +b2 +b2 +b2 +p2+s2(cid:1) (18)
01 02 03 12 13 23
= 1.
Thisleadstotwoconditions:
(cid:26)
ps−b b +b b −b b =0,
01 23 02 13 03 12 (19)
b2 +b2 +b2 +b2 +b2 +b2 +p2+s2 =1.
01 02 03 12 13 23
Wedefine:
ε=f(r)=ps−b b +b b −b b =ps−∆, (20)
01 23 02 13 03 12
l2 =b2 +b2 +b2 +b2 +b2 +b2 +p2+s2. (21)
01 02 03 12 13 23
Regardingthefirstcondition,ourgoalistoachieveε=f(r)=0. Nearthezeropoint,whenf(r)isconceptualizedasa
linearfunctionofr,therootcanbeapproximatedutilizingthefirstderivative. Thus,weassume
0=f(r+δ∇ε), (22)
whereδisasmallnumber,and∇εisthegradientofε=f(r),whichcanbecomputedas:
 ∂ε =s

∂∂
∂b∂
∂
∂
0ε
εp
1ε
s
=
=
=p
− −b
b23
∂b23 01 , (23)
∂ε =b
 ∂
∂
∂∂
∂
∂b
b
b0
1
0ε
ε
ε2
3
3
=
=
=b
−
−1
0
b
b3
2
12
∂b12 03
resultingin
f(r+δ∇ε)=(ps−∆)(1+δ2)+l2δ. (24)
Tocomputeδ,itisevidentthatthesolution’sexistencecondition
l4 ⩾4(ps−∆)2 (25)
14isinherentlysatisfied. Consequently,twosolutionsforδarededuced:
(cid:112)
−l2± l4−4(ps−∆)2
δ = . (26)
2(ps−∆)
Todeterminethesign,letl2 =1+χ,ps−∆=ξ. Asχ→0,ξ →0,δmustsatisfy
±(1+χ−2ξ2)−1−χ
δ = →0, (27)
2ξ
andthepositivesignistaken,δ =−ξ,whichmeetstherequirement. Therefore,
(cid:112)
−l2+ l4−4(ps−∆)2
δ = , (28)
2(ps−∆)
andapplyingr:=r+δ∇εsatisfiesthefirstnormalizationcondition.
Regardingthesecondcondition,itsufficestocalculatel2 forr+δ∇εpost-updateandthendivideeachcomponentbyl.
Aseachcomponentundergoesproportionalscaling,theconditionps−∆=0remainsintact.
Tosummarize,withinthe4DrotornormalizationF ,wefirstapply:
norm
r:=r+δ∇ε, (29)
where
√
−l2+ l4−4ε2
δ = , (30)
2ε
ε=ps−b b −b b +b b . (31)
01 23 02 13 03 12
Thenwiththeupdatedr,wecalculate
l2 =b2 +b2 +b2 +b2 +b2 +b2 +p2+s2, (32)
01 02 03 12 13 23
andthefinalnormalizedcoefficientsareobtainedas:
r:=r/l. (33)
Thisresultsinanormalized4Drotorrsuitablefor4Drotation.
A.2.4DRotortoRotationMatrixTransformation
Afternormalization,wemapasource4Dvectorutoatargetvectoru′via
u′ =rur†, (34)
wheresuchmappingcanalsobewrittenin4DrotationmatrixR form
4D
 R00 R01 R02 R03 
4D 4D 4D 4D
u′ =R 4Du=F map(r)u=  R R1 4 2D0
0
R R1 4 21 1D R R1 4 2D2
2
R R1 4 2D3
3
  u, (35)
4D 4D 4D 4D
R30 R31 R32 R33
4D 4D 4D 4D
15where
R00 =−b2 −b2 −b2 +b2 +b2 +b2 −p2+s2, (36)
4D 01 02 03 12 13 23
R01 =2(b s−b b −b b +b p), (37)
4D 01 02 12 03 13 23
R02 =2(b b +b s−b b −b p), (38)
4D 01 12 02 03 23 13
R03 =2(b b +b b +b s+b p), (39)
4D 01 13 02 23 03 12
R10 =2(−b s−b b −b b −b p), (40)
4D 01 02 12 03 13 23
R11 =−b2 +b2 +b2 −b2 −b2 +b2 −p2+s2, (41)
4D 01 02 03 12 13 23
R12 =2(−b b +b p+b s−b b ), (42)
4D 01 02 03 12 13 23
R13 =2(−b b −b p+b b +b s), (43)
4D 01 03 02 12 23 13
R20 =2(b b −b s−b b +b p), (44)
4D 01 12 02 03 23 13
R21 =2(−b b −b p−b s−b b ), (45)
4D 01 02 03 12 13 23
R22 =b2 −b2 +b2 −b2 +b2 −b2 −p2+s2, (46)
4D 01 02 03 12 13 23
R23 =2(b p−b b −b b +b s), (47)
4D 01 02 03 12 13 23
R30 =2(b b +b b −b s−b p), (48)
4D 01 13 02 23 03 12
R31 =2(−b b +b p+b b −b s), (49)
4D 01 03 02 12 23 13
R32 =2(−b p−b b −b b −b s), (50)
4D 01 02 03 12 13 23
R33 =b2 +b2 −b2 +b2 −b2 −b2 −p2+s2. (51)
4D 01 02 03 12 13 23
A.3.Temporally-Slicing4DGuassians
Inthissection,weprovidethedetailsaboutslicingthe4DGaussianto3Dovertimet. Thatis,wecalculatethe3Dcenter
positionand3Dcovarianceafterbeinginterceptedbythetplane.
Calculation of the 3D Center Position and 3D Covariance. First, we have the 4D covariance matrix represented by
Σ andtherotationR
4D 4D
Σ =R S ST RT . (52)
4D 4D 4D 4D 4D
Thenweget
 
c c c c
xx xy xz xt
Σ− 4D1 =R 4DS− 4D1(cid:0) S− 4D1(cid:1)T RT
4D
=  c cx xy
z
c cy yy
z
c cy zzz c cy ztt  . (53)
c c c c
xt yt zt tt
Sincea4DGaussiancanbeexpressedas
G 4D(x)=e− 21x′TΣ− 4D1x′ =e− 21B, (54)
wherex′ =x−(µ ,µ ,µ ,µ ). Thenweget
x y z t
B =x′2c +y′2c +z′2c +2x′y′c +2x′z′c +2y′z′c +2x′t′c +2y′t′c +2z′t′c +t′2c . (55)
xx yy zz xy xz yz xt yt zt tt
Toobtainthe3Dcenterpositionslicedbythetplane,weset
B = c (x′+αt′)2+c (y′+βt′)2+c (z′+γt′)2+2c (x′+αt′)(y′+βt′)
xx yy zz xy
+2c (y′+βt′)(z′+γt′)+2c (x′+αt′)(z+γt′) (56)
yz xz
+(c −c α2−c β2−c γ2−2c αβ−2c βγ−2c αγ)t′2.
tt xx yy zz xy yz xz
Thentheequationgroupisobtained:

c α+c β+c γ =c
 xx xy xz xt
c α+c β+c γ =c . (57)
xy yy yz yt
 c α+c β+c γ =c
xz yz zz zt
16Aftersolvingα,β,andγ,the3Dcenterpositionattimetisobtained

µ (t)=µ −α(t−µ )
 x x t
µ (t)=µ −β(t−µ ) , (58)
y y t
 µ (t)=µ −γ(t−µ )
z z t
Inaddition,fromEq.56,theinverseof3DcovariancematrixΣ−1 is:
3D
 
c c c
xx xy xz
Σ− 3D1 =c
xy
c
yy
c yz, (59)
c c c
xz yz zz
Let
λ=c −c α2−c β2−c γ2−2c αβ−2c βγ−2c αγ, (60)
tt xx yy zz xy yz xz
afteraddingµ ,µ ,µ ,µ backtox′,y′,z′,t′,weget
x y z t
G 3D(x,t)=e− 21λ(t−µt)2 e−1 2[x−µ(t)]TΣ− 3D1[x−µ(t)]. (61)
Avoiding Numerical Instability. Directly calculating Σ from Σ−1 according to Eq. 59 can induce numerical insta-
3D 3D
bilityofmatrixinverse. Thisissuepredominantlyariseswhenthescalesofthe3DGaussianexhibitsubstantialmagnitude
discrepancies,leadingtosignificanterrorsincalculatingΣ ,andresultinginexcessivelylargeGaussians. Tocircumvent
3D
thischallenge,directcalculationofΣ mustbeavoided.
3D
GiventhatΣ anditsinverseΣ−1 arebothsymmetricmatrices,weset:
4D 4D
(cid:18) (cid:19) (cid:18) (cid:19)
U V A M
Σ = andΣ−1 = , (62)
4D VT W 4D MT Z
whereUandAare3×3matrices. Byapplyingtheinverseformulaforsymmetricblockmatrices,itfollowsthat:
 A−1ηηTA−1 A−1η
A−1+ − (cid:18) (cid:19)
Σ =(cid:0) Σ−1(cid:1)−1 = h h = U V , (63)
4D 4D  ηTA−1 1  VT W
−
h h
whereh=Z−ηTA−1η.
ComparingEq.53withEq.59,wehaveΣ−1 =A−1,andcomparingthetwoexpressionsofΣ ,weget
3D 4D
VVT
Σ−1 =A−1 =U− , (64)
3D W
thus effectively avoiding direct inversion of A and leveraging the computationally feasible sub-blocks of Σ to compute
4D
Σ .
3D
Additionally,wecanusetheaboveexpressiontosimplifythecalculationof(α,β,γ,λ). Letg =(α,β,γ)T,thenEq. 57
isreformulatedas:
Ag =η, (65)
anditssolutionis
V
g =A−1η =−hV=− . (66)
W
Similarly,forλ:
λ =Z−gTAg
=Z−ηT(A−1)Tη
=Z−ηTA−1η . (67)
=h+ηTA−1η−ηTA−1η
=W−1
17Table 4. Influence of Background Colors on D-NeRF Dataset. We report the per-scene and average PSNR for each method. All
methodsusethedefaultbackgroundcolorsassetintheirofficialreleasedcode. ItisobservedthatmostscenesyieldahigherPSNRon
blackbackgrounds,especiallywhentheforegroundsaredarker.ThisphenomenonhasalsobeenobservedbyDeformable3DGS[60].Our
methodoutperformsbaselinesonbothbackgroundcolors.
Background Method T-Rex JumpingJacks HellWarrior StandUp BouncingBalls Mutant Hook Lego Avg
D-NeRF[43] 31.45 32.56 24.70 33.63 38.87 21.41 28.95 21.76 29.17
TiNeuVox[15] 32.78 34.81 28.20 35.92 40.56 33.73 31.85 25.13 32.87
K-Planes[19] 31.44 32.53 25.38 34.26 39.71 33.88 28.61 22.73 31.07
White Deformable4DGS[56] 33.12 34.65 25.31 36.80 39.29 37.63 31.79 25.31 32.99
Deformable3DGS[60] 40.14 38.32 32.51 42.65 43.97 42.20 36.40 25.55 37.72
RealTime4DGS[61] 31.22 31.29 24.44 37.89 35.75 37.69 30.93 24.85 31.76
Ours 31.24 33.37 36.85 38.89 36.30 39.26 33.33 25.24 33.06
Deformable3DGS[60] 38.55 39.21 42.06 45.74 41.33 44.16 38.04 25.38 39.31
Black RealTime4DGS[61] 29.82 30.44 34.67 39.11 32.85 38.74 31.77 24.29 32.71
Ours 31.77 33.40 34.52 40.79 34.74 40.66 34.24 24.93 34.26
Tosummarize,the3DGaussianprojectfromthe4Dattimetisobtainedas:
G 3D(x,t)=e−1 2λ(t−µt)2 e−1 2[x−µ(t)]TΣ− 3D1[x−µ(t)], (68)
where
λ=W−1,
VVT
Σ 3D =A−1 =U− W , (69)
V
µ(t)=(µ ,µ ,µ )T +(t−µ ) .
x y z t W
B.AdditionalExperiments
B.1.DatasetsDetails
Plenoptic Video Dataset [33]. The real-world dataset captured by a multi-view GoPro camera system. We evaluate the
baselineson6scenes: CoffeeMartini,FlameSalmon,CookSpinach,CutRoastedBeef,FlameSteak,andSearSteak. Each
sceneincludesfrom17to20viewsfortrainingandonecentralviewforevaluation. Thesizeoftheimagesisdownsampled
to1352×1014forfaircomparison. Thisdatasetpresentsavarietyofchallengingelements,includingthesuddenappearance
offlames,movingshadows,aswellastranslucentandreflectivematerials.
D-NeRF Dataset [43]. The synthetic dataset of monocular video, presenting a significant challenge due to the single
cameraviewpointavailableateachtimestamp. Thisdatasetcontains8scenes: HellWarrior,Mutant,Hook,BouncingBalls,
Lego, T-Rex, Stand Up, and Jumping Jacks. Each scene comprises 50 to 200 images for training, 10 or 20 images for
validation, and20imagesfortesting. Eachimagewithinthisdatasetisdownsampledtoastandardresolutionof400×400
fortrainingandevaluationfollowingthepreviouswork[43].
B.2.AdditionalImplementationDetails
InthespatialinitializationforPlenopticVideoDataset[33],wedefineanaxis-alignedboundingboxsizedaccordingtothe
range of SfM points. For D-NeRF dataset, the box dimensions are set to [−1.3,1.3]3. Within these boxes, we randomly
sample100,000pointsasthepositionsoftheGaussians.ThetimemeansofGaussiansareuniformlysampledfromtheentire
timeduration,i.e.,[0,1]forD-NeRFdatasetand[0,10]forPlenopticVideoDataset.Theinitializedtimescaleissetto0.1414
forD-NeRFdatasetand1.414forPlenopticdataset.
Following[26], weemploytheAdamoptimizerwithspecificlearningrates: 1.6e−4forpositions, 1.6e−4fortimes,
5e−3for3Dscalesandtimescales, 1e−3forrotation, 2.5e−3forSHlowdegreeand1.25e−4forSHhighdegrees,
and0.05foropacity. Weapplyanexponentialdecayscheduletothelearningratesforpositionsandtimes,startingfromthe
initialrateanddecayingto1.6e−6forpositionsandtimesatstep30,000. Thetotaloptimizationconsistsof30,000stepson
Plenopticdatasetand20,000stepsonD-NeRFdataset. Opacityisresetevery3,000steps,whiledensificationisexecutedat
intervalsof100steps,startingfrom500to15,000steps.
18B.3.InfluenceofBackgroundColorsonD-NeRFDataset.
D-NeRFdatasetprovidessyntheticimageswithoutbackgrounds.Consequently,previousbaselinemethodsincorporateeither
a blackor white backgroundduring training andevaluation. Specifically, Deformable3DGS [60]and RealTime4DGS [61]
utilizeablackbackground,whileothermethodsoptforawhitebackground.
Ourobservations,asshowninTab.4,indicatethatourmethodyieldshigherrenderingquality(PSNR34.26)whentrained
with a black background as opposed to a white one (PSNR 33.06). In particular, for scenes with brighter foregrounds
(Jumping Jacks, Bouncing Balls, and Lego), models trained using white backgrounds perform higher. These performance
discrepanciesbetweenthetwobackgroundcolorshavealsobeenobservedandreportedinpriorwork[60].
TheresultsreportedinTab.2forourmethodarebasedonexperimentsusingblackbackground.Foralltheotherbaselines,
wefollowtheiroriginalsettings.Notethatourresultsonthewhitebackgroundoutperformallpreviousmethodsthatusewhite
background. Andsimilarly,ourresultsontheblackbackgroundoutperformallpreviousmethodsthatuseblackbackground
intheiroriginalexperiments.
TheresultsreportedinablationTab.3areconductedwithwhitebackground. Forthepurposeofvisualization,weshow
images trained with white background in Fig. 6 and Fig. 7 which include our method and two reproduced baselines (De-
formable3DGS [60] and RealTime4DGS [60]). Finally, for all experiments on Plenoptic Video Dataset [33], we simply
chooseblackbackgroundforourmethodandfollowtheoriginalsettingsofallthebaselines.
19