Uncertainty of Thoughts: Uncertainty-Aware Planning
Enhances Information Seeking in Large Language Models
ZhiyuanHu1∗ ChuminLiu2 XidongFeng3 YilunZhao4
See-KiongNg1 AnhTuanLuu2 JunxianHe5 PangWeiKoh6 BryanHooi1
1NationalUniversityofSingapore 2NanyangTechnologicalUniversity
3UniversityCollegeLondon 4YaleUniversity
5TheHongKongUniversityofScienceandTechnology 6UniversityofWashington
Abstract
In the face of uncertainty, the ability to seek information is of fundamental im-
portance. In many practical applications, such as medical diagnosis and trou-
bleshooting, theinformation needed to solve the task is not initially given, and
has to be actively sought by asking follow-up questions (for example, a doctor
askingapatientformoredetailsabouttheirsymptoms). Inthiswork,weintroduce
UncertaintyofThoughts(UoT),analgorithmtoaugmentlargelanguagemodels
withtheabilitytoactivelyseekinformationbyaskingeffectivequestions. UoT
combines1)anuncertainty-awaresimulationapproachwhichenablesthemodelto
simulatepossiblefuturescenariosandhowlikelytheyaretooccur,2)uncertainty-
basedrewardsmotivatedbyinformationgainwhichincentivizesthemodeltoseek
information,and3)arewardpropagationschemetoselecttheoptimalquestion
toaskinawaythatmaximizestheexpectedreward. Inexperimentsonmedical
diagnosis,troubleshootingandthe‘20Questions’game,UoTachievesanaverage
performanceimprovementof57.8%intherateofsuccessfultaskcompletionacross
multipleLLMscomparedwithdirectprompting,andalsoimprovesefficiency(i.e.,
thenumberofquestionsneededtocompletethetask). Ourbenchmarkandcode
arepubliclyreleased2.
1 Introduction
As the capabilities of large language models
(LLMs)grow,theyarebeingincreasinglydeployed
I'vebeenhavingabadheadachesinceyesterday
inchallengingreal-worldsettingsinvolvinguncer-
Have you observed any vision changes, or
tainty and ambiguity. In particular, recent work sensitivitytolight?
aimstodevelopLLMagentsorassistantsXietal. Oh,nowyoumentionit,Ihavebeenmore
(2023); Park et al. (2023) that effectively com- Patient sensitivetolightrecently. Doctor
pletetasksininteractiveenvironments,leadingto Have you injured your head recently?
a growing need for LLMs that can actively seek Ah,Ididhitmyheadafewdaysago,butit
didn’tseemserious.
theinformationtheyneedtosolveataskbyasking
Youmayhavepost-concussionsyndrome.
questionsinconversationalsettings. Forexample,
inmedicaldiagnosis,patientsoftendonotinitially Figure1: Theimportanceofinformationseek-
report their symptoms in full detail. In such sit- inginmedicaldiagnosis. Thepatientinitially
uations, the ability of the doctor to ask effective only complains of a headache, but by asking
questionsiscrucial,asasuccessfuldiagnosisof- therightquestions,thedoctoruncoversthecrit-
tendependsonrevealingimportantdetailsthatthe icalinformationneededforacorrectdiagnosis.
patientdidnotinitiallyprovide(Figure1).
∗Correspondingto:ZhiyuanHu,zhiyuan_hu@u.nus.edu
2https://github.com/zhiyuanhubj/UoT
Preprint.Underreview.
4202
beF
5
]LC.sc[
1v17230.2042:viXraRecenttechniquesaimtoimproveLLMs’reasoningorplanningabilitiesbasedonthegiveninforma-
tionratherthanenablingLLMstoseekinformationefficiently. Forexample,Chain-of-Thought(CoT)
(Weietal.,2022)andTree-of-Thoughts(ToT)(Yaoetal.,2023)allowLLMstoexpressintermediate
‘thoughts’andreasonoverthem. Unlikethesemethods,ourfocusisonenablingtheLLMtoask
questionseffectivelybyexplicitlyguidingthemodeltowardreducinguncertainty,whichthesedonot
consider. Thus,theylackeffectivesignalsforwhatquestionscanbetterreducemodeluncertaintyby
uncoveringcriticalinformation.
ToenhanceLLMsinactivelyseekinginformation,weintroduceUncertaintyofThoughts(UoT),
aplug-and-playapproachthatimprovesLLMs’abilitiestoaskusefulquestionsbymodelingtheir
ownuncertainty. UoTisaprincipledapproachrelyingonuncertainty-basedrewardsmotivatedby
informationgain,whichincentivizesamodeltoseekinformationinawaythatmaximallyreducesthe
amountofinformationitdoesnotknow. Toutilizetheserewards,wedevelopanuncertainty-aware
simulationframework,enablingthemodeltosimulatepossiblefuturescenariosalongwithhowlikely
theyaretooccur. Giventhesescenarios,weutilizearewardpropagationschemetoselecttheoptimal
questiontoaskinawaythatmaximizestheexpectedreward.
Additionally,moststandardbenchmarksforLLMs,particularlyinquestionanswering,assumethat
allnecessaryinformationtosolveataskisprovidedattheoutset,andthusdonotevaluatethemodel’s
activeinformation-seekingcapabilities. Toclosethisgap,wefirstintroduceabenchmarkcomprising
5datasets3on3tasks: 20Questions,asimplifiedmedicaldiagnosistask,andabasictroubleshooting
task. Thesetasksaredesignedtomeasurethemodel’sabilitytoaskquestionseffectivelytogather
theinformationtheyneed. Forexample,the20Questionsgame,alsostudiedbyNoever&McKee
(2023),requiresthemodeltoask‘yes’or‘no’questionstodetermineanunknownobjectorentity.
Thisscenarioservesasaclearandeasilyanalyzedtestcase,isolatingthemodel’sabilitytorecognize
itsownuncertainty,andtoaskquestionsthatguideittothecorrectanswer.
OurworkisasteptowardLLMsthatcaneffectivelyoperateinsettingswithhighuncertaintyand
ambiguity, beyond conventional QA settings where all the information needed to solve the task
is provided to the model at the outset. To the best of our knowledge, UoT is the first approach
for enabling LLMs to ask effective questions by explicitly modeling and seeking to reduce their
uncertainty. Ourkeycontributionsareasfollows:
1. WeintroduceUncertaintyofThoughts(UoT),aplug-and-playapproachenablingLLMsto
explicitlymodelandseektoreducetheiruncertainty. UoTutilizesaprincipledapproach
basedonanuncertainty-awareframeworkforsimulatingpossiblefutures,rewardsmotivated
byinformationgain,andarewardpropagationschemetoselecttheoptimalquestiontoask.
2. We introduce a benchmark of 3 tasks and 5 datasets, designed to evaluate the ability of
LLMstoseektheinformationtheyneedbyaskingquestions.
3. Experiments show that UoT improves the success rate of multiple LLMs by 57.8% on
averagecomparedwithdirectprompting,achievingtopperformanceonbothtasksuccess
andefficiency. Ourbenchmarkandcodearepubliclyavailable.
2 Methodology
2.1 ProblemFormulation
Theproblemsettinginvolvestworoles: theQuestionerandtheAnswerer,performedbytheLLM
and a human, respectively. The goal of the Questioner is to deduce an unknown piece of infor-
mation. We formulate this using a possibility space Ω, which is the set of all possible options,
of which a single element ω ∈ Ω, is the true option in each given scenario4. For example,
in a medical diagnosis setting, Ω is the set of all possible diseases relevant in the context, e.g.,
Ω={Bronchitis,Flu,...,Hypertension},andforeachpatient,ωistheactualdiseaseofthepatient.
3WealsoincorporatetheeffortsofpriordatasetsSrivastavaetal.(2022);Xuetal.(2019);Liuetal.(2022);
Raghuetal.(2021),throughfurtherworkandrefinementtoconstructthisbenchmark.Detailsareintroducedin
section3ExperimentsandAppendixF.2.
4Underthemeasure-theoreticformulationofprobability,thesamplepointωisanelementofthesample
spaceΩ,andallrandomvariablesaredefinedtobefunctionsofω.Whileweconformtothisformulation,wetry
toavoidunnecessarymeasure-theoreticbackgroundforeaseofunderstanding;hence,itissufficientforreaders
tounderstandωasthe‘trueoption’ineachscenario.
2(a) Question Generation and Simulation (b) Uncertainty-based Reward (c) Reward Propagation Scheme
W thh ea pt ad ti is ee na ts he a d vo es L?e
L a sM
D vi od m y io t?u vL oL N nmIr oM e a oi ts tl , er p hs Ie do
a
i a dm vn td ow es yu e i l csa ete EQ stu i G Eme pc Aas noaat s ti nni ett so r ee rd oin i
m
t ni tr ie d is ie isr n, aam ,( g ftL F …oe …a loL si u n d M , i n) g sD t oo m y …
…
…ao cu h h pa av ine ? … …
Did you vomit?
I v a o twl mre ii ca t eed dy poG isa f os o nt or idi nt i gs …, = 0.6
RuG bea ls lt ar ,i t Ais n, eF mlu i, a …questioner haD veo fy eo vu e r? I'Y me s fe, vse ev r-e fr re ee F Hlu yNR, p oP u orn b goe e lv yu ll i cm a r eu… mo sn , i i aa , , … … … EG na p tes ot rir isi tot ii sns ,i , Fn f go lu,o …d N n Io no t f, ohI re d mvo e a tionE Gn F at le u inr …iti s, = 0.8 = 0.7 = 1.2 = 1.1= 0.6
Potential Disese Anemia …
Candidates Uncertainty Based Reward : 0.8 Accumulated Reward Expected Reward
Possible Question Generation Simulate Possible Futures Conditional Probability of Answer: = + =1.4 =1.5
Figure2: UoTOverview: UoTincludesthreecomponents: (a)QuestionGenerationandSimulation,
whereanLLMproposescandidatequestionsandsimulatesfuturescenarios;(b)Uncertainty-based
Rewards,measuringtheuncertaintyreductionfromanswerstoaquestion,and(c)RewardPropagation
computingaccumulatedrewardsR overpastquestions,andexpectedrewardsR capturingexpected
a e
future gains. This process concludes with the selection of candidate questions with the highest
expectedreward.
TheinteractionbetweentheQuestionerandtheAnswereroccursovermultipleturns. Forinstance,
theQuestionermayask,“Doyouhaveafever?",towhichtheAnswererresponds,“Yes,I’vehad
ahighfeverforthepasttwodays."TheQuestionerthenasksanotherquestionsuchas“Haveyou
vomited?"ThisexchangecontinueseitheruntiltheQuestionercorrectlydeterminesthefinalanswer,
ortheconversationreachesamaximumnumberofturns. Atthispoint,theinteractionends,andthe
Questionerissuccessfulifithascorrectlydeterminedthetrueoptionω.
Mostofthedescriptionofourapproachfocusesontheclosedsetscenario,inwhichweassumethat
theQuestionerstartswithknowledgeofthepossibilityspaceΩ,e.g.,thesetofallpossiblediseases
inmedicaldiagnosis. Inourextensionsection2.7,weadaptourapproachtotheopensetscenario,
in which this knowledge is absent. Moreover, as the questioning progresses, we use an LLM to
graduallyrefinethissetofpossibilitiestothosethatareconsistentwiththecurrentanswersgivenso
farbytheAnswerer. DefinethecurrentpossibilitysetΩ asthesubsetofΩthatisconsistentwithall
i
answersgivenbytheAnswererbeforethestartoftheithinteractionstep.
As we discuss more later, we focus on applications where answers can be grouped into a small
numberofsemanticallydistinctcategories(inourcase,affirmativeandnegativeresponses),asthis
allowsustocomputemeaningfuluncertaintymetricsinasimplerway. Conceptually,ourframework
canstraightforwardlybeextendedtoallowforawiderselectionofanswers.
2.2 UncertaintyofThoughts: Overview
As Figure 2 shows, to effectively reduce uncertainty, our UoT method first generates multiple
questionsascandidatestoask,andsimulatespossiblefuturesforeachoneintheformofatree
structure. Next,uncertainty-basedrewards,motivatedbyinformationgain,areusedtoassessthe
questions within the simulation. Finally, a reward propagation scheme is used to compute the
expectedrewardfromaskingeachcandidatequestion, allowingustoselecttheonewithhighest
expectedreward,toasktheAnswerer.
2.3 QuestionGenerationandSimulation
UoTstartsbyusinganLLMtogenerateseveralcandidatequestions,thensimulatesfuturescenarios
foreachone. Thissimulationprocessallowsustomeasurehowmuchinformationwecanexpectto
gaininthenextfewstepsfromeachquestion,andthustochoosethemostsuitablequestion.
QuestionGeneration RecallthatoursettinginvolvessequentialinteractionsbetweenaQuestioner
(e.g., a chatbot) and an Answerer (e.g., a human patient). During the ith interaction step, the
Questionergeneratescandidatequestions,thenselectsoneofthesetoask,denotedasq .
i
Togeneratecandidatequestionstoask, UoTusestwoinputs: (1)thehistoryofpastinteractions
h ={q ,a ,q ,a ,...,q ,a },comprisingthesequenceofpastquestionsandanswers;and
i 1 1 2 2 i−1 i−1
(2)thecurrentpossibilityset Ω . Thesetwoinputsarecombinedtoformapromptthatincludes
i
instructionsexplainingthenatureofthetask(e.g.,howthe20Questionsgameworks),providesthe
currenthistoryh andthecurrentpossibilitysetΩ ,andasksanLLMtogeneratemcandidatenext
i i
questions,conditionedonthepreviouscontext. Thisprompt,denotedasPrompt (h ,Ω ),isfedto
gen i i
3ourgeneratorLLM ,whichthengeneratesmcandidatequestions,denotedq1,q2,...,qm:
gen i i i
q1,q2,...,qm =LLM (Prompt (h ,Ω )) (1)
i i i gen gen i i
MultistepSimulation AsshowninFigure2(a),theQuestionGenerationstagegeneratescandidate
questionssuchasq1 =“Didyouvomit?"Next,duringSimulationstage,foreachsuchgenerated
i
candidatequestion,wesimulatepossiblefuturesforafewsteps,formingatreeofpossibilities. This
processwillenableustocomputerewardsforeachquestion,helpingustodecidewhichquestionto
ask.
Eachnodeofthetreecanbeoneoftwotypes: AnswererNodeswhereitistheAnswerer’sturnto
answeraquestion,andQuestionerNodeswhereitistheQuestioner’sturntoaskaquestion. Atthe
root,aquestionhasjustbeenasked(e.g.,q1),sotherootisanAnswererNode. Next,weexplainhow
i
toconstructtreebyrecursivelyexpanding(or‘branching’)eachnodetoconstructitschildren,i.e.,
startingfromtheroot,thenproceedingtoitschildren,andsoon.
• AteachAnswererNode,aquestionhasjustbeenasked. Next,weneedtofurther‘branch’
the tree based on the possible answers to the current question. Rather than allowing
completelyopen-endedanswers,weinsteadfocusonaffirmativeandnegativeresponses5,
asthisallowsustocomputemeaningfuluncertaintymetrics,aswediscusslater. Hence,we
branchthenodeintotwochildren,correspondingtoaffirmativeandnegativeanswers.
• AteachQuestionerNode,wepromptanLLMtogeneratemquestionsusingthecurrent
historyandpossibilityset,inthesamewayasintheQuestionGenerationstep. Notethat
whilethegenerationprocedureissimilar,thepurposeisdifferent: theQuestionGeneration
stepgeneratescandidatequestionstoselectfrom,whileherewearegeneratingsimulated
questionstoformatreeforthepurposeofevaluatingthecurrentquestion. Theresultingm
generatedquestionsareaddedtothetreeaschildrenofthecurrentnode.
Inthisway,werecursivelygeneratenodesofthetree,stoppingwhenwereachafixednumberof
levels(i.e.,depth).
Whilegeneratingthistree,wealsorecursivelycomputethecurrentpossibilitysetΩ ateachnode
v
v. Specifically,leth bethecurrentconversationhistoryuptonodev,combiningboththeactual
v
conversationhistoryh andthesimulatedconversationuptonodev. Thenthecurrentpossibilityset
i
atthisnode,denotedΩ ,isthesubsetofthepossibilityspaceconsistentwithh . Attheroot,the
v v
currentpossibilitysetisonlylimitedbytheactualconversationhistory,i.e.,Ω . Then,asweproceed
i
overthesimulatedtree,notethatthecurrentpossibilitysetonlychangesatAnswerernodes,when
ananswerisaddedtothecurrenthistory. Hence,ateachAnswerernodev,wepromptanewLLM
(an‘AnswererSimulator’LLM ),todeterminethefurthersubsetΩA ⊆Ω forwhichtheanswer
ans v v
tothecurrentquestionisaffirmative,andthecorrespondingΩN =Ω \ΩA forwhichtheanswer
v v v
isnegative.6 Thisallowsustorecursivelycomputethepossibilitysetsofthechildrenofv(which
themselvescorrespondtotheaffirmativeandnegativeanswers).
ΩA,ΩN =LLM (Prompt (h ,Ω )) (2)
v v ans ans v v
Inthisway,wecanrecursivelycomputethepossibilitysetoneachnodeofthetree.
2.4 Uncertainty-BasedRewardCalculation
To develop suitable information-seeking approaches, a critical question is how to evaluate the
effectivenessofaquestion,i.e.,itscontributiontoreducinguncertainty. Toaddressthis,weturn
toinformationtheory,specificallytheconceptofinformationgain,whichmeasurestheamountby
whichuncertaintydecreasesafteraparticularobservation. Torewardinformation-seekingbehavior,
weassignrewardstoquestionsbasedonhowmuchtheyreducethemodel’suncertaintyaboutthe
unknownrandomvariable. TheserewardsignalsareusedbyourUoTframeworktodeterminewhich
questiontoselect,tomaximizethereductionofuncertainty.
5AsshownFigure2(a),forquestion‘Didyouvomit?’,possibleaffirmativeresponsesinclude‘yes’or‘I
alreadyvomitedtwice’,whilenegativeresponsescouldbe‘no’or‘Idon’thave’.
6In practice, allowing overlap between ΩA and ΩN may be more realistic. However, in this work, we
v v
consideronlythesimplifiedscenariowheretheyaredisjoint.
4Entropy. Entropyandinformationgainarewell-knownconceptsininformationtheoryShannon
(1948). Inourwork,weusetheseconceptstomeasurehowmuchinformationisgained(orequiva-
lently,howmuchuncertaintyisreduced)byaskingaquestion,toformulateourrewards. Entropy
measuresthelevelofuncertaintyinarandomvariable: higherentropyindicatesgreateruncertainty.
TheentropyofadiscreterandomvariableX takingvaluesx ,...,x is:
1 n
n
(cid:88)
H(X)=− p(x )logp(x ) (3)
i i
i=1
Since our goal is to reduce the uncertainty in the unknown ω ∈ Ω, we use entropy to measure
thisuncertainty. Formally,letΩ={ω ,··· ,ω },andwedefineanadditionalsetofarbitraryreal
1 n
numbersX ={x ,··· ,x }⊆Rwhichwewillassociatewitheachofthesepossibilities. Definea
1 n
randomvariableX :Ω→X suchthatX(ω )=x . Intuitively,X isadiscreterandomvariablethat
i i
takesthevaluex iftheithpossibilityistrue,i.e.,ifω = ω . X servestocaptureouruncertainty
i i
about ω, since observing X is equivalent to observing the true option ω. As a simple example,
supposeourpossibilityspaceisΩ={ω ,ω ,ω };weaccompanythesewithrealnumbersx ,x ,x ,
1 2 3 1 2 3
andhaveadistributionforourrandomvariableX reflectingpriorbeliefsoverthesepossibilities:
e.g.,p(x ) = 0.2, p(x ) = 0.3, p(x ) = 0.5. Conceptually,ourframeworkallowsforanyprior
1 2 3
probability distribution over the possibilities (i.e., p(x )), but in our experiments, we assume a
i
uniformdistributionoverthemduetothelackofaninformativeprior.
Beforeaskinganyquestions,ouruncertaintyabouttheunknownωisgivenbyH(X),asinEq. (3).
Atanynodevofthetreesdescribedintheprevioussection,recallthatwehaveaconversationhistory
h whichcontainssomeanswersgivenbytheAnswerer. Thishistorylimitsthecurrentpossibility
v
settothoseinΩ ⊆Ω,therebyreducingouruncertainty. Wemodelthisusingthestandardnotionof
v
conditionalprobabilityonanevent: sinceΩ ⊆Ω,thusΩ isaneventwhichwecanconditionon:
v v
p(x |Ω )=p(x )/p(Ω ) ∀isuchthatω ∈Ω (4)
i v i v i v
wherep(Ω )isthesumofprobabilitiesoftheelementsinΩ . Toillustrate,wecontinuefromthe
v v
earlier example, where p(x ) = 0.2, p(x ) = 0.3, p(x ) = 0.5. If the conversation history h
1 2 3 v
at node v is only consistent with x and x , i.e., Ω = {ω ,ω }, we can adjust the probability
1 2 v 1 2
distributionbyconditioning:e.g.,theadjustedprobabilityofx isp(x )/p(Ω )=0.2/(0.2+0.3)=
1 1 v
0.4.
Next,toquantifytheuncertaintyatnodev,notethatsinceX isconditionallydistributedbasedon
p(·|Ω ),theentropyofthisdistributionis:
v
(cid:88)
H (X):= p(x |Ω )logp(x |Ω ) (5)
v i v i v
i:ωi∈Ωv
Intuitively,H (X)istheremaininguncertaintyinX atnodev(i.e.,afterobservingthehistoryh ).
v v
InformationGainataNode Wenowquantifytheuncertaintyreductionwhenreceivinganswers
atanAnswerernodev. Recallthattheanswergivenatv partitionsΩ intotwodisjointsubsets:
v
Ω = ΩA ∪ΩN, where ΩA and ΩN are the subsets of possibilities resulting in affirmative and
v v v v v
negativeanswersrespectivelytothelastaskedquestion. Givenanaffirmativeanswer,theremaining
entropybecomes:
(cid:88)
HA(X):= p(x |ΩA)logp(x |ΩA) (6)
v i v i v
i:ωi∈ΩA
v
We define HN(X) analogously for negative answers. Let pA = p(ΩA)/p(Ω ) and pN =
v v v v v
p(ΩN)/p(Ω ) be the conditional probabilities of affirmative and negative answers at node v. To
v v
computetheexpectedentropyafterreceivingtheansweratnodev,sincewehaveapAprobabilityof
v
receivinganaffirmativeanswerandpN ofanegativeanswer,theexpectedentropyis:
v
pA·HA(X)+pN ·HN(X) (7)
v v v v
As such, the expected information gain at node v is the difference in entropies before and after
receivingtheanswer:
IG (X):=H (X)−pA·HA(X)−pN ·HN(X) (8)
v v v v v v
5Wecansimplifythis: asproveninAppendixA,theaboveequationreducesto:
IG (X)=−pAlogpA−pNlogpN (9)
v v v v v
ThisrepresentstheexpectedreductionofuncertaintyinX whenreceivinganansweratnodev. Note
thatithasanentropy-likeexpression,andisthereforenonnegative.
RewardFormulation AnaturalapproachwouldbetodefinetherewardfunctionR (v)atnode
u
vastheinformationgainIG (X): thatis,therewardfromthequestionatnodevistheexpected
v
informationgainIG (X)fromreceivingitsanswer. Inpractice,wefindthataslightlymodified
v
functionI(cid:102)G v(X)ispreferable. Inparticular, wefindthatIG v(X)doesnotresultinsufficiently
sharpdifferencesinrewardoverthetypicalrangesweencounter. Hence,weintroduceanadditional
hyperparameterλ≥0whichhelpstosharpentherewardsusingascalingapproach:
−pAlogpA−pNlogpN
R u(v)=I(cid:102)G v(X):= 1v +λ−v 1|pA−v
pN|
v (10)
v v
ThisdefinitionensuresthatR (v)fallswithintherange[0,1],providinganormalizedandconsistent
u
rewardtomeasureuncertaintyreduction. Therewardfunctionreachesitsmaximumwhenthesubsets
ΩA and ΩN have equal probability, reflecting the maximum reduction in uncertainty. It reaches
v v
itsminimumwhenoneofthesubsetshaszeroprobability,indicatingnoreductioninuncertainty.
AppendixDplotstherewardfunctioncurveacrossvaluesofpAandpN.
v v
2.5 QuestionSelectionViaRewardPropagation
Single-step rewards often fall short in dynamic settings as they only consider immediate impact,
overlookinglong-termeffects. Toovercomethis,ourmethodemploysarewardpropagationscheme
oversimulationtrees. Specifically,wewilldefine‘accumulatedrewards’whichaccumulaterewards
overmultiplestepsofthesimulations,capturingtheeffectivenessofpastdecisions. Theseareusedto
calculate’expectedrewards,’indicatingtheexpectedinformationfromthequestions. Therewards
thenguidetheselectionofcandidatequestions.
AccumulatedReward Wefirstdefinetheaccumulatedrewardateachnodev,whichaccumulates
therewardsatvandallitsancestorsonthetree,definedrecursivelyas:
(cid:26)
0 visroot
R (v):=R (v)+
a u R (Parent(v)) otherwise
a
HereR (v)istheuncertainty-basedrewardatnodevdefinedinEq. (10),andR (Parent(v))isthe
u a
accumulatedrewardoftheparentofv. Wecomputetheseaccumulatedrewardsbystartingattheroot
andpropagatingdowntotheleaves. Intuitively,theaccumulatedrewardateachleafnoderepresents
thetotalrewardweendupwithattheendoftheconversationatthatnode.
ExpectedReward Next,wecomputetheexpectedrewardforeachnodeR (v),whichrepresents
e
theexpectedtotalvalueofrewardsreceivedonexpectationonanodeandallitsdescendantsontree:

R a(v) ifvisaleaf;otherwise:
R (v):= pAR (vA)+pNR (vN) ifvisanAnswererNode
e v e v e
1 (cid:80)m R (w) ifvisaQuestionerNode
m w∈Children(v) e
ForthecasewherevisanAnswererNode,recallthatpA andpN aretheconditionalprobabilities
v v
ofaffirmativeandnegativeanswersatnodev,definedinsection2.4. vA andvN areitschildren,
correspondingtotheaffirmativeandnegativeanswers. ForthecasewherevisaQuestionerNode,
weassignequalprobabilitytothemquestionsaskedfromthisnode. Inthisway,wepropagatethe
expectedrewardsfromtheleavesuptotheroot,allowingustocomputetheexpectedgainattheroot
(i.e.,theexpectedrewardforthatcandidatequestion).
DeterminingtheOptimalQuestion Finally,todecidethequestiontoask,weselectthequestion
withhighestexpectedreward(andtherefore,thehighestexpectedinformationgain,consideringboth
immediateandfutureinformationgains):
q =argmaxR (qn) (11)
i e i
n=1
62.6 UoTSummary
UoTfirstgeneratescandidatequestionsq1,q2,...,qmbasedonthehistoryh andcurrentpossibility
i i i i
setΩ . Then, weconductmultistepsimulationtogenerateatreeforeachcandidatequestionqn.
i i
Next,wecomputetheuncertainty-basedrewardsR (v),andpropagateoverthetreestocompute
u
accumulatedrewardR (v)andexpectedrewardR (v). Lastly,theoptimalquestionqnwithhighest
a e i
expectedrewardwillbeselectedasq tointeractwiththeAnswerer.
i
2.7 ExtensionsandDiscussion
PrunedUoT.Toenhanceefficiencyduringsimulation,pruningakintoBeamSearchcanbeemployed
whenconstructingthesimulationtrees,whichlimitsthenumberofpathstoexploreoverthetreetoa
predeterminedsize.
OpenSetUoT.Recallthatintheclosedsetscenario,theQuestionerstartswithknowledgeofthe
possibilityspaceΩ. Inpractice, thepossibilityspaceisoftenunknown, resultingintheopenset
setting. ToadaptUoTtothiscase,weprompttheQuestionertoinitializethepossibilityspaceΩand
thenreinitializethepossibilitysetΩ accordingtocurrenthistoryh . Followingthis,therestofUoT
i i
proceedsunchanged.
3 Experiments
3.1 ExperimentalSetup
Models WetestvariousLLMstoevaluatethegeneralityofourmethod,includingLlama2-70B-
ChatTouvronetal.(2023),CohereCohere(2023),PaLM2Aniletal.(2023),Claude2Anthropic
(2023),GPT-3.5-turboOpenAI(2023a)andGPT-4OpenAI(2023b).
Baselines DirectPromptinginOpen-Set(DPOS)promptsanLLMdirectlytogeneratethenext
response, without using information about the possibility set. Direct Prompting in Closed-Set
(DPCS)Atthestartoftheinteraction,theLLMisprovidedwithfullinformationaboutthepossibility
set. The subsequent baselines also follow this setting. Planning Prompting (PP) is motivated
byWangetal.(2023). WeleverageanotherLLMtoplanthefutureand,consequently,determine
thequestiontoask. Chain-of-Thought(CoT)Weietal.(2022)includesaseriesofintermediate
reasoningsteps,whichsignificantlyimprovestheabilityofLLMstoperformcomplexreasoning.
CoT-SC(Self-Consistency)Wangetal.(2022)isanensembleapproachthatsamplesavarietyof
reasoningpathsratherthanjustthemostobviousone. Wesetaspecificsamplingcounttofairly
compareCoT-SCwithotherapproachesincomputationalcost. ReflexionShinnetal.(2023)enables
theagenttoproposeanaction,andusesself-evaluation-basedrewardstoassesswhethertheagent
shouldreflecttodevelopanewidea. Tree-of-Thoughts(ToT)Yaoetal.(2023)enablesLMstomake
decisionsbyexploringandevaluatingmultiplereasoningpathsoveratreestructure. Weexamine
ToTundertwosetups: Original-ToT,whichusesthestandardapproachofgeneratingandevaluating
questions, and Adapted-ToT, where we integrate heuristic experience into prompt for question
generationandevaluation,focusingonquestionsthathalvethesearchspace. Wematchedthetree
depthtothesimulationstepsinourUoTmethodforafaircomparison. Hyperparametersanddetailed
experimentalsettingsareinAppendixF.1,andpromptsinAppendixH.
ScenariosandDatasets 20Questionsisaclassicgamewheretheanswererthinksofawordor
item,andthequestionerasksupto20yes-or-noquestionstoguessit. Weusetwodistinctdatasets,
namely20Questions(20Q)inBIG-benchSrivastavaetal.(2022),andCommon(collectedbyus,
refertoAppendixF.2formoredetails),including29and111itemsseparately. Inthisscenario,the
maximalturnsissetto20. InMedicalDiagnosis,thedoctorneedstoaskquestionstopatientsabout
theirsymptoms,todetermineanaccuratediagnosis. Weusetwodatasets: DXXuetal.(2019),with
104doctor-patientdialoguesand5diseasesintestset,andMedDGLiuetal.(2022),withover17K
conversationsand15typediseases,fromwhichwemanuallyselect350high-qualitytestsamples
forevaluation. Bothdatasetshaveamaximumof5turns. Troubleshootingisascenariowherea
customersupporttechnicianinteractswithcustomerstoidentifyandresolvefaultsorissueswithin
computersystems,electronicdevices,machinery,orothercomplexsystems. Raghuetal.(2021)
introduceFloDialwith894dialogues,containing153faults. Weevaluateusingamaximumof20
turns. TheanswererissimulatedbyGPT-4,promptedwiththegroundtruthinformationforacase
(e.g., thepatient’s groundtruthdisease, self-report, and conversation). Formoredetails, refer to
AppendixF.2. WeprovideexamplesofthesescenariosinAppendixG.
7Table1: Resultsfromthreedifferentscenarios,assessingSuccessRate(SR),MeanConversation
LengthinSuccessfulCases(MSC),andMeanConversationLength(MCL).
20Questions MedicalDiagnosis Troubleshooting
Model Method 20QinBIG-bench Common DX MedDG FloDial
SR↑ MSC↓ MCL↓ SR↑ MSC↓ MCL↓ SR↑ MSC↓ MCL↓ SR↑ MSC↓ MCL↓ SR↑ MSC↓ MCL↓
DPOS 6.9 12.0 19.5 1.80 11.0 19.8 13.4 3.1 4.8 23.7 3.4 4.6 11.1 15.1 19.5
Llama2-70B DPCS 17.2 13.5 18.9 6.31 12.0 19.7 29.8 3.0 4.4 28.0 3.5 4.6 24.2 14.5 18.7
UoT 20.7 13.2 18.6 10.8 15.6 19.5 51.9 1.8 3.4 33.9 1.4 3.8 31.4 15.8 18.7
DPOS 3.45 15.0 19.8 1.80 14.0 19.9 19.8 3.7 4.7 25.0 3.6 4.7 16.3 16.7 19.5
Cohere DPCS 6.90 12.0 19.4 1.80 12.5 19.8 35.6 3.3 4.4 33.3 4.0 4.7 27.5 16.3 19.0
UoT 34.3 8.50 16.0 16.2 11.7 18.6 45.5 2.6 3.9 75.7 2.7 3.3 41.4 8.7 15.3
DPOS 37.9 13.5 17.5 35.1 14.4 18.0 7.69 3.9 4.9 11.3 4.0 4.9 22.6 15.2 19.0
PaLM2 DPCS 51.7 13.2 16.5 53.1 13.9 16.8 7.92 3.4 4.9 34.0 4.4 4.8 30.1 15.0 18.5
UoT 72.4 7.0 10.6 62.1 12.5 15.3 75.0 2.1 2.8 80.7 2.2 2.7 48.4 7.6 14.0
DPOS 48.3 9.8 15.1 29.7 13.8 18.2 45.2 3.0 4.1 60.7 4.1 4.5 39.7 14.3 17.7
Claude2 DPCS 72.4 11.6 13.9 43.2 13.8 17.3 97.1 2.4 2.5 83.0 4.3 4.4 42.9 15.7 18.2
UoT 75.9 5.1 8.69 61.3 9.8 13.7 98.0 2.3 2.4 88.3 2.7 2.9 52.6 6.3 12.8
DPOS 36.0 12.6 17.3 32.6 14.6 18.2 18.8 3.5 4.7 25.0 3.5 4.6 19.4 12.3 18.5
GPT-3.5 DPCS 44.8 13.2 17.0 40.0 14.8 17.8 49.5 2.7 3.3 42.3 3.8 4.5 22.6 13.3 18.5
UoT 51.7 5.3 12.4 44.2 10.9 16.0 92.1 2.1 2.4 81.3 2.4 2.9 67.1 6.9 11.2
DPOS 37.9 14.0 17.7 48.6 14.0 17.1 44.2 3.5 4.9 45.7 4.2 4.6 38.4 13.0 17.3
DPCS 48.3 11.1 15.7 50.5 13.1 16.5 91.3 3.0 3.3 72.3 4.2 4.4 43.7 13.4 17.1
PP 58.6 14.0 16.5 38.7 14.9 18.0 58.6 2.5 3.5 62.3 3.8 4.3 39.2 14.2 17.7
CoT 27.6 13.1 18.1 20.7 16.0 19.2 33.7 3.7 4.4 20.0 3.8 4.3 32.8 10.1 16.8
CoT-SC 37.9 13.5 17.6 55.1 14.0 16.7 48.5 3.6 4.3 26.7 4.2 4.8 42.5 11.0 16.2
GPT-4
Reflexion 67.9 9.80 13.1 67.6 12.0 14.6 52.5 3.7 4.3 30.3 4.0 4.7 28.6 11.5 17.8
Original-ToT 34.5 14.3 18.1 28.8 15.5 18.7 70.3 3.3 3.8 60.3 3.2 3.9 40.4 11.6 16.6
Adapted-ToT 41.4 10.5 16.1 42.6 12.2 16.1 92.1 1.9 2.2 78.0 3.0 3.4 60.3 8.2 12.9
PrunedUoT 65.5 8.7 12.6 62.2 10.8 14.3 92.1 1.9 2.1 83.3 2.7 3.1 63.2 8.2 12.5
UoT 79.3 7.0 9.7 71.2 10.8 13.5 97.0 2.0 2.1 88.0 2.6 2.9 67.3 7.8 11.8
EvaluationMetrics Tomeasureefficacyandefficiency,weuse: SuccessRate(%): SR=S/T,
whereS isthenumberofsuccessfulcases,andT isthetotalnumberofcases;MeanConversation
LengthinSuccessfulCases: MSC=R /S,whereR isthetotalroundsinsuccessfulcases;Mean
s s
Conversation Length: MCL = R/T, where R is the total rounds in all cases. MCL measures
efficiencybasedontheresourcesusedinbothsuccessesandfailures.
3.2 Performance
20Questions AsillustratedinTable1,foralltypesofLLMs,thoseequippedwithUoToutperform
thebaselines inbothDPOSand DPCSsettings. Among themethodsused onGPT-4toenhance
planning and reasoning, CoT and PP show inferior performance even compared to GPT-4 alone.
UoTachievesthehighestsuccessrate,surpassingthesecond-bestReflexionbyanaverageof7.5%.
Moreover,itdemonstratessuperiorefficiency,withanaverageof2fewerroundsthanReflexionin
successfulcases.
Medical Diagnosis UoT outperforms baselines in simplified medical diagnostics, achieving a
97.0%successrateontheDXdatasetwithGPT-4. OntheMedDGdataset,UoTonPaLM2and
GPT-4achievesuccessratesof80.7%and88.0%. UoTsignificantlyreducesconversationlength,
withaverageMSCof2.0onGPT-4forDX,lowerthan3.5and3.0forDPOSandDPCSmethods.
Troubleshooting UoTsimilarlyachievesthehighestSRof67.3%,andthelowestMSCof7.8.
WhenequippedwithUoT,GPT-3.5showsaremarkableimprovementinSRfrom22.6%to67.1%in
theclosedsetsetting.
OverallPerformance Onaverage,UoTenhancesthesuccessrateby57.8%comparedtoDPCS
across5datasetsand6differentLLMs,includingopensourceandcommercialmodels. Notably,
successrateincreases102.8%forCohere. Furthermore,UoToutperformsCoT-SCby38.4%and
Reflexionby31.2%. EvencomparedtotreestructuremethodslikeOriginal-ToTandAdapted-ToT,
UoTstillshowssuperiorperformancewithgainsof33.7%and17.7%respectively. Additionally,
PrunedUoT,ourpruningmethodtoimproveefficiency,outperformsAdapted-ToTby10.4%.
3.3 Analysis
3.3.1 ComparingModelPerformanceatEqualComputationalEfficiency
Here,wecompareperformanceacrossapproacheswithsimilarcomputationalcost,intermsoftoken
consumption. Todoso,wefirstpruneourUoTasdescribedinsection2.7. Secondly,weexpandthe
explorationdepthofAdapted-ToTmethodtobringitstokencostinlinewiththatofUoT.
8Table 2: Average success rates for 20Q, MD, and TB
AsshowninthetophalfofTable2,the datasetsatcomparableefficiency,measuredbyGPT-4
PrunedUoTmodel,despiteitsreduced tokenuse. kissamplingcount,Dtreedepth.
efficacy compared to UoT, still outper- Method Tokens 20Q MD TB
formsToTandothermethods. Also,the
CoT-SC(k=33) 4.6k 46.5 37.6 42.5
bottompartofTable2showsthateven
Orig-ToT(D=3) 4.5k 31.7 65.3 40.4
when increasing the depth of Adapted
Adapt-ToT(D=3) 4.5k 37.4 85.1 60.3
ToT (Adapted-ToT (D = 4)) to match
PrunedUoT(D=3) 4.7k 64.8 88.4 63.2
thetokencostof UoT(D = 3), itstill
underperformscomparedtoUoT. Adapt-ToT(D=4) 9.3k 54.4 86.7 63.7
UoT(D=3) 9.2k 75.3 92.5 66.0
3.3.2 EffectivenessofUncertaintyRewards
Tofurtherdemonstratetheeffectivenessofouruncertainty-basedreward,wecompareitwiththe
self-evaluationrewardusedintheoriginalToTbasedonGPT-4model.Weimplementtheuncertainty-
basedrewardinplaceoftheself-evaluationrewardinToT,creatingavariantwecallToT(+UR).The
results,asshowninleftsideofFigure3,indicatethatourrewardsignificantlyenhancesplanning
efficacybyanaverageof9.3%. Additionally,weusetheheuristicself-evaluationrewardinAdapted-
ToTtoreplace ourcurrent uncertainty-basedrewardin UoT, avariantwerefer toas UoT(-UR).
ThischangeresultsinaperformancedecreaseshownintherightpartofFigure3,furthervalidating
theeffectivenessofouruncertainty-basedreward. Moreover,theperformanceofUoT(-UR)still
surpassesthatofAdapted-ToTillustratedinTable1,
ToT ToT (+UR) UoT (-UR) UoT
90 85.185.6 100 89.091.7 Table3: AveragedSuccessratecompari-
80 90
67 00 63.8 60.366.0 78 00 70.074.8 66.067.3 sonoverdatasetsonopensetsettingby
45 00 42.0 456 000 usingGPT-4in20Q,MD,andTBtasks.
30 30
20 20
10 10 Method 20Q MD TB
0 0
20Q MD TB 20Q MD TB
DPOS 43.3 45.0 38.4
Figure3: SuccessratecomparisonbetweenAdapted-
OpensetUoT 46.0 60.9 42.5
ToT and Adapted-ToT using uncertainty reward, and
betweenUoTandUoTwithoutuncertaintyreward.
3.3.3 CaseStudies
AsdemonstratedinFigure4,comparedtodirectprompting,UoTasksquestionsthatbetterreduce
uncertainty and narrow down candidates, rather than overly specific questions that yield limited
information. Moreover,onceourmethodacquiresinitialinformation(e.g.,stomachpain),itgenerates
moretargetedquestions(relatedtodigestiveissues),insteadofgeneralinquiries.
Direct Prompting UoT Direct Prompting UoT
Q1:IsXalivingthing? Q1:IsXaman-madeobject? Self-Report: Stomachache. Pain in the upper abdominal area. No diarrhea. (Female, 27 years old)
A1:Yes,Xisalivingthing. A1:No,Xisnotaman-madeobject.
… Q A8 8: :I Ns oX ,Xlar isge nr ot th la an rga erhu thm anan a? human. Q A2 2: :I Ys eX s,a Xn ia sn aim na al n? imal. Q v Ao1 1m :: i NH ti ona ,gv I?e hy ao vu en'e tx ep xe pr eie rin ec ne cd edan any yn na au us se ea ao or r Q g esn1 pa: ew cD ii ano llg yy oso neu n anse a ex t mp ioe pnr ti ye i sn n tc oe myo aa cu hrb ?u sr tn oi mng aco hr ,
Q3:IsXacarnivore? vomiting.Thepainstartedaroundsix A1: Yes, I do have this sensation,
Q9:IsXfoundinNorthAmerica? A3:Yes,Xisacarnivore. o'clock…. especially on an empty stomach.
A9:No,XisnotfoundinNorthAmerica. Q4:IsXamammal? Q2:Doesthepainradiatetoanyotherpartof Q2:Haveyounoticedanydark,tarry
Q10:IsXfoundinAfrica? A4:Yes,Xisamammal. yourbody,suchasyourbackorshoulder? stoolsorvomitthatlookslikecoffee
A Q A … Q A1 1 1 1 10 1 41 4: : : : :N I Y I Ys so e eX Xs s, , ,X f aX Xo li u es i in s son d pf ao o ait tu rn ydf n po A ?d eu sn i oi nad f? A li en s oiA pa. afr ri dc .a. Q a A … Qs5 5 9so : :: c N II sis a o Xt ,eX Xd aw lia esi ot n pm h o atwa rdm aa s ?tm se ora ? cl iatt eh dat wi is tht wyp ati ec ra .lly A p a … Q pba a2 5r d r: :t to iN o cm D ufo e lom, an rt ,y y lh yr oe b ig u fop ahd ta t e tyi yxan . b p oId o et r'o v rs gie e ej rs u enmn s ac' tt sy e yir bna p fed ot alhi ol ia y ne dte sbm a ?ut fio td tt eoda rnln e .y eo ao f tit m nhe gyr , g A b A ur u l2 3 co t: : e u I rI Yn ? hd h oas a uv? v e ae n rn ' et' t v pn o ro omt bi ic ate e bd d ly .th ee x pc eo rl io er n co if n m gy G s at so tro il c,
A9:Yes,Xisatypeofleopard. A5:No,Idon'tfeelanypainaftereating.But
Q A1 15 5: :I Ys oX ua gus en so sw edle ito .p Xar id s? 'Snowleopard'. Q10:IsXasnowleopard? Istartedfeelingpainaround6o'clock….
A10:Youguessedit.Xis'Snowleopard'.
Figure4: Casestudiesfromthe20Questionsgame(left)andsimplifiedmedicaldiagnosis(right).
93.3.4 PerformanceofOpenSetUoT
TofurtherassessthepracticalityofUoT,wetestitintheopensetsettingandcompareitsperformance
withDPOS.AsshowninTable3,UoTenhancesDPOSbyanaveragerelativeimprovementof17.4%.
3.3.5 FurtherAnalysis
OurstudyfindsthattheUoT’sone-stepplanningperformswell,thankstoeffectiverewarddesign
and question selection. Although deeper planning improves performance, we cap simulations at
three steps for budget reasons, striking a balance between efficiency and effectiveness. We also
assessGPT-4’saccuracyasananswererbyanalyzing10%ofinteractionsfromeachdataset. GPT-4
consistentlyprovidesaccurateandreliableresponses. Formoredetailsandquantitativeresults,see
AppendixBandC.
4 RelatedWork
PlanningandReasoningofLLMs LLMsshowprowessinplanningandreasoning. Weietal.
(2022) introduced CoT prompting for intermediate reasoning; Yao et al. (2023) proposed ToT
promptingusingDFS/BFS.Bestaetal.(2023)presentGoTtosolveelaborateproblems. Fengetal.
(2023)illustratedTS-LLM’stree-searchguideddecoding.ReActYaoetal.(2022)offersacting-based
prompting,whileReflexionShinnetal.(2023)enhancesthiswithfeedbackreflection. Zhouetal.
(2023)unifyreasoningandplanning.
Decision-makingandInformation-seekingbyLLMs LLMshaveevolvedasdecision-making
tools,withmodelslikeLLM+PLiuetal.(2023)andLLM-DPDaganetal.(2023)combiningexternal
plannersandLLMsfornaturallanguage-basedprogramming. RAPHaoetal.(2023)goesbeyond
structuredlanguage,usingLLMswithMonteCarloTreeSearch(MCTS)Chaslotetal.(2008)for
dynamicdecision-making. ThisapproachisalsoseenintheworkofZhaoetal.(2023),applying
MCTS and LLM knowledge for complex tasks like robot control. However, MCTS struggles in
uncertainscenariosduetoitsrelianceonterminalstatesandspecificmodulesforrewardsandaction
selection. Additionally,toenhanceLLMs’questioningabilities,Dengetal.(2023)introducethe
RephraseandRespondmethod. AVISHuetal.(2023)representsanautonomousvisualquestion
answeringsystemthatusesexternaltools. Panetal.(2023)introduceKwaiAgentsforprocessing
queries,followingguidelines,andaccessingexternaldocuments.
5 ConclusionandDiscussion
ThispaperpresentstheUncertaintyofThoughts(UoT)algorithm,significantlyimprovingLLMsin
tasksrequiringactiveinformationseekingthroughtree-basedsimulation,uncertainty-basedrewards
andarewardpropagationscheme. OnfivedatasetsUoTincreasessuccessrateby57.8%onaverage,
establishinganewbenchmarkforevaluatingLLMsinactiveinformation-seekingtasks. Weevaluate
UoT on simplified scenarios; more realistic scenarios raise challenges like allowing incomplete
eliminationofpossibilitiesbyanswers,andissueswithcompletelyopen-endedquestions/answers,
whichweleaveforfuturework. WefurtherdiscusstheselimitationsandfutureworkinAppendixE.
10References
RohanAnil, AndrewM.Dai, OrhanFirat, MelvinJohnson, DmitryLepikhin, AlexandrePassos,
Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu, Jonathan H. Clark,
LaurentElShafey,YanpingHuang,KathyMeier-Hellstern,GauravMishra,EricaMoreira,Mark
Omernick,KevinRobinson,SebastianRuder,YiTay,KefanXiao,YuanzhongXu,YujingZhang,
GustavoHernandezAbrego,JunwhanAhn,JacobAustin,PaulBarham,JanBotha,JamesBradbury,
SiddharthaBrahma,KevinBrooks,MicheleCatasta,YongCheng,ColinCherry,ChristopherA.
Choquette-Choo,AakankshaChowdhery,ClémentCrepy,ShachiDave,MostafaDehghani,Sunipa
Dev, Jacob Devlin, Mark Díaz, Nan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng, Vlad
Fienber, Markus Freitag, Xavier Garcia, Sebastian Gehrmann, Lucas Gonzalez, Guy Gur-Ari,
StevenHand,HadiHashemi,LeHou,JoshuaHowland,AndreaHu,JeffreyHui,JeremyHurwitz,
MichaelIsard,AbeIttycheriah,MatthewJagielski,WenhaoJia,KathleenKenealy,MaximKrikun,
SnehaKudugunta,ChangLan,KatherineLee,BenjaminLee,EricLi,MusicLi,WeiLi,YaGuang
Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick Liu, Marcello Maggioni,
Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John
Nham, EricNi, AndrewNystrom, AliciaParrish, MariePellat, MartinPolacek, AlexPolozov,
ReinerPope,SiyuanQiao,EmilyReif,BryanRichter,ParkerRiley,AlexCastroRos,AurkoRoy,
BrennanSaeta,RajkumarSamuel,ReneeShelby,AmbroseSlone,DanielSmilkov,DavidR.So,
DanielSohn,SimonTokumine,DashaValter,VijayVasudevan,KiranVodrahalli,XuezhiWang,
PidongWang,ZiruiWang,TaoWang,JohnWieting,YuhuaiWu,KelvinXu,YunhanXu,Linting
Xue,PengchengYin,JiahuiYu,QiaoZhang,StevenZheng,CeZheng,WeikangZhou,Denny
Zhou,SlavPetrov,andYonghuiWu. Palm2technicalreport. arXivpreprintarXiv:2305.10403,
2023.
Anthropic. Claude2,2023. URLhttps://www.anthropic.com/index/claude-2.
Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Lukas Gianinazzi, Joanna
Gajda, Tomasz Lehmann, Michal Podstawski, Hubert Niewiadomski, Piotr Nyczyk, et al.
Graph of thoughts: Solving elaborate problems with large language models. arXiv preprint
arXiv:2308.09687,2023.
GuillaumeChaslot,SanderBakkes,IstvanSzita,andPieterSpronck. Monte-carlotreesearch: A
newframeworkforgameai. InProceedingsoftheAAAIConferenceonArtificialIntelligenceand
InteractiveDigitalEntertainment,volume4,pp.216–217,2008.
Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human
evaluations? arXivpreprintarXiv:2305.01937,2023.
Cohere. Cohereforai,2023. URLhttps://cohere.com/.
GautierDagan,FrankKeller,andAlexLascarides. Dynamicplanningwithallm. arXivpreprint
arXiv:2308.06391,2023.
YiheDeng, WeitongZhang, ZixiangChen, andQuanquanGu. Rephraseandrespond: Letlarge
languagemodelsaskbetterquestionsforthemselves. arXivpreprintarXiv:2311.04205,2023.
XidongFeng,ZiyuWan,MuningWen,YingWen,WeinanZhang,andJunWang.Alphazero-liketree-
searchcanguidelargelanguagemodeldecodingandtraining. arXivpreprintarXiv:2309.17179,
2023.
ShiboHao,YiGu,HaodiMa,JoshuaJiahuaHong,ZhenWang,DaisyZheWang,andZhitingHu.
Reasoningwithlanguagemodelisplanningwithworldmodel. arXivpreprintarXiv:2305.14992,
2023.
ZiniuHu,AhmetIscen,ChenSun,Kai-WeiChang,YizhouSun,DavidARoss,CordeliaSchmid,
andAlirezaFathi. Avis: Autonomousvisualinformationseekingwithlargelanguagemodelagent.
InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
BoLiu,YuqianJiang,XiaohanZhang,QiangLiu,ShiqiZhang,JoydeepBiswas,andPeterStone.
Llm+p: Empoweringlargelanguagemodelswithoptimalplanningproficiency. arXivpreprint
arXiv:2304.11477,2023.
11WengeLiu,JianhengTang,YiCheng,WenjieLi,YefengZheng,andXiaodanLiang. Meddg: an
entity-centricmedicalconsultationdatasetforentity-awaremedicaldialoguegeneration. InCCF
InternationalConferenceonNaturalLanguageProcessingandChineseComputing,pp.447–459.
Springer,2022.
YangLiu,DanIter,YichongXu,ShuohangWang,RuochenXu,andChenguangZhu. G-eval: Nlg
evaluationusinggpt-4withbetterhumanalignment,may2023. arXivpreprintarXiv:2303.16634.
DavidNoeverandForrestMcKee. Chatbotsasproblemsolvers: Playingtwentyquestionswithrole
reversals. arXivpreprintarXiv:2301.01743,2023.
OpenAI. Gpt-3.5turbo: Ahigh-performancelanguagemodel,2023a. URLhttps://www.openai.
com/research/gpt-3-5-turbo. Whitepaper.
OpenAI. Gpt-4technicalreport. arXivpreprintarXiv:2303.08774,2023b.
HaojiePan,ZepengZhai,HaoYuan,YaojiaLv,RuijiFu,MingLiu,ZhongyuanWang,andBingQin.
Kwaiagents: Generalizedinformation-seekingagentsystemwithlargelanguagemodels. arXiv
preprintarXiv:2312.04889,2023.
JoonSungPark,JosephO’Brien,CarrieJunCai,MeredithRingelMorris,PercyLiang,andMichaelS
Bernstein. Generativeagents: Interactivesimulacraofhumanbehavior. InProceedingsofthe36th
AnnualACMSymposiumonUserInterfaceSoftwareandTechnology,pp.1–22,2023.
DineshRaghu,ShantanuAgarwal,SachindraJoshi,andMausam. End-to-endlearningofflowchart
groundedtask-orienteddialogs. InProceedingsofthe2021ConferenceonEmpiricalMethods
inNaturalLanguageProcessing,pp.4348–4366,OnlineandPuntaCana,DominicanRepublic,
November2021.AssociationforComputationalLinguistics. doi: 10.18653/v1/2021.emnlp-main.
357. URLhttps://aclanthology.org/2021.emnlp-main.357.
Claude Elwood Shannon. A mathematical theory of communication. The Bell system technical
journal,27(3):379–423,1948.
NoahShinn,BeckLabash,andAshwinGopinath. Reflexion: anautonomousagentwithdynamic
memoryandself-reflection. arXivpreprintarXiv:2303.11366,2023.
AarohiSrivastava,AbhinavRastogi,AbhishekRao,AbuAwalMdShoeb,AbubakarAbid,Adam
Fisch,AdamRBrown,AdamSantoro,AdityaGupta,AdriàGarriga-Alonso,etal. Beyondthe
imitationgame: Quantifyingandextrapolatingthecapabilitiesoflanguagemodels. arXivpreprint
arXiv:2206.04615,2022.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
LeiWang,WanyuXu,YihuaiLan,ZhiqiangHu,YunshiLan,RoyKa-WeiLee,andEe-PengLim.
Plan-and-solveprompting: Improvingzero-shotchain-of-thoughtreasoningbylargelanguage
models. arXivpreprintarXiv:2305.04091,2023.
XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,SharanNarang,AakankshaChowdh-
ery,andDennyZhou. Self-consistencyimproveschainofthoughtreasoninginlanguagemodels.
arXivpreprintarXiv:2203.11171,2022.
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny
Zhou,etal. Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesin
NeuralInformationProcessingSystems,35:24824–24837,2022.
ZhihengXi,WenxiangChen,XinGuo,WeiHe,YiwenDing,BoyangHong,MingZhang,Junzhe
Wang,SenjieJin,EnyuZhou,etal. Theriseandpotentialoflargelanguagemodelbasedagents:
Asurvey. arXivpreprintarXiv:2309.07864,2023.
Lin Xu, Qixian Zhou, Ke Gong, Xiaodan Liang, Jianheng Tang, and Liang Lin. End-to-end
knowledge-routed relational dialogue system for automatic diagnosis. In Proceedings of the
AAAIconferenceonartificialintelligence,volume33,pp.7346–7353,2019.
12ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikNarasimhan,andYuanCao.
React: Synergizingreasoningandactinginlanguagemodels. arXivpreprintarXiv:2210.03629,
2022.
Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik
Narasimhan. Treeofthoughts: Deliberateproblemsolvingwithlargelanguagemodels. arXiv
preprintarXiv:2305.10601,2023.
ZiruiZhao,WeeSunLee,andDavidHsu. Largelanguagemodelsascommonsenseknowledgefor
large-scaletaskplanning. arXivpreprintarXiv:2305.14078,2023.
AndyZhou,KaiYan,MichalShlapentokh-Rothman,HaohanWang,andYu-XiongWang. Language
agent tree search unifies reasoning acting and planning in language models. arXiv preprint
arXiv:2310.04406,2023.
13A DerivationofInformationGainFormula
Recallthattheinformationgainatnodevisdefinedastheexpectedchangeinuncertainty(orentropy)
whenreceivinganansweratthisnode,whichwedefinedas:
IG (X):=H (X)−pY ·HY(X)−pN ·HN(X) (12)
v v v v v v
Wenowshowthat:
Proposition1. Theinformationgainatnodevisequalto:
IG (X)=−pAlogpA−pNlogpN (13)
v v v v v
Proof. Notethatforanyoutcomex ,wehavebytherulesofconditionalprobability:
i
p(x |Ω ) p(x |Ω )
p(x |ΩA)= i v = i v (14)
i v p(ΩA|Ω ) pA
v v v
Nowtheinformationgainis:
IG (X)
v
=H (X)−pA·HA(X)−pN ·HN(X)
v v v v v
(cid:88)
=− p(x |Ω )logp(x |Ω )
i v i v
i:ωi∈Ωv
(cid:88)
+pA p(x |ΩA)logp(x |ΩA)
v i v i v
i:ωi∈ΩA
v
(cid:88)
+pN p(x |ΩN)logp(x |ΩN)
v i v i v
i:ωi∈ΩN
v
(cid:88)
= p(x |ΩA)(logp(x |ΩA)−logp(x |Ω ))
i v i v i v
i:ωi∈ΩA
v
(cid:88)
+ p(x |ΩN)(logp(x |ΩN)−logp(x |Ω )),
i v i v i v
i:ωi∈ΩN
v
wherethelastequalityholdsbypA·p(x |ΩA)=p(x |Ω ),andsimilarlyforpN.Wefurthercompute
v i v i v v
that
(cid:88)
p(x |ΩA)(logp(x |ΩA)−logp(x |Ω ))
i v i v i v
i:ωi∈ΩA
v
=
(cid:88)
p(x
|ΩA)logp(x i|ΩA v)
i v p(x |Ω )
i v
i:ωi∈ΩA
v
(cid:88)
=− p(x |ΩA)logpA
i v v
i:ωi∈ΩA
v
=−pAlogpA
v v
Analogouslytheremainingtermis−pNlogpN. Finallyweconcludethat
v v
IG (X)=−pY logpY −pNlogpN (15)
v v v v v
Infact,thispropositioncanalsobeprovenusingsomepropertiesofinformationtheory,particularly
thedefinitionsofconditionalentropyandmutualinformation.Asthemorecomputationalproofshown
hereisstillrelativelyshortanddoesnotrequiredefiningcertainadditionalprobabilitydistributions,
weprovidethecomputationalproofhereinstead.
14B EffectofSimulationDepth
AsFigure5illustrates,weanalyzetheimpactofsimulationsteps. Evenwithone-stepreasoningand
planning,ourmethodcanstillhaveastrongperformance,furtherindicatingtheeffectivenessofour
rewarddesignandquestionselectionmechanism. Withtheincreaseofthestep,theperformancecan
graduallyrise. However,duetotheconstraintsofcomputationresourcesandOpenAIAPIbudgets,
weonlyexplorethesimulationtothethirdstepandarguethatitcanbethepracticaltradeoffbetween
performanceandefficiency.
95
92.5
90
86.7
85
82.6
80
75.2
75
70
67 67.3
66.1
64.7
65 63.5
60
1 2 3
Depth
20 Questions Medical Diagnosis Troubleshooting
Figure5: ResultscomparisonofdifferentdepthsettingsinUoT.
C ReliabilityofGPT-4astheEnvironment
AstheimpressiveunderstandingabilityofLLMs,previousresearchhasvalidatedtheeffectivenessof
evaluatorsservedbyChatGPTorGPT-4Chiang&Lee(2023);Liuetal.. Consequently,wealso
adoptGPT-4astheenvironmenttoprovidefeedbackonourwork. PromptscanbefoundinAppendix
H.4. ToassesstheaccuracyandreliabilityofemployingGPT-4astheenvironmentsimulator,we
randomlysample10%interactionrecords(includingthefinaljudgmentandintermediatefeedback
fromtheenvironment)fromeachdataset. AsFigure4shows,GPT-4canprovidecompletelyaccurate
judgmentandalsokeepahighlevelofaccuratefeedbackduringtheinteraction. Theseexperimental
resultscanfurthersupporttheeffectivenessofourmethod.
Table4: HumanevaluationresultsfortheaccuracyofenvironmentfeedbackservedbyGPT-4. IF
representtheAccuracyofIntermediateFeedback
Scenario Judgement IF
20Questions 100 93.7
MedicalDiagnosis 100 94.4
Troubleshooting 100 92.9
D RewardFunctionDetailsanditsCurve
RefertoFigure6forthecurveofuncertainty-basedrewardfunction.
E LimitationandFutureWork
In practice, ΩA and ΩN might overlap, as different answers (such as “yes" or “no") may lead to
v v
the exclusion of different sets of possibilities. Another similar limitation is that some questions
15
)%(
etaR
sseccuSUncertainty Reward Function Ru(v) with =0.4
0.7
Ru(v) with =0.4
0.6
0.5
0.4
0.3
0.2
0.1
0.0
0.0 0.2 0.4 0.6 0.8 1.0
pvA
Figure 6: Curve of uncertainty based reward on Eq 10, where pN can be replaced by (1−pA).
v v
ThehorizontalaxispAisconditionalprobabilitiesofaffirmativeatnodev,whichareintroducedin
v
Section§2.4.
oranswersmaynotfullyeliminatecertainpossibilities(e.g.,“Idon’thaveafever"doesnot100%
eliminatethepossibilityofhavingCOVID-19). Furthermore,comparedtocompletelyopen-ended
interactioninmedicaldiagnosisortroubleshooting,ourcurrentbenchmarkrepresentsasimplified
scenario. Intheory,suchcasescouldbehandledusingthemethodofconvertinginteractionsinto
probability estimations and applying some kind of Bayesian update to the probabilities of each
possibility,ratherthanjusteliminatingsomesubset.
F ExperimentalSetups
F.1 BaselinesSetup
Chain-of-Thought(CoT) WeadaptthetypicalCoTpromptwhichinstructLLMtogeneratethe
explanationormotivationfortheproposedquestionfirst,thengivethequestiontoask.
Chain of Thought with Self-Consistency (CoT-SC) To make the method spend comparable
computetoourapproachforafaircomparison,wesampled33timesbeforedecidingoneachaction
withtheLLM’stemperatureof0.7. Thefinalselectedquestionistheonerepeatedmosttimesamong
33samples.
PlanningPrompting TomeasurewhetherLLMs’planningabilitycanbeenhancedthroughsome
crafted prompts like CoT, ToT or Reflexion. We design the prompt to enable LLM to simulate
multipledifferentsetsoffutureinteractionsbetweenquestionerandanswerer,thenletLLMchoose
onemostpromisinginteraction(question)toask.
TreeofThoughts InthecaseofOriginal-ToT,asamplingmethodisemployedtogenerate3
questionsfromeachanswernode,andtheself-evaluationmethodisutilizedforrewardcalculation.
Subsequently, breadth-first search will be used and 10 nodes from each step will be selected for
latersimulation. Additionally,thetemperatureoftheLLMisconfiguredto0.7,consistentwiththe
settingsinoriginalToTpaper. InthecaseofAdapted-ToT,weprovidemoreheuristicalhintsin
prompttogeneratethequestions,e.g. ‘youshouldtrytoproposethequestiontohalvetheprobability
set. Likewise,eachanswernodegenerates3questions,andtheLLMselects10nodeswithhigher
self-evaluationrewardstofurthersimulation. Thesimulationstepsarealso3.
Reflextion ThisapproachinvolvestheLLMagentsuggestingquestionsiterativelyuntilthequestion
rewardexceedsthethresholdof0.7orreachesthemaximumlimitof3questions. Therewardscore
siscalculatedusingtheformulas = min(pA,pN)/max(pA,pN). Thisheuristicisbasedonthe
principleofwhetherthequestioncaneffectivelyhalvetheprobabilityset. Ifacandidatequestion
achieves a score above the threshold, the process of proposing questions is concluded, and that
16
)v(uRquestionisselected. Incaseswherenoquestionmeetsthethreshold,theonewiththehighestscoreis
chosen.
UncertaintyofThoughtsPruned Aftergeneratingthecandidatequestionbasedonthepossibility
setΩ ,wesortedthesequestionnodesbyuncertaintybasedrewardandreservedhalfofthem,serving
i
thepurposeofpruning.Insubsequentstepsofthesimulation,thispruningoperationwillbecontinued.
OthersettingswerethesameasUoT,describedinSection§F.3.
F.2 ScenariosSettingsandDatasets
20Questionsgameisaclassicguessinggamewheretheanswererthinksofanobject,person,place,
orother,andthequestioner,possessingnopriorknowledgeaboutthechosenentity,proceedsto
poseaseriesofupto20yes-or-noquestionstodeterminewhatthesecretitemis. Thequestionsare
designedtonarrowthepossibilitiesandultimatelyguessthesecretitemwithinthe20questions. 20
QuestionsinBIG-bench: Itisthesub-taskofBIG-benchandcanbefoundontheGitHubwebsite7.
CommonDatasetConstruction: Wecameacrossanofficialwebsite8thatintroducesa20Questions
game, whichmentionsthatcommontargetcategoriesinthisgameincludeanimals, places, food,
andobjects. Therefore,weextractedandmanuallyscreenedthetargetsmentionedonthiswebsite,
resultinginadatasetnamed"Common"comprising111targets,eachbelongingtooneofthefour
aforementionedcategories.
MedicalDiagnosis Inthisscenario,thepatientwillsimplydescribetheirsymptomfirstwhichwe
calla‘Self-report’,thendoctoractedbyLLMwillstarttoaskquestionstointeractwithpatientto
determinethedisease.
Troubleshooting In FloDial dataset, tt includes faults of car and laptop. Similar to Medical
Diagnosis, the customer first describes some simple fault symptoms, then the customer support
technicianwillchatwithcustomertofurthercheckthespecificissuesofdevice.
LLMsServeasQuestioner(PatientorCustomer) Insimulatedinteractionsinvolvingquestioner
andanswererscenarios,particularlyformedicaldiagnosisandtroubleshooting,theresponsegiven
byanLMMactinganswererisguidedbyscenarioinstructionsandreal-worlddialogueexamples.
ThisapproachmakestheLM’sresponsesmorehuman-likeandenhancesitsaccuracyindiagnosing
diseasesoridentifyingfaults. While,inagamelike"20Questions,"wheretheobjectiveistoguess
commonitems,theLMMactingastheanswereronlyneedstoprovidesimple’yes’or’no’answers.
Therefore,incorporatingreal-worlddialogueintotheLMM’spromptsforthisgameisnotnecessary.
F.3 Implementation
Empirically,wesettheplan(simulation)stepsas3andthenumberofquestionsduringthesimulation
is3. Thehyperparameterλinuncertainty-basedrewardis0.4.
G ExamplesinScenarios
SomeexamplesofconversationindifferentscenariosareprovidedinTable5-9.
H Prompts
H.1 DPOSandDPCSPrompt
ThespecificpromptsfortheDPOSandDPCSbaselinesineachscenarioareprovidedinTable10-12.
H.2 PlanningPrompt
The specific prompts for Planning Prompt baselines in each scenario are provided in Table 13,
Table14andTable15. Asplanningpromptmethodisclosesetsetting,hencetheprobabilitysetwill
alsobeinformedinthepromptasDPCSprompt. Wedonotrepeatitinthetables.
7https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/twenty_questions
8https://blog.prepscholar.com/20-questions-game
17Table5: Examplefor20Question. QstandsforQuestionerandAstandsforAnswerer.
Target: Glassmicropipette
Conversation
1: Q:IsXatangibleobject? A:Yes.
2: Q:IsXalivingorganism? A:No.
3: Q:IsXahouseholditem? A:No.
4: Q:IsXatypeoftransportation? A:No.
5: Q:IsXusedinconstruction? A:No.
6: Q:IsXawearableitem? A:No.
7: Q:IsXatypeoftoy? A:No.
8: Q:IsXa’Glassmicropipette’?
A:Youguessedit. Xisa’Glassmicropipette’.
Table6: ExampleforMedicalDiagnosis. DstandsforDoctor(Questioner)andPstandsforPatient
(P).
Disease: Allergicrhinitis
Self-report
Hellodoctor,myson’sthroateasilybecomesredandswollenassoonaswintercomes. Hehas
beencoughingandhisthroathasbeenredandswollenforthepasthalfmonth.
Conversation
1: D:Doyouexperiencearunnyorstuffynose? P:Yes.
2: D:Youmayhave’Allergicrhinitis’.
H.3 UoTPrompt
ThedetailedpromptsforourUoTmethodineachscenarioareattachedinTable16,Table17and
Table18.
H.4 QuestionerPrompt
ThespecificprompttoguidetheexaminerinallscenariosisprovidedinTable19, Table20and
Table21.
Table7: ExampleforMedDG.DstandsforDoctor(Questioner)andPstandsforPatient(Answerer).
Disease: Gastritis
Self-report
Recently,IalwaysfeelnauseouswhenIwakeupinthemorning,butduringtheday,everything
seemsfine. (Female,33yearsold)
Conversation
1: D:Haveyoubeenexperiencingvomitingordiarrhearecently?
P:No,Ihaven’t. Ijustfeelnauseousandmystomachisuncomfortable.
2: D:Haveyounoticedanydark,tarrystoolsorvomitthatlookslikecoffeegrounds?
E:No,Ihaven’tnoticedanythinglikethat.
3: D:Areyouexperiencingpregnancy-relatedmorningsickness?
P:No,I’mnotpregnant. It’sjustthatmystomachfeelsuncomfortableandnauseous.
4: D:Youmayhave’Gastritis’.
18Table8: ExampleforMedDG.DstandsforDoctor(Questioner)andPstandsforPatient(Answerer).
Disease: Gastriculcer
Self-report
Stomachache. Painintheupperabdominalarea. Nodiarrhea. (Female,27yearsold)
Conversation
1: D: Do you experience a burning or gnawing sensation in your stomach, especially on an
emptystomach?
P:Yes,Idohavethissensation,especiallyonanemptystomach.
2: D:Haveyounoticedanydark,tarrystoolsorvomitthatlookslikecoffeegrounds?
P:Ihaven’tnoticedthecolorofmystool,butIhaven’tvomited.
3: D:Youmayhave’Gastriculcer’.
Table9: ExampleforTroubleshooting. TstandsforcustomersupporttechnicianandCstandsfor
customer.
Fault: Brakefluidoverflow
Self-report
Lately,Ihavebeenhavingtroublewiththebrakesofmycar. Ihavecheckeditagainandagain
butcan’tfigureoutwhat’scausingthis. Pleasehelpme.
Conversation
2: T:Doestheissueaffecttherotationormovementofthewheels? C:No.
3: T:Doestheissueseemtoberelatedtotheoverallbrakingsystem? C:Yes.
4: T:Areyouexperiencingabrakefluidleak? C:No.
5: T:Areyourbrakepadswornout? C:No.
6: T:Isthebrakepedalfeelingspongyorsoftwhenyoupressit? C:Yes.
7: T:Isthereanissuewiththebrakelinesbeingair-filledordamaged? C:No.
8: T:Yourcarmayhaveabrakefluidoverflowissue.
Table10: DPOSandDPCSPromptfor20Questiongame. Thetwosettingsarethesameexceptfor
thecontentnotedinred,whichisonlyforDPCSPrompt. Thefirsttimethecontentmarkedinredis
mentionedatthebeginningofinteractionistoinformthequestioneroftheentireprobabilityset. The
secondmentionservesasareminderforthequestionertodeterminethetargetassoonaspossible
whentheconversationisnearingitsend. Weestablishtwophaseswiththeaimofencouragingthe
questionertoguessthetargetassoonaspossiblewhenthesecondphaseofinteractionisnearingits
conclusion,inordertoavoidfailureattheendofinteraction.
Prologue
Letusplaythegameof20questions. Iamimpersonatingthething,X.Youwillaskme
upto20questionsthatstartwith’IsX’andcanonlybeansweredbyyesorno,andIwill
answereachonetruthfullybasedonbeingX.
Letusbegin. Askmethefirstquestion. Replytomewithyourfirstquestion.
TheXmaybeoneoftheitemsbelow: [listofitems]
Phase1(Q1-Q13)
None
Phase2(Q14-Q20)
TheXmaybeoneoftheitemsbelow: [listofitems]
19Table11:DPOSandDPCSaremedicaldiagnosticpromptswithsimilarstructures.Thekeydifference
is the inclusion of probability set information in red text for DPCS. This red text appears twice:
initially to inform the questioner about all potential diagnoses and again towards the end of the
interactionasaremindertoquicklyconfirmthedisease. Weestablishtwophaseswiththeaimof
encouraging the questioner to confirm the disease as soon as possible when the second phase of
interactionisnearingitsconclusion,avoidingfailureattheendofinteraction.
Prologue
Youareadoctorandyourpatientreportsthat: [self-report]
You should ask your patient questions in English with symptoms which can only be
answeredbyyesorno,tofindwhatdiseasethispatientsuffers.
Letusbegin. Askmethefirstquestion.
Basedonthesymptomsabove,ifyoufindoutthedisease,pleaseask’Areyoua[disease
name]?’
Thepatientmaysufferfromoneofthediseasesbelow: [listofdiseases]
Phase1(Q1)
None
Phase2(Q3-Q5)
Basedonthesymptomsabove,ifyoufindoutthedisease,pleaseask’Areyoua[disease
name]?’
Thepatientmaysufferfromoneofthediseasesbelow: [listofdiseases]
Table12: DPOSandDPCSaretroubleshootingpromptswithsimilarstructures,butDPCSincludes
unique content highlighted in red. This red content appears first at the beginning, outlining all
potentialfaults,andagaintowardstheendasaremindertoswiftlyidentifythefault. Thetwo-phase
structureofthesepromptsaimstoensurequickfaultconfirmation,especiallyinthefinalstagesofthe
interaction,topreventfailure.
Prologue
Youareatechnicianandyourclientreportsthat: [self-report]
Youshouldaskyourclientquestionsaboutaspecificsituationwhichcanonlybeanswered
byyesorno,inordertofindwheretheissuethisclientfaceswithlocated.
Letusbegin. Askmethefirstquestion.
Theclientmayfaceoneoftheissuesbelow: [listofissues]
Phase1(Q1-Q13)
None
Phase2(Q14-Q20)
Basedonthesituationsabove,ifyoufindouttheissue,pleaseask’Areyoua[issuename]?’
Theclientmayfaceoneoftheissuesbelow: [listofissues]
20Table13: PlanningPromptfor20Questiongame. [C1]isthecountofquestionsaskedand[C2]is
thecountofquestionsremaining. The‘informationgained’markedbluerepresentstheprevious
interactionhistory. Wedivideitintothreephasestodiscusstheprobabilitysetasquicklyaspossible,
conductsimulationforplanning,andremindthequestionertoguesstheanswer.
Prologue
SameaspromptsinAppendixH.1
Phase1(Q1-Q4)
ThenextquestionshouldnarrowdownthepossiblerangeofX,preferablyinhalf.
Phase2(Q5-Q15)
Weareplayingthe20Questiongame,[C1]questionshavebeenasked. Andnowweknow:
[informationgained]
BasedonthefeaturesofXabove,pleaseguesswhatXexactlyisandtellmeyourtop3
mostlikelyanswers.
ForthesethreecandidateX,pleaseseparatelycompletetheremaining[C2]questionsand
answeryes/nobyyourself. Notably,youmustguessthecorrespondingXbeforethelast
question.
Phase3(Q16-Q20)
NotethatyoushouldguesswhatXexactlyisfromnowon. Thequestionmuststartwith
’IsX...’
Table14: PlanningPromptforMedicalDiagnosis. [C1]isthecountofquestionsaskedand[C2]
isthecountofquestionsremaining. The‘informationgained’markedbluerepresentstheprevious
interactionhistory. Wedivideitintothreephasestodiscusstheprobabilitysetasquicklyaspossible,
conductsimulationforplanning,andremindthequestionertoconfirmthedisease.
Prologue
SameaspromptsinAppendixH.1
Phase1
SkipbecauseofthelimitedQAroundsinthisscenario
Phase2(Q1-Q3)
Youarethedoctoraskingquestionstodiagnose,[C1]questionshavebeenasked. Andnow
weknowaboutthepatient:
[informationgained]
Basedonthesymptomsofthepatientabove,pleasethinkaboutwhatdiseasethepatient
suffersfromandtellmeyourtopthreemostlikelyanswers.
Forthesethreecandidatediseases,pleaseseparatelycompletetheremaining[C2]questions
andansweryes/nobyyourself. Notably,youmustdeterminethecorrespondingdisease
beforethelastquestion.
Phase3(Q4-Q5)
Notethatyoushoulddeterminewhatdiseasethepatientsuffersfromnow. Thequestion
muststartwith’Areyoua[diseasename]?’
21Table15: PlanningPromptforTroubleshooting. [C1]isthecountofquestionsaskedand[C2]is
thecountofquestionsremaining. The‘informationgained’markedbluerepresentstheprevious
interactionhistory. Wedivideitintothreephasestodiscusstheprobabilitysetasquicklyaspossible,
conductsimulationforplanning,andremindthequestionertoconfirmthefault.
Prologue
SameaspromptsinAppendixH.1
Phase1(Q1-Q4)
Thenextquestionshouldnarrowdownthepossiblerangeoftroubleissues,preferablyin
half
Phase2(Q5-Q15)
Youareatechniciantotroubleshoot,[C1]questionshavebeenasked. Andnowweknow:
[informationgained]
Basedonthesituationyourclientfaces,pleasethinkaboutwhattheissueexactlyisand
tellmeyourtop3mostlikelyanswers.
Forthesethreecandidateissues,pleaseseparatelycompletetheremaining[C2]questions
andansweryes/nobyyourself.Notably,youmustdeterminethecorrespondingissuebefore
thelastquestion.
Phase3(Q16-Q20)
Notethatyoushoulddeterminewhatissueyourclientfacesfromnowon. Thequestion
muststartwith’Areyoua[issuename]?’
Table16: UoTPromptforthe20QuestionsGame: Asitisbasedonaclosed-setsetting,information
aboutprobabilitieswillbegivenatthebeginningoftheinteractionandwillberemindedafterQ14.
Since it is similar to previous prompts, we will not repeat it here. In the ’Prompt for Question
GenerationandSimulation’,thecountofYES/NOindicatesthenumberofitemsthatareconsistent
withtheaffirmative/negativeresponse.
Prologue
Letusplaythegameof20questions. Iamimpersonatingthething,X.Youwillaskme
upto20questionsthatstartwith’IsX’andcanonlybeansweredbyyesorno,andIwill
answereachonetruthfullybasedonbeingX.
Letusbegin. Askmethefirstquestion. Replytomewithyourfirstquestion.
PromptforQuestionGenerationandSimulation
PleasedesignaquestionaboutXandcanonlybeansweredbyYESorNO.askedThen
classifythepossibleXabovebasedonthisquestion. Iftheansweris’YES’,putthisXinto
’YES:...’,otherwiseto’NO:...’. FinallycalculatehowmanyXinYESandNO.
Notably,thisquestionshouldfulfillthatthecountofYESandNOarealmostthesamewith
apermissiblediscrepancyofnomorethanone!
Youshouldthinkaboutbestnquestionstorespondto. Andyouranswershouldbe:
Question1: IsX...?
YES:item1,item2,...
CountofYES:...
NO:item1,item2,...
CountofNO:...
AdditionalReminderinQ14-Q20
Note that you should guess and ask what X exactly is from now on. X is possible a:
[item_list_str],orother. Thequestionmuststartwith’IsX...
22Table17: UoTPromptformedicaldiagnosis: Asitisbasedonaclosed-setsetting,informationabout
probabilitieswillbegivenatthebeginningoftheinteractionandwillberemindedafterQ3. Sinceit
issimilartopreviousprompts,wewillnotrepeatithere. Inthe’PromptforQuestionGeneration
andSimulation’,thecountofYES/NOindicatesthenumberofdiseasesthatareconsistentwiththe
affirmative/negativeresponse.
Prologue
Youareadoctorandyourpatientreportsthat: [self-report]
You should ask your patient questions in English with symptoms which can only be
answeredbyyesorno,tofindwhatdiseasethispatientsuffers.
Letusbegin. Askmethefirstquestion.
PromptforQuestionGenerationandSimulation
Pleasedesignaquestiontoaskyourpatientwithsymptomsaboutdiseaseandcanonlybe
answeredbyYESorNO.Thenclassifythepossiblediseaseabovebasedoneachquestion.
If the answer is ’YES’, put this disease into ’YES: ...’, otherwise to ’NO: ...’. Finally
calculatehowmanyXinYESandNO.
Notably,thisquestionshouldfulfillthatthecountofYESandNOarealmostthesamewith
apermissiblediscrepancyofnomorethanone!
Youshouldthinkaboutbestnquestionstorespondto.
Andyouranswershouldbe:
Question1: ...?
YES:disease1,disease2,... (diseasenamesonly)
CountofYES:...
NO:disease1,disease2,... (diseasenamesonly)
CountofNO:...
AdditionalReminderinQ3-Q5
Notethatyoushouldpointoutandaskwhatdiseasethepatientsuffersfromnow. The
patientmaysufferfromoneofdiseasesbelow: [listofdisease],orother. Thequestionmust
be’Youmayhavea[diseasename]?’
23Table18: UoTPromptfortroubleshooting: Asitisbasedonaclosed-setsetting,informationabout
probabilitieswillbegivenatthebeginningoftheinteractionandwillberemindedafterQ14. Sinceit
issimilartopreviousprompts,wewillnotrepeatithere. Inthe’PromptforQuestionGeneration
andSimulation’,thecountofYES/NOindicatesthenumberoffaultswhichareconsistentwiththe
affirmative/negativeresponse.
Prologue
Youareatechnicianandyourclientreportsthat: [self-report]
Youshouldaskyourclientquestionsaboutaspecificsituationwhichcanonlybeanswered
byyesorno,inordertofindwheretheissuethisclientfaceswithlocated.
Letusbegin. Askmethefirstquestion.
PromptforQuestionGenerationandSimulation
Pleasedesignaquestiontoaskyourclientwithspecificsituationandcanonlybeanswered
byYESorNO.askedThenclassifythepossibleissueabovebasedonthisquestion. Ifthe
answeris’YES’,putthisissueinto’YES:...’,otherwiseto’NO:...’. Finallycalculatehow
manyissuesinYESandNO.
Notably,thisquestionshouldfulfillthatthecountofYESandNOarealmostthesamewith
apermissiblediscrepancyofnomorethanone!
Youshouldthinkaboutbestnquestionstorespondto. Andyouranswershouldbe:
Question1: ...?
YES:issue1,issue2,... (issuenamesonly)
CountofYES:...
NO:issue1,issue2,... (issuenamesonly)
CountofNO:...
AdditionalReminderinQ14-Q20
Notethatyoushouldpointoutandaskwhatdiseasethepatientsuffersfromnow. The
patientmaysufferfromoneofdiseasesbelow: [listofdisease],orother. Thequestionmust
be’Youmayhavea[diseasename]?’
Table19: PromptforAnswererin20questionsgame. [target]isthenameofthefinaltargetforeach
dialogue.
Prologue
Letusplaythegameof20questions. YouaretheanswererandIamtheguesser. Xis
’[target]’.Iwillaskyouupto20questionsandyoushouldanswereachonetruthfullybased
onbeingX.IfIguesscorrectlywhatisX,answerme"Youguessedit. Xis’[target]’."
NotethatneverdirectlytellmewhatXisallthetime.
Letusbegin. Hereismyfirstquestion.
Table20: PromptforAnswererinmedicaldiagnosis. [disease]isthenameofthefinaldiseasefor
eachdialogue.
Prologue
Youarethepatientsuffering’[target]’andIamthedoctor. Iwillaskyouupto5questions
andyoushouldanswereachonetruthfullybasedonyourdisease. IfIpointoutcorrectly
whatdiseaseyouexperience,answerme"Youareright. Iamexperiencing’[target]’."Note
thatneverdirectlytellmewhatdiseaseisallthetime.
Letusbegin. Hereismyfirstquestion.
Table21: AnswererPromptintroubleshooting. [fault]isthenameofthefinalfaultforeachdialogue.
Prologue
Youaretheclientwithadevicethathas’[target]’andIamthetechnician. Iwillaskyou
upto20questionsandyoushouldanswereachonetruthfullybasedontheissueofyour
device. IfIpointoutcorrectlywhatyourissueis,answerme"Youareright. Mydevicehas
’[target]’."Notethatneverdirectlytellmewhattheissueisallthetime.
Letusbegin. Hereismyfirstquestion.
24