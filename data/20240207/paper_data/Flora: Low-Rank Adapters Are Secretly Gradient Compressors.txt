FLORA: Low-Rank Adapters
Are Secretly Gradient Compressors
YongchangHao♠ YanshuaiCao♢ LiliMou♠♣
yongcha1@ualberta.ca yanshuai.cao@borealisai.com doublepower.mou@gmail.com
♠AlbertaMachineIntelligenceInstitute(Amii)
Dept. ComputingScience,UniversityofAlberta
♢BorealisAI ♣CanadaCIFARAIChair
ABSTRACT
Despite large neural networks demonstrating remarkable abilities to complete different
tasks,theyrequireexcessivememoryusagetostoretheoptimizationstatesfortraining. To
alleviatethis,thelow-rankadaptation(LoRA)isproposedtoreducetheoptimizationstates
bytrainingfewerparameters. However,LoRArestrictsoverallweightupdatematricesto
below-rank,limitingthemodelperformance. Inthiswork,weinvestigatethedynamics
ofLoRAandidentifythatitcanbeapproximatedbyarandomprojection. Basedonthis
observation,weproposeFLORA,whichisabletoachievehigh-rankupdatesbyresampling
the projection matrices while enjoying the sublinear space complexity of optimization
states. Weconductexperimentsacrossdifferenttasksandmodelarchitecturestoverifythe
effectivenessofourapproach.1
1 Introduction
Gradient-based optimization powers the learning part of deep neural networks. In the simplest form,
stochasticgradientdescent(SGD)updatesthemodelparametersusingnoisyestimationofthenegative
gradient. Moreadvancedmethodstrackvariousgradientstatisticstostabilizeandacceleratetraining(Duchi
et al., 2011; Hinton et al., 2012). For example, the momentum technique tracks an exponential moving
averageofgradientsforvariancereduction(CutkoskyandOrabona,2019)anddamping(Goh,2017). Onthe
otherhand,gradientaccumulationcomputestheaverageofgradientsinthelastfewbatchestosimulatea
largereffectivebatchforvariancereduction(Wangetal.,2013). Bothcasesrequireanadditionalmemory
bufferequaltothemodelsizetostoretheinformation.
However, such a linear space complexity of optimization states becomes problematic in modern deep
learning. Forexample,GPT-3(Brownetal.,2020)andStableDiffusion(Rombachetal.,2022)aretrained
withAdam(KingmaandBa,2015)wheremomentumisapplied. Foreachscalarintheparameterset,Adam
maintainstwoadditionalvariables(i.e.,first-andsecond-momentestimates),triplingthememoryusage.
ThelargestGPT-3, forexample, has175billionparameterstaking700GBofmemory. Adamrequiresan
additional1.4TBmemoryforoptimizationstates. Thisexcessiveamountofmemoryusageposesascaling
challenge.
Onelineofresearchsavesmemorybytrainingasubsetofparameters(Houlsbyetal.,2019;Zakenetal.,2022),
sotheoptimizeronlystoresinformationaboutasmallsetoftrainableparameters. Onenotableexampleis
thelow-rankadaptation(LoRA,Huetal.,2022). LoRAupdatesparametermatricesbylow-rankpatches,
whichcontainmuchfewertrainableparameters. Inthisway,themomentumandgradientaccumulationalso
havemuchsmallersizes. However,LoRArestrictstheweightupdatetobeinthelow-rankform,limiting
theoptimizationspaceofthemodelparameters.
1Ourcodeispubliclyavailableathttps://github.com/MANGA-UOFA/Flora.
4202
beF
5
]GL.sc[
1v39230.2042:viXraAnotherlineofworkdesignsnewoptimizersthatuselessmemory(Dettmersetal.,2021;Feinbergetal.,
2023). Forinstance,Adafactor(ShazeerandStern,2018)leveragestheclosed-formsolutionofgeneralized
Kullback–Leiblerdivergence(FinessoandSpreij,2006)toreconstructthesecond-momentestimateinAdam.
To optimize a matrix in Rn×m, Adafactor reduces the memory from O(nm) to O(n+m), making the
spacecomplexityofsecond-momentestimationsublinearinmodelsize. However, Adafactordropsthe
momentumtechniquetoachievethesublinearity,sacrificingthevariancereductionanddampingeffectof
momentum(Raeetal.,2021). Moreover,itdoesnotreducethememoryforgradientaccumulation.
Inthiswork,weproposeFLORA(fromLoRAtohigh-rankupdates),whichisanoveloptimizationtechnique
thatusessublinearmemoryforgradientaccumulationandmomentumcalculation. Ourintuitionarises
frominvestigatingLoRAandobservingthataLoRAupdateisdominatedbyarandomprojection,which
compressesthegradientintoalower-dimensionalspace(Dasgupta,2000;BinghamandMannila,2001).Thus,
weproposeFLORAthatappliessuchacompressiontechniquedirectlytotheupdateoftheoriginalweight
matrix. OurFLORAresamplestherandomprojectionandisabletomitigatethelow-ranklimitationofLoRA.
Further,ourapproachonlystoresthecompressedgradientaccumulationandmomentum,thussavingthe
memoryusageofoptimizationstatestothesublinearlevel. Weconductexperimentsacrossdifferenttasks
andmodelarchitecturestoverifytheeffectivenessofourapproach. WhencombinedwithAdafactorasa
baseoptimizer,ourapproachyieldssimilarperformancetouncompressed,full-matrixupdate,whilelargely
outperformingothercompressiontechniquessuchasLoRA.Interestingly,thespacecomplexityofFLORAis
inthesameorderasLoRAbuthasasmallerconstantinpractice,leadingtolessmemoryusagethanLoRA.
2 Approach
Inthissection,wefirstpresentourobservationofthedynamicsofLoRAupdates(§2.1). Then,weshowthat
LoRAcanbeapproximatedbyrandomprojection(§2.2),whichservesasgradientcompression(§2.3)and
canbeusedforsublinear-spacegradientaccumulationandmomentumcalculation(§2.4).
2.1 Dynamicsoflow-rankadaptation(LoRA)
Forupdateapre-trainedweightmatrixW ∈ Rn×m, LoRAparameterizes B ∈ Rn×r and A ∈ Rr×m with
r ≪min{n,m}. AfterapplyingLoRA,theforwardpassbecomes
y = (W+BA)x =Wx+BAx, (1)
where x ∈ Rm istheinputforcurrentlayerand y ∈ Rn isthepre-activationvalueofthenextlayer. At
thebeginningofLoRAupdates,BAshouldnotchangetheoriginalweightW. Thecommonpracticeisto
initializethematrixBwithanall-zeromatrixand Awithanormaldistribution.
Duringback-propagation,thematrixW hasgradient
∂L
∇ L = x⊤ , (2)
W
∂y
where ∂L ∈Rn isthepartialderivativew.r.t. y. LoRAonlycalculatesthegradientw.r.t. thematrices Aand
∂y
B,givenby
∂L ∂L
= B⊤ x⊤ = B⊤(∇ L) (3)
W
∂A ∂y
∂L ∂L
and = x⊤ A⊤ = (∇ L)A⊤ . (4)
W
∂B ∂y
OurinsightisthatinEquations(3)and(4),LoRAessentiallydown-projectstheoriginalgradienttoalower
dimension. Infact,wefoundthatLoRArecoversthewell-knownrandomprojectionmethod(Dasgupta,
2000;BinghamandMannila,2001). Weformallystatethisinthefollowingtheorem.
Theorem2.1. LetLoRAupdatematrices AandBwithSGDforeverysteptby
A t+1 ←A t−ηB t⊤(∇ WL t) (5)
B t+1 ←B t−η(∇ WL t)A⊤ t . (6)
2We assume (cid:13) (cid:13)∑ tT =0∇ WL t(cid:13) (cid:13)
F
≤ L for every T during training, which implies that the model stays within a finite
Euclideanball. Inthiscase,thedynamicsof A andB aregivenby
t t
A = A +ηA f (T), B = ηf (T)A⊤ , (7)
T 0 0 A T B 0
where the forms of f (t) ∈ Rm×m and f (t) ∈ Rn×m are expressed in the proof. In particular, ∥f (t)∥ ≤
A B A 2
(cid:0) (cid:1)
ηL2 1−(η2L2)t
foreveryt.
1−η2L2
Proof. SeeAppendixA.
Theorem2.1describestheSGDdynamicsofLoRAupdates. Withoutlossofgenerality,wedenotethetotal
changeof AandBafterTstepas∆Aand∆B,respectively. Thenthefine-tunedforwardfunctionwillbe
W+(B +∆B)(A +∆A) =W+B A +B ∆A+∆BA +∆B∆A (8)
0 0 0 0 0 0
=W+∆BA +∆B∆A, (9)
0
whereB =0isduetotheinitializationoftheBmatrix. ThefinalexpressiondissectstheLoRAweightinto
0
twoparts. Weobservethatitisthefirstpartthatdominatesthetotalweightchange,asstatedbelow.
Observation2.2. Whenthelearningrateissmall,wehaveanapproximationthat
W+(B +∆B)(A +∆A) ≈W+∆BA . (10)
0 0 0
ThiscanbeseenbyexpandingB and A givenbyTheorem2.1. Specifically,wehavethat
0 0
W+∆BA +∆B∆A =W+ηf (t)A⊤ A +η2f (t)A⊤ A f (t) (11)
0 B 0 0 B 0 0 A
Ourinsightisthatthethirdtermhasasmallermagnitudewhenthelearningrateisnotlarge. Thisisbecause
(cid:0) (cid:1)
ηL2 1−(η2L2)t
∥f A(t)∥
2
≤ ∥f A(t)∥
F
≤
1−η2L2
givenbyTheorem2.1. Ifη ≪ 1/L,wehavelim t→∞η∥f A(t)∥ ≪ 1,
whichindicatesthatthethirdtermissignificantlysmallerthanthesecondterm,makingitnegligibleinthe
finalupdates.
2.2 Randomprojectionofgradients
ByObservation2.2,thechangeofthematrixBdominatesthefinalweight. Astraightforwardsimplification
istofreezethematrix AbuttotunethematrixBonly(denotedby∆B˜). Inthiscase,wehave
W+(B +∆B)(A +∆A) ≈W+∆B˜A (12)
0 0 0
=:W+ηf˜ (T)A⊤ A . (13)
B 0 0
InEquation(12),B isdroppedbecauseBisinitializedasanall-zeromatrix. Equation(13)defines f˜ (T),
0 B
whichwillhavetheupdateform
f˜ (t+1) := f˜ (t)−∇ L (14)
B B W t
followingthederivationsinTheorem2.1. Therefore, f˜ (t) = −∑ ∇ L. PuttingittoEquation(13),we
B i W i
have
(cid:32) (cid:33)
T T (cid:104) (cid:105)
W+ηf˜ (T)A⊤ A =W−η ∑ ∇ L A⊤ A =W−η ∑ (∇ L )A⊤ A . (15)
B 0 0 W t 0 0 W t 0 0
t=0 t=0
In other words, our derivation reveals that, with some approximations, LoRA updates can be viewed
as performing random projection to the gradient. In particular, it compresses a gradient by a random
down-projection A⊤,andthendecompressesitbyanup-projection A .
0 0
2.3 OurinterpretationofLoRA
To this end, we provide a novel interpretation of a LoRA update by framing it as the compression and
decompressionofgradients.
3Figure1:TheresultsofLoRAanditssimplifications.WeapplytheLoRApatchtothefirstlayerofthenetwork
withashapeof768×768andsetr =8. ThelegendLoRAistheoriginalLoRAmethod,whileLoRA(B)isthe
simplificationwhereonlythematrixBisupdated. RP(randomprojection)andRRP(resampledRP)follow
thesameupdaterule(15),butRRPusesdifferentprojectionmatricesatdifferentsteps. Inaddition,weshow
theresultsofSGDonthefullmodelforcomparison. Allexperimentsusethesameη =0.01.
Compression. LoRAfirstcompressesthegradientbyarandomdown-projection,whichcanbejustified
bythefollowingresultbasedontheJohnson–Lindenstrausslemma(DasguptaandGupta,2003;Matousˇek,
2008).
Lemma2.3(IndykandMotwani). Letϵ ∈ (0,1/2]andδ ∈ (0,1). Let A ∈Rr×m bearandommatrixwhereeach
elementisindependentlysampledfromastandardGaussiandistribution. Thereexistsaconstantcsuchthatwhen
r = cϵ−2log(δ/2),wehave
√
(1−ϵ)∥x∥ ≤ (1/ r)∥Ax∥ ≤ (1+ϵ)∥x∥ (16)
withprobabilityatleast1−δforeveryx ∈Rm.
Essentially,thislemmasuggeststhat,withahighprobability,theprojectionbyarandomGaussianmatrix
largelypreservesthescalednormintheoriginalspace. InthecaseofLoRA,sucharandomprojectionis
appliedtoeachrowofthegradientmatrices,whosedimensionisthusreducedfromRn×m toRr×m. The
lemmaassertsthatthenormstructureoftherowsisapproximatelypreserved.
Decompression. Afterdown-projectionby A⊤,LoRAdecomposesthegradientbyanup-projection A .
0 0
Weshowthatthiswillrecovertheoriginalgradientinexpectation:
(cid:104) (cid:105)
E W+(∇ L )A⊤ A =W+(∇ L )E[A⊤ A ] (17)
W t 0 0 W t 0 0
A0 A0
where(1/r)E [A⊤A ]isanidentitymatrix. Moreover,thelargertherankr,theclosertheexpectationisto
A0 0 0
theidentity. Wequantifytheerrorinourfollowingtheorem.
Theorem 2.4. Let A be a matrix of shape Rr×m where each element is independently sampled from a standard
Gaussiandistribution. Letϵ,δ ∈ (0,1]. Thereexistsaconstantcsuchthatwhenr = clog(2m/δ)ϵ−2,wehaveforall
i,jthat
(cid:12) (cid:12)
(cid:12)[A⊤ A−I] (cid:12) ≤ ϵ (18)
(cid:12) i,j(cid:12)
withconfidenceatleast1−δ.
4Proof. SeeAppendixB.
OurTheorem2.4impliesthatronlyneedstoscalelogarithmicallytopreservetheelement-wisereconstruction
error,whichisefficientinbothcomputationandmemory. Further,thelogarithmicasymptoticratemakesit
anidealcandidatetobeappliedtothetrainingofmodernneuralmodelswheremislarge.
WeempiricallyverifyourinterpretationofLoRAbyapilotstudyontheFashion-MNISTdataset(Xiaoetal.,
2017)withasimplefeed-forwardnetwork. WeexperimentwithavariantofLoRAwhereonlyBistuned;
wecallthevariantLoRA(B).AsshowninFigure1,theperformanceofLoRA(B)isclosetotheoriginalLoRA,
which is consistent with Observation 2.2 and suggests the overall update of LoRA is dominated by the
compression-and-decompressionstep. Further,thecurveisidenticaltorandomprojection(RP),wellaligned
withourderivationinSection2.2.
2.4 Ourmethod: FLORA
Basedontheanalyses,weproposeourmethod,FLORA(fromLoRAtohigh-rankupdates),toenableoverall
high-rankupdates. OneofthemaininsightsofFLORAisthatitconstantlyresamplestheprojectionmatrixin
Equation(15). Therefore,ourtotalweightchangeisnolongerconstrainedtobelow-rank. Moreover,wecan
applytherandom-projectioncompressiontotheoptimizationstatesformemorysaving. Wedemonstrate
twocommonscenarioswhereFLORAcanbeapplied: (1)anarithmeticmean(AM)overaperiodofhistory,
forwhichaconcreteexampleisgradientaccumulation;(2)anexponentialmovingaverage(EMA),forwhich
anexamplecouldbemomentumcalculation. WeshowthatcompressionofFLORAhasthesameasymptotic
rateasLoRAbutwithalowerconstant.
Resamplingrandomprojectionmatrices. WiththeapproximationinObservation2.2,LoRAcanbeviewed
as having a fixed random projection matrix A . This restricts the overall change of W to be low-rank.
0
However,ouranalysisinSection2.3holdsforanyrandommatrixateverytimestep.
Therefore, we propose in FLORA to resample a new random matrix to avoid the total change restricted
in a low-rank subspace. In our pilot study, resampling the random matrix (RRP) largely recovers the
performance of full-matrix SGD, significantly surpassing both the original LoRA and its approximated
version in Equation (15). The empirical evidence highlights the effectiveness of avoiding the low-rank
constraintinourFLORA.
Itshouldbeemphasizedthatwecannotresamplethedown-projectionmatrix AinLoRA.Thisisbecause
Aand Barecoupledduringtheupdates,andifthedown-projectionmatrix Aisresampled,thealready
updatedmatrixBwillnotfit. Ontheotherhand,ourFLORAdirectlyupdatestheweightmatrixW during
training,makingitflexibletochoosearandomdown-projectionateverystep.
Sublinearmemorygradientaccumulation. OneapplicationofourFLORAistocompresstheoptimization
statestosavememoryduringtraining. Wefirstshowthiswithanexampleofgradientaccumulation,which
iswidelyusedinpracticetosimulatealargerbatchsize(Smithetal.,2018). Specifically,itcalculatesthe
arithmeticmean(AM)ofgradientsforτstepsandupdatesthemodelbytheAM.Inthisway,theeffective
batchsizeisτtimeslargerthantheoriginalbatchsize. However,itrequiresamemorybuffer,whosesizeis
equaltothemodelitself,tostoretheaccumulatedgradients.
InFLORA,weproposetocompressthegradientaccumulationwiththedown-projection. Withinanaccumu-
lationcycle,weonlymaintaintheaccumulatedgradientintherandomlydown-projectedspace. During
decompression,wecanreusethememoryforgradientstoreduceadditionaloverheads. Theresamplingof
theprojectionmatrixoccurswhenanaccumulationcycleisfinished. Theoverallalgorithmissummarizedin
Algorithm1.
Sublinearmemorymomentum. Themomentumtechniqueiswidelyusedinmoderndeeplearningto
reducethevarianceofthegradientandacceleratetraining(Nesterov,1998;Goh,2017;JelassiandLi,2022).
However,itrequiresmaintaininganadditionalmomentumscalarforeachparameter,whichisexpensive
whenthemodelislarge.
Similartothecompressedgradientaccumulation,wecanalsocompressthemomentumwithFLORA. For
each time step t, we down-project the new gradient G by a random projection matrix A⊤. However,
t t
thedifficultyforaccumulatingmomentumemergeswhenweusea A
t
differentfrom A t−1,aswecannot
reconstruct the original momentum from G t−1A⊤ t−1+G tA⊤
t
. This difficulty applies to all EMA updates
5Algorithm2MomentumwithFLORA.
Require: decayrates0≤β≤1
Require: rankr∈Z +,intervalκ∈Z +
Algorithm1GradientaccumulationwithFLORA. Require: gradientfunction∇ f(·;·)
W
(cid:110) (cid:111)
Require: rankr∈Z + Require: weightW= W(l) :dim(W(l))=2
Require: accumulatingstepsτ∈Z + ▷Initializetheoptimizerstate
Require: gradientfunction∇ f(·;·)
W (cid:110) (cid:111) 1: t←0
Require: weightmatricesW= W(l) :dim(W(l))=2 2: forW ∈Wdo
▷Initializationoftheaccumulatorstate 3: M
t,W
←0n×r
1: forW ∈Wdo 4: s t,W ←anindependentrandomseed
2: C ←0n×r ▷O(nr) 5: endfor
3:
sW
←anindependentrandomseed
▷Trainingprocedure
W
4: endfor 6: whiletrainingnotconvergeddo
▷Accumulatingthecompressedgradients 7: forW ∈Wdo
5: fori∈[τ]do 8: G t,W ←∇ Wft(W) ▷G t,W ∈Rn×m
6 7:
:
for GW W∈ ←W
∇
Wdo
f i(W) ▷G W ∈Rn×m
19 0:
:
iA ft t,W ≡∼ 0N (mst,W od(0 κ,1 )/ thr)
en
▷A t,W ∈Rr×m
98 :: CA WW ←← CN WsW +(0 G,1 W/ Ar)
W⊤
▷▷ CoA mW p∈ reR ssr s× iom
n
1 11 2:
:
s At+ W′1, ∼W N← sta +n 1,Wi (n 0d ,e 1p /e rn )dentran ▷do Am W′se ∈ed
Rr×m
10: endfor 13: M′ ← M A A⊤
t,W t,W t+1,W
11: endfor 14: else
11 1 42 3 :: : ▷ forR GA ˜We Wco ∈ ←←ns Wt (Nr 1u /sdc
W
not (i )o 0 Dn ,1/ Ar) ▷D▷ ecA
oW
mp∈ reR sr s× iom
n
11 1 75 6 :: : s MAt+ W′ ′1 ←, ←W M← A tt ,,s WWt,W
W W W 18: endif
1 15 6: : e ren td urf nor {G˜ :W ∈W} ▷Overwrite{G } 19: M t+1,W ←βM′+(1−β)G t,WA W′ ⊤
W W 20: endfor
21: yield{M t+1,WA W′ :W ∈W} ▷Decompression
22: t←t+1
23: endwhile
wherethenumberofaccumulationstepsisnotfinite. Inthiscase,resamplingnewmatriceswillresultina
lossofhistoricalaccumulation.
Weproposetworemediestoaddressthisissue. First,wekeepthesameprojectionmatrixforalongtimeto
reducethedistortion. Second,weproposetotransferthecompressedmomentumfromtheoldprojectionto
anewoneby M t = M t−1A t−1A⊤ t . Thisisjustifiedby A⊤ t−1A t−1 and A⊤ t A t areapproximatelytheidentity
matrixbasedonTheorem2.4.
The final algorithm is shown in Algorithm 2. Overall, for each weight matrix W ∈ Rn×m, we preserve
themomentumterm M withsublinearmemory. Comparedwiththeoriginalmomentum,wereducethe
t
memoryfromO(nm)toO(nr). Itshouldbepointedoutthatalthoughwetrackmomentumspecificallyin
thiscase,ouralgorithmcanbeeasilyextendedtootherEMA-basedstatistics.
Memory analysis. It should be pointed out that neither LoRA nor our FLORA saves the memory for
back-propagation. Thisisbecause ∂L isneededfortheupdateof AandBinLoRA(Dettmersetal.,2023),
∂W
whileinourapproachwealsocompressanddecompressthegradient.
Thatbeingsaid, savingthememoryofoptimizationstatesalonecouldbecriticaltotraininglargemod-
els(Dettmersetal.,2021). OurFLORAcompressesboththeAMandEMAofgradientstothesublinearlevel,
whichsharesthesameasymptoticrateasLoRA.Inimplementation,wemaystoretherandomseedthat
generatestheprojectionmatrix—whichishighlyefficient,aseachelementcanbesampledindependently
with simple operations—instead of maintaining the same project matrix over batches. This allows the
programtofurthersavememoryinpracticewithbufferreuse. Onthecontrary,LoRAneedstomaintain
twoweightmatrices AandB,aswellastheirAMorEMAmatrices. EmpiricallyshowninSection3,FLORA
consumeslessmemorythanLoRAwhilefacilitatinghigh-rankupdatesandoutperformingLoRAtoalarge
extent.
6Table1: Theresultsofdifferentmethodstocompressgradientaccumulation. Thesizeindicatesthetotal
numberoftheoriginalmodelparameters. Thenumbersinthebracketsdenotetherankroftherandom
projectionmatrix.
(a)TheresultsofT5variantsonXSum. (b)TheresultsofGPT-2variantsonIWSLT17De-En.
Size Accumulation Mem ∆ R /R /R Size Accumulation Mem ∆ BLEU
M 1 2 L M
None 0.75 - 33.4/11.4/26.4 None 2.77 - 17.9
Naive 0.87 0.12 34.0/11.5/26.7 Naive 3.24 0.47 24.9
LoRA(8) 0.82 0.07 30.4/8.60/23.6 LoRA(8) 3.25 0.48 9.94
LoRA(32) 0.86 0.11 30.7/8.90/23.9 LoRA(32) 3.29 0.52 11.2
LoRA(128) 0.94 0.19 31.0/9.10/24.1 LoRA(128) 3.38 0.60 12.2
60M 110M
LoRA(256) 1.07 0.32 31.4/9.34/24.5 LoRA(256) 3.52 0.75 13.4
FLORA(8) 0.75 0.00 31.5/9.67/24.6 FLORA(8) 2.93 0.15 16.3
FLORA(32) 0.75 0.00 32.2/10.3/25.2 FLORA(32) 2.94 0.16 22.0
FLORA(128) 0.77 0.02 33.2/10.9/26.0 FLORA(128) 2.98 0.20 24.0
FLORA(256) 0.79 0.04 33.6/11.3/26.5 FLORA(256) 3.03 0.26 25.4
None 16.7 - 42.5/19.1/34.6 None 20.8 - 28.2
Naive 26.6 9.9 44.4/20.9/36.3 Naive 26.5 5.78 33.2
LoRA(16) 27.8 11.1 42.2/18.4/34.0 LoRA(16) 26.8 6.02 17.4
LoRA(64) 29.5 12.8 42.3/18.6/34.1 LoRA(64) 27.4 6.68 19.5
LoRA(256) 33.4 16.7 42.6/18.9/34.4 LoRA(256) 28.9 8.15 20.7
3B 1.5B
LoRA(512) OOM - - LoRA(512) OOM - -
FLORA(16) 17.0 0.3 43.5/20.0/35.5 FLORA(16) 21.1 0.34 29.7
FLORA(64) 18.2 1.5 43.9/20.3/35.8 FLORA(64) 21.3 0.52 31.6
FLORA(256) 19.5 2.8 44.3/20.7/36.2 FLORA(256) 21.9 1.17 33.2
FLORA(512) 22.1 5.4 44.5/20.9/36.4 FLORA(512) 22.8 2.04 33.6
3 Experiments
Inthissection,weempiricallyverifytheeffectivenessofourapproachacrossdifferentmodelarchitectures
anddatasets.
3.1 Experimentsetup
Models. Giventheexceptionalabilityoflanguagemodels,weconsiderTransformer-basedmodelsforour
experiments. Specifically, weselecttworepresentativemodels, includingtheT5(Raffeletal.,2020)and
GPT-2(Radfordetal.,2019)series. FortheT5series,weuseT5-smalltorepresentsmallmodelsandT5-3Bto
representlargemodels. T5-smallhasaround60Mparameters,withahiddendimensionsetto512,while
T5-3Bhasaround3Bparameterswithahiddendimensionof1024. FortheGPT-2series,weusethebase
versiontorepresentsmallmodelsandGPT-2-XLtorepresentlargemodels. Thebaseversionhasaround
110Mparameters,withahiddendimensionsetto768,whilethelargeversionhasaround1.5Bparameters
withahiddendimensionof1600.
Datasets. Tofacilitateevaluation,weusetwoconditionallanguagemodelingtasks,includingsummariza-
tionandtranslation.
For the summarization task, we train2 T5 on the XSum dataset (Narayan et al., 2018). Each sample is a
newsarticlewithasummary. Thetaskistogenerateasummaryofthearticle. Foreachinput,weprepend
theprefix“summarize:” tothesourcesentenceintheencoder(Raffeletal.,2020). Thesourceandtarget
sentencesaretruncatedto512and128tokens,respectively.
Forthetranslationtask,wefollowthesettingofLinetal.(2020)andtrainGPT-2ontheIWSLT-2017German-
Englishdataset(Cettoloetal.,2017). EachsampleisaGermansentencewithitsEnglishtranslation. Thetask
istogeneratetheEnglishtranslationoftheGermansentence. Foreachinput,weusethetemplate“translate
GermantoEnglish: [source]. English: [target]”fortraining(Raffeletal.,2020).
2Inourexperiments,wefine-tuneapre-trainedmodelinthegradientaccumulationexperiment,whiletrainingfrom
scratchinthemomentumexperiment(Section3.2).
7Evaluation metrics. For the summarization task, we use the widely used ROUGE scores (Lin, 2004),
includingROUGE-1,ROUGE-2,andROUGE-L(R /R /R )toevaluatethequalityofthegeneratedsummary.
1 2 L
For the translation task, we use the most commonly used SacreBLEU score (Post, 2018) to evaluate the
translationquality. ForbothROUGEandSacreBLEUscores,thehigherthescore,thebetterthequalityofthe
generatedtext.
Togetmoreinsightsintothetrainingprocess,wemonitorthepeakmemoryusagewiththebuilt-inJAX
profilingtool(Bradburyetal.,2018). Wealsoshowtheexcessivememory∆ comparedwiththemethod
M
whereaccumulationormomentumisdisabled. ThememoryisreportedinGiB(10243bytes).
Competingmethods. Inourexperiment,wetakeAdafactorasthebaseoptimizer,whichisthedefault
optimizerformanyTransformermodelsincludingT5(Raffeletal.,2020),andisreportedtobeempirically
betterthanAdam(Raeetal.,2021). WeusetheofficialAdafactorimplementationinOptax(DeepMindetal.,
2020).
We compare the following methods: (1) None: a baseline that does not use gradient accumulation or
momentum;(2)Naive: anaiveimplementationofgradientaccumulationormomentum,whichstoresthe
full information along training; (3) LoRA: the original LoRA method where only the LoRA patches are
trained;(4)FLORA: ourapproachthatcompressesthegradientsanddecompressesthemwhenupdatingthe
originalweights. ForLoRAandFLORA,weapplytheprojectionstoattentionandfeed-forwardlayersonly,
whilefollowingthenaiveprocedureforotherlayers(i.e.,tokenembeddingsandvectorweights).
Forsmallmodels(T5-smallandGPT-2base),wetesttherankrfrom8to256,rangingfromtheverylow
dimensiontohalfofthehiddendimension, forathoroughexaminationofdifferentmethods. Forlarge
models(T5-3BandGPT-2-XL),wetestrfrom16to512toapproximatelymaintainthesamepercentageof
memorysavingassmallmodels. Wedonotapplylearningrateschedules(LoshchilovandHutter,2017)or
weightdecay(LoshchilovandHutter,2019)inanyexperimentstoruleouttheinfluenceofthesetechniques.
3.2 Mainresults
Gradientaccumulation. Inthissetting,wefine-tunepre-trainedmodelswith16gradientaccumulation
steps. Toachieveaminimalmemoryfootprintandfitlargemodels,thephysicalbatchsizeissetto1. We
sweepthelearningratefrom10−5to10−1withthenaiveaccumulationmethodonthevalidationloss. The
bestlearningrateisappliedtoothermethodsexcludingLoRA,whichistunedindividuallyasitisreported
tohavedifferentoptimallearningrates(Huetal.,2022). Foreachrun,themodelisfine-tunedfor1epochto
preventover-fittingfollowingthecommonpractice(Wuetal.,2021). Theresultsarereportedonthetestset
basedonthecheckpointwiththelowestvalidationloss.
WepresenttheresultsinTable1. Asshown,thenaivegradientaccumulationimprovestheROUGEscores
overthemethodwithoutaccumulation,butitleadstoalargememoryusage,whichissimilartothemodel
size,tostoretheaccumulation. ForLoRA,weempiricallyobservethatitgenerallydoesnotreducememory
usage in this case as the state of Adafactor is already sublinear. In fact, it increases memory because it
storesanotherfourlow-rankmatricesforeachweightmatrixandaddsanadditionalJacobianpathforthe
automaticdifferentiation.
Onthecontrary, our FLORA reducesthememoryfootprintonallbenchmarkscomparedwiththenaive
accumulation. Inaddition,whenrisreasonablylarge,ourmethodisabletorecovertheperformance(in
ROUGE or BLEU scores) of full-matrix accumulation and surpass the baseline that accumulation is not
enabled. Notably,forthelargemodels(T5-3BandGPT-2-XL),thememoryoverheadofFLORA(r =256)is
only30%ofthenaiveaccumulation,whiletheperformanceisonpar.
Momentum. Giventhatthemomentumtechniqueisineffectiveinfine-tuning(Lietal.,2020),wetrain
all models from scratch in this setting. The physical batch size is set to 4 as a result of balancing the
generalizationandvariancereduction(MastersandLuschi,2018). Wedisablethegradientaccumulation
techniquetoruleoutitsimpact. Duetotheexpenseoftrainingfromscratch,weonlytestthesmallvariants
ofeachseries. Similartothesettingsingradientaccumulation,wesweepthelearningrateforthenaive
momentum method from 10−5 to 10−1 on the validation loss. The best learning rates are applied to all
methodsexcludingLoRA,whichagainhasitsownoptimallearningrate.Thehyper-parameterκ(resampling
interval)issetto1000forallrunsofFLORA. TheeffectofdifferentvaluesofκisshowninSection3.3.
8Table2: Theresultsofcompressingmomentum. Thesizeindicatesthetotalnumberoftheoriginalmodel
parameters. Thenumbersinthebracketsdenotetherankroftherandomprojectionmatrix.
Setting Momentum Mem R /R /R Setting Momentum Mem BLEU
1 2 L
None 1.65 29.4/9.11/23.3 None 8.95 19.4
Naive 1.89 29.9/9.40/23.8 Naive 9.45 19.9
LoRA(8) 1.88 18.0/3.33/14.9 LoRA(8) 9.42 4.98
LoRA(32) 1.91 20.4/4.20/16.7/ LoRA(32) 9.46 6.76
T560M LoRA(128) 2.05 21.5/4.82/17.4 GPT-2110M LoRA(128) 9.55 8.72
XSum LoRA(256) 2.13 22.2/5.04/17.9 IWSLT17 LoRA(256) 9.76 9.83
FLORA(8) 1.71 25.5/6.56/20.4 FLORA(8) 9.09 9.14
FLORA(32) 1.72 26.9/7.32/21.5 FLORA(32) 9.10 14.9
FLORA(128) 1.75 29.1/8.76/23.2 FLORA(128) 9.14 18.6
FLORA(256) 1.79 30.2/9.51/24.0 FLORA(256) 9.20 19.9
AsshowninTable2,thenaivemomentumtechniqueachievesbetterperformancethanatacostofmore
memoryusage. Similartotheresultsingradientaccumulation, LoRAdoesnotsavememorygiventhe
optimizationstateisalreadysublinear. Italsohasasignificantlylowerperformancewhentrainedfrom
scratch,astheoverallmatrixupdatecanonlybelow-rank.
Our FLORA utilizeslessmemorythanthenaivemomentum. Inaddition,ourmethodrecovers(oreven
surpasses)theperformanceofnaivemomentumwhenrisincreased. Thissignificantlydistinguishesour
methodsfromLoRAasitachievesmemory-efficienttrainingevenwhentheinitializationisrandom.
3.3 In-depthanalyses
Theeffectofκinmomentum. Inourmomentumimplementation,wehaveahyper-parameterκcontrolling
the resampling frequency of the random-projection matrix. In this part, we analyze the effect of κ with
T5-smallonthesummarizationtaskasourtestbed,duetothelimitoftimeandresources. Wevaryκ by
keepingotherhyper-parametersthesameasinSection3.2.
TheresultsareshowninTable3. Itisseenthat, whenκ isbelow1000, theROUGEscoresincreasewith
κ. Afteracertainthreshold,however,weseethattheperformancestartstodecrease. Thisalignswithour
interpretationthattheinformationisbetterpreservedwithintheinterval,buteachintervalisbottlenecked
bytherank. Giventheresults,wechooseκ =1000inSection3.2tobalancethepreservedinformationand
theoverallrankofmomentum.
Optimizerwithlinearmemory. Inourmainexperiments,weobservedacounter-intuitivephenomenon
that LoRA empirically increases memory usage. This is likely because that the optimization states in
Adafactor are already sublinear, rendering the ineffectiveness of LoRA to save memory in this case. To
furtherverifyourmethodinlinear-memoryoptimizers,wetesttheperformancewithavariantofAdafactor
wherethesecond-momentestimatesarenotfactorized,essentiallymakingitalinear-memoryoptimizer. All
theotherhyper-parametersremainthesameasSection3.2.
Table4showstheresults. Asseen,LoRAindeedsavesmorememorythanourFLORAwhentherankislow
(r <128)inlinear-memoryoptimizers. However,ourFLORAbecomesmorememory-efficientforr =256,
becausewehavealowerconstantinthecomplexity. Moreover,ourFLORAlargelyoutperformsLoRAinall
settingsby2–3ROUGEpoints,showingthesuperiorityofourapproach.
4 Relatedworkanddiscussion
Parameterefficientfine-tuning. Manymethodshavebeenproposedtoimprovetheparameterefficiency
offine-tuninglargemodels. Astraightforwardwayistotuneasubsetofthemodel,suchasthetoplayers(Li
andLiang,2021)andbiasvectors(Zakenetal.,2022). Anotherwayistoaddsmalltunablemodules(e.g.,
AdapterHoulsbyetal.,2019andLoRAHuetal.,2022)tothepre-trainedmodel. Althoughreducingthe
optimizationmemory,thesemethodssufferfromtheproblemthatthemodelparametersarerestricted. For
example,thetotalweightchangeofLoRAisconstrainedtobelow-rank. Inanattempttoachievehigh-rank
updates, ReLoRA (Lialin et al., 2023) proposes to periodically reinitialize the LoRA patch. However, it
requiresfull-weightpre-trainingtoworkproperly,growingthepeakmemorylinearlyinmodelsize. We
9Table4: Theresultsoflinear-memoryoptimizers.
Setting Accumulation Mem R /R /R
1 2 L
Table3: Theeffectofκinmomentum.
None 0.99 33.0/11.1/26.1
Setting κ Mem R /R /R Naive 1.12 34.0/11.5/26.7
1 2 L
LoRA(8) 0.82 28.7/7.51/22.0
1 1.79 0.00/0.00/0.00
LoRA(32) 0.86 29.0/7.71/22.3
10 1.79 27.5/7.68/31.8
T560M T560M LoRA(128) 1.00 29.7/8.02/22.9
100 1.79 29.3/8.89/23.2
XSum XSum LoRA(256) 1.20 30.0/8.28/23.2
1000 1.79 30.4/9.70/24.2
FLORA(8) 1.00 31.6/9.72/24.7
10000 1.79 29.5/9.11/23.5
FLORA(32) 1.00 32.3/10.3/25.3
FLORA(128) 1.00 33.2/10.9/26.0
FLORA(256) 1.04 33.5/11.1/26.3
hencedonotincludeReLoRAasasublinear-memorybaseline. Bycontrast,ourmethodisabletodirectly
start from scratch and achieve the full-training performance, while maintaining a sublinear complexity
throughouttheprocess.
Matrixcompression. Ourmethodiscloselyconnectedtomatrixcompressiontechniques. Forexample,
principalcomponentanalysis(Shlens,2014)ormatrixsketching(Liberty,2013)usesingularvaluedecom-
position (SVD) to approximate the large matrix with smaller matrices. However, the SVD procedure is
computationallyexpensiveanddifficulttobeparallelized,makingitimpracticalforlarge-scaletraining.
Anotherwaytocompressthematrixistouserandomprojection(BinghamandMannila,2001),whichlies
thefoundationofourmethod. Ourmethodadditionallyinvolvesasimpleandefficientdecompression
procedurejustifiedbyTheorem2.4. Thesimplificationsavesbothcomputationandmemoryusage.
Memory-efficientoptimizers. Optimizationstatescontributesignificantlytomemoryusageforlarge-scale
training(Dettmersetal.,2021). Memory-efficientoptimizers(ShazeerandStern,2018;Feinbergetal.,2023)
areshowntoeffectivelyreducethememoryfootprint. Ourmethodisorthogonaltothesemethods,asitcan
beappliedtoenhanceexistingoptimizersbycompressingthemomentumorgradientaccumulation.
Memory-efficientautomaticdifferentiation. Itisalsopossibletoreducethememoryfootprintofback-
propagation with advanced techniques like gradient checkpointing (Chen et al., 2016), mixed-precision
training (Micikevicius et al., 2018), randomized auto differentiation (Oktay et al., 2020), or zeroth-order
optimization(Malladietal.,2023). Technically,FLORAcanbecombinedwiththesemethodstofurthersave
memory. Weleaveittofutureworkgivenourfocusinthispaperistocompressoptimizationstates.
5 Conclusion
Summary. Inthiswork,weintroduceFLORA,amethodbasedonrandomprojectionsthatachievessub-
linear memory usage for gradient accumulation and momentum. In addition, our approach effectively
addressesthelow-ranklimitationofLoRAbyresamplingprojectionmatrices. Experimentalresultsdemon-
strate significant memory reduction with maintained model performance, highlighting the potential of
randomprojectionindeeplearning.
Futurework. Inthispaper,thelargestmodelis3B.ForextremelylargemodelslikeGPT-3,weestimate
thatthecompressedoptimizationstateofr =256isonly2.08%ofitsoriginalmemory,whichwouldbeof
greatpracticalsignificance. Weleavetheempiricalverificationtofuturework. Further,theapplicablescope
ofFLORAisnotlimitedtolanguagemodels. Wewouldliketotestitinmorecases.
Acknowledgements
This research was supported in part by Natural Sciences and Engineering Research Council of Canada
(NSERC) under Grant No. RGPIN2020-04465, the Amii Fellow Program, the Canada CIFAR AI Chair
Program,theDigitalResearchAllianceofCanada(alliancecan.ca),andaMitacsAccelerate(Cluster)grant.
10References
JohnDuchi,EladHazan,andYoramSinger.Adaptivesubgradientmethodsforonlinelearningandstochastic
optimization. JMLR,12(7),2011. URLhttps://jmlr.org/papers/v12/duchi11a.html.
GeoffreyHinton,NitishSrivastava,andKevinSwersky. Neuralnetworksformachinelearninglecture6a
overviewofmini-batchgradientdescent. Coursera,2012. URLhttps://www.cs.toronto.edu/~tijmen/
csc321/slides/lecture_slides_lec6.pdf.
Ashok Cutkosky and Francesco Orabona. Momentum-based variance reduction in non-convex
SGD. In NeurIPS, 2019. URL https://papers.nips.cc/paper_files/paper/2019/hash/
b8002139cdde66b87638f7f91d169d96-Abstract.html.
GabrielGoh. Whymomentumreallyworks. Distill,2017. URLhttp://distill.pub/2017/momentum.
Chong Wang, Xi Chen, Alexander J Smola, and Eric P Xing. Variance reduction for stochastic gra-
dient optimization. In NIPS, 2013. URL https://papers.nips.cc/paper_files/paper/2013/hash/
9766527f2b5d3e95d4a733fcfb77bd7e-Abstract.html.
TomBrown, BenjaminMann, NickRyder, MelanieSubbiah, JaredDKaplan, PrafullaDhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
GretchenKrueger,TomHenighan,RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,Clemens
Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,
ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,andDarioAmodei. Languagemodels
arefew-shotlearners. InNeurIPS,pages1877–1901,2020. URLhttps://papers.nips.cc/paper/2020/
hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolution image synthesis with latent diffusion models. In CVPR, pages 10684–10695, 2022.
URL https://openaccess.thecvf.com/content/CVPR2022/html/Rombach_High-Resolution_Image_
Synthesis_With_Latent_Diffusion_Models_CVPR_2022_paper.html.
DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InYoshuaBengioand
YannLeCun,editors,ICLR,2015. URLhttp://arxiv.org/abs/1412.6980.
N.Houlsby,A.Giurgiu,StanislawJastrzebski,BrunaMorrone,QuentindeLaroussilhe,AndreaGesmundo,
MonaAttariyan,andS.Gelly. Parameter-efficienttransferlearningforNLP. InICML,pages2790–2799,
2019. URLhttps://proceedings.mlr.press/v97/houlsby19a.html.
Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine-tuning
for transformer-based masked language-models. In ACL, volume 2, pages 1–9, 2022. URL https:
//aclanthology.org/2022.acl-short.1.
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. URL https:
//openreview.net/forum?id=nZeVKeeFYf9.
TimDettmers,M.Lewis,SamShleifer,andLukeZettlemoyer. 8-bitoptimizersviablock-wisequantization.
InICLR,2021. URLhttps://openreview.net/forum?id=shpkpVXzo3h.
VladimirFeinberg,XinyiChen,Y.JenniferSun,RohanAnil,andEladHazan. Sketchy: Memory-efficient
adaptive regularization with frequent directions. In NeurIPS, 2023. URL https://openreview.net/
forum?id=DeZst6dKyi.
NoamShazeerandMitchellStern. Adafactor: Adaptivelearningrateswithsublinearmemorycost. InICML,
pages4596–4604,2018. URLhttps://proceedings.mlr.press/v80/shazeer18a.html.
LorenzoFinessoandPeterSpreij. NonnegativematrixfactorizationandI-divergencealternatingminimiza-
tion. LinearAlgebraanditsApplications,416(2-3):270–287,2006. URLhttps://doi.org/10.1016/j.laa.
2005.11.012.
JackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,JohnAslanides,
SarahHenderson,RomanRing,SusannahYoung,etal. Scalinglanguagemodels: Methods,analysis&
insightsfromtraininggopher. arXivpreprintarXiv:2112.11446,2021. URLhttps://arxiv.org/abs/2112.
11446.
SanjoyDasgupta. Experimentswithrandomprojection. InUAI,pages143–151,2000. URLhttps://dl.acm.
org/doi/10.5555/647234.719759.
EllaBinghamandHeikkiMannila. Randomprojectionindimensionalityreduction: applicationstoimage
andtextdata. InKDD,pages245–250,2001. URLhttps://dl.acm.org/doi/10.1145/502512.502546.
11SanjoyDasguptaandAnupamGupta. Anelementaryproofofatheoremofjohnsonandlindenstrauss.
RandomStructures&Algorithms,22(1):60–65,2003. URLhttps://doi.org/10.1002/rsa.10073.
Jirˇ´ı Matousˇek. On variants of the johnson–lindenstrauss lemma. Random Structures & Algorithms, 33(2):
142–156,2008. URLhttps://doi.org/10.1002/rsa.20218.
PiotrIndykandRajeevMotwani. Approximatenearestneighbors: towardsremovingthecurseofdimen-
sionality. InSTOC,pages604–613,1998. URLhttps://dl.acm.org/doi/10.1145/276698.276876.
Han Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking
machinelearningalgorithms. arXivpreprintarXiv:1708.07747,2017. URLhttps://arxiv.org/abs/1708.
07747.
SamuelLSmith,Pieter-JanKindermans,ChrisYing,andQuocVLe. Don’tdecaythelearningrate,increase
thebatchsize. InICLR,2018. URLhttps://openreview.net/forum?id=B1Yy1BxCZ.
YuriiNesterov. IntroductoryLecturesonConvexOptimization: ABasicCourse. Springer,1998. URLhttps:
//doi.org/10.1007/978-1-4419-8853-9.
SamyJelassiandYuanzhiLi. Towardsunderstandinghowmomentumimprovesgeneralizationindeep
learning. InICML,pages9965–10040,2022. URLhttps://openreview.net/forum?id=lf0W6tcWmh-.
TimDettmers, ArtidoroPagnoni, AriHoltzman, andLukeZettlemoyer. QLoRA:Efficientfinetuningof
quantizedLLMs. InNeurIPS,2023. URLhttps://openreview.net/forum?id=OUIFPHEgJU.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.
JMLR,21(1):5485–5551,2020. URLhttp://jmlr.org/papers/v21/20-074.html.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,2019. URLhttps://openai.com/research/
better-language-models.
ShashiNarayan,ShayBCohen,andMirellaLapata.Don’tgivemethedetails,justthesummary!topic-aware
convolutional neural networks for extreme summarization. In EMNLP, pages 1797–1807, 2018. URL
https://aclanthology.org/D18-1206.
Zhaojiang Lin, Andrea Madotto, and Pascale Fung. Exploring versatile generative language model
via parameter-efficient transfer learning. In EMNLP Findings, pages 441–459, 2020. URL https:
//aclanthology.org/2020.findings-emnlp.41.
MauroCettolo,MarcelloFederico,LuisaBentivogli,JanNiehues,SebastianStu¨ker,KatsuhitoSudoh,Koichiro
Yoshino,andChristianFedermann. OverviewoftheIWSLT2017evaluationcampaign. InIWSLT,pages
2–14,2017. URLhttps://aclanthology.org/2017.iwslt-1.1.
Chin-YewLin. ROUGE:Apackageforautomaticevaluationofsummaries. InTextSummarizationBranches
Out,pages74–81,2004. URLhttps://aclanthology.org/W04-1013.
Matt Post. A call for clarity in reporting BLEU scores. In WMT, pages 186–191, 2018. URL https://
aclanthology.org/W18-6319.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal Maclaurin,
GeorgeNecula,AdamPaszke,JakeVanderPlas,SkyeWanderman-Milne,andQiaoZhang. JAX:compos-
abletransformationsofPython+NumPyprograms,2018. URLhttp://github.com/google/jax.
DeepMind,IgorBabuschkin,KateBaumli,AlisonBell,SuryaBhupatiraju,JakeBruce,PeterBuchlovsky,
David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Antoine Dedieu, Claudio Fantacci, Jonathan
Godwin,ChrisJones,RossHemsley,TomHennigan,MatteoHessel,ShaoboHou,StevenKapturowski,
ThomasKeck,IuriiKemaev,MichaelKing,MarkusKunesch,LenaMartens,HamzaMerzic,Vladimir
Mikulik,TamaraNorman,GeorgePapamakarios,JohnQuan,RomanRing,FranciscoRuiz,AlvaroSanchez,
LaurentSartran,RosaliaSchneider,ErenSezener,StephenSpencer,SrivatsanSrinivasan,MilosˇStanojevic´,
WojciechStokowiec,LuyuWang,GuangyaoZhou,andFabioViola. TheDeepMindJAXEcosystem,2020.
URLhttp://github.com/google-deepmind.
IlyaLoshchilovandFrankHutter. SGDR:Stochasticgradientdescentwithwarmrestarts. InICLR,2017.
URLhttps://openreview.net/forum?id=Skq89Scxx.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. URL https:
//openreview.net/forum?id=Bkg6RiCqY7.
12Jeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano.
Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862, 2021. URL
https://arxiv.org/abs/2109.10862.
HaoLi, PratikChaudhari, HaoYang, MichaelLam, AvinashRavichandran, RahulBhotika, andStefano
Soatto. Rethinkingthehyperparametersforfine-tuning. InICLR,2020. URLhttps://openreview.net/
forum?id=B1g8VkHFPH.
DominicMastersandCarloLuschi. Revisitingsmallbatchtrainingfordeepneuralnetworks. arXivpreprint
arXiv:1804.07612,2018. URLhttps://arxiv.org/abs/1804.07612.
XiangLisaLiandPercyLiang.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InACL-IJCNLP,
volume1,pages4582–4597,2021. URLhttps://aclanthology.org/2021.acl-long.353.
VladislavLialin,NamrataShivagunde,SherinMuckatira,andAnnaRumshisky.Stackmorelayersdifferently:
High-rank training through low-rank updates. arXiv preprint arXiv: 2307.05695, 2023. URL https:
//arxiv.org/abs/2307.05695.
JonathonShlens. Atutorialonprincipalcomponentanalysis. arXivpreprintarXiv: 1404.1100,2014. URL
https://arxiv.org/abs/1404.1100.
Edo Liberty. Simple and deterministic matrix sketching. In KDD, pages 581–588, 2013. URL https:
//doi.org/10.1145/2487575.2487623.
TianqiChen,BingXu,ChiyuanZhang,andCarlosGuestrin. Trainingdeepnetswithsublinearmemorycost.
arXivpreprintarXiv:1604.06174,2016. URLhttps://arxiv.org/abs/1604.06174.
Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris
Ginsburg,MichaelHouston,OleksiiKuchaiev,GaneshVenkatesh,andHaoWu. Mixedprecisiontraining.
InICLR,2018. URLhttps://openreview.net/forum?id=r1gs9JgRZ.
DenizOktay,NickMcGreivy,JoshuaAduol,AlexBeatson,andRyanPAdams. Randomizedautomatic
differentiation. InICLR,2020. URLhttps://openreview.net/forum?id=xpx9zj7CUlY.
SadhikaMalladi,TianyuGao,EshaanNichani,AlexDamian,JasonD.Lee,DanqiChen,andSanjeevArora.
Fine-tuninglanguagemodelswithjustforwardpasses. InNeurIPS,2023. URLhttps://openreview.net/
forum?id=Vota6rFhBQ.
Beatrice Laurent and Pascal Massart. Adaptive estimationof a quadratic functional bymodel selection.
Annalsofstatistics,28(5):1302–1338,2000. URLhttps://www.jstor.org/stable/2674095.
13A ProofofTheorem2.1
Theorem2.1. LetLoRAupdatematrices AandBwithSGDforeverysteptby
A t+1 ←A t−ηB t⊤(∇ WL t) (5)
B t+1 ←B t−η(∇ WL t)A⊤ t . (6)
We assume (cid:13) (cid:13)∑ tT =0∇ WL t(cid:13) (cid:13)
F
≤ L for every T during training, which implies that the model stays within a finite
Euclideanball. Inthiscase,thedynamicsof A andB aregivenby
t t
A = A +ηA f (T), B = ηf (T)A⊤ , (7)
T 0 0 A T B 0
where the forms of f (t) ∈ Rm×m and f (t) ∈ Rn×m are expressed in the proof. In particular, ∥f (t)∥ ≤
A B A 2
(cid:0) (cid:1)
ηL2 1−(η2L2)t
foreveryt.
1−η2L2
Beforeprovingthetheorem,weneedtoobtaintheformof f (t)and f (t). Wederivetheminthefollowing
A B
lemma.
LemmaA.1. Whent =0, f (0) = f (0) =0. Fort ≥1,thevaluesof f (t)and f (t)areiterativelyobtainedby:
A B A B
t−1
f (t) = −η ∑ f⊤(i)(∇ L ) (19)
A B W i
i=0
t−1
f (t) = −∑ (∇ L )(ηf⊤(i)+I) (20)
B W i A
i=0
Proof. Weprovethisbyinduction. Forthebasecaset =0,itisstraightforwardtoshow f (0) = f (0) =0.
A B
Assume A = A +ηA f (t)andB = ηf (t)A⊤ holdswithsuchfunctions f and f for1...t. Thenfor
t 0 0 A t B 0 A B
t+1,wehave
A t+1 =A t−ηB t⊤(∇ WL t) (21)
=A +ηA f (t)−η2A f⊤(t)(∇ L ) (22)
0 0 A 0 B W t
(cid:16) (cid:17)
=A +ηA f (t)−ηf⊤(t)(∇ L ) (23)
0 0 A B W t
=A +ηA (f (t+1)), (24)
0 0 A
wherewehave f (t+1) = f (t)−ηf⊤(t)(∇ L ) = −η∑t f⊤(i)(∇ L )inthelastline.
A A B W t i=0 B W i
Similarly,wehave
B t+1 =B t−η(∇ WL t)A⊤ t (25)
=ηf (t)A⊤−η(∇ L )(ηf⊤(t)+I)A⊤ (26)
B 0 W t A 0
(cid:16) (cid:17)
=η f (t)−(∇ L )(ηf⊤(t)+I) A⊤ (27)
B W t A 0
=ηf (t+1)A⊤ , (28)
B 0
wherewehave f (t+1) = f (t)−(∇ L )(ηf⊤(t)+I) = −∑t (∇ L )(ηf⊤(i)+I)inthelastline.
B B W t A i=0 W i A
(cid:0) (cid:1)
ηL2 1−(η2L2)t
ProofofTheorem2.1. Definea := .Weprove∥f (t)∥ ≤ a byinduction.Forthebasecaset =0,
t 1−η2L2 A t
itistrivialtosee∥f (0)∥ = 0 ≤ a . Wethenassume∥f (i)∥ ≤ a fori ≤ t−1. Sincea ismonotonically
A 0 A i t
increasing,weknow∥f A(i)∥ ≤ a t−1fori ≤ t−1. ByusingLemmaA.1,wehave
t−1
f (t) =−η ∑ f⊤(i)(∇ L ) (29)
A B W i
i=0
t−1i−1
=−η ∑ ∑ (ηf (j)+I)(∇ L )⊤(∇ L ). (30)
A W j W i
i=1j=0
14Takingthenorm,wehave
(cid:13) (cid:13)
(cid:13) t−1i−1 (cid:13)
∥f (t)∥ =(cid:13)η ∑ ∑ (ηf (j)+I)(∇ L )⊤(∇ L )(cid:13) (31)
A F (cid:13) A W j W i (cid:13)
(cid:13) i=1j=0 (cid:13)
F
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)t−2 t−1 (cid:13) (cid:13)t−1i−1 (cid:13)
≤η2(cid:13)∑ (f (j))(∇ L )⊤ ∑ (∇ L )(cid:13) +η(cid:13)∑ ∑ (∇ L )⊤(∇ L )(cid:13) (32)
(cid:13) A W j W i (cid:13) (cid:13) W j W i (cid:13)
(cid:13)j=0 i=j+1 (cid:13) (cid:13)i=1j=0 (cid:13)
F F
(cid:13) (cid:13)
(cid:13)t−2 (cid:13)
≤η2L(cid:13)∑ (f (j))(∇ L )⊤(cid:13) +ηL2 (LemmaA.2)
(cid:13) A W j (cid:13)
(cid:13)j=0 (cid:13)
F
≤η2L2a t−1+ηL2 (LemmaA.3)
ηL2−(η2L2)t
=η2L2 +ηL2 (33)
1−η2L2
ηL2−(η2L2)t+1
= (34)
1−η2L2
=a . (35)
t
Therefore,wehave∥f (t)∥ ≤ ∥f (t)∥ ≤ a foreveryt.
A A F t
LemmaA.2. If∥∑t−1(∇ L )⊤∥ ≤ L,wehave
k=0 W j F
(cid:13) (cid:13)
(cid:13)t−1i−1 (cid:13)
(cid:13)∑ ∑ (∇ L )⊤(∇ L )(cid:13) ≤ L2 (36)
(cid:13) W j W i (cid:13)
(cid:13)i=1j=0 (cid:13)
F
foreveryt.
Proof. LetG(k) := ∇ L forsimplicity. Takingthesquare,
W k
(cid:13) (cid:13)2
(cid:13) n (cid:13) n
(cid:13)∑ ∑ G(m)⊤ G(n)(cid:13) =∑∑ ∑ [G(m)⊤ G(n)]2 (37)
(cid:13) (cid:13) i,j
(cid:13) n m=1 (cid:13) F i,j n m=1
∑∑
∑n (cid:16)
∑
(cid:17)2
= [G(m)] [G(n)] (38)
k,i k,j
i,j n m=1 k
(cid:16) (cid:17)(cid:16) t−1 (cid:17)
≤ ∑∑∑ [G(m)]2 ∑ ∑∑ [G(n)]2 (Cauchy-Schwarz)
k,i k,j
m i k m=n j k
(cid:18) (cid:19)2
≤ ∑∑∑ [G(m)]2 (39)
k,i
m i k
(cid:18) ∑∑(cid:16)
∑
(cid:17)2(cid:19)2
≤ [G(m)] (40)
k,i
i k m
=(cid:13) (cid:13)∑ G(m)(cid:13) (cid:13)4
(41)
(cid:13) (cid:13)
m F
≤L4, (42)
whichcompletestheproofbytakingsquarerootsonbothsides.
LemmaA.3. If∥f (k)∥ ≤ a forallk < t,wehave
A F k
(cid:13) (cid:13)
(cid:13)t−1 (cid:13)
(cid:13) (cid:13)∑ f A(k)(∇ WL k)⊤(cid:13)
(cid:13)
≤ a t−1L (43)
(cid:13)k=0 (cid:13)
F
foreveryt.
15Proof. Let A(k) := f (k)andG(k) = (∇ L )forsimplicity. Takingthesquare,
A W k
(cid:13) (cid:13)t ∑−1 A(k)G(k)⊤(cid:13) (cid:13)2 =t ∑−1 ∑∑ [A(k)G(k)⊤]2
(44)
(cid:13) (cid:13) i,j
k=0 F k=0 i j
t ∑−1 ∑∑(cid:16)
∑
(cid:17)2
= [A(k)] [G(k)] (45)
i,l j,l
k=0 i j l
t−1(cid:16) (cid:17)(cid:16) (cid:17)
= ∑ ∑∑ [A(k)]2 ∑∑ [G(k)]2 (Cauchy–Schwarz)
i,l j,l
k=0 i l j l
(cid:16) (cid:17)t−1(cid:16) (cid:17)
≤ max ∑∑ [A(k)]2 ∑ ∑∑ [G(k)]2 (46)
i,l j,l
0≤k<t
i l k=0 j l
(cid:16) (cid:17)(cid:16) t−1 (cid:17)
≤ max ∑∑ [A(k) ]2 ∑∑ (∑ [G(k)] )2 (47)
i,l j,l
0≤k<t
i l j l k=0
=(cid:16)
max
∥A(k)∥2(cid:17)(cid:13) (cid:13)t ∑−1 G(k)(cid:13) (cid:13)2
(48)
F (cid:13) (cid:13)
0≤k<t k=0 F
≤a2 L2, (49)
t−1
whichcompletestheproofbytakingsquarerootsonbothsides.
B ProofofTheorem2.4
Theorem 2.4. Let A be a matrix of shape Rr×m where each element is independently sampled from a standard
Gaussiandistribution. Letϵ,δ ∈ (0,1]. Thereexistsaconstantcsuchthatwhenr = clog(2m/δ)ϵ−2,wehaveforall
i,jthat
(cid:12) (cid:12)
(cid:12)[A⊤ A−I] (cid:12) ≤ ϵ (18)
(cid:12) i,j(cid:12)
withconfidenceatleast1−δ.
Proof. Foreachelementof A⊤A,wehave
(cid:40)
∑r a2 , ifi = j,
[A⊤ A] = k=1 k,i (50)
i,j ∑r a a , otherwise,
k=1 k,i k,j
whereeachelementa isanindependentrandomvariablefollowingN(0,1).
i,k
Forz := ∑r a2 ,itfollowstheχ2(r)distribution. BythestandardLaurent-MassartboundsLaurentand
i,i k=1 k,i
Massart(2000),weobtain
(cid:115)
(cid:12)z (cid:12) log(2/δ′) log(2/δ′)
δ := (cid:12) i,i −1(cid:12) ≤2 1 +2 1 (51)
i,i (cid:12) r (cid:12) r r
withprobabilityatleast1−δ′.
1
Forz := ∑r a a wherei ̸= j,wecanrewriteitasz = ∑r
[(ak,i+ak,j)2−(ak,i−ak,j)2].
Inaddition,all
i,j k=1 k,i k,j i,j k=1 2 2
(ak,i+ak,j)2and(ak,i−ak,j)2arei.i.d.
χ2(1)distributions. Define
2 2
r (cid:18)a +a (cid:19)2 r (cid:18)a −a (cid:19)2
z+ := ∑ k,i k,j and z− := ∑ k,i k,j , (52)
i,j 2 i,j 2
k=1 k=1
itiseasytoseethatz+ andz− arei.i.d. χ2(r)distributions. Inaddition,z = z+ −z− . Therefore,wehave
i,j i,j i,j i,j i,j
16δ i,j :=(cid:12) (cid:12) (cid:12)z ri,j(cid:12) (cid:12) (cid:12) = (cid:12) (cid:12) (cid:12) (cid:12)(cid:16)z ri+ ,j −1(cid:17) −(cid:16)z ri− ,j −1(cid:17)(cid:12) (cid:12) (cid:12) (cid:12) (53)
+ −
(cid:12)z (cid:12) (cid:12)z (cid:12)
≤(cid:12) i,j −1(cid:12)+(cid:12) i,j −1(cid:12) (54)
(cid:12) r (cid:12) (cid:12) r (cid:12)
(cid:114)
log(4/δ′) log(4/δ′)
≤4 2 +4 2 (55)
r r
withprobabilityatleast1−δ′.
2
ByusingaunionbounduponEquation(51)and(55),wecanobtain
(cid:114)
2log(2m/δ) 2log(2m/δ)
δ ≤4 +4 (56)
i,j
r r
foralli,jwithprobabilityatleast1−δ. Underthiscondition,wefurtherhave
(cid:115)
2log(2m/δ) 2log(2m/δ)
δ =4 +4 (Letr =128log(2m/δ)ϵ−2)
i,j 128log(2m/δ)ϵ−2 128log(2m/δ)ϵ−2
1
= (ϵ+ϵ2) (57)
2
≤ϵ, (ϵ2 ≤ ϵ ≤1)
(cid:12) (cid:12)
concludingtheproofbynoticingδ = (cid:12)[A⊤A−I] (cid:12).
i,j (cid:12) i,j(cid:12)
17