AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion
MohamadQadri∗1 KevinZhang∗2 AkshayHinduja1 MichaelKaess1 AdithyaPediredla3
ChristopherMetzler2
CarnegieMellonUniversity1 UniversityofMaryland,CollegePark2 DartmouthCollege3
{mqadri,ahinduja}@andrew.cmu.edu, {kzhang24,metzler}@umd.edu, kaess@cmu.edu,
Adithya.K.Pediredla@dartmouth.edu
https://aoneus.github.io/
NeuS NeuSIS AONeus
camera and sonar ground truth
(Optical) (Acoustic) (Acoustic-optical)
Figure 1. AONeuS Experimental Results: Under the restricted baseline operating conditions commonly encountered in underwater
constructionandnavigation,camera-onlyreconstructiontechniques(NeuS[46])andsonar-onlyreconstructiontechniques(NeuSIS[39])
struggle to accurately recover 3D surface geometry. This is due to the highly underdetermined nature of their respective measurement
processes; cameraslackdepthinformation,andimagingsonarsdonotcaptureelevationinformation. Wehavedevelopedamultimodal
acoustic-opticalneuralsurfacesreconstructionframework(AONeuS)thateffectivelycombinesdatafromthesecomplementarymodalities.
Abstract stratethatAONeuSdramaticallyoutperformsrecentRGB-
onlyandsonar-onlyinverse-differentiable-rendering–based
Underwater perception and 3D surface reconstruction surfacereconstructionmethods.
are challenging problems with broad applications in con-
struction, security, marine archaeology, and environmen-
tal monitoring. Treacherous operating conditions, frag- 1.Introduction
ile surroundings, and limited navigation control often dic-
The 3D reconstruction of underwater environments is an
tate that submersibles restrict their range of motion and,
important problem with applications in myriad fields, in-
thus, the baseline over which they can capture measure-
cludingunderwaterconstruction,marineecology,archaeol-
ments. In the context of 3D scene reconstruction, it
ogy, mapping, inspection, and surveillance [1, 25, 33, 45].
is well-known that smaller baselines make reconstruction
The underwater robots applied to this task are typically
more challenging. Our work develops a physics-based
equippedwithbothimagingsonars(i.e.,acousticcameras)
multimodal acoustic-optical neural surface reconstruction
and optical cameras [23, 28]. These sensors capture com-
framework (AONeuS) capable of effectively integrating
plementaryinformationabouttheiroperatingenvironments.
high-resolution RGB measurements with low-resolution
Forward-lookimagingsonarsconsistofauniformlinear
depth-resolved imaging sonar measurements. By fusing
array of transducers which, through beamforming, recover
thesecomplementarymodalities,ourframeworkcanrecon-
bothrangeandazimuthinformation(butnotelevation).(3D
structaccuratehigh-resolution3Dsurfacesfrommeasure-
imaging sonars which record both azimuth and elevation
ments captured over heavily-restricted baselines. Through
alsoexist,butcanbeprohibitivelyexpensive.)Unlikelight-
extensive simulations and in-lab experiments, we demon-
based sensors, imaging sonars are highly robust to scatter-
*:equalcontribution. ingandlow-lightconditions.Unfortunately,imagingsonars
1
4202
beF
5
]VC.sc[
1v90330.2042:viXragenerally have poor spatial resolution; sonar images of an 2.RelatedWork
object of interest often appear textureless and hard to rec-
SonarImaging 3Dreconstructionfromsonarimageryis
ognize; and imaging sonar measurements can suffer from
an important and widely studied problem. Over the last
complex artifacts caused by multipath reflections and the
decade a variety of 3D reconstruction methods have been
variable speed of sound passing through inhomogeneous
proposed based on space carving [5, 6], classical point-
water[44].
cloud processing algorithms [43, 51], generative model-
Bycontrast,opticalcamerashavehighspatialresolution
ing [5, 7, 35, 50], convex optimization [52], graph-based
and can resolve object appearance in great detail. How-
processing [47, 48], and supervised machine learning [3,
ever, in turbid water light scattering and absorption can
15,49].
severely restrict the range and contrast of optical cam-
Last year, two research groups employed neural ren-
eras [18]. Moreover, to recover depth information pas-
dering to enable breakthrough 3D sonar imaging perfor-
sive optical sensors rely on large displacements/baselines
mance. Qadri et al. [39] developed a Neural Implicit Sur-
between measurements. In constrained operating environ-
faceReconstructionUsingImagingSonar(NeuSIS)method
ments,suchmeasurementsareofteninaccessible.
which forms high-fidelity 3D surface reconstructions from
By leveraging the complementary strengths and weak-
forward imaging sonar measurements by combining neu-
nessesofcamerasandimagingsonars,acoustic-opticalsen-
ral surface representations with a novel acoustic differen-
sorfusionpromisestoenablerobustandhigh-resolutionun-
tiable volumetric renderer. Similarly, Reed et al. [40] em-
derwaterperceptionandscenereconstruction[16,31]. Ex-
ployed neural rendering to recover 3D volumes from syn-
isting contour matching based acoustic-optical reconstruc-
theticaperturesonarmeasurements. Theformermethodre-
tion methods based can already reconstruct accurate high-
lies upon a large number of sonar images captured over a
resolution 3D surfaces [8]. Unfortunately, these methods
large baseline while the latter applies to synthetic aperture
require a 360-degree view of the scene and are inappli-
sonar, not forward imaging sonar, and relies on access to
cable in the small-baseline operating conditions prevalent
raw time-based sonar measurements. These methods rep-
inreal-worldunmannedunderwatervehicleoperation. Al-
resentthestate-of-the-artin3Dsurfacereconstructionwith
ternatively, one can reconstruct the scene from optical and
sonar.
acousticmeasurementsindependentlyandthenfusethere-
Todate,nomethodhaseffectivelyrecoveredadense3D
sult [20]. However, this simple approach provides limited
scene from 2D sonar images captured over a limited base-
benefitsovercamera-onlysurfacereconstruction.
line. Without additional constrains, e.g., optical measure-
In this paper, we develop an inverse-differentiable-
ments,orstrongpriorsthereconstructionproblemishope-
rendering–basedapproachtoacoustic-opticalsensorfusion
lesslyunderdetermined.
thatcanformdense3Dsurfacereconstructionsfromcamera
and sonar measurements captured across a small baseline.
Ourworkconsistsoffourkeycontributions. NeuralRendering Intheirbreakthroughneuralradiance
• Wedevelopaphysics-basedmultimodalacoustic-optical fields(NeRF)paper,Mildenhalletal.[32]combinedneural
neural surface framework which simultaneously inte- signalrepresentationswithdifferentiablevolumerendering
grates RGB and imaging sonar measurements. Our ap- to perform novel view synthesis. The underlying differen-
proach extends the neural surfaces 3D reconstruction tiablevolumerenderingconcepthassincebeenextendedto
framework[46]bycombiningaunifiedrepresentationof representandrecoverscenegeometry. TheImplicitDiffer-
the scene geometry with modality-specific (acoustic and entiableRenderer(IDR)approach,introducedin [53],rep-
optical)representationsofappearance. resents geometry as the zero-level set of a neural network
• We conduct experiments on both synthetic and andusesdifferentiablesurfacerenderingtofittheparame-
experimentally-captured datasets and demonstrate tersofaneuralnetwork. IDRrequiresobjectmasksforsu-
our method can effectively reconstruct high-fidelity pervision.Latermethods,likeNeuralSurfaces(NeuS)[46],
surface geometry from noisy measurements captured UnifiedSurfaces(UNISURF)[37],andVolumeSignedDis-
overlimitedbaselines. tanceFunctions(VolSDF)[54]combineanimplicitsurface
• We theoretically support our strong empirical perfor- representation with differentiable volume rendering to re-
mance by analyzing the conditioning of the acoustic- cover 3D geometry from images without the need for ob-
opticalforwardmodel. Weshowthattheforwardprocess ject masks. Recent work has sought to reduce the number
associatedwithtriangulatingapointin3Dfromacoustic- oftrainingimagesrequired[29]andtoacceleraterendering
optical measurements is better conditioned and easier to toenablereal-timeapplications[55].
inverttheunimodalforwardmodels. Theinverse-differentiable-renderingframeworkhasalso
• Wereleaseapublicdatasetandopen-sourceimplementa- beenextendedtohandlemeasurementsfromadiverserange
tionofourmethod. of sensors. Cross-spectral radiance fields (X-NeRF) were
2proposedin[38]tomodelmultispectral,infrared,andRGB
images. Transient neural radiance fields were proposed
in [30] to model the measurements from a single-photon
lidar. Time-of-flightradiancefieldswereproposedin [4]to
model the measurements from a continuous wave time-of-
flightsensor.Polarization-aideddecompositionofradiance, (a) Camera image formation model (b) Sonar image formation model
orPANDORA,wasproposedin[14]tomodelpolarimetric
measurementsoflight. Radarneuralradiancefields(RaN-
eRF)wereproposedin[27]tomodelinversesyntheticaper-
tureradarmeasurements.
Several recent works have modeled light scattering
within the neural rendering framework to improve recon-
structionsthroughwater[24,42],haze[12],andfog[2].
(c) Sample camera image (d) Sample sonar image
MultimodalImaging Toovercomethedisadvantagesin- Figure 2. Acoustic-Optical Measurement Processes. (a) RGB
measurementprocessandexamplemeasurement. Pixelsalonga
herenttousingasinglesensingmodality,numerousmulti-
commonraypassingthroughthecameracentermaptothesame
modalsensingalgorithmshavebeendeveloped[10,22,26,
imagepixelontheimageplane. (b)Sonarmeasurementprocess
36]. Most related to our work, Babaee and Negahdaripour
andexamplemeasurement. Inasonarimage, theazimuthθ and
[8]reconstruct3DobjectsfromRGBandsonarimageryby
rangeroftheimagedobjectareresolved. However,theelevation
matchingoccludingcontoursacrossRGBimagesandimag- informationϕislost;allobjectslocatedalongtheelevationarcin
ing sonar measurements, performing stereo matching, and bluemaptothesamepixel.
interpolating the curves in 3D space. Unfortunately, this
mulativereflectedacousticenergyfromallreflectingpoints
methodisinapplicabletothesmall-baselinesetting;itfun-
alongtheelevationarc.
damentallyrequires360-degreeviewsofthescene.Another
workthatreconstructions3DfromRGBimagesandimag- 3.2.ImageFormationModelofanImagingSonar
ingsonaris[20]. Thisworkreconstructsascenefromop-
Similarto[39],weusethefollowingsonarimageformation
tical and acoustic measurements independently using clas-
model:
sical methods like COLMAP [41] and then fuses the re-
sult. AswewilldemonstrateinSec.6.2,thisapproachpro- (cid:90) ϕmax(cid:90) ri+ϵ E
videslimitedbenefitsoverapurelyopticalapproachandis Ison(r i,θ i)= reT(r,θ i,ϕ)σ(r,θ i,ϕ)drdϕ,
notcompetitivewithsate-of-the-artneuralrenderingbased ϕmin ri−ϵ
(1)
methods.
Outside of sonar, several neural-rendering based ap-
whereϕ ,ϕ aretheminimumandmaximumelevation
min max
proaches to sensor fusion have recently been developed.
angles,E istheacousticenergyemittedbythesonar. T =
e
In Multimodal Neural Radiance Field, Zhu et al. [56] use e−(cid:82) 0riσ(r′,θi,ϕi)dr′
is the transmittance term, and σ is the
neuralrenderingtocombineRGB,thermal,andpointcloud
particledensity. (See[39]formoredetails.)
data. Similarly,[21]useneuralrenderingtocombinemul-
tispectralmeasurementsofdifferentpolarizationsandCarl- 3.3.ImageFormationModelofanOpticalCamera
sonetal.[11]fusesparselidarandRGBmeasurementsto
We adopt the optical camera image formation model pro-
build3Doccupancygridofunboundedscenes,
posed by [46] where a pixel intensity at (x,y) is approxi-
To our knowledge, ours is the first work to perform
matedby:
acoustic-opticalsensorfusionwithneuralrendering.
(cid:90) ∞
3.Background Icam(x,y)= T(t)σ(t)c(p(t),v)dt, (2)
0
3.1.ImagingSonars
wheretheintegralisovertheraystartingatthecameracen-
Imagingsonarsareactivesensorsthatemitacousticpulses terandpassingthroughpixel(x,y). T,σ arethetransmit-
andmeasuretheintensityofthereflectedwave. Theypro- tanceanddensityvaluesatpointp(t),andc(p(t),v)isthe
duce a 2D acoustic image in which the range and azimuth colorofapointviewedfromdirectionv.
oftheimagedobjectareresolved. However,theobject’sel-
4.ProblemStatement
evation remains ambiguous. I.e., the reflecting object can
belocatedanywhereontheelevationarc(fig. 2)andthein- Our goal in this work is to reconstruct the 3D surface
tensityofapixelinasonarimageisproportionaltothecu- of an underwater object using a small collection of RGB
3ismotivatedbythefactthatdifferentmaterialshavediffer-
entacousticandopticalreflectanceproperties.Forexample,
Depth
World Ambiguity World Elevation glass is invisible to optical cameras but visible to imaging
Coordinates Coordinates Ambiguity sonar, andPVCisinvisibletoimagingsonarbutvisibleto
opticalcameras.
In this work, we sample and sum points along acoustic
andopticalraystoapproximatetherenderingintegralsde-
finedbyEq.(1)andEq.(2).Ourrenderingfunctionscanbe
expressedas
(a) Local Coordinates (b) Local Coordinates
Figure3.Acoustic-OpticalMeasurementAmbiguities.(a)Two Iˆson(r,θ)= (cid:88) 1 T[x]α[x]Mson(x), and (3)
r(x)
RGB measurements captured over a limited baseline struggle to x∈Apson
localizeapointalongthedepth-axis.(b)Twosonarmeasurements Iˆcam(x,y)= (cid:88) T[x]α[x]Mcam(x), (4)
capturedoveralimitedbaselinestruggletolocalizeapointalong
thex-axis. Becausetheyhaveorthogonalambiguities, RGBand x∈Rpcam
sonarmeasurementsarehighlycomplementary.
whereA isthesetsampledpointsalongtheacousticarc
pson
at pixel pson and R is the set of sampled points along
and sonar measurements captured over a limited base-
pcam
opticalraypassingthroughpixelpcam. Mson andMcam are
line. Specifically, we assume access to two datasets,
thepredictedradianceatx.
Dcam = {Icam,Pson} and Dson = {Ison,Pson}, consisting
i i i i Thecomputationofthediscretetransmittanceandopac-
ofRGB/sonarimagesandtheirrespectiveposes.
itytermsinEq.(3)andEq.(4)requiressamplingalongboth
Givenalargedatasetcapturedoverasufficientlydiverse
acoustic and optical rays. For any such spatial sample x ,
s
range of poses (e.g., thousands of images captured from
(i.e.,anypointalonganacousticoropticalray),thediscrete
360-degrees [39]), existing unimodal (camera-only/sonar-
opacityatx canbeapproximatedas
s
only) surface reconstruction methods are already effec-
tive [39, 46]. In this work, we focus on the small baseline (cid:18) Φ (N(x ))−Φ (N(x )) (cid:19)
α[x ]=max s s s s+1 ,0 , (5)
operating conditions—pervasive in underwater robotics— s Φ (N(x ))
s s
whereopticalcamerasrecordinsufficientinformationtore-
coverdepthinformation(seeFig.3(a))andimagingsonars whereΦ (x)=(1+e−sx)−1istheSigmoidfunctionands
s
record insufficient information to recover elevation infor- isatrainableparameter. Thediscretetransmittanceismod-
mation(seeFig.3(b)). eledas
Specifically, we introduce a physics-based multimodal (cid:89)
T[x ]= (1−α[x ]). (6)
inverse-differentiable-rendering framework that integrates s r
informationfrombothacousticandopticalsensorstogen- xr|r<s
erate accurate 3D reconstructions. Our approach auto-
matically exploits the complementary information (eleva-
5.1.1 LossFunction
tion/range)providedbyeachsensor.
Because our shared pool-based testing facility does not Ourlossfunctioncomprisesthesonarandcameraintensity
allowustointroduceturbidity,inthisworkwefocusonthe losses:
clear-watersetting. Modelingtheeffectsoflightscattering
inourforwardmodel[2,12,24,42]wouldlikelyimprove Lson ≡ 1 (cid:88) ||Iˆ(p)−I(p)|| , and (7)
int N 1
oursystem’sin-the-wildperformance. Pson p∈Pson
5.Method Lcam ≡ 1 (cid:88) ||Iˆ(p)−I(p)|| , (8)
int N 1
Pcam
p∈Pcam
5.1.Acoustic-OpticalNeuS
where Pcam and Pson is the set of sampled pixels in the
Our AONeuS reconstruction framework is illustrated
cameraandsonarimagesrespectively. Weadditionallyuse
inFig.4. FollowingQadrietal.[39],Wangetal.[46],we
the eikonal loss as an implicit regularization to encourage
representtheobject’ssurfaceusingaSignedDistanceFunc-
smoothreconstructions:
tion (SDF), N(x), which outputs the distance of each 3D
point x = (X,Y,Z) to the nearest surface. Distinct from 1 (cid:88)
L ≡ (||∇N(x)|| −1)2, (9)
theseworks,weusetwoseparaterenderingneuralnetworks eik |X| 2
(Mcam andMson)thatapproximatetheopticalandacoustic x∈X
outgoingradianceateachspatialcoordinatex. Thischoice whereXisthesetofallsampledpoints.
464 64 64 64 World
Coordinates
Camera image
plane
1
64 64 64 64
64 Sonar image
g
n id Neural Renderer (Sonar) plane
o
cn
Camera
3
E
la
3 Sonar
n
o itiso
1
64 64 64 64
P
Figure 5. Simulation setup. We simulate capturing sonar and
camerameasurementsoveralimitedbaseline.
Neural Implicit Surface
Representation
64 3 the weights of the SDF network N to bias it towards re-
constructionsinwhichthegeometryoftheobjectarebetter
constrainedinthedepthdirection. Thisprocessestablishes
Neural Renderer (Camera) aninitializationforthelateriterations.
In later iterations, t > E , more emphasis is placed on
t
Figure4. AONeuSReconstructionFramework. Asharedsur-
the camera measurements. These measurements cosntrain
facegeometrySDFnetworkNisusedincombinationwithren-
thexandy directionsandhelpresolvetheelevationambi-
deringspecificneuralrenderingmodules. Foreachsampledpoint
guityinherentinsonardata. Inthisphase, sonarmeasure-
xalonganacousticoropticalray,Noutputsitssigneddistance,
itsgradientaswellas2featuresvectorsFsonandFcamallserving mentsreceivelessweightandactasadepthregularizer.
asinputtotheirrespectiverenderingnetworks. 6.ExperimentalResults
Wealsoutilizeanℓ losstermasanadditionalpriorterm InthissectionweevaluatetheproposedAONeuStechnique
1
whichbiasesthenetworktowardsreconstructionsthatmin- onbothsyntheticandexperimentallycaptureddata.
imize the total opacity of the scene (for example in cases
6.1.ResultsonSyntheticData
where the object is on the seafloor and only specific sides
canbeimaged): To generate synthetic measurements, we implemented the
sonar image formation model Eq. (1) in Blender [13] and
1 (cid:88)
L ≡ ||α[x]|| . (10) collected simulated sonar-camera datasets for various ob-
reg |X| 1
x∈X jects.ThesimulationsetupisillustratedinFig.5.Thesonar
andcameraareapproximatelycollocated,andaretranslated
Hence,ourtotallossis
linearlyoverashortbaselinealongtheX axisoftheworld
L=α(t)Lson+(1−α(t))Lcam+λ L +λ L . frame for a distance of 1.2m with the sonar’s azimuthal
int int eik eik reg reg planeparalleltheYZplaneintheworldframe. Thesonar’s
(11)
azimuthal plane is oriented orthogonal to the direction of
ThenetworkistrainedwiththeADAMoptimizer. motion to ensure the trajectory was non-degenerate; mul-
tiple measurements captured from positions within the az-
imuthal plane of the sonar would be highly redundant and
5.1.2 WeightScheduling
uninformative[33].
The weights assigned to the sonar and camera intensity For each object, the trajectory is sub-sampled into
losses (respectively α(t) and 1 − α(t) in eq. 11) impact smaller baselines: 0.96m, 0.72m, 0.48m, and 0.24m for
thereconstructionqualityastheydeterminewhichmeasure- analysis. We scaled the meshes so that the objects are ap-
ments the network should emphasize throughout training. proximately∼ 1minsizeandthesensorsareplacedabout
Weadoptasimpletwo-stepweightingscheme: 1.5m–2mawayfromtheobject. Theelevationapertureof
the sonar is 12◦. We benchmark our method against two
(cid:40)
α(t)=
1ift<E t,
(12)
methods: NeuS[46]andNeuSIS[39], executingallmeth-
λifE <t<E . ods9timeswithrandomlyinitializedseeds. Toensurewe
t e
hadreasonablecamera-onlyresults,weprovidedNeuSwith
Intheearlyiterations,t < E ,thesonarmeasurementsare masksoftheobject. Thisinformationisnotrequiredbynor
t
usedexclusivelyandserveto”mask“theobject;i.e.,update providedtoNeuSISandAONeuS.
5InFig.9(a),wecomparethereconstructionperformance
of all three techniques for a total of five scenes. We could
observethatAONeusconsistentlyreconstructsthescenege-
ometrybetterthanNeuSandNeuSIS.Further,wecanalso
observe that NeuS (camera-only) incorrectly reconstructs
the depth axis (Z-axis) whereas NeuSIS (sonar-only) can
reconstruct only the depth-axis accurately. The proposed
AONeus was able to recover underlying scene geometry
alongalltheaxes.
In Fig. 9(b), we show the results for the turtle mesh
for various baselines. To visualize the ambiguities associ-
ated with camera and sonar modalities and the benefit of
thefusionalgorithm,werenderedthereconstructedmeshes
with a virtual camera pointing in Y-axis. Hence, the ren-
dered images are projections of the reconstructed mesh on
ZX-plane. As we decrease the baseline (top to bottom),
for NeuS, we observe an increasing loss of features along
depth direction: the back legs of the turtle are progres-
sively lost and depth-reconstruction worsens with decreas-
ingbaselines. Forsonar-onlymethods,significantambigui-
tiesalongtheelevationaxiscanbeseenacrossallbaselines:
duetothelimitedtranslationofthesonar,thecollectedmea-
surements are not enough to constrain and resolve the tur-
tleshelladequately. OurframeworkAONeuSintegratesor-
thogonalinformationfrombothimagingmodalitiestoyield
reconstructions of higher quality across all baselines: all
features of the turtle including its shell and its back legs
areclearlydiscernible. Theseobservationsarefurthersup-
portedbythequantitativeanalysisinTab.1wherewereport
the mean and variance of Chamfer L distance, precision,
1
andrecallofthereconstructionsoverninetrials.Theresults
demonstrate that AONeuS outperforms the existing meth-
ods,particularlywithreducedbaselines. Notethatrecallof
Figure6.Experimentalhardwaresetup.(a)Testwatertankused
NeuSISappearstobeslightlybetterthanAONeuSbutthat toconducttheexperimentsanditsdimensions.(b)Testobject.(c)
isonlybecausetheNeuSISgeneratesalargeblobthatcov- BluefinHoveringAutonomousUnderwaterVehicle(HAUV)and
ers most part of the object. The per-baseline quantitative itsmountedhardware(DidsonimagingsonarandDopplerVeloc-
andqualitativeresultsfortheremainingfourmeshescanbe ityLog(DVL).(d)FLIRBlackflySGigEcamerausedforimage
captureanditswatertightenclosure.
NeuS NeuSIS AONeuS
Chamfer↓ 0.123±0.028 0.130±0.013 0.075±0.006 foundinthesupplementarymaterial.
1.2m Precision↑ 0.653±0.095 0.566±0.043 0.862±0.042
Recall↑ 0.526±0.134 0.836±0.022 0.825±0.056
6.2.ResultsonExperimentally-CapturedData
Chamfer↓ 0.139±0.024 0.134±0.011 0.079±0.005
0.96m Precision↑ 0.602±0.076 0.531±0.031 0.840±0.017
We also perform real-world experiments on an object
Recall↑ 0.470±0.132 0.816±0.031 0.807±0.017
Chamfer↓ 0.205±0.027 0.135±0.011 0.081±0.005 (Fig. 6b) submerged in a water tank (Fig. 6a). Please
0.72m Precision↑ 0.423±0.051 0.537±0.062 0.810±0.032 check the supplementary video for more visualizations of
Recall↑ 0.279±0.071 0.768±0.029 0.792±0.022
thesetup.WeusedaSoundMetricsDIDSONimagingsonar
Chamfer↓ 0.249±0.045 0.139±0.012 0.088±0.006
0.48m Precision↑ 0.337±0.062 0.470±0.028 0.791±0.023 mounted on a Bluefin Hovering Autonomous Underwater
Recall↑ 0.189±0.074 0.706±0.028 0.770±0.023 Vehicle(HAUV)(Fig.6c)tocapturetwosonardatasetsof
Chamfer↓ 0.406±0.087 0.146±0.009 0.111±0.017
the test object with two different elevation apertures 14◦
0.24m Precision↑ 0.223±0.060 0.450±0.028 0.690±0.045
Recall↑ 0.107±0.049 0.587±0.042 0.679±0.042 and28◦. ThevehicleusesanIMUandaDopplerVelocity
Log (DVL) to measure sonar pose information. We asyn-
Table 1. Metrics for synthetic turtle data. Best metrics are chronouslycaptureopticalimagesofthesameobjectusing
bolded. a FLIR Blackfly S GigE 5MP camera (Fig. 6d) with cam-
6era pose information computed with COLMAP. The sonar NeuS NeuSIS AONeuS
and camera trajectories were aligned post-capture. Sim-
ilar to the simulation setup, both camera and sonar fol-
lowedanapproximately1.2mnon-degeneratelineartrajec-
tory,whichwelatersub-sampledintothesame5baselines.
Webenchmarkedourmethodagainstthreealgorithms: The
COLMAPbasedsensorfusionmethodintroducedin[20]1,
NeuS [46], and NeuSIS [39]. For each dataset and sensor
baseline,weexecutedeachmethodsixtimeswithrandomly
initializedseedsexceptthatof [20],whichisdeterministic.
Qualitatively, we observe in Fig. 10 that AONeuS out-
puts a more complete shape across baselines compared to
sonar-only (NeuSIS) and camera-only (NeuS) only meth-
ods:thehole,twolegs,andcrossbarareclearlydiscernible.
Conversely, when using only sonar, parts of the object are Deviation Deviation Deviation
notwellreconstructedaswecanobserve,forexample,with
the long leg with NeuSIS at 14◦. Similarly, camera-only Figure7. Per-axiserrordistributions. At0.2xbaselineforthe
turtle example, we plot the distributions of deviations from the
methods result in the loss of features such as the hole ac-
ground truth mesh along all three axes for NeuS, NeuSIS, and
companied with significant introduced depth errors. We
AONeuSreconstructions. TheNeuSreconstructionhaslargerZ
quantify the results in Tab. 2, where we report the mean
errors,noticeablefromthelongtail,whereasNeuSISreconstruc-
andvarianceoftheChamferL distance,precision,andre-
1 tionhaslargerX errors. AONeuShastighterdistributionsalong
call against the ground truth mesh computed over six tri-
allthreeaxescomparedtoNeuSorNeuSISshowingthatthepro-
als with different random seeds for training. We observe posedtechniquetakesthebestofbothoftheimagingmodalities.
thatthefusionoftheacousticandopticalsignalsgenerates
higher quality reconstruction, even with very short base-
linesmeasuringonly24cm,asindicatedbythemeanvalue togramthesedeviationsalongallthreeaxesandshowthem
ofeachmetric. WhencomparingAONeuswithsonar-only along rows in the Fig. 7. We have repeated this procedure
methods(NeuSIS),wenotethat,despitetheincreasedele- forNeuS,NeuSIS,andAONeuSandshowthemalongthe
vationambiguityintroducedbythe28◦ elevationaperture, columns.
ourtechniqueisabletoleveragecamerainformationandits Fromthedata,wecanobserve(1)NeuShaslargedevi-
constraints in the x and y axes to resolve spatial locations ationsalongZ axes,(2)NeuSIShaslargedeviationsalong
thatareotherwiseunder-constrainedwhensolelyrelyingon X axes. These results are consistent with the ambigui-
sonar.Techniquesthatrelyonacameraonly(NeuS)exhibit tiesassociatedwiththeirrespectivemeasurementprocesses.
adecreaseinperformanceasthesensorbaselineisreduced. AONeuS has low spread on all axes as it captures the best
Complementing camera with sonar information introduces ofbothcamera(NeuS)andsonar(NeuSIS)imagingmodal-
constraints in the depth direction easing the resolution of ities.
depthwhichisknowntobedifficulttoresolvewithlimited
cameramotion. Weadditionallyemphasizethevarianceof 7.2.MultimodalSensingisBetterConditioned
the reconstruction quality as measured by the variance of
Thestrongempiricalperformanceofourmultimodalrecon-
the Chamfer distance: the fusion of both modalities result
structionscanbeexplainedintermsofsystemconditioning.
inoutputsthataremorerobusttotherandomnessoftheal-
Given point correspondences between measurements, it is
gorithm(i.e. networkinitialization,pointsamples,etc.).
fareasiertotriangulateapointusingmultimodalacoustic-
opticalmeasurementsthancamera-onlyorsonar-onlymea-
7.DiscussionandAnalysis
surements.
7.1.Distributionofper-AxisErrors To illustrate this fact, consider a point P = [X,Y,Z]t
thatisobservedbyanacoustic-opticalsensorfromtwopo-
In Fig. 7, we visualize the per-axis deviations from the
sitions. The sonar’s azimuthal plane is the yz plane, in its
ground truth for the synthetic turtle scene at 0.24 m base-
own coordinate system. The camera’s image plane is the
line. We compute per-axis deviations by first determining
z = f plane,initsowncoordinatesystem. Withoutlossof
the closest vertex in the dense ground truth mesh and tak-
generality,assumethesensor’scoordinatesystematitsini-
ingabsolutedifferencesinx,y,andzcoordinates. Wehis-
tial location is the world coordinate system and its coordi-
1COLMAPoutputsasparsepointcloud.Hence,ameshwascomputed natesystematitssecondpositionisdescribedbyarotation
usingtheballpivotingalgorithm[9]. Randtranslationt=[t x,t y,t z]. Thatis,thecoordinateof
7
srorre
X
srorre
Y
srorre
Z
tnuoC
tnuoC
tnuoCpointP inthenewcoordinatesystemisP′ =RP +t.
Under this model, the acoustic-optical sensor records 8
measurements:
[1,0,0]P [0,1,0]P
x =f , y =f , 0 10 20 30 40 50 60 70 80 90 100
c [0,0,1]P c [0,0,1]P 5(Acam)
(cid:16)[0,1,0]P(cid:17)
R=∥P∥, θ =tan−1 ,
[0,0,1]P
rtP +t rtP +t
x′ =f 1 x, y′ =f 2 y,
c rtP +t c rtP +t
3 z 3 z 0 10 20 30 40 50 60 70 80 90 100
R′
=(cid:112)
∥R∥2+∥t∥2+2ttRP, θ′
=tan−1(cid:16)rt 2P +t y(cid:17)
,
5(Ason)
rtP +t
3 z
(13)
wherer denotestheithrowofR.
i
Loosely following Negahdaripour [33], Negahdaripour 0 10 20 30 40 50 60 70 80 90 100
et al. [34], we can turn each of these measurements into
5(Amulti)
sevenlinearconstraintsandonenon-linearconstraintonP. Figure 8. System Conditioning. Histograms of the condition
numbersofthecamera-only(top),sonar-only(middle),andmul-
A P =band∥P∥2 =R2with
multi timodal (bottom) forward models. Median condition numbers
 (−f,0,x )   0  are highlighted in red. The acoustic-optical multimodal forward
c
 (0,−f,y c)   0  modelisgenerallybetterconditionedandeasiertoinvert(triangu-
 (0,−1,tan(θ))    0   lation).
A multi =   

tax y nc′′ c (rr θt 3t 3 ′)−−
rt
3ff −rr t 2t 2
rt
2   


andb=   
 t
yf f −t tx y t− − anx y (θc′′ c tt ′z )z
t
z   

.
modality-specificappearancemodulesandrenderingfunc-
ttR (R′)2−R2−∥t∥2 tions. Byextractinginformationfromthesecomplementary
2
modalities,ourframeworkisabletoofferbreakthroughun-
(14)
derwater sensing capabilities for restricted baseline imag-
One can similarly form camera-only, A cam, and sonar- ingscenarios. WehavedemonstratedthatAONeuScanac-
only,A son,forwardmodelsbyconsideringonlyrows1,2, curately reconstruct the geometry of complex 3D objects
4, and5androws3, 6, and7, respectively, ofA multi. By from synthetic as well as noisy, real-world measurements
invertingthesesystems,onecantriangulateP inspace. capturedoverseverelyrestrictedbaselines.
HereweperformMonteCarlosamplingtocomparethe
conditioning of A , A , and A . We sample P While we demonstrate the first neural fusion of camera
cam son multi
uniformlyina1m3 cubecenteredat(0,0,1.5)withedges and sonar measurements, there are many interesting direc-
paralleltothex,y,andzaxis;weassumef =100mm;we tions to explore this amalgamation. In Sec. 5.1.2, we in-
samplet ,t ,andt uniformlyintherange0cmto10cm; troduced a heuristic for weighing camera and sonar mea-
x y z
and we sample the yaw, pitch, and roll between measure- surements. Astructuredwayofcombiningthecameraand
mentsuniformlyintherange−5◦to5◦. sonar data, which is aware of the uncertainties [17, 19] in
Foreachrealizationoftheseparameters,wecomputethe the complementary imaging systems could result in faster
conditionnumber, κ, ofA , A , andA . Were- convergenceratesandbetterreconstructions.
cam son multi
peat this process 50,000 times to form histograms, illus-
The sonar we have used in our implementations are
trated in Fig. 8. The condition number of the multimodal
forward-looking sonars. Fusion algorithms for side-scan
systemisgenerallymuchlowerandthesystemisthuseas-
sonars,synthetic-aperturesonars,sonarsofdifferentranges
iertoinvert;multimodaltriangulationiseasier.
and wavelengths, could be an interesting forward direc-
tion. Similarly,extendingthetechniqueforvariousgeome-
8.Conclusion
tries and materials including multi-object scenes, dynamic
We have introduced and validated a multimodal inverse- scenes,clutteredscenesandscatteringmedia(murkywater)
differentiable-rendering framework for reconstructing 3D wouldmakeAONeuSmorepractical.Finally,on-the-flyre-
surfaceinformationfromcameraandsonarmeasurements. constructionscouldallowonetoselectthebestnextunder-
Ourframeworkcombinescameraandsonarinformationus- waterviewtoimprovereconstructionaccuracyandfurther
ing a unified surface representation module and separate reducetherequiredbaselineandacquisitiontime.
89.Acknowledgements theIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages11682–11692,2020. 3
The authors would like to thank Tianxiang Lin and Jui-Te
[11] Alexandra Carlson, Manikandasriram S. Ramanagopal,
HuangfortheirhelpwithdatacollectionandSarahFriday
Nathan Tseng, Matthew Johnson-Roberson, Ram Vasude-
forprovidingananimationshowcasingtherealexperimen- van, and Katherine A. Skinner. Cloner: Camera-lidar fu-
talsetup. M.Q.,A.H.,andM.K.weresupportedinpartby sionforoccupancygrid-aidedneuralrepresentations. IEEE
ONRgrantN00014-21-1-2482.A.P.wassupportedbyNSF RoboticsandAutomationLetters,8(5):2812–2819,2023. 3
grant2326904. K.Z.andC.A.M.weresupportedinpartby [12] W.Chen,W.Yifan,S.Kuo,andG.Wetzstein. Dehazenerf:
AFOSR Young Investigator Program award no. FA9550- Multiple image haze removal and 3d shape reconstruction
22-1-0208,ONRawardno. N00014-23-1-2752,andaseed usingneuralradiancefields. In3DV,2024. 3,4
grantfromSAAB,Inc. [13] BlenderOnlineCommunity. Blender-a3Dmodellingand
renderingpackage. BlenderFoundation, StichtingBlender
References Foundation,Amsterdam,2022. 5
[14] AkshatDave,YongyiZhao,andAshokVeeraraghavan.PAN-
[1] JanAlbiez,SylvainJoyeux,ChristopherGaudig,JensHill- DORA: Polarization-Aided Neural Decomposition of Radi-
jegerdes, Sven Kroffke, Christian Schoo, Sascha Arnold, ance,page538–556. SpringerNatureSwitzerland,2022. 3
Geovane Mimoso, Pedro Alcantara, Rafael Saback, et al. [15] RobertDeBortoli,FuxinLi,andGeoffreyAHollinger. El-
Flatfish-a compact subsea-resident inspection auv. In evatenet: Aconvolutionalneuralnetworkforestimatingthe
OCEANS2015-MTS/IEEEWashington,pages1–8,201Wa- missingdimensionin2dunderwatersonarimages. In2019
terfrontStreetNationalHarbor,Maryland20745USA,2015. IEEE/RSJ International Conference on Intelligent Robots
IEEE,IEEE. 1 andSystems(IROS),pages8040–8047.IEEE,2019. 2
[2] Ramazzina Andrea, Bijelic Mario, Walz Stefanie, Sanvito [16] FaustoFerreira, DiogoMachado, GabrieleFerri, Samantha
Alessandro,ScheubleDominik,andHeideFelix. Scattern- Dugelay,andJohnPotter. Underwateropticalandacoustic
erf:Seeingthroughfogwithphysically-basedinverseneural imaging: A time for fusion? a brief overview of the state-
rendering. 2023. 3,4 of-the-art. OCEANS2016MTS/IEEEMonterey,pages1–6,
[3] Sascha Arnold and Bilal Wehbe. Spatial acoustic projec- 2016. 2
tion for 3d imaging sonar reconstruction. In 2022 Inter- [17] Lily Goli, Cody Reading, Silvia Sella´n, Alec Jacobson,
national Conference on Robotics and Automation (ICRA), and Andrea Tagliasacchi. Bayes’ Rays: Uncertainty
pages3054–3060,Philadelphia,2022.IEEE. 2 quantification in neural radiance fields. arXiv preprint
[4] Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil arXiv:2309.03185,2023. 8
Kim, Christian Richardt, James Tompkin, and Matthew [18] Jules S Jaffe. Underwater optical imaging: the past, the
O’Toole. To¨rf: Time-of-flight radiance fields for dynamic present, and the prospects. IEEE Journal of Oceanic En-
sceneviewsynthesis. AdvancesinNeuralInformationPro- gineering,40(3):683–700,2014. 2
cessingSystems,34,2021. 3 [19] Wen Jiang, Boshu Lei, and Kostas Daniilidis. Fisherrf:
[5] MuratDAykinandShahriarNegahdaripour. On3-dtarget Active view selection and uncertainty quantification for
reconstructionfrommultiple2-dforward-scansonarviews. radiance fields using fisher information. arXiv preprint
InOCEANS2015-Genova,pages1–10.IEEE,2015. 2 arXiv:2311.17874,2023. 8
[6] Murat D Aykin and Shahriar Negahdaripour. Three- [20] JasonKim,MeungsukLee,SeokyongSong,ByeongjinKim,
dimensionaltargetreconstructionfrommultiple2-dforward- and Son-Cheol Yu. 3-D Reconstruction of Underwater
scansonarviewsbyspacecarving.IEEEJournalofOceanic Objects Using Image Sequences from Optical Camera and
Engineering,42(3):574–589,2016. 2 Imaging Sonar. In OCEANS 2019 MTS/IEEE SEATTLE,
[7] Murat D Aykin and Shahriar S Negahdaripour. Modeling pages1–6,2019. 2,3,7
2-dlens-basedforward-scansonarimageryfortargetswith [21] YoungchanKim, WonjoonJin, SunghyunCho, andSeung-
diffusereflectance. IEEEjournalofoceanicengineering,41 Hwan Baek. Neural spectro-polarimetric fields. In SIG-
(3):569–582,2016. 2 GRAPHAsia2023ConferencePapers,NewYork,NY,USA,
[8] Mohammadreza Babaee and Shahriar Negahdaripour. 3- 2023.AssociationforComputingMachinery. 3
d object modeling from 2-d occluding contour correspon- [22] Young Min Kim, Christian Theobalt, James Diebel, Jana
dences by opti-acoustic stereo imaging. Computer Vision Kosecka, BranislavMiscusik, andSebastianThrun. Multi-
andImageUnderstanding,132:56–74,2015. 2,3 view image and tof sensor fusion for dense 3d reconstruc-
[9] Fausto Bernardini, Joshua Mittleman, Holly Rushmeier, tion. In2009IEEE12thInternationalConferenceonCom-
Cla´udioSilva, andGabrielTaubin. Theball-pivotingalgo- puter Vision Workshops, ICCV Workshops, pages 1542–
rithmforsurfacereconstruction. IEEEtransactionsonvisu- 1549,2009. 3
alizationandcomputergraphics,5(4):349–359,1999. 7 [23] Samuel Lensgraf, Amy Sniffen, Zachary Zitzewitz, Evan
[10] MarioBijelic,TobiasGruber,FahimMannan,FlorianKraus, Honnold,JenniferJain,WeifuWang,AlbertoLi,andDevin
Werner Ritter, Klaus Dietmayer, and Felix Heide. See- Balkcom. Droplet:Towardsautonomousunderwaterassem-
ingthroughfogwithoutseeingfog: Deepmultimodalsen- blyofmodularstructures. InProceedingsofRobotics: Sci-
sor fusion in unseen adverse weather. In Proceedings of enceandSystems,2021. 1
9[24] DeborahLevy,AmitPeleg,NaamaPearl,DanRosenbaum, [38] MatteoPoggi,PierluigiZamaRamirez,FabioTosi,Samuele
DeryaAkkaynak,SimonKorman,andTaliTreibitz.Seathru- Salti, Luigi Di Stefano, and Stefano Mattoccia. Cross-
nerf:Neuralradiancefieldsinscatteringmedia. InProceed- spectralneuralradiancefields. InProceedingsoftheInter-
ingsoftheIEEE/CVFConferenceonComputerVisionand nationalConferenceon3DVision,2022. 3DV. 3
PatternRecognition,pages56–65,2023. 3,4 [39] Mohamad Qadri, Michael Kaess, and Ioannis Gkioulekas.
[25] Tianxiang Lin, Akshay Hinduja, Mohamad Qadri, and Neuralimplicitsurfacereconstructionusingimagingsonar.
Michael Kaess. Conditional gans for sonar image filtering In2023IEEEInternationalConferenceonRoboticsandAu-
withapplicationstounderwateroccupancymapping.In2023 tomation(ICRA),pages1040–1047.IEEE,2023. 1,2,3,4,
IEEEInternationalConferenceonRoboticsandAutomation 5,7
(ICRA),pages1048–1054.IEEE,2023. 1 [40] Albert Reed, Juhyeon Kim, Thomas Blanford, Adithya
[26] DavidB.Lindell,MatthewO’Toole,andGordonWetzstein. Pediredla, Daniel Brown, and Suren Jayasuriya. Neural
Single-Photon3DImagingwithDeepSensorFusion. ACM VolumetricReconstructionforCoherentSyntheticAperture
Trans.Graph.(SIGGRAPH),(4),2018. 3 Sonar.ACMTransactionsonGraphics,42(4):113:1–113:20,
2023. 2
[27] AfeiLiu,ShuanghuiZhang,ChiZhang,ShuaifengZhi,and
Xiang Li. Ranerf: Neural 3-d reconstruction of space tar- [41] JohannesLSchonbergerandJan-MichaelFrahm. Structure-
getsfromisarimagesequences. IEEETransactionsonGeo- from-motion revisited. In Proceedings of the IEEE con-
scienceandRemoteSensing,61:1–15,2023. 3 ference on computer vision and pattern recognition, pages
4104–4113,2016. 3
[28] Haowen Liu, Monika Roznere, and Alberto Quattrini Li.
Deep underwater monocular depth estimation with single- [42] Advaith Venkatramanan Sethuraman, Manikandasri-
beam echosounder. In 2023 IEEE International Confer- ram Srinivasan Ramanagopal, and Katherine A Skinner.
enceonRoboticsandAutomation(ICRA),pages1090–1097. Waternerf: Neuralradiancefieldsforunderwaterscenes. In
IEEE,2023. 1 OCEANS2023-MTS/IEEEUSGulfCoast,pages1–7.IEEE,
2023. 3,4
[29] XiaoxiaoLong,ChengLin,PengWang,TakuKomura,and
[43] PedroVTeixeira,MichaelKaess,FranzSHover,andJohnJ
WenpingWang. Sparseneus: Fastgeneralizableneuralsur-
facereconstructionfromsparseviews. ECCV,2022. 2 Leonard.Underwaterinspectionusingsonar-basedvolumet-
ric submaps. In 2016 IEEE/RSJ International Conference
[30] Anagh Malik, Parsa Mirdehghan, Sotiris Nousias, Kiri-
onIntelligentRobotsandSystems(IROS),pages4288–4295.
akosN.Kutulakos, andDavidB.Lindell. Transientneural
IEEE,2016. 2
radiancefieldsforlidarviewsynthesisand3dreconstruction.
[44] Nguyen Dinh Tinh and T Dang Khanh. A new imaging
NeurIPS,2023. 3
geometrymodel formulti-receiversynthetic aperturesonar
[31] Fabio Menna, Panagiotis Agrafiotis, and Andreas Geor-
consideringvariationofthespeedofsoundinseawater.IEIE
gopoulos. Stateoftheartandapplicationsinarchaeological
Transactions on Smart Processing and Computing, 10(4):
underwater3drecordingandmapping. JournalofCultural
302–308,2021. 2
Heritage,33:231–248,2018. 2
[45] JinkunWang,TixiaoShan,andBrendanEnglot.Underwater
[32] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
terrain reconstruction from forward-looking sonar imagery.
JonathanT.Barron,RaviRamamoorthi,andRenNg. Nerf:
In2019InternationalConferenceonRoboticsandAutoma-
Representingscenesasneuralradiancefieldsforviewsyn-
tion(ICRA),pages3471–3477.IEEE,2019. 1
thesis. InECCV,2020. 2
[46] PengWang,LingjieLiu,YuanLiu,ChristianTheobalt,Taku
[33] ShahriarNegahdaripour. Applicationofforward-scansonar
Komura, and Wenping Wang. Neus: Learning neural im-
stereofor3-dscenereconstruction. IEEEjournalofoceanic
plicit surfaces by volume rendering for multi-view recon-
engineering,45(2):547–562,2018. 1,5,8
struction. AdvancesinNeuralInformationProcessingSys-
[34] ShahriarNegahdaripour,HichamSekkati,andHamedPirsi- tems,34:27171–27183,2021. 1,2,3,4,5,7
avash. Opti-acousticstereoimaging: Onsystemcalibration
[47] Yusheng Wang, Yonghoon Ji, Hanwool Woo, Yusuke
and3-dtargetreconstruction. IEEETransactionsonimage
Tamura, Atsushi Yamashita, and Asama Hajime. 3d oc-
processing,18(6):1203–1214,2009. 8
cupancy mapping framework based on acoustic camera in
[35] Shahriar Negahdaripour, Victor M Milenkovic, Nikan underwaterenvironment. IFAC-PapersOnLine,51(22):324–
Salarieh,andMahsaMirzargar. Refining3-dobjectmodels 330,2018. 2
constructedfrommultiplefssonarimagesbyspacecarving. [48] Yusheng Wang, Yonghoon Ji, Hanwool Woo, Yusuke
InOCEANS2017-Anchorage,pages1–9.IEEE,2017. 2 Tamura, Atsushi Yamashita, and Hajime Asama. Three-
[36] MarkNishimura,DavidBLindell,ChristopherMetzler,and dimensional underwater environment reconstruction with
Gordon Wetzstein. Disambiguating monocular depth esti- graph optimization using acoustic camera. In 2019
mationwithasingletransient. InEuropeanConferenceon IEEE/SICEInternationalSymposiumonSystemIntegration
ComputerVision,pages139–155.Springer,2020. 3 (SII),pages28–33.IEEE,2019. 2
[37] Michael Oechsle, Songyou Peng, and Andreas Geiger. [49] YushengWang,YonghoonJi,DingyuLiu,HiroshiTsuchiya,
Unisurf: Unifying neural implicit surfaces and radiance AtsushiYamashita,andHajimeAsama.Elevationangleesti-
fields for multi-view reconstruction. In International Con- mationin2dacousticimagesusingpseudofrontview. IEEE
ferenceonComputerVision(ICCV),2021. 2 RoboticsandAutomationLetters,6(2):1535–1542,2021. 2
10[50] Eric Westman and Michael Kaess. Wide aperture imag-
ing sonar reconstruction using generative models. In 2019
IEEE/RSJ International Conference on Intelligent Robots
andSystems(IROS),pages8067–8074.IEEE,2019. 2
[51] EricWestman, Ioannis Gkioulekas, and Michael Kaess. A
theoryoffermatpathsfor3dimagingsonarreconstruction.
In 2020 IEEE/RSJ International Conference on Intelligent
RobotsandSystems(IROS),pages5082–5088.IEEE,2020.
2
[52] Eric Westman, Ioannis Gkioulekas, and Michael Kaess.
A volumetric albedo framework for 3d imaging sonar re-
construction. In 2020 IEEE International Conference on
RoboticsandAutomation(ICRA),pages9645–9651.IEEE,
2020. 2
[53] LiorYariv,YoniKasten,DrorMoran,MeiravGalun,Matan
Atzmon, BasriRonen, andYaronLipman. Multiviewneu-
ralsurfacereconstructionbydisentanglinggeometryandap-
pearance. AdvancesinNeuralInformationProcessingSys-
tems,33:2492–2502,2020. 2
[54] Lior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman.
Volume rendering of neural implicit surfaces. In Thirty-
FifthConferenceonNeuralInformationProcessingSystems,
2021. 2
[55] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin,
Pratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron,
andBenMildenhall.Bakedsdf:Meshingneuralsdfsforreal-
timeviewsynthesis. arXiv,2023. 2
[56] HaidongZhu,YuyinSun,ChiLiu,LuXia,JiajiaLuo,Nan
Qiao,RamNevatia,andCheng–HaoKuo. Multimodalneu-
ralradiancefield.In2023IEEEInternationalConferenceon
RoboticsandAutomation(ICRA),pages9393–9399,2023.3
11GT NeuS NeuSIS AONeuS GT NeuS NeuSIS AONeuS
(a) (b)
Figure9.SimulatedReconstructionsOverVaryingBaselines.((5x5collectionofimages)1stcolumn:Groundtruthmeshandmeasure-
mentgeometry. 2ndcolumn: Sonar-onlyneuralsurfacereconstructions. 3rdrow: RGB-onlyneuralsurfacereconstructions. 4thcolumn:
Multimodalbackprojectionreconstruction. 5thcolumn: Multimodalneuralsurfacereconstructions. (Therowsshow1m, 80cm, 60cm,
40cm,and20cmbaselinereconstructions,respectively(orsomethinglikethat).)(Thisistransposedfromwhatwediscussedyesterdayand
includesmultimodalback-prop).).
Sonardataset1 Sonardataset2
14◦elevationangle 28◦elevationangle
Kimetal. NeuSIS AONeuS NeuSIS AONeuS
Baseline Metric NeuS
(2019) (14◦) (14◦) (28◦) (28◦)
1.2m ChamferL1↓ 0.092±0.015 0.177 0.187±0.032 0.093±0.005 0.217±0.036 0.105±0.008
Precision↑ 0.693±0.066 0.336 0.482±0.063 0.661±0.025 0.444±0.026 0.602±0.023
Recall↑ 0.679±0.098 0.387 0.355±0.051 0.707±0.047 0.318±0.040 0.614±0.037
0.96m ChamferL1↓ 0.107±0.013 0.182 0.180±0.026 0.095±0.008 0.214±0.032 0.097±0.006
Precision↑ 0.661±0.048 0.318 0.514±0.036 0.689±0.042 0.462±0.032 0.645±0.038
Recall↑ 0.563±0.084 0.345 0.370±0.053 0.695±0.037 0.314±0.043 0.648±0.038
0.72m ChamferL1↓ 0.127±0.013 0.178 0.174±0.034 0.094±0.008 0.202±0.043 0.095±0.010
Precision↑ 0.651±0.047 0.368 0.550±0.080 0.646±0.035 0.502±0.063 0.640±0.037
Recall↑ 0.500±0.062 0.396 0.374±0.061 0.698±0.067 0.352±0.065 0.668±0.073
0.48m ChamferL1↓ 0.150±0.022 0.179 0.180±0.032 0.097±0.012 0.215±0.034 0.098±0.010
Precision↑ 0.626±0.055 0.324 0.521±0.050 0.656±0.065 0.473±0.037 0.607±0.050
Recall↑ 0.415±0.022 0.218 0.372±0.065 0.678±0.075 0.336±0.061 0.653±0.063
0.24m ChamferL1↓ 0.167±0.012 0.198 0.173±0.020 0.087±0.007 0.203±0.023 0.089±0.007
Precision↑ 0.580±0.031 0.305 0.535±0.063 0.699±0.057 0.501±0.021 0.651±0.049
Recall↑ 0.363±0.056 0.140 0.347±0.030 0.760±0.030 0.317±0.024 0.705±0.037
Table2. Forthehardwarereconstructionof”H”objectinFig.10,wereportthemeanandstandarddeviationoftheChamferL1distance,
precision,andrecallcomparedtothegroundtruth(obtainedfromalaserscanoftherealstructure)forvariousreconstructiontechniques.
Wecomputedthestandarddeviationover6trials.Formetriccalculation,weusedathresholdof0.05m.
12
enalP
retsboL
hsifratS
llehS
eltruT
m
2.1
m
69.0
m
27.0
m
84.0
m
42.0NeuSIS NeuSIS AONeuS AONeuS
GT NeuS (14deg) (28deg) Kim et al. (14deg) (28deg)
m
2
.
1
~
m
6
9
.
0
~
m
2
7
.
0
~
m
8
4
.
0
~
m
4
2
.
0
~
Figure10. ExperimentalResultswith“h”Object. Asthebaselinediminishes,NeuSexhibitsincreasingamountofdistortionalongthe
depthdirectionascanseenattheintersectionoftheshortpilingandcrossbaratthe0.72mand0.96mbaselines.NeuSISsimilarlygenerates
reconstructionswithsignificanterrors(forexample,thelongpilingispoorlyreconstructedwiththe14◦elevation). Conversely,AONeuS
consistentlyproducesfaithfulreconstructionsacrossarangeofbaselines.
139.1.VarianceoftheDensityFieldOverRealizations
oftheAlgorithm
NeuS NeuSIS AONeuS
Chamfer↓ 0.147±0.017 0.187±0.012 0.105±0.019
Neus NeuSIS AONeus 1.2m Precision↑ 0.448±0.073 0.328±0.020 0.592±0.110
s Recall↑ 0.454±0.100 0.460±0.037 0.626±0.090
s n o itc
u
rtsd e e s
tn
e
re
0.96m
PC
r
Rh
e
ea
c
cm
is
aif loe lnr ↑↓
↑
00
0
..
.
31
3
96 97
4
8±
±
±0
0
0.
.
.0
0
12
7
07
0
6
0
0
0.
.
.2
3
40
0
43
2
5±
±
±0
0
0.
.
.0
0
01
2
34
5
3
0
0
0.
.
.1
5
54
3
332
4
±±
±
00
0
..
.
20
2
27
3
84
0
n
o c
e
Rffid
h tiw 0.72m
PC rh ea cm isif oe nr↓
↑
0 0. .1 39 51 7± ±0 0. .0 01 59
3
0 0. .2 22 75 5± ±0 0. .0 03 40
0
0 0. .1 52 38 3± ±0 0. .0 12 11
2
Recall↑ 0.417±0.077 0.390±0.040 0.576±0.105
Chamfer↓ 0.237±0.039 0.243±0.019 0.143±0.015
0.48m Precision↑ 0.321±0.062 0.257±0.012 0.523±0.069
Recall↑ 0.392±0.091 0.370±0.017 0.577±0.043
Chamfer↓ 0.293±0.044 0.272±0.055 0.186±0.034
0.24m Precision↑ 0.251±0.049 0.225±0.039 0.440±0.116
Recall↑ 0.277±0.103 0.290±0.076 0.483±0.130
Figure11. DistributionofReconstructionsoftheRealObject.
Top: Different Reconstructions from different random seeds for Table4.Quantitativemetricsforthelobstermesh.
the20%baseline. Bottom: Histogramdistributionofthevariance
ofthedensityfieldVAR(σ(x))vs. voxelcount. Neus’sdistribu-
tionisheavilytailedwhileNeuSIS’sdistributionexhibitsalarge
mean and variance. AONeuS’s distribution is light-tailed with a
smallmeanandthereforeitisbetterconstrained. NeuS NeuSIS AONeuS
Chamfer↓ 0.108±0.020 0.177±0.010 0.088±0.022
1.2m Precision↑ 0.585±0.070 0.335±0.021 0.714±0.106
9.2.Additionaltables Recall↑ 0.722±0.061 0.894±0.031 0.886±0.026
Chamfer↓ 0.122±0.030 0.170±0.014 0.089±0.016
Tables3to6provideadditionalquantitativemetricsforour 0.96m Precision↑ 0.539±0.117 0.352±0.032 0.720±0.063
syntheticexperiments. Recall↑ 0.689±0.122 0.848±0.022 0.829±0.033
Chamfer↓ 0.159±0.035 0.171±0.010 0.126±0.035
0.72m Precision↑ 0.435±0.089 0.352±0.033 0.571±0.118
NeuS NeuSIS AONeuS Recall↑ 0.550±0.127 0.791±0.026 0.764±0.092
Chamfer↓ 0.112±0.018 0.197±0.011 0.117±0.014 Chamfer↓ 0.255±0.041 0.170±0.006 0.143±0.046
1.2m Precision↑ 0.652±0.058 0.295±0.019 0.582±0.053 0.48m Precision↑ 0.175±0.059 0.372±0.019 0.486±0.121
Recall↑ 0.650±0.044 0.643±0.025 0.741±0.028 Recall↑ 0.201±0.081 0.742±0.039 0.657±0.080
Chamfer↓ 0.144±0.021 0.200±0.019 0.134±0.016 Chamfer↓ 0.548±0.144 0.191±0.006 0.196±0.033
0.96m Precision↑ 0.559±0.045 0.291±0.014 0.575±0.017 0.24m Precision↑ 0.067±0.038 0.344±0.023 0.377±0.088
Recall↑ 0.579±0.042 0.650±0.043 0.697±0.027 Recall↑ 0.071±0.045 0.627±0.069 0.516±0.072
Chamfer↓ 0.146±0.021 0.200±0.016 0.141±0.023
0.72m Precision↑ 0.554±0.052 0.289±0.029 0.558±0.035 Table5.Quantitativemetricsfortheseastarmesh.
Recall↑ 0.599±0.039 0.629±0.067 0.689±0.048
Chamfer↓ 0.174±0.016 0.199±0.012 0.146±0.033
0.48m Precision↑ 0.468±0.039 0.287±0.047 0.533±0.087
Recall↑ 0.516±0.040 0.569±0.076 0.668±0.044
Chamfer↓ 0.223±0.046 0.182±0.011 0.166±0.034
0.24m Precision↑ 0.341±0.090 0.358±0.042 0.451±0.103 NeuS NeuSIS AONeuS
Recall↑ 0.413±0.072 0.555±0.069 0.644±0.045
Chamfer↓ 0.063±0.002 0.077±0.010 0.066±0.006
1.2m Precision↑ 0.847±0.011 0.754±0.066 0.858±0.024
Table3.Quantitativemetricsfortheairplanemesh. Recall↑ 0.941±0.018 0.814±0.035 0.844±0.038
Chamfer↓ 0.068±0.005 0.078±0.012 0.078±0.006
0.96m Precision↑ 0.833±0.023 0.756±0.072 0.816±0.022
Recall↑ 0.929±0.020 0.803±0.043 0.794±0.031
Chamfer↓ 0.078±0.006 0.091±0.016 0.090±0.006
0.72m Precision↑ 0.769±0.035 0.747±0.086 0.774±0.029
Recall↑ 0.876±0.044 0.746±0.075 0.730±0.021
Chamfer↓ 0.107±0.014 0.107±0.017 0.098±0.009
0.48m Precision↑ 0.620±0.064 0.791±0.077 0.714±0.031
Recall↑ 0.690±0.084 0.676±0.078 0.691±0.037
Chamfer↓ 0.199±0.029 0.168±0.008 0.109±0.010
0.24m Precision↑ 0.309±0.028 0.832±0.041 0.667±0.048
Recall↑ 0.302±0.029 0.465±0.032 0.640±0.047
Table6.Quantitativemetricsfortheshellmesh.
14
margotsiH noitubirtsid