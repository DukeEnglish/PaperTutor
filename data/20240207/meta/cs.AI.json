[
    {
        "title": "HASSOD: Hierarchical Adaptive Self-Supervised Object Detection",
        "authors": "Shengcao CaoDhiraj JoshiLiang-Yan GuiYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2402.03311v1",
        "entry_id": "http://arxiv.org/abs/2402.03311v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03311v1",
        "summary": "The human visual perception system demonstrates exceptional capabilities in\nlearning without explicit supervision and understanding the part-to-whole\ncomposition of objects. Drawing inspiration from these two abilities, we\npropose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a\nnovel approach that learns to detect objects and understand their compositions\nwithout human supervision. HASSOD employs a hierarchical adaptive clustering\nstrategy to group regions into object masks based on self-supervised visual\nrepresentations, adaptively determining the number of objects per image.\nFurthermore, HASSOD identifies the hierarchical levels of objects in terms of\ncomposition, by analyzing coverage relations between masks and constructing\ntree structures. This additional self-supervised learning task leads to\nimproved detection performance and enhanced interpretability. Lastly, we\nabandon the inefficient multi-round self-training process utilized in prior\nmethods and instead adapt the Mean Teacher framework from semi-supervised\nlearning, which leads to a smoother and more efficient training process.\nThrough extensive experiments on prevalent image datasets, we demonstrate the\nsuperiority of HASSOD over existing methods, thereby advancing the state of the\nart in self-supervised object detection. Notably, we improve Mask AR from 20.2\nto 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page:\nhttps://HASSOD-NeurIPS23.github.io.",
        "updated": "2024-02-05 18:59:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03311v1"
    },
    {
        "title": "V-IRL: Grounding Virtual Intelligence in Real Life",
        "authors": "Jihan YangRunyu DingEllis BrownXiaojuan QiSaining Xie",
        "links": "http://arxiv.org/abs/2402.03310v1",
        "entry_id": "http://arxiv.org/abs/2402.03310v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03310v1",
        "summary": "There is a sensory gulf between the Earth that humans inhabit and the digital\nrealms in which modern AI agents are created. To develop AI agents that can\nsense, think, and act as flexibly as humans in real-world settings, it is\nimperative to bridge the realism gap between the digital and physical worlds.\nHow can we embody agents in an environment as rich and diverse as the one we\ninhabit, without the constraints imposed by real hardware and control? Towards\nthis end, we introduce V-IRL: a platform that enables agents to scalably\ninteract with the real world in a virtual yet realistic environment. Our\nplatform serves as a playground for developing agents that can accomplish\nvarious practical tasks and as a vast testbed for measuring progress in\ncapabilities spanning perception, decision-making, and interaction with\nreal-world data across the entire globe.",
        "updated": "2024-02-05 18:59:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03310v1"
    },
    {
        "title": "Do Diffusion Models Learn Semantically Meaningful and Efficient Representations?",
        "authors": "Qiyao LiangZiming LiuIla Fiete",
        "links": "http://arxiv.org/abs/2402.03305v1",
        "entry_id": "http://arxiv.org/abs/2402.03305v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03305v1",
        "summary": "Diffusion models are capable of impressive feats of image generation with\nuncommon juxtapositions such as astronauts riding horses on the moon with\nproperly placed shadows. These outputs indicate the ability to perform\ncompositional generalization, but how do the models do so? We perform\ncontrolled experiments on conditional DDPMs learning to generate 2D spherical\nGaussian bumps centered at specified $x$- and $y$-positions. Our results show\nthat the emergence of semantically meaningful latent representations is key to\nachieving high performance. En route to successful performance over learning,\nthe model traverses three distinct phases of latent representations: (phase A)\nno latent structure, (phase B) a 2D manifold of disordered states, and (phase\nC) a 2D ordered manifold. Corresponding to each of these phases, we identify\nqualitatively different generation behaviors: 1) multiple bumps are generated,\n2) one bump is generated but at inaccurate $x$ and $y$ locations, 3) a bump is\ngenerated at the correct $x$ and y location. Furthermore, we show that even\nunder imbalanced datasets where features ($x$- versus $y$-positions) are\nrepresented with skewed frequencies, the learning process for $x$ and $y$ is\ncoupled rather than factorized, demonstrating that simple vanilla-flavored\ndiffusion models cannot learn efficient representations in which localization\nin $x$ and $y$ are factorized into separate 1D tasks. These findings suggest\nthe need for future work to find inductive biases that will push generative\nmodels to discover and exploit factorizable independent structures in their\ninputs, which will be required to vault these models into more data-efficient\nregimes.",
        "updated": "2024-02-05 18:58:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03305v1"
    },
    {
        "title": "Nevermind: Instruction Override and Moderation in Large Language Models",
        "authors": "Edward Kim",
        "links": "http://arxiv.org/abs/2402.03303v1",
        "entry_id": "http://arxiv.org/abs/2402.03303v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03303v1",
        "summary": "Given the impressive capabilities of recent Large Language Models (LLMs), we\ninvestigate and benchmark the most popular proprietary and different sized open\nsource models on the task of explicit instruction following in conflicting\nsituations, e.g. overrides. These include the ability of the model to override\nthe knowledge within the weights of the model, the ability to override (or\nmoderate) extracted knowledge in the prompt, and lastly the ability to perform\na full jailbreak. Experimentation performed suggest several key findings to\nimprove instruction following - larger models perform the best in following\ninstructions that override internal and contextual instructions, and are\nobedient, even to a fault. When scaling to longer contexts via rope scaling, a\nsignificant buffer needs to be maintained from the edge of the perplexity cliff\nin order to maintain instruction following capabilities. Finally, we observe\nimproving instruction following, and subsequently instruction\noverrides/jailbreaks, is fundamentally at odds with the ability of a language\nmodel to follow given safety filters or guidelines. Thus, we postulate the most\neffective approach for safe, trustworthy AI should be dealt external to the LLM\nitself.",
        "updated": "2024-02-05 18:58:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03303v1"
    },
    {
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "authors": "Zhihong ShaoPeiyi WangQihao ZhuRunxin XuJunxiao SongMingchuan ZhangY. K. LiY. WuDaya Guo",
        "links": "http://arxiv.org/abs/2402.03300v2",
        "entry_id": "http://arxiv.org/abs/2402.03300v2",
        "pdf_url": "http://arxiv.org/pdf/2402.03300v2",
        "summary": "Mathematical reasoning poses a significant challenge for language models due\nto its complex and structured nature. In this paper, we introduce DeepSeekMath\n7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B\nmath-related tokens sourced from Common Crawl, together with natural language\nand code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the\ncompetition-level MATH benchmark without relying on external toolkits and\nvoting techniques, approaching the performance level of Gemini-Ultra and GPT-4.\nSelf-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\nThe mathematical reasoning capability of DeepSeekMath is attributed to two key\nfactors: First, we harness the significant potential of publicly available web\ndata through a meticulously engineered data selection pipeline. Second, we\nintroduce Group Relative Policy Optimization (GRPO), a variant of Proximal\nPolicy Optimization (PPO), that enhances mathematical reasoning abilities while\nconcurrently optimizing the memory usage of PPO.",
        "updated": "2024-02-06 18:39:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03300v2"
    }
]