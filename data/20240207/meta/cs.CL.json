[
    {
        "title": "Nevermind: Instruction Override and Moderation in Large Language Models",
        "authors": "Edward Kim",
        "links": "http://arxiv.org/abs/2402.03303v1",
        "entry_id": "http://arxiv.org/abs/2402.03303v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03303v1",
        "summary": "Given the impressive capabilities of recent Large Language Models (LLMs), we\ninvestigate and benchmark the most popular proprietary and different sized open\nsource models on the task of explicit instruction following in conflicting\nsituations, e.g. overrides. These include the ability of the model to override\nthe knowledge within the weights of the model, the ability to override (or\nmoderate) extracted knowledge in the prompt, and lastly the ability to perform\na full jailbreak. Experimentation performed suggest several key findings to\nimprove instruction following - larger models perform the best in following\ninstructions that override internal and contextual instructions, and are\nobedient, even to a fault. When scaling to longer contexts via rope scaling, a\nsignificant buffer needs to be maintained from the edge of the perplexity cliff\nin order to maintain instruction following capabilities. Finally, we observe\nimproving instruction following, and subsequently instruction\noverrides/jailbreaks, is fundamentally at odds with the ability of a language\nmodel to follow given safety filters or guidelines. Thus, we postulate the most\neffective approach for safe, trustworthy AI should be dealt external to the LLM\nitself.",
        "updated": "2024-02-05 18:58:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03303v1"
    },
    {
        "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
        "authors": "Zhihong ShaoPeiyi WangQihao ZhuRunxin XuJunxiao SongMingchuan ZhangY. K. LiY. WuDaya Guo",
        "links": "http://arxiv.org/abs/2402.03300v2",
        "entry_id": "http://arxiv.org/abs/2402.03300v2",
        "pdf_url": "http://arxiv.org/pdf/2402.03300v2",
        "summary": "Mathematical reasoning poses a significant challenge for language models due\nto its complex and structured nature. In this paper, we introduce DeepSeekMath\n7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B\nmath-related tokens sourced from Common Crawl, together with natural language\nand code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the\ncompetition-level MATH benchmark without relying on external toolkits and\nvoting techniques, approaching the performance level of Gemini-Ultra and GPT-4.\nSelf-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\nThe mathematical reasoning capability of DeepSeekMath is attributed to two key\nfactors: First, we harness the significant potential of publicly available web\ndata through a meticulously engineered data selection pipeline. Second, we\nintroduce Group Relative Policy Optimization (GRPO), a variant of Proximal\nPolicy Optimization (PPO), that enhances mathematical reasoning abilities while\nconcurrently optimizing the memory usage of PPO.",
        "updated": "2024-02-06 18:39:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03300v2"
    },
    {
        "title": "GUARD: Role-playing to Generate Natural-language Jailbreakings to Test Guideline Adherence of Large Language Models",
        "authors": "Haibo JinRuoxi ChenAndy ZhouJinyin ChenYang ZhangHaohan Wang",
        "links": "http://arxiv.org/abs/2402.03299v1",
        "entry_id": "http://arxiv.org/abs/2402.03299v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03299v1",
        "summary": "The discovery of \"jailbreaks\" to bypass safety filters of Large Language\nModels (LLMs) and harmful responses have encouraged the community to implement\nsafety measures. One major safety measure is to proactively test the LLMs with\njailbreaks prior to the release. Therefore, such testing will require a method\nthat can generate jailbreaks massively and efficiently. In this paper, we\nfollow a novel yet intuitive strategy to generate jailbreaks in the style of\nthe human generation. We propose a role-playing system that assigns four\ndifferent roles to the user LLMs to collaborate on new jailbreaks. Furthermore,\nwe collect existing jailbreaks and split them into different independent\ncharacteristics using clustering frequency and semantic patterns sentence by\nsentence. We organize these characteristics into a knowledge graph, making them\nmore accessible and easier to retrieve. Our system of different roles will\nleverage this knowledge graph to generate new jailbreaks, which have proved\neffective in inducing LLMs to generate unethical or guideline-violating\nresponses. In addition, we also pioneer a setting in our system that will\nautomatically follow the government-issued guidelines to generate jailbreaks to\ntest whether LLMs follow the guidelines accordingly. We refer to our system as\nGUARD (Guideline Upholding through Adaptive Role-play Diagnostics). We have\nempirically validated the effectiveness of GUARD on three cutting-edge\nopen-sourced LLMs (Vicuna-13B, LongChat-7B, and Llama-2-7B), as well as a\nwidely-utilized commercial LLM (ChatGPT). Moreover, our work extends to the\nrealm of vision language models (MiniGPT-v2 and Gemini Vision Pro), showcasing\nGUARD's versatility and contributing valuable insights for the development of\nsafer, more reliable LLM-based applications across diverse modalities.",
        "updated": "2024-02-05 18:54:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03299v1"
    },
    {
        "title": "Deal, or no deal (or who knows)? Forecasting Uncertainty in Conversations using Large Language Models",
        "authors": "Anthony SiciliaHyunwoo KimKhyathi Raghavi ChanduMalihe AlikhaniJack Hessel",
        "links": "http://arxiv.org/abs/2402.03284v1",
        "entry_id": "http://arxiv.org/abs/2402.03284v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03284v1",
        "summary": "Effective interlocutors account for the uncertain goals, beliefs, and\nemotions of others. But even the best human conversationalist cannot perfectly\nanticipate the trajectory of a dialogue. How well can language models represent\ninherent uncertainty in conversations? We propose FortUne Dial, an expansion of\nthe long-standing \"conversation forecasting\" task: instead of just accuracy,\nevaluation is conducted with uncertainty-aware metrics, effectively enabling\nabstention on individual instances. We study two ways in which language models\npotentially represent outcome uncertainty (internally, using scores and\ndirectly, using tokens) and propose fine-tuning strategies to improve\ncalibration of both representations. Experiments on eight difficult negotiation\ncorpora demonstrate that our proposed fine-tuning strategies (a traditional\nsupervision strategy and an off-policy reinforcement learning strategy) can\ncalibrate smaller open-source models to compete with pre-trained models 10x\ntheir size.",
        "updated": "2024-02-05 18:39:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03284v1"
    },
    {
        "title": "Uncertainty of Thoughts: Uncertainty-Aware Planning Enhances Information Seeking in Large Language Models",
        "authors": "Zhiyuan HuChumin LiuXidong FengYilun ZhaoSee-Kiong NgAnh Tuan LuuJunxian HePang Wei KohBryan Hooi",
        "links": "http://arxiv.org/abs/2402.03271v1",
        "entry_id": "http://arxiv.org/abs/2402.03271v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03271v1",
        "summary": "In the face of uncertainty, the ability to seek information is of fundamental\nimportance. In many practical applications, such as medical diagnosis and\ntroubleshooting, the information needed to solve the task is not initially\ngiven, and has to be actively sought by asking follow-up questions (for\nexample, a doctor asking a patient for more details about their symptoms). In\nthis work, we introduce Uncertainty of Thoughts (UoT), an algorithm to augment\nlarge language models with the ability to actively seek information by asking\neffective questions. UoT combines 1) an uncertainty-aware simulation approach\nwhich enables the model to simulate possible future scenarios and how likely\nthey are to occur, 2) uncertainty-based rewards motivated by information gain\nwhich incentivizes the model to seek information, and 3) a reward propagation\nscheme to select the optimal question to ask in a way that maximizes the\nexpected reward. In experiments on medical diagnosis, troubleshooting and the\n'20 Questions' game, UoT achieves an average performance improvement of 57.8%\nin the rate of successful task completion across multiple LLMs compared with\ndirect prompting, and also improves efficiency (i.e., the number of questions\nneeded to complete the task).",
        "updated": "2024-02-05 18:28:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03271v1"
    }
]