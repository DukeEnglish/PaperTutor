[
    {
        "title": "Ginger: An Efficient Curvature Approximation with Linear Complexity for General Neural Networks",
        "authors": "Yongchang HaoYanshuai CaoLili Mou",
        "links": "http://arxiv.org/abs/2402.03295v1",
        "entry_id": "http://arxiv.org/abs/2402.03295v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03295v1",
        "summary": "Second-order optimization approaches like the generalized Gauss-Newton method\nare considered more powerful as they utilize the curvature information of the\nobjective function with preconditioning matrices. Albeit offering tempting\ntheoretical benefits, they are not easily applicable to modern deep learning.\nThe major reason is due to the quadratic memory and cubic time complexity to\ncompute the inverse of the matrix. These requirements are infeasible even with\nstate-of-the-art hardware. In this work, we propose Ginger, an\neigendecomposition for the inverse of the generalized Gauss-Newton matrix. Our\nmethod enjoys efficient linear memory and time complexity for each iteration.\nInstead of approximating the conditioning matrix, we directly maintain its\ninverse to make the approximation more accurate. We provide the convergence\nresult of Ginger for non-convex objectives. Our experiments on different tasks\nwith different model architectures verify the effectiveness of our method. Our\ncode is publicly available.",
        "updated": "2024-02-05 18:51:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03295v1"
    },
    {
        "title": "Flora: Low-Rank Adapters Are Secretly Gradient Compressors",
        "authors": "Yongchang HaoYanshuai CaoLili Mou",
        "links": "http://arxiv.org/abs/2402.03293v1",
        "entry_id": "http://arxiv.org/abs/2402.03293v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03293v1",
        "summary": "Despite large neural networks demonstrating remarkable abilities to complete\ndifferent tasks, they require excessive memory usage to store the optimization\nstates for training. To alleviate this, the low-rank adaptation (LoRA) is\nproposed to reduce the optimization states by training fewer parameters.\nHowever, LoRA restricts overall weight update matrices to be low-rank, limiting\nthe model performance. In this work, we investigate the dynamics of LoRA and\nidentify that it can be approximated by a random projection. Based on this\nobservation, we propose Flora, which is able to achieve high-rank updates by\nresampling the projection matrices while enjoying the sublinear space\ncomplexity of optimization states. We conduct experiments across different\ntasks and model architectures to verify the effectiveness of our approach.",
        "updated": "2024-02-05 18:50:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03293v1"
    },
    {
        "title": "A Framework for Partially Observed Reward-States in RLHF",
        "authors": "Chinmaya KausikMirco MuttiAldo PacchianoAmbuj Tewari",
        "links": "http://arxiv.org/abs/2402.03282v1",
        "entry_id": "http://arxiv.org/abs/2402.03282v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03282v1",
        "summary": "The study of reinforcement learning from human feedback (RLHF) has gained\nprominence in recent years due to its role in the development of LLMs.\nNeuroscience research shows that human responses to stimuli are known to depend\non partially-observed \"internal states.\" Unfortunately current models of RLHF\ndo not take take this into consideration. Moreover most RLHF models do not\naccount for intermediate feedback, which is gaining importance in empirical\nwork and can help improve both sample complexity and alignment. To address\nthese limitations, we model RLHF as reinforcement learning with partially\nobserved reward-states (PORRL). We show reductions from the the two dominant\nforms of human feedback in RLHF - cardinal and dueling feedback to PORRL. For\ncardinal feedback, we develop generic statistically efficient algorithms and\ninstantiate them to present POR-UCRL and POR-UCBVI. For dueling feedback, we\nshow that a naive reduction to cardinal feedback fails to achieve sublinear\ndueling regret. We then present the first explicit reduction that converts\nguarantees for cardinal regret to dueling regret. We show that our models and\nguarantees in both settings generalize and extend existing ones. Finally, we\nidentify a recursive structure on our model that could improve the statistical\nand computational tractability of PORRL, giving examples from past work on RLHF\nas well as learning perfect reward machines, which PORRL subsumes.",
        "updated": "2024-02-05 18:38:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03282v1"
    },
    {
        "title": "Learning Best-in-Class Policies for the Predict-then-Optimize Framework",
        "authors": "Michael HuangVishal Gupta",
        "links": "http://arxiv.org/abs/2402.03256v1",
        "entry_id": "http://arxiv.org/abs/2402.03256v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03256v1",
        "summary": "We propose a novel family of decision-aware surrogate losses, called\nPerturbation Gradient (PG) losses, for the predict-then-optimize framework.\nThese losses directly approximate the downstream decision loss and can be\noptimized using off-the-shelf gradient-based methods. Importantly, unlike\nexisting surrogate losses, the approximation error of our PG losses vanishes as\nthe number of samples grows. This implies that optimizing our surrogate loss\nyields a best-in-class policy asymptotically, even in misspecified settings.\nThis is the first such result in misspecified settings and we provide numerical\nevidence confirming our PG losses substantively outperform existing proposals\nwhen the underlying model is misspecified and the noise is not centrally\nsymmetric. Insofar as misspecification is commonplace in practice -- especially\nwhen we might prefer a simpler, more interpretable model -- PG losses offer a\nnovel, theoretically justified, method for computationally tractable\ndecision-aware learning.",
        "updated": "2024-02-05 18:14:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03256v1"
    },
    {
        "title": "Minimum Description Length and Generalization Guarantees for Representation Learning",
        "authors": "Milad SefidgaranAbdellatif ZaidiPiotr Krasnowski",
        "links": "http://arxiv.org/abs/2402.03254v1",
        "entry_id": "http://arxiv.org/abs/2402.03254v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03254v1",
        "summary": "A major challenge in designing efficient statistical supervised learning\nalgorithms is finding representations that perform well not only on available\ntraining samples but also on unseen data. While the study of representation\nlearning has spurred much interest, most existing such approaches are\nheuristic; and very little is known about theoretical generalization\nguarantees.\n  In this paper, we establish a compressibility framework that allows us to\nderive upper bounds on the generalization error of a representation learning\nalgorithm in terms of the \"Minimum Description Length\" (MDL) of the labels or\nthe latent variables (representations). Rather than the mutual information\nbetween the encoder's input and the representation, which is often believed to\nreflect the algorithm's generalization capability in the related literature but\nin fact, falls short of doing so, our new bounds involve the \"multi-letter\"\nrelative entropy between the distribution of the representations (or labels) of\nthe training and test sets and a fixed prior. In particular, these new bounds\nreflect the structure of the encoder and are not vacuous for deterministic\nalgorithms. Our compressibility approach, which is information-theoretic in\nnature, builds upon that of Blum-Langford for PAC-MDL bounds and introduces two\nessential ingredients: block-coding and lossy-compression. The latter allows\nour approach to subsume the so-called geometrical compressibility as a special\ncase. To the best knowledge of the authors, the established generalization\nbounds are the first of their kind for Information Bottleneck (IB) type\nencoders and representation learning. Finally, we partly exploit the\ntheoretical results by introducing a new data-dependent prior. Numerical\nsimulations illustrate the advantages of well-chosen such priors over classical\npriors used in IB.",
        "updated": "2024-02-05 18:12:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03254v1"
    }
]