[
    {
        "title": "Test-Time Adaptation for Depth Completion",
        "authors": "Hyoungseob ParkAnjali GuptaAlex Wong",
        "links": "http://arxiv.org/abs/2402.03312v1",
        "entry_id": "http://arxiv.org/abs/2402.03312v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03312v1",
        "summary": "It is common to observe performance degradation when transferring models\ntrained on some (source) datasets to target testing data due to a domain gap\nbetween them. Existing methods for bridging this gap, such as domain adaptation\n(DA), may require the source data on which the model was trained (often not\navailable), while others, i.e., source-free DA, require many passes through the\ntesting data. We propose an online test-time adaptation method for depth\ncompletion, the task of inferring a dense depth map from a single image and\nassociated sparse depth map, that closes the performance gap in a single pass.\nWe first present a study on how the domain shift in each data modality affects\nmodel performance. Based on our observations that the sparse depth modality\nexhibits a much smaller covariate shift than the image, we design an embedding\nmodule trained in the source domain that preserves a mapping from features\nencoding only sparse depth to those encoding image and sparse depth. During\ntest time, sparse depth features are projected using this map as a proxy for\nsource domain features and are used as guidance to train a set of auxiliary\nparameters (i.e., adaptation layer) to align image and sparse depth features\nfrom the target test domain to that of the source domain. We evaluate our\nmethod on indoor and outdoor scenarios and show that it improves over baselines\nby an average of 21.1%.",
        "updated": "2024-02-05 18:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03312v1"
    },
    {
        "title": "HASSOD: Hierarchical Adaptive Self-Supervised Object Detection",
        "authors": "Shengcao CaoDhiraj JoshiLiang-Yan GuiYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2402.03311v1",
        "entry_id": "http://arxiv.org/abs/2402.03311v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03311v1",
        "summary": "The human visual perception system demonstrates exceptional capabilities in\nlearning without explicit supervision and understanding the part-to-whole\ncomposition of objects. Drawing inspiration from these two abilities, we\npropose Hierarchical Adaptive Self-Supervised Object Detection (HASSOD), a\nnovel approach that learns to detect objects and understand their compositions\nwithout human supervision. HASSOD employs a hierarchical adaptive clustering\nstrategy to group regions into object masks based on self-supervised visual\nrepresentations, adaptively determining the number of objects per image.\nFurthermore, HASSOD identifies the hierarchical levels of objects in terms of\ncomposition, by analyzing coverage relations between masks and constructing\ntree structures. This additional self-supervised learning task leads to\nimproved detection performance and enhanced interpretability. Lastly, we\nabandon the inefficient multi-round self-training process utilized in prior\nmethods and instead adapt the Mean Teacher framework from semi-supervised\nlearning, which leads to a smoother and more efficient training process.\nThrough extensive experiments on prevalent image datasets, we demonstrate the\nsuperiority of HASSOD over existing methods, thereby advancing the state of the\nart in self-supervised object detection. Notably, we improve Mask AR from 20.2\nto 22.5 on LVIS, and from 17.0 to 26.0 on SA-1B. Project page:\nhttps://HASSOD-NeurIPS23.github.io.",
        "updated": "2024-02-05 18:59:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03311v1"
    },
    {
        "title": "V-IRL: Grounding Virtual Intelligence in Real Life",
        "authors": "Jihan YangRunyu DingEllis BrownXiaojuan QiSaining Xie",
        "links": "http://arxiv.org/abs/2402.03310v1",
        "entry_id": "http://arxiv.org/abs/2402.03310v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03310v1",
        "summary": "There is a sensory gulf between the Earth that humans inhabit and the digital\nrealms in which modern AI agents are created. To develop AI agents that can\nsense, think, and act as flexibly as humans in real-world settings, it is\nimperative to bridge the realism gap between the digital and physical worlds.\nHow can we embody agents in an environment as rich and diverse as the one we\ninhabit, without the constraints imposed by real hardware and control? Towards\nthis end, we introduce V-IRL: a platform that enables agents to scalably\ninteract with the real world in a virtual yet realistic environment. Our\nplatform serves as a playground for developing agents that can accomplish\nvarious practical tasks and as a vast testbed for measuring progress in\ncapabilities spanning perception, decision-making, and interaction with\nreal-world data across the entire globe.",
        "updated": "2024-02-05 18:59:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03310v1"
    },
    {
        "title": "AONeuS: A Neural Rendering Framework for Acoustic-Optical Sensor Fusion",
        "authors": "Mohamad QadriKevin ZhangAkshay HindujaMichael KaessAdithya PediredlaChristopher A. Metzler",
        "links": "http://arxiv.org/abs/2402.03309v1",
        "entry_id": "http://arxiv.org/abs/2402.03309v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03309v1",
        "summary": "Underwater perception and 3D surface reconstruction are challenging problems\nwith broad applications in construction, security, marine archaeology, and\nenvironmental monitoring. Treacherous operating conditions, fragile\nsurroundings, and limited navigation control often dictate that submersibles\nrestrict their range of motion and, thus, the baseline over which they can\ncapture measurements. In the context of 3D scene reconstruction, it is\nwell-known that smaller baselines make reconstruction more challenging. Our\nwork develops a physics-based multimodal acoustic-optical neural surface\nreconstruction framework (AONeuS) capable of effectively integrating\nhigh-resolution RGB measurements with low-resolution depth-resolved imaging\nsonar measurements. By fusing these complementary modalities, our framework can\nreconstruct accurate high-resolution 3D surfaces from measurements captured\nover heavily-restricted baselines. Through extensive simulations and in-lab\nexperiments, we demonstrate that AONeuS dramatically outperforms recent\nRGB-only and sonar-only inverse-differentiable-rendering--based surface\nreconstruction methods. A website visualizing the results of our paper is\nlocated at this address: https://aoneus.github.io/",
        "updated": "2024-02-05 18:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03309v1"
    },
    {
        "title": "4D Gaussian Splatting: Towards Efficient Novel View Synthesis for Dynamic Scenes",
        "authors": "Yuanxing DuanFangyin WeiQiyu DaiYuhang HeWenzheng ChenBaoquan Chen",
        "links": "http://arxiv.org/abs/2402.03307v1",
        "entry_id": "http://arxiv.org/abs/2402.03307v1",
        "pdf_url": "http://arxiv.org/pdf/2402.03307v1",
        "summary": "We consider the problem of novel view synthesis (NVS) for dynamic scenes.\nRecent neural approaches have accomplished exceptional NVS results for static\n3D scenes, but extensions to 4D time-varying scenes remain non-trivial. Prior\nefforts often encode dynamics by learning a canonical space plus implicit or\nexplicit deformation fields, which struggle in challenging scenarios like\nsudden movements or capturing high-fidelity renderings. In this paper, we\nintroduce 4D Gaussian Splatting (4DGS), a novel method that represents dynamic\nscenes with anisotropic 4D XYZT Gaussians, inspired by the success of 3D\nGaussian Splatting in static scenes. We model dynamics at each timestamp by\ntemporally slicing the 4D Gaussians, which naturally compose dynamic 3D\nGaussians and can be seamlessly projected into images. As an explicit\nspatial-temporal representation, 4DGS demonstrates powerful capabilities for\nmodeling complicated dynamics and fine details, especially for scenes with\nabrupt motions. We further implement our temporal slicing and splatting\ntechniques in a highly optimized CUDA acceleration framework, achieving\nreal-time inference rendering speeds of up to 277 FPS on an RTX 3090 GPU and\n583 FPS on an RTX 4090 GPU. Rigorous evaluations on scenes with diverse motions\nshowcase the superior efficiency and effectiveness of 4DGS, which consistently\noutperforms existing methods both quantitatively and qualitatively.",
        "updated": "2024-02-05 18:59:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.03307v1"
    }
]