[
    {
        "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
        "authors": "Shuming MaHongyu WangLingxiao MaLei WangWenhui WangShaohan HuangLi DongRuiping WangJilong XueFuru Wei",
        "links": "http://arxiv.org/abs/2402.17764v1",
        "entry_id": "http://arxiv.org/abs/2402.17764v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17764v1",
        "summary": "Recent research, such as BitNet, is paving the way for a new era of 1-bit\nLarge Language Models (LLMs). In this work, we introduce a 1-bit LLM variant,\nnamely BitNet b1.58, in which every single parameter (or weight) of the LLM is\nternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16)\nTransformer LLM with the same model size and training tokens in terms of both\nperplexity and end-task performance, while being significantly more\ncost-effective in terms of latency, memory, throughput, and energy consumption.\nMore profoundly, the 1.58-bit LLM defines a new scaling law and recipe for\ntraining new generations of LLMs that are both high-performance and\ncost-effective. Furthermore, it enables a new computation paradigm and opens\nthe door for designing specific hardware optimized for 1-bit LLMs.",
        "updated": "2024-02-27 18:56:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17764v1"
    },
    {
        "title": "Massive Activations in Large Language Models",
        "authors": "Mingjie SunXinlei ChenJ. Zico KolterZhuang Liu",
        "links": "http://arxiv.org/abs/2402.17762v1",
        "entry_id": "http://arxiv.org/abs/2402.17762v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17762v1",
        "summary": "We observe an empirical phenomenon in Large Language Models (LLMs) -- very\nfew activations exhibit significantly larger values than others (e.g., 100,000\ntimes larger). We call them massive activations. First, we demonstrate the\nwidespread existence of massive activations across various LLMs and\ncharacterize their locations. Second, we find their values largely stay\nconstant regardless of the input, and they function as indispensable bias terms\nin LLMs. Third, these massive activations lead to the concentration of\nattention probabilities to their corresponding tokens, and further, implicit\nbias terms in the self-attention output. Last, we also study massive\nactivations in Vision Transformers.",
        "updated": "2024-02-27 18:55:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17762v1"
    },
    {
        "title": "Towards Optimal Learning of Language Models",
        "authors": "Yuxian GuLi DongYaru HaoQingxiu DongMinlie HuangFuru Wei",
        "links": "http://arxiv.org/abs/2402.17759v1",
        "entry_id": "http://arxiv.org/abs/2402.17759v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17759v1",
        "summary": "This work studies the general principles of improving the learning of\nlanguage models (LMs), which aims at reducing the necessary training steps for\nachieving superior performance. Specifically, we present a theory for the\noptimal learning of LMs. We first propose an objective that optimizes LM\nlearning by maximizing the data compression ratio in an\n\"LM-training-as-lossless-compression\" view. Then, we derive a theorem, named\nLearning Law, to reveal the properties of the dynamics in the optimal learning\nprocess under our objective. The theorem is then validated by experiments on a\nlinear classification and a real-world language modeling task. Finally, we\nempirically verify that the optimal learning of LMs essentially stems from the\nimprovement of the coefficients in the scaling law of LMs, indicating great\npromise and significance for designing practical learning acceleration methods.\nOur code can be found at https://aka.ms/LearningLaw.",
        "updated": "2024-02-27 18:52:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17759v1"
    },
    {
        "title": "Evaluating Very Long-Term Conversational Memory of LLM Agents",
        "authors": "Adyasha MaharanaDong-Ho LeeSergey TulyakovMohit BansalFrancesco BarbieriYuwei Fang",
        "links": "http://arxiv.org/abs/2402.17753v1",
        "entry_id": "http://arxiv.org/abs/2402.17753v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17753v1",
        "summary": "Existing works on long-term open-domain dialogues focus on evaluating model\nresponses within contexts spanning no more than five chat sessions. Despite\nadvancements in long-context large language models (LLMs) and retrieval\naugmented generation (RAG) techniques, their efficacy in very long-term\ndialogues remains unexplored. To address this research gap, we introduce a\nmachine-human pipeline to generate high-quality, very long-term dialogues by\nleveraging LLM-based agent architectures and grounding their dialogues on\npersonas and temporal event graphs. Moreover, we equip each agent with the\ncapability of sharing and reacting to images. The generated conversations are\nverified and edited by human annotators for long-range consistency and\ngrounding to the event graphs. Using this pipeline, we collect LoCoMo, a\ndataset of very long-term conversations, each encompassing 300 turns and 9K\ntokens on avg., over up to 35 sessions. Based on LoCoMo, we present a\ncomprehensive evaluation benchmark to measure long-term memory in models,\nencompassing question answering, event summarization, and multi-modal dialogue\ngeneration tasks. Our experimental results indicate that LLMs exhibit\nchallenges in understanding lengthy conversations and comprehending long-range\ntemporal and causal dynamics within dialogues. Employing strategies like\nlong-context LLMs or RAG can offer improvements but these models still\nsubstantially lag behind human performance.",
        "updated": "2024-02-27 18:42:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17753v1"
    },
    {
        "title": "Tower: An Open Multilingual Large Language Model for Translation-Related Tasks",
        "authors": "Duarte M. AlvesJosé PombalNuno M. GuerreiroPedro H. MartinsJoão AlvesAmin FarajianBen PetersRicardo ReiPatrick FernandesSweta AgrawalPierre ColomboJosé G. C. de SouzaAndré F. T. Martins",
        "links": "http://arxiv.org/abs/2402.17733v1",
        "entry_id": "http://arxiv.org/abs/2402.17733v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17733v1",
        "summary": "While general-purpose large language models (LLMs) demonstrate proficiency on\nmultiple tasks within the domain of translation, approaches based on open LLMs\nare competitive only when specializing on a single task. In this paper, we\npropose a recipe for tailoring LLMs to multiple tasks present in translation\nworkflows. We perform continued pretraining on a multilingual mixture of\nmonolingual and parallel data, creating TowerBase, followed by finetuning on\ninstructions relevant for translation processes, creating TowerInstruct. Our\nfinal model surpasses open alternatives on several tasks relevant to\ntranslation workflows and is competitive with general-purpose closed LLMs. To\nfacilitate future research, we release the Tower models, our specialization\ndataset, an evaluation framework for LLMs focusing on the translation\necosystem, and a collection of model generations, including ours, on our\nbenchmark.",
        "updated": "2024-02-27 18:09:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17733v1"
    }
]