[
    {
        "title": "Robustly Learning Single-Index Models via Alignment Sharpness",
        "authors": "Nikos ZarifisPuqian WangIlias DiakonikolasJelena Diakonikolas",
        "links": "http://arxiv.org/abs/2402.17756v1",
        "entry_id": "http://arxiv.org/abs/2402.17756v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17756v1",
        "summary": "We study the problem of learning Single-Index Models under the $L_2^2$ loss\nin the agnostic model. We give an efficient learning algorithm, achieving a\nconstant factor approximation to the optimal loss, that succeeds under a range\nof distributions (including log-concave distributions) and a broad class of\nmonotone and Lipschitz link functions. This is the first efficient constant\nfactor approximate agnostic learner, even for Gaussian data and for any\nnontrivial class of link functions. Prior work for the case of unknown link\nfunction either works in the realizable setting or does not attain constant\nfactor approximation. The main technical ingredient enabling our algorithm and\nanalysis is a novel notion of a local error bound in optimization that we term\nalignment sharpness and that may be of broader interest.",
        "updated": "2024-02-27 18:48:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17756v1"
    },
    {
        "title": "When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning",
        "authors": "Leon LangDavis FooteStuart RussellAnca DraganErik JennerScott Emmons",
        "links": "http://arxiv.org/abs/2402.17747v1",
        "entry_id": "http://arxiv.org/abs/2402.17747v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17747v1",
        "summary": "Past analyses of reinforcement learning from human feedback (RLHF) assume\nthat the human fully observes the environment. What happens when human feedback\nis based only on partial observations? We formally define two failure cases:\ndeception and overjustification. Modeling the human as Boltzmann-rational\nw.r.t. a belief over trajectories, we prove conditions under which RLHF is\nguaranteed to result in policies that deceptively inflate their performance,\noverjustify their behavior to make an impression, or both. To help address\nthese issues, we mathematically characterize how partial observability of the\nenvironment translates into (lack of) ambiguity in the learned return function.\nIn some cases, accounting for partial observability makes it theoretically\npossible to recover the return function and thus the optimal policy, while in\nother cases, there is irreducible ambiguity. We caution against blindly\napplying RLHF in partially observable settings and propose research directions\nto help tackle these challenges.",
        "updated": "2024-02-27 18:32:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17747v1"
    },
    {
        "title": "Batched Nonparametric Contextual Bandits",
        "authors": "Rong JiangCong Ma",
        "links": "http://arxiv.org/abs/2402.17732v1",
        "entry_id": "http://arxiv.org/abs/2402.17732v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17732v1",
        "summary": "We study nonparametric contextual bandits under batch constraints, where the\nexpected reward for each action is modeled as a smooth function of covariates,\nand the policy updates are made at the end of each batch of observations. We\nestablish a minimax regret lower bound for this setting and propose Batched\nSuccessive Elimination with Dynamic Binning (BaSEDB) that achieves optimal\nregret (up to logarithmic factors). In essence, BaSEDB dynamically splits the\ncovariate space into smaller bins, carefully aligning their widths with the\nbatch size. We also show the suboptimality of static binning under batch\nconstraints, highlighting the necessity of dynamic binning. Additionally, our\nresults suggest that a nearly constant number of policy updates can attain\noptimal regret in the fully online setting.",
        "updated": "2024-02-27 18:06:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17732v1"
    },
    {
        "title": "Transfer Learning Bayesian Optimization to Design Competitor DNA Molecules for Use in Diagnostic Assays",
        "authors": "Ruby SedgwickJohn P. GoertzMolly M. StevensRuth MisenerMark van der Wilk",
        "links": "http://arxiv.org/abs/2402.17704v1",
        "entry_id": "http://arxiv.org/abs/2402.17704v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17704v1",
        "summary": "With the rise in engineered biomolecular devices, there is an increased need\nfor tailor-made biological sequences. Often, many similar biological sequences\nneed to be made for a specific application meaning numerous, sometimes\nprohibitively expensive, lab experiments are necessary for their optimization.\nThis paper presents a transfer learning design of experiments workflow to make\nthis development feasible. By combining a transfer learning surrogate model\nwith Bayesian optimization, we show how the total number of experiments can be\nreduced by sharing information between optimization tasks. We demonstrate the\nreduction in the number of experiments using data from the development of DNA\ncompetitors for use in an amplification-based diagnostic assay. We use\ncross-validation to compare the predictive accuracy of different transfer\nlearning models, and then compare the performance of the models for both single\nobjective and penalized optimization tasks.",
        "updated": "2024-02-27 17:30:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17704v1"
    },
    {
        "title": "Gradient-based Discrete Sampling with Automatic Cyclical Scheduling",
        "authors": "Patrick PynadathRiddhiman BhattacharyaArun HariharanRuqi Zhang",
        "links": "http://arxiv.org/abs/2402.17699v1",
        "entry_id": "http://arxiv.org/abs/2402.17699v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17699v1",
        "summary": "Discrete distributions, particularly in high-dimensional deep models, are\noften highly multimodal due to inherent discontinuities. While gradient-based\ndiscrete sampling has proven effective, it is susceptible to becoming trapped\nin local modes due to the gradient information. To tackle this challenge, we\npropose an automatic cyclical scheduling, designed for efficient and accurate\nsampling in multimodal discrete distributions. Our method contains three key\ncomponents: (1) a cyclical step size schedule where large steps discover new\nmodes and small steps exploit each mode; (2) a cyclical balancing schedule,\nensuring ``balanced\" proposals for given step sizes and high efficiency of the\nMarkov chain; and (3) an automatic tuning scheme for adjusting the\nhyperparameters in the cyclical schedules, allowing adaptability across diverse\ndatasets with minimal tuning. We prove the non-asymptotic convergence and\ninference guarantee for our method in general discrete distributions. Extensive\nexperiments demonstrate the superiority of our method in sampling complex\nmultimodal discrete distributions.",
        "updated": "2024-02-27 17:23:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17699v1"
    }
]