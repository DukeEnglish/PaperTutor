[
    {
        "title": "Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning",
        "authors": "Xiaoyu ZhangMatthew ChangPranav KumarSaurabh Gupta",
        "links": "http://arxiv.org/abs/2402.17768v1",
        "entry_id": "http://arxiv.org/abs/2402.17768v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17768v1",
        "summary": "A common failure mode for policies trained with imitation is compounding\nexecution errors at test time. When the learned policy encounters states that\nwere not present in the expert demonstrations, the policy fails, leading to\ndegenerate behavior. The Dataset Aggregation, or DAgger approach to this\nproblem simply collects more data to cover these failure states. However, in\npractice, this is often prohibitively expensive. In this work, we propose\nDiffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without\nthe cost for eye-in-hand imitation learning problems. Instead of collecting new\nsamples to cover out-of-distribution states, DMD uses recent advances in\ndiffusion models to create these samples with diffusion models. This leads to\nrobust performance from few demonstrations. In experiments conducted for\nnon-prehensile pushing on a Franka Research 3, we show that DMD can achieve a\nsuccess rate of 80% with as few as 8 expert demonstrations, where naive\nbehavior cloning reaches only 20%. DMD also outperform competing NeRF-based\naugmentation schemes by 50%.",
        "updated": "2024-02-27 18:59:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17768v1"
    },
    {
        "title": "Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator",
        "authors": "Arjun GuptaMichelle ZhangRishik SathuaSaurabh Gupta",
        "links": "http://arxiv.org/abs/2402.17767v1",
        "entry_id": "http://arxiv.org/abs/2402.17767v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17767v1",
        "summary": "Pulling open cabinets and drawers presents many difficult technical\nchallenges in perception (inferring articulation parameters for objects from\nonboard sensors), planning (producing motion plans that conform to tight task\nconstraints), and control (making and maintaining contact while applying forces\non the environment). In this work, we build an end-to-end system that enables a\ncommodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in\ndiverse previously unseen real world environments. We conduct 4 days of real\nworld testing of this system spanning 31 different objects from across 13\ndifferent real world environments. Our system achieves a success rate of 61% on\nopening novel cabinets and drawers in unseen environments zero-shot. An\nanalysis of the failure modes suggests that errors in perception are the most\nsignificant challenge for our system. We will open source code and models for\nothers to replicate and build upon our system.",
        "updated": "2024-02-27 18:58:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17767v1"
    },
    {
        "title": "Learning to Program Variational Quantum Circuits with Fast Weights",
        "authors": "Samuel Yen-Chi Chen",
        "links": "http://arxiv.org/abs/2402.17760v1",
        "entry_id": "http://arxiv.org/abs/2402.17760v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17760v1",
        "summary": "Quantum Machine Learning (QML) has surfaced as a pioneering framework\naddressing sequential control tasks and time-series modeling. It has\ndemonstrated empirical quantum advantages notably within domains such as\nReinforcement Learning (RL) and time-series prediction. A significant\nadvancement lies in Quantum Recurrent Neural Networks (QRNNs), specifically\ntailored for memory-intensive tasks encompassing partially observable\nenvironments and non-linear time-series prediction. Nevertheless, QRNN-based\nmodels encounter challenges, notably prolonged training duration stemming from\nthe necessity to compute quantum gradients using backpropagation-through-time\n(BPTT). This predicament exacerbates when executing the complete model on\nquantum devices, primarily due to the substantial demand for circuit evaluation\narising from the parameter-shift rule. This paper introduces the Quantum Fast\nWeight Programmers (QFWP) as a solution to the temporal or sequential learning\nchallenge. The QFWP leverages a classical neural network (referred to as the\n'slow programmer') functioning as a quantum programmer to swiftly modify the\nparameters of a variational quantum circuit (termed the 'fast programmer').\nInstead of completely overwriting the fast programmer at each time-step, the\nslow programmer generates parameter changes or updates for the quantum circuit\nparameters. This approach enables the fast programmer to incorporate past\nobservations or information. Notably, the proposed QFWP model achieves learning\nof temporal dependencies without necessitating the use of quantum recurrent\nneural networks. Numerical simulations conducted in this study showcase the\nefficacy of the proposed QFWP model in both time-series prediction and RL\ntasks. The model exhibits performance levels either comparable to or surpassing\nthose achieved by QLSTM-based models.",
        "updated": "2024-02-27 18:53:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17760v1"
    },
    {
        "title": "Evaluating Very Long-Term Conversational Memory of LLM Agents",
        "authors": "Adyasha MaharanaDong-Ho LeeSergey TulyakovMohit BansalFrancesco BarbieriYuwei Fang",
        "links": "http://arxiv.org/abs/2402.17753v1",
        "entry_id": "http://arxiv.org/abs/2402.17753v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17753v1",
        "summary": "Existing works on long-term open-domain dialogues focus on evaluating model\nresponses within contexts spanning no more than five chat sessions. Despite\nadvancements in long-context large language models (LLMs) and retrieval\naugmented generation (RAG) techniques, their efficacy in very long-term\ndialogues remains unexplored. To address this research gap, we introduce a\nmachine-human pipeline to generate high-quality, very long-term dialogues by\nleveraging LLM-based agent architectures and grounding their dialogues on\npersonas and temporal event graphs. Moreover, we equip each agent with the\ncapability of sharing and reacting to images. The generated conversations are\nverified and edited by human annotators for long-range consistency and\ngrounding to the event graphs. Using this pipeline, we collect LoCoMo, a\ndataset of very long-term conversations, each encompassing 300 turns and 9K\ntokens on avg., over up to 35 sessions. Based on LoCoMo, we present a\ncomprehensive evaluation benchmark to measure long-term memory in models,\nencompassing question answering, event summarization, and multi-modal dialogue\ngeneration tasks. Our experimental results indicate that LLMs exhibit\nchallenges in understanding lengthy conversations and comprehending long-range\ntemporal and causal dynamics within dialogues. Employing strategies like\nlong-context LLMs or RAG can offer improvements but these models still\nsubstantially lag behind human performance.",
        "updated": "2024-02-27 18:42:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17753v1"
    },
    {
        "title": "When Your AI Deceives You: Challenges with Partial Observability of Human Evaluators in Reward Learning",
        "authors": "Leon LangDavis FooteStuart RussellAnca DraganErik JennerScott Emmons",
        "links": "http://arxiv.org/abs/2402.17747v1",
        "entry_id": "http://arxiv.org/abs/2402.17747v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17747v1",
        "summary": "Past analyses of reinforcement learning from human feedback (RLHF) assume\nthat the human fully observes the environment. What happens when human feedback\nis based only on partial observations? We formally define two failure cases:\ndeception and overjustification. Modeling the human as Boltzmann-rational\nw.r.t. a belief over trajectories, we prove conditions under which RLHF is\nguaranteed to result in policies that deceptively inflate their performance,\noverjustify their behavior to make an impression, or both. To help address\nthese issues, we mathematically characterize how partial observability of the\nenvironment translates into (lack of) ambiguity in the learned return function.\nIn some cases, accounting for partial observability makes it theoretically\npossible to recover the return function and thus the optimal policy, while in\nother cases, there is irreducible ambiguity. We caution against blindly\napplying RLHF in partially observable settings and propose research directions\nto help tackle these challenges.",
        "updated": "2024-02-27 18:32:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17747v1"
    }
]