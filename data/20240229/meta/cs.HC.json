[
    {
        "title": "An Eye Gaze Heatmap Analysis of Uncertainty Head-Up Display Designs for Conditional Automated Driving",
        "authors": "Michael A. GerberRonald SchroeterDaniel JohnsonChristian P. JanssenAndry RakotonirainyJonny KuoMike G. Lenne",
        "links": "http://dx.doi.org/10.1145/3613904.3642219",
        "entry_id": "http://arxiv.org/abs/2402.17751v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17751v1",
        "summary": "This paper reports results from a high-fidelity driving simulator study\n(N=215) about a head-up display (HUD) that conveys a conditional automated\nvehicle's dynamic \"uncertainty\" about the current situation while fallback\ndrivers watch entertaining videos. We compared (between-group) three design\ninterventions: display (a bar visualisation of uncertainty close to the video),\ninterruption (interrupting the video during uncertain situations), and\ncombination (a combination of both), against a baseline (video-only). We\nvisualised eye-tracking data to conduct a heatmap analysis of the four groups'\ngaze behaviour over time. We found interruptions initiated a phase during which\nparticipants interleaved their attention between monitoring and entertainment.\nThis improved monitoring behaviour was more pronounced in combination compared\nto interruption, suggesting pre-warning interruptions have positive effects.\nThe same addition had negative effects without interruptions (comparing\nbaseline & display). Intermittent interruptions may have safety benefits over\nplacing additional peripheral displays without compromising usability.",
        "updated": "2024-02-27 18:38:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17751v1"
    },
    {
        "title": "Content-Centric Prototyping of Generative AI Applications: Emerging Approaches and Challenges in Collaborative Software Teams",
        "authors": "Hari SubramonyamDivy ThakkarJürgen DieberAnoop Sinha",
        "links": "http://arxiv.org/abs/2402.17721v1",
        "entry_id": "http://arxiv.org/abs/2402.17721v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17721v1",
        "summary": "Generative AI models are increasingly powering software applications,\noffering the capability to produce expressive content across varied contexts.\nHowever, unlike previous iterations of human-AI design, the emerging design\nprocess for generative capabilities primarily hinges on prompt engineering\nstrategies. Given this fundamental shift in approach, our work aims to\nunderstand how collaborative software teams set up and apply design guidelines\nand values, iteratively prototype prompts, and evaluate prompts to achieve\ndesired outcomes. We conducted design studies with 39 industry professionals,\nincluding designers, software engineers, and product managers. Our findings\nreveal a content-centric prototyping approach in which teams begin with the\ncontent they want to generate, then identify specific attributes, constraints,\nand values, and explore methods to give users the ability to influence and\ninteract with those attributes. Based on associated challenges, such as the\nlack of model interpretability and overfitting the design to examples, we\noutline considerations for generative AI prototyping.",
        "updated": "2024-02-27 17:56:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17721v1"
    },
    {
        "title": "OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist Autonomous Agents for Desktop and Web",
        "authors": "Raghav KapoorYash Parag ButalaMelisa RussakJing Yu KohKiran KambleWaseem AlshikhRuslan Salakhutdinov",
        "links": "http://arxiv.org/abs/2402.17553v2",
        "entry_id": "http://arxiv.org/abs/2402.17553v2",
        "pdf_url": "http://arxiv.org/pdf/2402.17553v2",
        "summary": "For decades, human-computer interaction has fundamentally been manual. Even\ntoday, almost all productive work done on the computer necessitates human input\nat every step. Autonomous virtual agents represent an exciting step in\nautomating many of these menial tasks. Virtual agents would empower users with\nlimited technical proficiency to harness the full possibilities of computer\nsystems. They could also enable the efficient streamlining of numerous computer\ntasks, ranging from calendar management to complex travel bookings, with\nminimal human intervention. In this paper, we introduce OmniACT, the\nfirst-of-a-kind dataset and benchmark for assessing an agent's capability to\ngenerate executable programs to accomplish computer tasks. Our scope extends\nbeyond traditional web automation, covering a diverse range of desktop\napplications. The dataset consists of fundamental tasks such as \"Play the next\nsong\", as well as longer horizon tasks such as \"Send an email to John Doe\nmentioning the time and place to meet\". Specifically, given a pair of screen\nimage and a visually-grounded natural language task, the goal is to generate a\nscript capable of fully executing the task. We run several strong baseline\nlanguage model agents on our benchmark. The strongest baseline, GPT-4, performs\nthe best on our benchmark However, its performance level still reaches only 15%\nof the human proficiency in generating executable scripts capable of completing\nthe task, demonstrating the challenge of our task for conventional web agents.\nOur benchmark provides a platform to measure and evaluate the progress of\nlanguage model agents in automating computer tasks and motivates future work\ntowards building multimodal models that bridge large language models and the\nvisual grounding of computer screens.",
        "updated": "2024-02-28 17:27:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17553v2"
    },
    {
        "title": "A TDM-based Analog Front-End for Ear-EEG Recording with 83-G$Ω$ Input Impedance, 384-mV DC Tolerance and 0.47-$μ$Vrms Input-Referred Noise",
        "authors": "Huiyong Zheng",
        "links": "http://arxiv.org/abs/2402.17538v1",
        "entry_id": "http://arxiv.org/abs/2402.17538v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17538v1",
        "summary": "This paper presents the design of a time-division multiplexed\ncapacitively-coupled chopper analog front end with a novel impedance boost loop\n(IBL) and a novel DC servo loop (DSL). The proposed IBL boosts the input\nimpedance of the analog front end to up to several tens of G$\\Omega$. It\nfirstly utilizes an external IBL to prevent the total input impedance from\ndegradation caused by parasitic capacitance from the ESD pad and external\ninterconnections, and secondly relies on an internal IBL to compensate for the\nleakage current introduced by the chopper. The proposed DSL consists of a\ncoarse DSL driven by square waveforms and a fine DSL driven by five\nphase-interleaving PWM waveforms, which up modulate the harmonics 5 times\nhigher. An edge-pursuit comparator (EPC) is utilized to monitor the residual\nelectrode offset voltage (EDO) at the LNA's output. Designed in a 0.18-$\\mu$m\nCMOS process, the AFE consumes 4.5 $\\mu$A from a 1.2-V supply. The simulated\ninput referred noise is 0.47 $\\mu$Vrms from 0.5 to 100 Hz in the presence of a\n384-mV EDO. The proposed AFE achieves a high input impedance of 83 G$\\Omega$ at\n1 Hz and 9.3 G$\\Omega$ at 100 Hz even with the presence of 20-pF parasitic\ncapacitance.",
        "updated": "2024-02-27 14:24:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17538v1"
    },
    {
        "title": "A Piece of Theatre: Investigating How Teachers Design LLM Chatbots to Assist Adolescent Cyberbullying Education",
        "authors": "Michael A. HedderichNatalie N. BazarovaWenting ZouRyun ShimXinda MaQian Yang",
        "links": "http://arxiv.org/abs/2402.17456v1",
        "entry_id": "http://arxiv.org/abs/2402.17456v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17456v1",
        "summary": "Cyberbullying harms teenagers' mental health, and teaching them upstanding\nintervention is crucial. Wizard-of-Oz studies show chatbots can scale up\npersonalized and interactive cyberbullying education, but implementing such\nchatbots is a challenging and delicate task. We created a no-code chatbot\ndesign tool for K-12 teachers. Using large language models and prompt chaining,\nour tool allows teachers to prototype bespoke dialogue flows and chatbot\nutterances. In offering this tool, we explore teachers' distinctive needs when\ndesigning chatbots to assist their teaching, and how chatbot design tools might\nbetter support them. Our findings reveal that teachers welcome the tool\nenthusiastically. Moreover, they see themselves as playwrights guiding both the\nstudents' and the chatbot's behaviors, while allowing for some improvisation.\nTheir goal is to enable students to rehearse both desirable and undesirable\nreactions to cyberbullying in a safe environment. We discuss the design\nopportunities LLM-Chains offer for empowering teachers and the research\nopportunities this work opens up.",
        "updated": "2024-02-27 12:27:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17456v1"
    }
]