[
    {
        "title": "Diffusion Meets DAgger: Supercharging Eye-in-hand Imitation Learning",
        "authors": "Xiaoyu ZhangMatthew ChangPranav KumarSaurabh Gupta",
        "links": "http://arxiv.org/abs/2402.17768v1",
        "entry_id": "http://arxiv.org/abs/2402.17768v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17768v1",
        "summary": "A common failure mode for policies trained with imitation is compounding\nexecution errors at test time. When the learned policy encounters states that\nwere not present in the expert demonstrations, the policy fails, leading to\ndegenerate behavior. The Dataset Aggregation, or DAgger approach to this\nproblem simply collects more data to cover these failure states. However, in\npractice, this is often prohibitively expensive. In this work, we propose\nDiffusion Meets DAgger (DMD), a method to reap the benefits of DAgger without\nthe cost for eye-in-hand imitation learning problems. Instead of collecting new\nsamples to cover out-of-distribution states, DMD uses recent advances in\ndiffusion models to create these samples with diffusion models. This leads to\nrobust performance from few demonstrations. In experiments conducted for\nnon-prehensile pushing on a Franka Research 3, we show that DMD can achieve a\nsuccess rate of 80% with as few as 8 expert demonstrations, where naive\nbehavior cloning reaches only 20%. DMD also outperform competing NeRF-based\naugmentation schemes by 50%.",
        "updated": "2024-02-27 18:59:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17768v1"
    },
    {
        "title": "Opening Cabinets and Drawers in the Real World using a Commodity Mobile Manipulator",
        "authors": "Arjun GuptaMichelle ZhangRishik SathuaSaurabh Gupta",
        "links": "http://arxiv.org/abs/2402.17767v1",
        "entry_id": "http://arxiv.org/abs/2402.17767v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17767v1",
        "summary": "Pulling open cabinets and drawers presents many difficult technical\nchallenges in perception (inferring articulation parameters for objects from\nonboard sensors), planning (producing motion plans that conform to tight task\nconstraints), and control (making and maintaining contact while applying forces\non the environment). In this work, we build an end-to-end system that enables a\ncommodity mobile manipulator (Stretch RE2) to pull open cabinets and drawers in\ndiverse previously unseen real world environments. We conduct 4 days of real\nworld testing of this system spanning 31 different objects from across 13\ndifferent real world environments. Our system achieves a success rate of 61% on\nopening novel cabinets and drawers in unseen environments zero-shot. An\nanalysis of the failure modes suggests that errors in perception are the most\nsignificant challenge for our system. We will open source code and models for\nothers to replicate and build upon our system.",
        "updated": "2024-02-27 18:58:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17767v1"
    },
    {
        "title": "ShapeLLM: Universal 3D Object Understanding for Embodied Interaction",
        "authors": "Zekun QiRunpei DongShaochen ZhangHaoran GengChunrui HanZheng GeLi YiKaisheng Ma",
        "links": "http://arxiv.org/abs/2402.17766v1",
        "entry_id": "http://arxiv.org/abs/2402.17766v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17766v1",
        "summary": "This paper presents ShapeLLM, the first 3D Multimodal Large Language Model\n(LLM) designed for embodied interaction, exploring a universal 3D object\nunderstanding with 3D point clouds and languages. ShapeLLM is built upon an\nimproved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view\nimage distillation for enhanced geometry understanding. By utilizing ReCon++ as\nthe 3D point cloud input encoder for LLMs, ShapeLLM is trained on constructed\ninstruction-following data and tested on our newly human-curated evaluation\nbenchmark, 3D MM-Vet. ReCon++ and ShapeLLM achieve state-of-the-art performance\nin 3D geometry understanding and language-unified 3D interaction tasks, such as\nembodied visual grounding.",
        "updated": "2024-02-27 18:57:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17766v1"
    },
    {
        "title": "ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living",
        "authors": "Marsil ZakourPartha Pratim NathLudwig LohmerEmre Faik GökçeMartin PiccolrovazziConstantin PatschYuankai WuRahul ChaudhariEckehard Steinbach",
        "links": "http://arxiv.org/abs/2402.17758v1",
        "entry_id": "http://arxiv.org/abs/2402.17758v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17758v1",
        "summary": "Hand-Object Interactions (HOIs) are conditioned on spatial and temporal\ncontexts like surrounding objects, pre- vious actions, and future intents (for\nexample, grasping and handover actions vary greatly based on objects proximity\nand trajectory obstruction). However, existing datasets for 4D HOI (3D HOI over\ntime) are limited to one subject inter- acting with one object only. This\nrestricts the generalization of learning-based HOI methods trained on those\ndatasets. We introduce ADL4D, a dataset of up to two subjects inter- acting\nwith different sets of objects performing Activities of Daily Living (ADL) like\nbreakfast or lunch preparation ac- tivities. The transition between multiple\nobjects to complete a certain task over time introduces a unique context\nlacking in existing datasets. Our dataset consists of 75 sequences with a total\nof 1.1M RGB-D frames, hand and object poses, and per-hand fine-grained action\nannotations. We develop an automatic system for multi-view multi-hand 3D pose\nan- notation capable of tracking hand poses over time. We inte- grate and test\nit against publicly available datasets. Finally, we evaluate our dataset on the\ntasks of Hand Mesh Recov- ery (HMR) and Hand Action Segmentation (HAS).",
        "updated": "2024-02-27 18:51:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17758v1"
    },
    {
        "title": "LoDIP: Low light phase retrieval with deep image prior",
        "authors": "Raunak ManekarElisa NegriniMinh PhamDaniel JacobsJaideep Srivastava",
        "links": "http://arxiv.org/abs/2402.17745v1",
        "entry_id": "http://arxiv.org/abs/2402.17745v1",
        "pdf_url": "http://arxiv.org/pdf/2402.17745v1",
        "summary": "Phase retrieval (PR) is a fundamental challenge in scientific imaging,\nenabling nanoscale techniques like coherent diffractive imaging (CDI). Imaging\nat low radiation doses becomes important in applications where samples are\nsusceptible to radiation damage. However, most PR methods struggle in low dose\nscenario due to the presence of very high shot noise. Advancements in the\noptical data acquisition setup, exemplified by in-situ CDI, have shown\npotential for low-dose imaging. But these depend on a time series of\nmeasurements, rendering them unsuitable for single-image applications.\nSimilarly, on the computational front, data-driven phase retrieval techniques\nare not readily adaptable to the single-image context. Deep learning based\nsingle-image methods, such as deep image prior, have been effective for various\nimaging tasks but have exhibited limited success when applied to PR. In this\nwork, we propose LoDIP which combines the in-situ CDI setup with the power of\nimplicit neural priors to tackle the problem of single-image low-dose phase\nretrieval. Quantitative evaluations demonstrate the superior performance of\nLoDIP on this task as well as applicability to real experimental scenarios.",
        "updated": "2024-02-27 18:29:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.17745v1"
    }
]