When Your AI Deceives You: Challenges with
Partial Observability of Human Evaluators in Reward Learning
LeonLang*1 DavisFoote*2 StuartRussell2 AncaDragan2 ErikJenner2 ScottEmmons*2
Abstract In reality, however, this assumption is false. Even when
humanshaveacompleteviewoftheenvironment,theymay
Pastanalysesofreinforcementlearningfromhu-
nothaveacompleteunderstandingofitandcannotprovide
man feedback (RLHF) assume that the human
ground-truth feedback (Evans et al., 2016). Furthermore,
fully observes the environment. What happens
asAIagentsaredeployedinincreasinglycomplexenviron-
when human feedback is based only on partial
ments,humanswillonlyhaveapartialviewofeverything
observations? We formally define two failure
theagentsview. Howdoesthehuman’spartialobservability
cases: deception and overjustification. Model-
impactlearningfromhumanfeedback?
ingthehumanasBoltzmann-rationalw.r.t. abe-
liefovertrajectories,weproveconditionsunder Webeginourinvestigationwithanexample,illustratedin
which RLHF is guaranteed to result in policies Figure1. AnAIassistantishelpingauserinstallsoftware.
that deceptively inflate their performance, over- It is possible for the assistant to hide error messages by
justifytheirbehaviortomakeanimpression,or redirectingthemto/dev/null. Wemodelthehumanas
both. Tohelpaddresstheseissues,wemathemat- havingabeliefBoverthestateandextendtheBoltzmann-
ically characterize how partial observability of rational assumption from prior work to incorporate this
theenvironmenttranslatesinto(lackof)ambigu- belief. In the absence of an error message, the human is
ityinthelearnedreturnfunction. Insomecases, uncertainiftheagentleftthesystemuntouchedorhidtheer-
accountingforpartialobservabilitymakesitthe- rormessagefromafailedinstallation. Wefindthatbecause
oreticallypossibletorecoverthereturnfunction thehumandispreferstrajectorieswitherrormessages,the
andthustheoptimalpolicy,whileinothercases, AIlearnstohideerrormessagesfromthehuman. Figure2
thereisirreducibleambiguity. Wecautionagainst showsinfullmathematicaldetailhowthisfailureoccurs. It
blindly applying RLHF in partially observable alsoshowsasecondfailurecase,wheretheAIcluttersthe
settingsandproposeresearchdirectionstohelp outputwithoverlyverboselogs.
tacklethesechallenges.
Generalizingfromtheseexamples,weformalizetworisks,
dual to each other: deception and overjustification. We
provideamathematicaldefinitionofeach. Whentheobser-
1.Introduction
vationkernel,i.e. thefunctionspecifyingtheobservations
Reinforcementlearningfromhumanfeedback(RLHF)and given states, is deterministic, Theorem 4.5 analyzes the
itsvariantsarewidelyusedforfinetuningfoundationmod- propertiesofsuboptimalpolicieslearnedbyRLHF.These
els, includingChatGPT(OpenAI,2022), Bard(Manyika, policiesthenexhibitdeceptiveinflation—wheretheyap-
2023), Gemini (Gemini Team, 2023), Llama 2 (Touvron peartoproducehigherrewardthantheyactuallydo—or
et al., 2023), and Claude (Bai et al., 2022; Anthropic, overjustification—wheretheyincuracostinordertomake
2023a;b). PriortheoreticalanalysisofRLHFassumesthat agoodappearance—orboth.
the human fully observes the state of the world (Skalse
AfterseeinghownaiveRLHFfails,weask: canwedobet-
etal.,2023). Underthisassumption,itispossibletorecover
ter? Underourmodelofthehuman’sbeliefandfeedback,
theground-truthreturnfunctionfromBoltzmann-rational
wemathematicallycharacterizetheambiguityinthereturn
humanfeedback(seeProposition3.1).
functionthatariseswithhumanfeedbackfrompartialob-
servations. ThisisTheorem5.1. ApplyingTheorem5.1to
*Core research contributor 1Amsterdam Machine Learning
Lab, UniversityofAmsterdam2CenterforHuman-Compatible exampleswherenaiveRLHFfails,weseecaseswherethe
Artificial Intelligence, University of California, Berkeley. Cor- ambiguityinthereturnfunctionistotallyresolved. Inthese
respondence to: Leon Lang <l.lang@uva.nl>, Scott Emmons cases, we also show that the return function inference is
<emmons@berkeley.edu>.
robusttosmallmisspecificationsinthehumanbeliefmodel.
Inothercases,wefindirreducibleambiguity,leadingtore-
1
4202
beF
72
]GL.sc[
1v74771.2042:viXraChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Figure1.AhumancomparestrajectoriestoprovidedataforRLHF.Ratherthanobserving⃗sand⃗s′directly,thehumanseesobservations
⃗oand⃗o′,whichtheyusetoestimatethetotaltruerewardofeachtrajectory.Inthisexample,anagentexecutesshellcommandstoinstall
NvidiadriversandCUDA.Both⃗sand⃗s′containanerror,butin⃗s′,theagenthidestheerror.Thehumanthusbelieves⃗s′isthebetter
trajectory,rewardingtheagent’sdeceptivebehavior.TheunderlyingMDPandobservationfunctionareinFig.2A.
turnfunctionsconsistentwiththehuman’sfeedbackwhose onthehumanbeliefmodelandmisspecificationsthereoffor
optimalpolicieshavearbitrarilyhighregret. thecasethatthehumanonlyreceivespartialobservations.
We conclude by discussing the implications of our work. Theproblemofhumaninterpretationsofobservationswas
Whenfeedbackisbasedonpartialobservations,wecaution alreadybrieflymentionedinAmodeietal.(2017),wherehu-
againstblindlyapplyingRLHF.Tohelpaddressthischal- manevaluatorsmisinterpretedthemovementofarobothand
lenge, wesuggestseveralavenuesforfutureresearch. In insimulation. ElicitingLatentKnowledge(Christianoetal.,
particular,modelinghumanbeliefscouldhelpAIinterpret 2021)positsthatinordertogiveaccuratefeedbackfrom
humanfeedback,andelicitingknowledgefromAIscould partialobservations,thehumanneedstobeabletoquery
providehumanswithinformationtheycan’tobserve. latent knowledge of the AI system about the world state.
Howtodothisiscurrentlyanunsolvedproblem(Christiano
2.Relatedwork
&Xu,2022). Comparedtotheseearlyinvestigations,we
clearlyformalizeamodelofhumansunderpartialobserv-
A systematic review of open problems and limitations of
abilityandprovidenewmathematicalresultsanalyzingthe
RLHF,includingabriefdiscussionofpartialobservability,
resultingfailuremodesandtheirpotentialmitigations.
can be found in Casper et al. (2023). RLHF is a special
case of reward-rational choice (Jeon et al., 2020), a gen- Relatedwork(Zhuang&Hadfield-Menell,2020)analyzes
eral framework which also encompasses demonstrations- the consequences of aligning an AI with a proxy reward
basedinversereinforcementlearning(Ziebartetal.,2008; functionthatomitsattributesthatareimportanttothehu-
Ngetal.,2000)andlearningfromtheinitialenvironment man’svalues,whichcouldhappeniftherewardfunctionis
state(Shahetal.,2019),andcanbeseenasaspecialcase basedonabeliefovertheworldstategivenlimitedinforma-
ofassistanceproblems(Fernetal.,2014;Hadfield-Menell tion. Anotherinstancearerecommendationsystems(Stray,
etal.,2016;Shahetal.,2021). Inallofthese,thereward 2023), where user feedback does not depend on informa-
functionislearnedfromhumanactions,whichinthecase tionnotshown—whichiscruciallypartoftheenvironment.
ofRLHFaresimplypreferencestatements. Thisrequiresus Siththaranjan et al. (2023) analyze what happens under
tospecifythehumanpolicyofactionselection—Boltzmann RLHF if the learning algorithm doesn’t have all the rel-
rationalityintypicalRLHF—whichcanleadtowrongre- evantinformation(e.g.abouttheidentityofhumanraters),
ward inferences when this specification is wrong (Skalse complementing our study of what happens when human
&Abate,2022);unfortunately,thehumanpolicycanalso ratersaremissinginformation.
notbelearnedalongsidethehuman’svalueswithoutfurther
Our work argues that deception can result from applying
assumptions(Mindermann&Armstrong,2018). Insteadof
RLHFfrompartialobservations. Butdeceptionmayalso
amodelofthehumanpolicy,inthispaperwemostlyfocus
2ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
emergeforotherreasons: Hubingeretal.(2019)introduced fromπinteractingwiththeenvironment. Thepolicyisthen
thehypotheticalconceptofdeceptivealignment,asituation usuallytrainedtomaximizethepolicyevaluationfunction,
inwhichanAIsystemmaydeceivethehumanintobelieving which is the on-policy expectation of the return function:
(cid:2) (cid:3)
itisalignedwhileitplansalatertakeover. Recently,there J(π):=E G(⃗s) .
⃗s∼Pπ(·)
hasbeenacallforempiricalsupportofthispossibility(Hub-
3.2.RLHFandidentifiabilityfromfullobservations
ingeretal.,2023). Onceamodelisdeceptivelyaligned,this
maybehardtoremove(Hubingeretal.,2024). Underthe Inpractice,therewardfunctionRmaynotbeknownand
definitionfromParketal.(2023)wepreviouslydiscussed, needstobelearnedfromhumanfeedback. Inasimpleform
GPT-4wasshowntoengageindeceptivebehaviorinasim- ofRLHF(Christianoetal.,2017),thisfeedbacktakesthe
ulatedenvironment(Scheureretal.,2023). Thereisalsothe formofbinarypreferencecomparisonsbetweentrajectories:
systematicinducementoftruebeliefs(Reddyetal.,2020), a human is presented with state sequences ⃗s and ⃗s′ and
whichwewouldnotcalldeceptive. Athirdlineofresearch choosetheonetheyprefer. UndertheBoltzmannrationality
definesdeceptioninstructuralcausalgamesandaddsthe model,weassumethehumanpicks⃗swithprobability
aspectofintentionality(Wardetal.,2023),whichrecently
gotpreliminaryempiricalsupport(Hofsta¨tteretal.,2023).
PR(cid:0) ⃗s≻⃗s′(cid:1) :=σ(cid:16) β(cid:0) G(⃗s)−G(⃗s′)(cid:1)(cid:17)
, (1)
Finally,wementionconnectionstotruthfulAI(Evansetal.,
where β > 0 is an inverse temperature parameter and
2021; Lin et al., 2022; Burns et al., 2023; Huang et al., σ(x) := 1 is the sigmoid function (Bradley &
2023),whichisaboutensuringthatAIsystemstellthetruth 1+exp(−x)
Terry,1952;Christianoetal.,2017;Jeonetal.,2020).
about aspects of the real world. Partial observability is a
mechanismthatmakesitfeasibleformodelstoliewithout Animportantquestionisidentifiability: Intheinfinitedata
beingcaught: Ifthehumanevaluatordoesnotobservethe limit, do the human choice probabilities PR collectively
fullenvironment,ordoesnotfullyunderstandit,thenthey provideenoughinformationtouniquelyidentifythereward
maynotdetectwhentheAIislying. Morespeculatively, functionR? ThisisansweredbySkalseetal.(2023,Theo-
wecanimaginethatAImodelswillatsomepointbepartof rem3.9andLemmaB.3):
thedistributionofhumanobservationsP bytellingusthe
O Proposition3.1(Skalseetal.(2023)). LetR bethetrue
outcomesoftheiractions. E.g.,imagineanAIsystemthat
rewardfunctionandGthecorrespondingreturnfunction.
managesyourassetsandassuresyouthattheyareincreasing
ThenthecollectionofallchoiceprobabilitiesPR(⃗s≻⃗s′)
invaluewhiletheyareactuallynot. Inourwork,weleave
for state sequence pairs⃗s,⃗s′ ∈ S⃗ determines the return
this additional problem out of the analysis by assuming
functionGonsequences⃗s∈S⃗uptoanadditiveconstant.
that the observation distribution P is a fixed part of the
O
environmentthatcannotbeoptimized.
Thereasonissimple:sincethesigmoidfunctionisbijective,
3.Rewardidentifiabilityfromfullobservations PR determinesthedifferenceinreturnsbetweenanytwo
trajectories. Fromthatwecanreconstructindividualreturns
InthissectionwereviewMarkovdecisionprocessesand uptoanadditiveconstant.
previousresultsontheidentifiabilityoftherewardfunction
TherewardfunctionRisnotnecessarilyidentifiablefrom
underfullyobservedRLHF.
preference comparisons, see Skalse et al. (2023, Lemma
3.1.Markovdecisionprocesses B.3) for a precise characterization of the remaining am-
biguity. However, the optimal policy only depends on R
We assume Markov decision processes (MDPs) given by
indirectlythroughthereturnfunctionG,andisfurthermore
(S,A,T,P ,R,γ). ForanyfinitesetX,let∆(X)bethe
0 invariant under adding a constant to G. This means that
setofprobabilitydistributionsonX. ThenS isafiniteset
inthefullyobservablesetting,Boltzmannrationalcompar-
ofstates,Aisafinitesetofactions,T :S×A→∆(S)is
isonscompletelydeterminetheoptimalpolicy. InSection5,
atransitionkernelwrittenT(s′ |s,a)∈[0,1],P ∈∆(S)
0 wewillshowunderwhichconditionsthisguaranteebreaks
isaninitialstatedistribution,R:S →Risthetruereward
inthepartiallyobservablesetting.
function,andγ ∈[0,1]isadiscountfactor.
Apolicyisgivenbyafunctionπ :S →∆(A). Weassume
4.TheimpactofpartialobservationsonRLHF
afinitetimehorizonT. LetS⃗bethesetofstatesequences
⃗s=s ,...,s thatarepossible,so⃗s∈S⃗ifithasastrictly We now analyze failure modes of a naive application of
0 T
positiveprobabilityofbeingsampledfromP ,T,andan RLHFfrompartialobservations,boththeoreticallyandwith
0
explorationpolicyπwithπ(a|s)>0foralls∈S,a∈A. examples. Theresultingpoliciescanshowtwodistinctbe-
Asequence⃗sgivesrisetoareturnG(⃗s):=(cid:80)T γtR(s ). haviorpatternsthatwecalldeceptiveinflationandoverjus-
t=0 t
Let Pπ(⃗s) be the on-policy probability that⃗s is sampled tification,bothofwhichwewillalsoformallydefine. This
canleadtosignificantregretcomparedtoanoptimalpolicy.
3ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Later, in Section 5, we will see that an adaptation of the ThebeliefBcaninprinciplebeanydistributionaslongas
usualRLHFprocesscansometimesavoidtheseproblems. itsumsto1over⃗s. Thehumancouldarriveatsuchabelief
viaBayesianupdates,assumingknowledgeofP ,T,P ,
To model partial observability, we introduce a space of 0 O
and a prior over the policy that generates the trajectories
possible observations o ∈ Ω and and observation ker-
(seeAppendixB.1). Noneofourresultsrelyonthismore
nel with probabilities P (o | s) ∈ [0,1]. We write
O detailedmodel.
P (⃗o | ⃗s) :=
(cid:81)T
P (o | s ) for the probability of
O⃗ t=0 O t t
an observation sequence. Analogously to S⃗, we write Ω⃗ What happens if the human gives feedback according to
Eq. (2) but we infer a return function using the standard
for the set of observation sequences that occur with non-
zero probability, i.e.,⃗o ∈ Ω⃗ if and only if there is⃗s ∈ S⃗ RLHFalgorithmbasedonEq.(1)? Itiseasytoshow(as-
suchthat(cid:81)T
P (o | s ) > 0. IfP andP aredeter-
sumingdeterministicobservations,seeAppendixC.1)that,
t=0 O t t O O⃗
ministic, then we write O : S → Ω and O⃗ : S⃗ → Ω⃗ for uptoanadditiveconstant,RLHFwillinferthefollowing
observationreturnfunction:
the corresponding observation functions with O(s) = o
and O⃗(⃗s) = ⃗o for o and ⃗o with P (o | s) = 1 and
O
P O⃗(⃗o|⃗s)=1,respectively. G obs(⃗s):= E (cid:104)(cid:0) B·G(cid:1) (⃗o)(cid:105) , (3)
4.1.WhatdoesRLHFlearnfrompartialobservations?
⃗o∼P O⃗(·|⃗s)
Weconsiderthesettingwherethestateisfullyobservableto which is simply (cid:0) B·G(cid:1)(cid:0) O⃗(⃗s)(cid:1) for deterministic P and
O⃗
thelearnedpolicy,buthumanfeedbackmaydependonlyon correspondingobservationfunctionO⃗.
asequenceofobservations. Wethenassumethatthehuman
givesfeedbackunderaBoltzmannrationalmodelsimilar SounlikeinthefullyobservablecaseofProposition3.1,an
toEq.(1). However,thehumannowdoesn’thaveaccessto incorrectreturnfunctionmightbeinferred. Now,definethe
thetruesequence⃗s. Soweassumethatinstead,theyform resultingpolicyevaluationfunctionJ obsby
some belief B(⃗s | ⃗o) ∈ [0,1] about the state sequence⃗s
(cid:2) (cid:3)
basedontheobservations⃗o. Wethenassumepreferences J obs(π):= E G obs(⃗s) . (4)
⃗s∼Pπ(⃗s)
areBoltzmannrationalintheexpected returnsunderthis
belief,insteadoftheactualreturns.
Thisisthefunctionwhichastandardreinforcementlearning
The assumption of Boltzmann rationality is false in prac- algorithmwouldoptimizegiventheinferredreturnfunction
tice (Evans et al., 2015; Majumdar et al., 2017; Buehler G obs. Wesummarizethisasfollows:
et al., 1994), but note that it is an optimistic assumption: Proposition4.1. Inpartiallyobservablesettingswithde-
Eventhoughourmodelisasimplification,weexpectthat terministicobservations,apolicyisoptimalaccordingto
practicalissuescanbeatleastasbadastheoneswewill RLHF,i.e.,accordingtoareturnfunctionmodelthatwould
discuss. SeealsoExampleC.4foranexampleshowingthat be learned by RLHF with infinite comparison data, if it
itissometimesgenerallynotpossibletofindahumanmodel maximizesJ .
obs
thatleadstogoodoutcomesunderRLHF.Futureworkcould
investigatedifferenthumanmodelsandtheirimpactunder Notethatinthisdefinition,andspecificallyintheformula
partialobservabilityingreaterdetail. forG ,thehumandoesnothaveknowledgeofthepolicy
obs
π that generates the state sequence ⃗s. In Appendix C.2,
Toformalizethismodel,wecollectthehumanbeliefsinto
amatrixB := (cid:0) B(⃗s |⃗o)(cid:1) ∈ RΩ⃗×S⃗ . Thentheexpected webrieflydiscusstheunrealisticcasethatthehumandoes
⃗o,⃗s (cid:2) (cid:3) knowtheprecisepolicyandisanidealBayesianreasoner
returnsforobservations⃗oaregivenbyE G(⃗s) =
⃗s∼B(·|⃗o) overthetrueenvironmentdynamics. Inthatcase,J =
(B·G)(⃗o). Here,weviewG∈RS⃗ andB·G∈RΩ⃗ asboth J, i.e. there is no discrepancy between true and info eb rs red
columnvectorsandasfunctions. Pluggingtheseexpected returns. Intuitively,evenifthehumanwouldnotmakeany
returnsintoEq.(1),weget observations,theycouldgivecorrectfeedbackessentially
byestimatingthepolicy’sexpectedreturnexplicitly.
PR(cid:0) ⃗o≻⃗o′(cid:1) :=σ(cid:16) β(cid:0) (B·G)(⃗o)−(B·G)(⃗o′)(cid:1)(cid:17)
. (2)
Inourcase,however,apolicyachievinghighJ produces
obs
statesequences⃗swhoseobservationsequenceO⃗(⃗s)looks
Ifobservationsaredeterministic,wecanwriteO⃗(⃗s)=⃗ofor good according to the human’s belief B(cid:0) ⃗s′ | O⃗(⃗s)(cid:1) and
⃗owithP O⃗(⃗o|⃗s)=1. Wecanthenrecoverthefullyobserv- return G(⃗s′). This already hints at a possible source of
ablecaseEq.(1)withBandO⃗ beingtheidentity. Thisis deception: ifthepolicyachievessequenceswhoseobserva-
aninstanceofreward-rational(implicit)choice(Jeonetal., tionslookgoodattheexpenseofactualvalueG(⃗s),thenwe
2020), with the function⃗o (cid:55)→ B(· | ⃗o) as the grounding mightintuitivelycallthisdeceptivebehavior. Wewillnext
function. definedeceptionandanalyzethispointingreaterdetail.
4ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
4.2.DeceptiveinflatingandoverjustificationinRLHF actions that are more visible to the human than optimal
actions. Another pattern is to “pay” some resource (wall
We will evaluate state sequences based on the extent to
time;humanattention)toprovideinformationtothehuman,
which they lead to the human overestimating or underes-
whosedesireforthatinformationdoesnotjustifythecost.
timatingtherewardinexpectation. RecallthatG from
obs
Eq.(3)measurestheexpectedreturnfromtheperspective Wenowstateakeyresult. SeeAppendixC.3forproofs.
ofahumanwithsomebelieffunctionBandaccesstoonly Lemma 4.4. Let π and π be two policies. If J(π) <
ref
observations,whereasGarethetruereturns. Thatleadsus J(π ) and J (π) ≥ J (π ), then relative to π , π
ref obs obs ref ref
tothefollowingdefinition: mustexhibitdeceptiveinflating,overjustification,orboth.
Definition4.2(OverestimationandUnderestimationError). Theorem4.5. AssumethatP isdeterministic. Letπ∗ be
O obs
Let⃗sbeastatesequence. Wedefineitsoverestimationerror anoptimalpolicyaccordingtoanaiveapplicationofRLHF
E+andunderestimationerrorE−by underpartialobservability,andletπ∗beanoptimalpolicy
accordingtothetrueobjectiveJ. Ifπ∗ isnotJ-optimal,
E+(⃗s):=max(cid:0)
0,G
obs(⃗s)−G(⃗s)(cid:1)
, then relative to π∗, π o∗
bs
must
exhibitob ds
eceptive inflating,
E−(⃗s):=max(cid:0)
0,G(⃗s)−G
(⃗s)(cid:1)
.
overjustification,orboth.
obs
Anygivenstatetrajectory⃗smaybemoreorlesslikelyunder
Wefurtherdefinetheaverageoverestimation(underestima-
tion)errorunderapolicyπ byE+ (π):=E [E+(⃗s)] π o∗ bsthanπ∗,regardlessofhumanestimation,solongason
⃗s∼Pπ netπ∗ exhibitsdeceptiveinflatingoroverjustification.
andE− (π):=E [E−(⃗s)]. obs
⃗s∼Pπ
Weendthissectionwithabriefdiscussionof“deception.”
Considertwopoliciesπ andπ whichattainthesameav- Parketal.(2023)definedeceptionas
1 2
+ +
eragereturn;J(π )=J(π ). SupposeE (π )>E (π ).
1 2 1 2 “thesystematicinducementoffalsebeliefsinthe
Thencomparedtoπ ,π putsmoreprobabilitymassontra-
2 1 pursuitofsomeoutcomeotherthanthetruth.”
jectories⃗swhichproduceobservationsthatthehumanover-
estimates(believestohavebeenproducedbybettertrajec- We formalize this definition for our setting in which an
toriesthan⃗s). Similarly,supposethatE− (π )<E− (π ). agentactsinanMDPandahumanestimatesavaluegiven
1 2
Then compared to π , π puts less probability mass on partial observations of the agent’s trajectories. The esti-
2 1
trajectories⃗swhichproduceobservationsthatthehuman mationtargetcouldbethetruereturn,asinthepreceding
mistakenlyunderestimates(believestohavebeenproduced definitions, or the human could instead estimate whether
by worse trajectories than⃗s). An agent seeking to maxi- a particular undesired state was accessed or estimate the
mizepositivefeedbackwillprefersuchaπ toπ despite statesequenceitself. Wedefine“falsebeliefs”withsome
1 2
their identical performance. We formalize this intuition measureE(⃗s)oftheaverageerrorofthehumanestimation
beginningwiththefollowingdefinitions. givenobservationssampledfrom⃗s. Wecapture“systematic
inducement” by considering the on-policy average of E,
Definition4.3(DeceptiveInflatingandOverjustification).
E(π)=E [E(⃗s)] and measuring this against a refer-
A policy π exhibits deceptive inflating relative to π if ⃗s∼Pπ
ref encepolicy. Wefurtherintroduceanobjectivefunctionuto
+ +
E (π)>E (π )andJ (π)≥J (π ).
ref obs obs ref operationalize“thepursuitofsomeoutcome.”
A policy π exhibits overjustification relative to π if Definition4.6(Deception). Apolicyπexhibitsdeception
ref
E− (π)<E− (π ref)andJ(π)<J(π ref). (“is deceptive”) relative to the reference policy π ref with
respect to error measure E and objective u if E(π) >
Deceptive inflating behaviors mislead the human toward E(π )andu(π)≥u(π ).
ref ref
higherestimatesoftheagent’sperformance(bydefinition
Thisdefinitiontendstoagreewithhumanintuitionabout
theyproducemorefavorableobservations). Onesuchpat-
whether“deception”hasoccurredwhenuisanobjective
terninvolvestakingundesiredactions(stealingmoney;ma-
forwhichπwasselected,e.g. ifweobtainedπfromanRL
nipulatingexperimentaldata)concealedwithinhumanblind
spots if those actions allow the agent to later take a visi-
algorithmanduistheon-policyexpectedtotalreward.
blehigh-rewardaction(depositingmoney;positiveresults). We can thus see that deceptive inflating is a special case
Anotherpatterniscreatingblindspotsbytamperingwith ofdeception: anypolicyexhibitingdeceptiveinflatingrela-
sensors(turningacamera;alteringloggingbehavior). tivetoπ isdeceptiverelativetoπ withrespecttoerror
ref ref
Overjustification1 behaviors correct the human toward measureE+andobjectiveJ obs.
higher estimates of the agent’s performance when this is
1ThisnameisanodtotheOverjustificationeffectfrompsychol-
notdesiredbythehuman(bydefinition,theyattainlower
ogy(Deci&Flaste,1995),inwhichsubjectsbecomedependent
reward). Onesuchpatterninvolvesfavoringlesseffective onanextrinsicsourceofmotivationtosustainworkonatask.
5ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Figure2. TwoexampleMDPswithobservationfunctionsinwhichRLHFchoosesundesirablepolicies.Eachboxdepictsastatewith
afootershowingthe(deterministic)observationproducedbythatstate.Outgoingedgesfromeachboxareavailableactions.Amore
detaileddiagramforthefirstMDP,withexplicitshellcommandsandlogmessages,isavailableinAppendixC.4.
4.3.Deceptionandoverjustificationinexamples There are three pairs of trajectories which produce iden-
tical observations. Here we address the most promi-
Wenowexamineexamplesthatillustratethefailuremodes
nent (see Appendix C.4 for the others): after success-
ofRLHFinthepresenceofpartialobservability. Foreach
fully installing drivers, a failed CUDA installation with
of the following, we will characterize the policy which
2>/dev/null (SIL T) and simply exiting (SITT)
maximizesJ ,asthisisthepolicyRLHFselectsforwhen H
obs bothproducealogcontainingonlyasuccessconfirmation
observationsaredeterministic;seeProposition4.1.
for the drivers (o o o o ), stylized as ⃗o in Fig. 1. Let
∅ I ∅ ∅ 2
(cid:0) (cid:1)
OurexamplesfeatureanagenttrainedwithRLHFtocom- p H :=B ⃗s=SIL HT |⃗o=o ∅o Io ∅o ∅ ∈(0,1)bethehu-
plete tasks in a user’s terminal. The output of each com- man’ssuspicion,uponseeingonlyasuccessconfirmation
mand(stdoutandstderr)ispipedtoalogfile,which forthedrivers,thatafailedCUDAinstallationwashidden.
iswhatthehumanevaluatorseeswhenmakingchoicesfor
We consider two cases which differ only in whether the
RLHF.WeassumethatthefinalstateT hasaself-transition,
expectedvalueofattemptingtoinstallCUDA(withdefault
episodes have a fixed horizon length of 3 (meaning state
logging,a )afterinstallingthedriversispositive. InExam-
sequences have length 4: s ,...,s ), γ = 1, and there C
0 3 ple4.7,thesuccessprobabilitypishighenoughthatthetrue
is a fixed initial state s = S. Furthermore, both exam-
0 optimalpolicyistoattempttheinstallation;inExample4.8,
plesfeatureafixedtransitionprobability0 < p < 1. We
pislowenoughthattheoptimalpolicyistoterminate.
alsoassumethatthehuman’sbeliefonlysupportspossible
explanations: B(⃗s|⃗o)>0 =⇒ O⃗(⃗s)=⃗o. Example 4.7. If the true-reward-optimal policy π∗ is to
attempt to install CUDA with default logging (a ) after
C
4.3.1.EXAMPLEA:HIDINGFAILURES
installingdrivers,butp < 5 ,thentheJ -maximizing
H 5+r obs
SeeAppendixC.4forderivationsandatutorial-styleanaly- policy,whichwillbechosenbyRLHF,istoinsteadusethe
sisofthisexample. 2>/dev/nulltrick. Intuitively,thisisbecauseifthein-
stallationfails,thehumanislikelytobelievethattheagent
InFig.2A,theagent’staskistoinstallCUDA(statesW and
justdidn’tattempttoinstallCUDA,whichispreferredtoa
W ,with“W”for“Win”),whichrequiresfirstinstalling
H failedinstallation. Thisisdeceptiveinflating;theoveresti-
Nvidia drivers (state I). If the CUDA installation fails, +
mationerrorE forthispolicyis(1−p)(1−p )(5+r)>0,
one enters L or L (“L” for “lose”). This is the exam- H
H whereasE+ (π∗)=0,inlinewithTheorem4.5.
ple from Figure 1. The a action corresponds to a com-
C
mandtoinstallCUDAthatlogsbothsuccessesandfailures, Ifp > 5 ,thenRLHFwillselectπ∗.
H 5+r
whereasa (“H”for“Hiding”)correspondstoa with2>
H C Example4.8. Ifπ∗istoterminate(a )afterinstallingthe
/dev/nullappended,whichsuppresseserrormessages T
drivers,butp < 5 ,thenRLHFwilllearnthedeceptive
suchthatnologmessageisproducediftheinstallationfails. H 5+r
inflating2>/dev/nullpolicyasinExample4.7.
Thehumanmaydistinctlypreferthattheagentnothidea
failure(ahiddenfailurecannotbeimmediatelyaddressed); However,ifp > 5 ,RLHFwillchoosethepolicythat
H 5+r
wepricethisintotherewardfunctionwithapenaltyr ≥0 attemptstoinstallCUDAwithdefaultlogging. Intuitively,
on the reward at L . The agent may attempt the CUDA thisisbecausethecombinationofincreasedsuspicionthat
H
installationbeforeinstallingdrivers,butthiswillfail. theagentmightbehidingerrors(higherp )andincreased
H
penaltyonthissuspectedbehavior(higherr)alsopenalizes
6ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
our preferred trajectory SITT since it can be misinter- torythepolicyhasnocontrolover: Thepolicytriesto“hide
pretedashidingerrors. Thisisoverjustification;theoveres- badluck”and“revealgoodluckatacost”. ExampleC.7,
timationerrorE+ is0(thusthisisnotdeceptiveinflating), especially(a)and(c),showsthatthepoliciescomingoutof
andasrequiredbyTheorem4.5,theunderestimationerror anaiveapplicationofRLHFunderpartialobservabilitymay
E− is0,lowerthanE−(π∗)=p (5+r). besuboptimalwithpositiveE− (andzeroE+ )oroptimal,
H
+ −
but with positive E (and zero E ). Thus, there can be
4.3.2.EXAMPLEB:PAYINGTOREVEALINFORMATION suboptimalityevenifthepolicyisbetterthanitseems,and
anoverestimationerrorevenwhenthepolicyisoptimal.
In Fig. 2B, the agent’s task is to install Python (state I)
andtooptionallyfurtherinstallNumPy(statesW andW ). 5.Towardaddressingpartialobservabilityby
V
Thea D actioncorrespondstoacommandtoinstallNumPy modelinghumanbeliefs
with“default”settingswhichonlylogserrors,whereasa
V
correspondstothesamecommandwitha--verboseflag We’ve seen issues with standard RLHF when applied to
thataddsadditionalinfo.Inthecaseofasuccess,thehuman feedbackfrompartialobservations. Partoftheproblemis
distinctlyprefersnottoseethisverboseoutput; weprice modelmisspecification:thestandardRLHFmodelimplicitly
thisintotherewardfunctionwithapenaltyr > 0onthe assumesfullobservability. Assumingaccesstothehuman
rewardatW . andobservationmodels,whathappensiftheAIaccounts
V
forthemwhenlearningfromhumanfeedback?
Thereisonlyonepairoftrajectorieswhichproduceidenti-
calobservations: aftersuccessfullyinstallingPython,asuc- Inthissection,weshowthatcorrectlymodelingthehuman
cessfulNumPyinstallationwithdefaultlogging(SIWT) cansometimesresolvetheissuesfromtheprevioussection,
andsimplyexiting(SITT)bothproducealogcontaining lettingusfindanoptimalpolicy. Inothercases,however,
only a success confirmation for Python (o o o o ). Let a new ambiguity problem arises: feedback from partial
∅ I ∅ ∅
p :=B(⃗s=SIWT |⃗o=o o o o )∈(0,1) be the hu- observationsisnotalwaysenoughtouniquelydeterminethe
D ∅ I ∅ ∅
man’soptimism,uponseeingonlyasuccessconfirmation optimalpolicy. Wecharacterizetheextentofthisambiguity
for Python, that NumPy was also successfully installed inTheorem5.1.
(withoutthe--verboseflag).
5.1.Ambiguityandidentifiabilityofreturnfunctions
Hereweconsideronlythecasewherepislargeenoughthat
Recallthatinthefullyobservablesetting,Boltzmannratio-
thetrueoptimalpolicyistoinstallPythonthenattemptto
nalcomparisonsletusinferthereturnfunctionG(uptoan
installNumPywithdefaultlogging(a ).
D additiveconstant),seeProposition3.1. Withananalogous
Example4.9. Ifπ∗istoattempttoinstallNumPywitha argument, we can show that the expected return function
D
(cid:16) (cid:17)
after installing Python, and p >q := 1 p(6−r)−1 , B·Gcanbeinferredusingfeedbackfrompartialobserva-
D 5
tionsthatfollowsourmodelinEq.(2).
then RLHF will select the policy that terminates after in-
stallingPython. Intuitively, thisisbecausetheagentcan Thisgivesusanimmediatefirstresult: ifthematrixBis
exploit the human’s optimism that NumPy was installed knownandinjective,i.e.,haslinearlyindependentcolumns,
quietlywithouttakingtheriskofanobservablefailure(L). wecanrecoverthereturnfunctionG. Moregenerally,we
+
Thisisdeceptiveinflating,withanoverestimationerrorE canrecoverGuptoanelementofkerB,thesetofallreturn
of5p ,greaterthanE+ (π∗)=0. functionsmappedtozerobyB. However,injectivityofB
D
is an unreasonably strong condition. At best, it can hold
If instead p < q, then RLHF will select the policy that
D whentherearejustasmanydifferentobservationsequences
attemptstheNumPyinstallationwithverboselogging(a ).
V asstatesequences,whichtypicallywon’tholdinrealistic
Intuitively,thisisbecausetheagentiswillingto“pay”the
environments.
costofrtruerewardtoprovetothehumanthatitinstalled
NumPy,evenwhenthehumandoesnotwanttoseethisproof. Interestingly, we can sometimes infer G even when B is
+ not injective if we assume that G is induced by a reward
Thisisoverjustification;theoverestimationerrorE is0
functionR. Intuitively,inferringGnaively—withouttaking
(thusthisisnotdeceptiveinflating),andtheunderestimation
errorE− is0,lowerthanE− (π∗)=5p(1−p ). intoaccountthatitisinducedbyR—meanslearningareturn
D valueforeachpossibletrajectory⃗s.Butthe“actual”degrees
offreedomareonlyonerewardvalueforeachstates,so
4.3.3.FURTHEREXAMPLES
therearemanymorefunctionsS⃗ →Rthanrealizablereturn
We show further, purely mathematical, examples in Ap- functions. Onlynon-injectivities“comingfrom”realizable
pendix C.5. Example C.6 shows that deceptiveness and returnfunctionsleadtoambiguity.
overjustifyingbehaviorevenappliestoaspectsofthetrajec-
7ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Figure3. ByTheorem5.1,evenwithinfinitecomparisondata,therewardlearningsystem(depictedasarobot)canonlyinferGuptothe
ambiguityimΓ∩kerB(purple).AddinganelementoftheambiguitytoGleadstotheexactsamechoiceprobabilitiesforallpossible
comparisons,andtherewardlearningsystemhasnowaytoidentifyGamongthereturnfunctionsinG+(imΓ∩kerB)(yellow).This
abstractdepictionignoresthelinearityofthesespaces;foramoreprecisegeometricdepictionofB,seeFigure4intheappendix.
Toformalizethisidea,wewriteΓ∈RS⃗×S forthematrix asΩ⃗ = S⃗ andtheinjectivityofO)andifthehumanisa
thatmapsanyrewardfunctiontothecorrespondingreturn
BayesianreasonerwithafullysupportedprioroverS⃗,then
function, i.e., (Γ·R)(⃗s) := (cid:80)T γtR(s ). Explicitly, its thereturnfunctionisidentifiablefromthechoicedataeven
t=0 t
matrixelementsaregivenbyΓ =(cid:80)T δ (s )γt,where ifthehuman’sobservationsarenotknowntothelearning
⃗ss t=0 s t
δ (s ) = 1 if s = s and δ (s ) = 0, else. Then the system,seealsoExampleB.30.
s t t s t
image imΓ is the set of all return functions that can be
For Theorem 5.1, we assumed knowledge of the human
realizedfromarewardfunction. TakingintoaccountthatG
beliefmatrixB,whichrealisticallywouldatbestbeknown
itselfisinimΓ,wecanimproveambiguityfromkerBto
approximately. ButinAppendixB.6,weshowthatsmall
kerB∩imΓ:
errorsinthebeliefmatrixusedforinferenceleadtoonly
Theorem 5.1. The collection of choice probabilities smallerrorsintheinferredreturnfunction:
(cid:16) PR(cid:0) ⃗o ≻ ⃗o′(cid:1)(cid:17) following a Boltzmann rational Theorem 5.3. Assume kerB∩imΓ = {0}. Let B :=
∆
⃗o,⃗o′∈Ω⃗
B+∆ be a small perturbation of B, where ∥∆∥ ≤ ρ
model as in Eq. (2) determines the return function G up
forsufficientlysmallρ. LetGbethetruereturnfunction
toadditionofanelementinkerB∩imΓandaconstant.
andassumethatthelearningsystem,assumingthehuman’s
Inparticular,thechoiceprobabilitiesdetermineGuptoan
beliefisB ,infersthereturnfunctionG˜ withtheproperty
additiveconstantifandonlyifkerB∩imΓ={0}. ∆
thatB ·G˜ hasthesmallestpossibleEuclideandistanceto
∆
B·G.
See Theorem B.2 and Corollary B.4 for full proofs, and
Figure 3 for a visual depiction. This result motivates the Let r(B) := B| be the (injective) restriction of the
imΓ
followingdefinition: operator B to imΓ. Then r(B)Tr(B) is invertible, and
thereexistsapolynomialQ(X,Y)ofdegree5suchthat
Definition5.2(Ambiguity). WecallkerB∩imΓtheambi-
g bu ai st ey dth chat oii cs ele pft roin bath be ilir te it eu sr an rf eun knct oio wn n.whentheobservation- ∥G˜−G∥≤ρ·∥G∥·Q(cid:16)(cid:13) (cid:13)(cid:0) r(B)Tr(B)(cid:1)−1(cid:13) (cid:13),∥r(B)∥(cid:17)
.
NotethatTheorem5.1generalizesthefullyobservedcase Inparticular,asweshowintheappendix,onecanuniformly
from Section 3.2 (Corollary B.10). We also extend the bound the difference between J and J . This yields a
G˜ G
theorem in Appendix B.4 to the case when the human’s regretboundbetweenthepolicyoptimalunderG˜ andπ∗.
observationsarenotknowntothelearningsystem. Special
casesofkerBandimΓandourtheoremcanbefoundin
5.2.ImprovementovernaiveRLHF
Appendices B.7 and B.5. In particular, if P is possibly
O⃗
non-deterministic and there is only “noise” in it (defined We saw in Example 4.9 a case where naive RLHF under
8ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
partial observability can lead to a suboptimal policy that B·G′ =0isequivalentto:
usesdeceptionoroverjustification. Appropriatelymodeling 0=(cid:0) B·G′(cid:1)
(o o o o )
partial observability can avoid this problem since in this ∅ I W ∅
case,kerB∩imΓ={0}. ThereasonisthatkerBleaves = E
(cid:2) G′(⃗s)(cid:3)
onlyonedegreeoffreedomthatisnot“time-separable”over
⃗s∼B(⃗s|o∅oIoWo∅)
=p′ ·G′(SIW T)+(1−p′ )·G′(SIWT)
states. H H H
=p′ ·R′(W )+(1−p′ )·R′(W).
Toshowthisindetail,letG′ =Γ(R′)∈kerB∩imΓ. We H H H
n abe oed utt to hesh so taw teG se′ q= ue0 n. ceS sin coce rreth spe oh nu dm ina gn tois to hn el oy bu sn erc ve ar tt ia oin
n
Thus,ifR′(W)=
p′
Hp′ H −1R′(W H),thenG′ ∈kerB∩imΓ,
meaningthatR+R′ hasthesamechoiceprobabilitiesas
sequence o o o o , the condition B·G′ = 0 already im-
∅ I ∅ ∅ R and can be inferred by the reward learning algorithm.
pliesG′(⃗s)=0forallstatesequencesexceptSIWT and
Inparticular,ifR′(W ) ≫ 0issufficientlylarge,thenin
SITT. From(B·G′)(o o o o )=0,onethenobtainsthe H
∅ I ∅ ∅ subsequentpolicyoptimization,thereisanincentivetohide
equation
themistakesandπ willbeselected,whichissuboptimal
H
withrespecttothetruerewardfunctionR.
(1−p
D)·(cid:0) R′(S)+R′(I)+2R′(T)(cid:1)
InExampleB.29,weshowacasewheresomereturnfunc-
(5)
+p
·(cid:0) R′(S)+R′(I)+R′(W)+R′(T)(cid:1)
=0.
tionswithintheambiguityofthetruereturnfunctioncan
D
beevenworsethansimplymaximizingJ . Thisgenerally
obs
raisesthequestionofhowtotie-breakreturnfunctionsin
Thus,ifoneofthetwostatesequencesinvolvedhaszero the ambiguity, or how to act conservatively given the un-
return,thentheotherhasaswell,assumingthat0̸=p ̸=1, certainty,inordertoconsistentlyimproveuponthesetting
D
andwearedone. inSection4.1.
Toshowthis,weusethatallotherstatesequenceshavezero 6.Conclusionsandfuturework
return: R′(S)+3R′(T)=0=R′(S)+R′(L)+2R′(T),
Inthispaper,weprovidedaconceptualandtheoreticalin-
fromwhichR′(L)=R′(T)follows. Then,fromR′(S)+
vestigationofchallengeswhenapplyingRLHFfrompartial
R′(I)+R′(L)+R′(T)=0,substitutingthepreviousresult
observations. First, we saw that applying RLHF naively
givesR′(S)+R′(I)+2R′(T) = 0, andsoEquation(5)
whenassumingfullobservabilitycanleadtodeceptivein-
resultsinR′(S)+R′(I)+R′(W)+R′(T)=0. Overall,
flating and overjustification behavior. Then, we showed
thisshowsG′ =Γ(R′)=0,andsokerB∩imΓ={0}.
thattheseproblemscansometimesbemitigatedbymaking
5.3.Returnambiguitycanremain thelearningalgorithmawareofthehuman’spartialobserv-
ability and belief model. This method, however, can fail
In Example 4.8, ambiguity that can lead to problematic
whenthereistoomuchremainingambiguityinthereturn
rewardinferencesremainsevenwhenaccountingforpartial
function. WethusrecommendcautionwhenusingRLHF
observability.Intuitively,sinceW andW receivethesame
H insituationsofpartialobservability,andtostudytheeffects
observation,thehumanchoiceprobabilitiesdon’tdetermine
ofthisinpractice. Werecommendfurtherresearchtostudy
thevaluesofR(W)andR(W )—theyonlydeterminetheir
H andimproveRLHFincaseswherepartialobservabilityis
averageoverthehuman’sbeliefwhenobservingo . Thus,
W unavoidable:
therewardlearningsystemcaninferarbitrarilyhighvalues
forR(W )whenproportionallydecreasingthevaluefor Fullfailuretaxonomy. Weshowedthatnaivelyapplying
H
R(W). Thiscanthenleadtoanincentivetohidetheerror RLHFinsituationsofpartialobservabilitycanleadtode-
messages,andthussuboptimality. ceptiveinflation,overjustification,orboth. Futureresearch
couldqualitativelyinvestigatepoliciesthatshowbothdecep-
Concretely, by Theorem 5.1, the ambiguity in the return
tiveandoverjustifiedbehaviororlookatstatesequences⃗s
functionleavingthechoiceprobabilitiesinvariantisgivenby
thatdecomposeintopartsthataredeceptiveoroverjustified.
kerB∩imΓ. LetR′ = (0,0,R′(W),0,R′(W ),0,0) ∈
H Finally,itwouldbedesirabletolearnwhichotherproblems
R{S,I,W,L,WH,LH,T}bearewardfunctionthatwewantto
may exist that are not covered by our taxonomy and can
parameterizesuchthatG′ :=Γ·R′endsupintheambigu-
occuralongsidedeceptionandoverjustification.
ity;here,R′isinterpretedasacolumnvector.
We want B·G′ = 0. Since the observation sequences Humanmodelsensitivityandgeneralizations. Inboth
⃗o = o o o o , ⃗o = o o o o , ⃗o = o o o o , or ⃗o = our analysis of a naive application of RLHF (Section 4)
∅ ∅ ∅ ∅ ∅ L ∅ ∅ ∅ I ∅ ∅
o o o o all cannot involve the states W or W , it is and accounting for partial observability (Section 5), we
∅ I L ∅ H
clear that they have zero expected return (B·G′)(⃗o). Set assumed that the actual human evaluators are Boltzmann
p′ := B(cid:0) SIW T | o o o o (cid:1) . Thentheconditionthat rationalasinEquation(2).AsarguedinExampleC.4,naive
H H ∅ I W ∅
9ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
RLHFsometimesfailsregardlessofthehumanmodel. In justificationthatcanberesolvedbyTheorem5.1,andwrote
general, itwouldbedesirabletoinvestigatebothsettings the majority of the appendix. Davis conjectured Propo-
with more general human models, and to learn how the sition 4.1, provided early empirical evidence that RLHF
results generalize to RLHF-variants like DPO (Rafailov underpartialobservationscanleadtodeception(notinthe
etal.,2023),othervariantsofrewardlearning(Jeonetal., paper),defineddeception/deceptiveinflationandoverjusti-
2020),andassistancegames(Hadfield-Menelletal.,2016). fication(withScott),provedTheorem4.5,anddeveloped
therunningexamplesandfigures. Scottguidedtheproject
Correctbeliefspecification. Whenourmodelofhuman directionandprioritization,gavetheconjectureandproof
choiceprobabilitiesfrom(2)issufficientlycorrectandwe ideaforTheorem5.3,andhelpeddeveloptherunningex-
wanttoapplyTheorem5.1inpractice,asoutlinedinAp- amples and deception definitions. Erik provided regular
pendixB.3,thenweneedtospecifythehumanbeliefmodel detailedfeedbackandguidanceandeditedthepaper. Anca
B(⃗s | ⃗o); how to do this is an open question. If one as- andStuartadvisedthisproject.
sumesthehumanisrationalasinAppendixB.1,thenthis
requiresspecifyingthehuman’spolicypriorB(π),which Acknowledgements
isalsoopen. Whetheritispossibletomeaningfullylearna
generativemodelforB(⃗s|⃗o)remainsanopenquestion. LeonLangthankstheCenterforHuman-CompatibleArtifi-
cialIntelligenceforhostinghimduringpartofthisproject,
Understandingtheambiguity. OnceB(⃗s|⃗o)isknown, andOpenPhilanthropyforfinancialsupport. WethankBen-
the reward inference may still have undesired outcomes jaminEysenbachandBenjaminPlautfordetailedcomments
unlesstheambiguitykerB∩imΓissufficientlysmall. A and feedback on this work, and we thank Elio A. Farina,
general characterization of this ambiguity going beyond Mary Marinou, and Alexandra Horn for assistance with
AppendixB.7wouldbedesirable. Thiswouldrequireun- graphicdesign.
derstandingimΓ,thesetofreturnfunctionsthatarisefrom
arewardfunctionoverstates. Whentheambiguityistoo References
large, as in Section 5.3, then learning a suitable reward
Amodei, D., Christiano, P., and Ray, A. Learning from
functionrequiresfurtherinductivebiases.
human preferences. https://openai.com/res
earch/learning-from-human-preferences,
Increasingtheeffectiveobservability. Nexttoincreas-
2017. Accessed: 2023-12-13.
ingtheobservabilityofthehumandirectly,itwouldhelp
ifthehumancouldquerythepolicyaboutreward-relevant Anthropic. IntroducingClaude. https://www.anthro
aspects of the environment to bring the setting closer to pic.com/index/introducing-claude,2023a.
RLHFfromfullobservations. Thisissimilartotheproblem Accessed: 2023-09-05.
ofelicitingthelatentknowledgeofapredictoroffutureob-
servations(Christianoetal.,2021;Christiano&Xu,2022). Anthropic. Claude’sConstitution. https://www.anth
Whilethismayavoidtheneedtospecifythehuman’sbelief
ropic.com/index/claudes-constitution,
modelB(⃗s | ⃗o), itrequiresunderstandingandeffectively 2023b. Accessed: 2023-09-05.
queryinganMLmodel’sbelief,includingtranslatingfrom
Bai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J.,
anMLmodel’sontologyintoahumanontology.
Jones, A., Chen, A., Goldie, A., Mirhoseini, A., McK-
innon, C., Chen, C., Olsson, C., Olah, C., Hernandez,
Impactstatement D., Drain, D., Ganguli, D., Li, D., Tran-Johnson, E.,
Perez, E., Kerr, J., Mueller, J., Ladish, J., Landau, J.,
RLHFanditsvariantsarewidelyusedtosteerthebehavior
Ndousse, K., Lukosuite, K., Lovitt, L., Sellitto, M.,
oflanguagemodels.Thus,thesoundnessofRLHFiscritical
Elhage, N., Schiefer, N., Mercado, N., DasSarma, N.,
to language models’ trustworthy deployment. Our work
Lasenby,R.,Larson,R.,Ringer,S.,Johnston,S.,Kravec,
showsthatpartialobservabilityofhumansposestheoretical
S.,ElShowk,S.,Fort,S.,Lanham,T.,Telleen-Lawton,
challengeforRLHF.Wehopeourworkstimulatesfurther
T.,Conerly,T.,Henighan,T.,Hume,T.,Bowman,S.R.,
researchinovercomingthischallenge.
Hatfield-Dodds,Z.,Mann,B.,Amodei,D.,Joseph,N.,
McCandlish, S., Brown, T., and Kaplan, J. Consti-
Authorcontributions tutional AI: Harmlessness from AI Feedback. arXiv
e-prints, art. arXiv:2212.08073, December 2022. doi:
TheprojectwasconceivedinparallelbyScottandDavis,
10.48550/arXiv.2212.08073.
withakeyshiftproposedbyLeon. LeonprovedProposi-
tion4.1andTheorems5.1and5.3,foundthefirstmathemat- Bradley,R.A.andTerry,M.E.RankAnalysisofIncomplete
icalexamplesofwhatbecamedeceptiveinflationandover- Block Designs: I.The Method ofPairedComparisons.
10ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Biometrika, 39(3/4):324–345, 1952. ISSN 00063444. Evans,O.,Stuhlmu¨ller,A.,andGoodman,N. Learningthe
URLhttp://www.jstor.org/stable/23340 preferencesofignorant,inconsistentagents. InProceed-
29. ings of the AAAI Conference on Artificial Intelligence,
volume30,2016.
Buehler,R.,Griffin,D.,andRoss,M. Exploringthe”Plan-
ning Fallacy”: Why People Underestimate Their Task Evans,O.,Cotton-Barratt,O.,Finnveden,L.,Bales,A.,Bal-
Completion Times. Journal of Personality and Social wit,A.,Wills,P.,Righetti,L.,andSaunders,W. Truthful
Psychology,67:366–381,091994. doi: 10.1037/0022-3 AI:DevelopingandGoverningAIthatdoesnotlie. arxiv
514.67.3.366. e-prints,2021.
Burns, C., Ye, H., Klein, D., and Steinhardt, J. Discov- Fern, A., Natarajan, S., Judah, K., and Tadepalli, P. A
ering Latent Knowledge in Language Models Without Decision-Theoretic Model of Assistance. J. Artif. Int.
Supervision. InTheEleventhInternationalConference Res.,50(1):71–104,may2014. ISSN1076-9757.
on Learning Representations, 2023. URL https:
//openreview.net/forum?id=ETKGuby0hcs. Geiger, D., Verma, T., and Pearl, J. Identifying indepen-
denceinbayesiannetworks.Networks,20:507–534,1990.
Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer,
URL https://api.semanticscholar.org/
J., Rando, J., Freedman, R., Korbak, T., Lindner, D.,
CorpusID:1938713.
Freire, P., Wang, T., Marks, S., Segerie, C.-R., Carroll,
M., Peng, A., Christoffersen, P., Damani, M., Slocum, Gemini Team, G. Gemini: A Family of Highly Capable
S.,Anwar,U.,Siththaranjan,A.,Nadeau,M.,Michaud, MultimodalModels. https://storage.google
E.J.,Pfau,J.,Krasheninnikov,D.,Chen,X.,Langosco, apis.com/deepmind-media/gemini/gemini
L.,Hase,P.,Bıyık,E.,Dragan,A.,Krueger,D.,Sadigh, _1_report.pdf,2023. Accessed: 2023-12-11.
D., and Hadfield-Menell, D. Open Problems and Fun-
damentalLimitationsofReinforcementLearningfrom Hadfield-Menell,D.,Dragan,A.,Abbeel,P.,andRussell,
HumanFeedback. arxive-prints,2023. S. CooperativeInverseReinforcementLearning. arXiv
e-prints,art.arXiv:1606.03137,June2016. doi: 10.485
Christiano,P.andXu,M.ELKprizeresults.https://ww
50/arXiv.1606.03137.
w.alignmentforum.org/posts/zjMKpSB2X
ccn9qi5t/elk-prize-results,2022.Accessed: Hofsta¨tter, F., Ward, F. R., HarrietW, Thomson, L., J, O.,
2024-02-15. Bartak,P.,andBrown,S.F. TallTalesatDifferentScales:
Evaluating Scaling Trends for Deception in Language
Christiano,P.,Leike,J.,Brown,T.B.,Martic,M.,Legg,S.,
Models. https://www.alignmentforum.org
andAmodei,D. DeepReinforcementLearningfromHu-
/posts/pip63HtEAxHGfSEGk/tall-tales-a
manPreferences. arXive-prints,art.arXiv:1706.03741,
t-different-scales-evaluating-scaling
June2017. doi: 10.48550/arXiv.1706.03741.
-trends-for,2023. Accessed: 2024-01-23.
Christiano,P.,Cotra,A.,andXu,M.ElicitingLatentKnowl-
Huang,L.,Yu,W.,Ma,W.,Zhong,W.,Feng,Z.,Wang,H.,
edge. https://docs.google.com/document
Chen,Q.,Peng,W.,Feng,X.,Qin,B.,etal. ASurveyon
/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1
HallucinationinLargeLanguageModels:Principles,Tax-
dwZXR37PC8/edit,2021. Accessed: 2023-04-25.
onomy,Challenges,andOpenQuestions. arXivpreprint
Deci,E.L.andFlaste,R. Whywedowhatwedo: Thedy- arXiv:2311.05232,2023.
namicsofpersonalautonomy. GPPutnam’sSons,1995.
Hubinger,E.,vanMerwijk,C.,Mikulik,V.,Skalse,J.,and
ElGhaoui,L.Inversionerror,conditionnumber,andapprox- Garrabrant,S. RisksfromLearnedOptimizationinAd-
imateinversesofuncertainmatrices. LinearAlgebraand vancedMachineLearningSystems. arXive-prints,art.
its Applications, 343-344:171–193, 2002. ISSN 0024- arXiv:1906.01820,June2019. doi: 10.48550/arXiv.1906.
3795. doi: https://doi.org/10.1016/S0024-3795(01)002 01820.
73-7. URLhttps://www.sciencedirect.com/
science/article/pii/S0024379501002737. Hubinger,E.,Schiefer,N.,Denison,C.,andPerez,E.Model
OrganismsofMisalignment: TheCaseforaNewPillar
SpecialIssueonStructuredandInfiniteSystemsofLinear
ofAlignmentResearch. https://www.alignmen
equations.
tforum.org/posts/ChDH335ckdvpxXaXX/m
Evans,O.,Stuhlmueller,A.,andGoodman,N.D. Learning odel-organisms-of-misalignment-the-c
thePreferencesofIgnorant,InconsistentAgents. arxiv ase-for-a-new-pillar-of-1,2023. Accessed:
e-prints,2015. 2024-01-23.
11ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Hubinger,E.,Denison,C.,Mu,J.,Lambert,M.,Tong,M., Rafailov,R.,Sharma,A.,Mitchell,E.,Ermon,S.,Manning,
MacDiarmid,M.,Lanham,T.,Ziegler,D.M.,Maxwell, C.D.,andFinn,C. DirectPreferenceOptimization: Your
T., Cheng, N., Jermyn, A., Askell, A., Radhakrishnan, LanguageModelisSecretlyaRewardModel. arxive-
A.,Anil,C.,Duvenaud,D.,Ganguli,D.,Barez,F.,Clark, prints,2023.
J., Ndousse, K., Sachan, K., Sellitto, M., Sharma, M.,
Reddy, S., Levine, S., and Dragan, A. D. Assisted Per-
DasSarma, N., Grosse, R., Kravec, S., Bai, Y., Witten,
ception: OptimizingObservationstoCommunicateState.
Z.,Favaro,M.,Brauner,J.,Karnofsky,H.,Christiano,P.,
arxive-prints,2020.
Bowman,S.R.,Graham,L.,Kaplan,J.,Mindermann,S.,
Greenblatt,R.,Shlegeris,B.,Schiefer,N.,andPerez,E.
Scheurer, J., Balesni, M., and Hobbhahn, M. Technical
SleeperAgents: TrainingDeceptiveLLMsthatPersist
Report: Large Language Models can Strategically De-
ThroughSafetyTraining. arxive-prints,2024.
ceivetheirUserswhenPutUnderPressure.arxive-prints,
2023.
Jeon, H. J., Milli, S., and Dragan, A. Reward-rational
(implicit)choice: Aunifyingformalismforrewardlearn- Shah,R.,Krasheninnikov,D.,Alexander,J.,Abbeel,P.,and
ing. In Larochelle, H., Ranzato, M., Hadsell, R., Bal- Dragan, A. The Implicit Preference Information in an
can, M., and Lin, H. (eds.), Advances in Neural In- Initial State. In International Conference on Learning
formation Processing Systems, volume 33, pp. 4415– Representations,2019. URLhttps://openreview
4426. Curran Associates, Inc., 2020. URL https: .net/forum?id=rkevMnRqYQ.
//proceedings.neurips.cc/paper_files
/paper/2020/file/2f10c1578a0706e06b6 Shah,R.,Freire,P.,Alex,N.,Freedman,R.,Krasheninnikov,
d7db6f0b4a6af-Paper.pdf. D.,Chan,L.,Dennis,M.D.,Abbeel,P.,Dragan,A.,and
Russell,S. BenefitsofAssistanceoverRewardLearning,
Lin,S.,Hilton,J.,andEvans,O. TruthfulQA:Measuring 2021. URL https://openreview.net/forum
HowModelsMimicHumanFalsehoods. arxive-prints, ?id=DFIoGDZejIB.
2022.
Siththaranjan, A., Laidlaw, C., and Hadfield-Menell, D.
Majumdar, A., Singh, S., Mandlekar, A., and Pavone, M. DistributionalPreferenceLearning: Understandingand
Risk-sensitiveinversereinforcementlearningviacoher- AccountingforHiddenContextinRLHF. arXivpreprint
entriskmodels. InAmato,N.,Srinivasa,S.,Ayanian,N., arXiv:2312.08358,2023.
andKuindersma,S.(eds.),Robotics,Robotics: Science
Skalse, J. and Abate, A. Misspecification in In-
andSystems, UnitedStates, 2017.MITPressJournals.
verse Reinforcement Learning. arXiv e-prints, art.
doi: 10.15607/rss.2017.xiii.069.
arXiv:2212.03201, December 2022. doi: 10.48550/a
rXiv.2212.03201.
Manyika, J. An overview of Bard: an early experiment
withgenerativeAI. https://ai.google/static
Skalse,J.M.V.,Farrugia-Roberts,M.,Russell,S.,Abate,
/documents/google-about-bard.pdf, 2023.
A., and Gleave, A. Invariance in Policy Optimisation
Accessed: 2023-09-05.
andPartialIdentifiabilityinRewardLearning. InKrause,
A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S.,
Mindermann,S.andArmstrong,S. Occam’sRazorisIn-
andScarlett, J.(eds.), Proceedingsofthe40thInterna-
sufficient to Infer the Preferences of Irrational Agents.
tionalConferenceonMachineLearning,volume202of
InProceedingsofthe32ndInternationalConferenceon
ProceedingsofMachineLearningResearch,pp.32033–
Neural Information Processing Systems, NIPS’18, pp.
32058.PMLR,23–29Jul2023. URLhttps://proc
5603–5614, Red Hook, NY, USA, 2018. Curran Asso-
eedings.mlr.press/v202/skalse23a.html.
ciatesInc.
Stray,J. TheAILearnstoLietoPleaseYou: Preventing
Ng,A.Y.,Russell,S.,etal. AlgorithmsforInverseRein- BiasedFeedbackLoopsinMachine-AssistedIntelligence
forcementLearning. InICML,volume1,pp. 2,2000. Analysis. Analytics, 2(2):350–358, 2023. ISSN 2813-
2203. doi: 10.3390/analytics2020020. URLhttps:
OpenAI. IntroducingChatGPT. https://openai.c //www.mdpi.com/2813-2203/2/2/20.
om/blog/chatgpt,2022. Accessed: 2024-02-06.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
Park, P. S., Goldstein, S., O’Gara, A., Chen, M., and A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
Hendrycks,D. Aideception:Asurveyofexamples,risks, Bhosale,S.,Bikel,D.,Blecher,L.,Ferrer,C.C.,Chen,
andpotentialsolutions. arXivpreprintarXiv:2308.14752, M.,Cucurull,G.,Esiobu,D.,Fernandes,J.,Fu,J.,Fu,W.,
2023. Fuller,B.,Gao,C.,Goswami,V.,Goyal,N.,Hartshorn,
12ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
A.,Hosseini,S.,Hou,R.,Inan,H.,Kardas,M.,Kerkez,
V.,Khabsa,M.,Kloumann,I.,Korenev,A.,Koura,P.S.,
Lachaux,M.-A.,Lavril,T.,Lee,J.,Liskovich,D.,Lu,Y.,
Mao,Y.,Martinet,X.,Mihaylov,T.,Mishra,P.,Molybog,
I.,Nie,Y.,Poulton,A.,Reizenstein,J.,Rungta,R.,Saladi,
K.,Schelten,A.,Silva,R.,Smith,E.M.,Subramanian,R.,
Tan,X.E.,Tang,B.,Taylor,R.,Williams,A.,Kuan,J.X.,
Xu,P.,Yan,Z.,Zarov,I.,Zhang,Y.,Fan,A.,Kambadur,
M.,Narang,S.,Rodriguez,A.,Stojnic,R.,Edunov,S.,
and Scialom, T. Llama 2: Open Foundation and Fine-
TunedChatModels. arxive-prints,2023.
Ward,F.R.,Belardinelli,F.,Toni,F.,andEveritt,T.Honesty
IstheBestPolicy: DefiningandMitigatingAIDeception.
arxive-prints,2023.
Zhuang,S.andHadfield-Menell,D. ConsequencesofMis-
aligned AI. In Proceedings of the 34th International
ConferenceonNeuralInformationProcessingSystems,
NIPS’20,RedHook,NY,USA,2020.CurranAssociates
Inc. ISBN9781713829546.
Ziebart,B.D.,Maas,A.L.,Bagnell,J.A.,andDey,A.K.
Maximum entropy inverse reinforcement learning. In
Fox,D.andGomes,C.P.(eds.),AAAI,pp.1433–1438.
AAAI Press, 2008. ISBN 978-1-57735-368-3. URL
http://dblp.uni-trier.de/db/conf/aaai
/aaai2008.html#ZiebartMBD08.
13ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
APPENDIX
Intheappendix,weprovidemoreextensivetheory,proofs,andexamples. Theappendixmakesfreeuseofconceptsand
notationdefinedinthemainpaper. Inparticular,throughoutweassumeageneralMDPtogetherwithobservationkernel
P :S →ΩandahumanwithgeneralbeliefkernelB(⃗o|⃗s),unlessotherwisestated. SeethelistofSymbolsinSectionA
O
torefreshnotation.
InSectionB,weprovideanextensivetheoryforappropriatelymodeledpartialobservabilityinRLHF.Thiscanmainlybe
consideredasupplementtoSection5andcontainsourmaintheorems,supplementaryresults,analysisofspecialcases,and
examples.
InSectionC,weanalyzethenaiveapplicationofRLHFunderpartialobservability,whichmeansthatthelearningsystemis
notawareofthehuman’spartialobservability. ThissectionisessentiallyasupplementtoSection4andcontainsananalysis
of the policy evaluation function J , of deceptive inflation and overjustification, and further extensive mathematical
obs
examplesshowingthefailuresofnaiveRLHFunderpartialobservability.
ContentsoftheAppendix
A ListofSymbols 14
B ModelingtheHumaninPartiallyObservableRLHF 16
B.1 TheBeliefovertheStateSequenceforRationalHumans . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.2 AmbiguityandIdentifiabilityofRewardandReturnFunctionsunderObservationSequenceComparisons 18
B.3 TheAmbiguityinRewardLearninginPractice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.4 IdentifiabilityofReturnFunctionsWhenHumanObservationsAreNotKnown . . . . . . . . . . . . . . 22
B.5 SimpleSpecialCases: FullObservability,DeterministicP ,andNoisyP . . . . . . . . . . . . . . . . 25
O⃗ O⃗
B.6 RobustnessofReturnFunctionIdentifiabilityunderBeliefMisspecification . . . . . . . . . . . . . . . . 26
B.6.1 SomeNormTheoryforLinearOperators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
B.6.2 ApplicationtoBoundsintheErroroftheReturnFunction . . . . . . . . . . . . . . . . . . . . . 29
B.7 PreliminaryCharacterizationsoftheAmbiguity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B.8 ExamplesSupplementingSection5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
C IssuesofNaivelyApplyingRLHFunderPartialObservability 35
C.1 OptimalPoliciesunderRLHFwithDeterministicPartialObservationsMaximizeJ . . . . . . . . . . . 36
obs
C.2 Interlude: WhentheHumanKnowsthePolicyandisaBayesianReasoner,thenJ =J . . . . . . . . . 37
obs
C.3 ProofofTheorem4.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
C.4 DerivationsandFurtherDetailsforSection4.3ExampleA . . . . . . . . . . . . . . . . . . . . . . . . . 39
C.5 FurtherExamplesSupplementingSection4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
A.ListofSymbols
GeneralMDPs
S Setofenvironmentstatess∈S
A Setofactionsa∈Aofthepolicy
∆(S) SetofprobabilitydistributionsoverS. Canbedefinedforanyfiniteset
T :S×A→∆(S) Transitionkernel
14ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
P ∈∆(S) Initialstatedistribution
0
R∈RS Usuallythetruerewardfunction
R′ ∈RS UsuallyarewardfunctioninthekernelofB◦Γ
R˜ ∈RS Usuallyanotherrewardfunction,e.g. inferredbyalearningsystem
γ ∈[0,1] Discountfactor
π :S →∆(A) Apolicy
Tπ :S →∆(S) TransitionkernelforafixedpolicyπgivenbyTπ(s′ |s)=(cid:80) T(s′ |
a∈A
s,a)·π(a|s)
T ∈N Finitetimehorizon
Pπ ∈∆(ST) Statesequencedistributioninducedbythepolicyπ
S⃗ ⊆ST Statesequences⃗s∈S⃗supportedbyPπ
G∈RS⃗ UsuallythetruereturnfunctiongivenbyG(⃗s)=(cid:80)T γtR(s ).
t=0 t
G′ ∈RS⃗ UsuallyareturnfunctioninkerB
G˜ ∈RS⃗ Usuallyanotherreturnfunction,e.g. inferredbyalearningsystem
(cid:2) (cid:3)
J ThetruepolicyevaluationfunctiongivenbyJ(π)=E G(⃗s) .
⃗s∼Pπ
AdditionstoGeneralMDPswithPartialObservability
Ω Setofpossibleobservationso∈Ω
P :S →∆(Ω) Observationkerneldeterminingthehuman’sobservations
O
P :S⃗ →∆(cid:0) ΩT(cid:1) TheobservationsequencekernelgivenbyP (cid:0) ⃗o|⃗s(cid:1) =(cid:81)T P (cid:0) o |s (cid:1)
O⃗ O⃗ t=0 O t t
Ω⃗ ⊆ΩT Thesetofobservedsequences⃗o∈ΩT thatcanbesampledfromP (·|⃗s)
O⃗
for⃗s∈S⃗
O :S →Ω ObservationfunctionforthecasethatP isdeterministic;givenbyO(s)=
O
owithosuchthatP (o|s)=1
O
O⃗ :S⃗ →Ω⃗ ObservationsequencefunctionforthecasethatP isdeterministic;given
O⃗
byO⃗(⃗s)=⃗owith⃗osuchthatP (⃗o|⃗s)=1
O⃗
G ∈R{⃗s∈S⃗|O⃗(⃗s)=⃗o} Restriction of the return function G ∈ RS⃗ to (cid:8) ⃗s ∈ S⃗ | O⃗(⃗s) = ⃗o(cid:9) for
⃗o
fixed⃗o∈Ω⃗
G ∈RS⃗ Return function that can be inferred when partial observability is not
obs
properlymodeled,givenbyG
(⃗s):=(cid:0) B·G(cid:1)(cid:0) O⃗(⃗s)(cid:1)
obs
J Observationpolicyevaluationfunction,definedinEq.(4)
obs
State-andObservationSequences
s ∈S Thet’thentryinastatesequence⃗s
t
⃗s∈ST Statesequence⃗s=s ,...,s
0 T
sˆ∈St Statesequencesegmentsˆ=s ,...,s fort≤T
0 t
o ∈Ω Thet’thentryinanobservationsequence⃗o
t
⃗o∈ΩT Observationsequence⃗o=o ,...,o
0 T
oˆ∈Ωt Observationsequencesegmentoˆ=o ,...,o fort≤T
0 t
TheHuman’sBelief
B(π′) Thehuman’spolicyprior
B(⃗s) The human’s prior belief that a sequence ⃗s will be sampled, given by
B(⃗s)=(cid:82) B(π′)Pπ′(⃗s)dπ′
(cid:0) (cid:1) π′
B ⃗s|⃗o Thehuman’sbeliefofastatesequencegivenanobservationsequence,see
PropositionB.1foraBayesianversion
15ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Bπ(⃗s|⃗o) Thehuman’sbeliefofastatesequencegivenanobservationsequence;it
isallowedtodependonthetruepolicyπ,seePropositionB.1
B ∈R{⃗s∈S⃗|O⃗(⃗s)=⃗o} VectorofpriorprobabilitiesB(⃗s)for⃗s∈(cid:8) ⃗s∈S⃗ |O⃗(⃗s)=⃗o(cid:9)
⃗o
IdentifiabilityTheorem
β >0 TheinversetemperatureparameteroftheBoltzmannrationalhuman
σ :R→(0,1) Thesigmoidfunctiongivenbyσ(x)= 1
1+exp(−x)
Γ:RS →RS⃗ FunctionthatmapsarewardfunctionRtothereturnfunctionΓ(R)with
(cid:2) Γ(R)(cid:3) (⃗s)=(cid:80)T γtR(s )
t=0 t
B:RS⃗ →RΩ⃗ Function that maps a return function G to the expected return function
(cid:2) (cid:3) (cid:2) (cid:3)
B(G)onobservationsequencesgivenby B(G) (⃗o)=E G(⃗s)
⃗s∼B(⃗s|⃗o)
F:RS →RΩ⃗ ThecompositionF=B◦Γ
PR(cid:0) ⃗s≻⃗s′(cid:1)
Boltzmann rational choice probability in the case of full observability
(Eq.(1))
PR(cid:0) ⃗o≻⃗o′(cid:1)
Boltzmannrationalchoiceprobabilityinthecaseofpartialobservability
(Eq.(2))
O:RΩ⃗ →RS⃗ Abstractlinearoperatorgivenby(cid:2) O(v)(cid:3) (⃗s)=E (cid:2) v(⃗o)(cid:3)
⃗o∼P O⃗(⃗o|⃗s)
O⊗O:RΩ⃗×Ω⃗ →RS⃗×S⃗ Formally the Kronecker product of O with itself, explicitly given by
(cid:2) (O⊗O)(C)(cid:3) (⃗s,⃗s′)=E (cid:2) C(⃗o,⃗o′)(cid:3)
⃗o,⃗o′∼P O⃗(·|⃗s,⃗s′)
RobustnesstoMisspecifications
∥x∥ Euclideannormofthevectorx∈Rk
∥A∥ MatrixnormofthematrixA,givenby∥A∥:=max ∥Ax∥
x,∥x∥=1
τ(A) MatrixquantitydefinedinEquation(9)
C(A,ρ) MatrixquantitydefinedinEquation(10)
r(B) RestrictionofBtoimΓ
GeneralSetsand(Linear)Functions
|A| NumberofelementsinthesetA
A∩C IntersectionofsetsAandC
A∪C UnionofsetsAandC
A\C RelativecomplementofC inA
δ TheDiracdeltadistributionofapointxinaset;givenbyδ (A) = 1if
x x
x∈Aandδ (A)=0,else
x
(cid:8)
kerA ThekernelofalinearoperatorA:V →W;givenbykerA= v ∈V |
(cid:9)
A(v)=0
(cid:8)
imA TheimageofalinearoperatorA:V →W;givenbyimA= w ∈W |
(cid:9)
∃v ∈V :A(v)=w
f−1(y) Preimageofy underafunctionf : X → Y; givenbyf−1(y) = (cid:8) x ∈
(cid:9)
X |f(x)=y
B.ModelingtheHumaninPartiallyObservableRLHF
Inthisappendix,wedevelopthetheoryofRLHFwithappropriatelymodeledpartialobservability,includingfullproofsof
alltheorems.
InSectionB.1,weexplainhowthehumancanarriveatthebeliefB⃗s|⃗oviaBayesianupdates. Themaintheoryandthe
mainpaperingeneraldonotdependonthisspecificformofthehuman’sbelief,butsomeexamplesintheappendixdo.
16ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
InSectionB.2wethenexplainourmainresult: theambiguityandidentifiabilityofbothrewardandreturnfunctionsunder
observedsequencecomparisons. InSectionB.3,wethenexplainthatthistheoremmeansthatonecouldinprincipledesign
apracticalrewardlearningalgorithmthatconvergesonthecorrectrewardfunctionuptotheambiguitycharacterizedinthe
sectionbefore,if thehuman’sbeliefkernelB(⃗s|⃗o)isfullyknown.
InSectionB.4,wegeneralizethetheorytothecasethatthehuman’sobservationsarenotnecessarilyknowntothelearning
system and again characterize precisely when the return function is identifiable from sequence comparisons. We then
consider special cases in Section B.5, where we show that the fully observable case is covered by our theory, that a
deterministicobservationkernelP usuallyleadstonon-injectivebeliefmatrixB,andthat“noise”intheobservationkernel
O⃗
P leads,underappropriateassumptions,totheidentifiabilityofthereturnfunction.
O⃗
Ouridentifiabilityresultsrequirethatthelearningsystemknowsthehuman’sbeliefkernelB(⃗s|⃗o). InSectionB.6,we
thenshowthattheseresultsarerobusttoslightmisspecifications: aboundintheerrorinthespecifiedbeliefleadstoa
correspondingboundintheerrorofthepolicyevaluationfunctionusedforsubsequentreinforcementlearning.
InSectionB.7,wethenprovideaverypreliminarycharacterizationoftheambiguityinthereturnfunctionunderspecial
cases.
Finally,inSectionB.8,westudyexamplesofidentifiabilityandnon-identifiabilityofthereturnfunctionforthecasethatwe
domodelthehuman’spartialobservabilitycorrectly. Thisrevealsqualitativelyinterestingcasesofidentifiability,evenwhen
Bisnotinjective,andcatastrophiccasesofnon-identifiability.
B.1.TheBeliefovertheStateSequenceforRationalHumans
Beforewediveintothemaintheory,wewanttoexplainhowthehumancaniterativelycomputetheposteriorofthestate
sequencegivenanobservationsequencewithsuccessivelynewobservations. ThisisdonebydefiningaBayesiannetwork
forthejointprobabilityofpolicy,states,actions,andobservations,anddoingBayesianinferenceoverthisBayesiannetwork.
Thedetailsofthissubsectionareonlyrelevantforafewsectionsintheappendixsinceitisusuallyenoughtoassumethat
theposteriorbeliefexists. Additionally,inthecoretheory,wedonotevenassumethatB(⃗s|⃗o)isaposterior: itissimply
anyprobabilitydistribution. Thereasonwhyitcanstillbeinterestingtoanalyzethecasewhenthehumanisarational
BayesianreasoneristhatonecanthenanalyzeRLHFundergenerousassumptionstothehuman.
We model the human to have a joint distribution B(π,⃗s,⃗a,⃗o) over the policy π, state sequence⃗s = s ,...,s , action
0 T
sequence⃗a = a ,...,a , and observation sequence⃗o = o ,...,o . This is given by a Bayesian network with the
0 T−1 0 T
followingcomponents:
• apolicypriorB(π′);
• theprobabilityoftheinitialstateB(s ):=P (s );
0 0 0
• actionprobabilitiesB(a|s,π):=π(a|s);
• transitionprobabilitiesB(s |s ,a ):=T(s |s ,a );
t+1 t t t+1 t t
• andobservationprobabilitiesB(o |s ):=P (o |s ).
t t O t t
Together,thisdefinesthejointdistributionB(π,⃗s,⃗a,⃗o)overthepolicy,states,actions,andobservationsthatfactorizes
accordingtothefollowingdirectedacyclicgraph:
17ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
π′
s a s a s a s ... (6)
0 0 1 1 2 2 3
o o o o
0 1 2 3
The following proposition clarifies the iterative Bayesian update of the human’s posterior over state sequences, given
observationsequences:
Proposition B.1. Let t ≤ T −1 and denote by sˆ = s ,...,s a state sequence segment of length t ≥ 0. Similarly,
0 t
oˆ=o ,...,o denotesanobservationsequencesegment. Wehave
0 t
(cid:34) (cid:35)
(cid:88)
B(sˆ,s ,π |oˆ,o )∝P (o |s )· T(s |sˆ,a )·π(a |s ) ·B(sˆ,π |oˆ).
t+1 t+1 O t+1 t+1 t+1 t t t t
at∈A
Thus,thehumancaniterativelycomputeB(sˆ,π |oˆ)fromthepriorB(s ,π)=P (s )·B(π′)usingtheaboveBayesian
0 0 0
update.
Theposterioroverthestatesequencecansubsequentlybecomputedby
(cid:90)
B(sˆ|oˆ)= B(sˆ,π |oˆ).
π
Proof. TheproofisessentiallyjustBayesruleappliedtotheBayesiannetworkinEquation(6). Werepeatedlymakeuseof
conditionalindependencesthatfollowfromd-separationsinthegraph(Geigeretal.,1990). Moreconcretely,wehave
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
B sˆ,s ,π |oˆ,o ∝B o |sˆ,s ,π,oˆ ·B sˆ,s ,π |oˆ
t+1 t+1 t+1 t+1 t+1
(cid:0) (cid:1) (cid:0) (cid:1)
=P o |s ·B s |sˆ,π,oˆ)·B(sˆ,π |oˆ
O t+1 t+1 t+1
(cid:34) (cid:35)
(cid:0) (cid:1) (cid:88) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
=P o |s · B s |a ,sˆ,π,oˆ ·B a |sˆ,π,oˆ ·B sˆ,π |oˆ
O t+1 t+1 t+1 t t
at∈A
(cid:34) (cid:35)
(cid:0) (cid:1) (cid:88) (cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
=P o |s · T s |s ,a ·π a |s ·B sˆ,π |oˆ .
O t+1 t+1 t+1 t t t t
at∈A
Instep1,weusedBayesrule. Instep2,wemadeuseoftheindependenceo ⊥⊥(sˆ,π,oˆ)|s ,pluggedintheobservation
t+1 t+1
kernel,andusedthechainruleofprobabilitytocomposethesecondtermintoaproduct. Instep3,wemarginalizedand
used,onceagain,thechainruleofprobability. Instep4,weusedtheindependencess ⊥⊥(s ,...,s ,π,oˆ)|(s ,a)
t+1 0 t−1 t
anda ⊥⊥(s ,...,s ,oˆ)|(π,s )andpluggedinthetransitionkernelandthepolicy.
t 0 t−1 t
Thelastformulaisjustamarginalizationoverthepolicy.
B.2.AmbiguityandIdentifiabilityofRewardandReturnFunctionsunderObservationSequenceComparisons
Inthissection,weprovethemaintheoremofthispaper: acharacterizationoftheambiguitythatisleftintherewardand
returnfunctiononcethehuman’sBoltzmann-rationalchoiceprobabilitiesareknown. Wechangetheformulationslightlyby
formulatingthelinearoperators“intrinsically”inthespacestheyaredefinedin,insteadofusingmatrixversions. Thisdoes
18ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Figure4. Thelineargeometryofambiguityforahypotheticalexamplewiththreestatesequencesandtwoobservationsequences.G∗is
thetruereturnfunction,and“G”isusedinlabelingtheaxestorefertosomearbitraryreturnfunction.Thisisamoreaccurategeometric
depictionofthemiddleandrightspacesinFigure3.ThesubspaceimΓ∩kerB(purple)istheambiguityinreturnfunctions,meaning
thataddinganelementwouldnotchangethehuman’sexpectedreturnfunctiononobservations. Thusthesetofreturnfunctionsthat
therewardlearningsystemcaninferistheaffinesetG+(imΓ∩kerB)(yellow). Notethattheplanesontheleftaredrawntobe
axis-alignedforeaseofvisualization;thiswillnotbethecaseforrealMDPs.
notchangethegeneralpicture,butisamorenaturalsettingwhenthinking,e.g.,aboutgeneralizingtheresultstoinfinite
statesequences.
Thus,wedefineB:RS⃗ →RΩ⃗
asthelinearoperatorgivenby
(cid:2) (cid:3) (cid:2) (cid:3)
B(G) (⃗o):= E G(⃗s) .
⃗s∼B(⃗s|⃗o)
Here,Bisthehuman’sbelief,whichcaneitherbecomputedasintheprevioussubsectionorsimplybeanyconditional
probabilitydistribution. Similarly,wedefineΓ:RS →RS⃗ asthelinearoperatorgivenby
T
(cid:2) Γ(R)(cid:3) (⃗s):=(cid:88) γtR(s ).
t
t=0
ThematrixproductB·ΓthenbecomesthecompositionB◦Γ:RS →RΩ⃗ . Finally,recallthatthekernelkerAofalinear
operatorAisdefinedasitsnullspace,andtheimageimAasthesetofelementshitbyA. Weobtainthefollowingtheorem:
TheoremB.2. LetRbethetruerewardfunctionandR˜ anotherrewardfunction. LetG˜ =Γ(R˜)andG=Γ(R)bethe
correspondingreturnfunctions. Thefollowingthreestatementsareequivalent:
(i) TherewardfunctionR˜givesrisetothesamevectorofchoiceprobabilitiesasR,i.e
(cid:16) PR˜(cid:0) ⃗o≻⃗o′(cid:1)(cid:17) =(cid:16) PR(cid:0) ⃗o≻⃗o′(cid:1)(cid:17)
.
⃗o,⃗o′∈Ω⃗ ⃗o,⃗o′∈Ω⃗
(ii) ThereisarewardfunctionR′ ∈ker(B◦Γ)andaconstantc∈Rsuchthat
R˜ =R+R′+c.
(iii) ThereisareturnfunctionG′ ∈kerB∩imΓandaconstantc′ ∈Rsuchthat
G˜ =G+G′+c′.
Inotherwords,theambiguitythatisleftintherewardfunctionwhenitsobservation-basedchoiceprobabilitiesareknown
is,uptoanadditiveconstant,givenbyker(B◦Γ);theambiguityleftinthereturnfunctionisgivenbykerB∩imΓ.
19ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Proof. Assume(i). Toprove(ii),letσ bythesigmoidfunctiongivenbyσ(x) = 1 . ThenbyEquation(2),the
1+exp(−x)
equalityofchoiceprobabilitiesmeansthefollowingforall⃗o,⃗o′ ∈Ω⃗:
σ(cid:16) β·(cid:0)(cid:2) B(G˜)(cid:3) (⃗o)−(cid:2) B(G˜)(cid:3) (⃗o′)(cid:1)(cid:17) =σ(cid:16) β·(cid:0)(cid:2) B(G)(cid:3) (⃗o)−(cid:2) B(G)(cid:3) (⃗o′)(cid:1)(cid:17)
.
Sincethesigmoidfunctionisinjective,thisimplies
(cid:2) B(G˜)(cid:3) (⃗o)−(cid:2) B(G˜)(cid:3) (⃗o′)=(cid:2) B(G)(cid:3) (⃗o)−(cid:2) B(G)(cid:3) (⃗o′).
Fixinganarbitrary⃗o′,thisimpliesthatthereexistsaconstantc′suchthatforall⃗o∈Ω⃗,thefollowingholds:
(cid:2) B(G˜)(cid:3) (⃗o)−(cid:2) B(G)(cid:3) (⃗o′)−c′ =0.
NotingthatB(c′)=c′,thisimpliesG˜−G−c′ ∈ker(B). Now,definetheconstantrewardfunction
1−γ
c:=c′· .
1−γT+1
Weobtain
T
(cid:2) Γ(c)(cid:3) (⃗s)=(cid:88) γt·c
t=0
T
1−γ (cid:88)
=c′· · γt
1−γT+1
t=0
=c′.
Thus,wehave
Γ(R˜−R−c)=G˜−G−c′ ∈ker(B),
implyingR′ :=R˜−R−c∈ker(B◦Γ). Thisshows(ii).
That(ii)implies(iii)followsbyapplyingΓtobothsidesoftheequation.
Nowassume(iii),i.e. G˜ = G+G′+c′ foraconstantc′ ∈ RandareturnfunctionG′ ∈ ker(B)∩imΓ. Thisimplies
B(G˜)=B(G)+c′. Thus,forall⃗o,⃗o′ ∈Ω⃗,wehave
(cid:2) B(G˜)(cid:3) (⃗o)−(cid:2) B(G˜)(cid:3) (⃗o′)=(cid:2) B(G)(cid:3) (⃗o)−(cid:2) B(G)(cid:3) (⃗o′),
whichimpliestheequalchoiceprobabilitiesaftermultiplyingwithβ andapplyingthesigmoidfunctionσonbothsides.
Thus,(iii)implies(i).
CorollaryB.3. Thefollowingtwostatementsareequivalent:
(i) ker(B◦Γ)=0.
(ii)
Thedata(cid:16) PR(cid:0) ⃗o≻⃗o′(cid:1)(cid:17)
determinetherewardfunctionRuptoanadditiveconstant.
⃗o,⃗o′∈Ω⃗
Proof. That(i)implies(ii)followsimmediatelyfromtheimplicationfrom(i)to(ii)withintheprecedingtheorem.
Nowassume(ii). LetR′ ∈ ker(B◦Γ). DefineR˜ := R+R′. Thentheimplicationfrom(ii)to(i)withinthepreceding
theoremimpliesthatR˜andRhavethesamechoiceprobabilities. Thus,theassumption(ii)inthiscorollaryimpliesthat
R′ isaconstant. SinceΓandBmapnonzeroconstantstononzeroconstants,thefactthatR′ ∈ ker(B◦Γ)impliesthat
R′ =0,showingthatker(B◦Γ)={0}.
Asmentionedinthemainpaper,thepreviousresultalreadyleadstothenon-identifiabilityofRwheneverΓisnotinjective,
correspondingtothepresenceofzero-initialpotentialshaping(Skalseetal.(2023),LemmaB.3). Thus,wenowstrengthen
thepreviousresultsothatitdealswiththeidentifiabilityofthereturnfunction,whichissufficientforthepurposeofpolicy
optimization:
20ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
CorollaryB.4. Considerthefollowingfourstatements(whichcaneachbetrueorfalse):
(i) kerB={0}.
(cid:0)
(ii) ker B◦Γ)={0}.
(iii) kerB∩imΓ={0}.
(iv) The data (cid:16) PR(cid:0) ⃗o ≻ ⃗o′(cid:1)(cid:17) determine the return function G = Γ(R) on sequences⃗s ∈ S⃗ up to a constant
⃗o,⃗o′∈Ω⃗
independentof⃗s.
Thenthefollowingimplications,andnootherimplications,aretrue:
(i)
(iii) (iv)
(ii)
Inparticular,allof(i),(ii),and(iii)aresufficientconditionsforidentifyingthereturnfunctionfromthechoiceprobabilities.
Proof. That (i) implies (iii) is trivial. That (ii) implies (iii) is a simple linear algebra fact: Assume (ii) and that G′ ∈
kerB∩imΓ. ThenG′ =Γ(R′)forsomeR′ ∈RS and
0=B(G′)=B(cid:0) Γ(R′)(cid:1) =(B◦Γ)(R′).
By(ii),thisimpliesR′ =0andthereforeG′ =Γ(R′)=0,showing(iii).
That(iii)implies(iv)immediatelyfollowsfromtheimplicationfrom(i)to(iii)inTheoremB.2.
Now,assume(iv). Toprove(iii),assumeG′ ∈kerB∩imΓ. Thentheimplicationfrom(iii)to(i)inTheoremB.2implies
thatG+G′inducesthesameobservation-basedchoiceprobabilitiesasG. Thus,(iv)impliesG+G′ =G+c′forsome
constantc′,whichimpliesG′ =c′. SinceG′ ∈kerB,thisimplies0=B(G′)=B(c′)=c′ andthusG′ =0. Thus,we
showedkerB∩imΓ={0}.
Wenowshowthatnootherimplicationholdsingeneral. ExampleB.32willshowthat(ii)doesnotimply(i). Wenowshow
that(i)doesalsonotimply(ii),fromwhichitwilllogicallyfollowthat(iii)doesneitherimply(i)nor(ii). Namely,consider
thefollowingsimpleMDPwithtimehorizonT =1:
a b (7)
InthisMDP,everystatesequencestartsina,deterministicallytransitionstob,andthenends. Thismeansthat⃗s=abisthe
onlysequence. Now,letR′ ∈R{a,b}betherewardfunctiongivenby
−1
R′(a)=1, R′(b)= .
γ
Weobtain
(cid:2) Γ(R′)(cid:3) (⃗s)=R′(a)+γR′(b)=1+γ· −1 =0.
γ
Thus,Γ(R′)=0,(B◦Γ)(R′)=0,and,therefore,ker(cid:0) B◦Γ(cid:1)
̸={0}. Thus,(ii)doesnothold. However,itispossible
tochooseB(⃗s |⃗o)suchthat(i)holds: e.g.,ifΩ = S andB(⃗s |⃗o) := δ (⃗s),thenkerB = {0}sincethisoperatoristhe
⃗o
identity.
21ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
B.3.TheAmbiguityinRewardLearninginPractice
Inthissection,wepointoutthatTheoremB.2isnotjustatheoreticaldiscussion: WhenBandtheinversetemperature
parameterβ areknown,thenitispossibletodesignarewardlearningalgorithmthatlearnsthetruerewardfunctionupto
theambiguityker(B◦Γ)intheinfinitedatalimit. Indoingso,weessentiallyusethelossfunctionproposedinChristiano
etal.(2017).
Namely,assumeDisadatadistributionofobservationsequences⃗o∈Ω⃗ suchthatallsequencesinΩ⃗ haveastrictlypositive
probabilityofbeingsampled;forexample,DcoulduseanexplorationpolicyandtheobservationsequencekernelP . For
O⃗
eachpairofobservationsequences(⃗o,⃗o′),wethengetaconditionaldistributionP(µ|⃗o,⃗o′)overaone-hotencodedhuman
choiceµ∈{(1,0),(0,1)},withprobability
P(cid:0) µ=(1,0)|⃗o,⃗o′(cid:1) =PR(cid:0) ⃗o≻⃗o′(cid:1)
.
Together,thisgivesrisetoadataset(⃗o ,⃗o′,µ ),...,(⃗o ,⃗o′ ,µ )ofobservationsequencesplusahumanchoice.
1 1 1 N N N
NowassumewelearnarewardfunctionR : S → Rthatisdifferentiableintheparameterθ andthatcanrepresentall
θ
possiblerewardfunctionsR∈RS. LetG :=Γ(R )bethecorrespondingreturnfunction. Writeµ =(µ(1),µ(2)). As
θ θ k k k
inChristianoetal.(2017),wedefineitslossoverthedatasetaboveby
N
L(cid:101)(θ)=− N1 (cid:88) µ( k1)·logPRθ(cid:0) ⃗o
k
≻⃗o′ k(cid:1) +µ( k2)·logPRθ(cid:0) ⃗o′
k
≻⃗o k(cid:1) .
k=1
NotethatbyEquation(2),thislossfunctionessentiallyusesBandalsotheinversetemperatureparameterβinitsdefinition.
Thismeansthattheseneedtobeexplicitlyrepresentedtobeabletousethelossfunctioninpractice.
PropositionB.5. ThelossfunctionL(cid:101)isdifferentiable. Furthermore,intheinfinitedatalimititsminimaarepreciselygiven
byparametersθ suchthatR = R+R′+cforR′ ∈ ker(cid:0) B◦Γ(cid:1) andc ∈ R,orequivalentlyG = G+G′+c′ for
θ θ
G′ ∈kerB∩imΓandc′ ∈R.
Proof. ThedifferentiabilityofthelossfunctionfollowsfromthedifferentiabilityofmultiplicationwiththematrixB,see
Equation(2),andoftherewardfunctionR initsparameterθthatweassumed.
θ
Forthesecondstatement,letN(⃗o,⃗o′)bethenumberoftimesthatthepair(⃗o,⃗o′)appearsinthedataset,andletN(⃗o,⃗o′,1)
bethenumberoftimesthatthehumanchoiceisµ=(1,0)andthesampledpairis(⃗o,⃗o′),andsimilarfor2insteadof1.
Weobtain
(cid:34)
L(cid:101)(θ)=−
(cid:88) N(⃗o,⃗o′)
·
N(⃗o,⃗o′,1) logPRθ(cid:0) ⃗o≻⃗o′(cid:1)
N N(⃗o,⃗o′)
⃗o,⃗o′∈Ω⃗
(cid:35)
+
N(⃗o,⃗o′,2)
logPRθ(cid:0) ⃗o′ ≻⃗o(cid:1)
N(⃗o,⃗o′)
≈ E
(cid:20) CE(cid:16) PR(cid:0) ⃗o≻≺⃗o′(cid:1)(cid:13) (cid:13)PRθ(cid:0) ⃗o≻≺⃗o′(cid:1)(cid:17)(cid:21)
⃗o,⃗o′∼D
=:L(θ).
Here,CEisthecrossentropybetweenthetwobinarydistributions. SinceweassumedthatDgivesapositiveprobabilityto
allobservationsequencesinΩ⃗,andsincethecrossentropyisgenerallyminimizedexactlywhentheseconddistribution
equalsthefirst,thelossfunctionL(θ)isminimizedifandonlyifR givesrisetothesamechoiceprobabilitiesasRforall
θ
pairsofobservationsequences. TheoremB.2thengivestheresult.
B.4.IdentifiabilityofReturnFunctionsWhenHumanObservationsAreNotKnown
CorollaryB.4assumesthatthechoiceprobabilitiesofeachobservationsequencepairareknowntotherewardlearning
algorithm. However,thisrequiresthealgorithmtoknowwhatthehumanobserved. Insomeapplications,thisisareasonable
assumption,e.g. ifthehuman’sobservationsarethemselvesproducedbyanalgorithmthatcanfeedtheobservationsalso
backtothelearningalgorithm. Ingeneral,however,theobservationshappeninthephysicalworld,andareonlyknown
22ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
probabilisticallyviatheobservationkernelP . Thelearningsystemdoeshoweverhaveaccesstothefullstatesequences
O
thatgeneratetheobservationsequences. Thisleadstoknowledgeofthefollowingchoiceprobabilitiesfor⃗s,⃗s′ ∈S⃗:
PR(cid:0) ⃗s≻⃗s′(cid:1) := E (cid:104) PR(cid:0) ⃗o≻⃗o′(cid:1)(cid:105) ,2 (8)
⃗o,⃗o′∼P O⃗(·|⃗s,⃗s′)
wheretheobservation-basedchoiceprobabilitiesaregivenasinEquation(2). Inotherwords,thelearningalgorithmcan
onlyinferanaggregateoftheobservation-basedchoiceprobabilities. Again,wecanaskaquestionsimilartotheones
before,extendingtheinvestigationsintheprevioussection:
(cid:16) (cid:17)
Question B.6. Assume the vector of choice probabilities PR(⃗s ≻ ⃗s′) is known. Additionally, assume that it
⃗s,⃗s′∈S⃗
is known that the human’s observations are governed by P , and that the human is Boltzmann rational with inverse
O
temperatureparameterβ andbeliefsB(⃗s|⃗o),seeEquation(8). DoesthisdataidentifythereturnfunctionG:S⃗ →R?
Iftheobservation-basedchoiceprobabilitiesfromEquation(2)wouldbeknown,thenCorollaryB.4wouldprovidethe
answertothisquestion. Thus,similartohowwepreviouslyinvertedthebeliefoperatorB,wearenowsimplytaskedwith
invertingtheexpectationoverobservationsequences. Thisleadsustothefollowingdefinition:
DefinitionB.7(UngroundingOperator). TheungroundingoperatorsO : RΩ⃗ → RS⃗ andO⊗O : RΩ⃗×Ω⃗ → RS⃗×S⃗ are
definedby
(cid:2) O(v)(cid:3) (⃗s):= E (cid:2) v(⃗o)(cid:3) , (cid:2) (O⊗O)(C)(cid:3) (⃗s,⃗s′):= E (cid:2) C(⃗o,⃗o′)(cid:3) .
⃗o∼P O⃗(⃗o|⃗s) ⃗o,⃗o′∼P O⃗(·|⃗s,⃗s′)
Here,v ∈RΩ⃗ isanarbitraryvector,andC ∈RΩ⃗×Ω⃗ isalsoanarbitraryvector,wherethenotationcanremindof“Choice”
sincetheinputstoO⊗Oare,inpractice,vectorsofobservation-basedBoltzmann-rationalchoiceprobabilities.
Formally,O⊗OistheKroneckerproductofOwithitself,butitisnotnecessarytounderstandthisfacttofollowthe
discussion. Ultimately, to be able to recover the observation-based choice probabilities, what matters is that O⊗O is
injectiveonwholevectorsofthesechoiceprobabilities. TheinjectivityofOisasufficientconditionforthis,whichexplains
itsusefulness. Weshowthisinthefollowinglemma:
LemmaB.8. O:RΩ⃗ →RS⃗ isinjectiveifandonlyifO⊗O:RΩ⃗×Ω⃗ →RS⃗×S⃗ isinjective.
Proof. ThisisageneralpropertyoftheKroneckerproductofalinearoperatorwithitself. Forcompleteness,wedemonstrate
thecalculationinourspecialcase. First,assumethatOisinjective. Assumethat(O⊗O)(C)=0forsomeC ∈RΩ⃗×Ω⃗ .
WeneedtoshowC =0.
Forallpairsofstatesequences(⃗s,⃗s′),wehave
0=(cid:2) (O⊗O)(C)(cid:3) (⃗s,⃗s′)= E (cid:2) C(⃗o,⃗o′)(cid:3)
⃗o,⃗o′∼P O⃗(·|⃗s,⃗s′)
(cid:20) (cid:21)
= E E
(cid:2) C(⃗o,⃗o′)(cid:3)
⃗o∼P O⃗(⃗o|⃗s) ⃗o′∼P O⃗(⃗o′|⃗s′)
(cid:104) (cid:105)
= E C′ (⃗o)
⃗s′
⃗o∼P O⃗(⃗o|⃗s)
=(cid:104) O(cid:0) C′ (cid:1)(cid:105) (⃗s),
⃗s′
whereC′ (⃗o):=E (cid:2) C(⃗o,⃗o′)(cid:3) . BytheinjectivityofO,weobtainC′ =0forall⃗s′. Thismeansthatforall⃗s′
⃗s′ ⃗o′∼P O⃗(⃗o′|⃗s′) ⃗s′
and⃗o,wehave
0=C′ (⃗o)= E (cid:2) C(⃗o,⃗o′)(cid:3) =(cid:104) O(cid:0) C′′(cid:1)(cid:105) (⃗s′),
⃗s′ ⃗o
⃗o′∼P O⃗(⃗o′|⃗s′)
whereC′′(⃗o′):=C(⃗o,⃗o′). Again,bytheinjectivityofO,weobtainC′′ =0forall⃗o,leadingtoC =0. Thatprovesthe
⃗o ⃗o
directionfromlefttoright.
2Weexcusethefollowingabuseofnotation:thesechoiceprobabilitiesrunthroughtheobservationsofthehumanandarenotthesame
asthechoiceprobabilitiesfromEquation(1).
23ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Toprovetheotherdirection,assumethatOisnotinjective. Thismeansthereexists0 ̸= C ∈ RΩ⃗ suchthatO(C) = 0.
DefineC⊗C ∈RΩ⃗×Ω⃗ by
(C⊗C)(⃗o,⃗o′):=C(⃗o)C(⃗o′).
Thenclearly,C⊗C ̸=0. Wearedoneifwecanshowthat(O⊗O)(C⊗C)=0sincethatestablishesthatO⊗Oisalso
notinjective. Forany⃗s,⃗s′ ∈S⃗,wehave
(cid:104) (cid:105) (cid:104) (cid:105)
(O⊗O)(C⊗C) (⃗s,⃗s′)= E (C⊗C)(⃗o,⃗o′)
⃗o,⃗o′∼P O⃗(·|⃗s,⃗s′)
(cid:104) (cid:105)
= E C(⃗o)·C(⃗o′)
⃗o,⃗o′∼P O⃗(·|⃗s,⃗s′)
= E
(cid:2) C(⃗o)(cid:3)
· E
(cid:2) C(⃗o′)(cid:3)
⃗o∼P O⃗(⃗o|⃗s) ⃗o′∼P O⃗(⃗o′|⃗s′)
=(cid:2) O(C)(cid:3) (⃗s)·(cid:2) O(C)(cid:3) (⃗s′)
=0·0
=0.
Thisfinishestheproof.
WenowstateandprovethefollowingextensionofCorollaryB.4:
TheoremB.9. Considerthefollowingstatements(whichcaneachbetrueorfalse):
1. O:RΩ⃗ →RS⃗ isaninjectivelinearoperator: kerO={0}.
2. O⊗O:RΩ⃗×Ω⃗ →RS⃗×S⃗ isaninjectivelinearoperator: kerO⊗O={0}.
3. O⊗O is injective on vectors of observation-based choice probabilities
(cid:16) PR(cid:0)
⃗o ≻
⃗o′(cid:1)(cid:17)
over the set of return
⃗o,⃗o′
functionsG∈RS⃗.
4.
Thedataofstate-basedchoiceprobabilities(cid:16) PR(cid:0) ⃗s≻⃗s′(cid:1)(cid:17)
fromEquation(8)determinethedataofobservation-
⃗s,⃗s′∈S⃗
basedchoiceprobabilities(cid:16) PR(cid:0) ⃗o≻⃗o′(cid:1)(cid:17)
fromEquation(2).
⃗o,⃗o′∈Ω⃗
Thenthefollowingimplicationsholdand3doesnotimply2:
1 2 3 4.
Consequently,ifanyoftheconditions1,2,or3hold,andadditionallyanyoftheconditions(i),(ii)or(iii)fromCorollaryB.4,
thenthedata(cid:16) PR(cid:0) ⃗s≻⃗s′(cid:1)(cid:17) determinethereturnfunctionGonsequences⃗s∈S⃗uptoaconstantindependentof⃗s.
⃗s,⃗s′∈Ω⃗
Proof. That1and2areequivalentwasshowninLemmaB.8. That2implies3isclear. Toprovethat3implies4,simply
putbothsetsofchoiceprobabilitiesintoavector. ThenEquation(8)andDefinitionB.7showthefollowingequalityof
vectorsinRS⃗×S⃗
:
(cid:16) PR(cid:0) ⃗s≻⃗s′(cid:1)(cid:17) =(cid:0) O⊗O(cid:1)(cid:18)(cid:16) PR(cid:0) ⃗o≻⃗o′(cid:1)(cid:17) (cid:19)
.
⃗s,⃗s′ ⃗o,⃗o′
TheinjectivityofO⊗Oonsuchinputsensuresthattheobservation-basedchoiceprobabilitiescanberecoveredusingthis
equation.
Wenowshowthat(3)doesnotimply(2). Again,weusethesimpleMDPfromEquation(7),butthistimewithadifferent
observationkernel. Namely,wechoose
P (o(a) |a)=P (o(a)′ |a)= 1 , P (o(b) |b)=1,
O O 2 O
24ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
where o(a)′ ̸= o(a) and o(a) ̸= o(b) ̸= o(a)′ . This results in two possible observation sequences: o(a)o(b) and o(a)′ o(b).
Thus,RΩ⃗ istwo-dimensional,whereasRS⃗
isonlyone-dimensional.
Consequently,O:RΩ⃗ →RS⃗
cannotbeinjective,so
kerO̸={0},so(2)doesnotholdsince(1)and(2)areequivalent. However,(3)stillholds: Sincethereisonlyonestate
sequence,Equation(2)showsthattheonlyvectorofchoiceprobabilitieshas1/2inallitsentries,irrespectiveofthereturn
functionG. Thus,O⊗Ohasonlyoneinputofobservation-basedchoiceprobabilities,andisthusautomaticallyinjective
onitsinputs.
ThefinalresultofidentifiabilityofthereturnfunctionGfollowsusingCorollaryB.4.
B.5.SimpleSpecialCases: FullObservability,DeterministicP ,andNoisyP
O⃗ O⃗
Inthissection,weanalyzethreesimplespecialcasesofthegeneraltheory.
Theorem3.9(togetherwithLemmaB.3)fromSkalseetal.(2023),reproducedasacorollarybelow,isaspecialcaseofour
theorem:
CorollaryB.10(Skalseetal.(2023)). Assumethehumandirectlyobservesthetruesequences,andthechoiceprobabilities
aregivenby
PR(cid:0) ⃗s≻⃗s′(cid:1) =σ(cid:16) β(cid:0) G(⃗s)−G(⃗s′)(cid:1)(cid:17)
.
ThisdatadeterminesthereturnfunctionG=Γ(R)onstatesequences⃗s∈S⃗uptoaconstantindependenton⃗s.
Proof. WecanembedthiscaseintotheoneofTheoremB.9bydefiningtheobservationkernelasP (⃗s′ |⃗s)=δ (⃗s′)(i.e.,
O⃗ ⃗s
thecorrectsequenceisdeterministicallyobserved)anddefiningthehuman’sbeliefasB(⃗s′ |⃗s)=δ (⃗s′)(i.e.,thehuman
⃗s
knowsthattheobservationreflectsthetruesequence). ThisshowsthatP(⃗s≻⃗s′)isoftheformofEquation(8). Theresult
followsfromTheoremB.9: theoperatorsOandBaretheidentityinthiscase,duetothedefiningpropertyoftheKronecker
delta,andsotheyareinjective.
ThefollowingpropositionshowsthatCorollaryB.10isessentiallytheonlyexampleofdeterministicobservationkernelP
O⃗
forwhichBisinjective. Note,however,thatinsomesituations,wecanhaveimΓ∩kerB={0}evenifBisnotinjective,
seeExampleB.32.
PropositionB.11. AssumeP ,theobservationkernelonthelevelofsequences,isdeterministicandnotinjective. ThenO
O⃗
isautomaticallyinjective. However,Bisnotinjective.
Proof. ToshowthatOisinjective,assumev ∈RΩ⃗ issuchthatO(v)=0. Thenforall⃗s∈S⃗,weget
0=(cid:2) O(v)(cid:3)
(⃗s)= E
(cid:2) v(⃗o)(cid:3) =v(cid:0) O⃗(⃗s)(cid:1)
.
⃗o∼P O⃗(⃗o|⃗s)
SinceO⃗ :S⃗ →Ω⃗ isbydefinitionsurjective,weobtainv =0.
O⃗ :S⃗ →Ω⃗ isbydefinitionsurjective,andhereassumedtobenon-injective,whichimpliesthatS⃗hasahighercardinality
thanΩ⃗. Thus,B:RS⃗ →RΩ⃗ cannotbeinjective.
Inthefollowing,weanalyzeasimplecasethatguaranteesidentifiability. Itrequiresthattheobservationkernelis“well-
behaved”ofaformwheretheobservationsaresimply“noisystates”,andthatthehumanisaBayesianreasonerwithany
priorB(⃗s)thatsupportseverystatesequence⃗s∈S⃗.
DefinitionB.12(NoiseintheObservationKernel). ThenwesaythatthereisnoiseintheobservationkernelP :S⃗ →∆(Ω⃗)
O
ifS⃗ =Ω⃗ andifOisaninjectivelinearoperator.
Proposition B.13. Assume that S⃗ = Ω⃗. Furthermore, assume that B(⃗s | ⃗o) is given by the posterior with likelihood
P (⃗o|⃗s)andanypriorB(⃗s)withB(⃗s)>0forall⃗s∈S⃗. ThenthereisnoiseintheobservationkernelifandonlyifBis
O⃗
injective.
25ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Proof. AssumeOisinjective. ToshowthatBisinjective,assumethereisG′ ∈RS⃗ withB(G′)=0. Thenforall⃗o∈Ω⃗,
wehave
0=(cid:2) B(G′)(cid:3)
(⃗o)= E
(cid:2) G′(⃗s)(cid:3) =(cid:88) B(⃗s|⃗o)G′(⃗s)∝(cid:88)
P
(⃗o|⃗s)·(cid:0) B(⃗s)·G′(⃗s)(cid:1)
O⃗
⃗s∼B(⃗s|⃗o)
⃗s ⃗s
=(cid:2) OT(B⊙G′)(cid:3)
(⃗o).
Here,OT isthetransposeofOandB⊙G′isthecomponentwiseproductofthepriorBwiththereturnfunctionG′. Since
Oisinjectiveandthusinvertible,OT isaswell. Thus,B⊙G′ =0,whichimpliesG′ =0sincethepriorgivespositive
probabilitytoallstatesequences. Thus,Bisinjective.
Fortheotherdirection,assumeBisinjective. ToshowthatOisinjective,letv ∈RΩ⃗ beanyvectorwithO(v)=0. Wedo
asimilarcomputationasabove:
forall⃗s∈RS⃗
,wehave
(cid:2) (cid:3) (cid:2) (cid:3) (cid:88) (cid:88) (cid:0) (cid:1)
0= O(v) (⃗s)= E v(⃗o) = P (⃗o|⃗s)v(⃗o)∝ B(⃗s|⃗o)· P (⃗o)·v(⃗o)
O⃗ O⃗
⃗o∼P O⃗(⃗o|⃗s)
⃗o ⃗o
=(cid:104) BT (cid:0) P ⊙v(cid:1)(cid:105) (⃗s).
O⃗
Here, BT is the transpose of B, P (⃗o) is the denominator in Bayes rule, and P ⊙v is the vector with components
O⃗ O⃗
P (⃗o)·v(⃗o). FromtheinjectivityandthusinvertibilityofB,itfollowsthatBT isinvertibleaswell,andsoP ⊙v =0,
O⃗ O⃗
whichimpliesv =0. Thus,Oisinjective.
CorollaryB.14. WhenthereisnoiseintheobservationkernelandthehumanisaBayesianreasonerwithsomepriorB
suchthatB(⃗s)>0forall⃗s∈S⃗,thenthereturnfunctionisidentifiablefromchoiceprobabilitiesofstatesequencesevenif
thelearningsystemdoesnotknowthehuman’sobservations.
Proof. ThisfollowsfromtheinjectivityofO,theinjectivityofBthatweprovedinPropositionB.13,andTheoremB.9.
RemarkB.15. Wementionthefollowingcaveat: intuitively,onecouldthinkthatO(andthusB,byPropositionB.13)will
beinjectiveifevery⃗sisidentifiablefrominfinitelymanyi.i.d. samplesfromP (⃗o|⃗s). Acounterexampleisthefollowing:
O⃗
 
1/2 1/4 1/4
O=1/4 1/2 1/4.
3/8 3/8 1/4
Inthiscase,therowsarelinearlydependentwithcoefficients1/2,1/2and−1. Consequently,OandBarenotinjective,
andsoifthisobservationkernelcomesfromamulti-armedbanditwiththreestates,thenCorollaryB.4showsthatthereturn
functionisnotidentifiable.
Nevertheless,thedistributionsP (·|⃗s)(givenbytherows)alldifferfromeachother,andsoinfinitelymanyi.i.d. samples
O⃗
identifythestatesequence⃗s.
B.6.RobustnessofReturnFunctionIdentifiabilityunderBeliefMisspecification
Wenowagainlookatthecasewheretheobservationsthatthehumanobservesareknowntotherewardlearningsystem,as
inSectionB.2. Furthermore,weassumethatB:RS⃗ →RΩ⃗ issuchthatkerB∩imΓ={0}. Inthiscase,wecanapply
CorollaryB.4andidentifythetruereturnfunctionGfromB(G),which,inturn,canbeidentifieduptoanadditiveconstant
fromtheobservation-basedchoiceprobabilitieswiththeargumentasforProposition3.1.
Inthissection,weinvestigatewhathappenswhenthehumanbeliefmodelisslightlymisspecified. Inotherwords: the
learningsystemusesaperturbedmatrixB :=B+∆withsomesmallperturbation∆. Howmuchwilltheinferredreturn
∆
functiondeviatefromthetruth? Toanswerthis,wefirstneedtooutlinesomenormtheoryoflinearoperators.
B.6.1.SOMENORMTHEORYFORLINEAROPERATORS
Inthissection,letV,W betwofinite-dimensionalinnerproduct-spaces. Inotherwords,V andW eachhaveinnerproducts
⟨·,·⟩andtherearelinearisomorphismsV ∼ = Rk,W ∼ = Rm suchthattheinnerproductsinV andW correspondtothe
26ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
standardscalarproductsinRk andRm. Thereasonthatwedon’tdirectlyworkwithRk andRm itselfisthatwewill
laterapplytheanalysistothecasethatV =imΓ⊆RS⃗ . LetinthiswholesectionA:V →W bealinearoperatorand
∆:V →W beaperturbance,sothatA :=A+∆isaperturbedversionofA.
∆
TheinnerproductsgiverisetoanormonV andW definedby
(cid:112) (cid:112)
∥v∥= ⟨v,v⟩, ∥w∥= ⟨w,w⟩.
Asiswellknown,foreachlinearoperatorA:V →W thereexistsaunique,basis-independentadjoint(generalizingthe
notionofatranspose)AT :W →V suchthatforallv ∈V andw ∈W,wehave
(cid:68) (cid:69)
⟨Av,w⟩= v,AT w .
Letusrecallthefollowingfactthatisoftenusedinlinearregression:
LemmaB.16. AssumeA:V →W isinjective. ThenAT A:V →V isinvertibleand(AT A)−1AT isaleftinverseof
A.
Proof. ToshowthatAT Aisinvertible,weonlyneedtoshowthatitisinjective. Thus,let0̸=x∈V. Then
(cid:68) (cid:69)
x,AT Ax =⟨Ax,Ax⟩=∥Ax∥2 >0,
wherethelaststepfollowedfromtheinjectivityofA. Thus,AT Ax̸=0,andsoAT Aisinjective,andthusinvertible.
Consequently,(AT A)−1AT isawell-definedoperator. ThatitistheleftinverseofAisclear.
DefinitionB.17(OperatorNorm). ThenormofanoperatorA:V →W isgivenby
∥A∥:= max ∥Ax∥.
x,∥x∥=1
Ithasthefollowingwell-knownproperties,whereA,BandCarematricesofcompatiblesizes:
∥A+B∥≤∥A∥+∥B∥, ∥CA∥≤∥C∥·∥A∥, ∥AT ∥=∥A∥.
TostudyhowaperturbanceinA(andthusAT A)transfersintoaperturbanceof(cid:0) AT A(cid:1)−1 ,wewillusethefollowing
theorem:
TheoremB.18(ElGhaoui(2002)). LetB:V →V beaninvertibleoperator. Letρ<∥B−1∥−1. Let∆:V →V beany
operatorwith∥∆∥≤ρ. ThenB+∆isinvertibleandwehave
(cid:13) (cid:13)(B+∆)−1−B−1(cid:13)
(cid:13)≤
ρ·∥B−1∥
.
∥B−1∥−1−ρ
Proof. SeeElGhaoui(2002),Section7andinparticularEquation7.2. Notethatthereferencedefines∥A∥tobethelargest
singularvalueofA;bythewell-knownmin-maxtheorem,thisisequivalenttoDefinitionB.17.
We will apply this theorem to AT A, which raises the question about the size of the perturbance in AT A for a given
perturbanceinA. Thisisclarifiedinthefollowinglemma. Beforestatingit,foragivenperturbanceρ,define
(cid:0) (cid:1)
ρ(A):=ρ· 2·∥A∥+ρ ,
(cid:101)
whichdependsonAandρ. Also,recallthatforagivenperturbance∆,wedefineA :=A+∆. Weobtain:
∆
LemmaB.19. Assumethat∥∆∥≤ρ. Then
∥AT A −AT A∥≤ρ(A).
∆ ∆ (cid:101)
27ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Proof. Wehave
(cid:13) (cid:13)AT ∆A ∆−AT A(cid:13) (cid:13)=(cid:13) (cid:13)(A+∆)T(A+∆)−AT A(cid:13) (cid:13)
=(cid:13) (cid:13)AT ∆+∆T A+∆T ∆(cid:13)
(cid:13)
≤∥A∥·∥∆∥+∥∆∥·∥A∥+∥∆∥2
(cid:16) (cid:17)
≤ρ· 2·∥A∥+ρ
=ρ(A).
(cid:101)
TobeabletoapplyTheoremB.18toAT A,weneedtomakesurethatρ (cid:101)(A)isboundedaboveby(cid:13) (cid:13)(AT A(cid:1)−1 ∥−1. The
nextlemmaclarifieswhatconditionρneedstosatisfyforρ(A)toobeythatbound. Forthis,define
(cid:101)
(cid:113)
τ(A):=−∥A∥+ ∥A∥2+(cid:13) (cid:13)(AT A)−1(cid:13) (cid:13)−1 , (9)
whichonlydependsonA.
LemmaB.20. Assumeρ<τ(A). Then
ρ (cid:101)(A)<(cid:13) (cid:13)(AT A)−1(cid:13) (cid:13)−1 .
Proof. Notethatρ=τ(A)isthepositivesolutiontothefollowingquadraticequationintheindeterminateρ:
ρ2+2·∥A∥·ρ−(cid:13) (cid:13)(AT A)−1(cid:13) (cid:13)−1 =ρ (cid:101)(A)−(cid:13) (cid:13)(AT A)−1(cid:13) (cid:13)−1 =0.
Sincethisisaconvexparabola, wegettheinequalityρ (cid:101)(A)−(cid:13) (cid:13)(AT A)−1(cid:13) (cid:13)−1 < 0wheneverwehave0 ≤ ρ < τ(A),
whichshowstheresult.
Finally,weputitalltogethertoobtainaboundontheperturbanceof(cid:0) AT A(cid:1)−1 AT. Forthis,set
(cid:13) (cid:13)
C(A,ρ):=
(cid:13)
(cid:13)(cid:0)ρ (cid:101) A(A T) A·(cid:13) (cid:13) (cid:1)(cid:0) −A
1(cid:13)
(cid:13)T −A
1
−(cid:1)− ρ1 ((cid:13) (cid:13)
A)
·(cid:16)(cid:13) (cid:13)A(cid:13) (cid:13)+ρ(cid:17) +(cid:13) (cid:13) (cid:13)(cid:0) AT A(cid:1)−1(cid:13) (cid:13) (cid:13)·ρ. (10)
(cid:13) (cid:13) (cid:101)
Weobtain:
PropositionB.21. Assume∥∆∥≤ρ<τ(A). ThenAT A isinvertible,andwehave
∆ ∆
(cid:13) (cid:13)
(cid:13)(cid:0) AT A (cid:1)−1 AT −(cid:0) AT A(cid:1)−1 AT (cid:13)≤C(A,ρ).
(cid:13) ∆ ∆ ∆ (cid:13)
Proof. TheinvertibilityofAT A followsfromTheoremB.18,LemmaB.19andLemmaB.20. Weget
∆ ∆
(cid:13) (cid:13)
(cid:13)(cid:0) AT A (cid:1)−1 AT −(cid:0) AT A(cid:1)−1 AT (cid:13)
(cid:13) ∆ ∆ ∆ (cid:13)
(cid:13) (cid:13)
=(cid:13) (cid:13) (cid:13)(cid:104)(cid:0) AT ∆A ∆(cid:1)−1 −(cid:0) AT A(cid:1)−1(cid:105) ·AT ∆+(cid:0) AT A(cid:1)−1 ·(cid:0) AT ∆−AT (cid:1)(cid:13) (cid:13)
(cid:13)
≤(cid:13) (cid:13) (cid:13)(cid:0) AT ∆A ∆(cid:1)−1 −(cid:0) AT A(cid:1)−1(cid:13) (cid:13) (cid:13)·(cid:13) (cid:13)A ∆(cid:13) (cid:13)+(cid:13) (cid:13) (cid:13)(cid:0) AT A(cid:1)−1(cid:13) (cid:13) (cid:13)·∥∆∥
(cid:13) (cid:13)
≤
(cid:13)
(cid:13)(cid:0)ρ (cid:101) A(A T) A·(cid:13) (cid:13) (cid:1)(cid:0) −A
1(cid:13)
(cid:13)T −A
1
−(cid:1)− ρ1 ((cid:13) (cid:13)
A)
·(cid:16)(cid:13) (cid:13)A(cid:13) (cid:13)+ρ(cid:17) +(cid:13) (cid:13) (cid:13)(cid:0) AT A(cid:1)−1(cid:13) (cid:13) (cid:13)·ρ
(cid:13) (cid:13) (cid:101)
=C(A,ρ).
Inthesecond-to-laststep,weusedTheoremB.18.
28ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
TheconstantC(A,ρ),definedinEquation(10),hasafairlycomplicatedform. Inthefollowingproposition,wefindan
easier-to-studyupperboundinaspecialcase:
(cid:113)
PropositionB.22. Assumethatρ≤∥A∥andρ≤−∥A∥+ ∥A∥2+1/2·(cid:13) (cid:13)(AT A)−1(cid:13) (cid:13)−1 .3 Thenwehave
C(A,ρ)≤ρ·(cid:13) (cid:13)(AT A)−1(cid:13) (cid:13)·(cid:104) 12·∥A∥2·(cid:13) (cid:13)(AT A)−1(cid:13) (cid:13)+1(cid:105) .
Proof. Thesecondassumptiongives,asintheproofofLemmaB.20,thatρ (cid:101)(A)≤1/2·(cid:13) (cid:13)(AT A)−1(cid:13) (cid:13)−1 . Togetherwith
ρ≤∥A∥,theresultfollows.
B.6.2.APPLICATIONTOBOUNDSINTHEERROROFTHERETURNFUNCTION
Wenowapplytheresultsfromtheprecedingsectiontoourcase.
Definer(B):imΓ→RΩ⃗
astherestrictionofthebelief
operator B to imΓ. Assume that kerB∩imΓ = {0}, which is, according to Corollary B.4, a sufficient condition for
identifiability. Notethatthisconditionmeansthatr(B)isinjective. Thus,LemmaB.16ensuresthatr(B)Tr(B)isinvertible
andthat(cid:0) r(B)Tr(B)(cid:1)−1 r(B)T isaleftinverseofr(B).
Consequently,fromtheequation
r(B)(G)=B(G)
weobtain
G=(cid:0) r(B)Tr(B)(cid:1)−1 r(B)T(B(G)).
ThisistheconcreteformulawithwhichGcanbeidentifiedfromB(G). WhenperturbingB,thisleadstoacorresponding
perturbancein(cid:0) r(B)Tr(B)(cid:1)−1 r(B)T whosesizeinfluencesthemaximalerrorintheinferenceofG.This,inturn,influences
thesizeoftheerrorinJ ,thepolicyevaluationfunction,where
G
(cid:2) (cid:3)
J (π):= E G(⃗s) .
G
⃗s∼Pπ(⃗s)
Weobtain:
TheoremB.23. LetGbethetruerewardfunction,Bthebeliefoperatorcorrespondingtothehuman’struebeliefmodel
B(⃗s | ⃗o), and B(G) be the resulting observation-based return function. Assume that kerB∩imΓ = {0}, so that
r(B)Tr(B)isinvertible. Let∆ : RS⃗ → RΩ⃗ beaperturbationsatisfying∥∆∥ ≤ ρ,whereρsatisfiesthefollowingtwo
properties:
(cid:113)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2 (cid:13)(cid:0) (cid:1)−1(cid:13)−1
ρ≤(cid:13)r(B)(cid:13), ρ≤−(cid:13)r(B)(cid:13)+ (cid:13)r(B)(cid:13) +1/2·(cid:13) r(B)Tr(B) (cid:13) .
Let B := B+∆ be the misspecified belief operator. The first claim is that r(B )Tr(B ) is invertible under these
∆ ∆ ∆
conditions.
Now,assumethatthelearningsysteminfersthereturnfunctionG˜ :=(cid:0) r(B )Tr(B )(cid:1)−1 r(B )T(B(G)).4 Thenthereis
∆ ∆ ∆
apolynomialQ(X,Y)ofdegreefivesuchthat
∥G˜−G∥≤∥G∥·Q(cid:16)(cid:13) (cid:13)(r(B)Tr(B))−1(cid:13) (cid:13),∥r(B)∥(cid:17)
·ρ.
Thus,forallpoliciesπ,weobtain
(cid:12) (cid:12) (cid:12)J G˜(π)−J G(π)(cid:12) (cid:12) (cid:12)≤∥G∥·Q(cid:16)(cid:13) (cid:13)(r(B)Tr(B))−1(cid:13) (cid:13),∥r(B)∥(cid:17) ·ρ.
Inparticular,forsufficientlysmallperturbancesρ,theerrorintheinferredpolicyevaluationfunctionJ becomesarbitrarily
G˜
small.
3Notethefactor1/2comparedtothedefinitionofτ(A)inEquation(9).
4Note that there is not necessarily a G˜ with r(B )(G˜) = B(G) since r(B ) is not always surjective. Nevertheless, G˜ :=
∆ ∆
(cid:0) r(B )Tr(B )(cid:1)−1r(B )T(B(G))isthebestattemptatasolutioninthesensethatr(B )(G˜)thenminimizestheEuclideandistance
∆ ∆ ∆ ∆
toB(G).
29ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Proof. Thatr(B )Tr(B )isinvertiblefollowsimmediatelyfromPropositionB.21byusingthat∥r(∆)∥≤∥∆∥and
∆ ∆
thatr(B )=r(B) ,togetherwiththesecondboundonρ(whichimpliestheassumedboundinPropositionB.21).
∆ r(∆)
Wehave
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)J (π)−J (π)(cid:12)=(cid:12) E (cid:2) (G˜−G)(⃗s)(cid:3)(cid:12)
(cid:12) G˜ G (cid:12) (cid:12) (cid:12)
⃗s∼Pπ(⃗s)
≤ E
(cid:104)(cid:12) (cid:12)(G˜−G)(⃗s)(cid:12) (cid:12)(cid:105)
⃗s∼Pπ(⃗s)
≤max(cid:12) (cid:12)(G˜−G)(⃗s)(cid:12)
(cid:12)
⃗s∈S⃗
≤∥G˜−G∥
(cid:13) (cid:13)
=(cid:13) (cid:13)(cid:104)(cid:0) r(B ∆)Tr(B ∆)(cid:1)−1 r(B ∆)T −(cid:0) r(B)Tr(B)(cid:1)−1 r(B)T(cid:105) ·B(G)(cid:13) (cid:13)
(cid:13) (cid:13)
≤(cid:13) (cid:13)(cid:0) r(B ∆)Tr(B ∆)(cid:1)−1 r(B ∆)T −(cid:0) r(B)Tr(B)(cid:1)−1 r(B)T(cid:13) (cid:13)·(cid:13) (cid:13)B(G)(cid:13) (cid:13)
≤C(r(B),ρ)·∥r(B)(G)∥
≤C(r(B),ρ)·∥r(B)∥·∥G∥.
Inthesecondtolaststep,weusedPropositionB.21. ByPropositionB.22,wecandefinethepolynomialQ(X,Y)by
(cid:104) (cid:105)
Q(X,Y)=XY · 12XY2+1 ,
whichisofdegreefive.
Thelastclaimfollowsfromlim ρ=0.
ρ→0
RemarkB.24. InthecaseofasquarematrixBthatisinjective,wecanapplyTheoremB.18directlytoB−1(whichisnow
invertible)andobtainthefollowingsimplificationofTheoremB.23forthecasethat∥∆∥≤ρ≤ 1 ·∥B−1∥−1:
2
(cid:12) (cid:12)J G˜(π)−J G(π)(cid:12) (cid:12)≤ρ·2·∥B∥·∥G∥·∥B−1∥2.
Thepolynomialisthenonlyofdegree3.
B.7.PreliminaryCharacterizationsoftheAmbiguity
Recallthesequenceoffunctions
RS Γ RS⃗ B RΩ⃗.
Inthissection,weclarifyimΓandkerBinspecialcases,astheirintersectionisthecrucialambiguityinTheoremB.2.
ThefollowingpropositionshowsthatfordeterministicP andarationalhuman,kerBdecomposesintohyperplanesdefined
O⃗
bynormalvectorsofprobabilitiesofsequencesmappingtothesameobservationsequence:
PropositionB.25. AssumethehumanreasonsasinSectionB.1. AssumeP isdeterministic. LetB(⃗s)bethedistribution
O⃗
ofsequencesunderthehuman’sbeliefoverthepolicy,givenbyB(⃗s)=(cid:82) B(π′)Pπ′(⃗s)forsomepolicypriorB(π′). For
π′
each⃗o,letB :=[B(⃗s)] ∈R{⃗s∈S⃗|O⃗(⃗s)=⃗o}bethevectorofprobabilitiesofsequencesthatareobservedas⃗o.
⃗o ⃗s:O⃗(⃗s)=⃗o
Let G′ be a return function. For each ⃗o ∈ Ω⃗, define the restriction G′ ∈ R{⃗s∈S⃗|O⃗(⃗s)=⃗o} by G′(⃗s) := G′(⃗s) for all
⃗o ⃗o
⃗s∈{⃗s∈S⃗ |O⃗(⃗s)=⃗o}. AssumethatB(⃗s|⃗o)istheBayesianposterior. ThenG′ ∈kerBifandonlyiftheproperty
B ·G′ =0
⃗o ⃗o
holdsforall⃗o∈Ω⃗.
Proof. ForadeterministicobservationkernelP ,byBayesrulewehave
O⃗
P (⃗o|⃗s)·B(⃗s)
B(⃗s|⃗o)=
O⃗
(cid:80) P (⃗o|⃗s′)·B(⃗s′)
⃗s′ O⃗
30ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
δ
(cid:0) O⃗(⃗s)(cid:1)
·B(⃗s)
⃗o
=
(cid:80) δ (cid:0) O⃗(⃗s′)(cid:1) ·B(⃗s′)
⃗s′ ⃗o
(cid:40) 0, O⃗(⃗s)̸=⃗o
=
B(⃗s) , O⃗(⃗s)=⃗o.
(cid:80) ⃗s′:O⃗(⃗s′)=⃗oB(⃗s′)
Thus,foranyreturnfunctionG′andanyobservationsequence⃗o,wehave
(cid:2) B(G′)(cid:3)
(⃗o)= E
(cid:2) G′(⃗s)(cid:3)
⃗s∼B(⃗s|⃗o)
(cid:88)
= B(⃗s|⃗o)G′(⃗s)
⃗s
(cid:88) B(⃗s)
= G′(⃗s)
(cid:80) B(⃗s′)
⃗s:O⃗(⃗s)=⃗o
⃗s′:O⃗(⃗s′)=⃗o
(cid:32) (cid:33)−1
(cid:88) (cid:88)
= B(⃗s′) · B(⃗s)G′(⃗s).
⃗s′:O⃗(⃗s′)=⃗o ⃗s:O⃗(⃗s)=⃗o
Thus,wehaveG′ ∈kerBifandonlyif
(cid:88)
B ·G′ = B(⃗s)G′(⃗s)=0
⃗o ⃗o
⃗s:O⃗(⃗s)=⃗o
forall⃗o. Thatwastoshow.
RemarkB.26. Onecaninterpretthepreviouspropositionasfollows:
AslongasO⃗ isinjective,wehave(cid:12) (cid:12){⃗s∈S⃗ |O⃗(⃗s)=o}(cid:12) (cid:12)=1forall⃗o,meaningthatB ⃗oandG ⃗′ ohaveonlyoneentry. Thus,
B ·G′ =0impliesG′ =0. Ifthatholdsforall⃗o,thenG′ ∈kerBimpliesG′ =0,meaningBisinjective.
⃗o ⃗o ⃗o
However,assoonasthereisan⃗owithk ⃗o := (cid:12) (cid:12){⃗s ∈ S⃗ | O⃗(⃗s) = o}(cid:12) (cid:12) > 1,theequationB ⃗o·G ⃗′ o = 0leadstok ⃗o−1free
parametersinG′. G′ canthenbechosenfreelyinthehyperplaneofvectorsorthogonaltoB withoutmovingoutofthe
⃗o ⃗o ⃗o
kernelofB.
AnotherwayofwritingPropositionB.25istowritekerBasadirectsumofthesehyperplanesperpendiculartoB :
⃗o
(cid:77)
kerB= B⊥.
⃗o
⃗o:|O⃗−1(⃗o)|≥2
RecallthatareturnfunctionGiscalledtime-separableifthereexistsarewardfunctionRsuchthatΓ(R)=G.
Beforewediscusstime-separabilityinmoreinterestingexamples,wewanttotalkaboutonesimplecasewhereallreturn
functionsaretime-separable. WeleaveageneralcharacterizationofimΓtofuturework.
PropositionB.27. Lettherebeanordering⃗s(1),⃗s(2),... ofallsequencesinS⃗,andafunctionϕ:S⃗ →S fromsequences
tostatessuchthatϕ(⃗s)∈⃗sandϕ(⃗s(k))∈/⃗s(i)foralli<k. Theneveryreturnfunctionistime-separable.
Proof. LetGbeareturnfunction. InitializeR(s)=0forallsandinductivelyupdateitforalli=1,2,...:
(cid:32) (cid:33)−1 (cid:32) (cid:33)
R(cid:0) ϕ(⃗s(i))(cid:1) := (cid:88) γt · G(⃗s(i))− (cid:88) γt·R(cid:0) s(i)(cid:1) ,
t
t:s(i)=ϕ(⃗s(i)) t:s(i)̸=ϕ(⃗s(i))
t t
wheretheinductivedefinitionalwaysusesRasitisdefinedbythatpointintime.
OnceR(cid:0) ϕ(⃗s(i))(cid:1)
isdefined,butnotyet
anyfuturevaluesR(cid:0) ϕ(⃗s(k))(cid:1)
,k >i,wehave
T
(cid:2) Γ(R)(cid:3) (⃗s(i))=(cid:88) γt·R(cid:0) s(i)(cid:1)
t
t=0
31ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
(cid:32) (cid:33)
= (cid:88) γt ·R(cid:0) ϕ(⃗s(i))(cid:1) + (cid:88) γt·R(cid:0) s(i)(cid:1)
t
t:s(i)=ϕ(⃗s(i)) t:s(i)̸=ϕ(⃗s(i))
t t
=G(⃗s(i)).
Furthermore,thepropertyϕ(⃗s(k))∈/⃗s(i)foralli<kensuresthatchangestotherewardfunctionfork >idonotaffectthe
valueof(cid:2) Γ(R)(cid:3) (⃗s(i)). ThisshowsΓ(R)=G,andthusGistime-separable.
CorollaryB.28. Inamulti-armedbandit,everyreturnfunctionistime-separable.
Proof. Inamulti-armedbandit,statesandsequencesareequivalent,andsowecanchooseϕ(s)=sforeverystate/sequence
s. TheresultfollowsfromPropositionB.27.
Alternatively,simplydirectlynoticethatinamulti-armedbandit,Γistheidentitymapping,andsoforeveryreturn/reward
functionR,wehaveΓ(R)=R.
B.8.ExamplesSupplementingSection5
Inthiswholesection,theinversetemperatureparameterinthehumanchoiceprobabilitiesisgivenbyβ = 1. Wenow
considerfourmoremathematicalexamplesofCorollaryB.4andTheoremB.9. Inthefirstexample,theambiguityissobad
thattherewardinferencecanbecomeworsethansimplymaximizingJ asinnaiveRLHF.InExampleB.30,thereis
obs
simply“noise”intheobservationsandthehuman’sbelief,thematricesBandOareinjective,andidentifiabilityworks,as
inCorollaryB.14. Inthethirdexample,thematrixBisnotinjectiveandidentifiabilityfails,whichisaminimalexample
showingthelimitsofourmaintheorems. Inthefourthexample,thematrixBisnotinjective,butkerB∩imΓ={0},and
soidentifiabilityworks. Thisexampleisinterestinginthattheidentifiabilitysimplyemergesthroughdifferentdistributions
ofdelaythatarecausedbythedifferentunobservedevents.
Inthissection,boththelinearoperatorsB:RS⃗ →RΩ⃗ andO:RΩ⃗ →RS⃗
areconsideredasmatrices
O=(cid:0)
P
(⃗o|⃗s)(cid:1) ∈RS⃗×Ω⃗
,
B=(cid:0) B(⃗s|⃗o)(cid:1) ∈RΩ⃗×S⃗
.
O⃗ ⃗s,⃗o ⃗o,⃗s
Noticethatbothhaveaswapintheirindices.
Example B.29. Theorem 5.1 shows that the remaining ambiguity from the human’s choice probabilities is given by
kerB∩imΓ,butitdoesn’texplainhowtoproceedgiventhisambiguity. Withoutfurtherinductivebiases,somereward
functionswithintheambiguityofthetruerewardfunctioncanbeevenworsethansimplymaximizingJ .
obs
E.g., consider a multi-armed bandit with three actions a,b,c, observation-kernel o = O(a) = O(b) ̸= O(c) = c
and reward function R(a) = R(b) < R(c). If the human belief is given by B(a | o) = p = 1 − B(b | o), then
R′ = α·(p−1,p,0) ∈ R{a,b,c} isintheambiguityforallα ∈ R,andsoR˜ := R+R′ iscompatiblewiththechoice
probabilities. However,forα≪0,wehaveR˜(a)>R˜(b)andR˜(a)>R˜(c),andsooptimizingagainstthisrewardfunction
leadstoasuboptimalpolicy.
Incontrast,maximizingJ leadstothecorrectpolicysincea,b,andcallobtaintheirgroundtruthrewardinthisexample.
obs
Thisgenerallyraisesthequestionofhowtotie-breakrewardfunctionsintheambiguity,orhowtoactconservativelygiven
theuncertainty,inordertoconsistentlyimproveuponthesettinginSection4.1.
ExampleB.30. ThisexampleisaspecialcaseofCorollaryB.14. Consideramulti-armedbanditwithtwoactions(which
areautomaticallyalsostatesandsequences)aandb. Inthiscase,therewardfunctionandreturnfunctionisthesame.
We assume there to be two possible observations o(a),o(b) and the observation kernel to be non-deterministic, with
probabilities
(cid:40)
2/3, ifi=j,
P (o(j) |i)=
O
1/3, else.
IfweassumethehumanformsBayesianposteriorbeliefsasinSectionB.1andtohaveapolicypriorB(π′)suchthat
B(a) = (cid:82) π(a)B(π′)dπ = 1/2 and B(b) = 1/2, then it is easy to show that the human’s belief is the “reversed”
π
observationkernel:
B(j |o(i))=P (o(i) |j).
O
32ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Weobtain
(cid:18) (cid:19) (cid:18) (cid:19)
2/3 1/3 1 2 1
O=B= = ·
1/3 2/3 3 1 2
Thesematricesareinjectivesincetheyareinvertible:
(cid:18) (cid:19)
2 −1
O−1 =B−1 = .
−1 2
Moregenerally,evenifthehumandoesnotformfullyrationalposteriorbeliefs,itiseasytoimaginethatthematrixBcan
endupbeinginvertible. Thus,CorollaryB.4guaranteesthattherewardfunctioncanbeinferreduptoanadditiveconstant
fromthechoiceprobabilitiesofobservations,andTheoremB.9showsthatthisevenworkswhenthelearningsystemdoes
notknowwhatthehumanobserved.
Intherestofthisexample,weexplicitlywalkthereaderthroughtheprocessofhowtherewardfunctioncanbeinferred,
inthegeneralcasethattheobservationsarenotknown. Intheprocess,weessentiallyrecreatetheproofofthetheorems
forthisspecialcase.
Forthisaim,wefirstwanttocomputethechoiceprobabilitiesPR(cid:0) i≻j(cid:1)
thatthelearningsystem
hasaccesstointhelimitofinfinitedata. WeassumethattherewardfunctionisgivenbyR(a)=−1andR(b)=2. We
compute:
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
1 2 1 −1 0
B(R)= · · = .
3 1 2 2 1
Inotherwords,wehaveE [R(s)]=0andE [R(s)]=1. Fromthis,wecancomputetheobservation-
s∼B(s|o(a)) s∼B(s|o(b))
basedchoiceprobabilitiesP(cid:101)
o(i)o(j)
=σ(cid:0) B(R)(o(i))−B(R)(o(j))(cid:1)
,seeEquation(2),andobtain:
1 1 e
P(cid:101)
o(a)o(a)
=P(cid:101)
o(b)o(b)
= 2, P(cid:101)
o(a)o(b)
= 1+e, P(cid:101)
o(b)o(a)
= 1+e.
WecannowdeterminethefinalchoiceprobabilitiesP
:=PR(cid:0) i≻j(cid:1)
againbyamatrix-vectorproduct,withtheindices
ij
orderedlexicographically,seeEquation(8). Here,O⊗OistheKroneckerproductofthematrixOwithitself:
     
4 2 2 1 1/2 1/2
1 2 4 1 2 1/(1+e) 1/3·(2+e)/(1+e)
P =(O⊗O)·P(cid:101) = · · = .
9 2 1 4 2 e/(1+e) 1/3·(1+2e)/(1+e)
1 2 2 4 1/2 1/2
Forexample,thesecondentryinP isP =PR(cid:0) a≻b(cid:1) = 2+e . Thisisthelikelihoodthat,forground-truthactionsa,b,
ab 3·(1+e)
thehumanwillpreferaafteronlyreceivingobservationso(a)oro(b)accordingtoOandfollowingaBoltzman-rational
policybasedonthebeliefoftherealaction,seeEquation(8).
Overtime,thelearningsystemwillbeabletoestimatetheseprobabilitiesbasedonrepeatedhumanchoices,assumingall
state-pairsaresampledinfinitelyoften. ThequestionofidentifiabilityiswhethertheoriginalrewardfunctionRcanbe
inferredfromthatdata,giventhatthelearningsystemknowsOandB. Weassumethatthelearningsystemdoesn’tapriori
knowRoranyoftheintermediatestepsinthecomputation. First,P(cid:101)canbeinferredbyinvertingO⊗O:
     
4 −2 −2 1 1/2 1/2
P(cid:101) =(O⊗O)−1·P = −2 4 1 −2 · 1/3·(2+e)/(1+e) = 1/(1+e) .
−2 1 4 −2 1/3·(1+2e)/(1+e) e/(1+e)
1 −2 −2 4 1/2 1/2
ThelearningsystemwantstousethistoinferB(R˜)(forthelater-to-beinferredrewardfunctionR˜thatmaydifferfromthe
truerewardfunctionR)andusestheequation
exp(cid:0) B(R˜)(o(a))(cid:1)
P(cid:101)
o(a)o(b)
=
exp(cid:0) B(R˜)(o(a))(cid:1) +exp(cid:0)
B(R˜)(o(b))(cid:1),
33ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
whichcanberearrangedto
B(R˜)(o(a))=log
P(cid:101)
o(a)o(b)
+B(R˜)(o(b))=log1/(1+e)
+B(R˜)(o(b))=B(R˜)(o(b))−1.
1−P(cid:101)
o(a)o(b)
e/(1+e)
ThisrelationisallwhichcanbeinferredaboutB(R˜)(o(a))andB(R˜)(o(b));theprecisevaluecannotbedeterminedand
B(R˜)(o(b))isafreeparameter. OnecancheckthatforB(R˜)(o(b))=1thiscoincideswiththetruevalueB(R). Finally,
onecaninvertBtoinferR˜fromthis:
R˜ =B−1·B(R˜)
(cid:18)
2
−1(cid:19) (cid:18) B(R˜)(o(b))−1(cid:19)
= ·
−1 2 B(R˜)(o(b))
(cid:18) B(R˜)(o(b))−2(cid:19)
=
1+B(R˜)(o(b))
(cid:18) −1(cid:19) (cid:18) B(R˜)(o(b))−1(cid:19)
= +
2 B(R˜)(o(b))−1
(cid:18) B(R˜)(o(b))−1(cid:19)
=R+ .
B(R˜)(o(b))−1
Thus,theinferredandtruerewardfunctionsdiffermaximallybyaconstant,aspredictedinTheoremB.9.
Inthefollowingexample,weworkoutacasewheretherewardfunctionissoambiguousthatanypolicyisoptimaltosome
rewardfunctionconsistentwiththehumanfeedback:
Example B.31. Consider a multi-armed bandit with exactly three actions/states a,b,c. We assume a deterministic
observationkernelwitho:=O(a)=O(c)̸=O(b)=b. AssumethehumanhassomearbitrarybeliefsB(a|o),B(c|o)=
1−B(a|o),andcanidentifyb: B(b|b)=1. ThenifthehumanmakesobservationcomparisonswithaBoltzman-rational
policy,asinTheoremB.2,theresultingrewardfunctionissoambiguousthatsomerewardfunctionsconsistentwiththe
feedbackplacethehighestvalueonactiona,nomatterthetruerewardfunctionR. Thus,evenifthetruerewardfunctionR
regardsaastheworstaction,acanresultfromtherewardlearningandsubsequentpolicyoptimizationprocess.
Proof. ThematrixB:R{a,b,c} →R{o,b}isgivenby
(cid:18) (cid:19)
B(a|o) 0 B(c|o)
B= .
0 1 0
Its kernel is given by reward functions R′ with R′(b) = 0 and R′(c) = −B(a|o)R′(a), with R′(a) a free parameter.
B(c|o)
Theorem B.2 shows that, up to an additive constant, the reward functions consistent with the feedback of observation
comparisonsaregivenbyR˜ = R+R′ foranyR′ ∈ kerB. Thus,wheneverthefreeparameterR′(a)satisfiesR′(a) >
R(b)−R(a)andR′(a)>B(c|o)·(cid:0) R(c)−R(a)(cid:1) ,weobtainR˜(a)>R˜(b)andR˜(a)>R˜(c),showingtheclaim.
WenowinvestigateanotherexamplewhereBisnotinjective,andyet,identifiabilityworksbecauseB◦Γ̸={0}. Wesaw
suchcasesalreadyinExampleC.6,butincludethisadditionalexamplesinceitshowsaconceptuallyinterestingcase: two
differentstatesleadtotheexactsameobservations,butcanbedisambiguatedsincetheyleadtodifferentamountsofdelay
untilamoreinformativeobservationismadeagain.
Example B.32. In this example, we assume that the human knows the policy π that generates the state sequences
(corresponding to a policy prior B(π′) = δ (π′) concentrated on π), which together with knowledge of the transition
π
dynamicsoftheenvironmentdeterminesthetruestatetransitionprobabilitiesTπ(s′ |s)=(cid:80) T(s′ |s,a)·π(a|s).
a∈A
We consider an environment with three states s,s′,s′′ and the following transition dynamics Tπ, where p ̸= 1/2 is a
34ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
probability:
1/3
s
1/3 1/3
1−p p
p s′ s′′ 1−p
WeassumethatP (s)=1. Furthermore,weassumedeterministicobservationsands=O(s)̸=O(s′)=O(s′′)=:o.
0
AssumethetimehorizonT is3,i.e.,therearetimesteps0,1,2,3. Assumethatthehumanformsthebeliefoverthetruestate
sequencebyBayesianposteriorupdatesasinSectionB.1. Inthiscase,kerB̸={0}byPropositionB.11. However,wewill
nowshowthatker(B◦Γ)={0}. IfthehumanmakesBoltzmann-rationalcomparisonsofobservationsequences,thenthis
impliestheidentifiabilityofthereturnfunctionuptoanadditiveconstantbyCorollaryB.4.5
Thus,letR′ ∈ker(B◦Γ),i.e.,(cid:104) B(cid:0) Γ(R′)(cid:1)(cid:105) (⃗o)=0foreveryobservationsequence⃗o. For⃗o=ssssbeingtheobservation
sequencethatonlyconsistsofstates,thisimpliesR′(s)=0. Consequently,forgeneralobservationsequences⃗o,wehave:
(cid:34) 3 (cid:35) (cid:34) 3 (cid:35)
0=(cid:104) B(cid:0) Γ(R′)(cid:1)(cid:105) (⃗o)= E (cid:88) δ (s )·γt ·R′(s′)+ E (cid:88) δ (s )·γt ·R′(s′′).
s′ t s′′ t
⃗s∼B(⃗s|⃗o) ⃗s∼B(⃗s|⃗o)
t=0 t=0
Nowwespecializethisequationtothetwoobservationsequences⃗o(1) =sossand⃗o(2) =soos. Westartbyconsidering
⃗o(1). Thisisconsistentwiththetwostatesequences⃗s(1),(s′) =ss′ssand⃗s(1),(s′′) =ss′′ss. Wehaveposteriorprobabilities
B(cid:0) ⃗s(1),(s′) |⃗o(1)(cid:1)
=1−p,
B(cid:0) ⃗s(1),(s′′) |⃗o(1)(cid:1)
=p,
andtherefore
0=(cid:2) B(cid:0) Γ(R′)(cid:1)(cid:105) (⃗o(1))=(1−p)·γ·R′(s′)+p·γ·R′(s′′),
andso
p
R′(s′)= ·R′(s′′). (11)
p−1
Similarly,⃗o(2)isconsistentwiththesequences⃗s(2),(s′) =ss′s′sand⃗s(2),(s′′) =ss′′s′′s. Theyhaveposteriorprobabilities
B(cid:0) ⃗s(2),(s′) |⃗o(2)(cid:1)
=
1
,
B(cid:0) ⃗s(2),(s′′) |⃗o(2)(cid:1)
=
1
,
2 2
leadingto
1 1
0= ·(γ+γ2)·R′(s′)+ ·(γ+γ2)·R′(s′′).
2 2
TogetherwithEquation(11),weobtain
p
R′(s′′)=−R′(s′)= ·R′(s′′),
1−p
whichimpliesR′(s′′) = 0becausep ̸= 1,andthusalsoR′(s′) = 0. Overall,wehaveshowedR′ = 0,andsoB◦Γis
2
injective. Thismeansthatrewardfunctionsareidentifiableinthisexampleuptoanadditiveconstant,seeCorollaryB.4.
C.IssuesofNaivelyApplyingRLHFunderPartialObservability
Inthissection,westudythenaiveapplicationofRLHFunderpartialobservability. Thus,mostofittakesastepbackfrom
thegeneraltheoryofappropriatelymodeledpartialobservabilityinRLHF.Later,wewillanalyzeexampleswherewealso
applythegeneraltheory,whichiswhythisappendixsectioncomessecond.
5Weassumethatthelearningsystemknowswhatthehumanobserves,whichisvalidsinceP isdeterministic.Alternatively,onecan
O
arguewithPropositionB.11thatOisautomaticallyinjective,meaningonecanapplyTheoremB.9.
35ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
InSectionC.1,wefirstbrieflyexplainwhathappenswhenthelearningsystemincorrectlyassumesthatthehumanobserves
thefullenvironmentstate. Weshowthatasaconsequence,thesystemisincentivizedtoinferwhatwecalltheobservation
returnfunctionG ,whichevaluatesastatesequencebasedonthehuman’sbeliefofthestatesequencegiventhehuman’s
obs
observations. Inthepolicyoptimizationprocess,thepolicyisthenselectedtomaximizeJ ,anexpectationoverG . In
obs obs
theinterludeinSectionC.2,wethenbrieflyanalyzetheunrealisticcasethatthehuman,whenevaluatingapolicyπ,fully
knowsthecompletespecificationofthatpolicyandalloftheenvironmentandengagesinrationalBayesianreasoning;in
thiscase,J =J isthetruepolicyevaluationfunction.
obs
Realistically,however,maximizingJ canleadtofailuremodes. InAppendixC.3weprovethatasuboptimalpolicy
obs
thatisoptimalaccordingtoJ causesdeceptiveinflation, overjustification, orboth. InAppendixC.4, weexpandon
obs
theanalysisofthemainexamplesinthemainpaper. Finally,inSectionC.5,westudyfurtherconcreteexampleswhere
maximizingJ revealsdeceptiveandoverjustifyingbehaviorbytheresultingpolicy.
obs
C.1.OptimalPoliciesunderRLHFwithDeterministicPartialObservationsMaximizeJ
obs
AssumethatP isdeterministicandthatthehumanmakesBoltzmann-rationalsequencecomparisonsbetweenobservation
O⃗
sequences. Thetruechoiceprobabilitiesarethengivenby(SeeEquations(2)and(8)):
PR(cid:0) ⃗s≻⃗s′(cid:1) =σ(cid:18) β·(cid:16)(cid:0) B·G(cid:1)(cid:0) O⃗(⃗s)(cid:1) −(cid:0) B·G(cid:1)(cid:0) O⃗(⃗s′)(cid:1)(cid:17)(cid:19)
(12)
Now,assumethatthelearningsystemdoesnotmodelthesituationcorrectly. Inparticular,weassume:
• ThesystemisnotawarethatthehumanonlyobservesobservationsequencesO⃗(⃗s)insteadofthefullstatesequences.
• Thesystemdoesnotmodelthatthehuman’sreturnfunctionistime-separable,i.e.,comesfromarewardfunctionR
overenvironmentstates.
ThelearningsystemthenthinksthatthereisareturnfunctionG˜ ∈RS⃗ suchthatthechoiceprobabilitiesaregivenbythe
followingfaultyformula:
PR(cid:0) ⃗s≻⃗s′(cid:1) :=σ(cid:16) β(cid:0) G(⃗s)−G(⃗s′)(cid:1)(cid:17)
Now,assumethatthelearningsystemhasaccesstothechoiceprobabilitiesandwantstoinferG. Invertingthesigmoid
functionandthenplugginginthetruechoiceprobabilitiesfromEquation(12),weobtain:
1 PR(⃗s≻⃗s′)
G˜(⃗s)= log +G˜(⃗s′)
β PR(⃗s′ ≻⃗s)
= 1(cid:104) β·(cid:16)(cid:0) B·G(cid:1)(cid:0) O⃗(⃗s)(cid:1) −(cid:0) B·G(cid:1)(cid:0) O⃗(⃗s′)(cid:1)(cid:17)(cid:105) +G˜(⃗s′)
β
=(cid:0) B·G(cid:1)(cid:0) O⃗(⃗s)(cid:1) +C(⃗s′).6
Here,C(⃗s′)issomequantitythatdoesnotdependon⃗s. Now,fix⃗s′asareferencesequence. Thenforvarying⃗s,C(⃗s′)is
simplyanadditiveconstant. Consequently,uptoanadditiveconstant,thisdeterminesthereturnfunctionthatthelearning
systemisincentivizedtoinfer. Wecallittheobservationreturnfunctionsinceitisthereturnfunctionbasedonthehuman’s
observations:
G
(⃗s):=(cid:0) B·G(cid:1)(cid:0) O⃗(⃗s)(cid:1)
.
obs
Thisreturnfunctionisnotnecessarilytime-separable,butweassumethattime-separabilityisnotmodeledcorrectlybythe
learningsystem. Now,definetheresultingpolicyevaluationfunctionJ by
obs
(cid:2) (cid:3)
J (π):= E G (⃗s) .
obs obs
⃗s∼Pπ(⃗s)
Thisisthepolicyevaluationfunctionthatwouldbeoptimizedifthelearningsystemerroneouslyinferredthereturnfunction
G .
obs
6Notethatinthecaseofnon-deterministicobservationkernelsandchoiceprobabilitiesgivenasinEquation(8),thisargumentdoes
notworksincethelogarithmcannotbeswappedwiththeouterexpectationofthechoiceprobabilities.
36ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
C.2.Interlude: WhentheHumanKnowsthePolicyandisaBayesianReasoner,thenJ =J
obs
Inthissection,webrieflyconsiderwhatwouldhappenifinJ ,thehuman’sbeliefBwouldmakeuseofthetruepolicy
obs
andbearationalBayesianposteriorasinSectionB.1. Wewillshowthatundertheseconditions,wehaveJ =J. Since
obs
theseareunrealisticassumptions,noothersectiondependsonthisresult.
Fortheanalysis,wedroptheassumptionthattheobservationsequencekernelP isdeterministic,andassumethatJ is
O⃗ obs
givenasfollows:
(cid:34) (cid:20) (cid:21)(cid:35)
J (π):= E E E
(cid:2) G(⃗s′)(cid:3)
. (13)
obs
⃗s∼Pπ(⃗s) ⃗o∼P O⃗(⃗o|⃗s) ⃗s′∼Bπ(⃗s′|⃗o)
Inthisformula,Bπ(⃗s|⃗o):=B(⃗s|⃗o,π)withBbeingthejointdistributionfromSectionB.1. Formally,thisistheposterior
ofthejointdistributionB(⃗s,⃗o|π)thatisgivenbythefollowinghiddenMarkovmodel:
s Tπ s Tπ s Tπ s Tπ ...
0 1 2 3
(14)
PO PO PO PO
o o o o ...
0 1 2 3
Here,Tπ(s′ | s) := (cid:80) T(s′ | s,a)·π(a | s). s issampledaccordingtotheknowninitialdistributionP (s ). The
a∈A 0 0 0
human’sposteriorBπ(⃗s′ |⃗o)isthenthetrueposteriorinthisHMM.Weobtain:
PropositionC.1. Letπbeapolicythatisknowntothehuman. ThenJ (π)=J(π).
obs
Proof. ByEquation(13),wehave
(cid:34) (cid:20) (cid:21)(cid:35)
J (π)= E E E
(cid:2) G(⃗s′)(cid:3)
obs
⃗s∼Pπ(⃗s) ⃗o∼P O⃗(⃗o|⃗s) ⃗s′∼Bπ(⃗s′|⃗o)
( =1)(cid:88) Pπ(⃗s)(cid:88) P (⃗o|⃗s)(cid:88) Bπ(⃗s′ |⃗o)G(⃗s′)
O⃗
⃗s ⃗o ⃗s′
(cid:34) (cid:34) (cid:35)(cid:35)
( =2)(cid:88) (cid:88) Bπ(⃗s′ |⃗o) (cid:88) P (⃗o|⃗s)Pπ(⃗s) G(⃗s′)
O⃗
⃗s′ ⃗o ⃗s
(cid:34) (cid:35)
( =3)(cid:88) (cid:88) Bπ(⃗s′ |⃗o)Bπ(⃗o) G(⃗s′)
⃗s′ ⃗o
(cid:34) (cid:35)
( =4)(cid:88) (cid:88) Pπ(⃗s′)P (⃗o|⃗s′) G(⃗s′)
O⃗
⃗s′ ⃗o
( =5)(cid:88) Pπ(⃗s′)G(⃗s′)
⃗s′
( =6)(cid:88) Pπ(⃗s)G(⃗s)
⃗s
(7)
= J(π).
Instep(1),wewrotetheexpectationsoutintermsofsums. Instep(2),wereorderedthem. Instep(3),weobservedthatthe
innersumover⃗sevaluatestothemarginaldistributionBπ(⃗o)oftheobservationsequence⃗ointheHMMinEquation(13).
Instep(4),weusedBayesruleintheinnersum. ThisispossiblesinceBπ(⃗s′ |⃗o)isthetrueposteriorwhenπisknown. In
step(5),wepullPπ(⃗s′)outandnoticethattheremaininginnersumevaluatesto1. Step(6)isarelabelingandstep(7)the
definitionofthetruepolicyevaluationfunctionJ.
37ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
C.3.ProofofTheorem4.5
Wefirstprovethefollowinglemma(thisisLemma4.4,repeatedhereforconvenience):
Lemma C.2. Let π and π be two policies. If J(π) < J(π ) and J (π) ≥ J (π ), then relative to π , π must
ref ref obs obs ref ref
exhibitdeceptiveinflating,overjustification,orboth.
Proof. Westartbyestablishingaquantitativerelationshipbetweentheaverageoverestimationandunderestimationerrors
+ −
E andE asdefinedinDefinition4.2,thetruepolicyevaluationfunctionJ,andtheobservationevaluationfunctionJ
obs
definedinEquation(4). Define∆:S⃗ →Rby∆(⃗s)=G (⃗s)−G(⃗s),whereG isasdefinedinEquation(3). Consider
obs obs
thequantity
E+(⃗s)−E−(⃗s)=max(cid:0) 0,∆(⃗s)(cid:1) −max(cid:0) 0,−∆(⃗s)(cid:1)
.
If∆(⃗s)>0,thenthefirsttermis∆(⃗s)andthesecondoneis0. If∆(⃗s)<0,thenthefirsttermiszeroandthesecondone
is∆(⃗s). If∆(⃗s)=0,thenbothtermsarezero. Inallcasestheright-handsideisequalto∆(⃗s). Unpackingthedefinitionof
∆again,wehavethatforall⃗s,
E+(⃗s)−E−(⃗s)=G (⃗s)−G(⃗s). (15)
obs
Foranypolicyπ,ifwetaketheexpectationofbothsidesofthisequationovertheon-policydistributionadmittedbyπ,Pπ,
weget
+ −
E (π)−E (π)=J (π)−J(π). (16)
obs
We now prove the lemma. Let π and π be two policies, and assume that J(π) < J(π ) and J (π) ≥ J (π ).
ref ref obs obs ref
Equivalently,wehaveJ (π)−J (π )≥0andJ(π )−J(π)>0,whichwecombinetostate
obs obs ref ref
(cid:16) (cid:17) (cid:16) (cid:17)
J (π)−J (π ) + J(π )−J(π) >0. (17)
obs obs ref ref
Rearrangingtermsyields
(cid:16) (cid:17) (cid:16) (cid:17)
J (π)−J(π) − J (π )−J(π ) >0.
obs obs ref ref
Thesetwodifferencesinsideparenthesesareequaltotheright-handsideof(16)forπandπ ,respectively. Wesubstitute
ref
theleft-handsideof(16)twicetoobtain
(cid:16) (cid:17) (cid:16) (cid:17)
+ − + −
E (π)−E (π) − E (π )−E (π ) >0.
ref ref
Rearrangingtermsagainyields
(cid:16) (cid:17) (cid:16) (cid:17)
+ + − −
E (π)−E (π ) + E (π )−E (π) >0. (18)
ref ref
+ + + +
IfE (π)−E (π )>0thenwehaveE (π)>E (π )and,byassumption,J (π)≥J (π ). ByDefinition4.3,
ref ref obs obs ref
thismeansπexhibitsdeceptiveinflatingrelativetoπ .
ref
− − − −
IfE (π )−E (π)>0thenwehaveE (π)<E (π )and,byassumption,J(π)<J(π ). ByDefinition4.3,this
ref ref ref
meansπexhibitsoverjustificationrelativetoπ .
ref
Atleastoneofthetwodifferencesinparenthesesin(18)mustbepositive,otherwisetheirsumwouldnotbepositive. Thus
πmustexhibitdeceptiveinflatingrelativetoπ ,overjustificationrelativetoπ ,orboth.
ref ref
WecannowcombineearlierresultstoproveTheorem4.5,repeatedhereforconvenience:
TheoremC.3. AssumethatP isdeterministic. Letπ∗ beanoptimalpolicyaccordingtoanaiveapplicationofRLHF
O obs
underpartialobservability,andletπ∗beanoptimalpolicyaccordingtothetrueobjectiveJ. Ifπ∗ isnotJ-optimal,then
obs
relativetoπ∗,π∗ mustexhibitdeceptiveinflating,overjustification,orboth.
obs
Proof. BecauseP isdeterministic,π∗ mustbeoptimalwithrespecttoJ byProposition4.1(provedinAppendixC.1).
O obs obs
Thus J (π∗ ) ≥ J (π∗). Since π∗ is J-optimal and π∗ is not, J(π∗) < J(π∗ ). By Lemma 4.4 (repeated as
obs obs obs obs obs
LemmaC.2above),relativetoπ∗,π∗ mustexhibitdeceptiveinflating,overjustification,orboth.
obs
38ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
C.4.DerivationsandFurtherDetailsforSection4.3ExampleA
Figure5.AnexpandedviewofFigure2A.Commandscorrespondingtothevariousactionsaredepictedalongedges,andlogmessages
correspondingtothevariousobservationsaredepictedunderneatheachstate.
WefirstincludeFigure5,amoredetailedpictureoftheMDPandobservationfunctioninSection4.3.1,tohelpgroundthe
narrativedetailsoftheexample.
NextweformallyenumeratethedetailsoftheMDPandobservationfunction.
• S ={S,I,W,W ,L,L ,T}.
H H
• A={a ,a ,a ,a }.
I C H T
• T isasdepictedinFigure5andFigure2A.Forastates,anyoutgoingarrowlabeledwithanactiona(suchasa )
I
describesthedistributionT(s′ |s,a)asfollows: ifthearrowdoesnotsplit,thenT(s′ |s,a)=1wheres′isthestate
thearrowpointsto;ifthearrowdoessplit,thenforeachsuccessorstates′ iteventuallyreaches,aprobabilityq is
writtenjustbeforetheboxcorrespondingtos′(forthisexample,q =porq =1−p),andT(s′ |s,a)=q.
◦ Additionally,anyactiontakenfromastatethatdoesnothaveanoutgoingarrowcorrespondingtothatactionwill
immediatelytransitiontostateT,asthougha hadbeentaken.
T
◦ AnyactiontakenfromstateT transitionsdeterministicallytoT.
• P (S)=1.
0
• R is as described in the table (the numbers in the top right of each state box) with r ≥ 0. Additionally,
R(S)=R(T)=0.
• γ =1.
Weworkwithafixedhorizonlengthof3,meaningstatesequenceshavelength4(sincetimeiszero-indexed: s s s s ).
0 1 2 3
TheobservationfunctionisalsodepictedinFigure5. Eachstatedeterministicallyproducestheobservationinthelower-right
cornerofitsboxinthefigure. WealsowriteitinanotherformatinTable8.
s S I W W L L T
H H
O(s) o o o o o o o
∅ I W W L ∅ ∅
Table8. TheobservationfunctionOfortheexampleinSection4.3.1andAppendixC.4.
WemaketheadditionalassumptionthatthehumanbeliefB(⃗s|⃗o)onlysupportsstatesequences⃗swhichactuallyproduce⃗o
underthesequenceobservationfunctionO⃗: B(⃗s|⃗o)>0 =⇒ O⃗(⃗s)=⃗o. Inparticular,thismeansthatforany⃗owhichis
onlyproducedbyone⃗s,B(⃗o|⃗s)=1.
Therearethreepairsofstatesequenceswhichproduceidenticalobservationsequences. Foreach,weintroduceaparameter
representingtheprobabilitythehumaninfersthefirstofthepairofstatesequencesuponseeingtheirsharedobservation
sequence.
39ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
1. SIL T andSITT bothproduceo o o o ,alogcontainingonlyasuccessconfirmationforinstallingdrivers,again
H ∅ I ∅ ∅
becauseO(L )=O(T)=o . Letp =B(⃗s=SIL T |⃗o=o o o o ).
H ∅ H H ∅ I ∅ ∅
2. STTT andSL TT bothproduceo o o o ,anemptylog,sinceO(L )=O(T)=o . Letp′ =B(⃗s=SL TT |
H ∅ ∅ ∅ ∅ H ∅ H H
⃗o=o o o o ).
∅ ∅ ∅ ∅
3. SIWT andSIW T bothproduceo o o o ,alogcontainingsuccessconfirmationsforinstallingbothdriversand
H ∅ I W ∅
CUDA.Letp =B(⃗s=SIWT |⃗o=o o o o ).
W ∅ I W ∅
Weassumeforsimplicitythatp′ =p ,i.e. thatthehumanisjustaslikelytothinkanemptylogfollowingasuccessful
H H
driverinstallationcontainsanerrorthatwashiddenwith2>/dev/null(p ),astheyaretothinkthatanentirelyempty
H
log contains a hidden error (p′ ). In principle, this need not be true: the human may have differing priors about the
H
agent’sbehaviorinthetwodifferentcircumstances. However,thealgebratoreasonaboutsuchacaseissignificantlymore
cumbersome,andthiscaserevealsnofundamentallydifferentagentbehaviorunderourframeworkthatisn’tpresentinsome
simplercase.
WecanthuswritethefullBasamatrixasinTable9.
STTT SL TT SLTT SITT SIL T SILT SIWT SIW T
H H H
o o o o 1−p p
∅ ∅ ∅ ∅ H H
o o o o 1
∅ L ∅ ∅
o o o o 1−p p
∅ I ∅ ∅ H H
o o o o 1
∅ I L ∅
o o o o p 1−p
∅ I W ∅ W W
Table9.TheparameterizedhumanbelieffunctionBfortheexampleinSection4.3.1andAppendixC.4,expressedasamatrix(rendered
asatable).Anyemptycellisequalto0.
Wehavelaidthegroundworksufficientlytobeginreasoningabouttheobservationreturn,overestimationandunderestimation
error,policieswhichareoptimalundertherewardfunctionlearnedbynaiveRLHF,andtheresultingdeceptiveinflatingand
overjustificationfailuremodes. Webeginbycomputingthemeasuresofinterestforeachstatesequence,showninTable10.
E+(⃗s):=max(0, E−(⃗s):=max(0,
⃗s G(⃗s) G (⃗s):=E [G(⃗s′)]
obs ⃗s′∼B(·|O⃗(⃗s)) G (⃗s)−G(⃗s)) G(⃗s)−G (⃗s))
obs obs
STTT 0 p G(SL TT)+(1−p )G(STTT) 0 p (5+r)
H H H H
SL TT −5−r =−p (5+r) (1−p )(5+r) 0
H H H
SLTT −5 −5 0 0
SITT 1 p G(SIL T)+(1−p )G(SITT) 0 p (5+r)
H H H H
SIL T −4−r =1−p (5+r) (1−p )(5+r) 0
H H H
SILT −4 −4 0 0
SIWT 11 p G(SIWT)+(1−p )G(SIW T) 0 0
W W H
SIW T 11 =11 0 0
H
Table10.MeasuresofinterestforeachstatesequencefortheexampleinSection4.3.1andAppendixC.4.Statesequenceswhichproduce
thesameobservationshavetheirG columnsmerged,sincetheynecessarilyhavethesameG .
obs obs
Asanexercise,checkingthecomputationsinTable10isaquickwaytogainsomeintuitionforhowthesequantitiesrelate.
It’sfurtherusefultospeakaboutthesequantitiesusingtheirnames,andworkthroughthestoriestheseexpressionstell.
Consider the E+(SL TT) cell. What does it mean that this is (1 − p )(5 + r)? E+ is the overestimation error;
H H
(1−p )(5+r)istheexpectedamountbywhichthehumanobserver,uponseeingacompletelyemptylog(o o o o ),
H ∅ ∅ ∅ ∅
willoverestimatethetotalrewardtheagentattainedwhenthoseemptylogswereactuallyproducedbySL TT. Thisisa
H
trajectoryinwhichtheagentimmediatelyusesthe2>/dev/nulltrick,failstoinstallCUDAwithoutloggingtheerror,
andterminates. Underwhatcircumstancesmightthehumanoverestimatethetotalrewardwhen⃗s=SL TT? Uponseeing
H
theemptylog,thehumanthinkswithprobability1−p thattheagentsimplyterminatedimmediately,whichwouldbe
H
worthatotalrewardof0. Sincetheactualtotalrewardis−5−r,thisisanoverestimateby5+r. Thehumanthinkswith
40ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
probabilityp thatSL TT occurred. Thisiscorrect,sothereisnooverestimationandthisp casedoesnotcontributeto
H H H
theoverestimationerror. Sowehavethatwithprobability1−p ,thehumanoverestimatesthetotalrewardby5+r.
H
Wecankeepgoing! WhyistheunderestimationerrorofSIWT equalto0? Becausetheonlyothertrajectorywithwhich
itcanbeconfusedattainsthesametotalreward,soregardlessofhowtheprobabilitymassofthehuman’sbeliefdivides
between them, there will be no underestimation. Can all of the zeros in the overestimation and underestimation error
columnsbeexplainedthisway?
Wenowmoveontoconsiderpoliciesratherthanstatesequences. SinceapolicyπimposesadistributionPπ overstate
sequences(the“on-policydistribution”),ourpolicymeasuresareinfactexactlyparalleltoourstatesequencemeasures.Each
oneisanexpectationovertheon-policydistributionofthecolumnsofTable10. Werestrictourattentiontodeterministic
policieswhichonlytakeactionsdepictedinFigure5(i.e. thatneverterminateviaanactionotherthana ),ofwhichthere
T
areonlysixinthisMDP.Theyareenumerated,alongwiththepolicy-levelmeasures,inTable11. Policieswillbewrittenas
asequenceofactionsenclosedinbrackets,omittingtrailingrepeateda actions. ThisisnonstandardnotationinanMDP
T
withstochastictransitions,butisunambiguousinthisexample,becausealldecisionsaremadebeforeanystochasticity
occurs. Thepoliciesare[a ],[a a ],[a a ],[a a ],[a a a ],and[a a a ].
T H T C T I T I H T I C T
+ −
π J(π) J (π) E (π) E (π)
obs
[a ] 0 −p (5+r) 0 p (5+r)
T H H
[a a ] −5−r −p (5+r) (1−p )(5+r) 0
H T H H
[a a ] −5 −5 0 0
C T
[a a ] 1 1−p (5+r) 0 p (5+r)
I T H H
pG(SIW T) pG (SIW T)
H obs H
[a a a ] +(1−p)G(SIL T) +(1−p)G (SIL T) (1−p)(1−p )(5+r) 0
I H T H obs H H
=11−(1−p)(15+r) =11−(1−p)[10+p (5+r)]
H
pG(SIWT) pG (SIWT)
obs
[a a a ] +(1−p)G(SILT) +(1−p)G (SILT) 0 0
I C T obs
=11−(1−p)·15 =11−(1−p)·15
Table11.MeasuresofinterestforeachpolicyfortheexampleinSection4.3.1andAppendixC.4. Eachofthecolumnshereisthe
on-policyaverageofthecorrespondingcolumninTable10.Policiesarewrittenassequencesofactions,omittingtrailingrepeateda
T
actions.ThisisnonstandardnotationinanMDPwithstochastictransitions,butisunambiguousinthisexamplesincealldecisionsare
madebeforeanystochasticityoccurs.
With this we have everything we need to characterize optimal policies under the reward function learned by a naive
applicationofRLHF(“policiesselectedbyRLHF”).ByProposition4.1,weknowthatifP isdeterministic,asinthis
O
example,RLHFselectspolicieswhichmaximizeJ . Inordertounderstandthebehaviorofthesepolicies,we’llalsoneed
obs
todeterminethetrueoptimalpolicies,i.e. thosewhichmaximizeJ. We’llproceedincases,onlyconsideringboundary
cases(specificmeasure-zeroparametervaluesforwhichtheresultisdifferent)insofarastheyareinteresting.
Case1: p> 1. Ifp> 1,theCUDAinstall(withdefaultlogging,a )islikelyenoughtosucceedthatit’sworthattempting
3 3 C
it: p·R(W)+(1−p)·R(L)>0. Italsoimmediatelyfollowsthat
J([a a a ])=J ([a a a ])=11−(1−p)·15>1.
I C T obs I C T
Thisallowsustoeliminatepolicies[a ],[a a ],[a a ],and[a a ],whichallhaveJ ≤1andJ ≤1. Noneofthem
T H T C T I T obs
canthusbeJ-optimalorJ -optimal. AllthatremainsistocompareJ andJ for[a a a ]and[a a a ]. Wecan
obs obs I H T I C T
checkthesignofthedifferencesofthesepairsofvalues,startingwithJ.
J([a a a ])−J([a a a ])=(1−p)r.
I C T I H T
Sincepisaprobabilityandr isnonnegative, thisvalueispositive(andthus[a a a ]ispreferredto[a a a ]bythe
I C T I H T
human)ifandonlyifp<1andr >0.
J ([a a a ])−J ([a a a ])=(1−p)[5−p (5+r)].
obs I H T obs I C T H
41ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Thisvalueispositive(andthus[a a a ]isthepolicyRLHFselects)ifandonlyifp<1andp < 5 .
I H T H 5+r
Ifp=1,thenbothdifferencesare0,andbothJ andJ areindifferentbetweenthetwopolicies. Thismakessense,as
obs
theydifferonlyinthecasewheretheCUDAinstallationfails;thishappenswithprobability1−p=0whenp=1. Now
supposep<1. Ifr =0,thenthehumanisindifferentbetweenthetwopolicies. Thisalsomakessense,asrismeantto
quantifytheextenttowhichthehumandislikessuppressedfailures;ifit’szero,thenthehumandoesn’tcare. However,if
p < 5 ,thenJ ([a a a ])>J ([a a a ]),andthusRLHFfavorsthe2>/dev/nullpolicy[a a a ].
H 5+r obs I H T obs I H T I H T
Ifp<1,r >0,andp < 5 ,thenwehavethatJ([a a a ])>J([a a a ])butJ ([a a a ])>J ([a a a ]).
H 5+r I C T I H T obs I C T obs I H T
ThusRLHFwillselectthe2>/dev/nullpolicy[a a a ],andbyTheorem4.5,since[a a a ]isnotJ-optimal,then
I H T I H T
relativeto[a a a ], itmustexhibitdeceptiveinflating, overjustification, orboth. Intuitively, weshouldbesuspicious
I C T
thatdeceptiveinflatingisatplaywhenevertheagenthidesinformationfromthehuman. Weshouldbesuspiciouswhen
theconditionforapolicytobeJ -optimalisthatp ,theagent’ssuspicionthattheagentmaybehidingerrors,islow!
obs H
+ +
Indeed,referencingTable11,wehaveE ([a a a ])=(1−p)(1−p )(5+r)>0=E ([a a a ]). Togetherwith
I H T H I C T
J ([a a a ])>J ([a a a ]),thissatisfiestheconditionsofDefinition4.3,andthusthisisaninstanceofdeceptive
obs I H T obs I C T
inflating.
Ifp<1,r >0,andp > 5 ,then[a a a ]isoptimalunderbothJ andJ ,andinthiscase,RLHFselectsthetrue
H 5+r I C T obs
optimalpolicy.
Case2: p< 1. Inthiscase,theCUDAinstallisnotlikelyenoughtosucceedtobeworthattempting(underthetruereward
3
function). Mathematically,J([a a a ]) ≤ J([a a a ]) < 1 = J([a a ]). Theotherthreepoliciesarealwaysworse
I H T I C T I T
underJ than[a a ],sowehaveouroptimalpolicyπ∗ =[a a ]. However,
I T I T
J ([a a a ])−J ([a a ])=p(10+p (5+r)),
obs I H T obs I T H
whichisalwayspositiveunlessp=0. Aslongasp>0,i.e. installingCUDAisactuallypossibleatall,thenregardless
ofeverythingelse,J ([a a a ])>J ([a a ]). Wecaninterpretthisreasonably: howevergoodorbaditmightlook
obs I H T obs I T
toterminateafterinstallingdrivers(SITT),usingthe2>/dev/nulltrickwilleitherproducethesameobservationas
SITT (andthusthesamefeedbackdistribution)orwillactuallyvisiblyinstallCUDAsuccessfully—themostlucrative
possiblesourceofpositivefeedback. Evenifthere’sonlyasliverofachanceofsuccessandit’sacatastrophetohavea
silentfailure(i.e. r ≫0),howeverlikelyyouaretoinferthatthelackofalogmessageaboutCUDAmeanstheagenthida
failure,itwillpenalize[a a ]morethanitwillpenalize[a a a ]: where[a a a ]producesthisambiguousobservation
I T I H T I H T
withprobability1−p(whenitfails),[a a ]willalwaysproducetheambiguousobservation.
I T
Thismeansthatwhen0<p< 1,itisimpossibletorecoverthetrueoptimalpolicywithnaiveRLHF.Whichpoliciescan
3
possiblybeJ -optimalforsomesettingoftheparameters? Wecansimilarlyruleout[a ]and[a a ]for0<p< 1:
obs T H T 3
J ([a a a ])−J ([a a ])=p(10+p (5+r))>0.
obs I H T obs I T H
Wecanruleout[a a ]bycomparisonto[a a a ]: J ([a a a ])−J ([a a ])=16−(1−p)15>0. Soweare
C T I C T obs I C T obs C T
leftwithonly[a a a ]and[a a a ]ascandidateJ -optimalpolicies.
I H T I C T obs
AsinCase1,wefindthatJ ([a a a ])>J ([a a ])ifandonlyifp=1orp < 5 . Incase2wehaveassumed
obs I H T obs I T H 5+r
p< 1,leavingonlythep condition.
3 H
Ifp < 5 ,thenRLHFselects[a a a ]. AsinCase1,thisisdeceptiveinflatingrelativetoπ∗ =[a a ],because
H 5+r I H T I T
E+ ([a a a ])=(1−p)(1−p )(5+r)>0=E+ (π∗).
I H T H
Ifp > 5 ,thenRLHFselects[a a a ]. BecausethispolicyisnotJ-optimal,byTheorem4.5,wemusthavedeceptive
H 5+r I C T
inflating,overjustification,orboth. Whichisit? Heretheoptimalpolicyistoterminateafterinstallingdrivers,[a a ].
I T
However,p > 5 . Thiscanberewrittenasp (5+r) > 5. Wehaveseenthisexpressionp (5+r)before;itisthe
H 5+r H H
underestimationerrorincurredon⃗s = SITT andthereforealsotheaverageunderestimationerrorofpolicy[a a ]. So
I T
heretheunderestimationerrorontheoptimalpolicy—thatis,theriskthatthehumanmisunderstandsoptimalbehavior
(terminatingafterinstallingdriver)asundesiredbehavior(attemptingaCUDAinstallthatwasunlikelytoworkandhiding
themistake)—issevereenoughthattheagentoptsinsteadfor[a a a ],aworsepolicythatattemptstheill-fatedCUDA
I C T
42ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
installationonlytoprovethatitwasn’tdoingsosecretly. Inqualitativeterms,thisisquintessentialoverjustificationbehavior.
Indeed,relativetoreferencepolicyπ∗ =[a a ],wehave
I T
E− ([a a a ])=0<p (5+r)=E− (π∗)
I C T H
J([a a a ])=11−(1−p)·15<1=J(π∗),
I C T
andthusbyDefinition4.3,thisisoverjustification.
C.5.FurtherExamplesSupplementingSection4.3
Inthissection,wepresentfurthermathematicalexamplessupplementingthoseinSection4.3. Wefoundmanyofthem
before finding the examples we discuss in the main paper, and show the same and additional conceptual features with
somewhatlesspolish. WeagainassumethatP isdeterministic.
O⃗
Example C.4. In the main paper, we have assumed a model where the human obeys Eq. (2) and showed that a naive
applicationofRLHFcanleadtosuboptimalpolicies,andthespecificfailuremodesofdeceptiveinflationandoverjustification.
Whatifthehumanmakesthechoicesinadifferentway?Specifically,assumethatallweknowisthatPR(⃗o≻⃗o′)+PR(⃗o′ ≻
⃗o) = 1. CanthehumangenerallychoosethesechoiceprobabilitiesinsuchawaythatRLHFisincentivizedtoinfera
rewardfunctionwhoseoptimalpoliciesarealsooptimalforR? Theanswerisno.
Takethefollowingexample:
s
a b c
Inthisexample,thereisafixedstartstatesandthreeactionsa,b,cthatalsoserveasthefinalstates. Thetimehorizon
is T = 1, so the only state sequences are sa,sb,sc. Assume T(a | s,a) = 1, T(b | s,b) = 1, T(c | s,c) = 1−ϵ,
T(a | s,c) = ϵ, i.e., selectingactioncsometimesleadstostate a. Also, assumea = O(a) ̸= O(b) = O(c) =: oand
R(a)=R(b)<R(c).
Sincebandchavethesameobservationo,thehumanchoiceprobabilitiesdonotmakeadifferencebetweenthem,andso
RLHFisincentivizedtoinferarewardfunctionR˜ withR˜(b) = R˜(c) =: R˜(o). IfR˜(o) > R˜(a),thenthepolicyoptimal
underR˜willproduceactionbsincethisdeterministicallyleadstoobservationo,whereascdoesnot. IfR˜(o)<R˜(a),then
thepolicyoptimalunderR˜willproduceactiona. Inbothcases,theresultingpolicyissuboptimalcomparedtoπ∗,which
deterministicallychoosesactionc.
Inthecomingexamples,itwillalsobeusefultolookatthemisleadingnessofstatesequences:
DefinitionC.5(Misleadingness). Let⃗s∈S⃗beastatesequence. Thenitsmisleadingnessisdefinedby
M(⃗s):=G (⃗s)−G(⃗s)= E
(cid:2) G(⃗s′)−G(s)(cid:3)
.
obs
⃗s′∼B(⃗s′|O⃗(⃗s))
We call a state sequence positively misleading if M(⃗s) > 0, which means the sequence appears better than it is, and
negativelymisleadingifM(⃗s)<0. ThemisleadingnessvectorisgivenbyM∈RS⃗.
NotethatthemisleadingnessisrelatedtoE+andE−,asdefinedinDefinition4.2: IfM(⃗s)>0thenM(⃗s)=E+(⃗s),and
ifM(⃗s)<0thenM(⃗s)=−E−(⃗s).
ExampleC.6. Inthisexample,weassumethehumanisaBayesianreasonerasinSectionB.1. ConsidertheMDPthatis
suggestivelydepictedasfollows:
a b
c
The MDP has states S = {a,b,c} and actions A = {b,c}. The transition kernel is given by T(c | a,c) = 1 and
T(b | a,b) = 1, meaning that the action determines whether to transition from a to b or c. All other transitions are
43ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
deterministicanddonotdependontheaction,asdepicted. WeassumeaninitialstatedistributionP overstateswith
0
probabilitiesp =P (a),p =P (b),p =P (c). ThetruerewardfunctionR∈R{a,b,c} anddiscountfactorγ ∈[0,1)
a 0 b 0 c 0
are,fornow,keptarbitrary. ThetimehorizonisT =2,meaningwehavefourpossiblestatesequencesacc,abc,bcc,ccc.
Furthermore,assumethato:=O(a)=O(b)̸=O(c)=c,i.e.,cisobservedandaandbareambiguous.
Finally,assumethatthehumanhasapolicypriorB(λ),whereλ=π (c|a)isthelikelihoodthatthepolicychoosesaction
λ
cwheninstatea,whichisaparameterthatdeterminestheentirepolicy.
Weclaimthefollowing:
1. Ifp ̸=γ·E [λ]·p ,thenkerB∩imΓ={0},sothereisnoreturnfunctionambiguityunderappropriately
b λ∼B(λ) a
modeledpartiallyobservableRLHF,seeCorollaryB.4.
2. TherearetruerewardfunctionsRforwhichoptimizingJ leadstoasuboptimalpolicyaccordingtothetruepolicy
obs
evaluationfunctionJ,acaseofmisalignment. Thus,anaiveapplicationofRLHFunderpartialobservabilityfails,see
Section4.1.
3. Thefailuremodesarerelatedtohidingnegativeinformation(deception)andpurposefullyrevealinginformationwhile
incuringaloss(overjustifyingbehavior).
Proof. Writep:=B(bcc|occ),thehuman’sposteriorprobabilityofstatesequencebccforobservationsequenceocc. We
have1−p=B(acc|occ).
ConsiderthelinearoperatorsΓ : R{a,b,c} → R{abc,bcc,ccc,acc} andB : R{abc,bcc,ccc,acc} → R{ooc,occ,ccc} definedinthe
mainpaper. Whenorderingthestates,statesequences,andobservationsequencesaswejustwrotedown,weobtain
 1 γ γ2   1 0 0 0   1 γ γ2 
0 1 γ+γ2 
Γ=
0 0
1+γ+γ2 , B=0
0
p
0
10 1−
0
p, B◦Γ=1−
0
p p
0
1+γ+ γ+γ2 γ2.
1 0 γ+γ2
ByCorollaryB.4,ifB◦Γisinjective,thenthereisnorewardfunctionambiguity. Clearly,thisisthecaseifandonlyif
p̸=γ·(1−p). FromBayesrule,wehave
B(bcc) B(acc)
p= , 1−p= .
B(acc)+B(bcc) B(acc)+B(bcc)
Sotheconditionforinjectivityholdsifandonlyif
B(bcc)̸=γ·B(acc).
Now,notice
(cid:90) (cid:90)
B(bcc)= B(λ)·B(bcc|λ)dλ= B(λ)·p dλ=p
b b
λ λ
and
(cid:90) (cid:90)
(cid:2) (cid:3)
B(acc)= B(λ)B(acc|λ)dλ= B(λ)·p ·λdλ=p · E λ .
a a
λ λ λ∼B(λ)
Thisshowsthefirstresult.
Forthesecondstatement,weexplicitlycomputeJ uptoanaffinetransformation,whichdoesnotchangethepolicy
obs
ordering. LetRbethetruerewardfunction,G=Γ(R)thecorrespondingreturnfunction,andB(G)theresultingreturn
functionatthelevelofobservations. Forsimplicity,assumeR(c)=0,whichcanalwaysbeachievedbyaddingaconstant.
Wehave:
J (λ)= E
(cid:104) B(G)(cid:0) O⃗(⃗s)(cid:1)(cid:105)
obs
⃗s∼Pλ(⃗s)
=Pλ(abc)·B(G)(ooc)+Pλ(bcc)·B(G)(occ)+Pλ(ccc)·B(G)(ccc)+Pλ(acc)·B(G)(occ)
44ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
=p ·(1−λ)·G(abc)+p ·B(G)(occ)+p ·G(ccc)+p ·λ·B(G)(occ)
a b c a
(cid:104) (cid:105)
∝λ· B(G)(occ)−G(abc) .
Wehave
G(abc)=R(a)+γR(b), B(G)(occ)=(1−p)·G(acc)+p·G(bcc)=(1−p)·R(a)+p·R(b).
Thus,theconditionB(G)(occ)>G(abc)isequivalentto
p−γ
R(a)< ·R(b).
p
Thus,wehave
(cid:40)
1, ifR(a)< p−γ ·R(b),
argmaxJ (λ)= p
obs
0, else.
λ∈[0,1]
NowconsiderthecaseR(b)>0. Inthiscase,λ=0givesrisetotheoptimalpolicyaccordingtoGsincegoingtobgives
extrarewardthatonemisseswhengoingtocdirectly. However,whenR(a)≪0,thenJ selectsforλ=1. Intuitively,
obs
thepolicytriesto“hidethattheepisodestartedina”bygoingdirectlytoc,whichleadstoambiguitybetweenaccandbcc.
ThisisacaseofdeceptiveinflatingasinTheorem4.5.
Now,considerthecaseR(b) < 0. Inthiscase,λ = 1givesrisetotheoptimalpolicyaccordingtoG. However,when
R(a)≫0,thenJ selectsforλ=0. Intuitively,thepolicytriesto“revealthattheepisodestartedwitha”bygoingtob,
obs
whichispositiveinformationtothehuman,butnegativefromtheperspectiveofoptimizingG. AsinTheorem4.5,wesee
thatthisisacaseofoverjustification.
ExampleC.7. Inthisexample,weconsideranMDPthat’ssimilartoamulti-armedbanditwithfourstates/actionsa,b,c,d
andobservationkernelO(a)=O(b)̸=O(c)=O(d). Formally,wecanimaginethatitisgivenbytheMDP
s
a b c d
withR(s)=0andatime-horizonofT =1.Inthisexample,werevealthatmisleadingnessandnon-optimality(accordingto
thetruerewardR,orJ)areinprincipleorthogonalconcepts.Weconsiderthefollowingfourexamplecases.Ineachone,we
varysomeenvironmentparametersandthendeterminea∗ ,theactionthatresultsfromoptimizingJ (correspondingtoa
obs obs
naiveapplicationofRLHFunderpartialobservability,seeSection4.1),itsmisleadingnessM(a∗ )(seeDefinitionC.5),and
obs
theactiona∗thatwouldresultfromoptimizingJ. Ifa∗ =a∗,thenJ selectsfortheoptimalaction. Forsimplicity,we
obs obs
canimaginethatthehumanhasauniformprioroverwhatactionresultseventually(outoftheactiontakenandpotentially
adeviationdefinedbyϵ,seebelow)istakenbeforemakinganobservation,i.e. B(a)=B(b)=B(c)=B(d)= 1.
4
(a) AssumeR(a)>R(c)>R(d)≫R(b). Alsoassumethatactiondleadswithprobabilityϵ>0tostateb,whereasall
otheractionsleaddeterministicallytothespecifiedstate. Thena∗ =c,M(c)<0anda∗ =a.
obs
(b) AssumeR(d)>R(a)>R(c)≫R(b). Again,assumethereisasmallprobabilityϵ>0thatactiondleadstostateb.
Thena∗ =c,M(c)>0,anda∗ =dora∗ =a,dependingonthesizeofϵ.
obs
(c) AssumeR(a)>R(b)>R(c)>R(d). Additionally,assumethatthereisalargeprobabilityϵ>0thatactionaleads
tostated,whereasallotheractionsleadtowhat’sspecified. Ifϵislargeenough,thena∗ =b. Additionally,wehave
a∗ =bandM(b)>0.
obs
(d) AssumeR(a)>R(b)>R(c)>R(d). Also,assumesomeprobabilityϵ>0thatactionbleadstostated,whereasall
otheractionsleaddeterministicallytowhat’sspecified. Thena∗ =a,M(a)<0,anda∗ =a.
obs
Overall,wenotice:
45ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
• Example(a)showsahighregretandnegativemisleadingnessofa∗ =c. Theactionisbetterthenitseems,butaction
obs
awouldbebetterstillbutcannotbeselectedbecauseitcanbeconfusedwiththeverybadactionb.
• Example(b)showsahighregretandhighmisleadingnessofa∗ =c. Theactionisworsethanitseemsandalsonot
obs
optimal.
• Example(c)showszeroregretandhighmisleadingnessofa∗ =b. Theactionisworsethanitseemsbecauseitcan
obs
beconfusedwitha,butitisstilltheoptimalactionbecauseacanturnintod.
• Example(d)showszeroregretnegativemisleadingnessofa∗ =a. Theactionischoseneventhoughitseemsworse
obs
thanitis,andisalsooptimal.
Thus,weshowedallcombinationsofregretandmisleadingnessoftheactionoptimizedforunderJ .
obs
Wecanalsonoticethefollowing: Examples(a)and(b)onlydifferintheplacementofR(d). Inparticular,thereasonthat
a∗ =cisstructurallythesameinboth,butthemisleadingnesschanges. Thisindicatesthatmisleadingnessisnotonits
obs
owncontributingtowhatJ optimizesfor.
obs
Thefollowingisthesmallestexamplewefoundwiththefollowingproperties:
• Thereisauniquestartstateandterminalstate.
• AnaiveapplicationofRLHFfailsinawaythatshowsdeceptionandoverjustification.
• Modelingpartialobservabilityresolvestheproblems.
ExampleC.8. Considerthefollowinggraph:
A
S C T
B
This depicts an MDP with start state S, terminal state T and possible state sequences
STTT,SATT,SACT,SCTT,SBCT,SBTT and no discount, i.e. γ = 1. Assume that S,B,C are observed,
i.e. O(S) = S, O(B) = B, O(C) = C, and that A and T are ambiguous: O(A) = O(T) = X. Then there are five
observationsequencesSXXX,SXCX,SCXX,SBCX,SBXX. Assumethatthehumancanidentifyallobservation
sequencesexceptSXXX,withbeliefb=B(STTT |SXXX)and1−b=B(SATT |SXXX).
Thenthereturnfunctionisidentifiableundertheseconditionswhenthehuman’sbeliefiscorrectlymodeled. However,for
somechoicesofthetruerewardfunctionRandtransitiondynamicsofthisMDP,wecanobtaindeceptiveoroverjustified
behaviorforanaiveapplicationofRLHF.
Proof. WeapplyCorollaryB.4. Weorderstates,statesequences,andobservationsequencesasfollows:
S =S,A,B,C,T,
S⃗ =STTT,SATT,SACT,SCTT,SBCT,SBTT,
Ω⃗ =SXXX,SXCX,SCXX,SBCX,SBXX.
46ChallengeswithPartialObservabilityofHumanEvaluatorsinRewardLearning
Ascaneasilybeverified,withthisorderingthematricesB∈RΩ⃗×S⃗ andΓ∈RS⃗×S
aregivenby:
 
  1 0 0 0 3
b 1−b 0 0 0 0
1 1 0 0 2
0 0 1 0 0 0  
  1 1 0 1 1
B=0 0 0 1 0 0, Γ= .
  1 0 0 1 2
0 0 0 0 1 0  
1 0 1 1 1
0 0 0 0 0 1
1 0 1 0 2
Toshowidentifiability,weneedtoshowthatkerB∩imΓ={0}. Clearly,thekernelofBisgivenbyallreturnfunctions
inRS⃗ thataremultiplesofG′ =(b−1,b,0,0,0,0). AssumeG′ ∈imΓ,meaningthereisarewardfunctionR′ ∈RS⃗ with
Γ·R′ =G′. Weneedtodeducefromthisacontradiction. Theassumptionmeansweobtainthefollowingequations:
(i) R′(S)+3R′(T)=b−1,
(ii) R′(S)+R′(A)+2R′(T)=b,
(iii) R′(S)+R′(A)+R′(C)+R′(T)=0,
(iv) R′(S)+R′(C)+2R′(T)=0,
(v) R′(S)+R′(B)+R′(C)+R′(T)=0
(vi) R′(S)+R′(B)+2R′(T)=0
(iii) and (v) together imply R′(A) = R′(B); (iv) and (vi) together imply R′(B) = R′(C); (v) and (vi) together imply
R′(C)=R′(T);sotogether,wehaveR′(A)=R′(T). Thus,replacingR′(A)in(ii)byR′(T)andcomparing(i)and(ii),
weobtainb−1=b,acontradiction. Overall,thisshowskerB∩imΓ={0},andthusidentifiabilityofthereturnfunction
byCorollaryB.4.
Nowweinvestigatethecaseofunmodeledpartialobservability.
Fordemonstratingoverjustification,assumedeterministictransitiondynamicsinwhicheveryarrowinthediagramcan
bechosenbythepolicy. Also,assumeR(A) ≪ 0,R(T) > 0,R(S) = 0,R(B) = 0,andR(C) = 0. Thentheoptimal
policy chooses the state sequence STTT. However, this trajectory has low observation value since G (STTT) =
obs
(B·G)(SXXX)=bG(STTT)+(1−b)G(SATT),whichislowsinceR(A)≪0. J thenselectsforthesuboptimal
obs
policieschoosingSBTT orSCTT,whichisoverjustifiedbehaviorthatmakessurethatthehumandoesnotthinkstateA
wasaccessed.
For demonstrating deception, assume that R(A) ≫ 0, R(T) < 0, R(S) = R(B) = R(C) = 0 and that the transition
dynamicsaresuchthatwhenthepolicyattemptstotransitionfromS toA,itwillsometimestransitiontoB,withallother
transitionsdeterministic. Inthiscase,theoptimalbehaviorattemptstoenterstateAsincethishasveryhighvalue. J ,
obs
however,willselectforthepolicythatchoosesSTTT. Thisisdeceptivebehavior.
47