Diffusion Meets DAgger:
Supercharging Eye-in-hand Imitation Learning
Xiaoyu Zhang∗ Matthew Chang∗ Pranav Kumar Saurabh Gupta
∗project co-leads
University of Illinois at Urbana-Champaign
{zhang401,mc48,pranav9,saurabhg}@illinois.edu
https://sites.google.com/view/diffusion-meets-dagger
Abstract—A common failure mode for policies trained with Expert : More expert data : Synthesized data
imitation is compounding execution errors at test time. When demonstration Learner’s
trajectory
the learned policy encounters states that were not present in a˜t
theexpertdemonstrations,thepolicyfails,leadingtodegenerate I˜
behavior. The Dataset Aggregation, or DAgger approach to this Get expert I t Δp t
problem simply collects more data to cover these failure states. actions on
However,inpractice,thisisoftenprohibitivelyexpensive.Inthis learner’s
state
work, we propose Diffusion Meets DAgger (DMD), a method
to reap the benefits of DAgger without the cost for eye-in-hand a) Compounding b) Current Practice c) Proposed Approach (Out-of-
imitation learning problems. Instead of collecting new samples Execution Error Problem (Dataset Aggregation) distribution data and label synthesis)
to cover out-of-distribution states, DMD uses recent advances
I I˜ in diffusion models to create these samples. This leads to robust t t
performancefromfewdemonstrations.Inexperimentsconducted
fornon-prehensilepushingonaFrankaResearch3,weshowthat
DMD can achieve a success rate of 80% with as few as 8 expert
demonstrations,wherebehaviorcloningreachesonly20%.DMD
also outperform competing NeRF-based augmentation schemes
by 50%.
d) Perturbed view (right) synthesized from the reference image (left)
I. INTRODUCTION
Imitation learning is an effective way to train robots. How- Fig.1:Eye-in-handImitationlearningwithDMD:Acommonfailuremode
ever, even when testing policies in environments similar to inanimitationlearningsettingistheproblemofpoorgeneralizationdueto
compoundingexecutionerrorsattesttimeasshownin(a).Thiscanbesolved
those used for training, imitation learning suffers from the
by collecting more expert data to cover these off-trajectory states as shown
well-known Compounding Execution Errors problem: small in (b) however, this is an expensive process. Our proposed approach is to
errors made by the learned policy lead the robot to out-of- synthesize data instead of collecting it (c). Magenta arrow represents small
distribution states causing the learner to make even bigger
perturbation(∆p)tothetrajectory.Cyanarrowrepresentslabel(a˜t)forthis
out-of-distribution observation. We use a state-of-the-art diffusion model to
errors (compounding errors) [56]. takeimagesItfromexpertdemonstrations(d,left)andgeneraterealisticoff-
One solution to this problem is via manual collection of trajectory images I˜ t (d, right). Note the distance between the grabber and
the apple denoted by the green line. This synthetic data augments expert
expert-labeled data on states visited by the learner, a strategy
demonstrationsforpolicylearning,leadingtomorerobustpolicies.
popularly known as Dataset Aggregation or DAgger [56]. In
theory, this solution works. However, putting it into prac-
tice is challenging from a practical stand point: driving a
robot that doesn’t obey your commands is difficult. Many In more detail: given a behavior cloning trajectory, τ
alternatives have been proposed, e.g. [35, 36], but they all collected from an expert operating the system, we design
requirecollectingmoreexpertdata.Inthispaper,wepursuean generative models to synthesize off-distribution states and
alternateparadigm:automaticallygeneratingobservationsand corresponding action labels. For example, for the object
action labels for out-of-distribution states. By replacing data pushing application in Figure 1, this would correspond to
collectionwithdatacreation,weimprovethesampleefficiency generatingoff-centerviewsoftheobjectfromthegoodexpert
of imitation learning. In fact, data creation was Pomerleau’s trajectory τ. Specifically, we learn a function f(I ,∆p) that
t
originalsolutiontothisproblem[51].Werevisit,improve,and synthesizestheobservationI˜ atasmallperturbation∆ptothe
t
automate his solution from 30 years ago using modern data- trajectoryattimestept.Knowledgeofthis∆pletsuscompute
driven image generation methods. We do this for imitation the ground truth action label a˜ for this out-of-distribution
t
learningproblemsincontextofeye-in-handsetups(i.e.setups observation. We augment the expert demonstration τ with
whereimagescomefromacameramountedontherobothand) multiple off-distribution samples (I˜,a˜ ) along the trajectory.
t t
that are increasingly becoming more popular [64, 77, 84]. Access to out-of-distribution states and action labels for how
1
4202
beF
72
]OR.sc[
1v86771.2042:viXrato recover from them improves the performance of the task conduct these augmentation in low dimensional state spaces.
policy. Ke et al. [32] leverage local continuity in the environment
In this paper, we develop and evaluate this idea in context dynamics to generate corrective labels for low-dimensional
of eye-in-hand manipulation policies in the real world. While state spaces and LiDAR observations in simulation. Closest
the idea is intuitive, a number of nuances have to be dealt to our work, Zhou et al. [85]’s SPARTN model train NeRF’s
with carefully to get it to work. As we have access to the on demonstration data to synthesize out-of-distribution views.
entiretrajectory,thenaturalchoiceistoadoptNeRFs[48,85] NeRF assumes a static scene. Thus, SPARTN can’t reliably
to realize the function f. While NeRFs work very well for synthesizeimageswhenthesceneundergoesdeformationupon
view synthesis in computer vision, we found that they don’t interaction of the robot with the environment. Our use of
work well for the synthesis task at hand because of the diffusion models to synthesize images gets around this issue
inevitabledeformationsinthesceneasthegrippermanipulates and experimental comparison to SPARTN demonstrate the
the objects. We thus switched to using diffusion models for effectiveness of our design choice.
this synthesis task. This led to good image synthesis even in
B. Image and View Synthesis Models
situationswherethescenedeformedoverthecourseofmanip-
ulation. Computing action labels for these samples presented Recentyearshavewitnessedlargeprogressinimagegener-
yet another challenge (Figure 6). The labels should align ation[18,24,54,63,65].Thishasledtoanumberofapplica-
with making progress towards the goal. We thus investigated tions:text-conditionedimagegeneration[5,20,52,54,57,82],
different schemes for sampling ∆p’s and computing action image and video inpainting [9, 44, 47], image to image
labels a˜ . translation [27] and generation of novel views for objects and
t
Thispaperpresentsexperimentsthatevaluatetheaforemen- scenes from one or a few images [34, 42, 69, 72, 80]. While
tioned design choices to develop a data creation framework to diffusion models have proven effective at image synthesis
supercharge eye-in-hand imitation learning. Experiments on a tasks, Neural Radiance Fields (NeRFs) [48] excel at view
physical platform demonstrate the effectiveness of our pro- interpolation tasks. Given multiple (20 – 100) images of a
posed framework over baseline data augmentation techniques. static scene, vanilla NeRFs can effectively interpolate among
Onanon-prehensilepushingonaFrankaResearch3,weshow the views to generate photo-realistic renders. Researchers
that DMD can achieve a success rate of 80% with as few as have pursued NeRF extensions that enable view synthesis
8 expert demonstrations, where behavior cloning reaches only from few images [7, 19, 75, 79] and even model deforming
20%. We also outperform a recently proposed augmentation scenes [8, 66]. In general, NeRFs are more effective at inter-
scheme based on NeRFs [85] by 50%. polation problems, while diffusion models excel at problems
involving extrapolation (e.g. synthesizing content not seen at
II. RELATEDWORK
all) or where exactly modeling changes in 3D is hard (e.g.
A. Imitation Learning deforming scenes). As our work involves speculating how the
Behavior cloning (BC), or training models to mimic expert scenewouldlookifthegripperwereatadifferentlocation,we
behavior, has been a popular strategy to train robots over adoptadiffusionbasedmethods.Weshowcomparisonagainst
the last many decades [50, 51, 58] and has seen a recent a NeRF based method as well.
resurgence in interest [28, 86]. Ross et al. [56] theoretically
C. Diffusion Models and Neural Fields for Robot Learning
andempiricallydemonstratethecompoundingexecutionerror
problem (small errors in the learner’s predictions steer the Effective generative models and neural field based repre-
agent into out-of-distribution states causing the learner to sentations have been used in robotics in other ways than for
make even bigger errors) and propose a Dataset Aggregation dataaugmentation.Researchershaveuseddiffusionmodelsfor
(DAgger) strategy to mitigate it. Over the years, researchers representing policies [1, 13, 37, 38, 71], generating goals or
have sought to improve the basic BC and DAgger recipe in subgoals[6,10,30],offlinereinforcementlearning[3,43,71],
several ways [4, 25]. [17, 62, 64, 77, 84] devise ways to ease planning [29, 40, 41, 73], behaviorally diverse policy genera-
and scale-up expert data collection, while [83] design ways tion [23], and predicting affordances [76]. Additionally, they
to collect demonstrations in virtual reality. [35, 36] devise havebeenusedtogeneratesampleswithhighreward[26]and
ways to simplify DAgger data collection. [14, 61] model the skill aquisition from task or play data [11]. Neural fields have
multiple modes in expert demonstrations, while [49] employ been used to represent scenes for manipulation [39, 70] and
a non-parameteric approach. Researchers have also shown the navigation [2].
effectivenessofpre-trainedrepresentationsforimitationlearn-
III. APPROACH
ing[49,74].[12,46,81,85]employimageandviewsynthesis
A. Overview
models to synthetically augment data. Complementary to our
work, [12, 46, 81] focus on generalization across objects via Given task data D, imitation learning learns a task policy
semantic data augmentation by adding, deleting and editing π. The task data takes the form of a collection of trajectories
objects in the scene. τ , where each trajectory in turn is a collection of image
i
[16, 31, 32, 85] pursue synthetic data augmentation to action pairs, (I ,a ). π is trained using supervised learning
t t
address the compounding execution error problem. [16, 31] to regress action a from images I over the task data D. For
t t
2a) Diffusion Model Training b) Augmented Dataset D˜ Generation D˜ Label Generation
Play Data Task Data D Diffusion Off-Center I 4 I 3
Task Data
D Model Views
Label
Generation
Relative Transform
I
a aT
b
I˜2
c) Policy Training 2
I
2
Diffusion Task Data Augmented BC Policy I˜ 21
D
Model Dataset D˜
I
1
I
b
Fig. 2: DMD System Overview: Our system operates in three stages. a) A diffusion model is trained, using task and play data, that can synthesize novel
viewsofascenerelativetoagivenimage.b)Thisdiffusionmodelisusedtogenerateanaugmentingdatasetthatcontainsoff-trajectoryviews(I˜1,I˜2)from
2 2
expert demonstrations. Labels for these views (cyan arrows) are constructed such that off-trajectory views will still converge towards task success (right).
Images with a green border are from trajectories in the original task dataset. Purple-outlined images are diffusion-generated augmenting samples. c) The
originaltaskdataandaugmentingdatasetarecombinedforpolicylearning.
Pose T Condit Tioning and detail the augmenting image sampling and action labels
a b,b a
computation procedure in Section III-C.
I Featuresa B. Diffusion Model for Synthesizing ∆p Perturbed Views
a xa
Conv AttS ee nl tf i on The function f(I,∆p) is realized using a conditional dif-
VQ Cross … ϵ̂ fusion model that is pretrained on large Internet-scale data.
Encoder Attention
I
b
+
x tb
Conv AttS ee nl tf i on
Featuresb S fip ne ec tuifi nc eal il ty, ow ne da ad taop ft roth me r Dec .e Tnt hewo mrk odfr eo lm prY ou due ct ea sl. im[8 a0 g] ean Id
b
by conditioning on a reference image I taken by camera a
a
and a transformation matrix T that maps points in camera
a b
Noise ϵ b’s frame to camera a’s frame. By representing the desired
U-Net Block
∆p as the transformation between two cameras ( T = ∆p),
a b
Fig. 3: DMD Architecture: We use the architecture introduced in [80], a
and using images from D as the reference images (I ∼D),
U-Net diffusion model with blocks composed of convolution, self-attention, a
and cross attention layers. The conditioning image Ia, and noised target thelearneddiffusionmodelcangeneratethedesiredperturbed
imageI b areprocessedinparallelexceptatcross-attentionlayers.Thepose views for augmenting policy learning.
conditioninginformationisinjectedatcross-attentionlayers. 1) Model Architecture: A diffusion model can be thought
of as an iterative denoiser: given a noisy image x , diffusion
t
timestep t, it is trained to predict the noise ϵ added to the
the non-prehensile manipulation of objects using an eye-in-
image.
hand cameras considered in this paper, images I correspond
t L=∥ϵ−ϵ (x ,t)∥2
toviewsfromtheeye-in-handcameraasinput,andtheactions θ t 2
a aretherelativeend-effectorposesbetweenconsecutivetime It is typically realized via a U-Net [55]. Our application
t
steps. requires conditional image generation where the conditions
When trained with few demonstrations, π exhibits poor are source image I and transformation T . Thus, by using
a a b
generalization performance. To address poor generalization, I as the diffusion target, and conditioning on I and T ,
b a a b
our approach generates an augmented dataset D˜ and trains the model ϵ learns to denoise new views of the scene in I
θ a
the policy jointly on D˜ ∪D. Augmented samples are specif- based on the transformation T . Finally, rather than directly
a b
ically generated to be out-of-distribution from D and thus denoising in the high-resolution pixel space, the denoising is
aid the policy’s generalization capabilities. D˜ is generated doneinthelatentspaceofaVQ-GANautoencoder,E[15,53].
through a conditional diffusion model f that generates a ∆p This gives the final training objective of:
perturbed view I˜ of an image I from a given trajectory τ:
t L=||ϵ−ϵ (xb,E(I ), T ,t)|| where xb =E(I ).
I˜=f(I ,∆p). We use the action labels in the trajectory τ to θ t a a b 0 b
t
computetheactionlabela˜forthisperturbedview.Wedescribe Following [80], the pose conditioning information T is in-
a b
the design of the conditional diffusion model in Section III-B jectedintotheU-Netviacross-attentionasshowninFigure3.
3I I˜ Move Left I˜ Move Right I˜ Move Forward I˜ Move Backward
t t t t t
Fig. 4: Visualization of perturbed images generated by diffusion model for augmenting policy learning. In each row, we show I˜ t generated from It
throughdifferentcameratranslations.
2) Model Training: Training the model requires access to C. Generating Out-of-Distribution Images and Labels
triples: (I ,I , T ). We process data from D to generate
a b a b 1) Sampling Out-of-distribution Images: We generate out
thesetriples.LeteachtrajectoryconsistofimagesI ,...,I .
1 N of distribution views for images in the task dataset D. ∆p’s
We use the structure from motion [21, 68] implementation
aresampledasfollows.ForanimageI fromtrajectoryτ,we
from the COLMAP library [59, 60] to extract poses for the t
sample vectors in a random direction, with magnitude drawn
images in the trajectory τ. This associates each image I in
t uniformly from [0.2s,s], where s is the largest displacement
the trajectory with the (arbitrary) world frame T . We can
t w between adjacent frames in τ. ∆p simply corresponds to
thencomputerelativeposebetweenarbitraryimagesI andI
a b translation along this randomly chosen vector. We constrain
from a trajectory via T = T T . We sample random pair
a b a ww b these samples to not be too aligned with the actual action.
of images from the trajectory to produce triples (I ,I , T )
a b a b 2) Labeling Generated Images: For each image I˜ =
that are used to train the model ϵ . COLMAP also outputs t
θ f(I ,∆p) generated from an original image I , we use I
the camera intrinsics necessary for training the model. Since t t t+k
as the target for generating the action label. The action label
COLMAP reconstruction is only correct up to a scale factor,
for I˜ is simply the action that would convey the agent from
wenormalizetranslationsbythelargestdisplacementbetween t
the pose depicted in I˜ to the pose in I .
adjacent frames per trajectory. t t+k
FromCOLMAP,weobtaincameraposeinformation T for
t w
As D contains much less data than is typically required to timesteptand T fortimestept+k.Thesynthesizedview
t+k w
train a diffusion models from scratch, we finetune the model I˜ canbetreatedasanimagetakenbyavirtualcamerawhose
t
from Yu et al. [80]. We find that finetuning with as few as 24 pose is represented as tT t˜. The diffusion model synthesizes
trajectories leads to realistic novel view synthesis for our task perturbed view conditioned on I
t
and tT t˜, or I˜
t
=f(I t, tT t˜)
as shown in Figure 4. We only finetune the diffusion model (Section III-B). The action label a˜ for I˜ is then computed as
t t
weights and use the pre-trained VQ-GAN codebooks. We a˜
t
= t˜T
t+k
= t˜T ttT w( t+kT w)−1. Examples of (I˜ t,a˜ t) are
experimentwithdifferentdatasetsforfinetuningthisdiffusion shown in Figure 5.
model: task data from D, play data [45, 78] collected in our One might wonder why we don’t just use k = 1? As
setup, and their combinations. the scale of each trajectory is different, the diffusion model
4Fig.5:TrainingExamplesfromtheDiffusionModel:Wevisualizegeneratedexamples,I˜,usedtotrainourpoliciesalongwiththecomputedactionlabel
(thearrowisaprojectionofthe3Dactionintothe2Dimageplane:thearrowpointingupmeansmovethegripperforward,pointingtotherightmeansmove
itright).Weshowaugmentationsgeneratedbyourdiffusionmodelforatdifferentstagesofthetask.Thefirstrowshowsaugmentationsfromimageswhere
thegrabberapproachestheapple,whilethesecondrowshowsaugmentationsfromimageswherethegrabbermovestheappletowardsthegoallocation.
Action
w.r.t I
Action t+3
w.r.t I
t+1
a) I b) I˜ c) I d) I
t t t +1 t +3
Fig.6:Theovershootingproblem:Undercertainconditions,theinferredactionforI˜ t maydirecttheagentawayfromtasksuccess.Werefertothisasthe
overshooting problem. At time step t+1, the view It+1 in (c) has moved to the right from It in (a). However, the synthesized sample I˜(b) has moved
evenfurthertotherightthanIt+1.In(b),CyanarrowrepresentsactionlabelforI˜ t computedusingIt+1 asthetarget;greenarrowrepresentsactionlabel
computed using It+3 as the target. Since I˜ t has “overshot” It+1, an action taken with It+1 as the next intended target moves backward, away from the
goal.Computingtheactionwithrespecttoafartherimage,sayIt+3 in(d),doesnothavethisissue.
learns the distribution of scales over D. Depending on the robot end-effector as shown in Figure 7. This allows for easy
scaleoftheCOLMAPreconstruction,thiscausesthegenerated collection of expert demonstrations using a GoPro mounted
samples to move more than 1 unit for some trajectories. on a Grabber. Due to the difference in joint limits between
When using k = 1, some generated samples move past the the XArm (used by VIME) and the Franka Research 3 robot,
target image I (as described in Figure 6). This causes weredesigntheattachmentbetweenthegrabberandtherobot
t+1
conflictingsupervision,asthecomputedactiondoesnotmake flange. In this way, the robot can execute planar push in the
progress toward completing the task. Using a large k guards typical top-down configuration, extending the workspace to a
against this, leading to stronger performance downstream, as widerrangeofthetable.ThecomparisonisshowninFigure7.
our experiments verify (Section IV-C2). Observations come from the eye-in-hand GoPro camera. We
use the action space used in VIME’s [77] publicly available
IV. EXPERIMENTS code: relative 3D translations of the end-effector.
A. Experiment Setup
2) Task and Play Data: We collect task and play data
1) Task,ObservationSpace,ActionSpace: Weusethenon- trajectories using a GoPro mounted on a Grabber. For task
prehensilepushingtaskfromVIME[77]asabenchmarktask. data, an expert pushes the object to a target location. For play
Thetaskistopushthetargetobjecttoatargetlocationmarked data, we move the grabber around the apple and push it in
by a red circle and requires both reaching and pushing the many different directions. Through this procedure, we only
object. Following Young et al. [77], we use a grabber as the get image sequences. We use structure from motion [21, 68]
5left / right and front / back with minimal artifacts.
Wealsocomparethevisualqualityofourgeneratedimages
to those generated using the NeRF based approach from
SPARTN [85]. The moving gripper in the scene breaks the
GoPro Hero 9 Old VIME Robot
static scene assumption made by NeRF. Thus, [85] mask out
Configuration
the gripper before training the NeRF. While this works for
the pre-grasp trajectory that Zhou et al. [85] seek to imitate,
this strategy fails when the gripper actually manipulates the
scene, as in our task. Thus, we investigate different masking
schemes:a)nomasking,b)maskingalargerregionaroundthe
gripper, c) masking just the gripper; and show visualizations
inFigure8.Nomaskingleadstotheworstresultsasexpected.
Masking just the grabber fixes the grabber, but causes the
object being manipulated to blur out. Masking a larger region
Push Target around the grabber fixes the blurring but also eliminates the
object being manipulated. In contrast, images synthesized
usingourdiffusionmodelmovethecameraasdesiredwithout
Fig. 7: Robotic Platform for Online Testing:Weconductourexperiments creating any artifacts.
on a Franka Research 3 robot with a wrist-mounted GoPro Hero 9. In the
depictedconfiguration,thetaskistopushtheappleontotheredcircle.Since
C. Offline Validation
VIME’s[77]actionspaceisrelative3Dtranslations,wemodifieditsgrabber
mountontherobot,allowingtherobottomovewithoutreachingjointlimits
Wecollected74expertdemonstrationsintotalfortheapple
whilepreservingtheoriginalrotationoftheend-effector.
pushing task. We use 24, 23, and 27 trajectories for train,
validation, and test, respectively. We use vanilla behavior
to extract camera poses T for images in the collected cloning on the expert data as the baseline as done in past
t w
sequences. Action a is computed as the relative translation work [77]. Our offline validation involves evaluating the
t
between I and I in I ’s camera frame. Since COLMAP policy’s predictions on the test set. While the test set samples
t+1 t t
doesn’t contain examples of going off the distribution, it does
only gives reconstructions upto an unknown scale factor,
measuregeneralizationtosomeextentbyevaluatingthepolicy
following [77] we normalize the action to unit length and
on states outside of the training set. For the error metric, we
interpret it as just a direction. Since the final robot action
use the median angle between the predicted and the ground
space is 3D translation, we avoid rotating the grabber during
truth translation.
both type of data collection; however, slight rotation of the
grabber still exists in expert demonstrations. We conduct offline validation to compare with other data
3) Policy Architecture and Execution: We adopt the archi- augmentationschemesandselectthevalueofktomitigatethe
tecture for the task policy π from VIME [77] but replace the over-shooting problem. Online experiments in Section IV-D
AlexNet[33]backbonewithanImageNetpre-trainedResNet- test the success rate of the policy.
18backbone[22].ThepolicyconsistsoftheResNetbackbone
followed by 3 MLPs that predict the action. This task policy TABLEI:Comparisonsbetweendiffusion-generatedaugmenteddataset
and standard augmentation schemes.Thetableshowsmedianangleerror
accepts a center cropped image from the GoPro camera and (inradians)betweenpredictedandground-truthtranslations.AddingD˜ gen-
outputs the direction in which the camera should move. The eratedbydiffusionmodelsimproveperformanceontopofotheraugmentation
policy is trained by L1 regression to the (unit length) actions techniques.
usingtheAdamoptimizerwithalearningrateof1e-4.Actions
Methods BC DMD
areexecutedontherobotbycommandingtherobottogo1cm
NoAug 0.376 0.349
in the predicted direction.
w/Colorjitter 0.381 0.350
4) Comparisons: We use behavior cloning on the expert w/Flip 0.356 0.338
w/Colorjitter&Flip 0.355 0.328
data as done in past work [77], as the baseline. We refer to
this as BC. We also compare to SPARTN [85] and evaluate
the various design choices. Since there is no publicly avail- 1) Comparison with other data augmentation schemes:
able code from SPARTN, we replicate their procedure using We want to understand the effectiveness of augmenting with
NerfStudio [67]. diffusion-generatedimagescomparedtoaugmentingwithstan-
dardtechniques,suchascolorjitterandhorizontalflip.TableI
B. Visual Comparison of Generated Images
shows that augmentation, in general, improves performance,
Westartbyassessingthevisualqualityofsamplesgenerated andaugmentingwithout-of-distributionimagesleadstolarger
by the diffusion model in DMD. Figure 4 and Figure 5 shows improvementcomparedtostandardtechniques.Comparingthe
sampleimagesgeneratedbyourdiffusionmodel.Themodelis two columns, we see that adding diffusion-generated images
able to faithfully generate images where the camera is moved improve performance in all settings.
6a) I˜ NeRF No-Mask b) I˜ NeRF Box-Mask c) I˜ NeRF Grabber-Mask d) I˜ Diffusion Model
t t t t
Cannot generate
in-hand apple
I t Move C ma on vn io nt g g oe bn je er ca tt se No apple
Forward
Artifacts Grabber
needs to be
paste in
Move
Backward
Fig. 8: Diffusion vs NeRFHerewevisualizeperturbedsamplesgeneratedusingDMDandNeRFbaselineswithdifferentmaskingstrategies.Thetoprow
showsimagesgeneratedcorrespondingtoaforwardmovementrelativetoIt,andthebottomrowforabackwardmovement.WhenusingaNeRFwithno
masking (a), the reconstructions of the gripper and apple are degenerate, as they violate the static scene assumption in NeRFs. With a box-mask (b), the
appleisoccludedbythemask,andconsequently,missingfromthereconstruction.Evenwithamaskaroundtheend-effector(Grabber-Mask),therearemajor
artifactsinreconstructingtheapple(c).Withours(DiffusionModel),themodelfaithfullyreconstructsthegrabberandapple(d).
TABLEII:OnlineRobotExperimentResults.Weconduct5experiments:
0.37 10trialspermethodandrandomizetheapplestartlocationsbetweentrials.
0.36 Method Success Rate Method Success Rate
BC 30% SPARTN 50%
0.35
DMD 100% DMD 100%
0.34 a) DMD vs. Behavior Cloning b) DMD vs. SPARTN
0.33 Method 16 Demonstrations 8 Demonstrations
BC 50% 20%
0.32
1 2 3 4 5 DMD 90% 80%
k: Label Augmenting Images using I t+k c) DMD with Few Demonstrations
Fig. 9: Effect of using different future frames for labeling augmenting
images on translation prediction: to overcome the overshooting problem Method Success Rate Method Success Rate
described in Figure 6, we experiment with different future frame I t+k for Task 80% BC 80%
labelingthediffusionmodelgeneratedimages.Errordecreasesaskincreases
andplateausaroundk=3.Therefore,weuseframeIt+3 forlabelingI˜ t Task + Play 100% DMD 90%
d) Utility of Play Data for e) Scaling to Other Objects
Training Diffusion Model
2) Overshooting problem, what frame to use?: In order to
mitigate the overshooting problem, we use image I ,k >1
t+k
can be found on the project website.
to label the augmenting image instead of I . We evaluate
t+1
policies trained with labels produced from different k’s, and 1) DMDvs.BehaviorCloning: Wetakethebestmodelfor
showtheresultsinFigure9.WefindthatlabelingwithI ’s BC and DMD from Table I and evaluate them on the robot.
t+1
poseworksworsethanlabelingwithposeforI andbeyond. WefindthevanillaBCmodeltogeta30%successratewhile
t+3
As the curve plateaus beyond k =3 and using larger k leads DMD achieves 100% (Table IIa). This significant difference
to the exclusion of the last k−1 frames in each trajectory, we shows that minor error at each step can accumulate and result
use k =3 for our online experiments. in task failure.
2) DMDwithFewDemonstrations: Motivatedbythe100%
D. Online Validation
performanceofDMDwith24demonstrations,weaskhowlow
Ouronlineexperimentsmeasurethesuccessrateatthenon- canthenumberofdemonstrationsbe?Weconsider2lowdata
prehensile pushing task. We conduct 10 trials per method settings with 8 and 16 demonstrations. For these settings, the
and randomize the apple start location between trials. To diffusion model is trained with only play data. Success rate
prevent human biases, we choose the apple start location forthesetwomodelsontherobotisshowninTableIIc.Once
beforesamplingthemethodtorunnext.Executiontrajectories again DMD outperforms BC and achieves 80% success even
7
)snaidar(
rorrE
elgnAFig. 10: Comparison of different methods on staying on course.WeshowthetrajectoriesexecutedbyBC,SPARTN[85],andDMDoverseveralsteps
towardsthetarget.UnlikeBCandSPARTN,whichgraduallydeviatefromtheintendedpath,DMDmaintainitscoursetowardstherightandreachthetarget
successfully.Seeprojectwebsiteformoreexecutionvideos.
when trained with only 8 demonstrations. V. DISCUSSION
3) DMD vs. SPARTN [85]: Following up on the visual
DMDwasdesignedtoimprovethesampleefficiencyofeye-
comparison of the quality of the augmenting samples from
in-hand imitation learning. Compared to traditional behavior
Section IV-B, we conduct a head-to-head comparison against
cloning, DMD shows significant improvement in experiments
SPARTN on the real robot. SPARTN achieves a success rate
involving non-prehensile pushing across 2 objects. Remark-
of 50% vs. DMD succeeds 100% of the time (Table IIb). In
ably, DMD provides effective augmenting strategies using
Figure 10, we show that while SPARTN diverges from the
generatedimagesandenabledlearnedpoliciestoachieve80%
expert course and pushes the apple off the table, DMD brings
success rate from as few as 8 demonstrations. DMD not only
the apple to the target sucessfully.
performs better than conventional augmentation schemes used
4) Utility of Play Data for Training Diffusion Model: We in robotics but also shows improved metrics when combined
also experimentally validate what the choice of the dataset with them.
necessary for training an effective diffusion model for view
Our experiments revealed the benefits of the different parts
synthesis. We repeat the experiment with 24 demos but vary
of DMD over past approaches. The use of diffusion models
the diffusion model that generated the augmenting samples.
to synthesize images allows DMD to generate augmenting
Using the diffusion model trained on just task data lead to
images for manipulation tasks that involve non-static scene.
an 80% success rate vs. 100% success rate with the diffusion
Prior work [85] used NeRFs instead of diffusion models,
model trained on a combination of task and play data (Ta-
restricting generation to samples from pre-grasp part of the
ble IId). Thus, while just training the diffusion model on task
trajectories. We showcase the advantages of diffusion models
dataiseffective(aniceresultinpractice),performancecanbe
over NeRFs through qualitative results in Figure 8 and real
boosted further by training the diffusion model on play data.
robot experiments in Section IV-D3. We also tested which
5) ScalingtoOtherObjects: Weconductanexperimenton type of data: task data, play data, or a combination of both,
anotherobject,acup.TheBCbaselineachievesasuccessrate should be used for training the diffusion model. We found all
of 80% vs DMD achieves a success rate of 90% (Table IIe). versions to outperform behavior cloning. This suggests two
8
CB
NTRAPS
sruOpractically useful results. First, DMD can directly be used to [10] Chang Chen, Fei Deng, Kenji Kawaguchi, Caglar Gul-
improveataskpolicywithouttheneedtocollectanyadditional cehre, and Sungjin Ahn. Simple hierarchical planning
data, i.e. the task data itself can be used to finetune a novel- with diffusion. arXiv preprint arXiv:2401.02644, 2024.
view synthesis diffusion model to augment the task dataset [11] LiliChen,ShikharBahl,andDeepakPathak. Playfusion:
to improve performance. Second we can benefit from play Skill acquisition via diffusion from language-annotated
data, which maybe simpler to collect than task data. To foster play. In Conference on Robot Learning, pages 2012–
replication and future work, we will make code, data, and 2029. PMLR, 2023.
models publicly available. [12] Zoey Chen, Sho Kiami, Abhishek Gupta, and Vikash
Kumar. Genaug: Retargeting behaviors to unseen sit-
ACKNOWLEDGMENTS
uations via generative augmentation. arXiv preprint
We give special thanks to Kevin Zhang for 3D printing the arXiv:2302.06671, 2023.
attachmentforus.Thismaterialisbaseduponworksupported [13] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric
by the USDA/NSF AIFARMS National AI Institute (USDA Cousineau, Benjamin Burchfiel, and Shuran Song. Dif-
#2020-67021-32799) and NSF (IIS-2007035). fusion policy: Visuomotor policy learning via action
diffusion. arXiv preprint arXiv:2303.04137, 2023.
REFERENCES
[14] Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric
[1] Suzan Ece Ada, Erhan Oztop, and Emre Ugur. Dif- Cousineau, Benjamin Burchfiel, and Shuran Song. Dif-
fusion policies for out-of-distribution generalization fusion policy: Visuomotor policy learning via action
in offline reinforcement learning. arXiv preprint diffusion. In Proceedings of Robotics: Science and
arXiv:2307.04726, 2023. Systems (RSS), 2023.
[2] Michal Adamkiewicz, Timothy Chen, Adam Caccavale, [15] Patrick Esser, Robin Rombach, and Bjo¨rn Ommer. Tam-
RachelGardner,PrestonCulbertson,JeannetteBohg,and ing transformers for high-resolution image synthesis.
Mac Schwager. Vision-only robot navigation in a neural arXiv, 2020.
radiance world. IEEE Robotics and Automation Letters, [16] Peter Florence, Lucas Manuelli, and Russ Tedrake. Self-
7(2):4606–4613, 2022. supervised correspondence in visuomotor policy learn-
[3] AnuragAjay,YilunDu,AbhiGupta,JoshuaTenenbaum, ing. IEEE Robotics and Automation Letters, 5(2):492–
Tommi Jaakkola, and Pulkit Agrawal. Is conditional 499, 2019.
generative modeling all you need for decision-making? [17] AlessandroGiusti,Je´roˆmeGuzzi,DanC. Cires¸an,Fang-
arXiv preprint arXiv:2211.15657, 2022. Lin He, Juan P. Rodr´ıguez, Flavio Fontana, Matthias
[4] Brenna D Argall, Sonia Chernova, Manuela Veloso, Faessler, Christian Forster, Ju¨rgen Schmidhuber, Gi-
and Brett Browning. A survey of robot learning from anni Di Caro, Davide Scaramuzza, and Luca M. Gam-
demonstration. Robotics and autonomous systems, 57 bardella. A machine learning approach to visual percep-
(5):469–483, 2009. tionofforesttrailsformobilerobots. IEEERoboticsand
[5] YogeshBalaji,SeungjunNah,XunHuang,ArashVahdat, Automation Letters, 1(2):661–667, 2016. doi: 10.1109/
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, LRA.2015.2509024.
Samuli Laine, Bryan Catanzaro, et al. ediffi: Text- [18] IanGoodfellow,JeanPouget-Abadie,MehdiMirza,Bing
to-image diffusion models with an ensemble of expert Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville,
denoisers. arXiv preprint arXiv:2211.01324, 2022. and Yoshua Bengio. Generative adversarial nets. Ad-
[6] Kevin Black, Mitsuhiko Nakamoto, Pranav Atreya, vances in neural information processing systems, 27,
Homer Walke, Chelsea Finn, Aviral Kumar, and Sergey 2014.
Levine. Zero-shot robotic manipulation with pre- [19] Jiatao Gu, Alex Trevithick, Kai-En Lin, Joshua M
trained image-editing diffusion models. arXiv preprint Susskind, Christian Theobalt, Lingjie Liu, and Ravi
arXiv:2310.10639, 2023. Ramamoorthi. Nerfdiff: Single-image view synthesis
[7] Valts Blukis, Taeyeop Lee, Jonathan Tremblay, Bowen with nerf-guided distillation from 3d-aware diffusion. In
Wen, In So Kweon, Kuk-Jin Yoon, Dieter Fox, and Stan International Conference on Machine Learning, pages
Birchfield. Neural fields for robotic object manipulation 11808–11826. PMLR, 2023.
from a single image. arXiv preprint arXiv:2210.12126, [20] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen,
2022. Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo.
[8] Ang Cao and Justin Johnson. Hexplane: A fast rep- Vector quantized diffusion model for text-to-image syn-
resentation for dynamic scenes. In Proceedings of the thesis. In Proceedings of the IEEE/CVF Conference on
IEEE/CVF Conference on Computer Vision and Pattern ComputerVisionandPatternRecognition,pages10696–
Recognition, pages 130–141, 2023. 10706, 2022.
[9] Matthew Chang, Aditya Prakash, and Saurabh Gupta. [21] Richard Hartley and Andrew Zisserman. Multiple view
Look ma, no hands! agent-environment factorization of geometryincomputervision.Cambridgeuniversitypress,
egocentric videos. In Advances in Neural Information 2003.
Processing Systems, 2023. [22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian
9Sun. Deep residual learning for image recognition. In demonstrations in high dimensional state spaces. In
ProceedingsoftheIEEEConferenceonComputerVision 2016 IEEE International Conference on Robotics and
and Pattern Recognition (CVPR), 2016. Automation (ICRA), pages 462–469. IEEE, 2016.
[23] Shashank Hegde, Sumeet Batra, KR Zentner, and Gau- [36] Michael Laskey, Jonathan Lee, Roy Fox, Anca Dragan,
rav S Sukhatme. Generating behaviorally diverse poli- and Ken Goldberg. Dart: Noise injection for robust
cies with latent diffusion models. arXiv preprint imitation learning. In Conference on robot learning,
arXiv:2305.18738, 2023. pages 143–156. PMLR, 2017.
[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising [37] Wenhao Li, Xiangfeng Wang, Bo Jin, and Hongyuan
diffusion probabilistic models. Advances in Neural Zha. Hierarchical diffusion for offline decision making.
Information Processing Systems, 33:6840–6851, 2020. InInternationalConferenceonMachineLearning,pages
[25] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, 20035–20064. PMLR, 2023.
and Chrisina Jayne. Imitation learning: A survey of [38] XiangLi,VarunBelagali,JinghuanShang,andMichaelS
learning methods. ACM Computing Surveys (CSUR), 50 Ryoo. Crossway diffusion: Improving diffusion-based
(2):1–35, 2017. visuomotor policy via self-supervised learning. arXiv
[26] Dongyoon Hwang, Minho Park, and Jiyoung Lee. Sam- preprint arXiv:2307.01849, 2023.
ple generations for reinforcement learning via diffusion [39] Yunzhu Li, Shuang Li, Vincent Sitzmann, Pulkit
models. 2023. Agrawal, and Antonio Torralba. 3d neural scene rep-
[27] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A resentations for visuomotor control. In Conference on
Efros.Image-to-imagetranslationwithconditionaladver- Robot Learning, pages 112–123. PMLR, 2022.
sarial networks. In Proceedings of the IEEE conference [40] Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni,
oncomputervisionandpatternrecognition,pages1125– MasayoshiTomizuka,andPingLuo. Adaptdiffuser:Dif-
1134, 2017. fusion models as adaptive self-evolving planners. arXiv
[28] Eric Jang, Alex Irpan, Mohi Khansari, Daniel Kappler, preprint arXiv:2302.01877, 2023.
FrederikEbert,CoreyLynch,SergeyLevine,andChelsea [41] Zhixuan Liang, Yao Mu, Hengbo Ma, Masayoshi
Finn. Bc-z: Zero-shot task generalization with robotic Tomizuka, Mingyu Ding, and Ping Luo. Skilldiffuser:
imitation learning. In Conference on Robot Learning, Interpretable hierarchical planning via skill abstrac-
pages 991–1002. PMLR, 2022. tions in diffusion-based task execution. arXiv preprint
[29] Michael Janner, Yilun Du, Joshua B Tenenbaum, and arXiv:2312.11598, 2023.
Sergey Levine. Planning with diffusion for flexible [42] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tok-
behavior synthesis. arXiv preprint arXiv:2205.09991, makov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-
2022. 3: Zero-shot one image to 3d object. In Proceedings
[30] Ivan Kapelyukh, Vitalis Vosylius, and Edward Johns. of the IEEE/CVF International Conference on Computer
Dall-e-bot: Introducing web-scale diffusion models to Vision, pages 9298–9309, 2023.
robotics. IEEE Robotics and Automation Letters, 2023. [43] CongLu,PhilipJBall,andJackParker-Holder.Synthetic
[31] Liyiming Ke, Jingqiang Wang, Tapomayukh Bhattachar- experience replay. arXiv preprint arXiv:2303.06614,
jee, Byron Boots, and Siddhartha Srinivasa. Grasping 2023.
withchopsticks:Combatingcovariateshiftinmodel-free [44] Andreas Lugmayr, Martin Danelljan, Andres Romero,
imitation learning for fine manipulation. In 2021 IEEE Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint:
International Conference on Robotics and Automation Inpainting using denoising diffusion probabilistic mod-
(ICRA), pages 6185–6191. IEEE, 2021. els. In Proceedings of the IEEE/CVF Conference on
[32] Liyiming Ke, Yunchu Zhang, Abhay Deshpande, Sid- ComputerVisionandPatternRecognition,pages11461–
dharthaSrinivasa,andAbhishekGupta. Ccil:Continuity- 11471, 2022.
baseddataaugmentationforcorrectiveimitationlearning. [45] Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar,
arXiv preprint arXiv:2310.12972, 2023. Jonathan Tompson, Sergey Levine, and Pierre Sermanet.
[33] AlexKrizhevsky,IlyaSutskever,andGeoffreyE.Hinton. Learning latent plans from play. In Conference on robot
Imagenet classification with deep convolutional neural learning, pages 1113–1132. PMLR, 2020.
networks. InAdvancesinNeuralInformationProcessing [46] Zhao Mandi, Homanga Bharadhwaj, Vincent Moens,
Systems (NeurIPS), 2012. Shuran Song, Aravind Rajeswaran, and Vikash Ku-
[34] Jeong-gi Kwak, Erqun Dong, Yuhe Jin, Hanseok Ko, mar. Cacti: A framework for scalable multi-task
Shweta Mahajan, and Kwang Moo Yi. Vivid-1-to-3: multi-scene visual imitation learning. arXiv preprint
Novelviewsynthesiswithvideodiffusionmodels. arXiv arXiv:2212.05711, 2022.
preprint arXiv:2312.01305, 2023. [47] Chenlin Meng, Yutong He, Yang Song, Jiaming Song,
[35] Michael Laskey, Sam Staszak, Wesley Yu-Shu Hsieh, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit:
Jeffrey Mahler, Florian T Pokorny, Anca D Dragan, and Guided image synthesis and editing with stochastic dif-
Ken Goldberg. Shiv: Reducing supervisor burden in ferential equations. arXiv preprint arXiv:2108.01073,
dagger using support vectors for efficient learning from 2021.
10[48] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, [62] Nur Muhammad Mahi Shafiullah, Anant Rai, Haritheja
Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Etukuru,YiqianLiu,IshanMisra,SoumithChintala,and
Nerf: Representing scenes as neural radiance fields for Lerrel Pinto. On bringing robots home. arXiv preprint
view synthesis. Communications of the ACM, 65(1):99– arXiv:2311.16098, 2023.
106, 2021. [63] Jascha Sohl-Dickstein, Eric Weiss, Niru
[49] Jyothish Pari, Nur Muhammad, Sridhar Pandian Maheswaranathan, and Surya Ganguli. Deep
Arunachalam, and Lerrel Pinto. The surprising unsupervised learning using nonequilibrium
effectiveness of representation learning for visual thermodynamics. In International conference on
imitation. arXiv preprint arXiv:2112.01511, 2021. machine learning, pages 2256–2265. PMLR, 2015.
[50] DeanAPomerleau. Alvinn:Anautonomouslandvehicle [64] Shuran Song, Andy Zeng, Johnny Lee, and Thomas
in a neural network. Advances in neural information Funkhouser. Graspinginthewild:Learning6dofclosed-
processing systems, 1, 1988. loop grasping from low-cost demonstrations. Robotics
[51] Dean A Pomerleau. Efficient training of artificial neural and Automation Letters, 2020.
networks for autonomous navigation. Neural computa- [65] Yang Song and Stefano Ermon. Generative modeling by
tion, 3(1):88–97, 1991. estimating gradients of the data distribution. Advances
[52] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey in neural information processing systems, 32, 2019.
Chu, and Mark Chen. Hierarchical text-conditional [66] Shih-Yang Su, Frank Yu, Michael Zollho¨fer, and Helge
image generation with clip latents. arXiv preprint Rhodin. A-nerf: Articulated neural radiance fields for
arXiv:2204.06125, 1(2):3, 2022. learning human shape, appearance, and pose. Advances
[53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, in Neural Information Processing Systems, 34:12278–
Patrick Esser, and Bjo¨rn Ommer. High-resolution image 12291, 2021.
synthesiswithlatentdiffusionmodels.2022ieee. InCVF [67] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li,
ConferenceonComputerVisionandPatternRecognition Brent Yi, Terrance Wang, Alexander Kristoffersen, Jake
(CVPR), pages 10674–10685, 2021. Austin, Kamyar Salahi, Abhik Ahuja, et al. Nerfs-
[54] Robin Rombach, Andreas Blattmann, Dominik Lorenz, tudio: A modular framework for neural radiance field
Patrick Esser, and Bjo¨rn Ommer. High-resolution image development. In ACM SIGGRAPH 2023 Conference
synthesis with latent diffusion models. In CVPR, pages Proceedings, pages 1–12, 2023.
10684–10695, 2022. [68] SebastianThrun. Probabilisticrobotics. Communications
[55] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. of the ACM, 45(3):52–57, 2002.
U-net: Convolutional networks for biomedical image [69] Hung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan,
segmentation. In MICCAI, pages 234–241. Springer, Jia-BinHuang,andJohannesKopf. Consistentviewsyn-
2015. thesiswithpose-guideddiffusionmodels. InProceedings
[56] Ste´phane Ross, Geoffrey Gordon, and Drew Bagnell. A of the IEEE/CVF Conference on Computer Vision and
reduction of imitation learning and structured prediction Pattern Recognition, pages 16773–16783, 2023.
to no-regret online learning. In Proceedings of the four- [70] Yixuan Wang, Zhuoran Li, Mingtong Zhang, Katherine
teenth international conference on artificial intelligence Driggs-Campbell, Jiajun Wu, Li Fei-Fei, and Yunzhu
and statistics, pages 627–635, 2011. Li. d3 fields: Dynamic 3d descriptor fields for zero-
[57] Chitwan Saharia, William Chan, Saurabh Saxena, Lala shot generalizable robotic manipulation. arXiv preprint
Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, arXiv:2309.16118, 2023.
Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Sali- [71] Zhendong Wang, Jonathan J Hunt, and Mingyuan
mans,etal. Photorealistictext-to-imagediffusionmodels Zhou. Diffusion policies as an expressive policy
with deep language understanding. Advances in Neural class for offline reinforcement learning. arXiv preprint
InformationProcessingSystems,35:36479–36494,2022. arXiv:2208.06193, 2022.
[58] Stefan Schaal. Learning from demonstration. Advances [72] Daniel Watson, William Chan, Ricardo Martin-Brualla,
in neural information processing systems, 9, 1996. Jonathan Ho, Andrea Tagliasacchi, and Mohammad
[59] Johannes Lutz Scho¨nberger and Jan-Michael Frahm. Norouzi. Novel view synthesis with diffusion models.
Structure-from-motion revisited. In Conference on Com- arXiv preprint arXiv:2210.04628, 2022.
puter Vision and Pattern Recognition (CVPR), 2016. [73] Zhou Xian, Nikolaos Gkanatsios, Theophile Gervet,
[60] Johannes Lutz Scho¨nberger, Enliang Zheng, Marc Polle- Tsung-Wei Ke, and Katerina Fragkiadaki. Chaineddif-
feys, and Jan-Michael Frahm. Pixelwise view selection fuser: Unifying trajectory diffusion and keypose predic-
for unstructured multi-view stereo. In European Confer- tion for robotic manipulation. In Conference on Robot
ence on Computer Vision (ECCV), 2016. Learning, pages 2323–2339. PMLR, 2023.
[61] Nur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty [74] TeteXiao,IlijaRadosavovic,TrevorDarrell,andJitendra
Altanzaya, and Lerrel Pinto. Behavior transformers: Malik. Masked visual pre-training for motor control.
Cloning k modes with one stone. Advances in neural arXiv preprint arXiv:2203.06173, 2022.
information processing systems, 35:22955–22968, 2022. [75] Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf:
11Improvingfew-shotneuralrenderingwithfreefrequency skills. arXiv preprint arXiv:1802.09564, 2018.
regularization. In Proceedings of the IEEE/CVF Confer-
enceonComputerVisionandPatternRecognition,pages
8254–8263, 2023.
[76] Yufei Ye, Xueting Li, Abhinav Gupta, Shalini De Mello,
Stan Birchfield, Jiaming Song, Shubham Tulsiani, and
Sifei Liu. Affordance diffusion: Synthesizing hand-
object interactions. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,
pages 22479–22489, 2023.
[77] SarahYoung,DhirajGandhi,ShubhamTulsiani,Abhinav
Gupta, Pieter Abbeel, and Lerrel Pinto. Visual imitation
made easy. In Conference on Robot Learning, pages
1992–2005. PMLR, 2021.
[78] Sarah Young, Jyothish Pari, Pieter Abbeel, and Lerrel
Pinto. Playfulinteractionsforrepresentationlearning. In
2022 IEEE/RSJ International Conference on Intelligent
RobotsandSystems(IROS),pages992–999.IEEE,2022.
[79] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo
Kanazawa. pixelnerf: Neural radiance fields from one or
fewimages.InProceedingsoftheIEEE/CVFConference
on Computer Vision and Pattern Recognition, pages
4578–4587, 2021.
[80] Jason J Yu, Fereshteh Forghani, Konstantinos G Derpa-
nis, and Marcus A Brubaker. Long-term photometric
consistent novel view synthesis with diffusion models.
arXiv preprint arXiv:2304.10700, 2023.
[81] Tianhe Yu, Ted Xiao, Austin Stone, Jonathan Tompson,
Anthony Brohan, Su Wang, Jaspiar Singh, Clayton Tan,
JodilynPeralta,BrianIchter,etal. Scalingrobotlearning
with semantically imagined experience. arXiv preprint
arXiv:2302.11550, 2023.
[82] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.
Adding conditional control to text-to-image diffusion
models. In Proceedings of the IEEE/CVF International
ConferenceonComputerVision,pages3836–3847,2023.
[83] Tianhao Zhang, Zoe McCarthy, Owen Jow, Dennis Lee,
Xi Chen, Ken Goldberg, and Pieter Abbeel. Deep
imitation learning for complex manipulation tasks from
virtual reality teleoperation. In 2018 IEEE International
Conference on Robotics and Automation (ICRA), pages
5628–5635. IEEE, 2018.
[84] TonyZZhao,VikashKumar,SergeyLevine,andChelsea
Finn. Learning fine-grained bimanual manipulation with
low-cost hardware. arXiv preprint arXiv:2304.13705,
2023.
[85] Allan Zhou, Moo Jin Kim, Lirui Wang, Pete Florence,
and Chelsea Finn. Nerf in the palm of your hand:
Correctiveaugmentationforroboticsvianovel-viewsyn-
thesis. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,pages17907–
17917, 2023.
[86] Yuke Zhu, Ziyu Wang, Josh Merel, Andrei Rusu,
Tom Erez, Serkan Cabi, Saran Tunyasuvunakool, Ja´nos
Krama´r, Raia Hadsell, Nando de Freitas, et al. Rein-
forcement and imitation learning for diverse visuomotor
12