OmniACT: A Dataset and Benchmark for Enabling Multimodal Generalist
Autonomous Agents for Desktop and Web
RaghavKapoor† YashParagButala†
MelisaRussak JingYuKoh KiranKamble
WaseemAlShikh RuslanSalakhutdinov
CarnegieMellonUniversity Writer.com
{raghavka, ypb}@cs.cmu.edu
TTaasskk IImmaaggee TTaasskk DDeessccrriippttiioonn AAccttiioonn SSccrriipptt IIlllluussttrraattiioonn
Check the rate of import pyautogui
change in Google's pyautogui.moveTo(210,
stock price over 179)
last one month pyautogui.dragTo(243,
179, button="left")
import pyautogui
Find Rental
Properties in pyautogui.click(398, 167)
Seattle, pyautogui.write
("Seattle, WA")
Washington pyautogui.press
("enter")
Using the popup
import pyautogui
opened, scroll over
to find weather in pyautogui.moveTo(410,
Chicago on 18th 207)
pyautogui.hscroll(10)
September pyautogui.click(410, 207)
Figure1.OmniACTdatasetandbenchmarkforenablingautonomoushuman-computerinteractionagents.Theleftshowsanimagepaired
withanaturallanguagetaskdescriptionastheinput,therightshowstheresultingactionscripttobeexecutedonthescreen.Examplesare
presentedfromStocks,Apartments.com,andWeatherapplication.
Abstract complextravelbookings,withminimalhumanintervention.
In this paper, we introduce OmniACT, the first-of-a-kind
For decades, human-computer interaction has funda- datasetandbenchmarkforassessinganagent’scapability
mentally been manual. Even today, almost all productive to generate executable programs to accomplish computer
work done on the computer necessitates human input at tasks. Our scope extends beyond traditional web automa-
every step. Autonomous virtual agents represent an excit- tion,coveringadiverserangeofdesktopapplications. The
ingstepinautomatingmanyofthesemenialtasks. Virtual datasetconsistsoffundamentaltaskssuchas“Playthenext
agents would empower users with limited technical profi- song”, as well as longer horizon tasks such as “Send an
ciencytoharnessthefullpossibilitiesofcomputersystems. emailtoJohnDoementioningthetimeandplacetomeet”.
Theycouldalsoenabletheefficientstreamliningofnumer- Specifically, given a pair of screen image and a visually-
ouscomputertasks,rangingfromcalendarmanagementto grounded natural language task, the goal is to generate
a script capable of fully executing the task. We run sev-
†Theseauthorscontributedequally. Theorderisdeterminedbydice eral strong baseline language model agents on our bench-
rolling.
1
4202
beF
82
]IA.sc[
2v35571.2042:viXramark. Thestrongestbaseline,GPT-4,performsthebeston tions. Theobjectiveoftheseinstructionsistogenerateexe-
ourbenchmarkHowever,itsperformancelevelstillreaches cutablecommandsusingthePyAutoGUIPythonlibrary[1].
only15%ofthehumanproficiencyingeneratingexecutable PyAutoGUI enablestheautomationofthemouseandkey-
scripts capable of completing the task, demonstrating the boardoperations,whichhelpstofacilitateinteractionswith
challenge of our task for conventional web agents. Our various native applications across macOS, Windows, and
benchmark provides a platform to measure and evaluate Linux. This simplifies completing specified tasks across
theprogressoflanguagemodelagentsinautomatingcom- differentwebdomainsandnativedesktopapplications.
putertasksandmotivatesfutureworktowardsbuildingmul- We evaluate several language model-based agent base-
timodalmodelsthatbridgelargelanguagemodelsandthe lines on this dataset, including LLaMA [43], Vicuna [7],
visualgroundingofcomputerscreens. Palmyra-X (43B) [2], InstructPalmyra-30B [41], GPT 3.5,
and GPT-4 [30]. We experiment with fine-tuning Vicuna-
1.Introduction 13BandLLaMA-13BmodelsusingQLoRA[10]. Wealso
benchmark multimodal baseline LLaVa-v1.5-7B, LLaVa-
Performing computer tasks based on natural language in-
v1.5-13B[43]andGPT-4-vision-preview[50]forthetask.
structionshasbeenalong-standinggoalofartificialintelli-
Ourfindingshighlightthenecessityforamultimodalmodel
gence [45]. One concrete objective in the line of research
capableofexecutingthesetasks,andouranalysisprovides
is to develop generalist agents that can assist humans in
insights into promising future work in the space. Our key
doing computer tasks [19], such as “Order a pizza from
contributionsareoutlinedasfollows:
Domino’s”or“WriteamessagetoJohn.”Theagentshould
1. Wereleaseanoveldatasetofdesktopandwebsiteappli-
beabletoopentheapplicationandperformthetask. Exe-
cations consisting of over 9.8K natural language tasks,
cuting these actions on a personal computer involves a se-
UI screens, and corresponding code snippets collected
quenceofinteractionswithamouseandkeyboard. Forex-
throughhumanannotation. Weintroducecustomperfor-
ample, the simple task of writing an email involves hover-
mancemetricstailoredforcomputertasks.
ingovertheapplicationicon,clickingit,clickingthe‘New
2. WeproposeDetACT,amoduleforcreatingtextualrep-
Email’ button, writing the content of the email, and click-
resentationsofthescreenusingsignalsfromOCR,color,
ingsend. Successfullysendinganemailrequiresaccurately
andicon-templatematching.
predictingthecorrectactionateachstepandaccuratelyex-
3. We conduct a comprehensive benchmark and analysis
ecutingit,whichisaherculeantaskevenforthebestagents
ofstate-of-the-artLLMsandmultimodalmodelsonour
today[13].
benchmark. OurresultsshowthatOmniACTisachal-
A generalist agent for computer tasks must understand
lenging task for even the best LLM agents today, and
natural language instructions, process visual screenshots,
existingmodelsarefarbelowhumanperformance.
and produce the correct sequence of actions to be per-
formed to achieve the intended task. Several existing ap- 2.RelatedWork
proaches focus on building agents based on the HTML
2.1.UIUnderstanding
model[9,37,57]. However, thisapproachintroducessev-
eralchallengesandconstraints. Theseagentsarelimitedto User interface (UI) understanding has garnered interest
webapplicationsandoftenstrugglewithcomplexorlong- from researchers in the machine learning and human-
contextHTMLcode.Theycannotinteractwithnativedesk- computer interaction communities, evolving with various
top applications or perform tasks that span multiple appli- models focusing on understanding the semantics of mo-
cations, like drafting an email using text from a code ed- bile and web user interfaces. UIBert [3], PixelBERT [15],
itor, without significant alterations. Furthermore, HTML- ActionBert [14], VUT [23], Screen2Words [44], Widget-
based agents, which are inherently powered by text-only Captioning [22] and Pix2Act [36] are notable models in
languagemodels,typicallyunderperformintasksrequiring this area. They propose approaches for learning the user-
visual cues, such as identifying and clicking a blue button interface semantics of the mobile screen using the image
onadesktop’stop-rightcorner.Incontrast,humanscaneas- and view hierarchy. These models have demonstrated ef-
ily understand UI elements like dropdown menus, typable fectiveness in tasks like capability prediction, screen seg-
areas,redirections,andoptionswithjustaglance. mentation and understanding, and screen caption genera-
Towardsthegoalofdevelopingageneralistautonomous tion. Lexi [4] and Spotlight [20] propose models that use
agent with robust visual and user interface (UI) under- vision-only inputs to minimize the reliance on metadata
standingcapabilities, weintroduceanewtaskanddataset, such as view hierarchy. Furata et al. [11] demonstrates
OmniACT, containing over 9.8K pairs of images and in- theuseoffine-tuningformultimodalwebnavigation. The
structions (Figure 1) across different operating systems majorityofmachinelearningmodelstrainedforUIunder-
and the web. This dataset includes screenshots of vari- standing leverage the Rico dataset [8] and its extensions,
ousUIscreensandcorrespondingnaturallanguageinstruc- which contain 64,462 unique Android screens and meta-
2Supports ContinuousScale
Task Real-World Executional
Datasets Size EnvType Desktop Adaptive Task
Heterogeneity Portayal Correctness
Apps Evaluation
WebArena[57] 812 Web Yes Yes Yes No No WebNavigation
Mind2Web[9] 2350 Web Yes Yes No No No WebNavigation
WebShop[51] 12000Products Web No No Yes No No WebNavigation
RUSS[49] 80 Web Yes Yes No No No WebNavigation
WebSRC[6] 2735 Web Yes Yes - No No QA
Mobile
MiniWoB++[16] 100 No No Yes No No WebNavigation
Websites
PixelHelp[21] 187 Mobile Yes Yes No No No UIGrounding
MetaGUI [39] 1125 Mobile Yes Yes Yes No No MobileNavigation
MoTIF[5] 756 Mobile Yes Yes Yes No No MobileNavigation
Mobile/Web
AITW[32] 715142 MobileandWeb Yes Yes Yes No No
Navigation
OmniACT(Ours) 9802 DesktopandWeb Yes Yes Yes Yes Yes CodeGeneration
Table1.ComparisonofOmniACTwithotherrelatedbenchmarks.
data. In addition, Banerjee et al. [4] released the UICap- parisonbetweentheexistingbenchmarksandourproposed
tionsdataset,whichconsistsofdiverseimage-captionspairs benchmark,OmniACT,inTable1.
acrossawiderangeofapplications. PixelHelp[21]alsore-
3.OmniACT
leased a corpus to train models that can interpret natural
language instructions and map them to mobile user inter-
We introduce a novel dataset and benchmark, OmniACT,
faceactions.
whichmeasurestheperformanceofautonomousagentson
bothwebanddesktopapplications. Comparedtoprevious
2.2.AutonomousComputerAgents
benchmarkswhichfocusontext-basedreasoning[9,16,37,
The advent of large language models (LLMs) has been 51,57],ourbenchmarkaimstomeasuremultimodalagents
pivotal in the rapid advancement of agents that operate that bridge large language model planners and UI under-
on web pages. Recent research such as ViperGPT [40] standing vision models. OmniACT can be accomplished
Chameleon [26], RCI Agent [17], VisProg [12], and [28] asastandalonetaskasitisnotunderamockenvironment.
employ LLMs for planning or action prediction in devel- Allactionsthatahumancanexecuteonthecomputercan
oping autonomous agents. Benchmark datasets, such as beencodedinthePyAutoGUI [1]Pythonframework. This
MiniWoB [37], WebShop [51], Macaw-LLM [27], ASH- frameworkallowsausertoexecutekeyboardandmouseop-
Prompting [38] Mind2Web [9] and WebArena [57] have erationsbyrunningPythoncode. ThePyAutoGUI codeto
also been proposed to measure the ability of LLM-based executethesetasksisshowninthethirdcolumnofFigure1.
agentsinautomatingwebtasks. Thesemethodsmainlyin- For other computer tasks, the PyAutoGUI library provides
volveagentsthatoperateonatext-basedDocumentObject functionssuchas‘press’,‘write’,and‘scroll’whichcanbe
Model (DOM) of HTML scripts. This limits their under- used to execute the task. Our dataset consists of parallel
standingofscreencontext,whichiscrucialforthemodel’s dataofnaturallanguagetasks, UIscreenshots, andground
decision-making and action-taking processes. To address truthPyAutoGUIscriptsthatachievesuccessfulexecution.
this limitation, Rawles et al. [32] released Android in the
3.1.TaskFormulation
Wild, a dataset comprising screens, natural language in-
structions,andcorrespondingactions. Followingthis, [54] Given an input state of a computer defined by the screen
proposed a multimodal model, AutoUI, which is designed S and the task description T in natural language, the
to build an agent on the Android in the Wild dataset con- goal of the task is to output a sequence of actions A that
finedtotheAndroidecosystem. can successfully accomplish the task T within a screen-
Current benchmarks for autonomous agents focus shotS ∈ {Linux,Windows,MacOS,Webpage}. Formally,
mainlyontheWeborAndroidenvironments, posingchal- the task can be defined as learning the transition func-
lengesfortasksinvolvingdesktopapplicationsorspanning tion f : T × S → A. During dataset collection, we
multipleapplicationsbeyondthewebdomain. Theabsence ensure that all task descriptions T are feasible and can
ofestablishedbenchmarksanddatasetsinthisarea,coupled be accomplished in the current screenshot S. To reduce
with basic methods for extracting user interface (UI) ele- ambiguity and facilitate better evaluation, we ensure that
ments, underscorestheneedforsignificantprogressinde- task descriptions are detailed and unambiguous. Tasks
velopingmoreversatileautonomousagentscapableofhan- can also be visually grounded (e.g., ‘Click the red but-
dling diverse tasks beyond the current scope. To highlight ton to start recording’) or natural language based (e.g.,
the unique features that OmniACT introduces in the as- ‘Click the My Account button’). We define the action
sessmentofcapableautonomousagents,weprovideacom- space using the functionalities in the PyAutoGUI library:
3Application/Website Selection Functionality Tagging Reverse Mapping and Filtering
1 3 5
Task 1: Task 1:
Label: pyautogui.click("book_1") pyautogui.click(324, 236)
Book_Code_Red pyautogui.press("enter") pyautogui.press("enter")
Label:
Browse_Romance
Shopping Information _Books Incorrect Syntax
Executable Tasks
Government Travel or Action Sequence
Health Entertainment
2 4
Task 1:
pyautogui.click("book_1")
pyautogui.press("enter")
...
Task 10:
UI Screen Segmentation Task Creation
Figure2. DataCollectionPipeline. (1)Weselectover60applicationsandwebsitestoensurediversity,(2)segmentthescreenthrough
human-annotatedboundingboxes,(3)labeltheboundingboxesbasedonfunctionality,(4)askstudentvolunteerstocomeupwithtasks,
givenascreenimage,and(5)reversemapthetextuallabelstocoordinatesandfilterthescriptsbasedonexecutionandsyntax.
A ∈ {‘click’,‘dragTo’,‘scroll’,‘write’, ...}. Theexhaus- Type Action %
tivelistofactionsisprovidedinTable2. Ouractionspace Click 63.73
is much larger than other benchmarks [9, 37, 57] that re- DoubleClick 0.58
RightClick 0.77
sort to two or three interaction options. Mouse actions
Mouse Move/Hover 1.85
suchas‘moveTo’, ‘click’, ‘rightClick’, ‘doubleClick’, and Drag 0.29
‘dragTo’, additionally require screen coordinates as argu- Scroll 1.68
ments,whichindicatethepixellocationoftheaction. HorizontalScroll 0.17
Figure1illustratessampletasksandcorrespondingout- Press 16.28
Keyboard Hotkey 3.00
puts for three applications within OmniACT: (1) Stocks
Write 11.65
(MacOS),(2)Apartments.com(webpage),and(3)Weather
(MacOS).Thefirstcolumndepictstheinputimage,andthe Table2. ActiontypesupportedbyOmniACTandthenumberof
instancesforeachoftheactionsinthedataset.
second column shows the natural language task that is to
be executed on the current screen. To execute these tasks,
making it a valuable resource for training and evaluating
ausermustaccuratelyperformaseriesofoperationsusing
autonomousagents.
the mouse and keyboard. For example, to check the rate
of change in Google’s stock price over the last month, the
3.2.1 Application/WebsiteSelection
mousehastobemovedtothelastmonthanddraggedwhile
holdingtheleft-clickbuttontothecurrentmonth. To test the computer agents’ generalization ability across
different tasks, we collect tasks across multiple domains
3.2.DatasetPreparation
on both desktop and web applications. In total, we col-
To prepare our dataset, we followed a pipelined approach, lectandannotate9802datapoints(Table3), withthesplit
as summarized in Figure 2. We first selected a variety of between desktop and web applications approximately 3:1.
applicationsandwebsites. Foreachapplicationorwebsite, The emphasis on desktop applications, which do not con-
wecreatedboundingboxesaroundkeyUIelementsandla- tain Document Object Model (DOM) hierarchies unlike
beledthemaccordingtotheirfunctionality,whichiscrucial HTML-based web pages, presents a more complex multi-
forassistinghumanannotatorsinwritingaccuratePyAuto- modal challenge where visual cues are crucial. We collect
GUI scripts. Aftereachscriptiswritten, weconvertedthe tasksfromapplicationswithinthethreemostpopularoper-
labels back into numeric coordinates, allowing us to align ating systems. We select 22 native applications from Ma-
the scripts precisely with the locations of the UI elements. cOS, and 8 each from Linux and Windows. We annotate
Finally, we thoroughly reviewed each script, focusing on roughly3to4screensforeveryapplication. Thefulllistof
its executability and adherence to syntax standards. This applicationsisprovidedintheAppendix.
ensured the high quality and functionality of our dataset, Manycommoncomputertaskstodayarestillperformed
4throughwebapplications,sowealsocollect3-4screenshots Domain Train Validation Test Total
from 27 different web applications. To ensure diversity in MacOS 3028 444 786 4258
task intents, we categorize these tasks into one of the fol- Desktop Linux 761 126 247 1134
lowing 6 categories: (1) Shopping, (2) Entertainment, (3) Windows 1573 216 458 2247
Web - 1427 206 530 2163
Service, (4) Government, (5) Travel, (6) Health. Inspired
Total 6789 992 2,021 9802
by the methodology of [9], these categories were selected
tocoverawiderangeofuserintentsandfunctionalities. Table3.Datasetdistributionacrosssplitsandplatforms.
3.2.2 UIScreenSegmentation
“Calculatethesumof2and3”or“Addtwotothree”. To
avoid train-test leakage, rephrased tasks were consistently
Tocollectgold-standarddata,wefirstannotateandsegment
placed in the same dataset split. Further details on the an-
thescreenbyidentifyingtheboundingboxespresentonthe
notationprocessareavailableintheAppendix.
screen. We employ slightly different techniques for web
anddesktopapplicationstocreatetheboundingboxes:
3.2.5 ReverseMappingandFiltering
1. Desktop Applications: We build a custom annotation
interface based on PyQt51 to create bounding boxes To ensure high-quality data, we incorporate an additional
manuallyoverascreenimageusingasimpledrag-and- step into the data collection pipeline. We build scripts to
click mechanism. This custom interface expedites the map the text-based labels of each bounding box back to
process and allows us to get highly accurate gold-label their numeric coordinates, and then match the syntax and
datapointsfordesktopimages. verifyifthetaskwillbeexecutedonthescreen. Usingthis
2. Websites: For webpages, we write JavaScript code to filter,weremoveallthenon-workingorsyntacticallyincor-
extract all interactable (click, hover, type, etc.) regions rectdatapointsandfinallymanuallyreviewthesetoftasks.
from the HTML source code. We also extract banners, After filtering, we obtain 9802 human-annotated, gold-
dropdowns, submit, and radio buttons from the screen. label data points across more than 200 desktop and web
Wefiltertheelementstoretainonlythosethatarevisible screens (Table 3), split into train, validation, and test sets
andinteractablewithinthescreen. ina7:1:2ratio. Allcollecteddatawillbepubliclyreleased
toencouragefutureworkonmultimodalagents.
3.2.3 FunctionalityTagging
4.EvaluationMetrics
Tomapeachboundingboxtoitscorrectfunctionaldescrip-
tion, we leverage Amazon MTurk workers (see details in In this section, we detail various evaluation metrics
Appendix), who are given an image with a bounding box for benchmarking model performance on the OmniACT
andarerequiredtowritethecorrectdescriptionorlabelof dataset. UI screens have additional constraints such as
theboundingbox’sfunction. Forexample,givenanimage spatial relevance which are not factored in most conven-
of an Amazon webpage with a search bar, the annotator tional similarity-based metrics such as BLEU [31], Code-
labelsitas“find-product-search-bar”. Thelogicaldescrip- BLEU [33], BERTScore [53] and CodeBERTScore [56].
tionsareusedtocreatetasksinastructuredmannerwithout Forexample,avalidclickactionisusuallynotconstrained
theneedtoidentifyindividualboundingboxcoordinates. to a single coordinate but can be any coordinate within a
specified region. In the event of invalid coordinate predic-
3.2.4 TaskCreation tions, anagentthatpredictscoordinatesfurtherawayfrom
Ourapproachforeachscreeninvolvesutilizingallhuman- thevalidregionshouldinvokeahigherpenaltycomparedto
annotated bounding boxes and their labels to create tasks anagentthatpredictedcoordinatesclosetotheregion. We
thatcanbeexecutedwithintheconfinesofasinglescreen. propose two new metrics adapted: Sequence Score (Sec-
Thesetasksaredesignedtobevisuallygroundedinorderto tion 4.1)and Action Score (Section4.2) aimed at utilizing
measurethecapabilitiesofmultimodalagents. Weplanto UIinformation.
releasetheboundingboxandtheircorrespondinglabelsas
4.1.SequenceScore
themetadataforevaluationpurposes.
For dataset compilation, college students with basic Thesequencescoremeasureswhetherthepredictedaction
Pythonprogrammingskillsservedasannotators,accessing sequence(e.g.,‘click’,‘write’,‘press’)exactlymatchesthe
API references for PyAutoGUI and examples of potential gold sequence. Since predicting the first action in the se-
tasks. Each student generated multiple tasks, each accom- quence is relatively straightforward and later actions are
paniedbythreealternativenaturallanguagereformulations. moredifficult,wedefinesequencescoreasfollows:
For instance, “What is 3+2?” might be reformulated as (cid:40)
β +β ∗(s−1) ifallactionsmatch
SeqScore = 1 2
i
1https://pypi.org/project/PyQt5/ 0 otherwise
5Task
Click on the Black location OCR Module
icon and enter destination
as "Paris" Flights <x: 230, y: 410>
Going to: <x: 190, y: 410>
Cruise: <x: 250, y: 410>
Hotels: <x: 280, y: 410>
Search: <x: 300, y: 450>
Icon Matching Module
Location: <x: 180, y: 410>
Bell: <x: 270, y: 93> LLM-based
Globe: <x: 210, y: 147> Filter
Person: <x: 270, y: 410>
…
Text:
Going to: <x: 190, y: 410>
Icon:
blue: [<x:300, y:450>, Location icon: <x: 180, y:180>
<x:249, y: 512>, …]
yellow: [<x:100, y: 53>]
black: [<x:190, y: 410>] Color
Segment Anything Black: [<x:180, y:41]
Model
Color Matching Module
Figure3. DetACTModule. Givenaninitialimageandanaturallanguagetaskdescription,weuseapipelinedapproachtorunOCRand
SAMonthescreen. TheoutputsfromSAMarethenusedbyiconandcolor-matchingmodulestoobtainanexhaustivesetofusefulUI
elements.ThelistofelementsispassedthroughLLMbasedfiltertoselectonlytheelementsrelatedtothegiventask.
where s is the action sequence length, β is set to 0.1 and resented as GKj) and predicted code (represented as
1 i
β 2issetto1. PK ij)arethesame. Itisformallydefinedas:
(cid:40)
0 ifGKj =PKj andSeqScore >0
4.2.ActionScore Kj =α × i i i
i i 1 otherwise
The action score measures how well a code snippet con-
3. Writepenalty(W ): Foractiontype‘write’,wepenal-
taining the correct action sequence can perform the task. p
izetheoutputforthesentencetobetyped. Specifically,
Specifically,forascriptwithacorrectactionsequence,we
wetheemployBLEUscore[31],andcompute:
introduce penalties for inaccurate behavior. The penalties
(cid:40)
aredescribedbelow: 1−BLEU(GSj,PSj) ifSeqScore >1
Wj =α × i i i
1. Click penalty (M): For actions ‘click’, ‘rightClick’, i i 1 otherwise
‘doubleClick’, ‘moveTo’, and ‘dragTo’, we penalize Here, GSj represents the actual sentence to be typed,
codesnippetswherepredictedcoordinateslieoutsideof i
andPSj representsthesentencepredictedbythemodel
the bounding box of the UI element. The click penalty i
inthejthactionofexamplei.
forthejthactionoftheithexampleisdefinedas:
 µ Intheaboveequations,(α i)istheweightingfactor:
M ij =α
i×1−
µ+L 2
ifSeqScore
i
>0
α i =SeqScore i/lengthofsequencei
1 otherwise
This ensures that the action score ∈ [0,1]. The mean
HereL 2 correspondstothesmallestEuclideandistance actionscoreiscalculatedasfollows:
betweenthepredictedcoordinateandboundingbox. L
2
ActionScore=
iszerowhenthepredictedcoordinatelieswithinthetar-
(cid:16) (cid:17)
get bounding box. µ is the Dirichlet smoothing coef- (cid:80) max SeqScore −(cid:80) (Mj +Kj +Wj),0
ficient which we dynamically set to the inverse of the i i j i i i
(cid:80)
SeqScore
length of the diagonal of the bounding box. This en- i i
sures that the penalty for points outside the bounding Wereportanddiscussthesemetricsforallbaselinemod-
box varies based on the size of the bounding box. For elsinSection7.
two predicted points with the same L , the metric pe-
2 5.DetACT:DETectingACTionsfromUI
nalizes more heavily if the box is larger. This is sound
withtheintuitionthatthechancesofclickingonalarger Understanding UI screens is crucial for multimodal com-
boxarehigherandshouldbepenalizedmoreincaseof putertasks. Web-basedagentstypicallyuselanguage-only
amistake. inputsfromtheHTMLDOM.Thisisinsufficientforcom-
2. Key penalty (K): For actions ‘press’ and ‘hotkey’, we prehending the full extent of an application UI, as many
check whether the set of keys in the target code (rep- componentsmaynotbeeasilydescribedwithHTMLcode.
6Input Prompt seeninFigure3,afewiconsmatchedonthescreenare
Role: You are an excellent robotic process Globe icon, Calendar icon, Person icon, and Location
automation agent .... Role
icon;eachdepictingadifferentusecase.
Below is the API Reference to use for the
Text: process
"See deals": <x: 412, y: 826>def click(x: float, y: float): 3. Color Module: Finally, to place all segments of inter-
"""takes the mouse to location (X, Y) and
D Me ot dA uC leT Mail Icon: I <c xo :n 2: 1, y:654> d e p f wad s riso tee (s te a x tl :e sft t rc )l :ick""" RefA erP eI nce e Rs Gt Bint po ixa ep lp vr ao lup eri sa ote veb ru tc hk ee Rts Oo If ac no dl ,o br as, sew de onav the ara tg ve alt uh ee ,
Color: """Types the text, which is a string"""
Blue <x: 412, y:826> . . . .pass bucket theminto differentcolor categories. We catego-
Pink [<x: 43, y: 545>,
x: 307, y: 100] Here are a few examples for reference rize colors differently based on the human perspective
Example 1:
Task: Open the email from Google oftherangesofeachcolor. Toavoidambiguity,wecon-
Output Script: Examples
p ...yautogui.click(410, 578) sider eleven major colors, namely yellow, blue, green,
Here are UI elements relevant for the task red,pink,violet,white,black,orange,brown,andgrey.
"Search Icon": <x: 412, y:826> UI
" ..B .lue": <x:412, y: 826> Elements Werecordthecenteroftheelementalongwiththecolor.
Given the task below, complete the output Once all the elements of each category are extracted
Task: Right Click to copy link for s Tc ar sip k:t : Right click to copy link for "See Deals" in Task
"See Deals" in the Expedia email the Expedia email with their coordinates, we then filter these UI elements by
prompting GPT-4 [30]. We ensure that the elements se-
lected are suited only for our task, for which we also pro-
Python Script
videthetaskdescriptioninourpromptsalongwiththelist
pyautogui.rightClick(412, 826)
pp yy aa uu tt oo gg uu ii .. pw rr eit se s( (" "C eo np tey r" ") ) ofelements. Fulldetailsofthepromptareprovidedinthe
LLM appendix section of the paper. As we observe in Figure 3,
Task and Image Input Output Script given an image from the Expedia application, and a task
Figure4. BaselineModelArchitecture. Imageandtaskdescrip- (“Click on the Black Location icon and enter the destina-
tionsaresenttoDetACTmodule,whichgivesafilteredlistofUI tion as Paris.”), the LLM filters out the elements to retain
elementsrelevanttofeedintothepromptalongwiththetask. We only “Going To”, “Location Icon”, and the Black colored
alsoshowthepromptstructureusedforactionscriptgeneration.
elements from the screen. This is passed as input to the
ThisstructureispassedthroughtheLLM(alongwiththeimage
LLMorvision-languagemodelbackbone.
formultimodalLLM)togeneratetheautomationscript.
6.Baselines
To address this, we propose DetACT, which allows us to
convert images of UI layouts into structured code and text To evaluate the performance of existing language model-
outputsforadownstreamLLM.DetACTisasystemcom- basedagentsonOmniACT,weconductexperimentswith
prised of three distinct modules: the text module, the icon both language-based and multimodal baselines. The De-
module,andthecolormodule. tACT module takes in image and text descriptions of the
taskandoutputsthecolor,icon,andtext-basedsignals.This
1. TextExtraction: WeusetheEasyOCRmodel2 toparse
is concatenated to the prompt for the LLM prompt-based
over the UI screens and collect all text-based elements.
baselines (see Figure 4). Every prompt starts with a role
Along with the text, we also note the locations of each
assignment[55],followedbythedetailedAPIreferenceof
of these elements. This is depicted in Figure 3, along
the PyAutoGUI function set, along with a textual descrip-
withalistoftextelementsfoundonthescreenusingthe
tionoftheirfunction. Wethenaddfivein-contextexamples
OCRModule. Wesegmentandclassifythedifferentre-
fromthetrainingsetthatmostcloselymatchthetask(based
gionswithinthescreenshotusingtheSegmentAnything
onthecosinesimilarityoftheMiniLM[46]embeddingsof
Model (SAM) [18]. From the outputs, we filter out the
thereferencetaskandthetrainexamples). Weaddalistof
non-textualsegmentsforouriconandcolordetection.
UIelementsfilteredbytheDetACTmoduletotheprompt.
2. Icon Module: For matching with the appropriate icon,
Finally,weprovidetheruleswiththetaskdescription. For
we use a pack of 1600 icons3 as templates. Each of
multimodalbaselines, wealsopasstheimagepixelstothe
theseiconsislabeledwiththeirappropriatefunctionality
visionencoder. Wereporttheresultsofseveralbaselines:
andismatchedwiththefilteredoutputsSAM[18]. For
• Few-shot Generative LLM: We experiment with mod-
thesimilarityofthetwoimages,weresizethereference
els from LLaMA-2 [43], Vicuna-1.5 [7], CodeLLaMA-
iconsandsegmentedregionofinterest(ROI)tothesame
34B [34], Palmyra [42], and GPT [30] series. We use
size,andconvertbothimagestograyscale.Afterthis,we
the prompts structure as shown in Figure 4 to prompt
use the Structural Similarity Index (SSIM) [48], to find
themodel. ForLLaMAandCodeLLaMa,wereducethe
theclosestmatchoftheROItotheiconsinourset,and
promptlengthto2000tokensbyremovingoutputsfrom
select the ones above the SSIM threshold of 0.95. As
the DetACT module with lower confidence, as we ob-
2https://github.com/JaidedAI/EasyOCR servedpoorperformanceonlongerprompts.Fortheother
3https://icomoon.io/ models,weallowpromptswithupto4000tokensizes.
7Model SS(↑) Mp Kp Wp AS(↑) tially improves LLaMA-13B’s sequence score (4.80 to
PromptbasedLLMs 8.92) and action score (1.62 to 2.14), as well as the other
LLaMA-7B[43] 4.12 1.24 1.83 0.57 0.48
Vicuna-7B [7] 3.88 1.17 1.51 0.43 0.77 metrics. Despitethis,weobservedthatboth,prompt-based
LLaMA-13B [43] 4.80 1.32 0.93 0.93 1.62 LLMs and finetuned LLMs face severe mouse penalties,
Vicuna-13B [7] 5.44 1.65 0.94 1.06 1.78
Palmyra-Instruct-30B[41] 7.51 5.68 0.12 0.40 1.31 especially on click coordinates. This is because they rely
CodeLLaMA-34B[35] 10.09 2.99 2.71 0.66 3.72 solelyontext-basedsignals.
Palmyra-X43B[2] 11.20 3.12 3.02 2.12 2.94
GPT-3.5-turbo-0613[29] 22.85 8.13 4.51 2.31 7.89 To address this, we experiment with multimodal lan-
GPT-4[30] 32.75 10.27 6.99 3.89 11.60
guage models (Table 4). We observe that the coordinate
FinetunedLLMs
prediction improves significantly when we provide the en-
LLaMA-13BFT 8.92 4.61 1.43 0.74 2.14
Vicuna-13BFT 8.78 4.12 1.31 0.63 2.72 tireimageasinputtothemultimodalLLM,asthisenablesit
MultimodalLLMs tofullyutilizethescreenrepresentation.Inadditiontoopen
LLaVA-v1.5-7B[25] 13.23 4.73 1.24 1.44 5.82
LLaVA-v1.5-13B[24] 20.56 6.07 3.44 2.85 8.19 sourcedmodels,wealsoexperimentwiththeGPT-4-vision-
HumanPerformance 82.23 0.12 0.36 1.61 80.14 previewAPI[50]onasubsetof500datapoints(duetocost
Table 4. Baseline Performance. (A) Prompt-only LLMs, (B) overheadsandOpenAIAPIrequestlimits). Table5shows
FineTunedLLMs,(C)Prompt-onlyMultimodalModels. Theta- thatGPT-4Vision[50]outperformsGPT-4significantlyon
blerepresentstheSequencescore(SS),clickpenalty(M p), Key theActionScorealongwithimprovingthesequencescore,
penalty (K p), Write Penalty (W p), and Action Score (AS). The whichweattributetothestrongreasoningabilitiesofGPT-
bestresultsforthe(SS)and(AS)arehighlighted. 4coupledwiththeimprovedvisualunderstandingcapabil-
ities of the GPT-4-vision-preview model [50]. These find-
Model SequenceScore(↑) ActionScore(↑) ingspavethewaytowardsexcitingnewresearchdirections
GPT-4[30] 36.42 12.77 on building multimodal models for long-horizon planning
GPT-4V[50] 39.43 20.76
andcodegeneration.
Table5.ResultsofGPT-4andGPT-4Vonasubsetof500samples.
Human performance over the task: OmniACT consists
of visually complicated tasks, and tests various types of
• Finetuned Generative LLM: We fine-tuned the
computer skills. In order to get a gauge of how well hu-
LLaMA-13B model and Vicuna-13B using QLoRa [10]
mansperform,wecollectevaluationdatafromhumaneval-
with rank 64 and scaling factor 16 for 300 steps to
uators. We split the test set uniformly amongst 10 human
generate the code given screen description from the
evaluators,andprovidedthemwiththescreenshotandtask
DetACTmoduleandtheinstruction.
instruction. Werecordtheactionstakenbytheannotators,
• Few-shot Generative Multimodal Models: As
and measure their performance on our predefined metrics
OmniACTispredominantlymultimodal,withamajority
(Table4). Wefindthatusersgenerallyexhibitahighlevel
of tasks being visually grounded, we conduct experi-
ofproficiencywhenattemptingmosttasksforthefirsttime.
ments with large multimodal models. Given the limited
However, there are instances where users face difficulties
research in this domain [47, 52], there is a scarcity
in successfully completing certain tasks. These are due to
of available multimodal models with significant size
factors including the user’s inability to fully comprehend
adept for this task. Here, we experiment with [24, 25],
the task, difficulties in grounding the task to the provided
providingasimilarpromptaswellasthescreenimage.
screenshot,oralackoffamiliaritywiththeUI.
WereportallresultsoverthetestsetinTable4.
8.ConclusionandFutureWork
7.ResultsandAnalysis
Autonomous virtual agents offer the potential to automate
AsshowninTable4,weexperimentwiththreedifferentcat- routinetasks,benefitinguserswithlimitedtechnicalexper-
egoriesofmodels,namelyPrompt-basedLLMs,Fine-tuned tise. Tosolvethistask,weintroduceOmniACT,aunique
LLMs,andPrompt-basedMultimodalModels.GPT-4isthe dataset of 9.8K human-labeled data points. OmniACT
best-performing approach, scoring higher on the sequence benchmarks autonomous agents across a range of tasks on
scoreandinvokinglowerpenaltiesoncoordinatepredicting web and desktop applications. LLM-based agents, like
GPT-4, achieve a respectable action score of 11.6 on our
and text input. For prompt-only LLMs, the GPT-3.5-turbo
dataset. However, OmniACT presents a challenge for the
andGPT-4modelsoutperformtheotherLLMbaselines,in-
current state-of-the-art language and multimodal models.
cluding the LLaMA [43] and Vicuna [7] models. We ob-
It provides a direction for future research on foundational
servethatCodeLLaMA-34B[35],whichistrainedforcode
multimodal models that seamlessly integrate language
generation, also achieves a higher performance than other
and visual understanding of computer screens and stands
modelsofthesamesizeatpredictingtheactionsequences. poised to drive the next wave of advancements in gener-
Fine-tuned models also perform much better than their alist autonomous agents offering omnipotent assistance to
few-shot prompt-only counterparts. Fine-tuning substan- humans.
8References aginguseractionsforsemanticunderstandingofuserinter-
faces,2021. 2
[1] Pyautogui: A cross-platform gui automation python mod-
[15] ZhichengHuang,ZhaoyangZeng,BeiLiu,DongmeiFu,and
ule for human beings. https://github.com/
JianlongFu. Pixel-bert: Aligningimagepixelswithtextby
asweigart/pyautogui,2023. 2,3
deepmulti-modaltransformers,2020. 2
[2] Waseem AlShikh, Manhal Daaboul, Kirk Goddard, Brock
[16] PeterCHumphreys,DavidRaposo,TobiasPohlen,Gregory
Imel, Kiran Kamble, Parikshith Kulkarni, and Melisa Rus-
Thornton,RachitaChhaparia,AlistairMuldal,JoshAbram-
sak. Becomingself-instruct: introducingearlystoppingcri-
son,PetkoGeorgiev,AdamSantoro,andTimothyLillicrap.
teriaforminimalinstructtuning,2023. 2,8
A data-driven approach for learning to control computers.
[3] ChongyangBai,XiaoxueZang,YingXu,SrinivasSunkara,
In International Conference on Machine Learning, pages
AbhinavRastogi,JindongChen,andBlaiseAguerayArcas.
9466–9482.PMLR,2022. 3
Uibert: Learninggenericmultimodalrepresentationsforui
[17] Geunwoo Kim, Pierre Baldi, and Stephen McAleer. Lan-
understanding,2021. 2
guage models can solve computer tasks. arXiv preprint
[4] Pratyay Banerjee, Shweti Mahajan, Kushal Arora, Chitta
arXiv:2303.17491,2023. 3
Baral, and Oriana Riva. Lexi: Self-supervised learning of
[18] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
theuilanguage,2023. 2,3
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-
[5] AndreaBurns,DenizArsan,SanjnaAgrawal,RanjithaKu-
head, Alexander C. Berg, Wan-Yen Lo, Piotr Dolla´r, and
mar,KateSaenko,andBryanAPlummer. Mobileapptasks
RossGirshick. Segmentanything,2023. 7
with iterative feedback (motif): Addressing task feasibility
[19] YannLeCun. Apathtowardsautonomousmachineintelli-
ininteractivevisualenvironments. 3
genceversion0.9.2,2022-06-27. 2022. 2
[6] Xingyu Chen, Zihan Zhao, Lu Chen, Jiabao Ji, Danyang
[20] Gang Li and Yang Li. Spotlight: Mobile ui understanding
Zhang, Ao Luo, Yuxuan Xiong, and Kai Yu. Websrc: A
usingvision-languagemodelswithafocus,2023. 2
datasetforweb-basedstructuralreadingcomprehension. In
[21] Yang Li, Jiacong He, Xin Zhou, Yuan Zhang, and Jason
Proceedingsofthe2021ConferenceonEmpiricalMethods
Baldridge. Mappingnaturallanguageinstructionstomobile
inNaturalLanguageProcessing,pages4173–4185,2021. 3
uiactionsequences,2020. 3
[7] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-
[22] YangLi,GangLi,LuhengHe,JingjieZheng,HongLi,and
haoWu,HaoZhang,LianminZheng,SiyuanZhuang,Yong-
Zhiwei Guan. Widget captioning: Generating natural lan-
hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.
guagedescriptionformobileuserinterfaceelements,2020.
Xing. Vicuna: An open-source chatbot impressing gpt-4
2
with90%*chatgptquality,2023. 2,7,8
[23] YangLi,GangLi,XinZhou,MostafaDehghani,andAlexey
[8] Biplab Deka, Zifeng Huang, Chad Franzen, Joshua Hib-
Gritsenko. Vut: Versatile ui transformer for multi-modal
schman,DanielAfergan,YangLi,JeffreyNichols,andRan-
multi-taskuserinterfacemodeling,2021. 2
jithaKumar. Rico: Amobileappdatasetforbuildingdata-
drivendesignapplications. InProceedingsofthe30thAn- [24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee.
nualACMSymposiumonUserInterfaceSoftwareandTech- Improvedbaselineswithvisualinstructiontuning,2023. 8
nology,page845–854,NewYork,NY,USA,2017.Associa- [25] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
tionforComputingMachinery. 2 Visualinstructiontuning,2023. 8
[9] Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel [26] Pan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei
Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Chang,YingNianWu,Song-ChunZhu,andJianfengGao.
Towards a generalist agent for the web. arXiv preprint Chameleon: Plug-and-play compositional reasoning with
arXiv:2306.06070,2023. 2,3,4,5 largelanguagemodels,2023. 3
[10] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke [27] Chenyang Lyu, Minghao Wu, Longyue Wang, Xinting
Zettlemoyer. Qlora: Efficientfinetuningofquantizedllms, Huang, Bingshuai Liu, Zefeng Du, Shuming Shi, and
2023. 2,8 ZhaopengTu. Macaw-llm:Multi-modallanguagemodeling
[11] HirokiFuruta,OfirNachum,Kuang-HueiLee,YutakaMat- withimage,audio,video,andtextintegration.arXivpreprint
suo, Shixiang Shane Gu, and Izzeddin Gur. Multimodal arXiv:2306.09093,2023. 3
web navigation with instruction-finetuned foundation mod- [28] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,
els. arXivpreprintarXiv:2305.11854,2023. 2 LongOuyang,ChristinaKim,ChristopherHesse,Shantanu
[12] TanmayGuptaandAniruddhaKembhavi. Visualprogram- Jain, Vineet Kosaraju, William Saunders, et al. Webgpt:
ming: Compositionalvisualreasoningwithouttraining. In Browser-assistedquestion-answeringwithhumanfeedback.
ProceedingsoftheIEEE/CVFConferenceonComputerVi- 3
sionandPatternRecognition,pages14953–14962,2023. 3 [29] OpenAI. Introducingchatgpt,2023. 8
[13] IzzeddinGur,UlrichRueckert,AleksandraFaust,andDilek [30] OpenAI. Gpt-4technicalreport,2023. 2,7,8
Hakkani-Tur.Learningtonavigatetheweb.InInternational [31] KishorePapineni,SalimRoukos,ToddWard,andWei-Jing
ConferenceonLearningRepresentations,2018. 2 Zhu. Bleu: a method for automatic evaluation of machine
[14] ZechengHe,SrinivasSunkara,XiaoxueZang,YingXu,Li- translation.InProceedingsofthe40thannualmeetingofthe
juanLiu,NevanWichers,GabrielSchubiner,RubyLee,Jin- AssociationforComputationalLinguistics, pages311–318,
dongChen,andBlaiseAgu¨erayArcas. Actionbert: Lever- 2002. 5,6
9[32] Christopher Rawles, Alice Li, Daniel Rodriguez, Oriana uisummarizationwithmultimodallearning.InThe34thAn-
Riva,andTimothyLillicrap. Androidinthewild: Alarge- nualACMSymposiumonUserInterfaceSoftwareandTech-
scaledatasetforandroiddevicecontrol,2023. 3 nology,pages498–510,2021. 2
[33] Shuo Ren, Daya Guo, Shuai Lu, Long Zhou, Shujie [45] LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,
Liu, Duyu Tang, Neel Sundaresan, Ming Zhou, Ambro- JingsenZhang,ZhiyuanChen,JiakaiTang,XuChen,Yankai
sio Blanco, and Shuai Ma. Codebleu: a method for Lin, et al. A survey on large language model based au-
automatic evaluation of code synthesis. arXiv preprint tonomous agents. arXiv preprint arXiv:2308.11432, 2023.
arXiv:2009.10297,2020. 5 2
[34] Baptiste Rozie`re, Jonas Gehring, Fabian Gloeckle, Sten [46] WenhuiWang,FuruWei,LiDong,HangboBao,NanYang,
Sootla,ItaiGat,XiaoqingEllenTan,YossiAdi,JingyuLiu, andMingZhou. Minilm: Deepself-attentiondistillationfor
Tal Remez, Je´re´my Rapin, Artyom Kozhevnikov, Ivan Ev- task-agnosticcompressionofpre-trainedtransformers. Ad-
timov,JoannaBitton,ManishBhatt,CristianCantonFerrer, vancesinNeuralInformationProcessingSystems,33:5776–
AaronGrattafiori,WenhanXiong,AlexandreDe´fossez,Jade 5788,2020. 7
Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nico- [47] Xiao Wang, Guangyao Chen, Guangwu Qian, Pengcheng
lasUsunier,ThomasScialom,andGabrielSynnaeve. Code Gao, Xiao-Yong Wei, Yaowei Wang, Yonghong Tian, and
llama:Openfoundationmodelsforcode,2023. 7 Wen Gao. Large-scale multi-modal pre-trained models: A
[35] Baptiste Rozie`re, Jonas Gehring, Fabian Gloeckle, Sten comprehensivesurvey,2023. 8
Sootla,ItaiGat,XiaoqingEllenTan,YossiAdi,JingyuLiu, [48] ZhouWang,AlanCBovik,HamidRSheikh,andEeroPSi-
Tal Remez, Je´re´my Rapin, Artyom Kozhevnikov, Ivan Ev- moncelli. Imagequalityassessment: fromerrorvisibilityto
timov,JoannaBitton,ManishBhatt,CristianCantonFerrer, structuralsimilarity.IEEEtransactionsonimageprocessing,
AaronGrattafiori,WenhanXiong,AlexandreDe´fossez,Jade 13(4):600–612,2004. 7
Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nico-
[49] NancyXu,SamMasling,MichaelDu,GiovanniCampagna,
lasUsunier,ThomasScialom,andGabrielSynnaeve. Code
Larry Heck, James Landay, and Monica Lam. Grounding
llama:Openfoundationmodelsforcode,2023. 8
open-domaininstructionstoautomatewebsupporttasks. In
[36] Peter Shaw, Mandar Joshi, James Cohan, Jonathan Berant,
Proceedingsofthe2021ConferenceoftheNorthAmerican
PanupongPasupat,HexiangHu,UrvashiKhandelwal,Ken-
Chapter of the Association for Computational Linguistics:
tonLee,andKristinaToutanova. Frompixelstouiactions:
HumanLanguageTechnologies,pages1022–1032,2021. 3
Learningtofollowinstructionsviagraphicaluserinterfaces.
[50] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,
arXivpreprintarXiv:2306.00245,2023. 2
Chung-ChingLin,ZichengLiu,andLijuanWang.Thedawn
[37] TianlinShi, AndrejKarpathy, LinxiFan, JonathanHernan-
oflmms:Preliminaryexplorationswithgpt-4v(ision).arXiv
dez,andPercyLiang. Worldofbits: Anopen-domainplat-
preprintarXiv:2309.17421,9,2023. 2,8
formforweb-basedagents. InInternationalConferenceon
[51] Shunyu Yao, Howard Chen, John Yang, and Karthik
MachineLearning,pages3135–3144.PMLR,2017. 2,3,4
Narasimhan.Webshop:Towardsscalablereal-worldwebin-
[38] Abishek Sridhar, Robert Lo, Frank F Xu, Hao Zhu, and
teractionwithgroundedlanguageagents. AdvancesinNeu-
ShuyanZhou.Hierarchicalpromptingassistslargelanguage
ralInformationProcessingSystems,35:20744–20757,2022.
modelonwebnavigation.arXivpreprintarXiv:2305.14257,
3
2023. 3
[52] Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun,
[39] Liangtai Sun, Xingyu Chen, Lu Chen, Tianle Dai, Zichen
TongXu,andEnhongChen. Asurveyonmultimodallarge
Zhu, andKaiYu. Meta-gui: Towardsmulti-modalconver-
languagemodels,2023. 8
sational agents on mobile gui. In Proceedings of the 2022
[53] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Wein-
ConferenceonEmpiricalMethodsinNaturalLanguagePro-
berger, andYoavArtzi. Bertscore: Evaluatingtextgenera-
cessing,pages6699–6712,2022. 3
tionwithbert,2020. 5
[40] D´ıdac Sur´ıs, Sachit Menon, and Carl Vondrick. Vipergpt:
[54] Zhuosheng Zhang and Aston Zhang. You only look at
Visualinferenceviapythonexecutionforreasoning,2023.3
screens:Multimodalchain-of-actionagents,2023. 3
[41] Writer Engineering team. InstructPalmyra-30b : Instruct
tuned Palmyra-Large model. https://dev.writer. [55] WayneXinZhao,KunZhou,JunyiLi,TianyiTang,Xiaolei
com,2023. 2,8 Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie
Zhang,ZicanDong,etal. Asurveyoflargelanguagemod-
[42] WriterEngineeringteam. Palmyra-baseParameterAutore-
els. arXivpreprintarXiv:2303.18223,2023. 7
gressiveLanguageModel.https://dev.writer.com,
2023. 7 [56] ShuyanZhou,UriAlon,SumitAgarwal,andGrahamNeu-
big. Codebertscore: Evaluating code generation with pre-
[43] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste trained models of code. arXiv preprint arXiv:2302.05527,
Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, Aure- 2023. 5
lien Rodriguez, Armand Joulin, Edouard Grave, and Guil- [57] Shuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert
laume Lample. Llama: Open and efficient foundation lan- Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel
guagemodels,2023. 2,7,8 Fried, Uri Alon, et al. Webarena: A realistic web envi-
[44] Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi ronment for building autonomous agents. arXiv preprint
Grossman,andYangLi. Screen2words: Automaticmobile arXiv:2307.13854,2023. 2,3,4
10Appendix
In this paper, we present a novel dataset OmniACT models that will heavily rely on the screen image in addi-
thataidsinbuildingmorerobustmultimodalgeneralistau- tiontotheelementsextractedfromDetACTmodule.
tonomousagentsforDesktopandWeb. Alongwiththis,we From the histograms at the top and right of the screens
alsoproposeanewcontinuousscalemetricthatallowsbet- infigure5, wenoticethatthedatasetisnotheavilybiased
terassessmentofactionsonacomputerscreenandtheDe- towards any particular region of the screen. This implies
tACT module which we integrate into multiple LLMs and thatmodelsthatarelikelytoattendtoonlycertainregions
(VisionLanguageModels)VLMsthatcanextracttheuse- of the screen or predict the coordinates in the same region
ful features from the screen image and help us benchmark of the screen will not have a good performance over the
thedataset. Wepresentthefollowingitemsthatgivefurther dataset.
insightintothedatasetandexperimentsweperformed:
A Listofapplicationsandwebsites
B AdditionalQuantitativeResults count
200
C AnnotationProcess
D DatasetandMetadataFormat 1
150
E ParametersforModelTraining 0.8
F LimitationsandBroaderImpact 0.6 100
G EthicsStatement y
0.4
H Sampleoftaskexecution 50
I Prompts for DetACT and Baselines and Sample Re- 0.2
sponses 00 0.2 0.4 0.6 0.8 1 0
x
A.Listofapplicationsandwebsites
The complete list of applications and websites chosen for count
the dataset are listed in table 6. We ensure that the data
40
is equitably represented across all domains and operating 1
systems. We choose desktop applications that are more 0.8 30
commonly used for each of the operating systems. The
0.6
commonly used applications we select are representative y 20
of eclecticdemographics. Wealso select afew third-party 0.4
10
applicationsthatuserscommonlyuseonalloperatingsys- 0.2
tems.Toavoidredundancyandensurediversityinourtasks, 00 0.2 0.4 0.6 0.8 1 0
werefrainfromchoosingthesamesetofapplicationsforall x
Figure5.Screenregionattentioninoutputscriptsfordesktopand
threeoperatingsystems. Forwebsites,weselectourcandi-
webdata
date set based on user intent. This is sub-categorized into
sixdomains-(1)Shopping,(2)Entertainment,(3)Service, B.2.ImpactofDetACTModule
(4)Government,(5)Travel,and(6)Health.
Inthissetofexperimentswepresenttheresultswithoutus-
ingourDetACTmoduleandtheircomparisonwiththecor-
B.AdditionalDiscussions
responding model with DetACT outputs. We can clearly
observe in table 7 that even models like GPT-4 perform
B.1.ScreenRegionAttention
close to zero when not prompted by DetACT as they are
Weperformanin-depthanalysisofourdatasettoseewhere only language-based. This highlights the importance of
thecoordinatepredictionsaregravitated. Wefindinterest- well-equippedscreenparsingmodulessuchasourDetACT
ing insights about the parts of the screen that are attended framework that effectively extract multimodal information
towards. Most of the predictions for the desktop data are in the form of text, which is an essential cue for every
focused either on the center of the screen or the bottom model.
corners. While most of the web interactions are pointing
B.3.SequencelengthversusPerformance
towardthebottomhalfofthescreen,especiallyconcentrat-
ing toward the corners. This is depicted in figure 5. This We analyze the performance of predicting the sequence of
presents some exciting directions for training multimodal actions as a function of sequence length. Specifically, we
11Desktop Websites
MacOS Linux Windows
Mail Music AppStore Audible Grammarly AMC Coursera NPS
Maps News Calculator Camera Outlook Apartments FlightAware NYGov
Message Photos Calendar Desktop Spotify Aramex GoogleCareers Samsung
Prime Preview Clock Files WeChat Armani HealthLine TripAdvisor
Terminal Reminder Expedia Settings Calendar Asics Hertz UHaul
Weather Stocks Finder Todo Files AT&T Indeed Udemy
Zoom SystemPreferences iBooks TextEditor Settings BestBuy Instacart UnitedAirlines
Desktop VLC WindowsStore Booking MayoClinic UPS
Cmu.edu MintMobile Walmart
Table6.ListofapplicationsfordesktopandwebinOmniACT
Sequence Action use a custom PyQt5 tool that helps us create the bounding
Model
Score Score boxes using a click-and-drag mechanism. This enhances
GPT-4(w/oDetACT) 31.87 0.32 theboundingboxcreationprocess,makingitmoreintuitive
GPT-4+DetACT 32.75 11.60 and user-friendly. It also enables us to encompass interac-
tiveregionsofUIelements. Forinstance,whenclickingon
Table 7. Performance of DetACT Module with GPT-4 Series of
asearchbar,userscaninputtext,whileclickingonasearch
Models
baricon(suchasamagnifyingglass)doesnotactivatethe
typing area. Next, for the website, we use the DOM from
HTML and draw bounding boxes using JavaScript. Both
these processes ensure a high degree of accuracy and are
donebytheauthorsofthepaper. Samplesforeachofthem
are shown in figure 7. After the bounding boxes are es-
tablished, the authors exchange their work and review the
boxescreatedbyeachother.
C.2.AnnotatorGuidelines
For labeling the bounding boxes for both desktop applica-
tionsandwebsites,weleverageMTurk. Thisisdepictedin
figure8. MTurkworkersarepresentwithasinglebounding
box on the screen removing the other boxes and are then
askedtolabeltheboxbasedonthefunctionalityoftheUI
Figure6. SequencepredictionaccuracyofGPT4onthetestset.
Theperformancedecreasesasthesequencelengthincreases. element in context to the screen. They are asked to come
upwithlessthana5-worddescriptionorlabelforeachbox.
Everyboxisdoublyannotatedkeepingthemoreappropri-
want to check whether it becomes difficult for models to ate label during the final filtering. Every MTurk worker is
predict the sequence correctly when the target sequence is paidatanhourlyrateof$20,whichiswellabovePennsyl-
longer. Figure6showstheaccuracyofsequenceprediction vania’sminimumwagerate.
as a function of sequence length for GPT-4 on the test set. Nextforformulatingthetasks,werecruitstudentvolun-
Only when the entire sequence of actions predicted by the teerswhohavebasicknowledgeofPythonandcoding. We
model is the same as the gold sequence of actions, do we traineverystudentvolunteeraboutthePyAutoGUIusagein
give the score as 1. We report the accuracy percentage in athirty-minutesessionbeforestartingthetaskformulation
the table. From table 6, we observe that the model perfor- process.Everystudentisthenhandedoverasampleoftasks
manceisbetterforshortersequencesandgetsworseasthe that can be created using that screen and is asked to come
sequencelengthincreases. upwithmoretasksalongwiththeirscripts,whichareonly
a few lines. Every volunteer is asked to come up with as
C.AnnotationProcess manytasksaspossibleinonehouralongwithitslinguistic
variations to ensure diversity in the language of the tasks.
C.1.CreatingtheBoundingBoxes
Everyvolunteerispaidanhourlywagerateof$25, which
To create the bounding boxes, we use two different tech- againsurpassesthestate’sminimumrate.
niques for desktop and web data. For desktop data, we Webuildscriptstotakeinthetasksgeneratedbythestu-
12Figure7.AnnotationProcessforcreatingboundingboxesfordesktopapplicationsandwebsites.
C.3.HumanFeedbackandEvaluations
To evaluate the human baselines, we again recruit student
volunteers. Thistimethevolunteersarenotgiventherefer-
enceoutputscriptandonlyapairofanimageandthetask
description.Thestudentsareaskedtothencomeupwithan
automation script using a few samples as references. The
volunteersusetheirexperienceandintuitiontocomprehend
thescreenandthefunctionalitiesandwritescriptsbasedon
that. Here the student worker is compensated at an hourly
rateof$25,wellexceedingthestate’sminimumwage.
D.DatasetandMetadataFormat
Thedatasetconsistsofthreefiles,oneforeachsplit. Every
file has a list of task numbers for that particular split. Ev-
erytaskdirectoryconsistsof(1)image.pngfile,whichisa
screenshotofthescreen,and(2)task.txtfilewhichcontains
thetaskdescriptionandthegroundtruthoutputscript. We
also include another file, (3) box.json, which is part of the
metadataandismeanttobeonlyusedforrunningtheeval-
uationsthatrequirethecoordinatesoftheboxesandnotfor
performingthetaskoraidingthemodelinanyway.
E.ParametersforModelTraining
Figure8. ScreenshotsfromMTurkPortalthatdepictthelabeling
processforallboxes. We enlist the model specifications for models we run for
DetACTfilteringandbaselinesusingDetACTintable8.
F.BroaderImpact
dents and run over the screens to ensure the tasks are exe- Thebroaderimpactofourintricatelycurateddatasetismul-
cutable.Oncedone,theauthorsperformafinalpasstokeep tifaceted.Firstly,ourdatasetholdsthepotentialtoempower
onlythehigh-qualitytasks. the technologically illiterate, providing them with a more
13Model Hyperparameters
DetACTFiltering GPT-4 temperature=0.1
LLMPromptOnlyBaselines LLaMA-7B maxnewtokens=400,
Vicuna-7B temperature=0.8,repetitionpenalty=1.1
LLaMA-13B maxnewtokens=400,
Vicuna-13B temperature=0.8
Palmyra-Instruct30B temperature=0.7
maxnewtokens=400,
CodeLLaMA-34B
temperature=0.7
Palmyra-X43B temperature=0.7
GPT-3.5-turbo
temperature=0.3
GPT-4
LLMFine-tunedbaselines LLaMA-13BFT LoRArank=64,scalingfactor=16,
Vicuna-13BFT LoRAdropout=0.05,
batchsize=4,learningrate=5e-5
MultimodalModelPromptOnlyBaselines LLaVA-v1.5-7B temperature=0.2,
LLaVA-v1.5-13B maxnewtokens=512
Table8.HyperparametersusedforeachofthemodelsforDetACTfilteringandBaselines.
automatedandaccessiblewaytonavigateandoperatecom- Reliance on closed models like GPT-4V poses integration
puters. challenges due to high costs and time constraints in real-
Additionally, our dataset opens avenues for ground- worldsystems. Despiteeffortsforequalrepresentationand
breaking UI grounding research, enabling a deeper under- datacollectionwithoutpersonalinformation,biasesmaybe
standing of user interactions at the operating system level. introducedasthedatasetisexclusivelyinEnglish, andthe
This,inturn,contributestoadvancingthefieldofartificial human-curatedcontentmayhavetemporalbiases.
intelligence (AI) by expanding the scope of AI to encom-
pass OS-level interactions. The dataset’s rich content fur- H.Sampleoftaskexecution
ther facilitates the enhancement of large language models
We present figures 9 containing screenshots and corre-
(LLMs),broadeningtheirskillsetsandimprovingtheirca-
spondingoutputscriptsforaselectedtask. Infigure9,the
pabilitiesincomprehendingandrespondingtodiverseuser
taskistakenfromaMacOSnativeapplication,Books.Here
inputs.
wehaveabookchapteropenedalongwiththetaskdescrip-
This dataset serves as a foundation for the develop-
tion to highlight the last line in green color and scroll to
ment of multimodal agents capable of operating seam-
thenextpage. Followedbythetaskaredescriptivescreen-
lesslyacrossvariousscreensandexecutingtaskswithlong-
shots representing every action in the sequence along with
term horizons. This versatility in application positions the
itsoutput.
datasetasavaluableresourceforthecreationofinnovative
AI-drivensolutionsthattranscendtraditionallimitations.
I. Prompts for DetACT and Baselines and
An additional and impactful use case involves building
SampleResponses
assistive tools for handicapped individuals who may face
challenges in operating computer systems or performing
In the following pages, we present the full length prompts
routinetasksthatinvolvehuman-computerinteraction. By
thatweuseforthefollowingtasks:
tailoringtechnologiestomeettheneedsofindividualswith
• Listing1: FilteringUIelementsinDetACTModule
varyingabilities,ourdatasetcontributestothedevelopment
• Listing2: RunningbaselinesmodelsusingDetACT
ofassistivetechnologiesthatenhanceaccessibilityandim-
provethequalityoflifeforadiverserangeofusers.
Insummary,thebroaderimpactofourmeticulouslycu-
rated dataset manifests in its potential to drive advance-
ments in technology accessibility, UI grounding research,
AI capabilities, and assistive tools, ultimately fostering a
moreinclusiveandempoweredtechnologicallandscape.
G.EthicsStatement
This work introduces a valuable dataset, yet we recognize
a few limitations that exist. State-of-the-art models like
GPT-4,mayexhibitsusceptibilitytohallucinationsandbias
towards specific data types, hindering broad applicability.
141 pyautogui.click(789,765)
2 pyautogui.dragTo(812, 799, button="left") 3 pyautogui.rightClick(1081, 762)
4 pyautogui.write("green") 5 pyautogui.press("enter")
6 pyautogui.hscroll(10)
Figure9.TaskDemoalongwithintermediatestepsandcorrespondingscriptsforeveryactionsequence.
15Listing1.PromptforFilteringUIelementsinDetACTModule
Given below are the UI elements extracted from the screen. You have to filter the elements
for this UI screen that are relevant for carrying out the task given below.
Make sure to filter the UI elements that may be helpful to carry out the task and mention
the element description and the corresponding coordinates for the task.
Format for every element would be in the form of (Element Label, X-Coordinate,
Y-Coordinate).
[IMPORTANT!!] Only remove the elements if you are sure that they will not help the task.
[IMPORTANT!!] Follow the output structure strictly as given in the below example and output
nothing else other than the required output.
Sample Task:
{SAMPLE_TASK}
Sample UI Elements:
{LIST_OF_SAMPLE_UI_ELEMENTS}
Sample Filtered UI Elements:
{LIST_OF_SAMPLE_FILTERED_UI_ELEMENTS}
Given Task:
{TASK}
Given UI Elements:
{LIST_OF_UI_ELEMENTS}
16Listing2.PromptforbaselinesmodelsusingDetACToutputs
You are an excellent robotic process automation agent who needs to generate a pyautogui
script for the tasks given to you. You will be given UI elements on the screen that you
can interact with as a list of tuples, where each tuple will be of the form [UI Text, X
coordinate, Y coordinate]. You will also be given an example to help with the format of
the script that needs to be generated.
[IMPORTANT!!] Stick to the format of the Output scripts in the example
[IMPORTANT!!] Use only the functions from API docs
[IMPORTANT!!] Follow the Output format strictly. Only write the script and nothing else.
Here is the API reference for generating script:
def click(x=moveToX, y=moveToY):
"""takes the mouse to location (moveToX, moveToY) and does a left click
Example:
High Level Goal: Click at co-ordinate (150, 230).
Python script:
import pyautogui
pyautogui.click(150, 230)
"""
pass
def rightClick(x=moveToX, y=moveToY):
"""takes the mouse to location (moveToX, moveToY) and does a right click
Example:
High Level Goal: Right click at co-ordiante (350, 680).
Python script:
import pyautogui
pyautogui.rightClick(350, 680)
"""
pass
def doubleClick(x=moveToX, y=moveToY):
"""takes the mouse to location (moveToX, moveToY) and does a right click
Example:
High Level Goal: Right click at co-ordiante (350, 680).
Python script:
import pyautogui
pyautogui.rightClick(350, 680)
"""
pass
def scroll(clicks=amount_to_scroll):
"""scrolls the window that has mouse pointer by float value (amount_to_scroll)
Example:
High Level Goal: Scroll screen by (30).
Python script:
import pyautogui
pyautogui.scroll(30)
"""
pass
def hscroll(clicks=amount_to_scroll):
"""scrolls the window that has mouse pointer horizontally by float value
(amount_to_scroll)
Example:
High Level Goal: Scroll screen horizontally by (30).
17Python script:
import pyautogui
pyautogui.hscroll(30)
"""
pass
def dragTo(x=moveToX, y=moveToY, button=holdButton):
"""drags the mouse to (moveToX, moveToY) with (holdButton) pressed. holdButton can be
’left’, ’middle’ or ’right’.
Example:
High Level Goal: drag the screen from current position to (450, 600) with left click of
the mouse.
Python script:
import pyautogui
pyautogui.dragTo(450, 600, ’left’)
"""
pass
def pyautogui.moveTo(x=moveToX, y=moveToY)
"""take the mouse pointer to (moveToX, moveToY)
Example:
High Level Goal: hover the mouse pointer to (450, 600).
Python script:
import pyautogui
pyautogui.moveTo(450, 600)
"""
pass
def write(str=stringType, interval=secs_between_keys):
"""writes the string wherever keyboard cursor is at the function calling time with
(secs_between_keys) seconds between characters
Example:
High Level Goal: Write "Hello world" with 0.1 seconds rate
Python script:
import pyautogui
pyautogui.write("Hello world", 0.1)
"""
pass
def press(str=string_to_type):
"""simulate pressing a key down and then releasing it up. Sample keys include ’enter’,
’shift’, arrow keys, ’f1’.
Example:
High Level Goal: Press enter key now
Python script:
import pyautogui
pyautogui.press("enter")
"""
pass
def hotkey(*args = list_of_hotkey):
"""Keyboard hotkeys like Ctrl-S or Ctrl-Shift-1 can be done by passing a list of key
names to hotkey(). Multiple keys can be pressed together with a hotkey.
Example:
High Level Goal: Use Ctrl and V to paste from clipboard
Python script:
import pyautogui
pyautogui.hotkey("ctrl", "v")
18"""
pass
Here are a few examples for generating the script and format to be followed:
Example 1:
{RETREIVED_EXAMPLE_1}
Example 2:
{RETREIVED_EXAMPLE_2}
Example 3:
{RETREIVED_EXAMPLE_3}
Example 4:
{RETREIVED_EXAMPLE_4}
Example 5:
{RETREIVED_EXAMPLE_5}
Here are the Textual Elements found on the screen along with their coordinates:
{TEXTUAL_ELEMENTS_FROM_DETACT}
Here are the Icon/Image Elements found on the screen along with their coordinates:
{ICON_ELEMENTS_FROM_DETACT}
Here are the Color Elements found on the screen along with their coordinates:
{COLOR_ELEMENTS_FROM_DETACT}
Based on the list of UI elements and the examples above, generate the pyautogui script for
the following task:
{TASK}
19