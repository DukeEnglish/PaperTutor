Opening Cabinets and Drawers in the Real World
using a Commodity Mobile Manipulator
Arjun Gupta⋆ Michelle Zhang⋆ Rishik Sathua Saurabh Gupta
University of Illinois at Urbana-Champaign
https://arjung128.github.io/opening-cabinets-and-drawers
Execution
Environments
Fig. 1: This paper presents a system for opening cabinets and drawers in novel real world environments using a commodity mobile manipulator. Top. An
exampleexecutionofoursysteminteractingwithanovelobjectinanunseenenvironment.Weincludethefollowingframes:beforenavigation,afternavigation,
pre-grasppose,duringmanipulation,andattheendofmanipulation.Bottom.Avisualizationoftheother12unseenenvironmentsusedfortestingoursystem.
Theseareacross10buildingsforatotalof31uniquearticulatedobjectsintherealworld.
Abstract—Pulling open cabinets and drawers presents many our system. We will open source code and models for others to
difficulttechnicalchallengesinperception(inferringarticulation replicate and build upon our system.
parameters for objects from onboard sensors), planning (pro-
ducingmotionplansthatconformtotighttaskconstraints),and I. INTRODUCTION
control (making and maintaining contact while applying forces
ontheenvironment).Inthiswork,webuildanend-to-endsystem This paper develops and evaluates a system for pulling
that enables a commodity mobile manipulator (Stretch RE2) to open cabinets and drawers in diverse previously unseen real
pullopencabinetsanddrawersindiversepreviouslyunseenreal world environments (Figure 1). Opening articulated objects
world environments. We conduct 4 days of real world testing
like cabinets and drawers presents hard technical challenges
of this system spanning 31 different objects from across 13
spanning perception, planning, and last centimeter control.
differentrealworldenvironments.Oursystemachievesasuccess
rate of 61% on opening novel cabinets and drawers in unseen These include accurate perception of object handles that are
environmentszero-shot.Ananalysisofthefailuremodessuggests typically small and shiny, whole-body planning to drive the
that errors in perception are the most significant challenge for end-effector along the task constraint (i.e. trajectory dictated
1
4202
beF
72
]OR.sc[
1v76771.2042:viXraby the articulating handle), and dealing with execution errors (SectionIV-CandFigure11).Finally,errorsfromearlierparts
in a task with low tolerances. All of these pieces have been of the pipeline (e.g. calibration and navigation) compound to
studied at length in isolation [4, 16, 35]. Yet, how these lead to failures in grasping of the handle (Section V). This
modules interact with one another and what matters for suc- motivatestheneedforalearnedvisuallast-centimetergrasping
cessfullycompletingthetaskarenotwellunderstood.End-to- policy to correct for compounding errors.
end learning via imitation or reinforcement circumvents these Insummary,thispaperpresentsthedesignandevaluationof
issues but is itself difficult because of the sample efficiency a mobile manipulation system to open cabinets and drawers
of learning and the unavailability of large-scale datasets for using a commodity mobile manipulator. Large scale testing
learning [12]. We take a modular approach and bring to bear across 13 test sites in 10 buildings and 31 different cabinets
state-of-the-art modules for perception and planning with a and drawers reveals guidance for practitioners aiming to build
specificfocusonstudyinghowthedifferentmodulesplaywith similar systems. We will release code and models for others
one another. to replicate our results and build upon our system.
Specifically, for perception we extend a Mask RCNN
II. RELATEDWORK
model [13] to also output articulation parameters. For plan-
ning, we extend SeqIK, the recently proposed trajectory opti- A. Predicting Articulation Parameters
mizationframework[12]toproducewholebodymotionplans. Many recent works have sought to predict articulation pa-
Contrary to our expectation, just putting these two modules rameters for articulated objects. Researchers have constructed
together did not lead to a successful system because of last datasets from simulation [7, 11, 19, 38], real world images
centimeter errors in execution. Even slight inaccuracies in [15, 35], and real world 3D scans [12]. Some works study
navigation and extrinsic camera calibration cause the end- the use of different input modalities (RGB images [35], point
effector to just be slightly off from the handle preventing clouds [21], RGB-D images [15]) and sources of supervi-
handle grasping. To tackle this problem, we close the loop sion [28]. Most works only focus on predicting articulation
with proprioceptive feedback: predictions from visual sensors parameters and ignore the site for interaction i.e. the handle,
getstheend-effectorinthevicinityofthehandleandtheactual which is crucial for our application. On the other hand, some
grasping is done by leveraging contact sensors in the gripper works purely aim to predict which pixels, from an image of
and the arm. an articulated object, are good for interaction [24, 25, 37].
Two other unique aspects of our study are a) the use of However, these works are largely evaluated in simulation, and
a commodity mobile manipulator and b) extensive testing in becausenopredictionismadeforotherarticulationparameters
previouslyunseendiverserealworldenvironments.Manypre- (e.g. radius), these works cannot be used to generate motion
viouspapershavedemonstratedspecializedsystemsforsimilar plans which fully open the objects. Work from Sun et al. [35]
problems [14]. Constructing specialized hardware for a given adds additional heads to Mask RCNN to predict articulation
task can simplify the task at hand, at the cost of generality parameters in 3D. We also add additional heads to Mask
to other tasks. Therefore, we test our proposed system using RCNN but rather than directly predicting 3D outputs using
the Stretch RE2, a general purpose commodity mobile ma- RGB-D input, we take a 2D prediction from RGB and 3D
nipulator, without any hardware modifications. Furthermore, lifting using depth image approach.
thistestingisconductedacross31differentarticulatedobjects
B. Generating Motion Plans
in 10 different buildings. Testing sites include offices, class-
rooms, apartments, office kitchenettes, and lounges. Figure 1 Opening doors and drawers requires the end-effector to
showsthevarietyinappearanceandarticulationparametersin conform to the constraint defined by the object handle’s
our experiments. Our system achieves a 61% success rate in trajectory (which in turn is determined by the location of
a zero-shot manner across this challenging testbed. the articulation joint and the handle). Depending on the robot
This broad study has allowed us to answer numerous morphology, robot configurations that conform to this end-
questions about deploying such a system in the real world: effector constraint might represent a measure zero set. This
a) what are the current bottlenecks in deploying a system for makes it challenging to directly use sampling-based motion
articulating objects in novel environments, b) how accurate planners [17, 20]. Past research has therefore developed spe-
should motion plans be for articulating objects, and c) what cialized methods for planning under such constraints [3, 18]
aspects of the current pipeline could benefit from machine and used it to articulate objects [4, 5, 6, 22, 27, 30, 36].
learning? We find that perception is a major bottleneck for Anotherlineofworkcastsitasatrajectoryoptimizationprob-
such a system, where inability to detect objects and handles lem[12,31,41]oroptimalcontrol [8,23,26,33]problem.As
accounts for 59% of the failures of the system (Figure 12). theseonlysearchforasolutionlocally(ascomparedtomotion
Thiscallsforbroaddatasetswithlabelsforcabinetanddrawer planning approaches which search globally via sampling), it
articulation parameters in diverse settings. Our study also re- is important to properly initialize the trajectory optimizers.
vealsthatcontrolissurprisinglyrobusttomisestimationsinthe Recent work from Gupta et al. [12] alleviates this limitation
articulation parameters. Once the end-effector has acquired a by using machine learning to predict good initializations for
firmgraspofthehandle,thesystemisabletoopenthecabinet trajectory optimization and is able to generate high-quality
a non-trivial amount even with a radius error of up to 10cm motion plans quickly. We adopt their approach but extend
2Motion Pre-Grasp Grasp
Perception Transforms Navigation Execution
Planning Pose Correction
Fig.2:SystemOverview.Theperceptionmoduleoutputsarticulationparametersin3DusingRGB-Dimages.Theseareconvertedintothebasecoordinate
frame.Therobotthennavigatestothetargetnavigationlocation.Next,weuseSeqIKtofindawhole-bodymotionplan.Weexecutethefirstrobotconfiguration
ofthemotionplantoobtainapre-grasppose,afterwhichweuseourcontact-basedadaptationforimprovedgrasping.Oncethehandleisgrasped,weexecute
therestofthemotionplan.
it to produce whole body motion plans as we describe in articulation parameters for the detected instances. These artic-
Section III-B. ulation parameters include the 3D handle location and surface
normalforallobjects,andadditionallytheaxisofrotationand
C. Mobile Manipulation radius for cabinets. These articulation parameters help deduce
the end-effector trajectory needed to open the given object.
Many recent papers look at mobile manipulation tackling
We predict 2D quantities from RGB images, and lift these
a wide range of problems: pick-move-place tasks [40], high-
predictions to 3D using the depth image.
levelplanninggivennaturallanguageinstructions[1],dynamic
whole body control [9]. Fu et al. [10] build a low-cost mobile For2DpredictionfromRGBimages,weadoptMaskRCNN
manipulation system that is easy to tele-operate. Yang et [13].Asis,MaskRCNNpredictsa2Dsegmentationmaskand
al. [39] also work with a Stretch Robot but focus on sim2real the class of each detected object (in our case, the articulation
type: drawer, left-hinged cabinet, or right-hinged cabinet).
transferandmodifytheenvironmenttosimplifythechallenges
due to closed-kinematic chains. Bajracharya et al. [2] tackle a We add additional heads to Mask RCNN to.predict a) the
2D coordinate of the handle, and b) the handle orientation
grocery shopping like scenario with a custom robot platform,
(horizontal or vertical). Both of these additional heads are
and conduct extensive field tests over 18 months. The Dobb-
treated as classification tasks and trained with a cross-entropy
E system from Shafiullah et al. [32] showcases interactions
loss. For the 2D handle coordinate prediction, we minimize
witharticulatedobjects,butrequiresthepolicytoberetrained
the cross-entropy loss over a 2D spatial feature map.
on each test object using human-collected demonstrations.
Sleiman et al. [34] enable quadrupedal robot to articulate We use the depth image to lift these 2D predictions to 3D.
heavy doors and dishwashers, but assume privileged infor- Forthesurfacenormal,wefitaplanetothedepthimagepoints
mation about the environment (e.g. a model of the door). In within the predicted segmentation mask. We use the depth
contrasttotheseapproaches,wedevelopasystemthatoperates image averaged over ten previous frames from a more robust
onnovelobjectinstancesinnovelenvironmentsinazero-shot estimate. For the 3D handle position, we lift the predicted 2D
manner without requiring any privileged information. handle coordinate to 3D using the depth image.
For cabinets, we also need the radius and the axis of
III. SYSTEMDESIGN rotation. We compute the convex hull of the predicted 2D
segmentation mask, and simplify it to a quadrilateral. We lift
Beingabletoopenanarbitrarydrawerorcabinetinanovel
the vertices of this quadrilateral to 3D using the depth image
environment requires us to a) detect the object and predict its
and infer the rotation axis from the corners, e.g. for a left-
articulation parameters purely using on-board sensors, b) use
hinged cabinet we use the left-most two points to define the
thepredictedarticulationparameterstogenerateawhole-body
axis of rotation. We use the distance of the handle to its
motionplan,andc)adaptandexecutethemotionplanwiththe
projected point on the axis of rotation as the radius. Figure 3
aid of proprioceptive feedback. We first present our approach
shows an overview of the perception module.
forestimatingarticulationparametersgivenRGB-Dimagesin
We train our modified Mask RCNN on the ArtObjSim
SectionIII-A.Wethendiscussourmethodologyforgenerating
dataset [12]. ArtObjSim contains information about 3500+
anavigationtargetandawhole-bodymotionplanforopening
articulated objects across 97 scenes from the HM3D dataset
the given object in Section III-B. Finally, in Section III-C, we
[29]. Each articulated object comes with 3D annotations for
describe how proprioceptive feedback from the robot is used
its extent, handle location, articulation type, and articulation
to adapt the generated motion plan during execution.
parameters. We train on images and 2D annotations rendered
out from arbitrary locations in the scene.
A. Predicting Articulation Parameters using On-board RGB-
D Sensors B. Motion Plan Generation
Given an RGB-D image pair containing articulated objects, Ournextgoalistogenerateanavigationtargetandamotion
our goal is to a) detect cabinets and drawers and b) predict plan to open the given articulated object in a collision-free
3Mask R-CNN Depth
2D Handle
ROIAlign Locations
Masks 3D
Handle Points
Orientation Plane
Class and
Fitting
Bbox
Simplify
Convex Hull Camera Frame
Surface Normal
Surface
Base Frame 3D Handle Radius Camera to Base
Normal Lift to 3D
Transforms
Fig.3:OverviewofthePerceptionModule.GivenanRGBimageourmodifiedMaskRCNNdetectsarticulatedobjectsandpredictsthearticulationtype,
thehandleorientation,the2Dsegmentationmask,andthe2Dhandlekeypoint.Wefitaconvexhulltothesegmentationmaskandsimplifyittoaquadrilateral.
Wefitaplanetothedepthimagepointsthatlieinsidethesegmentationmasktoestimatethesurfacenormal.The2Dhandleandquadrilateralcornersare
liftedto3Dusingthedepthimage.Allpredictionsaretransformedtotherobotbasecoordinateframe.Thefinaloutputofthemoduleincludesthe3Dhandle
coordinateandsurfacenormalinthebasecoordinateframeforallarticulatedobjects,andadditionallytheradiusforcabinets.
Left-hinged Right-hinged Pulls out our predicted articulation parameters from Section III-A and
convert them to an end-effector pose trajectory in the same
manner. Finally, rather than finding motion plans just for the
0.45 m 0.625 m arm assuming a fixed base position and orientation as in [12],
0.725 m
we obtain whole-body motion plans for interacting with a
0.65 m
givenarticulatedobjectusingourpredictedtrajectory.Wefind
Drawer/Cabinet
0.125 m this essential to fully opening a wide variety of cabinets and
Robot 0.025 m drawers due to the limited number of degrees of freedom of
the Stretch RE2. Here, in each step of SeqIK, given the fixed
Fig. 4: Topdown Navigation Target Locations. We visualize the topdown
navigation target locations relative to the handle for each articulation type. initialbaseposition,weallowinversekinematicstoalsosearch
WeusetheMPAO(Noneuralnetwork)methodfrom[12]toextractthesein for a base rotation in addition to the arm joints.
adatadrivenmannerfromtheArtObjSimtrainingset.
SeqIK requires an initial base position and arm joint an-
gles θ . For the initial base position, we utilize MPAO (No
0
neural network), a data-driven method from [12] which ranks
manner. We build upon past work from Gupta et al. [12]
robot configurations (base positions and arm joint angles) by
that assumes ground truth articulation parameters and tackles
how successfully SeqIK can decode these configurations into
the problem of converting end-effector pose trajectories into
collision-free motion plans which adhere to the constraints on
robot joint angle trajectories. Specifically, rather than casting
thetrainingset.Figure4showsthebasepositionsfoundbythis
it as a constrained motion planning problem, Gupta et al.
procedure for drawers, left-hinged and right-hinged cabinets.
view it as a trajectory optimization problem. They design
Weusetheseasthenavigationtargetsforeacharticulationtype
SeqIK, a trajectory optimizer specifically suited to this task.
respectively.Asforθ ,theinitialarmjointangles,unlike [12]
0
SeqIK translates an initial robot configuration (base position
which made use of the 7DOF Franka Emika Panda robot, we
and arm joint angles denoted by θ ) into a strategy that can
0 find that using the default neutral joint angles suffices for the
be decoded into a motion plan (desired joint angle trajectory)
Stretch.
when provided with a desired end-effector pose trajectory w,
via SeqIK(θ 0)(w). SeqIK performs inverse kinematics calls C. Adapting and Executing Motion Plans using Propriocep-
sequentially, warm-starting the next inverse kinematics call tive Feedback
with the output of the current. This leads to accurate motion
Minor errors in state estimation, navigation and calibration
plans within only a few seconds.
compound to prevent handle grasping. In particular, when
We extend the framework presented in Gupta et al. [12] in
approaching either a horizontal or vertical handle with a wide
three ways. First, while [12] works with the Franka Emika
gripper, there is sufficient tolerance in both the horizontal and
Panda robot, we adopt their implementation to work with the
vertical directions, but a much smaller tolerance in the depth
Stretch RE2 robot, which has fewer degrees of freedom.1
direction. Moreover, the hardware design of the gripper is
Secondly, we work with predicted articulation parameters
such that as the gripper closes, the gripper becomes shorter,
as opposed to ground truth articulation parameters. We use
which further exacerbates the problem. We develop a method
for contact-based correction of the pre-grasp pose to combat
1TheFrankaEmikahas{3,7,1}degreesoffreedomwhiletheStretchRE2
has{2,5,1}degreesoffreedomforthebase,arm,andgripper,respectively. this: we extend the gripper towards the object until contact
4Without Contact With Contact Left-Hinged Right-Hinged Drawer
Correction Correction
Pre-Grasp Pose Pre-Grasp Pose
Contact Correction
Grasp Failure Grasp Success
(a) (b)
Fig. 5: (a) Contact Correction. The left column visualizes the grasping attempt of the pipeline with No contact correction, whereas the right visualizes
the grasping attempt of the pipeline with contact correction. Due to compounding errors and shrinkage of the gripper when closing, the without contact
correction version fails to grasp, whereas our contact-based correction mechanism leads to a successful grasp. (b) Corrective Motions. We visualize the
correctivemotionsforthedifferentarticulationtypes.Forleft-hingedcabinets,thisisacounter-clockwiserotationin1◦increments.Forright-hingedcabinets
anddrawers,weextendthearmin1cmincrements.
Reached Retract Tilt Down Open Re-extend Tilt Up Number of Waypoints Executed after Grasping
Handle Arm Gripper Arm 12
10
8
Fig.6:VerticalPrimitiveMotion.Wevisualizeourverticalhandleprimitive
motion.Oncethefirstsetofjointangleshasbeenexecutedtoobtainthepre- 6
grasp pose, we retract the arm by 3cm. Next, we tilt down the gripper and
openit.Afterthis,ifcontactwasmadeinachievingthepre-grasppose,we 4
extendthearmby5cmtogoaround thehandle;otherwiseweextenditby
3cm.Finally,wetiltthegripperbackup. 2
0
0 1 2 3 4 5 6 7 8 9 10
is detected. For drawers and right-hinged objects, because the Number of Waypoints Executed
Fig.7:NumberofWaypointsExecuted.Histogramvisualizingthenumber
arm is largely parallel to the surface normal of the object,
of waypoints executed, given a grasp of the handle is achieved. For most
we keep extending the arm in 1cm increments until contact cases,all10waypointsaresuccessfullyexecuted.
is made. For left-hinged objects, because the arm is largely
perpendicular to the surface normal of the object, we rotate
the base by 1◦ counter-clockwise until contact is made. See object. The base is arbitrarily oriented, as long as the desired
Figure5(b)foravisualizationonourcontact-basedcorrection object is within the field-of-view.
primitives. Our perception module, as described in Section III-A,
Asis,theend-effectorapproachesthehandlesfromtheside, produces a prediction for the handle location (in the camera’s
which works well for horizontal handles, but poses issues for coordinate frame), the surface normal, and the radius (if the
verticalhandles.Forobjectswithverticalhandles,wehavean target object is a cabinet). We use a calibrated robot URDF to
additional vertical primitive motion (visualized in Figure 6) transform the 3D predictions (handle, surface normal, axis of
beforeourcontact-basedcorrectionmechanism.Weablatethe rotation, and the navigation target) from the camera frame to
inclusion of this vertical primitive in Section IV-A. To detect the base frame.
contact in our correction mechanism, we design a heuristic After this, we generate a whole-body motion plan using
based on the arm effort signal and the gripper’s yaw effort the methodology described in Section III-B. We execute the
signal.Oncecontactisdetected,thechangeintheend-effector firstqpose(fullrobotconfiguration),andsubsequentlyrunour
pose during the correction phase is used to update the desired contact-basedcorrectionmechanismfromSectionIII-C.Once
trajectory, and SeqIK is run once again to find an improved thehandlehasbeengrasped,weexecutetherestofthemotion
motion plan. plan. Figure 2 shows an overview of the pipeline.
D. Pipeline IV. EXPERIMENTS
Here, we describe the full end-to-end pipeline. The robot We work with the Stretch RE2 robot for our experiments.
begins approximately 1.5m away from the target articulated We present our full end-to-end system test results, in which
5
ycneuqerFBefore Navigation After Navigation Pre-Grasp Pose During Manipulation After Manipulation
Right-
Hinged
Cabinet
Left-
Hinged
Cabinet
Drawer
Right-
Hinged
Cabinet
Left-
Hinged
Cabinet
Drawer
Fig. 8: Examplerolloutsofourfullsysteminvariousunseenenvironments.Foreachenvironment,weshowthefollowingframes:beforenavigation,after
navigation,pre-grasppose,duringmanipulation,andattheendofmanipulation.
our system is evaluated across 10 buildings and a total of 31 oriented slightly to the left, or oriented slightly to the right.
novelarticulatedobjects.Tobetterunderstandtheperformance Across ablations for each object, the robot’s positioning and
of the system, We then evaluate various modules of the orientation remains consistent.
pipeline individually. This includes evaluating the quality of
We represent each trajectory by ten end-effector waypoints,
ourperceptionmoduleonrealworldimages,theeffectiveness
for which our whole-body motion planner attempts to find
of our methodology to find whole-body motion plans, and
joint angles. We define a successful opening of an object if
two ablations for the contact-based correction mechanism and
our system is able to execute at least 7 out of 10 waypoints.
the vertical primitive. To understand the interaction between
For cabinets, this corresponds to opening the cabinet over
perception and control, we quantifying the robustness of
60-degrees. Figure 7 shows the distribution of our method’s
execution to state estimation inaccuracies.
number of waypoints executed after a grasp of the handle has
been acquired. In the vast majority of times, the number of
A. End-to-end System Tests waypointsourwhole-bodymotionplannerwasabletoplanfor
werefullyexecuted.Forobjectswherelessthan10waypoints
We test our end-to-end system across 8 campus buildings
were executed, the planner would have resulted in a motion
and 2 apartments for a total of 31 distinct articulated objects.
plan spanning fewer than 10 waypoints. This was particularly
This test set of objects does not overlap with the set used
common for cabinets with large radii. In a minority of cases,
for development. We assume the robot previously navigated
a motion plan for 10 waypoints was found, but a strong grasp
toward the target object, so for each test it is positioned
of the handle was not acquired, and slippage resulted in a
approximately 1.5m from the object with the camera oriented
failure to open the object. Please refer to Section V for a
to have the target in view. We allow for some variance across
more extensive discussion of the failure modes.
tests in the exact positioning and orientation of the robot
base due to environmental constraints and potential variance End-to-endSystemTest.Overall,oursystemachievesa61%
in the ending pose of any previous navigation. In particular, success rate across 31 unseen cabinets and drawers in unseen
the base orientation is randomly chosen to be facing forward, realworldenvironments.Forexampledeploymentsofourfull
6Success
2.0 Failure
1.5
1.0
0.5
0.0
0.0 0.2 0.4 0.6 0.8 1.0 1.2
Radius (m)
Fig.10:EffectivenessofWhole-BodySeqIK.Weplotsuccess/failurestatus
ofwhole-bodySeqIK(SectionIII-B)forthecabinetsintheArtObjSimdataset
as a function of radius and handle height. We observe that handles above a
certain height are difficult due to hardware limitations of the Stretch RE2,
Fig. 9: Qualitative Results from Mask RCNN. Output of our modified andcabinetswitharadiuslargerthanacertainthresholdarechallengingdue
MaskRCNNonimagesfromourrealworldtesting.Weshowthepredicted toourparameterizationofmotionplans.
segmentationmasksinblue,thefittedquadrilateralsinred,thehandlepoint
ingreen,andthepredictedarticulationtypeastextontheobject.
TABLEI:AccuracyofourmodifiedMaskRCNNonimagesfromourlarge- categories,which,contrarytoouroriginalbelief,didnotprove
scalerealworldevaluation.Thepredictionaccuraciesarehigh,andthemean to be a major obstacle during deployment. Figure 9 shows
handleerrorandradiuserrorareboth<1cm.
the predictions of our modified Mask RCNN model on eight
Drawer Left-Hinged Right-Hinged All differentviewpointsfromourrealworldtesting.Themodelis
largely able to detect fully visible objects, and is sometimes
Detectionaccuracy 9/9 8/9 12/13 29/31
Handleorientationaccuracy 9/9 8/8 11/12 28/29 able to detect occluded objects. Due to the relatively high
Articulationtypeaccuracy 9/9 8/8 12/12 29/29 qualityofthepredictedsegmentationmasks,theconvexhullof
Meanhandlelocationerror 0.37cm 0.67cm 0.45cm 0.49cm
these masks yields visually accurate quadrilaterals conveying
Meanradiuserror n/a 0.47cm 1.32cm 0.98cm
the extent of the objects.
2) Motion Plan Generation: Next, we study the effective-
pipeline in the testing environments, refer to Figure 8. ness of our methodology for generating succcessful whole-
body motion plans for interacting with objects in the Ar-
B. Evaluation of Individual Modules tObjSim dataset using the Stretch RE2. Figure 10 plots the
1) Articulation Parameter Prediction Accuracy: We quali- success of various cabinets in the dataset as a function of
tatively and quantitatively evaluate our perception module on both the handle height and the radius. We notice two trends:
real world images. For a quantitative evaluation, we compute cabinets which have handles higher than 1.2m and cabinets
a) the detection accuracy, b) handle orientation accuracy, c) with radii larger than 0.5m cannot be solved. The former
articulation type accuracy, d) mean handle location error, and can be attributed to a hardware limitation of the Stretch RE2.
e) mean radius error across the all of the real world objects in The lift joint of the robot cannot achieve a height of 1.2m or
our test set. The latter four are computed only for detected higher, leading to this failure. The latter, on the other hand,
instances. The ground truth handle location is obtained by can be attributed to our parameterization of motion plans.
annotating the 2D keypoint in the image and lifting to 3D Our current formulation only allows for rotation of the base
using the depth image. The ground truth radius is obtained and no translation. Allowing for the translation of the base in
by physically measuring the horizontal distance between the addition to rotation while interacting with a given object will
handle and the axis of rotation for each of our real world likelyallowtherobottosolveformorecomplexmotionplans,
cabinets. including the kind needed for large radii.
Table I presents our results. Our perception module’s de- 3) Effectiveness of Adaptation Strategies: We ablate the
tection accuracy is high, but not perfect. See Section V for two adaptation strategies: a) Contact correction, ie use of
a more thorough discussion of the failure modes of percep- proprioceptive feedback to correct the robot’s grasp, and b)
tion. The handle orientation prediction and the articulation Vertical primitive, ie the handcrafted primitive (before using
type predictions are successful the vast majority of the time. proprioceptive feedback) for vertical handles.
Furthermore,themeanhandlelocationerrorissmallat 0.5cm Ablation: Contact Correction.TableIIreportsresultsacross
acrosscategories.Themeanradiuserroriscloseto1cmacross all real world tests for the full system and the contact correc-
7
)m(
thgieh
eldnaHTABLEII:ContactCorrectionAblation.Successratesforourfullpipeline
withandwithoutthecontactcorrectionadaptationdescribedinSectionIII-C Degrees Opened vs Radius Used
across the 31 novel articulated objects in the real world. Contact correction
helpsimprovethesuccessrate. Perturbed Radius Ground Truth
100
Drawer Left-Hinged Right-Hinged Total
Oursw/contactcorrection 8/9 4/9 7/13 19/31 75
Oursw/ocontactcorrection 1/9 4/9 6/13 11/31
50
TABLEIII:VerticalPrimitiveAblation.Successratesforourfullpipeline
with and without the vertical primitive (described in Section III-C) across
25
the 11 novel articulated objects with vertical handles in the real world. The
verticalprimitivehelpsimprovethesuccessrate.
0
0.25 0.30 0.35 0.40
Left-Hinged Right-Hinged Total
Ours w/ vertical primitive 2/4 3/7 5/11 Radius Used (m)
Ours w/o vertical primitive 0/4 3/7 3/11
Fig.11:RobustnessofExecutiontoInaccuraciesinStateEstimation.We
plottheanglethecabinetdoorisopenedasafunctionoftheradiususedfor
generatingmotionplans.Surprisingly,evenwitharadiusestimationerroras
tionablation.Ourmethodwithcontactcorrectionsignificantly largeas10cm,theexecutionisabletosignificantlyopenthecabinetdoor.
outperforms our method without. Refer to Figure 5(a) for
a visualization of how contact correction helps improve the
motion plan with this inaccurate state estimate, deploying this
grasping. Interestingly, almost all of our improvements come
inaccurate motion plan, and measuring the angle the cabinet
from objects with horizontal handles. We postulate that this is
door is opened. Note that we use ground truth parameters for
because our pipeline for horizontal handles does not contain
everything except the radius estimate to ensure a tight grasp
a hand-crafted corrective motion (unlike for vertical handles).
of the handle, before the inaccurate motion plan is deployed.
Ourcontact-basedcorrectionhelpsimprovethepre-grasppose
Figure 11 plots our results for one right-hinged cabinet. To
in the direction of the surface normal, which the vertical
our surprise, even with a radius estimation error as large
handle primitive also helps with. In particular, as described in
as 10cm (in either direction), the robot is able to open the
Figure6,ifcontactisdetectedoncethehandleisreached,then
cabinet a non-trivial amount. We observe that as the radius is
duringarmre-extension,weextendthearmby2cmmorethan
decreased, the angle opened decreases, and vice versa. This
it was retracted so as to help go around the vertical handle,
experiment suggests that once a solid grasp of the handle
making contact-based correction less essential.
has been acquired, other state estimates can be relatively in-
Ablation: Vertical Primitive. For objects with vertical han-
accurate and execution can still succeed. This indicates that
dles, we performed an additional vertical primitive ablation
acquiring a solid grasp of the handle may be more important
on a subset of cabinets and report the results in Table III. Our
than millimeter-level accuracy of predicting the articulation
method with the vertical primitive outperforms our method
parameters, hinting that handle grasping could be a major
without it. The failure cases of this ablation are exactly what
bottleneck for such a system.
originally motivated the inclusion of such a primitive: when
approaching a vertical handle from the side, the open gripper
V. DISCUSSION
ends up colliding with the handle, and as a result, the handle
is not in between the two finger grippers when the gripper is In this work, we develop an end-to-end system for opening
closed.Insuchcases,ourcontact-basedcorrectionmechanism cabinets and drawers in novel real world environments. While
of extending the gripper in the direction of the object until our system is able to solve a majority of the novel objects we
contactisdetecteddoesnothelpacquireagraspofthehandle. testedon,ourlargescaleevaluationrevealedunforeseenfailure
modes. This included failures in perception, navigation, and
C. Evaluation of Interaction between Perception and Control
execution, which we describe in more detail below. Figure 12
We study how robust the execution of our motion plans is summarizes and visualizes these failure modes.
toinaccuraciesinthestateestimate.Whenamotionplanfora Failure in Perception. One of the main failure modes we
slightlyinaccuratestateestimateisdeployed,thereisacertain encounter is error in perception. While control turns out to
level of tolerance that can be provided by a tight grasp of the be surprisingly robust to misestimations of the articulation
handle. For instance, while our radius estimate was off by parameters, this failure in perception includes failure to detect
approximately 1cm from the ground truth on average across the target object and erroneous handle orientation prediction
our real world evaluation, we were still able to execute all byourMaskRCNNmodel.Theseperceptionerrorsareduein
10 waypoints in a majority of the cases. Our goal here is to parttotestingonoutofdistributionobjects.TheadaptedMask
quantify and study the extent to which execution is tolerant to RCNN model is trained on luxury homes from the HM3D
inaccuratestateestimates.Wedothisbyperturbingtheground dataset, whereas we mainly test on the more readily available
truth radius by small increments of 2cm, re-computing a academic office buildings and apartments on campus.
8
denepO
seergeDPerception Failure Examples Navigation Failure Examples
6%
10%
Failed to detect cabinets 23%
61%
Carpeted floor caused larger
Success Perception Failure
Mistook key holes as handles navigational errors
Execution Failure Navigation Failure
Fig.12:FailureCases.Piechartcharacterizingandvisualizingthevariousfailuremodesofoursystem.59%offailuresareduetoperceptionfailures.Our
modified Mask RCNN failed to detect the cabinets outlined in the image on top, and confused the keyholes for handles in the image at the bottom. Some
failures were during execution, where the gripper would lose its grasp of the handle while pulling the articulated object open. We also observe navigation
failures,wherenavigatingoncarpetsislessaccuratethannavigatingontiles.
Failure in Navigation. Our real world system was developed the motion plan which allows for base translation may allow
in an environment with tiled floors, so tests on carpeted floors ustofindsuccessfulmotionplansforcabinetswithlargeradii
introducedanunanticipatedfailuremode.Whennavigatingon or those surrounded by challenging collision geometry.
carpetedfloors,therobotaudiblystrainsduringbaserotations.
This affects both the initial navigation and the deployment of ACKNOWLEDGMENTS
the pre-grasp robot configuration, both of which involve base
This material is based upon work supported by DARPA
rotation, ultimately leading to a failed grasp of the handle.
(MachineCommonSenseprogram),anNSFCAREERAward
Failure in Execution. In some cases, a firm, centered grasp
(IIS-2143873), and the Andrew T. Yang Research and En-
of the handle would not be acquired. This would be due to
trepreneurship Award. We are grateful to the Centre for
imperfect calibration of the robot leading an error in lifting
Autonomy for lending us the Stretch RE2 robot used in this
the 2D predictions to 3D, or minor navigation errors (even on
work. We thank Aditya Prakash and Matthew Chang for their
tiled floors), which would compound. In such cases, as the
feedback on manuscript.
cabinet was pulled open, the gripper would eventually let go
of the handle. The vast majority of the cabinets we tested on
REFERENCES
hadrecoil,duetowhichthecabinetwouldcloseshutafterthe
gripper let go of the handle, even after as many as 5 of the [1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen
10 waypoints of the motion plan had been executed. Chebotar, Omar Cortes, Byron David, Chelsea Finn,
Limitations. While we are able to validate our system on a Chuyuan Fu, Keerthana Gopalakrishnan, Karol Haus-
large variety of objects, there are some limitations to the ob- man,AlexHerzog,DanielHo,JasmineHsu,JulianIbarz,
jects we could test on. To avoid collisions with environmental Brian Ichter, Alex Irpan, Eric Jang, Rosario Jauregui
structures(i.e.walls,counters,etc),cabinetsanddrawerswith Ruano, Kyle Jeffrey, Sally Jesmonth, Nikhil Joshi, Ryan
sufficientroomoneithersidewereselected.Forinstance,left- Julian,DmitryKalashnikov,YuhengKuang,Kuang-Huei
hinged cabinets which had a wall directly to their left could Lee,SergeyLevine,YaoLu,LindaLuu,CarolinaParada,
notbeusedfortesting,asournavigationtargetforleft-hinged PeterPastor,JornellQuiambao,KanishkaRao,JarekRet-
cabinetswouldnotbeachievable.Wealsolimitedourselvesto tinghouse, Diego Reyes, Pierre Sermanet, Nicolas Siev-
elongatedhandlessinceoursystemhasnotyetbeendeveloped ers,ClaytonTan,AlexanderToshev,VincentVanhoucke,
to take knob like handles into account. Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Mengyuan
Future Work. Our characterization of the failure modes Yan, and Andy Zeng. Do as i can and not as i say:
suggests avenues for future work to improve the system. Per- Grounding language in robotic affordances. In arXiv
ceptionfailuresrevealtheneedforbroaderdatasetsconsisting preprint arXiv:2204.01691, 2022.
ofannotationsforarticulatedobjectsnotjustinluxuryhomes, [2] Max Bajracharya, James Borders, Richard Cheng, Dan
but also in other kinds of diverse indoor environments such Helmick, Lukas Kaul, Dan Kruse, John Leichty, Jeremy
asofficesandapartments.Furthermore,thefailureingrasping Ma,CarolynMatl,FrankMichel,ChavdarPapazov,Josh
due to compounding errors in calibration and navigation may Petersen,KrishnaShankar,andMarkTjersland. Demon-
call for more sophisticated mechanisms than our contact- strating mobile manipulation in the wild: A metrics-
based correction, for instance a learned visual last-centimeter driven approach. In Robotics: Science and Systems XIX,
graspingpolicy.Finally,amoreexpressiveparameterizationof RSS2023. Robotics: Science and Systems Foundation,
9July 2023. doi: 10.15607/rss.2023.xix.055. URL http: 2022. Springer Nature Switzerland. ISBN 978-3-031-
//dx.doi.org/10.15607/RSS.2023.XIX.055. 19842-7.
[3] Dmitry Berenson. Obeying Constraints During Motion [16] Yiannis Karayiannidis, Christian Smith, Francisco
Planning, pages 1–32. Springer Netherlands, 2018. EliVinaBarrientos,PetterO¨gren,andDanicaKragic.An
[4] Dmitry Berenson, Siddhartha Srinivasa, and James adaptivecontrolapproachforopeningdoorsanddrawers
Kuffner. Task space regions: A framework for pose- under uncertainties. IEEE Transactions on Robotics, 32
constrained manipulation planning. IJRR, 30(12):1435– (1):161–175, 2016.
1460, 2011. [17] LydiaEKavraki,PetrSvestka,J-CLatombe,andMarkH
[5] Felix Burget, Armin Hornung, and Maren Bennewitz. Overmars. Probabilistic roadmaps for path planning in
Whole-body motion planning for manipulation of artic- high-dimensional configuration spaces. IEEE transac-
ulated objects. In ICRA, pages 1656–1662, 2013. ISBN tionsonRoboticsandAutomation,12(4):566–580,1996.
9781467356411. doi: 10.1109/ICRA.2013.6630792. [18] Zachary Kingston, Mark Moll, and Lydia E Kavraki.
[6] Sachin Chitta, Benjamin Cohen, and Maxim Likhachev. Sampling-based methods for motion planning with con-
Planning for autonomous door opening with a mobile straints. Annual review of control, robotics, and au-
manipulator. In 2010 IEEE International Conference on tonomous systems, 1:159–185, 2018.
RoboticsandAutomation,pages1799–1806.IEEE,2010. [19] EricKolve,RoozbehMottaghi,WinsonHan,EliVander-
[7] Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli Van- Bilt, Luca Weihs, Alvaro Herrasti, Daniel Gordon, Yuke
derBilt, Luca Weihs, Eric Kolve, Aniruddha Kembhavi, Zhu, Abhinav Gupta, and Ali Farhadi. AI2-THOR: An
and Roozbeh Mottaghi. Manipulathor: A framework for Interactive 3D Environment for Visual AI. arXiv, 2017.
visual object manipulation. In CVPR, pages 4497–4506, [20] James J Kuffner and Steven M LaValle. RRT-connect:
2021. An efficient approach to single-query path planning. In
[8] Farbod Farshidian, Edo Jelavic, Asutosh Satapathy, ICRA, 2000.
Markus Giftthaler, and Jonas Buchli. Real-time motion [21] JiayiLiu,AliMahdavi-Amiri,andManolisSavva. Paris:
planning of legged robots: A model predictive control Part-level reconstruction and motion analysis for articu-
approach. In ICHR, pages 577–584, 2017. lated objects. In Proceedings of the IEEE/CVF Inter-
[9] Zipeng Fu, Xuxin Cheng, and Deepak Pathak. Deep national Conference on Computer Vision (ICCV), pages
whole-body control: Learning a unified policy for ma- 352–363, October 2023.
nipulation and locomotion. In Conference on Robot [22] Wim Meeussen, Melonee Wise, Stuart Glaser, Sachin
Learning (CoRL), 2022. Chitta, Conor McGann, Patrick Mihelich, Eitan Marder-
[10] Zipeng Fu, Tony Z Zhao, and Chelsea Finn. Mo- Eppstein, Marius Muja, Victor Eruhimov, Tully Foote,
bile aloha: Learning bimanual mobile manipulation with John Hsu, Radu Bogdan Rusu, Bhaskara Marthi, Gary
low-cost whole-body teleoperation. arXiv preprint Bradski, Kurt Konolige, Brian Gerkey, and Eric Berger.
arXiv:2401.02117, 2024. Autonomous door opening and plugging in with a per-
[11] Chuang Gan, Jeremy Schwartz, Seth Alter, Martin sonal robot. In 2010 IEEE International Conference on
Schrimpf, James Traer, Julian De Freitas, Jonas Kubil- Robotics and Automation, pages 729–736. IEEE, 2010.
ius, Abhishek Bhandwaldar, Nick Haber, Megumi Sano, [23] MayankMittal,DavidHoeller,FarbodFarshidian,Marco
et al. Threedworld: A platform for interactive multi- Hutter,andAnimeshGarg. Articulatedobjectinteraction
modal physical simulation. In Thirty-fifth Conference in unknown scenes with whole-body mobile manipula-
on Neural Information Processing Systems Datasets and tion. In IROS, 2022.
Benchmarks Track, 2021. [24] Kaichun Mo, Leonidas J. Guibas, Mustafa Mukadam,
[12] Arjun Gupta, Max Shepherd, and Saurabh Gupta. Pre- Abhinav Gupta, and Shubham Tulsiani. Where2act:
dicting motion plans for articulating everyday objects. From pixels to actions for articulated 3d objects. In
InInternationalConferenceonRoboticsandAutomation Proceedings of the IEEE/CVF International Conference
(ICRA). IEEE, 2023. on Computer Vision (ICCV), pages 6813–6823, October
[13] Kaiming He, Georgia Gkioxari, Piotr Dolla´r, and Ross 2021.
Girshick. Maskr-cnn. InICCV,pages2961–2969,2017. [25] Chuanruo Ning, Ruihai Wu, Haoran Lu, Kaichun Mo,
[14] Advait Jain and Charles C Kemp. Pulling open doors and Hao Dong. Where2explore: Few-shot affordance
and drawers: Coordinating an omni-directional base and learning for unseen novel categories of articulated ob-
a compliant arm with equilibrium point control. In jects. In Advances in Neural Information Processing
2010 IEEE International Conference on Robotics and Systems, 2023.
Automation, pages 1807–1814. IEEE, 2010. [26] Johannes Pankert and Marco Hutter. Perceptive model
[15] Hanxiao Jiang, Yongsen Mao, Manolis Savva, and An- predictive control for continuous mobile manipulation.
gel X. Chang. Opd: Single-view 3d openable part IEEE RA-L, pages 6177–6184, 2020.
detection. In Shai Avidan, Gabriel Brostow, Moustapha [27] LPeterson,DavidAustin,andDanicaKragic. High-level
Cisse´,GiovanniMariaFarinella,andTalHassner,editors, control of a mobile manipulator for door opening. In
Computer Vision – ECCV 2022, pages 410–426, Cham, Proceedings. 2000 IEEE/RSJ International Conference
10on Intelligent Robots and Systems (IROS 2000)(Cat. No. interactive environment. In CVPR, pages 11097–11107,
00CH37113), volume 3, pages 2333–2338. IEEE, 2000. 2020.
[28] Shengyi Qian and David F Fouhey. Understanding 3d [39] Ruihan Yang, Yejin Kim, Aniruddha Kembhavi, Xi-
object interaction from a single image. arXiv preprint aolong Wang, and Kiana Ehsani. Harmonic mobile
arXiv:2305.09664, 2023. manipulation. arXiv preprint arXiv:2312.06639, 2023.
[29] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik [40] Sriram Yenamandra, Arun Ramachandran, Karmesh Ya-
Wijmans, Oleksandr Maksymets, Alexander Clegg, dav, Austin Wang, Mukul Khanna, Theophile Gervet,
John M Turner, Eric Undersander, Wojciech Galuba, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg,
Andrew Westbury, Angel X Chang, Manolis Savva, Yili John Turner, et al. Homerobot: Open-vocabulary mobile
Zhao, and Dhruv Batra. Habitat-matterport 3d dataset manipulation. arXiv preprint arXiv:2306.11565, 2023.
(HM3D):1000large-scale3denvironmentsforembodied [41] MattZucker,NathanRatliff,AncaDDragan,MihailPiv-
AI. In Thirty-fifth Conference on Neural Information toraiko, Matthew Klingensmith, Christopher M Dellin,
Processing Systems Datasets and Benchmarks Track J Andrew Bagnell, and Siddhartha S Srinivasa. Chomp:
(Round 2), 2021. URL https://openreview.net/forum?id= Covariant hamiltonian optimization for motion planning.
-v4OuqNs5P. The International Journal of Robotics Research, 32(9-
[30] T. Ruhr, J. Sturm, D. Pangercic, M. Beetz, and D. Cre- 10):1164–1193, 2013.
mers. A generalized framework for opening doors and
drawers in kitchen environments. In ICRA, pages 3852–
3858, 2012. doi: 10.1109/ICRA.2012.6224929.
[31] John Schulman, Yan Duan, Jonathan Ho, Alex Lee,
Ibrahim Awwal, Henry Bradlow, Jia Pan, Sachin Patil,
Ken Goldberg, and Pieter Abbeel. Motion planning
with sequential convex optimization and convex colli-
sion checking. The International Journal of Robotics
Research, 33(9):1251–1270, 2014.
[32] Nur Muhammad Mahi Shafiullah, Anant Rai, Haritheja
Etukuru,YiqianLiu,IshanMisra,SoumithChintala,and
Lerrel Pinto. On bringing robots home, 2023.
[33] Jean-Pierre Sleiman, Farbod Farshidian, Maria Vittoria
Minniti, and Marco Hutter. A unified mpc framework
for whole-body dynamic locomotion and manipulation.
IEEE RA-L, pages 4688–4695, 2021.
[34] Jean-Pierre Sleiman, Farbod Farshidian, and Marco Hut-
ter. Versatile multicontact planning and control for
legged loco-manipulation. Science Robotics, 8(81), Au-
gust 2023. ISSN 2470-9476. doi: 10.1126/scirobotics.
adg5014. URL http://dx.doi.org/10.1126/scirobotics.
adg5014.
[35] Xiaohao Sun, Hanxiao Jiang, Manolis Savva, and An-
gel Xuan Chang. Opdmulti: Openable part detection for
multiple objects, 2023.
[36] N. Vahrenkamp, T. Asfour, and R. Dillmann. Robot
placement based on reachability inversion. In ICRA,
pages 1970–1975, 2013. doi: 10.1109/ICRA.2013.
6630839.
[37] Ruihai Wu, Yan Zhao, Kaichun Mo, Zizheng Guo, Yian
Wang,TianhaoWu,QingnanFan,XuelinChen,Leonidas
Guibas,andHaoDong.VAT-mart:Learningvisualaction
trajectory proposals for manipulating 3d ARTiculated
objects. In International Conference on Learning Repre-
sentations, 2022. URL https://openreview.net/forum?id=
iEx3PiooLy.
[38] FanboXiang,YuzheQin,KaichunMo,YikuanXia,Hao
Zhu, Fangchen Liu, Minghua Liu, Hanxiao Jiang, Yifu
Yuan, He Wang, Li Yi, Angel X. Chang, Leonidas J.
Guibas, and Hao Su. Sapien: A simulated part-based
11