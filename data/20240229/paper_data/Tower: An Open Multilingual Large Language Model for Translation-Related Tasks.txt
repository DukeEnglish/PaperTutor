TOWER: An Open Multilingual Large Language
Model for Translation-Related Tasks
DuarteM.Alves†2,4 Jose´ Pombal†1 NunoM.Guerreiro†1,2,4,5
PedroH.Martins1 Joa˜oAlves1 AminFarajian1 BenPeters2,4
RicardoRei1,3 PatrickFernandes2,4,7 SwetaAgrawal⋆2
PierreColombo5,6 Jose´ G.C.deSouza1 Andre´ F.T.Martins1,2,4
1Unbabel,2InstitutodeTelecomunicac¸o˜es,3INESC-ID,4InstitutoSuperiorTe´cnico&
UniversidadedeLisboa(LisbonELLISUnit),5MICS,CentraleSupe´lec,Universite´
Paris-Saclay,6Equall,7CarnegieMellonUniversity
†Equalcontribution,orderedalphabeticallybythefirstname.
⋆WorkpartiallydevelopedduringaninternshipatUnbabel.
duartemalves@tecnico.ulisboa.pt,{jose.pombal, nuno.guerreiro}@unbabel.com.
While general-purpose large language models (LLMs) demonstrate proficiency on
multipletaskswithinthedomainoftranslation,approachesbasedonopenLLMsare
competitiveonlywhenspecializingonasingletask. Inthispaper,weproposearecipe
for tailoring LLMs to multiple tasks present in translation workflows. We perform
continued pretraining on a multilingual mixture of monolingual and parallel data,
creatingTOWERBASE,followedbyfinetuningoninstructionsrelevantfortranslation
processes, creating TOWERINSTRUCT. Our final model surpasses open alternatives
on several tasks relevant to translation workflows and is competitive with general-
purposeclosedLLMs. Tofacilitatefutureresearch,wereleasetheTOWERmodels,our
specializationdataset,anevaluationframeworkforLLMsfocusingonthetranslation
ecosystem,andacollectionofmodelgenerations,includingours,onourbenchmark.
1 Introduction
ManyimportanttaskswithinmultilingualNLP,suchasqualityestimation,automaticpost-
edition,orgrammaticalerrorcorrection,involveanalyzing,generatingoroperatingwith
textinmultiplelanguages,andarerelevanttovarioustranslationworkflows—wecallthese
translation-relatedtasks. Recently,general-purposelargelanguagemodels(LLMs)chal-
lengedtheparadigmofper-taskdedicatedsystems,achievingstate-of-the-artperformance
onseveralrecentWMTsharedtasks(Kocmietal.,2023;Freitagetal.,2023;Nevesetal.,
2023). Unfortunately,strongcapabilitiesformultipletranslation-relatedtaskshavesofar
beenexhibitedbyclosedLLMsonly(Hendyetal.,2023;Kocmi&Federmann,2023;Fernan-
desetal.,2023;Raunaketal.,2023). PerhapsbecausemostopenLLMsareEnglish-centric,
approachesleveragingthesemodelsstilllagbehind,havingthusfarachievedcompetitive
resultsonlywhenspecializingonasingletask(Xuetal.,2024a;2023;Iyeretal.,2023).
In this paper, we bridge this gap with a detailed recipe to develop an LLM for multiple
translation-relatedtasks. Ourapproach, illustratedinFigure1andinspiredbyXuetal.
Figure1: IllustrationofourmethodforbuildingTOWERBASEandTOWERINSTRUCT.
1
4202
beF
72
]LC.sc[
1v33771.2042:viXraTOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
FLORES-200 WMT23
86
89 TOWERINSTRUCT13B GPT-4 GPT-4
TOWERINSTRUCT7B GPT-3.5-turbo 84 TOWERINSTRUCT-13B GPT-3.5-turbo
ALMA-R13B
88 TOWERINSTRUCT-7B
Mixtral-8x7B-Instruct LLaMA-270B ALMA-R7B Mixtral-8x7B-Instruct
LLaMA-270B
87 Gemma7B NLLB54B 82 Gemma7B
LLaMA-213B
LLaMA-213B
86 Mistral-7B-Instruct-v0.2 80 Mistral-7B-Instruct-v0.2
LLaMA-27B
LLaMA-27B NLLB54B
85 78
7 13 46 54 70 7 13 46 54 70
Modelsize(#billionparameters) Modelsize(#billionparameters)
Figure2: TranslationqualityonFLORES-200andWMT23forTOWERINSTRUCTmodelsand
acollectionofopenandclosemodelsacrossdifferentscales. AsthescaleofGPTmodelsis
notknown,werepresentthemwithahorizontalline. TOWERINSTRUCToutperformsopen
alternatives—evenoflargerscales—andiscompetitivewithGPTmodels.
(2024a),reliesonthreesteps. First,weextendthemultilingualcapabilitiesofLLaMA-2(Tou-
vronetal.,2023b)throughcontinuedpretrainingonadatasetcomprising20Btokens,creat-
ing TOWERBASE(§2.1). Importantly,whileXuetal.(2024a)employadatasetexclusively
composedbymonolingualdata,ourapproachincludesparalleldataasanadditionalcross-
lingualsignal. Second,wecurateadatasettospecializeLLMsfortranslation-relatedtasks,
TOWERBLOCKS(§2.2). Third,weperformsupervisedfinetuningtoobtainaninstruction-
followingmodeltailoredforthefieldoftranslation,TOWERINSTRUCT(§2.3).
Weextensivelyevaluateallourmodels,comparingwithopenandclosedalternativesona
widerangeoftasks(§3). TOWERINSTRUCTconsistentlyachieveshighertranslationquality
thanopenalternativesandiscompetitivewiththeclosedGPT-4andGPT-3.5-turbomodels—
seeFigure2. Additionally,TOWERINSTRUCToutperformsopenmodelsinautomaticpost-
edition,grammaticalerrorcorrection,andnamedentityrecognition. Carefulablationsalso
outlinetheinfluenceofeachelementinourrecipe(§4). Wehighlighttheimportanceof
addingparalleldataduringcontinuedpretrainingforimprovedtranslationquality,andthe
effectivenessofincludingconversationalandcodingdataonTOWERBLOCKS.
Accompanyingthiswork,werelease1)theTOWER family,comprisingourTOWERBASE
and TOWERINSTRUCT models in the sizes of 7B and 13B; 2) our specialization dataset
TOWERBLOCKS;3)TOWEREVAL,theevaluationframeworkforLLMsfortranslation-related
tasksthatweusedtoperformallevaluationsinthispaper;4)acollectionofmodelofour
benchmarktoensurereproducibilityandencouragefutureexploration.1
2 TOWER: AnOpenMultilingualLLMforTranslation-RelatedTasks
OurbackbonelanguagemodelisLLaMA-2, whichisverycompetitiveonawiderange
oftasks(Touvronetal.,2023b)andachievesthebestzero-shottranslationqualityacross
availableopenLLMs(Xuetal.,2024a). Nevertheless,theLLaMA-2familywasexposedto
relativelylittlenon-Englishdataduringpretraining,limitingitspotentialformultilingual
tasks,suchasmachinetranslation. Wealleviatethiseffectbycontinuingthepretrainingof
LLaMA-2onahighlymultilingualcorpus(§2.1). Afterwards,weintroduceourdatasetto
tailorLLMsfortranslation-relatedtasks(§2.2)andfinetuneourcontinuedpretrainedmodel
toobtainaninstruction-followingmodelcenteredaroundtranslation(§2.3).
1LinksfortheTOWERmodels;TOWERBLOCKS;TOWEREVAL;Zeno(Cabreraetal.,2023)project
withmodelgenerations.
2
22-TEMOCTOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
Figure3: TasksincludedinoursupervisedfinetuningdatasetTOWERBLOCKS.
2.1 TOWERBASE: ExtendingthemultilingualcapabilitiesofLLaMA-2
WeextendLLaMA-2’strainingonahighly-multilingualdatasetcomprising20billiontokens
—measuredwiththemodel’stokenizer—for10languages: English(en), German(de),
French(fr),Dutch(nl),Italian(it),Spanish(es),Portuguese(pt),Korean(ko),Russian(ru),
andChinese(zh). Whilepreviousworkexclusivelyleveragesmonolingualdata(Xuetal.,
2024b), we draw inspiration from Anil et al. (2023); Briakou et al. (2023), which include
parallel data during pretraining. Specifically, we mix parallel sentences (one-third) along
withmonolingualdata(two-thirds). Ourresultsshowthatthisapproachgreatlybenefits
translationquality(§4).
Monolingualdata. WecollectmonolingualdatafrommC4(Xueetal.,2021),amultilingual
web-crawledcorpus,uniformlysamplingacrossourlanguages. Additionally,weimprove
dataqualitywithstandardcleaningprocedures(Wenzeketal.,2019;Touvronetal.,2023a):
deduplication, language identification, and perplexity filtering with KenLM (Heafield,
2011).
ParallelData. Weuniformlysampleto-English(xx en)andfrom-English(en xx)lan-
→ →
guage pairs from various public sources. Additionally, we ensure translation quality by
removingsentencepairsbelowqualitythresholdsforBicleaner(Sa´nchez-Cartagenaetal.,
2018;Ram´ırez-Sa´nchezetal.,2020)andCOMETKIWI-22(Reietal.,2022b)—wedetailparal-
leldatasourcesandfilteringthresholdsformonolingualandparalleldatainAppendixC.
ModelTraining. WetrainourmodelswithacodebasebasedonMegatron-LLM(Cano
etal.,2023)on8A100-80GBGPUs,aneffectivebatchsizeof1.57milliontokenspergradient
step,andacosineschedulerwithinitialandfinallearningratesof3 10 5and3 10 6,
− −
× ×
respectively. ThetrainingtimesforTOWERBASE7Band13Bwere10and20days.
2.2 TOWERBLOCKS: AdatasettotailorLLMsfortranslation-relatedtasks
WebuildTOWERBLOCKSprioritizingdatadiversityandquality. Figure3illustratesalltasks
inthedataset.Theyincludetasksimportanttotranslationworkflows,appliedbeforeorafter
translation,anddatasetstoimprovemultilingualunderstandingandinstruction-following.
Diversity. Wecollectrecordsfromexistingdatasetsforalltranslation-relatedtasks,pro-
motingdomaindiversitybyincludingmultipledatasetsforeachtask—wedetailalldata
sourcesinAppendixD.Wethenreformulateallrecordsasquestion-answerpairs. Similar
toWeietal.(2022),wefocusontemplatediversitywithmultiplemanuallycuratedzero-and
few-shottemplatesforeachtask. Afterwards,wefollowtheinsightsfromLongpreetal.
(2023),constructing75%oftherecordsaszero-shotinstructions. Fortheremainingrecords,
we include either 1, 3, or 5 in-context examples uniformly sampled from the respective
dataset. Finally, we increase task diversity, which improves held-in performance up to a
3TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
moderatenumberoftasks(Longpreetal.,2023),byaddingaparaphrasingtask,dialogdata
fromUltraChat(Dingetal.,2023),andcodinginstructionsfromGlaive-Code-Assistant.2
Quality. SimilartoXuetal.(2024a),weconstructourquestion-answerpairsfromhuman-
annotated records,3 prioritizing validation or older test sets. Importantly, we ensure that
recordsfrom2023onwardsareexcludedfromthetrainingdata.Wealsoavoidreferencequality
issues(Xuetal.,2024b)fortaskswithreferencetranslations,suchastranslationandautomatic
post-edition,byscoringsource-referencepairswith XCOMET-QE-ENSEMBLE (Guerreiro
etal.,2023)anddiscardingrecordswithqualityscoresbelow0.85. Additionally,weavoid
translationeseonthesourceside,whichisassociatedwithnumerousqualityissues(Zhang&
Toral,2019;Rileyetal.,2020),byonlyincludingtranslationpairsintheiroriginaldirection.
Finally,weadopttheUltraChat(Dingetal.,2023)dialoguesfilteredbyTunstalletal.(2023)
and additionally exclude records respective to translation requests, conversations with
formattingissues(e.g.,instructionsstartingwithpunctuation,andothers),andinstances
wheretheassistantrefusestoanswer.
2.3 TOWERINSTRUCT: SpecializingTOWERBASEforTranslation-RelatedTasks
Asafinalstep,weobtainTOWERINSTRUCTbyfinetuningTOWERBASEonTOWERBLOCKS.
Dialogtemplate. Weformateachdialogasasingletokenizablestringusingthechatml
template(OpenAI,2023);weprovideanexampleinAppendixE.2. Thistemplateclearly
separatesbetweeninstructionsandanswers,andallowsformutli-turndialog. Thetemplate
hasthreespecialidentifiers(controltokens)todelimitmessages: <|im start|>userand
<|im start|>assistantpreemptthebeginningofaturn,and<|im end|>marksitsend.
Weavoidtheseparationof<|im start|>and<|im end|>intomultipletokensbyextending
thetokenizerforTOWERINSTRUCTwithtwodedicatedtokens.Wedonotexplicitlyaddnew
tokensforuserandassistant,asbothstringsalreadyhavededicatedtokens. Additionally,
weoverwritetheend-of-sequencetokenwiththe<|im end|>token.
Modeltraining. Wefinetunethemodelwiththestandardcross-entropyloss,enabling
bfloat16 mixed precision and packing (Raffel et al., 2020). We only calculate the loss on
target(answer)tokens. Wetrainfor4epochsusingalowlearningrateandalargebatch
size—wedetailallhyperparametersinAppendixE.1. Wefoundthatthiscombination
performedthebestandeliminatedstep-wisetraininglossesthathavebeenobservedin
recentmodels(Tunstalletal.,2023;Lvetal.,2023).4 Ourtrainingtookaround50hon4
NVIDIAA100-80GBGPUsandleveragedtheAxolotlframework5 andDeepSpeed(Rasley
etal.,2020)formodelparallelism.
3 Experiments
3.1 ExperimentalSetup
DatasetsandTasks. WeanalyzetranslationcapabilitiesusingFLORES-200(NLLBTeam
etal.,2022),WMT23(Kocmietal.,2023),andTICO-19(Anastasopoulosetal.,2020). Ad-
ditionally,weexaminethreetranslation-relatedtasks. First,weevaluateautomaticpost-
edition(APE)bymeasuringfinaltranslationqualityafterpost-editingNLLB-3.3B(NLLB
Teametal.,2022)translationsforWMT23. Second,weevaluatenamedentityrecognition
2https://huggingface.co/datasets/glaiveai/glaive-code-assistant
3Fornamedentityrecognition,wedidnotfindapermissivelylicensedhuman-annotateddataset,
soweuseMultiCoNER(Malmasietal.,2022;Fetahuetal.,2023).Forgeneraltranslation,weincludea
smallamountofparalleldatafromOPUStocoveralllanguagepairs.Nevertheless,weapplyBicleaner
usingathresholdof0.85followedbythequalityfilteringproceduredescribedinthissection.
4OnehypothesisputforwardinHoward&Whitaker(2023)isthatLLMscanrapidlymemorize
examplesduringtrainingwithonegradientstep.Infact,thesuddendownwardshiftsinlossoccur
preciselywhenanewepochstarts.
5https://github.com/OpenAccess-AI-Collective/axolotl
4TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
(NER),usefulforentityanonymization,usingthetestsplitfromMultiCoNER2023(Fetahu
et al., 2023).6 Third, we evaluate grammatical error correction (GEC), which is held out
fromourtrainingdataandcanbeappliedtocorrectthesourcesentencebeforetranslation.
WetestGEConCoNLL-2014(Ngetal.,2014)(English),COWSL2H(Yamadaetal.,2020)
(Spanish),andmlconvgec2018(Chollampatt&Ng,2018)(German).
Baselines. Onalltasks,wecomparetheTOWERmodelswiththeopenmodelsLLaMA-
270B(Touvronetal.,2023b)andMixtral-8x7B-Instruct(Jiangetal.,2024),andtheclosed-
source models GPT-3.5-turbo and GPT-4.7 For the task of machine translation, we also
comparewithdedicatedsystemsNLLB-54B(NLLBTeametal.,2022)andALMA-R(Xuetal.,
2024b). Wealsoreportnumbersonotheropenalternatives—Gemma7B(GemmaTeam,
2024),Mistral-7B-Instruct-v0.2(Jiangetal.,2023)andQwen1.572B(Baietal.,2023)—in
AppendixF.8 Allmodelgenerationsareperformedwithgreedydecoding—weexplore
alternativedecodingmethodsinAppendixA.ForLLaMA-270BandMixtral-8x7B-Instruct,
wealwaysprovide5in-contextlearningexamplesrandomlyselectedfromthedevelopment
setintheprompt. Unlessspecified,weevaluateallothermodelsina0-shotfashion.
Evaluation. Weevaluatetranslationqualitywith COMET-22 (Reietal.,2022a)forboth
MTandAPE.Fortranslation,wealsoreportXCOMET(Guerreiroetal.,2023),COMETKIWI-
22(Reietal.,2022b),BLEURT(Sellametal.,2020),andCHRF(Popovic´,2015)inAppendixF.9
ForGEC,wemeasureeditrate(ER)(Snoveretal.,2006)andreportERRANT(Bryantetal.,
2017;Feliceetal.,2016)inAppendixG.ForNER,wemeasuresequenceF1score. Onall
tasks,wealsoreportperformanceclustersbasedonstatisticallysignificantperformance
gaps. Foragivenlanguage,weverifywhethermeasureddifferencesbetweenallsystem
pairs are statistically different.10 Afterwards, we create per-language groups for systems
with similar performance by following the clustering procedure in Freitag et al. (2023).
Finally, we obtain system-level rankings across multiple languages using a normalized
Bordacount(Colomboetal.,2022),whichisdefinedasanaverageoftheobtainedclusters.
Notethatafirstclusterwillnotexistifnomodelsignificantlyoutperformsallothersona
majorityoflanguages.
3.2 Translation
WereportaggregatedresultsforallmodelsonFLORES-200,WMT23andTICO-19inTable1.
In Table 2, we study the translation quality on all languages in our training set using
FLORES-200,consideringbothen xxandxx entranslationdirections.
→ →
TOWERINSTRUCT13Bistheopensystemwithhighesttranslationquality. TOWERIN-
STRUCT13BconsistentlyoutperformsthelargeropenmodelsLLaMA-270BandMixtral-
8x7B-Instruct,aswellasthededicatedsystemsNLLB-54BandALMA-Racrosstheboard.On
FLORES-200,TOWERINSTRUCT13Bisoftenrankedfirst,andisclosetoGPT-4performance
onWMT23andTICO-19. Uponinspectingbothsystems’outputs,weverifiedthatthegap
between them increases with longer sentences, as is shown in Figure 4.11 Notably, this
6We uniformly sample 1000 of the more than 200k records due to the computational costs of
evaluatingallmodelsonthewholetestset.
7Weusegpt-3.5-turbo-0613andgpt-4-0613availablefromtheofficialOpenAIAPI.
8TOWERINSTRUCToutperformsalltheseopenalternatives.
9Wefindthatperformancetrendslargelyholdacrossmetrics.Yet,thereisasignificantqualitygap
betweenALMA-RandTOWERmodelsintermsofCHRF—e.g.,over7pointsinen xxdirectionson
→
WMT23—whichisnotfoundwithneuralmetrics. WepositthatALMA-R’salignmentprocesson
translationspreferredbyCOMETKIWI-XXL(Reietal.,2023)andXCOMETmayinadvertentlydegrade
performance on lexical metrics. Exploring evaluation dynamics after alignment with translation
qualitymetricsisapromisingdirectionforfuturework.
10Weapplysignificancetestingataconfidencethresholdof95%.Forsegment-levelmetricssuch
asCOMET-22wecanperformsignificancetestingatthesegmentlevel. However,forcorpus-level
metricssuchasERandSequenceF1,wefollowKoehn(2004)andperformbootstrappingwith100
samplesofsize500each,applyingsignificancetestingonthesamplescores.
11Asimilardomain-levelanalysisdidnotfindanydomaindissimilarfromtheothers.
5TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
FLORES-200 WMT23 TICO19
Models en xx xx en en xx xx en en xx
→ → → → →
Closed
GPT-3.5-turbo 88.952 88.143 85.562 83.482 87.362
GPT-4 89.131 88.421 86.011 83.691 87.521
Open
NLLB54B 86.794 87.953 78.607 79.066 87.052
LLaMA-270B 87.824 88.192 82.956 82.564 86.464
Mixtral-8x7B-Instruct 87.763 88.172 83.605 82.843 86.604
ALMA-R7B — — 83.405 82.394 —
ALMA-R13B — — 84.463 83.033 —
TOWERINSTRUCT7B 88.513 88.272 84.283 82.774 87.013
TOWERINSTRUCT13B 88.882 88.471 85.142 83.182 87.322
Table1: Resultsformachinetranslationaggregatedbylanguagepair. Modelswithstatisti-
callysignificantperformanceimprovementsaregroupedinqualityclusters. Wehighlight
thebestrankedmodelsinboldandunderlinethebestrankedopenmodels.
(a)en xxdirections. (b)xx endirections.
→ →
Figure4: WinratesmarginofTOWERINSTRUCT-13Bbylengthofthetokenizedsourcefor
(a) en xx and (b) xx en language pairs for the WMT23 test set. We compare against
→□ →
GPT-4 ( ) and ALMA-R ( ). We define a (sentence-level) win if the delta between two
△
systemsissuperiorto1COMET-22point.
trendvanisheswhencomparingTOWERINSTRUCT13BtoALMA-R.Wepositthisdifference
stemsfromaprevalenceofshortersentence-leveltranslationsinthetrainingdataofboth
TOWERINSTRUCT13BandALMA-R.Infuturework,wewouldliketoexplorehowtobetter
leveragelongercontexts,whichcanbenefitinstruction-following(Zhaoetal.,2024).
TOWERINSTRUCT13Bachieveshightranslationqualityacrossalllanguagedirections.
In Table 2, TOWERINSTRUCT 13B is ranked first for the majority of en xx directions,
→
andisamongthetopperformingmodelsforallbutonexx enlanguagepair. Notably,
→
TOWERINSTRUCTstandsoutasthebestoverallmodel—outperformingGPT-4—forboth
pt en and ru en language pairs. This outcome likely stems from the English-centric
→ →
pretrainingoftheLLaMA-2family. Alonger,moreexpensivecontinuedpretrainingmight
improveperformanceonen xxdirectionsfurther. Infact,weshowinSection4thatthe
→
translationqualitygainsfromLLaMA-2arelargerforen xxlanguagedirections.
→
6TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
FLORES-200(en xx)
→
Models de es fr it ko nl pt ru zh
Closed
GPT-3.5-turbo 88.782 87.081 89.021 89.061 89.362 88.631 90.461 89.563 88.582
GPT-4 88.981 87.101 88.931 89.051 90.061 88.561 90.431 90.191 88.871
Open
NLLB54B 87.185 85.924 87.713 88.103 89.003 87.333 88.725 88.894 78.267
LLaMA-270B 87.315 86.413 87.823 88.223 88.074 87.473 89.114 88.655 87.325
Mixtral-8x7B-Instruct 87.993 86.802 88.532 88.772 85.635 87.573 89.453 89.094 85.996
TOWERINSTRUCT7B 87.824 86.762 88.442 88.732 89.412 88.382 89.603 89.533 87.904
TOWERINSTRUCT13B 88.163 87.061 88.921 89.211 89.921 88.631 89.782 89.952 88.293
FLORES-200(xx en)
→
Models de es fr it ko nl pt ru zh
Closed
GPT-3.5-turbo 89.602 87.263 89.463 88.033 87.833 87.712 89.783 86.694 86.922
GPT-4 89.761 87.571 89.611 88.212 88.581 87.881 89.942 86.942 87.291
Open
NLLB54B 89.174 87.253 89.294 87.913 87.863 87.493 89.384 86.664 86.553
LLaMA-270B 89.443 87.492 89.552 88.182 87.913 87.523 89.842 86.872 86.912
Mixtral-8x7B-Instruct 89.572 87.651 89.562 88.441 87.374 87.543 89.733 86.813 86.882
TOWERINSTRUCT7B 89.483 87.482 89.502 88.391 88.162 87.662 89.922 86.902 86.962
TOWERINSTRUCT13B 89.612 87.621 89.671 88.421 88.481 87.921 90.071 87.201 87.271
Table 2: Translation quality on FLORES-200 by language pair. Models with statistically
significantperformancearegroupedinqualityclusters. Bestrankedmodelsareinboldand
bestrankedopenmodelsareunderlined.
APE GEC NER
↑ ↓ ↑
Models en xx xx en Multilingual Multilingual
→ →
Baseline(noedits) 76.80 79.99 16.66 —
Closed
GPT-3.5-turbo 81.474 78.685 15.062 50.224
GPT-4 85.201 84.301 15.082 59.883
Open
LLaMA-270B 78.345 81.034 21.745 44.625
Mixtral-8x7B-Instruct 82.643 82.812 17.104 41.776
TOWERINSTRUCT7B 82.692 81.564 15.133 71.682
TOWERINSTRUCT13B 83.312 82.262 15.682 74.701
Table 3: Results for translation-related tasks aggregated by language or language pair.
Models with statistically significant performance improvements are grouped in quality
clusters. Wehighlightthebestrankedmodelsinboldandunderlinethebestrankedopen
models. SinceGECisaheldouttask,weevaluateallmodelswith5in-contextexamples.
TOWERINSTRUCT7Bachievesatrade-offbetweenperformanceandscale. Thesmaller
TOWERINSTRUCT 7B, although behind TOWERINSTRUCT 13B, is competitive with other
open systems and achieves GPT-3.5-turbo translation quality for some language pairs.
Importantly,itoutperformstheonlysystemofthesamesize,ALMA-R7B.
7TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
Figure5:ComparisonofNLLB3Boriginaltranslationquality(x-axis)withTOWERINSTRUCT
13Bposteditionquality(y-axis),andaconcreteexample(left).EachdotisaWMT23zh en
→
translation. Markersizeandhuerepresentthedifferencebetweenpost-editionandoriginal
translationqualities. Thesourceandreferenceofthehighlightedposteditionare“对这个代
理公司和亚马逊实在是很无语。”and“AsitrelatestothisagencyandAmazon,Iamtruly
stunned.”,respectively. SimilarpatternsholdonotherLPs.
WMT23zh →en Unedited
1.0 translations 100
7
28
0.5 75
64
0.0 50
0.5 25
−
1.0 0
−
Toweronly GPT-4only Both Model
Editedby
Figure 6: Difference in translation quality after post-edition for cases where only TOW-
ERINSTRUCT13Bedits( ),onlyGPT-4edits( ),orbothmodelsedit(□).Thebartotheright
⋄ ◦
representsthepercentageofinstancescorrespondingtoeachcase. EachdotisaWMT23zh en
→
NLLB3.3Btranslation,andsimilarpatternsareobservedonotherLPs.
3.3 Translation-RelatedTasks
InTable3,wereporttheresultsforalltranslation-relatedtasks,forbothopenandclosed
models,aggregatedbylanguageorlanguagepair.12
TOWERINSTRUCTisaneffectivetranslationposteditor. TOWERINSTRUCToutperforms
openmodelsandGPT-3.5-turboonAPE.Themodel’sposteditionsconsistentlyandsignifi-
cantlyimprovethequalityofNLLB3Btranslations,goingasfarasconvertingoscillatory
hallucinations into high-quality translations (Figure 5). However, GPT-4 is still the top
performeronthistask. OnefactorthatcouldbebehindthisgapisthatGPT-4editsmuch
moreoftenthanTOWERINSTRUCT,asshownbyFigure6: almost90%ofinstancesareedited
12AppendixG.1detailsevaluatedlanguagesandprovidesresultsforAPEandGEC.
8
)4-TPG-rewoT(22-TEMOC∆TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
FLORES-200(en xx) FLORES-200(xx en) APE(en xx) APE(xx en)
90 → 90 → 84 → 84 →
88 88 81 81
86 86 78 78
84 84 75 75
7B 13B 7B 13B 7B 13B 7B 13B
Modelsize Modelsize Modelsize Modelsize
(#parameters) (#parameters) (#parameters) (#parameters)
TOWERINSTRUCT TOWERBASE LLaMA-2Base
Figure7: Recipeablationacross TOWER scaleson FLORES-200 andAPEforen xxand
→
xx endirections. Numberswithpretrainedmodelsareobtainedina5-shotsetup;TOW-
→
ERINSTRUCT,ontheotherhand,isobtainedina0-shotfashion.
by GPT-4, compared to the 30% of TOWERINSTRUCT.13 We posit that TOWERINSTRUCT
learnsatendencyformoreminimaleditingfromtherelativeabundance—roughly38%—
ofuneditedsegmentsinTOWERBLOCKS.
Thereisroomforimprovementongrammaticalerrorcorrection. Onthistask,nomodel
significantlyoutperformstheothersonthemajorityoflanguagesconsidered. Wehypoth-
esizetherelativelyaverageperformanceofTOWERINSTRUCTiscausedbytheabsenceof
GECdatainTOWERBLOCKS.
TOWERINSTRUCT can identify named entities in multiple languages. TOWERIN-
STRUCT 13B shows promising performance on NER, surpassing GPT-4 by about 15 F1
points. Similar to APE,most of theseimprovements are already reflected on TOWERIN-
STRUCT 7B,highlightingitscapabilitiesdespitethesmallerparameterscale. Otheropen
modelsdonotperformwellonthistask,evenwith5in-contextexamples. Wehypothe-
sizetheseresultsstemfromNERbeingatoken-levelclassificationtask,asopposedtoa
generativeone. Whilethemodelscanlearntheexpectedoutputformatfromtheexamples
or task description, they struggle to grasp the classification function itself. Conversely,
TOWERINSTRUCTcanlearnthetaskfromtherecordsinTOWERBLOCKS.
4 Dissectingthetrainingrecipe
Weperformedmultipleablationstoprovideinsightsontheimpactoftheseveraldesign
choicesmadeinthedevelopmentoftheTOWERmodels.
Continuedpretrainingandsupervisedfinetuningyieldindependentperformancegains.
ThetwoleftmostplotsofFigure7illustratetranslationqualityaftercontinuedpretraining
and supervised finetuning. Both steps bring performance improvement at both model
scales. Remarkably,TOWERBASE7BandTOWERINSTRUCT7BoutperformLLaMA-213B,
and TOWERINSTRUCT 7Boutperforms TOWERBASE 13B.Inthetworightmostplots, we
analyzeAPE.Forthistask,whilesupervisedfinetuningyieldsbetterperformance,continued
pretraining—andinparticularparalleldata—doesnotimproveperformanceasobserved
fortranslation. Infuturework,wewouldliketoexploreadditionaltrainingsignalsduring
continuedpretrainingtoincreaseperformancefortranslation-relatedtasks.
Paralleldataduringcontinuedpretrainingimprovestranslationquality. Figure8reports
5-shottranslationqualityonFLORES-200formultiplecontinuedpretrainingdatarecipes.
Mixingmonolingualandparalleldataachievesthehighestquality, outperformingboth
monolingualonlyandparallelonlydata. Ingeneral,improvementsaremorenoticeableon
13ThisresultsuggeststhatGPT-4isover-editing,whichwefurtheranalyzeinAppendix§B
9
22-TEMOC 22-TEMOC 22-TEMOC 22-TEMOCTOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
FLORES-200(en xx) FLORES-200(xx en)
→ →
88 88
87 87
86 86
85 85
84 84
1 2 3 4 5 6 7 8 1 2 3 4 5 6 7 8
Trainingtokens(billions) Trainingtokens(billions)
TOWERBASE Parallelonly Monolingualonly
TOWERBASE@20b Monolingualonly@20b LLaMA-2
Figure 8: Translation quality on FLORES-200 for continue pretraining data recipes. The
TOWERBASErecipe,outlinedinSection2.1,mixturesmonolingualwithparalleldata. The
“Parallelonly”recipeonlyprocessed8billiontokensduetocomputeconstraints.
MT APE GEC NER
↑ ↓ ↑
Model en xx xx en en xx xx en Multilingual Multilingual
→ → → →
LLaMA-27B 84.23 87.10 76.56 79.91 15.95 20.09
TOWERBASE7B 87.46 88.02 76.79 79.83 15.41 20.51
SupervisedFinetuning
+MT 88.45 88.28 79.19 79.36 54.76 0.00
+Pre-MT+Post-MT 87.92 87.96 81.95 81.73 17.44 74.92
+General-Purpose 88.51 88.27 82.69 81.56 15.13 71.68
Table 4: Ablation results for the components of TOWERBLOCKS. Results for pretrained
modelsareobtainedwith5in-contextexampleswhileresultsforsupervisedmodelsare
obtainedina0-shotsetup. WeconsiderFLORES-200toevaluatetranslationquality.
en xxdirections,likelyduetotheEnglish-centricnatureofLLaMA-2’straining. Never-
→
theless,whilemonolingualonlydataimprovesoverthebaseLLaMA-2by0.1COMET-22
pointsonxx endirections,ourrecipegainsnearlyafullpoint.14
→
Paralleldataduringcontinuedpretrainingissampleefficient,butqualitycontinuesto
improvewithmoretokens. Atthe2billiontokensmark,combiningparallelsentences
withmonolingualdata(i)yieldsmorethan50%oftheimprovementoverthebasemodel,
and(ii)surpassestherecipeleveragingsolelymonolingualdata.Additionally,whiletraining
onmoretokenshasdiminishingreturns—85%ofthetotalperformancegainsappearby
the5billiontokensmark—itcontinuestoimprovetranslationquality.
Transfer/interferencerelationsbetweentasksarecomplex. Table4ablatesthecompo-
nentsofTOWERBLOCKS. Wefinetuneontranslationdata,translation-relatedtasksincluding
pre-andpost-translation,andthefulldatasetwithgeneral-purposetasks. Whileadding
translation-relatedtasksimprovestheirperformance,itdecreasestranslationquality. We
hypothesizethatthereducednumberoftasksencouragesthemodelto“split”itscapacity,
independentlylearningeachtask. Remarkably,introducinggeneral-purposeinstructions
recoverstranslationquality,possiblyduetothedifficultyof“splitting”capacityforalarge
14While0.1COMET-22pointstranslatesto54.9%humanagreement,oneCOMET-22pointtranslates
to90.9%(Kocmietal.,2024).
10
22-TEMOCTOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
numberoftasks. Infuturework,wewouldliketoexploretransfer/interferencebetween
tasksusingscalinglaws.
5 RelatedWork
Previousworkexploredvariousapproachesforadaptingopenmodelstosingletaskswithin
the field of machine translation (Xu et al., 2024a; 2023; Iyer et al., 2023), yielding results
competitivewithclosedmodelsordedicatedsystems. Notably,Xuetal.(2024a)proposes
atwo-stepapproachtoadaptLLaMA-2fortranslation. Theirapproachfirstextendsthe
multilingualcapabilitiesofLLaMA-2withcontinuedpretrainingonmonolingualdataand
thenspecializesfortranslationbyfinetuningonhighqualityparalleldata.
Ourworkalsoadoptsasimilarapproach, butintroducesparalleldataduringcontinued
pretrainingandleveragesLLMs’instruction-followingcapabilitiestobuildasystemcapable
ofperformingmultipletranslation-relatedtasks.
Multilinguality in LLMs. While English-centric LLMs can solve tasks in non-English
languages,theirpotentialisoftenlimitedbythelackofmultilingualdataintheirtraining
corpus. WorksonbuildingmoremultilingualLLMsbridgethisgapinoneoftwoways:
eitherbytrainingamodel“fromscratch”onmoremultilingualdata(Weietal.,2023;Faysse
etal.,2024),orbycontinuingthepretrainingondataforthelanguage(s)ofinterest,possibly
withvocabularyextension(Cuietal.,2023;Xuetal.,2024a;Piresetal.,2023).
Ourmultilingualextensionapproachbuildsuponinsightsshowcasingtheeffectiveness
ofparalleldataduringpretraining(Aniletal.,2023;Weietal.,2023)andincludesparallel
sentences during continued pretraining of LLaMA-2 without vocabulary extension, as
preliminaryexperimentsyieldednegativeresults.
SpecializationofLLMs. RecentresearchalsohighlightstheefficacyoftailoringLLMsfor
subsetsofclosely-relatedtasks. Again,worksaresplitintotrainingmodels“fromscratch”
withdomain-specificdata(Tayloretal.,2022;Wuetal.,2023),continuedpretrainingwith
datatailoredtoincreaseknowledgeofthefield(Lewkowyczetal.,2022;Chenetal.,2023),
supervisedfinetuningondomain-specificdatasets(Yueetal.,2024)oracombinationofthe
lasttwo(Rozie`reetal.,2023;Liuetal.,2023).
Our specialization approach is broadly inspired by instruction tuning (Wei et al., 2022;
Sanhetal.,2022),15 whichfinetuneslanguagemodelsonacollectionoftasksformattedas
naturallanguageinstructions. Specifically,wecurateadatasetforsupervisedfinetuningto
specializeLLMsfortranslation-relatedtasks. WealsoleveragethefindingsfromLongpre
et al. (2023); Wang et al. (2023); Zhou et al. (2023); Xu et al. (2024a), and prioritize data
qualityanddiversityinourdataset.
6 Conclusion
WeproposeanewrecipeforspecializingLLMstomultipletranslation-relatedtasks. First,
weexpandthemultilingualcapabilitiesofLLaMA-2withcontinuedpretrainingonahighly
multilingualcorpus. Then,wefinetunethemodelonadatasetofhigh-qualityanddiverse
instructionsfortranslation-relatedtasks. Ourfinalmodelconsistentlyoutperformsopen
alternatives on multiple translation-related tasks, and is competitive with closed-source
modelssuchasGPT-4.
WereleasetheTOWERmodels,aswellasTOWERBLOCKS. Moreover,wealsomakeavailable
allthecodeusedforthispaper’sbenchmark,TOWEREVAL,aswellasallmodelgenerations
forthetranslationbenchmark. TheGithubrepositorycomeswithinstructionsonhowto
reproducethepaper’sresults,andthegenerationsareavailableontheZenoplatformto
allowforinteractiveexploration.
15Inthispaper,weadoptthenomenclatureofsupervisedfinetuningtorefertoinstructiontuning.
11TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
Acknowledgments
WethankAnto´nioFarinhasandManuelFaysseforthefruitfuldiscussionthroughoutthe
project. PartofthisworkwassupportedbytheEU’sHorizonEuropeResearchandInno-
vationActions(UTTER,contract101070631),bytheprojectDECOLLAGE(ERC-2022-CoG
101088763),bythePortugueseRecoveryandResiliencePlanthroughprojectC645008882-
00000055(CenterforResponsibleAI),andbyFundac¸a˜oparaaCieˆnciaeTecnologiathrough
contract UIDB/50008/2020. We also thank GENCI-IDRIS for the technical support and
HPC resources — Jeanzay grants 101838, 103256, 103298 and Adastra grants C1615122,
CAD14770,CAD15031—usedtopartiallysupportthiswork.
References
AntoniosAnastasopoulos,AlessandroCattelan,Zi-YiDou,MarcelloFederico,Christian
Federmann,DmitriyGenzel,FransciscoGuzma´n,JunjieHu,MacduffHughes,Philipp
Koehn, Rosie Lazar, Will Lewis, Graham Neubig, Mengmeng Niu, Alp O¨ktem, Eric
Paquin,GraceTang,andSylwiaTur. TICO-19: thetranslationinitiativeforCOvid-19.
InProceedingsofthe1stWorkshoponNLPforCOVID-19(Part2)atEMNLP2020,Online,
December2020.AssociationforComputationalLinguistics.URLhttps://aclanthology.
org/2020.nlpcovid19-2.5.
Rohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexan-
dre Passos, Siamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng Chen, Eric Chu,
JonathanH.Clark,LaurentElShafey,YanpingHuang,KathyMeier-Hellstern,Gaurav
Mishra,EricaMoreira,MarkOmernick,KevinRobinson,SebastianRuder,YiTay,Kefan
Xiao,YuanzhongXu,YujingZhang,GustavoHernandezAbrego,JunwhanAhn,Jacob
Austin, Paul Barham, Jan Botha, James Bradbury, Siddhartha Brahma, Kevin Brooks,
MicheleCatasta,YongCheng,ColinCherry,ChristopherA.Choquette-Choo,Aakanksha
Chowdhery,Cle´mentCrepy,ShachiDave,MostafaDehghani,SunipaDev,JacobDevlin,
MarkD´ıaz,NanDu,EthanDyer,VladFeinberg,FangxiaoyuFeng,VladFienber,Markus
Freitag,XavierGarcia,SebastianGehrmann,LucasGonzalez,GuyGur-Ari,StevenHand,
Hadi Hashemi, Le Hou, Joshua Howland, Andrea Hu, Jeffrey Hui, Jeremy Hurwitz,
MichaelIsard,AbeIttycheriah,MatthewJagielski,WenhaoJia,KathleenKenealy,Maxim
Krikun,SnehaKudugunta,ChangLan,KatherineLee,BenjaminLee,EricLi,MusicLi,
Wei Li, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin, Zhongtao Liu, Frederick
Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam
Moussalem,ZacharyNado,JohnNham,EricNi,AndrewNystrom,AliciaParrish,Marie
Pellat,MartinPolacek,AlexPolozov,ReinerPope,SiyuanQiao,EmilyReif,BryanRichter,
ParkerRiley,AlexCastroRos,AurkoRoy,BrennanSaeta,RajkumarSamuel,ReneeShelby,
AmbroseSlone,DanielSmilkov,DavidR.So,DanielSohn,SimonTokumine,DashaValter,
VijayVasudevan,KiranVodrahalli,XuezhiWang,PidongWang,ZiruiWang,TaoWang,
JohnWieting,YuhuaiWu,KelvinXu,YunhanXu,LintingXue,PengchengYin,Jiahui
Yu, Qiao Zhang, Steven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou, Slav Petrov,
andYonghuiWu. Palm2technicalreport. arXivpreprintarXiv:2305.10403,2023. URL
https://arxiv.org/abs/2305.10403.
JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,
YuHan,FeiHuang,BinyuanHui,LuoJi,MeiLi,JunyangLin,RunjiLin,DayihengLiu,
GaoLiu,ChengqiangLu,KemingLu,JianxinMa,RuiMen,XingzhangRen,Xuancheng
Ren,ChuanqiTan,SinanTan,JianhongTu,PengWang,ShijieWang,WeiWang,Sheng-
guangWu,BenfengXu,JinXu,AnYang,HaoYang,JianYang,ShushengYang,YangYao,
BowenYu,HongyiYuan,ZhengYuan,JianweiZhang,XingxuanZhang,YichangZhang,
ZhenruZhang,ChangZhou,JingrenZhou,XiaohuanZhou,andTianhangZhu. Qwen
technicalreport. arXivpreprintarXiv:2309.16609,2023.
EleftheriaBriakou,ColinCherry,andGeorgeFoster. Searchingforneedlesinahaystack:
On the role of incidental bilingualism in palm’s translation capability. In Proceedings
of the 61st Annual Meeting of the Association for Computational Linguistics, 2023. URL
https://aclanthology.org/2023.acl-long.524.pdf.
12TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
ChristopherBryant,MarianoFelice,andTedBriscoe. Automaticannotationandevaluation
oferrortypesforgrammaticalerrorcorrection. InProceedingsofthe55thAnnualMeetingof
theAssociationforComputationalLinguistics(Volume1: LongPapers),Vancouver,Canada,
July2017.AssociationforComputationalLinguistics. URLhttps://aclanthology.org/
P17-1074.
A´ngelAlexanderCabrera,EricaFu,DonaldBertucci,KennethHolstein,AmeetTalwalkar,
JasonI.Hong,andAdamPerer.Zeno:Aninteractiveframeworkforbehavioralevaluation
ofmachinelearning. InCHIConferenceonHumanFactorsinComputingSystems,NewYork,
NY,USA,2023.AssociationforComputingMachinery. URLhttps://doi.org/10.1145/
3544548.3581268.
AlejandroHerna´ndezCano,MatteoPagliardini,AndreasKo¨pf,KyleMatoba,Amirkeivan
Mohtashami, Xingyao Wang, Olivia Simin Fan, Axel Marmet, Deniz Bayazit, Igor
Krawczuk,ZemingChen,FrancescoSalvi,AntoineBosselut,andMartinJaggi. epfllm
megatron-llm,2023. URLhttps://github.com/epfLLM/Megatron-LLM.
Zeming Chen, Alejandro Herna´ndez Cano, Angelika Romanou, Antoine Bonnet, Kyle
Matoba, Francesco Salvi, Matteo Pagliardini, Simin Fan, Andreas Ko¨pf, Amirkeivan
Mohtashami, AlexandreSallinen, AlirezaSakhaeirad, VinitraSwamy, IgorKrawczuk,
DenizBayazit,AxelMarmet,SyrielleMontariol,Mary-AnneHartley,MartinJaggi,and
AntoineBosselut. Meditron-70b: Scalingmedicalpretrainingforlargelanguagemodels.
arXivpreprintarXiv:2311.16079,2023. URLhttps://arxiv.org/abs/2311.16079.
Shamil Chollampatt and Hwee Tou Ng. A multilayer convolutional encoder-decoder
neuralnetworkforgrammaticalerrorcorrection. InProceedingsoftheThirty-SecondAAAI
ConferenceonArtificialIntelligenceandThirtiethInnovativeApplicationsofArtificialIntelligence
Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence.
AAAIPress,2018. URLhttps://dl.acm.org/doi/10.5555/3504035.3504741.
PierreColombo,NathanNoiry,EkhineIrurozki,andSte´phanCle´menc¸on. Whatarethe
bestsystems? newperspectivesonnlpbenchmarking. InAdvancesinNeuralInformation
ProcessingSystems,2022. URLhttps://arxiv.org/abs/2202.03799.
YimingCui, ZiqingYang, andXinYao. Efficientandeffectivetextencodingforchinese
llamaandalpaca. arXivpreprintarXiv:2304.08177,2023. URLhttps://arxiv.org/abs/
2304.08177.
AnnaCurrey,MariaNadejde,RaghavendraPappagari,MiaMayer,StanislasLauly,Xing
Niu,BenjaminHsu,andGeorgianaDinu. MT-GenEval: Acounterfactualandcontextual
datasetforevaluatinggenderaccuracyinmachinetranslation. InProceedingsofthe2022
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing.AssociationforComputa-
tionalLinguistics,December2022. URLhttps://arxiv.org/pdf/2211.01355.pdf.
NingDing,YulinChen,BokaiXu,YujiaQin,ShengdingHu,ZhiyuanLiu,MaosongSun,
andBowenZhou. Enhancingchatlanguagemodelsbyscalinghigh-qualityinstructional
conversations.InProceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguage
Processing,Singapore,December2023.AssociationforComputationalLinguistics. URL
https://aclanthology.org/2023.emnlp-main.183.
BryanEikemaandWilkerAziz. IsMAPdecodingallyouneed? theinadequacyofthemode
inneuralmachinetranslation. InProceedingsofthe28thInternationalConferenceonCompu-
tationalLinguistics,Barcelona,Spain(Online),December2020.InternationalCommitteeon
ComputationalLinguistics. URLhttps://aclanthology.org/2020.coling-main.398.
Andreas Eisele and Yu Chen. MultiUN: A multilingual corpus from united nation doc-
uments. InProceedingsoftheSeventhInternationalConferenceonLanguageResourcesand
Evaluation (LREC’10), Valletta, Malta, May 2010. European Language Resources Asso-
ciation (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2010/pdf/686_
Paper.pdf.
13TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
AhmedEl-Kishky,VishravChaudhary,FranciscoGuzma´n,andPhilippKoehn. CCAligned:
A massive collection of cross-lingual web-document pairs. In Proceedings of the 2020
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),Online,Novem-
ber2020.AssociationforComputationalLinguistics. URLhttps://aclanthology.org/
2020.emnlp-main.480.
MiquelEspla`,MikelForcada,GemaRam´ırez-Sa´nchez,andHieuHoang. ParaCrawl: Web-
scaleparallelcorporaforthelanguagesoftheEU. InProceedingsofMachineTranslation
SummitXVII:Translator,ProjectandUserTracks,Dublin,Ireland,August2019.European
AssociationforMachineTranslation. URLhttps://aclanthology.org/W19-6721.
Europat. Europat. europat.net/.
ManuelFaysse,PatrickFernandes,NunoM.Guerreiro,Anto´nioLoison,DuarteM.Alves,
CaioCorro,NicolasBoizard,Joa˜oAlves,RicardoRei,PedroH.Martins,AntoniBigata
Casademunt,Franc¸oisYvon,Andre´ F.T.Martins,GautierViaud,Ce´lineHudelot,and
PierreColombo. Croissantllm: Atrulybilingualfrench-englishlanguagemodel. arXiv
preprintarXiv:2402.00786,2024. URLhttps://arxiv.org/abs/2402.00786.
ChristianFedermann,TomKocmi,andYingXin. NTREX-128–newstestreferencesforMT
evaluationof128languages. InProceedingsoftheFirstWorkshoponScalingUpMultilingual
Evaluation, Online, nov 2022. Association for Computational Linguistics. URL https:
//aclanthology.org/2022.sumeval-1.4.
MarianoFelice,ChristopherBryant,andTedBriscoe. Automaticextractionoflearnererrors
inESLsentencesusinglinguisticallyenhancedalignments. InProceedingsofCOLING2016,
the26thInternationalConferenceonComputationalLinguistics: TechnicalPapers,Osaka,Japan,
December2016.TheCOLING2016OrganizingCommittee. URLhttps://aclanthology.
org/C16-1079.
PatrickFernandes,Anto´nioFarinhas,RicardoRei,Jose´ G.C.deSouza,PerezOgayo,Gra-
hamNeubig,andAndreMartins. Quality-awaredecodingforneuralmachinetransla-
tion. InProceedingsofthe2022ConferenceoftheNorthAmericanChapteroftheAssociation
for Computational Linguistics: Human Language Technologies, Seattle, United States, July
2022.AssociationforComputationalLinguistics.URLhttps://aclanthology.org/2022.
naacl-main.100.
PatrickFernandes,DanielDeutsch,MaraFinkelstein,ParkerRiley,Andre´ Martins,Graham
Neubig,AnkushGarg,JonathanClark,MarkusFreitag,andOrhanFirat. Thedevilis
in the errors: Leveraging large language models for fine-grained machine translation
evaluation. In Proceedings of the Eighth Conference on Machine Translation, Singapore,
December2023.AssociationforComputationalLinguistics.URLhttps://aclanthology.
org/2023.wmt-1.100.
Besnik Fetahu, Zhiyu Chen, Sudipta Kar, Oleg Rokhlenko, and Shervin Malmasi. Mul-
tiCoNER v2: a large multilingual dataset for fine-grained and noisy named entity
recognition. In Findings of the Association for Computational Linguistics: EMNLP 2023,
Singapore, December 2023. Association for Computational Linguistics. URL https:
//aclanthology.org/2023.findings-emnlp.134.
MarkusFreitag,DavidGrangier,QijunTan,andBowenLiang. Highqualityratherthan
highmodelprobability: MinimumBayesriskdecodingwithneuralmetrics. Transactions
oftheAssociationforComputationalLinguistics,10,2022.URLhttps://aclanthology.org/
2022.tacl-1.47.
Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian
Thompson,TomKocmi,FredericBlain,DanielDeutsch,CraigStewart,ChrysoulaZerva,
SheilaCastilho,AlonLavie,andGeorgeFoster. Resultsofwmt23metricssharedtask:
Metricsmightbeguiltybutreferencesarenotinnocent. InProceedingsoftheEighthConfer-
enceonMachineTranslation,Singapore,December2023.AssociationforComputational
Linguistics. URLhttps://aclanthology.org/2023.wmt-1.51.
14TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
Google DeepMind Gemma Team. Gemma: Open Models Based on Gemini Research
and Technology, howpublished = https://blog.google/technology/developers/
gemma-open-models/,note=Accessed: 2024-02-27,2024.
YvetteGraham,TimothyBaldwin,AlistairMoffat,andJustinZobel. Continuousmeasure-
mentscalesinhumanevaluationofmachinetranslation. InProceedingsofthe7thLinguistic
AnnotationWorkshopandInteroperabilitywithDiscourse,Sofia,Bulgaria,August2013.Asso-
ciationforComputationalLinguistics. URLhttps://aclanthology.org/W13-2305.
NunoM.Guerreiro,RicardoRei,DaanvanStigt,LuisaCoheur,PierreColombo,andAndre´
F.T.Martins. xCOMET:Transparentmachinetranslationevaluationthroughfine-grained
error detection. arXiv preprint arXiv:2310.10482, 2023. URL https://arxiv.org/abs/
2310.10482.
Kenneth Heafield. KenLM: Faster and smaller language model queries. In Proceedings
of the Sixth Workshop on Statistical Machine Translation, Edinburgh, Scotland, July 2011.
AssociationforComputationalLinguistics. URLhttps://www.aclweb.org/anthology/
W11-2123.
AmrHendy,MohamedAbdelrehim,AmrSharaf,VikasRaunak,MohamedGabr,Hitokazu
Matsushita,YoungJinKim,MohamedAfify,andHanyHassanAwadalla. Howgood
are gpt models at machine translation? a comprehensive evaluation. arXiv preprint
arXiv:2302.09210,2023. URLhttps://arxiv.org/abs/2302.09210.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of
neuraltextdegeneration. InInternationalConferenceonLearningRepresentations,2020. URL
https://openreview.net/forum?id=rygGQyrFvH.
JeremyHowardandJonathanWhitaker. CanLLMslearnfromasingleexample?,howpub-
lished=https://www.fast.ai/posts/2023-09-04-learning-jumps/,note=Accessed:
2024-02-22,2023.
Vivek Iyer, Pinzhen Chen, and Alexandra Birch. Towards effective disambiguation for
machinetranslationwithlargelanguagemodels. InProceedingsoftheEighthConferenceon
MachineTranslation,Singapore,December2023.AssociationforComputationalLinguistics.
URLhttps://aclanthology.org/2023.wmt-1.44.
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSingh
Chaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile
Saulnier,Le´lioRenardLavaud,Marie-AnneLachaux,PierreStock,TevenLeScao,Thibaut
Lavril,ThomasWang,Timothe´eLacroix,andWilliamElSayed. Mistral7b. arXivpreprint
arXiv:2310.06825,2023. URLhttps://arxiv.org/abs/2310.06825.
AlbertQ.Jiang, AlexandreSablayrolles, AntoineRoux, ArthurMensch, BlancheSavary,
ChrisBamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,Florian
Bressand,GiannaLengyel,GuillaumeBour,GuillaumeLample,Le´lioRenardLavaud,
LucileSaulnier,Marie-AnneLachaux,PierreStock,SandeepSubramanian,SophiaYang,
SzymonAntoniak,TevenLeScao,The´ophileGervet,ThibautLavril,ThomasWang,Tim-
othe´eLacroix,andWilliamElSayed. Mixtralofexperts. arXivpreprintarXiv:2401.04088,
2024. URLhttps://arxiv.org/abs/2401.04088.
DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InInter-
nationalConferenceonLearningRepresentations(ICLR),SanDiega,CA,USA,2015. URL
https://arxiv.org/abs/1412.6980.
TomKocmiandChristianFedermann. GEMBA-MQM:Detectingtranslationqualityerror
spanswithGPT-4. InProceedingsoftheEighthConferenceonMachineTranslation,Singapore,
December2023.AssociationforComputationalLinguistics.URLhttps://aclanthology.
org/2023.wmt-1.64.
TomKocmi,EleftheriosAvramidis,RachelBawden,OndrˇejBojar,AntonDvorkovich,Chris-
tianFedermann,MarkFishel,MarkusFreitag,ThammeGowda,RomanGrundkiewicz,
15TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
BarryHaddow,PhilippKoehn,BenjaminMarie,ChristofMonz,MakotoMorishita,Ken-
ton Murray, Makoto Nagata, Toshiaki Nakazawa, Martin Popel, Maja Popovic´, and
Mariya Shmatova. Findings of the 2023 conference on machine translation (WMT23):
LLMsareherebutnotquitethereyet. InProceedingsoftheEighthConferenceonMachine
Translation,Singapore,December2023.AssociationforComputationalLinguistics. URL
https://aclanthology.org/2023.wmt-1.1.
TomKocmi,Vile´mZouhar,ChristianFedermann,andMattPost. Navigatingthemetrics
maze:Reconcilingscoremagnitudesandaccuracies. arXivprepringarXiv:2401.06760,2024.
URLhttps://arxiv.org/abs/2401.06760.
PhilippKoehn.Statisticalsignificancetestsformachinetranslationevaluation.InProceedings
ofthe2004ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,Barcelona,Spain,
July2004.AssociationforComputationalLinguistics. URLhttps://aclanthology.org/
W04-3250.
Philipp Koehn. Europarl: A parallel corpus for statistical machine translation. In
Proceedings of Machine Translation Summit X: Papers, Phuket, Thailand, 2005. URL
https://aclanthology.org/2005.mtsummit-papers.11.
Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk
Michalewski,VinayVenkateshRamasesh,AmbroseSlone,CemAnil,ImanolSchlag,Theo
Gutman-Solo,YuhuaiWu,BehnamNeyshabur,GuyGur-Ari,andVedantMisra. Solving
quantitativereasoningproblemswithlanguagemodels. InAdvancesinNeuralInformation
ProcessingSystems,2022. URLhttps://openreview.net/forum?id=IFXTZERXdM7.
MingjieLiu,Teodor-DumitruEne,RobertKirby,ChrisCheng,NathanielPinckney,Rongjian
Liang,JonahAlben,HimyanshuAnand,SanmitraBanerjee,IsmetBayraktaroglu,Bonita
Bhaskaran, Bryan Catanzaro, Arjun Chaudhuri, Sharon Clay, Bill Dally, Laura Dang,
ParikshitDeshpande,SiddhanthDhodhi,SameerHalepete,EricHill,JiashangHu,Sumit
Jain, Brucek Khailany, George Kokai, Kishor Kunal, Xiaowei Li, Charley Lind, Hao
Liu, Stuart Oberman, Sujeet Omar, Sreedhar Pratty, Jonathan Raiman, Ambar Sarkar,
Zhengjiang Shao, Hanfei Sun, Pratik P Suthar, Varun Tej, Walker Turner, Kaizhe Xu,
and Haoxing Ren. Chipnemo: Domain-adapted llms for chip design. arXiv preprint
arXiv:2311.00176,2023. URLhttps://arxiv.org/abs/2311.00176.
ArleLommel,AljoschaBurchardt,andHansUszkoreit. Multidimensionalqualitymetrics
(mqm):Aframeworkfordeclaringanddescribingtranslationqualitymetrics.Traduma`tica:
tecnologiesdelatraduccio´,0,2014.
ShayneLongpre,LeHou,TuVu,AlbertWebson,HyungWonChung,YiTay,DennyZhou,
QuocVLe,BarretZoph,JasonWei,andAdamRoberts. Theflancollection: Designing
dataandmethodsforeffectiveinstructiontuning. InProceedingsofthe40thinternational
conference on machine learning. PMLR, 2023. URL https://proceedings.mlr.press/
v202/longpre23a.html.
KaokaoLv,WenxinZhang,andHaihaoShen. Supervisedfine-tuninganddirectpreference
optimization on intel gaudi2. https://medium.com/intel-analytics-software/the-
practice-of-supervised-finetuning-and-direct-preference-optimization-on-habana-
gaudi2-a1197d8a3cd3,2023.
Shervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta Kar, and Oleg Rokhlenko. Multi-
CoNER: A large-scale multilingual dataset for complex named entity recognition. In
Proceedingsofthe29thInternationalConferenceonComputationalLinguistics,Gyeongju,Re-
publicofKorea,October2022.InternationalCommitteeonComputationalLinguistics.
URLhttps://aclanthology.org/2022.coling-1.334.
ThomasMayerandMichaelCysouw. CreatingamassivelyparallelBiblecorpus. InProceed-
ingsoftheNinthInternationalConferenceonLanguageResourcesandEvaluation(LREC’14),
Reykjavik, Iceland, 2014. European Language Resources Association (ELRA). URL
http://www.lrec-conf.org/proceedings/lrec2014/pdf/220_Paper.pdf.
16TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
Mariana Neves, Antonio Jimeno Yepes, Aure´lie Ne´ve´ol, Rachel Bawden, Giorgio Maria
DiNunzio,RolandRoller,PhilippeThomas,FedericaVezzani,MaikaVicenteNavarro,
LanaYeganova,DinaWiemann,andCristianGrozea. FindingsoftheWMT2023biomed-
ical translation shared task: Evaluation of ChatGPT 3.5 as a comparison system. In
ProceedingsoftheEighthConferenceonMachineTranslation,Singapore,2023.Associationfor
ComputationalLinguistics. URLhttps://aclanthology.org/2023.wmt-1.2.
HweeTouNg,SiewMeiWu,TedBriscoe,ChristianHadiwinoto,RaymondHendySusanto,
andChristopherBryant. TheCoNLL-2014sharedtaskongrammaticalerrorcorrection.
In Proceedings of the Eighteenth Conference on Computational Natural Language Learning:
SharedTask,Baltimore,Maryland,2014.AssociationforComputationalLinguistics. URL
https://aclanthology.org/W14-1701.
NLLB Team, Marta R. Costa-jussa`, James Cross, Onur C¸elebi, Maha Elbayad, Kenneth
Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard,
Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Bar-
rault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett,
Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews,
NecipFazilAyan, ShrutiBhosale, SergeyEdunov, AngelaFan, CynthiaGao, Vedanuj
Goswami,FranciscoGuzma´n,PhilippKoehn,AlexandreMourachko,ChristopheRop-
ers, SafiyyahSaleem, HolgerSchwenk, andJeffWang. Nolanguageleftbehind: Scal-
ing human-centered machine translation. arXiv preprint arXiv:2207.04672, 2022. URL
https://arxiv.org/abs/2207.04672.
OpenAI,2023. URLhttps://github.com/openai/openai-python/blob/release-v0.28.
1/chatml.md.
Ramon Pires, Hugo Abonizio, Thales Sales Almeida, and Rodrigo Nogueira.
Sabia´: Portuguese large language models. In Intelligent Systems, Cham, 2023.
Springer Nature Switzerland. URL https://link.springer.com/chapter/10.1007/
978-3-031-45392-2_15#chapter-info.
MajaPopovic´. chrF:charactern-gramF-scoreforautomaticMTevaluation. InProceedingsof
theTenthWorkshoponStatisticalMachineTranslation,Lisbon,Portugal,2015.Association
forComputationalLinguistics. URLhttps://aclanthology.org/W15-3049.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a
unifiedtext-to-texttransformer. JournalofMachineLearningResearch,2020. URLhttps:
//jmlr.org/papers/volume21/20-074/20-074.pdf.
GemaRam´ırez-Sa´nchez,JaumeZaragoza-Bernabeu,MartaBan˜o´n,andSergioOrtizRojas.
Bifixerandbicleaner:twoopen-sourcetoolstocleanyourparalleldata.InProceedingsofthe
22ndAnnualConferenceoftheEuropeanAssociationforMachineTranslation,Lisboa,Portugal,
2020.EuropeanAssociationforMachineTranslation. URLhttps://aclanthology.org/
2020.eamt-1.31.
JeffRasley,SamyamRajbhandari,OlatunjiRuwase,andYuxiongHe. Deepspeed: System
optimizations enable training deep learning models with over 100 billion parameters.
InProceedingsofthe26thACMSIGKDDInternationalConferenceonKnowledgeDiscovery
&DataMining,NewYork,NY,USA,2020.AssociationforComputingMachinery. URL
https://doi.org/10.1145/3394486.3406703.
VikasRaunak,AmrSharaf,YirenWang,HanyAwadalla,andArulMenezes. Leveraging
GPT-4forautomatictranslationpost-editing.InFindingsoftheAssociationforComputational
Linguistics: EMNLP2023, Singapore, 2023.AssociationforComputationalLinguistics.
URLhttps://aclanthology.org/2023.findings-emnlp.804.
RajReddy. Speechunderstandingsystems: Asummaryofresultsofthefive-yearresearch
effortatcarnegiemellonuniversity.,1977.
17TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
RicardoRei,Jose´ G.C.deSouza,DuarteAlves,ChrysoulaZerva,AnaCFarinha,Taisiya
Glushkova,AlonLavie,LuisaCoheur,andAndre´ F.T.Martins. COMET-22: Unbabel-IST
2022submissionforthemetricssharedtask. InProceedingsoftheSeventhConferenceonMa-
chineTranslation(WMT),AbuDhabi,UnitedArabEmirates(Hybrid),2022a.Association
forComputationalLinguistics. URLhttps://aclanthology.org/2022.wmt-1.52.
RicardoRei,MarcosTreviso,NunoM.Guerreiro,ChrysoulaZerva,AnaCFarinha,Christine
Maroti,Jose´ G.C.deSouza,TaisiyaGlushkova,DuarteAlves,LuisaCoheur,AlonLavie,
and Andre´ F. T. Martins. CometKiwi: IST-unbabel 2022 submission for the quality
estimation shared task. In Proceedings of the Seventh Conference on Machine Translation
(WMT),AbuDhabi,UnitedArabEmirates(Hybrid),2022b.AssociationforComputational
Linguistics. URLhttps://aclanthology.org/2022.wmt-1.60.
RicardoRei,NunoM.Guerreiro,Jose´Pombal,DaanvanStigt,MarcosTreviso,LuisaCoheur,
Jose´ G. C. de Souza, and Andre´ Martins. Scaling up CometKiwi: Unbabel-IST 2023
submissionforthequalityestimationsharedtask. InProceedingsoftheEighthConference
onMachineTranslation,Singapore,2023.AssociationforComputationalLinguistics. URL
https://aclanthology.org/2023.wmt-1.73.
Parker Riley, Isaac Caswell, Markus Freitag, and David Grangier. Translationese as a
languagein“multilingual”NMT.InProceedingsofthe58thAnnualMeetingoftheAssociation
for Computational Linguistics, Online, 2020. Association for Computational Linguistics.
URLhttps://aclanthology.org/2020.acl-main.691.
ParkerRiley,TimothyDozat,JanA.Botha,XavierGarcia,DanGarrette,JasonRiesa,Orhan
Firat, and Noah Constant. FRMT: A benchmark for few-shot region-aware machine
translation. arXivpreprintarXiv:2210.00193,2022. URLhttps://arxiv.org/abs/2210.
00193.
Roberts Rozis and Raivis Skadin¸sˇ. Tilde MODEL - multilingual open data for EU
languages. In Proceedings of the 21st Nordic Conference on Computational Linguistics,
Gothenburg, Sweden, 2017. Association for Computational Linguistics. URL https:
//aclanthology.org/W17-0235.
BaptisteRozie`re,JonasGehring,FabianGloeckle,StenSootla,ItaiGat,XiaoqingEllenTan,
YossiAdi, JingyuLiu, TalRemez, Je´re´myRapin, ArtyomKozhevnikov, IvanEvtimov,
JoannaBitton,ManishBhatt,CristianCantonFerrer,AaronGrattafiori,WenhanXiong,
Alexandre De´fossez, Jade Copet, Faisal Azhar, Hugo Touvron, Louis Martin, Nicolas
Usunier,ThomasScialom,andGabrielSynnaeve. Codellama: Openfoundationmodels
for code. arXiv preprint arXiv:2308.12950, 2023. URL https://arxiv.org/abs/2308.
12950.
V´ıctorM.Sa´nchez-Cartagena,MartaBan˜o´n,SergioOrtiz-Rojas,andGemaRam´ırez. Promp-
sit’s submission to WMT 2018 parallel corpus filtering shared task. In Proceedings of
theThirdConferenceonMachineTranslation: SharedTaskPapers,Belgium,Brussels,2018.
AssociationforComputationalLinguistics. URLhttps://aclanthology.org/W18-6488.
VictorSanh,AlbertWebson,ColinRaffel,StephenBach,LintangSutawika,ZaidAlyafeai,
Antoine Chaffin, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu,
UrmishThakker,ShanyaSharmaSharma,ElizaSzczechla,TaewoonKim,GunjanChh-
ablani, Nihal Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han
Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Baw-
den, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli,
Thibault Fevry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo
Gao,ThomasWolf,andAlexanderMRush. Multitaskpromptedtrainingenableszero-
shottaskgeneralization. InInternationalConferenceonLearningRepresentations,2022. URL
https://openreview.net/forum?id=9Vrb9D0WI4.
HolgerSchwenk,VishravChaudhary,ShuoSun,HongyuGong,andFranciscoGuzma´n.
Wikimatrix: Mining135mparallelsentencesin1620languagepairsfromwikipedia. arXiv
preprintarXiv:1907.05791,2019. URLhttps://arxiv.org/abs/1907.05791.
18TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
HolgerSchwenk,GuillaumeWenzek,SergeyEdunov,EdouardGrave,andArmandJoulin.
Ccmatrix: Miningbillionsofhigh-qualityparallelsentencesontheweb. arXivpreprint
arXiv:1911.04944,2020. URLhttps://arxiv.org/abs/1911.04944.
ThibaultSellam,DipanjanDas,andAnkurParikh. BLEURT:Learningrobustmetricsfor
textgeneration. InProceedingsofthe58thAnnualMeetingoftheAssociationforComputational
Linguistics, Online, 2020. Association for Computational Linguistics. URL https://
aclanthology.org/2020.acl-main.704.
MatthewSnover,BonnieDorr,RichSchwartz,LinneaMicciulla,andJohnMakhoul. Astudy
oftranslationeditratewithtargetedhumanannotation. InProceedingsofthe7thConference
of the Association for Machine Translation in the Americas: Technical Papers, Cambridge,
Massachusetts,USA,2006.AssociationforMachineTranslationintheAmericas. URL
https://aclanthology.org/2006.amta-papers.25.
FelipeSoares,VivianeMoreira,andKarinBecker.Alargeparallelcorpusoffull-textscientific
articles. InProceedingsoftheEleventhInternationalConferenceonLanguageResourcesand
Evaluation(LREC2018),Miyazaki,Japan,2018.EuropeanLanguageResourcesAssociation
(ELRA). URLhttps://aclanthology.org/L18-1546.
LuciaSpecia,KimHarris,Fre´de´ricBlain,AljoschaBurchardt,VivivenMacketanz,Inguna
Skadin,MatteoNegri,andMarcoTurchi. Translationqualityandproductivity: Astudy
onrichmorphologylanguages. InProceedingsofMachineTranslationSummitXVI:Research
Track,NagoyaJapan,2017. URLhttps://aclanthology.org/2017.mtsummit-papers.5.
RossTaylor,MarcinKardas,GuillemCucurull,ThomasScialom,AnthonyHartshorn,Elvis
Saravia,AndrewPoulton,ViktorKerkez,andRobertStojnic. Galactica: Alargelanguage
modelforscience. arXivpreprintarXiv:2211.09085,2022. URLhttps://arxiv.org/abs/
2211.09085.
Jo¨rgTiedemann.TheTatoebaTranslationChallenge–Realisticdatasetsforlowresourceand
multilingualMT. InProceedingsoftheFifthConferenceonMachineTranslation,Online,2020.
AssociationforComputationalLinguistics. URLhttps://www.aclweb.org/anthology/
2020.wmt-1.139.
Jo¨rg Tiedemann. Parallel data, tools and interfaces in opus. In Proceedings of the eighth
international conference on language resources and evaluation (LREC’12), Istanbul, Turkey,
2012.EuropeanLanguageResourcesAssociation(ELRA). URLhttp://www.lrec-conf.
org/proceedings/lrec2012/pdf/463_Paper.pdf.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux,
Timothe´eLacroix,BaptisteRozie`re,NamanGoyal,EricHambro,FaisalAzhar,Aurelien
Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023a. URL
https://arxiv.org/abs/2302.13971.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,Lukas
Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude
Fernandes,JeremyFu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,Naman
Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas,
ViktorKerkez,MadianKhabsa,IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Marie-AnneLachaux,ThibautLavril,JenyaLee,DianaLiskovich,YinghaiLu,Yuning
Mao,XavierMartinet,TodorMihaylov,PushkarMishra,IgorMolybog,YixinNie,Andrew
Poulton, JeremyReizenstein, RashiRungta, KalyanSaladi, AlanSchelten, RuanSilva,
EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,BinhTang,RossTaylor,
AdinaWilliams, Jian XiangKuan, Puxin Xu, Zheng Yan, IliyanZarov, Yuchen Zhang,
Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,
SergeyEdunov,andThomasScialom. Llama2: Openfoundationandfine-tunedchat
models. arXiv preprint arXiv:2307.09288, 2023b. URL https://arxiv.org/abs/2307.
09288.
19TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
LewisTunstall,EdwardBeeching,NathanLambert,NazneenRajani,KashifRasul,Younes
Belkada, Shengyi Huang, Leandro von Werra, Cle´mentine Fourrier, Nathan Habib,
Nathan Sarrazin, Omar Sanseviero, Alexander M. Rush, and Thomas Wolf. Zephyr:
Directdistillationoflmalignment. arXivpreprintarXiv:2310.16944, 2023. URLhttps:
//arxiv.org/abs/2310.16944.
YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,NoahA.Smith,DanielKhashabi,
andHannanehHajishirzi. Self-instruct: Aligninglanguagemodelswithself-generated
instructions. InProceedingsofthe61stAnnualMeetingoftheAssociationforComputational
Linguistics(Volume1: LongPapers),Toronto,Canada,2023.AssociationforComputational
Linguistics. URLhttps://aclanthology.org/2023.acl-long.754.
JasonWei,MaartenBosma,VincentZhao,KelvinGuu,AdamsWeiYu,BrianLester,Nan
Du,AndrewM.Dai,andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners.
InInternationalConferenceonLearningRepresentations,2022. URLhttps://openreview.
net/forum?id=gEZrGCozdqR.
Xiangpeng Wei, Haoran Wei, Huan Lin, Tianhao Li, Pei Zhang, Xingzhang Ren, Mei Li,
YuWan, ZhiweiCao, BinbinXie, TianxiangHu, ShangjieLi, BinyuanHui, BowenYu,
DayihengLiu,BaosongYang,FeiHuang,andJunXie. PolyLM:AnOpenSourcePolyglot
LargeLanguageModel. arXivpreprintarXiv:2307.06018,2023. URLhttp://arxiv.org/
abs/2307.06018.
GuillaumeWenzek,Marie-AnneLachaux,AlexisConneau,VishravChaudhary,Francisco
Guzma´n,ArmandJoulin,andEdouardGrave.Ccnet:Extractinghighqualitymonolingual
datasetsfromwebcrawldata. arXivpreprintarXiv:1911.00359,2019. URLhttps://arxiv.
org/abs/1911.00359.
PhilipWilliamsandBarryHaddow. Theelitrecacorpus. arXivpreprintarXiv:2109.07351,
2021. URLhttps://arxiv.org/abs/2109.07351.
Krzysztof Wołk and Krzysztof Marasek. Building subject-aligned comparable corpora
and mining it for truly parallel sentence pairs. Procedia Technology, 2014. URL http:
//dx.doi.org/10.1016/j.protcy.2014.11.024.
ShijieWu,OzanIrsoy,StevenLu,VadimDabravolski,MarkDredze,SebastianGehrmann,
PrabhanjanKambadur, DavidRosenberg, andGideonMann. Bloomberggpt: Alarge
languagemodelforfinance. arXivpreprintarXiv:2303.17564,2023. URLhttps://arxiv.
org/abs/2303.17564.
HaoranXu,YoungJinKim,AmrSharaf,andHanyHassanAwadalla. Aparadigmshift
in machine translation: Boosting translation performance of large language models.
In The Twelfth International Conference on Learning Representations, 2024a. URL https:
//openreview.net/forum?id=farT6XXntP.
HaoranXu,AmrSharaf,YunmoChen,WeitingTan,LingfengShen,BenjaminVanDurme,
KentonMurray,andYoungJinKim. Contrastivepreferenceoptimization: Pushingthe
boundariesofllmperformanceinmachinetranslation. arXivpreprintarXiv:2401.08417,
2024b. URLhttps://arxiv.org/abs/2401.08417.
WendaXu,DanqingWang,LiangmingPan,ZhenqiaoSong,MarkusFreitag,WilliamWang,
and Lei Li. INSTRUCTSCORE: Towards explainable text generation evaluation with
automaticfeedback. InProceedingsofthe2023ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,Singapore,2023.AssociationforComputationalLinguistics. URL
https://aclanthology.org/2023.emnlp-main.365.
LintingXue,NoahConstant,AdamRoberts,MihirKale,RamiAl-Rfou,AdityaSiddhant,
AdityaBarua,andColinRaffel. mt5: Amassivelymultilingualpre-trainedtext-to-text
transformer. In Proceedings of the 2021 Conference of the North American Chapter of the
Association for Computational Linguistics: Human Language Technologies (NAACL-HLT),
Online,2021.AssociationforComputationalLinguistics. URLhttps://aclanthology.
org/2021.naacl-main.41.
20TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
AaronYamada,SamDavidson,PalomaFerna´ndez-Mira,AgustinaCarando,KenjiSagae,
andClaudiaSa´nchez-Gutie´rrez. Cows-l2h: Acorpusofspanishlearnerwriting. Research
inCorpusLinguistics,2020. URLhttps://ricl.aelinco.es/index.php/ricl/article/
view/109.
Yinfei Yang, Yuan Zhang, Chris Tar, and Jason Baldridge. PAWS-X: A cross-lingual ad-
versarial dataset for paraphrase identification. In Proceedings of the 2019 Conference on
EmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJointConferenceon
NaturalLanguageProcessing(EMNLP-IJCNLP),HongKong,China,2019.Associationfor
ComputationalLinguistics. URLhttps://aclanthology.org/D19-1382.
XiangYue,XingweiQu,GeZhang,YaoFu,WenhaoHuang,HuanSun,YuSu,andWenhu
Chen. MAmmoTH:Buildingmathgeneralistmodelsthroughhybridinstructiontuning.
In The Twelfth International Conference on Learning Representations, 2024. URL https:
//openreview.net/forum?id=yLClGs770I.
BiaoZhang,PhilipWilliams,IvanTitov,andRicoSennrich. Improvingmassivelymultilin-
gualneuralmachinetranslationandzero-shottranslation. arXivpreprintarXiv:2004.11867,
2020. URLhttps://arxiv.org/abs/2004.11867.
Mike Zhang and Antonio Toral. The effect of translationese in machine translation test
sets. In Proceedings of the Fourth Conference on Machine Translation (Volume 1: Research
Papers), Florence, Italy, 2019. Association for Computational Linguistics. URL https:
//aclanthology.org/W19-5208.
HaoZhao,MaksymAndriushchenko,FrancescoCroce,andNicolasFlammarion. Longis
moreforalignment: Asimplebuttough-to-beatbaselineforinstructionfine-tuning. arXiv
preprintarXiv:2402.04833,2024. URLhttps://arxiv.org/abs/2402.04833.
ChuntingZhou,PengfeiLiu,PuxinXu,SriniIyer,JiaoSun,YuningMao,XuezheMa,Avia
Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer,
and Omer Levy. LIMA: Less is more for alignment. In Thirty-seventh Conference on
NeuralInformationProcessingSystems,2023. URLhttps://openreview.net/forum?id=
KBMOKmX2he.
MichałZiemski, MarcinJunczys-Dowmunt, andBrunoPouliquen. TheUnitedNations
parallelcorpusv1.0. InProceedingsoftheTenthInternationalConferenceonLanguageRe-
sourcesandEvaluation(LREC’16),Portorozˇ,Slovenia,2016.EuropeanLanguageResources
Association(ELRA). URLhttps://aclanthology.org/L16-1561.
21TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
A Analysisofalternativedecodingstrategies
FLORES-200 WMT23 TICO19
Models en xx xx en en xx xx en en xx
→ → → → →
GPT-3.5-turbo 77.08 78.12 72.06 72.50 75.91
GPT-4 77.26 78.51 72.54 72.91 76.16
TOWERINSTRUCT13B
Greedy 76.89 78.67 70.87 71.75 75.40
Beam 77.40 78.87 71.31 71.88 75.66
MBR 77.79 78.96 72.29 72.36 76.13
Table5: ImpactofbeamsearchandminimumBayesrisk(MBR)decodingintranslation
qualityforTOWERINSTRUCT13B.Inbold,wehighlightsystemsinthefirstqualitycluster.
ForTICO-19thereisnofirstclustersincenomodelsignificantlyoutperformstheotherson
amajorityofthelanguagepairs.
In this section, we analyse the performance of TOWERINSTRUCT 13B with beam-
search(Reddy,1977)usingbeamsizeof5andminimumBayesrisk(MBR)decoding(Eikema
&Aziz,2020;Fernandesetal.,2022;Freitagetal.,2022)with20hypothesesandCOMET-22as
anutilityfunction.Wegeneratehypothesesusingtemperatureandnucleussampling(Holtz-
manetal.,2020),witht = 0.9and p = 0.6. Weavoid“optimizing”theevaluationmetric
(Fernandesetal.,2022)bymeasuringtranslationqualitywithBLEURT.
Table5reportstranslationqualityacrossalltestsets. Bothdecodingstrategiesconsistently
improvetranslationqualityovergreedydecoding,withMBRdecodingachievinghigher
quality. Additionally,forbothWMT23andTICO-19,decodingstrategiesclosethegapto
GPT-4. Notably,onFLORES-200,TOWERINSTRUCT13Bappearsisolatedinthefirstcluster.
B Furtheranalysison TOWERINSTRUCT andGPT-4editingtendencies
Figure 9 shows that differences between GPT-4 and TOWERINSTRUCT edit rates are not
stronglycorrelatedtodifferencesinCOMET-22(0.34Spearmanρ). ThismeansthatGPT-
4 edits often do not correspond to gains in performance. This finding, allied with the
discussioninSection3.3aboutGPT-4editingconsiderablymorethanTOWERINSTRUCT,
suggeststhatGPT-4maybeeditingtoomuch.
WMT23zh en
1.0 →
0.5
0.0
0.5
−
1.0
− 400 300 200 100 0 100 200 300 400
− − − −
∆EditRate(Tower-GPT-4)
Figure9: DifferencebetweenTOWERINSTRUCT13BandGPT-4editrate(comparedtothe
originalNLLBtranslation)(x-axis),anddifferencebetweenTOWERINSTRUCT13BandGPT-4
post-editionCOMET-22(y-axis). Thecorrelationbetweenthetwovariablesis0.34Spearman
ρ. Similarpatternsareobservedforotherlanguagepairs.
22
)4-TPG-rewoT(22-TEMOC∆TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
C Detailsofthecontinuedpretrainingdataset
InTable6,wereporttheperplexityfloorsandceilingsusedtofilterthemonolingualdatain
thecontinuedpretrainingcorpus,aswellastheBicleanerandCometKiwi-22thresholds
usedtofiltertheparalleldata. InTable7,wealsodetailallsourcesoftheparallelsentences
usedinthecontinuedpretrainingdataset.
en de fr nl es pt ru zh ko
Min.perplexity 50 50 50 50 50 50 50 50 50
∗
Max.perplexity 516 611 322 649 275 257 334 2041 198
∗
Bicleaner† - 0.5 0.5 0.5 0.5 0.5 0.5 0.0 0.5
COMETKIWI-22† - 0.75 0.75 0.75 0.75 0.75 0.75 0.75 0.75
Table6: Qualityfilteringthresholdsappliedonmonolingualdata( )andparalleldata(†)
∗
by language. On the latter, the to-English language pair’s threshold is the same as the
correspondingfrom-Englishone.
Dataset Version
Europarl(Koehn,2005) v8
ParaCrawl(Espla`etal.,2019) v9
MultiParaCrawl(Espla`etal.,2019) v7.1
CCMatrix(Schwenketal.,2020) v1
CCAligned(El-Kishkyetal.,2020) v1
MultiCCAligned(El-Kishkyetal.,2020) v1
WikiTitles(Tiedemann,2012) v2014
WikiMatrix(Schwenketal.,2019) v1
News-Commentary(Tiedemann,2012) v16
OPUS100(Zhangetal.,2020) v1
TildeModel(Rozis&Skadin¸sˇ,2017) v2018
Bible(Mayer&Cysouw,2014) v1
Ubuntu(Tiedemann,2012) v14.10
Tatoeba(Tiedemann,2012) v2
GNOME(Tiedemann,2012) v1
GlobalVoices(Tiedemann,2012) v2018q4
KDE4(Tiedemann,2012) v2
KDE-Doc(Tiedemann,2012) v1
PHP(Tiedemann,2012) v1
Wikipedia(Wołk&Marasek,2014) v1.0
Wikimedia(Tiedemann,2012) v20210402
JRC(Tiedemann,2012) v3.0
DGT(Tiedemann,2012) v2019
EuroPat(Europat) v3
EUbookshop(Tiedemann,2012) v2
EMEA(Tiedemann,2012) v3
EUConst(Tiedemann,2012) v1
tico-19(Anastasopoulosetal.,2020) v20201028
ECB(Tiedemann,2012) v1
Elitr-ECA(Williams&Haddow,2021) v1
MultiUN(Eisele&Chen,2010) v1
OpenOffice(Tiedemann,2012) v3
Ada83(Tiedemann,2012) v1
infopankki(Tiedemann,2012) v1
Scielo(Soaresetal.,2018) v1
giga-fren(Tiedemann,2012) v2
UNPC(Ziemskietal.,2016) v1.0
Table 7: The various data sources used to create the parallel data with the number of
availablelanguagepairs.
23TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
D Detailsof TOWERBLOCKS
ThisappendixdetailsalldatasetsutilizedinTOWERBLOCKS:
• WMT14toWMT2116—Evaluationsetsforthegeneralmachinetranslationsharedtask;
• WMT22withquality-shots(Hendyetal.,2023)—EvaluationsetfromWMT23withhigh
qualityin-contextexamples;
• NTREX(Federmannetal.,2022)—ProfessionaltranslationsoftheWMT19testset;
• FLORES-200(NLLBTeametal.,2022)—DevelopmentsetoftheFLORES-200datasetfor
alllanguagesincludedintraining;
• FRMT (Riley et al., 2022) — Human translations of English Wikipedia sentences into
regionalvariants;
• OPUS(Tiedemann,2012)—Parallelcorporafromwhichwesampledveryhigh-quality
samplesforalllanguagepairs;
• QT21(Speciaetal.,2017)andApeQuest17—Translationdatawithpost-editsutilizedfor
generaltranslationandautomaticpost-editing;
• MT-GenEval(Curreyetal.,2022)—Gendertranslationbenchmarkwhichweleveraged
forgeneraltranslationandcontext-awaretranslation;
• WMT20 to WMT22 Metrics MQM18 — MT evaluation data annotated with multidi-
mensional quality metrics (Lommel et al., 2014) that we used to perform error span
detection;
• WMT17toWMT22MetricsDAs19—MTevaluationdataannotatedwithdirectassesse-
ments(DAs)(Grahametal.,2013)whichweutilizedfortranslationranking.
• WMT21Terminology20—DevelopmentsetfortheWMT21terminologytask;
• Tatoeba(Tiedemann,2020)—DevelopmentsetoftheTatoebadatasetwhichweusedto
generatetranslationsindifferentlanguagesforthesamesource—weidentifiedthistask
asmulti-referencetranslation;
• MultiCoNER2022and2023(Malmasietal.,2022;Fetahuetal.,2023)—Development
setsofthenamedentityrecognitionMultiCoNERdatasets. ForMultiCoNER2023,we
adoptedthecoarse-grainedentitycategorization;
• PAWS-X(Yangetal.,2019)—DevelopmentsetofthePAWS-Xdatasetwhichweusedas
paraphrasegeneration;
• UltraChat(Dingetal.,2023)—FilteredversionoftheUltraChatdatasetusedinTunstall
etal.(2023);
• GlaiveCodeAssistant21—Codingquestionsandanswersacrossawiderangeofpro-
gramminglanguages.
16https://www2.statmt.org/wmt23/translation-task.html
17https://apequest.wordpress.com/
18https://www.statmt.org/wmt22/results.html
19https://www.statmt.org/wmt22/results.html
20https://www.statmt.org/wmt21/terminology-task.html
21https://huggingface.co/datasets/glaiveai/glaive-code-assistant
24TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
E Detailsof TOWERINSTRUCT
E.1 Hyperparameters
Table8detailsthefullhyperparametersconfigurationforthetrainingofTOWERINSTRUCT.
Wealsoutilizedbfloat16mixedprecisionandpacking.
Globaltrainbatchsize 256
NumberofEpochs 4
Learningrate 7e-6
LRScheduler cosine
WarmupSteps 500
WeightDecay 0.01
Optimizer Adam(Kingma&Ba,2015)
Adamβ 0.9
1
Adamβ 0.999
2
Adamϵ 1e-8
MaximumSequenceLength 2048
Table8: HyperparameterconfigurationtofinetuneTOWERINSTRUCTonTOWERBLOCKS.
E.2 ChatTemplate
WefinetunedTOWERINSTRUCTwiththechatmltemplate(OpenAI,2023). Table9provides
anexampleofaninteractionusingtheaforementionedtemplate.
User <|im start|>user
TranslatethefollowingtextfromPortugueseintoEnglish.
Portuguese:Ontem,aminhaamigafoiaosupermercadomasestavafechado.Queria
comprarlegumesefruta.
English:<|im end|>
<|im start|>assistant
Model Yesteday,myfriendwenttothesupermarketbutitwasclosed. Shewantedtobuy
vegetablesandfruit.<|im end|>
User <|im start|>user
CanyounowtranslateitintoSpanish?<|im end|>
<|im start|>assistant
Model Ayermiamigafuealsupermercado,peroestabacerrado.Quer´ıacomprarverdurasy
fruta.<|im end|>
Table9: ExampleofadialoguewithTOWERINSTRUCT’suserandmodelcontroltokens.
25TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
F Translationfullresults
OnTables10toTables13,wetablesequivalenttoTable1,butwithdifferentmetrics(one
pertable): XCOMET,COMETKIWI-22,BLEURT,andCHRF.TheequivalentforTable2isdone
inTables14to17. OnTables18,19,and20,wepresenttranslationresultsforawidervariety
ofmodels,brokendownbylanguagepair.
FLORES-200 WMT23 TICO19
Models en xx xx en en xx xx en en xx
→ → → → →
Closed
GPT-3.5-turbo 94.412 95.541 88.992 89.752 91.192
GPT-4 94.751 96.011 89.461 90.281 91.382
Open
NLLB54B 90.044 93.784 78.996 81.386 90.113
LLaMA-270B 92.804 94.154 84.856 87.215 89.025
Mixtral-8x7B-Instruct 91.903 94.403 85.676 87.814 89.304
ALMA-R7B — — 86.504 87.674 —
ALMA-R13B — — 88.882 88.973 —
TOWERINSTRUCT7B 93.852 94.673 87.204 87.884 90.563
TOWERINSTRUCT13B 94.801 95.222 88.712 88.653 91.302
Table 10: Translation quality on WMT23 and TICO-19 by language pair measured by
XCOMET. Modelswithstatisticallysignificantperformancearegroupedinqualityclusters.
Bestperformingmodelsareinboldandbestperformingopenmodelsareunderlined.
FLORES-200 WMT23 TICO19
Models en xx xx en en xx xx en en xx
→ → → → →
Closed
GPT-3.5-turbo 86.252 85.642 80.822 80.352 85.652
GPT-4 86.421 85.771 81.201 80.541 85.792
Open
NLLB54B 82.935 84.894 70.966 76.695 85.163
LLaMA-270B 85.304 84.974 78.435 79.364 84.665
Mixtral-8x7B-Instruct 85.243 85.323 79.015 79.823 84.814
ALMA-R7B — — 79.254 79.794 —
ALMA-R13B — — 80.123 80.212 —
TOWERINSTRUCT7B 85.963 85.413 79.804 79.953 85.323
TOWERINSTRUCT13B 86.192 85.512 80.572 80.252 85.592
Table 11: Translation quality on WMT23 and TICO-19 by language pair measured by
COMETKIWI-22. Models with statistically significant performance are grouped in qual-
ity clusters. Best performing models are in bold and best performing open models are
underlined.
26TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
FLORES-200 WMT23 TICO19
Models en xx xx en en xx xx en en xx
→ → → → →
Closed
GPT-3.5-turbo 77.081 78.123 72.062 72.501 75.912
GPT-4 77.261 78.512 72.541 72.911 76.162
Open
NLLB54B 74.293 77.993 62.736 66.465 75.492
LLaMA-270B 75.044 78.282 68.035 71.013 74.004
Mixtral-8x7B-Instruct 74.783 78.102 68.815 71.323 74.224
ALMA-R7B — — 68.645 70.664 —
ALMA-R13B — — 70.094 71.473 —
TOWERINSTRUCT7B 76.103 78.262 69.774 71.113 74.834
TOWERINSTRUCT13B 76.892 78.671 70.872 71.752 75.403
Table 12: Translation quality on WMT23 and TICO-19 by language pair measured by
BLEURT. Modelswithstatisticallysignificantperformancearegroupedinqualityclusters.
Bestperformingmodelsareinboldandbestperformingopenmodelsareunderlined.
FLORES-200 WMT23 TICO19
Models en xx xx en en xx xx en en xx
→ → → → →
Closed
GPT-3.5-turbo 58.201 63.753 56.381 60.922 64.182
GPT-4 58.611 64.352 56.941 61.331 64.342
Open
NLLB54B 54.704 63.872 42.986 52.086 63.842
LLaMA-270B 55.194 64.152 52.314 59.662 61.654
Mixtral-8x7B-Instruct 54.504 63.383 51.224 58.634 61.344
ALMA-R7B — — 45.207 57.334 —
ALMA-R13B — — 46.526 58.373 —
TOWERINSTRUCT7B 56.163 64.082 52.254 58.884 62.074
TOWERINSTRUCT13B 57.192 64.791 54.103 59.782 62.813
Table13: TranslationqualityonWMT23andTICO-19bylanguagepairmeasuredbyCHRF.
Models with statistically significant performance are grouped in quality clusters. Best
performingmodelsareinboldandbestperformingopenmodelsareunderlined.
27TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
FLORES-200 WMT23 TICO19
Models en xx xx en en xx xx en en xx
→ → → → →
Closed
GPT-3.5-turbo 94.412 95.541 88.992 89.752 91.192
GPT-4 94.751 96.011 89.461 90.281 91.382
Open
NLLB54B 90.044 93.784 78.996 81.386 90.113
LLaMA-270B 92.804 94.154 84.856 87.215 89.025
Mixtral-8x7B-Instruct 91.903 94.403 85.676 87.814 89.304
ALMA-R7B — — 86.504 87.674 —
ALMA-R13B — — 88.882 88.973 —
TOWERINSTRUCT7B 93.852 94.673 87.204 87.884 90.563
TOWERINSTRUCT13B 94.801 95.222 88.712 88.653 91.302
Table 14: Translation quality on FLORES-200 by language pair measured by XCOMET.
Models with statistically significant performance are grouped in quality clusters. Best
performingmodelsareinboldandbestperformingopenmodelsareunderlined.
FLORES-200(en xx)
→
Models de es fr it ko nl pt ru zh
Closed
GPT-3.5-turbo 85.152 87.041 87.181 87.471 86.923 86.881 85.692 85.582 84.372
GPT-4 85.271 87.071 87.251 87.511 87.471 86.901 85.682 85.991 84.681
Open
NLLB54B 82.596 85.184 85.234 85.664 86.114 84.714 83.455 83.564 69.887
LLaMA-270B 84.195 86.403 86.683 86.773 85.465 85.873 84.574 84.593 83.135
Mixtral-8x7B-Instruct 84.723 86.742 87.042 87.182 83.496 85.953 84.993 84.783 82.306
TOWERINSTRUCT7B 84.414 86.772 87.082 87.312 86.703 86.482 85.572 85.502 83.784
TOWERINSTRUCT13B 84.733 86.941 87.181 87.451 87.222 86.602 85.851 85.682 84.093
FLORES-200(xx en)
→
Models de es fr it ko nl pt ru zh
Closed
GPT-3.5-turbo 84.642 86.272 86.481 86.842 85.692 86.182 85.311 84.592 84.762
GPT-4 84.711 86.391 86.501 86.951 86.151 86.251 85.311 84.751 84.921
Open
NLLB54B 84.095 85.515 86.043 86.064 85.134 85.595 84.454 83.954 83.186
LLaMA-270B 84.294 85.784 86.053 86.383 84.456 85.565 84.873 83.774 83.575
Mixtral-8x7B-Instruct 84.453 86.073 86.342 86.782 84.745 85.784 85.132 84.453 84.144
TOWERINSTRUCT7B 84.413 86.123 86.352 86.792 85.214 85.983 85.172 84.472 84.164
TOWERINSTRUCT13B 84.443 86.093 86.392 86.832 85.473 86.043 85.172 84.691 84.473
Table15: TranslationqualityonFLORES-200bylanguagepairmeasuredbyCOMETKIWI-22.
Models with statistically significant performance are grouped in quality clusters. Best
performingmodelsareinboldandbestperformingopenmodelsareunderlined.
28TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
FLORES-200(en xx)
→
Models de es fr it ko nl pt ru zh
Closed
GPT-3.5-turbo 79.091 76.751 79.541 79.832 69.392 77.791 80.311 77.312 73.692
GPT-4 79.131 76.641 79.291 80.002 70.311 77.582 80.221 78.161 73.981
Open
NLLB54B 77.713 75.374 77.963 79.263 68.952 76.473 77.804 76.813 58.326
LLaMA-270B 76.754 75.285 76.964 78.704 67.013 75.984 77.504 75.794 71.414
Mixtral-8x7B-Instruct 77.733 76.083 78.393 79.573 61.774 76.353 78.143 76.064 68.945
TOWERINSTRUCT7B 77.613 75.714 78.033 79.583 69.252 77.731 78.433 77.022 71.534
TOWERINSTRUCT13B 78.152 76.422 78.962 80.391 70.531 77.931 78.782 77.971 72.853
FLORES-200(xx en)
→
Models de es fr it ko nl pt ru zh
Closed
GPT-3.5-turbo 80.382 77.273 80.553 77.913 75.223 77.022 80.863 77.733 76.122
GPT-4 80.741 77.612 80.722 78.142 76.511 77.231 81.112 78.022 76.541
Open
NLLB54B 80.123 77.093 80.642 77.793 75.322 76.992 80.813 77.952 75.194
LLaMA-270B 80.382 77.651 80.792 78.052 75.582 76.773 81.162 78.182 75.962
Mixtral-8x7B-Instruct 80.402 77.791 80.752 78.531 74.154 76.872 80.853 78.022 75.573
TOWERINSTRUCT7B 80.173 77.472 80.672 78.401 75.622 76.962 81.302 78.102 75.683
TOWERINSTRUCT13B 80.551 77.651 81.031 78.541 76.531 77.221 81.511 78.511 76.461
Table16:TranslationqualityonFLORES-200bylanguagepairmeasuredbyBLEURT.Models
withstatisticallysignificantperformancearegroupedinqualityclusters. Bestperforming
modelsareinboldandbestperformingopenmodelsareunderlined.
29TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
FLORES-200(en xx)
→
Models de es fr it ko nl pt ru zh
Closed
GPT-3.5-turbo 67.222 57.391 72.791 60.671 35.492 59.572 72.961 58.482 39.211
GPT-4 67.891 57.132 72.891 60.601 37.181 59.971 72.981 59.501 39.321
Open
NLLB54B 63.185 55.305 70.253 58.833 36.541 56.995 68.194 57.283 25.735
LLaMA-270B 63.435 55.395 69.544 58.203 32.073 56.535 69.612 56.584 35.383
Mixtral-8x7B-Instruct 64.144 56.144 70.912 59.012 27.544 56.226 69.432 56.074 31.014
TOWERINSTRUCT7B 63.874 56.044 70.233 59.452 35.442 58.164 68.744 57.773 35.783
TOWERINSTRUCT13B 65.163 56.583 71.262 60.321 37.101 59.043 69.063 58.772 37.402
FLORES-200(xx en)
→
Models de es fr it ko nl pt ru zh
Closed
GPT-3.5-turbo 69.312 60.463 69.542 62.763 57.503 60.752 72.563 62.803 58.072
GPT-4 69.741 61.092 69.941 62.753 59.551 60.882 72.912 63.402 58.871
Open
NLLB54B 68.543 60.722 69.702 62.953 58.552 60.672 72.263 62.663 58.831
LLaMA-270B 69.222 61.341 70.081 63.512 57.822 60.902 72.962 63.612 57.942
Mixtral-8x7B-Instruct 69.002 61.291 69.322 63.382 55.564 59.983 72.184 62.773 56.973
TOWERINSTRUCT7B 68.942 61.391 69.562 63.592 58.482 60.652 73.002 63.372 57.792
TOWERINSTRUCT13B 69.391 61.501 70.071 64.061 59.811 61.401 73.541 64.411 58.901
Table17: TranslationqualityonFLORES-200bylanguagepairmeasuredbyCHRF.Models
withstatisticallysignificantperformancearegroupedinqualityclusters. Bestperforming
modelsareinboldandbestperformingopenmodelsareunderlined.
30TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
FLORES-200(en xx)
→
Models de es fr it ko nl pt ru zh
Closed
GPT-3.5-turbo 88.78 87.08 89.02 89.06 89.36 88.63 90.46 89.56 88.58
GPT-4 88.98 87.10 88.93 89.05 90.06 88.56 90.43 90.19 88.87
Open
NLLB54B 87.18 85.92 87.71 88.10 89.00 87.33 88.72 88.89 78.26
LLaMA-27B 84.03 84.37 85.18 85.18 80.20 84.48 87.01 85.09 82.50
LLaMA-213B 85.60 85.45 86.74 87.02 84.22 86.11 88.33 87.02 84.83
LLaMA-270B 87.31 86.41 87.82 88.22 88.07 87.47 89.11 88.65 87.32
Mistral-7B-Instruct-v0.2 84.27 84.87 86.16 85.86 79.20 84.43 87.53 85.78 82.41
Mixtral-8x7B 87.95 86.64 88.39 88.44 85.72 87.26 89.34 88.89 86.23
Mixtral-8x7B-Instruct 87.99 86.80 88.53 88.77 85.63 87.57 89.45 89.09 85.99
Qwen1.572B 87.20 86.46 87.78 88.19 87.64 87.40 89.13 88.41 88.85
Gemma7B 86.13 85.84 87.09 87.03 84.89 86.03 88.60 87.24 85.75
ALMA-PRETRAIN7B 86.47 83.18 84.23 83.59 68.06 81.05 84.80 87.96 85.80
ALMA-PRETRAIN13B 87.07 84.90 86.05 86.09 77.10 84.36 87.47 88.91 86.58
TOWER
TOWERBASE7B 86.91 85.95 87.76 87.93 86.55 87.37 89.47 88.72 86.48
TOWERBASE13B 87.21 86.01 88.34 88.25 88.78 87.52 89.36 88.30 87.14
TOWERINSTRUCT7B 87.82 86.76 88.44 88.73 89.41 88.38 89.60 89.53 87.90
TOWERINSTRUCT13B 88.16 87.06 88.92 89.21 89.92 88.63 89.78 89.95 88.29
FLORES-200(xx en)
→
Models de es fr it ko nl pt ru zh
Closed
GPT-3.5-turbo 89.60 87.26 89.46 88.03 87.83 87.71 89.78 86.69 86.92
GPT-4 89.76 87.57 89.61 88.21 88.58 87.88 89.94 86.94 87.29
Open
NLLB54B 89.17 87.25 89.29 87.91 87.86 87.49 89.38 86.66 86.55
LLaMA-27B 88.47 86.63 88.78 87.48 85.52 86.67 88.98 85.87 85.53
LLaMA-213B 89.01 86.98 89.14 87.87 86.95 87.23 89.26 86.37 86.35
LLaMA-270B 89.44 87.49 89.55 88.18 87.91 87.52 89.84 86.87 86.91
Mistral-7B-Instruct-v0.2 88.83 87.07 88.81 87.69 85.16 86.93 89.05 86.21 85.65
Mixtral-8x7B 89.55 87.57 89.58 88.35 87.03 87.54 89.80 86.79 86.63
Mixtral-8x7B-Instruct 89.57 87.65 89.56 88.44 87.37 87.54 89.73 86.81 86.88
Qwen1.572B 89.67 87.66 89.58 88.41 88.42 87.72 89.88 87.13 87.94
Gemma7B 89.17 87.09 89.12 87.81 87.28 87.23 89.48 86.59 86.59
ALMA-PRETRAIN7B 89.23 86.84 89.01 87.68 83.35 86.92 89.05 86.81 86.59
ALMA-PRETRAIN13B 89.81 87.42 89.42 88.18 86.26 87.59 89.70 87.23 87.16
TOWER
TOWERBASE7B 89.26 87.15 89.47 88.14 87.80 87.45 89.77 86.41 86.72
TOWERBASE13B 89.54 87.42 89.55 88.11 88.24 87.61 89.71 86.18 87.02
TOWERINSTRUCT7B 89.48 87.48 89.50 88.39 88.16 87.66 89.92 86.90 86.96
TOWERINSTRUCT13B 89.61 87.62 89.67 88.42 88.48 87.92 90.07 87.20 87.27
Table18: COMET-22onFLORES-200forawidevarietyofmodels.
31TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
WMT23
Models en de en ru en zh de en ru en zh en
→ → → → → →
Closed
GPT-3.5-turbo 84.61 85.38 86.70 85.91 83.02 81.52
GPT-4 84.89 86.07 87.08 86.17 83.63 81.27
Open
NLLB54B 77.40 83.91 74.48 80.06 80.52 76.60
LLaMA-27B 75.02 77.87 79.16 83.36 80.58 77.40
LLaMA-213B 78.29 80.44 81.30 83.92 81.54 78.73
LLaMA-270B 81.62 83.04 84.19 85.12 82.84 79.73
Mistral-7B-Instruct-v0.2 76.78 80.27 81.26 84.18 81.52 79.11
Mixtral-8x7B 81.92 83.39 83.81 85.04 82.70 79.50
Mixtral-8x7B-Instruct 83.07 83.79 83.94 85.45 83.02 80.04
Qwen1.572B 81.44 83.31 86.48 85.54 83.01 80.60
Gemma7B 79.56 82.20 83.56 84.60 82.14 79.24
ALMA-PRETRAIN7B 80.20 83.01 82.68 83.51 81.82 78.66
ALMA-PRETRAIN13B 81.18 83.72 83.83 84.32 82.71 79.22
ALMA-R7B 82.41 84.28 83.51 84.55 82.50 80.13
ALMA-R13B 83.59 85.37 84.43 85.39 83.23 80.48
TOWER
TOWERBASE7B 81.03 83.25 84.00 84.09 80.08 78.92
TOWERBASE13B 81.18 83.46 84.03 83.89 80.03 78.94
TOWERINSTRUCT7B 83.22 84.73 84.89 85.24 82.94 80.13
TOWERINSTRUCT13B 83.98 85.51 85.92 85.62 83.21 80.72
Table19: COMET-22onWMT23forawidevarietyofmodels.
TICO-19
Models en es en fr en pt en ru en zh
→ → → → →
Closed
GPT-3.5-turbo 88.67 81.86 90.30 87.88 88.09
GPT-4 88.76 81.85 90.30 88.36 88.32
Open
NLLB54B 88.74 82.01 89.84 88.67 85.97
LLaMA-27B 85.77 78.08 86.97 82.99 81.86
LLaMA-213B 86.94 79.83 88.48 85.44 84.89
LLaMA-270B 87.84 80.67 89.24 87.12 87.44
Mistral-7B-Instruct-v0.2 86.25 79.18 87.87 84.35 84.13
Mixtral-8x7B 88.12 81.15 89.27 87.14 86.58
Mixtral-8x7B-Instruct 88.23 81.39 89.48 87.04 86.84
Qwen1.572B 86.08 80.32 88.20 80.53 86.68
Gemma7B 87.30 78.20 88.66 86.16 86.78
ALMA-PRETRAIN7B 84.42 76.74 84.92 86.53 85.27
ALMA-PRETRAIN13B 86.17 79.09 87.56 87.27 86.54
ALMA-R7B 84.63 76.02 82.92 87.80 85.41
ALMA-R13B 85.93 79.90 87.41 88.58 86.22
TOWER
TOWERBASE7B 87.90 81.20 89.45 86.94 86.97
TOWERBASE13B 87.90 81.48 89.54 87.26 87.57
TOWERINSTRUCT7B 88.34 81.60 89.38 88.11 87.63
TOWERINSTRUCT13B 88.63 81.82 89.48 88.49 88.20
Table20: COMET-22onTICO-19forawidevarietyofmodels.
32TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
G Translation-relatedtasksfullresults
G.1 Languagesconsidered
ForAPE,onTable3,weconsider4languagepairs: en de,en zh,de en,andru en.
→ → → →
We leave out en ru and zh en, because we had no post editions to serve as fewshot
→ →
examples for LLaMA-2 and Mixtral-8x7B-Instruct. In any case, we provide results for
TOWERINSTRUCT,GPT-3.5-turbo,andGPT-4onthe6languagepairsinTable21.
ForNER,weconsiderEnglish,German,French,Spanish,Italian,Portuguese,Russian,and
Chinese. Finally,weevaluateGEConEnglish,German,andSpanish. Forthistask,besides
thenumbersshowninTable3,wealsomeasureERRANTinTable22.
ResultsbrokendownbylanguagemaybefoundinTables23,24,and25.
APE
Models en xx xx en
→ →
Baseline(noedits) 78.844 78.804
GPT-3.5-turbo 82.323 77.915
GPT-4 85.521 83.121
TOWERINSTRUCT7B 83.103 80.193
TOWERINSTRUCT13B 83.652 80.892
Table21:APEresultsforthe6WMT23LPsconsidered.NLLBcorrespondstothetranslations
thatweresubjecttoediting,sotheirqualityservesasthebaselineforthetask. Table3did
notincludezh-enanden-rutoguaranteeafaircomparisonwithopenmodels—therewere
nofewshotexamplesavailablefortheseLPs.
GEC
Models Multilingual
Closed
GPT-3.5-turbo 0.491
GPT-4 0.483
Open
LLaMA-270B 0.434
Mixtral-8x7B-Instruct 0.434
TOWERINSTRUCT7B 0.424
TOWERINSTRUCT13B 0.434
Table22: GECERRANTresults.
33TOWER:AnOpenMultilingualLargeLanguageModelforTranslation-RelatedTasks
WMT23
Models en de en ru en zh de en ru en zh en
→ → → → → →
Baseline(noedits) 77.87 82.93 75.72 79.92 80.05 76.44
Closed
GPT-3.5-turbo 80.67 84.03 82.27 78.48 78.88 76.37
GPT-4 84.65 86.15 85.75 85.39 83.21 80.75
Open
GPT-3.5-turbo 80.67 84.03 82.27 78.48 78.88 76.37
GPT-4 84.65 86.15 85.75 85.39 83.21 80.75
LLaMA-270B 78.49 — 78.20 81.30 80.76 —
Mixtral-8x7B-Instruct 82.12 — 83.15 83.40 82.22 —
TOWER
TOWERINSTRUCT7B 81.86 83.92 83.52 82.29 80.82 77.45
TOWERINSTRUCT13B 82.03 84.34 84.59 83.22 81.30 78.15
Table23: APECOMET-22resultsbylanguagepair.
Models en de es
Baseline(noedits) 13.75 18.23 18.00
Closed
GPT-3.5-turbo 14.71 13.19 17.29
GPT-4 16.48 12.89 15.86
Open
LLaMA-270B 17.46 20.67 27.09
Mixtral-8x7B-Instruct 16.44 15.38 19.47
TOWER
TOWERINSTRUCT7B 13.39 14.77 17.23
TOWERINSTRUCT13B 13.13 14.42 19.48
Table24: GECeditrateresultsbylanguage.
Models en de es fr it pt zh
Closed
GPT-3.5-turbo 55.43 60.12 56.82 53.34 55.46 52.57 17.82
GPT-4 63.61 66.58 65.24 58.72 63.39 61.74 39.88
Open
LLaMA-270B 46.34 48.79 50.69 47.50 53.96 45.60 19.44
Mixtral-8x7B-Instruct 45.74 46.94 46.03 46.11 50.86 40.21 16.51
TOWER
TOWERINSTRUCT7B 75.09 78.01 74.89 70.35 76.39 73.88 53.13
TOWERINSTRUCT13B 77.52 79.73 76.69 74.55 80.36 77.47 56.57
Table25: NERF1resultsbylanguage.
34