ADL4D: Towards A Contextually Rich Dataset for 4D Activities of Daily Living
Marsil Zakour∗ Partha Pratim Nath∗ Ludwig Lohmer
Emre Faik Go¨kc¸e Martin Piccolrovazzi Constantin Patsch
Yuankai Wu Rahul Chaudhari Eckehard Steinbach
Technical University of Munich, School of Computation, Information and Technology,
Department of Computer Engineering, Chair of Media Technology,
Munich Institute of Robotics and Machine Intelligence (MIRMI), Munich, Germany
firstname.lastname@tum.de
Figure 1. Dataset and Annotation Setup Overview. Figure a. Rich Context Interactions: An overview of our dataset Activities of Daily
Living 4D (ADL4D). The top row shows an activity sequence view of our multi-view system. The middle row shows a rendering overlay
of the object and hand poses. The bottom row shows a different point of view for the 3D scene. The subjects perform activities that
require multiple objects while sharing some objects like the milk carton (highlighted with a circle in the middle and bottom rows). Figure
b. Robust marker-less Hand Pose Tracking: The proposed region-growing approach for finding an inlier set of 3D hand proposals. The
search is parametrized by the threshold δtracking, which estimates the minimum hand displacement over consecutive frames.
Abstract Daily Living (ADL) like breakfast or lunch preparation ac-
tivities. The transition between multiple objects to complete
a certain task over time introduces a unique context lacking
Hand-Object Interactions (HOIs) are conditioned on
in existing datasets. Our dataset consists of 75 sequences
spatial and temporal contexts like surrounding objects, pre-
with a total of 1.1M RGB-D frames, hand and object poses,
vious actions, and future intents (for example, grasping and
and per-hand fine-grained action annotations. We develop
handover actions vary greatly based on objects’ proximity
an automatic system for multi-view multi-hand 3D pose an-
and trajectory obstruction). However, existing datasets for
notation capable of tracking hand poses over time. We inte-
4D HOI (3D HOI over time) are limited to one subject inter-
grate and test it against publicly available datasets. Finally,
acting with one object only. This restricts the generalization
we evaluate our dataset on the tasks of Hand Mesh Recov-
of learning-based HOI methods trained on those datasets.
ery (HMR) and Hand Action Segmentation (HAS).
We introduce ADL4D, a dataset of up to two subjects inter-
acting with different sets of objects performing Activities of1. Introduction that is annotated through human supervision, as well as
other publicly available datasets [2] and [33].
Activities of Daily Living (ADL) involve humans interact-
Our contribution consists of 3 main parts
ing with different objects in their surroundings, transition-
• A dataset with 1.1M images (75 long sequences for break-
ing from one set of objects to the next in order to execute
fast and lunch activities). These sequences include up to
a structured plan. Hands and object poses convey most of
2 subjects interacting with multiple objects. The annota-
the information about activities. For example, the knowl-
tions include hand, object, 3D poses, and hand-level fine-
edge about the hand and a milk bottle pose can differen-
grained annotations.
tiate between the “pour milk” and “open milk” actions.
• An automatic annotation tool (with semi-automatic GUI
Therefore, a 4D understanding of these interactions requires
support) compatible with different existing datasets.
data with hands, object poses, and actions over time. Re-
• An extensive analysis of the automatic hand tracking
cently, several new Hand-Object Interaction (HOI) datasets
method across different datasets and an evaluation of our
were introduced. However, these datasets do not cover
proposed dataset (ADL4D) across tasks like Hand Action
full activities, rather only single or few consecutive interac-
Segmentation (HAS) and Hand Mesh Recovery (HMR).
tions like pickup box, open milk, handover, read book, etc.
[2, 33, 57], or they cover consecutive interactions includ-
2. Related Work
ing a single object with no long-term plan or action anno-
tations [13]. Dexterous hand-object interaction sequences
2.1. Datasets for 3D Hand-Object Activities
are key for tasks like robot learning from demonstration;
for example, Chao et al. [2] used hand-object interaction There have been many datasets capturing 3D human in-
data to learn to predict plausible clamp grasp from hand- teractions with surrounding objects. These datasets vary
object images. Another important application is the genera- in the richness of annotations, data source, and annotation
tive modeling of hand-object animations. For example, He method. Table 1 shows an overview of recent datasets cov-
et al. [59] generate hand interaction poses given hand-object ering one or both areas. Similar datasets such as those refer-
distance information and trajectories. Similarly, Taheri et enced in [20], and [48] are not included in the table as they
al.[51, 52] use hand object datasets for grasp and pickup ac- are superseded by a newer version. The existing work could
tion generation. These use cases are currently limited by be classified into two major types. First, datasets with a fo-
existing datasets complexity and cannot be generalized to cus on long and complex tasks with multiple objects and
more complex scenarios. Given the existing dataset limi- a single subject like assembly tasks in [44, 46], as shown
tations, we introduce our dataset ADL4D which we curate in Table 1. These dataset provide 2D or 3D hand annota-
towards contextually rich activities, and at-length interac- tions, but no 3D object annotations. Second, HOI datasets
tions of multiple objects and subjects. A sample of ADL4D like [2, 13, 21, 33, 37] are mainly focusing on short inter-
is shown in Figure 1 (a). The need for further complex data actions with one object. Table 1 shows that these datasets
with more hands and sophisticated object interactions pro- have action annotations however, there is no transition be-
motes the need for a more robust tracking system. The chal- tween different objects in order to accomplish a higher level
lenge with activity sequences is maintaining consistent 3D task.
tracking of hands over time. Existing marker-free automatic Due to self-occlusions and inter-instance occlusions, ac-
approaches use a high frame rate of 60 to 90 FPS during an- curate acquisition of hand-object poses is a very challenging
notation for a short duration and assume a maximum of two problem. Mono and stereo camera systems in datasets like
hands only [20, 21]. Alternatively, using a very large num- [17, 37] would require human annotation or additional sen-
ber of cameras [31, 32, 41, 49] would increase the robust- sor data. Therefore, multiple datasets follow a multi-view
ness of the systems, the scale comes with a high cost, syn- system [2, 21, 33, 44]. These annotation methods extract se-
chronization, and hardware requirements. This will limit mantic information from 2D views and fit hand and object
the number of interactions that could occur in one sequence. parametric models to them [3, 47]. These systems are lim-
Unlike multi-person pose estimation and tracking problems, ited by the number of objects actively moving in the scene
hands are visually more similar and have finer articulation as they are prone to failure. For example, [2] is limited to
which results in less continuity in trajectories. one hand and one moving object with multiple static objects
In our method, we build on top of the Dynamic Match- in the scene. Alternatively, [21, 33] are limited to 2 hands
ing (DM) component from Huang et al. [25], to robustly and one object only. A more reliable approach is to use
estimate 3D hands in sequences. We have new data patches marker-based motion capture systems, for example, Yang et
continuously added to the system, therefore, we develop a al.[57] use Optitrack cameras to acquire the object pose and
model-ensemble method. This facilitates fine-tuning the 2D annotate hand 2D key points through crowd-sourcing ser-
detection models on the new data patches only. vices. Similarly, Fan et al. [13] use Vicon Mocap cameras
We run exhaustive experiments on the part of our data to fully annotate hands and objects. The main issue with thelatter approach is that, unlike objects, hand keypoint estima-
tion methods should generalize to unseen hands, and the ex-
istence of markers will bias the model to detect the markers
instead. Therefore, there have been attempts to mitigate this
issue by erasing the markers using generative image models
[56]. Based on this information, we annotate the objects us-
ing a marker-based system and implement our own method
to semi-automatically annotate the hands with no markers.
2.2. Hand Mesh Recovery Methods
There have been many publications into the work of HMR
from single RGB images. Existing methods can be broadly
divided into two approaches, model based and model free
methods. Jiang et al. [29] attempts to combine the two
by using a MANO[47] model as a prior net to capture
the probability distribution of hand joints and mesh ver-
Figure 2. Tabletop Setup. Our setup consists of 8 RealSense
tices. Their Attention based Mesh Vertices Uncertainty Re-
D435 cameras (green ellipses), 8 Optitrack Prime 13 X cameras
gression(AMVUR) model learns the conditioned probabil-
(blue ellipses), and 4 spotlights (red ellipses). On the tabletop, we
ity distribution of joints and vertices based on the MANO
show the interaction objects
priors, allowing representation of hand poses larger than the
MANO subspace.
[18] uses a GCN to directly regress the mesh vertices onto
heatmaps generated by stacked hourglass network [42].
Model Based Methods: These approaches attempt to re-
Also works in 3D hand pose estimation like [28, 61], can
construct the 3D hand by regressing the pose and shape
easily be extended to generate hand meshes using paramet-
parameters of a parametric hand model like MANO [47].
ric model regression.
MANO defines a hand mesh captured by a differentiable
parameter space of 61 values learned from over 1000 high-
2.3. Activity Recognition and Segmentation Meth-
resolution 3D hand scans. Zhang et al. [60] utilizes an iter-
ods
ative regression approach to fit model parameters from 2D
joint heatmaps. Action recognition is an important part of human-centered
Transformers are a good approach in robustly estimating computer vision. Short-term action classification model
hand features from a Convolutional Neural Network (CNN) average 3D temporal convolutions [5][16][16]. However,
like Resnet [23] embeddings. Park et al. [45] regress these models have a limited clip duration that they can pro-
a MANO model onto these features to generate the hand cess. Models segmenting longer sequences of videos lever-
mesh. Hasson et al. [22] provide an end-to-end learn- age the latter classification models as feature extractors [34]
able model that predicts a MANO hand model and a recon- [14][58]. Alternatively, action detection based on hand 2D
structed interacting object from Resnet18 [23] embeddings, and 3D gained additional importance due to its applications
and regresses the poses together using a novel contact loss in AR/VR [43] [33] [48].
for realistic hand mesh recovery.
3. Data Acquisition and Annotation Setup
Model Free Methods: Hampali et al. [21, 35] utilises 3.1. Data Recording
transformer network on 2D heatmaps and embeddings from
Figure 2 shows our data recording setup. We use 8 RGB-
by a CNN to generate a 3D handpose. Moon et al.
D RealSense cameras and 8 Optitrack cameras for object
[40] creates an image-to-lixel prediction network using a
tracking. Since our activity sequences could last up to
PoseNet[6] to predict 3-dimensional lixel based heatmaps
2 minutes, reducing blurry frames is essential for smooth
of every joint. This is fed as input to a Meshnet model that
hand tracking. Even though we are using D435 model of
predicts the 3-dimensional lixel heatmaps of every vertex
cameras that advertise a global shutter. However this does
of the hand mesh. Graph Convolutional Network (GCN)
not apply to the color imager on the D435 which is a rolling
are also demonstrated as a reliable approach to estimate
shutter. In our recordings, we notice that normal ambient
hand meshes directly. For example Choi et al. [9] utilises
daylight and indoor light conditions produce multiple blurry
a PoseNet [6] to estimate relative 3D joints and a GCN to
images. In order to solve this issue while maintaining a sta-
estimate the 3D human and hand mesh vertices. Ge et al.
ble frame rate, we use 4 spotlights with diffusers and con-Dataset Modality Funct. Action. 3D 6D #Obj. Hands Mul. Subj.
intent annot Hands Object per seq per seq Inter #Sub. #Obj. #Env. #Seq. #Img #Cam Resolution
FPHA [17] RGB-D Y Y Y Y 1 1 N 6 4 3 1.2K 105K 1 1920x1080
DexYCB [2] RGB-D N N Y Y 1 1 N 10 20 1 1K 582K 8 640x480
H2O [33] RGB-D Y N Y Y 1 2 N 4 8 3 24 571K 5 1280x720
H2O3D [21] RGB-D N N Y Y 1 2 N 6 10 1 65 76K 5 640x480
MECCANO [46] RGB-D Y Y N N 20 2 N 20 20** 2 20 300K 1 1920x1080
OakInk [57] RGB-D Y Y Y Y 1 2 Y 12 32 1 1.3K 230K 4 640x480
HOI4D [37] RGB-D Y Y Y Y 1 1 N 9 20 610 5K 2.4M 1 1280x800
AssemblyHands[44] RGB/BW Y Y Y N - 2 N 34 - 1 4.3K* 3M 8+4 1080p/480p
ARCTIC [13] RGB Y N Y Y 1 2 N 9 11 1 339 2.1M 9 2880x2000
ADL4D (ours) RGB-D Y Y Y Y 4-12 2-4 Y 8 12 1 75 1.1M 8 1920x1080
Table 1. Related Datasets Comparison The action column denotes presence of action annotations if any. Number of objects Per seq.
denotes range of objects present and used in a single recording. *Assembly101[48] with 4.3K sequences and 20M images are subsampled
to generate Assembly Hands. Data is not provided if any sequence in whole is excluded. **Meccano sequnces consist of a single toy
assembly from 19 unique components, and 2 tools. Which are altogether grouped into 20 classes
Figure 3. Object Models. The set consists of 12 high-quality
textured meshes with their markers
Figure 4. Tail Action Distribution. A histogram plot of frame-
wise action distributions sorted by occurrence frequency. Fig-
figure the RealSense cameras with a fixed exposure time to ure(a). Shows Verb-Noun pairs co-occurrence frequency. Figure
the smallest possible value while no frames are blurry (e.g. (b) shows verb frequencies. Figure (c) shows Nouns (Interaction
0.2 ms). We collect the RGB-D data at 30 FPS and the Op- Objects) frequencies.
titrack cameras at 120 FPS.
3.2. Object Meshes in sub-figure (a), we can notice that actions with repeating
patterns (stir and eat) occupy many frames. Similarly, in
The used models are shown in Figure 3. We use a mix of sub-figure (c), multi-functional objects like spoon and fork,
YCB [4] and our own objects in order to have meaning- which correspond to (eat, stir) verbs, are the most frequent
ful activity scenarios. The objects are scanned using two among nouns.
high-resolution structured-light scanners [1] that are suit-
3.4. Calibration and Synchronization
able for small and medium object sizes. We provide the
textured meshes with 3 different resolutions. We ensure that
We use a checkerboard pattern to calibrate the RealSense
the YCB [3] scans are aligned with the original models.
cameras and ensure the highest re-projection error is less
than 0.05 pixels at 1280x720 resolution. By annotating
3.3. Action annotations
markers manually, we align the transformation trees of the
We use VIA software [12] to annotate videos. Since the RGB-D cameras to the mocap system world coordinate
videos are software-synchronized, we annotate one view frame to form one transformation tree. We also align the
and apply the labels to other views. In 4, we plot the fre- mesh scans to their markers by picking 3D points on the
quency of verbs, nouns, and actions (verb-noun pairs) in spherical markers’ surface and clustering them such that
multiples of 1 thousand. In sub-figure (b), we observe that each cluster centroid matches the marker centroid. The dif-
verbs that initiate interactions (pickup) and finish interac- ferent recorded sequences are then synchronized and sub-
tion (place) are the most common among verbs. Meanwhile, sampled at 20 FPS. We find that at 20 FPS is a good balancesearches for all clusters of detection pairs whose 3D pose
lies within δ of each other. This algorithm has a 3-
dynamic
camera dependency, such that it requires a single hand to be
detected in atleast 3 camera views to isolate it into an output
cluster. While we observe this approach to work well with
finetuned models on existing datasets, our initial batches of
data annotation are often unable to isolate all hands of users,
primarily due to insufficient detections caused by lower de-
tector performance. Therefore, in order to robustly uplift
2D detections to 3D, we also extend the DM algorithm with
temporal information from the previous frames to reduce
the 3-camera dependency to a 2-camera dependency. We
refer to this as the Tracking Mode (TM) in our paper. In
DM, the association to the previous frame is done based on
spatial proximity post-cluster estimation.
The DM and TM approaches search clusters within a cer-
tain radius in the 3D space. Due to varying speed of hand
Figure 5. Hand Pose Annotation GUI. We also support semi-
automatic annotation mode with live human supervision and con- movement or factors affecting 2D detector accuracy, a fixed
trol. threshold may however fail to annotate all frames for a full
sequence. Cases for these include when two hands approach
closer together but move slowly; a smaller threshold is re-
between redundancy and smooth image stream. quired to prevent incorrect clustering compared to when a
hand starts moving or has a sudden change in velocity or tra-
3.5. Hand Pose Annotation and Tracking
jectory during a sequence. We, therefore, design and com-
We take into consideration two aspects regarding the anno- pare two search approaches with different matching criteria
tation method. Firstly, the annotated sequences are long, (discussed in 3.6.2) for evaluating the quality of the clus-
1-2 minutes, synced at 20 fps. Secondly, We have 1-2 sub- ters over a given specific threshold range for δ or
tracking
jects in each sequence. Therefore, we developed our an- δ dynamic.
notation method to support automatic and semi-automatic
annotation. The early stages of data collection will require
more careful annotation with human supervision since key 3.6.1 Semi-Auto Annotation-Manual Searching
point estimators are not curated toward the data distribution.
As shown in Figure 5, we support a manual search method
Figure 5 shows the annotation GUI. The usage of the GUI
for our clustering parameters and overlay results on a GUI
is detailed in 3.6.1
where the multi-camera images are shown concurrently,
In Figure 6 we visualize the PCA components of the
with overlaid hands detection. There are 4 primary control
hand poses of ADL4D, DexYCB[2], and H2O [33]. The
windows providing access to different parameters of our ap-
visualization suggests that the poses in ADL4D are more di-
plication. Window A: The Main Window provides access
verse while sharing similarities with poses in DexYCB and
to incremental or automated frame steps forward or back,
H2O. We integrate multiple existing datasets into our sys-
the model selection from detector zoo, overlay configura-
tem, including [2, 33]. We use these two datasets for eval-
tion between raw 2D detections, matched 2D detections or
uating our hand annotation. We train/use different keypoint
re-projected 3D estimation. Window B: Is provided in case
estimators [30][55][38][11] on different dataset including
a user wishes to manually match hand indices. Window C:
[7] [33] [41], and incrementally add our own data patches.
Provides controls to the 3D matching parameters. Window
Since we may have multiple subjects in each scene, hand-
D: Provides a camera filtering option to reject detections.
side information is not enough to uplift 2D detections to 3D.
Therefore, we extend over the idea of DM by [25] to isolate
3D hands. We pair this with a weighted averaging of the 3.6.2 Search Matching Criterion and Auto Annotation
detected side classes to robustly estimate the side from all
matched detections. The search method will grow the set of inliers by searching
in the neighborhood of the accepted threshold δ or
dynamic
3.6. 3D Pose Subspace Clustering
δ found from the previous frame in the sequence.
tracking
The DM[25] algorithm is a clustering approach applied to We incrementally increase the magnitude of the offset and
the 3D pose space obtained by pairwise uplifting of every pick a result based on the matching criteria. We evaluate
detection from all views. Given a threshold δ , it three different matching criteria:
dynamicFigure 6. Hand Pose Variation. A PCA visualization of the first
two principal components of the hand poses. The plot shows that
our dataset ADL4D poses (red) are more diverse than DexYcb [2] Figure 7. H2O Annotation (Best shown with color). A com-
H2O [33] (blue) parison between H2O [33] Ground Truth (left) and our sequence
annotation (right). We render the corresponding Mano[47] Mesh
to one of the views. The red keypoints are the hand-tracking out-
• Re-projection Error (Repr), this approach filters clusters put, while the blue keypoints correspond to optimised Mano joints.
based on a cluster size threshold. We denote the set of From a qualitative perspective there is almost no observable differ-
accepted clusters as C . This approach will select the ence to H2O GT and our annotation.
large
re-projection-error minimizer c∗ as described in Equation
1, where π projects the 3D joints to each camera space. cm, the number of total skipped(unannotated) frames. For
c∗ = argmin c∈C large ||π(J 3c D ) − Jc 2d|| 2 2 (1) c bo om unp dle inte gn be oss x, w eve a a luls ao ti ore np mor et tt rh ice s 2 [D 3 6C ] O aC fteO r K pre oy jep co ti in nt g a n thd e
• Closest Displacement (CD) this searches from the last joints from 3D to 2D.
accepted threshold with increasing offsets until the first
4.2. Ablation Study on ADL4D
valid set of clusters for all hands is returned.
• No Search (NS) an average threshold with no search per- We conduct an ablation on all possible combinations of the
formed. search methods introduced in section 3.6 and the search
The search criteria algorithms are detailed in the supple- matching criteria introduced in 3.6.2 . This will enumer-
mentary material. ate six possible configurations. We automatically annotate
In the case where the search criteria fail to find a valid the single-subject test sequences (4 sequences) in ADL4D
cluster, the last successful cluster is used instead. using each combination. The results of the experiment can
be found in Table 3. We observe a significant improvement
4. Experiments across all metrics when comparing NS-DM to NS-TM, with
a 50% reduction in skipped frames and a 12% increase in
4.1. Annotation Method Experiments
tracking accuracy. With the implementation of the Repr,
We perform a detailed ablation study of the matching cri- we see a further reduction of 80% in skipped frames and a
teria introduced in section 3.6.2 to evaluate their impact 10% increase in tracking accuracy.
on the annotation accuracy. We perform this on subsets
4.3. Tracking Evaluation Across Multiple Datasets
of the test split of 3 different datasets in increasing or-
der of complexity. DexYCB[2] H2O[33], and our dataset In this experiment, we evaluate the performance of our an-
ADL4D. For each study a HRNet model[55] is trained on notation method on the sequences of DexYCB and H2O in-
the train splits of the respective dataset. To evaluate 3D er- troduced at the beginning of the section 4.1. We evaluate
ror, we report Mean Per-Joint Position Error (MPJPE) in the TM search method performance and report the results
millimeters, Percentage of Correct Keypoints - Area Under in Table 2. From Table 2 and the TM results in Table 3 we
the Curve (PCK-AUC) of each approach. Also, we evalu- can observe that the search method has a marginal impact on
ate the tracking accuracy for 3D keypoint trajectories over simple and short sequences like the ones in DexYCB. For
time by reporting the keypoint tracking accuracy[39] at 1 a slightly more difficult sequence like the ones in H2O, weMethod Criterion MPJPE AUC mAPBox mAPKpt Skipped Track Acc.
(abs) (abs) 50% 50:90% 50% 50:90% Frames @1cm
TM NS 5.18 0.8963 0.9900 0.7837 0.9759 0.7768 19 0.8353
H2O CD 5.15 0.8968 0.9899 0.7846 0.9759 0.7783 0 0.8367
Repr 5.79 0.8842 0.9845 0.7560 0.9565 0.7317 3 0.7634
TM NS 6.64 0.8671 0.9949 0.7690 0.8022 0.4414 0 0.6463
DYCB CD 6.64 0.8671 0.9949 0.7690 0.8022 0.4414 0 0.6463
Repr 6.62 0.8676 0.9899 0.7699 0.7944 0.4518 0 0.6405
Table 2. 3D Annotation Experiment DexYCB++H2O(10 seqs each). H2O contains 6654 total frames and DexYCB is 720 frames total
Method Criterion MPJPE AUC mAPBox mAPKpt Skipped Track Acc.
(abs) (abs) 50% 50:90% 50% 50:90% Frames @1cm
DM NS 18.30 0.7952 0.7027 0.6121 0.6861 0.6123 2005 0.6261
CD 18.30 0.7952 0.7027 0.6121 0.6861 0.6123 2005 0.6261
Repr 18.33 0.7947 0.6978 0.6101 0.6861 0.6112 2005 0.6261
TM NS 13.60 0.8326 0.8410 0.7312 0.8172 0.7279 1049 0.7413
CD 8.47 0.8769 0.9319 0.8043 0.9017 0.7974 332 0.7971
Repr 5.94 0.8952 0.9596 0.8323 0.9371 0.8328 226 0.8433
Table 3. 3D Annotation Experiment 4 ADL4D sequences with 6815 total frames. Our region growing approach combined with tracking
mode demonstrates a significant improvement in percentage of annotated frames over the Dynamic Matching, resulting in better absolute
MPJPE and Tracking score
can observe a slight drop in the number of skipped frames Train/Test DexYCB[2] H2O [33] ADL4D
(un-annotated frames) when using a search method com- DexYCB[2] 12.48 44.96 48.94
pared to NS. The impact of the search methods CD and H2O [33] 54.76 14.35 40.89
Repr increases with more complex sequences like the ones ADL4D 54.67 33.34 13.18
in ADL4D reported in Table 3. We can also observe that
the search criterion is inconsistent among different datasets. Table 4. I2L Meshnet[40] Cross-Dataset Evaluation. Cross
We hypothesize that this could be related to multiple factors, dataset evaluation against DexYCB and H2O datasets the reported
such as the number of hands, views, and frame rate. There- metrics are root relative MPJPE in millimeters
fore, we consider that the search criteria hyper-parameters
like cluster size threshold should be tuned for every multi- Train/Test DexYCB[2] H2O [33] ADL4D
camera system separately. DexYCB[2] 13.88 39.27 47.51
Finally, we visualize qualitative results on our annotation H2O [33] 54.58 13.28 35.36
method with TM - CD on one sequence of H2O 7. ADL4D 54.73 35.16 13.19
4.4. HMR from a Monocular RGB Image Table 5. HandOccnet[45] Cross-Dataset Evaluation. Cross
dataset evaluation against DexYCB and H2O datasets the reported
We run a cross-dataset evaluation against [2] and [33] where metrics are root relative MPJPE in millimeters
we use one dataset for training and testing on others. We
use the standard protocol for HMR evaluation introduced
4.5. Hand Action Segmentation
by [10], and compare the performance using root-relative
MPJPE. In Table 4 we show the results of the Lixel model
We evaluate the performance of hand pose information for
[40] (hand-model free) while in Table 5 show HandOccNet
action segmentation. The hand pose could be used as a com-
[45] (parametric model). We can see from Table 4 that mod-
mon input modality for deep learning models independent
els trained on ADL4D and tested on H2O are always better
of the input sensor (color-image, depth-image, or wearable
than the ones trained on DexYCB. The min cross dataset er-
glove). In this experiment, we use the verb labels for pre-
ror is 33.34 mm (in Table 4) when testing ADL4D on H2O.
diction as we are interested in the hand pose as input. When
We also can observe that the results on DexYCB are bad for
evaluating Action Segmentation (AS) models on datasets
both training and testing on DexYCB in Table 4 and Table
like [15, 37, 50], the TCN family of models have shown
5. This suggests that the data distribution is far away from
good performance, and different variations of these models
ADL4D and H2O. We suspect that this could be due to the
like the one referenced in [34, 58] are among the state of
blurry, low-resolution images in DexYCB. In Figure 8, we
the art models. We use an ASFormer to aggregate features,
show qualitative results from the model trained on ADL4D
generate frame-wise verb predictions, and evaluate the seg-
to other datasets.
mentation frame-wise accuracy, edit distance, and F1 scorepadded to have equal width and height. We use the video-
features library [26] to compute the I3D features based on
Raft flow [53] and RGB streams. We use I3D features as
a baseline and compare it to the hand pose predicted by an
HMR model [45]. We report the performance difference
in Table 6. We can observe that ASFormer performance is
better when using pose features for both frame-wise accu-
racy and segment-wise metrics. We suspect that the bet-
ter performance can be attributed to two reasons. First, the
hand pose values are local joint rotations, which will have
large changes in values only during action transitions and
dexterous manipulation. This “locality” will help the TCN
to better detect events. Secondly, the dynamic background
in the video generated by the hand trajectory is unsuitable
and would generate noisy I3D features due to the constant
motion and background changes. Finally, we note that the
frame-wise accuracy and F1 score values suggest that our
dataset ADL4D is challenging for the HAS task. We hy-
pothesize that this is due to the fine-grained actions with
long sequences and complex temporal contexts.
Features Acc. Edit F1@10,25,50
Figure 8. Cross Dataset HMR Qualitative Results. The input I3D 32.77 41.66 24.59 18.21 7.12
images, from DexYCB [2], H2O[33], and ADL4D in order. Odd Pose [45] 55.04 52.66 55.01 33.78 33.78
rows correspond to I2L [40] and Even rows correspond to Han-
dOccNet [45] both trained on ADL4D. Table 6. Hand Action Segmentation using ASFormer [58]
5. Conclusion
We introduce ADL4D, a dataset for 4D human activities.
Unlike existing HOI datasets, we focus on rich context
long-term activities with multi-subject and multi-object in-
teractions. We evaluated the dataset in the areas of HMR
and HAS. We also demonstrated the effectiveness of our
marker-free hand annotation system for long sequences and
crowded scenes.
5.1. Limitations and Future Work
As we use an aggregate over multiple 2D keypoint esti-
mators in our method, hence we use SVD to triangulate
the keypoints. The results can still be improved by using
Figure 9. Hand Action Segmentation Pipeline. An illustrative
a learnable triangulation approach [27, 44], or enhancing
figure for our HAS feature extraction pipeline. We use the hand-
the 2D keypoints using Epipolar Transformers [24]. How-
bounding box trajectory as input to apply hand-level action seg-
ever, we argue that in the early stages of data collection,
mentation. As a baseline, I3D features are extracted based on
RGB and Flow. We compare this against hand pose predictions the deep-learning-based methods have a bad cross-dataset
from [45]. ASFormer is used as a Temporal Convolution Net- generalization [44], and they have to be trained on simi-
work (TCN). lar data distribution. We think that our annotation pipeline
can help with accelerating the annotation process for similar
datasets, especially with the support of our semi-automatic,
with overlap thresholds 10%,25%, and 50%. We extract and automatic annotation setups. Regarding our proposed
I3D [5] using RGB and Flow. To support hand-level pre- dataset, we think that ADL4D will facilitate a new range
dictions, we generate a video of each hand trajectory us- of applications that was not possible before. For example,
ing the hand bounding box and the hand image. Figure using hand-level action annotations which are missing in
9 illustrates the feature extraction pipeline. The image isthe existing dataset, ADL4D could help transition action to References
motion models [8, 19, 54] where existing generative action-
[1] Arctec 3d structured light scanning. https://www.
based HOI animations are limited to pick up, and grasp, or
artec3d.com/ [Accessed: (18.11.2023)]. 4
handover actions [52][57].
[2] 2, 4, 5, 6, 7, 8
[3] Berk Calli, Arjun Singh, Aaron Walsman, Siddhartha Srini-
vasa, Pieter Abbeel, and Aaron M. Dollar. The ycb object
and model set: Towards common benchmarks for manipu-
lation research. In 2015 International Conference on Ad-
vanced Robotics (ICAR), pages 510–517, 2015. 2, 4
[4] Berk Calli, Aaron Walsman, Arjun Singh, Siddhartha Srini-
vasa, Pieter Abbeel, and Aaron M. Dollar. Benchmarking in
manipulation research: Using the yale-cmu-berkeley object
and model set. IEEE Robotics and Automation Magazine, 22
(3):36–52, 2015. 4
[5] Joao Carreira and Andrew Zisserman. Quo vadis, action
recognition? a new model and the kinetics dataset. In pro-
ceedings of the IEEE Conference on Computer Vision and
Pattern Recognition, pages 6299–6308, 2017. 3, 8
[6] J. Chang, G. Moon, and K. Lee. V2v-posenet: Voxel-to-
voxel prediction network for accurate 3d hand and human
pose estimation from a single depth map. In 2018 IEEE/CVF
Conference on Computer Vision and Pattern Recognition
(CVPR), pages 5079–5088, Los Alamitos, CA, USA, 2018.
IEEE Computer Society. 3
[7] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov,
Ankur Handa, Jonathan Tremblay, Yashraj S. Narang, Karl
Van Wyk, Umar Iqbal, Stan Birchfield, Jan Kautz, and Dieter
Fox. DexYCB: A benchmark for capturing hand grasping of
objects. In IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), 2021. 5
[8] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao
Chen, and Gang Yu. Executing your commands via motion
diffusion in latent space. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 18000–18010, 2023. 9
[9] Hongsuk Choi, Gyeongsik Moon, and Kyoung Mu Lee.
Pose2mesh: Graph convolutional network for 3d human pose
and mesh recovery from a 2d human pose. In European Con-
ference on Computer Vision (ECCV), 2020. 3
[10] Jimei Yang Bryan Russel Max Argus Christian Zimmer-
mann, Duygu Ceylan and Thomas Brox. Freihand: A dataset
for markerless capture of hand pose and shape from single
rgb images. In IEEE International Conference on Computer
Vision (ICCV), 2019. 7
[11] MMPose Contributors. Openmmlab pose estimation tool-
box and benchmark. https://github.com/open-
mmlab/mmpose, 2020. 5
[12] A. Dutta, A. Gupta, and A. Zissermann. VGG image anno-
tator (VIA). http://www.robots.ox.ac.uk/ vgg/software/via/,
2016. 4
[13] Zicong Fan, Omid Taheri, Dimitrios Tzionas, Muhammed
Kocabas, Manuel Kaufmann, Michael J. Black, and Otmar
Hilliges. ARCTIC: A dataset for dexterous bimanual hand-
object manipulation. In Proceedings IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), 2023. 2,
4[14] Yazan Abu Farha and Juergen Gall. Ms-tcn: Multi-stage transformer: Anchor-to-joint transformer network for 3d in-
temporal convolutional network for action segmentation, teracting hand pose estimation from a single rgb image. In
2019. 3 Proceedings of the IEEE/CVF Conference on Computer Vi-
[15] Alireza Fathi, Xiaofeng Ren, and James M Rehg. Learning sion and Pattern Recognition, pages 8846–8855, 2023. 3
to recognize objects in egocentric activities. In CVPR 2011, [29] Zheheng Jiang, Hossein Rahmani, Sue Black, and Bryan M
pages 3281–3288. IEEE, 2011. 7 Williams. A probabilistic attention model with occlusion-
[16] Christoph Feichtenhofer. X3d: Expanding architectures for aware texture regression for 3d hand reconstruction from a
efficient video recognition. In Proceedings of the IEEE/CVF single rgb image. In Proceedings of the IEEE/CVF Con-
conference on computer vision and pattern recognition, ference on Computer Vision and Pattern Recognition, pages
pages 203–213, 2020. 3 758–767, 2023. 3
[17] Guillermo Garcia-Hernando, Shanxin Yuan, Seungryul [30] Glenn Jocher, Ayush Chaurasia, and Jing Qiu. Ultralytics
Baek, and Tae-Kyun Kim. First-person hand action bench- yolov8, 2023. 5
mark with rgb-d videos and 3d hand pose annotations. In [31] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe,
Proceedings of Computer Vision and Pattern Recognition Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser
(CVPR), 2018. 2, 4 Sheikh. Panoptic studio: A massively multiview system for
social motion capture. In The IEEE International Conference
[18] Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue, Yingy-
on Computer Vision (ICCV), 2015. 2
ing Wang, Jianfei Cai, and Junsong Yuan. 3d hand shape
[32] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei
and pose estimation from a single rgb image. pages 10825–
10834, 2019. 3 Tan, Lin Gui, Sean Banerjee, Timothy Scott Godisart, Bart
Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and
[19] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao
Yaser Sheikh. Panoptic studio: A massively multiview sys-
Sun, Annan Deng, Minglun Gong, and Li Cheng. Ac-
tem for social interaction capture. IEEE Transactions on Pat-
tion2motion: Conditioned generation of 3d human motions.
tern Analysis and Machine Intelligence, 2017. 2
In Proceedings of the 28th ACM International Conference on
[33] Taein Kwon, Bugra Tekin, Jan Stu¨hmer, Federica Bogo,
Multimedia, pages 2021–2029, 2020. 9
and Marc Pollefeys. H2o: Two hands manipulating objects
[20] Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vin-
for first person interaction recognition. In Proceedings of
cent Lepetit. Honnotate: A method for 3d annotation of hand
the IEEE/CVF International Conference on Computer Vision
and object poses. In CVPR, 2020. 2
(ICCV), pages 10138–10148, 2021. 2, 3, 4, 5, 6, 7, 8
[21] Shreyas Hampali, Sayan Deb Sarkar, Mahdi Rad, and Vin-
[34] Shijie Li, Yazan Abu Farha, Yun Liu, Ming-Ming Cheng, and
cent Lepetit. Keypoint transformer: Solving joint identifica-
Juergen Gall. Ms-tcn++: Multi-stage temporal convolutional
tion in challenging hands and object interactions for accurate
network for action segmentation, 2020. 3, 7
3d pose estimation. In IEEE Computer Vision and Pattern
[35] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end hu-
Recognition Conference, 2022. 2, 3, 4
man pose and mesh reconstruction with transformers. In
[22] Yana Hasson, Gu¨l Varol, Dimitris Tzionas, Igor Kalevatykh,
CVPR, 2021. 3
Michael J. Black, Ivan Laptev, and Cordelia Schmid. Learn-
[36] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.
ing joint reconstruction of hands and manipulated objects. In
Bourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva
CVPR, 2019. 3
Ramanan, Piotr Doll’a r, and C. Lawrence Zitnick. Microsoft
[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
COCO: common objects in context. CoRR, abs/1405.0312,
Deep residual learning for image recognition. In 2016 IEEE
2014. 6
Conference on Computer Vision and Pattern Recognition
[37] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan,
(CVPR), pages 770–778, 2016. 3
Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, and Li Yi.
[24] Yihui He, Rui Yan, Katerina Fragkiadaki, and Shoou-I Yu. Hoi4d: A 4d egocentric dataset for category-level human-
Epipolar transformers. In Proceedings of the ieee/cvf con- object interaction. In Proceedings of the IEEE/CVF Con-
ference on computer vision and pattern recognition, pages ference on Computer Vision and Pattern Recognition, pages
7779–7788, 2020. 8 21013–21022, 2022. 2, 4, 7
[25] Congzhentao Huang, Shuai Jiang, Yang Li, Ziyue Zhang, [38] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris Mc-
Jason Traish, Chen Deng, Sam Ferguson, and Richard Yi Clanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuo-
Da Xu. End-to-end dynamic matching network for multi- Ling Chang, Ming Yong, Juhyun Lee, Wan-Teh Chang, Wei
view multi-person 3d pose estimation. In Computer Vision – Hua, Manfred Georg, and Matthias Grundmann. Mediapipe:
ECCV 2020, pages 477–493, Cham, 2020. Springer Interna- A framework for perceiving and processing reality. In Third
tional Publishing. 2, 5 Workshop on Computer Vision for AR/VR at IEEE Computer
[26] Vladimir Iashin and Esa Rahtu. Multi-modal dense video Vision and Pattern Recognition (CVPR) 2019, 2019. 5
captioning, 2020. 8 [39] Anton Milan, Laura Leal-Taixe´, Ian Reid, and Stefan Roth.
[27] Karim Iskakov, Egor Burkov, Victor Lempitsky, and Yury Mot16: A benchmark for multi-object tracking. 2016. 6
Malkov. Learnable triangulation of human pose. In Interna- [40] Gyeongsik Moon and Kyoung Mu Lee. I2l-meshnet: Image-
tional Conference on Computer Vision (ICCV), 2019. 8 to-lixel prediction network for accurate 3d human pose and
[28] Changlong Jiang, Yang Xiao, Cunlin Wu, Mingyang Zhang, mesh estimation from a single rgb image. In European Con-
Jinghong Zheng, Zhiguo Cao, and Joey Tianyi Zhou. A2j- ference on Computer Vision (ECCV), 2020. 3, 7, 8[41] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori, [54] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel
and Kyoung Mu Lee. Interhand2. 6m: A dataset and base- Cohen-or, and Amit Haim Bermano. Human motion diffu-
line for 3d interacting hand pose estimation from a single sion model. In The Eleventh International Conference on
rgb image. In Computer Vision–ECCV 2020: 16th Euro- Learning Representations, 2023. 9
pean Conference, Glasgow, UK, August 23–28, 2020, Pro- [55] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang,
ceedings, Part XX 16, pages 548–564. Springer, 2020. 2, 5 Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui
[42] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour- Tan, Xinggang Wang, Wenyu Liu, and Bin Xiao. Deep
glass networks for human pose estimation. In Computer Vi- high-resolution representation learning for visual recogni-
sion – ECCV 2016, pages 483–499, Cham, 2016. Springer tion. IEEE Transactions on Pattern Analysis and Machine
International Publishing. 3 Intelligence, 43(10):3349–3364, 2021. 5, 6
[43] Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, [56] Erwin Wu, Hayato Nishioka, Shinichi Furuya, and Hideki
Luan Tran, and Cem Keskin. AssemblyHands: towards Koike. Marker-removal networks to collect precise 3d hand
egocentric activity understanding via 3d hand pose estima- data for rgb-based estimation and its application in piano. In
tion. In Proceedings of the IEEE/CVF Conference on Com- 2023 IEEE/CVF Winter Conference on Applications of Com-
puter Vision and Pattern Recognition (CVPR), pages 12999– puter Vision (WACV), pages 2976–2985, 2023. 3
13008, 2023. 3 [57] Lixin Yang, Kailin Li, Xinyu
[44] Takehiko Ohkawa, Kun He, Fadime Sener, Tomas Hodan, Z@INPROCEEDINGS7780459, author=He, Kaiming
Luan Tran, and Cem Keskin. AssemblyHands: towards and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian,
egocentric activity understanding via 3d hand pose estima- booktitle=2016 IEEE Conference on Computer Vision and
tion. In Proceedings of the IEEE/CVF Conference on Com- Pattern Recognition (CVPR), title=Deep Residual Learning
puter Vision and Pattern Recognition (CVPR), pages 12999– for Image Recognition, year=2016, volume=, number=,
13008, 2023. 2, 4, 8 pages=770-778, doi=10.1109/CVPR.2016.90han, Fei Wu,
[45] JoonKyu Park, Yeonguk Oh, Gyeongsik Moon, Hongsuk Anran Xu, Liu Liu, and Cewu Lu. OakInk: A large-scale
Choi, and Kyoung Mu Lee. Handoccnet: Occlusion-robust knowledge repository for understanding hand-object inter-
3d hand mesh estimation network. In Conference on Com- action. In IEEE/CVF Conference on Computer Vision and
puter Vision and Pattern Recognition (CVPR), 2022. 3, 7, Pattern Recognition (CVPR), 2022. 2, 4, 9
8 [58] Fangqiu Yi, Hongyu Wen, and Tingting Jiang. Asformer:
[46] Francesco Ragusa, Antonino Furnari, and Giovanni Maria Transformer for action segmentation, 2021. 3, 7, 8
Farinella. Meccano: A multimodal egocentric dataset for hu- [59] He Zhang, Yuting Ye, Takaaki Shiratori, and Taku Komura.
mans behavior understanding in the industrial-like domain. Manipnet: Neural manipulation synthesis with a hand-object
Computer Vision and Image Understanding (CVIU), 2023. spatial representation. ACM Trans. Graph., 40(4), 2021. 2
2, 4
[60] Xiong Zhang, Qiang Li, Hong Mo, Wenbo Zhang, and Wen
[47] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Zheng. End-to-end hand mesh recovery from a monocular
Embodied hands: Modeling and capturing hands and bod- rgb image. 2019. 3
ies together. ACM Transactions on Graphics, (Proc. SIG-
[61] Christian Zimmermann and Thomas Brox. Learning to es-
GRAPH Asia), 36(6), 2017. 2, 3, 6
timate 3d hand pose from single rgb images. In IEEE In-
[48] F. Sener, D. Chatterjee, D. Shelepov, K. He, D. Singhania, R.
ternational Conference on Computer Vision (ICCV), 2017.
Wang, and A. Yao. Assembly101: A large-scale multi-view
https://arxiv.org/abs/1705.01389. 3
video dataset for understanding procedural activities. CVPR
2022. 2, 3, 4
[49] Tomas Simon, Hanbyul Joo, and Yaser Sheikh. Hand key-
point detection in single images using multiview bootstrap-
ping. CVPR, 2017. 2
[50] Sebastian Stein and Stephen J McKenna. Combining em-
bedded accelerometers with computer vision for recognizing
food preparation activities. In Proceedings of the 2013 ACM
international joint conference on Pervasive and ubiquitous
computing, pages 729–738, 2013. 7
[51] Omid Taheri, Nima Ghorbani, Michael J. Black, and Dim-
itrios Tzionas. GRAB: A dataset of whole-body human
grasping of objects. In European Conference on Computer
Vision (ECCV), 2020. 2
[52] Omid Taheri, Vasileios Choutas, Michael J. Black, and Dim-
itrios Tzionas. GOAL: Generating 4D whole-body motion
for hand-object grasping. In Conference on Computer Vision
and Pattern Recognition (CVPR), 2022. 2, 9
[53] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field
transforms for optical flow, 2020. 8