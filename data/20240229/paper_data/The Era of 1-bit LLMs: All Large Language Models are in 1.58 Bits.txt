The Era of 1-bit LLMs:
All Large Language Models are in 1.58 Bits
ShumingMa∗ HongyuWang∗ LingxiaoMa LeiWang WenhuiWang
ShaohanHuang LiDong RuipingWang JilongXue FuruWei⋄
https://aka.ms/GeneralAI
Abstract
Recentresearch,suchasBitNet[WMD+23],ispavingthewayforaneweraof1-
bitLargeLanguageModels(LLMs).Inthiswork,weintroducea1-bitLLMvariant,
namelyBitNetb1.58,inwhicheverysingleparameter(orweight)oftheLLMis
ternary{-1,0,1}. Itmatchesthefull-precision(i.e.,FP16orBF16)Transformer
LLM with the same model size and training tokens in terms of both perplexity
andend-taskperformance,whilebeingsignificantlymorecost-effectiveinterms
oflatency,memory,throughput,andenergyconsumption. Moreprofoundly,the
1.58-bitLLMdefinesanewscalinglawandrecipefortrainingnewgenerationsof
LLMsthatarebothhigh-performanceandcost-effective. Furthermore,itenables
anewcomputationparadigmandopensthedoorfordesigningspecifichardware
optimizedfor1-bitLLMs.
P Pareto Improvement
e
ofr
mr BitNet b1.58 (This Work) Transformer LLMs
a
n {-1, 0, 1} 16-bit Float (FP16/BF16)
c
e
1 -1 … 1 0.2961-0.0495 … -0.4765
W= 0 … -1 -1 W= 0.0413 ... 0.2812 0.2403
-1 1 … 0 -0.1808 0.1304 … -0.1771
-1 … 0 -1 -0.4809 … -0.1741-0.3853
Cost
Y = f(W, X) Model W Input X Output Y
0.2961 -0.0495 -0.0924-0.4765 𝒙𝟎 𝟎.𝟐𝟗𝟔𝟏𝒙𝟎−𝟎.𝟎𝟒𝟗𝟓𝒙𝟏−𝟎.𝟎𝟗𝟐𝟒𝒙𝟐−𝟎.𝟒𝟕𝟔𝟓𝒙𝟑
FP16
0.0413 0.3397 0.28120.2403 𝒙𝟏 GPU
…
-0.1808 0.1304 0.4322-0.1771 𝒙𝟐
-0.4809 0.3244 -0.1741-0.3853 𝒙𝟑
1 -1 -1 1 𝒙𝟎 𝒙𝟎−𝒙𝟏−𝒙𝟐+𝒙𝟑
0 1 -1 -1 𝒙𝟏 New
1(.58)-bit …
-1 0 1 -1 𝒙𝟐 Hardware
-1 1 1 0 𝒙𝟑
Figure1: 1-bitLLMs(e.g.,BitNetb1.58)provideaParetosolutiontoreduceinferencecost(latency,
throughput, and energy) of LLMs while maintaining model performance. The new computation
paradigmofBitNetb1.58callsforactionstodesignnewhardwareoptimizedfor1-bitLLMs.
∗Equalcontribution.⋄Correspondingauthor.S.Ma,L.Ma,L.Wang,W.Wang,S.Huang,L.Dong,J.Xue,
F.WeiarewithMicrosoftResearch.H.WangandR.WangarewithUniversityofChineseAcademyofSciences.
4202
beF
72
]LC.sc[
1v46771.2042:viXra1 TheEraof1-bitLLMs
Inrecentyears,thefieldofAIhasseenarapidgrowthinthesizeandcapabilitiesofLargeLanguage
Models(LLMs). Thesemodelshavedemonstratedremarkableperformanceinawiderangeofnatural
languageprocessingtasks,buttheirincreasingsizehasposedchallengesfordeploymentandraised
concerns about their environmental and economic impact due to high energy consumption. One
approachtoaddressthesechallengesistousepost-trainingquantizationtocreatelow-bitmodels
for inference [XLS+23, FAHA23, CCKS23, TCS+24]. This technique reduces the precision of
weightsandactivations,significantlyreducingthememoryandcomputationalrequirementsofLLMs.
Thetrendhasbeentomovefrom16bitstolowerbits,suchas4-bitvariants[FAHA23,LTT+23].
However,post-trainingquantizationissub-optimal,eventhoughitiswidelyusedinindustryLLMs.
Recentworkon1-bitmodelarchitectures,suchasBitNet[WMD+23],presentsapromisingdirection
forreducingthecostofLLMswhilemaintainingtheirperformance. VanillaLLMsarein16-bit
floatingvalues(i.e.,FP16orBF16),andthebulkofanyLLMsismatrixmultiplication. Therefore,
themajorcomputationcostcomesfromthefloating-pointadditionandmultiplicationoperations. In
contrast,thematrixmultiplicationofBitNetonlyinvolvesintegeraddition,whichsavesordersof
energycostforLLMs. Asthefundamentallimittocomputeperformanceinmanychipsispower,the
energysavingscanalsobetranslatedintofastercomputation.
Inadditiontocomputation,theprocessoftransferringmodelparametersfromDRAMtothememory
ofanon-chipaccelerator(e.g.,SRAM)canbeexpensiveduringinference. Therehavebeenattempts
toenlargeSRAMtoimprovethroughput,butthisintroducessignificantlyhighercoststhanDRAM.
Comparedtofull-precisionmodels,1-bitLLMshaveamuchlowermemoryfootprintfrombotha
capacityandbandwidthstandpoint. Thiscansignificantlyreducethecostandtimeofloadingweights
fromDRAM,leadingtofasterandmoreefficientinference.
Inthiswork,weintroduceasignificant1-bitLLMvariantcalledBitNetb1.58,whereeveryparameter
isternary,takingonvaluesof{-1,0,1}. Wehaveaddedanadditionalvalueof0totheoriginal1-bit
BitNet,resultingin1.58bitsinthebinarysystem. BitNetb1.58retainsallthebenefitsoftheoriginal
1-bit BitNet, including its new computation paradigm, which requires almost no multiplication
operationsformatrixmultiplicationandcanbehighlyoptimized. Additionally,ithasthesameenergy
consumptionastheoriginal1-bitBitNetandismuchmoreefficientintermsofmemoryconsumption,
throughputandlatencycomparedtoFP16LLMbaselines. Furthermore, BitNetb1.58offerstwo
additionaladvantages. Firstly,itsmodelingcapabilityisstrongerduetoitsexplicitsupportforfeature
filtering,madepossiblebytheinclusionof0inthemodelweights,whichcansignificantlyimprove
theperformanceof1-bitLLMs. Secondly,ourexperimentsshowthatBitNetb1.58canmatchfull
precision(i.e.,FP16)baselinesintermsofbothperplexityandend-taskperformance,startingfroma
3Bsize,whenusingthesameconfiguration(e.g.,modelsize,trainingtokens,etc.).
2 BitNetb1.58
BitNetb1.58isbasedontheBitNetarchitecture,whichisaTransformerthatreplacesnn.Linearwith
BitLinear. Itistrainedfromscratch,with1.58-bitweightsand8-bitactivations. Comparedtothe
originalBitNet,itintroducessomemodificationsthatwesummarizebelow.
QuantizationFunction. Toconstraintheweightsto-1,0,or+1,weadoptanabsmeanquantization
function. Itfirstscalestheweightmatrixbyitsaverageabsolutevalue,andthenroundeachvalueto
thenearestintegeramong{-1,0,+1}:
W
W(cid:102) =RoundClip( ,−1,1), (1)
γ+ϵ
RoundClip(x,a,b)=max(a,min(b,round(x))), (2)
1 (cid:88)
γ = |W |. (3)
nm ij
ij
ThequantizationfunctionforactivationsfollowsthesameimplementationinBitNet,exceptthat
we do not scale the activations before the non-linear functions to the range [0,Q ]. Instead, the
b
2Models Size Memory(GB)↓ Latency(ms)↓ PPL↓
LLaMALLM 700M 2.08(1.00x) 1.18(1.00x) 12.33
BitNetb1.58 700M 0.80(2.60x) 0.96(1.23x) 12.87
LLaMALLM 1.3B 3.34(1.00x) 1.62(1.00x) 11.25
BitNetb1.58 1.3B 1.14(2.93x) 0.97(1.67x) 11.29
LLaMALLM 3B 7.89(1.00x) 5.07(1.00x) 10.04
BitNetb1.58 3B 2.22(3.55x) 1.87(2.71x) 9.91
BitNetb1.58 3.9B 2.38(3.32x) 2.11(2.40x) 9.62
Table1: PerplexityaswellasthecostofBitNetb1.58andLLaMALLM.
Models Size ARCe ARCc HS BQ OQ PQ WGe Avg.
LLaMALLM 700M 54.7 23.0 37.0 60.0 20.2 68.9 54.8 45.5
BitNetb1.58 700M 51.8 21.4 35.1 58.2 20.0 68.1 55.2 44.3
LLaMALLM 1.3B 56.9 23.5 38.5 59.1 21.6 70.0 53.9 46.2
BitNetb1.58 1.3B 54.9 24.2 37.7 56.7 19.6 68.8 55.8 45.4
LLaMALLM 3B 62.1 25.6 43.3 61.8 24.6 72.1 58.2 49.7
BitNetb1.58 3B 61.4 28.3 42.9 61.5 26.6 71.5 59.3 50.2
BitNetb1.58 3.9B 64.2 28.7 44.2 63.5 24.2 73.2 60.5 51.2
Table2: Zero-shotaccuracyofBitNetb1.58andLLaMALLMontheendtasks.
activationsareallscaledto[−Q ,Q ]pertokentogetridofthezero-pointquantization. Thisis
b b
moreconvenientandsimpleforbothimplementationandsystem-leveloptimization,whileintroduces
negligibleeffectstotheperformanceinourexperiments.
LLaMA-alikeComponents. ThearchitectureofLLaMA[TLI+23,TMS+23]hasbeenthede-
facto backbone for open-source LLMs. To embrace the open-source community, our design
of BitNetb1.58 adopts the LLaMA-alike components. Specifically, it uses RMSNorm [ZS19],
SwiGLU[Sha20],rotaryembedding[SAL+24],andremovesallbiases. Inthisway,BitNetb1.58
canbeintegratedintothepopularopen-sourcesoftware(e.g.,Huggingface,vLLM[KLZ+23],and
llama.cpp2)withminimalefforts.
3 Results
WecomparedBitNetb1.58toourreproducedFP16LLaMALLMinvarioussizes. Toensureafair
comparison,wepre-trainedthemodelsontheRedPajamadataset[Com23]for100billiontokens.
Weevaluatedthezero-shotperformanceonarangeoflanguagetasks,includingARC-Easy[YBS19],
ARC-Challenge[YBS19],Hellaswag[ZHB+19],Winogrande[SBBC20],PIQA[BZB+19],Open-
bookQA [MCKS18], and BoolQ [CLC+19]. We also reported the validation perplexity on the
WikiText2[MXBS16]andC4[RSR+19]datasets.
WecomparedtheruntimeGPUmemoryandlatencyofbothLLaMALLMandBitNetb1.58. The
resultsweremeasuredusingtheFasterTransformer3 codebase,whichiswell-optimizedforLLM
inferencelatencyonGPUdevices. The2-bitkernelfromLadder[WMC+23]isalsointegratedfor
BitNetb1.58. Wereportedthetimeperoutputtoken,asitisthemajorcostforinference.
Table1summarizestheperplexityandthecostforBitNetb1.58andLLaMALLM. Itshowsthat
BitNetb1.58startstomatchfullprecisionLLaMALLMat3Bmodelsizeintermsofperplexity,
whilebeing2.71timesfasterandusing3.55timeslessGPUmemory. Inparticular,BitNetb1.58with
a3.9Bmodelsizeis2.4timesfaster,consumes3.32timeslessmemory,butperformssignificantly
betterthanLLaMALLM3B.
2https://github.com/ggerganov/llama.cpp
3https://github.com/NVIDIA/FasterTransformer
3102
BitNet b1.58 102 BitNet b1.58
LLaMA 4.10x LLaMA 7.16x
101 3.68x 5.12x
101
2.90x 4.40x
2.71x 3.55x
2.93x
1.67x
100
100
1.3B 3B 7B 13B 70B 1.3B 3B 7B 13B 70B
Model Size Model Size
Figure2: Decodinglatency(Left)andmemoryconsumption(Right)ofBitNetb1.58varyingthe
modelsize.
Models Size MaxBatchSize Throughput(tokens/s)
LLaMALLM 70B 16(1.0x) 333(1.0x)
BitNetb1.58 70B 176(11.0x) 2977(8.9x)
Table3: ComparisonofthethroughputbetweenBitNetb1.5870BandLLaMALLM70B.
Table2reportsthedetailedresultsofthezero-shotaccuracyontheendtasks.Wefollowedthepipeline
fromlm-evaluation-harness4toperformtheevaluation. Theresultsshowthattheperformancegap
between BitNetb1.58 and LLaMALLM narrows as the model size increases. More importantly,
BitNetb1.58canmatchtheperformanceofthefullprecisionbaselinestartingfroma3Bsize.Similar
totheobservationoftheperplexity,theend-taskresultsrevealthatBitNetb1.583.9Boutperforms
LLaMALLM 3Bwith lower memory andlatency cost. This demonstratesthat BitNetb1.58 is a
Paretoimprovementoverthestate-of-the-artLLMmodels.
MemoryandLatency Wefurtherscaledupthemodelsizeto7B,13B,and70Bandevaluatedthe
cost. Figure2illustratesthetrendsoflatencyandmemory,showingthatthespeed-upincreasesasthe
modelsizescales. Inparticular,BitNetb1.5870Bis4.1timesfasterthantheLLaMALLMbaseline.
Thisisbecausethetimecostfornn.Lineargrowswiththemodelsize. Thememoryconsumption
followsasimilartrend,astheembeddingremainsfullprecisionanditsmemoryproportionissmaller
forlargermodels. Bothlatencyandmemoryweremeasuredwitha2-bitkernel,sothereisstillroom
foroptimizationtofurtherreducethecost.
Energy WealsoestimatethearithmeticoperationsenergyconsumptionofbothBitNetb1.58and
LLaMALLM. Wefocusmainlyonthecalculationformatrixmultiplication, sinceitcontributes
themosttothecostofLLMs. Figure3illustratesthecompositionoftheenergycost. Themajority
ofBitNetb1.58isINT8additioncalculation,whileLLaMALLMconsistsofbothFP16addition
andFP16multiplication. Accordingtotheenergymodelin[Hor14,ZZL22], BitNetb1.58saves
71.4timesarithmeticoperationsenergyconsumptionformatrixmultiplicationon7nmchips. We
furtherreportedtheend-to-endenergycostformodelswith512tokens. Ourresultsshowthatasthe
modelsizescales,BitNetb1.58becomesincreasinglymoreefficientintermsofenergyconsumption
comparedtotheFP16LLaMALLMbaseline. Thisisduetothefactthatthepercentageofnn.Linear
growswiththemodelsize,whilethecostfromothercomponentsissmallerforlargermodels.
Throughput WecomparethethroughputofBitNetb1.58andLLaMALLMwith70Bparameters
ontwo80GBA100cards,usingpipelineparallelism[HCB+19]sothatLLaMALLM70Bcouldbe
runonthedevices. WeincreasedthebatchsizeuntiltheGPUmemorylimitwasreached,witha
sequencelengthof512. Table3showsthatBitNetb1.5870Bcansupportupto11timesthebatch
sizeofLLaMALLM,resultingan8.9timeshigherthroughput.
4https://github.com/EleutherAI/lm-evaluation-harness
4
)sm(
ycnetaL
)BG(
yromeM0.5 INT8 Add
101 B LLit aN Me At b1.58
FP16 Add
41.2x
FP16 Mul
0.4
100
32.9x
0.3
71.4x 29.1x
0.2 10 1 21.7x
18.6x
0.1
1.3B 3B 7B 13B 70B
0.0
BitNet b1.58 LLaMA Model Size
Figure3: EnergyconsumptionofBitNetb1.58comparedtoLLaMALLMat7nmprocessnodes. On
theleftisthecomponentsofarithmeticoperationsenergy. Ontherightistheend-to-endenergycost
acrossdifferentmodelsizes.
Models Tokens Winogrande PIQA SciQ LAMBADA ARC-easy Avg.
StableLM-3B 2T 64.56 76.93 90.75 66.09 67.78 73.22
BitNetb1.583B 2T 66.37 78.40 91.20 67.63 68.12 74.34
Table4: ComparisonofBitNetb1.58withStableLM-3Bwith2Ttokens.
BitNetb1.58isenablinganewscalinglawwithrespecttomodelperformanceandinference
cost. Asareference,wecanhavethefollowingequivalencebetweendifferentmodelsizesin1.58-bit
and16-bitbasedontheresultsinFigure2and3.
• 13BBitNetb1.58ismoreefficient,intermsoflatency,memoryusageandenergyconsump-
tion,than3BFP16LLM.
• 30BBitNetb1.58ismoreefficient,intermsoflatency,memoryusageandenergyconsump-
tion,than7BFP16LLM.
• 70BBitNetb1.58ismoreefficient,intermsoflatency,memoryusageandenergyconsump-
tion,than13BFP16LLM.
Trainingwith2TTokens ThenumberoftrainingtokensisacrucialfactorforLLMs. Totest
the scalability of BitNetb1.58 in terms of tokens, we trained a BitNetb1.58 model with 2T to-
kensfollowingthedatarecipeofStableLM-3B[TBMR],whichisthestate-of-the-artopen-source
3Bmodel. BothmodelswereevaluatedonabenchmarkthatconsistsofWinogrande[SBBC20],
PIQA[BZB+19],SciQ[WLG17],LAMBADA[PKL+16],andARC-easy[YBS19]. Wereported
thezero-shotaccuracyinTable4. Fortasksmeasuredwithaccuracyandnormalizedaccuracy,we
taketheaverageofthetwo. TheresultsofStableLM3bat2Ttokensaretakendirectlyfromits
technicalreport. OurfindingsshowsthatBitNetb1.58achievesasuperiorperformanceonallend
tasks,indicatingthat1.58-bitLLMsalsohavestronggeneralizationcapabilities.
4 DiscussionandFutureWork
1-bitMixture-of-Experts(MoE)LLMs
Mixture-of-Experts(MoE)haveproventobeacost-effectiveapproachforLLMs. Whileitsignifi-
cantlyreducesthecomputationFLOPs,thehighmemoryconsumptionandinter-chipcommunication
overheadlimititsdeploymentandapplication. Thesechallengescanbeaddressedby1.58-bitLLMs.
Firstly,thereducedmemoryfootprintreducesthenumberofdevicesrequiredtodeployMoEmodels.
Moreover,itsignificantlyreducestheoverheadoftransferringactivationsacrossnetworks.Ultimately,
therewouldbenooverheadiftheentiremodelscouldbeplacedonasinglechip.
5
)Jp(
tsoC
ygrenE
mn7
)J(
ygrenENativeSupportofLongSequenceinLLMs
IntheeraofLLMs,theabilitytohandlelongsequencehasbecomeacriticaldemand. Onemajor
challengeforlongsequenceinferenceisthememoryconsumptionintroducedbytheKVcaches.
BitNetb1.58representsasignificantsteptowardsnativesupportforlongsequences,asitreducesthe
activationsfrom16bitsto8bits,allowingthecontextlengthtobedoubledgiventhesameresources.
Thiscanbefurtherlosslesslycompressedto4bitsorevenlowerfor1.58-bitLLMs,whichweleave
asfuturework.
LLMsonEdgeandMobile
Theuseof1.58-bitLLMshasthepotentialtogreatlyimprovetheperformanceoflanguagemodels
onedgeandmobiledevices. Thesedevicesareoftenlimitedbytheirmemoryandcomputational
power,whichcanrestricttheperformanceandthescaleofLLMs. However,thereducedmemoryand
energyconsumptionof1.58-bitLLMsallowsthemtobedeployedonthesedevices,enablingawide
rangeofapplicationsthatwerepreviouslynotpossible. Thiscangreatlyenhancethecapabilities
ofedgeandmobiledevicesandenablenewandexcitingapplicationsofLLMs. Moreover,1.58-bit
LLMsaremorefriendlytoCPUdevices,whicharethemainprocessorsusedinedgeandmobile
devices. ThismeansthatBitNetb1.58canbeefficientlyexecutedonthesedevices,furtherimproving
theirperformanceandcapabilities.
NewHardwarefor1-bitLLMs
RecentworklikeGroq5hasdemonstratedpromisingresultsandgreatpotentialforbuildingspecific
hardware(e.g.,LPUs)forLLMs. Goingonestepfurther,weenvisionandcallforactionstodesign
newhardwareandsystemspecificallyoptimizedfor1-bitLLMs,giventhenewcomputationparadigm
enabledinBitNet[WMD+23].
References
[BZB+19] YonatanBisk,RowanZellers,RonanLeBras,JianfengGao,andYejinChoi. PIQA:
reasoningaboutphysicalcommonsenseinnaturallanguage. CoRR,abs/1911.11641,
2019.
[CCKS23] Jerry Chee, Yaohui Cai, Volodymyr Kuleshov, and Christopher De Sa. QuIP: 2-bit
quantizationoflargelanguagemodelswithguarantees. CoRR,abs/2307.13304,2023.
[CLC+19] ChristopherClark,KentonLee,Ming-WeiChang,TomKwiatkowski,MichaelCollins,
andKristinaToutanova. Boolq: Exploringthesurprisingdifficultyofnaturalyes/no
questions. CoRR,abs/1905.10044,2019.
[Com23] TogetherComputer. Redpajama: anopendatasetfortraininglargelanguagemodels,
2023.
[FAHA23] EliasFrantar, SalehAshkboos, TorstenHoefler, andDanAlistarh. OPTQ:accurate
quantization for generative pre-trained transformers. In The Eleventh International
ConferenceonLearningRepresentations,2023.
[HCB+19] YanpingHuang,YoulongCheng,AnkurBapna,OrhanFirat,DehaoChen,MiaXuChen,
HyoukJoongLee,JiquanNgiam,QuocV.Le,YonghuiWu,andZhifengChen. Gpipe:
Efficienttrainingofgiantneuralnetworksusingpipelineparallelism. InAdvancesin
NeuralInformationProcessingSystems,pages103–112,2019.
[Hor14] MarkHorowitz.1.1computing’senergyproblem(andwhatwecandoaboutit).In2014
IEEEInternationalConferenceonSolid-StateCircuitsConference,ISSCC2014,Digest
ofTechnicalPapers,SanFrancisco,CA,USA,February9-13,2014,pages10–14,2014.
[KLZ+23] WoosukKwon,ZhuohanLi,SiyuanZhuang,YingSheng,LianminZheng,CodyHao
Yu,JosephE.Gonzalez,HaoZhang,andIonStoica. Efficientmemorymanagementfor
largelanguagemodelservingwithpagedattention. InProceedingsoftheACMSIGOPS
29thSymposiumonOperatingSystemsPrinciples,2023.
5https://groq.com/
6[LTT+23] JiLin,JiamingTang,HaotianTang,ShangYang,XingyuDang,andSongHan. AWQ:
activation-awareweightquantizationforLLMcompressionandacceleration. CoRR,
abs/2306.00978,2023.
[MCKS18] Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of
armorconductelectricity? Anewdatasetforopenbookquestionanswering. CoRR,
abs/1809.02789,2018.
[MXBS16] StephenMerity,CaimingXiong,JamesBradbury,andRichardSocher. Pointersentinel
mixturemodels,2016.
[PKL+16] DenisPaperno,GermánKruszewski,AngelikiLazaridou,QuanNgocPham,Raffaella
Bernardi,SandroPezzelle,MarcoBaroni,GemmaBoleda,andRaquelFernández. The
LAMBADAdataset: Wordpredictionrequiringabroaddiscoursecontext. InProceed-
ingsofthe54thAnnualMeetingoftheAssociationforComputationalLinguistics,ACL
2016,August7-12,2016,Berlin,Germany,Volume1: LongPapers.TheAssociation
forComputerLinguistics,2016.
[RSR+19] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,Michael
Matena,YanqiZhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearning
withaunifiedtext-to-texttransformer. CoRR,abs/1910.10683,2019.
[SAL+24] JianlinSu,MurtadhaH.M.Ahmed,YuLu,ShengfengPan,WenBo,andYunfengLiu.
Roformer: Enhancedtransformerwithrotarypositionembedding. Neurocomputing,
568:127063,2024.
[SBBC20] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Wino-
Grande: anadversarialwinogradschemachallengeatscale. InTheThirty-FourthAAAI
ConferenceonArtificialIntelligence,pages8732–8740,2020.
[Sha20] NoamShazeer. GLUvariantsimprovetransformer. CoRR,abs/2002.05202,2020.
[TBMR] JonathanTow,MarcoBellagente,DakotaMahan,andCarlosRiquelme. Stablelm3b
4e1t.
[TCS+24] Albert Tseng, Jerry Chee, Qingyao Sun, Volodymyr Kuleshov, and Christopher De
Sa. Quip#: Even better LLM quantization with hadamard incoherence and lattice
codebooks. CoRR,abs/2402.04396,2024.
[TLI+23] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,
TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,Aurelien
Rodriguez,ArmandJoulin,EdouardGrave,andGuillaumeLample. LLaMA:openand
efficientfoundationlanguagemodels. CoRR,abs/2302.13971,2023.
[TMS+23] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,Yasmine
Babaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,Dan
Bikel,LukasBlecher,CristianCantonFerrer,MoyaChen,GuillemCucurull,David
Esiobu,JudeFernandes,JeremyFu,andetal. Llama2: openfoundationandfine-tuned
chatmodels. CoRR,abs/2307.09288,2023.
[WLG17] Johannes Welbl, Nelson F. Liu, and Matt Gardner. Crowdsourcing multiple choice
sciencequestions. InLeonDerczynski,WeiXu,AlanRitter,andTimBaldwin,editors,
Proceedingsofthe3rdWorkshoponNoisyUser-generatedText,NUT@EMNLP2017,
Copenhagen,Denmark,September7,2017,pages94–106.AssociationforComputa-
tionalLinguistics,2017.
[WMC+23] LeiWang,LingxiaoMa,ShijieCao,NingxinZheng,QuanluZhang,JilongXue,Ziming
Miao,TingCao,,andYuqingYang.Ladder:Efficienttensorcompilationoncustomized
dataformat. InOSDI,2023.
[WMD+23] HongyuWang,ShumingMa,LiDong,ShaohanHuang,HuaijieWang,LingxiaoMa,
FanYang,RuipingWang,YiWu,andFuruWei. Bitnet: Scaling1-bittransformersfor
largelanguagemodels. CoRR,abs/2310.11453,2023.
7[XLS+23] GuangxuanXiao,JiLin,MickaëlSeznec,HaoWu,JulienDemouth,andSongHan.
SmoothQuant: accurate and efficient post-training quantization for large language
models. InInternationalConferenceonMachineLearning,ICML2023,23-29July
2023,Honolulu,Hawaii,USA,2023.
[YBS19] VikasYadav,StevenBethard,andMihaiSurdeanu. Quickand(notso)dirty: Unsuper-
visedselectionofjustificationsentencesformulti-hopquestionanswering. InKentaro
Inui,JingJiang,VincentNg,andXiaojunWan,editors,EMNLP-IJCNLP,2019.
[ZHB+19] RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. HellaSwag:
canamachinereallyfinishyoursentence? InProceedingsofthe57thConferenceof
theAssociationforComputationalLinguistics,pages4791–4800,2019.
[ZS19] BiaoZhangandRicoSennrich. Rootmeansquarelayernormalization. InHannaM.
Wallach,HugoLarochelle,AlinaBeygelzimer,Florenced’Alché-Buc,EmilyB.Fox,
andRomanGarnett,editors,AdvancesinNeuralInformationProcessingSystems,pages
12360–12371,2019.
[ZZL22] YichiZhang,ZhiruZhang,andLukaszLew. PokeBNN:Abinarypursuitoflightweight
accuracy.InIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
12465–12475.IEEE,2022.
8