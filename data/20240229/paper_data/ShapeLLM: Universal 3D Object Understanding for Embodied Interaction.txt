ShapeLLM: Universal 3D Object Understanding for Embodied Interaction
ZekunQi1¶ RunpeiDong1¶𭟋 ShaochenZhang1 HaoranGeng2 ChunruiHan3 ZhengGe3
LiYi4† KaishengMa4†
¶InternshipatMEGVII 𭟋Projectlead †Correspondingauthors
1 Xi’anJiaotongUniversity 2 PekingUniversity 3 MEGVIITechnology 4 TsinghuaUniversity
https://qizekun.github.io/ShapeLLM
point cloud input point cloud input point cloud
input
original mesh
(onlyforreference)
original mesh
(onlyforreference)
original mesh
I can't find my file, where is it (onlyforreference) mostlikelytobeinthismodel? User
Whatmighthavejusthappened? User
Whatkindofanimalisthis Thisstoragefurniturehasadrawer,andfiles
Theegghasjustbeencracked, like?Andwhatcolorisit? User a thr ee dlik rae wly et ro tobe vii en wth ite ,d ar na dw te hr e.Y bo ou unn de ined gt bo oxpu ol fl
revealingitscontents. the drawer is [[-0.14, -0.44, 0.75], [-1.05, -
Thefigureresemblesabullora 0.44, 0.75],[-1.05, 0.45, 0.75], [-0.14, 0.45,
As an embodied AI assistant, buffalo.Itsprimarycolorisblue. 0.75],[-0.14,-0.44,0.42],[-1.05,-0.44,0.42],
please give me steps to cook User [-1.05,0.45,0.42],[-0.14,0.45,0.42]].
thisobject.
What kind of sports might
hebedoing? User
1.Crackaneggintoabowl.
2. Add salt, pepper, and any
desiredingredients. Heappearstobedoingasport
3. Scramble them together with a thatinvolvesjumpingorleaping,
spatulauntiltheyarewellcombined. possiblybasketballorvolleyball.
(a)Planning &TaskDecomposition (b)VisionQuestionAnswering (c)EmbodiedVisual Grounding
point cloud point cloud input PB_T50_RS OBJ_ONLY ReCon-2
input
PointGPT
original mesh
(onlyforreference) original mesh ReCon
(onlyforreference) MN-40 OBJ_BG
Canyoutellmewhatisit? Generate a brief caption I2P-MAE
[[x1,y1,z1],...,[x8,y8,z8]] User forthis3Dmodel. User
ACT
Itappearstobeatoiletlid.Itcan
be used to cover the bowl when A playful 3D model of a cat,
notinusetopreventodorsandto rendered in a soft pastel 10w-10s Linear PointMAE
keeptheareaclean. colorpalette. Few-shoton MN-40 SVMon MN40
(d)Precise Referring Dialogue (e)3DCaptioning (f)RepresentationLearning
single-view Pleasegivemeseveralstepstoweigh Is the sofa on the carpet? User
point cloud 2thingsandsortthembytheweight. User
input
Yes, the sofa is on the carpet.
1. Make sure the scale is set to zero. 2. Place
the first item on the scale. 3. Record the How many doors are there
weight of the first item. 4. Place the second beneath the sink cabinet? User
item on the scale. 5. Record the weight of the
original pointcloud second item. 6. Compare the weights of the 2
(onlyforreference) items. 7. Sort the items based on their weights. There are two doors
beneath the sink cabinet.
(g)Single-ViewPointCloudUnderstanding (h)SceneUnderstanding
Figure1.DemonstrationsofSHAPELLM.WepresentSHAPELLM,amulti-modallargelanguagemodeldesignedforembodiedscenes.
SHAPELLMcangenerateaccurateandpromptresponsestouserqueriesbyleveragingcomprehensivepriorknowledgeofthelanguage.
ThisfiguredemonstratesthepowerfulcapabilitiesofSHAPELLMinplanning&taskdecomposition,3Dvisionquestionansweringand
embodiedvisiongrounding,etc.Thepredictedboundingboxisshownintheredboxfromthreedistinctperspectives.
1
4202
beF
72
]VC.sc[
1v66771.2042:viXraAbstract i. 3DPointCloudsasInputs Someconcurrentworks[54]
recentlyproposetousepointcloud-renderedimages[186]
This paper presents SHAPELLM, the first 3D Multi- asmultimodalLLMs’inputsanddemonstrateeffective-
modalLargeLanguageModel(LLM)designedforembodied ness. However,theseworksfailtoachieveaccurate3Dge-
interaction, exploring a universal 3D object understand- ometryunderstandingandoftensufferfromawell-known
ing with 3D point clouds and languages. SHAPELLM is visualhallucinationissue[87,138,196]. Comparedto2D
built upon an improved 3D encoder by extending RE- images,3Dpointcloudsprovideamoreaccuraterepresen-
CON[129]toRECON++thatbenefitsfrommulti-viewim- tationofthephysicalenvironment,encapsulatingsparse
agedistillationforenhancedgeometryunderstanding. By yethighlyprecisegeometricdata[1,36,127]. Moreover,
utilizing RECON++ as 3D point cloud input encoder for 3Dpointcloudsarecrucialinfacilitatingembodiedinter-
LLMs, SHAPELLM is trained on constructed instruction- actions necessitating accurate 3D structures like 6-DoF
following data and tested on our newly human-curated objectposeestimation[84,154,156,160,167].
evaluation benchmark, 3D MM-Vet. RECON++ and ii. Selective Multi-View Distillation Interacting with ob-
SHAPELLMachievestate-of-the-artperformancein3Dge- jects typically necessitates an intricate 3D understand-
ometryunderstandingandlanguage–unified3Dinteraction ing that involves knowledge at various levels and gran-
tasks,suchasembodiedvisualgrounding. ularities. For instance, a whole-part high-level seman-
tic understanding is needed for interactions like open-
ing a large cabinet, while detailed, high-resolution (i.e.,
1.Introduction low-level) semantics are crucial for smaller objects like
manipulatingadrawerhandle[177]. However, existing
3Dshapeunderstanding,servingasafundamentalcapability
works mainly distill single-view high-resolution object
formoldingintelligentsystemsinbothdigitalandphysical
features from 2D foundation models [134], providing a
worlds,haswitnessedtremendousprogressingraphics,vi-
complementary understanding [36, 129, 169]. The po-
sion,augmentedreality,andembodiedrobotics. However,
tentialofmulti-viewimages,whichofferabundantmulti-
tobeeffectivelydeployedbyreal-worldagents,severalcrit-
level features due to view variation and geometry con-
ical criteria must be fulfilled: (i) Sufficient 3D geometry
sistency [9, 58, 63, 79, 101, 143], is often neglected.
information needs to be captured for accurate spatial and
SHAPELLMextendsRECON[129]toRECON++ asthe
structureprocessing[10,13,79,126]. (ii)Modelsshouldbe
3Dencoderbyintegratingmulti-viewdistillation. Toen-
endowed with a foundational knowledge of the embodied
able the model to selectively distill views that enhance
interactionfashionwithobjects—oftenphysically—for
optimizationandgeneralization,inspiredbyCarionetal.,
functional comprehension [52, 65–67, 80, 125, 192, 193].
RECON++ isoptimizedthroughadaptiveselectivematch-
(iii)Auniversalinterfaceisrequiredasabridgebetween
ingusingtheHungarianalgorithm[82].
informationencodinganddecoding,whichcouldhelptrans-
iii. 3D Visual Instruction Tuning Instruction tuning has
latehigh-orderinstructionsforagentreactionslikedialogue
beenproveneffectiveinimprovingLLMs’alignmentca-
responseandembodiedfeedback[27,71,194].
pability[117,121]. Torealizevarious3Dunderstanding
Recent advancements in Large Language Models
taskswithauniversallanguageinterface,SHAPELLMis
(LLMs) [11, 116, 132, 133, 150] have demonstrated un-
trained through instruction-following tuning on con-
precedentedsuccessoffoundationalknowledgeandunified
structed language-output data. However, similar to 2D
reasoningcapabilitiesacrosstasks[7,20,28,37,39,70,73,
visualinstructiontuning[4,93],thedata-dessertissue[36]
78,124]. Itmakesitpossibletoutilizelanguageasauniver-
isevenworsesincenoobject-levelVQAdataisavailable,
salinterfacethatenablesthecomprehensivecommonsense
unlike2D[92]. TovalidatetheefficacyofSHAPELLM,
knowledge embedded in LLMs to enhance understanding
wefirstconstruct∼45Kinstruction-followingdatausing
of 3D shapes. This is particularly evident in physically-
theadvancedGPT-4V(ision)[115]ontheprocessedObja-
groundedtasks,wherethewealthofcommonsenseknowl-
versedataset[29]and30Kembodiedpartunderstanding
edgesimplifiestheinterpretationofanobject’sfunctionality,
datafromGAPartNet[47]forsupervisedfine-tuning. Fol-
mobility,anddynamics,etc. However,theaforementioned
lowingMM-Vet[179],wefurtherdevelopanovelevalua-
challengesremainwhenincorporatingLLMsfor3Dobject
tionbenchmarknamed3DMM-Vet. Thisbenchmarkis
understanding—especiallyembodiedinteractionthatrelies
designedtoassessthecorevision-languagecapabilities,
onprecisegeometry—currentlyunder-explored.
includingembodiedinteractionina3Dcontext,thereby
The question is: What makes better 3D representa-
stimulatingfutureresearch. The3DMM-Vetbenchmark
tionsthatbridgelanguagemodelsandinteraction-oriented
comprises59diverseInternet13Dobjectsand232human-
3D object understanding? In this work, we introduce
writtenquestion-answerpairs.
SHAPELLM that meets the requirements, which is estab-
lishedbasedonthefollowingthreedesigningpolicies: 1URL&License.
2Multi-View A yellow chicken Task Decomposition 3DVisual Grounding There is a [[x1,y1,z1],
Images plays basketball EmbodiedInteraction, 3D Captioning line fixed ······
wearing overalls. Scene Comprehension 3DVQA handle at [x8,y8,z8]].
······ ······
MAE 2D Image Text
Decoder Encoder Encoder
Multimodal LLMs
Reconstruction Hungarian Contrastive Learnable
Loss Algorithm Loss Prompts projector projector projector
Local Cross Global Absolute RECON++ Tokenizer
Attention
3DPoint Cloud 3DPoint Cloud Position
Encoder Decoder Encoding
stop-grad
(optional)
3 CD loP uo din st mask Fu /ll N/ oS ii sn yg il ne p-V ui tew
Global Queries
Emb Io nd stie rud c/ tiG oP nT-4V
Object/Scene
PointClouds
(a)RECON++ Pipeline (b) MLLM Design
Figure2.OverviewofourSHAPELLMframework.(a)TheintroducedRECON++pipelineincorporatestherequired3Dencoder.(b)The
comprehensivedesignoftheMLLM,featuringaninstruction-modetokenizerandtheintegrationofanalignedmulti-modalrepresentation,
equipstheMLLMwiththecapabilitytoeffectivelyhandle3Dvisionlanguagetasks.
Throughextensiveexperimentation,wefirstdemonstrate 2.1.OverallArchitecture
thatourimproved3DencoderRECON++setsanewstate-
The main objective of this work is interactive 3D under-
of-the-art representation transferring on both downstream
standingbyusingtheLLMasauniversalinterface. Draw-
fine-tunedandzero-shot3Dobjectrecognition. Specifically,
ing inspiration from recent work in visual understanding
RECON++ has obtained 95.25% and 95.0% fine-tuned
[93], the proposed SHAPELLM consists a pre-trained 3D
accuracy on ScanObjectNN and ModelNet40, surpassing
encoderandanLLMforeffective3Drepresentationlearn-
previousbestrecordsby+1.85%onthemostchallenging
ingandunderstanding,respectively. Specifically,weadopt
ScanObjectNN.Besides, RECON++ achieved53.7%and
LLaMA [150] as our LLM, building upon the success of
65.4%zero-shotaccuracyonObjaverse-LVISandScanOb-
previous work [24, 37, 93]. As for the 3D encoder, we
jectNN,whichis+0.6%and+1.6%higherthanprevious
proposeanovel3DmodelnamedRECON++basedonthe
best. ByutilizingourRECON++asSHAPELLM’s3Den-
recentworkRECON[129]withmultipleimprovementsas
coder,SHAPELLMsuccessfullyunifiesvariousdownstream
the3Dunderstandinggenerallydemandsmoreinformation,
tasks, including 3D captioning, 3D VQA, embodied task
suchasaccuratespatialandmulti-viewdetails,etc. Toen-
planning&decomposition,3Dembodiedvisualgrounding,
surecompatibilitywiththeLLMinputs,therepresentation
and3Dprecisereferringdialogue(SeeFig.1).Onournewly
constructed3DMM-Vetbenchmark,42.7%and49.3%To-
of a 3D object obtained from RECON++ undergoes a lin-
ear projection before being fed into the LLM. To further
tal accuracy have been achieved by SHAPELLM-7B and
improvelow-levelgeometryunderstanding,whichbenefits
SHAPELLM-13B, surpassing previous best records [166]
taskslike6-DoFposeestimation,weappendtheabsolute
that also uses 3D point clouds by +2.1% and +5.1%, re-
positionencoding(APE)obtainedbylinearprojectionof3D
spectively. Thisworkinitiatesafirststeptowardsleveraging
coordinates. Besides, weuseprefix-tuningwithlearnable
LLMs for embodied object interaction, and we hope our
prompts[36,37,76,86]toadaptivelymodulatethedifferent
SHAPELLM andproposed3DMM-Vetbenchmarkcould
semanticsofAPEandRECON++representations.
spurmorerelatedfutureresearch.
2.2. How to alleviate interactive 3D understanding
2. SHAPELLM DataDessert?
Inthissection,wefirstintroducetheoverallarchitectureof Mostpublished3Ddataistypicallypresentedas3Dobject-
SHAPELLM. Then, we delve into two critical challenges captionpairs,lackinganinteractivestyle. Althoughafew
facedininteractive3Dunderstanding: datadessert[36]and concurrentworks[62,166]haveattemptedtoconstructinter-
representationof3Dpointclouds. Wepresentthedetailed active3Dunderstandingdatasets,thequestions-and-answers
designofourmethodtotacklethesechallenges,respectively. (Q&As)areprimarilybasedonannotatedcaptions,oftenpro-
3[ImgaeUpload] [PointClouds]
[SystemPrompt]
[Question1 Type:Knowledge]
You are a3D Computer visual assistant.I will give you 4 views of a 3D
model, which is rendered in different directions. What is this woman’s occupation?
Design a conversation between you and a person asking about this 3D [Answer1]
model. The answers should be in a tone that a visual Al assistant is seeing She is a medical worker.
the 3Dmodel and answering the question. Ask diverse questions and give
corresponding answers.You need to consider the spatial relation in different [Question2 Type:General Visual Recognition]
views. What clothes are the women wearing and what colors are they?
You can ask somecomplexquestions based on these 6aspects: Object [Answer2]
Detailed Description, General Visual Recognition, Knowledge,
She is wearing protective clothing and two medical gloves. the
Language Generation, Spatial Relationship, and Embodied Interaction.
protective clothing is blue while the medical gloves are green.
I will give you several examples, and please generate questions and answer
pairs with the format of my examples.For example: [Question3 Type:EmbodiedInteraction]
Q: <ExampleQ1>A: <ExampleA1>Q: <ExampleQ2>A: <ExampleA2> As an AI assistant, please give me some steps to put her in all the
protective equipment.
Provide detailed answers when answering complex questions. You can give
detailed examples or reasoning steps to make the content more convincing [Answer3]
and well-organized. You can include multiple paragraphs if necessary.Do Step 1: Put her in the protective clothing and tie the knot.
not ask about uncertain details.
Step 2: Put her hair up and cover it with the hat.
Please give me 6 correct and detailed question-answer pairs basedonthe6
Step 3: Open the gloves and put them on her.
aspects. You should try to avoid asking “Q: Is there xxx? A: Yes, xxx.”.
Figure3. Constructionillustrationofinstruct-followingdata Figure4.3DMM-vetdatasetsample.Awealthofpreciseevalua-
usingGPT-4V[115].FourperspectiveviewsareinputintoGPT- tionmetricsenableacomprehensiveassessment.
4V.In-contextpromptsfocusingondifferenttopicsareexplicitly
EmbodiedObjectUnderstanding Acomprehensiveun-
incorporatedtoensuredatadiversity.
derstanding of the spatial positions and semantics at the
vidingalimitedperspectivewithoutsufficientdetails. Addi- part level is crucial to facilitate effective object grasping
tionally,thoseworkshavegenerallybeenlimitedtosemantic and interaction in embodied scenarios. Fortunately, the
understandingwithoutconsideringembodiedinteraction. To GAPartNet [47] provides rich part annotations, including
addresstheselimitations,ourworkconstructsquestion-and- semanticsandposes,whichareinstrumentalinconstructing
answer pairs based on multi-view images of a 3D object instruction-tuningdataforembodiedinteractivepartsofa
usingGPT-4V(ision)[115]. Fordatadiversity,weexplicitly subject. Specifically,givena3Dobject,questionsarefor-
introducesixaspectsasprompts,asillustratedFig.3. Inthe mulated based on the semantics of its different parts, and
following,weprovidethedetailsaboutdatacollectionand answersareconstructedinboththesemanticsand3Dposi-
constructionregardinggeneralsemanticunderstandingand tions. Thepositionsarerepresentedas6-DoF3Dbounding
embodiedobjectunderstanding,respectively. boxes in a straightened Python multidimensional list for-
Data Objaverse-LVIS [29, 107] and GAPartNet [47] are mat, denoted as [[x1, y1, z1], [x2, y2, z2],
datasources. Objaverse-LVIScovers1,156LVIS[55]cate- ..., [x8, y8, z8]], to meet characteristics of the
gories,andwesampleTop-10“likes”23Dobjectspercate-
textualdialoguesresponseinLLMs. Thecanonicalspace
goryandgenerateQ&Apairspersample. Afterfilteringout oftheobjectdeterminesthesequenceofcoordinates. Using
noisy Q&As, we obtain ∼45K instruction-following sam- boundingboxcoordinatesleveragestheinherentspatialrela-
ples. Weuse12categoriesfromGAPartNetbyremoving tionship,allowingLLMstoreadilylearnthesepatternsand
“Remote”toavoidtoomanytinyboxes,whichleadstofil- generateaccurateoutputcoordinates. Thisapproachcanof-
tered∼30KQ&Asamplesconstructedfrom∼8Kpartsof ferspecificpositioninformationforembodiedmanipulation.
the∼4Kobjectsstatescovering∼1.1Kdifferentobjects.
GeneralSemanticUnderstanding Thisaimstoenhance 2.3. RECON++: Scaling Up 3D Representation
the model’s generalization abilities in visual recognition, Learning
knowledgeintegration,spatialunderstanding,andotheras-
Interaction with objects such as object grasping [95, 154,
pects.WepromptGPT4-VtogenerateQ&Asinsixdifferent
167]typicallyrequiresaccurateperceptionof3Dshapein-
aspectsbasedonimagescapturedfromfourdifferentviews
formationatmulti-levelandmulti-granularity. Thisimposes
ofa3Dsubject,asillustratedinFig.3.
heightenedrequirementson3Drepresentations,callingfora
2“Likes”statisticscanbefoundatSketchfab. higherstandardofaholisticunderstandingof3Dgeometry.
4point cloud input point cloud input (onlyo fr oi rgi rn ea fel rm ene cs eh ) (onlyo fr oi rgi rn ea fel rm ene cs eh )
original mesh
(onlyforreference) point cloud input
point cloud
original mesh input
(onlyforreference)
Whatisthis?Whichcountry HowcanIopenthedrawer? User I ts hat the isre dea sn iy gnp ea drt foin rit nh ti es ram co tid oe nl ? User
W ush ea dt tc oa dn ot ?hisobjectbe
User
doesitcomefrom? User Y haou ndlc ea .n Amo op ne gn thth ee gid vr ea nw oe br jeb cy t,tu hs ein reg isth ae
Yes!Lookforthehingedoorinthestorage
This object can be used to
T dih si hs .is Its iu ss mhi a, da et fr ra od miti vo inn ea gl aJ ra ep dan rie cs ee
,
l [i [n -0e .6fi 9xe ,d -0h .5an 4d ,l 0e .1a 7n ]d ,i [t -s 0.b 6o 9u ,n -d 0i .n 5g 4b ,o 0x .3is
2],
f bu or un nit du inre g, bop xrecisely located within the
playmusic,asitappearstobe oftenservedwithrawfishorother [-0.69,0.54,0.32],[-0.69,0.54,0.17], [[-0.90,-0.30,-0.46],[-0.90,-0.30,0.39],
astringedinstrument. ingredients,wrappedinseaweedor [-0.64,-0.54,0.17],[-0.64,-0.54,0.32], [-0.40,-0.65,0.39],[-0.40,-0.65,-0.46],
wrappedinpaper. [-0.64,0.54,0.32],[-0.64,0.54,0.17]]. [-0.88,-0.28,-0.46],[-0.88,-0.28,0.39],
Whatkindofmaterialmightbe Grabandpullthelinefixedhandleinthe [-0.38,-0.63,0.39],[-0.38,-0.63,-0.46]].
usedtoproducethisobject? User AsanEmbodiedAIassistant,please directionthatallowsittoturnormove. It'samovableelementyoucaninteractwith.
givemestepstofeedmenoodles. User
Theobjectappearstobemade
of wood, which is commonly 1. Pick up the noodles with your
usedforstringedinstruments chopsticks.
2.Dipthenoodlesintothesoysauce.
3.Placethenoodlesintoyourmouth,
ensuringtochewthemthoroughly.
Figure5.Selectedmultimodaldialogueexamples. SHAPELLMpossessesrobustcapabilitiesinknowledgerepresentation,reasoning,and
instruction-followingdialogue.WithitspowerfulpointcloudencoderRECON++,SHAPELLMcanevenmakeaccuratepredictionsabout
minuteinteractivecomponents,e.g.,handle.Therenderedmeshimagesaresolelyforvisualreferencehereanddonotconstituteinputdata.
However,existing3Dcross-modalrepresentationlearn- thepermutationσ. Inpractice,weemploycosinesimilarity
ingmethods[94,170]mainlydistillhigh-resolutionobject asthematchingcost. Inthisfashion,thequeryofeachview
featuresfromsingle-view2Dfoundationmodels,resulting is learned to gather accurate 3D shape information from
inaunilateralshapeunderstanding. Besides,theygenerally the 3D point clouds. Concatenating the features from the
employmulti-viewimagesasadataaugmentationstrategy, local 3D point cloud encoder and global 3D point cloud
imposingthelearnedrepresentationtotheaveragerepresen- decoder together provides comprehensive information for
tationofallviews. Thus,theaccurate3Dshapeinformation 3DunderstandingofmultimodalLLMs.
ismissing. Recently,RECON[129]utilizescontrastguided
byreconstructiontoaddressthepatterndisparitiesbetween 3.3DMM-Vet: 3DMultimodalComprehension
localmaskeddatamodelingandglobalcross-modalalign- EvaluationBenchmark
ment. This results in remarkable performance in various
tasks, including transfer learning, zero-shot classification, Awiderangeofdiversevisual-languagecapabilitiesisessen-
andpartsegmentation. However,itspotentialishinderedby tialtodevelopamultimodallargelanguagemodeltailored
thescarcityofpretrainingdata[13]. for embodied scenarios, particularly addressing task and
Toaddresstheabovelimitations,thispaperproposesRE- actionplanning.
CON++withmultipleimprovements. First,multi-viewim- Themodel’sproficiencyinprocessingpointcloudsen-
agequerytokenscollaborativelycomprehendthesemantic ables it to perform general recognition tasks effortlessly,
information of 3D objects across different views, encom- demonstratingabroadunderstandingofcoloredpointclouds.
passing both RGB images and depth maps. Considering Thiscapabilityservesasthegroundworkformoreintricate
thedisorderlinessofpretrainingdataintermsofpose,we tasks. Beyond3Drecognition,theLLMshouldexhibitcom-
proposeacross-modalalignmentmethodbasedonbipartite petenceinaddressingtasksinreal-worldembodiedscenarios.
matching,whichimplicitlylearnstheposeestimationof3D Thisentailsunifyingtheaforementionedabilitiestogener-
objects. Second,wescaleuptheparametersofRECONand atedecomposedtaskactionsstep-by-stepinaninstruction-
broadenthescaleofthepretrainingdataset[18,29,107]for followingfashion,addressingspecificproblems.
robust3Drepresentations. Hence, toformulateanevaluationsystemalignedwith
DenoteN asthenumberofmulti-viewimages,I isthe theaforementionedtaskdescription, weestablishamulti-
i
imagefeaturefromi-thview,andQ representstheglobal levelevaluationtasksystemencompassingfour-leveltasks:
i
queryofi-thview. FollowingCarionetal.,wesearchforan GeneralRecognition,KnowledgeandLanguageGenera-
optimalpermutationσofN elementswiththelowestcost: tion,SpatialAwareness,andEmbodiedInteraction. This
frameworksystematicallyandcomprehensivelyassessesthe
N
(cid:88) model’sproficiencyininformationcomprehensionandlan-
σˆ =argmin L (I ,Q ), (1)
match i σ(i) guagegenerationwhenprocessinginteractiveobjects. The
σ
i
detaileddescriptionsofthetasksarelistedasfollows:
whereL (I ,Q )isapair-wisematchingcostbetween i. GeneralRecognition: FollowingMM-Vet[179],weas-
match i σ(i)
i-thviewimagefeaturesI andmatchedqueryQ with sess the fundamental comprehension abilities of LLMs
i σ(i)
5Table1.Fine-tuned3DrecognitiononScanObjectNNandMod- Table2.Zero-shot3DrecognitiononObjaverse-LVIS[29],Mod-
elNet40.BG,ON,andRSareshortforOBJ BG,OBJ ONLY,and elNet40[164]andScanObjectNN[151].Ensembled:pretraining
PB T50 RS,respectively.Overallaccuracy(%)withvoting[98]is withfourdatasets,Objaverse[29],ShapeNet[13],ABO[22]and
reported.†:resultswithapost-pretrainingstage[18]. 3D-FUTURE[42],followingOpenShape[94].†:Uni3Demploys
anEVA-CLIP-E[146]teacherthatislargerandimprovesperfor-
ScanObjectNN ModelNet40
Method mance,whileothermethodsemployOpenCLIP-bigG[74].
BG ON RS 1kP 8kP
Objaverse-LVIS ModelNet40 ScanObjectNN
SupervisedLearningOnly Method
Top1 Top3 Top5 Top1 Top3 Top5 Top1 Top3 Top5
PointNet[126] 73.3 79.2 68.0 89.2 90.8
2DInference
PointNet++[127] 82.3 84.3 77.9 90.7 91.9
DGCNN[157] 82.8 86.2 78.1 92.9 - PointCLIP[186] 1.9 4.1 5.8 19.3 28.6 34.8 10.5 20.8 30.6
PointMLP[108] - - 85.4 94.5 - PointCLIP2[198] 4.7 9.5 12.9 63.6 77.9 85.0 42.2 63.3 74.5
PointNeXt[131] - - 87.7 94.0 - TrainedonShapeNet
Transformer[153] 83.04 84.06 79.11 91.4 91.8 RECON[129] 1.1 2.7 3.7 61.2 73.9 78.1 42.3 62.5 75.6
CLIP2Point[69] 2.7 5.8 7.9 49.5 71.3 81.2 25.5 44.6 59.4
withSelf-SupervisedRepresentationLearning
ULIP[169] 6.2 13.6 17.9 60.4 79.0 84.4 51.5 71.1 80.2
Point-BERT[180] 87.43 88.12 83.07 93.2 93.8 OpenShape[94] 10.8 20.2 25.0 70.3 86.9 91.3 47.2 72.4 84.7
Point-MAE[119] 90.02 88.29 85.18 93.8 94.0 TrainedonEnsembled
Point-M2AE[185] 91.22 88.81 86.43 94.0 -
ULIP-2[170] 26.8 44.8 52.6 75.1 88.1 93.2 51.6 72.5 82.3
ACT[36] 93.29 91.91 88.21 93.7 94.0
OpenShape[94] 46.8 69.1 77.0 84.4 96.5 98.0 52.2 79.7 88.7
TAP [158] - - 88.5 94.0 -
Uni3D-B†[195] 51.7 74.1 80.8 86.3 96.5 97.9 63.8 82.7 90.2
VPP[130] 93.11 91.91 89.28 94.1 94.3 Uni3D-L†[195] 53.1 75.0 81.5 86.3 96.8 98.3 58.2 81.8 89.4
I2P-MAE[188] 94.15 91.57 90.11 94.1 - RECON++-B 53.2 75.3 81.5 86.5 94.7 95.8 63.6 80.2 90.6
ULIP-2[170] - - 91.5 - - RECON++-L 53.7 75.8 82.0 87.3 95.4 96.1 65.4 84.1 89.7
RECON[129] 95.35 93.80 91.26 94.5 94.7
PointGPT-B†[18] 95.8 95.2 91.9 94.4 94.6
requesting LLMs to provide execution steps toward an
PointGPT-L†[18] 97.2 96.6 93.4 94.7 94.9
RECON++-B† 98.62 96.21 93.34 94.6 94.8 instruction. Thisapproachaimstoestablishconnections
RECON++-L† 98.80 97.59 95.25 94.8 95.0 forhandlingEmbodiedInteractiontasks[39,70].
Topreventanyoverlapwithtrainingdata,ourcollection
involvingbothcoarse-andfine-grainedaspects. Coarse- of3DmodelsissourcedexclusivelyfromTurbosquid[142],
grainedrecognitionfocusesonbasicobjectattributessuch a platform not included in the acquisition lists of Obja-
ascolor,shape,action,etc. Whilefine-grainedrecognition verse [29] and ShapeNet [13]. We meticulously curated
delvesintodetailslikesubpartsandcounting,etc. adatasetof593Dmodels,generating232Q&Asforeval-
ii. Knowledge Capability & Language Generation: To uationpurposes. Inourpursuitofapreciseassessmentof
examine the models’ capacity to understand and utilize single-task capabilities, each question is designed to test
knowledge,drawinginspirationfromMMBench[100],we onlyonespecificcapacityoutlinedearlier. Everyquestionis
integrateitsreasoningcomponents. Thisincludesknowl- pairedwithacorrespondinganswertailoredtotheparticular
edgespanningnaturalandsocialreasoning,physicalprop- 3D model, serving as the ground truth. More details and
erties,sequentialprediction,math,etc.,evaluatinggauges analysiscanbefoundinAppendixB.
whethermultimodalLLMspossesstherequisiteexpertise
4.Experiments
andcapacitytosolveintricatetasks.Weutilizecustomized
promptstostimulatemodelsandextractdetailedresponses
4.1.3DRepresentationTransferringwithRECON++
toevaluatelanguagegeneration.
iii. SpatialAwareness:In3D,spatialawarenessholdsheight- Fine-tuned 3D Object Recognition In Tab. 1, we first
ened significance compared to 2D due to the provided evaluate the representation transfer learning capabilities
geometryinformation. Thepointcloudscontainlocation of self-supervised RECON++ by fine-tuning on ScanOb-
informationcrucialfordiscerningspatialrelationshipsbe- jectNN[151]andModelNet[164],whicharecurrentlythe
tweendifferentparts. In2D,achievingthesameinforma- two most challenging 3D object datasets. ScanObjectNN
tionintensitylevelwouldnecessitatemulti-viewimages. is a collection of ∼15K 3D object point clouds from the
Therefore,ourevaluationincludesquestionsprobingthe real-worldscenedatasetScanNet[23], whichinvolves15
abilityofLLMstounderstandspatialrelations. categories. ModelNet is one of the most classical 3D ob-
iv. Embodied Interaction: The utilization scope of multi- jectdatasetscollectedfromclean3DCADmodels,which
modalLLMsextendsintothefieldofembodiedinteraction, includes∼12Kmeshed3DCADmodelscovering40cat-
facilitatedbytheutilizationofinstruction-followingdata. egories. Following PointGPT [18], we adopt the inter-
Our evaluation system tests their capacity by formally mediate fine-tuning strategy and use the post-pretraining
6Table3.Zero-shot3Dmultimodalcomprehensionevaluationof Table 4. Zero-shot 3D multimodal comprehension evalua-
coreVLcapabilitiesin3Dcontexton3DMM-Vet.Rec:General tion of robustness on 3D MM-Vet-C. Clean: no corruptions.
VisualRecognition,Know:Knowledge,Gen:LanguageGenera- Single-View: randomlyselectacameraviewpointwithinthe
tion,Spat:SpatialAwareness,Emb:EmbodiedInteraction. unitsphereandgenerateasingleviewpointwithintheFoVonpolar
coordinates.Jitter:Gaussianjitteringwithnoiseϵ∼N(0,σ2)
Method Input Rec Know Gen Spat Emb Total
andσ = 0.01. Rotate: randomSO(3)rotationsamplingover
LLaVA-13B[93] 1-ViewImg. 40.0 55.3 51.3 43.2 51.1 47.9
X-Y-ZEulerangle(α,β,γ)∼U(−θ,θ)andθ=π/6.
DreamLLM-7B[37] 4-ViewImg. 42.2 54.4 50.8 48.9 54.5 50.3
GPT-4V[115] 1-ViewImg. 53.7 59.5 61.1 54.7 59.0 57.4 3DMM-Vet-CVariants
GPT-4V[115] 4-ViewImg. 65.1 69.1 61.4 52.9 65.5 63.4 Method
Clean Single-View Jitter Rotate
PointBind&LLM[54] PointCloud 16.9 13.0 18.5 32.9 40.4 23.5
PointLLM-7B[166] PointCloud 40.6 49.5 34.3 29.1 48.7 41.2 PointBind&LLM[54] 23.5 20.4 19.7 19.5
PointLLM-13B[166] PointCloud 46.6 48.3 38.8 45.2 50.9 46.6 PointLLM-7B[166] 41.2 33.6 38.8 40.6
SHAPELLM-7B PointCloud 45.7 42.7 43.4 39.9 64.5 47.4 PointLLM-13B[166] 46.6 41.3 42.3 44.2
SHAPELLM-13B PointCloud 46.8 53.0 53.9 45.3 68.4 53.1 SHAPELLM-7B 47.4 38.3 45.8 42.7
SHAPELLM-13B 53.1 43.6 47.8 49.3
stagetotransferthegeneralsemanticslearnedthroughself-
• 3D MM-Vet. Tab. 3 shows the detailed results of
supervised pretraining on ShapeNetCore [13]. For a fair
SHAPELLM on different tasks of 3D MM-Vet. It can
comparison,ourBaseandLargemodelsadoptthesamear-
beobservedthat SHAPELLM significantlyoutperforms
chitectureasPointGPTregardinglayers, hiddensize, and
PointLLM [166] across various metrics, particularly in
attentionheads. Tab.1showsthat: (i)RECON++exhibits
EmbodiedTasks. Thissubstantiatesourmodel’sversatile
representationperformancesignificantlysurpassingthatof
capabilityinaddressingreal-worldscenariotasks.
otherbaselines,achievingstate-of-the-artresults. (ii)Partic-
• 3D MM-Vet-C. Tab. 4 shows the comparison of model
ularly,RECON++achievesaremarkableaccuracyof95.25%
robustnessagainst“single-view”,“jitter”and“rotate”cor-
onthemostchallengingScanObjectNNPB T50 RSbench-
ruptions,whicharethemostcommoncorruptionsinreal-
mark,boostingtheTransformerbaselineby+16.14%.
world scenarios. The results demonstrate significantly
Zero-Shot 3D Open-World Recognition Similar to
superior robustness of SHAPELLM against corruption,
CLIP[134],ourmodelalignsthefeaturespaceoflanguages
indicatingstrongerpotentialinreal-worldapplicability.
and other modalities, which results in a zero-shot open-
QualitativeAnalysis Fig.5illustratesqualitativeexamples
world recognition capability. In Tab. 2, we compare the
ofSHAPELLMinmultimodaldialogue. SHAPELLMisca-
zero-shot3Dopen-worldobjectrecognitionmodelstoeval-
pableofsupportinggeneralVQA,embodiedtaskandaction
uate the generalizable recognition capability. Following
planning,aswellas6-DoFposeestimation. Notably,due
OpenShape[94],weevaluateonModelNet[164],ScanOb-
tothestrictspatialrelationshipinherentin6-DoFbounding
jectNN [151], and Objaverse-LVIS [29]. Objaverse-LVIS
boxcoordinates,weobservethatLLMseasilygraspsuch
isabenchmarkinvolving∼47Kclean3Dmodelsof1,156
patternsandconsistentlyproducevalidcoordinates.
LVIS categories [55]. We compare RECON++ with 2D
inferencemethods,ShapeNetpretrainedmethods,and“En-
5.Discussions
sembled”datasets-pretrainedmethods. Itcanbeconcluded
from Tab. 2: i) Compared to 2D inference and ShapeNet- 5.1.Whatislearnedfrommulti-viewalignment?
pretrainedmethods,RECON++demonstratessignificantly
Fig.6illustratesthevisualizationoftheattentionmapinthe
superior performance, showing the necessity of 3D point
lastcross-attentionlayer,documentingtheimagequeryto
clouds as inputs and scaling up. ii) Compared to state-
whicheachlocalpatchintheattentionmapprimarilyattends.
of-the-art methods trained on “Ensembled” datasets, RE-
Itprovidesevidencethatmulti-viewalignmentachievesgeo-
CON++demonstratessuperiororon-parperformanceacross
metricallyinformedspatialunderstanding,whichmayim-
allbenchmarks. Notably,RECON++-Lachievesaremark-
able Top-1 accuracy, which is +0.6% and +7.2% higher
thanUni3D-LonthemostchallengingObjaverse-LVISand
ScanObjectNNbenchmarks,respectively. Query1 Query2
4.2.MultimodalComprehensionwithSHAPELLM Query3 Query4
Quantitative Analysis To assess the comprehensive ca-
Query5 Query6
pabilitiesof SHAPELLM,wefirstquantitativelycompare
variousbaselinesandourmodelontheproposed3DMM- Query7 Query8
VetbenchmarkusingGPT-4. FollowingModelNet-C[136]
and ModelNet40-C [144], we construct 3D MM-Vet-C to Figure6.Visualizationofmulti-viewqueryresults.Thedistinct
benchmarktherobustnessagainst3Dcorruptions. colorsservetodenotedistinctimagequeries.
7Table5.3DreferringexpressiongroundingonGAPartNet.Ac-
curacywithanIoUthresholdof0.25isreported.†:Fine-tunedon original mesh original mesh
GaPartNetimages.‡:Inferencewith3in-contextdemonstrations. (onlyforreference) (onlyforreference)
Method Input Avg
Whereisitshat? Asarobot,howcanyoupick
LLaVA-13B[93] 1-ViewImg. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 User upthisflower? User
LLaVA-13B[93] 4-ViewImg. 0.0 0.0 0.0 0.0 0.0 0.0 0.0 Itshatislocatedat[[-0.4,-0.45,0.86], To pick up the flower, you would need to
LLaVA-13B†[93] 1-ViewImg. 1.8 9.3 3.8 0.0 2.1 11.1 4.4 [ [- 00 .. 04 8, ,0 -0.2 .48 5, ,0 0. .8 86 6] ], ,[ [0 -0.0 .48 ,, -0 0. .2 48 5, ,0 0. .8 46 4] ], , a thp ep r so tea mch a i tt tf hro em p oth ine t f cr lo on st e sa tn td o g the en t rly o bg ora t,s p
LLaVA-13B†[93] 4-ViewImg. 2.5 13.7 7.7 0.0 4.3 11.1 6.2 [[ 0- .0 0. 84 ,, -0 0. .2 48 5, ,0 0. .4 44 4] ], ].[0.08,0.28,0.44], s 0u .4c 6h ] .as [-0.01, 0.02, -0.49] or [-0.01, 0.02, -
GPT-4V[115] 4-ViewImg. 0.0 0.0 0.0 0.0 0.0 0.0 0.0
Figure7.Partunderstandingexamplesofunseenobjects.
GPT-4V‡[115] 4-ViewImg. 0.1 1.6 0.0 0.0 0.0 0.0 0.3
SHAPELLM-7B PointCloud 5.9 25.8 11.5 3.4 5.1 11.1 10.5
tilledfrom2DimagesintoLLMs[62,68,199]. Thesecond
SHAPELLM-13B PointCloud 7.6 26.7 11.5 6.7 6.8 11.1 11.7
kindofinteractiontypicallyrequiresphysicalunderstand-
ing in 3D, such as part understanding [47, 96, 105, 113],
plicitlyencompasstheestimationoftheobjectposeanda
6-DoFposeestimation[84,97,156,160,177],particularly
moreprofoundknowledgeof3Dspatialrelationships.
usefulforhuman-objectinteraction(HOI)androboticma-
5.2.IsSHAPELLMgroundedinphysicalworlds? nipulation[45–48,50,85,95,128,139,154,167,178]and
complexroboticplanning[14,34,39,72,90,141]. Inthis
Tab.5comparesSHAPELLMwithimage-onlymethodson
work,wefocusonbothphysicalandconceptualinteractions
3Dreferringexpressiongrounding(REG)of6-DoFposeson
with3Dshapesforembodiedunderstanding.
GAPartNet. Theresultsshowthat: i)Image-onlymethods
MultimodalLargeLanguageModels Multimodalcom-
cannotperformzero-shotgeometry-necessary6-DoFpose
prehension, which allows human interaction with textual
estimation. ii)Comparedtoimage-onlymethodswith2Dto
andvisualelements,haswitnessedsignificantadvancements,
6-DoFposeestimationfine-tuningorin-contextprompting,
particularlyinextendingLLMslikeLLaMA[21,149,150].
SHAPELLM still performs significantly better. It demon-
Theearlyeffortspredominantlyrevolvedaroundintegrating
strates the necessity of geometry and the difficulty of the
LLMswithvariousdownstreamsystemsbyemployingitas
ill-posed2Dto6-DoFposeestimationproblem.
anagent[6,57,88,140,148,155,161,171,172]. Signifi-
cantsuccesshasbeendemonstratedwithinthisplugin-style
5.3.CanSHAPELLMgeneralizetounseenobjects?
framework. Due to the remarkable capabilities of LLMs,
Fig.7showsthepartunderstandingexamplesofunseenob- aligning the visual semantic space with language through
jects. WhileSHAPELLM’s6-DoFposeestimationistrained parameter-efficient tuning [2, 64, 83, 173, 187, 197] and
on GAPartNet, which primarily consists of indoor articu- instructiontuning[24,37,93,168]hasemergedasthepre-
latedfurniture.Ithasdemonstratedpromisinggeneralization vailingapproachincurrentresearch. Tofurtherenhancein-
potentialofspatialunderstandingontheopen-worldobjects, teractivecapabilities,someapproacheshavebeendeveloped
pavingwaysforscalingupspatial-awarenesstraining. towards visual-interactive multimodal comprehension by
preciselyreferringtoinstructiontuning[19,123,189,191].
6.RelatedWorks Another family advances the developments of LLMs en-
dowedwithcontentcreationbeyondcomprehension,notable
Interaction-oriented3DUnderstanding Interactionwith
effortsincludeDreamLLM[37],GILL[81],Emu[145,147],
3Dobjectstypicallyinvolvesconcept-onlyinteractionand
SEED[44],NeXt-GPT[162],andKosmos-G[118].
physical-groundedinteraction[15]. Theformerworksfo-
cus on 3D perception and semantic parsing, such as 3D 7.Conclusions
objectrecognitionandsceneperception[99,126,127,157,
159]. By utilizing language for open-ended interaction in ThispaperpresentsSHAPELLM,a3DmultimodalLLMfor
3D, a number of works demonstrate successful 3D scene embodiedinteraction,capableofgeneralizablerecognition
QA [109, 174], grounding [16], and captioning [17]. Re- andembodiedinteractioncomprehension. Wefirstproposea
cently, some works propose to utilize foundation models novel3Dpointcloudencoder,RECON++,byutilizingmulti-
like LLMs or CLIP for open-ended 3D object recogni- viewdistillationandscalingup3Drepresentationlearning,
tion[94,186,198]andscenesegmentation[122,181]. Guo whichservesasthefoundation3Drepresentationencoder
& Zhang et al. [54] utilizes ImageBind [49] and LLaMA- for SHAPELLM. Then, we perform 3D visual instruction
Adapter[187]torealizepointcloud-basedinteractiveQA. tuningonconstructedinstruction-followingdataforgeneral
Following LLaVA, PointLLM [166] conducts supervised and embodied comprehension. We also established a 3D
fine-tuning by constructing a visual instruction-following evaluationbenchmark,3DMM-Vet,severingasassessingthe
dataset. Other works focus on scene-level tasks utilizing 4-levelcapacityinembodiedinteractionscenarios,varying
comprehensive 2D features [68, 199] or 3D features dis- frombasicperceptiontocontrolstatementsgeneration.
8References objectsfrommultiple2-dviewsbyaself-organizingneural
architecture. InFromStatisticstoNeuralNetworks: The-
[1] PanosAchlioptas,OlgaDiamanti,IoannisMitliagkas,and
oryandPatternRecognitionApplications,pages349–375.
LeonidasJ.Guibas.Learningrepresentationsandgenerative
Springer,1994. 2
models for 3d point clouds. In Int. Conf. Mach. Learn.
[10] AlexanderM.Bronstein,MichaelM.Bronstein,LeonidasJ.
(ICML),2018. 2
Guibas,andMaksOvsjanikov. Shapegoogle: Geometric
[2] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,Antoine
wordsandexpressionsforinvariantshaperetrieval. ACM
Miech,IainBarr,YanaHasson,KarelLenc,ArthurMen-
Trans.Graph.,30(1):1:1–1:20,2011. 2
sch,KatieMillican,MalcolmReynolds,RomanRing,Eliza
[11] TomB.Brown,BenjaminMann,NickRyder,MelanieSub-
Rutherford,SerkanCabi,TengdaHan,ZhitaoGong,Sina
biah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-
Samangooei,MarianneMonteiro,JacobMenick,Sebastian
tan, PranavShyam, GirishSastry, AmandaAskell, Sand-
Borgeaud,AndrewBrock,AidaNematzadeh,SahandShar-
hiniAgarwal,ArielHerbert-Voss,GretchenKrueger,Tom
ifzadeh,MikolajBinkowski,RicardoBarreira,OriolVinyals,
Henighan,RewonChild,AdityaRamesh,DanielM.Ziegler,
AndrewZisserman,andKarenSimonyan. Flamingo:avi-
JeffreyWu,ClemensWinter,ChristopherHesse,MarkChen,
suallanguagemodelforfew-shotlearning. InAdv.Neural
EricSigler,MateuszLitwin,ScottGray,BenjaminChess,
Inform.Process.Syst.(NeurIPS),2022. 8
JackClark,ChristopherBerner,SamMcCandlish,AlecRad-
[3] IroArmeni,OzanSener,AmirRZamir,HelenJiang,Ioannis
ford,IlyaSutskever,andDarioAmodei. Languagemodels
Brilakis,MartinFischer,andSilvioSavarese. 3dsemantic
arefew-shotlearners. InAdv.NeuralInform.Process.Syst.
parsingoflarge-scaleindoorspaces. InIEEE/CVFConf.
(NeurIPS),2020. 2
Comput.Vis.PatternRecog.(CVPR),2016. 22
[12] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas
[4] YutongBai, XinyangGeng, KarttikeyaMangalam, Amir
Usunier,AlexanderKirillov,andSergeyZagoruyko. End-
Bar, Alan L. Yuille, Trevor Darrell, Jitendra Malik, and
to-end object detection with transformers. In Eur. Conf.
AlexeiA.Efros.Sequentialmodelingenablesscalablelearn-
Comput.Vis.(ECCV),2020. 2,5
ingforlargevisionmodels. CoRR,abs/2312.00785,2023.
[13] Angel X. Chang, Thomas A. Funkhouser, Leonidas J.
2
Guibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio
[5] SatanjeevBanerjeeandAlonLavie. METEOR:anauto-
Savarese,ManolisSavva,ShuranSong,HaoSu,Jianxiong
maticmetricforMTevaluationwithimprovedcorrelation
Xiao,LiYi,andFisherYu. Shapenet:Aninformation-rich
withhumanjudgments. InProceedingsoftheWorkshopon
3dmodelrepository. CoRR,abs/1512.03012,2015. 2,5,6,
IntrinsicandExtrinsicEvaluationMeasuresforMachine
7,21
Translationand/orSummarization@ACL2005,AnnArbor,
[14] MatthewChang,The´ophileGervet,MukulKhanna,Sriram
Michigan,USA,June29,2005,2005. 18
Yenamandra,DhruvShah,SoYeonMin,KavitShah,Chris
[6] JamesBetker,GohGabriel,LiJing,TimBrooks,Jianfeng
Paxton,SaurabhGupta,DhruvBatra,RoozbehMottaghi,
Wang,LinjieLi,LongOuyang,JuntangZhuang,JoyceLee,
JitendraMalik,andDevendraSinghChaplot. GOAT:GOto
YufeiGuo,WesamManassra,PrafullaDhariwal,CaseyChu,
anything. CoRR,abs/2311.06430,2023. 8
YunxinJiao,andAdityaRamesh. Improvingimagegenera-
[15] BoyuanChen,ZhuoXu,SeanKirmani,BrianIchter,Danny
tionwithbettercaptions. 2023. 8
Driess,PeteFlorence,DorsaSadigh,LeonidasGuibas,and
[7] RishiBommasani,DrewA.Hudson,EhsanAdeli,RussAlt-
Fei Xia. Spatialvlm: Endowing vision-language models
man,SimranArora,SydneyvonArx,MichaelS.Bernstein,
withspatialreasoningcapabilities. CoRR,abs/2401.12168,
JeannetteBohg,AntoineBosselut,EmmaBrunskill,Erik
2023. 8
Brynjolfsson,ShyamalBuch,DallasCard,RodrigoCastel-
[16] DaveZhenyuChen,AngelX.Chang,andMatthiasNießner.
lon, Niladri S. Chatterji, Annie S. Chen, Kathleen Creel,
Scanrefer: 3d object localization in RGB-D scans using
Jared Quincy Davis, Dorottya Demszky, Chris Donahue,
naturallanguage. InEur.Conf.Comput.Vis.(ECCV),2020.
MoussaDoumbouya,EsinDurmus,StefanoErmon,John
8
Etchemendy,KawinEthayarajh,LiFei-Fei,ChelseaFinn,
TrevorGale,LaurenGillespie,KaranGoel,NoahD.Good- [17] DaveZhenyuChen, AliGholami, MatthiasNießner, and
man,ShelbyGrossman,NeelGuha,TatsunoriHashimoto, AngelX.Chang.Scan2cap:Context-awaredensecaptioning
PeterHenderson,JohnHewitt,DanielE.Ho,JennyHong, inRGB-Dscans. InIEEE/CVFConf.Comput.Vis.Pattern
Kyle Hsu, Jing Huang, Thomas Icard, Saahil Jain, Dan Recog.(CVPR),2021. 8
Jurafsky,PratyushaKalluri,SiddharthKaramcheti,Geoff [18] GuangyanChen,MeilingWang,YiYang,KaiYu,LiYuan,
Keeling, Fereshte Khani, Omar Khattab, Pang Wei Koh, andYufengYue. Pointgpt:Auto-regressivelygenerativepre-
MarkS.Krass,RanjayKrishna,RohithKuditipudi,andetal. trainingfrompointclouds. InAdv.NeuralInform.Process.
Ontheopportunitiesandrisksoffoundationmodels. CoRR, Syst.(NeurIPS),2023. 5,6,20
abs/2108.07258,2021. 2 [19] KeqinChen,ZhaoZhang,WeiliZeng,RichongZhang,Feng
[8] BernhardE.Boser,IsabelleGuyon,andVladimirVapnik. Zhu,andRuiZhao. Shikra: Unleashingmultimodalllm’s
A training algorithm for optimal margin classifiers. In referentialdialoguemagic. CoRR,abs/2306.15195,2023. 8
ACMConf.Comput.Learn.Theory(COLT),pages144–152. [20] XiChen,JosipDjolonga,PiotrPadlewski,BasilMustafa,
ACM,1992. 20 SoravitChangpinyo,JialinWu,CarlosRiquelmeRuiz,Se-
[9] GaryBradskiandStephenGrossberg. Recognitionof3-d bastian Goodman, Xiao Wang, Yi Tay, Siamak Shakeri,
9Mostafa Dehghani, Daniel Salz, Mario Lucic, Michael [30] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou,
Tschannen,ArshaNagrani,HexiangHu,MandarJoshi,Bo YanyongZhang,andHouqiangLi. Voxelr-cnn: Towards
Pang, CesleeMontgomery, PaulinaPietrzyk, MarvinRit- highperformancevoxel-based3dobjectdetection. InAAAI
ter, A. J. Piergiovanni, Matthias Minderer, Filip Pavetic, Conf.Artif.Intell.(AAAI),2021. 22
Austin Waters, Gang Li, Ibrahim Alabdulmohsin, Lucas [31] JacobDevlin,Ming-WeiChang,KentonLee,andKristina
Beyer,JulienAmelot,KentonLee,AndreasPeterSteiner, Toutanova. BERT:pre-trainingofdeepbidirectionaltrans-
YangLi, DanielKeysers, AnuragArnab, YuanzhongXu, formersforlanguageunderstanding. InProceedingsofthe
KeranRong,AlexanderKolesnikov,MojtabaSeyedhosseini, 2019 Conference of the North American Chapter of the
AneliaAngelova,XiaohuaZhai,NeilHoulsby,andRadu Association for Computational Linguistics: Human Lan-
Soricut. Pali-x: On scaling up a multilingual vision and guageTechnologies,NAACL-HLT2019,Minneapolis,MN,
language model. In Int. Conf. Learn. Represent. (ICLR), USA,June2-7,2019,Volume1(LongandShortPapers),
2023. 2 pages4171–4186.AssociationforComputationalLinguis-
[21] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,Zhanghao tics,2019. 22
Wu,HaoZhang,LianminZheng,SiyuanZhuang,Yonghao [32] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang,
Zhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Song Bai, and Xiaojuan Qi. Lowis3d: Language-driven
Vicuna:Anopen-sourcechatbotimpressinggpt-4with90%* open-worldinstance-level3dsceneunderstanding. CoRR,
chatgptquality,2023. 8,21 abs/2308.00353,2023. 22
[22] Jasmine Collins, Shubham Goel, Kenan Deng, Achlesh-
[33] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang,
warLuthra,LeonXu,ErhanGundogdu,XiZhang,Tomas
SongBai,andXiaojuanQi. PLA:language-drivenopen-
FYagoVicente,ThomasDideriksen,HimanshuArora,etal.
vocabulary 3d scene understanding. In IEEE/CVF Conf.
Abo:Datasetandbenchmarksforreal-world3dobjectun-
Comput.Vis.PatternRecog.(CVPR),2023. 22
derstanding.InIEEE/CVFConf.Comput.Vis.PatternRecog.
[34] YanDing,XiaohanZhang,ChrisPaxton,andShiqiZhang.
(CVPR),2022. 6,21
Taskandmotionplanningwithlargelanguagemodelsfor
[23] AngelaDai,AngelXChang,ManolisSavva,MaciejHalber,
objectrearrangement. CoRR,abs/2303.06247,2023. 8
ThomasFunkhouser,andMatthiasNießner.Scannet:Richly-
[35] RunpeiDong,ZhanhongTan,MengdiWu,LinfengZhang,
annotated3dreconstructionsofindoorscenes.InIEEE/CVF
and Kaisheng Ma. Finding the task-optimal low-bit sub-
Conf.Comput.Vis.PatternRecog.(CVPR),2017. 6,22
distributionindeepneuralnetworks. InInt.Conf.Mach.
[24] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuat
Learn.(ICML),2022. 23
Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale
[36] RunpeiDong,ZekunQi,LinfengZhang,JunboZhang,Jian-
Fung,andStevenC.H.Hoi. Instructblip:Towardsgeneral-
jianSun,ZhengGe,LiYi,andKaishengMa. Autoencoders
purposevision-languagemodelswithinstructiontuning. In
ascross-modalteachers:Canpretrained2dimagetransform-
Adv.NeuralInform.Process.Syst.(NeurIPS),2023. 3,8,19
ershelp3drepresentationlearning? InInt.Conf.Learn.
[25] WenliangDai, ZihanLiu, Ziwei Ji, DanSu, andPascale
Represent.(ICLR),2023. 2,3,6,17,20,22
Fung. Plausiblemaynotbefaithful:Probingobjecthallu-
cinationinvision-languagepre-training. InProceedingsof [37] RunpeiDong,ChunruiHan,YuangPeng,ZekunQi,Zheng
the17thConferenceoftheEuropeanChapteroftheAssocia- Ge,JinrongYang,LiangZhao,JianjianSun,HongyuZhou,
tionforComputationalLinguistics,EACL2023,Dubrovnik, HaoranWei,XiangwenKong,XiangyuZhang,Kaisheng
Croatia,May2-6,2023,2023. 19 Ma,andLiYi.DreamLLM:Synergisticmultimodalcompre-
hensionandcreation.InInt.Conf.Learn.Represent.(ICLR),
[26] TriDao,DanFu,StefanoErmon,AtriRudra,andChristo-
2024. 2,3,7,8,17,21
pherRe´. Flashattention: Fastandmemory-efficientexact
attentionwithio-awareness. InAdv.NeuralInform.Process. [38] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,
Syst.(NeurIPS),2022. 22 Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
[27] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
DeshrajYadav,StefanLee,Jose´M.F.Moura,DeviParikh, vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis
andDhruvBatra. Visualdialog. IEEETrans.PatternAnal. worth16x16words:Transformersforimagerecognitionat
Mach.Intell.(TPAMI),41(5):1242–1256,2019. 2 scale. InInt.Conf.Learn.Represent.(ICLR),2021. 21
[28] Joe Davison, Joshua Feldman, and Alexander M. Rush. [39] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey
Commonsenseknowledgeminingfrompretrainedmodels. Lynch,AakankshaChowdhery,BrianIchter,AyzaanWahid,
InProceedingsofthe2019ConferenceonEmpiricalMeth- Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong
odsinNaturalLanguageProcessingandthe9thInterna- Huang, YevgenChebotar, PierreSermanet, DanielDuck-
tionalJointConferenceonNaturalLanguageProcessing, worth,SergeyLevine,VincentVanhoucke,KarolHausman,
EMNLP-IJCNLP2019,HongKong,China,November3-7, MarcToussaint,KlausGreff,AndyZeng,IgorMordatch,
2019,2019. 2 andPeteFlorence. Palm-e:Anembodiedmultimodallan-
[29] MattDeitke,DustinSchwenk,JordiSalvador,LucaWeihs, guagemodel. InInt.Conf.Mach.Learn.(ICML),2023. 2,
Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana 6,8
Ehsani,AniruddhaKembhavi,andAliFarhadi. Objaverse: [40] Nico Engel, Vasileios Belagiannis, and Klaus Dietmayer.
A universe of annotated 3d objects. In IEEE/CVF Conf. Pointtransformer. IEEEAccess,9:134826–134840,2021.
Comput.Vis.PatternRecog.(CVPR),2023. 2,4,5,6,7,21 22
10[41] HaoqiangFan,HaoSu,andLeonidasJ.Guibas. Apoint [54] ZiyuGuo,RenruiZhang,XiangyangZhu,YiwenTang,Xi-
setgenerationnetworkfor3dobjectreconstructionfrom anzhengMa,JiamingHan,KexinChen,PengGao,Xianzhi
asingleimage. InIEEE/CVFConf.Comput.Vis.Pattern Li, Hongsheng Li, and Pheng-Ann Heng. Point-bind &
Recog.(CVPR),2017. 22 point-llm:Aligningpointcloudwithmulti-modalityfor3d
[42] HuanFu,RongfeiJia,LinGao,MingmingGong,Binqiang understanding,generation,andinstructionfollowing. CoRR,
Zhao,SteveMaybank,andDachengTao. 3d-future:3dfur- abs/2309.00615,2023. 2,7,8,19
nitureshapewithtexture.InternationalJournalofComputer [55] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: A
Vision,129:3313–3337,2021. 6,21 dataset for large vocabulary instance segmentation. In
[43] Tianyu Gao, Xingcheng Yao, and Danqi Chen. Simcse: IEEE/CVFConf.Comput.Vis.PatternRecog.(CVPR),2019.
Simple contrastive learning of sentence embeddings. In 4,7
Proceedingsofthe2021ConferenceonEmpiricalMethods [56] SaurabhGupta,RossB.Girshick,PabloAndre´sArbela´ez,
in Natural Language Processing, EMNLP 2021, Virtual andJitendraMalik. LearningrichfeaturesfromRGB-D
Event/PuntaCana,DominicanRepublic,7-11November, imagesforobjectdetectionandsegmentation. InEur.Conf.
2021,2021. 18 Comput.Vis.(ECCV),2014. 18
[44] YuyingGe,YixiaoGe,ZiyunZeng,XintaoWang,andYing [57] TanmayGuptaandAniruddhaKembhavi. Visualprogram-
Shan. PlantingaSEEDofvisioninlargelanguagemodel. ming:Compositionalvisualreasoningwithouttraining. In
InInt.Conf.Learn.Represent.(ICLR),2024. 8 IEEE/CVFConf.Comput.Vis.PatternRecog.(CVPR),2023.
[45] Haoran Geng, Ziming Li, Yiran Geng, Jiayi Chen, Hao 8
Dong,andHeWang. Partmanip: Learningcross-category [58] Abdullah Hamdi, Silvio Giancola, and Bernard Ghanem.
generalizable part manipulation policy from point cloud MVTN: multi-view transformation network for 3d shape
observations. In IEEE/CVF Conf. Comput. Vis. Pattern recognition. InInt.Conf.Comput.Vis.(ICCV),pages1–11.
Recog.(CVPR),2023. 8 IEEE,2021. 2,22
[46] HaoranGeng,SonglinWei,CongyueDeng,BokuiShen,He [59] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-
Wang,andLeonidasGuibas. Sage:Bridgingsemanticand Kirkpatrick,andGrahamNeubig. Towardsaunifiedview
actionablepartsforgeneralizablearticulated-objectmanipu- ofparameter-efficienttransferlearning. InInt.Conf.Learn.
lationunderlanguageinstructions. CoRR,abs/2312.01307, Represent.(ICLR),2021. 18
2023. [60] KaimingHe,XinleiChen,SainingXie,YanghaoLi,Piotr
[47] HaoranGeng,HelinXu,ChengyangZhao,ChaoXu,LiYi, Dolla´r, and Ross B. Girshick. Masked autoencoders are
SiyuanHuang,andHeWang. Gapartnet: Cross-category scalablevisionlearners. InIEEE/CVFConf.Comput.Vis.
domain-generalizableobjectperceptionandmanipulation PatternRecog.(CVPR),2022. 22
viageneralizableandactionableparts. InIEEE/CVFConf. [61] DanHendrycksandKevinGimpel. Gaussianerrorlinear
Comput.Vis.PatternRecog.(CVPR),2023. 2,4,8,23 units(gelus). CoRR,abs/1606.08415,2016. 21
[48] Yiran Geng, Boshi An, Haoran Geng, Yuanpei Chen, [62] YiningHong,HaoyuZhen,PeihaoChen,ShuhongZheng,
YaodongYang, andHaoDong. Rlafford: End-to-endaf- YilunDu,ZhenfangChen,andChuangGan. 3d-llm:Inject-
fordance learning for robotic manipulation. In IEEE Int. ingthe3dworldintolargelanguagemodels. InAdv.Neural
Conf.Robot.Autom.(ICRA),2023. 8 Inform.Process.Syst.(NeurIPS),2023. 3,8,19
[49] RohitGirdhar,AlaaeldinEl-Nouby,ZhuangLiu,Mannat [63] JiHou,SainingXie,BenjaminGraham,AngelaDai,and
Singh,KalyanVasudevAlwala,ArmandJoulin,andIshan MatthiasNießner. Pri3d: Can3dpriorshelp2drepresen-
Misra. Imagebind:Oneembeddingspacetobindthemall. tationlearning? InInt.Conf.Comput.Vis.(ICCV),pages
InProceedingsoftheIEEE/CVFConferenceonComputer 5673–5682.IEEE,2021. 2
VisionandPatternRecognition,pages15180–15190,2023. [64] EdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-
8 Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen.
[50] RanGong,JiangyongHuang,YizhouZhao,HaoranGeng, Lora:Low-rankadaptationoflargelanguagemodels. InInt.
Xiaofeng Gao, Qingyang Wu, Wensi Ai, Ziheng Zhou, Conf.Learn.Represent.(ICLR),2022. 8
DemetriTerzopoulos,Song-ChunZhu,BaoxiongJia,and [65] RuizhenHu,ChenyangZhu,OlivervanKaick,LigangLiu,
Siyuan Huang. ARNOLD: A benchmark for language- ArielShamir,andHaoZhang. Interactioncontext(ICON):
groundedtasklearningwithcontinuousstatesinrealistic3d towardsageometricfunctionalitydescriptor. ACMTrans.
scenes. CoRR,abs/2304.04321,2023. 8 Graph.,34(4):83:1–83:12,2015. 2
[51] PriyaGoyal, DhruvMahajan, AbhinavGupta, andIshan [66] RuizhenHu,OlivervanKaick,BojianWu,HuiHuang,Ariel
Misra. Scaling and benchmarking self-supervised visual Shamir,andHaoZhang. Learninghowobjectsfunctionvia
representationlearning. InInt.Conf.Comput.Vis.(ICCV), co-analysisofinteractions. ACMTrans.Graph.,35(4):47:1–
pages6390–6399.IEEE,2019. 20 47:13,2016.
[52] HelmutGrabner,JuergenGall,andLucVanGool. What [67] RuizhenHu,WenchaoLi,OlivervanKaick,ArielShamir,
makesachairachair? InIEEE/CVFConf.Comput.Vis. HaoZhang,andHuiHuang. Learningtopredictpartmobil-
PatternRecog.(CVPR),2011. 2 ityfromasinglestaticsnapshot. ACMTrans.Graph.,36(6):
[53] AnishaGunjal,JihanYin,andErhanBas. Detectingand 227:1–227:13,2017. 2
preventinghallucinationsinlargevisionlanguagemodels. [68] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun
CoRR,abs/2308.06394,2023. 19 Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu,
11BaoxiongJia,andSiyuanHuang. Anembodiedgeneralist [79] TakeoKanadeandMasatoshiOkutomi. Astereomatching
agentin3dworld. CoRR,abs/2311.12871,2023. 8 algorithmwithanadaptivewindow:Theoryandexperiment.
[69] Tianyu Huang, Bowen Dong, Yunhan Yang, Xiaoshui IEEE Trans. Pattern Anal. Mach. Intell., 16(9):920–932,
Huang,RynsonW.H.Lau,WanliOuyang,andWangmeng 1994. 2
Zuo. Clip2point: TransferCLIPtopointcloudclassifica- [80] VladimirG.Kim,SiddharthaChaudhuri,LeonidasJ.Guibas,
tionwithimage-depthpre-training. CoRR,abs/2210.01055, andThomasA.Funkhouser. Shape2pose: human-centric
2022. 6 shapeanalysis. ACMTrans.Graph.,33(4):120:1–120:12,
[70] WenlongHuang,IgorMordatch,andDeepakPathak. One 2014. 2
policytocontrolthemall:Sharedmodularpoliciesforagent- [81] JingYuKoh,DanielFried,andRuslanSalakhutdinov. Gen-
agnosticcontrol. InInt.Conf.Mach.Learn.(ICML),2020. eratingimageswithmultimodallanguagemodels. InAdv.
2,6 NeuralInform.Process.Syst.(NeurIPS),2023. 8
[71] Wenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky [82] HaroldWKuhn. Thehungarianmethodfortheassignment
Liang,PeteFlorence,AndyZeng,JonathanTompson,Igor problem. Navalresearchlogisticsquarterly,2(1-2):83–97,
Mordatch,YevgenChebotar,PierreSermanet,TomasJack- 1955. 2
son,NoahBrown,LindaLuu,SergeyLevine,KarolHaus- [83] JunnanLi,DongxuLi,SilvioSavarese,andStevenC.H.
man,andBrianIchter. Innermonologue:Embodiedreason- Hoi. BLIP-2: bootstrappinglanguage-imagepre-training
ingthroughplanningwithlanguagemodels. InAnnu.Conf. withfrozenimageencodersandlargelanguagemodels. In
Robot.Learn.(CoRL),2022. 2 Int.Conf.Mach.Learn.(ICML),2023. 8
[72] WenlongHuang,ChenWang,RuohanZhang,YunzhuLi, [84] XiaolongLi,HeWang,LiYi,LeonidasJ.Guibas,A.Lynn
JiajunWu,andLiFei-Fei. Voxposer:Composable3dvalue Abbott,andShuranSong. Category-levelarticulatedobject
mapsforroboticmanipulationwithlanguagemodels. In poseestimation. InIEEE/CVFConf.Comput.Vis.Pattern
Annu.Conf.Robot.Learn.(CoRL),2023. 8,23 Recog.(CVPR),2020. 2,8
[73] BrianIchter,AnthonyBrohan,YevgenChebotar,Chelsea [85] XiaoqiLi,MingxuZhang,YiranGeng,HaoranGeng,Yux-
Finn,KarolHausman,AlexanderHerzog,DanielHo,Ju- ingLong,YanShen,RenruiZhang,JiamingLiu,andHao
lian Ibarz, Alex Irpan, Eric Jang, Ryan Julian, Dmitry Dong. Manipllm: Embodied multimodal large language
Kalashnikov,SergeyLevine,YaoLu,CarolinaParada,Kan- modelforobject-centricroboticmanipulation,2023. 8
ishkaRao,PierreSermanet,AlexanderToshev,VincentVan- [86] XiangLisaLiandPercyLiang. Prefix-tuning:Optimizing
houcke,FeiXia,TedXiao,PengXu,MengyuanYan,Noah continuouspromptsforgeneration. InProceedingsofthe
Brown,MichaelAhn,OmarCortes,NicolasSievers,Clay- 59thAnnualMeetingoftheAssociationforComputational
tonTan,SichunXu,DiegoReyes,JarekRettinghouse,Jor- Linguisticsandthe11thInternationalJointConferenceon
nellQuiambao,PeterPastor,LindaLuu,Kuang-HueiLee, Natural Language Processing (Volume 1: Long Papers),
YuhengKuang,SallyJesmonth,NikhilJ.Joshi,KyleJeffrey, 2021. 3,17
RosarioJaureguiRuano,JasmineHsu,KeerthanaGopalakr- [87] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXin
ishnan,ByronDavid,AndyZeng,andChuyuanKellyFu. Zhao,andJi-RongWen. Evaluatingobjecthallucinationin
DoasIcan,notasIsay: Groundinglanguageinrobotic largevision-languagemodels.CoRR,abs/2305.10355,2023.
affordances. InAnnu.Conf.Robot.Learn.(CoRL),2022. 2 2
[74] GabrielIlharco,MitchellWortsman,RossWightman,Cade [88] YaoboLiang,ChenfeiWu,TingSong,WenshanWu,Yan
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Xia,YuLiu,YangOu,ShuaiLu,LeiJi,ShaoguangMao,
VaishaalShankar,HongseokNamkoong,JohnMiller,Han- Yun Wang, Linjun Shou, Ming Gong, and Nan Duan.
nanehHajishirzi,AliFarhadi,andLudwigSchmidt. Open- Taskmatrix.ai:Completingtasksbyconnectingfoundation
clip,2021. 6 modelswithmillionsofapis. CoRR,abs/2303.16434,2023.
[75] Ajay Jaiswal, Zhe Gan, Xianzhi Du, Bowen Zhang, 8
ZhangyangWang,andYinfeiYang. Compressingllms:The [89] Chin-Yew Lin. Rouge: A package for automatic evalua-
truthisrarelypureandneversimple.CoRR,abs/2310.01382, tionofsummaries. InProc.WorkshoponTextSummaria-
2023. 23 tionBranchesOut,Post-ConferenceWorkshopofACL2004,
[76] MenglinJia,LumingTang,Bor-ChunChen,ClaireCardie, 2004. 18
SergeJ.Belongie,BharathHariharan,andSer-NamLim. [90] Kevin Lin, Christopher Agia, Toki Migimatsu, Marco
Visualprompttuning. InEur.Conf.Comput.Vis.(ECCV), Pavone,andJeannetteBohg.Text2motion:Fromnaturallan-
2022. 3,18 guageinstructionstofeasibleplans. CoRR,abs/2303.12153,
[77] YunfanJiang,AgrimGupta,ZichenZhang,GuanzhiWang, 2023. 8
YongqiangDou,YanjunChen,LiFei-Fei,AnimaAnandku- [91] FuxiaoLiu,KevinLin,LinjieLi,JianfengWang,YaserYa-
mar,YukeZhu,andLinxiFan. VIMA:generalrobotma- coob,andLijuanWang. Aligninglargemulti-modalmodel
nipulationwithmultimodalprompts. InAnnu.Conf.Robot. withrobustinstructiontuning.CoRR,abs/2306.14565,2023.
Learn.(CoRL),2023. 23 19
[78] ZhengbaoJiang,FrankF.Xu,JunAraki,andGrahamNeu- [92] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee.
big. Howcanweknowwhatlanguagemodelsknow. Trans. Improvedbaselineswithvisualinstructiontuning. CoRR,
Assoc.Comput.Linguistics,8:423–438,2020. 2 abs/2310.03744,2023. 2
12[93] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. [108] XuMa,CanQin,HaoxuanYou,HaoxiRan,andYunFu.Re-
Visualinstructiontuning. InAdv.NeuralInform.Process. thinkingnetworkdesignandlocalgeometryinpointcloud:
Syst.(NeurIPS),2023. 2,3,7,8,19,21 A simple residual MLP framework. In Int. Conf. Learn.
[94] Minghua Liu, Ruoxi Shi, Kaiming Kuang, Yinhao Zhu, Represent.(ICLR).OpenReview.net,2022. 6
Xuanlin Li, Shizhong Han, Hong Cai, Fatih Porikli, and [109] XiaojianMa,SilongYong,ZilongZheng,QingLi,Yitao
HaoSu. Openshape: Scalingup3dshaperepresentation Liang,Song-ChunZhu,andSiyuanHuang. SQA3D:situ-
towardsopen-worldunderstanding. InAdv.NeuralInform. atedquestionansweringin3dscenes. InInt.Conf.Learn.
Process.Syst.(NeurIPS),2023. 5,6,7,8,21 Represent.(ICLR),2023. 8
[95] Xueyi Liu and Li Yi. GeneOH diffusion: Towards gen- [110] HaleyMacLeod,CynthiaL.Bennett,MeredithRingelMor-
eralizablehand-objectinteractiondenoisingviadenoising ris,andEdwardCutrell. Understandingblindpeople’sex-
diffusion. InInt.Conf.Learn.Represent.(ICLR),2024. 4,8 perienceswithcomputer-generatedcaptionsofsocialmedia
images. In Proceedings of the 2017 CHI Conference on
[96] Xueyi Liu, Bin Wang, He Wang, and Li Yi. Few-shot
Human Factors in Computing Systems, page 5988–5999,
physically-awarearticulatedmeshgenerationviahierarchi-
New York, NY, USA, 2017. Association for Computing
caldeformation. InInt.Conf.Comput.Vis.(ICCV),2023.
Machinery. 19
8
[111] JiagengMao,YujingXue,MinzheNiu,HaoyueBai,Jiashi
[97] XueyiLiu,JiZhang,RuizhenHu,HaibinHuang,HeWang,
Feng,XiaodanLiang,HangXu,andChunjingXu. Voxel
andLiYi. Self-supervisedcategory-levelarticulatedobject
transformerfor3dobjectdetection. InInt.Conf.Comput.
poseestimationwithpart-levelSE(3)equivariance. InInt.
Vis.(ICCV),2021. 22
Conf.Learn.Represent.(ICLR),2023. 8
[112] DanielMaturanaandSebastianA.Scherer. Voxnet: A3d
[98] YongchengLiu,BinFan,ShimingXiang,andChunhong
convolutionalneuralnetworkforreal-timeobjectrecogni-
Pan. Relation-shapeconvolutionalneuralnetworkforpoint
tion. InIEEE/RSJInt.Conf.Intell.Robot.andSyst.(IROS),
cloudanalysis. InIEEE/CVFConf.Comput.Vis.Pattern
pages922–928.IEEE,2015. 22
Recog.(CVPR),2019. 6
[113] KaichunMo,ShilinZhu,AngelXChang,LiYi,Subarna
[99] YunzeLiu,JunyuChen,ZekaiZhang,JingweiHuang,and
Tripathi,LeonidasJGuibas,andHaoSu. Partnet:Alarge-
LiYi. Leaf: Learningframesfor4dpointcloudsequence
scalebenchmarkforfine-grainedandhierarchicalpart-level
understanding. InInt.Conf.Comput.Vis.(ICCV),2023. 8
3dobjectunderstanding. InIEEE/CVFConf.Comput.Vis.
[100] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,Songyang
PatternRecog.(CVPR),2019. 8
Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui
[114] OpenAI. Introducingchatgpt. 2022. 21
He,ZiweiLiu,KaiChen,andDahuaLin. Mmbench: Is
[115] OpenAI. Gpt-4v(ision)systemcard,2023. 2,4,7,8
your multi-modal model an all-around player? CoRR,
[116] OpenAI. GPT-4technicalreport. CoRR,abs/2303.08774,
abs/2307.06281,2023. 6
2023. 2,19,21
[101] YuanLiu,ChengLin,ZijiaoZeng,XiaoxiaoLong,Lingjie
[117] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,Car-
Liu,TakuKomura,andWenpingWang. Syncdreamer:Gen-
rollL.Wainwright,PamelaMishkin,ChongZhang,Sand-
eratingmultiview-consistentimagesfromasingle-viewim-
hiniAgarwal,KatarinaSlama,AlexRay,JohnSchulman,
age. CoRR,abs/2309.03453,2023. 2
JacobHilton,FraserKelton,LukeMiller,MaddieSimens,
[102] Ze Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong.
Amanda Askell, Peter Welinder, Paul F. Christiano, Jan
Group-free 3d object detection via transformers. In Int.
Leike,andRyanLowe. Traininglanguagemodelstofollow
Conf.Comput.Vis.(ICCV),2021. 22
instructionswithhumanfeedback. InAdv.NeuralInform.
[103] IlyaLoshchilovandFrankHutter. SGDR:stochasticgradi- Process.Syst.(NeurIPS),2022. 2
entdescentwithwarmrestarts. InInt.Conf.Learn.Repre-
[118] Xichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,
sent.(ICLR),2017. 22
Wenhu Chen, and Furu Wei. Kosmos-g: Generating im-
[104] IlyaLoshchilovandFrankHutter. Decoupledweightdecay agesincontextwithmultimodallargelanguagemodels. In
regularization. InInt.Conf.Learn.Represent.(ICLR),2019. Int.Conf.Learn.Represent.(ICLR),2024. 8
22 [119] YatianPang,WenxiaoWang,FrancisE.H.Tay,WeiLiu,
[105] CewuLu,HaoSu,YongluLi,YongyiLu,LiYi,Chi-Keung Yonghong Tian, and Li Yuan. Masked autoencoders for
Tang,andLeonidasJ.Guibas. Beyondholisticobjectrecog- pointcloudself-supervisedlearning. InEur.Conf.Comput.
nition:Enrichingimageunderstandingwithpartstates. In Vis.(ECCV),2022. 6,20,22
IEEE/CVFConf.Comput.Vis.PatternRecog.(CVPR),2018. [120] KishorePapineni,SalimRoukos,ToddWard,andWei-Jing
8 Zhu. Bleu:amethodforautomaticevaluationofmachine
[106] ShiyangLu,HaonanChang,EricPuJing,AbdeslamBoular- translation. 2002. 18
ias,andKostasE.Bekris. OVIR-3D:open-vocabulary3d [121] BaolinPeng,ChunyuanLi,PengchengHe,MichelGalley,
instanceretrievalwithouttrainingon3ddata.InAnnu.Conf. andJianfengGao. InstructiontuningwithGPT-4. CoRR,
Robot.Learn.(CoRL),2023. 22 abs/2304.03277,2023. 2
[107] TiangeLuo,ChrisRockwell,HonglakLee,andJustinJohn- [122] SongyouPeng,KyleGenova,Chiyu”Max”Jiang,Andrea
son. Scalable3dcaptioningwithpretrainedmodels. InAdv. Tagliasacchi,MarcPollefeys,andThomasA.Funkhouser.
NeuralInform.Process.Syst.(NeurIPS),2023. 4,5 Openscene:3dsceneunderstandingwithopenvocabularies.
13InIEEE/CVFConf.Comput.Vis.PatternRecog.(CVPR), [135] NilsReimersandIrynaGurevych. Sentence-bert:Sentence
2023. 8,22 embeddingsusingsiamesebert-networks. InProceedings
[123] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shaohan ofthe2019ConferenceonEmpiricalMethodsinNatural
Huang,ShumingMa,andFuruWei. Kosmos-2:Grounding LanguageProcessingandthe9thInternationalJointCon-
multimodal large language models to the world. CoRR, ferenceonNaturalLanguageProcessing,EMNLP-IJCNLP
abs/2306.14824,2023. 8 2019,HongKong,China,November3-7,2019,2019. 18
[124] FabioPetroni,TimRockta¨schel,SebastianRiedel,Patrick [136] JiaweiRen,LiangPan,andZiweiLiu. Benchmarkingand
S.H.Lewis,AntonBakhtin,YuxiangWu,andAlexanderH. analyzingpointcloudclassificationundercorruptions. In
Miller. Language models as knowledge bases? In Pro- Int.Conf.Mach.Learn.(ICML),2022. 7
ceedings of the 2019 Conference on Empirical Methods [137] AnnaRohrbach,LisaAnneHendricks,KayleeBurns,Trevor
inNaturalLanguageProcessingandthe9thInternational Darrell, andKateSaenko. Objecthallucinationinimage
JointConferenceonNaturalLanguageProcessing,EMNLP- captioning. InProceedingsofthe2018ConferenceonEm-
IJCNLP 2019, Hong Kong, China, November 3-7, 2019, piricalMethodsinNaturalLanguageProcessing,2018. 19
2019. 2 [138] AnnaRohrbach,LisaAnneHendricks,KayleeBurns,Trevor
[125] So¨ren Pirk, Vojtech Krs, Kai-Mo Hu, Suren Deepak Ra- Darrell, andKateSaenko. Objecthallucinationinimage
jasekaran, HaoKang, YusukeYoshiyasu, BedrichBenes, captioning. InProceedingsofthe2018ConferenceonEm-
andLeonidasJ.Guibas. Understandingandexploitingob- piricalMethodsinNaturalLanguageProcessing,Brussels,
jectinteractionlandscapes.ACMTrans.Graph.,36(3):31:1– Belgium,October31-November4,2018,2018. 2
31:14,2017. 2 [139] WilliamShen,GeYang,AlanYu,JansenWong,LesliePack
[126] Charles Ruizhongtai Qi, Hao Su, Kaichun Mo, and Kaelbling,andPhillipIsola. Distilledfeaturefieldsenable
LeonidasJ.Guibas. Pointnet:Deeplearningonpointsets few-shot language-guided manipulation. In Annu. Conf.
for3dclassificationandsegmentation. InIEEE/CVFConf. Robot.Learn.(CoRL),2023. 8
Comput.Vis.PatternRecog.(CVPR),pages77–85,2017. 2, [140] YongliangShen,KaitaoSong,XuTan,DongshengLi,Weim-
6,8,17,22 ingLu,andYuetingZhuang. Hugginggpt:SolvingAItasks
[127] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J. withchatgptanditsfriendsinhuggingface. InAdv.Neural
Guibas. Pointnet++:Deephierarchicalfeaturelearningon Inform.Process.Syst.(NeurIPS),2023. 8
pointsetsinametricspace. InAdv.NeuralInform.Process. [141] HaochenShi,HuazheXu,SamuelClarke,YunzhuLi,andJi-
Syst.(NIPS),pages5099–5108,2017. 2,6,8,17,22 ajunWu. Robocook:Long-horizonelasto-plasticobjectma-
[128] HaozhiQi,AshishKumar,RobertoCalandra,YiMa,and nipulationwithdiversetools. InAnnu.Conf.Robot.Learn.
Jitendra Malik. In-hand object rotation via rapid motor (CoRL),2023. 8
adaptation. InAnnu.Conf.Robot.Learn.(CoRL),2023. 8 [142] Shutterstock. Turbosquid. https://www.
[129] ZekunQi,RunpeiDong,GuofanFan,ZhengGe,Xiangyu turbosquid.com/. 6
Zhang,KaishengMa,andLiYi. Contrastwithreconstruct: [143] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and
Contrastive3drepresentationlearningguidedbygenerative ErikG.Learned-Miller. Multi-viewconvolutionalneural
pretraining. InInt.Conf.Mach.Learn.(ICML),2023. 2,3, networksfor3dshaperecognition. InInt.Conf.Comput.
5,6,17,20,21,22 Vis.(ICCV),2015. 2,22
[130] ZekunQi, MuzhouYu, RunpeiDong, andKaishengMa. [144] JiachenSun,QingzhaoZhang,BhavyaKailkhura,Zhiding
VPP: efficient conditional 3d generation via voxel-point Yu,ChaoweiXiao,andZMorleyMao. Modelnet40-c: A
progressiverepresentation. InAdv.NeuralInform.Process. robustnessbenchmarkfor3dpointcloudrecognitionunder
Syst.(NeurIPS),2023. 6,17,20,22 corruption.InICLR2022WorkshoponSociallyResponsible
[131] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, MachineLearning. 7
HasanAbedAlKaderHammoud,MohamedElhoseiny,and [145] QuanSun,YufengCui,XiaosongZhang,FanZhang,Qiy-
BernardGhanem. Pointnext: Revisitingpointnet++with ing Yu, Zhengxiong Luo, Yueze Wang, Yongming Rao,
improved training and scaling strategies. In Adv. Neural Jingjing Liu, Tiejun Huang, and Xinlong Wang. Gener-
Inform.Process.Syst.(NeurIPS),2022. 6,22 ative multimodal models are in-context learners. CoRR,
[132] AlecRadford,KarthikNarasimhan,TimSalimans,andIlya abs/2312.13286,2023. 8
Sutskever. Improvinglanguageunderstandingbygenerative [146] QuanSun,YuxinFang,LedellWu,XinlongWang,andYue
pre-training. 2018. 2 Cao. EVA-CLIP:improvedtrainingtechniquesforCLIPat
[133] AlecRadford,JeffreyWu,RewonChild,DavidLuan,Dario scale. CoRR,abs/2303.15389,2023. 6
Amodei,andIlyaSutskever. Languagemodelsareunsuper- [147] QuanSun,QiyingYu,YufengCui,FanZhang,Xiaosong
visedmultitasklearners. OpenAIblog,1(8):9,2019. 2 Zhang,YuezeWang,HongchengGao,JingjingLiu,Tiejun
[134] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Huang,andXinlongWang. Generativepretraininginmulti-
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, modality. CoRR,abs/2307.05222,2023. 8
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen [148] D´ıdacSur´ıs,SachitMenon,andCarlVondrick. Vipergpt:
Krueger,andIlyaSutskever. Learningtransferablevisual Visualinferenceviapythonexecutionforreasoning. CoRR,
models from natural language supervision. In Int. Conf. abs/2303.08128,2023. 8
Mach.Learn.(ICML),pages8748–8763.PMLR,2021. 2,7, [149] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,
22 XuechenLi,CarlosGuestrin,PercyLiang,andTatsunoriB.
14Hashimoto. Stanford alpaca: An instruction-following [163] TongWu, GuandaoYang, ZhibingLi, KaiZhang, Ziwei
llamamodel. https://github.com/tatsu-lab/ Liu,LeonidasJ.Guibas,DahuaLin,andGordonWetzstein.
stanford_alpaca,2023. 8 Gpt-4v(ision)isahuman-alignedevaluatorfortext-to-3d
[150] HugoTouvron,ThibautLavril,GautierIzacard,XavierMar- generation. CoRR,abs/2401.04092,2024. 19
tinet, Marie-Anne Lachaux, Timothe´e Lacroix, Baptiste [164] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu,
Rozie`re,NamanGoyal,EricHambro,FaisalAzhar,Aure´lien Linguang Zhang, Xiaoou Tang, and Jianxiong Xiao. 3d
Rodriguez,ArmandJoulin,EdouardGrave,andGuillaume shapenets:Adeeprepresentationforvolumetricshapes. In
Lample. Llama: Openandefficientfoundationlanguage IEEE/CVFConf.Comput.Vis.PatternRecog.(CVPR),pages
models. CoRR,abs/2302.13971,2023. 2,3,8,21 1912–1920,2015. 6,7,20,22
[151] MikaelaAngelinaUy,Quang-HieuPham,Binh-SonHua, [165] SainingXie,JiataoGu,DemiGuo,CharlesR.Qi,LeonidasJ.
ThanhNguyen,andSai-KitYeung. Revisitingpointcloud Guibas, andOrLitany. Pointcontrast: Unsupervisedpre-
classification:Anewbenchmarkdatasetandclassification training for 3d point cloud understanding. In Eur. Conf.
modelonreal-worlddata. InIEEE/CVFConf.Comput.Vis. Comput.Vis.(ECCV),pages574–591.Springer,2020. 22
PatternRecog.(CVPR),pages1588–1597,2019. 6,7,21, [166] RunsenXu,XiaolongWang,TaiWang,YilunChen,Jiang-
22 miao Pang, and Dahua Lin. Pointllm: Empowering
[152] VladimirVapnik. Statisticallearningtheory. Wiley,1998. largelanguagemodelstounderstandpointclouds. CoRR,
20 abs/2308.16911,2023. 3,7,8,18,19,21
[153] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko- [167] Yinzhen Xu, Weikang Wan, Jialiang Zhang, Haoran Liu,
reit,LlionJones,AidanN.Gomez,LukaszKaiser,andIllia Zikang Shan, Hao Shen, Ruicheng Wang, Haoran Geng,
Polosukhin.Attentionisallyouneed.InAdv.NeuralInform. YijiaWeng,JiayiChen,TengyuLiu,LiYi,andHeWang.
Process.Syst.(NIPS),pages5998–6008,2017. 6,20,22 Unidexgrasp:Universalroboticdexterousgraspingvialearn-
[154] Weikang Wan, Haoran Geng, Yun Liu, Zikang Shan, ingdiverseproposalgenerationandgoal-conditionedpolicy.
YaodongYang,LiYi,andHeWang. Unidexgrasp++: Im- InIEEE/CVFConf.Comput.Vis.PatternRecog.(CVPR),
provingdexterousgraspingpolicylearningviageometry- 2023. 2,4,8
awarecurriculumanditerativegeneralist-specialistlearning. [168] Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct:
InInt.Conf.Comput.Vis.(ICCV),2023. 2,4,8 Improvingmulti-modalzero-shotlearningviainstruction
[155] GuanzhiWang,YuqiXie,YunfanJiang,AjayMandlekar, tuning. InProceedingsofthe61stAnnualMeetingofthe
ChaoweiXiao,YukeZhu,LinxiFan,andAnimaAnandku- AssociationforComputationalLinguistics(ACL)(Volume1:
mar. Voyager: Anopen-endedembodiedagentwithlarge LongPapers),2023. 8
languagemodels. CoRR,abs/2305.16291,2023. 8 [169] LeXue,MingfeiGao,ChenXing,RobertoMart´ın-Mart´ın,
[156] HeWang,SrinathSridhar,JingweiHuang,JulienValentin, JiajunWu,CaimingXiong,RanXu,JuanCarlosNiebles,
ShuranSong,andLeonidasJ.Guibas. Normalizedobject andSilvioSavarese. ULIP:learningunifiedrepresentation
coordinatespaceforcategory-level6dobjectposeandsize oflanguage,imageandpointcloudfor3dunderstanding.
estimation. InIEEE/CVFConf.Comput.Vis.PatternRecog. InIEEE/CVFConf.Comput.Vis.PatternRecog.(CVPR),
(CVPR),2019. 2,8 2023. 2,6
[157] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, [170] LeXue,NingYu,ShuZhang,JunnanLi,RobertoMart´ın-
MichaelM.Bronstein,andJustinM.Solomon. Dynamic Mart´ın, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Car-
graphCNNforlearningonpointclouds.ACMTrans.Graph., los Niebles, and Silvio Savarese. ULIP-2: towards scal-
38(5):146:1–146:12,2019. 6,8 ablemultimodalpre-trainingfor3dunderstanding. CoRR,
[158] ZiyiWang,XuminYu,YongmingRao,JieZhou,andJiwen abs/2305.08275,2023. 5,6,18
Lu. Take-a-photo:3d-to-2dgenerativepre-trainingofpoint [171] Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge,
cloudmodels. InInt.Conf.Comput.Vis.(ICCV),2023. 6 XiuLi,andYingShan. Gpt4tools:Teachinglargelanguage
[159] HaoWen,YunzeLiu,JingweiHuang,BoDuan,andLiYi. modeltousetoolsviaself-instruction.InAdv.NeuralInform.
Point primitive transformer for long-term 4d point cloud Process.Syst.(NeurIPS),2023. 8
videounderstanding. InEur.Conf.Comput.Vis.(ECCV), [172] Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin,
2022. 8 Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu,
[160] Yijia Weng, He Wang, Qiang Zhou, Yuzhe Qin, Yueqi Michael Zeng, and Lijuan Wang. MM-REACT: prompt-
Duan,QingnanFan,BaoquanChen,HaoSu,andLeonidasJ. ing chatgpt for multimodal reasoning and action. CoRR,
Guibas.CAPTRA:category-levelposetrackingforrigidand abs/2303.11381,2023. 8
articulatedobjectsfrompointclouds. InInt.Conf.Comput. [173] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,
Vis.(ICCV),2021. 2,8 YiyangZhou,JunyangWang,AnwenHu,PengchengShi,
[161] ChenfeiWu,ShengmingYin,WeizhenQi,XiaodongWang, YayaShi,ChenliangLi,YuanhongXu,HehongChen,Jun-
Zecheng Tang, and Nan Duan. Visual chatgpt: Talking, fengTian,QianQi,JiZhang,andFeiHuang. mplug-owl:
drawingandeditingwithvisualfoundationmodels. CoRR, Modularizationempowerslargelanguagemodelswithmul-
abs/2303.04671,2023. 8 timodality. CoRR,abs/2304.14178,2023. 8
[162] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat- [174] ShuquanYe,DongdongChen,SongfangHan,andJingLiao.
SengChua. Next-gpt:Any-to-anymultimodalLLM. CoRR, 3dquestionanswering. IEEETransactionsonVisualization
abs/2309.05519,2023. 8 andComputerGraphics,2022. 8
15[175] Li Yi, Vladimir G Kim, Duygu Ceylan, I-Chao Shen, IEEE/CVFConf.Comput.Vis.PatternRecog.(CVPR),2023.
MengyanYan,HaoSu,CewuLu,QixingHuang,AllaShef- 6,20
fer, and Leonidas Guibas. A scalable active framework [189] ShilongZhang,PeizeSun,ShoufaChen,MinXiao,Wenqi
forregionannotationin3dshapecollections. ACMTrans. Shao,WenweiZhang,KaiChen,andPingLuo. Gpt4roi:In-
Graph.,35(6):1–12,2016. 22 structiontuninglargelanguagemodelonregion-of-interest.
[176] LiYi,HaoSu,XingwenGuo,andLeonidasJ.Guibas.Sync- CoRR,abs/2307.03601,2023. 8
speccnn:SynchronizedspectralCNNfor3dshapesegmen- [190] YueZhang, YafuLi, LeyangCui, DengCai, LemaoLiu,
tation. In IEEE/CVF Conf. Comput. Vis. Pattern Recog. TingchenFu,XintingHuang,EnboZhao,YuZhang,Yu-
(CVPR),2017. 22 longChen,LongyueWang,AnhTuanLuu,WeiBi,Freda
[177] Li Yi, Haibin Huang, Difan Liu, Evangelos Kalogerakis, Shi, and Shuming Shi. Siren’s song in the AI ocean: A
HaoSu,andLeonidasJ.Guibas. Deeppartinductionfrom surveyonhallucinationinlargelanguagemodels. CoRR,
articulated object pairs. ACM Trans. Graph., 37(6):209, abs/2309.01219,2023. 19
2018. 2,8 [191] LiangZhao,EnYu,ZhengGe,JinrongYang,HaoranWei,
[178] YangYou,BokuiShen,CongyueDeng,HaoranGeng,He Hongyu Zhou, Jianjian Sun, Yuang Peng, Runpei Dong,
Wang,andLeonidasJ.Guibas. Makeadonut: Language- ChunruiHan, andXiangyuZhang. Chatspot: Bootstrap-
guidedhierarchicalemd-spaceplanningforzero-shotde- pingmultimodalllmsviaprecisereferringinstructiontuning.
formableobjectmanipulation.CoRR,abs/2311.02787,2023. CoRR,abs/2307.09474,2023. 8
8 [192] XiZhao,HeWang,andTakuKomura. Indexing3dscenes
[179] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, usingtheinteractionbisectorsurface. ACMTrans.Graph.,
KevinLin,ZichengLiu,XinchaoWang,andLijuanWang. 33(3):22:1–22:14,2014. 2
Mm-vet:Evaluatinglargemultimodalmodelsforintegrated
[193] Juntian Zheng, Qingyuan Zheng, Lixing Fang, Yun Liu,
capabilities. CoRR,abs/2308.02490,2023. 2,5
andLiYi. CAMS:canonicalizedmanipulationspacesfor
[180] XuminYu,LuluTang,YongmingRao,TiejunHuang,Jie
category-levelfunctionalhand-objectmanipulationsynthe-
Zhou,andJiwenLu. Point-bert:Pre-training3dpointcloud
sis.InIEEE/CVFConf.Comput.Vis.PatternRecog.(CVPR),
transformerswithmaskedpointmodeling. InIEEE/CVF
2023. 2
Conf.Comput.Vis.PatternRecog.(CVPR),2022. 6,17,20,
[194] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan
22
Zhuang,ZhanghaoWu,YonghaoZhuang,ZiLin,Zhuohan
[181] JunboZhang,RunpeiDong,andKaishengMa.CLIP-FO3D:
Li,DachengLi,EricP.Xing,HaoZhang,JosephE.Gonza-
learningfreeopen-world3dscenerepresentationsfrom2d
lez,andIonStoica. Judgingllm-as-a-judgewithmt-bench
dense CLIP. In Int. Conf. Comput. Vis. Worksh. (ICCV
andchatbotarena. CoRR,abs/2306.05685,2023. 2
Workshop),2023. 8,22
[195] JunshengZhou,JinshengWang,BaoruiMa,Yu-ShenLiu,
[182] LinfengZhang,ChenglongBao,andKaishengMa. Self-
TiejunHuang,andXinlongWang.Uni3d:Exploringunified
distillation:Towardsefficientandcompactneuralnetworks.
3drepresentationatscale. InInt.Conf.Learn.Represent.
IEEETrans.PatternAnal.Mach.Intell.,44(8):4388–4403,
(ICLR),2024. 6,21
2022. 23
[196] YiyangZhou,ChenhangCui,JaehongYoon,LinjunZhang,
[183] LinfengZhang,XinChen,RunpeiDong,andKaishengMa.
ZhunDeng,ChelseaFinn,MohitBansal,andHuaxiuYao.
Region-awareknowledgedistillationforefficientimage-to-
Analyzingandmitigatingobjecthallucinationinlargevision-
imagetranslation. InBrit.Mach.Vis.Conf.(BMVC),2023.
languagemodels. InInt.Conf.Learn.Represent.(ICLR),
[184] LinfengZhang,RunpeiDong,Hung-ShuoTai,andKaisheng
2024. 2,19
Ma.Pointdistiller:Structuredknowledgedistillationtowards
[197] DeyaoZhu,JunChen,XiaoqianShen,XiangLi,andMo-
efficient and compact 3d detection. In IEEE/CVF Conf.
hamedElhoseiny. Minigpt-4: Enhancingvision-language
Comput.Vis.PatternRecog.(CVPR),2023. 23
understandingwithadvancedlargelanguagemodels. CoRR,
[185] RenruiZhang, ZiyuGuo, PengGao, RongyaoFang, Bin
abs/2304.10592,2023. 8
Zhao, Dong Wang, Yu Qiao, and Hongsheng Li. Point-
[198] XiangyangZhu,RenruiZhang,BoweiHe,ZiyuGuo,Ziyao
m2AE:Multi-scalemaskedautoencodersforhierarchical
Zeng,ZipengQin,ShanghangZhang,andPengGao. Point-
pointcloudpre-training. InAdv.NeuralInform.Process.
clipv2:Promptingclipandgptforpowerful3dopen-world
Syst.(NeurIPS),2022. 6,20
learning. InInt.Conf.Comput.Vis.(ICCV),2023. 6,8
[186] RenruiZhang,ZiyuGuo,WeiZhang,KunchangLi,Xupeng
[199] ZiyuZhu,XiaojianMa,YixinChen,ZhidongDeng,Siyuan
Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li.
Huang,andQingLi. 3d-vista:Pre-trainedtransformerfor
Pointclip:PointcloudunderstandingbyCLIP.InIEEE/CVF
3d vision and text alignment. In Int. Conf. Comput. Vis.
Conf.Comput.Vis.PatternRecog.(CVPR),2022. 2,6,8
(ICCV),2023. 8
[187] Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,
ShilinYan,PanLu,HongshengLi,PengGao,andYuQiao.
Llama-adapter: Efficient fine-tuning of language models
withzero-initattention. CoRR,abs/2303.16199,2023. 8
[188] Renrui Zhang, Liuhui Wang, Yu Qiao, Peng Gao, and
HongshengLi. Learning3drepresentationsfrom2dpre-
trainedmodelsviaimage-to-pointmaskedautoencoders. In
16Contents A.AdditionalExperiments
1.Introduction 2 A.1.AblationStudy
2.SHAPELLM 3
A.1.1 SHAPELLMArchitecture
2.1.OverallArchitecture . . . . . . . . . . . . . 3
Architecture LetF bethemultimodalLLMparameter-
θ
2.2.Howtoalleviateinteractive3Dunderstanding
izedbyθ,weuseaRECON++encoderH ϕasSHAPELLM’s
DataDessert? . . . . . . . . . . . . . . . . 3
3Dpointcloudencoder,followedbytwoMLPprojection
2.3.RECON++: ScalingUp3DRepresentation
layersM andM for3Dembeddingprojectionof
ζlocal ζglobal
Learning . . . . . . . . . . . . . . . . . . . 4
RECON++’slocalandglobalrepresentations,respectively.
Tofacilitategeometry-necessarytaskslike6-DoFposees-
3. 3D MM-Vet: 3D Multimodal Comprehension
timation, we use absolute position encoding (APE) with
EvaluationBenchmark 5
an MLP projection M to provide additional precise
ζAPE
low-level geometric information. Given the original 3D
4.Experiments 6
point cloud inputs P = {p |i = 1,2,...,N} ∈ RN×3
4.1. 3D Representation Transferring with RE- i
withN coordinatesencodedina(x,y,z)Cartesianspace.
CON++ . . . . . . . . . . . . . . . . . . . 6
Following previous works [36, 129, 180], N seed points
4.2.MultimodalComprehensionwithSHAPELLM 7 s
arefirstsampledusingfarthestpointsampling(FPS).The
5.Discussions 7 point cloud P is then grouped into N s neighborhoods
5.1.Whatislearnedfrommulti-viewalignment? 7 N = {N i|i = 1,2,...,N s} ∈ RNs×K×3 withgroupcen-
5.2.IsSHAPELLMgroundedinphysicalworlds? 8 troidsfromtheseedpointsetPs. TheAPErepresentation
5.3.CanSHAPELLMgeneralizetounseenobjects? 8 canbewrittenas
6.RelatedWorks 8 E =M ◦Ps. (2)
APE ζAPE
7.Conclusions 8
Thelocalandtransformation-invariant3Dgeometricembed-
(cid:0) (cid:1)
dings x = MAX Φ (ξ ) for P ,i = 1,2,...,N is
A.AdditionalExperiments 17 i pi,j∈Ni γ i,j i s
A.1.AblationStudy . . . . . . . . . . . . . . . . 17 usedas3DtokenembeddingsofRECON++,whereΦ γ isa
A.1.1 SHAPELLMArchitecture . . . . . 17 per-pointMLPpointfeatureextractor[126,127]andξ i,j is
A.1.2 BaselineImprovement . . . . . . . 18 thefeatureofj-thneighbourpointp i,j intheneighbourhood
A.2.MultimodalComprehensionwithSHAPELLM 18 N i. Let{g qimage}G q=1beGmulti-viewimageglobalqueries
A.3.RepresentationTransferringwithRECON++ 20 and gtext be the global text query. RECON++ outputs the
localandglobal3Dpointcloudrepresentationsbytaking
B.AdditionalInformationabout3DMM-vet 20 3Dembeddingsandglobalqueriesasinputs:
B.1.EvaluationSystem . . . . . . . . . . . . . . 20
B.2.Analysis . . . . . . . . . . . . . . . . . . . 21 (cid:104) e ,e (cid:105) =(cid:104) H (cid:0) [P ,{gimage}G ,gtext](cid:1)(cid:105) , (3)
local global ϕ s q q=1
C.Implementationdetails 21
andtherepresentationtoSHAPELLMis:
D.Trainingdetails 22
[E ,E ]=[M ◦e ,M ◦e ]. (4)
local global ζlocal local ζglobal global
E.AdditionalRelatedWork 22
E.1.3DRepresentationLearning . . . . . . . . . 22
In addition, inspired by prefix-tuning [86] and dream
queries [37], we append Q-length learnable embeddings
F.FutureWorks 23 {dAPE}Q ,{dlocal}Q ,{dglobal}Q asvisualpromptsrep-
q q=1 q q=1 q q=1
resentation[130]E foradaptivelymodulatingdifferent
prompt
semanticinformationencodedinAPE,localandglobalRE-
CON++representations,respectively.
Formally,theencoded3DrepresentationstoSHAPELLM
canbewrittenas:
(cid:104) (cid:105)
{dAPE}Q ,E ,{dlocal}Q ,E ,{dglobal}Q ,E .
q q=1 APE q q=1 local q q=1 global
(5)
17Table6.AblationstudyonthededicateddesignsofSHAPELLM Table 7. Ablation study on baseline improvements. Results
architecture.Theperformanceofmultimodalcomprehensionon are tested on 3D MM-Vet with the baseline model PointLLM-
3DMM-VetandreferringexpressiongroundingonGAPartNetwith 13B[166]usingdifferentpointcloudencodersandSFTdata.Line
SHAPELLM-13Bisreported.NotethatE globaliscalculatedwith 0istheresultwiththeoriginalPointLLMconfiguration. Line1
bothglobalqueriesandcross-attentionwithlocal3Dembeddings. and2denotetheresultsofchangingthepointcloudencoderto
E E E E 3DMM-Vet GAPartNet RECON++andusingourSFTdata.NotethatwhenRECON++is
APE prompt local global
usedastheencoder,E andE arealsousedbydefault.
✓ ✗ ✗ ✗ 30.8 12.3 APE prompt
✓ ✓ ✗ ✗ 32.0 11.4 Encoder SFTData Rec Know Gen Spat Emb Total
✗ ✗ ✓ ✗ 42.2 10.0
✗ ✓ ✗ ✓ 50.3 10.5 0 ULIP-2[170] PointLLM 46.6 48.3 38.8 45.2 50.9 46.6
✓ ✗ ✓ ✓ 52.3 10.5 1 RECON++ PointLLM 47.5 52.8 43.6 44.9 54.5 50.8
✓ ✓ ✗ ✓ 50.3 11.7 2 RECON++ Ours 46.8 53.0 53.9 45.3 68.4 53.1
✗ ✗ ✗ ✓ 52.4 11.7
✗ ✗ ✓ ✓ 49.6 10.1 ULIP-2 by a large margin regarding 3D representation
✗ ✓ ✓ ✓ 51.7 10.1
transferringlearningandzero-shotlearning.
✓ ✓ ✓ ✓ 53.1 11.7
• Improvementfromdata. (Line2)AsstatedinSec.2.2,we
haveconstructedinstruction-followingdataforsupervised
InputComponents Tab.6showstheablationstudyof
fine-tuning (SFT) using GPT-4V involving comprehen-
eachinputcomponentbysupervisedfine-tuningwithdiffer-
sivetopics. ByfurtherusingtheSFTdatacuratedbyus,
entinputrepresentations,demonstratingthatitisnecessary
toemployalldesignsforachievingdecentperformanceon
PointLLM’s performance gap to SHAPELLM has been
fulfilled. This demonstrates the superiority of our SFT
both3Dcomprehensionandreal-worldgrounding.
data,wherethedecentqualitycomesfromthemoread-
VisualPromptNumber Fig.8showstheperformanceof
vancedGPT4-Vmodelusingmulti-viewimagesandthe
SHAPELLMusingdifferentnumbersofprompts,including
comprehensivetopicscoveredinthedata.
1,8,16,32,and64. Thisablationstudyhasshownthata
differentnumberofpromptsleadstovariedimprovements,
A.2.MultimodalComprehensionwithSHAPELLM
andtheoptimalsettingis32. Thisobservationissimilarto
VPT[76]wherethepromptsusedtomodulateTransformer Generative3DObjectRecognition&Captioning Follow-
attentionshouldbestudied[59]. ingPointLLM[166],weconductgenerative3Drecognition
andcaptioningexperiments. Tab.8shows3Dobjectclassifi-
53.5
53.1 cationoverallaccuracy(%)andcaptioningperformanceeval-
53 uatedbyGPT-4anddata-drivenmetrics: Sentence-BERT
(S-BERT)[135]andSimCSE[43]. Itcanbeobservedthat 52.5 52.2
52 51.9 SHAPELLMconsistentlyoutperformsothermethodsacross
52 allmetrics,demonstratingrobustrecognitionandinstruction-
51.5 51.3 followingcapabilities.
Note that similar to PointLLM’s findings, we also no-
51
1 8 16 32 64 tice that the 3D captioning performance evaluated by tra-
ditional metrics like BLEU-1 [120], ROUGE-L [89], and
No. of Learnable Prompts
METEIOR [5] are highly unreliable in accurately reveal-
Figure8.Ablationstudyonvisualpromptnumber.Theperfor-
ing the response quality. This is further demonstrated by
manceofSHAPELLM-13Bon3DMM-Vetisreported.
human-orientedevaluation,suchasthepreferencewinrate
comparisonpresentednext.
Singe-View Point Cloud Inputs As stated in Sec. 4.2,
A.1.2 BaselineImprovement
we construct 3D MM-Vet-C which studies three kinds of
Can we improve the baseline to bridge the gap between corruptionscommonlymetinreal-worldscenarios: “single-
PointLLMandSHAPELLM?InTab.6,westudytwotech- view”,“jitter”,and“rotate”. Amongthesecorruptions,the
nicalfactorsthatarecontributedbySHAPELLM:3Dpoint “singe-view”issuestandsoutasthemostcriticalchallenge
cloudencoderandSFTdata. sinceobtainingtheobjects’completepointcloudsisnon-
• Improvement from encoder. (Line 1) First, by changing trivial,similartomulti-viewimages. Asaresult,everyday
PointLLM’sencodertoRECON++,asignificantimprove- real-worldrobotsonlygetsingle-view3Dperceptionswith
mentof+4.20%isobtained. Thisdemonstratesthesignif- sensorssuchasRGB-D[56]. Fig.9showsthequalitative
icantlybetter3DrepresentationextractionofRECON++ examplesofSHAPELLM-13B’sresponseusingsingle-view
comparedtoULIP-2. Itisconsistentwithpreviousfind- pointcloudinputs,demonstratingsurprisinglyoutstanding
ings in Tab. 1 and Tab. 2 that RECON++ outperforms robustnessinprocessingsuchoccludedinputs.
18
)%(
teV-MM
D3clean point cloud original mesh clean point cloud original mesh
(onlyforreference) (onlyforreference)
As a robot, Where Are all the chairs
single-view point cloud
s roh bo ou tl id cy ao rmup toos gi rti ao sn pt ih t?e User
single-view point cloud
c tho em tp ale bt lee ?lyplacedunder User
As a robot, you should Yes, all the chairs are
position the robotic arm completely placed under the
around the middle of the table.
spherical object to grasp it
securely.
clean point cloud (ono lyri fg oin ra rl e m fee resh nce) clean point cloud (ono lyri fg oin ra rl e m fee resh nce)
Pleasegivemeseveralstepsto
weighing2thingsandsortthem
bytheweights. User
Givemesomestepsto
u base sinth ois fwth ai tn eg r.togeta User Toweigh2thingsandsortthembyweight,youcan
single-view point cloud Tousethisthingtogetabasinofwater, single-view point cloud f 1o .llo Gw at th he es re ths ete 2ps i: temsyouwanttoweigh.
youwouldfollowthesesteps: 2. Placetheitemsonthescale.
1. Turn the handle counter-clockwise to 3. Makesurethescaleissettozero.
openthevalveandallowwatertoflow 4. Placethefirstitemonthescale.
fromthespout. 5. Recordtheweightofthefirstitem.
2.Fillthebasinwithwater. 6. Placetheseconditemonthescale.
3.Oncethebasinisfull,turnthehandle 7. Recordtheweightoftheseconditem.
clockwisetoclosethevalveandstopthe 8. Comparetheweightsofthe2items.
flowofwater. 9. Determinewhichitemisheavierandwhichislighter.
4.Enjoyyourfreshlyfilledbasinofwater. 10.Sorttheitemsbasedontheirweights.
clean point cloud (ono lyri fg oin ra rl e m fee resh nce) clean point cloud (ono lyri fg oin ra rl e m fee resh nce)
Please describe the
single-view point cloud t ta hs iste thina gn .d texture of User single-view point cloud Whatisshedoingnow? User
The taste of a banana is Thefigureappearstobein
sweet with a slightly firm mid-motion,asifshe'sinthe
texture. act of reaching or pointing
towardssomething.
Figure9.3Dmultimodaldialogueusingsingle-viewpointcloudinputs.AllanswersaregeneratedbySHAPELLM-13Bwithsingle-view
occludedinputs. SHAPELLMachievesoutstandingrobustnessagainstsuchocclusion,whichisthemostcommonlymetinrealworlds.
Table8.Generative3Drecognitionandcaptioning.Theaccuracy win tie lose
(%)averagedundertheinstruction-typedprompt“Whatisthis?” S Poh ia np te LLL MLMvs. 66 15 19
andthecompletion-typedprompt“Thisisanobjectof”isreported. ShapeLLMvs. 73 20 7
LLaVA
Classification Captioning S Hh ua mp ae nLLMvs. 52 18 30
Method Input
MN-40ObjaverseGPT-4S-BERTSimCSE 0 10 20 30 40 50 60 70 80 90 100
InstructBLIP-7B[24] 1-ViewImg. 25.51 43.50 45.34 47.41 48.48
InstructBLIP-13B[24] 1-ViewImg. 28.69 34.25 44.97 45.90 48.86 Figure10.Winratecomparison.
LLaVA-7B[93] 1-ViewImg. 39.71 50.00 46.71 45.61 47.10
LLaVA-13B[93] 1-ViewImg. 36.59 51.75 38.28 46.37 45.90
3DObj.+
3D-LLM[62] - 45.25 33.42 44.48 43.68 VisualHallucination Visualhallucinationisawell-known
Mul.-V.Img.
issue in LLMs and MLLMs that generate non-existent
PointLLM-7B[166] PointCloud 52.63 53.00 44.85 47.47 48.55
PointLLM-13B[166] PointCloud 52.78 54.00 48.15 47.91 49.12 objects or identities from the input data, significantly
SHAPELLM-7B PointCloud 53.08 54.50 46.92 48.20 49.23
compromising their multimodal comprehension capabili-
SHAPELLM-13B PointCloud 52.96 54.00 48.94 48.52 49.98
ties[25,53,91,196]andmayposesafetyrisks[110,137].
Human Win Rate Comparison GPT-4 [116] is widely Recentresearch[190]suggeststhathallucinationmaystem
usedasanevaluatorinnaturallanguageandvisionlanguage frombiasesintrainingdata,particularlywithinsupervised
processing,asseeninrecentmodernbenchmarkslikeMM- fine-tuningdata,orinappropriategenerationstrategies. In
BenchandMM-Vet.Recentstudies[163]havedemonstrated Fig. 11, we qualitatively demonstrate the illusion evalua-
thatChatGPT-basedevaluationismorecloselyalignedwith tionofSHAPELLMcomparedtoothermethods. Weassess
human preferences compared to traditional metrics. With themodel’sabilitytocounteractillusionsbypromptingit
GPT4-turbo,thestandarddeviationof3DMM-Vetisless with detailed captions and misleading questions. The re-
than0.1. Tofurtherverifythesoundnessofthemodels’re- sults in Fig. 11 demonstrate that previous methods Point-
sponse,wealsoconducthumanevaluationandreportthewin Bind&Point-LLM [54] and PointLLM [166] suffer from
rateinFig.10,where SHAPELLM demonstratessuperior the problems of mis-recognition and mis-associating non-
preferencebyhumans. existingidentities.
19point cloud input point cloud input point cloud input
original mesh original mesh original mesh
(onlyforreference) (onlyforreference) (onlyforreference)
Describethe3Dobjectindetail. User
Describethe3Dobjectindetail. User
Describethe3Dobjectindetail. User
Thisisa3Dmodelofatraditionalgrand
piano, painted in a rich black hue. The The 3D model presents a grand piano,
The 3D object is a clock, which is a piano,knownforproducingawiderange knownforitselegantandrichly textured
timepiecedesignedtotellthetime.Itis ofsounds,boastsafullsetofwhiteand design. Characterized by its black and
a mechanical device that uses a black keys, indicating a full octave is white keys, the piano showcases a leg-
combination of gears, wheels, and available.Asolitarylamp,possiblytobe supportingstandatoneendandamusic
springstomovethehandsanddisplay used for reading or near vision work, racktoholdthesheetmusicattheother
the time. The clock in the image is a rests on its rack. The piano, with its end.Thepiano,typicallyusedinclassical
small,decorativepiece,possiblyatoy gleaming black surface, forms a striking music, appears to be full-sized and is
or a novelty item. It has a unique contrast against the lamp. This model likely capable of producing a wide range
design,featuringawoodenbaseanda suggests uses in professional settings, ofsounds.
metal structure. The clock face is suchasmusicperformancesandpractice,
prominently displayed on the front of as well as leisure activities like reading Isthereanyotherobjectinit? User
the device, with the clock hands andstudying.
positioned at the top and the bottom.
Theclockismountedonastand,which Yes,thereisasetofkeysinthegrand
allows it to be easily displayed and Isthereanyotherobjectinit? User piano. The keys are arranged in a
admired. horizontal row, and the body has a
Yes,thereisasmallbook smoothsurfacewithaglossyfinish.
openonthepiano'srack.
(a)Point-Bind&Point-LLM (b)PointLLM (c)ShapeLLM
Figure11.Qualitativecomparisonexamplesofvisualhallucination.
A.3.RepresentationTransferringwithRECON++ CON++ achieves state-of-the-art performance in all the
benchmarkscomparedtopreviousworks.
LinearSVMEvaluation LinearSVMevaluation[8,152]
can be used to evaluate the discriminative quality of pre-
trainedfeatures[51]. TheresultsonModelNet40areshown Table10.Few-shotclassificationresultsonModelNet40.Overall
inTab.9. ItshowsthatourRECON++outperformsPoint- accuracy(%)withoutvotingisreported.
BERT,whichalsousesplainTransformerswithcontrastive
5-way 10-way
objectives,byaclearmarginof+6.2%. Comparedtohierar- Method
10-shot 20-shot 10-shot 20-shot
chicalTransformersmethods,ourRECON++outperforms
PointM2AE[185]by+0.7%. Transformer[153] 87.8±5.2 93.3±4.3 84.6±5.5 89.4±6.3
Point-BERT[180] 94.6±3.1 96.3±2.7 91.0±5.4 92.7±5.1
Point-MAE[119] 96.3±2.5 97.8±1.8 92.6±4.1 95.0±3.0
Table 9. Linear SVM classification on ModelNet40. Overall
Point-M2AE[185] 96.8±1.8 98.3±1.4 92.3±4.5 95.0±3.0
accuracy(%)withoutvotingisreported. ACT[36] 96.8±2.3 98.0±1.4 93.3±4.0 95.6±2.8
VPP[130] 96.9±1.9 98.3±1.5 93.0±4.0 95.4±3.1
Method Hierachical ModelNet40
RECON[129] 97.3±1.9 98.9±1.2 93.3±3.9 95.8±3.0
Point-BERT[180] ✗ 87.4 PointGPT[18] 98.0±1.9 99.0±1.0 94.1±3.3 96.1±2.8
PointMAE[119] ✗ 91.0 RECON++ 98.0±2.3 99.5±0.8 94.5±4.1 96.5±3.0
PointM2AE[185] ✓ 92.9
ACT[36] ✗ 93.1
I2P-MAE[188] ✓ 93.4
RECON[129] ✗ 93.4 B.AdditionalInformationabout3DMM-vet
RECON++ ✗ 93.6
B.1.EvaluationSystem
Unlikeclassificationorregressiontasks,languagegeneration
Few-Shot3DObjectRecognition Few-shotlearningis taskslackadefinitivegroundtruththatcancomprehensively
criticalforevaluatingtherepresentationtransferringcapabil- coverdiversereal-lifescenarios. Therefore,evaluatingthe
itiesindataandtrainingefficiency. Weconductfew-shot3D alignmentofmodel-generatedresultswiththequestionand
object recognition experiments on the ModelNet40 [164] assessingtheirappropriatenessbecomesachallengingprob-
dataset, and the results are shown in Tab. 10. Our RE- lem,requiringareasonablequantitativescore. Fortunately,
20Table11.DetailsofRECON++modelvariants.Thistableformat
followsDosovitskiyetal..
44, 19%
59, 25%
Model Layers Hiddensize MLPsize Heads
Emb
Rec
RECON++-S 12 384 1536 6
RECON++-B 12 768 3072 12
Spat
40, 17% RECON++-L 24 1024 4096 16
Know
Gen
54, 23% VisualRecognitionpartscontainmultiplesubpartsthatcom-
36, 16% prehensivelyevaluatethesecapacitiesandthusholdhigher
proportions. Fig.13showsanexampleofhowweprompt
Figure12. ThenumberofdiversequestionsofcoreVLcapa- GPT-4for3DMM-Vetevaluation. Fig.14andFig.15illus-
bilitieson3DMM-Vet.Rec:GeneralVisualRecognition,Know: trateadditionalexamplesof3DMM-VetQ&As.
Knowledge, Gen: LanguageGeneration, Spat: SpatialAware-
ness,Emb:EmbodiedInteraction. C.Implementationdetails
wehaveobservedtherecentsurgeinthepopularityofGPT, RECON++ FollowingthestandardViT[38]architecture,
providing us with a dependable tool for conducting open- we design four different model structures consistent with
endedevaluations. prior work [94, 129, 195]. The model parameters are
ToenhancetheperformanceofGPT,weemployafew- showninTab.11. FollowingOpenShape[94],weemploy
shot style in-context prompt. This involves feeding GPT four datasets as pretraining data, namely Objaverse [29],
withpromptsfromevaluativeexamplesandinstructingitto ShapeNet [13], ABO [22], and 3D-FUTURE [42]. Each
generatescores. Specifically,wepresentpromptstoobtain pointcloudsamplehasasizeof10000×6,wherethefirst
ascorerangingfrom0to1,indicatingthedegreeofsimi- threedimensionsrepresentxyz coordinates,andthelatter
laritybetweenthemodel-generatedanswersandtheground threedimensionsrepresentrgbvalues.
truthsweprovided. Whenimplementingthisapproach,we Regarding the masked modeling strategy, we experi-
observedthatresultsgeneratedmultipletimesmayvaryalot. mentedwithbothrandommaskingstrategiesandthelatest
Toaddressit,weapplythesameevaluationsettingtoasingle causalmaskingstrategy. Usingcausalmaskingasinitializa-
answerforK iterations,obtainingtheaverageresultasthe tionsignificantlyimprovestransferlearningcapability, as
finalscoreforapreciseanswer. ThescoreofananswerS a shownintheablationexperimentsinTab.12. Specifically,
andthetotalscoreS tofanswersetAarecalculatedby: the point encoder of SHAPELLM still employs the origi-
nallocal-guidedstop-gradientstrategy[129]. Additionally,
(cid:80)K
s
(cid:80)
S to enhance global classification and retrieval capabilities,
ai a
S = i=1 , S = a∈A . webackpropagategradientsfromtheglobalbranchtothe
a K t N
localbranchinopenvocabularyzero-shotexperiments,as
HerewesetK = 5, ands isthescoreofthei testof demonstratedintheablationexperimentsinTab.12.
ai th
answera. Theaveragescoreforaspecificcapabilityisthe
Table12.Ablationstudyonmasktype&stopgradient.transfer:
sumofscoresincategoryC answersetA :
C
fine-tuned3DrecognitiononScanObjectNN[151].zero-shot:zero-
(cid:80) S shot3DrecognitiononObjaverse-LVIS[29].Allexperimentsare
a
S =
a∈AC
,
conductedonRECON++-LandSHAPELLM-13B.
c N
c
MaskType StopGrad Fine-Tune Zero-Shot 3DMM-Vet
whereN cisthenumberofanswersineachcapabilityset. Random ✓ 92.5 52.8 53.1
Tomitigateexcessivestandarddeviation,weoptforGPT- Random ✗ 93.6 53.7 52.9
4inaseriesofK scoringroundstogetroundsofoutputs Causal ✓ 95.3 49.8 50.7
withastandarddeviationbelow0.1.Thischoiceismotivated Causal ✗ 92.8 51.0 51.6
by the enhanced stability offered by GPT-4 [116], in con-
trasttoGPT-3.5[114],wherescoresacrossdifferentrounds
exhibitsignificantvariability. SHAPELLM WeusetheLLaMAmodel[150]asourLLM
backbone,withthe7Band13BVicuna-1.1[21]checkpoint
B.2.Analysis
asthedefaultsettings. Wepartitionedthepointcloudsinto
The3DMM-Vetevaluationbenchmarkconsistsof5differ- 512 patches using furthest point sampling and k-nearest
entcategoriesofquestions. InFig.12wereportthedistri- neighbors. SimilartootherMLLMs[37,93,166],weem-
butionofproblemcategories. TheknowledgeandGeneral ploya2-layerMLPwithGELU[61]astheprojector,with
21
noitubirtsiD
yrogetaCTable13.TrainingrecipesforRECON++andSHAPELLM.
RECON++ SHAPELLM
Config HyBrid/Ensembled ScanObjectNN ModelNet Cap3D LVIS/GaPartNet
optimizer AdamW AdamW AdamW AdamW AdamW
learningrate 5e-5 2e-5 1e-5 2e-3 2e-5
weightdecay 5e-2 5e-2 5e-2 - -
learningratescheduler cosine cosine cosine cosine cosine
trainingepochs 300 300 300 3 1
warmupepochs 10 10 10 0.03 0.03
batchsize 512 32 32 256 128
droppathrate 0.1 0.2 0.2 - -
numberofpoints 1024/10000 2048 1024/10000 10000 10000
numberofpointpatches 64/512 128 64/512 512 512
pointpatchsize 32 32 32 32 32
augmentation Rot&Scale&Trans Rot Scale&Trans - -
GPUdevice 8×A800 1×A800 1×A800 8×A800 8×A800
hiddenlayersizesof1,024and2,048,respectively.Notethat E.AdditionalRelatedWork
differentprojectorparametersareutilizedforabsoluteposi-
tionalencoding,local,andglobalfeatures. Throughtraining E.1.3DRepresentationLearning
the projector, multi-scale and multi-mode features of the
pointcloudaremappedintothetextspace. Afteraddingtwo Researchon3DRepresentationLearningencompassesvar-
specialtokens,thevocabularysizebecomes32,003. ious methods, including point-based [126, 127], voxel-
based [112], and multiview-based approaches [58, 143].
Point-based methods [40, 131] have gained prominence
D.Trainingdetails
inobjectclassification[151,164]duetotheirsparsityyet
geometry-informative representation. On the other hand,
RECON++ Due to the sensitivity of the Chamfer Dis-
voxel-basedmethods[30,130,176]offerdenserepresenta-
tance[41]losstoaccuracy,allexperimentswereconducted
tionandtranslationinvariance,leadingtoaremarkableper-
atFP32precisionusing8×80GA800GPUS.Westilluse
formanceinobjectdetection[23]andsegmentation[3,175].
thestrategyofcontrastwithreconstruct[129]. Tosavepa-
Theevolutionofattentionmechanisms[153]hasalsocon-
rameter tuning time and improve performance, we divide
tributedtothedevelopmentofeffectiverepresentationsfor
thetrainingprocessintotwostages: thereconstructionstage
downstreamtasks,asexemplifiedbytheemergenceof3D
based on mask modeling and the cross-modal alignment
Transformers[40,102,111]. Notably, 3Dself-supervised
stagebasedonknowledgedistillation. Fortransferlearning
representationlearninghasgarneredsignificantattentionin
classificationtasks,RECON++ispretrainedon1,024points.
recentstudies. PointContrast[165]utilizescontrastivelearn-
For zero-shot tasks and SHAPELLM tasks, RECON++ is
ingacrossdifferentviewstoacquirediscriminative3Dscene
pretrainedon10,000points. Furtherdetailsregardingthe
representations. InnovationssuchasPoint-BERT[180]and
hyperparametersettingsaredocumentedinTab.13.
Point-MAE[119]introducemaskedmodeling[31,60]pre-
SHAPELLM Allexperimentswereconductedusing8× trainingintothe3Ddomain. ACT[36]pioneerscross-modal
80GA800GPUswithaBF16datatype. Duringthemulti- geometry understanding through 2D or language founda-
modalalignmentstage, wetrainourmodelforoneepoch tionmodelssuchasCLIP[134]orBERT[31]. Following
withabatchsize256andalearningrate2e-3. Duringthein- ACT, RECON [129]furtherproposesalearningparadigm
structiontuningstage,wetrainourmodelforoneepochwith thatunifiesgenerativeandcontrastivelearning. Addition-
abatchsizeof128andalearningrate2e-5.Throughoutboth ally, leveraging foundation vision-language models like
stages, weemployflash-attention[26], theAdamW[104] CLIP [36, 134] has spurred the exploration of a new di-
optimizer,andacosinelearningratescheduler[103]. For rectioninopen-world3Drepresentationlearning. Thisline
theentiretrainingprocess,the7Band13Bmodelsrequire of work seeks to extend the applicability and adaptability
approximately10and20hours,respectively. Furtherdetails of3Drepresentationsindiverseandopen-world/vocabulary
regardinghyper-parametersaredocumentedinTab.13. scenarios[32,33,106,122,181].
22F.FutureWorks
SHAPELLM has made significant progress in advancing
3Dshapeunderstandingandembodiedperceptionthrough
MLLMs. Futureendeavorsaimtoscaleupembodiedunder-
standingtrainingusingdatasetslargerthanGAPartNet[47],
potentially leading to open-vocabulary part-level compre-
hension,including6-DoFposeestimation. Excitingly,there
isavisiontoestablishaunifiedframeworkcapableofcom-
prehending not only 3D shapes but also entire 3D scenes.
Toenhancereal-worldapplicationsonrobots,apromising
approachinvolvesaroboticsco-designthateffectivelycon-
nects3Drepresentationswithdownstreamlanguage-based
tasks[72,77]. Additionally,addressingefficiencyforreal-
time deployment is crucial, emphasizing techniques like
modelcompression[35,75,182–184].
23[SystemPrompt]
You are a helpful AI assistant.
[UserPrompt]
Now I will give you a question, the type of the question, an answer from model, and an answer from label. All you need to
do is focus on these two answers and figure out whether they are saying the same thing about the specific type of question.
Your response should only be a confidence score ranging from 0 to 100. Remember the confidence score is to evaluate how
much two answers are describing the same thing. Your response confidence score shouldfollow the scoring standard of the
prompt I gave.
Firstly I will give you several question-and-answer pairs as long as their confidence score:
question1: How many oranges will there be if 1/3 of them are removed?
question type: Knowledge
answer from model: There will be 6 left.
answer from label: As there are 9 oranges in total, there will be 6 oranges left if 1/3 of them are removed.
confidence score: 100
question2: What is this object?
question type: General Visual Recognition
answer from model: This is a bathtub
answer from label: This is a dirty bathtub.
confidence score: 80
question3: What is this object?
question type: General Visual Recognition
answer from model: This is a bottle of water
answer from label: This is a bottle of oil
confidence score: 50
question4: What is holding in this boy's right hand?
question type: Spatial Recognition
answer from model: He is holding a white cup in his right hand.
answer from label: He is holding a sword in his right hand.
confidence score: 0
Next, I will give you the elements:
question: {},
question type: {},
answer from model: {},
answer from label: {}.
Please remember, while outputting the confidence score, do not include any words, just the number.
[Example1]
Question: When grasping it, should you apply a greater or lesser force?
question type: Embodied Interaction
answer from model: When grasping it, you should apply a lesser force to avoid damaging the doughnut.
answer from label:His cloak was attached to his shoulder armor and hung on his back.
confidence score: 90
[Example2]
Question: Where is its cloak?
question type: Spatial Recognition
answer from model: The cloak is draped over its shoulders.
answer from label:A lesser force.
confidence score: 100
Figure13.GPT-4evaluationtemplateandexamplesofour3DMM-Vetbenchmark.
24[PointClouds]
[Question1 Type:Embodied Interaction]
Please give me steps to roll the dice until it hits 6.
[Answer1]
Step 1: Roll the dice.
Step 2: If the dice hits 1, 2, 3, 4, or 5, roll it again.
Step 3: If the dice hits 6, stop rolling and show the roll of the dice.
[Question2 Type:Spatial Awareness]
Change the 1 and 5 on the object, which number will be the opposite of the number 5?
[Answer2]
Change the number 1 and number 5 on this dice, number 6 will be on the opposite of number 5.
[Question3 Type:Knowledge]
At which places will this object be used?
[Answer3]
This object is a dice, so it might be used at places like bars and gambling houses.
[PointClouds]
[Question1 Type:General Visual Recognition]
What subparts are there in the scene?
[Answer1]
There is a bag of cookies, a mug of milk and a China bowl.
[Question2 Type:Embodied Interaction]
As an AI robot, please give me steps to mix the milk and cookies in a bowl.
[Answer2]
Step 1: Pour the milk into the bowl.
Step 2: Put the cookies into the bowl.
Step 3: Stir with a spoon.
[Question3 Type:Knowledge]
Describe the physical properties of the milk.
[Answer3]
The milk is a kind of liquid with a white color, whose density and boiling point is higher than water while the freezing
point is lower than water, has a mild, slightly sweet odor and taste.
[PointClouds]
[Question1 Type:Embodied Interaction]
I want to change the place of the spoon and the fork, please give me steps.
[Answer1]
Step 1: Pick up the fork and the spoon.
Step 2: Put down the spoon at the place of the fork.
Step 3: Put down the fork at the place of the spoon.
[Question2 Type:Language Generation]
What are these objects commonly used to do in usual life? Please speak in detail.
[Answer2]
Forks, spoons, and table knives are common eating utensils used in everyday life for various purposes. Forks are primarily
used for piercing and picking up solid food item. Spoons are used for scooping and conveying liquids, semi-liquids, and
foods with a sauce or broth. Table knives, also known as dinner knives, are used for cutting and slicing food on your plate.
They are typically not as sharp as kitchen knives, as their primary purpose is to assist with cutting while dining.
Figure14.AdditionalVisualizationexampleof3DMM-VetQ&Apairs.
25[PointClouds]
[Question1 Type:General Visual Recognition]
What is this object?
[Answer1]
This is an ukulele.
[Question2 Type:Knowledge]
What is the difference in shape between this object and the guitar?
[Answer2]
This is a ukulele that has 4 strings, while the guitar has 6 strings.
[Question3 Type:Knowledge]
What are the physical properties when this object meets fire?
[Answer3]
It's basically made of wood and nylon so fire would cause great damage to it.
[PointClouds]
[Question1 Type:General Visual Recognition]
Who is this anime character?
[Answer1]
This is SpongerBob.
[Question2 Type:Language Generation]
This is a classic anime character,please describe the anime it appears.
[Answer2]
SpongebobSquarepantsis an American animated comedy series set in the Pacific Ocean in a city called Bikini Bottom. The
plot content of SpongeBob Squarepantsbasically has nothing to do with Marine knowledge, and even exaggerates to be
completely inconsistent with science and common sense, such as making fire under the sea, bathing under the sea, having a lake
under the sea (Cool Lake), drowning fish in the lake (Cool Lake), and burning underwater buildings.
[Question3 Type:Language Generation]
Tell me about his best friend Patrick Star.
[Answer3]
Pink Starfish, Spongebob'sbest friend, is also one of Squidward's hate objects, wearing a pair of purple flower beach shorts, in
the series always with SpongeBob to make a lot of jokes. He has no job, is a loyal customer of the KrustyKrab, and alsolikes
to eat Haibapaste. Sometimes he works for the KrustyKrab or Sea Bully, but only as a one-day employee, and most of the
work is messed up, but the artistic talent is more than Squidward and SpongeBob Squarepants.
[PointClouds]
[Question1 Type:Embodied Interaction]
Give me several steps to take the rusty barrel away from this pack.
[Answer1]
Step 1:Clamp the rusty barrel.
Step 2:Take it down from the height.
Step 3:Trun around and take it away from the pack.
[Question2 Type:Spatial Recognition]
Where is the rusty barrel?
[Answer2]
The rusty barrel is in the top row, next to the yellow one.
[Question3 Type:Spatial Recognition]
Please describe the spatial relation of this entirety.
[Answer3]
The barrels are stacked in two layers, the bottom layer is three yellow barrels, and the top layer is a yellow barrel and a
rusted barrel in the gap between the bottom three buckets.
Figure15.AdditionalVisualizationexampleof3DMM-VetQ&Apairs.
26