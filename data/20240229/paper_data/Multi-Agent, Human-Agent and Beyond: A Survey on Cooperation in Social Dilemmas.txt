Multi-Agent, Human-Agent and Beyond: A Survey on
Cooperation in Social Dilemmas
HaoGuo1,2∗,ChunjiangMu1∗,YangChen3,ChenShen4,ShuyueHu5†,ZhenWang1†
1NorthwesternPolytechnicalUniversity,2TsinghuaUniversity,3UniversityofAuckland,4KyushuUniversity
5ShanghaiArtificialIntelligenceLaboratory
Email: ghfreezing@mail.nwpu.edu.cn,chunjiang mu@mail.nwpu.edu.cn,yang.chen@auckland.ac.nz
shen.chen.415@m.kyushu-ac.jp,hushuyue@pjlab.org.cn,w-zhen@nwpu.edu.cn
∗Equalcontributions,†Correspondingauthors
Abstract
Thestudyofcooperationwithinsocialdilemmashaslongbeenafundamentaltopicacrossvariousdis-
ciplines,includingcomputerscienceandsocialscience. RecentadvancementsinArtificialIntelligence(AI)
havesignificantlyreshapedthisfield, offeringfreshinsightsintounderstandingandenhancingcooperation.
ThissurveyexaminesthreekeyareasattheintersectionofAIandcooperationinsocialdilemmas. First,fo-
cusingonmulti-agentcooperation,wereviewtheintrinsicandexternalmotivationsthatsupportcooperation
amongrationalagents,andthemethodsemployedtodevelopeffectivestrategiesagainstdiverseopponents.
Second, looking into human-agent cooperation, we discuss the current AI algorithms for cooperating with
humansandthehumanbiasestowardsAIagents.Third,wereviewtheemergentfieldofleveragingAIagents
toenhancecooperationamonghumans. Weconcludebydiscussingfutureresearchavenues, suchasusing
largelanguagemodels,establishingunifiedtheoreticalframeworks,revisitingexistingtheoriesofhumanco-
operation,andexploringmultiplereal-worldapplications.
1 Introduction
Socialdilemmas(SDs,e.g.,prisoner’sdilemma),spanningvariousdomainsincludingenvironmentalpollution,
publichealthcrises, andresourcemanagement, presentafundamentalconflictbetweenpersonalinterestsand
the common good [Nowak, 2006]. While cooperation is beneficial for the collective, individuals are tempted
to exploit or free-ride others’ efforts, potentially leading to a tragedy of the commons. Historically rooted in
thestudyofbiologicalaltruism[Smith,1982],thetraditionalresearchoncooperationinSDshasunveiledthe
pivotalrolesofreciprocityandsocialpreferencesinfosteringcooperativebehaviorsinhumansocieties[Fehret
al.,2002;RandandNowak,2013]. Recently,propelledbyadvancesinartificialintelligence(AI),thisfieldhas
beenundergoingaprofoundtransformation—asAIagentsnowincreasinglyrepresentandengagewithhumans,
our understanding of how cooperation emerges, evolves, and sustains in SDs is being significantly reshaped.
This is particularly evident in two lines of research: multi-agent cooperation, where AI agents interact with
eachotherinSDs,andhuman-agentcooperation,whichexaminestheintricaciesofhumaninteractionswithAI
agentsinSDs.
AfundamentalassumptionofAIagentsistheirrationality. Thisraisesacriticalquestion: howcanrational
agentsbesteeredtowardseffectivecooperation,requiringthemtoovercomethelureofexploitingorfree-riding
others? This question has received much interest in recent studies of multi-agent cooperation, particularly
in the context of sequential social dilemmas (SSDs) [Leibo et al., 2017]. As opposed to normal-form SDs,
SSDs are characterized by stochastic environments as well as larger state and policy spaces. It is shown that
embedding human-like motives, such as fairness [Hughes et al., 2018] and social preferences [McKee et al.,
2020], into agents’ rewards can promote cooperative behavior. In addition, mechanisms, like peer rewarding
[Wang et al., 2018] and formal contracts [Christoffersen et al., 2023], have also shown potential in fostering
cooperationamongagents. Ontheotherhand, itisshownthatagentscansafeguardthemselvesagainstother
agents’ exploitation by pre-training a suite of policies and then choosing them adaptively in real-time [Lerer
andPeysakhovich,2017],orbyinfluencingthefuturepoliciesoftheiropponents[Foersteretal.,2018].
1
4202
beF
72
]IA.sc[
1v07271.2042:viXraFigure1:Understandingcooperationwithinmulti-agent,human-agentsystems,andbeyond.(A)Normal-formsocialdilem-
masandsequentialsocialdilemmas. (B)Twoapproachestosolvingsequentialsocialdilemmasinmulti-agentsystems: i)
promoting cooperation among agents through shaping their intrinsic and external motivations; ii) devising and selecting
strategiesinresponsetodiverseopponents. (C)Fourperspectivesofstudyingcooperationinhuman-agenthybridsystems:
i)designingalgorithmsforcooperatingwithhumans;ii)identifyingandmitigatinghumanbiasesinhuman-agentcoopera-
tion;iii)scaffoldingcooperationinhuman-humaninteractions,e.g.,byengineeringtheinteractionstructure;iv)delegating
humandecisionmakingtoagents.
In addition to multi-agent cooperation, human-agent cooperation is equally important, as interactions be-
tweenhumansandagentsarebecomingubiquitous. Thisfieldiscenteredaroundtwokeyquestions. Thefirst
concernsthedesignofAIalgorithms: howcanagentscooperatewithhumansatalevelcomparabletohuman-
humancooperation? Recentstudieshaveshownthatcombiningexpertalgorithmsandreinforcementlearning
canleadtohuman-levelcooperationinSDsinvolvingbothhumansandagents[Crandalletal.,2018].Moreover,
simplerstrategies,suchastheextortionandgenerousstrategies,arealsoinstrumentalinenhancinghuman-agent
cooperation[Hilbeetal.,2014]. Thesecondquestiondelvesintothesocio-cognitiveaspect: inwhatwaysdo
humanperceptionsandreactionsdifferwhendealingwithAIagentscomparedtootherhumans? Interestingly,
behavioralevidencehasrevealedthathumansarepronetocooperatelessandevenexploitAIagentsmorewhen
they are aware of the agents’ non-human nature [Ishowo-Oloko et al., 2019]. To mitigate such human bias
towardsagents,oneapproachistoembedculturallyrelevantsignalsandemotionallyexpressivecharacteristics
inAIagents[deMeloandTerada,2019].
Beyondmulti-agentandhuman-agentcooperation,emergingresearchindicatesthatAIcanenhancehuman-
human cooperation in SDs, consequently, in turn, enriching the long-standing study of cooperative behaviors
in human societies. Specifically, AI agents have been found to promote cooperation among humans by pro-
vidingrecommendationsonpartnerselection[McKeeetal.,2023]. Moreover,analysesbasedonevolutionary
game theory suggest that human-AI hybrid systems can surpass pure human populations in achieving higher
cooperationlevels[Guoetal.,2023].
Each aforementioned field—multi-agent cooperation, human-agent cooperation, as well as the emergent
fieldofleveragingAIagentstopromotehuman-humancooperation—hasattractedsignificantinterestthough,
there lacks a comprehensive review that integrates insights from these fields. Existing reviews have touched
onlearninginSSDs,butthosereviewsprimarilyemphasizemulti-agentreinforcementlearning(MARL),with
SSDsasspecializedscenarioswithinmixed-motiveMARL [e.g.,Zhangetal.,2021]. Ontheotherhand,nu-
merousreviewshaveexaminedcooperativebehaviorsbutpredominantlywithinthecontextofhumansocieties,
andarerestrictedtonormal-formSDs [e.g.,RandandNowak,2013;Percetal.,2017]. Inlightoftheseobser-
vations, thissurveyaimstoprovideacomprehensiveunderstandingthatunifiesthesefields, andencapsulates
thediversefacetsofcooperationacrosstheseinterconnectedfields,asillustratedinFigure1. Thiscannotonly
reflectthecurrentstateofresearchattheintersectionofAIandcooperationinSDs,butalsoilluminatepotential
paths for future investigations, particularly in the interplay of multi-agent, human-agent, and human-human
cooperation.
Theremainderofthissurveyisorganizedasfollows. Webeginbybrieflyintroducingthenormal-formSDs
2Table1:PayoffmatricesofPrisoner’sDilemma(PD),StagHunt(SH)andHawkDove(HD).Theelementsinthematrixare
thepayoffsfortherowplayer,andthepayoffsforthecolumnplayerareatthetransposedmirrorpositionoftherowplayer.
PD C D SH C D HD C D
C 3 0 C 5 0 C 3 1
D 5 1 D 3 1 D 5 0
and SSDs. We then delve into multi-agent cooperation, summarizing the mechanisms that support agents to
achieve mutual cooperation and the methods for developing strategies against diverse opponents. The focus
thenshiftstohuman-agentcooperation,discussingthecurrentAIalgorithmsforcooperatingwithhumansand
the human biases towards AI agents. Subsequently, we review how the advances of AI agents can inspire
human-humancooperation. Finally,weconcludewithdiscussionsonfutureresearchdirections: (i)enhancing
the study of cooperation with large language models, (ii) establishing theoretical frameworks for cooperation
inSSDsandhuman-agentcooperation,(iii)applicationstomultiplereal-worldscenarios,(iv)bridginghuman-
agentcooperationandSSDs,and(v)revisitingtheexistingtheoryofcooperationinhumansocieties.
2 Preliminaries
Thissectiondefinesnormal-formandsequentialsocialdilemmas,uponwhichthemulti-agentandhuman-agent
cooperationmethodssurveyedinthispaperarebuilt.
2.1 Normal-FormSocialDilemmas
Social dilemmas (SDs) involve a conflict between immediate self-interest and longer-term collective interests
[Van Lange et al., 2013]. A normal-form SD is a general-sum normal-form game, where two players must
choose one of two actions simultaneously: cooperation (C) or defection (D). Mutual cooperation yields the
rewardRforboth,mutualdefectionleadstothepunishmentpayoffP forboth,anddifferentchoicesgivethe
cooperatorthesucker’spayoffSandthedefectorthetemptationT.InanSD,thesepayoffssatisfythefollowing
relationships: R > P (mutualcooperationvalueshigherthanmutualdefection), R > S (mutualcooperation
values higher than unilateral cooperation), 2R > T +S (social welfare of mutual cooperation values higher
than that of unilateral cooperation), and either or both of the following two: T > R (unilateral defection for
greedvalueshigherthanmutualcooperation)orP >S (unilateraldefectionforfearvalueshigherthanmutual
cooperation). Table1showsthreenormal-formSDs.
2.2 SequentialSocialDilemmas
Sequential Social Dilemmas (SSDs) are social dilemmas happening over a sequence of time steps, defined
on stochastic games that typically have larger state spaces and policy spaces than normal-form games and
additionally consider the states of the decision-making environment. Formally, an n-agent SSD is a tuple
(cid:70)
⟨M,Π = Π Π ⟩,whereMisastochasticgame(alsoknownasMarkovgame),Π isasetofcooperative
c d c
policies,andΠ isasetofdefectivepolicies.Astochasticgameisarepeatedgameassociatedwithprobabilistic
d
statetransitions,whereeachagentchoosesactionsusingapolicyandallagentstakeactionssimultaneouslyat
thecurrentenvironmentalstate;then,theenvironmentwilltransfertoanewstate,andeachagentwillreceivea
rewardfromtheenvironmentasaresultofagents’actionsandthestatetransition. ConsiderasetN ofagents
c
whereeachagentj ∈ N adoptsapolicyπj ∈ Π ,andasetN ofagentswhereeachagentk ∈ N adoptsa
c c d d
policyπk ∈Π ,suchthatN ∪N =N,N ∩N =∅.Denotetheexpectedrewardofanyagenti∈N byRi(l),
d c d c d
withthenumberofagentsadoptingcooperativepoliciesbeingl = |N |. ForaSSD,thefollowingconditions
c
hold: Rj(|N|)>Rk(0),∀j ∈N ∀k ∈N (cooperatorsunderfullcooperationgainmorethandefectorsunder
c d
fulldefection);Rj(|N|)>Rj(1),∀j ∈N (cooperatorsunderfullcooperationgainmorethanonecooperator
c
under the others’ defection); and either or both of the following two: Rk(l) > Rj(l),∀k ∈N ∀j ∈N if
d c
|N | > |N | (defectors gain more than cooperators when the number of cooperators is relatively large) or
c d
Rk(l) > Rj(l),∀k ∈N ∀j ∈N if|N | < |N |(defectorsgainmorethancooperatorswhenthenumberof
d c c d
cooperatorsisrelativelysmall).
Compared to the highly abstract normal-form SDs, SSDs have some characteristics that are more aligned
withreal-worldscenarios,whicharewell-summarizedbyLeiboetal.[2017]: First,real-worldsocialdilemmas
are temporally extended. Then, from an individual’s policy perspective, cooperation and defection are labels
3thatapplytopoliciesimplementingindividualstrategicdecisions,andone’scooperativenessmaybeagraded
quantity. Additionally,fromtheperspectiveofgameparticipation,decisionstocooperateordefectoccuronly
quasi-simultaneously, andsometimesdecisionsmustbemadewhenonlyhavingpartialinformationaboutthe
stateoftheworldandotherplayers’activities.
3 Multi-Agent Cooperation in SSDs
Whilemulti-agentcooperationinsequentialdecision-makinghasbeenalong-standingtopicinMARLthattakes
stochasticgamesasthestandardmodel,thissurveyfeaturesanarrowed-downfocusonmulti-agentcooperation
in SSDs. For multi-agent cooperation in stochastic games using MARL, we refer readers to [Busoniu et al.,
2008]foracomprehensivesurvey.
Leiboetal.[2017]pioneeredSSDs,definingtheframeworkandproposingamodelingapproachconnecting
two-agentSSDsandnormal-formSDsthroughempiricalgametheoreticanalysis(EGTA).Cooperativeandde-
fectivepoliciesaretrainedusingindependentdeepreinforcementlearningundervariousenvironmentalsettings.
Thesepoliciesarethenevaluatedtoconstructempiricalgamematrices. SSDsdifferfromnormal-formSDsas
agentsinSSDsmustlearnboththepolicyanditseffectiveexecution,whilenormal-formSDsinvolveexecuting
predefinedactionswithoutlearning.
The challenges in solving SSDs include developing usable policies in stochastic games with large state
andpolicyspaces,andaddressingsub-optimalequilibriathatindependentlymaximizingtheindividualinterest
of each agent can lead to counterproductive outcomes in SSD. Fortunately, with the powerful generalization
abilityofdeepneuralnetworks,MARLoffersapromisingsolution[Hernandez-Lealetal.,2019]. Thissection
reviewspastworksonsolvingSSDswithMARLontwolinesasshowninFigure1(B).Onelineofresearchis
concernedwithhowtopromotemulti-agentcooperationforhighersocialwelfarebydesigningsuitablereward
functionsthroughshapingintrinsicandexternalmotivations. Theotherlinefocusesonfindinggoodsolutions
toplayagainstopponentswithuncertaindiversepolicies,inordertoexploitthemoravoidbeingexploitedby
them.
3.1 ShapingIntrinsicandExternalMotivations
In SSDs, achieving cooperation can lead to higher collective long-term returns for all agents than each agent
actingselfishly. Inordertoencourageagentstolearncooperativepolicies,itisusuallynecessarytointroduce
additional motivation rewards in addition to environmental rewards. Most works introduce the motivation by
shapingagenti’srewardrtotal astheweightedsumoftheenvironmentalrewardrenv andthemotivationreward
i i
rmotlikethefollowingform:
i
rtotal =αrenv+βrmot,
i i i
whereαandβareweights. Accordingtothesourcesofthemotivationreward,wenowreviewthemechanisms
forshapingmotivationsinSSDsfromtwoperspectives: theintrinsicmotivationandtheexternalmotivation.
3.1.1 IntrinsicMotivation
Werefertomotivationsthatagentsnotonlycareaboutone’sownbutalsotheothers’rewardastheintrinsicmo-
tivation. TheideaoftheintrinsicmotivationinSSDsisrootedinbehavioraleconomicsandsocialpsychology,
inwhichresearchhasfoundthatindividualshavetheintrinsicmotivationtodocertainthingsfortheirinherent
satisfactions rather than for some specific consequence [Ryan and Deci, 2000], e.g., inequity aversion, social
valueorientation,altruism,socialinfluence,andreputation.
Inequity aversion. Hughes et al. [2018] pioneer the incorporation of intrinsic motivation within SSDs, and
defineitthroughthelensofinequalityaversion. Byinequalityaversion,agentspreferfairnessandresistinci-
dentalinequalities. ToprovideaglimpseofthedefinitionofintrinsicmotivationinSSDs,wepresentHughes’s
formalizationbasedoninequalityaversionasfollows:
αi (cid:88)
rmot =− max{ej(oj,aj)−ei(oi,ai),0}
i N −1 j̸=i t t t t t t
βi (cid:88)
− max{ei(oi,ai)−ej(oj,aj),0},
N −1 j̸=i t t t t t t
4where oi is agent i’s observation of the global state s , and the agent i’s temporal smoothed rewards ei are
t t t
updated by ei(oi,ai) = γλei (oi,ai) + renv(oi,ai). Intuitively, the first term characterizes the agent i’s
t t t t−1 t t i t t
rewardlosswhenotheragentsachieverewardsgreaterthanagenti’sownandthusrepresentsthedisadvantage
inequityaversion,andthesecondtermcharacterizestherewardlosswhenagentiperformingbetterthanothers
and thus represents the advantage inequity aversion. αi and βi control agent i’s aversion to disadvantageous
inequityandadvantageousinequity.Interestingly,theauthorsfindthatadvantageousinequityaversionpromotes
cooperation in public goods dilemmas by providing an unambiguous feedback signal, while disadvantageous
inequityaversionpromotescooperationincommondilemmasbyprovidinganoisysignalforagents’sustainable
behaviors.
Socialvalueorientation. SocialValueOrientation(SVO)measuresthemagnitudeoftheconcernpeoplehave
for others. Schwarting et al. [2019] highlight the nature of SDs in autonomous driving and enhance the pre-
dictionofotherdrivers’behaviorsbyestimatingtheirSVOs. McKeeetal.[2020]useSVOtocharacterizethe
social preferences of agents in SSDs. The relationship between the agent i’s own reward r and the average
i
rewardofotheragentsr ischaracterizedbyarewardangleθ(R⃗)=arctan(r /r ). Bydefiningthemotiva-
−i −i i
tionrewardrmotas−ω·|θSVO−θ(R⃗)|,theagentswithspecificSVOgivenbyθSVOcanbetrained. Inaddition,
i
theyalsofindthatpopulationstrainedwithheterogeneousSVOdevelopmoreuniversalpolicieswithhigherco-
operativelevelsthanhomogeneouspopulations. Furthermore,Madhushanietal.[2023]findthatlearningbest
responsestodiversepoliciestrainedwithheterogeneousSVOleadstobetterzero-shotgeneralizationinSSDs.
Li et al. [2023] propose a learning framework based on SVO called RESVO, which emerges stable roles in
populationsandefficientlysolvesSSDsthroughthedivisionoflabor. Inaddition,similartoSVO,Baker[2020]
proposeanenvironmentaugmentationcalledRandomizedUncertainSocialPreferences(RUSP)tocharacterize
socialpreferences,andfindemergentdirectreciprocity,indirectreciprocityandreputation,andteamformation
whentrainingagentswithRUSP.
Altruism. Altruismamountstotakingcostlyactionsthatmostlybenefitotherindividuals. InSSDs,altruistic
agentsprioritizethecollectiverewardoftheentiregroupovertheirownindividualrewards.Wangetal.[2019a]
introducethealtruisticintrinsicmotivationparameterizedbyasharedrewardnetwork,whichisupdatedslowly
tosimulatenaturalselectionforahighercollectiverewardofallagents. Theiraltruisticevolutionaryparadigm
solvesdifficultSSDswithouthandcrafting. Someworksassumethatindividualaltruismonlyappliestocertain
members within a group, which divides the entire group into different teams to promote cooperation more
effectively. Radkeetal.[2023]providestheoreticalgroundworkastowhyandunderwhichconditionssmaller
teamsoutperformlargerteams.
Socialinfluence. Manyexperimentalstudieshaverevealedthatindividualbehaviorissubjecttosocialinflu-
ences. Jaquesetal.[2019]considerone’ssocialinfluenceastheintrinsicmotivationinSSDs,whichisdefined
astowhatextenttheotheragentschangetheiractionsbecauseoftheinfluencer’saction.High-levelcooperation
inSSDswithinthreesocialinfluencemodelsdemonstratesthatsocialinfluenceleadstoenhancedcoordination
andcommunicationamongagents.
Reputation. Reputationservesasacrucialreciprocalmechanismforfosteringcooperationwithinlarge-scale
human populations. McKee et al. [2021] model one’s reputation based on the matching level of its own and
groupcontributionsinSSDs. TheauthorsconductMARLsimulationsandhumanbehaviorexperimentsunder
identifiable (contributions of each group member can be perfectly monitored) and anonymous (contributions
cannotbeperfectlymonitored)conditions. Resultsofseveralmetricsofagents’behaviordemonstratethatrep-
utationpromotescooperationamongagentsbydevelopinganon-territorial, turn-takingstrategytocoordinate
collectiveaction.
3.1.2 ExternalMotivation
Incontrasttointrinsicmotivation,externalmotivationisaconstructthatpertainswheneveranactivityisdonein
ordertoattainsomeseparableoutcome[Chentanezetal.,2004]. Inthissurvey,werefertomotivationdirectly
fromotheragentsasexternalmotivation.
5Peer rewarding. Peer rewarding is a type of external motivation, allowing agents’ reward functions to be
directly modified by others. Technically, peer rewarding improves reward sparsity and encourages agents to
explorecooperativebehaviorsinSSDs. LupuandPrecup[2020]introducegiftingoptionsintoagents’action
spaceandfindthatzero-sumgiftingsolvesthetragedyofthecommonsmosteffectively. Further, somework
studylearnablepeerrewarding.Yangetal.[2020]proposetheLearningtoIncentivizeOthers(LIO)framework,
whereeachagentlearnsitsownincentivefunctionbyexplicitlyaccountingforitsimpactonrecipients’behavior
directlyanditsownextrinsicobjectiveindirectly.TheyfindthatLIOagentsfindanear-optimaldivisionoflabor
inSSDs.Inaddition,someeconomicconcepts,suchasmarketparticipation[Schmidetal.,2022],whereagents
learntoparticipateinothers’rewardsbyacquiringshares,andtaxation[Huaetal.,2023],whereacentralized
agentlearnsthePigoviantax/allowancetomaximizesthesocialwelfare,havealsobeenintroducedintoSSDs
tomodelpeerrewarding.
Agreements. Humansocietal structuresareunder-girded byamultitude ofagreements, which aredesigned
toincentivizecompliancethroughrewardsforthosewhoconformtotheirmandates,andtoenforcediscipline
through penalties for those who violate the same. Can these agreements also promote cooperation among AI
agents in SSDs? Vinitsky et al. [2023] construct an agent architecture called Classifier Norm Model (CNM)
thatcanusepublicsanctionstosparktheemergenceofsocialnormsinSSDs.TheCNMagentslearntoclassify
others’transgressionandenforcesocialnormsfromexperience,andtheyconvergeonbeneficialequilibriaand
arebetteratresolvingfree-riderproblems. Christoffersenetal.[2023]considerthecontractingaugmentation
among agents, where agents voluntarily agree to binding state-dependent transfers of reward. Gemp et al.
[2022]constructD3Cagentstominimizethepriceofanarchy,agapbetweenthewelfarethatcanbeachieved
through perfect coordination against that achieved by self-interested agents at Nash equilibria. Eccles et al.
[2019]constructonline-learningreciprocalagents,whotrytomeasureandmatchthelevelofsocialityofothers
andinfluencenaiveagentstopromotetheircooperation.
3.2 PlayingagainstDiverseOpponents
TheotherparadigmofsolutionstoSSDsisplayingwithdiverseopponents.Onemayfeelthisresemblessolving
azero-sumgameatthefirstglance, butthedifferenceisthatinmixed-motivationSSDs, directlymaximizing
one’sownrewardsmayhaveoppositeeffects. Infact,itismorereasonabletodecideone’sownchoicebasedon
theopponent’sbehaviorstyle,asisverifiedinthefamousstrategiesTit-for-Tat(TFT)orWin-Stay,Lose-Shift
(WSLS) in iterated SDs. Based on the principles of constructing strategies, we divide the existing work into
twocategories: two-stageadaptivepolicyselectionandonlinelearningopponentshaping.
Adaptive policy selection. This type of method typically involves two stages: first pre-training different
stylesofpolicies;thenadaptivelyselectingbydesignedstrategiesinreal-timegames. LererandPeysakhovich
[2017] construct an approximate Markov Tit-for-Tat (amTFT) agent. They train cooperative and defective
policies by reward shaping, and design a TFT-like strategy manually to decide when to cooperate or defect.
Further,Wangetal.[2019b]proposethecooperationdegreedetectionnetwork,whichisaclassifiertrainedon
trajectories generated by pre-trained policies with various degrees of cooperation. Experimental results show
thatthestrategycanavoidbeingexploitedbyexploitativeopponentsandachievecooperationwithcooperative
opponents. Gle´auetal.[2022]introduceCircularSSDandconstructGraph-basedTFTforasymmetricgame
scenarios. Moreover,O’CallaghanandMannion[2021]proposeamethodoftrainingagentswithtunablelevels
ofcooperationinSSDs,basedonmulti-objectivereinforcementlearning.
Opponent shaping. An opponent shaping agent tries to influence the future strategies of its learning oppo-
nentsonlinebyofferingfeedbackontheirbehavior. Foersteretal.[2018]proposetheLearningwithOpponent
Learning Awareness (LOLA) rule, which takes into account the learning dynamics of the opponent. In de-
tail, theLOLAlearningruleincludesanadditionaltermthataccountsfortheimpactofoneagent’spolicyon
theanticipatedparameterupdateofitsopponent. TheauthorsdemonstratethattheLOLAmethodcanachieve
reciprocity-basedcooperationunderself-playsettingsbyfindingTFT-likestrategiesandalsoexploitothernaive
learningalgorithms. Subsequently,aseriesofimprovementstoLOLAareproposed. Letcheretal.[2018]pro-
pose Stable Opponent Shaping that interpolates robustly between LOLA and LookAhead [Zhang and Lesser,
2010]topreventLOLAagentsfromarrogantbehaviorandconvergingtonon-fixedpoints. Willietal.[2022]
propose Consistent LOLA to address the consistency problem of LOLA. And Zhao et al. [2022] introduce
ProximalLOLAwhichguaranteesbehaviorallyequivalentpoliciesresultinbehaviorallyequivalentupdatesto
6addressLOLA’ssensitivitytopolicyparameterization.Furthermore,inordertoaddressthelimitationofexplicit
gradientsrequiredbytheLOLAseriesalgorithms,Luetal.[2022]proposeModel-FreeOpponentShaping(M-
FOS)thatlearnsinameta-gameinwhicheachmeta-stepisanepisodeoftheunderlying(“inner”)game. The
meta-stateconsistsoftheinnerpolicies,andthemeta-policyproducesanewinnerpolicytobeusedinthenext
episode. Inadditiontopolicy-basedmethod,avalue-basedopponentshapingmethodcalledMeta-ValueLearn-
ing is proposed by Cooijmans et al. [2023], which can be seen as the value-based complement to the policy
gradientofM-FOS.
4 Human-Agent Cooperation in SDs
Thissectionexploreshuman-agentcooperationinSDsfromtwoperspectives: (1)developingalgorithmsthat
effectively enhance cooperative behavior; (2) the biases humans exhibit in their decision-making processes
wheninteractingwithbothhumansandagents.
4.1 DesigningAlgorithmsforCooperationwithHumans
FromS++toS#. SuccessfulAIagentsrequirethreekeyproperties: generalityacrossvariousgames,adapt-
abilitytounfamiliarcounterparts,andswiftlearningindecision-making[Crandalletal.,2018]. TheS++and
S# family addresses these needs, focusing on iterated interactions where individuals repeatedly engage in the
samegamestructureandmakedecisionsbasedonpastexperiences. [Crandall,2014;Oudahetal.,2018].
S++ [Crandall, 2014], an algorithm originally designed for iterated normal-form games, stands out for its
twokeyfeatures. First,itincludesarangeofexperts,suchasfollowersandmodel-basedreinforcementlearn-
ers,derivedfromthedescriptionofthegameenvironment. Second,itemploysanexpert-selectionmechanism,
whichdeterminestheappropriateexperttobeusedineachgameround. S++outperformsotheralgorithmsin
interactions, including with model-based (mode-free) reinforcement learning algorithms, and exhibits as pro-
ficient at establishing cooperative relationships with humans as humans are [Crandall et al., 2018]. However,
high-level cooperation with humans often falls short, leading to the introduction of a communication mecha-
nism.
S#,anadvancementofS++,incorporatesacommunicationframeworkenablingagentstoengageincheap
talk,enhancingcooperationinsimulationsandhuman-agentexperiments. Theresultsdemonstrateitsabilityto
cooperatewithhumansandotheralgorithmsatlevelsthatrivalhumancooperation. Subsequently,Oudahetal.
[2018] further develops the S++ and S# algorithms, introducing varied signaling strategies based on different
philosophical approaches. Their experiments show that agents combining a behavioral strategy that learns
quicklyandeffectivelywithasignalingstrategygroundedinCarnegie’sPrinciplesandexplainableAIaremost
effectiveinwinningfriendsandinfluencingpeople.
Avoid exploitation by self-seeking individuals. Mitigating the obstacle of cooperation requires handling
humanselfishness.Zero-determinantstrategybecomesaviableoption.Itincludesextortionstrategies,ensuring
a player consistently outperforms its counterpart by a fixed percentage, and generosity strategies, aiming to
incentivizecooperationfromthecounterpart. Hilbeetal.[2014]observethatalthoughextortionstrategiesdo
prevailovertheirhumancounterparts,retaliationoftenfollows,leadingtodefection. Consequently,generosity
proves more profitable and effective. In contrast, extortion strategies excel in longer interactions, especially
when humans are aware they interact with a computerized counterpart [Wang et al., 2016]. In a safe control
context, Zhang et al. [2023b] propose an algorithm that can prevent AI agents from being exploited by self-
seekinghumansandseekstoachieveamoreoptimalbalancebetweensafetyandperformanceinhuman-agent
interactions.
Experiment-driven insights. Designing agents that promote cooperation can be inspired by experimental
findings.Santosetal.[2020]initiatehuman-agentexperimentsandfindthathumansprefercooperativepartners
after falling into collective failures, without expressing a significant preference for partners after collective
successes. Expanding on this insight, they develop an evolutionary game theoretical model and validate the
effectivenessofthepartnerselectionrule.Anonlinehuman-agentstudyreinforcesthesefindingsandhighlights
thesignificantinfluenceofoutcome-basedstrategiesinhuman-agentinteractions.
74.2 RevealingHumanBiastowardsAgents
Bias caused by the nature of counterparts. In human-agent systems, revealing the true nature of agents
can influence human behavior. Individuals tend to cooperate more with humans than agents [Karpus et al.,
2021]. Ishowo-Oloko et al. [2019] conduct an iterated PD experiment where humans are either accurately
informedaboutthenatureoftheircounterparts(humansoragents)orgivenfalseinformation. Employingthe
S++ algorithm, agents are more successful in eliciting human cooperation when their non-human identity is
undisclosed. Yet, this advantage diminishes once their non-human status is revealed. The bias against agents
persistsandcannotbemitigatedovertime,evenafterhumansrealizethatagentsaremorepowerfulinpromoting
human cooperation. Moreover, even when humans anticipate their counterparts to cooperate, they are more
likelytoexploitanagent’sbenevolencethanthatofahuman[Karpusetal.,2021]. Suchdistinctionsarealso
prevalent in emotional responses, particularly those related to feelings of guilt and envy towards agents and
humans. Empiricalresultsindicatethathumansexhibitsimilarlevelsofenvywheninteractingwithbothagents
andhumans,butreportsignificantlylessguiltininteractionswithagents[Meloetal.,2016].
Mitigatinghumanbias. Culturalcues,emotion,andverbalcommunicationcanreducehumanbiasesininter-
actionswithagents. deMeloandTerada[2019]organizehuman-agentexperimentswhereinhumansparticipate
initeratedPDwithagentsfeaturingvirtualfacesandemotionalexpressions. Theresultsindicatethatcultural
and emotional features can lessen biases and promote more effective and cooperative interactions. Maggioni
andRossignoli[2023]furtherdemonstratethatverbalcommunicationinhuman-agentinteractionsintheiter-
ated PD can encourage cooperative strategies, somewhat alleviating biases against agent counterparts. These
findings underscore the importance of nuanced communication in fostering cooperation between humans and
agents.
5 Inspirations on Human-Human Cooperation
Inhuman-agentsystems,agentsscaffoldhuman-humancooperationbyassumingvariousroles. Theseinclude
actingasplannerstostructurenetworkinteractions,operatingasindependentdecision-makerstoaffectpopula-
tioncomposition,andmakingdecisionsonbehalfofhumans.
Engineeringnetworkstructure. SocialnetworksplayakeyroleinovercomingSDs. ShiradoandChristakis
[2020] utilize intervention AI agents to reshape local social connections between humans. The results reveal
asignificantincreaseinhuman-humancooperationwhenagentsemployingdisengagedintervention(cuttinga
defective neighbor) are introduced. Moreover, even a single agent implementing a mixed strategy (including
engaged, disengaged, and self-rewiring intervention) significantly alters social dynamics and improves coop-
eration,fosteringthedevelopmentofcooperativeclusters. Subsequently,McKeeetal.[2023]explorehuman-
human cooperation using deep reinforcement learning in network games. They train an agent as a GraphNet
planner,responsibleforguidinghumansincreatingorbreakinginteractionlinks. Thisplannerconsistentlyrec-
ommends building links between cooperators and discourages new links with defectors. Notably, the planner
adoptsaconciliatorystrategytoaddressdefectors, initiallyestablishingacertainnumberofcooperate–defect
linksandprogressivelysuggestingtheseveranceoftheselinksovertime.
Influencingevolutionarydynamics. Theevolutionofcooperationissignificantlyinfluencedbythebehav-
ior, proportion, and spatial distribution of the agents. Terrucha et al. [2022] utilize evolutionary game theory
to model the interactions and strategies of adaptive (human-like) and fixed-behavior agents in collective risk
dilemmagames. Thefindingssuggestthatthepresenceandbehaviorofagentssignificantlyaffecthumanco-
operation levels, with humans adjusting their strategies to compensate for the agents’ actions. Motivated by
this,Guoetal.[2023]examinetheinfluenceofagentcompositiononcooperationwithinhuman-agentsystems.
Theyfindthataminoritypresenceofagentscanboostfullyhuman-humancooperation. However,whenagents
formthemajority,thismayleadtoacollapseinhumancooperation. Additionally,agentswithapurelyconstant
strategycaneffectivelyencouragehumancooperation(e.g., loneragents[Sharmaetal.,2023]). Ontheother
hand,insituationswherehumanslacktransparencyregardingtheactionsofagents,Hanetal.[2021]underscore
theimportanceofconsideringtheopportunitycostwhenadoptingareciprocalstrategy. Theydevelopatrust-
basedstrategythatincorporatesprobabilisticcheckstominimizepotentialopportunitycosts,demonstratingits
effectivenessinmaintainingcooperation.
8Making decisions on behalf of humans Humans have the capacity to entrust decision-making authority to
agents. Particularly, this mode of interaction can change the way humans solve these SDs. de Melo et al.
[2019] find that humans tend to exhibit higher cooperation when decisions are agent-driven rather than made
personally. The rationale is that programming encourages individuals to consider long-term interests while
diminishingtheinfluenceofimmediateshort-termself-interest,therebyfosteringgreatercooperativebehavior.
Ferna´ndez Domingos et al. [2022] also find that cooperation and group success increase when participants
delegatetheiractionstoagents,allowingittoplayontheirbehalfinacollectiveriskdilemma.
6 Directions of Future Research
WhilefruitfuleffortshavebeenputintotheinvestigationofcooperationinSDs,severalaspectshaveyettobe
exploredinthedomain,whicharesummarizedbelow.
(1) Enhancing cooperation in multi-agent and human-agent settings with large language models (LLMs),
knownforadvancedreasoning,planningandabstractingcapabilities[Parketal.,2023].LLMscanempowerthe
studyofcooperationinSDsintwoways. Firstly,theycanserveaspriorknowledgeforshapingagentrewards,
drawinginspirationfromtheirrecentusein(multi-agent)reinforcementlearning[HuandSadigh,2023]. The
secondusageistoemployLLMsasthebackboneofagentarchitecturesforgenerativeagentsinSDs. Unlike
existingworkonfullycooperativesettingsusingLLMs[Zhangetal.,2023a],creatinggenerativeagentsthatcan
achieve cooperation in SDs involves additional challenges, such as building reputation, identifying deception
andavoidingexploitationfromothers,aswellasnudgingotherstocooperatethroughpotentialpunishment.
(2) Establishing a theoretically guaranteed framework for fostering and sustaining cooperation in SDs.
WhilemethodssurveyedinthispapershowempiricalpromiseinencouragingcooperationinSDs,noneoffers
theoretical guarantees. A theoretical framework is envisioned to bridge past and future SD studies: precisely
explaining existing empirical findings and guiding the design of more effective methods. It is crucial to note
thatconditionsformaintainingcooperationdifferbetweennormal-formSDsandSSDs[Kleshninaetal.,2023],
necessitating a separate consideration in constructing the theoretical framework. Moreover, for human-agent
hybridsystems,existingstudiesaremostlyempirical;thoseexceptionsthatleverageevolutionarygametheory
cannotcapturehumanbiasestowardsAIagents,acriticalaspectofhuman-agenthybridsystems,andthusfail
toprovideanaccuratepredictionorexplanationoftheevolutionofcooperationinthesesystems.
(3) Further employing SDs to model real-world decision-making scenarios. Large-scale simulation sys-
tems on economics, sociology and ecology with diverse agents, e.g., the board game Diplomacy [Krama´r et
al., 2022], offer insights into the mechanism of system evolution. These low-cost, realistic simulations better
inform policymakers and even inspire solutions to relief SDs. As AI agents are integrated with humans, they
provideconveniencewhilstintroducepossiblynegativesocialinfluencetothishuman-agenthybridsystem. A
typicalreal-worldscenarioisautonomousdriving,whereAIagentsfacedilemmaslikechoosingbetweenspeed
andsafetyatintersectionsandmustnavigateundesirableselfishbehaviorfromhumandrivers. Addressingcol-
lisionrisksamongautonomousandhuman-operatedvehiclesnecessitatesdesigningalgorithmsforlarge-scale
human-agenthybridsystems. SuchAIagentsmustadeptlyrespondtohumanbehaviorsandbiases, underlin-
ingtheimportanceofdevelopinghuman-agenthybridsystemsforrealisticscenariostoenhancehuman-agent
cooperationandtacklekeycooperativechallengesinSDs.
(4)Bridginghuman-agentcooperationandSSDs. Comparedtothefruitfulliteratureonmulti-agentcoop-
erationinSSDs,theinvestigationofhuman-agentinSSDsisinsufficientasofthissurvey,thoughwithonlyfew
exceptions[Crandalletal.,2018]. InSSDs,theprocessofdecision-makingisextendedoveraseriesofsteps,
consequently leading to human behavioral patterns that often diverge from those observed in the singe-stage
settings(e.g.,normal-formSDs)[Simsetal.,2013]. Thisbringsnewchallengestohuman-agentcooperation:
It will become difficult to develop AI systems that can nudge humans towards cooperation by avoiding being
myopic and focusing on longer-term benefits, and to understand the behavioral tendencies humans exhibit in
sequentialdecision-makingprocesses.
(5)Revisitingexistingtheoryofhumancooperation,particularlyinlightofrecentfindingsindicatingthatAI
systems,suchasautonomoussafetyfeaturesinvehicles,canpotentiallydisruptestablishednormsofreciprocity
among humans [Shirado et al., 2023]. It has been shown that social norms are important mechanisms for
maintaining human cooperation [Santos et al., 2018]; however, these norms may be at risk of erosion when
decision-makingispotentiallydelegatedtoagentsorwhenhumansinteractwithagents. Suchconcernsextend
to other mechanisms, such as network reciprocity and group identity. Thus, this calls for a rethinking of the
existingtheoryofhumancooperation,takingintoaccountthegrowingintegrationofagentsinhumansocieties.
9References
Bowen Baker. Emergent reciprocity and team formation from randomized uncertain social preferences.
NeurIPS,2020.
LucianBusoniu,RobertBabuska,andBartDeSchutter. Acomprehensivesurveyofmultiagentreinforcement
learning. IEEETrans.Syst.ManCybern.Syst.,2008.
Nuttapong Chentanez, Andrew Barto, and Satinder Singh. Intrinsically motivated reinforcement learning.
NeurIPS,2004.
Phillip JK Christoffersen, Andreas A Haupt, and Dylan Hadfield-Menell. Get it in writing: Formal contracts
mitigatesocialdilemmasinmulti-agentrl. InAAMAS,2023.
TimCooijmans,MiladAghajohari,andAaronCourville.Meta-valuelearning:ageneralframeworkforlearning
withlearningawareness. arXiv,2023.
JacobWCrandall,MayadaOudah,andTennometal. Cooperatingwithmachines. Nat.Commun.,2018.
JacobWCrandall. Towardsminimizingdisappointmentinrepeatedgames. J.Artif.Intell.Res.,2014.
CelsoMdeMeloandKazunoriTerada. Cooperationwithautonomousmachinesthroughcultureandemotion.
PLoSOne.,2019.
CelsoMdeMelo,StacyMarsella,andJonathanGratch. Humancooperationwhenactingthroughautonomous
machines. Proc.Natl.Acad.Sci.U.S.A.,2019.
TomEccles,EdwardHughes,andJa´nosKrama´retal. Learningreciprocityincomplexsequentialsocialdilem-
mas. arXiv,2019.
ErnstFehr,UrsFischbacher,andSimonGa¨chter. Strongreciprocity,humancooperation,andtheenforcement
ofsocialnorms. Hum.Nat.,2002.
EliasFerna´ndezDomingos,IneˆsTerrucha,andRe´miSuchonetal. Delegationtoartificialagentsfostersproso-
cialbehaviorsinthecollectiveriskdilemma. Sci.Rep.,2022.
JakobFoerster,RichardYChen,andMaruanAl-Shedivatetal. Learningwithopponent-learningawareness. In
AAMAS,2018.
Ian Gemp, Kevin R McKee, and Richard Everett et al. D3c: Reducing the price of anarchy in multi-agent
learning. InAAMAS,2022.
Tangui Le Gle´au, Xavier Marjou, and Tayeb Lemlouma et al. Tackling asymmetric and circular sequential
socialdilemmaswithreinforcementlearningandgraph-basedtit-for-tat. arXiv,2022.
HaoGuo,ChenShen,andShuyueHuetal.Facilitatingcooperationinhuman-agenthybridpopulationsthrough
autonomousagents. Iscience,2023.
TheAnhHan,CedricPerret,andSimonTPowers. Whento(ornotto)trustintelligentmachines:Insightsfrom
anevolutionarygametheoryanalysisoftrustinrepeatedgames. Cogn.Syst.Res.,2021.
PabloHernandez-Leal,BilalKartal,andMatthewETaylor. Asurveyandcritiqueofmultiagentdeepreinforce-
mentlearning. Auton.Agent.Multi.Agent.Syst.,2019.
ChristianHilbe,TorstenRo¨hl,andManfredMilinski. Extortionsubdueshumanplayersbutisfinallypunished
intheprisoner’sdilemma. Nat.Commun.,2014.
HengyuanHuandDorsaSadigh.Languageinstructedreinforcementlearningforhuman-aicoordination.arXiv,
2023.
YunHua,ShangGao,andWenhaoLietal. Learningoptimal”pigoviantax”insequentialsocialdilemmas. In
AAMAS,2023.
EdwardHughes,JoelZLeibo,andMatthewPhillipsetal. Inequityaversionimprovescooperationinintertem-
poralsocialdilemmas. NeurIPS,2018.
10Fatimah Ishowo-Oloko, Jean-Franc¸ois Bonnefon, and Zakariyah Soroye et al. Behavioural evidence for a
transparency–efficiencytradeoffinhuman–machinecooperation. Nat.Mach.Intell.,2019.
Natasha Jaques, Angeliki Lazaridou, and Edward Hughes et al. Social influence as intrinsic motivation for
multi-agentdeepreinforcementlearning. InICML,2019.
JurgisKarpus,AdrianKru¨ger,andJuliaTovarVerbaetal. Algorithmexploitation: Humansarekeentoexploit
benevolentai. Iscience,2021.
MariaKleshnina,ChristianHilbe,andSˇteˇpa´nSˇimsaetal.Theeffectofenvironmentalinformationonevolution
ofcooperationinstochasticgames. Nat.Commun.,2023.
Ja´nosKrama´r,TomEccles,andIanGempetal. Negotiationandhonestyinartificialintelligencemethodsfor
theboardgameofdiplomacy. Nat.Commun.,2022.
Joel Z Leibo, Vinicius Zambaldi, and Marc Lanctot et al. Multi-agent reinforcement learning in sequential
socialdilemmas. InAAMAS,2017.
Adam Lerer and Alexander Peysakhovich. Maintaining cooperation in complex social dilemmas using deep
reinforcementlearning. arXiv,2017.
AlistairLetcher,JakobFoerster,andDavidBalduzzietal. Stableopponentshapingindifferentiablegames. In
ICLR,2018.
WenhaoLi,XiangfengWang,andBoJinetal. Learningroleswithemergentsocialvalueorientations. arXiv,
2023.
ChristopherLu,TimonWilli,andChristianASchroederDeWittetal. Model-freeopponentshaping. InICML,
2022.
AndreiLupuandDoinaPrecup. Giftinginmulti-agentreinforcementlearning. InAAMAS,2020.
UdariMadhushani,KevinRMcKee,andJohnPAgapiouetal. Heterogeneoussocialvalueorientationleadsto
meaningfuldiversityinsequentialsocialdilemmas. arXiv,2023.
MarioAMaggioniandDomenicoRossignoli.Ifitlookslikeahumanandspeakslikeahuman...communication
andcooperationinstrategichuman–robotinteractions. J.Behav.Exp.Finance,2023.
Kevin R McKee, Ian Gemp, and Brian McWilliams et al. Social diversity and social preferences in mixed-
motivereinforcementlearning. InAAMAS,2020.
Kevin R McKee, Edward Hughes, and Tina O Zhu et al. Deep reinforcement learning models the emergent
dynamicsofhumancooperation. arXiv,2021.
KevinRMcKee,AndreaTacchetti,andMichielABakkeretal. Scaffoldingcooperationinhumangroupswith
deepreinforcementlearning. Nat.Hum.Behav.,2023.
Celso De Melo, Stacy Marsella, and Jonathan Gratch. People do not feel guilty about exploiting machines.
TOCHI,2016.
MartinANowak. Fiverulesfortheevolutionofcooperation. Science,2006.
DavidO’CallaghanandPatrickMannion.Exploringtheimpactoftunableagentsinsequentialsocialdilemmas.
arXiv,2021.
MayadaOudah,TalalRahwan,andTawnaCrandalletal. Howaiwinsfriendsandinfluencespeopleinrepeated
gameswithcheaptalk. InAAAI,2018.
JoonSungPark,JosephO’Brien,andCarrieJunCaietal. Generativeagents: Interactivesimulacraofhuman
behavior. InUIST,2023.
MatjazˇPerc,JillianJJordan,andDavidGRandetal.Statisticalphysicsofhumancooperation.PhysicsReports,
2017.
11DavidRadke,KateLarson,andTimBrechtetal. Towardsabetterunderstandingoflearningwithmultiagent
teams. InIJCAI,2023.
DavidGRandandMartinANowak. Humancooperation. TrendsCogn.Sci.,2013.
RichardMRyanandEdwardLDeci.Intrinsicandextrinsicmotivations:Classicdefinitionsandnewdirections.
Contemp.Educ.Psychol.,2000.
FernandoPSantos,FranciscoCSantos,andJorgeMPacheco. Socialnormcomplexityandpastreputationsin
theevolutionofcooperation. Nature,2018.
Fernando P. Santos, Samuel Mascarenhas, and Francisco C. Santos et al. Picky losers and carefree winners
prevailincollectiveriskdilemmaswithpartnerselection. AutonAgentMultiAgentSyst,2020.
KyrillSchmid,MichaelKo¨lle,andTimMatheis. Learningtoparticipatethroughtradingofrewardshares. In
ALA,Workshop,2022.
Wilko Schwarting, Alyssa Pierson, and Javier Alonso-Mora et al. Social behavior for autonomous vehicles.
Proc.Natl.Acad.Sci.U.S.A.,2019.
GopalSharma,HaoGuo,andChenShenetal. Smallbots,bigimpact: Solvingtheconundrumofcooperation
inoptionalprisoner’sdilemmagamethroughsimplestrategies. J.R.Soc.Interface,2023.
HirokazuShiradoandNicholasAChristakis. Networkengineeringusingautonomousagentsincreasescooper-
ationinhumangroups. Iscience,2020.
Hirokazu Shirado, Shunichi Kasahara, and Nicholas A Christakis. Emergence and collapse of reciprocity in
semiautomaticdrivingcoordinationexperimentswithhumans. Proc.Natl.Acad.Sci.U.S.A.,2023.
ChrisRSims, Hansjo¨rgNeth, andRobertAJacobsetal. Meliorationasrationalchoice: Sequentialdecision
makinginuncertainenvironments. Psychol.Rev.,2013.
John MaynardSmith. Evolutionand the theory ofgames. In Did Darwinget it right? Essayson games, sex
andevolution.1982.
IneˆsTerrucha,EFDmingos,andFCSantosetal. Theartofcompensation: howhybridteamssolvecollective
riskdilemmas. InALA,Workshop,2022.
Paul AM Van Lange, Jeff Joireman, and Craig D Parks et al. The psychology of social dilemmas: A review.
Organ.Behav.HumDecis.Process.,2013.
EugeneVinitsky,RaphaelKo¨ster,andJohnPAgapiouetal. Alearningagentthatacquiressocialnormsfrom
publicsanctionsindecentralizedmulti-agentsettings. CollectiveIntelligence,2023.
Zhijian Wang, Yanran Zhou, and Jaimie W Lien et al. Extortion can outperform generosity in the iterated
prisoner’sdilemma. Nat.Commun.,2016.
ZhenWang,MarkoJusup,andLeiShietal.Exploitingacognitivebiaspromotescooperationinsocialdilemma
experiments. Nat.Commun.,2018.
Jane X Wang, Edward Hughes, and Chrisantha Fernando et al. Evolving intrinsic motivations for altruistic
behavior. InAAMAS,2019.
WeixunWang,JianyeHao,andYixiWangetal. Achievingcooperationthroughdeepmultiagentreinforcement
learninginsequentialprisoner’sdilemmas. InDAI,2019.
TimonWilli,AlistairHpLetcher,andJohannesTreutleinetal.Cola:consistentlearningwithopponent-learning
awareness. InICML,2022.
JiachenYang, AngLi, andMehrdadFarajtabaretal. Learningtoincentivizeotherlearningagents. NeurIPS,
2020.
ChongjieZhangandVictorLesser. Multi-agentlearningwithpolicyprediction. InAAAI,2010.
12KaiqingZhang,ZhuoranYang,andTamerBas¸ar. Multi-agentreinforcementlearning: Aselectiveoverviewof
theoriesandalgorithms. Handbookofreinforcementlearningandcontrol,2021.
Hongxin Zhang, Weihua Du, and Jiaming Shan et al. Building cooperative embodied agents modularly with
largelanguagemodels. InNeurIPS,Workshop,2023.
Zixuan Zhang, AL-Sunni Maitham, and Haoming Jing et al. Rethinking safe control in the presence of self-
seekinghumans. InAAAI,2023.
Stephen Zhao, Chris Lu, and Roger B Grosse et al. Proximal learning with opponent-learning awareness.
NeurIPS,2022.
13