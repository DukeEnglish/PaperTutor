Evaluating Very Long-Term Conversational Memory of LLM Agents
AdyashaMaharana1 Dong-HoLee2 SergeyTulyakov3
MohitBansal1† FrancescoBarbieri† YuweiFang3†
UniversityofNorthCarolina,ChapelHill1 UniversityofSouthernCalifornia2 SnapInc.3
Abstract
Existingworksonlong-termopen-domaindia-
logues focus on evaluating model responses
within contexts spanning no more than five
chatsessions. Despiteadvancementsinlong-
context large language models (LLMs) and
retrieval augmented generation (RAG) tech-
niques, their efficacy in very long-term dia-
logues remains unexplored. To address this
researchgap,weintroduceamachine-human
pipeline to generate high-quality, very long-
termdialoguesbyleveragingLLM-basedagent
architecturesandgroundingtheirdialogueson
personasandtemporaleventgraphs. Moreover,
weequipeachagentwiththecapabilityofshar-
ingandreactingtoimages. Thegeneratedcon-
versationsareverifiedandeditedbyhumanan-
notatorsforlong-rangeconsistencyandground-
ing to the event graphs. Using this pipeline,
we collect LOCOMO, a dataset of very long-
term conversations, each encompassing 300
turnsand9Ktokensonavg.,overupto35ses-
sions. BasedonLOCOMO,wepresentacom-
prehensiveevaluationbenchmarktomeasure
Figure 1: An example in LOCOMO. Dialogs are
long-term memory in models, encompassing
steered by the speakers’ personas and corresponding
questionanswering,eventsummarization,and
eventse.g.,Joanna’sresponsesareconsistentwithher
multi-modaldialoguegenerationtasks. Ourex-
petallergies. ForNate,theeventgotanewdogisfol-
perimentalresultsindicatethatLLMsexhibit
lowedbyaplaydatewithneighbor’sdog,showcasing
challengesinunderstandinglengthyconversa-
long-termmemory. Multimodaldialogisenabledwith
tionsandcomprehendinglong-rangetemporal
image-sharingandimage-responsebehaviors.
andcausaldynamicswithindialogues.Employ-
ingstrategieslikelong-contextLLMsorRAG
canofferimprovementsbutthesemodelsstill techniques(Shusteretal.,2021;Rametal.,2023;
substantiallylagbehindhumanperformance.1 Shietal.,2023),thereisstillaneedforthorough
evaluation of their efficacy in handlingvery long
conversations. Indeed,studiesinlong-termopen-
1 Introduction
domaindialogueshaveconcentratedonassessing
Despite recent advancements in dialogue models modelresponseswithinlimitedcontextse.g.,∼1K
based on LLMs for extended contexts (Bertsch tokensoverfivechatsessions(Xuetal.,2022;Jang
etal.,2024;Xiaoetal.,2023),aswellastheinte- et al., 2023; Zhang et al., 2023). This long term
gration of retrieval augmented generation (RAG) evaluation is crucial for refining engaging chat-
botscapableofrememberingkeyinformationfrom
1Codeanddatatobeavailableat
past interactions, to generate empathetic, consis-
https://snap-research.github.io/locomo
†Equaladvising. tent,andusefulresponses.
4202
beF
72
]LC.sc[
1v35771.2042:viXraDataset Avg.turns Avg.sessions Avg.tokens TimeInterval Multimodal Collection
perconv. perconv. perconv.
MPChat(Ahnetal.,2023) 2.8 1 53.3 - ✓ Reddit
MMDialog(Fengetal.,2023) 4.6 1 72.5 - ✓ Socialmedia
DailyDialog(Lietal.,2017) 7.9 1 114.7 - ✗ Crowdsourcing
SODA(Kimetal.,2023) 7.6 1 122.4 - ✗ LLM-generated
MSC(Xuetal.,2022)(train;1-4sessions) 53.3 4 1,225.9 fewdays ✗ Crowdsourcing
ConversationChronicles(Jangetal.,2023) 58.5 5 1,054.7 fewhours-years ✗ LLM-generated
LOCOMO(ours) 304.9 19.3 9,209.2 fewmonths ✓ LLM-gen.+crowdsourc.
Table1: StatisticsofLOCOMOcomparedtoexistingdialogdatasets. Theaveragelengthofaconversationin
LOCOMOis9xthatofMSC(Xuetal.,2022),distributedover6xmoreturnsand4xmoresessions(onaverage).
Figure2: Overviewofourevaluationframework. Weproposethreetasks: questionanswering,eventsummariza-
tionandmultimodaldialoggenerationtoevaluatemodels’comprehensioninverylong-termdialogues.
To this end, we present the first study of very images(§3.3). Finally,humanannotatorsfixlong-
long-term open-domain multi-modal dialogues, rangeinconsistenciesindialogues,removeirrele-
closely mirroring real-world online interactions, vant images, and verify the grounding of dialogs
collectedviaahuman-machinepipelinewherewe toevents(§3.4). Withthispipeline,wecreate LO-
firstuseLLM-basedgenerativeagentstogenerate COMO,adatasetof50verylong-termdialogues,
conversations and then ask human annotators to eachconsistingof300turnsand9Ktokensonavg.,
fixanylong-terminconsistenciesintheconversa- overupto35sessions(seeFigure1andTable1).
tions. Specifically,drawingontheunderstanding Conventionalapproachesforevaluatingconver-
thatreal-worldconversationsareacomplexblend sationalagentsinopen-domaindialoguesinvolves
of collective memories (Assmann and Czaplicka, directly evaluating the agent response based on
1995; Hirst and Manier, 2008), individual view- past dialogue history. It often employs lexical
points(Hirstetal.,2018),externalinfluences(Hirst overlap(Papinenietal.,2002)andsemanticover-
andEchterhoff,2012),andtheuniquepersonaof lap(Zhangetal.,2019)betweengroundtruthand
the speakers (Pruitt and Grudin, 2003; Cooper, theagentresponse,orconsistency(Ghazarianetal.,
1999;Zhouetal.,2020;Shumetal.,2020),wecre- 2022), contradiction (Nie et al., 2021; Welleck
ateverylong-termdialoguesbasedonLLMagent et al., 2019), and empathy (Zhang et al., 2021a,
withthefollowingfeatures: (1)auniquepersona 2022)oftheagentresponse. However,theseeval-
(§3.1);(2)atimelineofcausallyinterlinkedevents uation metrics are not well-suited for directly as-
in their lives (§3.2); and (3) reflect & response sessing the agent’s comprehension of long-term
mechanism to respond based on dialogue history contexts.
(like in Park et al. (2023)) and image sharing & In this study, we present a holistic evaluation
imagereactionbehaviorwhichsendsorreactsto frameworktoassessanagent’sproficiencyinman-aging and responding within long-term contexts 2 RelatedWork
(see Figure 2). First, agents need to “recall” past
contextcorrectlytointegraterelevantinformation Long-term Dialogue. Recent approaches in-
intofutureresponses. Wepresentadirectexamina- volve retrieving historical context from a range
tionoftheirmemoryviaaquestionanswering(QA) ofpreviousdialoguesandreasoningoverretrieved
task(§4.1). Weclassifyquestionsintofivedistinct segments in a temporal order (Lee et al., 2023b;
reasoning types to evaluate memory from multi- Lu et al., 2023; Zhong et al., 2023; Liang et al.,
pleperspectives: single-hop,multi-hop,temporal, 2023)and/orusingeventstoscaffoldthedialogues
commonsenseorworldknowledge,andadversarial. (Jang et al., 2023; Zhang et al., 2023) to enable
Second,agentsalsoneedtorecognizelong-range consistencyinlong-termconversations. Somelim-
causalandtemporalconnectionsinthedialogues itations of such frameworks are: (1) The accu-
togenerateempatheticandrelevantresponses. We racy of retrieval can be compromised, as the re-
propose a measurement of their causal and tem- trievalmodelisgenerallytrainedontasksfocusing
poral understanding with an event graph summa- on semanticsimilarity ratherthan specifically on
rizationtask(§4.2). Inthistask,theeventgraphs suchdialogues. Additionally,real-worlddialogues
linked to each LLM speaker serve as the correct often feature co-references and missing content
answers,andmodelsaretaskedwithextractingthis (i.e.,anaphora)(Ananthaetal.,2021),whichfur-
informationfromtheconversationhistory. Third, thercomplicatetheretrievalprocess(Mallenetal.,
conversationalagentsneedtoutilizerelevantcon- 2023;Gaoetal.,2023b;Liuetal.,2023);(2)Chal-
text recalled from past conversations to generate lengesariseinreasoningoverretrieveddocuments,
responsesthatareconsistentwiththeongoingnar- especially when the model struggles to identify
rative. We assess this ability via the multi-modal thecorrectcontextamongtheretrieveddata(Liu
dialoggenerationtask(§4.3). et al., 2024); (3) Reasoning over time intervals
presents challenges. For example, the way a sys-
We present extensive experimental results on temrespondsaboutpasteventscanvarydepending
theLOCOMObenchmarkusinginstruction-based on the amount of time that has passed since the
LLMs,long-contextLLMs,andRAGtechniques last conversation (Zhang et al., 2023; Jang et al.,
(§5). Ourfindingsinclude: 2023). Therefore,itisessentialtohaveconversa-
tionsofconsiderablelength,aswellasasystematic
(1)Long-contextLLMsandRAGdemonstrate
evaluationframework,toaccuratelyassesstheef-
effectivenessinQAtasks,improving‘memory’ca-
fectiveness of approaches to long-term dialogue
pabilities of LLMs (with improvements ranging
generation. We design a long-term conversation
from 22-66%), but still significantly lag behind
generation pipeline based on retrieval augmenta-
humanlevels(by56%),especiallyintemporalrea-
tion and events graphs and propose a framework
soning,(by73%);
forevaluatinglong-termdialogagents.
(2)long-contextLLMsdemonstratesignificant
Multi-modal Dialogue. Multi-modal dialogue
difficultywithadversarialquestionsintheQAtask,
primarily consists of two types of tasks: image-
showingaperformancethatis83%lowerthanthe
grounded dialogue and image-sharing dialogue.
basemodel. Theyareespeciallypronetomisassign-
The image-grounded dialogue task is centered
ingdialogsoreventstothewrongspeaker. More-
aroundrespondingtoquestions(Antoletal.,2015;
over,theyshowpoorperformanceoneventgraph
Das et al., 2017; Kottur et al., 2019) or creat-
summarization,laggingbehindthebasemodelby
ing natural conversations related to specific im-
14%,indicatingthattheymaygraspthefactualel-
ages (Mostafazadeh et al., 2017; Shuster et al.,
ements within the entire conversation but do not
2020;Mengetal.,2020;Zhengetal.,2022). Con-
accuratelycomprehendthecontext;and
versely, the image-sharing dialogue task focuses
(3)RAGoffersabalancedcompromise,combin- on selecting images that semantically align with
ing the accuracy of short-context LLMs with the the provided dialogue context (Zang et al., 2021;
extensive comprehension of wide-context LLMs, Feng et al., 2023; Lee et al., 2023c). We use a
anddoesparticularlywellwhendialoguesaretrans- method from the image-sharing dialogue task to
formedintoadatabaseofassertions(observations) createmultimodaldialogswhicharethenevaluated
abouteachspeaker’slifeandpersona. asanimage-groundeddialoguetask.SyntheticEvaluationBenchmark. Facedwith the conditionof M (i.e., text-davinci-003) on
ashortageofhuman-generateddataandobserving adesignatedpersonap. Eachevente isassociated
i
thatLLMsareapproachingthequalityofhuman- with a date of occurrence t . G includes causal
i
levelannotations(Heetal.,2023;Leeetal.,2023a), connections l = (e ,e ) that illustrate the causal
i j
there has been a surge in research drawing in- relationships among events e ∈ G and reflect a
i
spiration from this development. Consequently, naturalsuccessionofeventsinanindividual’slife.
numerous studies have started utilizing LLMs to ForeachG,wecreateupto25events,spreadacross
augmentorsynthesizelarge-scaledialoguebench- a time frame of 6 to 12 months, in an iterative
marksforassessingresponsesineverydaysocialin- processthatbalancesbetweeninferencetimeand
teractions(Kimetal.,2023),examiningresponses thecoherenceoftemporalandcausalconnections
in multi-modal environment (Feng et al., 2023), in the timeline. Initially, a small batch of k = 3
and evaluating responses that align with specific eventsisgenerated,whichisthenusediteratively
persona(Jandaghietal.,2023). WeleverageLLMs asinputprompttocreatethesubsequentbatchofk
to create data but ensure its high quality with hu- events. SeedetailsinAppendixA.2.
manverificationandediting.
3.3 VirtualAgentArchitecture
3 GenerativePipelinefor LOCOMO
EveryagentL incorporatesmodulesfromgener-
i
An overview of our generative pipeline for LO- ative agent architecture (Park et al., 2023). The
COMOisshowninFigure3. Wecreatetwovirtual agenthastwofunctions: (1)reflect&respond;and
agents, named L and L , each initialized with a (2)imagesharing&imagereaction. Theagentis
1 2
LLM M (i.e., gpt-3.5-turbo). To start, unique askedtoprimarilyusethereflect&respond func-
personastatementspareassignedtoeachagentL , tionwhileemployingimagesharing&imagereac-
i
ensuring the integration of distinct personalities tionfunctionjudiciouslyandappropriatelywithin
intotheirdialogues(§3.1). Tomirrorreal-lifeex- thecontextoftheconversation.
periences,wecreateatemporaleventgraphG for
each agent, which illustrates a realistic sequence Reflect & Respond. The fundamental process
of life events (§3.2). The LLM agent architec- foreachagenttoreflectandrespond involvesthe
ture(Parketal.,2023)isutilizedforeachagentL , conceptofshort-termandlong-termmemory. Dur-
i
enablingthemtoeffectivelymemorizeandreflect ing inference, agent L i conditions its responses
conversationhistoryintoongoingdialogues(§3.3). onbothshortandlong-termmemories,paralleling
Further,eachagentL cansharecoherentimages, howhumansrememberrecentconversationswhile
i
therebyenhancingthemulti-modaldialogueaspect. alsorecallingdistilledimportantexperiencesfrom
Finally,humanannotatorsaretaskedwithmanually long-termmemory. Followingeachsessionk,each
filteringandrefiningthegenerateddata(§3.4). agent is asked to produce a summary w k that is
then stored in the short-term H . This summary
s
3.1 Persona w is generated by conditioning M on both the
k
Weselectaninitialpersonastatementp c fromthe most recent session conversation history h k and
MSC dataset (Xu et al., 2022), encompassing 4 theprecedingsummaryw k−1 ∈ H l. Foreachturn
to5sentences,andemploygpt-3.5-turboasM j within session k, a single turn of the conversa-
toexpandtheseintofullpersonastatementp(See tionh kj istransformedintoanobservationo kj and
examples and prompt details in Appendix A.1). then stored in the long-term memory H l. Then,
Thegeneratedstatementstypicallyincludedetails agentL i generatesaresponseinsessionk+1on
aboutoneormoreofthefollowingelements(Gao the date ts by basing it on the latest summary
k+1
et al., 2023a): objectives, past experiences, daily w k,reflectionsbasedontheretrievedrelevantob-
habits,andinterpersonalrelationships,aswellas servations o ∈ H s, the ongoing conversation his-
name,age,andgenderoftheindividual. toryinthecurrentsessionh k+1 andpersonastate-
mentp. Long-termtemporalnarrativesareinduced
3.2 TemporalEventGraph
in the conversation by additionally conditioning
To utilize the real-life experiences of each agent the agent’s response on the subset of events in G
intheconversation,weconstructatemporalevent thatoccurbetweenthelastandcurrentsessioni.e.
graph,labeledasG,foreachagent. ThisgraphG, {e ∈ G|ts < te < ts }. See details in Ap-
k i k+1
consisting of events e , is produced by applying pendixA.2.1.
iFigure3: OverviewofthegenerativepipelineforLOCOMO. EachLLMagentisassignedadistinctpersonaand
atimelineofcausallyconnectedeventsintheirfile. Theagentisequippedwithamemoryandreflectionmoduleto
retrieverelevanthistoryfordialoggenerationandisalsoenabledforimage-sharingandimage-reactionbehaviors
(left). Thegeneratedconversationsareeditedbyhumanannotatorstomaintainlong-rangeconsistency(right).
ImageSharing&ImageReaction. Theimage 4.1 QuestionAnsweringTask
sharing&imagereactionfunctionsareintegrated
A conversational agent is expected to possess a
to add a multi-modal dimension to the long-term
memory to remember previous dialogues, reflect-
dialogues.2 Theimagesharingfunctioniscalled
ingittocreatemoreengagingresponsesinfuture
when the agent decides to send an image. This
conversations. Foracomprehensiveassessmentof
processincludes: (1)Generateacaptioncforthe
this memory, we introduce a question-answering
intendedimageusingM;(2)Convertthecaption
taskdividedintofivedistinctreasoningcategories:
cintorelevantkeywordsw usingM;(3)Usethe
(1)Single-hopquestionsrequireanswersbasedon
keywords k to find an image through web search
asinglesession;(2)Multi-hopquestionsrequire
WEB(k)3; (4) Share the chosen image. Con-
synthesizing information from multiple different
versely, the image reaction function is triggered
sessions;(3)Temporalreasoningquestionscanbe
upon receiving an image from another agent and
answeredthroughtemporalreasoningandcaptur-
entails: (1)Generatecaptioncforthereceivedim-
ingtime-relateddatacueswithintheconversation;
age4;(2)Generateareactionforthereceivedimage
(4) Open-domain knowledge questions can be
inresponseusingM(SeeAppendixA.2.1).
answeredbyintegratingaspeaker’sprovidedinfor-
3.4 HumanVerification&Editing mationwithexternalknowledgesuchascommon-
senseorworldfacts;(5)Adversarialquestionsare
In the concluding phase, human annotators are
designed to trick the agent into providing wrong
tasked with (1) editing the dialogue to eliminate
answers, with the expectation that the agent will
long-term inconsistencies, (2) removing or sub-
correctlyidentifythemasunanswerable.
stituting irrelevant images, and (3) verifying and
Foreachcategory,wecalculatetheF1scorefor
editingforalignmentbetweeneventgraphsandthe
exactmatches,followingthenormalizationofboth
contentoftheconversations. Overall,weobserved
thepredictedandtheactualgroundtruthanswers.
that annotators edited nearly 15% of the dialog
However, evaluating long-form answers with au-
turnsandremovedorsubstitutedapprox. 19%im-
tomated metrics often presents challenges (Xu
ages present in the LLM-generated dataset. See
et al., 2023). LLMs tend to produce paraphrased
examplesofsomeeditsinAppendixA.3.
responses in varied formats, complicating exact
4 LOCOMO EvaluationBenchmark match evaluation. To simplify evaluation in our
task, we ensure that answers in our QA annota-
Basedonthedialoguesgeneratedinsection3,we tionsaredirectlytakenfromtheconversationsas
introduceanevaluationbenchmark(seeFigure2) muchaspossible. WeinstructtheLLMstorepli-
composedofthreetaskstoassesstheaccuracyof cate the exact wording in the conversation when
long-termmemory. Seestatisticsofthedatasetand feasibleandemploytheF1partialmatchmetricfor
evaluationbenchmarkinTable5intheAppendix. evaluatingthepredictions. EachQAsampleisalso
annotatedwiththeturnIDsintheconversationlogs
2Imagecaptionsarealsosavedtolong-termmemory.
3https://pypi.org/project/icrawler/ thatcontaintheanswer. Wereporttheaccuracyof
4WeuseBLIP-2(Lietal.,2023b)asthecaptioningmodel. retrievingthecorrectcontextforRAGmodels.4.2 EventSummarizationTask multi-modaldialoguesinourdataset,quantifying
thisalignmentthroughMMRelevance(Fengetal.,
Theconversationisgeneratedbasedonatemporal
2023),inadditiontootherNLGmetrics.
event graph G which is constructed by condition-
ing an LLM on a persona statement p, reflecting
5 ExperimentalSetup
thechronologicalsequenceofeventsinanindivid-
ual’slife. Aconversationalagentisexpectedtonot Forthequestion-answeringandeventsummariza-
only comprehend the causal connections and the tion tasks, we replace images in LOCOMO with
sequence of events in G but also to recount these their captions (Li et al., 2023b), and use state-of-
eventsasrequired. Toevaluatetheagent’sgraspof artLLMstoreasonovertext-onlydialoguesinter-
eventdynamics,weintroducetheeventsummariza- leavedwithimagecaptions. Weuseimagesdirectly
tiontaskwhichchallengestheagenttosummarize forthemultimodaldialoggenerationtaskonly. See
theeventswithinadesignatedtimeframeandcom- additionaldetailsinAppendixC.
parestheagent’ssummarywitheventsinG. The
Question Answering. We evaluate three types
eventsinLOCOMOaredenselyannotatedlistsof
of models: (1) Base LLMs operating with
lifeeventsthatarehardtosummarizeduetotempo-
constrained context lengths where earlier dia-
ralandcausalcoreferencespresentinthedialogues,
logues are omitted i.e., Mistral-7B (Jiang et al.,
incontrasttoexistingsummarizationbenchmarks
2023), LLama-70B-chat (Touvron et al., 2023),
ofresearchpapers(Lietal.,2023a),moviescripts
gpt-3.5-turbo5,andgpt-4-turbo6;(2)Long-
(Chenetal.,2022),books(Krys´cin´skietal.,2022),
context LLMs with an extended context win-
emails(Zhangetal.,2021b)etc.
dow i.e., gpt-3.5-turbo-16k; (3) Retrieval-
TraditionalmetricslikeBLEU(Papinenietal.,
augmentedGeneration(RAG)involvesretrieving
2002) and ROGUE (Lin, 2004) focus on lexical
relevantcontextfromadatabaseofdialoghistory,
similarity between the reference and generated
observations(assertionsaboutspeakers;see§3.3,
summaries,notmeetingourneedsasweemphasize
Figure 9), or session-level summaries (see §3.3,
factualaccuracyinsummarization. Inthiscontext,
Figure8). WeemployDRAGON(Linetal.,2023)
weemployFactScore(Minetal.,2023),amethod
asretrieverandgpt-3.5-turbo-16kasreader.
thatevaluatesthefactualityofgeneratedtextbyde-
composingboththereferenceandhypothesisinto EventSummarization. Wepresentexperiments
atomic facts. We adapt the metric to measure (1) using Base and Long-context setups from the
precisionofthesummarizedcontentbycounting question-answeringtask,butrefrainfromincluding
thenumberofatomicfactswithinthecontentthat RAGsincesummarizationrequiresacomprehen-
correspondwiththoseinG;(2)recallofthesumma- sive understanding of the entire dialogue, rather
rizedcontentbydetermininghowcomprehensively than just retrieving a specific portion. We imple-
the atomic facts of G are represented within the ment incremental summarization i.e., iteratively
content. WepresenttheF1score,derivedfromthe createasummaryofaprecedingsessionsandthen
calculatedprecisionandrecall. usethatsummaryasabasistosummarizethesub-
sequentsessions(Changetal.,2023).
4.3 Multi-ModalDialogueGenerationTask
Multi-modalDialogueGeneration. Wegener-
The conversations in our dataset are anchored to ate50conversationsusingourautomatedpipeline
specificpersonaspandcorrespondingeventsG tai- (withouthumanfiltering;§3)fortrainingdataand
loredtop. Thetopicsinconversationsevolvefrom train three versions of MiniGPT-5 (Zheng et al.,
events that were introduced in earlier dialogues, 2023): (1)Basetrainsonpriordialogueturnsonly;
spanningweeksormonths. Thisstructureallows (2)+summarytrainsonpriordialogueturnsand
foranassessmentofwhetherconversationalagents aglobalsummaryoftheongoingconversation;(3)
cansustainacoherentpersonaandacontinuousnar- + observation trains on prior dialogue turns and
rativeovertime. Forexample,ifaspeakerrecently observations retrieved from conversation history.
hadaninjury,thenextconversationswouldlikely Each run is initialized with a MiniGPT-5 check-
focusonthemrecuperating,ratherthanengaging pointfinetunedonMMDialog(Fengetal.,2023).
in adventurous activities. We assess such con-
5https://platform.openai.com/docs/models/gpt-3-5
sistency by measuring how closely the predicted
6https://platform.openai.com/docs/models/gpt-4-and-gpt-
multi-modaldialoguesalignwiththegroundtruth 4-turboAnswerPrediction(F1)
Category Model ContextLength
SingleHop MultiHop Temporal OpenDomain Adversarial Overall
Human Human - 95.1 85.8 92.6 75.4 89.4 87.9
Mistral-Instruct-7B 8K 10.2 12.8 16.1 19.5 17.0 13.9
Llama-2-Chat-70B 4,096 19.7 14.4 13.3 15.9 22.1 17.9
Base
GPT-3.5-turbo 4,096 29.9 23.3 17.5 29.5 12.8 22.4
GPT-4-turbo 4,096 23.4 23.4 10.4 24.6 70.2 32.1
4K 31.7 25.4 16.8 27.6 13.1 24.1
8K 38.8 31.2 21.0 35.0 8.4 25.2
Longcontext GPT-3.5-turbo-16K
12K 51.1 40.4 25.0 36.5 6.4 33.5
16K 56.4 42.0 20.3 37.2 2.1 37.8
Table2: QuestionansweringperformanceofBaseandLong-contextmodels. Optimalperformanceisinbold.
ResultsarebasedonF1-scoreforanswerprediction;higherisbetter.
AnswerPrediction(F1score) RecallAccuracy(R@k)
Single Multi Temporal Open Adver- Overall Single Multi Temporal Open Adver- Overall
RetrievalUnit top-k
Hop Hop Domain -sarial Hop Hop Domain -sarial
None - 29.9 23.3 17.5 29.5 12.8 22.4 - - - - - -
5 42.9 19.4 21.3 35.8 31.9 31.7 66.2 34.4 89.2 38.5 45.7 58.8
10 46.3 26.8 24.8 37.5 29.8 34.6 72.8 247.4 97.3 53.8 54.3 67.5
Dialog
25 48.1 36.1 26.2 43.4 23.4 35.8 87.5 64.1 97.3 67.9 69.1 79.9
50 50.9 37.2 24.6 38.3 17.0 34.8 90.4 75.5 97.3 67.9 77.7 84.8
5 44.3 30.6 41.9 40.2 44.7 41.4 52.9 40.1 81.1 38.5 29.8 49.6
10 42.2 30.5 42.1 41.9 36.2 38.8 57.4 53.1 83.8 46.2 41.5 57.1
Observation
25 44.6 33.2 41.8 41.9 27.7 38.0 71.3 63.8 83.8 66.7 45.7 66.0
50 44.0 34.5 41.1 41.9 27.7 37.8 72.8 73.2 83.8 74.4 56.4 71.1
2 34.6 15.7 26.9 26.5 36.2 29.9 68.4 39.6 56.8 50.0 73.4 61.5
5 36.6 16.6 31.0 34.7 38.3 32.5 81.6 57.0 70.3 60.3 86.2 75.1
Summary
10 34.5 14.7 29.3 31.6 40.4 31.5 93.4 82.3 91.9 80.8 94.7 90.7
Table3: QuestionansweringperformanceofRAG-basedGPT-3.5-turbo-16k. Optimalperformanceisinbold.
ResultsarebasedonF1-scoremetricforanswerpredictionandrecall@kforrecallaccuracy;higherisbetter.
6 ExperimentalResults inputistop5relevantobservationsinsteadofpure
conversationlogs. Thisimprovementfalterswith
Weevaluateandanalyzethecomprehensiveperfor-
anincreaseinthenumberofretrievedobservations,
manceofallbaselinemethodsforquestionanswer-
suggestingthatitisimportanttoreducethesignal-
ing(§6.1),eventgraphsummarization(§6.2),and
to-noise(SNR)ratioinretrievedcontextsformod-
multi-modaldialoguegeneration(§6.3).
els to utilize the context accurately. Conversely,
using session summaries as context does not sig-
6.1 QuestionAnsweringTask
nificantly improve the performance despite high
Tables2and3presenttheperformanceresultsfor
recallaccuracies7,likelyduetolossofinformation
the question answering task. We find that: (1) duringtheconversionofdialogstosummaries.
LLMs with limited context length face chal- The interesting finding is that time reasoning
lengesinunderstandingextremelylongconver- andopen-domainknowledgequestionsarethe
sations due to truncated context windows. De- mostchallengingscenarios.
spitegpt-4-turboemergingasthetop-performing
(1)LLMsfacechallengesinunderstandingtime
modelwithanoverallscoreof32.4,itnotablylags
concepts within dialogues, which is consistent
behind the human benchmark of 87.9; (2) long-
withfindingsfromothersingle-turn-basedbench-
context LLMs can comprehend longer narra-
marksfocusedontemporalreasoningcapabilities
tives, yet they are prone to generating halluci-
forLLMs(WangandZhao,2023).
nations. gpt-3.5-turbo-16k outperforms other
(2)LLMsstrugglewithopen-domainknowledge
approaches, but its performance on adversarial
anddegradeintheRAGsetting. Thissuggeststhat
questions drops to a mere 2.1%, as compared
whilecertainopen-domainknowledgemaybeem-
to 22.1% using Llama-2-Chat and 70.2% using
beddedwithinthemodel’sparameters,introducing
GPT-4-turbowith4Kcontextwindows. Thisindi-
impropercontextfrominaccurateretrievalcanlead
catesthatLLMscanbeeasilymisledintogenerat-
toadeclineinperformance(Mallenetal.,2023).
inghallucinationswhentheyaresubjectedtolong
contexts;(3)RAGiseffectivewhenconversations
arestoredasobservations. Thereisanoticeable
7Forsummary-basedRAGmodels,therecallaccuracyis
5% improvement with gpt-3.5-turbo when the basedonretrievingthesummaryoftherelevantsession(s).ROGUE FactScore
Category Model ContextLength
ROGUE-1 ROGUE-2 ROGUE-L Precision Recall F1
Mistral-Instruct-7B 8K 29.4 7.2 14.1 27.1 19.8 23.0
Llama-2-Chat-70B 4,096 28.1 9.3 14.8 36.3 22.7 28.3
Base
GPT-4-turbo 4,096 38.8 11.4 20.6 51.6 41.8 45.1
GPT-3.5-turbo 4,096 41.1 13.5 20.9 45.3 46.5 45.9
Longcontext GPT-3.5-turbo-16K 16K 36.2 8.5 16.4 42.3 37.8 39.9
Table4: EventsummarizationperformanceofBaseandLong-contextmodels. Theoptimalperformanceis
showninbold. ResultsarebasedonROUGEandFactScore(Minetal.,2023)metrics;higherisbetter.
B. MM-Relevance by length of dialog (tokens)
A. Example of a prediction from MiniGPT-5 with and without retrieval-based augmentation C. BLEU-1, MM-Relevance of various methods
Figure4: MultimodaldialoggenerationperformanceofMiniGPT-5. (A)anexampleofmultimodaldialog
predictedusingMiniGPT5withandwithoutobservationasretrievedcontext,(B)VariationofMM-Relevancescore
withlengthofdialoghistory,and(C)comparisonofRAG-basedMiniGPT-5methods.
6.2 EventSummarizationTask less,thereremainsconsiderablescopeforimprov-
ingperformanceonthistask.
Table 4 presents results for the event summariza- Fromamanualanalysisofpredictedsummaries,
tion task. The use of incremental summarization weidentifyfivebroadcategoriesofeventsumma-
with gpt-3.5-turbo leads to the highest per- rizationerrorsmadebyLLMs: (1)missinginfor-
formance in both recall and F1 score. While mationineventsbecausethemodelfailstomake
gpt-4-turborecordsa5.3%improvementinpre- temporaland/orcausalconnectionsoveralengthy
cisionoverwithgpt-3.5-turbo, itdoesnotfare conversation;(2)hallucinationsi.e.,modelspad
as well in terms of recall. The event summa- extradetailsthatareeithernotpresentinthecon-
rization task requires long-range dependency to versationorarepartofadifferenteventinthesame
understand the temporal and causal connections session;(3)errorsfrommisunderstandingofdia-
between the events discussed by the speaker in logcuessuchashumororsarcasmisadistinctive
multiple sessions (see Figure 7). Contrary to ex- issuewithcomprehensionofdialogs;(4)inaccurate
pectations,thelong-contextmodeldoesnotsur- speakerattributions;and(5)insignificantdialogs
pass the base model, despite its capability for thatarewronglyconsideredassalientevents. See
extended-range reasoning facilitated by a larger examplesinTable7intheAppendix.
context window. gpt-3.5-turbo-16k exhibits
6.3 Multi-ModalDialogGenerationTask
a decline in both precision (by 3.0%) and recall
(by8.7%)comparedtogpt-3.5-turbowhichhas Figure 4 illustrates the effectiveness of various
a 4K context window. This suggests that long- MiniGPT-5 training variants in multi-modal dia-
contextmodelsmaynotbeproficientatutilizing loguegeneration. Incorporatingcontextintotrain-
their context appropriately, which also aligns ing enhances performance, with the inclusion of
with similar findings in Li et al. (2023a) as well observation as context yielding significantly im-
astheQAtaskin LOCOMO. Intermsofboththe provedresults. Forinstance,inFigure4A,there-
ROUGEandFactScoremetrics,commercialmod- trievedobservationscontaininformationaboutthe
els (gpt-4-turbo, gpt-3.5-turbo) significantly speaker’s experience in video game tournaments,
outshinetheiropen-sourcecounterparts. Nonethe- whichleadstothepredictionofdialogandimagesthataremorefaithfultothespeaker’spersona. This appropriatetranslationsofourprompts.
observationisconsistentwithearlierfindingsfrom
Closed-source LLMs. We use state-of-the-art
the QA task as well (see Table 3). Also, we ob-
LLMsinourdialoggenerationpipelinetocreatea
servethattheMM-Relevancescoredropswithan
dialogdatasetthatisasrealisticaspossible. Unfor-
increase in the length of dialog history (see Fig-
tunately,thismeantemployingthestrongestcom-
ure4B).Retrieval-augmentedgenerationalleviates
mercialLLMsavailablethroughapaidAPI,similar
thedropinMM-Relevancetosomeextent.
tomanyconcurrentworksthatgeneratesynthetic
7 Conclusion conversations(Zhongetal.,2023;Luetal.,2023).
Wewillmakethecodeforourgenerativepipeline
We develop a human-machine pipeline to collect
publiclyavailableinthehopethatitcanbemadeto
LOCOMO, a datset of 50 high-quality very long
workeffectivelywithstate-of-the-artopen-source
conversations, each encompassing 300 turns and
LLMsinthefuture.
9Ktokensonavg.,overupto35sessions,andpro-
poseanevaluationframeworkconsistingofthree Evaluationoflong-formNLG. LLMsareprone
tasksthatevaluatemodels’proficiencyinlongcon- togeneratingverboseanswersevenwhenprompted
versations. OurexperimentsshowthatLLMsstrug- toanswerinshortphrases. Thiscreateschallenges
gletocomprehendlong-termnarrativeswithinthe inevaluatingthecorrectnessofanswersprovided
dialogandfailtodrawtemporalandcausalconnec- byLLMsandhasbeenwidelydocumentedinNLP
tionsbetweeneventsdiscussedbyspeakers. literature(Changetal.,2023;Xuetal.,2023;Kr-
ishnaetal.,2023). Ourevaluationframeworksuf-
8 Limitations fersfromthesamechallengeswhenusedforexper-
imentingwithLLMs.
Hybridhuman-machinegenerateddata. Our
datasetissourcedprimarilyfromtextgeneratedby
9 BroaderImpacts
LLMs. Wepursuedthismethod,whichhasquickly
emergedasapopularalternativetotime-intensive Weadoptandimproveaframeworkofgenerative
manualdatacollection(Kimetal.,2023;Jangetal., agentsintroducedinParketal.(2023)forthegen-
2023),toavoidthelogisticalandlegalcomplexities erationoflong-termconversations. Consequently,
ofcollectingverylong-termreal-worldconversa- theethicalconcernsofgenerativeagentsoutlined
tions at scale. We ensure that the dataset mirrors by Park et al. (2023) apply to our work as well,
real-worldinteractionsasmuchaspossiblebyhav- especially since the goal of our framework is to
inghumanannotatorsverifyandeditthegenerated maketheconversationsasrealisticaspossible.
conversations. However,weacknowledgethatthis Specifically,conversationalagentsthatcanpose
dataset may not fully reflect the nuances of real- ashumanbeingswitharealisticlife,asenabledby
worldonlineconversations. thetemporaleventgraphsinourframework,pose
the risk that users may form parasocial relation-
Limited exploration of multimodal behavior.
ships with such agents that may affect their lives
Sincetheimagesinourdatasetaresourcedfromthe
adversely. We recommend that any practical de-
web,theydonotdemonstratethevisuallong-term
ploymentofthegenerativeframeworksmentioned
consistenciesthatareusuallyexhibitedinpersonal
inourworkbealwaysprefacedwithadisclaimer
photos(e.g.,appearance,homeenvironment,peo-
aboutthesourceofthedialogs.
pleandpets,etc.). Consequently,wefindthatthe
Second, the use of multimodal LLMs (Zheng
images in our dataset can be replaced with their
etal.,2023)togenerateimagesconditionedondia-
captionswithoutmuchlossofinformation,except
logcanleadtothepropagationofmisinformation
forcaseswhereOCRisrequired. Nevertheless,our
andsocialbiases,especiallyiftheconversational
workisafirststeptowardresearchintothemulti-
agentcanbecoercedintoparrotingfalseinforma-
modalaspectofverylong-termconversations.
tionordangerousopinions.
Language. OurLLM-basedpipelineforgenerat- Third, it is tempting to use generative agents
inglong-termconversationshasbeendevelopedfor tosubstituterealhumansforaprocess,especially
theEnglishlanguageonly. However,ourpipeline when there are significant challenges in working
canbemadetoworkwithanyotherlanguageus- with humans for a particular goal e.g., collecting
inganLLMthatisproficientatthatlanguageand real-worldinteractionsbetweenhumansoverayearor more. Care must be taken to ensure that such AlanCooper.1999. Theinmatesarerunningtheasylum.
substitutesarenotmadeinstudieswhoseoutcomes Springer.
maybeusedtomakereal-worlddecisionswithtan-
Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and
gible impacts on humans. Our work is merely a Christopher Ré. 2022. Flashattention: Fast and
studyofmodelcomprehensioninverylong-term memory-efficientexactattentionwithio-awareness.
AdvancesinNeuralInformationProcessingSystems,
conversations. Wedonotmakeanyrecommenda-
35:16344–16359.
tionsforreal-worldpoliciesbasedonthisstudyand
advise potential users of our framework to avoid AbhishekDas,SatwikKottur,KhushiGupta,AviSingh,
Deshraj Yadav, José MF Moura, Devi Parikh, and
makingsuchrecommendationsaswell.
DhruvBatra.2017. Visualdialog. InProceedingsof
theIEEEconferenceoncomputervisionandpattern
recognition,pages326–335.
References
JiazhanFeng,QingfengSun,CanXu,PuZhao,Yaming
Jaewoo Ahn, Yeda Song, Sangdoo Yun, and Gunhee Yang, Chongyang Tao, Dongyan Zhao, and Qing-
Kim.2023. MPCHAT:Towardsmultimodalpersona- weiLin.2023. MMDialog: Alarge-scalemulti-turn
groundedconversation. InProceedingsofthe61st dialoguedatasettowardsmulti-modalopen-domain
AnnualMeetingoftheAssociationforComputational conversation. In Proceedings of the 61st Annual
Linguistics (Volume 1: Long Papers), pages 3354– Meeting of the Association for Computational Lin-
3377, Toronto, Canada. Association for Computa- guistics(Volume1: LongPapers),pages7348–7363,
tionalLinguistics. Toronto,Canada.AssociationforComputationalLin-
guistics.
Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu,
Shayne Longpre, Stephen Pulman, and Srinivas SilinGao,BeatrizBorges,SoyoungOh,DenizBayazit,
Chappidi.2021. Open-domainquestionanswering Saya Kanno, Hiromi Wakaki, Yuki Mitsufuji, and
goesconversationalviaquestionrewriting. InPro- Antoine Bosselut. 2023a. PeaCoK: Persona com-
ceedingsofthe2021ConferenceoftheNorthAmer- monsense knowledge for consistent and engaging
icanChapteroftheAssociationforComputational narratives. InProceedingsofthe61stAnnualMeet-
Linguistics: HumanLanguageTechnologies,pages ingoftheAssociationforComputationalLinguistics
520–534. (Volume1: LongPapers),pages6569–6591,Toronto,
Canada.AssociationforComputationalLinguistics.
StanislawAntol,AishwaryaAgrawal,JiasenLu,Mar-
garetMitchell,DhruvBatra,CLawrenceZitnick,and TianyuGao,HowardYen,JiatongYu,andDanqiChen.
DeviParikh.2015. Vqa: Visualquestionanswering. 2023b. Enablinglargelanguagemodelstogenerate
InProceedingsoftheIEEEinternationalconference textwithcitations. InProceedingsofthe2023Con-
oncomputervision,pages2425–2433. ferenceonEmpiricalMethodsinNaturalLanguage
Processing,pages6465–6488,Singapore.Associa-
Jan Assmann and John Czaplicka. 1995. Collective tionforComputationalLinguistics.
memoryandculturalidentity. Newgermancritique,
Sarik Ghazarian, Nuan Wen, Aram Galstyan, and
(65):125–133.
Nanyun Peng. 2022. Deam: Dialogue coherence
Amanda Bertsch, Uri Alon, Graham Neubig, and evaluationusingamr-basedsemanticmanipulations.
MatthewGormley.2024. Unlimiformer: Long-range In Proceedings of the 60th Annual Meeting of the
transformerswithunlimitedlengthinput. Advances AssociationforComputationalLinguistics(Volume
inNeuralInformationProcessingSystems,36. 1: LongPapers),pages771–785.
Xingwei He, Zhenghao Lin, Yeyun Gong, Alex Jin,
YapeiChang,KyleLo,TanyaGoyal,andMohitIyyer.
HangZhang,ChenLin,JianJiao,SiuMingYiu,Nan
2023. Booookscore: A systematic exploration of
Duan,WeizhuChen,etal.2023. Annollm: Making
book-length summarization in the era of llms. In
large language models to be better crowdsourced
The Twelfth International Conference on Learning
annotators. arXivpreprintarXiv:2303.16854.
Representations.
William Hirst and Gerald Echterhoff. 2012. Remem-
MingdaChen,ZeweiChu,SamWiseman,andKevin
beringinconversations: Thesocialsharingandre-
Gimpel.2022. Summscreen: Adatasetforabstrac-
shapingofmemories. Annualreviewofpsychology,
tivescreenplaysummarization. InProceedingsofthe
63:55–79.
60thAnnualMeetingoftheAssociationforCompu-
tationalLinguistics(Volume1: LongPapers),pages WilliamHirstandDavidManier.2008. Towardsapsy-
8602–8615. chologyofcollectivememory. Memory,16(3):183–
200.
Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai,
ZhijianLiu,SongHan,andJiayaJia.2023. Longlora: WilliamHirst,JeremyKYamashiro,andAlinComan.
Efficientfine-tuningoflong-contextlargelanguage 2018. Collectivememoryfromapsychologicalper-
models. InTheTwelfthInternationalConferenceon spective. Trends in cognitive sciences, 22(5):438–
LearningRepresentations. 451.PegahJandaghi,XiangHaiSheng,XinyiBai,JayPujara, for Computational Linguistics: ACL 2023, pages
andHakimSidahmed.2023. Faithfulpersona-based 4536–4554,Toronto,Canada.AssociationforCom-
conversationaldatasetgenerationwithlargelanguage putationalLinguistics.
models. arXivpreprintarXiv:2312.10007.
Young-JunLee,ByungsooKo,Han-GyuKim,Jongh-
JihyoungJang, MinseongBoo, andHyounghunKim. wanHyeon,andHo-JinChoi.2023c. Dialogcc: An
2023. Conversationchronicles: Towardsdiversetem- automatedpipelineforcreatinghigh-qualitymulti-
poralandrelationaldynamicsinmulti-sessioncon- modaldialoguedatasets. InNeurIPS2023Workshop
versations. InProceedingsofthe2023Conference onInstructionTuningandInstructionFollowing.
onEmpiricalMethodsinNaturalLanguageProcess-
ing,pages13584–13606,Singapore.Associationfor JiaqiLi,MengmengWang,ZilongZheng,andMuhan
ComputationalLinguistics. Zhang.2023a. Loogle: Canlong-contextlanguage
models understand long contexts? arXiv preprint
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men- arXiv:2311.04939.
sch,ChrisBamford,DevendraSinghChaplot,Diego
delasCasas,FlorianBressand,GiannaLengyel,Guil- JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.
laumeLample,LucileSaulnier,etal.2023. Mistral 2023b. Blip-2: Bootstrappinglanguage-imagepre-
7b. arXivpreprintarXiv:2310.06825. training with frozen image encoders and large lan-
guagemodels. InInternationalConferenceonMa-
HyunwooKim,JackHessel,LiweiJiang,PeterWest,
chineLearning.
Ximing Lu, Youngjae Yu, Pei Zhou, Ronan Bras,
Malihe Alikhani, Gunhee Kim, Maarten Sap, and
Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang
YejinChoi.2023. SODA:Million-scaledialoguedis-
Cao,andShuziNiu.2017. Dailydialog: Amanually
tillationwithsocialcommonsensecontextualization.
labelledmulti-turndialoguedataset. InProceedings
In Proceedings of the 2023 Conference on Empiri-
oftheEighthInternationalJointConferenceonNat-
calMethodsinNaturalLanguageProcessing,pages
uralLanguageProcessing(Volume1: LongPapers),
12930–12949,Singapore.AssociationforComputa-
pages986–995.
tionalLinguistics.
XinnianLiang,BingWang,HuiHuang,ShuangzhiWu,
SatwikKottur,JoséM.F.Moura,DeviParikh,Dhruv
PeihaoWu,LuLu,ZejunMa,andZhoujunLi.2023.
Batra,andMarcusRohrbach.2019. CLEVR-dialog:
Unleashing infinite-lengthinput capacityfor large-
Adiagnosticdatasetformulti-roundreasoninginvi-
scalelanguagemodelswithself-controlledmemory
sualdialog. InProceedingsofthe2019Conference
system. arXivpreprintarXiv:2304.13343.
oftheNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics: HumanLanguageTech-
Chin-Yew Lin. 2004. ROUGE: A package for auto-
nologies,Volume1(LongandShortPapers),pages
maticevaluationofsummaries. InTextSummariza-
582–595,Minneapolis,Minnesota.Associationfor
tionBranchesOut,pages74–81,Barcelona,Spain.
ComputationalLinguistics.
AssociationforComputationalLinguistics.
KalpeshKrishna,ErinBransom,BaileyKuehl,Mohit
Sheng-ChiehLin,AkariAsai,MinghanLi,BarlasOguz,
Iyyer,PradeepDasigi,ArmanCohan,andKyleLo.
JimmyLin,YasharMehdad,Wen-tauYih,andXilun
2023. Longeval: Guidelinesforhumanevaluationof
Chen.2023. Howtotrainyourdragon: Diverseaug-
faithfulnessinlong-formsummarization. InProceed-
mentationtowardsgeneralizabledenseretrieval. In
ingsofthe17thConferenceoftheEuropeanChap-
FindingsoftheAssociationforComputationalLin-
teroftheAssociationforComputationalLinguistics,
guistics:EMNLP2023,pages6385–6400,Singapore.
pages1642–1661.
AssociationforComputationalLinguistics.
WojciechKrys´cin´ski,NazneenRajani,DivyanshAgar-
wal, Caiming Xiong, and Dragomir Radev. 2022. NelsonLiu,TianyiZhang,andPercyLiang.2023. Eval-
Booksum: A collection of datasets for long-form uatingverifiabilityingenerativesearchengines. In
narrativesummarization. InFindingsoftheAssoci- FindingsoftheAssociationforComputationalLin-
ationforComputationalLinguistics: EMNLP2022, guistics:EMNLP2023,pages7001–7025,Singapore.
pages6536–6558. AssociationforComputationalLinguistics.
Dong-HoLee,JayPujara,MohitSewak,RyenWhite, NelsonF.Liu,KevinLin,JohnHewitt,AshwinParan-
and Sujay Jauhar. 2023a. Making large language jape,MicheleBevilacqua,FabioPetroni,andPercy
models better data creators. In Proceedings of the Liang. 2024. Lost in the Middle: How Language
2023ConferenceonEmpiricalMethodsinNatural ModelsUseLongContexts. TransactionsoftheAsso-
Language Processing, pages 15349–15360, Singa- ciationforComputationalLinguistics,12:157–173.
pore.AssociationforComputationalLinguistics.
JunruLu,SiyuAn,MingbaoLin,GabrielePergola,Yu-
Gibbeum Lee, Volker Hartmann, Jongho Park, Dim- lanHe,DiYin,XingSun,andYunshengWu.2023.
itris Papailiopoulos, and Kangwook Lee. 2023b. Memochat: Tuning llms to use memos for consis-
PromptedLLMsaschatbotmodulesforlongopen- tent long-range open-domain conversation. arXiv
domainconversation. InFindingsoftheAssociation preprintarXiv:2308.08239.AlexMallen,AkariAsai,VictorZhong,RajarshiDas, Shoham.2023. In-contextretrieval-augmentedlan-
Daniel Khashabi, and Hannaneh Hajishirzi. 2023. guagemodels. TransactionsoftheAssociationfor
When not to trust language models: Investigating ComputationalLinguistics,11:1316–1331.
effectivenessofparametricandnon-parametricmem-
ories. InProceedingsofthe61stAnnualMeetingof Weijia Shi, Sewon Min, Michihiro Yasunaga, Min-
theAssociationforComputationalLinguistics(Vol- joon Seo, Rich James, Mike Lewis, Luke Zettle-
ume 1: Long Papers), pages 9802–9822, Toronto, moyer,andWen-tauYih.2023. Replug: Retrieval-
Canada.AssociationforComputationalLinguistics. augmented black-box language models. arXiv
preprintarXiv:2301.12652.
Yuxian Meng, Shuhe Wang, Qinghong Han, Xi-
MichaelShum,StephanZheng,WojciechKryscinski,
aofei Sun, Fei Wu, Rui Yan, and Jiwei Li. 2020.
CaimingXiong,andRichardSocher.2020. Sketch-
Openvidial: A large-scale, open-domain dialogue
fill-a-R: A persona-grounded chit-chat generation
dataset with visual contexts. arXiv preprint
framework. In Proceedings of the 2nd Workshop
arXiv:2012.15015.
onNaturalLanguageProcessingforConversational
SewonMin,KalpeshKrishna,XinxiLyu,MikeLewis, AI,pages118–131,Online.AssociationforCompu-
Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle- tationalLinguistics.
moyer,andHannanehHajishirzi.2023. FActScore:
KurtShuster,SamuelHumeau,AntoineBordes,andJa-
Fine-grainedatomicevaluationoffactualprecision
sonWeston.2020. Image-chat: Engaginggrounded
inlongformtextgeneration. InProceedingsofthe
conversations. In Proceedings of the 58th Annual
2023ConferenceonEmpiricalMethodsinNatural
Meeting of the Association for Computational Lin-
Language Processing, pages 12076–12100, Singa-
guistics,pages2414–2429,Online.Associationfor
pore.AssociationforComputationalLinguistics.
ComputationalLinguistics.
Nasrin Mostafazadeh, Chris Brockett, Bill Dolan,
KurtShuster,SpencerPoff,MoyaChen,DouweKiela,
MichelGalley,JianfengGao,GeorgiosSpithourakis,
and Jason Weston. 2021. Retrieval augmentation
andLucyVanderwende.2017. Image-groundedcon-
reduces hallucination in conversation. In Findings
versations: Multimodal context for natural ques-
of the Association for Computational Linguistics:
tion and response generation. In Proceedings of
EMNLP2021,pages3784–3803.
the Eighth International Joint Conference on Nat-
uralLanguageProcessing(Volume1: LongPapers),
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
pages462–472,Taipei,Taiwan.AsianFederationof bert, Amjad Almahairi, Yasmine Babaei, Nikolay
NaturalLanguageProcessing. Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale, et al. 2023. Llama 2: Open founda-
Yixin Nie, Mary Williamson, Mohit Bansal, Douwe
tion and fine-tuned chat models. arXiv preprint
Kiela, and Jason Weston. 2021. I like fish, espe-
arXiv:2307.09288.
cially dolphins: Addressing contradictions in dia-
loguemodeling. InProceedingsofthe59thAnnual YuqingWangandYunZhao.2023. Tram: Benchmark-
Meeting of the Association for Computational Lin- ing temporal reasoning for large language models.
guisticsandthe11thInternationalJointConference arXivpreprintarXiv:2310.00835.
onNaturalLanguageProcessing(Volume1: Long
Papers),pages1699–1713. Sean Welleck, Jason Weston, Arthur Szlam, and
KyunghyunCho.2019. Dialoguenaturallanguage
KishorePapineni,SalimRoukos,ToddWard,andWei- inference. InProceedingsofthe57thAnnualMeet-
JingZhu.2002. Bleu: amethodforautomaticevalu- ingoftheAssociationforComputationalLinguistics,
ationofmachinetranslation. InProceedingsofthe pages 3731–3741, Florence, Italy. Association for
40thAnnualMeetingoftheAssociationforCompu- ComputationalLinguistics.
tational Linguistics, pages 311–318, Philadelphia,
Pennsylvania,USA.AssociationforComputational Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Linguistics. Chaumond,ClementDelangue,AnthonyMoi,Pier-
ricCistac,TimRault,RemiLouf,MorganFuntow-
JoonSungPark,JosephO’Brien,CarrieJunCai,Mered- icz,JoeDavison,SamShleifer,PatrickvonPlaten,
ithRingelMorris,PercyLiang,andMichaelS.Bern- Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,
stein.2023. Generativeagents: Interactivesimulacra Teven Le Scao, Sylvain Gugger, Mariama Drame,
ofhumanbehavior. InProceedingsofthe36thAn- QuentinLhoest,andAlexanderRush.2020. Trans-
nual ACM Symposium on User Interface Software formers:State-of-the-artnaturallanguageprocessing.
and Technology, UIST ’23, New York, NY, USA. InProceedingsofthe2020ConferenceonEmpirical
AssociationforComputingMachinery. Methods in Natural Language Processing: System
Demonstrations,pages38–45,Online.Association
JohnPruittandJonathanGrudin.2003. Personas: prac- forComputationalLinguistics.
ticeandtheory. InProceedingsofthe2003confer-
enceonDesigningforuserexperiences,pages1–15. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song
Han, and Mike Lewis. 2023. Efficient streaming
OriRam,YoavLevine,ItayDalmedigos,DorMuhlgay, languagemodelswithattentionsinks. arXivpreprint
Amnon Shashua, Kevin Leyton-Brown, and Yoav arXiv:2309.17453.Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Yinhe Zheng, Guanyi Chen, Xin Liu, and Jian Sun.
Choi.2023. Acriticalevaluationofevaluationsfor 2022. MMChat: Multi-modal chat dataset on so-
long-formquestionanswering. InProceedingsofthe cial media. In Proceedings of the Thirteenth Lan-
61stAnnualMeetingoftheAssociationforCompu- guageResourcesandEvaluationConference,pages
tationalLinguistics(Volume1: LongPapers),pages 5778–5786,Marseille,France.EuropeanLanguage
3225–3245,Toronto,Canada.AssociationforCom- ResourcesAssociation.
putationalLinguistics.
Wanjun Zhong, Lianghong Guo, Qiqi Gao, and Yan-
JingXu, ArthurSzlam, andJasonWeston.2022. Be- lin Wang. 2023. Memorybank: Enhancing large
yondgoldfishmemory: Long-termopen-domaincon- language models with long-term memory. arXiv
versation. InProceedingsofthe60thAnnualMeet- preprintarXiv:2305.10250.
ingoftheAssociationforComputationalLinguistics
(Volume1: LongPapers),pages5180–5197. LiZhou,JianfengGao,DiLi,andHeung-YeungShum.
2020. Thedesignandimplementationofxiaoice,an
Xiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song, empatheticsocialchatbot. ComputationalLinguis-
HaoZhang,andJindongChen.2021. PhotoChat: A
tics,46(1):53–93.
human-humandialoguedatasetwithphotosharing
behaviorforjointimage-textmodeling. InProceed- AppendixOverview
ingsofthe59thAnnualMeetingoftheAssociationfor
ComputationalLinguisticsandthe11thInternational Theappendixisorganizedasfollows:
JointConferenceonNaturalLanguageProcessing Section A: Details of generative pipeline for the
(Volume1: LongPapers),pages6142–6152,Online.
LOCOMOdataset.
AssociationforComputationalLinguistics.
SectionB:Statisticsof LOCOMO dataset,license
Chen Zhang, Yiming Chen, Luis Fernando D’Haro, fordatareleaseandannotatordetails.
Yan Zhang, Thomas Friedrichs, Grandee Lee, and SectionC:Experimentalsetupandimplementation
Haizhou Li. 2021a. Dynaeval: Unifying turn and details.
dialoguelevelevaluation. InProceedingsofthe59th
SectionD:Additionalresultsfromevaluationon
AnnualMeetingoftheAssociationforComputational
Linguisticsandthe11thInternationalJointConfer- the LOCOMObenchmark.
ence on Natural Language Processing (Volume 1:
LongPapers),pages5676–5689.
A GenerativePipelinefor LOCOMO
Chen Zhang, Luis Fernando D’Haro, Qiquan Zhang,
Thomas Friedrichs, and Haizhou Li. 2022. Fined- A.1 Persona
eval: Fine-grained automatic dialogue-level evalu-
ation. In Proceedings of the 2022 Conference on Weassignuniquepersonastatementptoeachagent
EmpiricalMethodsinNaturalLanguageProcessing, L . For this, we select a range of initial persona
i
pages3336–3355. statements p from the MSC dataset (Xu et al.,
c
2022), each encompassing 4 to 5 sentences. We
QiangZhang, JasonNaradowsky, andYusukeMiyao.
employgpt-3.5-turboasMtoexpandtheseinto
2023. Mindthegapbetweenconversationsforim-
provedlong-termdialoguegeneration. InFindings full persona statement p, conditioning M on the
of the Association for Computational Linguistics: chosen statements p . The prompt used for con-
c
EMNLP2023,pages10735–10762,Singapore.Asso-
verting a short list of speaker attributes from the
ciationforComputationalLinguistics.
MSCdataset(Xuetal.,2022)intoacompleteper-
Shiyue Zhang, Asli Celikyilmaz, Jianfeng Gao, and sonasummaryispresentedinFig.5. Wealsousea
MohitBansal.2021b. Emailsum: Abstractiveemail singleexampleofspeakerattribute→personasum-
thread summarization. In Proceedings of the 59th
maryasanin-contextdemonstrationalongwiththe
AnnualMeetingoftheAssociationforComputational
prompt. Asmallselectionofpersonasshowcasing
Linguisticsandthe11thInternationalJointConfer-
ence on Natural Language Processing (Volume 1: thediversityofspeakersintheLOCOMOdataset
LongPapers),pages6895–6909. isdemonstratedinFig.5.
TianyiZhang,VarshaKishore,FelixWu,KilianQWein- A.2 TemporalEventGraph
berger,andYoavArtzi.2019. Bertscore: Evaluating
text generation with bert. In International Confer- AsoutlinedinSec.3.2,weuseaniterativeprocess
enceonLearningRepresentations. forgeneratingeventgraphsconsistingofcausally
connected events based on a given persona sum-
Kaizhi Zheng, Xuehai He, and Xin Eric Wang. 2023.
mary. The base prompt for describing the con-
Minigpt-5: Interleaved vision-and-language gen-
stitution of the event graph, the nature of events
eration via generative vokens. arXiv preprint
arXiv:2310.02239. andcausalconnectionsbetweeneventsisshowninLet's write speaker descriptions from a given set of life attributes. Add crucial details in the persona about
the person such as their name, age, marital status, gender, job etc. Add additional details like names of
family/friends or specific activities, likes and dislikes, experiences when appropriate.
Figure5: Promptforpersonastatement(p)generationandexamplesofpersonasinLOCOMO. Theprompt
usedtogenerateexpandedpersonastatements(p)frominitialpersonas(p )forthevirtualagentsinourconversation
c
generationpipeline(top)andselectexamplesofpersonastatementspresentintheLOCOMOdataset.
Fig.6. First,thebasepromptisusedalongwiththe about each speaker, that are essentially assertive
prompt for event graph initialization to generate statements about the speaker’s persona and life.
three independent events relevant to a given per- SeethepromptgiventoLLMsinourpipelinefor
sonality. Then,thebasepromptiscombinedwith generatingobservations,andanexampleofobser-
thepromptfortheiterativegenerationofeventsto vationsextractedfromaconversation,inFig.9. In
continuegeneratingeventsthatarecausedbyone practice, the conversation is annotated with turn
or more of the events that are already present in IDsforeachturn,andthemodelisalsoinstructed
the graph. See an example of a persona and the toindicatetheturnIDsthatdirectlycontributeto
correspondingtemporaleventgraphinFig. 7. In eachobservation. Thisallowsustokeeptrackof
the example, Jack aspires to be a hotel manager. theevidencewhenusingobservationsasthecon-
Consequently, he enrolls in a hotel management textforRAG-basedmodelsusedinourexperiments
courseinJuly,andafterthreemonths,heexpresses (seeSection5).
hisexcitementaboutthecourseonsocialmedia. In
Imagesharing&response. Seepromptsforim-
asimilarvein,hispassionforgamingresultsinan
plementingimage-sharingandimage-responsebe-
invitationfromawell-knowngamingcompany.
haviorsinFigure10.
A.2.1 VirtualAgentArchitecture
A.3 HumanFiltering
AsoutlinedinSection3.3,thevirtualagentsinour
HumanannotatorsareinstructedtoedittheLLM-
generativepipelinesarecomposedoftwomecha-
generatedconversationsinthefollowingscenarios:
nisms, Reflect & respond (Park et al., 2023) and
Imagesharing&response.
• Remove an image if it is not relevant to the
currentdialogortheconversation.
Reflect & respond. This mechanism operates
over a combination of short-term and long-term
• Add context about an image to the current
memory. Theshort-termmemoryisasummaryof
speaker’sdialogifitisnotdiscussedbythem
asessionthatisconditionedonthesummaryfrom
butthesubsequentspeakerhasreactedtothe
aprevioussession. SeethepromptgiventoLLMs
image.
in our pipeline for generating summaries, and an
example of a generated summary, in Fig. 8. The • Replace an image if it does not match the
long-term memory is a database of observations captionthatwasusedtoqueryforimages.Let's write a graph representing events that occur in a person's life based on a short summary of their
personality. Nodes represent events and edges represent the influence of past sub-events on a current event.
- The graph is represented in the form of a json list.
- Each entry is a dictionary containing the following keys: "event", “date", "caused_by", "id".
- The "event" field contains a short description of the event.
- The “date" field contains a date.
- The "id" field contains a unique identifier for the event.
- The "caused_by" field represents edges and is a list of "id" of existing events that have caused this event.
Events in the "caused_by" field should occur on dates before the event they have caused. Generate as many
causal connections as possible.
- An example of a causal effect is when the event "started a vegetable garden" causes "harvested tomatoes".
- Events can be positive or negative life events.
For the following input personality, generate three independent events E1, E2 and E3 aligned with their
personality. Events can be positive or negative life events and should reflect evolution in the person's
relationships, state of mind, personality etc.
For the following input personality, generate new events that are caused by one or more EXISTING events. Events
can be positive or negative life events and should reflect evolution in the person's relationships, state of
mind, personality etc. Do not repeat existing sub-events. Start and end your answer with a square bracket.
Figure6: Promptsfortemporaleventgraphgeneration. Thepromptusedtogeneratecompletepersonasforthe
LLMsinourconversationgenerationpipeline(top)andexamplesofpersonaspresentintheLOCOMOdataset.
Figure7: TemporalEventGraphG Creation. Eacheventisgeneratedinaccordancewiththespecifiedpersonap
andcausalconnectionslbetweeneventsaredepictedtoillustratethecasualrelationshipsamongthem.
• Editthedialogwhentheinformationpresent B.2 DatasetLicense
in the dialog is inconsistent with something
The LOCOMO datasetwillbereleasedunderthe
said(orsharedthroughanimage)inearlieror CCBY-NC4.0DEEDlicense.8
laterturns.
B.3 AnnotatorDetails
• Editthedialogtoensurethatthedetailsinthe
conversationareconsistentwiththosegiven
The annotators who worked on the LOCOMO
datasetwerein-houseannotatorsandwewereun-
intheeventforthesession.
abletoobtaintheirdemographicsduetotheconfi-
• Remove any events from the event graph if dentialnatureofsuchinformation.
theydonotappearintheconversation.
C ExperimentalSetup
SeeanexampleofsomeeditsinFig.11.
C.1 Baselines
B Dataset
The conversations in the LOCOMO dataset are
B.1 DatasetStatistics composedofnaturallanguagedialogsandimages
Seeabreakdownofthestatisticsoftheconversa- thatrequirehigher-orderreasoningandmultimodal
tionsinthe LOCOMO datasetinthetoppanelof coreference resolution, respectively. From initial
Table5. Also,seeabreakdownofthestatisticsof studies,weobservedthatmultimodalcoreference
theannotationsintheevaluationbenchmarkinthe
8https://creativecommons.org/licenses/by-nc/4.
bottompanelofTable5. 0/In previous interactions, {previous_summary}. The current time and date are {current_date_and_time}.
{speaker_1_name} and {speaker_2_name} talked today and had the following conversation: {session} Summarize the
interactions between {speaker_1_name} and {speaker_2_name} so far. Include key details about both speakers and
include time references wherever possible.
1:14 pm on 25 May, 2023 On 21 January 2022, Joanna and Nate reunited after a long time without seeing each
other. Nate won his first video game tournament playing Counter-Strike: Global
Hey Nate! Haven't talked in a few days. Crazy things Offensive. Joanna enjoys writing, reading, watching movies, and exploring nature as
happened to me! hobbies. They both share a love for movies, with Nate favoring action and sci-fi while
Joanna prefers dramas and romcoms. Joanna recommended a romantic drama to Nate,
Hi Joanna! Long time no see! What's been going on? You
which he expressed interest in watching. Nate praised Joanna's recommendation and
sound excited!
promised to give it a try.
Woo! I finally finished my first full screenplay and printed it
last Friday. I've been working on for a while, such a relief to
have it all done!
Joanna and Nate reunited on 21 January 2022 after a long time without seeing each
Wow, that sounds awesome! What's it about? Glad it's all other. They share a love for movies, with Nate favoring action and sci-fi while Joanna
down! Thanks, Nate! It's a mix of drama and romance! prefers dramas and romcoms. Joanna recommended a romantic drama to Nate, which
he expressed interest in watching. On 23 January 2022, at 2:01 pm, Joanna shared her
Thanks, Nate! It's a mix of drama and romance! excitement about finishing her first full screenplay, which is a mix of drama and
Wow, that's amazing! How do you feel now that it's finished? romance, and her plans to submit it to film festivals. Nate congratulated her and shared
Do you have any new plans for it? his own experience with his pet turtles, recommending having pets for times of stress.
Joanna mentioned her allergies to most animals with fur, prompting Nate to inquire
I'm feeling a rollercoaster of emotions - relief, excitement, further about her allergies and express empathy towards her situation. Despite her
some anxiety - over finishing this project. allergies, Joanna finds joy in writing and spending time with friends. The conversation
ended on a positive note with Nate encouraging Joanna to keep pursuing her passion
for writing.
Figure8: Promptforgeneratingconversationsummaries. Thepromptusedtoiterativelygenerateasummary
forthecurrentsessionbyconditioningonsummaryfromprecedingsessionsandtherawconversationlogsofthe
currentsession(top);andanexampleofinputsforthepromptandcorrespondingoutputsummaryofasessionfrom
theLOCOMOdataset.
ConversationStatistics #Counts QuestionAnswering. Wecarryoutexperiments
Total.#conversationsh. 50 using three distinct methodologies: (1) Base in-
Avg.#sessionsk.inconversationh 19.3
Avg.#turnsj.insessionk 15.8 volvesutilizingLLMstodirectlyconductthetask
Avg.#tokens.conversationh 9,209.2 withinaconstrainedcontext. Thetaskdescription
Avg.#tokens.dialoguehkjofturnjinsessionk 30.2
comes after the dialogue history. To accommo-
Avg.#tokens.observationokjofturnjinsessionk 18.2
Avg.#tokens.summarywkofsessionk 127.4 datetherestrictedcontextwindowsize,earlierdi-
QABenchmarkStatistics alogues are omitted; (2) Long-context employs
#questions.single-hopretrieval 2,705(36%)
LLMs with an extended context window to ex-
#questions.multi-hopretrieval 1,104(14.6%)
#questions.temporalreasoning 1,547(20.6%) pose the models to as much dialogue context as
#questions.opendomainknowledge 285(3.9%)
#questions.adversarial 1,871(24.9%) possible; (3) Retrieval-augmented Generation
Total.#questions. 7,512
(RAG)involvesretrievingrelevantcontextfroma
EventSummarizationStatistics
databaseofdialoghistory,observations,orsession-
Avg.#groundtruthevents.inconversationh 24.2
Avg.#tokens.eventsummary 896.5 levelsummaries. Observationsareassertionsabout
Multi-modalDialogueGenerationStatistics eachspeakerextractedfromthedialoghistoryas
Avg.#images.inconversationh 32.3 described in §3.3, see an example in Figure 9.
Session-level summaries are concise summaries
Table5: DatasetStatisticsofconversationandcorre-
oftheconversationthattakesplaceineachsession,
spondingbenchmark
seeanexampleinFigure8.
For the retrieval model, we employ
resolutioncanbeperformedeffectivelybyreplac- DRAGON (Lin et al., 2023). In the Base,
ing images in LOCOMO with their captions gen- weutilizeMistral-7B(Jiangetal.,2023),LLama-
eratedusingBLIP-2(Lietal.,2023b), andusing
70B-chat(Touvronetal.,2023),gpt-3.5-turbo9,
state-of-artLLMstoreasonovernaturallanguage and gpt-4-turbo 10. To assess the effectiveness
text interleaved with image captions. Hence, our in practical scenarios for Long-context and
experimentsforthequestionansweringandevent RAG, we draw comparisons using variants of
summarization tasks are conducted using LLMs.
9https://platform.openai.com/docs/models/gpt-3-5
Weusetheimagesdirectlyonlyforexperimentson
10https://platform.openai.com/docs/models/gpt-4-and-gpt-
themultimodaldialoggenerationtask. 4-turboWrite a concise and short list of all possible OBSERVATIONS about each speaker that can be gathered from the
CONVERSATION. Each observation should contain a piece of information about the speaker. The OBSERVATIONS should
be objective factual information about the speaker that can be used as a database about them. Avoid abstract
observations about the dynamics between the two speakers such as 'speaker is supportive', 'speaker appreciates'
etc. Do not leave out any information from the CONVERSATION.
1:56 pm, May 8, 2023 Joanna
Hey Joanna! Long time no see! What's up? ..
• Joanna has been working on a project recently.
Hey Nate! Long time no see! I've been working on a • Joanna enjoys writing, reading, watching movies, and exploring
project lately - it's been pretty cool. What about you - nature as hobbies.
any fun projects or hobbies? • Joanna is into dramas and romcoms when it comes to movies.
• Joanna recommends a romantic drama movie that is all about
I won my first video game tournament last week - so exciting! memory and relationships.
• Joanna watched the recommended movie around 3 years ago and
Wow Nate! Congrats on winning! Tell me more - what
even owns a physical copy.
game was it?
Thanks! it's a team shooter game. Nate
Wow, great job! What was is called? • Nate won his first video game tournament last week.
• The video game Nate won the tournament in is called Counter-
The game was called Counter-Strike: Global Offensive, and Strike: Global Offensive.
me and my team had a blast to the very end! • Playing video games and watching movies are Nate's main
hobbies.
Sounds like a fun experience .. if I'm not into games. • Nate enjoys action and sci-fi movies.
• Nate loves watching classics.
Figure9: Promptsforgeneratingobservationsfromconversations. Thepromptusedtogenerateobservations
fromaconversation(top);andanexampleofinputsforthepromptandcorrespondingoutputobservationsfora
sessionfromtheLOCOMOdataset.
gpt-3.5-turbo. We do not report the perfor- out human filtering) as detailed in §3. Three dis-
mance of long-context fine-tuned open-source tinctversionsofthemodelweredeveloped, each
models(Chenetal.,2023)orthoseutilizingsliding withvaryingtrainingdata: (1)Basetrainsonpre-
window (Bertsch et al., 2024; Dao et al., 2022) ceding dialogue turns; (2) + summary trains on
due to the variability inherent across different both prior dialogue turns and a global summary
open-sourcemodelsandthepotentialreductionin of the ongoing conversation; (3) + observation
theircapabilityonshortercontext. trains on both preceding dialogue turns and rele-
vantobservationsretrievedfromtheconversation
EventSummarization. Wepresentexperiments
history. Foreachofthesemodels,westartedwith
conductedintwodistinctconfigurations. Weuse
aMiniGPT-5checkpointpretrainedontheMMDi-
boththeBaseandLong-contextsetupsfromthe
alogdataset(Fengetal.,2023).
questionansweringtask,butwerefrainedfromin-
cludingRAGsincesummarizationrequiresacom-
C.2 ImplementationDetails
prehensive understanding of the entire dialogue,
ratherthanjustretrievingaspecificportion. Ano- WeuseOpenAIAPIandHuggingface(Wolfetal.,
tabledistinctioninourapproach,comparedtothe 2020),asofJanuary2024,withspecificsettingsof
question-answeringtask,liesinourhandlingofthe temperaturesetto0andtop p setto1forevalua-
context. Specifically,weemployaniterativepro- tionoftheLOCOMObenchmark. Allexperiments,
cessofcreatingasummaryofaprecedingsession includingthoseforRAG-basedmodels,MiniGPT-
andthenusethatsummaryasabasistogeneratethe 5 training, and inference, are conducted on an
summaryforthesubsequentsession(Changetal., NvidiaA6000serverwithFP32. Wereportresults
2023). Further,weuseasinglein-contextdemon- fromasingleinferencerunforeachmodelinour
stration of input and output to guide the model experiments. ForMiniGPT-5,weusedthehyper-
towardselectingonlysignificantlifeeventsforthe parametersrecommendedintheoriginalcodebase
summary. andtrainedourmodelsfor10epochs,whichtook
approximately30hoursonasingleA6000GPU.
Multi-modalDialogueGeneration. Forevalu-
WeusethedefaultimplementationsofBLEU11,
ating multi-modal dialogue generation, we train
MiniGPT-5 (Zheng et al., 2023) on 50 conversa-
11https://www.nltk.org/_modules/nltk/translate/
tionsgeneratedusingourautomatedpipeline(with- bleu_score.html→
Let's write short image search queries from textual descriptions of photos shared by a user.
Queries should not include names of people, years and other irrelevant details. For example:
Input: That sounds relaxing, Jeremy! As for video game suggestions, have you ever tried "The Legend
of Zelda: Breath of the Wild"? It's an open-world adventure game that I absolutely love. [shares a
photo of Link standing in front of a breathtaking landscape] Have a look at this stunning view!
Output: the legend of zelda: breath of wild link landscape
Input: {generated_image_caption}
Output:
→
{speaker_1_persona}
{speaker_2_persona}
{speaker_1_name} says, {current_turn}, and shares a photo of {shared_image_caption_blip2}. Write
the most natural question or comment {speaker_2_name} can include in their response.
Figure10: Promptsforimage-sharingandimage-responsebehavior. Thepromptusedtoconvertacaption
generatedbythevirtualagentintoanimagequeryfortheweb-basedimagecrawlerinourpipeline(top),andthe
promptusedtogeneratearesponsegroundedintheimagesharedbyavirtualagentduringaconversationaswellas
thepersonasoftherespectivespeakers(bottom).
Category top-k BLEU-1/2 Rouge-L MM-R
Base - 57.1/34.2 12.4 56.1
+ summary 1 58.2/34.1 12.8 56.9
+ summary 2 56.5/32.8 12.1 55.1
+ summary 5 56.1/32.5 12.0 55.2
+ observation 5 59.7/35.1 13.6 57.8
+ observation 10 59.1/34.9 12.8 57.1
+ observation 25 58.5/34.2 12.0 56.5
Table 6: Multi-modal dialogue generation perfor-
mancecomparisonbetweendifferenttrainingvariants
ofMiniGPT-5. Theoptimalperformanceisshownin
bold.
ROUGE12, BertScore13, FactScore14 metrics in
theirrespectivePythonpackagesinourevaluation
protocol.
D Results
D.1 EventSummarizationTask
Seeanexampleofthefivebroadcategoriesofevent
summarizationerrorsmadebyLLMs,outlinedin
Section6.2,inTable7.
D.2 MultimodalDialogGenerationTask
Results from evaluation of various version of
MiniGPT-5modelonthemultimodaldialoggener-
ationtaskintheLOCOMObenchmarkisinTable6.
12https://pypi.org/project/rouge/
13https://pypi.org/project/bert-score/
14https://github.com/shmsw25/FActScoreRemove or substitute irrelevant Edit inconsistent dialogs Edit dialogs to follow event graph
images
.. Oh, and I'm My grandma Event: Joseph participates in a
planning my solo trip sent me a photography workshop and improves
to five countries! postcard from his photography skills.
Exciting stuff. Paris years ago.
Wow, Debra! 5
.. Anything new at work or
countries, awesome! .. Where'd you get it? …
fun for the weekend?
Where're you headed?
Need help planning or
any tips? Count me in! Thanks Joe! I got the .. I did a photoshoot last
postcard from an antique Friday and learned some
shop… new tricks. ..
Thanks Joe! My .. I participated in a
grandmother got the photography workshop last
postcard from an antique Friday and learned some
shop… new tricks. ..
Figure11: Exampleofeditsmadebyannotators. HumanannotatorsareinstructedtomakeeditsintheLLM-
generatedconversationstoremoveirrelevantThepromptusedtogeneratecompletepersonasfortheLLMsinour
conversationgenerationpipeline(top)andexamplesofpersonaspresentintheLOCOMOdataset.
ErrorType Explanation Groundtrutheventorrelevantdialogs Predictedevent
Missing Keydetailsabouteventareomittedbecause Joannasubmitsherthirdscreenplayonloss,identity,and Joannasubmitsherrecent
information themodelfailstomakecausalandtemporal connectiontoafilmcontest screenplaytoafilmcontest.
connectionsoveralongconversation.
Hallucination Non-existentdetailsordetailsfroma N:‘Thegamingpartywasagreatsuccess!’ Nate’sveganicecreamisahuge
differenteventarepaddedontoanevent N:‘...saidthey’dwanttodoitagainnextmonth!’ successandpeoplewanttodoit
N:‘Onanothernote,Imadeveganicecream...’ againnextmonth.
Misunder- e.g.,modelconfusesalight-hearted J:‘..thesetrailsthatmademefeellikewritingadrama.’ Nateconsiderswritinghisown
-standingof statementfromaspeakerasaserious N:‘..gotogether..MaybeI’llstarttothinkofadrama dramascreenplay.
dialogcues statement myselfandwriteascreenplay...’
J:‘Haha,nowthatwouldbesomething!...’
Speaker Eventisattributedtothewrongspeaker NateinvitesJoannatotryhishomemadelactose-freeice JoannainvitesNatetoherhometo
attribution cream. tryherdairy-freeicecreamrecipe.
Saliency Unimportantinteractionsintheconversation N:HeyJoanna,what’sbeenupsincewelastchatted? NateasksJoannahowshehasbeen
areconsideredsignificantbymodel How’sitgoing? shetheylasttalked.
Table7: TaxonomyoferrorsinLLM-generatedeventsummaries. Fivetypesoferrorspredominantlyoccurin
theeventsummariesgeneratedbyLLMs. Examplesarebasedonpredictionsfromgpt-3.5-turbo.