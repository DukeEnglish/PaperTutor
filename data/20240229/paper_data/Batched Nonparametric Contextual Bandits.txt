Batched Nonparametric Contextual Bandits
Rong Jiang1 and Cong Ma2
1Committee on Computational and Applied Mathematics, University of Chicago
2Department of Statistics, University of Chicago
February 28, 2024
Abstract
We study nonparametric contextual bandits under batch constraints, where the expected reward for
each action is modeled as a smooth function of covariates, and the policy updates are made at the
end of each batch of observations. We establish a minimax regret lower bound for this setting and
propose Batched Successive Elimination with Dynamic Binning (BaSEDB) that achieves optimal regret
(up to logarithmic factors). In essence, BaSEDB dynamically splits the covariate space into smaller bins,
carefully aligning their widths with the batch size. We also show the suboptimality of static binning
underbatchconstraints,highlightingthenecessityofdynamicbinning. Additionally,ourresultssuggest
that a nearly constant number of policy updates can attain optimal regret in the fully online setting.
1 Introduction
Recentyearshavewitnessedsubstantialprogressinthefieldofsequentialdecisionmakingunderuncertainty.
Especiallynoteworthyaretheadvancementsinpersonalizeddecisionmaking, wherethedecisionmakeruses
side-information to make customized decision for a user. The contextual bandit framework has been widely
adopted to model such problems because of its capability and elegance [35, 53, 6]. In this framework, one
interactswithanenvironmentforanumberofrounds: ateachround,oneisgivenacontext,picksanaction,
and receives a reward. One can update the action-assignment policy based on previous observations and
the goal is to maximize the expected cumulative rewards. For example, in online news recommendation,
a recommendation algorithm selects an article for each newly arrived user based on the user’s contextual
information, and observes whether the user clicks the article or not. The goal is to try to maximize the
number of clicks received. Apart from news recommendation, contextual bandits have found numerous
applications in other fields such as clinical trials, personalized medicine, and online advertising [30, 62, 13].
At the core of designing a contextual bandit algorithm is deciding how to update the policy based on
prior observations. A standard metric of performance for bandit algorithms is regret, which is the expected
difference between the cumulative rewards obtained by an oracle who knows the optimal action for every
contextandthatobtainedbytheactualalgorithmunderconsideration. Manyexistingregretoptimalbandit
algorithmsrequireapolicyupdateperobservation(unit)[4,1,39,34]. Atafirstglance,suchfrequentpolicy
updatesareneededsothatthealgorithmcanquicklylearntheoptimalactionundereachcontextandreduce
regret. However, this kind of algorithm ignores an important concern in the practice of sequential decision
making—the batch constraint.
In many real world scenarios, the data often arrive in batches: the statistician can only observe the
outcomes of the policy at the end of a batch, and then decides what to do for the next batch. For example,
thisbatchconstraintisubiquitousinclinicaltrials: statisticiansneedtodividetheparticipantsintobatches,
determine a treatment allocation policy before the batch starts, and then observe all the outcomes at the
end of the batch [49]. Policy updates are made per batch instead of per unit. In fact, it is infeasible to
apply unit-wise policy update in this case because observing the effect of a treatment takes time and if one
waits forthe result before deciding howto treatthe next patient, theentireexperiment willtaketoolong to
completewhenthenumberofparticipantsishuge. Thebatchconstraintalsoappearsinareassuchasonline
1
4202
beF
72
]TS.htam[
1v23771.2042:viXramarketing, crowdsourcing, and simulations [8, 50, 31, 15]. Clearly, the batch constraint presents additional
challenges to online learning. Indeed, from an information perspective, the statistician’s information set is
largely restricted since she can only observe all the responses at the end of a batch. The following questions
naturally arise:
Given a batch budget M and a total number of T rounds, how should the statistician determine the size
of each batch, and how should she update the policy after each batch? Can the statistician design batch
learning algorithms that achieve regret performances on par with the fully online setting using as few policy
updates as possible?
1.1 Main contributions
In this work, we address the aforementioned questions under a classical framework for personalized decision
making—nonparametriccontextualbandits[48,39]. Inthisframework,theexpectedrewardassociatedwith
each treatment (or arm in the language of bandits) is modeled as a nonparametric smooth function of the
covariates[59]. Inthefullyonlinesetup,seminalworks[48,39]establishtheminimaxoptimalregretbounds
for the nonparametric contextual bandits. Nevertheless, under the more challenging setting with the batch
constraint, the fundamental limits for nonparametric bandits remain unknown. Our paper aims to bridge
this gap. More concretely, we make the following three novel contributions:
• First, we establish a minimax regret lower bound for the nonparametric bandits with the batch con-
straint M. Our proof relies on a simple but useful insight that the worst-case regret over the entire
horizon is greater than the worst-case regret over the first i batches for all 1≤i≤M. To fully exploit
thisinsight,foreachdifferentbatchnumberi,weconstructdifferentfamiliesofhardinstancestotarget
this batch, leading to a maximal regret over this batch.
• In addition, we demonstrate that the aforementioned lower bound is tight by providing a matching
upper bound (up to log factors). Specifically, we design a novel algorithm—Batched Successive Elimi-
nationwithDynamicBinning(BaSEDB)—forthenonparametricbanditswithbatchconstraints. BaSEDB
progressively splits the covariate space into smaller bins whose widths are carefully selected to align
wellwiththecorrespondingbatchsize. Thedelicateinterplaybetweenthebatchsizeandthebinwidth
is crucial for obtaining the optimal regret in the batch setting.
• On the other hand, we show the suboptimality of static binning under the batch constraint by proving
analgorithm-specificlowerbound. Unlikethefullyonlinesettingwherepoliciesthatuseafixednumber
ofbinscanattaintheoptimalregret[39],ourlowerboundindicatesthatbatchedsuccessiveelimination
with static binning is strictly suboptimal. This highlights the necessity of dynamic binning in some
sense under the batch setting, which is uncommon in classical nonparametric estimation.
It is also worth mentioning that an immediate consequence of our results is that M ≳ loglogT number of
batches suffices to achieve the optimal regret in the fully online setting. In other words, we can use a nearly
constantnumberofpolicyupdatesinpracticetoachievetheoptimalregretobtainedbypoliciesthatrequire
one update per round.
1.2 Related work
Nonparametric contextual bandits. [58]introducedthemathematicalframeworkofcontextualbandit.
The theory of contextual bandits in the fully online setting has been continuously developed in the past few
decades. On one hand, [4, 1, 23, 6, 7, 41] obtained learning guarantees for linear contextual bandits in
both low and high dimensional settings. On the other hand, [59] introduced the nonparametric approach
to model the mean reward function. [48] proved a minimax lower bound on the regret of nonparametric
bandit and developed an upper-confidence-bound (UCB) based policy to achieve a near-optimal rate. [39]
improved this result and proposed the Adaptively Binned Successive Elimination (ABSE) policy that can
alsoadapttotheunknownmarginparameter. Furtherinsightsinthisnonparametricsettingweredeveloped
in subsequent works [42, 43, 45, 24, 27, 52, 25, 10, 51, 9]. The smoothness assumption is also adopted in
another line of work [37, 36, 33, 11] on the continuum-armed bandit problems. However in contrast to what
we study, the reward is assumed to be a Lipschitz function of the action, and the covariates are not taken
into considerations.
2Batchlearning. Thebatchconstrainthasreceivedincreasingattentioninrecentyears. [40,21]considered
the multi-armed bandit problem under the batch setting and showed that O(loglogT) batches are adequate
inachievingtherate-optimalregret,comparedtothefullyonlinesetting. [26,47]extendedbatchlearningto
the(generalized)linearcontextualbanditsand[46,56,17]furtherstudiedthesettingwithhigh-dimensional
covariates. [29,28]establishedbatchlearningguaranteesfortheThompsonsamplingalgorithm. [18]consid-
eredLipschitzcontinuum-armedbanditproblemwiththebatchconstraint. Inferenceforbatchedbanditswas
consideredin[60]. Aconceptrelatedtobatchlearninginliteratureiscalleddelayedfeedback[14,13,55,19].
These works consider the setting where rewards are observed with delay and analyze effects of delay on the
regret. [32, 2] studied delayed feedback in nonparametric bandits and the key difference to batch learning is
that the batch size is given, whereas in our case, it is a design choice by the statistician. Batch learning’s
focus is different to that of delayed feedback in the sense that the former gives the decision maker discretion
to choose the batch size which makes it possible to approximate the optimal standard online regret with a
small number of batches. Finally, the notion switching cost is intimately related to the batch constraint.
[12] studied online learning with low switching cost and obtained minimax optimal regret with O(loglogT)
batches. [5, 61, 20, 57, 44] developed regret guarantees with low switching cost for reinforcement learning.
Low switching cost can be interpreted as infrequent policy updates, but it does not require the learner to
divide the samples into batches with feedback only becoming available at the end of a batch.
2 Problem setup
We begin by introducing the problem setup for nonparametric bandits with the batch constraint.
A two-arm nonparametric bandit with horizon T ≥ 1 is specified by a sequence of independent and
identically distributed random vectors
(X ,Y(1),Y(−1)), for t=1,2,...,T, (1)
t t t
where X is sampled from a distribution P . Throughout the paper, we assume that X ∈X :=[0,1]d, and
t X t
P hasadensity(w.r.t.theLebesguemeasure)thatisboundedbelowandabovebysomeconstantsc,c¯>0,
X
respectively. For k ∈{1,−1} and t≥1, we assume that Y(k) ∈[0,1] and that
t
E[Y(k) |X ]=f(k)(X ).
t t t
Here f(k) is the unknown mean reward function for the arm k.
Without the batch constraint, the game of nonparametric bandits plays sequentially. At each step t, the
statistician observes the context X , and pulls an action A ∈{1,−1} according to a rule π :X (cid:55)→{1,−1}.
t t t
Then she receives the corresponding reward Y(At). In this case, the rule π for selecting the action at time
t t
t is allowed to depend on all the observations strictly anterior to t.
In an M-batch game, the statistician is asked to divide the horizon [1 : T] into M disjoint batches
[1:t ], [t +1:t ], ...,[t +1,T]. In contrast to the case without the batch constraint, only the rewards
1 1 2 M−1
associated with timesteps prior to the current batch are observed and available for making decisions for the
current batch. More formally, an M-batch policy is composed of a pair (Γ,π), where Γ = {t t t } is a
0, 1,..., M
partition of the entire time horizon T that satisfies 0 = t < t < ... < t < t = T, and π = {π }T
0 1 M−1 M t t=1
is a sequence of random functions π : X (cid:55)→ {1,−1}. Let Γ(t) be the batch index for the time t, i.e., Γ(t)
t
is the unique integer such that t < t ≤ t . Then at time t, the available information for π is only
Γ(t)−1 Γ(t) t
{X }t ∪{Y(Al)}Γ(t)−1, which we denote by Ft. The statistician’s policy π at time t is allowed to depend
l l=1 l l=1 t
on F .
t
ThegoalofthestatisticianistodesignanM-batchpolicy(Γ,π)thatcancompetewithanoraclethathas
perfect knowledge (i.e., the law of (X ,Y(1),Y(−1))) of the environment. Formally, we define the cumulative
t t t
regret as
(cid:34) T (cid:35)
(cid:88)(cid:16) (cid:17)
R (π):=E f⋆(X )−f(πt(Xt))(X ) , (2)
T t t
t=1
where f⋆(x):=max f(k)(x) is the maximum mean reward one could obtain on the context x. Note
k∈{1,−1}
here we omit the dependence on Γ for simplicity.
32.1 Assumptions
We adopt two standard assumptions in the nonparametric bandits literature [48, 39]. The first assumption
is on the smoothness of the mean reward functions.
Assumption 1 (Smoothness). We assume that the reward function for each arm is (β,L)-smooth, that is,
there exist β ∈(0,1] and L>0 such that for k ∈{1,−1},
|f(k)(x)−f(k)(x′)|≤L∥x−x′∥β
2
holds for all x,x′ ∈X.
The second assumption is about the separation between the two reward functions.
Assumption 2 (Margin). We assume that the reward functions satisfy the margin condition with parameter
α>0, that is there exist δ ∈(0,1) and D >0 such that
0 0
(cid:16) (cid:12) (cid:12) (cid:17)
P 0<(cid:12)f(1)(X)−f(−1)(X)(cid:12)≤δ ≤D δα
X (cid:12) (cid:12) 0
holds for all δ ∈[0,δ ].
0
Assumption2isrelatedtothemarginconditioninclassification[38,54,3]andisintroducedtobanditsin
[22, 48, 39]. The margin parameter affects the complexity of the problem. Intuitively, a small α, say α≈0,
means the two mean functions are entangled with each other in many regions and hence it is challenging to
distinguish them; a large α, on the other hand, means the two reward functions are mostly well-separated.
From now on, we use F(α,β) to denote the class of nonparametric bandit instances (i.e., distributions
over (1)) that satisfy Assumptions 1-2.
Remark 1. Throughout the paper, we assume that αβ ≤ 1. By proposition 2.1 from [48], when αβ > 1,
one of the arms will dominate the other one for the entire covariate space. The instance is reduced to a
multi-armed bandit without covariates which is not the interest of the current paper. Therefore, we focus on
the case αβ ≤1 hereafter.
3 Fundamental limits of batched nonparametric bandits
Somewhat unconventionally, we start with stating a minimax lower bound, as well as its proof, for regret
minimization in batched nonparametric contextual bandits. As we will soon see, the proof of the lower
bound is extremely instrumental in our development of an optimal M-batch policy (Γ,π), to be detailed in
Section 4.
Recall that F(α,β) denotes the class of nonparametric bandit instances (i.e., distributions over (1)) that
obey Assumptions 1-2. We have the following minimax lower bound for any M-batch policy, in which we
define
β(1+α)
γ := ∈(0,1).
2β+d
Theorem 1. Suppose that αβ ≤1, and assume that P is the uniform distribution on X =[0,1]d. For any
X
M-batch policy (Γ,π), there exists a nonparametric bandit instance in F(α,β) such that the regret of (Γ,π)
on this instance is lower bounded by
E[R T(π)]≥D˜T1−1− γγ M,
where D˜ >0 is a constant independent of T and M.
See Section 3.1 for the proof of this lower bound.
As a sanity check, one sees that as M increases, the lower bound decreases. This is intuitive, as the
policyismorepowerfulasM increases. Asaresult, theproblemofbatchednonparametricbanditsbecomes
easier.
43.1 Proof of Theorem 1
Let (Γ,π) be the M-batch policy under consideration, with
Γ={t =0,t ,t ,...,t =T}.
0 1 2 M
Throughout this proof, we consider Bernoulli reward distributions, that is Y(1),Y(−1) are Bernoulli random
t t
variables with mean f(1)(X ), and f(−1)(X ), respectively. In addition, we fix f(−1)(x) = 1. Let f be the
t t 2
mean reward function of the first arm. To make the dependence on the reward instance clear, we write the
cumulative regret up to time n as R (π;f).
n
Our proof relies on a simple observation: the worst-case regret over [T] is larger than the worst-case
regret over the first i batches. Formally, we have
sup R (π;f)≥ max sup R (π;f). (3)
(f,1)∈F(α,β)
T
1≤i≤M (f,1)∈F(α,β)
ti
2 2
Though simple, this observation lends us freedom on choosing different families of instances in F(α,β)
targeting different batch indices i.
Our proof consists of four steps. In Step 1, we reduce bounding the regret of a policy to lower bounding
its inferior sampling rate to be defined. In Step 2, we detail the choice of different families of instances for
each different batch index i. Then in Step 3, we apply an Assouad-type of argument to lower bound the
average inferior sampling rate of the family of hard instances. Lastly in Step 4, we combine the arguments
to complete the proof.
Step 1: Relating regret to inferior sampling rate. Given an M-batch policy, we define its inferior
sampling rate at time n on an instance (f,1) to be
2
(cid:34) n (cid:35)
(cid:88) 1
S (π;f):=E 1{π (X )̸=π⋆(X ),f(X )̸= } .
n t t t t 2
t=1
In words, S (π;f) counts the number of times π selects the strictly suboptimal arm up to time n. Thanks
n
to the following lemma, we can reduce lower bounding the regret to the inferior sampling rate.
Lemma 1 (Lemma 3.1 in [48]). Suppose that (f,1)∈F(α,β). Then for any 1≤n≤T, we have
2
S n(π;f)≤Dn1+1 αR n(π;f)1+α α,
for some constant D >0.
As an immediate consequence of the above lemma, we obtain
1 1+α −1 1+α
(f,1)s ∈u Fp (α,β)R T(π;f)≥ 1≤m ia ≤x
M
(f,1)s ∈u Fp (α,β)( D) α t i α(S ti(π;f)) α
2 2
(cid:34) (cid:35)1+α
1 1+α −1 α
=( D) α 1≤m ia ≤x Mt i α (f,1)s ∈u Fp (α,β)S ti(π;f) .
2
From now on, we focus on lower bounding sup S (π;f).
(f,1 2)∈F(α,β) ti
Step 2: Introducing the family of reward instances for t . Our construction of the family of hard
i
instances is adapted from [48]. Define z = 1, and z = ⌈(t 1/(2β+d)⌉ for i = 2,3,...,M. Henceforth, we
1 i i−1
will fix some i and write z as z. We partition [0,1]d into zd bins with equal width. Denote the bins by C
i j
for j =1,...,zd, and let q be the center of C .
j j
Define a set of binary sequences Ω := {±1}s, with s := ⌈zd−αβ⌉. For each ω ∈ Ω we define a function
s s
f :[0,1]d (cid:55)→R:
ω
s
1 (cid:88)
f (x)= + ω φ (x),
ω 2 j j
j=1
5whereφ (x)=D z−βϕ(2z(x−q ))1{x∈C }withϕ(x)=(1−∥x∥ )β1{∥x∥ ≤1},andD =min(2−βL,1/4).
j ϕ j j ∞ ∞ ϕ
In all, we consider the family of reward instances
(cid:26) (cid:27)
1
C := f(1)(x)=f (x),f(−1)(x)= |ω ∈Ω .
z ω 2 s
With slight abuse of notation, we also use C to denote {f : ω ∈ Ω }. It is straightforward to check that
z ω s
C ⊆F(α,β).
z
Step 3: Lower bounding the inferior sampling rate. Fix some i ∈ [M], and consider z = z . Since
i
C ⊆F(α,β), we have
z
sup S (π;f)≥ sup S (π;f).
ti ti
(f,1 2)∈F(α,β) f∈Cz
Using the definitions of C and S (π;f), we have
z ti
(cid:34) (cid:88)ti
1 1
(cid:35)
sup S (π;f)= sup E 1{π (X )̸=sign(f (X )− ),f (X )̸= }
ti π,fω t t ω t 2 ω t 2
f∈Cz ω∈Ωs
t=1
1 (cid:88)
(cid:34) (cid:88)ti
1 1
(cid:35)
≥ E 1{π (X )̸=sign(f (X )− ),f (X )̸= } .
2s π,fω t t ω t 2 ω t 2
ω∈Ωs t=1
Since f (x)= 1 for x∈/ ∪ C , we further obtain
ω 2 j=1,...s j
1 (cid:88)
(cid:88)ti (cid:88)s
sup S (π;f)≥ Et [1{π (X )̸=ω ,X ∈C }]. (4)
ti 2s π,fω t t j t j
f∈Cz
ω∈Ωst=1j=1
Here we use Pt to denote the joint distribution of {X }t ∪{Yπl(Xl)}Γ(t)−1 , where Γ(t) is the batch
π,fω l l=1 l l=1
index for t, i.e., the unique integer such that t < t ≤ t . We use Et to denote the corresponding
expectation. Expand the right hand side of
(4)Γ( tt o)− s1
ee that
Γ(t) π,fω
1
(cid:88)s (cid:88)ti
(cid:88) (cid:88)
sup S (π;f)≥ Et [1{π (X )̸=h,X ∈C }], (5)
f∈Cz
ti 2s
j=1t=1ω[−j]∈Ωs−1h∈{±1}
π,f ω[h
−j]
t t t j
(cid:124) (cid:123)(cid:122) (cid:125)
Wj,t,ω[−j]
whereωh isthesameasω exceptforthej-thentrybeingh. Notethathereweusethefactthatforf ,
[−j] ωh
[−j]
the optimal arm in the bin C is h. We then relate W to a binary testing error,
j j,t,ω[−j]
1 (cid:88)
W = Pt (π (X )̸=h|X ∈C )
j,t,ω[−j] zd
h∈{±1}
π,f ω[h
−j]
t t t j
(cid:20) (cid:21)
1
≥ exp −KL(Pt ,Pt ) , (6)
4zd π,f ω[− −1
j]
π,f ω[1
−j]
wherethesecondstepinvokesLeCam’smethod. Underthebatchsetting,attimet,theavailableinformation
is only up to t . Consequently, the KL divergence KL(Pt ,Pt ) can be controlled by
Γ(t)−1 π,f ω[− −1
j]
π,f ω[1
−j]
(i) tΓ (cid:88)(t)−1
KL(Pt−1 ,Pt−1 ) ≤ 8E [ (f (X )−f (X ))21{π (X )=1}]
π,f ω[− −1 j] π,f ω[1 −j] π,f ω[− −1 j] t=1 ω [− −1 j] t ω [1 −j] t t t
(ii) tΓ (cid:88)(t)−1
≤ 32D2z−2βE [ 1{π (X )=1,X ∈C }]
ϕ π,f ω[− −1
j] t=1
t t t j
6tΓ(t)−1
( =iii) 32D2z−(2β+d) (cid:88) Pt (π (X )=1|X ∈C )
ϕ
t=1
π,f ω[− −1
j]
t t t j
(iv)
≤ 32D2z−(2β+d)t . (7)
ϕ Γ(t)−1
Here, step (i) uses the standard decomposition of KL divergence and Bernoulli reward structure; step (ii) is
due to the definition of f ; step (iii) uses P(X ∈ C ) = 1/zd, and step (iv) arises from Pt (π (X ) =
ω t j π,f ω[− −1
j]
t t
1|X ∈C )≤1 for any 1≤t≤T. Combining (5), (6), and (7), we arrive at
t j
1(cid:88)s (cid:88)ti
1 (cid:16) (cid:17)
sup S (π;f)≥ exp −32D2z−(2β+d)t
ti 8 zd ϕ Γ(t)−1
f∈Cz
j=1t=1
zd−αβ i
≥ 1 (cid:88) (cid:88)t l−t l−1 exp(cid:16) −32D2z−(2β+d)t (cid:17)
8 zd ϕ l−1
j=1 l=1
zd−αβ i
≥ 1 (cid:88) (cid:88)t l−t l−1 exp(cid:16) −32D2z−(2β+d)t (cid:17) ,
8 zd ϕ i−1
j=1 l=1
where the second line uses the fact that s = ⌈zd−αβ⌉, and the last inequality holds since t ≤ t for all
l−1 i−1
1 ≤ l ≤ i. Now recall that z = z = ⌈(t )1/(2β+d)⌉ for i ≥ 1, and z = 1 for i = 1. We can continue the
i i−1
lower bound to see that
zd−αβ i
sup S (π;f)≥ 1 (cid:88) (cid:88)t l−t l−1 exp(cid:16) −32D2z−(2β+d)t (cid:17)
ti 8 zd ϕ i−1
f∈Czi
j=1 l=1
zd−αβ i
≥c⋆ (cid:88) (cid:88)t l−t l−1
zd
j=1 l=1

=c⋆· t i
=c⋆· t2βαti
+β
d, i>1
,
zαβ i−1
c⋆t , i=1
1
for some c⋆ >0.
Step 4: Combining bounds together. Combining the previous arguments together leads to the con-
clusion that
sup R (π;f)≥ max sup R (π;f)
(f,1 2)∈F(α,β)
T
1≤i≤Mf∈Czi
ti
(cid:34) (cid:35)1+α
1 1+α −1 α
≥( D) α 1≤m ia ≤x Mt i α fs ∈u Cp ziS ti(π;f)
(cid:26) (cid:27)
t T
≳max t , 2,..., (8)
1 tγ tγ
1 M−1
≥D˜T1−1− γγ
M.
This finishes the proof.
3.2 Implications on design of the optimal M-batch policy
Aswehavementioned,theproofofthelowerbound,i.e.,Theorem1facilitatesthedesignofoptimalM-batch
policy.
7Algorithm 1 Batched successive elimination with dynamic binning (BaSEDB)
Input: Batch size M, grid Γ={t }M , split factors {g }M−1.
i i=0 i i=0
L←B
1
for C ∈L do
I =I
C
for i=1,...,M −1 do
for t=t +1,...,t do
i−1 i
C ←L(X )
t
Pull an arm from I in a round-robin way.
C
if t=t then
i
Update L and {I } by Algorithm 2 (L,{I } ,i,g ).
C C∈L C C∈L i
for t=t +1,...,T do
M−1
C ←L(X )
t
Pull any arm from I .
C
Grid selection. First, the lower bound of the whole horizon is reduced to the worst-case regret over a
specific batch; see (3). Consequently, we need to design the grid Γ=(t ,t ,t ,...,t ,t ) such that the
0 1 2 M−1 M
total regret is evenly distributed across batches. More concretely, in view of the lower bound (8), one needs
to set t
1
≍ tγti ≍T1−1− γγ M for 2≤i≤M.
i−1
Dynamic binning. Inaddition,intheproofofthelowerbound,foreachdifferentbatchi,weusedifferent
families of hard reward instances, parameterized by the number of bins z = ⌈t1/(2β+d)⌉. In other words,
i i−1
from the lower bound perspective, the granularity (i.e., the bin width 1/z ) at which we investigate the
i
mean reward function depends crucially on the grid points {t }: the larger the grid point t , the finer the
i i
granularity. This key observation motivates us to consider the batched successive elimination with dynamic
binning algorithm to be introduced below.
4 Batched successive elimination with dynamic binning
In this section, we present the batched successive elimination with dynamic binning policy (BaSEDB) that
nearly attains the minimax lower bound, up to log factors; see Algorithm 1. On a high level, Algorithm 1
gradually partitions the covariate space X into smaller hypercubes (i.e., bins) throughout the batches based
on a list of carefully chosen cube widths, and reduces the nonparametric bandit in each cube to a bandit
problem without covariates.
A tree-based interpretation. The process is best illustrated with the notion of a tree T of depth M;
see Figure 1. Each layer of of the tree T is a set of bins that form a regular partition of X using hypercubes
with equal widths. And the common width of the bins B in layer i is dictated by a list {g }M−1 of split
i i i=0
factors. More precisely, we let
i−1
(cid:89)
w :=( g )−1 (9)
i l
l=0
bethewidthofthecubesinthei-thlayerB fori≥1,andw =1. Inotherwords,B containsallthecubes
i 0 i
C ={x∈X :(v −1)w ≤x <v w ,1≤j ≤d},
i,v j i j j i
where v =(v ,v ,...,v )∈[ 1 ]d. As a result, there are in total ( 1 )d bins in B .
1 2 d wi wi i
8Algorithm 2 Tree growing subroutine
Input: Active nodes L, active arm sets {I } , batch number i, split factor g .
C C∈L i
L′ ←{}
for C ∈L do
if |I |=1 then
C
L′ ←L′∪{C}
Proceed to next C in the iteration.
Y¯max ←max Y¯(k)
C,i k∈IC C,i
for k ∈I do
C
if Y¯max−Y¯(k) >U(m ,T,C) then I ←I −{k}
C,i C,i C,i C C
if |I |>1 then
C
I ←I for C′ ∈child(C,g )
C′ C i
L′ ←L′∪child(C,g )
i
else
L′ ←L′∪{C}
Return L′
Algorithm 1 proceeds in batches and maintains two key objects: (1) a list L of active bins, and (2) the
corresponding active arms I for each C ∈ L; see Figure 1 for an example. Specifically, prior to the game
C
(i.e., prior to the first batch), L is set to be B , all bins in layer 1, and I ={1,−1} for all C ∈L. Within
1 C
this batch, the statistician tries the arms in I equally likely for all bins in L. Then at the end of the
C
batch, given the revealed rewards in this batch, we update the active arms I for each C ∈L via successive
C
elimination. If no arm were eliminated from I , this suggests that the current bin is not fine enough for the
C
statisticiantotellthedifferencebetweenthetwoarms. Asaresult, shesplitsthebinC ∈Lintoitschildren
child(C) in T. All the child nodes will be included in L, while the parent C stops being active (i.e., C is
removed from L). The whole process repeats in a batch fashion. 1
When to eliminate arms? Now we zoom in on the elimination process described in Algorithm 2. The
basic idea follows from successive elimination in the bandit literature [16, 39, 21]: the statistician eliminates
an arm from I if she expects the arm to be suboptimal in the bin C given the rewards collected in C.
C
Specifically, for any node C ∈T, define
(cid:114)
log(2T|C|d)
U(τ,T,C):=4 ,
τ
where |C| denotes the width of the bin. Let m := (cid:80)ti 1{X ∈ C} be the number of times we
C,i t=ti−1+1 t
observe contexts from C in batch i. We then define for k ∈{1,−1} that
(cid:80)ti
Y ·1{X ∈C,A =k}
Y¯(k) := t=ti−1+1 t t t ,
C,i (cid:80)ti 1{X ∈C,A =k}
t=ti−1+1 t t
which is the empirical mean reward of arm k in node C during the i-th batch. It is easy to check that Y¯(k)
C,i
has expectation f¯(k) given by
C
1 (cid:90)
f¯(k) :=E[f(k)(X)|X ∈C]= f(k)(x)dP (x).
C P (C) X
X C
1For the final batch M, the split factor gM−1 = 1 by default because there is no need to further partition the nodes for
estimation.
9[0,1]
[0,1) [1,1) [1,3) [3,1]
4 4 2 2 4 4
[0, 1 ) [ 1 ,1) [1,1) [3,5) [5,11) [11,1]
12 12 6 6 4 4 6 6 12 12
Figure 1: An example of the tree growing process for d=1,M =3,G={4,3,1}. The root node is at depth
0. For the first batch, the 4 nodes located at depth 1 of the tree were used. Both [1,1) and [1,3) only
4 2 2 4
had one active arm remaining so they were not further split and remained in the set of active nodes (green).
Meanwhile, |I | = |I | = 2 so each of them was split into 3 smaller nodes, and both nodes were
[0,1) [3,1]
marked as inacti4ve (red).4For the second batch, all the green nodes were actively used but arm elimination
was performed at the end of batch 2 only for nodes located at depth 2 (the green nodes at depth 1 already
have 1 active arm remaining so there is no need to eliminate again).
Similarly, we define the average optimal reward in bin C to be
1 (cid:90)
f¯⋆ := f⋆(x)dP (x).
C P (C) X
X C
TheeliminationthresholdU(m ,T,C)ischosensuchthatanarmk withf¯⋆−f¯(k) ≫|C|β iseliminated
C,i C C
with high probability at the end of batch i. Therefore, when |I | > 1, the remaining arms are statistically
C
indistinguishable from each other, so C is split into smaller nodes to estimate those arms more accurately
using samples from future batches. On the other hand, when |I | = 1, the remaining arm is optimal in
C
C with high probability—a consequence of the smoothness condition, and it will be exploited in the later
batches.
Grid Γ and split factors {g }M−1. Asonecansee, thesplitfactorg controlshowmanychildrenanode
i i=0 i
at layer i can have and its appropriate choice is crucial for obtaining small regret. Intuitively, g should be
i
selectedinawaysuchthatanodeC withwidthw canfullyleveragethenumberofsamplesallocatedto
i+1 i
it during the (i+1)-th batch. With these goals in mind, we design the grid Γ={t } and split factors {g }
i i
as follows. Recall that γ = β(1+α). We set
2β+d
(cid:16) 1−γ (cid:17)
b=Θ T1−γM .
The split factors are chosen according to
g
0
=⌊b2β1 +d⌋, and g
i
=⌊g iγ −1⌋,i=1,...,M −2. (10)
In addition, the grid is chosen such that
t −t =⌊l w−(2β+d)log(Twd)⌋,1≤i≤M −1, (11)
i i−1 i i i
10where l >0 is a constant to be specified later. It is easy to check that with these choices, we have
i
t
1
≍T1−1− γγ M, and t
i
=⌊b(t i−1)γ⌋, for i=2,...,M.
In particular, we set b properly to make t =T. Indeed, these choices taken together meet the expectation
M
laid out in Section 3.2: we need to choose the grid and the split factors appropriately so that (1) the total
regretspreadsoutacrossdifferentbatches, and(2)thegranularitybecomesfineraswemovefurthertolater
batches.
Connections and differences with ABSE in [39]. In appearance, BaSEDB (Algorithm 1) looks quite
similar to the Adaptively Binned Successive Elimination (ABSE) proposed in [39]. However, we would
like to emphasize several fundamental differences. First, the motivations for the algorithms are completely
different. [39] designs ABSE to adapt to the unknown margin condition α, while our focus is to tackle the
batch constraint. In fact, without the batch constraints, if α is known, adaptive binning is not needed to
achieve the optimal regret [39]. This is certainly not the case in the batched setting. Fixing the number of
bins used across different batches is suboptimal because one can construct instances that cause the regret
incurred during a certain batch to explode. We will expand on this phenomenon in Section 4.3. Secondly,
thealgorithmin[39]partitionsabinintoafixed number2d ofsmalleronesoncetheoriginalbinisunableto
distinguish the remaining arms. In this way, the algorithm can adapt to the difference in the local difficulty
of the problem. In comparison, one of our main contributions is to carefully design the list of varying split
factorsthatallowsthenew cubes tomaximallyutilizethe numberofsamples allocatedtoitduringthe next
batch.
4.1 Regret guarantees
Now we are ready to present the regret performance of BaSEDB (Algorithm 1).
Theorem 2. Suppose that αβ ≤ 1. Fix any constant D > 0 and suppose that M ≤ D logT. Equipped
1 1
with the grid and split factors list that satisfy (11) and (10), the policy πˆ given by Algorithm 1 obeys
E[R T(πˆ)]≤C˜(logT)2·T1−1− γγ M,
where C˜ >0 is a constant independent of T and M.
See Section 5 for the proof.
While Theorem 2 requires M ≲logT, we see from the corollary below that it is in fact sufficient to show
the optimality of Algorithm 1.
Corollary 1. As long as M ≥D loglog(T), where D depends on γ = β(1+α), Algorithm 1 achieves
2 2 2β+d
E[R (πˆ)]≤C˜(logT)2·T1−γ,
T
where C˜ >0 is a constant independent of T and M.
Theorem 2, together with Corollary 1 and Theorem 1 establish the fundamental limits of batch learning
forthenonparametricbanditswithcovariates,aswellastheoptimalityof BaSEDB,uptologarithmicfactors.
To see this, when M ≲ loglog(T), the upper bound in Theorem 2 matches the lower bound in Theorem 1,
apart from log factors. On the other end, when M ≳ loglog(T), Algorithm 1, while splitting the horizon
into M batches, achieves the optimal regret (up to log factors) for the setting without the batch constraint
[39]. It is evident that Algorithm 1 is optimal in this case.
4.2 Numerical experiments
In this section, we provide some experiments on the empirical performance of Algorithm 1. We set T =
50000,d = β = 1,α = 0.2. We let P be the uniform distribution on [0,1]. Denote q = (j −1/2)/4 and
X j
11Figure 2: Regret vs. batch budget M.
C = [q −1/8,q +1/8] for 1 ≤ j ≤ 4. For the mean reward functions, we choose f(1),f(−1) : [0,1] → R
j j j
such that
4
1 (cid:88) 1
f(1)(x)= + ω φ (x), f(−1)(x)= ,
2 j j 2
j=1
where ω′s are sampled i.i.d. from Rad(1), φ (x)= 1ϕ(8(x−q ))1{x∈C } and ϕ(x)=(1−|x|)1{|x|≤1}.
j 2 j 4 j j
We let Y(k) ∼ Bernoulli(f(k)(x)). To illustrate the performance of Algorithm 1, we compare it with the
Binned Successive Elimination (BSE) policy from [39], which is shown to be minimax optimal in the fully
online case. Figure 2 shows the regret of Algorithm 1 under different batch budegts. One can see that it is
sufficient to have M =5 batches to achieve the fully online efficiency.
4.3 Failure of static binning
Wehaveseenthepowerofdynamicbinninginsolvingbatchednonparametricbanditsbyestablishingitsrate-
optimality in minimizing regret. Now we turn to a complimentary but intriguing question: is it necessary
to use dynamic binning to achieve optimal regret under the batch constraint? To formally address this
question,weinvestigatetheperformanceofsuccessiveeliminationwithstatic binning,i.e.,Algorithm1with
g =g, and g =g =···g =1. Although static binning works when M is large (e.g., a single choice of
0 1 2 M−2
g attains the optimal regret [48, 39] in the fully online setting), we show that it must fail when M is small.
To bring the failure mode of static binning into focus, we consider the simplest scenario when M = 3,
and α=β =d=1. Note that the successive elimination with static binning algorithm is parameterized by
the grid choice Γ={t =0,t ,t ,t =T} and the fixed number g of bins. The following theorem formalizes
0 1 2 3
the failure of static binning in achieving optimal regret when M =3.
Theorem 3. Consider M =3, and α=β =d=1. For any choice of 1≤t <t ≤T −1, and any choice
1 2
of g, there exists a nonparametric bandit instance in F(1,1) such that the resulting successive elimination
with static binning algorithm πˆ satisfies
static
E[R T(πˆ static)]≥C˜ 1T19 9+κ,
for some κ,C˜ 1 > 0 that are independent of T. Here T19 9 is the optimal regret achieved by BaSEDB—an
successive elimination algorithm with dynamic binning.
While the formal proof is deferred to Section 6, we would like to immediately point out the intuition under-
lying the failure of static binning.
Necessary choice of grid Γ. It is evident from the proof of the minimax lower bound (Theorem 1)
that one needs to set t ≍ T9/19, and t ≍ T15/19. Otherwise, the inequality (8) guarantees the worst-case
1 2
regret of πˆ
static
exceeds the optimal one T19 9. Consequently, we can focus on the algorithm with t
1
≍T9/19,
t ≍T15/19, and only consider the design choice g.
2
121/g δ/2
x
1/z
Figure 3: Instance with g >z. Each bin B produced by πˆ has width 1/g.
static
δ/2
x
1/z
1/g
Figure 4: Instance with g <z. Each bin B produced by πˆ has width 1/g.
static
Why fixed g fails. As a baseline for comparison, recall that in the optimal algorithm with dynamic
binning, we set g ≍ T3/19, and g g ≍ T5/19 so that the worst case regret in three batches are all on the
0 0 1
order of T19 9. In view of this, we split the choice of g into three cases.
• Suppose that g ≫ T3/19. In this case, we can construct an instance such that the reward difference
only appears on an interval with length 1/z ≫ 1/g; see Figure 3. In other words, the static binning
is finer than that in the reward instance. As a result, the number of pulls in the smaller bin (used
by the algorithm) in the first batch is not sufficient to tell the two arms apart, that is with constant
probability, arm elimination will not happen after the first batch. This necessarily yields the blowup
of the regret in the second batch.
• Suppose that g ≪ T3/19. In this case, we can construct an instance such that the reward difference
only appears on an interval with length 1/z ≪ 1/g; see Figure 4. In other words, the static binning
is coarser than that in the reward instance. Since the aggregated reward difference on the larger bin
is so small, the number of pulls in the larger bin (used by the algorithm) in the first batch is still not
sufficient to result in successful arm elimination. Again, the regret on the second batch blows up.
• Supposethatg ≍T3/19. Sincethischoicesmatchesg usedintheoptimaldynamicbinningalgorithm,
0
there is no reward instance that can blow up the regret in the first two batches. Nevertheless, since
g ≪g g ≍T5/19, onecanconstructtheinstancesimilartothepreviouscase(i.e., Figure4)suchthat
0 1
the regret on the third batch blows up.
5 Regret analysis for BaSEDB
Our proof of the regret upper bound is inspired by the framework developed in [39]. Our setting presents
additional technical difficulty due to the batch constraint.
We begin with introducing some useful notations. Recall the tree growing process described in section 4,
where we have defined a tree T of depth M. The root (depth 0) of the tree is the whole space X. In depth
1, X has gd children, each of which is a bin of width 1/g . For each bin in depth 1, it has gd children, each
0 0 1
of which is a bin of width 1/(g g ). These children form the depth 2 nodes of the tree T. We form the tree
0 1
recursively until depth M.
13ForabinC ∈T,wedefineitsparentbyp(C)={C′ ∈T :C ∈child(C′)}. Moreover,weletp1(C)=p(C)
and define pk(C) = p(pk−1(C)) for k ≥ 2 recursively. In all, we denote by P(C) = {C′ ∈ T : C′ =
pk(C) for some k ≥1} all the ancestors of the bin C.
We also define L to be the set of active bins at time t, with the dummy case L = {X}. Clearly, for
t 0
1≤t≤t , one has L =B , where B are all the bins in the first layer.
1 1 1 1
5.1 Two clean events
The regret analysis relies on two clean events. First, fix a batch i≥1, and recall L is the set of active
ti−1+1
bins at time t +1. We denote the random number of pulls for a bin C ∈L within batch i to be
i−1 ti−1+1
(cid:88)ti
m := 1{X ∈C}.
C,i t
t=ti−1+1
Clearly, it has expectation
m⋆ =E[m ]=(t −t )P (X ∈C).
C,i C,i i i−1 X
Thefirstcleaneventclaimsthatm concentrateswellarounditsexpectationm⋆ uniformlyoverallC ∈T.
C,i C,i
We denote this event by E.
Lemma 2. Suppose that M ≤ D log(T) for some constant D > 0. With probability at least 1−1/T, for
1 1
all 1≤i≤M, and C ∈L , we have
ti−1+1
1 3
m⋆ ≤m ≤ m⋆ .
2 C,i C,i 2 C,i
See Section 5.5.1 for the proof.
Since M ≤D log(T) by assumption, we can apply Lemma 2 to obtain
1
E[R (πˆ)1(Ec)]≤TP(Ec)=1.
T
Therefore, in the remaining proof, we condition on E and focus on bounding E[R (πˆ)1(E)].
T
Thesecondcleaneventisontheeliminationprocess. Sinceweusesuccessiveeliminationineachbin,itis
natural to expect that the optimal arm in each bin is not eliminated during the process. To mathematically
specify this event, we need a few notations.
For each bin C ∈L , let I′ be the set of remaining arms at the end of batch i, i.e., after Algorithm 2 is
i C
invoked. Define
(cid:26) (cid:27)
I¯ = k ∈{1,−1}: supf⋆(x)−f(k)(x)≤c |C|β ,
C 1
x∈C
(cid:26) (cid:27)
I = k ∈{1,−1}: supf⋆(x)−f(k)(x)≤c |C|β ,
C 0
x∈C
where c =2Ldβ/2+1 and c =8c . Clearly, we have
0 1 0
I ⊆I¯ .
C C
Define a good event A ={I ⊆I′ ⊆I¯ }, which is the event that the remaining arms in C have gaps of
C C C C
correctorder. Inaddition,defineG =∩ A . RecallB isthesetofbinsC with|C|=((cid:81)i−1g )−1 =
C C′∈P(C) C′ i l=0 l
w for i≥1.
i
Lemma 3. For any 1≤i≤M −1 and C ∈B , we have
i
4m⋆
P(E∩G ∩Ac )≤ C,i.
C C T|C|d
In words, Lemma 3 guarantees that A happens with high probability if E holds and A holds for all the
C C′
ancestors C’ of C. See Section 5.5.2 for the proof.
145.2 Regret decomposition
In this section, we decompose the regret into three terms. First, for a bin C, we define
T
(cid:88)(cid:16) (cid:17)
rlive(C):= f⋆(X )−f(πt(Xt))(X ) 1(X ∈C)1(C ∈L ).
T t t t t
t=1
In addition, define J := ∪ L to be the set of bins that have been live up until time t. Correspondingly
t s≤t s
we define
T
(cid:88)(cid:16) (cid:17)
rborn(C):= f⋆(X )−f(πt(Xt))(X ) 1(X ∈C)1(C ∈J ).
T t t t t
t=1
It is clear from the definition that for any C ∈T, one has
(cid:88)
rborn(C)=rlive(C)+ rborn(C′)
T T T
C′∈child(C)
(cid:88)
=rborn(C)1(Ac )+rlive(C)1(A )+ rborn(C′)1(A ).
T C T C T C
C′∈child(C)
Applying this relation recursively leads to the following regret decomposition:
R (π)=rborn(X)
T T
(cid:88)
=rlive(X)+ rborn(C′)
T T
(cid:124) (cid:123)(cid:122) (cid:125)
=0
C′∈child(X)
 
 
= (cid:88)   (cid:88) r Tborn(C)1(G C ∩Ac C)+ (cid:88) r Tlive(C)1(G C ∩A C) 

1≤i<MC∈Bi C∈Bi 
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=:Ui =:Vi
(cid:88)
+ rlive(C)1(G ),
T C
C∈BM
where the second equality arises from the fact that rlive(X)=0. Indeed, X ∈/ L for any 1≤t≤T.
T t
5.3 Controlling three terms
In what follows, we control V ,U and the last batch separately.
i i
5.3.1 Controlling V
i
Fix some 1 ≤ i ≤ M −1, and some bin C ∈ B . On the event G we have I′ ⊆ I¯ , that is, for any
i C p(C) p(C)
k ∈I′ ,
p(C)
sup f⋆(x)−f(k)(x)≤c |p(C)|β.
1
x∈p(C)
This implies that for any x∈C, and k ∈I′ ,
p(C)
(cid:16) (cid:17) (cid:12) (cid:12)
f⋆(x)−f(k)(x) 1{G }≤c |p(C)|β1(0<(cid:12)f(1)(x)−f(−1)(x)(cid:12)≤c |p(C)|β). (12)
C 1 (cid:12) (cid:12) 1
As a result, we obtain
(cid:34) T (cid:35)
(cid:88)(cid:16) (cid:17)
E[rlive(C)1(G ∩A )]=E f⋆(X )−f(πt(Xt))(X ) 1(X ∈C)1(C ∈L )1(G ∩A )
T C C t t t t C C
t=1
15(i) (cid:34) (cid:88)T (cid:12) (cid:12) (cid:35)
≤ E c |p(C)|β1(0<(cid:12)f(1)(X )−f(−1)(X )(cid:12)≤c |p(C)|β)1(X ∈C,C ∈L )1(G ∩A )
1 (cid:12) t t (cid:12) 1 t t C C
t=1
 
(ii) (cid:88)ti (cid:12) (cid:12)
≤ c 1|p(C)|βE  1(0<(cid:12) (cid:12)f(1)(X t)−f(−1)(X t)(cid:12) (cid:12)≤c 1|p(C)|β,X t ∈C)1(G C ∩A C)
t=ti−1+1
(iii) (cid:88)ti (cid:12) (cid:12)
≤ c |p(C)|β P(0<(cid:12)f(1)(X )−f(−1)(X )(cid:12)≤c |p(C)|β,X ∈C)
1 (cid:12) t t (cid:12) 1 t
t=ti−1+1
(cid:12) (cid:12)
=c |p(C)|β(t −t )P(0<(cid:12)f(1)(X)−f(−1)(X)(cid:12)≤c |p(C)|β,X ∈C).
1 i i−1 (cid:12) (cid:12) 1
Here, step (i) uses relation (12), and the fact that π (X ) ∈ I′ when X ∈ C. For step (ii), if C is split,
t t p(C) t
then it is no longer live, so the live regret incurred on the remaining batches is zero. On the other hand, if
C is not split, then |I′ | = 1. Without loss of generality, assume that arm −1 is eliminated. Conditioned
C
on A , this means −1 ∈/ I and there exists x ∈ C such that f(1)(x )−f(−1)(x ) > c |C|β. By the
C C 0 0 0 0
smoothness condition, having a gap at least c |C|β on a single point in C implies f(1)(x)−f(−1)(x)>|C|β
0
for all x∈C. Therefore, arm 1 which is the remaining one is the optimal arm for all x∈C and would not
incur any regret further. The third inequality holds since 1(G ∩A )≤1.
C C
Taking the sum over all bins in B and using the fact that |p(C)|=w , we obtain
i i−1
(cid:88) (cid:88) (cid:12) (cid:12)
E[rlive(C)1(G ∩A )]≤ c wβ (t −t )P(0<(cid:12)f(1)(X)−f(−1)(X)(cid:12)≤c |p(C)|β,X ∈C)
T C C 1 i−1 i i−1 (cid:12) (cid:12) 1
C∈Bi C∈Bi
(cid:88) (cid:12) (cid:12)
=c wβ (t −t ) P(0<(cid:12)f(1)(X)−f(−1)(X)(cid:12)≤c wβ ,X ∈C). (13)
1 i−1 i i−1 (cid:12) (cid:12) 1 i−1
C∈Bi
Note that
(cid:88) (cid:12) (cid:12) (cid:12) (cid:12)
P(0<(cid:12)f(1)(X)−f(−1)(X)(cid:12)≤c wβ ,X ∈C)=P(0<(cid:12)f(1)(X)−f(−1)(X)(cid:12)≤c wβ )
(cid:12) (cid:12) 1 i−1 (cid:12) (cid:12) 1 i−1
C∈Bi
(cid:104) (cid:105)α
≤D · c wβ , (14)
0 1 i−1
where the last inequality follows from the margin condition. Combining relations (14) and (13), we reach
(cid:88)
E[rlive(C)1(G ∩A )]≤(t −t )·[c wβ ]1+α·D .
T C C i i−1 1 i−1 0
C∈Bi
5.3.2 Controlling U
i
Fix some 1≤i≤M −1, and some bin C ∈B . Again, using the definition of G , we obtain
i C
(cid:34) T (cid:35)
(cid:88)(cid:16) (cid:17)
E[rborn(C)1(G ∩Ac )]=E f⋆(X )−f(πt(Xt))(X ) 1(X ∈C)1(C ∈J )1(G ∩Ac )
T C C t t t t C C
t=1
(cid:34) (cid:88)T (cid:12) (cid:12) (cid:35)
≤E c |p(C)|β1(0<(cid:12)f(1)(X )−f(−1)(X )(cid:12)≤c |p(C)|β)1(X ∈C,C ∈J )1(G ∩Ac )
1 (cid:12) t t (cid:12) 1 t t C C
t=1
(cid:12) (cid:12)
≤c |p(C)|βTP(0<(cid:12)f(1)(X)−f(−1)(X)(cid:12)≤c |p(C)|β,X ∈C)P(G ∩Ac ).
1 (cid:12) (cid:12) 1 C C
Apply Lemma 3 to see that
(cid:12) (cid:12) 4m⋆
E[rborn(C)1(G ∩Ac )]≤c |p(C)|βTP(0<(cid:12)f(1)(X)−f(−1)(X)(cid:12)≤c |p(C)|β,X ∈C) C,i
T C C 1 (cid:12) (cid:12) 1 T|C|d
16(cid:12) (cid:12) 4(t −t )P (X ∈C)
=c wβ P(0<(cid:12)f(1)(X)−f(−1)(X)(cid:12)≤c wβ ,X ∈C) i i−1 X
1 i−1 (cid:12) (cid:12) 1 i−1 |C|d
(cid:12) (cid:12)
≤4c¯c wβ P(0<(cid:12)f(1)(X)−f(−1)(X)(cid:12)≤c wβ ,X ∈C)(t −t ),
1 i−1 (cid:12) (cid:12) 1 i−1 i i−1
where we use the fact that P (X ∈ C) ≤ c¯|C|d in the second inequality. Summing over all bins in B , we
X i
obtain
(cid:88) (cid:88) (cid:12) (cid:12)
E[rborn(C)1(G ∩Ac )]≤4c¯c wβ (t −t ) P(0<(cid:12)f(1)(X)−f(−1)(X)(cid:12)≤c wβ ,X ∈C)
T C C 1 i−1 i i−1 (cid:12) (cid:12) 1 i−1
C∈Bi C∈Bi
(cid:104) (cid:105)α
≤4c¯c wβ (t −t )D · c wβ
1 i−1 i i−1 0 1 i−1
=4D c¯(t −t )[c wβ ]1+α,
0 i i−1 1 i−1
where the second inequality reuses the bound in (14).
5.3.3 Last Batch
For C ∈B , one can similarly obtain
M
(cid:12) (cid:12)
E[rlive(C)1(G )]≤c |p(C)|β(T −t )P(0<(cid:12)f(1)(X)−f(−1)(X)(cid:12)≤c |p(C)|β,X ∈C).
T C 1 M−1 (cid:12) (cid:12) 1
Consequently, summing over C ∈B yields
M
(cid:88) (cid:88) (cid:12) (cid:12)
E[rlive(C)1(G )]≤ c |p(C)|β(T −t )P(0<(cid:12)f(1)(X)−f(−1)(X)(cid:12)≤c |p(C)|β,X ∈C)
T C 1 M−1 (cid:12) (cid:12) 1
C∈BM C∈BM
(cid:104) (cid:105)α
≤c wβ (T −t )D · c wβ
1 M−1 M−1 0 1 M−1
=D (T −t )[c wβ ]1+α.
0 M−1 1 M−1
5.4 Putting things together
In sum, the total regret is bounded by
(cid:32) M−1 (cid:33)
(cid:88)
E[R (π)]≤c t + (t −t )·wβ+αβ +(T −t )wβ+αβ ,
T 1 i i−1 i−1 M−1 M−1
i=2
where c is a constant that depends on (α,β,D,L). Recall that w = ((cid:81)i−1g )−1, and the choices for the
i l=0 l
batch size and the split factors (11)-(10). We then obtain
t
1
≲T1−1− γγ
M logT,
(t i−t i−1)·w iβ −+ 1αβ ≲T1−1− γγ M logT, for 2≤i≤M −1,
(T −t M−1)w Mβ+ −α 1β ≤Tw Mβ+ −α 1β ≲T1−1− γγ M logT.
The proof is finished by combining the above three bounds.
5.5 Proofs for the clean events
We are left with proving that the two clean events happen with high probability.
175.5.1 Proof of Lemma 2
Fix the batch index i, and a node C in layer-i of the tree T. By relation (11), we have
m⋆ =(t −t )P (X ∈C)
C,i i i−1 X
≍|C|−(2β+d)log(T|C|d)P (X ∈C)
X
|C|−2β ≥g2β ≍(T1−1− γγ M· 2β2 +β d),
0
where the last step uses the fact that P(cid:63) (X ∈C)≥c|C|d. Therefore, m⋆ ≥ 3log(2T2) for all i and C, as
X C,i 4
long as T is sufficiently large. This allows us to invoke Chernoff’s bound to obtain that with probability at
most 1/T2
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)(cid:88)t ti =ti−1+11{X t ∈C}−m⋆ C,i(cid:12) (cid:12) (cid:12)≥(cid:113) 3log(2T2)m⋆ C,i.
(cid:113)
Denote Ec = {∃1 ≤ i ≤ M,C ∈ L such that | (cid:80)ti 1{X ∈ C}−m⋆ |≥ 3log(2T2)m⋆ }.
ti−1+1 t=ti−1+1 t C,i C,i
Applying union bound to reach
(cid:32) M i−1 (cid:33) M−1
(cid:88) 1 (i) 1 (cid:88) (cid:89) (ii) 1 (cid:89)
P(Ec)≤ ≤ ( g )d ≤ ·M ·( g )d,
T2 T2 l T2 l
C∈T i=1 l=0 l=0
wherestep(i)sumsoverallpossiblenodesofT acrossbatches,andstep(ii)isdueto((cid:81)i−1g )d ≤((cid:81)M−1g )d
l=0 l l=0 l
for any 1≤i≤M. Since g =1, we further obtain
M−1
M−2
P(Ec)≤ T1
2
·M ·((cid:89) g l)d ( ≤iii) T1
2
·M ·t M2βd −+d
1
( ≤iv) D 1T1
2
·logT ·T2βd +d ≤ T1 ,
l=0
where step (iii) invokes relation (11), and step (iv) uses the assumption M ≤ D logT. This completes the
1
proof.
5.5.2 Proof of Lemma 3
To simplify notation, for any event F, we define PGC(F)=P(E∩G
C
∩F).
Let D1 be the event that an arm k ∈I is eliminated at the end of batch i, and D2 be the event that
C C C
an arm k ∈/ I¯ is not eliminated at the end of batch i. Consequently, we have
C
PGC(Ac )=PGC(D1)+PGC((D1)c∩D2).
C C C C
(cid:113)
Recall U(τ,T,C)=4 log(2T|C|d) . By relation (11), we can write
τ
m⋆ =(t −t )P (X ∈C)
C,i i i−1 X
=l |C|−(2β+d)log(T|C|d)P (X ∈C),
i X
where l > 0 is a constant chosen such that U(2m⋆ ,T,C) = 2c |C|β. Under E, we have U(m ,T,C) ≤
i C,i 0 C,i
4c |C|β because m ≥ 1m⋆ .
0 C,i 2 C,i
1. Upper bounding PGC(D1): when D1 occurs, an arm k ∈ I is eliminated by some k′ ∈ I′ at the
C C C p(C)
end of batch i. This means Y¯(k′)−Y¯(k) >U(m ,T,C). Meanwhile,
C,i C,i C,i
f¯(k′)−f¯(k) ≤f¯⋆ −f¯(k) ( ≤i) c |C|β ≤ 1 U(2m⋆ ,T,C),
C C C C 0 2 C,i
where step (i) uses the definition of I . Consequently, |Y¯(k′)−f¯(k′)|≤U(m ,T,C)/4 and |Y¯(k)−
C C,i C C,i C,i
f¯(k)| ≤ U(m ,T,C)/4 cannot hold simultaneously. Otherwise, this would contradict with Y¯(k′) −
C C,i C,i
Y¯(k) >U(m ,T,C) because m ≤2m⋆ under E. Therefore,
C,i C,i C,i C,i
(cid:26) (cid:27)
1
PGC(D1)≤P ∃k ∈I′ ,m ≤2m⋆ :|Y¯(k)−f¯(k)|≥ U(m ,T,C) .
C p(C) C,i C,i C,i C 4 C,i
182. Upperbounding PGC((D1)c∩D2): when(D1)c∩D2 happens, noarm inI iseliminatedwhile some
C C C C C
k ∈/ I¯ remainsintheactivearmset. Bydefinition,thereexistsx(k)suchthatf⋆(x(k))−f(k)(x(k))>8c |C|β.
C 0
Let η(k) be any arm that satisfies f⋆(x(k))=f(η(k))(x(k)), and one can easily verify η(k)∈I . Since
C
k is not eliminated, we have Y¯(η(k))−Y¯(k) ≤U(m ,T,C). On the other hand,
C,i C,i C,i
(iii)
f¯(η(k)) ≥ f(η(k))(x(k))−c |C|β
C 0
≥f(k)(x(k))+8c |C|β −c |C|β
0 0
=f(k)(x(k))+7c |C|β
0
(iv) 3
≥ f¯(k)+6c |C|β ≥f¯(k)+ U(m ,T,C), (15)
C 0 C 2 C,i
where steps (iii) and (iv) use Lemma 5. Inequality (15) together with the fact that Y¯(η(k))−Y¯(k) ≤
C,i C,i
U(m ,T,C) imply |Y¯(k0)−f¯(k0)|≥U(m ,T,C)/4 for either k =k or k =η(k). Consequently,
C,i C,i C C,i 0 0
(cid:26) (cid:27)
1
PGC((D1)c∩D2)≤P ∃k ∈I′ ,m ≤2m⋆ :|Y¯(k)−f¯(k)|≥ U(m ,T,C) .
C C p(C) C,i C,i C,i C 4 C,i
Combining the two parts we obtain
PGC(Ac )=PGC(D1)+PGC((D1)c∩D2)
C C C C
(cid:26) (cid:27)
1
≤2·P ∃k ∈I′ ,m ≤2m⋆ :|Y¯(k)−f¯(k)|≥ U(m ,T,C)
p(C) C,i C,i C,i C 4 C,i
4m⋆
≤ C,i,
T|C|d
where the last inequality applies Lemma 4.
5.5.3 Auxiliary lemmas
Lemma 4. For any 1≤i≤M −1 and C ∈B , one has
i
(cid:26) 1 (cid:27) 2m⋆
P ∃k ∈I′ ,m ≤2m⋆ :|Y¯(k)−f¯(k)|≥ U(m ,T,C) ≤ C,i.
p(C) C,i C,i C,i C 4 C,i T|C|d
Proof. Recall in Algorithm 1 we pull each arm in a round-robin fashion within a bin during batch i. Fix
τ >0. LetY¯(k) =(cid:80)τ Y(k)/τ whereY(k)’sarei.i.d.randomvariableswithY(k) ∈[0,1]andE[Y(k)]=f¯(k).
τ j=1 j j j j C
By Hoeffding’s inequality, with probability 1/(T|C|d), we have
(cid:114)
log(2T|C|d)
|Y¯(k)−f¯(k)|≥ .
τ C 2τ
Applying union bound to get
(cid:40) (cid:114) (cid:41)
log(2T|C|d) 2m⋆
P ∃k ∈I ,0≤τ ≤m⋆ :|Y¯(k)−f¯(k)|≥ ≤ C,i,
p(C) C,i τ C 2τ T|C|d
which completes the proof.
Lemma 5. Fix k ∈{1,−1} and C ∈T, for any x∈C, one has
|f¯(k)−f(k)(x)|≤c |C|β,
C 0
where c =2Ldβ/2+1.
0
19Proof. For notation simplicity, we write f for f(k) in the following proof. By definition,
1 (cid:90)
|f¯ −f(x)|=| (f(y)−f(x))dP(y)|
C P(C)
C
1 (cid:90)
≤ |f(y)−f(x)|dP(y)
P(C)
C
1 (cid:90)
≤ L∥x−y∥βdP(y),
P(C) 2
C
where the first inequality uses the triangle inequality, and the second inequality is due to the smoothness
condition. Since x∈C, we further have
1 (cid:90)
|f¯ −f(x)|≤ L∥x−y∥βdP(y)
C P(C) 2
C
1 (cid:90)
≤ Ldβ/2|C|βdP(y)
P(C)
C
≤c |C|β.
0
This completes the proof.
6 Proof of suboptimality of static binning
As we argued after the statement of Theorem 3, one needs to set t ≍ T9/19, and t ≍ T15/19. Therefore,
1 2
throughout the proof, we assume this is true and only focus on the number g of bins.
To construct a hard instance, we partition [0,1] into z bins with equal width. Denote the bins by C for
j
j = 1,...,z, and let q be the center of C . Define a function ϕ : [0,1] (cid:55)→ R as ϕ(x) = (1−|x|)1{|x| ≤ 1}.
j j
Correspondingly define a function φ : [0,1] (cid:55)→ R as φ (x) = D z−1ϕ(2z(x−q ))1{x ∈ C }, where D =
j j ϕ j j ϕ
min(2−1L,1/4). Define a function f :[0,1](cid:55)→R:
1
f(x)= +φ (x).
2 1
The problem instance of interest is v = (f(1)(x) = f(x),f(−1)(x) = 1). It is easy to verify v ∈ F(1,1).
2
Throughouttheproof,weconditionontheeventE specifiedbyLemma2,whichsaysthenumberofsamples
allocated to a bin concentrates well around its expectation. We will show even under this good event, there
exists a choice of z that makes successive elimination fail to remove the suboptimal arms at the end of a
batch with constant probability.
6.1 A helper lemma
We begin with presenting a helper lemma that will be used extensively in the later part of the proof. The
claim is intuitive: if the sample size is small, it is not sufficient to tell apart two Bernoulli distributions with
similar means. Then, in our context, arm elimination will not occur.
Lemma 6. Assume m ≤ 2m⋆ . For any B ⊆ [0,1] and i ∈ {1,2}. If f¯(1)−f¯(−1) ≤ δ ≤ 1/(cid:112) m⋆ for
B,i B,i B B B,i
some δ >0 , then
(cid:16) (cid:17) t
P Y¯(1)−Y¯(−1) >U(m ,T,B) ≤ i.
B,i B,i B,i T
Proof. Fix0<τ ≤m⋆ . LetY¯(k) =(cid:80)τ Y(k)/τ whereY(k)’sarei.i.d.randomvariableswithY(k) ∈[0,1]
B,i τ l=1 l l l
(cid:113)
and E[Y(k)]=f¯(k) for k ∈{1,−1}. Recall U(τ,T,B)=4 log(2T|B|)2. Then,
l B τ
(cid:32) (cid:114) (cid:33)
(cid:16) (cid:17)(i) log(2T/g)
P Y¯(1)−Y¯(−1) >U(2τ,T,B) ≤ P Y¯(1)−Y¯(−1) >δ+
τ τ τ τ 2τ
2Weremarktheconstant4isnotessentialfortheprooftowork. Foranyc>0, clog(2T|B|)=log((2T|B|)c)sothefinal
successprobabilityisstilltinyaslongasT issufficientlylarge.
20(cid:32) (cid:114) (cid:33)
(ii) log(2T/g)
≤ P Y¯(1)−Y¯(−1) >f¯(1)−f¯(−1)+
τ τ B B 2τ
(iii) g
≤ ,
T
√
where step (i) is because δ ≤ 1/(cid:112) m⋆ ≤ 1/ τ, step (ii) is due to f¯(1) −f¯(−1) ≤ δ, and step (iii) uses
B,i B B
Hoeffding’s inequality. Applying union bound to get
(cid:16) (cid:17) m⋆ g t
P ∃0<τ ≤m⋆ :Y¯(1)−Y¯(−1) >U(2τ,T,B) ≤ B,i ≤ i.
B,i τ τ T T
This finishes the proof.
6.2 Three failure cases for g
Fix some small constant ε>0 to be specified later. From now on, we use πˆ to denote πˆ for simplicity.
static
We split the proof into three cases: (1) g ≥T3/19+ε; (2) g ≤T3/19−ε; (3) and g ∈(T3/19−ε,T3/19+ε).
Case 1: g ≥T3/19+ε. Setz =T3/19−ε/2. Assumewithoutlossofgeneralitythatg =H·z forsomeH ≥4;
see Figure 3 for an illustration of the instance. Suppose C =∪H B , where B ’s are the bins produced by
1 l=1 l l
πˆ that lie in C . It is clear that
1
(i)
(cid:34) (cid:88)t2
(cid:16)
(cid:17)(cid:35)
E[R (πˆ)] ≥ E f⋆(X )−fπˆt(Xt)(X )
T t t
t=t1+1
(
=ii)E(cid:34) (cid:88)t2
(cid:16) f⋆(X )−fπˆt(Xt)(X )(cid:17) 1{X ∈C
}(cid:35)
t t t 1
t=t1+1
(iii)
(cid:88)t2 3 (cid:88)H/4
(cid:104)(cid:16) (cid:17) (cid:105)
≥ E f⋆(X )−fπˆt(Xt)(X ) 1{X ∈B } , (16)
t t t l
t=t1+1l=H/4
where step (i) is because the total regret is greater than the regret incurred during the second batch, step
(ii) uses the fact that under the instance v, the mean rewards of the two arms differ only in C , and step
1
(iii) arises since C
1
= ∪H l=1B l. Now we turn to lower bounding E(cid:2)(cid:0) f⋆(X t)−fπˆt(Xt)(X t)(cid:1) 1{X
t
∈B l}(cid:3) for
each H/4≤l≤3H/4.
Consider any such B . We drop the subscripts and write B instead for simplicity. By the design of v,
l
we have f¯(1) −f¯(−1) ≤ D z−1 = δ, which obeys D z−1 ≤ 1/(cid:112) m⋆ —a consequence of the choice of z.
B B ϕ ϕ B,1
Additionally, we have m ≤2m⋆ under E. Therefore, we can invoke Lemma 6 to obtain
B,1 B,1
(cid:16) (cid:17) t 1
P Y¯(1)−Y¯(−1) >U(m ,T,B) ≤ 1 ≤ .
B,1 B,1 B,1 T 2
In words, with probability exceeding 1/2, no elimination will happen for the bin B. As a result, we obtain
(cid:88)t2 3 (cid:88)H/4
(cid:104)(cid:16) (cid:17) (cid:105)
E[R (πˆ)]≥ E f⋆(X )−fπˆt(Xt)(X ) 1{X ∈B }
T t t t l
t=t1+1l=H/4
t t
≳H · 2 ·z−1 ≍ 2 =T19 9+ϵ,
g z2
where we have used the choice of z. So Theorem (3) holds with κ=ϵ.
21Case 2: g ≤ T3/19−ε. Set z = T3/19−ε/8. We have g < z and there exists H > 1 such that z = H ·g;
see Figure 4 for an illustration of the instance. Let B be the bin produced by πˆ such that C ⊂B. By the
1
design of v, we have
1 1 1 1 D z−1
f¯(1)−f¯(−1) ≤ (1/2+D z−1)+(1− ) − = ϕ .
B B H ϕ H 2 2 H
Let δ = Dϕz−1, we have δ ≤ 1/(cid:112) m⋆ due to our choice of z. Additionally, we have m ≤ 2m⋆ under
H B,1 B,1 B,1
E. Therefore, we can invoke Lemma 6 to obtain
(cid:16) (cid:17) t 1
P Y¯(1)−Y¯(−1) >U(m ,T,B) ≤ 1 ≤ .
B,1 B,1 B,1 T 2
Thus, with probability exceeding 1/2, the suboptimal arm is not eliminated in B. Similar to the previous
case, we obtain
(cid:34) (cid:88)t2
(cid:16)
(cid:17)(cid:35)
E[R (πˆ)]≥E f⋆(X )−fπˆt(Xt)(X )
T t t
t=t1+1
(cid:34) (cid:88)t2
(cid:16) (cid:17)
(cid:35)
=E f⋆(X )−fπˆt(Xt)(X ) 1{X ∈C }
t t t 1
t=t1+1
t
2 =T19 9+ 4ϵ.
z2
(cid:63)
So Theorem (3) holds with κ=ϵ/4.
Case 3: g ∈(T3/19−ε,T3/19+ε). Set z ≍T1/4. We then have g <z, as long as ε≤1/19. And there exists
H >1 such that z =H ·g; see Figure 4 for an illustration of the instance. Let B be the bin produced by πˆ
such that C ⊂B. By the design of v, we have
1
1 1 1 1 D z−1
f¯(1)−f¯(−1) ≤ (1/2+D z−1)+(1− ) − = ϕ .
B B H ϕ H 2 2 H
Let δ = Dϕz−1, we have δ ≤ 1/(cid:112) m⋆ due to our choice of z. Additionally, we have m ≤ 2m⋆ under
H B,1 B,1 B,1
E. Therefore, we can invoke Lemma 6 to obtain
(cid:16) (cid:17) t 1
P Y¯(1)−Y¯(−1) >U(m ,T,B) ≤ 1 ≤ .
B,1 B,1 B,1 T 4
Thismeanswithprobabilityatleast3/4,armeliminationdoesnotoccurinB afterthefirstbatch. Moreover,
since δ ≤1/(cid:112) m⋆ by the choice of z, and m ≤2m⋆ under E, we can apply Lemma 6 again to get
B,2 B,2 B,2
(cid:16) (cid:17) t 1
P Y¯(1)−Y¯(−1) >U(m ,T,B) ≤ 2 ≤ .
B,2 B,2 B,2 T 4
In all, with probability at least 1/2, arm elimination does not occur in B after the second batch. Similar to
before, we reach the conclusion that
(cid:34) T (cid:35)
(cid:88) (cid:16) (cid:17)
E[R (πˆ)]≥E f⋆(X )−fπˆt(Xt)(X )
T t t
t=t2+1
(cid:34) T (cid:35)
(cid:88)
=E (f⋆(X )−fπˆt(Xt)(X ))1{X ∈C }
t t t 1
t=t2+1
T
≳ =T1 2.
z2
We see that Theorem (3) holds with κ=1/38.
227 Discussion
Inthispaper, wecharacterizethefundamentallimitsofbatchlearninginnonparametriccontextualbandits.
In particular, our optimal batch learning algorithm (i.e., Algorithm 1) is able to match the optimal regret
in the fully online setting with only O(loglogT) policy updates. Our work open a few interesting avenues
to explore in the future.
Extensions to multiple arms. Withslightmodification, ouralgorithmworksfornonparametriccontex-
tual bandits with more than two arms. However, it remains unclear what the fundamental limits of batch
learning are in this multi-armed case.
Improving the log factor. Comparing the upper and lower bounds, it is evident that Algorithm 1 is
near-optimal up to log factors. It is certainly interesting to improve this log factor, either by strengthening
the lower bound, or making the upper bound more efficient.
Adaptingtosmoothnessandmarginparameters. Inpractice,wedonotalwaysknowthesmoothness
and the margin parameters. Can one develop a batch learning algorithm that can adapt to these unknown
parameters? This question is intriguing because in the fully online setting, a similar adaptively binned
successive elimination algorithm was proposed in [39] with the sole purpose of adapting to the margin
parameter.3
Acknowledgements
CM is partially supported by the National Science Foundation via grant DMS-2311127.
References
[1] Yasin Abbasi-Yadkori, Dávid Pál, and Csaba Szepesvári. Improved algorithms for linear stochastic
bandits. Advances in neural information processing systems, 24, 2011.
[2] Sakshi Arya and Yuhong Yang. Randomized allocation with nonparametric estimation for contextual
multi-armed bandits with delayed rewards. Statistics & Probability Letters, 164:108818, 2020.
[3] Jean-Yves Audibert and Alexandre B Tsybakov. Fast learning rates for plug-in classifiers. The Annals
of Statistics, 35(2):608–633, 2007.
[4] PeterAuer. Usingconfidenceboundsforexploitation-explorationtrade-offs. Journal of Machine Learn-
ing Research, 3(Nov):397–422, 2002.
[5] YuBai,TengyangXie,NanJiang,andYu-XiangWang. Provablyefficientq-learningwithlowswitching
cost. Advances in Neural Information Processing Systems, 32, 2019.
[6] Hamsa Bastani and Mohsen Bayati. Online decision making with high-dimensional covariates. Opera-
tions Research, 68(1):276–294, 2020.
[7] Hamsa Bastani, Mohsen Bayati, and Khashayar Khosravi. Mostly exploration-free algorithms for con-
textual bandits. Management Science, 67(3):1329–1349, 2021.
[8] DimitrisBertsimasandAdamJMersereau. Alearningapproachforinteractivemarketingtoacustomer
segment. Operations Research, 55(6):1120–1135, 2007.
[9] Moise Blanchard, Steve Hanneke, and Patrick Jaillet. Non-stationary contextual bandits and universal
learning. arXiv preprint arXiv:2302.07186, 2023.
3We emphasize again that if adaptivity to the margin parameter is not needed, then static binning suffices for the online
setting.
23[10] Changxiao Cai, T Tony Cai, and Hongzhe Li. Transfer learning for contextual multi-armed bandits.
arXiv preprint arXiv:2211.12612, 2022.
[11] T Tony Cai and Hongming Pu. Stochastic continuum-armed bandits with additive models: Minimax
regrets and adaptive algorithm. The Annals of Statistics, 50(4):2179–2204, 2022.
[12] Nicolo Cesa-Bianchi, Ofer Dekel, and Ohad Shamir. Online learning with switching costs and other
adaptive adversaries. Advances in Neural Information Processing Systems, 26, 2013.
[13] Olivier Chapelle. Modeling delayed feedback in display advertising. In Proceedings of the 20th ACM
SIGKDD international conference on Knowledge discovery and data mining, pages 1097–1105, 2014.
[14] Olivier Chapelle and Lihong Li. An empirical evaluation of thompson sampling. Advances in neural
information processing systems, 24, 2011.
[15] Stephen E Chick and Noah Gans. Economic analysis of simulation selection problems. Management
Science, 55(3):421–437, 2009.
[16] Eyal Even-Dar, Shie Mannor, Yishay Mansour, and Sridhar Mahadevan. Action elimination and stop-
ping conditions for the multi-armed bandit and reinforcement learning problems. Journal of machine
learning research, 7(6):1079–1105, 2006.
[17] JianqingFan,ZhaoranWang,ZhuoranYang,andChenluYe. Provablyefficienthigh-dimensionalbandit
learning with batched feedbacks. arXiv preprint arXiv:2311.13180, 2023.
[18] Yasong Feng, Zengfeng Huang, and Tianyu Wang. Lipschitz bandits with batched feedback. Advances
in Neural Information Processing Systems, 35:19836–19848, 2022.
[19] ManegueuAnneGael,ClaireVernade,AlexandraCarpentier,andMichalValko. Stochasticbanditswith
arm-dependent delays. In International Conference on Machine Learning, pages 3348–3356. PMLR,
2020.
[20] Minbo Gao, Tianle Xie, Simon S Du, and Lin F Yang. A provably efficient algorithm for linear markov
decision process with low switching cost. arXiv preprint arXiv:2101.00494, 2021.
[21] Zijun Gao, Yanjun Han, Zhimei Ren, and Zhengqing Zhou. Batched multi-armed bandits problem.
Advances in Neural Information Processing Systems, 32, 2019.
[22] AlexanderGoldenshlugerandAssafZeevi.Woodroofe’sone-armedbanditproblemrevisited.TheAnnals
of Applied Probability, 19(4):1603–1633, 2009.
[23] Alexander Goldenshluger and Assaf Zeevi. A linear response bandit problem. Stochastic Systems,
3(1):230–261, 2013.
[24] Melody Guan and Heinrich Jiang. Nonparametric stochastic contextual bandits. In Proceedings of the
AAAI Conference on Artificial Intelligence, volume 32, 2018.
[25] Yonatan Gur, Ahmadreza Momeni, and Stefan Wager. Smoothness-adaptive contextual bandits. Oper-
ations Research, 70(6):3198–3216, 2022.
[26] Yanjun Han, Zhengqing Zhou, Zhengyuan Zhou, Jose Blanchet, Peter W Glynn, and Yinyu Ye. Se-
quential batch learning in finite-action linear contextual bandits. arXiv preprint arXiv:2004.06321,
2020.
[27] YichunHu, NathanKallus, andXiaojieMao. Smoothcontextualbandits: Bridgingtheparametricand
nondifferentiable regret regimes. Operations Research, 70(6):3261–3281, 2022.
[28] CemKalkanliandAyferOzgur. Batchedthompsonsampling. Advances in Neural Information Process-
ing Systems, 34:29984–29994, 2021.
24[29] AminKarbasi,VahabMirrokni,andMohammadShadravan.Parallelizingthompsonsampling.Advances
in Neural Information Processing Systems, 34:10535–10548, 2021.
[30] Edward S Kim, Roy S Herbst, Ignacio I Wistuba, J Jack Lee, George R Blumenschein Jr, Anne Tsao,
David J Stewart, Marshall E Hicks, Jeremy Erasmus Jr, Sanjay Gupta, et al. The battle trial: person-
alizing therapy for lung cancer. Cancer discovery, 1(1):44–53, 2011.
[31] Aniket Kittur, Ed H Chi, and Bongwon Suh. Crowdsourcing user studies with mechanical turk. In
Proceedings of the SIGCHI conference on human factors in computing systems, pages 453–456, 2008.
[32] Anders Bredahl Kock and Martin Thyrsgaard. Optimal sequential treatment allocation. arXiv preprint
arXiv:1705.09952, 2017.
[33] Akshay Krishnamurthy, John Langford, Aleksandrs Slivkins, and Chicheng Zhang. Contextual ban-
dits with continuous actions: Smoothing, zooming, and adapting. The Journal of Machine Learning
Research, 21(1):5402–5446, 2020.
[34] Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.
[35] Lihong Li, Wei Chu, John Langford, and Robert E Schapire. A contextual-bandit approach to person-
alized news article recommendation. In Proceedings of the 19th international conference on World wide
web, pages 661–670, 2010.
[36] Andrea Locatelli and Alexandra Carpentier. Adaptivity to smoothness in x-armed bandits. In Confer-
ence on Learning Theory, pages 1463–1492. PMLR, 2018.
[37] Tyler Lu, Dávid Pál, and Martin Pál. Showing relevant ads via context multi-armed bandits. In
Proceedings of AISTATS, 2009.
[38] Enno Mammen and Alexandre B Tsybakov. Smooth discrimination analysis. The Annals of Statistics,
27(6):1808–1829, 1999.
[39] VianneyPerchetandPhilippeRigollet. Themulti-armedbanditproblemwithcovariates. Ann. Statist.,
41(2):693–721, 2013.
[40] Vianney Perchet, Philippe Rigollet, Sylvain Chassang, and Erik Snowberg. Batched bandit problems.
Ann. Statist., 44(2):660–681, 2016.
[41] Wei Qian, Ching-Kang Ing, and Ji Liu. Adaptive algorithm for multi-armed bandit problem with
high-dimensional covariates. Journal of the American Statistical Association, pages 1–13, 2023.
[42] Wei Qian and Yuhong Yang. Kernel estimation and model combination in a bandit problem with
covariates. Journal of Machine Learning Research, 17(149), 2016.
[43] Wei Qian and Yuhong Yang. Randomized allocation with arm elimination in a bandit problem with
covariates. Electronic Journal of Statistics, 10(1):242–270, 2016.
[44] DanQiao,MingYin,MingMin,andYu-XiangWang.Sample-efficientreinforcementlearningwithloglog
(t)switchingcost. InInternational Conference on Machine Learning,pages18031–18061.PMLR,2022.
[45] Henry Reeve, Joe Mellor, and Gavin Brown. The k-nearest neighbour ucb algorithm for multi-armed
bandits with covariates. In Algorithmic Learning Theory, pages 725–752. PMLR, 2018.
[46] ZhimeiRenandZhengyuanZhou. Dynamicbatchlearninginhigh-dimensionalsparselinearcontextual
bandits. Management Science, 2023.
[47] Zhimei Ren, Zhengyuan Zhou, and Jayant R Kalagnanam. Batched learning in generalized linear
contextual bandits with general decision sets. IEEE Control Systems Letters, 6:37–42, 2020.
[48] Philippe Rigollet and Assaf Zeevi. Nonparametric bandits with covariates. arXiv preprint
arXiv:1003.1630, 2010.
25[49] Herbert E. Robbins. Some aspects of the sequential design of experiments. Bulletin of the American
Mathematical Society, 58:527–535, 1952.
[50] Eric M Schwartz, Eric T Bradlow, and Peter S Fader. Customer acquisition via display advertising
using multi-armed bandit experiments. Marketing Science, 36(4):500–522, 2017.
[51] Joe Suk and Samory Kpotufe. Tracking most significant shifts in nonparametric contextual bandits.
arXiv preprint arXiv:2307.05341, 2023.
[52] Joseph Suk and Samory Kpotufe. Self-tuning bandits over unknown covariate-shifts. In Algorithmic
Learning Theory, pages 1114–1156. PMLR, 2021.
[53] Ambuj Tewari and Susan A Murphy. From ads to interventions: Contextual bandits in mobile health.
In Mobile Health, pages 495–517. Springer, 2017.
[54] AlexanderBTsybakov.Optimalaggregationofclassifiersinstatisticallearning.TheAnnalsofStatistics,
32(1):135–166, 2004.
[55] ClaireVernade, OlivierCappé, andVianneyPerchet. Stochasticbanditmodelsfordelayedconversions.
arXiv preprint arXiv:1706.09186, 2017.
[56] Chi-Hua Wang and Guang Cheng. Online batch decision-making with high-dimensional covariates. In
International Conference on Artificial Intelligence and Statistics, pages 3848–3857. PMLR, 2020.
[57] Tianhao Wang, Dongruo Zhou, and Quanquan Gu. Provably efficient reinforcement learning with
linearfunctionapproximationunderadaptivityconstraints. Advances in Neural Information Processing
Systems, 34:13524–13536, 2021.
[58] MichaelWoodroofe. Aone-armedbanditproblemwithaconcomitantvariable. JournaloftheAmerican
Statistical Association, 74(368):799–806, 1979.
[59] Yuhong Yang and Dan Zhu. Randomized allocation with nonparametric estimation for a multi-armed
bandit problem with covariates. Ann. Statist., 30(1):100–121, 2002.
[60] Kelly Zhang, Lucas Janson, and Susan Murphy. Inference for batched bandits. Advances in neural
information processing systems, 33:9818–9829, 2020.
[61] Zihan Zhang, Yuan Zhou, and Xiangyang Ji. Almost optimal model-free reinforcement learning via
reference-advantage decomposition. Advances in Neural Information Processing Systems, 33:15198–
15207, 2020.
[62] Zhijin Zhou, Yingfei Wang, Hamed Mamani, and David G Coffey. How do tumor cytogenetics in-
form cancer treatments? dynamic risk stratification and precision medicine using multi-armed bandits.
Dynamic Risk Stratification and Precision Medicine Using Multi-armed Bandits (June 17, 2019).
26