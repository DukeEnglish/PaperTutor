Gradient-based Discrete Sampling with Automatic Cyclical Scheduling
PatrickPynadath1 RiddhimanBhattacharya1 ArunHariharan1 RuqiZhang1
Abstract ods is their susceptibility to becoming trapped in local
modes(Ruder,2016;Ziyinetal.,2021),whichsignificantly
Discrete distributions, particularly in high-
reducestheaccuracyandefficiencyofsamplingresults. In
dimensionaldeepmodels,areoftenhighlymul-
continuousspaces,severalstrategiessuchascyclicalstep
timodal due to inherent discontinuities. While
sizes(Zhangetal.,2020),paralleltempering(Swendsen&
gradient-baseddiscretesamplinghasprovenef-
Wang,1986;Dengetal.,2020a),andflathistograms(Berg
fective,itissusceptibletobecomingtrappedin
&Neuhaus,1991;Dengetal.,2020b),havebeenproposed
localmodesduetothegradientinformation. To
to address this issue. When it comes to discrete distribu-
tackle this challenge, we propose an automatic
tions,whichareinherentlymoremultimodalduetotheirdis-
cyclical scheduling, designed for efficient and
continuousnature,theproblembecomesevenmoresevere.
accuratesamplinginmultimodaldiscretedistri-
Despitethepressingneed,thereisalackofmethodology
butions. Ourmethodcontainsthreekeycompo-
forgradient-baseddiscretesamplerstoeffectivelyexplore
nents:(1)acyclicalstepsizeschedulewherelarge
multimodal distributions. Current methods often fall far
stepsdiscovernewmodesandsmallstepsexploit
shortintraversingthecomplexlandscapesofmultimodal
eachmode;(2)acyclicalbalancingschedule,en-
discretedistributions,asillustratedinFigure1.
suring“balanced”proposalsforgivenstepsizes
andhighefficiencyoftheMarkovchain;and(3) Inthispaper,weproposeautomaticcyclicalschedulingfor
anautomatictuningschemeforadjustingthehy- gradient-based discrete sampling to efficiently and accu-
perparametersinthecyclicalschedules,allowing ratelysamplefrommultimodaldistributions. Tobalancebe-
adaptabilityacrossdiversedatasetswithminimal tweenuncoveringnewmodesandcharacterizingthecurrent
tuning.Weprovethenon-asymptoticconvergence mode,weparameterizeafamilyofgradient-basedproposals
and inference guarantee for our method in gen- thatspanaspectrumfromlocaltoglobalproposals. The
eraldiscretedistributions. Extensiveexperiments parameterizedproposal dynamically adjustsaccordingto
demonstratethesuperiorityofourmethodinsam- cyclicalschedulesofbothstepsizeandthebalancingparam-
plingcomplexmultimodaldiscretedistributions. eter,smoothlytransitioningfromglobalexploratorymoves
tomorelocalizedmoveswithineachcycle. Thesecyclical
schedulesareautomaticallytunedbyaspeciallydesigned
algorithm,whichidentifiesoptimalstepsizesandbalancing
1.Introduction
parametersfordiscretedistributions. Ourcontributionsare
summarizedasfollows:
Discretevariablesarecommoninmanymachinelearning
problems, highlighting the crucial need for efficient dis-
cretesamplers. Recentadvances(Grathwohletal.,2021; • Wepresentthefirstgradient-baseddiscretesampling
Zhangetal.,2022b;Sunetal.,2021;2023b;a;Xiangetal., methodthattargetsmultimodaldistributions,incorpo-
2023)haveleveragedgradientinformationindiscretedis- ratingcyclicalschedulesforbothstepsizeandbalanc-
tributions to improve proposal distributions, significantly ingparametertofacilitatetheexplorationandexploita-
boosting their efficiency. These advancements have set tionindiscretedistributions.
newbenchmarksindiscretesamplingtasksacrossgraphical
• Weproposeanautomatictuningalgorithmtoconfig-
models,energy-basedmodels,andcombinatorialoptimiza-
urethecyclicalschedule,enablingeffortlessandcus-
tion(Goshvadietal.,2023).
tomizedadjustmentsacrossvariousdatasetswithout
However, one major limitation of gradient-based meth- muchmanualintervention.
1DepartmentofComputerScience,PurdueUniversity,West
• We offer non-asymptotic convergence and inference
Lafayette,UnitedStates. Correspondenceto: PatrickPynadath
guarantees for our method in general discrete dis-
<ppynadat@purdue.edu>,RuqiZhang<ruqiz@purdue.edu>.
tributions. To our knowledge, this is the first non-
asymptoticconvergenceboundofgradient-baseddis-
1
4202
beF
72
]GL.sc[
1v99671.2042:viXraGradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
(a)GroundTruth (b)RandomWalk (c)DMALA (d)AB (e)ACS(Ours)
Figure1.Samplingona2ddistributionwithmultiplemodes.(a)showsthegroundtruth.(b)showsresultsfromarandomwalksampler.
(c)showsresultsfromDMALA(Zhangetal.,2022b)withtheoptimallymanually-tunedstepsize.(d)showsresultsfromAB(Sunetal.,
2023a).(e)showsresultsfromourmethodACS.Whiletherandomwalksamplercanfindallmodes,itscharacterizationisnoisyand
lacksdetailsforeachmode.Gradient-basedsamplers(b)and(c)effectivelycharacterizeaspecificmodebutareeasilytrappedinsome
localmodes.Ourmethod(d)canfindallmodesefficientlyandcharacterizeeachmodeaccurately.
cretesamplingtothetargetdistributionwithinference andtargetsaspecificMetropolis-Hastingsacceptancerate,
guarantees,whichcouldbeofindependentinterest. ratherthanmaximizingtheaveragecoordinateschangedper
step. (3)Ourmethodappliestobothsamplingandlearn-
• We demonstrate the superiority of our method inginenergy-basedmodels(EBM),whereastheirapproach
for both sampling and learning tasks includ- cannotbeusedforEBMlearningtasks.
ing restricted Boltzmann machines and deep
energy-based models. We release the code at SamplingonMultimodalDistributions Thereexistsev-
https://github.com/patrickpynadath1/ eral sampling methods targeting discrete multimodal dis-
automatic_cyclical_sampling tributions,suchassimulatedtempering(Marinari&Parisi,
1992),theSwendsen-Wangalgorithm(Swendsen&Wang,
1987), and the Wolff algorithm (Wolff, 1989). However,
2.RelatedWork thesemethodsusuallyuserandomwalkorGibbssampling
astheirproposals. Itisunclearhowthesemethodscanbe
Gradient-based Discrete Sampling Zanella (2017) in- adaptedforgradient-baseddiscretesampling.
troducedafamilyoflocallyinformedproposals,layingthe
In continuous spaces, various gradient-based methods
foundationforrecentdevelopmentsinefficientdiscretesam-
havebeendevelopedspecificallyformultimodaldistribu-
pling. Buildinguponthis,Grathwohletal.(2021)further
tions(Zhangetal.,2020;Dengetal.,2020a;b). Ourmethod
incorporatesgradientapproximation,significantlyreducing
distinguishes from the cyclical step size in Zhang et al.
computational costs. Following these pioneering efforts,
(2020)byincorporatinganadditionalcyclicalbalancingpa-
numerousstudieshaveproposedvariousgradient-baseddis-
rameterscheduleandanautomatictuningscheme,whichare
cretesamplingtechniques(Rhodes&Gutmann,2022;Sun
crucialforefficientexplorationindiscretedistributions.Fur-
etal.,2021;2022;2023b;Xiangetal.,2023). Zhangetal.
thermore,ourtheoreticalanalysisofconvergenceisdifferent
(2022b)developsadiscreteLangevinproposal,translating
fromthatinZhangetal.(2020)whichreliesoncontinuous
thepowerfulLangevinalgorithmtodiscretespaces. San-
stochasticprocesses.
sone(2022)introducesaself-balancingmethodtooptimize
thebalancingfunctionsinlocallybalancedproposals.While
ourworkalsoutilizesanadaptivephase, itdiffersinthat 3.Preliminaries
ourparameterizationextendsbeyondthelocalregime,and
3.1.ProblemDefinition
ourproposalparameterizationisconsiderablysimpler.
Perhapsthemostcloselyrelatedstudyistheany-scalebal- We are concerned with the task of sampling from some
anced sampler (Sun et al., 2023a). This method uses a targetdistributiondefinedoveradiscretespace
non-localbalancingproposalandadaptivelytunesit. Our
1
work, however, differs in several key aspects: (1) We fo- π(θ)= exp(U(θ)), θ ∈Θ.
Z
cus on combining both local and non-local proposals to
effectivelycharacterizemultimodaldiscretedistributions,as Here,θisaddimensionaldiscretevariableindomainΘ,U
opposedtofocusingonasingleoptimalproposal. (2)Our istheenergyfunction,andZ isthenormalizingconstant.
automatictuningalgorithmadjuststhestepsizeandbalanc- Wemakethefollowingassumptionsofthedomainandthe
ingparameterbyconsideringthespecialdiscretestructures energyfunction,followingtheliteratureofgradient-based
2Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
discretesampling(Grathwohletal.,2021;Sunetal.,2021; 2023a))areveryeffectiveatcharacterizingagivenmode,
Zhangetal.,2022b): (1)Thedomainiscoordinatewisely theytendtogettrappedinsomesmallneighborhood,pre-
factorized,Θ=Πd Θ . (2)TheenergyfunctionU canbe venting a proper characterization of the distribution as a
i=1 i
extendedtoadifferentiablefunctioninRd. whole.
Wecanunderstandthisbehaviorofgradientbasedsamplers
3.2.LocallyBalancedProposals
bycomparingthemtoarandomwalksampler(RW),which
Zanella(2017)introducesafamilyofinformedproposals, isabletoexploreallthemodesbutunabletofullycharacter-
whichisdefinedbelow: izethedetailofeachone. WhiletheRWsamplerproposes
movementsuniformlyoverthesamplespace,gradientbased
(cid:16) (cid:17)
Q (θ′|θ)=
g π π( (θ θ′ )) K α(θ,dθ′)
(1)
samplersproposemovementbasedonthegeometryofthe
g,α Z (θ) distributionascapturedbythegradient. Becausethepro-
g,α
posedmovementsareinthedirectionofincreasingdensity,
Here, K α isakernelthatdeterminesthescaleofthepro- these proposals are able to characterize a given mode in
posalwhereαplaysasimilarroleasthestepsize. g(t)isa detail. Atthesametime,theseproposalshinderescapeto
balancingfunctionthatdetermineshowtoincorporateinfor- moredistantmodesasthegradientpointsawayfromtheir
mationaboutπ. Ifg(t) = tg(1),theproposalbecomesa direction. Forthisreason,weobservethatlocalmodesare
t
locallybalancedproposal,whichisasymptoticallyoptimal ableto“trap”gradient-basedsamplers.
inthelocalregime,thatis,whenthestepsizeα→0.
4.2.ParameterizedProposalDistribution
4.AutomaticCyclicalSampler
Toderiveanautomaticschedulefortheproposal,weneed
We aim to develop a sampler capable of escaping local toparameterizetheproposalfirst. WedefineK α andg(t)
modesingeneralmultimodaldiscretedistributions,includ- intheinformedproposal(Zanella,2017)asfollows:
ingthosethatappearindeepenergybasedmodels(EBMs). exp−||θ′−θ||2
First,wemotivatetheuseofthecyclicalschedulebydemon- K (θ,dθ′)= 2α , α∈(0,∞) (3)
α Z
stratingtheissueofgradient-basedsamplerstogetstuckin
g(t)=tβ, β ∈[0.5,1) (4)
localmodesonatoydataset. Wethenpresentoursampler’s
parameterization of the step size and balancing function. whereβ iscalledabalancingparameter. α → 0,β = 0.5
Next, we introduce a cyclical schedule for the proposal correspondtoalocally-balancedproposalandα→∞,β =
distributionthatenableseffectiveexplorationandcharac- 1 correspond to a globally-balanced proposal. Values in
terizationofdiscretemultimodaldistributions. Finally,we betweenresultininterpolationsbetweenlocally-balanced
developanautomatictuningmethodthatsimplifiesthepro- and globally-balanced proposals. Note that β ∈ (0,1) in
cessofidentifyinghyperparametersincyclicalschedules. Sunetal.(2023a)whileourrangeisnarrower.
WesubstitutethesedefinitionsintoEquation1andapply
4.1.MotivatingExample: ASyntheticMultimodal
thefirstorderTaylorexpansiontogetthefollowing:
DiscreteDistribution
(cid:18) ||θ′−θ||2(cid:19)
To demonstrate the crucial issue of local modes trapping Q (θ′|θ)∝exp β(U(θ′)−U(θ))−
α,β 2α
gradient-based samplers, we construct a 2-dimensional
dataset consisting of integers. We define Θ =
≈exp(cid:18)
β(∇ U(θ)(θ′−θ))−
||θ′−θ||2(cid:19)
(5)
{0,1,···N}2, where N is the maximum value for each θ 2α
coordinate. Givenasetofmodes{µ ,µ ,...µ }, wede-
1 2 l As in Zhang et al. (2022b), we use the assumption
finetheenergyofasampleθasfollows:
of coordinate-wise factorization to obtain the following
U(θ)=log(cid:32) (cid:88) i=l 1exp(cid:18) ||θ− 2σµ i||2(cid:19)(cid:33)
. (2)
co Co ard t(cid:18)in Sat oe f- tw mi as xe (cid:18)pr βo ∇po Usa (l θf )u i(n θc i′ti −on θQ i)i α −,β( (θ θi′ i′|θ
−
2) α:
θ
i)2(cid:19)(cid:19)
(6)
Thisdistributionenableseasycomparisonbetweendifferent
methodsintermsoftheirabilitytobothexploreandexploit InordertomaketheresultingMarkovchainreversible,we
thetargetdistribution.Wedemonstratetheresultsofvarious applytheMetropolis-Hastingscorrection,whereweaccept
samplers in Figure 1. More experimental details can be theproposedstepwiththefollowingprobability:
foundinAppendixD.1. (cid:18)
Q
(θ|θ′)(cid:19)
A(θ′|θ,α,β)=min 1,exp(U(θ′)−U(θ))) α,β .
Avisualcomparisonrevealsthatwhilegradient-basedsam- Q (θ′|θ)
α,β
plers(DMALA(Zhangetal.,2022b)andAB(Sunetal., (7)
3Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
Insummary,weparameterizeourproposalasinEquation(6) follows:
which includes a spectrum of local and global proposals.
β =argmax (cid:0)E [{A(θ′|θ,α ,β)](cid:1)
Our proposal is determined by two hyperparameters, the i β∈[.5,βi−1] θ∼π,θ′∼Qα,β i
stepsizeαandthebalancingparameterβ. (9)
Intuitively,thisdefinitionmeansthatthebestβ foragiven
i
4.3.CyclicalHyperparameterSchedules
stepsizeα maximizestheaverageacceptancerateforthe
i
Cyclical Step Size Schedule In order to effectively ex- proposalfunctionQ α,β.Italsoconveysthatlargerstepsizes
plorethewholetargetdistributionwhileretainingtheability willhavelargerbalancingparameters. SeeFigure2fora
toexploitlocalmodes,weadoptthecyclicalstepsizesched- visualizationofthisschedule.
ulefromZhangetal.(2020). Thedefinitionofstepsizeα
Given α, β schedules, we introduce the cyclic sampling
foriterationkisasfollows:
algorithminAlgorithm1. Notethatitincursnoextraover-
headcomparedtopreviousgradient-baseddiscretesampling
(cid:18) (cid:18) (cid:19) (cid:19)
πmod(k,s)
α =max α ·cos +1,α (8) methodsasitonlyadjustshyperparametersαandβ. Byus-
k max s min ingacombinationoflargeandsmallαandβ,weenablethe
samplertoexplorethedistributionfullywithoutsacrificing
Herewedefinetheinitialstepsizeasα max,theminimum theabilitytocharacterizeeachmode. Thisisdemonstrated
stepsizeasα min,andthenumberofsamplingstepspercycle inFigure1e.
as s. Differing from the cyclical schedule in continuous
spaces (Zhang et al., 2020), we additionally add α to Algorithm1CyclicalSamplingAlgorithm
min
makesurethateventhesmalleststepsizeremainseffective
Require: stepsizeschedule{α }s ,balancingparameter
in discrete spaces. This schedule effectively captures the k k=1
schedule{β }s ,cyclenumbern,stepspercycles
balancebetweenexplorationandexploitationthatwewant k k=1
1: samples←[]
toleverageinoursampler. SeeFigure2foravisualization.
2: forcyclecinrangendo
3: forstepkinrangesdo
4: θ ←samples[-1]
2.5 0.9 5: forcoordinateiinrangeddo
2.0 0.8 6: constructQi (·|θ)asin(6)
αk,βk
1.5 7: sampleθ′ ∼Qi (·|θ) 0.7 i αk,βk
1.0 8: endfor
0.5 0.6 9: samples←θ′withprobabilityasin(7)
0.5 10: endfor
0 20 40 60 80 100 0 20 40 60 80 100
Sampling Step Sampling Step 11: endfor
Figure2.α-schedulealongwithcorrespendingβ schedule. The 12: returnsamples
initiallargestepsenablethesamplertoexploredifferentregions
ofthedistribution,whilethesmallerstepsenablegoodcharacteri-
zationofeachregion.Thebalancingparameterβvarieswiththe
stepsizetoenablehighacceptanceratesforallstepsizes. 4.4.AutomaticScheduleTuning
ForschedulesinEquations(8)and (9), wehaveparame-
tersα ,α ,and{β ,β ...β }tobedecided. Inthis
Cyclical Balancing Schedule As discussed in Zanella max max 1 2 s
section,wewillintroduceanautomatictuningalgorithmto
(2017);Sunetal.(2023a),thebalancingparametershould
easilyfindsuitablevalues.
varywithdifferentstepsizestoachievea“balanced”pro-
posal. AbalancedproposalensuresthattheMarkovchain
MainIdea Ourautomatictuningalgorithmdependson
is reversible with respect to a certain distribution, which
the initial balancing parameter β , the final balancing
willconvergeweaklytothetargetdistribution. Forexample, max
parameterβ ,atargetacceptancerateρ∗,andthenumber
whenthestepsizeα→0,theoptimalbalancingparameter min
of steps per cycle s. These values are relatively easy to
isβ=.5,whereasforα→∞,theidealbalancingparameter
select,asdetailedinAppendixA.Below,weassumethey
becomesβ =1.
are already determined. The tuning algorithm constructs
Thus for a schedule of step sizes, each α requires a dif- thehyperparameterschedulebyfirstestimatingtheoptimal
i
ferentβ ∈ [.5,1),withlargerstepsizeshavingβ closer choicesforα andα basedonthetargetacceptancerate
i i max min
to1andsmallerstepsizeshavingβ closerto0.5. Using ρ∗. Oncethesestepsizesaredetermined,thefullstepsize
i
theMetropolis-Hastingsacceptanceratetocharacterizethe schedulecanbedeterminedusing(8). Finally,thetuning
quality of a given α,β pair, we define the value of β as algorithmconstructsacorrespondingbalancingparameter
i
4
eziS
petS
retemaraP
gnicnalaBGradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
schedule using the definition in (9). We summarize our
methodinAlgorithm2,wherethesubroutinesaredetailed 1.0
inAppendixA.Ourautomatictuningintroducesminimal acceptance rate with beta=.8
target acceptance rate
overheadrelativetothemoreexpensivesamplingprocess. 0.8
Forexample,inSection6,weuse500stepsasthebudget
forAlgorithm2wherethetotalnumberofsamplingstepsis 0.6
atleast5000.
0.4
In short, our tuning algorithm adopts an alternative opti-
mizationstrategy,leveragingexistingknowledgeabouthy- 0.2
perparametervalues(e.g. β andβ shouldbearound
min max
0.5and1respectively). Whileestimatingthebestpairα,β 0.0
0 1 2 3 4 5
ischallengingduetotheirinterdependence,itismucheasier 0.190.74 Step Size
tofixoneandoptimizetheother(Sunetal.,2023a).
Figure3.Acceptance rate v.s. step size on EBM sampling on
MNISTshowsanon-monotonicrelationship.
Algorithm2AutomaticScheduleTuningAlgorithm
Require: β = .5, β , target acceptance rate ρ∗, ini-
min max
webeginwithanupperlimitα andgraduallyreducethe
tialstateθ ,stepspercycles,initiallargeststepsize ceil
init
stepsize,ensuringwedonotoverlookanylargerαvalues
α =60,initialsmalleststepsizeα =.05
ceil floor
thatfulfillourcriteria. Detailedimplementationisoutlined
1: θ ←InitBurnin(α ,β ,θ )
ceil max init
2: α ←EstAlpha(α ,β ,θ,ρ∗,MAX=False) inAlgorithm4inAppendix.
min floor min
3: α ←EstAlpha(α ,β ,θ,ρ∗,MAX=True)
max ceil max
Estimating Balancing Schedule After setting the start
4: Constructα-schedoflengthsusing(8)
and end pairs for the α and β schedules, we now define
5: β-sched←EstBalSched(α-sched,β ,β ,θ)
max min
intermediateβ values. Astheentirestepsizescheduleis
6: returnα-sched,β-sched
fixedby(8),theproblemistodeterminethebestbalancing
parameterforeachstepsize. Asimplestrategyistotestdif-
Estimatingα max,α
min
Foragivenβ max,β min,ourgoalis ferentβspacedoutevenlythroughouttheinterval[.5,β i−1]
tofindstepsizesα ,α thatenableanacceptancerate andselectthebestvalueintermsofacceptancerate. This
max min
closetoρ∗. Wecanformallystatethisgoalasfollows: approachleveragestheobservationthatsmallerstepsizes
tendtohavesmalleroptimalbalancingconstants. Detailed
J(α,β)=E θ∼π(cid:2)E θ′∼Qα,β(·|θ)|ρ∗−A(θ′|θ,α,β)|(cid:3) . implementationisgiveninAlgorithm5inAppendix.
(10)
5.TheoreticalAnalysis
Givenβ ,β ,weconstructthefollowingobjectivesto
max min
pickthecorrespondingα ,α :
max min Inthissection,wepresentaconvergencerateanalysisfor
Algorithm1. Forgeneralstepsizeandbalancingparameter
α =max{αs.tJ(α,β )≈0}
max max schedules,i.e.,ateachcycle,thealgorithmwillgothrough
α =min{αs.tJ(α,β )≈0}. (11)
min min sstepsinwhichitwillusestepsizesα ,α ,··· ,α and
1 2 s
balancing parameters β ,β ,··· ,β . Note that for each
Bydefiningtheinitialandfinalstepsizesinthismanner,we 1 2 s
pair(α ,β ),wehaveaMarkovtransitionoperatorwhich
ensurethatourcyclicalscheduleincludesawiderrangeof i i
welabelP fori=1,2,··· ,s. HencetheMarkovoperator
hyperparameterpairswithdifferenttrade-offsintermsof i
forasinglecycleisgivenbyPˆ = P P ···P . Wehave
explorationandexploitation. 1 2 s
thefollowingtwoassumptions:
Nowwediscusshowtosolve(11). Toestimateα max,we Assumption 5.1. The function U(·) ∈ C2(Rd) has M-
beginwithasufficientlylargestepsizeandincrementally
Lipschitzgradient. Thatis
decreaseittofindthestepsizethatyieldsρ∗. Ourapproach
divergesfromexistingworks(Sunetal.,2023a)whichoften
∥∇U(θ)−∇U(θ′)∥≤M∥θ−θ′∥.
startfromsmallstepsizes. Thisisbecauseweobservedthat
foragivenβ,therecanbemultipleαvaluesyieldingthe NotethatitimplicitlyassumesthatthesetindomainΘis
sameacceptancerate,asshowninFigure3.Wehypothesize finite. Wedefineconv(Θ)astheconvexhullofthesetΘ.
thatitiscausedbythestructuresofthediscretedistribution, Assumption5.2. Foreachθ ∈Rd,thereexistsanopenball
such as the correlation among coordinates. Our goal is containingθofsomeradiusr ,denotedbyB(θ,r ),such
θ θ
toidentifythemaximumfeasiblestepsizethatmeetsthe thatthefunctionU(·)ism -stronglyconcaveinB(θ,r )
θ θ
desiredacceptanceratetoenhanceexploration. Tothisend, forsomem >0.
θ
5
etaR
ecnatpeccA
HMGradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
Assumptions5.1and5.2arestandardinoptimizationand Theorem 5.4. Let Assumptions 5.1-5.2 hold with α <
sampling literature (Bottou et al., 2018; Dalalyan, 2017). 1/βM. ThenfortheMarkovchainP,thefollowinghold:
Under Assumption 5.2, U(·) is m-strongly concave on i. P isuniformlyergodicwith
conv(Θ),followingLemmaC.3inAppendix.
∥Pn−π∥ ≤(1−ϵ )n.
Wedefinediam(Θ)=sup ∥θ−θ′∥andϵ tobe TV β,α
θ,θ′∈Θ αi,βi
ii. For any real-valued function f and samples
exp(cid:26) −(cid:18) 1 +βiM− βim(cid:19) diam(Θ)2−∥∇U(a)∥diam(Θ)(cid:27) . X 1,X 2,X 3,··· ,X nfromP,onehas
2αi 2
TheMarkovkernelcorrespondingtoeachP i ineachstep √ n(cid:32) 1 (cid:88)n f(X )−(cid:88) f(θ)π(θ)(cid:33) →d N(0,σ˜2)
ofthecycleinAlgorithm1is n i ∗
i=1 θ∈Θ
pi(θ′|θ)=A(θ′|θ,αi,βi)Q αi,βi(θ′|θ)+(1−L(θ))δ θ(θ′) (12) forsomeσ˜ ∗ >0asn→∞.
Proof. The proof directly follows from our Lemma 5.3
where
andJones(2004)[Corollary5].
L(θ)=
(cid:88) (cid:18) π(θ′)Q αi,βi(θ|θ′) ∧1(cid:19)
Q (θ′|θ)
π(θ)Q (θ′|θ) αi,βi Notethatasα→0,wehaveϵ →1whichimpliesthat
θ′∈Θ
αi,βi β,α
small step sizes result in low convergence rates. This is
isthetotalrejectionprobabilityfromθ. Finally,recallthat intuitive as the algorithm could not explore much in this
the total variation distance between two probability mea- case. Furthermore,ourresultssuggestthatlargeβ restricts
suresµandν,definedonsomespaceΘ⊂Rdis α to small values. Given that large β generally requires
largeα,ourfindingsimplyanupperboundforthestepsize.
∥µ−ν∥ = sup |µ(A)−ν(A)|
TV
A∈B(Θ) 5.2.AdaptiveStepSizeandBalancingParameter
whereB(Θ)isthesetofallmeasurablesetsinΘ. Now we tackle the case of varying step sizes and balanc-
ing parameters. Each cycle has s steps with step sizes
5.1.ConstantStepSizeandBalancingParameter α ,α ,··· ,α and balancing parameters β ,β ,··· ,β .
1 2 s 1 2 s
Notethatthiscaseismorechallengingasateachstepthe
ToanalyzeAlgorithm1withstepsizeandbalancingparam-
transition operator changes and the Markov chain is no
eterschedules,wefirstsolveasimplerproblemwherethe
longerhomogeneous. However,themarginalchainforeach
stepsizeandbalancingparameterarefixedandthenextend
cycle is indeed homogeneous and can be analyzed. We
theanalysistothesettingofAlgorithm1.
presentourresultsinthissettingasfollows:
Ourmainmethodofproofistoestablishuniformergodicity Theorem5.5. LetAssumptions5.1and5.2holdwithα <
i
oftheMarkovchainP,forasingleα,β,byestablishinga 1/β M ,i = 1,2,···s. ThenfortheMarkovchainPˆ,the
i
uniformminorizationforP. Wedenotethetransitionkernel followinghold
forthisMarkovchainP asp(· | ·),whichisgivenin(12) i. Pˆ isuniformlyergodicwith
withα ,β replacedbyafixedα,β.
i i
(cid:13) (cid:13)
Lemma5.3. LetAssumptions5.1-5.2withα< β1
M
hold. (cid:13) (cid:13)Pˆn−π(cid:13)
(cid:13) TV
≤(1−ϵ βs,αs)n.
ThenfortheMarkovchainP wehave,foranyθ,θ′ ∈Θ,
ii. For any real-valued function f and samples
exp{βU(θ′)} X ,X ,X ,··· ,X fromPˆ,onehas
p(θ |θ′)≥ϵ , 1 2 3 n
β,α (cid:80) exp{βU(θ′)}
θ′∈Θ √ n(cid:32) 1 (cid:88)n f(X )−(cid:88) f(θ)π(θ)(cid:33) →d N(0,σ˜2)
where n i ∗
i=1 θ∈Θ
(cid:26) (cid:18) (cid:19)
1 βm
ϵ β,α =exp − 2α +βM − 2 diam(Θ)2 forsomeσ˜ ∗ >0asn→∞,where,
(cid:26) (cid:18) (cid:19) (cid:27)
−∥∇U(a)∥diam(Θ)} 1 β m
ϵ =exp − +β M − s diam(Θ)2
βs,αs 2α s 2
s
witha∈argmin ∥∇U(θ)∥.
θ∈Θ ·exp{−∥∇U(a)∥diam(Θ)}
Proof. TheproofisprovidedinAppendixC.1. witha∈argmin ∥∇U(θ)∥.
θ∈Θ
6Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
Proof. The proof directly follows from our Lemma 5.3, andthosefromBlock-Gibbs. TofurthertestwhetherACS
PropositionC.1andJones(2004)[Corollary5]. canescapelocalmodes,weinitializeallsamplerstostart
withinthemostlikelymodeofthedatasetasmeasuredby
BothTheorems5.4and5.5holduniformlyoverallfunctions themodeldistribution. Block-Gibbscanberegardedasthe
in the class of functions with at least a local minima in groundtruthforthedistributionasitleveragestheknown
Θ. The Central Limit Theorem results in Theorems 5.4 structureoftheRBM.
and5.5implythatwemayperforminferenceonthetarget
distributionπ(·)eventhoughtheasymptoticvariancesare
Results As seen in Figure 4, our method ACS consis-
unknown,aswemayperformbatch-meanstoestimatethese
tentlyachievescompetitiveorsuperiorconvergenceacross
variances(Vatsetal.,2019).
alldatasets,withnotableimprovementsondatasetslikekm-
Insummary,wehaveestablishedageometricconvergence nistandcaltech. InFigure6,weseethatACSsignificantly
ratetothetargetdistributionforoursampler. Previousre- outperforms baseline methods in both final convergence
searchhasonlyestablishedasymptoticconvergence(Zhang andconvergencespeed. ThissuggeststhatACSeffectively
etal.,2022b)orrelativeconvergenceratebounds(Grath- escapestheinitialmodewhereasbaselinemethodssuffer.
wohletal.,2021)forgradient-baseddiscretesamplers. To These results demonstrate ACS’s ability to efficiently ex-
the best of our knowledge, our results present the first ploreandaccuratelycharacterizemultiplemodes. Wefur-
non-asymptoticconvergenceboundsthatexplicitlyquantify ther include the results of generated images and runtime
thedistancebetweentheestimatedandtargetdistributions. comparisoninAppendixD.2.
Sincetheboundsareexplicit,theyalsoprovideuswithcon-
servativeguaranteesonthenumberofiterationsneededto 6.1.2.DEEPEBMSAMPLING
achieveacertainleveloferror,ie.,conservativemixingtime
Toassesstheperformanceofoursamplingmethodonmore
guarantees. Further,ourconvergenceboundalsoshowsthat
complicated energy functions, we adopt a similar experi-
discretespacesplayafundamentalpartintheergodicnature
ment set-up to Sun et al. (2023a), where we measure the
ofthesealgorithms. Westronglybelievethatsomeofthese
mixingtimeofsamplingalgorithmsbyhowquicklytheycan
ideasmaybecarefullyextendedtocompactdomains(but
achievebetterqualitysamplesasmeasuredbythemodel’s
extendingtheseideastoRddoesnotseempossible).
unnormalizedenergyfunction.
6.Experiments
Results Figure5showsthatourmethodisabletoachieve
WecallourmethodthatcombinesAlgorithm1and2Au- competitiveperformanceonalldatasets.OnStatic/Dynamic
tomaticCyclicalSampler (ACS).Forsamplingtasks, we MNIST and Omniglot, our method is able to burn in far
compareourmethodtoGibbs-with-Gradient(GWG)(Grath- quicker than other methods. Our method maintains com-
wohl et al., 2021), Any-scale sampler (AB) (Sun et al., petitiveperformanceonCaltechaswell. Formoreamore
2023a), and Discrete Metropolis Adjusted Langevin Al- detaileddiscussionontheresults,seeAppendixD.3.
gorithm(DMALA)(Zhangetal.,2022b),whicharepopular
andrecentgradient-baseddiscretesamplers. Forlearning 6.2.LearningEnergyBasedModels
tasks,weomitAny-scalesamplerasitisnotoriginallyap-
OnecommonapplicationofMCMCtechniquesisthelearn-
pliedtothemodellearningtasks. Moreexperimentaldetails
ing of energy-based models (EBMs). EBMs are a class
canbefoundinAppendixD.
of generative models where a neural network parameter-
izedbyϕrepresentsanenergyfunctionE . Thesemodels
6.1.SamplingTasks ϕ
aretypicallytrainedviaPersistentContrastiveDivergence
6.1.1.RBMSAMPLING (PCD).ThedetailsonACSforEBMlearningcanbefound
inAppendixB.
We evaluate our method on Restricted Boltzman Ma-
chines(RBMs)trainedonvariousbinarydatasets. RBMs
6.2.1.LEARNINGRBM
learnthefollowingenergyfunction:
WedemonstratethebenefitsofACSonRBMs. Weevaluate
(cid:88)
U(θ)= Softplus(Wθ+a) i+bTθ the learned model using Annealed Importance Sampling
i (AIS)(Neal,2001). Wecomparethesamplingmethodsof
interesttoBlock-Gibbs.
{W,a,b} are parameters for the model, and θ ∈ {0,1}d.
FollowingZhangetal.(2022b);Grathwohletal.(2021),we ResultsFromTable1,wenotethatACSoutperformsthe
randomlyinitializeallsamplersandmeasuretheMaximum baselinesonallofthedatasets. Weincludemorediscussion
MeanDivergence(MMD)betweenthegeneratedsamples ontheseresultsandthegeneratedimagesinD.4.
7Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
mnist kmnist emnist omniglot caltech
4.0
3.0 4.0 5.6 4.0 G DMW AG LA
4.5 3.5 4.5 5.8 4.5 A ACB S (Ours)
5.0 6.0
5.5 4.0 5.0 6.2 5.0
4.5 5.5 6.4 5.5
6.0
5.0 6.0 6.6
6.5 6.8 6.0
5.5 6.5
0 2000 4000 0 2000 4000 0 2000 4000 0 2000 4000 6.50 2000 4000
Sampling Iteration
Figure4.RBMsamplingwithrandominitialization.ACSconsistentlyachievescompetitiveorsuperiorconvergenceacrossalldatasets.
static_mnist dynamic_mnist omniglot caltech
350
400 400 400 1000
450 450 450 1100
500 500 500 1200
550 550 550 1300 GWG
600 600 600 D ABMALA
650 1400 ACS
0 10002000300040005000 0 10002000300040005000 0 10002000300040005000 0 10002000300040005000
Sampling Iterations
Figure5.ConvergenceofvarioussamplersondeepconvolutionEBMsasmeasuredbyhigherenergy.ACSdemonstratesquickconvergence
acrossallbaselines.
3.0 mnist emnist Table1.LoglikelihoodscoresforRBMlearningontestdataas
3.5 3.0 estimatedbyAIS.ACSoutperformsallgradient-basedbaselines
4.0 3.5 acrossalldatasets.
4.5 4.0 GB GWG DMALA ACS
5.0
4.5
5.5 GWG
6.0
5.0 D ABMALA MNIST -191.98 -387.34 -278.35 -249.55
6.5 0 2000 4000 5.5 0 ACS ( 2O 0u 0r 0s) 4000 eMNIST -317.78 -590.97 -324.34 -304.96
Sampling Iteration kMNIST -357.69 -681.28 -436.3538 -407.39
Figure6.RBM sampling with mode initialization. ACS main- Omniglot -161.73 -276.81 -222.61 -220.71
tainsfastconvergencewhereasbaselinemethodshavesignificantly Caltech -511.65 -827.45 -427.29 -396.04
slowerconvergenceratesduetobeingtrappedinthemode.
Table2.DeepConvolutionEBMLoglikelihoodscoresontestdata
6.2.2.LEARNINGEBM asestimatedbyAIS.GWGresultsaretakenfrom(Grathwohletal.,
2021).ACSisabletoachievebetterresultsthanthebaselines.
WealsotestACSondeepconvolutionalEBMstodemon- GWG* DMALA ACS
strate that our method scales to more complex deep neu-
StaticMNIST 80.01 -79.93 -79.76
ralnetworks. Weuse10samplestepsperiterationonall
DynamicMNIST -80.51 -80.13 -79.70
datasetsexceptCaltech,whereweuse30. Weincludemore
Omniglot -94.72 -100.08 -91.32
experimentaldetailsalongwiththegeneratedimagesinD.5.
Caltech -96.20 -99.35 -88.34
Results The results in Table 2 show that our method
outperformsDMALAandGWGsignificantly,evenwhen discretespaces. First,wedemonstratethatgradient-based
GWGusesalargernumberofsamplingsteps(40). Thiscan samplersarepronetogettingtrappedinlocalmodes,pre-
beattributedtoourmethodsabilitytoexploremoremodes venting a full characterization of target distributions. To
ataquickerrate,enablingthetrainingprocesstoprovide addressthisissue,wecombineacyclicalstepsizeschedule
bettergradientupdatesforthemodel. withacyclicalbalancingparameterschedulealongwithan
automatictuningalgorithmtoconfiguretheseschedules.We
Conclusion alsotheoreticallyestablishthenon-asymptoticconvergence
boundofourmethodtothetargetdistributioninadditionto
Inthiswork,weproposeAutomaticCyclicalSampler(ACS) providingextensiveexperimentalresults.
tomoreeffectivelycharacterizemultimodaldistributionsin
8
DMM
goL
DMM
goL
ygrenE
egarevAGradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
References Sansone, E. Lsb: Local self-balancing mcmc in discrete
spaces. InInternationalConferenceonMachineLearn-
Berg,B.A.andNeuhaus,T. Multicanonicalalgorithmsfor
ing,pp.19205–19220.PMLR,2022.
firstorderphasetransitions. PhysicsLettersB,267(2):
249–253,1991.
Sun,H.,Dai,H.,Xia,W.,andRamamurthy,A. Pathauxil-
Bottou, L., Curtis, F. E., and Nocedal, J. Optimization
iaryproposalformcmcindiscretespace.InInternational
methodsforlarge-scalemachinelearning. SiamReview, ConferenceonLearningRepresentations,2021.
60(2):223–311,2018.
Sun,H.,Dai,H.,andSchuurmans,D. Optimalscalingfor
Dalalyan, A. S. Theoretical guarantees for approximate locallybalancedproposalsindiscretespaces,2022.
samplingfromsmoothandlog-concavedensities. Jour-
naloftheRoyalStatisticalSocietySeriesB:Statistical Sun,H.,Dai,B.,Sutton,C.,Schuurmans,D.,andDai,H.
Methodology,79(3):651–676,2017. Any-scalebalancedsamplersfordiscretespace. InThe
EleventhInternationalConferenceonLearningRepresen-
Deng,W.,Feng,Q.,Gao,L.,Liang,F.,andLin,G. Non- tations,2023a. URLhttps://openreview.net/
convexlearningviareplicaexchangestochasticgradient forum?id=lEkl0jdSb7B.
mcmc.InInternationalConferenceonMachineLearning,
pp.2474–2483.PMLR,2020a. Sun, H., Dai, H., Dai, B., Zhou, H., and Schuurmans, D.
Discretelangevinsamplersviawassersteingradientflow.
Deng, W., Lin, G., and Liang, F. A contour stochastic
InInternationalConferenceonArtificialIntelligenceand
gradientlangevindynamicsalgorithmforsimulationsof
Statistics,pp.6290–6313.PMLR,2023b.
multi-modaldistributions. Advancesinneuralinforma-
tionprocessingsystems,33:15725–15736,2020b.
Swendsen, R. H. and Wang, J.-S. Replica monte carlo
Du,Y.andMordatch,I. Implicitgenerationandmodeling simulation of spin-glasses. Physical review letters, 57
withenergybasedmodels. AdvancesinNeuralInforma- (21):2607,1986.
tionProcessingSystems,32,2019.
Swendsen, R. H. and Wang, J.-S. Nonuniversal critical
Goshvadi,K.,Sun,H.,Liu,X.,Nova,A.,Zhang,R.,Grath- dynamics in monte carlo simulations. Physical review
wohl,W.S.,Schuurmans,D.,andDai,H.Discs:Abench- letters,58(2):86,1987.
markfordiscretesampling. InThirty-seventhConference
onNeuralInformationProcessingSystemsDatasetsand Tieleman,T. Trainingrestrictedboltzmannmachinesusing
BenchmarksTrack,2023. approximationstothelikelihoodgradient.InProceedings
ofthe25thinternationalconferenceonMachinelearning,
Grathwohl, W., Swersky, K., Hashemi, M., Duvenaud,
pp.1064–1071,2008.
D., and Maddison, C. J. Oops I took A gradient:
Scalable sampling for discrete distributions. CoRR, Vats,D.,Flegal,J.M.,andJones,G.L. Multivariateoutput
abs/2102.04509,2021. URLhttps://arxiv.org/ analysisformarkovchainmontecarlo. Biometrika,106
abs/2102.04509. (2):321–337,2019.
Hinton,G.E. Trainingproductsofexpertsbyminimizing
Wolff,U. Collectivemontecarloupdatingforspinsystems.
contrastivedivergence. Neuralcomputation,14(8):1771–
PhysicalReviewLetters,62(4):361,1989.
1800,2002.
Xiang,Y.,Zhu,D.,Lei,B.,Xu,D.,andZhang,R. Efficient
Jones, G. L. On the markov chain central limit theorem.
informedproposalsfordiscretedistributionsvianewton’s
2004.
seriesapproximation. InInternationalConferenceonAr-
Marinari, E. and Parisi, G. Simulated tempering: a new tificialIntelligenceandStatistics,pp.7288–7310.PMLR,
monte carlo scheme. Europhysics letters, 19(6):451, 2023.
1992.
Zanella,G. Informedproposalsforlocalmcmcindiscrete
Neal,R.M. Annealedimportancesampling. Statisticsand
spaces,2017.
computing,11:125–139,2001.
Zhang,R.,Li,C.,Zhang,J.,Chen,C.,andWilson,A.G.
Rhodes, B. and Gutmann, M. Enhanced gradient-based
Cyclical stochastic gradient mcmc for bayesian deep
mcmcindiscretespaces,2022.
learning. InInternationalConferenceonLearningRep-
Ruder, S. An overview of gradient descent optimization resentations, 2020. URL https://openreview.
algorithms. arXivpreprintarXiv:1609.04747,2016. net/forum?id=rkeS1RVtPS.
9Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
Zhang, R., Liu, X., and Liu, Q. A langevin-like sampler
fordiscretedistributions. InInternationalConferenceon
MachineLearning,pp.26375–26396.PMLR,2022a.
Zhang,R.,Liu,X.,andLiu,Q. Alangevin-likesamplerfor
discretedistributions,2022b.
Ziyin, L., Li, B., Simon, J. B., and Ueda, M. Sgd can
convergetolocalmaxima. InInternationalConference
onLearningRepresentations,2021.
10Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
A.DetailsofAutomaticCyclicalSamplerAlgorithm
InitialBurnin Wefindthatinordertoproducemeaningfulestimatesfortheobjectivein(10),itisnecessarytoburninthe
MCMCsamplingchain. Thisisduetothedependenceoftheacceptancerateoncurrentsampleθ. Ifweuseθverylowin
densitywithrespecttothetargetdistribution,theacceptanceratesestimatedbythetuningalgorithmwillloseaccuracyas
thesamplerconvergestothetargetdistribution. Inordertoavoidthisissue,werunaquickburn-instagewithtwodistinct
stages.
Thefirststageusesthegradientinformationtomovethesamplerawayfromtheinitializedpointasquicklyaspossible. We
usetheparameterizedproposalfromEquation(5)withstepsizeα ,β withoutanyMetropolis-Hastingscorrectionas
ceil max
thisenablesverylargemovementsfromtheinitialsample.
For some datasets, this enables a very quick burn-in. This can be noticed in Figure 5 for Static/Dynamic MNIST and
Omniglot. Wehypothesizethatthisisduetothedistributionhavingarelativelysimplestructurethatenablesthegradient
toprovidemeaningfulinformationforverylargesamplingsteps. Itisimpossibletodetermineaprioriwhetheragiven
distributionwillhavethisproperty,soweincludeafollowingstagethatusesaMetropolis-Hastingscorrectiontoincrease
thechanceofarrivingatareasonablesampleθ.
For this stage, we construct a naive step size schedule and balancing constant schedule using the values of
α ,α ,β ,β . WethenruntheparameterizedsamplerfromEquation(5)withtheMetropolis-Hastingscorrection.
ceil floor max min
Ourgoalistomovethesamplertosamplesθthataremorelikelyinthetargetdistribution. Thiswillenabletheacceptance
ratescomputedduringthetuningalgorithmtobeclosertotheacceptanceratesforthesteady-statechain.
Forallthesamplingexperiments,thesetwostagescombineduse100samplingsteps.
EstimateAlpha Here we discuss the algorithm used to calculate both α ,α as defined in Equation (11). When
max min
calculatingα ,thegoalistopickthelargeststepsizeα thatacheivestheacceptancerateρ∗foragivenβ . When
max max max
calculatingα ,thegoalistodeterminethesmalleststep-sizecapableofacheivingthetargetacceptancerate. Weputthe
min
fullpseudo-codeinAlgorithm4.
Forcalculatingα andα ,thealgorithmfollowsthegeneralpatternofautomaticallyshiftingtherangeofpotential
max min
α based on the best values calculated from the previous iteration. When calculating α , the algorithm starts with an
max
upper-boundinitializedtoα =α anditerativelydecreasestherangeofproposedα. Forα ,thealgorithmstarts
bound ceil min
with a lower bound α = α and iteratively increases the range. For both, the other bound is calculated by the
bound floor
followinglearningrule:
α =α ±ζ|ρ−ρ∗|.
prop bound
Here,ζ isthelearningratethatdetermineshowmuchwecanadjustthestepsizeinonetuningstep. Wefoundζ insensitive
andsetζ =.5inalltasks. Additionally,ρisthebestacceptanceratecomputedfromthepreviousiterationofthealgorithm.
Forthefirststepofthealgorithm,wesetρ=0.
Thealgorithmusesα ,α todeterminetherangeofαtotest. Forcalculatingα ,thealgorithmsearchesinthe
prop bound max
rangeof[α ,α ]. Forcalculatingα ,therangeis[α ,α ].
prop bound min bound prop
Giventheappropriaterangeofαandaninitialθ,wetesttpotentialαandcalculatetheirrespectiveacceptanceratesusing
Equation(7). Oncewehavecomputedalltheacceptancerates,wesetα tothevaluethatresultedinthemostoptimal
bound
acceptancerateasdeterminedbyEquation(10),θtothecorrespondingθ′,andρtothecorrespondingacceptancerate.
Choiceofβ ,β ,ρ∗,s Theautomatictuningalgorithmdependsonaninitialchoiceofβ ,,β ,ρ∗,sthatenableit
max min max min
toautomaticallyconfigureaneffectivehyper-parameterschedule. Herewedescribethegeneralapproachtopickingthese
values.
Forsometargetdistributions,itispossiblethatthebestpossibleacceptanceratewithaveryhighβ ,suchasβ =.95,
max max
willnotbeclosetothetargetacceptancerateρ∗. Inthiscase,theEstimateAlphaMaxalgorithmwillkeepondecreasing
theproposedα ,whichwillresultinaverysmallα . Inordertoavoidthisbehavior,werecommendstartingwith
max max
β =.95,anddecreasingitby.05iftheresultingα isreasonable.
max max
Wealwayssetβ =0.5whichisthesmallestvalueβ cantake.
min
11Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
Wedeterminethetargetρ∗bystartingwithavalueof.5andincreasingitby.05untildesirableperformancemetricsare
obtained. Whilethisprocessisessentiallythesameasagridsearch,wenotethatweonlyneededtoapplythisprocess
inthespecificcaseoftrainingadeepEBMontheCaltechSilhouettesdataset. Forallothertasksanddatasets,thetarget
acceptancerateofρ∗ =.5waseffective. WediscusstheuniquedifficultypresentedwithintheCaltechSilhouettesdatasetin
D.5.
Todeterminethestepspercycles,werequiredasimilarapproachtodeterminetheoptimalvalue. Inourexperiments,we
onlylookattwodifferentvalues: eithers=8,ors=20. Havingalongercyclelengthtendstoenablemoreexploitationof
thetargetdistribution,whereashavingashortercycleenablesmoreexploration. Whilewedonothaveanalgorithmfor
automaticallyconfiguringthisvalue,wewereabletoachievegoodresultsacrossalltasksanddatasetsbychoosingeitherof
thesetwovalues.
Formoredetailsontheresultinghyper-parametersusedforeachexperiment,seeAppendixD.
Algorithm3InitBurnin
Require: α ,α , β ,β , steps per cycle s, steps to take without MH correction l = 50, steps to take with MH
ceil floor max min
correctionl =50,initialstateθ
MH
1: forstepiinrange(l)do
2: θ ∼Q αceil,βmax(·|θ)▷RunburninstepswithoutMHcorrection
3: endfor
4: {α 0,α 1···α s−1}←valuesfromEquation(8)usingα ceil,α floor.
5: {β 0,β 1···β s−1}←valuesfromEquation(8)usingβ max,β min▷WecanuseEquation(8)togetinterpolationsofβ
6: numberofcyclesn=floor(lMH)
s
7: ObtainθbyrunningAlgorithm1usingthecalculatedα,β schedule▷RunburninstepswithMHcorrection
8: returnθ
Algorithm4EstimateAlpha
Require: α ,BUDGET,initialstateθ,Balancingparameterβ,targetacceptancerateρ∗,learningrateζ,numberof
bound
proposalsperstept=5,flagMAX
1: ρ ←0
cur
2: whileiterationi≤BUDGETdo
3: ifMAXthen
4: α =α(1−ζ|ρ∗−ρ |)▷adaptivelydecreasetherangeofpotentialα
prop cur
5: proposed-params←LinSpace(α prop,α bound,t)▷weuseα bound =α ceilastheceilingforproposedα
6: else
7: α =α(1+ζ|ρ∗−ρ |)▷ForAlphaMin,adaptivelyincreasetherangeofpotentialα
prop cur
8: proposed-params←LinSpace(α bound,α prop,t)▷ForAlphaMin,useα bound =α floorasthefloorforproposedα
9: endif
10: initializebookkeepingtokeeptrackofproposedstatesandacceptancerates
11: forα∈proposed-paramsdo
12: θ′ ∼Q αprop,β(·|θ)▷Useproposedαtotakesamplingstep
13: ρ=A(θ′|θ,α ,β)▷Computeacceptancerateforproposedα
prop
14: i=i+1
15: endfor
16: Setρ totheacceptancerateclosesttothetargeta∗
cur
17: Setα boundtothecorrespondingα▷Updateα boundtoshifttherangeofproposedαforthenextstep
18: setθ tothecorrespondingθ
cur
19: endwhile
20: ifMAXthen
21: returnα =α
max bound
22: else
23: returnα =α
min bound
24: endif
12Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
Algorithm5EstimateBalSched
Require: Stepsizeschedule{α ,α ,...α },β ,β ,numberofproposalsperstept = 10,initialstateθ,target
max 1 min max min
acceptancerateρ∗
1: β =β ,β =β
floor min ceil max
2: β-sched←{β }
max
3: foriin{1,2,...s−1}do
4: proposed-params←LinSpace(β ,β ,t)▷Createtpotentialbalanceparametersforindexiintheschedule
floor max
5: initializebookkeepingtokeeptrackofproposedstatesandacceptancerates
6: forβ ∈proposed-paramsdo
7: θ′ ∼Q αi,β(·|θ)▷Usecurrentproposedβ totakeasamplingstep
8: ρ=A(θ′|θ,α i,β)▷Evaluatetheacceptancerateofproposedβ forcurrentα i
9: bookkeeping[β]←θ′,ρ
10: endfor
11: pickβ iasβ ∈bookkeepinglargestρ
12: β ceil ←β i▷Shrinktherangeofpotentialbalancingparametersbyusingassumptionβ i >β i+1
13: θ =θ′correspendingtoβ i
14: endfor
15: β-sched.append(β )
min
16: returnβ-sched
B.ACSforEBMLearning
B.1.Background
EnergyBasedModels(EBMs)areaclassofgenerativemodelsthatlearnsomeunnormalizeddistributionoverasample
space. Asdiscussedin(Hinton,2002),thesemodelscanbetrainedviathefollowingMaximumLikelihoodobjective:
L(ϕ)=E [−logp (x)] (13)
x∼pdata ϕ
Thegradientupdatesforthislossfunctionareknowntobeasfollows:
∇ L(ϕ)=E [∇ E (x)]−E [∇ E (x)] (14)
ϕ x∼pdata ϕ ϕ x∼pϕ ϕ ϕ
Whiletheexpectationontheleftisstraightforwardtocalculategivenadataset,calculatingtherightexpectationisnotas
clear. HerewewillmentionthetwomethodsthatarerelevanttowardsourexperimentswithEBMs.
ContrastiveDivergence(CD) Inordertoestimatethesecondterm,weinitializesomesamplerusingthexinthefirst
termandrunitforasetnumberofsamplingsteps. Foramoredetaileddescription,refertoHinton(2002).
PersistentContrastiveDivergence(PCD) Theexpectationontherightcanbecalculatedusingsamplesfromapersistent
MarkovChainthatapproximatesthetruedistributionTieleman(2008). Insteadofresettingthechaineachtrainingiteration,
wemaintainabufferofthegeneratedsamplesthatweusetocalculatethesecondexpectation. Thismethodreliesonthe
intuitionthatthemodeldistributiondoesnotvarytoowidelywithinoneiteration. Usingtheintuitionprovidedby(Du&
Mordatch,2019),wecanviewthisprocessasupdatingthemodelparametersϕtoputmoreweightontruesamplesandless
weightonfakesamples. Bydoingso,themodelwillinturngeneratesamplesthatclosertothosefromthetruedistribution.
B.2.PersistentContrastiveDivergencewithACS
MainIdea WecanapplytheACSalgorithmcombiningtheautomatictuningofthecyclicalschedulewiththeoriginal
PCD learning algorithm. Our goal in doing so is to improve PCD through better characterization of the entire model
distribution. Duringtraining,wecanviewPCDasadjustingthemodelparametersto“pushdown”theprobabilityofsamples
fromthemodeldistributionwhile“pushingup”samplesfromthetruedatadistribution. Becauseoursamplingmethod
isabletoexplorethemodel’sdistributionmoreeffectivelythanothersamplers,wecanadjustmoreregionsofthemodel
distributionataquickerratethanprevioussamplingmethods,whichshouldimprovethequalityofgradientupdatesand
thusleadtobettermodelparameters. WeadaptACStoworkwithinPCDbyhavingthestepsizedependonthetraining
13Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
iterationasopposedtothesamplingiteration,withthecorrespondingα,β pairbeingusedforallthesamplingstepswithin
theiteration. WeincludethecompletelearningalgorithminAlgorithm6.
Cyclical Scheduling We find that the learning task requires a different approach to the cyclical scheduling than the
samplingtask. Ratherthanhavingarelativeequalamountofexplorationandexploitation,wefindthatitismoreeffectiveto
useacyclicalschedulebiasedtowardsexploitation. However,explorationisstillimportantasitenablesthemodeltobetter
representthedistributionasawholeratherthanafewlocalmodes. Giventhis,weconstructacyclicalscheduleconsisting
ofoneiterationthatusesα ,β withtherestusingα ,β .
max max min min
Tuning Oneoftheadvantagesofusingthesimplifiedcyclicalscheduleisthatitonlyrequirestwopairsofhyper-parameters
tobeoptimized. ThuswecanleveragetheEstimateAlphaMaxandEstimateAlphaMinalgorithmtobothtunetherespective
α,βpairwhilealsoupdatingthepersistentbuffer.Notonlydoesthisreducetheadditionaloverheadofthetuningcomponent,
butitallowsthehyper-parameterstoadapttothechangingEBMdistribution.
14Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
Algorithm6ACSforPersistentContrastiveDivergence
Require: NumberIterationsN,EBME ,data-loaderD,samplerQ,smallsamplingstepsS ,bigsamplingstepsS ,
ϕ small big
initialbufferX ,cyclelengths,α ,α ,adaptivelearningrateζ,adaptivebudgetBUDGET
f floor ceil
1: whileiterationi≤N do
2: forX t ∼Ddo
3: cyclenumberc=floor(i)
C
4: ifc mod K =0then
5: ifi mod s=0then
6: X f,α max ←EstAlphaMax(α ceil,budget=BUDGET,learning-rate=γ)
7: else
8: X f,α min ←EstAlphaMin(α floor,budget=BUDGET,learning-rate=γ)
9: endif
10: UpdateSamplerStepSchedule▷UpdatethebufferbyrunningeithertheAlphaMaxorAlphaMinestimation
algorithm
11: else
12: ifi mod s=0then
13: S =S
big
14: α=α ,β =β ▷Usetheα,β pairthatbestenablesexploration
max max
15: else
16: S =S
small
17: α=α ,β =β ▷Usetheα,β pairthatbestenablesexploitation
min min
18: endif
19: ConstructQ=Q α,β(·|X f)using(5)
20: forsamplingstepinrange(S )do
big
21: X ∼Q(·|X f)
22: ifi mod s=0then
23: X f ←X
24: continue▷Ifiisthefirststepofthecycle,omittheMHcorrection
25: endif
26: X f ←X withacceptanceprobabilityascalculatedin(7)
27: endfor
28: endif
29: CalculateE x∼pϕ[∇ ϕE ϕ(x)]usingX f
30: CalculateE x∼pdata[∇ ϕE ϕ(x)]usingX t
31: ∇L(ϕ)=E x∼pϕ[∇ ϕE ϕ(x)]−E x∼pdata[∇ ϕE ϕ(x)]▷EstimatethegradientoftheMaximum-Likelihoodobjective
asin(13)
32: ϕ=ϕ−γ ϕ∇L(ϕ)
33: i+=1
34: endfor
35: endwhile
15Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
C.TheoreticalResults
Wedefinetheproblemsettinginmoredetail. Wehaveatargetthatisoftheform
1
π(θ)= exp(U(θ)).
Z
Weconsidertheproposalkernelas
(cid:26) (cid:27)
1
Q (θ′|θ)∝exp β∇U(θ)T (θ′−θ)− ∥θ′−θ∥2
α,β 2α
andconsiderthetransitionkernelas
(cid:18) π(θ′)Q (θ |θ′) (cid:19)
p(θ′ |θ)= α,β ∧1 Q (θ′ |θ)+(1−L(θ))δ (θ′)
π(θ)Q (θ′ |θ) α,β θ
α,β
whereδ (θ′)istheKroneckerdeltafunctionandL(θ)isthetotalacceptanceprobabilityfromthepointθwith
θ
L(θ)=
(cid:88) (cid:18) π(θ′)Q α,β(θ|θ′) ∧1(cid:19)
Q (θ′|θ).
π(θ)Q (θ′|θ) α,β
α,β
θ′∈Θ
Wealsodefine
(cid:26) (cid:27)
(cid:88) 1
Z (θ)= exp β∇U(θ)T (x−θ)− ∥x−θ∥2
α,β 2α
x∈Θ
whichisthenormalizingconstantfortheproposalkernel.
C.1.ProofofLemma5.3
Proof. Byincludingthebalancingparameter,westartbynotingthat
exp(cid:8) β∇U(θ)T (θ′−θ)− 1 ∥θ′−θ∥2(cid:9)
Q (θ′|θ)= 2α (15)
α,β (cid:80) exp(cid:8) β∇U(θ)T (θ−θ)− 1 ∥θ−θ∥2(cid:9)
θ∈Θ 2α
Considertheterm,
β (cid:90) 1
β ∇U(θ)T (θ′−θ)=β (−U(θ)+U(θ′))− (θ−θ′)T( ∇2U((1−s)θ+sθ′)ds)(θ−θ′) (16)
2
0
Substituting(16)in(15),thenumeratorofQ (θ,θ′)
α,β
1
β∇U(θ)T (θ′−θ)− ∥θ′−θ∥2 =β (−U(θ)+U(θ′))
2α
β (cid:18)(cid:90) 1 (cid:19)
− (θ−θ′)T ∇2U((1−s)θ+sθ′)ds (θ−θ′)
2
0
1
− (θ−θ′)TI(θ−θ′)
2α
=β (−U(θ)+U(θ′))
1 (cid:18) (cid:90) 1 1 (cid:19)
− (θ−θ′)T β ∇2U((1−s)θ+sθ′)ds+ I (θ−θ′)
2 α
0
.
16Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
FromAssumption5.1(UisM-gradientLipschitz),wehave
(cid:90) 1 1 (cid:18) 1 (cid:19)
β ∇2U((1−s)θ+sθ′)ds)(θ−θ′)+ I ≥ −βM I
α α
0
Sinceα<1/βM,thematrix(cid:0) 1 −βM(cid:1) I ispositivedefinite. Wenotethat
2α
(cid:18) π(θ′)Q (θ|θ′) (cid:19)
p(θ′|θ)= α,β ∧1 Q (θ′|θ)+(1−L(θ))δ (θ′) (17)
π(θ)Q (θ′|θ) α,β θ
α,β
(cid:18) π(θ′)Q (θ|θ′) (cid:19)
≥ α,β ∧1 Q (θ′|θ) (18)
π(θ)Q (θ′|θ) α,β
α,β
(cid:18) (cid:19)
Z (θ)
= α,β ∧1 Q (θ′|θ). (19)
Z (θ′) α,β
α,β
(cid:26) (cid:27)
(cid:88) 1
Z (θ)= exp β∇U(θ)T (x−θ)− ∥x−θ∥2
α,β 2α
x∈Θ
(cid:88) (cid:26) 1 (cid:90) 1 1 (cid:27)
= exp −β (U(θ)−U(x))− (θ−x)T(β ∇2U((1−s)θ+sx)ds)(θ−x)+ I)(θ−x) .
2 α
x∈Θ 0
Thiscanbeseenas
1 (cid:26) (cid:18) 1 β (cid:90) 1 (cid:19) (cid:27)
π(θ)Q (θ′|θ)= exp β(U(θ)+U(θ′))−(θ′−θ)T I+ ∇2U((1−s)θ+sθ′)ds (θ′−θ) .
α,β ZZ (θ) 2α 2
α,β 0
SinceAssumption5.2holdstrueinthissetting,wehaveanm>0suchthatforanyθ ∈conv(Θ)
−∇2U(θ)≥mI.
Fromthis,onenotesthat
(cid:18) (cid:18) (cid:19) (cid:19)
1 1 (cid:88) (cid:88)
exp −βU(θ)− −β m diam(Θ)2 exp(βU(x))≤Z (θ)≤exp(−βU(θ)) exp(βU(x))
2 α α,β
x∈Θ x∈Θ
wheretheright-handsidefollowsfromthefactthatα<1/(βM). Therefore,
Z (θ) exp{β(−U(θ)+U(θ′))}
α,β ≥
Z (θ′)
exp(cid:8)1(cid:0)1 −βm(cid:1) diam(Θ)2(cid:9)
α,β 2 α
Alsonotethat
(cid:110) (cid:16) (cid:17) (cid:111)
exp β(−U(θ)+U(θ′))−(θ−θ′)T 1 I+ β (cid:82)1 ∇2U((1−s)θ+sθ′) (θ−θ′)
Q (θ′|θ)= 2α 2 0
α,β (cid:80) exp(cid:110) β(−U(θ)+U(θ′))−(θ−θ′)T (cid:16) 1 I+ β (cid:82)1 ∇2U((1−s)θ+sθ′)(cid:17) (θ−θ′)(cid:111)
θ′∈Θ 2α 2 0
exp(cid:8) β⟨∇U(θ),θ′−θ⟩− 1 ∥θ−θ′∥2(cid:9)
≥ 2α .
(cid:80) exp{β(−U(θ)+U(θ′))}
θ′Θ
17Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
Wealsonotethat
1 1
−β⟨∇U(θ),θ′−θ⟩+ ∥θ−θ′∥2 =β⟨−∇U(θ)+∇U(a),θ′−θ⟩+β⟨−∇U(a),θ′−θ⟩+ ∥θ−θ′∥2
2α 2α
1
≤β⟨−∇U(θ)+∇U(a),θ′−θ⟩+β⟨−∇U(a),θ′−θ⟩+ diam(Θ)2
2α
1
≤β∥−∇U(θ)+∇U(a)∥∥θ′−θ∥+β∥∇U(a)∥∥θ′−θ∥+ diam(Θ)2
2α
1
≤β∥−∇U(θ)+∇U(a)∥diam(Θ)+β∥∇U(a)∥diam(Θ)+ diam(Θ)2
2α
(cid:18) (cid:19)
1
≤ βM + diam(Θ)2+β∥∇U(a)∥diam(Θ).
2α
Combining,weget
exp{βU(θ′)}
p(θ′|θ)≥ϵ
β,α (cid:80) exp{βU(θ′)}
θ′Θ
where
(cid:26) (cid:18) (cid:19) (cid:27)
1 βm
ϵ =exp − +βM − diam(Θ)2−∥∇U(a)∥diam(Θ) .
β,α α 2
C.2.ProofsofPropositionC.1andCorollaryC.2
PropositionC.1. LetP ,P ,···P beMarkovtransitionoperatorswithkernelsp ,p ,···p withrespecttoareference
1 2 s 1 2 s
measureη. Also,letp (θ′|θ)≥ϵ ν (θ′)forsomedensityν onΘandϵ >0withrespecttoareferencemeasureη. Then,
i i i i i
fortheMarkovoperatorPˆ definedwithrespecttothekernelas
i
(cid:90)
pˆ(θ′|θ)= p (θ |θ)p (θ |θ )···p (θ |θ )
i i+1 1 i+2 2 1 s s−i+1 s−i
ΘS−1
···p (θ′|θ )dη(θ )dη(θ )···dη(θ ),
i s−1 1 2 s−1
wehave
pˆ(θ′|θ)≥ϵ ν (θ′),∀θ ∈Θ·
i i i
Proof. Theproofisstraightforwardbyusingtheminorizationofp . Indeed,onehas
i
(cid:90)
pˆ(θ′|θ)= p (θ |θ)p (θ |θ )···p (θ |θ )···p (θ′|θ )dη(θ )dη(θ )···dη(θ )
i+1 1 i+2 2 1 s s−i+1 s−i i s−1 1 2 s−1
ΘS−1
(cid:90)
≥ϵ ν (θ′) p (θ |θ)p (θ |θ )···p (θ |θ )···p (θ |θ )dη(θ )···dη(θ )
i i i+1 1 i+2 2 1 s s−i+1 s−i i−1 s−1 s−2 1 s−1
ΘS−1
≥ϵ ν (θ′)
i i
whichestablishestheresult.
NotethatinAlgorithm1,foreachcycle,wegothroughsstepscorrespondingtothestepsizeandbalancingparameter
schedules({α ,α ,···α })and({β ,β ,···β }). LetP ,P ,··· ,P betheMarkovoperatorscorrespondingtothem.
1 2 s 1 2 s 1 2 s
CorollaryC.2. LetAssumptions5.1and5.2hold. Then
P P P ···P (θ,A)≥ϵ ν (A)
1 2 3 s s s
foranymeasurablesubsetAofΘ.
Proof. TheproofisimmediatefromPropositionC.1.
18Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
C.3.AdditionalLemma
LemmaC.3. LetAssumption5.2holdwithΘcompact. Then,thereexistssomem > 0suchthatforanyθ ∈ conv(Θ),
λ (∇2−U(θ))>m.
min
Proof. NotethatsinceΘiscompactconv(Θ)isalsocompact. Thisiseasytoseeasweonlyneedtoestablishthatconv(Θ)
isclosedandboundedbytheHeine-BorelTheorem. Takeanyelementinθ ∈conv(Θ). Bydefinition,θ =αθ +(1−α)θ
1 2
forsomeθ ,θ ∈Θand0≤α≤1. SinceΘiscompact,weknowthatthereexistsM >0suchthat∥θ ∥<M fori=1,2.
1 2 i
Therefore∥θ∥ < M bytriangleinequality. Thusthesetisbounded. Thefactthatitisclosedisalsoeasytosee. Take
anysequencex inconv(Θ). Thisimpliesthereexistsα ,θ ,θ suchthatx = α θ +(1−α )θ . Sincex
n n 1,n 2,n n n 1,n n 2,n n
convergesasourassumption,itisCauchywhichinturnimplieseachofα ,θ ,θ isCauchyasΘisbounded. Thusthe
n 1,n 2,n
proofimmediatelyfollows. Now,considereachθ ∈conv(Θ). ThereexitsaB(θ,r )suchthat∇2−U(θ′)≥m I forall
θ θ
θ′ ∈B(θ,r ). Sinceconv(Θ)⊂∪ B(θ,r ),thisisanopencoverofconv(Θ). Sinceconv(Θ)iscompact,thereexists
θ θ∈Θ θ
θ ,θ ,··· ,θ suchthatconv(Θ) ⊂ ∪k B(θ ,r ). Thusforeachiwehave∇2−U(θ′) ≥ m I whenθ ∈ B(θ ,r ).
1 2 k i=1 i θi θi i θi
Thus∇2−U(θ)≥min m I forallθ ∈conv(Θ). Hencewearedone.
1≤i≤k θi
D.AdditionalExperimentalResultsandDetails
Here,weincludethefulldetailsforalltheexperimentsweincludeinthispaper,aswellassomeadditionalresults.
D.1.Multi-modalExperimentDesign
SyntheticDistribution Inordertoconstructadistributionthatiseasytovisualize,wefirstmustdefineafewexperiment
parameters. Wemustdefinethespacebetweenthemodes,thetotalnumberofmodes,andthevarianceofeachmodeσ. For
convenience,wehavethenumberofmodesas25,whichisaperfectsquare. Wedefinethespacebetweenmodesas75,and
thevarianceforeachmodeσ2as.15. Giventhis,wecancalculatethemaximumvalueforeachcoordinateasfollows:
√
MaxVal=( NumModes+1)∗SpaceBetweenModes
Wecancalculatethecoordinatevalueforeachmodeµ asfollows:
i,j
MaxVal
µ [0]= √ (i+1)
i,j
NumModes+2
MaxVal
µ [1]= √ (j+1)
i,j
NumModes+2
SamplerConfiguration Ourgoalinthisexperimentistodemonstratehowgradient-basedsamplerstypicallybehave
whenfacedwithadistributionwithmodesthatarefarapart. Inorderforthisexperimenttobemeaningful,itisimportant
thattherepresentationofeachsamplerespectthenotionofdistancebetweentheintegervalues. Forthisreason,wecannot
useacategoricaldistributionorrepresenteachcoordinatewithaone-hotencoding,aseverysampleinthisrepresentation
wouldbewithina2-hammingballofeveryotherpoint.
Inordertodeterminethestepsizesforthebaselines, wetuneeachuntilwereachanacceptanceratearound.574. For
DMALA,thisendsupbeingaroundα=53. Fortheany-scalesampler,wesettheinitialstepsizetobethesameanduse
theirimplementedadaptivealgorithm.
Forthecyclicalsampler,wesetα =1575,α =3,andstepspercycles=20. Becausethegoaloftheexperimentis
max min
todemonstratetheneedforlargerstepsizesalongwithsmallerstepsizes,wedonotusetheautomatictuningalgorithmon
thisexampleasrestrictingthespacetobeordinalchangestheoptimalsettingforα . Inmostpracticalcases,thesamples
ceil
wouldberepresentedbyacategoricalorbinaryform,whichtheproposedtuningalgorithmisabletohandleasdemonstrated
bytheperformanceonrealdatadistributions.
D.2.RBMSampling
RBMOverview WewillgiveabriefoverviewoftheBlock-GibbssamplerusedtorepresentthegroundtruthoftheRBM
distribution. Foramorein-depthexplanation,seeGrathwohletal.(2021). Giventhehiddenunitshandthesamplex,we
19Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
definetheRBMdistributionasfollows:
logp(x,h)=hTWx+bTx+cT −logZ (20)
Asbefore,Zisthenormalizingconstantforthedistribution. Thesamplexisrepresentedbythevisiblelayerwithunits
corresponding to the sample space dimension and h represents the model capacity. It can be shown that the marginal
distributionsareasfollows:
p(x|h)=Bernoulli(Wx+c)
p(h|x)=Bernoulli(Wth+b)
TheBlock-Gibbssamplerupdatesxandhalternatively,allowingformanyofthecoordinatestogetchangedatthesame
time,duetoutilizingthespecificstructureoftheRBMmodel.
ExperimentSetup SimilartotheexperimentalsetupofZhanget al.(2022a), we useRBMmodelswith500hidden
unitsand784visibleunits. Weadoptthesametrainingprotocol,exceptwetraintheRBMwith100stepsofContrastive
Divergenceasopposedto10. Wealsotrainthemodelsfor1000iterationsasopposedtoasinglepassthroughthedataset.
WefindthatthisenablestheRBMstogeneratemorerealisticsamples. WeincludethegeneratedimagesinFigure7to
demonstratethatthesemodelshavelearnedthedatasetreasonablywell.
(a)MNIST (b)eMNIST (c)kMNIST (d)Omniglot (e)Caltech
Figure7. ImagesgeneratedfromRBMstrainedbyContrastive-DivergencewithBlockGibbs.
EscapefromLocalModes InadditiontousingthesameinitializationasZhangetal.(2022a);Grathwohletal.(2021),
weextendtheexperimenttomeasuretheabilityofasamplertoescapefromlocalmodes. Weinitializethesamplerwithin
themostlikelymode,asmeasuredbyunnormalizedenergyoftheRBM.Samplersthatarelesspronetogettingtrappedin
localmodeswillbeabletoconvergequickertothegroundtruth,asmeasuredbylogMMD.
SamplerConfiguration ForGWG,weusethesamesettingsasGrathwohletal.(2021),forDMALA,wesetstepsizeto
.2,andforABweusethedefaulthyper-parametersforthefirstordersampler.
ForACS,weuseρ∗ =.5,β =.95,ζ =.5,cyclelengths=20forallthedatasets. Wealsofixthetotaloverheadofthe
max
tuningalgorithmto10%ofthetotalsamplingsteps.
GeneratedImages WefoundthatavisualinspectionofthegeneratedimagesdemonstratestheabilityofACStoescape
localmodes. WeincludethegeneratedimagesinFigure8.
Wecanmaketwoprimaryinferencesfromthegeneratedimages: thefirstbeingthatACSisabletoescapefromlocalmodes
andexplorethedistributionasawhole,asdemonstratedbythewiderangeofgeneratedimages;andthatACSdoesnot
compromiseontheabilitytocharacterizeeachmodeasevidencedbythequalityofgeneratedsamples.
SamplingSpeed Whiletheruntimecanvarydependingonthespecificimplementationofagivensamplingalgorithm,
weillustratetheefficiencyofACSinFigure9. ACSisabletocaptureboththeefficiencyofDMALAwhiledisplayingthe
accuracyoftheABsampler,seeminglycapturingthebestofbothworlds.
20Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
(a)GWG (b)AB (c)DMALA (d)ACS
Figure8.ImagessampledfromRBMtrainedonMNISTwhenthesamplerisinitializedtomostlikelymode.ACSisabletogeneratea
diverserangeofdigits,demonstratingitsabilitytoescapefrommodes.ItshouldalsonotedthatwhileABisabletogenerateadiverse
rangeofdigitsaswell,theimagesareslightlylessclearthanthosegeneratedbyACS.
mnist kmnist emnist omniglot caltech
4.0
3.0 4.0 5.6 4.0 G DMW AG LA
4.5 3.5 4.5 5.8 4.5 A ACB S (Ours)
5.0 6.0
5.5 4.0 5.0 6.2 5.0
4.5 5.5 6.4 5.5
6.0
5.0 6.0 6.6
6.5 6.8 6.0
5.5 6.5
0 20 40 60 0 20 40 0 20 40 60 0 20 40 6.50 20 40
Time (s)
mnist emnist
3.0
3.5 3.0
4.0 3.5
4.5 4.0
5.0
4.5
5.5 GWG
6.0
5.0 D ABMALA
ACS (Ours)
6.5 0 20 40 5.5 0 20 40
Time (s)
Figure9.RBMsamplingwith5,000samplingstepswithresultsmeasuredagainsttimeinseconds. Thetoprowdisplaysresultsfor
randominitialization,thebottomrowdisplaysresultsformodeinitialization.ACSiscompetitivebothintermsofaccuracyandefficiency.
D.3.EBMSampling
BaseEBMTraining InordertotraintheEBMs,weuseGibbs-with-GradienttosampletheEBMdistributionduringPCD,
followingthesametrainingprotocolasGrathwohletal.(2021). Wetrainthesemodelsfor50,000iterationstotalwith40
samplingstepsperiterationandusetheparameterscorrespondingtothebestloglikelihoodscoresonthevalidationdataset.
Experimental Design For each of the trained models, we evaluate the samplers based on how quickly the average
energyofthegeneratedsamplesrises. Thisgivesanestimateofthespeedatwhichasamplerisabletoreachastationary
distribution.
SamplerConfiguration ForGWG,weusethesamesettingsasGrathwohletal.(2021),forDMALA,wesetstepsizeto
.2,andforABweusethedefaulthyper-parametersforthediagonalvariantoftheABsampler. Wechoosethisvariantas
thisiswhattheyevaluatefortheirexperimentswhenmeasuringmixingspeedofsamplersonEBMs.
ForACS,weuseρ∗ =.5,β =.8,ζ =.5,cyclelengths=20forallthedatasets. AsinRBMSampling,wefixthetotal
max
overheadofthetuningalgorithmto10%ofthetotalsamplingsteps.
Sampler Performance It is worth commenting on the similarity in performance between ACS and DMALA when
samplingfromCaltech. WefindthatwhensamplingfromtheEBMtrainedontheCaltechdataset,ACSfindsaα similar
max
toα ,thusmakingtheACSsamplersimilartoDMALAforthisspecificcase. Wehypothesizethatsmallstepsizesare
min
mosteffectiveforthisdataset. TheresultsinFigure5demonstratethatACScanhandlesuchcasesautomatically: whilethe
stepsizeforDMALAmustbehand-tuned,theACSmethodcanautomaticallyadapttoasuitablestepsizeschedule.
21
DMM
goL
DMM
goLGradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
GeneratedImages WeincludetheimagesgeneratedbyACSwhensamplingfromdeepEBMsinFigure10.
(a)Static (b)Dynamic (c)Omniglot (d)Caltech
Figure10.GeneratedImagesfromsamplingdeepEBMstrainedwithGWG.Thesesamplescapturemultipledifferentmodeswhile
retaininggoodsamplequality,demonstratingthebenefitofourACSmethod.
D.4.RBMLearning
ExperimentDesign WeusethesameRBMstructureasthesamplingtask,with500hiddenunitsand784visibleunits.
However,weapplythesamplersofinteresttothePCDalgorithmintroducedbyTieleman(2008). Themodelparametersare
tunedviatheAdamoptimizerwithalearningrateof.001.
InordertoevaluatethelearnedRBMs,werunAISwithBlock-Gibbsasthesamplertocalculatetheloglikelihoodvalues
forthemodelsNeal(2001). WerunAISfor100,000steps,whichisadequategiventheefficiencyofBlockGibbsforthis
specificmodel.
SamplerConfiguration ForDMALA,weuseastepsizeof.2. FortheACSalgorithm,wesetβ =.9,ρ∗ =.5for
max
allthedata-sets. Wedomodifythenumberofcyclesforeachdata-setasdifferentdistributionsrequiredifferentamounts
ofexplorationandexploitationdepending. Weusecyclelengthof8forMNIST,eMNIST,andkMNIST;weuse20for
OmniglotandCaltechsilhouettes. Thisdifferencereflectsthespecificneedsforeachdatasetintermsofexplorationand
exploitation–morecomplexdatasetstendtoneedlongercyclesinordertobetterexploiteachregion,whilesimplerdatasets
tendtoneedshortercyclesinordertocaptureallthemodesofthelearneddistribution. InFigure11,weshowthesamples
generatedfromAISfor100,000stepsasopposedtothepersistentbufferasthisformsalongerMCMCchain,thusgivinga
bettervisualofwhatthelearneddistributionrepresents.
Inordertoensurethattheoverheadforthetuningalgorithmdoesnotaddtotheoverallcomputationalcost,wespreadout
thecomputationsoftheEstimateAlphaMinalgorithmthroughoutthetrainingprocess. Wekeeparunninglistofα and
min
setα tobeonestandarddeviationbelowthemeanofthislist. Bydoingthis,westartclosertowhattheidealα . For
floor min
EstimateAlphaMax,wesimplycallthetuningfunctionevery50cyclescontaining50trainingiterations,withα =5. As
ceil
theinitialstepdoesnotusetheMetropolis-Hastingscorrectionandhashalfthesamplingsteps,thebudgetforeachcallof
EstimateAlphaMaxcanbeseenascominginpartbythecomputationsaved.
GeneratedImages WeincludethegeneratedimagesfromtheRBMstrainedusingdifferentsamplersinFigure11.
Ingeneral,theimagesgeneratedfromtheACS-trainedRBMcapturemoremodesthanothermethods,exceptfortheCaltech
Silhouettesdataset. Inparticular,allthemethodsstruggletogeneratereasonableimagesforthisdataset. Wehypothesize
thatthisisduetotheincreasedcomplexityofthedistributionrelativetotheotherdatasets–CaltechSilhouettesiscomposed
ofthesilhouettesfromrealobjects,whereastheotherdatasetsarehand-writtensymbols. Thishypothesisissupportedby
thegeneratedimagesinFigure7,wheretheimagesgeneratedwhenusingBlock-GibbsonCaltechSilhouettesalsoseem
lessreasonablethanthesamplesobtainedfromdifferentdatasets. SinceBlock-Gibbsisthebestsamplerforthisspecific
modelasitleveragestheknownstructureoftheRBM,thisappearstobeunavoidableasaresultoflimitedmodelcapacity.
ThismotivatesourexperimentswithdeepconvolutionalEBMs,wherewecanunderstandhowourmethoddoeswhenusing
amodelarchitecturewithsufficientcapacity.
22Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
Figure11.GeneratedimagesfromRBMstrainedwithdifferentsamplers.FirstrowcorrespondstoGWG,secondrowcorrespondsto
DMALA,andfinalrowcorrespondstoACS.FirstcolumnrepresentsmodelstrainedonMNIST,secondoneMNIST,thirdonkMNIST,
fourthonOmniglot,andfifthonCaltechSilhouettes.ImagesaregeneratedviaAISfor100,000steps.
D.5.EBMLearning
ExperimentDesign WeusethesameEBMmodelarchitectureasZhangetal.(2022a);Grathwohletal.(2021)andfollow
thesameexperimentaldesign,withtheonlychangebeingtothenumberofsamplingstepsalottedforeachsampler.
InordertodeterminethenumberofsamplingstepsthatwecoulduseforACS-PCD,wetesteddifferentsamplingsteps. For
Static/DynamicMNISTandOmniglot,wefoundthatweonlyneededtouse10samplingstepstoachievegoodmodels.
However,weobserveddivergencewhentrainingmodelsonCaltech. Inordertodeterminewhatnumberofsamplingsteps
touseforACS,wedoagridsearchoverthenumberofsamplingstepsandρ∗ withallothervaluesremainingthesame.
Wetestsamplingstepsof10,20,and30;andweuseρ∗.5,.6,.7,.8. Wedecidewhichhyper-parameterstousebasedon
when training divergedthe latest; and we use the bestmodel parameters as indicatedby validation log likelihood. We
use10samplingstepsforStatic,DynamicMNIST,andOmniglot,whilewefound30wastheminimumwecouldusefor
CaltechSilhouettesandobtainreasonableresults. WeapplythisnumberofsamplingstepsforbothDMALAandACSto
demonstratehowthemethodscomparewhenfacingasimilarbudgetconstraint.
Inordertoevaluatetheselearnedmodels,weusethesameevaluationprotocolasZhangetal.(2022a);Grathwohletal.
(2021). WerunAISfor300,000iterationsusingGibbs-With-Gradientastheevaluationsampler. Byfollowingthesame
experimentaldesignaspreviousworks,wecandrawmeaningfulcomparisonsfrompreviousresultsinGrathwohletal.
(2021).
SamplerConfiguration ForDMALA,weuseastepsizeof.15asusedin(Zhangetal.,2022b). ForACS,weuse200
samplingstepsforEstimateAlphaMaxandEstimateAlphaMin. ForStaticMNIST,DynamicMNIST,andOmniglot,weset
thealgorithmtotuneα andα every25cycles,whereeachcyclehas50trainingiterations. Theadditionaloverhead
max min
ofthisis16,000extrasamplingsteps,whichisa3.2%ofthetotalbudgetof500,000samplingsteps. ForCaltechSilhouettes,
wehavetoadaptevery10cycleswiththesamenumberoftrainingiterations. Thisresultsin40,000additionalsampling
stepsduetothetuningalgorithm. Forthisspecificdataset,becauseweuse30samplingsteps,theadditionalcostis2.6%of
thetotalsamplingsteps1,500,000.
Intermsofthefinalparametersforcyclelengthandsamplingsteps,wefindthatwecanusethesameρ∗acrossalldatasets,
withtheexceptionofCaltechSilhouettes. ForStatic/DynamicMNISTandOmniglot,wewereabletouseρ∗ =.5andFor
thisdataset,wefoundgoodresultsbysettingρ∗ =.7. Wehypothesizethattheneedforahigheracceptancerateisdueto
thefundamentaldifferencebetweenCaltechSilhouettesandtheotherdatasets,aspreviouslymentioned. BecauseCaltech
23Gradient-basedDiscreteSamplingwithAutomaticCyclicalScheduling
Silhouettescontainsamplesarederivedfromrealobjects,theyaremorecomplexthanthehand-writtenfigures.
ExperimentalResults InadditiontotheempiricalresultsinTable2,weprovidesomequalitativedataintheformofthe
generatedimagesfromthePCDbufferwhenusingACS.Wechoosetoincludethebufferimagesforthisexperimentasthe
chainfromthepersistentbufferismuchlongerthanthechainfromAISduetotheincreasedtrainingduration: thechain
fromAISisobtainedusing300,000samplingstepswhereasthepersistentbufferisobtainedfrom500,000samplingsteps
onStatic/DynamicMNISTandOmniglot,1,500,000samplingstepsforCaltechSilhouettes. Byvisualizingthegenerated
imagesfromthelongerchain,wegetabetterunderstandingofthequalityofthetraineddistribution. Weputtheimagesin
Figure12.
WealsoobservethatthisbehaviorisnotuniquetoACSanddoesoccurwhenGibbs-With-GradientandDMALAareused
with40samplingstepsasindicated. InstabilityiscommonwhentrainingdeepEBMs, andthisismostlikelywhythe
originalexperimentaldesignincludedcheck-pointingthroughoutthetrainingprocessaswellascomparisonsbasedonthe
validationset. Wealsonotethatdespitethisbehavior,thetrainedmodelsareabletogeneratefairlyrealisticimages. We
presenttheimagesfromthePCDbufferforACSbelowinfigure.
(a)Static (b)Dynamic (c)Omniglot (d)Caltech
(e)Static (f)Dynamic (g)Omniglot (h)Caltech
Figure12.Theexampleimagesfromtherepresentativedatasets,alongwiththesamplesgeneratedfromthepersistentbufferwhenusing
ACSasthesamplerforPCD.Theimagesonthetoprowareexamplesfromthedataset,whilethebottomrowarefromthetrainedEBM.
TheimagesgeneratedfromACSareremarkablysimilartothosefromthedataset,demonstratingthatthemodeliscapableofgenerating
high-qualitysamples.
WhentheimagesinFigure12aretakenincontextoftheimprovementsinloglikelihoodsaspresentedin2,theresults
indicatethebenefitsofusingACSwhenlearningmulti-modaldiscretedistributions.
24