Towards Optimal Learning of Language Models
YuxianGu1,2‚àó, LiDong2, YaruHao2, QingxiuDong2, MinlieHuang1, FuruWei2
1TheCoAIGroup,TsinghuaUniversity
2MicrosoftResearch
https://aka.ms/GeneralAI
Abstract
Thisworkstudiesthegeneralprinciplesofimprovingthelearningoflanguage
models(LMs),whichaimsatreducingthenecessarytrainingstepsforachieving
superiorperformance. Specifically,wepresentatheoryfortheoptimallearningof
LMs. WefirstproposeanobjectivethatoptimizesLMlearningbymaximizingthe
datacompressionratioinan‚ÄúLM-training-as-lossless-compression‚Äùview. Then,
wederiveatheorem,namedLearningLaw,torevealthepropertiesofthedynamics
intheoptimallearningprocessunderourobjective. Thetheoremisthenvalidated
byexperimentsonalinearclassificationandareal-worldlanguagemodelingtask.
Finally,weempiricallyverifythattheoptimallearningofLMsessentiallystems
fromtheimprovementofthecoefficientsinthescalinglawofLMs,indicatinggreat
promise and significance for designing practical learning acceleration methods.
Ourcodecanbefoundathttps://aka.ms/LearningLaw.
Conventional LM Learning Learning Speed( | ) > Learning Speed( | )
Optimal LM Learning
Conventional LM Learning >
Optimal LM Learning
Area Under Loss Curve( | ) Area Under Loss Curve( | )
0 Training Steps ùëá Data Compression Ratio( | ) > Data Compression Ratio( | )
Figure1: Ourobjectiveistominimizetheareaunderlosscurve,whichisequivalenttomaximizing
the compression ratio of training corpus in the ‚ÄúLM-training-as-lossless-compression‚Äù view. A
learninglawisproposedtorevealthetrainingdynamicsoftheaboveoptimallearning.
5.0 Conventional LM Learning
(Near-)Optimal LM Learning
4.5 ScalingLaws B Œ≤
ConventionalLMLearning 3.16√ó108 0.12
4.0
(Near-)OptimalLMLearning 1.99√ó107 0.14
3.5 2.41√ó Speedup
0 1000Traini2n0g0 S0teps3000 4000 Table1: The(near-)optimalLMlearningimproves
thescalinglaws[KMH+20]overconventionalLM
Figure2: Optimallearninggetsthetheoreti-
training. The coefficients B,Œ≤ are used to fit the
calspeedupupperboundofTransformerLM
trainingonTinyStoriescorpus[EL23].
losscurvesinFigure2,i.e.,Loss=(B/t)Œ≤.
‚àóContributionduringaninternshipatMicrosoftResearch.‚ü®guyx21@mails.tsinghua.edu.cn‚ü©
4202
beF
72
]LC.sc[
1v95771.2042:viXra
ssoL
ssoL1 Introduction
Withthethrivingoflanguagemodels(LMs;HZD+21,BHA+21),thereisanincreasingfocuson
improvingthelearning[PR12,WSSC19]ofLMs,whichaimsatacceleratingthelearningspeed
and achieving a certain model performance with as few training steps as possible [WWL+23].
ThisfocushelpshumansexplorethelimitsofLMsgiventherapidgrowthoftheircomputational
demand[HBM+22],andpromotesdemocratization[CMS+23]oflargelanguagemodels(LLMs;
BMR+20,Ope22,Ope23,CND+23,ADF+23),whichisvaluableforbothresearchcommunities
andindustrysectors[TLI+23,TMS+23,JSM+23].
Inthispaper,wepresentatheoryforoptimallearningofLMs. Unlikepriorworksexploringpractical
accelerationmethodsatthemodel-level[XYH+20,ZH20],optimizer-level[YLR+20,LLH+24],or
data-level[TSAM23,ATS+23,XPD+23],ourworkdemonstratestheprinciplesofoptimizingthe
LMlearningspeed,includingtheoptimizationobjective,thepropertyofoptimallearningdynamics,
andtheessentialimprovementofthelearningacceleration.
Specifically,fortheoptimizationobjective,weproposetominimizetheareaunderthelosscurve
(AUC;CM03),whichhasaclearphysicalsignificance:thedescriptionlengthwhenweviewthenext-
token-predictionLMtrainingprocessaslosslesscompressionofthetrainingdata[Bel19,MCKX22,
Rae23]. AsshowninFigure1,alearningprocesswiththesmallestlossAUCcorrespondstothe
highestcompressionratio. Simultaneously,thelossinthisprocessalsoconvergestoasmallvalueat
thehighestrate,givensufficientlylargetotaltrainingsteps. Therefore,weconsideroptimizingLM
learningequivalenttomaximizingthecorrespondingcompressionratioofthelearningprocess,
andadoptthelatterastheoptimizationobjectiveinourtheory. Similarobjectivesarealsoemployed
tointerprettheremarkablegeneralizationperformanceofrecentLLMs[VNK+23,DRD+24].
Wethenderiveatheorem,namedLearningLaw,thatcharacterizesthepropertyofdynamicsinthe
LMlearningprocessthatachievestheoptimumofourobjective. Here,alearningprocessisinduced
byalearningpolicythatdetermineswhichdatapointstheLMlearnsasthetrainingprogresses. In
thisway,wesolvetheoptimallearningpolicyinthesensethatthecorrespondingcompressionratio
ismaximized,andobtainourLearningLaw(seeTheorem3.1foraformalexpression):
LearningLaw
AllexampleshavethesamecontributiontotheLMintheoptimallearningprocess.
A B
Optimal LM Learning Conventional LM Learning Compression Ratio
of Optimal Learning
Conventional
Learning
Contribution of the ùëõ!" example Contribution of the ùëõ!" example Compression Ratio
Figure3: A:3-DillustrationofLearningLaw(Theorem3.1). Intheoptimallearningprocess,all
trainingexamplesshouldhavethesamecontributiontoLMlearning,wherethecontributionisdefined
asthedot-productofthegradientonindividualsamples(‚àál ,‚àál ,and‚àál )andthegradientofa
m n k
desiredloss(‚àáL). SeeSection3.2forrigorousnotationdefinitions. B:Experimentalevidenceof
LearningLaw. WhenLMlearningapproachestheoptimum,thesimilarityofexamplecontributions
tendsto+‚àû,whichmeansallexampleshavethesamecontributiontotheLM.
AsshowninFigure3,thecontributionofanexampleisdefinedasthedot-productofitsgradient
andthegradientofadesiredloss2,whichmeasuresitsinfluenceontheLMinthedesiredlearning
2NotethatthedesiredlossisnotnecessarilythesameasthetraininglossasdiscussedinSection2.
2
snoitubirtnoC
elpmaxE
fo
ytiralimiSdirection. LearningLawalsosuggestsamatchingoflocalandgloballearningspeedintheoptimal
learningprocess,whichinterpretstheoptimallearningpolicyasadynamicdatare-weightingstrategy
thatencouragestheLMtolearnhighlycontributiveexamplesandsimultaneouslyavoidover-fitting
them. Similar mechanisms are also found critical to the best teaching methods for humans in
psychologicalresearch[Met09,KPA12].
WeexamineourtheorybyexperimentsonlinearclassificationtasksbasedonPerceptron3[MP43]
andreal-worldlanguagemodelingtasksbasedonTransformer[VSP+17]. Wefirstdesignagradient-
basedmethodtosearchfortheoptimallearningpolicyunderourobjective. Then,weverifythat
the dynamics of the learning process induced by the found near-optimal policy aligns well with
ourLearningLaw. Finally,asshowninTable1,weprovideempiricalevidenceshowingthatthe
near-optimallearningpolicyessentiallyimprovesthecoefficientsinthetrainingstepscalinglawof
LMs[KMH+20],whichleadsto5.50√óand2.41√óspeeduptoPerceptronandTransformerlearning,
respectively. Thisemphasizesthepromiseandsignificanceofexploringmorescalablemethodsto
optimizethelearningpolicyinpracticeandacceleratethetrainingofLLMs.
2 ProblemFormulation
We consider LM training on a large-scale dataset with N examples {xtrn}N for a sufficiently
n n=1
largetotaltrainingtimestepsT. LetŒ≥ denotetheweightofthenthtrainingexampleatthetime
n,t
step t, a learning policy is represented by a time-variant distribution over N training examples
Œ≥ =[Œ≥ ,Œ≥ ,¬∑¬∑¬∑ ,Œ≥
]‚ä§,satisfying(cid:80)N
Œ≥ =1andŒ≥ ‚â•0for1‚â§n‚â§N,0‚â§t‚â§T ‚àí1.
t 1,t 2,t n,t n=1 n,t n,t
The conventionally trained LM learns with a policy Œ≥c = 1 (conventional learning). Recent
n,t N
works[PLKS20,ABL+22]haveshownthattheoriesderivedbasedonGradientDecent(GD)offer
insightsintoothergradient-basedalgorithms[KB15]. Therefore,forsimplicity,weassumetheLMis
trainedwithGDfort=0,1,¬∑¬∑¬∑ ,T ‚àí1:
N
(cid:88)
Ltrn(Œ∏ )= Œ≥ l(xtrn,Œ∏ ),
t t n,t n t (1)
n=1
Œ∏ =Œ∏ ‚àíŒ∑‚àáLtrn(Œ∏ ),
t+1 t t t
whereŒ∏ ‚àà RD isthemodelparametersflattenedintoaD-dimensionalvectoratthetimestept,
t
Œ∑ is the learning rate, and l(¬∑,¬∑) is the loss function of the learning problem. For LMs, l(¬∑,¬∑) is
typicallytheMaximumLikelihoodEstimation(MLE)loss: l(x,Œ∏ ) = ‚àílogp (x),wherexisa
t Œ∏t
textsequence. Following[XSML23]and[MBR+22],wefocusonthelearningspeedreflectedby
thereductionrateofadesiredlossLdsrcomputedonK examples{xdsr}K thatdonotnecessarily
k k=1
followthesamedistributionasthetrainingexamples:
K
1 (cid:88)
Ldsr(Œ∏ )= l(xdsr,Œ∏ ). (2)
t K k t
k=1
Thisformulationappliestoabroadofpracticalscenariosincludingclassicalmachinelearningusing
avalidationsettopreventover-fitting[Vap99],large-scalepre-trainingrelyingonacarefullycurated
held-outcorpustoevaluategeneralizationperformance[KMH+20],anddomainadaptationwherea
naturaldifferenceexistsbetweentrainingandtargetdistribution[XSML23]. Assuch,wesearchfor
thelearningpolicyŒ≥ thatmaximizesthereductionrateofLdsr(Œ∏ )tooptimizeLMlearning.
t t
However, direct analysis of this optimization problem is difficult due to the discreteness of GD.
Therefore,wefocusonthecontinuouslimitofGDbyconsideringthecorrespondinggradientflowof
Equation1fort‚àà[0,T],whichismoreamenabletotheoreticalanalysis[SDBD20]:
N
d (cid:88)
Œ∏(t)=‚àí‚àáLtrn(Œ∏(t),t)=‚àí‚àá Œ≥ (t)l(xtrn,Œ∏(t)), (3)
dt n n
n=1
whereŒ≥ (t)isasmoothinterpolationfunctionofŒ≥ . Accordingtotheresultsinnumericalanalysis,
n n,t
GDdefinedinEquation1istheEulermethodtoapproximatelysolvetheinitialvalueproblemof
thegradientflowinEquation3,andŒ∏(t)‚âàŒ∏ whenŒ∑issufficientlysmall[EC21]. InSection4,we
t
showthattheresultsderivedfromthislimitalignwellwiththeexperimentsindiscretesettings.
3InAppendixA.3,weprovidealosslessdatacompressionviewofthePerceptrontraining,indicatingthat
ourtheoryalsoapplies.
33 TheoryforOptimalLearningofLMs
Inthissection,wepresentourtheoryinthecontinuouslimitofGD.Wefirstproposeanobjective
for‚ÄúmaximizingthereductionrateofLdsrbyoptimizingthelearningpolicy‚Äù. Then,wederiveour
maintheorem,namedLearningLaw,whichintroducesanecessaryconditionforthedynamicsofthe
learningprocessinducedbythepolicythatachievestheoptimumoftheobjective.
3.1 Objective: MaximizingCompressionRatio
WecharacterizethereductionrateofLdsrwiththeareaunderthecurveofLdsr(Œ∏(t))(AUCofLdsr)
andminimizethisareatoachievehighlearningspeed:
(cid:90) T
min Ldsr(Œ∏ (t))dt,
Œ≥
Œ≥(t) 0
(cid:88)N (4)
s.t. Œ≥ (t)=1,
n
n=1
Œ≥ (t)‚â•0,n=1,2,¬∑¬∑¬∑ ,N,
n
where Œ≥(t) = [Œ≥ (t),Œ≥ (t),¬∑¬∑¬∑ ,Œ≥ (t)]‚ä§ and Œ∏ (t) is an alias of Œ∏(t) satisfying Equation 3 to
1 2 n Œ≥
emphasizeitsdependencyonŒ≥(t). AsshowninFigure1,forsufficientlylargeT,alearningprocess
withminimallossAUCownsthehighestlossreductionrate. Interestingly,theAUCofLdsr hasa
physicalsignificancefromthe‚ÄúLM-training-as-lossless-compression‚Äùview[Rae23]: theresulting
descriptionlengthofcompressingdatadrawnfromthedesireddatadistribution. Therefore,
Equation 4 is equivalent to maximizing the corresponding compression ratio. Note that unlike
[DRD+24]thatstudiesencodingdatausingawell-trainedLM,weviewtheentireLMtrainingasa
compressionprocess. WeprovidemorediscussionofthesetwoperspectivesinSection5. Besides,
therearestillslightdifferencesbetweenourstatementandthatinpriorworksviewingthetraining
processaslosslesscompression[Bel19,MCKX22,Rae23]: weconsiderthedesiredlossAUCof
GDtrainingformultipleepochs,whilethepreviousstatementisaboutthetraininglossAUCwith
single-epochSGDtraining. MorediscussionaboutthisdifferencecanbefoundinAppendixA.2.
3.2 LearningLaw
Equation4definesanOptimalControlproblemthatcanbesolvedbyMaximumPrinciple[Pon18].
However,wefindthesolutionhardtointerpretandverifyinpracticalLMlearning. Therefore,inthis
work,wederivealoosernecessaryconditionfortheoptimumofEquation4.
Theorem3.1(LearningLaw). WhenanLMistrainedwithanoptimallearningpolicy,which
yieldsalearningprocesscorrespondingtoamaximumcompressionratioonthedesireddata
distribution,thefollowingconditionholdsfor0<t‚â§T andanym,nsuchthatŒ≥ (t)>0,
m
Œ≥ (t)>0:
n
‚àáL¬∑‚àál =‚àáL¬∑‚àál =Const, (5)
m n
where ‚àáL = ‚àáLdsr(Œ∏(t)) = ‚àá1 (cid:80)K l(xdsr,Œ∏(t)), ‚àál = ‚àál(xtrn,Œ∏(t)), ‚àál =
K k=1 k m m n
‚àál(xtrn,Œ∏(t)),and¬∑isdot-product. Const=‚àídLdsr(Œ∏(t))isthedesiredlosschangerate
n dt
overtimeandisindependentofnandm.
To prove Theorem 3.1, we apply the Euler-Lagrange (EL) equation [GS+00] and
Karush‚ÄìKuhn‚ÄìTucker (KKT) conditions [Ber16] to Equation 4, which results in the condition:
‚àáLdsr(Œ∏(t))¬∑‚àál(xtrn,Œ∏(t))=‚àídLdsr(Œ∏(t)). AfullproofisshowninAppendixB.
n dt
‚àáL¬∑‚àál inEquation5representsthecontributionofthetrainingexamplextrntoLdsr(Œ∏(t)),which
n n
ismaximizedwhenthegradientonxtrn sharesthesamedirectionwiththegradientofLdsr(Œ∏(t)).
n
WedenoteCT (t)=‚àáL¬∑‚àál =‚àáLdsr(Œ∏(t))¬∑‚àál(xtrn,Œ∏(t))forconvenienceintherestof
n n n
thepaper. Notethatwhenthemodelisconverged(‚àáLtrn(Œ∏(t),t) ‚âà 0),CT (t)canbeviewedas
n
anapproximationoftheInfluenceFunction[KL17]bysettingtheHessianmatrixofLtrn(Œ∏,t)at
Œ∏ = Œ∏(t) to an identity matrix [PLKS20]. In essence, Equation 5 means CT (t) equals a value
n
4independentofn. Sincethezero-weightexamples(Œ≥ (t)=0)aretypicallynoisy(verifiedinSection
n
4.5),Theorem3.1suggeststhatallnon-noisyexamplesshouldbeidenticallycontributivetothe
LMintheoptimallearningprocess. Inthefollowing,weprovidemorediscussionofthistheorem.
3.3 Discussion
Theorem3.1suggestsamatchingofthelocalandgloballearning. Anotherinterpretationof
CT (t)isthe‚Äúlocallearningspeed‚Äù: howfasttheLMlearnstheknowledgeinxtrnthatishelpfulto
n n
reduceLdsr. Thisisbecausethedot-productoperationinCT (t)canbeviewedastheprojection
n
oftheindividuallossdescendingvelocity‚àál(xtrn,Œ∏(t))onthedesireddirection. Correspondingly,
n
dLdsr(Œ∏(t))representstheLM‚Äôs‚Äúgloballearningspeed‚Äù: howfasttheLMgetsbetterbylearning
dt
allindividualxtrn. Asaresult,CT (t)=Const=‚àídLdsr(Œ∏(t))inTheorem3.1indicatesthatthe
n n dt
locallearningspeedshouldmatchthegloballearningspeedintheoptimallearningprocess.
The optimal learning policy establishes a dynamic data re-weighting strategy. Generally,
as the learning of LM progresses, CT (t) drops because the gradient norm on each example
n
||‚àál(xtrn,Œ∏(t))||decreasesastheLMfitsxtrn. Inaddition,thedirectionof‚àál(xtrn,Œ∏(t))diverges
n n n
from‚àáLdsr(Œ∏(t))duetothepossiblediscrepancybetweenthedistributionofxtrnandxdsr,whichalso
n k
contributestothedecreaseofCT (t). Therefore,Theorem3.1guaranteesthathighlycontributive
n
example xtrn with high CT (t) obtains large weights for training, in order to reduce CT (t) to
n n n
meetthevalueofotherexamples. Ontheotherhand, Theorem3.1alsoensuresthattheweights
ofxtrnareloweredbeforetheLMover-fitsitbecauseCT (t)shouldnotbetoosmalltomatchthe
n n
globallearningspeed. Altogether,thisformsadynamictrainingdatare-weightingstrategy,whichis
intuitivelyessentialfortheoptimallearningpolicythatmaximizesthelearningspeedofanLM.
Theorem3.1isanecessaryconditionfortheoptimallearningdynamics. ThisisbecausetheE-L
equationandKKTconditionsarenecessaryconditionsfortheglobaloptimumwhentheoptimization
problem is non-convex. Therefore, a learning process satisfying Theorem 3.1 is not guaranteed
optimal. Forexample,bysettingŒ≥ (t) = 1andŒ≥ (t) = Œ≥ (t) = ¬∑¬∑¬∑ = Œ≥ (t) = 0,Equation5is
1 2 3 N
satisfied,regardlessofthevaluesofCT (t). ThislearningpolicycorrespondstousingSGDwith
n
mini-batchsize=1,whichisunlikelytobetheoptimal[MKAT18]. Therefore,searchingforthe
optimalpolicyaccordingtoTheorem3.1mayneedregularizationtermsinpractice,whichweleave
forfutureworktoexplore.
4 Experiments
WeconductexperimentsinthediscretesettingofEquation1,wheretheconclusionsderivedfromthe
continuouslimitsinSection3arestillapplicablewhenŒ∑issufficientlysmall[EC21]. Wefirstdesign
amethodtofindtheoptimallearningpolicyŒ≥ ‚ààRN for0‚â§t‚â§T ‚àí1,byexplicitlyminimizing
t
theAUCofLdsr(Œ∏ )inthediscretesetting,whichmaximizesthecorrespondingcompressionratio
t
ofdatadrawnfromthedesireddistribution. ThenweexamineourLearningLaw(Theorem3.1)on
thelearningprocessinducedbythefoundpolicies. Finally,weempiricallyverifythatmaximizing
thecompressionratioessentiallyimprovesthescalinglawcoefficients[KMH+20],indicatingthe
practicalsignificanceandpromiseofourtheory.
4.1 FindingtheOptimalLearningPolicy
TofindtheoptimalŒ≥ ,wedirectlysolvethediscreteversionoftheoptimizationproblemdefinedin
t
Equation4withaProximalGradientMethod[BC11]:
T
(cid:88)
J(Œ≥)= Ldsr(Œ∏ ),
t
(6)
t=1
Œ≥ ‚ÜêProj[Œ≥ ‚àíœµ‚àá J(Œ≥)], 0‚â§t‚â§T ‚àí1,
t t Œ≥t
whereJ(Œ≥)isadiscreteapproximationoftheintegralinEquation4,œµisthelearningrateandProj[¬∑]
projectsapointinRN totheN-simplex,ensuringthatŒ≥ isaprobabilitydistributionoverN training
t
examples. Theoptimizationprocesscanbeimplementedefficientlyusingdynamicprogrammingand
Jacobian-Vector-ProductinPyTorch[PGM+19],whichisdescribedindetailinAppendixC.
57 3.3
4.1
0.25
Learning Oplicy Optimization Loss 6 4.0 Learning Oplicy Optimization Loss 3.2
Compression Rate Compression Rate
0.20 N Coe na vr- eO np tt ioim naa ll LL ee aa rr nn ii nn gg 5 3.9 N Coe na vr- eO np tt ioim naa ll LL ee aa rr nn ii nn gg 3.1
3.8 0.15 4 3.0
0 100 200 300 400 500 0 5 10 15
Optimization Epochs Optimization Epochs
(a) PerceptronLinearClassification (b) TransformerLanguageModeling
Figure4: LearningpolicyoptimizationresultsinPerceptronlinearclassification(a)andTransformer
language modeling tasks (b). We plot the learning policy optimization loss J(Œ≥) (solid lines),
definedinEquation6,whichrepresentstheareaunderthecurve(AUC)ofthedesiredPerceptronor
Transformerloss. Wealsoshowthecorrespondingcompressionratioofthetrainingprocess(dashed
lines)inan"LM-as-Lossless-Compression"view. Theoptimizationstartsfromconventionallearning
andsmoothlyconvergestonear-optimallearningwithlowlossAUCandhighcomprehensionrate.
0.20
Conventional Learning Conventional Learning
5.0 Near-Optimal Learning Near-Optimal Learning
4.0
2.41√ó Speedup
0.100 5.50√ó Speedup
0.09
0 500 1000 1500 2000 0 1000 2000 3000 4000
Training Time Steps t Training Time Steps t
(a) PerceptronLinearClassification (b) TransformerLanguageModeling
Figure5: CurvesofthedesiredlossLdsr(Œ∏ )whenthemodelistrainedusingtheconventionaland
t
the near-optimal learning policy. The near-optimal learning process achieves 5.50√ó speedup in
Perceptronlinearclassification(a)and2.41√óspeedupinTransformerlanguagemodeling(b).
4.2 ExperimentalSetup
WeconductexperimentsonalinearclassificationtaskbasedonPerceptron[MP43]andalanguage
modelingtaskbasedonTransformer[VSP+17]. SeeAppendixDforhyper-parameterconfigurations.
PerceptronLinearClassification. Weadoptateacher-studentsetting[Eng01]whereeachexample
x =(z ,y )isapairofD-dimensionalvectorz ‚ààRD drawni.i.d. fromGaussiandistribution,
n n n n
andascalary =sign(T¬∑z )giventhegroundtruthweightT‚ààRD. Weintroduceashiftbetween
n n
thetrainingandthedesireddatadistributiontoreflecttheirdifferences. Thedataarelearnedbyan
one-layerPerceptionparameterizedbyŒ∏ ‚ààRD: o =œÉ(Œ∏¬∑z )= 1 ,whichistrained
n n 1+exp(‚àíŒ∏¬∑zn)
withMaximumLikelihoodEstimation(MLE)lossl(x n,Œ∏)=‚àílogoy nn(1‚àío n)1‚àíyn. InAppendix
A.3,weshowthatPerceptroncanbeviewedasaone-stepLM,whichmeansourtheorystillapplies.
TransformerLanguageModeling. Consideringthecomputationcostoftheoptimalpolicysearch-
ing,weadoptatwo-layerTransformerwithabout1.7MparametersandtrainitonTinyStories[EL23],
ahigh-qualitypre-trainingcorpus. Weaddperturbationstothetrainingexamples(seeAppendixD
fordetails),whichmimicstherelativelylowqualityofthepre-trainingcorpusinpractice. Sinceour
theoreticalderivationisgenerallyapplicable,webelievethatourtheoryalsoappliestolargerLMs.
To migrate the risk of over-fitting the K examples used to compute Ldsr(Œ∏ ) in Section 4.1, we
t
additionallyconstructaheld-outtestsetwithK examplesfromthedesireddatadistributioninboth
PerceptronlinearclassificationandTransformerlanguagemodelingexperiments. Inthefollowing,
6
ssoL
deriseD
eht
fo
CUA
)t
(rsdL
ssoL
deriseD
Compression
Rate
(CR)
ssoL
deriseD
eht
fo
CUA
)t
(rsdL
ssoL
deriseD
Compression
Rate
(CR)6
0.75
6 3.2
4 0.50
5 3.1
0.25 2
0.00 4 0 3.0
100 10 1CR 6√ó100 4√ó100 CR
Desired Loss Ldsr( t) Desired Loss Ldsr( t)
(a) PerceptronLinearClassification (b) TransformerLanguageModeling
Figure6: EmpiricalevidenceofourLearningLaw(Theorem3.1)inPerceptronlinearclassification
(a)andTransformerlanguagemodeling(b)tasks. Wemeasurethedegreeofsimilarityincontribution
among different samples by SIM , the Signal-Noise-Ratio of the contribution CT of training
t n,t
examples, calculated as the mean divided by the standard deviation of CT across examples
n,t
(Equation7). HigherSIM meansbettercontributionsimilarity. WeplotSIM withrespecttothe
t t
desiredlossLdsr(Œ∏ )underdifferentlearningprocesses. Eachlineisacertainlearningprocess,whose
t
colormeansthecorrespondingcompressionratio(CR). RunswithhigherCRgenerallygethigher
SIM throughoutlearning,indicatingthattheexamplecontributionsaremoresimilartoeachotherin
t
alearningprocessclosertotheoptimum,whichisinlinewithourLearningLaw(Theorem3.1).
wecomputeandreporttheevaluationmetricsbytreatingthetestexamples,unseenduringthe
policyoptimization,asxdsrinEquation2.
k
4.3 LearningPolicyOptimizationResults
Anear-optimallearningpolicycanbefoundwiththemethodinSection4.1. InFigure4,we
showtheoptimizationprocessoffindingtheoptimallearningpolicy. Weplotthelearningpolicy
optimizationlossJ(Œ≥),whichisalsotheAUCofLdsr(Œ∏ )inthelearningprocessinducedbyŒ≥ ,and
t t
thecorrespondingcompressionratioCR= Tlog|V| ,whereV isthesizeofthelabel/vocabulary
(cid:80)T t=1Ldsr(Œ∏t)
spaceforPerceptron/transformer(seeAppendixA.1formoreexplanation). ThecurveofJ(Œ≥)is
smoothandalmostconvergesattheend,indicatingthatanear-optimallearningpolicyisfound.
Thenear-optimallearningpolicyyieldsahighaccelerationratioofthelearningspeed. In
Figure5,weplotthecurveofLdsr(Œ∏ )whenthePerceptronandTransformeraretrainedunderthe
t
conventionalandnear-optimallearningpolicies. Thenear-optimalpoliciessignificantlyimprovethe
lossAUC,bringingaboutacceleration5.50√óand2.41√óattheendofthePerceptronandTransformer
training,respectively. Notethatallreportedmetricsarecomputedonthetestsetunseenduringthe
policyoptimization,suggestingthatthenear-optimalpolicydoesnotover-fitthespecificexamples
usedtocomputeLdsr(Œ∏ )buthelpsthemodellearnfasteronthedesireddistribution.
t
4.4 DirectVerificationofLearningLaw(Theorem3.1)
We examine the similarity between CT which is the discrete version of the individual sample
n,t
contributionCT (t)inacertainlearningpolicyandsatisfiesCT =CT (t)fort=1,2,¬∑¬∑¬∑ ,T.
n n,t n
Thesimilarity(SIM)ismeasuredbytheSignal-Noise-RatioofCT :
n,t
CT
SIM = t , (7)
t s
CT,t
(cid:114)
whereCT =
(cid:80)N
Œ≥ CT istheweightedmeanands =
(cid:80)N n=11[Œ≥n,tÃ∏=0](CTn,t‚àíCTt)2
t n=1 n,t n,t CT,t (cid:80)N n=11[Œ≥n,tÃ∏=0]‚àí1
isthestandarddeviationofCT fortrainingexampleswithnon-zeroweight. ThehigherSIM
n,t t
meansthatthetrainingexampleshavemoresimilarCT . NotethatSIM isdimensionless,which
n,t t
avoidstheimpactoftheabsolutevaluescalechangeofCT duringlearning. Wealsoconsider
n,t
SIM= 1 (cid:80)T SIM ,whichsummarizesthesimilaritiesofCT throughoutthelearningprocess.
T t=1 t n,t
7
tRNS tRNSCompression Ratio Compression Ratio
of Optimal Learning of Optimal Learning
Conventional Conventional
Learning Learning
(a) PerceptronLinearClassification (b) TransformerLanguageModeling
Figure 7: Empirical evidence of the Learning Law (Theorem 3.1) in Perceptron linear classi-
fication (a) and Transformer language modeling (b) tasks. Following Figure 6, we consider
SIM= 1 (cid:80)T SIM ,whichsummarizesthesimilarityofthetrainingexamplecontributionsina
T t=1 t
learningprocess. WeplottherelationshipbetweenSIMandCR,andobserveanevidenttendency
(cid:16) (cid:17)c
thatSIM ‚Üí +‚àûwhenCRapproachesacertainvalue,whichcanbefitbySIM = log a .
b‚àíCR
Whenthelearningprocessapproachestheoptimum(CR‚Üíb),thestandarddeviationsoftraining
examplecontributionsshouldbezerotoallowSIM‚Üí+‚àû. ThisverifiesLearningLaw(Theorem
3.1)thatalltrainingexampleshavethesamecontributiontothemodelinoptimallearning.
Highercompressionratiocorrelateswithhighersamplecontributionsimilarities. InFigure
6,weexaminethevalueofSIM inthelearningprocessinducedbyeachpolicyfoundalongthe
t
optimizationprocessofŒ≥ . Sincethefoundpoliciesbringaboutfasterconvergence,weplotSIM
t t
withrespecttoLdsr(Œ∏ ),ratherthant. Inthisway,SIM arecomparedatthesame‚Äústage‚Äùofthe
t t
modellearning,migratingtheimpactofdifferentconvergencespeeds. Figure6demonstratesthat
thelearningprocesswithahighercompressionratio(CR)generallykeepshigherSIM inmodel
t
learning,indicatingthatthecontributionsCT ofindividualsamplesaremoresimilartoeachother
n,t
throughoutthelearningprocess,whichalignswithourLearningLaw(Theorem3.1).
Sample contributions tend to be equal when the learning process approaches the optimum.
In Figure 7, we plot SIM with respect to CR for each learning process. We observe an evident
tendencythatSIM‚Üí+‚àûwhenCRapproachesacertainvalue. Accordingly,weusethefunction
(cid:16) (cid:17)c
SIM = log a tofitthetendencyoftheexperimentalobservations. Figure7indicatesthat
b‚àíCR
whenthelearningprocesscontinuouslyimprovesuntiltheoptimum(CR‚Üíb),thestandarddeviation
ofCT shouldbezerotoallowSIM‚Üí+‚àû. ThisverifiesLearningLaw(Theorem3.1)thatthe
n,t
contributionsofnon-zero-weighttrainingsamples(CT )areidenticalinoptimallearning.
n,t
4.5 PropertiesofZero-WeightExamples
TheexperimentsinSection4.4mostlyfocusonthenon-zero-weightexamples. Inthissection,we
providemoreempiricalevidenceforLearningLaw(Theorem3.1)byexaminingthepropertiesofthe
exampleswithŒ≥ =0. WederivethreepropertiesoftheoptimallearningdynamicsfromTheorem
n,t
3.1andthenverifythemthroughexperiments. Thefirstpropertyguaranteesthatexampleswith
non-positivecontributionsreceiveŒ≥ =0,indicatingthatthe‚Äúnoisy‚Äùexamplesateachtimestep
n,t
areexcludedbytheoptimallearningpolicy:
Property4.1. ThetrainingexamplextrnwhoseCT ‚â§0getsŒ≥ =0beforethemodelconverges.
n n,t n,t
Proof. Before convergence, dLdsr(Œ∏(t)) < 0 holds, indicating CT > 0 for xtrn that satisfies
dt n,t n
Œ≥ >0,accordingtoTheorem3.1. Therefore,CT ‚â§0‚áíŒ≥ =0.
n,t n,t n,t
880 Near-Optimal Learning 80 Near-Optimal Learning
Conventional Learning Conventional Learning
60 60
40 40
20 20
0 0
4 5 6 7 3.0 3.1 3.2 3.3
Compresson Rate (CR) Compresson Rate (CR)
(a) PerceptronLinearClassification (b) TransformerLangnaugeModeling
Figure8: EmpiricalevidenceofProperty4.1: non-contributiveandnoisyexamplesareexcludedin
optimallearning. They-axisisthefractionofzero-weightexamplesamongthosewithCT ‚â§0at
n,t
thesametimestep. Eachpointrepresentsalearningpolicy,whichtendstoassigntheexampleweight
Œ≥ =0to100%ofnoisyandnon-contributivedatawhenitapproachestheoptimum.
n,t
1.0
6
0.5
5
4
0.0
0.0 0.5 1.0 CR
n,t /max{ n,t}
n
Figure9: EmpiricalevidenceofProperty4.2: perfectlylearnedexamplesareignoredinoptimal
learning.Weplotthecumulativedistributionfunction(CDF)oftheexampleweightsŒ≥ thatsatisfies
n,t
l(xtrn,Œ∏ ) < 1√ó10‚àí6. Eachlinecorrespondstoalearningprocess. Alargefractionoflow-loss
n t
examples(perfectlylearned)inthenear-optimallearningobtainsmallŒ≥ values(ignored),andthis
n,t
tendencybecomesmoreevidentwhenthelearningapproachesitsoptimum(CRincreases).
Empirical Evidence. We calculate the fraction of zero-weight examples (Œ≥ = 0) among all
n,t
exampleswithnon-positivecontributionsatt(CT ‚â§ 0):
(cid:80) n,t1[Œ≥n,t=0]1[CTn,t‚â§0]
andplotthis
n,t (cid:80) n,t1[CTn,t‚â§0]
fractionwithrespecttotheCRvalueofthecorrespondinglearningprocessinFigure8. Wecansee
thatwhenthelearningprocessapproachestheoptimum,thefractiontendsto100%,indicatingthat
thenon-contributiveexamplesarediscarded.
ThesecondpropertyisderivedonlyforPerceptronlinearclassification,whichindicatesthatthe
optimallearningpolicywillignorethoseperfectlylearnedtrainingexamples:
Property4.2. ForPerceptrons,theperfectlylearnedxtrn,whosemargin(2ytrn‚àí1)Œ∏ ¬∑ztrn ‚Üí+‚àû
n n t n
atthetimestept,getsŒ≥ =0intheoptimallearningpolicywhenthemodelisyetconverged.
n,t
Proof. When (2ytrn ‚àí1)Œ∏ ¬∑ztrn ‚Üí +‚àû, we have otrn ‚àíytrn ‚Üí 0, which means ‚àál(xtrn,Œ∏ ) =
n t n n n n t
(otrn ‚àíytrn)ztrn ‚Üí 0 and CT ‚Üí 0. Assuming Œ≥ Ã∏= 0, according to Theorem 3.1, we have
n n n n,t n,t
CT n,t = ‚àí dd tLdsr(Œ∏(t))intheoptimallearningprocess, whichmeansthat(cid:12) (cid:12) dd tLdsr(Œ∏(t))(cid:12) (cid:12)should
be arbitrarily small. This does not hold when the model is not converged. Therefore, we have
Œ≥ =0.
n,t
Empirical Evidence. In Figure 9, we plot the cumulative probability distribution function of
Œ≥n,t forthewell-learnedPerceptrontrainingexamplesxtrnwithnear-zeroper-instancetraining
maxn{Œ≥n,t} n
loss: l(xtrn,Œ∏) < 1√ó10‚àí6. Figure9showsthatforthenear-optimalpolicy,morethan90%well-
n
learnedexampleshaverelativelylowŒ≥ (<0.2max {Œ≥ }). Thistrendbecomesmoreevidentas
n,t n n,t
thelearningpolicyapproachestheoptimum(CRincreases),whichverifiesProperty4.2.
9
)%(
0=t,n
fo
noitcarF
ytiliborP
evitalumuC
)%(
0=t,n
fo
noitcarF(a) PerceptronLinearClassification (b) TransformerLanguageModeling
Figure10: EmpiricalevidenceofProperty4.3: redundanttrainingexamplesarediscardedinoptimal
learning. We randomly sample 2048 training examples satisfying CT > 0 (contributive and
n,t
unlearnedexamples)throughoutthenear-optimallearningprocessandshowthedynamicsofthe
exampleweightŒ≥ (representedbythecolorin(a)and(b)). SincePerceptronconvergesquickly,
n,t
weonlyplotitsŒ≥ dynamicsfort‚â§50. Thenear-optimalpoliciesassignŒ≥ =0toredundant
n,t n,t
examplesinadditiontotheperfectlylearnedandnon-contributivedatapoints.
Thethirdpropertysuggeststhattheoptimallearningpolicywilldiscardthe‚Äúredundant‚Äùtraining
examples. AlthoughthispropertyisderivedfromPerceptronlinearclassification,weempirically
findthatitalsoappliestoTransformerlanguagemodeling. Wecallaset{x }N has‚Äúredundant‚Äù
n n=1
examples when the example inputs in the set are linearly correlated, i.e., there exist K scalars
{Œ± }N ,notallzero,suchthat(cid:80)N Œ± z =0.
n n=1 n=1 n n
Property4.3. ForPerceptrons,ifthetrainingset{xtrn}N hasredundantexamples,withprobability
n n=1
1,atleastoneexamplextrn getsŒ≥ =0atthetimesteptwhenthemodelisyetconvergedinthe
i i,t
optimallearningprocess.
Proof. Given that {xtrn}N has redundant examples, there exist scalars {Œ± }N , not all zero,
n n=1 n n=1
such that (cid:80)N n=1Œ± nzt nrn = 0, which means (cid:80)N n=1 otrnŒ± ‚àín ytrnCT n,t = 0. Assuming ‚àÄ1 ‚â§
n n
n ‚â§ N, Œ≥ Ã∏= 0, according to Theorem 3.1, we have CT = ‚àídLdsr(Œ∏(t)), suggesting
n,t n,t dt
(cid:16) (cid:17)
(cid:80)N Œ±n dLdsr(Œ∏(t))=0. Fori.i.d. inputs{ztrn}N ,withprobability1,(cid:80)N Œ±n Ã∏=
n=1 otrn‚àíytrn dt n n=1 n=1 otrn‚àíytrn
n n n n
0,whichmeans dLdsr(Œ∏(t))=0. Thisdoesnotholdwhenthemodelisyetconverged. Therefore,
dt
wehavethepropertythat‚àÉ1‚â§n ‚â§N,suchthatŒ≥ =0.
0 n0,t
EmpiricalEvidence. InFigure10,wevisualizethedynamicsoftheŒ≥ valuesatisfyingCT >0
n,t n,t
throughoutthelearningprocessofPerceptronandTransformer. ForPerceptron,themodeldimension
(128) is lower than the number of training examples (4096), which means the training dataset is
redundant. Figure 10(a) shows that, given the absence of the non-contributive examples, a large
fractionofŒ≥ stillreceivesrelativelysmallvaluesbeforethemodelconverges,whichiscausedbythe
t
redundancyofthetrainingset. InFigure10(b),weobserveasimilarphenomenonforTransformer,
althoughthedimensionofŒ∏ islargerthanthenumberoftraininginstances. Wesuspectthereasonis
t
thattheintrinsicdimensionofTransformerisusuallymuchsmallerthanthedimensionofŒ∏ [AGZ21],
t
whichleadstotheredundancyofthetrainingset.
4.6 EssenceofLearningAcceleration
We investigate the essential improvement brought by the near-optimal learning policy in the per-
spectiveofthescalinglawsofLMs[KMH+20],whichrevealsapowerlawbetweentheincreaseof
trainingstepsandthereductionofthetestloss(Ldsr(Œ∏ ))afterawarming-upstaget :
t 0
(cid:18) B(cid:19)Œ≤
Ldsr(Œ∏ )= , t>t , (8)
t t 0
where(B,Œ≤)arescalinglawcoefficients. Inthefollowing,westudythescalingpropertiesofthe
learningprocessesinducedbytheconventionalandnear-optimallearningpolicies.
10Figure 11: Illustration of the scaling law [KMH+20]: Ldsr(Œ∏ ) = (B/t)Œ≤ for conventional and
t
near-optimalLMlearninginTransformerlanguagemodeling. Wefitthelosscurvesbythescaling
lawtoobtainthecorrelationcoefficientr2andshowthelosscurve(solidlines)togetherwiththefit
curve(dashedlines)inalog-logplot. Thescalinglawfitswellforbothconventionalandnear-optimal
LM learning. The near-optimal LM learning essentially improves the coefficients (B,Œ≤) in the
scalinglawby96.6%and21.2%,whichshowsgreatpotentialforspeedupintrainingLLMs.
T N |‚àÜB|(%) |‚àÜŒ≤|(%) AR
B Œ≤
1K 212 88.5 10.0 2.16
2K 213 94.9 18.0 2.31
4K 214 93.7 18.7 2.41
8K 215 94.8 19.0 2.48
Table 2: The improvements of the scaling law coefficients brought by the near-optimal learning
policyfordifferenttotaltrainingsteps(T)anddatasizes(N)inTransformerlanguagemodeling.
ThevocabularysizeincreaseswiththegrowthofN (seeAppendixDfordetails). ARstandsforthe
accelerationratioasdefinedinEquation9. TheimprovementsholdforlargerT andN.
Thenear-optimallearningpolicyimprovesthescalinglawcoefficientsofLMs. InFigure11,
wefittheTransformer‚Äôslosscurvesinducedbytheconventionalandnear-optimallearningpolicies
withEquation8bysettingt =4004. Weobservethatthenear-optimallearningprocessstillfollows
0
thescalinglaw,withB andŒ≤ improvedby96.6%and21.2%respectively. Additionally,Table2
showsthattheimprovementholdsforthenear-optimalpoliciesfoundinthesettingoflargerT and
N. WeletN growwithT toensurethesufficiencyoftrainingdata [HBM+22]. Theimprovement
ofscalinglawcoefficients,especiallyŒ≤,providessignificantpotentialinboostingthespeedofLLM
learningbytakingadvantageofpowerlawgrowth. FortwolearningpoliciesŒ≥(1) andŒ≥(2) which
inducetwolosscurvesLdsr (Œ∏ )andLdsr (Œ∏ )withtwosetsofscalinglawcoefficients(B ,Œ≤ )
Œ≥(1) t Œ≥(2) t 1 1
and(B ,Œ≤ ),theaccelerationratioofŒ≥(2)overŒ≥(1)is:
2 2
Œ≤1
AR= (cid:110)
T
(cid:111) =
B 1Œ≤2 T1‚àíŒ≤ Œ≤1
2. (9)
argmin Ldsr (Œ∏ )‚â§Ldsr (Œ∏ ) B 2
t Œ≥(2) t Œ≥(1) T
ForanLMpre-trainedfor10Msteps,wewillobtainmorethan9√óaccelerationattheendofthe
training if the scaling property of the LM is improved as in Figure 11 and Table 2. Based on
therecentexperienceintrainingLLMs[TLI+23,TMS+23],modelsarefarfromfullyconverged
underthecurrenttrainingbudget,whichmeanssmallmodels(like7B)havethepotentialtoreach
theperformanceoflargemodels(like65B),givenenoughtrainingsteps. However, accordingto
Chinchilla‚Äôslaw[HBM+22],extendingthetrainingstepsrequiresmorecomputationthanenlarging
themodeltoachieveacertainperformance. Therefore,byoptimizingthelearningpolicytoimprove
4Inpractice,weconvertEquation8tolnLdsr(Œ∏ )=‚àíŒ≤lnt+Œ≤lnBandperformlinearregression.
t
11learningspeed,thecostoftrainingwell-performedsmallmodelscanbelargelyreduced,whichis
beneficialbothforopen-sourceendeavorsintheLMresearchcommunityandfortheefficiencyof
industrialproducts. Thisindicatesthepromiseandsignificanceofdesigningpracticallearningpolicy
optimizationapproaches,andourtheorycanbeavaluableguide.
5 RelatedWork
ImprovingtheLearningSpeedofLanguageModel. Thereisabroadrangeofworksthatpropose
approaches to accelerate LM learning speed such as modifying model architectures [XYH+20,
ZH20]oroptimizers[YLR+20,LLH+24,ZHSJ20]. Therearealsoworksstudyingthepre-training
dataprogrammingtospeedupLMconvergence,suchasdatade-duplication[TSAM23,ATS+23],
domain mixture [XPD+23], intrinsic task discovery [GHLH22], and online data selection or re-
ordering[CRB+23,GAH23,APRW23],whichcanbeviewedasspecialcasesofoptimizinglearning
policy. Unliketheseworks,weinvestigatetheprinciplesofoptimizingLMlearninginthispaper.
Language Modeling and Lossless Compression. The recent success of LLMs calls for new
interpretationsbeyondclassicalstatisticlearningtheoryforthefactthatlargermodelsizesconstantly
causebetterdownstreamgeneralization[NKB+19,WTB+22]. Oneoftheinterpretationsistoview
thenext-token-predictiontrainingprocessofanLMaslosslessdatacompression[Bel19,MCKX22,
Rae23]. In this perspective, larger LMs have higher compression ratios, corresponding to better
modeling of data generation regularities. It is worth noting that some recent works [VNK+23,
DRD+24] explore using well-trained LMs as compressors and thus the model sizes should be
countedintothecompresseddata. Unliketheseworks,viewingLMtrainingascompressiondoesnot
requireincludingthemodelparametersinthecompresseddata(seeAppendixA.1foraconstructive
proof)andthusismorecompatiblewiththemodelsizescalinglawofLMs[KMH+20].
6 DiscussionandConclusion
Summary. In this work, we establish a theory for the optimal learning of LMs. We propose
anobjectivethatmaximizesthecompressionratioinanLM-training-as-losses-compressionview.
Thenwederiveatheorem,namedLearningLaw,suggestingthatallexamplesshouldbeequally
contributivetotheLMintheoptimallearningprocess,whichisthenvalidatedbyexperimentsin
linearclassificationandreal-worldlanguagemodelingtasks. Finally,weempiricallyshowthatthe
optimallearningprocessessentiallyimprovesthescalinglawcoefficientsofLMs,whichshedslight
onfutureworksthatdesignpracticallearningaccelerationapproaches.
Limitations. Onelimitationofourworkisthattheexperimentsareconductedonrelativelysmall
scales. Thisisbecauseourmethodtofindthenear-optimallearningpolicycorrespondstotraininga
neuralnetworkwithL√óT layers,whereListhelayersoftheLMandT istheLM‚Äôstotaltraining
steps(seeAppendixCfordetails). ThisleadstoahighcomputationaloverheadwhenLandT scale
up. However, since the theoretical derivation is generally applicable, we believe that our theory
canbeappliedtoLLMs. AnotherlimitationisthatourderivationassumestheLMistrainedwith
full-batchGD,ratherthansomemorecommonlyusedtechniqueslikemini-batchAdam[KB15].
Sincethesemethodsareessentiallygradient-based,ourtheorycanstillofferinsightstofutureLM
learningaccelerationstudiesbasedonthesetechniques[PLKS20,ABL+22].
FutureWork. Webelievethatanimportantdirectionoffutureworkisdesigningpracticalmethods
tofindtheoptimallearningpoliciesbasedonourtheoryforthelarge-scaletrainingofLMs. Indeed,
therearenon-negligiblechallengesinthisdirection. Sincethelearninglawprovidesanecessary
conditionforthelearningpolicy‚Äôsoptimality, moreregularizationconditionsmayberequiredto
preventsub-optimalsolutions. Inaddition,theapproachtofindingtheoptimallearningpolicyshould
beefficientenoughwithoutcontributingmuchtotheoverallcomputationcost. Nevertheless,our
workdemonstratesthepromiseandpotentialofthisdirection. AccordingtorecentworksonLLMs
training[TLI+23,TMS+23,JSM+23],thelossesarestillfarfromconvergence,whichmeansthat
smallmodelshavethepotentialtoreachthesimilarperformanceaslargemodels,butarehindered
bythecomputationoverheadbroughtbythelargetotaltrainingsteps. Theoptimallearningpolicy
potentiallybringsaboutalargeaccelerationoftrainingwiththehelpofthepower-lawgrowthin
Equation 9, which makes it possible to explore the limits of LMs given (inevitably) constrained
computationandtrainawell-performedsmallLMthatreplacescurrentLLMsinpractice.
12References
[ABL+22] Ekin Akyurek, Tolga Bolukbasi, Frederick Liu, Binbin Xiong, Ian Tenney, Jacob
Andreas,andKelvinGuu. Towardstracingknowledgeinlanguagemodelsbacktothe
trainingdata. InYoavGoldberg,ZornitsaKozareva,andYueZhang,editors,Findings
ofEMNLP,2022.
[ADF+23] RohanAnil,AndrewMDai,OrhanFirat,MelvinJohnson,DmitryLepikhin,Alexandre
Passos,SiamakShakeri,EmanuelTaropa,PaigeBailey,ZhifengChen,etal. Palm2
technicalreport. arXivpreprintarXiv:2305.10403,2023.
[AGZ21] Armen Aghajanyan, Sonal Gupta, and Luke Zettlemoyer. Intrinsic dimensionality
explainstheeffectivenessoflanguagemodelfine-tuning. InProceedingsofACL,2021.
[APRW23] AlonAlbalak,LiangmingPan,ColinRaffel,andWilliamYangWang. Efficientonline
datamixingforlanguagemodelpre-training. InNeurIPS2023WorkshoponR0-FoMo:
RobustnessofFew-shotandZero-shotLearninginLargeFoundationModels,2023.
[ATS+23] AmroKamalMohamedAbbas,KushalTirumala,DanielSimig,SuryaGanguli,and
Ari S Morcos. SemDeDup: Data-efficient learning at web-scale through semantic
deduplication. InICLR2023WorkshoponMathematicalandEmpiricalUnderstanding
ofFoundationModels,2023.
[BC11] HHBauschkeandPLCombettes. ConvexAnalysisandMonotoneOperatorTheoryin
HilbertSpaces. Springer,2011.
[Bel19] FabriceBellard. NNCP:Losslessdatacompressionwithneuralnetworks,2019.
[Ber16] DimitriBertsekas. Nonlinearprogramming,volume4. Athenascientific,2016.
[BHA+21] RishiBommasani,DrewAHudson,EhsanAdeli,RussAltman,SimranArora,Sydney
vonArx,MichaelSBernstein,JeannetteBohg,AntoineBosselut,EmmaBrunskill,etal.
Ontheopportunitiesandrisksoffoundationmodels. arXivpreprintarXiv:2108.07258,
2021.
[BMR+20] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,etal. Languagemodels
arefew-shotlearners. InProceedingsofNeurIPS,2020.
[CM03] CorinnaCortesandMehryarMohri. AUCoptimizationvs.errorrateminimization. In
ProceedingsofNeurIPS,2003.
[CMS+23] ArnoCandel,JonMcKinney,PhilippSinger,PascalPfeiffer,MaximilianJeblick,Prithvi
Prabhu, JeffGambera, MarkLandry, ShivamBansal, RyanChesler, etal. h2oGPT:
Democratizinglargelanguagemodels. arXivpreprintarXiv:2306.08161,2023.
[CND+23] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,
AdamRoberts,etal. PaLM:Scalinglanguagemodelingwithpathways. JMLR,2023.
[CRB+23] MayeeFChen,NicholasRoberts,KushBhatia,JueWANG,CeZhang,FredericSala,
and Christopher Re. Skill-it! a data-driven skills framework for understanding and
traininglanguagemodels. InProceedingsofNeurIPS,2023.
[DRD+24] Gr√©goireDel√©tang,AnianRuoss,Paul-AmbroiseDuquenne,ElliotCatt,TimGenewein,
ChristopherMattern,JordiGrau-Moya,LiKevinWenliang,MatthewAitchison,Laurent
Orseau,etal. Languagemodelingiscompression. InProceddingsofICLR,2024.
[EC21] OmerElkabetzandNadavCohen. Continuousvs.discreteoptimizationofdeepneural
networks. InA.Beygelzimer,Y.Dauphin,P.Liang,andJ.WortmanVaughan,editors,
ProceedingsofNeurIPS,2021.
[EL23] RonenEldanandYuanzhiLi. TinyStories: Howsmallcanlanguagemodelsbeandstill
speakcoherentenglish? arXivpreprintarXiv:2305.07759,2023.
[Eng01] AndreasEngel. Statisticalmechanicsoflearning. CambridgeUniversityPress,2001.
13[GAH23] DavidGrangier,PierreAblin,andAwniHannun. Bileveloptimizationtolearntraining
distributionsforlanguagemodelingunderdomainshift. InNeurIPS2023Workshopon
DistributionShifts: NewFrontierswithFoundationModels,2023.
[GHLH22] YuxianGu,XuHan,ZhiyuanLiu,andMinlieHuang. PPT:Pre-trainedprompttuning
forfew-shorlearning. InProceedingsofACL,2022.
[GS+00] IzrailMoiseevitchGelfand,RichardASilverman,etal. Calculusofvariations. Courier
Corporation,2000.
[HBD+20] AriHoltzman,JanBuys,LiDu,MaxwellForbes,andYejinChoi. Thecuriouscaseof
neuraltextdegeneration. InProceedingsofICLR,2020.
[HBM+22] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor
Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl,
AidanClark,etal. Trainingcompute-optimallargelanguagemodels. arXivpreprint
arXiv:2203.15556,2022.
[HZD+21] Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, et al. Pre-trained models: Past,
presentandfuture. AIOpen,2021.
[HZRS16] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningfor
imagerecognition. InProceedingsofCVPR,2016.
[JSM+23] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Deven-
draSinghChaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,Guillaume
Lample,LucileSaulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.
[KB15] DiederikP.KingmaandJimmyBa. Adam: Amethodforstochasticoptimization. In
ProceedingsofICLR,2015.
[KEC22] IliaKulikov,MaksimEremeev,andKyunghyunCho. Characterizingandaddressingthe
issueofover-smoothinginneuralautoregressivesequencemodeling. InProceedingsof
AACL,2022.
[KL17] PangWeiKohandPercyLiang. Understandingblack-boxpredictionsviainfluence
functions. InProceedingsofICML,2017.
[KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,
RewonChild,ScottGray,AlecRadford,JeffreyWu,andDarioAmodei. Scalinglaws
forneurallanguagemodels. arXivpreprintarXiv:2001.08361,2020.
[KPA12] CelesteKidd,StevenTPiantadosi,andRichardNAslin. Thegoldilockseffect: Human
infantsallocateattentiontovisualsequencesthatareneithertoosimplenortoocomplex.
PloSone,7(5):e36399,2012.
[LLH+24] HongLiu,ZhiyuanLi,DavidHall,PercyLiang,andTengyuMa. Sophia: Ascalable
stochasticsecond-orderoptimizerforlanguagemodelpre-training. InProceedingsof
ICLR,2024.
[LXLM23] HongLiu, Sang MichaelXie, ZhiyuanLi, and Tengyu Ma. Samepre-trainingloss,
betterdownstream: Implicitbiasmattersforlanguagemodels. InProceedingsofICML,
2023.
[MBR+22] S√∂renMindermann,JanMBrauner,MuhammedTRazzak,MrinankSharma,Andreas
Kirsch, Winnie Xu, Benedikt H√∂ltgen, Aidan N Gomez, Adrien Morisot, Sebastian
Farquhar,etal. Prioritizedtrainingonpointsthatarelearnable,worthlearning,andnot
yetlearnt. InProceedingsofICML,2022.
[MCKX22] YuMao,YufeiCui,Tei-WeiKuo,andChunJasonXue. Trace:Afasttransformer-based
general-purposelosslesscompressor. InProceedingsofWWW,NewYork,NY,USA,
2022.
[Met09] JanetMetcalfe. Metacognitivejudgmentsandcontrolofstudy. Currentdirectionsin
psychologicalscience,18(3):159‚Äì163,2009.
14[MKAT18] SamMcCandlish,JaredKaplan,DarioAmodei,andOpenAIDotaTeam. Anempirical
modeloflarge-batchtraining. arXivpreprintarXiv:1812.06162,2018.
[MP43] WarrenSMcCullochandWalterPitts. Alogicalcalculusoftheideasimmanentin
nervousactivity. Thebulletinofmathematicalbiophysics,1943.
[NKB+19] PreetumNakkiran,GalKaplun,YaminiBansal,TristanYang,BoazBarak,andIlya
Sutskever. Deep Double Descent: Where bigger models and more data hurt. In
ProceedingsofICLR,2019.
[Ope22] OpenAI. OpenAI:Introducingchatgpt,2022.
[Ope23] OpenAI. GPT-4technicalreport,2023.
[PGM+19] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal. Pytorch:
Animperativestyle,high-performancedeeplearninglibrary.InProceedingsofNeurIPS,
2019.
[PLKS20] Garima Pruthi, Frederick Liu, Satyen Kale, and Mukund Sundararajan. Estimating
trainingdatainfluencebytracinggradientdescent. InNeurIPS,2020.
[Pon18] LevSemenovichPontryagin. Mathematicaltheoryofoptimalprocesses. Routledge,
2018.
[PR12] WarrenBPowellandIlyaORyzhov. Optimallearning,volume841. JohnWiley&
Sons,2012.
[Rae23] JackRae. Compressionforagi,2023.
[RM87] DavidE.RumelhartandJamesL.McClelland.Learninginternalrepresentationsbyerror
propagation. InParallelDistributedProcessing: ExplorationsintheMicrostructureof
Cognition: Foundations,1987.
[RRRH20] SamyamRajbhandari,JeffRasley,OlatunjiRuwase,andYuxiongHe. ZeRO:Memory
optimizationstowardtrainingtrillionparametermodels. InProceedingsofSC20,2020.
[SDBD20] SamuelLSmith,BenoitDherin,DavidBarrett,andSohamDe. Ontheoriginofimplicit
regularizationinstochasticgradientdescent. InProceedingsofICLR,2020.
[TLI+23] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,
Timoth√©eLacroix,BaptisteRozi√®re,NamanGoyal,EricHambro,FaisalAzhar,Aurelien
Rodriguez,ArmandJoulin,EdouardGrave,andGuillaumeLample. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[TMS+23] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,Yasmine
Babaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal.
Llama2:Openfoundationandfine-tunedchatmodels.arXivpreprintarXiv:2307.09288,
2023.
[TSAM23] KushalTirumala,DanielSimig,ArmenAghajanyan,andAriSMorcos. D4: Improving
llm pretraining via document de-duplication and diversification. In Proceedings of
NeurIPS,2023.
[Vap99] VladimirVapnik. Thenatureofstatisticallearningtheory. Springerscience&business
media,1999.
[VNK+23] Chandra Shekhara Kaushik Valmeekam, Krishna Narayanan, Dileep Kalathil, Jean-
FrancoisChamberland,andSrinivasShakkottai. LLMZip: Losslesstextcompression
usinglargelanguagemodels. arXivpreprintarXiv:2306.04050,2023.
[VSP+17] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN
Gomez,≈ÅukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InProceedings
ofNeurIPS,2017.
15[WKR+19] SeanWelleck,IliaKulikov,StephenRoller,EmilyDinan,KyunghyunCho,andJason
Weston. Neuraltextgenerationwithunlikelihoodtraining. InProceedingsofICLR,
2019.
[WSSC19] RobertCWilson,AmitaiShenhav,MarkStraccia,andJonathanDCohen. Theeighty
fivepercentruleforoptimallearning. Naturecommunications,10(1):4646,2019.
[WTB+22] JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,
etal. Emergentabilitiesoflargelanguagemodels. TMLR,2022.
[WWL+23] ZhongweiWan,XinWang,CheLiu,SamiulAlam,YuZheng,ZhongnanQu,ShenYan,
YiZhu,QuanluZhang,MosharafChowdhury,etal. Efficientlargelanguagemodels: A
survey. arXivpreprintarXiv:2312.03863,2023.
[XPD+23] SangMichaelXie,HieuPham,XuanyiDong,NanDu,HanxiaoLiu,YifengLu,Percy
Liang,QuocVLe,TengyuMa,andAdamsWeiYu.DoReMi:Optimizingdatamixtures
speedsuplanguagemodelpretraining. InProceedingsofNeurIPS,2023.
[XSML23] SangMichaelXie,ShibaniSanturkar,TengyuMa,andPercyLiang. Dataselectionfor
languagemodelsviaimportanceresampling. InProceedingsofNeurIPS,2023.
[XYH+20] RuibinXiong,YunchangYang,DiHe,KaiZheng,ShuxinZheng,ChenXing,Huishuai
Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the
transformerarchitecture. InProceedingsofICML,2020.
[YLR+20] YangYou,JingLi,SashankReddi,JonathanHseu,SanjivKumar,SrinadhBhojana-
palli,XiaodanSong,JamesDemmel,KurtKeutzer,andCho-JuiHsieh. Largebatch
optimizationfordeeplearning: Trainingbertin76minutes. InProceedingsofICLR,
2020.
[ZH20] MinjiaZhangandYuxiongHe. Acceleratingtrainingoftransformer-basedlanguage
modelswithprogressivelayerdropping. ProceedingsofNeurIPS,2020.
[ZHSJ20] JingzhaoZhang,TianxingHe,SuvritSra,andAliJadbabaie. Whygradientclipping
acceleratestraining: Atheoreticaljustificationforadaptivity. InProceedingsofICLR,
2020.
16A DiscussionofLMTrainingasLosslessCompression
A.1 OriginalView: CompressingtheTrainingData.
TheideaofusinganLMtocompressdataoriginatesfromtheliteratureinthelosslesstextcompression
field[Bel19,MCKX22],andisrecentlyadoptedtointerprettheessenceofthenext-token-prediction-
based pre-training of LMs [Rae23]. We restate its core spirit by the following Theorem and the
constructiveprooffrom[Rae23]:
TheoremA.1. ConsideranLMtrainedonatextcorpusDwithM tokensusingmini-batchnext-token-
predictionforoneepoch. LetB bethenumberoftokensinabatchandL bethebatch-averaged
t
traininglossatthetimestept. AssumethatM isdivisiblebyB. Thetrainingprocesscanbeviewed
aslosslesscompressionofthetrainingdata. ThedescriptionlengthofthecompresseddataC is
M/B
(cid:88)
d(C)= B¬∑L +d(LM), (10)
t
t=1
whered(LM)isthelengthofthenecessarycoderepresentedbya0-1stringtoruntheLMtraining.
Proof. The basic idea of the proof is to construct a lossless encoding and decoding process for
D with the LM. Let p (¬∑|w ) be the output distribution of the LM parameterized by Œ∏ at the
Œ∏t <m t
time step t, conditioning on the token prefix w = [w ,w ,¬∑¬∑¬∑ ,w ]. For simplicity,
<m m‚àí1 m‚àí2 1
we assume that the LM is trained using mini-batch Stochastic Gradient Decent (SGD) with a
learningrateŒ∑,whereeachbatchislinearizedtoacontinuouslistoftokens. Thebatch-averaged
training loss is L = ‚àí1 (cid:80)B logp (w |w )5. The encoding the decoding process are
t B m=1 Œ∏t m <m
described in Algorithm 1 and 2. Basically, the main body of the algorithms other than the blue-
colored parts implements the LM training. For encoding, the description length of a token w
m
is ‚àílogp (w |w ) according to Arithmetic Coding 6 , and thus the compressed length of a
Œ∏t m <m
batchW ={w }B is(cid:80)B [‚àílogp (w |w )]=B¬∑L . d(C)equalsthesumofper-batch
m m=1 m=1 Œ∏t m <m t
descriptionlengthsthroughoutthetrainingplusthelengthofthecodeforLMtraining. Therefore,we
getd(C)=B¬∑(cid:80)M/B¬∑L
+d(LM). Fordecoding,sincethecodeforLMtrainingisthesameas
t=1 t
thatinencoding,wehaveŒ∏‚Ä≤ =Œ∏ ,andthusw‚Ä≤ =w foranystepsinAlgorithm2,whichcanbe
1 1 m m
easilyprovedbymathematicalinduction. Asaresult,DcanbecompletelyreconstructedfromC,
indicatingtheencoding(compression)islossless.
Remark1. Thedescriptionlengthofthecompresseddatad(C)isapproximatelyproportionaltothe
areaunderthetraininglosscurve(AUC)whenM ‚â´ 1becausethesizeofLMtrainingcodesis
muchsmallerthanthatofthecompressedcorpusandthusd(C)‚âà(cid:80)M/BB¬∑L
=B¬∑AUC.
t=1 t
Remark 2. Let V be the vocabulary size of the LM and assume M ‚â´ 1. The corresponding
compressionratioofthelearningprocessinTheoremA.1isCR= MlogV ‚âà MlogV ‚àù 1 .
d(C) (cid:80)M t=/ 1BB¬∑Lt AUC
As the LM fits the data, we generally have L < logV because logV is the loss for a randomly
t
initializedLM.Thismeansthecompressionisvalid,resultinginacompressionratioCR>1.
Altogether,TheoremA.1bridgesaconnectionbetweendatacompressionandLMtraining. Generally,
a higher compression ratio indicates that the compression algorithm models the underlying data
knowledgebetterandcorrespondstoabetterperformedLM,asstatedinthefollowingremark:
Remark3. TheLM‚Äôsabilitytomodeltheknowledgeindataischaracterizedbythecorresponding
losslesscompressionratioofitslearningprocess,whichisinverselyproportionaltothelossAUC.
Notethatthemodelparametersarenotincludedinthecalculationofd(C),andenlargingthemodel
sizes typically reduces the loss AUC, which explains the remarkable performance of LLMs. In
addition,d(C)relatestothewholeLMtrainingprocess,notjustthefinalloss. Thisisinlinewith
thefactthatlargerLMstendtoperformbetterthansmallermodels,eveniftheirfinallossesarethe
same[LXLM23]. ThisobservationsupportstheperspectivethatLMtrainingcanbeconceptualized
asaprocessoflosslessdatacompression.
5log(¬∑)standsforlog (¬∑)inthefollowingsections.
2
6https://en.wikipedia.org/wiki/Arithmetic_coding
17Algorithm1Encoding Algorithm2Decoding
Input: TrainingcorpusD Input: CompresseddataC: listof0-1string
Input: ThecodeforLMtrainingasa0-1string Output: TrainingcorpusD
Output: CompresseddataC: listof0-1strings GettheLMtrainingcodefromthefirststringinC
InitializeC toanemptylist PopthefirststringfromC
AppendtheLMtrainingcodetoC InitializeDtoanemptylist
InitializetheLM‚ÄôsparameterstoŒ∏ InitializetheLM‚ÄôsparameterstoŒ∏‚Ä≤
1 1
fort‚Üê1toM/Bdo fort‚Üê1toM/Bdo
Get a batch of tokens W = {w }B Getabatchof0-1stringsS ={s }B from
m m=1 m m=1
fromthetrainingcorpusD thecompresseddataC
form‚Üê1toB,w ‚ààW do fork ‚Üê1toB,s ‚ààS do
m m
Encodew toa0-1stringswithArith- Decodew‚Ä≤ froms withArithmeticCoding
m m m
meticCodingbasedonp (¬∑|w ) basedonp (¬∑|w‚Ä≤ )
Œ∏t <m Œ∏ t‚Ä≤ <m
Appendthe0-1stringstoC Appendthetokenw‚Ä≤ toD
m
endfor endfor
L t ‚Üê‚àí B1 (cid:80)B m=1logp Œ∏t(w m|w <m) L t ‚Üê‚àí B1 (cid:80)B m=1logp Œ∏ t‚Ä≤(w m‚Ä≤ |w <‚Ä≤ m)
Œ∏ ‚ÜêŒ∏ ‚àíŒ∑‚àáL Œ∏‚Ä≤ ‚ÜêŒ∏‚Ä≤ ‚àíŒ∑‚àáL
t+1 t t t+1 t t
endfor endfor
A.2 OurView: CompressingDatafromtheDesiredDistribution.
Although we also focus on the loss AUC throughout the paper, our setting differs from that in
AppendixA.1: (1)weassumetheLMistrainedwithfull-batchGradientDescent(GD)formultiple
epochswhileTheoremA.1liesinthescenariowheretheLMistrainedwithSGDforonlyoneepoch;
(2)weconsiderLdsrcomputedondataotherthanthetrainingexamples,whilep inEquation10is
Œ∏t
computedonthetrainingdata. However,althoughnotentirelyrigorous,wearguethatRemark3still
holdsdespitethedifferencesin(1)and(2). Thereasonisthat: regarding(1),mini-batchSGDisan
approximationofGD,whichmeanstheysharethesimilartrainingdynamicswhenthebatchsizeof
SGDislargeenough;regarding(2),justlikethetraininglossAUC,theAUCofLdsrcanbeviewedas
thedescriptionlengthofcompressingexamplesfromthedesireddatadistributionduringthelearning
process. Inthisway,Remark3indicatesthatminimizingtheAUCofLdsrcorrespondstooptimizing
the data compression on the desired distribution, which improves the LM‚Äôs ability to model the
desireddataknowledge. Thisismoreofpracticalconcernbecauseinmostscenarios, themodel
performanceismeasuredonadatasetotherthanthetrainingset,suchasthevalidationsetinclassical
machinelearning[Vap99],thehigh-qualityheld-outcorpusinlarge-scalingpre-training[KMH+20],
orthetargetsetindomainadaption[XSML23].
A.3 PerceptronTrainingasLosslessCompression
Viewing model training as lossless compression stems from the next-token-prediction learning
paradigmofLMs. Weshowthatthisperspectivealsofitsintheone-epochMaximumLikelihood
Estimation(MLE)trainingofPerceptronsonthelinearclassificationtask,wherethelabelofeach
exampleiscompressedgiventheinputvectors. Specifically,theproofinAppendixA.1stillapplies
if we treat linear classification as a one-step language modeling with vocabulary size V = 2.
FollowingthenotationinSection4.2,foraPerceptronparameterizedbyŒ∏ atthetimestept,its
t
probability of outputting y conditioning on z is p (y|z) = oy(1‚àío)1‚àíy, where o = œÉ(Œ∏ ¬∑z).
Œ∏t t
ForabatchB ={(z ,y )}B ,thebatch-averagedlossisL =‚àí1 (cid:80)B logp (y |z ). With
t n n n=1 t B n=1 Œ∏t n n
Algorithm 1 and 2 applied for encoding and decoding, the description length of the compressed
B
is(cid:80)B
[‚àílogp (y |z )]=B¬∑L ,whichmeansTheoremA.1stillholdsandthediscussion
t n=1 Œ∏t n n t
in Appendix A.2 also applies. For a dataset with N examples in total, the compression ratio
CR ‚âà NlogV = N . ForarandomlyinitializedŒ∏ ,L ‚âà 1,andasthemodeltrains,
(cid:80)N t=/ 1BB¬∑Lt (cid:80)N t=/ 1BB¬∑Lt 1 1
L ‚Üí0,indicatingavaliddatacompressingprocessofcompressionratioCR>1.
t
18B ProofofTheorem3.1
Theorem3.1essentiallyreflectsthepropertyofthedynamicsinthelearningprocessinducedbythe
optimallearningpolicyŒ≥(t)fortheproblemdefinedinEquation4. Wechoosetheaccumulation
of each Œ≥ (t) over time: Œì (t) = (cid:82)t Œ≥ (t‚Ä≤)dt‚Ä≤, as a set of free variables that Œ∏(t) depends on to
n n 0 n
solvetheoptimizationproblem. Inthisway,theproblemissimplifiedbyconsideringascalarthat
summarizes‚Äúhowmuch‚Äùanexampleisusedfortraininguntilt,ratherthanthewholetrajectoryof
Œ≥ (t). Assuch,ŒìÀô (t)= dŒì (t)=Œ≥ (t)andEquation4becomes:
n n dt n n
(cid:90) T
min Ldsr(Œ∏ (t))dt,
Œì,Œ≥
Œì(t),Œ≥(t) 0
N
s.t. (cid:88) ŒìÀô (t)=1, (11)
n
n=1
ŒìÀô (t)‚â•0,n=1,2,¬∑¬∑¬∑ ,N,
n
whereŒì(t)=[Œì (t),Œì (t),¬∑¬∑¬∑ ,Œì (t)]‚ä§,andŒ∏ (t)isanaliasofŒ∏(t)toshowitsdependencyon
1 2 N Œì,Œ≥
ŒìandŒ≥. LetLbetheLagrangiandependingon{Œì (t)}N and{ŒìÀô (t)}N :
n n=1 n n=1
N N
L=Ldsr(t)+Œª(t)((cid:88) ŒìÀô (t)‚àí1)+(cid:88) ¬µ (t)ŒìÀô (t), (12)
n n n
n=1 n=1
whereŒª(t)and¬µ (t)areLagrangemultipliersandLdsr(t)=Ldsr(Œ∏ (t))=Ldsr(Œ∏(t)).Toachieve
n Œì,Œ≥
theoptimumofEquation11,LshouldsatisfytheEuler-Lagrange(EL)Equation[GS+00]:
d ‚àÇL ‚àÇL
‚àí =0. (13)
dt‚àÇŒìÀô
n
‚àÇŒì
n
TogetherwithotherconstraintsintheKarush‚ÄìKuhn‚ÄìTucker(KKT)conditions[Ber16],wegetthe
followingformulasthatcharacterizetheoptimumoftheEquation11:
Ô£± ‚àÇLdsr(t)
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≤
(cid:88)N‚àÇ ŒìÀôŒì
nn
(t)= =Œª 1Àô ,(t)+¬µÀô n(t),
(14)
n=1
Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£¥Ô£≥
¬µ
(t)Œì Œì¬µÀô
Àôn
n(
(
(t
t
t)
)
)‚â•
‚â•
=0
0
0,
,
.
n n
Note that we only consider the E-L Equations on {Œì (t)}N and {ŒìÀô (t)}N , part of the free
n n=1 n n=1
variablesintheoriginalproblem,whichalsodependsontheŒ≥(t)trajectory. SinceKKTconditions
arenecessaryconditionstotheoptimizationproblem,Equation14isalsonecessary. Inthefollowing,
wesimplifyEquation14toreachTheorem3.1.
Simplifying Equation 14. We study the examples with non-zero weights during training. For
Œ≥ (t) = ŒìÀô (t) > 0 we have ¬µ (t) = 0 according to ¬µ (t)ŒìÀô (t) = 0 in Equation 14 and the
n n n n n
followingLemmabridgesaconnectionbetween¬µ (t)and¬µÀô (t):
n n
LemmaB.1. For¬µ (t)‚ààC1[0,T],¬µ (t)=0‚áí¬µÀô (t)=0atthespecifictimestept.
n n n
Proof. Assuming‚àÉt ‚àà [0,T],s.t. ¬µ (t ) = 0but¬µÀô (t ) Ã∏= 0,welet¬µÀô (t ) > 0withoutlossof
0 n 0 n 0 n 0
generality. Then‚àÉŒ¥t > 0,s.t. ¬µ (t ‚àíŒ¥t) < 0accordingto¬µ (t) ‚àà C1[0,T],whichcontradicts
n 0 n
¬µ (t)‚â•0inEquation14. Therefore,wehave¬µÀô (t )=0.
n n 0
Assuch,Œ≥ (t)>0‚áí¬µ (t)=0‚áí¬µÀô (t)=0(LemmaB.1),whichmeans:
n n n
‚àÇLdsr(t) ‚àÇLdsr(t)
= =ŒªÀô(t), forŒ≥ (t)>0andŒ≥ (t)>0. (15)
‚àÇŒì ‚àÇŒì m n
m n
NotethatŒªÀô(t)isindependentofmandn. Equation15alreadyresemblesEquation5intheirformats,
ifwehave ‚àÇLdsr(t) ‚àù‚àáL¬∑‚àál ,where‚àáL=‚àáLdsr(Œ∏(t))and‚àál =‚àál(xtrn,Œ∏(t)).
‚àÇŒìn n n n
19Interpreting ‚àÇLdsr(t). ‚àÇLdsr(t) measureshowthechangeofŒì (t)influencethechangeofLdsr(t)
‚àÇŒìn ‚àÇŒìn n
at the time step t when other free variables are fixed. Specifically, if Œì (t) changes by a small
n
valueŒì (t)‚ÜíŒì (t)+‚àÜŒì (t),thenLdsr(t)correspondinglychangesbyasmallvalueLdsr(t)‚Üí
n n n
Ldsr(t)+‚àÜLdsr(t),and ‚àÇLdsr(t) = ‚àÜLdsr(t). Then,weconsider dLdsr(t) withEquation3:
‚àÇŒìn ‚àÜŒìn(t) dt
dLdsr(t) dŒ∏(t)
=‚àáLdsr(Œ∏(t))¬∑
dt dt
N
(cid:88)
=‚àí Œ≥ (t)‚àáLdsr(Œ∏(t))¬∑‚àál(xtrn,Œ∏(t))
n n (16)
n=1
N
=‚àí(cid:88) dŒì n(t)
‚àáL¬∑‚àál .
dt n
n=1
Asaresult,forasmall‚àÜt,wehave:
N
(cid:88)
Ldsr(t+‚àÜt)‚àíLdsr(t)=‚àí [Œì (t+‚àÜt)‚àíŒì (t)]‚àáL¬∑‚àál . (17)
n n n
n=1
NowweconsiderthechangeofLdsr(t)andŒì att+‚àÜt. Since‚àáL¬∑‚àál iscomputedatthetime
n n
stept,itisnotaffectedbythevariants. Therefore,weget:
‚àÜLdsr(t+‚àÜt)=‚àí‚àÜŒì (t+‚àÜt)‚àáL¬∑‚àál , (18)
n n
When‚àÜt‚Üí0, ‚àÜLdsr(t+‚àÜt) ‚Üí ‚àÜLdsr(t) = ‚àÇLdsr ,whichmeans:
‚àÜŒìn(t+‚àÜt) ‚àÜŒìn(t) ‚àÇŒìn
‚àÇLdsr(t)
=‚àí‚àáL¬∑‚àál . (19)
‚àÇŒì n
n
BysubstitutingEquation19intoEquation15,weobtainthatforthemthandnthtrainingexamples
satisfyingŒ≥ (t)>0andŒ≥ (t)>0thefollowingequationholds:
m n
‚àáL¬∑‚àál =‚àáL¬∑‚àál =‚àíŒªÀô(t)=Const, (20)
m n
whereConststandsfor‚Äúaconstantindependentofmandn‚Äù. Equation20isessentiallyequivalent
toEquation5.
ProvingConst=‚àídLdsr(t).
Bysubstituting‚àáL¬∑‚àál withConstinEquation16,weget:
dt n
dLdsr(t) =‚àí(cid:88)N dŒì n(t)
¬∑Const,
dt dt
n=1
N (21)
(cid:88)
=‚àíConst Œ≥ (t),
n
n=1
=‚àíConst.
Assuch,bycombiningEquation20withEquation21,wecompletetheproofofTheorem3.1.
C DetailsofLearningPolicyOptimization
In Section 4.1, we search for the optimal learning policy by Proximal Gradient Decent [BC11].
Specifically,weviewthewholelearningprocessin0 ‚â§ t ‚â§ T asaneuralnetworkwithT layers
parameterizedbyŒ≥ =[Œ≥ ,¬∑¬∑¬∑ ,Œ≥ ]‚ààRN√óT.AsillustratedinFigure12,eachlayerofthenetwork
0 t‚àí1
consistsofthegradientupdatefunctionandaresidualconnection[HZRS16], wherethe‚Äúhidden
states‚ÄùareŒ∏ . Then,weadoptBackwardPropagation(BP;RM87)tocompute‚àá J(Œ≥)inEquation
t Œ≥t
6. Thebackwardoperationateachlayeris:
T
‚àÇJ =‚àíŒ∑ (cid:88) ‚àáLdsr(Œ∏ ) ‚àÇŒ∏ t‚Ä≤ Gtrn(Œ∏ )
‚àÇŒ≥ t‚Ä≤ ‚àÇŒ∏ t
t t+1
t‚Ä≤=t+1 (22)
‚àÇŒ∏ t‚Ä≤ = ‚àÇŒ∏ t‚Ä≤ (cid:2) I‚àíŒ∑Htrn(Œ∏ )(cid:3) ,
‚àÇŒ∏ ‚àÇŒ∏ t+1
t+1 t+2
20¬∑¬∑¬∑
+
¬∑¬∑¬∑
Figure12: Thearchitectureoftheequivalentneuralnetworktofindtheoptimallearningpolicy,Each
layerconsistsofthegradientupdateandaresidualconnection.
where Gtrn(Œ∏ ) = [‚àál(xtrn,Œ∏ ),¬∑¬∑¬∑ ,‚àál(xtrn,Œ∏ )] , I is the identity matrix, and Htrn(Œ∏ ) is the
t 1 t N t t+1
HessainmatrixofLtrn(Œ∏)atŒ∏ =Œ∏ . WeimplementtheBPoperationswithdynamicprogramming
t+1
andJacobian-Vector-Product7inPyTorch[PGM+19]forefficiency. Toreducethesingle-deviceGPU
memoryuse,wealsoimplementanactivationpartitionalgorithminspiredbyZeRO-2[RRRH20],
wherethe‚Äúhiddenstates‚ÄùŒ∏ inonemodelarestoredindifferentGPUdevices.
t
D Hyper-ParameterConfigurations
PerceptronLinearClassification. Followingtheteacher-settingdescribedinSection4.2,weuse
‚àö
D = 128andTisrandomlydrawnfromaGaussiandistributionT ‚àº N(0, DI). Wegenerate
N = 4096 training inputs ztrn from N(0,3I), and M = 512 target inputs zdsr from N(0.51,I),
where 1 = [1,1,¬∑¬∑¬∑ ,1]‚ä§ ‚àà RD. For each learning policy, we initialize Œ≥ = 1 and train the
n,0 N
PerceptronwithŒ∑ =0.1forT =2000timesteps,whichissufficientforthemodeltoconverge. For
learningpolicyoptimization,weinitializethelearningpolicytotheconstantpolicyŒ≥c = 1,setting
n,t N
œµ=5√ó10‚àí6andtrainthenetworkfor500epochs.
TransformerLanguageModeling. Weconductexperimentsbasedonatwo-layerTransformer
with128hiddendimensionsand8attentionheads. ForallexperimentsexceptthatinTable2,we
randomlysampleN =16,384examplesasxtrnandK =512examplesasxdsrwiththemaxsequence
n k
length 64 from the TinyStories [EL23] corpus8. We use the BPE tokenizer of Mistral [JSM+23]
andconstructavocabularywith5Ktokensbymappingtheinfrequenttokensto[UNK].Themodel
containsabout1.7Mparameters.Toreflectthedifferencebetweenthetraininganddesireddistribution,
weaddperturbationsto50%trainingsentencesbyiterativelyapplyingoneofthefollowingoperations
20times: (1)replacingonetokenwitharandomtokeninthevocabulary[HBD+20];(2)deleting
thelasttoken[KEC22];(3)repeatingonetokeninasentence[WKR+19]. Thiscorrespondstothe
factthatthelarge-scalepre-trainingcorpustendstobemorenoisythanthedesiredset(thecarefully
curated held-out corpus or high-quality downstream data) to evaluate the model generalization
performanceinpractice. WesetŒ∑ =0.1,T =4,000,Œ≥ = 1 foreachlearningpolicy. Westart
n,0 N
from the constant policy and optimize the learning policy for 15 epochs using Œ∑ = 0.1,0.2,0.4.
Œ∑ = 0.4yieldsthelowestlossattheendofthetraining. Therefore,weonlyplottheoptimization
processforŒ∑ =0.4inFigure4(b)and5(b). ForexperimentsinTable2,wevarythetotaltraining
stepsandthecorrespondingtrainingdatasizesandsimultaneously,changethevocabularysizesto
adapttodifferentdatasizes. Weusevocabularysizes4K,4.5K,5K,and6KforN =212,213,214,
and215,respectively.
7https://pytorch.org/docs/stable/func.api.html
8https://huggingface.co/datasets/roneneldan/TinyStories/tree/main
21