Language Agents as Optimizable Graphs
MingchenZhuge1⋆,WenyiWang1⋆,LouisKirsch2,FrancescoFaccio1,2
DmitriiKhizbullin1♦,Ju¨rgenSchmidhuber1,2
1AIInitiative,KingAbdullahUniversityofScienceandTechnology,
2TheSwissAILabIDSIA,USI,SUPSI
{mingchen.zhuge, wenyi.wang, francesco.faccio,
dmitrii.khizbullin, juergen.schmidhuber}@kaust.edu.sa, louis@idsia.ch
https://gptswarm.org
Abstract 2022),ReAct(Yaoetal.,2022),treeofthought(TOT)(Yao
et al., 2023), Reflexion (Shinn et al., 2023), and Graph
Varioushuman-designedpromptengineeringtech-
of Thought (GOT) (Besta et al., 2023), to improve text-
niques have been proposed to improve prob-
basedreasoning. SingleagentapplicationssuchasAuto-
lem solvers based on Large Language Models
GPT(Torantulinoetal.,2023),BabyAGI(Nakajima,2023),
(LLMs),yieldingmanydisparatecodebases. We
LangChain (Chase, 2022), and Llama-index (Liu, 2022)
unifytheseapproachesbydescribingLLM-based
utilizeLLMsforvariousfunctionalities,includingtoolus-
agents as computational graphs. The nodes im-
age,functioncalling,andembodiedactions. Inmulti-agent
plementfunctionstoprocessmultimodaldataor
frameworks(Zengetal.,2022;Zhugeetal.,2023),several
queryLLMs,andtheedgesdescribetheinforma-
LLMs take on different roles (Li et al., 2023; Park et al.,
tionflowbetweenoperations. Graphscanbere-
2023;Qianetal.,2023;Wuetal.,2023),tocommunicate
cursivelycombinedintolargercompositegraphs
innaturallanguageandcollectivelysolveagiventask. This
representinghierarchiesofinter-agentcollabora-
approach often outperforms single agents, exploiting the
tion(whereedgesconnectoperationsofdifferent
specialization(Hongetal.,2023)ofvariousLLMagents.
agents). Our novel automatic graph optimizers
Unfortunately, it also leads to increasingly different and
(1) refine node-level LLM prompts (node opti-
disparatecodebasesthatrequirealotofhumanengineering
mization)and(2)improveagentorchestrationby
todefinepromptingschemesandtheworkflowofagents.
changinggraphconnectivity(edgeoptimization).
Experimentsdemonstratethatourframeworkcan Ina“societyofmind”(SOM)(Minsky,1988;Zhugeetal.,
beusedtoefficientlydevelop,integrate,andauto- 2023),higher-levelintelligenceemergesfromthecombina-
maticallyimprovevariousLLMagents. Thecode tionofsimplerandmodularcognitivecomponents. Inspired
canbefoundhere. by SOMs, we describe language agent systems through
graph representations. Language agents querying LLMs
andutilizingexternaltoolsaremodeledascomputational
1.Introduction graphswhereeachnodeisdedicatedtoaspecificfunction,
while the edges define a topology of how inputs are pro-
Interest in LLM-powered autonomous problem solvers
cessedacrossnodes,mirroringthepromptingschemesin
or agents and their varied applications is continually ris-
prior studies. A swarm is defined as a composite graph,
ing (Wang et al., 2023; Xi et al., 2023). However, much
whereeachsubgraphrepresentsacollaborativeagent. This
work remains to be done to effectively incorporate these
creates a deeper hierarchy of intelligence. Agent graphs
agentsintoacohesivesocietyandimprovetheirstructure
combinebasicLLMoperations(Kennedy,2006;Nepusz&
automatically.
Vicsek,2013),andswarmgraphscontainsubgraphsrepre-
Earlyapproacheszero-shot-promptedLLMsorprompted sentingagents. ApproachessuchasCOT(Weietal.,2022),
themwithfew-shotexamples(Kojimaetal.,2022;Brown TOT(Yaoetal.,2023),andSelf-Consistency(Wangetal.,
et al., 2020). Recent methods prompt LLMs in a struc- 2022)canberepresentedbyourgraphs.
tured way, such as chain of thought (COT) (Wei et al.,
Our graph representation lends itself to optimization via
⋆EqualContribution♦ProjectEngineerLead promptingandevolutionaryorreinforcement-learningtech-
1
4202
beF
72
]IA.sc[
2v32861.2042:viXraLanguageAgentsasOptimizableGraphs
Main Features
Query
TOT
1 Operation = Node
Search
2 Agent = Graph of Nodes
3 Swarm = Composite Graph
Answer 1 2 3 B
A Collaboration and Communication = A C
Information Flow between Graphs
B Orchestration = TOT COT
Edge Connections in the Composite Graph
C Optimization =
Optimization of Nodes or Edges
GPTSwarm: Language Agents as Optimizable Graphs
Figure1.GPTSwarmisaframeworkthatrepresentsagentsasgraphs.Inthisframework,eachnoderepresentsanoperation(e.g.,LLM
inferenceortooluse).Anagentisagraphcomposedofthesenodes.Anedgebetweentwoagentgraphscharacterizesacommunication
channel;eachagentcollaborateswithothersthroughdifferentchannels.Whenconnected,multipleagentsformacompositegraphwitha
certainorchestrationtopology.Thisgraphrepresentationlendsitselftooptimizationofnodesandedgesviapromptingandevolutionary
orreinforcementlearningtechniques.
niques,sothatagentscanimprovetheircommunication(or 2.GPTSwarm
orchestration)patterns. Thegraphconnectivity(adjacency
2.1.LanguageAgentsasGraphs
matrices)betweenagentscanself-improveonlineasatask
isbeingsolvedoritssolutionistransferredtoanothertask. Takinginspirationfromthesocietyofmind(SOM)(Minsky,
1988; Zhuge et al., 2023), we propose to organize intelli-
As a proof-of-concept, we demonstrate how suboptimal
gencewithinamodularandhierarchicalframework. This
agent organization can be overcome and how existing
frameworkconsistsofnodes,graphs,andcompositegraphs,
promptingtechniques,suchasTreeofThoughtandReflex-
witheachcomponentplayingaspecificrole. Anoderep-
ion,canbeautomaticallyrecombinedbyoptimizingedges
resents a fundamental operation that includes, but is not
inacompositegraph. Apartfromedgeoptimization, our
limitedto,LLMinference,tooluse,functioncalls,andvari-
frameworkallowseachnodeinthegraphtoself-improve
ousembodiedactions. Anagent,conceptualizedasagraph,
byadaptingitspromptsbasedonpreviousinputandtask
consistsofmultiplenodesthatformacoherentfunctional
feedback.
entity. Aswarm,orcompositegraph,representsacomplex
Ourcontributionscanbesummarizedasfollows: system of agents where the collective capabilities of this
systemmayexceedthoseofindividualagents. Finally,the
(1)Weunifylanguageagentsystemsbydescribingthemas
edgeswithinanagentdefineitsexecutiontopology,while
optimizablecomputationalgraphs.
theedgesbetweenagentsestablishcollaborationandcom-
(2) We introduce an open-source framework that allows
municationamongthem.
for constructing arbitrary agent systems by recombining
fundamental operations. We describe these engineering-
2.2.GraphDefinition
levelcontributionsinAppendixA.
(3)Wedevelopoptimizationmethodsfornodesandedges, Singlelanguageagentasagraph. Wemodelalanguage
enabling automatic improvements of agent prompts and agent as a directed computational graph G, defined by a
inter-agentorchestration. tuple(N,E,F,o),whereN isasetofcomputationalnodes,
E ⊂N×N isasetofdirectededges,F ={f } isaset
(4)Wevalidateourframeworkonvariousbenchmarksin- n n∈N
ofcomputationalroutinesando∈N isanoutputnode. The
cludingMMLU,MiniCrossWords,HumanEval,andGAIA,
setofpredecessorsofnodenisdenotedbypre(n). Inthis
withanemphasisonthebenefitsofautomaticgraphopti-
paper,wefocusondirectedacyclicgraphs(DAGs). Given
mization.
aninputx,agraphGiterativelyexecutesitsnodesaccording
2LanguageAgentsasOptimizableGraphs
to their topological order. Each node n ∈ N receives as 2.3.EdgeOptimization
input x and the output z from its predecessor nodes. In
n Given a task τ and its associated utility function u that
thiswork,inputsandoutputsarestringsinnaturallanguage, τ
mapsthecandidategraphstorealnumbers,weformulate
butmaytakeonotherdatatypesmoregenerally. Noden
an optimization problem about the choice of additional
appliesthecomputationalroutinef (z ,x)andsendsthe
n n
edges. The goal is to identify the edges that connect var-
outputtoitssuccessornodes. Thegraphoutput, denoted
ious language agents in a swarm, maximizing the utility.
yˆ= G(x),istheoutputf (z ,x)fromtheoutputnodeo.
o o
This process involves determining the most effective pat-
NotethatinaDAG,somenodeswillnothavepredecessors.
ternsofcommunicationandinformationexchangeamong
For such nodes, the context z will be empty. This graph
agents for the task at hand. We consider a set of poten-
executionprocedureissummarizedinAlgorithm1.
tial edges {e }d = E, which leads to 2d possible edge
i i=1
configurations,symbolizedasE ∈ {0,1}d. Wefurtherre-
Algorithm1GraphExecution
strictthesearchspacetoonlyconsidercompositegraphs
Require: ComputationalgraphG=(N,E,F,o),inputx, that are DAGs. Formally, optimization of the composite
emptycontextzforeachnodewithoutpredecessors. graphoflanguageagentsisachievedbysolvingtheprob-
forninTopologicalSort(N)do lemmax u (G ).
E τ E
z ←{f (z ,x):v ∈pre(n)}
n n v
endfor 2.3.1.PROBLEMREFORMULATION
Ensure: f (z ,x)
o o
DAG optimization through pruning of nodes and edges
was already present in the first work on “deep learning”
withdeepfeedforwardnetworks(Ivakhnenkoetal.,1965;
In the context of language agents, for example, the input
Ivakhnenko,1968). Duetothecombinatorialcomplexity
xmaycorrespondtoaquestioninnaturallanguage. Each
inducedbyDAGs,recentstudieshaveincreasinglyfocused
nodeprocessestheinputxandcontextinformationzfrom
on the continuous optimization approach (Vowels et al.,
itspredecessornodesbyapplyingacomputationalroutinef.
2022). Thisisparticularlyrelevantinscenarioswheremost
ExamplesofroutinesincludeLLMquerieswithinputdata
nodeexecutionsrequireoneormorequeriestoLLMsfor
fromotheragents,instructionstogeneratepromptsforweb
moderate-scaleapplications. Moreover,theutilityfunction
searchesthatgathertask-relatedinformation,ortoolusage.
is typically non-differentiable due to the tokenization of
Althoughourformalizationspecifiesthattheinputxisgiven
LLMs, and this remains true even when a differentiable
toeachnode,inpractice,manyroutinesmightbedesigned
DAGsamplingtechniqueisemployed. Therefore,werefor-
toignoretheinputandoperatesolelyinthecontextprovided
mulateouredgeoptimizationasacontinuousoptimization
bythepredecessornodes.Finally,theoutputprovidedbythe
problem. Instead of optimizing in a discrete space, our
outputnodecorrespondstotheanswertotheinputquestion
approachistooptimizeoveracontinuumofprobabilistic
or,moregenerally,tothesolutionoftheinputtask.
distributions,eachrepresentingadistributionoverthefea-
sible DAGs. Formally, rather than solving the maximum
utilityfunctionargmax u (G ),weproposesolving
Swarmoflanguageagentsasacompositegraph. Given E τ E
a set of K language agents, each represented by a com- argmaxE [u (G′)], (1)
putational graph {G = (N ,E ,F ,o )}K , one can G′∼Dθ τ
k k k k k k=1 θ∈Θ
composetheseagentstoachievehighperformanceinspe-
whereD isaparameterizeddistributionandΘrepresents
cific tasks. Let N′ = ∪ N represent the union of the θ
k k
afeasiblesetofreal-valuedparameters.
nodesoftheagents,E′ =∪ E betheunionoftheedges
k k
of the agents, F′ = ∪ F be the union of the computa-
k k
tionalroutinesoftheagents,ando′ ∈∪ {o }betheoutput
2.3.2.SOLUTIONPARAMETERIZATION
k k
nodeforthecompositegraph. Consideraselectionofedges Astraightforwardwaytodefineaparameterizedprobabilis-
E ⊂∪ i̸=jN i×N jthatdescribeasetofconnectionsbetween ticdistributionoverDAGswithfixednodesN andrequired
nodesfromdifferentagents. Wedefinethecompositegraph edgesE istoassignareal-valuedparameterθ ∈Rtoeach
i
representingtheswarmofagentsasG E =(N′,E E,F′,o′), potentialedgee i. Letθ = [θ 1; θ 2; ...; θ d] ∈ [0,1]d. The
whereE E =E′∪E istheunionoftheedgesoftheagents probabilityofG′ =G E forG′ ∼D θ is
andthenewedgesconnectingthem. Compositegraphsare
restrictedtoDAGs. ThecompositegraphG
E
canbeexe- (cid:89)d (cid:40) θ
i
if(N,E∪({e j}i j− =1 1∩E)∪{e i})isaDAG,
cutedasdescribedinAlgorithm1. Inaswarmoflanguage 0 otherwise.
agents,thenewlyspecifiededgesrepresentcommunication i=1
channels between agents. In the following sections, we Asamplingmethodthatrealizesthisdistributionisfirstto
explorehowtooptimizesuchacomputationalgraph. initializeagraphG′ ← (N,E). Then, iterativelysample
3LanguageAgentsasOptimizableGraphs
whethertoincludeedgee inG′foralli’s. Ifincludinge alsorequireanaturallanguagedescriptionoftheintended
i i
causes a cycle in current G′, then the edge would not be functionforeachroutinef npn ∈F denotedbyd n. Forex-
included. Otherwise,addtheedgetoG′withprobabilityθ . ample,asuitabledescriptionforanodedesignedtowrite
i
Pythonprogramswouldbe“aPythoncodegenerator”.Here,
2.3.3.OPTIMIZATIONALGORITHM existingpromptoptimizationmethods,suchasOPRO(Yang
etal.,2023),canbedescribedasafunctionI thatiteratively
Tooptimizetheobjectivefunction(Equation(1)),weapply
theREINFORCEalgorithm(Williams,1992)byapplyinga maps a prompt, a function description, and a set of node
gradientascentvariant(e.g.,Adam(Kingma&Ba,2014)) input-outputpairs(whichmayincludeannotationssuchas
withanunbiasedgradientestimation: aqualitymeasureforeachpair)toanimprovedprompt. For
example,I couldtakeapromptsuchas“generatePython
M
1 (cid:88) code”, a description “a Python code generator”, and an
∇ E [u (G )]≈ uˆ (G )∇ log(p (G )), (2)
θ GE∼Dθ τ E M τ i θ θ i input-outputpair“Input: evaluatetwodividedbyoneasan
i=1
integer. Output: 2/1”,wheretheoutputyields1.0asthe
where G 1,G 2,...,G N ∼ D θ are mutually independent resultofexecution. Apromptoptimizationmethodwould
anduˆ τ(G i)isanindependentunbiasedestimateofu τ(G i) returnanimprovedprompt“generatePythoncodeandpay
for all i and some M ∈ N. Algorithm 2 describes the attentiontodatatypes”.
optimizationalgorithmwithvanillagradientascent.
Formally,ourmethodbeginsbyinitializinganemptyhis-
toryset,denotedh ,oneforeachnoden∈N. Theprocess
Algorithm2EdgeOptimizationwithREINFORCE n
thenproceedsiteratively: first,thegraphGP(x)isexecuted
Require: Aparameterizedprobabilisticdistributionover
using a randomly sampled input x following Algorithm
computation graphs D , an unbiased utility estimator
θ 1. Subsequently, for each node, a tuple consisting of the
uˆ (·),andalearningrateα.
τ input to the node (z ,x), where z is the context vector
Initializeθ ∈Rd. n n
thatincludestheoutputsofthepredecessornodes,andthe
whileterminateconditionnotmetdo
SampleG ∼D fori=1,2,...,M.
node’sownoutputf npn(z n,x), isaddedtothenode’shis-
Updateθ ←i θ+θ α (cid:80)M uˆ (G )∇ log(p (G )). toryh n. Thefinalstepinvolvesupdatingthenodeprompts.
M i=1 τ i θ θ i This is done by applying I to the node’s updated history,
endwhile
its current prompt, and its function description, resulting
in an improved prompt I(h ,p ,d ). This iterative pro-
n n n
cess,describedinAlgorithm3,continuouslyimprovesthe
2.4.NodeOptimization
operationsofthenodesintheentiregraph.
In our framework, each node implements a fundamental
operation,suchasqueryinganLLM,usingatool,calling Algorithm3NodeOptimization
anAPI,etc. Inalanguageagent,mostoftheseoperations Require: A parameterized graph GP = (N,E,FP,o),
involvepromptinganLLMonceorseveraltimes. Optimiz- natural language function descriptions D = {d } ,
n n∈N
ingthepromptsofthesenodesiscrucialforimprovingthe andadistributionofinputsD .
X
system’soverallperformance. Initializep foralln∈N.
n
Initializeh ←∅foralln∈N.
Unlikemanyotherworksonpromptoptimization,which n
whileterminateconditionnotmetdo
optimize a single global prompt (e.g., Yang et al., 2023;
Sampleinputx∼D .
Pryzantetal.,2023;Dengetal.,2022),ournodeoptimiza- X
y ←GP(x)followingAlgorithm1.
tion problem naturally involves several operations where
eachofthemconsistsofanode-levelprompt. Inourcase,
h
n
←h n∪{((z n,x),f npn(z n,x))}foralln∈N.
p ←I(h ,p ,d ),foralln∈N.
theoptimizationproblemismorecomplexduetoprompts n n n n
endwhile
affectinghowotherpromptsoperateonconnectednodes.
Atthesametime,ourgraphrepresentationleadstoasepa-
rationofconcernswhereeachnodehasaspecificpurpose 2.5.GeneralApplicability
withitsownassociatedprompt. Duetothisseparationof
Frameworks such as AutoGPT (Torantulino et al., 2023)
concerns,wehypothesizethat,foreveryoptimizationstep,
andLangChain(Chase,2022)havesetastandardforflexi-
itissufficienttoupdateeachnode-levelpromptindividually,
bilityandreusabilityinvariouslanguage-basedtasks. Our
assumingthatallotherpromptsarefixed.
framework,GPTSwarm,introducesagraph-baseddesignof
Consider a parameterized computational graph GP = agentsandswarms. Thisdesignfurthersimplifiesthereuse
(N,E,FP,o),whereFP ={fpn}arecomputationalrou- ofmodularcomponents(nodes&agents)andtheintegra-
n
tines,eachparameterizedbyapromptp tobeoptimized tionofsuchmodules. Forinstance, GPTSwarmsupports
n
foralln ∈ N. Toenableeffectivenodeoptimization,we 41typesoffileanalysis,websearch(e.g.,GoogleSearch),
4LanguageAgentsasOptimizableGraphs
Evaluation Scores
Evaluation Mode
0 2 4
Full graph 1T1A
Randomly-connected
Optimized
Baseline
3T3A
5T5A
6 8 10
7T7A
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8
Score
Figure2.Scorerecoverythroughedgeoptimization. “T”de-
notestruthfuland“A”adversarialagents,e.g.,a3T3Aswarmhas Figure3.Visualizingtheevolutionoftheprobabilitydistribu-
3ofeach. Ablationstudiesincludea“fullgraph”andrandom tionduringoptimizationinadjacency-likematrices. Inthis
graphssampledaccordingtodistributionD . Thedashedline figure,weshowtheprobabilityparameters(onecorrespondsto
0.5
correspondstothedirectanswerbaseline. anedge)inanadjacency-likematrixforiterations0,2,4,6,8,
and10ofoptimizingtheobjectivefortheMiniCrosswordstask.
Weobservethattheparametersfirstchangechaotically.However,
afteriteration6,theparameterschangealmostmonotonically.
andindex-basedmemory. Byofferingawiderangeofmod-
ules,ourframeworkmakesiteasiertoimplementvarious
tionprocessusesREINFORCE(Alg.2)over200iterations.
languageagentsystems. SeeSection3.4forfurtherdetails.
Eachiterationassessesfourgraphsamples,eachonaspe-
cificproblemsourcedfromtheMMLUdevset. Throughout
3.Experiments these experiments, we employed GPT-4-Turbo, with the
tokensamplingtemperaturefixedat0.2. Figure9demon-
3.1.MMLU
strateshowtheoptimizedswarmscorealignsasymptotically
Motivation: Inourfirstexperiments,wedemonstratethat withthatofthebaseline. Table3compilesthekeystatistics
and findings of these experiments. The findings indicate
edgeoptimizationeffectivelyfiltersadversarialagentsfrom
thatourapproachsuccessfullysafeguardsaswarmagainst
aswarm,mirroringascenarioinmulti-agentsystemswhere
harmfuladversaries.
someagentsaredetrimentalratherthanbeneficial. Ideally,
optimizationwouldautomaticallyeliminateharmfulagents.
Weconductedthisexperimentusingthe4-choiceMMLU 3.2.MiniCrosswords
generalknowledgequestionansweringdataset,asdetailed
Motivation: Thissectioninvestigatestowhatextentedge
byHendrycksetal.(2021b;a). Oursetupinvolvesinitializ-
optimization can improve the performance of standard
ingaswarmconsistingofkInput-Output(IO)agentsand
agentsfromthe literature. We conductourevaluationon
k adversarial agents, following the terminology by Besta
theMiniCrosswordsdataset1. Asubsetof20problemsis
etal.(2023). TheIOagentsqueryanLLMandrelaythe
usedtooptimizeandevaluateourmethods,inagreement
LLM’sresponsesdirectly. Incontrast, adversarialagents
with previous studies (Yao et al., 2023; Sel et al., 2023).
are deliberately programmed to manipulate the LLM to
ThechoiceofMiniCrosswordsforthisanalysisisstrate-
provideincorrectanswers. Thecollectivedecisiononthe
gic, as it highlights how the algorithmic structure of the
final answer is made through majority voting, bypassing
solvers,suchasthetreesearchemployedbyTOT,signifi-
anyadditionalLLMquerythatcouldintroducecorrective
cantlyinfluencestheirperformance(Yaoetal.,2023). Our
intelligenceagainstadversarialinfluence. Webenchmark
hypothesisisthatedgeconnectionscanmeaningfullydeter-
theperformanceofasingleIOagentasourbaseline. An
minethealgorithmicstructure. Throughedgeoptimization,
effective optimization is expected to elevate the swarm’s
weanticipatetheautomaticdiscoveryandimplementation
performanceontheMMLUdatasettomatchthisbaseline
ofhigh-performancealgorithms.
level.
Analysis: Inourexperiments,weexploretheperformance
Analysis: InFigure2,wepresentthecomparativeperfor-
ofswarmsofthreedistinctagents. Thefirstagent,which
mancescoresofdifferentswarmconfigurations: thebase-
implements the TOT approach, iteratively branches over
line,thegraphformedbysequentiallyincludingedgesthat
candidatesolutionsprovidedbyanLLM,processingone
donotcreateloops(denotedasthe‘fullgraph’),arandomly
wordateachstep. ThesecondagentisbasedontheReflex-
connectedswarmsampledfromtheinitialdistributionD
θ ionmethod(Shinnetal.,2023). Thisagentfirstproposesa
withθ = 0.5,andtheoptimizedswarm. Thesescoresare
solutionthroughagreedyapproachandthencreatesanal-
based on an evaluation of 10% of the MMLU validation
set, which consists of 153 questions. The edge optimiza- 1https://www.goobix.com/crosswords
5LanguageAgentsasOptimizableGraphs
ternativesolutioninformedbyfeedbackfromacritic,which
Mini Crosswords Performance
is based on an LLM analysis of the initial solution. The
Final Distribution with
thirdagentweexamineisaChainofThought(COT)agent GPT-4-Turbo
consistingofthreenodes. EachnodewithintheCOTper-
Final Distribution with
forms an internalbrute-force search to select theoptimal GPT-3.5-Turbo
subsetofcandidatesgeneratedbytheLLMforthecurrent 0.125 Parameter-Valued
Distribution
state,scoredbytheLLM.Theagentorswarmthenreturns
allthesolutionsgeneratedbytheiroutputnode. Initial Distribution
Fortheutilityfunction,wechoosethebestofallthegraph-
Best-of-Three
returned solutions according to the number of words cor- ToT (0.675)
rectlyfilled(i.e.,beststatewordaccuracy)asdonebyYao 0.0 0.2 0.4 0.6 0.8
Objective Value
et al. (2023). During the evaluation, we average over 20
graphsamplesfromthegraphdistribution,eachevaluated
Figure4.EdgeoptimizationontheMiniCrosswordsdataset
onauniquequestionrandomlysampledfromthedataset. improvesoverstandardmethodsThebaselinemethodsareeval-
uatedwithGPT-3.5-Turbo.Theoptimizedfinaldistributionoutper-
WeoptimizeourcompositegraphofagentsusingtheRE-
formsseveralbaselines. Whenevaluatingthealreadyoptimized
INFORCE(Alg. 2),settingtheinitialedgeprobabilityat
edgedistributionwithGPT-4-Turbo,weachievebetterresultscom-
θ = 10%andthelearningrateatα = 0.4. Foreachitera-
paredtothepreviousstate-of-the-artmethod(TreeofThought).
tion,thegradientisestimatedaccordingtoequation(2)by
samplingM = 20graphs,eachevaluatedonacrossword
model2. Itachievesanaccuracyof0.800(±0.0616),signifi-
problem. Forcost-effectiveness,weoptimizeandevaluate
cantlyexceedingthepreviousstate-of-the-artperformance
graphswiththeGPT-3.5-Turbolanguagemodel,wherethe
of0.675(Yaoetal.,2023). Allreportedmetricsaresumma-
temperature is set to zero. Figure 3 visualizes the evolu-
rizedinFigure4.
tionofprobabilityparametersintheformofadjacency-like
matricesoverteniterations. Weobservethattheparame-
3.3.HumanEval
tersfirstchangechaotically. However,afteriteration6,the
parameterschangealmostmonotonically. Motivation: InthepreviousexperimentsonMMLU(math
problems)andMiniCrosswords(open-endedpuzzles)we
We follow Alg. 2 to optimize the objective in Equation
havevalidatedtheutilityofoptimizinggraphedges. Inthis
1,achievinganaverageaccuracyof0.575(±0.0275)after
section,wetesttheHumanEvaldataset(Chenetal.,2021),
ten iterations (we report the average over 3 runs and the
which is known to be sensitive to prompt design. Previ-
standarderror).Thissurpassestheinitialdistribution’sscore
ous research involved manually crafting prompts (Shinn
of0.465(±0.0509). Furthermore,weevaluatethebest-of-
et al., 2023; Hong et al., 2023) and achieved impressive
threeperformancebyaggregatingthetopresultsfromeach
performance. Incontrast,here,weexplorehownode-based
problemacrossthethreeagents,whichyieldsanaccuracy
optimizationcansimplifythisprocess.Weemployanonline
of0.320(±0.0415).
learningsetting,continuouslyoptimizingwithoutrestarting.
Note that denser graphs are likely to require more com-
Analysis: In this section, we optimize the prompts of a
putational resources. To verify that the improvements of
ReAct-style(Yaoetal.,2022)agent. Theagentfirstgener-
our method are not solely due to an increase in the num-
atesaPythonprograminresponsetoagivenquestion. If
berofedgesandthereforealargercomputationalbudget,
thegeneratedprogrampassesalltestcasesincludedinthe
we compare it with a distribution with all parameters set
problemstatement,thentheprogramisreturned. Otherwise,
toθ = 12.5%. Thisvaluereflectstheaveragenumberof
the agent regenerates a program based on the execution
edgesinthelearneddistribution,determinedbysampling
feedback. Weexperimentwithtwonode-leveloptimization
1000graphsfromeachrun’sresultingdistribution. Theex-
strategies: (1) modifying the instruction prompts and (2)
pectednumberofedgesforboththelearneddistributionand
addingdemonstrationexamplestotheprompts.
the0.125parameter-valueddistributionareapproximately
29.71(±1.74)and29.68(±0.10),respectively. Despitethe Alteringtheinstructionpromptsrarelyimprovedtheresults,
similarityintheedgecount,the0.125parameter-valueddis- possibly due to the limited sophistication of our current
tributionachievesanaccuracyof0.510(±0.0552),allowing meta-promptscomparedtoOPRO(Yangetal.,2023)and
ustoattributetheimprovementstofactorsbeyondthemere PromptBreeder (Fernando et al., 2023). However, selec-
edgedensity. tivelyincorporatingpreviouslyexecutedinput-outputpairs,
Furthermore,weevaluateoneofourfinaloptimizeddistri- 2Wearelimitingourevaluationtoasinglegraphdistribution
butions(randomlyselected)withtheGPT-4-Turbolanguage due to the high cost associated with API calls for this type of
evaluation.
6LanguageAgentsasOptimizableGraphs
Usingthisbenchmark,weevaluatethegeneralapplicabil-
ityofourframework. Weconstructswarmswithmultiple
agents of the same type and employ self-consistency (a
prompt-basedmajorityvote)forthefinaldecision(Wang
etal.,2022). Wealsoexperimentedwithaddingdifferent
typesofagentstotheswarmandusingprompt-basedbest
answer selection. The results indicate that prompt-based
self-consistency yields the best performance. Note that
theseexperimentsaremeanttodemonstratethegenericca-
pabilities of our modular framework and include neither
edge-based nor node-level optimization, which is left for
futurework.
Figure5.Solvingawiderangeoftasksrequiresmanydifferent Analysis: Table 1 shows the results of our swarm with
tools.TheGAIAbenchmark(Mialonetal.,2023)testsformany seven TOT agents and the self-consistency strategy for
ofthesecapabilitiesbyincludingquestionsthatrequireseveralof the final decision. We compare the performance of the
thesetoolsforsuccessfulcompletion.
GPT-Series(Achiametal.,2023)withpluginsandAuto-
GPT(Torantulinoetal.,2023)performanceasreportedby
Table1.PerformanceontheGAIABenchmark(Mialonetal., Mialonetal.(2023). Ourmethodssignificantlyoutperform
2023).Usingourframework,wedemonstratesignificantimprove- thesebaselines.
mentsacrossseverallevelsofdifficulty.The‘GPT-4withplugins’
baselineislesssignificantsinceitinvolvesthemanualselection Table2presentsamorecomprehensivesetofresults.Weex-
of the appropriate tools per question. We report the mean and perimentwithvaryingnumbersofagentsanddifferentnode
standarddeviationacross5runs. operations, such as different tool uses. Our observations
Method Level1 Level2 Level3 Avg. indicate that the time requirement of a swarm grows ap-
GPT-3.5 7.55 4.65 0 4.85 proximatelylinearlywiththenumberofagents. Despitethe
GPT-4 15.09 2.33 0 6.06
increased computational time, incorporating more agents
GPT-4-Turbo 20.75 5.81 0 9.70
notablyimprovestheoverallperformanceofthesystem.We
AutoGPT 13.21 0 3.85 4.85
alsofoundthatagreatervarietyofnodeoperationsleadsto
GPTSwarm 30.56 20.93 3.85 18.45
±3.25 ±1.27 ±2.43 betterperformance.AsillustratedinFigure5,webbrowsing
Improvement 47.3%↑ 260.2%↑ 0.0% 90.2%↑
isrequiredfor43.9%ofthetasks. Ourcurrentimplemen-
GPT4withPlugins* 30.30 9.70 0 14.6
tationaccessestheInternetbyonlydownloadingmaterials
directlyfromtheURLsprovidedintheproblemstatementor
queryingaGooglesearch3 withoutfurtherwebsitenaviga-
whichcorrespondtosuccessfulprogramgenerationsofthe
tion. Therefore,webelievethatenhancingwebcapabilities
graph, as demonstration examples in the context of the
wouldfurtherincreaseperformancesignificantly.
nodes, increases the pass@1 accuracy from 77% to 89%.
Weselectinput-outputpairsbyassessingtheireffectiveness
4.RelatedWork
asdemonstrationexamples,particularlyinimprovingthe
nodeoperationappliedtothenode’stenmostrecentinputs.
4.1.LLM-basedAutonomousAgents
Toevaluateanodeoperation,wedetermineifthegenerated
programsuccessfullysolvestheunittestsprovidedinthe Current works on LLM-based autonomous agents or lan-
inputproblemstatement. Formoredetailsonthenodeop- guage agents vary in focus. Methods such as Chain of
timizer, please refer to Appendix D.3.1. We hypothesize Thought(Weietal.,2022), ReAct(Yaoetal.,2022), Re-
thattheperformancecouldbefurtherimprovedwithmore flexion(Shinnetal.,2023),andTreeofThought(ToT)im-
sophisticated(meta-)prompts. provepromptstrategiesandstructuretoimprovereasoning
capabilities. SingleLLMagentframeworkssuchasAuto-
3.4.GAIA GPT(Gravitas,2023),LangChain(Chase,2022),LlamaIn-
dex(Liu,2022),andXAgent(XAgentTeam,2023)show-
Motivation: GAIAisabenchmarkspecificallydesigned caseproblemsolvingthroughvariousexternalfunctionsand
fortestingthegeneralityofAIassistantsfocusingonreal- tools. InthespaceofLLM-basedmultiagentsystems(Xie
worldquestions(Mialonetal.,2023). Abilitiesrequiredto etal.,2023;Chenetal.,2023a;b),NLSOMs(Zhugeetal.,
answerGAIAquestionsincludereasoning,multi-modality 2023) employ various social structures for task-specific
processing, web browsing, and other tool use. Although applications(inspiredbySOMs(Minsky,1988)),without
conceptuallystraightforwardforhumans,thesequestions
presentsignificantchallengesforcurrentAIsystems.
3WeuseSearchAPI(https://serpapi.com)intheexperiments
7LanguageAgentsasOptimizableGraphs
Table2.AblationsontheGAIAbenchmark(Level1validationset)(Mialonetal.,2023).DA=DirectAnswer,GQ=GenerateQuery,
WS=WebSearch,FA=FileAnalyzer,CA=ComebineAnswer. ‘(cid:33)’ indicatesthepresenceofaspecificfeatureinthecorresponding
framework,‘(cid:37)’itsabsence.Eachtypeofexperimentisrunfivetimestorecordthemean,standarddeviation,andbestrun(markedas
Best).Self-Consistencydescribesprompt-basedself-consistency(Wangetal.,2022);Choose“Best”referstotheLLM’sfavoriteanswer
amongthedifferentagents’answers.AllagentsandswarmsareimplementedusingourGPTSwarmframework.
AgentorSwarm DA GQ WS FA CA DecisionStrategy Accuracy Best Duration(s)
(A)Agent:IO (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) N/A 16.60±3.02 20.75% ∼13.37
(B)Agent:COT (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) N/A 18.87±2.67 22.64% ∼60.90
web
(C)Agent:COT (cid:37) (cid:33) (cid:37) (cid:33) (cid:33) N/A 25.28±3.50 30.18% ∼56.42
FA
(D)Agent:TOT (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) N/A 25.66±3.50 30.18% ∼71.31
(E)Swarm (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) Choose“Best” 15.85±0.92 18.87% ∼45.65
(3×IO)
(F)Swarm (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) Choose“Best” 27.17±3.29 32.08% ∼152.89
(3×COT)
(G)Swarm (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) Choose“Best” 30.18±4.30 35.85% ∼198.50
(3×TOT)
(H)Swarm (cid:33) (cid:37) (cid:37) (cid:37) (cid:37) Self-Consistency 18.11±3.07 22.64% ∼45.70
(3×IO)
(I)Swarm (cid:37) (cid:33) (cid:33) (cid:37) (cid:33) Self-Consistency 27.17±4.06 32.08% ∼150.26
(3×COT)
(J)Swarm (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) Self-Consistency 28.30±3.38 32.08% ∼181.15
(3×TOT)
(K)Swarm (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) Self-Consistency 29.06±2.56 32.08% ∼291.07
(5×TOT)
(L)Swarm (cid:37) (cid:33) (cid:33) (cid:33) (cid:33) Self-Consistency 30.56±3.25 35.85% ∼414.89
(7×TOT)
(M)Human - - - - - - 94% - ∼422.26
exploringoptimizationoverthesocialstructureofagents. 4.3.OptimizingLLMInferenceandSelf-Improvement
CAMEL (Li et al., 2023), Generalist Agents (Park et al.,
Muchofdeeplearningresearchisconcernedwithtuning
2023),ChatDev(Qianetal.,2023),andAutoGen(Wuetal.,
the learning algorithms, architectures, hyper-parameters,
2023)focusonrole-playcommunication,butstrugglewith
and other aspects of the learning pipeline (Schmidhuber,
hallucinations. MetaGPT (Hong et al., 2023) introduces
2015;Yanetal.,2015). Meta-learningattemptstoautomate
standardoperatingproceduresforbetterroledefinitionand
largepartsofthatprocess(Schmidhuber,1987;Elskenetal.,
communication,makingthecollaborationbetweenagents
2019;Kirsch&Schmidhuber,2021). Similarly,recently,a
moreeffective.Incontrasttotheseframeworks,weautomat-
lotofresearchandengineeringhasgoneintotheprompting
icallyoptimizenodesandedgesinaself-organizingsociety
and structuring of LLM inference to make better use of
ofagents.
LLMsandbuildbetteragents. DuetotheabilityofLLMs
tolearnincontext(Brownetal.,2020;Kirschetal.,2022),
4.2.LanguageAgentswithGraphs
onecanviewthisprocessasconfiguringlearningalgorithms.
Bestaetal.(2023)introducedLLM-basedproblem-solving Theoptimizationoftheinferencestructureandtheprompts
withgraphs;however,theapproachonlyencompassesLLM canthenbeviewedasmeta-learninginLLMs.
prompting schemes without modeling other fundamental
In the realm of prompt optimization, OPRO (Yang et al.,
capabilitiesoflanguageagents,suchasuseofexternaltools.
2023) generates better prompts through iterative LLM
LangGraph (langchain ai, 2024), on the other hand, is a
queriesusingpriorsolutionsandtheirperformance.Prompt-
concurrentopen-sourceframeworkthatfocusesonbuilding
Breeder(Fernandoetal.,2023)implementsamechanism
multi-actorstateLLMapplicationsthroughpossiblycycli-
that evolves and self-improves task-specific and meta-
caloperations. However,itspracticalapplicabilityhasnot
promptsthroughmutationandLLMprompting. Relatedto
yet been systematically studied. Unlike previous studies,
theseworks,weself-improvefuturepromptsbyprompting
ourapproachemphasizesthedevelopmentofhierarchical
LLMs. Similarlytoourwork,DSPy(Khattabetal.,2023)
intelligence,asdiscussedbyMinsky(1988)andKennedy
implementsLLMpipelinesascomputationalgraphswith
(2006), through the construction of agent graphs and the
modularLLMqueriesasnodes,parameterizedbyprompts
compositionofmultiplegraphsintoswarms. Crucially,the
andneuralnetworkweights. Itproposesatwo-stageprocess
graphrepresentationfacilitatesautomaticoptimizationon
tooptimizetheparametersofthesenodes. Initially,itgener-
two levels. First, at the node level, since the majority of
atesasetofcandidatesolutionsforeachnode.Subsequently,
nodesinthegraphinvolvepromptinganLLM,promptop-
itoptimizesacrosstheCartesianproductofthesecandidate
timizationmethodscanbeemployed. Second,attheedge
solutionsets,aimingtoidentifyaneffectivecombinationof
level,wedemonstratetheapplicationoftheREINFORCE
parametersfortheentiregraph. Toaddressthecombinato-
algorithm(Williams,1992)tooptimizethepotentialcon-
rialoptimizationchallengeraisedinDSPy,weproposean
nectionsbetweennodes.
iterativeoptimizationprocess. Byvirtueofdecomposinga
8LanguageAgentsasOptimizableGraphs
solutionintonodeswithexpectedfunctions,ateachitera- coreideaswithMingchen,contributedtothecodebase,con-
tion,weimproveeachnodeindividually,conditionedonthe ductedMiniCrossWords&HumanEvalexperiments,and
executionhistoryofthegraphwiththecurrentpromptsof draftedtheinitialmanuscript. Louisreviewedandpolished
eachnode. the paper, extensively rewrote the introduction, and coor-
dinatedteammeetings. Francescoreviewedandpolished
Regardingtheoptimizationoftheinferencestructure,Dy-
thepaper,significantlyrevisingthemethodssection. Fur-
LAN (Liu et al., 2023) uses a fixed heuristic to improve
thermore, LouisandFrancescodiscussedandformalized
the collaboration of LLM agents by selecting agents and
various techniques for graph optimization. As the senior
determiningthenumberofcommunicationrounds. Inline
engineeringlead,Dmitriiadvisedandmadesignificantrevi-
withpreviousideasonself-referentiallearning(Schmidhu-
sionstothecodebase,conductedtheMMLUexperiments,
ber,1993;Irieetal.,2022;Kirsch&Schmidhuber,2022),
and contributed to the visualizations. Juergen, as mentor
STOP(Zelikmanetal.,2023)optimizesboththeprompts
andadvisor,offeredguidanceandsupportthroughoutthe
andtheinferencestructuretogetherbyintroducinganini-
project’sprogression.
tialimproverprogramthatisappliedtoitselftoiteratively
improve its performance. In our work, we optimize the
inferencestructurebyemployingRLtechniquesappliedto References
thepotentialedgesofagivengraph.
Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,
Aleman,F.L.,Almeida,D.,Altenschmidt,J.,Altman,S.,
5.Conclusion Anadkat,S.,etal. Gpt-4technicalreport. arXivpreprint
arXiv:2303.08774,2023.
ThispaperintroducesGPTSwarm,anopen-sourceframe-
workthatconstructslanguageagentsfromgraphsandagent Besta,M.,Blach,N.,Kubicek,A.,Gerstenberger,R.,Gi-
societiesfromgraphcompositions. Thisapproachallows aninazzi, L., Gajda, J., Lehmann, T., Podstawski, M.,
fortheeasyimplementationofexistingmethodsfrombasic Niewiadomski,H.,Nyczyk,P.,etal. Graphofthoughts:
nodeoperationsandenablesautomaticoptimizationofthe Solvingelaborateproblemswithlargelanguagemodels.
graphintheformofnode-levelimprovementandedge-level arXivpreprintarXiv:2308.09687,2023.
REINFORCEoptimization. Ourexperimentsdemonstrate
theadvantagesofourlanguageagentgraphsandautomatic Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,J.D.,
optimizationonseveralbenchmarks. Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell,A.,etal. Languagemodelsarefew-shotlearners.
Advancesinneuralinformationprocessingsystems,33:
ImpactStatement
1877–1901,2020.
Thispaperpresentsworkwhosegoalistoadvancethefield
Chase, H. LangChain. https://github.com/
of Machine Learning. The societal consequences of our
hwchase17/langchain,2022.
work are multifaceted. On the one hand, it could lead to
significantadvancementsintheefficiencyandeffectiveness
Chen, G., Dong, S., Shu, Y., Zhang, G., Sesay, J., Karls-
of machine learning systems. On the other hand, the in-
son, B. F., Fu, J., and Shi, Y. Autoagents: A frame-
creased capability and automation of LLM agents might
work for automatic agent generation. arXiv preprint
raise ethical and employment concerns. As AI systems
arXiv:2309.17288,2023a.
becomemoreautonomousandpowerful,itiscrucialtocon-
sidertheirimpactonjobdisplacementandtheimportance
Chen,M.,Tworek,J.,Jun,H.,Yuan,Q.,Pinto,H.P.d.O.,
ofimplementingsafeguardstopreventbiasedorunethical
Kaplan,J.,Edwards,H.,Burda,Y.,Joseph,N.,Brockman,
AIbehaviors. Furthermore,thepotentialformisuseofad-
G., etal. Evaluatinglargelanguagemodelstrainedon
vancedAItechnologiesrequiresrigorousoversightandthe
code. arXivpreprintarXiv:2107.03374,2021.
developmentofethicalguidelinestoensurethatthesetech-
nologiesareusedresponsiblyandforthebenefitofsociety Chen,W.,Su,Y.,Zuo,J.,Yang,C.,Yuan,C.,Qian,C.,Chan,
asawhole. C.-M.,Qin,Y.,Lu,Y.,Xie,R.,etal. Agentverse: Facili-
tatingmulti-agentcollaborationandexploringemergent
AuthorContributions behaviors in agents. arXiv preprint arXiv:2308.10848,
2023b.
Mingcheninitiatedtheprojectandconceivedtheinitialidea,
ledthedevelopmentofthecodebase,conductedGAIA& Deng,M.,Wang,J.,Hsieh,C.-P.,Wang,Y.,Guo,H.,Shu,T.,
HumanEvalexperiments,draftedtheinitialmanuscript,and Song,M.,Xing,E.P.,andHu,Z. Rlprompt: Optimizing
created most of the visualizations. Wenyi discussed the discretetextpromptswithreinforcementlearning. arXiv
preprintarXiv:2205.12548,2022.
9LanguageAgentsasOptimizableGraphs
Elsken,T.,Metzen,J.H.,andHutter,F. Neuralarchitecture Kirsch,L.andSchmidhuber,J. Eliminatingmetaoptimiza-
search: A survey. The Journal of Machine Learning tionthroughself-referentialmetalearning. arXivpreprint
Research,20(1):1997–2017,2019. arXiv:2212.14392andFirstConferenceonAutomated
MachineLearning(Workshop),2022.
Fernando, C., Banarse, D., Michalewski, H., Osindero,
S.,andRockta¨schel,T. Promptbreeder: Self-referential
Kirsch, L., Harrison, J., Sohl-Dickstein, J., and Metz, L.
self-improvementviapromptevolution. arXivpreprint
General-purpose in-context learning by meta-learning
arXiv:2309.16797,2023.
transformers. arXivpreprintarXiv:2212.04458,2022.
Gravitas,S. Auto-gpt. GitHubrepository,2023.
Kojima,T.,Gu,S.S.,Reid,M.,Matsuo,Y.,andIwasawa,
Hendrycks,D.,Burns,C.,Basart,S.,Critch,A.,Li,J.,Song, Y. Largelanguagemodelsarezero-shotreasoners. Ad-
D., and Steinhardt, J. Aligning ai with shared human vances in neural information processing systems, 35:
values. ProceedingsoftheInternationalConferenceon 22199–22213,2022.
LearningRepresentations(ICLR),2021a.
langchain ai. LangGraph. https://github.com/
Hendrycks,D.,Burns,C.,Basart,S.,Zou,A.,Mazeika,M., langchain-ai/langgraph,2024.
Song,D.,andSteinhardt,J. Measuringmassivemultitask
languageunderstanding.ProceedingsoftheInternational Li,G.,Hammoud,H.A.A.K.,Itani,H.,Khizbullin,D.,and
ConferenceonLearningRepresentations(ICLR),2021b. Ghanem,B. Camel: Communicativeagentsfor”mind”
explorationoflargescalelanguagemodelsociety. arXiv
Hong,S.,Zheng,X.,Chen,J.,Cheng,Y.,Zhang,C.,Wang,
preprintarXiv:2303.17760,2023.
Z.,Yau,S.K.S.,Lin,Z.,Zhou,L.,Ran,C.,etal.Metagpt:
Metaprogrammingformulti-agentcollaborativeframe- Liu,J. LlamaIndex,112022. URLhttps://github.
work. arXivpreprintarXiv:2308.00352,2023. com/jerryjliu/llama_index.
Irie,K.,Schlag,I.,Csorda´s,R.,andSchmidhuber,J.Amod-
Liu, Z., Zhang, Y., Li, P., Liu, Y., and Yang, D. Dy-
ernself-referentialweightmatrixthatlearnstomodify
namic llm-agent network: An llm-agent collaboration
itself. InInternationalConferenceonMachineLearning,
frameworkwithagentteamoptimization. arXivpreprint
pp.9660–9677.PMLR,2022.
arXiv:2310.02170,2023.
Ivakhnenko, A., Lapa, V., and ENGINEERING., P. U. L.
I. S. O. E. Cybernetic Predicting Devices. JPRS 37, Mialon,G.,Fourrier,C.,Swift,C.,Wolf,T.,LeCun,Y.,and
803.JointPublicationsResearchService[availablefrom Scialom,T. Gaia: abenchmarkforgeneralaiassistants.
the Clearinghouse for Federal Scientific and Technical arXivpreprintarXiv:2311.12983,2023.
Information],1965. URLhttps://books.google.
com.sa/books?id=l38DHQAACAAJ. Minsky,M. Societyofmind. SimonandSchuster,1988.
Ivakhnenko,A.G. Thegroupmethodofdatahandling,a Nakajima, Y. Babyagi. Python. https://github.
rivalofthemethodofstochasticapproximation. Soviet com/yoheinakajima/babyagi,2023.
AutomaticControl,13(3):43–55,1968.
Nepusz, T. and Vicsek, T. Hierarchical self-organization
Kennedy,J. Swarmintelligence. InHandbookofnature- ofnon-cooperatingindividuals. Plosone,8(12):e81449,
inspired and innovative computing: integrating classi- 2013.
cal models with emerging technologies, pp. 187–219.
Springer,2006. Park, J. S., O’Brien, J., Cai, C. J., Morris, M. R., Liang,
P.,andBernstein,M.S. Generativeagents: Interactive
Khattab,O.,Singhvi,A.,Maheshwari,P.,Zhang,Z.,San-
simulacraofhumanbehavior. InProceedingsofthe36th
thanam,K.,Vardhamanan,S.,Haq,S.,Sharma,A.,Joshi,
AnnualACMSymposiumonUserInterfaceSoftwareand
T.T.,Moazam,H.,etal. Dspy: Compilingdeclarative
Technology,pp.1–22,2023.
languagemodelcallsintoself-improvingpipelines.arXiv
preprintarXiv:2310.03714,2023.
Pryzant,R.,Iter,D.,Li,J.,Lee,Y.T.,Zhu,C.,andZeng,M.
Automaticpromptoptimizationwith”gradientdescent”
Kingma,D.P.andBa,J. Adam: Amethodforstochastic
optimization. arXivpreprintarXiv:1412.6980,2014.
andbeamsearch.arXivpreprintarXiv:2305.03495,2023.
Kirsch,L.andSchmidhuber,J. Metalearningbackpropaga- Qian, C., Cong, X., Yang, C., Chen, W., Su, Y., Xu, J.,
tionandimprovingit. AdvancesinNeuralInformation Liu,Z.,andSun,M. Communicativeagentsforsoftware
ProcessingSystems,34:14122–14134,2021. development. arXivpreprintarXiv:2307.07924,2023.
10LanguageAgentsasOptimizableGraphs
Schmidhuber,J. Evolutionaryprinciplesinself-referential Xi, Z., Chen, W., Guo, X., He, W., Ding, Y., Hong, B.,
learning, or on learning how to learn: the meta-meta- Zhang,M.,Wang,J.,Jin,S.,Zhou,E.,etal. Theriseand
...hook. PhD thesis, TechnischeUniversita¨tMu¨nchen, potentialoflargelanguagemodelbasedagents: Asurvey.
1987. arXivpreprintarXiv:2309.07864,2023.
Schmidhuber, J. A ‘self-referential’weight matrix. In Xie,T.,Zhou,F.,Cheng,Z.,Shi,P.,Weng,L.,Liu,Y.,Hua,
ICANN’93: ProceedingsoftheInternationalConference T.J., Zhao, J., Liu, Q., Liu, C., etal. Openagents: An
on Artificial Neural Networks Amsterdam, The Nether- open platform for language agents in the wild. arXiv
lands13–16September19933,pp.446–450.Springer, preprintarXiv:2310.10634,2023.
1993.
Yan, L. C., Yoshua, B., and Geoffrey, H. Deep learning.
Schmidhuber, J. Deep learning in neural networks: An nature,521(7553):436–444,2015.
overview. Neuralnetworks,61:85–117,2015.
Yang,C.,Wang,X.,Lu,Y.,Liu,H.,Le,Q.V.,Zhou,D.,and
Sel, B., Al-Tawaha, A., Khattar, V., Wang, L., Jia, R., Chen,X. Largelanguagemodelsasoptimizers. arXiv
and Jin, M. Algorithm of thoughts: Enhancing explo- preprintarXiv:2309.03409,2023.
rationofideasinlargelanguagemodels. arXivpreprint
Yao,S.,Zhao,J.,Yu,D.,Du,N.,Shafran,I.,Narasimhan,
arXiv:2308.10379,2023.
K.,andCao,Y. React: Synergizingreasoningandacting
Shinn, N., Cassano, F., Labash, B., Gopinath, A., inlanguagemodels. arXivpreprintarXiv:2210.03629,
Narasimhan, K., and Yao, S. Reflexion: Language 2022.
agentswithverbalreinforcementlearning. arXivpreprint
Yao,S.,Yu,D.,Zhao,J.,Shafran,I.,Griffiths,T.L.,Cao,
arXiv:2303.11366,14,2023.
Y.,andNarasimhan,K.R. Treeofthoughts: Deliberate
Torantulino et al. Auto-gpt. https://github.com/ problemsolvingwithlargelanguagemodels. InThirty-
Significant-Gravitas/Auto-GPT,2023. seventh Conference on Neural Information Processing
Systems,2023. URLhttps://openreview.net/
Vowels,M.J.,Camgoz,N.C.,andBowden,R. D’yalike forum?id=5Xc1ecxO1h.
dags? asurveyonstructurelearningandcausaldiscovery.
ACMComputingSurveys,55(4):1–36,2022. Zelikman,E.,Lorch,E.,Mackey,L.,andKalai,A.T. Self-
taughtoptimizer(stop): Recursivelyself-improvingcode
Wang,L.,Ma,C.,Feng,X.,Zhang,Z.,Yang,H.,Zhang,J., generation. arXivpreprintarXiv:2310.02304,2023.
Chen,Z.,Tang,J.,Chen,X.,Lin,Y.,etal. Asurveyon
largelanguagemodelbasedautonomousagents. arXiv Zeng,A.,Attarian,M.,Ichter,B.,Choromanski,K.,Wong,
preprintarXiv:2308.11432,2023. A.,Welker,S.,Tombari,F.,Purohit,A.,Ryoo,M.,Sind-
hwani,V.,etal. Socraticmodels: Composingzero-shot
Wang,X.,Wei,J.,Schuurmans,D.,Le,Q.,Chi,E.,Narang, multimodal reasoning with language. arXiv preprint
S., Chowdhery, A., and Zhou, D. Self-consistency im- arXiv:2204.00598,2022.
proves chain of thought reasoning in language models.
arXivpreprintarXiv:2203.11171,2022. Zhuge, M., Liu, H., Faccio, F., Ashley, D. R., Csorda´s,
R., Gopalakrishnan, A., Hamdi, A., Hammoud, H. A.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., A.K.,Herrmann,V.,Irie,K.,etal. Mindstormsinnat-
Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought ural language-based societies of mind. arXiv preprint
prompting elicits reasoning in large language models. arXiv:2305.17066,2023.
AdvancesinNeuralInformationProcessingSystems,35:
24824–24837,2022.
Williams,R.J. Simplestatisticalgradient-followingalgo-
rithmsforconnectionistreinforcementlearning. Machine
learning,8:229–256,1992.
Wu,Q.,Bansal,G.,Zhang,J.,Wu,Y.,Zhang,S.,Zhu,E.,Li,
B.,Jiang,L.,Zhang,X.,andWang,C.Autogen:Enabling
next-gen llm applications via multi-agent conversation
framework. arXivpreprintarXiv:2308.08155,2023.
XAgentTeam. Xagent: Anautonomousagentforcomplex
tasksolving,2023.
11LanguageAgentsasOptimizableGraphs
A.TheGPTSwarmFramework
A.1.TheVision
Manyrecentlanguageagentsaredescribedascompositionsofcomponentsofdifferentfunctionalities(Wangetal.,2023).
A popular tweet states: “agent = LLM + memory + planning skills + tool use.”4 Such additive formulations highlight
individualcomponents,butfailtoaddresstheessentialaspectofcomponentintegration. GPTswarm’scomputationalgraph
formulation,however,preciselyfocusesonintegrationthroughedgeoptimization,tolearnimprovedrecommendationsof
agentorchestrationandpreciseagentrouting. Thiswillbecomeincreasinglyrelevantasswarmsizeincreasestomillionsor
billionsofagents.
A.2.ClassDiagram
TheGPTSwarmframeworkisdevelopedusingPythonandPyTorch. ItsclassdiagramisillustratedinFigure6.
Onthegraphlevel,Node,Graph,andCompositeGrapharedirectlyimplementedasclasses. Graphedgesareimplicitly
storedasanadjacencylistwithineachNode. Functionally,theframeworkdistinguishesbetweenAgentsandOperations
throughvariousclasses,suchasDirectAnswerandWebSearchforoperations,andIOandTOTforagents. Toencapsulate
theabstractionofanexternalLLM,weintroduceaninterfacenamedafterit. Theprimaryimplementationofthisinterfaceis
alightweightwrapperaroundtheOpenAIAPI.Tofacilitatedatasetintegrationforoptimizationandevaluation,weprovide
implementationsfortwointerfaces: DatasetandPromptSet. TheDatasetinterfaceisdesignedtoloadbenchmarkdatasets
likeGAIAandMMLU,whilethePromptSetcustomizesnodebehaviorforaspecificDataset. TheEvaluatorclassmanages
theoptimizationprocessesforbothedgesandnodes.
Theframeworkishighlycustomizable,allowinguserstoaddmoreLLMbackends,DatasetandPromptSetcombinations,
Agents,andNodesasneeded. Additionally,theframeworkmakesextensiveuseofasynchronouscomputationsfortask
parallelism,leveragingPython’sasync-awaitsyntax.
4https://twitter.com/lilianweng/status/1673535600690102273
12LanguageAgentsasOptimizableGraphs
Figure6. ThecLassdiagramoftheGPTSwarmframework.
13LanguageAgentsasOptimizableGraphs
B.Swarmexamples
Tofacilitateunderstandingoftheconceptspresentedinthisstudy,weareshowingasimpleexampleofaswarmconsisting
of3agents: Tree-of-Thought,Input-Output,andDecisionAgentsinFigure7.
Figure7. AsimpleexampleofaswarmconsistingofoneTree-of-Thought,oneInput-Output,andtheDecisionagent.
14LanguageAgentsasOptimizableGraphs
C.MoreVisualizations
Agent: GAIA_WebCOT
Swarm: MMLU_5T5A Swarm: MMLU_3COT
Swarm: CrossWords_COT_TOT_Reflexion_1
Agent: GAIA_FileCOT
Agent: HumanEval_ReAct
Agent: GAIA_IO
Swarm: CrossWords_COT_TOT_Reflexion_2
Agent: GAIA_TOT
Swarm: CrossWords_COT_TOT_Reflexion_3
Swarm: GAIA_3WebCOT
Swarm: GAIA_3FileCOT
Swarm: GAIA_3TOT Swarm: GAIA_5TOT Swarm: GAIA_7TOT
Figure8. DifferentagentsorswarmsimplementedbyGPTSwarm.
15LanguageAgentsasOptimizableGraphs
D.ExperimentalDetails
Inourexperimentsinvolvingmultipleagents,weincorporateanadditionalvirtualagent,representedbyasinglenode,to
serveasafinaldecisionaggregator. Thisnodeisdesignatedastheoutputnodeforthecompositegraph,anditsspecific
implementationvariesbetweendifferentexperiments. Commonimplementationsforthisnodeincludeemployingamajority
voteandaself-consistencystrategyfordecision-making. Unlessexplicitlystated,communicationbetweenagentswithin
a composite graph does not include this virtual agent. Additionally, in all our experiments, the potential edge set of a
compositegraphisdefinedasallpossiblenodepairs,providedthatthenodesineachpairoriginatefromdifferentagents.
Weexcludeanyedgesthatwouldconnecttheoutputnodeofacompositegraphtoothernodes. Moreover, weemploy
theAdam(Kingma&Ba,2014)optimizerwithparametersβ = 0.9,β = 0.999andavariablelearningrateinplace
1 2
of the vanilla stochastic gradient accent method described in Alg. 2. Finally, we use the version gpt-4-1106-preview
andgpt-3.5-turbo-1106forLLMs. Forthevision-languagemodelutilizedintheGAIAexperiments,weemployedthe
gpt-4-1106-vision-previewversionofGPT-4-Turbo.
D.1.MMLU
StatisticalinformationonadversarialrubustnessexperimentsiscollectedinTable3. Theconvergenceofthetrainutility
withthebaselineforthe3T3AexperimentisshowninFigure9.
Table3.Statsfortheadversarialexperiments. #Nodesmeansthenumberofnodesintheswarmexcludingthefinaldecisionnode.
#Potentialedgesisthetotalnumberofedgesthatareoptimizedandpotentiallyrealized.Theoptimizationtimeismeasuredasthewall
clocktime.#LLMinferencesisthetotalnumberofLLMqueriesmadeduringtheoptimizationcyclewhengraphpruningisturnedoff.
Swarmconfiguration #Nodes #Potentialedges Optimizationtime,mins
1TrustfulAgent+1AdversarialAgent 2 4 9
3TrustfulAgents+3AdversarialAgents 6 36 23
5TrustfulAgents+5AdversarialAgents 10 100 58
7TrustfulAgents+7AdversarialAgents 14 196 95
0.75
0.70
0.65
0.60
0.55
Mean score
0.50 1-sigma range
Direct answer baseline (dev split)
0 25 50 75 100 125 150 175 200
Iteration
Figure9.Thetrainingscoreduringtheoptimizationoftheadversarialswarm(3T3A)onMMLU.Weapplysmoothingwithan
unbiasedexponentialmovingaverageandthesmoothnessfactorof0.97.
D.1.1.HYPER-PARAMETERS&PROMPTS
WeusetheAdamoptimizerwithalearningrateof0.1toupdatethelogitparametersassociatedwitheachpotentialedge.
ThepromptsthathavebeenusedfortheadversarialrobustnessexperimentsarecollectedinTable4.
16
erocs
niarTLanguageAgentsasOptimizableGraphs
Table4. PromptsfortheAdversarialexperimentsonMMLU.
Promptpurpose Prompt
Systemprompt Youareaknowledgeableexpertinquestionanswering.Iwillaskyouaquestion.Iwillalsogive
you4answersenumeratedasA,B,CandD.Onlyoneansweroutoftheoffered4iscorrect.
Youmustchoosethecorrectanswertothequestion.Yourresponsemustbeoneofthe4letters:
A,B,CorD,correspondingtothecorrectanswer.Onlyoneletter(A,B,CorD)isallowedin
youranswer.
Directanswertemplate {question}
Adversarial answer tem- Answeralietothefollowingquestion:{question}.
plate
Questiontemplate {open-endedquestion}.OptionA:{optionA},OptionB:{optionB},OptionC:{optionC},
OptionD:{optionD}.
D.1.2.ADVERSARIALSWARMOPTIMIZATIONCASESTUDY
Anexampleofaswarmwith2truthfuland2adversarialexamplesisshowninFigure10. Figure10ashowsallpotential
edgesbeforeoptimization. Figure10bshowsonlytheedgesthatwereconnectedafteroptimizationwascomplete. Notethat
thedisconnectedagentsandedgesarepruned.
(a)Annon-optimizedswarmwith2truthfulagents
and 2 adversarial agents. Dotted arrows depict
potentialedges.
(b)Anoptimizedswarmwithrealizededges.
Figure10. A2T2Aswarmbefore(a)andafter(b)optimization
17LanguageAgentsasOptimizableGraphs
D.2.MiniCrosswords
D.2.1.AGENTSSETTING
InourMiniCrosswordsexperiments, eachnodereturnsoneortwosolutions—eitherupdatedorunchanged—foreach
receivedsolution. Thesolutionsproducedbyanodeareconditionallyindependentofeachother,giventheinputsolutions
ofthenode. Theoutputnodeofacompositegraphforwardsallreceivedsolutionswithoutalteration. Toensureintegration
withinthesystem,wemandatetheexistenceofedgesfromanyagent’soutputnodedirectlytothecompositegraph’soutput
node.
OurTOTagentusesatree-searchstrategyacrossaperfectbinarytreewithadepthofeight. Insteadofconstructingagraph
of29−1nodestorepresentthistree,thesearchiscarriedoutthroughachainofeightbranchingnodes. Eachbranching
node is designed to generate two solutions from every input solution that it processes, effectively embodying the TOT
strategy.
D.2.2.HYPER-PARAMETERS&PROMPTS
ThecandidatewordgenerationpromptandthepruningpromptareadaptedfromtheoriginalTOTwork(Yaoetal.,2023)
anddetailedinTable5. Aclueisdefinedasapartialfillingofthecrossword,accompaniedbyitsintendedworddescription
andspecificpositionontheboard.
D.3.HumanEval
D.3.1.THENODEOPTIMIZATIONMETHOD
Fornodeoptimization,weupdateeachnodeaftereveryfournewproblemexecutions. Whenaddressinganewproblemq
withgraphG,executingG(q)producesaprogram,denotedbys,whoseeffectivenessisassessedagainsttestexamples
associatedwithq. Theinput-outputpairsofthenodesgeneratedduringtheevaluationofG(q)areclassifiedaspositiveif
spassesthetestsandasnegativeotherwise. Welimiteachnodetoincludeamaximumoffourdemonstrationexamples.
Letnbeanodeinthegraphassociatedwithacomputationalroutinefp,whichreturnsPythonprograms,parameterized
n
bydemonstrationexamplesp. Duringanoptimizationstepofn,thatis,anapplicationofI asdescribedinSection2.4,
weassesswhethertoretainexistingdemonstrationexamples(p1)ortoaugmentthemwithpositiveexamplesfromthe
n
fourmostrecentproblems,subsequentlyrandomlyselectinguptofouruniqueexamplesfromthispool(denotedasp2).
n
Morespecifically,letZ bethesetofthelastteninputsofnodenreceivedwhensolvingthefirst-seenproblems. Weselect
pi ntoupdatethedemonstrationexamplesofn,wherei=argmax i∈{1,2}(cid:80) z∈Z1 z(f npi n(z,q z)),1
z
determineswhethera
programpassestheunittestsstatedinz,andq istheoriginalgraphinputassociatedwithz.
z
TheutilitymeasureforMiniCrosswordsexperimentsisdefinedasthebeststatewordaccuracy, asdetailedinSection
3.2. ToreducethevarianceingradientestimationwiththeREINFORCEalgorithm,weadjusttheutilitybysubtractinga
constantof0.4. Forexample,aperfectlycompletedsolutionresultsinautilityof0.6,whileanemptysolutionyieldsa
utilityof−0.4.
D.3.2.HYPER-PARAMETERS&PROMPTS
Table6showsthepromptsusedinourexperimentsfollowingtheprincipleofReAct(Yaoetal.,2022).
D.4.GAIA
D.4.1.AGENTSETTING
Wedesigndifferentagentsandswarms. RepresentativeagentsandswarmsarevisualizedinFigure8.
D.4.2.HYPER-PARAMETERS&PROMPTS
WeuseGPT-4-TurbofortheexperimentsanddesigndifferentnodeoperationstosolvetheGAIAtasks. Table7andTable8
showthepromptsusedinourexperiments.
18LanguageAgentsasOptimizableGraphs
Table5. PromptsfortheMiniCrosswordsExperiments.
Promptpurpose Prompt
Candidate words Let’splaya5x5minicrossword,whereeachwordshouldhaveexactly5letters.
generationprompt {currentboardstatus}
Unfilled:
{Unfilledclues}
Filled:
{filledclues}
Changed:
{Changedclues}
Suggestions:
{suggestionsgeneratedbypreviousReflectionnodes}
Giventhecurrentstatus,listallpossibleanswersforunfilledorchangedwords,andyourconfidencelevels(cer-
tain/high/medium/low),usingtheformat”h1.apple(medium)”.Use”certain”cautiouslyandonlywhenyouare
100%surethisisthecorrectword.Youcanlistmorethenonepossibleanswerforeachword.
Pruningprompt Evaluateifthereexistsafiveletterwordofsomemeaningthatfitsomeletterconstraints(sure/maybe/impossible).
Incorrect;toinjure:w o g
Theletterconstraintis:5letters,letter1isw,letter3iso,letter5isg.
Somepossiblewordsthatmean”Incorrect;toinjure”:
wrong(wrong):5letters,letter1isw,letter3iso,letter5isg.fit!
sure
Apersonwithanall-consumingenthusiasm,suchasforcomputersoranime: u
Theletterconstraintis:5letters,letter5isu.
Somepossiblewordsthatmean”Apersonwithanall-consumingenthusiasm,suchasforcomputersoranime”:
geek(geek):4letters,not5
otaku(otaku):5letters,letter5isu
sure
Dewy;roscid:r l
Theletterconstraintis:5letters,letter1isr,letter5isl.
Somepossiblewordsthatmean”Dewy;roscid”:
moist(moist):5letters,letter1ism,notr
humid(humid):5letters,letter1ish,notr
Icannotthinkofanywordsnow.Only2lettersareconstrained,itisstilllikely
maybe
Awoodland: l deTheletterconstraintis:5letters,letter2isl,letter4isd,letter5ise.
Somepossiblewordsthatmean”Awoodland”:
forest(forest):6letters,not5woods(woods):5letters,letter2iso,notl
grove(grove):5letters,letter2isr,notlIcannotthinkofanywordsnow.3lettersareconstrained,and l de
seemsacommonpattern
maybe
Aninn: d wf
Theletterconstraintis:5letters,letter2isd,letter4isw,letter5isf.
Somepossiblewordsthatmean”Aninn”:hotel(hotel):5letters,letter2iso,notd
lodge(lodge):5letters,letter2iso,notd
Icannotthinkofanywordsnow.3lettersareconstrained,anditisextremelyunlikelytohaveawordwithpattern d
wftomean”Aninn”
impossible
Chance;aparasiticworm;afish:wrak
Theletterconstraintis:5letters,letter1isw,letter2isr,letter3isa,letter4isk.
Somepossiblewordsthatmean”Chance;aparasiticworm;afish”:
fluke(fluke):5letters,letter1isf,notw
Icannotthinkofanywordsnow.4lettersareconstrained,anditisextremelyunlikelytohaveawordwithpatternwr
ak tomean”Chance;aparasiticworm;afish”
impossible
{clue}
SuggestionPrompt Youareplayinga5x5minicrossword,whereeachwordshouldhaveexactly5letters. Giventhecurrentstatus:
{currentboardstatus}
ThetargetwordsareclassifiedasImpossibleWords,CorrectWords,andIncorrectWords.
---
ImpossibleWords:
{impossibleclues}
CorrectWords:
{correctclues}
IncorrectWords:
{incorrectclues}
Respondatmostfivesentences,onesentenceperline.Donotincludethephrase”nexttime”inyourresponse.
19LanguageAgentsasOptimizableGraphs
Table6. PromptsfortheNodeOptimizationexperimentsonHumanEval.
Promptpurpose Prompt
Systemprompt YouareanAIthatonlyrespondswithonlyPythoncode.
CodeWriting Youwillbegivenafunctionsignatureanditsdocstringbytheuser.Writeyourfullimplemen-
tation(restatethefunctionsignature). UseaPythoncodeblocktowriteyourresponse. For
example:“‘pythonprint(‘Helloworld!’)”’
{Demonstrations}
{problemstatement}
CodeWriting(ReAct) Youwillbegivenafunctionsignatureanditsdocstringbytheuser.Writeyourfullimplemen-
tation(restatethefunctionsignature). UseaPythoncodeblocktowriteyourresponse. For
example:“‘pythonprint(‘Helloworld!’)”’
{Demonstrations}
Hereisanunsuccessfulattempttosolvethefollowingquestion:
Question:
{problemstatement}
AttemptedSolution:
{previouslygeneratedprogram}
Feedback:
{internalunittestresults}
Rewritethecodebasedonthefeedbackandthefollowingquestion:
{problemstatement}
20LanguageAgentsasOptimizableGraphs
Table7. PromptsfortheTask-SolvingexperimentsonGAIA(1).
Promptpurpose Prompt
Systemprompt YouareageneralAIassistant.Iwillaskyouaquestion.Reportyourthoughts,andfinishyour
answerwiththefollowingtemplate: FINALANSWER:[YOURFINALANSWER].YOUR
FINALANSWERshouldbeanumberORasfewwordsaspossibleORacommaseparated
listofnumbersand/orstrings. Ifyouareaskedforanumber,don’tusecommatowriteyour
numberneitheruseunitssuchas$orpercentsignunlessspecifiedotherwise.Ifyouareasked
forastring,don’tusearticles,neitherabbreviations(e.g.forcities),andwritethedigitsinplain
textunlessspecifiedotherwise. Ifyouareaskedforacommaseparatedlist,applytheabove
rulesdependingofwhethertheelementtobeputinthelistisanumberorastring.
DirectAnswer {question}
GenerateQuery #InformationGatheringforQuestionResolution
Evaluateifadditionalinformationisneededtoanswerthequestion.
Ifawebsearchorfileanalysisisnecessary,outlinespecificcluesordetailstobesearchedfor.
##TargetQuestion:
question
##CluesforInvestigation:
Identifycriticalcluesandconceptswithinthequestionthatareessentialforfindingtheanswer.
WebSearch #WebSearchTask
##OriginalQuestion:
—
{question}
—
##TargetedSearchObjective:
—
query
—
##SimplifiedSearchInstructions:
Generatethreespecificsearchqueriesdirectlyrelatedtotheoriginalquestion.Eachqueryshould
focusonkeytermsfromthequestion.Formattheoutputasacomma-separatedlist.Forexample,
ifthequestionis’WhowillbethenextUSpresident?’,yourqueriescouldbe:’USpresidential
candidates,currentUSpresident,nextUSpresident’.Remembertoformatthequeriesas’query1,
query2,query3’.
DistillWebSearch ##RequiredInformationforSummary:
—
{query}
—
##AnalyzedSearchResults:
—
{results}
—
##InstructionsforSummarization:
1.Reviewtheprovidedsearchresultsandidentifythemostrelevantinformationrelatedtothe
questionandquery.
2.Extractandhighlightthekeyfindings,facts,ordatapointsfromtheseresults.
3.Organizethesummarizedinformationinacoherentandlogicalmanner.
4.Ensurethesummaryisconciseanddirectlyaddressesthequery,avoidingextraneousdetails.
5.Iftheinformationfromwebsearchisuseless,directlyanswer:N¨ousefulinformationfrom
WebSearch.
¨
FileAnalyse #FileAnalysisTask
##InformationExtractionObjective:
—
{query}
—
##FileUnderAnalysis
—
{file}
—
##Instructions:
1.Identifythekeysectionsinthefilerelevanttothequery.
2.Extractandsummarizethenecessaryinformationfromthesesections.
3.Ensuretheresponseisfocusedanddirectlyaddressesthequery.
Example:’Identifythemainthemeinthetext.’”
21LanguageAgentsasOptimizableGraphs
Table8. PromptsfortheTask-SolvingexperimentsonGAIA(2).
Promptpurpose Prompt
CombineAnswer ReferenceinformationforFileAnalysis:
—
{file analysis}
—
ReferenceinformationforWebsearch:
—{web search}—
Provideaspecificanswer.Forquestionswithknownanswers,ensuretoprovideaccurateand
factualresponses.Avoidvagueresponsesorstatementslike’unableto...’thatdon’tcontribute
toadefinitiveanswer.Forexample:ifaquestionasks’whowillbethepresidentofAmerica’,
andtheansweriscurrentlyunknown,youcouldsuggestpossibilitieslike’DonaldTrump’,or
’Biden’.However,iftheanswerisknown,providethecorrectinformation.”
FinalDecision #Self-ConsistencyEvaluationTask
(Self-Consistency) ##QuestionforReview:
—
{question}
—
##ReviewableAnswers:
—
{formatted answers}
—
##InstructionsforSelection:
1.Readeachanswerandassesshowitaddressesthequestion.
2.Comparetheanswersfortheiradherencetothegivenquestion’scriteriaandlogicalcoherence.
3.Identifytheanswerthatbestalignswiththequestion’srequirementsandisthemostlogically
consistent.
4.Ignorethecandidateanswersiftheydonotgiveadirectanswer,forexample,using’unableto
...’,’asanAI...’.
5.Copythemostsuitableanswerasitis,withoutmodification,tomaintainitsoriginalform.
6.Adheretotheconstraints:{constraint}.
Note: If no answer fully meets the criteria, choose and copy the one that is closest to the
requirements.
FinalDecision ##Question:
(Choose“Best”) —
{question}
—
##CandidateAnswersforEvaluation:
—
{formatted answers}
—
##EvaluationInstructions:
1.Examinethequestioncloselytounderstanditsrequirements.
2. Readeachcandidateanswerthoroughlyandassessitsrelevanceandaccuracyaboutthe
question.
3.Choosetheanswerthatmostaccuratelyandcompletelyaddressesthequestion.
4.Ignorethecandidateanswersiftheydonotgiveadirectanswer,forexample,using’unableto
...’,’asanAI...’.
”5.Copythechosenanswerexactlyasitispresented,maintainingitsoriginalformat.
6.Adheretotheconstraints:
{constraint}.
Note:Ifnoneoftheanswersfullymeetthequestion’scriteria,selecttheoneclosesttofulfilling
them.
22