Robust Reward Design for Markov Decision Processes
Shuo Wu∗ Haoxiang Ma† Jie Fu† Shuo Han∗
June 10, 2024
Abstract
The problem of reward design examines the interaction between a leader and a follower,
where the leader aims to shape the follower’s behavior to maximize the leader’s payoff by mod-
ifying the follower’s reward function. Current approaches to reward design rely on an accurate
modelofhowthefollowerrespondstorewardmodifications, whichcanbesensitivetomodeling
inaccuracies. To address this issue of sensitivity, we present a solution that offers robustness
against uncertainties in modeling the follower, including 1) how the follower breaks ties in the
presenceofnonuniquebestresponses,2)inexactknowledgeofhowthefollowerperceivesreward
modifications, and 3) bounded rationality of the follower. Our robust solution is guaranteed to
exist under mild conditions and can be obtained numerically by solving a mixed-integer linear
program. Numericalexperimentsonmultipletestcasesdemonstratethatoursolutionimproves
robustnesscomparedtothestandardapproachwithoutincurringsignificantadditionalcomput-
ing costs.
1 Introduction and Background
The problem of reward design is concerned with interactions between two types of players, a leader
andafollower. Theleaderaimstoinducethefollowertobehaveinadesirablewaybymodifyingthe
follower’s reward function. The terms “leader” and “follower” derive from Stackelberg games (Hicks,
1935) to describe the asymmetric roles of the players: The leader always modifies the reward before
the followers take action. As an example of reward design, imagine a class consisting of a teacher
playing the role of the leader and a group of students playing the role of the follower. The teacher
creates a grading policy that aims to motivate the students to achieve better learning outcomes. If
the policy determines the final grade of a student solely based on the performance of the final exam,
the student might neglect homework and class attendance, opting to cram for the final exam the
night before. In contrast, a more effective grading policy is to include homework and attendance
in the evaluation, which encourages students to learn on a regular basis. Another example arises
in the training of autonomous agents via reinforcement learning, in which the agents are trained to
maximize a predefined reward function. However, if the reward function is not carefully designed, it
may be exploited by the agents to achieve high rewards in unintended ways, a phenomenon known
as reward hacking (Pan et al., 2022). This can lead to behaviors that do not align with human
values.
We focus on the case where the sequential decision-making problem of the follower is modeled
by a Markov decision process (MDP). Specifically, the follower faces a sequential decision-making
∗Department of Electrical and Computer Engineering, University of Illinois Chicago, Chicago, IL 60607. Email:
swu99@uic.edu; hanshuo@uic.edu.
†Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611. Email:
hma2@ufl.edu; fujie@ufl.edu.
1
4202
nuJ
7
]CO.htam[
1v68050.6042:viXraproblem and aims to maximize the cumulative reward. Such a setting is motivated by problems
in cybersecurity. In a network, an attacker, who acts as the follower in our problem setting, seeks
to compromise a set of target hosts to acquire confidential data or to disrupt the network opera-
tion. To achieve the objective, the attacker must breach a sequence of hosts within the network
by exploiting causal and logical dependencies between vulnerabilities and carrying out sequential
attacks. The planning of the sequential attacks is captured using deterministic or probabilistic at-
tack graphs (Frigault et al., 2008; Hewett and Kijsanayothin, 2008; Jha et al., 2002; Nguyen et al.,
2018), which can be viewed as a special instance of MDPs. A defender, who acts as the leader
in this setting, aims to protect the network by allocating fake hosts or honey-patching vulnera-
bilities (Qin et al., 2023) and, consequently, mislead the attacker by manipulating the attacker’s
perceived rewards/payoffs.
The reward design problem can be formulated as a Stackelberg game (Ben-Porat et al., 2024;
Chakrabortyetal.,2024;Chenetal.,2022;Maetal.,2023;Thakooretal.,2020;ZhangandParkes,
2008),forwhichalgorithmicsolutionsareavailable. Nevertheless,existingsolutionstorewarddesign
sufferfromthreeissuesofsensitivitytouncertaintiesinthemodelofthefollower. First,thedesigned
reward may be sensitive to tie-breaking of the follower. When the best response of the follower is
not unique under the designed reward, the leader’s payoff may depend on the follower’s choice, as
illustrated in Example 2 and Example 3 in this paper. Second, when the leader does not know the
exact reward function of the follower, the leader’s payoff may drop significantly upon small errors
in modeling the follower’s reward function. This is because the follower’s response does not vary
continuously as his reward changes. Lastly, when the follower is irrational and does not play a best
response, the leader’s payoff may be sensitive to the level of suboptimality of the follower’s response
(Table 4 and Table 5).
Solutions to these three issues of sensitivity remain under-explored. For achieving robustness to
nonunique best responses, one notable folklore solution is to slightly perturb the optimal strategy
of the leader (von Stengel and Zamir, 2004). However, it remains unclear how such a perturbation
can be determined algorithmically. The method of perturbation may also fail in the presence of
a budget constraint on reward allocation, as illustrated in Example 2. The closest work to ours
is the study of robust Stackelberg equilibria in Gan et al. (2023), which addresses the issue of
sensitivity to irrational followers. In their setting, the follower may play any δ-optimal response,
i.e., a response that yields a follower’s payoff within δ from optimality, whereas the leaders aims to
find a robust strategy that maximizes the worst-case payoff against all possible δ-optimal responses.
They discussed the existence of such a robust strategy and showed how the robust strategy can
be computed in theory. In comparison, our work uses the quantal response model (Luce, 1959),
which is a probabilistic model for characterizing the irrationality of the follower. Interestingly, our
proposed robust solution is found to relate to the robust solution studied in Gan et al. (2023).
Proposition 33 shows that our robust solution is nearly robust optimal against δ-optimal responses
of the follower.
Main results The main results of this paper are summarized below:
• We formulate the reward design problem as a Stackelberg game and identify the issue of
sensitivity to illustrate the need for obtaining a robust solution. When the follower’s best
response is nonunique, we give examples showing that the leader’s payoff can be sensitive to
how the follower breaks ties (Example 2). While a folklore solution in the case of normal-
form Stackelberg games is to induce a unique best response of the follower by perturbing the
leader’s strategy, we show that a unique best response may not be inducible in reward design
for MDPs, even for very common cases (Example 3).
2• We propose a robust solution, named an optimal interior-point allocation, for the reward
design problem. An optimal interior-point allocation is provably robust in three aspects.
First, the solution is robust to the follower’s tie-breaking choices (Proposition 6). Second,
the solution offers robustness when the leader does not know the follower’s reward function
precisely (Proposition 7). Third, the solution is robust when the follower’s decision-making
deviates from rationality, a phenomenon often referred to as bounded rationality; the robust-
ness guarantee holds under three different models of bounded rationality, including two based
on quantal response models (Propositions 8 and 32), and one based on δ-optimal responses
(Proposition 33).
• Weprovethattheexistenceofanoptimalinterior-pointallocationonlydependsonthereward
allocationbudgetoftheleader(Theorem14). Specifically, anoptimalinterior-pointallocation
is guaranteed to exist when the leader can achieve the optimal payoff without exhausting the
reward allocation budget in the optimistic case, i.e., when the follower breaks ties in favor of
the leader.
• Moreover, we prove that the existence of an optimal interior-point allocation is not only suffi-
cient but also necessary for achieving robustness to tie-breaking of the follower (Theorem 15).
Thisestablishesthepivotalroleofinterior-pointallocationsinachievingrobustness. Itfurther
motivates the development of algorithms for finding an optimal interior-point allocation.
• We show that an optimal interior-point allocation can be computed from a mixed-integer
linear program (MILP). Numerical experiments were conducted to validate the robustness of
the solution and to evaluate the computational cost on problems of practical interest (e.g.,
defending against cyberattacks). Compared to the standard solution to reward design given
by Ma et al. (2023), our solution was found to improve robustness in all the test cases while
maintaining a reasonable computational cost.
2 Related Work
Reward design The problem of choosing a suitable reward function to achieve a desirable out-
comeisalsoknownasreward shaping. EarlyworkonrewardshapinginMDPsstudiespolicyinvari-
ance, i.e., how the reward function may be modified without changing the set of optimal policies.
Ng et al. (1999) introduced the notion of potential-based reward shaping and showed guaranteed
policy invariance in MDPs. Potential-based reward shaping was later extended to multi-agent sys-
tems for preserving the set of Nash equilibria (Devlin and Kudenko, 2011). More recently, reward
shaping was studied in a leader-follower setup (Ben-Porat et al., 2024) to incentivize the follower
to act in the leader’s interest. Aside from reward shaping, the problem of reward design has also
been studied in the context of policy teaching (Banihashem et al., 2022; Zhang and Parkes, 2008;
Zhang et al., 2009b) and can be viewed as a special case of model design (Chen et al., 2022) and
environment design (Yu and Ho, 2022; Zhang et al., 2009a), in which elements (e.g., the transition
kernel) other than the reward function of the MDP can also be modified.
For solving the reward design problem, Zhang and Parkes (2008) provided a solution based on
mixed-integer programming. Ben-Porat et al. (2024) established the NP-hardness of the reward
design problem and gave a polynomial-time approximation algorithm. A special case is when the
leader aims to induce a specific policy rather than a policy that maximizes the leader’s payoff. For
this case, Zhang et al. (2009b) showed that the desired reward function can be obtained by finding
a feasible solution to a linear program.
3Principal-agent problem In the reward design problem, the leader and the follower are some-
timescalledtheprincipal andtheagent,respectively,whicharetermsoriginatedfromeconomics(Gross-
man and Hart, 1983; Ross, 1973). In the principal-agent problem, an agent takes actions on behalf
of the principal through a contract designed by the principal (Bolton and Dewatripont, 2005; Gan
et al., 2024; Hart and Holmström, 1987; Salanié, 2005). Two key aspects of the principal-agent
problem are adverse selection and moral hazard (Myerson, 1982). The former refers to a situation
in which the agents have private information that the principal cannot readily access; the latter
refers to the situation in which the agents take private actions that the principal cannot directly
control or observe. The problem of reward design resembles the principal-agent problem, except
that it typically does not involve adverse selection or moral hazard.
The principal-agent problem has also been studied in control theory, where it is commonly
known as incentive design (Ho et al., 1982); see recent survey papers such as Ratliff et al. (2019)
andBaşar(2024). Inincentivedesign,adecision-makerneedstodetermineareward-basedincentive
strategy to encourage a desired behavior of another agent. The incentive strategy plays a similar
role as the contract in the principal-agent problem. The form of optimal incentive strategies was
studied in Başar (1982); Zheng and Basar (1982) for deterministic decision problems and in Başar
(1984) for stochastic decision problems. Similar to moral hazard in the principal-agent problem,
some settings of incentive design assume that the agent may influence the state of the environment
through actions and that the decision-maker only has access to partial information of the state.
Stackelberg games The reward design problem can be viewed as a Stackelberg game or, math-
ematically, a bilevel optimization problem (Ben-Porat et al., 2024; Chakraborty et al., 2024; Chen
etal.,2022;Maetal.,2023). StackelberggameswerefirstintroducedbyHicks(1935)todemonstrate
the benefits of leadership in games involving two parties. von Stengel and Zamir (2004) extended
the strategy space in the model from pure strategies to mixed strategies. When the number of pure
strategies is finite, Conitzer and Sandholm (2006) showed that an optimal strategy of the leader can
be computed in polynomial time by solving multiple linear programs. Our results are inspired by
the alternative solution presented by Paruchuri et al. (2008), who showed that an optimal strategy
for the leader can be computed from a single MILP.
Robust solution to Stackelberg games In a Stackelberg game, the influence of nonunique
best responses of the follower on the leader’s payoff is captured by the notions of strong and weak
Stackelberg equilibria (Breton et al., 1988). In a strong Stackelberg equilibrium, the leader plays
optimally assuming that the follower breaks ties in favor of the leader. In contrast, in a weak
Stackelbergequilibrium,theleaderassumesthatthefollowerbreakstiesadversarially. Consequently,
the leader will play a strategy that is robust to tie-breaking. Nevertheless, a weak Stackelberg
equilibrium may not exist in general (Dempe and Zemkoho, 2020, Section 4.3.2).
Robustness to the unknown follower’s reward has been studied by Cansever and Başar (1983,
1985). The results are based on analyzing the local sensitivity of the leader’s payoff, which is
characterized by derivatives of the payoff function with respect to unknown parameters in the
follower’s reward function. In comparison, our work uses the notion of robustness margin and can
guarantee non-local robustness against bounded modeling errors of the follower’s reward function.
RobustnesstoirrationalfollowershasbeenstudiedbyGanetal.(2023), wherethegoalistofind
anoptimalleader’sstrategyagainstafollowerwhoplaysanynear-optimalresponsedeterministically.
Theirmodelofirrationalitydiffersfromthequantalresponsemodelusedinourwork,whichassumes
that the follower chooses suboptimal responses probabilistically.
4Models of irrationality Severalmodelshavebeenusedintheliteraturetocharacterizeirrational
behaviors of a decision-making agent. Of particular interest is modeling irrational behaviors due to
limited reasoning capability of the agent, commonly known as bounded rationality (Simon, 1955).
Our work adopts the quantal response model, which assumes that the agent may play suboptimal
strategies with a probability that decreases exponentially as the corresponding payoff decreases.
The quantal response model in economics intends to model how the selection probabilities of an
individual depend on explanatory variables (McFadden, 1976). In game-theoretic settings, quantal
response was used to model the behaviors of players in normal-form games (McKelvey and Palfrey,
1995)andinStackelberggames(Yangetal.,2012). Theresultingequilibriumisknownasthequantal
response equilibrium. Besidesquantalresponse, anotherpopularmodelofboundedrationalityisthe
cognitivehierarchy model(Camereretal.,2004). Themodelusesiterativedecisionrulestorepresent
strategic players with different levels of sophistication and can be used to explain nonequilibrium
behaviors of the players. Lastly, one may assume that the agent plays any suboptimal strategy that
leads to a payoff close to optimality (Gan et al., 2023).
3 Preliminaries
3.1 Reward design for MDPs
The decision-making process of the follower is modeled as an MDP M = (S,A,T,r,γ,ρ). The set
S is the state space, A is the action space, and ρ ∈ ∆(S) is the initial distribution, where ∆(S)
denotes the set of probability distributions over S. The mappings T : S × A × S → [0,1] and
r : S×A → R represent the transition kernel and the reward function, respectively. We sometimes
also abuse the notation and view r as a vector of dimension |S||A|. The constant γ ∈ (0,1] is the
discount factor. The policy of the follower is denoted by π : S ×A → [0,1], where π(s,·) ∈ ∆(A)
for all s ∈ S. The set of all policies is denoted by Π.
The leader may modify the reward function of the follower by allocating rewards in the environ-
ment. Let S = {s ,...,s } ⊆ S be the set of candidate states at which reward can be allocated.
d 1 K
The reward allocation is described by a function x : S → R, where x(s ) is the amount of reward
d i
allocated at s . For convenience, we sometimes abuse the notation and treat x as a vector of di-
i
mension |S | such that x = x(s ). A reward allocation x is said to be admissible if x satisfies the
d i i
following conditions:
• The total amount of allocated resource cannot exceed a given budget C: (cid:80)|S d| x ≤ C.
i=1 i
• The amount of resource allocated to each state is nonnegative: x ≥ 0, i = 1,...,|S |.
i d
(cid:110) (cid:111)
The set of admissible reward allocations is denoted by X = x | (cid:80)|S d| x ≤ C, x ⪰ 0 . An ad-
i=1 i
missible reward allocation x induces a new reward function rx of the follower, where rx(s,a) ≜
2 2
r(s,a) + x(s). Throughout most of the paper, the leader’s reward function r : S × A → [0,1]
1
is given by r (s,a) = 1 when s ∈ S and r (s,a) = 0 otherwise. The form of r is motivated
1 d 1 1
by problems in cybersecurity, where a defender (i.e., the leader) is rewarded by misleading an at-
tacker (i.e., the follower) into reaching a set S of desired states. The case of general r will be
d 1
discussed in Section 9. Under a given policy π of the follower, the leader’s value function is de-
fined by Vπ(s) = E (cid:2)(cid:80)∞ γtr (s ,a ) | s = s(cid:3), and the follower’s value function is defined by
1 π t=0 1 t t 0
Vπ(s;x) = E (cid:2)(cid:80)∞ γtrx(s ,a ) | s = s(cid:3). The expected payoffs of the leader and the follower are
2 π t=0 2 t t 0
given by E [Vπ(s)] and E [Vπ(s;x)], respectively. The goal of the leader is to maximize her
s∼ρ 1 s∼ρ 2
payoff by influencing the policy of the follower through the reward allocation x. Mathematically,
5this can be cast as the following optimization problem:
maximize E [Vπ(s)]
s∼ρ 1
x,π
subject to π ∈ argmaxE [Vπ(s;x)] (1)
s∼ρ 2
π
x ∈ X.
We do not assume that argmax E [Vπ(s;x)] is a singleton set. When the optimal policy of the
π s∼ρ 2
follower is not unique under a reward allocation x, the problem formulation in (1) assumes that the
follower will choose from the set of optimal policies in favor of the leader. This assumption will be
revisited in Section 4.
3.2 Reformulation via the occupancy measure
While the expected payoff in an MDP is a nonlinear function of the policy π, it can be rewritten as
a linear function by a change of variables. As shown in Section 5.1, writing the expected payoff as a
linear function is useful in revealing important geometric structures of (1). The change of variables
involves replacing the policy π with m: S ×A → R defined by
(cid:34) ∞ (cid:35)
(cid:88)
m(s,a) ≜ E γtP(s = s,a = a) , (2)
π,s0∼ρ t t
t=0
knownastheoccupancy measure inducedbyπ. Itcanbeshownthatmisavalidoccupancymeasure
if and only if m(s,a) ≥ 0 for all (s,a) ∈ S ×A and
(cid:88) (cid:88)
m(s,a) = ρ(s)+γ T(s′,a′,s)m(s′,a′) ∀s ∈ S. (3)
a∈A s′,a′
For convenience, we abuse the notation and treat m as a vector of dimension |S||A|, ρ as a vector
of dimension |S|, and succinctly write (3) as Am = ρ for some matrix A. Denote the set of valid
occupancy measures by
M = {m | m ⪰ 0, Am = ρ}.
For the direction from policies to occupancy measures, however, there could be many policies
that induce the same occupancy measure.
Proposition 1. Given any m ∈ M, a policy π induces m if and only if
(cid:40) m(s,a)/(cid:80) m(s,a′) if (cid:80) m(s,a′) ̸= 0,
π(s,a) = a′∈A a′∈A (4)
π (s,a) otherwise
0
for some policy π .
0
Proof. See Appendix A.1.1.
Define ⟨rx,m⟩ ≜ (cid:80) rx(s,a)m(s,a). It can be shown (see Appendix A.2) that the
2 (s,a)∈S×A 2
expected payoff of the follower becomes a linear function of m:
E [Vπ(s;x)] = ⟨rx,m⟩.
s∼ρ 2 2
6Similarly, the expected payoff of the leader satisfies E [Vπ(s)] = ⟨r ,m⟩. For any reward alloca-
s∼ρ 1 1
tion x, define the set of best responses of the follower to x by
BR(x) ≜ argmax⟨rx,m⟩.
2
m∈M
Using the occupancy measure, the reward design problem in (1) can be equivalently reformulated
as
maximize ⟨r ,m⟩
1
x∈X,m∈M (5)
subject to m ∈ BR(x).
We will hereafter denote the optimal value of (5) by v⋆ and an optimal solution of (5) by (x⋆,m⋆),
1
where x⋆ is called an optimal allocation and m⋆ an optimal occupancy measure.
3.3 Computing an optimal reward allocation
The reward design problem in (5) can be solved by an MILP (Ma et al., 2023). In the following, we
will briefly review the procedure for completeness. For a given reward allocation x, the condition
m ∈ BR(x) is equivalent to that m is an optimal solution of the following problem:
maximize ⟨rx,m⟩
2
m (6)
subject to Am = ρ, m ⪰ 0,
where the constraints come from the condition m ∈ M. Because problem (6) is a linear program
and is always feasible, it is known that m is optimal if and only if there exists (a dual variable) ν
such that (m,ν) satisfies the Karush–Kuhn–Tucker (KKT) conditions:
Am = ρ, m ⪰ 0, ATν −rx ⪰ 0, m ⊥ ATν −rx. (7)
2 2
The reward design problem in (5) can then be rewritten as
maximize ⟨r ,m⟩ (8a)
1
x,m,ν
subject to Am = ρ, m ⪰ 0, ATν −rx ⪰ 0 (8b)
2
m ⊥ ATν −rx (8c)
2
x ∈ X. (8d)
The complementary slackness constraint in (8c) can be reformulated as affine constraints with
integer variables (Vielma, 2015). Because other constraints are affine, and the objective is linear,
problem (8) can be reformulated as an MILP.
3.4 Notation
A policy π is called deterministic if for any s ∈ S, there exists a ∈ A such that π(s,a) = 1. A policy
iscalledrandomized ifitisnotdeterministic. ThesetofalldeterministicpoliciesisdenotedbyΠ .
det
For any π ∈ Π, we sometimes denote by mπ the occupancy measure induced by π according to (2).
An occupancy measure is called deterministic (resp. randomized) if it is induced by a deterministic
(resp. randomized) policy. The set of deterministic occupancy measures is denoted by M . For
det
any m ∈ M, denote the set of policies of the form (4) by Π(m), which is the set of policies that
induce m according to Proposition 1.
7For a given (possibly randomized) policy π, we abuse the notation of Π and define
det
Π (π) ≜ {π′ ∈ Π | π′(s,a) = 0 when π(s,a) = 0}.
det det
In plain words, at any s ∈ S, a policy π′ ∈ Π (π) is only allowed take an action that π takes with
det
nonzero probability. The set of occupancy measures induced by policies in Π (π) is denoted by
det
M (π) ≜ {mπ′ | π′ ∈ Π (π)} with an abuse of notation. Such a definition has a nice property
det det
that for any π ,π ∈ Π(m), it holds that M (π ) = M (π ). Interested reader may refer to
1 2 det 1 det 2
Appendix A.1.2.
4 Sensitivity Issues of Optimal Allocations
Inproblem(5),themaximizationovermimplicitlyassumesthatthefollowerchoosesabestresponse
that maximizes the expected payoff of the leader. Under this assumption, the expected payoff
received by the leader under an allocation x is given by the optimal value of the problem
maximize ⟨r ,m⟩
1
m∈M (9)
subject to m ∈ BR(x).
Inpractice, however, thefollowermaychoosearbitrarilyfromthesetofbestresponses. Intheworst
case for the leader, the follower may choose a best response that is most unfavorable to the leader.
In this case, the leader’s expected payoff under the allocation x is given by the optimal value of the
problem
minimize ⟨r ,m⟩
1
m∈M (10)
subject to m ∈ BR(x).
We define the optimal value of (9) as the optimistic value of x, denoted by OptiVal(x), and the
optimal value of (10) as the pessimistic value of x, denoted by PessVal(x). We also refer to
sup OptiVal(x) as the optimal optimistic value and sup PessVal(x) as the optimal pessimistic
x∈X x∈X
value. In general, when the best response of the follower under an allocation x is not unique, the
optimisticvalueOptiVal(x)andthepessimisticvaluePessVal(x)maydiffer. Inaddition,theoptimal
optimistic value and the optimal pessimistic value may also differ, as illustrated in Example 2.
Example 2 (Optimistic and pessimistic values). Consider a 4 × 4 grid world in Figure 1. The
follower starts from the position (1,2) and can move in any direction or stay in the same place by
taking an action in A = {left,right,up,down,stay}. The true goals are at (4,1) and (4,3). The
leader is allowed to allocate the reward at (3,3). The game’s discount factor is 1. The payoff for
arriving at (4,1) is 3, and for arriving at (4,3) is 2. The allocation budget is 1, and (4,1) and (4,3)
are the absorbing states, meaning that the follower cannot leave the state once enter them. The
leader will get 1 unit of payoff once the follower arrives at the states with allocated reward.
First,considertheoptimisticcasewherethefollowerbreaksanytieinfavoroftheleader. Denote
by x the allocation strategy that spends the entire budget of 1 at (3,3). Under x , choosing Path
1 1
1 to arrive at (3,3) and (4,3) in 4 steps gives the follower a total payoff of 3. Path 2 has the same
payoff for the follower because the true goal at (4,1) is in the path. Since the game ends in 4 steps,
3 is the maximum payoff that the follower can achieve, implying that two paths are the only best
responses. The leader, however, will obtain different payoffs when the follower chooses different
paths. If the follower reaches the allocated reward at (3,3) with probability 1 by following Path 1,
the leader will receive a payoff of 1. In comparison, if the follower reaches the true goal at (4,1) by
84
Path1
3
Truegoals
Obstacles
Allocationsites
2 Agent
Path2
1
1 2 3 4
Figure1: Rewarddesignproblemwithonestateadmitsallocatedreward. Thepayoffforarriving
at(4,1)is3andthatforarrivingat(4,3)is2. Atmost1unitofresourcecanbeallocatedtoward
the reward at (3,3). The follower starts at (1,2), and all the transitions are deterministic .
following Path 2, the leader will receive no payoff. Under the optimistic assumption, the follower
will choose Path 1. Therefore, the leader’s payoff for allocating all the budget at (3,3) is 1. For any
otherallocationstrategy, theallocationat(3,3)islessthan1, andthefollower’spayoffforfollowing
Path 1 is less than the payoff for following Path 2. The follower will then follow Path 2, in which
case the payoff for the leader is 0. Therefore, x is an optimal allocation strategy, and the optimal
1
optimistic value of the game is 1.
Next, consider the pessimistic case. It can be seen that the follower will always follow Path 2
under any admissible allocation. When the allocation at (3,3) is less than 1, the payoff for the
leader remains the same as in the optimistic case because the best response of the follower is unique
and is following Path 2. When the allocation at (3,3) is 1, the follower will still take Path 2 under
the pessimistic assumption. This is because both paths are best responses of the follower, but Path
2 leads to a lower payoff for the leader. Therefore, the leader will always receive a payoff of 0 in
the pessimistic case regardless of the allocation strategy, which implies that the optimal pessimistic
value of the game is 0.
In summary, this example shows that the optimistic value and pessimistic value under x are
1
different: OptiVal(x ) = 1 and PessVal(x ) = 0. In addition, the optimal optimistic value of the
1 1
game is 1, whereas the optimal pessimistic value of the game is 0.
Besides the handcrafted setup in Example 2, our numerical experiments in Section 10.2 suggest
that the optimal allocation obtained using the method in Section 3.3 also exhibits sensitivity to
nonunique best responses. This motivates us to search for an optimal allocation x⋆ that is robust
to nonunique best responses of the follower. Mathematically, this requires finding x⋆ that satisfies
OptiVal(x⋆) = PessVal(x⋆) = v⋆.
1
The issue of sensitivity to tie-breaking in Stackelberg games is a known issue in the literature,
where a folklore solution (see page 12 in von Stengel and Zamir (2004)) is to perturb the optimal
allocation slightly such that the best response becomes unique. Nevertheless, two issues prevent the
immediate application of this solution. First, it remains unclear how such a perturbation should be
obtained algorithmically. Indeed, when the perturbation is not chosen appropriately, the induced
best response may be to the disadvantage of the leader and yield the pessimistic value. Second,
94
Path1
3
Truegoals
Obstacles
Path3 Allocationsites
2 Agent
Path4
Path2
1
1 2 3 4
Figure 2: Reward design problem with two obstacles. Other settings are similar to Example 2.
the best response may remain nonunique regardless of how the optimal allocation is perturbed, as
illustrated in Example 3.
Example 3 (Best response is never unique). Consider the environment in Figure 2, which has a
similar setting as Example 2 with the exception that one obstacle has been removed. Both Path
1 and Path 3 go through the allocated reward at (3,3) and the true goal at (4,3) but never reach
the true goal at (4,1). Therefore, for the follower, the payoff for choosing Path 1 and Path 3 are
always the same under any reward allocation. This implies that Path 1 can never be a unique best
response. For a similar reason, for the follower, the payoff for choosing Path 2 and Path 4 are
always the same under any reward allocation. This implies that Path 2 can never be a unique best
response.
In the meantime, it is not difficult to see that either Path 1 or Path 2 must be a best response
under any allocation. Therefore, the existence of Paths 3 and 4 shows that it is impossible to induce
a unique best response regardless of the allocation strategy.
5 Interior-Point Allocation Are Robust
We introduce the concept of an optimal interior-point allocation, which is a special type of opti-
mal allocations, and show that any optimal interior-point allocation is robust to nonunique best
responses of the follower. Namely, by using an optimal interior-point allocation, the leader is always
guaranteed to receive the optimal expected payoff of the game regardless of how the follower breaks
ties. Moreover, an optimal interior-point allocation is able to provide other forms of robustness
guarantees. One is robustness to uncertainty in the follower’s reward function: It ensures that
the leader still receives the optimal payoff without exactly knowing how the follower perceives the
modified reward function. Another form is robustness to bounded rationality in the follower: It
ensures that the leader does not lose much payoff when the follower starts to behave irrationally by
responding with a slightly suboptimal policy.
5.1 Optimal interior-point allocations
Since any allocation x ∈ R|S d| corresponds to one or more best responses described by BR(x), it is
possibletodivideR|S | into(possiblyoverlapping)regionsbasedonthecorrespondingbestresponse.
d
10Each region is called an allocation region and is identified by a unique occupancy measure.
Definition 4 (Allocation region). Let m ∈ M be an occupancy measure. The allocation region of
m is defined by
P = {x ∈ R|S d| | m ∈ BR(x)} = {x ∈ R|S d| | ⟨rx,m⟩ ≥ ⟨rx,m′⟩ for all m′ ∈ M}.
m 2 2
Furthermore, the allocation region of an optimal occupancy measure is called an optimal allocation
region.
The term allocation region highlights the fact that P
m
is a subset of R|S d|, the space where
allocation vectors live. For x⋆ to be an optimal allocation, it is necessary from Definition 4 that
x⋆ belongs to some optimal allocation region. However, when x⋆ is on the boundary of an optimal
allocationregion, thebestresponsesunderx⋆ arenotuniqueandmayleadtoundesirablesensitivity
to tie-breaking. This motivates us to examine the interior points of an allocation region, which we
refer to as interior-point allocations.
Definition 5 (Interior-point allocation). Let m ∈ M be an occupancy measure. An allocation
vector x ∈ X is called an interior-point allocation of P if there exists c > 0 such that
m
x+cv ∈ P for all ∥v∥ ≤ 1. (11)
m 1
The largest constant c for (11) to hold is called the margin of x. An interior-point allocation of an
optimal allocation region is called an optimal interior-point allocation.
Because all norms are equivalent in a finite-dimensional space, the choice of the norm in (11)
does not affect the definition of interior-point allocation: An interior-point allocation in one norm
must also be an interior-point allocation in another norm. The choice of ∥·∥ is for the convenience
1
of computation. For computing an optimal interior-point allocation, the condition in (11) can be
rewritten equivalently as finitely many constraints given by (17) in Section 8.
5.2 Robustness to nonunique best responses
We will show that an optimal interior-point allocation x⋆ always yields the optimal value v⋆ of the
1
game regardless of how the follower breaks ties. In other words, the pessimistic value of x⋆ is equal
to the optimistic value of x⋆.
Proposition 6 (Robustness to nonunique best responses). If x⋆ is an optimal interior-point allo-
cation, then OptiVal(x⋆) = PessVal(x⋆) = v⋆.
1
Proof. See Appendix B.1.
The importance of an interior-point allocation can be understood as follows. Suppose that
x⋆ is an optimal interior-point allocation of P for some optimal occupancy measure m . This
m1 1
implies m ∈ BR(x⋆). When OptiVal(x⋆) ̸= PessVal(x⋆), there must exist m ∈ BR(x⋆) with
1 2
⟨r ,m ⟩ > ⟨r ,m ⟩. Let x(ϵ) = x⋆−ϵ1. It follows that rx(ϵ) = rx⋆−ϵr . Because m ,m ∈ BR(x⋆),
1 1 1 2 2 2 1 1 2
it holds that ⟨rx⋆ ,m ⟩ = ⟨rx⋆ ,m ⟩. Thus, for any ϵ > 0, it holds that ⟨rx(ϵ) ,m ⟩ < ⟨rx(ϵ) ,m ⟩
2 1 2 2 2 1 2 2
or, equivalently, x(ϵ) ∈/ P . This implies that no neighborhood of x⋆ is contained in P , which
m1 m1
contradicts with the fact that x⋆ is an interior-point allocation of P .
m1
While Proposition 6 shows that finding an optimal interior-point allocation is sufficient for
achievingrobustnesstononuniquebestresponsesofthefollower,theexistenceofanoptimalinterior-
point allocation is, in fact, also necessary for robustness to nonunique best responses. We shall
postpone the discussion on necessity until Theorem 15.
115.3 Robustness to uncertain reward perception of the follower
Besidesrobustnesstononuniquebestresponses,onemaybeinterestedinothernotionsofrobustness
motivated by practical considerations. One such notion is robustness to uncertainty in how the
follower perceives the modified reward function. The original formulation in (5) assumes that the
modifiedrewardfunctionofthefollowerisexactlyrx whentheallocationisx. However, thefollower
2
may perceive the modified reward differently as rx+δ, where δ ∈ R|S d| represents uncertainty in how
2
the follower perceives reward modifications. In light of such uncertainty, one reasonable goal is to
seek an allocation x⋆ that is robust to any uncertainty δ up to a certain magnitude.
The following proposition shows that an optimal interior-point allocation offers robustness to
uncertainty in reward perception of the follower.
Proposition 7 (Robustnesstorewardperceptionofthefollower). If x⋆ is an optimal interior-point
allocation with margin c > 0, then OptiVal(x⋆+δ) = PessVal(x⋆+δ) = v⋆ for all ∥δ∥ < c.
1 1
Proof. Since x⋆ is an optimal interior-point allocation, there exists m such that x⋆ +cv ∈ P for
m
all ∥v∥ ≤ 1 and ⟨r ,m⟩ = v⋆. Then x⋆ +δ is in the interior of P when ∥δ∥ < c. According to
1 1 1 m 1
Proposition 6, OptiVal(x⋆+δ) = PessVal(x⋆+δ) = v⋆.
1
It can be seen that Proposition 7 implies Proposition 6: By setting δ = 0, the result in Proposi-
tion 7 recovers the one in Proposition 6. In other words, robustness to uncertainty in the follower’s
perceived reward is a stronger notion than robustness to nonunique best responses.
5.4 Robustness to a boundedly rational follower
In practice, the follower may not be able to solve the MDP to optimality and may instead produce
a response that is only near-optimal, a phenomenon known as bounded rationality (Simon, 1955).
One common model of bounded rationality is quantal response (Luce, 1959), which assumes that
the decision-maker takes suboptimal actions with probabilities that diminish exponentially as the
corresponding payoffs decrease (McFadden, 1976). More concretely, under a reward allocation x, a
boundedly rational follower attempts to choose a policy that maximizes the payoff
(cid:88) m(s,a)
⟨rx,m⟩−τ · m(s,a)log , (12)
2 (cid:80) m(s,a′)
a′∈A
(s,a)∈S×A
where m is the occupancy measure induced by the policy of the follower, τ > 0 is a constant. The
form of the payoff in (12) is inspired by the objectives used in entropy-regularized MDPs (Neu
et al., 2017). The level of irrationality is modeled through the constant τ. When τ = 0, the model
recovers the rational case, where the follower will choose a best response. At the other extreme, as
τ → ∞,theroleofrx diminishes,andthefollowerisinclinedtomaketheoccupancymeasurespread
2
uniformly among all state-action pairs regardless of r 2x. Since τ·(cid:80) (s,a)∈S×Am(s,a)log (cid:80) a′m ∈A(s m,a () s,a′)
is strictly convex (Neu et al., 2017, Proposition 1) in m, the function in (12) admits a unique
maximizer.
It can be shown that an optimal interior-point allocation is robust to unmodeled bounded ratio-
nality of the follower.
Proposition 8 (Robustness to bounded rationality). Suppose that (x⋆,m⋆) is an optimal solution
to the reward design problem in (5), and x⋆ is an interior point of P m⋆. Let
 
m⋆ = argmax ⟨rx⋆ ,m⟩−τ · (cid:88) m(s,a)log m(s,a)  (13)
τ 2 (cid:80) m(s,a′)
m∈M  a′∈A 
(s,a)∈S×A
12and M ⋆ = M ∩BR(x⋆). Then for any τ > 0, it holds that
det det
(cid:18) (cid:19)
2τ
⟨r ,m⋆⟩ ≥ 1− log|A| ⟨r ,m⋆⟩, (14)
1 τ b(1−γ) 1
where γ is the discount factor, and b = ⟨r 2x⋆ ,m⋆⟩−max m∈M det\M⋆ det⟨r 2x⋆ ,m⟩.
Proof. See Appendix C.1.
SinceP
m⋆
isanoptimalallocationregion,theallocationx⋆isanoptimalinterior-pointallocation.
The left side of (14) is the expected payoff of the leader when the follower is boundedly rational at
level τ. Proposition 8 provides a quantitative performance characterization on any optimal interior-
point allocation in the presence of a near-rational follower, i.e., when τ is small. (The bound
in (14) becomes vacuous when τ is large enough for the right side to become negative.) Specifically,
the leader is guaranteed to receive an expected payoff not much worse than the optimal payoff v⋆
1
againstacompletelyrationalfollower. Inotherwords,evenwhenincorrectlytreatinganear-rational
follower as completely rational, the leader will still receive a payoff not far from her prediction.
The discrepancy between the predicted payoff and the actual payoff depends on b, which mea-
sures the gap between the optimal payoff and the second-best payoff of a follower who only plays
deterministicpolicies. TheconstantbdependsnotonlyontheconfigurationoftheMDPbutalsoon
x⋆. As b decreases, the follower becomes more indifferent between a best response and a suboptimal
response. This implies that a boundedly rational follower is less likely to play an optimal policy as
desired by the leader, leading to a potential decrease in the payoff of the leader.
A similar analysis can be carried out for other models of bounded rationality. For instance,
another model of the payoff function of the follower, which is also used in entropy-regularized
MDPs (Neu et al., 2017), is given by
⟨rx,m⟩+τ ·Ent(m), (15)
2
where Ent(m) ≜ −(cid:80) m(s,a)logm(s,a) is the entropy of m. For the payoff function in
(s,a)∈S×A
(15), an optimal interior-point allocation also provably provides a robustness guarantee similar to
the one in Proposition 8; see Proposition 32 in Appendix C.2 for details. The third model of
bounded rationality assumes that the follower can take any δ-optimal response, which is a response
that yields a payoff within δ from the optimal payoff (Gan et al., 2023). Appendix C.3 discusses
how an optimal interior-point allocation is robust to δ-optimal responses.
6 Choosing an Optimal Allocation Region
When an optimal interior-point allocation exists, some optimal allocation region P⋆ must have a
nonempty interior. If P⋆ can be identified, then an optimal interior-point allocation can be found
by picking any interior point of P⋆. However, is it possible to choose an arbitrary optimal allocation
region? If not, is it necessary to examine all optimal allocation regions or only a subset? These
questions will be answered in this section.
6.1 Optimal allocation regions may have an empty interior
Since an optimal occupancy measure m⋆ can be computed by solving the optimization problem
in (8), it may be tempting to use the corresponding region P as the candidate optimal allocation
m⋆
region P⋆. However, this procedure may fail because not all optimal allocation regions have a
nonempty interior.
134
Path1
3
Truegoals
Obstacles
Allocationsites
2 Agent
Path2
1
1 2 3 4
Figure 3: A reward design problem with two states that allow reward allocation, and the budget
is C > 1. There are 3 obstacles. All the transitions are deterministic except at (1,2). If the
follower chooses right at (2,2), he will end up in (1,3) or (1,1) with equal probability; otherwise,
the follower will move deterministically according to his action.
Example 9. Consider a similar setting as Example 2. As shown in Figure 3, aside from (3,3),
we also allow reward allocated at (3,1). The transition kernel at (1,2) is modified as follows: If
the follower chooses right, he will arrive at (1,3) or (1,1), each with probability 0.5. If the follower
chooses up (resp. down), he will arrive at (1,3) (resp. (1,1)). If the follower chooses left or stay,
he will remain at (1,2). For any other state, the transition kernel remains deterministic. Denote
by x and x the amount of resource allocated to the reward at (3,3) and (3,1), respectively. The
1 2
total allocation budget C satisfies C > 1. Here, we also assume that the transition kernel at the
states with the allocated reward only allows the follower to keep going right. Therefore, the follower
cannot go back after arriving at the state with allocated reward.
Define a policy π of the follower such that π always chooses right in any state. As a result,
r r
because the follower starts at (1,2), by following π and taking right, the follower may end up in
r
two possible paths: 1) The follower arrives at (1,3) with probability 0.5 and subsequently follows
Path 1; 2) the follower arrives at (1,1) with probability 0.5 and subsequently follows Path 2.
Let X = {x | x ⪰ 0, x = x +1} and m be the occupancy measure induced by π . We will
1 1 2 r r
show that P = X . For any x ∈ X , the payoff for following Path 1 and that for following Path 2
mr 1 1
are identical: In either case, the follower will receive a payoff of 2+x . Thus, although the follower
1
does not follow either Path 1 or 2 deterministically under π , the follower is still always guaranteed
r
to receive a payoff of 2+x . This is also the largest payoff possible because the follower get into
1
absorbing states after 4 steps. Therefore, π is an optimal policy under x, or equivalently x ∈ P .
r mr
On the other hand, for any x ∈/ X , the corresponding payoff of Path 1 will differ from that of Path
1
2. Let π (resp. π ) be a policy that chooses up (resp. down) at (2,1) and follows Path 1 (resp.
u d
Path 2) onward, and m (resp. m ) be the induced occupancy measure. The payoff under π will
u d r
be strictly lower than the payoff of the better policy between π and π . The suboptimality of π
u d r
implies x ∈/ P .
mr
Consider instead X = {x | x ⪰ 0, x ≥ x +1}. It is not difficult to verify that P = X .
2 1 2 mu 2
Both m and m are optimal occupancy measures because the follower is guaranteed to reach a
r u
statewithallocatedrewardwithprobability1, givingtheleaderthemaximumpayoffof1. However,
while P has a nonempty interior, the interior of P is empty because its dimension is 1.
mu mr
146.2 Allocation regions of deterministic occupancy measures
Since not all optimal allocation regions have a nonempty interior, which region should be chosen to
produce an optimal interior-point allocation? At first glance, it appears that one needs to examine
everyoptimalallocationregionor,equivalently,everyoptimaloccupancymeasureuntilaregionwith
anonemptyinteriorisidentified. Nevertheless, wewillshowthatitsufficestofocusondeterministic
optimal occupancy measures.
Let (x⋆,m⋆) be an optimal solution of the reward design problem in (5). Let π⋆ be a policy that
induces m⋆. This immediately implies that π⋆ is an optimal policy of the follower. The following
proposition ensures that any π ∈ Π (π⋆) is also an optimal policy of the follower.
det
Proposition 10. Let (x⋆,m⋆) be an optimal solution of the reward design problem in (5) and π⋆ be
a policy that induces m⋆, i.e., π⋆ ∈ Π(m⋆). For any π ∈ Π (π⋆), the induced occupancy measure
det
mπ is a best response of x⋆, i.e. mπ ∈ BR(x⋆) or, equivalently, x⋆ ∈ P mπ.
Proof. See Appendix D.1.
Not only is any policy π ∈ Π (π⋆) optimal for the follower, but the policy is also optimal for
det
the leader even though the reward function r of the leader generally differs from rx⋆.
1 2
Proposition 11. Under the same conditions in Proposition 10, the induced occupancy measure mπ
also satisfies ⟨r ,mπ⟩ = v⋆ .
1 1
Proof. See Appendix D.2.
Proposition 11 not only shows the existence of deterministic optimal occupancy measures but
also suggests an algorithm to find them. If the given optimal occupancy measure m⋆ is randomized,
one can first recover a randomized policy π⋆ from m⋆. Then, a deterministic policy π ∈ Π (π⋆)
det
can be constructed by examining the actions taken by π⋆ at each s ∈ S: Whenever more than one
action is taken by π⋆ with nonzero probability, an arbitrary one of these actions will be assigned to
π to ensure π ∈ Π (π⋆). The induced deterministic occupancy measure mπ from π is guaranteed
det
to be optimal according to Proposition 11.
Theorem 12. Let m⋆ be a randomized optimal occupancy measure and π⋆ ∈ Π(m⋆). For any
m ∈ M det(π⋆), it holds that P m⋆ ⊆ P m, and P m is an optimal allocation region.
Proof. Consider any x ∈ P m⋆. Because m ∈ M det(π⋆), m is induced by a policy in Π det(π⋆)
by definition. From Proposition 10, we know that x ∈ P m, implying P m⋆ ⊆ P m. In addition,
Proposition 11 ensures that m is an optimal occupancy measure, implying that P is an optimal
m
allocation region.
As implied by Theorem 12, if P has a nonempty interior, so does P for any (determinis-
m⋆ m
tic) occupancy measure m ∈ M (π⋆), where π⋆ is an arbitrary policy that induces m⋆. Thus,
det
only allocation regions of deterministic optimal occupancy measures need to be examined to find
an optimal interior-point allocation. Recall that the set M of occupancy measures is generally
uncountably infinite. Theorem 12 reduces the relevant occupancy measures to a finite set M .
det
Such a reduction has important theoretical implications and will be used in Section 7 to establish
conditions for the existence of optimal interior-point allocations.
It is worth noting that Theorem 12 does not imply that the allocation region of a deterministic
occupancy measure always has a nonempty interior. For instance, although π in Example 9 is a
r
deterministic policy, its corresponding allocation region P has an empty interior.
mr
157 Existence of Optimal Interior-Point Allocations
While Section 5 shows that optimal interior-point allocations offer robustness, it remains a question
whether an optimal interior-point allocation is guaranteed to exist. Indeed, as shown in Section 6,
although it suffices to examine deterministic occupancy measures, the allocation region of a deter-
ministic occupancy measure may still have an empty interior. As will be shown in this section, the
allocation budget plays an important role in the existence of an optimal interior-point allocation.
Moreover, the existence of an optimal interior-point allocation is not only sufficient (Proposition 6)
but also necessary for the existence of a robust allocation against nonunique best responses of the
follower.
7.1 Influence of allocation budget
Before presenting the general result, we would like to use an example to illustrate how the existence
of an optimal interior-point allocation can be affected by the allocation budget.
Example 13. Consider the environment in Example 2. Let π be the policy that follows Path 1
u
and m the occupancy measure induced by π . Without considering the allocation budget, we will
u u
show that an allocation x is optimal for the leader if and only if x ≥ 1. When x ≥ 1, from the
perspective of the follower, the total payoff of Path 1 is not less than that of Path 2. Thus, π
u
is optimal for the follower and will give the leader the maximum payoff of 1. On the other hand,
when x < 1, the follower will choose Path 2 over Path 1 and give the leader a suboptimal payoff
of 0. This also shows that m is the only optimal occupancy measure and that the corresponding
u
optimal allocation region is given by P = {x | x ≥ 1}.
mu
Next, wewillshowhowtheallocationbudgetC affectstheexistenceofanoptimalinterior-point
allocation. When C > 1, the leader can choose any x ∈ (1,C], which lies within the interior of P .
mu
Because P is an optimal allocation region, x must be an optimal interior-point allocation. How-
mu
ever, an optimal interior-point allocation does not exist when C = 1: The only optimal allocation
the leader can choose is x = 1, which does not belong to the interior of P .
mu
Example 13 shows that the allocation budget is an important factor in the existence of an
optimal interior-point allocation. It suggests that an optimal interior-point allocation may fail to
exist when any optimal allocation must exhaust the allocation budget. To examine whether the
budget needs to be exhausted, consider the following problem:
|S |
d
(cid:88)
maximize C − x
i
x∈X, m∈M
i=1 (16)
subject to ⟨r ,m⟩ = v⋆
1 1
m ∈ BR(x).
Theconstraintsin(16)ensurethat(x,m)isanoptimalsolutiontotherewarddesignproblemin(5).
Thus, the optimal value of (16) is 0 if and only if any optimal allocation must exhaust the budget.
The following theorem shows that the optimal value of problem (16) determines the existence of an
optimal interior-point allocation.
Theorem 14. Therewarddesignproblemin(5)admitsatleastoneoptimalinterior-pointallocation
if and only if the optimal value of problem (16) is strictly positive.
Proof. See Appendix E.1.
167.2 Necessity for robustness to nonunique best responses
Example 13 also hints at a relationship between the existence of an optimal interior-point allocation
and robustness to nonunique best responses (introduced in Section 5.2). When C = 1, under the
only optimal allocation x = 1, the best responses of the follower is not unique: It is optimal for
the follower to choose either Path 1 or Path 2. However, the former will give the leader a payoff
of 1, whereas the latter will give a payoff of 0. In other words, an optimal allocation that is
robust to nonunique best responses does not exist. Such nonexistence happens to coincide with the
nonexistence of optimal interior-point allocations.
In fact, this is more than a coincidence. The existence of an optimal interior-point allocation
is a sufficient and necessary condition for the existence of an allocation robust to nonunique best
responses. Notice that the sufficiency has already been established in Proposition 6. The following
theorem focuses only on the necessity.
Theorem 15 (Necessity of optimal interior-point allocations). If there exists an optimal allocation
x⋆ satisfying OptiVal(x⋆) = PessVal(x⋆) = v⋆, then an optimal interior-point allocation must exist.
1
Proof. See Appendix E.2.
Theorem 15 does not, however, guarantee that x⋆ is an optimal interior-point allocation when
x⋆ satisfies OptiVal(x⋆) = PessVal(x⋆) = v⋆. For instance, consider any allocation x⋆ ∈ X in
1 1
Example9. DuetoTheorem12,onlyoptimalallocationregionsofdeterministicoccupancymeasures
need to be examined. The allocation x⋆ belongs to three optimal allocation regions of deterministic
occupancy measures: P , P , and P . It is not difficult to see that x⋆ is robust to nonunique
mr mu m
d
best responses. However, x⋆ is not an interior point of any optimal allocation region. First, x⋆
cannot be an interior point of P because Example 9 has already shown that P has an empty
mr mr
interior. Second, x⋆ is not in the interior of P or P since it is possible to perturb x⋆ arbitrarily
mu m
d
small to make m or m no longer a best response of the follower.
u d
8 Computing an Optimal Interior-Point Allocation
For finding an optimal interior-point allocation, Theorem 12 shows that only a finite number of
optimalallocationregions(i.e., theonesofdeterministicoccupancymeasures)needtobeexamined.
Nevertheless, the number of regions can be as large as the number of deterministic policies of the
follower,whichisgivenby|A||S|. Thus,itmaybeimpracticaltoenumeratealltheallocationregions
of deterministic occupancy measures. This section presents a practical method for computing an
optimal interior-point allocation via MILP.
Suppose that the optimal value v⋆ of problem (5) has been obtained using the procedure de-
1
scribed in Section 3.3. Our goal is to find (x,m) that satisfies the following conditions:
1. x is an admissible allocation: x ∈ X.
2. m is an optimal occupancy measure: m ∈ M, and ⟨r ,m⟩ = v⋆.
1 1
3. x is an interior point of P : There exists c > 0 such that (11) holds.
m
An issue with condition (11) is that the condition would lead to infinitely many constraints because
it requires checking every v satisfying ∥v∥
1
≤ 1. Denote by {e i}| iS =d 1| the standard basis of R|S d|.
Recall that the unit ℓ 1-norm ball in R|S d| is the convex hull of {±e i}| iS =d 1|. Since P
m
is a convex set,
condition (11) is equivalent to
17x+ce ∈ P , x−ce ∈ P , i = 1,...,|S | (17)
i m i m d
or, equivalently,
m ∈ BR(x+ce ), m ∈ BR(x−ce ), i = 1,...,|S |. (18)
i i d
According to the KKT conditions in (7), condition (18) holds if and only if there exist ν+ and ν−
i i
for i = 1,...,|S | such that
d
Am = ρ, m ⪰ 0 (19)
ATν+−rx+cei ⪰ 0, ATν−−rx−cei ⪰ 0, i = 1,...,|S | (20)
i 2 i 2 d
m ⊥ ATν+−rx+cei, m ⊥ ATν−−rx−cei, i = 1,...,|S |. (21)
i 2 i 2 d
Rather than an arbitrary optimal interior-point allocation, it is actually possible to find an
optimal interior-point allocation with the maximum margin by solving the following optimization
problem:
maximize c
x,m,c,ν+,ν− (22)
i i
subject to x ∈ X, ⟨r ,m⟩ = v⋆, (19)–(21).
1 1
Let (x⋆,m⋆,c⋆) be an optimal solution of problem (22). If c⋆ > 0, then x⋆ is an interior point of the
optimal allocation region P
m⋆
and hence is an optimal interior-point allocation with margin c⋆.
Similarto(8c), thecomplementarityconstraintsin(21)canbereformulatedasaffineconstraints
with integer variables. Since all the remaining constraints in (22) are affine, problem (22) can be
reformulatedasanMILPandsolvedbyoff-the-shelfsolversincludingGurobi(GurobiOptimization,
LLC, 2024) and CPLEX (IBM, 2022).
9 Interior-Point Reward Functions Are Robust
9.1 Optimal interior-point reward functions
In Section 5, we introduced the concept of an optimal interior-point allocation and proved its
robustness when the leader’s reward function is in a special form given in Section 3.1. In this
section, we will extend the definition of the optimal interior-point allocation in order to study
robust reward design when the leader uses a general reward function. When r is chosen arbitrarily,
1
the following example shows that an interior-point allocation may no longer offer robustness to
nonunique best responses, let alone the other two stronger notions of robustness.
Example 16. Consider a 4×4 grid world in Figure 4. The follower starts from (1,2). The action
space of the follower and the transition kernel are the same as Example 2. We assume a discount
factor of 1. There is no true goal for the follower. The leader is interested in the state (3,3): When
the follower enters (3,3), the leader receives a payoff of 1. The leader’s reward is 0 in all other
states. The reward allocation is only allowed at (4,2), and the allocation budget is 1. Once the
follower enters (4,2), the follower cannot leave the state.
Suppose a positive reward is allocated at (4,2). In this case, both Path 1 and Path 2 lead the
follower to (4,2), the only state that the follower can get rewarded. Denote the reward allocation
by x. Denote the occupancy measures induced by following Path 1 and Path 2 by m and m ,
1 2
respectively. Then, P = P = {x | 0 < x ≤ 1}, and both P and P are optimal allocation
m1 m2 m1 m2
regions. Consider an optimal interior-point allocation, for instance, x = 0.5. When x = 0.5, the
follower is indifferent to taking Path 1 or Path 2. However, the leader’s payoff will be 1 if the
184
Path1
3
Leader’sinterestedstate
Obstacles
Allocationsites
2 Agent
Path2
1
1 2 3 4
Figure 4: The reward design problem with a general leader’s reward function. The leader is
interested in the state (3,3), and the leader’s reward is 1 in this state and 0 in all other states.
The reward allocation is only admissible at (4,2). There is no true goal for the follower.
follower chooses Path 1 and 0 otherwise. This means PessVal(x) = 0 ̸= OptiVal(x) = 1 when x is
an optimal interior-point allocation. The example, however, does not contradict with Proposition 6
because r differs from the one defined in Section 3.1.
1
We first examine how the induced occupancy measure m of the follower is affected by its reward
function. For a given follower’s reward function r˜ ∈ R|S||A|, define the set of best responses of r˜
2 2
by
BR(r˜ ) ≜ argmax⟨r˜ ,m⟩.
2 2
m∈M
ItispossibletodividethespaceR|S||A| ofrewardfunctionsinto(possiblyoverlapping)regionsbased
on the corresponding best response. Each region is called a reward region and is identified by a
unique occupancy measure.
Definition 17 (Reward region). Let m ∈ M be an occupancy measure. The reward region of m is
defined by
P˜ = {r˜ ∈ R|S||A| | m ∈ BR(r˜ )} = {r˜ ∈ R|S||A| | ⟨r˜ ,m⟩ ≥ ⟨r˜ ,m′⟩ for all m′ ∈ M}.
m 2 2 2 2 2
Furthermore, the reward region of an optimal occupancy measure of the reward design problem (5)
is called an optimal reward region.
Thenamerewardregionstandsforthefactthattheregionconsistsofrewardfunctions. Similarly
to Definition 5, it is possible to define interior-point reward functions and optimal interior-point
reward functions.
Definition 18 (Interior-point reward function). Let m ∈ M be an occupancy measure. A reward
function r˜ ∈ R|S||A| is called an interior-point reward function of P˜ if there exists c˜> 0 such that
2 m
r˜ +c˜v ∈ P˜ for all ∥v∥ ≤ 1. (23)
2 m 1
The largest value of c˜ for (23) to hold is called the reward margin of r˜ . An interior-point of an
2
optimal reward region is called an optimal interior-point reward function.
199.2 Robustness of optimal interior-point reward functions
The properties of optimal interior-point reward functions are similar to those of optimal interior-
point allocations shown in Section 5. Previously, we defined OptiVal(x) and PessVal(x) as the
optimal value of (9) and (10), respectively. Given a reward function r˜ , we abuse the notation and
2
similarly define OptiVal(r˜ ) as the optimal value of
2
maximize ⟨r ,m⟩
1
m∈M
subject to m ∈ BR(r˜ ),
2
and PessVal(r˜ ) as the optimal value of
2
minimize ⟨r ,m⟩
1
m∈M
subject to m ∈ BR(r˜ ).
2
Denote the interior of a reward region P˜ by int(P˜).
Proposition 19. If r˜ ∈ int(P˜ ) for some m, then BR(r˜ ) = {m}.
2 m 2
Proof. Since r˜ ∈ int(P˜ ), for any standard basis vector e (i = 1,...,|S|·|A|) of R|S|·|A|, there
2 m i
exists some c > 0 such that r˜ +ce ,r˜ −ce ∈ P˜ . Therefore, given any m′ ∈ M, it holds that
2 i 2 i m
⟨r˜ +ce ,m⟩ ≥ ⟨r˜ +ce ,m′⟩ or equivalently
2 i 2 i
⟨r˜ ,m⟩ ≥ ⟨r˜ ,m′⟩+⟨ce ,m′−m⟩. (24)
2 2 i
Similarly, it holds that ⟨r˜ −ce ,m⟩ ≥ ⟨r˜ −ce ,m′⟩ or equivalently
2 i 2 i
⟨r˜ ,m⟩ ≥ ⟨r˜ ,m′⟩+⟨ce ,m−m′⟩. (25)
2 2 i
Combine (24) and (25) to obtain
⟨r˜ ,m⟩ ≥ ⟨r˜ ,m′⟩+|⟨ce ,m−m′⟩|. (26)
2 2 i
When m′ ̸= m, there must exist (s,a) such that m(s,a) ̸= m′(s,a). Thus, there exists some
i ∈ {1,2,...,|S|·|A|} such that ⟨e ,m−m′⟩ = m(s,a)−m′(s,a) ̸= 0. It then follows from (26)
i
that ⟨r˜ ,m⟩ > ⟨r˜ ,m′⟩.
2 2
If r˜ is an optimal interior-point reward function, then OptiVal(r˜ ) = v⋆. Furthermore, Propo-
2 2 1
sition 19 shows that the best response of r˜ is unique, which immediately implies OptiVal(r˜ ) =
2 2
PessVal(r˜ ) = v⋆.
2 1
Corollary 20 (Robustness to nonunique best responses). For a reward allocation x, if rx is an
2
optimal interior-point reward function, then OptiVal(rx) = PessVal(rx) = v⋆.
2 2 1
Using the notion of optimal interior-point reward functions, Propositions 6 and 8 can also be
extended to the case of general r .
1
Proposition 21 (Robustness to reward perception of the follower). For a reward allocation x,
if rx is an optimal interior-point reward function with margin c˜ > 0, then OptiVal(rx + δ) =
2 2
PessVal(rx+δ) = v⋆ for all ∥δ∥ < c˜.
2 1 1
Proof. Similar to the proof of Proposition 6.
20Proposition 22 (Robustness to bounded rationality). Suppose that ⟨r ,m⋆⟩ = v⋆ and rx⋆ is an
1 1 2
interior-point reward function of P˜ m⋆. Denote by M⋆
det
≜ M
det
∩BR(r 2x⋆ ) the set of deterministic
optimal occupancy measures under rx⋆. Then for any τ > 0, it holds that
2
(cid:18) (cid:19)
2τ
⟨r ,m⋆⟩ ≥ 1− log|A| v⋆,
1 τ b(1−γ) 1
where m⋆ τ is given by (13), γ is the discount factor, and b = ⟨r 2x⋆ ,m⋆⟩−max m∈M det\M⋆ det⟨r 2x⋆ ,m⟩.
Proof. Similar to the proof of Proposition 8.
Results similar to Proposition 22 hold when the bounded rationality of the follower is charac-
terized by other models, including an alternative entropy-regularized MDP (Appendix C.2) and the
δ-optimal responses (Appendix C.3).
9.3 Existence of an optimal interior-point reward function
Although the existence of an optimal interior-point reward function cannot be shown similarly to
Theorem 14, some insights can still be gained on the relationship between different reward regions.
Thefollowingcorollaryfollows immediatelyfrom Proposition19 onthe uniquenessof best response.
Corollary 23. For any m ,m ∈ M with m ̸= m , int(P˜ )∩P˜ = ∅.
1 2 det 1 2 m1 m2
Proof. Consider r˜ ∈ int(P˜ ). It follows from Proposition 19 that BR(r˜ ) = {m }. This implies
2 m1 2 1
m ∈/ BR(r˜ ) or, equivalently, r˜ ∈/ P˜ .
2 2 2 m2
Theorem 12 can be generalized to the case of general r to characterize the relationship between
1
reward regions of optimal deterministic and randomized occupancy measures.
Proposition 24. Let m⋆ be a randomized optimal occupancy measure and π⋆ ∈ Π(m⋆). For any
m ∈ M det(π⋆), it holds that P˜ m⋆ ⊆ P˜ m, and P˜ m is an optimal reward region.
Proof. The proof is based on Lemma 38 and Lemma 39 in Appendix F and is similar to the proof
of Theorem 12.
Corollary 25. Let m⋆ be an optimal occupancy measure and π⋆ ∈ Π(m⋆). For any m ∈ M (π⋆),
det
if m ̸= m⋆, it holds that P˜ m⋆ ⊆ bd(P˜ m), where bd(P˜ m) is the boundary of P˜ m.
Proof. Since m ̸= m⋆, from Corollary 23, it holds that int(P˜ m)∩P˜ m⋆ = ∅. Furthermore, P˜ m is
a closed set by definition, which implies P˜ m = bd(P˜ m)∪int(P˜ m). Since P˜ m⋆ ⊆ P˜ m according to
Proposition 24, it follows that P˜
m⋆
⊆ bd(P˜ m).
Proposition 24 states the following fact: Suppose the reward region P˜ of some optimal ran-
m⋆
domized occupancy measure m⋆ contains an optimal interior-point reward function with margin
c˜. Let π⋆ ∈ Π(m⋆). Then for any m ∈ M (π⋆), the corresponding reward region P˜ must also
det m
contain an optimal interior-point reward function whose margin is no smaller than c˜. This reveals
the special role of deterministic occupancy measures for the problem.
2110 Numerical Experiments
Inthissection,wenumericallyvalidatetherobustnessofthemaximum-marginoptimalinterior-point
allocation given by (22) and compare it with an arbitrary optimal allocation given by (8). Denote
byx theoptimalinterior-pointallocationgivenby(22), anddenotebyx anarbitraryoptimal
IP MILP
allocation given by (8). We sometimes refer to x as the interior-point solution and x as the
IP MILP
MILP solution. Therobustnessofallocationisevaluatedinthreedifferentenvironments: a6×6grid
world,a10×10gridworld,andaprobabilisticattackgraph. Wewillchecktwonotionsofrobustness:
robustness to nonunique best responses and robustness to a boundedly rational attacker. The
remaining notion of robustness, i.e., robustness to uncertain reward perception of the attacker, will
be demonstrated by the computed margin of allocation. All numerical experiments were performed
on a Macbook Air laptop computer with an Apple M2 processor and 8 GB RAM running macOS
Sonoma 14.3.1. The interior-point solutions and MILP solutions in different environments are
computed using the Python MIP package with Gurobi 11.0.0.
10.1 Environments
10.1.1 6×6 grid world
The 6 × 6 grid world illustrated in Figure 5 is modeled as an MDP with finite state and action
spaces. Each cell is associated with a 2-tuple that represents the horizontal and vertical coordinates
of the cell: The coordinate of the lower-left cell is (1,1), and that of the upper-right cell is (6,6).
The grid world consists of a defender and an attacker, who play the roles of the leader and the
follower, respectively. The attacker always starts from the state (1,3) and can choose one of the
four directions, left, right, up, and down, as the desired direction to move. The attacker will transit
to the adjacent cell in the desired direction with probability 0.8 if the new cell remains within the
boundary. The attacker may also move laterally to a new cell in one of the two directions, each
with probability 0.1. The attacker will never move opposite to the desired direction. In any case,
if the new cell is out of bounds, then the attacker will stay at the current cell. The grid world
has two true goals at (1,6) and (5,4) and two allocated rewards set up by the defender at (5,2)
and (6,5). From the perspective of the attacker, the allocated rewards are indistinguishable from
the true targets. The perceived reward of the attacker for reaching any of the true goals is 1.
The perceived reward for reaching an allocated reward is equal to the allocated resource therein by
the defender. The allocated resource at each allocated reward must be nonnegative, and the total
budget of allocation is 4. The MDP ends when the attacker arrives at a true goal or an allocated
reward. The goal of the defender is to find an allocation strategy that maximizes the probability
for the attacker to receive an allocated reward. This goal is captured by the reward function of the
defender: r (s,a) = 1 ∀(s,a) : s ∈ S , and r (s,a) = 0 ∀(s,a) : s ∈/ S , which is consistent with
1 d 1 d
the form of r introduced in Section 3.1. Using the notation from Section 3, the problem setup is
1
summarized as follows:
• State space S = {1,2,··· ,6}2 represents the cells in the 6×6 grid world.
• Action space A = {left,right,up,down} represents the four desired directions of movement.
• The set S = {s ,s }, where s = (1,4) and s = (4,5), represents the cells at which the
d 1 2 1 2
allocated rewards are located.
• The reward function of the attacker is given by r, and that of the defender is given by r .
1
Both functions are defined in Section 3.1.
226
5
4
Truegoals
Allocationsites
Agent
3
2
1
1 2 3 4 5 6
Figure 5: The 6×6 grid world. The true goals are at (1,6) and (5,4), and the defender can
allocate rewards at (5,2) and (6,5). The attacker always starts from (1,3).
10.1.2 10×10 grid world
The transition kernel, the total budget of allocation, and actions of the attacker in the 10×10 grid
world (Figure 6) are the same as those in the 6×6 grid world. The true goals are at (6,9), (8,5),
and (10,3), and the allocated rewards are at s = (8,1), s = (7,4), and s = (6,9). The attacker
1 2 3
always starts from the state (2,6).
10.1.3 Probabilistic attack graph
Figure 7 shows the probabilistic attack graph used in our numerical experiments. The probabilistic
attack graph is an MDP whose state space consists of all the nodes in the graph. The attacker
has four actions {a,b,c,d}. The transition probabilities of the MDP are defined by the edges of
the graph. For clarity, the graph only shows the possible transitions given action a, where a thick
(resp. thin) arrow represents a high (resp. low) transition probability. For example, when the
attacker is at node 0 and takes action a, the transition probabilities are given by T(0,a,1) = 0.7
andT(0,a,i) = 0.1fori = 2,3,4. AcompletedescriptionofthetransitionfunctionT isprovidedat
the URL in Footnote1. The attacker always starts from state 0. The only true goal is node 10. The
reward allocations are set at {11,12}. Similar to the grid world environments, the perceived reward
oftheattackerforreachingthetruegoalis1. Theperceivedrewardforreachinganallocatedreward
is equal to the allocated resource therein by the defender. The total budget of allocation is 4. The
reward function r of the defender is the same as the one used in the grid world environments.
1
10.2 Robustness to nonunique best responses
10.2.1 Method of validation
Our goal is to verify Proposition 6 that the interior-point solution x has the property that all
IP
its best responses are optimal, i.e., OptiVal(x ) = PessVal(x ) = v⋆. Since OptiVal(x ) = v⋆
IP IP 1 IP 1
1https://www.dropbox.com/s/nyycf57vdry139j/MDPTransition.pdf
2310
9
6
8
7
5 Truegoals
6 Allocationsites
Agent
5
4
4
3
3
2
1
2
1 2 3 4 5 6 7 8 9 10
1
Figure 6: The 10×10 grid world. The true goals are at (6,9), (8,5) and (10,3), and the defender
can allocate rewards at (8,1), (7,4) and (6,9). The attacker always starts from (2,6).
241 5
2 8 12
start 0 9
3 7 11
10
4 6
Figure 7: A probabilistic attack graph.
by the definition of x , it remains to show that PessVal(x ) = v⋆. Although PessVal(x ) can
IP IP 1 IP
be obtained by solving the problem in (10) in theory, the constraint may become infeasible due to
finite numerical precision. Instead, given an optimal allocation x⋆ of the reward design problem in
(5), we solve the relaxed problem
minimize ⟨r ,m⟩
1
m∈M (27)
subject to
max⟨rx⋆ ,m′⟩−⟨rx⋆
,m⟩ ≤ ϵ
2 2
m′∈M
for different values of ϵ. Denote the optimal value of (27) by v (x⋆). If PessVal(x⋆) = v⋆, then v (x⋆)
ϵ 1 ϵ
can be made arbitrarily close to v⋆ by choosing ϵ sufficiently small.
1
Proposition 26. Let x⋆ be an optimal allocation. Then lim v (x⋆) = v⋆ if and only if x⋆ is
ϵ→0+ ϵ 1
robust to nonunique best responses, i.e., OptiVal(x⋆) = PessVal(x⋆) = v⋆.
1
Proof. See Appendix B.2.
10.2.2 6×6 grid world
For the 6 × 6 grid world, the MILP defined in (8) gives an optimal reward allocation x =
MILP
(1.946, 1.774). The defender’s expected payoff is 0.433. In comparison, the maximum-margin
optimal interior-point allocation given by (22) is x = (2.122, 1.869). The defender’s expected
IP
payoff under x is the same as the MILP solution. The margin of the interior-point solution is
IP
0.087.
Wethencomputedv (x )andv (x )underdifferentvaluesofϵbysolvingproblem(27). The
ϵ MILP ϵ IP
results are shown in Table 1 and Figure 8. As ϵ approaches 0, the value of v (x ) also approaches
ϵ IP
v⋆ = 0.433. By Proposition 26, the results support Proposition 6 that the interior-point solution is
1
robust to nonunique best responses. In comparison, x is not an interior-point allocation. The
MILP
value of v (x ) appears to converge as ϵ approaches 0, with the gap v⋆−v (x ) > 0.03 even
ϵ MILP 1 ϵ MILP
when ϵ = 10−10, showing that x is not robust to nonunique best responses.
MILP
25ϵ 10−3 10−4 10−5 10−6 10−7
v (x ) 0.107 0.195 0.378 0.396 0.398
ϵ MILP
v (x ) 0.379 0.425 0.432 0.432 0.433
ϵ IP
Table1: 6×6gridworldcase: Theoptimalvaluesof(27)fortheMILPsolutionandtheinterior-
point solution under different values of ϵ.
0.40
0.35
0.30
0.25
0.20
0.15 v(cid:15)(x1)
v(cid:15)(x2)
v?
0.10 1
10−3 10−4 10−5 10−6 10−7 10−8 10−9 10−10
(cid:15)
Figure8: 6×6gridworldcase: Theoptimalvaluesof(27)fortheMILPsolutionandtheinterior-
point solution under different values of ϵ.
26
ffoyapdetcepxes’rednefeDϵ 10−3 10−4 10−5 10−6 10−6
v (x ) 0.491 0.494 0.494 0.494 0.494
ϵ MILP
v (x ) 0.494 0.495 0.495 0.495 0.495
ϵ IP
Table 2: 10×10 grid world case: The optimal values of (27) for the MILP solution and the
interior-point solution under different values of ϵ.
ϵ 10−3 10−4 10−5 10−6 10−6
v (x ) 0.247 0.248 0.248 0.248 0.248
ϵ MILP
v (x ) 0.654 0.654 0.655 0.655 0.655
ϵ IP
Table 3: Attack graph case: The optimal values of (27) for the MILP solution and the interior-
point solution under different values of ϵ.
10.2.3 10×10 grid world
Forthe10×10gridworld, theoptimalsolutiongivenbytheMILPisx = (1.429, 0.000, 1.466),
MILP
and the interior-point solution is x = (1.836, 0.000, 2.164). The optimal expected payoffs of the
IP
defender are both 0.495 given two reward allocations x and x . The margin of the interior-
MILP IP
point solution is 0.061.
The optimal values of (27) for x and x under different ϵ are given in (2). In this case,
MILP IP
the interior-point solution remains robust to nonunique best responses according to the numerical
results. The MILP solution may also be an optimal interior-point allocation, but it is difficult to
conclude definitively due to finite numerical precision. If the defender’s payoff finally approaches
0.495 under x , then x must be robust to nonunique best responses due to Proposition 26.
MILP MILP
10.2.4 Attack graph
For the probabilistic attack graph, the MILP solution is x = (1.218, 0), and the interior-point
MILP
solution is x = (2.667,1.333) with a margin of 1.333. Both solutions lead to a payoff of 0.655
IP
for the defender. Table 3 shows the optimal values of (27) for x and x under different ϵ.
MILP IP
The numerical results imply that the interior-point solution is robust to nonunique best responses,
whereas the MILP solution is not.
10.3 Robustness to a boundedly rational attacker
For each environment introduced in Section 10.1, we computed the defender’s payoff against a
boundedly rational attacker. Our model of bounded rationality assumes that the attacker solves an
entropy-regularized MDP in (12). The parameter τ in (12) reflects the level of rationality of the
attacker: The smaller the value of τ, the more rational the attacker.
Forthe6×6gridworld, Table4givestheexpectedpayoffsofthedefenderwhenfacingattackers
with different levels of rationality for the interior-point solution x (Row 3) and the MILP solution
IP
x (Row 4), respectively. From Proposition 8, when the interior-point solution is used for
MILP
allocation, a lower bound for the defender’s payoff should approach v⋆ = ⟨r ,m⋆⟩ as τ → 0. Recall
1 1
from Section 10.2.2 that the optimal expected payoff against a rational attacker is given by v⋆ =
1
0.433. It can be seen that the theoretical result given by Proposition 8 is consistent with the
numerical results in Row 3 of Table 4. In comparison, the MILP solution does not enjoy the same
payoff guarantee. The payoff for the MILP solution remains noticeably below v⋆ even when τ is as
1
27τ 10−1 10−2 10−3 10−4 10−5
Optimal solution 1.31×10−3 0.429 0.433 0.433 0.433
Interior-point solution 1.29×10−3 0.429 0.433 0.433 0.433
MILP solution 8.63×10−4 0.335 0.324 0.385 0.414
Table4: 6×6gridworldcase: Expectedpayoffsofthedefenderfortheinterior-pointsolutionand
the MILP solution when facing attackers with different levels of rationality. The optimal payoff
against a rational attacker (τ =0) is given by v⋆ =0.433.
1
τ 10−1 10−2 10−3 10−4 10−5
Optimal solution 0.194 0.489 0.495 0.495 0.495
Interior point solution 1.21×10−3 0.489 0.495 0.495 0.495
MILP solution 1.78×10−4 0.484 0.495 0.495 0.495
Table 5: 10×10 grid world case: Expected payoffs of the defender for the interior-point solution
and the MILP solution when facing attackers with different levels of rationality. The optimal
payoff against a rational attacker (τ =0) is given by v⋆ =0.495.
1
small as 10−5. In addition, the MILP solution performs consistently worse than the interior-point
solution for all values of τ.
For comparison, Row 2 of Table 4 shows the optimal expected payoffs of the defender against
a boundedly rational attacker, where the defender is assumed to know the actual value of τ and
choose the reward allocation accordingly. The expected payoff of the defender for the interior-point
solution is found to be close to the optimal payoff. In other words, the interior-point solution leads
to a near-optimal payoff even when the defender incorrectly assumes a rational attacker.
Similar relationships between the payoffs were observed for the 10 × 10 grid world and the
probabilistic attack graph. The numerical results are shown in Table 5 and Table 6.
10.4 Scalability
This subsection discusses the scalability of solving the optimization problem in (22). Instead of
solving (22) directly, we obtained the optimal value of (22) via the bisection method. Each step of
the bisection checks whether a proposed margin c is feasible. The initial upper bound for c is given
by C, the total budget of allocation, and the initial lower bound for c is 0. For both environments,
the bisection procedure typically ended after 10 to 20 steps. Each bisection step ranged from 0.2
to 354 seconds for the 6×6 grid world and from 1.9 to 85 seconds for the 10×10 grid world. We
suspect that the longer running times for the 6×6 grid world were instance-dependent; the MILP
in (22) for the 6×6 grid world was possibly a harder problem instance than the one for the 10×10
grid world. The running time was also found to depend on whether the proposed margin is feasible.
τ 10−1 10−2 10−3 10−4 10−5
Optimal solution 0.620 0.655 0.655 0.655 0.655
Interior-point solution 0.583 0.655 0.655 0.655 0.655
MILP solution 0.407 0.446 0.446 0.446 0.500
Table 6: Attack graph case: Expected payoffs of the defender for the interior-point solution and
the MILP solution when facing attackers with different levels of rationality. The optimal payoff
against a rational attacker (τ =0) is given by v⋆ =0.655.
1
28Longer running times were observed when the proposed margin was infeasible, especially when the
proposed margin was close to feasibility.
11 Conclusions
We study the problem of reward design for Markov decision processes in a leader-follower setup,
wheretheleaderistaskedwithmodifyingthefollower’srewardfunctiontoinduceapolicyfavorable
to the leader. Existing methods for reward design typically rely on exact knowledge of the follower’s
reward function and can be sensitive to modeling errors. Motivated by the issue of sensitivity, we
present a new method of reward design with robustness guarantees when the follower’s reward
function is unknown. Our method can be viewed as a formal algorithmic counterpart of the folklore
solutionforachievingrobustnessbasedonperturbingtheleader’sstrategy. Therewardmodification
inourmethodisbasedontheconceptofoptimal interior-point allocation. Weshowthatanoptimal
interior-point allocation provably offers robustness to three types of uncertainties, including 1) how
the follower breaks ties in the presence of nonunique best responses, 2) inexact knowledge of how
the follower perceives reward modifications, and 3) bounded rationality of the follower.
We also study the existence of an optimal interior-point allocation when the leader’s reward
function adopts a special form. One complication in finding an optimal interior-point allocation is
thatthecorrespondingoptimalallocationregioncannotbepredeterminedbecausesomemayhavean
empty interior. Nevertheless, our result shows that it suffices to focus on optimal allocation regions
of deterministic occupancy measures. This characterization of the optimal allocation regions leads
to two sufficient and necessary conditions for the existence of an optimal interior-point allocation.
One examines whether the allocation budget needs to be exhausted to achieve the optimal leader’s
payoff, which can be verified numerically. The other requires the existence of an allocation robust
to nonunique best responses of the follower, which establishes the central role that optimal interior-
point allocations play in achieving robustness. When the leader uses a general reward function,
most results can be generalized by replacing the optimal interior-point allocation with the optimal
interior-point reward function. However, a sufficient and necessary condition for the existence of an
optimal interior-point reward function remains an open question.
We further show that an optimal interior-point allocation can be found by mixed-integer linear
programming if such an allocation exists. In fact, the mixed-integer linear program can be used
to find an optimal interior-point allocation with the largest margin. Numerical experiments have
been conducted in several simulation environments inspired by problems in cybersecurity. The
experiments not only validate the theoretical guarantees on robustness but also show that our
method scales to problems of practical size.
Acknowledgments
We would like to thank Tamer Başar and Haifeng Xu for helpful discussions.
References
Banihashem,K.,Singla,A.,Gan,J.,andRadanovic,G.(2022). AdmissiblePolicyTeachingthrough
Reward Design. Proceedings of the AAAI Conference on Artificial Intelligence, 36(6):6037–6045.
Başar, T. (1982). A General Theory for Stackelberg Games with Partial State Information. Large
Scale Systems, 3(1):47–56.
29Başar,T.(1984). AffineIncentiveSchemesforStochasticSystemswithDynamicInformation. SIAM
Journal on Control and Optimization, 22(2):199–210.
Başar, T. (2024). Inducement of Desired Behavior via Soft Policies. International Game Theory
Review, page 2440002.
Ben-Porat, O., Mansour, Y., Moshkovitz, M., and Taitler, B. (2024). Principal-Agent Reward
Shaping in MDPs. Proceedings of the AAAI Conference on Artificial Intelligence, 38(9):9502–
9510.
Bolton, P. and Dewatripont, M. (2005). Contract Theory. MIT Press, Cambridge, MA Londin,
England.
Breton, M., Alj, A., and Haurie, A. (1988). Sequential Stackelberg equilibria in two-person games.
Journal of Optimization Theory and Applications, 59(1):71–97.
Camerer, C. F., Ho, T.-H., and Chong, J.-K. (2004). A Cognitive Hierarchy Model of Games. The
Quarterly Journal of Economics, 119(3):861–898.
Cansever,D.H.andBaşar,T.(1983). Aminimumsensitivityapproachtoincentivedesignproblems.
Large Scale Systems, 5(3):233–244.
Cansever, D. H. and Başar, T. (1985). Optimum/near-optimum incentive policies for stochastic
decision problems involving parametric uncertainty. Automatica, 21(5):575–584.
Chakraborty, S., Bedi, A., Koppel, A., Wang, H., Manocha, D., Wang, M., and Huang, F. (2024).
PARL: A unified framework for policy alignment in reinforcement learning. In The Twelfth
International Conference on Learning Representations (ICLR).
Chen, S., Yang, D., Li, J., Wang, S., Yang, Z., and Wang, Z. (2022). Adaptive Model Design
for Markov Decision Process. In Proceedings of the 39th International Conference on Machine
Learning, pages 3679–3700. PMLR.
Conitzer,V.andSandholm,T.(2006). Computingtheoptimalstrategytocommitto. InProceedings
of the 7th ACM Conference on Electronic Commerce, pages 82–90, Ann Arbor Michigan USA.
ACM.
Dempe,S.andZemkoho,A.B.,editors(2020). Bilevel Optimization: Advances and next Challenges.
Numbervolume161inSpringerOptimizationandItsApplications.Springer, Cham, Switzerland.
Devlin,S.andKudenko,D.(2011). Theoreticalconsiderationsofpotential-basedrewardshapingfor
multi-agentsystems. InThe10thInternationalConferenceonAutonomousAgentsandMultiagent
Systems - Volume 1, AAMAS ’11, pages 225–232, Richland, SC. International Foundation for
Autonomous Agents and Multiagent Systems.
Frigault, M., Wang, L., Singhal, A., and Jajodia, S. (2008). Measuring network security using
dynamic bayesian network. In Proceedings of the 4th ACM workshop on Quality of protection -
QoP ’08, page 23, Alexandria, Virginia, USA. ACM Press.
Gan, J., Han, M., Wu, J., and Xu, H. (2023). Robust Stackelberg Equilibria. In Proceedings of the
24th ACM Conference on Economics and Computation, EC ’23, page 735, New York, NY, USA.
Association for Computing Machinery.
30Gan,J.,Han,M.,Wu,J.,andXu,H.(2024). GeneralizedPrincipal-Agency: Contracts,Information,
Games and Beyond.
Grossman, S. J. and Hart, O. D. (1983). An analysis of the principal-agent problem. Econometrica,
51(1):7–45.
Gurobi Optimization, LLC (2024). Gurobi Optimizer Reference Manual.
Hart, O. and Holmström, B. (1987). The theory of contracts. In Advances in Economic Theory,
pages 71–156. Cambridge University Press, 1 edition.
Hewett, R. and Kijsanayothin, P. (2008). Host-centric model checking for network vulnerability
analysis. In 2008 Annual Computer Security Applications Conference (ACSAC), pages 225–234.
IEEE.
Hicks, J. R. (1935). Marktform und Gleichgewicht. The Economic Journal, 45(178):334–336.
Ho, Y.-C., Luh, P. B., and Olsder, G. J. (1982). A control-theoretic view on incentives. Automatica,
18(2):167–179.
IBM (2022). IBM ILOG CPLEX 22.1.1 User’s Manual.
Jha, S., Sheyner, O., and Wing, J. (2002). Two formal analyses of attack graphs. In Proceedings
15th IEEE Computer Security Foundations Workshop. CSFW-15, pages 49–63. ISSN: 1063-6900.
Kakade, S. and Langford, J. (2002). Approximately Optimal Approximate Reinforcement Learning.
InProceedings of the Nineteenth International Conference on Machine Learning, ICML’02, pages
267–274, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Luce, R. D. (1959). Individual Choice Behavior. Individual Choice Behavior. John Wiley, Oxford,
England.
Ma, H., Han, S., Kamhoua, C., and Fu, J. (2023). Optimal Resource Allocation for Proactive
Defense with Deception in Probabilistic Attack Graphs. In Fu, J., Kroupa, T., and Hayel, Y.,
editors, Decision and Game Theory for Security, volume 14167, pages 215–233. Springer Nature
Switzerland, Cham.
McFadden, D. (1976). Quantal Choice Analysis: A Survey. NBER Chapters, National Bureau of
Economic Research, Inc.
McKelvey, R. D. and Palfrey, T. R. (1995). Quantal Response Equilibria for Normal Form Games.
Games and Economic Behavior, 10(1):6–38.
Myerson, R. B. (1982). Optimal coordination mechanisms in generalized principal–agent problems.
Journal of Mathematical Economics, 10(1):67–81.
Neu, G., Jonsson, A., and Gómez, V. (2017). A unified view of entropy-regularized Markov decision
processes.
Ng, A. Y., Harada, D., and Russell, S. J. (1999). Policy invariance under reward transforma-
tions: Theory and application to reward shaping. In Proceedings of the Sixteenth International
Conference on Machine Learning, ICML ’99, pages 278–287, San Francisco, CA, USA. Morgan
Kaufmann Publishers Inc.
31Nguyen, T. H., Wright, M., Wellman, M. P., and Singh, S. (2018). Multistage Attack Graph
Security Games: Heuristic Strategies, with Empirical Game-Theoretic Analysis. Security and
Communication Networks.
Pan, A., Bhatia, K., and Steinhardt, J. (2022). The effects of reward misspecification: Mapping
and mitigating misaligned models. In International Conference on Learning Representations.
Qin, X., Jiang, F., Cen, M., and Doss, R. (2023). Hybrid cyber defense strategies using Honey-X:
A survey. Computer Networks, 230:109776.
Ratliff,L.J.,Dong,R.,Sekar,S.,andFiez,T.(2019). Aperspectiveonincentivedesign: Challenges
and opportunities. Annual Review of Control, Robotics, and Autonomous Systems, 2:305–338.
Ross, S. A. (1973). The Economic Theory of Agency: The Principal’s Problem. The American
Economic Review, 63(2):134–139.
Salanié, B. (2005). The Economics of Contracts: A Primer. MIT Press, Cambridge, Mass, 2nd ed
edition.
Simon, H. A. (1955). A Behavioral Model of Rational Choice. The Quarterly Journal of Economics,
69(1):99–118.
Thakoor,O.,Jabbari,S.,Aggarwal,P.,Gonzalez,C.,Tambe,M.,andVayanos,P.(2020).Exploiting
Bounded Rationality in Risk-based Cyber Camouflage Games. In Conference on Decision and
Game Theory for Security, page 20.
Vielma,J.P.(2015).Mixedintegerlinearprogrammingformulationtechniques.SIAMRev.,57(1):3–
57.
von Stengel, B. and Zamir, S. (2004). Leadership with commitment to mixed strategies. Technical
Report LSE-CDAM-2004-01, London School of Economics, London, UK.
Yang,R.,Ordonez,F.,andTambe,M.(2012). Computingoptimalstrategyagainstquantalresponse
in security games. In Proceedings of the 11th International Conference on Autonomous Agents
and Multiagent Systems - Volume 2, AAMAS ’12, pages 847–854, Richland, SC. International
Foundation for Autonomous Agents and Multiagent Systems.
Yu, G. and Ho, C.-J. (2022). Environment Design for Biased Decision Makers. In Thirty-First
International Joint Conference on Artificial Intelligence, volume 1, pages 592–598.
Zhang, H., Chen, Y., and Parkes, D. (2009a). A General Approach to Environment Design with
One Agent. In Proceedings of the 21st International Joint Conference on Artificial Intelligence,
IJCAI’09, pages 2002–2008, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.
Zhang, H. and Parkes, D. (2008). Value-based policy teaching with active indirect elicitation. In
Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 1, AAAI’08, pages
208–214. AAAI Press.
Zhang, H., Parkes, D. C., and Chen, Y. (2009b). Policy teaching through reward function learning.
In Proceedings of the 10th ACM Conference on Electronic Commerce, pages 295–304, Stanford
California USA. ACM.
32Zheng, Y.-P. and Basar, T. (1982). Existence and derivation of optimal affine incentive schemes
for Stackelberg games with partial information: A geometric approach. International Journal of
Control, 35(6):997–1011.
A Basic results about MDPs
A.1 Policies and occupancy measures
A.1.1 Proof of Proposition 1
Proof. By definition
(cid:34) ∞ (cid:35)
(cid:88)
m(s,a) = E γtP(s = s,a = a)
π,s0∼ρ t t
t=0
(cid:34) ∞ (cid:35)
(cid:88)
= E γtP(s = s)·π(s,a)
π,s0∼ρ t
t=0
(cid:34) ∞ (cid:35)
(cid:88)
= π(s,a)·E γtP(s = s)
π,s0∼ρ t
t=0
(cid:34) ∞ (cid:35)
(cid:88)(cid:88)
= π(s,a)·E γtP(s = s,a = a)
π,s0∼ρ t t
t=0a∈A
(cid:34) ∞ (cid:35)
(cid:88) (cid:88)
= π(s,a)· E γtP(s = s,a = a)
π,s0∼ρ t t
a∈A t=0
(cid:88)
= π(s,a)· m(s,a).
a∈A
The exchangeability of the summation is due to the absolute convergence of (cid:80)∞ γtP(s = s,a =
t=0 t t
a). If (cid:80) am(s,a) ̸= 0, the definition requires that π(s,a) = (cid:80)m m(s, (a s) ,a). Otherwise, π(s,·) can be an
arbitrary probability distribution over A. a
A.1.2 Occupancy measures induced by deterministic policies
Proposition 27. For any π ,π ∈ Π(m), it holds that M (π ) = M (π ).
1 2 det 1 det 2
Proof. Let S = {s ∈ S | (cid:80) m(s,a) = 0}. Observe that for any π ,π ∈ Π(m),
0 a∈A 1 2
m(s,a)
π (s,a) = π (s,a) = ∀s ∈ S \S . (28)
1 2 (cid:80) 0
m(s,a)
a
Suppose a policy π1 ∈ Π (π ) induces the occupancy measure m1 ∈ M (π ). Consider
det det 1 det det 1
another policy π2 defined by
det
(cid:40)
π1 (s,·) if s ∈ S \S ,
π2 (s,·) ≜ det 0 (29)
det π (s,·) if s ∈ S ,
det 0
whereπ isanarbitrarydeterministicpolicyinΠ (π ). Sinceπ1 andπ arebothdeterministic
det det 2 det det
policies, it is not hard to see that π2 is also a deterministic policy.
det
33We shall first show that π2 ∈ Π (π ). By definition, it suffices to show that π2 (s,a) = 0
det det 2 det
when π (s,a) = 0. When π (s,a) = 0 and s ∈ S , it holds that π (s,a) = 0 by the definition of
2 2 0 det
Π (π ), which implies that π2 (s,a) = 0. On the other hand, when π (s,a) = 0 and s ∈ S\S , it
det 2 det 2 0
follows from (28) that π (s,a) = 0. From the definition of Π (π ), we know π1 (s,a) = 0, which
1 det 1 det
implies that π2 (s,a) = π1 (s,a) = 0 according to (29).
det det
Meanwhile, since π2 (s,·) = π1 (s,·) when s ∈ S \ S , it follows from Proposition 1 that
det det 0
the occupancy measures induced by π2 and π1 are equal. Because π1 is selected arbitrarily
det det det
from Π (π ), it follows that M (π ) ⊆ M (π ). The same procedure can be used to show
det 1 det 1 det 2
M (π ) ⊆ M (π ), which completes the proof.
det 2 det 1
Lemma 28. M consists of all the vertices of M.
det
Before the proof, recall that for a polyhedron P, a point x ∈ P is an extreme point if one
cannot find two points y,z ∈ P that are different from x, and a constant α ∈ (0,1) such that
x = αy+(1−α)z. A point x ∈ P is a vertex if there exists a vector c such that ⟨c,x⟩ > ⟨c,y⟩ for
all y ∈ P \{x}. A polytope is a bounded polyhedron. It is well known that for a polytope, a vertex
is equivalent to an extreme point.
Proof. Consider any m ∈ M . We shall first show that m cannot be expressed as any convex
det det det
combination of two other occupancy measures and hence is an extreme point of M. Assume, for
the sake of contradiction, that
m = αm +(1−α)m
det 1 2
for some m ,m ∈ M \ {m }, where m ̸= m , and α ∈ (0,1). Denote by π , π , and
1 2 det 1 2 det 1
π any policies that induce m , m , and m , respectively. Consider any s ∈ S such that
2 det 1 2
(cid:80) m (s,a) ̸= 0. Because π is a deterministic policy, there exists a unique action a⋆ ∈ A
a∈A det det
such that π (s,a⋆) = 1 and π (s,a) = 0 for all a ∈ A\{a⋆}. Consider any a ∈ A\{a⋆}. Since
det det
π (s,a) = m (s,a)/(cid:80) m (s,a), it follows that m (s,a) = 0. Because m ,m ⪰ 0, this
det det a det det 1 2
implies m (s,a) = m (s,a) = 0. It follows that π and π satisfy π (s,a) = π (s,a) = 0. Since
1 2 1 2 1 2
(cid:80) π (s,a′) = 1 for i ∈ {1,2}, it must hold that π (s,a⋆) = π (s,a⋆) = 1, which implies that
a′∈A i 1 2
π (s,a) = π (s,a) = π (s,a) for all a ∈ A. Since s ∈ S is arbitrarily selected, we know that
1 2 det
π (s,a) = π (s,a) = π (s,a) holds for all (s,a) such that (cid:80) m (s,a) ̸= 0. From Proposi-
1 2 det a∈A det
tion 1, the two policies π ,π induce the same occupancy measures as π , i.e., m = m = m ,
1 2 det 1 2 det
which leads to a contradiction.
Suppose some m ∈/ M is a vertex of M. From the definition of vertices, there is a vector
det
y, viewed as a reward function, such that m is the unique occupancy measure that achieves the
maximum expected reward. However, this is impossible since for any reward function in an MDP,
there must exist a deterministic optimal policy.
A.2 Expected payoff and occupancy measure
Lemma 29. Let x be a reward allocation. Suppose the follower uses a policy π, which induces an
occupancy measure m. The expected payoff of the follower satisfies E [Vπ(s;x)] = ⟨rx,m⟩.
s∼ρ 2 2
Proof. The expected cumulative reward of the follower satisfies
34(cid:34) ∞ (cid:35)
(cid:88)
E [Vπ(s)] = E γtrx(s ,a )
s0∼ρ 2 π,s0∼ρ 2 t t
t=0
 
∞
(cid:88) (cid:88)
= E π,s0∼ρ γtP(s t = s,a t = a)r 2x(s,a)
t=0(s,a)∈S×A
(cid:34) ∞ (cid:35)
(cid:88) (cid:88)
= E γtP(s = s,a = a) rx(s,a)
π,s0∼ρ t t 2
(s,a)∈S×A t=0
(cid:88)
= m(s,a)rx(s,a).
2
(s,a)∈S×A
The exchangeability of the summation is from the absolute convergence of (cid:80)∞ γtP(s = s,a =
t=0 t t
a).
A.3 Useful properties of occupancy measures
(cid:80)
Lemma 30. Let m be an occupancy measure. Then m(s,a) = 1/(1−γ).
(s,a)∈S×A
Proof. By the definition of occupancy measures,
(cid:34) ∞ (cid:35)
(cid:88)
m(s,a) = E γtP(s = s,a = a | s ) .
s0∼ρ t t 0
t=0
Sum over the states and actions to obtain
(cid:34) ∞ (cid:35)
(cid:88) (cid:88) (cid:88)
m(s,a) = E γtP(s = s,a = a | s )
s0∼ρ t t 0
(s,a)∈S×A (s,a)∈S×A t=0
 
∞
(cid:88) (cid:88)
= E s0∼ρ γtP(s t = s,a t = a | s 0)
(s,a)∈S×At=0
 
∞
(cid:88) (cid:88)
= E s0∼ρ γt P(s t = s,a t = a | s 0)
t=0 (s,a)∈S×A
(cid:34) ∞ (cid:35)
(cid:88)
= E γt
s0∼ρ
t=0
1
= .
1−γ
It was possible to change the order of summation because (cid:80)∞ γtP(s = s,a = a | s ) ≤ 1/(1−γ),
t=0 t t 0
which is an absolutely convergent series.
35B Proofs on the robustness to nonunique best response
B.1 Proof of Proposition 6
Lemma 31. Let v be a vector in R|S d|. Suppose x¯ is an interior point of an allocation region P m,
i.e., x¯+cv ∈ P for some constant c > 0 and for all ∥v∥ ≤ 1. For any m′ ∈ M, it holds that
m 1
(cid:12) (cid:12)
⟨rx¯,m−m′⟩ ≥
c·max(cid:12) (cid:12)(cid:88)(cid:0) m(s,a)−m′(s,a)(cid:1)(cid:12)
(cid:12).
2 (cid:12) (cid:12)
s∈S d(cid:12)
a∈A
(cid:12)
Proof. Consider any m′ ∈ M. It follows from the definition of P that
m
0 ≤ min
(cid:8) ⟨rx¯+cv,m⟩−⟨rx¯+cv,m′⟩(cid:9)
2 2
∥v∥1=1
= min ⟨rx¯+cv,m−m′⟩
2
∥v∥1=1
= min
(cid:8) ⟨rx¯,m−m′⟩+c⟨δv,m−m′⟩(cid:9)
2
∥v∥1=1
= ⟨rx¯,m−m′⟩+c min ⟨δv,m−m′⟩,
2
∥v∥1=1
where δv satisfies δv(s,a) = v(s) for all s ∈ S and δv(s,a) = 0 otherwise. Therefore, we have
d
⟨rx¯,m−m′⟩ ≥ −c min ⟨δv,m−m′⟩
2
∥v∥ =1
1
= c max ⟨δv,m′−m⟩
∥v∥ =1
1
(cid:12) (cid:12)
≥
c·max(cid:12) (cid:12)(cid:88)(cid:0) m(s,a)−m′(s,a)(cid:1)(cid:12)
(cid:12),
(cid:12) (cid:12)
s∈S d(cid:12)
a∈A
(cid:12)
which completes the proof.
Proof of Proposition 6. Since x⋆ is an optimal interior-point allocation, there exists an optimal
occupancy measure m⋆ such that x⋆ is an interior point of P m⋆. By Lemma 31, there exists c > 0
such that
(cid:12) (cid:12)
⟨rx⋆ ,m⋆−m⟩ ≥ c·max(cid:12) (cid:12)(cid:88) (m(s,a)−m⋆(s,a))(cid:12) (cid:12) ∀m ∈ M.
2 (cid:12) (cid:12)
s∈S d(cid:12)
a∈A
(cid:12)
When m ∈ BR(x⋆), it holds that ⟨rx⋆ ,m⟩ = ⟨rx⋆ ,m⋆⟩, i.e., ⟨rx⋆ ,m⋆ − m⟩ = 0. We obtain 0 ≥
2 2 2
c·max
s∈S
d(cid:12) (cid:12)(cid:80) a∈A(m(s,a)−m⋆(s,a))(cid:12) (cid:12). This implies
(cid:88)
(m(s,a)−m⋆(s,a)) = 0 ∀s ∈ S .
d
a∈A
By the definition of r , it holds that ⟨r ,m−m⋆⟩ = (cid:80) (cid:80) (m(s,a)−m⋆(s,a)) = 0, implying
1 1 s∈S a∈A
⟨r ,m⟩ = ⟨r ,m⋆⟩. d
1 1
B.2 Proof of Proposition 26
Proof. We first show the if direction. Suppose that x⋆ satisfies ⟨r ,m⟩ = v⋆ for all m ∈ BR(x⋆).
1 1
Consideranyϵ > 0andmsatisfying⟨rx⋆ ,m⋆⟩−⟨rx⋆ ,m⟩ ≤ ϵ. Letb = ⟨rx⋆ ,m⋆⟩−max ⟨rx⋆ ,m⟩
2 2 2 m∈M det\BR(x⋆) 2
36and M⋆ = M ∩BR(x⋆). By Lemma 28, we can express m as a convex combination of deter-
det det
ministic occupancy measures: m = (cid:80)h λ m +(cid:80)l λ m′, where m ∈ M⋆ for i = 1,2,...,h,
i=1 i i j=1 j j i det
m′ ∈ M \M⋆ forj = 1,2,...,l,λ ,λ ≥ 0and(cid:80)h λ +(cid:80)l λ = 1. Foranyi ∈ {1,2,...,h},
j det det i j i=1 i j=1 j
because m ∈ M⋆ , it holds that ⟨rx⋆ ,m ⟩ = ⟨rx⋆ ,m⋆⟩. For any j = {1,2,...,l}, because
i det 2 i 2
m′ ∈ M \M⋆ = M \BR(x⋆), it holds that ⟨rx⋆ ,m⋆⟩−⟨rx⋆ ,m′⟩ ≥ b. Therefore,
j det det det 2 2 j
h l
⟨rx⋆ ,m⋆⟩−⟨rx⋆ ,m⟩ = ⟨rx⋆ ,m⋆⟩−⟨rx⋆ ,(cid:88) λ m ⟩−⟨rx⋆ ,(cid:88) λ m′⟩
2 2 2 2 i i 2 j j
i=1 j=1
h l
= ⟨rx⋆ ,m⋆⟩−(cid:88) λ ⟨rx⋆ ,m ⟩−(cid:88) λ ⟨rx⋆ ,m′⟩
2 i 2 i j 2 j
i=1 j=1
h l
= (cid:88) λ ⟨rx⋆ ,m⋆−m ⟩+(cid:88) λ ⟨rx⋆ ,m⋆−m′⟩
i 2 i j 2 j
i=1 j=1
l
= (cid:88) λ ⟨rx⋆ ,m⋆−m′⟩
j 2 j
j=1
l
(cid:88)
≥ b· λ .
j
j=1
This implies that ϵ ≥ b·(cid:80)l λ , i.e., (cid:80)l λ ≤ ϵ/b. In the meantime, because m ∈∈ M⋆ , it
j=1 j j=1 j i det
follows from the given assumption that ⟨r ,m ⟩ = v⋆ = ⟨r ,m⋆⟩. Thus,
1 i 1 1
h l
(cid:88) (cid:88)
⟨r ,m⋆⟩−⟨r ,m⟩ = ⟨r ,m⋆⟩−⟨r , λ m ⟩−⟨r , λ m′⟩
1 1 1 1 i i 1 j j
i=1 j=1
h l
(cid:88) (cid:88)
= λ ⟨r ,m⋆−m ⟩+ λ ⟨r ,m⋆−m′⟩
i 1 i j 1 j
i=1 j=1
l
(cid:88)
= λ ⟨r ,m⋆−m′⟩.
j 1 j
j=1
When ϵ → 0+, it follows from (cid:80)l λ ≤ ϵ/b that (cid:80)l λ → 0+, which further implies that
j=1 j j=1 j
⟨r ,m⋆⟩ − ⟨r ,m⟩ → 0+. Since m is selected arbitrarily such that ⟨rx⋆ ,m⋆⟩ − ⟨rx⋆ ,m⟩ ≤ ϵ, this
1 1 2 2
implies that v⋆−v (x⋆) → 0+ when ϵ → 0+.
1 ϵ
For the only if direction, we assume for the sake of contradiction that there exists m ∈ BR(x⋆)
suchthat⟨r ,m⋆⟩−⟨r ,m⟩ = c > 0. Thenmisafeasiblesolutionto(27),whichimpliesv⋆−v (x⋆) ≥
1 1 1 ϵ
c for any ϵ > 0. This contradicts the given assumption that v⋆−v (x⋆) → 0+ when ϵ → 0+.
1 ϵ
C Proofs on the robustness to bounded rationality
C.1 Model of bounded rationality
In the original entropy-regularized MDP, the follower chooses a policy π to maximize
Vπ(ρ) ≜ Vπ(ρ)+τ ·H(ρ,π),
τ
37where Vπ(ρ) = E [Vπ(s)] and
s∼ρ 2
(cid:34) ∞ (cid:35)
(cid:88)
H(ρ,π) ≜ E −γtlogπ(s ,a ) .
π,s0∼ρ t t
t=0
Let m be the occupancy measure induced by π. It then holds that
(cid:88) (cid:88) m(s,a)
H(ρ,π) = − m(s,a)logπ(s,a) = − m(s,a)log ,
(cid:80) m(s,a′)
a′∈A
(s,a)∈S×A (s,a)∈S×A
wherethelastequalityfollowsfromProposition1. CombineLemma29toobtainthepayofffunction
in (12).
Proof of Proposition 8. Notice that
(cid:88)
(cid:18) m⋆(s,a) (cid:19)
− m⋆(s,a)log
(cid:80) m⋆(s,a′)
a′∈A
(s,a)∈S×A
(cid:32)(cid:32) (cid:33) (cid:33)
(cid:88) (cid:88)
(cid:18) m⋆(s,a) (cid:19) m⋆(s,a)
= − m⋆(s,a) log
(cid:80) m⋆(s,a′) (cid:80) m⋆(s,a′)
(s,a)∈S×A a∈A
a′∈A a′∈A
(cid:32) (cid:33)
(cid:88) (cid:88)
(cid:88)(cid:18)(cid:18) m⋆(s,a) (cid:19) m⋆(s,a) (cid:19)
= − |A| m⋆(s,a) log
(cid:80) m⋆(s,a′) (cid:80) m⋆(s,a′)
s∈S a∈A a∈A
a′∈A a′∈A
(cid:32) (cid:33)
(cid:88) (cid:88) 1
≤ |A| m⋆(s,a)· log|A|
|A|
s∈S a∈A
(cid:88)(cid:88)
= m⋆(s,a)log|A|
s∈Sa∈A
1
= log|A|.
1−γ
This implies
 
(cid:88)
m⋆(s,a)
(cid:88)
m⋆(s,a)
τ · m⋆(s,a)log (cid:80) m⋆(s,a′) − m⋆ τ(s,a)log (cid:80) τ m⋆(s,a′)
a′∈A a′∈A τ (30)
(s,a)∈S×A (s,a)∈S×A
1
≤ 2τ · ·log|A|.
1−γ
It follows from the optimality of m⋆ that
τ
 
(cid:88)
m⋆(s,a)
(cid:88)
m⋆(s,a)
τ · m⋆(s,a)log (cid:80) m⋆(s,a′) − m⋆ τ(s,a)log (cid:80) τ m⋆(s,a′) (31)
a′∈A a′∈A τ
(s,a)∈S×A (s,a)∈S×A
≥ ⟨rx⋆ ,m⋆⟩−⟨rx⋆ ,m⋆⟩.
2 2 τ
Combine (30) and (31) to obtain
1
⟨rx⋆ ,m⋆⟩−⟨rx⋆ ,m⋆⟩ ≤ 2τ · ·log|A|. (32)
2 2 τ 1−γ
38Let h = |M⋆ | and l = |M \ M⋆ |. Since m⋆ ∈ M, according to Lemma 28, we can
det det det τ
write m⋆ = (cid:80)h λ m +(cid:80)l λ m′, where m ∈ M⋆ for i = 1,2,...,h, m′ ∈ M \M⋆ for
τ i=1 i i j=1 j j i det j det det
j = 1,2,...,l, λ ,λ ≥ 0 and (cid:80)h λ +(cid:80)l λ = 1. We can rewrite the left side of (32) as
i j i=1 i j=1 j
h l
⟨rx⋆ ,m⋆⟩−⟨rx⋆ ,m⋆⟩ = ⟨rx⋆ ,m⋆⟩−⟨rx⋆ ,(cid:88) λ m +(cid:88) λ m′⟩
2 2 τ 2 2 i i j j
i=1 j=1
h l
= (1−(cid:88) λ )⟨rx⋆ ,m⋆⟩−(cid:88) λ ⟨rx⋆ ,m′⟩
i 2 j 2 j
i=1 j=1
l l
= (cid:88) λ ⟨rx⋆ ,m⋆⟩−(cid:88) λ ⟨rx⋆ ,m′⟩
j 2 j 2 j
j=1 j=1
l
= (cid:88) λ ⟨rx⋆ ,m⋆−m′⟩.
j 2 j
j=1
The second equality is because ⟨rx⋆ ,m⋆⟩ = ⟨rx⋆ ,m ⟩ for i = 1,2,...,h, which is obtained by the
2 2 i
assumption that m ∈ M⋆ = M ∩BR(x⋆) ⊆ BR(x⋆). It then follows that
i det det
l
(cid:88) λ ⟨rx⋆ ,m⋆−m′⟩ ≤ 2τ · 1 ·log|A|. (33)
j 2 j 1−γ
j=1
Since M det\M⋆ det is a finite set and hence max m∈M det\M⋆ det⟨r 2x⋆ ,m⟩ = b exists, one can obtain
l l
b(cid:88) λ ≤ (cid:88) λ ⟨rx⋆ ,m⋆−m′⟩.
j j 2 j
j=1 j=1
Combine with (33) to obtain b(cid:80)l λ ≤ 2τ · 1 · log|A|, i.e., (cid:80)l λ ≤ 2τ · 1 · log|A|.
j=1 j 1−γ j=1 j b(1−γ)
Therefore,
h l
(cid:88) (cid:88)
⟨r ,m⋆⟩ = ⟨r , λ m + λ m′⟩
1 τ 1 i i j j
i=1 j=1
h l
(cid:88) (cid:88)
= λ ⟨r ,m⋆⟩+ λ ⟨r ,m′⟩
i 1 j 1 j
i=1 j=1
h
(cid:88)
≥ λ ⟨r ,m⋆⟩
i 1
i=1
l
(cid:88)
= (1− λ )⟨r ,m⋆⟩
j 1
j=1
(cid:18) (cid:19)
1
≥ 1−2τ · ·log|A| ⟨r ,m⋆⟩.
1
b(1−γ)
The second equality follows from the assumption that x⋆ is an optimal interior-point allocation.
From Proposition 6, any best response of x⋆ is an optimal occupancy measure. Thus, ⟨r ,m ⟩ =
1 i
⟨r ,m⋆⟩ since m ∈ BR(x⋆).
1 i
39C.2 Alternative model of bounded rationality
Proposition 32. Suppose that (x⋆,m⋆) is an optimal solution to the reward design problem in (5),
and x⋆ is an interior point of P m⋆. Define M⋆
det
≜ M det∩BR(x⋆), the set of optimal deterministic
occupancy measures under x⋆. Let
(cid:110) (cid:111)
m⋆ = argmax ⟨rx⋆ ,m⟩−τ ·Ent(m) .
τ 2
m∈M
Then for any τ > 0, it holds that
(cid:18) (cid:19)
2τ
⟨r ,m⋆⟩ ≥ 1− |log(|S||A|·(1−γ))| ⟨r ,m⋆⟩,
1 τ b(1−γ) 1
where γ is the discount factor, and b = ⟨r 2x⋆ ,m⋆⟩−max m∈M det\M⋆ det⟨r 2x⋆ ,m⟩.
Proof. Notice that
τ ·(Ent(m⋆)−Ent(m⋆)) ≤ τ|Ent(m⋆)+Ent(m⋆)|
τ τ
(cid:18) (cid:19)
1
= 2τ |log(|S||A|·(1−γ))| .
1−γ
The upper bound of Ent(m⋆)+Ent(m⋆) is derived from the facts that m ∈ M is non-negative and
τ
the sum over states and actions is bounded by 1 by Lemma 30. The rest of the proof is similar
1−γ
to the proof of Proposition 8.
C.3 δ-optimal response as a model for bounded rationality
Consider a follower who chooses a worst response for the leader among all the δ-optimal responses.
Denote by BR δ(x) ≜ {m | max m′∈M⟨r 2x,m′⟩−⟨r 2x,m⟩ ≤ δ} the set of δ-optimal responses under a
reward allocation x. The payoff of the leader under x is given by
min ⟨r ,m⟩.
1
m∈BR (x)
δ
Thefollowingpropositionshowsthatanoptimalinterior-pointallocationisrobustagainstδ-optimal
responses.
Proposition 33. Suppose that (x⋆,m⋆) is an optimal solution to the reward design problem in (5),
and x⋆ is an interior point of P m⋆. Let m⋆
δ
∈ argmin
m∈BR
δ(x⋆)⟨r 1,m⟩ and M⋆
det
= M det∩BR(x⋆).
Then for any δ > 0, the value ⟨r ,m⋆⟩ satisfies
1 δ
(cid:18) (cid:19)
δ
⟨r ,m⋆⟩ ≥ 1− ⟨r ,m⋆⟩,
1 δ b 1
where b = ⟨r 2x⋆ ,m⋆⟩−max m∈M det\M⋆ det⟨r 2x⋆ ,m⟩.
Proof. Notice that
⟨rx⋆ ,m⋆⟩−⟨rx⋆ ,m⋆⟩ ≤ δ
2 2 δ
according to the definition of BR (x⋆). The rest of the proof is similar to the proof of Proposition 8.
δ
40D Proofs on the optimality of deterministic occupancy measures
D.1 Proof of Proposition 10
Lemma 34. Let Qopt be the optimal Q-function of an MDP, and πopt be any deterministic greedy
policywithrespecttoQopt,i.e.,foralls ∈ S,itholdsthatπopt(s,a) = 1forsomea ∈ argmax Qopt(s,a′).
a′∈A
For any optimal policy π (not necessarily deterministic) of the MDP, it holds that
E (cid:2) Qopt(s,a)(cid:3) = maxQopt(s,a′) ∀s ∈ S.
a∼π(s,·)
a′∈A
Proof. Letdπ(s′) = (1−γ)(cid:80)∞ γtP(s = s′ | s = s). Bytheperformancedifferencelemma(Kakade
s t=0 t 0
and Langford, 2002),
Vπ(s)−Vopt(s) = 1−1 γE s′∼dπ
s
(cid:2)E a∼π(s′,·)Qopt(s′,a)−Vopt(s′)(cid:3)
(cid:20) (cid:21)
1
= 1−γE s′∼dπ
s
E a∼π(s′,·)Qopt(s′,a)−m a′∈a AxQopt(s′,a′) .
Meanwhile, it follows from the optimality of π that Vπ(s)−Vopt(s) = 0, which implies that
(cid:20) (cid:21)
E s′∼dπ E a∼π(s′,·)Qopt(s′,a)−maxQopt(s′,a′) = 0.
s a′∈A
Because E a∼π(s′,·)Qopt(s′,a)−max a′∈AQopt(s′,a′) ≤ 0 for all s′ ∈ S, it must hold that
E Qopt(s′,a)−maxQopt(s′,a′) = 0
a∼π(s′,·)
a′∈A
for all s′ ∈ S.
Proof of Proposition 10. If π⋆ ∈ Π , then Π (π⋆) = {π⋆} by definition. Therefore, any π ∈
det det
Π det(π⋆) trivially satisfies ⟨r 2x⋆ ,mπ⟩ = ⟨r 2x⋆ ,m⋆⟩ = max m′∈M⟨r 2x⋆ ,m′⟩.
Consider the case when π⋆ ∈/ Π . Let Vopt and Qopt be the optimal value function and the
det 2 2
optimal Q-function of the follower’s MDP, respectively. Because π⋆ is an optimal policy, it follows
from Lemma 34 that
E Qopt (s,a) = maxQopt (s,a′) ∀s ∈ S. (34)
a∼π⋆(s,·) 2 2
a′∈A
Hence, if π⋆(s,a) ̸= 0 for some (s,a) ∈ S ×A, then
Qopt (s,a) = maxQopt (s,a′) = Vopt (s). (35)
2 2 2
a′∈A
For any s ∈ S, let
A⋆ = {a | π⋆(s,a) ∈ (0,1)},
s
and consider the following two cases.
Case 1: Consider s ∈ S such that A⋆ is nonempty. Since π ∈ Π (π⋆) is deterministic, there
s det
existssomea⋆ ∈ Asuchthatπ(s,a⋆) = 1. FromthedefinitionofΠ (π⋆),itfollowsthatπ⋆(s,a⋆) ̸=
det
0. Use (35) to obtain Qopt (s,a⋆) = Vopt (s). This implies
2 2
E Qopt (s,a) = Qopt (s,a⋆) = Vopt (s).
a∼π(s,·) 2 2 2
41Case 2: Consider s ∈ S such that A⋆ is empty, i.e., π⋆(s,a) ∈ {0,1} for all a ∈ A. Since
s
π ∈ Π (π⋆), one must have π(s,a) = π⋆(s,a) for all a ∈ A. Use (34) to obtain
det
E Qopt (s,a) = E Qopt (s,a) = maxQopt (s,a′) = Vopt (s).
a∼π(s,·) 2 a∼π⋆(s,·) 2 2 2
a′∈A
Therefore, for all s ∈ S, it holds that E Qopt (s,a) = Vopt (s). Applying the performance
a∼π(s,·) 2 2
difference lemma again, one can obtain that for all s ∈ S,
0
1 (cid:104) (cid:105)
V 2π(s 0)−V 2opt (s 0) = 1−γE s∼dπ
s0
E a∼π(s,·)Qo 2pt (s,a)−V 2opt (s) = 0.
(cid:104) (cid:105)
This implies E [Vπ(s ;x⋆)] = E Vopt (s ;x⋆) or, equivalently, ⟨rx⋆ ,mπ⟩ = ⟨rx⋆ ,m⋆⟩ =
s0∼ρ 2 0 s0∼ρ 2 0 2 2
max m′∈M⟨r 2x⋆ ,m′⟩, from which one obtains mπ ∈ BR(x⋆).
D.2 Proof of Proposition 11
Proof. If π⋆ ∈ Π , then Π (π⋆) = {π⋆}, and the proposition holds trivially. Consider the case
det det
when π⋆ ∈/ Π . For any s ∈ S, let
det
A = {a ∈ A | π⋆(s,a) ∈ (0,1)}.
s
Because π⋆ ∈/ Π det, there exists s′ ∈ S such that A s′ is nonempty. Choose a′ ∈ A s′ arbitrarily.
Define a policy π′ as follows: π′(s,·) = π⋆(s,·) when s ̸= s′, and π′(s,a′) = 1 when s = s′. We shall
first show that π′ ∈ BR(x⋆). By Lemma 34, since π⋆ is optimal,
(cid:104) (cid:105)
E Qopt (s′,a) = maxQopt (s′,a).
a∼π⋆(s′,·) 2 2
a∈A
This implies that Qo 2pt (s′,a) = max a∈AQo 2pt (s′,a) for all a ∈ A s′. Because a′ ∈ A s′, it holds that
(cid:104) (cid:105)
E Qopt (s′,a) = Qopt (s′,a′) = maxQopt (s′,a).
a∼π′(s′,·) 2 2 2
a∈A
Meanwhile, for (s,a) such that s ̸= s′, since π′(s,a) = π⋆(s,a),
(cid:104) (cid:105) (cid:104) (cid:105)
E Qopt (s,a) = E Qopt (s,a) = maxQopt (s,a).
a∼π′(s,·) 2 a∼π⋆(s,·) 2 2
a∈A
By the performance difference lemma, π′ is an optimal policy of the follower.
Define V⋆ ≜ Vπ⋆ and Q⋆ ≜ Qπ⋆. Because π′ is an optimal policy of the follower and hence a
1 1 1 1
feasible solution to (1), it follows from the optimality of π⋆ that
Vπ′ (s )−V⋆(s ) ≤ 0. (36)
1 0 1 0
By the performance difference lemma,
V 1π′ (s 0)−V 1⋆(s 0) = 1−1 γE s∼dπ s0′ (cid:2)E a∼π′(s,·)Q⋆ 1(s,a)−V 1⋆(s)(cid:3) . (37)
Since π′(s,·) = π⋆(s,·) for any s ̸= s′, we know that E Q⋆(s,a)−V⋆(s) = 0 for s ̸= s′. It
a∼π′(s,·) 1 1
then follows from (36) and (37) that E Q⋆(s,a)−V⋆(s) ≤ 0 when s = s′, i.e.,
a∼π′(s,·) 1 1
Q⋆(s′,a′) ≤ V⋆(s′).
1 1
42Sinces′ ∈ S anda′ ∈ A s′ werechosenarbitrarily,itholdsthatQ⋆ 1(s,a) ≤ V 1⋆(s)forall(s,a) ∈ S×A
such that π⋆(s,a) ∈ (0,1). However, the inequality can never be achieved, else
E Q⋆(s,a)−V⋆(s) < 0,
a∼π⋆(s,·) 1 1
which violates the Bellman consistency equation. In other words, Q⋆(s,a) = V⋆(s) when π⋆(s,a) ∈
1 1
(0,1). In addition, Q⋆(s,a) = V⋆(s) when π⋆(s,a) = 1. Combine the two conditions to obtain
1 1
Q⋆(s,a) = V⋆(s) when π⋆(s,a) ̸= 0. Consider any π ∈ Π (π⋆). When π(s,a) = 1, it must hold
1 1 det
thatπ⋆(s,a) ̸= 0,andconsequentlyQ⋆(s,a) = V⋆(s). Foranys ∈ S,bytheperformancedifference
1 1 0
lemma,
V 1π(s 0)−V 1⋆(s 0) = 1−1 γE s∼dπ
s0
(cid:2)E a∼π(s,·)Q⋆ 1(s,a)−V 1⋆(s)(cid:3) = 1−1 γE s∼dπ
s0
[V 1⋆(s)−V 1⋆(s)] = 0.
This implies E [Vπ(s )] = E [V⋆(s )] or, equivalently, ⟨r ,mπ⟩ = v⋆.
s0∼ρ 1 0 s0∼ρ 1 0 1 1
E Proofs of the existence of optimal interior-point allocation
E.1 Proof of Theorem 14
Lemma 35. Let X and Y be two sets. Suppose that X is convex with a nonempty interior and that
Y is open. Then either X ∩Y = ∅, or X ∩Y has a nonempty interior.
Proof. The proof is reproduced from a post on StackExchange (see Footnote2). Within this proof,
for a given set X, we shall denote its interior by X◦, its boundary by ∂X, its complement by XC,
and its closure by X.
We first show that ∂X = ∂(X◦). Let c ∈ X◦ andx ∈ ∂X. Without loss of generality, we assume
that x = 0 by a change of coordinates. Then λc ∈ X for 0 ≤ λ ≤ 1 by convexity. Because c ∈ X◦,
there is an open set U ⊆ X◦ that contains c. Define U ≜ {y | y = λu, u ∈ U}. It is not hard to
λ
verify that when 0 < λ ≤ 1, the set U is open, contains λc, and U ⊆ X by the convexity of X,
λ λ
which implies λc ∈ X◦ when 0 < λ ≤ 1. This shows x ∈ X◦. Meanwhile, since x ∈ ∂X, it holds
that x ∈ XC ⊆ X◦C. We obtain that x ∈ X◦∩X◦C = ∂(X◦), which implies ∂X ⊆ ∂(X◦). For the
other direction, recall that XC = X◦C for any X. Use this result to obtain X = (XC)C = XC◦C,
X◦ = (X◦C)C = X◦C◦C, and X◦C = X◦◦C = X◦C. Thus,
∂X = X ∩XC = X◦C ∩XC◦C,
and
∂(X◦) = X◦∩X◦C = X◦C ∩X◦C◦C.
From the fact XC◦C ⊇ X◦C◦C, we obtain ∂X ⊇ ∂(X◦).
Suppose that X∩Y is nonempty, and let x ∈ X∩Y. If x ∈ X◦, then X◦∩Y ̸= ∅ is a nonempty
open set in X∩Y. If x ∈ ∂X, then it follows from the previous discussion that x ∈ ∂(X◦). Because
Y is open, there exists an open ball B(x) ⊆ Y centered at x. Moreover, B(x) ∩ X◦ ̸= ∅ since
x ∈ ∂(X◦). Therefore, B(x)∩X◦ is a nonempty open set in X ∩Y.
Lemma 36. Given a function v: S → R and an occupancy measure m, define
d
(cid:40)
v(s) if s ∈ S ,
δv(s,a) = d (38)
0 otherwise,
and ν(s) = (cid:80) m(s,a). Then |⟨δv,m⟩| ≤ ∥v∥ max ν(s).
a∈A 1 s∈S d
2https://math.stackexchange.com/q/2701244
43Proof. According to the definition,
(cid:88)(cid:88) (cid:88) (cid:88)
⟨δv,m⟩ = m(s,a)δv(s,a) = v(s)m(s,a)
s∈Sa∈A s∈S a∈A
d
(cid:88) (cid:88) (cid:88)
= v(s) m(s,a) = v(s)ν(s). (39)
s∈S a∈A s∈S
d d
Apply Hölder’s inequality to complete the proof.
Lemma 37. Suppose that x ∈ X and m ∈ M satisfy ⟨rx,m⟩ > ⟨rx,m ⟩ for all m ∈ M \{m}.
2 2 det det det
Then P has a nonempty interior, and x is an interior-point allocation of P .
m m
Proof. Let m ∈ M . Define σ(m,m ) ≜ ⟨rx,m − m ⟩. Let x′(v) ≜ x + ϵv, where ϵ =
det det det 2 det
min m′ det∈M det\{m}σ(m,m′ det), and v is any vector satisfying ∥v∥ 1 ≤ 21(1−γ). It follows that 0 <
ϵ ≤ σ(m,m ). Define ν (s) ≜ (cid:80) m (s,a) and ν(s) ≜ (cid:80) m(s,a). It holds that
det det a∈A det a∈A
⟨rx′(v) ,m ⟩−⟨rx′(v) ,m⟩ = ⟨rx+ϵv,m −m⟩
2 det 2 2 det
= ⟨rx,m −m⟩+ϵ·⟨δv,m −m⟩
2 det det
≤ −ϵ+ϵ·⟨δv,m ⟩−ϵ·⟨δv,m⟩
det
1 1
≤ −ϵ+ϵ· (1−γ)·maxν (s)+ϵ· (1−γ)·maxν(s)
det
2 s∈S 2 s∈S
d d
≤ −ϵ+ϵ
= 0, (40)
where δv is defined in (38). The second inequality is due to Lemma 36. The third inequality holds
because (cid:80) m(s,a) is bounded by 1 according to Lemma 30. Because any MDP must
(s,a)∈S×A 1−γ
haveadeterministicoptimaloccupancymeasure,(40)impliesm ∈ BR(x′(v))forall∥v∥ ≤ 1(1−γ).
1 2
In other words, the open ball
1
{x+ϵv | ∥v∥ < (1−γ), ϵ = min σ(m,m′ )}
1 2 m′ ∈M \{m} det
det det
contains x ∈ X and is in P , proving that x is an interior point of P .
m m
Proof of Theorem 14. (Sufficiency) Let (x0,m0) be an optimal solution of (16) satisfying C −
(cid:80) x0(s) > 0. Define
s∈S
d
M⋆(x0) ≜ {m | ⟨r ,m⟩ = ⟨r ,m0⟩}∩(M ∩BR(x0)) (41)
1 1 det
and
b ≜ ⟨rx0 ,m0⟩− max ⟨rx0 ,m′⟩ > 0.
2 2
m′∈M \BR(x0)
det
(cid:110) (cid:16) (cid:17)(cid:111)
Consider x′ = x0 +ϵ· (1−γ) ·1, where ϵ = min b, 1 C −(cid:80) x0(s) > 0. It follows that
2|S d| (1−γ) s∈S d
44x′ ∈ X because
(cid:88) (cid:88) (1−γ)
C − x′(s) = C − x0(s)−|S |·ϵ·
d
2|S |
d
s∈S s∈S
d d
 
(cid:88) 1 (cid:88) (1−γ)
≥ C − x0(s)− C − x0(s)·
(1−γ) 2
s∈S s∈S
d d
 
1 (cid:88)
= C − x0(s)
2
s∈S
d
> 0.
For any m ∈ M, it holds that
x0+ϵ·(1−γ)·1
⟨rx′ ,m⟩ = ⟨r 2|Sd| ,m⟩
2 2
=
⟨rx0
,m⟩+ϵ·
(1−γ) (cid:88)
m(s,a)
2 2|S |
d
(s,a):s∈S
d
(1−γ)
= ⟨rx0 ,m⟩+ϵ· ·⟨r ,m⟩. (42)
2 2|S | 1
d
Furthermore, when m ∈ M \BR(x0), it follows from (42) that
det
(1−γ)
⟨rx′
,m⟩ =
⟨rx0
,m⟩+ϵ· ·⟨r ,m⟩
2 2 2|S | 1
d
(1−γ) 1
≤
⟨rx0
,m⟩+ϵ· ·
2 2|S | 1−γ
d
1
≤
⟨rx0
,m⟩+ϵ·
2 2|S |
d
b
≤
⟨rx0
,m⟩+
2
2
b
≤ ⟨rx0 ,m0⟩−
2
2
b
≤ ⟨rx′ ,m0⟩− , (43)
2
2
where the first inequality is from Lemma 30. In the meantime, consider any m˜ ∈ (cid:0) M ∩BR(x0)(cid:1) \
det
M⋆(x0). It follows that ⟨r ,m˜⟩ ≠ ⟨r ,m0⟩ and ⟨rx0 ,m˜⟩ = ⟨rx0 ,m0⟩ from the definition of M⋆(x0).
1 1 2 2
Since (x0,m0) is an optimal solution of the reward design problem in (5), it holds that ⟨r ,m0⟩ >
1
⟨r ,m˜⟩. Hence, for any m˜ ∈ (cid:0) M ∩BR(x0)(cid:1) \M⋆(x0), it follows from (42) that
1 det
(1−γ)
⟨rx′
,m˜⟩ =
⟨rx0
,m˜⟩+ϵ· ·⟨r ,m˜⟩
2 2 2|S | 1
d
(1−γ)
< ⟨rx0 ,m˜⟩+ϵ· ·⟨r ,m0⟩
2 2|S | 1 (44)
d
(1−γ)
= ⟨rx0 ,m0⟩+ϵ· ·⟨r ,m0⟩
2 2|S | 1
d
= ⟨rx′ ,m0⟩.
2
45Letc = ⟨rx′ ,m0⟩−max ⟨rx′ ,m′⟩. Itfollowsfrom(44)thatc > 0. Letd =
m′∈(M ∩BR(x0))\M⋆(x0)
min(cid:8)1b,c(cid:9) > 0. Recall that Mdet = (cid:0) M \BR(x0)(cid:1) ∪(cid:0)(cid:0) M ∩BR(x0)(cid:1) \M⋆(x0)(cid:1) ∪M⋆(x0).
2 det det det
For any m′ ∈ M \M⋆(x0) = (cid:0) M \BR(x0)(cid:1) ∪(cid:0)(cid:0) M ∩BR(x0)(cid:1) \M⋆(x0)(cid:1), from (43) and
det det det
(44), we obtain that
(cid:26) (cid:27)
1
⟨rx′ ,m′⟩ ≤ ⟨rx′ ,m0⟩−min b,c
2 2
2
= ⟨rx′ ,m0⟩−d.
2
We will continue the proof by considering two mutually exclusive cases.
Case 1: Suppose for all s ∈ S and m ∈ M⋆(x0), it holds that
d
(cid:88) (cid:88)
m(s,a) = m0(s,a). (45)
a∈A a∈A
For any m ∈ M⋆(x0), it follows from (41) that ⟨rx0 ,m0⟩ = ⟨rx0 ,m⟩ and ⟨r ,m0⟩ = ⟨r ,m⟩. Use
2 2 1 1
(42) to obtain
(1−γ)
⟨rx′ ,m0⟩ = ⟨rx0 ,m0⟩+ϵ· ·⟨r ,m0⟩
2 2 2|S | 1
d
(1−γ) (46)
=
⟨rx0
,m⟩+ϵ· ·⟨r ,m⟩
2 2|S | 1
d
=
⟨rx′
,m⟩.
2
Given a function v: S → R, define δv as in (38). In the following, we sometimes abuse the notation
d
and treat v as a vector in R|S d|. Define x′(v) ≜ x′+dv. For any m ∈ M⋆(x0) and v ∈ R|S d|,
⟨rx′(v) ,m0⟩ = ⟨rx′+dv,m0⟩ = ⟨rx′ ,m0⟩+d·⟨δv,m0⟩ = ⟨rx′ ,m⟩+d·⟨δv,m0⟩. (47)
2 2 2 2
From (39) and (45),
(cid:88) (cid:88) (cid:88) (cid:88)
⟨δv,m0⟩ = v(s) m0(s,a) = v(s) m(s,a) = ⟨δv,m⟩. (48)
s∈S a∈A s∈S a∈A
d d
Substitute (48) into (47) to obtain
⟨rx′(v) ,m0⟩ = ⟨rx′ ,m⟩+d·⟨δv,m⟩ = ⟨rx′(v) ,m⟩. (49)
2 2 2
Furthermore, when ∥v∥ ≤ 1(1−γ), for any m′ ∈ M \M⋆(x0), it holds that
1 2 det
⟨rx′(v) ,m′⟩ = ⟨rx′+dv,m′⟩
2 2
= ⟨rx′ ,m′⟩+d·⟨δv,m′⟩
2
≤ ⟨rx′ ,m′⟩+d· 1 (1−γ)·max(cid:88) m′(s,a)
2
2 s∈S
da∈A
1
≤ ⟨rx′ ,m′⟩+ d
2
2
1
≤ ⟨rx′ ,m0⟩− d
2
2
≤ ⟨rx′ ,m0⟩−d· 1 (1−γ)·max(cid:88) m0(s,a)
2
2 s∈S
da∈A
46≤ ⟨rx′ ,m0⟩+d·⟨δv,m⟩
2
=
⟨rx′+dv,m0⟩
2
= ⟨rx′(v) ,m0⟩. (50)
2
The first inequality is from Lemma 36. From (49) and (50), it follows that ⟨rx′(v) ,m0⟩ ≥ ⟨rx′(v) ,m⟩
2 2
for any m ∈ M and ∥v∥ < (1−γ)/2. This implies that the open ball
det 1
1
{x′+dv | ∥v∥ < (1−γ)}
1
2
is contained in P . Since x′ ∈ X, and m0 is an optimal occupancy measure, x′ must be an optimal
m0
interior-point allocation.
Case 2: Consider m ∈ M⋆(x0). When condition (45) in Case 1 does not hold, there must exist
some s⋆ ∈ S such that
d
(cid:88) (cid:88)
min m(s⋆,a) ̸= m0(s⋆,a). (51)
m∈M⋆(x0)
a∈A a∈A
The minimum on the left side of (51) exists because M⋆(x0) is finite. Moreover, there must exist
m ∈ M⋆(x0) satisfying (cid:80) m (s⋆,a) = min (cid:80) m(s⋆,a). Consider x′′ such that
⋆ a∈A ⋆ m∈M⋆(x0) a∈A
(cid:110) (cid:111)
x′′(s) = x′(s) when s ∈ S d \ {s⋆} and x′′(s⋆) = x′(s⋆) − min 2(cid:80) a∈Ad m⋆(s⋆,a),ϵ· ( 21 |− S dγ |) . Notice
that x′′(s⋆) ≥ 0 because x′(s⋆) ≥ ϵ · (1−γ). Moreover, since C − (cid:80) x′(s) ≥ 0, it holds that
C − (cid:80) x′′(s) ≥ 0, implying that
2 x|S ′′d|
∈ X. Since m ∈
M⋆(x0s )∈ ,S d
it follows from (46) that
s∈S ⋆
d
⟨rx′ ,m ⟩ = ⟨rx′ ,m0⟩. Thus, for any m ∈ M \M⋆(x0),
2 ⋆ 2 det
⟨rx′′ ,m ⟩ ≥ ⟨rx′ ,m ⟩− d (cid:88) m (s⋆,a)
2 ⋆ 2 ⋆ 2(cid:80) m (s⋆,a) ⋆
a∈A ⋆ a∈A
d
=
⟨rx′
,m ⟩−
2 ⋆
2
d
= ⟨rx′ ,m0⟩−
2
2
> ⟨rx′ ,m0⟩−d
2
≥
⟨rx′
,m⟩
2
≥ ⟨rx′′ ,m⟩. (52)
2
In the meantime, for any m′ ∈ M⋆(x0)\{m },
⋆
(cid:26) (cid:27)
⟨rx′′ ,m ⟩ = ⟨rx′ ,m ⟩−min d ,ϵ· (1−γ) · (cid:88) m (s⋆,a)
2 ⋆ 2 ⋆ 2(cid:80) m (s⋆,a) 2|S | ⋆
a∈A ⋆ d a∈A
(cid:26) (cid:27)
> ⟨rx′ ,m ⟩−min d ,ϵ· (1−γ) · (cid:88) m′(s⋆,a)
2 ⋆ 2(cid:80) a∈Am ⋆(s⋆,a) 2|S d|
a∈A
(53)
(cid:26) (cid:27)
= ⟨rx′ ,m′⟩−min d ,ϵ· (1−γ) · (cid:88) m′(s⋆,a)
2 2(cid:80) m (s⋆,a) 2|S |
a∈A ⋆ d a∈A
= ⟨rx′′ ,m′⟩.
2
Hence we obtain x′′ ∈ X and ⟨rx′′ ,m ⟩ > ⟨rx′′ ,m ⟩ for all m ∈ M \{m } from (52) and (53).
2 ⋆ 2 det det det ⋆
By Lemma 37, we know that x′′ is an interior-point allocation of P . Since m is also an optimal
m⋆ ⋆
47occupancy measure of the reward design problem in (5) by the definition of M⋆(x0), it follows that
x′′ is an optimal interior-point allocation.
(Necessity) Assume toward a contradiction that x⋆ is an optimal interior-point allocation in P
m
for some optimal occupancy measure m. Then there must exist an open set B(x⋆) containing x⋆
with B(x⋆) ⊆ P . Since X is convex, the set B(x⋆)∩X has a nonempty interior by Lemma 35.
m
On the other hand, from the given condition, any x ∈ P must satisfy C−(cid:80) x0 = 0 and is hence
m i i
on the boundary of X. This implies that the interior of P ∩X is empty. Since B(x⋆) ⊆ P , it
m m
follows that the interior of B(x⋆)∩X is also empty, leading to a contradiction.
E.2 Proof of Theorem 15
Proof. Consider m⋆ ∈ BR(x⋆). Define
(cid:110) (cid:111)
BR⋆ ≜ M ∩BR(x⋆) = m ∈ M | ⟨rx⋆ ,m⟩ = ⟨rx⋆ ,m⋆⟩
det det det 2 2
⟨a rnd ,mb ⟩≜ =⟨ vr ⋆2x ,⋆ i, tm h⋆ o⟩ ld− sm tha ax tm ⟨∈ rM ,mdet ⟩\B =R⋆ d ⟨e rt⟨r ,2 mx⋆ ⋆, ⟩m f⟩ o. r aB llec mau ∈se Ba Rn ⋆y .best response m ∈ BR(x⋆) satisfies
1 1 1 1 det
Case 1: Suppose that
(cid:88) (cid:88)
m(s,a) = m′(s,a) (54)
a∈A a∈A
for any m,m′ ∈ BR⋆ and any s ∈ S . Given a function v: S → R, define δv as in (38). In
det d d
the following, we will sometimes abuse the notation and treat v as a vector in R|S d|. Consider any
x′ ∈ {x⋆+ϵv | ∥v∥ < 1}. For any m ∈ BR⋆ , it holds that
1 det
⟨rx′ ,m⋆⟩ = ⟨rx⋆+ϵv,m⋆⟩
2 2
= ⟨rx⋆ ,m⋆⟩+ϵ⟨δv,m⋆⟩
2 (55)
= ⟨rx⋆ ,m⟩+ϵ⟨δv,m⟩
2
=
⟨rx′
,m⟩.
2
The third equality is due to the condition in (54) and the fact that m ∈ BR⋆ . In addition, when
det
ϵ ≤ 1−γ ·b, it holds that
2|A|
⟨rx′ ,m⋆⟩ = ⟨rx⋆ ,m⋆⟩+ϵ⟨δv,m⋆⟩
2 2
> ⟨rx⋆ ,m⋆⟩−ϵ·|A| max |m⋆(s,a)|
2
(s,a):s∈S
d
1
≥ ⟨rx⋆ ,m⋆⟩−ϵ·|A|·
2 1−γ
b
≥ ⟨rx⋆ ,m⋆⟩−
2
2
b
= max ⟨rx⋆ ,m′⟩+
2
m′∈M \BR⋆ 2
det det
(cid:18) (cid:19)
1
≥ max ⟨rx⋆ ,m′⟩+ϵ·|A|·
m′∈M \BR⋆ 2 1−γ
det det
(cid:18) (cid:19)
≥ max ⟨rx⋆ ,m′⟩+ϵ·|A| max |m′(s,a)|
2
m′∈M \BR⋆ (s,a):s∈S
det det d
(cid:16) (cid:17)
≥ max ⟨rx⋆ ,m′⟩+ϵ⟨δv,m′⟩
2
m′∈M \BR⋆
det det
48= max ⟨rx′ ,m′⟩. (56)
2
m′∈M \BR⋆
det det
It follows from (55) and (56) that ⟨rx′ ,m⋆⟩ ≥ ⟨rx′ ,m⟩ for any m ∈ M when ϵ ≤ 1−γ ·b. Hence
2 2 det 2|A|
m⋆ ∈ BR(x′) or, equivalently, x′ ∈ P m⋆. Because x′ is chosen arbitrarily from {x⋆+ϵv | ∥v∥
1
< 1},
it follows that {x⋆ +ϵv | ∥v∥ 1 < 1} ⊂ P m⋆ when ϵ ≤ 21 |− Aγ | ·b and x⋆ ∈ X, implying that x⋆ is an
optimal interior-point allocation.
Case2: If(54)doesnothold,theremustexists ∈ S suchthat(cid:80) m(s ,a) ̸= (cid:80) m′(s ,a)
0 d a∈A 0 a∈A 0
forsomem,m′ ∈ BR⋆ ,BecauseBR⋆ isfinite,theremustexistm ∈ BR⋆ suchthat(cid:80) m (s ,a) =
det det 2 det a∈A 2 0
m forin am ll∈B sR ∈⋆ det S(cid:80) \a∈ {A s m }.(s T0 h, ea n). , fL oe rt ac n> y m0, ∈an Bd Rd ⋆efi ,n ie tx h′ os ldu sch tht ah tat x′(s 0) = x⋆(s 0)−c and x′(s) = x⋆(s)
d 0 det
⟨rx′
,m ⟩ =
⟨rx⋆
,m
⟩−c(cid:88)
m (s ,a)
2 2 2 2 2 0
a∈A
=
⟨rx⋆ ,m⟩−c(cid:88)
m (s ,a)
2 2 0
(57)
a∈A
≥
⟨rx⋆ ,m⟩−c(cid:88)
m(s ,a)
2 0
a∈A
=
⟨rx′
,m⟩.
2
Meanwhile, choose c < b(1−γ). This implies that c(cid:80) m (s ,a) ≤ c/(1−γ) < b. Thus,
a∈A 2 0
⟨rx′
,m ⟩ =
⟨rx⋆
,m
⟩−c(cid:88)
m (s ,a)
2 2 2 2 2 0
a∈A
>
⟨rx⋆
,m ⟩−b
2 2
= ⟨rx⋆ ,m⋆⟩−b
2
= max ⟨rx⋆ ,m′⟩ (58)
2
m′∈M \BR⋆
det det
(cid:32) (cid:33)
≥ max ⟨rx⋆ ,m′⟩−c(cid:88) m′(s ,a)
2 0
m′∈M \BR⋆
det det a∈A
= max ⟨rx′ ,m′⟩.
2
m′∈M \BR⋆
det det
It follows from (57) and (58) that ⟨rx′ ,m ⟩ ≥ max ⟨rx′ ,m⟩. Because any MDP has a
2 2 m∈M det 2
deterministic optimal policy, it holds that max ⟨rx′ ,m⟩ = max ⟨rx′ ,m⟩. This implies
m∈M det 2 m∈M 2
⟨rx′ ,m ⟩ ≥ max ⟨rx′ ,m⟩ or, equivalently, m ∈ BR(x′). Since m ∈ BR⋆ = M ∩BR(x⋆),
2 2 m∈M 2 2 2 det det
and any best response m ∈ BR(x⋆) satisfies ⟨r ,m⟩ = v⋆, it follows that ⟨r ,m ⟩ = v⋆. This implies
1 1 1 2 1
that (x′,m ) is a feasible solution of (16). Since C − (cid:80)|S d| x′ = C − (cid:80)|S d| x⋆ + c ≥ c > 0, the
2 i=1 i i=1 i
optimal value of (16) must be strictly positive. It then follows from Theorem 14 that there exists
an optimal interior-point allocation.
F Proof of Proposition 24
Lemma38. Letm⋆ beanoptimaloccupancymeasureoftherewarddesignproblemin(5), r˜ 2⋆ ∈ P˜ m⋆,
and π⋆ be a policy that induces m⋆, i.e., π⋆ ∈ Π(m⋆). For any π ∈ Π (π⋆), the induced occupancy
det
measure mπ is a best response of x⋆, i.e. mπ ∈ BR(x⋆) or, equivalently, r˜ 2⋆ ∈ P˜ mπ.
49Proof. Similar to Proposition 10.
Lemma 39. Under the same conditions in Lemma 38, the induced occupancy measure mπ also
satisfies ⟨r ,mπ⟩ = v⋆ .
1 1
Proof. Similar to Proposition 11.
Proof of Proposition 24. TheproofisbasedonLemma38andLemma39andissimilartotheproof
of Theorem 12.
50