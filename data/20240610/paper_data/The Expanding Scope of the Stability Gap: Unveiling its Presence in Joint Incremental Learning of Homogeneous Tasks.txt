The Expanding Scope of the Stability Gap: Unveiling its Presence in Joint
Incremental Learning of Homogeneous Tasks
SandeshKamath1,2 AlbinSoutif-Cormerais1,2 JoostvandeWeijer1,2 BogdanRaducanu1,2
1DepartmentofComputerScience,UniversitatAuto`nomadeBarcelona
2ComputerVisionCenter,Barcelona
{skamath, albin, joost, bogdan}@cvc.uab.es
Abstract tion setup: at the start of training a new task, the perfor-
manceofprevioustasksdrasticallydrops, andonlyslowly
Recent research identified a temporary performance recoversduringthesubsequenttrainingofthenewtask. De
drop on previously learned tasks when transitioning to a Langeetal.[10]coinedthetermstabilitygapforthisphe-
newone. Thisdropiscalledthestabilitygapandhasgreat nomenon. This observation should be taken into account
consequencesforcontinuallearning: itcomplicatesthedi- fortheapplicationofcontinuallearningsystems(especially
rect employment of continually learning since the worse- in safety-critical contexts) since it significantly lowers the
case performance at task-boundaries is dramatic, it limits worst-case performance of these algorithms. Furthermore,
its potential as an energy-efficient training paradigm, and it can potentially worsen the final accuracy of the learner,
finally,thestabilitydropcouldresultinareducedfinalper- since it might not recover totally from the knowledge loss
formanceofthealgorithm. Inthispaper,weshowthatthe incurred during the stability gap. Addressing the stability
stability gap also occurs when applying joint incremental gapisthereforeofutmostimportance[6,18].
trainingofhomogeneoustasks.Inthisscenario,thelearner The underlying mechanism responsible for the stabil-
continues training on the same data distribution and has itygapremainsthesubjectoflivelyscientificdebate, with
accesstoalldatafromprevioustasks. Inaddition,weshow no clear explanation available yet. Originally, Caccia et
that in this scenario, there exists a low-loss linear path to al. [1] hypothesized that the cause for the stability gap is
thenextminima,butthatSGDoptimizationdoesnotchoose because old class prototypes receive a large gradient from
this path. We perform further analysis including a finer closely lying new class prototypes. However, this hypoth-
batch-wise analysis which could provide insights towards esis could not fully explain the phenomenon, because the
potentialsolutiondirections. stabilitygaphadalsobeenobservedindomainincremental
learning(wherethesetofclassesremainsthesame)[10].A
possibleremainingexplanationisthefollowing. Whenop-
1.Introduction timizingonnewdata, theobjectiveistominimizetheloss
onboththeavailablenewdataandunavailableolddata.The
Deepneuralnetworksdemonstrateremarkableperformance lossontheunavailablepreviousdataisthenapproximated
across numerous machine-learning tasks. Nevertheless, with various continual learning strategies, such as regular-
when trained on non-IID streaming data these networks ization [9, 11] and data rehearsal [2, 16]. An explanation
struggle to accumulate knowledge, and tend to forget pre- forthestabilitygapcouldbethefailuretoapproximatethis
viously acquired knowledge. Continual learning develops ideal joint loss on previous and current task data. Surpris-
theory and methods to address this problem [3, 12]. It ingly, a recent paper [8], showed that even in the case of
aims to develop algorithms that prevent catastrophic for- jointincrementaltrainingthestabilitygapoccurred(inthis
getting[13]andachieveamorefavorabletrade-offbetween case, wedohaveaccesstobotholdandnewtaskdataand
stabilityandplasticity[14]whilelearningonadatastream. canminimizethejointlossonbotholdandnewdata).They,
Atypicaltestsettingthatcontinuallearningconsidersis therefore,cametotheimportantrealizationthatweshould
learning from a sequence of tasks (each task with another notonlyfocusonwhattooptimizebutmoreimportantlyon
data distribution) [3]. Usually, continual learning method howtooptimizeourobjective.
performance is evaluated at the end of each of the tasks. Hess et al. [8] made their important observation when
Recently, researchers [1, 10] have observed an interesting learning on heterogeneous tasks, referring to the fact that
phenomenon that went unnoticed in this standard evalua- eachtaskisdrawnfromadifferentdistribution. Inthispa-
4202
nuJ
7
]GL.sc[
1v41150.6042:viXraPaper Type Tasks chitectures,VGG-16[17]andResNet-18[7]forourstudy.
Cacciaetal.[1] CI:c ̸=c disjoint heterogeneous
1 2 Training Setup: Our code base uses the pytorch library.
Langeetal.[10] DI:c =c disjoint heterogeneous
1 2 FortrainingweusetheSGDoptimizerwithhyperparame-
Hessetal.[8] CI:c ̸=c jointincr. heterogeneous
1 2
ters: learningrate(lr)of0.01,momentum(m)of0.9,batch
Ours DI:c =c jointincr. homogeneous
1 2
size(bs)of64.
Table 1. Summary of the expanding scope of the stability gap: Notation: In this work, we mainly study the two-task set-
fromheterogeneoustohomogeneoustasks. ting. All results reported will be in the homogeneous task
setting, where the various tasks are drawn from the same
distribution. We use the notation of A-B to indicate task
per,weshowthatthestabilitygapisevenpresentinthecase AwillcontainA%ofthedataandtaskBwillcontainB%
ofjointincrementallearningonhomogeneoustasks(where of the data from the original training dataset. We will use
each task is drawn from the same distribution). This re- thenotationA-B∗toidentifythejointincrementallearning
sultinpresentedinFigure1. So,eveninthecasethatboth setting. InthiscasewhentrainingtaskB,thealgorithmhas
taskshavethesamedistribution,SGDoptimizationdoesnot accesstoallthedataoftaskA.Inpractice, forthissetting
succeed in going to the ‘nearby’ optimal position without fortaskB,wejustcombinethedataofbothtasks,andcon-
derailing through a high-loss region. The only difference tinuetrainingonthecombineddataset. Note, thatthedata
between the new and old data is that the network has seen of task A and B in our paper are disjoint data sets and do
theolddata(typicallyfor100epochshere)andhasnotyet notcontainthesamedatasamples.
seenthenewlyarrivingdata. Wethinkthatthisfurthercon-
Noteonplots: Mostplotsinthispaperarewithawarm-
firmsthefundamentalnatureofthestabilitygapincontinual
startedmodel. ThismeansamodeltrainedontaskAwith
learning: itevenoccursinthemostsimplecontinuallearn-
the data as prescribed in the setting was used to continue
ingsettingwhentrainingfromanincreasingamountofdata
training on task B. The starting point of the x-axis is then
drawn from the same distribution. The main contributions
the iterations directly after the task-switch. This was done
ofthisworkare:
to better study the effect of the actual stability gap. Note,
1. We show that the stability gap also occurs during joint
thatwedonotshowtheendoftrainingontaskB.
incrementallearningfromhomogeneoustasks,arguably
theleastchallengingcontinuallearningsetting. 2.2.StabilityGapinJointIncrementalLearningof
2. We show that there exists a linear low-loss path to the HomogeneousTasks
optimalloss,butthatSGDisnotfollowingthispath(this
To establish the occurrence of the stability gap in the set-
washypothesizedin[8]butwasnotdemonstrated).
tingofjointincrementallearningofhomogeneoustasks,we
3. Weperformananalysisatmini-batchlevel,anddiscover
studythe50-50*setting.Thissettingdividesthetrainingset
thatthegradientjustafterthetaskboundarysuccessfully
intotwoequallysizedtasks,AandB.Bothtasksaredrawn
decreases the mini-batch loss but results in an overall
from the same distribution. The test accuracy is provided
loss increase on the test set. Addressing this might po-
for two datasets in Figure 1. We can observe that even in
tentiallyleadtoasolutiontothestabilitygapproblem.
forthiscase,thereisaclearstabilitygap. Theperformance
This manuscript does not provide a new possible explana-
dropsfrom0.89to0.74onCIFAR-10andfrom0.65to0.38
tion for the stability gap. We think the observation that it
onCIFAR-100. WepositalargergaponCIFAR-100tobe
occursevenforjointincrementallearningofhomogeneous
related to the smaller number of samples per class. Note
tasksisrelevant.Ourresults,confirmthoseofHessetal.[8]
that for both these graphs performance has not returned to
and we agree with them that the focus should shift to how
its task A level consistently even after the 2000 iterations
tooptimizeratherthanwhattooptimize.
showing the long-lasting impact of the stability gap. Af-
ter continued training for around 3500-4500 iterations the
2.StabilityGapAnalysis
models start to achieve more consistently a performance
above0.89and0.65,respectively.
2.1.Experimentalsetup
InTable1weprovideasummaryofthemainpaperson
Datasets:Weusethestandardbenchmarktrain-testsplitfor thestabilitygap. Thestabilitygaphasbeenobservedinin-
allthedatasetsusedinthiswork,thatispubliclyavailable. creasinglygeneralsettings.Here,weshowthatitisalsoob-
CIFAR-10 dataset consists of 60,000 images of 32 × 32 servedforjointincrementaltrainingofhomogeneoustasks,
size, dividedinto10classes: 50,000usedfortrainingand which is arguably the most simple continual learning set-
10,000fortesting. CIFAR-100datasetconsistsof60,000 ting. Thisobservationisrelevantsinceitdiscardsexplana-
images of 32×32 size, divided into 100 classes: 50,000 tionsforthestabilitygapwhicharebasedoncharacteristics
usedfortrainingand10,000fortesting. that are not present (e.g. it cannot be uniquely explained
Architectures: Weconsidertwoconvolutionalnetworkar- bythepresenceofdisjointtasksorheterogeneousdistribu-Figure1. Occurrenceofthestabilitygapinjointincrementallearningwithhomogeneoustasksinthe50-50∗ settingon(left)CIFAR-10
and(right)CIFAR-100datasetsonaResNet-18model. ThisplotstartsaftertrainingwithtaskA,andthex-axisrepresentsthenumberof
iterationsoftrainingontaskB.
tions).
2.3.LinearModeConnectivity
Garipov et al. [5] were the first to study the mode con-
nectivitypropertiesofneuralnetworksweightsbyconnect-
ing two independent minima obtained through differently
seededoptimizationprocessesusingasimplecurvedpathof
lowloss. Frankleetal.[4]latershowedthatasimplerkind
Figure 2. In the 50-50* setting, we present the loss path with
ofpathnaturallyemergesearlyintraining. Theyobserved SGDandthelinearconnectivitylosspathbetweenthewarm-start
thatmodelsthataretrainedfromawarm-startedmodelver- andfinalmodelsusingwithResNet-18modelon(left)CIFAR-10,
siononthesamedatasetbutwithdifferentSGD-noiselead (right)CIFAR-100dataset. Inordertoobservethestabilitygap,
to two checkpoints that are connected by a linear path of wezoominonthefirstfewiterationsofthenewtask.
lowloss.Mirzadehetal.[15]laterextendedthatpropertyto
optimaofmultitaskmodelstrainedonincrementallylarger
datasets.Hessetal.[8]hypothesizedthatthereexistsalow- pathbetweentheinitialandfinalmodelisoflowloss. The
loss path between the optima when doing joint incremen- linear path results confirm that a low-loss path exists be-
tal learning of heterogeneous tasks. However, they do not tween the minima achieved after training task A, and the
demonstratethisintheirpaper.Inthisarticle,weinvestigate minima after training task B. Surprisingly, SGD does not
whetheritisthecasethattrainingonincrementalhomoge- take this path and instead passes through a high-loss area
neoustasksleadstolinearlyconnectedoptimasornot(and beforeconvergingtowardstheminimawhichisoptimalfor
weverifythis).Todoso,wetaketheinitialcheckpointwith taskAandBdata. Wehaveshownherethefirstfewitera-
weightsθ andfinalcheckpointwithweightsθ andinter- tions after the task-switch. Refer to the supplementary for
1 2
polatebetweenthetwobytakingθ =λθ +(1−λθ )with plotsshowingtheentirepathbasedon5epochsoftraining
λ 1 2
λ∈[0,1]. Welatercomputeandreportthetestaccuracyof withtaskB.
eachθ λtodetermineifthelinearpathisoflowloss. Permini-batchlossanalysis. InFigure3,weobservewith
Figure 2 compares the loss of the models obtained by a microscope the learning of the model per mini-batch. In
linearlyinterpolatingbetweentheinitialandfinalmodelto thisplotweshowthetrainingbatchaccuracyforthecurrent
theonesofthemodelcheckpointsalongtheSGDoptimiza- mini-batch before (blue line) and after (red line) the SGD
tiontrajectory. Unsurprisingly,thepathtakenbySGDdur- update. We observe that the SGD update results in a loss
ing optimization is not linear. More surprisingly, it goes decrease(oraccuracyincrease)fortheparticularmini-batch
throughareasofhigherlossespeciallyduringtheinitialpe- (thebluelineisbelowtheredline).However,whenwelook
riod that corresponds to the stability gap, while the linear atthetestaccuracy(blackline),weseethateventhoughini-Figure5.UsingCIFAR-100withResNet-18,stabilitygapin(left)
10-90* (right) 75-25* setting. We can see that the stability gap
increasesforasmaller-sizedfirsttask.
Figure3. UsingCIFAR-100withResNet-18,wepresentthefiner
analysisofthelocalimprovementobtainedatthebatchlevelby
observingthetrainaccuracyperbatchbefore(blueline)andafter
(redline)SGDupdateisappliedforthebatchinthe50-50*setting.
Theblacklineisthecorrespondingtestaccuracy.
Figure 6. Using CIFAR-100 with ResNet-18, stability gap in
(left) 50-50, (right) 75-25 setting. We can see that the stability
gap increases when comparing (left) with the 50-50* setting in
Fig.1(right)and(right)withthe75-25*settinginFig.5(right).
50 and 75-25 which is equal to incremental training with
new data from the same distribution (without access to all
previous data). We observe in Figure 6 that the stability
Figure4. UsingCIFAR-100withVGG-16,stabilitygapin(left) gapoccursinthissettingtooandismorepronouncedthan
50-50*(right)75-25*setting. the corresponding 50-50* and 75*-25* setting studied be-
fore. Thegapislargerfrom0.65to0.20and0.70to0.23as
against0.65to0.38and0.70to0.44,respectively.
tialstepsleadtoalowerlossonthemini-batch,theydonot
resultinbettertestperformance. Theblacklinegoesdown
3.Conclusions
in the initial iterations. This means that the SGD update
movesthenetworkparametersawayfromtheoptimalpath.
In this article, we present compelling insights into the sta-
2.4.AdditionalAnalysis bility gap phenomenon. In particular, we show that it also
manifestswhenapplyingjointincrementaltrainingonase-
Here we verify if the stability gap also occurs for several quence of homogeneous tasks, which is often considered
othersettings. thesimplestscenarioforcontinuallearning.Throughexper-
Stabilitygapusingotherarchitectures. Whilewepresent imentalevidence,wedemonstratethatwhilethelossalong
a detailed study of the stability gap on ResNet-18 archi- the SGD path displays a stability gap, this discrepancy is
tecture, in Figure 4 we show this phenomenon is not re- notmirroredinthelossalongthelineartrajectorybetween
stricted to a specific architecture by using another well- checkpoints. An analysis at the mini-batch level showed
knownVGG-16architectureontheCIFAR-100dataset. thatthegradientcomputedontheinitialmini-batches(after
Stabilitygapinothersettings. InSection2.2,wemainly the task-switch) does reduce the loss for each mini-batch
considered the 50-50* setting which is the joint incremen- but it results in an increased loss on the test data. We also
tal training with homogeneous task. Here, we look at the observethatintheincrementallearningwithhomogeneous
stabilitygapwithdifferentfirsttasksizeandincluderesults tasks, whenweremoverehearsal(going50-50*to50-50),
forthesetting10-90*and75-25*inFigure5. Weobserve thestabilitygapincreases. Infurtherresearch, wewillex-
thatthegapislargerwhenstartingfromasmallerfirsttask. plorethisdirectiontopossiblydiscoverthecauseofthesta-
In addition, we conduct experiments with the splits 50- bilitygapandpossibleremedies.Acknowledgement. WeacknowledgeprojectsTED2021- [11] ZhizhongLiandDerekHoiem. Learningwithoutfor-
132513B-I00 and PID2022-143257NB-I00 funded by getting. IEEE Transactions on Pattern Analysis and
MCIN/AEI/10.13039/501100011033, by European Union MachineIntelligence,40:2935–2947,2016. 1
NextGenerationEU/PRTR,byERDFAWayofMakingEu- [12] Marc Masana, Xialei Liu, Bartłomiej Twardowski,
rope,andbyGeneralitatdeCatalunyaCERCAProgram. Mikel Menta, Andrew D Bagdanov, and Joost Van
De Weijer. Class-incremental learning: survey and
References
performanceevaluationonimageclassification. IEEE
TransactionsonPatternAnalysisandMachineIntelli-
[1] Lucas Caccia, Rahaf Aljundi, Nader Asadi, Tinne
gence,45(5):5513–5533,2022. 1
Tuytelaars, Joelle Pineau, and Eugene Belilovsky.
[13] MichaelMcCloskeyandNealJ.Cohen. Catastrophic
New insights on reducing abrupt representation
interferenceinconnectionistnetworks:Thesequential
change in online continual learning. In International
learningproblem. PsychologyofLearningandMoti-
ConferenceonLearningRepresentations,2022. 1,2
vation,24:109–165,1989. 1
[2] ArslanChaudhry,MarcusRohrbach,MohamedElho-
seiny, Thalaiyasingam Ajanthan, Puneet K. Dokania, [14] Martial Mermillod, Aure´lia Bugaiska, and Patrick
PhilipH.S.Torr,andMarc’AurelioRanzato. Ontiny Bonin. The stability-plasticity dilemma: Investigat-
episodicmemoriesincontinuallearning,2019. 1 ingthecontinuumfromcatastrophicforgettingtoage-
[3] Matthias Delange, Rahaf Aljundi, Marc Masana, limited learning effects. Frontiers in psychology,
SarahParisot,XuJia,AlesLeonardis,GregSlabaugh, 2013. 1
and Tinne Tuytelaars. A continual learning survey: [15] Seyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan
Defyingforgettinginclassificationtasks.IEEETrans. Gorur, Razvan Pascanu, and Hassan Ghasemzadeh.
PatternAnal.Mach.Intell.,2021. 1 Linear mode connectivity in multitask and continual
[4] JonathanFrankle,GintareKarolinaDziugaite,Daniel learning,2020. 3
Roy, and Michael Carbin. Linear mode connectiv- [16] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov,
ity and the lottery ticket hypothesis. In International Georg Sperl, and Christoph H Lampert. icarl: Incre-
Conference on Machine Learning, pages 3259–3269. mentalclassifierandrepresentationlearning. InProc.
PMLR,2020. 3 IEEEConf.Comput.Vis.PatternRecognit.,2017. 1
[5] Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, [17] Karen Simonyan and Andrew Zisserman. Very deep
Dmitry P Vetrov, and Andrew Gordon Wilson. Loss convolutionalnetworksforlarge-scaleimagerecogni-
surfaces, mode connectivity, and fast ensembling of tion,2015. 2
dnns. InAdvancesinNeuralInformationProcessing [18] Albin Soutif-Cormerais, Antonio Carta, and Joost
Systems,2018. 3 van de Weijer. Improving online continual learning
[6] MdYousufHarunandChristopherKanan. Overcom- performance and stability with temporal ensembles.
ingthestabilitygapincontinuallearning,2023. 1 CoRR,abs/2306.16817,2023. 1
[7] KaimingHe,XiangyuZhang,ShaoqingRen,andJian
Sun. Deep residual learning for image recognition,
2015. 2
[8] Timm Hess, Tinne Tuytelaars, and Gido M. van de
Ven. Two complementary perspectives to continual
learning:Asknotonlywhattooptimize,butalsohow,
2023. 1,2,3
[9] James Kirkpatrick, Razvan Pascanu, Neil C. Rabi-
nowitz,JoelVeness,GuillaumeDesjardins,AndreiA.
Rusu,KieranMilan,JohnQuan,TiagoRamalho,Ag-
nieszkaGrabska-Barwinska,DemisHassabis,Claudia
Clopath,DharshanKumaran,andRaiaHadsell. Over-
coming catastrophic forgetting in neural networks.
Proceedings of the National Academy of Sciences,
114:3521–3526,2016. 1
[10] Matthias De Lange, Gido M van de Ven, and Tinne
Tuytelaars. Continualevaluationforlifelonglearning:
Identifyingthestabilitygap. InTheEleventhInterna-
tionalConferenceonLearningRepresentations,2023.
1,2The Expanding Scope of the Stability Gap: Unveiling its Presence in Joint
Incremental Learning of Homogeneous Tasks
Supplementary Material
Wehaveincludedmoredetailedempiricalresultsinthe
supplementarymaterial.
Linear Mode Connectivity In Figure 7 and Figure 8 we
showthefulllosstrajectoryofthesecondtasktrainingfor
5 epochs for both the linear path and the SGD path for
CIFAR-10andCIFAR-100datasets,respectively.
Figure7. Inthe50-50*setting,wepresentthecompletelosspath
withSGDandlinearconnectivitybetweenthewarm-startandfinal
modelsafter5epochsoftrainingusingλchangedinstepsof0.01
onCIFAR-10datasetwithResNet-18model. Inordertoobserve
thestabilitygap,wehadzoomedonthefirst400iterationsinthe
mainpaper.
Figure8. Inthe50-50*setting,wepresentthecompletelosspath
withSGDandlinearconnectivitybetweenthewarm-startandfinal
modelsafter5epochsoftrainingusingλchangedinstepsof0.01
onCIFAR-100datasetwithResNet-18model.Inordertoobserve
thestabilitygap,wehadzoomedonthefirst400iterationsinthe
mainpaper.