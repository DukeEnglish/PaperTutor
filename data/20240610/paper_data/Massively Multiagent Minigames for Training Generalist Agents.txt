Massively Multiagent Minigames for
Training Generalist Agents
KyoungWhanChoe(최경환),RyanSullivan,JosephSuárez
Abstract
We present Meta MMO, a collection of many-agent minigames for use as a re-
inforcement learning benchmark. Meta MMO is built on top of Neural MMO,
a massively multiagent environment that has been the subject of two previous
NeurIPScompetitions.OurworkexpandsNeuralMMOwithseveralcomputation-
allyefficientminigames.WeexploregeneralizationacrossMetaMMObylearning
toplayseveralminigameswithasinglesetofweights.Wereleasetheenvironment,
baselines, and training code under the MIT license. We hope that Meta MMO
willspuradditionalprogressonNeuralMMOand,moregenerally,willserveasa
usefulbenchmarkformany-agentgeneralization.
1 Introduction
Intelligence in the real world requires simultaneous competence on a broad range of tasks. The
firstwaveofmoderndeepreinforcementlearning(RL)researchfocusedonnarrowcompetencyin
individualtasks,suchassingularAtarigames[4,30].Thislineofworkevaluatesgeneralizationusing
stickyactions[29]orbyusingAtarigameswithdifferentmodes[13].Severalmorerecentbenchmarks
includeprocedurallevelgeneration[7,25]thatenablesmorevariationamongenvironmentinstances.
Multi-taskenvironmentslikeXLand[43,44],andMinecraft[15,23,24]introducelargedistributions
ofobjectivesandtrainingscenariosthatdemandevengreatergeneralization.
Ofthese,XLandhas2agentsandisnotpubliclyavailable.Therestaresingle-agent.Incontrast,
NeuralMMOfeatures100+agentsinamulti-task,open-sourceenvironmentthatallowsustostudy
generalizationacrosstasks,opponents,andmaps[40,41].InNeuralMMO,agentsarepresented
withdiversechallengesincludingcollectingresources,engagingincombat,trainingprofessions,and
tradingonaplayer-controlledmarket.MostoftheprogressonNeuralMMOhasbeendrivenby
competitionsatNeurIPSandIJCAItotaling1300+participants.InthemostrecentNeurIPS2023
competition,participantstrainedgoal-conditionedagentscapableofcompletingavarietyoftasks.
Despiteyearsofsustainedinterest,thebestNeuralMMOagentsareonlyproficientatafewtasks
andcannotreachhighlevelsorplayeffectivelyasateam.
MetaMMOextendsNeuralMMOwithadiversesetofminigames.Ourmaincontributionsare:
1. MetaMMOasabenchmarkformany-agentgeneralization.Minigamesfeaturefree-for-all
andteamsettings,built-indomainrandomization,andadaptivedifficulty.
2. Optimizedtrainingupto3xfasterwithminigames.Eachofourexperimentsisrunona
commercialoff-the-shelfdesktopwithasingleRTX4090.
3. Ageneralistagentcapableofplayingseveralminigameswithasinglesetofweights.Itis
trainedusingPPOandasimplecurriculumlearningmethod.
NeuralMMOevaluatesgeneralizationovertasks,opponents,andmaps.MetaMMOenablesfurther
evaluationovervariationsingameplaymechanicsandrunsupto10timesfasterthanNeuralMMO2.
WedemonstratethataRLcanlearnsophisticatedbehaviorsonmultipleindividualandteam-based
Preprint.Underreview.
4202
nuJ
7
]IA.sc[
1v17050.6042:viXraFigure1:MetaMMO’sminigameframeworkenablesfine-grainedcontrolovergameobjectives,agent
spawning,teamassignments,andvariousgameelements.Subsystemsmanageresourcegeneration,
combatrules,NPCbehavior,itemsupply,andmarketdynamics,eachofwhichcanbecustomized
usingconfigurableattributes(seeAppendixA.1formoredetails).Theseconfigurablesettingsprovide
aconvenientmethodforcreatingadaptivedifficulty,allowingfortheimplementationofcurriculum
learningtechniquesthatgraduallyintroduceagentstomorechallengingtasksduringtraining.
minigameswithasinglesetofweightsinlessthanadayoftrainingusingasingleGPU.Tosupport
further research, we release Meta MMO, baselines, and training code1 as free and open-source
softwareundertheMITlicense.
2 MetaMMO
2.1 MinigameDesign
MetaMMOcanbeviewedasasortof"configurationconfiguration"thatallowsuserstospecify
multiple distributions of Neural MMO environments with different gameplay and objectives. As
showninFig1,MetaMMOprovidesfine-grainedcontrolovergameelementsincludingcombat
rules,NPCbehavior,marketrules,mapsize,terrainandresourcegeneration,agentspawningrules,
andwinconditions.Anexplanationofeachgamesystemaswellasalistofconfigurableattributes
islistedinAppendixA.1.MetaMMOalsohooksintotheNeuralMMOtasksystem[41],which
allowsflexibleobjectiveassignmenttoarbitrarygroupsofagents.Oneparticularlyusefulpropertyof
MetaMMOisthatitprovidesaconvenientmethodofcreatingadaptivedifficulty.Thisproducesa
formofcurriculumlearningbywhichagentsaregraduallyintroducedtohardertasksoverthecourse
oftraining(AppendixA.2).Toourknowledge,thesefeaturesareuniquetoourwork:theyarenot
availableinthebaseNeuralMMOenvironmentorinanyothersettingofcomparablecomplexity.
2.2 ImplementedMinigames
Thisworkincludesimplementationsofseveralminigamesthatshowcasetheflexibilityanddiversity
ofMetaMMO.Additionally,weincludetherulesetsofthe2022and2023NeuralMMOcompetitions
asseparateminigamescalledTeamBattleandMulti-taskTrainingrespectively.A.3provideslinksto
asampleofminigamereplays.
SurvivalisthedefaultMetaMMOminigame.Theobjectiveforeachagentistostayaliveuntilthe
endof theepisode(1024ticks). 128agentsare spawnedaroundthe edgeofa 128x128mapand
rewardedforeachticktheysurvive.Bydefault,thisminigamerunswithallthegameelements,butit
canalsobeminimizedtocompetitiveforagingwithnodirectcombat.
TeamBattlereplicatesthe2022NeurIPSNeuralMMOchallenge,wherethelastteamstandingwins.
16teamsof8agentsarespawnedaroundtheedgeofa128x128map,withteammembersstartingin
thesametile.Agentsarerewardedforeachticktheyremainalive.Comparedtothechallenging2022
1https://github.com/kywch/meta-mmo
2Figure 2: Snapshots of King of the Hill (A) and Sandwich (B), showcasing the same policy’s
adaptabilitytodifferentgamesettings.(A)Whentheresourcesubsystemisenabled,teammembers
spreadouttoforageforfoodandwater.(B)Whentheresourcesubsystemisdisabled,eachteam
groupstogethertomaximizetheiroffensiveanddefensivecapabilities.
competition,thisminigameprovidesadditionalconfigurationoptionsforsimplifyingthetask,such
asbydisablingtheneedtoforageforfood.
Multi-taskTraining/Evaluationreplicatesthe2023NeurIPSNeuralMMOchallenge,afree-for-all
that evaluated how well agents could generalize to tasks, opponents, and maps not seen during
training.Weincludethe1,298trainingtasksand63evaluationtasks(AppendixA.4)usedinthe
competition.128agentsarespawnedaroundtheedgeofa128x128mapandassignedrandomtasks
fromthetaskset.Agentsarerewardedforprogresstowardcompletingtheirtask,definedusingthe
original2023competitionmetrics.
Inadditiontoexpandingtheconfigurationoptionsforthecompetitionrulesets,weintroducefour
newminigames.Theseshowcasethediversityofgamesthatcanbecreatedbycombiningexisting
NeuralMMOcomponents.
ProtecttheKingisavariationofTeamBattlewhereeachteamhasadesignatedleader.Iftheleader
dies,theentireteamiseliminated.Succeedinginthisenvironmentrequiresanadditionallayerof
coordinationandstrategy.
RacetotheCenterfocusesonforagingandnavigation.Anagentwinsifitreachesthecentertile
first.Thisminigamerequiresagentstoforageforfoodandwaterefficientlyonthewaytothecenter.
Themapsizecanbeadaptivelyscaledfrom40x40to128x128toincreasethedifficultyasagents
improve(AppendixA.2).
KingoftheHill(Fig.2A)combinesforagingandteamcombatina60x60map.Ateamconsistsof
8agentsandwinsbyseizinganddefendingthecentertileforaspecifiedduration.Ifnoteamhas
3seizedthecenterbythetimetheepisodeends,thereisnowinner.Teamsmustforage,survive,and
fendoffotherteams,makingitdifficulttomaintaincontrolofthehillforlong.Therequireddefense
durationcanalsobeadaptivelyscaledfrom10to200ticksasagentsbecomemoreproficient.
Sandwich(Fig.2B)focusesonteamcombatagainstNPCsandotherteamsinan80x80map.Eight
teams of 16 agents each are spawned in a circle. To win, a team must defeat all other teams and
surviveforatleast500ticks.Thisminigamedoesnotincludeforaging,butitfeaturesthreeexternal
threats:(1)scriptedNPCsspawnedattheedgeofthemap,(2)adeathfogpushingagentstowardsthe
center,and(3)NPCsconstantlybeingspawnedfromthecenterofthemap.Thenumberofspawned
NPCscanbeadaptivelyincreasedthroughouttraining.
2.3 TeamGameSupport
ThecoreNeuralMMOenvironmentdoesnotassumeanyrelationshipsbetweenagents,doesnot
impose any constraints on actions, and does not provide masks based on team assignments. For
example,agentsonthesameteamcanattackandpotentiallykilltheirteammates,andagentscan
giveitemsorgoldtoopposingagents.InMetaMMO,wecreateageneralwrapperforteam-based
minigamesthatimplementsthefollowingfunctions:
ActionMasking:MetaMMOmasksattackactionstargetinganagent’steammates,whichcould
delaylearninginthepreviousiterationsofNeuralMMO.
Shared Reward: Meta MMO implements team-level reward using the built-in task system.
Minigamescandefineandassigntaskstoeachteam.Inthecurrentbaseline,theteamtaskrewardis
addedtotheindividualagents’rewards.
Observation Augmentation: Neural MMO’s observations do not include team information. To
facilitatecollaboration,MetaMMOaugmentstheentityandtileobservationstoindicatewhichagents
belongtowhichteam.Thisledtoanincreaseincoordinationinourexperiments.
Spawning: Neural MMO can be particularly sensitive to initial conditions. Meta MMO can be
configuredtospawnsagentsonthesameteamatthesamelocationontheedgeofthemap.This
behaviorcanbesetperepisode,supportinggame-dependentteamspawning.Minigamescanalsoset
customspawnlocationsorcreatecustomspawningbehaviorsifnecessary.
Communication: Neural MMO provides a basic communication system that lets agents sent an
integertoken(1-127)toallagentswithinvisualrange.MetaMMOprovidesacommunicationprotocol
thatallowsanagenttoinsteadbroadcastitshealth,thenumberofnearbyNPCsandfoes,andthe
presenceofkeytargets.Thiscouldenableagentstoshareinformationbeyondtheirvisualrangeand
developcommunicationprotocols,thoughweleaveathoroughstudyofmulti-agentcommunication
inMetaMMOforfuturework.
Table1:Gamesubsystemsenabledineachminigame.TeamBattlewasusedinbothFullandMini
Configexperimentsbutwithdifferentsubsystemsenabled.Extras:Therestofthesubsystems–Item,
Equipment,Profession,Progression,andExchange.
Experiment Minigame Team Resources Combat NPC Comm Extras
Survival ✓ ✓ ✓ ✓ ✓
FullConfig TeamBattle ✓ ✓ ✓ ✓ ✓ ✓
Multi-taskTraining ✓ ✓ ✓ ✓ ✓
TeamBattle ✓ ✓ ✓ ✓ ✓
MiniConfig ProtecttheKing ✓ ✓ ✓ ✓ ✓
RacetotheCenter ✓
KingoftheHill ✓ ✓ ✓ ✓
Sandwich ✓ ✓ ✓ ✓
4Figure3:TrainingcurvesfortheFullConfigexperiment.Forthegeneralistpolicy,onlysamplesfrom
thetargetminigamewerecounted.Astrainingprogresses,agentslearntosurvivelonger,engagewith
moregamesubsystems(AppendixA.8),andencounterdiverseevents,asevidencedbytheunique
eventcount.
3 Experiments
Wetraingeneralistpoliciescapableofplayingmultiplegameswithasinglesetofweights.Throughout
thissection,wewillrefertotheAppendix,whichcontainsextensiveenvironmentandexperimental
details.OurexperimentsconsidertwosetsofMetaMMOconfigurations.The"full"configuration
featuresresourcecollection,combat,professions,trade,andalloftheothercomplexitiespresentin
NeuralMMO.Inthe"mini"config,eachminigameusesasubsetofthefollowing:team-basedplay,
resourcecollection,combat,NPCs,andcommunication.Foreachconfiguration,wetrainedtwotypes
ofpolicies:specialistsandgeneralist.Specialistpolicieslearntoplayasingleminigame.Generalist
policiesweretrainedonmultipleminigamessimultaneously.Thefullexperimentaldetailsarestated
inAppendixA.5.
Ourmainresultisasfollows:ageneralistcanmatchthecapabilityofaspecialistwhentrained
onthesamenumberofsamplesfromthetargettask.Usingasimplecurriculumlearningmethod,
adding samples from other tasks does not degrade performance on the target task. Instead, the
generalistisabletosimultaneouslysolveseveralminigames.Stateddifferently:generalistpolicies
performed comparably to or better than specialist policies after training on the same number of
samplesofthespecialist’stask,plusextraauxiliarydata.
Ourbaselinebuildsuponthewinningsolutionfromthe2023competition.Thepolicyarchitecture
(AppendixA.6)comprisesencodersfortiles,agents,tasks,items,andmarketinformation,followedby
arecurrentlayer(LSTM[18]),anactiondecoder,andavaluenetworkhead.WeusetheIndependent
PPO(IPPO)algorithm[8,39]withhistoricalself-play,utilizingPufferLib’sCleanPuffeRLscript,
whichextendsCleanRL’sPPOimplementation[19]tosupportmany-agenttraining.
Training and execution are performed in a decentralized manner using only local observations,
allowingflexibleteamsizesandcompositions.Weprovidetrainedcheckpointsatvarioustraining
steps,alongwithscriptsforevaluationusingeitheranEloratingsystemforcompetitivegamesortask
completionmetricstailoredforthemulti-tasksetting(AppendixA.7).Thetrainedmodels,training
scripts,andhyperparametersarepubliclyavailableinourGitHubrepository.
3.1 FullConfigExperiment
WetrainedspecialistpoliciesforSurvival,TeamBattle,andMulti-taskTraining,respectively,anda
generalistpolicythatcanplayallthreeminigames.Figure3showsthetrainingcurvesofthepolicies.
As training progresses, agents learn to survive longer and engage with more game subsystems
(AppendixA.8).
Toevaluatethetrainedpolicies(Figure4),weusedmultiplemetrics.WeusedEloratingforSurvival
andTeamBattle,wherethelaststandingagentorteamisdeclaredthewinner,andtaskcompletion
rate for Multi-task Training. To ensure a fair comparison with the training samples, we selected
5checkpointsat25M,50M,75M,and100Magentstepsforspecialistpolicies,andcheckpointsat
100M,200M,300M,and400Magentstepsforthegeneralistpolicy.
Asasanitycheck,weconfirmedthattrainingonmoresamplesresultsinabetterpolicy.InSurvival
andTeamBattle,weobservedthatthegeneralistpolicyperformedbetterthanthespecialistpolicies
evenwhentrainedwithfewersamples,suggestingthatthegeneralistpolicybenefitedfrompositive
transferlearning.InMulti-taskEvaluation,thegeneralistandspecialistpoliciesperformedcomparably
acrossalltrainingsamplesizestested.
Atthesametime,thetaskcompletionratebelow16%observedinMulti-taskEvaluation,evenfor
thebest-performingcheckpoint,underscoresthesignificantchallengesposedbyMetaMMO.Itis
alsoimportanttonotethattheseevaluationswereconductedina"checkpointvs.checkpoint"setting,
wheretheincreasingcapabilityofopponentsmakesmaintainingcurrentscorelevelsmoredifficult,
furtheremphasizingtheinherentcomplexityofmulti-agentRL.
Figure4:EvaluationsfortheFullConfigexperiment.SeeAppendixA.7formethods.AnElorating
of 1000 represents the initial anchor value. Training samples of the generalist checkpoints were
adjustedbasedontheminigamesamplingratioduringtraining(AppendixA.9).
3.2 MiniConfigExperiment
This section explores the optimization benefits of Meta MMO. Using a restricted set of Neural
MMO’sfeatures,asinMiniConfig,causestheenvironmentrunsfasterandtheactionandobservation
spacestobecomesmaller.Asaresult,theoveralltrainingthroughputcanbeincreasedmorethan
twofoldcomparedtothefullconfiguration(Table2).
Figure5displaysthetrainingcurvesofthepolicieswithdifferentmetricsasproxiesforlearning,
dependingontheminigame.InTeamBattleandProtecttheKing,whichareteamsurvivalgames,
trainedagentssurvivelonger.RacetotheCenteriseasilysolvedbybaselineagents,andaftertraining,
theagents’startinglocationslargelydeterminethewinner;agentsstartingonthenodeshouldtravel
twice as far as those starting on the edges. For King of the Hill, we observed that after training,
possessionofthecentertileswitchedmultipletimesuntiltheend.SeeAppendixA.3forasampleof
replaysforeachminigame.
Wealsoobservedagentsadaptingtheirbehaviorbasedonthegamedynamicscausedbytogglinga
subsystem(Figure2).Whentheresourcesubsystemisenabled,asinKingoftheHill,teammembers
spread out to forage for food and water, because it takes time for a foliage tile to regenerate its
resources.However,whentheresourcesubsystemisdisabled(i.e.,Sandwich),eachteammovesin
tightformations,maximizingtheiroffensiveanddefensivecapabilities.
We used Elo to assess the competency of the trained policies (Figure 6). The generalist policy
outperformedthespecialistpolicieswithlesstrainingsamplesinTeamBattle,ProtecttheKing,Race
totheCenter,andKingoftheHill.Thiswasmostpronouncedinthemorechallengingminigames
likeProtecttheKingandKingoftheHill,wheretheobjectiveswereharder(e.g.,protectingthekey
agentortile).InSandwich,thegeneralistpolicyperformedcomparablytothespecialistpolicy.
6Figure5:TrainingcurvesfortheMiniConfigexperiment,showingmetricsspecifictoeachminigame.
InTeamBattleandProtecttheKing,agentlifespanincreaseswithtraining.InRacetotheCenterand
KingoftheHill,agentslearnedtonavigatemapsandholdthecenterwithin25Msteps.InSandwich,
thegeneralistpolicydidnotconvergetothemaximumNPCmultiplierafter100Msteps.
Figure6:EvaluationsfortheMiniConfigexperiment.Trainingsamplesofthegeneralistcheckpoints
wereadjustedbasedontheminigamesamplingratioduringtraining(AppendixA.9).
4 RelatedWork
Previous works like IMPALA [12] and PopArt [17] have trained multi-task polices on multiple
distinctAtarienvironments.Thefieldofcurriculumlearningandunsupervisedenvironmentdesign
seektotrainagentsthatarecompetentatabroadrangeoftasksinmulti-taskenvironments[9,20,21].
Theseworkstypicallyfocusoncloselyrelatedtasks,suchasenvironmentseedsormapconfigurations.
OtherrecentworkssuchasGato[35],Genie[6],andSIMA[45]learntoplaydiversesetsofgames
fromlarge-scaleofflinedatasetsratherthanonlineinteractionwiththeenvironment.
NetHack[25]andMiniHack[38]exemplifyhowsimplifyingcomplexenvironmentscanaccelerate
research progress. NetHack is a procedurally generated stochastic video game with hundreds of
Table2:Trainingperformance.Throughputistheaverageagentstepspersecondduringtheentire
RLlearningprocess,providingarealisticwalltimeestimatefortraining.
Experiment Minigame/Note AgentSteps Duration Throughput
Survival 100M 9h46m 2858
TeamBattle 100M 9h15m 3019
FullConfig
Multi-taskTraining 100M 11h00m 2535
Generalist 400M 37h17m 2997
TeamBattle 101M 4h08m 6758
ProtecttheKing 100M 4h40m 5976
RacetotheCenter 100M 3h28m 8047
MiniConfig
KingoftheHill 100M 4h11m 6672
Sandwich 100M 3h48m 7359
Generalist 400M 16h17m 6866
2023NeurIPSCompetition
CompetitionBaseline 10M 3h34m 779
NMMO2.0Multi-task
7enemies, items, and playable characters. Winning a game of NetHack is incredibly challenging
evenforproficienthumanplayers,makingitdifficultforresearcherstomakereasonableprogress.
MiniHack was introduced as a small scale, flexible framework for building NetHack levels and
testingspecificobjectives.Thisbenchmarkhasledtosignificantprogressoncurriculumlearning,
unsupervised environment design, and exploration [16, 22]. Similarly, our work takes the most
difficultmany-agentRLbenchmarkandprovidesaflexibletoolfordesigningsmall-scalechallenges
withintheNeuralMMOframework.
Othermany-agentenvironmentsexisttosupportdifferentresearchfocuses.Griddly[3],Megaverse
[34],andMeltingPot[1,27]facilitaterapidprototypingandgenerationofdiversescenarios,but
typicallyinvolvefeweragents.Multi-particleenvironments[28],VMAS[5],JaxMARL[36],and
Gigastep [26] prioritize efficient many-agent communication and coordination, but with simpler
per-agentcomplexity.LuxAI[10,42],SMAC[37],andSMACV2[11]featureheterogeneousagent
teamstrainedonfixedobjectivesandarelimitedtotwo-teamscenarios.Hide-and-Seek[2]teaches
asmallnumberofagentstohidefromtheiropponentsbymanipulatingafewinteractiveobjects.
XLand[43,44]offersadiversetaskspaceanduptothreeagents,butisnotopensourceandrequires
prohibitivelyexpensivetrainingforacademicbudgets.XLand-Minigrid[32]introducedanefficient
grid-basedimplementationofXLand’stasksystembutdoesnotcurrentlysupportmultiagentgames.
Broadly,theyarealleithercomplexenvironmentswithfewagentsorsimpleenvironmentswithmany
agents.
MetaMMOdiffersfromtheseenvironmentsbyintroducingminigamesthatfeaturealargepopulation
ofagents,multipleteamswithflexiblesizes,highper-agentcomplexity,andtheflexibilitytodefine
diverseenvironmentsandobjectives.Theplatformaccommodatesbothintra-teamcollaborationand
inter-teamcompetitiononalargescale.Allofthesefeaturesareprovidedwithinanopen-sourceand
computationallyefficientframework,positioningthisworkasavaluablecontributiontothestudyof
generalizationandskilltransferinmany-agentRL.
5 Discussion
Taskvs.Minigames.NeuralMMO2issufficientlyrich,makingitpossibletodefinemeaningfully
different objectives (e.g., reach the center tile vs. make profit from trade, last team standing vs.
protecttheleader)withinthesamesimulation,similartoXLand[43,44].However,minigameswith
differentsubsystemconfigurationscanleadtoevenmoredistinctchallenges.Forexample,enabling
the resources subsystem encourages agents to spread out and forage, while disabling it with the
combatsubsystemencouragesagentstogroupupandfighttogether(Figure2).
Meta MMO’s minigames also allow researchers to optimize training performance by selectively
enablingsubsystems.Improvementsintheenvironment,infrastructure,andhyperparametershave
resultedin3xfastertrainingcomparedtothepreviouscompetitionbaseline2(Table2).ByusingMeta
MMOtoselectminimalsubsystems,researcherscanalsotriplethetrainingspeedforspecificresearch
questions,thengeneralizebygraduallyaddingcomplexitysimilartotheapproachinMiniHack[38].
MetaMMOsimplifiesgeneralizationexperimentsacrossdiverseminigamesbymaintainingconsistent
observation/actionspaces.Furthermore,sinceMetaMMO’stasksandminigamesaredefinedincode,
itispossibletogeneratenoveltasksandminigamesendlessly,enablingopen-endedlearningbased
onunsupervisedenvironmentdesign[9,47].
Strength of Generalization. While minigames may be more distant from each other than tasks,
theyarestillclosertoeachotherthancompletelyindependentgames.Theysharecommonelements
suchasthestructureofobservationsandbasicgameplayfeatures.Atthesametime,therearefew
successesintheliteratureconcerninggeneralizationatsmallscale,andevenfewerinmany-agent
learningsettings.Weclaimnomethodforevaluatinghowimpressiveourresultsare,savethatour
environmentsarelikelytobeusefultootherresearchers.However,wewouldliketotakeamoment
toaddresstheproblemofevaluationmorebroadly.
Amajordifficultyofworkinthisspaceisthatthereislittleintuitionastowhatweshouldexpect.
ApersonthatplaysoneAtarigamemaythenbeabletolearntoplayasecondmorequickly,buta
personalsobenefitsfromawealthofexternalknowledge.Itisquitelikelythat,fromtheperspective
ofanyreasonabletabularasalearner,twoAtarigameswilllookmuchmoredifferentfromeachother
2https://github.com/NeuralMMO/baselines/tree/2.0
8thantheylooktoahuman.Thismakesquantifying"reasonable"transferperformancedifficult.One
mightassumethatthebroaderthetrainingcurriculum,themorelikelyitisthatthereisroomfor
positivetransfer.InGato[35],theauthorsshowedpositivetransferwitharound600tasks.Inour
case,wearesurprisedthatitworksatallwithonlyahandfuloftasks,eventakingintoaccountthe
relativesimilaritiesofminigamesandthepresenceofdomainrandomization.Previously,wehad
expectedtoonlyachievecompetenceoveronerandomizedminigameperpolicy.
TheMetaMMObaselinehasincorporatedmulti-tasktraining,allowingagentstolearnacomplex
task embedding space produced by large language models (LLMs) and perform zero-shot gener-
alizationtounseentasks.ThesuccessofthisapproachlikelydependsontheLLM’sperformance,
code comprehension,and prompt design. Althoughthe training required forgood generalization
isuncertain,MetaMMO’sfastertrainingisbeneficial.Thesefeaturescollectivelyprovidearich
playgroundforcurriculumlearningresearch.
MultiagentCoordination.Weobservedcompellingteambehaviors,withstrongerteamperformance
emergingfromincreasedtraining.IPPO,whichusesonlylocalobservationsfordecentralizedtraining
and execution, performed well in our experiments, consistent with previous research [8, 11, 48].
IPPO’s advantages include compatibility with arbitrary team sizes and efficiency in training and
inference.Incontrast,poolingallteamagents’observationscansubstantiallyslowtraining;the2022
competitionwinnersolutiontookweekstotrain.Futureresearchshouldexploreothermulti-agent
RLalgorithmstofurtherimproveteamperformanceandtrainingefficiency.MetaMMOprovidesa
complex,yetefficientmany-agentenvironmentthatcandemocratizeresearchincoordination,credit
assignment[33],multiagentautocurricula[2],andtheemergenceoflanguage[31].
Limitations.MetaMMOmayhavegamebalanceissuesasthecapableagentsthatcanstresstest
thegamemechanicsbecameavailableonlyrecently.MetaMMOdoesnothaveaninteractiveclient,
limiting its potential for human-multi-agent collaboration research. The lack of absolute scoring
metrics makes multi-agent evaluation challenging, calling for an openly available diverse policy
populationandpeer-to-peerarena.
PotentialNegativeSocietalImpacts.MetaMMOminigamesareabstractgamesimulationswith
basiccombatandcommercesystems,substantiallydifferentfromreal-worldcounterparts.Webelieve
thatMetaMMOisnotdirectlyapplicabletodevelopingreal-worldsystemswithsocietalimpact.Its
primarygoalistoadvanceresearchonlearningagents’capabilities.
9AcknowledgmentsandDisclosureofFunding
WethankPufferAIforsponsoringthecomputeusedinthiswork.
References
[1] J. P. Agapiou, A. S. Vezhnevets, E. A. Duéñez-Guzmán, J. Matyas, Y. Mao, P. Sunehag,
R.Köster,U.Madhushani,K.Kopparapu,R.Comanescu,D.Strouse,M.B.Johanson,S.Singh,
J.Haas,I.Mordatch,D.Mobbs,andJ.Z.Leibo. Meltingpot2.0,2023.
[2] B.Baker,I.Kanitscheider,T.Markov,Y.Wu,G.Powell,B.McGrew,andI.Mordatch. Emer-
gent tool use from multi-agent autocurricula. In 8th International Conference on Learning
Representations,ICLR2020,AddisAbaba,Ethiopia,April26-30,2020.OpenReview.net,2020.
URLhttps://openreview.net/forum?id=SkxpxJBKwS.
[3] C.Bamford,S.Huang,andS.Lucas. Griddly:Aplatformforairesearchingames,2022.
[4] M.G.Bellemare,Y.Naddaf,J.Veness,andM.Bowling. Thearcadelearningenvironment:An
evaluationplatformforgeneralagents. JournalofArtificialIntelligenceResearch,47:253–279,
June2013. ISSN1076-9757. doi:10.1613/jair.3912. URLhttp://dx.doi.org/10.1613/
jair.3912.
[5] M.Bettini,R.Kortvelesy,J.Blumenkamp,andA.Prorok. Vmas:Avectorizedmulti-agent
simulator for collective robot learning. The 16th International Symposium on Distributed
AutonomousRoboticSystems,2022.
[6] J.Bruce,M.Dennis,A.Edwards,J.Parker-Holder,Y.Shi,E.Hughes,M.Lai,A.Mavalankar,
R.Steigerwald,C.Apps,Y.Aytar,S.Bechtle,F.Behbahani,S.Chan,N.Heess,L.Gonzalez,
S. Osindero, S. Ozair, S. Reed, J. Zhang, K. Zolna, J. Clune, N. de Freitas, S. Singh, and
T.Rocktäschel. Genie:Generativeinteractiveenvironments,2024.
[7] K.Cobbe,C.Hesse,J.Hilton,andJ.Schulman.Leveragingproceduralgenerationtobenchmark
reinforcementlearning. InInternationalconferenceonmachinelearning,pages2048–2056.
PMLR,2020.
[8] C.S.deWitt,T.Gupta,D.Makoviichuk,V.Makoviychuk,P.H.S.Torr,M.Sun,andS.Whiteson.
Isindependentlearningallyouneedinthestarcraftmulti-agentchallenge?,2020.
[9] M.Dennis,N.Jaques,E.Vinitsky,A.Bayen,S.Russell,A.Critch,andS.Levine. Emergent
complexityandzero-shottransferviaunsupervisedenvironmentdesign,2021.
[10] B.Doerschuk-TiberiandS.Tao. LuxAIChallengeSeason1,72021. URLhttps://github.
com/Lux-AI-Challenge/Lux-Design-2021.
[11] B.Ellis,J.Cook,S.Moalla,M.Samvelyan,M.Sun,A.Mahajan,J.N.Foerster,andS.Whiteson.
Smacv2:Animprovedbenchmarkforcooperativemulti-agentreinforcementlearning,2023.
[12] L.Espeholt,H.Soyer,R.Munos,K.Simonyan,V.Mnih,T.Ward,Y.Doron,V.Firoiu,T.Harley,
I.Dunning,S.Legg,andK.Kavukcuoglu. Impala:Scalabledistributeddeep-rlwithimportance
weightedactor-learnerarchitectures,2018.
[13] J. Farebrother, M. C. Machado, and M. Bowling. Generalization and regularization in dqn.
arXivpreprintarXiv:1810.00123,2018.
[14] D. Guo, Q. Zhu, D. Yang, Z. Xie, K. Dong, W. Zhang, G. Chen, X. Bi, Y. Wu, Y. Li, et al.
Deepseek-coder:Whenthelargelanguagemodelmeetsprogramming–theriseofcodeintelli-
gence. arXivpreprintarXiv:2401.14196,2024.
[15] W.H.Guss,M.Y.Castro,S.Devlin,B.Houghton,N.S.Kuno,C.Loomis,S.Milani,S.Mo-
hanty,K.Nakata,R.Salakhutdinov,etal. Theminerl2020competitiononsampleefficient
reinforcementlearningusinghumanpriors. arXivpreprintarXiv:2101.11071,2021.
[16] M. Henaff, R. Raileanu, M. Jiang, and T. Rocktäschel. Exploration via elliptical episodic
bonuses. AdvancesinNeuralInformationProcessingSystems,35:37631–37646,2022.
[17] M.Hessel,H.Soyer,L.Espeholt,W.Czarnecki,S.Schmitt,andH.vanHasselt. Multi-task
deepreinforcementlearningwithpopart,2018.
[18] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):
1735–1780,1997.
10[19] S.Huang,R.F.J.Dossa,C.Ye,andJ.Braga. Cleanrl:High-qualitysingle-fileimplementations
ofdeepreinforcementlearningalgorithms,2021.
[20] M.Jiang,M.Dennis,J.Parker-Holder,J.Foerster,E.Grefenstette,andT.Rocktäschel. Replay-
guidedadversarialenvironmentdesign. AdvancesinNeuralInformationProcessingSystems,
34:1884–1897,2021.
[21] M. Jiang, E. Grefenstette, and T. Rocktäschel. Prioritized level replay. In International
ConferenceonMachineLearning,pages4940–4950.PMLR,2021.
[22] M.Jiang,M.Dennis,J.Parker-Holder,A.Lupu,H.Küttler,E.Grefenstette,T.Rocktäschel,and
J.Foerster. Groundingaleatoricuncertaintyforunsupervisedenvironmentdesign. Advancesin
NeuralInformationProcessingSystems,35:32868–32881,2022.
[23] A. Kanervisto, S. Milani, K. Ramanauskas, N. Topin, Z. Lin, J. Li, J. Shi, D. Ye, Q. Fu,
W. Yang, W. Hong, Z. Huang, H. Chen, G. Zeng, Y. Lin, V. Micheli, E. Alonso, F. Fleuret,
A.Nikulin,Y.Belousov,O.Svidchenko,andA.Shpilman. Minerldiamond2021competition:
Overview, results, and lessons learned. In D. Kiela, M. Ciccone, and B. Caputo, editors,
Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track, volume 176 of
Proceedings of Machine Learning Research, pages 13–28. PMLR, 06–14 Dec 2022. URL
https://proceedings.mlr.press/v176/kanervisto22a.html.
[24] I.Kanitscheider,J.Huizinga,D.Farhi,W.H.Guss,B.Houghton,R.Sampedro,P.Zhokhov,
B.Baker,A.Ecoffet,J.Tang,O.Klimov,andJ.Clune. Multi-taskcurriculumlearningina
complex,visual,hard-explorationdomain:Minecraft,2021.
[25] H.Küttler,N.Nardelli,A.H.Miller,R.Raileanu,M.Selvatici,E.Grefenstette,andT.Rock-
täschel. Thenethacklearningenvironment,2020.
[26] M. Lechner, L. Yin, T. Seyde, T.-H. Wang, W. Xiao, R. Hasani, J. Rountree, and D. Rus.
Gigastep-onebillionstepspersecondmulti-agentreinforcementlearning. InAdvancesin
NeuralInformationProcessingSystems,2023. URLhttps://openreview.net/forum?id=
UgPAaEugH3.
[27] J. Z. Leibo, E. Duéñez-Guzmán, A. S. Vezhnevets, J. P. Agapiou, P. Sunehag, R. Koster,
J.Matyas,C.Beattie,I.Mordatch,andT.Graepel. Scalableevaluationofmulti-agentreinforce-
mentlearningwithmeltingpot,2021.
[28] R.Lowe,Y.Wu,A.Tamar,J.Harb,P.Abbeel,andI.Mordatch. Multi-agentactor-criticfor
mixedcooperative-competitiveenvironments,2020.
[29] M. C. Machado, M. G. Bellemare, E. Talvitie, J. Veness, M. Hausknecht, and M. Bowling.
Revisitingthearcadelearningenvironment:Evaluationprotocolsandopenproblemsforgeneral
agents. JournalofArtificialIntelligenceResearch,61:523–562,2018.
[30] V.Mnih,K.Kavukcuoglu,D.Silver,A.Graves,I.Antonoglou,D.Wierstra,andM.Riedmiller.
Playingatariwithdeepreinforcementlearning,2013.
[31] I.MordatchandP.Abbeel. Emergenceofgroundedcompositionallanguageinmulti-agent
populations. 2018.
[32] A.Nikulin,V.Kurenkov,I.Zisman,A.Agarkov,V.Sinii,andS.Kolesnikov. Xland-minigrid:
Scalablemeta-reinforcementlearningenvironmentsinjax,2024.
[33] OpenAI, C. Berner, G. Brockman, B. Chan, V. Cheung, P. De˛biak, C. Dennison, D. Farhi,
Q.Fischer,S.Hashme,C.Hesse,R.Józefowicz,S.Gray,C.Olsson,J.Pachocki,M.Petrov,
H.P.deOliveiraPinto,J.Raiman,T.Salimans,J.Schlatter,J.Schneider,S.Sidor,I.Sutskever,
J.Tang,F.Wolski,andS.Zhang. Dota2withlargescaledeepreinforcementlearning,2019.
URLhttps://arxiv.org/abs/1912.06680.
[34] A.Petrenko,E.Wijmans,B.Shacklett,andV.Koltun. Megaverse:Simulatingembodiedagents
atonemillionexperiencespersecond,2021.
[35] S.Reed,K.Zolna,E.Parisotto,S.G.Colmenarejo,A.Novikov,G.Barth-Maron,M.Gimenez,
Y.Sulsky,J.Kay,J.T.Springenberg,T.Eccles,J.Bruce,A.Razavi,A.Edwards,N.Heess,
Y.Chen,R.Hadsell,O.Vinyals,M.Bordbar,andN.deFreitas. Ageneralistagent,2022.
[36] A.Rutherford,B.Ellis,M.Gallici,J.Cook,A.Lupu,G.Ingvarsson,T.Willi,A.Khan,C.S.
de Witt, A. Souly, S. Bandyopadhyay, M. Samvelyan, M. Jiang, R. T. Lange, S. Whiteson,
B. Lacerda, N. Hawes, T. Rocktaschel, C. Lu, and J. N. Foerster. Jaxmarl: Multi-agent rl
environmentsinjax,2023.
11[37] M.Samvelyan,T.Rashid,C.S.deWitt,G.Farquhar,N.Nardelli,T.G.J.Rudner,C.-M.Hung,
P.H.S.Torr,J.Foerster,andS.Whiteson. Thestarcraftmulti-agentchallenge,2019.
[38] M.Samvelyan,R.Kirk,V.Kurin,J.Parker-Holder,M.Jiang,E.Hambro,F.Petroni,H.Küt-
tler, E. Grefenstette, and T. Rocktäschel. Minihack the planet: A sandbox for open-ended
reinforcementlearningresearch,2021.
[39] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov. Proximalpolicyoptimization
algorithms,2017.
[40] J. Suarez, Y. Du, P. Isola, and I. Mordatch. Neural mmo: A massively multiagent game
environmentfortrainingandevaluatingintelligentagents. arXivpreprintarXiv:1903.00784,
2019.
[41] J.Suarez,P.Isola,K.W.Choe,D.Bloomin,H.X.Li,R.Sullivan,N.K.Ravichandran,D.Scott,
R.S.Shuman,H.Bradley,L.Castricato,K.You,Y.Jiang,Q.Li,J.Chen,andX.Zhu. Neural
MMO2.0:Amassivelymulti-taskadditiontomassivelymulti-agentlearning. InThirty-seventh
ConferenceonNeuralInformationProcessingSystemsDatasetsandBenchmarksTrack,2023.
URLhttps://openreview.net/forum?id=DSYuRMJnaY.
[42] S. Tao, I. Pan, B. Doerschuk-Tiberi, and A. Howard. Lux ai season 2, 2023. URL https:
//kaggle.com/competitions/lux-ai-season-2.
[43] A.A.Team,J.Bauer,K.Baumli,S.Baveja,F.Behbahani,A.Bhoopchand,N.Bradley-Schmieg,
M.Chang,N.Clay,A.Collister,V.Dasagi,L.Gonzalez,K.Gregor,E.Hughes,S.Kashem,
M.Loks-Thompson,H.Openshaw,J.Parker-Holder,S.Pathak,N.Perez-Nieves,N.Rakicevic,
T. Rocktäschel, Y. Schroecker, J. Sygnowski, K. Tuyls, S. York, A. Zacherl, and L. Zhang.
Human-timescaleadaptationinanopen-endedtaskspace,2023.
[44] O.E.L.Team,A.Stooke,A.Mahajan,C.Barros,C.Deck,J.Bauer,J.Sygnowski,M.Trebacz,
M.Jaderberg,M.Mathieu,N.McAleese,N.Bradley-Schmieg,N.Wong,N.Porcel,R.Raileanu,
S.Hughes-Fitt,V.Dalibard,andW.M.Czarnecki. Open-endedlearningleadstogenerally
capableagents,2021.
[45] S. Team, M. A. Raad, A. Ahuja, C. Barros, F. Besse, A. Bolt, A. Bolton, B. Brownfield,
G.Buttimore,M.Cant,S.Chakera,S.C.Y.Chan,J.Clune,A.Collister,V.Copeman,A.Cullum,
I.Dasgupta,D.deCesare,J.D.Trapani,Y.Donchev,E.Dunleavy,M.Engelcke,R.Faulkner,
F. Garcia, C. Gbadamosi, Z. Gong, L. Gonzales, K. Gupta, K. Gregor, A. O. Hallingstad,
T.Harley,S.Haves,F.Hill,E.Hirst,D.A.Hudson,J.Hudson,S.Hughes-Fitt,D.J.Rezende,
M. Jasarevic, L. Kampis, R. Ke, T. Keck, J. Kim, O. Knagg, K. Kopparapu, A. Lampinen,
S. Legg, A. Lerchner, M. Limont, Y. Liu, M. Loks-Thompson, J. Marino, K. M. Cussons,
L.Matthey,S.Mcloughlin,P.Mendolicchio,H.Merzic,A.Mitenkova,A.Moufarek,V.Oliveira,
Y. Oliveira, H. Openshaw, R. Pan, A. Pappu, A. Platonov, O. Purkiss, D. Reichert, J. Reid,
P. H. Richemond, T. Roberts, G. Ruscoe, J. S. Elias, T. Sandars, D. P. Sawyer, T. Scholtes,
G.Simmons,D.Slater,H.Soyer,H.Strathmann,P.Stys,A.C.Tam,D.Teplyashin,T.Terzi,
D. Vercelli, B. Vujatovic, M. Wainwright, J. X. Wang, Z. Wang, D. Wierstra, D. Williams,
N.Wong,S.York,andN.Young. Scalinginstructableagentsacrossmanysimulatedworlds,
2024.
[46] J.K.Terry,B.Black,N.Grammel,M.Jayakumar,A.Hari,R.Sullivan,L.Santos,R.Perez,
C. Horsch, C. Dieffendahl, N. L. Williams, Y. Lokesh, and P. Ravi. Pettingzoo: Gym for
multi-agentreinforcementlearning,2021.
[47] R.Wang,J.Lehman,J.Clune,andK.O.Stanley.Pairedopen-endedtrailblazer(poet):Endlessly
generatingincreasinglycomplexanddiverselearningenvironmentsandtheirsolutions,2019.
[48] C.Yu,A.Velu,E.Vinitsky,J.Gao,Y.Wang,A.Bayen,andY.Wu. Thesurprisingeffectiveness
ofppoincooperative,multi-agentgames,2022.
Checklist
1. Forallauthors...
(a) Do the main claims made in the abstract and introduction accurately reflect the pa-
per’scontributionsandscope?[Yes]WeintroduceMetaMMO’sminigames,speed
improvement,andgeneralistpolicy.Thesearefreelyavailablefordownload.
12(b) Didyoudescribethelimitationsofyourwork?[Yes]SeeSection5,wherewedescribe
threeshortcomings.
(c) Did you discuss any potential negative societal impacts of your work? [Yes] See
Section 5. Minigames are simulated games that are substantially abstracted from
real-worldscenarios.
(d) Haveyoureadtheethicsreviewguidelinesandensuredthatyourpaperconformsto
them?[Yes]Thispaperconformstotheethicsreviewguidelines.
2. Ifyouareincludingtheoreticalresults...
(a) Didyoustatethefullsetofassumptionsofalltheoreticalresults?[N/A]
(b) Didyouincludecompleteproofsofalltheoreticalresults?[N/A]
3. Ifyouranexperiments(e.g.forbenchmarks)...
(a) Didyouincludethecode,data,andinstructionsneededtoreproducethemainexperi-
mentalresults(eitherinthesupplementalmaterialorasaURL)?[Yes]Therepository
url,https://github.com/kywch/meta-mmo,ismentionedinboththeIntroduction
andAppendix.
(b) Didyouspecifyallthetrainingdetails(e.g.,datasplits,hyperparameters,howthey
werechosen)?[Yes]WespecifiedtheseindetailinAppendixA.5.
(c) Didyoureporterrorbars(e.g.,withrespecttotherandomseedafterrunningexperi-
mentsmultipletimes)?[Yes]ThetrainingcurvesinFigs3and5weregeneratedwith
fiverandomseeds,andtheerrorbarswerepresentedaccordingly.
(d) Did you include the total amount of compute and the type of resources used (e.g.,
typeofGPUs,internalcluster,orcloudprovider)?[Yes]Wesummarizedthetraining
durationandincludedlinkstothewandbsinTable2.Thehardwareconfiguration(RTX
4090)isdescribedinAppendixA.5.
4. Ifyouareusingexistingassets(e.g.,code,data,models)orcurating/releasingnewassets...
(a) Ifyourworkusesexistingassets,didyoucitethecreators?[Yes]Thisworkisbasedon
NeuralMMO2andCleanRL,bothcitedinthispaper,andPufferLib,whichiscurrently
unpublished.
(b) Didyoumentionthelicenseoftheassets?[Yes]EverythingispublishedundertheMIT
license.
(c) Did you include any new assets either in the supplemental material or as a URL?
[Yes]TheupdatesmadetoNeuralMMO2havebeenmergedintotheNeuralMMO
repositoryandarenowfreelyavailable.
(d) Didyoudiscusswhetherandhowconsentwasobtainedfrompeoplewhosedatayou’re
using/curating?[N/A]Thisworkdoesnothavehumandata.
(e) Didyoudiscusswhetherthedatayouareusing/curatingcontainspersonallyidentifi-
ableinformationoroffensivecontent?[N/A]Thisworkdoesnotcontainpersonally
identifiableinformationoroffensivecontent.
5. Ifyouusedcrowdsourcingorconductedresearchwithhumansubjects...
(a) Didyouincludethefulltextofinstructionsgiventoparticipantsandscreenshots,if
applicable?[N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board(IRB)approvals,ifapplicable?[N/A]
(c) Didyouincludetheestimatedhourlywagepaidtoparticipantsandthetotalamount
spentonparticipantcompensation?[N/A]
13A Appendix
The Meta MMO baselines, training, and evaluation code are available at https://github.
com/kywch/meta-mmo. The Meta MMO environment is available at https://github.com/
NeuralMMO/environment/tree/2.1, as Neural MMO 2.1. Both are published under the MIT
license. The authors confirm that they have the permission to license these as such and bear all
responsibilityinthecaseofviolationofrights.
HostingandMaintenance:Thecode,documentation,andbaselineswillcontinuetobehostedonthe
NeuralMMOGitHubaccount,astheywereforthelastfiveyears.SupportisavailableontheNeural
MMODiscord,availablefromhttps://neuralmmo.github.io/.Wewillcontinuetoupdatethe
platformtoresolvemajorbreakingchanges.
Reproducibility: We provide the training and evaluation scripts to reproduce the results in the
repository.Thesemaybeusedasbaselinesbyfutureworks.
A.1 MetaMMOSubsystemsandConfigurableAttributes
Meta MMO’s minigame framework allows a single policy to be trained on multiple minigames
simultaneously,evenwhentheyhavedifferentobservationandactionspaces.Forexample,Raceto
theCenterisafree-for-allminigamewithoutobservationsoractionsrelatedtocombat,items,or
themarket,whileTeamBattleisateam-basedminigamethatincludesthesefeatures.Tofacilitate
concurrenttrainingontheminigameswithdifferentobservationandactionspaces,theenvironment
isinitializedwithasupersetofobservationsandactionsthatencompassallminigames,andeach
subsystemcanbeturnedonandoffduringreset.Duringtraining,theappropriateobservationsand
actions are used based on the current minigame, allowing the policy to learn from diverse game
configurationsseamlessly.Thisfeatureenablesresearcherstoeasilytraingeneralistagentsoutofthe
boxandinvestigatetheimpactofdiversecurriculaongeneralistlearning.
TableA1:NeuralMMOsubsystemsandassociatedobservation/actionspaces.
Subsystem Obsspace Actionspace
Tick(1),AgentId(1),Task(27),
Base Move(5)
Tile(225x7),Entity(100x31)
Terrain . .
Resource . .
Combat . Attackstyle(3),target(101)
NPC . .
Communication Comm(32x4) Commtoken(127)
Use(13),Destroy(13),
Item Inventory(12x16)
Giveitem(13),target(101)
Equipment . Useactsasequipandunequip
Profession . .
Progression . .
Sellitem(13),price(99),Buy(385),
Exchange Market(384x16)
GiveGoldtarget(101),amount(99)
Thesectionsbelowlisttheconfigurableattributesineachsubsystem.
Base:Thebaseattributesthatdonotbelongtoanysubsystems.
• HORIZON:Numberofstepsbeforetheenvironmentresets.
• ALLOW_MOVE_INTO_OCCUPIED_TILE:Whetheragentscanmoveintooccupiedtiles.
• PLAYER_VISION_RADIUS:Numberofvisibletilesinanydirection.
• PLAYER_HEALTH_INCREMENT:Healthincrementpertickforplayers.
• DEATH_FOG_ONSET:Ticksbeforespawningdeathfog,Nonefornofog.
• DEATH_FOG_SPEED:Tilespertickthefogmoves.
14• DEATH_FOG_FINAL_SIZE:Fogradiusfromcenter.
• MAP_CENTER:Playablemapsizeintilesperside.
• MAP_RESET_FROM_FRACTAL:Whethertoregeneratemapfromfractal.
Terrain:Procedurallygeneratemaps.
• TERRAIN_FLIP_SEED:Whethertonegatetheseedusedforterraingeneration.
• TERRAIN_FREQUENCY:Basenoisefrequencyrangeforterraingeneration.
• TERRAIN_FREQUENCY_OFFSET:Noisefrequencyoctaveoffsetforterraingeneration.
• TERRAIN_LOG_INTERPOLATE_MIN:Mininterpolationlog-strengthfornoisefreqs.
• TERRAIN_LOG_INTERPOLATE_MAX:Maxinterpolationlog-strengthfornoisefreqs.
• TERRAIN_TILES_PER_OCTAVE:Numberofoctavessampledfromlog2spacedTER-
RAIN_FREQUENCYrange.
• TERRAIN_VOID:Noisethresholdforvoidgeneration.
• TERRAIN_WATER:Noisethresholdforwatergeneration.
• TERRAIN_GRASS:Noisethresholdforgrassgeneration.
• TERRAIN_FOILAGE:Noisethresholdforfoilage(foodtile)generation.
• TERRAIN_RESET_TO_GRASS:Makealltilesgrasswhenresettingfromthefractalnoise.
• TERRAIN_DISABLE_STONE:Whethertodisablestone(obstacle)tiles.
• TERRAIN_SCATTER_EXTRA_RESOURCES:Scatterextrafoodandwateronthemap
whenresettingfromthefractalnoise.
Resource:Addfoodandwaterforagingtomaintainagenthealth.RequiresTerrain.
• RESOURCE_BASE:Initiallevelandcapacityforfoodandwater.
• RESOURCE_DEPLETION_RATE:Depletionrateforfoodandwater.
• RESOURCE_STARVATION_RATE:Damagepertickwithoutfood.
• RESOURCE_DEHYDRATION_RATE:Damagepertickwithoutwater.
• RESOURCE_RESILIENT_POPULATION:Proportionresilienttostarvation/dehydration.
• RESOURCE_DAMAGE_REDUCTION:Damagereductionforresilientagents.
• RESOURCE_FOILAGE_CAPACITY:Maximumfoilagetileharvestsbeforedecay.
• RESOURCE_FOILAGE_RESPAWN:Probabilityharvestedfoilageregeneratespertick.
• RESOURCE_HARVEST_RESTORE_FRACTION:Fractionofmaximumcapacityrestored
onharvest.
• RESOURCE_HEALTH_REGEN_THRESHOLD:Resourcecapacityfractionrequiredto
regenhealth.
• RESOURCE_HEALTH_RESTORE_FRACTION: Health fraction restored when above
threshold.
Combat:AllowagentstofightotheragentsandNPCswithMelee,Range,andMagic.
• COMBAT_SPAWN_IMMUNITY:Ticksbeforenewagentscanbeattacked.
• COMBAT_ALLOW_FLEXIBLE_STYLE:Whetheragentscanattackwithanystyle.
• COMBAT_STATUS_DURATION:Tickscombatstatuslastsafterevent.
• COMBAT_WEAKNESS_MULTIPLIER:Multiplierforsuper-effectiveattacks.
• COMBAT_MINIMUM_DAMAGE_PROPORTION:Minimumdamageproportiontoinflict.
• COMBAT_DAMAGE_FORMULA:Damageformulaforcombat.
• COMBAT_MELEE_DAMAGE:Meleeattackdamage.
• COMBAT_MELEE_REACH:ReachofattacksusingtheMeleeskill.
15• COMBAT_RANGE_DAMAGE:Rangeattackdamage.
• COMBAT_RANGE_REACH:ReachofattacksusingtheRangeskill.
• COMBAT_MAGE_DAMAGE:Mageattackdamage.
• COMBAT_MAGE_REACH:ReachofattacksusingtheMageskill.
NPC:AddNon-PlayableCharactersofvaryinghostility.RequiresCombat.
• NPC_N:MaximumnumberofNPCsspawnableintheenvironment.
• NPC_DEFAULT_REFILL_DEAD_NPCS:WhethertorefilldeadNPCs.
• NPC_SPAWN_ATTEMPTS:NumberofNPCspawnattemptspertick.
• NPC_SPAWN_AGGRESSIVE:PercentagedistancethresholdforaggressiveNPCs.
• NPC_SPAWN_NEUTRAL:PercentagedistancethresholdfromspawnforneutralNPCs.
• NPC_SPAWN_PASSIVE:PercentagedistancethresholdfromspawnforpassiveNPCs.
• NPC_LEVEL_MIN:MinimumNPClevel.
• NPC_LEVEL_MAX:MaximumNPClevel.
• NPC_BASE_DEFENSE:BaseNPCdefense.
• NPC_LEVEL_DEFENSE:BonusNPCdefenseperlevel.
• NPC_BASE_DAMAGE:BaseNPCdamage.
• NPC_LEVEL_DAMAGE:BonusNPCdamageperlevel.
• NPC_LEVEL_MULTIPLIER:MultiplierforNPCleveldamageanddefense.
• NPC_ALLOW_ATTACK_OTHER_NPCS:WhetherNPCscanattackotherNPCs.
Communication:Addlimited-bandwidthteammessagingobsandaction.
• COMMUNICATION_N_OBS:Numberofsame-teamplayerssharingobs.
• COMMUNICATION_NUM_TOKENS:NumberofdistinctCOMMtokens.
Item:Addinventoryanditem-relatedactions.
• ITEM_N:Numberofuniquebaseitemclasses.
• ITEM_INVENTORY_CAPACITY:Numberofinventoryspaces.
• ITEM_ALLOW_GIFT:Whetheragentscangivegold/itemtoeachother.
• INVENTORY_N_OBS:Numberofdistinctitemobservations.
Equipment: Add armor, ammunition, and weapons to increase agents’ offensive and defensive
capabilities.RequiresItem.
• WEAPON_DROP_PROB:Chanceofgettingaweaponwhileharvestingammunition.
• EQUIPMENT_WEAPON_BASE_DAMAGE:Baseweapondamage.
• EQUIPMENT_WEAPON_LEVEL_DAMAGE:Addedweapondamageperlevel.
• EQUIPMENT_AMMUNITION_BASE_DAMAGE:Baseammunitiondamage.
• EQUIPMENT_AMMUNITION_LEVEL_DAMAGE:Addedammunitiondamageperlevel.
• EQUIPMENT_TOOL_BASE_DEFENSE:Basetooldefense.
• EQUIPMENT_TOOL_LEVEL_DEFENSE:Addedtooldefenseperlevel.
• EQUIPMENT_ARMOR_BASE_DEFENSE:Basearmordefense.
• EQUIPMENT_ARMOR_LEVEL_DEFENSE:Baseequipmentdefense.
Profession: Add resources and tools to practice Herbalism, Fishing, Prospecting, Carving, and
Alchemy.RequiresTerrainandItem.
• PROFESSION_TREE_CAPACITY:Maximumtreetileharvestsbeforedecay.
16• PROFESSION_TREE_RESPAWN:Probabilityharvestedtreeregeneratespertick.
• PROFESSION_ORE_CAPACITY:Maximumoretileharvestsbeforedecay.
• PROFESSION_ORE_RESPAWN:Probabilityharvestedoreregeneratespertick.
• PROFESSION_CRYSTAL_CAPACITY:Maximumcrystaltileharvestsbeforedecay.
• PROFESSION_CRYSTAL_RESPAWN:Probabilityharvestedcrystalregeneratespertick.
• PROFESSION_HERB_CAPACITY:Maximumherbtileharvestsbeforedecay.
• PROFESSION_HERB_RESPAWN:Probabilityharvestedherbregeneratespertick.
• PROFESSION_FISH_CAPACITY:Maximumfishtileharvestsbeforedecay.
• PROFESSION_FISH_RESPAWN:Probabilityharvestedfishregeneratespertick.
• PROFESSION_CONSUMABLE_RESTORE:Food/waterrestoredbyconsumingitem.
Progression:Addlevelstoskills,items,andequipmenttoincreaseagents’anditemattributes.
• PROGRESSION_BASE_LEVEL:Initialskilllevel.
• PROGRESSION_LEVEL_MAX:Maxskilllevel.
• PROGRESSION_EXP_THRESHOLD:Experiencethresholdsforeachlevel.
• PROGRESSION_COMBAT_XP_SCALE:AddXPforMelee/Range/Mageattacks.
• PROGRESSION_AMMUNITION_XP_SCALE:XPforProspecting/Carving/Alchemy.
• PROGRESSION_CONSUMABLE_XP_SCALE:AddXPforFishing/Herbalismharvests.
• PROGRESSION_MELEE_BASE_DAMAGE:BaseMeleeattackdamage.
• PROGRESSION_MELEE_LEVEL_DAMAGE:BonusMeleedamageperlevel.
• PROGRESSION_RANGE_BASE_DAMAGE:BaseRangeattackdamage.
• PROGRESSION_RANGE_LEVEL_DAMAGE:BonusRangedamageperlevel.
• PROGRESSION_MAGE_BASE_DAMAGE:BaseMageattackdamage.
• PROGRESSION_MAGE_LEVEL_DAMAGE:BonusMagedamageperlevel.
• PROGRESSION_BASE_DEFENSE:Basedefense.
• PROGRESSION_LEVEL_DEFENSE:Bonusdefenseperlevel.
Exchange:Addgoldandmarketactionstoenabletradingitemsandequipmentwithotheragentson
aglobalmarket.RequiresItem.
• EXCHANGE_BASE_GOLD:Initialgoldamount.
• EXCHANGE_LISTING_DURATION:Ticksitemislistedforsale.
• MARKET_N_OBS:Numberofdistinctitemobservations.
• PRICE_N_OBS:Numberofdistinctpriceobservationsandmaxprice.
17A.2 AdaptiveDifficultyandDomainRandomizationExamples
Thecodesnippetsbelowareexcerptsfromhttps://github.com/kywch/meta-mmo/blob/main/
reinforcement_learning/environment.py.
AdaptiveDifficulty.Duringreset,the _set_config() functioncanoverridethedefaultconfig
values.Thus,itispossibleforaminigametolookatthehistoryofgameresultsandadjusttheconfig
forthenextepisode.ThefollowingisanexcerptfromRacetotheCenter,wherethedifficultyis
determinedbythemapsize.
class RacetoCenter(Game):
def _set_config(self):
self.config.reset()
...
self._determine_difficulty() # sets the map_size
self.config.set_for_episode("MAP_CENTER", self.map_size)
def _determine_difficulty(self):
# Determine the difficulty (the map size) based on the previous results
if self.adaptive_difficulty and self.history \
and self.history[-1]["result"]: # the last game was won
last_results = [r["result"] for r in self.history if r["map_size"] == self.map_size]
if sum(last_results) >= self.num_game_won \
and self.map_size <= self.config.original["MAP_CENTER"] - self.step_size:
self._map_size += self.step_size
DomainRandomizationcanalsobeachievedusingthe _set_config() function.Tomaintain
determinism,usetheenvironment’srandomnumbergenerator, self._np_random.
class Survive(ng.DefaultGame):
def _set_config(self):
self.config.reset()
...
fog_onset = self._next_fog_onset or self._np_random.integers(32, 256)
fog_speed = self._next_fog_speed or 1 / self._np_random.integers(7, 12)
self.config.set_for_episode("DEATH_FOG_ONSET", fog_onset)
self.config.set_for_episode("DEATH_FOG_SPEED", fog_speed)
npc_num = self._next_num_npc or self._np_random.integers(64, 256)
self.config.set_for_episode("NPC_N", npc_num)
18A.3 MinigameReplays
FullConfigMinigames
• Survival
• TeamBattle
• Multi-taskTraining
MiniConfigMinigames
• TeamBattle
• ProtecttheKing
• RacetotheCenter
• KingoftheHill
• Sandwich
Making New Replays can be done using the scripts and policies provided in the base-
lines. The checkpoints should be copied or symlinked into a directory; in the baseline reposi-
tory, each experiment folder contains four specialist and four generalist checkpoints. Running
python train.py -m replay generatesareplay.The -p argumentspecifiesthedirectorycon-
taining the policies, and the -g argument specifies the minigames to run. The –train.seed
argumentcanbeusedtospecifyarandomseed.
# Full config experiments
$ python train.py -m replay -p experiments/full_sv -g survive
$ python train.py -m replay -p experiments/full_mt -g task
$ python train.py -m replay -p experiments/full_tb -g battle --train.seed 11
# Mini config experiments need --use-mini flag
$ python train.py -m replay --use-mini -p experiments/mini_tb -g battle
$ python train.py -m replay --use-mini -p experiments/mini_pk -g ptk
$ python train.py -m replay --use-mini -p experiments/mini_rc -g race
$ python train.py -m replay --use-mini -p experiments/mini_kh -g koh
$ python train.py -m replay --use-mini -p experiments/mini_sw -g sandwich
19A.4 Multi-taskTrainingandEvaluationTasks
Thefulltrainingandevaluationtasksareavailableinthebaselinerepository:https://github.com/
kywch/meta-mmo/blob/main/curriculum/neurips_curriculum.py.Theevaluationtasksare
taggedwith tags=["eval"].
Thereare63evaluationtasksacrosssixcategories.Thetaskprogressmetricisobtainedbyaveraging
allthemaximumprogressfromeachtask.Tocalculateanormalizedscore(max100),eachcategory
isassignedaweightof100/6,andwithineachcategory,themaximumprogressacrossalltaskswas
averagedtodeterminethecategoryscore.
Survival:
• TickGE:num_tick=1024
Combat:
• CountEvent:PLAYER_KILLn=20
• DefeatEntity:type=npc,level=1+,n=20
• DefeatEntity:type=npc,level=3+,n=20
Exploration:
• CountEvent:GO_FARTHESTn=64
• OccupyTile:row=80,col=80
Skill:
• AttainSkill:skill=Melee,level=10
• AttainSkill:skill=Mage,level=10
• AttainSkill:skill=Range,level=10
• AttainSkill:skill=Fishing,level=10
• AttainSkill:skill=Herbalism,level=10
• AttainSkill:skill=Prospecting,level=10
• AttainSkill:skill=Alchemy,level=10
• AttainSkill:skill=Carving,level=10
Item:
• HavestItem:item=Whetstone,level=1+,n=20
• HavestItem:item=Arrow,level=1+,n=20
• HavestItem:item=Runes,level=1+,n=20
• HavestItem:item=Whetstone,level=3+,n=20
• HavestItem:item=Arrow,level=3+,n=20
• HavestItem:item=Runes,level=3+,n=20
• ConsumeItem:item=Ration,level=1+,n=20
• ConsumeItem:item=Potion,level=1+,n=20
• ConsumeItem:item=Ration,level=3+,n=20
• ConsumeItem:item=Potion,level=3+,n=20
• EquipItem:item=Hat,level=1+,n=1
• EquipItem:item=Top,level=1+,n=1
• EquipItem:item=Bottom,level=1+,n=1
• EquipItem:item=Spear,level=1+,n=1
20• EquipItem:item=Bow,level=1+,n=1
• EquipItem:item=Wand,level=1+,n=1
• EquipItem:item=Axe,level=1+,n=1
• EquipItem:item=Gloves,level=1+,n=1
• EquipItem:item=Rod,level=1+,n=1
• EquipItem:item=Pickaxe,level=1+,n=1
• EquipItem:item=Chisel,level=1+,n=1
• EquipItem:item=Whetstone,level=1+,n=1
• EquipItem:item=Arrow,level=1+,n=1
• EquipItem:item=Runes,level=1+,n=1
• EquipItem:item=Hat,level=3+,n=1
• EquipItem:item=Top,level=3+,n=1
• EquipItem:item=Bottom,level=3+,n=1
• EquipItem:item=Spear,level=3+,n=1
• EquipItem:item=Bow,level=3+,n=1
• EquipItem:item=Wand,level=3+,n=1
• EquipItem:item=Axe,level=3+,n=1
• EquipItem:item=Gloves,level=3+,n=1
• EquipItem:item=Rod,level=3+,n=1
• EquipItem:item=Pickaxe,level=3+,n=1
• EquipItem:item=Chisel,level=3+,n=1
• EquipItem:item=Whetstone,level=3+,n=1
• EquipItem:item=Arrow,level=3+,n=1
• EquipItem:item=Runes,level=3+,n=1
• FullyArmed:skill=Melee,level=1+,n=1
• FullyArmed:skill=Mage,level=1+,n=1
• FullyArmed:skill=Range,level=1+,n=1
• FullyArmed:skill=Melee,level=3+,n=1
• FullyArmed:skill=Mage,level=3+,n=1
• FullyArmed:skill=Range,level=3+,n=1
Market:
• CountEvent:EARN_GOLDn=20
• CountEvent:BUY_ITEMn=20
• EarnGold:amount=100
• HoardGold:amount=100
• MakeProfit:amount=100
21A.5 ExperimentalDetails
A.5.1 HardwareConfiguration
ThetrainingsessionspresentedinTable2wereconductedusingaconsumer-gradedesktopwithan
i9-13900KCPU,128GBRAM,andasingleRTX4090GPU,totalingaround$4,000USDretail.
A.5.2 ExperimentConfigs:MiniandFull
Thecodesnippetsbelowareexcerptsfromhttps://github.com/kywch/meta-mmo/blob/main/
reinforcement_learning/environment.py.
MiniConfig:BelowarethedetailsofthesubsystemsandconfigurationsusedintheMiniConfig
experiment.Thedefaultvaluesusedinthebaselinerepositoryareincludedascomments.Thesizeof
observationspaceis5,068.
import nmmo.core.config as nc
class MiniGameConfig(
nc.Medium,
nc.Terrain,
nc.Resource,
nc.Combat,
nc.NPC,
nc.Communication,
):
def __init__(self, env_args: Namespace):
super().__init__()
self.set("PROVIDE_ACTION_TARGETS", True)
self.set("PROVIDE_NOOP_ACTION_TARGET", True)
self.set("PROVIDE_DEATH_FOG_OBS", True)
self.set("TASK_EMBED_DIM", 16)
self.set("MAP_FORCE_GENERATION", env_args.map_force_generation) # False
self.set("PLAYER_N", env_args.num_agents) # 128
self.set("HORIZON", env_args.max_episode_length) # 1024
self.set("MAP_N", env_args.num_maps) # 256
# num_agent_per_team = 8, but minigames can override the below
self.set("TEAMS", get_team_dict(env_args.num_agents, env_args.num_agents_per_team))
self.set("PATH_MAPS", f"{env_args.maps_path}/{env_args.map_size}/") # "maps/train/"
self.set("MAP_CENTER", env_args.map_size) # 128
self.set("RESOURCE_RESILIENT_POPULATION", env_args.resilient_population) # 0
self.set("COMBAT_SPAWN_IMMUNITY", env_args.spawn_immunity) # 20
# The default is "curriculum/neurips_curriculum_with_embedding.pkl"
self.set("CURRICULUM_FILE_PATH", env_args.curriculum_file_path)
# Make the high-level npcs weaker. Huge impact on the difficulty
self.set("NPC_LEVEL_MULTIPLIER", 0.5)
22Full Config: Below are the details of the subsystems and configurations used in the Full Config
experiment.ThefullconfigaddsProgression,Item,Equipment,Profession,andExchangesubsystems
totheminiconfig.Thesizeofobservationspaceis12,241.
class FullGameConfig(
MiniGameConfig,
nc.Progression,
nc.Item,
nc.Equipment,
nc.Profession,
nc.Exchange,
):
pass
CurriculumLearningwithMinigames
Whentrainingageneralist,eachminigameissampledwithequalprobabilityduringreset.Thecode
snippetbelowshowshowthecurrentbaselineimplementsasimplecurriculumlearningmethod.
def make_env_creator(
reward_wrapper_cls: BaseParallelWrapper,
train_flag: str = None,
use_mini: bool = False,
):
if train_flag is None or train_flag == "full_gen":
game_packs = [
(Survive, 1),
(TeamBattle, 1),
(MultiTaskTraining, 1),
]
elif train_flag == "sv_only":
game_packs = [(Survive, 1)]
...
elif train_flag == "mini_gen":
game_packs = [
(TeamBattle, 1),
(ProtectTheKing, 1),
(RacetoCenter, 1),
(KingoftheHill, 1),
(Sandwich, 1),
]
def env_creator(*args, **kwargs):
if use_mini is True:
config = MiniGameConfig(kwargs["env"])
else:
config = FullGameConfig(kwargs["env"])
config.set("GAME_PACKS", game_packs)
env = nmmo.Env(config)
env = reward_wrapper_cls(env, **kwargs["reward_wrapper"])
env = pufferlib.emulation.PettingZooPufferEnv(env)
return env
return env_creator
23A.5.3 BaselineComponents
StatWrapper: This wrapper subclasses Pettingzoo [46]’s BaseParallelWrapper and handles the
trainingmetricsloggedtoWeights&Biases.Themainmetricstrackedarethetotalagentsteps(sum
ofallagents’lifespansinanepisode)andthenormalizedprogresstowardthecenter(0attheedge,1
atthecenter,averagedacrossagents).ProgressingtowardthecenteriscrucialinNeuralMMOsince
higher-levelNPCsanditemsareconcentratedthere.Additionalgame-specificmetricsincludethe
proportionofagentsthatperformedvariousevents(e.g.,eatingfood,drinkingwater,scoringhits,
killingplayers,firingammunition,consumingitems,etc.),andagentachievementssuchasmaximum
skilllevels,itemlevels,killcounts,andthenumberofuniqueevents.
TeamWrapper: Subclassing the StatWrapper, this component handles team-related observation
augmentationandmanualactionoverriding,asdescribedinSection2.3.Italsoaugmentsthetask
observationwithateamgameflag,agentgameflag,andon/offflagsforeachsubsystem.
RewardWrapper:SubclassingtheTeamWrapper,thiswrapperimplementscustomrewardshaping
basedonfactorslikeagenthealth,experience,attackanddefensecapabilities,andgold,inaddition
tothetaskreward.Team-levelrewardshapinglikeTeamSpirit[33]couldbeincorporatedhere.
TaskEmbedding:Toconditionagentsduringtrainingandevaluation,eachagentreceivesatask
embeddingvectorconsistingof27floats:11one-hotencodingsforagent/teamgameandsubsys-
temenablement,and16floatsforthetaskembeddingitself.Forminigames,taskembeddingsare
created by taking the SHA-256 hash of the reward function’s source code. For Multi-task Train-
ing and Evaluation, task embeddings are generated by (1) prompting a coding language model
(DeepSeek-Coder-1.3b-Instruct[14])withtherewardfunction’ssourcecodeandprovidedkwargs,
and(2)reducingtheresulting2048-dimensionalvectorto16dimensionsusingprincipalcomponent
analysis.Werecognizetheimportanceoftaskembeddingsforsteeringgeneralistagentsandhighlight
opportunitiesforimprovementinthisarea.
A.5.4 TrainingScripts
MiniConfigExperiment: –use-mini setstheminiconfigmode.The -t argumentisusedto
specify the minigames for training. The default training steps are 100M for specialists, and the
generalistpolicywastrainedfor400Msteps.
# Train specialists for Team Battle (tb), Protect the King (pk),
# Race to the Center (rc), King of the Hill (kh), and Sandwich (sw)
$ python train.py --use-mini -t tb_only
$ python train.py --use-mini -t pk_only
$ python train.py --use-mini -t rc_only
$ python train.py --use-mini -t kh_only
$ python train.py --use-mini -t sw_only
# Train a generalist for playing all five games
$ python train.py --use-mini -t mini_gen --train.total-timesteps 400_000_000
FullConfigExperiment:Runningthescriptwithout –use-mini setsupthefullconfigandpolicy.
# Train specialists for Survive (sv), Team Battle (tb), Multi-task Training (mt)
$ python train.py -t sv_only
$ python train.py -t tb_only
$ python train.py -t mt_only
# Train a generalist for playing all three games
$ python train.py -t full_gen --train.total-timesteps 400_000_000
24A.5.5 TrainingHyperparameters
Pufferlib0.7.3wasusedfortraining.Thesevaluescanbefoundathttps://github.com/kywch/
meta-mmo/blob/main/config.yaml.
PPOparameters
learning_rate 1.0e-4
anneal_lr True
gamma 0.99
gae_lambda 0.95
norm_adv True
clip_coef 0.1
clip_vloss True
ent_coef 0.01
vf_coef 0.5
vf_clip_coef 0.1
max_grad_norm 0.5
batch_size 32768
batch_rows 128
bptt_horizon 8
update_epochs 2
Vec-envparameters
env_pool True
num_envs 15
envs_per_worker 1
envs_per_batch 6
Historicself-playparameters
pool_kernel [0]*112+[1]*16
25A.6 Modelarchitectures
Thesourcecodeofthepolicyisathttps://github.com/kywch/meta-mmo/blob/main/agent_
zoo/baseline/policy.py.
Mini Config model consists of three encoders (TileEncoder, PlayerEncoder, and TaskEncoder),
fully-connectedlayerstothehiddenlayer(256units),1layerofLSTM,anactiondecoder,anda
valuenetwork.Thenumberofparametersis1.74M.
RecurrentPolicy(
(policy): Recurrent(
(policy): Policy(
(tile_encoder): TileEncoder(
(type_embedding): Embedding(16, 30)
(entity_embedding): Embedding(8, 15)
(rally_embedding): Embedding(8, 15)
(tile_resnet): ResnetBlock(
(model): Sequential(
(0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
(1): LayerNorm((64, 15, 15), eps=1e-05, elementwise_affine=True)
(2): ReLU()
(3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
(4): LayerNorm((64, 15, 15), eps=1e-05, elementwise_affine=True)
)
)
(tile_conv_1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
(tile_conv_2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1))
(tile_fc): Linear(in_features=968, out_features=256, bias=True)
(tile_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
)
(player_encoder): PlayerEncoder(
(embedding): Embedding(7936, 32)
(id_embedding): Embedding(512, 64)
(agent_mlp): MLPBlock(
(model): Sequential(
(0): Linear(in_features=93, out_features=256, bias=True)
(1): ReLU()
(2): Linear(in_features=256, out_features=256, bias=True)
)
)
(agent_fc): Linear(in_features=256, out_features=256, bias=True)
(my_agent_fc): Linear(in_features=256, out_features=256, bias=True)
(agent_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
(my_agent_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
)
(task_encoder): TaskEncoder(
(fc): Linear(in_features=27, out_features=256, bias=True)
(norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
)
(proj_fc): Linear(in_features=768, out_features=256, bias=True)
(action_decoder): ActionDecoder(
(layers): ModuleDict(
(attack_style): Linear(in_features=256, out_features=3, bias=True)
(attack_target): Linear(in_features=256, out_features=256, bias=True)
(comm_token): Linear(in_features=256, out_features=127, bias=True)
(move): Linear(in_features=256, out_features=5, bias=True)
)
)
(value_head): Linear(in_features=256, out_features=1, bias=True)
)
(recurrent): LSTM(256, 256)
)
)
26FullConfigmodel:ItemEncoderandMarketEncoderwereaddedtotheMiniConfigmodel,andthe
actiondecodersupportsthefullactionspace.Thenumberofparametersis3.33M.
RecurrentPolicy(
(policy): Recurrent(
(policy): Policy(
(tile_encoder): TileEncoder(
(type_embedding): Embedding(16, 30)
(entity_embedding): Embedding(8, 15)
(rally_embedding): Embedding(8, 15)
(tile_resnet): ResnetBlock(
(model): Sequential(
(0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
(1): LayerNorm((64, 15, 15), eps=1e-05, elementwise_affine=True)
(2): ReLU()
(3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
(4): LayerNorm((64, 15, 15), eps=1e-05, elementwise_affine=True)
)
)
(tile_conv_1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1))
(tile_conv_2): Conv2d(32, 8, kernel_size=(3, 3), stride=(1, 1))
(tile_fc): Linear(in_features=968, out_features=256, bias=True)
(tile_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
)
(player_encoder): PlayerEncoder(
(embedding): Embedding(7936, 32)
(id_embedding): Embedding(512, 64)
(agent_mlp): MLPBlock(
(model): Sequential(
(0): Linear(in_features=93, out_features=256, bias=True)
(1): ReLU()
(2): Linear(in_features=256, out_features=256, bias=True)
)
)
(agent_fc): Linear(in_features=256, out_features=256, bias=True)
(my_agent_fc): Linear(in_features=256, out_features=256, bias=True)
(agent_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
(my_agent_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
)
(task_encoder): TaskEncoder(
(fc): Linear(in_features=27, out_features=256, bias=True)
(norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
)
(item_encoder): ItemEncoder(
(embedding): Embedding(256, 32)
(item_mlp): MLPBlock(
(model): Sequential(
(0): Linear(in_features=76, out_features=256, bias=True)
(1): ReLU()
(2): Linear(in_features=256, out_features=256, bias=True)
)
)
(item_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
)
(inventory_encoder): InventoryEncoder(
(fc): Linear(in_features=3072, out_features=256, bias=True)
(norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
)
(market_encoder): MarketEncoder(
(fc): Linear(in_features=256, out_features=256, bias=True)
(norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
)
(proj_fc): Linear(in_features=1280, out_features=256, bias=True)
(action_decoder): ActionDecoder(
(layers): ModuleDict(
27(attack_style): Linear(in_features=256, out_features=3, bias=True)
(attack_target): Linear(in_features=256, out_features=256, bias=True)
(market_buy): Linear(in_features=256, out_features=256, bias=True)
(comm_token): Linear(in_features=256, out_features=127, bias=True)
(inventory_destroy): Linear(in_features=256, out_features=256, bias=True)
(inventory_give_item): Linear(in_features=256, out_features=256, bias=True)
(inventory_give_player): Linear(in_features=256, out_features=256, bias=True)
(gold_quantity): Linear(in_features=256, out_features=99, bias=True)
(gold_target): Linear(in_features=256, out_features=256, bias=True)
(move): Linear(in_features=256, out_features=5, bias=True)
(inventory_sell): Linear(in_features=256, out_features=256, bias=True)
(inventory_price): Linear(in_features=256, out_features=99, bias=True)
(inventory_use): Linear(in_features=256, out_features=256, bias=True)
)
)
(value_head): Linear(in_features=256, out_features=1, bias=True)
)
(recurrent): LSTM(256, 256)
)
)
28A.7 EvaluationMetrics
Theperformanceofanagentorpolicyinmultiagentsettingsisrelativetootheragentsorpolicies
intheenvironment.Inourbaselinerepository,weincludetrainedpolicycheckpointsatdifferent
trainingsteps,alongwithscriptsforevaluatingpoliciesina"checkpointvs.checkpoint"manner.
EloRating:Eloratingscanbeusedforallminigamesinvolvingmultiplecheckpoints.Thegame
scoreforeachcheckpointinanepisodeiscalculatedbyaveragingthemaximumtaskprogressofthe
agentscontrolledbythatcheckpoint,andthenaddingalargebonustothewinningagentorteam
tomarkthewinner.Theevaluationscript(evaluate.py)runs200episodeswitharandomseed
andsavesthegamescoresinaJSONfile.Weused10randomseeds,resultingin2000episodesfor
evaluation.TheEloscript(proc_elo.py)convertstheseresultfileswithgamescoresintopairwise
win-lossrecordsforeachcheckpointpair(e.g.,forfourcheckpoints,sixwin-losspairsarecreated)
andcalculatesthecorrespondingEloratings.
Task Completion: For the multi-task evaluation setting, which implements the 2023 multi-task
completionchallenge,the63evaluationtasksarerandomlyassignedtoeachagent,whichmaybe
controlledbydifferentcheckpoints.Theevaluationscript(evaluate.py)runs200episodeswith
arandomseedandsavesthetaskprogressinaJSONfile.Weused10randomseeds,resultingin
2000episodesforevaluation.Thescoringscript(proc_task_eval.py)aggregatestheprogress
foreachcheckpoint,printingtheaveragelifespan,averagetaskcompletionrateacrossthe63tasks,
andascorenormalizedacrosssixcategories:survival,combat,exploration,skill,item,andmarket.
EvaluationScripts:The evaluate.py scriptrunstheevaluation.Adirectorywithcheckpoints
mustbespecified;inthebaselinerepository,eachexperimentfoldercontainsfourspecialistandfour
generalistcheckpoints.The -g argumentspecifiestheminigame,andthe -r argumentspecifies
the number of repetitions. The proc_elo.py script takes two arguments: a directory with the
resultJSONandtheprefixoftheresultsfiles,anditprintsouttheEloratingsforeachpolicy.The
proc_task_eval.py scriptonlytakesthedirectoryandprintsoutthetaskcompletionmetrics.
# Full config minigames: survive, task, battle
$ python evaluate.py experiments/full_sv -g survive -r 10
$ python proc_elo.py experiments/full_sv survive
$ python evaluate.py experiments/full_mt -g task -r 10
$ python proc_task_eval.py experiments/full_mt task
$ python evaluate.py experiments/full_tb -g battle -r 10
$ python proc_elo.py experiments/full_tb battle
# Mini config minigames: battle, ptk, race, koh, sandwich
$ python evaluate.py experiments/mini_tb -g battle -r 10
$ python proc_elo.py experiments/mini_tb battle
$ python evaluate.py experiments/mini_pk -g ptk -r 10
$ python proc_elo.py experiments/mini_pk ptk
$ python evaluate.py experiments/mini_rc -g race -r 10
$ python proc_elo.py experiments/mini_rc race
$ python evaluate.py experiments/mini_kh -g koh -r 10
$ python proc_elo.py experiments/mini_kh koh
$ python evaluate.py experiments/mini_sw -g sandwich -r 10
$ python proc_elo.py experiments/mini_sw sandwich
29A.8 ExtendedTrainingCurvesfromtheFullConfigExperiment
ThepanelsbelowrepresentdiverseeventsprovidedbyMetaMMO’sfullconfiguration.Astraining
progresses,agentslearntoengagewithmoregamesubsystemsandencounteravarietyofevents.
FigureA1:Survivalspecialist
30FigureA2:TeamBattlespecialist
31FigureA3:Multi-taskTrainingspecialist
32A.9 MinigameSamplingRatioforGeneralistsTraining
Whentrainingthegeneralistpolicy,theminigamesineachepisodearesampledwithequalprobability
duringreset.Theminigamesamplingratioiscalculatedasthecumulativeagentstepscollectedin
theminigamedividedbythetotalagentsteps.Someminigamesareoversampledbecausethelength
and/ortotalagentstepsofeachepisodemayvaryacrossminigamesandchangeduetotraining.
FigureA4:TasksamplingratiofortrainingtheFullConfiggeneralistpolicy.
FigureA5:TasksamplingratiofortrainingtheMiniConfiggeneralistpolicy.
33