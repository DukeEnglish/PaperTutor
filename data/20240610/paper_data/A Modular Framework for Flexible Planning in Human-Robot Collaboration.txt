A Modular Framework for Flexible Planning in Human-Robot Collaboration
Valerio Belcamino∗,1, Mariya Kilina1, Linda Lastrico2, Alessandro Carf`ı1, Fulvio Mastrogiovanni1
Abstract—This paper presents a comprehensive framework
to enhance Human-Robot Collaboration (HRC) in real-world
scenarios.Itintroducesaformalismtomodelarticulatedtasks,
requiring cooperation between two agents, through a smaller
set of primitives. Our implementation leverages Hierarchical
Task Networks (HTN) planning and a modular multisensory
perception pipeline, which includes vision, human activity
recognition, and tactile sensing. To showcase the system’s
scalability, we present an experimental scenario where two
humans alternate in collaborating with a Baxter robot to
assemble four pieces of furniture with variable components.
This integration highlights promising advancements in HRC,
suggesting a scalable approach for complex, cooperative tasks
across diverse applications.
I. INTRODUCTION
Withtheadvancementoftechnology,Human-RobotInter-
action (HRI) has emerged as a central research area at the
intersection of robotics, artificial intelligence, and cognitive
science. The interaction between humans and robots often
involves collaboration, aiming to combine the best humans
and robots skills to enhance productivity, efficiency, and
versatility across numerous applications. These applications Fig.1:Atopviewoftheexperimentalscenario,inwhichthe
range from manufacturing [1] and logistics [2] to healthcare collaborative robot Baxter waits for the human to assemble
[3] and domestic assistance [4]. For the collaboration to the furniture pieces before continuing the interaction. We
be successful, the robot should be able to perceive the show in blue the robot workspace and in green the shared
environment,andplanandacttoaccommodatehumanneeds workspace. The labels O1 to O6 point to the components
and objectives. needed for the assembly.
Coordination is the cornerstone of effective collaboration
inamulti-agentscenario[5],[6],asitallowsforminimizing
eachparticipant’sidletimeanddecreasestheglobalduration Humans have excellent capabilities in terms of both
of the tasks. Coordination is influenced by several factors, movement and perception. For example, they can estimate
such as the agents’ dexterity and their sensorial capabilities. dynamically their position in space through a multi-modal
However, we focus our analysis on the perception, since the ability called proprioception. Moreover, they mainly rely on
dexterity is strictly coupled to the kinematic structure of the visionandtouchtogatherinformationabouttheenvironment
agent and is out of the scope of our work. In the case of and nearby objects, enabling them to navigate and interact
Human-Robot Collaboration, to better understand how the smoothly with their surroundings. Lastly, there is the ability
coordinationisinfluencedbythesensorialabilitiesofthetwo to read the intentions of others, which, in human-human
agents, it is necessary to describe their abilities separately. collaboration, is achieved through various communication
Morespecifically,wecandistinguishthreeimportantaspects: means, including gestures, posture, gaze, and speech. HRC
self-perception, environment perception, and the perception architectures aim to achieve the same ability resorting to
of the coworker. gestures [7], mixed reality [8], [9], digital twins [10], voice
interfaces [11] and non-verbal cues [12]–[14], or by making
∗Correspondingauthorvalerio.belcamino@edu.unige.it
1 TheEngineRoom,DepartmentofInformaticsBioengineering,Robotics robot’s motions more communicative [15].
andSystemEngineering,UniversityofGenoa,Genoa,Italy. Ontherobotside,postureisdeterminedbytherotationof
2 Cognitive Architecture for Collaborative Technologies Unit (CON- the joints, which is precisely measurable, while the percep-
TACT),ItalianInstituteofTechnology,Genoa,Italy
tionoftheenvironmentandthecoworkersreliesonmultiple
©2024IEEE.Personaluseofthismaterialispermitted.Permissionfrom
IEEE must be obtained for all other uses, in any current or future media, typesofsensorymodulesthatcanbeintegrated.Furthermore,
includingreprinting/republishingthismaterialforadvertisingorpromotional the robot should be able to anticipate, or at least recognize,
purposes, creating new collective works, for resale or redistribution to
the upcoming actions of the human partner. This predictive
servers or lists, or reuse of any copyrighted component of this work in
otherworks. ability can facilitate action planning to avoid collisions and
4202
nuJ
7
]OR.sc[
1v70940.6042:viXraoptimize the robot’s effectiveness. Drawing upon workspace that resonates with human thinking patterns and facilitates
observations and predictions of future human actions, the adaptation to different scenarios. Additionally, P3 comple-
robot must dynamically select the most appropriate action ments P1, as the careful selection of templates promotes the
online.Thisreal-timeadaptationiscrucialsincepre-planned modularity of the system.
actionsmaynotalignwiththeevolvingsituation.Finally,the P4 underlines the importance of a low computational
robotneedstoexecuteactionsinwaysthatalignwithhuman overhead. The framework should have a low impact on
needs and conform to safety constraints, thereby ensuring a system resources at runtime, ensuring effective performance
secure interaction. without weighing on the application. Solutions that solely
WeexaminedrecentliteratureonHRCframeworksaiming
plan actions offline, lacking support for online re-planning
to uncover common features contributing to the success and
or error handling, naturally incur minimal computational
effectivenessofthesesolutions.Throughoutthisanalysis,we
overhead.However,theabsenceofthesevitalfunctionalities
identified five core features that contribute to the effective-
compromises the system’s effectiveness. The main metrics
ness of a framework:
usedtoassessthesystem’scomputationalloadaretheoverall
P1 Modular Architecture task execution time [16], [19], as well as the execution time
P2 Runtime Flexibility for each action [25] and the idle time of agents [19], [25].
P3 Extensible Task Description
Lastly, P5 emphasizes the importance of providing trans-
P4 Low Computational Overhead
parentandcoherentjustificationfortheframework’sactions,
P5 Interpretability
promoting user comprehension and trust. In this aspect,
P1 refers to the modularity of the system. More in detail,
machine learning-based approaches, such as reinforcement
the architecture of the HRC framework must ensure the
learning [19] and LSTM [20], often exhibit sub-optimal
ability to model collaboration among a variable number
performance due to their lack of explainability. Conversely,
of agents, either human or robot [16], [17]. Modularity
hierarchically structured solutions like HTNs [26]–[29] and
is crucial, as it allows to model dynamically collaborative
AND/ORgraphs[17],[21]offerclearandeasilyunderstand-
scenarios varying over time. Additionally, the framework
able reasoning. Other methods employ node-based graphical
must be designed to easily accommodate the integration,
interfaces [30] to visualize the sequence of actions in their
removal or substitution of modules within the perception
planners for enhanced clarity. Another viable option is the
pipeline [14], [18], ensuring adaptability and customization.
HiddenMarkovModel[31],whichaidsintrackingdecision-
Thisallowsfortheextensionofframeworkstonewdomains
making processes based on environmental observations.
or the addition of new functionalities, unlike ML-based
Drawing from observations in the literature, it becomes
frameworks which often require extensive retraining [19],
evident that hierarchically structured planners, particularly
[20].
HTNs, are predominantly favoured for human-robot collab-
P2 emphasizes runtime flexibility within the HRC frame-
oration frameworks. This preference stems from their ability
work and requires the ability to adapt the planned action
to address the properties outlined above comprehensively.
to accommodate the different conditions that may occur
While certain implementations may deviate and lack one
during the collaboration. In particular, the system should be
or more of these properties — such as opting for separate
flexible enough to adapt to different human behaviours and
plannersforeachagent[28],whichsacrificesmodularityand
recover from potential errors. To support this, systems often
increases computational overhead — HTNs remain overall a
integrate knowledge about agents’ expertise and allocate
compelling choice. As such, we have selected them for our
tasks accordingly [21], ensuring efficient task execution and
frameworkandendeavourtoleveragetheirstrengthstofulfil
error handling. However, not all frameworks support online
all five features efficiently.
re-planning [22], [23] due to computational constraints, par-
ticularly those utilizing techniques with high computational This work aims to present an extensible framework for
overhead such as digital twin technology [18]. Despite its human-robot collaboration considering the properties we
potential benefits, offline planning followed by execution is identified. Additionally, we present an implementation of
oftensub-optimalduetoitsinabilitytoresponddynamically our framework designed for a human-robot collaborative
to errors or changing conditions. assembly scenario. We describe the choice of the action
P3 emphasizes the framework’s capacity for expansion templates, their implementations in a specific hardware ar-
and adjustment via an Extensible Task Description. This chitecture,theplanningpipelinewiththecorrespondingstate
task description should feature thoughtfully chosen general variables and the perception pipeline. Finally, to showcase
action templates that can be extended to accommodate the adaptability of our framework to diverse tasks, we de-
varying hardware architectures. This characteristic enhances signed an experimental setup involving the assembly of four
the framework’s versatility by enabling updates to the task distinct pieces of furniture. This scenario serves as a proof
description and the modelling of problems from diverse of concept, demonstrating our framework’s adherence to the
domains. Moreover, this aspect aligns with the findings in five properties previously outlined. Additionally, we provide
the literature, where hierarchically structured task planners quantitativemetricsconcerningfluency[32],includingrobot
are favoured [17], [18], [21], [24] for their ability to break idle time, human idle time, functional delay, and concurrent
downtasksintosmaller,moremanageablesteps—astrategy action time.Preconditions Effects
Grasp Release Move Manipulate Wait Perceive Grasp Release Move Manipulate Wait Perceive
End-Effector
✓ ✓ ✓ ✓ ✓
Availability(EEA)
AgentPose(AP) ✓ ✓ ✓ ✓ ✓
ObjectPose(OP) ✓ ✓ ? ✓
ObjectCharacteristics(OC) ✓ ✓ ✓ ✓ ✓
TABLEI:Descriptionoftheinterplayofactionsandstatefeatures.Ontheleftarethepreconditionstobemettoexecutethe
actions;ontherightarethestatefeatures(effects)thatgetupdatedafterthegivenaction.Thesymbol‘V’inthePreconditions
section means that a certain action requires the corresponding state variable to respect some constraints. Instead, for the
Effects, it means that the action updates the state variable. The symbol ‘?’ describes an action that can update a feature
only if some additional requirements are met. In this case, Move can update the Object Pose feature only if End-Effector
Availability was False when the action began.
II. PROBLEMFORMALIZATION pose (AP) contains the agents’ joint states and positions in
space. Given the unique joint configuration of an agent, this
Forarobottocollaborateeffectivelywithahuman,itmust feature allows for determining the pose in the space of each
understand the state of the interaction and act accordingly. of their links, including the end-effector. The objects’ pose
Thisproblemcanbeformalizedbyintroducingtwoconcepts: (OP) contains the objects’ positions, orientations in space,
the interaction state and the task plan. The interaction state and the occasional internal degree of freedom that some
includes all relevant variables necessary to describe the col- objects may have. Finally, the objects’ characteristic (OC) is
laborativetaskanditseffectontheenvironment,forexample, a generic container used to collect the unique characteristics
thepositionsofobjectsandtheposesofinvolvedagents.On of the objects. For example, a container could have a char-
theotherhand,thetaskplandetailstheactionsthatmusttake acteristic describing if it is full or empty, while a tool might
place to reach a desired goal, which is a specific interaction have one concerning the most appropriate grasping type.
state. Regardless of who performs it, each action affects the Hence, the OC allows for describing multiple and dissimilar
state,andtheircumulativeeffectshouldleadtothefinalgoal characteristics of each object. For the assembly task shown
state. However, as the state has a preconditioning effect on in Fig. 1, the state will contain:
the actions, only those whose preconditions are met in the
state can be performed; for instance, an agent must grasp a
EEA={eea ,eea ,eea ,eea }
tool before using it. Therefore, an accurate task planning H,r H,l R,r R,l
solution for human-robot collaboration should be able to AP ={ap ,ap }
H R
choose between feasible actions, evaluating their effect on OP ={op ,op ,op ,op ,op ,op ,op }
box O1 O2 O3 O4 O5 O6
the environment and considering that the human counterpart
OC ={oc ,oc ,oc ,oc ,oc ,oc ,oc }
could, in the meantime, alter the state. box O1 O2 O3 O4 O5 O6
In human-robot collaboration, the state description, the where eea and eea describe the availability of the
H,r H,l
set of actions in the plan, and their allocation to agents are human hands and eea , eea describe the availability
R,r R,l
influencedbyvariousfactors,suchasthegoal,thenumberof of the robot grippers. Additionally, ap , ap contain the
H R
agents involved, and the limitations of the robotic platform joint states of the two agents as well as their location and
[33]. For these reasons, providing an extensive and general orientation in the world frame. As for OP, it holds an entry
formalization of the problem is difficult. In this section, we for each of the five wooden pieces and one for the box
tackletheproblembydescribinggeneralstatepropertiesand of screws; each entry contains the location and orienta-
actions and how their interaction can be used to carry on tion of the corresponding object. Similarly, OC considers
the collaboration. This formalization has been empirically a characteristic for each object; oc shows that the box
box
synthesized and will be tested in a practical scenario in has been emptied, while oc and oc represent the two
O1 O2
the following sections. It is worth noting that although this pieceslocatedintherobotworkspace,eachcharacterizedby
formalization is provided in the HRC context, it can also be the attribute of being ”not grasped”. Finally, oc −oc
O3 O6
used to describe planning problems where a single agent is refer to the pieces located in the shared workspace and their
involved. characteristics show that they have been assembled.
Inourformalization,wehaveidentifiedfourstatefeatures Having described the state variables we can introduce our
that describe the interaction of agents with the environment. choice for the actions; we have identified six actions that
These features are end-effectors availability (EEA), agents’ complywiththerequirementssetbyP3beinggenericenough
pose (AP), objects’ pose (OP), and objects’ characteristics to describe different collaborative assembly scenarios and
(OC), where each feature is represented as a vector contain- allowing the development of specific implementations based
ingafeaturedescriptorforeachentityofinterest.Theagents’ on the hardware architecture. These templates are: grasp,release, move, manipulate, wait, and perceive. Grasp and accommodate varying numbers of agents, and it is possible
releaseinvolveopeningandclosingtheend-effector,respec- to assign a restricted subset of actions to each agent by
tively.Movereferstoanyagentmotionunrelatedtotheend- adjusting the preconditions of the actions. On the other
effector.Manipulatedescribesfinemotionsperformedbythe hand,runtimeflexibilityisensuredbytheplanner’srecursive
end-effector to alter the state of an object, such as activating nature, enabling dynamic adaptation to changing conditions.
a power tool. Wait and perceive are two actions that do
not directly affect the environment. Wait refers to instances
III. IMPLEMENTATION
where the agent must wait for an action precondition to be In a real HRC scenario, the formalism introduced in the
satisfiedbyachangeinthestate.Perceiveallowstheagentto previous section can be used by a robot to describe the
updateitsinternalrepresentationoftheenvironmentstate.Of environmentanddeterminewhichactiontotakeaccordingly.
course,noteveryagentcanperformallactions.Forexample, Tothisextent,alongsidetheHTNplanningmodule,itisnec-
a robot with a gripper-like end-effector cannot manipulate essarytodevelopaperceptionsystemcapableofrecognizing
objects,andafullypassiverobotcanonlyperceivethescene. the features that characterize the collaboration state.
Asmentionedearlier,eachactionaffectsthecollaboration In our implementation, the perception includes an ex-
state and has preconditions that must be met before its ternal camera, two wrist-mounted robot cameras, a single
execution. Table I highlights the state features that each tactile sensor on the robot gripper, and a set of wear-
actiontemplateaffectsandthestatefeaturethatcouldimpose able inertial measurement units (IMUs) monitoring human
apreconditionforexecution.Forexample,theReleaseaction motion. We developed the appropriate software module to
requires to satisfy a precondition on the EEA (i.e., the agent perceive a subset of the previously described state features.
is supposed to be holding something) and affects the EEA, In particular, the external camera, together with the wrist-
the AP, and the OC. The first two features are updated mounted cameras, is used to determine the positions of
considering the current state of the gripper, while, in this objects in the environment; this perception is simplified by
case, OC refer to the object property of not being grasped using ArUco markers attached to the objects. In addition,
by any agent. wrist-mounted cameras have been adopted to determine the
Considering the proposed interaction between state and object characteristic (OC) (i.e., being empty or not) of some
actions, we can model multiple collaborative tasks. Indeed, boxes containing small components. This way, through a
given the appropriate description of the state evolution and simplecoloursegmentationmodule,therobotcandetermine
associated actions, a single agent or a team can execute the whether the handled box is full or empty. Finally, the force
resulting plan. In the latter case, actions should be assigned sensormountedon thegripperallowstherobot todetermine
to the team members based on their capabilities. As such, another property of a grasped object, i.e., if grasped by
it is necessary to encode the proposed formalism within a another agent. Regarding the agents’ properties, the end-
framework that supports online implementation and allows effector availability and pose of the robot are measured
for the differentiation of agents’ skills. using its internal sensors, while the perception of the human
AsuitablesolutionforsolvingthisproblemisHierarchical relies on the IMU sensors. Specifically, users wear 4 inertial
Task Networks (HTNs). As introduced in Section I, HTNs sensors, placed on the backs of their hands and under the
are a formal planning framework for addressing complex wrist joint; the accelerations and angular velocities provided
problems by breaking them down into smaller hierarchically by the sensors are fed through an LSTM module to classify
structuredtasks.HTNplannersdistinguishbetweentwofun- whether the person is idle or not (F1 score 98%). Therefore,
damental types of tasks: primitive and compound. Primitive thesystemhasonlyapartialobservationofthehumanagent
tasksaretheelementalbuildingblocksofaplan,eachrepre- poseandnoinformationabouttheirend-effectoravailability.
senting a single executable step. In addition, primitive tasks Given the described sensorial setup, the robot does not
arecharacterizedbytheoperator,i.e.,alistofagentscapable have complete knowledge of the state features. Furthermore,
of performing the task, a set of preconditions that must be the Baxter robot is equipped with two simple grippers
met, and the effects of their execution. As such, the six unsuitable for complex manipulation. Therefore, it can only
actions introduced in our formalization can be implemented performactionsbelongingtotheclassesgrasp,release,move,
as primitive tasks within an HTN framework. In contrast, perceive, and wait. On the other hand, the human agent can
compound tasks result from the concatenation of several perform all six activities, but most of them are not directly
primitives. observed by the robot perception system. However, as stated
Moreover, HTNs’ hierarchical structure complies with P5 inSectionIV,therobotcanempiricallydeducewhichactions
as it provides explainable plans, which result in significant the human performed by observing the evolution of the
advantage when interacting with humans. Finally, this type environment, e.g., the motion of an object implies that an
of scheduler, together with the choice of state variables and agent performed a specific set of actions.
thesetofactiontemplatesillustratedabove,makesiteasyto As for the implementation of the actions, we distinguish
achieve a modular architecture (P1) and runtime flexibility the templates into two groups; the first one includes Grasp,
(P2). Regarding the modularity property, changes in the Release,andMoveactions,whichdirectlyinfluencethestate
perception pipeline only require redefining a single action variables of our system. The second comprises Perceive,
implementation. Moreover, the arrays of state features can which updates the planner’s internal representation of theFig. 2: Architecture diagram of the system. The HTN planner can activate the perception modules to update its state and
move the robot using the Joint Trajectory Client. The perception module is composed of cameras and wearable and tactile
sensors. The vision has three different modules: Localize Multiple Markers to identify and estimate the positions of the
markers in the scene, Refine Marker Pose to improve the estimated position of a single marker before grasping, and Box
Handover Detection to detect handover of small components such as screws. The wearables have been used to detect when
the human is idle and the Tactile sensing to automatize the handover of tools using the shear forces.
state, and Wait, which has been used to synchronize the content. Detect Tool Pulling refers to the tactile perception
collaboration. Considering our setup with a single stationary module, it is called when the robot performs the handover
robot and two simple grippers, we designed only one im- of an object to monitor the behaviour of the shear forces
plementation for each template in the first group. The Grasp sending a signal when an adaptive threshold is exceeded.
changes the gripper width to exert a force onto an object. Finally, Detect Idle activates the human activity recognition
Its preconditions require the end-effector location to match on the IMU sensors data stream firing a signal during the
the object location and the gripper end-effector availability idle time.
(EEA)statevariabletobetrue.ThisactionsetsEEAtofalse
IV. EXPERIMENTALSETUP
andupdatesthecorrespondinggripperjointstateintheagent
pose(AP)statevariable.Releaseistheoppositeoperationas The system described in the previous section was devel-
it opens the gripper releasing the object. The precondition is opedinPython,usingtheGTPyhop[34]librarytoimplement
that EEA must be false, and the effects include setting EEA the HTN planner. The workspace was framed using a Zed2
to true and updating the gripper joint state in AP just like in camera, and the collaborative robot Baxter by ReThink
the previous case. Move describes the kinematic motion that Roboticswasutilized.Fortheperceptionofhumanactivities,
leads the end-effector from location A to location B with a 4 MPU9250 IMU sensors [35] were worn on the human
given orientation. The only precondition is that location B hands and wrists, and a Tactaxis [36] sensor by Melexis1
must be in the robot’s reachable space, while the effect is was used for tactile perception. The robot’s trajectories
the update of the agent pose (AP) and, in case the robot is were planned using MoveIt!, the position and orientation
holding an object, the object pose (OP). Similarly, Wait has of the ArUco markers were estimated using OpenCV, and
a single implementation that pauses the planner until certain the communication between modules was managed using
conditionsaremet.Forexample,inourimplementation,Wait ROS Noetic. A graphic representation of the architecture
is used in conjunction with the tactile perception module to is provided in Fig.2, while the full code that drives these
stop the execution until the human grasps the object held experiments is publicly available on our GitHub repository2.
by the robot. As for the Perceive template, we provide five Additionally, we developed a collaborative assembly sce-
different implementations that refer to different sensors that nario showcasing all proposed functionalities and the plan-
constituteourperceptionpipeline.Additionally,theseactions ner’s ability to adapt to multiple consecutive interactions.
do not impact the state variables as they just update the Thisscenarioinvolvestwopeoplealternatingincollaborating
internal representation of the HTN; therefore, they do not with Baxter to assemble one of four pieces of IKEA furni-
have any preconditions or effects. Check Available Objects, ture. The available pieces of furniture are: i) an ODDVAR3
Precise Marker Detection and Detect Empty Box are the stool with 31 pieces, ii) a HUTTEN4 bottle rack with 24
implementation related to the vision. The first one activates pieces, iii) a KRITTER5 chair with 13 pieces and iv) a
the ArUco detection and pose estimation from the stream of RA˚GRUND6 paper roll stand with 16 pieces. To initiate
images obtained from the external camera and fills a list of interaction with the robot, each participant selects a piece of
available objects and their location. The second one refines furniture, wears IMU sensors, places markers on assembly
the pose obtained by the previous action applying the same
module on Baxter’s wrist camera and it is called when the
1melexis.com/en
2github.com/TheEngineRoom-UniGe/HTNPlanner
robotarrivesontopofamarkerduringapicktask.Lastly,the 3ikea.com/it/it/p/oddvar-sgabello-pino-20249330/
third one is activated when the robot performs the handover 4ikea.com/it/it/p/hutten-portabottiglie-9-scomparti-legno-massiccio-70032451/
of a box and it employs color segmentation to track the box 5ikea.com/it/it/p/kritter-seggiolina-bianco-40153699/
6ikea.com/it/it/p/ragrund-porta-carta-igienica-bambu-30253072/components, and positions them on tables near the robot.
Once the preparation phase is complete, the planner waits
for the user to be idle, adds the required actions to the plan
to construct the chosen object, and begins execution. The
human is aware of the action execution order and can refer
to the instruction manual for the object, if necessary. The
HTNplanforeachpieceoffurnitureismanuallybuiltusing
the primitives introduced in the previous section.
Duringruntime,therobotcollectsandhandsthenecessary
components to the human. However, depending on the type
of component, the transport modality differs. Using the
perception pipeline, if larger components such as stool legs
are detected, the robot retrieves them and places them on Fig. 3: The plots show the fluency metrics expressed as
a table near the user. The user can then grab the objects, a percentage of the assembly time. From left to right we
remove the markers, and proceed to assemble them. The provide human idle time, robot idle time, functional delay
human can also choose to pick up some of these items inde- and concurrent action time.
pendently, and the perception modules will update the plan
accordingly. The screwdriver and screw boxes are placed in
predefined locations, where the robot will collect them and
holdtheminitsgripperuntiltheuserneedsthem.Thetactile
sensor detects the handover of the screwdriver using shear
forces,whileacomputervisionalgorithmmonitorscontainer
contentthroughtherobot’swrist-mountedcamera.Theuser’s
primary objective is to assemble the components and secure
the screws, whose number depends on the selected object.
Some operations, such as connecting the legs of the stool to
the seat and tightening all the screws, may require different
times depending on the ability of the user. In that case, the
framework relies on the human idle recognition to detect
the end of the action. A video showing the whole assembly
process is available on our YouTube channel7.
V. RESULTS
The experimental scenario showed the ability of the
Fig. 4: Time needed for each action involving perception
architecture to adapt to the assembly of multiple objects
during the collaborative scenario. The different colours refer
with variable numbers and types of components. Each trial
to the perception modality associated with each action.
comprehended the assembly of four pieces of furniture and
lasted approximately 40 minutes. We repeated the process
for four trials and the planner was never stopped for the
entiredurationofeachtrial.Theonlypauseswereduetothe 0.23 seconds and the paper stand is the quickest at 0.08
waiting for the input with the name of the following object seconds. These delays are negligible compared to the time
to be assembled. After subtracting the intervals required required to complete the task, therefore, we can conclude
for component alignment and sensor exchange between the that the scheduler does not have a significant effect on the
experimenters, who are the only two users involved, the net performance of the framework, adhering to P4. Fig. 3 shows
average duration of the collaborative activity is 29 minutes the fluency metrics evaluated in our scenario. The results
and 28 seconds. Additionally, the average time needed to showthattherobot’sidletimeandconcurrentactionoccupy
build each object was 7 minutes and 31 seconds (SD=1.84). most of the total assembly time. Specifically, robot idle time
The assembly of the stool was the longest with 8 minutes has the highest average with 30.83% and concurrent action
and 25 seconds, while all the other objects had an average follows with 30.74%. The second category, however, has a
assembly time close to 6 minutes and a half. larger spread and manages to reach almost 40% in some
Thisresultisinfluencedbythenumberofactionsincluded instances. As for the human’s idle time, it has a lower
intheplanning,whichis545forthestool,385forthebottle average of 24.33% and the overlap time in which both
holder, 322 for the small chair and 195 for the paper stand. agents are idle is very low with 3.86% on average. Fig. 5
Similarly, the planning time is directly proportional to the shows an example for each of the four categories from the
number of actions; therefore, the stool takes the longest at experimental scenario.
It is necessary to observe that these percentages are not
7youtube.com/watch?v=Ogpvd0zKiU only influenced by the framework but also by the structureFig. 5: The picture represents four frames from the experimental scenario referring to the four fluency metrics. The frames
respectively represent Robot Idle time (a), Human Idle time (b), Functional Delay (c) and Concurrent Action (d).
of the task. In addition, robot idle time includes all actions entscenarios,whileadheringtofivekeyproperties:Modular
related to perception. Therefore, a large part of the robot’s Architecture (P1), Runtime Flexibility (P2), Extensible Task
idle time is due to awaiting human actions in assembly. Description(P3),LowComputationalOverhead(P4)andIn-
Additionally,Inthecurrentsystemimplementation,therobot terpretability (P5). Interpretability (P5) is inherently derived
is limited to performing only one action at a time and using from the hierarchical nature of our HTN planner, while P1
both robot arms in parallel for collecting the objects could and P3 are additionally shaped by the selection of specific
significantly decrease the human waiting. Finally, regarding actiontemplates,asoutlinedinsectionII.ToaddressP2,we
the spread of the data shown in the boxplots, we should designedactionimplementationsthatfacilitateerrorrecovery
point out that the framework allows adjustment of the plan and adjust to diverse human behaviours, further illustrated
at runtime to accommodate user preferences and re-plan in through a video demonstration. Additionally, we provided
case of an error. This feature, which introduces variability metrics for the planning time that evidence P4 is preserved
in action coordination, corresponds to point P2 in the list in our framework.
of properties defined in the introduction, and we show an Intheexperimentalscenario,twohumanagentsalternated
example of it in a second video on our YouTube channel8. in collaborating with a robot to assemble objects of varying
Referring to the function implementations provided in shapes and complexity. Notably, the collaborative process
SectionIII,weextrapolatedthetimerequiredtoexecuteeach proceeded seamlessly without interruptions in the planning
of them, and the results are represented in Fig.4. The data process achieving satisfactory fluency metrics. However,
shown only pertain to perception-related actions, as these our findings indicated that there is room for improvement
are the only ones that vary according to the behaviour of in the system. Firstly, in terms of perception capabilities,
the two agents and the state of the plan. It can be seen we recognize the importance of avoiding scripted locations
that the “Detect Idle”, “Detect Tool Pulling”, and “Detect for objects such as screwdrivers and boxes. Additionally,
Empty Box” activities, which need active user participation, enhancing human action recognition beyond simple idle
take longer and have a higher variation. “Detect Idle”, in and active states and using the results of this recognition
particular, has a significantly greater duration since it is as preconditions for other actions could further refine our
typically employed while waiting for the human to finish system. Finally, at present, the construction of the plan is
assembly operations. The sole exception is “Transfer and manually done offline and automating this process could
Precise Marker Location”, which has a variable duration greatly improve the versatility of the framework.
although it does not require interaction between the two
Moreover, we observed significant idle time for both hu-
agents.However,inourimplementation,thisactionincludes
manandrobotagents,withthedurationofperceptionactions
both the movement of the robotic arm and the refinement
primarily dependent on human decisions. As a promising
of the marker pose; therefore, most of the variability for
avenue for future work, we suggest exploring the potential
this action can be explained by the fact that the time
for parallel execution by involving multiple human agents
needed to complete each trajectory depends on its length.
simultaneously, especially when tasks such as assembling
Finally,“Wait”wasnotreportedinthegraphbecause,inour
chairs can benefit from utilizing both arms of the robot
implementation, it always takes place in conjunction with a
concurrently.Thisapproachpromisestoachieveevengreater
perception-related action, and its length correlates with the
efficiencyinfutureHRIapplications,aswecontinuetorefine
data already shown.
and expand upon our formalization framework and plan to
VI. CONCLUSIONS test it with a broader sample of participants. Additionally,
whileourframeworkprovidesruntimeflexibility,itcurrently
In conclusion, our study has introduced a flexible frame-
does not support dynamic task sequence adjustments, which
workforHuman-RobotCollaborationabletoadapttodiffer-
are critical for effectively handling significant deviations by
8youtube.com/watch?v=c2YShK02fsI humanagentsduringcollaborativeactivities.Futureresearchshouldfocusonenablingthesehigher-leveltaskadjustments [16] C. Petzoldt, D. Niermann, E. Maack, M. Sontopski, B. Vur, and
to improve collaboration efficiency and adaptability. M.Freitag,“Implementationandevaluationofdynamictaskallocation
forhuman–robotcollaborationinassembly,”AppliedSciences,vol.12,
no.24,2022.
ACKNOWLEDGEMENT
[17] L. Johannsmeier and S. Haddadin, “A hierarchical human-robot
The authors would like to acknowledge Melexis2 for interaction-planning framework for task allocation in collaborative
industrialassemblyprocesses,”IEEERoboticsandAutomationLetters,
providing the Tactaxis sensor, which constitutes the tactile
vol.2,no.1,pp.41–48,2016.
perception of the proposed architecture. Moreover, this re- [18] W.Ren,X.Yang,Y.Yan,Y.Hu,andL.Zhang,“Adigitaltwin-based
search was partially supported by the Italian government frameworkfortaskplanningandrobotprogramminginhrc,”Procedia
CIRP,vol.104,pp.370–375,2021.
under the National Recovery and Resilience Plan (NRRP),
[19] R.Zhang,Q.Lv,J.Li,J.Bao,T.Liu,andS.Liu,“Areinforcement
Mission 4, Component 2 Investment 1.5, funded from the learning method for human-robot collaboration in assembly tasks,”
European Union NextGenerationEU and awarded by the RoboticsandComputer-IntegratedManufacturing,vol.73,p.102227,
2022.
Italian Ministry of University and Research.
[20] Y. Cheng, L. Sun, C. Liu, and M. Tomizuka, “Towards efficient
human-robotcollaborationwithrobustplanrecognitionandtrajectory
REFERENCES
prediction,”IEEERoboticsandAutomationLetters,vol.5,no.2,pp.
2602–2609,2020.
[1] L.Wang,S.Liu,H.Liu,andX.V.Wang,“Overviewofhuman-robot
[21] A. Pupa, W. Van Dijk, C. Brekelmans, and C. Secchi, “A resilient
collaboration in manufacturing,” in Proceedings of 5th International
and effective task scheduling approach for industrial human-robot
Conference on the Industry 4.0 Model for Advanced Manufacturing,
collaboration,”Sensors,vol.22,no.13,p.4901,2022.
Cham,2020,pp.15–58.
[22] G. Evangelou, N. Dimitropoulos, G. Michalos, and S. Makris, “An
[2] F.Lorson,A.Fu¨gener,andA.Hu¨bner,“Newteammatesintheware-
approach for task and action planning in human–robot collaborative
house: Human interactions with automated and robotized systems,”
cellsusingai,”ProcediaCirp,vol.97,pp.476–481,2021.
IISETransactions,vol.55,no.5,pp.536–553,2023.
[23] M.-L.Lee,S.Behdad,X.Liang,andM.Zheng,“Taskallocationand
[3] K. Gleichauf, R. Schmid, and V. Wagner-Hartl, “Human-robot-
planning for product disassembly with human–robot collaboration,”
collaboration in the healthcare environment: An exploratory study,”
in HCI International 2022 - Late Breaking Papers. Multimodality in
RoboticsandComputer-IntegratedManufacturing,vol.76,p.102306,
AdvancedInteractionEnvironments,Cham,june2022. 2022.
[4] C.Qin,A.Song,L.Wei,andY.Zhao,“Amultimodaldomesticservice [24] A. Cesta, A. Orlandini, G. Bernardi, and A. Umbrico, “Towards a
robotinteractionsystemforpeoplewithdeclinedabilitiestoexpress planning-basedframeworkforsymbiotichuman-robotcollaboration,”
themselves,”IntelligentServiceRobotics,vol.16,pp.373–392,2023. in Proceedings of the 21st international conference on emerging
[5] A. Angleraud, A. Mehman Sefat, M. Netzev, and R. Pieters, “Co-
technologiesandfactoryautomation(ETFA),Berlin,Germany,march
ordinatingsharedtasksinhuman-robotcollaborationbycommands,” 2016.
FrontiersinRoboticsandAI,vol.8,p.734548,2021. [25] L. Han, S. Yang, and Q. Tyroller, “Cognitive human-robot-
[6] B. Mutlu, A. Terrell, and C.-M. Huang, “Coordination mechanisms collaboration in assembly: a framework for cognitive interaction
in human-robot collaboration,” in Proceedings of the Workshop on planningandsubjectstudy,”ProcediaManufacturing,vol.55,pp.24–
CollaborativeManipulation,8thACM/IEEEInternationalConference 31,2021.
onHuman-RobotInteraction,012013,pp.1–6. [26] G. Buisan and R. Alami, “A human-aware task planner explicitly
[7] A.Tellaeche,J.Kildal,andI.Maurtua,“Aflexiblesystemforgesture reasoning about human and robot decision, action and reaction,”
based human-robot interaction,” Procedia CIRP, vol. 72, pp. 57–62, in Companion of the 2021 ACM/IEEE International Conference on
2018. Human-RobotInteraction,Boulder,Colorado,USA,march2021.
[8] S.Maccio`,A.Carf`ı,andF.Mastrogiovanni,“Mixedrealityascommu- [27] M. Tewari and M. Persiani, “Towards we-intentional human-robot
nicationmediumforhuman-robotcollaboration,”inProceedingsofthe interaction using theory of mind and hierarchical task network,” in
39th International Conference on Robotics and Automation (ICRA), The 5th International Conference on Computer-Human Interaction
Philadelphia,USA,may2022. ResearchandApplications(CHIRA2021),october2021.
[9] F.DePace,A.Sanna,F.Manuri,D.Oriti,S.Panicucci,andV.Bel- [28] A. Favier, S. Shekhar, and R. Alami, “Robust planning for human-
camino, “Assessing the effectiveness of augmented reality handheld robotjointtaskswithexplicitreasoningonhumanmentalstate,”arXiv
interfaces for robot path programming,” in Intelligent Technologies preprintarXiv:2210.08879,2022.
forInteractiveEntertainment,Cham,2022,pp.336–352. [29] F. Stramandinoli, A. Roncone, O. Mangin, F. Nori, and B. Scassel-
[10] M.Shaaban,S.Maccio`,A.Carf`ı,andF.Mastrogiovanni,“Integrating lati, “An affordance-based action planner for on-line and concurrent
digitaltwinandmixedrealityinhuman-robotcollaboration,”102022. human-robotcollaborativeassembly,”in2ndICRAInternationalWork-
[11] S.Nikolaidis,M.Kwon,J.Forlizzi,andS.Srinivasa,“Planningwith shoponComputationalModelsofAffordanceinRobotics,Montre´al,
verbalcommunicationforhuman-robotcollaboration,”J.Hum.-Robot Canada,may2019.
Interact.,vol.7,no.3,pp.1–21,2018. [30] C. Petzoldt, D. Niermann, E. Maack, M. Sontopski, B. Vur, and
[12] B.Mutlu,F.Yamaoka,T.Kanda,H.Ishiguro,andN.Hagita,“Nonver- M.Freitag,“Implementationandevaluationofdynamictaskallocation
balleakageinrobots:communicationofintentionsthroughseemingly forhuman–robotcollaborationinassembly,”AppliedSciences,vol.12,
unintentional behavior,” in Proceedings of the 4th ACM/IEEE inter- no.24,p.12645,2022.
nationalconferenceonHumanrobotinteraction,LaJolla,California, [31] X. Zhang, S. Tian, X. Liang, M. Zheng, and S. Behdad, “Early
USA,2009,pp.69–76. prediction of human intention for human–robot collaboration using
[13] E. Calisgan, A. Haddadi, H. M. Van der Loos, J. A. Alcazar, and transformernetwork,”JournalofComputingandInformationScience
E. A. Croft, “Identifying nonverbal cues for automated human-robot inEngineering,vol.24,no.5,2024.
turn-taking,”inProceedingsofthe21stIEEEInternationalSymposium [32] G.Hoffman,“Evaluatingfluencyinhuman–robotcollaboration,”IEEE
onRobotandHumanInteractiveCommunication,Paris,France,2012, TransactionsonHuman-MachineSystems,vol.49,no.3,pp.209–218,
pp.418–423. 2019.
[14] V.Belcamino,M.Takase,M.Kilina,A.Carf`ı,A.Shimada,S.Shimizu, [33] E.Matheson,R.Minto,E.G.G.Zampieri,M.Faccio,andG.Rosati,
andF.Mastrogiovanni,“Gaze-basedintentionrecognitionforhuman- “Human–robotcollaborationinmanufacturingapplications:Areview,”
robot collaboration,” in Proceedings of the 17th International Con- Robotics,vol.8,no.4,2019.
ference on Advanced Visual Interfaces, Arenzano, Genoa, Italy, june [34] D. Nau, Y. Bansod, S. Patra, M. Roberts, and R. Li, “Gtpyhop: A
2024. hierarchicalgoal+taskplannerimplementedinpython,”HPlan2021,
[15] L. Lastrico, N. F. Duarte, A. Carf´ı, F. Rea, F. Mastrogiovanni, p.21,2021.
A. Sciutti, and J. Santos-Victor, “If You Are Careful, So Am I! [35] A. Carf`ı, M. Alameh, V. Belcamino, and F. Mastrogiovanni, “A
HowRobotCommunicativeMotionsCanInfluenceHumanApproach modulararchitectureforimu-baseddatagloves,”2024.
in a Joint Task,” in Social Robotics, ser. Lecture Notes in Computer [36] T. Le Signor, N. Dupre´, and G. F. Close, “A gradiometric magnetic
Science,2022,pp.267–279. force sensor immune to stray magnetic fields for robotic hands andgrippers,” IEEE Robotics and Automation Letters, vol. 7, no. 2, pp.
3070–3076,2022.