Int. J. System of Systems Engineering, Vol. x, No. x, xxxx 1
Expansion of situations theory for exploring shared
awareness in human-intelligent autonomous systems
Scott A. Humr* and Mustafa Canan
Information Science Department,
Naval Postgraduate School,
Monterey, CA 93943, USA
Email: scott.humr@nps.edu
Email: mustafa.canan@nps.edu
*Corresponding author
Mustafa Demir
Ira A. Fulton Schools of Engineering,
Arizona State University,
Mesa, AZ 85212, USA
Email: mdemir@asu.edu
Abstract: Intelligent autonomous systems (IAS) are part of a system of
systems (SoS) that interact with other agents to accomplish tasks in complex
environments. However, IAS-integrated SoS add additional layers of
complexity based on their limited cognitive processes, specifically shared
situation awareness (SSA) that allows a team to respond to novel tasks. IAS’s
lack of SSA adversely influences team effectiveness in complex task
environments, such as military command-and-control. A complmentary
approach of SSA, called situations theory, is beneficial for understanding the
relationship between SoS’s SSA and effectiveness. The current study elucidates
a conceptual discussion on situations theory to investigate the development of
an SoS’s shared situational awareness when humans team with IAS agents. To
ground the discussion, the reviewed studies expanded situations theory within
the context of SoS that result in three major conjectures that can be beneficial
to the design and development of future SoSs.
Keywords: artificial intelligence; human-machine interaction; IAS; intelligent
autonomous systems; shared situational awareness; situations theory.
Reference to this paper should be made as follows: Humr, S.A., Canan, M. and
Demir, M. (xxxx) ‘Expansion of situations theory for exploring shared
awareness in human-intelligent autonomous systems’, Int. J. System of Systems
Engineering, Vol. x, No. x, pp.xxx–xxx.
Biographical notes: Scott A. Humr is a PhD candidate in the Department of
Information Sciences at the Naval Postgraduate School. His research interests
include decision-making, trust in artificial intelligence, and sociotechnical
systems research.
Mustafa Canan is an Associate Professor in the Department of Information
Sciences at the Naval Postgraduate School. His research focuses on decision
making, information operations, situational awareness, implicit cognition, and
human-machine teams. Before joining the NPS, he was a National Research
Copyright © 200x Inderscience Enterprises Ltd.2 S.A. Humr et al.
Council (NRC) Postdoctoral Fellow at the Air Force Research Laboratory at
Wright-Patterson AFB. He holds two PhD degrees, in Particle Physics, and
Engineering Management. He teaches decision making in complex situations,
and project management, and has published articles in Nature, Nature
Communication, IEEE, and other leading journals and conferences.
Mustafa Demir is a senior quantitative research scientist at M2 Research
Enterprise in New York and a faculty teaching statistics at Arizona State
University (ASU) in Arizona, USA. He received his PhD in Simulation,
Modelling, and Applied Cognitive Science, focusing on team coordination
dynamics and effectiveness in human-machine teaming from ASU in the
Spring of 2017. His specific research interests include human-system
integration, operations research, dynamical systems, and quantitative research
methods – he has been a member of IEEE since 2015.
1 Introduction
Advancements in machine learning algorithms and Artificial intelligence (AI) are making
many automated systems more autonomous (Chiou and Lee, 2016). One of the shifts in
these technological advancements is the development of adaptive intelligent autonomous
systems (IAS) as team members who can interact with other agents (i.e., human or
machine) to accomplish a common goal. IAS is “the confluence of Autonomy with
Unmanned Systems and AI” in the context of Human-Machine Teaming (HMT) in
Command and Control (C2) task environments (Department of the Navy Strategy for
IAS, 2021). IAS enables organisations to automate a variety of individual tasks that
previously required human-level oversight, abilities, and support. Moreover,
accompanying advancements in sensors, onboard processing, and AI programming
sophistication are broadening the boundaries of where IAS can strongly operate. For
these reasons, we define the system of systems (SoS) level as a combination of both
humans and IAS (i.e., HMTs) operating towards a common goal while interacting with an
environment. The concept of an SoS composed of humans, machines, and accompanying
C2 systems that connect them is commensurate with Maier’s (1998) taxonomic
distinctions of an SoS and generic collaborative systems. These resulting developments in
IAS signify increased effectiveness and greater efficiencies in cognitive processes and
performance while reducing the mental and physical workload on human operators
(Matthews et al., 2021).
The introduction of IAS as a team member into C2 operations introduces new
dimensions of operations with humans operating. However, these new HMT formations
are also introducing uncertainty as the SoS level. For instance, IAS and accompanying
networked decision support systems could create cognitive overload by introducing an
overwhelming number of dynamic environmental factors (e.g., human and machine
communications) or presenting anomalous behaviours to humans who are part of this
larger SoS. Moreover, a human operator’s lack of understanding of IAS’s behaviours or
outputs may equally produce subsystem uncertainty (Canan et al., 2017). Such
algorithmic-induced uncertainty could degrade, slow, or impede decision-making
processes, resulting in higher risk to human team members and, in turn, task missions.
Current AI research in HMT explores team cognitive processes and performance at theExpansion of situations theory for exploring shared awareness 3
system (i.e., team) level, specifically the SoS level, including interaction (Klien et al.,
2004), trust (Bindewald et al., 2018), and decision-making (Kase et al., 2022). However,
in the context of human-IAS teams, additional in-depth research, especially in the
perspective of team situation awareness (TSA) is needed for engineering these systems
for appropriate use.
In general, TSA is defined as “…whereby each team member has a specific set of SA
elements about which he or she is concerned, as determined by each member’s
responsibilities within the team” (Endsley, 1995, pp.38–39). Later, Endsley’s TSA
perspective was extended by Gorman et al. (2012) by strongly emphasising
spatiotemporal aspects of team communication and coordination (i.e., sending
information to the right team member, at the right place, at the right time). However,
before sending information, the initial decision-making process must also be considered
within the TSA concept, especially in the complex domain. To close this gap, the current
study aims to understand the dynamics of team-level cognitive processes, including
communication, coordination, and decision-making, by exclusively expounding the SoS-
level of TSA. Additionally, TSA is more generally related to shared situation awareness
(SSA). The research uses SSA and TSA interchangeably in the HMT research (Ososky et
al., 2012; Schaefer et al., 2017; Wildman et al., 2014). Therefore, we use SSA throughout
the remainder of the paper.
To understand this phenomenon better, this study begins by defining IAS. Next, we
examine situational awareness and mental models. We then introduce situations theory to
explore how IAS fits within the reality domain perspective (RDP) to better understand
SSA among IAS and humans. Finally, by developing an expanded situations theory
approach to understanding human-IAS SSA at an SoS level, we outline significant
challenges with developing and engineering SSA between humans and IAS. In our final
analysis, we demonstrate that IAS lacks requisite variety, the ability to reason
counterfactually, and an algedonic feedback loop, and, therefore, cannot generate SSA
with human agents. These SoS engineering challenges are discussed and summarised.
2 Human and intelligent autonomous systems
IAS technologies pose many new challenges across the spectrum of teaming constructs in
sociotechnical systems engineering. Sociotechnical systems engineering takes the
principles and methods of sociotechnical systems (e.g., jointly optimising the complex
interactions between humans and machines) and applies them to engineering systems
(Baxter and Sommerville, 2011). These SoS are composed of numerous subsystems that
exhibit an intelligence that is more than the sum of its parts. To understand these systems
better, we address several different aspects of IAS intelligence and teaming in this
section.
2.1 IAS intelligence
The concept of intelligence is subject to a diversity of definitions. Intelligence may
include a variety of different aspects within a single definition (Schlinger, 2003). For
instance, Clark (2001) points out that human intelligence is hidden in complex and
dynamic interactions between technology and the brain (Clark, 2001). The label
“intelligent” is often used to describe an interaction behaviour, which creates a circular4 S.A. Humr et al.
explanation for the same behaviour (Schlinger, 2003). Stuart Russell (2019) states, “an
entity is intelligent to the extent that what it does is likely to achieve what it wants, given
what it has perceived” (p.14). Russell’s definition provides a performative element based
on some goal an entity may hold. However, Russell’s definition, like many definitions,
enables syllogisms in intelligence discussions. For instance, how does an entity develop
“wants” that are “intelligent,” or what if it achieves what it wants at the expense of some
higher-order virtues? Questions such as these leave most desiring a more comprehensive
definition.
A more comprehensive definition by Albus (1991) describes intelligence as “an
ability of a system to act appropriately in an uncertain environment, where appropriate
action is that which increases the probability of success, and success is the achievement
of behavioural sub-goals that supports the system’s ultimate goal” (p.474). First, the
notion of ‘acting appropriately in an uncertain environment’ connotes the fulfilment of a
system stakeholder’s reasonable expectation. However, deviations from a potential range
of expected behaviours can engender uncertainty. This definition is also commensurate
with INCOSE’s description of an open system, which is described as a “system with
flows of information, energy, and/or matter between the system and its environment, and
which adapts to the exchange” (Sillitto et al., 2019, p.11). Therefore, an important and
often overlooked aspect of intelligence in these definitions is the recognition of the
environment itself. The environment is concomitant with awareness and understanding,
both of which entail change. As a result, including the environment within the definition
of intelligence renders it an adaptive notion. In doing so, a distinction between human
and machine agent subsystems of the SoS can, in part, be achieved.
While intelligence is usually associated with human behaviour, it is not necessarily
limited to only anthropocentric conceptions. Intelligent machines such as computers have
augmented human capabilities and have allowed people to perform even more complex
work more efficiently. Assemblages of computers into networks, along with associated
miniaturisation, have allowed engineers to create advanced systems that can often mimic
degrees of human intelligence (Gere, 2009). Producing intelligent machines to replace or
relieve humans from the drudgery of dull, dangerous, and dirty occupations is, however,
not new (Sharkey, 2008). Yet, defining intelligent machines has proven difficult and is
the subject of much debate (Rudas and Fodor, 2008). Intelligent machines often
incorporate a range of technologies and methods from the fields of artificial intelligence,
cybernetics, operations research, and systems theory (Meystel and Messina, 2000).
Intelligent systems consist of a variety of heterogenous components that interact and are
interconnected through communicative feedback mechanisms (Merali, 2006). These
systems form a complex and integrated whole with clear goals (Choudhury et al., 2019)
which are evident in SoS. Yet, these heterogeneous systems can make disambiguating
intelligence difficult between the SoS level and the component level. When viewed at a
SoS-level, for instance, assemblages of human and machine agents jointly optimised
within a framework of a sociotechnical system can manifest emergent intelligence greater
than the sum of its parts. However, component-level analysis of individual intelligence
contributions may fail to capture the emergent properties of the apparatus of the entire
system. Hence, viewing IAS and the human components holistically develops an
appreciation for how SSA is based on the interaction these SoS components, and their
interaction with the task environment is developed and maintained.Expansion of situations theory for exploring shared awareness 5
2.2 Intelligent autonomous systems
IAS are systems that integrate advanced technologies, often with pattern recognition and
learning, that demonstrates intelligence by carrying out tasks in specified environments
independent of direct human control and supervision. The U.S. Navy (2021), for instance,
defines IAS as “the confluence of autonomy with unmanned systems (UxS) and artificial
intelligence (AI)” (p.4). These systems are characterised by their ability to mimic human
information processing by alleviating cognitive workloads for systems that exceed human
processing capacities (Matthews et al., 2021). Autonomy also implies a level of
adaptability that exhibits characteristics of not only self-regulation, but also self-
sufficiency (Hoffman et al., 2018). The idea of autonomy here is commensurate with
Ireland (2016) who describes autonomy as constituent systems that fulfil the overall
purpose or goal of the SoS.
IAS is also an umbrella term that encompasses a variety of technologies. These
technologies include self-driving vehicles, the Internet of Things (IoT), the Mars Rover,
and drones (Mashkoor et al., 2020). However, groupings of humans and IAS platforms
connected via a vast network of supporting technologies form a SoS and may generate a
complex situation characterised by different domains of awareness and perspectives. For
these reasons, understanding SSA as an ecologically relevant construct amongst humans
and IAS technologies is critical for command and control.
2.3 Teaming with an IAS
Teams can consist of two or more interdependent agents composed of humans or
machines. Although HMT is considered a newer concept, it has been in use with varying
levels of interdependency. HMTs can go by several names as well. These range from
human-autonomy teams (HATs) (Schaefer et al., 2021) to manned-unmanned teams
(MUM-T) (Das et al., 2018). Damacharla et al. (2018) define “HMTs as a combination of
cognitive, computer, and data sciences; embedded systems; phenomenology; psychology;
robotics; sociology and social psychology; speech-language pathology; and visualisation,
aimed at maximising team performance in critical missions where a human and machine
are sharing a common set of goals” (p.38637). The comprehensiveness of this definition
signifies the complex nature of these advanced systems and the challenges that arise
through these interactions. Additionally, what makes the concept of HMT challenging
today is the inclusion of intelligent machines and their associated behaviours with higher
task interdependency without the capability of eliciting any causal reasoning for its
behaviour. More importantly, in the context of SSA between humans and machines, the
challenge of negotiating rationality for two (human and machine) as Russell (2019)
argues, the intelligent machine in an HMT produces complex behaviours humans cannot
rationalise. As a result, humans may not understand or be able to explain a machine’s
behaviour (Phillips et al., 2011). To this end, an important step to ameliorate human
understanding of machine behaviour is having an observable reasoning process based on
its external behaviours. Such reasoning has provided leading motives for the development
of Explainable AI (XAI) (Jacovi et al., 2021). Yet, there are currently limited theoretical
constructs that adequately address teamwork in the context of complex environments
with heterogenous teams of humans and intelligent machines (Cooke et al., 2020;
Schaefer et al., 2021). The incomprehensibility, concomitant with the absence of an
inclusive theory, makes understanding between humans and machines difficult. A great6 S.A. Humr et al.
deal of research on situation awareness has attempted to address this, yet, there is still a
notable lack of literature on the concept of SSA (Canan and Sousa-Poza, 2019).
Situations, such as military operations, may present complex circumstances where IAS
and humans may focus on different aspects of information or arrive at different
interpretations of the same information. However, SSA requires more than the sharing of
information. For example, differences in shared meanings result in contradictory
interpretations of battlefield information which are unconducive to SSA (Canan and
Sousa-Poza, 2019). Hence, situations may entail different or suboptimal decisions,
decision aporia within teams, or poor judgements, all of which may impede timely or
appropriate actions at the SoS level.
3 Shared situation awareness
Shared situation awareness is an important concept in group decision-making. Because
information is what is ultimately shared between agents within an SoS, particular
concepts are fundamental to appreciate information’s ecological relevance. This below
sections introduce several concepts that build on each other to develop a full-orbed
understanding the phenomenon and its importance for engineering heterogenous systems
of humans and machines.
3.1 Shared awareness
Shared awareness is a dynamically adaptive and ecologically relevant cognitive construct
that requires the existence of rationality of two or more agents. Alberts et al. (2001)
define shared awareness as “a state that exists in the cognitive domain when two or more
entities are able to develop a similar awareness of a situation” (p.26). Agents that achieve
shared awareness are assumed to have a functional dependency regarding the
comprehension of salient aspects within an environment for achieving a particular goal
(Canan et al., 2015). Shared awareness is also important for synchronising actions,
especially when situations deviate from pre-planned behaviours (Alberts et al., 2001).
However, for such sharing to take place, several streams of concepts require convergence
to create the concept of shared awareness. Expanding on shared awareness, Nofi (2000)
points out that “awareness of a shared situation” implies that a group understands they
exist within the same situation; thus, shared awareness of a situation connotes that the
group “understands a given situation in the same way” (p.12). Key to awareness also
involves a level of transparency of both shared intent and shared awareness (Lyons and
Havig, 2014). Yet, understanding a situation in the same way is a complex concept
because it implies sharing a single mental model and judgements for how the situation
will unfold, given various potential actions. For instance, two agents may share the same
social reality, but categorise the phenomenon in two distinct ways as depicted in Figure 1.
This discrepancy can therefore result in different actions taken by each agent. However,
agents can develop shared awareness in other ways as well.
Alberts et al. (2001) depict four ways two agents can develop shared awareness in
Figure 2. However, this depiction of shared awareness is imperfect for several reasons.
First, it makes a bid for a three-level ontology. This three-tier ontology makes distinct
cuts that remove the nuances of agent interactions with the physical environment.
Second, it places the observers outside of the physical realm creating a disconnectExpansion of situations theory for exploring shared awareness 7
between cognitive and physical domains; agents, rather, interact across all three
dimensions. Third, it is unclear how two agents, human or otherwise, interact or share a
cognitive domain in Alberts’s conceptualisation. If one takes the view that cognition can
be extended, as Clark and Chalmers (1998) advocate, cognitive artefacts also inhabit the
physical world. Alberts et al. model, therefore, provides a confined view of the process of
shared awareness that is difficult, if not impossible to achieve. In short, a trifurcated
ontology of shared awareness fails to capture how agents interact within situations. These
shortfalls demonstrate the need for a more thorough model of awareness to account for
developing awareness to better understand if or how a human and IAS may form a truly
human-machine team.
Figure 1 Sharing the same social reality with different understandings (see online version
for colours)
Figure 2 Achieving shared awareness (see online version for colours)
Source: Adapted from Alberts et al. (2001)
For these reasons, shared awareness may be partially conceptualised and explicated
between human and machine agents through shared mental models.8 S.A. Humr et al.
3.2 Shared mental models
Humans often simplify objects and explanations into heuristics via mental models.
Mental models represent relationships and sets of assumptions about a system (Ford,
2019). The meanings of mental models are also embedded within a network of other
relationships and models (Carley and Palmquist, 1992). In general, mental models
provide insight into relevant internal representations that can be activated in situations for
comprehension and prediction of the system’s future states (Converse et al., 1991).
Mental representations contain both declarative and procedural knowledge (Al-Diban,
2012). Procedural knowledge consists of knowledge “know-how” while declarative
knowledge is characterised by knowledge of facts and events (ten Berge and van
Hezewijk, 1999). Mental models, therefore, combine elements of both procedural and
declarative knowledge to instantiate representations of a specific phenomenon within
human cognition. Such mental models, while limited in some regards, may also become
relatively enduring (Ford, 2019). Additionally, a mental model is an operator’s internal
representation of a situation or object. A mental model, therefore, allows for the
prediction and explanation of outcomes and exerts a major influence on team processes
(Matthews et al., 2021). Thus, mental models provide a lens to filter future behaviours to
help with prediction.
Mental models afford comprehension of complex situations by extrapolating the state
of a system based on the general behaviour of its components. The ability to predict the
future behaviour of a system, therefore, requires a comprehensive mental model (Lee and
Moray, 1989). Mental models provide a coupling between a system’s state and desired
goals for allowing a simulation of potential actions (Bryant, 2006; Oury and Ritter,
2021). However, due to information loss through faulty cognitive memory or systems
change, a mental model can deviate from reality (Moray, 1996). For example, researchers
found that confirmation bias can play a detrimental role in supporting erroneous mental
models (Besnard et al., 2004; Kahneman, 2013). While no model is perfect, inaccuracies
within models can become more problematic within team settings where agents attempt
to develop shared mental models.
Shared mental models are defined by the degree members of a team mutually
comprehend a situation (Cannon-Bowers et al., 1993; Klimoski and Mohammed, 1994;
Van den Bossche et al., 2011). Shared mental models have been studied in a variety of
team contexts, such as cyber security teams (Cotoranu and Chen, 2020), augmented
threat detection (Matthews et al., 2021), emergency response management (McNeese et
al., 2021), and information systems successes and failures (Carley, 1997). Yet, not all
pairings of human and machines are equal. Annett and Stanton (2000) state, a team is a
type of group, but not all groups may constitute teams. The key difference hinges on
whether team members share a common goal pursued collaboratively (Annett and
Stanton, 2000). For instance, the similar concept of shared team cognition is often
described in terms of how a team’s individual mental models overlap with each other
(Cannon-Bowers et al., 1993; Matthews et al., 2021) and, in turn, exhibit high team
performance (DeChurch and Mesmer-Magnus, 2010; Johnson-Laird, 1983; Mathieu et
al., 2000; Matthews et al., 2021; Rouse et al., 1992). Not surprisingly, Espevik et al.
(2006) found that shared mental models added to performance above and beyond
operator skill alone. For these reasons, shared mental models have similar roots in other
cognitive constructs within the literature.Expansion of situations theory for exploring shared awareness 9
Perspectives on shared mental models have taken the forms of other cognitive
constructs as well. Concepts such as distributed cognition and transactive memory are
knowledge focused and act as repositories team members can access to accomplish their
goals (Cooke, 2015; Wegner et al., 1985). These concepts, including other boundary
objects such as charts, displays, and notes, imply that cognition extends outside of the
human mind into the world, which is made accessible to others (Clark and Chalmers,
1998). Similarly, Cooke (2015) states that cognition is often found in checklists, digital
artefacts and within the surrounding context of individuals.
If cognition does in fact, extend into shared spaces, then it follows that teams can
share cognition through such artefacts. Taking the idea of extended cognition even
further, Cooke et al. (2013) proposed a theory of interactive team cognition (ITC). ITC
posits that team cognition is made explicit through interactive communications among its
members (Cooke, 2015). Likewise, Nofi (2000) states that in sharing individual mental
models, “[c]ommunication is the most critical issue in creating shared awareness” and is
part of sharing perception of a situation (p.29). ITC, therefore, can be seen as an
emergent property of the team when viewed from the SoS level and, therefore, performs
two important functions for understanding shared mental models. First, the degree of
sharing in mental models can be explicit through communications; second, the degree of
sharing between mental models can permit measurement through lexical content analysis,
for instance (Carley and Palmquist, 1992; Hutchins et al., 2011). Equally significant,
team interactions demonstrate a generative process that gives rise to the importance of
how information is shared amongst team members. Yet, it is still unclear how humans
and machines could share a mental model when there is still no tractable or
computationally shared mental model implemented within a robotic system (Scheutz,
2013; Scheutz et al., 2017). Consequently, this leads to a conceptually impoverished view
of SSA of the human-machine teams.
3.3 Shared situation awareness
An equally important concept is situation awareness. Tremblay and Banbury (2004),
citing Bryant et al. (2004) define “situation awareness (SA) as a cognitive construct that
refers to an awareness and understanding of external events in our immediate and near
future surroundings” (p.104). The concept, by definition, implies a particular spatial and
temporal situation for an agent. Situations can range from simple to complex depending
on the interplay between dynamics within a variety of different contexts. Similarly,
Endsley (1995) is careful to describe SA as a “state of knowledge” and the process for
achieving SA as a “situation assessment” (p.36).
SA is composed of several components. First, Endsley (1995) defines SA as
“situation awareness is the perception of the elements in the environment within a
volume of time and space, the comprehension of their meaning, and the projection of
their status in the near future” (p.36). Endsley’s definition consists of three levels of SA:
1 perception
2 comprehension
3 projection.
Level 1 SA (perception) is the process of perceiving essential attributes of the particular
task environment (Endsley, 1995). This can be accomplished sensorially, whether it is10 S.A. Humr et al.
observed directly or mediated through digital sensors. Level 2 SA (comprehension)
involves synthesising elements of Level 1 SA and one’s knowledge to create an
understanding of a situation (Endsley, 1995). Level 3 SA (projection) entails constituent
elements of Level 1 and 2 SA along with knowledge of the spatial-temporal aspects of a
situation to forecast a potential future state or its consequences (Endsley, 1995).
However, Endsley’s model does not specifically address sharing between agents and
principally addresses individual SA.
Current models of SA suffer from several shortcomings in conceptualising shared
situational awareness. First, Endsley’s model is primarily a descriptive theory (Bryant et
al., 2004). While descriptive theories are helpful for guiding experimental inquiry, they
lack the ability to make predictions and, therefore, provide little explanatory power
(Bryant et al., 2004). Thus, Endsley’s model lacks explanatory power which hinders
relating the SA concept to information processing at the system level. In other words,
without an understanding from the process perspective of how individual SA is related to
multi-agent informational processing, it is unclear what influence it ultimately plays in
accessing decision-making and performance at the SoS level. Second, Endsley’s SA
model makes it an isolated phenomenon disconnected from more encompassing team
processes, such as interdependency and relative information gain. Bryant et al. (2004)
argue:
“Because command and control is not an individual task but rather an intricate
set of organisational procedures, we must finally also consider SA as an
organisational framework. That is, as well as being the situational
representation of the battlespace, SA exists in the organisational structure used
to assign information processing roles, decision-making authority, and
responsibilities to push information needs down to the sensor level and pull
relevant information up to the decision-making units.” (p.111)
In other words, SA is a dynamically collective phenomenon inextricably linked to the
interaction between components and the task environment. Teams can achieve SSA by
beginning with a shared awareness via team interaction (i.e., communication and
coordination), mental models of the environment, tasks at hand, and common goals.
When teammates share a situation about the current or future state of the system, the
degree of similarity between mental models, and knowledge about a mission and tasks,
are critical to improving team performance (Mathieu et al., 2000; Schaefer et al., 2021).
Similarly, Salas et al. (2005) posit that team adaptability requires members to have a
global perspective of the team tasks, understand how changes may alter a team member’s
role in relation to the team tasks, and be able to recognise changes in other team
members, the task, or the environment as they occur. Recognition of team, task,
teamwork, and the environment’s dynamics implicitly assumes an awareness to glean
changes. However, physical separation may inhibit awareness through noise within the
environment or communication breakdowns. Team SA (TSA) would likely become more
challenging as multiple perspectives increase the dimensions and degrees of
dependencies (Dekker and Lützhöft, 2004). To this end, SSA differs from shared
awareness of a situation. Shared awareness of a situation, therefore, carries an important
nuance, which requires yet further elaboration.Expansion of situations theory for exploring shared awareness 11
4 Expansion of situations theory
Hitherto studies on shared awareness considered shared mental models and SSA.
However, these concepts have not provided a comprehensive understanding of what
exactly is being shared. To share an understanding in toto requires going beyond sharing
of information but also includes shared awareness of a situation (Canan and Sousa-Poza,
2019). Moreover, sharing implies mutual communication and coordination for an
understanding of particular aspects of reality for the purpose of achieving some mutual
goal or end-state (Gorman et al., 2020). Yet, the question remains, how is understanding
generated in the first place? To apprehend how agents can share understanding or
awareness for behaving intelligently, Situations Theory (ST) is introduced (Sousa-Poza,
2013).
Intelligence requires consideration of the agents and the environment defined by a
particular situation. Dekker and Lützhöft (2004) define a situation as “a nested set of
constraints that have the potential to shape performance” (p.44). Situations often involve
multiple agents and dynamic environments that yield a complex situation. For this reason,
ST helps facilitate understanding of complex situations insofar as the perspectives it
generates address the encountered problems (Sousa-Poza, 2013). ST defines a situation as
“a set of conditions that human beings expand on with the requirement that an individual
is or becomes cognizant of the set of the condition(s)” (Canan et al., 2015, p.267).
Consequently, ST is a meta-theoretical construct that considers the surrounding
conditions of a problem while focusing on understanding the generated agential
perspectives for addressing problems within a situation (Canan et al., 2015). Key to
understanding ST is the construction of the RDP model.
4.1 Reality, domain, and perspective model
The RDP model considers three levels developed through awareness of the self and an
other-than-self in a system, e.g., a team (Canan et al., 2015). In ST, the self refers to a
self-reflecting agent who can represent the thoughts, perspectives, and intentions of
another agent (e.g., theory of mind). The other-than-self refers to other agents that the self
seeks to share a situation with (human or machine). However, depending on the order in
which one arrives at an awareness of self or other than self, such different perspectives
may change how one perceives a particular reality (e.g., order effects). To circumvent
this primacy problem, Canan et al. (2015) proposed to view this conundrum as a
generative process (GP). A GP resembles a rhizomatic process that has no one starting
point. Rhizomatic processes do not follow a linear evolution but is rather an acentric
multiplicity that does not develop from a single starting point (de Freitas, 2012).
Therefore, we expand on the previously proposed concept of a GP with the additional
introduction of liminality.
4.2 Generative process
A GP describes a system state in continuous transition. A GP, therefore, lies within a
space of liminality, which is between a previous state and a future state of a system. The
GP is a subsystem of the SoS because it is related to human cognition as an agent forms
awareness of self and other than self (Canan et al., 2015). A GP instantiates a liminal
space through an evolving process that results from the onset of a problem. The onset of a12 S.A. Humr et al.
problem generates a domain of self-awareness in an agent which is an abstraction from
reality. A domain of self-awareness is then sustained and maintained by an agent’s
cognition as an agent balances the exploration and exploitation of information within the
environment. The domain of awareness can thus expand and contract as information is
sought, found, compared, and discarded through this system subprocess. As a result, the
domain of awareness occupies a continuous space of liminality (e.g., state of continuous
transition). Yet, as part of the larger SoS environment, the domain of awareness exhibits
dissipative characteristics.
Domains of awareness can be viewed as dissipative system which are systems far
from an equilibrium point and yet, maintains a systems’ stability (Capra, 1996).
Dissipative structures also receive their sources of energy from exogenous sources and
can amplify feedback which may cause the system to evolve (Anderson and Meyer, 2016;
Capra, 1996). Since agents within a system or team are part of their task environments,
they continuously process information both implicitly and explicitly. Agents interact with
their environment to maintain a continuous influx of salient information to maintain the
structure of a domain of awareness to prevent it from collapsing. However, domains of
awareness are not synonymous with the environment, but is rather an abstraction from the
environment. An abstraction distance represents the degree of induced complexity
(Canan and Sousa-Poza, 2016). If domain of awareness no longer takes in new
information to maintain its structure (e.g., maintaining congruency and coherence with
the environment), the domain of awareness becomes incoherent, which may result in a
steady state system, but one that fails to correctly model reality. In other words, when a
domain of awareness no longer resembles reality, the domain of awareness becomes
incongruent with the environment. Domains of awareness are, therefore, ephemeral and
require the addition of relevant information on a periodic basis to maintain a coherent and
congruent understanding of the environment and the problem. Most importantly, domains
of awareness also sustain a second-order structure called a perspective.
4.3 Perspectives
Perspectives are abstractions within a domain of awareness and represent what is
understood of the situation (Canan et al., 2015). Kim et al. (2019) state that perspectives
are also viewpoints to which agents may assign different weights to concepts that are the
most relevant foci. Perspectives can therefore behave as basins of attraction for
information within a domain of awareness as perspectives necessitate information
gathering. Additionally, perspectives may also influence the selection of mental models
that provide the framework for organising information from the environment. In Figure 3,
the congruency of the perspective to reality is judged by an abstraction distance, A’(d).
A”(d) denotes what is understood from what is comprehended. The complexity of the
situation is designated by A(d) which is a combination of A’(d) and A”(d). As
information enters the system, the distances (d) will fluctuate and influence change in the
other distances as well. For instance, when an agent conducts exploration, they are
attempting to decrease A’(d). Conversely, when an agent conducts exploitation, they are
attempting to decrease A”(d). Agents within this system will expend resources and time
on both exploration and exploitation to decrease A(d) by decreasing A’(d) and A”(d) to
take an action. Once an action is taken, a new state is reached which generates a new
domain of awareness. As a result, abstractions and perspectives are reciprocal phenomena
which mutually emerge through the process of interacting with the environment. ThisExpansion of situations theory for exploring shared awareness 13
conceptualisation of relating information exploration to reducing A’(d) and information
exploitation to reducing A”(d) is an augmentation to current ST that has not existed
before.
Figure 3 The domain of awareness and perspective with respect to reality
Source: Adapted from Canan et al. (2015)
New states within the domain of awareness are reached through actions which could
result from conjectural statements, or claims (Canan et al., 2015). These conjectures
become the models which are viewed as a kind of hypothesis test. Subsequently, what is
understood becomes determined by acting upon these models and the feedback one
receives. Feedback then helps determine the usefulness of the models. These models are
typically tested through the act of human cognition, namely through scenarios supported
by counterfactual thinking.
Counterfactual thinking is fundamental to being human and to understanding in
general. Pearl and Mackenzie (2018) state that counterfactuals are building blocks of
scientific thought. Counterfactuals are events that have not taken place, but through
imagination, a human agent rationalises a future or past that could or would be different
(Starr, 2021). In other words, counterfactuals are conceptions of past or future events that
may have turned out differently if certain conditions were met, ceteris paribus. For
example, the nature of scientific hypothesis testing asks “what if” questions which are
formulated into testable, falsifiable experiments. Similarly, when humans develop
conjectural statements or claims, they are engaging in a rudimentary form of hypothesis
formation or development of a provisional counterfactual. To understand this interplay
better, we expand ST by adding a domain of counterfactuals, thus introducing the
Expanded Situations Theory.
4.4 Expanded situations theory
Expanded Situations Theory (EST) amends ST by the addition of the counterfactual
domain. Counterfactuals are informed by an agent’s history (hysteresis), experiences,
training, and attitude. More importantly, counterfactuals are used for making plans and
assessing outcomes, which are essential for learning and prediction (Narayanan, 2010).
As Weick (1985) states, “[t]o go beyond detail is to move to higher levels of abstraction
and to invoke alternative realities” (p.61). Therefore, the counterfactual domain is14 S.A. Humr et al.
essential for understanding shared awareness for three reasons. First, the counterfactual
domain is fundamental to assessing mental models. As a domain of awareness is
developed, the generated perspectives will favour certain mental models that may best
explain a phenomenon. Succinctly put, a perspective selects a particular model of
behaviour that best explains the data. Subsequent actions are then taken by the system
agent to confirm or update the mental model. Simulating these mental models based on
the data likewise generates counterfactual thinking. This process continues until a
sufficient reason for action is taken or the situation changes. Feedback effects from
actions also update an agent’s mental models for future use. Second, counterfactuals are
essential for understanding causal models of the world. Sharing a causal model implies
that two agents also share the same counterfactual judgements (Pearl and Mackenzie,
2018). Sharing judgement is, therefore, sufficient for developing consensus and making
decisions in a team setting, which are essential for positive team behaviours. Third, the
counterfactual domain represents a realm of possible states, with some being more ideal
than others. As Dewey (1933) states, “[t]o grasp the meaning of a thing, an event, or a
situation is to see it in its relations to other things: to see how it operates or functions,
what consequences follow from it, what causes it, what uses it can be put to…” (p.137).
Stated differently, counterfactual reasoning creates a space to cognitively simulate a
system to help understand the outcomes of decisions. Understanding through
counterfactuals thus allows agents to act rationally within their systems. The
counterfactual domain thus co-generates the perspective with reality to create, bound, and
bind a domain of awareness. The counterfactual domain, therefore, holds the domain of
awareness in tension with reality and can drive actional behaviours of information
exploration and exploitation. Figure 4 shows the amended RDP model with the
counterfactual domain represented by “C”. The addition of the counterfactual domain not
only bounds the abstraction distance but provides a necessary supplement to assessing
differences between human and IAS agents within team settings requiring shared
awareness within an EST.
Figure 4 Expanded situations theory (EST)
Source: Adapted from Canan et al. (2015)
5 Three areas of investigation
Expanded situations theory can provide a meta-theoretical framework for understanding
human and IAS interactions in complex task environments, which require proactiveExpansion of situations theory for exploring shared awareness 15
interaction between the team members (either human or machine) to attain a shared
understanding of the situation (Atkinson et al., 2014). On this more solid footing of EST,
three areas that present challenges to achieving shared awareness with IAS:
1 requisite variety in IAS
2 counterfactual reasoning in IAS
3 algedonic feedback loops within the system.
5.1 Requisite variety in IAS
Developed by Ashby (1991), the law of requisite variety simply states that a system
should exhibit a number of different responses to address inherent complexity within any
particular environment. Thus, only a system’s variety in the number of possible responses
can delimit the number of potential system outcomes (Beer, 1984). In other words, a
system must have an equal or greater variety than the number of environmental
perturbations to avoid failure or collapse. When addressing shared awareness between
human and IAS agents, requisite variety plays a vital role in determining the possible
outcomes a human-machine system may exhibit when developing a domain of awareness
or shared awareness.
Developing and sustaining a domain of awareness requires an adequate amount of
information from the task environment in which the agents are subject to. Information
may be gathered through a variety of different sources, such as videos, reports, news
feeds, and the like. However, functions of IAS often operate at run time which makes it
subject to dynamic inputs from the environment (Johnson and Verdicchio, 2017). While
humans can consume a variety of different media in different ways to build a domain of
awareness, IAS platforms may be limited. For example, autonomous systems such as IAS
platforms will have a limited array of sensors or are equipped with only a small number
of different actuators that inherently bounds the range of what it can receive and, and by
extension, what it can perform (Johnson and Verdicchio, 2017). These limitations
accentuate three issues for achieving shared awareness when it comes to requisite variety.
First, human agents will not know the variety of possible inputs an autonomous platform
will actually sense and, therefore, not know how the platform may respond, thus making
it less rational (Johnson and Verdicchio, 2017). This lack of understanding thus limits
what a human agent can possibly understand. Second, if IAS platforms provide inputs to
decision support systems such as target identification in military operations, which also
populates a common operational picture, the IAS’s algorithm filtering may prevent or
suppress any number of other important environmental attributes, as portrayed in
Figure 5. Limiting to only what IAS platforms can detect, may result in an incomplete
picture of reality, which can lead to poor decision-making. Third, intelligent systems are
computationally constrained, which determines their range of behaviours (Pothos and
Pleskac, 2022). Such computational constraints can potentially limit rational behaviours
and inhibit timely decision making. Overall, the results are two incongruent domains of
awareness and constrains an agent’s (e.g., IAS) requisite variety.
Resolving two different domains of awareness between a human agent and an IAS
may result in several other unintended consequences. For instance, human agents can
directly limit or constrain an IAS’s autonomy by attempting to resolve uncertainty within
an environment through redirecting an IAS platform to focus on a particular area.16 S.A. Humr et al.
Through such actions, time is now spent supervising autonomy over exploiting or
synthesising information. Moreover, new interventions from human agents could
potentially introduce new or unpredictable system behaviours which require human
agents to have an unanticipated requisite variety (Behymer and Flach, 2016). Yet, shared
awareness still may be unachievable due to the technological mediation of the
environment. Weick (1985) points out that for “[systems] to register and absorb
information (to listen and think), the sensor must be at least as complex as the
information it is receiving and, often, information systems fall short” (p.61). Moreover,
Canan et al. (2022) state: no extra information can be obtained to reduce ontic
uncertainty; it can only be resolved when the system interacts with the environment.
Therefore, if the systems are unable to truly capture the complexity and a human agent,
as part of the more extensive system, IAS cannot directly interact with the environment,
then shared awareness is not genuinely possible. As a result, IAS has a constrained
requisite variety and could introduce the need for additional variety not previously
foreseen, which ultimately prevents shared awareness.
Figure 5 Constrained domain of awareness
5.2 Counterfactual reasoning in IAS
Counterfactual reasoning allows agents to contemplate what-if types of questions that
help sustain the domain of awareness and the types of perspectives that could be adopted.
The very notion of planning or simulating implies a rational intention to proceed from a
current state to a desired future state by manipulation of system variables or the
environment. As van de Poel (2020) states:
“The fact that human agents can take an external perspective and can reflect
implies that they are able to improvise in unexpected situations and can
therefore contribute to the proper functioning of the system and the
achievement of certain values which would not have been realised without their
intervention.” (p.397)
However, it is known that computers today are largely relegated to the “seeing” rung of
Pearl and Mackenzie’s (2018) ladder of causation. Figure 6 demonstrates how SA maps
to Pearl and Mackenzie’s (2018) ladder of causation. Without the ability to reason
counterfactually, IAS agents’ communication and coordination to human agents can
suffer. Counterfactual reasoning provides the necessary “why” for prospective actions
and actions not taken. Nevertheless, this may be an unreasonable requirement inExpansion of situations theory for exploring shared awareness 17
environments that have many dynamics and levels of understanding, such as military,
environmental, and political objectives that must be considered. For this reason,
attempting to bridge the shared awareness gap between a human agent and IAS will
suffer from several subtle sub-goals as well.
Figure 6 Levels of situational awareness and ladder of causation
Source: Adapted from Pearl and Mackenzie (2018)
First, the autonomy of a machine will likely be a composition of many different types of
code bases. For instance, some parts of the system will be deterministically established at
compile time, while other components that drive behaviour are established at run-time
(Johnson and Verdicchio, 2017). This makes autonomy a system composed of an
admixture of deterministic and non-deterministic components that can make
understanding behaviours difficult. Moreover, behaviours that entail judgement or
intuition fundamentally lack a formal mathematical representation, thus making it
extremely difficult to code for within the system (Cummings, 2014). Second, these
formal rationalities exhibited by intelligent machines are not only deterministic, but can
also suppress a human’s substantive rationality needed to make better decisions
(Lindebaum et al., 2020). Third, high levels of consistency within these systems have
also been shown to be detrimental to performance. For instance, an aerial drone equipped
to autonomously take off from the same location unintentionally created divots in the
runway overtime until one day, the programmed thrust was not able to move it from this
position, which resulted in a flight cancellation and an a detailed investigation into the
system (McLean, 2022). In a similar case, autonomous mining trucks had to be
programmed to vary their routes over time to avoid creating ever-deepening tracks in the
road that made it difficult for other vehicles to operate (Cummings, 2014). These two
examples demonstrate that a lack of counterfactual thinking may result in behaviour that
becomes too predictable because it lacks the foresight to anticipate suboptimal
consequences. While such consistent behaviour may seem beneficial for shared18 S.A. Humr et al.
awareness, it is not always the case. Rather, shared awareness implies intentionality and
counterfactual reasoning that anticipates how rote actions may result in undesirable
system behaviours at higher system levels. Therefore, without the ability to reason
counterfactually, it will be improbable for human agents and IAS to develop shared
awareness.
5.3 Algedonic feedback loops within the system
All systems require feedback mechanisms to drive behaviours toward established goals.
Feedback is simply information but can manifest in many different forms. Of interest are
feedback loops of consequence or those which could particularly result in high
uncertainty in mission failure or significant injuries to humans. Principally, algedonic
feedback are cybernetic loops that signal pain or pleasure upon a part of the system (Beer,
1972). Pain or pleasure in this instance, may not necessarily be physical but could
encompass the mental as well. For instance, a human soldier experiencing mental anguish
by coming under the threat of an enemy weapons system, can experience algedonic
feedback (e.g., existential anxiety, duress) until the threat is removed. Algedonic
feedback can therefore regulate a system by offering a reward or punishment to normalise
the system toward its goals. Developing and maintaining shared awareness between
human and IAS agents would require both entities share in an algedonic feedback loop.
However, IAS cannot possess a true algedonic feedback because it does not experience
an existential crisis or risk as a human would, which is an important part of shared
awareness.
Humans operating IAS comprise a sociotechnical system in which the components
might receive, process, and output information to achieve a common goal(s) of a team
task (Galbraith, 1974). Such a system would also contain a number of various feedback
mechanisms which help drive acceptable or beneficial behaviours by all members.
However, it is well-documented that autonomous systems without a sense of “skin-in-the-
game” could result in a number of unanticipated system behaviours that unfavourably
affect only human agents in the system (Larsson et al., 2020; Solovyeva and Hynek,
2018). For example, IAS such as autonomous weapons systems (AWS) could remove
humans from physical harm, but could also result in a minimisation of human life or turn
combat into an “unempathetic automated industrial process” (Solovyeva and Hynek,
2018, p.177). Systems such as IAS that have elements of AI are unlikely to cope with the
complex, ever evolving environment while simultaneously trying to support human
intentions (Payne, 2018). Lyons et al. (2021) underline that most pilots would not
countenance embarking on a dangerous mission with an autonomous wingman because
of the fears and uncertainty it raises. Additionally, IAS would not come under
punishment for choices it was programmed to make or direct (Larsson et al., 2020). In all
cases, the human shares in a feedback loop that IAS cannot be programmed to
accommodate, and therefore, cannot truly share awareness.
6 Discussion and future research
The development of shared awareness between a human and IAS agents present many
challenges. Much of this stems from the fact that autonomous platforms inherently
produce de novo outcomes (Kallinikos et al., 2013). Novel effects make it difficult toExpansion of situations theory for exploring shared awareness 19
develop true shared awareness due to both the limited rationality, unexplainability, and
unpredictability of current machine autonomy, including the concomitant uncertainty the
developed within the human agent. Without established shared awareness between an
opaque system and a human agent, planning, monitoring, and common goal achievement
could suffer or come to cross purposes at points in time if not engineered
comprehensively. Consequently, such considerations will likely drive changes in other
parts of the system and to higher system levels.
The development of shared awareness includes changes within higher SoS levels.
Many current theories of technology and work are too course-grained to capture the
nuances of technical change (Orlikowski and Scott, 2008). For instance, next-generation
systems engineering must consider how augmentation influences shared awareness
among heterogeneous members of a hybrid team (Canan, 2017) and digitally model
human-machine interactions (Huang et al., 2020). The development of sociotechnical
systems within modern organisations will bring many challenges that are not neutral.
Anytime a new technology is introduced in a social context, there are always
consequences that require further engineering and analysis (Selbst et al., 2019). We,
therefore, must avoid sociotechnical blindness caused by ignoring the human component
in attempts at engineering a completely self-sufficient AI; rather, human agents must be
at the centre of the design efforts (Johnson and Verdicchio, 2017).
Systems engineering must take a holistic approach that emphasises designing a
seamlessly integrated human and technological system (Behymer and Flach, 2016).
Automation should not be looked at as comparable or as a replacement for human
capabilities, but should be seen as harmonising the capabilities of both (Hoffman et al.,
2018; Zahedi and Kambhampati, 2021). Future systems engineering research could
develop a mission engineering approach that appropriately orders the myriad perspectives
into a more cohesive whole (Sousa-Poza, 2015). As a result, IAS could become a
complementary system component rather than a replacement for human agents for
developing awareness at the SoS level.
7 Summary
This paper takes and discusses the position of developing an expansion to situations
theory in the context of human-IAS teams. The further development of a domain of
awareness as a generative process provides additional insights and considerations for how
shared awareness is formed and sustained over time to achieve complementary goals
between IAS and humans for improved performance at the SoS level. Understanding the
role of IAS within human-IAS teams still requires a great deal of more research from the
perspective of the shared awareness concept (Yan et al., 2021). Such complex
assemblages of humans and IAS might suffer from what Cummings calls (2014) “the
curse of dimensionality” and not have any “closed-form solutions and will be intractable
from a mathematical perspective” (p.68). As argued by Keating and Katina (2012), the
field of SoS engineering must garner a greater appreciation for how information
technologies are rapidly introducing new complexities. Consequently, the study of IAS is
likely to involve a strong systems engineering approach with an emphasis on
interdisciplinary perspectives to address these challenges. Systems engineering must
develop new insights to further progress in the development of new approaches for
engineering a SoS composed of human-machine teams.20 S.A. Humr et al.
The introduction and use of IAS portend manifold engineering, organisational, and
cognitive challenges for achieving shared awareness. With the US military as a primary
supporter and promoter of human-IAS teams, much of the research will likely center
around dynamic C2 task environments (Lyons et al., 2021). In fact, C2 within a military
context demonstrates the importance of shared awareness at a team level with IAS. Yet,
such dynamic environments may present some of the most critical complex situations
with novel challenges for human comprehension of IAS behaviours and subsequent
decision-making that may equally apply to the private sector.
As stated in this paper, shared awareness is not a synonym for SSA or information
sharing. Rather, shared awareness, while encompassing situation awareness and
information sharing, also includes sharing a domain of awareness generated through
participating and interacting in a situation with equal comprehension, risks, and goals.
Yet, IAS without a requisite variety, the ability to reason counterfactually, or an
algedonic feedback loop, shared awareness between humans will likely be an
unbridgeable chasm for the near future. Nevertheless, concepts such as digital
engineering and mission engineering can potentially mitigate these shortcomings by
providing the bridging solutions necessary for organisations to remain competitive while
operating safely and effectively.
References
Alberts, D.S., Garstka, J.J., Hayes, R.E. and Signori, D.A. (2001) Understanding Information Age
Warfare: Defense Technical Information Center, Fort Belvoir, VA, https://doi.org/10.21236/
ADA386374
Albus, J.S. (1991) ‘Outline for a theory of intelligence’, IEEE Trans. Syst. Man Cybern., Vol. 21,
pp.473–509, https://doi.org/10.1109/21.97471
Al-Diban, S. (2012) ‘Mental models’, in Seel, N.M. (Ed.): Encyclopedia of the Sciences of
Learning. Springer, USA, Boston, MA, pp.2200–2204, https://doi.org/10.1007/978-1-4419-
1428-6_586
Anderson, P. and Meyer, A.D. (2016) ‘Complexity theory and process organization studies’, The
SAGE Handbook of Process Organization Studies, SAGE Publications Ltd., 55 City Road,
pp.127–143, https://doi.org/10.4135/9781473957954
Annett, J. and Stanton, N.A. (2000) ‘Team work--a. problem for ergonomics?’, Ergonomics,
Vol. 43, pp.1045–1051, https://doi.org/10.1080/00140130050084860
Ashby, W.R. (1991) Requisite Variety and Its Implications for the Control of Complex Systems,
Facets of Systems Science, Springer, USA, Boston, MA, pp.405–417, https://doi.org/10.1007/
978-1-4899-0718-9_28
Atkinson, D.J., Clancey, W.J. and Clark, M.H. (2014) ‘Shared awareness, autonomy and trust in
human-robot teamwork’, 2014 AAAI Fall Symposium Series. Presented at the 2014 AAAI Fall
Symposium Series, Arlington, VA, pp.36–38, https://cdn.aaai.org/ocs/9146/9146-40047-1-
PB.pdf
Baxter, G. and Sommerville, I. (2011) ‘Socio-technical systems: from design methods to systems
engineering’, Interact. Comput., Vol. 23, pp.4–17, https://doi.org/10.1016/j.intcom.2010.
07.003
Beer, S. (1972) ‘Cybernetics – A systems approach to management’, Pers. Rev., Vol. 1, pp.28–39,
https://doi.org/10.1108/eb055198
Beer, S. (1984) ‘The viable system model: its provenance, development, methodology and
pathology’, J. Oper. Res. Soc., Vol. 35, pp.7–25, https://doi.org/10.2307/2581927Expansion of situations theory for exploring shared awareness 21
Behymer, K.J. and Flach, J.M. (2016) ‘From autonomous systems to sociotechnical systems:
designing effective collaborations’, She Ji J. Des. Econ. Innov., Vol. 2, pp.105–114,
https://doi.org/10.1016/j.sheji.2016.09.001
Besnard, D., Greathead, D. and Baxter, G. (2004) ‘When mental models go wrong: co-occurrences
in dynamic, critical systems’, Int. J. Hum.-Comput. Stud., Vol. 60, pp.117–128,
https://doi.org/10.1016/j.ijhcs.2003.09.001
Bindewald, J.M., Rusnock, C.F. and Miller, M.E. (2018) ‘Measuring human trust behavior in
human-machine teams’, in Cassenti, D.N. (Ed.): Advances in Human Factors in Simulation
and Modeling, Advances in Intelligent Systems and Computing. Springer International
Publishing, Cham, pp.47–58, https://doi.org/10.1007/978-3-319-60591-3_5
Bryant, D., Lichacz, F., Hollands, J. and Baranski, J. (2004) Modeling Situation Awareness in an
Organizational Context: Military Command and Control, pp.104–116.
Bryant, D.J. (2006) ‘Rethinking OODA: toward a modern cognitive framework of command
decision making’, Mil. Psychol., Vol. 18, pp.183–206, https://doi.org/10.1207/s15327876
mp.1803_1
Canan, M. (2017) A Hilbert Space Geometric Representation of Shared Awareness and Joint
Decision Making (Dissertation), Old Dominion University, Virginia.
Canan, M., Demir, M. and Kovacic, S. (2022) ‘A probabilistic perspective of human-machine
interaction’, Hawaii International Conference on System Sciences, Maui, HI, pp.7607–7616,
http://hdl.handle.net/10125/80256
Canan, M. and Sousa-Poza, A. (2016) ‘Complex adaptive behavior: pragmatic idealism’, Procedia
Comput. Sci., Complex Adaptive Systems, 2–4 November, Los Angeles, CA, Vol. 95,
pp.73–79, https://doi.org/10.1016/j.procs.2016.09.295
Canan, M. and Sousa-Poza, A. (2019) ‘Pragmatic idealism: towards a probabilistic framework of
shared awareness in complex situations’, 2019 IEEE Conference on Cognitive and
Computational Aspects of Situation Management (CogSIMA). Presented at the 2019 IEEE
Conference on Cognitive and Computational Aspects of Situation Management (CogSIMA),
Las Vegas, pp.114–121, https://doi.org/10.1109/COGSIMA.2019.8724208
Canan, M., Sousa-Poza, A. and Dean, A. (2017) ‘Complex adaptive behavior of hybrid teams’,
Procedia Comput. Sci., Complex Adaptive Systems Conference with Theme: Engineering
Cyber Physical Systems, CAS, 30 October–1 November, Chicago, Illinois, USA, Vol. 114,
pp.139–148, https://doi.org/10.1016/j.procs.2017.09.013.
Canan, M., Sousa-Poza, A. and Kovacic, S.F. (2015) ‘Semantic shift to pragmatic meaning in
shared decision making: situation theory perspective’, Int. J. Des. Nat. Ecodynamics, Vol. 10,
pp.267–278, https://doi.org/10.2495/DNE-V10-N3-267-278
Cannon-Bowers, J.A., Salas, E. and Converse, S.A. (1993) ‘Shared mental models in expert team
decision making’, Individual and Group Decision Making: Current Issues, pp.221–246.
Capra, F. (1996) The Web of Life: A New Scientific Understanding of Living Systems, Anchor
Books, New York, NY.
Carley, K. and Palmquist, M. (1992) ‘Extracting, representing, and analyzing mental models’,
Soc. Forces, Vol. 70, pp.601–636, https://doi.org/10.2307/2579746
Carley, K.M. (1997) ‘Extracting team mental models through textual analysis’, J. Organ. Behav.,
Vol. 18, pp.533–558, https://doi.org/10.1002/(SICI)1099-1379(199711)18:1+<533::AID-
JOB906>3.0.CO;2-3
Chiou, E.K. and Lee, J.D. (2016) ‘Cooperation in human-agent systems to support resilience: a
microworld experiment’, Hum. Factors, Vol. 58, pp.846–863, https://doi.org/10.1177/00187
20816649094
Choudhury, A., Asan, O. and Mansouri, M. (2019) ‘Role of artificial intelligence, clinicians &
policymakers in clinical decision making: a systems viewpoint’, 2019 International
Symposium on Systems Engineering (ISSE). Presented at the 2019 International Symposium
on Systems Engineering (ISSE), Edinburgh, Scotland, pp.1–8, https://doi.org/10.1109/
ISSE46696.2019.898457322 S.A. Humr et al.
Clark, A. (2001) Mindware: An Introduction to the Philosophy of Cognitive Science. Oxford
University Press, New York.
Clark, A. and Chalmers, D.J. (1998) The Extended Mind. Analysis 58, https://doi.org/10.1093/
analys/58.1.7
Converse, S.A., Cannon-Bowers, J.A. and Salas, E. (1991) ‘Team member shared mental models: a
theory and some methodological issues’, Proc. Hum. Factors Soc. Annu. Meet., Vol. 35,
pp.1417–1421, https://doi.org/10.1177/154193129103501917
Cooke, N., Demir, M. and Huang, L. (2020) ‘A framework for human-autonomy team research’,
in Harris, D. and Li, W-c. (Eds.): Engineering Psychology and Cognitive Ergonomics.
Cognition and Design, Lecture Notes in Computer Science. Springer International Publishing,
Cham, pp.134–146, https://doi.org/10.1007/978-3-030-49183-3_11
Cooke, N.J. (2015) ‘Team cognition as interaction’, Curr. Dir. Psychol. Sci., Vol. 24, pp.415–419,
https://doi.org/10.1177/0963721415602474
Cooke, N.J., Gorman, J.C., Myers, C.W. and Duran, J.L. (2013) ‘Interactive team cognition’, Cogn.
Sci., Vol. 37, pp.255–285, https://doi.org/10.1111/cogs.12009
Cotoranu, A. and Chen, L-C. (2020) ‘Applying text analytics to examination of end users’ mental
models of cybersecurity’, Proceedings of the Americas Conference on Information Systems
(AMCIS) 2020, Salt Lake City, UT.
Cummings, M. (2014) ‘Man versus machine or man + machine?’, IEEE Intell. Syst., Vol. 29,
pp.62–69, https://doi.org/10.1109/MIS.2014.87
Damacharla, P., Javaid, A.Y., Gallimore, J.J. and Devabhaktuni, V.K. (2018) ‘Common metrics to
benchmark human-machine teams (HMT): A review’, IEEE Access, Vol. 6, pp.38637–38655,
https://doi.org/10.1109/ACCESS.2018.2853560
Das, A., Kol, P., Lundberg, C., Doelling, K., Sevil, H.E. and Lewis, F. (2018) ‘A rapid situational
awareness development framework for heterogeneous manned-unmanned teams’, NAECON
2018 – IEEE National Aerospace and Electronics Conference. Presented at the NAECON
2018 -IEEE National Aerospace and Electronics Conference, Dayton, OH, pp.417–424,
https://doi.org/10.1109/NAECON.2018.8556769
de Freitas, E. (2012) ‘The classroom as rhizome: new strategies for diagramming knotted
interactions’, Qual. Inq., Vol. 18, pp.557–570, https://doi.org/10.1177/1077800412450155
DeChurch, L.A. and Mesmer-Magnus, J.R. (2010) ‘Measuring shared team mental models: a meta-
analysis’, Group Dyn. Theory Res. Pract., Vol. 14, pp.1–14, https://doi.org/10.1037/a0017455
Dekker, S. and Lützhöft, M. (2004) Correspondence, Cognition and Sensemaking: A Radical
Empiricist Approach to Situation Awareness. Ashgate Publishing Co, London, pp.22–41.
Department of the Navy (2021) Department of the Navy Science and Technology Strategy for
Intelligent Autonomous Systems.
Department of the Navy Strategy for Intelligent Autonomous Systems (2021) USNI News, URL
https://news.usni.org/2021/07/29/department-of-the-navy-strategy-for-intelligent-autonomous-
systems (Accessed 24 September, 2022).
Dewey, J. (1933) How We Think. A Restatement of the Relation of Reflective Thinking to the
Educative Process, Boston etc. (D.C. Heath and Company).
Endsley, M.R. (1995) ‘Toward a theory of situation awareness in dynamic systems’, Hum. Factors,
Vol. 37, pp.32–64, https://doi.org/10.1518/001872095779049543
Espevik, R., Johnsen, B.H., Eid, J. and Thayer, J.F. (2006) ‘Shared mental models and operational
effectiveness: effects on performance and team processes in submarine attack teams’,
Mil. Psychol., Vol. 18, pp.S23–S36, https://doi.org/10.1207/s15327876mp.1803s_3
Ford, D.N. (2019) ‘A system dynamics glossary’, Syst. Dyn. Rev., Vol. 35, pp.369–379,
https://doi.org/10.1002/sdr.1641
Galbraith, J.R. (1974) ‘Organization design: an information processing view’, Interfaces, Vol. 4,
pp.28–36.
Gere, C. (2009) Digital Culture, Reaktion Books, Chicago, IL.Expansion of situations theory for exploring shared awareness 23
Gorman, J.C., Cooke, N.J., Amazeen, P.G. and Fouse, S. (2012) ‘Measuring patterns in team
interaction sequences using a discrete recurrence approach’, Hum. Factors, Vol. 54,
pp.503–517, https://doi.org/10.1177/0018720811426140
Gorman, J.C., Grimm, D.A., Stevens, R.H., Galloway, T., Willemsen-Dunlap, A.M. and
Halpin, D.J. (2020) ‘Measuring real-time team cognition during team training’, Hum. Factors,
Vol. 62, pp.825–860, https://doi.org/10.1177/0018720819852791
Hoffman, R.R., Sarter, N., Johnson, M. and Hawley, J.K. (2018) ‘Myths of automation and their
implications for military procurement’, Bull. At. Sci., Vol. 74, pp.255–261, https://doi.org/10.
1080/00963402.2018.1486615
Huang, J., Gheorghe, A., Handley, H., Pazos, P., Pinto, A., Kovacic, S., Collins, A., Keating, C.
and Sousa-Poza, A., Rabadi, G., Unal, R., Cotter, T., Landaeta, R., Daniels, C. (2020)
‘Towards digital engineering: the advent of digital systems engineering’, Int. J. Syst. Syst.
Eng., Vol. 10, pp.234–261, https://doi.org/10.1504/IJSSE.2020.109737
Hutchins, S.G., Zhao, Y. and Kendall, T. (2011) Investigating Inter-Organizational Collaboration
during the Haiti Relief Effort From a Macrocognition Perspective, Naval Postgraduate School
Monterey CA, Graduate School of Operational and Information Sciences.
Ireland V. (2016) ‘Governance of collaborative system of systems’, Int. J. Syst. Syst. Eng., Vol. 7,
pp.159–188, https://doi.org/10.1504/IJSSE.2016.076129
Jacovi, A., Marasović, A., Miller, T. and Goldberg, Y. (2021) ‘Formalizing trust in artificial
intelligence: prerequisites, causes and goals of human trust in AI’, Proceedings of the 2021
ACM Conference on Fairness, Accountability, and Transparency, FAccT ‘21.Association for
Computing Machinery, New York, NY, USA, pp.624–635, https://doi.org/10.1145/34421
88.3445923
Johnson, D.G. and Verdicchio, M. (2017) ‘Reframing AI discourse’, Minds Mach., Vol. 27,
pp.575–590, https://doi.org/10.1007/s11023-017-9417-6
Johnson-Laird, P.N. (1983) Mental Model’s: Towards a Cognitive Science of Language, Inference,
and Consciousness, Harvard University Press, Cambridge, MA.
Kahneman, D. (2013) Thinking, Fast and Slow, 1st ed., Farrar, Straus and Giroux, New York.
Kallinikos, J., Aaltonen, A. and Marton, A. (2013) ‘The ambivalent ontology of digital artifacts’,
MIS Q, Vol. 37, pp.357–370.
Kase, S.E., Hung, C.P., Krayzman, T., Hare, J.Z., Rinderspacher, B.C. and Su, S.M. (2022) ‘The
future of collaborative human-artificial intelligence decision-making for mission planning’,
Front. Psychol., p.13.
Keating, C.B. and Katina, P.F. (2012) ‘Prevalence of pathologies in systems of systems’, Int. J.
Syst. Syst. Eng., Vol. 3, pp.243–267, https://doi.org/10.1504/IJSSE.2012.052688
Kim, M.K., Link to external site, this link will open in a new window, Gaul, C.J., Kim, S.M. and
Madathany, R.J. (2019) ‘Advance in detecting key concepts as an expert model: using student
mental model analyzer for research and teaching (SMART)’, Technol. Knowl. Learn.,
pp.1–24 http://dx.doi.org/10.1007/s10758-019-09418-5
Klien, G., Woods, D.D., Bradshaw, J.M., Hoffman, R.R., Feltovich, P.J. (2004) ‘Ten challenges for
making automation a ‘team player’, in Joint Human-Agent Activity (Ed.): IEEE Intell. Syst.,
Vol. 19, pp.91–95, https://doi.org/10.1109/MIS.2004.74
Klimoski, R., Mohammed, S. (1994) ‘Team mental model: construct or metaphor?’, J. Manag.,
Vol. 20, pp.403–437, https://doi.org/10.1177/014920639402000206
Larsson, S., Bogusz, C.I., Schwarz, J.A., Heintz, F. and European Liberal Forum (2020) Human-
Centred AI in the EU Trustworthiness as a Strategic Priority in the European Member States,
Publisher European Liberal Forum asbl., Stockholm, https://lucris.lub.lu.se/ws/portalfiles/
portal/87349373/Larsson_et_al_2020_AI_in_the_EU_final.pdf
Lee, J. and Moray, N. (1989) ‘Experiments on a lattice theory of operator mental models’,
Conference Proceedings, IEEE International Conference on Systems, Man and Cybernetics.
Presented at the Conference Proceedings., IEEE International Conference on Systems, Man
and Cybernetics, Vol. 3, p.1157, https://doi.org/10.1109/ICSMC.1989.7148224 S.A. Humr et al.
Lindebaum, D., Vesa, M. and den Hond, F. (2020) ‘Insights from ‘The machine stops’, to better
understand rational assumptions in algorithmic decision making and its implications for
organizations’, Acad. Manage. Rev., Vol. 45, pp.247–263, https://doi.org/10.5465/amr.
2018.0181
Lyons, J.B. and Havig, P.R. (2014) ‘Transparency in a human-machine context: approaches for
fostering shared awareness/intent’, in Shumaker, R. and Lackey, S. (Eds.): Virtual, Augmented
and Mixed Reality. Designing and Developing Virtual and Augmented Environments, Lecture
Notes in Computer Science, Springer International Publishing, Cham, pp.181–190, https://doi.
org/10.1007/978-3-319-07458-0_18
Lyons, J.B., Sycara, K., Lewis, M. and Capiola, A. (2021) ‘Human-autonomy teaming: definitions,
debates, and directions’, Front. Psychol., Vol. 12, p.589585, https://doi.org/10.3389/fpsyg.
2021.589585
Maier, M.W. (1998) ‘Architecting principles for systems-of-systems’, Syst. Eng., Vol. 1,
pp.267–284, https://doi.org/10.1002/(SICI)1520-6858(1998)1:4<267::AID-SYS3>3.0.CO;2-D
Mashkoor, A., Arcaini, P. and Gargantini, A. (2020) ‘Intelligent autonomous systems’, Computer,
Vol. 53, pp.20–23, https://doi.org/10.1109/MC.2020.3023158
Mathieu, J.E., Heffner, T.S. and Goodwin, G.F. (2000) The Influence of Shared Mental Models on
Team Process and Performance, p.11.
Matthews, G., Panganiban, A.R., Lin, J., Long, M. and Schwing, M. (2021) ‘Chapter 3 -super-
machines or sub-humans: mental models and trust in intelligent autonomous systems’,
in Nam, C.S. and Lyons, J.B. (Eds.): Trust in Human-Robot Interaction, Academic Press,
pp.59–82, https://doi.org/10.1016/B978-0-12-819472-0.00003-4
McLean, J. (2022) ‘Every warfighter must understand autonomy [WWW document]’, US Nav. Inst.
Proc., https://www.usni.org/magazines/proceedings/2022/january/every-warfighter-must-
understand-autonomy (Accessed 1 July, 2022).
McNeese, N.J., Schelble, B.G., Canonico, L.B. and Demir, M. (2021) ‘Who/what is my teammate?
Team composition considerations in human–ai teaming’, IEEE Trans. Hum.-Mach. Syst.,
Vol. 51, pp.288–299, https://doi.org/10.1109/THMS.2021.3086018
Merali, Y. (2006) ‘Complexity and information systems: the emergent domain’, J. Inf. Technol.,
Vol. 21, pp.216–228, https://doi.org/10.1057/palgrave.jit.2000081
Meystel, A. and Messina, E. (2000) ‘The challenge of intelligent systems’, Proceedings of the 2000
IEEE International Symposium on Intelligent Control. Held Jointly with the 8th IEEE
Mediterranean Conference on Control and Automation (Cat. No. 00CH37147) Presented at
the Proceedings of the 2000 IEEE International Symposium on Intelligent Control. Held
Jointly with the 8th IEEE Mediterranean Conference on Control and Automation (Cat. No.
00CH37147), Rio, Greece, pp.211–216, https://doi.org/10.1109/ISIC.2000.882925
Moray, N. (1996) ‘A taxonomy and theory of mental models’, Proc. Hum. Factors Ergon. Soc.
Annu. Meet., Vol. 40, pp.164–168, https://doi.org/10.1177/154193129604000404
Narayanan, S. (2010) ‘Mind changes: a simulation semantics account of counterfactuals’,
Cogn. Sci., p.47.
Nofi, A. (2000) Defining and Measuring Shared Situational Awareness, Center for Naval Analysis,
Alexandria, VA, https://apps.dtic.mil/sti/pdfs/ADA390136.pdf
Orlikowski, W.J. and Scott, S.V. (2008) ‘10 sociomateriality: challenging the separation of
technology, work and organization’, Acad. Manag. Ann., Vol. 2, pp.433–474, https://doi.org/
10.5465/19416520802211644
Ososky, S., Schuster, D., Jentsch, F., Fiore, S., Shumaker, R., Lebiere, C., Kurup, U., Oh, J. and
Stentz, A. (2012) ‘The importance of shared mental models and shared situation awareness for
transforming robots from tools to teammates’, Unmanned Systems Technology, X.I.V.
Presented at the Unmanned Systems Technology, XIV, SPIE, pp.397–408, https://doi.org/10.
1117/12.923283Expansion of situations theory for exploring shared awareness 25
Oury, J.D. and Ritter, F.E. (2021) ‘How user-centered design supports situation awareness for
complex interfaces’, in Oury, J.D. and Ritter, F.E. (Eds.): Building Better Interfaces for
Remote Autonomous Systems : An Introduction for Systems Engineers, Human–Computer
Interaction Series. Springer International Publishing, Cham, pp.21–35, https://doi.org/10.
1007/978-3-030-47775-2_2
Payne, K. (2018) ‘Artificial intelligence: a revolution in strategic affairs?’, Survival, Vol. 60,
pp.7–32, https://doi.org/10.1080/00396338.2018.1518374
Pearl, J. and Mackenzie, D. (2018) The Book of Why: The New Science of Cause and Effect,
Basic Books, New York, NY.
Phillips, E., Ososky, S., Grove, J. and Jentsch, F. (2011) ‘From tools to teammates: toward the
development of appropriate mental models for intelligent robots’, Proc. Hum. Factors Ergon.
Soc. Annu. Meet., Vol. 55, pp.1491–1495, https://doi.org/10.1177/1071181311551310
Pothos, E.M. and Pleskac, T.J. (2022) ‘Rethinking rationality’, Topics in Cognitive Science,
Vol. 14, No. 3, pp.451–466, https://doi.org/10.1111/tops.12585.
Rouse, W.B., Cannon-Bowers, J.A. and Salas, E. (1992) ‘The role of mental models in team
performance in complex systems’, IEEE Trans. Syst. Man Cybern., Vol. 22, pp.1296–1308,
https://doi.org/10.1109/21.199457
Rudas, I.J. and Fodor, J. (2008) ‘Intelligent systems’, Presented at the International Journal of
Computers, Communications and Control, Agora University Ed. House, pp.132–138.
Schaefer, K.E., Perelman, B., Rexwinkle, J., Canady, J., Neubauer, C., Waytowich, N., Larkin, G.
and Cox, K., Geuss, M., Gremillion, G., Metcalfe, J.S., DeCostanza, A. and Marathe, A.
(2021) ‘Human-autonomy teaming for the tactical edge: the importance of humans in artificial
intelligence research and development’, in Lawless, W.F., Mittu R, Sofge, D.A., Shortell, T.
and McDermott, T.A. (Eds.): Systems Engineering and Artificial intelligence, Springer
International Publishing, Cham, pp.115–148, https://doi.org/10.1007/978-3-030-77283-3_7
Schaefer, K.E., Straub, E.R., Chen, J.Y.C., Putney, J. and Evans, A.W. (2017) ‘Communicating
intent to develop shared situation awareness and engender trust in human-agent teams’, Cogn.
Syst. Res., Situation Awareness in Human-Machine Interactive Systems, Vol. 46, pp.26–39,
https://doi.org/10.1016/j.cogsys.2017.02.002
Scheutz, M. (2013) ‘Computational mechanisms for mental models in human-robot interaction’, in
Shumaker, R. (Ed.): Virtual Augmented and Mixed Reality. Designing and Developing
Augmented and Virtual Environments, Lecture Notes in Computer Science, Springer Berlin
Heidelberg, Berlin, Heidelberg, pp.304–312, https://doi.org/10.1007/978-3-642-39405-8_34
Scheutz, M., DeLoach, S.A. and Adams, J.A. (2017) ‘A framework for developing and using
shared mental models in human-agent teams’, J. Cogn. Eng. Decis. Mak., Vol. 11,
pp.203–224, https://doi.org/10.1177/1555343416682891
Schlinger, H.D. (2003) ‘The myth of intelligence’, Psychol. Rec., Vol. 53, pp.15–32.
Selbst, A.D., Boyd, D., Friedler, S.A., Venkatasubramanian, S. and Vertesi, J. (2019) ‘Fairness and
abstraction in sociotechnical systems’, Proceedings of the Conference on Fairness,
Accountability, and Transparency, FAT*’19, Association for Computing Machinery, New
York, NY, USA, pp.59–68, https://doi.org/10.1145/3287560.3287598
Sharkey, N. (2008) ‘The ethical frontiers of robotics’, Science, Vol. 322, pp.1800–1801,
https://doi.org/10.1126/science.1164582
Sillitto, H., Martin, J., McKinney, D., Griego, R., Dori, D., Krob, D., Godfrey, P., Arnold, E. and
Jackson, S. (2019) Systems Engineering and Systems Definitions (INCOSE-TP-2020-002-06),
International Council on Systems Engineering (INCOSE), https://www.incose.org/docs/
default-source/default-documentlibrary/ incose-se-definitions-tp-2020-002-06.pdf
Solovyeva, A. and Hynek, N. (2018) ‘Going beyond the ‘Killer robots’, debate: six dilemmas
autonomous weapon systems raise’, Cent. Eur. J. Int. Secur. Stud., Vol. 1, No. 3, pp.166–208.26 S.A. Humr et al.
Sousa-Poza, A. (2013) ‘A narrative of [Complex] situations and situations theory’, in Kovacic, S.F.
and Sousa-Poza, A. (Eds.): Managing and Engineering in Complex Situations, Topics in
Safety, Risk, Reliability and Quality. Springer Netherlands, Dordrecht, pp.13–44,
https://doi.org/10.1007/978-94-007-5515-4_2
Sousa-Poza, A. (2015) ‘Mission engineering’, Int. J. Syst. Syst. Eng., Vol. 6, p.161, https://doi.org/
10.1504/Ijsse.2015.071453
Starr, W. (2021) ‘Counterfactuals’, in Zalta, E.N. (Ed.): The Stanford Encyclopedia of Philosophy.
Metaphysics Research Lab, Stanford University.
ten Berge, T. and van Hezewijk, R. (1999) ‘Procedural and declarative knowledge: an evolutionary
perspective’, Theory Psychol., Vol. 9, pp.605–624, https://doi.org/10.1177/095935439909
5002
Tremblay, S. and Banbury, S. (2004) ‘Modeling situation awareness in an organizational context:
military command and control’, A Cognitive Approach to Situation Awareness: Theory and
Application, Routledge, London.
van de Poel, I. (2020) ‘Embedding values in artificial intelligence (AI) systems’, Minds Mach.,
Vol. 30, pp.385–409, https://doi.org/10.1007/s11023-020-09537-4
Van den Bossche, P., Gijselaers, W., Segers, M., Woltjer, G. and Kirschner, P. (2011) ‘Team
learning: building shared mental models’, Instr. Sci., Vol. 39, pp.283–301, https://doi.org/
10.1007/s11251-010-9128-3
Wegner, D.M., Giuliano, T. and Hertel, P.T. (1985) ‘Cognitive interdependence in close
relationships’, in Ickes, W.J. (Ed.): Compatible and Incompatible Relationships, Springer-
Verlag, New York, NY, pp.253–276, https://doi.org/10.1007/978-1-4612-5044-9_12
Weick, K.E. (1985) ‘Cosmos vs. chaos: sense and nonsense in electronic contexts’, Organ. Dyn.,
Vol. 14, pp.51–64, https://doi.org/10.1016/0090-2616(85)90036-1
Wildman, J.L., Salas, E. and Scott, C.P.R. (2014) ‘Measuring cognition in teams: a cross-domain
review’, Hum. Factors, Vol. 56, pp.911–941, https://doi.org/10.1177/0018720813515907
Yan, B., Hollingshead, A.B., Alexander, K.S., Cruz, I. and Shaikh, S.J. (2021) ‘Communication in
transactive memory systems: a review and multidimensional network perspective’, Small
Group Res., Vol. 52, pp.3–32, https://doi.org/10.1177/1046496420967764
Zahedi, Z. and Kambhampati, S. (2021) Human-AI Symbiosis: A Survey of Current Approaches.
ArXiv.