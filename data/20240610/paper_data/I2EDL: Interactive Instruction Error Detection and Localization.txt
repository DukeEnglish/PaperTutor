I2EDL: Interactive Instruction Error Detection and Localization
Francesco Taioli1,4, Stefano Rosa2, Alberto Castellini1, Lorenzo Natale2,
Alessio Del Bue2, Alessandro Farinelli1, Marco Cristani1, Yiming Wang3
Fig. 1: A human user guides an agent (bottom right) to reach a target goal (green path) with instructions expressed in natural
language (top) and which may contain errors (red words). During the navigation, the agent is able to detect and localize
instruction errors. Upon detection, the agent asks the user if a particular word in the instruction is correct or not. In case of
an incorrect word, the user can reply with the correct one, allowing the agent to resume the navigation. Such human-agent
interaction can occur multiple times depending on the error detection algorithm.
Abstract—In the Vision-and-Language Navigation in Contin- InstructionErrorDetectorandLocalizer(I2EDL)thattriggersthe
uous Environments (VLN-CE) task, the human user guides an user-agent interaction upon the detection of instruction errors
autonomous agent to reach a target goal via a series of low- during the navigation. We leverage a pre-trained module to
level actions following a textual instruction in natural language. detectinstructionerrorsandpinpointthemintheinstructionby
However, most existing methods do not address the likely cross-referencingthetextualinputandpastobservations.Insuch
case where users may make mistakes when providing such way, the agent is able to query the user for a timely correction,
instruction (e.g., “turn left” instead of “turn right”). In this without demanding the user’s cognitive load, as we locate the
work, we address a novel task of Interactive VLN in Continuous probableerrorstoaprecisepartoftheinstruction.Weevaluate
Environments(IVLN-CE),whichallowstheagenttointeractwith the proposed I2EDL on a dataset of instructions containing
the user during the VLN-CE navigation to verify any doubts errors, and further devise a novel metric, the Success weighted
regarding the instruction errors. We propose an Interactive by Interaction Number (SIN), to reflect both the navigation
performanceandtheinteractioneffectiveness.Weshowhowthe
proposed method can ask focused requests for corrections to
1 UniversityofVerona,Verona,Italy.
the user, which in turn increases the navigation success, while
2 IstitutoItalianodiTecnologia(IIT),Genova,Italy.
minimizing the interactions.
3 FondazioneBrunoKessler,Trento,Italy.
4 PolytechnicofTurin,Turin,Italy.
francesco.taioli@polito.it I. INTRODUCTION
We acknowledge the CINECA award under the ISCRA initiative, for
The research of Vision-and-Language Navigation (VLN)
theavailabilityofhigh-performancecomputingresourcesandsupport.This
work was partially sponsored by the PNRR project FAIR - Future AI aimstodevelopagentsthatcannavigatetoaspecifiedlocation
Research (PE00000013), under the NRRP MUR program funded by the within a 3D space by following instructions expressed in
NextGenerationEU.ThisprojecthasreceivedfundingfromtheEuropean
natural language. This research task aligns with the broader
Union’sHorizonresearchandinnovationprogrammeG.A.n.101070227
(CONVINCE). ambition of embodied AI, which allows automated agents to
©2024IEEE.Personaluseofthismaterialispermitted.PermissionfromIEEEmustbeobtainedforallotheruses,
includingreprinting/republishingthismaterialforadvertisingorpromotionalpurposes,collectingnewcollected
worksforresaleorredistributiontoserversorlists,orreuseofanycopyrightedcomponentofthisworkinother
works.
4202
nuJ
7
]OR.sc[
1v08050.6042:viXraengage with human users via natural language conversations, onlypartialobservations.InspiredbytheofflinemethodIEDL
understanding their surroundings [1], and executing tasks proposed in [7], we first collect a set of visual observations
in the real world. There are many benchmark datasets in from the agent. By leveraging the pre-trained models in
the literature of VLN, including the seminal dataset Room- IEDL, we can identify errors within the instruction and
to-Room (R2R) for discrete environments operating on precisely locate them. Then, upon positive detection, the
discrete navigation graphs [2], and the more realistic R2R for agent asks the user if a specific word is wrong and, if this
continuousenvironments(R2R-CE)operatingviaasetoflow- is the case, it obtains an accurate replacement. In addition
level actions to any unobstructed point in a scene [3]. Most to common VLN performance metrics, i.e., Success Rate
of the benchmark datasets consider only correct language (SR) and Success weighted by Path Length (SPL), we also
instructions, implying that the users never make mistakes. propose a novel metric that is specific for the IVLN-CE
However,thisisnotalwaysthecase,sinceinstructionscanbe task, the Success weighted by Interaction Number (SIN),
very complex and the ability of giving right directions vary whichreflectsboththenavigationperformanceandinteraction
greatly among people [4]. In fact, it is a matter of spatial effectiveness, by encouraging a higher success rate while
cognition skills, i.e., how humans mentally represent and limiting the interaction numbers. We evaluate our method on
process spatial information [5], and the ability of creating R2RIE-CE under different instruction errors and prove that
cognitivemaps,whicharementalrepresentationofthelayout our baseline is more effective than an agent that randomly
and contents of an environment [6]. interacts with the user. In summary, our contributions are
Recently, Taioli et al. propose the R2RIE-CE benchmark listed below:
dataset [7], which introduces wrong instructions in the form
• We establish the IVLN-CE task, i.e., Interactive VLN
of incorrect directions (left, right, etc.) and misplaced rooms
in Continuous Environments, simulating the real world
or objects, to model the effect of inaccurate cognitive maps.
caseswherehumansareallowedtomakemistakeswhen
This is helpful in benchmarking the robustness of state-of-
providing instruction, and agents are allowed to interact
the-art VLN policies, that have been designed to cope with
with humans to correct them.
ideal interactions only. The R2RIE-CE dataset can also be
• Weproposeaneffectivebaseline,I2EDL,whichinteracts
usedtoevaluatealgorithmsfordetectingandlocalizingerrors
with the user in an online manner upon detecting
in a given instruction. Yet, their detection and localization
instruction errors and prompting the focused question
baselines operate in an offline mode, i.e., the errors in the
with localized errors.
instructions are individuated only after an agent has finished
• We propose a novel metric that measure the interaction
itssearch,thusleavingnochancefortheagentandtheuserto
effectiveness in terms of navigation performance by
interactandrecovertheerrorswhileexploringtheenvironment.
combining the Success Rate and interaction numbers
On the contrary, enabling human-agent interaction during
with the user. Our metric serves as a primary quantifier
navigation could be effective. As shown in Fig. 1, a human
for comparing the performance of different agents.
user may initially give an erroneous instruction, with a
wrong direction word “left” to “right” and a wrong object
II. RELATEDWORKS
landmark “lamp” to “fireplace”, which can cause the agent
to deviate from its target position, failing the navigation task. Vision and Language Navigation.ThetaskofVision-and-
However, if the agent can detect and locate potential errors Language Navigation (VLN) was initially introduced in [2].
while navigating and observing the scene, it can prompt the Early iterations were based on the Matterport3D Sim [8] and
user for instruction corrections, thus improving the success on the Room-to-Room (R2R) dataset [2], which represents
rate of the task. Such interactive VLN with error awareness the environment as a sparse undirected graph of poses with
introduces additional challenges on top of the existing VLN associated panoramic views. In this discrete environment,
task (e.g., visual perception, spatial reasoning and vision- agents can only move between pre-existing nodes of the
language alignment). In particular, the agent should identify graph. A continuous environment variant of the task (VLN-
potential errors promptly at an early stage, with only partial CE), together with a new dataset (R2R-CE) was introduced
observations of the scene. Moreover, since it is not ideal to in [3], using the Habitat simulator [9]. In VLN-CE, agents
have an agent that constantly interacts with a human user are required to follow the instructions while navigating freely
asking for potential errors (both for human disturbance and in the environment rather than teleporting between nodes.
cognitive load), it is essential to have an accurate online While initial approaches used recurrent representations to
instruction error detector and localizer, thus “asking the right encode the agent’s history and predict the next action [10],
question at the right time.” morerecentmethodsproposedtopredictcandidatewaypoints
In this work, we address Interactive VLN in Continuous and select the next best goal [11], the use of topological
Environments (IVLN-CE). Human users are allowed to make memory [12] to better encode the history of observations,
errors in their initial instructions and subsequently correct andmetricmappre-training[13]toincreasespatialawareness.
them if the agent accurately detects and locates the errors Recent works have studied how VLN agents use directions
through human-agent interactions. We propose an effective andobjectstonavigatebymaskingwords[14],[15]aswellas
baseline, named Interactive Instruction Error Detector and the performance drop in unseen environments [16]. However,
Localizer (I2EDL), that operates in an online mode given current methods and datasets do not consider the case ofpotentially wrong instructions. We propose to specifically I can have up to E words that are incorrect. We then define
i
study the case of instructions containing errors. theinstructionembeddingasΥ ∈RW×D,i.e.,theinstruction
i
Cooperative Vision-dialog Navigation. The task of Navi- I is tokenized and padded up to W =80 tokens, while D
i
gation from Dialogue History (NDH) was introduced in [17]. is the dimension of the latent space in which each token is
In NDH, an agent is given a target object and a dialogue projected, following [12], [13]. Without loss of generality,
history between two humans cooperating to find it. These we assume that each word is tokenized into one token.
approaches are usually evaluated only in terms of progress At each time step t, the agent receives a visual observation
towards the goal. [18] propose turn-based dialogue between O , namely an RGB-D image. Let T be the total number
t
two agents: the navigating agent and the guiding agent. Both of steps executed by policy π, we define the set of visual
agents learn to simulate questions or answers by the other. observation O = {O ,...,O }. Policy π, for every step t,
1 T
Vision-dialog navigation has been extended to the real world. predicts an action a in the set {Forward 0.25m, Turn
t
[19] proposed RobotSlang, a dataset of natural language Left 15°, Turn Right 15°, Stop}.
dialogues between a human operator teleoperating a robot For every episode i, and at every step t, the agent has the
and a human commander that provides guidance towards a possibility to query the human, checking if a particular token
goal.In[20]theagentistrainedtoidentifywhentoengagein ℓj is correct or not, where j ∈ [0,len(Υ )−1] and len(·)
i i
dialogue with a navigator agent via masking and directional returns the total number of tokens for instruction Υ .
i
grounding. [21] proposes to factor out the action of querying Asking just a token to the user would be ineffective, since
into two different whether-to-ask and a what-to-ask policies. the user would hardly understand the sense of a single word
Interactive VLN. More closely related to our proposal (token), and, in the case of multiple instances of the same
are approaches where the agent interacts with an oracle by word, misunderstandings could easily arise. Therefore, the
asking for help. In [22] the agent uses a dedicated policy agentpassestotheusersaportion(context)oftheinstruction,
action for asking for help. The action of querying the oracle madebymultipletokensL =[ℓj−ςl,ℓj+ςl],whereς isthe
j,ςl i i l
is based on model confusion (i.e., the agent is unsure about contextualizationlength.Thehuman,uponreceivingarequest
which action to take next) and is penalized via a negative fromtheagent,returnstherealtokenifawrongtokenisfound
reward.Whenqueried,theoraclereturnsthenextshortestpath within[ℓj−τl,ℓj+τl],whereτ isalocalizationthreshold.This
i i l
action to the goal. A metric is also introduced to evaluate the correction mechanism ensures that the human can provide
effectivenessofhuman-agentinteraction,asthepercentageof the correct token even if there is a slight discrepancy of τ
l
total ask actions per episode. [22] injects a probability of the tokens in the location pointed out by the agent.
oraclemakingamistake,tosimulateamorerealisticuser.We Policy. The agent’s policy π is implemented in this paper
directly start with a dataset containing mistakes, to simulate by the current state-of-the-art method for VLN-CE1, i.e.,
wronginstructionsgiventotheagent.Adifferentwaytolimit BEVBert [13]. However, our approach is model-agnostic.
interactions with the oracle is fixing a query budget. In [23] BEVBert is composed of three essential modules that allow
the agent asks for help when unsure about the next action or the agent to balance the demand for short-term reasoning and
lost. Upon being called, the oracle provides a short-term goal long-term planning: (i) a graph-based topological map for
in natural language. Intervention could be direct (the oracle long-termplanningequippedwithaglobalactionspace;(ii)a
takes control of the agent) or indirect (the oracle adds new local metric map for short-term reasoning equipped with a
information via short-term textual instructions). A dedicated local action space; (iii) a map-based pre-training paradigm to
policy is trained to ask for intervention based on the budget. enhance the spatial-aware cross-modal reasoning. Formally,
[24] address the task of Audio-Visual-Language Embodied given an episode i, let t be the current time step and n the
Navigation (AVLEN). The agent can query an oracle under a current node of the topological graph inside the environment.
budget (i.e., the maximum number of queries is limited). The Policy π selects the best candidate node from the topological
effect of number of interactions on success is not directly mapofpoint(i),andlow-levelactionsareperformedtobring
evaluated, but only indirectly through success metrics. [25] the agent from node n to n . To be comparable to other
t t+1
relaxesassumptionsontheoraclebysimulatingassistantsthat VLN-CE agents, we maintain a maximum number of k =15
are only aware of the agent (and can thus provide assistance) steps, as done in BEVBert [13]. Notice that we do not train
when it enters their zone of attention. Interaction is evaluated the policy π, as it is considered as given. The focus of our
intermsofnumberofrequestspertask.Inboth[24]and[25], work is user-agent interactions during navigation when any
the oracle replies in the form of a full textual instruction. In instructionerrorisdetectedandlocalizedinanonlinemanner.
contrast, in our proposal, the oracle only substitutes a wrong Instruction Error Detection & Localization. We employ
word with the correct one in the original instruction. the recently proposed Instruction Error Detector & Localizer
(IEDL) [7]. IEDL is firstly composed of a cross-modal
III. OURCONTRIBUTIONS
transformer, which fuses together the semantic meaning of
Task Formulation of IVLN-CE. For each episode i, a the language instruction with the sequence of the visual
human describes to an agent how to reach a target goal by observations of the agent, producing visual-language-aligned
means of a natural language instruction I i composed of F features. Then, these features are fed to two heads: (i) a
words, i.e., I = {w ,...,w }. Note that the instruction
i 1 F
given by the human may contain errors, formally, instruction 1EvalAI-VLN-CEChallengedetection head f , trained to detect when the instruction does SIN properties. SIN is ranged between 0 and 1. A
d
not align with the sequence of observations, which outputs higher value indicates a better navigation performance with
an alignment score a ∈ [0,1]; (ii) a localization head f , interaction efficiency. Moreover, the proposed SIN metric
l
which predicts the locations of the words that may introduce possesses several favourable properties:
errors within the instructions. Notably, IEDL is trained and (i) when no interaction is performed with the human (i.e.,
evaluated in an offline manner where errors are detected and when NI is 0), SIN is mathematically equivalent to SR.
i
localized after each complete episode. (ii) SIN penalizes false positives detections.
I2EDL:Interactive-IEDL.Foreachepisodei,weexecute Proof: for every correct episode i (i.e., the instruction
the policy π following instruction I for at least p steps to is correct and thus NE = 0), a perfect agent will not
i i
acquire the set of visual observations O = {O ,...,O }. interact with the human, thus NI = 0. If this is not the
1 p i
When the current step t ≥ p, we use the detection head case, NI is increased accordingly, thus minimizing the
i
f of IEDL to check if the alignment score is a ≥ τ , SIN metric. Note that, in this scenario, the denominator
d d
where τ = 0.6 is a detection threshold. If the detection is 1, resulting in increased importance assigned to each
d
is positive, meaning that the instruction contains at least one unnecessary interaction.
error, we use the IEDL localization head f l to localize the (iii) SIN penalizes repetitive interactions.
errors. Formally, we apply the softmax operator over the Proof: for every incorrect episode i (i.e., the instructions
output of f l, and then select the token ℓj
i
with the highest containserror),theSINmetricwillbepenalizedastheagent
probability,wherej istheindexofthetoken.Whenapositive request multiple interactions with the human.
detection occurs, we increment variable NI i, showing that (iv) the weighting factor λ prevents the denominator from
the agent has detected and localized errors in the instruction. becoming excessively large. The metric can still show the
We then simulate an agent-human interaction by asking improvement in SR while penalizing excessive interaction.
the question “I think there is an error in this part of the We found that a λ = 0.01 is a good compromise between
instruction: <part>, and specifically on this <token>. Is weighting SR and number of interactions.
this the case?” In this question, <token> refers to token ℓj,
i
while<part>referstocontextL .Iftherangeidentifiedby
IV. EXPERIMENTS
j,ςl
tokens[ℓj−τl,ℓj+τl]containstheerror,thentheagentreceives Dataset. We evaluate our method I2EDL in the recently
i i
the correct token, and the embedding for instruction Υ is proposed R2RIE-CE dataset [7], i.e., R2R with Instruction
i
recomputed. Otherwise, if the detection is a false positive or Errors in Continuous Environment. R2RIE-CE is composed
no error is found by the human within the specified range, of five sets by incorporating various types of instruction
no action is performed by the human. After the interaction, errors, including: (i) Direction (one error), (ii) Object (one
the agent resumes its navigation, having the possibility to error), (iii) Room (one error), (iv) Room&Object (two errors);
query the user for every step until T steps are performed or and finally (v) All (three errors). Each set E is composed
action Stop action is selected by the policy, meaning that of correct episodes E and perturbed episodes E . For each
c p
the agent believes to have reached the goal. episode e ∈E , the authors derived an associated perturbed
i c
Success weighted by Interaction Number. Since our episode containing specific instruction errors, which is stored
interaction scheme for VLN is new, we need to propose a in the associated set E . The ratio of E and E is 50%.
p c p
novel figure of merit. The rationale is that we want to weight Metrics. To appreciate the qualities of our proposed
the success rate, dependently on how many times the agent metric SIN w.r.t. other, standard, VLN metrics [2], [26],
requires the human intervention: the higher the number of we consider: (i) Success Rate (SR): an episode is considered
interventions, the less valuable the success rate. We thus successful if the distance between the final position of the
propose SIN, i.e., Success weighted by Interaction Number, agentandthetargetlocationislessthan3meters.(ii)Success
specifically designed to combine, in a single measure, both weighted by Path Length [26] (SPL): defined as
the SR and the number of interactions with the user. Inspired
N
by the Success weighted by Path Length metric [26], we SPL= 1 (cid:88) S ℓ i
define SIN as: N imax(p i,ℓ i)
i=1
N
1 (cid:88) 1 whereN arethetotalnumberofepisodesE,ℓ istheshortest
SIN = S (1) i
N i 1+λ NIi path distance from the agent’s starting position to the goal
i=1 max(1,NEi)
in episode i, and p is the length of the path actually taken
where NI is the number of interactions with the user, NE i
i i by the agent in episode i and S is the binary indicator of
is the number of errors in episode i and S is a binary i
i successforepisodei.Finally,(iii)wereportMeanInteraction
indicator of success for episode i. The max(·) operator at
Number (MIN), which is defined as
the denominator ensures the number of interactions NI are
i
weighted by the number of errors NE i. Note that if no 1 (cid:88)N
errors are present for episode i, max(1,NE i) = 1. λ is MIN = N NI i
a weighting factor that modulates the penalisation for the i=1
interaction numbers. We consider SIN as the primary metric where NI is the number of times the agent interacts with
i
in evaluating methods addressing the IVLN-CE task. the user in episode i.TABLE I: Results show the increase of SIN (in %) under different paradigms of interaction on R2RIE-CE benchmark, with
localization threshold τ =1, weighting factor λ=0.01 from step p=4 onwards. The primary metric SIN is highlighted.
l
Under the “No Interaction” column, we report the SR, SPL metrics of the BEVBert policy[13], also showing the Success
Rate Upper Bound (SR). For I2EDL, we set detection threshold τ =0.6. Error type based on R2RIE-CE Val Unseen Dataset.
d
Nointeraction RandomInteraction AlwaysAsk I2EDL
Errortype
SR↑ SPL↑ SR↑ SIN↑ MIN↓ SR↑ SPL↑ SIN↑ MIN↓ SR↑ SPL↑ SIN↑ MIN↓ SR↑ SPL↑
Direction 53.4 43.5 58.5 52.9 1.82 53.6 43.6 52.8 3.64 54.3 44.0 53.2 0.50 53.4 43.5
Room⋆ 58.1 48.6 60.4 57.1 1.81 57.9 48.4 56.8 3.62 58.4 48.9 58.1 0.79 58.4 48.8
Object⋆ 56.1 46.1 58.7 55.8 1.75 56.6 46.4 55.2 3.53 56.7 46.7 56.1 0.70 56.3 46.3
Room&Object⋆ 57.3 46.9 61.1 56.9 1.86 57.4 47.3 57.3 3.75 58.4 47.8 58.3 1.15 58.5 47.7
All⋆ 52.4 42.6 61.9 53.3 1.97 53.8 43.5 53.2 3.95 54.1 43.8 53.8 1.37 54.0 43.4
Avg. 55.4 45.5 60.1 55.2 1.84 55.9 45.8 55.0 3.70 56.4 46.2 55.9 0.90 56.1 46.0
Baseline. As IVLN-CE is a novel task, there exists the “Always Ask” baseline, I2EDL has a higher SIN (55.9
no baselines. We thus compare our method I2EDL with vs 55.0) while having an extremely low MIN score of 0.90
a “Random Interaction” and an “Always Ask” baseline. vs 3.70. Notably, as also reported by [7], I2EDL has the
Specifically, the “Random Interaction” for every episode lowest results on the Direction error benchmark, indicating
i and for every step, randomly predicts if instruction Υ the challenging of R2RIE-CE.
i
containserrors.Ifthedetectionispositive(i.e.,theinstruction What is the SR upper bound on R2RIE-CE? In
containsanerror),wethenrandomlypredictatokenℓj,where
this experiment, we want to establish the Success Rate
i
j =rand(0,len(Υ i)−1)andrandreturnsarandomnumber upper bound (SR) that agents can reach in R2RIE-CE, i.e.,
between the arguments. The “Always Ask” baseline prompts simulating a perfect agent that is not affected by instruction
an interaction at every step from step p = 4 onwards. At perturbations. Indeed, ideal agents, as humans, are capable
each interaction, it randomly predicts the erroneous token in of identifying instruction errors, reasoning and automatically
the same way as the “Random Interaction” baseline. recovering from them. To do this, for each perturbed episode
i in E , we substitute the associated perturbed instruction
Success Rate and interaction paradigm. In Tab. I, under p
with the correct one, thus giving the correct instruction from
the “No interaction” column, we report the SR and SPL
the beginning of the episode. Note that we do not change
metrics of the current state-of-the-art method [13] for VLN-
instructions for correct episode i in E . We report the results
CE, operating under the different benchmarks of R2RIE- c
in Tab. I, under the SR metric. As we can see, the biggest
CE [7], thus establishing the lower bound of performances of
increment is in the All benchmark (52.4 vs 61.9), in which
theagentwithoutinteraction.IntheIVLN-CEscenario,agents
the wrong episode i ∈ E contains three errors. Notably,
should minimize the number of interactions with the human p
the second biggest increment is in the Direction benchmark,
while maximizing their effectiveness. Thus, in Tab. I we
which brings the SR from 53.4 to 58.5. Overall, on average,
report under the “Random Interaction” column the Success
we have an 8.48% increment from 55.4 to 60.1.
weighted by Interaction Number (SIN), Mean Interaction
Number (MIN), SR and SPL scores. We can see that the Can agents recover from instruction errors? Here, we
“RandomInteraction”,withoutbeingabletodistinguishcorrect want to show the capability of the agent to recover from
instruction from instruction with errors, performs an average different instruction errors at different steps. Thus, we want
of ∼ 2 interactions per episode, thus annoying users with to simulate the following interaction from the human: “Sorry,
unnecessary requests. This behaviour is also reflected in the instruction I gave you before is wrong. This is the correct
the SIN metric, which penalizes unnecessary interactions. one: <instruction>” where <instruction> is the correct
Such behaviour is even more evident with the “Always Ask” instruction. Specifically, for every perturbed episode i in E p,
baseline. By constantly interacting with the user, the SR is we let the agent navigate with the perturbed instruction for
high, however, achieved by an annoyingly large number of t-steps before providing the correct instruction at step t+1.
user-agent interactions. Finally, in the last columns of Tab. I As done before, it is important to note that instructions for
we present our method I2EDL, checking instruction errors correct episodes i in E c remain unchanged. We show the
from step p = 4 onwards, with a detection threshold τ = results in Fig. 3, where we plot the SR at different t-steps
d
0.6 and a localization threshold τ = 1 (i.e., the predicted for each benchmark present in R2RIE-CE. As we can see,
l
token position should differ at most for 1 token). First of the Direction and All benchmarks exhibit the most significant
all, we note that I2EDL has a much higher SIN than the decrease in SR. Note that, Direction and All have up to one
“Random Interaction”, meaning that our method is able to and three errors per instruction, respectively. Particularly for
detect instruction errors and localize them more precisely, the Direction and All benchmarks, early error detection is
thus maximizing the effectiveness of the interactions. This is crucial since these errors have a strong impact on the SR.
also reflected under the SR column, in which, apart from the HowdoSINandSRevolveoversteps?Inthisexperiment,
Direction error benchmark, I2EDL has an equal or better SR we want to analyze how SR and SIN evolve across steps
performance, while halving the average MIN (0.90 vs 1.84) usingdifferentinteractionparadigms.InFig.2,wethusshow
and scoring consistently lower in terms of SIN. Compared to the SR values (solid lines) and SIN values (dashed lines) forREFERENCES
[1] F.Taioli,F.Cunico,F.Girella,R.Bologna,A.Farinelli,andM.Cristani,
“Language-EnhancedRNR-Map:QueryingRenderableNeuralRadiance
FieldMapswithNaturalLanguage,”inICCVW,Oct.2023.
[2] P.Anderson,Q.Wu,D.Teney,J.Bruce,M.Johnson,N.Sunderhauf,
I. Reid, S. Gould, and A. van den Hengel, “Vision-and-Language
Navigation:InterpretingVisually-GroundedNavigationInstructionsin
RealEnvironments,”inCVPR,Jun.2018.
[3] J.Krantz,E.Wijmans,A.Majumdar,D.Batra,andS.Lee,Beyond
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 theNav-Graph:Vision-and-LanguageNavigationinContinuousEnvi-
ronments. ECCV,2020,p.104–120.
Fig. 2: SR and SIN plotted at different step-t for localization [4] R.Lloyd,“Cognitivemaps:Encodinganddecodinginformation,”AAG,
threshold τ =1. Specifically, dashed lines indicate value for vol.79,no.1,1989.
l [5] L.S.Liben,“Conceptualissuesinthedevelopmentofspatialcognition,”
the SIN metric, while solid lines for SR. The “Always Ask”
inSpatialcognition. PsychologyPress,2022,pp.167–194.
baseline always interacts with the user from step-t onwards. [6] H.Couclelis,“Verbaldirectionsforway-finding:Space,cognition,and
language,”inTheconstructionofcognitivemaps. Springer,1996.
[7] F. Taioli, S. Rosa, A. Castellini, L. Natale, A. D. Bue, A. Farinelli,
M.Cristani,andY.Wang,“MindtheError!DetectionandLocalization
ofInstructionErrorsinVision-and-LanguageNavigation,”2024.
Direction Object Room Room&Object All [8] A.Chang,A.Dai,T.Funkhouser,M.Halber,M.Niebner,M.Savva,
S.Song,A.Zeng,andY.Zhang,“Matterport3D:LearningfromRGB-D
62
DatainIndoorEnvironments,”in3DV,oct2017,pp.667–676.
[9] M.Savva,A.Kadian,O.Maksymets,Y.Zhao,E.Wijmans,B.Jain,
60
J.Straub,J.Liu,V.Koltun,J.Malik,D.Parikh,andD.Batra,“Habitat:
APlatformforEmbodiedAIResearch,”inICCV,Oct.2019.
58 [10] S. Chen, P.-L. Guhur, C. Schmid, and I. Laptev, “History Aware
Multimodal Transformer for Vision-and-Language Navigation,” in
56 NeurIPS,vol.34,2021,pp.5834–5847.
[11] Y.Hong,Z.Wang,Q.Wu,andS.Gould,“BridgingtheGapBetween
54 Learning in Discrete and Continuous Environments for Vision-and-
LanguageNavigation,”inCVPR,Jun.2022.
[12] D.An,H.Wang,W.Wang,Z.Wang,Y.Huang,K.He,andL.Wang,
52
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 “ETPNav:EvolvingTopologicalPlanningforVision-LanguageNaviga-
Step-t tioninContinuousEnvironments,”arXivarXiv:2304.03047,2023.
[13] D.An,Y.Qi,Y.Li,Y.Huang,L.Wang,T.Tan,andJ.Shao,“BEVBert:
Fig. 3: Success Rate upper bound (SR) at different step-t. MultimodalMapPre-trainingforLanguage-guidedNavigation,”ICCV,
2023.
[14] W. Zhu, Y. Qi, P. Narayana, K. Sone, S. Basu, X. Wang, Q. Wu,
M. Eckstein, and W. Y. Wang, “Diagnosing Vision-and-Language
Navigation:WhatReallyMatters,”inNAACL,Seattle,UnitedStates,
differentinteractionparadigmscalledfordifferentvaluesstep- Jul.2022,pp.5981–5993.
t. Specifically, (i) “Random interaction” and (ii) “Always [15] M.Hahn,A.Raj,andJ.M.Rehg,“Whichwayis‘right’?:Uncovering
limitationsofVision-and-LanguageNavigationmodels,”inAAMAS,
Ask”: as described in Sec. IV; and (iii) I2EDL: as described
2023.
in Sec. IV, in which we set detection threshold τ d = 0.6. [16] Y.Zhang,H.Tan,andM.Bansal,“DiagnosingtheEnvironmentBias
For all the interaction paradigms, τ = 1. As we can see inVision-and-LanguageNavigation,”inIJCAI-PRICAI,Jul.2020.
l
[17] J.Thomason,M.Murray,M.Cakmak,andL.Zettlemoyer,“Vision-and-
from Fig 2, the policy “Always Ask” has the highest SR,
DialogNavigation,”inCoRL,ser.ProceedingsofMachineLearning
since it continuously ask the user. On the other hand, this is Research,vol.100. PMLR,30Oct–01Nov2020,pp.394–406.
very inconvenient, since humans do not want to be disturbed [18] H.RomanRoman,Y.Bisk,J.Thomason,A.Celikyilmaz,andJ.Gao,
“RMM: A Recursive Mental Model for Dialogue Navigation,” in
constantly. This behaviour is indeed reflected by our SIN
EMNLP. AssociationforComputationalLinguistics,2020.
metric, in which “Always Ask” has the lowest SIN values. [19] S.Banerjee,J.Thomason,andJ.Corso,“TheRobotSlangBenchmark:
This behaviour is also reflected in the “Random interaction”. Dialog-guided Robot Localization and Navigation,” in CoRL, ser.
ProceedingsofMachineLearningResearch,J.Kober,F.Ramos,and
Finally, we can see that our I2EDL has the best compromise
C.Tomlin,Eds.,vol.155. PMLR,16–18Nov2021,pp.1384–1393.
between SR and the number of interactions, behaviour that [20] A. Shrivastava, K. Gopalakrishnan, Y. Liu, R. Piramuthu, G. Tur,
is correctly modelled by the SIN metric. D.Parikh,andD.Hakkani-Tur,“VISITRON:VisualSemantics-Aligned
InteractivelyTrainedObject-Navigator,”2022.
Conclusions. We presented a novel task, IVLN-CE, which
[21] Y. Zhu, Y. Weng, F. Zhu, X. Liang, Q. Ye, Y. Lu, and J. Jiao,
enablesinteractionbetweenanembodiedagentandthehuman “Self-MotivatedCommunicationAgentforReal-WorldVision-Dialog
user to correct instruction errors while navigating to a goal Navigation,”inICCV,Oct.2021.
[22] T.-C.Chi,M.Shen,M.Eric,S.Kim,andD.Hakkani-tur,“JustAsk:An
described by textual navigation instructions. We proposed
InteractiveLearningFrameworkforVisionandLanguageNavigation,”
an effective baseline, I2EDL, to perform error detection and vol.34,no.03,p.2459–2466,Apr.2020.
localization in an online fashion. Compared to baselines, we [23] K. Nguyen, D. Dey, and B. Brockett, Chrnd Dolan, “Vision-Based
NavigationWithLanguage-BasedAssistanceviaImitationLearning
showed that our proposed I2EDL is generally more effective
WithIndirectIntervention,”inCVPR,Jun.2019.
in improving navigation performance when erroneous instruc- [24] S. Paul, A. K. Roy-Chowdhury, and A. Cherian, “AVLEN: Audio-
tions are given, while lowering the interaction load. Future Visual-LanguageEmbodiedNavigationin3DEnvironments,”2022.
[25] K. Nguyen and H. Daume´ III, “Help, Anna! Visual Navigation
workswillinvestigatemulti-modalinteractionviaimagesand
with Natural Multimodal Assistance via Retrospective Curiosity-
text, with a thorough study on the user’s cognitive load. EncouragingImitationLearning,”inEMNLP-IJCNLP,2019.
RS
dna
NIS
RS[26] P. Anderson, A. Chang, D. S. Chaplot, A. Dosovitskiy, S. Gupta,
V.Koltun,J.Kosecka,J.Malik,R.Mottaghi,M.Savvaetal.,“Onevalu-
ationofembodiednavigationagents,”arXivpreprintarXiv:1807.06757,
2018.