The Price of Implicit Bias in
Adversarially Robust Generalization
Nikolaos Tsilivis1∗, Natalie Frank1, Nathan Srebro2, Julia Kempe1,3
1New York University, 2TTI-Chicago, 3Meta FAIR
Abstract
We study the implicit bias of optimization in robust empirical risk minimization (robust ERM) and
its connection with robust generalization. In classification settings under adversarial perturbations with
linear models, we study what type of regularization should ideally be applied for a given perturbation set
to improve (robust) generalization. We then show that the implicit bias of optimization in robust ERM
can significantly affect the robustness of the model and identify two ways this can happen; either through
the optimization algorithm or the architecture. We verify our predictions in simulations with synthetic
data and experimentally study the importance of implicit bias in robust ERM with deep neural networks.
1 Introduction
Robustness is a highly desired property of any machine learning system. Since the discovery of adversarial
examples in deep neural networks (Szegedy et al., 2014; Biggio et al., 2013), adversarial robustness - the
ability of a model to withstand small, adversarial, perturbations of the input at test time - has received
significant attention. A canonical way to obtain a robust model f, parameterized by w, is to optimize it for
robustness during training, i.e. given a set of training examples (x ,y )m , optimize the empirical, worst-case,
i i i=1
loss l, where worst-case refers to a predefined threat model ∆(·) which encodes our notion of proximity for
the task:
m
1 (cid:88)
min max l(f(x′;w),y ). (1)
w m i=1x′ i∈∆(xi) i i
This method of robust Empirical Risk Minimization (robust ERM aka adversarial training (Madry et al.,
2018)) has been the workhorse in deep learning for optimizing robust models in the past few years. However,
despite the outstanding performance of deep networks in “standard” classification settings, the same networks
under robust ERM lag behind; progress, in terms of absolute performance, has stagnated as measured
on relevant benchmarks (Croce et al., 2021) and predicted by experimental scaling laws for robustness
(Debenedetti et al., 2023), and any advances mainly rely on extreme amounts of synthetic data (see, e.g.,
(Wang et al., 2023)). Additionally, the (robust) generalization gap of neural networks obtained with robust
ERM is large and, during training, networks typically exhibit overfitting (Rice et al., 2020); (robust) test
error goes up after initially going down, even though (robust) train error continues to decrease. How can we
reconcile all this with the modern paradigm of deep learning, where overparameterized models interpolate
their (even noisy) training data and seamlessly generalize to new inputs (Belkin, 2021)? What is different in
robust ERM?
In“standard” classification,itisnowunderstoodthattheoptimizationprocedureisresponsibleforcapacity
control during ERM (Neyshabur et al., 2015) and this in turn permits generalization. We use the term
capacity control to refer to the way that our algorithm imposes constraints on the hypotheses considered
∗nt2231@nyu.edu. PartofthisworkwasdonewhileauthorwasvisitingToyotaTechnologicalInstituteofChicago(TTIC).
1
4202
nuJ
7
]GL.sc[
1v18940.6042:viXraClass B Class B
min ℓ
1
Class A Class A
min ℓ
2
ERM (d=512, =0,k =512,k =512) Robust ERM (d=512, = ,k =512,k =512)
2
80 80
60 m=d 60 m=d
40 40
price of implicit bias
20 20
26 27 28 29 210 26 27 28 29 210
m m
CD GD diag-net-GD
Figure 1: The price of implicit bias in adversarially robust generalization. Top: An illustration of the role of
geometry in robust generalization: a separator that maximizes the ℓ distance between the training points
2
(circles) might suffer a large error for test points (stars) perturbed within ℓ balls, while a separator that
∞
maximizes the ℓ distance might generalize better. Bottom: Binary classification of Gaussian data with
∞
(right) or without (left) ℓ perturbations of the input in Rd using linear models. We plot the (robust)
∞
generalization gap, i.e., (robust) train minus (robust) test accuracy, of different learning algorithms versus
the training size m. In standard ERM (ϵ=0), the algorithms generalize similarly. In robust ERM, however,
the implicit bias of gradient descent is hurting the robust generalization of the models, while the implicit bias
of coordinate descent/gradient descent with diagonal linear networks aids it. See Section 5 for details.
during learning; this can be achieved by means of either explicit (e.g. weight decay (Krogh and Hertz, 1991))
or implicit regularization (Neyshabur et al., 2015) induced by the optimization algorithm (Soudry et al.,
2018; Gunasekar et al., 2018a), the loss function (Gunasekar et al., 2018a), the architecture (Gunasekar et al.,
2018b) and more. This implicit bias of optimization towards empirical risk minimizers with small capacity
(some kind of “norm”) is what allows them to generalize, even in the absence of explicit regularization (Zhang
et al., 2017), and can, at least partially, explain why gradient descent returns well-generalizing solutions
(Soudry et al., 2018).
Our contributions In this work, we explore the implicit bias of optimization in robust ERM and study
carefully how it affects the robust generalization of a model. In order to overcome the hurdles of the
bilevel optimization in the definition of robust ERM (eq. (1)), we seek to first understand the situation in
linear models, where the inner minimization problem admits a closed form solution. Prior work (Yin et al.,
2019; Awasthi et al., 2020) that studied generalization bounds for this class of models for ℓ norm-constrained
p
perturbationsobservedthatthehypothesisclass(classoflinearpredictors)shouldbetterbeconstrainedinits
ℓ norm with r smaller than or equal to p⋆, where p⋆ is the dual of the perturbation norm p, i.e 1 + 1 =1.
r p p⋆
For instance, in the case of ℓ perturbations, these works postulated that searching for robust empirical
∞
risk minimizers with small ℓ norm is beneficial for robust generalization. In Section 3, we further refine
1
these arguments and demonstrate that there are also other factors, namely the sparsity of the data and the
magnitude of the perturbation, which can influence the choice of the regularizer norm r. Nevertheless, in
accordance with (Yin et al., 2019; Awasthi et al., 2020), we do identify cases where insisting on a suboptimal
2
paG
noitazilareneG
%
tsuboR
paG
noitazilareneG
%typeofregularizationmakesgeneralizationmoredifficult-muchmoredifficultthanin“standard” classification.
This observation has significant implications for training robust models. The hidden gift of optimization
that allowed generalization in the context of ERM can now become a punishment in robust ERM, if implicit
bias and threat model happen to be “misaligned” with each other. We call this the price of implicit bias in
adversarially robust generalization and demonstrate two ways this price can appear; either by varying the
optimization algorithm or the architecture. In particular, we first focus on robust ERM over the class of
linear functions f (x;w)=⟨w,x⟩ with steepest descent with respect to an ℓ norm, a class of algorithms
lin r
which generalizes gradient descent to other geometries besides the Euclidean (Section 4.1). In the case of
separabledata,weprovethatrobustERMwithinfinitesimalstepsizewiththeexponentiallossasymptotically
reachesasolutionwithminimumℓ normthatclassifiesthetrainingpointsrobustly(Theorem4.3). Although
r
this result is to be expected, given that standard ERM with steepest descent also converges to a minimum
norm solution (without the robustness constraint, however) (Gunasekar et al., 2018a), it lets us argue that,
in certain cases, gradient descent-based robust ERM will generalize poorly despite the existence of better
alternatives - see Figure 1 (bottom). We then turn our attention to study the role of architecture in robust
ERM. In Section 4.2, we study the implicit bias of gradient descent-based robust ERM in models of the
form f (x;u ,u )=(cid:10) u2 −u2,x(cid:11), commonly referred to as diagonal neural networks (Woodworth et al.,
diag + − + −
2020). These are just reparameterized linear models f , and thus their expressive power does not change.
lin
Yet, as we show, robust ERM drives them to solutions with very different properties (Proposition 4.8) than
those of f , which can generalize robustly much better - see Figure 1 (bottom).
lin
Finally, in Section 5, we perform extensive simulations with linear models over synthetic data which
illustrate the theoretical predictions and, then, investigate the importance of implicit bias in robust ERM
with deep neural networks over image classification problems. In analogy to situations we encountered in
linear models, we find evidence that the choice of the algorithm and the induced implicit bias affect the final
robustness of the model more and more as the magnitude of the perturbation increases.
Notation Let [m] = {1,...,m}. The dual norm of a vector z is defined as ∥z∥ = sup ⟨z,x⟩. The
⋆ ∥x∥≤1
dual of an ℓ norm is the ℓ with 1 + 1 = 1. For a function f : Rd → R, we use ∂f(x) to denote the
p p⋆ p p⋆
set of subgradients of f at x: ∂f(x) = (cid:8) g∈Rd :f(z)≥f(x)+⟨g,z−x⟩(cid:9). We denote by x2 ∈ Rd the
element-wise square of x.
2 Related work
Yin et al. (2019); Awasthi et al. (2020) derived generalization bounds for adversarially robust classification
for linear models and simple neural networks, based on the notion of Rademacher Complexity (Koltchinskii
and Panchenko, 2002), and form the starting point of our work (Section 3). Gunasekar et al. (2018a) studied
the implicit bias of steepest descent in ERM for linear models, while Li et al. (2020) analyzed the implicit
bias of gradient descent in robust ERM. Theorem 4.3 can be seen as a generalization of these results. In
Section4.2,weanalyzerobustERMwithgradientdescentindiagonalneuralnetworks,whichwereintroduced
in (Woodworth et al., 2020) as a model of feature learning in deep neural networks. The work of Faghri et al.
(2021)discussestheconnectionbetweenoptimizationbiasandadversarialrobustness, yetwithaverydifferent
focus than ours; the authors identify conditions where “standard” ERM produces maximally robust classifiers,
and, leveraging results on the implicit bias of CNNs (Gunasekar et al., 2018b), they design a new adversarial
attack that operates in the frequency domain. We defer a full discussion of related work to Appendix A.
3 Capacity Control in Adversarially Robust Classification
We begin by studying the connection between explicit regularization and robust generalization error in linear
models. Inparticular, wesetouttounderstandhowconstrainingtheℓ normofamodelaffectsitsrobustness
r
with respect to ℓ norm perturbations, i.e., how r interacts with p.
p
33.1 Generalization Bounds for Adversarially Robust Classification
We focus on binary classification with linear models over examples x ∈ X ⊆ Rd and labels y ∈ {±1}.
We denote by D an unknown distribution over X ×{±1}. We assume access to m pairs from D, S =
{(x ,y ),...,(x ,y )}. Let H be the class of linear hypotheses with a restricted ℓ norm:
1 1 m m r r
H ={x(cid:55)→⟨w,x⟩:∥w∥ ≤W }, (2)
r r r
where W > 0 is an arbitrary upper bound. We consider loss functions of the form l(h(x),y) = l(yh(x)),
r
by explicitly overloading the notation with l : R → [0,1]. The quantity yh(x) is sometimes referred to as
the confidence margin of h on (x,y). We assume a threat model of ℓ balls of radius ϵ centered around the
p
original samples and we define G to be the class of functions that map samples to their worst-case loss value,
r
i.e. G ={(x,y)(cid:55)→max l(yh(x′)):h∈H }. We define the (expected) risk and empirical risk of a
r ∥x′−x∥p≤ϵ r
hypothesis with respect to the worst-case loss as:
(cid:20) (cid:21) m
1 (cid:88)
L(cid:101)D(h)=E
(x,y)∼D
∥x′m −xa ∥x p≤ϵl(yh(x′)) and L(cid:101)S(h)=
m i=1∥x′
i−m xa ix ∥p≤ϵl(y ih(x′ i)), (3)
respectively. Let us also define the robust 0-1 risk as: L(cid:101)D,01(h) = E (x,y)∼D(cid:2) max ∥x′−x∥p≤ϵ1{yh(x′)≤0}(cid:3).
Central to the analysis of the robust generalization error is the notion of the (empirical) Rademacher
Complexity of the function class G :
r
(cid:34) m (cid:35) (cid:34) m (cid:35)
Rˆ (G )=E 1 sup (cid:88) σ g((x ,y )) =E 1 sup (cid:88) σ max l(y h(x′)) , (4)
S r σ m g∈Gr
i=1
i i i σ m h∈Hr
i=1
i ∥x′ i−xi∥p≤ϵ i i
where the σ ’s are Rademacher random variables. If, additionally, we consider decreasing, Lipschitz, losses
i
l(·), then, as observed by Yin et al. (2019); Awasthi et al. (2020), we can equivalently analyse the following
Rademacher Complexity Rˆ S(H(cid:101)r)=E σ(cid:2) m1 sup h∈Hr(cid:80)m
i
(cid:16)=1σ imin
(cid:16)∥x′ i−xi∥p≤ (cid:17)ϵ
(cid:17)y ih(x′ i)(cid:3), and by taking the loss in
L(cid:101)D(·),L(cid:101)S(·) in eq. (3) to be the ramp loss: l(u)=min 1,max 0,1− u
ρ
, ρ>0, we arrive at the following
margin-based generalization bound.
Theorem 3.1. (Mohri et al., 2012; Awasthi et al., 2020) Fix ρ>0. For any δ >0, with probability at least
1−δ over the draw of the dataset S, for all h∈H with H defined as in eq. (2), it holds:
r r
(cid:114)
2 log2/δ
L(cid:101)D(h)≤L(cid:101)S(h)+ ρRˆ S(H(cid:101)r)+3
2m
. (5)
Margin bounds of this kind are attractive, since they promise that, if the empirical margin risk is small
for a large ρ then the second term in the RHS will shrink, and expected and empirical risk will be close. As
shown in (Awasthi et al., 2020), the above Rademacher complexity admits an upper bound (and a matching
lower bound) of the form:
W (cid:16) (cid:17)
Rˆ S(H(cid:101)r)≤Rˆ S(H r)+ϵ 2√ mr max dp1 ⋆− r1 ,1 , (6)
where Rˆ (H ) is the “standard” Rademacher complexity. As pointed out in (Awasthi et al., 2020), there is a
S r
dimension dependence appearing in this bound, that is not present in the “standard" case of ϵ=0, and, thus,
it makes sense to choose r so that we eliminate that term. One such choice is of course r =p⋆, the dual of p.
This made the works of Yin et al. (2019); Awasthi et al. (2020) to advocate for an ℓ regularization during
p⋆
training, in order to minimize the complexity term and, hence, the robust generalization error. However, the
factor W that appears in the RHS of eq. (6) might also depend on r (and potentially d) so it is not entirely
r
clear what the optimal choice of r is for a problem at hand.
43.2 Optimal Regularization Depends on Sparsity of Data
To illustrate the previous point, we place ourselves in the realizable setting, where there exists a linear
“teacher” which labels the samples robustly. That is, there is a vector w⋆ ∈Rd which labels points and their
neighbors with the same label: y =sgn(⟨w⋆,x′⟩) for all x′ ∈{z∈Rd :∥z−x∥ ≤ϵ}. Let us specialize to
p
hypothesis classes with bounded ℓ or ℓ norm, i.e. H ,H . Since the data are assumed to be labeled by a
1 2 1 2
robust “teacher”, the robust empirical risk that corresponds to the ramp loss can be driven to zero with a
sufficiently large hypothesis class. The next Proposition provides a bound on the robust generalization of
predictors who belong to such a class.
Proposition 3.2. (Generalization bound for robust interpolators) Consider a distribution D over
Rd × {±1} with P [y =sgn(⟨w⋆,x⟩), ∀x′ :∥x′−x∥ ≤ϵ] = 1 for some w⋆ ∈ Rd. Let
(x,y)∼D p
S ∼ Dm be a draw of a random dataset S = {(x ,y ),...,(x ,y )} and let H′ =
1 1 m m r
(cid:110) (cid:111)
x(cid:55)→⟨w,x⟩:∥w∥ ≤∥w⋆∥ ∧ w∈argmax min min y ⟨u,x′⟩ be a hypothesis class
r r ∥u∥r≤1 i∈[m] ∥x′ i−x∥p≤ϵ i i
of maximizers of the robust margin. Then, for any δ >0, with probability at least 1−δ over the draw of the
random dataset S, for all h∈H′, it holds:
r
L(cid:101)D,01(h)≤  2 2√ √11 m m(cid:16) (cid:16)m ma ax xi i∥ ∥x xi i∥ ∥∞ 2∥∥ ww ⋆⋆ ∥∥ 21 +(cid:112) ϵ2 ∥l wog ⋆( ∥2 2d d) m+ axϵ (∥
p1
⋆w −⋆
1
2∥ ,1 0(cid:17) )(cid:17)+ +3(cid:113) 3(cid:113)lo lg 2 om
g
22 m/ 2δ /, δ,r r= =1
2.
(7)
The proof appears in Appendix B and follows from standard arguments based on the properties of the
ramp loss and standard Rademacher complexity bounds. Notice that eq. (7) depends on the various norms of
w⋆ and x, so we can consider specific cases in order to probe its behaviour in different regimes. In particular,
we assume that all the entries of the vectors are normalized to be O(1). We call a vector z∈Rd “dense” when
√ √
it satisfies ∥z∥ =Θ(d) and ∥z∥ =Θ( d), while we call it “k-sparse” if ∥z∥ =Θ(k) and ∥z∥ =Θ( k) for
1 2 1 2
k <d. Let us also specialize to p=∞. We enumerate the cases:
1. Dense, Dense: Ifboththegroundtruthvectorw⋆ andthesamplesx(withprobability1)aredense,then
(cid:16) √ (cid:17) (cid:16) (cid:17)
the bounds evaluate to Θ √1 (d logd+ϵd) and Θ √1 (d+ϵd) for r =1 and r =2, respectively.
m m
In particular, for ϵ=0, the r =2 bound is smaller only by a logarithmic factor, and as ϵ increases the
bounds should behave the same. So, we expect an ℓ regularization to yield smaller generalization error
2
for ϵ=0, while for larger ϵ, ℓ and ℓ regularization should perform roughly similarly.
2 1
2. k-Sparse, Dense: If the ground truth vector is k-sparse and the samples are dense, then the bounds
(cid:16) √ (cid:17) (cid:16) √ √ √ √ (cid:17)
yield Θ √1 (k logd+ϵk) and Θ √1 ( d k+ϵ k d) for r = 1 and r = 2, respectively. For
m m
k =O(1), ℓ regularization is expected to generalize better than ℓ already for ϵ=0. As ϵ increases, ℓ
1 2 2
regularized solutions should continue generalizing worse, as the “worst-case” dimension-dependent term
makes its appearance.
(cid:16) √ (cid:17)
3. Dense, k-Sparse: If w⋆ is dense and the samples x are k-sparse, then we get Θ √1 (d logd+ϵd)
m
(cid:16) √ √ (cid:17)
and Θ √1 ( k d+ϵd) for r =1 and r =2, respectively. The r =2 bounds provides more favorable
m
guarantees in this case, even for ϵ>0.
(cid:16) √ (cid:17)
4. k-Sparse, k-Sparse: If both w⋆ and x are k-sparse, then we have Θ √1 (k logd+ϵk) and
m
(cid:16) √ √ (cid:17)
Θ √1 m(k+ϵ k d) forr =1andr =2,respectively. Forϵ=0,ℓ 1andℓ 2regularizationshouldbehave
similarly, but, as ϵ increases, ℓ regularization starts “paying” the “worst-case” dimension-dependent
2
term, making the ℓ solution more appealing.
1
Notice how the “price” of robustness especially manifests itself in Case 4, where our input is “embedded”
in a k-dimensional space: the bounds are very similar for ϵ=0, but as soon as ϵ becomes positive, the extra
5x
Sparse Dense
w
Sparse ℓ ,ℓ similar as ϵ→0, ℓ better ℓ better as ϵ→0,ϵ↑0
1 2 1 1
as ϵ↑0
Dense ℓ better as ϵ→0,ϵ↑0 ℓ ,ℓ similar as ϵ→0,ϵ↑0
2 1 2
Table 1: A summary of the expected generalization behavior for the various distributions of Section 3.2. ϵ
denotes the strength of ℓ perturbations and ℓ ,ℓ denote the type of regularization applied to the solution.
∞ 1 2
penalty of ℓ solutions over ℓ grows with dimension. Moreover, Case 3 highlights that an ℓ regularization is
2 1 1
not always optimal for ℓ perturbations. To summarize, we see that the optimal choice of regularization
∞
depends not only on the choice of norm p and the value of ϵ, but also on the sparsity of the data-generating
process (see also Table 1 for a summary). In particular, in order for the dimension-dependent term to appear
in the r = 2 bound, the model w⋆ itself needs to be sparse. We provide extensive simulations with such
distributions in Section 5.1.
4 Implicit Biases in Robust ERM
In the previous section, we saw that the way we choose to constrain our hypothesis class can significantly
affect the robust generalization error. In this section, we connect this with the implicit bias of optimization
during robust ERM and demonstrate cases where the implicit regularization is either working in favor of
robust generalization or against it. The term implicit bias refers to the tendency of optimization methods
to infuse their solutions with properties that were not explicitly “encoded” in the loss function. It usually
describes the asymptotic behavior of the algorithm. We study two ways that an implicit bias can affect
robustness in robust ERM: through the optimization algorithm and through the parameterization of the
model.
4.1 Price of Implicit Bias from the Optimization Algorithm
In this section, we study the implicit bias of robust ERM in linear models with steepest descent, a family of
algorithmswhichgeneralizesgradientdescenttootherthantheEuclideangeometries. Wefocusonminimizing
the worst-case exponential loss, which has the same asymptotic properties as the logistic or cross-entropy loss
(see e.g. (Telgarsky, 2013; Soudry et al., 2018; Lyu and Li, 2020)):
m m
(cid:88) (cid:88)
L(cid:101)S(h):=L(cid:101)S(w)= max exp(−y i⟨w,x′ i⟩)= exp(−y i⟨w,x i⟩+ϵ∥w∥ p⋆). (8)
i=1∥x′ i−xi∥p≤ϵ
i=1
The above corresponds to choosing l(u)=exp(−u) in the definition of eq. (3). We first proceed with some
definitions about the margin and the separability of a dataset.
Definition 4.1. We call ℓ -margin of a dataset {x ,y }m the quantity max min yi⟨w,xi⟩.
p i i i=1 w̸=0 i∈[m] ∥w∥p⋆
Definition 4.2. A dataset {x ,y }m is (ϵ,p)-linearly separable if max min yi⟨w,xi⟩ ≥ϵ.
i i i=1 w̸=0 i∈[m] ∥w∥p⋆
Geometrically, the ℓ -margin of a dataset captures the largest possible ℓ -distance of a decision boundary
p p
to their closest data point x (see Lemma C.5 for completeness). Requiring separability is a natural starting
i
point for understanding training methods that succeed in fitting their training data and has been widely
adopted in prior work (Soudry et al., 2018; Li et al., 2020; Lyu and Li, 2020)).
6Steepest Descent (Normalized) steepest descent is an optimization method which updates the variables
with a vector which has unit norm, for some choice of norm, and aligns maximally with minus the gradient of
the objective function (Boyd and Vandenberghe, 2014). Formally, the update for normalized steepest descent
with respect to a norm ∥·∥ for a loss L (w) is given by:
S
w =w +η ∆w , where ∆w satisfies
t+1 t t t t
(9)
∆w =argmin⟨u,∇L (w )⟩.
t S t
∥u∥≤1
Unnormalized steepest descent, or simply steepest descent, further scales the magnitude of the update by
∥∇L (w )∥ , where ∥·∥ denotes the dual norm of ∥·∥. The case ∥·∥=∥·∥ corresponds to familiar gradient
S t ⋆ ⋆ 2
descent. We will be interested in understanding the steepest descent trajectory, when minimizing L(cid:101)S from
eq. (8), in the limit of infinitesimal stepsize, i.e. steepest flow dynamics:
(cid:40) (cid:41)
dw
dt
∈ v∈Rd :v∈ argmin ⟨u,g⟩,g∈∂L(cid:101)S . (10)
u∈Rd:∥u∥≤∥g∥⋆
Notice that loss L(cid:101)S is not differentiable everywhere (due to the norm in the exponent), but we can consider
subgradients in our analysis. We are ready to state our result for the asymptotic behavior of steepest flow in
minimizing the worst-case exponential loss.
Theorem4.3. Forany(ϵ,p)-linearlyseparabledatasetandanyinitializationw ,considersteepestflowwithre-
0
spect to the ℓ
r
norm, r ≥1, on the worst-case exponential loss L(cid:101)S(w)=(cid:80)m i=1max
∥x′
i−xi∥p≤ϵexp(−y i⟨w,x′ i⟩).
Then, the iterates w satisfy:
t
y ⟨w ,x′⟩ y ⟨w,x′⟩
lim min min i t i =maxmin min i i . (11)
t→∞ i ∥x′ i−xi∥p≤ϵ ∥w t∥ r w̸=0 i ∥x′ i−xi∥p≤ϵ ∥w∥ r
Theorem 4.3 can be seen as a generalization of the results of Gunasekar et al. (2018a) to robust ERM (for
any ℓ perturbation norm), modulo our continuous time analysis. The choice of analyzing continuous-time
p
dynamics was made to avoid many technical issues related to the non-differentiability of the norm, which do
not affect the asymptotic behavior of the algorithm. Li et al. (2020) studied the implicit bias of gradient
descent in robust ERM, and showed that it converges to the minimum ℓ solution that classifies the training
2
points robustly, which agrees with the special case of r =2 in Theorem 4.3. For the proof, we need to lower
bound the margin at all times t with a quantity that asymptotically goes to the maximum margin. This
requiresaduality lemmathatrelatesthe(sub)gradientofthelosswiththemaximummargin, andgeneralizes
previous results that only apply to either gradient descent, or the unperturbed loss, but not to both. The
proof appears in Appendix C.
Remark 4.4. The right hand side of eq. (11) is equivalent to:
min∥w∥ r s.t. min y i⟨w,x′ i⟩≥1, ∀i∈[m]. (12)
w ∥x′ i−xi∥p≤ϵ
Thus, the solution converges, in direction, to the hyperplane with the smallest ℓ norm which classifies the
r
training points correctly (and robustly). As a result, we can leverage Proposition 3.2 to reason about the
robust generalization of the solution returned by steepest descent. An equivalent viewpoint of (12), first
observed by Li et al. (2020) about a version of this result for gradient descent (r =2), is the following:
min∥w∥ +λ(ϵ,m)∥w∥ s.t. y ⟨w,x ⟩≥1, ∀i∈[m], (13)
r p⋆ i i
w
for some λ(ϵ,m)>0. Thus, the problem can be understood as performing norm minimization for a norm
which is a linear combination of the algorithm norm r and the dual of the perturbation norm p⋆. The
coefficient of the latter increases with ϵ, which, hereby, means that the bias induced from the perturbation
starts to dominate over the bias of the algorithm with increasing ϵ.
7In light of Section 3 and Proposition 3.2, we see that the implications of this result are twofold. First, on
the negative side, Theorem 4.3 implies that robust ERM with gradient descent (ℓ ) can harm the robust
2
generalization error if p=∞. For instance, as we saw in Cases 2 and 4 in Section 3.2, gradient descent will
suffer dimension dependent statistical overheads. On the positive side, Theorem 4.3 supplies us with an
algorithm that can achieve the desired regularization. In Cases 2 and 4 this would correspond to steepest
descent with respect to r =1. In general, we have the following corollary:
Corollary 4.5. Minimizing the loss of eq. (8) with steepest flow with respect to the ℓ norm (on (ϵ,p)
p⋆
separable data) convergences to a minimum ℓ norm solution that classifies all the points correctly.
p⋆
The notable case of steepest descent w.r.t. the ℓ norm is called coordinate descent. It amounts to
1
updating at each step only the coordinate that corresponds to the largest absolute value of the gradient
(Appendix D). In Section 5.1, we demonstrate how robust ERM w.r.t. ℓ perturbations with coordinate
∞
descent, can enjoy much smaller robust generalization error than gradient descent.
Finally, although the perturbation magnitude ϵ did not influence the conversation so far in terms of
the choice of the algorithm, it is important to note that, as ϵ increases, the max-margin solution will look
similar for any choice of norm. In fact, in the limiting case of the largest possible ϵ that does not violate the
separability assumption, all max-margin separators are the same - see Lemma C.4 - so the implicit bias will
cease to be important for generalization.
4.2 Price of Implicit Bias from Parameterization
We reasoned in the previous section that robust ERM with gradient descent over the class of linear functions
of the form f (x;w) = ⟨w,x⟩ can result in excessive (robust) test error for ℓ perturbations. We now
lin ∞
demonstrate how the same algorithm, but applied to a different architecture, can induce much more robust
models. In particular, consider the following architecture:
f (x;u)=(cid:10) u2 −u2,x(cid:11) ,u=[u ,u ]∈R2d,x∈Rd, (14)
diag + − + −
which consists of a reparameterization of f . In terms of expressive power, the two architectures are the
lin
same. However, optimizing them can result in very different predictors. In fact, this class of homogeneous
models, known as diagonal linear networks, have been the subject of case studies before for understanding
feature learning in deep networks, because, whilst linear in the input, they can exhibit non-trivial behaviors
of feature learning (Woodworth et al., 2020). In order to study the implicit bias of robust ERM with gradient
descent on f , we leverage a result by (Lyu and Zhu, 2022) which shows that, under certain conditions, the
diag
implicit bias of gradient flow based robust ERM for homogeneous networks, is towards solutions with small
ℓ norm.
2
Theorem 4.6 (Paraphrased Theorem 5 in (Lyu and Zhu, 2022)). Consider gradient flow minimizing a
w neo tr wst o- rc kas fe (e xx ;p ·o )n :en Rt pial →los Rs ,L(cid:101)
aS
n(u d) a= ssum1 m(cid:80)
e
m
i t= h1
am
t
fa ox
r∥x ai l− lx t′ i i∥ mp≤ eϵ
se t−y >if( 0x′ i; au n), dfo for ra eh ao cm ho pg oe in ne tou xs, tl hoc eal pl ey rL tui rp bs ac th ii ot nz,
i
argmax
∥xi−x′
i∥p≤ϵe−yif(x′ i;u) is scale invariant and that the loss gets minimized, i.e. L(cid:101)S(u)t→ →∞ 0. Then, u
converges in direction to a KKT point of the following optimization problem:
1
min ∥u∥2 s.t. min y f(x′;u)≥1, ∀i∈[m]. (15)
u 2 2 ∥x′ i−xi∥p≤ϵ i i
As we show, f satisfies the conditions of Theorem 4.6, and, thus, we get the following description of
diag
its asymptotic behavior.
Corollary 4.7. Consider gradient flow on the worst-case exponential loss L(cid:101)S(u) =
m1 (cid:80)m i=1max
∥xi−x′
i∥p≤ϵe−yifdiag(x′ i;u) and assume that L(cid:101)S(u) → 0. Then, u converges in direction to
a KKT point of the following optimization problem:
min 1(cid:0) ∥u ∥2+∥u ∥2(cid:1) s.t. min y (cid:10) u2 −u2,x′(cid:11) ≥1, ∀i∈[m]. (16)
u+∈Rd,u−∈Rd 2 + 2 − 2 ∥x′ i−xi∥p≤ϵ i + − i
8However,theoptimizationproblemofeq.(16),whichisoverR2d isnothingbutadisguisedℓ minimization
1
problem, when viewed in the prediction (Rd) space.
Proposition 4.8. Problem (16) has the same optimal value as the following constrained opt. problem:
min ∥w∥ 1 s.t. min y i⟨w,x′ i⟩≥1, ∀i∈[m]. (17)
w∈Rd ∥x′ i−xi∥p≤ϵ
The proofs appear in Appendix C.3. These results suggest that the bias of gradient-descent based robust
ERM over diagonal networks is towards minimum ℓ solutions, which as we argued in the previous section
1
can have very different robust error compared to ℓ solutions, which are returned by gradient-descent based
2
robust ERM over linear models. We verify this in the simulations of Section 5.1.
Remark 4.9. Technically, Corollary 4.7 only proves convergence to a first order (KKT) point, so we cannot
conclude equivalence with the minimum of the ℓ problem in eq. 17. Yet, we believe that global optimality,
1
under the condition of (ϵ,p) separability, can be proven by extending the techniques of (Moroshko et al.,
2020) in robust ERM.
5 Experiments
In this section, we explore with simulations how the implicit bias of optimization in robust ERM is affecting
the (robust) generalization of the models. Appendix F contains full experimental details.
5.1 Linear models
Setup We compare different steepest descent methods in minimizing a worst-case loss with either linear
models or diagonal neural networks on synthetic data, and study their robust generalization error. In
accordance with Section 3.2, we consider distributions that come from a “teacher” w⋆ with y =sgn(⟨w⋆,x⟩)
that can have sparse or dense x,w⋆. We denote by k and k the expected number of non-zero entries
W X
of the ground truth w⋆ and the samples x, respectively. We train linear models f (x;w) = ⟨w,x⟩ with
lin
steepest descent with respect to either the ℓ (coordinate descent - CD) or the ℓ norm (gradient descent -
1 2
GD), and diagonal neural networks f (x;u ,u ) = (cid:10) u2 −u2,x(cid:11) with gradient descent (diag-net-GD).
diag + + + −
We consider ℓ perturbations. We design the following experiment: first, we fit the training data with CD for
∞
ϵ=0 and we obtain the value of the ℓ margin of the dataset (denoted as ϵ⋆) at the end of training1. This
∞
supplies us with an upper bound on the value of ϵ for our robust ERM experiments, i.e. we know there exists
a linear model with 100% robust train accuracy for ϵ less than or equal to this ϵ⋆ margin. We then perform
robust ERM with (full batch) GD/CD/diag-net-GD for various values of ϵ less than ϵ⋆. We repeat the above
for multiple values of dataset size m (and draws of the dataset), and aggregate the results.
Results We plot the (robust) generalization gap of the three learning algorithms versus the dataset size for
(k ,k )=(512, 512) (Dense, Dense) and (k ,k )=(4, 512) (4-Sparse, Dense) in Figures 1 (bottom) and 2
W X W X
(left), respectively. In each figure, we show the performance of the methods both in ERM (no perturbations
during training) and robust ERM. The evaluation is w.r.t. the ϵ used in training. For both distributions, we
observe a significant change in the relative performance of the methods, when we pass from ERM to robust
ERM. For data with a sparse teacher (Figure 2), CD and diag-net-GD already outperform GD in terms of
generalization when implementing ERM, as a result of their bias towards minimum ℓ (sparse solutions).
1
However, in agreement with the bounds of Section 3.2, the interval between the algorithms grows when
performing robust ERM as a result of their different biases. In the case of Dense, Dense data (Figure 1), the
effect of robust ERM is more dramatic, as the algorithms generalize similarly when implementing ERM, yet
their gap between their robust generalization in robust ERM exceeds 20% in the case of few training data!
1Running this algorithm to convergence is guaranteed to result in the largest possible ℓ∞ separator of the training data
(Gunasekaretal.,2018a). Recallthattheℓ∞-marginismax w̸=0min i∈[m]yi⟨ ∥x wi, ∥w 1⟩.
9ERM (d=512, =0,k =4,k =512) k = (d) k = ( d)
80
0 0
60 m=d
40
4 4
20
2 2
0
26 27 28 29 210 logd 2logd d d logd 2logd d d
Robust ERM (d=512, = ,k =4,k =512) k = (2logd) k = (logd)
4
80
0 0
60 m=d
40
4 4
20
2 2
0
26 27 28 29 210 logd 2logd d d logd 2logd d d
m k k
CD GD diag-net-GD
0% 5% 10% 15% 20%
Figure 2: Left: Binary classification of data coming from a sparse teacher w⋆ and dense x, with (bottom) or
without (top) ℓ perturbations of the input in Rd using linear models. We plot the (robust) generalization
∞
gap, i.e., (robust) train minus (robust) test accuracy, of different learning algorithms versus the training size
m. For robust ERM, ϵ is set to be 1 of the largest permissible value ϵ⋆. The gap between the methods grows
4
when we pass from ERM to robust ERM. Right: Average benefit of CD over GD (in terms of generalization
gap) for different values of teacher sparsity k , data sparsity k and magnitude of ℓ perturbation ϵ.
W X ∞
Notice that the bounds in Section 3.2 were less optimistic than the experiments show for the performance of
CD and diag-net-GD in this case. Plots with other distributions appear in Figure 5.
To get a fine-grained understanding of the interactions between the hyperparameters of the learning
problem, we measure the average difference of (robust) generalization gaps between GD and CD. In particular,
for each different combination of sparsities (k , k ) and perturbation ϵ, we summarize curves of the form
W X
of Figure 2 (left) into one number, by calculating: 1 (cid:82)210 (GD(m)−CD(m))dm. The results are shown
210−26 26
in Figure 2 (right). Notice that, as argued in Section 3.2, there are cases with ϵ > 0 where CD does not
outperform GD (k =Θ(d), k =Θ(logd)), because the learning problem is much more “skewed” towards
W X
dense solutions. We also observe that when ϵ goes from 0 to ϵ⋆ the edge of CD over GD grows. Past a certain
4
thresholdofϵ,thetwomethodswillstarttoperformthesame,sinceforϵ=ϵ⋆ thealgorithmsreturnthesame
solution (Lemma C.4). See also Appendix E and Figure 6 for the average difference of “clean” generalization
gaps between GD and CD.
5.2 Neural networks
Our discussion has focused so far on linear (with respect to the input) models, where a closed form solution
for the worst-case loss allowed us to obtain precise answers for the connection between generalization and
optimization bias in robust ERM. Such a characterization for general models is too optimistic at this point,
because, even for a kernelized model f(x;w)=⟨w,ϕ(x)⟩, it is not clear how to compute the right notion of
margin that arises from min ⟨w,ϕ(x′)⟩ without making further assumptions about ϕ(·). As such, it
∥x′−x∥p≤ϵ
is difficult to reason that one set of optimization choices will lead to better suited implicit bias than another.
We assess, however, experimentally, what effect (if any) the choice of the optimization algorithm has on the
robustness of a non-linear model. To this end, we train neural networks with two optimization algorithms,
gradientdescent(GD)andsign(gradient)descent(SD)forvariousvaluesofperturbationmagnitudeϵ,focusing
on ℓ perturbations. SD corresponds to steepest descent with respect to the ℓ norm and is expected to
∞ ∞
obtainaminimumwithverydifferentpropertiesthantheoneobtainedwithGD(AppendixD).Inpractice, we
10
tsuboR
paG
noitazilareneG
paG
noitazilareneG
%
%
noitabrutreP
noitabrutrePERM - m=250, =0.0 Performance Gap - GD vs SD
100
90
0.4
80 100
95
70 103
101 102 103 104
0.3
Robust ERM - m=250, =0.2
100
90
80 90 0
70 85 103
101 102 103 104 100 250 10,000
Epochs #train points m
GD train GD test SD train SD test
0% 2% 4% 6% 8%
Figure 3: Left: Comparison of two optimization algorithms, gradient descent and sign gradient descent, in
ERM and robust ERM on a subset of MNIST (digits 2 vs 7) with 1 hidden layer ReLU nets. Train and test
accuracy correspond to the magnitude of perturbation ϵ used during training. We observe that in robust
ERM the gap between the generalization of the two algorithms increases. Right: Gap in (robust) test
accuracy (with respect to the ϵ used in training) of CNNs trained with GD and SD (GD accuracy minus SD
accuracy) on subsets of MNIST (all classes) for various of ϵ and m.
found it easier to train neural networks with SD than with any other steepest descent algorithm (besides GD).
Fully Connected NNs We first focus on ReLU networks with 1 hidden layer without a bias term:
f(x) = (cid:80)k u σ(W x), where σ(u) = max(0,u) is applied elementwise. For this class of homogeneous
j=1 j j
networks, we expect very different implicit biases when performing (robust) ERM with GD versus SD (see
Appendix D for details). In Figure 3, we plot the accuracy of models trained on random subsets of MNIST
(LeCun et al., 1998) with “standard” ERM (ϵ = 0) and robust ERM (ϵ = 0.2). We observe that in ERM
(top), the choice of the algorithm does not affect the generalization error much. But, for ϵ=0.2 (bottom), SD
significantly outperforms GD (4.11% mean difference over 3 random seeds), even though both algorithms reach
100% robust train accuracy. Notably, in this case, robust ERM with SD not only achieves smaller robust
generalization error, but also avoids robust overfitting during training, in contrast to GD. It is plausible that
robust overfitting, which gets observed during the late phase of training (Rice et al., 2020)), is due to (or
attenuated by) the implicit bias of an algorithm kicking in late during robust ERM. This bias can either aid
or harm the robust generalization of the model and perhaps this is why the two algorithms exhibit different
behavior. It would be interesting for future work to further study this connection. See Appendix E for plots
with different values of ϵ and m.
Convolutional NNs Departing from the homogeneous setting, where the implicit bias of robust ERM is
known or can be “guessed”, we now train convolutional neural networks (with bias terms). As a result, we do
not have direct control over which biases our optimization choices will elicit, but changing the optimization
algorithm should still yield biases towards minima with different properties. In Figure 3, we plot the mean
difference (over 3 random seeds) between the generalization of the converged models. We see that the harder
theproblemis(fewersamplesm, requestforlargerrobustnessϵ), thebiggerthepriceofimplicitbiasbecomes.
Note that for this architecture it turns out that the implicit bias of GD is better “aligned” with our learning
problem and GD generalizes better than SD, despite facing the opposite situation in homogeneous networks.
This should not be entirely surprising, since we saw already in linear models that a reparameterization can
drastically change the induced bias of the same algorithm.
11
ycaruccA
ycaruccA
tsuboR
%
%
noitabrutreP6 Conclusion
In this work, we studied from the perspective of learning theory the issue of the large generalization gap when
training robust models and identified the implicit bias of optimization as a contributing factor. Our findings
seem to suggest that optimizing models for robust generalization is challenging because it is tricky to do
capacity control “right” inrobustmachinelearning. TheexperimentsofSection5seemtosuggestsearchingfor
different first-order optimization algorithms (besides gradient descent) for robust ERM (adversarial training)
as a promising avenue for future work.
Acknowledgments. NT and JK acknowledge support through the NSF under award 1922658. Sup-
ported in part by the NSF-Simons Funded Collaboration on the Mathematics of Deep Learning
(https://deepfoundations.ai/), the NSF TRIPOD Institute on Data Economics Algorithms and Learning
(IDEAL) and an NSF-IIS award. Part of this work was done while NT was visiting the Toyota Technological
Institute of Chicago (TTIC) during the winter of 2024, and NT would like to thank everyone at TTIC for
their hospitality, which enabled this work. This work was supported in part through the NYU IT High
Performance Computing resources, services, and staff expertise.
References
Arora, S., Cohen, N., Hu, W., and Luo, Y. (2019). Implicit regularization in deep matrix factorization.
In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., and Garnett, R.,
editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information
Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 7411–7422.
(Cited on page 19.)
Athalye, A., Carlini, N., and Wagner, D. A. (2018). Obfuscated gradients give a false sense of security:
Circumventing defenses to adversarial examples. In Dy, J. G. and Krause, A., editors, Proceedings of the
35th International Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden,
July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 274–283. PMLR. (Cited
on page 19.)
Awasthi, P., Frank, N., and Mohri, M. (2020). Adversarial learning guarantees for linear hypotheses and
neural networks. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020,
13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 431–441.
PMLR. (Cited on pages 2, 3, 4, and 19.)
Awasthi, P., Mao, A., Mohri, M., and Zhong, Y. (2023). Theoretically grounded loss functions and algorithms
for adversarial robustness. In Ruiz, F., Dy, J., and van de Meent, J.-W., editors, Proceedings of The 26th
International Conference on Artificial Intelligence and Statistics, volume 206 of Proceedings of Machine
Learning Research, pages 10077–10094. PMLR. (Cited on page 19.)
Bartlett, P. L., Foster, D. J., and Telgarsky, M. (2017). Spectrally-normalized margin bounds for neural
networks. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N.,
and Garnett, R., editors, Advances in Neural Information Processing Systems 30: Annual Conference on
Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA,pages6240–6249.
(Cited on page 19.)
Belkin, M. (2021). Fit without fear: remarkable mathematical phenomena of deep learning through the prism
of interpolation. Acta Numer., 30:203–248. (Cited on page 1.)
Biggio, B., Corona, I., Maiorca, D., Nelson, B., Srndic, N., Laskov, P., Giacinto, G., and Roli, F. (2013).
Evasion attacks against machine learning at test time. In Blockeel, H., Kersting, K., Nijssen, S., and
Zelezný, F., editors, Machine Learning and Knowledge Discovery in Databases - European Conference,
12ECML PKDD 2013, Prague, Czech Republic, September 23-27, 2013, Proceedings, Part III, volume 8190 of
Lecture Notes in Computer Science, pages 387–402. Springer. (Cited on pages 1 and 19.)
Boyd, S. P. and Vandenberghe, L. (2014). Convex Optimization. Cambridge University Press. (Cited on
page 7.)
Bubeck, S., Lee, Y. T., Price, E., and Razenshteyn, I. P. (2019). Adversarial examples from computational
constraints. In Chaudhuri, K. and Salakhutdinov, R., editors, Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97
of Proceedings of Machine Learning Research, pages 831–840. PMLR. (Cited on page 19.)
Carlini, N. and Wagner, D. A. (2017). Towards evaluating the robustness of neural networks. In 2017 IEEE
Symposium on Security and Privacy, SP 2017, San Jose, CA, USA, May 22-26, 2017, pages 39–57. IEEE
Computer Society. (Cited on page 19.)
Cortes, C., Mohri, M., and Suresh, A. T. (2021). Relative deviation margin bounds. In Meila, M. and Zhang,
T., editors, Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July
2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 2122–2131. PMLR.
(Cited on page 19.)
Cortes, C. and Vapnik, V. (1995). Support-vector networks. Machine Learning, 20(3):273–297. (Cited on
page 19.)
Croce, F., Andriushchenko, M., Sehwag, V., Debenedetti, E., Flammarion, N., Chiang, M., Mittal, P.,
and Hein, M. (2021). RobustBench: a standardized adversarial robustness benchmark. In Thirty-fifth
Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2). (Cited
on pages 1 and 19.)
Debenedetti, E., Wan, Z., Andriushchenko, M., Sehwag, V., Bhardwaj, K., and Kailkhura, B. (2023). Scaling
compute is not all you need for adversarial robustness. CoRR. (Cited on page 1.)
Faghri, F., Vasconcelos, C. N., Fleet, D. J., Pedregosa, F., and Roux, N. L. (2021). Bridging the gap between
adversarial robustness and optimization bias. CoRR, abs/2102.08868. (Cited on page 3.)
Fawzi, A., Fawzi, H., and Fawzi, O. (2018). Adversarial vulnerability for any classifier. In Bengio, S., Wallach,
H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R., editors, Advances in Neural
Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018,
NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 1186–1195. (Cited on page 19.)
Foster, D. J., Greenberg, S., Kale, S., Luo, H., Mohri, M., and Sridharan, K. (2019). Hypothesis set stability
and generalization. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., and
Garnett, R., editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural
Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages
6726–6736. (Cited on page 19.)
Frei, S., Vardi, G., Bartlett, P. L., and Srebro, N. (2023). The double-edged sword of implicit bias:
Generalization vs. robustness in relu networks. CoRR, abs/2303.01456. (Cited on page 19.)
Gilmer, J., Metz, L., Faghri, F., Schoenholz, S. S., Raghu, M., Wattenberg, M., and Goodfellow, I. J. (2018).
Adversarial spheres. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver,
BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings. (Cited on page 19.)
Goodfellow, I. J., Shlens, J., and Szegedy, C. (2015). Explaining and harnessing adversarial examples. In
Bengio, Y. and LeCun, Y., editors, 3rd International Conference on Learning Representations, ICLR 2015,
San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. (Cited on page 19.)
13Gunasekar, S., Lee, J. D., Soudry, D., and Srebro, N. (2018a). Characterizing implicit bias in terms of
optimization geometry. In Dy, J. G. and Krause, A., editors, Proceedings of the 35th International
Conference on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15, 2018,
volume 80 of Proceedings of Machine Learning Research, pages 1827–1836. PMLR. (Cited on pages 2, 3, 7,
9, 19, and 29.)
Gunasekar, S., Lee, J. D., Soudry, D., and Srebro, N. (2018b). Implicit bias of gradient descent on linear
convolutional networks. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N.,
and Garnett, R., editors, Advances in Neural Information Processing Systems 31: Annual Conference on
Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages
9482–9491. (Cited on pages 2, 3, and 19.)
Gunasekar, S., Woodworth, B. E., Bhojanapalli, S., Neyshabur, B., and Srebro, N. (2017). Implicit
regularization in matrix factorization. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus,
R., Vishwanathan, S. V. N., and Garnett, R., editors, Advances in Neural Information Processing Systems
30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,
CA, USA, pages 6151–6159. (Cited on page 19.)
Ilyas, A., Santurkar, S., Tsipras, D., Engstrom, L., Tran, B., and Madry, A. (2019). Adversarial examples
are not bugs, they are features. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox,
E.B.,andGarnett,R.,editors,Advances in Neural Information Processing Systems 32: Annual Conference
on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC,
Canada, pages 125–136. (Cited on page 19.)
Ji,Z.andTelgarsky,M.(2019a).Gradientdescentalignsthelayersofdeeplinearnetworks.In7thInternational
Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. (Cited on
page 19.)
Ji, Z. and Telgarsky, M. (2019b). The implicit bias of gradient descent on nonseparable data. In Beygelzimer,
A. and Hsu, D., editors, Proceedings of the Thirty-Second Conference on Learning Theory, volume 99 of
Proceedings of Machine Learning Research, pages 1772–1798. PMLR. (Cited on page 19.)
Jia, R. and Liang, P. (2017). Adversarial examples for evaluating reading comprehension systems. In Palmer,
M., Hwa, R., and Riedel, S., editors, Proceedings of the 2017 Conference on Empirical Methods in Natural
Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11, 2017, pages 2021–2031.
Association for Computational Linguistics. (Cited on page 19.)
Kakade, S. M., Sridharan, K., and Tewari, A. (2008). On the complexity of linear prediction: Risk
bounds, margin bounds, and regularization. In Koller, D., Schuurmans, D., Bengio, Y., and Bottou, L.,
editors, Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual
Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December
8-11, 2008, pages 793–800. Curran Associates, Inc. (Cited on pages 19 and 21.)
Koltchinskii, V. (2001). Rademacher penalties and structural risk minimization. IEEE Trans. Inf. Theory,
47(5):1902–1914. (Cited on page 19.)
Koltchinskii, V. and Panchenko, D. (2002). Empirical margin distributions and bounding the generalization
error of combined classifiers. The Annals of Statistics, 30(1):1–50. (Cited on pages 3 and 19.)
Krogh, A. and Hertz, J. A. (1991). A simple weight decay can improve generalization. In Moody, J. E.,
Hanson, S. J., and Lippmann, R., editors, Advances in Neural Information Processing Systems 4, [NIPS
Conference, Denver, Colorado, USA, December 2-5, 1991], pages 950–957. Morgan Kaufmann. (Cited on
page 2.)
14Kurakin, A., Goodfellow, I. J., and Bengio, S. (2017a). Adversarial examples in the physical world. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Workshop Track Proceedings. (Cited on page 19.)
Kurakin, A., Goodfellow, I. J., and Bengio, S. (2017b). Adversarial machine learning at scale. In 5th
International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings. (Cited on page 19.)
LeCun, Y., Bottou, L., Bengio, Y., and Haffner, P. (1998). Gradient-based learning applied to document
recognition. Proc. IEEE, 86(11):2278–2324. (Cited on page 11.)
Li, Y., Fang, E. X., Xu, H., and Zhao, T. (2020). Implicit bias of gradient descent based adversarial training
on separable data. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa,
Ethiopia, April 26-30, 2020. (Cited on pages 3, 6, 7, 20, and 22.)
Long, P. M. and Sedghi, H. (2020). Generalization bounds for deep convolutional neural networks. In 8th
International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. (Cited on page 19.)
Lyu, B. and Zhu, Z. (2022). Implicit bias of adversarial training for deep neural networks. In The Tenth
International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.
(Cited on pages 8 and 20.)
Lyu, K. and Li, J. (2020). Gradient descent maximizes the margin of homogeneous neural networks. In 8th
International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. (Cited on pages 6, 19, 26, and 29.)
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. (2018). Towards Deep Learning Models
Resistant to Adversarial Attacks. In International Conference on Learning Representations. (Cited on
pages 1, 19, and 30.)
McAllester,D.A.(1998). Somepac-bayesiantheorems. InBartlett,P.L.andMansour,Y.,editors,Proceedings
of the Eleventh Annual Conference on Computational Learning Theory, COLT 1998, Madison, Wisconsin,
USA, July 24-26, 1998, pages 230–234. ACM. (Cited on page 19.)
Mohri, M., Rostamizadeh, A., and Talwalkar, A. (2012). Foundations of Machine Learning. Adaptive
computation and machine learning. MIT Press. (Cited on pages 4, 19, and 20.)
Moroshko, E., Woodworth, B. E., Gunasekar, S., Lee, J. D., Srebro, N., and Soudry, D. (2020). Implicit
bias in deep linear classification: Initialization scale vs training accuracy. In Larochelle, H., Ranzato, M.,
Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information Processing Systems 33:
Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
virtual. (Cited on page 9.)
Mustafa, W., Lei, Y., and Kloft, M. (2022). On the generalization analysis of adversarial learning. In
Chaudhuri,K.,Jegelka,S.,Song,L.,Szepesvari,C.,Niu,G.,andSabato,S.,editors,Proceedings of the 39th
International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research,
pages 16174–16196. PMLR. (Cited on page 19.)
Nacson,M.S.,Gunasekar,S.,Lee,J.D.,Srebro,N.,andSoudry,D.(2019). Lexicographicanddepth-sensitive
margins in homogeneous and non-homogeneous deep models. In Chaudhuri, K. and Salakhutdinov, R.,
editors,Proceedingsofthe36thInternationalConferenceonMachineLearning, ICML2019, 9-15June2019,
Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 4683–4692.
PMLR. (Cited on page 19.)
Nakkiran, P. (2019). Adversarial robustness may be at odds with simplicity. CoRR, abs/1901.00532. (Cited
on page 19.)
15Neyshabur, B., Bhojanapalli, S., and Srebro, N. (2018). A pac-bayesian approach to spectrally-normalized
margin bounds for neural networks. In 6th International Conference on Learning Representations, ICLR
2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. (Cited on page 19.)
Neyshabur,B.,Tomioka,R.,andSrebro,N.(2015). Insearchoftherealinductivebias: Ontheroleofimplicit
regularization in deep learning. In Bengio, Y. and LeCun, Y., editors, 3rd International Conference on
Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Workshop Track Proceedings.
(Cited on pages 1, 2, and 19.)
Ng, A. Y. (2004). Feature selection, l1 vs. l2 regularization, and rotational invariance. In Proceedings of the
Twenty-First International Conference on Machine Learning, ICML ’04, page 78, New York, NY, USA.
Association for Computing Machinery. (Cited on page 20.)
Papernot, N., McDaniel, P. D., Jha, S., Fredrikson, M., Celik, Z. B., and Swami, A. (2016). The limitations
of deep learning in adversarial settings. In IEEE European Symposium on Security and Privacy, EuroS&P
2016, Saarbrücken, Germany, March 21-24, 2016, pages 372–387. IEEE. (Cited on page 19.)
Rice, L., Wong, E., and Kolter, J. Z. (2020). Overfitting in adversarially robust deep learning. In Proceedings
of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event,
volume 119 of Proceedings of Machine Learning Research, pages 8093–8104. PMLR. (Cited on pages 1, 11,
and 19.)
Rudin, C., Cortes, C., Mohri, M., and Schapire, R. E. (2005). Margin-based ranking meets boosting in the
middle. In Auer, P. and Meir, R., editors, Learning Theory, 18th Annual Conference on Learning Theory,
COLT 2005, Bertinoro, Italy, June 27-30, 2005, Proceedings, volume 3559 of Lecture Notes in Computer
Science, pages 63–78. Springer. (Cited on page 19.)
Schapire, R. E., Freund, Y., Barlett, P., and Lee, W. S. (1997). Boosting the margin: A new explanation for
the effectiveness of voting methods. In Fisher, D. H., editor, Proceedings of the Fourteenth International
Conference on Machine Learning (ICML 1997), Nashville, Tennessee, USA, July 8-12, 1997, pages322–330.
(Cited on page 19.)
Shafahi, A., Huang, W. R., Studer, C., Feizi, S., and Goldstein, T. (2019a). Are adversarial examples
inevitable? In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019. (Cited on page 19.)
Shafahi, A., Najibi, M., Ghiasi, A., Xu, Z., Dickerson, J. P., Studer, C., Davis, L. S., Taylor, G., and
Goldstein, T. (2019b). Adversarial training for free! In Wallach, H. M., Larochelle, H., Beygelzimer, A.,
d’Alché-Buc, F., Fox, E. B., and Garnett, R., editors, Advances in Neural Information Processing Systems
32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14,
2019, Vancouver, BC, Canada, pages 3353–3364. (Cited on page 19.)
Shaham, U., Yamada, Y., and Negahban, S. (2018). Understanding adversarial training: Increasing local
stability of supervised models through robust optimization. Neurocomputing, 307:195–204. (Cited on
page 19.)
Shalev-Shwartz, S. and Ben-David, S. (2014). Understanding Machine Learning - From Theory to Algorithms.
Cambridge University Press. (Cited on page 19.)
Soudry, D., Hoffer, E., Nacson, M. S., Gunasekar, S., and Srebro, N. (2018). The implicit bias of gradient
descent on separable data. J. Mach. Learn. Res., 19:70:1–70:57. (Cited on pages 2, 6, and 19.)
Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. J., and Fergus, R. (2014).
Intriguing properties of neural networks. In Bengio, Y. and LeCun, Y., editors, 2nd International
Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference
Track Proceedings. (Cited on pages 1 and 19.)
16Taskar, B., Guestrin, C., and Koller, D. (2003). Max-margin markov networks. In Thrun, S., Saul, L. K.,
and Schölkopf, B., editors, Advances in Neural Information Processing Systems 16 [Neural Information
Processing Systems, NIPS 2003, December 8-13, 2003, Vancouver and Whistler, British Columbia, Canada],
pages 25–32. MIT Press. (Cited on page 19.)
Telgarsky, M. (2013). Margins, shrinkage, and boosting. In Dasgupta, S. and McAllester, D., editors,
Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of
Machine Learning Research, pages 307–315, Atlanta, Georgia, USA. PMLR. (Cited on pages 6 and 19.)
Tsilivis, N. and Kempe, J. (2022). What can the neural tangent kernel tell us about adversarial robustness?
In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural
Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022,
NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. (Cited on page 19.)
Tsipras, D., Santurkar, S., Engstrom, L., Turner, A., and Madry, A. (2019). Robustness may be at odds with
accuracy. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA,
USA, May 6-9, 2019. (Cited on pages 19 and 26.)
Vapnik, V. (1998). Statistical learning theory. Wiley. (Cited on page 19.)
Vardi, G. (2023). On the implicit bias in deep-learning algorithms. Commun. ACM, 66(6):86–93. (Cited on
page 19.)
Vardi, G., Yehudai, G., and Shamir, O. (2022). Gradient methods provably converge to non-robust networks.
In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A., editors, Advances in Neural
Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022,
NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022. (Cited on page 19.)
Wang, Z., Pang, T., Du, C., Lin, M., Liu, W., and Yan, S. (2023). Better diffusion models further improve
adversarial training. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., and Scarlett, J.,
editors, International Conference on Machine Learning, ICML 2023, 23-29 July 2023, Honolulu, Hawaii,
USA, volume 202 of Proceedings of Machine Learning Research, pages 36246–36263. PMLR. (Cited on
pages 1 and 19.)
Wong, E., Rice, L., and Kolter, J. Z. (2020). Fast is better than free: Revisiting adversarial training. In 8th
International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30,
2020. (Cited on page 19.)
Woodworth, B.E., Gunasekar, S., Lee, J.D., Moroshko, E., Savarese, P., Golan, I., Soudry, D., andSrebro, N.
(2020). Kernel and rich regimes in overparametrized models. In Abernethy, J. D. and Agarwal, S., editors,
Conference on Learning Theory, COLT 2020, 9-12 July 2020, Virtual Event [Graz, Austria], volume 125 of
Proceedings of Machine Learning Research, pages 3635–3673. PMLR. (Cited on pages 3, 8, and 29.)
Yin, D., Ramchandran, K., and Bartlett, P. L. (2019). Rademacher complexity for adversarially robust
generalization. In Chaudhuri, K. and Salakhutdinov, R., editors, Proceedings of the 36th International
Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97
of Proceedings of Machine Learning Research, pages 7085–7094. PMLR. (Cited on pages 2, 3, 4, and 19.)
Zhang, C., Bengio, S., Hardt, M., Recht, B., and Vinyals, O. (2017). Understanding deep learning requires
rethinking generalization. In 5th International Conference on Learning Representations, ICLR 2017,
Toulon, France, April 24-26, 2017, Conference Track Proceedings. (Cited on page 2.)
Zhang, D., Zhang, T., Lu, Y., Zhu, Z., and Dong, B. (2019a). You only propagate once: Accelerating
adversarial training via maximal principle. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’Alché-
Buc, F., Fox, E. B., and Garnett, R., editors, Advances in Neural Information Processing Systems 32:
Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
Vancouver, BC, Canada, pages 227–238. (Cited on page 19.)
17Zhang, H., Yu, Y., Jiao, J., Xing, E. P., Ghaoui, L. E., and Jordan, M. I. (2019b). Theoretically principled
trade-off between robustness and accuracy. In Chaudhuri, K. and Salakhutdinov, R., editors, Proceedings
of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach,
California, USA, volume 97 of Proceedings of Machine Learning Research, pages 7472–7482. PMLR. (Cited
on page 19.)
Zou, A., Wang, Z., Kolter, J. Z., and Fredrikson, M. (2023). Universal and transferable adversarial attacks
on aligned language models. CoRR, abs/2307.15043. (Cited on page 19.)
18A Related work
Adversarial Robustness Our discussion is focused on so-called white-box robustness (where an adversary
has access to any information about the model) - see (Papernot et al., 2016) for a taxonomy of threat models.
Adversarial examples in machine learning were first studied in (Biggio et al., 2013) for simple models and
in (Szegedy et al., 2014) for deep neural networks deployed in image classification tasks. The adversarial
vulnerability of neural networks is ubiquitous (Papernot et al., 2016) and it has been observed in many
settings and several modalities (see, for instance, (Kurakin et al., 2017a; Jia and Liang, 2017; Zou et al.,
2023)). The reasons for that still remain unclear - explanations in the past have entertained hypotheses
such as high dimensionality of input space (Fawzi et al., 2018; Gilmer et al., 2018; Shafahi et al., 2019a),
presence of spurious features in natural data (Ilyas et al., 2019; Tsipras et al., 2019; Tsilivis and Kempe,
2022), limited model complexity (Nakkiran, 2019), fundamental computational limits of learning algorithms
(Bubeck et al., 2019), and the implicit bias of standard (non-robust) algorithms (Frei et al., 2023; Vardi
et al., 2022). Many empirical methods for defending neural networks have been proposed, but most of them
failed to conclusively solve the issue (Carlini and Wagner, 2017; Athalye et al., 2018). The only mechanism
that can be adapted to any threat model and has passed the test of time is Adversarial Training (Madry
et al., 2018; Goodfellow et al., 2015; Kurakin et al., 2017b; Shaham et al., 2018), i.e., robust ERM. For neural
network training, this translates to calculating at each step adversarial examples with (projected) gradient
ascent (or some variant), and then updating the weights with gradient descent (using the gradient of the
loss evaluated on the adversarial points). There have been many attempts on improving this method, either
computationally by reducing the amount of gradient calculations (Shafahi et al., 2019b; Zhang et al., 2019a;
Wong et al., 2020), or statistically by modifying the loss function (Zhang et al., 2019b; Awasthi et al., 2023).
A common pitfall of all these methods is large (robust) generalization gap (Croce et al., 2021) and (robust)
overfitting during training (Rice et al., 2020); towards the end of training, the robust test error increases even
though the robust training error continues to decrease. Vast amounts of synthetic training data have been
shown to help on both accounts, alleviating the need for early stopping during training (Wang et al., 2023).
Margin-based Generalization bounds The idea of (confidence) margin has been central in the develop-
ment of many machine learning methods (Vapnik, 1998; Taskar et al., 2003; Rudin et al., 2005) and it has
been used in several contexts for justifying their empirical success (Cortes and Vapnik, 1995; Schapire et al.,
1997; Koltchinskii and Panchenko, 2002). In linear models and kernel methods, it is closely related to the
notion of geometric margin, and margin-based generalization bounds can explain the strong generalization
performance in high-dimensions (Vapnik, 1998; Mohri et al., 2012; Shalev-Shwartz and Ben-David, 2014).
For neural networks, they are still the object of active research (Bartlett et al., 2017; Neyshabur et al., 2018;
Long and Sedghi, 2020; Cortes et al., 2021). For these kind of bounds, the Rademacher complexity of the
hypothesis class plays a central role (Koltchinskii, 2001). Rademacher complexity-type analyses have been
shown to subsume other similar frameworks (Kakade et al., 2008; Foster et al., 2019), such as the PAC-Bayes
one (McAllester, 1998), and in many cases they can provide the finest known guarantees. Yin et al. (2019)
and Awasthi et al. (2020) recently derived margin-based bounds for adversarially robust classification - see
also Mustafa et al. (2022) for non-additive perturbations.
Implicit Bias of Optimization Algorithms The implicit bias (or regularization) of optimization
algorithms refers to the tendency of gradient methods to induce properties to the solution that were not
explicitly specified. It is believed to be beneficial for generalization in learning (Neyshabur et al., 2015).
The implicit bias of gradient descent towards margin maximization/norm minimization has been studied in
many learning setups including matrix factorization (Arora et al., 2019; Gunasekar et al., 2017), learning
with linear models (Soudry et al., 2018; Ji and Telgarsky, 2019b), deep linear (Ji and Telgarsky, 2019a)
and convolutional networks (Gunasekar et al., 2018b), and homogeneous models (Lyu and Li, 2020; Nacson
et al., 2019). Telgarsky (2013) and Gunasekar et al. (2018a) have analyzed implicit biases beyond ℓ -like
2
margin maximization for other optimization algorithms; namely Adaboost and Steepest (and Mirror) Descent,
respectively. See Vardi (2023) for a comprehensive survey of the area. The importance of sparsity (min-ℓ
1
19solutions) in binary classification has been studied in (Ng, 2004). In the context of adversarial training, Li
et al. (2020) analyzed the implicit bias of optimizing a worst-case loss with gradient descent in linear models,
and Lyu and Zhu (2022) extended these results to deep models.
B Generalization Bounds for Robust Interpolators
In this Section, we provide the proof of Proposition 3.2, which we now restate for convenience.
Proposition B.1. (Generalization bound for robust interpolators) Consider a distribution D over
Rd × {±1} with P [y =sgn(⟨w⋆,x⟩), ∀x′ :∥x′−x∥ ≤ϵ] = 1 for some w⋆ ∈ Rd. Let
(x,y)∼D p
S ∼ Dm be a draw of a random dataset S = {(x ,y ),...,(x ,y )} and let H′ =
1 1 m m r
(cid:110) (cid:111)
x(cid:55)→⟨w,x⟩:∥w∥ ≤∥w⋆∥ ∧ w∈argmax min min y ⟨u,x′⟩ be a hypothesis class
r r ∥u∥r≤1 i∈[m] ∥x′ i−x∥p≤ϵ i i
of maximizers of the robust margin. Then, for any δ >0, with probability at least 1−δ over the draw of the
random dataset S, for all h∈H′, it holds:
r
L(cid:101)D,01(h)≤  √ √22 m m(cid:16) (cid:16)m ma ax xi i∥ ∥x xi i∥ ∥∞ 2∥∥ ww ⋆⋆ ∥∥ 21 +(cid:112) ϵ2 ∥l wog ⋆( ∥2 2d d) m+ axϵ (∥
p1
⋆w −⋆
1
2∥ ,1 0(cid:17) )(cid:17)+ +3(cid:113) 3(cid:113)lo lg 2 om
g
22 m/ 2δ /, δ,r r= =1
2.
(18)
Proof. First, notice that H′ ⊆H , so, by the definition of the Rademacher complexity, it holds: Rˆ (H′)≤
r r S r
Rˆ (H ). Thus, from Theorem 3.1, we have for all h∈H′ and for ρ>0 with probability 1−δ:
S r r
(cid:114)
2 log2/δ
L(cid:101)D(h)≤L(cid:101)S(h)+ ρRˆ S(H(cid:101)r)+3
2m
. (19)
Observe that the ramp loss l (u)=min(1,max(0,1− u)),ρ>0 is an upper bound on the 0-1 loss, thus we
ρ ρ
readily get a bound for the 0-1 robust risk:
(cid:114)
2 log2/δ
L(cid:101)D,01(h)≤L(cid:101)S(h)+ ρRˆ S(H(cid:101)r)+3
2m
. (20)
Now, let us specialize the ramp loss for ρ=1 (a stronger version of this Proposition can be obtained for a ρ
that depends on the data - see, for instance, the techniques in Theorem 5.9 in (Mohri et al., 2012)). Then,
the bound becomes:
m (cid:114)
L(cid:101)D,01(h)≤ m1 (cid:88)
i=1∥x′
i−m xa ix ∥p≤ϵmin(1,max(0,1−y i⟨w,x′ i⟩))+2Rˆ S(H(cid:101)r)+3 log 2m2/δ . (21)
But, notice that for all h ∈ H′ and their corresponding w, the empirical loss
r
1 (cid:80)m max min(1,max(0,1−y ⟨w,x′⟩)) is 0, since:
m i=1 ∥x′ i−xi∥p≤ϵ i i
y ⟨w,x′⟩
argmax min min y ⟨w,x′⟩=argmax min min i i
w:∥w∥r≤1i∈[m]∥x′ i−xi∥p≤ϵ i i w∈Rd i∈[m]∥x′ i−xi∥p≤ϵ ∥w∥ r
w̸=0
1
= argmax
∥w∥ (22)
w∈Rd,w̸=0: r
mini∈[m]min ∥x′ i−xi∥p≤ϵyi⟨w,x′ i⟩=1
= argmin ∥w∥ .
r
w∈Rd,w̸=0:
min ∥x′ i−xi∥p≤ϵyi⟨w,x′ i⟩≥1∀i∈[m]
20h⋆
H
2
H
H ∞
1
Figure 4: An illustration of the model selection problem we are facing in Section 3. We depict hypothesis
classes which correspond to H ={x(cid:55)→⟨w,x⟩:∥w∥ ≤W} for r =1,2,∞ (notice that here, for illustration
r r
purposes, we keep W constant and not dependent on r). Increasing the order r of H can decrease the
r
approximationerroroftheclass, butitmightincreasethecomplexitycapturedbytheworst-caseRademacher
Complexity term of eq. (6). Furthermore, this complexity might increase significantly more than in “standard”
classification (see 2nd term in the RHS of eq. (6)), and this is where the price of misselection comes from.
Note that the set of solutions is not empty, since w⋆ satisfies the constraints with probability 1. As a result,
for all h∈H′ we obtain:
r
(cid:114)
log2/δ
L(cid:101)D,01(h)≤2Rˆ S(H(cid:101)r)+3
2m
. (23)
Combining this with the upper bound of the adversarial Rademacher complexity of eq. (6), together with the
standard Rademacher complexity bounds for r =1,2 (Kakade et al., 2008):
 (cid:16) √ (cid:17)
O
maxi∈[m]∥xi√∥∞W1 2log2d
, r =1,
Rˆ S(H r)≤ O(cid:16) maxi∈[m√]∥xi∥2m W2(cid:17)
, r =2,
(24)
m
we obtain the result.
C Implicit Biases in Robust ERM
C.1 Robust ERM over Linear Models with Steepest Flow
First, we provide the proof of Theorem 4.3. Recall that we are interested in analyzing the implicit bias of
steepest descent algorithms with infinitesimal step size when minimization of a worst case loss.
The steepest flow update with respect to a norm ∥·∥ is written as follows:
(cid:40) (cid:41)
dw
dt
∈ v∈Rd :v∈ argmin ⟨u,g⟩,g∈∂L(cid:101)S :=S, (25)
u∈Rd:∥u∥≤∥g∥⋆
where recall ∂L(cid:101)S is the set of subgradients of L(cid:101)S.
We restate Theorem 4.3.
Theorem C.1. For any (ϵ,p)-linearly separable dataset and any initialization w , steepest flow with respect
0
to the ℓ
r
norm, r ≥ 1, on the worst-case exponential loss L(cid:101)S(w) = (cid:80)m i=1max
∥x′
i−xi∥p≤ϵexp(−y i⟨w,x′ i⟩)
satisfies:
y ⟨w ,x′⟩ y ⟨w,x′⟩
lim min min i t i =maxmin min i i . (26)
t→∞ i ∥x′ i−xi∥p≤ϵ ∥w t∥ r w̸=0 i ∥x′ i−xi∥p≤ϵ ∥w∥ r
21Proof. Throughout the proof, we suppress the dependence of w on time. Let us define the maximum,
worst-case, margin:
y ⟨w,x′⟩
γ =max min min i i . (27)
(cid:101)r⋆
w̸=0i∈[m]∥x′ i−∥p≤ϵ ∥w∥ r
We use the subscript r⋆, because it maximizes distance with respect to the ℓ norm. Recall that the loss
r⋆
function is given as:
m m
(cid:88) (cid:88)
L(cid:101)S(w)= max exp(−y i⟨w,x′ i⟩)= exp(−y i⟨w,x i⟩+ϵ∥w∥ p⋆). (28)
i=1∥x′ i−xi∥p≤ϵ
i=1
By the definition of the loss function, we have for any t>0:
m
(cid:88)
L(cid:101)S(w)= exp(−y i⟨w,x i⟩+ϵ∥w∥ p⋆)≥ maxexp(−y i⟨w,x i⟩+ϵ∥w∥ p⋆). (29)
i∈[m]
i=1
Thus, we obtain the following relation between the loss and the current margin:
1
min y ⟨w,x ⟩−ϵ∥w∥ ≥log , (30)
i i p⋆
i∈[m] L(cid:101)S(w)
so the goal will be to lower bound the RHS. For that we need the following Lemma, which consists of the
core of the proof and quantifies the relation between maximum margin and loss (sub) gradients. This Lemma
generalizes Lemma C.1 in (Li et al., 2020) that applies to gradient descent. We will overload notation and
denote by ∂f any subgradient of f.
Lemma C.2. For any w∈Rd, it holds:
γ ≤
∥∂L(cid:101)S(w)∥
r⋆ . (31)
(cid:101)r⋆
L(cid:101)S(w)
Proof. Let u be a vector that attains γ , i.e. u is a worst-case ℓ maximum margin separator:
(cid:101)r⋆ (cid:101)r⋆ (cid:101)r⋆ r⋆
y ⟨w,x′⟩
u ∈argmax min min i i . (32)
(cid:101)r⋆
w̸=0 i∈[m]x′ i∈Bϵp(x) ∥w∥ r
Then, we have (since L(cid:101)S is convex, the “chain rule” holds):
m
(cid:68) (cid:69) (cid:88)
u (cid:101)r⋆,−∂L(cid:101)S(w) = ⟨u (cid:101)r⋆,y ix i−ϵ∂∥w∥ p⋆⟩e−yi⟨w,xi⟩+ϵ∥w∥p⋆
i=1 (33)
m
=(cid:88)
∥u (cid:101)r⋆∥
r⟨u (cid:101)r⋆,y ix i⟩ ∥− uϵ⟨u ∥(cid:101)r⋆,∂∥w∥ p⋆⟩
e−yi⟨w,xi⟩+ϵ∥w∥p⋆.
(cid:101)r⋆ r
i=1
But,bythedefinitionofthedualnormandofthesubgradient,wehave⟨u ,∂∥w∥ ⟩≤∥u ∥ ∥∂∥w∥ ∥ =
(cid:101)r⋆ p⋆ (cid:101)r⋆ p⋆ p⋆ p
∥u ∥ , so eq. (33) becomes:
(cid:101)r⋆ p⋆
m
(cid:68) (cid:69) (cid:88)
u (cid:101)r⋆,−∂L(cid:101)S(w) ≥ ∥u (cid:101)r⋆∥ rγ (cid:101)r⋆e−yi⟨w,xi⟩+ϵ∥w∥p⋆, (34)
i=1
which by rearranging can be written as:
(cid:28) (cid:29)
u
∥u(cid:101)r⋆
∥
,−∂L(cid:101)S(w) ≥γ (cid:101)r⋆L(cid:101)S(w). (35)
(cid:101)r⋆ r
Finally, again, by the definition of the dual norm, we get the desired result:
∥∂L(cid:101)S(w)∥
r⋆
≥γ (cid:101)r⋆L(cid:101)S(w). (36)
22In light of this Lemma, we can lower bound the derivative of the RHS of eq. (30) as follows:
dlog 1
L(cid:101)S =−
1 dL(cid:101)S
dt L(cid:101)S dt
(cid:28) (cid:29)
1 dw
=−
L(cid:101)S
∂L(cid:101)S,
dt
(Chain rule)
(37)
=
∥∂L(cid:101)S∥ r⋆(cid:13) (cid:13)d dw t(cid:13) (cid:13)
r (Def. of steepest flow)
L(cid:101)S
(cid:13) (cid:13)
≥γ (cid:101)r⋆(cid:13) (cid:13) (cid:13)d dw
t
(cid:13) (cid:13)
(cid:13)
(Lemma C.2).
r
Thus, eq. (30) becomes:
y ⟨w,x ⟩−ϵ∥w∥
(cid:82)t(cid:13) (cid:13)dw(cid:13)
(cid:13) ds
min i i p⋆ ≥γ 0 ds r (from Eq. (37))
i∈[m] ∥w∥ r
(cid:101)r⋆
∥w∥ r
(cid:13) (cid:13)
(cid:13)(cid:82)t dwds(cid:13)
(cid:13) 0 ds (cid:13)
≥γ r
(cid:101)r⋆ ∥w∥ (38)
r
∥w−w ∥
=γ 0
(cid:101)r⋆
∥w∥
r
(cid:13) (cid:13)
=γ (cid:101)r⋆(cid:13) (cid:13) (cid:13)∥ww
∥
− ∥ww 0
∥
(cid:13) (cid:13)
(cid:13)
→γ (cid:101)r⋆,
r r r
sinceL(cid:101)S →0(d dL(cid:101) tS ≤0-seeLemmaC.3-andL(cid:101)S isboundedfrombelow)andhenceitmustbe∥w∥→∞.
Lemma C.3. For any convex L, for the steepest flow updates of eq. (25) it holds:
dL dw
≤0and =argmin∥v∥ ∀t>0. (39)
dt dt
v∈S
Proof. Since L is convex, the “chain rule holds”, that is, for all g∈∂L we have:
(cid:28) (cid:29)
dL dw
= g, . (40)
dt dt
First, apply this to the element of ∂L, g, that corresponds to dw, then, by the definition of S and that of a
dt
dual norm, we have:
dL =−(cid:13) (cid:13) (cid:13)dw(cid:13) (cid:13) (cid:13)2 . (41)
dt (cid:13) dt (cid:13)
Now, apply the “chain rule” for g⋆ =argmin ∥g∥ :
g∈∂L ⋆
(cid:28) (cid:29) (cid:13) (cid:13)
d dL
t
= g⋆,d dw
t
≤∥g⋆∥ ⋆(cid:13) (cid:13) (cid:13)d dw
t
(cid:13) (cid:13) (cid:13). (42)
By combining (41),(42), we get:
∥g⋆∥ ⋆(cid:13) (cid:13) (cid:13) (cid:13)d dw
t
(cid:13) (cid:13) (cid:13) (cid:13)≥(cid:12) (cid:12) (cid:12) (cid:12)d dL t(cid:12) (cid:12) (cid:12) (cid:12)=(cid:13) (cid:13) (cid:13) (cid:13)d dw
t
(cid:13) (cid:13) (cid:13) (cid:13)2 , (43)
which implies (cid:13) (cid:13)d dw t(cid:13) (cid:13)≤∥g⋆∥ ⋆.
23C.2 Equivalence of Max-Margin Solutions
Lemma C.4. Let {(x ,y )}m be a dataset with ℓ margin equal to ϵ⋆. Any hyperplane that separates
(cid:8)(cid:8) x′ ∈Rd :∥x′ −x ∥ ≤i ϵi ⋆(cid:9)i ,= y1 )(cid:9)m is an ℓ max-mp argin separator for any r.
i i i p i i=1 r
This result informs us that any choice of algorithm in robust ERM (w.r.t to ℓ perturbations) for ϵ equal
p
to the ℓ margin of the dataset will produce the same solutions.
p
We provide the proof of Lemma C.4. First, one can calculate the margin of a point x with respect to a
linear separator w:
Lemma C.5. The ℓ margin of a linear hyperplane w at a datapoint (x,y) is y⟨w,x⟩/∥w∥ .
p p∗
Proof. We want to find the largest c for which ⟨w,x+ch⟩≥0 for all ∥h∥ ≤1.
p
for all h with ∥h∥ ≤1 Taking an infimum over h results in
p
⟨w,x⟩−c∥w∥ ≥0
p∗
and thus c=⟨w,x⟩/∥w∥ .
p∗
Next, this lemma allows one to calculate the margin of a ball around a point. We denote by Bp(x) the ℓ
ϵ p
ball around x, i.e. Bp(x)={x′ :∥x′−x∥ ≤ϵ}.
ϵ p
Lemma C.6. The ℓ -margin of the set Bp(x) with label y is
r ϵ
yw·x−ϵ∥w∥
p∗
∥w∥
r∗
Proof. We want to find the largest constant c for which
y(w·(x+h +h ))≥0
1 2
for all h ∈Bp(0) and h ∈Br(0). Taking an infimum over all possible h and h results in
1 ϵ 2 c 1 2
yw·x−ϵ∥w∥ −c∥w∥ ≥0
p∗ r∗
Therefore, the largest such possible c is
yw·x−ϵ∥w∥
c=
r∗
∥w∥
r∗
This result immediately implies Lemma C.4:
Proof of Lemma C.4. Let w∗ be the ℓ max-margin hyperplane separating the {(Bp(x ),y )}m . If the
r ϵ i i i=1
ℓ -margin of the dataset is ϵ, then Lemma C.5 implies that
p
yw∗·x−ϵ∥w∥
min
p∗
=0
i∈[1,m] ∥w∥ r∗
and therefore Lemma C.6 implies that the ℓ max-margin hyperplane has margin 0. On the other hand, any
r
separating hyperplane for {(Bp(x ),y )}m has a separation margin that is at worst zero. Therefore, any
ϵ i i i=1
separating hyperplane is an ℓ max-margin hyperplane.
r
24C.3 Robust ERM over Diagonal Networks
We first show Corollary 4.7.
Proof. A diagonal neural network f (x;u) is 2-homogeneous, since for any c>0, it holds:
diag
f (x;cu)=(cid:10) (cu )2−(cu )2,x(cid:11) =c2(cid:10) u2 −u2,x(cid:11) . (44)
diag + − + −
Furthermore, foranyx, theoptimalperturbationisscaleinvariantasitis: argmin (cid:10) u2 −u2,x+δ(cid:11) =
∥δ∥∞≤ϵ + −
−ϵsign(u2 −u2), which is scale invariant, i.e., sign((αu )2−(αu )2)=sign(u2 −u2) for any α>0. Thus,
+ − + − + −
f satisfies the conditions of Theorem 4.6.
diag
Now, we provide the proof of Proposition 4.8, which states that ℓ minimization in parameter space is
2
equivalent to ℓ minimization in predictor space for robust ERM in diagonal networks.
1
Proof. Let us recall the two optimization problems:
min 1(cid:0) ∥u ∥2+∥u ∥2(cid:1)
u+∈Rd,u−∈Rd 2 + 2 − 2 (45)
s.t. min y (cid:10) u2 −u2,x′(cid:11) ≥1, ∀i∈[m],
i + − i
∥x′ i−xi∥p≤ϵ
and
1
min ∥w∥
w 2 1 (46)
s.t. min y ⟨w,x′⟩≥1, ∀i∈[m].
i i
∥x′ i−xi∥p≤ϵ
We will show that the two problems share the same optimal value OPT. Let (u ,u ),w⋆ be optimal
(cid:101)+ (cid:101)−
solutions of (45) and (46), respectively, with corresponding values OPT ,OPT .
A B
• First, we show that OPT ≤OPT . Let wˆ =u2 −u2, then wˆ satisfy the constraints of (46) and:
B A (cid:101)+ (cid:101)−
d
(cid:88)
∥wˆ∥ =∥u2 −u2∥ = |u −u ||u +u |
1 (cid:101)+ (cid:101)− 1 (cid:101)+,j (cid:101)−,j (cid:101)+,j (cid:101)−,j
j=1
≤
1(cid:88)d
(u −u )2+(u +u )2
(47)
4 (cid:101)+,j (cid:101)−,j (cid:101)+,j (cid:101)−,j
j=1
= 1(cid:0) ∥u ∥2+∥u ∥2(cid:1) =OPT .
2 (cid:101)+ 2 (cid:101)− 2 A
As wˆ is a feasible point of (46), it is OPT ≤∥wˆ∥ and we deduce OPT ≤OPT .
B 1 B A
• Now,weprovethereverserelation. Wedecomposew⋆toitspositiveandnegativepart,i.ew⋆ =uˆ2 −uˆ2,
+ −
where, observe, the supports (set of indices with non-zero values) of uˆ ,uˆ do not overlap. Then,
+ −
(uˆ ,uˆ ) satisfy the constraints of (45) and, furthermore:
+ −
1(cid:0) ∥uˆ ∥2+∥uˆ ∥2(cid:1) = 1 ∥w⋆∥ ≤OPT . (48)
2 + 2 − 2 2 1 B
Since (uˆ ,uˆ ) is a feasible point of (45), we deduce that OPT ≤OPT .
+ − A B
25D Cases of Steepest Descent & Implicit Bias
Coordinate Descent In coordinate descent, at each step we only update the coordinate with the largest
absolute value of the gradient. Formally, its update is given by:
(cid:40) (cid:12) (cid:12)(cid:41)
∆w t ∈conv −∂ ∂L w(w [it ]) e i :i=argmax(cid:12) (cid:12) (cid:12)∂ ∂L w(w [jt ])(cid:12) (cid:12)
(cid:12)
, (49)
t j∈[d] t
where e ,i∈[d], denotes the standard basis and conv(·) stands for the convex hull. Coordinate descent has
i
long been studied for its connection with the ℓ regularized exponential loss and Adaboost. It corresponds
1
to Steepest Descent with respect to the ℓ norm, and at each step it holds: ∥∆w∥ = ∥∇L∥ . In our
1 1 ∞
experiments, we found it difficult to run robust ERM with coordinate descent for large values of perturbation
ϵ, both in linear models and neural networks. Also, it is computationally challenging to scale coordinate
descent to large models, since only one coordinate gets updated at a time. These are the main reasons why
we chose to experiment with Sign (Gradient) Descent in Section 5.2.
Sign (Gradient) Descent In Sign (Gradient) Descent, we only use the sign of the gradient to update the
iterates, i.e.
∆w =−sign(∇L(w )). (50)
t t
It corresponds to steepest descent with respect to the ℓ norm. Its connection with popular adaptive
∞
optimizers has made it an interesting algorithm to study for deep learning applications.
Implicit bias in homogeneous networks From the results of (Lyu and Li, 2020), we know that, for
homogeneous networks, gradient descent converges in direction to a KKT point of a maximum margin
optimization problem defined by the ℓ norm. For steepest descent, on the other hand, there is no such
2
characterization, yet we expect a similar result to hold; namely, we expect running ERM with steepest
descent to converge in direction to a point that has some relation to the maximum margin optimization
problem defined by the norm of the algorithm. By making a leap of faith, we expect something similar to
hold for robust ERM. Since the promotion of a margin in one norm can have very different properties from
the promotion of a margin in a different norm, we expect robust ERM with gradient descent and sign descent
to yield solutions with different properties.
E Additional Experiments
Linear models We plot (robust) generalization gaps vs dataset size m for distributions with (k ,k )
W X
equal to (512, 4) (Dense, Sparse) and (k ,k ) equal to (4, 4) (Sparse, Sparse) on the top and the bottom of
W X
Figure 5, respectively. In accordance to the bounds of Section 3.2, it can still be the case that ℓ solutions
2
will generalize better in robust ERM, due to the significant advantage of them in ERM. In Figure 6, we
produce heatmaps similar to those of Figure 2, but the benefit of CD over GD is measured with respect to
“clean” generalization (ϵ=0), no matter what value of ϵ was used during training. In particular, for each
combination of data/weight sparsity and perturbation ϵ used at training, we compute clean generalization
gaps of CD and GD solutions for various values of dataset size m. We then aggregate the results over m and
we compute 1 (cid:82)210 (GD(m)−CD(m))dm, whereas in Section 5.1 the curves GD(m),CD(m) referred to the
210−26 26
robust error (w.r.t. the value of ϵ used during training). We observe that there are cases such as the Dense,
Dense one with k =k =d=512 that for ϵ>0 (in particular equal to ϵ⋆/2 - bottom right corner in top
W X
left subplot), GD generalizes better than CD in terms of clean error, even though it was the other way around
for robustness - see Figure 2 and Figure 1 (bottom right). This suggests that even if we nail the optimization
bias in robust ERM, we might still incur a tradeoff between robustness and accuracy (Tsipras et al., 2019).
26ERM (d=512, =0,k =4,k =4) Robust ERM (d=512, = ,k =4,k =4)
4
80 80
60 m=d 60 m=d
40 40
20 20
26 27 28 29 210 26 27 28 29 210
m m
CD GD
ERM (d=512, =0,k =4,k =4) Robust ERM (d=512, = ,k =4,k =4)
4
80 80
60 m=d 60 m=d
40 40
20 20
26 27 28 29 210 26 27 28 29 210
m m
CD GD
Figure 5: Binary classification of data coming from a dense teacher w⋆ and sparse data x (top) and from
a sparse w⋆ and sparse data x (bottom). We compare performance of different algorithms with (right) or
without (left) ℓ perturbations of the input in Rd using linear models. We plot the (robust) generalization
∞
gap, i.e., (robust) train minus (robust) test accuracy, of different learning algorithms versus the training
size m. For robust ERM, ϵ is set to be 1 of the largest permissible value ϵ⋆. In accordance to the bounds
4
of Section 3.2, it can still be the case that ℓ solutions will generalize better in robust ERM, due to the
2
significant advantage of them in ERM.
Standard Generalization
k = (d) k = ( d)
0 0
4 4
2 2
logd 2logd d d logd 2logd d d
k = (2logd) k = (logd)
0 0
4 4
2 2
logd 2logd d d logd 2logd d d
k k
0% 5% 10% 15% 20%
Figure 6: Average benefit, i.e. 1 (cid:82)210 (GD(m)−CD(m))dm, of CD over GD (in terms of “clean” generaliza-
210−26 26
tion gap) for different values of teacher sparsity k , data sparsity k and magnitude of perturbation ϵ used
W X
during training (evaluation here is always with respect to ϵ=0 - “standard” generalization). The dimension d
is fixed to be 512.
27
paG
noitazilareneG
paG
noitazilareneG
%
%
noitabrutreP
noitabrutreP
tsuboR
tsuboR
paG
noitazilareneG
paG
noitazilareneG
%
%Fully-Connected Neural Networks InFigure7,
Accuracy - 2 vs 7, m=100
we plot (robust) accuracy during training in ERM
ERM - =0.0
and robust ERM (ϵ = 0.2,0.3) for 1 hidden layer 100
ReLU networks trained on a subset of 100 images 95
(randomly drawn in each seed) of digits 2 and 7 90
85
from the MNIST dataset. We observe that the gap
80
between the performance of gradient descent and
75
steepest descent in larger in robust ERM than in
Robust ERM - =0.2
ERM. 100
95
90
Convolutional Neural Networks In Figure 8, 85
we plot the (robust) train and test accuracy dur- 80
ing training for various combinations of dataset size 75
Robust ERM - =0.3
and perturbation magnitude. The main observation, 100
summarized also in Figure 3 (left) in the main text, 95
is that when there are little available data m, the 90
85
implicit bias in robust ERM affects generalization
80
more than in “standard” ERM (row-wise comparison
75 102 103 104
in the Figure). Notice that the artifact of the light- Epochs
ish bottom right corner in Figure 3 (non-trivial gap
GD train GD test SD train SD test
between GD and SD for m=10,000 and ϵ=0) is due Figure 7: Accuracy during training in ERM (top) and
to the fact that SD becomes unstable at that time robust ERM for ϵ=0.2 (center) and ϵ=0.3 (bottom).
near convergence. Reducing the learning rate would
Setting: 1 hidden layer ReLU networks trained on a
have allievated this “anomaly”.
subsetofMNIST.Meanover3randomseeds-random-
ness affects initialization and draw of random dataset.
The gap between the generalization of the algorithms
F Experimental details
is more significant in robust ERM.
In this Section, we provide more details about our
experimental setup. All experiments are implemented in PyTorch and were run either on multiple CPUs
(experiments with linear models) or GPUs. Estimated GPU hours: 200.
F.1 Experiments with synthetic data
We consider the following distributions:
1. Dense, Dense: We sample points x ∼N(0,I ),i∈[m], and a ground truth vector w⋆ ∼N(0,I ) that
i d d
labels each of the m points with y =sgn(⟨w⋆,x ⟩).
i i
2. k-Sparse, Dense: We sample points x ∼ {−1,0,+1},i ∈ [m], with corresponding probabilities
i
{ k ,1− k, k } (so expected number of non-zero entries is k) and a ground truth vector w⋆ ∼N(0,I )
2d d 2d d
that labels each of the m points with y =sgn(⟨w⋆,x ⟩).
i i
3. Dense, k-Sparse: Same as before, but now x is dense and w⋆ is k-sparse (with high probability).
4. k-Sparse, k-Sparse: We sample points x ∼ N(0,I ),i ∈ [m], and a ground truth vector w⋆ ∼
i d
{−1,0,+1} with corresponding probabilities { k ,1− k, k } (so expected number of non-zero entries is
2d d 2d
k) that labels each of the m points with y =sgn(⟨w⋆,x ⟩).
i i
Linear models For the experiments with linear models f(x;w)=⟨w,x⟩, we train with the exponential
loss and we use an (adaptive) learning rate schedule η =min{η , 1 }, where η is a finite upper
t + +
(B+ϵ)2L(cid:101)(wt)
bound (105 in our experiments) and B is the largest ℓ norm of the train data. This type of learning rate
∞
schedule can be derived by a discrete-time analysis of robust ERM over linear models (the direct analogue of
28
ycaruccA
ycaruccA
tsuboR
ycaruccA
tsuboR
%
%
%Accuracy - multiclass, (m=100, =0) Accuracy - multiclass, (m=100, =0.3) Accuracy - multiclass, (m=100, =0.4)
100 GD train 100 GD train 100 GD train
90 G SDD tt re as it n 90 G SDD tt re as it n 90 G SDD tt re as it n
80 SD test 80 SD test 80 SD test
70 70 70
60 60 60
50 50 50
40 40 40
30 100 101 102 30 100 101 102 30 100 101 102
Epochs Epochs Epochs
Accuracy - multiclass, (m=250, =0) Accuracy - multiclass, (m=250, =0.3) Accuracy - multiclass, (m=250, =0.4)
100 GD train 100 GD train 100 GD train
GD test GD test GD test
90 SD train 90 SD train 90 SD train
SD test SD test SD test
80 80 80
70 70 70
60 60 60
50 100 101 102 50 100 101 102 50 100 101 102
Epochs Epochs Epochs
Accuracy - multiclass, (m=10000, =0) Accuracy - multiclass, (m=10000, =0.3) Accuracy - multiclass, (m=10000, =0.4)
100 100 100 GD train
GD test
90 90 90 SD train
SD test
80 80 80
70 70 70
GD train GD train
60 GD test 60 GD test 60
SD train SD train
SD test SD test
50 100 101 102 50 100 101 102 50 100 101 102
Epochs Epochs Epochs
Figure 8: Training curves of CNNs trained on subsets of MNIST for various combinations of dataset size m
and perturbation magnitude ϵ.
Theorem 4.3). A similar learning rate schedule appears in the works of Gunasekar et al. (2018a); Lyu and Li
(2020). To allow a fair comparison between the two algorithms, we stop their execution when they reach the
same training loss value2 (10−3). We start from the all-zero, w=0, initialization.
Diagonal neural networks For the experiments with the diagonal linear network f(x;w)=⟨w,x⟩, with
w =u2 −u2, we use a constant, small, learning rate 2×10−3 and we adopt the same stopping criterion
+ −
as with the “vanilla” linear models. We initialize u +,u
−
with a constant value of √α (where α is the
2d
initializationscaleanddtheinputdimension)whichhasbeenstandardinpriorworkswithdiagonalnetworks.
We set α = 10−3 to promote “feature” learning, i.e. to induce the implicit bias “faster” - see (Woodworth
et al., 2020).
For all the runs with the various models/algorithms, we sample d2 independent points and use them
as a test set (one draw per dimension, i.e. same test dataset across the different values of m and ϵ). The
maximum value of perturbation, ϵ⋆, is estimated by running ERM with coordinate descent for 105 iterations.
Notice that this results to a different ϵ⋆ for each different draw of the dataset. The robust test accuracy is
efficiently calculated, since the adversarial points can be calculated in closed form for linear models. Our
experimental protocol tried to ensure that we reach 100% (robust) train accuracy in all runs. This is true
in all cases but the distributions with sparse data, where we found that for training datasets of cardinality
m=2d, the margin of the dataset was small and, thus, convergence was very slow. In these cases, the robust
train accuracy approached 100%, but it was not exactly equal to it.
F.2 Experiments with neural networks
In all the experiments with the various values of ϵ and m, we train with 3 random seeds which correspond to
a different set of samples drawn from the full dataset and a different initialization.
2Ifthealgorithmhasnotreachedthisvalueafter2×105 iterations,westopatthatepoch.
29Fully Connected Neural Networks We run GD and SD with the same set of hyperparameters; constant
learning rate equal to 10−5, batch size equal to the whole available data (the amount varies across different
experiments), and, when ϵ>0, we estimate robustness by calculating perturbations with (projected) gradient
descent (PGD). We use 10 iterations of PGD with step size α = ϵ. The initialization of the networks is
5
scaled down by a factor of 10−2 to promote feature learning and faster margin maximization. We use the
exponential loss.
Convolutional Neural Networks We use a standard architecture from (Madry et al., 2018) consisting
of convolutional, max-pooling and fully connected layers. The fully connected layers contain biases, so the
networkisnothomogeneous. WestartfromPyTorch’sdefaultinitialization. Weusedaconstantlearningrate
equal to 0.1 for GD and equal to 0.0001 for SD (SD would diverge during training with larger learning rates
thatwetried). InFigure3(right)inthemaintext, wereportthedifferencebetweenaccuraciesobtainedafter
convergence of train error to 0. In order to standardize the evaluation of the two algorithms, the reporting
time corresponds to the first epoch hitting a certain train loss threshold, i.e. 10−3. For more challenging
training regimes (i.e. large ϵ), this threshold was set larger (10−1), as we found it difficult to optimize to very
small train loss values. In the experiments with CNNs, we used the cross entropy loss during training. When
ϵ>0, we use 10 iterations of PGD with step size α= ϵ.
5
30