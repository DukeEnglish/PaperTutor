DVOS: Self-Supervised Dense-Pattern Video Object Segmentation
KeyhanNajafian
DepartmentofComputerScience,UniversityofSaskatchewan
Saskatoon,Saskatchewan,Canada
keyhan.najafian@usask.ca
FarhadMaleki
DepartmentofComputerScience,UniversityofCalgary
Calgary,Alberta,Canada
farhad.maleki1@ucalgary.ca
IanStavness*
DepartmentofComputerScience,UniversityofSaskatchewan
Saskatoon,Saskatchewan,Canada
ian.stavness@usask.ca
LinglingJin*
DepartmentofComputerScience,UniversityofSaskatchewan
Saskatoon,Saskatchewan,Canada
lingling.jin@usask.ca
Abstract
Videoobjectsegmentationapproachesprimarilyrelyonlarge-scalepixel-accuratehuman-annotateddatasetsformodel
development. In Dense Video Object Segmentation (DVOS) scenarios, each video frame encompasses hundreds of small,
dense,andpartiallyoccludedobjects. Accordingly,thelabor-intensivemanualannotationofevenasingleframeoftentakes
hours,whichhindersthedevelopmentofDVOSformanyapplications. Furthermore,invideoswithdensepatterns,following
a large number of objects that move in different directions poses additional challenges. To address these challenges, we
proposed a semi-self-supervised spatiotemporal approach for DVOS utilizing a diffusion-based method through multi-task
learning. Emulatingrealvideos’opticalflowandsimulatingtheirmotion,wedevelopedamethodologytosynthesizecom-
putationally annotated videos that can be used for training DVOS models; The model performance was further improved
by utilizing weakly labeled (computationally generated but imprecise) data. To demonstrate the utility and efficacy of the
proposedapproach,wedevelopedDVOSmodelsforwheatheadsegmentationofhandheldanddrone-capturedvideos,cap-
turingwheatcropsinfieldsofdifferentlocationsacrossvariousgrowthstages,spanningfromheadingtomaturity. Despite
usingonlyafewmanuallyannotatedvideoframes,theproposedapproachyieldedhigh-performingmodels,achievingaDice
scoreof0.82whentestedonadrone-capturedexternaltestset. Whileweshowedtheefficacyoftheproposedapproachfor
wheatheadsegmentation,itsapplicationcanbeextendedtoothercropsorDVOSinotherdomains,suchascrowdanalysis
ormicroscopicimageanalysis.
1.Introduction
Deeplearning(DL)techniquesarewidelyusedforanalyzingvisualdata,suchasimagesandvideos. Developingadvanced
methodsforcomprehendingvisualdataenhancestheefficiencyandspeedofvarioustasksinvolvingvisualdataprocessing
across various computerized tasks [5, 29, 47, 51], specifically video-based models that benefit from videos’ spatial and
temporalformsofinformation. Videoobjectsegmentation(VOS)isacriticalcomputervisiontaskthattacklesthechallenge
4202
nuJ
7
]VC.sc[
1v13150.6042:viXraofautomaticallyextractingandpreciselyoutliningobjectsofinterestatthepixellevelacrossconsecutiveframesinavideo
sequence. InVOS,theassociationbetweenpixelsandobjectscanchangeovertimeastheobjectorcameramoves. Videos,
in contrast to static images, encapsulate the dynamic scope of motion and interaction unfold over time. This temporal
informationcanbeutilizedforDLanalysis.Nevertheless,videosconstantlyexhibithighlevelsofnoise,influencedbynatural
factorsinuncontrollableenvironments,suchascameramovementandspecifications,wind,andsuboptimalilluminationsuch
asbrightsunlight. Videosinagriculturecontextsoftenrepresentthesecharacteristics.
Developing high-performing DL models, either for image- or vide-based models, is not a trivial task and faces several
challenges,suchasdataannotationbottlenecks[16,29,46]anddesigningorchoosingsuitableDLarchitectures,considering
theavailabledataandcomputationalresourcesaswellasthecomplexityoftaskathand[22,43,51].Mosteffortsonutilizing
DL for visual data processing have been focused on supervised learning approaches, where large-scale annotated datasets
areneededfordevelopingthesemodels. Theresource-intensivenatureofdataannotationhashinderedthefullutilizationof
DL-basedimageprocessingapproachesformanytasksthatneedpixel-levelannotations. Thisissueismorepronouncedfor
VOStasks. Torepresentcontinuousandsmoothmotion, asinglevideofilemaycontainthousandsofvideoframes, where
eachvideoframeisasingleimage [21]. Assuch,manualannotationofvideosisahighlylaboriousandexpensiveprocess,
rendering the use of supervised learning approaches impractical for many tasks requiring object-level and pixel-level data
annotation[32,36]. Annotatingvideosbecomeevenmoreresource-intensivewhentheyinvolvealargenumberofobjects,a
commonscenarioisprecisionagriculture[28,29],wheretheagriculturalfieldimagesusuallycontainmanydenselylocated
plants,resultinginalargenumberofobjectsofinterest—suchasleaves,stems,flowers,andspikes—ineachimage.
Despitethesubstantialchallengesindevelopinglarge-scaleannotateddatasets,particularlyforVOS,videosaregenerally
consideredmoresuitableforsegmentation,comparedtostaticimageframes. Videoscanhelpidentifypartialocclusions,as
analysisofvideoframesindependentlypresentstransientartifacts.Theseartifactscouldbeovercomebyanalyzingasequence
offrames. Furthermore,analyzingvideoframesindependentlyresultsininformationlossregardingobjects’movementand
deformation.Independentsingle-frameanalysisalsofailstodealwithvisualartifactssuchasbackgroundclutterandtransient
objects, which complicates the segmentation of an object within consecutive frames. For instance, the shadow of moving
wheatheadsinwindyenvironmentsonthegroundcancauseaone-frametransientobjectsartifactthatadverselyaffectsthe
accuracy of the single-frame object segmentation models. The temporal information within videos can serve as a crucial
differentiator, enabling video-based models to accurately follow object movement and deformations in the presence of the
artifacts [20, 30]. This capability is indispensable for tasks involving the segmentation of objects undergoing dynamic
appearancechanges. Examplesincludecropsswayinginthewind,movinganimals,andcrowdanalysis.
Whilethestate-of-the-artVOSapproachescommonlyopttocomputetemporalmatchingintheformofopticalflowand
dense trajectories [5, 38, 55], others prefer the parallel strategy that process frames independently [25, 33]. These studies
relyontheavailabilityoflarge-scaleannotateddatasets,i.e.,thepixel-levelannotationforeachtargetobjectineveryvideo
frame, to serve as the primary input for training DL models [5, 18, 38, 55]. Also, It has been reported that for general-
purpose tasks involving one or two large target objects, these models often take shortcuts by predicting subsequent masks
solelybasedontheprovidedprecedingframe’smask,therebybypassingtherichinformationofferedbytheprecedingframes
themselves[55]. Thisoversightledtoshortcomingsinthepredictedsegmentationmaps,stemmingfromobscuredobjectsin
earlierframesandmissedobjectswithinthemostrecentmask[25].
Videodata,usingeitherspatialortemporalinformation,hasfoundwidespreadapplicationsinprecisionagriculture. Cam-
pos et al. [4] employed conventional image processing methods for detecting both static and dynamic obstacles. Through
obstaclesegmentationanddetectionoperations,theyanalyzedspatiotemporalinformationextractedfromvideoscapturedby
camerasmountedonmobilevehiclesinagriculturalenvironments.
The study of image-based dense object segmentation has been explored within the agricultural domain, ranging from
smaller-scaleinvestigations[1,9,45]tolarger-scaledataanalyses,forexamplesstudiesonwheatheaddetection[3,10,14],
counting [44], and segmentation [24, 35, 41, 42]. Sabzi et al. [40] employed traditional image processing techniques to
segmentagriculturalvideoframescharacterizedbycomplexanddensepatterns. Theirproposedapproachinvolvedselecting
suitable color spaces, texture features, intensity transformation methods, morphological operators, and color thresholding.
Thevideoframeswerecapturedusingahandheldcamera,simulatingnaturalpictorialposes,acrossadiverserangeofenvi-
ronmentalconditions,includingvaryinglightintensities,distancesfromsubjects,andcomplexbackgroundsceneries. Ariza-
Sentisetal.[2]utilizedphenotypingtechniquestoassessthephysicalcharacteristics,includingsize,shape,andquantity,of
grape bunches and berries. This involved employing multi-object tracking and instance segmentation (spatial embedding)
methodstodeterminetheattributesofindividualwhitegrapebunchesandberriesfromRGBvideoscapturedbyunmanned
aerialvehiclesflyingoveracommercialvineyarddenselycoveredwithleaves.
Gibbs et al. [11] aimed to create a generalizable feature detection method combined with a tracking algorithm to en-hancefeaturedetectionandenablethedeterminationofplantmovementtraits. Specifically,theyproposedadetection-based
tracking approach to analyze the movement patterns of wheat ears in field-grown plants in a supervised manner, using the
YOLOv3[37]networkforboundingboxregressionon290human-annotatedhigh-resolutionimagesofsize3456×2304.
Duringinference,theyusedaMultipleObjectTrackingalgorithmtoreconstructthetrajectoriesofindividualwheatearinfor-
mation,invideoframesofwheatplantsmovinginthewind,basedonthedetectedboundingboxesbythetrainedYOLOv3
model.
Generalvideoobjectsegmentationmethodshavebeeninvestigatedbyanalyzingthespatialandtemporalcharacteristics
inherenttovideodata.Someapproachesutilizetemporalalignmentmethodologiessuchasopticalflowanddensetrajectories
toensuretemporalcoherence[5,38,55]. Conversely,alternativeparadigmsembraceaparallelprocessingapproach,where
eachframeistreatedindependently[25,33]. However,certainstudiessuggestleveragingsolelytemporalsignalsforobject
segmentation. Forinstance,arecentwork[50]utilizedopticalflowextractedfromreferenceframestotracethemainobject
inthequeryframe,employingastraightforwardtransformer-basedarchitecture.
Recent research efforts have concentrated on semi-supervised and self-supervised methodologies for wheat head detec-
tion[28]andsegmentation[27,29],underscoringtheinherentchallengesposedbyagriculturaldatacharacterizedbydense
patterns. GiventhechallengesoutlinedinthedevelopmentofVOSmodels,weextendedtheimage-basedworkbyNajafian
etal.[29]andinitiatedtheDense-patternVideoObjectSegmentation(DVOS)byproposingasemi-self-supervisedlearning
approachforpredictingqueryimages/masks, utilizingtheprecedingvideoframeswithoutanyannotation. Self-supervised
learning methodologies such as pretext tasks [49], Generative adversarial networks (GANs) [12], contrastive learning [7],
Generative AI [15], and redundancy-reduction-based [54] do not require any data annotation while semi-supervised learn-
ingrequiresamixtureofannotatedandunannotateddatatodevelopdeeplearningmodels,therebysomewhatreducingthe
needforcomplexandtime-consumingdataannotationprocesses[53]. Thesemethodologieshaveemergedtomitigatedata
bottlenecksassociatedwithdevelopingdeeplearningmodels,particularlyaddressingscenarioswherethemanualannotation
ofdataprovesimpractical. Weassessedourproposedapproachtothechallengingtaskofwheatheadsegmentationwithin
videoscapturingdenseandrepetitivepatternsofwheatheadsacrossvariousgrowthstagesofphenotypicallyrichanddiverse
domains.
Employing only a few pixel-level human-annotated frames, we synthesized a large-scale dataset of computationally-
annotatedvideoswithdensepatterns,therebyavoidingthetediousmanualannotationofsuchvideoswithsmall,overlapping,
and intricate patterns. We also introduced a UNet-based [39] architecture designed for multi-task training of DVOS. Our
model development process consisted of two key stages: We constructed the foundational model solely using synthesized
videos. Wethenfine-tunedtheresultingmodelbyleveragingweakly-labeledvideos,enabledbyanimage-basedmodel[29].
Toshowtheutilityoftheproposedapproach, weconductedtheperformanceevaluationonthreedistincttestsets: (1)a
small-scalesingle-domainpixel-annotatedvideodataset,(2)adiverselarge-scalewheatfieldvideodataset,whichiscaptured
byhandheldcamerasandlabeledinasemi-automatedmanner,and(3)adrone-capturedmanuallyannotatedwheatfieldvideo
dataset.
2.Method
This section describes the data used in this study. It also provides a detailed description of the diffusion-based DVOS
architecturerepresentingamulti-taskinglearningapproachforthesemi-self-supervisedsegmentationofvideoswithdense
patterns.
2.1.Data
InthecontextofDVOS,anannotatedvideoclipV = {(x ,y )}T , isdefinedasaseriesofT consecutiveimageframes,
t t t=1
where x represents the tth video frame and y represents the annotation (mask) for x . Give the video V, the annotated
t t t
query frame Q = (x ,y ) is contextualized by a sequence of reference, i.e., video frames proceeding x , denoted as R =
t t t
{x |t−τ ≤r ≤t−1},whereτ isthecontextwindowrepresentingtemporalinformation. VideoframesinRandx are
r t
usedasinputforthemodel,andy isusedasthedesiredoutput,i.e.,thegroundtruth.
t
2.1.1 Weakly-labeledDatasets
Wecollectedatop-viewvideodatasetofwheatfields,denotedasV = {V ,V ,··· ,V },whereV ∈ Vrepresentsasingle
1 2 n i
videoclipconsistingofaseriesofvideoframes. Thesevideoclipscapturevariousphenotypicfeatures—suchasspikeand
spikeletcolor,shape,andarrangement;leafsizeandcolor;growthstage;andawntypeandlength. Wecapturedthevideos
using handheld cameras with 12 Megapixels resolution, recorded at various altitudes, specifically 1.0, 1.5, and 2.0 meters.These recordings provide a comprehensive perspective, focusing on the wheat heads within real agricultural landscapes.
Figure1illustratesimageframesfromdifferentvideosrecordedatvariousaltitudesandgrowthstages.
Figure1.Depictionofdiverseimageframescapturingvideosfilmedatvariousaltitudesviahandheldcameras.
Tomaintainthecoherenceandconsistencyofvisualelements,content,andobjectcharacteristicswithineachvideoandto
reducethecomputationcomplexity,weresizedeachframefromitsoriginalsizeof2160×3840suchthatitsheightisscaled
to 1024 pixels while maintaining the aspect ratio, then center-crop it into the spatial size of 1024×1024. Segmentation
label for each frame was generated using the model detailed in [29]. Hereafter, we refer to this weakly labeled dataset as
W=(cid:110)(cid:8)
(x ,y )|1≤t≤T
(cid:9)
|1≤i≤n;V
∈V(cid:111)
,whereT representthenumberofvideoframesinvideoclipV .
t t i i i i
Agroup-wisedatasplitwasconductedonWtopartitionitintotraining(W ), validation(W ), andtest(W )
train valid test
sets. Thegroup-wisedatasplitwasperformedtoensuredatapointsfromonevideoclipareeitherintraining,validation,or
testset. DetailsofW ,W ,andW arepresentedinTable1.
train valid test
Dataset Notation #ofVideos #ofFrames #ofAnnotatedFrames
Training W 8 37,613 37,613
train
Validation W 4 14,182 14,182
valid
Testing W 5 34,777 34,777
test
Table1.Summaryofweaklylabeleddata.
2.1.2 SyntheticDataset
Inthisresearch,weextendedourpriorworkonimagesynthesization[28,29]tosimulatecomputationallyannotatedvideo
clips, alleviating the challenge of manually annotating video clips, which is a laborious and resource-intensive process. In
thefollowing,weprovideadetaileddescriptionofthemethodologyusedforsynthesizingthesevideoclips.
Weusedagroupofbackgroundvideos(B),whicharefieldswithoutwheatcropsexhibitingvariousvegetationtypesand
environmental conditions. Figure 2 illustrates the process for extraction of the background video frames that will be used
for synthesizing computationally annotated video clips. This is achieved by, first, randomly choosing video B from the
i
backgroundvideosB = {B ,B ,···B }. Then,τ consequentframesfromB areselectedtoformasetB′ = {b | k ≤
1 2 n i i,k t
t≤k+τ and1≤k ≤T −τ}. Next,arandomregionofsize1024×1024ischosen,andthecropdefinedbythisregionis
i
extractedforallframesinB′ ,resultinginasetofτ consecutiveframesofsize1024×1024,denotedasC′ .
i,k i,kFigure2.TheprocedureforextractingvideoframesfrombackgroundvideosB ∈B.
i
Additionally,weextractedwheatheadsfromasmallnumberofmanuallyannotatedframesdepictingbothearliergrowth
stagesingreenshadesandmature,harvestablestagesinyellowcoloration,resultinginasetofrealwheatheads(H)consisting
ofyellow(mature)andgreen(mid-season)wheatheads.Notethat,forwheatheadextraction,wechosetheframesfromthree
distinct videos that have no intersection with the V dataset. We also used extracted wheat heads in H as cookie-cutters to
extractnon-wheatheadregions(fakewheatheads)fromtheoriginalframes. ThissetisdenotedasH. Thepresenceofthe
fake wheat head prevents models from relying only on changes in intensity values and contrast for segmenting simulated
videoclips.
Figure3illustratestheprocessforsynthesizingamanuallyannotatedvideoclip.ForsetC′ ofτ consecutivebackground
i,k
frames, a random number of fake and real wheat heads are chosen randomly from H and H, respectively. The fake wheat
headsareoverlaidonthefirstframeofC′ ,andthenrealwheatheadsareoverlaidontheframe.
i,k
To simulate the movement and deformation of the crop, which naturally happens due to wind, we apply a sequence of
spatial- and pixel-level transformations to each wheat head before overlying them on consecutive frames in C′ . These
i,k
sequencesoftransformationsareautomaticallygeneratedforeachwheatheadtoallowforconsistencyinmovementwhile
showingvariousdegreesofdeformations. Morespecifically,wedefinetwotypesofmovementforeachwheatheadobject:
object-levelandframe-levelmovement. Inobject-levelmovement,weonlyupdatedthepositionanddirectionofeachobject
individually. However, in the frame-level motion, all the objects’ positions will be adjusted with regard to the predefined
motionbehaviorintheframelevel.
This process results in synthesizing frames and their corresponding masks as we apply the special transformations and
deformationstoboththeframeandmask,providingaconsistentannotationfortheresultingvideoclip. Tocreatethemask
corresponding to each synthesized video frame, a 1024×1024 zero-value matrix is allocated for each of the τ frames in
C′ . Wekeeptrackofthepositionofeachrealwheatheadwhenoverlaidonthebackgroundimageanditsmovementand
i,k
deformationtoconvertthecorrespondingregiononthemaskto1. Notethatthefakewheatheadsareignored.
Table2.SummaryofthesyntheticvideosandtheirframesdistributionsintheS dataset.
train
Dataset Subset #ofBackgroundVideos #ofHeads #ofSynthesizedVideos #ofFrames
GreenShaded 13 101 260 15600
S
train YellowShaded 15 251 600 36000
Figure 4 illustrates real and synthetic video frames, where their respective masks are annotated with pink color. This
visualizationhighlightsthesystematicdifferencebetweenrealandsimulatedvideoclips, aphenomenonknownasdomain
shiftinthemachinelearningfield,andoftenleadstoalackofgeneralizability,i.e.,themodelunderperformswhenappliedto
realunseendatasets. Therestofthissectionfocusesonthedataandmethodologyusedforbridgingthisgapandalleviating
domainshift.
2.2.AdditionalDatasets
Wealsousedavalidationset,∆,consistingof300samples,whichincludedfiveconsecutivevideoframeswherethefirstfour
framesareunannotatedandthelastoneisannotated. Theannotationswereconductedinasemi-automatedmanner,where
the predictions made by our prior model from [29] were manually refined as needed. Furthermore, we used a manually
annotatedset,Ψ,consistingof100samplesfortesting. EachsampleinΨincludesacombinationoffiveconsecutiveimageFigure3. Thisdiagramillustratestheprocessofsynthesizingvideoswithdimensionsof1024×1024andalengthofT. Wegeneratea
synthesizedvideoV′byinitializingarandomizedselectionofrealandfakeheads,eachuniquelydefinedbyaugmentationandpositioning
parametersdrawnfromapoolofrealandfakeobjects(HandH)overlaidonthefirstframeofT randomlyselectedframesfrom{c }i+T.
t t=i
SubsequentframesofV′aresimulatedbasedonthepositionsofwheatheadsintheprecedingframe,utilizingobject-levelandframe-level
motioninformation. Objectsthatareremovedaresubsequentlyrestoredbyincorporatingadditionalrealheadsintothechosenobjectset
byoverlayingonthecurrentframebeforeproceedingtothenextframe. Byapplyingthesameobject-andframe-levelmotionsonthe
segmentationcounters,theframemasksarealsogeneratedsimultaneously.
frames,wherethefirstfourareunannotated,andthefinaloneismanuallyannotated.
Wealsousedanexternaltestset,D,of48samples,eachcontainingfiveframes. Thefirstfourframeswereunannotated,
andthefinalframewasmanuallyannotated.
These samples were extracted from three distinctly diverse drone-captured videos of wheat fields to evaluate the model
performanceonthepixel-accurateannotatedimages. Eachdronevideocontributed16samplestoD. Thedronevideoswere
capturedusingaDJIMini3ProDrone,fromvariousaltitudes. Figure5illustratesafewimageframesfromD.
2.3.ModelArchitecture
WedesignedaUNet-style[39]architectureforDVOSbyemployingamulti-tasktrainingparadigmconsistingofasegmen-
tationtaskandanimagereconstructiontask. Thearchitectureisdesignedtoprocessasequenceofτ consecutivereference
frames and predict the following frame and its mask. Hereafter, we refer to the τ reference frames as R and the query
frame as Q. Figure 6 illustrates the model architecture designed to perform both reconstruction and segmentation tasks
simultaneously. ThisallowsthemodeltoutilizebothspatialandtemporalinformationforDVOS.
The proposed architecture processes the frames in R individually using a 2D convolutional neural network. The keyFigure4.Visualizationofsyntheticversusrealvideos,lefttoright,withthetimeintervalof4seconds.Thesyntheticvideospresentaseries
ofcolor-augmentedfakewheatheadsandmaskedrealheads,isolatedwithoutwheatcanopyandstems,overlaidonasequenceofuniform,
flatbackgroundframes. Theseframesillustraterandomhead-levelmovementsandsynchronizedframe-leveldynamics. Incontrast,the
realvideosdepictactualwheatfields,capturingnormalmotionindense-patternwheatspikesattachedtotheirstemsandsurroundedby
wheatcanopyincludingleavesandawns.
Figure5.Depictionofhuman-annotateddrone-capturedvideoframes,overlaidbytheirmasksinpink.
componentsoftheproposedarchitecturesareaninitialconvolutionblock,acontractionpath,skipattentions,skipdiffusion,
anexpansionpath,anddecoderheads. Inthefollowing,wedescribeeachcomponentindetail.
Theinitialconvolutionblockcomprisesa3×3convolutionlayer,withstride1andpadding1,followedbytwoResNet
blocks(Figure7). Eachresidualblockconsistsoftwoconsequentblocks. EachblockcomprisesaGroupNormalization[48]
layer of size 8, followed by Swish activation [34], and then a 3 × 3 convolution with padding 1. Note that this initial
convolutionblockdoesnotchangethespatialdimensionoftheinputimages.
The Contracting Path of the network is constructed of contractive blocks. Each contractive block is comprised of two
residual blocks, followed by a 3 convolution with the stride of 2 and padding of 1 for spatial downsampling. The output
of each contractive block undergoes a Spatiotemporal Attention Block comprising a Spatiotemporal Attention Module, a
channelreductionlayer,followedbySkipDiffusionandDropoutlayers.
TheSpatiotemporalAttentionModule, inspiredby[17], aimsatattendingtothemostinformativesignalsfromtheτ =
|R|×|F|concatenatedfeaturemapsandgeneratesonerepresentativefeaturemap.Figure8illustratesthismodule.ItconsistsFigure6.AnoverviewoftheproposedUNet-based[39]architectureforDVOSviaprediction.The|R|featuremapsetsareconcatenated
andfedintothespatiotemporalattentionmoduleateachcontractinglevel.Followingthis,channelreductionisachievedusinga3Dpooling
operation,noiseisaddedthroughtheforwarddiffusionprocess,andchannel-wisedropoutisappliedbeforeutilizingtheunifiedfeature
mapsintheexpansionpath.Featuremapsderivedfromthelatentspacearesubjecttothesameprocess,exceptfordiffusionanddropout.
Figure7.Anoverviewoftheresidualbuildingblock,thebasecomponentoftheproposedarchitecture.
oftwospatialandtemporalattentionstreamscombinedtocapturethespatialandtemporalfeaturemaps’importance. The
spatialattentionstreamconsistsoftwoblocks,eachcomprisingadepth-wiseseparableconvolution[13],followedbyGroup
Normalization [19], and finally a Swish nonlinearity [34]. The temporal attention stream combines the τ feature maps
using an adaptive average pooling across feature maps followed by two linear layers, wrapping a Swish non-linearity, and
completedbyaSigmoidfunction.
The whole Spatiotemporal Attention Module is formulated in details by Formula 1, where Group Normalization is rep-
resented by GN, Depth-wise Seperable Convolution by DSC, Linear layers by FC, Adaptive Average Polling layer byAvgPool,FMistheinputfeaturemaps,WFMpresentsweightedfeaturemaps,andthedlnotationindicatesthedilation
size.
S =Swish(GN(DSCdl=·(·)))
a
T =FC(Swish(FC(AvgPool(·))))
a
S =Sigmoid(Sdl=2(Sdl=1(FM ))) (1)
a a l
T =Sigmoid(T (S))
a
WFM =(T ⊙S)⊙input
l
Figure8.Thisfigureillustratestheproposedspatiotemporalattentionmodule,whichincorporatestwoprocessingstreamsforinputfeature
maps: (1) Spatial Attention Stream (Top) captures and weights informative spatial regions within individual reference frames. Group
normalizationisemployedtohighlightthespatialimportanceoffeaturemapscorrespondingtoeachinputframe. Additionally, depth-
wiseseparableconvolutionsareusedfortheefficientprocessingoflarge-scalefeaturemaps;(2)theTemporalAttentionStream(Bottom)
focusesoncapturingtemporaldynamicsanddependencies. Itweightseachtimestepacrossfeaturemapsextractedfromallconsecutive
frames.
GiventheconcatenatedfeaturemapsFM ofsize|R|×|F|,theSpatiotemporalAttentionModulepreservestheinputted
l
featuremapdimensions,b×c×h×w. Inthisnotation,F representsthefeaturesextractedforeachinputreferenceframein
Rindownsamplinglevell. However,furtherprocessingoftheattentionoutput(WFM )isrequiredtoachievethedesired
l
output,addressingbothdimensionalityandfeaturequalityconcerns,beforeenteringtheExpansionpath. Insteadofemploy-
ing feature dimension reduction using the DSC layers, we opt to retain the entire input dimensionality and subsequently
applya3DAveragePoolinglayerforthispurpose.Thisprocessintegratesspatiotemporalfeaturemapsandgeneratesfeature
mapsofupsamplinglevelchannelsize(|F|).Topreventthemodelfromexcessivelyrelyingonskipconnectionsasshortcuts,
weintroducerandomnoisetothefeaturemapsusingadiffusion-basednoisescheduler,asdetailedasfollows. Additionally,
we incorporate a random 2D Dropout layer with a specified probability to further increase randomization and mitigate the
modelmaintainingtheskip-connectionshortcutsbyrandomlyremovingasubsetof2Dfeaturemaps.
Considering the unique design of the UNet architecture [39], skip connections enable the network to retain and reuse
low-andhigh-levelspatialinformationfromearlierlayersduringdecoding. ThisfeatureempowersUNetmodelsforprecise
objectlocalizationbyintegratinghigh-resolutionfeaturemapsfromthedownscalingpathwithcoarsesemanticinformation
from the upscaling path, leading to promising segmentation performance on clear input images. However, UNet models
encounterchallengeswhenreconstructingmaskedornoisyinputimages,orwhenpredictingactualimagesorvideoframes,
astheyfailtooperaterichsemanticinformationobtainedfromthelatentspace[31]. Thislimitationarisesfromthemodel’s
tendencytotakeshortcutsbytransferringknowledgesolelythroughhigh-resolutionfeaturemaps,therebylimitingitsability
toeffectivelyutilizetheavailablemid-levelinformation,ontopoftheotherfeatures,forreconstructingimagesproperly.Addressing this, we integrated a weighted version of the forward diffusion process into each of the skip connections
aimed at balancing various-level feature contribution, thus enhancing the model’s reconstruction performance, improving
themodel’srepresentationlearningcapabilities, stabilizingthemodel’strainingwhenexposedtosyntheticdata, aswellas
introducinganimplicitdomainadaptationmechanismbyallowingthemodeltolearnlow-levelandmid-leveldatafeatures
forreconstruction,ratherthansolelyrelyingonlow-levelandhigh-levelclassification-relatedfeaturesforpixel-levelclassifi-
cation. Thisapproachdivergesfromgenerativediffusionmodels,whichprimarilyfocusedonnoiseprediction[15,52]. The
integrationfollowstheMarkovprocessproposedin [15].
√ √
x = α¯x + 1−α¯ ϵ (2)
t t 0 t
√
whereq issampledfromthediffusionkernel,specifiedasq(x |x ) =N (x ; α¯ x ,(1−α¯ )I). Here,ϵ ∼ N (0,I),the
t t 0 t t 0 t
scalarα¯
=(cid:81)t
(1−β ),andβ servesasthevariancescheduler,formulatedtoensurethatα¯ →0.
t i=1 i t T
Commencingfromthelatentspace,weinitiallyrefrainfromapplyingnoisetothefeaturemaps. However,asweadvance
alongtheupsamplingpathofthenetwork,noiseaugmentationintensifiesateachlevel,correspondingtohigherresolutions;
hence,noiseincreasesprogressively. Thenoiselevelisregulatedbyselectingtimestepstfromthebetaprobabilitydensity
function. Thebetadistributionisacontinuousprobabilitydistributiondefinedontheinterval[0,1]. Itisparameterizedby
twopositiveshapeparameters,denotedbyαandβ.
1
f(x,α,β)= xα−1(1−x)β−1 (3)
B(α,β)
where x ∈ [0,1], and B(α,β) is the beta function, which is a normalization constant that ensures the total area under the
curveequals1. Thebetafunctionisalsodefinedasfollows:
(cid:90) 1
B(α,β)= tα−1(1−t)β−1dt (4)
0
Moreover, the mean and variance of the beta distribution are calculated as follows, which perfectly control the randomly
generatedtimesteps.
α
µ= (5)
α+β
αβ
σ2 = (6)
(α+β)2(α+β+1)
Weestablishedaschedulertodeterminethevaluesofαandβforassigningdistinctbetadistributionfunctionstoeachlevel,
therebyselectingdiffusiontimesteps. Thisschedulingmechanismisgovernedbythefollowingequations:
α=α +l (7)
0
β =β −(β ∗l) (8)
0 c
where α , β and β represent initial and coefficient hyperparameters, and l is the upsampling level index. which starts
0 0 c
at the last skip connection, The index l begins at the last skip connection and progresses towards the first skip connection,
signifyingthefinalupsamplingstep. Figure9depictstheshapesofbetadistributionsgeneratedusingourscheduleracross
varyingvaluesofαandβ.
Thedecoderarchitecturewithinourmodelreplicatesthedownsamplingpath, maintainingconsistencyinthenumberof
residual locks at each level. However, it employs a nearest-neighbor operation with a scaling factor of two, followed by
a single 3×3 convolution operation, serving as the upsampling layer. Moreover, we leverage concatenation to integrate
the skip connection and lower-resolution feature maps at each level. Functioning as a shared decoder for segmentation
and reconstruction tasks, the UpScaling Path incorporates high-level semantic features within a localized context, while
preserving spatial resolution from the latent space. This architecture adapts two decoding heads, each incorporating three
residual blocks and one final 3×3 convolution layer, ending in three-channel outputs for reconstruction and one-channel
onesforsegmentation.Figure9.VisualizationofBetaDistributionsGeneratedbytheScheduler:Thisfiguredisplaystheshapesofbetadistributionsgeneratedby
ourschedulerforvariouscombinationsofαandβvalues.Eachcurvecorrespondstoadistinctbetadistribution,illustratingtheimpactof
parametervariationonthedistribution’sshape.Thecurvesarearrangedfromlefttorightforamodelwithsixskipconnections,progressing
bottom-upfromthelatentspacetothehighestresolutionoftheupsamplingpath.
2.4.ModelDevelopmentandTraining
Weassessedtheeffectivenessandperformanceofourdevisedarchitectureonvideosfromawidervarietyofdomains. Yet,
conventional UNet-style models demonstrate easily attainable promising performance in reconstructing clear inputs, with
limitationsconcerningtheirutilityindomainadaptation,imagesegmentation,andover-fittingmitigation. Consequently,we
introduce a strategy involving diffusing input images in a patching style. This involves partitioning each whole reference
frameinthemini-batchsubjectedtodiffusion(withP ratesetto0.5)usingtheemployedforwarddiffusionprocess[15],
d
with random time steps limited to the range of 0 to 1000. Figure 10 displays a representative example of an input image
subjectedtothediffusionprocess.
Figure10.Adepictionofanimageframealongsideitsdiffusedversion,producedthroughtheforwarddiffusionprocess.
In addition, we involved a pixel-level (color) argumentation transformation, only for reference frames, with light color-
altering and blur transformations before applying the group-wise spatial transformations—including small-angle rotation,randomcropping,andnormalization—appliedtoreferenceframesandqueryframe/masktogether.
Weperformedmodeltrainingintwophases. WebeganthemodeldevelopmentbytrainingtheM modelfromscratchon
s
theS trainingdatasets,with∆servingasthevalidationset. Subsequently,inthesecondphaseoftraining,wefine-tune
train
theM modelontheweakly-labeledW andW datasets,yieldingourfinalmodeldenotedasM .
s train valid w
In this study, we utilized two commonly used segmentation metrics, Dice and Intersection over Union (IoU) scores for
segmentationevaluation. Additionally,weutilizedMeanSquaredError(MSEorL2)andStructuralSimilarityIndex(SSIM)
lossestogetherforreconstruction(L inFigure6),andBinaryCrossEntropy(BCE)andDicelosses(L inFigure6)jointly
r s
forsegmentation. Duetothesubstantialdatasetsizes,wetrainedeachmodelfor15epochsandselectedthemodelwiththe
highestvalidationDicescoreasthebest-performingmodelforfurtherevaluation. WeexploitedtheAdamWoptimizer[23]
withalearningrateof1e−4,weightdecayof1e−5inthefirsttrainingphase,andnoweightdecayinthesecondphasefor
M .
w
We employed randomly sized squared-shape crops ranging between 400 to 750 from the original 1024×1024 image
frames, resizing them to the training size of 256 for training purposes. However, during evaluation, we replaced the ran-
domcroppingoperationswithasinglecentercropoperationofsize512toensureconsistencyacrossallepochsandmodel
developmentphasesforvalidationandtestsets.
3.Results
Table 3 demonstrates the performance of developed models across our test sets. Model M was exclusively trained on the
s
computationally annotated synthetic dataset, whereas the model M was trained on real but weakly-labeled wheat field
w
videos. Despite being trained solely with the synthetic dataset, M demonstrates reasonably high performance across all
s
threedatasets. Furthermore,modelM ,whichwastrainedonpreciselyannotateddataforimage/maskprediction,exhibits
s
superior performance on the manually-labeled datasets (Ψ and D) compared to the pseudo-labeled dataset (W ). This
test
potentially reflects the true pixel-level segmentation efficacy of the model, as indicated by the pixel-accurate annotation of
thesedatasets.
Table3.QuantitativeresultsofthedevelopedDVOSmodels(M ,M )ondiversetestsets.
s w
Model TrainedOn TestSet IoU Dice
Ψ 0.389 0.543
M s None W test 0.220 0.337
D 0.417 0.578
Ψ 0.546 0.699
M w M s W test 0.609 0.736
D 0.700 0.821
Additionally,thesefindingshighlighttherelativelycloseperformanceofmodelM acrossbothhuman-annotatedtestsets,
s
underscoringthepracticalutilityofsyntheticdatainvideomodeldevelopment. Notably,oursynthesisprocessutilizedonly
a limited selection of wheat heads from a few samples spanning two domains. Despite this constraint, the model demon-
stratesrelativelysimilarperformanceacrossthephenotypicallydiversedatasetW ,whichconsistsofvideoscapturedfrom
test
varyingaltitudes,aswellasdifferentweatherconditionsandlightlevels,affirmingitshighdegreeofgeneralizability.
Ourfine-tunedmodel,M ,exhibitsnotablyimprovedperformancewhentrainedonthelarge-scaleanddiverseweakly-
w
labeled dataset, W , achieving a substantial performance gain of over 15.6% on Ψ dataset, despite lacking training
train
samplesfromthisdomain,andnearly39.9%onW . Furthermore,themodel’saccuracyonthemanuallylabeleddataset
test
underscoresitsprecisioninpixel-accuratesegmentationofwheatheadobjects. Thismodelalsodemonstratessignificantly
enhancedperformancewhenevaluatedonD,achievingaDicescoreof0.821. Thisrepresentsanimprovementofover26%
comparedtomodelM .
s
Figure11,visually,illustratesthepredictionperformanceofmodelM onrandomlyselectedsamplesfromthetestsets.
w
Agridformatisusedtoarrangefourrandomlyselectedsamplesineachblock. Thefirstandthirdrowsshowtheprediction
performance, the second and third columns, of the model while the first column represents the manually annotated labels.
However,thefirstcolumnofthesecondrowshowstheweak(model-generated)masksoverlaidontheimages. Notethat,weFigure 11. Performance visualization of the M model across different test sets. The first two columns depict masks overlaid on the
w
corresponding images. Each block forms 4 different samples, which are visualized in the same grid cell across Ground Truth, Mask
Prediction,andImagePredictioncolumns.
solelyusedtheframepredictionpowerofthemodelformodeltraining;however,themodelperformswellinpredictingthe
queryframeintheexistenceofhierarchicallyaddedskip-levelnoise.
We also conducted individual evaluations of the model M performance on each video V ∈ W , each capturing a
w test
diverse exhibition of wheat plants, collectively, from diverse phenotypes, often from slightly variant altitudes. While the
modeldemonstratesconsistentperformanceacrossmostvideos,Video1provesmorechallengingtosegmentincomparison
to the others (see Figure 12). This difficulty is evident in the performance of both models (M , M ). Particularly, the
s w
presenceoflow-qualityvideoleadstoimpairedweakly-labeledgroundtruth,therebydiminishingthequantitativescoresof
our models. Despite this, visual inspection reveals the proper model segmentation of Video 1, capturing the wheat heads,
even those of the lowest quality. However, the model segments tighter regions covering each wheat head compared to the
groundtruth,whichincludesabroaderarea—wrappingeachwheathead—andincludingmanyfalsepositives.
4.Discussion
In this study, we proposed a semi-self-supervised learning methodology to develop a novel model architecture designed
forsegmentingvideoswithdensepatterns. Ourapproachissemi-supervisedasitinvolvestheinitialtrainingofthemodelTable4.Quantitativeresultsofthedevelopedmodels(M ,M ),onindividualvideosfromW .
s w test
Model TrainedOn TestSet IoU Dice
Video1 0.164 0.247
Video2 0.252 0.387
M None Video3 0.234 0.360
s
Video4 0.156 0.260
Video5 0.148 0.226
Video1 0.390 0.548
Video2 0.636 0.763
M M Video3 0.798 0.887
w s
Video4 0.785 0.878
Video5 0.780 0.867
Figure12.ThepredictionperformanceofmodelM ontheVideo1∈W (secondrow)comparedtotheirweakly-labeledgroundtruth
w test
(firstrow).Thesamplesineachcolumnshowthemismatchesbetweenthegroundtruthandpredictions,causingasignificantreductionin
modelperformanceonthisvideocomparedtotheothervideosinthisset.
usingcomputationallyannotatedsynthesizedvideosthatwereexclusivelygeneratedfromwheatheadsextractedfromseven
annotated frames originating from two domains. Additionally, our methodology is self-supervised, as we utilized model-
generatedannotations(weaklabels)forfurthermodeldevelopment. Wealsoincorporatedself-supervisedimageprediction
alongsidemaskpredictionduringmodeltraining.Byutilizingsynthesizedandweaklylabeledvideosformodeldevelopment,
wecircumventedthelabor-intensiveprocessofmanualsegmentationforhigh-frame-ratevideoscapturingdenseandsmall
objects.
We introduced a novel architecture for Dense Video Object Segmentation (DVOS) tasks. This architecture integrates a
hierarchicaldiffusionprocessasitscoreandemploysaspatiotemporalmechanismforobjectsegmentationthroughmulti-task
image/mask prediction. Specifically, we devised a new UNet-style [31, 39] architecture with reduced parameters achievedthroughsharedencodersanddecoders.Thisarchitecturefeaturestwodistinctheadsforframeandmaskprediction,enhancing
efficiencyinutilizingshareddecoderfeaturemapsandrobustnessduringmodeltraining.
In VOS, most studies [5, 6, 8, 25, 26, 38] incorporate the segmentation masks of reference frames as auxiliary inputs
of their network. To be more specific, they employ the manual annotation of the videos’ initial frame, coupled with either
human-ormodel-generatedsegmentation mapsforsubsequentreferenceframes, andthenutilizethem withintheencoder,
decoder,orbothcomponentsofthenetwork,withtheultimateobjectiveofgeneratingthesegmentationmaskcorresponding
toQ. Inthiswork, however, wefedsolelythereferenceimageframestothemodelandemployedonlythequerymaskas
thegroundtruth. Weexclusivelyleverageweakmasks,derivedfromanimage-basedsegmentationmodel[29],asopposed
torelyingonmanualorVOS-modelgeneratedsegmentationlabels.
Wedesignedourmethodologyandthearchitectureconcerningafewmainchallenges:
• Transferring the unprocessed high-level features, preferred for segmentation, through the skip connections [31], UNet-
stylemodels[39]ignoreintegratingrichlatentfeaturemaps,appropriateforreconstruction. Theeffectislesspronounced
in images containing a single large-size object of interest, compared to domains with densely packed, repetitive small
objects. Also, videos, captured in uncontrolled environments using either handheld or unmanned aerial vehicles such
as drones, are often influenced by environmental factors such as wind and sunshine and present a higher level of noise
comparedtoindividuallycapturedimages. Therefore,weintegratedadiffusion-baseddatapreprocessingstrategyaswell
as a hierarchical diffusion process as the core of our proposed architecture. This way we tried to regulate the data flow
within the network, completed by the adoption of a standard forward diffusion process to simulate noisy inputs. By
incorporatingthesemethodologies,weenhancedthemodel’srobustnesstoinputnoise,consequentlyimprovingthequality
ofreconstructedimages(ThirdcolumnofFigure11)andleadingtomoreeffectivemodeltrainingand,finally,segmentation
outcomes.
• WetransformedtheVOStaskintoamulti-tasklearningframeworkforframeandmaskprediction. Throughreconstruction
losses, our objective was to integrate an explicit domain adaptation process into the model development. This process
pushedthemodeltoacquiremiddle-levelfeaturemapsnecessaryforreconstructingtheinputimageframe,thusadapting
tovariousvisualpatterns,enhancedbyinputdataaugmentation,andimprovingthemodel’sgeneralizability. Furthermore,
by training the model on relatively simple synthetic data and employing reconstruction losses, we mitigated the risk of
overfitting,therebyenhancingtheregularizationeffectinthemodeltrainingprocess.
• We chose data synthesis with automatic annotation over the tedious process of manual data annotation. Simulating the
motionandopticalflowcharacteristicsofreal-worldcapturedvideos,wedevisedavideosynthesisstrategytogeneratea
diverseandlarge-scaledatasetofcomputationallyannotatedvideos. Here,weextendedthedatasynthesisproposedin[29]
by adding object-level and frame-level motions to the current frame to generate the subsequent frames. We generated
a consistent number of short video clips from various regions of each of the background videos to construct a well-
balanced large-scale dataset. This dataset holds videos with diverse backgrounds, thereby boosting the generalizability
ofthedevelopedmodel,demonstratedbymodeM intable3.
s
• Additionally, we proceeded to fine-tune the model using real-world examples paired with weak masks. This refinement
processresultedinanotableenhancementinmodelperformance,withimprovementsrangingbetween15%to20%across
varioustestsets. Themulti-tasktrainingstrategy,coupledwithourspecificallydesignedarchitecture,enabledthemodel
tolearnthepreciseboundariesofsmallanddenselypackedobjectsofinterest. Byreconstructingfutureimageframesand
theircorrespondingsegmentationsbasedonprovidedpreviousreferenceframes,themodelachievedgreaterperformance
onourmanuallylabeleddataset(D ).
Υ
Although model M was trained on synthetic data and fine-tuned on the weakly-labeled data (sampled in the first row
w
of Figure 12), the second row of this figure demonstrates its accuracy in properly segmenting wheat heads. This indicates
the robustness of the model’s training process, as it effectively disregards misleading segmentation signals in the weakly
labeleddataandfocusesonmulti-levelandsemanticfeaturesinourdefinedmulti-tasksegmentation/reconstructiontraining
paradigm. In addition, the performance of the models in this video is significantly lower than the other videos, which
encompassclipsofhigherquality. Thisfurtherhighlightsthecriticalimportanceofvideoqualityindeveloping,evaluating,
anddeployingmodelsinreal-worldenvironments.
In this study, our model development process relied exclusively on synthesized data accompanied by computationally
generated annotations. We used only five manually annotated video frames highlighted in yellow from one distribution,
supplementedbytwoadditionalgreen-shadedframesfromanotherdistribution. Wedeclarethataugmentingthedatasetwith
morewheatheadobjectsextractedfromframesofdiversewheatdomainswithhigherresolutionswouldfurtherenhancethe
model’s performance. Additionally, by extracting wheat heads from individually captured images rather than from noisy,
out-of-focus,orblurryvideoframes,wecouldobtainclearerwheatheadobjects,resultinginhigher-qualitysyntheticdata.We adopted a straightforward approach to overlaying wheat heads, prioritizing minimal movement relationships between
heads. Strengthentherelationshipandmoresynchronizedwheatheadmovementscouldpotentiallyimprovetheauthenticity
ofthewheatfieldvideos,andyieldabetterperformance.
In addition, in the second phase of model development, we use authentic but weakly labeled videos for model fine-
tuning. Inotherwords,themodelwasnottrainedusingpixel-levelaccuratehuman-annotateddata. Furthermore,theweakly
labeleddatasetincludedprimarilylongvideoclips,eachrepresentingonlyalimitednumberofphenotypicallydistinguished
domains. Given these constraints, we recommend expanding the scope of our proposed architecture by incorporating a
broadercollectionofshortervideoclipscapturingamorediverserangeofdatadomains.
Inthiswork,weshowedtheutilityofsyntheticvideoclipswithpixel-accurateannotation,alone,formodeldevelopment
(M ). Developing models from scratch using only weakly labeled video frames, even on such an extensive scale, poses a
s
notable challenge in determining and extracting informative patterns or features from the input data, providing misleading
signals.Therefore,usingonlyafewsamplesfromtwodomainsfordatasynthesis,weexperimentedwiththecapabilityofthe
developedmodeltoobtainreasonablyhighperformanceevenwhentrainedonsuchcomputationallyannotatedanduniform
syntheticdata.Webelievethatincreasingthequantityanddiversityofhuman-annotatedimages,usedforforegroundrealand
fakeobjectextraction(H,H),enhancesthediversityofsyntheticdata,thereby,resultsinsignificantimprovementinmodel
performance.
Consideringtheflexibility,scalability,andlearningadaptabilityofourproposedarchitectureandthewell-designeddata
synthesispipeline,wesuggesttheexplorationofthisapproachacrossvariousresearchdomainsbeyondwheatcropDVOS.
Forinstance,applicationsinmedicalimagingandotherdatamodalitieswithinprecisionagricultureholdpromiseforleverag-
ingthecapabilitiesofourarchitecture.Furthermore,ourarchitecturepresentsanadaptablesolutionapplicabletoanydomain
necessitatingprecisesemanticsegmentationcoupledwithdomainadaptationrequirements,suchasautonomousdrivingsce-
narios.
5.Conclusion
In conclusion, our study presented a novel semi-self-supervised strategy for developing Dense Video Segmentation (DVS)
models. Ourmethodologyaddressedthechallengesassociatedwithannotatingdense,small,andpartiallyoccludedobjects
withinvideoframes,anotoriouslytediousprocess. Morespecifically,weleveragedamulti-taskframe/maskpredictionar-
chitecture,computationallyannotatedsynthesizedvideos,andreal-lookingvideoswithweaklabelsformodeltraining. We
alsodemonstrateditspromisingutilityinprecisionagriculture,particularlyforwheatspikeVideoObjectSegmentation. No-
tably,despiterelyingonalimitednumberofhuman-annotatedvideoframes,ourmodelsachievedhighperformancesacross
diversetestsets. Importantly,wediscussedthattheproposedmethodologycanbeextendedbeyondprecisionagriculture. Its
applicability contains domains requiring video-centric tasks with multiple objects and dense patterns, such as autonomous
drivingandmedicalimaging.References
[1] TahaniAlkhudaydi,DanielReynolds,SimonGriffiths,JiZhou,andBeatrizDeLaIglesia. Anexplorationofdeep-learningbased
phenotypicanalysistodetectspikeregionsinfieldconditionsforUKbreadwheat. PlantPhenomics,2019,2019. 2
[2] MarAriza-Sent´ıs,HilmyBaja,SergioVe´lez,andJoa˜oValente. Objectdetectionandtrackingonuavrgbvideosforearlyextraction
ofgrapephenotypictraits. ComputersandElectronicsinAgriculture,211:108051,2023. 2
[3] SandeshBhagat,ManeshKokare,VineetHaswani,PrafulHambarde,andRaviKamble.WheatNet-Lite:Anovellightweightnetwork
forwheatheaddetection.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages1332–1341,2021.2
[4] Yerania Campos, Humberto Sossa, and Gonzalo Pajares. Spatio-temporal analysis for obstacle detection in agricultural videos.
AppliedSoftComputing,45:86–97,2016. 2
[5] HoKeiChengandAlexanderGSchwing. Xmem: Long-termvideoobjectsegmentationwithanatkinson-shiffrinmemorymodel.
InEuropeanConferenceonComputerVision,pages640–658.Springer,2022. 1,2,3,15
[6] HoKeiCheng,Yu-WingTai,andChi-KeungTang. Rethinkingspace-timenetworkswithimprovedmemorycoverageforefficient
videoobjectsegmentation. AdvancesinNeuralInformationProcessingSystems,34:11781–11794,2021. 15
[7] OzanCiga,TonyXu,andAnneLouiseMartel. Selfsupervisedcontrastivelearningfordigitalhistopathology. MachineLearning
withApplications,7:100198,2022. 3
[8] JifengDai,KaimingHe,andJianSun.Instance-awaresemanticsegmentationviamulti-tasknetworkcascades.InProceedingsofthe
IEEEConferenceoncomputerVisionandPatternRecognition,pages3150–3158,2016. 15
[9] MohanaDasandAbdulBais.DeepVeg:Deeplearningmodelforsegmentationofweed,canola,andcanolafleabeetledamage.IEEE
Access,9:119367–119380,2021. 2
[10] EtienneDavid, MarioSerouart, DanielSmith, SimonMadec, KaaviyaVelumani, ShouyangLiu, XuWang, FranciscoPinto, Sha-
hamehShafiee,IzzatSATahir,etal.Globalwheatheaddetection2021:Animproveddatasetforbenchmarkingwheatheaddetection
methods. PlantPhenomics,2021,2021. 2
[11] JonathonA.Gibbs,AlexandraJacquelynBurgess,MichaelP.Pound,TonyP.Pridmore,andErikHarryMurchie. Recoveringwind-
inducedplantmotionindensefieldenvironmentsviadeeplearningandmultipleobjecttracking1[cc-by].PlantPhysiology,181:28–
42,2019. 2
[12] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua
Bengio. Generativeadversarialnets. AdvancesinNeuralInformationProcessingSystems,27,2014. 3
[13] YunhuiGuo, YandongLi, LiqiangWang, andTajanaRosing. Depthwiseconvolutionisallyouneedforlearningmultiplevisual
domains. InProceedingsoftheAAAIConferenceonArtificialIntelligence,pages8368–8375,2019. 8
[14] FeixiangHanandJieLi. WheatheadsdetectionviaYOLOv5withweightedcoordinateattention. In20227thInternationalConfer-
enceonCloudComputingandBigDataAnalytics(ICCCBDA),pages300–306.IEEE,2022. 2
[15] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. AdvancesinNeuralInformationProcessing
Systems,33:6840–6851,2020. 3,10,11
[16] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mo-
hammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint
arXiv:2210.02303,2022. 2
[17] JieHu,LiShen,andGangSun. Squeeze-and-excitationnetworks. InProceedingsoftheIEEEConferenceonComputerVisionand
PatternRecognition,pages7132–7141,2018. 7
[18] LiHu,PengZhang,BangZhang,PanPan,YinghuiXu,andRongJin. Learningpositionandtargetconsistencyformemory-based
videoobjectsegmentation. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages4144–
4154,2021. 2
[19] SergeyIoffeandChristianSzegedy. Batchnormalization:Acceleratingdeepnetworktrainingbyreducinginternalcovariateshift. In
InternationalConferenceonMachineLearning,pages448–456.pmlr,2015. 8
[20] Irena Koprinska and Sergio Carrato. Temporal video segmentation: A survey. Signal processing: Image communication, 16(5):
477–500,2001. 2
[21] Lenny Lipton and Lenny Lipton. The zoe¨trope and the praxinoscope. The Cinema in Flux: The Evolution of Motion Picture
TechnologyfromtheMagicLanterntotheDigitalEra,pages55–60,2021. 2
[22] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo. Swintransformer: Hierarchical
visiontransformerusingshiftedwindows. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision, pages
10012–10022,2021. 2
[23] IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. arXivpreprintarXiv:1711.05101,2017. 12
[24] JunchengMa,YunxiaLi,HongjieLiu,KemingDu,FeixiangZheng,YongfengWu,andLingxianZhang. Improvingsegmentation
accuracy for ears of winter wheat at flowering stage by semantic segmentation. Computers and Electronics in Agriculture, 176:
105662,2020. 2
[25] K-KManinis, SergiCaelles, YuhuaChen, JordiPont-Tuset, LauraLeal-Taixe´, DanielCremers, andLucVanGool. Videoobject
segmentationwithouttemporalinformation. IEEETransactionsonPatternAnalysisandMachineIntelligence, 41(6):1515–1530,
2018. 2,3,15[26] TimMeinhardtandLauraLeal-Taixe´. Makeone-shotvideoobjectsegmentationefficientagain. AdvancesinNeuralInformation
ProcessingSystems,33:10607–10619,2020. 15
[27] JadenMyers,KeyhanNajafian,FarhadMaleki,andKatieOvens.Modifiedcycleganforthesynthesizationofsamplesforwheathead
segmentation. arXivpreprintarXiv:2402.15135,2024. 3
[28] KeyhanNajafian,AliGhanbari,IanStavness,LinglingJin,GholamHassanShirdel,andFarhadMaleki.Asemi-self-supervisedlearn-
ingapproachforwheatheaddetectionusingextremelysmallnumberoflabeledsamples. 2021IEEE/CVFInternationalConference
onComputerVisionWorkshops(ICCVW),pages1342–1351,2021. 2,3,4
[29] KeyhanNajafian,AlirezaGhanbari,MahdiSabetKish,MarkG.Eramian,GholamHassanShirdel,IanStavness,LinglingJin,and
FarhadMaleki. Semi-self-supervisedlearningforsemanticsegmentationinimageswithdensepatterns. PlantPhenomics,5,2022.
1,2,3,4,5,15
[30] BinhNguyen-Thai,VuongLe,CatherineMorgan,NadiaBadawi,T.Tran,andSvethaVenkatesh. Aspatio-temporalattention-based
modelforinfantmovementassessmentfromvideos. IEEEJournalofBiomedicalandHealthInformatics,25:3911–3920,2021. 2
[31] OzanOktay,JoSchlemper,LoicLeFolgoc,MatthewLee,MattiasHeinrich,KazunariMisawa,KensakuMori,StevenMcDonagh,
NilsYHammerla,BernhardKainz,etal.Attentionu-net:Learningwheretolookforthepancreas.arXivpreprintarXiv:1804.03999,
2018. 9,14,15
[32] FedericoPerazzi,JordiPont-Tuset,BrianMcWilliams,LucVanGool,MarkusGross,andAlexanderSorkine-Hornung.Abenchmark
datasetandevaluationmethodologyforvideoobjectsegmentation. InProceedingsoftheIEEEConferenceonComputerVisionand
PatternRecognition,pages724–732,2016. 2
[33] FedericoPerazzi,AnnaKhoreva,RodrigoBenenson,BerntSchiele,andAlexanderSorkine-Hornung.Learningvideoobjectsegmen-
tationfromstaticimages. InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages2663–2672,
2017. 2,3
[34] Prajit Ramachandran, Barret Zoph, and Quoc V. Le. Swish: a self-gated activation function. arXiv: Neural and Evolutionary
Computing,2017. 7,8
[35] ShivanganaRawat,AkshayLChandra,SaiVikasDesai,VineethNBalasubramanian,SeishiNinomiya,andWeiGuo. Howusefulis
image-basedactivelearningforplantorgansegmentation? PlantPhenomics,2022,2022. 2
[36] Esteban Real, Jonathon Shlens, Stefano Mazzocchi, Xin Pan, and Vincent Vanhoucke. Youtube-boundingboxes: A large high-
precisionhuman-annotateddatasetforobjectdetectioninvideo. InproceedingsoftheIEEEConferenceonComputerVisionand
PatternRecognition,pages5296–5305,2017. 2
[37] JosephRedmonandAliFarhadi. Yolov3:Anincrementalimprovement. ArXiv,abs/1804.02767,2018. 3
[38] AndreasRobinson,FelixJaremoLawin,MartinDanelljan,FahadShahbazKhan,andMichaelFelsberg.Learningfastandrobusttar-
getmodelsforvideoobjectsegmentation.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages7406–7415,2020. 2,3,15
[39] OlafRonneberger,PhilippFischer,andThomasBrox.U-net:Convolutionalnetworksforbiomedicalimagesegmentation.InMedical
ImageComputingandComputer-AssistedIntervention–MICCAI2015: 18thInternationalConference,Munich,Germany,October
5-9,2015,Proceedings,PartIII18,pages234–241.Springer,2015. 3,6,8,9,14,15
[40] SajadSabzi,YousefAbbaspour-Gilandeh,JoseLuisHernandez-Hernandez,FarzadAzadshahraki,andRouhollahKarimzadeh. The
useofthecombinationoftexture,colorandintensitytransformationfeaturesforsegmentationintheoutdoorswithemphasisonvideo
processing. Agriculture,9(5):104,2019. 2
[41] Pouria Sadeghi-Tehran, Nicolas Virlet, Eva M Ampe, Piet Reyns, and Malcolm J Hawkesford. DeepCount: in-field automatic
quantification of wheat spikes using simple linear iterative clustering and deep convolutional neural networks. Frontiers in Plant
Science,10:1176,2019. 2
[42] ChangweiTan,PengpengZhang,YongjiangZhang,XinxingZhou,ZhixiangWang,YingDu,WeiMao,WenxiLi,DunliangWang,
andWenshanGuo.Rapidrecognitionoffield-grownwheatspikesbasedonasuperpixelsegmentationalgorithmusingdigitalimages.
FrontiersinPlantScience,11:259,2020. 2
[43] ChengTan,ZhangyangGao,LirongWu,YongjieXu,JunXia,SiyuanLi,andStanZLi. Temporalattentionunit: Towardsefficient
spatiotemporalpredictivelearning.InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
18770–18782,2023. 2
[44] JordanRUbbens,TewodrosWAyalew,SteveShirtliffe,AniqueJosuttes,CurtisPozniak,andIanStavness.Autocount:Unsupervised
segmentationandcountingoforgansinfieldimages. InEuropeanConferenceonComputerVision,pages391–399.Springer,2020.
2
[45] HafizSamiUllah,MuhammadHamzaAsad,andAbdulBais. EndtoendsegmentationofcanolafieldimagesusingdilatedU-Net.
IEEEAccess,9:59741–59753,2021. 2
[46] CarlVondrick,HamedPirsiavash,andAntonioTorralba. Generatingvideoswithscenedynamics. AdvancesinNeuralInformation
ProcessingSystems,29,2016. 2
[47] Chien-YaoWang,I-HauYeh,andHong-YuanMarkLiao. Yolov9: Learningwhatyouwanttolearnusingprogrammablegradient
information. arXivpreprintarXiv:2402.13616,2024. 1[48] YuxinWuandKaimingHe. Groupnormalization. InProceedingsoftheEuropeanConferenceonComputerVision(ECCV),pages
3–19,2018. 7
[49] Shin’yaYamaguchi,SekitoshiKanai,TetsuyaShioda,andShoichiroTakeda. Imageenhancedrotationpredictionforself-supervised
learning. In2021IEEEInternationalConferenceonImageProcessing(ICIP),pages489–493.IEEE,2021. 3
[50] CharigYang,HalaLamdouar,ErikaLu,AndrewZisserman,andWeidiXie. Self-supervisedvideoobjectsegmentationbymotion
grouping. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages7177–7188,2021. 3
[51] Linjie Yang, Yanran Wang, Xuehan Xiong, Jianchao Yang, and Aggelos K Katsaggelos. Efficient video object segmentation via
networkmodulation. InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition,pages6499–6507,2018.
1,2
[52] LingYang, ZhilongZhang, YangSong, ShendaHong, RunshengXu, YueZhao, WentaoZhang, BinCui, andMing-HsuanYang.
Diffusionmodels:Acomprehensivesurveyofmethodsandapplications. ACMComputingSurveys,56(4):1–39,2023. 10
[53] Xiangli Yang, Zixing Song, Irwin King, and Zenglin Xu. A survey on deep semi-supervised learning. IEEE Transactions on
KnowledgeandDataEngineering,35(9):8934–8954,2022. 3
[54] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Ste´phane Deny. Barlow twins: Self-supervised learning via redundancy
reduction. InInternationalConferenceonMachineLearning,pages12310–12320.PMLR,2021. 3
[55] YurongZhang,LiuleiLi,WenguanWang,RongXie,LiSong,andWenjunZhang. Boostingvideoobjectsegmentationviaspace-
time correspondence learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
2246–2256,2023. 2,3