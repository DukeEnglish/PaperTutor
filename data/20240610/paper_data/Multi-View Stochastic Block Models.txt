Multi-View Stochastic Block Models
VincentCohen-Addad*1 Tommasod’Orsi*12 SilvioLattanzi*1 RajaiNasser*1
Abstract
Graphclusteringisacentraltopicinunsupervisedlearningwithamultitudeofpracticalapplications. Inrecent
years,multi-viewgraphclusteringhasgainedalotofattentionforitsapplicabilitytoreal-worldinstanceswhere
onehasaccesstomultipledatasources. Inthispaperweformalizeanewfamilyofmodels,calledmulti-view
stochasticblockmodelsthatcapturesthissetting. Forthismodel,wefirststudyefficientalgorithmsthatnaively
workontheunionofmultiplegraphs. Then,weintroduceanewefficientalgorithmthatprovablyoutperforms
previousapproachesbyanalyzingthestructureofeachgraphseparately. Furthermore,wecomplementourresults
withaninformation-theoreticlowerboundstudyingthelimitsofwhatcanbedoneinthismodel. Finally,we
corroborateourresultswithexperimentalevaluations.
1.Introduction
Clusteringgraphsisafundamentaltopicinunsupervisedlearning. Itisusedinavarietyoffields,includingdatamining,
socialsciences,statistics,andmore. Thegoalofgraphclusteringistopartitiontheverticesofthegraphintodisjointsetsso
thatsimilarverticesaregroupedtogetheranddissimilarverticeslieindifferentclusters. Inthiscontext,severalnotionsof
similaritybetweenverticeshavebeenstudiedthroughouttheyearsresultingindifferentclusteringobjectivesandclustering
algorithms(VonLuxburg,2007;Ngetal.,2001;Bansaletal.,2004;Goldberg,1984;Dasgupta,2016).
Despitetherichliterature,mostofthealgorithmicresultsingraphclusteringonlyfocusonthesettingwhereasinglegraph
ispresentedininput. Thisisincontrastwiththeincreasingpracticalimportanceofmultimodalityandwiththegrowing
attentioninappliedfieldstomulti-viewormulti-layerclustering(Paul&Chen,2016;Cornelietal.,2016;Hanetal.,2015;
DeBaccoetal.,2017;Khan&Maji,2019;Zhong&Pun,2021;Huetal.,2019;Abavisani&Patel,2018;Kimetal.,
2016;Gujraletal.,2020;Nietal.,2016;DeSantiagoetal.,2023;Papalexakisetal.,2013;Gujral&Papalexakis,2018;
Gorovitsetal.,2018). Inpracticeitisinfactobservedthatwhileasingledatasourceonlyoffersaspecificcharacterization
oftheunderlyingobjects,leadingtoacoarsepartitionofthedata,acarefulcombinationofmultipleviewsoftenallowsa
semanticallyrichernetworkstructuretoemerge(Fuetal.,2020;Fangetal.,2023). Forapracticalexample,considerthe
taskofclusteringusersofasocialnetworkplatformlikeFacebook,InstagramorX.Tosolvesuchtaskonecouldsimply
clusterthefriendshipgraph,oronecouldclustersuchgraphbylookingtogetheratthefriendshipgraph,theco-likegraph(a
graphwheretwousersareconnectediftheylikethesamepicture/video),theco-commentgraph(agraphwheretwousers
areconnectediftheycommentonthesamepost),theco-repostgraph(agraphwheretwousersareconnectediftheyrepost
thesamepost)andsoonandsoforth. Inpractice,onewouldexpectthesecondapproachtoworkbetterinmanysettings
becauseitprovidesamorefine-graineddescriptionofthebehaviorsoftheusers.
Despite the large number of basic applications, very little is known on the theoretical aspect of the problem. Several
works(Paul&Chen,2016;Cornelietal.,2016;Hanetal.,2015;DeBaccoetal.,2017)considerthemulti-layerstochastic
blockmodelswherethegoalistoidentifykcommunitiesgivenseveralinstances(i.e.,layerorview)ofthestochasticblock
model,eachwithkcommunities. Inthispaper,wewouldliketoworkinamoregeneralandmorerealisticsetup,where
therearekcommunitiesbuteachinstanceonlyprovidespartialinformationaboutthesekcommunities. Veryrecentlyand
concurrentlytous,(DeSantiagoetal.,2023)introducedthemulti-viewstochasticblockmodel. Inthismodel,oneisgiven
ininputmultiplegraphs,eachcomingfromastochasticblockmodel,andthegoalistoleveragetheinformationcontained
*Equalcontribution 1GoogleResearch2BIDSA,Bocconi.
1
4202
nuJ
7
]GL.sc[
1v06840.6042:viXraMulti-ViewStochasticBlockModels
inthegraphstorecovertheunderlyingclusteringstructure. Moreprecisely,givenavectoroflabels1 zwherethelabels
capturetheclusteringassignmentandarein[k],andtgraphsG ,...,G ,whereeachgraphG isdrawnindependently
1 t ℓ
fromastochasticblockmodelwith2labelsandpossiblydistinctparameters,weareinterestedindesigninganalgorithmto
weaklyrecovertheunderlyingvectorz. OneimportantaspectofthemodelisthatnoneoftheinputgraphsG ,...,G may
1 t
containenoughinformationtorecoverthefullclusteringstructure(forexamplebecause2<k),neverthelessonecanshow
thatifenoughgraphsareobserveditispossibletorecovertheclusteringstructureoftheunderlyinginstance.
ArmedwiththisnewmodelwestudydifferentapproachestoclustertheinputgraphsG ,...,G . First,wenotethatthe
1 t
naturalapproach(sometimesusedinpractice)ofmergingthegraphsandthenclusteringtheunionofthegraphs,called
earlyfusion,leadstosuboptimalresults. Thenwedesignamorecarefullatefusionclusteringalgorithmthatfirstclustersall
thegraphsseparatelyandthencarefullymergestheirresults. Thisshowsthesuperiorityoflateoverearlyfusion. Finally,we
complementourresultswithaninformation-theoreticlowerboundstudyingthelimitsofwhatcanbedoneinthismodel.
Theboundsobtainedareadrasticimprovementovertheonesobtainedby(DeSantiagoetal.,2023).
Model Beforeformallyintroducingourmodel,werecalltheclassicdefinitionofthestochasticblockmodel.
Thekcommunitysymmetricstochasticblockmodel(see(Abbe,2017)forasurvey)denotesthefollowingjointdistribution
(x,G)∼SBM overavectorofnlabelsin[k]andagraphonnvertices:
n,k,d,ε
• drawxfrom[k]nuniformlyatrandom;
• foreachdistincti,j ∈ [n],independentlycreateanedgeij inGwithprobability(1+(1− 1)ε)d ifx = x and
k n i j
probability(1− ε)d otherwise.
k n
WedenotetheconditionaldistributionofGgivenx = xasSBM (x).GivenagraphGsampledaccordingtothis
k,d,ε
model,thegoalistorecoverthe(unknown)underlyingvectoroflabelsaswellaspossible.
Mostofthestatisticalandcomputationalphenomenaatplaycanalreadybeobservedinthesimplestsettingswithtwo
communities,sowewilloftenfocusonthose. Fork = 2,wedenotethedistributionbySBM ,i.e.,weexplicitly
n,2,d,ε
replace the subscript k. It will also be convenient to use {+1,−1} for the community labels insteadof [2], so wewill
sometimesdothis. Thelabelingconventionshouldbeclearfromthecontext.
Oneofthemostwidelystudiednaturalobjectiveinthecontextofstochasticblockmodelsisthatofweakrecovery–askingto
approximatelyrecoverthecommunities.Specifically,wesaythatanalgorithmachievesweakrecoveryfor{SBM n,k,d,ε} n∈N
ifthecorrelationofthealgorithm’soutputxˆ(G)∈[k]n andtheunderlyingvectorxoflabelsisbetterthanrandomasn
grows,2
(cid:18) (cid:19)
1
P R(xˆ(G ),x )⩾ +Ω (1) ⩾1−o(1), (1)
ℓ ℓ k d,ε/k
whereR(xˆ,x)istheagreementbetweenxˆandx,definedas3
1 (cid:88)
R(xˆ,x)= max xˆ =π(x ) , (2)
π∈Pk n (cid:74) i i (cid:75)
i∈[n]
andP isthepermutationgroupof[k].
k
Asequenceofworks(Decelleetal.,2011;Massoulie´,2014;Mosseletal.,2014;2015b;Abbe&Sandon,2016a;Mosseletal.,
2018;Montanari&Sen,2016),havestudiedthestatisticalandcomputationallandscapesofthisobjective,withgreatsuccess.
Theemergingpicture(Bordenaveetal.,2015;Abbe&Sandon,2016a;Montanari&Sen,2016)showsthatitispossibleto
achieveweakrecoveryinpolynomialtimewheneverdε2/k2 >1,thisvalueiscalledtheKesten-Stigumthreshold. Further
evidence(Hopkins&Steurer,2017)suggeststhatthisthresholdisoptimalforpolynomialtimealgorithms. Inparticular,
forthespecialcaseofweakrecoverywith2communities(Mosseletal.,2015b)showedthattheproblemissolvable(also
computationallyefficiently)ifandonlyif dε2/4>1. Forlargervaluesofkagapbetweeninformation-theoreticresultsand
efficientalgorithmsexists(Abbe&Sandon,2016b;Banksetal.,2016).
1Wewriterandomvariablesinboldface.
2Weuseo(1)todenoteafunctionf suchthatlim f(n)/1=0.
n→∞
3WeuseIverson’sbracketstodenotetheindicatorfunction.
2Multi-ViewStochasticBlockModels
Inthecontextofmultimodality,wedefinethefollowingmulti-viewmodel.
Model1.1(Multi-Viewstochasticblockmodel). Letk ⩾1andletT beasequenceofttuples(d ,ε )whered ⩾0and
ℓ ℓ ℓ
ε ∈ (0,1).Werefertothefollowingjointdistribution(z,(f ,G ),...(f ,G )) ∼ (T,k,t)-MV-SBM asthe(T,k,t)-
ℓ 1 1 t t n
multi-viewstochasticblockmodel:
1. foreachℓ∈[t],independentlydrawamappingf :[k]→{±1}uniformlyatrandom;
ℓ
2. independentlydrawavectorzfrom[k]nuniformlyatrandom;
3. foreachℓ∈[t],independentlydrawagraphG ∼SBM (f (z)),wheref (z)isthen-dimensionalvectorwith
ℓ n,2,dℓ,εℓ ℓ ℓ
entriesf (z ),...,f (z ).
ℓ 1 ℓ n
GivenG ,...,G ,thegoalistoapproximatelyrecovertheunknownvectorzoflabels.
1 t
WhenT ={(d ,ε )} issuchthat(d ,ε )=(d,ε)forsomed,ε,wedenotethemodelsimplyby(d,ε,k,t)-MV-SBM .
ℓ ℓ ℓ∈[t] ℓ ℓ n
AlthoughModel1.1capturesthealgorithmicphenomenaofmulti-viewmodelsusedinpractice,moregeneralversionsof
Model1.1couldbedefined,wediscusstheminSection6. Similarlytothevanillastochasticblockmodel,weakrecoverycan
alsobedefinedforModel1.1. Wesaythatanalgorithmachievesweakrecoveryfor(T,k,t)-MV-SBM withtobservations,
n
ifitoutputsavectorzˆ(G ,...G )satisfying:
1 t
(cid:18) (cid:19)
1
P R(zˆ(G ,...G ),z)⩾ +Ω(1) ⩾1−o (1). (3)
1 t k t
Differentlyfromthevanillastochasticblockmodel,thecomplexityofModel1.1isgovernedbothbytheSBMparameters
inT and bythenumberofobservationst. Agoodalgorithmshouldthenachieveweakrecoverywiththebestpossible
multiwaytradeoffbetweentheedge-densitiesofthegraphs,thebiasesandthenumberofobservationsathand. Thatis,
extractasmuchinformationaspossiblesotorequireasfewobservationsaspossible. Thisnovelinterplayofparameters
immediatelyraisestwonaturalquestions,whicharethemainfocusofthiswork.
Howmanyobservationsareneeded? Theproblemgetseasierthelargerthenumberofobservationsonehasaccessto(see
AppendixAforaformalproof). Itisalsoeasytoseethatfort = o(logk),itisinformationtheoreticallyimpossibleto
approximatelyrecoverthecommunities(sincelog k bitsareneededtoencodek labels). Furthermore, aswewillsee,
2
strongerlowerboundscanalsobeobtained.
Howmanyobservationssuffice? Tounderstandhowmanyobservationssufficetorecoverthecommunities,itisinstructive
to consider the union graph G∗ = (cid:83) G of an instance from (d,ε,k,t)-MV-SBM , which turns out to follow a
ℓ∈[t] ℓ n
k-communities stochastic block model with parameters d∗ = Θ(dt),ε∗ = Θ(ε) (see Appendix A). Building on the
aforementionedresults,thisimpliesthatatleastt⩾Ω(k2/dε2)observationsareneededforefficientweakrecoveryofthe
communitiesfromG∗! However,asweshowlater,exponentiallybetteralgorithmscanbridgethisgap.
1.1.Results
Weakrecovery Ourmainalgorithmicresultshowsthatweakrecoveryfor(T,k,t)-MV-SBM canbeachievedinpolyno-
n
mialtimewithonlyO(logk)manyobservations.
Theorem1.2(Weakrecoveryformulti-viewmodels). Letn,k >0.Let(z,(f ,G ),...(f ,G ))∼(T,k,t)-MV-SBM
1 1 t t n
forasequenceoftuplesT ={(d ,ε )}t ,eachsatisfyingd ·ε2/4>1. ThenthereexistsaconstantC >0depending
ℓ ℓ ℓ=1 ℓ ℓ T
(cid:16) (cid:17)
onlyonT,suchthatift⩾Ω logk ,weakrecoveryofzinthesenseof (3)ispossible. Moreover,theunderlyingalgorithm
C2
T
runsinpolynomialtime.
Theorem1.2impliesthatwheneverthealgorithmhasaccesstoΘ(logk)observations,eachabovetherelativeKesten-Stigum
threshold,theguaranteesoftheunderlyingalgorithmmatchtheaforementionedtriviallowerbound,uptoconstantfactors.
Moreover,aswewillseeinSection4,theunderlyingalgorithmturnsouttobesurprisinglysimpleandefficient.
The algorithm in Theorem 1.2 applies a specialized weak-recovery algorithm on each view G to obtain a matrix Xˆ
ℓ ℓ
estimatingf (z)f (z)Tandachievingthecorrelation
ℓ ℓ
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
C ⩽E Xˆ(G) (cid:12)f (z) =f (z) −E Xˆ(G) (cid:12)f (z)̸=f (z) , (4)
ℓ ij(cid:12) ℓ i ℓ j ij(cid:12) ℓ ℓ j
3Multi-ViewStochasticBlockModels
whereC dependsonlyond ,ε . ThealgorithmthenproceedsintoprocessingtheoutputsXˆ inablackboxfashionto
ℓ ℓ ℓ ℓ
produceanestimatezˆofz.
TheconstantC inTheorem1.2istheaverageofthecorrelations(C ) . Itisnaturaltowonderwhetherthedependency
T ℓ ℓ∈[t]
ofthenumberofobservationsonC isneeded. Moreover,asforcanonicalstochasticblockmodels,itisnaturaltoaskwhat
T
theexactphasetransitionofModel1.1is. Whileweleavethislatterfascinatingquestionopen,ournextresultshowsthatif
wewantanalgorithmthatprocessesestimatesinablackboxfashion,thensomedependencyonC isindeedneeded.
T
Theorem 1.3 (Lower bound for multi-view models - Informal). Let n,k > 0. Let (z,(f ,G ),...(f ,G )) ∼
1 1 t t
(T,k,t)-MV-SBM for a sequence of tuples T = {(d ,ε )}t , each satisfying d · ε2/4 > 1. Assume that for ev-
n ℓ ℓ ℓ=1 ℓ ℓ
eryℓ∈[t]wehaveanestimate4Xˆ off (z)f (z)Tsatisfyingapair-wisecorrelation(asin(4))ofatleastC >0,andlet
ℓ ℓ ℓ ℓ
C betheaveragecorrelation.
T
(cid:16) (cid:17)
Ift=o logk ,thenbyonlyusingtheestimatesXˆ ,...,Xˆ ,itisinformation-theoreticallyimpossibletoreturnavectorzˆ
CT 1 t
satisfying
(cid:18) (cid:19)
1
P R(zˆ,z)⩾ +Ω(1) ⩾0.99. (5)
k
Exactrecovery Anotherwidelystudiedobjectiveforstochasticblockmodelsisthatofexactrecovery,wherethegoal
istocorrectlyclassifyallverticesinthegraph(see(Abbeetal.,2015;Mosseletal.,2015a;Abbe,2017)andreferences
therein). InthecontextofModel1.1thisobjectivebecomes
P(R(zˆ,z)=1)⩾1−o(1). (6)
Asacorollaryweshowthat,whengivenaccesstomoreviews,thealgorithmbehindTheorem1.2canachieveexactrecovery.
(cid:16) (cid:17)
Corollary1.4(Exactrecoveryformulti-viewmodels). ConsiderthesettingsofTheorem1.2,ift⩾Ω logn thenexact
C2
T
recoveryofzinthesenseofEquation(6)ispossible. Moreover,theunderlyingalgorithmrunsinpolynomialtime.
Experiments Theorem1.2showhows,forModel1.1,latefusionalgorithmscanprovidebetterguarantees–byrequiring
anexponentiallysmallernumberofobservationstoachievethesameerrorinalargeparametersregime–thanearlyfusion
algorithms. WefurthercorroboratedthesefindingswithexperimentsonsyntheticdatainSection5.
Organization
Therestofthepaperisorganizedasfollows. WeintroducethemainideasinSection2. InSection3weintroduceour
specializedweakrecoveryalgorithmforthestandardstochasticblockmodel. Thisisthenusedinthedesignofthealgorithm
behindTheorem1.2inSection4. ExperimentsarepresentedinSection5. Futuredirectionsandconclusionsarediscussed
inSection6. InAppendixAweshowthelimitsofalgorithmsusingtheuniongraph. AppendixBcontainsaproofof(the
formalversionof)Theorem1.3. DeferredproofsarepresentedinAppendixC.
Notation
Wedenoterandomvariablesinboldface.WehideconstantfactorsusingthenotationO(·),Ω(·),Θ(·).WewriteO (·),Ω (·)
δ δ
tospecifythatthehiddenconstantmaydependontheparameterδ. Similarly,wesometimeswriteC todenoteaconstant
δ
dependingonlyonδ. WefurtherdenotetheindicatorfunctionwithIverson’sbrackets · .Givenfunctionsf,g :R→R,
wesayf ∈o (g)iflim f(n)/g(n)=0. Similarlywewritef ∈ω (g)ifg ∈o ((cid:74)f)(cid:75).Withaslightabuseofnotation
n n→∞ n n
weoftenwriteo (g)todenoteafunctionino (g).Whenthecontextisclearwedropthesubscript. Inparticular,weoften
n n
writeo(1)todenotefunctionsthattendstozeroasngrows. Wesaythataneventhappenswithhighprobabilityifthis
u.a.r.
probabilityisatleast1−o(1).ForasetS ⊆[n],wewritei ∼ S todenoteanelementdrawnuniformlyatrandom. For
agivenprobabilitydistributionandameasurableeventE,wedenotetheprobabilitythattheeventoccursbyP(E).We
denotethecomplementeventbyEc.
Foravectorv ∈Rn,wewrite∥v∥foritsEuclideannorm. ForamatrixM ∈Rn×n,wedenoteby∥M∥itsspectralnorm
(cid:80)
andby∥M∥ itsFrobeniusnorm. Wealsolet∥M∥ := |M | .Wedenotethei-throwofM byM . ForagraphG
F 1 ij ij i
4Suchestimatesmightbeobtainedbyapplyingablackboxweak-recoveryalgorithmforSBM oneachofG ,...,G ,and
n,2,d,ε 1 t
whichhasthementionedcorrelationguarantee.
4Multi-ViewStochasticBlockModels
withnvertices,wedenotebyA(G)itsadjacencymatrix. WhenthecontextisclearwesimplywriteA. Ifthegraphis
directed,rowA containstheoutgoingedgesofvertexi.
i
For a given vector of labels z ∈ [k]n, we denote by c (z)...,c (z) the n-dimensional indicator vectors of the k
1 k
communities defined by z. In the interest of simplicity, we often denote instances (z,(f ,G ),...(f ,G )) drawn
1 1 t t
from (T,k,t)-MV-SBM simply by I. For z ∈ [k]n, we also denote by (T,k,t)-MV-SBM (z) the distribution of
n n
(z,(f ,G ),...(f ,G )) ∼ (T,k,t)-MV-SBM conditionedontheeventz = z.Weoftencallz ∈ [k]n a“community
1 1 t t n
vector”.
WesaythatanalgorithmrunsintimeT,ifintheworstcaseitperformsatmostT elementaryoperations.
2.Techniques
WeoutlineherethemainideasbehindTheorem1.2andTheorem1.3. Intheinterestofclarity,welimitourdiscussionto
(d,ε,k,t)-MV-SBM .
n
Behavioroftheuniongraph ThealgorithmbehindTheorem1.2isremarkablysimpleandleveragesknownalgorithms
forweakrecoveryofstochasticblockmodels(particularlyrelatedtotherobustalgorithmsof(Dingetal.,2022;2023)). As
afirststep,togainintuition,itisinstructivetounderstandwhytheuniongraphinsteadrequirest⩾Ω(k2)observations(see
TheoremA.1). AninstanceI of(d,ε,k,t)-MV-SBM isgivenbyavectorz∈[k]nandacollectionoftindependentpairs
n
(f ,G ),... ,(f ,G ),whereeachG issampledfromSBM (f (z)).Since,bydefinition,eachedge{i,j}appears
1 1 t t ℓ 2,d,ε ℓ
(cid:16) ε (cid:17)
inG withprobability 1+ ·f (z) ·f (z) · d ,intheuniongraph(cid:83) G thesameedgeappearsroughlywith
ℓ 2 ℓ i ℓ j n ℓ∈[t] ℓ
probability
d (cid:88)(cid:16) ε (cid:17)
· 1+ ·f (z) ·f (z) .
n 2 ℓ i ℓ j
ℓ∈[t]
Theintuitionhereisthatifz ̸=z thesecondterminthesumwouldbecloseto0,whileforz =z itwouldbe εt.Hence,
i j i j 2
wewillseethisedgeintheuniongraphroughlywithprobability
dt
·(1+O(ε)· z =z ).
n (cid:74) i j (cid:75)
Thatis,theuniongraphbehavessimilarlytoavanillastochasticblockmodelwithkcommunities,biasO(ε)andexpected
degree dt. As stated in the introduction, existing efficient algorithms can achieve weak recovery for that distribution
whenever (dt)ε2/k2 > Ω(1), implying t ⩾ Ω(k2) in the regime 4 < dε2 ⩽ O(1) where weak recovery for each
observationispossible.
Amplifying the signal-to-noise ratio via black-box estimators The above approach of taking the union graph and
thenrunningcommunitydetectiononityieldssub-optimalguaranteesbecausethegraphdoesnotkeepallinformation
regarding the instance I. Our strategy to overcome this issue is to proceed in the reverse order: first extract as much
informationaspossiblefromeachgraph,andthencombinethedata. Concretely,inthecontextof(d,ε,k,t)-MV-SBM ,
n
ourplanistoaccuratelyestimatethematrixf (z)f (z)T foreachgraphG .Indeed,thepolynomial(cid:80) f (z) f (z)
ℓ ℓ ℓ ℓ∈[t] ℓ i ℓ j
stronglycorrelateswith z =z inthesensethat
i j
(cid:74) (cid:75)
 (cid:12)   (cid:12) 
(cid:12) (cid:12)
t=E (cid:88) f ℓ(z) if ℓ(z)
j
(cid:12) (cid:12)z
i
=z j>E (cid:88) f ℓ(z) if ℓ(z)
j
(cid:12) (cid:12)z
i
̸=z j=0.
(cid:12) (cid:12)
ℓ∈[t] (cid:12) ℓ∈[t] (cid:12)
(cid:80)
Inotherwords,wecanaccuratelyestimatewhetherz =z ornotbyaccuratelyestimatingtheproducts f (z) f (z) .
i j ℓ∈[t] ℓ i ℓ j
Now,wedonothaveaccesstothefunctionsf (z)butwecanhope(seethesubsequentparagraphs)thatexistingweak
ℓ
recoveryalgorithmscanprovideacloseenoughestimateinthesense
 (cid:12)   (cid:12) 
(cid:12) (cid:12)
t·C
d,ε
=E (cid:88) xˆ(G ℓ) ixˆ(G ℓ)
j
(cid:12) (cid:12)z
i
=z j>E (cid:88) xˆ(G ℓ) ixˆ(G ℓ)
j
(cid:12) (cid:12)z
i
̸=z j=0. (7)
(cid:12) (cid:12)
ℓ∈[t] (cid:12) ℓ∈[t] (cid:12)
5Multi-ViewStochasticBlockModels
(cid:80)
Ifso,wemaysimplydecidewhetheri,j shouldbeclusteredtogetherbasedonhowlarge xˆ(G ) xˆ(G ) is. By
ℓ∈[t] ℓ i ℓ j
independence of the observations, standard concentration of measure results tell us that Ω(log(n)/C2 ) observations5
d,ε
wouldsufficetoexactlypredictallthen2pairs(andhenceachieveexactrecoverywiththisnumberofobservations).
Improvementsvianeighborhoodsintersection Continuingwiththeabovelineofthinking,onecanfurtherimprove
thedependencyonttot=Θ(logk)aspromisedinTheorem1.2. Foralabelp∈[k],letc (z)∈{0,1}nbetheindicator
p
vectorofthecorrespondingcommunity. Theimprovementcomesfromobservingthatforℓ̸=ℓ′andforanytypicallabelling
z (i.e. a labelling that is approximately balanced), we have large separation between the community indicator vectors
∥c (z)−c (z)∥2 ⩾Ω(n/k).Thecrucialconsequenceisthatforafixedindexi∈[n],wedonotneedtoguesscorrectly
p p′
z =z forallj andwemaymisclassifysomepairs. IndeedifA ∈{0,1}nisavectorwithentries(A ) thataccurately
i j i i j
(cid:74)predicts (cid:75)z =z uptoaρ<n/kmisclassificationerror,thenwecandeducewhetheri,j comefromthesamecommunity
i j
byverify(cid:74)ingifA(cid:75)andA agreeonthemajorityoftheirentries. Concretely,bythereversetriangleinequalityitholdsthat
i j
|∥A −A ∥−∥c (z)−c (z)∥|⩽∥c (z)−A ∥+∥c (z)−A ∥⩽O(n/k).
i j p p′ p i p′ j
Thatis,wearestillabletoexactlydeducewhetheri,j sitinthesamecommunity! Theimprovementovertthencomesas
O(log(k)/C2)observationssufficetoboundthemisclassificationerrorbyn/ktimesatinyconstant. Finally,weremark
δ
thatweareboundtomisclassifysomeverticesasforsomeverticesitheestimatorvectorA willnotaccuratelyrepresentits
i
communitywhent⩽O(log(k)/C2)(thisisduetothewell-knowngapbetweenweakrecoveryandexactrecoveryinthe
δ
vanillastochasticblockmodel(Abbe,2017)).
Thepair-wiseweakrecoveryestimator Sofar,weglossedoverthefactthatwedonothaveanalgorithmreturning
anaccurateestimatexˆ(G )xˆ(G )T off (z)f (z)T giventhegraphG .Noticethatforapair(x,G) ∼ SBM , an
ℓ ℓ ℓ ℓ ℓ n,2,d,ε
algorithmachievingweakrecoveryreturnsavectorxˆ(G)∈{±1}nsuchthat
Ω(n2)⩽E(cid:2) ⟨xˆ(G),x⟩2(cid:3)⩽E(cid:2) ⟨xˆ(G)xˆ(G)T,xxT⟩(cid:3)
whichisenoughtoobtaintheseparationrequiredinEquation(7). Indeed,theaboveimpliesthatonaverage
E[xˆ(G) xˆ(G) |x =x ]−E[xˆ(G) xˆ(G) |x ̸=x ]⩾Ω (1),
i j i j i j i j d,ε
whichisenoughtocarryoutthestrategyoutlinedinthepreviousparagraphs.Noticethat,apriori,itisnotclearwhetherthese
estimatorsshouldworkfor(d,ε,k,t)-MV-SBM asthemodelintroducessomesubtledifficultiescomparedtoSBM .
n n,2,d,ε
Mostimportantly,thelabelsinf (z)–andhencetheedgesinG –arenotpair-wiseindependent.
ℓ ℓ
Webypassthisobstaclecarryingouttheanalysisafterconditioningonthechoiceoff ,sothat,eventhoughthecommunities
ℓ
maybeunbalanced,theedgesareagainindependent. Now,forhighlyunbalancedcommunitiestheexpecteddegreeofeach
vertexisanaccuratepredictorforitscommunity,henceweakrecoveryiseasytoachieve. Ontheotherhand,onecantreat
slightlyunbalancedcommunitiesasperturbedbalancedcommunitiesandapplythenode-robustalgorithmof(Dingetal.,
2023).
Remark 2.1(Connectionwith(Liuetal.,2022)). Liu, MoitraandRaghavendrastudiedajointdistributionmodelover
hypergraphs with independence edges. In the special case of graphs, their model can be seen as a simpler version of
Model1.1inwhicheveryf (·)isknown,andeachd isaconstant. Forthismodel,(Liuetal.,2022)beatsrandomguessing:
ℓ ℓ
Theyproduceaunitvectorthatcorrelateswiththecommunityvectorbetterthanarandomvectorguesswould. However,
theyprovidenoroundingstrategy. Thealgorithmictechniquesin(Liuetal.,2022)areverydifferentfromoursanddonot
implyaresultoftheformofTheorem1.2forModel1.1.
Informationtheoreticlowerboundsforblack-boxalgorithms TheproofofTheorem1.3consistsoftwomainsteps. In
thefirststep,weboundhowmuchinformationanestimateXˆ withpair-wisecorrelationC canrevealaboutz. Weshow
ℓ ℓ
thatthiscanbebounded(intermsofmutualinformation)asI(z;Xˆ )⩽O(C ·n). Inthesecondstepweuseanadapted
ℓ ℓ
versionofFano’sinequalitytoshowthatifzˆisanestimateofzachieving(5),thenzˆmusthaveatleastΩ(n·logk)bitsof
informationaboutz,i.e.,I(z;zˆ)⩾Ω(n·logk).
Nowifzˆisobtainedbyonlyprocessing(Xˆ ) ,thenbyusingthedataprocessinginequality,wecanshowthat
ℓ ℓ∈[t]
Ω(n·logk)⩽ (cid:88) I(z;Xˆ )⩽ (cid:88) O(C ·n)⩽O(C ·t·n),
ℓ ℓ T
t∈[t] t∈[t]
√
5ThisisbetterthanΩ(k2)aslongask⩾Ω( logn).
6Multi-ViewStochasticBlockModels
(cid:16) (cid:17)
whereC istheaverageofthecorrelations(C ) .Fromthiswededucethatwemusthavet⩾Ω logk .
T ℓ ℓ∈[t] CT
3.SpecializedweakrecoveryforvanillaSBMs
Generalweakrecoveryresults(Abbe,2017)forstochasticblockmodelsturnouttobetooweakforourobjective. Werely
insteadonthefollowingstrongerstatement,whichweobtainbyexploitingtherobustalgorithmsof(Dingetal.,2022),
(Dingetal.,2023),andwhichalsoworksdowntotheKesten-Stigumthreshold. Thisspecializedestimatorwillbeusedin
themainalgorithmbehindTheorem1.2.
Theorem3.1(Pair-wiseweakrecoveryforunbalanced2communitiesstochasticblockmodel). Letn,d,ε>0besatisfying
dε2/4−1>0.Letx=(x ) ∈{±1}nbeasequenceofi.i.d. binaryrandomvariableswithP(x =+1)=p.There
i i∈[n] i
existsapolynomialtimealgorithmsuchthat, oninputG ∼ SBM (x), returnswithprobability1−o(1)amatrix
n,2,d,ε
Xˆ(G)∈[−1,+1]n×nsatisfying∀i,j ∈[n]
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
C ⩽E Xˆ(G) (cid:12)x =x −E Xˆ(G) (cid:12)x ̸=x
d,ε ij (cid:12) i j ij (cid:12) i j
forsomeconstantC >0.
d,ε
NoticethatthealgorithmonlyreceivesG,d,εandhenceitdoesnot knowp.Notealsothatintheabove,ifoneofthe
eventsx =x orx ̸=x hasprobability0(i.e.,ifp=0orp=1),thenweadopttheconventionthatthecorresponding
i j i j
conditionalexpectationis0.
WedefertheproofofTheorem3.1toAppendixC.
4.Thealgorithm
InthissectionweproveTheorem1.2. Westartbydescribingtheunderlyingalgorithm. Throughoutthesection,foragiven
t,weassumeT ={(d ,ε )}t tobeasequenceofttuples(d ,ε )eachsatisfyingd ·ε2/4−1>0.Foreachℓ∈[t],
ℓ ℓ ℓ=1 ℓ ℓ ℓ ℓ
letC betheweak-recoveryconstantthatisachievableforSBM inthesenseofTheorem3.1(recallthisconstant
ℓ n,dℓ,εℓ
dependsonlyond ,ε )andlet
ℓ ℓ
1 (cid:88)
C = C >0.
t ℓ
ℓ∈[t]
Wealsoassumek ⩽n1−Ω(1).
Algorithm1Communitydetectionformulti-viewstochasticblockmodels
Input: k,G ,...,G .
1 t
Output: Communityvectorzˆin[k]n.
ForeachG withℓ∈[t],runthecommunitydetectionalgorithmofTheorem3.1.
ℓ
ConstructthedirectedgraphFonthevertexset[n]asfollows.
fori=1tondo
Addanoutgoingedgetothen/kverticesj ∈[n]withlargest(cid:80) Xˆ(G ) .
ℓ∈[t] ℓ ij
endfor
RunAlgorithm2ontheadjacencymatrixAofFandreturntheresultingvector.
Remark4.1(Runningtime).
ByTheorem3.1,thefirststepofthealgorithmtakestimeO(cid:0) t·nO(1)(cid:1)
.Computingthevalues
of(cid:80) Xˆ(G ) forallpairstakestimeO(t·n2).DrawingtheedgestakestimeO(n2logn). Hencethealgorithmruns
ℓ∈[t] ℓ ij
intimeO(cid:0) (tn)O(1)+T(cid:1)
whereT istherunningtimeofAlgorithm2.
Graph structure on balanced instances Algorithm 1 will work on typical instances from (T,k,t)-MV-SBM . In
n
particular,itwillworkoninstancesthatareapproximatelybalanced,asdescribedbelow.
Definition4.2(Balancedvector). Letz ∈[k]n.Wesaythatzisbalancedifforallp∈[k],itholds
(cid:16) (cid:17)n (cid:16) (cid:17)n
1−n−Ω(1) ⩽∥c (z)∥2 ⩽ 1+n−Ω(1) .
k p k
IfzisbalancedwealsosaythatI ∼(T,k,t)-MV-SBM (z)isbalanced.
n
7Multi-ViewStochasticBlockModels
It is immediate to see that a random instance (z,(f ,G ),...(f ,G )) ∼ (T,k,t)-MV-SBM is balanced with high
1 1 t t n
probability.
Fact 4.3 (Probability of balanced instance). Let I := (z,(f ,G ),...(f ,G )) ∼ (T,k,t)-MV-SBM ,. Then, with
1 1 t t n
probability1−n−10,I isbalanced.
TheproofofFact4.3isstraightforwardandwedeferittoAppendixC.
Onbalancedinstanceswithsufficientlymanyobservations,theadjacencymatrixAofFbecomesagoodapproximationof
thetruecommunitymatrix(cid:80) c (z)c (z)T.
i∈[k] i i
Lemma4.4(Graphsstructurefromgoodinstances). Letn,k,t>0. LetI :=(z,(f ,G ),...(f ,G )). ThenAlgorithm1
1 1 t t
constructsanadjacencymatrixAsuchthat∀p∈[k],∀i∈[n]
(cid:104) (cid:12) (cid:105) (cid:16) (cid:17)
E ∥A −c (z)∥2 (cid:12)z =p ⩽O(n)· n−Ω(1)+e−Ω(C¯2·t) .
i p (cid:12) i
ToproveLemma4.4werequireanintermediatestep.
Fact4.5. ConsiderthesettingofLemma4.4. ThereexistsaconstantC∗ ∈[−t,t]suchthat,fori,j ∈[n],
 (cid:12) 
(cid:12)
P (cid:88) Xˆ(G ℓ) ij <C∗− C 3·t (cid:12) (cid:12) (cid:12)z i =z j⩽e−Ω(C2·t) ,
ℓ∈[t] (cid:12)
 (cid:12) 
(cid:12)
P (cid:88) Xˆ(G ℓ) ij ⩾C∗− C 3·t (cid:12) (cid:12) (cid:12)z i ̸=z j⩽e−Ω(C2·t) .
ℓ∈[t] (cid:12)
WedefertheproofofFact4.5toAppendixC.WearenowreadytoproveLemma4.4.
ProofofLemma4.4. Fixi∈[n].WecanlimitouranalysisafterconditioningontheeventE(z)thatzisbalanced. Indeed,
wehave
(cid:104) (cid:12) (cid:105)
E ∥A −c (z)∥2 (cid:12)E(z)c,z =p ·P(E(z))⩽O(n)·P(E(z))⩽n−Ω(1).
i p (cid:12) i
LetC∗betheconstantofFact4.5. Define
B = (cid:88) Xˆ(G ) ,
ij ℓ ij
ℓ∈[t]
andletA′ =(A′ ) bethebinaryvectordefinedas
i ij j∈[n]
A′ =(cid:115) B ⩾C∗−
C¯·t(cid:123)
.
ij ij 3
Noticethat∥A′∥ isthenumberofindicesj withB ⩾C∗− C¯·t.NowfromthedefinitionofthebinaryvectorA ,we
i 1 ij 3 i
knowthatA =1ifandonlyifB isamongthetopn/kvaluesin{B :j′ ∈[n]} .Fromthisitisnothardtoseethat
ij ij ij′
that∥A −A′∥ canbeboundedfromabovebyhowmuch∥A′∥ deviatesfromn/k,thatis
i i 1 i 1
(cid:12) n(cid:12)
∥A −A′∥ ⩽(cid:12)∥A′∥ − (cid:12) .
i i 1 (cid:12) i 1 k(cid:12)
NowassumingthatE(z)holds,wehave
n
∥c (z)∥ =∥c (z)∥2 = ±n1−Ω(1),
p 1 p k
hence
∥A i−A′ i∥
1
⩽(cid:12) (cid:12)∥A′ i∥ 1−∥c p(z)∥ 1(cid:12) (cid:12)+n1−Ω(1) ⩽∥A′ i−c p(z)∥ 1+n1−Ω(1).
8Multi-ViewStochasticBlockModels
Sincec (z)andA arebinaryvectors,wehave
p i
∥A −c (z)∥2 =∥A −c (z)∥ ⩽∥A −A′∥ +∥A′ −c (z)∥ ,
i p i p 1 i i 1 i p 1
andhence,givenE(z),
∥A −c (z)∥2 ⩽2∥A′ −c (z)∥ +n1−Ω(1). (8)
i p i p 1
Nownoticethat
|(A′) −c (z) |= (A′) =1 c (z) =0 + (A′) =0 c (z) =1
i j p j i j p j i j p j
(cid:74) (cid:75)(cid:74) (cid:75) (cid:74) (cid:75)(cid:74) (cid:75)
= (A′) =1 z ̸=p + (A′) =0 z =p .
i j j i j j
(cid:74) (cid:75)(cid:74) (cid:75) (cid:74) (cid:75)(cid:74) (cid:75)
UsingFact4.5andleveragingthefactthatP(E(z)c |z =p)⩽O(cid:0) n−10(cid:1) weget
i
E[|(A′) −c (z) ||E(z),z =p]⩽e−Ω(C2·t) +O(cid:0) n−10(cid:1) .
i j p j i
Combiningthiswith(8)weconcludethat
(cid:104) (cid:12) (cid:105) (cid:16) (cid:17)
E ∥A −c (z)∥2 (cid:12)E(z),z =p ⩽O(n)· n−Ω(1)+e−Ω(C¯2·t) .
i p (cid:12) i
Rounding ThankstoLemma4.4,anapplicationofthefollowingroundingscheme–sometimescalledsecondmoment
rounding,asonemayseeAasanestimateof(cid:80) c (z)c (z)T–sufficestocomputethetruecommunities.
p p p
Algorithm2Secondmomentrounding
Input: AmatrixA∈{0,1}n×n.
Output: Communityvectorzˆin[n]k.
LetS =∅.
forp=1tok(stopearlierifS =[n])do
Pickuniformlyatrandomi∈[n]\S.SetS ={i}.
p
forj ∈[n]\S,j ̸=ido
Setj ∈S if∥A −A ∥2 ⩽ n.
p i j k
endfor
AddS toS.
p
endfor
Assigneachi∈[n]\StoasetS ,wherepischosenuniformlyatrandom.
p
Return: thevectorzˆwithzˆ =pifandonlyifi∈S .
i p
Remark4.6(Runningtime). EachstepintheouterlooptakestimeO(n2). OverallAlgorithm2runsinO(k·n2).
Asfirststepoftheproof,weintroduceanewdefinition.
Definition4.7(Representativerow). Letz ∈ [k]n,letc (z),...,c (z)betheindicatorvectorsofthelabelsinz andlet
1 k
A∗(z)=(cid:80) c (z)c (z)T.ForamatrixA∈{0,1}n×n,wesayA isq-representativeif
p p p i
∥A −A∗(z) ∥2 ⩽n·e−q·C2·t. (9)
i i
WedenotebyR thesetofqrepresentativesofA.
q
Theuseofrepresentativerowsisconvenientbecause,forsufficientlymanyobservations,itiseasytoseethatrowswhichare
representativeofthesamecommunitymustbeclosetogether,whilerowsthatarerepresentativeofdifferentcommunities
mustbefarfromeachother.
9Multi-ViewStochasticBlockModels
Lemma 4.8. Let n,k,t > 0, and let q > 0 be the hidden constant in Lemma 4.4. Let t ⩾ Clogk for a large enough
C2
universalconstantC >0. Letz ∈[k]n bebalanced. Supposethatateachiterationoftheouterloop,Algorithm2picks
(cid:80)
someA thatisaq-representative. Thenmax zˆ = π(z ) = |R | ,whereP isthepermutationgroup
i π∈Pk i∈Rq(z)
(cid:74)
i i
(cid:75)
q k
over[k].
WedefertheproofofLemma4.8toAppendixC.Now,toproveTheorem1.2,itremainstoarguethat,withhighprobability,
firsttherearefewnon-representativesinA,andsecondAlgorithm2picksq-representativesateachiterationoftheouter
loop.
Lemma 4.9. Let n,k,t > 0, and let q > 0 be the hidden constant in Lemma 4.4. Let t = Clogk for a large enough
C¯2
universalconstantC >0. LetI :=(z,(f ,G ),...(f ,G ))∼(T,k,t)-MV-SBM .LetAbethematrixconstructedby
1 1 t t n
Algorithm1. OninputA,withprobabilityatleast1−k−Ω(1),thefollowingholds:
• thereareatleastn·(1−k−Ω(1))rowsinAwhichareΩ(q)-representatives,
• step1.(a)ofAlgorithm2onlypicksΩ(q)-representativevectors.
Proof. ByFact4.3wemayassumezisbalanced.
ByLemma4.4andMarkov’sinequality,withprobability1−e−Ω(q·C2·t)
thereareatmostO(n)·e−Ω(q·C2·t)
indicesi∈[n]suchthatA isnotaΩ(q)-representative. NotethatifC islargeenough,
i
thene−Ω(q·C2·t) =k−Ω(1). Ateveryiterationoftheloop,ifwepickarepresentativevectorweremoveatmost(1+o(1))n
k
indices,sincezisbalanced. Henceateachiterationthereareatleast n(1−o(1))indicesthatareΩ(q)-representativeand
k
whichhavenotyetbeenpicked. ItfollowsthattheprobabilityweneverpickanindexthatisnotΩ(q)-representativeisat
(cid:16) (cid:17)k
least 1− n·k−Ω(1) ⩾1−k−Ω(1)asdesired(bychoosingC tobelargeenough).
n/k
Theorem1.2nowimmediatelyfollowscombiningFact4.3,Lemma4.4,Lemma4.8andLemma4.9.
Exactrecovery Lemma4.9alsoimplicitlyyieldexactrecoveryforsufficientlymanyobservations.
Corollary4.10. Letn,k,t>0,andletq >0bethehiddenconstantinLemma4.4. Lett⩾Clogn foralargeenough
C¯2
universalconstantC >0. LetI :=(z,(f ,G ),...(f ,G ))∼(T,k,t)-MV-SBM .LetAbethematrixconstructedby
1 1 t t n
Algorithm1. OninputA,withprobabilityatleast1−n−Ω(1),allrowsinAareΩ(q)-representatives.
Proof.
ByLemma4.4andMarkov’sinequality,withprobability1−e−Ω(q·C2·t)thereareatmostO(n)·e−Ω(q·C2·t)
indices
i∈[n]suchthatA isnotaΩ(q)-representative. Therefore,fort⩾ C lognnosuchindexexists.
i C¯2
SincebyLemma4.8onlyindicescorrespondingtorowsthatarenotΩ(q)-representativecanbemisclassified,byFact4.3,
Lemma4.4,Lemma4.9andCorollary4.10weobtainCorollary1.4.
5.Experiments
Weshowhereexperimentsonsyntheticdatasampledfrom(d,ε,k,t)-MV-SBM . TheestimatorinTheorem3.1iscomplex
n
andreliesonahighordersum-of-squaresprogram,makingithardtoimplementinpractice. Nevertheless,itisreasonable
tobelievethat: (i)theguaranteesofTheorem3.1areonlysufficient,butnotnecessary,toobtainTheorem1.2;(ii)other
estimatorsprovidetheguaranteesofTheorem3.1. Forthisreason,itmakessensetotestAlgorithm1withothercommunity
detectionalgorithms.
Thenextfigurescomparestheresultson(z,(f ,G ),...,(f ,G ))∼(d,ε,k,t)-MV-SBM (forawiderangeofparameters)
1 1 t t n
ofthefollowingalgorithms:
(cid:83)
A.1 Louvain’salgorithm(Blondeletal.,2008)ontheuniongraph G .
i∈[t] t
A.2 Algorithm1withLouvain’salgorithmappliedinplaceoftheestimatorofTheorem3.1.
They-axismeasuresagreementasdefinedinEquation(2). Resultsareaveragedover20simulations.
10Multi-ViewStochasticBlockModels
Figure1.Fixingt=10,n=1000,k=10,d=50andvaryingεin[0.5,1.5].
Figure2.Fixingt=10,n=1000,k=10,ε=0.5andvaryingdin[50,150].
6.Conclusionsandfuturedirections
TheintroductionofModel1.1raisesseveralnaturalquestions,forwhichweonlyprovideinitialanswers.
On the phase transition threshold One of the most interesting question concerns the phase transition of the model.
Concretely,onemayexpectarichinterplaybetweenthesignal-to-noiseratioofeachoftheobservedgraphs(possiblybelow
therelativeKSthreshold)andthenumberofobservationsrequiredtoweaklyrecoverthehiddenvectorz. Weleavethe
characterizationofthistrade-offbeyondTheorem1.2andTheorem1.3asafascinatingopenquestion.
From2communitiestokcommunitiesinthemulti-viewmodel Anothernaturalquestionconcernthegeneralization
to a modelin which eachview mayhave 2 ⩽ k ⩽ k communities. The ideasoutlined abovetranslate in principle to
ℓ
thesesettingsbutthecorrectnessappearsdifficulttoprove. Concretely,anyestimatorachievingguaranteescomparableto
Theorem3.1butformorethan2communitiescanimmediatelybeplugged-inAlgorithm1toachieveweakrecoveryin
thesemoregeneralsettings.6 However,tothebestofourknowledgeexistingweak-recoveryalgorithmsforSBM do
n,k,d,ε
notleadtoestimatorsofthisform.
6Withoutchangingtheproofofcorrectnessofthealgorithm!
11Multi-ViewStochasticBlockModels
Acknowledgments
WethankDavidSteurerforinsightfuldiscussionsintheearlystagesofthiswork. Tommasod’Orsiispartiallysupportedby
theprojectMURFARE2020PAReCoDi.
References
Abavisani,M.andPatel,V.M. Deepmultimodalsubspaceclusteringnetworks. IEEEJournalofSelectedTopicsinSignal
Processing,12(6):1601–1614,2018.
Abbe, E. Communitydetectionandstochasticblockmodels: recentdevelopments. TheJournalofMachineLearning
Research,18(1):6446–6531,2017.
Abbe,E.andSandon,C. Achievingtheksthresholdinthegeneralstochasticblockmodelwithlinearizedacyclicbelief
propagation. AdvancesinNeuralInformationProcessingSystems,29,2016a.
Abbe,E.andSandon,C. Crossingtheksthresholdinthestochasticblockmodelwithinformationtheory. In2016IEEE
InternationalSymposiumonInformationTheory(ISIT),pp.840–844.IEEE,2016b.
Abbe,E.,Bandeira,A.S.,andHall,G. Exactrecoveryinthestochasticblockmodel. IEEETransactionsoninformation
theory,62(1):471–487,2015.
Banks,J.,Moore,C.,Neeman,J.,andNetrapalli,P. Information-theoreticthresholdsforcommunitydetectioninsparse
networks. InConferenceonLearningTheory,pp.383–416.PMLR,2016.
Bansal,N.,Blum,A.,andChawla,S. Correlationclustering. Machinelearning,56:89–113,2004.
Blondel,V.D.,Guillaume,J.-L.,Lambiotte,R.,andLefebvre,E. Fastunfoldingofcommunitiesinlargenetworks. Journal
ofstatisticalmechanics: theoryandexperiment,2008(10):P10008,2008.
Bordenave, C., Lelarge, M., and Massoulie´, L. Non-backtracking spectrum of random graphs: community detection
andnon-regularramanujangraphs. In2015IEEE56thAnnualSymposiumonFoundationsofComputerScience,pp.
1347–1357.IEEE,2015.
Corneli,M.,Latouche,P.,andRossi,F. Exacticlmaximizationinanon-stationarytemporalextensionofthestochastic
blockmodelfordynamicnetworks. Neurocomputing,192:81–91,2016.
Dasgupta,S. Acostfunctionforsimilarity-basedhierarchicalclustering. InProceedingsoftheforty-eighthannualACM
symposiumonTheoryofComputing,pp.118–127,2016.
De Bacco, C., Power, E. A., Larremore, D. B., and Moore, C. Community detection, link prediction, and layer inter-
dependence in multilayer networks. Phys. Rev. E, 95:042317, Apr 2017. doi: 10.1103/PhysRevE.95.042317. URL
https://link.aps.org/doi/10.1103/PhysRevE.95.042317.
De Santiago, K., Szafranski, M., and Ambroise, C. Mixture of stochastic block models for multiview clustering. In
ESANN2023-EuropeanSymposiumonArtificialNeuralNetworks,ComputationalIntelligenceandMachineLearning,
pp.151–156,2023.
Decelle,A.,Krzakala,F.,Moore,C.,andZdeborova´,L. Asymptoticanalysisofthestochasticblockmodelformodular
networksanditsalgorithmicapplications. PhysicalReviewE,84(6):066106,2011.
Ding,J.,d’Orsi,T.,Nasser,R.,andSteurer,D. Robustrecoveryforstochasticblockmodels. In2021IEEE62ndAnnual
SymposiumonFoundationsofComputerScience(FOCS),pp.387–394.IEEE,2022.
Ding,J.,d’Orsi,T.,Hua,Y.,andSteurer,D. Reachingkesten-stigumthresholdinthestochasticblockmodelundernode
corruptions. InTheThirtySixthAnnualConferenceonLearningTheory,pp.4044–4071.PMLR,2023.
Fang,U.,Li,M.,Li,J.,Gao,L.,Jia,T.,andZhang,Y. Acomprehensivesurveyonmulti-viewclustering. IEEETransactions
onKnowledgeandDataEngineering,2023.
12Multi-ViewStochasticBlockModels
Fu,L.,Lin,P.,Vasilakos,A.V.,andWang,S. Anoverviewofrecentmulti-viewclustering. Neurocomputing,402:148–161,
2020.
Goldberg,A.V. Findingamaximumdensitysubgraph. 1984.
Gorovits,A.,Gujral,E.,Papalexakis,E.E.,andBogdanov,P. Larc: Learningactivity-regularizedoverlappingcommunities
acrosstime. InProceedingsofthe24thACMSIGKDDinternationalconferenceonknowledgediscovery&datamining,
pp.1465–1474,2018.
Gujral,E.andPapalexakis,E.E. Smacd: Semi-supervisedmulti-aspectcommunitydetection. InProceedingsofthe2018
SIAMInternationalConferenceonDataMining,pp.702–710.SIAM,2018.
Gujral,E.,Pasricha,R.,andPapalexakis,E. Beyondrank-1: Discoveringrichcommunitystructureinmulti-aspectgraphs.
InProceedingsofTheWebConference2020,pp.452–462,2020.
Han,Q.,Xu,K.,andAiroldi,E. Consistentestimationofdynamicandmulti-layerblockmodels. InBach,F.andBlei,D.
(eds.),Proceedingsofthe32ndInternationalConferenceonMachineLearning,volume37ofProceedingsofMachine
Learning Research, pp. 1511–1520, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.
press/v37/hanb15.html.
Hopkins,S.B.andSteurer,D. Efficientbayesianestimationfromfewsamples: communitydetectionandrelatedproblems.
In2017IEEE58thAnnualSymposiumonFoundationsofComputerScience(FOCS),pp.379–390.IEEE,2017.
Hu, D., Nie, F., and Li, X. Deep multimodal clustering for unsupervised audiovisual learning. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.9248–9257,2019.
Khan,A.andMaji,P. Approximategraphlaplaciansformultimodaldataclustering. IEEEtransactionsonpatternanalysis
andmachineintelligence,43(3):798–813,2019.
Kim,M.,Han,D.K.,andKo,H. Jointpatchclustering-baseddictionarylearningformultimodalimagefusion. Information
Fusion,27:198–214,2016.
Liu,S.,Mohanty,S.,andRaghavendra,P. Onstatisticalinferencewhenfixedpointsofbeliefpropagationareunstable. In
2021IEEE62ndAnnualSymposiumonFoundationsofComputerScience(FOCS),pp.395–405.IEEE,2022.
Massoulie´,L. Communitydetectionthresholdsandtheweakramanujanproperty. InProceedingsoftheforty-sixthannual
ACMsymposiumonTheoryofcomputing,pp.694–703,2014.
Montanari,A.andSen,S. Semidefiniteprogramsonsparserandomgraphsandtheirapplicationtocommunitydetection. In
Proceedingsoftheforty-eighthannualACMsymposiumonTheoryofComputing,pp.814–827,2016.
Mossel,E.,Neeman,J.,andSly,A. Beliefpropagation,robustreconstructionandoptimalrecoveryofblockmodels. In
ConferenceonLearningTheory,pp.356–370.PMLR,2014.
Mossel, E., Neeman, J., and Sly, A. Consistency thresholds for the planted bisection model. In Proceedings of the
forty-seventhannualACMsymposiumonTheoryofcomputing,pp.69–75,2015a.
Mossel,E.,Neeman,J.,andSly,A. Reconstructionandestimationintheplantedpartitionmodel. ProbabilityTheoryand
RelatedFields,162:431–461,2015b.
Mossel,E.,Neeman,J.,andSly,A. Aproofoftheblockmodelthresholdconjecture. Combinatorica,38(3):665–708,2018.
Ng, A., Jordan, M., andWeiss, Y. Onspectralclustering: Analysisandanalgorithm. Advancesinneuralinformation
processingsystems,14,2001.
Ni, J., Cheng, W., Fan, W., and Zhang, X. Self-grouping multi-network clustering. In 2016 IEEE 16th International
ConferenceonDataMining(ICDM),pp.1119–1124.IEEE,2016.
Papalexakis, E.E., Akoglu, L., andIence, D. Domoreviewsofagraphhelp? communitydetectionandclusteringin
multi-graphs. InProceedingsofthe16thInternationalConferenceonInformationFusion,pp.899–905.IEEE,2013.
13Multi-ViewStochasticBlockModels
Paul,S.andChen,Y. Consistentcommunitydetectioninmulti-relationaldatathroughrestrictedmulti-layerstochastic
blockmodel. 2016.
VonLuxburg,U. Atutorialonspectralclustering. Statisticsandcomputing,17:395–416,2007.
Zhong,G.andPun,C.-M. Latentlow-rankgraphlearningformultimodalclustering. In2021IEEE37thInternational
ConferenceonDataEngineering(ICDE),pp.492–503.IEEE,2021.
14Multi-ViewStochasticBlockModels
A.Failureofcommunitydetectionontheuniongraph
InthissectionweproviderigorousevidencethatefficientalgorithmcannotachievecomparableguaranteestoTheorem1.2
(cid:83)
byonlyconsideringtheuniongraph G for(z,(f ,G ),...(f ,G ))∼(d,ε,k,t)-MV-SBM . Concretely,weprove
i∈[t] i 1 1 t t n
thefollowingtheorem.
TheoremA.1(Limitsofweakrecoveryfromtheuniongraph). Letn,k,d,ε>0,assumethatt⩾100·(logk)2,andlet
I := (z,(f ,G ),...(f ,G )) ∼ (d,ε,k,t)-MV-SBM . Withprobabilityatleast1−k−Ω(1) overthedrawsoff ,...,f ,
1 1 t t n 1 t
theconditionaldistributionover(z,G ,...,G )satisfies:
1 t
• zisdrawnuniformlyatrandomfrom[k]n;
• eachedgeijappears(independently)in(cid:83) G withprobabilityatmost d∗(1+(1−1)ε∗)ifz =z andprobability
i∈[t] i n k i j
atleast d∗(1− ε)otherwise,forsomed∗,ε∗suchthat
n k
1+ ε 1+ ε∗
dt· 2 ⩽d∗ ⩽dt· 1−ε∗/k −o(1).
1+(cid:0)
1−
1(cid:1)
ε∗
1+(cid:0)
1−
1(cid:1)
ε∗
k k
(cid:83)
Inwords,TheoremA.1showsthattheuniongraph G isessentiallyak-communitystochasticblockmodelwith
i∈[t] i
parameters d∗ = Θ(dt),ε∗ = Θ(ε). As discussed in Section 1, it is conjecturally hard to achieve weak recovery in
polynomialtimeford∗(ε∗/k)2 ⩽1.InthecontextofTheoremA.1,thisimpliesthattheparametersofthedistributionof
(cid:83) G areabovetheKesten-Stigumthresholdonlyford∗(ε∗/k)2 ⩾Ω(1). Thatis,atleastt⩾Ω(k2)observationsare
i∈[t] i
required!
Nextweprovethetheorem.
ProofofTheoremA.1. Letq,q′ ∈[k]bedistinct. ByChernoff’sboundandchoiceoftwehave7
 
t (cid:88) t
P  2(1−o(1))⩽ (cid:74)f ℓ(q)=f ℓ(q′) (cid:75)⩽ 2(1+o(1))⩾1−k−5. (10)
ℓ∈[t]
Hence we may take a union over all such pairs q,q′ ∈ [k] as the corresponding event E will hold with probability at
least1−k−O(1).Soletf ...,f : [k] → ±1befixedfunctionsverifyingtheeventE of(10). Weconditiontherestof
1 t
the analysis on f = f ,...,f = f . In these settings each edge appears in G∗ := (cid:83) G independently of others.
1 1 t t i∈[t] i
Moreover,byunionbound
(cid:16) ε(cid:17)dt
P(ij ∈G∗ |z =z ,f =f ,...,f =f )⩽ 1+ ,
i j 1 1 t t 2 n
and
P(ij ∈G∗ |z ̸=z ,f =f ,...,f =f )
i j 1 1 t t
(cid:18) (cid:16) ε(cid:17) d(cid:19) 2t(1−o(1))(cid:18) (cid:16) ε(cid:17) d(cid:19) 2t(1+o(1))
⩾1− 1− 1+ 1− 1−
2 n 2 n
(cid:18) (cid:16) ε(cid:17) dt (cid:19)(cid:18) (cid:16) ε(cid:17) dt (cid:19)
⩾1− 1− 1+ (1−o(1)) 1− 1− (1+o(1))
2 2n 2 2n
dt
⩾(1−o(1)) ,
n
whereweusedtheinequality(1+s)r ⩾1+rs,forr >1,s>−1.Itremainstocomputed∗andε∗sothat
(cid:16) ε(cid:17)dt d∗ (cid:18) (cid:18) 1(cid:19) (cid:19)
1+ ⩽ 1+ 1− ε∗ ,
2 n n k
7Wecantakeo(1)tobet−1/4andbyHoeffding’sinequalitywecanboundtheprobabilityoftheeventE in(10)nothappeningby
√
2exp(− t)⩽2exp(−10logk)⩽k−5.
15Multi-ViewStochasticBlockModels
dt d∗ (cid:18) ε∗(cid:19)
(1−o(1)) ⩾ 1− .
n n k
Rearrangingtheinequalities,
1+ ε
d∗ ⩾dt· 2 ,
1+(cid:0)
1−
1(cid:1)
ε∗
k
1 1+ ε∗k
d∗ ⩽dt· −o(1)=dt· k−ε∗ −o(1)
1− ε∗ 1+(cid:0) 1− 1(cid:1) ε∗
k k
asdesired.
Remark A.2 (On the weighted union graph). A natural question to ask is whether the weighted union graph – the
(cid:80)
graph over [n], in which edge ij has weight ij ∈ G – could provide better guarantees. In the sparse settings
ℓ∈[t] ℓ
(cid:74) (cid:75)
d ⩽ no(1),t ⩽ no(1) only a no(1)−1 fraction of the edges have weight larger than 1 and thus one may expect that this
additionalinformationdoesnotsimplifytheproblem.
B.Informationtheoreticlowerboundforblackboxalgorithms
Inthissectionwewouldliketostudy,inthecontextof(T,k,t)-MV-SBM ,theinformation-theoreticlimitationsforhaving
n
analgorithmthat(i)runstheprocedureinTheorem3.1oneachobservationand(ii)usestheresultingmatrices(Xˆ ) to
ℓ ℓ∈[t]
reconstructtheoriginalkcommunities. Essentially,wewouldliketoproveaformalversionofTheorem1.3. Inordertodo
this,wefirstneedtointroducesomeusefulnotationandterminology.
Throughoutthesectionletz∈[k]nbethevectorofcommunities,andlet(f ) betindependentanduniformlydistributed
ℓ ℓ∈[t]
randommappings[k] → {+1,−1}. Foreveryℓ ∈ [t]letx = f (z)andletG ∼ SBM (f (z)). Weintroducea
ℓ ℓ ℓ n,2,dℓ,εℓ ℓ
quantitativeversionofweak-recovery.
DefinitionB.1(α -weak-recoveryalgorithm). Wesaythatanalgorithm8 Xˆ takingG asinputandproducinganestimate
ℓ ℓ ℓ
Xˆ (G )∈[+1,−1]n×nofx x Tisanα -weak-recoveryalgorithmifwehave
ℓ ℓ ℓ ℓ ℓ
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
E Xˆ (G) (cid:12)(x ) =(x ) −E Xˆ(G) (cid:12)(x ) ̸=(x ) ⩾α , ∀i,j ∈[n]. (11)
ℓ ij (cid:12) ℓ i ℓ j ij (cid:12) ℓ i ℓ j ℓ
Clearly,thealgorithmmentionedinTheorem3.1isaC -weak-recoveryalgorithm.
d,ε
Weareinterestedindeterminingtheinformation-theoreticlimitsforestimatingzbasedonlyontheoutputsofanα -weak-
ℓ
recoveryalgorithmwhenappliedontheobservations(G ) . Tothisend,letusintroduceblackboxestimators:
ℓ ℓ∈[t]
DefinitionB.2(Blackboxestimator). Ablackboxestimatorforzisamapping
zˆ:([+1,−1]n×n)t →[k]n.
Theblackboxestimatorisappliedasfollows:Foreveryℓ∈[t],wefirstcomputeXˆ =Xˆ (G ),forsomeα-weak-recovery
ℓ ℓ ℓ
algorithmXˆ forx forwhichwedonotknowanythingaboutexceptthatitisanα-weak-recoveryalgorithm,andthen
ℓ ℓ
computezˆ=zˆ(Xˆ ,...,Xˆ ).
1 t
Wewouldliketoguaranteethattheblackboxestimatoryieldsasuccessfulweak-recoveryofzusingonlythefactthat
Xˆ =Xˆ (G )satisfies(11)foreveryℓ∈[t]. Inordertoformalizethis,wewillusethenotionofα-estimates:
ℓ ℓ ℓ
Definition B.3 (α-estimates). Let α = (α ) ∈ (0,2]t be a sequence of t positive numbers. Let Xˆ ,...,Xˆ ∈
ℓ ℓ∈[t] 1 t
[+1,−1]n×nbetrandommatrices. Wesaythat(Xˆ ) areα-estimatesof(x x T) iftheysatisfythefollowingthree
ℓ ℓ∈[t] ℓ ℓ ℓ∈[t]
conditions:
(a) Given(x ) ,therandommatrices(Xˆ ) areconditionallyindependentfromz.
ℓ ℓ∈[t] ℓ ℓ∈[t]
(b) For every ℓ ∈ [t], given x , the random matrix Xˆ is conditionally independent from
ℓ ℓ
(x ,...,x ,x ,...,x ,Xˆ ,...,Xˆ ,Xˆ ,...,Xˆ ).
1 ℓ−1 ℓ+1 t 1 ℓ−1 ℓ+1 t
8NotethatXˆ maybearandomizedalgorithm.
ℓ
16Multi-ViewStochasticBlockModels
(c) Foreveryℓ∈[t]andeveryi,j ∈[n]withi̸=j wehave
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
E (Xˆ ) (cid:12)(x ) =(x ) −E (Xˆ ) (cid:12)(x ) ̸=(x ) ⩾α , ∀i,j ∈[n]. (12)
ℓ ij (cid:12) ℓ i ℓ j ℓ ij (cid:12) ℓ i ℓ j ℓ
ItisnothardtoseethatifXˆ isanα -weak-recoveryalgorithmsforeveryℓ∈[t],then(Xˆ(G )) areα-estimatesof
ℓ ℓ ℓ ℓ∈[t]
(x x T) .
ℓ ℓ ℓ∈[t]
Now we are ready to formally define what we mean by “guaranteeing that the blackbox estimator yields a successful
weak-recoveryofzusingonlythefactthatXˆ =Xˆ(G )satisfies(11)foreveryℓ∈[t]”:
ℓ ℓ
DefinitionB.4((ρ,τ,α,t)-weak-recoveryblackboxestimator). Letα = (α ) ∈ (0,2]t beasequenceoftpositive
ℓ ℓ∈[t]
numbers,andletρ>0andτ >0.
Amapping9
zˆ:([−1,+1]n×n)t →[k]n
issaidtobea(ρ,τ,α,t)-weak-recoveryblackboxestimatorforzfromα-estimatesof(x x T) ifforeverytrandom
ℓ ℓ ℓ∈[t]
matrices(Xˆ ) whichareα-estimatesof(x x T) ,if
ℓ ℓ∈[t] ℓ ℓ ℓ∈[t]
z=zˆ(Xˆ ,...,Xˆ ),
1 t
thenwithprobabilityatleastτ wehave
(cid:88) 1 1
max z =π(zˆ ) ⩾ +ρ, (13)
π∈Pk
i
n(cid:74) i i (cid:75) k
whereP isthesetofpermutations[k]→[k].
k
Wearenowreadytostatethemaintheoremofthesection,whichimpliesTheorem1.3.
TheoremB.5(FormalstatementofTheorem1.3). Letα = (α ) ∈ (0,2]t beasequenceoftpositivenumbersand
ℓ ℓ∈[t]
denote
1 (cid:88)
α= α .
t ℓ
ℓ∈[t]
Letρ > 0andτ > 0.Letz ∈ [k]n betheuniformlyrandomvectorofcommunities, andlet(f ) betindependent
ℓ ℓ∈[t]
and uniformly distributed random mappings [k] → {+1,−1}. For every ℓ ∈ [t] let x = f (z). If there exists a
ℓ ℓ
(ρ,τ,α,t)-weak-recoveryblackboxestimatorforzfromα-estimatesof(x x T) ,andifnislargeenoug,thenwemust
ℓ ℓ ℓ∈[t]
have
(cid:18) (cid:19)
τ ·logk
t⩾Ω .
ρ α
We prove Theorem B.5 in Appendix B.1, Appendix B.2, and Appendix B.3. We conclude this section showing how
Algorithm1isindeedablackboxestimator.
ProofthatAlgorithm1isablackboxestimator. WecansplitAlgorithm1intotwosteps:
(1) ComputingXˆ =Xˆ (G ),...,Xˆ =Xˆ (G )byapplyingthealgorithminTheorem3.1toG ,...,G ,respectively.
1 1 1 t t t 1 t
(2) Computinganestimatezˆofzbasedonlyon(Xˆ ) .
ℓ ℓ∈[t]
Instep(1),sinceeachofXˆ isappliedtoG independentlyofallothergraphsandsinceG dependsonzonlythrough
ℓ ℓ ℓ
x ,itisnothardtoseethat(Xˆ ) satisfyconditions(a)and(b)ofDefinitionB.3. Nowifα =C isthecorrelation
ℓ ℓ ℓ∈[t] ℓ ℓ
guaranteedbyTheorem3.1forXˆ ,itfollowsthat(Xˆ ) areα-estimatesof(x x T) ,whereα=(α ) .
ℓ ℓ ℓ∈[t] ℓ ℓ ℓ∈[t] ℓ ℓ∈[t]
Now since step (2) of Algorithm 1 only processes (Xˆ ) , and since the guarantee on the agreement of zˆ with z is
ℓ ℓ∈[t]
provedbasedonlyonthefactthat(Xˆ ) areα-estimates,wecanseethat,assumingthattisalargeenoughmultipleof
ℓ ℓ∈[t]
(logk)/α2,step(2)isa(1−k−Ω(1),1−k−Ω(1),α,t)-weak-recoveryblackboxestimator.
9Notethatzˆmaybearandomizedfunction.
17Multi-ViewStochasticBlockModels
B.1.Upperboundontheinformationrevealedbyα-estimates
The first step in our proof is to determine how much information about z the α-estimates can reveal. Let (Xˆ ) be
ℓ ℓ∈[t]
α-estimatesof(x x T) . Themutualinformation(measuredinbits)betweenzandXˆ ,...,Xˆ canbeupperbounded
ℓ ℓ ℓ∈[t] 1 t
asfollows:
(∗)
I(z;Xˆ ,...,Xˆ ) ⩽ I(x ,...,x ;Xˆ ,...,Xˆ )
1 t 1 t 1 t (14)
=H(Xˆ ,...,Xˆ )−H(Xˆ ,...,Xˆ |x ,...,x ),
1 t 1 t 1 t
where(∗)followsfromthedata-processinginequality10.
TheentropyH(Xˆ ,...,Xˆ )canbeupperboundedasfollows:
1 t
H(Xˆ ,...,Xˆ )⩽ (cid:88) H(Xˆ ).
1 t ℓ (15)
ℓ∈[t]
Nowusingthechainrule,theconditionalentropyH(Xˆ ,...,Xˆ |x ,...,x )canberewrittenasfollows:
1 t 1 t
H(Xˆ ,...,Xˆ |x ,...,x )= (cid:88) H(Xˆ |x ,...,x ,Xˆ ,...,Xˆ )
1 t 1 t ℓ 1 t 1 ℓ−1
ℓ∈[t]
(16)
= (cid:88) H(Xˆ |x ),
ℓ ℓ
ℓ∈[t]
wherethelastequalityfollowsfromProperty(b)ofα-estimates.
Combining(14),(15)and(16)weget
I(z;Xˆ ,...,Xˆ )⩽
(cid:88)(cid:16)
H(Xˆ )−H(Xˆ |x
)(cid:17)
1 t ℓ ℓ ℓ
l∈[t]
(17)
= (cid:88) I(x ;Xˆ )
ℓ ℓ
l∈[t]
Now for each ℓ ∈ [t], we will derive an upper bound on I(x ;Xˆ ) (which would then induce an upper bound on
ℓ ℓ
I(z;Xˆ ,...,Xˆ )). Notethatwecannotobtainanon-trivialupperboundonI(x ;Xˆ )forarbitraryα-estimatesbecause
1 t ℓ ℓ
settingXˆ =x x Twouldsatisfythedefinitionofα-estimates,andforXˆ =x x TwehaveI(x ;Xˆ )=n,whichistoo
ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ
largeforourpurposes. Whatwewilldoinsteadistoshowthatthereexistα-estimatesforwhichwecangetthedesired
upperboundonI(x ;Xˆ ).
ℓ ℓ
Theα-estimatesthatwewillconsiderareoftheformXˆ =xˆ xˆ T wherexˆ ∈{+1,1}nisdefinedasfollows:
ℓ ℓ ℓ ℓ
(cid:89)
P(xˆ =xˆ |x =x )= P((xˆ ) =(xˆ ) |(x ) =(x ) ) ,
ℓ ℓ ℓ ℓ ℓ i ℓ i ℓ i ℓ i
i∈[n]
where
P((xˆ ) =(xˆ ) |(x ) =(x ) )=(cid:40) 21 +(cid:112)α 8ℓ if(xˆ ℓ) i =(x ℓ) i,
ℓ i ℓ i ℓ i ℓ i 1 2 −(cid:112)α 8ℓ if(xˆ ℓ) i =−(x ℓ) i.
Inotherwords,weobtainxˆ bysendingtheentriesofx throughabinarysymmetricchannelwithflippingprobability
ℓ ℓ
1 −(cid:112)αℓ.
2 8
10NoticethatduetoProperty(a)ofα-estimates,z−(x ) −(xˆ ) isaMarkovchain.
ℓ ℓ∈[t] ℓ ℓ∈[t]
18Multi-ViewStochasticBlockModels
Nownoticethat
E[(xˆ ) |(x ) ]=(x ) ·P((xˆ ) =(x ) |(x ) )−(x ) ·P((xˆ ) =−(x ) |(x ) )
ℓ i ℓ i ℓ i ℓ i ℓ i ℓ i ℓ i ℓ i ℓ i ℓ i
(cid:114) (cid:114)
α α
=2 ℓ(x ) = ℓ(x ) .
8 ℓ i 2 ℓ i
Hence,fori̸=j
(cid:104) (cid:12) (cid:105)
E (Xˆ ) (cid:12)(x ) ,(x ) =E[(xˆ ) ·(xˆ ) |(x ) ,(x ) ]
ℓ i,j (cid:12) ℓ i ℓ j ℓ i ℓ j ℓ i ℓ j
=E[(xˆ ) |(x ) ]E[(xˆ ) |(x ) ]
ℓ i ℓ i ℓ j ℓ j
(cid:114) (cid:114)
α α α
= ℓ(x ) · ℓ(x ) = ℓ(x ) ·(x ) ,
2 ℓ i 2 ℓ j 2 ℓ i ℓ j
fromwhichitisnothardtoseethat
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105) α (cid:16) α (cid:17)
E (Xˆ ) (cid:12)(x ) =(x ) −E (Xˆ ) (cid:12)(x ) =−(x ) = ℓ − − ℓ =α .
ℓ i,j (cid:12) ℓ i ℓ j ℓ i,j (cid:12) ℓ i ℓ j 2 2 ℓ
Thisprovesthatourchoiceof(Xˆ ) indeedyieldsα-estimates. Intheremainderofthissubsectionwewillshowthat
ℓ ℓ∈[t]
thisparticularchoiceofα-estimatesisnoisyenoughtoyieldausefulupperboundonthemutualinformationI(x ;Xˆ ).
ℓ ℓ
Foreveryℓ∈[t]wehave
I(x ;Xˆ )=I(x ;xˆ )=H(xˆ )−H(xˆ |x )⩽n−H(xˆ |x ), (18)
ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ ℓ
wherethefirstequalityfollowsfromthefactthatthereisaone-to-onemappingbetweenxˆ andXˆ =xˆ xˆ T,andthelast
ℓ ℓ ℓ ℓ
inequalityfollowsfromthefactthatxˆ ∈{+1,−1}nisabinaryvectoroflengthn.
ℓ
Nownoticethattheconditionaldistributionofxˆ givenx canbeseenasasequenceofnindependentBernoullirandom
ℓ ℓ
variableswithparameter 1 ±(cid:112)αℓ. Therefore11,
2 8
(cid:18) (cid:114) (cid:19)
1 α
H(xˆ |x )=n·h + ℓ , (19)
ℓ ℓ 2 2 8
where
h (p)=−plog p−(1−p)log (1−p)
2 2 2
isthebinaryentropyfunction.
Combining(18)and(19),weget
(cid:18) (cid:18) (cid:114) (cid:19)(cid:19)
1 α
I(x ;Xˆ )⩽n· 1−h + ℓ .
ℓ ℓ 2 2 8
Nownotethatthefunctionp(cid:55)→h (p)isastrictlyconcavefunctionachievingitsmaximumatp= 1 forwhichwehave
2 2
h (p)=1. Therefore,forsmallα ,wehave
2 ℓ
(cid:18)
1
(cid:114)
α
(cid:19) h′′(1/2)(cid:18)(cid:114)
α
(cid:19)2 (cid:18)(cid:114)
α
(cid:19)3
1−h + ℓ = 2 ℓ ±O ℓ ⩽O(α ).
2 2 8 2 8 8 ℓ
WeconcludethatthereexistsanabsoluteconstantC >0suchthat
I(x ;Xˆ )⩽C·α ·n.
ℓ ℓ ℓ
Combiningthiswith(17),weconcludethatforsomeα-estimates(Xˆ ) of(xxT) ,wehave
ℓ ℓ∈[t] ℓ ℓ∈[t]
I(z;Xˆ ,...,Xˆ )⩽ (cid:88) C·α ·n=C·α·t·n. (20)
1 t ℓ
ℓ∈[t]
11Noticethath (p)=h (1−p)andhenceh (cid:0)1 +(cid:112)αℓ(cid:1) =h (cid:0)1 −(cid:112)αℓ(cid:1) .
2 2 2 2 8 2 2 8
19Multi-ViewStochasticBlockModels
B.2.Weaklyrecoveringzreducesitsentropy
Nowletzˆ∈[k]nbeanestimateofzwhichsatisfies(13)withprobabilityatleastτ. Wewillapplyamodifiedversionofthe
standardFanoinequalityinordertoupperboundH(z|zˆ). Definetherandomvariable
(cid:40)
1 ifzandzˆsatisfy(13),
A=
0 otherwise.
Wehave
H(z|zˆ)⩽H(A,z|zˆ)
=H(A|zˆ)+H(z|zˆ,A)
⩽H(A)+H(z|zˆ,A=0)P[A=0]+H(z|zˆ,A=1)P[A=1]
⩽1+(nlog k)·P[A=0]+H(z|zˆ,A=1)·P[A=1],
2
wherethelastinequalityfollowsfromthefactthatAisabinaryrandomvariable(andhenceitsentropyisatmostonebit),
andthefactthatz∈[k]n,whichimpliesthatH(z|zˆ,A=0)⩽log (kn)=nlog k. Weconcludethat
2 2
H(z|zˆ)⩽1+nlog k−(nlog k−H(z|zˆ,A=1))·P[A=1]
2 2
⩽1+nlog k−τ ·(nlog k−H(z|zˆ,A=1)) ,
2 2
wherethelastinequalityfollowsfromthefactthatP[A=1]⩾τ andthefactthatnlog k−H(z|zˆ,A=1)⩾0(because
2
z∈[k]n). Nowsincezisauniformrandomvariablein[k]n,wehaveH(z)=log (kn)=nlog k,andhence
2 2
I(z;zˆ)=H(z)−H(z|zˆ)
(21)
⩾τ ·(nlog k−H(z|zˆ,A=1))−1.
2
NowwewillfocusonupperboundingH(z|zˆ,A=1). Foreveryzˆ∈[k]n,wehave
H(z|zˆ=zˆ,A=1)⩽log |Z(zˆ,ρ)|, (22)
2
where
(cid:40) (cid:41)
(cid:88) 1 1 (cid:91)
Z(zˆ,ρ)= z ∈[k]n : max z =π(zˆ) ⩾ +ρ = Z(zˆ,ρ,π),
π∈Pk
i
n(cid:74) i i (cid:75) k
π∈Pk
and
(cid:40) (cid:41)
(cid:88) 1 1
Z(zˆ,ρ,π)= z ∈[k]n : z =π(zˆ) ⩾ +ρ .
n(cid:74) i i (cid:75) k
i
Hence,
(cid:88)
|Z(zˆ,ρ)|⩽ |Z(zˆ,ρ,π)|.
π∈Pk
WewillfurtherdivideZ(zˆ,ρ,π)asfollows:
(cid:91)
Z(zˆ,ρ,π)= Z(zˆ,ρ,π,m),
βn⩽m⩽n
where
1
β = +ρ,
k
and
(cid:40) (cid:41)
(cid:88)
Z(zˆ,ρ,π,m)= z ∈[k]n : z =π(zˆ) =m .
i i
(cid:74) (cid:75)
i
20Multi-ViewStochasticBlockModels
Itisnothardtoseethat
(cid:18) (cid:19)
n
|Z(zˆ,ρ,π,m)|= ·(k−1)n−m.
m
Bydefiningβ = m andusingStirling’sformula12,weget:
m n
log |Z(zˆ,ρ,π,m)|=nlog n−nlog e
2 2 2
−mlog m+mlog e−(n−m)log (n−m)+(n−m)log e
2 2 2 2
±O(logn)+(n−m)log (k−1)
2
=nlog n−β nlog (β n)−(1−β )nlog ((1−β )n)
2 m 2 m m 2 m
+(1−β )nlog (k−1)±O(logn)
m 2
=(h (β )+(1−β )·log (k−1))·n±O(logn).
2 m m 2
By taking derivatives and analyzing the function g(β ) = h (β )+(1−β )·log (k −1), we can show that g is
m 2 m m 2
decreasingafterβ ⩾ 1,andhenceforβ ⩾β = 1+ρ⩾ 1,wehaveg(β )⩽g(β). Inparticular,foreverymsatisfying
m k m k k m
βn⩽m⩽n,wehave
log |Z(zˆ,ρ,π,m)|⩽(h (β)+(1−β)·log (k−1))·n+O(logn).
2 2 2
Therefore,
(cid:88) (cid:88)
|Z(zˆ,ρ)|⩽ |Z(zˆ,ρ,π,m)|
π∈Pkβn⩽m⩽n
(cid:88) (cid:88)
⩽ 2(h2(β)+(1−β)·log 2(k−1))·n+O(logn)
π∈Pkβn⩽m⩽n
=k!·n·2(h2(β)+(1−β)·log 2(k−1))·n+O(logn),
andhence
log |Z(zˆ,ρ)|⩽(h (β)+(1−β)·log (k−1))·n+O(klogk+logn).
2 2 2
Sincethisistrueforeveryzˆ∈[k]n,wegetfrom(22)that
H(z|zˆ,A=1)⩽(h (β)+(1−β)·log (k−1))·n+O(klogk+logn).
2 2
Combiningthiswith(21),weget
I(z;zˆ)⩾τ(nlog k−(h (β)+(1−β)·log (k−1))n−O(logn+klogk))−1
2 2 2
τ ·n (23)
⩾ (log k−h (β)−(1−β)·log (k−1)) ,
2 2 2 2
wherethelastinequalityassumes13thatnislargeenough(andinparticularn≫klogk).
B.3.Puttingeverythingtogether
ProofofTheoremB.5. Assumethatthereisa(ρ,τ,α,t)-weak-recoveryblackboxestimatorzˆforzandassumethatnis
largeenough. Let(Xˆ ) beα-estimatesof(x ) satisfying(20),i.e.,
ℓ ℓ∈[t] ℓ ℓ∈[t]
I(z;Xˆ ,...,Xˆ )⩽C·α·t·n.
1 t
Letzˆ=zˆ(Xˆ ,...,Xˆ ). Fromthedata-processinginequality,wehave
1 ℓ
I(z;zˆ)⩽I(z;Xˆ ,...,Xˆ )⩽C·α·t·n.
1 t
12Forlargen,wehavelog (n!)=nlog n−nlog e±O(log n).
2 2 2 2
13Itisworthnotingthatifβ >1/k,thenlog k−h (β)−(1−β)·log (k−1)>0,aswewillshowinthenextsubsection.
2 2 2
21Multi-ViewStochasticBlockModels
Ontheotherhand,sincezˆisa(ρ,τ,α,t)-weak-recoveryblackboxestimatorandsince(xˆ ) areα-estimatesof(x ) ,
ℓ ℓ∈[t] ℓ ℓ∈[t]
itfollowsthatzˆsatisfies(13)withprobability1−τ. Itfollowsfrom(23)thatfornlargeenough,wehave
τ ·n
I(z;zˆ)⩾ ·(log k−h (β)−(1−β)·log (k−1)) .
2 2 2 2
Weconcludethat
τ ·n
·(log k−h (β)−(1−β)·log (k−1))·n⩽C·α·t·n.
2 2 2 2
Therefore,wemusthave
log k−h (β)−(1−β)·log (k−1)
t⩾τ · 2 2 2 .
2C·α
Nowdefine
l(β)=log k−h (β)−(1−β)·log (k−1)
2 2 2
=log k+βlog β+(1−β)log (1−β)−(1−β)·log (k−1),
2 2 2 2
sothat
τ ·l(β) τ ·l(1/k+ρ)
t⩾ = .
2C·α 2C·α
Letusanalyzethefunctionl(β):
• Aquickcalculationshowsthat
l(1/k)=0.
• Thederivativeoflis
1 1
l′(β)=log β+ −log (1−β)− +log (k−1)
2 ln2 2 ln2 2
=log β−log (1−β)+log (k−1),
2 2 2
andhence
l′(1/k)=0.
• Thesecondderivativeoflis
1 1
l′′(β)= + >0, ∀β ∈(0,1).
βln2 (1−β)ln2
Hence,l′(β)>0forβ ∈(1/k,1),andsincel(1/k)=0wecanseethatl(1/k+ρ)>0wheneverρ>0.Furthermore,a
quickcalculationrevealsthatforfixedρwehave14
l(1/k+ρ)
lim =ρ>0,
k→∞ log 2(k)
whichmeansthat
l(1/k+ρ)
min >0,
k⩾2 log 2(k)
andhencel(1/k+ρ)=Ω (log (k)).Weconcludethat
ρ 2
(cid:18) (cid:19)
τ ·logk
t⩾Ω .
ρ α
14Notethatβlog β+(1−β)log (1−β) = −h (β)andsince0 ⩽ h (β) ⩽ 1,wecanseethatlim h2(β) = 0.Hence
2 2 2 2 k→∞ log2(k)
lim l(1/k+ρ) canbesimplifiedaslim 1−(1−ρ− 1)log2(k−1) =ρ.
k→∞ log2(k) k→∞ k log2(k)
22Multi-ViewStochasticBlockModels
C.Deferredproofs
Wepresenthereproofsdeferredinthemainbodyofthepaper.
DeferredproofsofSection3 .
ToobtainTheorem3.1weneedtointroduceresultsaboutrobustweakrecovery.
DefinitionC.1(µ-nodecorrupted,balanced2communitiesSBM). Letµ∈[0,1].Letx∈{±1}nbeavectorsatisfying
(cid:80) x =0andletG0 ∼SBM (x).Anadversarymaychooseuptoµ·nverticesinG0andarbitrarilymodifyedges
i i 2,d,ε
µ
(andnon-edges)incidenttoatleastoneofthemtoproducethecorruptedgraphG. WewriteG≈G0todenotethatGisa
µ-nodecorruptedversionofG0.
Inthecontextofnodecorruptedgraphs, thedefinitionofweakrecoveryisstillwithrespecttotheoriginalvectorxas
definedinEquation(1). Itisknownthatnoderobustweakrecoveryisachievable.
TheoremC.2(Implicitin(Dingetal.,2023)). Letn,d,ε>0besatisfyingd·ε2−1=:δ >0.Thereexist:
• constants0<µ <1and0<C <1,and
δ δ
• a (randomized) polynomial time algorithm15 Xˆ taking a graph G of n vertices as input and producing a matrix
r
Xˆ (G)∈[−1,+1]n×nasoutput,
r
suchthatXˆ isasuccessfulweak-recoveryrecoveryalgorithmrobustagainstanyµ-nodecorruptionforallµ⩽µ . More
r δ
formally,foreveryx∈{±1}nsatisfying(cid:80) x =0,andeveryµ⩽µ ,wehave
i i δ
(cid:34) (cid:35)
E min ⟨Xˆ (G),xxT⟩ ⩾C ·n2.
r δ
G0∼SBM2,d,ε(x) G:G≈µ
G0
WecanuseTheoremC.2toobtainTheorem3.1.
Letγ =(cid:12) (cid:12)p− 21(cid:12) (cid:12)betheunbalancednessinthevectoroflabelsofx,wherep=P(x
i
=+1).
The main idea behind the algorithm in Theorem 3.1 is to first distinguish whether the unbalancedness γ is sufficiently
smallornot. Ifitissufficientlysmall,thenweapplytherobustalgorithmofTheoremC.2. Otherwise,wecanachieve
weak-recoverybyrelyingonthedegreeofavertextoestimateitscommunitylabel.
Inthefollowingtwolemmas,wetreatthecasewheretheunbalancednessγ issufficientlysmall:
LemmaC.3. Letn,d,ε,p,x = (x ) andG ∼ SBM (x)beasinTheorem3.1andletδ = ε2d −1 > 0. Let
i i∈[n] n,2,d,ε 4
µ ,C andXˆ be16asinTheoremC.2anddefine
δ δ r
1
µ′ = min{µ ,C }.
δ 100 δ δ
Iftheunbalancednessγ =(cid:12) (cid:12)1 −p(cid:12) (cid:12)ofxsatisfiesγ ⩽µ′ ,thenfornlargeenough,wehave
2 δ
(cid:104) (cid:105) 3
E ⟨Xˆ (G),xxT⟩ ⩾ C ·n2.
r 4 δ
Proof. Forthesakeofsimplicity,wewillonlytreatthecasewhereniseven. SinceXˆ isrobustagainstnodecorruptions,it
r
isnothardtoseethattheproofscanbeadaptedtothecasewherenisodd.
15ThesubscriptrinXˆ standsfor“robust”.
r
16ItisworthnotingthatsinceTheoremC.2assumesthat(cid:80)
x =0,thennmustbeeveninTheoremC.2.However,inLemmaC.3
i i
wewouldlikentobegeneral.Hence,ifnisodd,weapplythefollowingprocedure:(1)weaddafictitiousvertexn+1whichisnot
incidenttoanyvertexin[n]andwecalltheresultinggraph(having[n+1]asitssetofvertices)asG˜ ,(2)weapplyXˆ onG˜ ,and(3)
r
wetakethesubmatrixofXˆ (G˜)inducedbytheverticesin[n].WestilldenotetheoverallalgorithmasXˆ .
r r
23Multi-ViewStochasticBlockModels
Foreveryx∈{±}n,letn (x)=|{i∈[n]:x =+1}|andn (x)=|{i∈[n]:x =−1}|. SinceP(x =+1)=p,then
+ i − i i
bythelawoflargenumbersweknowthatn (x)andn (x)concentratearoundpnand(1−p)n,respectively. Furthermore,
+ −
sinceγ =(cid:12) (cid:12)1 −p(cid:12) (cid:12)⩽µ′,wecanusestandardconcentrationinequalitiestoshowthatwithprobabilityatleast1−O(n−10),
2 δ
therandomvectorxsatisfiestheevent
(cid:26) (cid:12) (cid:12) (cid:12) (cid:12) (cid:27)
E = x∈{±}n :(cid:12) (cid:12)n +(x) − 1(cid:12) (cid:12)⩽2µ′ and (cid:12) (cid:12)n −(x) − 1(cid:12) (cid:12)⩽2µ′ .
(cid:12) n 2(cid:12) δ (cid:12) n 2(cid:12) δ
(cid:12) (cid:12)
NowsinceP(x∈E)=1−O(n−10)andsince(cid:12)⟨Xˆ (G),xxT⟩(cid:12)⩽n2,itisnothardtoseethat
(cid:12) r (cid:12)
(cid:104) (cid:105) (cid:104) (cid:12) (cid:105)
E ⟨Xˆ (G),xxT⟩ =E ⟨Xˆ (G),xxT⟩(cid:12)x∈E ±o(1), (24)
r r (cid:12)
(cid:104) (cid:12) (cid:105)
sowecanfocusonstudyingE ⟨Xˆ (G),xxT⟩(cid:12)x∈E .
r (cid:12)
Nowfixx∈E andconditionontheeventthatx=x. FromthedefinitionofE itisnothardtoseethatthereisx′ ∈{±}n
satisfying(cid:80) x′ =0and
i∈[n] i
|D |⩽4µ′n,
x,x′ δ
where
D =|i∈[n]:x ̸=x′|.
x,x′ i i
NowconstructarandomgraphG′asfollows:
• Ifi,j ∈[n]\D ,i.e.,ifx =x′ andx =x′ ,thenwelet{i,j}∈G′ifandonlyif{i,j}∈G.
x,x′ i i j j
• Ifeitheri∈D orj ∈D thenweputtheedge{i,j}inG′withprobability d (cid:0) 1+ εx′ ·x′(cid:1) .
x,x′ x,x′ n 2 i j
• Theevents({i,j}∈G′) aremutuallyindependent.
i,j∈[n]
Itisnothardtoseethat:
• G′ ∼SBM (x′),and
n,2,d,ε
4µ′
• G ≈δ G′,i.e.,GcanbeobtainedfromG′byaddingorremovingedgesincidenttoatmost4µ′nvertices.
δ
Since4µ′ ⩽ 4 µ ⩽µ ,itfollowsfromTheoremC.2that
δ 100 δ δ
(cid:104) (cid:12) (cid:105)
E ⟨Xˆ (G),x′x′T⟩(cid:12)x=x ⩾C ·n2. (25)
r (cid:12) δ
Nownoticethat
⟨Xˆ (G),xxT⟩=⟨Xˆ (G),x′x′T⟩+⟨Xˆ (G),xxT−x′x′T⟩,
r r r
and
(cid:12) (cid:12) (cid:12)⟨Xˆ r(G),xxT−x′x′T⟩(cid:12) (cid:12) (cid:12)⩽(cid:13) (cid:13) (cid:13)Xˆ r(G)(cid:13) (cid:13) (cid:13) ∞·(cid:13) (cid:13)xxT−x′x′T(cid:13) (cid:13) 1 ⩽1·(cid:13) (cid:13)xxT−x′x′T(cid:13) (cid:13) 1
⩽2∥x−x′∥ ·n=4|D |·n⩽16µ′ ·n2.
1 x,x′ δ
Combiningthiswith(25),weget
(cid:104) (cid:12) (cid:105) 84
E ⟨Xˆ (G),xxT⟩(cid:12)x=x ⩾(C −16µ′)·n2 ⩾ C ·n2.
r (cid:12) δ δ 100 δ
24Multi-ViewStochasticBlockModels
Nowsincethisistrueforallx∈E,weconcludethat
(cid:104) (cid:12) (cid:105) 84
E ⟨Xˆ (G),xxT⟩(cid:12)x∈E ⩾(C −16µ′)·n2 ⩾ C ·n2,
r (cid:12) δ δ 100 δ
wherethelastinequalityistruebecauseµ′ = 1 min{µ ,C }. Combiningtheabovewith(24),wecandeducethatforn
δ 100 δ δ
largeenough,wehave
(cid:104) (cid:105) 3
E ⟨Xˆ (G),xxT⟩ ⩾ C ·n2.
r 4 δ
ThefollowinglemmatakesthealgorithmofLemmaC.3andappliesasymmetrizationargumentinordertoget“apositive
correlationattheedgelevel”.
LemmaC.4(Pair-wiseweakrecoveryforsufficientlybalanced2communitiesstochasticblockmode). Letn,d,ε,p,x=
(x i) i∈[n]andG∼SBM n,2,d,ε(x)beasinTheorem3.1. Letδ = ε2 4d −1>0andletγ =(cid:12) (cid:12)1
2
−p(cid:12) (cid:12)betheunbalancedness
ofx. Thereexistconstantsµ′ >0andC′ >0andarandomizedpolynomial-timealgorithm17Xˆ takingGasinputand
δ δ sb
producingamatrixXˆ (G)∈[−1,+1]n×nsuchthatif
sb
γ ⩽µ′ ,
δ
thenforeveryi,j ∈[n]withi̸=j,wehave
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
C′ ⩽E Xˆ (G) (cid:12)x =x −E Xˆ (G) (cid:12)x ̸=x .
δ sb ij (cid:12) i j sb ij (cid:12) i j
Proof. Letµ ,C andXˆ beasinTheoremC.2,andletµ′ =C′ = 1 min{µ ,C }. LemmaC.3showsthat
δ δ r δ δ 100 δ δ
(cid:104) (cid:105) 3
E ⟨Xˆ (G),xxT⟩ ⩾ C ·n2.
r 4 δ
ThealgorithmXˆ (G)canbeobtainedby“symmetrizing”thealgorithmXˆ asfollows: Letσbea(uniformly)random
sb r
permutation[n]→[n]andletG bethegraphobtainedfromGbyσ-permutingitsvertices,i.e.,welettheedge18{σ ,σ }
σ i j
belongtoG ifandonlyif{i,j}∈G. WedefinethematrixXˆ (G)∈[−1,+1]nasfollows:
σ sb
Xˆ (G) =Xˆ (G ) .
sb ij r σ σiσj
Inotherwords,weapplytherandompermutationσtographG,weapplythealgorithmXˆ ,andthenweapplytheinverse
r
ofthepermutationontheresultingmatrix.
DuetothesymmetryoftheSBMdistribution,itisnothardtoseethatforeveryi,j ∈[n]withi̸=j,wehave
E(cid:104) Xˆ (G) ·x x (cid:105) = (cid:88) E(cid:104) Xˆ (G) ·x x (cid:12) (cid:12)σ =i′,σ =j′(cid:105) ·P(σ =i′,σ =j′)
sb ij i j sb ij i j (cid:12) i j i j
i′,j′∈[n]:
i′̸=j′
= (cid:88) E(cid:104) Xˆ (G ) ·x x (cid:12) (cid:12)σ =i′,σ =j′(cid:105) · 1
r σ σiσj i j (cid:12) i j n(n−1)
i′,j′∈[n]:
i′̸=j′
( =∗) 1 (cid:88) E(cid:104) Xˆ (G) ·x x (cid:12) (cid:12)σ =i′,σ =j′(cid:105)
n(n−1) r σiσj σi σj (cid:12) i j
i′,j′∈[n]:
i′̸=j′
17ThesubscriptsbinXˆ standsfor“sufficientlybalanced”.
sb
18Forsimplicity,Wedenoteσ(i)asσ .
i
25Multi-ViewStochasticBlockModels
= 1 (cid:88) E(cid:104) Xˆ (G) ·x x (cid:105)
n(n−1) r i′j′ i′ j′
i′,j′∈[n]:
i′̸=j′
= 1 E(cid:104) ⟨Xˆ (G),xxT⟩(cid:105) − 1 (cid:88) E(cid:104) Xˆ (G) ·x2(cid:105)
n(n−1) r n(n−1) r ii i
i∈[n]
1 3 1
⩾ · C ·n2−
n(n−1) 4 δ n−1
3
⩾ C −o(1),
4 δ
where(∗)followsfromthesymmetryoftheSBMdistributionunderthesimultaneouspermutationofverticesandlabels.
Nownoticethat
(cid:104) (cid:105) (cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
E Xˆ (G) ·x x =E Xˆ (G) (cid:12)x =x ·P(x =x )−E Xˆ (G) (cid:12)x ̸=x ·P(x ̸=x )
sb ij i j sb ij (cid:12) i j i j sb ij (cid:12) i j i j
=E(cid:104) Xˆ (G) (cid:12) (cid:12)x =x (cid:105) ·(cid:0) p2+(1−p)2(cid:1) −E(cid:104) Xˆ (G) (cid:12) (cid:12)x ̸=x (cid:105) ·(2p(1−p))
sb ij (cid:12) i j sb ij (cid:12) i j
(cid:104) (cid:12) (cid:105) (cid:18) 1 (cid:19) (cid:104) (cid:12) (cid:105)(cid:18) 1 (cid:19)
=E Xˆ (G) (cid:12)x =x · +2γ2 −E Xˆ (G) (cid:12)x ̸=x −2γ2
sb ij (cid:12) i j 2 sb ij (cid:12) i j 2
1 (cid:104) (cid:12) (cid:105) 1 (cid:104) (cid:12) (cid:105)
⩽ ·E Xˆ (G) (cid:12)x =x +2γ2− ·E Xˆ (G) (cid:12)x ̸=x +2γ2,
2 sb ij (cid:12) i j 2 sb ij (cid:12) i j
(cid:12) (cid:12)
whereinthelastinequalityweusedthefactthat(cid:12)Xˆ (G) (cid:12)⩽1. Wecandeducethatfornlargeenough,wehave
(cid:12) sb ij(cid:12)
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
E Xˆ (G) (cid:12)x =x −E Xˆ (G) (cid:12)x ̸=x
sb ij (cid:12) i j sb ij (cid:12) i j
(cid:104) (cid:105) 3
⩾2E Xˆ (G) ·x x −8γ2 ⩾ C −o(1)−8γ2 ⩾C ,
sb ij i j 2 δ δ
wherethelastinequalityfollowsfromthefactthat
(cid:18)
1
(cid:19)2
C
8γ2 ⩽8µ′2 ⩽8 C ⩽ δ .
δ 100 δ 100
BypickingC′ =C ,thelemmafollows.
δ δ
Nowweturntoshowthatifxissufficientlyunbalanced,thenthereexistsanefficientalgorithmthatachievespair-wise
weakrecovery.
LemmaC.5(Pair-wiseweakrecoveryforsufficientlyunbalanced2communitiesstochasticblockmode). Letn,d,ε,p,x=
(x ) andG ∼ SBM (x)beasinTheorem3.1. Furtherassumethatp ∈ [0,1]. Letδ = ε2d −1 > 0andlet
γ
=i i∈ (cid:12) (cid:12)1[n −] p(cid:12) (cid:12)betheunbaln a, n2 c,d e, dε
nessofx,andletµ′ beasinLemmaC.4. ThereexistsaconstantC′′
>4
0andarandomized
2 δ δ
polynomial-timealgorithm19Xˆ takingGasinputandproducingamatrixXˆ (G)∈[−1,+1]n×nsuchthatif
su su
1
γ ⩾ µ′ ,
2 δ
thenforeveryi,j ∈[n]withi̸=j,wehave
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
C′′ ⩽E Xˆ (G) (cid:12)x =x −E Xˆ (G) (cid:12)x ̸=x .
δ sb ij (cid:12) i j sb ij (cid:12) i j
Proof. Forthesakeofsimplicitywemayassumewithoutlossofgeneralitythatp> 1,i.e.,p= 1 +γ andhence“+1”is
2 2
thelargercommunityinexpectation.
19ThesubscriptsuinXˆ standsfor“sufficientlyunbalanced”.
su
26Multi-ViewStochasticBlockModels
Foreveryi,j ∈[n],let
deg (i)=|{v ∈[n]\{i,j}:{i,v}∈G}|
̸=j
bethenumberofverticesin[n]\{i,j}whichareadjacenttoiinG.
LetC >0bealargeenoughconstant(tobechosenlater)andlet
xˆ(j)(G)= 1 (cid:0) deg (i)−d(1−2/n)(cid:1) ·(cid:113)(cid:12) (cid:12)deg (i)−d(1−2/n)(cid:12) (cid:12)⩽C(cid:121)∈[−1,1],
i C ̸=j ̸=j
anddefinethematrixXˆ (G)∈[−1,+1]n×nas:
su
Xˆ (G) =xˆ(j)(G)·xˆ(i)(G).
su ij i j
It is not hard to see that given (x ,x ), the random variables deg (i) and deg (j) are conditionally independent.
i j ̸=j ̸=i
Therefore,xˆ(j)(G)andxˆ(i)(G)areconditionallyindependentgiven(x ,x ),hence
i j i j
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
E Xˆ (G) (cid:12)x ,x =E xˆ(j)(G)(cid:12)x ,x ·E xˆ(i)(G)(cid:12)x ,x
su ij (cid:12) i j i (cid:12) i j j (cid:12) i j
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
=E xˆ(j)(G)(cid:12)x ·E xˆ(i)(G)(cid:12)x .
i (cid:12) i j (cid:12) j
Now,foreveryv ∈[v]\{i,j},wehave
(cid:20) (cid:18) (cid:19)(cid:12) (cid:21)
P({i,v}∈G|x i)=E[P({i,v}∈G|x i,x v)|x i]=E nd 1+ 1 2ε·x ix v (cid:12) (cid:12) (cid:12)x i
(cid:18) (cid:19) (cid:18) (cid:19)
d 1 d 1
= 1+ ε·x E[x ] = 1+ ε·(p−(1−p))
n 2 i v n 2
d
= (1+εγx ) .
n i
Therefore,theconditionaldistributionofdeg (i)givenx isBinomial(cid:0) n−2, d (1+εγx )(cid:1) ,hence
̸=j i n i
E(cid:2)
deg
̸=j(i)(cid:12)
(cid:12)x
i(cid:3)
=(n−2)·
nd
(1+εγx i) ,
andso
(cid:20) (cid:12) (cid:21)
E C1 (cid:0) deg ̸=j(i)−d(1−2/n)(cid:1)(cid:12) (cid:12) (cid:12)x i = d(1−2/ Cn)·εγx i . (26)
Ontheotherhand,bytheCauchy-Schwarzinequality,wehave
(cid:20)(cid:12) (cid:12)(cid:12) (cid:21)
E (cid:12) (cid:12) (cid:12)xˆ( ij)(G)− C1 (cid:0) deg ̸=j(i)−d(1−2/n)(cid:1)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)x i
(cid:20) (cid:12) (cid:21)
⩽E C1 (cid:12) (cid:12)deg ̸=j(i)−d(1−2/n)(cid:12) (cid:12)·(cid:113)(cid:12) (cid:12)deg ̸=j(i)−d(1−2/n)(cid:12) (cid:12)>C(cid:121)(cid:12) (cid:12) (cid:12)x i
⩽ C1 E(cid:104)(cid:0) deg ̸=j(i)−d(1−2/n)(cid:1)2 (cid:12) (cid:12) (cid:12)x i(cid:105)1/2 ·E(cid:104) (cid:113)(cid:12) (cid:12)deg ̸=j(i)−d(1−2/n)(cid:12) (cid:12)>C(cid:121)2 (cid:12) (cid:12) (cid:12)x i(cid:105)1/2
⩽ C1 E(cid:104)(cid:0) deg ̸=j(i)−d(1−2/n)(cid:1)2 (cid:12) (cid:12) (cid:12)x i(cid:105)1/2 ·P(cid:0)(cid:12) (cid:12)deg ̸=j(i)−d(1−2/n)(cid:12) (cid:12)>C (cid:12) (cid:12)x i(cid:1)1/2 ,
andbyChebychev’sinequality,wehave
E(cid:104)(cid:0) deg (i)−d(1−2/n)(cid:1)2 (cid:12) (cid:12)x (cid:105)
P(cid:0)(cid:12)
(cid:12)deg
̸=j(i)−d(1−2/n)(cid:12)
(cid:12)>C
(cid:12)
(cid:12)x
i(cid:1)⩽ ̸=j
C2
(cid:12) i
,
27Multi-ViewStochasticBlockModels
hence
(cid:20)(cid:12) (cid:12)(cid:12) (cid:21) E(cid:104)(cid:0) deg (i)−d(1−2/n)(cid:1)2 (cid:12) (cid:12)x (cid:105)
E (cid:12) (cid:12) (cid:12)xˆ( ij)(G)− C1 (cid:0) deg ̸=j(i)−d(1−2/n)(cid:1)(cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12)x i ⩽ ̸=j C2 (cid:12) i . (27)
Nownoticethat
E(cid:104)(cid:0) deg (i)−d(1−2/n)(cid:1)2 (cid:12) (cid:12)x (cid:105) =E(cid:104)(cid:0)(cid:0) deg (i)−d(1−2/n)(1+εγx )+(εγx )d(1−2/n)(cid:1)(cid:1)2 (cid:12) (cid:12)x (cid:105)
̸=j (cid:12) i ̸=j i i (cid:12) i
( ⩽∗) E(cid:104) 2(cid:0) deg (i)−d(1−2/n)(1+εγx )(cid:1)2 +2((εγx )d(1−2/n))2 (cid:12) (cid:12)x (cid:105)
̸=j i i (cid:12) i
⩽2E(cid:104)(cid:0) deg (i)−d(1−2/n)(1+εγx )(cid:1)2 (cid:12) (cid:12)x (cid:105) +2d2ε2γ2,
̸=j i (cid:12) i
where(∗)istruebecause(a+b)2 ⩽2a2+2b2foralla,b∈R.
Nowsincetheconditionaldistributionofdeg (i)givenx isBinomial(cid:0) n−2, d (1+εγx )(cid:1) ,itsconditionalexpectation
̸=j i n i
isequaltod(1−2/n)(1+εγx )anditsconditionalvarianceisequalto
i
E(cid:104)(cid:0) deg (i)−d(1−2/n)(1+εγx )(cid:1)2 (cid:12) (cid:12)x (cid:105) =(n−2)(cid:18) d (1+εγx )(cid:19) ·(cid:18) 1− d (1+εγx )(cid:19)
̸=j i (cid:12) i n i n i
⩽d(1+εγ)⩽2d.
Therefore,
E(cid:104)(cid:0) deg (i)−d(1−2/n)(cid:1)2 (cid:12) (cid:12)x (cid:105) ⩽4d+2d2ε2γ2,
̸=j (cid:12) i
andhencefrom(27)weget
E(cid:20)(cid:12) (cid:12) (cid:12) (cid:12)xˆ( ij)(G)− C1 (cid:0) deg ̸=j(i)−d(1−2/n)(cid:1)(cid:12) (cid:12) (cid:12) (cid:12)(cid:12) (cid:12) (cid:12) (cid:12)x i(cid:21) ⩽ 4d+ C2d 22ε2γ2 .
Combiningthiswith(26)weget
(cid:104) (cid:12) (cid:105) d(1−2/n)·εγx (cid:18) d+d2ε2γ2(cid:19)
E xˆ(j)(G)(cid:12)x = i ±O
i (cid:12) i C C2
d(1−2/n)(cid:18) (cid:18) 1+dε2γ2(cid:19)(cid:19)
= εγx ±O .
C i C
Let
(cid:18) (cid:19)
1
C =C˜(1−2/n) dε+ ,
εµ′
δ
forsomelargeenoughconstantC˜ ⩾1tobechosenlater. Wehave
(cid:18) 1(cid:19) (cid:32) 1 (cid:33) (cid:18) εµ′ (cid:19) (cid:18) εγ(cid:19)
O ⩽O =O δ ⩽O ,
C C˜(1−2/n)/(εµ′) C˜ C˜
δ
wherethelastinequalityfollowsfromγ ⩾ µ′ δ. Furthermore,
2
(cid:18) dε2γ2(cid:19) (cid:18) dε2γ2 (cid:19) (cid:18) εγ2(cid:19) (cid:18) εγ(cid:19)
O ⩽O ⩽O ⩽O .
C C˜(1−2/n)·dε C˜ C˜
Weconcludethat
(cid:104) (cid:12) (cid:105) d(1−2/n)(cid:18) (cid:18) εγ(cid:19)(cid:19) d(1−2/n)εγ (cid:18) (cid:18) 1(cid:19)(cid:19)
E xˆ(j)(G)(cid:12)x = εγx ±O = x ±O
i (cid:12) i C i C˜ C i C˜
28Multi-ViewStochasticBlockModels
(cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:18) (cid:19)(cid:19)
dεγ 1 1
= x ±O =C′′γ x ±O ,
C˜(cid:16)
dε+ 1
(cid:17) i C˜ δ i C˜
εµ′
δ
where
dε2 4(1+δ) 4(1+δ)µ′
C′′ = = = δ .
δ C˜(cid:16) dε2+ 1 (cid:17) C˜(cid:16) 4(1+δ)+ 1 (cid:17) C˜(4(1+δ)µ′ +1)
µ′ µ′ δ
δ δ
NoticehowC′′dependsonlyonδ.
δ
Finally,
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
E Xˆ (G) (cid:12)x ,x =E xˆ(j)(G)(cid:12)x ·E xˆ(i)(G)(cid:12)x
su ij (cid:12) i j i (cid:12) i j (cid:12) j
(cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:18) (cid:19)(cid:19)
1 1
=C′′γ x ±O ·C′′γ x ±O
δ i C˜ δ j C˜
(cid:18) (cid:18) (cid:19)(cid:19)
1
=(C′′γ)2 x x ±O ,
δ i j C˜
andso
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105) (cid:18) (cid:18) 1(cid:19)(cid:19)
E Xˆ (G) (cid:12)x =x −E Xˆ (G) (cid:12)x ̸=x =(C′′γ)2 2±O .
su ij (cid:12) i j su ij (cid:12) i j δ C˜
BychoosingC˜ tobeanabsoluteconstantwhichislargeenough,weget
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105) (C′′µ′)2
E Xˆ (G) (cid:12)x =x −E Xˆ (G) (cid:12)x ̸=x ⩾(C′′γ)2 ⩾ δ δ ,
su ij (cid:12) i j su ij (cid:12) i j δ 4
wherethelastinequalityistruebecauseweassumethatγ ⩾ µ′ δ.
2
NowwecanleverageLemmaC.4andLemmaC.5inordertoproveTheorem3.1.
ProofofTheorem3.1. Letγ,µ′,C′,C′′beasinLemmaC.4andLemmaC.5.
δ δ δ
Wewillfirstdistinguishbetweenthesufficientlybalancedandsufficientlyunbalancedcasesbycountingthenumberof
edgeswhichareincidenttoarandomsublinear(butsufficientlyhigh)setofvertices. Letm=⌈n3/4⌉andletIbearandom
subsetof[n]ofsizem.
Letdeg(I)bethenumberofedgesinGfromIto[n]\I. Wehave
(cid:88) (cid:88)
E[deg(I)|I]= E[ {i,j}∈G ]= E[P({i,j}∈G|x ,x )]
i j
(cid:74) (cid:75)
i∈I,j∈[n]\I i∈I,j∈[n]\I
(cid:20) (cid:18) (cid:19)(cid:21) (cid:18) (cid:19)
(cid:88) d 1 d (cid:88) 1
= E 1+ εx x = 1+ εE[x x ]
n 2 i j n 2 i j
i∈I,j∈[n]\I i∈I,j∈[n]\I
(cid:18) (cid:19)
d (cid:88) 1
= 1+ ε(P(x =x )−P(x ̸=x ))
n 2 i j i j
i∈I,j∈[n]\I
(cid:18) (cid:19)
=
d (cid:88)
1+
1 ε(cid:0) p2+(1−p)2−2p(1−p)(cid:1)
n 2
i∈I,j∈[n]\I
(cid:18) (cid:19)
dm(n−m) 1
= 1+ ε(2p−1)2
n 2
=
dm(n−m)(cid:0) 1+2εγ2(cid:1) =(1±o(1))dn3/4(cid:0) 1+2εγ2(cid:1)
.
n
SothealgorithmXˆ isdefinedasfollows:
29Multi-ViewStochasticBlockModels
• Ifdeg(I)⩾dn3/4(cid:18) 1+2ε(cid:16) 3µ 4′ δ(cid:17)2(cid:19) weapplythealgorithmXˆ suonthesubgraphG([n]\I)ofGinducedonn\I
anddefine
(cid:40)
Xˆ (G([n]\I)) ifi,j ∈[n]\I,
Xˆ(G) = su ij
ij
0 otherwise.
• Ifdeg(I)<dn3/4(cid:18) 1+2ε(cid:16) 3µ 4′ δ(cid:17)2(cid:19) weapplythealgorithmXˆ sbonthesubgraphG([n]\I)ofGinducedonn\I
anddefine
(cid:40)
Xˆ (G([n]\I)) ifi,j ∈[n]\I,
Xˆ(G) = sb ij
ij
0 otherwise.
Bystandardconcentrationinequalities,wecanshowthat:
• Ifγ <
µ′
δ
,thenwithprobability1−o(1)wehavedeg(I)<dn3/4(cid:18) 1+2ε(cid:16) 3µ′ δ(cid:17)2(cid:19)
andsoweapplythealgorithm
2 4
Xˆ whichwillsucceedinachievingpair-wiseweakrecoveryaccordingtoLemmaC.4,assumingi,j ∈[I].
sb
• Ifγ ⩾ µ′ δ ,thenwithprobability1−o(1)wehavedeg(I)⩾dn3/4(cid:18) 1+2ε(cid:16) 3µ′ δ(cid:17)2(cid:19) andsoweapplythealgorithm
2 4
Xˆ whichwillsucceedinachievingpair-wiseweakrecoveryaccordingtoLemmaC.5,assumingi,j ∈[I].
ub
• If µ′ δ <γ <µ′ ,thenitfollowsfromLemmaC.4andLemmaC.5thatitdoesnotmatterwhichalgorithmweapply
2 δ
becausebothofthemachievepair-wiseweakrecovery,assumingi,j ∈[I].
Nowforanyi,j ∈[n],theprobabilityofpickinganyoftheminIisvanishinglysmall. WeconcludethatthealgorithmXˆ
satisfiestheguaranteessoughtinTheorem3.1.
DeferredproofofSection4 FirstweproveFact4.3.
(cid:18) (cid:113) (cid:19)
ProofofFact4.3. Letp∈[k]befixed. ByChernoff’sbound,withprobabilityatleast1−n20, 1− 400klogn n ⩽
n k
(cid:18) (cid:113) (cid:19)
∥c (z)∥2 ⩽ 1+ 400klogn n,hencethepropertyfollowsbyaunionbound.
p n k
NextweproveFact4.5.
Proof. Foreachℓ∈[t],define
(cid:104) (cid:12) (cid:105)
E Xˆ(G ) (cid:12)f (z) =f (z) =:C∗
ℓ ij (cid:12) ℓ i ℓ j ℓ
andlet
(cid:88)
C∗ = C .
ℓ
ℓ∈[t]
Byindependenceoftheobservations,theinequalityofFact4.5forthecasecasez = z followswithanapplicationof
i j
Hoeffding’sinequality.
Thecasez ̸=z needsabitmorework. ByTheorem3.1,foreachℓ∈[t],wehave
i j
(cid:104) (cid:12) (cid:105)
E Xˆ(G ) (cid:12)f (z) =f (z) =C∗,
ℓ ij (cid:12) ℓ i ℓ j ℓ
30Multi-ViewStochasticBlockModels
and
(cid:104) (cid:12) (cid:105)
E Xˆ(G ) (cid:12)f (z) ̸=f (z) ⩽C∗−C .
ℓ ij (cid:12) ℓ i ℓ j ℓ ℓ
Therefore,
(cid:104) (cid:12) (cid:105) (cid:104) (cid:12) (cid:105)
E Xˆ(G ) (cid:12)z ̸=z =E Xˆ(G ) (cid:12)f (z) =f (z) ·P(f (z) =f (z) |z ̸=z )
ℓ ij (cid:12) i j ℓ ij (cid:12) ℓ i ℓ j ℓ i ℓ j i j
(cid:104) (cid:12) (cid:105)
+E Xˆ(G ) (cid:12)f (z) ̸=f (z) ·P(f (z) ̸=f (z) |z ̸=z )
ℓ ij (cid:12) ℓ i ℓ j ℓ i ℓ j i j
1
⩽C∗−C · ,
ℓ ℓ 2
andso
 (cid:12) 
(cid:12) (cid:18) (cid:19)
E (cid:88) Xˆ(G ℓ) ij (cid:12) (cid:12) (cid:12)z i ̸=z j⩽ (cid:88) C ℓ∗−C ℓ· 1 2 =C∗− C 2·t .
ℓ∈[t] (cid:12) ℓ∈[t]
TheinequalityofFact4.5forcasez ̸=z followswithanotherapplicationofHoeffding’sinequality.
i j
NowweproveLemma4.8.
ProofofLemma4.8. Let A be a (p,q)-representative and A be a (p′,q)-representative, for p,p′ ∈ [k]. It suffices to
i j
showthatifp=p′ then∥A −A ∥2 ⩽n/kandotherwise∥A −A ∥2 >n/k.Thennoq-representativeindexremains
i j i j
unassignedattheendofstep1. Bythereversetriangleinequality
|∥A −A ∥−∥c (z)−c (z)∥|
i j p p′
⩽∥A −A −c (z)+c (z)∥
i j p p′
⩽∥A −c (z)∥+∥A −c (z)∥
i p j p′
⩽2n·e−q·C2·t.
For p = p′ we have ∥c (z)−c (z)∥ = 0 and by choice of t, the first inequality follows. For p ̸= p′ we have
p p′
∥c (z)−c (z)∥⩾ n(2−o(1))sincezisbalancedandthesecondinequalityfollowsagainbychoiceoft.
p p′ k
31