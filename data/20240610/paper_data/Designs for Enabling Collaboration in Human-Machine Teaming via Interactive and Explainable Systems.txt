Designs for Enabling Collaboration in Human-Machine Teaming via Interactive
and Explainable Systems
RohanPaleja1,2, MichaelMunje1, KimberleeChang2, ReedJensen2, MatthewGombolay1
1GeorgiaInstituteofTechnology
2MITLincolnLaboratory
rohan.paleja@gatech.edu,michaelmunje@gatech.edu,chestnut@ll.mit.edu,rjensen@ll.mit.edu,
matthew.gombolay@cc.gatech.edu
Abstract to support the development of mental models [Paleja et al.,
2021],andthelackofbidirectionalcommunication(i.e.,un-
Collaborative robots and machine learning-based
clear how humans can “tell” a machine online to perform a
virtual agents are increasingly entering the human
desired behavior) [Wright et al., 2022]. In this paper, we
workspace with the aim of increasing productivity
transitionfromtheconventionalapproachofcraftinganHMT
and enhancing safety. Despite this, we show in a
solutionthataimsforflawlessout-of-the-boxperformanceto
ubiquitous experimental domain, Overcooked-AI,
a paradigm where end-users can actively interact with and
thatstate-of-the-arttechniquesforhuman-machine
programAIteammates,fosteringamoredynamicanddevel-
teaming (HMT), which rely on imitation or rein-
opmental interaction between humans and AI. Specifically,
forcement learning, are brittle and result in a ma-
weexploreenablinghumanstoperformuser-specificmodifi-
chineagentthataimstodecouplethemachineand
cations to a collaborative AI’s interpretable policy represen-
human’sactionstoactindependentlyratherthanin
tationacrossrepeatediterationsofteamingepisodesandpro-
a synergistic fashion. To remedy this deficiency,
vide a set of design guidelines to support team development
wedevelopHMTapproachesthatenableiterative,
inHMTdrawnfromalarge-scaleuserstudy.
mixed-initiative team development allowing end-
Recently, data-driven techniques (e.g., imitation and rein-
users to interactively reprogram interpretable AI
forcement learning) have become popular in HMT, allow-
teammates. Our 50-subject study provides sev-
ing for the generation of collaborative agent behavior with-
eral findings that we summarize into guidelines.
outcumbersomemanualprogramming[Strouseetal.,2021;
While all approaches underperform a simple col-
Carroll et al., 2019]. However, these prior works utilize
laborative heuristic (a critical, negative result for
opaque, black-box models, limiting human’s ability to de-
learning-based methods), we find that white-box
velopasharedmentalmodelandmaintainsituationalaware-
approaches supported by interactive modification
ness [Mathieu et al., 2000], crucial for high-performance
can lead to significant team development, outper-
teaming [Salas et al., 1992]. We posit that successful, real-
forming white-box approaches alone, and black-
worldHMTisnotfeasiblewithouttheuseofwhite-boxmeth-
boxapproachesareeasiertotrainandresultinbet-
ods, especially in safety-critical domains such as healthcare
ter HMT performance highlighting a tradeoff be-
and manufacturing. Furthermore, collaborative interactions
tween explainability and interactivity versus ease-
with machines have often lacked the ability to effectively
of-training. Together, these findings present three
learnwithandadapttohumanteammatesinreal-time[Lake
important directions: 1) Improving the ability to
etal.,2016]. Inadhochuman-humanteams,effectiveteam-
generatecollaborativeagentswithwhite-boxmod-
ingisoftendevelopedthroughaniterativeprocess[Tuckman,
els, 2) Better learning methods to facilitate col-
1965]. Bi-directional communication is often a key compo-
laboration rather than individualized coordination,
nentofthisprocess, enablingthedevelopmentofsuccessful
and3)Mixed-initiativeinterfacesthatenableusers,
coordination strategies [Salas et al., 2008]. In our work, we
whomayvaryinability,toimprovecollaboration.
build towards such a team development paradigm in HMT
1 Introduction by 1) creating a pathway of bi-directional communication,
Successful human-machine teaming (HMT) has long been
soughtafterforitswideutilityacrosspotentialapplications, DISTRIBUTIONSTATEMENTA.Approvedforpublicrelease. Distributionis
unlimited. ThismaterialisbaseduponworksupportedbytheUnderSecretaryofDe-
ranging from virtual agents such as “clippy” that provide
fenseforResearchandEngineeringunderAirForceContractNo.FA8702-15-D-0001.
on-demand support for improving documents to embodied Anyopinions,findings,conclusionsorrecommendationsexpressedinthismaterialare
robotichealthcareaidesthatcanprovidedoctorswithahelp- thoseoftheauthor(s)anddonotnecessarilyreflecttheviewsoftheUnderSecretaryof
DefenseforResearchandEngineering.DeliveredtotheU.S.GovernmentwithUnlim-
ing hand [Muoio, 2019]. While promising, achieving fluent
itedRights,asdefinedinDFARSPart252.227-7013or7014(Feb2014).Notwithstand-
HMTischallengingbecauseinteractionswithhumanscanbe inganycopyrightnotice,U.S.GovernmentrightsinthisworkaredefinedbyDFARS
incrediblycomplexduetothediversityacrossusers[Mataric, 252.227-7013orDFARS252.227-7014asdetailedabove.Useofthisworkotherthan
asspecificallyauthorizedbytheU.S.Governmentmayviolateanycopyrightsthatexist
2018], human teammates benefit from explainable systems inthiswork.
4202
nuJ
7
]OR.sc[
1v30050.6042:viXrautilizinginterpretablepolicyrepresentationsasamechanism creased situational awareness over a teammate’s behavior in
to allow users to understand their machine teammates and anHMTsetting[Palejaetal.,2021]. Whiletree-basedmod-
allowing for explicit teammate policy modification through els can provide users insight into the model, the complex-
aninterface(userscanmodifythemachine’stree-basedpol- ity of the tree-based model limits its utility [Lipton, 2018].
icy via a GUI), and 2) allowing for the process of iterative While wenote this asa potentialweakness of utilizingtree-
mixed-initiative team development through repeated team- based models, effective state representations can provide a
ingepisodes. Webelievethisparadigmisnecessarybecause tradeoff between granular control and tree depth. Accord-
human-partnered systems need explainable components and ingly, we design our trees to reason over a state-space with
adaptablesystems. Weprovidethefollowingcontributions: high-level binary features and multi-step macro-actions, ex-
• We provide a case study regarding prior work in HMT panded on below. Furthermore, in our work, we explore
[Carrolletal., 2019;Strouseetal., 2021], findingthatthe a paradigm where a user can directly modify and visual-
generated machine behavior is unable to adapt to human- ize a tree-based AI teammate the user is interacting with
preferredstrategies,andthathighperformanceistypically after a teaming episode. Prior work in explainable debug-
drivenbyindependentmachineactionsratherthancollab- ging[Kuleszaetal.,2015]androbotics[Paxtonetal.,2017;
oration,whichcanultimatelyresultinahigherteamscore. Fogli et al., 2022] has explored similar paradigms, creating
• Wecreate anovel InterpretableMLarchitecture tosupport interactive systems that allowend-users tomodify agentbe-
thecreationoftree-basedcooperativeagentpoliciesviare- haviortoincreaseperformance,buthasnotexploreddeploy-
inforcement learning and a GUI to allow users to modify ingtree-basedmodelstrainedviaRLinanHMTsetting.
theAI’sbehaviortotheirspecifications. Thiscapabilityis Overcooked-AI – Overcooked-AI [Carroll et al., 2019] is a
promising, enabling end-users to “go under-the-hood” of testbed to evaluate human-AI interaction. Here, two agents
machine learning models and tune affordances or interac- aretaskedwithcreatinganddeliveringasmanysoupsaspos-
tivelyanditerativelyreprogrambehavior. sible within a given time. Achieving a high score requires
• We conduct a 50-participant between-subjects user study agents to navigate a kitchen and repeatedly complete a set
assessingtheeffectsofinterpretabilityandinteractivepol- of sequential high-level actions, including collecting ingre-
icy modification across repeated interactions with an AI. dients, placing ingredients in pots, cooking ingredients into
Wesummarizeourstudyfindingsintoasetofdesignguide- a soup, collecting a dish, getting the soup, and delivering
linestosupportfutureHMTresearch. it. Both players receive the same score increase upon de-
livering the soup. We modify the original Overcooked-AI
2 Preliminaries game to be a simultaneous-move game as opposed to the
Here, we introduce prior work in HMT and Explainable AI, original formulation of allowing agents to perform actions
Overcooked-AI,Tuckman’sModel,andMarkovGames. asynchronously. Thismodificationpreventsthecollaborative
Human-MachineTeaming–ThefieldofHMTisconcerned scoremetricfrombeingdominatedbysuper-humanAIspeed,
with understanding, designing, and evaluating machines for causingtheoverallscoretobemorereliantuponeffectivecol-
use by or with humans [Chen and Barnes, 2014]. A popu- laborationandstrategy.Weprovidedetailsaboutthestateand
lartechniquethathasbeenusedtoproducecollaborativeAI actionspacebelowandcompletedetailsintheappendix.
agents is Reinforcement Learning (RL) [Mnih et al., 2013], State-Space: Policiesreasonoverasemanticallymeaningful
where researchers have concentrated efforts on reducing the featurespaceasopposedtopixelspace,detailingtheobjects
dissimilarity between synthetic human training partners and eachagentisholding,potstatuses,andcounterobjects. This
testingwithhumanend-users.Approachesthathaveachieved statespaceallowsforlearninganinterpretabletree-basedpol-
somesuccessincludeutilizinghumangameplaydatatofine- icythatcanbeunderstoodandmanipulatedbyend-users.
tune simulated training partners to behave more human-like Action-Space: Insteadofusingcardinalactions,weallowthe
[Carroll et al., 2019], which can be expensive, and training AI to utilize macro-actions that can accomplish high-level
withadiverse-skilledpopulationofsyntheticpartnerstocre- objectives such as ingredient collection, ingredient place-
ateanagentthatcanbettergeneralizetonon-expertend-users ment, and soup serving. Macro-actions are planned using
[Strouseetal.,2021],whichmaybiastheAIteammatetoex- an A* planner, and we perform dynamic replanning at each
hibitindividualizedstrategies,aswedisplayinSection3. We timestep. Constructing trees on a higher level of abstraction
note our work focuses on an interaction different from AI- resultsinsmallertreesthatareeasiertointerpret.
assisteddecision-makingordecisionsupport. Here,ahuman Tuckman’sModel–Tuckmandescribesthedifferentstages
and an agent must collaborate across a series of timesteps, that a team goes through before reaching high performance,
aimingtomaximizeamultifacetedjointobjectivefunction. including“Forming”,“Storming”,“Norming”and“Perform-
Explainable AI – xAI is concerned with understanding and ing,”oftenseeingadropinperformanceasteammembersac-
interpreting the behavior of AI systems [Linardatos et al., climate,followedbyariseasteammembersunderstandhow
2021]. Inourwork,wefollowrecenttrendsthatshowblack- tocollaborate.Assumingthathuman-machineteamswillfol-
box methods paired with local explanations can be harm- low similar stages to human-human teams, this paper looks
ful [Rudin, 2018] and utilize interpretable, white-box tree- into how we can support human-machine teams in reaching
based models in a multi-agent sequential decision-making thePerformingstage,wheretheteamisachievingitsfullpo-
problem. These models have been shown to be beneficial tential and exhibiting the highest level of cooperation. We
inimprovingtheuser’sabilitytosimulateadecision-making provideadepictionofthesestagesaspartofFigure2.
model[Tambwekaretal.,2021]andprovidinguserswithin- MarkovGame–WeformulateoursettingasaMarkovGame[Littman,1994],definedbyasetofglobalstates,S ,S S,
1 2
∈
a set of actions, A ,A A, transition function, T : S
1 2
∈ ×
A
1
A
2
S. andrewardfunctionr
i
:S A
i
R. Agent
× 7→ × 7→
i aims to maximize its discounted reward R = T γtrt,
i t=0 i
whereγ [0,1]isadiscountfactor. Fortraining,weutilize
agent-age∈ ntcollaborativetraining,whichtrainstwPoseparate
(a) We display the human-preferred collaboration behavior that
agents jointly via single-agent PPO. We utilize PantheonRL
focusesonminimizingagentmovementandefficienthandoffsusing
[Sarkaretal.,2022]fortrainingouragents,incorporatingour
themiddlecounter.ThisunsuccessfulHMTreceivesascoreof0.
noveltree-basedarchitecture(Section4.1)intothecodebase.
3 AGapinTeamingPerformance
In this section, we present two examples to display a gap in
the quality of AIs in HMT. Specifically, we look at two re-
cent approachesto producecollaborative AI agents[Strouse
(b) We display a human adapting to an AI-preferred suboptimal
etal.,2021;Carrolletal.,2019]. Weargueanddisplaythat teamingstrategy,whereagentsactindividually.Thisindividualized
theAIstrainedviatheseapproachesarerigidandexhibitin- coordinationresultsinminorsuccess,achievingalowscoreof40.
dividualizedbehaviors,missingoutoncollaborativeteaming
strategiesthatcanultimatelyresultinhigherteamscores. We Figure 1: Case Study in Human-Machine Teaming with Different
requireAIagentsthatcaneffectivelyreachaconsensuswith TeamingStrategies.Itisclearthatthemodelsarenotrobusttomul-
humansonateamingstrategythatultimatelyresultsinhigh tiplestrategiesofplayandcanresultinagentsperformingnonsen-
performance.Incaseswherethehumanhasapreferredstrat- sicalbehavior(e.g.,stuckinplace).
egy,theAIteammateshouldbeabletosupportsaidstrategy.
we program two intelligent deterministic heuristics: In the
In Figure 1, we display the Coordination Ring scenario.
first,eachagentactscompletelyindividually,cookingsingle-
A simple collaboration strategy (which we term “human-
ingredient dishes and serving. In the second, agents share
preferred”) in this domain is to utilize the counter to con-
ingredients, which costs additional timesteps, but are able
tinuouslypassobjects, minimizingagentmovementthrough
to successfully cook mixed ingredient dishes. We find that
efficienthandoffs. Totestasetofcollaborationstrategies,we
thecollaborationstrategyachievesa408teamscore,approx-
utilize agents publicly available from Carroll et al. In Fig-
imately30%morescorecomparedtotheindividualizedstrat-
ure 1, we display a frame-by-frame of the human-preferred
egyof306. WefindthattrainedpoliciesunderFicticiousCo-
coordinationstrategy(Figure1a)andAI-preferredcoordina-
Play [Strouse et al., 2021] exhibit similar team score to that
tionstrategy(Figure1b),whichwasastrategywhereagents
of the individual coordination strategy and further, find that
actindividuallytocollectingredientsandplacetheminpots.
realhumanend-userscollaboratingwiththeseagentsareun-
The latter behavior was inferred through repeated play with
abletofarsurpasstheindividualstrategyscore. AsStrouseet
thepublicly-availableAI.Withthehuman-preferredstrategy,
al. trainsanagenttoworkwellwithapopulationofagents,
the AI agent freezes for the majority of the game, creating
whereapproximatelyathirdofthediverse-skilledpopulation
an extremely frustrating and low-performing AI teammate.
ofagentsusedintrainingarecompletelyrandomagents,we
In this scenario, the human (green) picks up an ingredient
posit that the teammate agent must compensate and exhibit
andplacesitonthecounteratthestartofthegame. TheAI
individualizedbehavior,limitingthealgorithm’sabilitytoef-
agent(blue),unfamiliarwiththisteamingstrategy,freezesfor
fectivelylearneffectiveteamcoordinationstrategies.
approximately 80% of the remaining episode before finally
Thus,intherestofthepaper,welooktoexplorexAItech-
placing an onion in the pot. With the AI-preferred strategy,
niques as a mechanism for closing this gap and allowing
thehumanisabletosuccessfullyteamwiththeAI,witheach
agents within a human-machine team to facilitate collabo-
agent retrieving and placing ingredients while moving in a
rativestrategiesthatoutperformtheindividualizedandrigid
clockwisemotion,butthestrategyisnotoptimalorwhatthe
behaviorstrainedagentsassume.
humanprefers. Weprovidevideosinthesupplementary. As
the AI produced in Carroll et al. is created via RL against 4 Methodology
human-like AI teammates, the generated behavior may not
Inthissection,wefirstpresentourarchitecturefortrainingin-
be ideal for the current teammate, especially if the current
terpretableAIteammates.Wethenpresentacontextualprun-
teammate’s preferred strategy was not present in the orig-
ingalgorithm,allowingforease-of-trainingandenhancedin-
inal training dataset used to create human-like AI training
terpretability for neural tree-based models. We display an
partners. This highlights a need for systems that take initial
overviewofourtrainingprocedureaspartofFigure2.
trainedpoliciesandadaptthemtohuman-preferredbehavior.
In a second example, we utilize the Optional Collabora- 4.1 InterpretableDiscreteControlTrees
tiondomain,displayedinFigure4b,whichisalsoutilizedin Wecreateaninterpretablemachinelearningarchitecture,In-
our human-subjects experiment. This domain was designed terpretableDiscreteControlTrees(IDCTs),thatcanbeused
toincentivizecollaboration,wherecreatingmixed-ingredient directlywithRLtoproduceinterpretableteammatepolicies.
dishes facilitated by agents passing ingredients across the Below,webrieflydetailourarchitecture,aswellasadvance-
central counter will result in a higher score per dish. Here, mentstoenhanceease-of-trainingandinterpretability.Architecture Our IDCTs are based on differentiable deci-
siontrees(DDTs)[Sua´rezandLutsko,1999]–aneuralnet-
work architecture that takes the topology of a decision tree
(DT). DDTs contain decision nodes and leaf nodes; how-
ever, each decision node within the DDT utilizes a sig-
moid activation function (i.e., a “soft” decision) instead of
a Boolean decision (i.e., a “hard” decision). Each decision
node, i, is represented by a sigmoid function, displayed as
y = (1+exp( α(w⃗T⃗x b ))) 1. As this representation
i − i − i −
is difficultto interpret, [Palejaet al., 2022]presented differ-
entiable crispification, which recasts each decision node to
split upon a single dimension of the input feature and trans-
lates the outcome of a decision node so that the outcome is
aBooleandecisionratherthanasetofprobabilities. This,in
turn,allowsforaninterpretableforwardpropagationthrough
themodelthattracesdownasinglebranchofatreeaswellas
gradientflowaffordedbythestraight-throughtricktoupdate
parametersoftheneuraltreemodel. Weutilizethisapproach
Figure 2: Here, we provide an overview of the steps to produce a
tolearninterpretabletree-basedteammatepoliciesviaRL.
collaborativeAIteammatewithaninterpretablepolicyandthepro-
WeinitializeourIDCTstobesymmetricDTswithN de-
l posedpolicymodificationschemeevaluatedinouruserstudy.
cisionleavesandN 1decisionnodes. Eachdecisionleaf
l
− Weprovidefurtherdetailsandanalgorithmforcontextual
isrepresentedbyasparsecategoricalprobabilitydistribution
pruninginthesupplementarymaterial. This, inturn, allows
overactions. Ateachtimestep,astatevariableispropagated
usthebenefitoftraininglargetree-basedmodels,greatlyim-
through each decision node, split on a single decision rule,
provingease-of-training,whilestillbeingabletosimplifythe
withtheoutputbeingaBooleancausingthedecisiontopro-
resultantmodeltoasmaller,equivalentrepresentation.
ceed via the left or right branch until arrival at a leaf node.
At each leaf node, we sample from the respective distribu- 4.2 ModifyinganInterpretablePolicy
tiontoproduceamacro-action(e.g.,inOvercooked-AI,“get While the above architecture can be used alongside RL to
anonion”or“placeingredientoncounter”). Further,weim- produceacollaborativeAIpolicy,theresultmaynotactually
provemodelpredictabilitybyapplyinganL1normlossover behelpfulorwhatthehumanwants. Humans,whenteaming
leafnodedistributionstoensuresparsity,penalizinghighen- withmachines, shouldbeabletointuitivelyupdatewhatthe
tropyactiondistributionsataleaf. Importantly,theresultant robot has learned or change it based upon preferences that
representationaftertrainingisthatofasimpledecisiontree may evolve over time. Such is critical in the positive devel-
withcategoricalprobabilitydistributionsateachleafnode. opment of coordination strategies and is associated with the
Contextual Pruning As we focus on creating agents that calibrationoftrust,assignmentofroles,anddevelopmentof
mustcooperatewithandbeinterpretedbyhumans,wemust asharedmentalmodel. Assuch,weproposeapolicymodifi-
limit the size of our tree-based models to a certain depth to cationschemethatallowstheusertorepeatedlyteamwithan
promoteuserunderstanding. Analogoustothe“lotteryticket AImaintaininganIDCTpolicy,visualizethecurrentbehav-
hypothesis” in network training that supports the practical- ior in tree form, and modify its AI’s behavior. The iterative
ity of employing large models [Frankle and Carbin, 2018], process generated through this scheme can facilitate a feed-
asmalltreewithalimitednumberofsub-trees(lotterytick- back loop, allowing for the possibility of team development
ets)maynothavetherepresentationalpowertolearnahigh- andimprovedHMTperformanceoverteamingepisodes.
performing policy. Thus, the ability to effectively train ID- Wetermourmodificationschemehuman-ledpolicymod-
CTsisatoddswithmaintaininguserreadabilityandsimulata- ification. We provide humans with an explicit pathway
bility. Following recent literature in neural network pruning to “communicate” with an AI after each teaming interac-
[Liangetal.,2021],wedesignapost-hoccontextualpruning tion through a GUI, with capabilities displayed in Figure 3.
algorithmthatallowsustosimplifylargeIDCTmodelswhile Within this interface, users start with the pre-trained collab-
preciselyadheringtomodelbehaviorbyaccountingfor: orative AI IDCT policy and can modify the AI’s behavior
by creating a new tree structure that may vary in what state
1. Boundariesofavariable’sstatedistribution:Weutilize
features appear in the decision nodes, actions taken in leaf
the minimum and maximum of each variable’s range to
nodes, and the respective probabilities of actions within the
parseimpossiblesubspacesofatree.
leafnode. Itisimportanttonotethatusersarelimitedtoex-
2. Node hierarchy: Ancestor nodes for a specific decision
pandingthetreetoadepthoffour(i.e.,amaxof16leaves).
node may have already captured a specific splitting cri-
terion and, thus, may lead to redundancy. By detecting
5 Human-SubjectsStudy
redundancies,wecanprunesubspacesofthetree.
Here, we discuss our between-subjects user study that seeks
WhileutilizingdeterministicAIpoliciesmaybeeasiertounder- to understand how users interact with an AI across repeated
standforusers,wefoundthesemodelscouldnotconvergetosimilar playunderdifferentfactors.Below,weintroduceourresearch
performanceasthestochastic-leafIDCTpoliciesduringtraining. questions,provideadescriptionoftheindependentvariablesin its interpretable tree form prior to the next interaction.
Throughoutthiscondition,theAI’spolicyisstatic.
4. IV1-C4: Static Policy - Black-Box: After interacting
withtheagent,theuserdoesnotseetheAI’spolicy. Here,
the AI policy is the same as IV1-C3, but the human has
lostaccesstodirectinsightintothemodel.
5. IV1-C5: Static Policy - Fictitious Co-Play: [Strouse et
al., 2021]: User teams with an AI maintaining a static
black-box, neural network (NN) policy trained across a
diversepartnerset. Asthisisabaseline,weutilizeanNN
ratherthanthehuman-understandableIDCTpolicymodel
usedinallotherconditions(IV1:C1-4).
Table1:AcomparisonacrossdifferentIV1factors.
Explicit PolicyChanges Base
Approaches Interaction AcrossIterations White-Box Policy
IV1-C1 ✓ ✓ ✓ IDCT
IV1-C2 ✗ ✓ ✓ IDCT
IV1-C3 ✗ ✗ ✓ IDCT
Figure 3: Users have several capabilities in creating an effective IV1-C4 ✗ ✗ ✗ IDCT
IV1-C5 ✗ ✗ ✗ NN
teammate, including modifying the tree structure by adding or re-
moving decision nodes (top), changing state features the tree is
ForIV2,weconsiderthefollowingdomainsinFigure4:
conditionedon(left),andmodifyingactionsand/ortheirrespective
probabilitiesatleafnodes(right). 1. IV2-D1: Forced Coordination: Users team with an AI
that is separated by a barrier and must pass over onions
and procedure, include brief descriptions of the behaviors andplatesinatimelymanner. Inthisdomain, agentsare
learnedbycollaborativeAI,andfinallydiscussourfindings. forcedtocollaborate.
ResearchQuestionsThepresentedresearchquestionsbelow
2. IV2-D2: Optional Collaboration: In this domain, the
seektounderstandchangesinoverallhuman-machineteam-
teamcanoperateindividuallyorcollaboratively. Thisdo-
ing performance and performance changes across repeated
main has increased complexity, both with respect to the
gameplay. The latter question pivots from an episodic atti-
size of the domain and the types of soups that can be
tudeofteamingtoalonger-termgauge,allowingustostudy
cooked.Collaborationisincentivizedthroughahigherre-
theprocessofadaptationinHMT.
ward for mixed-ingredient dishes (combining onions and
1. RQ1:HowdoesHMTperformancevaryacrossfactors? tomatoes)oversingle-ingredientdishes.
2. RQ2: Howdoesteamdevelopmentvaryacrossfactors?
We describe the reward scheme and domains further in the
IndependentVariablesWehavetwoindependentvariables, appendix.Importantly,weassessscenariosinwhichtheteam
IV1: theteamingmethod,andIV2: thedomain. ForIV1,we is rewarded more for collaboration than teammate-agnostic
considerthefollowingconditions(abbreviatedbyIV1-C): or independent behavior, which represents the highest level
1. IV1-C1: Human-Led Policy Modification: After inter-
ofteamwork[Kolbeinssonetal.,2019].
actingwiththeagent(oneteamingepisode),theusercan
modifythepolicyviatheGUI,allowingtheusertoupdate
decisionnodesandactionnodesinthetreeaswellastune
affordances. Uponcompletion, theusercanvisualizethe
updatedpolicyinitstreeformpriortothenextinteraction.
2. IV1-C2: AI-Led Policy Modification: After interacting
with the agent, the AI utilizes recent gameplay to fine-
(a)ForcedCoordination (b)OptionalCollaboration
tuneahumangameplaymodelviaBehavioralCloningand
Figure4:Thisfigureshowsthetwodomainsusedinourexperiment.
performs reinforcement learning for five minutes to opti-
mizeitsownpolicytobettersupportthehumanteammate.
For each domain, we train an IDCT policy via agent-
Uponcompletionofpolicyoptimization, theusercanvi-
agentcollaborativetraining(forconditionsIV1:C1-4)anda
sualizetheupdatedAIpolicyinitsinterpretabletreeform
NNpolicyfollowingthepopulation-basedtrainingschemein
prior to the next interaction. This is similar to HA-PPO
Strouseetal. (forconditionIV1:C5). InIV2-D1,theIDCT
[Carrolletal.,2019],adaptedtoanonlinesetting.
policy (utilized in IV1:C1-4) converged to a policy with an
3. IV1-C3: StaticPolicy-Interpretability: Afterinteract- averagerewardof315.22 14.59,andtheneuralnetworkpol-
ing with the agent, the user can visualize the AI’s policy icyconvergedtoanaverag± erewardof403.16 16.08over50
±
teaming simulations with the synthetic human teammate the
We limit the online optimization time for the AI teammate to
policy was trained with. In IV2-D2, the IDCT policy con-
fiveminutestocreateafeasibleuser-study. Onlineoptimizationvia
RLbecomeschallengingasalimitedsamplescanbeobtainedinthis
vergedtoapolicywithanaveragerewardof171.46 18.89,
±
time,andthepolicyisnotguaranteedtoimprove.Incaseswherethe andtheneuralnetworkpolicyconvergedtoanaveragereward
policydegrades,weusetheoriginalpolicypriortooptimization. of 295.02 1.86. Thus, a consequent confound due to the
±currentdifferenceinperformancecapabilitiesbetweeninter-
pretable vs. black-box models is that the NN policy outper-
formstheIDCTpolicyinbothdomains. Thisdisplaysaneed
forimprovingoptimizationalgorithmsforinterpretablemod-
els. Howeverimportantly,whiletheinitialsimulatedperfor-
manceofinterpretablemodelsmayunderperformblack-box
models,theabilityforhumanstounderstandmachinebehav-
iorandimproveuponbehaviormayallowtheseapproachesto
competeorevenoutperformblack-boxNNmodels. Wecan
alsocomparetotheheuristicpoliciespresentedinSection3,
(a)PerformanceDatainIV2-D1:ForcedCoordination
observingthatthetrainingperformanceoftheIDCTandNN
policies in IV2-D2 underperform the collaborative heuristic
(408 vs. 295.02 and 171.46). We provide the trained IDCT
policies for each domain in the appendix, finding that after
pruning,theAIpolicyhastwoandthreeleaves,respectively.
Procedure: A participant is first randomly placed into one
of the five conditions in IV1. The participant starts with a
pre-experiment survey collecting demographic information,
experiencewithvideogamesanddecisiontrees,andtheBig
Five Personality Questionnaire [Chmielewski and Morgan, (b)PerformanceDatainIV2-D2:OptionalCollaboration
2013]. Afterward, a participant conducts a brief tutorial in
Figure5:Gameplayscoresfromusersacrossteamingiterations.
Overcooked with a random AI agent, improving the user’s
understandingofgamecontrolsandtheassignedtask. Once
non-parametric test if the data failed to meet these assump-
completed, the primary experimentation begins. Users will
tions. WedisplayourobjectivefindingsinFigure5.
team with an AI four times in each domain (randomly or-
dered),andaretoldthattheirgoalistomaximizetheirscore RQ1:TeamCoordinationPerformance:Inanalyzingteam
inthelastteaminginteraction,the“performanceround.”Af- reward, we find trends with respect to the maximum reward
tereachteaminginteraction,inthefirstthreefactors,theuser participants obtained within a domain across all teaming it-
will modify and visualize the AI’s policy (IV1-C1), the AI erations(Figure6). UtilizingaFriedman’stest, wefindthat
willoptimizeitsownpolicyproceededbyuservisualization
thereisasignificantdifferenceacrossdomains(χ2(1)=46.08,
(IV1-C2),ortheuserwillsolelyviewthepolicy(IV1-C3).In p<0.001). Accordingly,weanalyzethedomainsseparately.
IV1-C4andIV1-C5,astheAIisblack-box(perceivedtobe In IV2-D1, a Kruskal-Wallis Test was conducted to ana-
black-boxinIV1-C4andtrulyblack-boxinIV1-C5),transi- lyze differences in maximum performance obtained across
tionarypagesareshowntotheparticipant, providingthema teaming paradigms, finding a significant effect (χ2(4) =
pausebeforetheyteamwiththeagentagain. Uponcomple- 20.146,p < 0.001). We conduct post-hoc pairwise compar-
tionofthecondition-specific(orlackof)actions,userscom- isons, utilizing Dunn’s test, and find that IV1-C5 (Fictitious
pleteaNASA-TLXWorkloadSurvey. Afterusershavecom- Co-Play) is significantly better than IV1-C1 (p < 0.001),
pletedadomain,providinguswithfourepisodesofteaming IV1-C2 (p < 0.01), IV1-C3 (p < 0.01), and IV1-C4
data and workload assessments, we administer several post- (p<0.05).EventhoughFictitiousCo-Play(IV1-C5)outper-
studyscales, includingtheHuman-RobotCollaborativeFlu- formedthetree-basedmodels,likelyduetoitsabilitytocon-
encyAssessment[Hoffman,2019],InclusionofOtherinthe verge to a higher-performance teaming policy, it is interest-
Self scale [Aron et al., 1991], and Godspeed Questionnaire ing that Human-Led Policy Modification (IV1-C1) has sev-
[Bartnecketal.,2019].Uponcompletionofthetwodomains, eral participants that outperform the maximum performance
theexperimentconcludes. Bycomparingconditions,wegain ofIV1-C5inteamingiterationsthreeandfour(Figure5a).
insightsintotheadvantagesofinterpretabilityandinteraction. In IV2-D2, a Kruskal-Wallis Test was conducted to an-
alyze differences in participant teaming performance across
5.1 Results conditions,findingasignificanteffect(χ2(4) = 29.922,p <
Our experiment is a 5 (teaming method; between-subjects) 0.001). We conduct post-hoc pairwise comparisons, utiliz-
2 (no. of domains; within-subjects) 4 (no. of repeated ing Dunn’s test, and find that IV1-C5 (Ficticious Co-Play)
× evaluations;within-subjects)mixed-fact× orialexperiment. We is significantly better than IV1-C2 (p < 0.001), IV1-C3
recruited 50 participants under an IRB-approved protocol, (p<0.001),andIV1-C4(p<0.001),andIV1-C1(Human-
whose ages range from 18 to 32 (Mean age: 24.14; Std. Led Policy Modication) is significantly better than IV1-C2
Dev.: 4.10; 46% Female, 52% Male, 2% Non-Binary), with (p<0.05),IV1-C3(p<0.05),andIV1-C4(p<0.05). For
participants randomly assigned to each of the factor levels, white-box AI teammates (IV1:C1-3), the latter finding dis-
with ten total subjects per level. The duration of the experi- plays the benefit of Human-Led Policy Modification in im-
mentwas70.98 19.71minutes. Ourdatawasmodeledas provingHMTperformanceforinterpretablemodels.
±
a full-factorial, between-subjects ANOVA. We test for nor- These findings display that 1) white-box approaches sup-
mality and homoschedascity and employed a corresponding ported with policy modification can outperform white-boxingtrend. WebelievethisrelatestotheFormingandStorm-
ingstages,whereteammembersarestilldevelopingeffective
strategies to coordinate. As the last iteration shows an im-
provementinperformance,wehypothesizethattheteamwas
shifting into the Norming stage. In future, it would be in-
terestingtoevaluatealargernumberofteamingiterationsto
see if the behavior would continue to trend upward. This is
anadditionalchallengethatrequiresfurtherresearchasmore
iterationsrequiremoreexperimentationtimeandresources.
Subjective Findings: InIV2-D1, we findthat users did not
findanysubjectivedifferencestowardtheteaminginteraction
across conditions. In IV2-D2 (Figure 6), we find that users
findcollaborationwithAIsunderconditionIV1-C2andIV1-
C4,onaverageaslessfluentthanIV1-C1(p<0.01,p<0.01),
and IV1-C4 as less fluent than IV1-C5 (p < 0.05). Users
also trusted the AI and perceived the AI contributed more
in IV1-C5 than in IV1-C2 (p<0.05, p<0.05) and IV1-C4
(p < 0.05,p < 0.05). Furthermore,theusersviewedtheAI
morepositivelyinIV1-C1andIV1-C5thaninbothIV1-C2
(p<0.05, p<0.05) and IV1-C4 (p<0.05, p<0.01). As seen
through these findings, the ability to interact with an inter-
pretablemodelisperceivedsignificantlybetteracrossseveral
measures. Weprovideourcodebaseforourexperimenthere.
Figure6:MaximumRewardAcrossEachCondition(top)andSub-
jectiveRatingsforPositiveTraitsandTeamFluency(bottom). Design Guidelines: Achieving fluent HMT is challenging.
Towardthisgoal,wespecifythefollowingguidelines.
approaches alone, 2) black-box models can outperform 1. The creation of white-box learning approaches that can
white-boxapproachesinHMT,and3)bycomparingIV1-C3 produceinterpretablemodelsthatachievecompetitiveini-
toIV1-C4,interpretabilityaffordedviatreevisualizationsdid tialperformancetothatofblack-boxmodels. Thisguide-
notprovideanydirectobjectivebenefits. Finally,inOptional lineiscriticaltoprovidinghumanswithinHMTwiththe
Collaboration,acrossallconditionsweseethatHMTscores subjective benefits of white-box models, objective bene-
are not near that of the collaborative heuristic, displaying a fits of black-box models, and the ability to interact with
gapthatmustbeaddressedtoachieveeffectiveHMT. policiestofacilitateteamdevelopment.
RQ2:TeamDevelopment:InanalyzingRQ2,welookatthe 2. Thedesignoflearningschemestosupportthegeneration
changeinrewardacrossiterationsonetofourandrelateour of collaborative AI behaviors rather than individual co-
findingstoTuckman’smodel. UtilizingaFriedman’stest,we ordination. We need techniques that avoid converging to
find that there is a difference across domains (χ2(1)=20.48, thelocalmaximaofindividualcoordinationandscenarios
p<0.001). Thus,weanalyzethedomainsseparately. thatallowforproperlyevaluatingthisobjective.
We conduct an analysis to see which conditions facilitate 3. The creation of mixed-initiative interfaces that enable
teamdevelopment. InIV2-D1,weseethatnoneofthecondi- users,whomayvaryinabilityandexperience,toimprove
tionsresultsinasignificantimprovementinteamingperfor- teamcollaborationacrossandwithininteractions. Aswe
mance over repeated iterations. In IV2-D2, we see IV1-C1 foundalargediversityinperceivedusabilityofourinter-
(p < 0.01) and IV1-C2 (p < 0.01) significantly improve face(findinganaverageusabilityscoreof58.25 27.61,
±
over repeated teaming interactions. The improving interac- withmanyusersfindingtheinterfacegood(>75)andoth-
tions can be connected to the Norming stage in team devel- ers poor (<35)), effective interfaces are vital in shifting
opment,whereteamsbegintodevelopastrategyandshared fromonlyasubsetofusersbenefitingfrompolicymodifi-
mentalmodels.WeseethatconditionsthatfacilitateNorming cationtoallusersbeingabletocreateeffectiveteammates.
havetheattributeofpolicyadaptationandarewhite-box. 4. The evaluation of teaming in a larger number of interac-
Next,weanalyzewhetherdifferentperson-specificfactors tions. Asagentsaredeployedwithhumans, teamperfor-
allowHMTtoimprovemorequicklythanothers. InIV2-D1, mancewillchangeovertime,goingthroughatransientpe-
we find that conscientiousness is trending in its correlation riod before reaching maximal performance. Understand-
with faster improvement (0.05 < p < 0.1). In IV2-D2, we ing this process of team development will be essential in
findthatthereisasignificanteffectinhowmuchparticipants creatinghigh-performancehuman-machineteams.
improveandtheirfamiliaritywithTrees(F(1)=7.448,p<
0.01). These findings signify that positive interaction with 6 Conclusion
interpretablemodelsmaybemorebeneficialtothosewithan This work investigates repeated interactions with machine
engineeringbackgroundandspecificpersonalitytraits. learning models within a sequential decision-making HMT
Finally,wedetectaninterestingtrendinIV2-D1underthe paradigm. WepresentakeygapinHMT,displayingthatcur-
IV1-C1condition.Weseeadropinperformancebetweenthe rent methods do not facilitate human-machine collaboration
first teaming iteration and later iterations, followed by a ris- to the fullest. We find that human-led policy modificationallows for a human-machine team to achieve higher perfor- [Lakeetal.,2016] Brenden M. Lake, Tomer D. Ullman,
mancethanwhite-boxmodelswithoutthiscapability. How- Joshua B. Tenenbaum, and Samuel J. Gershman. Build-
ever, as interpretable models are more difficult to generate, ing machines that learn and think like people. CoRR,
FictitiousCo-Playisabletobettersupporthigh-performance abs/1604.00289,2016.
HMT. Given these mixed findings, future work must focus
[Liangetal.,2021] Tailin Liang, John Glossner, Lei Wang,
on developing better interpretableML approaches to support
Shaobo Shi, and Xiaotong Zhang. Pruning andquantiza-
the generation of white-box teammates, study the modality
tionfordeepneuralnetworkacceleration: Asurvey. Neu-
of communication between agents and humans, and explore
rocomputing,461:370–403,2021.
mechanisms to allow HMT to scale beyond individual coor-
dinationandtowardeffectivecollaboration. [Linardatosetal.,2021] Pantelis Linardatos, Vasilis Pa-
pastefanopoulos, andS.Kotsiantis. Explainableai: Are-
7 Acknowledgements view of machine learning interpretability methods. En-
tropy,23,2021.
ThisworkwassupportedbyaresearchgrantfromMITLin-
colnLaboratory. [Lipton,2018] Zachary C Lipton. The mythos of model
interpretability: In machine learning, the concept of in-
terpretability is both important and slippery. Queue,
References
16(3):31–57,2018.
[Aronetal.,1991] A. Aron, E. Aron, Michael Tudor, and
[Littman,1994] Michael L Littman. Markov games as a
Greg Nelson. Close relationships as including other in
frameworkformulti-agentreinforcementlearning. InMa-
the self. Journal of Personality and Social Psychology,
chine learning proceedings 1994, pages 157–163. Else-
60:241–253,1991.
vier,1994.
[Bartnecketal.,2019] Christoph Bartneck, Dana Kulic,
E.Croft,andSusanaZoghbi. Godspeedquestionnairese- [Mataric,2018] Maja J. Mataric. Robots for the people, by
ries. 2019. thepeople:Personalizinghuman-machineinteraction.Sci.
Robotics,3(21),2018.
[Carrolletal.,2019] Micah Carroll, Rohin Shah, Mark K.
Ho,T.Griffiths,S.Seshia,P.Abbeel,andA.Dragan. On [Mathieuetal.,2000] John E Mathieu, Tonia S Heffner,
theutilityoflearningabouthumansforhuman-aicoordi- Gerald F Goodwin, Eduardo Salas, and Janis A Cannon-
nation. InNeurIPS,2019. Bowers. The influence of shared mental models on team
processandperformance. Journalofappliedpsychology,
[ChenandBarnes,2014] Jessie Y.C. Chen and M. Barnes.
85(2):273,2000.
Human–agent teaming for multirobot control: A review
of human factors issues. IEEE Transactions on Human- [Mnihetal.,2013] Volodymyr Mnih, Koray Kavukcuoglu,
MachineSystems,44:13–29,2014. David Silver, Alex Graves, Ioannis Antonoglou, Daan
Wierstra, and Martin A. Riedmiller. Playing atari with
[ChmielewskiandMorgan,2013] Michael S. Chmielewski
deepreinforcementlearning. CoRR,abs/1312.5602,2013.
and Theresa A. Morgan. Five-Factor Model of Person-
ality,pages803–804. SpringerNewYork,NewYork,NY, [Muoio,2019] Dave Muoio. Diligent robotics collects $3m
2013. seed funding, launches autonomous robot assistants for
[Foglietal.,2022] Daniela Fogli, L. Gargioni, Giovanni hospitals,Oct2019.
Guida, and Fabio Tampalini. A hybrid approach to user- [Palejaetal.,2021] Rohan Paleja, Muyleng Ghuy, Nadun
oriented programming of collaborative robots. Robotics Ranawaka Arachchige, Reed Jensen, and Matthew Gom-
Comput.Integr.Manuf.,73:102234,2022. bolay. The utility of explainable ai in ad hoc human-
[FrankleandCarbin,2018] Jonathan Frankle and Michael machine teaming. In M. Ranzato, A. Beygelzimer,
Carbin. The lottery ticket hypothesis: Finding sparse, Y.Dauphin,P.S.Liang,andJ.WortmanVaughan,editors,
trainableneuralnetworks. arXiv: Learning,2018. AdvancesinNeuralInformationProcessingSystems,vol-
ume34,pages610–623.CurranAssociates,Inc.,2021.
[Hoffman,2019] Guy Hoffman. Evaluating fluency in hu-
man–robot collaboration. IEEE Transactions on Human- [Palejaetal.,2022] Rohan R. Paleja, Yaru Niu, Andrew
MachineSystems,49:209–218,2019. Silva, Chace Ritchie, Sugju Choi, and Matthew Craig
Gombolay. Learninginterpretable, high-performingpoli-
[Kolbeinssonetal.,2019] AriKolbeinsson,ErikLagerstedt,
cies for autonomous driving. Robotics: Science and Sys-
and Jessica Lindblom. Foundation for a classification of
temsXVIII,2022.
collaborationlevelsforhuman-robotcooperationinman-
ufacturing. Production&ManufacturingResearch,7:448 [Paxtonetal.,2017] Chris Paxton, Andrew T Hundt, Felix
–471,2019. Jonathan, Kelleher Guerin, and Gregory Hager. Costar:
Instructingcollaborativerobotswithbehaviortreesandvi-
[Kuleszaetal.,2015] Todd Kulesza, Margaret M. Burnett,
sion. 2017 IEEE International Conference on Robotics
Weng-Keen Wong, and Simone Stumpf. Principles of
andAutomation(ICRA),pages564–571,2017.
explanatorydebuggingtopersonalizeinteractivemachine
learning. Proceedings of the 20th International Confer- [Rudin,2018] C.Rudin. Stopexplainingblackboxmachine
enceonIntelligentUserInterfaces,2015. learning models for high stakes decisions and use inter-pretable models instead. Nature Machine Intelligence,
1:206–215,2018.
[Salasetal.,1992] E.Salas,T.Dickinson,SharolynA.Con-
verse, and S. Tannenbaum. Toward an understanding of
teamperformanceandtraining. 1992.
[Salasetal.,2008] E. Salas, N. Cooke, and M. Rosen. On
teams,teamwork,andteamperformance: Discoveriesand
developments. Human Factors: The Journal of Human
FactorsandErgonomicSociety,50:540–547,2008.
[Sarkaretal.,2022] BidiptaSarkar,AditiTalati,AndyShih,
and Sadigh Dorsa. Pantheonrl: A marl library for dy-
namic training interactions. In Proceedings of the 36th
AAAIConferenceonArtificialIntelligence(DemoTrack),
2022.
[Strouseetal.,2021] DJStrouse,KevinR.McKee,MattM.
Botvinick,EdwardHughes,andRichardEverett. Collab-
orating with humans without human data. pages 14502–
14515,2021.
[Sua´rezandLutsko,1999] AlbertoSua´rezandJamesFLut-
sko. Globally optimal fuzzy decision trees for classifica-
tionandregression. IEEETransactionsonPatternAnaly-
sisandMachineIntelligence,21(12):1297–1311,1999.
[Tambwekaretal.,2021] Pradyumna Tambwekar, Andrew
Silva, Nakul Gopalan, and Matthew Craig Gombolay.
Specifying and interpreting reinforcement learning poli-
ciesthroughsimulatablemachinelearning. 2021.
[Tuckman,1965] Bruce W. Tuckman. Developmental se-
quence in small groups. Psychological bulletin, 63:384–
99,1965.
[Wrightetal.,2022] Julia L. Wright, Shan G. Lakhmani,
and Jessie Y. C. Chen. Bidirectional communications in
human-agentteaming:Theeffectsofcommunicationstyle
andfeedback. InternationalJournalofHuman–Computer
Interaction,38:1972–1985,2022.Supplementary for Designs for Enabling Collaboration in Human-Machine
Teaming via Interactive and Explainable Systems
RohanPaleja1,2, MichaelMunje1, KimberleeChang2, ReedJensen2, MatthewGombolay1
1GeorgiaInstituteofTechnology
2MITLincolnLaboratory
rohan.paleja@gatech.edu,michaelmunje@gatech.edu,chestnut@ll.mit.edu,rjensen@ll.mit.edu,
matthew.gombolay@cc.gatech.edu
1 Overcooked-AI foranitemplacedintoapot,3forausefuldishpickup,and5
forasouppickup. InOptionalCollaboration,forthereward
Overcooked-AI [Carroll et al., 2019] is a testbed to evalu-
scheme,wegivearewardscoreof50foramixed-ingredient
atehuman-AIinteractionandhasbeenusedacrossnumerous
dish,30forasingleingredientdish,3foranitemplacedinto
works[Strouseetal.,2021;Fontaineetal.,2021]. Here,two
apot,3forausefuldishpickup,and5forasouppickup.
agentsaretaskedwithcreatinganddeliveringasmanysoups
as possible within a given time. Achieving a high score re-
2 AdditionalIDCTModelDetails
quiresagentstonavigateakitchenandrepeatedlycompletea
set of sequential high-level actions, including collecting in- Here, we provide additional model details for the proposed
gredients, placing ingredients in pots, cooking ingredients InterpretableDiscreteControlTree(IDCT).
into a soup, collecting a dish, getting the soup, and deliv-
ering it. Both players receive the same score increase upon 2.1 Architecture
delivering the soup. We modify the original Overcooked-AI
OurIDCTsarebasedondifferentiabledecisiontrees(DDTs)
game to be a simultaneous-move game as opposed to the
[Sua´rez and Lutsko, 1999] – a neural network architecture
original formulation of allowing agents to perform actions
thattakesthetopologyofadecisiontree(DT).DDTscontain
asynchronously. Thismodificationpreventsthecollaborative
decision nodes and leaf nodes; however, each decision node
scoremetricfrombeingdominatedbysuper-humanAIspeed,
withintheDDTutilizesasigmoidactivationfunction(i.e.,a
causingtheoverallscoretobemorereliantuponeffectivecol-
“soft”decision)insteadofaBooleandecision(i.e., a“hard”
laborationandstrategy.
decision). Eachdecisionnode,i,isrepresentedbyasigmoid
We utilize two domains, Forced Coordination and Op- function, displayed as y = 1 , where w⃗
tional Collaboration, displayed in Figure 4 of the main pa- i 1+exp( −α(w⃗ iT⃗x −bi)) i
and b represents the weight and bias terms of the decision
per. Each domain was chosen so that collaborating with the i
node,respectively. Asthisrepresentationisdifficulttointer-
teammate would result in a higher score than working indi-
pret,Palejaetal. presenteddifferentiablecrispification,con-
vidually. In our newly-created domain, Optional Collabora-
sisting of two components: 1) Decision node crispification,
tion,creatingmixed-ingredientdishes(combiningonionsand
whichrecastseachdecisionnodetosplituponasingledimen-
tomatoes) will receive a higher score than single-ingredient
sionofourinputfeature,and2)Decisionoutcomecrispifica-
dishes. Teammates have 200 timesteps to collaborate and
tion,whichtranslatestheoutcomeofadecisionnodesothat
cookasmanydishesaspossible.
the outcome is a Boolean decision rather than a set of prob-
State-Space, Action-Space, and Reward Scheme Poli- abilities. Both operations utilize the straight-through trick
cies reason over a semantically meaningful 13-dimensional [Bengioetal.,2013]tomaintaingradients,allowingforboth
featurespaceasopposedtopixelspace,detailingtheobjects an interpretable forward propagation through the model that
eachagentisholding,potstatuses,andcounterobjects. Each tracesdownasinglebranchofatreeaswellasgradientflow
ofthesefeaturesisbinary(TrueorFalse). Thisstatespaceal- toupdateparametersoftheneuraltreemodel. Weutilizethis
lows for learning an interpretable tree-based policy that can approachinourIDCTstomaintaininterpretability.
be understood and manipulated by end-users. For the ac- WeinitializeourIDCTstobesymmetriccompletedecision
tion space, instead of using cardinal actions, we allow the treeswithN decisionleavesandN 1decisionnodes.Each
l l
−
AI to utilize macro-actions that can accomplish high-level decisionleafisrepresentedbyasparsecategoricalprobability
objectives such as ingredient collection, ingredient place- distribution over actions. At each timestep, a state variable
ment, and soup serving. Macro-actions are planned using is propagated through each decision node, split on a single
an A* planner, and we perform dynamic replanning at each decision rule, with the output being a Boolean causing the
timestep. Prior work has shown macro-actions can enhance decisiontoproceedviatheleftorrightbranchuntilarrivalat
interpretability[Beyretetal.,2019]. InForcedCoordination, aleafnode. Ateachleafnode,wesamplefromtherespective
for the reward scheme, we follow a similar distribution as probabilitydistributiontoproduceamacro-action(e.g.,“get
prior work and give a reward score of 60 per dish served, 3 anonion”or“placeheldingredientoncounter”).
4202
nuJ
7
]OR.sc[
1v30050.6042:viXra2.2 Training InAlgorithm1,wepresentdetailsofhowcontextualprun-
ing is accomplished. In Step 1, we initialize a domain vec-
For training this model, we utilize agent-agent collaborative
tor representing the current minimum and maximum values
trainingwhereaninterpretabletree-basedagent(maintaining
for each feature. Since our Overcooked domain utilizes bi-
anIDCT)ispairedwithasecondpolicy(representingthehu-
nary features, all bounds are initialized to 0 and 1. For-
man player), and both models are trained via decentralized
mally, this can be written as by the Cartesian product B =
PPO[Schulmanetal.,2017]. Itisimportanttonotethateach
[0,1] [0,1],ofcardinalityd(wheredisthedimension-
agent maintains its own buffer and optimizers. Further, we ×···
alityofthestatespace). InStep2,weinitializeaqueuethat
improve model predictability by applying an L1 norm loss
will be used to perform a breadth-first search to visit each
overleafnodedistributionsfortheIDCTagenttoensurespar-
node in a hierarchical order. In Step 4, we receive a node
sity,penalizinghighentropyactiondistributionsataleaf.Our
from the queue. In Step 5, we check the threshold value of
training procedure mimics that of PPO, utilizing a modified
the current node and compare it to the current node’s vector
loss function displayed in Equation 1, and policy update in
of minimum values. This operation looks to see if the node
Equation 2, where θ represents the aggregate set of weights
fortheIDCT,Aˆ representstheadvantageestimateattimet, results in a tree sub-space that is out of bounds (i.e., impos-
t sibletoreach). Weperformasimilarcomputationinstep8,
anda representsthedistributionmaintainedateachleaf,l.
l checkingthemaximumvalues. Inbothcases,welooktofind
childnodesthatdonotyieldareductioninthehyperspaceas
L(θ)=Eτ min r t(θ)Aˆ t,clip(r t(θ),1 ϵ,1+ϵ)Aˆ
t
candidates for pruning. In Step 11, we update the children
− basedonthethresholdvalueofourcurrentnodeanditssign
L h (cid:16) (cid:17)i (aswecanhave<or>withinanode),creatinganewbound-
+ λa ingbox. Instep12,weaddthechildrenofthecurrentnodeto
l
| |
1 thequeue,andloopbacktoStep4,repeatingsteps5-12until
X
(1) thequeueisempty. InStep14,weprunetreesub-spacesthat
areimpossibletoreach.
θ =argmaxL(θ) (2)
k+1
θ
ComputationalAnalysis
2.3 ContextualPruning
The computational complexity of our contextual pruning al-
Aswefocusoncreatingagentsthatcooperatewithhumans, gorithmcanbeanalyzedintermsofbothtimeandspacecom-
we must limit the size of our interpretable tree-based mod- plexity. Intermsoftimecomplexity,itisequivalenttothatof
els to a certain depth to promote user understanding. This Breadth-FirstSearch(BFS),specifically, (V +E),whereV
O
follows prior work, finding trees of arbitrarily large depths denotesthenumberofverticesandErepresentsthenumberof
can be difficult to understand [Ghose and Ravindran, 2020] edgesinthetree. Regardingspacecomplexity,ouralgorithm
and simulate [Lipton, 2018], and that a sufficiently sparse exhibitssimilarcharacteristicstoBFSfortreeswithonlytwo
DT is desirable and considered interpretable [Lakkaraju et leaves. Insuchcases,thespacecomplexityofBFSis (V),
O
al., 2016]. However, this can make training difficult, as a asitstoresalltheverticesatthemaximumbreadthlevelinthe
smalltreemaynothavetherepresentationalpowertolearna queueduringthetraversal.Consequently,thespacecomplex-
high-performingpolicy. ityofourcontextualpruningalgorithmisalso (V),making
O
it efficient and scalable for trees with a limited number of
Algorithm1ContextualPruningAlgorithm leaves.
Input: IDCTI(.) Utilizingcontextualpruningalongsideourtrainingframe-
Output:PrunedIDCT work allows us the benefit of training large tree-based mod-
1: SET NODE DOMAINS(IDCT=I,minValue=0,maxValue=1) els,greatlyimprovingease-of-training,whilestillbeingable
2: queue=[I.root] tosimplifytheresultantmodeltoasmaller,equivalentrepre-
3: whilequeueisnotemptydo sentation.
4: currentNode queue.pop()
←
5: if currentNode.compareValue < currentNode.lowerBound
ResultsofPruning
then
6: currentNode.prunable=True
To evaluate the utility of pruning, we train models
7: endif
of various sizes (8-leaf, 16-leaf, 32-leaf, 64-leaf, 128-
8: if currentNode.compareValue > currentNode.upperBound
leaf, 256-leaf) in Forced Coordination and perform prun-
then
ing on the resultant model. We find that models of
9: currentNode.prunable=True
10: endif larger size converge to higher performance (i.e., easier-
11: UPDATE DOMAINS FOR CHILDREN(currentNode, lower- to-train), following prior work displaying the utility of
Bound,upperBound,currentNode.compareValue) larger models. Further, empirically, we find we can re-
12: ADD CHILDREN TO QUEUE(currentNode,queue) duce model sizes by 8-16x in tree depth. We provide a
13: endwhile pipeline to allow for model training and contextual prun-
14: I PRUNE NODES FROM TREE(I) ing at our anonymous GitHub repository https://github.com/
←
15: return I
ghost12331/Team-Development-with-Transparent-Policies.2.4 Hyperparameters (F(4,38) = 18.93;p < 0.001) across conditions and deci-
siontreefamiliarity(F(1,38) = 16.12;p < 0.05). Wecon-
In IV2-D1: Forced Coordination and IV2-D2, we train an
IDCT with 256 leaves, a learning rate of 1e 3, and regular- duct post-hoc pairwise comparisons, utilizing Tukey HSD,
−
izationparameterof1e 4. Therestoftheparametersfollow and find that 1) IV1-C5 is significantly better than IV1-C2
−
defaultparametersfromthePantheonRLcodebase[Sarkaret (p<0.01),IV1-C3(p<0.01),andIV1-C4(p<0.01),and
al., 2022] for training Overcooked agents. After contextual 2)IV1-C1issignificantlybetterthanIV1-C3.
These results are similar to those in the paper when ana-
pruning,inbothdomains,weendupwithanAIpolicywith
lyzingthemaximumrewardandresultinasimilarsetofcon-
two and three leaves in Forced Coordination and Optional
clusions: 1)black-boxmodelscanoutperformwhite-boxap-
Collaboration,respectively.
proaches,and2)white-boxapproacheswithpolicymodifica-
For training fictitious co-play agents, we train 32 models
tionhavesomebenefitoverwhite-boxapproachesalone.Fur-
of teammates in each domain, saving policies at every 100
ther,asweseethattreefamiliaritypositivelycorrelateswith
epochs. At the end of training, we sort the performance of
performanceroundrewards,exploringalternativeparadigms,
saved policies and utilize the initial, mid-performing, and
such as natural language for describing and programming
highesttocreateourpopulationofdiverseagents,totaling96
treesmaybenefitusersunfamiliarwithdecisiontrees.
agents. Aneuralnetworkmodelisthenpairedinamulti-task
trainingframeworktoteamwiththisagent.
3.2 TeamDevelopment
Weprovideinstructionstoreplicateourmodelswithinthe
abovecodebase. Here,weanalyzethetrendsacrossiterations(didagentsim-
provefromiterationonetofour)andidentifycharacteristics
3 CompleteStatisticalAnalysis ofusersthatperformedwellinteamdevelopment. Utilizing
aFriedman’stest,wefindthatthereisasignificantdifference
Here,wepresentcompletedetailsregardingouranalysis,in- acrossdomains(χ2(1)=20.48,p<0.001).
cludingallteststatisticsaswellasnonsignificantandtrending
We conduct separate Wilcoxin signed-rank tests for each
comparisons.
condition, andutilizetheBonferronicorrectionindetermin-
ing significance (α/5). In IV2-D1, we see no condition
3.1 RQ1: TeamCoordinationPerformance
significantly improves significantly over repeated iterations.
As mentioned in the main paper, we allow humans to team In IV2-D2, we find that IV1-C1 (p < 0.01) and IV1-C2
withtheAIacrossfourepisodes,providinguswithfourteam- (p < 0.01) significantly improve over repeated teaming in-
ing scores. Within the main paper, we reported differences teractions.
withrespecttothemaximumscoreparticipantswereableto
obtain across iterations. Here, we analyze data in the per-
4 Discussion
formance round (the last iteration), where participants were
told to maximize performance. We note that participants In this paper, we provide several contributions towards in-
self-reported their gaming familiarity (100-point scale) and teractive HMT. We first present weaknesses in prior work,
weekly hours playing video games. Across all participants, displaying that learned collaborative agents can be individ-
self-reported gaming familiarity was rated as 73.19 23.80 ualistic and rigid. To address these weaknesses, we pro-
±
andweeklygaminghourswas4.44 5.32. Thisinformation poseaninteractiveschemetermedhuman-ledpolicymodifi-
±
wasusedinourstatisticalanalysis,andsignificancewasnot cationtobridgethegapbetweenindividualizedcoordination
foundinperformancevariationasafunctionofgamingexper- and adaptive, effective collaboration. We do so by creating
tise. UtilizingaFriedman’stest,wefindthatthereisasignif- a feedback loop that facilitates team policy changes during
icant difference across domains (χ2(1) = 38.7,p < 0.001). HMT. This is accomplished by 1) utilizing an interpretable
Accordingly,weanalyzethetwodomainsseparately. policy representation to provide users with insight into the
In IV2-D1, we find our data does not meet the necessary teammate’sdecision-making,specificallytheIDCT,aninter-
assumptions and utilize non-parametric tests. A Kruskal- pretable tree-based model that can be trained via reinforce-
Wallis Test was conducted to analyze differences in perfor- mentlearningandprunedtoasmaller,equivalentrepresenta-
manceroundrewardacrossconditions,andwefindasignif- tion, and 2) creating a user interface to support the end-user
icant effect (χ2(4) = 20.85,p < 0.001) across conditions. modifyingthepolicytotheirevolvingspecifications. Wede-
Weconductpost-hocpairwisecomparisons,utilizingDunn’s ployandcompareourinteractivepolicymodificationscheme
test,andfindthatIV1-C5issignificantlybetterthanIV1-C1 toseveralothertechniques,includingtwopopularpriorworks
(p < 0.01), IV1-C3 (p < 0.01), and IV1-C4 (p < 0.01). and variations of our proposed condition. While we do not
IV1-C5 is trending as significantly better than IV1-C2 with a direct objective benefit of human-led policy modification
ap-valueof0.0275(significanceis< 0.025or(α/2)dueto compared to utilizing a black-box model supported with a
theBejamini-Hochbergadjustment). population-based training scheme [Strouse et al., 2021], we
In IV2-D2, we test for normality and homoschedascity findimportanttakeawaysthatmotivatetheimportanceofcon-
and do not reject the null hypothesis in either case, using ducting longer-term, repeated-interaction studies. Specifi-
Shapiro-Wilk (p > .50) and Levene’s Test (p > 0.05). cally, white-box approaches that facilitate interpretation can
An ANOVA was conducted to analyze differences in per- be used within a feedback loop to lead to policy improve-
formanceroundrewardacrossconditions,takingseveralob- ment, users may require a larger number of interactions to
served variables into account. We find a significant effect reachateamconsensusandmaximalperformance,andthereareperson-specificcharacteristicsthatmayleadtosomeusers [Palejaetal.,2022] Rohan R. Paleja, Yaru Niu, Andrew
being able to take advantage of interpretable models and in- Silva, Chace Ritchie, Sugju Choi, and Matthew Craig
teractionmorethanothers. Gombolay. Learninginterpretable, high-performingpoli-
cies for autonomous driving. Robotics: Science and Sys-
5 LimitationsandFutureWork temsXVIII,2022.
Limitations:Thisstudywasconductedatauniversity.While [Sarkaretal.,2022] BidiptaSarkar,AditiTalati,AndyShih,
thepopulationwasdiverseinage,gender,anduniversityma- and Sadigh Dorsa. Pantheonrl: A marl library for dy-
jor, most students were based in engineering, presenting a namic training interactions. In Proceedings of the 36th
populationbias. AAAIConferenceonArtificialIntelligence(DemoTrack),
FutureWork:Inthefuture,itwouldbeinterestingtocon- 2022.
duct a similar experiment to a higher number of iterations, [Schulmanetal.,2017] John Schulman, Filip Wolski, Pra-
or until the team converges to a set of coordination strate- fullaDhariwal,AlecRadford,andOlegKlimov. Proximal
gies (the “performing” stage in Tuckman’s model). Further, policy optimization algorithms. ArXiv, abs/1707.06347,
the possibility of adding in feedback from the AI regarding 2017.
human-led policy modification (checking for logic inconsis-
[Strouseetal.,2021] DJStrouse,KevinR.McKee,MattM.
tencies, etc.) may be used to facilitate speedier team de-
Botvinick,EdwardHughes,andRichardEverett. Collab-
velopment. It would also be interesting to utilize different
orating with humans without human data. pages 14502–
paradigms in communicating with the human as language
14515,2021.
may be an easier medium than a decision tree interface.
Lastly, future work should be done to optimize the accessi- [Sua´rezandLutsko,1999] AlbertoSua´rezandJamesFLut-
bilityofGUIsforpolicymodificationviaxAItechniques. sko. Globally optimal fuzzy decision trees for classifica-
tionandregression. IEEETransactionsonPatternAnaly-
References sisandMachineIntelligence,21(12):1297–1311,1999.
[Bengioetal.,2013] YoshuaBengio,NicholasLe´onard,and
Aaron C. Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation.
ArXiv,abs/1308.3432,2013.
[Beyretetal.,2019] Benjamin Beyret, Ali Shafti, and
A.Faisal. Dot-to-dot: Explainablehierarchicalreinforce-
ment learning for robotic manipulation. 2019 IEEE/RSJ
International Conference on Intelligent Robots and Sys-
tems(IROS),pages5014–5019,2019.
[Carrolletal.,2019] Micah Carroll, Rohin Shah, Mark K.
Ho,T.Griffiths,S.Seshia,P.Abbeel,andA.Dragan. On
theutilityoflearningabouthumansforhuman-aicoordi-
nation. InNeurIPS,2019.
[Fontaineetal.,2021] MatthewC.Fontaine,Ya-ChuanHsu,
YulunZhang,BryonTjanaka,andStefanosNikolaidis.On
the importance of environments in human-robot coordi-
nation. In Dylan A. Shell, Marc Toussaint, and M. Ani
Hsieh, editors, Robotics: Science and Systems XVII, Vir-
tualEvent,July12-16,2021,2021.
[GhoseandRavindran,2020] Abhishek Ghose and Balara-
manRavindran. Interpretabilitywithaccuratesmallmod-
els. FrontiersinArtificialIntelligence,3,2020.
[Lakkarajuetal.,2016] Himabindu Lakkaraju, Stephen H.
Bach, and Jure Leskovec. Interpretable decision sets: A
joint framework for description and prediction. In Pro-
ceedings of the 22nd ACM SIGKDD International Con-
ferenceonKnowledgeDiscoveryandDataMining,KDD
’16,page1675–1684,NewYork,NY,USA,2016.Associ-
ationforComputingMachinery.
[Lipton,2018] Zachary C Lipton. The mythos of model
interpretability: In machine learning, the concept of in-
terpretability is both important and slippery. Queue,
16(3):31–57,2018.