Large Graph Generative Models
YuWang1,RyanA.Rossi3,NamyongPark,HuiyuanChen,NesreenK.Ahmed4,
PujaTrivedi2,FranckDernoncourt3,DanaiKoutra2,TylerDerr1
1VanderbiltUniversity 2UniversityofMichigan 3AdobeResearch 4IntelLabs.
{yu.wang.1,tyler.derr}@vanderbilt.edu,{ryrossi,dernonco}@adobe.com,
{pujat,dkoutra}@umich.edu,
park.namyong@gmail.com, nesreen.k.ahmed@intel.com, hxc501@case.edu,
Abstract
LargeGenerativeModels(LGMs)suchasGPT,StableDiffusion,Sora,andSuno
are trained on a huge amount of language corpus, images, videos, and audio
thatareextremelydiversefromnumerousdomains. Thistrainingparadigmover
diverse well-curated data lies at the heart of generating creative and sensible
content.However,allpreviousgraphgenerativemodels(e.g.,GraphRNN,MDVAE,
MoFlow,GDSS,andDiGress)havebeentrainedonlyononedataseteachtime,
whichcannotreplicatetherevolutionarysuccessachievedbyLGMsinotherfields.
To remedy this crucial gap, we propose a new class of graph generative model
calledLARGEGRAPHGENERATIVEMODEL(LGGM)thatistrainedonalarge
corpusofgraphs(over5000graphs)from13differentdomains. Weempirically
demonstratethatthepre-trainedLGGMhassuperiorzero-shotgenerativecapability
toexistinggraphgenerativemodels. Furthermore,ourpre-trainedLGGMcanbe
easily fine-tuned with graphs from target domains and demonstrate even better
performancethanthosedirectlytrainedfromscratch,behavingasasolidstarting
pointforreal-worldcustomization. InspiredbyStableDiffusion,wefurtherequip
LGGMwiththecapabilitytogenerategraphsgiventextprompts(Text-to-Graph),
suchasthedescriptionofthenetworknameanddomain(i.e.,"Thepower-1138-bus
graphrepresentsanetworkofbusesinapowerdistributionsystem."),andnetwork
statistics(i.e.,"Thegraphhasalowaveragedegree,suitableformodelingsocial
mediainteractions."). ThisText-to-Graphcapabilityintegratestheextensiveworld
knowledgeintheunderlyinglanguagemodel,offeringusersfine-grainedcontrolof
thegeneratedgraphs. Wereleasethecode,themodelcheckpoint,andthedatasets
athttps://lggm-lg.github.io/.
1 Introduction
Recently, Large Generative Models (LGMs) such as GPT, Stable Diffusion, Sora, and Suno [1,
6,50,62]haveachievedrevolutionarysuccessingeneratingcreativeandsensiblecontent,which
significantly increases the productivity of real-world applications [5, 58, 73]. Unlike previous
modelssuchasBert/Bart[11,29]inNaturalLanguageProcessing(NLP)andUnet[51]inImage
Segmentationthataretrainedonlyonsmall-scaledatasetsfromspecificdomainsovernarrowtasks,
the key to the success of these LGMs lies in their large training paradigm over the well-curated
training data from a wide variety of domains [7, 11, 49, 62]. Graph, as a data modality distinct
fromimage,text,andaudio,isubiquitousacrossnumerousfieldsandpresentsanewfrontierfor
applications of generative models such as drug discovery [22, 23], material design [30, 41] and
cyber-security[17,32]. GiventheunprecedentedsuccessachievedbyLGMsinotherdomainsand
thepromisingpracticalusageofgraphgenerativemodels,wenaturallyask:
Canweproposelargegenerativemodelsforgraph-structureddata?
4202
nuJ
7
]GL.sc[
1v90150.6042:viXraFigure 1: (a): Average degree and clustering coefficient of graphs from 13 domains. The graph
universeconsistsofgraphsfromdistinctdomains(e.g.,thetinyregionofChemicalGraphs),yet
therearesomecommontransferrablepatterns. (b): Ourpre-trainedLGGMafterfine-tuningoneach
domainachievesbettergenerativeperformancethanDiGresstrainedonthatsamedomain.
Althoughgraphgenerativemodelshavebeenthelong-standingfocusofgenerative-basedresearch[15,
21,76],previousoneshavebeentrainedongraphsfromonlyonedomaineachtime.Forexample,both
therepresentativeauto-regressive-basedGraphRNN[69],VAE-basedGraphVAE[57]anddiffusion-
basedDiGress[64]havebeentrainedonlyonsynthetic(Ego,Community,Grid)orchemistrygraphs
(Enzymes,QM9),thestatisticsofwhichonlycountsatinyregionofthewholegraphuniverseand
isdifferentfromgraphsinotherdomains,asshowninFigure1(a). RoadNetworkspossesslower
averageclusteringcoefficientsthanFacebookNetworks(FB).ThisisbecauseRoadNetworks,
by design, have square intersections, whereas social relationships in Facebook Networks (FB)
naturallyformtriangularconnections[54]. Moreover,TortoiseAnimalSocialNetworks(ASN)1
havealoweraveragedegreethanPowerNetworksbecausetortoises,assolitarycreatures,wouldnot
sharethesameburrow[56]. Asaresult,graphgenerativemodelstrainedononedomainarehardly
generalizabletounseengraphs,asshownbytheworsezero-shotgenerationperformanceofDiGress
inTable2. Morecritically,withouttrainingonnumerousgraphscoveringthewholegraphuniverse,
thesesmallmodelscanneverreplicatetherevolutionarysuccessachievedbyLGMsinotherfields.
RecognizingthesignificantgapindevelopingLGMsforgraph-structureddataandtheirpotential
revolutionary impact similar to LGMs in other fields, we develop the very first LARGE GRAPH
GENERATIVEMODEL(LGGM)thatispre-trainedover5000graphsfrom13domainssourcingfrom
theNetworkRepository-theinteractivedatarepositorycollectinggraphsfrom30domains[52,53].
Afterthispre-training,ourLGGMlearnssomefundamentalstructuralpatternsthataretransferrable
acrossdifferentdomains[39]andhenceforthdemonstratessignificantlybetterzero-shotgenerative
capabilityongraphsfromunseendomainsshowninTable2. Moreover,thepre-trainedLGGMare
highlyadaptableforfine-tuningonaspecificdomain,achievinganoverallperformanceincrease
of 29.59% compared to the smaller DiGress model trained on the same domain, as depicted in
Figure1(b). Thisimprovementisevenmoresignificantwhenonlylimitedgraphsareavailableshown
byFigure5,provingparticularadvantagesinsemi-supervisedgenerativesettings[10,30,35]. More
importantly,ourLGGMssupportText-to-Graphgeneration,whichallowsfiner-levelcontrolofthe
generatedgraphs(e.g.,theirdomains/namesinTable3andclusteringcoefficient/averagedegreein
Figure4). Ourcontributionsareasfollows:
• Large Graph Generative Model: We propose a pioneering Large Graph Generative Model
(LGGM), trained on thousands of graphs arising from 13 distinct domains. To the best of our
knowledge,thisworkistheveryfirstoneexploringthepotentialofLGMsongraph-structured
data. WehopeothersexpandthiscollectionandleverageourworktodevelopfutureLGGMsthat
couldeventuallyreplicatethesuccessofStableDiffusion[50]butinthegraphmodality.
• SuperiorZero-shotandFine-tuningGenerativeCapability: Ourpre-trainedLGGMdelivers
exceptional zero-shot generative performance on unseen graphs in Table 2 of Section 6.2 and
showsgreatadaptabilityforfine-tuninginFigure3ofSection6.3. Remarkably,thefine-tuned
LGGMoutperformsDiGresstrainedfromscratchonthesamegraphsespeciallyunderlimiteddata
scenarios,behavingasabetterstartingpointforreal-worlddevelopment.
• Text-to-GraphGeneration: WeequiptheLGGMwiththecapabilitytogenerategraphsgiven
user-specifiedtextprompts,allowingfiner-levelcontrolofthegeneratedgraphsintermsoftheir
domains/namesandnetworkstatistics.
1Nodesrepresenttortoisesandanedgeissetupbetweentwotortoisesiftheysharethesameburrow[55].
22 Relatedwork
2.1 LargeGenerativeModels(LGMs)
Recent years have witnessed unprecedented success achieved by LGMs in generating creative
and sensible content for a variety of downstream tasks across multiple modalities [1, 6, 8, 62,
75]. Forinstance,inNaturalLanguageProcessing(NLP),largelanguagemodelstrainedonnext-
tokenpredictioncaneffectivelyproducehuman-readabletextsforcompletingquestion-answering,
translation,andmoretasks[47,61,67]. Furthermore,theadvancementofmulti-modalgenerative
modelsnowsupportscross-modalitygeneration, suchasconvertingtextintoimageswithStable
Diffusion or vice versa with GIT [50, 72, 66]. The key to their success lies in their ability to
effectivelyutilizetheworldknowledgeobtainedduringthepre-trainingstageoveralargeamountof
well-curateddata. Thisworldknowledgehasbeendemonstratedtobepositivelytransferrableacross
numerousdomains, deliveringpromisingefficacywithfew-shottaskdemonstrations. Compared
with the recent large generative models in NLP/CV such as LLaMA3, Falcon, Stable Diffusion,
LLaVA[31,45,50,62],wealternativelyfocusondevelopinglargegenerativemodelsforgraphs
withtheexpectationtorealizeasimilarsetofadvantagesachievedbyLGMsinotherfields,including
enhancedzero-shotgeneralibility,improvedfine-tuningperformanceandcross-modalitygeneration.
2.2 GraphGenerativeModels
Table1: OurLGGMistrainedacross13domainson
Given the ubiquity of graphs in modeling thousandsofgraphswiththesupportforText-to-Graph
relationalinformationofreal-worldobjects (T2G)generation,controllingthedomain/propertyof
acrossmanydomains[34,52,54,67],graph thegeneratedgraphs.
generativemodelshavebeendevelopedto
generaterealisticgraphsforadvancingnu- Type Model #Domains Multi-Domain T2G T2G
Training Domain Property
merous applications [22, 25, 30, 35], such GraphRNN[69] 2 ✘ ✘ ✘
as generating molecular graphs with high A Ru egto r- essive E Md og le RR NN NN [4[3 6] ] 3 2 ✘ ✘ ✘ ✘ ✘ ✘
drug-likenessanddesigningimperceptible MDVAE[13] 1 ✘ ✘ ✘
adversarialattacks. Graphgenerativemod- VAE P (DC EV )A CE O-[ V14 A, E20 [1] 9] 3 1 ✘ ✘ ✘ ✘ ✘ ✘
els can generally be divided into two cat- GraphVAE[57] 1 ✘ ✘ ✘
egories: statistic-based ones [18, 26] and GAN LM Go Gl-C Ay Ncl [e 1G 6A ]N[40] 21 ✘ ✘ ✘ ✘ ✘ ✘
deeplearning-basedones[21,76]. Statistic- GraphNVP[38] 1 ✘ ✘ ✘
basedgenerativemodelssuchasStochastic Flow MoFlow[70] 1 ✘ ✘ ✘
GraphDF[36] 2 ✘ ✘ ✘
BlockModels[27]andSmallWorldMod- GDSS[24] 3 ✘ ✘ ✘
els[42]assumethatthereal-worldgraphfor- Diffusion DiGress[64] 2 ✘ ✘ ✘
GraphEBM[33] 1 ✘ ✘ ✘
mation adheres to specific statistical rules, LGGM-Ours 13 ✔ ✔ ✔
and define various sampling strategies to
simulate networks with prescribed properties. However, this approach oversimplifies the com-
plexdistributionofreal-worldgraphsandstrugglestogeneralizetothosedeviatingfromestablished
norms. Thislimitationhasspurredrecentresearchintodeep-learning-basedgenerativemodelsthat
automaticallycaptureintricatestatisticsbylearningtorecovergraphs[57,64,69,70]. Despitetheir
effectiveness,theyallfocusonanarrowrangeofdomainsandaretrainedsolelyonasingledomain
eachtime. Next,webrieflyreviewrepresentativedeepgraphgenerativemodelsinTable1.
GraphRNN [69], a pioneering model in autoregressive generation, employs breadth-first search
to establish node ordering and sequentially generates nodes and edges. In addition, variational
autoencoders[12,13,14,20,57]wereadoptedtoenableflexiblegraphgenerationtailoredtospecific
propertiesbyregularizingthelatentvariables. Followingthat,normalizingflowswereusedtolearn
invertible mappings between molecular graphs and latent representations (e.g., GraphNVP [38]
and MoFlow [70]). More recently following the success of diffusion-based models in images,
graphdiffusion-basedmodelslikeGDSS[24]andDiGress[64]haveemerged,allowinggradually
generating graphs from the noise either in the continuous or the discrete space. However, these
deepgraphgenerativemodelshaveprimarilyfocusedonlimiteddomainssuchaschemistry,social
networks,andsyntheticgraphs,neglectingavastarrayofunexploredgraphsinotherfields. More
critically,thesemodelshavebeenhistoricallytrainedonasingledomaineachtime,mirroringearlier
approacheslikeUnet[51]andBert/Bart[11,29]inCVandNLP,thuslimitingtheirabilitytolearn
fundamentalknowledgetransferrableacrossdifferentdomains. Incontrast, ourworkfocuseson
learningalargegenerativegraphmodelthatispre-trainedonthousandsofgraphsfrom13domains
andwedemonstratethat,throughextensiveexperiments,theproposedLGGMsuccessfullyreplicates
therevolutionaryachievementsgainedbytherecentLGMsinotherfields[1,6,50,62].
33 LargeGraphGenerativeModels
3.1 Notation
LetGbearandomvariableofuniversalgraphs,governedbyitsunderlyingdistributionP(G). Given
that real-world graphs originate from various domains, we introduce Gc to represent a random
variableforgraphsfromdomainc,withitsdistributionasP(Gc). Assumingtheuniversalgraph
spaceencompassesC distinctdomains,i.e.,G =∪ Gcwitheachsetofgraphsfromdomaincas
c∈C
Gc,thenP(Gc)/P(G)isdomain-specific/agnosticdistribution. Toeasetheintroductionoftraining
andevaluationsettinginSection6, wefurtherdivideeachdomain-specificsetofgraphsGc into
training,validationandtestingsubsets,notatedasGc =GTrain,c∪GVal,c∪GTest,c. Werepresenteach
graph G = (XG,EG) with XG ∈ RnG×dX/EG ∈ RnG×nG×dE as the one-hot encoding matrix
representingnode/edgecategorieswithn beingthenumberofnodesingraphGandd /d being
G X E
thenumberofnode/edgecategories,consideringtheedgeexistenceasaparticularedgecategory.
InText-to-Graphgeneration,eachgraphGispairedwithatextualdescriptionS fromthetextual
distributionP(S)andtheirjointdistributionisP(G,S).
3.2 LargeGraphCorpus
TrainingLGGMrequiresasubstantial,well-curatedcollectionofgraphsfrommultipledomains. We
selectgraphsfromtheNetworkRepositoryacross13distinctyetrepresentativedomainscoveringa
widevarietyofreal-worldscenarios,includingFacebook(FB),AnimalSocial(ASN),Email,Web,
Road,Power,Chemical(CHEM),Biological(BIO),Economic(ECON),Retweet(RT),Collaboration
(COL),Ecological(ECO),Citation,asshowninFigure2(a). Giventhatmanyreal-worldgraphs
(e.g.,socialnetworksandroadnetworks)comprisethousandsorevenmillionsofnodesandedges,
andthatstate-of-the-artdiffusionmodels,e.g.,DiGressandGDSS,arelimitedtohandlingnetworks
withonlyhundredsofnodes,wefurthersamplesubgraphsforcertaindomainstoaddressscalability
challenges. Specifically,wegenerate2/3-hopegosubgraphscenteredonmultiplerandomlychosen
nodesfollowedbytakingtheirinducedsubgraphs. Weapplythisstrategyiterativelyacrossallthe
initiallycollectedgraphsuntilhittingthepresetbudget. AppendixC.1presentsthegraphstatistics.
3.3 Pre-TrainingandGraphGenerationofLGGM
OurLGGMisdesignedbasedondiscretedenoisingdiffusion[2,9,64],whichiscomposedofa
diffusionforwardprocessbasedonatransitionmatrixandareversepredictionprocessbasedon
minimizingthecross-entropylossbetweentheground-truthgraphsandthepredictedcleangraphs.
Duringtheforwardprocess,foreachgraphGsampledfromthejointdistributionP(G),weobtain
itsnoisyversionGt =(Xt,Et)atsteptbysamplingfromtheconditionalcategoricaldistribution:
q(Gt|Gt−1)=(Xt−1Qt ,Et−1Qt ) and q(Gt|G0)=(XQ¯t ,EQ¯t ), (1)
X E X E
whereQt ∈ RdX×dX andQt ∈ RdE×dE arenode/edgetransitionmatricesandG0 = Gisthe
X E
originaldatadistributionofgraphs. Dependingonwhetherourgenerativedownstreamtasksrequire
generalizationtounseendomainsornot,wecaneitherusedifferenttransitionmatricesforgraphs
fromdifferentdomains,i.e.,domain-specifictransitionmatrixQt,c =αtI+(1−αt)1mc ,mc =
X X X
1 (cid:80) XG,∀c∈C orunifytransitionmatricesacrossdifferentdomains. Fortheunified
|GTrain,c| G∈GTrain,c
transition matrices, we can trivially use the uniform transition matrix, i.e. Qt,c = αtI+(1−
X
αt)(1 1⊤ )/d , or compute the marginal transition matrix across all graphs from all domains
Qt
=dX αtIdX +(1X
−αt)1m ,m = 1 (cid:80) XG. AndQt canbecomputedsimilarly. We
X X X |GTrain| G∈GTrain E
validatetheadvantagesofLGGMsunderbothofthesetwotransitionstrategiesinAppendixE.
Inthereverseprocess,aparametrizedneuralnetworkistrainedtopredictthecleangraphgiventhe
noisygraphsampledfollowingEq.(1)byoptimizingthefollowingloss:
Θ⋆ =argminL=E G∼P(G)E t∼TE Gt∼q(Gt|G)(−logp Θ(G|Gt)). (2)
Θ
Following[64],wecombinethelearnedP (G|Gt)andtheclosed-formposteriorP(Gt−1|Gt,G)
Θ⋆
toperformbackwardgenerationbysamplingfromthefollowingdistribution:
(cid:88)
P(Gt−1|Gt)∝ P(Gt−1|Gt,G)P (G|Gt). (3)
Θ∗
G
4Figure 2: The overview of LGGM framework and experimental settings. (a): Graph universe
includingourcollected13distinctyetrepresentativedomains. (b)-(c): Comparedwithallprevious
graphgenerativemodelsthathavebeentrainedonlyononedomaineachtime,ourLGGMistrained
onthousandsofgraphsfrom13domains. (d): Wepre-train/fine-tuneLGGMinSection3.3/4. (e):
GiventhetextpromptS andthecurrentgeneratedgraphatt,weconcatenateitstextualembedding
obtainedfromapre-trainedlanguagemodelwiththenode/edge/graphembeddingsafterspectral
featureextractionandforwardthemthroughtheGraphTransformertopredictthecleangraph.
4 Fine-tuningLGGM
Inmanyreal-worldapplications,thegraphsofinterestG(cid:101)mayhighlylikelycomefromcompletely
unseen domains, i.e., G(cid:101)∩G = ∅, and their corresponding distribution may also be significantly
different from the pre-trained one, i.e., P(G (cid:101)) ̸= P(G) as shown by comparing CHEM and FB
Networks in Figure 1(a). In this case, we further fine-tune our pre-trained LGGM based on the
observedgraphsG(cid:101)fromtheunseendomains:
Θ⋆⋆ =argminL=E G(cid:101)∼P(G(cid:101))E t∼TE G(cid:101)t∼q(G(cid:101)t|G(cid:101))(−logp Θ(G(cid:101)|G(cid:101)t)), (4)
Θ
whereΘ⋆⋆ isinitializedasΘ⋆ fromthepretainingphaseinEq(2). Afterfine-tuning,ourLGGM
caneffectivelyadapttounseendistributionsbyusingboththepriorknowledgefromthepre-training
stageandthespecificknowledgeofnewgraphsfromtheunseendomains,asverifiedinFigure3.
DespitethesuperiorcapabilitiesofLGGMingeneratinggraphsafterbothpre-trainingandfine-tuning
processes,theyessentiallymimictherandomsamplingfromthelearneddistributionP(G)thatis
prescribedbythetrainingdatawithoutanyfine-levelcustomization. Tocontrolthecharacteristics
ofthegeneratedgraphs,wefurtherproposetheveryfirstText-to-GraphLGGMtogenerategraphs
basedontextualdescription. Inthisway,userscouldspecifytheirdesiredpropertiesofthegraphs
throughnaturallanguagedescription,therebyguidingthegraphgenerationinamoretailoredmanner.
5 Text-to-GraphLGGM
Given the textual description S about the network to be generated, our goal here is to learn
P(Gt−1|Gt,S),whichisfurtherdecomposedas:
(cid:88)
P(Gt−1|Gt,S)∝ P(Gt−1|Gt,G,S)P(G|Gt,S). (5)
G
Theorem1provesthatifthetransitionmatricesQt ,Qt inEq.(1)areindependentofthetextual
X E
descriptionS,thefirsttermP(Gt−1|Gt,G,S)canthenbesimplifiedasP(Gt−1|Gt,G)withthe
analyticalformcomputation[64]. Forthesecondterm,weapproximateitbyaneuralnetwork,i.e.,
P(G|Gt,S)=P Θ▲(G|Gt,S)withΘ▲beingoptimizedby:
Θ▲ =argminL=E (G,S)∼P(G,S)E t∼TE Gt∼q(Gt|G)(−logp Θ(G|Gt,ϕ(S))), (6)
Θ
whereϕisapre-trainedtextualencoder. Figure2(e)showsthearchitectureofLGGM-Text2Graph,
whichfirstlyintegratesthetextualembeddingϕ(S)intothenode/edge/graph-levellatentembeddings
afterspectralfeatureextractionofthecurrentgeneratedgraphandfurtherpredictsthecleangraph.
Theorem2provesthatmodelingP(Gt−1|Gt,S)withP Θ▲(Gt−1|Gt,S)leadstohigherevidence
lowerboundofthelikelihoodlogP(G0,S).
5Trainingp (G|Gt,ϕ(S))inEq(6)requiresthejointdistributionbetweengraphsandtheircorre-
Θ
spondingtextualdescriptions,i.e.,P(G,S). Givenusers’specificinterestsinthegraphstogenerate,
weexploretwomaincategoriesoftextualpromptstoguidegraphgeneration: domain/name(e.g.,
Power Network, power-1138-bus) and structural characteristics (e.g., average degree, clustering
coefficient). Forexample,zoologistsinterestedinthedynamicsoftortoiseinteractionsmightseek
to generate Animal Social Networks [59], and social scientists studying social anomalies might
prioritizegeneratingsocialinteractionswithdenseandunexpectedconnections[37]. Sincethiswork
isapioneeringeffortinText-to-Graphgenerationandnopriorcollectionofuserpromptsforthis
purposeexists,followingpreviousworks,e.g.,LLaVA[31,71],weaskGPT3.5/4toemulatethe
humandraftingofpromptstoobtainpairsof(userprompt,graph). Forpreparingthegraphswithuser
promptsabouttheirdomains/names,weobtainthedomain/nameinformationofeachgraphdirectly
fromtheNetworkRepository[52]andpromptGPT3.5togeneratethehuman-readabledescription
pairedwiththecorrespondinggraph. SeemoredetailsinAppendixC.2. Forpreparingthegraphs
withuserpromptsabouttheiraverageclusteringcoefficient/degree,insteadofusinggraphsfrom
NetworkRepositorythatonlycountpartiallyoftheentiregraphuniverse(i.e.,noexistinggraphs
therecovertheareawithhighaveragedegreeandlowaverageclusteringcoefficientinFigure1(a)),
we use the Watts–Strogatz small-world graph model [68] to synthesize graphs covering the full
spectrumofthegraphuniverse. Afterthat,wecalculatetheaveragedegreeandclusteringcoefficient
foreachgraphandpromptGPT4togeneratetextualdescriptionsaboutthesenetworksusingtheir
statistics. SeemoredetailsinAppendixC.3. Wealsoemployt-SNEvisualization[63]toanalyzethe
generatedtextualdescriptions,asshowninFigure6. Thisvisualizationindicatesthattextsdescribing
graphsfromvariousdomainsorwithdistinctstatisticstendtoformseparateclusters,anecessary
conditionforthesuccessfulcontrolofthegeneratedgraphs.
6 Experiments
6.1 ExperimentalSetup
Inthissection,weconductfourexperimentsoverthegraphscollectedfrom13domainstodemonstrate
theeffectivenessofLGGMsinfourdifferentaspects,thedetailsofwhicharesummarizedasfollows:
• Pre-training Evaluation in Table 2 in Section 6.2: To demonstrate the superior zero-shot
performanceofLGGMingeneratingunseengraphscomparedtoconventionalgraphgenerative
models,weadopttheout-of-distributionevaluationwhereweiterativelytreateachdomainXas
theunseenoneandtraintheLGGMusingtraininggraphsfromallotherdomains,andevaluate
itsperformanceonthetestinggraphsfromtheunseendomainX.ThevariantofLGGMinthis
experimentiscalledLGGM-XwhereXrepresentstheunseendomain.
• Fine-tuningEvaluationinFigure3inSection6.3: Todemonstratethehighadaptabilityforfine-
tuningLGGM,wefurtherfine-tunetheabovepre-trainedLGGM.Specifically,wetakeLGGM-X
pre-trainedongraphsfromallotherdomainsbutdomainX,andthenfine-tuneitonthetraining
graphsfromdomainX.AfterthatweevaluateitonthetestinggraphsfromdomainX.Thevariant
ofLGGMinthisexperimentiscalledFine-tunedLGGMonX.
• Text-to-Graph Generation in Table 3 and Figure 4 in Section 6.4: To control the graph
generation,weconsidertwotypesofuserpromptinformation: thedomain/nameandthegraph
properties, i.e., we train LGGM on training graphs from all domains with user prompts either
describingthegraphdomains/namesorgraphstatistics. WecallthesetwovariantsofLGGMas
LGGM-T2GDandLGGM-T2GUP,respectively.
• Fine-tuned LGGM compared with DiGress trained directly on X in Figure 1(b)/5 in Sec-
tion6.5: WhenhavingaccesstographsofdomainX,userscoulddirectlytrainexistinggraph
generativemodelsandgenerategraphsforthedomainX.Todemonstratethepracticalusageof
LGGMs,wefurthercomparethefine-tunedLGGMonXwithDiGressdirectlytrainedonX.In
addition,wealsocomparetheirperformanceunderlimiteddatascenarios[17,30].
Figure7inAppendixD.3comprehensivelyillustrateseachoftheabovetrainingparadigms. Dueto
thepagelimitation,wepresenttheevaluationmetricsandmodelhyperparametersinAppendixD.
Moreover, we only present results under the uniform transition strategy in the main paper while
leavingtheoneunderdomain-specifictransitionstrategyinAppendixE.Itisimportanttonotethat
thebenefitsofLGGMsareconsistentacrossbothofthesetwotransitionstrategies.
6Table 2: Comparing Zero-shot Generative Performance on unseen Graphs in held-out domain X
betweenDiGresstrainedonQM9andLGGM-Xtrainedonallexcepttheheld-outdomainX.Result
"ALL"iscomputedbyaveragingacross12domainsandthebestresultforeachdomainisinbold.
Domain Method DEG CC Spec Orb Domain Method DEG CC Spec Orb
DiGress 0.3376 0.6298 0.0797 0.3593 DiGress 0.2712 0.5202 0.1127 0.3188
FB BIO
LGGM-X 0.4723 0.6843 0.2924 0.7555 LGGM-X 0.1081 0.2696 0.0900 0.2053
DiGress 0.1496 0.3258 0.1506 0.4420 DiGress 0.2987 0.4841 0.2162 0.3834
ASN ECON
LGGM-X 0.0281 0.2440 0.0830 0.0618 LGGM-X 0.1213 0.0920 0.1120 0.1086
DiGress 0.2192 0.6012 0.0702 0.3416 DiGress 0.4164 0.1327 0.4147 0.5957
EMAIL
LGGM-X 0.0751 0.2364 0.0768 0.3089
RT
LGGM-X 0.0525 0.1429 0.1330 0.2219
DiGress 0.2556 0.6186 0.1877 0.6045 DiGress 0.2473 0.5826 0.2314 0.7679
WEB
LGGM-X 0.0648 0.3961 0.0549 0.1127
COL
LGGM-X 0.0736 0.5769 0.0895 0.0988
DiGress 0.3705 0.8226 0.2801 0.7198 DiGress 0.5431 0.7915 0.2338 0.6045
ROAD
LGGM-X 0.0713 0.2193 0.0987 0.2986
ECO
LGGM-X 0.4753 0.3904 0.3194 0.3934
DiGress 0.3726 0.4582 0.3270 1.4732 DiGress 0.2527 0.7790 0.1315 0.4966
POWER
LGGM-X 0.0119 0.1293 0.0373 0.0754
CITATION
LGGM-X 0.1348 0.7257 0.1160 0.4981
DiGress 0.3112 0.5622 0.2030 0.5923
ALL
LGGM-X 0.1408 0.3422 0.1253 0.2616
DEG,CC,Spec,Orb:MMDofDegree,ClusteringCoefficient,Eigenvalues,andOrbits,moredetailsareinAppendixD.1.
(a)PerformanceofMMDofCC. (b)PerformanceofMMDofSpec.
Figure3: PerformancecomparisonbetweenFine-tunedLGGMandFine-tunedDiGress.
6.2 Pre-trainingEvaluation
Table2comparestheperformanceofourmodel,LGGM-X,pre-trainedonallgraphdomainsexcept
theheld-outdomainX,withDiGresstrainedontheQM9dataset. Bothofthemareevaluatedover
graphsfromtheunseendomainX.Overall,LGGM-XoutperformsDiGressacrossallevaluation
metricsshownbythe"ALL"result. Thissuperioritysuggeststhattrainingongraphsfromdiverse
domainscapturestransferablestructuralpatternsandenhancesthegeneralizationofthemodelto
unseendomains. TheonlyexceptionfromthistrendoccurswithFacebookNetworks(FB)where
ourLGGM-XperformsuniformlyworsethanDiGressacrossallevaluationmetrics. Thisisbecause
FacebookNetworks(FB)onlycountatinyregionamongthewholegraphuniverse. Asillustratedin
Figure1(a),theaverageclusteringcoefficientofFBgraphsrangesfrom0.301to0.407,anarrow
segmentwithinthebroaderglobalgraphspectrumspanningfrom0to1. Thisnarrowrangeposesa
challengeforthegeneralizedLGGM-Xtospecializeinlearningthegraphdatadistributionspecific
totheFBdomain. Furthermore, weconduct thesameexperimentbutunderthedomain-specific
transitionstrategyinTable6inAppendixE,andsimilarly,LGGM-XgenerallyoutperformsDiGress.
6.3 Fine-tuningEvaluation
Inadditiontothesuperiorzero-shotgenerativeperformanceofpre-trainedLGGM-X,manyreal-
world applications already possess exemplary graphs that can be leveraged, e.g., different types
of anomaly behaviors in social networks/e-commerce platforms, and molecules with predefined
chemicalstructuresindrugdiscovery. Inthesescenarios,userscanfine-tuneLGGM-Xwiththese
domain-specificgraphs,adaptingthebroadlytrainedmodeltospecializeingeneratinggraphstailored
totargetdomains. Figure3comparesthegenerativeperformanceoffine-tunedDiGressonXthatis
originallypre-trainedonQM9andfine-tunedLGGM-XonXthatisoriginallypre-trainedonallbut
domainX.WecanseethatLGGM-XconsistentlyoutperformsDiGressforgraphsfrommostofthe
domains,whichfurthervalidatestheadaptabilityofLGGMafterfine-tuningonaspecificdomain.
7Table3: ComparingtheGraphGenerativePerformanceofLGGMwith/withoutTextConditions.
Bestandrunner-upresultsareboldedandunderlined.
Domain Method DEG CC Spec Orb Domain Method DEG CC Spec Orb
LGGM 0.0321 0.4994 0.0763 0.3117 LGGM 0.2661 0.3120 0.1135 0.3835
FB LGGM-T2GD 0.1561 0.1639 0.0924 0.0417 BIO LGGM-T2GD 0.0099 0.1286 0.0303 0.1366
LGGM-T2GUP 0.0050 0.0545 0.0070 0.0251 LGGM-T2GUP 0.0028 0.0287 0.0236 0.0174
LGGM 0.1511 0.4325 0.1875 0.3896 LGGM 0.3828 0.1533 0.2039 0.2583
ASN LGGM-T2GD 0.0318 0.2821 0.0606 0.0631 ECON LGGM-T2GD 0.0666 0.0594 0.0650 0.0586
LGGM-T2GUP 0.0211 0.1191 0.0462 0.0195 LGGM-T2GUP 0.0132 0.0257 0.0053 0.0191
LGGM 0.2156 0.2450 0.0666 0.2757 LGGM 0.4395 0.2225 0.4337 0.6641
EMAIL LGGM-T2GD 0.0469 0.0982 0.0484 0.0505 RT LGGM-T2GD 0.0468 0.0955 0.0729 0.0393
LGGM-T2GUP 0.0073 0.0379 0.0127 0.0437 LGGM-T2GUP 0.0286 0.0933 0.0400 0.0312
LGGM 0.2725 0.2672 0.1900 0.4368 LGGM 0.3565 0.3554 0.2451 0.7874
WEB LGGM-T2GD 0.0255 0.0737 0.0354 0.1856 COL LGGM-T2GD 0.0395 0.3110 0.1146 0.1823
LGGM-T2GUP 0.0105 0.0941 0.0206 0.0451 LGGM-T2GUP 0.0265 0.2813 0.0895 0.0899
LGGM 0.4825 0.5373 0.3398 0.7542 LGGM 0.5466 0.6003 0.2257 0.7089
ROAD LGGM-T2GD 0.0088 0.1225 0.0399 0.0155 ECO LGGM-T2GD 0.2160 0.2917 0.1203 0.2569
LGGM-T2GUP 0.0177 0.0437 0.0336 0.0086 LGGM-T2GUP 0.0293 0.2885 0.0416 0.2556
LGGM 0.4394 0.4646 0.3473 1.3186 LGGM 0.2624 0.5374 0.1295 0.3419
POWER LGGM-T2GD 0.0162 0.1131 0.0479 0.1786 CITATION LGGM-T2GD 0.0101 0.1025 0.0315 0.0651
LGGM-T2GUP 0.0062 0.0570 0.0111 0.0084 LGGM-T2GUP 0.0072 0.0849 0.0115 0.0287
LGGM 0.3206 0.3856 0.2132 0.5526
ALL LGGM-T2GD 0.0562 0.1535 0.0633 0.1061
LGGM-T2GUP 0.0146 0.1007 0.0286 0.0494
Figure 4: Text-to-Graph Generation with Prescribed Graph Properties. (a) Controlling Average
ClusteringCoefficient;(b)ControllingAverageDegree.GT-GroundTruthGraphsandGen-Generated
Graphs. Beloweachgraph,thenumberofnodesandkeystatisticalmeasuresaredisplayed.
6.4 Text-to-GraphGeneration
HereweintegrateText-to-Graph(T2G)generationintoLGGMs. Weintroducetwovariants: LGGM-
T2GD,whichutilizesdomainlabelssuchas"PowerNetworks"astextualdescriptions,andLGGM-
T2GUP, which utilizes user prompts from GPT3.5, like "The power-1138-bus graph represents a
network of buses in a power distribution system". Table 3 compares the basic LGGM trained
withouttextconditions,againstLGGM-T2GDandLGGM-T2GUP. Firstly,weobserveasignificant
performance improvement from LGGM to LGGM-T2GD/LGGM-T2GUP. The inclusion of text
descriptionsactsasauniqueidentifierthatenablesLGGM-T2Gtospecializeingeneratinggraphs
aligningwithcorrespondingdomains. Moreover,thenetwork-leveluserpromptsinLGGM-T2GUP
provideafiner-levelcontrolcomparedtothedomain-leveldescriptionsinLGGM-T2GD, further
boostingtheperformance. Furthermore,weshufflethedomainnamespairedwitheachgraphfor
LGGM-T2GDinthetestingphaseandobservetheperformancedecreaseinTable15asexpected.
LGGM-T2Gcanalsocontrolthepropertiesofthegeneratedgraphs. Herewefirstsynthesizeground-
truthgraphswithclusteringcoefficientsbetween[0,0.75]andaveragedegreesbetween[0,100]. We
dividetheseground-truthgraphsintothreegroups,low/medium/high,andpromptGPT4togenerate
userinstructionsdescribingthesetwographproperties(AppendixC.3). Thenwecombinethesethree
groupsofgraphswiththeirinstructionstotrainLGGM-T2Gandevaluatewhetherthepropertiesof
thegeneratedgraphsalignwiththeinstructions. InFigure4(a)/(b),wecanseeaclearalignment
betweenthestatisticalpropertiesoftheground-truthgraphsandthoseofthegeneratedgraphs,both
intermsoftheaverageCCandDEG.Furthermore,wevisualizethegeneratedgraphsforthesethree
groupsinFigure4(a)/(b). Wecanseegraphsinlow-CCgroupspossessmanysquareswhiletheones
inhigh-CCgroupscontainmanytriangles,aligningwiththeintuitionofCC.
8(a)Road-DEG. (b)Road-Orbit. (c)Retweet-DEG. (d)Retweet-Orbit.
Figure5: Withfewertraininggraphs,Fine-tunedLGGMbecomesmoreadvantageousthanDiGress.
6.5 PracticalUsageofFine-tunedLGGM
TodemonstratethepracticalusageofLGGMingeneratinggraphsforreal-worlddeployment,we
furthercomparethefine-tunedLGGMwithDiGresstraineddirectlyoneachdomaininFigure1(b).
Wecanseethatevenusingthesamegraphsfortraining,duetotheadditionalknowledgeincorporated
during the pre-training phase of LGGM, it exhibits significantly better generative performance
formostdomains. Moreover,thisadvantagebecomesevenmorepronouncedwhenfewergraphs
areavailable. Figure5illustratestheenhancedperformanceoffine-tunedLGGMversusDiGress
trained on X, with a widening margin as the number of training graphs in X decreases. This is
particularlyusefulsincemanygraphgenerativeapplicationsinvolvesemi-supervisedsettings,e.g.,
generatinganomalysoftwareanddesignofdrugs,theamountofwhichonlycount0.05%-0.5%[4]
and0.01%[43]amongthewholepotentialcandidates,respectively.
7 ResearchProblemsEnabledbyLGGMs
AstheproposedLGGMsarethefirsttoexplorethepotentialoflargegenerativemodelsingraphs,it
willsparknumeroustransformativeresearchopportunities[28],whicharesummarizedbelow:
• Simulations,Extrapolation,Anonymization: SinceourLGGM-T2Gcangenerategraphswith
pre-definedproperties,wecansimulategraphswithvariouspropertiesandextrapolatenewinsights
fromthesesimulatedgraphs,e.g.,evaluateconventionalandnewlydesignedgraphalgorithms/
models[44]. Moreover,forsensitivereal-worldgraphs,suchasthoseinvolvingproprietarydrug
designsandcorporatenetworks,wecanmaintainconfidentialitybysharingonlythemodel,which
canthensimulatesimilargraphswithoutdisclosingprivateinformation.
• DataAugmentation: LGGMscanbeusedfordataaugmentationwhenonlylimitedgraphsare
availableforapplicationslikegraphanomalydetectionandmoleculartasks[30,48].
• GraphCompression: LGGMallowsforthecompressionofgraphsacrossmultipledomainsby
merelystoringmodelparametersinsteadoftheoriginalgraphs.
8 Limitations,FutureDirections,andConclusion
Limitations and Future Directions: Like LGMs in other fields [65, 74], our LGGMs are not
specialized in generating graphs for specific domains. One future direction could be exploring
strategiessuchasRetrieval-AugmentedGenerationtoenhancedomainproficiency[67]. Additionally,
ourevaluationofLGGMshasfocusedsolelyontheirgenerativecapabilities,withoutexaminingtheir
potentialusageindownstreamtasks. Apromisingfuturedirectionistoassesstheirpracticalutilityin
application-orientedmanners,e.g.,higherqualityofgeneratedgraphsforbetterdataaugmentation.
Conclusion: MotivatedbytherecentsuccessesofLargeGenerativeModels(LGMs)acrossfields
of Vision, Language, Video, and Audio, and recognizing the promising practical usage of graph
generativemodels,weintroduce,fortheveryfirsttime,LargeGraphGenerativeModels(LGGMs).
Thesemodelsaretrainedonover5,000graphssourcedfrom13distinctdomainsfromthewell-known
NetworkRepository.WeempiricallyverifythesuperiorityofourLGGMsinthreeaspects.Firstly,our
pre-trainedLGGM-Xmodelsdemonstrateexceptionalzero-shotgenerativecapabilities. Secondly,
LGGMs show remarkable adaptability for fine-tuning, and the fine-tuned LGGM is even more
powerfulthanpreviousgraphgenerativemodelstrainedfromscratch. Lastly,ourmodelsfacilitate
Text-to-Graphgeneration,enablinguserstospecifydomain/networknames/statisticsthroughprompts
tocontrolthegeneratedgraphs. Lookingahead,weidentifyseveralpotentialtransformativeresearch
problems in Section 7. To foster further innovation and community collaboration, we release
the complete resources of LGGMs, including code, data, and model checkpoints. We invite the
communitytousethesetoolstoexplorenewpossibilitiesingraphgenerationandbeyond.
9References
[1] JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoni
Aleman,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4
technicalreport. arXivpreprintarXiv:2303.08774,2023.
[2] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg.
Structureddenoisingdiffusionmodelsindiscretestate-spaces. AdvancesinNeuralInformation
ProcessingSystems,34:17981–17993,2021.
[3] DavideBacciu,AlessioMicheli,andMarcoPodda. Edge-basedsequentialgraphgeneration
withrecurrentneuralnetworks. Neurocomputing,416:177–189,2020.
[4] JürgenBajorath. Integrationofvirtualandhigh-throughputscreening. NatureReviewsDrug
Discovery,1(11):882–894,2002.
[5] DanielBeard. Firefly: web-basedinteractivetoolforthevisualizationandvalidationofimage
processingalgorithms. PhDthesis,UniversityofMissouri–Columbia,2009.
[6] TimBrooks,BillPeebles,ConnorHolmes,WillDePue,YufeiGuo,LiJing,DavidSchnurr,Joe
Taylor,TroyLuhman,EricLuhman,ClarenceNg,RickyWang,andAdityaRamesh. Video
generationmodelsasworldsimulators. 2024.
[7] SébastienBubeck,VarunChandrasekaran,RonenEldan,JohannesGehrke,EricHorvitz,Ece
Kamar,PeterLee,YinTatLee,YuanzhiLi,ScottLundberg,etal. Sparksofartificialgeneral
intelligence: Earlyexperimentswithgpt-4. arXivpreprintarXiv:2303.12712,2023.
[8] HanqunCao,ChengTan,ZhangyangGao,YilunXu,GuangyongChen,Pheng-AnnHeng,and
StanZLi. Asurveyongenerativediffusionmodels. IEEETransactionsonKnowledgeand
DataEngineering,2024.
[9] Xiaohui Chen, Jiaxing He, Xu Han, and Li-Ping Liu. Efficient and degree-guided graph
generationviadiscretediffusionmodeling. arXivpreprintarXiv:2305.04111,2023.
[10] Xuelong Dai, KaishengLiang, andBin Xiao. Advdiff: Generating unrestrictedadversarial
examplesusingdiffusionmodels. arXivpreprintarXiv:2307.12499,2023.
[11] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert: Pre-trainingof
deepbidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,
2018.
[12] YuanqiDu,XiaojieGuo,AmardaShehu,andLiangZhao. Interpretablemoleculegeneration
viadisentanglementlearning. InProceedingsofthe11thACMInternationalConferenceon
Bioinformatics,ComputationalBiologyandHealthInformatics,pages1–8,2020.
[13] YuanqiDu,XiaojieGuo,AmardaShehu,andLiangZhao. Interpretablemoleculargraphgener-
ationviamonotonicconstraints. InProceedingsofthe2022SIAMInternationalConferenceon
DataMining(SDM),pages73–81.SIAM,2022.
[14] YuanqiDu,YinkaiWang,FardinaAlam,YuanjieLu,XiaojieGuo,LiangZhao,andAmarda
Shehu. Deep latent-variable models for controllable molecule generation. In 2021 IEEE
InternationalConferenceonBioinformaticsandBiomedicine(BIBM),pages372–375.IEEE,
2021.
[15] FaezehFaez,YassamanOmmi,MahdiehSoleymaniBaghshah,andHamidRRabiee. Deep
graphgenerators: Asurvey. IEEEAccess,9:106675–106702,2021.
[16] ShuangfeiFanandBertHuang. Conditionallabeledgraphgenerationwithgans. InProc.ICLR
WorkshopRepresent.Learn.GraphsManifolds,2019.
[17] DmitriiGavrilevandEvgenyBurnaev. Anomalydetectioninnetworksviascore-basedgenera-
tivemodels. arXivpreprintarXiv:2306.15324,2023.
10[18] AnnaGoldenberg,AliceXZheng,StephenEFienberg,EdoardoMAiroldi,etal. Asurvey
ofstatisticalnetworkmodels. FoundationsandTrends®inMachineLearning,2(2):129–233,
2010.
[19] XiaojieGuo,YuanqiDu,SivaniTadepalli,LiangZhao,andAmardaShehu. Generatingtertiary
proteinstructuresviainterpretablegraphvariationalautoencoders. BioinformaticsAdvances,
1(1):vbab036,2021.
[20] XiaojieGuo,YuanqiDu,andLiangZhao. Propertycontrollablevariationalautoencodervia
invertiblemutualdependence. InInternationalConferenceonLearningRepresentations,2020.
[21] Xiaojie Guo and Liang Zhao. A systematic survey on deep generative models for graph
generation. IEEETransactionsonPatternAnalysisandMachineIntelligence,45(5):5370–5390,
2022.
[22] EmielHoogeboom,VıctorGarciaSatorras,ClémentVignac,andMaxWelling. Equivariant
diffusionformoleculegenerationin3d. InInternationalconferenceonmachinelearning,pages
8867–8887.PMLR,2022.
[23] IliaIgashov,HannesStärk,ClémentVignac,ArneSchneuing,VictorGarciaSatorras,Pascal
Frossard, MaxWelling, MichaelBronstein, andBrunoCorreia. Equivariant3d-conditional
diffusionmodelformolecularlinkerdesign. NatureMachineIntelligence,pages1–11,2024.
[24] Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. Score-based generative modeling of graphs
viathesystemofstochasticdifferentialequations. InInternationalConferenceonMachine
Learning,pages10362–10383.PMLR,2022.
[25] MintongKang,DawnSong,andBoLi. Diffattack: Evasionattacksagainstdiffusion-based
adversarialpurification. AdvancesinNeuralInformationProcessingSystems,36,2024.
[26] EricDKolaczykandGáborCsárdi. StatisticalanalysisofnetworkdatawithR,volume65.
Springer,2014.
[27] ClementLeeandDarrenJWilkinson. Areviewofstochasticblockmodelsandextensionsfor
graphclustering. AppliedNetworkScience,4(1):1–50,2019.
[28] JureLeskovec,DeepayanChakrabarti,JonKleinberg,ChristosFaloutsos,andZoubinGhahra-
mani. Kroneckergraphs: anapproachtomodelingnetworks. JournalofMachineLearning
Research,11(2),2010.
[29] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed,
Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence
pre-trainingfornaturallanguagegeneration,translation,andcomprehension. arXivpreprint
arXiv:1910.13461,2019.
[30] GangLiu,EricInae,TongZhao,JiaxinXu,TengfeiLuo,andMengJiang. Data-centriclearning
fromunlabeledgraphswithdiffusionmodel.Advancesinneuralinformationprocessingsystems,
36,2024.
[31] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.Advances
inneuralinformationprocessingsystems,36,2024.
[32] KayLiu,HuijunLonaYu,YaoYan,ZiqingHu,PankajRajak,AmilaWeerasinghe,OlcayBoz,
DeepayanChakrabarti,andFeiWang. Graphdiffusionmodelsforanomalydetection. 2024.
[33] MengLiu,KeqiangYan,BoraOztekin,andShuiwangJi. Graphebm: Moleculargraphgenera-
tionwithenergy-basedmodels. arXivpreprintarXiv:2102.00546,2021.
[34] ZewenLiu,GuanchengWan,BAdityaPrakash,MaxSYLau,andWeiJin. Areviewofgraph
neuralnetworksinepidemicmodeling. arXivpreprintarXiv:2403.19852,2024.
[35] Victor Livernoche, Vineet Jain, Yashar Hezaveh, and Siamak Ravanbakhsh. On diffusion
modelingforanomalydetection. arXivpreprintarXiv:2305.18593,2023.
11[36] YouzhiLuo,KeqiangYan,andShuiwangJi. Graphdf: Adiscreteflowmodelformolecular
graphgeneration. InInternationalconferenceonmachinelearning,pages7192–7203.PMLR,
2021.
[37] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and
LemanAkoglu. Acomprehensivesurveyongraphanomalydetectionwithdeeplearning. IEEE
TransactionsonKnowledgeandDataEngineering,35(12):12012–12038,2021.
[38] KaushalyaMadhawa,KatushikoIshiguro,KosukeNakago,andMotokiAbe. Graphnvp: An
invertibleflowmodelforgeneratingmoleculargraphs. arXivpreprintarXiv:1905.11600,2019.
[39] HaitaoMao,ZhikaiChen,WenzhuoTang,JiananZhao,YaoMa,TongZhao,NeilShah,Michael
Galkin,andJiliangTang. Graphfoundationmodels. arXivpreprintarXiv:2402.02216,2024.
[40] ŁukaszMaziarka,AgnieszkaPocha,JanKaczmarczyk,KrzysztofRataj,TomaszDanel,and
MichałWarchoł. Mol-cyclegan: agenerativemodelformolecularoptimization. Journalof
Cheminformatics,12(1):2,2020.
[41] Dhruv Menon and Raghavan Ranganathan. A generative approach to materials discovery,
design,andoptimization. ACSomega,7(30):25958–25973,2022.
[42] MarkEJNewman. Modelsofthesmallworld. JournalofStatisticalPhysics,101:819–841,
2000.
[43] Rajvardhan Oak, Min Du, David Yan, Harshvardhan Takawale, and Idan Amit. Malware
detectiononhighlyimbalanceddatathroughsequencemodeling. InProceedingsofthe12th
ACMWorkshoponartificialintelligenceandsecurity,pages37–48,2019.
[44] John Palowitch, Anton Tsitsulin, Brandon Mayer, and Bryan Perozzi. Graphworld: Fake
graphsbringrealinsightsforgnns. InProceedingsofthe28thACMSIGKDDConferenceon
KnowledgeDiscoveryandDataMining,pages3691–3701,2022.
[45] GuilhermePenedo,QuentinMalartic,DanielHesslow,RuxandraCojocaru,AlessandroCappelli,
HamzaAlobeidli,BaptistePannier,EbtesamAlmazrouei,andJulienLaunay. Therefinedweb
datasetforfalconllm: outperformingcuratedcorporawithwebdata,andwebdataonly. arXiv
preprintarXiv:2306.01116,2023.
[46] MariyaPopova,MykhailoShvets,JunierOliva,andOlexandrIsayev. Molecularrnn:Generating
realisticmoleculargraphswithoptimizedproperties. arXivpreprintarXiv:1905.13372,2019.
[47] ChengweiQin,AstonZhang,ZhuoshengZhang,JiaaoChen,MichihiroYasunaga,andDiyi
Yang. Ischatgptageneral-purposenaturallanguageprocessingtasksolver? arXivpreprint
arXiv:2302.06476,2023.
[48] YimingQin,HuangjieZheng,JiangchaoYao,MingyuanZhou,andYaZhang. Class-balancing
diffusionmodels. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages18434–18443,2023.
[49] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,
pages8748–8763.PMLR,2021.
[50] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages10684–10695,2022.
[51] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks
for biomedical image segmentation. In Medical image computing and computer-assisted
intervention–MICCAI2015: 18thinternationalconference,Munich,Germany,October5-9,
2015,proceedings,partIII18,pages234–241.Springer,2015.
[52] RyanRossiandNesreenAhmed. Thenetworkdatarepositorywithinteractivegraphanalytics
andvisualization. InProceedingsoftheAAAIconferenceonartificialintelligence,volume29,
2015.
12[53] RyanARossiandNesreenKAhmed. Aninteractivedatarepositorywithvisualanalytics. ACM
SIGKDDExplorationsNewsletter,17(2):37–41,2016.
[54] RyanARossiandNesreenKAhmed. Complexnetworksarestructurallydistinguishableby
domain. SocialNetworkAnalysisandMining,9:1–13,2019.
[55] Pratha Sah, José David Méndez, and Shweta Bansal. A multi-species repository of social
networks. Scientificdata,6(1):44,2019.
[56] PrathaSah,KennethENussear,ToddCEsque,ChristinaMAiello,PeterJHudson,andShweta
Bansal.Inferringsocialstructureanditsdriversfromrefugeuseinthedeserttortoise,arelatively
solitaryspecies. BehavioralEcologyandSociobiology,70:1277–1289,2016.
[57] MartinSimonovskyandNikosKomodakis. Graphvae: Towardsgenerationofsmallgraphs
usingvariationalautoencoders. InArtificialNeuralNetworksandMachineLearning–ICANN
2018: 27thInternationalConferenceonArtificialNeuralNetworks,Rhodes,Greece,October
4-7,2018,Proceedings,PartI27,pages412–422.Springer,2018.
[58] GowthamiSomepalli,VasuSingla,MicahGoldblum,JonasGeiping,andTomGoldstein. Diffu-
sionartordigitalforgery? investigatingdatareplicationindiffusionmodels. InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages6048–6058,
2023.
[59] SebastianSosa,DavidJacoby,MathieuLihoreau,andCédricSueur. Animalsocialnetworks:
Towardsanintegrativeframeworkembeddingsocialinteractions,spaceandtime. Methodsin
EcologyandEvolution,12(1):4–9,2021.
[60] RyleeThompson,BorisKnyazev,ElaheGhalebi,JungtaekKim,andGrahamWTaylor. On
evaluationmetricsforgraphgenerativemodels. arXivpreprintarXiv:2201.09871,2022.
[61] YijunTian,HuanSong,ZichenWang,HaozhuWang,ZiqingHu,FangWang,NiteshVChawla,
andPanpanXu. Graphneuralpromptingwithlargelanguagemodels. InProceedingsofthe
AAAIConferenceonArtificialIntelligence,volume38,pages19080–19088,2024.
[62] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[63] LaurensVanderMaatenandGeoffreyHinton. Visualizingdatausingt-sne. Journalofmachine
learningresearch,9(11),2008.
[64] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher, and Pas-
cal Frossard. Digress: Discrete denoising diffusion for graph generation. In The Eleventh
InternationalConferenceonLearningRepresentations,2023.
[65] CunxiangWang,XiaozeLiu,YuanhaoYue,XiangruTang,TianhangZhang,ChengJiayang,
YunzhiYao,WenyangGao,XumingHu,ZehanQi,etal. Surveyonfactualityinlargelanguage
models: Knowledge,retrievalanddomain-specificity. arXivpreprintarXiv:2310.07521,2023.
[66] JianfengWang,ZhengyuanYang,XiaoweiHu,LinjieLi,KevinLin,ZheGan,ZichengLiu,
CeLiu,andLijuanWang. Git: Agenerativeimage-to-texttransformerforvisionandlanguage.
arXivpreprintarXiv:2205.14100,2022.
[67] Yu Wang, Nedim Lipka, Ryan A Rossi, Alexa Siu, Ruiyi Zhang, and Tyler Derr. Knowl-
edgegraphpromptingformulti-documentquestionanswering. InProceedingsoftheAAAI
ConferenceonArtificialIntelligence,volume38,pages19206–19214,2024.
[68] DuncanJWattsandStevenHStrogatz. Collectivedynamicsof‘small-world’networks. nature,
393(6684):440–442,1998.
[69] JiaxuanYou,RexYing,XiangRen,WilliamHamilton,andJureLeskovec. Graphrnn: Generat-
ingrealisticgraphswithdeepauto-regressivemodels. InInternationalconferenceonmachine
learning,pages5708–5717.PMLR,2018.
13[70] ChengxiZangandFeiWang.Moflow:aninvertibleflowmodelforgeneratingmoleculargraphs.
InProceedingsofthe26thACMSIGKDDinternationalconferenceonknowledgediscovery&
datamining,pages617–626,2020.
[71] YanzheZhang,RuiyiZhang,JiuxiangGu,YufanZhou,NedimLipka,DiyiYang,andTongSun.
Enhancedvisualinstructiontuningfortext-richimageunderstanding. 2023.
[72] YanzheZhang,RuiyiZhang,JiuxiangGu,YufanZhou,NedimLipka,DiyiYang,andTongSun.
Llavar: Enhancedvisualinstructiontuningfortext-richimageunderstanding. arXivpreprint
arXiv:2306.17107,2023.
[73] ZhanjieZhang,QuanweiZhang,WeiXing,GuangyuanLi,LeiZhao,JiakaiSun,ZehuaLan,
JunshengLuan,YilingHuang,andHuaizhongLin. Artbank: Artisticstyletransferwithpre-
traineddiffusionmodelandimplicitstylepromptbank. InProceedingsoftheAAAIConference
onArtificialIntelligence,volume38,pages7396–7404,2024.
[74] XujiangZhao,JiayingLu,ChengyuanDeng,CanZheng,JunxiangWang,TanmoyChowdhury,
LiYun,HejieCui,ZhangXuchao,TianjiaoZhao,etal.Domainspecializationasthekeytomake
largelanguagemodelsdisruptive: Acomprehensivesurvey. arXivpreprintarXiv:2305.18703,
2023.
[75] CeZhou,QianLi,ChenLi,JunYu,YixinLiu,GuangjingWang,KaiZhang,ChengJi,Qiben
Yan,LifangHe,etal. Acomprehensivesurveyonpretrainedfoundationmodels: Ahistory
fromberttochatgpt. arXivpreprintarXiv:2302.09419,2023.
[76] YanqiaoZhu,YuanqiDu,YinkaiWang,YichenXu,JieyuZhang,QiangLiu,andShuWu. A
surveyondeepgraphgeneration:Methodsandapplications.InLearningonGraphsConference,
pages47–1.PMLR,2022.
14Appendix
Table of Contents
A Notations 16
B ProofofTheorems 17
C DataPreparation 18
C.1 Pre-processedGraphsforTrainingLGGMs . . . . . . . . . . . . . . . . . . . . 18
C.2 PreparationofGraphsandTextualDescriptionAboutTheirDomains/Names . . 18
C.3 PreparingGraphsandTheirTextualDescriptionaboutGraphProperty . . . . . . 20
D ExperimentalSetting 21
D.1 EvaluationMetrics. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D.2 HyperparameterDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D.3 ParadigmSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
E FullExperimentalResults 22
E.1 Out-of-domainPerformanceComparisonbetweenDiGressandLGGM . . . . . 22
E.2 PerformanceComparisonbetweenFine-tunedDiGressandFine-tunedLGGM . 23
E.3 PerformanceComparisonbetweenDiGressdirectlytrainedonXandFine-tuned
LGGM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
E.4 Text-to-GraphGeneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
E.5 SensitiveAnalysisonNumberofTrainingDataunderDomainSpecificTransition 26
E.6 SensitiveAnalysisonNumberofTrainingDataunderUniformTransitionStrategy 27
E.7 ComparingtheDomainastheTextualConditionbefore/aftershuffling . . . . . . 28
15A Notations
Thissectionsummarizesthenotationsusedthroughoutthispaper.
Table4: Notationsusedthroughoutthispaper.
Notations DefinitionsorDescriptions
G,Gc Randomvariableofuniversalgraphsandgraphsfromdomainc
G,Gc Setofuniversalgraphsandgraphsfromdomainc
GTrain,Val,Test,c Setoftraining/validation/testinggraphsfromdomainc
P(G),P(Gc) Distributionofuniversalgraphsandgraphsfromdomainc
G=(XG,EG) GraphGwithnode/edgecategorymatricesXG,EG
n NumberofnodesingraphG
G
d /d Numberofnode/edgecategories
X E
Qt ,Qt Node/edgetransitionmatrices
X E
Q¯t ,Q¯t Node/edgeaccumulativetransitionmatrices
X E
mc ,mc Distributionofnode/edgecategoriesofgraphsfromdomainc
X E
t,T DiffusionsteptandthesetoftotalstepsT
G (cid:101) Distributionofgraphsfromunseendomains
G(cid:101) Setofgraphsfromunseendomains
S,ϕ(S) Textwithitsembeddingfromthepre-trainedtextualencoderϕ
P(G,S) Jointdistributionofgraphsandtheirtextualdescriptions
Θ ParametersofNeuralNetworks
Θ⋆ OptimalParametersofNeuralNetworksafterpre-training
Θ⋆⋆ OptimalParametersofNeuralNetworksafterfine-tuning
Θ▲ OptimalParametersofNeuralNetworksafterText2GraphGeneration
FB FacebookNetworks
ASN AnimalSocialNetworks
EMAIL EmailNetworks
WEB WebGraphs
ROAD RoadNetworks
POWER PowerNetworks
CHEM ChemicalNetworks
BIO BiologicalNetworks
ECON EconomicNetworks
RT RetweetNetworks
COL CollaborationNetworks
ECO EcologicalNetworks
CITATION CitationNetworks
LGGM-X Pre-trainedLGGMonallotherdomainsexceptX
Fine-tunedLGGMonX Fine-tunedLGGM-XondomainX
LGGM-T2G LGGMtrainedongraphspairedwithtexts
LGGM-T2GD LGGMtrainedongraphswithtextsondomains
LGGM-T2GUP LGGMtrainedongraphswithuserpromptsondomains/names
LGGM LGGMtrainedonallgraphsfromalldomains
16B ProofofTheorems
Theorem1. IfthetransitionmatricesQt ,Qt inEq.(1)areindependentofthetextualdescription
X E
S, thenwehaveP(Gt−1|Gt,G,S) ∝ P(Gt|Gt−1)P(Gt−1|G)andcorrespondingly, wehavethe
analyticalformedsolution,i.e.,P(Xt−1|Xt,X,S)∝Xt(Qt )⊤⊙XQ¯t−1,P(Et−1|Et,E,S)∝
X X
Et(Qt )⊤⊙EQ¯t−1following[64].
E E
Proof. ApplyingtheBayesrule,wehave:
P(Gt−1|Gt,G,S)∝P(Gt−1,Gt,G,S)∝P(Gt|Gt−1,G,S)P(Gt−1,G,S) (7)
∝P(Gt|Gt−1,G,S)P(Gt−1|G,S)P(G,S). (8)
GiventheindependenceofthetransitionmatrixonthetextualdescriptionS andalsothenoiseis
Markovian [64], we have P(Gt|Gt−1,G,S) = P(Gt|Gt−1), P(Gt−1|G,S) = P(Gt−1|G), and
alsotheirrelevanceofP(G,S)toP(Gt−1|Gt,G,S),wethenendupwith:
P(Gt−1|Gt,G,S)∝P(Gt|Gt−1)P(Gt−1|G). (9)
Sincethedistributionofgraphscanbedecomposedintothedistributionofnodeandedgecategories,
following[64],wesimilarlyhave:
P(Xt−1|Xt,X,S)∝P(Xt|Xt−1)P(Xt−1|X)=Xt(Qt )⊤⊙XQ¯t−1, (10)
X X
P(Et−1|Et,E,S)∝P(Et|Et−1)P(Et−1|E)=Et(Qt )⊤⊙EQ¯t−1. (11)
E E
■
Theorem 2. Given the decomposition in Eq. (5) that P(Gt−1|Gt,S) ∝
(cid:80) P(Gt−1|Gt,G,S)P(G|Gt,S), optimizing Θ according to Eq. (6) essentially optimizes
G
thevariationallowerboundofthelog-likelihoodP (G0,S).
Θ
Proof. Westartdirectlyfromthelog-likelihoodofthejointdistributionofP (G0,S):
Θ
(cid:90)
logP (G0,S)=log P (G0,S,G1,...,GT)d(G1,G2,...,GT) (12)
Θ Θ
(cid:90) P (G0,S,G1,...,GT)
=log Θ q(G1,G2,...,GT)d(G1,G2,...,GT) (13)
q(G1,G2,...,GT)
P (G0,S,G1,...,GT)
=logE q(G1,G2,...,GT) Θ q(G1,G2,...,GT) (14)
P (G0,S,G1,...,GT)
≥E q(G1,G2,...,GT)log Θ
q(G1,G2,...,GT)
byJensen’sinequality (15)
P(GT,S)(cid:81)T P (Gt−1|Gt,S)
=E q(G1,G1,...,GT)log q(G1)(cid:81)t T=1 qΘ
(Gt|Gt−1)
byMarkovian (16)
t=2
=E
q(G0,G1,...,GT)[logP(GT,S)+(cid:88)T logP
Θ
q( (G Gt t− |G1| tG −1t, )S)
]+const. (17)
t=1
AccordingtothedecompositioninEq(3),optimizingΘaccordingtoEq.(6)leadstooptimizing
P (Gt−1|Gt,S),whichcorrespondstothesecondterminEq.(17)andsubsequentlyoptimizesthe
Θ
variationallowerboundofthelog-likelihoodP (G0,S)accordingtothederivationfromEq.(12)
Θ
toEq.(17). Therefore, trainingText-to-GraphLGGMaccordingtoEq.(6)enablesthemodelto
generategraphssuchthatthepairsoftextsandgraphsendupwithhigherlikelihoods.
■
17C DataPreparation
C.1 Pre-processedGraphsforTrainingLGGMs
WeselectgraphsfromtheNetworkRepositoryacross13distinctyetrepresentativedomainscovering
awidevarietyofreal-worldscenarios,includingFacebook(FB),AnimalSocial(ASN),Email,Web,
Road,Power,Chemical(CHEM),Biological(BIO),Economic(ECON),Retweet(RT),Collaboration
(COL),Ecological(ECO),Citation. Duetothescalabilityissuewithdiffusion-basedgraphgenerative
models,wefurthersamplesubgraphsforcertaindomains,andTable5presentsthecomprehensive
statisticsofthesampledsubgraphs,whichareusedfortrainingLGGMs. Wecanseethatgraphsfrom
differentdomainsarestatisticallydifferent.
Table5: SummaryofGraphStatistics. Facebook(FB),AnimalSocial(ASN),Email,Web,Road,
Power, Chemical (CHEM), Biological (BIO), Economic (ECON), Retweet (RT), Collaboration
(COL),Ecological(ECO),Citation.
Num Num Avg Avg Max Min Max Min Num
Category
Nodes Edges Degree Clustering Nodes Nodes Edges Edges Graphs
ASN 52.47±40.13 77.59±80.95 2.62±1.52 0.395±0.178 283 3 515 2 267
BIO 191.14±43.47 965.71±878.35 9.16±7.69 0.276±0.199 258 109 4392 96 504
CHEM 36.46±20.49 64.61±26.23 3.75±0.63 0.421±0.223 125 2 149 1 646
Citation 235.91±27.25 1287.16±1087.00 10.17±8.14 0.369±0.224 270 175 4474 188 504
COL 174.26±53.82 312.56±176.33 3.41±1.24 0.497±0.203 247 52 996 68 504
ECO 100.67±30.10 1490.00±673.87 27.72±7.00 0.406±0.082 128 54 2106 353 6
ECON 144.18±35.82 3258.76±3540.28 39.76±37.80 0.419±0.296 219 90 11142 188 504
Email 146.67±35.86 681.55±500.28 9.79±7.26 0.389±0.211 213 82 2909 216 504
Power 132.22±20.29 289.32±183.02 4.35±2.31 0.161±0.164 187 81 1332 133 512
Road 265.25±94.31 276.46±79.61 2.70±2.08 0.078±0.134 411 32 456 137 504
RT 104.11±35.23 110.99±46.44 2.11±0.37 0.028±0.038 175 35 295 34 558
FB 219.45±47.05 1863.44±701.53 16.36±6.17 0.315±0.083 259 48 3898 46 504
Web 173.32±24.86 462.21±336.46 5.09±3.06 0.404±0.196 231 119 1607 149 504
C.2 PreparationofGraphsandTextualDescriptionAboutTheirDomains/Names
Herewethoroughlydiscusstheprocessofobtaininggraphsandtheircorrespondingtextprompts
describingtheirdomains/names. AsgivenbytheNetworkRepository,wedirectlydownloadgraphs
alongwiththeirdomains/names. WethenpromptGPT3.5togenerateuserpromptsdescribingthe
graphgivenitsdomain/name. TheconcreteprompttemplateweusehereisshowninListing1with
exemplarygenerateduserpromptsshowninListing2. Moreover,weapplythesentencetransformer
toobtaintextembeddingsofthegeneratedpromptsforeachnetworkandperformt-SNEvisualization.
AsshowninFigure6a,weseepromptsforgraphsfromdifferentdomainsfromdifferentclusters.
More importantly, textual similarity can somewhat reflect their network similarity. For example,
promptsforroadandpowernetworksareveryclose,andtheybothbelongtoinfrastructure.Moreover,
FacebookNetworks,EmailNetworks,CollaborationNetworks,WebGraphsareveryclosesinceall
thesefourbelongtosomesub-variantsofsocialnetworks. Thisinherentrelationshipbetweenthe
textualsimilarityandstructuralsimilaritybetweentwographsdemonstratesthattheworldknowledge
encodedinthetextcouldsomehowprovideusefulpreferenceforthegraphstobegenerated.
(a)DomainandName (b)AverageClusteringCoefficient (c)AverageDegree
Figure 6: t-SNE visualization of textual description about network (a) domain/name (b) average
clusteringcoefficient(c)averagedegree.
18Listing1: PromptTemplateforGeneratingTextualDescriptionaboutNetworkDomain/Name.
PROMPT:Givenagraphcalled{NETWORKNAME}thatisfromtheDOMAIN{DOMAINNAME}.
Note:
*Donotgeneratemorethan20words.
Listing2: ExamplesofTextualDescriptionaboutNetworkDomain/Name.
==============================================================================
*DOMAIN:AnimalSocialNetworks
*NAME:reptilia−tortoise−network−sl
*TEXT:Thereptilia−tortoise−network−slgraphrepresentsthesocialconnectionsamongtortoisesinthe
reptilecommunity.
==============================================================================
*DOMAIN:PowerNetworks
*NAME:power−eris1176
*TEXT:Thepower−eris1176graphrepresentstheinterconnectednodesandedgesofapowernetworksystem
==============================================================================
*DOMAIN:EconomicNetworks
*NAME:econ−poli
*TEXT:Theecon−poligraphrepresentstheinterconnectednessofeconomicandpoliticalfactorsina
network.
==============================================================================
*DOMAIN:EcologyNetworks
*NAME:eco−evergla
*TEXT:Theeco−everglagraphrepresentstheinterconnectednessofspeciesintheEvergladesecosystem.
==============================================================================
*DOMAIN:EmailNetworks
*NAME:email−enron−only
*TEXT:Theemail−enron−onlygraphrepresentsthenetworkofemailcommunicationwithintheEnron
corporation.
==============================================================================
*DOMAIN:RoadNetworks
*NAME:road−roadNet−CA
*TEXT:Theroad−roadNet−CAgraphrepresentstheroadnetworkinCalifornia.
==============================================================================
*DOMAIN:RetweetNetworks
*NAME:rt_occupywallstnyc
*TEXT:Thegraphrt_occupywallstnycrepresentsretweetrelationshipsintheOccupyWallStreetmovement
inNewYorkCity.
==============================================================================
*DOMAIN:FacebookNetworks
*NAME:socfb−Haverford76
*TEXT:Thesocfb−Haverford76graphrepresentsthesocialconnectionsamongusersintheHaverford
CollegecommunityonFacebook.
==============================================================================
*DOMAIN:WebGraphs
*NAME:web−wiki−chameleon
*TEXT:Theweb−wiki−chameleongraphrepresentstheinterconnectionsbetweenwebpages,Wikipedia
articles,andchameleonspecies.
==============================================================================
*DOMAIN:BiologicalNetworks
*NAME:bio−WormNet−v3−benchmark
*TEXT:Thebio−WormNet−v3−benchmarkgraphrepresentsabiologicalnetworkrelatedtoworms.
==============================================================================
*DOMAIN:CitationNetworks
*NAME:cit−DBLP
*TEXT:cit−DBLPisagraphrepresentingthecitationrelationshipsbetweenresearchpapersinthefieldof
computerscience.
==============================================================================
*DOMAIN:CollaborationNetworks
*NAME:ca−netscienc
*TEXT:Theca−netsciencgraphrepresentsacollaborationnetworkinthefieldofscience.
==============================================================================
19C.3 PreparingGraphsandTheirTextualDescriptionaboutGraphProperty
Herewethoroughlydiscusstheprocessofobtaininggraphsandtheircorrespondingtextprompts
describing their properties. Our goal is to demonstrate that Text2Graph LGGM can control the
statisticsofthegeneratedgraphsinthefullspectrum. However,thegraphsobtaineddirectlyfrom
theNetworkRepositorydonotcoverthewholetopologicalspace(e.g.,Figure1(a)showsthatno
networks have a higher average degree while low clustering coefficient). Therefore, we plan to
synthesizegraphscoveringthewholespacebyWatts-StrogatzSmall-worldGraphModel. Wevary
thenumberofnodesbetween[10,110],thenumberofinitialneighborsbetween[5,numberofnodes],
andalsotheprobabilityofrewiringeachedgebetween[0,1]toensurethegeneratedgraphsspan
acrossthefullspectrum.Afterthat,wegroupthegeneratedgraphsintolow,medium,andhighgroups
intermsoftheirclusteringcoefficientandaveragedegree. WeimplementthisusingNetworkX.
Afterwesynthesizegraphsanddividethemintothreegroups,wegenerateuserpromptspairedwith
thesegraphsnext. Specifically,wepromptGPT4followingthetemplatesinListing3/4. Toensure
thecompatibilitybetweenthesynthesisgraphsandthegenerateduserprompts. Wefurtherreplace
thenumberoutputbyGPT4describingthenetworkpropertywiththerealstatisticcalculatedfrom
eachnetwork.
Listing3: PromptTemplateforGeneratingTextualDescriptionaboutNetworkProperty.
==============================================================================
PROMPT:Pleasegenerateashortsentenceaboutthegraph,includingitsclusteringcoefficientinformation.
Note:
*Donotgeneratemorethan20words.
*Makesurethegeneratedsentenceincludesthelevelofclusteringcoefficient,youcaneitherspecifyitvia
wordslike[’low’,’medium’,’high’].orspecifyitvianumberslike[(0,0.25),(0.25,0.5),(0.5,0.75)]"
*Youcanalsosometimesspecifyaconcreteapplicationscenarioofthegeneratednetwork.
*Pleasebeaccuratebutalsodiverse
==============================================================================
PROMPT:Pleasegenerateashortsentenceaboutthegraph,includingitsaveragedegreeinformation.
Note:
*Donotgeneratemorethan20words.
*Makesurethegeneratedsentenceincludesthelevelofaveragedegree,youcaneitherspecifyitviawords
like[’low’,’medium’,’high’].orspecifyitvianumberslike[(0,20),(20,50),(50,100)]"
*Youcanalsosometimesspecifyaconcreteapplicationscenarioofthegeneratednetwork.
*Pleasebeaccuratebutalsodiverse
==============================================================================
Listing4: ExamplesofTextualDescriptionaboutNetworkProperty.
==============================================================================
*Thisgraphhasahighclusteringcoefficient,suggestingstrongnodeclustering.
*Pleasegenerateanetworkwithaclusteringcoefficientaround0.61,indicatingstrongclustering.
*Thisretirementcommunity’ssocialinteractiongraphdisplaysahighclusteringcoefficientof0.73,
indicativeofcloserelationships.
==============================================================================
*Withanaveragedegreeof35,thisnetworkisidealforstudyingurbantransportationpatterns.
*Thegraph’smoderateconnectivitylevelhelpsinunderstandingthestructureofsmalltomedium−sized
musicbands.
*Anaveragedegreeof41makesthisnetworksuitableforsimulatingthecollaborationinlocalartisan
markets.
==============================================================================
20D ExperimentalSetting
D.1 EvaluationMetrics
Following[60,69],weevaluatethegraphgenerationperformancebythestandardMaximumMean
Discrepancy(MMD)betweengeneratedandreferencegraphsG ,G :
g r
m n n m
1 (cid:88) 1 (cid:88) 2 (cid:88)(cid:88)
MMD(G ,G )= k(xr,xr)+ k(xg,xg)− k(xg,xr), (18)
g r m2 i j n2 i j nm i j
i,j=1 i,j=1 i=1j=1
wherek(·,·)isageneralkernelfunctionandspecificallyweuseRBFkernelfollowing[69]:
k(x ,x )=exp(−d(x ,x )/2σ2), (19)
i j i j
whered(·,·)computespairwisedistancefollowing[64]andMMDisevaluatedoverthedistributions
ofdegree(DEG),clusteringcoefficients(CC),eigenvaluesofnormalizedLaplacianmatrix(Spec)
andorbitscountsrepresentingthedistributionofallsubstructuresofsize4(Orb).
D.2 HyperparameterDetails
Forallexperiments,weselectthebestconfigurationaccordingtothegenerationperformanceon
validationgraphsandreportthefinalperformanceongeneratingtestinggraphs. Weadoptthedefault
hyperparametersettingsfromDiGress[64]withthefollowingexceptions: wegenerate100graphs
perdomainforeachevaluationandsetthetrainingepochsat300toensureconvergence. Additionally,
weimplementgradientaccumulation,usingamini-batchsizeof12across4accumulations,resulting
inaneffectivebatchsizeof48. ForText-to-GraphGeneration,thetextualencoderusedtoobtain
textualdescriptionembeddingsis"all-MiniLM-L6-v2". Allexperimentsareperformedonamachine
withA100-80GGPURAMand128GBRAM.
D.3 ParadigmSetup
Figure7comprehensivelyvisualizesthetraining/evaluationparadigmsofthefourexperiments,the
detailsofwhicharediscussedinSection6.1.
Figure7: ComprehensiveOverviewoftheExperimentalSetupforourLGGMs.
21E FullExperimentalResults
E.1 Out-of-domainPerformanceComparisonbetweenDiGressandLGGM
E.1.1 DomainSpecificTransitionStrategy
Table6: ComparingZero-shotGenerationPerformanceonUnseenGraphsindomainXbetween
DiGresstrainedonQM9andLGGM-Xpre-trainedonalldomainsexcepttheheld-outdomainX.
Domain Method DEG CC Spec Orb Domain Method DEG CC Spec Orb
DiGress 0.2695 0.3452 0.0649 0.1489 DiGress 0.2419 0.2993 0.1101 0.2978
FB BIO
LGGM-X 0.4962 0.7625 0.3408 0.7982 LGGM-X 0.2117 0.6365 0.1690 0.5156
DiGress 0.1793 0.4721 0.1751 0.5654 DiGress 0.2811 0.2042 0.2028 0.2633
ASN ECON
LGGM-X 0.0220 0.4044 0.1274 0.0505 LGGM-X 0.1916 0.0917 0.1219 0.0640
DiGress 0.2312 0.5444 0.0674 0.2650 DiGress 0.4466 0.4170 0.4483 0.4551
EMAIL
LGGM-X 0.2618 0.8650 0.3013 1.0459
RT
LGGM-X 0.0721 0.0517 0.2331 0.4085
DiGress 0.2575 0.5955 0.1907 0.9282 DiGress 0.2393 0.5341 0.2247 0.7619
WEB
LGGM-X 0.1491 0.9436 0.1154 0.4016
COL
LGGM-X 0.1493 0.9200 0.1786 0.2057
DiGress 0.4111 0.6653 0.3084 0.6530 DiGress 0.4580 0.4546 0.2144 0.4417
ROAD
LGGM-X 0.0379 0.1191 0.0759 0.0401
ECO
LGGM-X 0.2049 0.2760 0.0691 0.2107
DiGress 0.5292 0.6083 0.3556 1.2124 DiGress 0.3159 0.3664 0.1299 0.2278
POWER
LGGM-X 0.0343 0.6290 0.0649 0.0228
CITATION
LGGM-X 0.1314 0.8908 0.1188 0.6391
DiGress 0.3217 0.4589 0.2077 0.5184
ALL
LGGM-X 0.1635 0.5492 0.1597 0.3669
E.1.2 UniformTransitionStrategy
Table7: ComparingZero-shotGenerationPerformanceonUnseenGraphsindomainXbetween
DiGresstrainedonQM9andLGGM-Xpre-trainedonalldomainsexcepttheheld-outdomainX.
Domain Method DEG CC Spec Orb Domain Method DEG CC Spec Orb
DiGress 0.3376 0.6298 0.0797 0.3593 DiGress 0.2712 0.5202 0.1127 0.3188
FB BIO
LGGM-X 0.4723 0.6843 0.2924 0.7555 LGGM-X 0.1081 0.2696 0.0900 0.2053
DiGress 0.1496 0.3258 0.1506 0.4420 DiGress 0.2987 0.4841 0.2162 0.3834
ASN ECON
LGGM-X 0.0281 0.2440 0.0830 0.0618 LGGM-X 0.1213 0.0920 0.1120 0.1086
DiGress 0.2192 0.6012 0.0702 0.3416 DiGress 0.4164 0.1327 0.4147 0.5957
EMAIL
LGGM-X 0.0751 0.2364 0.0768 0.3089
RT
LGGM-X 0.0525 0.1429 0.1330 0.2219
DiGress 0.2556 0.6186 0.1877 0.6045 DiGress 0.2473 0.5826 0.2314 0.7679
WEB
LGGM-X 0.0648 0.3961 0.0549 0.1127
COL
LGGM-X 0.0736 0.5769 0.0895 0.0988
DiGress 0.3705 0.8226 0.2801 0.7198 DiGress 0.5431 0.7915 0.2338 0.6045
ROAD
LGGM-X 0.0713 0.2193 0.0987 0.2986
ECO
LGGM-X 0.4753 0.3904 0.3194 0.3934
DiGress 0.3726 0.4582 0.3270 1.4732 DiGress 0.2527 0.7790 0.1315 0.4966
POWER
LGGM-X 0.0119 0.1293 0.0373 0.0754
CITATION
LGGM-X 0.1348 0.7257 0.1160 0.4981
DiGress 0.3112 0.5622 0.2030 0.5923
ALL
LGGM-X 0.1408 0.3422 0.1253 0.2616
22E.2 PerformanceComparisonbetweenFine-tunedDiGressandFine-tunedLGGM
E.2.1 DomainSpecificTransitionStrategy
Table8: ComparingGraphGenerationPerformancebetweenFine-tunedDiGressandFine-tuned
LGGMoneachdomain. DiGress-FT:DiGresspre-trainedonQM9andfine-tunedondomainX;
LGGM-FT:LGGMpre-trainedonallotherdomainsexceptXandfine-tunedonXunderDomain
SpecificTransitionStrategy.
Domain Method DEG CC Spec Orb Domain Method DEG CC Spec Orb
DiGress-FT 0.0159 0.0564 0.0082 0.0298 DiGress-FT 0.0391 0.0354 0.0347 0.0291
FB BIO
LGGM-FT 0.0065 0.0544 0.0069 0.0282 LGGM-FT 0.0036 0.0303 0.0102 0.0342
DiGress-FT 0.0189 0.0775 0.0729 0.0886 DiGress-FT 0.0301 0.0431 0.0372 0.0392
ASN ECON
LGGM-FT 0.0014 0.0509 0.0161 0.0084 LGGM-FT 0.0215 0.0330 0.0062 0.0249
DiGress-FT 0.0208 0.0448 0.0230 0.0447 DiGress-FT 0.0054 0.0464 0.0051 0.0437
EMAIL RT
LGGM-FT 0.0166 0.0364 0.0104 0.0463 LGGM-FT 0.0012 0.0075 0.0033 0.0162
DiGress-FT 0.0192 0.0808 0.0664 0.1361 DiGress-FT 0.0255 0.2279 0.0788 0.0731
WEB COL
LGGM-FT 0.0116 0.0721 0.0152 0.0656 LGGM-FT 0.0202 0.1621 0.0571 0.0631
DiGress-FT 0.0907 0.1404 0.1099 0.1097 DiGress-FT 0.1370 0.2747 0.0476 0.2109
ROAD ECO
LGGM-FT 0.0088 0.1349 0.0347 0.0125 LGGM-FT 0.0196 0.2343 0.0291 0.2100
DiGress-FT 0.0104 0.2197 0.1023 0.0445 DiGress-FT 0.0363 0.1140 0.0469 0.0423
POWER CITATION
LGGM-FT 0.0008 0.1539 0.0215 0.0081 LGGM-FT 0.0078 0.0827 0.0137 0.0316
DiGress-FT 0.0374 0.1134 0.0528 0.0743
All
LGGM-FT 0.0010 0.0877 0.0187 0.0458
E.2.2 UniformTransitionStrategy
Table9: ComparingGraphGenerationPerformancebetweenFine-tunedDiGressandFine-tuned
LGGMoneachdomain. DiGress-FT:DiGresspre-trainedonQM9andfine-tunedondomainX;
LGGM-FT:LGGMpre-trainedonallotherdomainsexceptXandfine-tunedonXunderUniform
TransitionStrategy.
Domain Method DEG CC Spec Orb Domain Method DEG CC Spec Orb
DiGress-FT 0.0039 0.0650 0.0090 0.0304 DiGress-FT 0.0274 0.0845 0.0493 0.0312
FB BIO
LGGM-FT 0.0050 0.0579 0.0059 0.0280 LGGM-FT 0.0049 0.0496 0.0056 0.0257
DiGress-FT 0.0249 0.5604 0.0779 0.0348 DiGress-FT 0.0133 0.0355 0.0223 0.0360
ASN ECON
LGGM-FT 0.0058 0.1098 0.0311 0.0101 LGGM-FT 0.0597 0.0594 0.0216 0.0535
DiGress-FT 0.0134 0.0709 0.0223 0.0694 DiGress-FT 0.0418 0.0243 0.0495 0.0583
EMAIL RT
LGGM-FT 0.0120 0.0559 0.0158 0.0444 LGGM-FT 0.0032 0.0163 0.0051 0.0227
DiGress-FT 0.0327 0.2025 0.0858 0.2033 DiGress-FT 0.0562 0.7070 0.1086 0.1471
WEB COL
LGGM-FT 0.0218 0.1398 0.0310 0.1262 LGGM-FT 0.1074 0.4265 0.1398 0.0897
DiGress-FT 0.0843 0.1010 0.1873 0.5155 DiGress-FT 0.1118 0.3016 0.0548 0.2102
ROAD ECO
LGGM-FT 0.0081 0.0547 0.0573 0.0228 LGGM-FT 0.0204 0.2347 0.0404 0.2100
DiGress-FT 0.0231 0.1029 0.0683 0.0441 DiGress-FT 0.0277 0.1622 0.0501 0.0813
POWER CITATION
LGGM-FT 0.0077 0.0570 0.0134 0.0040 LGGM-FT 0.0052 0.0821 0.0221 0.0443
DiGress-FT 0.0384 0.2015 0.0654 0.1218
All
LGGM-FT 0.0218 0.1120 0.0324 0.0568
23E.3 PerformanceComparisonbetweenDiGressdirectlytrainedonXandFine-tunedLGGM
E.3.1 DomainSpecificTransition
Table10: ComparingGraphGenerationPerformancebetweenDiGressandFine-tunedLGGMon
eachdomain. DiGress: DiGresstraineddirectlyondomainX;LGGM-FT:LGGMpre-trainedonall
otherdomainsexceptXandfine-tunedonXunderDomainSpecificTransitionStrategy.
Domain Method DEG CC Spec Orb Domain Method DEG CC Spec Orb
DiGress 0.0423 0.0718 0.0243 0.0298 DiGress 0.0481 0.1286 0.0487 0.0460
FB BIO
LGGM-FT 0.0065 0.0544 0.0069 0.0282 LGGM-FT 0.0036 0.0303 0.0102 0.0342
DiGress 0.0319 0.0835 0.0679 0.1463 DiGress 0.0224 0.0361 0.0084 0.0325
ASN ECON
LGGM-FT 0.0014 0.0509 0.0161 0.0084 LGGM-FT 0.0215 0.0330 0.0062 0.0249
DiGress 0.0145 0.0671 0.0143 0.0558 DiGress 0.0035 0.0111 0.0094 0.0207
EMAIL RT
LGGM-FT 0.0166 0.0364 0.0104 0.0463 LGGM-FT 0.0012 0.0075 0.0033 0.0162
DiGress 0.0204 0.0778 0.0695 0.1101 DiGress 0.0278 0.2192 0.0669 0.0284
WEB COL
LGGM-FT 0.0116 0.0721 0.0152 0.0656 LGGM-FT 0.0202 0.1621 0.0571 0.0631
DiGress 0.0333 0.1342 0.0932 0.0861 DiGress 0.0268 0.2356 0.0339 0.2100
ROAD ECO
LGGM-FT 0.0088 0.1349 0.0347 0.0125 LGGM-FT 0.0196 0.2343 0.0291 0.2100
DiGress 0.0143 0.2050 0.0776 0.0392 DiGress 0.0406 0.1790 0.0677 0.0944
POWER CITATION
LGGM-FT 0.0008 0.1539 0.0215 0.0081 LGGM-FT 0.0078 0.0827 0.0137 0.0316
DiGress 0.0272 0.1208 0.0485 0.0749
All
LGGM-FT 0.0100 0.0877 0.0187 0.0458
E.3.2 UniformTransition
Table11: ComparingGraphGenerationPerformancebetweenDiGressandFine-tunedLGGMon
eachdomain. DiGress: DiGresstraineddirectlyondomainX;LGGM-FT:LGGMpre-trainedonall
otherdomainsexceptXandfine-tunedonXunderUniformTransitionStrategy.
Domain Method DEG CC Spec Orb Domain Method DEG CC Spec Orb
DiGress 0.0177 0.0698 0.0138 0.0296 DiGress 0.0179 0.0499 0.0441 0.0526
FB BIO
LGGM-FT 0.0050 0.0579 0.0059 0.0280 LGGM-FT 0.0049 0.0496 0.0056 0.0257
DiGress 0.0337 0.1744 0.0482 0.0243 DiGress 0.0229 0.0430 0.0088 0.0427
ASN ECON
LGGM-FT 0.0058 0.1098 0.0311 0.0101 LGGM-FT 0.0597 0.0594 0.0216 0.0535
DiGress 0.0259 0.0901 0.0366 0.0743 DiGress 0.0336 0.0920 0.0432 0.0572
EMAIL RT
LGGM-FT 0.0120 0.0559 0.0158 0.0444 LGGM-FT 0.0032 0.0163 0.0051 0.0227
DiGress 0.0239 0.0898 0.1033 0.2371 DiGress 0.0252 0.5156 0.1171 0.2060
WEB COL
LGGM-FT 0.0218 0.1398 0.0310 0.1262 LGGM-FT 0.1074 0.4265 0.1398 0.0897
DiGress 0.1553 0.2788 0.2169 0.0542 DiGress 0.0263 0.2359 0.0439 0.2100
ROAD ECO
LGGM-FT 0.0081 0.0547 0.0573 0.0228 LGGM-FT 0.0204 0.2347 0.0404 0.2100
DiGress 0.0348 0.3174 0.1083 0.1393 DiGress 0.0217 0.1566 0.0645 0.1235
POWER CITATION
LGGM-FT 0.0077 0.0570 0.0134 0.0040 LGGM-FT 0.0052 0.0821 0.0221 0.0443
DiGress 0.0366 0.1761 0.0707 0.1042
All
LGGM-FT 0.0218 0.1120 0.0324 0.0568
24E.4 Text-to-GraphGeneration
E.4.1 DomainSpecificTransition
Table12: ComparingtheperformanceofgraphgenerationbetweenLGGMtrainedongraphsfrom
alldomainswithandwithoutdomain/nameastextualconditions.
Domain Method DEG CC Spec Orb Domain Method DEG CC Spec Orb
LGGM 0.2566 0.3552 0.0587 0.1614 LGGM 0.2860 0.3275 0.1117 0.2333
FB LGGM-T2GD 0.1533 0.1894 0.0817 0.0492 BIO LGGM-T2GD 0.1313 0.5111 0.1340 0.3736
LGGM-T2GUP 0.0053 0.0576 0.0076 0.0245 LGGM-T2GUP 0.0219 0.0251 0.0126 0.0190
LGGM 0.1477 0.3003 0.1551 0.3719 LGGM 0.3540 0.3404 0.2078 0.2740
ASN LGGM-T2GD 0.0429 0.4742 0.0949 0.0401 ECON LGGM-T2GD 0.2346 0.1572 0.1550 0.0579
LGGM-T2GUP 0.0161 0.1312 0.0344 0.0174 LGGM-T2GUP 0.0869 0.0601 0.0412 0.0592
LGGM 0.1957 0.2629 0.0646 0.2118 LGGM 0.4355 0.3924 0.4329 0.4966
EMAIL LGGM-T2GD 0.0874 0.3238 0.1472 0.2869 RT LGGM-T2GD 0.0050 0.0940 0.0415 0.2870
LGGM-T2GUP 0.0077 0.0316 0.0176 0.0365 LGGM-T2GUP 0.0034 0.0253 0.0225 0.0869
LGGM 0.2461 0.3570 0.1853 0.4832 LGGM 0.2616 0.3398 0.2305 0.7090
WEB LGGM-T2GD 0.1253 0.9088 0.1156 0.3884 COL LGGM-T2GD 0.1301 0.9384 0.1963 0.2032
LGGM-T2GUP 0.0771 0.2720 0.0732 0.1251 LGGM-T2GUP 0.0845 0.5070 0.1378 0.1531
LGGM 0.4315 0.8107 0.3192 0.6976 LGGM 0.4611 0.3108 0.1932 0.3468
ROAD LGGM-T2GD 0.0112 0.1611 0.0298 0.0120 ECO LGGM-T2GD 0.0575 0.2976 0.0585 0.2580
LGGM-T2GUP 0.0097 0.1316 0.0324 0.0119 LGGM-T2GUP 0.1070 0.2913 0.0410 0.2556
LGGM 0.4411 0.4694 0.3384 1.3222 LGGM 0.3392 0.5009 0.1295 0.2248
POWER LGGM-T2GD 0.0194 0.6031 0.0286 0.0193 CITATION LGGM-T2GD 0.1636 0.8868 0.2036 0.6142
LGGM-T2GUP 0.0227 0.4817 0.0330 0.0223 LGGM-T2GUP 0.0496 0.0914 0.0669 0.0318
LGGM 0.3213 0.3973 0.2022 0.4610
ALL LGGM-T2GD 0.0968 0.4621 0.1072 0.2158
LGGM-T2GUP 0.0410 0.1755 0.0434 0.0703
E.4.2 UniformTransition
Table13: ComparingtheperformanceofgraphgenerationbetweenLGGMtrainedongraphsfrom
alldomainswithandwithoutdomain/nameastextualconditions.
Domain Method DEG CC Spec Orb Domain Method DEG CC Spec Orb
LGGM 0.0321 0.4994 0.0763 0.3117 LGGM 0.2661 0.3120 0.1135 0.3835
FB LGGM-T2GD 0.1561 0.1639 0.0924 0.0417 BIO LGGM-T2GD 0.0099 0.1286 0.0303 0.1366
LGGM-T2GUP 0.0050 0.0545 0.0070 0.0251 LGGM-T2GUP 0.0028 0.0287 0.0236 0.0174
LGGM 0.1511 0.4325 0.1875 0.3896 LGGM 0.3828 0.1533 0.2039 0.2583
ASN LGGM-T2GD 0.0318 0.2821 0.0606 0.0631 ECON LGGM-T2GD 0.0666 0.0594 0.0650 0.0586
LGGM-T2GUP 0.0211 0.1191 0.0462 0.0195 LGGM-T2GUP 0.0132 0.0257 0.0053 0.0191
LGGM 0.2156 0.2450 0.0666 0.2757 LGGM 0.4395 0.2225 0.4337 0.6641
EMAIL LGGM-T2GD 0.0469 0.0982 0.0484 0.0505 RT LGGM-T2GD 0.0468 0.0955 0.0729 0.0393
LGGM-T2GUP 0.0073 0.0379 0.0127 0.0437 LGGM-T2GUP 0.0286 0.0933 0.0400 0.0312
LGGM 0.2725 0.2672 0.1900 0.4368 LGGM 0.3565 0.3554 0.2451 0.7874
WEB LGGM-T2GD 0.0255 0.0737 0.0354 0.1856 COL LGGM-T2GD 0.0395 0.3110 0.1146 0.1823
LGGM-T2GUP 0.0105 0.0941 0.0206 0.0451 LGGM-T2GUP 0.0265 0.2813 0.0895 0.0899
LGGM 0.4825 0.5373 0.3398 0.7542 LGGM 0.5466 0.6003 0.2257 0.7089
ROAD LGGM-T2GD 0.0088 0.1225 0.0399 0.0155 ECO LGGM-T2GD 0.2160 0.2917 0.1203 0.2569
LGGM-T2GUP 0.0177 0.0437 0.0336 0.0086 LGGM-T2GUP 0.0293 0.2885 0.0416 0.2556
LGGM 0.4394 0.4646 0.3473 1.3186 LGGM 0.2624 0.5374 0.1295 0.3419
POWER LGGM-T2GD 0.0162 0.1131 0.0479 0.1786 CITATION LGGM-T2GD 0.0101 0.1025 0.0315 0.0651
LGGM-T2GUP 0.0062 0.0570 0.0111 0.0084 LGGM-T2GUP 0.0072 0.0849 0.0115 0.0287
LGGM 0.3206 0.3856 0.2132 0.5526
ALL LGGM-T2GD 0.0562 0.1535 0.0633 0.1061
LGGM-T2GUP 0.0146 0.1007 0.0286 0.0494
25E.5 SensitiveAnalysisonNumberofTrainingDataunderDomainSpecificTransition
(a)Road-DEG (b)Road-CC (c)Road-Orb (d)Road-Spec
Figure8: EffectofNumberofTrainingGraphsonRoadNetworks.
(a)Retweet-DEG (b)Retweet-CC (c)Retweet-Orb (d)Retweet-Spec
Figure9: EffectofNumberofTrainingGraphsonRetweetNetworks.
(a)Email-DEG (b)Email-CC (c)Email-Orb (d)Email-Spec
Figure10: EffectofNumberofTrainingGraphsonEmailNetworks.
(a)Web-DEG (b)Web-CC (c)Web-Orb (d)Web-Spec
Figure11: EffectofNumberofTrainingGraphsonWebGraphs.
(a)Facebook-DEG (b)Facebook-CC (c)Facebook-Orb (d)Facebook-Spec
Figure12: EffectofNumberofTrainingGraphsonFacebookNetworks.
26E.6 SensitiveAnalysisonNumberofTrainingDataunderUniformTransitionStrategy
(a)Citation-DEG (b)Citation-CC (c)Citation-Orb (d)Citation-Spec
Figure13: EffectofNumberofTrainingGraphsonCitationNetworks.
(a)Retweet-DEG (b)Retweet-CC (c)Retweet-Orb (d)Retweet-Spec
Figure14: EffectofNumberofTrainingGraphsonRetweetNetworks.
(a)Email-DEG (b)Email-CC (c)Email-Orb (d)Email-Spec
Figure15: EffectofNumberofTrainingGraphsonEmailNetworks.
(a)Web-DEG (b)Web-CC (c)Web-Orb (d)Web-Spec
Figure16: EffectofNumberofTrainingGraphsonWebGraphs.
(a)Facebook-DEG (b)Facebook-CC (c)Facebook-Orb (d)Facebook-Spec
Figure17: EffectofNumberofTrainingGraphsonFacebookNetworks.
27E.7 ComparingtheDomainastheTextualConditionbefore/aftershuffling
E.7.1 DomainSpecificTransition
Table14: ComparingtheperformanceofgraphgenerationbetweenLGGMtrainedongraphsfrom
alldomainswithandwithoutdomain/nameastextualconditionsunderdomain-specifictransition.
Domain Method DEG CC Spec Orb Domain Method DEG CC Spec Orb
LGGM-T2GD 0.1533 0.1894 0.0817 0.0492 LGGM-T2GD 0.1313 0.5111 0.1340 0.3736
FB BIO
LGGM-T2GD* 0.2323 0.2618 0.1590 0.0923 LGGM-T2GD* 0.1762 0.5887 0.1460 0.4929
LGGM-T2GD 0.0429 0.4742 0.0949 0.0401 LGGM-T2GD 0.2346 0.1572 0.1550 0.0579
ASN ECON
LGGM-T2GD* 0.0891 0.5725 0.1446 0.0610 LGGM-T2GD* 0.2029 0.3393 0.2298 0.0579
LGGM-T2GD 0.0874 0.3238 0.1472 0.2869 LGGM-T2GD 0.0050 0.0940 0.0415 0.2870
EMAIL
LGGM-T2GD* 0.2169 0.7497 0.2825 0.8397
RT
LGGM-T2GD* 0.0240 0.1023 0.1374 0.4123
LGGM-T2GD 0.1253 0.9088 0.1156 0.3884 LGGM-T2GD 0.1301 0.9384 0.1963 0.2032
WEB
LGGM-T2GD* 0.1464 0.9776 0.1460 0.4211
COL
LGGM-T2GD* 0.1529 0.9684 0.2313 0.2089
LGGM-T2GD 0.0112 0.1611 0.0298 0.0120 LGGM-T2GD 0.0575 0.2976 0.0585 0.2580
ROAD
LGGM-T2GD* 0.0365 0.2430 0.0605 0.0500
ECO
LGGM-T2GD* 0.1964 0.3330 0.1438 0.2574
LGGM-T2GD 0.0194 0.6031 0.0286 0.0193 LGGM-T2GD 0.1636 0.8868 0.2036 0.6142
POWER
LGGM-T2GD* 0.0434 0.6721 0.0626 0.0231
CITATION
LGGM-T2GD* 0.1615 0.9553 0.1903 0.6078
LGGM-T2GD 0.0968 0.4621 0.1072 0.2158
ALL
LGGM-T2GD* 0.1399 0.5636 0.1611 0.2937
E.7.2 UniformTransition
Table15: ComparingtheperformanceofgraphgenerationbetweenLGGMtrainedongraphsfrom
alldomainswithandwithoutdomain/nameastextualconditionsunderuniformtransitionstrategy.
Domain Method DEG CC Spec Orb Domain Method DEG CC Spec Orb
LGGM-T2GD 0.1561 0.1639 0.0924 0.0417 LGGM-T2GD 0.0099 0.1286 0.0303 0.1366
FB BIO
LGGM-T2GD* 0.3018 0.4207 0.2069 0.2622 LGGM-T2GD* 0.0754 0.2889 0.0881 0.2783
LGGM-T2GD 0.0318 0.2821 0.0606 0.0631 LGGM-T2GD 0.0665 0.0594 0.0650 0.0586
ASN ECON
LGGM-T2GD* 0.0637 0.1561 0.1416 0.2351 LGGM-T2GD* 0.1035 0.0736 0.0971 0.0922
LGGM-T2GD 0.0469 0.0982 0.0484 0.0505 LGGM-T2GD 0.0468 0.0955 0.0729 0.0393
EMAIL
LGGM-T2GD* 0.1107 0.2322 0.1315 0.1692
RT
LGGM-T2GD* 0.1399 0.3913 0.2441 0.2497
LGGM-T2GD 0.0255 0.0737 0.0354 0.1856 LGGM-T2GD 0.0395 0.3110 0.1146 0.1823
WEB
LGGM-T2GD* 0.0485 0.0830 0.1340 0.2669
COL
LGGM-T2GD* 0.0323 0.4972 0.1159 0.5375
LGGM-T2GD 0.0088 0.1225 0.0399 0.0155 LGGM-T2GD 0.2160 0.2917 0.1203 0.2569
ROAD
LGGM-T2GD* 0.0453 0.1005 0.1257 0.3803
ECO
LGGM-T2GD* 0.3722 0.3210 0.2226 0.2771
LGGM-T2GD 0.0162 0.1131 0.0479 0.1786 LGGM-T2GD 0.0101 0.1025 0.0315 0.0651
POWER
LGGM-T2GD* 0.0225 0.1533 0.1264 0.2957
CITATION
LGGM-T2GD* 0.0375 0.2454 0.0699 0.1363
LGGM-T2GD 0.0562 0.1535 0.0633 0.1061
ALL
LGGM-T2GD* 0.1128 0.2469 0.1420 0.2650
28