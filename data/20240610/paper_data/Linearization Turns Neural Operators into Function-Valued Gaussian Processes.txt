Linearization Turns Neural Operators into
Function-Valued Gaussian Processes
EmiliaMagnani∗ MarvinPförtner∗ TobiasWeber∗ PhilippHennig
TübingenAICenter
UniversityofTübingen
Tübingen,Germany
Abstract
Modelingdynamicalsystems,e.g.inclimateandengineeringsciences,oftenne-
cessitatessolvingpartialdifferentialequations. Neuraloperatorsaredeepneural
networksdesignedtolearnnontrivialsolutionoperatorsofsuchdifferentialequa-
tionsfromdata. Asforallstatisticalmodels,thepredictionsofthesemodelsare
imperfectandexhibiterrors.Sucherrorsareparticularlydifficulttospotinthecom-
plexnonlinearbehaviourofdynamicalsystems. Weintroduceanewframeworkfor
approximateBayesianuncertaintyquantificationinneuraloperatorsusingfunction-
valued Gaussian processes. Our approach can be interpreted as a probabilistic
analogueoftheconceptofcurryingfromfunctionalprogrammingandprovidesa
practicalyettheoreticallysoundwaytoapplythelinearizedLaplaceapproximation
toneuraloperators. InacasestudyonFourierneuraloperators,weshowthat,even
foradiscretizedinput,ourmethodyieldsaGaussianclosure–astructuredGaussian
process posterior capturing the uncertainty in the output function of the neural
operator,whichcanbeevaluatedatanarbitrarysetofpoints. Themethodadds
minimalpredictionoverhead,canbeappliedpost-hocwithoutretrainingtheneural
operator,andscalestolargemodelsanddatasets. Weshowcasetheefficacyofour
approachthroughapplicationstodifferenttypesofpartialdifferentialequations.
1 Introduction
Partialdifferentialequations(PDEs)areapowerfullanguagefordescribingthecomplexinteractions
thatariseindynamicalsystems. SolvingPDEsisthereforeacrucialresearchtopicforexplainingthe
dynamicsinherentinphysical,biological,andengineeringsystems. Inscientificmachinelearning,
modelspredictingphysicalphenomenasuchasweatherorclimateareoftentrainedtoapproximate
thesolutionofanunderlyingPDEfromdata.
LearningtosolvePDEsiscloselyrelatedtooperatorlearning: Insteadoflearningtosolveaspecific
PDE,itcanbebeneficialtolearntheoperatorthatmapsafunctionalparameterofthePDE(suchas
initialvalues,boundaryconditions,forcefields,ormaterialparameters)tothesolutionassociated
with the given parameter. This approach is powerful because it learns to solve entire classes of
PDEssimultaneously. Additionally,learningmappingsbetweenfunctionspacescanmakemodels
discretizationinvariant[1],whichisbeneficialfortrainingefficiency. Thisisincontrasttoclassical
numericalsolverslikefinitedifferenceorfiniteelementmethodsthatdiscretizethespaceonamesh.
Neural operators, and in particular Fourier neural operators (FNOs), stand out among the deep
architecturesthatareabletoeffectivelylearnoperatorsandarenowwidelyusedinpractice. Their
applications span various domains such as weather forecasting [2, 3], fluid dynamics [4–6], and
automotiveaerodynamics[7].
∗Equalcontribution.
4202
nuJ
7
]GL.sc[
1v27050.6042:viXraThecomplexnatureofdynamicsystemsmakeserrorsinpredictionsdifficulttodetect. Uncertainty
quantificationaimstoaccountfortheseerrorsbyprovidinganestimateofpredictionquality.However,
currentmethodsinoperatorlearningareusuallynotabletoprovidesuchanestimate,hinderingtheir
utilityinapplications.
Toaddresstheseissues,wedeveloptheneuraloperatorLaplaceapproximation(NOLA),anovel
frameworkforapproximateBayesianuncertaintyquantification[8]inneuraloperatorsusingfunction-
valuedGaussianprocesses. Ourmethodleveragestheconceptofcurryinginfunctionalprogramming
toenabletheapplicationofthelinearizedLaplaceapproximation[9]toneuraloperators. Wethen
showthattheresultingapproximateBayesianposteriorovertheparametersoftheneuraloperator
induces a function-valued Gaussian process belief over the operator learned by the network. By
consideringFourierneuraloperators,weshowthatNOLAprovidesastructuredGaussianprocess
posterior,capturingtheuncertaintyintheneuraloperator’soutputfunctionandallowingforefficient
evaluation at arbitrary points. NOLA is practical, introduces minimal additional computational
overhead,andcanbeappliedpost-hoc,withoutthenecessitytoretraintheneuraloperator. Moreover,
itscalesefficientlytolargemodelsanddatasetsand,justasneuraloperators,isapplicabletodifferent
typesofPDEs.
InSection2weprovideabriefoverviewofneuraloperators,(multi-output)Gaussianprocesses,and
thelinearizedLaplaceapproximation. InSection3wefirstdevelopGaussianprocessestakingvalues
in(infinite-dimensional)Banachspacesoffunctions,aswellasthenotionofGaussiancurrying,
which formalizes their equivalence to multi-output Gaussian processes. We then use Gaussian
curryingtoconstructfunction-valuedGaussianprocessesfromneuraloperatorswithGaussianweight
posteriors. WediscussrelatedworkinSection4andshowcasetheefficacyondifferentPDEdatasets
inSection5.
2 Background
2.1 Neuraloperators
Neuraloperators(NOs)[1]areneuralnetworkarchitecturesthatmapbetween(infinite-dimensional)
Banachspacesoffunctions. Moreprecisely,aneuraloperatorisafunctionF: A×W→U,where
• AisaBanachspaceoffunctionsa: D A →Rd′ A withdomainD A ⊂RdA,
• UisaBanachspaceoffunctionsu: D U →Rd′ U withdomainD U ⊂RdU,
• Wisasetofparameters(typicallyW⊂RporW⊂Cp).
To keep the training process computationally tractable, neural operators are trained on datasets
{(a(i)(X(i)),u(i)(X(i)))}n
consisting of pairs of input and corresponding output functions
A U i=1
(a(i),u(i))∈A×UthatarediscretizedatfinitelymanypointsX A(i) ∈(D A)n( Ai) andX U(i) ∈(D U)n( Ui) ,
respectively. Thetrainingobjectiveistypicallygivenbytheempiricalrisk
n
R(w)= 1 (cid:88) L(u(i)(X(i)),F(a(i)(X(i)),w)(X(i)))
n U A U
i=1
oraregularizedversionoftheempiricalrisk. Neuraloperatorshaveoriginallybeenmotivatedandare
commonlyusedtolearnthesolutionoperatorofnon-linear,parametricpartialdifferentialequations.
Inthiscase,wetypicallyhaveD
A
=D U,theinputfunctionsa∈Acorrespondtoparametersand/or
initialconditionsofthePDE,andtheoutputfunctionsu ∈ Uarethecorrespondingsolutionsof
thePDE(atlatertimepoints). Therearemanydifferentrealizationsoftheabstractneuraloperator
framework,includinglow-rankneuraloperators[1],(multipole)graphneuraloperators[10,11],and
(spherical)Fourierneuraloperators[3,12]. Duetotheirrecentpopularity,thelaterpartsofthiswork
specificallyaddressFourierneuraloperators.
2.1.1 Fourierneuraloperators
InSection3.2.1wewillfocusinparticularonFourierneuraloperators(FNOs)[12],neuraloperator
architecturesthatapplyallspatiallyglobaloperationsinthespectraldomain. AnFNOF transforms
2aperiodicinputfunctionaintoaperiodicoutputfunctionF(a,w)(x):=q(v(L)(x),w )with
q
 
v(l+1)(x):=σ(l)
(cid:88)d′ v F−1(cid:18)(cid:16) R(l)F(cid:16) v(l)(cid:17) (cid:17)k max(cid:19)
(x)+W(l)v(l)(x)
i kij j ij j
k k=1
j=1
forl = 1,...,L−1andv(1)(x) = p(a(x),w p) ∈ Rd′ v,whereF denotestheFouriertransform
ofaperiodicfunction.2 p: Rd′ A ×W p →Rd′ v andq: Rd′ v ×W q →Rd′ U areparametricfunctions
called lifting and projection, respectively. R(l) ∈ Ck max×d′ v×d′ v and W(l) ∈ Rd′ v×d′ v, and w =
(w ,W(1),R(1),...,W(L−1),R(L−1),w ). Themapv(l) → v(l+1) isthel-thFourierlayer. If
p q
theinputsaarediscretizedonaregulargrid,F canbecomputedbyarealfastFouriertransform
(rfft).
2.2 (Multi-output)Gaussianprocesses
AsweaimtogeneralizethenotionofaGaussianprocesslater, weprovidetheformaldefinition
ofGaussianprocessesfrommathematicalstatisticsthatisnotoftenusedinmachinelearning. A
Gaussianprocess(GP)withindexsetAonaprobabilityspace(Ω,A,P)isafunctionf: A×Ω→R
such that ω (cid:55)→ (f(a ,ω),...,f(a ,ω)) is an Rn-valued Gaussian random variable for all n ∈ N
1 n
and a ,...,a ∈ A. We use the shorthand f(a) := f(a,·). The mean function of f is given by
1 n
a (cid:55)→ E [f(a)] and the covariance function of f is given by (a ,a ) (cid:55)→ Cov [f(a ),f(a )]. We
P 1 2 P 1 2
denotebyf ∼GP(m,k)thatf isaGaussianprocesswithmeanfunctionmandcovariancefunction
k.
IfRA isequippedwiththeproducttopologyτ,thenω (cid:55)→f(·,ω)isafunction-valuedrandomvariable
withvaluesin(RA,B(τ)).
ThisjustifiesinterpretingGPsasprobabilitymeasuresoverfunctions
A→R.
ItiscommontoextendtheconceptofaGPtofinitelymanyoutputdimensions. Ad′-outputGaussian
processwithindexsetAonaprobabilityspace(Ω,A,P)isafunctionf: A×Ω→Rd′ suchthat
ω (cid:55)→ (cid:0) f(a ,ω)⊤ ··· f(a ,ω)⊤(cid:1)⊤ isanRn·d′ -valuedGaussianrandomvariableforalln ∈ N
1 n
and a ,...,a ∈ A. We use the shorthand f(a) := f(a,·). The mean function of f is given by
1 n
a(cid:55)→E [f(a)]∈Rd′ andthecovariancefunctionoff isgivenby(a ,a )(cid:55)→Cov [f(a ),f(a )]∈
P 1 2 P 1 2
Rd′×d′ . Wedenotebyf ∼GP(m,K)thatf isamulti-outputGaussianprocesswithmeanfunction
mandcovariancefunctionK. Whilethenotionofamulti-outputGaussianprocessmightseemmore
generalthanthenotionofaGaussianprocess,itispossibleto“emulate”afunctionwithmultiple
outputsbyaugmentingtheinputspaceofaGaussianprocess:
Lemma2.1. Let(Ω,A,P)beaprobabilityspace,f: A×Ω→Rd′,I={1,...,d′},andf: (A×I)×
Ω→Rwith(f(a,·)) =f((a,i),·)foralla∈Aandi∈I(P-almostsurely). Thenf ∼GP(m,K)
i
ifandonlyiff ∼GP(m,k),where
(m(a)) =m(a,i)
i
foralla∈Aandi∈I,aswellas
(K(a ,a )) =k((a ,i),(a ,j))
1 2 ij 1 2
foralla ,a ∈Aandi,j ∈I.
1 2
2.3 LinearizedLaplaceapproximation
ThelinearizedLaplaceapproximation(LLA)[9,13,14]isaconceptuallysimple,yeteffective[15]
methodforobtaininganapproximateposteriordistributionovertheparametersw ∈Rpofaneural
networkf: Rd×Rp →Rd′ . ItapplieswhenevertheobjectivefunctionRusedtotraintheneural
networkis(equivalentto)anegativelog-posterior
n
(cid:88)
R(w)=−logp(w |D)=−logp(w)− logp(y(i) |f(x(i),w))+const.
i=1
2Moreprecisely,theoperatorF: L 2(Td,R)→ℓ 2(C)mapsareal-valuedsquare-integrablefunctiononthe
d-dimensionaltorustothecoefficientsofthecorrespondingFourierseries.
3ofthenetworkparametersgivendataD = {(x(i),y(i))}n . Itiscommonfortheprioroverthe
i=1
parameterstobeGaussian,inwhichcase−logp(w)actsasanL2-regularizerontheparameters.
Duringtraining,weattempttofindalocalminimumw⋆oftheobjectivefunctionR,i.e. amaximum
a-posteriori(MAP)estimatorofthenetworkparametersgiventhedata.
FollowingImmeretal.[9],weapproximatetheposteriorofthenetworkweightsasfollows: First,we
linearizethemodelusingafirst-orderTaylorapproximationintheweightsaroundtheMAPestimator
w⋆
f(x,w)≈flin(x,w):=f(x,w⋆)+ D f(x,w)| (w−w⋆)
w⋆ w w=w⋆
Afterwards,wecomputeasecond-orderTaylorapproximationofthenegativelog-posteriorRlin of
w⋆
thelinearizednetworkattheMAPw⋆
n
(cid:88)
Rlin (w):=−logp(w)− logp(y(i) |flin(x(i),w))+const.
w⋆ w⋆
i=1
1
≈R(w⋆)+∇R(w⋆)⊤(w−w⋆)+ (w−w⋆)⊤P(w−w⋆)
2
(cid:124) (cid:123)(cid:122) (cid:125)
≈0
1
= (w−w⋆)⊤P(w−w⋆)+const. (2.1)
2
withP :=−H logp(w)| +G,where
w w=w⋆
(cid:88)n (cid:12) (cid:12) (cid:12)⊤
G:=− D f(x(i),w)(cid:12) H logp(y(i) |f)(cid:12) D f(x(i),w)(cid:12)
w (cid:12) f (cid:12) w (cid:12)
w=w⋆ f=f(x(i),w⋆) w=w⋆
i=1
istheso-calledgeneralizedGauss-Newton(GGN)matrix[16]. TheGGNisguaranteedtobepositive-
semidefinite. Equation(2.1)isthenegativelog-densityofa(potentiallydegenerate)multivariate
Gaussiandistributionwithmeanw⋆andcovariancematrixP†,i.e.
p(w=w |D)=expR(w)≈expRlin (w)≈N (cid:0) w;w⋆,P†(cid:1) .
w⋆
ThisGaussiandistributionisreferredtoasthelinearizedLaplaceapproximationofp(w=w |D).
Underthelinearizedmodel,theapproximateGaussianposteriorovertheweightsinducesatractable
posterior predictive over the output of the neural network [9, 17]. More precisely, using closure
propertiesofGaussiandistributionsunderaffinemaps,onecanshowthatthepushforwardofthe
LLAposteriorthroughw (cid:55)→flin(x,w)definesa(d′-output)Gaussianprocess
w⋆
(cid:16) (cid:17)
f |D ∼GP f(·,w⋆),(x ,x )(cid:55)→ D f(x ,w)| P† D f(x ,w)|⊤ .
1 2 w 1 w=w⋆ w 2 w=w⋆
3 NOLA:TheneuraloperatorLaplaceapproximation
Inthissection,wedeveloptheneuraloperatorLaplaceapproximation(NOLA).NOLAconstructs
(approximate)BayesianneuraloperatorsbyapplyinglinearizedLaplaceapproximationtoneural
operatorsposttraining. Itcanbeappliedtoexistingtrainedmodelsasapost-processingstepanddoes
notrequireexpensiveretraining. Furthermore,NOLAemploystheframeworkoffunction-valued
Gaussianprocessestoquantifyuncertaintyintheoutputofneuraloperators. Tothatend,wefirst
develop the concept of a function-valued Gaussian process and draw an important parallel with
curryinginfunctionalprogramming. Figure1illustratesthemainstepscomprisingourmethodology.
3.1 Function-valuedGaussianprocessesandGaussianCurrying
AsseeninSection2.3,givenaGaussianbeliefovertheparametersofaneuralnetworkf: Rd×Rp →
Rd′
,modellinearizationyieldsa(multi-output)Gaussianprocessbeliefoverthefunctionlearned
bytheneuralnetwork. However,thisisnotimmediatelyapplicabletoneuraloperators,sincetheir
outputsdonotlieinRd′
,butinapotentiallyinfinite-dimensionalBanachspaceoffunctions. Hence,
weneedtogeneralize(multi-output)GaussianprocessestothenotionofaBanach-valuedGaussian
process.
4a∈A w∈W a∈A x∈D
U
w∈W
  ×  
    Currying u(x)=f((a,x),w)    
u=F(a,w)    
  =F(a,w)(x)  
   
. .
u∈U . . u(x)∈Rd′ U . .
Step0 Step1
LinearizedLaplace
NOLA
Approximation
a∈A w∈W a∈A x∈D
U
w∈W
  ×  
u=F(a)=f(a, ·)   Gaussian u(x)=f(a,x)  
  Currying  
   
=flin((a, ·),w)   =flin((a,x),w)  
µ  .  µ  . 
. .
u∈U . u(x)∈Rd′
U
.
Step3 Step2
Figure 1: Graphical illustration of the steps involved in computing a neural operator Laplace
approximation(NOLA).AtrainedneuraloperatorF (topleft)isconvertedintoanequivalentneural
networkf withoutputsinRd′ U using(reverse)currying(topright). Itisthenpossibletoapplythe
linearizedLaplaceapproximationtof,whichproducesaGaussianprocessposteriorf quantifying
theuncertaintyaboutthefunctionlearnedbyf (bottomright). Finally,wecanuseGaussiancurrying
totransformf intoafunction-valuedGaussianprocessposteriorFovertheoperatorlearnedbythe
neuraloperatorF (bottomleft).
Definition 3.1. Let U be a real separable Banach space. A U-valued Gaussian process with
index set A on a probability space (Ω,A,P) is a function F: A × Ω → U such that ω (cid:55)→
(F(a ,ω),...,F(a ,ω)) is a joint3, i.e. (Un,B(U)⊗n)-valued, Gaussian random variable for all
1 n
n∈Nanda ,...,a ∈A.
1 n
As above, we use the shorthand F(a) := F(a,·). Moreover, if we equip the vector space UA of
(linearandnon-linear)operatorsA→Uwiththeproducttopologyτ,thenthemapω (cid:55)→F(·,ω)isa
Gaussianrandomvariablewithvaluesin(UA,B(τ)). ThiswarrantstheinterpretationofU-valued
GaussianprocessesasGaussianrandomoperators.
Inthecontextofneuraloperators,UisaBanachspaceofRd′
U-valuedfunctionsonacommondomain
D U. Inthiscase,wecanshowthatU-valuedGaussianprocessesarecloselyrelatedtomulti-output
Gaussianprocesseswithanaugmentedinputspace. ThisisinanalogytoLemma2.1,butrequires
someadditionaltechnicalassumptions.
Theorem3.1(ProofinAppendixA.2). Let(Ω,A,P)beaprobabilityspaceandUarealseparable
Banach space of Rd′-valued functions with domain D U such that all evaluation maps δ x: U →
Rd′,u (cid:55)→ u(x) are continuous. Let F: A×Ω → U and f: (A×D U)×Ω → Rd′ such that
F(a,·)(x)=f((a,x),·)foralla∈Aandx∈D U(P-almostsurely). Then
(i) f isad′-outputGaussianprocessifFisaU-valuedGaussianprocess,
and,ifAssumptionA.1holdsforL
δ
:={u(cid:55)→δ x(u) i: x∈D U∧i=1,...,d′},
(ii) FisaU-valuedGaussianprocessiff isad′-outputGaussianprocess.
Theorem 3.1 provides a valuable insight into the abstract concept of function-valued Gaussian
processes. Itestablishesthatfunction-valuedGaussianprocessesareequivalentto(multi-output)
3SeeRemarkA.1.
5Gaussianprocesseswithaugmentedinputspaces. Thisequivalenceenablesthetranslationofabstract
function-valuedobjectsintocomputationallyfeasiblestructures(real-valuedGaussianprocesses).
GaussianCurrying Asausefulintuition,wenotethatTheorem3.1constitutesaprobabilistic
analogue of the concept of currying from functional programming (and category theory more
generally). TheTheoremshowstheequivalenceofthe(vector-valued)Gaussianrandomfunction
f: A×D U →Rd′ andtheGaussianrandomoperatorF: A→(D U →Rd′)withF(a)(x)a=.s.f(a,x).
Example3.1(CurryingaContinuousBivariateGaussianProcess). Letf ∼GP(m,k)beabivariate
2-outputGaussianprocesswithindexsetR2on(Ω,A,P)with(P-almostsurely)continuouspaths.
Forinstance,thisassumptionisfulfilledifmiscontinuousandkisamultivariateMatérncovariance
function[18]. Thena(cid:55)→f(a,·)isafunction-valuedGaussianprocess. Moreprecisely,Theorem3.1
showsthatthemapF: R×Ω → C(R),(a,ω) (cid:55)→ (x (cid:55)→ f((a,x),ω))isaC(R)-valuedGaussian
process.
Thus,anintuitivewaytounderstandfunction-valuedGaussianprocessesisasobjectsthat,when
evaluated,returnaGaussianprocess. Curryingcanalsobeusedtorelatethemeanandcovariance
functionsoffunction-valued(ormoregenerallyBanach-valued)Gaussianprocesses,andtheircoun-
terpartsdefinedonthecorrespondingmulti-outputGaussianprocess. Therathertechnicaldefinitions
ofmeanandcovariancefunctionsofBanach-valuedGaussianprocessesareinAppendixA.2.
3.2 Linearizationturnsneuraloperatorsintofunction-valuedGaussianprocesses
Havingintroducedfunction-valuedGaussianprocessesintheprevioussection,wecannowusethe
notiontobuildNOLA,i.e.construct(approximate)Bayesianneuraloperators. Wedelineatethekey
componentsofourmethodologyintodifferentsteps,whicharevisuallyrepresentedinFigure1.
Step0 LetF: A×W→U⊂(Rd′ U)D U beaneuraloperatorasinSection2.1withW=Rp.
Step1 UsingcurryingonF,wedefinethefunction
f: (A×D U)×W→Rd′ U,((a,x),w)(cid:55)→F(a,w)(x).
Step2 AssumethatwehaveaposteriorGaussianbeliefw∼N (µ,Σ)overtheparametersofthe
network. Inthiswork,wewillobtaintheposterioroverwviaLaplaceapproximation,butitcould
alsohaveoriginatedfromother(approximate)inferenceschemessuchasvariationalinferenceor
(approximate)momentmatching. Sincef hasvaluesinRd′ U,wecan,asinSection2.3,linearizethe
modelaroundµ:
f((a,x),w)≈flin((a,x),w):=f((a,x),µ)+ D f((a,x),w)| (w−µ)
µ w w=µ
toarriveataninducedapproximated′ U-outputGaussianprocessbeliefwithindexsetA×D
U
(cid:16) (cid:17)
f ∼GP f(·,µ),((a ,x ),(a ,x ))(cid:55)→ D f((a ,x ),w)| ΣD f((a ,x ),w)|⊤ .
1 1 2 2 w 1 1 w=µ w 2 2 w=µ
Step 3 Finally, we can use Gaussian currying to construct a Gaussian random operator from f.
Namely,wedefinethefunction
F: A×Ω→U,(a,ω)(cid:55)→(x(cid:55)→f((a,x),ω)).
UndersometechnicalassumptionsaboutU,Theorem3.1thenshowsthatFisaU-valuedGaussian
process. For the spaces U considered in the context of neural operators, these assumptions are
virtuallyalwaysmet. Moreover,
E[F(a)(x)]=F(a,µ)(x), and
Cov[F(a )(x ),F(a )(x )]= D F(a ,w)(x )| ΣD F(a ,w)(x )|⊤ .
1 1 2 2 w 1 1 w=µ w 2 2 w=µ
63.2.1 Casestudy: BayesianFourierneuraloperators
Theaboveexpositionappliesgenerallytooperator-valuedmodels. ApplyingitspecificallytoFourier
neuraloperatorsleadstoaparticularlyefficientrepresentationofthefunction-valuedposteriorprocess.
Forsimplicityofexposition,welimitourselvestoaso-calledlast-layerLaplaceapproximation,in
whichonlytheparametersw :=(R(L−1),W(L−1))aretreatedprobabilistically[19]. However,
L−1
wewouldliketopointoutthatitispossibletoproceedwithafullLaplaceapproximationoverall
parametersw. Foraninputa∈A,wecanfactorizetheFNOas
(cid:16) (cid:17)
F(a,w)(x)=(q(·,w )◦σ(L−1)) z(L−1)(x,w ) ,
q L−1
(cid:124) (cid:123)(cid:122) (cid:125)
=:q˜
with
d′ d′
z(L−1)(x,w
):=(cid:88)v F−1(cid:16) R(L−1)⊙vˆ(L−1)(cid:17) (x)+(cid:88)v
W(L−1)v(L−1)(x)
i L−1 :ij :j ij j
j=1 j=1
=(cid:88)d′ v (cid:88)k max
Re(R(L−1))Re(vˆ(L−1))cos(⟨ω ,x⟩)
kij kj k
j=1k=1 (cid:124) (cid:123)(cid:122) (cid:125)
=:ϕkj(x)
+(cid:88)d′ v (cid:88)k max
Im(R(L−1))(−1)Im(vˆ(L−1))sin(⟨ω ,x⟩)
kij kj k
j=1k=1 (cid:124) (cid:123)(cid:122) (cid:125)
=:φkj(x)
d′
+(cid:88)v
W(L−1)v(L−1)(x),
ij j
j=1 (cid:124) (cid:123)(cid:122) (cid:125)
=:ψj(x)
wherevˆ(L−1) :=F(v(L−1)) ∈Cfork ∈{1,...,k }. Wenotethatz(L−1)(x,w )islinear
kj j k max L−1
in w . Thus, assuming a Gaussian belief over w ∼ = (Re(R(L−1)),Im(R(L−1)),W(L−1))
L−1 L−1
inducesa(multi-output)Gaussianprocessbeliefz(L−1) ∼GP(m ,K )withm =
z(L−1) z(L−1) z(L−1)
z(L−1)(x,w⋆ )overz(L−1). Moreover,z(L−1)isthesumofthree(dependent)parametricGaus-
L−1
sianprocesseswithfeaturefunctionsϕ , φ , andψ , respectively. Consequently, thefunction-
kj kj j
valuedGPinducedbythelinearizedFNOisgivenby
F(a)(x)=q˜(m (x))+Dq˜(m (x))(z(L−1)(x)−m (x)),
z(L−1) z(L−1) z(L−1)
i.e. F(a)∼GP(m ,K )with
a a
m (x)=F(a,w⋆)(x), and
a
K (x ,x )=Dq˜(m (x ))K (x ,x )Dq˜(m (x ))⊤.
a 1 2 z(L−1) 1 z(L−1) 1 2 z(L−1) 2
If the input function a ∈ A is discretized on a grid X A(i) ∈ (D A)n( Ai) , we set vˆ k(L j−1) :=
rfft(v(L−1)(X(i))) ∈Candψ (x)interpolatesv(L−1)(X(i))(e.g. splineinterpolationorFourier
j A k j j A
interpolation).
There are two practical benefits arising from this representation. First, note that computing the
momentsofanddrawingsamplesfromF(a)onlyneedsaccesstothehiddenstatev(L−1) ofthe
neuraloperator. ThismeansthatwecanevaluatetheGaussianprocessbeliefatarbitraryoutputpoints
x∈D Uwithouttheneedtocomputemorethanoneforwardpassoftheneuraloperator. Secondly,
duetothefactthattheGaussianprocessbeliefF(a)overtheoutputfunctionisparametric,wecan
efficientlysampleentirefunctionsfromitthatcanthenbelazilyevaluatedatarbitrarypoints. This
isincontrasttogeneralnon-parametricGaussianprocesses,whereonetypicallydiscretizestheGP
beforedrawingsamplesofthefunctionvaluesatthegivenfinitesetofpoints. Suchlazyfunctional
samplescanbeusede.g. foractiveexperimentaldesignandBayesianoptimization[20].
3.3 Implementation
In the following, we focus on the linearized last-layer Laplace approximation for extracting a
tractableGaussianbeliefovertheweightsoftheFourierneuraloperator. Last-layerapproximations,
7wherethefeaturemapcorrespondingtothefirstL−1layersissettoitsMAPestimate[19,21],
allow the Laplace approximation to scale to deeper architectures and have proven effective in
boththeoreticalandpracticalapplications[15,19]. Sincethefinalstandard(MLP)decodinglayer
w appliesonlypointwiseandthereforelacksglobalcharacteristics,weconsiderthelastspectral
q
convolutionwithbiasfunctioninstead. Specifically,weusethefinalFourierlayerweightsw ∼ =
L−1
(Re(R(L−1)),Im(R(L−1)),W(L−1)).
CommonapproachesforapproximatingtheGGNoflargelayers-suchasK-FACordiagonal[15]-
leverageablock-diagonalstructurethattradescross-layercorrelationsforcomputationalefficiency.
However, since the Fourier convolution forms a collection of linear layers applied in parallel to
different Fourier modes, such block approximations would yield a belief over the weights that
isuncorrelatedacrossthesemodes, whichseemsinaccurateduetothefollowinginverseFourier
transformation. Toaddressthisissue,wefocusinsteadonfindingalow-rankapproximationVVT of
theinverseGGNviaLanczositerations(similartoatruncatedSVD)[22]. Theresultinglow-rank
plusscaleddiagonalapproximationoftheposteriorcovarianceyieldsanefficientlyrepresentedobject
suitableformatrix-freeimplementations,allowingforexampleforcomputationallycheapinversion
viatheWoodburymatrixidentity.
TheGaussianbeliefofwishencegivenby
w ∼N(w∗,(n·G+τI)−1),
where N is the number of input-output pairs following the Bayesian perspective. For a given
discretization,thiscovariancecanbepushedforwardontotheoutputviaefficientlyimplemented
Jacobian-vectorproductsfollowingStep2inSection3.2.
4 Relatedwork
Theliteratureonneuraloperatorshasbeenextensivelyreviewed,withAzizzadeneshelietal.[23]
providingacomprehensiveoverviewofvariousarchitectures. Theseincludegraphneuraloperators
[11],physics-informedneuraloperators[24],multi-waveletneuraloperators[25]andthewidelyused
Fourierneuraloperators[12]. Lanthaleretal.[26]furthercontributestothisfieldbyquantifyingthe
aliasingerrorresultingfromdiscretizationofFNOsandobtainingalgebraicratesofconvergencein
relationtogridresolution.
Despitetheadvancementsinneuraloperatorarchitectures,uncertaintyestimationremainsanun-
derexploredarea. SomeprogresshasbeenmadebyMagnanietal.[27], Kumaretal.[28], Garg
and Chakraborty [29]. Garg and Chakraborty [29] employs variational inference to estimate the
BayesianposterioroverDeepONetpredictions. Theworkin[27,28]ismorecloselyrelatedtothe
presentwork. Magnanietal.[27]useLaplaceapproximationtoprovideuncertaintyestimatesfor
graphneuraloperators,butdonotextendthistoFourierneuraloperatorsorconsiderafunctionspace
approach. Kumaretal.[28]incorporatesaGaussianProcesspriorwithameanfunctionderived
fromaWaveletNeuralOperator, optimizingmodelhyperparametersbyminimizingthenegative
log-marginallikelihood. OtherBayesianoperatorframeworkshavebeenconsideredinZouetal.[30]
andGargandChakraborty[31]. Operator-valuedkernelsandfunction-valuedGaussianprocesses
havebeenstudiedintheHilbertspacesetting,e.g. byMicchelliandPontil[32],Kadrietal.[33],and
Owhadi[34]. Ourapproach,however,formulatesthetheorywithinthecontextofBanachspaces,
asneural operatorsaredefined asmappings betweensuchspaces. Otherrelevant methodsusing
Gaussianprocessestosolvepartialdifferentialequations(PDEs)includetheworksofChenetal.
[35],Batlleetal.[36],andChenetal.[37].
Laplaceapproximation,introducedtodeeplearningbyMackay[38],hasgainedpopularityinthe
Bayesiandeeplearningcommunity[8,15,19,39]. Thisisalsoduetoitsscalability,achievedthrough
various strategies including using log-posterior Hessian approximations [39, 40], treating only a
subsetofthemodelprobabilistically[41],employinglinearizedLaplace[9,42],orusingscalable
Gaussianprocessesmethods[43,44]. OtherBayesiandeeplearningmethodsincludevariational
inference[45–48],MarkovChainMonteCarlo[49–51],orheuristicmethods[52,53].
8NOLA Inputperturbations Weightperturbations
2
mean 2
2 samples
target
0 0 0
Figure2: ComparingNOLA,inputperturbations,andweightperturbationsatasinglepredictedtime
pointofasampletrajectoryoftheKdVequation. ThecorrespondingFNOhasbeentrainedon32
trainingtrajectories.
5 Experiments
We evaluate our approach by quantifying the uncertainty of Fourier neural operator predictions,
that were trained on a limited amount of training samples. Following the experimental setup in
Brandstetteretal.[54],wetrainmultipleFourierneuraloperatorsonvaryingnumbersoftraining
trajectories (i.e. 32, 64, and 128 training samples, each having 256 spatial and 140 temporal
points)fortheKorteweg-deVries(KdV,∂ u+u∂ u+∂3u=0),theKuramoto-Sivashinsky(KS,
t x x
∂ u+∂2u+∂4u+u∂ u=0)andBurgers’(∂ u+u∂ u−ν∂2u=0whereν >0)equations. Each
t x x x t x x
neuraloperatoristrainedtopredicttwentyfuturetimestepsbasedonthetwentypreviousones. All
modelsconsistoffourFourierconvolutionallayerswith16Fouriermodesand32hiddenchannels.
Usingtheabove-outlinedLaplaceapproximation,wemodel33,824parameters(morethanone-fifth
ofallparameters)probabilistically.
WeevaluateNOLAoneachtrainedmodelagainstinputperturbationsandweightperturbationsasbase-
lineapproaches,whichcanbeseenasensembleforecast:theformerisgivenbyF w(a(x)+ϵ( xk))
k∈K
withϵ( xk) ∼ N(0,τ−2I |x|), andthelatterby(F wL−1+ϵ(k)(a(x))
k
withϵ(k) ∼ N(0,τ−2I |wL−1|).
ForNOLA,wecanestimatethestandarddeviationbythesquarerootofthepushed-forwardoutput
variance,whileforbothperturbation-basedmethods,wetaketheempiricalstandarddeviationof|K|
ensemblemembers,setting|K|=100.
Toquantifyandcomparetheuncertainty,weconsiderthenegativelog-likelihood(NLL),theroot
squaredmeanerror(RMSE),andtheq-statistic(Q)definedasfollows:
(cid:115)
1 (cid:88)
RMSE= |F(a )(x)−u (x)|2,
c i i
x∈x,i≤n
Q=
1 (cid:88) |F(a i)(x)−u i(x)|2
, and
c V[(a )(x)]
i
x∈x,i≤n
NLL=
1 (cid:88)
(cid:20)
log(2πV[F(a )(x)])+
|F(a i)(x)−u
i(x)|2(cid:21)
,
2c i V[(a )(x)]
i
x∈x,i≤n
where c is the number of summands and n is the number of input-output pairs considered for
evaluation. TheRMSEmeasurestheaccuracyoftheGP/ensemblemeancomparedtothetruetarget
andshouldbeclosetozero. TheQestimateindicatesposteriorcalibrationandshouldbeclosetoone.
AlowerNLLreflectsabalancebetweenlowstandarddeviationsanderrorcalibration. ForNOLAwe
computetheGGNon128input-outputpairsofthetrainingset. Thehyperparameterτ iscalibrated
foreachmethodonthevalidationsetandthemetricsarecomparedon100samplesofthetestset,as
showninTable1. AsinglesamplepredictionforatrajectoryoftheKdVequationcanbefoundin
Figure2. WhileNOLAoutperformsbothbaselinesinthemetricssometimesonlybyamargin,we
notethattheoverallpredictionerrorintheexperimentisalreadylowanditisthereforeimportantto
testNOLAonmorediversetasks,wheremoreuncertaintycanbecaptured. ObservationsfromFigure
2suggestthatNOLA’ssamplepathsalignmorecloselywiththetruetargetfunctionthanthosefrom
thebaselines.
9Table1: Evaluationmetricson100testsamples.
KdV KS Burgers
NLL RMSE Q NLL RMSE Q NLL RMSE Q
32trainingsamples
Inputperturbations 0.795 0.162 3.32 -0.390 0.152 1.636 0.596 0.331 2.397
Weightperturbations -0.306 0.169 1.929 -0.539 0.148 1.712 0.909 0.34 3.208
NOLA -0.577 0.163 1.152 -0.543 0.148 1.125 0.443 0.335 0.534
64trainingsamples
Inputperturbations -0.214 0.122 1.01 -0.767 0.108 1.664 -0.417 0.125 2.075
Weightperturbations -0.691 0.121 1.422 -0.862 0.106 1.786 -0.322 0.126 2.565
NOLA -0.920 0.119 1.003 -0.872 0.106 1.273 -0.517 0.123 1.987
128trainingsamples
Inputperturbations -0.383 0.094 1.54 -1.108 0.082 1.476 -1.207 0.044 3.059
Weightperturbations -0.913 0.095 0.558 -1.121 0.081 1.908 -1.397 0.042 2.505
NOLA -1.191 0.091 1.311 -1.158 0.081 1.417 -1.719 0.042 1.055
6 Conclusion
Inthiswork,wedevelopedaprobabilisticframeworkforneuralnetwork-learnedoperatorsusing
Banach-valuedGaussianprocesses. OurapproachextendsGaussianprocesstheoryintothedomain
ofoperators,facilitatingthetreatmentofinfinite-dimensionalfunctionspaces. Bydemonstratingthe
equivalencebetweenBanach-valuedGaussianprocessesandmulti-outputGaussianprocesseswith
anaugmentedinputspace,weestablishGaussiancurrying,aprobabilisticanalogueofcurryingin
functionalprogramming.
Thiscurryingprocesstransformscomplexoperatormappingsintomoremanageableandpractically
implementableforms,effectivelymodelingrelationshipsbetweeninfinite-dimensionalspaces. By
applying linearization and Laplace approximation, neural operators can be viewed as function-
valuedGaussianprocesses,providinganalyticuncertaintyestimatesoveranoperatorandyielding
a continuous function that can be evaluated on any grid. This function can also be analytically
propagatedtodownstreamanalyses.
Inourexperiments,weshowimprovementsoverperturbation-basedmethodsintermsofcommon
metrics.Especially,samplepredictionsgeneratedwithNOLAexhibitmorerealisticbehaviour.Future
directionsforthisworkincludeinvestigatingintotheinterpolationerrorthatarisesinconstructing
thefeaturefunctionsoftheparametricGaussianprocessintheBayesianFourierneuraloperator.
Moreover, we plan to apply our approach to a wider set of PDEs, including multidimensional
problems.
AcknowledgmentsandDisclosureofFunding
TheauthorsgratefullyacknowledgefinancialsupportbytheEuropeanResearchCouncilthrough
ERCCoGAction101123955ANUBIS;theDFGClusterofExcellence“MachineLearning-New
PerspectivesforScience”,EXC2064/1,projectnumber390727645;theGermanFederalMinistryof
EducationandResearch(BMBF)throughtheTübingenAICenter(FKZ:01IS18039A);theDFGSPP
2298(ProjectHE7114/5-1),andtheCarlZeissFoundation,(project"CertificationandFoundations
ofSafeMachineLearningSystemsinHealthcare"),aswellasfundsfromtheMinistryofScience,
ResearchandArtsoftheStateofBaden-Württemberg. TheauthorsthanktheInternationalMax
PlanckResearchSchoolforIntelligentSystems(IMPRS-IS)forsupportingEmiliaMagnani,Marvin
PförtnerandTobiasWeber.
References
[1] NikolaKovachki,ZongyiLi,BurigedeLiu,KamyarAzizzadenesheli,KaushikBhattacharya,
AndrewStuart,andAnimaAnandkumar. Neuraloperator: Learningmapsbetweenfunction
spaceswithapplicationstoPDEs. JMLR,24(89):1–97,2023.
10[2] JaideepPathak,ShashankSubramanian,PeterHarrington,SanjeevRaja,AsheshChattopadhyay,
Morteza Mardani, Thorsten Kurth, David Hall, Zongyi Li, Kamyar Azizzadenesheli, et al.
Fourcastnet: Aglobaldata-drivenhigh-resolutionweathermodelusingadaptivefourierneural
operators. arXivpreprintarXiv:2202.11214,2022.
[3] Boris Bonev, Thorsten Kurth, Christian Hundt, Jaideep Pathak, Maximilian Baust, Karthik
Kashinath, and Anima Anandkumar. Spherical Fourier neural operators: Learning stable
dynamicsonthesphere. InICML,volume202ofProceedingsofMachineLearningResearch,
pages2806–2823.PMLR,2023.
[4] TJGrady,RishiKhan,MathiasLouboutin,ZiyiYin,PhilippAWitte,RanveerChandra,RussellJ
Hewett,andFelixJHerrmann. Towardslarge-scalelearnedsolversforparametricPDEswith
model-parallelfourierneuraloperators. arXivpreprintarXiv:2204.01205,2022.
[5] Peter I Renn, Cong Wang, Sahin Lale, Zongyi Li, Anima Anandkumar, and Morteza
Gharib. Forecastingsubcriticalcylinderwakeswithfourierneuraloperators. arXivpreprint
arXiv:2301.08290,2023.
[6] ZhijieLi,WenhuiPeng,ZelongYuan,andJianchunWang. Fourierneuraloperatorapproach
tolargeeddysimulationofthree-dimensionalturbulence. TheoreticalandAppliedMechanics
Letters,12(6):100389,2022.
[7] Zongyi Li, Nikola Kovachki, Chris Choy, Boyi Li, Jean Kossaifi, Shourya Otta, Moham-
mad Amin Nabian, Maximilian Stadler, Christian Hundt, Kamyar Azizzadenesheli, et al.
Geometry-informedneuraloperatorforlarge-scale3dPDEs. NeurIPS,36,2024.
[8] TheodorePapamarkou,MariaSkoularidou,KonstantinaPalla,LaurenceAitchison,JulyanArbel,
DavidDunson,MaurizioFilippone,VincentFortuin,PhilippHennig,JoséMiguelHernández-
Lobato,AliaksandrHubin,AlexanderImmer,TheofanisKaraletsos,MohammadEmtiyazKhan,
AgustinusKristiadi,YingzhenLi,StephanMandt,ChristopherNemeth,MichaelAOsborne,
TimG.J.Rudner,DavidRügamer,YeeWhyeTeh,MaxWelling,AndrewGordonWilson,and
RuqiZhang. Position: Bayesiandeeplearningisneededintheageoflarge-scaleAI. InICML,
2024.
[9] AlexanderImmer,MaciejKorzepa,andMatthiasBauer. ImprovingpredictionsofBayesian
neuralnetworksvialocallinearization. InAISTATS,2020.
[10] ZongyiLi,NikolaKovachki,KamyarAzizzadenesheli,BurigedeLiu,KaushikBhattacharya,
AndrewStuart,andAnimaAnandkumar. Neuraloperator: Graphkernelnetworkforpartial
differential equations. In ICLR 2020 Workshop on Integration of Deep Neural Models and
DifferentialEquations,2020.
[11] ZongyiLi,NikolaKovachki,KamyarAzizzadenesheli,BurigedeLiu,KaushikBhattacharya,
AndrewStuart,andAnimaAnandkumar. Multipolegraphneuraloperatorforparametricpartial
differentialequations. InNeurIPS,volume33,pages6755–6766.CurranAssociates,Inc.,2020.
[12] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik
Bhattacharya,AndrewStuart,andAnimaAnandkumar. Fourierneuraloperatorforparametric
partialdifferentialequations. InICLR,2020.
[13] DavidJCMacKay. Bayesianinterpolation. Neuralcomputation,4(3):415–447,1992.
[14] David JC MacKay. The evidence framework applied to classification networks. Neural
computation,4(5):720–736,1992.
[15] ErikDaxberger,AgustinusKristiadi,AlexanderImmer,RunaEschenhagen,MatthiasBauer,
andPhilippHennig. Laplaceredux-effortlessbayesiandeeplearning. NeurIPS,2021.
[16] NicolSchraudolph. Fastcurvaturematrix-vectorproductsforsecond-ordergradientdescent.
Neuralcomputation,14:1723–38,2002.
[17] MohammadEmtiyazKhan,AlexanderImmer,EhsanAbedi,andMaciejKorzepa. Approximate
inferenceturnsdeepnetworksintoGaussianprocesses. InNeurIPS,2019.
[18] Nathaël Da Costa, Marvin Pförtner, Lancelot Da Costa, and Philipp Hennig. Sample path
regularityofGaussianprocessesfromthecovariancekernel. arXivpreprintarXiv:2312.14886,
2023. doi:10.48550/arXiv.2312.14886.
[19] AgustinusKristiadi,MatthiasHein,andPhilippHennig. Beingbayesian,evenjustabit,fixes
overconfidenceinrelunetworks. InICML,pages5436–5446.PMLR,2020.
11[20] JamesT.Wilson,ViacheslavBorovitskiy,AlexanderTerenin,PeterMostowsky,andMarcPeter
Deisenroth. Pathwise conditioning of Gaussian processes. Journal of Machine Learning
Research,22(105):1–47,2021.
[21] JasperSnoek,OrenRippel,KevinSwersky,RyanKiros,NadathurSatish,NarayananSundaram,
MostofaPatwary,MrPrabhat,andRyanAdams. Scalablebayesianoptimizationusingdeep
neuralnetworks. InICML,2015.
[22] FelixDangel,LukasTatzel,andPhilippHennig.Vivit:Curvatureaccessthroughthegeneralized
gauss-newton’slow-rankstructure. TransactionsonMachineLearningResearch,2023. ISSN
2835-8856. URLhttps://openreview.net/forum?id=DzJ7JfPXkE.
[23] KamyarAzizzadenesheli,NikolaKovachki,ZongyiLi,MiguelLiu-Schiaffini,JeanKossaifi,
andAnimaAnandkumar. Neuraloperatorsforacceleratingscientificsimulationsanddesign.
NatureReviewsPhysics,pages1–9,2024.
[24] ZongyiLi,HongkaiZheng,NikolaKovachki,DavidJin,HaoxuanChen,BurigedeLiu,Kamyar
Azizzadenesheli,andAnimaAnandkumar.Physics-informedneuraloperatorforlearningpartial
differentialequations. arXivpreprintarXiv:2111.03794,2021.
[25] Gaurav Gupta, Xiongye Xiao, and Paul Bogdan. Multiwavelet-based operator learning for
differentialequations. InNeurIPS,volume34,2021.
[26] SamuelLanthaler,AndrewMStuart,andMargaretTrautner. Discretizationerroroffourier
neuraloperators. arXivpreprintarXiv:2405.02221,2024.
[27] EmiliaMagnani,NicholasKrämer,RunaEschenhagen,LorenzoRosasco,andPhilippHennig.
Approximatebayesianneuraloperators: UncertaintyquantificationforparametricPDEs,2022.
[28] Sawan Kumar, Rajdip Nayek, and Souvik Chakraborty. Neural operator induced gaussian
processframeworkforprobabilisticsolutionofparametricpartialdifferentialequations. arXiv
preprintarXiv:2404.15618,2024.
[29] ShaileshGargandSouvikChakraborty. Vb-deeponet: Abayesianoperatorlearningframework
foruncertaintyquantification. EngineeringApplicationsofArtificialIntelligence,118:105685,
2023. ISSN0952-1976.
[30] Zongren Zou, Xuhui Meng, Apostolos F Psaros, and George Em Karniadakis. Neuraluq:
A comprehensive library for uncertainty quantification in neural differential equations and
operators. arXivpreprintarXiv:2208.11866,2022.
[31] ShaileshGargandSouvikChakraborty. Variationalbayesdeepoperatornetwork: adata-driven
bayesiansolverforparametricdifferentialequations. arXivpreprintarXiv:2206.05655,2022.
[32] CharlesAMicchelliandMassimilianoPontil. Onlearningvector-valuedfunctions. Neural
computation,17(1):177–204,2005.
[33] HachemKadri,EmmanuelDuflos,PhilippePreux,StéphaneCanu,AlainRakotomamonjy,and
JulienAudiffren. Operator-valuedkernelsforlearningfromfunctionalresponsedata. JMLR,17
(20):1–54,2016.
[34] HoumanOwhadi. Doideashaveshape? idearegistrationasthecontinuouslimitofartificial
neuralnetworks. PhysicaD:NonlinearPhenomena,444,2023.
[35] YifanChen,BamdadHosseini,HoumanOwhadi,andAndrewMStuart. Solvingandlearning
nonlinearPDEswithgaussianprocesses. JournalofComputationalPhysics,447:110668,2021.
[36] Pau Batlle, Matthieu Darcy, Bamdad Hosseini, and Houman Owhadi. Kernel methods are
competitiveforoperatorlearning. JournalofComputationalPhysics,496:112549,2024.
[37] YifanChen,HoumanOwhadi,andFlorianSchäfer. Sparsecholeskyfactorizationforsolving
nonlinearpdesviagaussianprocesses. arXivpreprintarXiv:2304.01294,2023.
[38] DavidJohnCameronMackay. Bayesianmethodsforadaptivemodels. CaliforniaInstituteof
Technology,1992.
[39] HippolytRitter,AleksandarBotev,andDavidBarber. Ascalablelaplaceapproximationfor
neuralnetworks. InICLR,2018.
[40] JamesMartens. Newinsightsandperspectivesonthenaturalgradientmethod. JMLR,21(146):
1–76,2020.
12[41] Erik Daxberger, Eric Nalisnick, James U Allingham, Javier Antoran, and Jose Miguel
Hernandez-Lobato. Bayesian deep learning via subnetwork inference. In ICML, volume
139ofPMLR,2021.
[42] Andrew YK Foong, Yingzhen Li, José Miguel Hernández-Lobato, and Richard E Turner.
’In-Between’uncertaintyinbayesianneuralnetworks. arXivpreprintarXiv:1906.11537,2019.
[43] Zhijie Deng, Feng Zhou, and Jun Zhu. Accelerated linearized laplace approximation for
bayesiandeeplearning. NeurIPS,2022.
[44] LuisAOrtega,SimónRodríguezSantana,andDanielHernández-Lobato. Variationallinearized
laplaceapproximationforbayesiandeeplearning. arXivpreprintarXiv:2302.12565,2023.
[45] AlexGraves. Practicalvariationalinferenceforneuralnetworks. InNeurIPS,2011.
[46] CharlesBlundell,JulienCornebise,KorayKavukcuoglu,andDaanWierstra.Weightuncertainty
inneuralnetwork. InICML,2015.
[47] MohammadEmtiyazKhan,DidrikNielsen,VootTangkaratt,WuLin,YarinGal,andAkash
Srivastava. FastandscalableBayesiandeeplearningbyweight-perturbationinadam. InICML,
2018.
[48] Guodong Zhang, Shengyang Sun, David Duvenaud, and Roger B. Grosse. Noisy natural
gradientasvariationalinference. InICML,2018.
[49] RadfordM.Neal. BayesianLearningforNeuralNetworks. Springer-Verlag,Berlin,Heidelberg,
1996. ISBN0387947248.
[50] MaxWellingandYeeWhyeTeh. BayesianlearningviastochasticgradientLangevindynamics.
InICML,2011.
[51] RuqiZhang,ChunyuanLi,JianyiZhang,ChangyouChen,andAndrewGordonWilson.Cyclical
stochasticgradientMCMCforBayesiandeeplearning. InICLR,2020.
[52] YarinGalandZoubinGhahramani. DropoutasaBayesianapproximation: Representingmodel
uncertaintyindeeplearning. InICML,2016.
[53] WesleyMaddox,T.Garipov,PavelIzmailov,DmitryP.Vetrov,andAndrewGordonWilson. A
simplebaselineforBayesianuncertaintyindeeplearning. InNeurIPS,2019.
[54] JohannesBrandstetter,MaxWelling,andDanielEWorrall. Liepointsymmetrydataaugmenta-
tionforneuralpdesolvers. arXivpreprintarXiv:2202.07643,2022.
[55] AchimKlenke. ProbabilityTheory: AComprehensiveCourse. Universitext.Springer,London,
secondedition,2014.
[56] Marvin Pförtner, Ingo Steinwart, Philipp Hennig, and Jonathan Wenger. Physics-informed
gaussianprocessregressiongeneralizeslinearpdesolvers. arXivpreprintarXiv:2212.12474,
2022.
13Appendix
A TheoreticalResults
A.1 GaussianmeasuresonseparableBanachspaces
Weaimtoquantifyepistemicuncertaintyaboutoperatorsbetween(infinite-dimensional)Banach
spacesoffunctionsusingaGaussianprocessframework. TobeabletodefineaGaussianprocess
beliefoversuchoperators,wehenceneedanotionofaGaussianrandomvariableswithvaluesin
(separable)Banachspaces.
DefinitionA.1(GaussianMeasureonseparableBanachspace). LetUbearealseparableBanach
space. Aprobabilitymeasureγon(U,B(U))iscalledGaussianifeverycontinuouslinearfunctional
ℓ∈U′ isaunivariateGaussianrandomvariableon(U,B(U),γ). AU-valuedrandomvariableis
calledGaussianifitslawisGaussian.
RemarkA.1(JointlyGaussianMeasureonseparableBanachspaces). Inthiswork,wefrequently
needtoconstructjointGaussianmeasuresonmultipleseparableBanachspaces. Fortunately,we
can also leverage Definition A.1 for this. More formally, we aim to define a Gaussian measure
ontheiteratedCartesianproduct Un ofarealseparableBanachspace U. WeequipUn withthe
productsigmaalgebraB(U)⊗n . SinceUisPolish,wehaveB(U)⊗n =B(τ),whereτ istheproduct
topologyonUn [55,Theorem14.8]. Moreover,(Un,τ)isBanachable,i.e. thereisanorm∥·∥Un
thatinducesτ suchthat(Un,∥·∥Un)iscomplete. Hence,asinDefinitionA.1,wecallaprobability
measureγ on(Un,B(U)⊗n)Gaussianifeveryℓ∈U′isaunivariateGaussianrandomvariableon
(Un,B(U)⊗n,γ).
Similartotheirfinite-dimensionalcounterparts,GaussianmeasureswithvaluesinseparableBanach
spacesadmitthedefinitionofameananda(cross-)covarianceoperator.
PropositionA.1(MeanandCovarianceOperator[seee.g.56,PropositionB.2]). LetγbeaGaussian
measureonarealseparableBanachspaceU. Thereisauniquem ∈Uwithℓ(m )=E [ℓ(u)]
γ γ u∼γ
foreverycontinuouslinearfunctionalℓ∈U′,referredtoasthemeanofγ.Similarly,thereisaunique
boundedlinearoperatorC : U′ →Uwithℓ (C (ℓ ))=Cov [ℓ (u),ℓ (u)]foranyℓ ,ℓ ∈U′,
γ 1 γ 2 u∼γ 1 2 1 2
theso-calledcovarianceoperatorofγ. ThemeanandcovarianceoperatorofaGaussianrandom
variablewithvaluesinUisdefinedaccordingly.
CorollaryA.2(Cross-CovarianceOperator). Letu ,u bejointlyGaussianrandomvariableson
1 2
(Ω,A,P) with values in real separable Banach spaces U ,U , respectively. There is a unique
1 2
bounded linear operator C : U′ → U with ℓ (C (ℓ )) = Cov [ℓ (u ),ℓ (u )] for all
ℓ ∈U′ andℓ ∈U′. Theou p1 e,u ra2 torC1 is2 referred1 tou a1 s,u t2 hec2 ross-covariP ance1 op1 erat2 orb2 etweenu
1 1 2 2 u1,u2 1
andu .
2
A.2 Banach-valuedGaussianprocesses
NowwehaveallthenecessarypreliminariestodefineaGaussianrandomprocessthattakesvaluesin
realseparableBanachspaces.
Definition 3.1. Let U be a real separable Banach space. A U-valued Gaussian process with
index set A on a probability space (Ω,A,P) is a function F: A × Ω → U such that ω (cid:55)→
(F(a ,ω),...,F(a ,ω)) is a joint4, i.e. (Un,B(U)⊗n)-valued, Gaussian random variable for all
1 n
n∈Nanda ,...,a ∈A.
1 n
As for (multi-output) Gaussian processes, we can also define mean and covariance functions for
Banach-valuedGaussianprocesses. However,theirdefinitionismoretechnicallyinvolved.
DefinitionA.2. LetFbeaU-valuedGaussianprocesswithindexsetAon(Ω,A,P). Thefunction
M: A→U,a(cid:55)→m
F(a,·)
iscalledthemeanfunctionofFandthefunction
K: A×A→(U′ →U),(a ,a )(cid:55)→C
1 2 F(a1,·),F(a2,·)
isreferredtoasthecovariancefunctionofF.
4SeeRemarkA.1.
14Inthefollowing,weaimtoestablishacorrespondencebetweenBanach-valuedGaussianprocesses
and(R-valued)Gaussianprocesses. UnlikeinLemma2.1,weneedadditionaltechnicalassumptions
forthistoworkbothways. Denotebyscl w∗(L):={ℓ∈U′|∃{ℓ i} i∈N ⊂L: ℓ i → w∗ ℓ}theweak-*
sequentialclosureofasetL⊂U′.
AssumptionA.1. LetUbearealseparableBanachspaceandL ⊂ U′ asetofcontinuouslinear
functionalsonUsuchthatthereisann ∈N withsclnscl(spanL)=U′.
scl 0 w∗
TheoremA.3. Let(Ω,A,P)beaprobabilityspace,UarealseparableBanachspace,andL⊂U′.
LetF: A×Ω → Uandf: (A×L)×Ω → Rsuchthatℓ(F(a,·)) = f((a,ℓ),·)foralla ∈ Aand
ℓ∈L(P-almostsurely). Then
(i) F∼GP(M,K)impliesf ∼GP(m,k),
and,ifAssumptionA.1holds,
(ii) f ∼GP(m,k)impliesF∼GP(M,K),
where,
(iii) inbothcases,
ℓ(M(a))=m(a,ℓ)
foralla∈Aandℓ∈L,aswellas
ℓ (K(a ,a )(ℓ ))=k((a ,ℓ ),(a ,ℓ ))
1 1 2 2 1 1 2 2
foralla ,a ∈Aandℓ ,ℓ ∈U.
1 2 1 2
Proof. Firstofall,notethatℓ∈(Un)′ifandonlyifthereareℓ ,...,ℓ ∈U′suchthat
1 n
n
(cid:88)
ℓ(u ,...,u )= ℓ (u ),
1 n i i
i=1
whereweequipUnwiththeproducttopology.
(i) Leta ,...,a ∈Aandℓ ,...,ℓ ∈U′. Bytheabove,thelinearfunctionals
1 n 1 n
ℓ˜: Un →R,(u ,...,u )(cid:55)→ℓ (u )
i 1 n i i
arecontinuousw.r.t. theproducttopologyonUn. Moreover,ω (cid:55)→(F(a ,ω),...,F(a ,ω))
1 n
isGaussianbyassumption. Hence,
(cid:16) (cid:17)n
ω (cid:55)→ ℓ˜(F(a ,ω),...,F(a ,ω))
i 1 n
i=1
=(ℓ (F(a ,ω)))n
i i i=1
=(f((a ,ℓ ),ω))n
i i i=1
isGaussianwithvaluesinRn.
(ii) Leta ,...,a ∈ A. Wehavetoshowthatω (cid:55)→ (F(a ,ω),...,F(a ,ω))isaGaussian
1 n 1 n
random variable with values in Un, i.e. that for every ℓ ∈ (Un)′, the random variable
ω (cid:55)→ℓ(F(a ,ω),...,F(a ,ω))isGaussianwithvaluesinR. Bytheabove,weknowthat
1 n
thereareℓ ,...,ℓ ∈U′suchthat
1 n
m m
(cid:88) (cid:88)
ω (cid:55)→ℓ(F(a ,ω),...,F(a ,ω))= ℓ (F(a ,ω))= f((a ,ℓ ),ω)
1 n i i i i
i=1 i=1
Claim. Let m ∈ N and ℓ ,...,ℓ ∈ sclm (spanL). Then ω (cid:55)→ (cid:80)n f((a ,ℓ ),ω) is
0 1 n w∗ i=1 i i
Gaussian.
ProofofClaim. ThisisastraightforwardmodificationofTheoremB.6from[56].
ByAssumptionA.1,thereism=n suchthatsclm (spanL)=U′. Hence,thestatement
scl w∗
followsfromtheclaim.
15(iii) UsingthefactthatBochnerintegralsandboundedlinearoperatorscommute,weobtain
(cid:18)(cid:90) (cid:19)
ℓ(M(a))=ℓ F(a,ω)P(dω)
Ω
(cid:90)
= ℓ(F(a,ω))P(dω)
Ω(cid:124) (cid:123)(cid:122) (cid:125)
a=.s.f((a,ℓ),ω)
=E [f(a,ℓ)]
P
=m(a,ℓ)
and
(cid:18)(cid:90) (cid:19)
ℓ (K(a ,a )(ℓ ))=ℓ ℓ (F(a ,ω)−M(a ))(F(a ,ω)−M(a ))P(dω)
1 1 2 2 1 2 2 2 1 1
Ω
(cid:90)
= ℓ (F(a ,ω)−M(a ))ℓ (F(a ,ω)−M(a ))P(dω)
2 2 2 1 1 1
Ω
(cid:90)
= (ℓ (F(a ,ω))−ℓ (M(a )))(ℓ (F(a ,ω))−ℓ (M(a )))P(dω)
1 1 1 1 2 2 2 2
Ω (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
a=.s.f((a1,ℓ1),ω) =m(a1,ℓ1) a=.s.f((a2,ℓ2),ω) =m(a2,ℓ2)
=Cov [f(a ,ℓ ),f(a ,ℓ )]
P 1 1 2 2
=k((a ,ℓ ),(a ,ℓ )).
1 1 2 2
CorollaryA.4. Let(Ω,A,P)beaprobabilityspaceandUarealseparableBanachspaceofreal-
valuedfunctionsonacommondomainD U withcontinuouspointevaluationfunctionalsδ x: U→
R,u(cid:55)→u(x). LetF: A×Ω→Uandf: (A×D U)×Ω→RsuchthatF(a,·)(x)=f((a,x),·)for
alla∈Aandx∈D U(P-almostsurely). Then
(i) F∼GP(M,K)impliesf ∼GP(m,k),
and,ifAssumptionA.1holds5forL
δ
:={δ x: x∈D U},
(ii) f ∼GP(m,k)impliesF∼GP(M,K),
where,
(iii) inbothcases,
M(a)(x)=m(a,x)
foralla∈Aandx∈D U,aswellas
K(a ,a )(ℓ )(x )=ℓ (x (cid:55)→k((a ,x ),(a ,x )))
1 2 2 1 2 2 1 1 2 2
foralla 1,a
2
∈A,ℓ
2
∈U,andx
1
∈D U.
Finally,TheoremA.3fromthemaintextisacorollaryoftheresultsdevelopedabove.
ProofofTheorem3.1. FollowsfromCorollaryA.4andLemma2.1.
5Forinstance,thisisthecaseifUisaseparableRKHS,U=C(D U)andD Uisacompactmetricspace,or
U=Ck(D U)andD
U
⊂Rd,whichfollowsfromPropositionsB.6,B.7,andB.10in[56].
16