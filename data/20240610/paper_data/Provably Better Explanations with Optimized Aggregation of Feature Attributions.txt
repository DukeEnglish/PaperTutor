Provably Better Explanations with Optimized Aggregation of
Feature Attributions
ThomasDecker12 AnantaR.Bhattarai23 JindongGu4 VolkerTresp15 FlorianBuettner267
Abstract instance,empiricalevidencehasrevealedthatsinglemeth-
odsoftenexhibitunreasonablesensitivitytoinputperturba-
Using feature attributions for post-hoc explana-
tions(Kindermansetal.,2019;Alvarez-Melis&Jaakkola,
tionsisacommonpracticetounderstandandver-
2018;Dombrowskietal.,2019;Ghorbanietal.,2019;Lin
ify the predictions of opaque machine learning
etal.,2023)andcriticallydependontheconcretehyperpa-
models. Despitethenumeroustechniquesavail-
rameterchoice(Bansaletal.,2020;Sturmfelsetal.,2020;
able,individualmethodsoftenproduceinconsis-
Pahde et al., 2023). This lack of explanation robustness
tentandunstableresults,puttingtheiroverallreli-
notonlycausesunstableattributionresultsbutcanevenbe
abilityintoquestion. Inthiswork,weaimtosys-
exploitedformaliciousmanipulations(Baniecki&Biecek,
tematicallyimprovethequalityoffeatureattribu-
2023). Ontopofthat,somemethodsmayfailtoidentify
tionsbycombiningmultipleexplanationsacross
relevantfeatures(Hookeretal.,2019;Zhouetal.,2022)and
distinctmethodsortheirvariations. Forthispur-
differenttechniquesfrequentlydisagreesubstantiallywhen
pose,weproposeanovelapproachtoderiveopti-
explainingthesameprediction(Krishnaetal.,2022;Neely
malconvexcombinationsoffeatureattributions
etal.,2021). Thesefindingscontesttheactualfidelityofa
thatyieldprovableimprovementsofdesiredqual-
singleattributionresultforthepurposemodelexplainability.
itycriteriasuchasrobustnessorfaithfulnesstothe
Inadditiontotheseobservations,thereisagrowingbodyof
modelbehavior. Throughextensiveexperiments
theoreticalworkthathighlightsthelimitationsofindividual
involvingvariousmodelarchitecturesandpopu-
attributionmethods(Nieetal.,2018;Sixtetal.,2020;Ku-
larfeatureattributiontechniques,wedemonstrate
maretal.,2021;Bilodeauetal.,2024;Fokkemaetal.,2023).
thatourcombinationstrategyconsistentlyoutper-
Morespecifically,in(Hanetal.,2022),theauthorsestablish
formsindividualmethodsandexistingbaselines.
a”no-freelunch”theoremformodelexplanations,which
impliesthatasingleattributionmethodcannotuniversally
approximatethebehaviorofanymodelfaithfully.
1.Introduction Nevertheless, each feature attribution method derives im-
portancebasedondifferentmechanismsandeachcanbe
Thepracticeofquantifyingtheinfluenceofindividualfea- associatedwithindividualbenefitsandshortcomings. As
turesthroughattributionmethodshasbeenestablishedasa aconsequence,thequestionarisesofhowtobestcombine
popularparadigmtoenhancethetransparencyofcomplex themtoattainbetterexplainabilityofopaquepredictions.
machinelearningmodels. Theseapproachestypicallypro- Inthiswork,weexplorethecapabilitiesofconvexcombi-
duceheatmapshighlightingindividualinputfeatures,such nations across different attribution results to improve the
aspixelsorimageregions,relevanttoaspecificmodelpre- overallreliabilityofexplanations. Guidedbyestablished
diction(seeFigure1,left). However,whileamultitudeof qualitycriteriaforfeatureattributions(Nautaetal.,2023),
techniqueshasbeendevelopedforthispurpose,concerns weproposeaneffectivestrategytoderiveconvexweight-
anddoubtsregardingthereliabilityofindividualmethods ings such that the corresponding aggregation of different
persist(Adebayoetal.,2018;2020;Zhouetal.,2022). For outcomesyieldssignificantimprovementsinrobustnessand
faithfulness. Thisisunderpinnedbyatheoreticalanalysis
1LMU Munich 2Siemens AG 3Technical University of
showingthattheimprovementsinrelevantqualitymetrics
Munich 4University of Oxford 5Munich Center for Machine
Learning(MCML)6GoetheUniversityFrankfurt7GermanCan- areprovableandevenclosetooptimalwithhighprobability.
cer Research Center (DKFZ). Correspondence to: Thomas Ourspecificcontributionsarethefollowing:
Decker<thomas.decker@siemens.com>,FlorianBuettner<flo-
rian.buettner@dkfz.de>.
• Weintroduceaninnovativeapproachforcombiningthe
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by resultsgeneratedbyvariousfeatureattributionmethods
theauthor(s). ordifferentvariantsofthesamemethod.
1
4202
nuJ
7
]GL.sc[
1v09050.6042:viXraProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
Figure1.Disagreementacrossattributionmethods(left):Differentfeatureattributionmethods(ϕ1,...,ϕ5)providedistinctperspec-
tivesaboutwhichparticularfeaturesofaninputxareimportantforanopaquemodelpredictionf(x).Oftentimestheytendtodisagree
causingambiguityaboutwhichinputstrulymatter. OurOptimizedAggregationapproach(right): Westudyhowtocombineall
individualattributionresultsfruitfullytoattainbetterexplanations.Weproposeanovelaggregationapproachtoretrieveoptimalconvex
weightsω suchthattheaggregatedfeatureattributionϕω =(cid:80) ω ϕiisprovablymorerobustandmorefaithfultotheunderlyingmodel.
i i i
• Weshowthatourmethodcanbeeffectivelyemployedto aggregation(seeFigure1,right).
optimizeexplanationsaccordingtocommonlyusedmea-
suresofquality,includingrobustnessandfaithfulnessto
3.BackgroundandRelatedWork
themodel’sbehaviorbasedonaunifyingframework.
3.1.Measuringattributionquality
• Weconductarigoroustheoreticalanalysisestablishing
provableimprovementsofexplanationqualityandcorre- Evaluatingthefidelityofexplanationsisachallengingen-
spondingoptimalityboundsforourapproach. deavorduetomissingknowledgeaboutanobjectiveground
truth. However,severalquantitativemetricshavebeenpro-
• We manifest these findings through a series of experi-
posedtoassessdifferentaspectsconcerningthequalityof
mentsinvolvingpopularfeatureattributiontechniques
feature attribution results (Nauta et al., 2023; Hedstro¨m
andmodelarchitectures,consistentlyoutperformingex-
et al., 2023a). In our study, we focus on two prominent
istingbaselinesandindividualmethods.
categoriesofexplanationquality:
2.ProblemSetup Robustness Manyexplanationmethodsexhibitinstabil-
ities under small input perturbations (Alvarez-Melis &
Ourgoalistoenhancethereliabilityofexplanationsbyde-
Jaakkola, 2018) leading to significantly different feature
velopingeffectivestrategiesforcombiningdiversefeature
attribution results for almost identical inputs. While this
attribution results. To illustrate, let’s consider explaining
notonlycastsdoubtsregardingtheexplanatoryintegrityof
a prediction, denoted as f(x), of a classification model
theconsideredtechnique, itmightfurtherbeexploitedto
f : Rd → R, for an input instance x ∈ Rd. A feature
manipulateexplanationsintentionally(Baniecki&Biecek,
attributionmethodϕ:Rd →[0,1]d,explainstheprediction
2023). Apopularmetrictoquantitativelymeasureattribu-
f(x) by associating to each separate input x a normal-
i tionrobustnessisMax-Sensitivity(Yehetal.,2019):
izedimportancescoreϕ (x). Supposewehaveaccesstok
i
differentattributionmethods,denotedbyϕ1,...,ϕk,each SENS : max ∥ϕ(x)−ϕ(x+ε)∥
MAX
offering a distinct perspective. Further, let ω ,...,ω be ∥ε∥≤δ
1 k
(cid:80)
scalar weights such that iω i = 1, and each weight is This quantity is typically estimated using a Monte Carlo
non-negative(ω i ≥0). Toaggregatedistinctattributionout- approach by sampling a fixed number of small perturba-
comesweconsidertheweightedsum(cid:80) iω iϕi(x)yielding tions,evaluatingtheexplanations,andstoringthemaximal
anovelexplanationthatcombinesindividualinsights. Our distortion. Analternativemetricforattributionrobustness
objectiveistodetermineprediction-specificweightsω iina isAverage-Sensitivity(Bhattetal.,2021):
mannerthatprovablyimprovesdesiredqualitymetrics.This
SENS :E [∥ϕ(x)−ϕ(x+ε)∥]
ultimatelyleadstomorereliableandbetterexplanationsvia AVG ε
2ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
whereε∼P isasmallrandominputperturbation,typically explanation. Theauthorsof(Bhattetal.,2021)proposeto
ε
eitherGaussianoruniformlydistributedwithmeanzero. enhanceexplanationsbycombiningtheShapleyValuesof
aninstancewiththeonesobtainedfromitsnearestneigh-
Faithfulness Thegoaloffaithfulnessmetricsistomea- borsinthetrainingdataset.
sure how aligned an attribution result is with the actual Ontheotherhand,theideaofaggregatingattributionresults
modelbehaviorinthesensethatperturbingimportantfea- acrossdistinctmethodshasreceivedconsiderablylessat-
tures should also alter the model prediction accordingly. tention. In(Rieger&Hansen,2019),theauthorspropose
Whiledifferentmathematicalformulationshavebeenpro- twobasicwaystocombinedistinctexplanationswhichare
posed,aprominentchoiceisInfidelity(Yehetal.,2019): definedasfollows. AGG = 1 (cid:80)k ϕi(x)simplyaver-
Mean k i=1
INFD:E (cid:2) (ITϕ(x)−(f(x)−f(x−I)))2(cid:3) agesdifferentattributionoutcomesandAGG Varincorporates
I alsofeature-wisevariabilitytodowngradetheimportance
Here,I ∈Rddescribesaprobabilisticperturbationsuchas offeatureswheremethodstendtodisagreeon:
replacingrandompartsofxwithafixedbaselinevalueor
1 (cid:88)k ϕi(x)
Gaussiannoise(Yehetal.,2019). Similarly,(Bhattetal., AGG =
Var k σ(ϕ1,...,ϕk)+ϵ
2021)proposedtoquantifyfaithfulnesstothemodel’sbe-
i=1
haviorusingacorrelationmeasure:
whereσ(ϕ1,...,ϕk)∈Rddescribesthefeature-wisestan-
FCOR:corr
I(cid:0) ITϕ(x),(f(x)−f(x−I))(cid:1)
darddeviationacrossthedifferentattributionresultsandϵ
isasmallconstantpromotingnumericalstability.
Thus,FaithfulnessCorrelation(FCOR)measureshowcor-
Nevertheless,atheoreticallygroundedstrategyofhowto
related the attribution scores are with prediction changes
best combine different attribution results for desired im-
undercorrespondinginputmodifications.
provementsisstillmissingandweaimtoaddressthisgap
intheremainderofthispaper.
Other metrics Beyond robustness and faithfulness, ad-
ditionaldimensionsofexplanationqualityhavealsobeen
4.OptimizingExplanationswithAggregation
investigatedintheliterature.Alignmentmetrics(Arrasetal.,
2022;Deckeretal.,2023)measuretowhichextentanex-
Generalized L2 metrics for explanations In this sec-
planation matches a desirable ground truth derived from
tion, we introduce a general class of quality metrics for
domainknowledgeandrandomization-basedsanitychecks
explanation methods that can efficiently be improved via
(Adebayo et al., 2018; Hedstro¨m et al., 2023b) ensure a
cross-methodcombinationasshownlater.
sufficientdependenceoftheattributionresultontheexam-
Definition4.1. LetQ:Rd →Rabeaqualitymetricfor
ined model. Moreover, Complexity metrics (Bhatt et al.,
feature attribution results. Then, Q belongs to the class
2021; Chalasani et al., 2020) quantify how comprehensi-
of generalized L2 metrics if there exist suitable random
bleamodel explanationisgiventhe premisethatsparser
variablesγ ∈Rg×dandγ ∈Rg suchthat:
attributionsaremoreinformativetohumansduetoreduced 1 2
cognitive load. Please refer to (Nauta et al., 2023) for a Q(ϕ(x))=E [∥γ ϕ(x)−γ ∥2]
morecomprehensiveoverviewofavailablemetrics.
γ1,γ2 1 2 2
Conceptually,anysuchmetricevaluatesthequalityofanat-
3.2.Aggregatingexplanations
tributionresultϕ(x)usingthefollowingintuitiveprinciple.
Theideaofaggregatingmultiplefeatureattributionresults First, a linear query γ 1 is applied to extract certain infor-
withinthesamemethodisalreadyanchoredinpopularex- mationfromtheattributionresults. Second, theobtained
plainabilitytechniques. SmoothGrad(Smilkovetal.,2017) informationcontentiscomparedtoadesiredqueryoutcome
and UniformGrad (Wang et al., 2020) combine gradients γ 2 usingthesquaredEuclideandistance. Thus, asmaller
intheproximityoftheinputandVarGrad(Adebayoetal., valueofQimpliesabetterattributionresultaccordingto
2018)usesthevarianceofgradientswithinaneighborhood theconsideredcriteria. Evaluatingsuchmetricscansimply
toderivefeatureimportance. Similarly, IntegratedGradi- beperformedbyestimatingtheexpectationwithafiniteset
ents(Sundararajanetal.,2017)andGradSHAP(Erionetal., ofmetricevaluationsamplesdenotedby{(γ(j),γ(j))}m .
1 2 i=1
2021)aggregategradientsalongaspecificpathtowardspre- Note that common quality metrics introduced above are
determinedbaselinevalues.Whilesuchtechniquescombine generalizedL2metrics. Forexample,Average-Sensitivity
gradientsfollowinginputperturbation,NoiseGrad(Bykov can be recovered in the following way: Let I be the d-
d
etal.,2022)averagesgradientsundermodelparametermod- dimensional identity matrix, then setting γ = I and
1 d
ificationstoformafinalexplanation.In(Rebuffietal.,2020) γ =ϕ(x+ε)withε∼P resultsinSENS withrespect
2 ε AVG
theauthorsanalyzehowthecombinationofattributionre- tothesquaredEuclideannorm. Similarly,Infidelitycanbe
sultsobtainedfromdifferentlayerscanimprovethefinal obtainedbychoosingγ =IT andγ =f(x)−f(x−I).
1 2
3ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
We further show in Appendix B how other categories of Provable improvement through aggregation The fol-
qualitymetricscanbeexpressedwithinthisframework. lowingtheoremallowsustopreciselyquantifythegainin
Inconclusion,manyestablishedqualitycriteriaformodel explanationqualityinducedviaconvexaggregation.
explanationscanbeassessedbasedonacorrespondingL2 Theorem4.2. Letϕω =(cid:80) ω ϕibetheaggregatedexpla-
i i
formulation. Next, weshowthatenhancingsuchmetrics nation,thenthequalitymetricofϕω isalwaysatleastas
through convex combinations leads to a well-posed opti-
goodastheweightedmetricsoftheindividualattributions:
mizationproblem.
(cid:34) (cid:35)
(cid:88) (cid:88)
Q(ϕω)= ω Q(ϕi)−E ω ∥γ (ϕ −ϕω)∥2
Derivingoptimalweights Rememberthatourgoalisto i γ1 i 1 i 2
combine multiple attribution results to provably improve i i
desiredqualitycriteria. Supposekdifferentattributionout-
comesϕ1(x)...ϕk(x)forwhichweseekoptimalconvex Note that this result can be related to the error ambigu-
ity decomposition for ensemble learning introduced in
weightfactorsω = (ω ,...,ω ). First,notethatevaluat-
1 k
ing an aggregated attribution result ϕω = (cid:80) ω ϕi via a (Krogh & Vedelsby, 1994) and we conduct the proof
i i in Appendix A. The achievable gain via aggregation
generalizedL2metricQreads:
E (cid:2)(cid:80) ω ∥γ (ϕ −ϕω)∥2(cid:3) ≥ 0dependsonhowdiverse
Q(ϕω)=E(cid:2) ∥γ ϕω−γ ∥2(cid:3) =E(cid:2) ∥(γ Φ)ω−γ ∥2(cid:3) thγ e1 diffei reni tex1 plani ationsbe2 haveunderqueriesγ compared
1 2 2 1 2 2 1
totheaggregatedone. Moreover,itsnon-negativityensures
whereΦ∈Rd×k describesthematrixofstackedindividual that the quality of the aggregated explanation is at least
attributionresultsΦ=(ϕ1,...,ϕk). Therefore,optimizing asgoodastheequivalentlyweightedindividualattribution
forconvexweightsωreducestosolving: qualitiessincelowervaluesofQimplyimprovements.
k
min E(cid:2) ∥(γ Φ)ω−γ ∥2(cid:3) s.t. ω ≥0, (cid:88) ω =1 Generalizationboundsforestimatedweights Obtain-
1 2 2 i i
ω ingoptimalweightsusuallyrequiresapproximatingtheob-
i=1
jective based on a limited set of metric evaluation sam-
Toeasethenotation,wedenotethesetoffeasibleaggrega-
tionweightsbyΩ = {ω ∈ Rk : ω ≥ 0, (cid:80)k ω = 1} ples{(γ 1(j),γ 2(j))}m i=1. Hence, theresultingestimateωˆ =
anddefineγ
2,k:
∈ Rg×k asmatrixi storingk ci= o1 piei sofγ
2
argmin ω∈Ω m1 (cid:80)m j=1∥γ 1(j)ϕω −γ 2(j)∥2 2 may deviate from
initscolumns. BysettingΓ := (γ Φ−γ ) ∈ Rg×k,it the ideal combination weights as it might not generalize
1 2,k:
well to unseen metric evaluations. As a consequence, it
holdswithinthesetoffeasibleweightsΩthat:
wouldbedesirabletoensurethatthequalityimprovement
min Q(ϕω) ⇔ min ωTE(cid:2) ΓTΓ(cid:3) ω withestimatedaggregationweightsiscloseenoughtothe
ω∈Ω ω∈Ω
bestpossiblestrategyconcerningtheentirequalitymetricQ.
Hence, searching for the best way to aggregate different Thefollowingtheoremestablishesacorrespondingresult.
attribution outcomes ends up in a constrained quadratic
Theorem4.3. ConsiderageneralizedL2metricdenotedby
program with convex constraints. This exhibits a global Qwithmax ∥γ ∥ ≤c andmax ∥γ ϕi−γ ∥2 ≤c .
optimumandcanefficientlybesolvedusingcorresponding
γ1 1 1 1 γ1,γ2 1 2 2 2
Additionally,letΩrepresentthesetoffeasibleweightsωand
numericalsolvers(Boyd&Vandenberghe,2004). Ontopof ϕω =(cid:80)k ω ϕidenoteanaggregatedfeatureattribution
i=1 i
that,thisobservationalsoenablesustooptimizemultiple
result. Suppose ωˆ is an estimate of aggregation weights
generalizedL2metricssimultaneouslyasquadraticforms
obtainedfrommmetricevaluationsamplesgivenby:
are additive. Suppose we seek to improve q independent
metricsQ 1,...Q qwithassociatedparametersΓ qasdefined
ωˆ =argmin
1 (cid:88)m
∥γ(j)ϕω−γ(j)∥2
above. Then,foranyscalersλ 1,...λ q wehave: ω∈Ω m 1 2 2
j=1
 
q q
(cid:88) λ jQ j(ϕω)=ωTE (cid:88) λ jΓT jΓ jω T anh den cth se ur ce he tx his at ta wc ito hn ps rta on bt abC il( ic ty1, oc f2 a) t> lea0 sd te (p 1e −nd δin ):gonc 1
2
j=1 j=1
(cid:114)
Thisimpliesthatsearchingforconvexweightsthatdirectly 4log(16k/δ)
Q(ϕωˆ)−minQ(ϕω)≤C
improvemultiplemetricsprioritizedbyλ i canalsobeex- ω∈Ω m
pressedasasingleconstrainedquadraticprogramandthus
efficientlybesolved. To prove this statement, we develop appropriate bounds
Inadditiontoitsnumericalappeal,optimizingexplanations ontheRademachercomplexityofvector-valuedfunctions
viaaggregationinthiswayalsocomeswiththeoreticalben- basedonaconcentrationresultfrom(Maurer,2016)andthe
efitsintheformofprovableimprovementguaranteesand specificpropertiesofgeneralizedL2metricsoverconvex
probabilisticoptimalitybounds. combinationsofnormalizedfeatureattributionresults. The
4ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
Table1.SENS (S )andSENS (S )resultsforgradient-basedattributionmethodsanddifferentaggregationstrategiesacross
AVG AVG MAX MAX
severalmodelarchitectures.OurapproachAGG consistentlyoutperformsallothertechniquesfollowedbyAGG assecondbest.
robust opt
Feature VGG16 AlexNet ResNet18 MobileNetV2 MLPMixer
Attribution S ↓ S ↓ S ↓ S ↓ S ↓ S ↓ S ↓ S ↓ S ↓ S ↓
AVG MAX AVG MAX AVG MAX AVG MAX AVG MAX
Saliency 0.994 1.214 0.800 0.942 0.964 1.163 1.022 1.265 1.200 1.591
GuidedBP 0.515 0.650 0.430 0.512 0.483 0.611 0.832 1.060 - -
DeepLift 0.893 1.090 0.791 0.932 0.857 0.999 0.894 1.075 0.961 1.165
IntGrad 0.888 1.065 0.724 0.854 0.838 0.991 0.910 1.084 0.941 1.141
InputxGrad 0.988 1.214 0.807 0.957 0.956 1.157 1.029 1.292 1.107 1,419
SmoothGrad 0.784 0.913 0.622 0.719 0.779 0.898 0.858 0.992 0.643 0.739
VarGrad 0.599 0.949 0.571 0.914 0.553 0.829 0.747 1.183 0.554 0.910
AGG 0.596 0.734 0.480 0.583 0.529 0.644 0.586 0.724 0.663 0.853
Mean
AGG 0.582 0.700 0.476 0.574 0.518 0.618 0.568 0.686 0.631 0.788
Var
AGG (ours) 0.644 0.833 0.535 0.679 0.471 0.578 0.792 1.036 0.696 0.892
faith
AGG (ours) 0.456 0.584 0.364 0.449 0.427 0.538 0.536 0.701 0.483 0.642
opt
AGG (ours) 0.424 0.543 0.349 0.426 0.410 0.513 0.505 0.654 0.473 0.634
robust
fullderivationandthepreciseexpressionfortheconstantC smallamountofmetricevaluationsamplestoapproximate
aregiveninAppendixA.Intuitively,theorem4.3guarantees theunderlyingmetric(m =50). Weexplicitlytesthow
agg
thatthemaximumpotentialdeviationofouraggregationap- well the improvements generalize to a larger sample of
proachfromtheoptimalimprovementcanbeboundedwith novel metric evaluations (m = 200) and if they trans-
eval
highprobability. Moreover,theworst-caseperformancegap fertoalternativequalitymeasures. Thefindingspresented
√
diminisheswithorderO(1/ m)forincreasingnumberof in this section are based on the ImageNet ILSVRC2012
metricevaluationsamplesm. datasetandconcreteimplementationdetailsaredocumented
inAppendixC. Accompanyingsourcecodeisreleasedat
Optimalaggregationfordesiredimprovements Based https://github.com/thomdeck/aggopt.
onthegeneralizedframeworkaboveweproposedifferent
aggregationstrategiestointentionallyenhancespecificprop- 5.1.Quantitativeevaluationofqualityimprovements
ertiesoffeatureattributionresults. Toexplicitlyenhanceex-
Increasing robustness via AGG and AGG We
planationrobustnessweobtaincombinationweightsωrobust robust opt
examinetowhichextentouraggregationapproachcanmit-
byoptimizingAverage-SensitivityasrelatedL2metric.
igatetypicallyencounteredinstabilitiesofgradient-based
AGG : ωrobust =argmin SENS (ϕω) explanations on convolutional models. For this purpose,
robust AVG
ω∈Ω weconsidersevencorrespondingattributiontechniquesas
Equivalently,tooptimizeforfaithfulnesswecancompute well as four different ways of combining them including
ωfaithbyconsideringInfidelityasunderlyingobjective: the two simple baselines AGG Mean and AGG Var and our
proposed strategies AGG and AGG . All resulting
robust opt
AGG : ωfaith =argmin INFD(ϕω) explanationsarecomputedfor500randomsamplesfrom
faith
ω∈Ω
ImageNetacrossfivepopularcomputervisionmodelsand
As a default strategy to increase explanation quality via weevaluatedtheirrobustnessbasedonthemetricsSENS AVG
aggregation, we further propose improving both metrics andSENS MAX. ThecorrespondingresultsinTable1indi-
simultaneously. WecointhisapproachAGG optoptimizing catethatourapproachAGG robustconsistentlyoutperforms
forbetterfeatureattributionsmoregenerically: all individual attribution methods as well as all other ag-
gregations followed by AGG , which is almost always
opt
AGG opt : ωopt =argmin INFD(ϕω)+SENS AVG(ϕω) secondbest. RememberthatAGG robust directlyoptimizes
ω∈Ω
forSENS usingasmallnumberofmetricevaluations.
AVG
Thus,thegeneralizationperformanceforthismetric,now
5.Experiments
evaluatedwithahighernumberofunseenmetricevaluation
Weconductedamultifacetedempiricalevaluationtoinves- samples,isinlinewithourtheoreticalframework. Ontop
tigatethecapacitiesofourproposedaggregationstrategies of that, the additional superiority in terms of the alterna-
tointentionallyenhancedesiredpropertiesofexplanations. tivemetricSENS MAX demonstratesthatourapproachalso
All our aggregation strategies are optimized using only a improvesattributionrobustnessingeneral.
5ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
Table2.INFDandFCORresultsfordifferentattributionmethodsandaggregationstrategiesacrossseveralmodelarchitectures. Our
approachAGG consistentlyoutperformsallothertechniquesandAGG iseithersecondbestorcomparable.
faith opt
Feature DenseNet121 ResNet18 MobileNetV2 DeiT SwinT
Attribution INFD↓ FCOR↑ INFD↓ FCOR↑ INFD↓ FCOR↑ INFD↓ FCOR↑ INFD↓ FCOR↑
GradSHAP 2.846 0.303 3.196 0.369 0.509 0.261 0.512 0.120 0.446 0.094
IntGrad 2.913 0.263 3.371 0.312 0.522 0.223 0.507 0.122 0.444 0.094
InputxGrad 3.097 0.205 3.587 0.259 0.538 0.192 0.516 0.113 0.446 0.079
SmoothGrad 2.604 0.388 2.916 0.444 0.494 0.296 0.392 0.297 0.367 0.208
GradCAM 2.646 0.388 2.922 0.459 0.478 0.319 0.385 0.311 0.373 0.227
GradCAM++ 2.687 0.376 2.988 0.438 0.484 0.306 0.487 0.213 0.394 0.187
EigenCAM 3.044 0.251 3.381 0.347 0.538 0.231 0.568 0.058 0.439 0.107
AGG 2.661 0.370 2.928 0.444 0.479 0.293 0.449 0.237 0.377 0.213
Mean
AGG 2.675 0.368 2.945 0.442 0.481 0.294 0.447 0.238 0.381 0.212
Var
AGG (ours) 2.678 0.339 2.956 0.415 0.483 0.282 0.407 0.288 0.366 0.224
robust
AGG (ours) 2.514 0.380 2.729 0.458 0.467 0.304 0.339 0.368 0.341 0.265
opt
AGG (ours) 2.390 0.406 2.595 0.481 0.443 0.325 0.335 0.372 0.335 0.275
faith
Increasing faithfulness via AGG and AGG and Relative Output Stability (ROS) evaluates sensitivity
faith opt
Similar to the robustness experiments, we evaluate the relativetochangesinoutputpredictionprobabilities. The
metrics Infidelity INFD and Faithfulness correlation corresponding results in Table 3 over 500 samples on a
FCOR to validate the capabilities of our approach for ResNet18showthatourdedicatedapproachAGG also
robust
improving attribution fidelity. For both metrics, features significantly improves all stability metrics. During the
areperturbedbyreplacingrandomlyselectedpixelswith experimentsabove,weusedblurringasbaseperturbation
thecorrespondingvaluesofablurredimageversion. More when optimizing aggregation for faithfulness and when
details regarding the precise design of these metrics and evaluatingInfidelityandFaithfulnesscorrelation. Tocheck
the underlying perturbations are specified in Appendix howwelltheimprovementsgeneralizetoslightlyaltering
C1. Weagainconsideredfivepopularmodelarchitectures notions of faithfulness, we computed variations of these
includingtwotransformer-basedonesandselectedseven metricsbasedonalternativecorruptions(Hedstro¨metal.,
applicable attribution methods. Table 2 summarizes the 2023a)suchaspixelvaluereplacementwithzeros(INFD
0
resultsandshowsthatourdedicatedaggregationapproach and FCOR ) and the image mean (INFD and FCOR ).
0 x¯ x¯
AGG performs best in every scenario, followed again Even though we explicitly kept blurring as perturbation
faith
by AGG being either second best or comparable. The during weight optimization for AGG and AGG the
opt opt faith
consistentsuperiorityforINFDagainsupportsourtheory resulting explanations still perform best when evaluated
thatoptimizingaggregationusingonlyasmallamountof with different corruptions. Finally, we computed the
evaluationsamplesisenoughtoattainsustainablequality RemoveandDebias(ROAD)metric(Rongetal.,2022)that
enhancement generalizing to novel out-of-sample metric assesses the fidelity of explanations by removing the top
evaluationsunseenduringoptimization. Interestingly,the featuresidentifiedbyanattributionmethodandestimating
additional improvement concerning FCOR seems to be thesubsequentdecreaseinpredictionconfidence. InTable
particularlystrongforthetwotransformer-basedmodels. 3 we report the outcomes of ablating the most relevant
p = 10,20,30 percent of pixels in an image (MoRF )
p
while additional percentiles are deferred to Appendix
Improving additional quality metrics To further D1. All results consistently indicate that also this metric
substantiatethebenefitsofourproposedmethods,wealso canbeimprovedbyrelyingononeofourproposedmethods.
investigated how well the improvements generalize to
additionalqualitymetricsthatexpressalternativenotions
Overall, the results of all experiments manifest that our
ofrobustnessandfaithfulness. Stabilitymetrics(Agarwal
aggregation techniques achieve desired and generalizing
etal.,2022a)offeracomplementaryapproachtoevaluating
improvementsinexplanationqualityinlinewithourthe-
therobustnessofexplanationsbyquantifyingthesensitivity
oretical analysis. They also suggest that the aggregation
ofattributionresultsrelativetochangesinvariousquantities
strategyAGG , whichoptimizesforfaithfulnessandro-
ofinterest. Morespecifically,RelativeInputStability(RIS) opt
bustness simultaneously, is an effective default approach
assesses how explanations vary relative to input changes,
to attain better explainability via aggregation when both
RelativeRepresentationStability(RRS)examinesvariations
criteria matter. All these findings are confirmed on four
relativetochangesinthemodel’sinternalrepresentations,
6ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
Table3.AdditionalqualitymetricresultsonaResNet18fordifferentattributionmethodsandaggregationstrategies.Acrossallevaluations,
oneofourproposedapproachesperformsbest,andAGG isconsistentlyatleastsecondbestorcomparable.
opt
Feature Stability↓ Infidelity↓ Faith. Corr. ↑ ROAD↓
Attribution RIS RRS ROS INFD INFD FCOR FCOR MoRF MoRF MoRF
0 x¯ 0 x¯ 10 20 30
Deeplift 8.10 5.26 8.92 3.59 3.64 0.33 0.32 -1.10 -2.02 -3.02
VarGrad 7.22 4.73 7.86 3.18 3.12 0.42 0.42 -2.37 -4.69 -6.79
GuidedBP 3.61 2.31 3.91 3.21 3.10 0.44 0.45 -3.09 -4.86 -6.31
IntGrad 7.69 4.88 8.28 3.14 3.12 0.43 0.43 -0.85 -1.79 -2.82
SmoothGrad 8.18 5.43 9.32 3.61 3.61 0.33 0.32 -1.57 -2.72 -3.84
InputxGrad 9.47 6.08 10.28 3.84 3.90 0.27 0.27 -0.63 -1.34 -2.24
Saliency 9.13 5.89 9.99 3.50 3.52 0.34 0.36 -0.58 -1.24 -2.04
AGG 5.33 3.41 5.75 3.16 3.23 0.43 0.41 -2.21 -3.98 -5.68
Mean
AGG 5.19 3.33 5.61 3.18 3.22 0.42 0.42 -2.21 -3.98 -5.69
Var
AGG (ours) 5.73 3.70 6.25 2.82 2.78 0.47 0.48 -2.66 -4.50 -6.12
faith
AGG (ours) 3.62 2.33 3.96 2.83 2.80 0.49 0.47 -3.30 -5.41 -7.14
opt
AGG (ours) 3.27 2.09 3.55 2.97 2.93 0.46 0.46 -3.36 -5.48 -7.17
robust
additionaldatasetsinAppendixD2. Furthermore,wepro- chitecturesothermethodsaremostfavored. Thisprovides
videsupplementaryablationstudiesinAppendixD4,which furtherevidencethatasingleattributionmethodseemsun-
implybeneficialeffectsresultingfromanincreasingnumber abletoexplaineverypredictionforallmodelarchitectures
ofcombinedexplanationsandgreatermethoddiversity. faithfully and supports our approach to rather aggregate
theminanoptimizingmanner.
5.2.Understandinghowoptimizedaggregationhelps
5.3.Enhancingindividualmethodsviaaggregation
InFigure2wedisplaysevenconcreteexampleswithcor-
respondingindividualandaggregatedattributionresultsto Manyfeatureattributionmethodsrelyonseveralhyperpa-
gainfurtherinsightsintohowoptimizedaggregationsuc- rametersandtheirconcretechoicecangreatlyimpactthere-
ceedsinimprovingexplanations. Noticethatourgeneric sultingexplanation(Bansaletal.,2020).Apopularexample
aggregationapproachAGG enhancesfeatureattributions isLIME(Ribeiroetal.,2016),whichderivesfeatureimpor-
opt
essentiallyviatwomechanisms. Particularlyinthefirsttwo tancebyfittingalinearsurrogatemodeltoapproximatethe
images, all considered methods highlight intuitively rele- modelbehaviorinthevicinityofaninput. Whenapplied
vantbutdivergingregionscausingambiguityaboutwhich toimagedata,LIMEtypicallycomputesattributionsatthe
pixelstrulymatter. AGG improvestheexplanationsby levelofsuperpixelsandincorporatesanL1regularizationto
opt
combiningallperspectivestocomplementeachother,which enforceacertainlevelofsparsityviaLASSO(Ribeiroetal.,
alsoleadstoavisuallymoreconvincingexplanation. Inthe 2016;Garreau&Mardaoui,2021). However,therequire-
lasttwoimages,someindividualmethodsseemtofailby mentoffixingtheregularizationstrengthinadvancemight
producing rather deteriorated results. For such instances resultininferiorexplanationsincaseswherethenumberof
AGG performsautomaticmethodselectionsintrinsically importantfeaturesdoesnotmatchtheenforcedlevelofspar-
opt
andaggregatesonlyvalidattributionoutcomestoforman sity. Toevaluateifoptimizedaggregationcaneffectively
enhancedexplanationthatismorerepresentativeoftheun- mediatethiscriticalhyperparameterchoice,weconducted
derlyingmodel. Consultingthedistributionofaggregation thefollowingexperiment. Weaggregatesixdifferentver-
weights retrieved during both experiments in section 5.1 sionsofLIMEcoveringtwodifferentsuperpixelalgorithms
also reveals that the optimal weighting is highly model- (SLIC (Achanta et al., 2012) and squared patches), each
dependentandevenexhibitsstrongvariabilityacrosssam- exhibitingeitherno,low,orhighlevelofsparsityregular-
ples. In Figure 4 we present corresponding boxplots for ization. Usingtheavailableboundingboxinformationfor
theweightsofAGG obtainedforthetwoconsideredsets ImageNet,wedistinguishbetweenimageswheretheobject
opt
of attribution techniques. For all methods, the allocated to be classified is particularly small (< 20% of the total
weightsduringtheexperimentsvarysubstantiallyamong picture) or rather larger (> 60%). We randomly selected
samplescoveringoftentimeseventheentirepossiblerange 200imagesperobjectsizeandFigure5displaystheaverage
between 0 and 1. Moreover, the distribution of allocated weightsallocatedbyAGG todifferentvariantsofLIME
opt
weightsdoesnottransferacrossmodelsasfordifferentar- grouped by sparsity regularization. For both considered
7ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
Figure2.IndividualoutcomesofdifferentfeatureattributionmethodsaswellasourapproachAGG (rightcolumn)forsevenimages
opt
basedonVGG16(row1-5)andDeiT(row6-8).Inadditiontothequantitativeimprovementsestablishedinsection5.1forrobustnessand
faithfulness,ouraggregationstrategyalsoproducesvisuallymoreintuitiveandconvincingexplanations.Itsucceedsinenhancingthe
attributionresultsbycombiningseveralvalidperspectivestocomplementeachother(e.grows1and2)andbyautomaticallydiscarding
seeminglydeterioratedexplanations(e.g.rows7and8).
Figure3.IndividualattributionresultsofdifferentLIMEvariantsvaryingbysuperpixelstructureandsparsityregularizationonVGG16.
TheobjecttobeclassifiedisrathersmallandAGG automaticallycombinesonlythesparsestexplanationstoenhancetheexplanation.
opt
8ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
Figure5.Average aggregation
weights obtained by AGG
opt
whileoptimizingtheresultsfrom
different versions of LIME on
Figure4.BoxplotsofaggregationweightsobtainedbyAGG forthetwoconsideredsetsof
opt
VGG16(top)andViT(bottom)in-
attributionmethodsduringtheevaluationsinsection5.1forrobustness(top)andfaithfulness
cluding95%confidenceintervals
(bottom)basedon500samples.Foreachmethod,theallocatedweightdifferssubstantially
aserrorbars.Forsmallerobjects,
amongsamplesasmostdistributionscoveralmosttheentirerangebetween0and1.Thereis
significantly more weight is put
alsohighvariabilityacrossmodelsindicatingthatasinglemethodaloneisunabletoprovide
onhighersparsityregularization.
areliableexplanationforeverypredictionconsistently.
models,significantlymoreweightisputontheexplanations validityasanyuniformweaknessmightalsocompromise
resultingfromhighersparsityregularizationwhentheobject theaggregation. Hence,werecommendconsideringasuffi-
to be classified is small compared to larger ones. This is cientlydiversesetofindividualtechniquesandweprovide
also exemplified in Figure 3, where the prediction for an correspondingablationstudiesinAppendixD4.
imagecontainingarathersmallobjectisexplained. AGG A natural extension of our work is to consider more so-
opt
optimizes the results of all considered LIME variants by phisticatedstrategiesbeyondconvexweightingtoperform
aggregatingonlythetwosparsestattributionsmatchingthe aggregation,suchasvotingalgorithmsorotherensemble
locationofinterest. methods. Furthermore,wespecificallyfocusedoncombin-
Thisdemonstratesthatourproposedaggregationapproach ingfairlycomparablefeatureattributiontechniques. Future
alsobooststheperformanceofindividualattributiontech- workcouldalsoexplorehowtobestincorporatesupplemen-
niques when combining different versions of the same taryinsightsderivedfromconcept-based(Hitzler&Sarker,
method. 2022),optimization-based(Dabkowski&Gal,2017;Fong
etal.,2019;Jethanietal.,2021)orcounterfactualexplana-
tions(Guidotti,2022)toevenfurtherenhanceexplainability
6.DiscussionandConclusion
withaggregation.
Inthiswork,weprovidedthefirsttheoreticallygrounded
approachtooptimallyleveragedistinctfeatureattribution Acknowledgements
results for improving explanations of opaque models. A
downsideofourtechniqueisthehigherinferencetimeespe- WeacknowledgethesupportfromtheGermanFederalMin-
ciallycomparedtorelyingonasinglemethodonly. How- istry for Economic Affairs and Climate Action (BMWK)
ever,sincethemainpurposeofexplainabilitytechniquesis viagrantagreement19I21039A.
toreliablyincreasethetransparencyofparticularlycritical
decisionswearguethattheaddedcomputationalcostsare
ImpactStatement
minor (see Appendix D3) and well justified for the sake
of provably better results. Another limiting aspect is the Thispaperpresentsworkthataimsatenhancingthetrans-
reliance on existing feature attribution methods and their parencyofpredictionsmadebyopaquemachinelearning
9ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
models. Enabling more reliable explanations can be as- InternationalJointConferencesonArtificialIntelligence,
sociated with a variety of societal benefits by promoting pp.3016–3022,2021.
trust, accountability, andcompliancewithregulatoryand
Bilodeau,B.,Jaques,N.,Koh,P.W.,andKim,B. Impos-
ethicalstandards. Hence,anyresearchdedicatedtoexplain-
sibilitytheoremsforfeatureattribution. Proceedingsof
able machine learning has the potential to contribute to
theNationalAcademyofSciences,121(2):e2304406120,
moreresponsibledevelopmentanddeploymentofmachine
2024.
learning-basedtechnology.
Boyd, S. P. and Vandenberghe, L. Convex optimization.
References Cambridgeuniversitypress,2004.
Achanta,R.,Shaji,A.,Smith,K.,Lucchi,A.,Fua,P.,and Bykov,K.,Hedstro¨m,A.,Nakajima,S.,andHo¨hne,M.M.-
Su¨sstrunk,S. Slicsuperpixelscomparedtostate-of-the- C. Noisegrad—enhancingexplanationsbyintroducing
art superpixel methods. IEEE transactions on pattern stochasticity to model weights. In Proceedings of the
analysis and machine intelligence, 34(11):2274–2282, AAAIConferenceonArtificialIntelligence, volume36,
2012. pp.6132–6140,2022.
Adebayo,J.,Gilmer,J.,Muelly,M.,Goodfellow,I.,Hardt, Castro,J.,Go´mez,D.,andTejada,J.Polynomialcalculation
M.,andKim,B. Sanitychecksforsaliencymaps. Ad- oftheshapleyvaluebasedonsampling. Computers&
vances in neural information processing systems, 31, OperationsResearch,36(5):1726–1730,2009.
2018.
Chalasani,P.,Chen,J.,Chowdhury,A.R.,Wu,X.,andJha,
Adebayo,J.,Muelly,M.,Liccardi,I.,andKim,B. Debug- S. Conciseexplanationsofneuralnetworksusingadver-
gingtestsformodelexplanations. AdvancesinNeural sarialtraining. InInternationalConferenceonMachine
InformationProcessingSystems,33:700–712,2020. Learning,pp.1383–1391.PMLR,2020.
Agarwal,C.,Johnson,N.,Pawelczyk,M.,Krishna,S.,Sax- Chattopadhay,A.,Sarkar,A.,Howlader,P.,andBalasubra-
ena, E., Zitnik, M., and Lakkaraju, H. Rethinking sta- manian,V.N. Grad-cam++: Generalizedgradient-based
bilityforattribution-basedexplanations. arXivpreprint visualexplanationsfordeepconvolutionalnetworks. In
arXiv:2203.06877,2022a. 2018IEEEwinterconferenceonapplicationsofcomputer
vision(WACV),pp.839–847.IEEE,2018.
Agarwal,C.,Krishna,S.,Saxena,E.,Pawelczyk,M.,John-
son,N.,Puri,I.,Zitnik,M.,andLakkaraju,H. OpenXAI: Dabkowski, P. and Gal, Y. Real time image saliency for
Towardsatransparentevaluationofmodelexplanations. black box classifiers. Advances in neural information
In Thirty-sixth Conference on Neural Information Pro- processingsystems,30,2017.
cessingSystemsDatasetsandBenchmarksTrack,2022b.
Decker,T.,Lebacher,M.,andTresp,V. Doesyourmodel
Alvarez-Melis, D. and Jaakkola, T. S. On the ro- thinklikeanengineer? explainableaiforbearingfault
bustness of interpretability methods. arXiv preprint detectionwithdeeplearning.InICASSP2023-2023IEEE
arXiv:1806.08049,2018. InternationalConferenceonAcoustics,SpeechandSig-
nalProcessing(ICASSP),pp.1–5.IEEE,2023.
Arras,L.,Osman,A.,andSamek,W. Clevr-xai: Abench-
mark dataset for the ground truth evaluation of neural Diamond,S.andBoyd,S. CVXPY:APython-embedded
network explanations. Information Fusion, 81:14–40, modelinglanguageforconvexoptimization. Journalof
2022. ISSN1566-2535. MachineLearningResearch,17(83):1–5,2016.
Baniecki,H.andBiecek,P.Adversarialattacksanddefenses Dicker,L. Sparsityandthetruncatedl2-norm. InArtificial
in explainable artificial intelligence: A survey. arXiv IntelligenceandStatistics,pp.159–166.PMLR,2014.
preprintarXiv:2306.06123,2023.
Dombrowski, A.-K., Alber, M., Anders, C., Ackermann,
Bansal, N., Agarwal, C., andNguyen, A. Sam: Thesen- M., Mu¨ller, K.-R., and Kessel, P. Explanations can be
sitivity of attribution methods to hyperparameters. In manipulated and geometry is to blame. Advances in
Proceedingsoftheieee/cvfconferenceoncomputervi- neuralinformationprocessingsystems,32,2019.
sionandpatternrecognition,pp.8673–8683,2020.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
Bhatt, U., Weller, A., and Moura, J. M. Evaluating and D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
aggregating feature-based model explanations. In Pro- M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby,
ceedingsoftheTwenty-NinthInternationalConferenceon N. An image is worth 16x16 words: Transformers for
10ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
imagerecognitionatscale. InInternationalConference Hitzler,P.andSarker,M. Human-centeredconceptexpla-
onLearningRepresentations,2021. nationsforneuralnetworks. Neuro-SymbolicArtificial
Intelligence: TheStateoftheArt,342(337):2,2022.
Erion, G., Janizek, J. D., Sturmfels, P., Lundberg, S. M.,
andLee,S.-I. Improvingperformanceofdeeplearning Hooker,S.,Erhan,D.,Kindermans,P.-J.,andKim,B. A
models with axiomatic attribution priors and expected benchmark for interpretability methods in deep neural
gradients. Nature machine intelligence, 3(7):620–631, networks. Advances in neural information processing
2021. systems,32,2019.
Fokkema,H.,deHeide,R.,andvanErven,T. Attribution- Huang,G.,Liu,Z.,VanDerMaaten,L.,andWeinberger,
basedexplanationsthatproviderecoursecannotberobust. K. Q. Densely connected convolutional networks. In
Journal of Machine Learning Research, 24(360):1–37, ProceedingsoftheIEEEconferenceoncomputervision
2023. andpatternrecognition,pp.4700–4708,2017.
Jethani,N.,Sudarshan,M.,Aphinyanaphongs,Y.,andRan-
Fong,R.,Patrick,M.,andVedaldi,A. Understandingdeep
ganath, R. Have we learned to explain?: How inter-
networksviaextremalperturbationsandsmoothmasks.
pretability methods can learn to encode predictions in
InProceedingsoftheIEEE/CVFinternationalconference
theirinterpretations. InInternationalConferenceonArti-
oncomputervision,pp.2950–2958,2019.
ficialIntelligenceandStatistics,pp.1459–1467.PMLR,
Garreau,D.andMardaoui,D. Whatdoeslimereallyseein 2021.
images? InInternationalconferenceonmachinelearning,
Kindermans, P.-J., Hooker, S., Adebayo, J., Alber, M.,
pp.3620–3629.PMLR,2021.
Schu¨tt, K. T., Da¨hne, S., Erhan, D., and Kim, B. The
Ghorbani,A.,Abid,A.,andZou,J. Interpretationofneural (un)reliabilityofsaliencymethods. InExplainableAI:
networksisfragile. InProceedingsoftheAAAIconfer- Interpreting,ExplainingandVisualizingDeepLearning,
enceonartificialintelligence,volume33,pp.3681–3688, pp.267–280.Springer,2019.
2019.
Kokhlikyan, N., Miglani, V., Martin, M., Wang, E., Al-
Gildenblat,J. Pytorchlibraryforcammethods. https:// sallakh,B.,Reynolds,J.,Melnikov,A.,Kliushkina,N.,
github.com/jacobgil/pytorch-grad-cam, Araya,C.,Yan,S.,etal. Captum: Aunifiedandgeneric
2021. modelinterpretabilitylibraryforpytorch. arXivpreprint
arXiv:2009.07896,2020.
Guidotti,R. Counterfactualexplanationsandhowtofind
Krishna,S.,Han,T.,Gu,A.,Pombra,J.,Jabbari,S.,Wu,
them: literaturereviewandbenchmarking. DataMining
S.,andLakkaraju,H. Thedisagreementprobleminex-
andKnowledgeDiscovery,pp.1–55,2022.
plainablemachinelearning: Apractitioner’sperspective.
Han,T.,Srinivas,S.,andLakkaraju,H. Whichexplanation arXivpreprintarXiv:2202.01602,2022.
shouldichoose? afunctionapproximationperspectiveto
Krizhevsky,A.,Sutskever,I.,andHinton,G.E. Imagenet
characterizingposthocexplanations. AdvancesinNeural
classification with deep convolutional neural networks.
InformationProcessingSystems,35:5256–5268,2022.
Advancesinneuralinformationprocessingsystems,25,
He,K.,Zhang,X.,Ren,S.,andSun,J. Deepresiduallearn- 2012.
ingforimagerecognition. InProceedingsoftheIEEE
Krogh, A. and Vedelsby, J. Neural network ensembles,
conferenceoncomputervisionandpatternrecognition,
crossvalidation,andactivelearning. Advancesinneural
pp.770–778,2016.
informationprocessingsystems,7,1994.
Hedstro¨m, A., Weber, L., Krakowczyk, D., Bareeva, D.,
Kumar, I., Scheidegger, C., Venkatasubramanian, S., and
Motzkus, F., Samek, W., Lapuschkin, S., and Ho¨hne,
Friedler,S. Shapleyresiduals: Quantifyingthelimitsof
M. M.-C. Quantus: An explainable ai toolkit for re-
theshapleyvalueforexplanations. AdvancesinNeural
sponsibleevaluationofneuralnetworkexplanationsand
InformationProcessingSystems,34:26598–26608,2021.
beyond. JournalofMachineLearningResearch,24(34):
1–11,2023a. Lin, C., Covert, I., and Lee, S.-I. On the robustness
of removal-based feature attributions. arXiv preprint
Hedstro¨m, A., Weber, L., Lapuschkin, S., andHo¨hne, M.
arXiv:2306.07462,2023.
Sanity checks revisited: An exploration to repair the
modelparameterrandomisationtest. InXAIinAction: Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,Lin,
Past,Present,andFutureApplications,2023b. S.,andGuo,B. Swintransformer: Hierarchicalvision
11ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
transformerusingshiftedwindows. InProceedingsofthe forattributionmethods. InInternationalConferenceon
IEEE/CVFinternationalconferenceoncomputervision, MachineLearning,pp.18770–18795.PMLR,2022.
pp.10012–10022,2021.
Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., and
Lundberg,S.M.andLee,S.-I. Aunifiedapproachtointer- Chen,L.-C. Mobilenetv2: Invertedresidualsandlinear
pretingmodelpredictions. Advancesinneuralinforma- bottlenecks. InProceedingsoftheIEEEconferenceon
tionprocessingsystems,30,2017. computervisionandpatternrecognition,pp.4510–4520,
2018.
Maurer,A. Avector-contractioninequalityforrademacher
Selvaraju, R. R., Cogswell, M., Das, A., Vedantam, R.,
complexities. In Algorithmic Learning Theory: 27th
Parikh, D., and Batra, D. Grad-cam: Visual explana-
InternationalConference,ALT2016Proceedings27,pp.
tionsfromdeepnetworksviagradient-basedlocalization.
3–17.Springer,2016.
InProceedingsoftheIEEEinternationalconferenceon
Muhammad,M.B.andYeasin,M. Eigen-cam: Classacti- computervision,pp.618–626,2017.
vationmapusingprincipalcomponents. In2020interna-
Shalev-Shwartz,S.andBen-David,S. Understandingma-
tionaljointconferenceonneuralnetworks(IJCNN),pp.
chinelearning: Fromtheorytoalgorithms. Cambridge
1–7.IEEE,2020.
universitypress,2014.
Nauta,M.,Trienes,J.,Pathak,S.,Nguyen,E.,Peters,M.,
Shrikumar,A.,Greenside,P.,Shcherbina,A.,andKundaje,
Schmitt,Y.,Schlo¨tterer,J.,vanKeulen,M.,andSeifert,
A. Not just a black box: Learning important features
C. From anecdotal evidence to quantitative evaluation
throughpropagatingactivationdifferences.arXivpreprint
methods: Asystematicreviewonevaluatingexplainable
arXiv:1605.01713,2016.
ai. ACMComputingSurveys,55(13s):1–42,2023.
Shrikumar, A., Greenside, P., and Kundaje, A. Learning
Neely, M., Schouten, S. F., Bleeker, M. J., and Lucic, A.
importantfeaturesthroughpropagatingactivationdiffer-
Orderinthecourt: Explainableaimethodspronetodis-
ences. InInternationalconferenceonmachinelearning,
agreement. arXivpreprintarXiv:2105.03287,2021.
pp.3145–3153.PMLR,2017.
Nie, W., Zhang, Y., and Patel, A. A theoretical explana- Simonyan, K., Vedaldi, A., and Zisserman, A. Deep in-
tionforperplexingbehaviorsofbackpropagation-based side convolutional networks: Visualising image clas-
visualizations. InInternationalconferenceonmachine sification models and saliency maps. arXiv preprint
learning,pp.3809–3818.PMLR,2018. arXiv:1312.6034,2013.
Pahde, F., Yolcu, G. U., Binder, A., Samek, W., and La- Sixt,L.,Granz,M.,andLandgraf,T. Whenexplanations
puschkin,S. Optimizingexplanationsbynetworkcanon- lie: Whymanymodifiedbpattributionsfail. InInterna-
izationandhyperparametersearch. InProceedingsofthe tionalConferenceonMachineLearning,pp.9046–9057.
IEEE/CVFConferenceonComputerVisionandPattern PMLR,2020.
Recognition (CVPR) Workshops, pp. 3819–3828, June
Smilkov,D.,Thorat,N.,Kim,B.,Vie´gas,F.,andWatten-
2023.
berg,M. Smoothgrad: removingnoisebyaddingnoise.
Rebuffi,S.-A.,Fong,R.,Ji,X.,andVedaldi,A. Thereand arXivpreprintarXiv:1706.03825,2017.
backagain:Revisitingbackpropagationsaliencymethods.
Springenberg, J. T., Dosovitskiy, A., Brox, T., and Ried-
In Proceedings of the IEEE/CVF Conference on Com-
miller,M. Strivingforsimplicity: Theallconvolutional
puter Vision and Pattern Recognition, pp. 8839–8848,
net. arXivpreprintarXiv:1412.6806,2014.
2020.
Sturmfels,P.,Lundberg,S.,andLee,S.-I. Visualizingthe
Ribeiro,M.T.,Singh,S.,andGuestrin,C. ”whyshould
impactoffeatureattributionbaselines. Distill,5(1):e22,
itrustyou?”explainingthepredictionsofanyclassifier.
2020.
InProceedingsofthe22ndACMSIGKDDinternational
conferenceonknowledgediscoveryanddatamining,pp. Sundararajan,M.,Taly,A.,andYan,Q. Axiomaticattribu-
1135–1144,2016. tionfordeepnetworks. InInternationalconferenceon
machinelearning,pp.3319–3328.PMLR,2017.
Rieger,L.andHansen,L.K. Aggregatingexplanationmeth-
Tolstikhin, I. O., Houlsby, N., Kolesnikov, A., Beyer, L.,
odsforstableandrobustexplainability. arXivpreprint
Zhai,X.,Unterthiner,T.,Yung,J.,Steiner,A.,Keysers,
arXiv:1903.00519,2019.
D.,Uszkoreit,J.,etal. Mlp-mixer: Anall-mlparchitec-
Rong,Y.,Leemann,T.,Borisov,V.,Kasneci,G.,andKas- tureforvision. Advancesinneuralinformationprocess-
neci, E. A consistent and efficient evaluation strategy ingsystems,34:24261–24272,2021.
12ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
Touvron,H.,Cord,M.,Douze,M.,Massa,F.,Sablayrolles,
A.,andJe´gou,H.Trainingdata-efficientimagetransform-
ers&distillationthroughattention. InInternationalcon-
ferenceonmachinelearning,pp.10347–10357.PMLR,
2021.
Wang,Z.,Wang,H.,Ramkumar,S.,Mardziel,P.,Fredrik-
son, M., and Datta, A. Smoothed geometry for robust
attribution. Advancesinneuralinformationprocessing
systems,33:13623–13634,2020.
Wightman,R.Pytorchimagemodels.https://github.
com/rwightman/pytorch-image-models,
2019.
Yang,J.,Shi,R.,Wei,D.,Liu,Z.,Zhao,L.,Ke,B.,Pfister,
H., and Ni, B. Medmnist v2-a large-scale lightweight
benchmarkfor2dand3dbiomedicalimageclassification.
ScientificData,10(1):41,2023.
Yeh, C.-K., Hsieh, C.-Y., Suggala, A., Inouye, D. I., and
Ravikumar,P.K. Onthe(in)fidelityandsensitivityof
explanations.AdvancesinNeuralInformationProcessing
Systems,32,2019.
Zhou, Y., Booth, S., Ribeiro, M.T., andShah, J. Dofea-
tureattributionmethodscorrectlyattributefeatures? In
ProceedingsoftheAAAIConferenceonArtificialIntelli-
gence,volume36,pp.9623–9633,2022.
13ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
A.TheoreticalProofs
Inthissection,weconducttheproofsofthetheoreticalresultspresentedinthemainpaper.
ProbableImprovementsviaAggregation
TheoremA.1. Letϕω = (cid:80) ω ϕi betheaggregatedexplanationthenthequalityofϕω isbetterthanthetheweighted
i i
qualityoftheindividualattributionresults:
(cid:34) (cid:35)
(cid:88) (cid:88)
Q(ϕω)= ω Q(ϕi)−E ω ∥γ (ϕ −ϕω)∥2
i γ1 i 1 i 2
i i
Proof. Theproofissimilartoarelatedresultestablishedin(Krogh&Vedelsby,1994):
k k (cid:34) k (cid:35)
(cid:88) ω Q(ϕi)−Q(ϕω)=(cid:88) ω E(cid:2) ∥γ ϕi−γ ∥2(cid:3) −E(cid:2) ∥γ ϕω−γ ∥2(cid:3) =E (cid:88) ω ∥γ ϕi−γ ∥2 −∥γ ϕω−γ ∥2
i i 1 2 2 1 2 2 i 1 2 2 1 2 2
i=1 i=1 i=1
(cid:34) k (cid:35)
=E (cid:88) ω (cid:16) ϕiT γTγ ϕi−2γTγ ϕi+γTγ (cid:17) − ϕωTγTγ ϕω−2γTγ ϕω+γTγ
i 1 1 2 1 2 2 1 1 2 1 2 2
i=1
(cid:34) k (cid:35) (cid:34) k (cid:35)
=E (cid:88) ω (cid:16) ϕiT γTγ ϕi(cid:17) − ϕωTγTγ ϕω =E (cid:88) ω (cid:0) γ (ϕi−ϕω)(cid:1)T (cid:0) γ (ϕi−ϕω)(cid:1)
i 1 1 1 1 i 1 1
i=1 i=1
(cid:34) k (cid:35)
(cid:88)
=E ω ∥γ (ϕ −ϕω)∥2
i 1 i 2
i=1
GeneralizationBound Theproofofthegeneralizationboundleveragesthefollowingresultfrom(Shalev-Shwartz&
Ben-David,2014),whereweslightlyadaptedthenotationtobettermatchoursetup:
TheoremA.2(26.5.3in(Shalev-Shwartz&Ben-David,2014)). Let(X ×Y)beaprobabilityspaceandℓ:X ×Y →R
beaboundedlossfunctionwithℓ(x,y) ≤ L. Letfˆ= argmin 1 (cid:80)m ℓ(x ,y )beanempiricalestimatorforthe
f∈F m i=1 i i
minimumofL(f)=E[ℓ(f(X),Y)]. Also,forRademachervariablesε ∈{−1,1}theempiricalRademacherComplexity
i
ofafunctionsetF isdefinedby:
(cid:34) m (cid:35)
Rˆ (F)= 1 E sup(cid:88) ε f(x )
m m ε i i
f∈F
i=1
Then,itholdswithaprobabilityofatleast1−δ:
(cid:114)
2ln(8/δ)
L(fˆ)−minL(f)≤2Rˆ (ℓ◦F)+5L
f∈F m m
Ontopofthat,weneedthefollowingLemmas:
LemmaA.3. Supposethatforallx,y ∈ S ⊂ Rg wehave∥x−y∥ ≤ cforaconstantc. Then,thesquaredEuclidean
2
distancel(x,y)=∥x−y∥2isLipschitzcontinuousinitsfirstargumentwithLipschitzconstantL=2c.
2
Proof. Thestatementfollowsfromquadraticfactorizationandthereversetriangularinequality.
Forallx,x′,y ∈S itholds:
|l(x,y)−l(x′,y)|=|∥x−y∥2−∥x′−y∥2|
2 2
=|(∥x−y∥ +∥x′−y∥ )(∥x−y∥ −∥x′−y∥ )|
2 2 2 2
≤|(∥x−y∥ +∥x′−y∥ )|∥x−y−x′+y∥
2 2 2
≤2c∥x−x′∥
2
14ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
LemmaA.4. Fori=1,...,m,letx ∈Rk andε ∈{−1,1}beRademachervariables,soP(ε=±1)=1/2. Then:
i i
(cid:34) (cid:88)m (cid:35) √ (cid:112)
E ∥ ε x ∥ ≤ m max ∥x ∥ 2ln(2k)
ε i i ∞ i ∞
i=1,...,m
i=1
Proof. TheproofleveragesMassart’slemmaandisforinstanceconductedwithintheproofofLemma26.11in(Shalev-
Shwartz&Ben-David,2014)
NowweareequippedtoproofTheorem4.3:
Theorem A.5. Let Q be a generalized L2 metric with max ∥γ ∥ ≤ c and let Φ = (ϕ1,...,ϕk) be the matrix of
γ1 1 1 1
stackedattributionoutcomestobeaggregatedintoϕω =(cid:80)k ω ϕi. Supposethatmax ∥γ ϕi−γ ∥2 ≤c aswellas
i=1 i γ1,γ2 1 2 2 2
∥ϕi∥ ≤ 1foralli = 1,...,k. AlsoletΩbethesetoffeasibleweightsω andletωˆ beanaggregationweightestimate
∞
obtainedfrommmetricevaluationsgivenby
m
ωˆ =argmin 1 (cid:88) ∥γ(j)ϕω−γ(j)∥2
ω∈Ω m 1 2 2
j=1
ThenthereexistaconstantC(c ,c )>0dependingonc andc suchthatwithprobabilityofatleast(1−δ):
1 2 1 2
(cid:114)
ln(16k/δ)
Q(ϕωˆ)−minQ(ϕω)≤C
ω∈Ω m
Proof. ThetheoremcanbeinterpretedasanextensionandadaptationofTheorem26.15in(Shalev-Shwartz&Ben-David,
2014)tothespecificsofoursetup.WedevelopappropriateboundsontheRademachercomplexityofvector-valuedfunctions
basedonaconcentrationresultfrom(Maurer,2016)andthespecificpropertiesofgeneralizedL2metricsoverconvex
combinationsofnormalizedfeatureattributionresults.
LetF ={f :Rg×k →Rg, f(A)=Aω |ω ∈Ω},thenwithTheoremA2aboveweimmediatelyget
(cid:114)
2ln(8/δ)
Q(ϕωˆ)−minQ(ϕω)≤2Rˆ (ℓ◦F)+5c (1)
ω∈Ω m 2 m
wherelisthesquaredEuclideandistance. Toeasethenotationinthefollowing,defineA(i) :=γ(i)Φandthej-throwof
1
A(i)asA(i) ∈Rk. Usingtheassumptionthatmax ∥γ ϕi−γ ∥2 ≤c ,weknowfromLemmaA.3thatlisLipschitz
j: √ γ1,γ2 1 2 2 2
continuouswithconstantL=2 c . Therefore,weareabletoleverageacorrespondingresultfrom(Maurer,2016)(⋆)to
2
upper-boundtheempiricalRademachercomplexityofthevector-valuedfunctionset. Moreprecisely,itholds:
mRˆ m(ℓ◦F)=E
ε(cid:34) sup(cid:88)m
ε i∥f(A(i))−γ 2(i)∥2
2(cid:35) ( ≤⋆)√
2LE
ε sup(cid:88)m (cid:88)g
ε i,jf
j(A(i))

f∈F f∈F
i=1 i=1j=1
=2√
2c 2E
ε sup(cid:88)m (cid:88)g
ε i,j(A(i)ω)
j =2√
2c 2E
ε sup(cid:42) (cid:88)m (cid:88)g
ε i,jA( ji
:),ω(cid:43)

ω∈Ω ω∈Ω
i=1j=1 i=1j=1
   
≤2√
2c 2E
εsup∥(cid:88)m (cid:88)g
ε i,jA j(i :)∥ ∞∥ω∥
1=2√
2c 2E
ε∥(cid:88)m (cid:88)g
ε i,jA j(i :)∥ ∞
ω∈Ω
i=1j=1 i=1j=1
whereε aswellasε areRademachervariablesandthelasttwostepsfollowfromtheHo¨lderinequalityaswellasthe
i i,j
constraintsonω. Next,noticethattheterm(cid:80)m (cid:80)g ε A(i)sumsoverallrowofAiacrossallsamples. Hencewecan
i=1 j=1 i,j j:
reindexthetermasasumoverallconsecutiverowsinthesampledenotedbyal ∈Rk withl=1,...,gm.
ApplyingLemmaA.4yields:
 m g  (cid:34) gm (cid:35)
E ε∥(cid:88)(cid:88) ε i,jA( ji :)∥ ∞=E
ε
∥(cid:88) ε lal∥
∞
≤√ gm max ∥al∥ ∞(cid:112) 2ln(2k)
l=1,...,gm
i=1j=1 l=1
15ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
Noticethatboundingmax ∥al∥ requirestoboundthemaximalentryofγ Φthatcouldbeencounteredwhile
l=1,...,gm ∞ 1
computingthemetric. Usingtheassumedconstraintsonγ andΦweobtain:
1
max ∥al∥ ≤max|(γ Φ) |≤max∥γ ∥ max∥ϕj∥ ≤c
∞ 1 i,j 1 1 ∞ 1
l=1,...,gm i,j γ1 j
Therefore,wefinallyhaveanupperboundontheempiricalRademachercomplexitygivenby:
(cid:114)
Rˆ (ℓ◦F)≤2c (cid:112) 2gc 2ln(2k) (2)
m 1 2 m
√
Combining(1)and(2)andsettingC :=max{4c 2gc ,5c }gives:
1 2 2
(cid:114) (cid:114) (cid:114)
(cid:112) 2ln(2k) 2ln(8/δ) 4ln(16k/δ)
Q(ϕωˆ)−minQ(ϕω)≤4c 2gc +5c ≤C
ω∈Ω 1 2 m 2 m m
√ √ (cid:112)
wherethelaststeputilizesthefactthat a+ b≤ 2(a+b)tomergethetwosquareroots.
B.AdditionalGeneralizedL2MetricsforotherDimensionsofExplanationQuality
Inthemainpaper,weshowedthatpopularmetricsforfeatureattributionsuchasInfidelityandAverage-Sensitivityare
generalizedL2metrics. Belowwealsoshowthatmetricsregardingotherqualitycriteriacanbeexpressedassuch.
AlignmentMetrics Alignmentmetrics,alsoreferredtoaslocalizationmetrics,measuretowhichextentanattribution
resultcorrespondstoanexpectedexplanationgroundedindomainknowledge. Forimageclassificationmodels,suchmetrics
typicallyquantifyhowwellimportantimageregionsoverlapwiththeactuallocationoftheclassifiedobjectintheimage. A
simplewaytoachievethisistodefineγ⋆asdesiredattributionresultsandmeasurealignmentviathesquaredEuclidean
2
distance:Q(ϕ(x))=∥ϕ(x)−γ⋆∥2.Anotherpossibilitythatcloselyresemblesthelogicoflocalizationmetricsforcomputer
2 2
visionmodelsistomeasureifimportantfeaturesliewithinaregionofinterest. LetI ⊂{1,...d}beanindexindicatingthe
positionofanobjecttobedetectedbyamodel. ThenQ(ϕ(x))=∥ϕ(x)−ϕ (x)∥2isageneralizedL2metricthatcaptures
I 2
howmuchattributionmassisallocatedtotheregionofinterest.
Randomization-basedsanitychecks Randomization-basedsanitycheckshavebeendevelopedtoverifythatanattribution
resultisnotabstractanddoesindeeddependsufficientlyonthemodelofinterest. Typicallytheyasseswhetherfeature
attributionschangeifcertainparametersofthemodelarerandomized. Ifanattributionresultisinvarianttoparameter
randomizationitmightnotbereliablyexplaintheexaminedmodel. ToexpressthisviaageneralizedL2metric,suppose
weareinterestedinexplainingthepredictionofamodelf withparametersθ. Letϕ (x)beafeatureattributionresult
θ θ
obtainedfromtheoriginalmodelf . Further,letf denotethecorrespondingmodelwhereallparametersoraspecific
θ θ˜
subsetisrandomizedbasedonθ˜∼P . Then,thevariabilityofϕ (x)underparameterrandomizationcanbecomputedvia
θ˜ θ
Q(ϕ (x))=−E ∥ϕ (x)−ϕ (x)∥2. Notethatweincorporateanegativesigntoindicatethatinvariantattributionresults
θ θ˜ θ θ˜ 2
correspondtolowerquality.
Complexity ToexpresscomplexitymeasuresforfeatureattributionsonecanusethetruncatedL2norm∥·∥ asasparsity
2,t
measure(Dicker,2014). ThisimpliesthatQ(ϕ(x))=∥min{ϕ(x),t}∥2wheremin{·,·}denotestheelementwiseminimum
2
operatorandtapredefinednoisethreshold. NotethatimprovingthismetricQrequirespushingmoreentriesofϕ(x)below
thethresholdtwhichalsopromotessparsityandreducescomplexity. Totranslatethismetrictothegenericformulation
proposedinDefinition4.1. oneneedstosetγ ∈Rd×dandγ ∈Rdlikethis:
1 2
(cid:40) (cid:40)
i=j :1if|ϕ |<telse0 −t if|ϕ |>t
(γ ) = i (γ ) = i
1 i,j 2 i
i̸=j :0 0 else
16ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
C.ExperimentalDetails
C.1.MetricDetails
Robustness ThroughoutallexperimentsAverage-Sensitivity(SENS )andandMax-Sensitivity(SENS )arecom-
AVG MAX
putedusinguniformlydistributedcorruptionsε∼U[−0.1,0.1]:
SENS :E [∥ϕ(x)−ϕ(x+ε)∥]2 and SENS :max∥ϕ(x)−ϕ(x+ε)∥2
AVG ε 2 MAX ε 2
TooptimizetheaggregationweightsforAGG andAGG theexpectationisestimatedusingonlym samplesforε.
robust opt agg
Duringtheevaluationinsection5.1,themetricsarecomputedusingm =200unseensamplestoexplicitlycheckfor
eval
generalization. DuringevaluationbothmetricsarecomputedusingtheimplementationprovidedbyQuantus(Hedstro¨m
etal.,2023a).
Faithfulness TocomputetheInfidelitymetric(INFD)werelyonoriginaldesignprinciplesproposedbytheauthors(Yeh
etal.,2019). Inparticular,weutilizedbinaryperturbationsI ∈{0,1}dthatrandomlyselectanimageareaof20%suchthat
ITϕequalsthesumofattributionscoresallocatedtotheselectedregion. Fromthisquantity,wesubtracttheprediction
changecausedbyreplacingtheselectedimageareawiththecorrespondingvaluesofablurredimageversionx . Thiscan
b
beformalizedusingamaph : Rd×Rd×Rd → Rd withh(x,x ,I) = (x ) if I = 1andh(x,x ,I) = x else. We
b i b i i b i i
alsoincorporatedthenormalizationutilizedbytheauthorsintheirimplementation(Yehetal.,2019). FortheFaithfulness
correlationmetric(FCOR)weusethesamekindofperturbationandusethePearsonCorrelationascorrelationmeasure
corrasproposedin(Bhattetal.,2021). Thisresultsin:
INFD:E (cid:2) (ITϕ(x)−(f(x)−f(h(x,x ,I)))2(cid:3) and FCOR:corr (cid:0) ITϕ(x),f(x)−f(h(x,x ,I))(cid:1)
I b I b
TooptimizetheaggregationweightsforAGG andAGG theexpectationandcorrelationisestimatedusingonlym
faith opt agg
samplesofI. Duringtheevaluationinsection5.1,themetricsareagaincomputedusingm = 200freshsamplesto
eval
explicitlycheckforgeneralization.
Stability AllstabilitymetricshavebeencomputedbasedontheirimplementationinOpenXAI(Agarwaletal.,2022b).
ForRelativeRepresentationStability(RRS)weusedtheactivationofthefinallayerbeforetheclassificationhappensas
underlyingrepresentationtocomputethemetric.
ROAD TocomputetheRemoveandDebiasmetric(Rongetal.,2022)weleveragedtheimplementationprovidedby
thepytorch-gradcamlibrary(Gildenblat,2021). Therefore,MoRF (Mostrelevantfirst)correspondstotheaverage
p
decreaseinconfidenceforthecorrectclassofanimageresultingfromremovingtheppercentofthemostimportantpixels
asindicatedbyanattributionresult. Note,thatfeatureremovalisperformedusingnoisylinearimputationwhichhasbeen
demonstratedtoproduceconsistentresultsmatchingtheoutcomesofretaining-basedmetricssuchasRemoveandRetrain
(Hookeretal.,2019).
C.2.FeatureAttributionMethodsandAggregations
IndividualMethods Duringtheexperiments,weevaluatedintotaltwelvedifferentfeatureattributiontechniques. The
methodsSaliency(Simonyanetal.,2013),InputxGrad(Shrikumaretal.,2016),GuidedBackpropagation(Springenberg
etal.,2014),DeepLift(Shrikumaretal.,2017),IntegratedGradients(Sundararajanetal.,2017),GradSHAP(Lundberg&
Lee,2017),SmoothGrad(Smilkovetal.,2017),VarGrad(Adebayoetal.,2018),ShapleyValues(Castroetal.,2009),LIME
(Ribeiroetal.,2016)andFeatureAblationarecomputedusingthecorrespondingimplementationprovidedbyCaptum
(Kokhlikyanetal.,2020). ForthemethodsGradCAM(Selvarajuetal.,2017),EigenCAM(Muhammad&Yeasin,2020)and
GradCAM++(Chattopadhayetal.,2018)weutilizedthepytorch-gradcam(Gildenblat,2021)library. Allattribution
resultsarenormalizedtoliewithintherange[0,1]bytakingtheabsolutevalueandrescalingthembasedonthemaximum
toensurecomparability.
NotethatweexcludedGuidedBackpropagationwhenevaluatingtheMLPMixerarchitecturesincethemethodwasoriginally
designedonlyfornetworkswiththeReLUactivationfunction.
Fortheexperimentsinsection5.3weutilizedLimewiththreedifferentLASSOregularizationparametersλ. Moreprecisely
highsparsityregularizationcorrespondstoλ =0.1,mediumtoλ =0.01andnoregularizationusesanordinary
high medium
leastsquareregressionapproachtoestimatetheLimecoefficients. FortheSLICvariantweprovidedafeaturemaskusing
theSLICalgorithm(Achantaetal.,2012)partitioninganimageintoapproximately100superpixels.
17ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
Optimized Aggregation The aggregation weights for our combination approaches are optimized by estimating the
(cid:92) (cid:91)
underlyingL2metricusingmmetricevaluationsamplesyieldingSENS andINFD. Inparticular,wehave:
AVG
m
S(cid:92) ENS (ϕω)= 1 (cid:88) ∥ϕω(x)−ϕω(x+ε(j))∥2
AVG m 2
j=1
m
I(cid:91) NFD(ϕω)= 1 (cid:88) ∥(cid:0) I(j)(cid:1)T ϕω(x)−(cid:0) f(x)−f(h(x,x ,I(j))(cid:1) ∥2
m b 2
j=1
andtheweightsarecomputedbysolving:
AGG : ωrobust =argmin S(cid:92) ENS (ϕω)
robust AVG
ω∈Ω
AGG : ωfaith =argmin I(cid:91) NFD(ϕω)
faith
ω∈Ω
AGG : ωopt =argmin I(cid:91) NFD(ϕω)+S(cid:92) ENS (ϕω)
opt AVG
ω∈Ω
Allobjectivesarereformulatedasconstrainedquadraticprogramsusingthelogicdescribedinsection4ofthemainpaperand
optimizedusingthedefaultsolverprovidedbycvxpy(Diamond&Boyd,2016). ForAGG weadditionallynormalized
opt
bothmetricsusingtheFrobeniusnormoftherespectiveparametermatrix∥ΓTΓ∥ toensurecomparabilitybetweenthetwo
F
consideredmetrics.
C.3.ModelDetails
Wedownloadedallconvolutionalmodels,includingVGG16(Simonyanetal.,2013),AlexNet(Krizhevskyetal.,2012),
ResNet18(Heetal.,2016),MobileNetV2(Sandleretal.,2018)andDenseNet121(Huangetal.,2017),fromtorchvison
withpre-trainedweights. Alltransformer-basedmodelsaredownloadedusingthetimmlibrary(Wightman,2019). More
precisely,weutilizedthefollowingmodelvariants:
DeiT(Touvronetal.,2021): deit tiny patch16 224.fb in1k
ViT(Dosovitskiyetal.,2021): vit tiny patch16 224.augreg in21k ft in1k
SwinT(Liuetal.,2021): swin tiny patch4 window7 224.ms in1k
MLPMixer(Tolstikhinetal.,2021): mixer b16 224.goog in21k ft in1k
D.AdditionalExperimentsandResults
D.1.ExtendedresultsforROAD
Table4.RemoveandDebiase(ROAD)metricresultsonaResnet18whereMoRF evaluatestheaveragedecreaseinconfidencecaused
p
byremovingthetopppercentofthemostrelevantpixelsasindicatedbytheexplanationmethod.
Method MoRF MoRF MoRF MoRF MoRF MoRF MoRF MoRF MoRF Average↓
10 20 30 40 50 60 70 80 90
Deeplift -1.10 -2.02 -3.02 -4.15 -5.42 -6.85 -8.56 -10.57 -13.03 -6.30
VarGrad -2.37 -4.69 -6.79 -8.45 -10.05 -11.35 -12.57 -13.63 -14.67 -9.84
GuidedBP -3.09 -4.86 -6.31 -7.62 -8.94 -10.17 -11.35 -12.59 -14.01 -9.77
IntGrad -0.85 -1.79 -2.82 -3.96 -5.31 -6.86 -8.67 -10.70 -13.13 -6.23
SmoothGrad -1.57 -2.72 -3.84 -4.94 -6.09 -7.35 -8.74 -10.38 -12.73 -6.82
InputxGrad -0.63 -1.34 -2.24 -3.37 -4.63 -6.18 -8.05 -10.23 -12.87 -5.72
Saliency -0.58 -1.24 -2.04 -3.10 -4.25 -5.65 -7.31 -9.33 -12.13 -5.18
AGG -2.21 -3.98 -5.68 -7.33 -8.82 -10.27 -11.60 -12.92 -14.31 -9.57
Mean
AGG -2.21 -3.98 -5.68 -7.33 -8.82 -10.28 -11.60 -12.93 -14.31 -9.57
Var
AGG -2.66 -4.50 -6.12 -7.49 -8.79 -9.99 -11.28 -12.57 -14.05 -9.50
faith
AGG -3.30 -5.41 -7.14 -8.62 -9.96 -11.25 -12.45 -13.51 -14.58 -10.25
opt
AGG -3.36 -5.48 -7.17 -8.78 -10.15 -11.40 -12.65 -13.70 -14.68 -10.37
robust
18ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
D.2.ResultsonotherDatasets
Tosubstantiatethefindingsinthemainpaper,werepeatedtheexperimentsinsection5.1onfouradditionaldatasets,namely
CIFAR10aswellasthreemedicalimagedatasetsBloodMNIST,DermaMNISTandPathMNIST(Yangetal.,2023). Tables
5and6summarizethecorrespondingresultsfortheconsideredfaithfulnessandrobustnessmetricsbasedon500images
evaluatedwithapre-trainedResNet18model.
Table5.INFDandFCORresultsfordifferentattributionmethodsandaggregationstrategiesforaResNet18model. Ourapproach
AGG consistentlyoutperformsallothertechniquesandAGG iseithersecondbestorcomparable.
faith opt
Feature CIFAR10 BloodMNIST DermaMNIST PathMNIST
Attribution INFD↓ FCOR↑ INFD↓ FCOR↑ INFD↓ FCOR↑ INFD↓ FCOR↑
Saliency 4.129 0.159 11.60 0.393 0.324 0.321 5.394 0.152
DeepLift 3.928 0.252 15.12 0.278 0.354 0.233 5.294 0.159
IntGrad 4.016 0.229 12.97 0.290 0.321 0.309 5.145 0.190
InputxGrad 4.326 0.133 13.56 0.206 0.322 0.320 5.466 0.118
SmoothGrad 3.736 0.313 13.04 0.326 0.360 0.276 5.395 0.127
VarGrad 3.607 0.319 11.84 0.379 0.380 0.145 6.106 0.103
AGG 3.802 0.290 12.71 0.390 0.332 0.362 5.153 0.183
Mean
AGG 3.817 0.290 12.73 0.391 0.335 0.361 5.132 0.186
Var
AGG (ours) 3.538 0.343 11.80 0.414 0.322 0.370 5.135 0.187
opt
AGG (ours) 3.342 0.378 10.66 0.465 0.281 0.433 4.850 0.286
faith
Table6.SENS (S )andSENS (S )resultsforgradient-basedattributionmethodsanddifferentaggregationstrategiesfora
AVG AVG MAX MAX
ResNet18model.OurapproachAGG consistentlyoutperformsallothertechniquesfollowedbyAGG assecondbest.
robust opt
Feature CIFAR10 BloodMNIST DermaMNIST PathMNIST
Attribution S ↓ S ↓ S ↓ S ↓ S ↓ S ↓ S ↓ S ↓
AVG MAX AVG MAX AVG MAX AVG MAX
Saliency 0.916 1.143 0.696 0.882 0.787 0.993 0.942 1.108
DeepLift 0.805 1.016 0.514 0.644 0.679 0.936 0.723 0.818
IntGrad 0.820 1.029 0.481 0.622 0.673 0.861 0.833 0.955
InputxGrad 0.910 1.152 0.708 0.893 0.795 1.000 0.932 1.076
SmoothGrad 0.818 0.978 0.549 0.683 0.566 0.694 0.828 1.000
VarGrad 0.617 0.953 0.449 0.677 0.390 0.606 0.583 0.961
AGG 0.553 0.699 0.384 0.526 0.475 0.615 0.537 0.655
Mean
AGG 0.549 0.689 0.386 0.526 0.474 0.609 0.536 0.653
Var
AGG (ours) 0.492 0.657 0.343 0.485 0.411 0.567 0.457 0.619
opt
AGG (ours) 0.491 0.650 0.339 0.476 0.389 0.547 0.439 0.627
robust
D.3.Computationtimesofdifferentaggregationstrategies
InTable7wereportthetimerequiredtoretrieveoptimalaggregationweightsacross7explainersfordifferentmodels
evaluatedonanNVIDIARTXA5000GPUandaveragedover100sampleswithcorrespondingstandarddeviations:
Table7.InferencetimestoperformweightoptimizationbasedonsevenexplanationmethodsforAGG andAGG asaverageover
faith robust
100sampleswithcorrespondingstandarddeviation.
Time(s) VGG16 ResNet18 MobileNetV2 DenseNet121 DeiT SwinT
AGG 0.79±0.06 0.75±0.06 0.77±0.06 1.49±0.51 1.33±0.92 0.83±0.09
faith
AGG 22.57±0.13 7.54±0.25 13.06±0.35 37.22±4.01 17.96±1.78 33.34±1.17
robust
AGG 23.39±0.14 8.31±0.23 13.91±0.27 38.52±3.28 19.39±4.59 34.47±1.13
opt
19ProvablyBetterExplanationswithOptimizedAggregationofFeatureAttributions
We believe the additional computational cost imposed by our aggregation technique is minor compared to the strong
improvementsinexplanationmetrics.
D.4.AblationStudiesregardingnumberanddiversityofcombinedexplanations
Varying the number of methods to be aggregated We anticipate that our method will benefit from an increasing
numberofconsideredattributionsbyautomaticallydown-weightingdisadvantageousexplanations. Thisbehaviourisalso
exemplifiedinthelasttworowsofFigure2wheredeterioratedresultsreceivedzeroweight. Tofurtherinvestigatethis,we
performedadedicatedexperimentinwhichweincreasedthenumberoffeatureattributionmethodsincrementallyfrom2to
7onaResNet18over100samples. TheresultsinTable8and9showthatthemetricsdoindeedgetbetterforrobustness
andfaithfulness,buttheimprovementsseemtosaturateatacertainpoint. Theorderedsetofexplainersthatwereusedfor
thisexperimentis: DeepLift,VarGrad,GuidedBackprop,SmoothGrad,IntGrad,InputxGrad,Saliency.
Table8.Robustness metrics for AGG combining an in- Table9.Faithfulness metrics for AGG combining an in-
robust faith
creasingnumberofexplanationmethods creasingnumberexplanationmethods
AGG 2 3 4 5 6 7 AGG 2 3 4 5 6 7
robust faith
S ↓ 0.52 0.43 0.42 0.41 0.41 0.41 INFD↓ 2.69 2.46 2.44 2.43 2.43 2.43
AVG
S ↓ 0.68 0.54 0.52 0.52 0.51 0.50 FCOR↑ 0.46 0.49 0.50 0.51 0.51 0.50
MAX
Combining different types of explanation techniques Concerning the diversity of explainers to use, we argue that
ourapproachcanbeappliedtofruitfullycombineallmethodsthatoutputfairlycomparableexplanations. Thisincludes
gradient-basedandperturbation-basedonesandweexpectthatalsoahigherdiversitywillbeadvantageous. Tofurther
investigate this, we evaluated our method again on 100 samples with a ResNet18 using 3 gradient-based methods, 3
perturbation-basedmethods,andacombinationofall6. TheresultsinTables10and11indicatethatincludingbothtypes
doesalsobenefitouraggregationapproach. Forinstance,theperturbation-basedmethodsseemtobesignificantlymore
robustthanthegradient-basedonesbutbycombiningthem,wecanevenfurtherimprovetheirrobustness. Wecanalso
enhancefaithfulnessthisway. Thegradient-basedmethodsusedinthisexperimentareDeepLift,SmoothGrad,InputxGrad
andtheperturbation-basedmethodsusedinthisexperimentareLime,ShapleyValuesandFeatureAblation.
AllmethodshavebeencomputedbasedontheirdefaultimplementationinCaptum(Kokhlikyanetal.,2020)whilewe
additionallyusedafeaturemaskof16x16patchesforallperturbation-basedmethods.
Table10.RobustnessmetricsforAGG combiningdiffer- Table11.FaithfulnessmetricsforAGG combiningdiffer-
robust faith
enttypesofexplanationmethods enttypesofexplanationmethods
AGG Grad.-based Pert.-based Both AGG Grad.-based Pert.-based Both
robust faith
S ↓ 0.65 0.49 0.46 INFD↓ 2.82 2.92 2.76
AVG
S ↓ 0.77 0.61 0.57 FCOR↑ 0.46 0.44 0.47
MAX
20