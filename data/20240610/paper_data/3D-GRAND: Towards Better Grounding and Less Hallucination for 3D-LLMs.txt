3D-GRAND: Towards Better Grounding and
Less Hallucination for 3D-LLMs
JianingYang∗∗♣ XuweiyiChen∗♣ NikhilMadaan† MadhavanIyengar♣
ShengyiQian♣♦ DavidF.Fouhey♦ JoyceChai♣
♣UniversityofMichigan ♦NewYorkUniversity
https://3d-grand.github.io/
3D-LLM 30.3
3D-GRAND 38.0
3D Rooms Grounding Accuracy
Stronger Grounding Capability
Q: Are there any A: Yes. Prev. SOTA 48.05
plants in this room?
Densely-grounded
3D-Text Pairs Q: Have you noticed any A: No. 3D-GRAND 6.67
refrigerators in the room?
3D-GRAND: Large, Densely 3D-POPE: Benchmark for Hallucination Rate
Grounded 3D-Text Dataset 3D-LLMHallucination Reduced Hallucination
Figure1:Weintroduce3D-GRAND,alarge-scale,denselygrounded3D-textdataset,and3D-POPE,a3D-LLM
hallucinationbenchmark.Trainingon3D-GRANDimprovesgroundingaccuracyandreduceshallucinations.
Abstract
Theintegrationoflanguageand3Dperceptioniscrucialfordevelopingembodied
agentsandrobotsthatcomprehendandinteractwiththephysicalworld. While
largelanguagemodels(LLMs)havedemonstratedimpressivelanguageunderstand-
ingandgenerationcapabilities,theiradaptationto3Denvironments(3D-LLMs)
remains in its early stages. A primary challenge is the absence of large-scale
datasetsthatprovidedensegroundingbetweenlanguageand3Dscenes. Inthis
paper, we introduce 3D-GRAND, a pioneering large-scale dataset comprising
40,087householdscenespairedwith6.2milliondensely-groundedscene-language
instructions. Our results show that instruction tuning with 3D-GRAND signifi-
cantlyenhancesgroundingcapabilitiesandreduceshallucinationsin3D-LLMs.
Aspartofourcontributions,weproposeacomprehensivebenchmark3D-POPE
tosystematicallyevaluatehallucinationin3D-LLMs,enablingfaircomparisons
amongfuturemodels. Ourexperimentshighlightascalingeffectbetweendataset
sizeand3D-LLMperformance,emphasizingthecriticalroleoflarge-scale3D-text
datasetsinadvancingembodiedAIresearch. Notably,ourresultsdemonstrateearly
signalsforeffectivesim-to-realtransfer,indicatingthatmodelstrainedonlarge
syntheticdatacanperformwellonreal-world3Dscans. Through3D-GRANDand
3D-POPE,weaimtoequiptheembodiedAIcommunitywithessentialresources
andinsights,settingthestageformorereliableandbetter-grounded3D-LLMs.
∗Equalcontribution.
†Independentresearcher.
Preprint.Underreview.
4202
nuJ
7
]VC.sc[
1v23150.6042:viXra1 Introduction
EmbodiedArtificialIntelligence(EAI)representsafrontierinroboticsandmachinelearning. InEAI,
theintegrationofperception,language,andactionwithinphysicalspacesiscrucialfordeveloping
intelligent systems capable of meaningfully navigating and interacting with their environments.
Centraltothisvisionistheconceptofgroundinglanguageinthephysicalworld[6,9]. Grounding
connectsabstractlinguisticconstructstoconcreteobjectsinthree-dimensionalspace,therebyenabling
robotsandintelligentagentstoeffectivelyunderstandandmanipulatetheirsurroundings.
RecentadvancementsinLargeLanguageModels(LLMs)havegreatlybenefitedEmbodiedArtificial
Intelligence(EAI).LLMsdemonstrateexceptionalcapabilitiesinunderstandinglanguageinstructions
[49,64],perceivingtheenvironment[44,39,3,81,71],andplanningdetailedactions[7,33]. The
primary inputs to LLMs, other than pure language, have been the combination of language and
2D images, categorizing these models as 2D-LLMs. The significant advancements in 2D-LLMs
can be largely attributed to their training on extensive vision-language datasets. These datasets
[58,82],comprisingbillionsofimageandtextpairs,havebeeninstrumentalinenhancingthemodels’
understanding of visual content and its contextual relevance to textual information. These large
datasetshaveprovidedthefoundationaldatanecessaryfortrainingmodelsthatexcelatintegrating
visualperceptionwithlanguageprocessing. DespitesomeprogressinequippingLLMstounderstand
3Dscenes(3D-LLMs)[27,29,66,30,83,11,53], thesemodelsremainintheirearlystagesdue
tothescarcityof3Dsceneandtextpairs. Inthiswork, weintroduce3D-GRAND,apioneering
million-scaledatasetdesignedfordensely-grounded3DInstructionTuning.
Recently,SceneVerse[35]concurrentlyintroducedalarge-scale3Dvision-languagedataset.However,
asignificantlimitationofthisdatasetistheabsenceofobjectgroundinginlanguage,whichiscrucial
forenhancingmodelusabilityinroboticstasksandreducinghallucination. Researchon2D-LLMs
indicatesthatgroundinglanguageto2Dcontextsnotablymitigateshallucinationinlanguagemodels
[74,51,5,38,54,78],therebyenhancingthereliabilityandinterpretabilityofgeneratedresponses.
While2Dgroundinghasbeenextensivelyexplored,extendingtheseprinciplesto3Denvironments
isstillunderdeveloped. Thissituationraisestwocriticalquestions: (1)Isthereanyhallucination
in 3D-LLMs and if so, how severe is it? (2) Can densely-grounded data mitigate hallucination
for3D-LLMs? Thesequestionsunderscoreacriticalneedwithintheresearchcommunityforthe
developmentofanevaluationbenchmarkspecificallydesignedfor3D-LLMsandtheconstructionof
alarge-scale,3D-groundeddataset.
Toquantifyhallucinationin3DLLMs,thisworkintroduces3D-POPE(3DPolling-basedObject
ProbingEvaluation). 3D-POPEprovidesacomprehensiveandstandardizedprotocolforevaluating
hallucinationthatenablessystematicassessmentandfacilitatesfaircomparisonsacross3D-LLMs,
enhancingourunderstandingofmodelcapabilitiesinobjecthallucination. Specifically, wepose
existencequestionsto3D-LLMsandevaluatetheirresponses,asillustratedinFig1.
Toevaluatetheroleofdensely-groundeddataset,weintroduceapioneeringmillion-scaledataset,3D-
GRAND,fordenselygrounded3Dinstructiontuning.3D-GRANDincludes40,087householdscenes
pairedwith6.2millionscene-languageinstructions,featuringdensephrase-to-objectgrounding. We
conduct rigorous human evaluations to ensure the dataset’s quality. Our results trained with 3D-
GRANDhighlightthedataset’seffectivenessinenhancinggroundingandreducinghallucinationfor
3D-LLMs. Wehighlighttheeffectivenessofincorporating3D-GRANDinFig1andintroduceeach
categoryof3D-GRANDandprovideexamplesinFig2.
Tosumup,ourcontributionsinclude:
• 3D-GRAND,thefirstmillion-scale,densely-grounded3D-textdatasetforgrounded3DInstruction
Tuning. 3D-GRANDincludes40Khouseholdscenespairedwith6.2Mdensely-groundedscene-
languageinstructions.
• 3D-POPE,asuiteofbenchmarksandmetricsthatsystematicallyevaluatehallucination,enabling
faircomparisonsoffuture3D-LLMmodelsintermsofobjecthallucination.
• Quantitativeresearchfindingsregardinghallucination,grounding,andscalingthatprovideguidance
tofutureresearch: (1). training3D-LLMswith3D-GRANDsignificantlyreduceshallucinations,
particularly when the data is densely grounded; (2). densely grounded instruction tuning sig-
nificantlyenhancesthegroundingcapabilitiesof3D-LLMs;(3). scalingdenselygroundeddata
consistentlyimprovesgroundingaccuracyandreduceshallucination;and(4). modelscansuccess-
fullytransferfromsim-to-real,providinganearlysignalforalow-costandsustainablefutureof
scalingsynthetic3Ddatatohelponrealtasks.
2Dataset Whichpartisgrounded? DenselyGrounded? Languagesource #3DScenes #Languagepairs
ReferIt3D[2] obj-refer ✗ Human,Template 0.7K 125K
ScanRefer[10] obj-refer ✗ Human 0.7K 51K
Scan2Cap[12] obj-refer ✗ Human 0.7K 51K
ScanEnts3D[1] obj-refer ✓ Human 0.7K 84K
PhraseRefer[76] obj-refer ✓ Human 0.7K 170K
ScanQA[4] answer ✗ Human 0.7K 41K
SQA3D[46] question ✗ Human 0.65K 33.4K
3DVQA[22] ✗ ✗ Template 0.7K 500K
CLEVR3D[70] ✗ ✗ Template 8.7K 171K
3DMV-VQA[26] ✗ ✗ Template 4.1K 55K
EmbodiedScan[65] ✗ ✗ Template 3.4K 970K
3DMIT[42] ✗ ✗ LLM 0.7K 75K
M3DBench[40] obj-refer,question ✗ LLM 0.7K 327K
3D-DenseOG[32] scene ✓ Human 0.7K 51K
3D-LLM[27] obj-refer ✗ LLM 0.9K 200K
LL3DA[11] question,answer question Template,LLM 0.9K 200K
Chat3D-v2[29] scene ✓ Human,LLM 0.7K 0.7K
3D-VisTA[83] question ✗ Template,LLM 3K 278K
LEO[30] question ✗ LLM 3K 579K
SceneVerse[35] obj-refer ✗ Template,LLM 62K 2.5M
3D-GRAND scene,obj-refer,question,answer ✓ Template,LLM 40K 6.2M
Table1:Comparisonof3D-GRANDwithexisting3Dscenedatasetswithlanguageannotations.3D-GRANDis
thelargestlanguage-groundeddataset.
2 RelatedWork
Injecting3DintoLLMs. Recentadvancementsinlargelanguagemodels(LLMs)haveinspired
researchintoextendingtheircapabilitiesto3Denvironments, leadingtothedevelopmentof3D-
LLMs[11,53,71,83]. Notableworksinthisfieldinclude3D-LLM[27],whichintegrates3Dpoint
cloudsandfeaturesintoLLMstoenabletaskssuchascaptioning,questionanswering,andnavigation.
LEO[30]excelsasanembodiedmulti-modalgeneralistagentinperception,grounding,reasoning,
planning,andactionin3Denvironments,highlightingthepotentialof3D-LLMsinunderstanding
andinteractingwiththephysicalworld. ThemostrelevantworktoourmodelisChat-3Dv2[66,29],
which grounds generated scene captions to objects in 3D scenes. However, Chat-3Dv2’s dataset
is limited to one type of 3D-text task (scene captioning) and only includes 705 captions from a
subsetofScanNetscenes. In3D-GRAND,weexpandthisconceptbydiversifying3D-texttasksand
increasingthedatasetsizetoamillion-scale. Ourresultsdemonstratepromisingdatascalingeffects
andsim-to-realtransfer,pavingthewayforfuturelarge-scaletrainingof3D-LLMs.
ObjectHallucinationofVLMs. While2DVLMshaveachievedimpressiveperformance,theyare
pronetohallucinatingobjectsthatdonotexistintheprovidedimages,aproblemknownasobject
hallucination[15,56].Severalmethodshavebeensuggestedtomitigatetheobjecthallucinationissue,
suchasintegratinganexternalobjectdetector[77],applyingvisuallygroundedvisualinstruction
tuning[74,78]orreinforcementlearning[61,24],performingiterativerefinement[80],andadapting
the decoding strategies [31]. To quantify and mitigate this issue, several benchmarks have been
proposed. CHAIR(CaptionHallucinationAssessmentwithImageRelevance)[56]measuresthe
frequencyofhallucinatedobjectsinimagecaptionsbycomparingtheobjectsmentionedtotheground
truthannotations. POPE(ProbingObjectHallucinationEvaluation)[41]assessesaVLM’sability
toidentifythepresenceorabsenceofobjectsthroughyes/noprobingquestions. However, these
studiesprimarilyfocuson2Dimage-textdatasetslikeCOCO[43]. Incontrast,objecthallucination
in3D-LLMsremainslargelyunexplored. Ourworkaddressesthisgapbyintroducing3D-POPE,
a comprehensive benchmark for evaluating object hallucination in 3D-LLMs. To the best of our
knowledge,thisisthefirstobjecthallucinationbenchmarkfor3D-LLMs.
Groundingdatasets. Inthe2Ddomain,large-scaledatasetswithgroundinginformationhavebeen
instrumentalinadvancingvision-languageresearch.NotableexamplesincludeRefCOCO[75],which
providesreferringexpressionsforobjectsinCOCOimages[43]. Additionally,2DLLMs[51,54,69,
38,74]havebeentrainedwithdensely-groundedweb-crawledimage-textpairs. Inthe3Ddomain,
thereisagrowinginterestincreatingdatasetsthatpair3Dsceneswithtextualannotations[76,1,
32,12]. ScanRefer[10]pioneeredthiseffortbycontributingadatasetofScanNet[14]sceneswith
referringexpressions. Table1introducestheeffortsmadebythecommunity. However,thesedatasets
havelimitedgroundingannotationsandoftenfocusonasingletask,suchasreferringexpression
comprehensionorvisualquestionanswering. Incontrast,ourproposeddataset,3D-GRAND,stands
outbyproviding6.2milliondensely-groundedscene-languageinstructionsacrossadiversesetof
3D-text tasks and 40,087 household scenes. This enables a wide range of grounding tasks and
facilitatesthedevelopmentofmorereliableandbetter-grounded3D-LLMs.
3Grounded QA: Existence Grounded QA: Spatial Relation
Q: Can you find any potted plants in the
room? obj_6 Q: What is the spatial relationship between
A: <ground> <p>potted plant</p>[<obj_7>] <p>neutral beige three-seat Grounded Scene
</ground> sofa</p>[<obj_0>] and <p>muted seafoam Description (118k)
G< roan us nw de er d> Y Qe As< :/ a Cn os uw ne tr in>. g obj_5 obj_7 obj_0 obj_4 A: The g <r pe >e tn h rt ev e s -t sa en ad t < s/ op f> a[ << /o pb >j [_ <1 o>] b? j_0>] is E Cx oi (s u5t ne 3 tn 2 inc k ge )
Q: What is the number of footstool present obj_2 in front of the tv stand [<obj_1>]. 3D-FRONT (190k)
in the room? (21.8k)
A:<groun <d < a> o n < b sp j w-> 4 ef >o r>]o 2<t <s /g /t ao r noo suls wn< d ep > r> > .[<obj-3>, obj_1 Grounded QA: Spatial Relation (y/n) Attribute (9 ( 0y 8/n k) )
Grounded QA: Attribute (y/n) obj_3 Q: The <p>floor lamp</p>[<obj_5>] is below Ground (e 5d .5 Q MA ) At (tr 1i .b 6u Mte )
Q: Is the color of the <p>corner the <p>footstool</p>[<obj_1>]. Is it correct?
cabinet</p>[<obj_6>] black?
A: No. The <p>floor lamp</p>[<obj_5>] is
A: No. The <p>cor in s e wr hc ia teb .inet</p>[<obj_6>] next to the <p>footstool</p>[<obj_1>]. Spatia (l 1( .y 6/ Mn) )
Grounded QA: Attribute Grounded Scene Description Grounded Object Reference Structured3D Spatial
(18.2k) (573k)
Q: What is the finish of the <p>three-seat The living room is a blend of Korean, Japanese, Light Luxury, This is <p>a three-seat sofa</p>[<obj_0>]
sofa</p>[<obj_1>]? and Modern styles, creating an eclectic and comfortable space. with a neutral beige color. The <p>neutral Grounded Object
At the center, the <p>three-seat sofa</p>[<obj_0>] offers beige sofa</p>[<obj_0>] is positioned in Reference (488k)
A: The finish of the <p>three-seat ample seating with its neutral beige color and soft texture. On front of the <p>tv stand</p>[<obj_1>] and is
sofa</p>[<obj_0>] is soft, textured fabric the opposite side, the <p>tv stand</p>[<obj_1>] in seafoam close to the <p>coffee table</p>[<obj_2>].
upholstery. green provides a stylish base for entertainment equipment...
Figure2:3D-GRANDdatasetandstatistics.(Left):3D-GRANDisalarge-scale,densely-grounded3D-text
datasetwith8differenttasks.(Right):From40K3Dscenes,3D-GRANDannotates6.2M3D-textpairs.
3 3D-GRAND:The3DGroundAnythingDataset
Inthissection,weintroduce3D-GRAND,alarge-scale,densely-grounded3D-textdatasetdesigned
forgrounded3Dinstructiontuning. Wedescribethedatacollectionprocess,datasetstatistics,and
theuniquefeaturesthatmake3D-GRANDavaluableresourceforadvancingresearchin3D-LLMs.
3Dscenecollection. Themajorityof3D-textresearchiscurrentlybasedonScanNetscenescollected
fromrealcamerascans,whicharelimitedinscale. However,recentadvancementshaveledtothe
developmentofnumeroussyntheticdatagenerationpipelines[48,17,20,18,37,52,62,47,73,25,
60,36,21]. Giventhescalabilityofthesesyntheticdatagenerationpipelines,weexplorethepotential
ofusingsynthetic3Dscenestoenhance3D-textunderstanding.
Syntheticdataofferssignificantadvantages,suchaslowercostsandgreateraccessibility,making
itanattractivealternative. Ifmodelstrainedonsimulated3D-textdatacaneffectivelytransferto
real-world3Dscenes,theresearchcommunitystandstobenefitimmensely.
Tothisend,wecurateadiversecollectionof40,087high-quality3Dindoorscenesfromthe3D-
FRONT[23]andStructured3D[79]datasets. Thesedatasetsarechosenfortheirlargequantities
ofsyntheticindoorsceneswithprofessionallydesignedlayouts. Thecollectionincludesavariety
of room types, such as living rooms, bedrooms, kitchens, office spaces, and conference rooms.
Wefurtherprocessthese3Dscenestogenerateper-room3Dpointclouds. Detailsonpointcloud
renderingandcleaningareprovidedintheAppendix.
Densely-grounded text annotation. The definition of densely-grounded text is that every noun
phraseofobjectmentionedinthetextshouldbeassociatedwithan3Dobjectinthe3Dscene. This
is illustrated in Figure 2. This is a difficult type of data to get annotations on. Early work such
asScanEnts3D[1]reliesonhiringprofessionalhumanannotatorstoobtainsuchannotations. The
authorsreportthatcrowd-sourcingannotators(AmazonMechanicalTurk(AMT)[13])werenotable
toreliablycompletethistaskandtheyhadtohireprofessionalannotators(errorrateAMT:16%,
professional: <5%). YetourhumanqualitycheckshowsthatLLMs(GPT-4[49])canachieve<8.2-
5.6%densely-groundingerrorrate(seeAppendixfordetail).Thisfindingisinaccordancewithrecent
studies[19,63]reportingLLMscanbehuman-levelannotators. TheaccuracyofLLM-annotation
providesonemotivationforconsideringLLMsasdenselygroundingannotationtool.
Thesecond, andperhapsmorecritical, motivationisthescalabilityofannotation. Whilewecan
potentiallyscaleup3Dscenesusingsyntheticdatagenerationpipelines, annotatingthesescenes
with human effort is both costly and time-consuming, especially for complex tasks like densely
groundingannotation. Toputthecostofmoneyandtimeinperspective,forthedataweannotated
inthispaper,weestimatethatobtainingthesameannotationswithhumanannotatorwouldcostat
least$539,000andrequire5.76years(noeat,nosleep)worthofworkfromaprofessionalannotator
(earningminimumwageof$10.67perhour). Incontrast,usingLLMs(GPT4[49]),weachievethe
sameresultsfor$3,030within2days,representinga178xreductionincostanda1051xreductionin
time. Atthetimeofwriting,thecostandtimefurtherdecreasesby50%to$1,500and1day,withthe
introductionofGPT-4o[50].
4There is a <lamp-6> that
offers a comfortable
Sofa light.
Grey
Cubic
Fabric There is a <tv_1> sitting
on the <tv_stand_0>.
❌
Lamp
White
Rounded
Glass
Hallucination
Drawer Filter
Brown
Rectangle
Wooden There is a <lamp-6> that
offers a comfortable
light.
ReT WcV B t o r as o ot nw da g en n u nd lar JSON The or ne ti hs e a < t< vt _v s_ t1 a> n ds _i 0t >t .ing o< fp f> el ra sm p a< /T cph o>e m[r f<e ol ri a ts m a p ba - l 6 e> ] l it gh ha tt .
ListofAttributes→ SceneGraph→ Generated Annotations →
3D→2D 2D→Attributes
SceneGraph Generated Annotations Processed Annotations
Figure3:3D-GRANDDataCurationPipeline.
Aspreviouslydiscussed,usinghumanstoannotate3Dscenescanbeanexhaustiveprocess. Mean-
while,2D-LLMsdemonstrateremarkablecapabilitiesinunderstandingvisualinputsandgenerating
language,makingthemwell-suitedforcreatinghigh-quality,groundedlanguageannotations. How-
ever,duetothehallucinationissuesanddataissuesin2D-LLMs,aggregatinginformationacross
images,eventhoseoriginatingfromthesamescene,isnotfeasibleyet.
Incontrast,LargeLanguageModels(LLMs)excelatunderstandingstructuraldataandgenerating
diverseandfluentlanguage[49].Theyhavedemonstratedcapabilitiesinspatialreasoning[8],solving
bothelementaryandsophisticatedmathproblems[68,34]. Toaddressthelimitationsof2D-LLMs
when annotate 3D scenes, we leverage the strengths of LLMs. By integrating detailed, accurate
informationintoareliablescenegraph,weprovideLLMswiththenecessarydatatoreasoneffectively
andgeneratepreciseannotations.
Here are the key steps of applying our pipeline to obtain densely-grounded annotation for any
synthetic3Dscene:
•3DModelto2DImage. Inthe3D-Frontdataset,eachobjectissourcedfrom3D-Future[23],which
providesagroundtruth2Dimageforeachobject. FortheStructured3Ddataset,individualimages
foreachobjectarenotavailable. Therefore,weutilizetheset-of-markpromptingtechnique[72],
whereeachobjecttobeannotatediscircledinredintheimages.
•2DImagetoAttributes. WeuseGPT-4Vtogeneratedetailedlanguageannotationsforeach2D
objectimage, includingattributeslikename, color, finish, andtexture. Thenamingisnowopen-
vocabulary,contrarytobeingclass-agnostic.
•ListofAttributestoSceneGraph. Westructureeachindividualobjects’annotationsintoaJSON-
basedscenegraphthatcapturestherelationshipsandattributesofobjectswithinthescene. Notethat
weobtainthisscenegraphfromsyntheticdatawhichwecanguaranteethecorrectness.
•SceneGraphtoGeneratedAnnotations. Basedonthegivenscenegraph,wewillbeabletoproduce
3D-GroundedObjectReference,3D-GroundedSceneDescriptionand3D-GroundedQAusingGPT-4
[49]andGPT4-Turbowithvariousprompts,whichwewillshowintheappendix.
•GeneratedAnnotationstoProcessedAnnotations. Afterweacquirerawannotations,wewillapply
hallucinationfiltersandtemplateaugmentationforthephrasetagtoremovelowqualityannotations
andaugmentgeneratedannotations.
We want to reinforce the set of grounded language tasks that cover a diverse range of language
understandingandgenerationscenariosin3DenvironmentswhichweshowinFigure2:
•3D-GroundedObjectReference: Givena3Dsceneandanobjectofinterest,3D-LLMisrequiredto
generateadescriptionthatuniquelyidentifiesthetargetobject. Thedescriptionincludestextand
groundinginformation,notonlyforthetargetobjectbutalsoforanylandmarkobjectsmentioned
in the description. This task is conceptually similar to Visual Grounding, Scene-aware Object
Captioning,andDenseCaptioningin2Dvision-languageresearch.
•3D-GroundedSceneDescription: Givena3Dscene, the3D-LLMgeneratesadescriptionthat
capturesthesalientaspectsoftheenvironment. Thedescriptionincludesbothtextandgrounding
information,linkingthelanguagetospecificobjectsorregionsinthescene.
•3D-GroundedQA:Givena3Dsceneandaquestionabouttheenvironment,the3D-LLMgenerates
ananswerthatisgroundedinthescene. Boththequestionandanswerincludetextandgrounding
5information,ensuringthatthe3D-LLM’sresponsesarecontextuallyrelevantandaccurate.
Datasethighlights. 3D-GRANDpossessesseveraluniquefeaturesthatdistinguishitfromexisting
3D-language datasets: (1). Large-scale: With 40,087 scenes and 6.2 million annotations, 3D-
GRANDisthelargest3D-languagedatasettodate,providingampledatafortrainingandevaluating
3D-LLMs. (2). Densegrounding: Unlikerecentmillion-scaledatasetslikeSceneVerse,whichlack
groundedlanguageannotations,eachlanguageannotationin3D-GRANDisdenselygroundedto
specificobjectsorregionswithinthe3Dscenes,facilitatingfine-grainedlanguageunderstanding
and generation. (3). Diverse language tasks: 3D-GRAND supports a broad array of grounded
languagetasks,includingobjectreference,spatialreasoning,andsceneunderstanding,makingita
comprehensivebenchmarkforevaluating3D-LLMs. (4). High-qualityannotations: Thelanguage
annotationsin3D-GRANDaremeticulouslycollected,filtered,andevaluatedtoensuretheyareof
highquality,diverse,andnatural.
Theseuniquefeaturesestablish3D-GRANDasavaluableresourceforadvancingresearchin3D-
LLMsandembodiedAI.Byprovidingalarge-scale,densely-grounded3D-textdataset,3D-GRAND
enablesthedevelopmentandevaluationofmorecapableandreliable3D-LLMsthatcaneffectively
understandandinteractwiththephysicalworld.
4 3D-POPE:ABenchmarkforEvaluatingHallucinationin3D-LLMs
Tosystematicallyevaluatethehallucinationbehaviorof3D-LLMs,weintroducethe3DPolling-based
ObjectProbingEvaluation(3D-POPE)benchmark. 3D-POPEisdesignedtoassessamodel’sability
toaccuratelyidentifythepresenceorabsenceofobjectsinagiven3Dscene.
Dataset. Tofacilitatethe3D-POPEbenchmark,wecurateadedicateddatasetfromtheScanNet
[14]dataset,utilizingthesemanticclassesfromScanNet200[57]. Specifically,weusetheScanNet
validationsetasthefoundationforevaluating3D-LLMsonthe3D-POPEbenchmark.
Benchmarkdesign. 3D-POPEconsistsof asetoftriples, eachcomprisinga 3Dscene, a posed
question,andabinaryanswer(“Yes”or“No”)indicatingthepresenceorabsenceofanobject(Fig.1
middle). Toensureabalanceddataset,wemaintaina1:1ratioofexistenttononexistentobjectswhen
constructingthesetriples. Fortheselectionofnegativesamples(i.e.,nonexistentobjects),weemploy
threedistinctsamplingstrategies:
• RandomSampling: Nonexistentobjectsarerandomlyselectedfromthesetofobjectsnotpresentin
the3Dscene.
• PopularSampling: Weselectthetop-kmostfrequentobjectsnotpresentinthe3Dscene,wherek
equalsthenumberofobjectscurrentlyinthescene.
• AdversarialSampling: Foreachpositivelyidentifiedobjectinthescene,werankobjectsthatare
notpresentandhavenotbeenusedasnegativesamplesbasedontheirfrequencyofco-occurrence
withthepositiveobjectinthetrainingdataset. Thehighest-rankingco-occurringobjectisthen
selectedastheadversarialsample. ThisapproachdiffersfromtheoriginalPOPE[41]toavoid
adversarialsamplesmirroringpopularsamples,asindoorscenesoftencontainsimilarobjects.
Thesesamplingstrategiesaredesignedtochallengethemodel’srobustnessandassessitssusceptibility
todifferentlevelsofobjecthallucination.
Metrics. Toevaluatethemodel’sperformanceonthe3D-POPEbenchmark, weusekeymetrics
includingPrecision,Recall,F1Score,Accuracy,andYes(%).PrecisionandRecallassessthemodel’s
abilitytocorrectlyaffirmthepresenceofobjectsandidentifytheabsenceofobjects,respectively.
Precisionisparticularlyimportantasitindicatestheproportionofnon-existingobjectsgeneratedby
the3D-LLMs. TheF1Score,combiningPrecisionandRecall,offersabalancedviewofperformance
andservesastheprimaryevaluationmetric. Accuracymeasurestheproportionofcorrectlyanswered
questions,encompassingboth“Yes”and“No”responses. Additionally,theYes(%)metricreportsthe
ratioofincorrect“Yes”responsestounderstandthemodel’stendenciesregardingobjecthallucination.
Leaderboard. Weestablishapublicleaderboardforthe3D-POPEbenchmark,allowingresearchers
tosubmittheir3D-LLMresultsandcomparetheirperformanceagainstotherstate-of-the-artmodels.
Theleaderboardreportstheevaluationmetricsforeachmodelunderthethreesamplingstrategies,
providingatransparentandstandardizedwaytoassessthehallucinationperformanceof3D-LLMs.
6Dataset 3D-POPE Model Precision Recall F1Score Accuracy Yes(%)
RandomBaseline 50.00 50.00 50.00 50.00 50.00
3D-LLM[27] 50.03 99.88 66.67 50.07 99.81
Random 3D-VisTA[83] 50.12 53.58 51.79 49.66 53.95
LEO[30] 51.95 77.65 62.25 52.91 74.73
Ourszero-shot(Grounding) 93.34 84.25 88.56 89.12 45.13
RandomBaseline 50.00 50.00 50.00 50.00 50.00
3D-LLM[27] 49.97 99.88 66.61 49.94 99.94
ScanNet200Val
Popular 3D-VisTA[83] 47.40 51.88 49.54 49.49 52.30
LEO[30] 48.30 77.65 59.55 47.27 80.38
Ourszero-shot(Grounding) 73.05 84.28 78.26 76.59 57.69
RandomBaseline 50.00 50.00 50.00 50.00 50.00
3D-LLM[27] 49.97 99.88 66.61 49.94 99.94
Adversarial 3D-VisTA[83] 48.28 54.39 51.15 51.14 52.99
LEO[30] 48.47 77.98 59.78 47.52 80.45
Ourszero-shot(Grounding) 69.86 84.21 76.37 73.95 60.26
Table2:3D-POPEbenchmarkresultsforevaluatinghallucinationin3Dlanguagemodels.RandomBaseline
referstoamodelrandomlypredicting“yes”or“no”with50%chance,giventhe1:1positive/negativesample
ratiointhedataset.
Model Acc@0.25 Acc@0.5 3D-POPE Model Precision
3D-LLM(flamingo)[27] 21.2 - Random 3D-GRAND 93.34
Non-zero-shot 3D-LLM(BLIP2-opt)[27] 29.6 - w/ogroundingtokens 91.96
3D-LLM(BLIP2-flant5)[27] 30.3 - 3D-GRAND 73.05
Popular
w/ogroundingtokens 70.37
Zero-shot LLM-Grounder[71] 17.1 5.3
3D-GRAND 69.86
Sim-to-realzero-shot 3D-GRAND 38.0 27.4 Adversarial w/ogroundingtokens 67.48
Table3:ScanReferResultsforevaluatingvisualgroundingcapability Table4:Ablationon3D-POPE.With-
of3D-LLMs. 3D-GRANDachievesthebestzero-shotperformance outthegroundingtokens,3D-GRAND
among3D-LLMs,providingsignalsforsim-to-realtransfer. hallucinatesmore.
5 Experiments
Inthissection,wepresentourexperimentalsetup,includingthebaselines,datasets,andimplementa-
tiondetails. Wethenreporttheresultsofourapproach,denotedas3D-GRANDontheScanRefer
[10] and the 3D-POPE benchmark, demonstrating the effectiveness in improving grounding and
reducing hallucination. Finally, we conduct an ablation study to analyze the impact of different
componentsofourmodelandtrainingstrategy.
5.1 ExperimentalSetup
Baselines. Wecompareour3D-GRANDagainstthefollowingbaselines: 3D-LLM[27],LEO[30],
and 3D-Vista [83]. Each model, along with the specific checkpoint used to obtain the results, is
documentedintheappendix. OurproposedmodelisbasedonLlama-2[64]. Theinputisobject-
centriccontext,includingascenegraphwitheachobject’scategory,centroid(x,y,z),andextent
(width,height,depth),alongwiththetextinstructionanduserquery. Duringtraining,weutilized
ground-truthcentroidsandextents. Forinference,weusedboundingboxespredictedbyMask3D
[59]. Examplesofinput/outputanddetailsofthemodelcanbefoundinthesupplementarymaterial.
Datasets. Weevaluateourmodel3D-GRANDontwodatasets: 3D-POPEandScanRefer. 3D-POPE
is our newly introduced benchmark dataset for evaluating object hallucination in 3D-LLMs, as
describedinSection4. ForScanRefer,Weutilizedthevalidationsplitwhichcontains9,508natural
languagedescriptionsof2,068objectsin141ScanNet[14]scenes.
Metrics. For the ScanRefer benchmark, we use the official evaluation metrics, including Accu-
racy@0.25IoUandAccuracy@0.5IoU.Forthe3D-POPEbenchmark,wereportaccuracy,precision,
recall,F1score,and“Yes”rateunderthethreesamplingstrategiesdescribedinSection4.
ImplementationDetails. The3D-GRANDmodelisLoRA-finetuned[28]basedoffLlama-2. We
useDeepSpeedZeRO-2[55]andFlashAttention[16]tosaveGPUmemoryandspeeduptraining.
ThemodelistrainedinBF16precisionon12NVIDIAA40GPUswithacombinedbatchsizeof96
andalearningrateof2e-4. WeusetheAdamW[45]optimizerwithaweightdecayof0.01anda
cosinelearningratescheduler. Wetrainthemodefor10ksteps,whichtakesapproximately48hours.
7Table5: AblationStudyonGroundingAccuracy(%)onScanRefer: Trainingwithdensely-groundeddata
significantlyimprovesgroundingaccuracy,particularlywhenmultipledistractorobjectsofthesamecategory
arepresentintheroom.
Unique Multiple Overall
Method Det.
Acc@0.25 Acc@0.5 Acc@0.25 Acc@0.5 Acc@0.25 Acc@0.5
BestIoU(upperbound) Mask3D(Top100) 93.7 66.8 91.6 70.7 92.4 69.2
BestIoU(upperbound) Mask3D(Top40) 81.2 58.7 80.7 62.4 80.9 61.0
Non-groundedModel Mask3D(Top40) 51.8 33.1 21.3 17.9 34.2 24.3
GroundedModel(groundlater) Mask3D(Top40) 50.4 32.4 26.0 20.5 36.3 25.5
GroundedModel(groundfirst) Mask3D(Top40) 54.4 36.4 26.0 20.8 38.0 27.4
BestIoU(upperbound) GT 100.0 100.0 100.0 100.0 100.0 100.0
Non-groundedModel GT 90.8 90.8 26.0 26.0 53.4 53.4
GroundedModel GT 91.0 91.0 32.1 32.1 57.0 57.0
ScanRefer: Acc@0.25 ScanRefer: Acc@0.5 Hallucination Rate on 3D-POPE
38
27 26
36 25
26
24
34
25
23
32 ded na sta e_ lyty _gp re ounded 24 ded na sta e_ lyty _gp re ounded 22 Densely_grounded
not_grounded not_grounded Not_grounded
30 23 21
12.525.0 50.0 100.0 12.525.0 50.0 100.0 12.525.0 50.0 100.0
Percentage of Data Percentage of Data Percentage of Data
(a)Groundingcapability.Higherisbetter. (b)Hallucinationrate.Lowerisbetter.
Figure4:3D-GRANDdatascalinganalysisonzero-shot,sim-to-realgroundingcapability,andhallucination.
Groundingperformanceconsistentlyimprovesasdatascalesup. Modeltrainedwithdensely-groundeddata
exhibitsbettergroundingcapabilitycomparedtothattrainedwithout. Additionally,modelhallucinatesless
whenexposedtomoredatafrom3D-GRAND.Here,theHallucinationRateiscalculatedas(1−Precision)on
3D-POPE.
5.2 Resultson3D-POPE
Wefirstevaluatetheseapproacheson3D-POPEandreportresultsonTable2. Resultsshowthat
3D-LLM [27] almost always produces yes responses to any question. 3D-VisTA [83] performs
similarlytotherandombaseline. LEO[30]tendstoansweryesfrequently,butitsprecisionindicates
asimilarobjecthallucinationratetotherandombaseline. Inourevaluation,3D-GRANDachieved
exceptionalperformance, with93.34%precisionand89.12%accuracywhentestedwithrandom
sampling. However, our model struggles with the more difficult splits, Popular and Adversarial,
whichdemonstratestheeffectivenessandrigorousnessof3D-POPEasabenchmark. Moreover,we
emphasize that our model has never encountered ScanNet during training. More analysis on 3D
hallucinationcanbefoundinthesupplementarymaterial.
5.3 ResultsonScanRefer
WereportresultsonScanReferinTable4. Ourmodelsignificantlyoutperformsallthebaselines,
achievingstate-of-the-artzero-shotresultsonbothAcc@0.25andAcc@0.5. Notably,ourmodelsur-
passesthepreviousbest-performingmodel,3D-LLM,by7.7%onaccuracy@0.25IoU.Weemphasize
thatourmodel,unlike3D-LLM,hasneverseenScanNetscenesduringitstraining(zero-shot)andis
onlytrainedonsynthetics3Dsceneinsteadofrealscans. Therefore,thisresultsprovideapromising
earlysignalthatsim-to-realtransfercanbeachievedviaourdense-groundedlarge-scaledataset.
5.4 AblationStudy
Tobetterunderstandtheimpactofdifferentcomponentsofour3D-LLM,weconductanablation
studyontheScanReferand3D-POPEbenchmarks.
Groundingtokens. Weshowtheresultsofourmodelwithdifferenttypesofgroundingmethods
in Table 5. We also show results on 3D-POPE in Table 4. In general, the model has a worse
grounding performance and more hallucinations without grounding tokens. “Ground First” and
8
)%(
52.0@ccA
)%(
5.0@ccA
)%(
etaR
noitanicullaH“groundlater”refertowhetherthedensegrounding(groundingeverysingleobjectmentioned)of
theobjectreferencequeryhappensbeforeorafterthemodeloutputsthefinalanswerfortherefer
expression. Theformereffectivelyconstitutesachain-of-thoughtreasoningprocess[67],whichis
likelywhytheperformanceincreasescomparedtothelatter. SeeAppendixfordetails.
Mask3Dproposals. Finally,weshowtheupperboundofourapproachinTable5. Ourresultsare
basedonMask3Dproposals. DuetothecontextlengthofLLM,weonlyusetop-40proposals.
5.5 DataScalingandSim-to-RealTransfer
TheresultsarepresentedinFigure4. Ourmodelistrainedonsynthetic3Dscenesfrom3D-FRONT
andStructured3D[79,23],andevaluatedonreal-world3DscansfromScanNet[14]. Thegrounding
performanceconsistentlyimproves,andthehallucinationratedropsasthedensely-groundeddata
scalesup. Notably,ourmodeltrainedondenselygroundeddatascalesbetterthanthesamemodel
trained without such data. These findings pave the way for a future where we can scale 3D-text
understandingusingsyntheticscenesobtainedfromsimulation,whichismuchcheaperandmore
accessibletoobtain.
6 Conclusion
Inthispaper,weintroduced3D-GRAND,alarge-scale,densely-grounded3D-textdatasetdesigned
forgrounded3Dinstructiontuning,and3D-POPE,acomprehensivebenchmarkforevaluatingobject
hallucinationin3D-LLMs. Throughextensiveexperiments,wedemonstratedtheeffectivenessofour
3D-LLMinimprovinggroundingandreducinghallucination,achievingstate-of-the-artperformance
ontheScanReferand3D-POPEbenchmarks. Ourablationstudyandqualitativeanalysishighlighted
the importance of large-scale densely-grounded instruction tuning and the choice of pre-trained
languagemodelsindevelopinghigh-performing3D-LLMs. Webelievethatourcontributionswill
inspirefurtherresearchandinnovationinthisfield,ultimatelyleadingtothedevelopmentofmore
advancedandcapable3D-LLMsforawiderangeofapplications.
References
[1] A.Abdelreheem,K.Olszewski,H.-Y.Lee,P.Wonka,andP.Achlioptas. Scanents3d:Exploitingphrase-
to-3d-objectcorrespondencesforimprovedvisio-linguisticmodelsin3dscenes. InProceedingsofthe
IEEE/CVFWinterConferenceonApplicationsofComputerVision,pages3524–3534,2024.
[2] P.Achlioptas,A.Abdelreheem,F.Xia,M.Elhoseiny,andL.J.Guibas. ReferIt3D:Neurallistenersfor
fine-grained3dobjectidentificationinreal-worldscenes. In16thEuropeanConferenceonComputer
Vision(ECCV),2020.
[3] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican,
M. Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural
InformationProcessingSystems,35:23716–23736,2022.
[4] D.Azuma,T.Miyanishi,S.Kurita,andM.Kawanabe. Scanqa:3dquestionansweringforspatialscene
understanding. InproceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pages19129–19139,2022.
[5] J.Bai,S.Bai,S.Yang,S.Wang,S.Tan,P.Wang,J.Lin,C.Zhou,andJ.Zhou.Qwen-vl:Aversatilevision-
languagemodelforunderstanding,localization,textreading,andbeyond.arXivpreprintarXiv:2308.12966,
2023.
[6] Y.Bisk,A.Holtzman,J.Thomason,J.Andreas,Y.Bengio,J.Chai,M.Lapata,A.Lazaridou,J.May,
A.Nisnevich,N.Pinto,andJ.Turian. Experiencegroundslanguage,2020.
[7] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,X.Chen,K.Choromanski,T.Ding,D.Driess,A.Dubey,
C.Finn,etal. Rt-2: Vision-language-actionmodelstransferwebknowledgetoroboticcontrol. arXiv
preprintarXiv:2307.15818,2023.
[8] S. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y. T. Lee, Y. Li,
S.Lundberg,H.Nori,H.Palangi,M.T.Ribeiro,andY.Zhang. Sparksofartificialgeneralintelligence:
Earlyexperimentswithgpt-4,2023.
[9] K.R.Chandu,Y.Bisk,andA.W.Black. Grounding‘grounding’inNLP. InC.Zong,F.Xia,W.Li,and
R.Navigli,editors,FindingsoftheAssociationforComputationalLinguistics:ACL-IJCNLP2021,pages
4283–4305,Online,Aug.2021.AssociationforComputationalLinguistics.
9[10] D.Z.Chen,A.X.Chang,andM.Nießner. Scanrefer:3dobjectlocalizationinrgb-dscansusingnatural
language. 16thEuropeanConferenceonComputerVision(ECCV),2020.
[11] S.Chen,X.Chen,C.Zhang,M.Li,G.Yu,H.Fei,H.Zhu,J.Fan,andT.Chen. Ll3da:Visualinteractive
instructiontuningforomni-3dunderstanding,reasoning,andplanning. arXivpreprintarXiv:2311.18651,
2023.
[12] Z.Chen,A.Gholami,M.Nießner,andA.X.Chang. Scan2cap:Context-awaredensecaptioninginrgb-d
scans. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
3193–3203,2021.
[13] K. Crowston. Amazon mechanical turk: A research tool for organizations and information systems
scholars. InA.BhattacherjeeandB.Fitzgerald,editors,ShapingtheFutureofICTResearch.Methodsand
Approaches,pages210–221,Berlin,Heidelberg,2012.SpringerBerlinHeidelberg.
[14] A.Dai,A.X.Chang,M.Savva,M.Halber,T.Funkhouser,andM.Nießner. Scannet:Richly-annotated3d
reconstructionsofindoorscenes. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pages5828–5839,2017.
[15] W.Dai,Z.Liu,Z.Ji,D.Su,andP.Fung. Plausiblemaynotbefaithful:Probingobjecthallucinationin
vision-languagepre-training,2023.
[16] T.Dao. FlashAttention-2:Fasterattentionwithbetterparallelismandworkpartitioning. InInternational
ConferenceonLearningRepresentations(ICLR),2024.
[17] M.Deitke,W.Han,A.Herrasti,A.Kembhavi,E.Kolve,R.Mottaghi,J.Salvador,D.Schwenk,E.Vander-
Bilt,M.Wallingford,L.Weihs,M.Yatskar,andA.Farhadi. RoboTHOR:AnOpenSimulation-to-Real
EmbodiedAIPlatform. InCVPR,2020.
[18] M.Deitke,E.VanderBilt,A.Herrasti,L.Weihs,J.Salvador,K.Ehsani,W.Han,E.Kolve,A.Farhadi,
A.Kembhavi,andR.Mottaghi. ProcTHOR:Large-ScaleEmbodiedAIUsingProceduralGeneration. In
NeurIPS,2022. OutstandingPaperAward.
[19] B.Ding,C.Qin,L.Liu,Y.K.Chia,S.Joty,B.Li,andL.Bing. Isgpt-3agooddataannotator?,2023.
[20] K. Ehsani, W. Han, A. Herrasti, E. VanderBilt, L. Weihs, E. Kolve, A. Kembhavi, and R. Mottaghi.
ManipulaTHOR:AFrameworkforVisualObjectManipulation. InCVPR,2021.
[21] EpicGames. Unrealengine.
[22] Y.Etesam,L.Kochiev,andA.X.Chang. 3dvqa:Visualquestionansweringfor3denvironments. In2022
19thConferenceonRobotsandVision(CRV),pages233–240.IEEE,2022.
[23] H.Fu,B.Cai,L.Gao,L.-X.Zhang,J.Wang,C.Li,Q.Zeng,C.Sun,R.Jia,B.Zhao,etal. 3d-front:3d
furnishedroomswithlayoutsandsemantics. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pages10933–10942,2021.
[24] A.Gunjal,J.Yin,andE.Bas. Detectingandpreventinghallucinationsinlargevisionlanguagemodels. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume38,pages18135–18143,2024.
[25] L.Höllein,A.Cao,A.Owens,J.Johnson,andM.Nießner. Text2room:Extractingtextured3dmeshes
from2dtext-to-imagemodels. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages7909–7920,2023.
[26] Y.Hong, C.Lin, Y.Du, Z.Chen, J.B.Tenenbaum, andC.Gan. 3dconceptlearningandreasoning
frommulti-viewimages. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages9202–9212,2023.
[27] Y.Hong,H.Zhen,P.Chen,S.Zheng,Y.Du,Z.Chen,andC.Gan. 3d-llm: Injectingthe3dworldinto
largelanguagemodels. AdvancesinNeuralInformationProcessingSystems,36:20482–20494,2023.
[28] E.J.Hu,Y.Shen,P.Wallis,Z.Allen-Zhu,Y.Li,S.Wang,L.Wang,andW.Chen. LoRA:Low-rank
adaptationoflargelanguagemodels. InInternationalConferenceonLearningRepresentations,2022.
[29] H.Huang,Z.Wang,R.Huang,L.Liu,X.Cheng,Y.Zhao,T.Jin,andZ.Zhao. Chat-3dv2:Bridging3d
sceneandlargelanguagemodelswithobjectidentifiers. arXivpreprintarXiv:2312.08168,2023.
[30] J.Huang,S.Yong,X.Ma,X.Linghu,P.Li,Y.Wang,Q.Li,S.-C.Zhu,B.Jia,andS.Huang. Anembodied
generalistagentin3dworld. InICML,2024.
10[31] Q.Huang,X.Dong,P.Zhang,B.Wang,C.He,J.Wang,D.Lin,W.Zhang,andN.Yu. Opera:Alleviating
hallucinationinmulti-modallargelanguagemodelsviaover-trustpenaltyandretrospection-allocation. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,2024.
[32] W.Huang, D.Liu, andW.Hu. Denseobjectgroundingin3dscenes. Proceedingsofthe31stACM
InternationalConferenceonMultimedia,2023.
[33] W.Huang,C.Wang,R.Zhang,Y.Li,J.Wu,andL.Fei-Fei. Voxposer:Composable3dvaluemapsfor
roboticmanipulationwithlanguagemodels. arXivpreprintarXiv:2307.05973,2023.
[34] S.Imani,L.Du,andH.Shrivastava. Mathprompter:Mathematicalreasoningusinglargelanguagemodels,
2023.
[35] B.Jia,Y.Chen,H.Yu,Y.Wang,X.Niu,T.Liu,Q.Li,andS.Huang. Sceneverse: Scaling3dvision-
languagelearningforgroundedsceneunderstanding. arXivpreprintarXiv:2401.09340,2024.
[36] A.Juliani,V.-P.Berges,E.Teng,A.Cohen,J.Harper,C.Elion,C.Goy,Y.Gao,H.Henry,M.Mattar,and
D.Lange. Unity:Ageneralplatformforintelligentagents,2020.
[37] E.Kolve,R.Mottaghi,W.Han,E.VanderBilt,L.Weihs,A.Herrasti,D.Gordon,Y.Zhu,A.Gupta,and
A.Farhadi. AI2-THOR:AnInteractive3DEnvironmentforVisualAI. arXiv,2017.
[38] X.Lai,Z.Tian,Y.Chen,Y.Li,Y.Yuan,S.Liu,andJ.Jia.Lisa:Reasoningsegmentationvialargelanguage
model. arXivpreprintarXiv:2308.00692,2023.
[39] J.Li,D.Li,S.Savarese,andS.Hoi. Blip-2:Bootstrappinglanguage-imagepre-trainingwithfrozenimage
encodersandlargelanguagemodels.InInternationalconferenceonmachinelearning,pages19730–19742.
PMLR,2023.
[40] M.Li,X.Chen,C.Zhang,S.Chen,H.Zhu,F.Yin,G.Yu,andT.Chen. M3dbench:Let’sinstructlarge
modelswithmulti-modal3dprompts. arXivpreprintarXiv:2312.10763,2023.
[41] Y.Li,Y.Du,K.Zhou,J.Wang,W.X.Zhao,andJ.-R.Wen. Evaluatingobjecthallucinationinlarge
vision-languagemodels. arXivpreprintarXiv:2305.10355,2023.
[42] Z.Li,C.Zhang,X.Wang,R.Ren,Y.Xu,R.Ma,andX.Liu. 3dmit:3dmulti-modalinstructiontuningfor
sceneunderstanding. arXivpreprintarXiv:2401.03201,2024.
[43] T.-Y.Lin,M.Maire,S.Belongie,J.Hays,P.Perona,D.Ramanan,P.Dollár,andC.L.Zitnick. Microsoft
coco:Commonobjectsincontext. InComputerVision–ECCV2014:13thEuropeanConference,Zurich,
Switzerland,September6-12,2014,Proceedings,PartV13,pages740–755.Springer,2014.
[44] H.Liu,C.Li,Q.Wu,andY.J.Lee. Visualinstructiontuning. InNeurIPS,2023.
[45] I.LoshchilovandF.Hutter. Decoupledweightdecayregularization,2019.
[46] X.Ma,S.Yong,Z.Zheng,Q.Li,Y.Liang,S.-C.Zhu,andS.Huang. Sqa3d:Situatedquestionanswering
in3dscenes. InInternationalConferenceonLearningRepresentations,2023.
[47] ManolisSavva*,AbhishekKadian*,OleksandrMaksymets*,Y.Zhao,E.Wijmans,B.Jain,J.Straub,
J.Liu,V.Koltun,J.Malik,D.Parikh,andD.Batra. Habitat:APlatformforEmbodiedAIResearch. In
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision(ICCV),2019.
[48] M.Mittal,C.Yu,Q.Yu,J.Liu,N.Rudin,D.Hoeller,J.L.Yuan,R.Singh,Y.Guo,H.Mazhar,A.Mandlekar,
B.Babich,G.State,M.Hutter,andA.Garg. Orbit:Aunifiedsimulationframeworkforinteractiverobot
learningenvironments. IEEERoboticsandAutomationLetters,8(6):3740–3747,2023.
[49] OpenAI. Gpt-4technicalreport,2024.
[50] OpenAI. Hellogpt-4o,May2024.
[51] Z.Peng,W.Wang,L.Dong,Y.Hao,S.Huang,S.Ma,andF.Wei. Kosmos-2: Groundingmultimodal
largelanguagemodelstotheworld. arXivpreprintarXiv:2306.14824,2023.
[52] X.Puig,E.Undersander,A.Szot,M.D.Cote,R.Partsey,J.Yang,R.Desai,A.W.Clegg,M.Hlavac,
T.Min,T.Gervet,V.Vondrus,V.-P.Berges,J.Turner,O.Maksymets,Z.Kira,M.Kalakrishnan,J.Malik,
D.S.Chaplot,U.Jain,D.Batra,A.Rai,andR.Mottaghi. Habitat3.0:Aco-habitatforhumans,avatars
androbots,2023.
11[53] Z.Qi,Y.Fang,Z.Sun,X.Wu,T.Wu,J.Wang,D.Lin,andH.Zhao. Gpt4point:Aunifiedframeworkfor
point-languageunderstandingandgeneration,2023.
[54] H.Rasheed,M.Maaz,S.Shaji,A.Shaker,S.Khan,H.Cholakkal,R.M.Anwer,E.Xing,M.-H.Yang,and
F.S.Khan. Glamm:Pixelgroundinglargemultimodalmodel. arXivpreprintarXiv:2311.03356,2023.
[55] J.Rasley,S.Rajbhandari,O.Ruwase,andY.He. Deepspeed:Systemoptimizationsenabletrainingdeep
learningmodelswithover100billionparameters. Proceedingsofthe26thACMSIGKDDInternational
ConferenceonKnowledgeDiscovery&DataMining,2020.
[56] A. Rohrbach, L. A. Hendricks, K. Burns, T. Darrell, and K. Saenko. Object hallucination in image
captioning. In E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages4035–4045,Brussels,Belgium,
Oct.-Nov.2018.AssociationforComputationalLinguistics.
[57] D.Rozenberszki,O.Litany,andA.Dai. Language-groundedindoor3dsemanticsegmentationinthewild.
InProceedingsoftheEuropeanConferenceonComputerVision(ECCV),2022.
[58] C.Schuhmann,R.Beaumont,R.Vencu,C.Gordon,R.Wightman,M.Cherti,T.Coombes,A.Katta,
C.Mullis,M.Wortsman,P.Schramowski,S.Kundurthy,K.Crowson,L.Schmidt,R.Kaczmarczyk,and
J.Jitsev. Laion-5b:Anopenlarge-scaledatasetfortrainingnextgenerationimage-textmodels,2022.
[59] J.Schult,F.Engelmann,A.Hermans,O.Litany,S.Tang,andB.Leibe. Mask3d:Masktransformerfor
3dsemanticinstancesegmentation. In2023IEEEInternationalConferenceonRoboticsandAutomation
(ICRA),pages8216–8223.IEEE,2023.
[60] J.Schult,S.Tsai,L.Höllein,B.Wu,J.Wang,C.-Y.Ma,K.Li,X.Wang,F.Wimbauer,Z.He,P.Zhang,
B.Leibe,P.Vajda,andJ.Hou. Controlroom3d:Roomgenerationusingsemanticproxyrooms,2023.
[61] Z.Sun,S.Shen,S.Cao,H.Liu,C.Li,Y.Shen,C.Gan,L.-Y.Gui,Y.-X.Wang,Y.Yang,etal. Aligning
largemultimodalmodelswithfactuallyaugmentedrlhf. arXivpreprintarXiv:2309.14525,2023.
[62] A.Szot,A.Clegg,E.Undersander,E.Wijmans,Y.Zhao,J.Turner,N.Maestre,M.Mukadam,D.Chaplot,
O.Maksymets,A.Gokaslan,V.Vondrus,S.Dharur,F.Meier,W.Galuba,A.Chang,Z.Kira,V.Koltun,
J.Malik,M.Savva,andD.Batra. Habitat2.0: Traininghomeassistantstorearrangetheirhabitat. In
AdvancesinNeuralInformationProcessingSystems(NeurIPS),2021.
[63] Z.Tan,A.Beigi,S.Wang,R.Guo,A.Bhattacharjee,B.Jiang,M.Karami,J.Li,L.Cheng,andH.Liu.
Largelanguagemodelsfordataannotation:Asurvey,2024.
[64] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,P.Bhargava,
S.Bhosale,D.Bikel,L.Blecher,C.C.Ferrer,M.Chen,G.Cucurull,D.Esiobu,J.Fernandes,J.Fu,W.Fu,
B.Fuller,C.Gao,V.Goswami,N.Goyal,A.Hartshorn,S.Hosseini,R.Hou,H.Inan,M.Kardas,V.Kerkez,
M.Khabsa,I.Kloumann,A.Korenev,P.S.Koura,M.-A.Lachaux,T.Lavril,J.Lee,D.Liskovich,Y.Lu,
Y.Mao,X.Martinet,T.Mihaylov,P.Mishra,I.Molybog,Y.Nie,A.Poulton,J.Reizenstein,R.Rungta,
K.Saladi,A.Schelten,R.Silva,E.M.Smith,R.Subramanian,X.E.Tan,B.Tang,R.Taylor,A.Williams,
J.X.Kuan,P.Xu,Z.Yan,I.Zarov,Y.Zhang,A.Fan,M.Kambadur,S.Narang,A.Rodriguez,R.Stojnic,
S.Edunov,andT.Scialom. Llama2:Openfoundationandfine-tunedchatmodels,2023.
[65] T.Wang,X.Mao,C.Zhu,R.Xu,R.Lyu,P.Li,X.Chen,W.Zhang,K.Chen,T.Xue,etal. Embodiedscan:
Aholisticmulti-modal3dperceptionsuitetowardsembodiedai. arXivpreprintarXiv:2312.16170,2023.
[66] Z.Wang,H.Huang,Y.Zhao,Z.Zhang,andZ.Zhao. Chat-3d: Data-efficientlytuninglargelanguage
modelforuniversaldialogueof3dscenes. arXivpreprintarXiv:2308.08769,2023.
[67] J.Wei,X.Wang,D.Schuurmans,M.Bosma,F.Xia,E.Chi,Q.V.Le,D.Zhou,etal. Chain-of-thought
promptingelicitsreasoninginlargelanguagemodels. Advancesinneuralinformationprocessingsystems,
35:24824–24837,2022.
[68] Y.Wu,F.Jia,S.Zhang,H.Li,E.Zhu,Y.Wang,Y.T.Lee,R.Peng,Q.Wu,andC.Wang. Anempirical
studyonchallengingmathproblemsolvingwithgpt-4,2023.
[69] J.Xu,X.Zhou,S.Yan,X.Gu,A.Arnab,C.Sun,X.Wang,andC.Schmid. PixelAlignedLanguage
Models. arXivpreprintarXiv:2312.09237,2023.
[70] X.Yan,Z.Yuan,Y.Du,Y.Liao,Y.Guo,Z.Li,andS.Cui.Clevr3d:Compositionallanguageandelementary
visualreasoningforquestionansweringin3dreal-worldscenes. arXivpreprintarXiv:2112.11691,2021.
12[71] J.Yang,X.Chen,S.Qian,N.Madaan,M.Iyengar,D.F.Fouhey,andJ.Chai. Llm-grounder: Open-
vocabulary3dvisualgroundingwithlargelanguagemodelasanagent. InICRA,2024.
[72] J.Yang,H.Zhang,F.Li,X.Zou,C.Li,andJ.Gao. Set-of-markpromptingunleashesextraordinaryvisual
groundingingpt-4v,2023.
[73] Y.Yang,F.-Y.Sun,L.Weihs,E.VanderBilt,A.Herrasti,W.Han,J.Wu,N.Haber,R.Krishna,L.Liu,etal.
Holodeck:Languageguidedgenerationof3dembodiedaienvironments. InTheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR2024),volume30,pages20–25.IEEE/CVF,2024.
[74] H.You,H.Zhang,Z.Gan,X.Du,B.Zhang,Z.Wang,L.Cao,S.-F.Chang,andY.Yang. Ferret:Refer
andgroundanythinganywhereatanygranularity. InTheTwelfthInternationalConferenceonLearning
Representations,2023.
[75] L.Yu,P.Poirson,S.Yang,A.C.Berg,andT.L.Berg. Modelingcontextinreferringexpressions. In
ComputerVision–ECCV2016:14thEuropeanConference,Amsterdam,TheNetherlands,October11-14,
2016,Proceedings,PartII14,pages69–85.Springer,2016.
[76] Z.Yuan,X.Yan,Z.Li,X.Li,Y.Guo,S.Cui,andZ.Li. Towardexplainableandfine-grained3dgrounding
throughreferringtextualphrases. arXivpreprintarXiv:2207.01821,2022.
[77] B.Zhai,S.Yang,X.Zhao,C.Xu,S.Shen,D.Zhao,K.Keutzer,M.Li,T.Yan,andX.Fan. Halle-switch:
Rethinkingandcontrollingobjectexistencehallucinationsinlargevisionlanguagemodelsfordetailed
caption. arXivpreprintarXiv:2310.01779,2023.
[78] Y.Zhang,Z.Ma,X.Gao,S.Shakiah,Q.Gao,andJ.Chai. Groundhog:Groundinglargelanguagemodels
toholisticsegmentation. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,2024.
[79] J.Zheng,J.Zhang,J.Li,R.Tang,S.Gao,andZ.Zhou. Structured3d:Alargephoto-realisticdatasetfor
structured3dmodeling. InProceedingsofTheEuropeanConferenceonComputerVision(ECCV),2020.
[80] Y.Zhou,C.Cui,J.Yoon,L.Zhang,Z.Deng,C.Finn,M.Bansal,andH.Yao. Analyzingandmitigating
objecthallucinationinlargevision-languagemodels. InTheTwelfthInternationalConferenceonLearning
Representations,2024.
[81] D.Zhu,J.Chen,X.Shen,X.Li,andM.Elhoseiny. Minigpt-4:Enhancingvision-languageunderstanding
withadvancedlargelanguagemodels. arXivpreprintarXiv:2304.10592,2023.
[82] W.Zhu,J.Hessel,A.Awadalla,S.Y.Gadre,J.Dodge,A.Fang,Y.Yu,L.Schmidt,W.Y.Wang,and
Y.Choi. Multimodalc4:Anopen,billion-scalecorpusofimagesinterleavedwithtext,2023.
[83] Z.Zhu,X.Ma,Y.Chen,Z.Deng,S.Huang,andQ.Li. 3d-vista:Pre-trainedtransformerfor3dvisionand
textalignment. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages
2911–2921,2023.
13A ModelDetails
A.1 Modelinputandoutputdemonstration
In Figure 5, we show an example of 3D-GRAND model’s input and output on the Grounded
Object Reference task. Note how in the “Response”, we train the model to generate a
⟨detailed_grounding⟩pairoftagstodenselygroundeverysingleobjectmentionedinthereferex-
pressionaftergeneratingthegroundinganswerin⟨refer_expression_grounding⟩. The“ground
first”“groundlater”inTable5meanswhetherthe⟨detailed_grounding⟩tagshappenbeforeor
afterthe⟨refer_expression_grounding⟩tags. Figure5isanexampleof“groundlater”.
Figure5:3D-GRANDmodelinputandoutputonGroundedObjectReferencetask.
B 3D-GRANDDataCuration
B.1 PointCloudGenerationPipelinefor3D-Front
Here,wepresentanexpandedversionofSection3,focusingonthemethodologiesemployedinthe
collectionandcleaningof3Dscenes,specificallydetailingourprocessforderiving3Dpointclouds
fromexistingdatasets.
Figure6:PointCloudGenerationfor3D-Front.
1Inourworkflowwith3D-FRONT,layoutsandmeshesareinitiallyprocessedinBlendertoproduce
multi-viewimages. Theseimagesaresubsequentlyusedtoconstructcomprehensivepointclouds
forentirehouses. Bothpointcloudsandper-roommeshesareutilizedtogeneratescene-levelpoint
clouds. Weavoiddirectuseofroommeshesbecausetheylackcolorinformationinceilings,walls,
andfloors,necessitatingthefinaloutputtobeapointcloud.
ForStructure3D,whileper-scenemulti-viewimagesfacilitatedirectrenderingofper-scenepoint
clouds,wefrequentlyencounterissueswherepartsofadjacentscenesareinadvertentlyreconstructed
duetowindowtransparency. Toaddressthis,weemploythelayoutofeachscenetotrimextraneous
points,thusenhancingtheprecisionoftheresultingpointclouds.
C Addtional3DPoPEresults
C.1 3DPopeResultsonNYU40
Table6presentsevaluationresultsfor3DPOPEusingtheNYU40classset. NYU40includesasubset
oftheclassesfromScanNet200featuredinthemainresultstable. TheNYU40classsetconsolidates
manyfine-grainedclassesintoan“other”category,potentiallyreducingthechallengeofnegative
samplinginthePopularandAdversarialsettingscomparedtotheScanNet200scenario.
Dataset 3D-POPE Model Accuracy Precision Recall F1Score Yes(%)
3D-LLM 50.00 50.00 100.00 66.67 100.00
3D-VisTA 50.12 50.08 77.13 60.73 77.01
Random
LEO 54.03 52.70 78.52 63.07 74.50
Ourszero-shot(NoGrounding) 86.45 87.26 85.36 86.30 48.91
Ourszero-shot(Grounding) 85.68 88.22 82.34 85.18 46.67
3D-LLM 50.00 50.00 100.00 66.67 100.00
3D-VisTA 50.27 50.23 77.13 60.84 76.91
Popular
ScanNetVal(NYU40) LEO 48.86 49.28 77.44 60.23 78.58
Ourszero-shot(NoGrounding) 80.85 78.30 85.35 81.68 54.50
Ourszero-shot(Grounding) 81.69 81.32 82.28 81.80 50.59
3D-LLM 50.00 50.00 100.00 66.67 100.00
3D-VisTA 50.44 50.48 77.14 61.03 76.86
Adversarial
LEO 49.77 49.85 77.67 60.73 77.91
Ourszero-shot(NoGrounding) 81.47 78.98 85.78 82.24 54.31
Ourszero-shot(Grounding) 82.10 81.72 82.72 82.22 50.61
Table6:Resultsof3D-LLMsunderthreeevaluationsettingsof3D-POPEonthevalidationsetofScanNetusing
NYU40classset.Yesdenotestheproportionofanswering“Yes”tothegivenquestion.Thebestresultsineach
blockaredenotedinbold.
D HumanValidation
Because our dataset generation process involves GPT-4V, there is a potential for hallucinations.
We identify three types of possible hallucinations that could impact our dataset: the text might
inaccuratelydescribeanobject’sproperty,suchascolororsize(termedincorrectobjectattribute);
it might incorrectly depict the spatial relationship between two objects (termed incorrect spatial
relation); or it might describe an object that does not exist in the referenced scene at all (termed
incorrectobjectexistence). Additionally,inaccuraciesinourdatasetmayalsoarisefromincorrectly
groundingthewrongobject.
Tovalidateourdatasetagainstthesepotentialfailures,weplantoverifyasubsetofourdatathrough
crowdsourcingtoascertainthefrequencyofthesefailurecases.
D.1 Crowd-sourcing
Wecrowd-sourcethevalidationofannotationsusingHive,aplatformcommonlyusedforsourcing
annotationsforcomputervisiontasks. Theplatformcanbeaccessedathttps://thehive.ai/.
Weconceptualizeourdatasetvalidationasadataannotationproblem,employingscene-textpairs
asthedataunit. Annotatorsareinstructedtolabelthesepairsas“True”or“False”toindicatethe
presenceorabsenceofhallucinationsorinaccuracies. Additionally,a“CannotDecide”optionis
providedtoaccommodatecaseswherethesceneviewisunclear.
2Task IdM: fafd0h4a7vba8nb I-yde9..d.8>-P4r7odje5c-8ts7>4b3-D7 7G6RfAfcN4D6 5D1a0ta1set Validation…>Honeypot Tasks PreviouMsadhavanN …ext
Hide Text Tools
M
English Spanish Portuguese More
Pan Zoom In Zoom Out Fit
Categories
True: everything in the highlighted Type category names to filter
sentence is accurate
Not true: there is either a wrong spatial True
location, wrong object attribute, or the
object doesn't exist in the scene Not true
Cannot decide: only choose this if you
are really unsure whether the sentence is Cannot decide
true or not
Image Settings
Brightness
Reset
Contrast
Reset
Hotkeys
View Keyboard Shortcuts
Figure7:Exampleofadatasetvalidationtaskpresentedtocrowd-sourcingannotators:Itdisplaysascenefrom
fourdifferentanglesalongsidethesentencetobevalidated,whichishighlighted.Annotatorshavetheoptionsto
select"True,""NotTrue,"or"CannotDecide."Ontheleftsideofthescreen,theinstructionsarerepeatedfor
annotators’reference.Inthisexample,"True"isselected.
D.1.1 TaskGeneration
Hive only supports presenting static images to annotators, so we generate annotation tasks by
composingsnapshotsofascenewithcorrespondingtextannotations.Foreachtask,wetakesnapshots
fromfourdifferentanglesandpairthemwithacorrespondingannotation. Tomaintainsimplicityand
conciseness,werequirevalidationofjustonesentencepertask,providingsomesurroundingcontext
andhighlightingthetargetsentence. Forgroundingvalidation,thegroundedobjectisoutlinedinthe
scenewithaboundingbox,andthereferringphraseinthesentenceisemphasized. Anexampleof
suchatask,alongwiththeannotationinterface,isdepictedinFigure7. Figure8displaystwotext
validationtasksandtwogroundingvalidationtasksthatwerepresentedtoannotators.
D.1.2 Crowd-sourcingValidity
Validatingadatasetnecessitatesahighlevelofattentionfromannotators. Wecuratesetsofinstruc-
tions,qualifyingtasks,andhoneypottaskstoensurethattheannotationsobtainedfromcrowdsourcing
arereliable. ThecrowdsourcingprocessisillustratedinFigure9.
Beforepresentinganytaskstotheworkers,wepresentthemwithasetofinstructiontasksthatshow
anexampleannotation,thecorrectresponse(asdeterminedbyus),andthereasonwhythatresponse
iscorrect. Theyarepairedwithanincorrectexampleandanexplanationofwhyitisincorrectin
ordertoensureunbiasedannotations. Examplesofqualifyinginstructionsareshownin10. These
instructionsareintentionallybrief,aswefoundthroughtrial-and-errorthatlonger,paragraph-based
instructionswerelargelyignoredbyannotators.
Qualifyingtasksarepresentedtotheannotatorsbeforetheyareshownanyrealtasksinordertotrain
themtocompletetherealtaskwithahighaccuracy. Annotatorsarebothshownthecorrectanswer
andareasoningastowhyitiscorrectforeveryqualifier. Wesettheminimumqualifieraccuracy
to0.75toensurethatannotatorsmustachieveaminimumcompetencybeforeannotatingrealtasks.
Every dataset is given between 12 and 30 specially crafted qualifying tasks thatdemonstrate the
possibleinaccuraciesthatcouldappearinthedata. Thesequalifiersaredividedequallybetweentrue
andfalseexamplessoasnottobiasworkerstowardsanyoneanswer.
3(a)"True"exampleofatextvalidationtask. (b)"False"exampleofatextvalidationtask.
(c)"True"exampleofagroundingvalidationtask. (d)"Falseexampleofagroundingvalidationtask.
Figure8:Examplesoftaskspresentedtoannotatorsforvalidatingbothtextaccuracyandgroundingaccuracy.
Theinstructionisdisplayedatthetopofthetask,whilethecentershowcasesfourdifferentviewsofthescene
toensurecomprehensivecoverageofallrelevantareas. Atthebottom,theannotationispresentedwiththe
pertinentsectionhighlighted.
Instruction Sets Qualifier Tasks Real Tasks
Banned Honeypots
Figure9:Illustrationofthecrowd-sourcingprocess.Annotatorsarefirstshowninstructionsetsthatdescribe
bothhowthetaskshouldbecompletedandthepossibleinaccuraciesthatcouldappearinthedata.Theyarethen
presentedwithqualifiertasks,andannotatorswhodonotgetahighenoughaccuracyonthesetasksarebanned
fromannotatingourdataset.Annotatorswhopassthequalifierareabletoannotaterealtasks,butarerandomly
presentedwithhoneypotsthatareindistinguishablefromrealtasks.Annotatorswhodonotgetahighenough
accuracyonhoneypotsarealsobannedfromourdataset.
4TRUE TRUE
NOT TRUE NOT TRUE
Reason: We can see that the lamp is Reason: There are no blue chairs in
hanging above the table, and nothing the room, only grey chairs
in the highlighted sentence is false
(a)"True"exampleinstructionforthetextvalida- (b)"False"exampleinstructionforthetextvalida-
tiontask. tiontask.
TRUE TRUE
NOT TRUE NOT TRUE
Reason: The red outlined object is Reason: The highlighted phrase is a
the armchair, which is the same as nightstand, but the red outlined
the highlighted word. The armchair is object is a sofa
also described correctly in the
sentence
(c)"True"exampleinstructionforthegrounding (d)"False"exampleinstructionforthegrounding
validationtask. validationtask.
Figure10:Examplesofinstructionspresentedtoannotatorsbeforetheyareshownanyactualtasksforannotation.
Foreverypossiblekindofhallucination(incorrectobjectattribute,spatialrelation,orobjectexistence),an
illustrativepositiveandnegativeexamplearepresentedinordertoinstructtheannotatortolookforallpossible
failurecases.
Honeypottasksarerandomlymixedinwithrealtasksinordertoensurethatannotatorsaremaintain-
ingahighqualityofannotationsthroughouttheentirejob. Becauseweannotatethehoneypottasks
beforeshowingthemtoannotators,weareabletoevaluateanygivenworker’saccuracyonhoneypot
tasks. We set the minimum honeypot accuracy to 0.89 to ensure that annotators are maintaining
correctannotations. Workersthatdonotmaintainthisaccuracyarebannedfromannotatingourtasks.
Thisishigherthantherequiredaccuracyforqualifiersbecauseweexpectannotatorstoalreadybe
welltrainedinourannotationtasksfromtheinstructionsandqualifiers. Everydatatypeisgiven
between18and35honeypottasks. Thehoneypotsarealsoapproximatelydividedequallybetween
true and false examples so that workers who consistently select a single answer without paying
attentiontothetask(e.g.,someonewhoalwaysselects“True")willbebanned.
Tofurtherensurehigh-qualityannotations,wesendeachquestionto3differentannotatorsandonly
acceptanannotationifatleast2outofthe3annotatorsagreewitheachotheronthetruthfulnessof
anitem. Ifagreementisnotreached,thetaskisreturnedasinconclusive.
D.2 Results
Weperformvalidationon10,200room-annotationpairs. Fromeachofthethreedatatypes,1,700
pairsaresampledforvalidationofbothtexttruthfulnessandgroundingaccuracy. Asubsetof800
roomsisuniformlychosen,with400designatedfortexttruthfulnessandanother400forgrounding
accuracy. Thetextdataisuniformlysampledfromtheserooms. Wereportaccuraciesforbothtext
truthfulnessandgroundingaccuracyinTable7.
WereportcomprehensivestatisticsfromtheannotationprocessinTable8. Weobserveaverylow
qualifierpassraterangingfrom11-20%acrossthedifferenttasksinourdata,suggestingthatour
qualifierswereeffectiveinallowingonlythemostattentiveannotatorsqualifytoannotaterealtasks.
Inaddition,noneoftheseannotatorswerebannedduetohoneypots. Thisincreasesourconfidence
thatourqualificationprocessiseffectiveintrainingannotatorsandfilteringoutthosewhowerenot
attentive. Wealsoobservethatworkersspendroughlythesametimeonrealtasksandhoneypottasks,
suggestingthatthehoneypotsareindistinguishablefromrealtasksfortheannotators. Thisfurther
supportsthevalidityofourannotations.
5Table7:TextTruthfulnessandGroundingAccuracyfromcrowdsourcing.Accuracyiscomputedbydividingthe
numberof“True”responsesbythetotalnumberoftasks(1700).
Method Text Truthfulness Grounding Accuracy
Grounded Scene Description 0.877 0.944
Grounded QA 0.852 0.956
Grounded Object Reference 0.863 0.918
Table8: Comprehensiveannotationmetrics. Includesqualifierpassrate,honeypotcount,honeypotbanrate,
percentoftasksmarkedinconclusive(whereworkerscouldnotcometoanagreementonthelabel),andthe
averagetimethatworkersspendonbothrealtasksandhoneypottasks. Eachdatasetwasevaluatedon1700
annotations.Atleast2workersmustagreeonthelabelforanannotationtobeconsideredvalid.
Category Type %QualifierPassRate #Honeypots %HoneypotBanRate %ofInconclusiveTasks Avg.RealTaskSpeed(s) Avg.HoneypotSpeed(s)
(Pass) (Total) (Ban) (Tasks) (Real) (Honeypot)
SceneDescription 17 35 0 2.03 17.91 17.08
TextAccuracy QA 19 18 0 1.35 16.22 16.64
ObjectReference 10 38 0 0.82 19.88 17.57
SceneDescription 20 18 0 1.66 9.69 12.84
GroundingAccuracy QA 16 18 0 1.11 6.65 11.14
ObjectReference 20 18 0 1.88 9.69 12.84
6