Towards Semantic Equivalence of Tokenization
in Multimodal LLM
ShengqiongWu1,2∗, HaoFei1,2†, XiangtaiLi2, JiayiJi1,
HanwangZhang2,3, Tat-SengChua1, ShuichengYan2
1NationalUniversityofSingapore, 2SkyworkAI,Singapore,
3NanyangTechnologicalUniversity
Abstract
MultimodalLargeLanguageModels(MLLMs)havedemonstratedexceptional
capabilities in processing vision-language tasks. One of the crux of MLLMs
lies in vision tokenization, which involves efficiently transforming input visual
signalsintofeaturerepresentationsthataremostbeneficialforLLMs. However,
existingvisiontokenizers,essentialforsemanticalignmentbetweenvisionand
language, remain problematic. Existing methods aggressively fragment visual
input,corruptingthevisualsemanticintegrity. Toaddressthis,thispaperproposes
anoveldynamicSemantic-EquivalentVisionTokenizer(SeTok),whichgroups
visual features into semantic units via a dynamic clustering algorithm, flexibly
determiningthenumberoftokensbasedonimagecomplexity. Theresultingvision
tokenseffectivelypreservesemanticintegrityandcapturebothlow-frequencyand
high-frequencyvisualfeatures. TheproposedMLLM(SETOKIM)equippedwith
SeToksignificantlydemonstratessuperiorperformanceacrossvarioustasks, as
evidencedbyourexperimentalresults. Theprojectpageathttps://chocowu.
github.io/SeTok-web/.
1 Introduction
Recently, the research on Multimodal Large Language Models (MLLMs) has garnered intense
interest[99,55,20,88]. Bybuildingupontheunprecedentedintelligenceoflanguage-basedLLMs
[16,87],andintegratingmultimodalencoders[71]attheinputsideanddecoders[76]attheoutput
side,currentMLLMshavedevelopedpowerfulmultimodalcapabilities. Particularly,inthevisual
modality,thestate-of-the-art(SoTA)MLLMshavenowachievedagrandslamacrossthefourmajor
visual-languagetaskgroups,i.e.,understanding[91,2,19],generating[53,72,100],segmenting
[50,40,92],andediting[17,102,7]. Centraltothiscapabilityisthedesignofvisiontokenization
[22,25],whichconcernshowtoeffectivelytransforminputvisualsignalsintofeaturerepresentations
thataremostconducivetobeingunderstoodbyLLMs. Specifically,existingvisiontokenizerscan
bemainlycategorizedintotwotypes: 1)tokenizationviaimagepatchifying(cf. Fig. 1(a)),and2)
tokenizationviacodebooks(cf. Fig. 1(b)).
While SoTA MLLMs have achieved promising performances across various tasks, a significant
bottleneckremainswithcurrentvisualtokenizationmethods,i.e.,resultingininsufficientsemantic
alignmentsbetweenlanguageandvisiontokens. Onthelanguageside,linguistictokens(orwords)
arenaturallydiscrete,representingwell-encapsulatedsemanticunits,whereas,onthevisionside,
visualpixelsareinherentlycontinuousdatawithnophysicalboundaries. Intuitively,languagetokens
shouldcorrespondtosemanticallyencapsulatedobjects(orcompositionalregions)withinanimage.
Forinstance,when“adog”ismentioned,the“dog”tokenshouldcorrespondtotheexactpixelregion
∗WorkisdoneasaninternatSkyworkAI.
†Correspondingauthor.
Preprint.Underreview.
4202
nuJ
7
]VC.sc[
1v72150.6042:viXraVisual embedding Text embedding
Visual Output Visual Output Visual Output
Image Decoder Text Image Decoder Text Image Decoder Text
… … … … … …
Visual Encoder LLM Visual Encoder LLM Visual Encoder LLM
MLP/Linear … … Quantization … … Clustering Merger … …
0
Tokenizer Text Tokenizer N1 CN-… 1
od ebook
Text Tokenizer Text
(a) Tokenizing via Patchifying Images (b) Tokenizing via Codebook (c) Tokenizing via Cluster Merger
(e.g., LLaVA, Emu ) (e.g., SEED, Unified-IO) (Ours)
Q:What are the letters appearing in the image? A brown horse standing in a field with a single
large oak tree and a distant mountain.
Vision Vision
Understanding Generation
A: The letter 'a' appears several times, and the word
'alamy' appears in the middle of the image . Language-driven
Vision Tasks
The background hill Remove the boat.
Vision Vision
Segmentation Editing
(d) Four language-driven vision task clusters
Figure1:ComparisonsbetweenexistingMLLMsintokenizinginputimagevia(a)Patchifyingimage,
(b)Codebook,and(c)ClusterMerger. In(d),weshowfourlanguage-drivenvisiontasksenhanced
withsemantic-equivalentvisiontokenswhereregionswiththesamecolordenoteavisiontoken.
ofthedogintheimage.However,asillustratedinFig.1(a&b),bothexistingtokenizationmethodsvia
imagepatchifyingorcodebooksdividetheimageintofixedpatchsquares,causingobjectswithinthe
imagetobesplitacrossdifferentpatches,thusdisruptingtheintegrityofvisualsemanticunits. This
semanticfragmentationfurthermightlargelyleadtothelossofhigh-frequencyvisualinformation,
e.g.,theobject’sedgesandlines. Ultimately,thismisalignmentbetweenvisionandlanguagewithin
MLLMsunderminestheeffectiveunderstandingofvisualsignals,significantlyhinderingprogressin
arangeofvision-languagetasksthatrequireprecise,fine-grainedsemanticalignmentbetweenvision
andlanguageelements.
To this end, this work proposes a Semantic-Equivalent Tokenizer (SeTok) for enhancing visual
MLLMs,whereweencouragethevisionandlanguagetokenstobesemanticallycongruent. The
coreideainvolvesautomaticallygroupingvisualfeaturesfrominputimagesbyapplyingaclustering
algorithm[24],suchthateachuniqueclusterrepresentsanencapsulatedsemanticunitwithinthe
vision. As illustrated in Figure 1(c), the red visual area aggregated by SeTok corresponds to a
completesemanticconcept—“person”,whiletheyellowareacorrespondstothe“surfaceboard”
concept. Furthermore, we recognize that tokenizing images into a fixed number of patches is
impractical. From a semantic perspective, different images should contain varying numbers of
semantically encapsulated objects, and the granularity of compositional regions also needs to be
flexiblydetermined. Forexample,weonlyneedtoidentifyapersonintheimage,whileatother
times,wemayneedtodelineatetheperson’sheadprecisely. Thisimpliesthatitismorereasonable
todeterminethedivisionofvisualtokenizationdynamically. Toaddressthis,weproposeadynamic
clusteringmechanism[24]thatiterativelydeterminesclustercentersbasedondensitypeaks,assigning
visualfeaturestothesecentersuntilallfeaturesareallocated. Thedesignofthismechanismallows
forthedynamicdeterminationofthenumberofconceptvisualtokens,ratherthanfixingtheratio
[36]ormerelymergingthetop-kvisualtokens[5]. Next,afterclustering,wedeviseaclustermerger
toaggregatethevisualfeatureswithineachcluster,thatisdedicatedtolearningacompletevisual
semanticunitfeature,includingbothhigh-frequencyandlow-frequencyinformation.
WefurtherbuildanMLLMequippedwithourSeTok,namedSETOKIM. Then,throughpre-training
andinstruction-tuningonthemultimodaldataacrossdiversetasks(e.g.,text-to-imagegeneration,
2Text Mask decoder Or Visual Decoder
[Img] [/Img] Mask decoder
LLM LoRA …
[Img] [/Img]
Image
Text Cluster Merger Diffusion Decoder
k … …
o
T Q-Former Learnable Queries
e
Tokenized S MMaasskk
Visual Emb.
… …
Text Emb.
Initial + A A A
PV ois su ita iol nE m Emb.
b. EV ni cs oio dn
er
Cluster
mE
lausiV Flat
…
+
+
+
+…
noitnettA
fleS
mroN
&
dd drawroF
deeF
mroN
&
dd gniloopgv
b
.
Figure 2: The overview of SETOKIM. The visual embedding extracted from a vision encoder is
tokenized into vision tokens by SeTok, combined with text tokens to be input LLM for vision-
languageunderstanding. TheoutputvisionembeddingsfromLLMarefedintoavisiondecoderand
maskdecodertogeneraterealisticimagesandsemanticsegmentationmasks,respectively.
imageediting,imageQA),ourmodelevolvesintoaversatilemultimodalgeneralistandexcelsinall
taskgroupsdemonstratedbyextensiveexperiments,asshowninFigure1(d). Comparedwithother
methodsthatfocusonvisiontokenization,in-depthanalysesrevealthatSeTokenjoysthesuperiority
oftokenizingvisioninputintomoreinterpretableandsemantic-completevisiontokens,achieving
theenhancementofvision-languagealignment. Moreover,SeTokcanalsobeeasilyintegratedinto
established MLLMs with minimal adaptation. Extensive experiments reveal that this integration
significantlybooststaskperformance,meanwhilekeepingstrongerinterpretability,andalsoreducing
trainingcostsandinferencetime.
2 Methodology
Inthiswork,weaimtogeneratesemanticallycompleteandtheleast-lossvisiontokensalignedwith
text tokens to facilitate fine-grained semantic interactions between vision and language, thereby
enhancing the performance of MLLMs in various multimodal tasks. In pursuit of this goal, we
proposeconstructingasemantic-equivalenttokenizer,whichtakesvisualfeaturesextractedfroma
well-trainedvisionencoderasinputandoutputssemanticallycompletevisualtokens,asillustrated
inFigure2. Coupledwiththevisiontokenizer,thevisioninputcanbetokenizedintosemantically
equivalenttokenstothetext,whicharethenfedintotheLLMtogetherwiththetexttokens.
2.1 Semantic-equivalentVisionTokenizer
Given an input image I ∈ RH×W×3, we first employ a vision encoder to extract a sequence of
visualembeddingsX ={x }∈Rh×w×d,wheredistheembeddingdimension3. Then,toobtain
i,j
semantically complete visual tokens, we propose to amalgamate the visual features into several
clustersbasedonsemanticsimilarity.
VisionCluster. Wetakethevisualembeddingsasinput,whicharethenclusteredintoavariable
numberofattentionmasksM ∈Rh×w×k,whichassignindividualvisualembeddingtoobject-like
scenecomponents. Inspiredby[24],thisisintuitivelyachievedby(1)selectingthelocation(i,j)
ofthevisualfeaturethathasnotyetbeenassignedtoacluster, (2)creatingaclusterassignment
accordingtothedistanceoftheembeddingsattheselectedpixellocationtoallotherembeddings
accordingtoadistancekernelφ4,and(3)repeatingthefirsttwostepsuntilallvisualembeddings
are accounted for or a stopping criterion is met. Different from [23] employing uniformed seed
scoresperformingthestochasticselectionofvisualembeddings,weproposetochoosethevisual
embeddingsbasedontheirdensitypeaks,asahigherdensityshowsahigherpotentialtobethecluster
center. Specifically,wefirstcalculatethelocaldensityρ ofthetokenx ∈ X byreferringits
i,j i,j
3WhenusingViT-basedvisionencoder,h= H,w= W,wherepisthepatchsize.Similarly,whenusinga
p p
CNN-basedencoder,handwarethesizesofthefeaturemapobtainedbytheconvolutionalneuralnetwork.
4Inthiswork,weemployφ=exp(−∥u−v∥2·kln2)
3neighbors:
1 (cid:88)
ρ =exp(− ||x −x ||2), (1)
i,j K m,n i,j
xm,n∈KNN(xi,j,X)
whereKNN(x ,X)denotestheK-nearestneighborsofx inX. Wethenmeasuretheminimal
i,j i,j
distanceδ betweenthefeaturex andotherfeatureswithhigherdensity:
i,j i,j
(cid:26) min ∥x −x ∥2, if∃m,n:ρ >ρ
δ = m,n:ρm,n>ρi,j i,j m,n m,n i,j (2)
i,j max ∥x −x ∥2, otherwise
m,n i,j m,n
Finally,wesummarizethescores ofthefeaturebycombiningthelocaldensityρ andminimal
i,j i,j
distanceδ asρ ×δ . Basedonthescore,weselectthelocation(i,j)ofthevisualfeaturethat
i,j i,j i,j
hasthehighestscores andhasnotyetbeenassignedtoacluster,anditerativelyassignthevisual
i,j
featureintoacertainclusteruntilastoppingconditionissatisfied,atwhichpointthefinalscopeis
addedasanadditionalmasktoexplainanyremainingvisualembeddings. Thedetailedprocessis
describedformallyinAppendix§C.
ClusterMerger. Afterclustering,thevisualembeddingsaregroupedbasedontheattentionmasks
M. Toretaininformationwithineachclusteroptimally,weadoptaclustermergerthataggregates
visual embeddings beyond merely using cluster centers as definitive vision tokens. This merger
consistsofLstackedblocks, eachcontainingaself-attentionlayer, aresidualandnormalization
layer,andafeed-forwardlayer. Inaddition,consideringthesignificanceofpositionalinformationfor
representingasemanticconceptinanimage,weintegrate2Dpositionembeddings(PE,[33])intothe
merger,calculatedasXˆ =PE(X)⊙M +X⊙M . Theclusterfeaturesarethenflattenedinto
i i i
asequenceXˆ ={x }∈RN×dinraster-scanorder,whereN isthenumberofthevisualfeatures
i i
withinacluster. Subsequentprocessinginvolvesapplyingaveragepoolingacrossthevisualfeatures
withinaclustertoobtainthefinalclusterfeatureu =Avg(Merger(Xˆ ))∈Rd. Consequently,our
i i
tokenizercantokenizetheinputvisionintoavariablenumberofsemantictokensU ={u }∈Rk×d.
i
2.2 MultimodalTrainingScheme
Inthiswork,weconsiderathree-stagetrainingprocedure. Itcanbesummarizedasfollows. The
implementationdetails,liketrainingdata,canbefoundinTable7inAppendix§D.
Stage-I:TokenizerPretraining. Thisstageaimstotrulyendowthetokenizerwiththecapability
oftokenizingtheinputvisionintoasemanticcomplementandindependenttokensthatcancapture
the low-frequency semantic features and high-frequency pixel features. Therefore, we employ a
visiondecodertodecoderealisticimagesbytakingthetokenizedvisualtokensasinputs. Specifically,
thevisiontokensobtainedfromthetokenizerarefedintoalearnablefeaturemapper(i.e.,Q-former
[46])moduleastheinputsoftheU-Netϵ [76]toinfillthevisualdetails,wherethelearnablemodule
θ
consistsoffourcross-attentionlayerstoconnectthevisualtokenizerandtheU-Net. Following[76],
theparametersθofthisU-Netareoptimizedbyϵprediction:
L =E (cid:2) ∥ϵ−ϵ (z ,t,U)∥2(cid:3) , (3)
unet ϵ∼N(0,1),t,zt,U θ t 2
where ϵ is the unscaled noise, t is the sampling step, z is latent noise at step t. In addition, an
t
accuratetokenizershouldbeequippedwithprecisedelineationoftheboundariesbetweentokens,
i.e.,theaccuracyoftheclustering. Meanwhile,weobservethatattentionmaskscorrelatewithobject
masks,buttheyoftenfailtoalignpreciselywithactualobjectboundaries. Weconjecturethatthis
misalignmentstemsfromthevisionencoder’stendencytospatiallydilateinformationaboutobjects,
thus obscuring precise boundary delineation. Consequently, we introduce to incorporate a mask
decoder [49] taking the vision tokens as input to decode the object mask. We follow [49] to use
binary cross-entropy loss L and dice loss L to compute the loss for masks. Moreover, we
bce dice
encompassanauxiliarymaskconsistencylossdesignedtoencouragegreatersimilaritybetweenthe
clusterresultsandtheobjectmasksπ,therebyenhancingtheiralignment.
L =KL(M∥nograd(π))+L +L , (4)
mask bce dice
whereπ ∈[0,1]h×w×k areasetoftheobjectmasks,and(cid:80) π =1.
n i,j,n
Overall,thetrainingofthetokenizerinvolvestwosub-phases: 1)inthefirstsub-phrase,wefreeze
thevisionencoderandfocussolelyontrainingthelearnablecomponents,whichincludethecluster
merger,theq-former,andkeysandvalueswithintheU-Net. Thisstageisconductedwith50ktraining
stepsusingdatasetssuchasCC3M[80]andLAION-AESTHETICS[77]. 2)inthesecondsub-phase,
we integrate a mask decoder to encourage fine-grained object boundary learning. This extended
trainingisperformedontheMSCOCO2017[56]andSAM-1B[39]datasets.
4ImageCaption ImageQA
Method Size
NoCaps Flickr30K VQAv2.0 OK-VQA GQA VSR IconQA VizWiz TextQA
BLIP-2[92] 13B 98.4 73.7 65.0 - 41.0 - - 19.6 42.5
InstructBLIP[57] 7B - - 49.2 - - 34.5 50.1
InstructBLIP[57] 13B 120.0 83.5 - - 49.5 - - 33.4 50.7
Emu-1[98] 7B 106.8 80.9 57.5 46.2 46.0 53.9 42.9 38.1 67.0
DreamLLM[21] 7B - - 34.8 45.2 36.0 - -
SEED[30] 7B 90.4 66.7 - - 34.8 45.2 36.0 - -
LaVIT[37] 7B 114.2 83.0 66.0 54.6 46.8 - - 38.5 -
Shikra[12] 13B - - 77.4* - - - - - -
LLaVA-1.5[59] 7B - - 78.5* - 62.0* - - 50.0 58.2
LLaVA-1.5[59] 13B - - 80.0* - 63.3* - - 53.6 61.3
Qwen-VL[3] 13B - - 78.8* - 59.3* - - 35.2 63.8*
Yi-VL[94] 6B - - 75.1* 56.9* 58.4* 60.4 47.6 50.0 -
MiniGPT-v2-chat[11] 7B - - - 57.8* 60.1 62.9 51.5 53.6 -
SETOKIM 7B 123.5 86.5 79.6* 58.7* 64.4* 63.4 54.1 58.1 60.5
SETOKIM 13B 124.0 87.2 83.9* 60.2* 67.2* 66.5 55.7 59.6 65.3
- - 86.1* 66.1* 94.7* 70.1* 82.6* 73.3* 71.4*
SoTASpecialist -
[14] [14] [54] [85] [65] [15] [14]
Table1: ComparisonbetweendifferentMLLMsonimageunderstandingbenchmarks. ∗denotesthe
correspondingtrainingsetsutilizedforSETOKIMinstruction-tuning.
Stage-II:MultimodalPretraining. Inthisphase, weenhancetheLLMtopossessinterleaved
understanding and generation capabilities for vision-language tasks. Thus, on the one hand, we
adoptnext-tokenprediction-basedcross-entropylossforthetextualcontentgeneration. Meanwhile,
weemployembeddingregressionlosstotrainLLMtogeneratevisualtokens,whicharetrainedto
reconstructthefeaturesonthepre-trainedvisiontokenizerwithaMeanSquaredError(MSE)loss.
Weaddtwospecialtokens‘[Img]’and‘[/Img]’torepresentthebeginningandtheendofthequery
embeddings,andthe‘[Img]’istrainedtopredictwhereanimageemerges. Wepre-traintheLLM
initializedfromVicuna-v1.5[16]onmassivemultimodaldata,including558Kimage-captionpairs
fromtheLLaVA-filteredCC3Mdataset[59]and695KsampledGPT-4V-respondedcaptionsfrom
theALLaVAdataset[9]. Inaddition,wepretrainingourmodelwiththeimage-editingdataset,i.e.,
InstructPix2Pix[6],toenablethemodeltoperformvisionediting.
Stage-III:InstructionTuning. Weperformmultimodalinstructiontuningthroughfine-tuning
LLMusingaLoRA[34]modulewithbothpublicdatasetscoveringfine-grainedvisualQA,image
generationandediting,andtext-richgroundeddatasets. Following[59],weutilize665Ksingle-and
multi-turnconversationsfromtheLLaVA-150K[60],ShareGPT4V[13],academic-task-oriented
VQA datasets (e.g., VQAv2[31], GQA [35], OK-VQA [67], A-OKVQA [78]), and region-level
perception(e.g.,VisualGenome[43],RefCOCO[38]),toenhancethemodel’svisionunderstanding
capabilities. Furthermore,wegeneratepromptsthatproduceoreditimagessuitablefortheconversa-
tioncontext,bringing15Kimage-relatedgenerationandeditingpairsbasedontheInstructPix2Pix
datasetforimage-relatedgenerationandediting. Appendix§Dprovidesmoretrainingdetails.
3 ExperimentsandAnalysis
Intheexperiments,weaimtoquantifytheperformanceofSETOKIMacrossvariousvision-language
tasksprimarilycomparedtoMLLMs([21,41,64,51,82,83])focusonvisualcomprehensionand
generation,aswellaswithwidely-usedMLLMs([59,3,46,18])thatspecializeinsegmentationor
editing. Appendix§Ddetailsexperimentalsetupsandimplementationspecifics.
3.1 MainResults
VisualUnderstanding. Wefirstevaluateourmodelacrossawiderangeofacademicbenchmarks,
including image captioning andimage QA datasets. As depicted in Table 1, our modelachieves
competitiveperformanceinvariousvision-understandingtasks. Specifically,inthecaptioningtasks,
ourmodeloutperformsboththespecialistInstructBLIPandthepreviousSoTAMLLMs, LaVIT,
and SEED, by achieving the highest CIDEr score. Moreover, our model demonstrates superior
performance on various QA datasets, notably improving GQA by 3.9% and iconQA by 4.2%,
suggestingthatourapproachhasadistinctadvantageinunderstandingtherelationshipsandquantities
betweenobjects.Further,acrossvariousMLLMbenchmarksshowninTable2,ourmodelconsistently
exhibitshigherperformance, underscoringitsrobustmultimodalcomprehensioncapabilitiesand
mitigatingthehallucinationissues.
ReferringExpressionComprehension Table3presentsvariousMLLMs’performancesonrefer-
ringexpressioncomprehensiontasks. Itisnoteworthythatourmodelconsistentlyoutperformsthe
5refCOCOg refCOCO+ Reaseg
Method POPE MME MMBen MM-Vet Method
BLIP-2-13B[46] 85.3 1293.8 - - val(U)test(U) val testAtestBgIoUcIoU
InstructBLIP-13B[18] 78.9 1212.8 - - PixelLM[75] 69.3 70.5 66.3 71.7 58.3 - -
Qwen-VL-Chat[3] - 1487.5 60.6 - NExT-Chat[97] 80.1 79.8 77.2 84.5 68.0 - -
LLaVA-1.5-7B[59] - 809.6 64.3 31.1 LISA[44] 67.9 68.5 62.4 67.4 56.5 44.4 46.0
LLaVA-1.5-13B[59] 85.9 1510.7 67.7 36.1 GLaMM[74] 74.2 74.9 72.6 78.7 64.6 - -
Unified-IO2xxl[64] 87.7 1500.1 71.5 - MiniGPT-v2[10] 84.2 84.3 79.6 85.5 73.3
DreamLLM-7B[20] - - 49.9 35.9 SETOKIM 83.8 85.2 81.4 87.5 72.8 49.4 53.7
Mini-Gemini-13B[51] - 1565.0 68.5 46.0 SoTA 86.1 87.0 82.8 89.0 75.9 - -
SETOKIM(13B) 89.1 1578.5 69.8 48.7 Specilist [14] [14] [54] [85] [65] - -
Table2: ComparisononPOPE[52],MME[26],Table3: Theperformanceofreferringexpres-
MMB[62],MM-Vet[95]. sioncomprehensiontasksinvariousMLLMs.
MS-COCO-30K MagicBruch Emu-Edit EVR
Method
FID↓ CLIPim↑ CLIPim↑ L1↓ CLIPim↑ L1↓ CLIPim↑ L1↓
•Diffusion-basedMethod
InstructPix2Pix[7] - - 83.4 12.1 35.9 17.6 81.4 18.9
MagicBrush[102] - - 83.8 10.0 - - - -
SD[76] 12.6 - - - - - - -
•MLLM-basedMethod
MGIE[27] - - 91.1 8.2 - - 81.7 16.3
GILL[41] 12.2 67.5 75.2 28.3 - - 65.0 31.8
Emu-1[98] 11.6 66.5 78.5 27.9 - - 69.2 30.7
Emu-2-gen[82] - 68.6 85.7 19.9 - - 80.3 22.8
SEED[30] - 68.2 80.9 24.5 - - 72.3 28.4
SETOKIM 8.5 69.8 90.7 5.7 87.2 8.5 83.5 15.1
Table4: Performanceofvariousmodelsinzero-shottext-to-imagegenerationandeditingonthe
MS-COCO-30K,MagicBrush,Emu-Edit,andEVRevaluationbenchmarks.
baselines,andsignificantlysurpassesthecurrentSoTAontheReaSegdataset,demonstratingthepro-
ficiencyofourvisiontokensderivedfromSeTokincapturingnotonlyobject-centriclow-frequency
semanticdetailsbutalsothehigh-frequencyboundarydetails.
VisualGenerationandEditing Wenextevaluateourmodel’seffectivenessinvisiongeneration
and editing. Table 4 demonstrates that images generated by our model consistently show higher
similaritytoground-truthimagescomparedwithotherMLLMs,highlightingsuperiorvision-text
translation. Further, we test our model in instruction-based image editing on MagicBrush [101],
Emu-Edit[81],andEvR[86]. Standardpixeldifference(L1)andvisualfeaturesimilarity(CLIP )
im
areemployedasmetrics. Itcanbeobservedthatcomparedtodiffusion-basedspecialists,ourmodel
enjoysahigherperformance. Inaddition,ourmodelalsodemonstratesdistinctsuperiorityinterms
ofL1andCLIPscorescomparedtoexistingMLLMs. ComparedtoEmuorSEED,whichtokenize
imagesintosemanticfragmentsleadingtotheneglectofthehigh-frequencydetailsinimages,our
modelshowsagreateradvantageinL1scores,indicatingrobustcapabilitiesofourvisiontokensin
bothtextualinterpretationandhigh-fidelityimagegeneration.
3.2 In-depthAnalysis
The Impact of the Clustering Mechanism
Here,wecomparetheimpactofdifferentclus-
Mechanism #Cluster TFLOPs Flickr30K OK-VQA
teringmechanismsonmodelperformance,w.r.t, Threshold 19∗ 7.8 84.6 57.8
trainingtime,computationaloverhead,andzero- Hard-clustering 25∗ 8.1 87.2 60.2
shotperformanceonmultimodalunderstanding Soft-clustering 23∗ 8.3 86.7 58.9
tasks. As shown in Table 5, we can observe 576 15.5 83.4 52.1
thattokenizersconstructedusingdynamicclus- 256 13.7 85.1 51.7
Fixed
teringmechanismsachievesuperioroverallper- 64 10.9 84.1 53.6
formancecomparedtothosewithafixedsetup 8 10.1 84.4 50.1
Resampler[1] 64 10.1 83.4 54.5
whilesimultaneouslyacceleratingtrainingtime
andreducingcomputationalcostsduringinfer- Table5: Theeffectofdifferentclusteringstrate-
ence. Specifically, we note that using a pre- gies.Thefirstthreerowsconsistofdynamicstrate-
definedthresholdtodynamicallydeterminethe gies. #Cluster is the number of clusters, and *
numberofclusterscanleadtovariationsinthe denotestheaverageclusternumber.
thresholdduetoscoreinconsistencies,resulting
6Where can I see this sign? What Do you see any person who is playing I want to open my own coffee shop,
does it mean? on the beach? which should have a very
comfortable and warm environment.
Can you help me design it? Show
me a picture of it.
Sure, l can show you a picture of a
cozy coffee shop. Here is a picture:
No, there is no one playing on the
lt's a traffic sign which is usually seen beach in this image. The beach is
beside the street, instructing drivers to mostly empty except for a few items
stop completely. like a striped towel, a blue surfboard, a
red bag, and a couple of chairs. There
Describe the image in detail. are also a few people in the distance,
but they are not actively playing. please give an explanation of your
design
show me a scene of a blue squirrel In this image, we can see a cozy coffee
holding a flower, perched on a fence shop. The shop is filled with wooden
beside a cluster of purple flowers! furniture, including tables and chairs,
The image captures an outdoor Here is the scene. which adds a warm and inviting
seating area of a restaurant or cafe. atmosphere. There are also some potted
The seating area is arranged around a plants placed around the shop, which
central table, with multiple chairs brings a touch of nature indoors. On the
placed around it. In the foreground, tables, there are various cups and
there is a large black umbrella saucers, as well as bottles and glasses,
providing shade for the seating area. suggesting that the shop offers a variety
To the left of the seating area, there is of beverages. The walls are adorned
a sign that read s "A NEW Can you change it into a cartoon style? with pictures and shelves, which add
EXPERIENCE COMING YOUR character to the space. The lighting in
WAY". The sign is white with black Sure, here are the cartoon image. the shop is dim, creating a relaxed and
text, and it is placed on a small stand. intimate ambiance. Overall, this coffee
In the background, there is a building shop is designed to be a comfortable
with a glass facade. The building and welcoming place for customers to
appears to be a commercial or retail enjoy their beverages and socialize.
space, possibly related to the new
experience mentioned in the sign ...
Figure3: Qualitativeresultsonimageunderstandingandgeneration. Thewordsmarkedingreenare
keyelementsinquestionsandanswers.
in poor generalization across different datasets. In contrast to soft-clustering, which yields soft
attentionmasks,ourfindingssuggestthathard-clusteringproducesbetterresults,asitmaybebecause
hardclusteringleadstohigherconsistencyofclusteroutcomes[32],leadingtomorestablevisual
tokensandenhancingboththestabilityandperformanceofthemodel. Whenemployingafixed
number of clusters, the key challenge is to determine the optimal number of clusters. As shown
inTable5,differentdatasetsachieveoptimalperformanceatvaryingnumbersofclusters,witha
uniformcountacrossalldatasets,resultinginsuboptimaloutcomes. Furthermore,wecompareour
mechanismtothecurrentresamplerapproach[1],inwhichafixednumberofqueriesareusedto
groupfeaturesviaacross-attentionmechanism. Ourexperimentsrevealthatdynamicclusteringstill
enjoysthesuperiorityinmultimodalunderstanding,likelybecausethefixedquerytokensarenottruly
alignedwiththenumberofsemanticunitsinvisualcontentandisessentiallyasoftclustermethod.
QualitativeAnalysisofVisualUnderstandingandGeneration AsillustratedinFigure3,our
modelexhibitsproficiencyinintricateimageunderstandingtasks,suchasdecipheringreversedtext,
exemplifiedbytheword“stop”,andaccuratelyidentifyingtext“ANEWEXPERIENCECOMING
YOURWAY”thatispartiallycovered. Intasksinvolvingdetailedimagedescriptions,ourapproach
prioritizesobject-levelinformationwithinimages,whichsubstantiallymitigatestheincidenceof
hallucinatoryresponsescommonlyobservedinMLLMs. Moreover,intext-to-imagegeneration,our
modeldemonstratesremarkablecapabilitiesinsynthesizingcoherentimages,whichmaintainahigh
degreeoffidelityandrelevancetothetextualcontext,suchasthe“flower”,“fence”and“squirrel”.
QualitativeAnalysisofVisualSegmentation WepresentthesegmentationexamplesinFigure
4. It is easy to note that the attention mask closely aligns with the object mask, and our model
showssuperiorityinachievingmoreaccurateanddetailedsegmentationresultscomparedtoother
LLM-basedsegmentationmethods. Notably,asdepictedinthesecondrowofthefigure,thevisual
token generated by our method encompasses all depicted fish, effectively achieving a complete
segmentationofthefishinthescene. Incontrast,othermodelsproduceonlypartialsegmentation.
Thiseffectivenessofthesegmentationhighlightstheprecisecontentrepresentationandimproved
7Input Attention Mark SETOKIM Ospery GLaMM
Figure4: ThevisualizationsforsegmentationresultscomparedwithGLaMM[74]andOsprey[96].
Original MagicBruch Emu-2-Gen MGIE Mini-Gemini Ours
Add some tomato
slices on the pizza
Change the color of
umbrella to red
Remove the unusual
part of this image
Change it into a
clay-style
Figure 5: Qualitative comparison between MLLMs for the image editing. SETOKIM excels in
adheringtoinstructionsandpreservinglow-levelimagedetails.
interpretability of the visual tokens. Such visual tokens eventually enhance the vision-language
understandingincorporatedwiththetexttokens.
Qualitative Analysis of Visual Editing Here, we evaluate the efficacy of image manipulation
usingourmodelcomparedtothepreviousdiffusion-basedmethodMagicBrush[101],andvarious
MLLMs including Emu-2-Gen [82], MGIE [27], and Mini-Gemini [51]. As depicted in Figure
5, SETOKIM displays superior performance by closely adhering to the provided instructions and
preservingintricateimagedetails.Forinstance,ourmodelseamlesslyadds‘tomatoslices’toanimage
withoutalteringotherelementsonthepizza,whileEmu-2-GenandMGIEfallshort. Furthermore,
ourmodelexhibitsremarkableprecisioninchangingthecolorofanumbrellawhilevisualobjects
notintendedforalterationretainahighlevelofconsistencybeforeandafterediting. Additionally,
SETOKIMdemonstratestopreciselyfollowimplicituserinstructionstoremoveunusualelements
fromanimage,i.e.,thebanana,preservingthesurroundingcontext,whereasEmu-2-Genmistakenly
removesatelephonecordandMGIEfailstoremovethebananaproperly,alteringthecord’stexture.
TheseexamplesunderscoretheeffectivenessofSETOKIMforhigh-precisionimagemanipulation,
leveragingsemanticallyequivalentvisualtokenstoachievenuancedandcontext-awareresults.
QualitativeAnalysisofVisualTokens InFigure6, wedemonstratehowinputvisualfeatures
areassignedtovisualtokensaftertokenization. Firstly,weobservethatourtokenizationmethod
duringclusteringbearsaresemblancetopartialsegmentationsrepresentingsemanticcompleteunits.
For instance, in the first and second images 1 and 2, a person is represented as a single token;
8Figure6: Thevisualizationsforvisualtokens.
however, inthethirdimage, thepersonissegmentedintodifferenttokensfortheheadandbody.
Thissegmentationissimilartothefifthimage,whereababyisdividedintodistincttokensforthe
hair,face,body,andhands. Furthermore,inscenariosinvolvingmultipleinstancesofthesameclass,
suchasthetwopersonsinthefinalimage,identicalpartsfromallinstancesareamalgamated. This
approachensuresthatsimilarvisualfeaturesacrossdifferententitiesarerecognizedandprocessed
collectively,enhancingthecoherenceandefficiencyofourmodel’stokenizationprocess.
4 RelatedWork
Currently,benefitingfromtheemergentphenomenon,LLMshavedemonstratednear-human-level
intelligenceinlanguageprocessing[16,87]. Simultaneously,researchershavebeenattemptingto
developMLLMsbyintegratingmultimodalencodersanddecodersintoLLMs[21,41,64,51,82,83].
FromtheinitialMLLMsthatcouldonlyunderstandmultimodalinputsignals[59,61]tolaterversions
supportingthegenerationofmultimodalcontents[83,82,42,89,103],MLLMshaveshownpowerful
capabilitiesandabroaderrangeofapplications. Amongallmodalities, theintegrationofvision,
knownasvisualMLLM,hasreceivedthemostextensiveresearchandapplication[29,79,58,66].
ThelatestMLLMresearchhasnotonlyachievedbothunderstandingandgenerationofvisualcontent,
butalsodevelopedmorerefined,pixel-levelvisualmodeling,includingsegmentationandediting
functions[96,73,97,93,103].
Ontheotherhand,anincreasingbodyofresearchindicatesthatvisualtokenization[22,30,37,48,47]
significantlyimpactsMLLMcapabilitiesinvisiontasks.Thefundamentalapproachinvolvesencoding
the input visual content into feature representations via a visual encoder (e.g., Clip-VIT [70])
and mapping these to an LLM, thus enabling a language-based LLM to understand vision. The
correspondingmethodinvolvespatchifyingtheoriginalvisualimagesofvarioussizesintosmaller
fixed-sizepatches[22,4,61,82,83],treatingtheseastokens,andencodingeachpatch/tokentoobtain
correspondingembeddings,whicharethenfedintotheLLM.Subsequentresearch[37,30],aiming
tofurtherunifythetrainingobjectivesoflanguageandvisualmodalities,proposedtheintroduction
ofcodebooktechniques,representingvisualelementsascodebooks. Thisallowsvisualtrainingto
betreatedsimilarlytolanguagetraining,i.e.,conductingnexttokenprediction[30]. Unfortunately,
whetherintheabovevisualencodingortokenizationtechniques,thereisasignificantbottleneckof
MLLMperformance: theintegrityofvisualsemanticunits,eithervisualobjectsorcompositional
regions, iscompromisedduringthepatchifyingprocess. Thisresultsinalesseffectivesemantic
alignmentbetweenvisionandlanguagewithintheLLM.Thispaperisthefirsttoproposeasolution
tothisproblem,introducinganovelSemanticEquivalentTokenizationforMLLM.
Inaddition,thisworkisalsorelatedtoscenedecomposition[90,68,63],whichinvolvessegmenting
asceneintoobjects. Typically,thesemethodsuseafixednumberofquerytokens[40,84]andapply
cross-attention[90,69]toaggregatevisualfeaturesimplicitly. However,thisfixed-tokenapproach
maynotonlycorrespondtotheactualvisualcontentbutalsorequirescomplexnetworkarchitectures
[8,28]andextensivedataforoptimization.WhencombinedwithLLMs,suchcomplexitysignificantly
increasescomputationalresourcedemands. Conversely,welearnadynamicnumberofsemantic
objectsanddonotrequirecomplexmodelstructuresforoptimization,therebyenhancingresource
efficiencyandprovidingamoreadaptablesolutionforintegratingvisualandlanguagemodalities.
5 Conclusion
In this paper, we present a semantic equivalent tokenization (SeTok) to enhance the semantic
alignment between vision and text. Technically, we propose a dynamic clustering mechanism
to automatically group the visual features into a variable number of semantic complete visual
9tokens. ThenweequipanMLLMwithSeTokandfurtherproposea3-stagetrainingstrategy. After
training,ourmodelshowcasesnotableperformanceonabroadrangeofcomprehension,generation,
segmentation,andeditingtasks,demonstratingourmethod’sefficacy.
References
[1] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatherineMillican,MalcolmReynolds,RomanRing,ElizaRutherford,SerkanCabi,
TengdaHan,ZhitaoGong,SinaSamangooei,MarianneMonteiro,JacobL.Menick,SebastianBorgeaud,
AndyBrock,AidaNematzadeh,SahandSharifzadeh,MikolajBinkowski,RicardoBarreira,OriolVinyals,
AndrewZisserman,andKarénSimonyan. Flamingo:avisuallanguagemodelforfew-shotlearning. In
ProceedingsoftheNeurIPS,2022. 6,7
[2] PeterAnderson,XiaodongHe,ChrisBuehler,DamienTeney,MarkJohnson,StephenGould,andLei
Zhang. Bottom-upandtop-downattentionforimagecaptioningandvisualquestionanswering. In
ProceedingsoftheCVPR,pages6077–6086,2018. 1
[3] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,
andJingrenZhou. Qwen-vl: Afrontierlargevision-languagemodelwithversatileabilities. CoRR,
abs/2308.12966,2023. 5,6
[4] RohanBavishi,ErichElsen,CurtisHawthorne,MaxwellNye,AugustusOdena,ArushiSomani,and
Sag˘nakTas¸ırlar. Introducingourmultimodalmodels, 2023. URLhttps://www.adept.ai/blog/
fuyu-8b. 9
[5] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy
Hoffman. Tokenmerging: Yourvitbutfaster. InTheEleventhInternationalConferenceonLearning
Representations,ICLR2023,Kigali,Rwanda,May1-5,2023,2023. 2
[6] TimBrooks,AleksanderHolynski,andAlexeiA.Efros. Instructpix2pix: Learningtofollowimage
editinginstructions. InIEEE/CVFConferenceonComputerVisionandPatternRecognition,CVPR2023,
Vancouver,BC,Canada,June17-24,2023,pages18392–18402,2023. 5,19
[7] TimBrooks,AleksanderHolynski,andAlexeiAEfros.Instructpix2pix:Learningtofollowimageediting
instructions. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages18392–18402,2023. 1,6
[8] MathildeCaron,PiotrBojanowski,ArmandJoulin,andMatthijsDouze.Deepclusteringforunsupervised
learningofvisualfeatures. InComputerVision-ECCV2018-15thEuropeanConference,Munich,
Germany,September8-14,2018,Proceedings,PartXIV,volume11218ofLectureNotesinComputer
Science,pages139–156,2018. 9
[9] GuimingHardyChen,ShunianChen,RuifeiZhang,JunyingChen,XiangboWu,ZhiyiZhang,Zhihong
Chen,JianquanLi,XiangWan,andBenyouWang. Allava:Harnessinggpt4v-synthesizeddataforAlite
vision-languagemodel. CoRR,abs/2402.11684,2024. 5,19
[10] JunChen,DeyaoZhu,XiaoqianShen,XiangLi,ZechunLiu,PengchuanZhang,RaghuramanKrish-
namoorthi,VikasChandra,YunyangXiong,andMohamedElhoseiny. Minigpt-v2:largelanguagemodel
asaunifiedinterfaceforvision-languagemulti-tasklearning. CoRR,abs/2310.09478,2023. 6
[11] JunChen,DeyaoZhu,XiaoqianShen,XiangLi,ZechunLiu,PengchuanZhang,RaghuramanKrish-
namoorthi,VikasChandra,YunyangXiong,andMohamedElhoseiny. Minigpt-v2:largelanguagemodel
asaunifiedinterfaceforvision-languagemulti-tasklearning. arXivpreprintarXiv:2310.09478,2023. 5
[12] KeqinChen,ZhaoZhang,WeiliZeng,RichongZhang,FengZhu,andRuiZhao. Shikra: Unleashing
multimodalllm’sreferentialdialoguemagic. CoRR,abs/2306.15195,2023. 5
[13] LinChen,JinsongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahuaLin.
Sharegpt4v:Improvinglargemulti-modalmodelswithbettercaptions. CoRR,abs/2311.12793,2023. 5
[14] XiChen,JosipDjolonga,PiotrPadlewski,BasilMustafa,SoravitChangpinyo,JialinWu,CarlosRiquelme
Ruiz,SebastianGoodman,XiaoWang,YiTay,SiamakShakeri,MostafaDehghani,DanielSalz,Mario
Lucic,MichaelTschannen,ArshaNagrani,HexiangHu,MandarJoshi,BoPang,CesleeMontgomery,
PaulinaPietrzyk,MarvinRitter,A.J.Piergiovanni,MatthiasMinderer,FilipPavetic,AustinWaters,
Gang Li, Ibrahim Alabdulmohsin, Lucas Beyer, Julien Amelot, Kenton Lee, Andreas Peter Steiner,
YangLi,DanielKeysers,AnuragArnab,YuanzhongXu,KeranRong,AlexanderKolesnikov,Mojtaba
Seyedhosseini,AneliaAngelova,XiaohuaZhai,NeilHoulsby,andRaduSoricut. Pali-x:Onscalingupa
multilingualvisionandlanguagemodel. CoRR,abs/2305.18565,2023. 5,6
10[15] XiChen,XiaoWang,SoravitChangpinyo,A.J.Piergiovanni,PiotrPadlewski,DanielSalz,Sebastian
Goodman,AdamGrycner,BasilMustafa,LucasBeyer,AlexanderKolesnikov,JoanPuigcerver,NanDing,
KeranRong,HassanAkbari,GauravMishra,LintingXue,AshishV.Thapliyal,JamesBradbury,and
WeichengKuo. Pali:Ajointly-scaledmultilinguallanguage-imagemodel. InTheEleventhInternational
ConferenceonLearningRepresentations,ICLR2023,Kigali,Rwanda,May1-5,2023,2023. 5
[16] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,andEricP.Xing. Vicuna:Anopen-source
chatbotimpressinggpt-4with90quality. 2023. 1,5,9
[17] GuillaumeCouairon,JakobVerbeek,HolgerSchwenk,andMatthieuCord. Diffedit:Diffusion-based
semanticimageeditingwithmaskguidance. InProceedingsoftheICLR,2023. 1
[18] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleFung,andStevenC.H.Hoi. Instructblip:Towardsgeneral-purposevision-languagemodels
withinstructiontuning. InAdvancesinNeuralInformationProcessingSystems36:AnnualConference
onNeuralInformationProcessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,December10-
16,2023,2023. 5,6
[19] RobertoDessì,MicheleBevilacqua,EleonoraGualdoni,NathanaëlCarrazRakotonirina,FrancescaFran-
zon,andMarcoBaroni. Cross-domainimagecaptioningwithdiscriminativefinetuning. InProceedings
oftheCVPR,pages6935–6944,2023. 1
[20] RunpeiDong,ChunruiHan,YuangPeng,ZekunQi,ZhengGe,JinrongYang,LiangZhao,Jianjian
Sun,HongyuZhou,HaoranWei,XiangwenKong,XiangyuZhang,KaishengMa,andLiYi. Dreamllm:
Synergisticmultimodalcomprehensionandcreation. CoRR,abs/2309.11499,2023. 1,6
[21] RunpeiDong,ChunruiHan,YuangPeng,ZekunQi,ZhengGe,JinrongYang,LiangZhao,JianjianSun,
HongyuZhou,HaoranWei,etal. Dreamllm:Synergisticmultimodalcomprehensionandcreation. arXiv
preprintarXiv:2309.11499,2023. 5,9
[22] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner, MostafaDehghani, MatthiasMinderer, GeorgHeigold, SylvainGelly, JakobUszkoreit,
andNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionatscale. In
ProceedingsoftheICLR,2021. 1,9
[23] Mingjing Du, Shifei Ding, and Hongjie Jia. Study on density peaks clustering based on k-nearest
neighborsandprincipalcomponentanalysis. Knowl.BasedSyst.,99:135–145,2016. 3
[24] MartinEngelcke,OiwiParkerJones,andIngmarPosner. GENESIS-V2: inferringunorderedobject
representationswithoutiterativerefinement. InAdvancesinNeuralInformationProcessingSystems34:
AnnualConferenceonNeuralInformationProcessingSystems2021,NeurIPS2021,December6-14,
2021,virtual,pages8085–8094,2021. 2,3
[25] Patrick Esser, Robin Rombach, and Björn Ommer. Taming transformers for high-resolution image
synthesis. InProceedingsoftheCVPR,pages12873–12883,2021. 1
[26] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,WeiLin,
JinruiYang,XiawuZheng,KeLi,XingSun,andRongrongJi. MME:Acomprehensiveevaluation
benchmarkformultimodallargelanguagemodels. CoRR,abs/2306.13394,2023. 6
[27] Tsu-JuiFu,WenzeHu,XianzhiDu,WilliamYangWang,YinfeiYang,andZheGan. Guidinginstruction-
basedimageeditingviamultimodallargelanguagemodels. CoRR,abs/2309.17102,2023. 6,8
[28] WouterVanGansbeke,SimonVandenhende,StamatiosGeorgoulis,andLucVanGool. Unsupervised
semanticsegmentationbycontrastingobjectmaskproposals.In2021IEEE/CVFInternationalConference
onComputerVision,ICCV2021,Montreal,QC,Canada,October10-17,2021,pages10032–10042,
2021. 9
[29] PengGao,JiamingHan,RenruiZhang,ZiyiLin,ShijieGeng,AojunZhou,WeiZhang,PanLu,Conghui
He,XiangyuYue,etal. Llama-adapterv2:Parameter-efficientvisualinstructionmodel. arXivpreprint
arXiv:2304.15010,2023. 9
[30] YuyingGe,YixiaoGe,ZiyunZeng,XintaoWang,andYingShan. PlantingaSEEDofvisioninlarge
languagemodel. CoRR,abs/2307.08041,2023. 5,6,9
[31] YashGoyal,TejasKhot,AishwaryaAgrawal,DouglasSummers-Stay,DhruvBatra,andDeviParikh.
MakingtheVinVQAmatter:Elevatingtheroleofimageunderstandinginvisualquestionanswering.
Int.J.Comput.Vis.,127(4):398–414,2019. 5,19
11[32] JoakimBruslundHaurum,SergioEscalera,GrahamW.Taylor,andThomasB.Moeslund. Whichtokens
touse?investigatingtokenreductioninvisiontransformers. InIEEE/CVFInternationalConferenceon
ComputerVision,ICCV2023-Workshops,Paris,France,October2-6,2023,pages773–783,2023. 7
[33] ByeonghoHeo,SongPark,DongyoonHan,andSangdooYun. Rotarypositionembeddingforvision
transformer. CoRR,abs/2403.13298,2024. 4
[34] EdwardJ.Hu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. Lora:Low-rankadaptationoflargelanguagemodels. InProceedingsoftheICLR,2022. 5,
18
[35] DrewAHudsonandChristopherDManning. Gqa:Anewdatasetforreal-worldvisualreasoningand
compositionalquestionanswering. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pages6700–6709,2019. 5,19
[36] PengJin,RyuichiTakanobu,CaiwanZhang,XiaochunCao,andLiYuan.Chat-univi:Unifiedvisualrepre-
sentationempowerslargelanguagemodelswithimageandvideounderstanding. CoRR,abs/2311.08046,
2023. 2
[37] YangJin,KunXu,KunXu,LiweiChen,ChaoLiao,JianchaoTan,QuzheHuang,BinChen,Chenyi
Lei,AnLiu,ChengruSong,XiaoqiangLei,DiZhang,WenwuOu,KunGai,andYadongMu. Unified
language-visionpretraininginLLMwithdynamicdiscretevisualtokenization. CoRR,abs/2309.04669,
2023. 5,9
[38] SaharKazemzadeh,VicenteOrdonez,MarkMatten,andTamaraL.Berg. Referitgame: Referringto
objectsinphotographsofnaturalscenes.InProceedingsofthe2014ConferenceonEmpiricalMethodsin
NaturalLanguageProcessing,EMNLP2014,October25-29,2014,Doha,Qatar,AmeetingofSIGDAT,
aSpecialInterestGroupoftheACL,pages787–798,2014. 5,19
[39] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloéRolland,LauraGustafson,TeteXiao,
SpencerWhitehead,AlexanderC.Berg,Wan-YenLo,PiotrDollár,andRossB.Girshick. Segment
anything. CoRR,abs/2304.02643,2023. 4,19
[40] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete
Xiao,SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. arXivpreprint
arXiv:2304.02643,2023. 1,9
[41] JingYuKoh,DanielFried,andRussSalakhutdinov.Generatingimageswithmultimodallanguagemodels.
InAdvancesinNeuralInformationProcessingSystems36:AnnualConferenceonNeuralInformation
ProcessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,December10-16,2023,2023. 5,6,9
[42] JingYuKoh,DanielFried,andRussRSalakhutdinov. Generatingimageswithmultimodallanguage
models. AdvancesinNeuralInformationProcessingSystems,36,2023. 9
[43] RanjayKrishna,YukeZhu,OliverGroth,JustinJohnson,KenjiHata,JoshuaKravitz,StephanieChen,
YannisKalantidis,Li-JiaLi,DavidA.Shamma,MichaelS.Bernstein,andLiFei-Fei. Visualgenome:
Connectinglanguageandvisionusingcrowdsourceddenseimageannotations. Int.J.Comput.Vis.,123
(1):32–73,2017. 5,19
[44] XinLai,ZhuotaoTian,YukangChen,YanweiLi,YuhuiYuan,ShuLiu,andJiayaJia. LISA:reasoning
segmentationvialargelanguagemodel. CoRR,abs/2308.00692,2023. 6
[45] HugoLaurençon,LéoTronchon,MatthieuCord,andVictorSanh. Whatmatterswhenbuildingvision-
languagemodels?,2024. 17
[46] JunnanLi,DongxuLi,SilvioSavarese,andStevenC.H.Hoi. BLIP-2:bootstrappinglanguage-image
pre-trainingwithfrozenimageencodersandlargelanguagemodels. InProceedingsoftheICML,pages
19730–19742,2023. 4,5,6
[47] XiangtaiLi,AnshengYou,ZhenZhu,HoulongZhao,MaokeYang,KuiyuanYang,ShaohuaTan,and
YunhaiTong. Semanticflowforfastandaccuratesceneparsing. InComputerVision–ECCV2020:
16thEuropeanConference,Glasgow,UK,August23–28,2020,Proceedings,PartI16,pages775–793.
Springer,2020. 9
[48] XiangtaiLi,HenghuiDing,WenweiZhang,HaoboYuan,GuangliangCheng,PangJiangmiao,KaiChen,
ZiweiLiu,andChenChangeLoy. Transformer-basedvisualsegmentation:Asurvey. arXivpre-print,
2023. 9
12[49] XiangtaiLi,HaoboYuan,WeiLi,HenghuiDing,SizeWu,WenweiZhang,YiningLi,KaiChen,and
ChenChangeLoy. Omg-seg:Isonemodelgoodenoughforallsegmentation? CoRR,abs/2401.10229,
2024. 4
[50] Xiangtai Li, Haobo Yuan, Wei Li, Henghui Ding, Size Wu, Wenwei Zhang, Yining Li, Kai Chen,
andChenChangeLoy. Omg-seg: Isonemodelgoodenoughforallsegmentation? arXivpreprint
arXiv:2401.10229,2024. 1
[51] YanweiLi,YuechenZhang,ChengyaoWang,ZhishengZhong,YixinChen,RuihangChu,Shaoteng
Liu,andJiayaJia. Mini-gemini:Miningthepotentialofmulti-modalityvisionlanguagemodels. CoRR,
abs/2403.18814,2024. 5,6,8,9
[52] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Evaluatingobject
hallucinationinlargevision-languagemodels. InProceedingsoftheEMNLP,pages292–305,2023. 6
[53] YuhengLi,HaotianLiu,QingyangWu,FangzhouMu,JianweiYang,JianfengGao,ChunyuanLi,and
YongJaeLee. Gligen:Open-setgroundedtext-to-imagegeneration. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages22511–22521,2023. 1
[54] WeixinLiang,YanhaoJiang,andZixuanLiu. GraghVQA:Language-guidedgraphneuralnetworksfor
graph-basedvisualquestionanswering. InProceedingsoftheThirdWorkshoponMultimodalArtificial
Intelligence,pages79–86,June2021. 5,6
[55] BinLin,YangYe,BinZhu,JiaxiCui,MunanNing,PengJin,andLiYuan. Video-llava:Learningunited
visualrepresentationbyalignmentbeforeprojection. CoRR,abs/2311.10122,2023. 1
[56] Tsung-YiLin,MichaelMaire,SergeJ.Belongie,JamesHays,PietroPerona,DevaRamanan,PiotrDollár,
andC.LawrenceZitnick. MicrosoftCOCO:commonobjectsincontext. InComputerVision-ECCV
2014-13thEuropeanConference,Zurich,Switzerland,September6-12,2014,Proceedings,PartV,
volume8693ofLectureNotesinComputerScience,pages740–755,2014. 4,19
[57] ChangLiu,HenghuiDing,andXudongJiang. Gres:Generalizedreferringexpressionsegmentation. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages23592–
23601,2023. 5
[58] FangyuLiu,GuyEmerson,andNigelCollier. Visualspatialreasoning. TransactionsoftheAssociation
forComputationalLinguistics,11:635–651,2023. 9
[59] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. CoRR,abs/2310.03744,2023. 5,6,9,19
[60] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InAdvances
inNeuralInformationProcessingSystems36: AnnualConferenceonNeuralInformationProcessing
Systems2023,NeurIPS2023,NewOrleans,LA,USA,December10-16,2023,2023. 5,18,19
[61] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. CoRR,
abs/2304.08485,2023. 9
[62] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,Jiaqi
Wang,ConghuiHe,ZiweiLiu,KaiChen,andDahuaLin. Mmbench: Isyourmulti-modalmodelan
all-aroundplayer? CoRR,abs/2307.06281,2023. 6
[63] FrancescoLocatello, DirkWeissenborn, ThomasUnterthiner, AravindhMahendran, GeorgHeigold,
JakobUszkoreit,AlexeyDosovitskiy,andThomasKipf. Object-centriclearningwithslotattention. In
AdvancesinNeuralInformationProcessingSystems33: AnnualConferenceonNeuralInformation
ProcessingSystems2020,NeurIPS2020,December6-12,2020,virtual,2020. 9
[64] JiasenLu,ChristopherClark,SanghoLee,ZichenZhang,SavyaKhosla,RyanMarten,DerekHoiem,
andAniruddhaKembhavi. Unified-io2:Scalingautoregressivemultimodalmodelswithvision,language,
audio,andaction. CoRR,abs/2312.17172,2023. 5,6,9
[65] PanLu,LiangQiu,JiaqiChen,TanglinXia,YizhouZhao,WeiZhang,ZhouYu,XiaodanLiang,andSong-
ChunZhu. Iconqa:Anewbenchmarkforabstractdiagramunderstandingandvisuallanguagereasoning.
InProceedingsoftheNeuralInformationProcessingSystemsTrackonDatasetsandBenchmarks1,
NeurIPSDatasetsandBenchmarks2021,December2021,virtual,2021. 5,6
[66] PanLu, LiangQiu, JiaqiChen, TonyXia, YizhouZhao, WeiZhang, ZhouYu, XiaodanLiang, and
Song-ChunZhu. Iconqa: Anewbenchmarkforabstractdiagramunderstandingandvisuallanguage
reasoning. arXivpreprintarXiv:2110.13214,2021. 9
13[67] KennethMarino,MohammadRastegari,AliFarhadi,andRoozbehMottaghi.OK-VQA:Avisualquestion
answeringbenchmarkrequiringexternalknowledge.InIEEEConferenceonComputerVisionandPattern
Recognition,CVPR2019,LongBeach,CA,USA,June16-20,2019,pages3195–3204,2019. 5,19
[68] DantongNiu,XudongWang,XinyangHan,LongLian,RoeiHerzig,andTrevorDarrell. Unsupervised
universalimagesegmentation. CoRR,abs/2312.17243,2023. 9
[69] LuQi,JasonKuen,TianchengShen,JiuxiangGu,WenboLi,WeidongGuo,JiayaJia,ZheLin,andMing-
HsuanYang. Highqualityentitysegmentation. InIEEE/CVFInternationalConferenceonComputer
Vision,ICCV2023,Paris,France,October1-6,2023,pages4024–4033,2023. 9
[70] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learning
transferable visual models from natural language supervision. In Proceedings of the ICML, pages
8748–8763,2021. 9
[71] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision.InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,
2021. 1
[72] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,andMarkChen.Hierarchicaltext-conditional
imagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,2022. 1
[73] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham
Cholakkal,RaoMAnwer,ErixXing,Ming-HsuanYang,andFahadSKhan. Glamm:Pixelgrounding
largemultimodalmodel. arXivpreprintarXiv:2311.03356,2023. 9
[74] HanoonaAbdulRasheed,MuhammadMaaz,SahalShajiMullappilly,AbdelrahmanShaker,SalmanH.
Khan,HishamCholakkal,RaoMuhammadAnwer,ErixXing,Ming-HsuanYang,andFahadShahbaz
Khan. Glamm:Pixelgroundinglargemultimodalmodel. CoRR,abs/2311.03356,2023. 6,8
[75] ZhongweiRen,ZhichengHuang,YunchaoWei,YaoZhao,DongmeiFu,JiashiFeng,andXiaojieJin.
Pixellm:Pixelreasoningwithlargemultimodalmodel. arXivpreprintarXiv:2312.02228,2023. 6
[76] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheCVPR,pages10674–10685,2022. 1,
4,6
[77] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeGordon,RossWightman,MehdiCherti,
Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa
Kundurthy,KatherineCrowson,LudwigSchmidt,RobertKaczmarczyk,andJeniaJitsev. LAION-5B:
an open large-scale dataset for training next generation image-text models. In Advances in Neural
InformationProcessingSystems35:AnnualConferenceonNeuralInformationProcessingSystems2022,
NeurIPS2022,NewOrleans,LA,USA,November28-December9,2022,2022. 4,19
[78] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.
A-OKVQA:Abenchmarkforvisualquestionansweringusingworldknowledge. InComputerVision-
ECCV2022-17thEuropeanConference,TelAviv,Israel,October23-27,2022,Proceedings,PartVIII,
volume13668ofLectureNotesinComputerScience,pages146–162,2022. 5,19
[79] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi.
A-okvqa:Abenchmarkforvisualquestionansweringusingworldknowledge. InEuropeanConference
onComputerVision,pages146–162.Springer,2022. 9
[80] PiyushSharma,NanDing,SebastianGoodman,andRaduSoricut. Conceptualcaptions: Acleaned,
hypernymed,imagealt-textdatasetforautomaticimagecaptioning. InProceedingsoftheACL,pages
2556–2565,2018. 4,19
[81] ShellySheynin,AdamPolyak,UrielSinger,YuvalKirstain,AmitZohar,OronAshual,DeviParikh,
and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. CoRR,
abs/2311.10089,2023. 6
[82] QuanSun,YufengCui,XiaosongZhang,FanZhang,QiyingYu,ZhengxiongLuo,YuezeWang,Yongming
Rao,JingjingLiu,TiejunHuang,andXinlongWang. Generativemultimodalmodelsarein-context
learners. CoRR,abs/2312.13286,2023. 5,6,8,9
14[83] QuanSun,QiyingYu,YufengCui,FanZhang,XiaosongZhang,YuezeWang,HongchengGao,Jingjing
Liu,TiejunHuang,andXinlongWang. Generativepretraininginmultimodality. CoRR,abs/2307.05222,
2023. 5,9
[84] TeppeiSuzuki. Clusteringasattention:Unifiedimagesegmentationwithhierarchicalclustering. CoRR,
abs/2205.09949,2022. 9
[85] HaoTanandMohitBansal.LXMERT:learningcross-modalityencoderrepresentationsfromtransformers.
InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe
9thInternationalJointConferenceonNaturalLanguageProcessing,EMNLP-IJCNLP2019,HongKong,
China,November3-7,2019,pages5099–5110,2019. 5,6
[86] HaoTan,FranckDernoncourt,ZheLin,TrungBui,andMohitBansal. Expressingvisualrelationships
vialanguage. InProceedingsofthe57thConferenceoftheAssociationforComputationalLinguistics,
ACL2019,Florence,Italy,July28-August2,2019,Volume1:LongPapers,pages1873–1883,2019. 6
[87] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,PercyLiang,
and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. 2023. URL
https://github.com/tatsu-lab/stanford_alpaca. 1,9
[88] JianzongWu,XiangtaiLi,ChenyangSi,ShangchenZhou,JingkangYang,JiangningZhang,YiningLi,
KaiChen,YunhaiTong,ZiweiLiu,etal. Towardslanguage-drivenvideoinpaintingviamultimodallarge
languagemodels. arXivpreprintarXiv:2401.10226,2024. 1
[89] ShengqiongWu,HaoFei,LeigangQu,WeiJi,andTat-SengChua. Next-gpt:Any-to-anymultimodal
LLM. CoRR,abs/2309.05519,2023. 9
[90] TaoYang,YuwangWang,YanLu,andNanningZheng. Visualconceptstokenization. InAdvances
inNeuralInformationProcessingSystems35: AnnualConferenceonNeuralInformationProcessing
Systems2022,NeurIPS2022,NewOrleans,LA,USA,November28-December9,2022,2022. 9
[91] Xu Yang, Kaihua Tang, Hanwang Zhang, and Jianfei Cai. Auto-encoding scene graphs for image
captioning. InProceedingsoftheCVPR,pages10685–10694,2019. 1
[92] ZhaoYang,JiaqiWang,YansongTang,KaiChen,HengshuangZhao,andPhilipHSTorr.Lavt:Language-
awarevisiontransformerforreferringimagesegmentation. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages18155–18165,2022. 1,5
[93] HaoxuanYou,HaotianZhang,ZheGan,XianzhiDu,BowenZhang,ZiruiWang,LiangliangCao,Shih-
FuChang,andYinfeiYang. Ferret: Referandgroundanythinganywhereatanygranularity. CoRR,
abs/2310.07704,2023. 9
[94] AlexYoung,BeiChen,ChaoLi,ChengenHuang,GeZhang,GuanweiZhang,HengLi,JiangchengZhu,
JianqunChen,JingChang,KaidongYu,PengLiu,QiangLiu,ShawnYue,SenbinYang,ShimingYang,
TaoYu,WenXie,WenhaoHuang,XiaohuiHu,XiaoyiRen,XinyaoNiu,PengchengNie,YuchiXu,
YudongLiu,YueWang,YuxuanCai,ZhenyuGu,ZhiyuanLiu,andZonghongDai. Yi:Openfoundation
modelsby01.ai. CoRR,abs/2403.04652,2024. 5
[95] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,
andLijuanWang. Mm-vet: Evaluatinglargemultimodalmodelsforintegratedcapabilities. CoRR,
abs/2308.02490,2023. 6
[96] YuqianYuan,WentongLi,JianLiu,DongqiTang,XinjieLuo,ChiQin,LeiZhang,andJiankeZhu.
Osprey:Pixelunderstandingwithvisualinstructiontuning. CoRR,abs/2312.10032,2023. 8,9
[97] AoZhang,WeiJi,andTat-SengChua. Next-chat:AnLMMforchat,detectionandsegmentation. CoRR,
abs/2311.04498,2023. 6,9
[98] AoZhang,LimingZhao,Chen-WeiXie,YunZheng,WeiJi,andTat-SengChua. Next-chat:Anlmmfor
chat,detectionandsegmentation. arXivpreprintarXiv:2311.04498,2023. 5,6
[99] AoZhang,HaoFei,YuanYao,WeiJi,LiLi,ZhiyuanLiu,andTat-SengChua. Vpgtrans:Transfervisual
promptgeneratoracrossllms. AdvancesinNeuralInformationProcessingSystems,36,2024. 1
[100] BowenZhang,ShuyangGu,BoZhang,JianminBao,DongChen,FangWen,YongWang,andBaining
Guo. Styleswin:Transformer-basedGANforhigh-resolutionimagegeneration. InProceedingsofthe
CVPR,pages11294–11304,2022. 1
15[101] KaiZhang,LingboMo,WenhuChen,HuanSun,andYuSu. Magicbrush:Amanuallyannotateddataset
forinstruction-guidedimageediting. InAdvancesinNeuralInformationProcessingSystems36:Annual
ConferenceonNeuralInformationProcessingSystems2023,NeurIPS2023,NewOrleans,LA,USA,
December10-16,2023,2023. 6,8
[102] KaiZhang,LingboMo,WenhuChen,HuanSun,andYuSu. Magicbrush:Amanuallyannotateddataset
forinstruction-guidedimageediting. AdvancesinNeuralInformationProcessingSystems,36,2024. 1,6
[103] TaoZhang,XiangtaiLi,HaoFei,HaoboYuan,ShengqiongWu,ShunpingJi,ChangeLoyChen,and
ShuichengYan. Omg-llava:Bridgingimage-level,object-level,pixel-levelreasoningandunderstanding.
arXivpreprint,2024. 9
16A EthicStatement
Thisworkaimstobuildsemanticequivalencetokenizationtosegmentinputimagesintosemantic
complete tokens to enhance the MLLMs in vision understanding, generation, segmentation, and
editing capabilities. Here we discuss all the possible potential impacts of the proposed MLLM,
SETOKIM.
UseofGenerativeContent The SETOKIM, limitedbythequantityoffine-tuningdataandthe
qualityofthebasemodels,maygeneratesomelow-qualitycontent. Also,asagenerativemodel,the
LLMwillproducehallucinatedcontentinmultimodalformatsthatmaybeharmfultosociety. We
haveremindeduserstointerprettheresultswithcaution. AnyonewhousesthisLLMshouldobey
therulesinalicense. Andalsocommercialuseofoursystemisnotallowed.
Data Privacy and Security Our research utilizes datasets that are either publicly available or
collectedwithexplicitconsent. Weadheretostrictdataprivacyandsecurityprotocolstoprotectthe
informationandensureitisusedsolelyforthisresearch.
BiasMitigation RecognizingthepotentialforbiasinAImodels,particularlyinvision-language
tasks,werigorouslytestourtokenizeracrossdiversedatasets. Thisapproachisdesignedtoidentify
and mitigate biases that may affect the model’s performance or lead to unfair outcomes in its
applications.
B Limitation
While SETOKIM hasachievedfurtherimprovementsacrossvariouslanguage-drivenvisiontasks,
becomingazero-shotgeneralspecialist,itstillfacesseverallimitations.
Model Scale. The evaluation of our model is currently constrained to configurations with 7B
and13Bparameters. Asshownin[45],theperformanceofMLLMsislimitedbythescaleofthe
corebackboneLLM.Despitetheimpressiveresultsachieved,thepotentialbenefitsofemploying
significantlylargermodels,suchas65Bor130B,areworthexploringinfuturestudies.
TheresolutionofImage. Regardingimageresolution,ourmodelcurrentlyhandlesimagesuptoa
maximumresolutionof336x336.Whiletherehavebeenimprovementsinunderstandingvisuallyfine-
grainedcontent,challengesremainwhenprocessinghigher-resolutionimages. Therefore,enhancing
themodeltosupportimagesofanyresolutioncouldfurtherimproveitsvisualperceptioncapabilities.
Hallucination. Althoughourmodelhasmadesomeprogressinmitigatinghallucinationthrough
fine-grainedvision-languagealignment,asdemonstratedinexperimentsonthePOPEdataset,the
issueofhallucinationsremainsinevitable. Thisareacontinuestoposechallengesandisacrucial
aspectforfutureexplorationandenhancement.
C DetailedClusteringAlgorithm
TheformalclusteringalgorithmisdescribedinAlgorithm1. Specifically, ascopec = [0,1]h×w
isinitializedtoamatrixofones1h×w totrackthedegreetowhichvisualembeddingshavebeen
assigned to clusters. In addition, the seed scores are initialized by combining the local density
inEq.(1)anddistanceinEq.(2)toperformtheselectionofvisualembeddings. Ateachiteration,
a single embedding vector x is selected at the spatial location (i,j) which corresponds to the
i,j
argmaxoftheelement-wisemultiplicationoftheseedscoresandthecurrentscope. Thisensures
thatclusterseedsaresampledfrompixelembeddingsthathavenotyetbeenassignedtoclusters. An
alphamaskα [0,1]h×w iscomputedasthedistancebetweentheclusterseedembeddingx andall
i i,j
individualpixelembeddingsaccordingtoadistancekernelφ. Theoutputofthekernelφisoneif
twoembeddingsareidenticalanddecreasestozeroasthedistancebetweenapairofembeddings
increases. TheassociatedattentionmaskM isobtainedbytheelement-wisemultiplicationofthe
k
alphamasksbythecurrentscopetoensurethatthefinalsetofattentionmasksisnormalized. The
scopeisthenupdatedbyanelement-wisemultiplicationwiththecomplementofthealphamasks.
17Thisprocessisrepeateduntilastoppingconditionissatisfied,atwhichpointthefinalscopeisadded
asanadditionalmasktoexplainanyremainingembeddings.
Algorithm1ClusteringAlgorithm
Require: visualembeddingsX ∈Rh×w×d
Ensure: masksM ∈Rh×w×k withM ∈[0,1]h×w
1:k
1: Initialize: masksM =∅,scopec=1H×W,seedscoress∈Rh×w
2: whilenotStopCondition(M)do
3: (i,j)=argmax(s>0)
4: α=DistanceKernel(X,(i,j))
5: M.append(s⊙α)
6: s=s⊙(1−α)
7: endwhile
8: M.append(s)
D DetailedExperimentsSettings
D.1 ImplementationDetails
Ourmodelutilizestwotypesofvisionencoders: Vit-G/14-336andConvNext-XXL.Forthelanguage
modelbackbone,wechooseVicuna-v1.5inboththe7Band13Bversions. Theclustermergerwithin
ourarchitecturecomprisestwoblocks,eachequippedwithamulti-headself-attentionmechanism
consistingoftwoheads,afeed-forwardlayer,andlayersforresidualconnectionsandnormalization.
Regarding feature mapping, we initialize the backbone with the pre-trained BERT-base model.
Additionally,SDv1.5isusedtosettheinitialparametersfortheU-Net. Thisintegratedapproach
ensuresarobustfoundationforprocessingandinterpretingmultimodalinputs. WeemployLoRA
[34]andpartialparameterfine-tuningtooptimizethemodel. Here,wedetailthetrainingdatautilized
ateachstageinTable6.
D.2 TrainingReceipt
InTable7,welistthedetailedhyper-parameterssettingatthreestages. AllthetrainingofSETOKIM
isconductedon16×A800(80G)GPUs.
E ExtendedExperimentalAnalysis
AblationStudy Here,weconductanablationstudytoassessthedesignadvantagesofourmodel,
withtheresultspresentedinTable8. Initially,removingthepositionembeddingfromeachcluster
significantlyimpairedperformanceinvisualunderstandingandeditingtasks. Subsequently,replacing
theclustermergerwithanaveragevisualrepresentationforeachclusteralsoledtoamodestdecline
infine-grainedvisualperformance.
TheAdaptionofSeTok Toinvestigatewhetherourproposedsemantic-equivalenttokenizercould
betransferredtoestablishedMLLMs,therebyenhancingtheirperformanceacrossvarioustasks,we
selectmodelsthatcatertovaryingresolutions: Emu-1supporting224*224,LLaVA-v1.5supporting
336*336,andYi-VLsupporting448*448. Then,theseMLLMsareincorporatedwithoursemantic-
equivalenttokenizerbehindtheinputconnectoroftheseexistingmodels,whereeachtokenrepresents
theaverageoftokenswithinacluster.Furthermore,wesample10kinstructiondatafromLLaVA-150k
[60]tofine-tunetheoriginalandmodifiedmodels. TheexperimentalresultsarepresentedinTable9.
Firstly,amarkedimprovementacrossvarioustaskscanbeobservedaftertheapplicationofSeTok,
indicatingtheeffectivenessandnecessityofaligningvisualtokenssemanticallywithtexttokens.
Furthermore,whiletheincreaseinresolutiontypicallyrequiresahighernumberofvisualtokensin
theoriginalMLLMs,theintroductionofclusteringresultsinamoreconsistentnumberoftokens
beingfedintotheLLM.ThisconsistencyiscrucialastheattentioncomputationinLLMsexhibitsa
quadraticrelationshipconcerningthetokenlength;thus,thissparsificationcanacceleratethetraining
processandsignificantlyreducecomputationalcostsduringinference.
18Table6: Thetrainingdatausedinourexperiments.
Name BriefDescription
CC3M[80] 3.3Mimage-textpairsfromtheweb.
LAION-AESTHETICS[77] LAION-AestheticsisasubsetscollectionfromLAION-5B
Stage-I
withhighvisualquality.
MSCOCO2017[56] COCOisalarge-scaleobjectdetection,segmentation,and
captioningdataset.
SAM-1B[39] 1Billion(SA-1B)isadatasetdesignedfortraininggeneral-
purposeobjectsegmentationmodelsfromopen-worldim-
ages
LLaVA-filterCC3Mdataset[59] LLaVAVisualInstructCC3MPretrain595Kisasubset
Stage-II ofCC3Mdataset,filteredwithamorebalancedconcept
coveragedistribution.
ALLaVA-pretraining[9] AdatasetisconstructedbyleveragingthesuperbGPT-4V
togeneratecaptionsandcomplexreasoningQApairs.
InstructPix2Pix[6] Alargescaleinstructin-basedimageeditingdataset.
LLaVA-150K[60] LLaVAVisualInstruct150KisasetofGPT-generated
multimodalinstruction-followingdata.
ALLaVA-instruction[9] AdatasetisconstructedbyleveragingthesuperbGPT-4V
togeneratecaptionsandcomplexreasoningQApairs.
Stage-III
VQAv2[31] VQAisadatasetcontainingopen-endedquestionsabout
images, which require an understanding of vision, lan-
guage,andcommonsenseknowledgetoanswer.
GQA[35] GQAfeaturescompositionalquestionsoverreal-worldim-
ages.
OKVQA[67] OK-VQAisadatasetforvisualquestionansweringthat
requiresmethodsthatcandrawuponoutsideknowledgeto
answerquestions
AOKVQA[78] AOKVQAisanewknowledge-basedvisualquestionan-
sweringbenchmark
RefCOCO[38] TheRefCOCOdatasetisareferringexpressiongeneration
(REG)datasetusedfortasksrelatedtounderstandingnat-
urallanguageexpressionsthatrefertospecificobjectsin
images.
VG[43] VisualGenomeisadataset,aknowledgebase,anongoing
efforttoconnectstructuredimageconceptstolanguage
Table7: Trainingrecipesfor SETOKIM. ThethreetrainingstagesareintroducedinSection2.2.
Stage-I:TokenizerPretraining,Stage-II:MultimodalPretraining,Stage-III:End-to-endInstruction
Tuning.
Configuration Stage-I Stage-II Stage-III
Optimizer AdamW AdamW AdamW
LearningRate 0.001&0.004 0.004 0.0005
WeightDecay 0.001 0.001 0.001
TrainingEpochs 1 1 1
WarmupRatio 0.1 0.1 0.1
LearningRateScheduler Cosine Cosine Cosine
BatchSizePerGPU 32 32 32
MaximumTokenLength 77 2048 2048
TheImpactofVisualEncoder Here,weanalyzetheimpactofusingthesameclusteringmecha-
nismontheeffectivenessofvisionencoders. Initially,asdemonstratedinFigure7(a),weobserve
thatthetransformer-basedvisionencoderexhibitssuperiorperformanceinbothunderstandingand
generatingvisualcontent.Uponinvestigatingthereasons,wefoundthatvisualfeaturesobtainedfrom
theConvNextmodeldonoteasilydifferentiatefocalpoints,asshowninFigure7(b). Thismaybe
duetothelocalreceptivefieldsinconvolutionalnetworks,resultingineachfeaturefocusingsolelyon
19Resolution #Token VQAv2 Flickr30K GQA
Emu-1-7B 256 57.8 80.9 46.5
VQAv2↑ GQA↑ MagicBrush↓ +SeTok 17* 58.6 39.6 48.3
LLaVA-1.5 78.5 62.0 - LLaVA-V1.5-7B 576 80.2 83.2 54.0
SETOKIM 83.9 67.2 5.7 +SeTok 19* 81.2 65.6 55.8
w/oPE 80.3(-3.6) 62.1(-5.1) 10.2(+4.5) Yi-VL-6B 1024 75.3 73.5 58.7
w/oClusterMerger 82.7(-1.2) 64.0(-3.2) 7.7(+2.5) +SeTok 22* 77.1 61.8 60.5
Table 8: Ablation study. ‘w/o PE’ denotes weTable9: TheperformanceofdifferentMLLMs
removethe position embedding foreach visual to-whenintegratingtheSeTokandfine-tuningon
ken. ‘w/oClusterMerger’meansweaveragetokensthe small instruction dataset. ‘#Token’: the
within each cluster to represent the correspondingnumberofvisualtokensfedintoLLM.∗: the
cluster. averagenumberofvisualtokens.
6
5
4
e
ro3
c
S2
1
0
ConvNext-XXL ViT-G/14 Input ConvNext-XXL ViT-G/14
(a) (b) (c)
Figure7: ComparativeanalysisofCNN-based(e.g.,ConvNext-XXL)andtransformer-based(e.g.,
ViT-G/14)visionencoders: (a)performanceonthreedistincttasks,(b)distributionofclusterpossibil-
ityscoresforvisualfeatures,and(c)clusteringoutcomeswithafixedsetof10clusters.
Original
Reconstruction
Figure8: TheimagereconstructionresultsfromvisualtokensbythedenoisingU-Net.
itsownregionandhighlydispersedscores. Incontrast,thescoredistributionoftheViT-G/14ismuch
morecompact,implyingagreaterconsistencyorreliabilityinthescoresitproduces. Furthermore,
employingthesamenumberofclustersforbothvisualfeaturegroups,furthervisualizationofthe
clusteringoutcomesinFigure7(c)revealsthatclustersbasedonconvolutionarequitearbitraryand
donotdiscernobject-levelinformation. Conversely,clustersderivedfromtransformer-basedmodels
delineateobjectcontours,enablingthegenerationofmorepreciseobject-centricvisualtokens,and
leadingtobetterperformanceinvariousmultimodaltasks.
The quality of Reconstruction In Figure 8, we visualize some reconstructed examples by the
traineddenoisingU-Net. Itcanbeseenthat,giventhetokenizedvisualtokens,theoriginalinput
images can be successfully recovered. The reconstructed examples exhibit a high degree of the
constructionofthemethod.
20