Compositional Curvature Bounds for Deep Neural Networks
TahaEntesari*1 SinaSharifi*1 MahyarFazlyab1
Abstract L ≥0suchthat
f
∥f(x)−f(y)∥≤L ∥x−y∥ ∀x,y.
f
Akeychallengethatthreatensthewidespreaduse
ofneuralnetworksinsafety-criticalapplications This constant quantifies the sensitivity of the model f to
is their vulnerability to adversarial attacks. In inputperturbations,motivatingtheneedtoestimateL and
f
thispaper,westudythesecond-orderbehaviorof controlitthrougharchitectureorthetrainingprocess.
continuouslydifferentiabledeepneuralnetworks,
Forcontinuouslydifferentiablefunctions,L isatightupper
focusingonrobustnessagainstadversarialpertur- f
boundonthefirstderivative(∥Df(x)∥≤L ∀x).However,
bations. First,weprovideatheoreticalanalysis f
onecangoonestepfurtherandleveragethesmoothnessof
ofrobustnessandattackcertificatesfordeepclas-
thefirstderivative,i.e.,boundsonthesecondderivativeto
sifiers by leveraging local gradients and upper
obtainamorerefinedmeasureofthefunction’ssensitivity.
boundsonthesecondderivative(curvaturecon-
Indeed,themeritofsecond-orderinformationincharacter-
stant). Next, weintroduceanovelalgorithmto
izingandenhancingrobustnesshasbeenestablished,e.g.,
analyticallycomputeprovableupperboundson
(Singla&Feizi,2020).
the second derivative of neural networks. This
algorithmleveragesthecompositionalstructure OurContributions: Inthiswork,weseektocharacterize
of the model to propagate the curvature bound the adversarial robustness of continuously differentiable
layer-by-layer,givingrisetoascalableandmod- neuralnetworkclassifiersthroughtheLipschitzconstantof
ularapproach. Theproposedboundcanserveas theirfirstderivativedefinedas
a differentiable regularizer to control the curva-
∥Df(x)−Df(y)∥≤L ∥x−y∥ ∀x,y
tureofneuralnetworksduringtraining,thereby Df
enhancing robustness. Finally, we demonstrate
If f is twice differentiable, this constant is a tight upper
theefficacyofourmethodonclassificationtasks bound on the second derivative, ∥D2f(x)∥ ≤ L ∀x.
Df
usingtheMNISTandCIFAR-10datasets.
With a slight abuse of the formal definition, we denote
L asthe“curvature”constant. Ourcontributionsareas
Df
follows.
1.Introduction
• Weprovideatheoreticalanalysisoftheinterplaybetween
Neuralnetworksareinfamouslypronetoadversariallyde- adversarialrobustnessandsmoothness. Specifically,for
signedperturbations(Szegedyetal.,2013). Toaddressthis classificationtasks,wederivelowerboundsonthemargin
vulnerability,manymethodshavebeenproposedtoquantify ofcorrectlyclassifieddatapointsusingthefirstderivative
andimprovetherobustnessofthesemodelsagainstadver- (the Jacobian) and its Lipschitz constant (the curvature
sarial attacks, such as adversarial training (Zhang et al., constant).Wethenshowthatthesecurvature-basedcertifi-
2019;Madryetal.,2018),regularization(Leinoetal.,2021; catesprovablyimproveuponLipschitz-basedcertificates,
Tsuzukuetal.,2018),randomizedsmoothing(Cohenetal., providedthecurvatureissufficientlysmall.
2019;Kumaretal.,2021),andmanyothers.Onemeasureof
• Weproposeanovelalgorithmtoderiveanalyticalupper
robustnessistheLipschitzconstantdefinedasthesmallest
boundsonthecurvatureconstantofneuralnetworks.This
*Equalcontribution 1DepartmentofElectricalandComputer algorithm leverages the compositional structure of the
Engineering,JohnsHopkinsUniversity,Baltimore,UnitedStates modeltocomputetheboundinascalableandmodular
of America. Correspondence to: Mahyar Fazlyab <mahyarfa- fashion, improving upon previous works that only con-
zlyab@jhu.edu>.
siderscalar-valuednetworkswithaffine-then-activation
architectures. Thederivedboundisdifferentiableandcan
Proceedings of the 41st International Conference on Machine
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by beusedasaregularizerfortraininglow-curvatureneural
theauthor(s). networks.
1
4202
nuJ
7
]GL.sc[
1v91150.6042:viXraCompositionalCurvatureBoundsforDeepNeuralNetworks
• We introduce a relaxed notion of smoothness, the An- LipschitzConstantCalculation: Inrecentyears,there
chored Lipschitz constant, which significantly reduces hasbeenafocusonfindingaccurateboundsontheLipschitz
conservatism in terms of robustness certification. Suc- constantofneuralnetworks. Hereweonlydiscusstheones
cinctly,thisdefinitionfixesoneofthetwopointsinvolved thatcanhandlecontinuously-differentiablenetworks. One
in the definition of Lipschitz continuity to the point of oftheearlyworks,(Szegedyetal.,2013),providedabound
interest. on the Lipschitz constant using the norm of each layer,
whichisknowntobealoosebound. (Fazlyabetal.,2019)
• Wealsopresentempiricalresultsdemonstratingtheper- formulatedtheproblemoffindingtheLipschitzconstantas
formanceofourmethodcomparedtopreviousworkson asemidefiniteprogram(SDP),providingaccuratebounds
calculatingcurvaturebounds,andweexaminetheimpact butattheexpenseoflimitedscalability. Later, (Hashemi
oflowcurvaturesontherobustnessofdeepclassifiers. et al., 2021) introduced a local version of LipSDP. Most
recently,(Fazlyabetal.,2023)proposedLipLT,ananalytic
Tothebestofourknowledge,thispaperisthefirsttode- methodforboundingtheLipschitzconstantthroughloop
velopamethodforobtainingprovableboundsonthesecond transformation,acontrol-theoreticconcept.Inthiswork,we
derivativeofgeneralsequentialneuralnetworks. Whilewe alsoleverageLipLTtoderiveupperboundsonthecurvature
consideradversarialrobustnessasanapplicationdomain, constant.
theproposedmethodisalsoofindependentinterestforother
Mostrelevanttooursetup,(Singla&Feizi,2020)develops
applicationsrequiringdifferentiableboundsonthesecond
amethodtoboundthecurvatureconstantofscalar-valued
derivativeofneuralnetworks,suchaslearning-basedcontrol
neural networks in the ℓ norm and introduces a numer-
2
forsafety-criticalapplications(Robeyetal.,2020).
ical optimization scheme to provide curvature-based cer-
tificates. Incontrast,ourmethodboundsthecurvatureof
1.1.RelatedWork
arbitraryfunctioncompositions,inparticularvector-valued
Withrespecttothelargebodyofworkinthisfield,here,we feedforwardneuralnetworks,inanyℓ pnorm,andprovides
focusontheworksthataremorerelevanttooursetup. analyticalcurvature-basedcertificates.
1.2.PreliminariesandNotation
AdversarialRobustness: Therobustnessofdeepmodels
againstadversarialperturbationshasbeenatopicofinter- We denote the n-dimensional real numbers as Rn. For
estinrecentyears(Singla&Feizi,2021;2022;Xuetal., a vector x ∈ Rn, x is its i-th element. For a matrix
i
2022;Zouetal.,2023). (Huangetal.,2021;Fazlyabetal., W ∈ Rn×m, W ∈ R1×m,W ∈ Rn,W ∈ R are
i,: :,j i,j
2023)usetheLipschitzconstantofthenetworkduringthe
thei-throw,j-thcolumn,andthej-thelementofW ,re-
i,:
training procedure to induce robustness by bounding the
spectively. Foravectorx,diag(x)isthediagonalmatrix
worst-caselogits. Toachieverobustness,insteadofpenaliz-
withdiag(x) = x andzerootherwise. Foranintegern
ii i
ingorconstrainingtheLipschitzconstantduringtraining,
let [n] = {1,··· ,n}. Moreover, the operator norm of a
somemethodsdirectlyconstruct1-Lipschitznetworks. The
matrixAisdenotedas∥A∥ = sup ∥Ax∥ . For
use of Lipschitz bounded networks has been encouraged areal-valuedp≥1,wedenop te→ iq tsH¨olde∥ rx c∥ op n≤ ju1 gatewq ithp∗,
by many recentworks (Be´thune etal., 2022)as they pro- i.e., 1 + 1 = 1. Foranyvectorx ∈ Rn andnorm∥·∥ ,
videdesirablepropertiessuchasrobustnessandimproved p p∗ p
wehave∥x∥ =sup x⊤y.
generalization. AOL(Prach&Lampert,2022)providesa p∗ ∥y∥p≤1
rescalingofthelayerweightsthatmakeseachlinearlayer Afunctionf : Rn → Rm isLipschitzcontinuousonC ⊆
1-Lipschitz. ToobtainLipschitzboundednetworks,many Rn if there exists a non-negative constant Lp,q such that
f
works have utilized LipSDP (Fazlyab et al., 2019) to pa- ∥f(x)−f(y)∥ ≤Lp,q∥x−y∥ ∀x,y ∈C. Thesmallest
q f p
rameterize 1-Lipschitz layers. SLL (Araujo et al., 2022) such Lp,q is the Lipschitz constant, in the corresponding
f
proposes1-LipschitzresiduallayersbysatisfyingLipSDP, norms,whichisgivenby
and(Fazlyabetal.,2023)generalizesSLLbyproposinga
√
ρ-Lipschitzlayer. Mostrecently,(Wang&Manchester,
∥f(x)−f(y)∥
2023) satisfies the LipSDP condition using Caley Trans- Lp,q = sup q.
f ∥x−y∥
formsandproposesanon-residual1-Lipschitzlayer. x,y∈C,x̸=y p
Otherworkslookbeyondthenetwork’sfirst-orderproper- Forbrevity,wedenoteLp,pasLp. Inthiswork,wedefine
tiesandcontrolthenetwork’scurvature(Moosavi-Dezfooli f f
anewnotionofLipschitzcontinuityataneighborhoodofa
et al., 2019; Singla et al., 2021). (Srinivas et al., 2022)
point.
proposesusingcentered-softplusactivationsandLipschitz-
boundedbatchnormalizationstocapthecurvatureandem- Definition1.1(AnchoredLipschitzconstant). Forafunc-
piricallyimproverobustness. tionf,theanchoredLipschitzconstantatapointx∈C is
2CompositionalCurvatureBoundsforDeepNeuralNetworks
definedas 2.1.RobustnessCertificatesforDeepClassifiers
Lp f,q(x)= sup ∥f( ∥x x) −− yf ∥(y)∥ q. C n Kons ci ld ae ssr ea s,c wla hss ei rfi ee frC :( Rx) n0:= →ar Rg nm Kax is1≤ ai≤ nen uK raf li( nx e) tww oi rth k
y∈C,x̸=y p
that parameterizes the vector of logits. For a given input
x with correct label y ∈ [n ], the condition for correct
K
Atanypointx,thisconstantisalowerboundontheLips- classificationis
chitzconstantasonecanconfirmLp,q = sup Lp,q(x).
f x∈C f
Figure1demonstratesthisconceptfurtherforthespecific f iy(x):=f i(x)−f y(x)<0, ∀i̸=y.
caseofthetanhfunction.
Assumingthatxiscorrectlyclassifiedasy,thedistanceof
Inthefollowinglemma,weestablishtherelationbetween xtotheclosestdecisionboundarymeasurestheclassifier’s
theanchoredLipschitzconstantandthenormofthederiva- local robustness against additive perturbations. We can
tive. computethisdistancebysolvingthefollowingoptimization
problem,
Lemma1.2. Consideradifferentiablefunctionf :Rn →
Rm andletLp(x)beacorrespondinganchoredLipschitz ε∗(x)=max ε
f
constant. Wehave (1)
s.t. sup f (x+δ)≤0, ∀i̸=y.
iy
∥δ∥p≤ε
∥Df(x)∥ ≤Lp(x).
p f
For ReLU networks and p ∈ {1,∞}, this optimization
problemcanbeencodedasaMixed-IntegerLinearprogram
SeeAppendixAfortheproof.
(MILP)byexploitingthepiece-wisenatureoftheactivation
Givenboundednumbersα ≤ β,afunctionϕ : R → Ris functions(Duttaetal.,2018;Fischetti&Jo,2018;Tjeng
sloperestrictedin[α,β]if et al., 2018). While these MILPs can be solved globally,
theysufferfrompoorscalability. Forneuralnetworkswith
ϕ(x)−ϕ(y) differentiableactivationfunctions,eventhismixed-integer
α≤ ≤β, ∀x,y.
x−y structure is absent, making the exact computation of dis-
tanceseffectivelyintractable. Therefore,wemustresortto
The Lipschitz constant of ϕ is then L ϕ = max(|α|,|β|). findinglowerboundsonthecertifiedradiustogaintractabil-
Forsimplicity,andbasedoncommonly-useddifferentiable ity.
activationfunctionssuchassigmoidandtanh,weassume
thatϕismonotone,i.e.,α≥0,implyingthatL ϕ =β. 2.1.1.LIPSCHITZ-BASEDCERTIFICATES
Supposethef ’sareLipschitzcontinuous. Wecanthen
2.Curvature-basedRobustnessAnalysis iy
write
Consideracontinuouslydifferentiablefunctionf :Rn → f iy(x+δ)≤f iy(x)+Lp fiy(x)∥δ∥ p. (2)
Rm parameterizedbyaneuralnetwork. Inthiswork,our whereLp (x) > 0istheanchored Lipschitzconstantof
goal is to derive provable upper bounds on the Lipschitz f . Bysf uiy bstituting(2)intheconstraintsof(1),weobtain
iy
constantoftheJacobianDf :Rn →Rm×n,definedasthe
the following optimization problem to compute a zeroth-
smallestconstantLp D,q f suchthat order(gradient-free)lowerbound,
∥Df(x 1)−Df(x 2)∥
q
≤Lp D,q f∥x 1−x 2∥ p, ∀x 1,x 2. ε∗ 0(x)=max ε
s.t. sup f (x)+Lp (x)∥δ∥ ≤0,∀i̸=y.
iy fiy p
Furthermore,wecanextendthistotheanchoredLipschitz ∥δ∥p≤ε
constantoftheJacobian,Lp D,q f(x),atagivenpointx,as Wenotethatduetoconstrainttightening,wehaveε∗ 0(x)<
ε∗(x). Usingsimilarargumentsasin(Fazlyabetal.,2023),
∥Df(x+δ)−Df(x)∥ ≤Lp,q(x)∥δ∥ , ∀δ. ε∗(x)hastheclosed-formexpression
q Df p 0
−f (x)
ε∗(x)=min iy . (3)
Whileprovidingprovableupperboundsontheseconstants 0 i̸=y Lp (x)
can be instrumental in various applications, in this work,
fiy
we primarily focus on the adversarial robustness of deep
2.1.2.CURVATURE-BASEDCERTIFICATES
classifiers. Wedevelopourmethodsandcertificatesbased
ontheLipschitzcontinuityoftheclassifieranditsJacobian. Whenthemodeliscontinuouslydifferentiable,wecanex-
Weelaboratemoreonthisinthefollowingsubsections. ploititscurvaturetoimprovethecertificatein(3). Specif-
3CompositionalCurvatureBoundsforDeepNeuralNetworks
Proposition 2.2. Suppose x is classified correctly, i.e.,
1.5 f iy(x) < 0 ∀i ̸= y. Fix a p ≥ 1, and define the zeroth-
Tanh
1.0
A Wn oc rh sto r Ce ad
s
eS l So lp oe
pe
orderε∗ 0(x)andfirst-orderε∗ 1(x)certifiedradiiasin(3)and
(6). Ifthefollowingconditionholds,
Anchor Point
0.5
0.0
Lp,p∗
(x)≤
−2(∥∇f iy(x)∥ p∗ε∗ 0(x)+f iy(x))
, i̸=y.
∇fiy ε∗(x)2
0
0.5
Thenε∗(x)≥ε∗(x).
1.0 1 0
1.5 SeeAppendixAfortheproof.
3 2 1 0 1 2 3
Figure 1: Depiction of anchored Lipschitz constants for 2.2.AttackCertificatesforDeepClassifiers
f(x)=tanh(x). TheanchoredLipschitzconstantatx=2
Consideringthesamesetupasbefore,wenowaimtoobtain
islessthan0.582,whereastheglobalLipschitzconstantis
thesmallestperturbationbywhichacorrectlyclassifieddata
1.
pointcanprovablybemisclassified. Thiscomputationcan
beformulatedasthefollowingoptimizationproblem,
ically,supposethelogitdifferencef iscontinuouslydif-
iy ′∗
ferentiablewithLipschitzgradientsandletLp,p∗
(x)bean
ε (x)=min ε
anchored Lipschitz constant of ∇f
iy
at x. T∇ hf eiy n, we can s.t. min inf f yi(x+δ)<0. (7)
computeanupperboundonf (x+δ)asfollows,
i̸=y ∥δ∥p≤ε
iy
Lp,p∗
(x) First,wenotethatproblems(1)and(7)areequivalent.
f (x+δ)≤f (x)+∇f (x)⊤δ+ ∇fiy ∥δ∥2. (4)
iy iy iy 2 p Proposition 2.3. Suppose f correctly classifies the data
(cid:124) (cid:123)(cid:122) (cid:125)
fiy(x,δ;Lp ∇, fp i∗ y(x)) vp ao li un et ox fa ps roy b, lei. me. s, (f 1iy )( ax n) d< (7)0 arf eor eqi u̸= al,y i. .eT .,h εe ∗n (xt )he =o εp ′t ∗im (xa )l
.
SeeAppendixAforaderivationofthisinequality. Incon-
trasttothezeroth-orderbound,thisupperboundusesthe Usingthecurvature-basedupperbound,onecantightenthe
localfirstderivative,∇f iy(x),aswellasboundsonits(an- constraintsoftheproblemandachieveafirst-order(gradient-
chored) Lipschitz constant,
Lp,p∗
(x), to obtain a locally informed)attackcertificateasfollows,
∇fiy
moreaccurateapproximationoff (x+δ). Bysubstituting
iy ε∗(x)=minε
theupperbound(4)in(1),weobtainafirst-order(gradient- 1
informed)lowerboundonε∗(x), s.t.min inf f (x,δ;Lp,p∗ (x))<0, (8)
i̸=y ∥δ∥p≤ε
yi ∇fyi
ε∗(x)=max ε
1
s.t. sup f
(x,δ;Lp,p∗
(x))≤0, ∀i̸=y
(5)
Weanalyticallyacquiretheoptimalvalueofthisproblemin
iy ∇fiy
∥δ∥p≤ε thefollowingproposition.
Proposition2.4(Curvature-basedattackcertificate). Sup-
Aswesummarizebelow,wecancomputethislowerbound
inclosedform,providedthatwecancomputeLp,p∗
(x).
posexisclassifiedcorrectly,i.e.,f iy(x) < 0∀i ̸= y. Let
∇fiy I ={i|i̸=y,2Lp,p∗ (x)f (x)≤∥∇f (x)∥2 }. Assum-
Proposition 2.1 (Curvature-based certified radius). Sup- ∇fyi yi yi p∗
ingthatI isnon-empty,theoptimizationproblem(8)has
posexisclassifiedcorrectly,i.e.,f (x)<0∀i̸=y. The
iy theclosed-formsolutionε∗(x)givenby
optimizationproblem(5)hastheclosed-formsolutionε∗(x) 1
1
g miv inen −b ∥y
∇f iy(x)∥ p∗+(∥∇f iy(x)∥2 p∗−2Lp ∇,p fi∗ y(x)f iy(x))21
.
m i∈i
In∥∇f yi(x)∥ p∗−(∥∇f
y Li
p
∇( ,x
p
f) y∗∥ i(2
p x∗
)−2Lp ∇,p fy∗ i(x)f yi(x))21
.
i̸=y Lp,p∗ (x) (9)
∇fiy
(6)
Given i∗ ∈ I minimizing (9), the perturbation real-
izing the attack certificate is obtained through solving
SeeAppendixAfortheproofofthisproposition.
sup ∇f (x)⊤δ.
Inthefollowingproposition,weshowthatifthecurvature
∥δ∥p≤ε i∗y
of the model is sufficiently small, we can certify a larger We note that while problem (7) is always feasible (for a
radiusthanLipschitz-basedcertificates. non-trivialclassifier),problem(8)canbeinfeasibledueto
4CompositionalCurvatureBoundsforDeepNeuralNetworks
the second derivative of h, assuming it exists, which is
Unsafe Radius
Safe Region =
0 athird-ordertensor(Srinivasetal.,2022)anddifficultto
Certified Safe Region 𝑓!$ characterize. OurgoalistocomputetheLipschitzconstant
Decision Boundary ofDhdirectlywithoutresortingtoanytensorcalculus.
𝜀%∗ safe radius against
𝜀#∗
𝑖-th class
𝜀$∗
𝜀∗
v
𝜀!∗
𝜀∗Certifiable
Attack
Radius
Usingthechain Dru hl (e x, )th =eJ Dac fo (b gia (xn )o )f Dh gc (xan ).bewrittenas
Thefollowingtheoremestablishesarelationbetweenthe
Lipschitz constant of Dh and the Lipschitz constants of
𝜀∗=min𝜀%∗ =𝜀%∗ f,g,Df,andDg.
𝜀∗≤% 𝜀∗≤𝜀∗ 𝑓 !#=
0
Theorem3.1(Compositionalcurvatureestimation). Given
functionsf,g,andhasdescribedabove,thefollowingin-
𝑓!"
equalityholds,
=
0
Figure2: Certified(ε∗)andattack(ε∗)radiiestimates. The Lp D,p h∗ ≤Lp D,p g∗ Lp f∗ +Lp D,p f∗ Lp gLp g∗ , (10)
greentangentcircledenotesthecertifiedradiusε∗.
whereLp,q denotestheLipschitzconstantofthefunction
s
s(·).
thetighteningoftheconstraints,whichisequivalenttothe
Theorem3.1providesabasistorecursivelycalculateaLip-
setI beingempty. Figure2illustratesanexamplescenario
schitzconstantfortheJacobianofthecompositionofmul-
forε∗andε∗.
tiple functions. In the following, we adapt this result to
The derivation of ε∗(x) and ε∗(x) is significant in two anchoredLipschitzconstants.
ways. First, we established an analytical solution to the
Theorem3.2(Anchoredcompositionalcurvatureestima-
curvature-based certified radius in Proposition 2.1. Al-
tion). Consider functions f,g, and h as in Theorem 3.1.
though curvature-based certificates have been studied in
ThefollowinginequalityholdsfortheanchoredLipschitz
(Singla&Feizi,2020),theirmethodinvolvesaniterative
constantoftheJacobianofhatx
algorithm to solve an optimization problem numerically,
whereasourmethodyieldsclosed-formsolutions. Second, Lp,p∗ (x)≤Lp∗ Lp,p∗ (x)+∥Dg(x)∥ Lp,p∗ (g(x))Lp(x),
Dh f Dg p∗ Df g
weintroducedcurvature-basedattackcertificates,anovel
method for narrowing the certification gap of classifiers. whereLp,q(x)denotestheanchoredLipschitzconstantof
s
Thecertificationgapisthe(empirical)probabilityquantify- thefunctions(·)atx.
ingcorrectlyclassifiedpointsthatlackthedesiredlevelof
certifieddefenseradiiandlackattackcertificatesatagiven ThestructureofTheorem3.1(andsimilarlyTheorem3.2)
perturbation budget ϵ, i.e., P {ε(x) ≤ ϵ ≤ ε(x)}. isofparticularinterestforsequentialneuralnetworksthat
(x,y)∼D
RefertoFigure6foranillustration. arethecompositionofindividuallayers. Inthefollowing
section,wewillinstantiateourframeworkforsuchmodels.
3.EfficientEstimationofCurvatureBounds Remark 3.3. We note that in Theorems 3.1 and 3.2, the
dualnormp∗ischosentotailortheboundsspecificallyfor
Havingestablishedtheimportanceofcurvatureinproviding Proposition2.1andProposition2.4. Ingeneral,thesame
robustness and attack certificates, in this section, we pro- statementsholdifwereplacep∗withageneralq ≥1. See
poseourmethodtoderiveupperboundsontheLipschitz AppendixAformoredetails.
constantoftheJacobian(thecurvatureconstant)ofgeneral
sequentialmodels.Wethencurateouralgorithmforresidual 3.2.CurvatureBoundsforSequentialNeuralNetworks
neuralnetworksandexplorevariousLipschitzestimation
techniquestocalculatethecurvatureconstant. Considerasequentialresidualneuralnetwork
xk+1 =hk(xk)=Hkxk+GkΦ(Wkxk), (11)
3.1.CurvatureBoundsforComposedFunctions
Let h = f ◦ g be the composition of two continuously where k = 0,1,···K − 1 and Wk ∈ Rn′ k×nk,Gk ∈
differentiable functions g : Rn1 → Rn2 and f : Rn2 → Rnk+1×n′ k,andHk ∈Rnk+1×nk aregeneralmatrices. For
Rn3 with Lipschitz Jacobians. The Lipschitz constant of x ∈ Rn, Φ(x) = [ϕ(x 1),··· ,ϕ(x n)]⊤, where ϕ is a dif-
the Jacobian Dh ∈ Rn3×n1 of h is an upper bound on ferentiablemonotoneactivationfunctionslope-restrictedin
5CompositionalCurvatureBoundsforDeepNeuralNetworks
p
[α,β](0≤α≤β <∞)withitsderivativeslope-restricted 3.2.2.COMPUTATIONOFL .
k
in[α′,β′](−∞<α′ ≤β′ <∞). BysettingHk =0and
Lp istheLipschitzconstantofthemapx0 (cid:55)→ xk defined
Gk =I,weobtainthestandardfeedforwardarchitecture. k
bythecomposedfunction(hk−1◦···◦h0)(x0). Anaive
LeveragingTheorem3.1,weproposearecursivealgorithm bound on Lp is the product of the Lipschitz constant of
k
tocomputeanupperboundontheJacobianoftheend-to- individuallayers,i.e.,Lp,naive =(cid:81)k−1Lp
,wherewecan
endmapx0 (cid:55)→ xK. WeestablishthisalgorithminCorol- upperboundeachLp frk om(15). Hi o= w0 eveh ri ,thisboundcan
lary3.4. hi
growquicklyasthedepthincreases. Tomitigatetheadverse
Corollary 3.4. Let Lp,p∗ , k = 0,··· ,K −1 be defined effectofdepth,weexploittheideaofLipLT.Specifically,
recursivelyas
Dk
wecanunroll(11)afterapplyinglooptransformationtoall
activationlayers,resultingin
p,p∗ p,p∗ p p∗ p∗ p,p∗
L =L L L +L L , (12)
Dk+1 Dhk k k hk Dk xk+1=Hˆk···Hˆ0x0+(cid:88)k
Hˆk···Hˆj+1GjΨ(Wjxj).
p,p∗ p p∗ p p∗
withL D0 = 0, L 0 = L 0 = 1, whereL k,L k areLips- j=0
chitzconstantsforthemapx0 (cid:55)→ xk, andLp D,p h∗ k isaLip- This representation enables us to obtain all the constants
schitz constant for the Jacobian of hk. Then Lp,p∗ is a Lp,LT ,··· ,Lp,LT recursivelyasfollows,
Dk 1 K
LipschitzconstantfortheJacobianofthemapx0 (cid:55)→xk.
Lp,LT =∥Hˆk···Hˆ0∥
k+1 p
p p∗
G piv ∗enuppe pr ,pb ∗oundsontheLipschitzconstantsasL k,L k , +β−α(cid:88)k
∥Hˆk···Hˆj+1Gj∥ ∥Wj∥ Lp,LT .
(16)
L hk,andL Dhk,Corollary3.4presentsanalgorithmtocal- 2 p p j
culateanupperboundonthecurvatureconstantofresidual j=0
neuralnetworksinalayer-by-layerfashion. Asshownin(Fazlyabetal.,2023),thisboundprovablyim-
provesthenaiveboundobtainedbytheproductofLipschitz
Next,wewillcomputetheindividualconstantsappearing
in(12).
constantsofindividuallayers,i.e.,Lp,LT ≤(cid:81)k−1Lp,naive
.
k i=0 hi
3.2.1.COMPUTATIONOFLp
hk
3.2.3.COMPUTATIONOFLp D,p h∗
k.
Lp hk istheLipschitzconstantofthek-thlayerhk. Starting Considerthek-thresidualblockhk in(11). TheJacobian
from(11),ananalyticalupperboundonthisconstantis ofthisblockisgivenas
Lp,naive =∥Hk∥ +β∥Gk∥ ∥Wk∥ . (13) Dhk(xk)=Hk+Gkdiag(Φ′(Wkxk))Wk. (17)
hk p p p
This bound is relatively crude as it does not exploit the Thefollowingpropositionprovidesanupperboundonthe
monotonicityoftheactivations,i.e.,(13)isagnostictothe Lipschitzconstantofthisdifferentialoperator.
valueofα. Asproposedin(Fazlyabetal.,2023),thisbound Proposition3.5. TheJacobianDhk definedin(17)isLip-
canbeimprovedbyapplyingalooptransformationonthe p,p∗
schitzcontinuouswithL beinganupperboundonthe
activationlayerΦ. Specifically,wecanrewritehk as Dhk
Lipschitzconstant,where
hk(xk)=Hˆkxk+GkΨ(Wkxk), (14) Lp,p∗
=L ∥Gk∥ ∥Wk∥ ∥Wk∥ ,
Dhk ϕ′ p∗ p∗ p→∞
where Hˆk = Hk+α+βGkWk and ψ(z) = ϕ(z)−(α+ whereL =max{|α′|,|β′|}.
2 ϕ′
β)z/2isthelooptransformedactivationlayer.Asaresultof
thistransformation,Ψisnowslope-restrictedin β−α[−1,1], It is worth mentioning that the upper bound in Proposi-
2
implyingthatψis(β−α)-Lipschitz. Anupperboundonthe tion3.5istractableandcanbecalculatedefficiently. Inpar-
Lipschitzconstantof2 hk,reformulatedasin(14),isthen ticular,forp = 2,thematrixnorms∥Gk∥ p∗ and∥Wk∥ p∗
canbecalculatedviathepoweriterationforfullyconnected
Lp,LT =∥Hk+α+β GkWk∥ + β−α ∥Gk∥ ∥Wk∥ . andconvolutionallayers. Furthermore,∥Wk∥ p→∞issim-
hk 2 p 2 p p plythemaximumrowℓ norm,whichisstraightforwardfor
p
(15) fullyconnectedlayersandalsoconvolutionallayerswith
respecttotherepetitivestructureoftheirToeplitzmatrices.
As shown in (Fazlyab et al., 2023), this bound, now in-
SeetheproofinAppendixAformoredetails.
formedbythemonotonicityconstantα,isprovablybetter
than(13). Thiscanbeprovedbyapplyingthetrianglein- For the choice p = p∗ = 2, we propose an alternative
equalityonthefirstterm. approachtoacquirebetterLipschitzestimatesforDhk. To
6CompositionalCurvatureBoundsforDeepNeuralNetworks
thisend,weproposetorewriteDhk asastandardnetwork Algorithm1CompositionalCurvatureEstimationofNeural
block. Networks
Input: K-layerneuralnetworkintheformof(11).
Lemma3.6(VectorizedJacobian). TheJacobianmatrixin
p p∗ p,p∗
(17)canberewrittenasastandardneuralnetworklayer InitializeL 0 =L 0 =1,L D0 =0.
fork =0toK−1do
p∗
dhk(xk):=vec(Dhk(xk))=bk+AkΦ′(Wkxk), CalculateL
hk
using(15).
p,p∗
CalculateaboundonL usingProposition3.5.
Dhk
where dhk(xk) ∈ Rnˆk with nˆ
k
= n
k+1
× n k. For all UpdateLp,p∗ =Lp,p∗ Lp Lp∗ +Lp∗ Lp,p∗
.
Ti h∈ en[n k A+1 ∈],j R∈ nˆk[ ×n k n]
′
k,a an nd dl b∈ k[n ∈′ k] Rle nˆt km ar= e( gj iv− e1 n)× byn k b+
k
m1+ =i.
endCa fl oc
rulateD Lk p k+ +1 1andD Lh p kk ∗ +1k usk ing(16h )k
.
Dk
H ik j, Ak
ml
=Gk ilW lk j. ReturnLp,p∗
,theLipschitzconstantoftheJacobianof
DK
x0 (cid:55)→xK.
Thefollowinglemmaestablishestherelationbetweenthe
Lipschitz constant of the Jacobian matrix (Lp ) and its
Dhk
vectorizedrepresentation(Lp )whenp=2.
dhk 3.2.5.COMPARISONWITHEXISTINGAPPROACHES
Lemma3.7. Letp=2,andsupposeLp istheLipschitz
dhk UnlikethepreviousworkbySinglaetal. (Singla&Feizi,
constantofthevectorizedJacobianfunctionxk (cid:55)→dhk(xk)
2020),whichislimitedtoscalar-valuedandnon-residual
defined in Lemma 3.6. Then Lp is a valid Lipschitz
dhk architectures,ourframeworkaccommodatesvector-valued
constantfortheJacobianfunctionxk (cid:55)→Dhk(xk).
generalsequentialmodels. Additionally, althoughSingla
et al. (Singla & Feizi, 2020) could theoretically handle
WecanimprovetheLipschitzboundprovidedinProposi-
convolutional neural networks by expressing such layers
tion3.5usingthepreviouslemmas.
as equivalent fully connected layers using their Toeplitz
Theorem3.8. Forp = 2, Lp = L ∥Ak∥ ∥Wk∥ isa matrices(Chenetal.,2020),thisapproachwouldbecom-
dhk ϕ′ 2 2
Lipcshitz constant for the Jaocbian matrix Dhk. Further- putationally prohibitive. In contrast, our method readily
more,Lp ≤Lp . appliestoconvolutionallayers.
dhk Dhk
Moreover,ourmethoddoesnotrequiretwicedifferentiabil-
Leveragingthevectorizedrepresentationdhk oftheJaco-
ity. ThisisparticularlyrelevantforfunctionswithLipschitz
bian Dhk, we can utilize more advanced techniques for
continuousfirstderivativesbutundefinedsecondderivatives.
LipschitzestimationsuchasLipSDPtofurtherreducethe
Forinstance,considerthewell-knownExponentialLinear
conservatism. Specifically,fornon-residualbuildingblocks,
Unit(ELU):
i.e.,whenHk =0andGk =I,wecanextractafeasibleso-
lution(optimalwhenα′ =−β′)totheLipSDPformulation (cid:40)
z z ≥0
fordhk(x). f(z)=
α(ez−1) z ≤0
Theorem 3.9. Let hk(x) = Φ(Wkx). Define L2,SDP =
dhk The ELU has a Lipschitz continuous first derivative, but
L ∥TWk∥ , where T is a diagonal matrix with T =
ϕ′ 2 ii
its second derivative is not defined at z = 0. Therefore,
∥Wk∥ . ThenL2,SDP isavalidLipschitzconstantfordhk
i,: 2 dhk Hessian-basedanalysiswouldfailforthisfunction,whereas
inℓ norm.
2 ourJacobianLipschitzanalysisisapplicabletonetworks
usingthisactivationfunction.
3.2.4.SUMMARYOFALGORITHM
In the following section, we will utilize Algorithm 1 to
Itnowremainstocombineallthecomponentsdeveloped
bound the Lipschitz constant of the Jacobian and exploit
thus far to obtain upper bounds on the curvature of the
itduringthetrainingphasetocontrolthecurvatureofthe
wholenetwork. ThisissummarizedinAlgorithm1. First,
neuralnetwork.
we use LipLT to calculate the Lipschitz constants of the
p∗ p p∗
individuallayers(L hk)andthesubnetworks(L k andL k ). 3.3.Curvature-ControlledNetworks
Then,usingProposition3.5,weprovideanupperboundon
Lp,p∗
.
Finally,wecalculateLp,p∗
using(12).
AsestablishedinSection2,modelswithlowcurvaturecon-
Dhk Dk+1 stantscanelicitmorerobustbehavioragainstnorm-bounded
In the algorithm, we can easily swap the use of Proposi- perturbations. Drivenbythisobservation,wecandesigna
tion3.5withanyLipschitzconstantacquiredbasedonthe curvature-basedregularizerthatwouldpromoterobustness
theoretical ground of Lemma 3.7, such as that of Theo- duringtraining. Oneapproachistorewardlargecertified
rem3.8orTheorem3.9(ifthenetworkisnon-residual). radii in the objective similar to (Fazlyab et al., 2023; Xu
7CompositionalCurvatureBoundsforDeepNeuralNetworks
Table1: Comparisonofcertifiedaccuraciesobtainedfrom
state-of-the-artmethodsSLL(Araujoetal.,2022)andCRM
CCRC
1000 CRC (Fazlyabetal.,2023)onCIFAR-10.
800 CertifiedAccuracy(ε)
Model Methods Accuracy Parameters
36 72 108
255 255 255
600
Standard 79.95 0 0 0 0.7M
6C2F CRM 58.57 36.25 18.36 7.37 0.7M
400 CCRC(Ours) 61.15 49.53 33.36 16.95 0.7M
SLL 57.2 45.0 35.0 26.5 1M
Lip-3C1F SLL+CCRC(Ours) 53.2 46.6 39.3 31.6 1M
200
Standard 61.89 0 0 0 4M
6F CRM 60.63 42.73 24.75 12.6 4M
0 CCRC(Ours) 62.1 52.09 40.8 29.17 4M
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6
Certified Radius
Figure3: Certifiedradiuscomparisonona6-layerneural
etal.,1998). Next,tofurthermotivatetheuseofanchored
network.
Lipschitz constant estimation, we train several networks
withdifferentdepthstoportrayitseffectivenessonbothLip-
etal.,2022),givingrisetothetraininglossfunction schitzconstantestimationandJacobianLipschitzconstant
estimation. Finally, we compare our robustness certifica-
L(x,y;f)=L (f(x),u )+λ1 g(ε∗(x)),
CE y {C(x)=y} 1 tion method with the state-of-the-art classification on the
whereL isthecross-entropyloss,u istheone-hotvec- CIFAR(Krizhevskyetal.,2009)datasetandprovideattack
CE y
tor corresponding to y, λ > 0 is the regularization con- certificatesonthesamenetworks.
stant, and g: R → R is a convex decreasing function
+
(e.g.,g(z)=exp(−z)). Theroleoftheindicatorfunction ComparisonwithotherCurvature-basedMethods In
1 {C(x)=y}istorestricttheregularizertocorrectlyclassified thisexperiment,wecompareourproposedcompositional
points only. This regularizer is differentiable due to the curvaturecalculationmethodwiththepreviousworks. Con-
closed-formexpressionforε∗(x)givenin(6). Nonetheless, sequently, we train a 6-layer fully connected network on
1
computingitforeachdatapointinthetrainingdatasetcan MNISTwithcurvatureregularizationandcomputethecer-
becomputationallycostlyforlarge-scaleinstances. Amore tified radii of the test data points for this network using
efficientapproachistoregularizetheboundontheglobal two curvature calculation algorithms. We denote (Singla
curvatureofthenetworkduringtraining, &Feizi,2020)asCurvature-basedRobustnessCertificate
(CRC),andourmethodasCompositionalCurvature-based
p,p∗
L(x,y;f)=L CE(f(x),u y)+λL Df , RobustnessCertificate(CCRC).Next,tofocustheexperi-
mentoncomparingthecurvaturebounds,weusethemethod
To further improve the efficiency, we propose to use 1-
of(Singla&Feizi,2020)toobtainthecertifiedradiusfor
Lipschitzlayerstobuildthearchitecture. Specifically,when
p = 2, if hk in (11) is modified to be a 1-Lipschitz func- eachpoint.
p
tion (L hk = 1), the concatenation of all layers will be Figure3comparesthecertifiedradiiofthesemethodsand
p
1-Lipschitz (L = 1), and thus (12) yields the curvature confirms the superior performance of the compositional
k
bound
Lp
=
(cid:80)L−1Lp
, which can be readily com- curvaturecalculationalgorithm.
Df k=0 Dhk
puted.
Anchored Lipschitz/Curvature Estimation Next, we
4.Experiments studytheimpactoflocalizingthecomputationsviathecon-
ceptofanchoredLipschitzconstantintroducedinthispaper.
In this section, we evaluate the performance of our pro-
Toachievethis,wetrainfullyconnectedneuralnetworksof
posed methods via a series of experiments. We contrast
varyingdepthsandcalculateupperboundsontheLipschitz
ourmethodsagainstthestate-of-the-artLipschitzconstant
andcurvatureconstantsofthenetwork,bothgloballyand
estimation,curvatureestimation,andneuralnetworkrobust-
inananchoredmanner. Figure4illustratestheresultson
ness certification algorithms. In our experiments we set
theMNISTdataset. Fortheanchoredbounds,weaverage
p = 2forallnormsandLipschitzcalculations. Wedefer
thevaluesoverthetestdataset. Theresultsdemonstratethat
thediscussionofhyperparameterstoAppendixD.1. Our
usingtheanchoredcounterpartssignificantlyimprovesthe
codeisavailableathttps://github.com/o4lc/Compositional-
bounds.
Curvature-Bounds-for-DNNs.
We first showcase the application and superiority of our AttackCertificationonCIFAR-10 Inthisexperiment,
JacobianLipschitzconstantestimationonMNIST(LeCun weprovideradiiforprovableattacksona6Fmodel. Fig-
8CompositionalCurvatureBoundsforDeepNeuralNetworks
1000
13 Global Lipschitz Global Curvature
Anchored Lipschitz 900 Anchored Curvature
12 800
11 700
600
10
500
9 400
8 300
3.0 3.5 4.0 4.5 5.0 5.5 6.0 3.0 3.5 4.0 4.5 5.0 5.5 6.0
Number of Layers Number of Layers
Figure 4: Comparison of global Lipschitz and Curvature
estimationagainsttheiranchoredcounterparts. Theshaded
areasdenotethestandarddeviationoverthewholedataset.
Figure 6: Illustration of the certificates provided by our
method. A andA arethecleanandverifiedaccuracies,
c v
respectively. Thecertificationradiusisε= 36 .
255
100
80 with different architectures on CIFAR-10 with the state-
of-the-art. Wetraintwo6C2Fand6Fnon-residualanda
60
1-LipschitzneuralnetworkwithLip-3C1Farchitectureon
40 theCIFAR-10datasetwiththefollowinglossfunction
20
L(x,y;f)=Lτ,ν(f(x),y)+λLp
, (18)
0 CE Df
0.00 0.05 0.10 0.15 0.20 0.25 0.30
Attack Radius
whereLτ,ν isamodifiedvariantofthecrossentropyloss
Figure 5: Histogram of the certified attack radii for a 6- CE
p
function(Prach&Lampert,2022)andL isthecurvature
layerneuralnetworktrainedviacurvatureregularizationon Df
boundacquiredthroughAlgorithm1.RefertoAppendixD.1
CIFAR-10.
formoreonthelossfunctiondetails.Table1showsthecom-
parisonbetweenthesemodels. Wefindthatincorporating
theadditionalregularizationtermleadstohighercertifiedac-
ure 5 shows the results. This model has an accuracy of
56.19%. Furthermore, with the perturbation budget 36 curacies,smallercertificationgaps,andoften,higherclean
255
themodelhascertifiedandPGDaccuracyof47.16%and accuracies.
48.46%,respectively. Byanalyzingtheattackcertificates
we find that our method is able to provide an attack cer- 5.Conclusion
tificate for a total of 808 samples, of which 645 require
aperturbationbudgetofatmost 36 . Usingthisinforma- Inthiswork,weproposedanovelmethodtocalculateprov-
255
ableupperboundsonthecurvatureconstantofsmoothdeep
tion,therobustaccuracyofthemodelwiththisperturbation
budgetisatmost56.19− 645 ×100=49.74%. Thisis neural networks, i.e., the Lipschitz constant of their first
10000
illustratedinFigure6,whereA isthecleanaccuracy,A∗is derivative. Ourmethodleveragesthecompositionalstruc-
c v
theverifiedaccuracy,andA andA ,arelowerandupper tureofthemodeltocomputethecurvatureboundinascal-
v v
ableandmodularfashion. Thegeneralityofourcurvature
boundsontheverifiedaccuracy,respectively.
estimationalgorithmcanenableitsuseforcompositional
Thishastwomainimplications. First,havingattackcertifi- functions beyond neural networks. Furthermore, we pro-
catesforanydataeliminatestheneedtoperformanattack videdanalyticalrobustnesscertificatesfordeepclassifiers
onthatdataastheexistenceofanattackwasverifiedbyour basedonthecurvatureofthemodel. Inthefuture,weaim
proposition. Second,theattackcertificatesfurthernarrow tofurthertightentheestimatedgapofthecurvaturebound,
downtheuncertaintyofthemodelaccuracy. Asthecertified enabling the algorithm to produce tighter bounds on the
accuracyisalowerboundontheactualcertifiedrobustness curvatureofevendeeperneuralnetworks.
ofthemodel,weconcludethattheactualcertifiedaccuracy
of this model is in the range [47.16,49.74], regardless of
ImpactStatement
thecertificationmethod.
Thispaperpresentsworkwhosegoalistoadvancethefield
RobustnessCertificationonCIFAR-10 Thefinalexper- ofMachineLearning. Wedonotforeseeanysocietalimpli-
iment aims to compare the certified accuracy of models cationsarisingsolelyfromourwork.
9CompositionalCurvatureBoundsforDeepNeuralNetworks
References LeCun,Y.,Bottou,L.,Bengio,Y.,andHaffner,P. Gradient-
basedlearningappliedtodocumentrecognition. Proceed-
Araujo,A.,Havens,A.J.,Delattre,B.,Allauzen,A.,and
ingsoftheIEEE,86(11):2278–2324,1998.
Hu,B. Aunifiedalgebraicperspectiveonlipschitzneural
networks. InTheEleventhInternationalConferenceon Leino,K.,Wang,Z.,andFredrikson,M. Globally-robust
LearningRepresentations,2022. neuralnetworks.InInternationalConferenceonMachine
Learning,pp.6212–6222.PMLR,2021.
Be´thune, L., Boissin, T., Serrurier, M., Mamalet, F.,
Friedrich, C., and Gonzalez Sanz, A. Pay attention to Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and
yourloss: understandingmisconceptionsaboutlipschitz Vladu, A. Towards deep learning models resistant to
neuralnetworks. AdvancesinNeuralInformationPro- adversarialattacks.InInternationalConferenceonLearn-
cessingSystems,35:20077–20091,2022. ingRepresentations,2018.
Chen,Y.,Xie,Y.,Song,L.,Chen,F.,andTang,T. Asur- Moosavi-Dezfooli, S.-M., Fawzi, A., Uesato, J., and
veyofacceleratorarchitecturesfordeepneuralnetworks. Frossard,P. Robustnessviacurvatureregularization,and
Engineering,6(3):264–274,2020. ISSN2095-8099. doi: viceversa. InProceedingsoftheIEEE/CVFConference
https://doi.org/10.1016/j.eng.2020.01.007. onComputerVisionandPatternRecognition,pp.9078–
9086,2019.
Cohen,J.,Rosenfeld,E.,andKolter,Z.Certifiedadversarial
robustnessviarandomizedsmoothing. Ininternational Prach,B.andLampert,C.H. Almost-orthogonallayersfor
conferenceonmachinelearning,pp.1310–1320.PMLR, efficientgeneral-purposelipschitznetworks. InEuropean
2019. ConferenceonComputerVision,pp.350–365.Springer,
2022.
Dutta, S., Jha, S., Sankaranarayanan, S., and Tiwari, A.
Outputrangeanalysisfordeepfeedforwardneuralnet- Robey,A.,Hu,H.,Lindemann,L.,Zhang,H.,Dimarogonas,
works. InNASAFormalMethodsSymposium,pp.121– D. V., Tu, S., and Matni, N. Learning control barrier
138.Springer,2018.
functionsfromexpertdemonstrations.In202059thIEEE
ConferenceonDecisionandControl(CDC),pp.3717–
Fazlyab,M.,Robey,A.,Hassani,H.,Morari,M.,andPap- 3724.IEEE,2020.
pas, G. Efficient and accurate estimation of lipschitz
Singla, S. and Feizi, S. Second-order provable defenses
constantsfordeepneuralnetworks. AdvancesinNeural
againstadversarialattacks. InInternationalconference
InformationProcessingSystems,32,2019.
onmachinelearning,pp.8981–8991.PMLR,2020.
Fazlyab,M.,Entesari,T.,Roy,A.,andChellappa,R. Cer-
Singla,S.andFeizi,S. Skeworthogonalconvolutions. In
tifiedrobustnessviadynamicmarginmaximizationand
InternationalConferenceonMachineLearning,pp.9756–
improvedlipschitzregularization,2023.
9766.PMLR,2021.
Fischetti,M.andJo,J. Deepneuralnetworksandmixed
Singla, S. and Feizi, S. Improved techniques for deter-
integerlinearoptimization. Constraints,23(3):296–309,
ministicl2robustness. AdvancesinNeuralInformation
2018.
ProcessingSystems,35:16110–16124,2022.
Hashemi, N., Ruths, J., and Fazlyab, M. Certifying in-
Singla,V.,Singla,S.,Feizi,S.,andJacobs,D. Lowcurva-
crementalquadraticconstraintsforneuralnetworksvia
tureactivationsreduceoverfittinginadversarialtraining.
convexoptimization. InLearningforDynamicsandCon-
InProceedingsoftheIEEE/CVFInternationalConfer-
trol,pp.842–853.PMLR,2021.
enceonComputerVision,pp.16423–16433,2021.
Huang, Y., Zhang, H., Shi, Y., Kolter, J. Z., and Anand- Srinivas, S., Matoba, K., Lakkaraju, H., and Fleuret, F.
kumar, A. Training certifiably robust neural networks Efficienttrainingoflow-curvatureneuralnetworks. Ad-
withefficientlocallipschitzbounds. AdvancesinNeural vances in Neural Information Processing Systems, 35:
InformationProcessingSystems,34:22745–22757,2021. 25951–25964,2022.
Krizhevsky,A.,Hinton,G.,etal. Learningmultiplelayers Szegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan,
offeaturesfromtinyimages. 2009. D.,Goodfellow,I.,andFergus,R.Intriguingpropertiesof
neuralnetworks. arXivpreprintarXiv:1312.6199,2013.
Kumar,A.,Levine,A.,andFeizi,S. Policysmoothingfor
provablyrobustreinforcementlearning. InInternational Tjeng,V.,Xiao,K.Y.,andTedrake,R. Evaluatingrobust-
ConferenceonLearningRepresentations,2021. nessofneuralnetworkswithmixedintegerprogramming.
10CompositionalCurvatureBoundsforDeepNeuralNetworks
InInternationalConferenceonLearningRepresentations,
2018.
Tsuzuku,Y.,Sato,I.,andSugiyama,M. Lipschitz-margin
training: Scalablecertificationofperturbationinvariance
fordeepneuralnetworks.Advancesinneuralinformation
processingsystems,31,2018.
Wang, R. and Manchester, I. Direct parameterization of
lipschitz-boundeddeepnetworks. InInternationalCon-
ferenceonMachineLearning,pp.36093–36110.PMLR,
2023.
Xu,Y.,Sun,Y.,Goldblum,M.,Goldstein,T.,andHuang,
F. Exploringandexploitingdecisionboundarydynamics
foradversarialrobustness. InTheEleventhInternational
ConferenceonLearningRepresentations,2022.
Zhang, H., Yu, Y., Jiao, J., Xing, E., El Ghaoui, L., and
Jordan, M. Theoretically principled trade-off between
robustnessandaccuracy. InInternationalconferenceon
machinelearning,pp.7472–7482.PMLR,2019.
Zou,A.,Wang,Z.,Kolter,J.Z.,andFredrikson,M. Uni-
versalandtransferableadversarialattacksonalignedlan-
guagemodels. arXivpreprintarXiv:2307.15043,2023.
11CompositionalCurvatureBoundsforDeepNeuralNetworks
A.TheoremsandProofs
ProofofLemma1.2
Proof. Foranydirectiond ∈ Rn, letg (t) = f(x+td). Evidently, wehaveg′(t) = dg (t) = Df(x+td)d, where
d d dt d
Df(x)∈Rm×n. Moreover,wehave
g (t)−g (0) ∥f(x+td)−f(x)∥
∥Df(x)d∥ =∥g′(0)∥ =∥lim d d ∥ = lim p ≤Lp(x)∥d∥ ,
p d p t→0 t−0 p t→0 t f p
wherethethirdequalityfollowsfromcontinuityof∥·∥ . Consequently,
p
∥Df(x)∥ = sup ∥Df(x)d∥ = sup ∥g′(0)∥ ≤Lp(x).
p p d p f
∥d∥p≤1 ∥d∥p≤1
Corollary A.1. Consider a differentiable function f : Rn → R and let Lp(x) be a corresponding anchored Lipschitz
f
constantinsomep-norm. Wehave
∥∇f(x)∥ ≤Lp(x).
p∗ f
Proof. TheprooffollowsfromthefactthatDf(x)=∇f(x)⊤andthatformatrixAandnorm∥·∥ ,wehave∥A⊤∥ =
p p
∥A∥ .
p∗
Derivationofequation(4)
Proof. ToprovethequadraticupperboundonthelogitgapwiththeanchoredLipschitzconstantweutilizethemeanvalue
theorem. Foranyxandδ,wecanwrite
(cid:90) 1
f (x+δ)=f (x)+∇f (x)⊤δ+ (∇f (x+tδ)−∇f (x))⊤δdt.
iy iy iy iy iy
0
Wecanthenwrite
(cid:90) 1
|f (x+δ)−f (x)−∇f (x)⊤δ|≤ |(∇f (x+tδ)−∇f (x))⊤δ|dt
iy iy iy iy iy
0
(cid:90) 1
H¨older’sInequality
−−−−−−−−−−→≤ ∥(∇f (x+tδ)−∇f (x)∥ ∥δ∥ dt
iy iy p∗ p
p1 ∗+ p1=1 0
(cid:90) 1
≤ Lp,p∗ (x)∥δ∥2tdt
∇fiy p
0
Lp,p∗
(x)
= ∇fiy ∥δ∥2.
2 p
Thus,wehave
Lp,p∗
(x)
f (x+δ)≤f (x)+∇f (x)⊤δ+ ∇fiy ∥δ∥2.
iy iy iy 2 p
ProofofProposition2.1
Proof. Define∆ = {δ|f (x+δ) ≤ 0}and∆ = {δ|f
(x,δ;Lp,p∗
(x)) ≤ 0}. Itisclearthat∆ ⊆ ∆. Consequently,
iy iy ∇fiy
{ε | sup f
(x,δ;Lp,p∗
(x)) ≤ 0} ⊆ {ε | sup f (x+δ) ≤ 0},andthus,(5)yieldsavalidlowerboundon
∥δ∥p≤ε iy ∇fiy ∥δ∥p≤ε iy
theoriginalradius(1).
12CompositionalCurvatureBoundsforDeepNeuralNetworks
Now,toacquiretheanalyticalresultwehave
Lp,p∗
(x)
Lp,p∗
(x)
sup f (x)+∇f (x)⊤δ+ ∇fiy ∥δ∥2 =f (x)+ε∥∇f (x)∥ + ∇fiy ε2.
iy iy 2 p iy iy p∗ 2
∥δ∥p≤ε
Thisequalityholdsduetothefactthatfortheδ∗achievingsup ∇f (x)⊤δwehave∥δ∗∥ =ε,implyingthatδ∗is
∥δ∥p≤ε iy p
Lp,p∗
(x)
alsoamaximizeroftheotherterm ∇fiy ∥δ∥2.
2 p
Asaresult,weobtainthefollowingoptimizationproblemthatisequivalentto(5)
max ε
Lp,p∗
(x)
s.t. ∇fiy ε2+∥∇f (x)∥ ε+f (x)≤0,∀i̸=y.
2 iy p∗ iy
Usingelementarycalculations,weobtaintheoptimalsolution
min−∥∇f iy(x)∥ p∗+(∥∇f iy(x)∥2 p∗−2Lp ∇,p fi∗ y(x)f iy(x))21
.
i̸=y Lp,p∗ (x)
∇fiy
ProofofProposition2.2
Proof. Supposexiscorrectlyclassified,i.e.,f (x)<0fori̸=y. Toenforcetheconditionε∗(x)≤ε∗(x)wemusthave
iy 0 1
ε∗(x)≤
−∥∇f iy(x)∥ p∗+(∥∇f iy(x)∥2 p∗−2Lp ∇,p fi∗ y(x)f iy(x))21
, ∀i̸=y.
0 Lp,p∗ (x)
∇fiy
Orequivalently,
Lp,p∗ (x)ε∗(x)2+2∥∇f (x)∥ ε∗(x)+2f (x)≤0.
∇fiy 0 iy p∗ 0 iy
Theaboveinequalityholdsifandonlyif
Lp,p∗
(x)≤
−2(∥∇f iy(x)∥ p∗ε∗ 0(x)+f iy(x))
. (19)
∇fiy ε∗(x)2
0
UtilizingCorollaryA.1,wenotethatε∗(x)=min −fiy(x) ≤ −fiy(x) ≤ −fiy(x) . ThisensuresthattheR.H.S.of
0 i̸=y Lp fiy(x) Lp fiy(x) ∥∇fiy(x)∥p∗
(19)ispositive.
ProofofProposition2.3
Proof. Weprovethisintwosteps:
1. ε∗(x) ≤
ε′∗
(x): Supposethisdoesnothold. Thenε∗(x) >
ε′∗
(x). However,havingasolutionfor(7)impliesthat
thereexistsapairδandj with∥δ∥
≤ε′∗
(x)<ε∗(x)suchthatf (x+δ)<0. Thisperturbation-indexpairisthen
p yj
aviolationfortheconstraintofproblem(1).
Thus,wemusthaveε∗(x)≤ε′∗
(x).
2.
ε∗(x)≥ε′∗
(x):Similarly,ifthisdoesnothold,then∀i̸=y,δ,∥δ∥
≤ε∗(x)<ε′∗
(x)wehavesup f (x+
p ∥δ∥p≤ε∗(x) iy
δ)≤0. Buthavingasolutionfor(7)assertsthatthereexistsonesuchindexthatf (x+δ)<0.
Thus,ε∗(x)≥ε′∗
(x)
yi
musthold.
Asaresult,wemusthaveε∗(x)=ε′∗
(x).
13CompositionalCurvatureBoundsforDeepNeuralNetworks
ProofofProposition2.4
Proof. Toprovethisproposition,wefirstfindtheanalyticalsolutiontotheinneroptimizationproblems,namely
Lp,p∗
(x)
inf f (x,δ;Lp,p∗ (x))= inf inf f (x)+∇f (x)⊤δ+ ∇fyi ∥δ∥2. (20)
∥δ∥p≤ε yi ∇fiy 0≤λ≤1∥δ∥p=λε yi yi 2 p
Let δ∗ = argmin ∇f (x)⊤δ, which yields ∇f (x)⊤δ∗ = −∥∇f (x)∥ . The inner optimization problem is
∥δ∥p≤1 yi yi yi p∗
minimizedatδ =λεδ∗. Consequently,wecanframeproblem(20)equivalentlyas
Lp,p∗
(x)
g∗(ε)=g (ε,λ∗)= min f (x)−ε∥∇f (x)∥ λ+ ∇fyi ε2λ2. (21)
i i 0≤λ≤1 yi yi p∗ 2
(cid:124) (cid:123)(cid:122) (cid:125)
gi(ε,λ)
Therearethreepossiblesolutions
1. λˆ =0. Inthisscenario
g (ε,λˆ)=f (x).
i yi
∥∇f (x)∥
2. λˆ = yi p∗ . Inthisscenario
Lp,p∗ (x)ε
∇fyi
∥∇f (x)∥2
g (ε,λˆ)=f (x)− yi p∗ .
i yi 2Lp,p∗ (x)
∇fyi
∥∇f (x)∥
Forthissolutionwemusthaveλˆ ≤1. Thisimposesthecondition yi p∗ ≤ε.
Lp,p∗ (x)
∇fyi
3. λˆ =1. Inthisscenariowehave
Lp,p∗
(x)
g (ε,λˆ)=f (x)−ε∥∇f (x)∥ + ∇fyi ε2.
i yi yi p∗ 2
Puttingtogetherthedifferentconditions,wefindthat
 f yi(x)− ∥ 2∇ Lf py ,i p( ∗x () x∥2 p )∗ ,∥∇ Lf py ,pi ∗(x () x∥ )p∗ ≤ε
g i∗(ε)= f yi(x)−ε∥∇∇ f yf iy (i x)∥
p∗
+ Lp ∇,p f 2y∗ i(x) ε2 ,∥∇ Lf∇ py ,pf iy ∗(i x () x∥ )p∗ >ε
∇fyi
Thenextstepissolving
min ε
s.t. ming∗(ε)<0.
i
i̸=y
Foreachi̸=y,if ∥∇f yi(x)∥ p∗ ≤εtheng∗(ε)≤0requires2Lp,p∗ (x)f (x)≤∥∇f (x)∥2 ,andif ∥∇f yi(x)∥ p∗ >ε
Lp,p∗ (x) i ∇fyi yi yi p∗ Lp,p∗ (x)
∇fyi ∇fyi
thesmallestvalueofεyieldingg∗(ε)≤0is
i
(cid:113)
∥∇f (x)∥ − ∥∇f (x)∥2 −2Lp,p∗ (x)f (x)
yi p∗ yi p∗ ∇fyi yi
, (22)
Lp,p∗ (x)
∇fyi
14CompositionalCurvatureBoundsforDeepNeuralNetworks
which similarly requires the condition 2Lp,p∗ (x)f (x) ≤ ∥∇f (x)∥2 for realizability. Consequently, if
∇fyi yi yi p∗
2Lp,p∗ (x)f (x) ≤ ∥∇f (x)∥2 holds for some i ̸= y, the smallest valid ε is given as in (22). Thus, we conclude
∇fyi yi yi p∗
that
(cid:113)
∥∇f (x)∥ − ∥∇f (x)∥2 −2Lp,p∗ (x)f (x)
ε∗(x)=min
yi p∗ yi p∗ ∇fyi yi
, (23)
1 i∈I Lp,p∗ (x)
∇fyi
whereI ={i|i̸=y,2Lp,p∗ (x)f (x)≤∥∇f (x)∥2 }. IfI =∅,theproblemisinfeasible.
∇fyi yi yi p∗
ProofofTheorem3.1
Proof. WritingthedefinitionofLipschitzcontinuity,wehave
∥Dh(x)−Dh(y)∥ =∥Df(g(x))Dg(x)−Df(g(y))Dg(y)∥
q q
=∥Df(g(x))Dg(x)−Df(g(x))Dg(y)+Df(g(x))Dg(y)−Df(g(y))Dg(y)∥
q
≤∥Df(g(x))Dg(x)−Df(g(x))Dg(y)∥ +∥Df(g(x))Dg(y)−Df(g(y))Dg(y)∥
q q
≤∥Df(g(x))∥ ∥Dg(x)−Dg(y)∥ +∥Df(g(x))−Df(g(y))∥ ∥Dg(y)∥
q q q q
≤Lp,q∥Df(g(x))∥ ∥x−y∥ +Lq′,q∥g(x)−g(y)∥ ∥Dg(y)∥
Dg q p Df q′ q
≤Lp,q∥Df(g(x))∥ ∥x−y∥ +Lq′,qLp,q′ ∥Dg(y)∥ ∥x−y∥ .
Dg q p Df g q p
BasedonLemma1.2,wenotethatLq isanupperboundon∥Df(·)∥ andthatLq isanupperboundon∥Dg(·)∥ . Thus,
f q g q
wearriveat
∥Dh(x)−Dh(y)∥ ≤(cid:0) Lp,qLq +Lq′,qLp,q′ Lq(cid:1) ∥x−y∥ .
q Dg f Df g g p
Finally,bysettingq =p∗andq′ =p,weobtainLp,p∗ ≤Lp,p∗ Lp∗ +Lp,p∗ LpLp∗.
Dh Dg f Df g g
ProofofTheorem3.2
Proof. TakingthesamestepsastheproofofTheorem3.1,wehave
∥Dh(x+δ)−Dh(x)∥ ≤∥Df(g(x+δ))∥ ∥Dg(x+δ)−Dg(x)∥ +∥Df(g(x+δ))−Df(g(x))∥ ∥Dg(x)∥
q q q q q
≤Lp,q(x)∥Df(g(x+δ))∥ ∥δ∥ +Lq′,q(g(x))Lp,q′ (x)∥Dg(x)∥ ∥δ∥
Dg q p Df g q p
≤(cid:0) LqLp,q(x)+∥Dg(x)∥ Lq′,q(g(x))Lp,q′ (x)(cid:1) ∥δ∥ .
f Dg q Df g p
Settingq =p∗andq′ =pyieldstheresult.
ProofofProposition3.5
Proof. Wedropthesuperscriptkforsimplicityhere. WritingoutthedefinitionofLipschitzcontinuitywehave
∥Dh(x)−Dh(y)∥ =∥Gdiag(Φ′(Wx))W −Gdiag(Φ′(Wy))W∥
q q
≤∥G∥ ∥W∥ ∥diag(Φ′(Wx))−diag(Φ′(Wy))∥
q q q
=∥G∥ ∥W∥ max|Φ′(Wx) −Φ′(Wy) | (24)
q q i i
i
≤L ∥G∥ ∥W∥ max|W (x−y)|,
ϕ′ q q i,:
i
whereL =max{|α′|,|β′|}istheLipschitzconstantofϕ′. Next,wecanwrite
ϕ′
max|W (x−y)|=∥W(x−y)∥ ≤∥W∥ ∥x−y∥ ,
i,: ∞ p→∞ p
i
Settingq =p∗yieldsthedesiredresult.
15CompositionalCurvatureBoundsforDeepNeuralNetworks
ProofofLemma3.6
Proof. Toperformtheconversiontoastandardlayer,weconsiderindividualentriesoftheoutput:
n′
(cid:88)k
Dhk(x) =Hk + GkWkΦ′(Wkx) .
ij ij il lj l
l=1
Thus,byflatteningthematrixDhk(x)intoavectordhk(x),wheretheij-thelementofDhk(x)ismappedtothe(cid:0)
(j−
1)×n +i(cid:1) -thelementofdhk(x),∀i ∈ [n ],j ∈ [n ],weobtainthedesiredresult. Thevectorbk andmatrixAk
k+1 k+1 k
definedinthelemmayieldthecorrectmap. Importantly,wehavetheidentityAk =GˆkW˜k,where
 Gk 0 ··· 0   diag(Wk )
:,1
Gˆk
=
  
0
. . .
G
. .
.k · .· .·
.
0
. . .

  , W˜k
=
 
diag(
. .
.W :k ,2)
  .
0 0 ··· Gk diag(Wk )
:,nk
Evidently,wehaveAk =(cid:0) Gkdiag(Wk )(cid:1) =GkWk.
ml :,j il il lj
ProofofLemma3.7
Proof. Byvectorization,wehave∥dhk(x)−dhk(y)∥ =∥Dhk(x)−Dhk(y)∥ ,where∥·∥ istheFrobeniusnorm. We
2 F F
canwrite
∥Dhk(x)−Dhk(y)∥ ≤∥Dhk(x)−Dhk(y)∥ =∥dhk(x)−dhk(y)∥ ≤Lp ∥x−y∥ ,
2 F 2 dhk 2
wherewehaveusedthefactthatforagivenmatrixA,∥A∥ =σ
(A)≤(cid:112)(cid:80)
σ2(A)=∥A∥ .
2 max i i F
ProofofTheorem3.8
Proof. UsingidentityAk =GˆkW˜k fromtheproofofLemma3.6,wehave
∥Ak∥ ≤∥Gˆk∥ ∥W˜k∥ =∥Gk∥ ∥W˜k∥ .
2 2 2 2 2
For∥W˜k∥ wehave(W˜k⊤W˜k) =(cid:80) W˜kW˜k. WithrespecttothesparsitypatternofthematrixW˜k,W˜k⊤W˜k isonly
2 ij l li lj
non-zeroonitsdiagonalwith(W˜k⊤W˜k) = ∥W ∥2. Thus∥W˜k∥ = max ∥W ∥ = ∥W∥ . Thisconcludesthe
ii i,: 2 2 i i,: 2 2→∞
proof.
ProofofTheorem3.9
Proof. Using Lemma 3.6 to vectorize the Jacobian of the layer hk(x) = Φ(Wkx), we obtain Ak = W˜k (see proof of
Lemma3.6). AsstatedintheproofofTheorem3.8,Ak⊤Ak isdiagonalwith(Ak⊤Ak) = ∥W ∥ . Next,westatethe
ii i,: 2
semidefiniteprogramof(Fazlyabetal.,2019)forcalculatingtheLipschitzconstantofdhk =Akϕ′(Wx). Define
(cid:34) (cid:35)
−α′β′Wk⊤TWk−ρI (α′+β′)Wk⊤T
M(ρ,T)= 2 ,
(α′+β′)TWk −T +Ak⊤Ak
2
whereT isadiagonalnon-negativematrixofappropriatedimensions. Wehavethefollowingoptimizationproblem.
min ρ
ρ,T
s.t. M(ρ,T)⪯0.
√
Asstatedin(Fazlyabetal.,2019),foranygivenfeasiblepair(ρ,T), ρisanupperboundontheLipschitzconstantofthe
desiredmap.
16CompositionalCurvatureBoundsforDeepNeuralNetworks
Wefirstassumethatα′ =−β′,implyingthattheoff-diagonaltermsofM(ρ,T)willbezero. Asaresult,thelinearmatrix
inequalityconstraintM(ρ,T)⪯0simplifiestosatisfyingtwosemidefiniteconditionsasfollows,
(cid:40)
β′2Wk⊤TWk ⪯ρI,
Ak⊤Ak ⪯T.
Weclaimthattheoptimalsolutionforthissystemofconstraintsisgivenby
(cid:40)
T∗ =Ak⊤Ak,
ρ∗ =β′2∥Wk⊤T∗Wk∥ .
2
Thechoiceofρ∗ istrivial. Next,itiseasytoseethatifweinsteaduseT′ = T∗+E,whereE isanothernon-negative
diagonalmatrix,wewillhaveWk⊤T′Wk =Wk⊤(T∗+E)Wk =Wk⊤T∗Wk+Wk⊤EWk ⪰Wk⊤T∗Wk,wherethe
lastinequalityfollowsasWk⊤EWk isarealsymmetricpositivesemidefnitematrix.Thisconcludestheproofofoptimality
oftheproposedsolutionforthecaseinwhichα′ =−β′.
Next, weconsiderthescenarioinwhich|α′| < β′, weobservethatafunctionthatisslope-restrictedin[α′,β′]isalso
slope-restrictedin[−β′,β′]. Consequently,theproposedρ∗isafeasiblepointinthiscase,althoughitmaynotbeoptimal.
Thecaseinwhich|β′|<|α′|followsthesameargument,yieldingafeasiblesolutionρ∗ =|α′|∥WkT∗Wk∥ .
√ 2
Thus,wealwayshaveLp ≤L ∥ T∗Wk∥ =Lp,SDP .
dhk ϕ′ 2 dhk
CorollaryA.2. Letp=2,andconsiderthemaphk(x)=Φ(Wkx)wherethederivativeoftheithactivationfunctionis
√
slope-restrictedin[α′,β′]. ThenLp (x)≤∥D T∗Wk∥ ,whereDisadiagonalmatrixwithD =max{|α′|,|β′|}.
i i dhk 2 ii i i
B.CalculationofAnchoredLipschitz
Inthissection,weelaborateonsomeaspectsoftheanchoredLipschitzcalculationthatweintroducedinthemaintext.
Consideracontinuouslydifferentiablefunctionϕ:R(cid:55)→R. TheanchoredLipschitzconstantofϕataxisgivenby
|ϕ(y)−ϕ(x)|
L (x)=max . (25)
ϕ y̸=x |y−x|
This optimization problem can be solved on a case-by-case basis. For example, for the case of the tanh function, the
maximizerof(25)isthepointfromwhichthetangentpassesthrough(x,ϕ(x)). Thisisgivenbysolvingthenonlinear
equation
|ϕ(t)−ϕ(x)|
|ϕ′(y)|= lim . (26)
t→y |t−x|
SeeFigure1forreference. Asimilarideafollowsforotherboundedactivationfunctions. Wenotethat(26)isingenerala
nonlinearequationwithoutaclosed-formsolution. Inpractice,weuseanumericalmethod(likebisection)tosolvethis
nonlinearequationatinitiationforagridofpointsofthereallineandthenquerythesevalueswhenevertheyareneededfor
Lipschitzcalculation.
Next,considerasingleresidualblockasin(11)
h(x)=Hx+GΦ(Wx).
WiththedefinitionofanchoredLipschitz,onecanusethelocalnaiveboundfortheLipschitzconstanttoobtainthefollowing
boundontheLipschitzconstantofh,
p,naive
L (x)=∥H∥ +∥G∥ ∥diag(L (x))W∥ ,
h p p Φ p
p,naive
whereL (x)=[L (x),··· ,L (x)]. ThenL (x)wouldbeanupperboundontheanchoredLipschitzofh. This
Φ ϕ1 ϕn1 h
canbeadaptedtoLipSDP(Fazlyabetal.,2019)orLipLT(Fazlyabetal.,2023).
Theaboveanalysiscanextendedtomulti-layerresidualneuralnetworks. Forexample,multiplyingthelayer-wiseanchored
LipschitzboundswillyieldtheanchorednaiveLipschitzboundforthewholenetwork.
17CompositionalCurvatureBoundsforDeepNeuralNetworks
C.TimeComplexity
WeanalyzethetimecomplexityofAlgorithm1whentheℓ normisused(p=2). Weutilizethepowermethodtocalculate
2
thematrixnorms. Weassumethatweperformonlyasingleloopofpoweriterationtocalculatethematrixnorms. This
assumptionisjustifiedinpreviouswork(Fazlyabetal.,2023;Huangetal.,2021). Weprovidethecomplexityintermsof
thenumberofmultiplications. Forageneralneuralnetworkoftheformofequation(11)wehavethefollowingcalculations:
• Lipschitzconstantofeachresidualblock,i.e. Lp :O(n n +n n′ +n′n ).
hk k+1 k k+1 k k k
• LipschitzconstantoftheJacobianofeachresidualblock,i.e.,Lp :O(n n′ +2n′n )orLp :O(n n′ +
Dhk k+1 k k k dhk k+1 k
n n′n ).
k+1 k k
• Lipschitzconstantofthesubnetworkfromthefirstlayertothe(k+1)-thlayer,i.e.,
k k k
Lp :O((cid:88)(cid:2) n n +n′n + (cid:88) (n n +n n′ +n′n )(cid:3) +(cid:88) (n n +n n′ +n′n )).
k+1 j+1 j j j i+1 i i+1 i i i i+1 i i+1 i i i
j=0 i=j+1 i=0
However,itisworthmentioningthatthecomputationalcomplexityofaforwardpassthroughthenetworkisalsoasumof
quadraticterms,i.e.,O((cid:80)K n n +n n′ +n′n ).Thus,themainbottleneckwouldbethecalculationofLp
k=0 k+1 k k+1 k k k k+1
p
andL . WeleveragethespecializedGPUimplementationofLipLT(Fazlyabetal.,2023),whichsubstantiallyreducesthe
dhk
p
timecomplexityofcalculatingL ,k =0,···K.
k
p p
Furthermore,byusing1-Lipschitznetworksasdoneinsomeoftheexperiments,thetimecomplexityofL andL
k+1 hk
wouldbeO(1).
D.Experiments
Inthissection,weprovidethedetailsofourmethodsandprovidefurthersupportingexperiments.
D.1.ImplementationDetails
Weusedthreedifferentarchitecturesinourexperiments. WeshowconvolutionallayersintheformC(c,k,s,p),where
cisthenumberoffilters,k isthesizeofthesquarekernel,sisthestridelength,andpisthesymmetricpadding. Fully
connectedlayersareoftheformL(n),wherenisthenumberofoutputneuronsofthislayer. Furthermore,residuallayers
oftheform(11)isdenotedbyanextracharacter‘R’,i.e.,CRandLRforresidualconvolutionalandfully-connectedlayers,
respectively. Thedetailsofourarchitecturesareasfollows:
• 6C2F:C(32,3,1,1),C(32,4,2,1),C(64,3,1,1),C(64,4,2,1),C(64,3,1,1)C(64,4,2,1),L(512),L(10).
• 6F:L(1024),L(512),L(256),L(256),L(128),L(10).
• Lip-3C1F:CR(15,3,1,1),CR(15,3,1,1),CR(15,3,1,1),LR(1024).
Forthehyperparameterλusedintheloss(18),weemployaprimal-dualapproach. Thatis,aftereachmini-batchweupdate
λ+ =min{λ+η(A −ε),λ },
c min
where λ is the current value of the regularizer, η is a step size, A is a moving average of the training accuracy of the
c
mini-batches, ε is the minimum train accuracy that we expect from the model, and λ is the smallest value for the
min
regularizer. FortrainingonCIFAR-10,wechoose(η,ϵ,λ )=(0.05,0.6,0.01).
min
Therestofthetrainingdetailsareasfollows. Weusethemodifiedcross-entropylossfunctionfrom(Prach&Lampert,
2022),
√
f(x)−ν 2Lu
Lτ,ν(f(x),y)=τ ·CE( y,y), (27)
CE τ
whereτ isatemperatureconstant,u istheone-hotencodingofthevalueofy,andLisanℓ Lipschitzconstantofthe
y 2
model. ν isazero-onevariablebasedonthearchitectureandmodeoftraining. Forthe6C2Fand6Fmodels,aswewant
18CompositionalCurvatureBoundsforDeepNeuralNetworks
Figure 7: Comparison of certified radii acquired via CRC/CCRC on a 6-layer neural network trained via curvature
regularizationonMNIST.(Left)Histogramofper-sampleradiusimprovementofourmethodover(Singla&Feizi,2020).
(Right)Plotofcertifiedradiiforcorrectlyclassifieddata. ThedataaresortedaccordingtotheCRCradii.
250 1400 Lipschitz Certificates
Curvature Certificates
1200
200
1000
150
800
100 600
400
50
200
0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75
Radius Difference Certified Radius
Figure8: ComparisonofcertifiedradiicalculatedthroughCurvatureCertificatesversusLipschitzCertificatesfora6-layer
neuralnetworktrainedviacurvatureregularizationonCIFAR-10. (Left)Histogramoftheper-sampleradiiimprovements.
(Right)Histogramofcertifiedradii.
regularizationdirectlythroughthecurvatureconstant,wesetν =0. FortheLip-3C1Fmodel,wesetν =1astheoriginal
(Araujoetal.,2022)work. Weuseτ =0.25. Furthermore,wetrainourmodelsfor1000epochswithabatchsizeof256
withacosineannealingstrategywithaninitiallearningrateof10−4andafinallearningrate10−5,andreporttheaverage
resultsontwoseedinTable1.
D.2.Per-sampleImprovement
Expandingonthe“ComparisonwithotherCurvature-basedMethods”experimentinSection4,weprovidetheper-sample
improvementsofthecertifiedradiiinFigure7,correspondingtoFigure3.
D.3.TrainingwithDirectCurvatureRegularization
We observed that for the models that are trained with direct regularization of the curvature, first-order certificates are
significantlybetterthanzeroth-ordercertificates,i.e.,byregularizingthemodel’scurvature,Proposition2.2wouldholdfor
allpointsofthetestdataset. ThisisshowninFigure8forthe6Fmodel.
D.4.AttackCertificateson1-Lipschitzmodels
Inthisexperiment,weprovideradiiforprovableattacksona1-LipschitzmodeltrainedontheCIFAR-10dataset. The
curvaturerequiredforthiscertificatewascalculatedusingAlgorithm1andutilizingthe1-Lipschitzstructure. Figure9
showsthebudgetrequiredforasubsetofthecorrectlyclassifieddatapointsthatcancertifiablybeattacked. Weconsiderthe
testsamplesoftheCIFAR-10dataset,whichincludes10,000samples. Themodel’saccuracyonthetestsetisapproximately
50%,resultinginabout5,000correctlylabeledsamples. Ofthese5,000samples,wecanprovideanattackcertificatefor
19CompositionalCurvatureBoundsforDeepNeuralNetworks
17.5
15.0
12.5
10.0
7.5
5.0
2.5
0.0
0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07
Attack Radius
Figure9: Attackradiicertificatesfora1-Lipschitzstructure.
approximately150ofthem. Thistranslatestoa3%successrate(150/5000)fortheattackcertificateamongthecorrectly
classifiedtestsamples. Itisworthnotingthatthesesamplescanallbeprovablymisclassifiedwithanattackbudgetofless
than0.07,evenon1-Lipschitznetworks.
20