Online Frequency Scheduling by
Learning Parallel Actions
Anastasios Giovanidis1∗, Mathieu Leconte1∗, Sabrine Aroua1, Tor Kvernvik2, David Sandberg2
1Ericsson Research, Ericsson France, 25 Carnot, Massy 91300, France
2Ericsson Research, Ericsson AB, Torshamnsgatan 21, Kista, Stockholm 164 83, Sweden
Email: {firstname.lastname}@ericsson.com
Abstract—Radio Resource Management is a challenging topic Equipments (UEs), with the aim to maximize some mea-
in future 6G networks where novel applications create strong sure of performance (sum-throughput, fairness, etc). Further
competition among the users for the available resources. In this
extensions of this challenging problem include solving for
work we consider the frequency scheduling problem in a multi-
beamforming and power control, as well as link adaptation.
user MIMO system. Frequency resources need to be assigned
to a set of users while allowing for concurrent transmissions To enhance the network’s spectral efficiency, multiple UEs
in the same sub-band. Traditional methods are insufficient to canbescheduledsimultaneouslyoverthesametime-frequency
copewithalltheinvolvedconstraintsanduncertainties,whereas blocks, by exploiting multiple antennas in both transmitter
reinforcement learning can directly learn near-optimal solutions
and receiver side, with the MU-MIMO paradigm [3]. An-
forsuchcomplexenvironments.However,theschedulingproblem
other important challenge is that resource allocation decisions
hasanenormousactionspaceaccountingforallthecombinations
of users and sub-bands, so out-of-the-box algorithms cannot be should be taken in real-time, within delays below 1 [msec]
used directly. In this work, we propose a scheduler based on for 5G and even lower for 6G [4]. The general RRM is
action-branching over sub-bands, which is a deep Q-learning known to be a mixed-integer problem [5] and decisions need
architecture with parallel decision capabilities. The sub-bands
to be taken with uncertain state information. Considering also
learncorrelatedbutlocaldecisionpoliciesandaltogethertheyop-
the potentially competing UE service requirements, the RRM
timizeaglobalreward.Toimprovethescalingofthearchitecture
withthenumberofsub-bands,weproposevariations(unibranch, task will become more and more complicated in the 6G and
GNN-based)thatreducethenumberofparameterstolearn.The can be better handled by AI tools and more specifically by
parallel decision making of the proposed architecture allows to Reinforcement Learning (RL), which should also adapt to
meetshortinferencetimerequirementsinrealsystems.Further-
evolving environments (traffic mix, channel). The compute-
more, the deep Q-learning approach permits online fine-tuning
heavy scheduling workloads from AI-based algorithms will
after deployment to bridge the sim-to-real gap. The proposed
architectures are evaluated against relevant baselines from the require eventually the use of GPUs for hardware acceleration
literature showing competitive performance and possibilities of [6], [7], [8]. A big challenge is then to adapt the size of
online adaptation to evolving environments. AI models to available memory limitations, and to propose
Index Terms—scheduling, reinforcement learning, action solutions whose inference time (e.g. forward pass of a neural
branching, graph neural networks, multi-user MIMO.
network) requirements are a fraction of the slot duration.
I. INTRODUCTION A. Related work
The road to 6G comes along with unique challenges raised There exists a series of research works that apply RL to
bynewtypesofapplications,suchasenhancedMobileBroad- the RRM problem. Motivated by AlphaGo Zero the authors
band(eMBB)forhighqualitymultimediaservices,interactive in [9] propose a Monte Carlo Tree Search (MCTS) RL
gaming, Augmented/Virtual Reality (AR/VR) and massive algorithm to solve the frequency scheduling with MU-MIMO
Ultra Reliable Low Latency Communications (mURLLC). userassociation.Theauthorstakesequentialdecisionsoverthe
These enforce strong requirements for even higher data rates, frequencybandsinagivenorder,andincorporateself-attention
lower latency and the support of a large and versatile number mechanisms to account for possibly interfering co-scheduled
of mobile devices, rendering rule-based or optimisation-based users. In another work, LEASCH [10] applies Double Deep
methods inefficient and necessitating Artificial Intelligence Q-Network (DQN) to learn the optimal time-frequency block
(AI) empowered solutions [1]. Essentially, the telecom in- allocationpolicyforasetofusersusingchannelstate,buffers,
dustry’s vision is “AI-native”, i.e. the concept where AI is HARQ, and allocation logs as input. The algorithm simplifies
a natural part of the network functionality, in terms of design, considerably the decision process, by allowing at most one
deployment, operation, and maintenance [2]. user per resource block (no MU-MIMO), and by deciding
Radio Resource Management (RRM) plays an important sequentially over blocks based on an input fairness metric,
role in satisfying the diverse future service requirements, while assuming perfect knowledge over achievable rates. The
by allocating dynamically time-frequency resources to User workin[11]appliesaDuelingDQNtosolvethejointnetwork
selection and sub-band frequency allocation problem for both
*Theseauthorscontributedequallytothework. terrestrial and aerial UEs. The large number of users and
4202
nuJ
7
]IN.sc[
1v14050.6042:viXracarriersconsiderablyincreasestheaction-spaceandchallenges C. Our contributions
the problem’s feasibility. The authors in [12] solve the RRM
We propose a Deep Q-Network (DQN) solution to the
time-frequency scheduling problem for slicing using DQN.
discrete scheduling problem that can overcome the above
The actions are the numerology choice and the number of
limitations by allowing for parallel (i.e. distributed) infer-
resourcesperUE.ThisisaLayer-3schedulingproblem,with- ence over all sub-bands simultaneously. We use the Action-
out resource sharing among UEs and without explicit channel Branching (AB) network architecture [16], where each sub-
knowledge.DQNwithAction-Branchingisappliedin[13]for
band is modelled as a separate branch that learns the locally
uplink power control. The simultaneous optimization of two
optimal actions. Coordination over branches is achieved by
correlatedvariablesisperformedastwooutputparallelbranch
learning a common shared representation of the joint input
decisions. This approach is proposed to reduce the action-
state, consisting of the complex channel state and buffer state
size of the problem instead of enumerating all possible action
of all UEs. An extra mechanism of coordination is introduced
combinations from both variables. An interesting approach
inthelossfunctionbyintroducingvaluedecompositionofthe
named GQN was introduced in [14] to remotely control the
common reward among the individual branches [15]. Also,
antenna-tilt and power for a number of base-stations in a
the relatively low compute requirements for training and the
cooperative manner. GQN combines graph neural networks
replay buffer architecture makes DQN applicable to cases
(GNN)withper-agentDQN,sothatthepolicycanlearnfrom
where learning from realtime interactions is required.
message-passinginteractionsbetweenagents.Thenetworkcan The introduced approach shows high performance but uses
learnusingasingleglobalrewardoverallagents,whereasthe large neural networks with a number of parameters that
global value function is decomposed among agents through increaseslinearlywiththenumberofbranches.Weinvestigate
value decomposition methods [15]. an alternative architecture where, after the shared representa-
tion block, a common sub-network (unibranch) is used for
B. Challenges both training and inference for all branches, thus reducing
considerably the number of parameters and allowing for the
In this work we consider the Layer-2 problem of frequency architecture to scale for very large number of branches. The
schedulinginthedownlinkwithMU-MIMOassociation,sim- common network uses as extra input the preprocessed local
ilar to [9]. It is combinatorial with very large action space, state(i.e.userchannelsonspecificsub-bandandtheirbuffers)
which consists of all possible user allocations over all sub- aswellaspositionalencodingforthebranchlabelinformation.
bands. To give a numerical example, for just 4 available users Wemakeafinalstepbyreplacingthesharedrepresentation
scheduled over 10 sub-bands, with at most 2 MU-MIMO block with a Graph Neural Network (GNN) [17], where we
users per sub-band, there are 1010 possible actions, rendering assume that a sub-band is a node on a graph whose edges are
vanilla RL solutions intractable. The suggested approaches in their pairwise interactions. The GNN takes as input the local
[9] and in [10] (the latter without MU-MIMO) propose to state, like in the unibranch case above, and outputs a learned
solve the problem autoregressively over frequency bands, by local representation for each branch. The latter is fed into a
associating users one sub-band at a time and then feeding the common DQN network for all branches, to learn the optimal
pastdecisionsasinputstatesforthenextsub-band.Thebenefit local actions, combined together using value decomposition.
of such approach is that it decomposes the general problem The remainder of the paper is organised as follows. Sec-
into smaller tractable sub-problems. At inference time (i.e. tion II formalises the scheduling problem under study. Sec-
when the algorithm is deployed in a radio network) actions tion III introduces the RL solution based on action-branching,
are produced per sub-band but without relying on MCTS whichallowsforreducedinferencelatencyduetoparallelism.
which reduces the latency by several orders of magnitude. Then, Section IV presents the memory efficient architectures
One drawback with the autoregressive approach is however with unibranch and GNNs, which use a smaller network but
that the inference time increases linearly with the number a part of the inference is done sequentially. The experimental
of sub-bands, since the learned network should be queried environment, the baselines used for comparison, as well as
as many times as the number of sub-bands to yield a full the performance evaluation are presented in Section V. All
scheduling decision. This is prohibitive however, because solutions are compared in terms of their performance and
of the aforementioned tight delay requirements well below achieved trade-off between inference time and network size.
1[msec]. Another drawback with the approach taken in [9] The possibility for online adaptation of the trained DQN
relates to how training is performed. This approach relies on architectures to realtime environments is showcased. Finally,
MCTS to compute policy targets to use for neural network Section VI concludes our work.
training. This works very well in a non-realtime setting (e.g.
trainingusinginteractionswithasimulator)butduetothevery
II. PROBLEMSTATEMENT
high computational requirements from MCTS it is extremely WeconsidertheproblemofschedulinganumberN >1of
u
challenging to apply in realtime settings. In many cases the users over time-frequency blocks. We take decisions over dis-
UE receiver structure, and thereby the quality of a scheduling cretetimeslotst=1,...eachwithduration∆ [msec].There
t
decision, is unknown at training time. It would therefore be are N >1 available sub-bands of size ∆ [Hz] with carrier
s f
beneficialtolearntherewardsfrominteractionswithrealUEs. frequency f [Hz] and total bandwidth BW = N ∆ [Hz].
0 s fThisisasequentialdecisionmakingproblemovertime,butthe This transition quantifies channel correlation as well as buffer
association over all sub-bands should be done simultaneously evolution due to service and random arrivals.
per slot. We describe it as a Markov Decision Process (MDP) Discount: The discount factor γ ∈ [0,1] weights the
characterised by the tuple {S,A,P,R,γ}. importanceoffuturerewardsinthecumulativesumofrewards
States:S isthesetofalljointUEstatespertimeslot.Each over a long horizon.
user provides information about their instantaneous estimated
We are looking for an optimal stationary user association
channel, as well as their buffer state. The channel state for all
policyπ :S×A→[0,1]tomaximisetheexpectedcumulative
users is a complex tensor with dimensions (N ×N ×N ×
u s rx discounted reward from any initial state s ∈S at time t=0.
0
N ), where N is the number of receive UE antennas and
tx rx The value function and state-action value function are
N the number of station transmit antennas. The estimation
tx
diverges from the exact channel due to noise in measurement, (cid:34) ∞ (cid:35)
(cid:88)
aswellasUEmobility.ThebufferstateisanN u-dimensional V π(s 0) := E
π
γtr t|s
0
, (2)
vectorwithpositiveintegersfrom0toamaximumbuffersize.
t=0
Actions: The set of actions A is discrete and refers to all (cid:34) ∞ (cid:35)
(cid:88)
possible combinations of associating the N u users to the N s Q π(s 0,a 0) := E π γtr t|s 0,a 0 . (3)
sub-bands. A user may be associated in general to more than t=0
one sub-band. The number of possible user associations per
sub-band, given at most M ≥ 1 co-channel transmissions in Thereasonwhywechosethespecifictypeofmyopicreward
MU-MIMO, is equal to N = (cid:80)Nu (cid:0)Nu(cid:1) . E.g. for N = 4 in eq.(1) is because the frequency scheduling problem is in
a x=1 x u
usersandM =2thereareN =10possibleassociations.The practical cellular systems distinguished from time scheduling.
a
globalactionsetoverallsub-bandsisA=A(1)×...×A(Ns) The latter is a separate function which keeps track of the
with cardinality |A| = NNs. As a numerical example, for bitrateanddelayexperiencedbyeachuserandschedulesthem
a
N = 4 users that need to be scheduled over N = 10 sub- aiming for specific performance intents over time. The same
u s
bands and M = 2 the action set has |A| = 1010 possible reward was also suggested in [9] for these reasons. To apply
actions. In realistic scenarios we may need to decide over the MDP to such myopic problem we can choose γ = 0 for
N =10 users over N =20 sub-bands. the discount factor and the problem simplifies to finding the
u s
Reward: The reward function R : S ×A → R maps the optimal solution to a very large discrete optimisation problem
+
currentstate-actionpair(s,a)toone-dimensionalrealpositive over given channel and buffer states. However, we need to
reward r, related to some scheduling performance metric over underline that the MDP formulation (and the DQN solution
all sub-bands and all users. In our work the reward per time approachinthenextsections)ismoreversatile.TheMDPcan
slot is the sum of Throughput-To-Average over all users [5], learnthelong-termoptimalfrequencyallocationpolicywithin
that quantifies myopic Proportional Fair (PF) decisions. The a window of time-slots, by setting the γ factor appropriately,
reward is calculated as in [9]. First a precoder is chosen per this being a major advantage compared to myopic solutions.
user and sub-band based on the estimated channel per slot.
Next, the resulting Signal to Interference plus Noise Ratio
(SINR) per user and sub-band is derived and their Transport III. METHODOLOGY
Block Size (TBS) is computed by a simple link adaptation
algorithm. Finally, the achieved rate per user R is calculated A. Deep Q-Network
k
by scaling the TBS with the success probability (one-minus
Block Error Probability). The PF utility function used is Inrealwirelessenvironmentsthetransitionprobabilitiesare
unknown, whereas the reward function cannot be explicitly
r =
ν(cid:88)Nu R
k. (1)
givenduetoverylargestatespaceandnon-idealuserchannel
estimation. Instead, by probing the state the corresponding
R
k=1 k reward can be sampled. Note that the reward can be fed back
IntheaboveR andR aretheinstantaneousandaveragerate with some delay in online settings, which is not critical for
k k
foruserk,wheretheaveragerateusesthechannelslow-fading training as we will clarify later. To solve the MDP for a real
values and a random allocation. ν is a normalising constant. environmentofchannelconditionsandrequestarrivals,wecan
The above metric quantifies the relative rate advantage of use a model-free approach, where the optimal actions can be
allocatingspecificsub-bandstoausercomparedtoanaverage learned using value-based methods such as Q-learning [18].
allocation and channel realisation. It is very relevant for Such choice of method is appropriate for discrete action sets.
Mobile Broadband (MBB) traffic whose quality of experience To account for very large state spaces (complex channels
(QoE) depends on the bitrate. For delay sensitive services and buffers) a parameterised value function Q(s,a;θ) should
utility metrics based on delay would be more appropriate. be learned, where θ are parameters of a deep network that
Transitions: The function P :S×A×S →[0,1] describes inputs the current system state s and outputs the vector over
the transition probability to the next state s′, given the current allpossiblestate-actionvalues.This deepnetworkistunedby
state-action pair (s,a) and is unknown in real environments. experience and can generalise to unseen states. The Deep Q-Network approach [19] optimally tunes the θ parameters by
minimisingovertheexpectedtemporaldifferenceTD(0)-error
(cid:104) (cid:105)
L(θ) = E (y−Q(s,a;θ))2 , (4)
s,a∼ρ
(cid:20) (cid:21)
y = E r+γmaxQ−(s′,a′;θ−)|s,a . (5)
r,s′∼E
a′∈A
The (s,a) pairs are sampled from a behavior distribution
which trades-off between exploration and exploitation. The
standard option is the ϵ-greedy policy, where with probability
ϵ(t)arandomactionisselected,whilewithprobability1−ϵ(t)
thegreedyactionisapplied.Theexplorationprobabilitycools-
down as the time t evolves. The distribution E is over all Fig.1. Actionbranchingarchitecture
possiblerewardsr andnextstatess′ theenvironmentcanran-
domlytransitiontofrom(s,a),dependingontheenvironment
an agent with action space A(d) of cardinality |A(d)| = N
dynamics. It should be noted that due to the relatively low a
andshoulddecideoverthebestlocalaction,givensomeinput.
computational complexity of the DQN these updates can be
In that case, the total number of actions for the problem can
executed in a base station in real time.
bereducedtoN N ,e.g.forthetoyexample,thetotalactions
PrioritizedExperienceReplay:Atsteptthesamplesfrom s a
now reduce to just 10·10 = 100 (i.e. 10 per sub-band). The
ρ and E form an experience e = (s ,a ,r ,s′ ) that is
t t t t t+1 big challenge, however, is that the agents should cooperate
pushedinalistoffixedlengthL,theso-calledreplaymemory
in order to maximize their joint reward R given in the MDP
D[19].Ateachloss-gradientupdateoftheparameters,abatch
of the original problem. An important extra benefit of this
ofB experiencesissampledatrandomfromthememory,thus
approach is the parallelisation of the decision making among
avoiding strong correlations between samples, and allowing
theagents,whocanalldecidesimultaneouslyabouttheirlocal
each experience to be re-used in many updates. We make
action, thus reducing the delay in the forward pass, compared
use of prioritized experience replay [20], where samples with
to sequential implementations over agents.
high learning progress are more frequently sampled from the
A good candidate method for discrete actions is the action
memory. Priority per experience (s ,a ,r ,s′ ) is quantified
t t t t+1 branching, introduced in [16]. The main idea of the proposed
by the absolute TD(0) difference. Sampling from memory is
architectureistodistributetheaction-makingacrossindividual
controlledbytwoexponents(α,β)relatedtoprioritizationand
network branches (one agent aka branch per sub-band), but at
importance sampling, respectively.
thesametime,maintainacommonsharedlatentrepresentation
Online Updates: In an online setting the result from a
of the input to help with the coordination of the branches.
downlink transmission after associating a user to a sub-
The proposed network architecture is shown in Fig.1. It
band will be available to the scheduler once the UE has
consistsofthefollowingblocks:(i.)Preprocessing,(ii.)Shared
acknowledged a reception and the result has been fed back
representation, (iii.) Value-function, (iv.) Advantage block per
to the base station over the Hybrid Automatic Repeat reQuest
sub-band, (v.) Dueling block per sub-band.
(HARQ) feedback channel. Hence, an extra delay is inherent
in the way the reward is calculated, when taking Block Error
C. Detailed Architecture
Probability (BLEP) into account as in our case. This delayed
The input consists of a batch of B joint states of com-
outcome can be handled by updating the replay buffer in two
plex channels for all N users over the N sub-bands, and
steps: (s ,a ) are updated first as soon as the action is chosen u s
t t
their buffer state. The complex channel input has dimension
and (r ,s′ ) are updated in a second step when the reward
t t+1 (B,2N ,N ,N ,N ), where the real and imaginary parts
and next state are fed back. Experience replay and random u s rx tx
are concatenated, N is the number of receive antennas at
sample updates permit such flexibility. rx
the user side and N the number of transmit antennas at the
Target network: In eq.(5) the state-action value to find the tx
base station.
maximumanditsparametersaremarkedbyaminus(−).This
Preprocessing: The input is preprocessed separately for
denotesatargetnetwork,whoseparametersareupdatedbysoft
eachsub-bandasintroducedin[9].Thechannelinformationis
iterationsslowerthantheactualQ-network.Thisapproachcan
transformed into engineered features aiming to quantify how
offer stabilization in DQN convergence [19].
muchco-scheduleduserscouldinterferewitheachother.They
B. Action Branching represent the channel of each user-pair by three scalars, the
The big challenge in the multi-user frequency scheduling magnitude of the dot product between the channel vectors of
problem is the enormous number of actions, equal to |A| = user pairs, the Hermitian angle and Kasner’s pseudo angle.
NNs, as explained in Section II. Such numbers of actions The output of this preprocessing step is a flat 1-dimensional
a
render the classical DQN method infeasible. vector of size N N for each sub-band, with number of
u feat
Theapproachwetakehereisinspiredbythemulti-agentlit- featuresperuserandsub-bandN =2+3N ,includingthe
feat u
erature [21]. Each sub-band d=1,...,N can be considered normalized buffer size and channel power. These vectors then
sgo through a dense layer to obtain a fixed-sized embedding wheretheexperience(s,a,r,∗)issampledfromtheprioritized
for the input of each sub-band, replaybufferD,butthenextstates′ neednotbeconsideredin
s(d) =p(s(d)), d=1,...,N . (6)
themyopiccase(sowemarkby∗).Thea=(a(1),...,a(Ns))
pre s is the concatenation of local actions over all sub-bands.
Shared Representation: The feature vectors of each sub- In practice the factorization in eq.(9) ensures that a set
band are then concatenated as s pre and enter the shared of parallelized individual argmax operations performed over
representation module (ii), which is a multi-layer perceptron individual Q(d)-functions to select greedily the optimal ac-
(MLP) with linear layers and ReLU activation. The set of its tion per branch, yields the same result as a global argmax
parameters is ϕ. The output is a common embedded state of on the global Q-function over the joint agent actions a =
size N
hidden
used as input for all branches. (a 1,...,a Ns).Noticethattherearemanypowerfulalternatives
totheVD,suchustheQMIX[23]whichfactorizestheglobal
s (ϕ)=f (s ;ϕ). (7)
emb MLP pre
Q-value assuming monotonicity, and the Deep Coordination
Branching with Dueling: We apply the dueling method Graphs [24], [25], which factorize not only into individual
[22] with action branching, to produce a separate estimate utilityQ-functionsbutadditionallyusingpairwisepayofffunc-
of the value function V(s) and a separate estimate for the tionsamongagents.Here,wefoundVDtocombinesimplicity
advantage functions. This architecture has been shown to lead with performance.
to better policy evaluation in the presence of actions with
similar state-action values. The value function module (iii) is
IV. MEMORY-EFFICIENTARCHITECTURES
anMLPblockwithparametersω,inputs andscalaroutput. The action branching architecture may be difficult to scale
pre
For each of the d=1,...,N parallel branches the common to large number of parallel decisions, because it requires a lot
s
embedded state s is input to each local MLP block of parameters. The two main reasons are that the number of
emb
(iv.) with parameters θ(d) that learns the advantage function parameters of the branches N ·θ(d) scales linearly with the
s
A(d)(s ,a(d)) for all possible local actions a(d) ∈ A(d). number of branches, and that the shared representation has to
emb
Followingtheproposalin[22],weobtainthelocalstate-action containusefulinformationforallthebranchesatthesametime
value Q(d) by aggregating the advantage and value functions and will thus tend to be large. In this section, we evolve from
at the (v.) dueling block as follows thebasicactionbranchingarchitecturetoattainbetter-scaling,
more parameter-efficient architectures. The first evolution is
Q(d)(s,a(d);ϕ,ω,θ(d))=A(d)(s,a(d);ϕ,θ(d))+
to have the branches share parameters, which we call the
 
1 (cid:88) unibranch architecture. The second evolution will avoid the
+V(s;ω)−
|A(d)|
A(d)(s,a′;ϕ,θ(d))(.8)
shared representation and let each decision dimension build
a′∈A(d) its own local representation by employing a Graph Neural
In the above, subtracting the average of advantages instead of Network (GNN) architecture.
the maximum increases the stability of the optimization.
A. Unibranch
D. Loss function
The advantage function in the action branching architecture
As mentioned, we will focus on the myopic scheduling was A(d)(s,a(d);ϕ,θ(d))=A(d)(s (ϕ),a(d);θ(d)). Sharing
emb
case, where the discount factor is γ =0, in order to compare theparametersofthebranchesbysimplyhavingθ(d) =θ,∀d,
with other myopic frequency-schedulers in the literature. The wouldleadtoallthebranchestakingthesamedecision.There-
temporal difference target in (5) now simplifies to y = r, fore, we add a second input to each branch, that is branch-
and we need not discuss ways to combine the N s branches dependent. The preprocessing step was actually computing a
of Q(d) in the reward-to-go. In [16] the loss was defined as separate embedding for each branch input: s(d) = g(s(d)),
pre
the expected value of the mean squared TD error averaged d=1,...,N . We concatenate this branch-specific state s(d)
s pre
over all branches d, i.e. the expected value of the quantity
tothesharedrepresentations (ϕ)toformthebranchinput:
1 (cid:80)Ns (cid:0) r−Q(d)(s,a(d))(cid:1)2 .Thiswayeachbranchapprox- emb
iN ms atesd= th1 e joint total reward r with its local Q(d) value. We A(d)(s,·;ϕ,θ)=f MLP([s emb(ϕ),s p(d r) e];θ), ∀d. (10)
have found empirically that this approach under performs.
The unibranch is again implemented using an MLP block.
ValueDecomposition(VD):Itismorebeneficialtoassume
Its input is slightly larger than in the initial action branching
that each of the local Q(d) values contributes partially to the architecture,asthelocalbranch-specificinputs(d) wasadded,
pre
total reward r. Hence, the idea of VD for cooperative multi-
yetthesharedrepresentationistypicallymuchlargerthanthis
agent learning [15] is applied, which states that the global
local input, so the gain obtained by sharing parameters across
action value function in eq.(3) can be additively decomposed
branchesstilllargelydominates.Theunibrancharchitectureis
into local value functions among the branches Q(s,a) ≈
shown on Figure 2.
(cid:80)Ns Q(d)(s,a(d)). The loss now takes the form
d=1
(cid:32)
(cid:88)Ns
(cid:33)2 B. Graph Neural Networks
L VD(Θ)=E (s,a,r,∗)∼D r− Q(d)(s,a(d)) . (9) The action branching and unibranch architectures both suf-
fer from the relatively large size of the shared representation.
d=1m away from the antenna. Their channel model is described
by 3GPP TR 38.901 [27]. The buffers of the users are drawn
uniformly at random as N ∼ U(b ,b ). For more
bits min max
details on the experimental setup, the reader can refer to [9].
The environment parametersare shown inTable I.We usethe
same number of users and sub-bands as the evaluation in [9].
TABLEI
ENVIRONMENTPARAMETERS
Parameter Symbol valueorsize
Nr.users Nu 4
Nr.frequencybands Ns 10
Fig.2. Unibrancharchitecture
Nr.actionspersub-band Na 10
Maximumnr.userspersub-band M 2
ChannelModel 3GPPUrbanMacro
CarrierFrequency 3.5GHz
Deployment SingleCell
Transmitantennas Ntx 2
Receiveantennas Nrx 1
Slotduration T slot 1ms
Minimumbuffer(bits) bmin 400
Maximumbuffer(bits) bmax ∈{∞,4000,2000,1000}
Userspeed(m/s) v ∈{0,1,3,5}
Channelestimationquality(dB) SNRCE ∈{∞,20,10,0}
Evaluated architectures: The parameters of the evaluated
Fig.3. GNNarchitecture architecturesareshowninTableII.ForalltheMLPsinvolved,
the size of the hidden layers evolves linearly between input
and output. This leads to the architecture sizes in Table III.
Indeed,itneedstocapturealltheinformationrelevanttoallthe
decision dimensions. In order to reduce further the size of the
TABLEII
architecture, we move to only using local representations for
ARCHITECTUREDESIGNPARAMETERS
each action dimension. However, these local representations
Parameter Symbol valueorsize
still need to enable coordination of the decisions. To this end,
Localinput s(d) 56
we use a Graph Neural Network (GNN) architecture, which
allows the local representations to be jointly updated over Localrepresentation h( id),sp(d r) e 64
Sharedrepresentation s emb 640
multiple iterations N i. For the GNN architecture, we used Sharedrepresentation:nr.MLPlayers 3
a Graph Attention Network (GAT) [26] layer followed by Branches/Unibranch:nr.MLPlayers 2
GNNmessaging:nr.GATheads 3
an MLP layer to update the representation. The parameters
GNNrepresentationupdate:nr.MLPlayers 2
(φ,ψ)=ϕoftheselayersplaythesameroleastheparameters GNNiterations Ni 3
of the shared representation update of the previous architec-
tures, yet the local representations h(d) involved in this GNN
i
architecture are much smaller than the shared representation. TABLEIII
The advantage function is then computed as SIZEOFTHEARCHITECTURESVSINFERENCETIME
A(d)(s,a(d);ϕ,θ) = A(h(d),a(d);θ) Architecture trainableparameters relativeinferencetime
Ni alphaZero[9] 5.2·104 16.3
h(d) = f (h(d),h(−d);ϕ) ActionBranching 3.6·106 1
i+1 GNN (cid:16)i i (cid:17) Unibranch 1.7·106 1.07
= f f (h(d),h(−d);φ);ψ GNN 2.8·104 1.76
MLP GAT i i
Having more GNN iterations leads to more coordinated local Training process: To benefit from parallelization of the
representations,yetincreasesinferencetime,henceatrade-off DQN architecture in sample generation and decision evalua-
needstobefound.TheGNNarchitectureisshowninFigure3. tion,wetrainthepolicyinmultipleiterations.Ineachiteration,
we generate 1000 new environment samples and draw actions
V. PERFORMANCEEVALUATION
according to an ε-greedy exploration policy, with a decaying
A. Experimental setup
value of ε. After evaluating the reward for these samples,
Network Simulator: We use a similar performance evalu- they are added to a replay buffer of size L = 105. At each
ation setup as in [9]. The setup represents an urban macro- iteration, we run 100 optimization steps with batches of 256
cell as a circle sector of radius 500 m and central angle samplesfromthereplaybuffer.Additionaltrainingparameters
65°. Users are dropped at random in this area, at least 35 are mentioned in Table IV.Algorithm 1 Traditional Baseline Algorithm
TABLEIV
TRAININGPARAMETERS Iavail∈{j|j∈1...Ns}
Is,k←{}∀k∈1...Nu
Parameter Symbol valueorsize Iu,j ←{}∀j∈1...Ns
Totalnumberoftrainingsamples 106 whileIavail̸={}do
Totalnumberofoptimizationbatches 105
forj∈{i∈1...Ns||Iu,i|<M}do
fork∈1...Nudo
Batchsize B 256 ComputeUPFTF,k(Is,k∪{j})
Experiencereplaymemorysize L 105 ifIs,k̸={}then
Discountfactor γ 0 ComputeUPFTF,k(Is,k)
Experiencereplayprioritization α 0.7 else
Experiencereplaysamplingexponent β 0.5 UPFTF,k(Is,k)←0
endif
Optimizer AdamW
Learningrate 10−4
Λk,j ←UPFTF,k(Is,k∪{j})−UPFTF,k(Is,k)
endfor
Explorationmethod ε-greedy endfor
(k∗,j∗)←argmax (k,j)Λk,j
ifΛk∗,j∗ >0then
Is,k∗ ←Is,k∗∪{j∗}
B. Baseline policies Iu,j∗ ←Iu,j∗∪{k∗}
else
In our experiments we compare the proposed solution Iavail←Iavail\{j∗}
endif
against two baselines, one traditional and one alphaZero-
endwhile
based, which are both described below.
Traditional Baseline: The traditional baseline scheduler
usedinthisworkisbasedontheoptimizationmethoddefined alphaZero-based Baseline: We also compare to the
by [28, Alg. 7.1], which is a strong heuristic algorithm with alphaZero-based approach outlined in [9]. In this approach,
quadratic complexity. Some modifications are introduced to a neural network is trained using policy and value targets
supportschedulingofuptoM userspersub-band(seeAlg.1). fromMonteCarloTreeSearch.Trainingisdonebyalternating
Sub-bands are allocated to users in a way that maximizes between two phases. In a first phase, MCTS with 1000
the marginal utility (Λ ), i.e. the gain in the utility U simulations is used to find actions to maximize the associated
k,j PFTF,k
when an extra sub-band j is allocated to user k, compared to rewards. In a second phase, a neural network is trained to
the utility of user k before the allocation of sub-band j. The predict the action probabilities and the corresponding values.
optimization algorithm (including modifications) is outlined These two phases are iterated for 25 iterations, with 500
in Algorithm 1. Scheduling decisions are represented by I episodes in each iteration.
s,k
corresponding to the set of sub-bands allocated to user k and
C. Results & Discussion
I corresponding to the set of users allocated to sub-band j.
u,j
We view the scenario with perfect channel estimation
The transmit power is split equally between sub-bands and
(SNR = ∞), infinite buffers (b = ∞), and no user
between the users allocated to a sub-band. The maximum CE max
mobility (v =0) as a reference. We let only one parameter at
number of co-scheduled users M is set to 2 in this work.
atimedeviatefromthisreferencescenario.TableVshowsthe
The instantaneous rate for user k is calculated by scaling
performance in terms of PF metric compared to the baseline
thetransportblocksize(TBS)withthesuccessprobabilityand
scheduler. The performance reported in these tables is based
dividing by the slot duration T as
slot on104 samples.RecallthatthebaselineandalphaZerosched-
(1−BLEP )∗min(TBS ,N ) ulers take sequential decisions, while the Action Branching,
R = k k bits,k . (11)
k T Unibranch, and GNN schedulers are taking parallel decisions.
slot
Table V (Top) shows the performance with limited buffers.
Here, the block error probability BLEP is again calculated
k In these scenarios there is no uncertainty on the state, but
using the method in [29], N is the number of bits in the
bits,k thetraditionalbaselineschedulerdoesnotproperlycoordinate
buffer for user k and T is the slot duration.
slot the amount of spectral resources allocated to a user with the
AsoptimizationcriterionweusetheProportionalFairTime
quantityofdatainthatuser’sbuffer.Alltheevaluatedmethods
Frequency (PFTF) metric which can be written as
exceed baseline. The benefits become more visible as the
 (cid:80) R buffersizesdecrease,whichrequiresmorecoordinationamong
j∈Is,k∪{i} k,j , (cid:80) R T ≤N sub-bands. Table V (Medium) shows the performance with
U PFTF,k = 0R ,k+ j∈(cid:80) Is,kR k,j j o∈ thI es, rk wisk e,j slot bits,k u ths eer pv ree dlo icc ti ety d. cA ht a1 nnm e/ ls s,s wpe he id le,t ah te 3ch ma /n sn ae nl dag 5in mg /ssl ti hg eht ply rea df if ce tc et ds
channels widely differ from the true ones. All the machine
where R is the rate for user k in sub-band j in the current learning methods provide substantial gain over the traditional
k,j
slot based on Eq. (11), R is the average rate of user k over baseline in these scenarios. The Action Branching seems a bit
k
a time window excluding the current slot. It should also be behind compared to Unibranch and GNN. Table V (Bottom)
noted that the utility for user k is set to 0 when the buffer for showstheperformancewithimperfectchannelestimation.The
the user is emptied, i.e. when the number of bits for user k results show the same trend as with user velocity, with higher
exceeds its buffer size N . benefits for lower SNR values.
bits,kFig.4. CDFsoftherewardsobtainedbytheGNNarchitecturedividedbytherespectivebaselinereward,obtainedover104validationepisodes.Environment
variationsover(left)buffersize,(middle)userspeed,(right)SNRestimation.TheotherDQN-basedparallelarchitecturevariationsshowsimilarcurves.
TABLEV architecture, which avoids the large shared representation. In
RELATIVEPERFORMANCEOVERTRADITIONALBASELINEWITHVARIOUS Table III, we also show the inference times we obtain for the
(TOP)BUFFERS,(MIDDLE)VELOCITY,(BOTTOM)SNRESTIMATION
differentarchitectures.Theinferencetimesareevaluatedfrom
bmax alphaZero[9] ActionBranching Unibranch GNN 300 runs on the same GPU. We normalize the inference times
∞ 102% 104% 107% 105% by that of the Action Branching architecture, which is the
4000bits 112% 110% 112% 110% lowest. We observe that, although the alphaZero architecture
2000bits 118% 119% 120% 115%
seems competitive in size with the GNN, it suffers however
1000bits 131% 130% 135% 126%
from very high inference time, 16-times larger than the AB.
v alphaZero[9] ActionBranching Unibranch GNN
TheUnibranchhasalmostthesameinferencetimeasAB.The
0 102% 104% 107% 105%
GNN exhibits very interesting properties by managing in just
1m/s 117% 97% 120% 123%
3m/s 189% 155% 179% 182% 1.7 times the delay of AB to provide the same performance
5m/s 194% 160% 184% 187% with two orders of magnitude fewer parameters.
SNRCE alphaZero[9] ActionBranching Unibranch GNN Online Adaptation: To test the ability of the proposed
∞ 102% 104% 107% 105% methodstoquicklyadaptafterdeployment,wetraintheGNN-
20dB 100% 102% 107% 106%
version of our scheduler for a specific scenario: 10 users (56
10dB 133% 127% 133% 133%
actions per sub-band) moving at 1 m/s. We then ”deploy” it
0dB 262% 244% 256% 257%
on a related but different scenario: users now move at 3 m/s.
The model is fine-tuned on the deployment scenario using
Overall,theproposedparalleldecisionarchitecturesmanage new samples. Fig. 5 shows the performance evolution during
to take scheduling decisions of competitive quality with the this fine-tuning compared to a new training from scratch.
alphaZero-based approach, largely improving over the base- We can see that our DQN parallel decision-making scheduler
line. The comparison of Action Branching with Unibranch manages to quickly fill the initial performance loss caused by
suggests that training one common branch may be more the scenario mismatch. Note that such fine-tuning would not
efficient than training a different sub-network per branch. be possible for the alphaZero scheduler of [9]. For a TTI of 1
Though much smaller in number of parameters, the GNN ms,thenfori.i.d.dataouralgorithmadaptsin25stothenew
scheduler manages to reach the same level of performance environment, compared to 3 minutes if trained from scratch.
as the other two parallel architectures, but at the price of
more message-passing iterations (N > 1), and thus a larger
VI. CONCLUSION
i
inferencetimewhilestillsmallerthanthesequentialdecisions Inthisworkwehaveintroducednewreinforcementlearning
of the alphaZero-based approach, as we will see next. architectures that can efficiently learn the optimal user as-
InFig.4weplotthecumulativedistributionfunction(CDF) sociation for MU-MIMO frequency scheduling using parallel
of the rewards from the GNN architecture divided by the decisionmaking.Thefirstmainbenefitcomparedtosequential
baseline. The plots show that the reward improvement over baselines is the inference speed that allows to take scheduling
baseline spreads over a large range depending on the scenario decisions within strict delay requirements, without any loss
realization, with only a few cases at the left of the vertical in performance. Another important benefit is that DQN-based
line x=1 where the baseline is better. algorithms do not depend on a simulator to roll-out the policy
Network Size vs Inference time trade-off:Comparingthe but can rather explore-exploit online from collected data,
sizeofthemodelsfromTableIII,wecanseethattheparameter thus learning in realtime and adapting the policy to changes
sharing in the branches allows the Unibranch architecture to in the environment. We proposed variations of the parallel
halvethenumberofparameterscomparedtoActionBranching architecture and showcased the trade-off between network
(AB) without performance loss as shown above, yet the size in number of variables and inference time, for the same
largest gain in number of parameters is obtained by the GNN performance in the target metric.[11] M.Rasti,S.K.Taskou,H.Tabassum,andE.Hossain,“Evolutiontoward
6gmulti-bandwirelessnetworks:Aresourcemanagementperspective,”
IEEEWirelessCommunications,vol.29,no.4,pp.118–125,2022.
[12] K. Boutiba, M. Bagaa, and A. Ksentini, “Radio resource management
in multi-numerology 5g new radio featuring network slicing,” in ICC
2022-IEEEInternationalConferenceonCommunications,2022.
[13] P.KelaandT.Veijalainen,“Cooperativeactionbranchingdeepreinforce-
mentlearningforuplinkpowercontrol,”in2023JointEuropeanCon-
ference on Networks and Communications & 6G Summit (EuCNC/6G
Summit),2023,pp.484–489.
[14] M.Bouton,J.Jeong,J.Outes,A.Mendo,andA.Nikou,“Multi-agent
reinforcement learning with graph q-networks for antenna tuning,” in
NOMS 2023-2023 IEEE/IFIP Network Operations and Management
Symposium,2023,pp.1–7.
[15] P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi,
M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo, K. Tuyls, and
T.Graepel,“Value-decompositionnetworksforcooperativemulti-agent
learning based on team reward,” in Proceedings of the 17th Interna-
tionalConferenceonAutonomousAgentsandMultiAgentSystems,ser.
AAMAS’18. Richland,SC:InternationalFoundationforAutonomous
AgentsandMultiagentSystems,2018,p.2085–2087.
Fig. 5. Training performance of fine-tuning vs. training from scratch, [16] A.Tavakoli,F.Pardo,andP.Kormushev,“Actionbranchingarchitectures
evaluatedon500validationepisodes. fordeepreinforcementlearning,”ProceedingsoftheAAAIConference
onArtificialIntelligence,vol.32,no.1,Apr.2018.
[17] M. Chen, Z. Wei, Z. Huang, B. Ding, and Y. Li, “Simple and deep
graphconvolutionalnetworks,”inProceedingsofthe37thInternational
The proposed architectures have been evaluated for the ConferenceonMachineLearning,ser.ProceedingsofMachineLearning
myopic case with instantaneous reward. However, for traffic Research, H. D. III and A. Singh, Eds., vol. 119. PMLR, 13–18 Jul
2020,pp.1725–1735.
mixeswherethealgorithmshouldscheduleuserswithdifferent
[18] R.S.SuttonandA.G.Barto,“Reinforcementlearning:Anintroduction,”
Quality-of-Experience(QoE)requirements(delay,throughput, inReinforcementLearning:AnIntroduction,T.M.Press,Ed.,2015.
reliability) the optimal scheduling policy should be learned [19] V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,A.Belle-
mare,MarcG.andGraves,M.Riedmiller,A.K.Fidjeland,G.Ostrovski,
jointly over longer time horizons, in which case the DQN
S.Petersen,C.Beattie,A.Sadik,I.Antonoglou,H.King,D.Kumaran,
algorithms are expected to show advantages compared to my- D. Wierstra, S. Legg, and D. Hassabis, “Human-level control through
opicsolutions.Suchtemporalaspectsstudiedwithinmulti-cell deepreinforcementlearning,”Nature,vol.518,pp.529–533,2015.
[20] T.Schaul,J.Quan,I.Antonoglou,andD.Silver,“Prioritizedexperience
scenariosareverychallengingtopicsforfutureinvestigations.
replay,”inICLR(Poster),2016.
[21] A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru,
REFERENCES J. Aru, and R. Vicente, “Multiagent cooperation and competition with
deepreinforcementlearning,”PLOSONE,vol.12,no.4,pp.1–15,04
[1] N.A.KhanandS.Schmid,“Ai-ranin6gnetworks:State-of-the-artand 2017.
challenges,”IEEEOpenJournaloftheCommunicationsSociety,vol.5, [22] Z. Wang, T. Schaul, M. Hessel, H. Van Hasselt, M. Lanctot, and
pp.294–311,2024. N. De Freitas, “Dueling network architectures for deep reinforcement
[2] “Defining AI native: A key enabler for advanced intelligent learning,”inProceedingsofthe33rdInternationalConferenceonInter-
telecom networks,” Ericsson White Paper BCSS-23:000056 Uen nationalConferenceonMachineLearning-Volume48,ser.ICML’16.
https://www.ericsson.com/49341a/assets/local/reports-papers/white- JMLR.org,2016,p.1995–2003.
papers/ai-native.pdf,February2023. [23] T. Rashid, G. Farquhar, B. Peng, and S. Whiteson, “Weighted qmix:
[3] E.Castan˜eda,A.Silva,A.Gameiro,andM.Kountouris,“Anoverview expanding monotonic value function factorisation for deep multi-agent
on resource allocation techniques for multi-user mimo systems,” IEEE reinforcementlearning,”inProceedingsofthe34thInternationalCon-
CommunicationsSurveys&Tutorials,vol.19,no.1,pp.239–284,2017. ferenceonNeuralInformationProcessingSystems,ser.NIPS’20. Red
[4] R.-M. Ursu, A. Papa, and W. Kellerer, “Experimental evaluation of Hook,NY,USA:CurranAssociatesInc.,2020.
downlinkschedulingalgorithmsusingopenairinterface,”in2022IEEE [24] W.Boehmer,V.Kurin,andS.Whiteson,“Deepcoordinationgraphs,”in
Wireless Communications and Networking Conference (WCNC), 2022, Proceedingsofthe37thInternationalConferenceonMachineLearning,
pp.84–89. ser.ProceedingsofMachineLearningResearch,H.D.IIIandA.Singh,
[5] F.Capozzi,G.Piro,L.Grieco,G.Boggia,andP.Camarda,“Downlink Eds.,vol.119. PMLR,13–18Jul2020,pp.980–991.
packet scheduling in lte cellular networks: Key design issues and a [25] M. Bouton, H. Farooq, J. Forgeat, S. Bothe, M. Shirazipour, and
survey,”IEEECommunicationsSurveys&Tutorials,vol.15,no.2,pp. P.Karlsson,“Coordinatedreinforcementlearningforoptimizingmobile
678–700,2013. networks,”arXivpreprintarXiv:2109.15175,2021.
[6] “EricssonSilicon,”www.ericsson.com/en/ran/ericsson-silicon,2023. [26] P.Velicˇkovic´,G.Cucurull,A.Casanova,A.Romero,P.Lio,andY.Ben-
[7] “Ericsson and Intel expand their strategic collaboration gio,“Graphattentionnetworks,”arXivpreprintarXiv:1710.10903,2017.
to advance next-generation optimized 5G infrastructure,” [27] 3GPP,“3rdgenerationpartnershipproject;technicalspecificationgroup
www.ericsson.com/en/news/2023/7/ericsson-and-intel-expand-strategic- radioaccessnetwork;studyonchannelmodelforfrequenciesfrom0.5
collaboration-to-advance-next-generation-optimized-5g-infrastructure, to100ghz(release18),”3GPP,Tech.Rep.TR38.901V18.0.0(2024-
July2023. 03),2024.
[8] L. Kundu, X. Lin, E. Agostini, V. Ditya, and T. Martin, “Hardware [28] E. Yaacoub and Z. Dawy, Resource Allocation in Uplink OFDMA
accelerationforopenradioaccessnetworks:Acontemporaryoverview,” Wireless Systems: Optimal Solutions and Practical Implementations,
IEEECommunicationsMagazine,pp.1–7,2023. 1sted. Wiley,2012.
[9] D. Sandberg, T. Kvernvik, and F. D. Calabrese, “Learning robust [29] L. Wan, S. Tsai, and M. Almgren, “A fading-insensitive performance
schedulingwithsearchandattention,”inICC2022-IEEEInternational metricforaunifiedlinkqualitymodel,”inIEEEWirelessCommunica-
ConferenceonCommunications,2022,pp.1549–1555. tionsandNetworkingConf.,vol.4,2006,pp.2110–2114.
[10] F. Al-Tam, N. Correia, and J. Rodriguez, “Learn to schedule (leasch):
A deep reinforcement learning approach for radio resource scheduling
inthe5gmaclayer,”IEEEAccess,vol.8,pp.108088–108101,2020.