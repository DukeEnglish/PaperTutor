An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal
Large Language Models
XiongtaoZhou1* JieHe2* YuhuaKe2 GuangyaoZhu1
VíctorGutiérrez-Basulto3 and JeffZ.Pan2†
1 WasedaUniversity,Japan
2 SchoolofInformatics,UniversityofEdinburgh,UK
3 SchoolofComputerScienceandInformatics,CardiffUniversity,UK
alenai.tao@ruri.waseda.jp, j.he@ed.ac.uk
s2484588@ed.ac.uk, zhuzgy@akane.waseda.jp
gutierrezbasultov@cardiff.ac.uk, j.z.pan@ed.ac.uk
Abstract visualencoders,connectorlayers,andLLMs. This
architecture is usually fine-tuned through multi-
Multimodallargelanguagemodels(MLLMs)
modalinstruction-followingdata(Xuetal.,2023).
fine-tunedwithmultimodalinstructiondatasets
During fine-tuning, most existing MLLMs (Cha
havedemonstratedremarkablecapabilitiesin
etal.,2023;Suetal.,2023;Linetal.,2023)typi-
multimodaltasks. However,fine-tuningallpa-
callyfreezethevisualencoder,focusingsolelyon
rametersofMLLMshasbecomechallengingas
theyusuallycontainbillionsofparameters. To connectorlayersandtheLLMcomponent. Since
addressthisissue,westudyparameter-efficient LLMs (e.g. LLaMA (Touvron et al., 2023) and
fine-tuning(PEFT)methodsforMLLMs. We Vicuna-v1.5 (Chiang et al., 2023)) often contain
aimtoidentifyeffectivemethodsforenhanc- hundredsofbillionsofparameters,fullfine-tuning
ing the performance of MLLMs in scenar-
(FFT) (Wang et al., 2022) is unfeasible. Conse-
ios where only a limited number of parame- quently,theparameter-efficientfine-tuning(PEFT)
ters are trained. This paper conducts empir-
(Houlsby et al., 2019; Hu et al., 2021) approach
ical studies using four popular PEFT meth-
(whichleverageslightweighttrainableparameters
odstofine-tunetheLLMcomponentofopen-
sourceMLLMs. Wepresentacomprehensive andkeepsthemajorityofparametersfrozen)has
analysisthatencompassesvariousaspects,in- been widely employed in NLP for fine-tuning
cluding the impact of PEFT methods on var- LLMswithinstructionortask-specificdatasets(Li
ious models, parameters and location of the
et al., 2023c; You et al., 2023), as they allow for
PEFTmodule,sizeoffine-tuningdata,model
significantresourcesavingswhileachievingcom-
stability based on PEFT methods, MLLM’s
parableperformanceorevensurpassingFFT(Man-
generalization, and hallucination. We eval-
grulkaretal.,2022).
uated four PEFT methods on seven datasets
fromtwodifferentcategories: unseenandseen IncontrasttostandardLLMs,MLLMsintroduce
datasets. Across all experiments, we show additionalmodules: visualencoderandconnector
that the adapter is the best-performing PEFT layers. During the fine-tuning process, unimodal
method. Atthesametime,fine-tuningthecon- LLMs only receive text features while MLLMs
nectorlayersleadstoimprovedperformancein
getmultimodalinputs,suchthatconnectorlayers
mostMLLMs. Codeanddataareavailableat
are also fine-tuned, not just fine-tuning the LLM.
https://github.com/alenai97/PEFT-MLLM.git
Therefore,itiscrucialtoreassesstheperformance
1 Introduction offine-tuningMLLMsusingvariousPEFTmeth-
ods,exploringtheimpactofconnectorfine-tuning
Inrecentyears,thelandscapeofmultimodallearn- onthemodel’sperformanceindownstreamtasks,
inghasbeentransformedbytheemergenceofmul- andexaminingPEFT’seffectsonmodelstability,
timodallargelanguagemodels(MLLMs),suchas generalization,andhallucination. Inthispaperwe
LLaVA(Liuetal.,2023b),MiniGPT4(Zhuetal., addresstheseissuesbyconductingcomprehensive
2024), and GPT4-Vision (OpenAI et al., 2023). studiesonthreerepresentativeMLLMscontaining
MLLMshaveshowcasedimpressivecompetency connectorlayers: LLaVA-1.5(7B,13B)(Liuetal.,
acrossaspectrumofmultimodalbenchmarks(Fu 2023a),ShareGPTv4(7B)(Chenetal.,2023),and
et al., 2023; Liu et al., 2023c; Li et al., 2023b) Qwen-VL-Chat(7B)(Baietal.,2023b). Ourstudy
thankstotheintegratedarchitectureofpre-trained looks at various issues related to PEFT methods.
Specifically, we design our study to address the
* EqualContribution.
† Correspondingauthor followingquestions: (1)Isitnecessarytofine-tune
1
4202
nuJ
7
]LC.sc[
1v03150.6042:viXratheconnectorwhenfine-tuningMLLMSviavari- an adapter module on LLaMA, keeping only the
ous PEFT methods on unseen and seen datasets? adapter parameters updated during training. In
(2) How does the position of the PEFT module contrast, LLaVA-1.5 (Liu et al., 2023a) employs
intheLLMaffecttheMLLM’sperformance? (3) two layers of MLP to connect the visual encoder
Facedwithdifferenttrainingdatascales,whatdif- andLLM.Duringfine-tuning,itonlyupdatesthe
ferencesexistintheperformanceofdifferentPEFT parameters of the MLP and LLM. Subsequent
methods? (4)HowdodifferentPEFTapproaches worksmostlybuilduponthisapproach,employing
impactthestabilityofthemodel? Isthereanyrela- the connector layers to link a visual encoder
tionshipbetweentrainableparametersandlearning and LLMs, and then fine-tune the model using
ratewithstability? multimodal instruction-following data (Li et al.,
Ourkeyfindingscanbesummarizedasfollows: 2023a; Hu et al., 2023a; Wang et al., 2023).
Recently,therearealsomanyworkonmultimodal
1. Fine-tuningtheconnectorlayersusuallyleads largelanguagemodels(Chenetal.,2024)fromthe
toperformanceimprovementwithinMLLMs. perspective of knowledge computing (Pan et al.,
2. More trainable parameters results in better 2023).
performanceonunseendatasets,whilefewer
Parameter-EfficientFine-Tuning. Parameter-
trainable parameters maintains the model’s
efficient fine-tuning emerges as an approach ca-
performanceonseendatasets.
pable of achieving performance comparable to
3. Generally, fine-tuning using large scale
full fine-tuning while keeping the majority of pa-
datasets leads to better performance. How-
rameters frozen. Prompt-based methods (Lester
ever,whenresourcesarelimited, oneshould
etal.,2021)incorporatesoftpromptsintotheinput
considermedium-sizedatasetsinstead.
prefixes, only updating these soft prompts. An-
4. Adaptersshowthebestoverallperformance
other widely used family of methods is based on
inmodelgeneralization,stability,andhalluci-
adapters(Pfeifferetal.,2020;Heetal.,2022;He
nation.
andFu,2023),whichinsertadaptermodulesatspe-
cificpositionswithintransformerlayersandupdate
Our contributions can be summarized as follows:
onlytheparametersoftheseinsertedmodulesdur-
(1)Wehaveassembledastandardizedevaluation
ingtraining. Also,inMLLMs,low-rankdecompo-
suite that includes seven benchmarks from the
sitionmethodsarecommonlyemployed(Huetal.,
vision-and-language research community. This
2021;Edalatietal.,2022). Thesemethodsinvolve
suiteencompassesfivetasksinvisualquestionan-
trainingonlytheparametersinlow-rankmatrices,
swering,oneinvisualreasoning,andoneinimage
significantlyreducingthenumberoftrainablepa-
caption, along with four PEFT methods. (2) We
rameters.
utilizedtheseresourcestoconductin-depthexperi-
mentsinvestigatingfourcrucialdesigndimensions
3 PEFTMethods
(cf.Fig.1,left): 1)datascaling,2)stabilityofthe
trainingprocess,3)overfittingandgeneralization,
Figure1illustratesthearchitectureofMLLMsand
and 4) hallucination. (3) Our empirical findings
thelocationofvariousPEFTmodules. Inourex-
showthatAdapteroutperformsotherPEFTmeth-
periments,allconsideredMLLMsconsistofthree
ods in all aspects, followed in second place by
components: a visual encoder, connector layers,
LoRA.Furthermore,weshowthatfine-tuningthe
and a LLM. The structure of the connector lay-
connectorlayersfrequentlyenhancesperformance
ers may vary depending on the specific MLLM.
withinMLLMs.
PEFT methods can be classified into three cate-
gories, from which we select four methods: (1)
2 RelatedWork
reparametrization-based tuning: LoRA, IA3 (2)
Multimodal Large Language Models. adapter-basedtuning: Adapter. (3)prompt-based
Flamingo (Alayrac et al., 2022) proposes a tuning: Prefix-Tuning.
GATED XATTN-DENSE layer to align visual LoRA. Weintegratethelow-rankstrategypro-
andtextualfeatures,connectingthevisualmodule posed by Hu et al. (2021) to adjust the network
and language model. LLaMA-adapter (Zhang weights,facilitatingthemodel’shandlingofcom-
etal.,2024)appliesaprojectionlayertoconnect plex tasks with an efficient parameter footprint.
avisualencoderandLLaMA.Itproposesadding Theoriginalpre-trainedweightmatrixW ∈ Rd×k
0
2Image Visual Encoder / Connector
Should we tune or freeze?
Text
Which type of force from
the child‘s finger presses Large Language Model LoRA Prefix-Tuning
the button? IA3 Adapter
Anyconnectionbetweengeneralizationandvarious
PEFT? How to place it for
better performance?
Any relationship between hallucination and different Do we need more
PEFT? trainable parameters?
WhichPEFTismoreefficient?
CanvariousPEFTaffectthestabilityofthemodel? Train Freeze
Figure1: Left): ArchitectureofaMultimodalLargeLanguageModel. Startingfrom7questions,wecomprehen-
sivelyexploredtheimpactofPEFTmethodsandtheconnectoronMLLMs,allofwhichareillustratedontheLeft.
Right): AdetailedillustrationofthePEFTmodulestructureforthefourPEFTmethods.
isupdatedthroughlow-rankdecompositionusing inputsequencefedintothepre-trainedmodel. We
Equation1,whereB ∈ Rd×r andA ∈ Rr×k. initialize a trainable matrix P with dimensions
θ
|P |×dim(y ), where P specifies the prefix
idx i idx
W +∆W = W +BA (1)
0 0 length. Thisyieldsthefollowingconditionalformu-
lationforeachelementy oftheoutputsequence:
This method ensures our model’s adaptability is i
improvedwithoutasignificantincreaseinthepa-
(cid:40)
rameterspace. P θ[i,:] ifi ∈ P idx,
y = (4)
i
IA3. Following Liu et al. (2022), we integrate LM (z ,y ) otherwise.
ϕ i <i
threevectorsv k ∈ Rd k,v v ∈ Rdv,andv ff ∈ Rd ff
intoanattentionmechanismsas: Ifi ∈ P , abidirectionalencodercomputesthe
idx
y . Fori ∈/ P ,y iscomputedbyanautoregres-
Q(v ⊙KT) i idx i
softmax(
k√
d )(v v ⊙V) (2) sive neural LM as a function of y i and the past
k activationsinitsleftcontext.
where ⊙ represents the element-wise multiplica-
tion,and(v ⊙γ(W x))W intheposition-wise 4 ExperimentSetup
ff 1 2
FFNlayers,leveragingγ astheactivationfunction.
Theseformulasguidethemodel’sattentiontobe 4.1 Datasets
fine-tunedtoprioritizerelevantfeatures,optimiz-
In the current era of large-scale models, dataset
ing performance without significantly increasing
contaminationisasignificantconcernasitischal-
themodel’scomplexityornumberofparameters.
lengingtoensurethatthedatawillbeusedforthe
Adapter. We adopt the structure proposed by
next training process constitutes unseen data for
Houlsbyetal.(2019),whichaddsadaptermodules
largelanguagemodels. Therefore, wecategorize
tothefully-connectednetworksafterattentionand
thedatasetsintotwotypes: Unseendatasets,com-
theFFNlayerwithinthetransformerlayers. This
prisingdatasetsthathavenotbeeninvolvedinthe
canbecapturedasfollows:
trainingofanyoftheconsideredmodels,including
(1)theScienceQAdataset(Luetal.,2022);(2)the
h +f(W (h ))W → h (3)
i down i up i
Vizwizdataset(Gurarietal.,2018);(3)theIconQA
whereh istheoutputofthepreviouslayer,which dataset (Lu et al., 2021); and (4) the Flickr30k
i
isinitiallydown-projectedbyW ∈ Rd×r toa dataset(Youngetal.,2014). Seendatasets,consist-
down
lowerdimensionr,andthenup-projectedbackby ingofdatasetsusedinthetrainingofallconsidered
W ∈ Rr×d to the original dimension d, f is a models,including(1)theOKVQAdataset(Marino
up
non-linearlayer. et al., 2019); (2) the OCRVQA dataset (Mishra
Prefix-Tuning. Wefollowtheapproachproposed et al., 2019); and (3) the VQAv2 dataset (Goyal
byLiandLiang(2021)toemployprefixlearning etal.,2017). Detailsaboutdatasetscanbefound
byappendingtask-specificvector“prefixes”tothe inApp.A.
390 ofPEFTmethodsonbothunseenandseendatasets.
Seen
85 83.0 82.9 Unseen Inourexperiments,followingexistingwork(Liu
80
76.3 etal.,2023b;Baietal.,2023b;Chenetal.,2023),
75
67 50 66.2 64.5 67.5 65.6 67.9 w me odf ure leez toe fith ne e-v tuis nu ea tl he en Lc Lod Me .r Fa on rd tha epp coly nnth ece toP rE lF ayT -
60
ers,weexperimentedwithbothFFTandfreezing.
55
Weinvestigatethemodel’sperformanceoncertain
50
Adapter LoRA IA3 Prefix
PEFT Method tasksduringthetask-specificfine-tuningwhenun-
(a)w/connector freezingthevisualencoder,seeApp.Cfordetails
90 about unfreezing the visual encoder. We investi-
Seen
85 82.3 83.4 Unseen gated the performance of LLaVA-1.5 (7B, 13B),
80
75 ShareGPT4v,andQwen-VL-Chatonthedatasets
70 67.2 67.2 68.6 67.3 mentionedinSection4.1.
65 63.8
61.3
60
The obtained results are presented in Table 1.
55
50 One can observe that LLaVA-1.5-13B with IA3
Adapter LoRA IA3 Prefix
PEFT Method
andfine-tunedconnectorlayersachievedthebest
(b)w/oconnector
results across all unseen and seen datasets. Most
Figure2: ThecomparativeperformanceoffourPEFT IA3 models with fine-tuned connector achieved
methodsonseenandunseendatasets,withandwithout comparableperformancestoLoRAandAdapteron
theuseofaconnector. unseendatasets,whilealsomaximizingthemodel’s
performance on seen datasets. Benefiting from
theincreasednumberofparametersinLLaVA-1.5-
4.2 Implementations
13B, we noticed that across various settings, the
Models. We selected LLaVA-1.5 (7B, 13B), average performance on all datasets of LLaVA-
ShareGPTv4(7B),andQwen-VL-Chat(7B)asthe 1.5-13BsurpassesthatofLLaVA-1.5-7B.Theav-
basemodelsforourexperiments. erageperformanceofShareGPTv4generallysur-
Hyperparameters. Weconductfine-tuningon passes(exceptfortheIA3methodwiththefrozen
thetrainingsetofeachdatasetseparatelyandthen connector) that of LLaVA-1.5-7B, because it has
testontheirrespectivetestorvalidationsets. Allex- beenfine-tunedonthehigher-qualitymultimodal
perimentswereconductedwithaglobalbatchsize instruction-following dataset (Chen et al., 2023).
of128. Wesettherandomseedoftheexperiment Underthesettingoffreezingtheconnectorlayers
to42. Additionally,eachPEFTmethodwastrained and fine-tuning the LLM with LoRA, Qwen-VL-
for three epochs on the fine-tuning dataset. For Chatachievedthebestperformance. Wefoundthat
LoRA,wesetitslearningrateto2e-4,theadapter’s choosingtotunetheconnectorlayersoftenleads
to5e-5,IA3’sto2e-4,andPrefix-Tuning’sto1e-5. to a significant deterioration in Qwen-VL-Chat’s
Moreinformationaboutmodelandhyperparameter performanceonseendatasets. Figure2illustrates
settingsisavailableinApp.B theperformanceofvariousPEFTmethodsunder
thesettingsoftuningorfreezingtheconnectorlay-
5 ExperimentalResults ers. Whenfine-tuningtheconnectorlayers,LoRA,
Prefix-Tuning, and IA3 all exhibit better perfor-
5.1 MainResults
mancethanfreezingtheconnectorlayersonunseen
Should we tune or freeze the connector when datasets. In this case, IA3 gets a 15.0% increase
considering unseen and seen datasets? Given in the average result. On seen datasets, in most
the increasing availability of pretraining data for cases, the performance of freezing the connector
MLLMs, encountering contaminated data (i.e. layersandthatoftheremainingPEFTmethods(ex-
training data contains information that is meant ceptforaslightdecreaseinLoRA’sperformance)
to be present only in the test set) is increasingly is similar. Note that whether the connector lay-
common. Additionally,mostcurrentMLLMstune ers are fine-tuned or not, the performance of the
theconnectorlayersduringfine-tuning,yettherole Adapterremainsrelativelyconsistentonbothseen
oftheconnectorremainsunclear. Ourmainexperi- and unseen datasets. Our main findings are the
mentthusfocusesoninvestigatingtheperformance following:
4
)%(ycaruccA
)%(ycaruccAModel Method SQA(img) VizWiz IconQA-txt IconQA-blank Flickr30k OKVQA OCRVQA VQAv2 Avg
Adapter 78.7 66.7 83.5 77.7 91.1 59.4 65.5 74.0 74.6
-w/connector 84.4 67.6 88.7 80.9 89.8 59.8 65.2 73.8 76.3
LoRA 85.2 64.7 89.9 85.5 85.6 56.3 68.2 73.2 76.1
-w/connector 86.2 66.5 90.6 88.8 85.2 56.5 66.7 73.1 76.7
LLaVA-1.5-7B
IA3 69.1 56.2 55.2 41.9 77.2 59.8 66.9 78.1 63.1
-w/connector 82.7 61.9 89.2 82.2 91.9 60.5 67.1 75.2 76.3
Prefix 68.2 59.0 73.0 46.8 91.5 61.1 68.6 76.9 68.1
-w/connector 69.7 60.8 76.7 50.9 91.9 61.3 68.5 77.0 69.6
Adapter 82.4 66.6 88.9 84.2 94.0 59.4 67.4 74.7 77.2
-w/connector 83.7 66.8 90.6 85.8 93.1 59.6 67.2 74.5 77.7
LoRA 86.3 66.3 90.9 90.3 87.9 59.1 70.8 74.4 78.3
-w/connector 87.8 66.1 91.6 90.4 84.1 59.9 68.6 73.6 77.8
LLaVA-1.5-13B
IA3 72.3 58.8 58.9 47.5 70.9 62.6 70.5 78.4 65.0
-w/connector 84.5 67.3 90.3 84.8 91.3 63.8 69.0 76.7 78.5
Prefix 70.4 68.7 65.2 41.5 88.2 64.4 66.8 77.9 67.9
-w/connector 71.7 69.1 65.7 46.8 89.1 64.7 67.4 78.6 69.1
Adapter 81.1 67.0 89.7 82.8 95.6 59.8 67.9 76.7 77.6
-w/connector 82.2 64.1 91.8 86.0 93.4 59.5 67.5 76.2 77.6
LoRA 86.7 65.6 91.8 90.4 85.0 57.9 69.8 75.9 77.9
-w/connector 86.7 67.3 91.9 90.6 84.9 57.6 69.0 75.3 77.9
ShareGPT4V
IA3 69.0 61.1 58.7 47.7 57.5 60.7 69.1 79.8 63.0
-w/connector 82.0 60.9 90.9 84.1 93.8 61.4 68.7 77.3 77.4
Prefix 67.9 63.6 73.8 45.2 91.6 62.4 68.9 78.7 69.0
-w/connector 68.4 65.2 81.3 53.2 92.4 62.3 67.7 78.8 71.2
Adapter 79.6 67.8 92.4 90.5 86.4 54.9 71.1 75.8 77.3
-w/connector 81.2 69.3 90.8 87.5 82.7 51.1 69.3 70.7 75.3
LoRA 86.8 68.5 91.5 85.5 82.6 53.8 71.4 75.7 77.0
-w/connector 84.0 68.8 71.9 90.3 83.5 43.5 67.0 63.3 71.5
Qwen-VL-Chat
IA3 70.0 66.9 71.0 41.8 73.6 50.7 68.3 77.8 65.0
-w/connector 67.3 69.8 57.3 28.7 65.1 50.5 62.1 77.5 59.8
Prefix 52.2 70.6 52.4 33.2 52.2 50.1 61.3 70.6 55.3
-w/connector 51.9 70.4 52.5 31.8 52.9 49.8 61.5 77.4 56.0
Table 1: Main experimental results of various MLLMs with four PEFT methods. w/ connector: Tuning the
connector.
• LoRAandAdapterhavethebestperformance layer, or both. For adapters, we placed them in
on all of the unseen datasets, while IA3 and thesamelocations. TheresultofQwen-VL-Chat
Prefix-TuningperformthebestontheOKVQA canbefoundinApp.D.Notethatwedonotcon-
andVQAv2. Moretrainableparametersallows siderPrefix-Tuningasthepositionisfixed. Table2
the model to better adapt to unseen datasets, presentstheresultsonLLaVA-1.5-7B,whichsug-
whilefewertrainableparameterscanmaintain gestthatdespitetheadditionalmodulesinMLLMs
themodel’sperformanceonseendatasets. comparedtoLLMs,theresultsofHuetal.(2023b)
• For the unseen datasets, tuning the connector forfine-tuningLLMsarealsovalidforMLLMs.
layers often outperforms freezing the connec-
• We observe that for LoRA and IA3, the Both
tor layers. For the seen datasets, freezing the
settingachievedthebestresults. AsforAdapter,
connectorlayersyieldsthebestperformance.
insertingitonlyintotheMLPlayeryieldedthe
bestperformance.
5.2 ModuleLocation
5.3 DataScale
WhatisthebestlocationforthePEFTmodulefor
MLLMs? Unlike LLMs, MLLMs include addi- In practical applications, MLLMs often require
tionalconnectorlayersandvisualencoders. There- fine-tuning on downstream datasets (Li et al.,
fore, we can not straightforwardly transfer exist- 2023c;Youetal.,2023),makingPEFTanefficient
ing results for LLMs to MLLMs. With this in choice. However, the sizes of these specific task
mind,wedirectlyaddressthisissuehere. Tothis datasets may vary, leading to the question: How
end, we selected all VQA datasets for the loca- to select PEFT methods for datasets of different
tionstudy. WechooseLLaVA-1.5-7Basthebase scales when training? Therefore, we investigate
model set the random seed to 42, freeze the vi- theperformanceofPEFTmethodsondatasetsof
sual encoder, and fine-tune the connector layers varyingscales. WefollowedChenetal.(2022)re-
and LLM. We used this setting in subsequent ex- sourcesetting,andrandomlysampled1k,5k,and
periments. ForLoRAandIA3,weintegratethem 10kdatapointsfromthetrainingsetofeachdataset.
intothemodel’smulti-headattentionlayer,MLP Wecategorize1kdatapointsasLow-Resource,5k
5Method Location SQA(img) VizWiz IconQA-txt IconQA-blank OKVQA OCRVQA VQAv2 Avg
LLaVA-1.57B
Attn 81.3 67.9 89.2 80.3 58.8 66.2 75.0 74.1
Adapter MLP 84.4 67.6 88.7 80.9 59.8 65.2 73.8 74.3
Both 82.9 67.8 88.8 81.4 55.2 64.3 72.1 73.2
Attn 84.1 68.1 90.5 83.8 58.4 67.0 73.5 75.1
LoRA MLP 85.6 66.3 90.8 88.0 56.5 66.5 73.0 75.2
Both 86.2 66.5 90.6 88.8 56.5 66.7 73.1 75.5
Attn 81.0 61.9 88.9 82.1 60.3 67.3 75.2 73.8
IA3 MLP 82.0 62.1 88.7 82.6 60.5 67.4 75.3 74.1
Both 82.7 61.9 89.2 82.2 60.5 67.1 75.2 74.1
Table2: AverageresultsofPEFTmodulelocationonLLaVA-1.5-7B.Attn: Placedonattentionlayer. MLP:Placed
onMLPlayer. Both: PlacedbothonattentionlayersandMLPlayers.
Method SQA(img) VizWiz IconQA-txt IconQA-blank Flickr30k OKVQA OCRVQA VQAv2 Avg
Adapter 63.0 62.5 52.4 35.3 87.6 57.5 61.1 73.4 61.6
LoRA 68.8 62.7 62.9 38.2 89.4 56.5 64.2 74.5 64.7
Low-Resource
IA3 67.0 50.3 60.6 40.9 86.0 59.2 64.0 75.4 62.9
Prefix 49.8 56.3 51.3 20.6 81.6 51.6 65.6 53.1 53.7
Adapter 74.9 63.5 72.9 66.5 85.4 58.1 64.1 73.3 69.8
LoRA 80.0 66.4 78.9 74.8 78.3 54.7 65.2 72.3 71.3
Medium-Resource
IA3 77.4 55.1 77.8 76.4 88.5 59.3 65.6 75.0 71.9
Prefix 56.5 52.2 63.1 38.8 88.9 60.0 65.9 74.7 62.5
Adapter 79.8 66.0 81.3 80.2 91.9 59.8 64.2 73.3 74.6
LoRA 84.7 64.5 84.9 87.1 83.5 55.9 65.8 72.4 74.9
High-Resource
IA3 80.9 58.3 84.0 85.0 88.9 60.5 65.8 74.7 74.8
Prefix 67.5 55.7 70.3 52.1 91.0 61.3 67.6 76.3 67.7
Table3: Fine-tunedaverageresultswithallPEFTmethodsondatasetsofdifferentsizes.
data points as Medium-Resource, and 10k data investigatetheinstabilityoffine-tuningLLMsus-
pointsasHigh-Resource. Notethatsincethetrain- ingPEFTmethods. Analogously,welookatsuch
ingsetofOKVQAcontainsonly9ksamples,we instability for MLLMs. We concentrate on the
consideredthefulldataashigh-resource. Table3 SQA(img)fromtheunseendatasetsandOKVQA
presentstheresults. fromseendatasetsandselectthreerandomseeds:
Ourmainfindingsarethefollowing: [seed21, seed42, seed63]. We present a stability
analysisforLLaVA-1.5-7B,moreanalysiscanbe
• High-Resource will make the MLLM more
foundinApp.F.
powerful, while Medium-Resource will be
Thenumberoftrainableparametersplaysacru-
more efficient. The performance of the four
cialroleinthefine-tuningprocessofamodel. How-
PEFT methods improves as the scale of re-
ever, in the multimodal setting, the relationship
sourcesgrows, i.e.allachievetheirbestper-
between the number of trainable parameters and
formance with high-resource. Thus, when
stability when fine-tuning with PEFT is not yet
resources are sufficient, fine-tuning on high-
clear. Withthisinmind,welookatthefollowing
resource datasets will yield better perfor-
question: Does fewer trainable parameters lead
mance. Averageperformanceimprovementis
tohigherstability? Weconductedanexperiment
showninApp.E.
underdifferenttrainableparameterconditions: on
• The unseen datasets tend to favor more re- seed21,seed42,andseed63,andvariedtheLora
sources. When fine-tuning on an unseen Rank,AdapterBottleneckSize,andVirtualTokens
dataset with more data, all PEFT methods tocontrolthenumberoftrainableparameters. Ta-
showasignificantperformanceimprovement. ble 6 presents the performance of various PEFT
In contrast, as the resources of the dataset methods and their standard deviations under dif-
increase, we did not observe significant per- ferentnumbersoftrainableparameters. IA3isnot
formanceimprovementonseendatasets. testedsinceitstrainableparameterscannotbemod-
ified. Wedrawthefollowingconclusions:
5.4 StabilityAnalysis
He et al. (2021) and Chen et al. (2022) carried • Adapter and LoRA exhibit drastically differ-
out experiments with different random seeds to ent levels of stability on the unseen and seen
6Sourcedomain Targetdomain overfittingepoch1 overfittingepoch2 overfittingepoch3 overfittingepoch4
SQA(img) 56.0 56.4 56.0 56.0
IconQA-txt
VizWiz 58.7 58.9 58.9 58.2
IconQA-txt 36.7 36.9 37.3 37.3
SQA(img)
Adapter VizWiz 58.3 58.0 57.8 57.7
SQA(img) 56.9 56.7 56.2 56.1
VizWiz
IconQA-txt 47.6 45.9 44.9 44.4
Avg - 52.4 52.1 51.9 51.6
SQA(img) 50.8 51.8 52.5 52.4
IconQA-txt
VizWiz 58.5 57.6 57.8 57.6
IconQA-txt 33.8 33.9 33.5 33.4
SQA(img)
LoRA VizWiz 56.8 57.2 56.9 56.8
SQA(img) 61.3 59.9 59.1 59.0
VizWiz
IconQA-txt 42.2 44.3 46.9 40.9
Avg - 50.6 50.8 51.1 50.0
SQA(img) 61.1 61.5 61.0 61.2
IconQA-txt
VizWiz 43.4 42.3 45.2 43.5
IconQA-txt 45.2 44.7 44.0 43.5
SQA(img)
IA3 VizWiz 53.5 53.1 53.1 52.9
SQA(img) 61.4 60.2 60.4 60.0
VizWiz
IconQA-txt 42.6 43.3 41.1 41.8
Avg - 51.2 50.9 50.8 50.5
SQA(img) 41.1 37.4 34.7 33.9
IconQA-txt
VizWiz 47.2 43.6 41.8 41.3
IconQA-txt 28.3 32.6 33.1 32.8
SQA(img)
Prefix VizWiz 54.0 53.7 53.9 53.5
SQA(img) 47.1 39.9 41.4 46.3
VizWiz
IconQA-txt 40.3 44.5 40.0 40.6
Avg - 43.0 42.0 40.8 41.4
Table4: PerformanceontargetdomainwithdifferentPEFTmethods. ForeachtargetdomainandPEFTmethod,
fourepochsclosesttotheoptimalpointofoverfittingwereselectedtotestonthetargetdomain. Avg: Theaverage
resultsoftargetdomainateachepoch.
Average performance fluctuations
10
5
0
IconQA->SQA
5 IconQA->VizWiz
SQA->VizWiz
10 SQA->IconQA
VizWiz->SQA
15 VizWiz->IconQA
Adapter LoRA IA3 Prefix
Peft Method
Figure 4: Average performance fluctuation of four
epochs on each source-target domain. We calculate
Figure3: Train-EvallossofallPEFTmethodsonSQA
themeanoffourPEFTmethodsoneachsource-target
(img). TheorangelineshowsTrainLoss. Evallossis
domainanddisplaytheaverageperformancefluctuation
coloredwithgreen.
ofallPEFTmethodsonthosedomain-pair.
datasets. Prefix-Tuning shows a strong insta- eredthreedatasetsfromtheunseendatasets: SQA
bility on the unseen datasets. Adapter gradu- (img), IconQA-txt, and Vizwiz. We choose one
ally stabilizes with decreasing parameters on datasetfromthesethreedatasetsasthesourcedo-
OKVQA,butbecomesunstablewithfewerpa- main, and fine-tuned LLaVA-1.5-7B with PEFT
rametersonSQA(img). Conversely,LoRAbe- methodsoneachsourcedomainfor12epochs.
comesunstablewithdecreasingparameterson
AdapterandLoRAexhibitstrongerrobustness.
OKVQA,butstabilizeswithfewerparameters
Figure3(andFigure8inApp.G)showstheevalu-
onSQA(img). Prefix-Tuningexhibitsstability
ationlossofvariousPEFTmethodsonSQAasthe
onOKVQA,andshowsarelativelystableper-
number of training epochs changes. We observe
formancewithfewerparametersonSQA(img).
thatwhenoverfittingoccurs,therearedifferences
intherobustnessexhibitedbyeachPEFTmethod
5.5 OverfittingandGeneralization
oneachdataset. OnSQA,LoRA,IA3,andAdapter
HowrobustdifferentPEFTmethodsarerelativeto arelativelystrongrobustnessisdemonstrated,with
overfitting? To address this question, we consid- LoRA performing the best. Prefix-Tuning shows
7
)%(egnahc
ecnamrofrePFine-tuningTask Method OverallScore↑ HallucinationRate↓ Attribute Adversarial Comparison Counting Relation Environment Holistic Other
Adapter 1.08 0.70 0.58 1.17 1.42 0.25 2.17 1.33 0.00 1.75
LoRA 0.69 0.83 0.58 0.00 1.42 0.67 1.17 1.00 0.00 0.67
IconQA-txt
IA3 0.76 0.83 0.42 0.42 1.67 1.00 0.50 0.75 0.00 1.33
Prefix 1.00 0.70 2.33 0.75 0.25 1.00 1.00 1.25 0.00 1.42
Adapter 0.73 0.81 0.08 0.00 1.92 0.75 0.33 1.08 0.27 0.92
LoRA 0.70 0.83 0.75 0.33 1.92 0.58 0.42 0.33 0.00 1.25
Flickr30k
IA3 0.70 0.84 0.25 0.42 1.33 1.17 0.75 0.67 0.00 1.00
Prefix 0.59 0.82 0.25 0.00 0.50 0.33 0.58 1.50 0.00 1.33
Table5: EvaluationresultsofLLaVA-1.5-7BwithdifferentPEFTmethodsonMMHAL-Bench.
OKVQA SQA(img) Method Epoch3 Epoch6 Epoch9 Epoch12 Avg
BottleneckSize=32 62.9±0.21 80.4±3.12 Adapter 17 12 14 10 13.3
Adapter
BottleneckSize=64 62.7±0.60 81.2±2.75 LoRA 15 14 18 20 16.8
BottleneckSize=128 61.4±0.20 81.3±1.80 IA3 14 17 17 16 16.0
BottleneckSize=256 58.8±0.84 82.2±1.91 Prefix 24 18 27 31 25.0
LoRARank=16 56.1±0.53 85.7±0.32
LoRA
LoRARank=32 56.1±0.27 85.3±0.92 Table7: HallucinationsstatisticofPEFTmethodson
LoRARank=64 56.4±0.20 85.0±0.85
fourepochs. Weselected100hallucination-freeexam-
LoRARank=128 56.6±0.12 85.4±0.85
plesfrom1krandomsampleddatafromtheoutputsof
VirtualTokens=10 62.2±0.10 73.4±2.62
Prefix
VirtualTokens=20 61.5±0.20 72.2±1.11 LLaVA-1.5-7B. We examined the outputs of LLaVA-
VirtualTokens=30 61.2±0.06 67.7±0.78 1.5-7B with four PEFT methods on those examples,
VirtualTokens=40 61.2±0.27 56.2±19.20
tablepresentsthenumberofoutputswithhallucination.
Table 6: Performance on three PEFT methods with
differenthyperparametersettings. Reportedresultsare
findthatAdapters,IA3,andPrefix-Tuning,all
averagesacrossthreerunswithdifferentrandomseeds.
achievethebestaveragegeneralizationperfor-
manceatthefirstoverfittingepoch. Ingeneral,
poorrobustnessonSQA.App.Gprovidesfurther themodel’sgeneralizationweakensasoverfit-
analysisonIconQA-txtandVizwiz. ting intensifies. However, LoRA achieves the
Whenfacingoverfittinganimportantquestionis best model generalization performance at the
HowdovariousPEFTmethodsperformintermsof thirdoverfittingepoch,indicatingthatmodels
generalization? Andhowtoachievethebestgener- fine-tunedwithLoRAexhibitthebestgeneral-
alizationperformanceduringtraining? Withthese izationwhenoverfittingreachesacertainlevel,
questions inmind, we conductedthe next experi- graduallyweakeningafterwards.
ment. BasedonFigure3,weidentifiedthetraining
5.6 Hallucination
step with the minimum evaluation loss for each
PEFTmethod. Weselectedfouroverfittingpoints The hallucination problem in LLMs has been
which are the closest to the minimum evaluation widelyacknowledged(Jietal.,2023;Gudibande
losspointonthesourcedomain. Subsequently,we etal.,2024). SinceMLLMsarebuiltuponLLMs,
testedtheperformanceofeachepochontheother this problem is also present in them. Zhai et al.
twotargetdomains,yieldingtheresultsshownin (2023) found that further fine-tuning with multi-
Table4. Wedrawthefollowingconclusions. modalinstruction-followingdataleadstohalluci-
nations. Therefore, we aim to investigate the fol-
• Adapterexhibitsthestrongestingeneralization. lowing question: Which PEFT method results in
Figure4showsthatwhenamodelisfine-tuned fewerhallucinationsduringfine-tuning? Weselect
using Prefix-Tuning its generalization perfor- IconQA-txtasthesourcedomainandtheFlickr30k
mance is quite poor. Models using Adapter dataset as the target domain to assess the out-of-
consistentlyexhibitagoodgeneralizationper- domain hallucinations of models fine-tuned with
formanceregardlessofthesituation,whilethe variousPEFTmethods. App.I.1elaboratesonhow
generalization performance of a model fine- weevaluatedthemodel’shallucination.
tuned with Prefix-Tuning is consistently neg- MMHAL-Bench (Sun et al., 2023) is used
ative. ModelsusingLoRAandIA3showfluc- to evaluate the hallucinations induced by fine-
tuationingeneralizationperformance. tuning LLaVA-1.5-7B with four PEFT methods
• IA3,Adapter,andPrefix-Tuningshowthebest onFlickr30kandIconQA-txt,yieldingtheresults
generalizationperformanceatthefirstoverfit- presentedinTable5. TheresultsshowthatAdapter
tingepoch. FromtheresultsofTable4,wecan consistentlyachievedthehighestAvgScoreandthe
8lowestHallucinationRateacrossbothfine-tuning References
tasks. Thisisconsistentwithourmanualevaluation
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
results.
Antoine Miech, Iain Barr, Yana Hasson, Karel
Adapterdemonstratespotentialforaddressing Lenc,ArthurMensch,KatherineMillican,Malcolm
hallucinations in MLLMs. The results are illus- Reynolds,etal.2022. Flamingo: avisuallanguage
model for few-shot learning. Advances in Neural
tratedinTable7. WeobservethatAdapterachieves
InformationProcessingSystems,35:23716–23736.
the lowest average hallucination rate across four
epochs, at only 13.3%. It can also be found that JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,
otherPEFTmethodstendtoproducemorehalluci- XiaodongDeng,YangFan,WenbinGe,YuHan,Fei
Huang,BinyuanHui,LuoJi,MeiLi,JunyangLin,
nationsafterfurtherfine-tuning,especiallyPrefix-
RunjiLin,DayihengLiu,GaoLiu,ChengqiangLu,
Tuning,whichgeneratesanadditional24%ofhal-
KemingLu,JianxinMa,RuiMen,XingzhangRen,
lucinationsfromepoch3toepoch12. Incontrast, XuanchengRen,ChuanqiTan,SinanTan,Jianhong
withfurtherfine-tuning,Adapterreducedthenum- Tu,PengWang,ShijieWang,WeiWang,Shengguang
Wu,BenfengXu,JinXu,AnYang,HaoYang,Jian
berofhallucinationsproduced. Inlinewithprevi-
Yang,ShushengYang,YangYao,BowenYu,Hongyi
ous studies (Wang et al., 2023), we attribute this
Yuan,ZhengYuan,JianweiZhang,XingxuanZhang,
phenomenontothenewparametersintheAdapter YichangZhang,ZhenruZhang,ChangZhou,Jingren
method, which provides a new module to adapt Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023a.
Qwentechnicalreport.
to downstream datasets while keeping the base
model’soriginalweights.
Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang,
SinanTan, PengWang, JunyangLin, ChangZhou,
6 Conclusion and Jingren Zhou. 2023b. Qwen-vl: A versatile
vision-languagemodelforunderstanding, localiza-
tion,textreading,andbeyond.
We conducted an extensive investigation on four
PEFTmethodsappliedtoMLLMsacrossdifferent
Junbum Cha, Wooyoung Kang, Jonghwan Mun, and
multimodaltasks. Byfine-tuningdifferentMLLMs Byungseok Roh. 2023. Honeybee: Locality-
inauniformwayandconductingthoroughhyper- enhancedprojectorformultimodalllm.
parameteroptimization,webenchmarkedtheper-
Guanzheng Chen, Fangyu Liu, Zaiqiao Meng, and
formanceofthesemethods. Ourfindingsindicate
Shangsong Liang. 2022. Revisiting parameter-
thatAdapterexcelsinaccuracy,stability,general- efficienttuning: Arewereallythereyet? InProceed-
ization, and producing fewer hallucinations. Ad- ingsofthe2022ConferenceonEmpiricalMethods
ditionally,wefoundthatfine-tuningtheconnector inNaturalLanguageProcessing,pages2612–2626,
AbuDhabi,UnitedArabEmirates.Associationfor
layersofMLLMssimultaneouslydoesnotalways
ComputationalLinguistics.
yield better results. Finally, comprehensive abla-
tionstudieswereperformedtounderstandthecon- LinChen,JinsongLi,XiaoyiDong,PanZhang,Con-
tributionsofthelocationofPEFTmodules,learn- ghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.
2023. Sharegpt4v: Improving large multi-modal
ing rate settings, and the size of training data on
modelswithbettercaptions.
PEFTperformance.
ZhuoChen,YichiZhang,YinFang,YuxiaGeng,Ling-
Limitations bingGuo,XiangChen,QianLi,WenZhang,Jiaoyan
Chen,YushanZhu,JiaqiLi,XiaozeLiu,JeffZ.Pan,
NingyuZhang,andHuajunChen.2024. Knowledge
Allourexperimentswereconductedwithinthede-
GraphsMeetMulti-ModalLearning: AComprehen-
finedframework,whichinvolvesconnectorlayers
siveSurvey. Inarxiv.
serving as the bridge between the visual encoder
andLLM,andnoadditionalmoduleswereinserted Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
on the LLM. Due to the limitation of computa-
Zhuang,YonghaoZhuang,JosephE.Gonzalez,Ion
tionalresources,wehavecurrentlyemployedonly
Stoica, and Eric P. Xing. 2023. Vicuna: An open-
a subset of datasets to conduct our analysis. Ad- sourcechatbotimpressinggpt-4with90%*chatgpt
ditionally, our choice of MLLMs on the analysis quality.
experimentsislimitedtoLLaVA-1.5-7BorQwen-
AliEdalati,MarziehTahaei,IvanKobyzev,VahidPar-
VL-Chat. In the future, we plan to conduct an
toviNia,JamesJ.Clark,andMehdiRezagholizadeh.
analysisonmoredatasetsandMLLMs.
2022. Krona: Parameter efficient tuning with kro-
neckeradapter.
9ChaoyouFu,PeixianChen,YunhangShen,YuleiQin, Zhiqiang Hu, Lei Wang, Yihuai Lan, Wanyu Xu, Ee-
MengdanZhang,XuLin,JinruiYang,XiawuZheng, PengLim,LidongBing,XingXu,SoujanyaPoria,
Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. and Roy Lee. 2023b. LLM-adapters: An adapter
2023. Mme:Acomprehensiveevaluationbenchmark family for parameter-efficient fine-tuning of large
formultimodallargelanguagemodels. languagemodels. InProceedingsofthe2023Con-
ferenceonEmpiricalMethodsinNaturalLanguage
YashGoyal,TejasKhot,DouglasSummers-Stay,Dhruv
Processing,pages5254–5276,Singapore.Associa-
Batra,andDeviParikh.2017. MakingtheVinVQA
tionforComputationalLinguistics.
matter: Elevating the role of image understanding
in Visual Question Answering. In Conference on GabrielIlharco,MitchellWortsman,RossWightman,
ComputerVisionandPatternRecognition(CVPR). CadeGordon,NicholasCarlini,RohanTaori,Achal
Dave,VaishaalShankar,HongseokNamkoong,John
ArnavGudibande,EricWallace,CharlieVictorSnell,
Miller,HannanehHajishirzi,AliFarhadi,andLud-
Xinyang Geng, Hao Liu, Pieter Abbeel, Sergey
wigSchmidt.2021. Openclip. Ifyouusethissoft-
Levine, and Dawn Song. 2024. The false promise
ware,pleaseciteitasbelow.
of imitating proprietary language models. In The
TwelfthInternationalConferenceonLearningRepre- ZiweiJi,NayeonLee,RitaFrieske,TiezhengYu,Dan
sentations. Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea
Madotto,andPascaleFung.2023. Surveyofhalluci-
DannaGurari,QingLi,AbigaleJ.Stangl,AnhongGuo,
nationinnaturallanguagegeneration. ACMComput.
ChiLin,KristenGrauman,JieboLuo,andJeffreyP.
Surv.,55(12).
Bigham.2018. Vizwizgrandchallenge: Answering
visualquestionsfromblindpeople. InProceedings
JaredKaplan,SamMcCandlish,TomHenighan,TomB.
of the IEEE Conference on Computer Vision and
Brown,BenjaminChess,RewonChild,ScottGray,
PatternRecognition(CVPR).
AlecRadford,JeffreyWu,andDarioAmodei.2020.
Scalinglawsforneurallanguagemodels.
Jie He and Yu Fu. 2023. Metaxcr: Reinforcement-
basedmeta-transferlearningforcross-lingualcom-
BrianLester,RamiAl-Rfou,andNoahConstant.2021.
monsense reasoning. In Proceedings of The 1st
The power of scale for parameter-efficient prompt
TransferLearningforNaturalLanguageProcessing
tuning. InProceedingsofthe2021Conferenceon
Workshop, volume203ofProceedingsofMachine
EmpiricalMethodsinNaturalLanguageProcessing,
LearningResearch,pages74–87.PMLR.
pages3045–3059,OnlineandPuntaCana,Domini-
JunxianHe,ChuntingZhou,XuezheMa,TaylorBerg- can Republic. Association for Computational Lin-
Kirkpatrick,andGrahamNeubig.2022. Towardsa guistics.
unifiedviewofparameter-efficienttransferlearning.
BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,
InProceedingsofthe10thInternationalConference
Jingkang Yang, and Ziwei Liu. 2023a. Otter: A
onLearningRepresentations(ICLR-2022).
multi-modalmodelwithin-contextinstructiontuning.
RuidanHe,LinlinLiu,HaiYe,QingyuTan,Bosheng arXivpreprintarXiv:2305.03726.
Ding,LiyingCheng,JiaweiLow,LidongBing,and
BohaoLi,RuiWang,GuangzhiWang,YuyingGe,Yix-
LuoSi.2021. Ontheeffectivenessofadapter-based
iaoGe,andYingShan.2023b. Seed-bench: Bench-
tuningforpretrainedlanguagemodeladaptation. In
marking multimodal llms with generative compre-
Proceedingsofthe59thAnnualMeetingoftheAsso-
hension. ArXiv,abs/2307.16125.
ciationforComputationalLinguisticsandthe11th
InternationalJointConferenceonNaturalLanguage
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto
Processing (Volume 1: Long Papers), pages 2208–
Usuyama,HaotianLiu,JianweiYang,TristanNau-
2222, Online. Association for Computational Lin-
mann, Hoifung Poon, and Jianfeng Gao. 2023c.
guistics.
LLaVA-med: Trainingalargelanguage-and-vision
Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, assistant for biomedicine in one day. In Thirty-
Bruna Morrone, Quentin De Laroussilhe, Andrea seventhConferenceonNeuralInformationProcess-
Gesmundo,MonaAttariyan,andSylvainGelly.2019. ingSystemsDatasetsandBenchmarksTrack.
Parameter-efficient transfer learning for NLP. In
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Proceedings of the 36th International Conference
Optimizing continuous prompts for generation. In
on Machine Learning, volume 97 of Proceedings
Proceedingsofthe59thAnnualMeetingoftheAsso-
of Machine Learning Research, pages 2790–2799.
ciationforComputationalLinguisticsandthe11th
PMLR.
InternationalJointConferenceonNaturalLanguage
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Processing (Volume 1: Long Papers), pages 4582–
Allen-Zhu,YuanzhiLi,SheanWang,LuWang,and 4597, Online. Association for Computational Lin-
WeizhuChen.2021. Lora: Low-rankadaptationof guistics.
largelanguagemodels.
ZiyiLin,ChrisLiu,RenruiZhang,PengGao,Longtian
WenboHu,YifanXu,YiLi,WeiyueLi,ZeyuanChen, Qiu, Han Xiao, Han Qiu, Chen Lin, Wenqi Shao,
andZhuowenTu.2023a. Bliva:Asimplemultimodal Keqin Chen, Jiaming Han, Siyuan Huang, Yichi
llmforbetterhandlingoftext-richvisualquestions. Zhang, Xuming He, Hongsheng Li, and Yu Qiao.
102023. Sphinx: The joint mixing of weights, tasks, graphs: Opportunitiesandchallenges. Transactions
and visual embeddings for multi-modal large lan- onGraphDataandKnowledge.
guagemodels.
Jonas Pfeiffer, Ivan Vulic´, Iryna Gurevych, and Se-
HaokunLiu,DerekTam,MohammedMuqeeth,JayMo- bastian Ruder. 2020. MAD-X: An Adapter-Based
hta,TenghaoHuang,MohitBansal,andColinRaffel. FrameworkforMulti-TaskCross-LingualTransfer.
2022. Few-shot parameter-efficient fine-tuning is InProceedingsofthe2020ConferenceonEmpirical
betterandcheaperthanin-contextlearning. MethodsinNaturalLanguageProcessing(EMNLP),
pages7654–7673,Online.AssociationforComputa-
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
tionalLinguistics.
Lee.2023a. Improvedbaselineswithvisualinstruc-
tiontuning.
Yixuan Su, Tian Lan, Huayang Li, Jialu Xu, Yan
Wang, and Deng Cai. 2023. Pandagpt: One
HaotianLiu,ChunyuanLi,QingyangWu,andYongJae
modeltoinstruction-followthemall. arXivpreprint
Lee. 2023b. Visual instruction tuning. ArXiv,
arXiv:2305.16355.
abs/2304.08485.
ZhiqingSun,ShengShen,ShengcaoCao,HaotianLiu,
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,
ChunyuanLi,YikangShen,ChuangGan,Liang-Yan
Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi
Gui, Yu-XiongWang, YimingYang, KurtKeutzer,
Wang,ConghuiHe,ZiweiLiu,KaiChen,andDahua
andTrevorDarrell.2023. Aligninglargemultimodal
Lin.2023c. Mmbench: Isyourmulti-modalmodel
modelswithfactuallyaugmentedrlhf.
anall-aroundplayer?
Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai- HugoTouvron,ThibautLavril,GautierIzacard,Xavier
WeiChang,Song-ChunZhu,OyvindTafjord,Peter Martinet,Marie-AnneLachaux,TimothéeLacroix,
Clark,andAshwinKalyan.2022. Learntoexplain: BaptisteRozière,NamanGoyal,EricHambro,Faisal
Multimodalreasoningviathoughtchainsforscience Azhar,AurelienRodriguez,ArmandJoulin,Edouard
questionanswering. InNeurIPS. Grave,andGuillaumeLample.2023. Llama: Open
andefficientfoundationlanguagemodels.
PanLu,LiangQiu,JiaqiChen,TonyXia,YizhouZhao,
WeiZhang,ZhouYu,XiaodanLiang,andSong-Chun Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi
Zhu.2021. IconQA:Anewbenchmarkforabstract Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang,
diagramunderstandingandvisuallanguagereason- LeiZhao,XixuanSong,JiazhengXu,BinXu,Juanzi
ing. InThirty-fifthConferenceonNeuralInformation Li, Yuxiao Dong, Ming Ding, and Jie Tang. 2023.
ProcessingSystemsDatasetsandBenchmarksTrack Cogvlm: Visualexpertforpretrainedlanguagemod-
(Round2). els.
Sourab Mangrulkar, Sylvain Gugger, Lysandre De-
YaqingWang,SahajAgarwal,SubhabrataMukherjee,
but, Younes Belkada, Sayak Paul, and Benjamin
Xiaodong Liu, Jing Gao, Ahmed Hassan Awadal-
Bossan. 2022. Peft: State-of-the-art parameter-
lah, and Jianfeng Gao. 2022. AdaMix: Mixture-
efficientfine-tuningmethods. https://github.
of-adaptationsforparameter-efficientmodeltuning.
com/huggingface/peft.
In Proceedings of the 2022 Conference on Empiri-
calMethodsinNaturalLanguageProcessing,pages
Kenneth Marino, Mohammad Rastegari, Ali Farhadi,
5744–5760,AbuDhabi,UnitedArabEmirates.As-
andRoozbehMottaghi.2019. Ok-vqa:Avisualques-
sociationforComputationalLinguistics.
tionansweringbenchmarkrequiringexternalknowl-
edge. InConferenceonComputerVisionandPattern
ZhiyangXu,YingShen,andLifuHuang.2023. Multi-
Recognition(CVPR).
Instruct: Improvingmulti-modalzero-shotlearning
viainstructiontuning. InProceedingsofthe61stAn-
AnandMishra,ShashankShekhar,AjeetKumarSingh,
nualMeetingoftheAssociationforComputational
and Anirban Chakraborty. 2019. Ocr-vqa: Visual
Linguistics(Volume1: LongPapers),pages11445–
question answering by reading text in images. In
11465,Toronto,Canada.AssociationforComputa-
ICDAR.
tionalLinguistics.
OpenAI,:JoshAchiam,StevenAdler,SandhiniAgar-
wal, Lama Ahmad, Ilge Akkaya, Florencia Leoni HaoxuanYou, HaotianZhang, ZheGan, XianzhiDu,
Aleman,DiogoAlmeida,JankoAltenschmidt,Sam BowenZhang,ZiruiWang,LiangliangCao,Shih-Fu
Altman,ShyamalAnadkat,RedAvila,andetal.2023. Chang, and Yinfei Yang. 2023. Ferret: Refer and
Gpt-4technicalreport. groundanythinganywhereatanygranularity. arXiv
preprintarXiv:2310.07704.
Jeff Z. Pan, Simon Razniewski, Jan-Christoph Kalo,
SnehaSinghania,JiaoyanChen,StefanDietze,Hajira PeterYoung,AliceLai,MicahHodosh,andJuliaHock-
Jabeen, Janna Omeliyanenko, Wen Zhang, Matteo enmaier. 2014. From image descriptions to visual
Lissandrini, ussa Biswas, Gerard de Melo, Angela denotations: Newsimilaritymetricsforsemanticin-
Bonifati, Edlira Vakaj, Mauro Dragoni, and amien ferenceovereventdescriptions. Transactionsofthe
Graux.2023. Largelanguagemodelsandknowledge AssociationforComputationalLinguistics,2:67–78.
11YuexiangZhai,ShengbangTong,XiaoLi,MuCai,Qing
Qu,YongJaeLee,andYiMa.2023. Investigatingthe
catastrophicforgettinginmultimodallargelanguage
modelfine-tuning. InConferenceonParsimonyand
Learning(ProceedingsTrack).
Renrui Zhang, Jiaming Han, Chris Liu, Aojun Zhou,
PanLu,HongshengLi,PengGao,andYuQiao.2024.
LLaMA-adapter: Efficientfine-tuningoflargelan-
guagemodelswithzero-initializedattention. InThe
TwelfthInternationalConferenceonLearningRepre-
sentations.
DeyaoZhu, JunChen, XiaoqianShen, XiangLi, and
MohamedElhoseiny.2024. MiniGPT-4: Enhancing
vision-languageunderstandingwithadvancedlarge
languagemodels. InTheTwelfthInternationalCon-
ferenceonLearningRepresentations.
12A DatasetsSetup theShareGPT4vdataset(Chenetal.,2023),which
comprises100Khigh-qualitycaptionsfromvarious
A.1 DetailedDescriptionofDatasets
imagesgeneratedbyGPT4-vision. Qwen-VL-Chat
Since OCRVQA and VQAv2 are very large, we comprisesViT-G/16(Ilharcoetal.,2021),Qwen-
randomlyextract20ksamplesfromtheirtraining 7B(Baietal.,2023a),andacross-attentionmodule
setsasthenewtrainingsetsandanother5ksamples servingastheconnector. Duringitsvisioninstruc-
from the test sets to create the new test sets. We tiontuningphase,Qwen-VL-Chatupdatesonlythe
utilized a variety of multimodal datasets for fine- parametersoftheconnectorandtheLLM.
tuning and evaluating. Detailed information for
B.2 HyperParameters
eachdatasetisprovidedinTable8below.
FollowingHuetal.(2023b),weconductedthefol-
A.2 SeenDatasetsforAllMLLMs
lowing parameter selection experiments. Due to
In our experiments, we divided the datasets into computationalconstraints,weconcentrateonthe
unseendatasetsandseendatasets. InTable9,we SQA (all) dataset, which has a test set that con-
presentthetrainingdatasetsusedbyeachMLLM tainsbothmultimodalandtext-onlydata. So,our
inourexperiments. Itisworthnotingthat,dueto goal is to enhance the model’s multimodal per-
thedatausedinbothpre-trainingandfine-tuning formance while maximizing its performance on
stagesbeingfilteredorsampledfromthedatasets, text-onlydatasets.
to ensure a fair comparison, it is imperative that WechooseLLaVA-1.5-7Basthebasemodelset
each seen dataset is fully seen by the model. We the random seed to 42, freeze the visual encoder,
composedamixeddatasetconsistingofthreeseen andfine-tunetheconnectorlayersandLLM.Note
datasets: OKVQA,VQAv2,andOCRVQA.Sub- thatwedonotconsiderIA3inthisexperimentasit
sequently, we kept the visual encoder frozen and cannotchangethenumberoftrainableparameters.
conducted a Full Fine-Tuning on each model us- Welookatdifferentparametersettings: LoRArank
ing this mixed dataset. During this process, we of {16, 32, 64, 128}, Adapter bottleneck size of
set the learning rate to 2e-5 and the global batch {32,64,128,256},andvirtualtokensof{10,20,30,
size to 128. We maintained all other settings the 40} in Prefix-Tuning. Figure 5 shows the results
same as those used in the original paper for Full withvariousparametersettingsonSQA(all). We
Fine-Tuning(Liuetal.,2023a;Baietal.,2023b). observe that for LoRA, a rank of 128 yielded an
accuracyof89.3%. Settingtheadapter’sbottleneck
A.3 Instruction-followingDataTemplate
sizeto256resultedinanaccuracyof87.4%. Inthe
MultimodalInstruction-FollowingTuningisacru-
caseofPrefix-Tuning,settingthevirtualtokensto
cial component for the success of MLLMs. The
20achievedthebestaccuracyof68.0%.
MLLMsusedinthisexperimentfollowthesame
Basedonourfindings,forallremainingexper-
approach in data processing as the original mod-
iments, we use the following PEFT parameters:
els. Therefore, there are slight differences in the
LoRARank=128,AdapterBottleneckSize=256,
processingofLLaVA-1.5,ShareGPT4v,andQwen-
andPrefixVirtualToken=20. Moredetailedhy-
VL-Chat. They employ different image annota-
perparameter settings are presented in Table 11.
tions,butthedatainstructionformatremainscon-
WeutilizedtwoNVIDIAA10080GBGPUsand
sistent. Table10showsthetemplateofalldataset
DeepSpeedfordistributedtraining.
typesusedintheexperiments.
C TrainingofVisualEncoderAnalysis
B ModelsandHyperparameters
Qwen-vl-chat unfroze the visual encoder during
B.1 Models
pre-training, which improved the model’s perfor-
LLaVA-1.5 consists of CLIP-ViT-L/14 (Ilharco mance. However,mostcurrentMLLMsmaintain
et al., 2021), Vicuna-v1.5 (Chiang et al., 2023), thevisualencoderfrozenduringtask-specificfine-
andanMLPservingastheconnectorlayer. During tuning. Wefine-tunedthevisualencoderonSQA
fine-tuningwithmultimodalinstruction-following andVizWizforunseentasks,andonOKVQAand
data,LLaVA-1.5updatesonlytheparametersofthe OCRVQAforseentasks. Theresultsareshownin
MLP connector and the LLM, while keeping the the Table 12. We found that although unfreezing
parametersofthevisualencoderfrozen. ShareG- thevisualencoderdoesnotsignificantlyincrease
PTv4isobtainedbyfine-tuningLLaVA-1.5using trainingresourceconsumption,theimprovementin
13Dataset Task Split Metric Answertype Description DatasetType #Train #Test(Val)
Flickr30K ImageCaption train&test CIDEr(↑) Caption Imagedatasetwithcaptionsfornatural Unseen 31k 4k
scenes.
IconQA-blank VisualReasoning train&test Accuracy(↑) Word Visualreasoningwithabstracticons,no Unseen 11k 4k
text.
IconQA-txt VisualReasoning train&test Accuracy(↑) Word Abstract icon reasoning with textual Unseen 19k 6k
hints.
OKVQA KnowledgeGroundedVQA train&test VQA-Score(↑) Phrase VQArequiringexternalknowledge. Seen 9k 5k
SQA(img) KnowledgeGroundedVQA train&test Accuracy(↑) Option Science-focusedmultiple-choiceVQA Unseen 13k 4k
OCRVQA ReadingComprehensionVQA train&test Accuracy(↑) Phrase VQAwithtextrecognitioninimages. seen 20k 5k
VQAv2 GeneralVQA train&test VQA-Score(↑) Phrase Diverseopen-endedvisualquestionan- Unseen 20k 5k
swering.
VizWiz GeneralVQA train&val VQA-Score(↑) Phrase VQAsourcedfromvisuallyimpaired seen 20k 4k
users’photos.
Table8: Detaileddescriptionforthedatasetsweused,includingtasktypes,trainingandtestsplit,evaluationmetric,
statistic,datasettype,andthetypeofanswer. Tobespecific,“Seen”meansthatthedatasethasbeenusedasa
pre-trainingdatasetinthemodelbeingevaluated. “Unseen”referstodatasetsthathavenotbeenencounteredbythe
model.
model Phrase Seendatasets
Pretrained CC-595K,LLaVA-Instruct-158K
LLaVA
Fine-tuning VQAv2,OKVQA,OCRVQA,GQA,A-OKVQA,TextCaps,RefCOCO,VG
LAION-en,LAION-COCO,DataComp,Coyo,CC12M,CC3M,SBU,COCO
Caption, LAION-zh, GQA, VGQA, VQAv2, DVQA, OCRVQA, DocVQA.
Pretrained
Qwen-VL-Chat TextVQA,ChartQA,AI2D,GRIT,VG,RefCOCO+,RefCOCOg,SynthDoG-en
&zh,CommonCrawlofpdf&HTML,In-houseData
Fine-tuning OKVQA,OCRVQA,VQAv2
Pretrained CC-595K,LLaVA-Instruct-158K,ShareGPT4V-PT
ShareGPT4v VQAv2,OKVQA,OCRVQA,A-OKVQA,GQA,TextCaps,RefCOCO,VG,
Fine-tuning
ShareGPT4V
Table9: Thedatasetsusedduringthepretrainingandfurtherfine-tuningprocessesofMLLMs.
model performance is limited and, in most cases, ismoreefficient.
canevenleadtoperformancedegradation. There-
F StabilityAnalysis
fore,inourexperiments,weadheredtothemain-
streamsettingandkeptthevisualencoderfrozen.
Figure7presentsthetrainingloss,showingthatthe
stability of PEFT varies across different datasets.
D LocationAnalysis
WeobservethatPrefix-TuningandAdapterexhibit
We also conducted the Module Location experi- largerfluctuationsintraininglossateachstepwhen
ment on Qwen-VL-Chat. Table 13 shows the re- trained with different seeds, followed by LoRA,
sults. We observe that for LoRA and IA3, Both whileIA3showsrelativelysmallerfluctuations.
settingsachievedthebestresults. AsforAdapter, Wealsoinvestigatewhetherthelearningratecor-
insertingitonlyintotheMLPlayeryieldedthebest relates with stability. We conducted experiments
performance. It reveals that the results on Qwen- withlearningratesof{2e-4,5e-5,1e-5,5e-6}. The
VL-ChatareconsistentwiththoseonLLaVA-1.5- resultsareshowninTable14. Itcanbeobserved
7B. thatIA3andPrefix-Tuningdemonstratemoresta-
ble performance at smaller learning rates, Prefix-
E DataScaleAnalysis
Tuningtendstostabilizegraduallyasthelearning
ratedecreases.
Figure6showstheimprovementintheaverageper-
formanceofthefourPEFTmethodsasresources
G OverftittingandGeneralization
transitionfromlowtohigh. Whendatasetstransi-
Analysis
tionfromlowtomediumresources,allfourPEFT
methods achieve performance improvements of Overfitting and Generalization experiments were
over 10%, higher than from medium to high re- conducted on three unseen VQA datasets. The
sources. Thus,whencomputationalresourcesare train-loss curves for IconQA-txt and VizWiz are
limited,fine-tuningonmedium-resourcedatasets depictedinFigure8. ForIconQA-txt,allfourPEFT
14Model ImageAnnotation[IMAGE]
LLaVAImage <Image>
QwenImage <img></img>
Task InstructionTemplate
[IMAGE]Shareaconciseinterpretationoftheimageprovided.
[IMAGE]Renderaclearandconcisesummaryofthephoto.
[IMAGE]Writeatersebutinformativesummaryofthepicture.
[IMAGE]Offerasuccinctexplanationofthepicturepresented.
ImageCaption [IMAGE]Describetheimageconcisely.
[IMAGE]Provideabriefdescriptionofthegivenimage.
[IMAGE]Createacompactnarrativerepresentingtheimagepresented.
[IMAGE]Relayabrief,clearaccountofthepictureshown.
[IMAGE]Summarizethevisualcontentoftheimage.
[IMAGE]Giveashortandclearexplanationofthesubsequentimage
KnowledgeGroundedVQA [IMAGE]{Question}Answerthequestionusingasinglewordorphrase.
[IMAGE]{Question}A.choice1,B.choice2,C.choice3,...
[IMAGE]{Question}Fillintheblanksin(_)oranswerthisquestion.
VisualReasoning [IMAGE]{Question}Choices:choice1,choice2,choice3,...Chooseanoptionfromthe
choicestoanswerthequestion.
ReadingComprehensionVQA [IMAGE]{Question}Answerthequestionusingasinglewordorphrase.
[IMAGE]Question:{Question}
[IMAGE]Question:{Question}Whentheinformationisinsufficient,respondwith"Unan-
GeneralVQA
swerable".Answerthequestionusingasinglewordorphrase.
[IMAGE]{Question}Answerthequestionusingasinglewordorphrase.
Table10: Theinstructionformatofdifferenttaskswhenwefine-tuneMLLMs.
methodsexhibitstrongrobustness,withLoRAper- ducing shorter and more accurate texts and thus
formingthebest. OnVizwiz,Prefix-Tuningshows requiringlesscomputingpower,evenasthenum-
thestrongestrobustnesscomparedtotheotherthree ber of trainable parameters grows. In the case of
PEFTmethods. the 13B model, the increase in the total number
ofparametersisnotnecessarilyreflectedinanin-
H Efficiency creaseinthepercentageoftrainableparametersto
reachagoodperformance. AccordingtoTable 15,
We investigate the number of trainable parame-
theIA3methodwithoutafrozenconnectoryields
ters, the training and inference Flops for vari-
significantlyfewertrainableparameterscompared
ousMLLMswhenfine-tunedwithdifferentPEFT
to the Adapter and LoRA methods, but achieves
methodsinthissection. Table15showstheresults.
thehighestperformance.
WederivethetrainingandinferenceFLOPsinac-
Thedetailsoftheflopscalculationare:
cordancewiththemethodologyoutlinedinKaplan
TrainingFlops Sincethecomputationalcostof
etal.(2020). Ouranalysisshowsthatmodelsper-
the backward pass is approximately twice as the
formbetterwhentheconnectorisnotfrozen,which
forwardpass,wemodifytheformulaas:
suggeststhathavingmoretrainableparametersim-
proves the performance, even though the overall
TrainFlops = (2P +4P )×N (5)
parameter efficiency might decrease. Within the f t t
7B model, the Adapter method without freezing where P and P represent the number of frozen
f t
connector is remarkably efficient, utilizing only and trainable parameters respectively, N is the
t
3.060% of trainable parameters and yet securing numberofinputandmodel-generatedtokens.
ahighperformancerateof76.3%,showcasingan
InferenceFlops Wecalculatetheinferenceflops
optimalbalancebetweenparameterefficiencyand
basedonthefollowingequation:
modelefficacy.
Additionally, using the IA3 method without
freezing the connector can reduce the computing InferenceFlops = 2×(P +N d N ) (6)
g layer model t
effort needed for training. With more trainable
parameters,themodelbecomesmoreefficient,pro- where P indicates non-embedding parameters,
g
1588.00 90.00 75.00
87.75 89.75 70.00 68
87.50 87.4 89.50
87.3 89.3 65.00
87.25 89.25
87 60.00
87.00 89.00 55.3 55.7
86.7 88.8 55.00 53.3
86.75 88.75 88.6
88.5 50.00
86.50 88.50
86.25 88.25 45.00
86.00 88.00 40.00
s32 s64 s128 s256 r16 r32 r64 r128 vt10 vt20 vt30 vt40
Bottleneck Size Rank Virtual Token
(a)Adapter (b)LoRA (c)Prefix
Figure5: AverageaccuracyofResultsofvariousPEFTparametersonSQA(all). s: BottleneckSize. r: LoRARank.
vt: VirtualToken
Configuration LLaVA-7B LLaVA-13B Qwen-VL-Chat ShareGPT4v
ViT Vicuna-v1.5-7B Vicuna-v1.5-7B Qwen-7B Vicuna-v1.5-7B
LLM CLIP-ViT-L/14 CLIP-ViT-L/14 ViT-G/16 CLIP-ViT-L/14
Connector MLP MLP CrossAttn MLP
Optimizer AdamW
Connectorlearningrate 2e-5 2e-5 1e-5 2e-5
Learningrateschedule cosinedecay
Warm-upratio 0.03 0.03 0.01 0.03
Weightdecay 0.0 0.0 0.1 0.0
Globalbatchsize 128 128 128 128
GradientAcc 1 1 1 1
Trainingepoch 3
Numericalprecision bfloat16
Table11: TraininghyperparameterswhenweusePEFTmethodstofine-tunethosemodels.
Average Performance Difference ployedLLaVA-1.5-7Btogeneratecaptionsforall
20%
Source(Low->Medium) images in the Flickr30k test set in a zero-shot
18% Source(Medium->High)
16.40% manner. Subsequently, we randomly sampled 1k
15% 14.30% captions and manually curated 100 correct cap-
13.30%
12% tionswithouthallucinations. Then,weutilizedthe
10.20%
10% LLaVA-1.5-7Bmodelfine-tunedwithfourPEFT
8.30%
8% 6.90% methods on IconQA-txt to generate captions for
5% 5.00% these 100 samples. Thereafter, we manually an-
4.00%
notatethefine-tunedmodel-generatedoutputsand
2%
countthenumberofhallucinationsamples.
0%
IA3 Adapter LoRA Prefix
PEFT Method
Onesampleisselected,asillustratedinFigure9.
Figure6: Averageperformancedifferencefordifferent
Inthisexample,theoriginalLLaVAmodeldelivers
PEFTmethodsinvariousdata-scalingsettings. Source
a hallucination-free description, accurately iden-
(Low->Medium): fine-tuned dataset scaling change
fromlow-resourcetohigh-resource. Source(Medium- tifying the color and actions depicted in the im-
>High):fine-tuneddatasetscalingchangefrommedium- age. Ontheotherhand,theAdaptermodelincor-
resourcetohigh-resource. rectly identifies the girl’s face as red. The IA3
model inaccurately attributes a mustache to the
N isthenumberofmodel’slayers,andd
layer model
girl and misidentifies her hair color as red. The
representsthedimensionoftheresidualstream.
LoRAmodelalsofailstorecognizethegirl’shand
I CaseStudy andreferstoaredsubstance,whichisnotpresent.
As for the Prefix model, it attempts to provide a
I.1 HallucinationAnalysis
detailed description of the picture, including the
In this section, we explain how we tested the mentionofaponytailandattributingemotionssuch
model’s hallucination. As the first step, we em- asa"funnyorplayfulgesture"tothesubject. How-
16
ycaruccA ycaruccA ycaruccAModel Method SQA(img) VizWiz OKVQA OCRVQA Avg
Adapter 84.4 67.6 59.8 65.2 69.3
-w/VisualEncoder 84.2 67.1 60.5 65.1 69.2
LoRA 86.2 66.5 56.5 66.7 69.0
-w/VisualEncoder 85.9 66.7 55.9 66.8 68.8
LLaVA-1.5-7B
IA3 82.7 61.9 60.5 67.1 68.1
-w/VisualEncoder 83.4 62.2 60.3 66.8 68.2
Prefix 68.2 60.8 61.3 68.5 64.7
-w/VisualEncoder 65.9 62.1 60.8 68.7 64.4
Table12: ResultsoftuningvisualencoderonLLaVA-1.5-7BwithfourPEFTmethods. w/VisualEncoder: Tuning
theVisualEncoder.
Method Location SQA(img) VizWiz IconQA-txt IconQA-blank OKVQA OCRVQA VQAv2 Avg
Qwen-VL-Chat
Attn 85.2 64.5 92.2 88.3 49.3 71.7 63.9 73.6
Adapter MLP 81.2 69.3 90.8 87.5 51.1 69.3 70.7 74.3
Both 85.6 67.1 91.8 90.5 50.7 55.9 69.2 73.0
Attn 80.2 67.3 80.5 87.1 43.3 69.6 39.1 66.7
LoRA MLP 74.5 60.7 78.5 88.5 43.6 67.8 60.1 67.7
Both 84.0 68.8 71.9 83.5 43.5 67.0 63.3 68.9
Attn 66.4 69.1 58.2 21.7 50.8 63.3 77.3 58.1
IA3 MLP 62.8 68.3 59.8 23.1 51.3 62.7 77.6 57.9
Both 67.3 69.8 57.3 28.7 50.5 62.1 77.5 59.0
Table13: AverageresultsofPEFTmodulelocationonQwen-VL-Chat.
OKVQA SQA(img) I.2 QualitativeIllustrations
learningrate=2e-4 54.4 ±0.44 81.2 ±1.27 Inthissection, Werandomlysampledseveralex-
Adapter learningrate=5e-5 58.9 ±0.81 81.3 ±1.85 amplesfromeachdatasetandprovidedtheoriginal
learningrate=1e-5 60.3 72.4
±0.10 ±0.25 labelsalongwiththeoutputsofvariousPEFTmod-
learningrate=5e-6 58.5 82.2
±0.35 ±1.31
els. SeeFigures14to23.
learningrate=2e-4 56.7 86.0
±0.40 ±0.35
learningrate=5e-5 59.8 83.1
LoRA ±0.10 ±0.45
learningrate=1e-5 62.9 74.0
±0.06 ±2.71
learningrate=5e-6 61.6 71.2
±0.06 ±1.64
learningrate=2e-4 62.7 81.7
±0.98 ±0.87
learningrate=5e-5 61.4 77.1
IA3 ±4.89 ±2.54
learningrate=1e-5 62.9 72.2
±0.20 ±1.28
learningrate=5e-6 58.8 70.8
±0.06 ±0.64
learningrate=2e-4 60.9 35.3
±0.21 ±0.85
learningrate=5e-5 59.6 64.6
Prefix ±0.06 ±0.01
learningrate=1e-5 61.5 72.3
±0.06 ±0.21
learningrate=5e-6 60.8 70.2
±0.01 ±0.01
Table 14: Performance on OKVQA and SQA (img)
datasets with different training learning rate of PEFT
module.
ever,theseemotionscannotbeconfirmedsimplyby
viewingthepicture. Additionally,thePrefixmodel
presents more severe hallucinations compared to
the others, as it "imagines" another hand that is
notvisibleintheimage. Moreexamplesatseveral
epochsareillustratedinthissection.
171.0
1.4 Adapter
IA3
1.2 0.8 LoRA
Prefix
1.0
0.6
0.8
0.6 0.4
0.4 Adapter
IA3 0.2
0.2 LoRA
Prefix
0.0 0 1 2 3 0.0 0 1 2 3
Epoch Epoch
(a)TrainLossofOKVQA (b)TrainLossofSQA
Figure7: Trainlossreportedacrossthreerunswithdifferentrandomseeds. Thelineplottedthemeanofthreeseeds,
wheretheshadedregionrepresentsits95%confidenceinterval.
Figure8: Train-LosscurveonIconQA-txtandVizwiz. TheorangelineshowsTrainLoss. Evallossiscoloredwith
green.
Model Method Trainableparameters Trainflops Inferenceflops Performance
Adapter 2.771% 1.526e+15 4.105e+10 74.6
-w/connector 3.060% 1.532e+15 4.109e+10 76.3
LoRA 4.332% 1.552e+15 4.092e+10 76.1
-w/connector 4.616% 1.561e+15 4.099e+10 76.7
LLaVA-v1.5
7B IA3 0.009% 1.424e+15 4.028e+10 63.1
-w/connector 0.306% 1.248e+15 3.695e+10 76.3
Prefix 0.074% 1.496e+15 4.159e+10 68.1
-w/connector 0.371% 1.498e+15 4.154e+10 69.6
Adapter 2.301% 2.898e+15 6.946e+10 77.2
-w/connector 2.532% 2.908e+15 6.951e+10 77.7
LoRA 3.615% 2.947e+15 6.942e+10 78.3
-w/connector 3.842% 3.083e+15 7.127e+10 77.8
LLaVA-v1.5
13B IA3 0.007% 2.660e+15 6.716e+10 65.0
-w/connector 0.243% 2.634e+15 6.667e+10 78.5
Prefix 0.061% 2.741e+15 6.838e+10 67.9
-w/connector 0.297% 2.749e+15 6.840e+10 69.1
Table15: Efficientcomparisonacrossalltypesofsettings. Thenumberoftrainableparameters,PEFTFlops,and
Inference Flops are analyzed, considering whether to tune the connector with different PEFT methods on two
modelswithdifferentparameters. w/connector: Tuningtheconnector.
18
ssoL ssoLPrompt:
Renderaclearandconcisesummaryofthephoto.
Label:
A blond girl in a yellow shirt with blue flowers is covering her face.
A blond girl covering her eyes with her hand in yellow dress.
Alittlegirlinayellowshirtisstandingcoveringhereyes.
A small girl covering her eyes in a yellow shirt , outdoors.
Littlegirlcoveringhereyes!
LLaVA-1.5-7B:
Ablondgirlinayellowshirtiscoveringherface.
LLaVA-1.5-7B-Adapter:
Alittlegirlwitharedfaceiscoveringhereyeswithherhand.
LLaVA-1.5-7B-IA3:
Agirlwithredhairandaredmustache.
LLaVA-1.5-7B-LoRA:
Ayounggirlwitharedsubstanceonherface,coveringhereyes.
LLaVA-1.5-7B-Prefix:
Alittlegirlwithaponytailisplayfullyhidingherfacewithonehandwhilethe otherhandisholding
hernose,possiblymakingafunnyorplayfulgesture.
Figure9: AqualitativehallucinationexamplegeneratedbyLLaVA-1.5-7BandLLaVA-1.5-7BwithvariousPEFT
methods. LLaVA-1.5-7Bgeneratedhallucination-freetext. Afterfine-tuning,allmodelsgeneratedhallucinations.
19Prompt:
Renderaclearandconcisesummaryofthephoto.
Label:
Three girls are walking across a bridge covered with wood and
surroundedbytrees.
Three girls walk under an arbor on a path surrounded by trees.
Three women walk through a plant covered atrium hallway.
Three girls walking along an arched garden pathway.
Threewomenwalkingthroughatunnelinapark.
LLaVA-1.5-7B:
Three girls walking along an arched garden pathway.
LLaVA-1.5-7B-Adapter:
Three people are standing under a bridge, enjoying the shade it provides.
LLaVA-1.5-7B-IA3:
Three people standing under a bridge.
LLaVA-1.5-7B-LoRA:
Threepeoplestandinginacircularwalkway.
LLaVA-1.5-7B-Prefix:
Three people walking on a bridge.
Prompt:
Offerasuccinctexplanationofthepicturepresented.
Label:
Threechildrenplayingwithhayoutdoors,throwingsome
at camera lens.
Three young children play in the hay, throwing some at the
camera.
Twoyoungchildrenarethrowinghay.
Three children play on corn stacks.
Three children playing in hay.
LLaVA-1.5-7B:
Threeyoungchildrenplayinthehay,throwingsomeatthecamera.
LLaVA-1.5-7B-Adapter:
Two young children playing in the hay
LLaVA-1.5-7B-IA3:
Two children playing in hay, one girl is jumping, the other is smiling.
LLaVA-1.5-7B-LoRA:
Two children are standing on a pile of hay, one of them is raising her arm in the air.
LLaVA-1.5-7B-Prefix:
Theimageshowstwolittlegirlsstandingonapileofhay.Theyseemtobeenjoyingtheirtime
together, possibly playing or celebrating.
Figure10: Arandomlysampledhallucinationresults(fromFlickr30K)ofvariousPEFTmethods’modelstrained
ontheIconQA-txtdatasetfor3epochs.
20Prompt:
Write a terse but informative summary of the picture.
Label:
A group of people in formal attire are in the background and a
maninawhiteshirtwithkhakipantsjumpsfromthedirtroadto
the green grass.
Someone is getting some serious air while the more formally
dressed group gathers in the background.
A guyisjumpingintheairwhileagroupofpeoplestandaround
and ignore him.
Man started jumping because the sprinklers were about to go off.
Man jumping over a small stone wall in front of a group of people.
LLaVA-1.5-7B:
Man started jumping because the sprinklers were about to go off.
LLaVA-1.5-7B-Adapter:
A man jumps in the air in apark.
LLaVA-1.5-7B-IA3:
A man jumping in air.
LLaVA-1.5-7B-LoRA:
A man is jumping in the air, while a group of people watches.
LLaVA-1.5-7B-Prefix:
A manisjumpingintheairinfrontofagroupofpeoplewhoarewatchinghim.
Prompt:
Renderaclearandconcisesummaryofthephoto.
Label:
Silverplaneinabluesky,readytolandwithitswheelsdown,
while spectators watch behind a high fence.
An airplane flying over a fence with a few bystanders watching it as
it approaches its landing.
Groupofpeoplestandnearafencewatchingaplaneflyoverthem.
People watch as a plane flies over their heads at an airfield.
A group of people are watching a plane flying low.
LLaVA-1.5-7B:
An airplane flying over a fence with a few bystanders watching it as it approaches its landing.
LLaVA-1.5-7B-Adapter:
A large commercial airplane is flying in the air above a fence with people sitting on the ground.
LLaVA-1.5-7B-IA3:
A plane flying in the sky.
LLaVA-1.5-7B-LoRA:
A planeisflyingintheair,withpeoplewatchingfrombelow.
LLaVA-1.5-7B-Prefix:
A large commercial airplaneis flying in the sky with its landing gear down, as people watch it from
the ground.
Figure11: Arandomlysampledhallucinationresults(fromFlickr30K)ofvariousPEFTmethods’modelstrained
ontheIconQA-txtdatasetfor6epochs.
21Prompt:
Provide a brief description of the given image.
Label:
A group of people , a wedding party including a bride and groom,
are walking down a path lined by benches and surrounded by grass
andtrees.
Several members of a wedding party , including the bride , are
walking past a lamp and some park benches.
A bride and a groom walk down a sidewalk with other guests
surroundingthem.
The people are following the Bride and the Groomat a wedding.
A wedding day with the bride in a bright white beautiful dress.
LLaVA-1.5-7B:
ThepeoplearefollowingtheBrideandtheGroomatawedding.
LLaVA-1.5-7B-Adapter:
A group of people are walking down a sidewalk, with some of them in wedding dresses.
LLaVA-1.5-7B-IA3:
A group of people walking down a sidewalk.
LLaVA-1.5-7B-LoRA:
A group of people walking down a path, with a bride in a white dress and a man in a blue suit.
LLaVA-1.5-7B-Prefix:
A group of people walking down a sidewalk.
Prompt:
Render a clear and concise summary of the photo
Label:
An older African-American male is playing an electric guitar in
the public street.
An older black man plays an electric guitar on a city street.
Anolderblackmanplaysanelectricguitaronthesidewalk.
An older man is playing a red guitar on the street.
A man trying to play guitar for entertainment.
LLaVA-1.5-7B:
An older man is playing red guitar on the street.
LLaVA-1.5-7B-Adapter:
Amanisplayingredguitar.
LLaVA-1.5-7B-IA3:
An older man is playing guitar.
LLaVA-1.5-7B-LoRA:
A man is sitting on a street corner playing a guitar.
LLaVA-1.5-7B-Prefix:
A man is sitting on a stool on a city street corner, playing a guitar.
Figure12: Arandomlysampledhallucinationresults(fromFlickr30K)ofvariousPEFTmethods’modelstrained
ontheIconQA-txtdatasetfor9epochs.
22Prompt:
Offer a succinct explanation of the picture presented.
Label:
Smiling boy in white shirt and blue jeans in front of rock wall with
maninoverallsbehindhim.
A little boy is standing on the street while a man in overalls is
working on a stone wall.
A young child is walking on a stone paved street with a metal pole
andamanbehindhim.
A boy smiles in front of a stony wall in a city.
A young boy runs arossthe street.
LLaVA-1.5-7B:
A little boy is standing on the street while a man in overalls is working on a stone wall.
LLaVA-1.5-7B-Adapter:
A young boy is walking on a sidewalk, smiling at the camera.
LLaVA-1.5-7B-IA3:
A boy in a white shirt running.
LLaVA-1.5-7B-LoRA:
A youngboyiswalkingdownthestreetwitha maninablueshirtandablacklamppost.
LLaVA-1.5-7B-Prefix:
A young boy in a white shirt and blue jeans walking down the street.
Prompt:
Renderaclearandconcisesummaryofthephoto.
Label:
Two paddlers in a red canoe approach a couple of large rocks in the
rushing river with conifers in the background.
Two people paddle a canoe down a rough stretch of river near a pine
forest.
A couple in a red canoe paddle down a rough river lined with trees.
Two people canoeing down a river in a red canoe.
Two people are in a canoe.
LLaVA-1.5-7B:
Two people paddle a canoe down a rough stretch of river near a pine forest.
LLaVA-1.5-7B-Adapter:
Two people in a red boat on a river.
LLaVA-1.5-7B-IA3:
Two people in a red and yellow boat on a river.
LLaVA-1.5-7B-LoRA:
A maninaredcanoeisnavigatingthroughroughwatersnearrocks.
LLaVA-1.5-7B-Prefix:
A man and a woman in a red canoe.
Figure13: Arandomlysampledhallucinationresults(fromFlickr30K)ofvariousPEFTmethods’modelstrained
ontheIconQA-txtdatasetfor12epochs.
23Prompt:
Relayabrief,clearaccountofthepictureshown.
Label:
A footballplayerdressedingreenandwhiteadvancestheball
while watched by a defender in blue .
LLaVA-1.5-13B-Adapter:
A soccer player in a blue uniform is chasing a player in a white uniform who is kicking the soccer ball.
-w/connector:
Two soccer players, one in blue and one in white, are running after the ball
LLaVA-1.5-13B-IA3:
A soccer player in a green and white uniform kicks the ball.
-w/ connector:
Two soccer players are playing a game in front of a crowd
LLaVA-1.5-13B-LoRA:
A soccer player in a green and white uniform kicks the ball while a player in a light blue uniform tries
tostophim.
-w/ connector:
Two soccer players, one in light blue and one in white, playing soccer in front of a crowd.
LLaVA-1.5-13B-Prefix:
A soccer player in a white shirt and green shorts kicks the ball.
-w/ connector:
A soccer player in a white shirt and green shorts kicks the ball
Figure14: AnexamplerandomlychosenfromtheFlickr30Kdataset. TheoutcomesproducedbyLLaVA-1.5-13B
usingdifferentPEFTmethods,eachwiththeconnectorfine-tunedandfrozen.
Prompt:
What company does this truckwork for?
Label:
1.wastemanagement,2.wastemanagement,3.wastemanagement,
4.waste management, 5.garbate, 6.garbate, 7.tow company, 8.tow
company, 9.up, 10.up
LLaVA-1.5-13B-Adapter:
construction
-w/ connector:
construction
LLaVA-1.5-13B-IA3:
12
-w/ connector:
Towcompany
LLaVA-1.5-13B-LoRA:
garbage
-w/connector:
city
LLaVA-1.5-13B-Prefix:
waste management
-w/ connector:
city
Figure15: AnexamplerandomlychosenfromtheOKVQAdataset. TheoutcomesproducedbyLLaVA-1.5-13B
usingdifferentPEFTmethods,eachwiththeconnectorfine-tunedandfrozen.
24Prompt:
What is the genre of this book?
Label:
Parenting&Relationships
LLaVA-1.5-13B-Adapter:
Health, Fitness & Dieting
-w/connector:
Health, Fitness & Dieting
LLaVA-1.5-13B-IA3:
Health,Fitness&Dieting
-w/ connector:
Health, Fitness & Dieting
LLaVA-1.5-13B-LoRA:
Health,Fitness&Dieting
-w/connector:
Health, Fitness & Dieting
LLaVA-1.5-13B-Prefix:
Parenting & Relationships
-w/ connector:
Parenting & Relationships
Figure16: AnexamplerandomlychosenfromtheOCRVQAdataset. TheoutcomesproducedbyLLaVA-1.5-13B
usingdifferentPEFTmethods,eachwiththeconnectorfine-tunedandfrozen.
Prompt:
Renderaclearandconcisesummaryofthephoto.
Label:
Themandoesatrickonthefourwheeler..
LLaVA-1.5-13B-Adapter:
A person is riding a four wheelervehicle.
-w/connector:
A person is riding a four wheelervehicle.
LLaVA-1.5-13B-IA3:
A manonadirtbike.
-w/ connector:
A man is riding a dirt bike
LLaVA-1.5-13B-LoRA:
A persononafourwheeler
-w/ connector:
A person jumping on a four wheeler.
LLaVA-1.5-13B-Prefix:
A manonadirtbikejumpingintheair.
-w/connector:
A manonadirtbikejumpingintheair.
Figure17: AnexamplerandomlychosenfromtheFlickr30Kdataset. TheoutcomesproducedbyLLaVA-1.5-13B
usingdifferentPEFTmethods,eachwiththeconnectorfine-tunedandfrozen.
25Prompt:
Whichpropertydothesefourobjectshaveincommon?
A. Opaque B. Salty C. Slippery
Label:
A
LLaVA-1.5-13B-Adapter:
A
-w/ connector:
A
LLaVA-1.5-13B-IA3:
A
-w/ connector:
A
LLaVA-1.5-13B-LoRA:
A
-w/connector:
A
LLaVA-1.5-13B-Prefix:
A
-w/ connector:
A
Figure18: AnexamplerandomlychosenfromtheSQA(img)dataset. TheoutcomesproducedbyLLaVA-1.5-13B
usingdifferentPEFTmethods,eachwiththeconnectorfine-tunedandfrozen.
Prompt:
Canyoutellmewhatpillthisis?Whentheprovided
information is insufficient, respond with 'Unanswerable'.
Label:
unanswerable
LLaVA-1.5-13B-Adapter:
unanswerable
-w/ connector:
unanswerable
LLaVA-1.5-13B-IA3:
unanswerable
-w/ connector:
unanswerable
LLaVA-1.5-13B-LoRA:
aspirin
-w/ connector:
unanswerable
LLaVA-1.5-13B-Prefix:
unanswerable
-w/ connector:
unanswerable
Figure19: AnexamplerandomlychosenfromtheVizWizdataset. TheoutcomesproducedbyLLaVA-1.5-13B
usingdifferentPEFTmethods,eachwiththeconnectorfine-tunedandfrozen.
26Prompt:
What is the title of this book? Answer the question using a
single word or phrase.
Label:
External Fixators of the Foot and Ankle
LLaVA-1.5-13B-Adapter:
ExternalFixatorsoftheFootandAnkle
-w/ connector:
External Fixators of the Foot and Ankle
LLaVA-1.5-13B-IA3:
External Fixators of the Foot and Ankle
-w/ connector:
ExternalFixatorsoftheFootandAnkle
LLaVA-1.5-13B-LoRA:
ExternalFixatorsoftheFootandAnkle
-w/ connector:
External Fixators of the Foot and Ankle
LLaVA-1.5-13B-Prefix:
External Fixators of the Foot and Ankle
-w/ connector:
External Fixators of the Foot and Ankle
Figure20: AnexamplerandomlychosenfromtheOCRVQAdataset. TheoutcomesproducedbyLLaVA-1.5-13B
usingdifferentPEFTmethods,eachwiththeconnectorfine-tunedandfrozen.
Prompt:
Whathasbeendonetothisletter?
Choices: turn, slide, flip.
Choose an option from choices to answer the question.
Label:
flip
LLaVA-1.5-13B-Adapter:
flip
-w/ connector:
flip
LLaVA-1.5-13B-IA3:
slide
-w/ connector:
flip
LLaVA-1.5-13B-LoRA:
turn
-w/ connector:
flip
LLaVA-1.5-13B-Prefix:
flip
-w/ connector:
flip
Figure21: AnexamplerandomlychosenfromtheIconQA-txtdataset. TheoutcomesproducedbyLLaVA-1.5-13B
usingdifferentPEFTmethods,eachwiththeconnectorfine-tunedandfrozen.
27Prompt:
The shape is made of unit squares. What is the area of the shape?
Fill in the blanks in (_) or answer this question.
Label:
3
LLaVA-1.5-13B-Adapter:
3
-w/connector:
3
LLaVA-1.5-13B-IA3:
12
-w/ connector:
3
LLaVA-1.5-13B-LoRA:
3
-w/ connector:
4
LLaVA-1.5-13B-Prefix:
4
-w/ connector:
4
Figure22: AnexamplerandomlychosenfromtheIconQA-blankdataset. TheoutcomesproducedbyLLaVA-1.5-
13BusingdifferentPEFTmethods,eachwiththeconnectorfine-tunedandfrozen.
Prompt:
Istheplayerright-handed?Answerthequestionusingasingle
word or phrase.
Label:
1.yes, 2. yes, 3. no, 4. yes, 5. yes,
6. yes, 7. yes, 8. yes, 9. yes, 10. yes
LLaVA-1.5-13B-Adapter:
no
-w/ connector:
no
LLaVA-1.5-13B-IA3:
yes
-w/ connector:
yes
LLaVA-1.5-13B-LoRA:
no
-w/ connector:
yes
LLaVA-1.5-13B-Prefix:
yes
-w/ connector:
yes
Figure23: AnexamplerandomlychosenfromtheVQAv2dataset. TheoutcomesproducedbyLLaVA-1.5-13B
usingdifferentPEFTmethods,eachwiththeconnectorfine-tunedandfrozen.
28