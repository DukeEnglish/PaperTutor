[
    {
        "title": "Categorizing Sources of Information for Explanations in Conversational AI Systems for Older Adults Aging in Place",
        "authors": "Niharika MathurTamara ZubatiyElizabeth Mynatt",
        "links": "http://arxiv.org/abs/2406.05111v1",
        "entry_id": "http://arxiv.org/abs/2406.05111v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05111v1",
        "summary": "As the permeability of AI systems in interpersonal domains like the home\nexpands, their technical capabilities of generating explanations are required\nto be aligned with user expectations for transparency and reasoning. This paper\npresents insights from our ongoing work in understanding the effectiveness of\nexplanations in Conversational AI systems for older adults aging in place and\ntheir family caregivers. We argue that in collaborative and multi-user\nenvironments like the home, AI systems will make recommendations based on a\nhost of information sources to generate explanations. These sources may be more\nor less salient based on user mental models of the system and the specific\ntask. We highlight the need for cross technological collaboration between AI\nsystems and other available sources of information in the home to generate\nmultiple explanations for a single user query. Through example scenarios in a\ncaregiving home setting, this paper provides an initial framework for\ncategorizing these sources and informing a potential design space for AI\nexplanations surrounding everyday tasks in the home.",
        "updated": "2024-06-07 17:42:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05111v1"
    },
    {
        "title": "Classification Metrics for Image Explanations: Towards Building Reliable XAI-Evaluations",
        "authors": "Benjamin FreszLena LörcherMarco Huber",
        "links": "http://dx.doi.org/10.1145/3630106.3658537",
        "entry_id": "http://arxiv.org/abs/2406.05068v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05068v1",
        "summary": "Decision processes of computer vision models - especially deep neural\nnetworks - are opaque in nature, meaning that these decisions cannot be\nunderstood by humans. Thus, over the last years, many methods to provide\nhuman-understandable explanations have been proposed. For image classification,\nthe most common group are saliency methods, which provide (super-)pixelwise\nfeature attribution scores for input images. But their evaluation still poses a\nproblem, as their results cannot be simply compared to the unknown ground\ntruth. To overcome this, a slew of different proxy metrics have been defined,\nwhich are - as the explainability methods themselves - often built on intuition\nand thus, are possibly unreliable. In this paper, new evaluation metrics for\nsaliency methods are developed and common saliency methods are benchmarked on\nImageNet. In addition, a scheme for reliability evaluation of such metrics is\nproposed that is based on concepts from psychometric testing. The used code can\nbe found at\nhttps://github.com/lelo204/ClassificationMetricsForImageExplanations .",
        "updated": "2024-06-07 16:37:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05068v1"
    },
    {
        "title": "Designs for Enabling Collaboration in Human-Machine Teaming via Interactive and Explainable Systems",
        "authors": "Rohan PalejaMichael MunjeKimberlee ChangReed JensenMatthew Gombolay",
        "links": "http://arxiv.org/abs/2406.05003v1",
        "entry_id": "http://arxiv.org/abs/2406.05003v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05003v1",
        "summary": "Collaborative robots and machine learning-based virtual agents are\nincreasingly entering the human workspace with the aim of increasing\nproductivity and enhancing safety. Despite this, we show in a ubiquitous\nexperimental domain, Overcooked-AI, that state-of-the-art techniques for\nhuman-machine teaming (HMT), which rely on imitation or reinforcement learning,\nare brittle and result in a machine agent that aims to decouple the machine and\nhuman's actions to act independently rather than in a synergistic fashion. To\nremedy this deficiency, we develop HMT approaches that enable iterative,\nmixed-initiative team development allowing end-users to interactively reprogram\ninterpretable AI teammates. Our 50-subject study provides several findings that\nwe summarize into guidelines. While all approaches underperform a simple\ncollaborative heuristic (a critical, negative result for learning-based\nmethods), we find that white-box approaches supported by interactive\nmodification can lead to significant team development, outperforming white-box\napproaches alone, and black-box approaches are easier to train and result in\nbetter HMT performance highlighting a tradeoff between explainability and\ninteractivity versus ease-of-training. Together, these findings present three\nimportant directions: 1) Improving the ability to generate collaborative agents\nwith white-box models, 2) Better learning methods to facilitate collaboration\nrather than individualized coordination, and 3) Mixed-initiative interfaces\nthat enable users, who may vary in ability, to improve collaboration.",
        "updated": "2024-06-07 15:17:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05003v1"
    },
    {
        "title": "Expansion of situations theory for exploring shared awareness in human-intelligent autonomous systems",
        "authors": "Scott A. HumrMustafa CananMustafa Demir",
        "links": "http://dx.doi.org/10.1504/IJSSE.2025.10058953",
        "entry_id": "http://arxiv.org/abs/2406.04956v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04956v1",
        "summary": "Intelligent autonomous systems are part of a system of systems that interact\nwith other agents to accomplish tasks in complex environments. However,\nintelligent autonomous systems integrated system of systems add additional\nlayers of complexity based on their limited cognitive processes, specifically\nshared situation awareness that allows a team to respond to novel tasks.\nIntelligent autonomous systems' lack of shared situation awareness adversely\ninfluences team effectiveness in complex task environments, such as military\ncommand-and-control. A complementary approach of shared situation awareness,\ncalled situations theory, is beneficial for understanding the relationship\nbetween system of systems shared situation awareness and effectiveness. The\ncurrent study elucidates a conceptual discussion on situations theory to\ninvestigate the development of an system of systems shared situational\nawareness when humans team with intelligent autonomous system agents. To ground\nthe discussion, the reviewed studies expanded situations theory within the\ncontext of a system of systems that result in three major conjectures that can\nbe beneficial to the design and development of future systems of systems.",
        "updated": "2024-06-07 14:21:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04956v1"
    },
    {
        "title": "A Modular Framework for Flexible Planning in Human-Robot Collaboration",
        "authors": "Valerio BelcaminoMariya KilinaLinda LastricoAlessandro CarfìFulvio Mastrogiovanni",
        "links": "http://arxiv.org/abs/2406.04907v1",
        "entry_id": "http://arxiv.org/abs/2406.04907v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04907v1",
        "summary": "This paper presents a comprehensive framework to enhance Human-Robot\nCollaboration (HRC) in real-world scenarios. It introduces a formalism to model\narticulated tasks, requiring cooperation between two agents, through a smaller\nset of primitives. Our implementation leverages Hierarchical Task Networks\n(HTN) planning and a modular multisensory perception pipeline, which includes\nvision, human activity recognition, and tactile sensing. To showcase the\nsystem's scalability, we present an experimental scenario where two humans\nalternate in collaborating with a Baxter robot to assemble four pieces of\nfurniture with variable components. This integration highlights promising\nadvancements in HRC, suggesting a scalable approach for complex, cooperative\ntasks across diverse applications.",
        "updated": "2024-06-07 12:58:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04907v1"
    }
]