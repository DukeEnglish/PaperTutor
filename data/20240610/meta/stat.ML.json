[
    {
        "title": "Linearization Turns Neural Operators into Function-Valued Gaussian Processes",
        "authors": "Emilia MagnaniMarvin PförtnerTobias WeberPhilipp Hennig",
        "links": "http://arxiv.org/abs/2406.05072v1",
        "entry_id": "http://arxiv.org/abs/2406.05072v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05072v1",
        "summary": "Modeling dynamical systems, e.g. in climate and engineering sciences, often\nnecessitates solving partial differential equations. Neural operators are deep\nneural networks designed to learn nontrivial solution operators of such\ndifferential equations from data. As for all statistical models, the\npredictions of these models are imperfect and exhibit errors. Such errors are\nparticularly difficult to spot in the complex nonlinear behaviour of dynamical\nsystems. We introduce a new framework for approximate Bayesian uncertainty\nquantification in neural operators using function-valued Gaussian processes.\nOur approach can be interpreted as a probabilistic analogue of the concept of\ncurrying from functional programming and provides a practical yet theoretically\nsound way to apply the linearized Laplace approximation to neural operators. In\na case study on Fourier neural operators, we show that, even for a discretized\ninput, our method yields a Gaussian closure--a structured Gaussian process\nposterior capturing the uncertainty in the output function of the neural\noperator, which can be evaluated at an arbitrary set of points. The method adds\nminimal prediction overhead, can be applied post-hoc without retraining the\nneural operator, and scales to large models and datasets. We showcase the\nefficacy of our approach through applications to different types of partial\ndifferential equations.",
        "updated": "2024-06-07 16:43:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05072v1"
    },
    {
        "title": "Progressive Entropic Optimal Transport Solvers",
        "authors": "Parnian KassraieAram-Alexandre PooladianMichal KleinJames ThorntonJonathan Niles-WeedMarco Cuturi",
        "links": "http://arxiv.org/abs/2406.05061v1",
        "entry_id": "http://arxiv.org/abs/2406.05061v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05061v1",
        "summary": "Optimal transport (OT) has profoundly impacted machine learning by providing\ntheoretical and computational tools to realign datasets. In this context, given\ntwo large point clouds of sizes $n$ and $m$ in $\\mathbb{R}^d$, entropic OT\n(EOT) solvers have emerged as the most reliable tool to either solve the\nKantorovich problem and output a $n\\times m$ coupling matrix, or to solve the\nMonge problem and learn a vector-valued push-forward map. While the robustness\nof EOT couplings/maps makes them a go-to choice in practical applications, EOT\nsolvers remain difficult to tune because of a small but influential set of\nhyperparameters, notably the omnipresent entropic regularization strength\n$\\varepsilon$. Setting $\\varepsilon$ can be difficult, as it simultaneously\nimpacts various performance metrics, such as compute speed, statistical\nperformance, generalization, and bias. In this work, we propose a new class of\nEOT solvers (ProgOT), that can estimate both plans and transport maps. We take\nadvantage of several opportunities to optimize the computation of EOT solutions\nby dividing mass displacement using a time discretization, borrowing\ninspiration from dynamic OT formulations, and conquering each of these steps\nusing EOT with properly scheduled parameters. We provide experimental evidence\ndemonstrating that ProgOT is a faster and more robust alternative to standard\nsolvers when computing couplings at large scales, even outperforming neural\nnetwork-based approaches. We also prove statistical consistency of our approach\nfor estimating optimal transport maps.",
        "updated": "2024-06-07 16:33:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05061v1"
    },
    {
        "title": "Root Cause Analysis of Outliers with Missing Structural Knowledge",
        "authors": "Nastaran OkatiSergio Hernan Garrido MejiaWilliam Roy OrchardPatrick BlöbaumDominik Janzing",
        "links": "http://arxiv.org/abs/2406.05014v1",
        "entry_id": "http://arxiv.org/abs/2406.05014v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05014v1",
        "summary": "Recent work conceptualized root cause analysis (RCA) of anomalies via\nquantitative contribution analysis using causal counterfactuals in structural\ncausal models (SCMs). The framework comes with three practical challenges: (1)\nit requires the causal directed acyclic graph (DAG), together with an SCM, (2)\nit is statistically ill-posed since it probes regression models in regions of\nlow probability density, (3) it relies on Shapley values which are\ncomputationally expensive to find.\n  In this paper, we propose simplified, efficient methods of root cause\nanalysis when the task is to identify a unique root cause instead of\nquantitative contribution analysis. Our proposed methods run in linear order of\nSCM nodes and they require only the causal DAG without counterfactuals.\nFurthermore, for those use cases where the causal DAG is unknown, we justify\nthe heuristic of identifying root causes as the variables with the highest\nanomaly score.",
        "updated": "2024-06-07 15:24:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05014v1"
    },
    {
        "title": "The Price of Implicit Bias in Adversarially Robust Generalization",
        "authors": "Nikolaos TsilivisNatalie FrankNathan SrebroJulia Kempe",
        "links": "http://arxiv.org/abs/2406.04981v1",
        "entry_id": "http://arxiv.org/abs/2406.04981v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04981v1",
        "summary": "We study the implicit bias of optimization in robust empirical risk\nminimization (robust ERM) and its connection with robust generalization. In\nclassification settings under adversarial perturbations with linear models, we\nstudy what type of regularization should ideally be applied for a given\nperturbation set to improve (robust) generalization. We then show that the\nimplicit bias of optimization in robust ERM can significantly affect the\nrobustness of the model and identify two ways this can happen; either through\nthe optimization algorithm or the architecture. We verify our predictions in\nsimulations with synthetic data and experimentally study the importance of\nimplicit bias in robust ERM with deep neural networks.",
        "updated": "2024-06-07 14:44:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04981v1"
    },
    {
        "title": "Multi-View Stochastic Block Models",
        "authors": "Vincent Cohen-AddadTommaso d'OrsiSilvio LattanziRajai Nasser",
        "links": "http://arxiv.org/abs/2406.04860v1",
        "entry_id": "http://arxiv.org/abs/2406.04860v1",
        "pdf_url": "http://arxiv.org/pdf/2406.04860v1",
        "summary": "Graph clustering is a central topic in unsupervised learning with a multitude\nof practical applications. In recent years, multi-view graph clustering has\ngained a lot of attention for its applicability to real-world instances where\none has access to multiple data sources. In this paper we formalize a new\nfamily of models, called \\textit{multi-view stochastic block models} that\ncaptures this setting.\n  For this model, we first study efficient algorithms that naively work on the\nunion of multiple graphs. Then, we introduce a new efficient algorithm that\nprovably outperforms previous approaches by analyzing the structure of each\ngraph separately. Furthermore, we complement our results with an\ninformation-theoretic lower bound studying the limits of what can be done in\nthis model. Finally, we corroborate our results with experimental evaluations.",
        "updated": "2024-06-07 11:45:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.04860v1"
    }
]