[
    {
        "title": "3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs",
        "authors": "Jianing YangXuweiyi ChenNikhil MadaanMadhavan IyengarShengyi QianDavid F. FouheyJoyce Chai",
        "links": "http://arxiv.org/abs/2406.05132v1",
        "entry_id": "http://arxiv.org/abs/2406.05132v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05132v1",
        "summary": "The integration of language and 3D perception is crucial for developing\nembodied agents and robots that comprehend and interact with the physical\nworld. While large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, their adaptation to 3D environments\n(3D-LLMs) remains in its early stages. A primary challenge is the absence of\nlarge-scale datasets that provide dense grounding between language and 3D\nscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset\ncomprising 40,087 household scenes paired with 6.2 million densely-grounded\nscene-language instructions. Our results show that instruction tuning with\n3D-GRAND significantly enhances grounding capabilities and reduces\nhallucinations in 3D-LLMs. As part of our contributions, we propose a\ncomprehensive benchmark 3D-POPE to systematically evaluate hallucination in\n3D-LLMs, enabling fair comparisons among future models. Our experiments\nhighlight a scaling effect between dataset size and 3D-LLM performance,\nemphasizing the critical role of large-scale 3D-text datasets in advancing\nembodied AI research. Notably, our results demonstrate early signals for\neffective sim-to-real transfer, indicating that models trained on large\nsynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and\n3D-POPE, we aim to equip the embodied AI community with essential resources and\ninsights, setting the stage for more reliable and better-grounded 3D-LLMs.\nProject website: https://3d-grand.github.io",
        "updated": "2024-06-07 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05132v1"
    },
    {
        "title": "DVOS: Self-Supervised Dense-Pattern Video Object Segmentation",
        "authors": "Keyhan NajafianFarhad MalekiIan StavnessLingling Jin",
        "links": "http://arxiv.org/abs/2406.05131v1",
        "entry_id": "http://arxiv.org/abs/2406.05131v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05131v1",
        "summary": "Video object segmentation approaches primarily rely on large-scale\npixel-accurate human-annotated datasets for model development. In Dense Video\nObject Segmentation (DVOS) scenarios, each video frame encompasses hundreds of\nsmall, dense, and partially occluded objects. Accordingly, the labor-intensive\nmanual annotation of even a single frame often takes hours, which hinders the\ndevelopment of DVOS for many applications. Furthermore, in videos with dense\npatterns, following a large number of objects that move in different directions\nposes additional challenges. To address these challenges, we proposed a\nsemi-self-supervised spatiotemporal approach for DVOS utilizing a\ndiffusion-based method through multi-task learning. Emulating real videos'\noptical flow and simulating their motion, we developed a methodology to\nsynthesize computationally annotated videos that can be used for training DVOS\nmodels; The model performance was further improved by utilizing weakly labeled\n(computationally generated but imprecise) data. To demonstrate the utility and\nefficacy of the proposed approach, we developed DVOS models for wheat head\nsegmentation of handheld and drone-captured videos, capturing wheat crops in\nfields of different locations across various growth stages, spanning from\nheading to maturity. Despite using only a few manually annotated video frames,\nthe proposed approach yielded high-performing models, achieving a Dice score of\n0.82 when tested on a drone-captured external test set. While we showed the\nefficacy of the proposed approach for wheat head segmentation, its application\ncan be extended to other crops or DVOS in other domains, such as crowd analysis\nor microscopic image analysis.",
        "updated": "2024-06-07 17:58:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05131v1"
    },
    {
        "title": "PatchSVD: A Non-uniform SVD-based Image Compression Algorithm",
        "authors": "Zahra GolpayeganiNizar Bouguila",
        "links": "http://dx.doi.org/10.5220/0012488500003654",
        "entry_id": "http://arxiv.org/abs/2406.05129v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05129v1",
        "summary": "Storing data is particularly a challenge when dealing with image data which\noften involves large file sizes due to the high resolution and complexity of\nimages. Efficient image compression algorithms are crucial to better manage\ndata storage costs. In this paper, we propose a novel region-based lossy image\ncompression technique, called PatchSVD, based on the Singular Value\nDecomposition (SVD) algorithm. We show through experiments that PatchSVD\noutperforms SVD-based image compression with respect to three popular image\ncompression metrics. Moreover, we compare PatchSVD compression artifacts with\nthose of Joint Photographic Experts Group (JPEG) and SVD-based image\ncompression and illustrate some cases where PatchSVD compression artifacts are\npreferable compared to JPEG and SVD artifacts.",
        "updated": "2024-06-07 17:57:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05129v1"
    },
    {
        "title": "Towards Semantic Equivalence of Tokenization in Multimodal LLM",
        "authors": "Shengqiong WuHao FeiXiangtai LiJiayi JiHanwang ZhangTat-Seng ChuaShuicheng Yan",
        "links": "http://arxiv.org/abs/2406.05127v1",
        "entry_id": "http://arxiv.org/abs/2406.05127v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05127v1",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated exceptional\ncapabilities in processing vision-language tasks. One of the crux of MLLMs lies\nin vision tokenization, which involves efficiently transforming input visual\nsignals into feature representations that are most beneficial for LLMs.\nHowever, existing vision tokenizers, essential for semantic alignment between\nvision and language, remain problematic. Existing methods aggressively fragment\nvisual input, corrupting the visual semantic integrity. To address this, this\npaper proposes a novel dynamic Semantic-Equivalent Vision Tokenizer (SeTok),\nwhich groups visual features into semantic units via a dynamic clustering\nalgorithm, flexibly determining the number of tokens based on image complexity.\nThe resulting vision tokens effectively preserve semantic integrity and capture\nboth low-frequency and high-frequency visual features. The proposed MLLM\n(Setokim) equipped with SeTok significantly demonstrates superior performance\nacross various tasks, as evidenced by our experimental results. The project\npage is at https://chocowu.github.io/SeTok-web/.",
        "updated": "2024-06-07 17:55:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05127v1"
    },
    {
        "title": "Energy Propagation in Scattering Convolution Networks Can Be Arbitrarily Slow",
        "authors": "Hartmut FührMax Getter",
        "links": "http://arxiv.org/abs/2406.05121v1",
        "entry_id": "http://arxiv.org/abs/2406.05121v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05121v1",
        "summary": "We analyze energy decay for deep convolutional neural networks employed as\nfeature extractors, such as Mallat's wavelet scattering transform. For\ntime-frequency scattering transforms based on Gabor filters, it has been\nestablished that energy decay is exponential, for arbitrary square-integrable\ninput signals. Our main results allow to prove that this is wrong for wavelet\nscattering in arbitrary dimensions. In this setting, the energy decay of the\nscattering transform acting on a generic square-integrable signal turns out to\nbe arbitrarily slow. The fact that this behavior holds for dense subsets of\n$L^2(\\mathbb{R}^d)$ emphasizes that fast energy decay is generally not a stable\nproperty of signals.\n  We complement these findings with positive results allowing to conclude fast\n(up to exponential) energy decay for generalized Sobolev spaces that are\ntailored to the frequency localization of the underlying filter bank.\n  Both negative and positive results highlight that energy decay in scattering\nnetworks critically depends on the interplay of the respective frequency\nlocalizations of the signal on the one hand, and of the employed filters on the\nother.",
        "updated": "2024-06-07 17:52:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05121v1"
    }
]