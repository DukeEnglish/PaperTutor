[
    {
        "title": "3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs",
        "authors": "Jianing YangXuweiyi ChenNikhil MadaanMadhavan IyengarShengyi QianDavid F. FouheyJoyce Chai",
        "links": "http://arxiv.org/abs/2406.05132v1",
        "entry_id": "http://arxiv.org/abs/2406.05132v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05132v1",
        "summary": "The integration of language and 3D perception is crucial for developing\nembodied agents and robots that comprehend and interact with the physical\nworld. While large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, their adaptation to 3D environments\n(3D-LLMs) remains in its early stages. A primary challenge is the absence of\nlarge-scale datasets that provide dense grounding between language and 3D\nscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset\ncomprising 40,087 household scenes paired with 6.2 million densely-grounded\nscene-language instructions. Our results show that instruction tuning with\n3D-GRAND significantly enhances grounding capabilities and reduces\nhallucinations in 3D-LLMs. As part of our contributions, we propose a\ncomprehensive benchmark 3D-POPE to systematically evaluate hallucination in\n3D-LLMs, enabling fair comparisons among future models. Our experiments\nhighlight a scaling effect between dataset size and 3D-LLM performance,\nemphasizing the critical role of large-scale 3D-text datasets in advancing\nembodied AI research. Notably, our results demonstrate early signals for\neffective sim-to-real transfer, indicating that models trained on large\nsynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and\n3D-POPE, we aim to equip the embodied AI community with essential resources and\ninsights, setting the stage for more reliable and better-grounded 3D-LLMs.\nProject website: https://3d-grand.github.io",
        "updated": "2024-06-07 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05132v1"
    },
    {
        "title": "Compositional Curvature Bounds for Deep Neural Networks",
        "authors": "Taha EntesariSina SharifiMahyar Fazlyab",
        "links": "http://arxiv.org/abs/2406.05119v1",
        "entry_id": "http://arxiv.org/abs/2406.05119v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05119v1",
        "summary": "A key challenge that threatens the widespread use of neural networks in\nsafety-critical applications is their vulnerability to adversarial attacks. In\nthis paper, we study the second-order behavior of continuously differentiable\ndeep neural networks, focusing on robustness against adversarial perturbations.\nFirst, we provide a theoretical analysis of robustness and attack certificates\nfor deep classifiers by leveraging local gradients and upper bounds on the\nsecond derivative (curvature constant). Next, we introduce a novel algorithm to\nanalytically compute provable upper bounds on the second derivative of neural\nnetworks. This algorithm leverages the compositional structure of the model to\npropagate the curvature bound layer-by-layer, giving rise to a scalable and\nmodular approach. The proposed bound can serve as a differentiable regularizer\nto control the curvature of neural networks during training, thereby enhancing\nrobustness. Finally, we demonstrate the efficacy of our method on\nclassification tasks using the MNIST and CIFAR-10 datasets.",
        "updated": "2024-06-07 17:50:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05119v1"
    },
    {
        "title": "The Expanding Scope of the Stability Gap: Unveiling its Presence in Joint Incremental Learning of Homogeneous Tasks",
        "authors": "Sandesh KamathAlbin Soutif-CormeraisJoost van de WeijerBogdan Raducanu",
        "links": "http://arxiv.org/abs/2406.05114v1",
        "entry_id": "http://arxiv.org/abs/2406.05114v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05114v1",
        "summary": "Recent research identified a temporary performance drop on previously learned\ntasks when transitioning to a new one. This drop is called the stability gap\nand has great consequences for continual learning: it complicates the direct\nemployment of continually learning since the worse-case performance at\ntask-boundaries is dramatic, it limits its potential as an energy-efficient\ntraining paradigm, and finally, the stability drop could result in a reduced\nfinal performance of the algorithm. In this paper, we show that the stability\ngap also occurs when applying joint incremental training of homogeneous tasks.\nIn this scenario, the learner continues training on the same data distribution\nand has access to all data from previous tasks. In addition, we show that in\nthis scenario, there exists a low-loss linear path to the next minima, but that\nSGD optimization does not choose this path. We perform further analysis\nincluding a finer batch-wise analysis which could provide insights towards\npotential solution directions.",
        "updated": "2024-06-07 17:44:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05114v1"
    },
    {
        "title": "LLavaGuard: VLM-based Safeguards for Vision Dataset Curation and Safety Assessment",
        "authors": "Lukas HelffFelix FriedrichManuel BrackKristian KerstingPatrick Schramowski",
        "links": "http://arxiv.org/abs/2406.05113v1",
        "entry_id": "http://arxiv.org/abs/2406.05113v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05113v1",
        "summary": "We introduce LlavaGuard, a family of VLM-based safeguard models, offering a\nversatile framework for evaluating the safety compliance of visual content.\nSpecifically, we designed LlavaGuard for dataset annotation and generative\nmodel safeguarding. To this end, we collected and annotated a high-quality\nvisual dataset incorporating a broad safety taxonomy, which we use to tune VLMs\non context-aware safety risks. As a key innovation, LlavaGuard's new responses\ncontain comprehensive information, including a safety rating, the violated\nsafety categories, and an in-depth rationale. Further, our introduced\ncustomizable taxonomy categories enable the context-specific alignment of\nLlavaGuard to various scenarios. Our experiments highlight the capabilities of\nLlavaGuard in complex and real-world applications. We provide checkpoints\nranging from 7B to 34B parameters demonstrating state-of-the-art performance,\nwith even the smallest models outperforming baselines like GPT-4. We make our\ndataset and model weights publicly available and invite further research to\naddress the diverse needs of communities and contexts.",
        "updated": "2024-06-07 17:44:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05113v1"
    },
    {
        "title": "Large Generative Graph Models",
        "authors": "Yu WangRyan A. RossiNamyong ParkHuiyuan ChenNesreen K. AhmedPuja TrivediFranck DernoncourtDanai KoutraTyler Derr",
        "links": "http://arxiv.org/abs/2406.05109v1",
        "entry_id": "http://arxiv.org/abs/2406.05109v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05109v1",
        "summary": "Large Generative Models (LGMs) such as GPT, Stable Diffusion, Sora, and Suno\nare trained on a huge amount of language corpus, images, videos, and audio that\nare extremely diverse from numerous domains. This training paradigm over\ndiverse well-curated data lies at the heart of generating creative and sensible\ncontent. However, all previous graph generative models (e.g., GraphRNN, MDVAE,\nMoFlow, GDSS, and DiGress) have been trained only on one dataset each time,\nwhich cannot replicate the revolutionary success achieved by LGMs in other\nfields. To remedy this crucial gap, we propose a new class of graph generative\nmodel called Large Graph Generative Model (LGGM) that is trained on a large\ncorpus of graphs (over 5000 graphs) from 13 different domains. We empirically\ndemonstrate that the pre-trained LGGM has superior zero-shot generative\ncapability to existing graph generative models. Furthermore, our pre-trained\nLGGM can be easily fine-tuned with graphs from target domains and demonstrate\neven better performance than those directly trained from scratch, behaving as a\nsolid starting point for real-world customization. Inspired by Stable\nDiffusion, we further equip LGGM with the capability to generate graphs given\ntext prompts (Text-to-Graph), such as the description of the network name and\ndomain (i.e., \"The power-1138-bus graph represents a network of buses in a\npower distribution system.\"), and network statistics (i.e., \"The graph has a\nlow average degree, suitable for modeling social media interactions.\"). This\nText-to-Graph capability integrates the extensive world knowledge in the\nunderlying language model, offering users fine-grained control of the generated\ngraphs. We release the code, the model checkpoint, and the datasets at\nhttps://lggm-lg.github.io/.",
        "updated": "2024-06-07 17:41:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05109v1"
    }
]