[
    {
        "title": "3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs",
        "authors": "Jianing YangXuweiyi ChenNikhil MadaanMadhavan IyengarShengyi QianDavid F. FouheyJoyce Chai",
        "links": "http://arxiv.org/abs/2406.05132v1",
        "entry_id": "http://arxiv.org/abs/2406.05132v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05132v1",
        "summary": "The integration of language and 3D perception is crucial for developing\nembodied agents and robots that comprehend and interact with the physical\nworld. While large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, their adaptation to 3D environments\n(3D-LLMs) remains in its early stages. A primary challenge is the absence of\nlarge-scale datasets that provide dense grounding between language and 3D\nscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset\ncomprising 40,087 household scenes paired with 6.2 million densely-grounded\nscene-language instructions. Our results show that instruction tuning with\n3D-GRAND significantly enhances grounding capabilities and reduces\nhallucinations in 3D-LLMs. As part of our contributions, we propose a\ncomprehensive benchmark 3D-POPE to systematically evaluate hallucination in\n3D-LLMs, enabling fair comparisons among future models. Our experiments\nhighlight a scaling effect between dataset size and 3D-LLM performance,\nemphasizing the critical role of large-scale 3D-text datasets in advancing\nembodied AI research. Notably, our results demonstrate early signals for\neffective sim-to-real transfer, indicating that models trained on large\nsynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and\n3D-POPE, we aim to equip the embodied AI community with essential resources and\ninsights, setting the stage for more reliable and better-grounded 3D-LLMs.\nProject website: https://3d-grand.github.io",
        "updated": "2024-06-07 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05132v1"
    },
    {
        "title": "An Empirical Study on Parameter-Efficient Fine-Tuning for MultiModal Large Language Models",
        "authors": "Xiongtao ZhouJie HeYuhua KeGuangyao ZhuVíctor Gutiérrez-BasultoJeff Z. Pan",
        "links": "http://arxiv.org/abs/2406.05130v1",
        "entry_id": "http://arxiv.org/abs/2406.05130v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05130v1",
        "summary": "Multimodal large language models (MLLMs) fine-tuned with multimodal\ninstruction datasets have demonstrated remarkable capabilities in multimodal\ntasks. However, fine-tuning all parameters of MLLMs has become challenging as\nthey usually contain billions of parameters. To address this issue, we study\nparameter-efficient fine-tuning (PEFT) methods for MLLMs. We aim to identify\neffective methods for enhancing the performance of MLLMs in scenarios where\nonly a limited number of parameters are trained. This paper conducts empirical\nstudies using four popular PEFT methods to fine-tune the LLM component of\nopen-source MLLMs. We present a comprehensive analysis that encompasses various\naspects, including the impact of PEFT methods on various models, parameters and\nlocation of the PEFT module, size of fine-tuning data, model stability based on\nPEFT methods, MLLM's generalization, and hallucination. We evaluated four PEFT\nmethods on seven datasets from two different categories: unseen and seen\ndatasets. Across all experiments, we show that the adapter is the\nbest-performing PEFT method. At the same time, fine-tuning the connector layers\nleads to improved performance in most MLLMs. Code and data are available at\nhttps://github.com/alenai97/PEFT-MLLM.git.",
        "updated": "2024-06-07 17:58:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05130v1"
    },
    {
        "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
        "authors": "Maciej BestaAles KubicekRoman NiggliRobert GerstenbergerLucas WeitzendorfMingyuan ChiPatrick IffJoanna GajdaPiotr NyczykJürgen MüllerHubert NiewiadomskiMarcin ChrapekMichał PodstawskiTorsten Hoefler",
        "links": "http://arxiv.org/abs/2406.05085v1",
        "entry_id": "http://arxiv.org/abs/2406.05085v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05085v1",
        "summary": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving motivation is that different attention\nheads can learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, synthetic datasets, and real-world use\ncases to demonstrate MRAG's effectiveness, showing improvements of up to 20% in\nrelevance over standard RAG baselines. MRAG can be seamlessly integrated with\nexisting RAG frameworks and benchmarking tools like RAGAS as well as different\nclasses of data stores.",
        "updated": "2024-06-07 16:59:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05085v1"
    },
    {
        "title": "On Ambiguity and the Expressive Function of Law: The Role of Pragmatics in Smart Legal Ecosystems",
        "authors": "Pompeu Casanovas",
        "links": "http://arxiv.org/abs/2406.05084v1",
        "entry_id": "http://arxiv.org/abs/2406.05084v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05084v1",
        "summary": "This is a long paper, an essay, on ambiguity, pragmatics, legal ecosystems,\nand the expressive function of law. It is divided into two parts and fifteen\nsections. The first part (Pragmatics) addresses ambiguity from the perspective\nof linguistic and cognitive pragmatics in the legal field. The second part\n(Computing) deals with this issue from the point of view of human-centered\ndesign and artificial intelligence, specifically focusing on the notion and\nmodelling of rules and what it means to comply with the rules. This is\nnecessary for the scaffolding of smart legal ecosystems (SLE). I will develop\nthis subject with the example of the architecture, information flows, and smart\necosystem of OPTIMAI, an EU project of Industry 4.0 for zero-defect\nmanufacturing (Optimizing Manufacturing Processes through Artificial\nIntelligence and Virtualization).",
        "updated": "2024-06-07 16:58:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05084v1"
    },
    {
        "title": "I2EDL: Interactive Instruction Error Detection and Localization",
        "authors": "Francesco TaioliStefano RosaAlberto CastelliniLorenzo NataleAlessio Del BueAlessandro FarinelliMarco CristaniYiming Wang",
        "links": "http://arxiv.org/abs/2406.05080v1",
        "entry_id": "http://arxiv.org/abs/2406.05080v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05080v1",
        "summary": "In the Vision-and-Language Navigation in Continuous Environments (VLN-CE)\ntask, the human user guides an autonomous agent to reach a target goal via a\nseries of low-level actions following a textual instruction in natural\nlanguage. However, most existing methods do not address the likely case where\nusers may make mistakes when providing such instruction (e.g. \"turn left\"\ninstead of \"turn right\"). In this work, we address a novel task of Interactive\nVLN in Continuous Environments (IVLN-CE), which allows the agent to interact\nwith the user during the VLN-CE navigation to verify any doubts regarding the\ninstruction errors. We propose an Interactive Instruction Error Detector and\nLocalizer (I2EDL) that triggers the user-agent interaction upon the detection\nof instruction errors during the navigation. We leverage a pre-trained module\nto detect instruction errors and pinpoint them in the instruction by\ncross-referencing the textual input and past observations. In such way, the\nagent is able to query the user for a timely correction, without demanding the\nuser's cognitive load, as we locate the probable errors to a precise part of\nthe instruction. We evaluate the proposed I2EDL on a dataset of instructions\ncontaining errors, and further devise a novel metric, the Success weighted by\nInteraction Number (SIN), to reflect both the navigation performance and the\ninteraction effectiveness. We show how the proposed method can ask focused\nrequests for corrections to the user, which in turn increases the navigation\nsuccess, while minimizing the interactions.",
        "updated": "2024-06-07 16:52:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05080v1"
    }
]