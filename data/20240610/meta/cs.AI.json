[
    {
        "title": "3D-GRAND: Towards Better Grounding and Less Hallucination for 3D-LLMs",
        "authors": "Jianing YangXuweiyi ChenNikhil MadaanMadhavan IyengarShengyi QianDavid F. FouheyJoyce Chai",
        "links": "http://arxiv.org/abs/2406.05132v1",
        "entry_id": "http://arxiv.org/abs/2406.05132v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05132v1",
        "summary": "The integration of language and 3D perception is crucial for developing\nembodied agents and robots that comprehend and interact with the physical\nworld. While large language models (LLMs) have demonstrated impressive language\nunderstanding and generation capabilities, their adaptation to 3D environments\n(3D-LLMs) remains in its early stages. A primary challenge is the absence of\nlarge-scale datasets that provide dense grounding between language and 3D\nscenes. In this paper, we introduce 3D-GRAND, a pioneering large-scale dataset\ncomprising 40,087 household scenes paired with 6.2 million densely-grounded\nscene-language instructions. Our results show that instruction tuning with\n3D-GRAND significantly enhances grounding capabilities and reduces\nhallucinations in 3D-LLMs. As part of our contributions, we propose a\ncomprehensive benchmark 3D-POPE to systematically evaluate hallucination in\n3D-LLMs, enabling fair comparisons among future models. Our experiments\nhighlight a scaling effect between dataset size and 3D-LLM performance,\nemphasizing the critical role of large-scale 3D-text datasets in advancing\nembodied AI research. Notably, our results demonstrate early signals for\neffective sim-to-real transfer, indicating that models trained on large\nsynthetic data can perform well on real-world 3D scans. Through 3D-GRAND and\n3D-POPE, we aim to equip the embodied AI community with essential resources and\ninsights, setting the stage for more reliable and better-grounded 3D-LLMs.\nProject website: https://3d-grand.github.io",
        "updated": "2024-06-07 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05132v1"
    },
    {
        "title": "LLavaGuard: VLM-based Safeguards for Vision Dataset Curation and Safety Assessment",
        "authors": "Lukas HelffFelix FriedrichManuel BrackKristian KerstingPatrick Schramowski",
        "links": "http://arxiv.org/abs/2406.05113v1",
        "entry_id": "http://arxiv.org/abs/2406.05113v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05113v1",
        "summary": "We introduce LlavaGuard, a family of VLM-based safeguard models, offering a\nversatile framework for evaluating the safety compliance of visual content.\nSpecifically, we designed LlavaGuard for dataset annotation and generative\nmodel safeguarding. To this end, we collected and annotated a high-quality\nvisual dataset incorporating a broad safety taxonomy, which we use to tune VLMs\non context-aware safety risks. As a key innovation, LlavaGuard's new responses\ncontain comprehensive information, including a safety rating, the violated\nsafety categories, and an in-depth rationale. Further, our introduced\ncustomizable taxonomy categories enable the context-specific alignment of\nLlavaGuard to various scenarios. Our experiments highlight the capabilities of\nLlavaGuard in complex and real-world applications. We provide checkpoints\nranging from 7B to 34B parameters demonstrating state-of-the-art performance,\nwith even the smallest models outperforming baselines like GPT-4. We make our\ndataset and model weights publicly available and invite further research to\naddress the diverse needs of communities and contexts.",
        "updated": "2024-06-07 17:44:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05113v1"
    },
    {
        "title": "Provably Better Explanations with Optimized Aggregation of Feature Attributions",
        "authors": "Thomas DeckerAnanta R. BhattaraiJindong GuVolker TrespFlorian Buettner",
        "links": "http://arxiv.org/abs/2406.05090v1",
        "entry_id": "http://arxiv.org/abs/2406.05090v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05090v1",
        "summary": "Using feature attributions for post-hoc explanations is a common practice to\nunderstand and verify the predictions of opaque machine learning models.\nDespite the numerous techniques available, individual methods often produce\ninconsistent and unstable results, putting their overall reliability into\nquestion. In this work, we aim to systematically improve the quality of feature\nattributions by combining multiple explanations across distinct methods or\ntheir variations. For this purpose, we propose a novel approach to derive\noptimal convex combinations of feature attributions that yield provable\nimprovements of desired quality criteria such as robustness or faithfulness to\nthe model behavior. Through extensive experiments involving various model\narchitectures and popular feature attribution techniques, we demonstrate that\nour combination strategy consistently outperforms individual methods and\nexisting baselines.",
        "updated": "2024-06-07 17:03:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05090v1"
    },
    {
        "title": "Robust Reward Design for Markov Decision Processes",
        "authors": "Shuo WuHaoxiang MaJie FuShuo Han",
        "links": "http://arxiv.org/abs/2406.05086v1",
        "entry_id": "http://arxiv.org/abs/2406.05086v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05086v1",
        "summary": "The problem of reward design examines the interaction between a leader and a\nfollower, where the leader aims to shape the follower's behavior to maximize\nthe leader's payoff by modifying the follower's reward function. Current\napproaches to reward design rely on an accurate model of how the follower\nresponds to reward modifications, which can be sensitive to modeling\ninaccuracies. To address this issue of sensitivity, we present a solution that\noffers robustness against uncertainties in modeling the follower, including 1)\nhow the follower breaks ties in the presence of nonunique best responses, 2)\ninexact knowledge of how the follower perceives reward modifications, and 3)\nbounded rationality of the follower. Our robust solution is guaranteed to exist\nunder mild conditions and can be obtained numerically by solving a\nmixed-integer linear program. Numerical experiments on multiple test cases\ndemonstrate that our solution improves robustness compared to the standard\napproach without incurring significant additional computing costs.",
        "updated": "2024-06-07 17:01:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05086v1"
    },
    {
        "title": "Multi-Head RAG: Solving Multi-Aspect Problems with LLMs",
        "authors": "Maciej BestaAles KubicekRoman NiggliRobert GerstenbergerLucas WeitzendorfMingyuan ChiPatrick IffJoanna GajdaPiotr NyczykJürgen MüllerHubert NiewiadomskiMarcin ChrapekMichał PodstawskiTorsten Hoefler",
        "links": "http://arxiv.org/abs/2406.05085v1",
        "entry_id": "http://arxiv.org/abs/2406.05085v1",
        "pdf_url": "http://arxiv.org/pdf/2406.05085v1",
        "summary": "Retrieval Augmented Generation (RAG) enhances the abilities of Large Language\nModels (LLMs) by enabling the retrieval of documents into the LLM context to\nprovide more accurate and relevant responses. Existing RAG solutions do not\nfocus on queries that may require fetching multiple documents with\nsubstantially different contents. Such queries occur frequently, but are\nchallenging because the embeddings of these documents may be distant in the\nembedding space, making it hard to retrieve them all. This paper introduces\nMulti-Head RAG (MRAG), a novel scheme designed to address this gap with a\nsimple yet powerful idea: leveraging activations of Transformer's multi-head\nattention layer, instead of the decoder layer, as keys for fetching\nmulti-aspect documents. The driving motivation is that different attention\nheads can learn to capture different data aspects. Harnessing the corresponding\nactivations results in embeddings that represent various facets of data items\nand queries, improving the retrieval accuracy for complex queries. We provide\nan evaluation methodology and metrics, synthetic datasets, and real-world use\ncases to demonstrate MRAG's effectiveness, showing improvements of up to 20% in\nrelevance over standard RAG baselines. MRAG can be seamlessly integrated with\nexisting RAG frameworks and benchmarking tools like RAGAS as well as different\nclasses of data stores.",
        "updated": "2024-06-07 16:59:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2406.05085v1"
    }
]