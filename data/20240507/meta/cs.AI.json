[
    {
        "title": "Language-Image Models with 3D Understanding",
        "authors": "Jang Hyun ChoBoris IvanovicYulong CaoEdward SchmerlingYue WangXinshuo WengBoyi LiYurong YouPhilipp KrähenbühlYan WangMarco Pavone",
        "links": "http://arxiv.org/abs/2405.03685v1",
        "entry_id": "http://arxiv.org/abs/2405.03685v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03685v1",
        "summary": "Multi-modal large language models (MLLMs) have shown incredible capabilities\nin a variety of 2D vision and language tasks. We extend MLLMs' perceptual\ncapabilities to ground and reason about images in 3-dimensional space. To that\nend, we first develop a large-scale pre-training dataset for 2D and 3D called\nLV3D by combining multiple existing 2D and 3D recognition datasets under a\ncommon task formulation: as multi-turn question-answering. Next, we introduce a\nnew MLLM named Cube-LLM and pre-train it on LV3D. We show that pure data\nscaling makes a strong 3D perception capability without 3D specific\narchitectural design or training objective. Cube-LLM exhibits intriguing\nproperties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting\nto improve 3D understanding from 2D context information. (2) Cube-LLM can\nfollow complex and diverse instructions and adapt to versatile input and output\nformats. (3) Cube-LLM can be visually prompted such as 2D box or a set of\ncandidate 3D boxes from specialists. Our experiments on outdoor benchmarks\ndemonstrate that Cube-LLM significantly outperforms existing baselines by 21.3\npoints of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7\npoints on the DriveLM dataset for complex reasoning about driving scenarios,\nrespectively. Cube-LLM also shows competitive results in general MLLM\nbenchmarks such as refCOCO for 2D grounding with (87.0) average score, as well\nas visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for\ncomplex reasoning. Our project is available at\nhttps://janghyuncho.github.io/Cube-LLM.",
        "updated": "2024-05-06 17:57:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03685v1"
    },
    {
        "title": "MemoryMamba: Memory-Augmented State Space Model for Defect Recognition",
        "authors": "Qianning WangHe HuYucheng Zhou",
        "links": "http://arxiv.org/abs/2405.03673v1",
        "entry_id": "http://arxiv.org/abs/2405.03673v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03673v1",
        "summary": "As automation advances in manufacturing, the demand for precise and\nsophisticated defect detection technologies grows. Existing vision models for\ndefect recognition methods are insufficient for handling the complexities and\nvariations of defects in contemporary manufacturing settings. These models\nespecially struggle in scenarios involving limited or imbalanced defect data.\nIn this work, we introduce MemoryMamba, a novel memory-augmented state space\nmodel (SSM), designed to overcome the limitations of existing defect\nrecognition models. MemoryMamba integrates the state space model with the\nmemory augmentation mechanism, enabling the system to maintain and retrieve\nessential defect-specific information in training. Its architecture is designed\nto capture dependencies and intricate defect characteristics, which are crucial\nfor effective defect detection. In the experiments, MemoryMamba was evaluated\nacross four industrial datasets with diverse defect types and complexities. The\nmodel consistently outperformed other methods, demonstrating its capability to\nadapt to various defect recognition scenarios.",
        "updated": "2024-05-06 17:49:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03673v1"
    },
    {
        "title": "Prompting Task Trees using Gemini: Methodologies and Insights",
        "authors": "Pallavi Tandra",
        "links": "http://arxiv.org/abs/2405.03671v1",
        "entry_id": "http://arxiv.org/abs/2405.03671v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03671v1",
        "summary": "Robots are the future of every technology where every advanced technology\neventually will be used to make robots which are more efficient. The major\nchallenge today is to train the robots exactly and empathetically using\nknowledge representation. This paper gives you insights of how we can use\nunstructured knowledge representation and convert them to meaningful structured\nrepresentation with the help of prompt engineering which can be eventually used\nin the robots to make help them understand how human brain can make wonders\nwith the minimal data or objects can providing to them.",
        "updated": "2024-05-06 17:48:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03671v1"
    },
    {
        "title": "ScrewMimic: Bimanual Imitation from Human Videos with Screw Space Projection",
        "authors": "Arpit BahetyPriyanka MandikalBen AbbatematteoRoberto Martín-Martín",
        "links": "http://arxiv.org/abs/2405.03666v1",
        "entry_id": "http://arxiv.org/abs/2405.03666v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03666v1",
        "summary": "Bimanual manipulation is a longstanding challenge in robotics due to the\nlarge number of degrees of freedom and the strict spatial and temporal\nsynchronization required to generate meaningful behavior. Humans learn bimanual\nmanipulation skills by watching other humans and by refining their abilities\nthrough play. In this work, we aim to enable robots to learn bimanual\nmanipulation behaviors from human video demonstrations and fine-tune them\nthrough interaction. Inspired by seminal work in psychology and biomechanics,\nwe propose modeling the interaction between two hands as a serial kinematic\nlinkage -- as a screw motion, in particular, that we use to define a new action\nspace for bimanual manipulation: screw actions. We introduce ScrewMimic, a\nframework that leverages this novel action representation to facilitate\nlearning from human demonstration and self-supervised policy fine-tuning. Our\nexperiments demonstrate that ScrewMimic is able to learn several complex\nbimanual behaviors from a single human video demonstration, and that it\noutperforms baselines that interpret demonstrations and fine-tune directly in\nthe original space of motion of both arms. For more information and video\nresults, https://robin-lab.cs.utexas.edu/ScrewMimic/",
        "updated": "2024-05-06 17:43:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03666v1"
    },
    {
        "title": "Can LLMs Deeply Detect Complex Malicious Queries? A Framework for Jailbreaking via Obfuscating Intent",
        "authors": "Shang ShangXinqiang ZhaoZhongjiang YaoYepeng YaoLiya SuZijing FanXiaodan ZhangZhengwei Jiang",
        "links": "http://arxiv.org/abs/2405.03654v1",
        "entry_id": "http://arxiv.org/abs/2405.03654v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03654v1",
        "summary": "To demonstrate and address the underlying maliciousness, we propose a\ntheoretical hypothesis and analytical approach, and introduce a new black-box\njailbreak attack methodology named IntentObfuscator, exploiting this identified\nflaw by obfuscating the true intentions behind user prompts.This approach\ncompels LLMs to inadvertently generate restricted content, bypassing their\nbuilt-in content security measures. We detail two implementations under this\nframework: \"Obscure Intention\" and \"Create Ambiguity\", which manipulate query\ncomplexity and ambiguity to evade malicious intent detection effectively. We\nempirically validate the effectiveness of the IntentObfuscator method across\nseveral models, including ChatGPT-3.5, ChatGPT-4, Qwen and Baichuan, achieving\nan average jailbreak success rate of 69.21\\%. Notably, our tests on\nChatGPT-3.5, which claims 100 million weekly active users, achieved a\nremarkable success rate of 83.65\\%. We also extend our validation to diverse\ntypes of sensitive content like graphic violence, racism, sexism, political\nsensitivity, cybersecurity threats, and criminal skills, further proving the\nsubstantial impact of our findings on enhancing 'Red Team' strategies against\nLLM content security frameworks.",
        "updated": "2024-05-06 17:26:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03654v1"
    }
]