[
    {
        "title": "Pose Priors from Language Models",
        "authors": "Sanjay SubramanianEvonne NgLea MüllerDan KleinShiry GinosarTrevor Darrell",
        "links": "http://arxiv.org/abs/2405.03689v1",
        "entry_id": "http://arxiv.org/abs/2405.03689v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03689v1",
        "summary": "We present a zero-shot pose optimization method that enforces accurate\nphysical contact constraints when estimating the 3D pose of humans. Our central\ninsight is that since language is often used to describe physical interaction,\nlarge pretrained text-based models can act as priors on pose estimation.\n  We can thus leverage this insight to improve pose estimation by converting\nnatural language descriptors, generated by a large multimodal model (LMM), into\ntractable losses to constrain the 3D pose optimization. Despite its simplicity,\nour method produces surprisingly compelling pose reconstructions of people in\nclose contact, correctly capturing the semantics of the social and physical\ninteractions. We demonstrate that our method rivals more complex\nstate-of-the-art approaches that require expensive human annotation of contact\npoints and training specialized models. Moreover, unlike previous approaches,\nour method provides a unified framework for resolving self-contact and\nperson-to-person contact.",
        "updated": "2024-05-06 17:59:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03689v1"
    },
    {
        "title": "Large Language Models Reveal Information Operation Goals, Tactics, and Narrative Frames",
        "authors": "Keith BurghardtKai ChenKristina Lerman",
        "links": "http://arxiv.org/abs/2405.03688v1",
        "entry_id": "http://arxiv.org/abs/2405.03688v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03688v1",
        "summary": "Adversarial information operations can destabilize societies by undermining\nfair elections, manipulating public opinions on policies, and promoting scams.\nDespite their widespread occurrence and potential impacts, our understanding of\ninfluence campaigns is limited by manual analysis of messages and subjective\ninterpretation of their observable behavior. In this paper, we explore whether\nthese limitations can be mitigated with large language models (LLMs), using\nGPT-3.5 as a case-study for coordinated campaign annotation. We first use\nGPT-3.5 to scrutinize 126 identified information operations spanning over a\ndecade. We utilize a number of metrics to quantify the close (if imperfect)\nagreement between LLM and ground truth descriptions. We next extract\ncoordinated campaigns from two large multilingual datasets from X (formerly\nTwitter) that respectively discuss the 2022 French election and 2023 Balikaran\nPhilippine-U.S. military exercise in 2023. For each coordinated campaign, we\nuse GPT-3.5 to analyze posts related to a specific concern and extract goals,\ntactics, and narrative frames, both before and after critical events (such as\nthe date of an election). While the GPT-3.5 sometimes disagrees with subjective\ninterpretation, its ability to summarize and interpret demonstrates LLMs'\npotential to extract higher-order indicators from text to provide a more\ncomplete picture of the information campaigns compared to previous methods.",
        "updated": "2024-05-06 17:59:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03688v1"
    },
    {
        "title": "Language-Image Models with 3D Understanding",
        "authors": "Jang Hyun ChoBoris IvanovicYulong CaoEdward SchmerlingYue WangXinshuo WengBoyi LiYurong YouPhilipp KrähenbühlYan WangMarco Pavone",
        "links": "http://arxiv.org/abs/2405.03685v1",
        "entry_id": "http://arxiv.org/abs/2405.03685v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03685v1",
        "summary": "Multi-modal large language models (MLLMs) have shown incredible capabilities\nin a variety of 2D vision and language tasks. We extend MLLMs' perceptual\ncapabilities to ground and reason about images in 3-dimensional space. To that\nend, we first develop a large-scale pre-training dataset for 2D and 3D called\nLV3D by combining multiple existing 2D and 3D recognition datasets under a\ncommon task formulation: as multi-turn question-answering. Next, we introduce a\nnew MLLM named Cube-LLM and pre-train it on LV3D. We show that pure data\nscaling makes a strong 3D perception capability without 3D specific\narchitectural design or training objective. Cube-LLM exhibits intriguing\nproperties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting\nto improve 3D understanding from 2D context information. (2) Cube-LLM can\nfollow complex and diverse instructions and adapt to versatile input and output\nformats. (3) Cube-LLM can be visually prompted such as 2D box or a set of\ncandidate 3D boxes from specialists. Our experiments on outdoor benchmarks\ndemonstrate that Cube-LLM significantly outperforms existing baselines by 21.3\npoints of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7\npoints on the DriveLM dataset for complex reasoning about driving scenarios,\nrespectively. Cube-LLM also shows competitive results in general MLLM\nbenchmarks such as refCOCO for 2D grounding with (87.0) average score, as well\nas visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for\ncomplex reasoning. Our project is available at\nhttps://janghyuncho.github.io/Cube-LLM.",
        "updated": "2024-05-06 17:57:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03685v1"
    },
    {
        "title": "Towards A Human-in-the-Loop LLM Approach to Collaborative Discourse Analysis",
        "authors": "Clayton CohnCaitlin SnyderJustin MontenegroGautam Biswas",
        "links": "http://arxiv.org/abs/2405.03677v1",
        "entry_id": "http://arxiv.org/abs/2405.03677v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03677v1",
        "summary": "LLMs have demonstrated proficiency in contextualizing their outputs using\nhuman input, often matching or beating human-level performance on a variety of\ntasks. However, LLMs have not yet been used to characterize synergistic\nlearning in students' collaborative discourse. In this exploratory work, we\ntake a first step towards adopting a human-in-the-loop prompt engineering\napproach with GPT-4-Turbo to summarize and categorize students' synergistic\nlearning during collaborative discourse. Our preliminary findings suggest\nGPT-4-Turbo may be able to characterize students' synergistic learning in a\nmanner comparable to humans and that our approach warrants further\ninvestigation.",
        "updated": "2024-05-06 17:53:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03677v1"
    },
    {
        "title": "GREEN: Generative Radiology Report Evaluation and Error Notation",
        "authors": "Sophie OstmeierJustin XuZhihong ChenMaya VarmaLouis BlankemeierChristian BluethgenArne Edward MichalsonMichael MoseleyCurtis LanglotzAkshay S ChaudhariJean-Benoit Delbrouck",
        "links": "http://arxiv.org/abs/2405.03595v1",
        "entry_id": "http://arxiv.org/abs/2405.03595v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03595v1",
        "summary": "Evaluating radiology reports is a challenging problem as factual correctness\nis extremely important due to the need for accurate medical communication about\nmedical images. Existing automatic evaluation metrics either suffer from\nfailing to consider factual correctness (e.g., BLEU and ROUGE) or are limited\nin their interpretability (e.g., F1CheXpert and F1RadGraph). In this paper, we\nintroduce GREEN (Generative Radiology Report Evaluation and Error Notation), a\nradiology report generation metric that leverages the natural language\nunderstanding of language models to identify and explain clinically significant\nerrors in candidate reports, both quantitatively and qualitatively. Compared to\ncurrent metrics, GREEN offers: 1) a score aligned with expert preferences, 2)\nhuman interpretable explanations of clinically significant errors, enabling\nfeedback loops with end-users, and 3) a lightweight open-source method that\nreaches the performance of commercial counterparts. We validate our GREEN\nmetric by comparing it to GPT-4, as well as to error counts of 6 experts and\npreferences of 2 experts. Our method demonstrates not only higher correlation\nwith expert error counts, but simultaneously higher alignment with expert\npreferences when compared to previous approaches.\"",
        "updated": "2024-05-06 16:04:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03595v1"
    }
]