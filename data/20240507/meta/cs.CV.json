[
    {
        "title": "Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs",
        "authors": "Muhammad Uzair KhattakMuhammad Ferjad NaeemJameel HassanMuzammal NaseerFederico TombariFahad Shahbaz KhanSalman Khan",
        "links": "http://arxiv.org/abs/2405.03690v1",
        "entry_id": "http://arxiv.org/abs/2405.03690v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03690v1",
        "summary": "Recent advancements in Large Language Models (LLMs) have led to the\ndevelopment of Video Large Multi-modal Models (Video-LMMs) that can handle a\nwide range of video understanding tasks. These models have the potential to be\ndeployed in real-world applications such as robotics, AI assistants, medical\nimaging, and autonomous vehicles. The widespread adoption of Video-LMMs in our\ndaily lives underscores the importance of ensuring and evaluating their robust\nperformance in mirroring human-like reasoning and interaction capabilities in\ncomplex, real-world contexts. However, existing benchmarks for Video-LMMs\nprimarily focus on general video comprehension abilities and neglect assessing\ntheir reasoning capabilities over complex videos in the real-world context, and\nrobustness of these models through the lens of user prompts as text queries. In\nthis paper, we present the Complex Video Reasoning and Robustness Evaluation\nSuite (CVRR-ES), a novel benchmark that comprehensively assesses the\nperformance of Video-LMMs across 11 diverse real-world video dimensions. We\nevaluate 9 recent models, including both open-source and closed-source\nvariants, and find that most of the Video-LMMs, {especially open-source ones,}\nstruggle with robustness and reasoning when dealing with complex videos. Based\non our analysis, we develop a training-free Dual-Step Contextual Prompting\n(DSCP) technique to enhance the performance of existing Video-LMMs. Our\nfindings provide valuable insights for building the next generation of\nhuman-centric AI systems with advanced robustness and reasoning capabilities.\nOur dataset and code are publicly available at:\nhttps://mbzuai-oryx.github.io/CVRR-Evaluation-Suite/.",
        "updated": "2024-05-06 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03690v1"
    },
    {
        "title": "Pose Priors from Language Models",
        "authors": "Sanjay SubramanianEvonne NgLea MüllerDan KleinShiry GinosarTrevor Darrell",
        "links": "http://arxiv.org/abs/2405.03689v1",
        "entry_id": "http://arxiv.org/abs/2405.03689v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03689v1",
        "summary": "We present a zero-shot pose optimization method that enforces accurate\nphysical contact constraints when estimating the 3D pose of humans. Our central\ninsight is that since language is often used to describe physical interaction,\nlarge pretrained text-based models can act as priors on pose estimation.\n  We can thus leverage this insight to improve pose estimation by converting\nnatural language descriptors, generated by a large multimodal model (LMM), into\ntractable losses to constrain the 3D pose optimization. Despite its simplicity,\nour method produces surprisingly compelling pose reconstructions of people in\nclose contact, correctly capturing the semantics of the social and physical\ninteractions. We demonstrate that our method rivals more complex\nstate-of-the-art approaches that require expensive human annotation of contact\npoints and training specialized models. Moreover, unlike previous approaches,\nour method provides a unified framework for resolving self-contact and\nperson-to-person contact.",
        "updated": "2024-05-06 17:59:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03689v1"
    },
    {
        "title": "Language-Image Models with 3D Understanding",
        "authors": "Jang Hyun ChoBoris IvanovicYulong CaoEdward SchmerlingYue WangXinshuo WengBoyi LiYurong YouPhilipp KrähenbühlYan WangMarco Pavone",
        "links": "http://arxiv.org/abs/2405.03685v1",
        "entry_id": "http://arxiv.org/abs/2405.03685v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03685v1",
        "summary": "Multi-modal large language models (MLLMs) have shown incredible capabilities\nin a variety of 2D vision and language tasks. We extend MLLMs' perceptual\ncapabilities to ground and reason about images in 3-dimensional space. To that\nend, we first develop a large-scale pre-training dataset for 2D and 3D called\nLV3D by combining multiple existing 2D and 3D recognition datasets under a\ncommon task formulation: as multi-turn question-answering. Next, we introduce a\nnew MLLM named Cube-LLM and pre-train it on LV3D. We show that pure data\nscaling makes a strong 3D perception capability without 3D specific\narchitectural design or training objective. Cube-LLM exhibits intriguing\nproperties similar to LLMs: (1) Cube-LLM can apply chain-of-thought prompting\nto improve 3D understanding from 2D context information. (2) Cube-LLM can\nfollow complex and diverse instructions and adapt to versatile input and output\nformats. (3) Cube-LLM can be visually prompted such as 2D box or a set of\ncandidate 3D boxes from specialists. Our experiments on outdoor benchmarks\ndemonstrate that Cube-LLM significantly outperforms existing baselines by 21.3\npoints of AP-BEV on the Talk2Car dataset for 3D grounded reasoning and 17.7\npoints on the DriveLM dataset for complex reasoning about driving scenarios,\nrespectively. Cube-LLM also shows competitive results in general MLLM\nbenchmarks such as refCOCO for 2D grounding with (87.0) average score, as well\nas visual question answering benchmarks such as VQAv2, GQA, SQA, POPE, etc. for\ncomplex reasoning. Our project is available at\nhttps://janghyuncho.github.io/Cube-LLM.",
        "updated": "2024-05-06 17:57:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03685v1"
    },
    {
        "title": "An Empty Room is All We Want: Automatic Defurnishing of Indoor Panoramas",
        "authors": "Mira SlavchevaDave GausebeckKevin ChenDavid BuchhoferAzwad SabikChen MaSachal DhillonOlaf BrandtAlan Dolhasz",
        "links": "http://arxiv.org/abs/2405.03682v1",
        "entry_id": "http://arxiv.org/abs/2405.03682v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03682v1",
        "summary": "We propose a pipeline that leverages Stable Diffusion to improve inpainting\nresults in the context of defurnishing -- the removal of furniture items from\nindoor panorama images. Specifically, we illustrate how increased context,\ndomain-specific model fine-tuning, and improved image blending can produce\nhigh-fidelity inpaints that are geometrically plausible without needing to rely\non room layout estimation. We demonstrate qualitative and quantitative\nimprovements over other furniture removal techniques.",
        "updated": "2024-05-06 17:57:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03682v1"
    },
    {
        "title": "MemoryMamba: Memory-Augmented State Space Model for Defect Recognition",
        "authors": "Qianning WangHe HuYucheng Zhou",
        "links": "http://arxiv.org/abs/2405.03673v1",
        "entry_id": "http://arxiv.org/abs/2405.03673v1",
        "pdf_url": "http://arxiv.org/pdf/2405.03673v1",
        "summary": "As automation advances in manufacturing, the demand for precise and\nsophisticated defect detection technologies grows. Existing vision models for\ndefect recognition methods are insufficient for handling the complexities and\nvariations of defects in contemporary manufacturing settings. These models\nespecially struggle in scenarios involving limited or imbalanced defect data.\nIn this work, we introduce MemoryMamba, a novel memory-augmented state space\nmodel (SSM), designed to overcome the limitations of existing defect\nrecognition models. MemoryMamba integrates the state space model with the\nmemory augmentation mechanism, enabling the system to maintain and retrieve\nessential defect-specific information in training. Its architecture is designed\nto capture dependencies and intricate defect characteristics, which are crucial\nfor effective defect detection. In the experiments, MemoryMamba was evaluated\nacross four industrial datasets with diverse defect types and complexities. The\nmodel consistently outperformed other methods, demonstrating its capability to\nadapt to various defect recognition scenarios.",
        "updated": "2024-05-06 17:49:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2405.03673v1"
    }
]