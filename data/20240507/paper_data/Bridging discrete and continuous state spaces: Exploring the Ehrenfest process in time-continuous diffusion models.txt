Bridging discrete and continuous state spaces:
Exploring the Ehrenfest process in time-continuous diffusion models
LudwigWinkler*1 LorenzRichter*23 ManfredOpper145
Abstract they are close to the equilibrium distribution of the pro-
cess,fromwhichweassumetobeabletosamplereadily,
Generativemodelingviastochasticprocesseshas
such that the time-reversal then brings us back to the de-
ledtoremarkableempiricalresultsaswellasto
sired target distribution (Sohl-Dickstein et al., 2015). In
recent advances in their theoretical understand-
thisgeneralsetup,onecanmakeseveralchoicesandtake
ing. In principle, both space and time of the
differentperspectives. Whiletheoriginalattemptconsid-
processescanbediscreteorcontinuous. Inthis
ers discrete-time, continuous-space processes (Ho et al.,
work, we study time-continuous Markov jump
2020), one can show that in the small step-size limit the
processesondiscretestatespacesandinvestigate
modelsconvergetocontinuous-time,continuous-spacepro-
theircorrespondencetostate-continuousdiffusion
cesses given by stochastic differential equations (SDEs)
processes given by SDEs. In particular, we re- (Songetal.,2021). Thiscontinuoustimeframeworkthen
visittheEhrenfestprocess,whichconvergestoan
allowsfruitfulconnectionstomathematicaltoolssuchas
Ornstein-Uhlenbeckprocessintheinfinitestate
partialdifferentialequations,pathspacemeasuresandop-
spacelimit. Likewise,wecanshowthatthetime-
timalcontrol(Berneretal.,2024). Asanalternative,one
reversaloftheEhrenfestprocessconvergestothe
can consider discrete state spaces in continuous time via
time-reversedOrnstein-Uhlenbeckprocess. This
Markov jump processes, which have been suggested for
observationbridgesdiscreteandcontinuousstate
generativemodelinginCampbelletal.(2022). Thoseare
spacesandallowstocarryovermethodsfromone
particularlypromisingforproblemsthatnaturallyoperate
totherespectiveothersetting. Additionally,we
ondiscretedata,suchas,e.g.,text,images,graphstructures
suggestanalgorithmfortrainingthetime-reversal
orcertainbiologicaldata,tonamejustafew. Whilediscrete
ofMarkovjumpprocesseswhichreliesoncondi-
inspace,anappealingpropertyofthosemodelsisthattime-
tionalexpectationsandcanthusbedirectlyrelated
discretizationisnotnecessary–neitherduringtrainingnor
todenoisingscorematching. Wedemonstrateour duringinference1.
methodsinmultipleconvincingnumericalexperi-
ments. While the connections between Markov jump processes
and state-continuous diffusion processes have been stud-
iedextensively(see,e.g.,Kurtz(1972)),arelationshipbe-
tweentheirtime-reversalshasonlybeenlookedatrecently,
1.Introduction
whereanexactcorrespondenceisstillelusive(Santosetal.,
Generativemodelingbasedonstochasticprocesseshasled 2023). Inthiswork,wemakethiscorrespondencemorepre-
tostate-of-the-artperformanceinmultipletasksofinterest, cise,thusbridgingthegapbetweendiscrete-stategenerative
allaimingtosampleartificialdatafromadistributionthat modelingwithMarkovjumpprocessesandthecelebrated
isonlyspecifiedbyafinitesetoftrainingdata(Nichol& continuous-statescore-basedgenerativemodeling. Akey
Dhariwal, 2021). The general idea is based on the con- ingredient will be the so-called Ehrenfest process, which
cept of time-reversal: we let the data points diffuse until can be seen as the discrete-state analog of the Ornstein-
Uhlenbeckprocess,thatisusuallyemployedinthecontin-
*Equal contribution (the author order was determined by uous setting, as well as a new loss function that directly
numpy.random.rand(1)) 1Technical University of Berlin
translateslearningratefunctionsofatime-reversedMarkov
2ZuseInstituteBerlin3didaDatenschmiedeGmbH4University
jumpprocesstoscorefunctionsinthecontinuous-stateana-
of Birmingham 5University of Potsdam. Correspondence
to: Ludwig Winkler <winkler@tu-berlin.de>, Lorenz Richter
1Notethatthisisnottrueforthetime-andspace-continuous
<richter@zib.de>.
SDEcase,wheretrainingcanbedonesimulation-free,however,
inferencereliesonadiscretizationofthereversestochasticprocess.
Proceedings of the 41st International Conference on Machine
However,seeSection4.2forhigh-dimensionalsettingsinMarkov
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by
jumpprocesses.
theauthor(s).
1
4202
yaM
6
]LM.tats[
1v94530.5042:viXraBridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
log. Ourcontributionscanbesummarizedasfollows: forwhichtheforwardtransitionprobabilityisapproximated
by solving the forward Kolmogorov equation. Sun et al.
• Weproposealossfunctionviaconditionalexpectations (2022) introduced the idea of categorical ratio matching
fortrainingstate-discretediffusionmodels,whichex- forcontinuous-timeMarkovChainsbylearningthecondi-
hibitsadvantagescomparedtopreviouslossfunctions. tionaldistributionoccurringinthetransitionratiosofthe
marginalswhencomputingthereverserates. Recently,ina
• WeintroducetheEhrenfestprocessandderivethejump similarsetting,Santosetal.(2023)introducedapuredeath
momentsofitstime-reversedversion. processastheforwardprocess,forwhichonecanderivean
alternativelossfunction. Further,theyformallyinvestigate
• Thosejumpmomentsallowanexactcorrespondence
thecorrespondencebetweenMarkovjumpprocessesand
toscore-basedgenerativemodeling,suchthat,forthe
SDEs,however,incontrasttoourwork,withoutidentify-
firsttime,thetwomethodscannowbedirectlylinked
ingadirectrelationshipbetweenthecorrespondinglearned
tooneanother.
models.
• Inconsequence,thebridgebetweendiscreteandcon-
Finally,werefertothemonographsGardineretal.(1985);
tinuousstatespacebringsthepotentialthatonesetting
VanKampen(1992);Bre´maud(2013)forageneralintro-
canbenefitfromtherespectiveother.
ductiontoMarkovjumpprocesses.
This paper is organized as follows. After listing related
1.2.Notation
work in Section 1.1 and defining notation in Section 1.2,
weintroducethetime-reversalofMarkovjumpprocesses For transition probabilities of a Markov jump process
inSection2andproposealossfunctionforlearningthis M we write p (x|y) := P(M(t)=x|M(s)=y) for
t|s
reversalinSection2.1. WedefinetheEhrenfestprocessin s,t ∈ [0,T] and x,y ∈ Ω. With p (x) we denote the
t
Section3andstudyitsconvergencetoanSDEinSection3.1. (unconditional)probabilityoftheprocessattimet. Weuse
InSection3.2wethenestablishtheconnectionbetweenthe p :=p .Withδ wedenotetheKroneckerdelta.Fora
data 0 x,y
time-reversedEhrenfestprocessandscore-basedgenerative functionf,wesaythatf(x)∈o(g(x))iflim f(x) =0.
x→0 g(x)
modeling. Section 4 is devoted to computational aspects
and Section 5 provides some numerical experiments that
2.Time-reversedMarkovjumpprocesses
demonstrateourtheory. Finally,weconcludeinSection6.
WeconsiderMarkovjumpprocessesM(t)thatrunonthe
1.1.Relatedwork timeinterval[0,T]⊂Randareallowedtotakevaluesina
discretesetΩ∼ ⊂Zd.Usually,weconsiderΩ∼ ={0,...,S}d
StartingwithapaperbySohl-Dicksteinetal.(2015),anum-
such that the cardinality of our space is |Ω| = (S +1)d.
berofworkshavecontributedtothesuccessofdiffusion-
Jumpsbetweenthediscretestatesappearrandomly,where
basedgenerativemodeling,allinthecontinuous-stateset-
therateofjumpingfromstatey toxattimetisspecified
ting, see, e.g., Ho et al. (2020); Song & Ermon (2020);
bythefunctionr (x|y). Thejumpratesdeterminethejump
Kingmaetal.(2021);Nichol&Dhariwal(2021);Vahdat t
probabilityinatimeincrement∆tviatherelation
et al. (2021). We shall highlight the work by Song et al.
(2021),whichderivesanSDEformulationofscore-based p (x|y)=δ +r (x|y)∆t+o(∆t), (1)
t+∆t|t x,y t
generativemodelingandthusbuildsthefoundationforfur-
i.e. thehighertherateandthelongerthetimeincrement,
thertheoreticaldevelopments(Berneretal.,2024;Richter
themorelikelyisatransitionbetweentwocorresponding
&Berner,2024). Wenotethattheunderlyingideaoftime-
states. For a more detailed introduction to Markov jump
reversingadiffusionprocessdatesbacktoworkbyNelson
processes,werefertoAppendixB.1. Inordertosimulate
(1967);Anderson(1982).
theprocessbackwardsintime,weareinterestedintherates
Diffusion models on discrete state spaces have been con- of the time-reversed process M(t), which determine the
sideredbyHoogeboometal.(2021)basedonappropriate backwardtransitionprobabilityvia
binningoperationsofcontinuousmodels. Songetal.(2020)
p (x|y)=δ +r (x|y)∆t+o(∆t). (2)
proposedamethodfordiscretecategoricaldata,however, t−∆t|t x,y t
didnotperformanyexperiment. Apurelydiscretediffusion
Thefollowinglemmaprovidesaformulafortheratesofthe
model,bothintimeandspace,termedDiscreteDenoising
time-reversedprocess,cf. Campbelletal.(2022).
Diffusion Probabilistic Models (D3PMs) has been intro-
Lemma2.1. Fortwostatesx,y ∈ Ω,thetransitionrates
duced in Austin et al. (2021). Continuous-time Markov ofthetime-reversedprocessM(t)aregivenby
jumpprocessesondiscretespaceshavefirstbeenapplied
to generative modeling in Campbell et al. (2022), where,
r (y|x)=E
(cid:20)p t|0(y|x 0)(cid:21)
r (x|y), (3)
however,differentforwardprocesseshavebeenconsidered, t x0∼p0|t(x0|x) p (x|x ) t
t|0 0
2
⃗
⃗
⃗
⃗Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
wherer istherateoftheforwardprocessM(t). theonehand,wemayfocusonbirth-deathprocesses,for
t
whichr(y|x)isnon-zeroonlyfory =x±1,suchthatwe
Proof. SeeAppendixA. onlyneedtolearn2insteadofS−1functionsφ y. Inthe
next section we will argue that birth-death process are in
Remark2.2(Conditionalexpectation). Wenotethattheex-
factfavorableformultiplereasons. Ontheonehand, we
pectationappearingin(3)isaconditionalexpectation,con-
candoaTaylorexpansionsuchthatforcertainprocesses
ditionedonthevalueM(t)=x. Thiscanbecomparedto
itsufficestoonlyconsideroneapproximatingfunction,as
theSDEsetting,wherethetime-reversalviathescorefunc-
willbeshowninRemark3.3.
tioncanalsobewrittenasaconditionalexpectation,namely
(cid:104) (cid:105)
∇ logpSDE(x) = E ∇ logpSDE(x|x ) ,
x t x0∼pS 0|D tE(x0|x) x t|0 0 3.TheEhrenfestprocess
seeLemmaA.1intheappendixformoredetails. Wewill
elaborateonthiscorrespondenceinSection3.2. Inprinciple,wearefreetochooseanyforwardprocessM(t)
forwhichwecancomputetheforwardtransitionprobabili-
Whiletheforwardtransitionprobabilityp canusuallybe
t|0 tiesp andwhichisclosetoitsstationarydistributionafter
t|0
approximated (e.g. by solving the corresponding master
anottoolongruntimeT. Inthesequel,wearguethatthe
equation,seeAppendixB.1),thetime-reversedtransition
Ehrenfestprocessisparticularlysuitable–bothfromatheo-
function p is typically not tractable, and we therefore
0|t reticalandpracticalperspective.Fornotationalconvenience,
mustresorttoalearningtask. Oneideaistoapproximate
wemaketheargumentindimensiond=1,noting,however,
p ≈pθ byadistributionparameterizedinθ ∈Rp (e.g.
0|t 0|t thatamultidimensionalextensionisstraightforward. For
vianeuralnetworks),see,e.g. Campbelletal.(2022)and
computationalaspectsinhigh-dimensionalspaceswerefer
Appendix C.2. We suggest an alternative method in the
toSection4.1.
following.
WedefinetheEhrenfestprocess2as
2.1.Lossfunctionsviaconditionalexpectations
S
(cid:88)
Recallingthatanyconditionalexpectationcanbewritten E S(t):= Z i(t), (6)
asanL2 projection(seeLemmaA.2intheappendix),we i=1
definetheloss
whereeachZ isaprocessonthestatespaceΩ = {0,1}
i
(cid:34)(cid:18) p (y|x )(cid:19)2(cid:35) withtransitionratesr(0|1)=r(1|0)= 1 (sometimescalled
L y(φ y)=E φ y(x,t)− pt|0 (x|x0
)
, (4) telegraphorKacprocess). Wenotetha2 ttheEhrenfestpro-
t|0 0
cessisabirth-deathprocesswithvaluesin{0,...,S}and
transitionrates
wheretheexpectationisoverx ∼p ,t∼U(0,T),x∼
0 data
p t|0(x|x 0). AssumingasufficientlyrichfunctionclassF,it 1 x
thenholdsthattheminimizerofthelossequalsthecondi- r(x+1|x)= (S−x), r(x−1|x)= . (7)
2 2
tionalexpectationinLemma2.1foranyy ∈Ω,i.e.
(cid:20)p (y|x )(cid:21) We observe that we can readily transform the time-
argminL y(φ y)=E x0∼p0|t(x0|x) pt|0 (x|x0 ) . (5) independentratesin(7)totime-dependentrates
φy∈F t|0 0
r (x±1|x):=λ r(x±1|x) (8)
t t
Wecanthusdirectlylearntheconditionalexpectation. In
contrasttoapproximatingthereversetransitionprobability viaatimetransformation,whereλ : [0,T] → R,seeAp-
p ,thishastheadvantagethatwedonotneedtomodela pendixB.2. Withoutlossofgenerality,wewillfocusonthe
0|t
distribution,butafunction,whichislesschallengingfrom time-independentrates(7)inthesequel.
anumericalperspective. Furthermore,wewillseethatthe
OnecompellingpropertyoftheEhrenfestprocessisthatwe
conditionalexpectationcanbedirectlylinkedtothescore
cansamplewithoutneedingtosimulatetrajectories.
function in the SDE setting, such that our approximating
functions φ can be directly linked to the approximated Lemma3.1. AssumingE (0)=x ,theEhrenfestprocess
y S 0
score. Wenotethatthelosshasalreadybeenderivedina canbewrittenas
moregeneralversioninMengetal.(2022)andappliedto
thesettingofMarkovjumpprocessesinLouetal.(2023), E S(t)=E 0,S(t)+E 1,S(t), (9)
however,followingadifferentderivation. Apotentialdis-
2TheEhrenfestprocesswasintroducedbytheRussian-Dutch
advantageoftheloss(4),ontheotherhand,isthatwemay
and German physicists Tatiana and Paul Ehrenfest to explain
need to approximate different functions φ y for different thesecondlawofthermodynamics, seeEhrenfest&Ehrenfest-
y ∈Ω. This,however,canbecopedwithintwoways. On Afanassjewa(1907).
3Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
where E (t) ∼ B(S − x ,1 − f(t)) and E (t) ∼ withµ (x )=x e−tandσ2 =(1−e−2t).Forthequantity
0,S 0 1,S t 0 0 t
B(x ,f(t)) are independent binomial random variables intheconditionalexpectation(3)wecanthuscompute
0
a tin od nf pr( ot) ba:= bil1 2 ity(1 is+ gie v− ent) b. yC to hn ese dq isu ce rn et tl ey, ct oh ne vf oo lr uw tia or ndtransi- p t|0(cid:16) x±δ(cid:12) (cid:12) (cid:12)x 0(cid:17) ≈exp(cid:18)
∓2(x−µ t(x
0))δ−δ2(cid:19)
p (x|x ) 2σ2
p (x|x )= (cid:88) P(E (t)=z)P(E (t)=x−z). t|0 0 t
t|0 0 0,S 1,S (15a)
z∈Ω
(10) (cid:18) δ2 (cid:19)(cid:32) (x−µ (x ))δ ((x−µ (x ))δ)2(cid:33)
≈exp − 1∓ t 0 + t 0 ,
2σ2 σ2 2σ4
t
Proof. SeeAppendixA.
(15b)
We note that the sum in (10) can usually be numerically whereweusedtheshorthandδ := √2 .
S
evaluatedwithoutgreateffort. Remark3.3(Learningofconditionalexpectation). Notethat
theapproximation(15b)allowsustodefinetheloss
3.1.Convergencepropertiesintheinfinitestatespace
L (φ):=
limit Gauß
(cid:34)(cid:18) (cid:18)
∓2(x−µ (x
))δ−δ2(cid:19)(cid:19)2(cid:35)
Itisknownthatcertain(appropriatelyscaled)Markovjump E φ(x,t)−exp t 0 .
2σ2
processesconvergetostate-continuousdiffusionprocesses t
whenthestatespacesizeS+1tendstoinfinity(see,e.g., (16)
Kurtz (1972); Gardiner et al. (1985)). For the Ehrenfest
Further,wecanwrite
process,thisconvergencecanbestudiedquiterigorously.
Tothisend,letusintroducethescaledEhrenfestprocess  p t|0(cid:16) x±δ(cid:12) (cid:12) (cid:12)x 0(cid:17) (cid:18) δ2 (cid:19)
2 (cid:18) S(cid:19) E x0 p (x|x ) ≈exp − 2σ2
E(cid:101)S(t):= √
S
E S(t)−
2
(11)

t|0 0
(cid:104)
t
(cid:105)
withtransitionrates 1∓
(x−E x0[µ t(x 0)])δ
+
E x0 ((x−µ t(x 0))δ)2
,
σ2 2σ4
(17)
√
r(cid:18) x± √2 (cid:12) (cid:12) (cid:12)x(cid:19) = S (√ S∓x), (12) wherex 0 ∼ p 0|t(x 0|x). Inconsequence,thisallowsusto
S(cid:12) 4
considerthelossfunctions
(cid:104) (cid:105)
(cid:110) √ √ √ (cid:111) L Taylor(φ 1):=E (φ 1(x,t)−µ t(x 0))2 , (18)
nowhavingvaluesinΩ= − S,− S+ √2 ,..., S .
S
We are interested in the large state space limit S → ∞, and
notingthatthisimplies √2 →0forthetransitionsteps,thus (cid:20)(cid:16) (cid:17)2(cid:21)
S L (φ ):=E φ (x,t)−((x−µ (x ))δ)2 ,
leadingtoarefinementofthestatespace. Thefollowing Taylor,2 2 2 t 0
convergenceresultisshowninSumitaetal.(2004,Theorem (19)
4.1). where the expectations are over x ∼ p ,t ∼
0 data
Proposition 3.2 (State space limit of Ehrenfest process). U(0,T),x ∼ p t|0(x|x 0). We can also only consider the
In the limit S → ∞, the scaled Ehrenfest process E(cid:101)S(t) firstordertermintheTaylorexpansion(15b),suchthatwe
convergesinlawtotheOrnstein-UhlenbeckprocessX for thenonlyhavetoapproximateoneinsteadoftwofunctions.
t
anyt∈[0,T],whereX isdefinedviatheSDE
t Since the scaled forward Ehrenfest process converges to
√ the Ornstein-Uhlenbeck process, we can expect the time-
dX =−X dt+ 2dW , (13)
t t t reversedscaledEhrenfestprocesstoconvergetothetime-
reversaloftheOrnstein-Uhlenbeckprocess. Weshallstudy
withW beingstandardBrownianmotion.
t
thisconjectureinmoredetailinthesequel.
ForanillustrationoftheconvergencewerefertoFigure1.
3.2.Connectionsbetweentime-reversalofMarkovjump
NotethattheconvergenceofthescaledEhrenfestprocess
processesandscore-basedgenerativemodeling
totheOrnstein-Uhlenbeckprocessimplies
Inspecting Lemma 2.1, which specifies the rate function
p (x|x )≈pOU(x|x ):=N(x;µ (x ),σ2) (14) of a backward Markov jump process, we realize that the
t|0 0 t|0 0 t 0 t
4Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
time-reversalessentiallydependsontwothings,namelythe E(cid:101)S arethengivenby
forwardratefunctionwithswitchedargumentsaswellas
theconditionalexpectationoftheratiobetweentwoforward
b(x,t)=−b(x)+D(x)E
(cid:20)∆ Sp t|0(x|x 0)(cid:21)
transitionprobabilities. Togainsomeintuition,letusfirst x0∼p0|t(x0|x) p (x|x )
t|0 0
assumethatthestatespacesizeS+1islargeenoughand
+o(S−1/2),
thatthetransitiondensityp canbeextendedtoR(which
t|0
(25)
wecallp )suchthatitcanbeapproximatedviaaTaylor
t|0
expansion. Wecanthenassumethat D(x)=D(x)+o(S−1/2), (26)
(cid:18) (cid:12) (cid:19) (cid:18) (cid:12) (cid:19) where
2 (cid:12) (cid:12) 2
r x± √ (cid:12)x ≈r x(cid:12)x∓ √ (20)
S(cid:12) (cid:12) S
∆ p (x|x ):=
p t|0(x+ √2 S|x 0)−p t|0(x|x 0)
(27)
S t|0 0 √2
aswellas S
isaonestepdifferenceandp andp aretheforward
(cid:16) (cid:12) (cid:17) t|0 0|t
p t|0 x± √2 S(cid:12) (cid:12)x 0
≈
p t|0(x|x 0)± √2 S∇p t|0(x|x 0) a prn od cr ee sv se .rsetransitionprobabilitiesofthescaledEhrenfest
p (x|x ) p (x|x )
t|0 0 t|0 0
(21a)
Proof. SeeAppendixA.
2
=1± √ ∇logp (x|x ), (21b)
t|0 0 Remark 3.5 (Convergence of the time-reversed Ehrenfest
S
process). WenotethatProposition3.4impliesthatthetime-
reversedEhrenfestprocessinexpectedtoconvergeinlaw
where the conditional expectation of ∇logp (x|x ) is tothetime-reversedOrnstein-Uhlenbeckprocess. Thiscan
t|0 0
reminiscentofthescorefunctioninSDE-baseddiffusion be seen as follows. For S → ∞, we know via Proposi-
models(cf. LemmaA.1intheappendix). Thisalreadyhints tion 3.2 that the forward Ehrenfest process converges to
atacloseconnectionbetweenthetime-reversalofMarkov theOrnstein-Uhlenbeckprocess,i.e. p convergestopOU,
t|0 t|0
jumpprocessesandscore-basedgenerativemodeling. Fur- wherepOU(x|x )isthetransitiondensityoftheOrnstein-
t|0 0
ther,notethat(21a)correspondsto(15b)forlargeenough Uhlenbeck process (13) starting at X = x . Together
0 0
S andp t|0 ≈pO t|0U. with the fact that the finite difference approximation op-
erator ∆ converges to the first derivative, this implies
Weshallmaketheaboveobservationmorepreciseinthe S
(cid:104) (cid:105)
following.Tothisend,letusstudythefirstandsecondjump thatE ∆Spt|0(x|x0) isexpectedtoconverge
x0∼p0|t(x0|x) pt|0(x|x0)
momentsoftheMarkovjumpprocess,givenas (cid:104) (cid:105)
to E ∇logpOU(x|x ) . Now, Lemma A.1
x0∼pO 0|U t(x0|x) t|0 0
in the appendix shows that this conditional expectation
(cid:88)
b(x)= (y−x)r(y|x), (22) is the score function of the Ornstein-Uhlenbeck process,
(cid:104) (cid:105)
y∈Ω,y̸=x i.e. ∇logpOU(x) = E ∇logpOU(x|x ) .
D(x)=
(cid:88)
(y−x)2r(y|x), (23)
t x0∼pO 0|U t(x0|x) t|0 0
Finally, we note that the first and second jump moments
y∈Ω,y̸=x convergetothedriftandthesquareofthediffusioncoef-
ficient of the limiting SDE, respectively (Gardiner et al.,
seeAppendixB.3. ForthescaledEhrenfestprocess(11)we 1985). Therefore,thescaledtime-reversedEhrenfestpro-
canreadilycompute
cessE(cid:101)S(t)isexpectedtoconvergeinlawtotheprocessY
t
givenby
b(x)=−x, D(x)=2, (24) √
dY =(cid:0) Y +2∇logpOU (Y )(cid:1) dt+ 2dW , (28)
t t T−t t t
whichalignwiththedriftanddiffusioncoefficient(which
whichisthetime-reversaloftheOrnstein-Uhlenbeckpro-
isthesquarerootofD)oftheOrnstein-Uhlebeckprocess
cess stated in (13). Note that we write (28) as a forward
inProposition3.2. Inparticular,wecanshowthefollowing
process from t = 0 to t = T, where W is a forward
t
relationbetweenthejumpmomentsoftheforwardandthe
Brownianmotion,whichinducesthetime-transformation
backwardEhrenfestprocesses,respectively.
t(cid:55)→T −tinthescorefunction.
Proposition3.4. LetbandDbethefirstandsecondjump Remark3.6(Generalizations). FollowingtheproofofPropo-
momentsofthescaledEhrenfestprocessE(cid:101)S. Thefirstand sition3.4,weexpectthattheformulasforthefirsttwojump
secondjumpmomentsofthetime-reversedscaledEhrenfest momentsofthetime-reversedMarkovjumpprocess,stated
5
⃗
⃗
⃗
⃗Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
in (25) and (26), are valid for any (appropriately scaled)
Prior and target distribution Time-reversed Ornstein-Uhlenbeck process
4
birth-deathprocesswhosetransitionratesfulfill 0.4
2
0.3
1
(r(x±δ|x)−r(x|x∓δ))=o(S−1), (29) 0.2 0
S
0.1 2
where δ is a jump step size that decreases with the state
0.0 4
4 2 0 2 4 2.0 1.5 1.0 0.5 0.0
spacesizeS+1. x t
Prior and target distribution Time-reversed Ehrenfest process
4
Crucially,Remark3.5showsthatwecandirectlylinkap- 0.4
proximationsinthe(scaled)state-discretesettingtostandard 0.3 2
state-continuousscore-basedgenerativemodelingvia 0.2 0
E x0∼p0|t(x0|x)(cid:34) p t|0 p( tx |0± (x|√ x2 S 0)|x 0)(cid:35) ≈1±√2 S∇logpO
t
U(x), 00 .. 01
4 2 x0 2 4
42
2.0 1.5 1 t.0 0.5 0.0
(30)
see also the proof of Proposition 3.4 in Appendix A. In
Figure1.Wedisplaytwotime-reversedprocessesfromt=2to
particular,thisallowsfortransferlearningbetweenthetwo t=0thattransportastandardGaussian(leftpanels,ingreen)to
cases. E.g.,wecantrainadiscretemodelandusetheap- amultimodalGaussianmixturemodel(leftpanels,inorange),or
proximationoftheconditionalexpectation(uptoscaling)as abinomialdistributiontoabinomialmixture,respectively,once
thescorefunctioninacontinuousmodel. Likewise,wecan usingadiffusionprocessincontinuousspace(upperpanel)and
trainacontinuousmodelandapproximatetheconditional onceatime-reversed(scaled)Ehrenfestprocessindiscretespace
expectationbythescore. Wehaveillustratedthelatterap- with S = 100 (lower panel). Crucially, in both cases we use
the(state-continuous)scorefunctiontoemploythetime-reversal,
proach in Figure 1, where we have used the (analytically
whichforthisproblemisknownanalytically,seeAppendixD.1.
available) score function that transports a standard Gaus-
Theplotsdemonstratethatthedistributionsoftheprocessesseem
siantoamultimodalGaussianmixtureinadiscrete-state
indeedverycloseoneanother,implyingthattheapproximation
Ehrenfestprocessthatstartsatabinomialdistributionwhich
(30)isquiteaccurateevenforamoderatestatespacesizeS+1.
isdesignedinsuchawaythatitconvergestothestandard
GaussianforS →∞.
S =2552suchthattheinterval[−1,1]contains256states
Similarto(4),thecorrespondence(30)motivatestotraina
thatcorrespondtotheRGBcolorvaluesofimages,recalling
state-discretescaledEhrenfestmodelwiththelossdefined that the increments between the states are √2 . Further,
by S
notingtheactualOrnstein-UhlenbeckprocessthatDDPMis
L
(φ):=E(cid:20)(cid:16)
φ(x,t)−∇logpOU(x|x
)(cid:17)2(cid:21)
(31a)
trainedon,weemploythetimescalingλ
t
= 1 2β(t),where
OU (cid:101) (cid:101) t|0 0 βandfurtherdetailsarestatedinAppendixD.2,andchoose
=E(cid:34)(cid:18)
φ (cid:101)(x,t)+
(x− σµ 2t(x
0))(cid:19)2(cid:35)
, (31b)
the(time-de (cid:18)pendent 2)ra
(cid:12)
(cid:12)te (cid:19)s
√
S √
t r t x± √ S(cid:12) (cid:12)x =β(t) 8 ( S∓x), (32)
wheretheexpectationisoverx ∼p ,t∼U(0,T),x∼
0 data
accordingto(8)and(12).
p (x|x )andwhereµ (x )=x e−tandσ2 =(1−e−2t),
t|0 0 t 0 0 t
as before. In fact, this loss is completely analog to the
denoisingscorematchinglossinthestate-continuoussetting. 4.Computationalaspects
Welatersetφ=1± √2 φ (cid:101)∗,whereφ (cid:101)∗istheminimizerof
S Inthissection,wecommentoncomputationalaspectsthat
(31),togettheapproximatedconditionalexpectation.
are necessary for the training and simulation of the time-
Remark 3.7 (Ehrenfest process as discrete-state DDPM).
reversalofour(scaled)Ehrenfestprocess. Forconvenience,
Tomaketheaboveconsiderationsmoreprecise,notethat
werefertoAlgorithm1andAlgorithm2inAppendixC.1
we can directly link the discrete-space Ehrenfest process
for the corresponding training and sampling algorithms,
to pretrained score models in continuous space, such as,
respectively.
e.g.,thecelebrateddenoisingdiffusionprobabilisticmodels
(DDPM)(Hoetal.,2020). Thosemodelsusuallytransport
4.1.Modelingofdimensions
astandardGaussiantothetargetdensitythatissupported
on[−1,1]d. Inordertocopewiththefactthatthescaled Inordertomakecomputationsfeasibleinhigh-dimensional
Ehrenfestprocessterminates(approximately)atastandard spacesΩd,wetypicallyfactorizetheforwardprocess,such
GaussianirrespectiveofthesizeS+1,wetypicallychoose thateachdimensionpropagatesindependently,cf. Camp-
6
x
xBridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
belletal.(2022). NotethatthisisanalogtotheOrnstein-
Uhlenbeckprocessinscore-basedgenerativemodeling,in
whichthedimensionsalsodonotinteract,see,e.g.,(13).
Wethusconsider
d
p (x|y)=(cid:89) p(i)(x(i)|y(i)), (33)
t|0 t|0
i=1
where p(i) is the transition probability for dimension i ∈
t|0
{1,...,d}andx(i)isthei-thcomponentofx∈Ωd.
InCampbelletal.(2022)itisshownthattheforwardand
backwardratesthentranslateto
Figure2.Weplothistogramsof500.000samplesfromthetime-
d
r (x|y)=(cid:88) r(i)(x(i)|y(i))Γ , (34) reversedscaledEhrenfestprocessatdifferenttimes.Theprocesses
t t x¬i,y¬i
havebeentrainedwiththreedifferentlosses.
i=1
whereΓ isoneifalldimensionsexceptthei-thdi-
x¬i,y¬i
mensionagree,and
seeGillespie(1976))wouldrequireaverylongsampling
r
(x|y)=(cid:88)d E(cid:34) p t|0(y(i)|x( 0i))(cid:35)
r(i)(x(i)|y(i))Γ ,
t fi om ree. reA lys osu ng τg -e ls et ae pd inin gC foa rm ap nb ae pll pe rot xa il m.( a2 t0 e2 s2 i) m, uw le atc ioa nn mth ee tr he --
t p (x(i)|x(i)) t x¬i,y¬i
i=1 t|0 0 ods(Gillespie,2001). Thegeneralideaistonotsimulate
(35)
jumpbyjump,butwaitforatimeintervaloflengthτ and
wheretheexpectationisoverx( 0i) ∼p 0|t(x( 0i)|x). Equation applyalljumpsatonce. Onecanshowthatthenumberof
(35)illustratesthatthetime-reversedprocessdoesnotfac- jumpsisPoissondistributedwithameanofτr (x|y). For
t
torizeinthedimensionseventhoughtheforwardprocess furtherdetailswerefertoAlgorithm2.
does.
Notewith(34)thatforabirth-deathprocessajumpappears 5.Numericalexperiments
onlyinonedimensionatatime,whichimpliesthat
Inthissection,wedemonstrateourtheoreticalinsightsin
r (x±δ |x)=r(i)(x(i)±δ(i)|x(i)), (36) numericalexperiments. Ifnotstatedotherwise,wealways
t i t i
considerthescaledEhrenfestprocessdefinedin(11). We
wherenowδ = (0,...,0,δ(i),0,...,0)⊤ withδ(i) being willcomparethedifferentvariantsoftheloss(4),namely
i i i
the jump step size in the i-th dimension. Likewise, (35) L defined in (16), L defined in (18) and L
Gauss Taylor OU
becomes definedin(31).
(cid:34)
p
(y(i)|x(i))(cid:35)
r (x±δ |x)=E t|0 0 r(i)(x(i)|x(i)+δ(i)), 5.1.Illustrativeexample
t i p (x(i)|x(i)) t i
t|0 0 Letusfirstconsideranillustrativeexample,forwhichthe
(37)
where the expectation is over x(i) ∼ p (x(i)|x), which
datadistributionistractable.Weconsideraprocessind=2
0 0|t 0 withS =32,wherethe(S+1)d =332differentstatecom-
stilldependsonalldimensions.
binationsinp aredefinedtobeproportionaltothepixels
data
Foreachdimensioni∈{1,...,d}wecanthereforeapprox- ofanimageoftheletter“E”.Sincethedimensionalityis
imatetheconditionalexpectationappearingin(37)viathe d=2,wecanvisuallyinspecttheentiredistributionatany
lossfunction(4)withtwofunctionsφ :Rd×[0,T]→R timet∈[0,T]byplotting2-dimensionalhistogramsofthe
i,b
and φ : Rd ×[0,T] → R. Alternatively, we can learn simulatedprocesses. Withthisexperimentwecaninpar-
i,d
justtwofunctionsφ : Rd×[0,T] → Rd fortheentire ticularcheckthatmodelingthedimensionsoftheforward
b/d
spaceandidentifyφ =φ(i) . process independently from one another (as explained in
i,b/d b/d
Section4.1)isnorestrictionforthebackwardprocess. In-
deedFigure2showsthatthetime-reversedprocess,whichis
4.2.τ-leaping
learnedwith(versionsof)theloss(4),cantransporttheprior
Thefactthatjumpsonlyhappeninonedimensionatatime distribution(whichisapproximatelybinomial,or,loosely
impliesthatthenaiveimplementationofchangingcompo- speaking,abinnedGaussian)tothespecifiedtarget. Again,
nentbycomponent(e.g. byusingtheGillespie’salgorithm, notethatthisplotdoesnotdisplaysinglerealizations,but
7
⃗
⃗
⃗Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
entiredistributions,which,inthiscase,areapproximated learning,thedifferentlossesindicatedifferentwaysofincor-
with500.000samples. Werealizethatinthissimpleprob- poratingthepretrainedmodel,seeAppendixD.2. Wereal-
lemL performsslightlybetterthanL andL . izethatbothlossesproducecomparableresults,withsmall
Gauß OU Taylor
Asexpected,theapproximationsworksufficientlywelleven advantagesforL . Evenwithouthavinginvestedmuch
OU
for a moderate state space size S +1. As argued in Sec- timeinfinetuninghyperparametersandsamplingstrategies,
tion 3.1, this should get even better with growing S. For we reach competitive performance with respect to the al-
furtherdetails,werefertoAppendixD.3. ternativemethodsLDR(Campbelletal.,2022)andD3PM
(Austin et al., 2021). Remarkably, even the attempt with
5.2.MNIST transfer learning returns good results, without having ap-
pliedanyfurthertraining. Forfurtherdetails, wereferto
Forabasicimagemodelingtask,weconsidertheMNIST
AppendixD.5,wherewealsodisplaymoresamplesinFig-
dataset,whichconsistsofgrayscalepixelsandwasresized
ures6-9.
to 32 × 32 to match the required input size of a U-Net
neural network architecture3, such that d = 32 × 32 =
1024andS = 255. Asbefore,wetrainourtime-reversed
Ehrenfestmodelbyusingthevariantsofthelossintroduced
inSection2.1. InFigure3wedisplaygeneratedsamples
fromamodeltrainedwithL . Themodelswiththeother
OU
losses look equally good, so we omit them. For further
details,werefertoAppendixD.4.
Figure4.CIFAR-10 samples Figure5.CIFAR-10 samples
from the Ehrenfest process from the Ehrenfest process
withapretrainedmodel,further withapretrainedmodel,further
finetunedwithL OU. finetunedwithL Taylor.
IS(↑) FID(↓)
Ehrenfest L 8.75 11.57
OU
(transferlearning) L 8.68 11.72
Taylor
Ehrenfest L 9.50 5.08
OU
(fromscratch) L 9.66 5.12
Taylor
Figure3.MNISTsamplesobtainedwiththetime-reversedscaled L Taylor2 9.40 5.44
EhrenfestprocesswhichwastrainedwithL OU. Ehrenfest L
OU
9.14 6.63
(pretrained) L 9.06 6.91
Taylor
τ-LDR(0) 8.74 8.10
5.3.ImagemodelingwithCIFAR-10
Alternative τ-LDR(10) 9.49 3.74
Asamorechallengingtask,weconsidertheCIFAR-10data methods D3PMGauss 8.56 7.34
set, with dimension d = 3×32×32 = 3072, each tak- D3PMAbsorbing 6.78 30.97
ing 256 different values (Krizhevsky et al., 2009). In the
experiments we again compare our three different losses, Table1.PerformanceintermsofInceptionScore(IS)(Salimans
however,realizethatL didnotproducesatisfyingre- etal.,2016)andFrechetInceptionDistance(FID)(Heuseletal.,
Gauß
sultsandhadconvergenceissues,whichmightfollowfrom 2017)onCIFAR-10over50.000samples.Wecomparetwolosses
numericalissuesduetotheexponentialtermappearingin and consider three different scenarios: we train a model from
scratch,wetaketheU-Netmodelthatwaspretrainedinthestate-
(16). Further, we consider three different scenarios: we
continuoussetting(called“transferlearning”)orwetakethesame
trainamodelfromscratch,wetaketheU-Netmodelthat
modelandfurthertrainitwithourstate-discretetrainingalgorithm
waspretrainedinthestate-continuoussetting,andwetake
(called“pretraining”).
thesamemodelandfurthertrainitwithourstate-discrete
trainingalgorithm(recallRemark3.7,whichdescribeshow
tolinktheEhrenfestprocesstoDDPM). 6.Conclusion
WedisplaythemetricsinTable1. Whenusingonlytransfer
Inthiswork,wehaverelatedthetime-reversalofdiscrete-
3Taken from the repository https://github.com/ space Markov jump processes to continuous-space score-
w86763777/pytorch-ddpm. basedgenerativemodeling,suchthat,forthefirsttime,one
8Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
can directly link models of the respective settings to one workfordiscretedenoisingmodels. AdvancesinNeural
another. Whilewehavefocusedonthetheoreticalconnec- InformationProcessingSystems,35:28266–28279,2022.
tions,ournumericalexperimentsdemonstratethatwecan
Ehrenfest, P. and Ehrenfest-Afanassjewa, T. U¨ber
alreadyreachcompetitiveperformancewiththenewloss
zwei bekannte Einwa¨nde gegen das Boltzmannsche H-
functionthatweproposed. Wesuspectthatfurthertuning
Theorem. Hirzel,1907.
andthenowpossibletransferlearningbetweendiscreteand
continuousstatespacewillfurtherenhancetheperformance. Gardiner, C. W. et al. Handbook of stochastic methods,
Onthetheoreticalside,weanticipatethattheconvergence volume3. SpringerBerlin,1985.
of the time-reversed jump processes to the reversed SDE
canbegeneralizedevenfurther,whichweleavetofuture Gillespie, D. T. A general method for numerically simu-
work. latingthestochastictimeevolutionofcoupledchemical
reactions. Journalofcomputationalphysics,22(4):403–
434,1976.
Acknowledgements
Gillespie, D. T. Approximate accelerated stochastic sim-
L.W.acknowledgessupportbytheFederalMinistryofEdu-
ulationofchemicallyreactingsystems. TheJournalof
cationandResearch(BMBF)forBIFOLD(01IS18037A).
chemicalphysics,115(4):1716–1733,2001.
TheresearchofL.R.hasbeenpartiallyfundedbyDeutsche
Forschungsgemeinschaft (DFG) through the grant CRC Heusel,M.,Ramsauer,H.,Unterthiner,T.,Nessler,B.,and
1114 “Scaling Cascades in Complex Systems” (project Hochreiter,S. Ganstrainedbyatwotime-scaleupdate
A05, project number 235221301). M.O. has been par- ruleconvergetoalocalnashequilibrium. Advancesin
tiallyfundedbyDeutscheForschungsgemeinschaft(DFG) neuralinformationprocessingsystems,30,2017.
throughthegrantCRC1294“DataAssimilation”(project
Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba-
number318763901).
bilisticmodels. Advancesinneuralinformationprocess-
ingsystems,33:6840–6851,2020.
Impactstatement
Hoogeboom,E.,Nielsen,D.,Jaini,P.,Forre´,P.,andWelling,
Thegoalofthisworkistoadvancethetheoreticalunder- M. Argmaxflowsandmultinomialdiffusion: Learning
standing of generative modeling based on stochastic pro- categoricaldistributions.AdvancesinNeuralInformation
cesses,eventuallyleadingtoimprovementsinapplications ProcessingSystems,34:12454–12465,2021.
aswell. Whiletherearepotentialsocietalconsequencesof
ourworkinprinciple,wedonotseeanyconcreteissuesand Kingma, D., Salimans, T., Poole, B., and Ho, J. Varia-
thus believe that we do not specifically need to highlight tionaldiffusionmodels. AdvancesinNeuralInformation
any.
ProcessingSystems,34:21696–21707,2021.
Kingma,D.P.andBa,J. Adam: Amethodforstochastic
References optimization. arXivpreprintarXiv:1412.6980,2014.
Anderson,B.D. Reverse-timediffusionequationmodels. Krizhevsky,A.,Hinton,G.,etal. Learningmultiplelayers
StochasticProcessesandtheirApplications,12(3):313– offeaturesfromtinyimages. 2009.
326,1982.
Kurtz,T.G. Therelationshipbetweenstochasticanddeter-
Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van ministicmodelsforchemicalreactions. TheJournalof
DenBerg,R. Structureddenoisingdiffusionmodelsin ChemicalPhysics,57(7):2976–2978,1972.
discrete state-spaces. Advances in Neural Information
Kurtz,T.G. Approximationofdiscontinuousprocessesby
ProcessingSystems,34:17981–17993,2021.
continuousprocesses. InStochasticNonlinearSystems
inPhysics,Chemistry,andBiology: Proceedingsofthe
Berner, J., Richter, L., and Ullrich, K. An optimal con-
WorkshopBielefeld,Fed.Rep.ofGermany,October5–11,
trolperspectiveondiffusion-basedgenerativemodeling.
1980,pp.22–35.Springer,1981.
TransactionsonMachineLearningResearch,2024.
Loshchilov, I. and Hutter, F. SGDR: Stochastic gra-
Bre´maud, P. Markov chains: Gibbs fields, Monte Carlo dient descent with warm restarts. arXiv preprint
simulation,andqueues,volume31. SpringerScience& arXiv:1608.03983,2016.
BusinessMedia,2013.
Lou,A.,Meng,C.,andErmon,S. Discretediffusionlan-
Campbell,A.,Benton,J.,DeBortoli,V.,Rainforth,T.,Deli- guagemodelingbyestimatingtheratiosofthedatadistri-
giannidis,G.,andDoucet,A. Acontinuoustimeframe- bution. arXivpreprintarXiv:2310.16834,2023.
9Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
Meng,C.,Choi,K.,Song,J.,andErmon,S. Concretescore
matching: Generalizedscorematchingfordiscretedata.
AdvancesinNeuralInformationProcessingSystems,35:
34532–34545,2022.
Metzner,P. TransitionpaththeoryforMarkovprocesses.
PhDthesis,FreieUniversita¨tBerlin,2008.
Nelson,E. DynamicaltheoriesofBrownianmotion. Press,
Princeton,NJ,1967.
Nichol,A.Q.andDhariwal,P.Improveddenoisingdiffusion
probabilistic models. In International Conference on
MachineLearning,pp.8162–8171.PMLR,2021.
Richter,L.andBerner,J. Improvedsamplingvialearned
diffusions. InInternationalConferenceonLearningRep-
resentations,2024.
Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V.,
Radford,A.,andChen,X. Improvedtechniquesfortrain-
ingGANs. Advancesinneuralinformationprocessing
systems,29,2016.
Santos,J.E.,Fox,Z.R.,Lubbers,N.,andLin,Y.T. Black-
out diffusion: Generative diffusion models in discrete-
statespaces. arXivpreprintarXiv:2305.11089,2023.
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and
Ganguli,S. Deepunsupervisedlearningusingnonequi-
libriumthermodynamics. InInternationalconferenceon
machinelearning,pp.2256–2265.PMLR,2015.
Song, J., Meng, C., and Ermon, S. Denoising diffusion
implicitmodels. arXivpreprintarXiv:2010.02502,2020.
Song, Y. and Ermon, S. Improved techniques for train-
ingscore-basedgenerativemodels. Advancesinneural
informationprocessingsystems,33:12438–12448,2020.
Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Er-
mon,S.,andPoole,B. Score-basedgenerativemodeling
throughstochasticdifferentialequations. InInternational
ConferenceonLearningRepresentations,2021.
Sumita,U.,Gotoh,J.-y.,andJin,H. Numericalexploration
ofdynamicbehavioroftheOrnstein-Uhlenbeckprocess
viaEhrenfestprocessapproximation.AppliedProbability
Trust,2004:194–195,2004.
Sun,H.,Yu,L.,Dai,B.,Schuurmans,D.,andDai,H. Score-
basedcontinuous-timediscretediffusionmodels. arXiv
preprintarXiv:2211.16750,2022.
Vahdat,A.,Kreis,K.,andKautz,J. Score-basedgenerative
modelinginlatentspace.AdvancesinNeuralInformation
ProcessingSystems,34:11287–11302,2021.
Van Kampen, N. G. Stochastic processes in physics and
chemistry,volume1. Elsevier,1992.
10Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
A.Proofsandadditionalstatements
Inthissection,weprovidetheproofsofthestatementsinthemaintextandstatesomeadditionallemmas,whicharehelpful
forourarguments.
ProofofLemma2.1. First,wenotethatthederivationofthebackwardratesisknown,see,e.g.,Campbelletal.(2022). For
convenience,werepeattheessentialpartoftheproof. Startingwiththeidentity
r (y|x)p (x)=r (x|y)p (y), (38)
t t t t
wecancompute
p (y)
r (y|x)= t r (x|y) (39a)
t p (x) t
t
(cid:80)
p (y|x )p (x )
= x0∈Ω t|0 0 0 0 r (x|y) (39b)
p (x) t
t
(cid:88) p t|0(y|x 0)p t|0(x|x 0)p 0(x 0)
= r (x|y) (39c)
p (x|x ) p (x) t
t 0 t
x0∈Ω
(cid:88) p t|0(y|x 0)
= p (x |x)r (x|y) (39d)
p (x|x ) 0|t 0 t
t|0 0
x0∈Ω
(cid:20)p (x|x )(cid:21)
=E t|0 0 r (x|y), (39e)
x0∼p0|t(x0|x) p (y|x ) t
t|0 0
whichshowstheidentity.
LemmaA.1(Scorefunctionasconditionalexpectation). ConsiderthediffusionprocessX definedbytheSDE
t
dX =b(X ,s)dt+σ(t)dW , X ∼p , (40)
t t t 0 data
withsuitabledriftfunctionb : Rd×[0,T] → Rd anddiffusioncoefficientσ : [0,T] → Rd×d,letpSDE beitsmarginal
t
densityandletpSDE(x|y):=P(X =x|X =y)fort>s≥0beatransitionprobability. Itthenholds
t|s t s
(cid:104) (cid:105)
∇ logpSDE(x)=E ∇ logpSDE(x|x ) . (41)
x t x0∼pS 0|D tE(x0|x) x t|0 0
Proof. NotingtheidentitypSDE(x)=(cid:82) pSDE(x|x )p (x )dx ,wecancompute
t Rd t|0 0 data 0 0
∇ pSDE(x)
∇logpSDE(x)= x t (42a)
t pSDE(x)
t
(cid:82) ∇ logpSDE(x|x )pSDE(x|x )p (x )dx
Rd x t|0 0 t|0 0 data 0 0
= (cid:82) pSDE(x|x )p (x )dx (42b)
Rd t|0 0 data 0 0
(cid:104) (cid:105)
=E ∇ logpSDE(x|x ) , (42c)
x0∼pS 0|D tE(x0|x) x t|0 0
whereitholdspSDE(x |x)=
pS t|D 0E(x|x0)pdata(x0)
byBayes’formula.
0|t 0 (cid:82) RdpS t|D 0E(x|x0)pdata(x0)dx0
Lemma A.2 (Conditional expectation as L2 projection). Let A ∈ Rd and B ∈ R be two random variables and let
φ∈C(Rd,R). Thenthesolutionto
(cid:104) (cid:105)
φ∗ = argmin E (φ(A)−B)2 (43)
φ∈C(Rd,R)
isgivenby
φ∗(a)=E[B|A=a]. (44)
11
⃗
⃗Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
Proof. LetφC(a)=E[B|A=a]. Wecompute
E(cid:2) (φ(A)−B)2(cid:3) =E(cid:2) (φ(A)−φC(A)+φC(A)−B)2(cid:3)
(45a)
=E(cid:2) (φ(A)−φC(A))2(cid:3) +E(cid:2) (φC(A)−B)2(cid:3) −2E(cid:2) (φ(A)−φC(A))(φC(A)−B)(cid:3)
, (45b)
whichisminimizedbyφ=φC sincethelasttermisequalto4
E (cid:2)E (cid:2) (φ(A)−φC(A))(φC(A)−B)(cid:3)(cid:3) =E (cid:2) (φ(A)−φC(A))E (cid:2) (φC(A)−B)(cid:3)(cid:3) =0. (46)
A B|A A B|A
Thereforeφ∗ =φC.
ProofofLemma3.1. WeconsidertheEhrenfestprocessasdefinedin(6),assumingthatitstartsatE (0)=x . Wecan
S 0
writetheprocessas
S
(cid:88) (cid:88) (cid:88)
E (t)= Z (t)= Z (t)+ Z (t)=:E (t)+E (t), (47)
S i i i 0,S 1,S
i=1 Zi(0)=0 Zi(0)=1
whereE isasumofx independentBernoullirandomvariablesZ with
1,S 0 i
f(t):=P(Z (t)=1|Z (0)=1)= 1(cid:0) 1+e−t(cid:1) , (48)
i i 2
andwhereE isthesumofS−x randomvariablesZ withP(Z (t)=1|Z (0)=0)=1−f(t)= 1(1−e−t). Thus,
0,S 0 i i i 2
bothE andE arebinomialrandomvariablesdistributedas
0,S 1,S
E (t)∼B(S−x ,1−f(t)), E (t)∼B(x ,f(t)). (49)
0,S 0 1,S 0
ProofofProposition3.4. WefirstrecallthescaledEhrenfestprocessfrom(11),
(cid:18) (cid:19)
2 S
E(cid:101)S(t):= √
S
E S(t)−
2
, (50)
(cid:110) √ √ √ (cid:111)
andnotethatE(cid:101)S ∈ − S,− S+ √2 ,..., S ,wherethebirth-deathtransitionstransformfrom±1inE
S
to±√2 in
S S
itsscaledversionE(cid:101)S. Accordingly,thereverseratesfromLemma2.1translateto
 (cid:16) (cid:12) (cid:17)
r t(cid:18) x± √2 S(cid:12) (cid:12) (cid:12) (cid:12)x(cid:19) =E x0∼p0|t(x0|x)p t|0
p
tx |0± (x|√ x2 S 0)(cid:12) (cid:12)x 0 r(cid:18) x(cid:12) (cid:12) (cid:12) (cid:12)x± √2 S(cid:19) . (51)
Letusintroducethenotation(whichslightlydeviatesfromthenotationinProposition3.4)
p (x+δ|x )−p (x|x )
t|0 0 t|0 0
∆ p (x|x ):= (52)
δ t|0 0 δ
andnotetheidentity
p (x+δ|x )=p(x|x )+δ∆ p (x|x ), (53)
t|0 0 0 δ t|0 0
4HerethenotationE referstotheexpectationoverA,whereasE referstotheexpectationoverBconditionalonA.
A B|A
12
⃗Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
whichissometimescalledNewton’sseriesforequidistantnodesandcanbeseenasadiscreteanalogofaTaylorseries,
where,however,termsoforderhigherthanonevanish. Wecannowcomputethefirstjumpmoment
b(x):= (cid:88) δr(x+δ|x)= (cid:88) δE (cid:20)p t|0(x+δ|x 0)(cid:21) r(x|x+δ) (54a)
x0∼p0|t(x0|x) p (x|x )
(cid:110) (cid:111) (cid:110) (cid:111) t|0 0
δ∈ −√2 ,√2 δ∈ −√2 ,√2
S S S S
=
(cid:88)
δE
(cid:20)
1+
δ∆ δp t|0(x|x 0)(cid:21)
r(x|x+δ) (54b)
x0∼p0|t(x0|x) p (x|x )
(cid:110) (cid:111) t|0 0
δ∈ −√2 ,√2
S S
=
(cid:88)
δr(x|x+δ)+
(cid:88)
δ2E
(cid:20)∆ δp t|0(x|x 0)(cid:21)
r(x|x+δ) (54c)
x0∼p0|t(x0|x) p (x|x )
(cid:110) (cid:111) (cid:110) (cid:111) t|0 0
δ∈ −√2 ,√2 δ∈ −√2 ,√2
S S S S
= (cid:88) δr(x|x+δ)+E (cid:20)∆ δ¯p t|0(x|x 0)(cid:21) (cid:88) δ2r(x|x+δ)+o(δ¯) (54d)
x0∼p0|t(x0|x) p (x|x )
(cid:110) (cid:111) t|0 0 (cid:110) (cid:111)
δ∈ −√2 ,√2 δ∈ −√2 ,√2
S S S S
=−b(x)+D(x)E
(cid:20)∆ δ¯p t|0(x|x 0)(cid:21)
+o(δ¯), (54e)
x0∼p0|t(x0|x) p (x|x )
t|0 0
whereforδ¯:= √2
S
wehaveusedthat∆ δ¯p 0|t(x|x 0)=∆ −δ¯p 0|t(x|x 0)+o(δ¯)since
p(x|x )−p(x−δ¯|x ) p(x+δ¯|x )−p(x|x ) 2p(x|x )−p(x+δ¯|x )−p(x−δ¯|x )
0 0 = 0 0 + 0 0 0 (55a)
δ¯ δ¯ δ¯
p(x+δ¯|x )−p(x|x )
= 0 0 +o(δ¯), (55b)
δ¯
aswellas
(cid:88) (cid:88)
δr(x|x+δ)=− δr(x+δ|x)+o(S−1/2), (56)
(cid:110) (cid:111) (cid:110) (cid:111)
δ∈ −√2 ,√2 δ∈ −√2 ,√2
S S S S
and
(cid:88) (cid:88)
δ2r(x|x+δ)= δ2r(x+δ|x)+o(S−1), (57)
(cid:110) (cid:111) (cid:110) (cid:111)
δ∈ −√2 ,√2 δ∈ −√2 ,√2
S S S S
since
√ √
(cid:18) (cid:12) (cid:12) 2 (cid:19) S (cid:18)√ (cid:18) 2 (cid:19)(cid:19) S (cid:16)√ (cid:17) 1
r x(cid:12)x± √ = S± x± √ = S±x ± (58a)
(cid:12) S 4 S 4 2
(cid:18) (cid:12) (cid:19)
2 (cid:12) 1
=r x∓ √ (cid:12)x ± . (58b)
S(cid:12) 2
B.Backgroundontime-continuousMarkovjumpprocesses
Inthissectionwewillprovidesomebackgroundoncontinuous-time,discrete-spaceMarkovjumpprocesses.
B.1.AbriefintroductiontoMarkovjumpprocesses
Inthissectionwewillgiveabriefintroductiontotime-continuousMarkovprocessesonadiscretestatespace,whichis
basedonasummaryinMetzner(2008,Section2.2). WerefertheinterestedreadertoGardineretal.(1985);VanKampen
(1992);Bre´maud(2013)forfurtherdetails.
WedenotewithX anΩ-valuedstochasticprocessonadiscrete(countable)statespaceΩwithacontinuoustimeparameter
t
0≤t<∞.TheprocessiscalledaMarkovprocessifforalltimest >t ≥···≥t =0andforanyx ,...,x ∈Ω
k+1 k 0 k+1 0
13
⃗ ⃗Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
itholds
P(cid:0)
X
tk+1
=x
k+1(cid:12)
(cid:12)X
tk
=x k,··· ,X
t0
=x
0(cid:1) =P(cid:0)
X
tk+1
=x
k+1(cid:12)
(cid:12)X
tk
=x
k(cid:1)
. (59)
Theprocessiscalledhomogeneousifthetransitionprobabilityonlydependsonthetimeincrementt −t . Wedenote
k+1 k
with
p (x|y)=P(X =x|X =y) (60)
t|s t s
thetransitionprobabilityfortimest>s>0anddefinethematrix
(cid:0) (cid:1)
P(t):= p (x|y) . (61)
t|0 x,y∈Ω
P(t)isastochasticmatrix,i.e.
(cid:88)
p (x|y)≥0, p (x|y)=1, (62)
t|0 t|0
y∈Ω
foreachtimet≥0andeachx,y ∈Ω. Thefamilyoftransitionmatrices{P(t)} iscalledtransitionsemi-groupsinceit
t≥0
obeystheChapman-Kolmogorovequation
P(t+s)=P(t)P(s) (63)
fors,t≥0withP(0)=Id.
AlocalcharacterizationofthetransitionsemigroupofaMarkovjumpprocesscanbeobtainedbyconsideringtheinfinitesimal
changesofthetransitionprobabilities. Onecanshowthatthelimit
P(t)−Id
R= lim (64)
t→0+ t
exists(entrywise),whichissometimeswrittenas
p (x|y)=δ +r (x|y)∆t+o(∆t), (65)
t+∆t|t x,y t
cf. equation(1)inSection2. ThematrixR=(r(x|y)) iscalledinfinitesimalgeneratorofthetransitionsemigroup
x,y∈Ω
{P(t)} becauseit“generates”thetransitionsemigroupviatherelation
t≥0
(cid:88)∞ tn
P(t)=etR = Rn. (66)
n!
n=0
Onecanshowthat
(cid:88)
0≤r(x|y)<∞, r(x|y)=0, (67)
y∈Ω
forallx,y ∈Ωwithx̸=y,andwecaninterpretr(x|y)asatransitionratefromstateytox,measuringtheaveragenumber
oftransitionsperunittime. ThediagonalelementsofRaredefinedas
(cid:88)
r(x|x)=− r(x|y) (68)
y̸=x
for each x ∈ Ω. Analog to the state-continuous case, it holds the backward Kolmogorov equation for the conditional
expectationψ(t)=(E[f(X )|X =x])⊤ foranobservablef :Ω→R,namely
t 0 x∈Ω
d
ψ(t)=Rψ(t), ψ(0)=(f(x))⊤ . (69)
dt x∈Ω
Further,forthevectorofstateprobabilitiesp(t):=(p (x)) ,recallingthenotationp (x):=P(X =x),itholdsthe
t x∈Ω t t
forwardKolmogorovequation,alsoknownasMasterequation,namely
d
p(t)=p(t)R. (70)
dt
Forthetransitiondensitieswehave
d (cid:88)
p (x|y)= p (z|y)R(z,x), (71)
dt t|0 t|0
z∈Ω
14Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
orinmatrixnotation
d
P(t)=P⊤(t)R. (72)
dt
Itcanbesolvedas
(cid:18)(cid:90) t (cid:19)
(cid:0) (cid:1)
p (x|y) =exp (r (x|y)) ds , (73)
t|0 x,y∈Ω s x,y∈Ω
0
whereexpisthematrixexponential.
B.2.Time-transformationofMarkovjumpprocesses
NotethatwecanalwaystransformaMarkovjumpprocesswithatimedependentrater intoonewithatimeindependent
t
raterusingatimetransformation. Thiscanbeseenbylookingatthemasterequationdefinedin(72),namely
d
P(t)=P⊤(t)R , (74)
dt t
wherenowtheratematrixR istime-dependent. Forsimplicity,letusassumeR =λ R,whereλ:[0,T]→RandRis
t t t
time-independent. Wecannowintroducethenewtimeτ =τ(t)andcompute
d dP(τ)dτ
P(τ(t))= =P⊤(τ(t))λ R. (75)
dt dτ dt t
Now,choosing dτ =λ andthusτ(t)=(cid:82)t λ ds(wherewehaveassumedτ(0)=0),yieldstheequation
dt t 0 s
d
P(τ)=P⊤(τ)R, (76)
dτ
wherenowtheratematrixdoesnotdependontimeanymore.
B.3.ConvergenceofMarkovjumpprocesses
TheconvergenceofMarkovjumpprocessestoSDEsinthelimitoflargestatespaces(withappropriatelyscaledjumpsizes)
hasformallybeenstudiedviatheKramers-Moyalexpansion(Gardineretal.,1985;VanKampen,1992). Formorerigorous
results,werefer,e.g.,to(Kurtz,1972;1981).
OnecangetsomeintuitionbylookingatthefirsttwojumpmomentsoftheMarkovjumpprocess. Thefirstjumpmomentis
definedas
1
b(x):= lim E[M(t+∆t)−M(t)|M(t)=x] (77a)
∆t→0∆t
 
1 (cid:88) (cid:88)
= ∆l tim →0∆t yp t+∆t|t(y|x)−x p t+∆t|t(y|x) (77b)
y∈Ω y∈Ω
 
1 (cid:88)
= ∆l tim →0∆t (y−x)p t+∆t|t(y|x) (77c)
y∈Ω;y̸=x
(cid:88)
= (y−x)r(y|x). (77d)
y∈Ω;y̸=x
Similarly,thesecondjumpmomentisdefinedas
1 (cid:104) (cid:105)
D(x):= lim E (M(t+∆t)−M(t))(M(t+∆t)−M(t))⊤|M(t)=x (78a)
∆t→0∆t
(cid:88)
= (y−x)(y−x)⊤r(y|x). (78b)
y∈Ω;y̸=x
NotethatthedriftanddiffusioncoefficientforSDEsaredefinedanalogously.
15Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
Algorithm1ApproximationofconditionalexpectationfortheEhrenfestprocess.
input BatchsizeK,gradientstepsM,twoneuralnetworksφ andφ withinitialparametersθ(0)andθ(0),respectively,
b d b d
approximationsp ≈p (typicallyeitherby(10)or(14)).
(cid:101)0|t 0|t
output Approximationsoftheconditionalexpectationsappearingin(3).
form←0,...,M −1do
Sampledatapointsx(1),...,x(K) ∼p .
0 0 data
Sampleterminaltimest ,...,t ∼U(0,T).
1 K
Simulate x(k) for each k ∈ {1,...,K} according to the forward (scaled) Ehrenfest process (11). Note that every
tk
dimensioncanbesampledindependentlyandsimulation-freeasbinomialrandomvariables,seeLemma3.1.
Computetwolosses:
L(cid:98)b(θ b(m))= K1
d
k(cid:88)K =1(cid:88) i=d 1 φ( bi)(x( tkk),t k)− p (cid:101)( tki) |
p
(cid:101)0
(
t(cid:16)
ki)
|x 0t( (cid:16)ki) x,( t(k ki) ),(+
k)(cid:12)
(cid:12)√ x2 S
(
0i(cid:12) (cid:12) )x ,(( 0 ki )), (cid:17)(k)(cid:17) 2 (79a)
L(cid:98)d(θ d(m))= K1
d
k(cid:88)K =1(cid:88) i=d 1 φ( di)(x( tkk),t k)− p (cid:101)( tki) |
p
(cid:101)0
(
t(cid:16)
ki)
|x 0t( (cid:16)ki) x,( t(k ki) ),(−
k)(cid:12)
(cid:12)√ x2 S
(
0i(cid:12) (cid:12) )x ,(( 0 ki )), (cid:17)(k)(cid:17) 2 (79b)
Dogradientdescent:
(cid:16) (cid:17)
θ b(m+1) ←step θ(m),∇L(cid:98)b(θ b(m)) (80a)
(cid:16) (cid:17)
θ d(m+1) ←step θ(m),∇L(cid:98)d(θ d(m)) (80b)
endfor
C.Computationalaspects
Inthissectionwecommentoncomputationalaspectsofthetime-reversalofMarkovjumpprocesses.
C.1.Approximationoftheconditionalexpectation
For the approximation of the conditional expectation appearing in the backward rates from Lemma 2.1 we propose
Algorithm1andforsamplingfromatime-reversedprocesswithapproximatedbackwardratesweproposeAlgorithm2.
C.2.Learningthereversedtransitionprobability
AnalternativewaytoapproximatethebackwardratesspecifiedinLemma2.1istoapproximatethereversedtransition
probabilityp (x |x)withatractabledistributionpθ (x |x). Tothisend,onecanconsidertheloss
0|t 0 0|t 0
(cid:104) (cid:105)
L(θ):=−E logpθ (x |x) . (87)
t∼U(0,T),x0∼pdata,x∼pt|0(x|x0) 0|t 0
Thefollowinglemmamotivatesthisloss(cf. Proposition8inCampbelletal.(2022)).
LemmaC.1. Itholds
(cid:104) (cid:16) (cid:17)(cid:105)
L(θ)=E D p (x |x)|pθ (x |x) +C, (88)
t∼U(0,T),x∼pt(x) KL 0|t 0 0|t 0
whereC isaconstantthatdoesnotdependonθ.
16Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
Algorithm2Samplingfromdatadistributionp .
data
input Rateoftheforwardprocessr ,approximationoftheconditionalexpectationsin(3)viaφ andφ forthebirthand
t b b
deathtransitions,respectively,approximationofterminaldistributionp ≈p ,leapingtimeτ >0.
ref T
output Datasamplethatisapproximatelydistributedaccordingtop .
data
Samplex ∼p .
T ref
Sett←T.
whilet>0do
fori=1,...,ddo
Computebackwardrates:
(cid:18) (cid:12) (cid:19) (cid:18) (cid:12) (cid:19)
r(i) x(i)+ √2 (cid:12) (cid:12)x(i) =φ(i)(x,t)r(i) x(i)(cid:12) (cid:12)x(i)+ √2 (81)
t S(cid:12) b t (cid:12) S
(cid:18) (cid:12) (cid:19) (cid:18) (cid:12) (cid:19)
r(i) x(i)− √2 (cid:12) (cid:12)x(i) =φ(i)(x,t)r(i) x(i)(cid:12) (cid:12)x(i)− √2 (82)
t S(cid:12) d t (cid:12) S
DrawPoissionrandomvariable:
(cid:18) (cid:18) 2 (cid:12) (cid:19)(cid:19)
ρ(i) ∼Pois τr(i) x(i)+ √ (cid:12)x(i) (83)
b t S(cid:12)
(cid:18) (cid:18) 2 (cid:12) (cid:19)(cid:19)
ρ(i) ∼Pois τr(i) x(i)− √ (cid:12)x(i) (84)
d t S(cid:12)
Doleapingstep:
2 2
x(i) ←x(i)+ρ(i)√ −ρ(i)√ (85)
t−τ t b S d S
√ √
x(i) ←Clamp(x(i) ,− S, S) (86)
t−τ t−τ
endfor
t←t−τ
endwhile
Returnx .
0
Proof. LetC beaconstantthatdoesnotdependonθ. Wecancompute
(cid:104) (cid:16) (cid:12) (cid:17)(cid:105)
E D p (x |x)(cid:12)pθ (x |x) (89a)
t∼U(0,T),x∼pt(x) KL 0|t 0 (cid:12) 0|t 0
(cid:34) (cid:34) (cid:35)(cid:35)
p (x |x)
=E E log 0|t 0 (89b)
t∼U(0,T),x∼pt(x) x0∼p0|t(x0|x) pθ (x |x)
0|t 0
(cid:34) (cid:35)
p (x |x)
=E log 0|t 0 (89c)
t∼U(0,T),x∼pt(x),x0∼p0|t(x0|x) pθ (x |x)
0|t 0
(cid:104) (cid:105)
=−E logpθ (x |x) −C, (89d)
t∼U(0,T),x0∼pdata,x∼pt|0(x|x0) 0|t 0
where we used the tower property of conditional expectations and the identity p (x)p (x |x) = p (x )p (x|x ).
t 0|t 0 data 0 t|0 0
NotingthedefinitionofLin(87)concludestheproof.
Theaboveguaranteesthatpθ (x |x)=p (x |x)ifandonlyifL(θ)=0.
0|t 0 0|t 0
WethereforecanuseAlgorithm3forapproximatingthebackwardratesandAlgorithm4forsamplingthetime-reversed
process.
NotethatallprobabilitiesareprobabilitiesonthediscretesetΩ⊂Z,fulfillinge.g. (cid:80) p (x |x)=1forallx∈Ω
x0∈Ω 0|t 0
andt∈[0,T]. Specifically,(p (x |x)) ∈[0,1](S+1)×(S+1) isa(stochastic)matrixforallt∈[0,T]. Inpractice,
0|t 0 x0,x∈Ω
17
⃗
⃗
⃗
⃗Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
Algorithm3Approximationofreversetransitionprobability
input BatchsizeK,gradientstepsM,parametrizationpθ(0) withinitialparametersθ(0).
0|t
output pθ(M) ≈p .
0|t 0|t
form←0,...,M −1do
Sampledatapointx(1),...,x(K) ∼p .
0 0 data
Sampleterminaltimest ,...,t ∼U(0,T).
1 K
Simulatex(k)foreachk ∈{1,...,K}accordingtotheforwardprocess.
tk
ComputeL(cid:98)(θ(m))=−1 (cid:80)K logpθ(m)(x(k)|x(k))
(cid:16)
K k=1
(cid:17)
0|t 0 tk
θ(m+1) ←step θ(m),∇L(cid:98)(θ(m))
endfor
Algorithm4Samplingfromdatadistributionp .
data
input Rate of the forward process r , approximation of forward transition probabilities p ≈ p , approximation of
t (cid:101)t|0 t|0
reversetransitionprobabilitiespθ ≈p ,approximationofterminaldistributionp ≈p .
0|t 0|t ref T
output Datasamplethatisapproximatelydistributedaccordingtop .
data
Samplex ∼p .
T ref
Simulatebirth-deathprocessfromtimet=T tot=0withbackwardrates
r (x±1|x)=
(cid:88)
pθ (x
|x)p (cid:101)t|0(x±1|x 0)
r (x|x±1). (95)
t 0|t 0 p (x|x ) t
(cid:101)t|0 0
x0∈Ω
Returnx .
0
however,weoftenmodelp (x |x)withacontinuousdistribution,parametrizedbyaneuralnetwork,e.g.
0|t 0
pθ (x |x):=N(x ;µθ(x,t),Σθ(x,t)), (90)
(cid:101)0|t 0 0
wheremeanandcovariancearelearnedwithaneuralnetworkφ:R×[0,T]→R2,i.e.
φθ(x,t)=(µθ(x,t),Σθ(x,t))⊤. (91)
Inordertorecoverprobabilitiesonadiscreteset,weusethecumulativedistributionfunction
(cid:90) z
Φ(z;x,t):= pθ (x |x)dx , (92)
(cid:101)0|t 0 0
−∞
whichisanalyticallyavailableforaGaussian. Foradiscretestatex ∈Ωwethenapproximatetheprobabilityvia
0
p (x |x)≈pθ (x |x):=Φ(x ;x,t)−Φ(x −1;x,t) (93)
0|t 0 0|t 0 0 0
for x ∈ Ω\{0,S}. For x = 0 we consider pθ (x |x) := Φ(x ;x,t) and for x = S we consider pθ (x |x) :=
0 0 0|t 0 0 0 0|t 0
Φ(−∞;x,t)−Φ(x −1;x,t).
0
Fortheforwardprobabilitiesp ,wecaneithercomputethematrix
t|0
(cid:18)(cid:90) t (cid:19)
(cid:0) (cid:1)
p (x|x ) =exp (r (x|x )) ds , (94)
t|0 0 x0,x∈Ω
0
s 0 x0,x∈Ω
whereexpisthematrixexponential,orwecanapproximatep (x|x )withGaussiansduetotheconvergencepropertiesof
t|0 0
theEhrenfestprocess.
D.Numericaldetails
InthissectionweelaborateonnumericaldetailsregardingourexperimentsinSection5.
18
⃗Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
D.1.AtractableGaussiantoyexample
InordertoillustratethepropertiesoftheEhrenfestprocess,weconsiderthefollowingtoyexample. Letusstartwiththe
SDEsettingandconsiderthedatadistribution
M
(cid:88)
p (x)= γ N(x;µ ,Σ ), (96)
data m m m
m=1
where(cid:80)M γ = 1andµ ∈ Rd,Σ ∈ Rd×d. Further,fortheinferenceSDEweconsidertheOrnstein-Uhlenbeck
m=1 m m m
process
dX =−AX dt+BdW , X ∼p , (97)
t t t 0 data
withmatricesA,B ∈Rd×d. Forsimplicity,letusconsiderA=α1andB =β1withα,β ∈R. Conditionedonaninitial
conditionx ,themarginaldensitiesofX arethengivenby
0
pSDE(x;x )=N
(cid:18)
x;x e−αt,
β2
(cid:0)
1−e−2αt(cid:1)1(cid:19)
. (98)
t 0 0 2α
Wecanthereforecompute
(cid:90)
pSDE(x)= pSDE(x;x )p (x )dx (99a)
t t 0 data 0 0
Rd
=
(cid:88)M
γ
(cid:90)
N
(cid:18)
x;y e−αt,
β2
(cid:0)
1−e−2αt(cid:1)1(cid:19)
N(x ;µ ,Σ )dx (99b)
m 0 2α 0 m m 0
m=1
Rd
=
(cid:88)M
γ N
(cid:18)
x;e−αtµ ,
β2
(cid:0) 1−e−2αt(cid:1)1+e−2αtΣ
(cid:19)
. (99c)
m m 2α m
m=1
Wecannowreadilycomputethescore∇logpSDE(x).
t
D.2.ConnectingtheEhrenfestprocesstoscore-basedgenerativemodeling
As we have outline in Section 3.2, we can directly link the Ehrenfest process to score-based generative modeling in
continuoustimeandspace. Inparticular,wecanuseanymodelthathasbeentrainedinthetypicallyusedsettingforour
state-discreteEhrenfestprocess. Forinstance,wecanrelyonDDPMmodels,whichtypicallyconsidertheforwardSDE
1 (cid:112)
dX =− β(t)X + β(t)dW (100)
t 2 t t
onthetimeinterval[0,1],whereβ :[0,1]→Risafunctionthatscalestime.ThiscanbeseebylookingattheFokker-Planck
equation. Fortheprocess(100)conditionedattheinitialvalueX =x itholds
0 0
(cid:18) (cid:18) 1(cid:90) t (cid:19) (cid:18) (cid:90) t (cid:19)(cid:19)
X ∼N exp − β(s)ds x ,1−exp − β(s)ds . (101)
t 2 0
0 0
Inpractice,wechooseβ(t) := β +t(β −β )withβ = 0.1,β = 20,assuggestedinSongetal.(2021).
min max min min max
NotethatthistypicallyguaranteesthatX isapproximatelydistributedaccordingtoN(0,1),independentofx . Forour
1 0
experimentsweuseamodelprovidedbytheDiffuserpackage. Notethatthismodelisactuallynotthescore,butthescaled
scoresandoneneedsthetransformation
(cid:101)
s(x,1000t)
∇logpSDE(x)=− (cid:101) . (102)
t (cid:114) (cid:16) (cid:17)
(cid:82)t
1−exp − β(s)ds
0
TheDDPMframeworkimplicitlytrainsamodelφ(x,t)on
(cid:101)
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:34)(cid:18)
(X −µ (x
))(cid:19)2(cid:35)
L (φ)=E φ(x,t)−∇logpOU(x|x ) =E φ(x,t)+ t t 0 , (103)
OU (cid:101) (cid:101) t|0 0 (cid:101) σ2
t
19Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
whichinanalternativeformulationsimplifiestopredictingthenoiseε of
t
(cid:115)
(cid:18) 1(cid:90) t (cid:19) (cid:18) (cid:90) t (cid:19)
X =x exp − β(s)ds +ε 1−exp − β(s)ds . (104)
t 0 2 t
0 0
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
µt(x0) σt
Sincethetermsµ (x )andσ cancel,wearriveatthesimplifiedloss
t 0 t
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:34)(cid:18)
ε
(cid:19)2(cid:35)
L (φ)=E φ(x,t)−∇logpOU(x|x ) =E φ(x,t)+ t . (105)
OU (cid:101) (cid:101) t|0 0 (cid:101) σ
t
TheOrstein-Uhlenbeckforwardratescanbethussubstitutedby
E
(cid:34) p t|0(x± √2 S|x 0)(cid:35)
≈1±
√2
∇logpOU(x)=1±
√2
φ(x,t). (106)
x0∼p0|t(x0|x) p t|0(x|x 0) S t S (cid:101)
Similarly,theTaylorratescanbecomputedwith
 (cid:16) (cid:12) (cid:17)
E
x0p t|0
p
x (± x|xδ(cid:12) (cid:12)x )0 ≈exp(cid:18)
−
2δ σ2 2(cid:19)(cid:18)
1∓
(x−E
x0
σ[ 2µ t(x 0)])δ(cid:19)
(107)
t|0 0 t
(cid:18) δ2 (cid:19)(cid:18) φ(x,t)δ(cid:19)
=exp − 1∓ (cid:101) . (108)
2σ2 σ
t t
D.3.Illustrativeexample
As an illustrative example we choose a distribution which is tractable and perceivable. We model a two dimensional
distributionofpixels,whicharedistributedproportionallytothepixelvalueofanimageofacapital“E”.Thevisualization
ofthedatadistributionisgovernedbyits33×33pixelsandasinglesamplefromthedistributionisablackpixelindexed
byitslocation(x,y)onthe33×33pixelgrid. Thediffusiveforwardprocessactsuponthecoordinateanddiffuseswith
progressingtimetheblackpixelsintoatwodimensional(approximately)binomialdistributionattimet=1.
Weusetheidenticalarchitectureas(Campbelletal.,2022),usedfortheirillustrativeexample. Subsequently,thearchitecture
incorporatestworesidualblocks,eachcomprisingaMultilayerPerceptron(MLP)withasinglehiddenlayercharacterized
byadimensionalityof32, aresidualconnectionthatlinksbacktotheMLP’sinput, alayernormalizationmechanism,
andultimately,aFeature-wiseLinearModulation(FiLM)layer,whichismodulatedinaccordancetothetimeembedding.
Thearchitectureculminatesinaterminallinearlayer,deliveringanoutputdimensionalityof2. Thetimeembeddingis
accomplishedutilizingtheTransformer’ssinusoidalpositionembeddingtechnique,resultinginanembeddingofdimension
32. ThisembeddingissubsequentlyrefinedthroughanMLPfeaturingasinglehiddenlayerofdimension32andanoutput
dimensionalityof128. InordertogeneratetheFiLMparameterswithineachresidualblock,thetimeembeddingundergoes
processingviaalinearlayer,yieldinganoutputdimensionof2.
Wetestourproposedreverserateestimatorsbytrainingthemtoreconstructthedatadistributionattimet=0.Forevaluation,
wedraw500.000individualpixelsproportionallytotheapproximatedequilibriumdistributionandplottheirrespective
histogramsattimet=0inFigure2.
Fortraining,wesample1.000.000pixelvaluesproportionaltothegrayscalevalueofthe’E’imageservingasthetruedata
distribution. WeperformoptimizationwithAdamwithalearningrateof0.001andoptimizefor100.000timestepswitha
batchsizeof2.000.
D.4.MNIST
TheMNISTexperimentswereconductedwiththescaledEhrenfestprocess. TheMNISTdatasetconsistsof28×28gray
scaleimageswhichweresizedto32×32inordertobeprocessablebyuseourstandardDDPMarchitectureWeused
S =2562statestoensure256statesintherangeof[−1,1]withadifferencebetweenstatesofof √2 . Foroptimization,we
S
20Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
resortedtothedefaulthyperparametersofAdam(Kingma&Ba,2014)andusedanEMAof0.99withabatchsizeof128.
FortherateswechosethecontinuousDDPMscheduleproposedbySongandwestoppedthereverseprocessatt=0.01
duetovanishingdiffusionandresultinghighvarianceratesclosetothedatadistribution.
D.5.ImagemodelingwithCIFAR-10
WeemploythestandardDDPMarchitecturefromHoetal.(2020)andadapttheoutputlayertotwicethesizewhenrequired
bytheconditionalexpectationandtheGaussianpredictor. ThescoreandfirstorderTaylorapproximationsdidnotneedto
beadapted. Fortheratiocase,weadaptedthearchitecturebydoublingthefinalconvolutionallayertosixchannelssuchthat
thefirsthalf(threechannels)predictedthedeathrateandthesecondthreechannelspredictedthebirthrate. Forthetime
dependentrateλ wetriedthecosinescheduleof(Nichol&Dhariwal,2021)andthevariancepreservingSDEscheduleof
t
(Songetal.,2021). ThecosinescheduleensurestheexpectedvalueofthescaledEhrenfestprocesstoconvergestozerowith
E [x ]=cos(cid:0)π t(cid:1)2 x andtranslatestoatimedependentjumpprocessrateofλ = 1πtan(cid:0)π t(cid:1) ,whichisunbounded
x0 t 2 0 t 4 2
closetotheequilibriumdistributionandthereforehastobeclamped. Wechooseλ ∈[0,500]inourcase. Duetonumerical
t
considerationsregardingtheexplodingratesduetodiminishingdiffusionclosetot=0,werestrictedthereverseprocessto
timest∈[0.01,1]. Ingeneral,wecantransformanydeliberatelylongsamplingtimeT toT =1viathetimetransformation
ofthemasterequationinB.2.
Weusethestandardprocedurefortrainingimagegeneratingdiffusionmodels(Loshchilov&Hutter,2016). Inparticular,
weemployalinearlearningratewarmupfor5.000stepsandacosineannealingfrom0.0002to0.00001withtheAdam
optimizer. The batch size was chosen as 256 and an EMA with the factor 0.9999 was applied for the model used for
sampling. Forsamplingweranthereverseprocessfor1.000stepsandemployedτ-LeapingasshowcasedinCampbell
etal.(2022)witharesultingτ =0.001. Wealsoutilizedthepredictor-correctorsamplingmethodstartingatt=0.1tothe
minimumtimeoft=0.01. WhereasCampbelletal.(2022)reportedsignificantgainsperformingcorrectorsampling,we
observebehaviorclosetootherstate-continuousdiffusionmodelswhichonlyapplyfewornocorrectorstepsatall.
Figure6.SamplesfromthereversescaledEhrenfestprocessob- Figure7.SamplesfromthereversescaledEhrenfestprocessob-
tainedbyfinetuningtheDDPMarchitecturewithL (18). tainedbyfinetuningtheDDPMarchitecturewithL (18).
Taylor Taylor
21Bridgingdiscreteandcontinuousstatespace:ExploringtheEhrenfestprocessintime-continuousdiffusionmodels
Figure8.SamplesfromthereversescaledEhrenfestprocessob- Figure9.SamplesfromthereversescaledEhrenfestprocessob-
tainedbyfinetuningtheDDPMarchitecturewithL (31). tainedbyfinetuningtheDDPMarchitecturewithL (31).
OU OU
22