ǫ-POLICY GRADIENT FOR ONLINE PRICING
LUKASZ SZPRUCH1,2, TANUT TREETANTHIPLOET3,4, AND YUFEI ZHANG5
Abstract. Combining model-based and model-free reinforcement learning approaches,
this paper proposes and analyzes an ǫ-policy gradient algorithm for the online pricing
learning task. The algorithm extends ǫ-greedy algorithm by replacing greedy exploitation
with gradient descent step and facilitates learning via model inference. We optimize the
regretofthe proposedalgorithmbyquantifyingthe explorationcostintermsoftheexplo-
rationprobabilityǫandtheexploitationcostintermsofthegradientdescentoptimization
andgradientestimationerrors. Thealgorithmachievesanexpectedregretoforder (√T)
O
(up to a logarithmic factor) over T trials.
1. Problem formulation
Model-based and model-free learning are two prominent approaches in the field of rein-
forcement learning (RL), each with its own set of advantages and disadvantages. Model-
based are sample efficient, requiring fewer interactions with the environment thus obviating
theneedforcostlyMonteCarlorollouts[19]. Furthermore, amodelcanbeusedtogeneralize
acrossvariousbutrelatedtasks, enhancing itsapplicability indiverse contexts. Importantly
the use of a model enhances decisions interpretability and auditability. Despite these ben-
efits, model-based learning is not without drawbacks. Model misspecification can lead to
poor performance and the complexity and computational cost involved in developing and
maintaining an accurate model might be significant. Conversely, model-free approaches are
generally simpler and scale effectively, as they learn policies directly, often through the use
of efficient gradient based algorithms, bypassing the burden associated with environmental
modeling. However, model-freelearning require extensive interaction with theenvironment,
which can be both costly and time-consuming. Additionally, the lack of a model means
that when the objective or environment changes, one often need to start learning task from
scratch. In this paper we are interested in the following natural question:
Can one integrate model-based and model-free methods to provably achieve the benefits of
both?
1School of Mathematics, University of Edinburgh
2
Alan Turing Institute
3The Institute for Fundamental Study, Naresuan University
4Quantum Technology Foundation (Thailand)
5Department of Mathematics, Imperial College London, London, UK
E-mail addresses: L.Szpruch@ed.ac.uk, ttreetanthiploet@gmail.com,
yufei.zhang@imperial.ac.uk.
2020 Mathematics Subject Classification. 62J12, 68Q32,65Y20 .
Key words and phrases. Onlinepricing,parametricinference,policygradient,regretanalysis,paramet-
ric contextual bandit, generalized linear model.
1
4202
yaM
6
]GL.sc[
1v42630.5042:viXra2 ǫ-POLICY GRADIENT FOR ONLINE PRICING
In this paper, we answer this question affirmatively within the context of online pricing
problems by developing ǫ-policy gradient (ǫ-PG) algorithm which extends ǫ-greedy algo-
rithm by replacing greedy exploitation with gradient descent step and facilitates learning
via model inference.
Motivating example. To motivate our learning problem, let us consider the following
dynamic pricing problem commonly encountered by many online retailers. In this problem,
customersarrivesequentially topurchaseaproductofferedbytheagent, witheachcustomer
belonging to a specific segment. For each customer, the agent selects a price from the set
of all admissible prices and observes whether the offered price is accepted by the customer.
The customer’s response is random and follows from an unknown distribution depending on
the customer segment and the quoted price. If the customer accepts the offered price, the
agent receives a reward based on the customer segment and the offered price; otherwise,
the agent’s reward is zero. The agent’s objective is to learn a pricing policy that maps
customer segments to optimal quoted prices while simultaneously maximizing cumulative
reward.
This example highlights several common features in online pricing problems: (1) The
agent’s reward for each trial depends on the agent’s action, and the customer’s segment and
response. (2) The customer’s responses for a given quotation are unknown and constitute
the main source of randomness in the realized reward. (3) If the offered price is accepted by
a customer, the precise dependence of the realized reward on the offered price and customer
segment is typically a known function to the agent, as it depends on the associated cost of
delivering the sold product and the strategic plan of the company. These stylized features
are the basis of our mathematical formulation of a learning framework, which we describe
next.
Problem formulation. Let ( ,Σ )beameasurable spacerepresenting thefeaturespace,
X
X
let ( ,Σ ) be a measurable space representing the agent’s admissible actions, and let
A
A
( ,Σ ) be a measurable space consisting of all potential responses. At each time step
Y
tY N, a feature x is sampled from an unknown distribution µ ( ) and revealed
t
∈ ∈ X ∈ P X
to the agent. The agent chooses an action a based on the feature x and all historical
t t
∈ A
observations. The agent observes the corresponding response y to her action, which
t
∈ Y
follows from some unknown conditional distribution ν(dy x ,a ) ( ). The resulting
t t
instantaneous reward is given by r(x ,a ,y ) for a known | function∈ rP : Y R.
t t t
X × A × Y →
Precise knowledge of r, ν and µ available to the agent will be given in Section 2.
To measure the performance of the agent’s actions, we define the expected reward r¯ :
R for a given feature x and action a :
X ×A → ∈ X ∈ A
r¯(x,a) := r a,x,y ν dy x,a , (1.1)
|
ZY
(cid:0) (cid:1) (cid:0) (cid:1)
where we average the realized reward with respect to the randomness in the response
distribution. For each T N, we define the regret of the agent’s actions (a )T by:
∈ t t=1
T
Reg (a )T := supr¯(x ,a) r¯(x ,a ) , (1.2)
t t=1 t − t t
a∈A
(cid:0) (cid:1)
Xt=1 (cid:16) (cid:17)
which compares the expected reward of action a against the optimal reward sup r¯(x ,a)
t a∈A t
for the context x at each time point t N. The regret Reg( ) characterizes the cumulative
t
∈ ·ǫ-POLICY GRADIENT FOR ONLINE PRICING 3
loss from taking sub-optimal actions up to the T-th trial. Agent’s aim is to construct
actions whose regret grows sublinearly with respect to T.
Our work. Thispaper proposesandanalyzes anǫ-policygradient (ǫ-PG)algorithmforthe
above learning task by integrating model-based and model-free RL approaches. The central
component of the algorithm is a PG method, which updates the pricing policy φ :
X → A
using thegradient of theexpected reward (1.1) at thecurrent policy φ. The unknown policy
gradient r¯ is not evaluated using standard black-box Monte-Carlo gradient estimation
a
∇
techniques (e.g., REINFORCE method [24]), which often suffer from slow convergence and
hence areprohibitively costly for many pricing problems. Instead, our algorithmleverages a
model-based approach to enhance sample efficiency in the gradient evaluation and to avoid
cold-start issues.
Specifically, we assume the distribution ν(dy x,a) of the response variable follows a para-
|
metric form π θ⋆(dy x,a) with an unknown parameter θ⋆. After each trial t = 1,2,..., we
|
estimate θ⋆ by solving an empirical risk minimisation problem using historical observations.
The policy is then updated based on the estimated gradient r¯ , where r¯ denotes the
∇a θt θt
reward function (1.1) with the estimated response distribution π , using policy gradient
θt
with a learning rate η of the form
t
φ = φ +η ( r¯ )( ,φ ( )) t = 1,2,... .
t t−1 t ∇a θt
·
t−1
·
Note that for a given parameter θ , the gradient r¯ can be computed with no variance.
t ∇a θt
To ensure the asymptotic consistency of the gradient evaluation, an exploration strategy is
exercised with probability ǫ to explore the parameter space, while the exploration proba-
bility ǫ is reduced at a suitable rate as the learning proceeds.
We optimize the regret of the proposed ǫ-PG algorithm by quantifying the exploration
and exploitation costs. Our analysis accommodates a general parametric model π of ν and
θ
identifies theintricate interplay between the loss function in theempirical risk minimization
and the structure of π . This subsequently facilitates quantifying the exploration cost in
θ
terms of the exploration probability ǫ used in the ǫ-PG strategy. The exploitation cost
is quantified in terms of the optimization error resulting from gradient descent and the
gradient estimation error resulting from the inexact response probability. By optimizing
the exploration probability and learning rate in the policy gradient update, our algorithm
achieves an expected regret of order (√T) (up to a logarithmic factor) over T trials.
O
Our approaches and most related works. To the best of our knowledge, this is the
first theoretical work on regret bounds of a policy gradient algorithm for online pricing
problems with general feature and action spaces. In the sequel, we will compare the pro-
posed algorithm with existing algorithms for two closely related learning tasks: contextual
bandits and online optimizations.
Contextual bandits algorithms. Most contextual bandit algorithms iteratively estimate the
expected reward r¯(without using the structure (1.1)) based on realized rewards from previ-
ous trials and exercise the greedy policy which maximizes the estimated expected rewards
over all actions (see, e.g., [8, 1, 5, 12, 23, 14] and references therein). However, there are
three drawbacks that make these algorithms unsuitable for the considered online pricing
problems: (1) Computing the greedy policy at each trial can be expensive, especially for
large action space . (2) The algorithm’s performance relies on the efficiency of the estima-
A
tion oracle for the expected reward r¯. Such sample efficiency is typically quantified under4 ǫ-POLICY GRADIENT FOR ONLINE PRICING
the condition that r satisfies a linear model [1, 12], a generalized linear model [8, 14], or a
Gaussian process model [5, 23]. Unfortunately, these structural assumptions of r may not
hold for practical online pricing problems due to nonlinearity of r¯; see [6] for a concrete
example where generalized linear models fail to represent the expected reward due to the
lack of monotonicity of r¯. (3) The algorithm is not sample efficient with changing objec-
tives. Indeed, when the instantaneous reward r changes, e.g., due the change of the cost
for delivering the product, the expected reward r¯ changes as well. Consequently, one often
needs to restart the learning task from scratch.
Our work tackles the aforementioned drawbacks by utilizing the decomposition (1.1) of
the expected reward and directly learning the response distribution, a methodology more
aligned with industrial practices [4]. This approach avoids potential model misspecification
in reward estimation and allows for imposing explicit and interpretable models of the re-
sponse distribution, thereby facilitating the development of effective learning algorithms.
Additionally, itensures thealgorithmadaptsquickly tochangesintheinstantaneousreward
r, as we can utilize the previously learned response distribution to initialize the learning
task. Furthermore, we enhance the algorithm’s efficiency by replacing greedy exploitation
with policy gradient exploitation, which is more efficient, particularly for continuous action
spaces.
Online optimization algorithms. The proposed ǫ-PG algorithm can be viewed as an online
gradient descent approach for maximizing the unknown expected reward over an infinite-
dimensional policy space. Due to this infinite-dimensional nature, existing convergence
analysis for online gradient descent in finite-dimensional problems (see e.g. [13]) is not
suitable for the ǫ-PG algorithm. Furthermore, the ǫ-PG algorithm updates using biased
gradients derived from an inexact response distribution. This introduces technical chal-
lenges in the regret analysis of ǫ-PG. Unlike existing online gradient descent algorithms
that work with unbiased gradients [13] or biased gradients whose biases diminish at pre-
scribed rates [2], our setting requires agents to actively manage the magnitude of gradient
evaluation errors through strategic exploration. Consequently, optimizing the algorithm’s
regret involves a critical balance between exploration and exploitation.
Notation. Forameasurablespace , wedenoteby ( )thespaceofprobabilitymeasures
on . For each d N, we denote bX y I the d d iP denX tity matrix, by Sd, Sd and Sd the
X ∈ d × ≥0 +
space of d d symmetric, symmetric positive semidefinite, and symmetric positive definite
×
matrices, respectively, and by ρ (A) and ρ (A) the largest and smallest eigenvalues of
max min
A Sd. We equip Sd with the Loewner (partial) order such that for each A,B Sd, A B
if A∈ B Sd . For a given A Rd×d, we denote by A its spectral norm or∈ equivale(cid:23) ntly
− ∈ ≥0 ∈ k kop
the operator norm induced by Euclidean norms.
2. Main results
This section introduces the precise assumptions of the model coefficients, derives the
policy gradient algorithm and analyzes its regret. To facilitate the analysis, we assume
without loss of generality that all random variables are supported on a probability space
(Ω,F,P), which can be defined as the product space of probability spaces associated with
each observation.ǫ-POLICY GRADIENT FOR ONLINE PRICING 5
2.1. Estimation of response distribution. This section studies the estimation of the
response distribution, which is crucial for evaluating the policy gradient. We assume a
generic parametric model π θ⋆ for ν and identify the essential structure of π θ⋆ that facilitates
efficient learning of θ⋆ using an empirical risk minimization. Specifically, we impose the
following model assumption for ν.
H.1. There exists a known function π : Rd ( ) such as ν(dy x,a) = π θ⋆(dy x,a)
for some unknown θ⋆ Rd, and y r(x× ,aX ,y× )A is→ intP egrY able under the | measure π (dy| x,a)
θ
for all (θ,x,a) Rd ∈ . 7→ |
∈ ×X ×A
We further assume for simplicity that the agent estimates θ⋆ through an empirical risk
minimization. Specifically, after the T-th trials, given the observations (x ,a ,y )T , the
t t t t=1
agent estimates θ⋆ by solving an empirical risk minimisation problem:
T
1
θ := argmin ℓ(θ,x ,a ,y )+ θ 2 , (2.1)
T t t t Rd
2k k
θ∈Rd !
t=1
X
where ℓ : Rd R is a suitable loss function satisfying the following conditions.
×X ×A×Y →
H.2. ℓ is compatible with π in (H.1) in the sense that, there exists H : Sd ,
θ X × A → ≥0
called an information matrix, such that for all (x,a,y) ,
∈ X ×A×Y
(1) ℓ( ,x,a,y)is twice differentiableand satisfies 2ℓ(θ,x,a,y) H(x,a) forall θ Rd;
(2) Y· exp λ⊤ ∇θℓ(θ⋆,x,a,y) π θ⋆(dy |x,a)
≤
exp∇ λθ ⊤H(x,a)λ (cid:23) for all λ
∈
Rd, whe∈ re θ⋆
is the (unknown) true parameter in (H.1).
R (cid:0) (cid:1) (cid:0) (cid:1)
Assumption (H.2) highlights the essential properties of the loss function ℓ that allow for
quantifying the error θ θ⋆ in high probability. These conditions generalize the results
T
−
from the special case where ℓ represents the logarithm of π ’s density and the estimator
θ
(2.1) corresponds to the (regularized) maximum likelihood estimator. In this context, it
is known that the Hessian 2ℓ is the (observed) Fisher information, and the gradient
∇θ
ℓ(θ⋆, ) asymptotically Gaussian distributed with a variance being the Fisher information
θ
∇ ·
(see e.g., [18, Section 8.12]). Here we relax the conditions by assuming that the gradient
ℓ(θ⋆,x,a, )issub-Gaussiandistributed, andits tailbehavior canbequantified by alower
θ
∇ ·
bound H(x,a) of the Hessian matrix 2ℓ.
∇θ
Here we give a concrete example of loss functions satisfying (H.2) where π is given by a
θ
(feature-dependent) generalized linear model.
Example 2.1. Suppose that π : Rd ( ) in (H.1) is of the form
×X ×A → P Y
π (dy x,a) = g(x,y)exp h(x,y)⊤ψ(x,a)θ b x,ψ(x,a)θ ν(dy), (2.2)
θ
| −
whereν isagivenreferencemeasureon ,andg : R,h : Rm,ψ :
(cid:0) (cid:0) (cid:1)(cid:1)
Rm×d, and b : Rm R are given fuY nctions sucX h× thY at→ g(x,yX )e× xY p → h(x,y)⊤w X ν× (dA y)→ =
X× → Y
exp b(x,w) for all w Rm.
Assume that there ex∈ ists a subset Rm such that ψ(R x,a)θ⋆ fo(cid:0) r all (x,a)(cid:1) .
(cid:0) (cid:1) H ⊆ ∈ H ∈ X×A
Assume further that there exists a function ˜ b : Rm R and constants c ,c > 0
1 2
˜ X × → ˜
such that for all x , b(x,w) = b(x,w) for all w , w (b(x,w),b(x,w)) is twice
∈ X ∈ H 7→
differentiable, and 2˜ b(x,w) c I and 2b(x,w) c I for all w Rd. Then the loss
function ℓ : Rd ∇w (cid:23) R1 dm efined∇ byw (cid:22) 2 m ∈
×X ×A×Y →
2c
ℓ(θ,x,a,y) = 1 h(x,y)⊤ψ(x,a)θ ˜ b x,ψ(x,a)θ , (2.3)
− c −
2
(cid:16) (cid:17)
(cid:0) (cid:1)6 ǫ-POLICY GRADIENT FOR ONLINE PRICING
satisfies (H.2) with H(x,a) =
2c2
1ψ(x,a)⊤ψ(x,a).
c2
The proof of Example 2.1 is given in Section 4.1.
Remark 2.1. In the special case where h is linear in y, h, ψ, and b are independent of x, and
is a finite set, the model (2.2) aligns with the generalized linear model for multi-armed
A
bandits studied in [8]. This family contains commonly used statistical models for response
distributions as outlined in [4], including the Gaussian and Gamma distributions when the
reference measure ν is the Lebesgue measure, and the Poisson and Bernoulli distributions
when ν is the counting measure on the integers.
To facilitate the analysis, we assume the agent has access to the magnitude of the
H
kernel ψ(x,a)θ⋆, and construct a compatible log-likelihood loss function (2.3) by extending
b with an arbitrary strongly convex function outside the set . Note that to analyze
H
algorithm regrets, it’s common to assume some a-priori information on the true parameter.
For instance, [8, 20, 3, 11, 21] assumes the range of the unknown parameter and [9, 10]
assumes known upper/lower bounds for holding times of continuous-time Markov chains.
Assumption(H.2)alsoincludesnonlinearleast-squareslossfunctionsforsuitablebounded
response variables, as shown in the following example. Given that, in most online pricing
problems, the response variable models whether a given quotation is accepted by the cus-
tomer as discussed in Section 1, we simplify our presentation by considering real-valued
response variables. Similar results can be naturally extended to multivariate response vari-
ables.
Example 2.2. Suppose that [y,y] for some < y < y < , and π : Rd
Y ⊂ −∞ ∞ ×X ×A →
( ) in (H.1) has first moments, i.e., µ(θ,x,a) := yπ (dy x,a) is well-defined for all
P Y Y θ |
(θ,x,a) Rd .
Assum∈ etha× tµX is× twA icedifferentiableinθ, andthereeR xistsH : Rd andconstants
X×A → ≥0
c ,c > 0suchthatc 1/(y y),( 2µ)(θ,x,a) H(x,a) c ( µ)(θ,x,a)( µ)(θ,x,a)⊤,
1 2 1 ≤ − ∇θ (cid:22) (cid:22) 1 ∇θ ∇θ
and( µ)(θ⋆,x,a)( µ)(θ⋆,x,a)⊤ c H(x,a). Thenthelossfunctionℓ : Rd
θ θ 2
R defi∇ ned by ∇ (cid:22) ×X×A×Y →
C 8 1
ℓ(θ,x,a,y) = 1 (y µ(θ,x,a))2, with C := +y y (2.4)
2 − 1 c (y y)2 c −
2 − (cid:18) 1 (cid:19)
satisfies (H.2) with H(x,a) = C 1 +y y H(x,a).
1 c1 −
(cid:16) (cid:17)
The proof of Example 2.2 is given in Section 4.2.
Remark 2.2. Example 2.1 includes the (nonlinear) least-squares estimators proposed in
[8, 12] for (generalized) linear models as special cases. In these settings, µ(θ,x,a) =
b(ψ(x,a)⊤θ), where ψ : Rd isagiven kernel functionandb : R Risa sufficiently
X×A → →
regular link function. Then ( µ)(θ,x,a)( µ)(θ,x,a)⊤ = (b′(ψ(x,a)⊤θ))2ψ(x,a)ψ(x,a)⊤
θ θ
∇ ∇
and ( 2µ)(θ,x,a) = b′′(ψ(x,a)⊤θ)ψ(x,a)ψ(x,a)⊤. Hence the desired constants c and c
∇θ 1 2
exist if min x∈R b′(x) > 0, max x∈R b′(x) < , and max x∈R b′′(x) is sufficiently small,
| | | | ∞ | |
which holds in particular if b is a non-constant affine function. The information matrix H
is the kernel’s covariance ψψ⊤ (scaled by an appropriate positive constant) as observed in
[8, 12].
Now we present the main theorem of this section, which quantifies the accuracy of the
estimator θ defined in (2.1) using the information matrix H in (H.2).
Tǫ-POLICY GRADIENT FOR ONLINE PRICING 7
Theorem 2.1. Suppose (H.1) and (H.2) hold. For all T N and all δ > 0,
∈
P θ θ⋆ 2 8ρ (V )−1 ln detV +2ln 1 + θ⋆ 2 1 δ.
T − Rd ≤ min T T δ VT−1 ≥ −
where V (cid:16) :=(cid:13) T H(cid:13)(x ,a )+2I . (cid:16) (cid:0) (cid:1) (cid:0) (cid:1) (cid:13) (cid:13) (cid:17)(cid:17)
T (cid:13) t=1 (cid:13) t t d (cid:13) (cid:13)
TheproofofTheorem 2.1isgiven inSection3.1. Theorem 2.1indicates thattheaccuracy
P
of θ can be measured by the minimum eigenvalue of V . This suggests us to design
T T
a learning algorithm such that ρ (V ) blows up to infinity at an appropriate rate as
min T
T .
→ ∞
2.2. ǫ-policy gradient algorithm and its regret. The section introduces an ǫ-policy
gradient algorithm that explores the environment with probability ǫ and exploits using a
gradient descent update. To this end, we assume that there exists an exploration policy
associated to the loss (2.1).
H.3. Let H : Sd be given in (H.2). There exists a known Markov kernel π :
X ×A → ≥0 Exp
( ) and a known constant ρ > 0 such that H(x,a)π (da x)µ(dx) ρ I .
X → P A H X×A Exp | (cid:23) H d
Moreover, there exists a known constant C 0 such that sup H(x,a) C .
H ≥ R (x,a)∈X×Ak kop ≤ H
Assumption(H.3)ensuresthattheagenthasaccesstoapolicyπ suchthatobservations
Exp
generated by π guarantees to explore the parameter space in expectation. This condition
Exp
is commonly imposed in the literature to facilitate learning [8, 1, 5, 23, 14]. It typically
holds where the action space is sufficiently rich for exploring the parameter space. We
assume for simplicity that H is bounded to quantify the behavior of (x,a) H(x,a) under
7→
the measure π (da x)µ(dx) in high probability.
Exp
|
The following proposition provides sufficient conditions for (H.3) if π is given by the
θ
generalized linear model in Example 2.1. It extends the conditions in [8] for bandits with
finite action spaces to the present setting with a general action space and an additional
A
feature space . The proof is given in Section 4.1.
X
Proposition 2.2. Let π be given as in Example 2.1. Assume that and are topological
θ
X A
spaces with the associated Borel σ-algebras, and µ ( ) has support supp(µ).1 Assume
that ψ : supp(µ) Rm×d is continuous an∈ d P bouX nded, and span( ψ(x,a)⊤ x
supp(µ),a
)× =ARd→
. Then (H.3) holds with π (da x) = η(da),
f{
or any
me| asur∈
e
Exp
∈ A} |
η ( ) with supp(η) = .
∈ P A A
For the least-squares loss in Example 2.2, it is easy to see that if the mean of the response
variable admits a nonlinear kernel representation as discussed in Remark 2.2, then the same
policy π given in Proposition 2.2 is also an exploratory policy for the least-squares loss
Exp
function (2.4). This is because in this case, the corresponding information matrix H can
also be selected as the (scaled) kernel’s covariance, akin to that for the log-likelilhood
loss function (2.3) in Example 2.1. The construction of an exploration policy for general
nonlinear models in Example 2.2 is more technically involved and is left for future work.
Wethenintroducestructuralpropertiesoftheexpectedrewardforthedesignandanalysis
of the gradient descent updates. Recall that under (H.1), the expected reward r¯ (x,a) =
θ
r(x,a,y)π (dy x,a) is well-defined for all (θ,x,a) Rd , and θ⋆ is the (unknown)
Y θ | ∈ ×X ×A
true parameter in (H.1).
R
1Given a topological space X equipped with its Borel σ-algebra, the support supp(µ) of a measure µ,
if it exists, is a closed set satisfying (1) µ(supp(µ)c) = 0, and (2) if G is open and G supp(µ) = , then
∩ 6 ∅
µ(G supp(µ))>0.
∩8 ǫ-POLICY GRADIENT FOR ONLINE PRICING
H.4. is a Hilbert space equipped with the norm A. For all x , a r¯ θ⋆(x,a) is
A k· k ∈ X 7→
Fr´echet differentiable, sup (x,a)∈X×A|r¯ θ⋆(x,a)
|
< ∞, and there exist L
a
≥
γ
a
> 0 such that
for all x and a,a′ , ( ar¯ θ⋆)(x,a′) ( ar¯ θ⋆)(x,a)
A
L
a
a′ a
A
and
∈ X ∈ A k ∇ − ∇ k ≤ k − k
1
supr¯ θ⋆(x,a) −r¯ θ⋆(x,a)
≤ 2γ
k( ∇ar¯ θ⋆)(x,a) k2 A, (2.5)
a∈A a
and
L := sup
k( ∇ar¯ θ)(x,a) −( ∇ar¯ θ⋆)(x,a)
kA < . (2.6)
θ θ6=θ⋆,x∈X,a∈A θ θ⋆ Rd ∞
k − k
The agent knows the constant L .
a
Assumption (H.4) assumes for simplicity that the action space is a Hilbert space, and
the expected reward is Fr´echet differentiable in action. This allows for developing a policy
gradient updatebasedontheFr´echet derivative oftherewardintheaction. Similar analysis
can be performed if is a convex subset of a Hilbert space. In that case, the gradient
A
ascent update can be replaced by a projected gradient ascent to incorporate the action
constraints. Condition (2.5) is commonly known as the Polyak-L ojasiewicz condition [15],
and guarantees linear convergence of gradient ascent using exact gradients. It is strictly
weaker than the strong concavity condition and is satisfied by many practically important
nonconvex/nonconcave optimization problems, such linear neural networks with suitable
initializations [16], nonlinear neural networks in the so-called neural tangent kernel regime
[17]. Condition (2.6) asserts the Lipschitz continuity of the reward’s gradient with respect
to theresponse distribution, which allows forquantifying theregret inthe exploitationstep.
Now we are ready to present the ǫ-PG algorithm.
Algorithm 1: ǫ-policy gradient (ǫ-PG) algorithm
Input: Exploration policy π Exp : ( ), exploration rates (ǫ t) t∈N [0,1], initial
X → P A ⊂
policy φ 0 : and learning rates (η t) t∈N [0, ).
X → A ⊂ ∞
1 for t = 1,2,... do
2 Observe a feature x .
t
3 Sample ξ from the uniform distribution on (0,1).
t
4 if ξ < ǫ then
t t
5 Execute a sampled from π ( x ).
t Exp t
·|
6 else
7 Execute a = φ (x ),
t t−1 t
8 end
9 Observe a response y , and update θ by (2.1).
t t
10 Update the policy φ = φ +η ( r¯ )( ,φ ( )).
t t−1 t ∇a θt
·
t−1
·
11 end
The following theorem refines the estimate in Theorem 2.1, and quantifies the accuracy
of (θ T) T∈N in Algorithm 1 in terms of the exploration rates (ǫ t) t∈N.
Theorem 2.3. Suppose (H.1), (H.2) and (H.3) hold. Consider Algorithm 1 with explo-
ration rates (ǫ t) t∈N [0,1). Then for all δ (0,3/π2], if
⊂ ∈
T 2dT2 2dT2
M(ǫ,T,δ) = 2+ρ ǫ 2C ln + 2T ln > 0, T N, (2.7)
H t H
− δ δ ∀ ∈
r !
t=1
Xǫ-POLICY GRADIENT FOR ONLINE PRICING 9
then with probability at least 1
π2
δ that
− 3
8dln 2+C 2T +6ln 2dT2 +16ln 2T2 +2 θ⋆ 2
θ θ⋆ 2 H δ δ Rd , T N. (2.8)
T − Rd ≤ (cid:16) (cid:16) M(ǫ,T(cid:17),(cid:17)δ) (cid:16) (cid:17) (cid:13) (cid:13) ∀ ∈
(cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13)The pro(cid:13)of of Theorem 2.3 is given in Section 3.2.
The following theorem optimizes the learning rates (η t) t∈N and the exploration rates
(ǫ t) t∈N so that Algorithm 1 achieves a regret of the order (√T) (up to a logarithmic term)
O
in expectation. This regret bound matches the lower bound for online convex optimization
algorithms[13]and(instant-independent) regretboundsforparametricmulti-armedbandits
[8].
Theorem 2.4. Suppose (H.1), (H.2), (H.3) and (H.4) hold. If one sets η η (0,1/L ]
T a
≡ ∈
for all T N, and c √T ln(dT) T ǫ c √T ln(dT) for all T N, with constants
∈ 1 ≤ t=1 t ≤ 2 ∈
c c 30C /ρ . Then there exists a constant C 0 such that the regret of Algorithm
2 1 H H
1 s≥ atisfi≥ es E Reg (a )T Cd√TP ln(dT) for all T≥ 2.
t t=1 ≤ ≥
The proof(cid:2)of Th(cid:0)eorem 2(cid:1).(cid:3)4 is given in Section 3.3. The precise expression of the constant
C can be found in the proof. It depends only on the learning rate η, the constant c , the
2
sup-norm of the reward r, the Euclidean norm of θ⋆, and the constants L , C and γ in
θ H a
(H.3) and (H.4).
3. Proofs of main results
3.1. Proof of Theorem 2.1. The following lemma bounds the gradient of the loss ℓ using
the information matrix H, which will be used in the proof of Theorem 2.1.
Lemma 3.1. Suppose (H.1) and (H.2) hold. For each T N 0 , define the random
∈ ∪ { }
variables S := T ℓ(θ⋆,x ,a ,y ) and V := T H(x ,a )+2I . Then for all T N
T t=1∇θ t t t T t=1 t t d ∈
and δ > 0,
P P
P S 2 ln(detV )+2ln 1 1 δ.
T VT−1 ≤ T δ ≥ −
(cid:16) (cid:17)
where v 2 := v⊤Av for a(cid:13) ll v(cid:13) Rd and A Sd . (cid:0) (cid:1)
k kA (cid:13) (cid:13) ∈ ∈ ≥0
Proof. The proof adapts the argument presented in [20, Lemma 4.5] to the current context,
andweprovidethedetailshereforthereader’sconvenience. Wefirstrewriteexp 1 S 2
2k T kVT−1
asanintegralofsomesuper-martingales. ForforeachA Sd,letc(V) := (2π)d/2(cid:16)(detA)−1/2(cid:17)=
∈ +
exp( λ 2)dλ be the normalising constant corresponding to the density of the normal
Rd −k kA
distribution (0,A−1). By completing the square,
R N
1
exp 1 S 2 = exp 1 S 2 1 λ V−1S 2 dλ
2k T kVT−1 c(V T)
Rd
2k T kVT−1 − 2 − T T VT
Z (cid:16) (cid:17)
(cid:0)1 (cid:1) 1 (cid:13) (cid:13)
= exp λ⊤S 1 λ 2 dλ = (cid:13) exp λ⊤S(cid:13) 1 λ 2 1 λ 2 dλ
c(V ) T − 2k kVT c(V ) T − 2k kVT−Id − 2k kId
T Rd T Rd
Z (cid:16) (cid:17) Z (cid:16) (cid:17)
1/2
1 detV 1
= Mλexp 1 λ 2 dλ = T Mλexp 1 λ 2 dλ,
c(V ) T − 2k kId detI c(I ) T − 2k kId
T ZRd (cid:18) d (cid:19) d ZRd
(cid:16) (cid:17)
(cid:0) (cid:1)10 ǫ-POLICY GRADIENT FOR ONLINE PRICING
where for each λ Rd, Mλ is defined by
∈ T
T
Mλ = exp λ⊤S λ⊤(V I )λ = exp λ⊤ ℓ(θ⋆,x ,a ,y ) λ⊤H(x ,a )λ λ 2 .
T T − T − d ∇θ t t t − t t −k kRd
!
(cid:0) (cid:1)
Xt=1 (cid:16) (cid:17)
ForeachT N 0 , definetheσ-algebra := σ (x ,a ,y )n ,x ,a ). Clearlyforall
λ Rd, (M∈ λ)∞∪{ is} adapted to the filtratioF nT ( )∞{ , t andt byt t (= H1 .2(T 2+ )1 ), ET [+ M1 λ ] Mλ
∈ t t=0 Ft t=0 T+1 | FT ≤ T
for all T N 0 , which implies that E[Mλ] E[Mλ] 1 for all T N 0 .
∈ ∪{ } T ≤ 0 ≤ ∈ ∪{ }
Therefore, by Markov’s inequality and Fubini’s theorem,
1/2
1 detV
P S 2 > ln detVT +2ln 1 = P exp 1 S 2 > T
T VT−1 detId δ
(cid:18)
2 T VT−1 δ (cid:18)detI
d (cid:19) (cid:19)
(cid:16) (cid:16) (cid:17) (cid:17) (cid:16) (cid:17)
(cid:13) (cid:13) 1 (cid:0) (cid:1) 1 (cid:13) (cid:13) 1
=(cid:13) P(cid:13) Mλexp 1 λ 2 dλ > (cid:13) δE(cid:13) Mλexp 1 λ 2 dλ
c(I ) T − 2k kId δ ≤ c(I ) T − 2k kId
(cid:18) d ZRd
(cid:16) (cid:17)
(cid:19) (cid:20) d ZRd
(cid:16) (cid:17)
(cid:21)
δ δ
= E Mλ exp 1 λ 2 dλ exp 1 λ 2 = δ,
c(I ) T − 2k kId ≤ c(I ) − 2k kId
d Rd d Rd
Z (cid:16) (cid:17) Z (cid:16) (cid:17)
(cid:2) (cid:3)
where the last inequality used from the fact that E[Mλ] 1 for all λ Rd and T N.
This along with detI = 1 proves the desired estimate.T ≤ ∈ ∈ (cid:3)
d
Proof of Theorem 2.1. Fix T N. For each τ [0,1], define the random variables θτ =
∈ ∈
θ⋆ +(1 τ)(θ θ⋆) and
T
− −
T
F(τ) := (θ⋆ θ )⊤ ℓ(θτ,x ,a ,y )+θτ .
T θ t t t
− ∇
!
t=1
X
Note that by (2.1), T ℓ(θ ,x ,a ,y ) + θ = 0 and hence F(0) = 0. As ℓ is twice
t=1∇θ T t t t T
differentiable in θ, by the mean value theorem, there exists τ (0,1) such that
P ∈
F(1) = F(0)+F′(τ) = F′(τ). (3.1)
Define S = T ℓ(θ⋆,x ,a ,y )) as in Lemma 3.1. Then
T t=1∇θ t t t
P
F(1) 2 = (θ⋆ θ )⊤(S +θ⋆)(S +θ⋆)⊤(θ⋆ θ )
T T T T
| | − −
= tr (θ⋆ θ )(θ⋆ θ )⊤(S +θ⋆)(S +θ⋆)⊤
T T T T
− −
= tr(cid:0)V1/2 (θ⋆ θ )(θ⋆ θ )⊤V1/2 V−1/2 (S +(cid:1)θ⋆)(S +θ⋆)⊤V−1/2
T − T − T T T T T T (3.2)
(cid:16) (cid:17)
tr V1/2 (θ⋆ θ )(θ⋆ θ )⊤V1/2 tr V−1/2 (S +θ⋆)(S +θ⋆)⊤V−1/2
≤ T − T − T T T T T T
= V(cid:16)1/2 (θ⋆ θ ) 2 S +θ⋆ 2 ,(cid:17) (cid:16) (cid:17)
T − T Rd T VT−1
(cid:13) (cid:13) (cid:13) (cid:13)
where the sec(cid:13) ond to last ine(cid:13) qua(cid:13) lity used(cid:13) the fact that tr(AB) tr(A)tr(B) for A,B Sd .
≤ ∈ ≥0ǫ-POLICY GRADIENT FOR ONLINE PRICING 11
By the chain rule and (H.2(1)),
T
F′(τ) = (θ⋆ θ )⊤ 2ℓ θτ,x ,a ,y +I (θ⋆ θ )
| | − T ∇θ t t t d − T
!
t=1
X (cid:0) (cid:1)
(θ⋆ θ )⊤(V I )(θ⋆ θ ) (3.3)
T T d T
≥ − − −
= (θ⋆ θ )⊤V1/2 V−1/2 (V I )V−1/2 V1/2 (θ⋆ θ )
− T T T T − d T T − T
V1/2 (θ⋆ θ ) 2 ρ (V−1/2 (V I )V−1/2 .
≥ k T − T kRd min T T − d T
Combining (3.1), (3.2) and (3.3) yields (cid:1)
V1/2 (θ⋆ θ ) 2 ρ (V−1/2 (V I )V−1/2 −2 S +θ⋆ 2 ,
T − T Rd ≤ min T T − d T T VT−1
(cid:13) (cid:13) (cid:1) (cid:13) (cid:13)
which implies(cid:13)that (cid:13) (cid:13) (cid:13)
θ⋆ −θ
T
2
Rd
≤
ρ min(V T)−1ρ min(V T−1/2 (V
T
−I d)V T−1/2 −2 kS
T VT−1
+ kθ⋆ kVT−1)2. (3.4)
(cid:13) (cid:13) (cid:1) (cid:0) (cid:13)
W(cid:13)e claim t(cid:13)hat ρ (V−1/2 (V I )V−1/2 1/2. To see it, l(cid:13)et H = T H(x ,a )
min T T − d T ≥ T t=1 t t ∈
Sd . Then V = H +2I , and
≥0 T T d (cid:1) P
x⊤V−1/2 (V I )V−1/2 x y⊤(V I )y
ρ (V−1/2 (V I )V−1/2 = min T T − d T = min T − d
min T T − d T x6=0 kx k2 Rd y6=0 kV T1/2 y k2 Rd
(cid:1)
y⊤H y +y⊤y y⊤y 1
T
= min = 1 min ,
y6=0 y⊤H Ty +2y⊤y − y6=0 y⊤H Ty +2y⊤y ≥ 2
as y⊤H y 0. Hence by (3.4),
T
≥
θ⋆ θ 2 8ρ (V )−1( S 2 + θ⋆ 2 ),
−
T Rd
≤
min T T VT−1 VT−1
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:3)
which along with Lemma 3.1 yields the desired result.
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
3.2. Proof of Theorem 2.3. The following lemma establishes a concentration inequality
of the information matrix (H(x t,a t)) t∈N, which will be used in the proof of Theorem 2.3.
The proof of the lemma is given in Section 4.3.
Lemma 3.2. Suppose (H.1) and (H.3) hold. Forall n N 0 , let = σ (x ,a ,y )n
and let H = H(x ,a ). Then for all n N and δ (∈ 0,1]∪ , { } Fn { k k k k=1}
n n n
∈ ∈
n 2
2C d d d
P H E[H ] H ln + ln +18nln 1 δ.
k k k−1
 (cid:13) − |F (cid:13) ≤ 3  δ s δ δ ≥ −
(cid:13)Xk=1(cid:16) (cid:17)(cid:13)op (cid:18) (cid:19)
(cid:13) (cid:13)
  
(cid:13) (cid:13)
Proof of(cid:13)Theorem 2.3. Fix δ ((cid:13)0,1]. For each T N, let V := T H(x ,a )+2I . By
Theorem 2.1 (with δ′ = δ/2) a∈ ndLemma 3.2 (with∈ δ′ = δ/2),T for all Tt=1 N, wt itht probad bility
P ∈
at least 1 δ,
−
θ θ⋆ 2 8ρ (V )−1 ln detV +2ln 2 + θ⋆ 2 , (3.5)
T − Rd ≤ min T T δ VT−1
(cid:16) (cid:17)
(cid:13) (cid:13) (cid:0) (cid:1) (cid:0) (cid:1) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)12 ǫ-POLICY GRADIENT FOR ONLINE PRICING
and
T 2
2C 2d d 2d
H(x ,a ) E[H(x ,a ) ] H ln + ln +18T ln .
t t t t t−1
(cid:13) − |F (cid:13) ≤ 3  δ s δ δ 
(cid:13)Xt=1 (cid:16) (cid:17)(cid:13)op (cid:18) (cid:19)
(cid:13) (cid:13)  (3.6)
(cid:13) (cid:13)
whe(cid:13)re = σ (x ,a ,y )t . In the se(cid:13)quel, we carry out the estimate conditioned on the
Ft { k k k k=1}
above event. Note by (H.3) and the exploration step in Algorithm 1,
T T
ρ ǫ I E[H(x ,a ) ] TC I . (3.7)
H t d t t t−1 H d
(cid:22) |F (cid:22)
t=1 t=1
X X
Combining (3.6) with the upper bound in (3.7) yields that
2
2C 2d d 2d
H
V 2+TC + ln + ln +18T ln .
k T kop ≤ H 3  δ s δ δ 
(cid:18) (cid:19)
 
This along with the fact that detA A d for all A Sd yields
≤ k kop ∈ ≥0
ln detV
T
(cid:0) (cid:1) 2C 2d d 2 2d
H
dln 2+TC + ln + ln +18T ln
H
≤  3  δ s δ δ  (3.8)
(cid:18) (cid:19)
  
4 2d 2d 2d
dln 2+C T + ln +2 2T ln dln 2+C 2T +6ln .
H H
≤ 3 δ δ ≤ δ
r !! (cid:18) (cid:18) (cid:19)(cid:19)
Similarly, combining (3.6) with the lower bound in (3.7) yields
T 2
2C 2d d 2d
H
ρ (V ) 2+ρ ǫ ln + ln +18T ln
min T H t
≥ − 3  δ s δ δ 
t=1 (cid:18) (cid:19)
X (3.9)
T  
2d 2d
2+ρ ǫ 2C ln + 2T ln .
H t H
≥ − δ δ
r !
t=1
X
Combining (3.5), (3.8) and (3.9) implies that for all δ (0,1) and T N, if
∈ ∈
T
2d 2d
M˜(ǫ,T,δ′) = 2+ρ ǫ 2C ln + 2T ln > 0, (3.10)
H t − H δ′ δ′
r !
t=1
X
then with probability at least 1 δ,
−
8dln 2+C 2T +6ln 2d +16ln 2 +2 θ⋆ 2
θ θ⋆ 2 H δ′ δ′ Rd . (3.11)
T − Rd ≤ M˜(ǫ,T,δ′)
(cid:0) (cid:0) (cid:1)(cid:1) (cid:0) (cid:1) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
Now supp(cid:13)ose that(cid:13)
T 2dT2 2dT2
M(ǫ,T,δ) = 2+ρ ǫ 2C ln + 2T ln > 0, T N, (3.12)
H t H
− δ δ ∀ ∈
r !
t=1
Xǫ-POLICY GRADIENT FOR ONLINE PRICING 13
thenapplying (3.10)withδ′ = δ/T2, itholdswithprobabilityatleast1 δ ∞ 1 = 1 π2 δ
that for all T N, − t=1 T2 − 3
∈ P
8dln 2+C 2T +6ln 2dT2 +16ln 2T2 +2 θ⋆ 2
θ θ⋆ 2 H δ δ Rd . (3.13)
T − Rd ≤ (cid:16) (cid:16) M(ǫ,T(cid:17),(cid:17)δ) (cid:16) (cid:17) (cid:13) (cid:13)
(cid:13) (cid:13)
(cid:13) (cid:13)
This p(cid:13)roves the(cid:13)desired estimate. (cid:3)
3.3. Proof of Theorem 2.4. We start by quantifying how errors in gradient evaluation
propagate through the policy gradient iterates. The proof of Lemma 3.3 is given in Section
4.4.
Lemma 3.3. Suppose (H.4) holds. Consider Algorithm 1 with learning rates η η
t
(0,1/L ] for all t N. Then for all T N, ≡ ∈
a
∈ ∈
supr¯ θ⋆(x,a) r¯ θ⋆(x,φ T(x))
−
a∈A
ηL2 T
2 sup r¯ θ⋆(x,a) (1 γ aη)T + θ (1 γ aη)T−t θ
t
θ⋆ 2 Rd.
≤ | | − 2 − k − k
(x,a)∈X×A !
t=1
X
Proof of Theorem 2.4. Throughout this proof, let C 0 be an absolute constant, which
≥
may take a different value at each occurrence. Note that if
T 2π2dT3 2π2dT3
M(ǫ,T) := 2+ρ ǫ 2C ln + 2T ln > 0, T N, (3.14)
H t H
− 3 3 ∀ ∈
r !
t=1
X
then for any given T N, by Theorem 2.3 with δ = 3/(π2T) (0,3/π2], there exists an
event A such that
P(∈
A ) 1 1/T and
∈
T T
≥ −
8dln 2+C 2T +6ln 2π2dT3 +16ln 2π2T3 +2 θ⋆ 2
θ θ⋆ 2 H 3 3 Rd , T N.
T − Rd ≤ (cid:16) (cid:16) M(ǫ,(cid:17)T(cid:17)) (cid:16) (cid:17) (cid:13) (cid:13) ∀ ∈
(cid:13) (cid:13)
(cid:13) (cid:13)
Su(cid:13) ppose th(cid:13) at for all T N, T ǫ C √T ln(2π2dT3/3) with C 5C /ρ , which can
∈ t=1 t ≥ 1 1 ≥ H H
be ensured by setting T ǫ 30C /ρ √T ln(dT) for all T 1. Then for all T 1,
t=1 Pt ≥ H H ≥ ≥
P
2π2dT3 2 2√2
M(ǫ,T) 2+C √T ln 5 CC √T ln(2dT),
H H
≥ 3 − √T − ln(2π2dT3/3)! ≥
(cid:18) (cid:19)
p
and hence on the event A ,
T
dln(2+C (2T +6ln2dT))+ln(2T)+ θ⋆ 2
θ θ⋆ 2 C H Rd
T − Rd ≤ C √T ln(2dT)
(cid:13) (cid:13) H (cid:13) (cid:13) (cid:13) (cid:13) (3.15)
(cid:13) (cid:13) d+lnC + θ⋆ 2
C H Rd , T N.
≤ C √T ∀ ∈
H (cid:13) (cid:13)
(cid:13) (cid:13)14 ǫ-POLICY GRADIENT FOR ONLINE PRICING
Nowlet∆
t
= sup a∈Ar¯ θ⋆(x t,a) −r¯ θ⋆(x t,a t)forallt
∈
N,andletC
r
= sup (x,a)∈X×A|r¯ θ⋆(x,a)
|
<
(see (H.4)). By the definitions of the regret (1.2) and Algorithm 1, for all T 1,
∞ ≥
T
E Reg (a )T = E[∆ ]
t t=1 t
t=1
(cid:2) (cid:0) (cid:1)(cid:3) X
T T T
≤
E[∆ t1 ξt<ǫt]+ E[∆ t1 ξt≥ǫt1 Ac t]+ E[∆ t1 ξt≥ǫt1 At]
t=1 t=1 t=1
X X X (3.16)
T T T
2C P(ξ < ǫ )+ 2C P(Ac)+ E[∆ 1 1 ]
≤ r t t r t t ξt≥ǫt At
t=1 t=1 t=1
X X X
T T T
1
2C ǫ +2C + E[∆ 1 1 ].
≤
r t r
t
t At ξt≥ǫt
t=1 t=1 t=1
X X X
For all t 1, on the event A and ξ ǫ , by Lemma 3.3 and (3.15),
t t t
≥ { ≥ }
∆ t1 At−11
ξt≥ǫt
= supr¯ θ⋆(x t,a) −r¯ θ⋆(x t,φ t−1(x t))1
At
a∈A
ηL2 t−1
(1 γ η)t−1 2C + θ (1 γ η)−s θ θ⋆ 2 1
≤ −
a r
2 −
a
k
s
−
kRd At
!
s=1
X
ηL2 t−1 C¯
(1 γ η)t−1 2C + θ (1 γ η)−sC ,
a r a
≤ − 2 − √s
!
s=1
X
where C¯ = (d+lnC + θ⋆ 2 )/C . This along with (3.16) shows that for all T 2,
H Rd H
≥
E Reg (a )T (cid:13) (cid:13)
t t=1 (cid:13) (cid:13)
T t−1
(cid:2) (cid:0) (cid:1)(cid:3) 1
CC c √T ln(dT)+lnT + +CηL2C¯ (1 γ η)t−1−ss−1/2
≤ r 2 γ η θ − a
(cid:18) a (cid:19) t=1 s=1
XX
T−1 T
1
= CC c √T ln(dT)+lnT + +CηL2C¯ (1 γ η)t−1−ss−1/2
r 2
(cid:18)
γ aη
(cid:19)
θ
s=1 t=s+1
− a (3.17)
X X
1
L2C¯ T−1
CC c √T ln(dT)+lnT + +C θ s−1/2
r 2
≤ γ η γ
(cid:18) a (cid:19) a s=1
X
C c L2(1+lnC + θ⋆ 2 )
Cdmax r 2 , θ H Rd √T ln(dT).
≤ γ η C γ
a H a (cid:13) (cid:13) !
(cid:13) (cid:13)
(cid:3)
This proves the desired regret bound.
4. Proofs of technical results
4.1. Proofs of Example 2.1 and Proposition 2.2.
Proof of Example 2.1. Astraightforwardcomputationusing thedefinitionofℓandH shows
that 2ℓ(θ,x,a,y) = 2c1ψ(x,a)⊤ 2˜ b(x,ψ(x,a))ψ(x,a) H(x,a). This verifies (H.2(1)).
∇θ c2 ∇w (cid:23)ǫ-POLICY GRADIENT FOR ONLINE PRICING 15
To prove (H.2(2)), recall that since for all (x,a) , ψ(x,a)θ⋆ and ˜ b(x,w) =
∈ X ×A ∈ H
b(x,w) for all w . Hence for all (x,a,y) ,
∈ H ∈ X ×A×Y
2c
ℓ(θ⋆,x,a,y) = 1 ψ(x,a)⊤h(x,y)+ψ(x,a)⊤ ˜ b(x,ψ(x,a)θ⋆)
θ w
∇ c − ∇
2
(cid:16) (cid:17)
2c
= 1 ψ(x,a)⊤h(x,y)+ψ(x,a)⊤ b(x,ψ(x,a)θ⋆) .
w
c − ∇
2
Hence for all λ Rd, (cid:0) (cid:1)
∈
exp λ⊤ θℓ(θ⋆,x,a,y) π θ⋆(dy x,a)
∇ |
ZY
(cid:0) (cid:1) 2c
= g(x,y)exp h(x,y)⊤ψ(x,a)(θ⋆ λ 1 ) ν(dy)
− c
ZY (cid:18) 2 (cid:19)
2c
exp b x,ψ(x,a)θ⋆ + 1 b x,ψ(x,a)θ⋆ ⊤ ψ(x,a)λ
w
× − c ∇
(cid:18) 2 (cid:19)
(cid:0) (cid:1)2c (cid:0) (cid:1) 2c
= exp b(x,ψ(x,a)(θ⋆ λ 1 )) b x,ψ(x,a)θ⋆ + 1 b x,ψ(x,a)θ⋆ ⊤ ψ(x,a)λ ,
w
− c − c ∇
(cid:18) 2 2 (cid:19)
(cid:0) (cid:1) (cid:0) (cid:1)
wherethelastinequalityusedthecondition g(x,y)exp h(x,y)⊤w ν(dy) = exp b x,w
Y
for all w Rm. Since 2b(x,w) c I , by Taylor’s expansion,
∈ ∇w (cid:22) 2 m R (cid:0) (cid:1) (cid:0) (cid:0) (cid:1)(cid:1)
exp λ⊤ θℓ(θ⋆,x,a,y) π θ⋆(dy x,a)
∇ |
ZY
(cid:0) c2 (cid:1)
exp 1 sup λ⊤ψ(x,a)⊤ 2b x,w ⊤ ψ(x,a)λ exp λ⊤H(x,a)λ .
≤ (cid:18)c2
2
w∈Rm ∇w
(cid:19)
≤
(cid:0) (cid:1) (cid:0) (cid:1) (cid:3)
This finishes the proof.
Proof of Proposition 2.2. By the boundedness of ψ an the definition of H in Example 2.1,
it is clear that exp(r H(x,a) )η(da)µ(dx) 2 for some r > 0. For all v
X×A H k kRd×d ≤ H ∈
Rd 0 ,
\{ } R
v⊤ H(x,a)η(da)µ(dx) v = ψ(x,a)v 2 η(da)µ(dx) > 0. (4.1)
Rd
k k
(cid:18)ZX×A (cid:19) ZX×A
Suppose that the above inequality does not hold. Then ψ(x,a)⊤v = 0 for η ν-a.s. (x,a)
⊗ ∈
. By the continuity of ψ and the definition of the support supp(µ), ψ(x,a)v = 0
fX or× alA l (x,a) supp(µ) . This along with span ψ(x,a)⊤ x supp(µ),a = Rd
implies that
v∈
= 0
and× heA
nce leads to a
contractio{
n.
Recall| tha∈
t for A
Sd,∈ ρA}
(A) =
min
∈
min v⊤Av. The desired conclusion then follows from the compactness of v
v∈Rd,kvk Rd=1
{ ∈
Rd v = 1 and (4.1). (cid:3)
Rd
| k k }
4.2. Proof of Example 2.2.
Proof of Example 2.2. Note that ℓ = C ℓ˜ with ℓ˜ (θ,x,a,y) = 1(y µ(θ,x,a))2 being the
1 2 −
unnormalized quadratic loss. For all (θ,x,a,y) Rd ,
∈ ×X ×A ∈ Y
˜
ℓ(θ,x,a,y) = (y µ(θ,x,a))( µ)(θ,x,a), (4.2)
θ θ
∇ − ∇
2ℓ˜ (θ,x,a,y) = (y µ(θ,x,a))( 2µ)(θ,x,a)+( µ)(θ,x,a)( µ)(θ,x,a)⊤. (4.3)
∇θ − ∇θ ∇θ ∇θ16 ǫ-POLICY GRADIENT FOR ONLINE PRICING
To verify (H.2(1)), by (4.3) and the condition of H,
2ℓ˜ (θ,x,a,y) y µ(θ,x,a) ( 2µ)(θ,x,a)+( µ)(θ,x,a)( µ)(θ,x,a)⊤
∇θ (cid:23) −| − | ∇θ ∇θ ∇θ
1 1
y µ(θ,x,a) H(x,a)+ H(x,a) +y y H(x,a).
(cid:23) −| − | c (cid:23) c −
1 (cid:18) 1 (cid:19)
Multiplying both sides of the inequality by C 0 yields
1
≥
1
2ℓ(θ,x,a,y) C +y y H(x,a) = H(x,a).
∇θ (cid:23) 1 c −
(cid:18) 1 (cid:19)
To verify (H.2(2)), let λ Rd be fixed. Note that as [y,y], by Hoeffding’s lemma,
∈ Y ⊂
(y y)2
exp λ˜ y µ(θ⋆,x,a) π θ⋆(dy x,a) exp − λ˜2 , λ˜ R.
− | ≤ 8 ∀ ∈
ZY
(cid:16) (cid:17) (cid:16) (cid:17)
(cid:0) (cid:1)
Setting λ˜ = C λ⊤( µ)(θ⋆,x,a) in the above inequality and use the condition of H yield
1 θ
∇
exp λ⊤ θℓ(θ⋆,x,a,y) π θ⋆(dy x,a)
∇ |
ZY
(cid:0) (y y)2 (cid:1)
exp − C2λ⊤( µ)(θ⋆,x,a)( µ)(θ⋆,x,a)⊤λ
≤ 8 1 ∇θ ∇θ
(cid:18) (cid:19)
(y y)2
exp − C2λ⊤c H(x,a)λ = exp λ⊤H(x,a)λ ,
≤ 8 1 2
(cid:18) (cid:19)
(cid:0) (cid:1)
where the last identity used
(y−y)2
C2c = C 1 +y y due to the definition of C . (cid:3)
8 1 2 1 c1 − 1
(cid:16) (cid:17)
4.3. Proof of Lemma 3.2. Lemma 3.2 follows from the following concentration inequality
for matrix-valued bounded martingale, which has been established in [22, Theorem 1.2].
Lemma 4.1. Let (Ω, ,( n) n∈N∪{0},P) be a filtered probability space, let (Y n) n∈N∪{0} be an
F F
Sd ≥0-valued martingale with respect to the filtration ( Fn) n∈N∪{0}, let (X n) n∈N be the difference
sequence such that X = Y Y . Assume that there exists R 0 such that X R
n n n−1 n op
for all n N. Define the p− redictable quadratic variation proce≥ ss W = n k E[Xk X≤ ⊤
] for∈ all n N. Then for all t 0 and σ2 > 0, n k=1 k k |
k−1
F ∈ ≥ P
t2/2
P n N Y t, W σ2 dexp .
∃ ∈ k n kop ≥ k n kop ≤ ≤ −σ2 +Rt/3
(cid:18) (cid:19)
(cid:0) (cid:12) (cid:1)
Proof of Lemma 3.2. For(cid:12)each n N, let X = H E[H ] and define Y = n X .
Note that X H
+E[∈
H
n
]
2n C− duen | tF on (− H1
.3). Fix n
Nn
. By
Lk= em1 mk
a
n op n op n op n−1 H
k k ≤ k k k k |F ≤ ∈ P
4.1 (with R = 2C ), for all t 0 and σ2 > 0,
H
≥
t2/2
P Y t, W σ2 dexp , (4.4)
k n kop ≥ k n kop ≤ ≤ −σ2 +2C t/3
(cid:18) H (cid:19)
(cid:0) (cid:1)
where W = n E[X X⊤ ]. Observe that for P-a.s.,
n k=1 k k | Fk−1
n n
P
W E[ X X⊤ ] = E[ X 2 ] n4C2,
k n kop ≤ k k k kop | Fk−1 k k kop | Fk−1 ≤ H
k=1 k=1
X Xǫ-POLICY GRADIENT FOR ONLINE PRICING 17
and hence by setting σ2 = n4C2 in (4.4), for all t 0,
H ≥
t2/2
P( Y t) = P Y t, W 4nC2 dexp .
k n kop ≥ k n kop ≥ k n kop ≤ H ≤ −4nC2 +2C t/3
(cid:18) H H (cid:19)
(cid:0) (cid:1)
Hence for all δ (0,1), by choosing
∈
2 2
2C d d2C d 2C d d d
t = H ln + ln H +8nC2 ln = H ln + ln +18nln 0,
3 δ s δ 3 H δ 3  δ s δ δ ≥
(cid:18) (cid:19) (cid:18) (cid:19)
 
it holds that dexp
t2/2
= δ, which completes the desired estimate for δ (0,1).
−4CH2+2CHt/3
∈
The conclusion for(cid:16)δ = 1 holds tr(cid:17)ivially. (cid:3)
4.4. Proof of Lemma 3.3.
Proof of Lemma 3.3. Fix x . Observe that for all t N, by (2.5),
∈ X ∈
2γ
a
supr¯ θ⋆(x,a) −r¯ θ⋆(x,φ t(x))
≤
k( ∇ar¯ θ⋆)(x,φ t(x)) k2 A. (4.5)
(cid:18)a∈A (cid:19)
Forallt
∈
N,BytheL a-Lipschitzcontinuityofa
7→
r¯ θ⋆(x,a)andφ
t
= φ t−1+η( ∇ar¯ θt)( ·,φ t−1( ·))
with η 1/L ,
a
≤
r¯ θ⋆(x,φ t(x))
L
≥
r¯ θ⋆(x,φ t−1(x))+ h( ∇ar¯ θ⋆)(x,φ t−1(x)),φ t(x) −φ t−1(x)
iA −
2a kφ t(x) −φ t−1(x) k2
A
L η2
= r¯ θ⋆(x,φ t−1(x))+ h( ∇ar¯ θ⋆)(x,φ t−1(x)),η( ∇ar¯ θt)(x,φ t−1(x))
iA −
a
2
k( ∇ar¯ θt)(x,φ t−1(x)) k2
A
η
≥
r¯ θ⋆(x,φ t−1(x))+ h( ∇ar¯ θ⋆)(x,φ t−1(x)),η( ∇ar¯ θt)(x,φ t−1(x))
iA −
2k( ∇ar¯ θt)(x,φ t−1(x)) k2
A
η
= r¯ θ⋆(x,φ t−1(x))+η h( ∇ar¯ θ⋆)(x,φ t−1(x)),( ∇ar¯ θt)(x,φ t−1(x))
iA −
2k( ∇ar¯ θt)(x,φ t−1(x)) k2
A
η η
= r¯ θ⋆(x,φ t−1(x))+ 2k( ∇ar¯ θ⋆)(x,φ t−1(x)) k2
A −
2kEt(x) k2 A,
(4.6)
tw hi ath
t
fE
ot
r(x a) ll: t= (
N∇
,ar¯ θ⋆)(x,φ t−1(x))
−
( ∇ar¯ θt)(x,φ t−1(x)). Combining (4.5) and (4.6) yields
∈
supr¯ θ⋆(x,a) r¯ θ⋆(x,φ t(x))
−
a∈A
η η
≤
supr¯ θ⋆(x,a) −r¯ θ⋆(x,φ t−1(x))
−
2k( ∇ar¯ θ⋆)(x,φ t−1(x)) k2
A
+ 2kEt(x) k2
A (4.7)
a∈A
η
≤
(1 −γ aη) supr¯ θ⋆(x,a) −r¯ θ⋆(x,φ t−1(x)) + 2L2 θkθ
t
−θ⋆ k2 Rd,
(cid:18)a∈A (cid:19)
where the last inequality used Condition (2.6). The desired inequality follows from the
(cid:3)
standard discrete Gronwall lemma; see e.g., [7, Proposition 3.1].18 ǫ-POLICY GRADIENT FOR ONLINE PRICING
References
[1] S. Agrawal and N. Goyal, Thompson sampling for contextual bandits with linear payoffs, in Inter-
national conference on machine learning, PMLR, 2013,pp. 127–135.
[2] A. Ajalloeian and S. U. Stich, On the convergence of sgd with biased gradients, arXiv preprint
arXiv:2008.00051,(2020).
[3] M. Basei, X. Guo, A. Hu, and Y. Zhang, Logarithmic regret for episodic continuous-time linear-
quadratic reinforcement learning over a finite-time horizon, JournalofMachineLearningResearch,23
(2022), pp. 1–34.
[4] C. Blier-Wong, H. Cossette, L. Lamontagne, and E. Marceau, Machine learning in P&C
insurance: A review for pricing and reserving, Risks, 9 (2020), p. 4.
[5] S.R.Chowdhury andA.Gopalan,Onkernelizedmulti-armedbandits,inInternationalConference
on Machine Learning, PMLR, 2017, pp. 844–853.
[6] S. Cohen and T. Treetanthiploet, Generalised correlated batched bandits via the arc algorithm
with application to dynamic pricing, arXiv preprint arXiv:2102.04263,(2021).
[7] E. Emmrich, Discrete versions of gronwall’s lemma and their applica-
tion to the numerical analysis of parabolic problems. preprint on webpage at
https://www3.math.tu-berlin.de/preprints/files/Preprint-637-1999.pdf, 1999.
[8] S. Filippi, O. Cappe, A. Garivier, and C. Szepesva´ri, Parametric bandits: The generalized
linear case, Advances in neural information processing systems, 23 (2010).
[9] X. Gao and X. Y. Zhou, Logarithmic regret bounds for continuous-time average-reward markov
decision processes, arXiv preprint arXiv:2205.11168,(2022).
[10] ,Square-rootregretboundsforcontinuous-timeepisodic markovdecision processes,arXivpreprint
arXiv:2210.00832,(2022).
[11] X. Guo, A. Hu, and Y. Zhang, Reinforcement learning for linear-convex models with jumps via
stability analysis of feedback controls,SIAMJournalonControlandOptimization,61(2023),pp.755–
787.
[12] B. Hao, T. Lattimore, and C. Szepesvari, Adaptive exploration in linear contextual bandit, in
International Conference on Artificial Intelligence and Statistics, PMLR, 2020, pp. 3536–3545.
[13] E. Hazan et al., Introduction to online convex optimization, Foundations and Trends® in Opti-
mization, 2 (2016), pp. 157–325.
[14] D. Janz, S. Liu, A. Ayoub, and C. Szepesva´ri, Exploration via linearly perturbed loss minimisa-
tion, arXiv preprint arXiv:2311.07565,(2023).
[15] H. Karimi, J. Nutini, and M. Schmidt, Linear convergence of gradient and proximal-gradient
methods under the Polyak-L ojasiewicz condition, in Machine Learning and Knowledge Discovery in
Databases: European Conference, ECML PKDD 2016,Riva del Garda, Italy, September 19-23,2016,
Proceedings,Part I 16, Springer, 2016,pp. 795–811.
[16] Y. Li, T. Ma, and H. Zhang, Algorithmic regularization in over-parameterized matrix sensing and
neuralnetworks with quadratic activations,inConferenceOnLearningTheory,PMLR,2018,pp.2–47.
[17] C. Liu, L. Zhu, and M. Belkin, Loss landscapes and optimization in over-parameterized non-linear
systems and neural networks, Applied and Computational Harmonic Analysis, 59 (2022), pp. 85–116.
[18] E. Malinvaud, Statistical methods of econometrics, Amsterdam: North-Holland, 1970.
[19] R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction, MIT press, 2018.
[20] L. Szpruch, T. Treetanthiploet, and Y. Zhang, Exploration-exploitation trade-off
for continuous-time episodic reinforcement learning with linear-convex models, arXiv preprint
arXiv:2112.10264,(2021).
[21] , Optimal scheduling of entropy regularizer for continuous-time linear-quadratic reinforcement
learning, SIAM Journal on Control and Optimization, 62 (2024), pp. 135–166.
[22] J. Tropp, Freedman’s inequality for matrix martingales, Electronic Communications in Probability,
16 (2011), pp. 262 – 270.
[23] S.Vakili, K. Khezeli, and V.Picheny,Oninformation gain andregretboundsin gaussian process
bandits, in International Conference on Artificial Intelligence and Statistics, PMLR, 2021, pp. 82–90.
[24] R.J.Williams,Simplestatisticalgradient-following algorithmsfor connectionistreinforcementlearn-
ing, Machine learning, 8 (1992), pp. 229–256.ǫ-POLICY GRADIENT FOR ONLINE PRICING 19
Statements & Declarations
Funding. Authors acknowledge the support of the UKRI Prosperity Partnership Scheme
(FAIR) under EPSRC Grant EP/V056883/1 and the Alan Turing Institute.
Competing Interests. The authors have no relevant financial or non-financial interests
to disclose.
Author Contributions. All authors have contributed equally to this study.