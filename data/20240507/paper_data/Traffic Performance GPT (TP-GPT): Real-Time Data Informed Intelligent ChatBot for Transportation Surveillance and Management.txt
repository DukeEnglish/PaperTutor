1
Traffic Performance GPT (TP-GPT): Real-Time
Data Informed Intelligent ChatBot for
Transportation Surveillance and Management
Bingzhang Wang, Zhiyu (Joey) Cai, Muhammad Monjurul Karim, Chenxi Liu, and Yinhai Wang, Fellow, IEEE
Abstract—Thedigitizationoftrafficsensinginfrastructurehas To properly manipulate databases even requires not only a
significantly accumulated an extensive traffic data warehouse, deep preliminary understanding of context but also special-
which presents unprecedented challenges for transportation an-
ized expertise in database programming. Secondly, traditional
alytics. The complexities associated with querying large-scale
methods of traffic analysis have primarily focused on the
multi-tabledatabasesrequirespecializedprogrammingexpertise
and labor-intensive development. Additionally, traditional anal- numerical aspects of the data, using statistical methods and
ysis methods have focused mainly on numerical data, often ne- machine learning techniques. This overlooks the semantic
glecting the semantic aspects that could enhance interpretability and interpretability attributes of the data. As a result, the
and understanding. Furthermore, real-time traffic data access is
exploration of traffic datasets has been limited to numerical
typicallylimitedduetoprivacyconcerns.Tobridgethisgap,the
imputations and prescriptive visualization, neglecting their
integration of Large Language Models (LLMs) into the domain
of traffic management presents a transformative approach to inherent natural-language significance. Thirdly, access to real-
addressing the complexities and challenges inherent in modern time traffic data is typically restricted to authorized entities
transportation systems. This paper proposes an intelligent on- such as government agencies and academic institutions, due
line chatbot, TP-GPT, for efficient customized transportation
to privacy concerns, making it inaccessible for direct data
surveillance and management empowered by a large real-time
handling by the general public. There is a pressing need for
traffic database. The innovative framework leverages contextual
and generative intelligence of language models to generate an intermediate framework that processes and interprets data
accurate SQL queries and natural language interpretations by forpractitionersinaprivacy-preservingmanner,whichhasthe
employingtransportation-specializedprompts,Chain-of-Thought potentialtonotonlyfacilitatetripplanningandpolicy-making
prompting,few-shotlearning,multi-agentcollaborationstrategy,
butalsoimprovetrafficanalysisbymakingitmoreaccessible,
and chat memory. Experimental study demonstrates that our
efficient, and equitable.
approach outperforms state-of-the-art baselines such as GPT-
4 and PaLM 2 on a challenging traffic-analysis benchmark In recent years Large Language Models (LLMs) have
TransQuery. TP-GPT would aid researchers and practitioners emerged as a groundbreaking development in the field of
in real-time transportation surveillance and management in a artificial intelligence (AI), demonstrating unparalleled capa-
privacy-preserving, equitable, and customizable manner.
bilitiesinunderstanding,andinterpretingreal-worldscenarios
Index Terms—Transportation Analytics, Pre-Trained Large in human-like ways [1]. These models, powered by advanced
Language Models, SQL Database. neural network architectures, have found applications across
a wide range of domains, from natural language processing
and machine translation to content generation and beyond.
I. INTRODUCTION
They offer unprecedented capabilities in data interpretation
WITH the rapid digitization of sensing infrastructure,
and decision-making.
an immense volume of traffic data is being collected
LLMs have proven their versatility in different fields, in-
at an ever-increasing rate. This presents both opportunities
cludingeducation,healthcare,softwareengineering[2],among
and challenges. On one hand, these unprecedented data re-
many others. While still a new area of exploration, LLMs
sourcesholdthepromiseofpropellingadvancementsintraffic
show promise for analyzing traffic data. Their general ability
analysis, making it more accurate and reliable; On the other
to adapt to different tasks makes them attractive for various
hand,theunpredictableaccumulationofdataposessignificant
intelligent transportation applications, including traffic signal
challengesforthedevelopmentofsophisticatedtrafficanalysis
control [3], and accident risk assessment [4]. However, effec-
techniques. Firstly, real-time traffic sensing data is typically
tively using LLMs in these specialized areas requires them to
stored in large-scale, multi-table databases. These databases
have a deep understanding of the specific domain [5]. This
are incredibly large and have complex relationships between
necessitates the integration of domain-specific databases with
different data points. Querying these database can be labor-
LLMs to foster data-driven reasoning capabilities. Recent re-
intensive and time-consuming, leading to significant latency.
search has explored leveraging LLMs for database operations.
FrameworkslikeLangChainfacilitateefficientinteractionwith
B. Wang, M.M. Karim, C. Liu, and Y. Wang are with the Department
of Civil and Environmental Engineering, University of Washington, Seattle, LLMs [6], while models like DB-GPT are fine-tuned with
WA. (Email: bzwang@uw.edu; mmkarim@uw.edu; lcx2017@uw.edu; yin- domain-specific database knowledge [7], enabling the transla-
hai@uw.edu)
tion of textual semantics into database queries (text-to-SQL).
Z. Cai is with the Department of Civil and Environmental Engineering,
UniversityofCalifornia,Berkeley,CA.(Email:zhiyu cai@berkeley.edu) These advancements offer exciting possibilities for advanced
4202
yaM
5
]AM.sc[
1v67030.5042:viXra2
analysis of large-scale traffic data. However, existing models bottleneckscenarioshavealsobeenstudied[9]combiningwith
primarilytargetgeneral-purposedatabasetasks,neglectingthe robotic vehicles and human drivers.
unique knowledge domain of traffic data. This omission of Other research expanded the scope of specific scenarios,
traffic-specific background knowledge during training may such as [10] combines LLMs to assist human users in traffic
lead to diminished query result accuracy and limit their networkanalysisandfurtherdecision-making.Therearemore
applicability in traffic research. research explored into forecasting tasks. A traffic prediction
DespitetheproliferationofLLMapplicationsacrossvarious modelproposedbyresearchersin[11]showscapacitytotackle
domains, their potential in traffic data analysis, especially flowpredictiontasksunderfull-sampleandfew-shothistorical
withindatabases,remainsanopenresearchtopic.Furthermore, data scenarios. By encoding the data in a specific way [12],
thereisapressingneedforauser-friendlyintelligentplatform LLMs can be widely used to conduct research on time series
that can effectively communicate and analyze real-time data. and spatio-temporal data [13], and their potential to analyze
The paper aims to delve into this gap by proposing an intelli- data of such modalities is being further explored, including
gent transportation analytics system that applies pre-trained univariatetimeseriesforecasting[14],multivariatetime-series
large language models to complex traffic data analysis. It data [15].
leveragestheircapabilitytogenerateaccurateSQLqueriesand One other significant challenge is model’s capability to
natural language interpretations based on contextual aware- understandcontextsintransportationsystems.Authorsin[16]
ness, demonstrating the extensive pre-trained knowledge of manually collected multi-modal traffic data sets,such as text
LLMs and their proficiency in adapting to the transportation and traffic signs alignment for model training, while authors
domain. in [17] focused on users’ itinerary planning demands. Among
The contributions of this paper are listed below: a variety of integration of Large Language Models into traffic
1) An intelligent online chatbot, Traffic Performance GPT management, this paper showcases a promising avenue for
(TP-GPT), is proposed for efficient personalized trans- enhancing real-time traffic analysis and management in order
portation analysis and management leveraging the sup- to benefit practitioner, traveler and researcher.
port of big real-time traffic data. To the best of our
knowledge, TP-GPT is the first real-time traffic analysis
B. LLMs in Database SQL Query
chatbot empowered by LLMs to be proposed.
LLMs exhibit remarkable capabilities in generating reliable
2) Leveraging contextual and generative intelligence of the
and accurate SQL queries, advancing database interaction
Generative Pre-trained Transformer (GPT), an innova-
efficiency. A recent study introduces BIRD [18], a benchmark
tive framework is constructed to serve as a connection
for large-scale database text-to-SQL tasks highlighting chal-
between public users and authorized data resources in a
lengessuchascontentquality,externalknowledgeintegration,
privacy-preserving, equitable, customizable way.
and SQL efficiency. Their inherent contextual intelligence
3) The developed chatbot is able to generate reliable,
enables them to understand data origins and relationships,
responsiveandaccuratetrafficanalysisandmanagement
thus enhancing the performance of data queries and analyses.
responses to input questions, by integrating designed
Many scholars use SQL agent to enhance database-related
prompts, few-shot learning module, multi-agent collab-
tasks, such as [19] employs a Data Expert LLM (DELLM)
oration strategy and conversation memory module. The
to provide knowledge for text-to-SQL models to generate
proposed method outperforms existing general-purpose
accuratequeries.Authorsin[20]enhancetext-to-SQLparsing
LLMs regarding traffic-domain analysis performance.
in multi-turn conversations by leveraging contextual informa-
tion from the dialogue history. Research [21] utilizes LLMs
II. RELATEDWORK
to automatically generate test cases for selecting the most
A. Traffic Management with LLMs accurate SQL query from a set of candidates.
Traffic management for road networks especially in urban Databases can be queried more effectively by leveraging
area encounters complex background knowledge, and real- LLMs,ensuringthattargetdataisextractedwithahighdegree
time demands. Previous research usually relied on traditional of precision. This prowess underscores the potential of LLMs
hardwareinfrastructure,suchassurveillancecamerasandloop indatabasemanagementandanalysis.Currentresearchmostly
sensors to observe traffic flow. Researchers have employed usesagentsfortaskprocessingfromtheperspectiveofcontext.
both micro and macro methods [8] based on theories and data The disadvantage of this is that LLM’s ability to perform
fromtraditionalmethodstosimulatetrafficflow,promotingthe SQL queries is relatively static and requires a large number
advancement of theoretical research as well as management of iterations to correct the model.
strategies.
Recent research addresses challenges of traffic more from
III. METHODOLOGY
data-driven perspectives with the emerging techniques of mo-
A. Problem Statement
bility and traffic data collection and analysis. There has been
a shift towards leveraging deep learning methods and LLMs Asubstantialmagnitudeoftrafficintensityisbeingobserved
to provide new solutions to traffic control and management. in modern mobility. As reported by the Washington State
Trafficcontrolatintersections[3]emphasizestrafficefficiency Department of Transportation, the annual Vehicle Miles of
and collision prevention. Other scenarios such as ring and Travel(VMT)onthestatehighwayofKingCountyreacheda3
total of 8,534 million in the year 2022. Meanwhile, the wide- access to the database for security and privacy concerns
range deployment of traffic sensing systems has employed limitsthepotentialparticipants’exploration,hinderingflexible
tons of real-time and historical data. The unpredictable, fast- investigation.
changing traffic patterns underlying the enormous numerical
TABLE I: Network-wide traffic database introduction
data pose an ever-challenging task for efficient and effective
traffic analysis and management. To tackle this problem,
TableName Columns Description
we incorporate a large-scale network-wide mobility database
dbo.cabinets 17 Loopdetectordetailsofunitname,
hosted by STAR Lab [22], which integrates the real-time data coordinate,route,milepost,and
resources of traffic counts (i.e., speed, volume, occupancy) direction.
dbo.cabinfo 6 Districtlocationofloopdetectors
collected from more than 8,000 inductive loop detectors, as sortedbycabinetstationID.
well as the route segment-wide Traffic Performance Score dbo.MinuteDataNW 6 One-minutetrafficspeed,volume,
occupancydatainWashington
(TPS) calculated using Equation 1 based on loop data [23]. NorthwestsortedbyloopdetectorID
andtimestamp.
(cid:80)n Vi·Qi·Li dbo.Segments 6 Roadsegmentdefinitionwith
TPS = i=1 t t ×100% (1) correspondinglocationinformation.
t (cid:80)n V ·Qi·Li dbo.SegmentTrafficIndex 8 Segment-basedtrafficperformance
i=1 f t dataongeneral-purposelanesand
where Vi and Qi represent traffic speed and volume of road carpoollanes,includingspeed,
t t volume,andTPS.
segment i at time t. Li is road segment length covered by dbo.TrafficIndex 9 Statisticaltrafficperformancedatafor
the i-th detector. V is free flow speed. Therefore, TPS is a eachdefinedroadsegment.
f
value ranging from 0% to 100% where 0% is the worst traffic
condition and 100% is the best. Inthiscontext,weproposeanintelligenttrafficperformance
chatbot TP-GPT for real-time transportation surveillance and
Theseloopdetectorsaredeployedonfreeways,includingI-
management in a privacy-preserving way. While, technical
5, I-90, I-99, I-167, I-405, and SR-520, in the Greater Seattle
problems need to be tackled for comprehensive development:
Area, WA, as shown in Figure 1. Data have been collected
1) ChatGPT does not have preliminary contextual knowledge
online in one-minute intervals from 2020 to the present, and
of the database setup, structures and content. Even though
it has accumulated around 1.89 Terabytes (TB) in the data
ChatGPT has strong abilities in programming and inference,
warehouse. There are a total of 6 tables in the database, with
direct deployment of the language model to database analysis
the details shown in Table I.
without prerequisites may result in hallucination that blind
actions irrelevant to the raised question would possibly be
taken. Thus, the compilation of appropriate input prompts
is essential for employing ChatGPT. 2) ChatGPT has a re-
liable performance of executing sequential commands acti-
vated by human’s input. However, the lack of monitoring
the response relevance and query correctness regarding to
the user’s question may generate unexpected, unreliable an-
swers.Itisnecessarytoincorporateanintelligentautonomous
streamlinewithself-managementcapabilityforqualitycontrol
of answer generation. Specifically, iterative communications
between ChatGPT and the database in the revision circulation
Fig.1:InductiveloopDetectorlocationsshownasbluepoints
isneededtokeepmodifyingresponsesuntilcertainstandardis
reached.Thisprocessnecessitatestheparticipationofmultiple
This level of database has great potential for insight-
intelligent virtual agents of different roles, such as database
ful network-wide analysis of real-time traffic performance.
query engineer, quality manager, consultant and more. 3) The
However, vast volume of data poses great challenges for
demonstration of a set of typically asked questions with their
practitioners’ and researchers’ inspection and management
corresponding answers are critical for ChatGPT to specify
of data. Firstly, exploring the data warehouse necessitates
the work scope, furthermore, to improve the effectiveness and
participants’deepunderstandingofdatabasesetupdetails(i.e.,
efficiency of target response. 4) In order for users to be able
tables, columns, data types, relations, and so on), as well
to consecutively interact with the chatbot, a chatting memory
as expertise in database programming especially processing
function is needed so that the chatbot has the historical
large-scale databases. Even though, manually programming
memory to support following generative process.
various queries and then executing commands for real-time
information in such a huge database is particularly time-
B. System Design
consuming and labor-intensive, making it almost a mission
impossible. Second, interpreting from numeric data results to TP-GPT aims to answer users’ questions by integrating
human-language traffic advisory or impactful analysis results GPT with real-time traffic SQL database. Directly applying
needs in-depth professional transportation-domain knowledge generativemodeltothedatabasehaspotentialtocauseseveral
and a wide range of historical knowledge base support such concerns such as reasoning hallucination, answer accuracy,
as urban planning, and social events. Third, the lack of direct and performance reliability. To overcome these obstacles,4
Fig. 2: Overall system architecture design demonstrating traffic analysis pipeline (right), example input prompts (left), and
incorporated Chain of Thought iterative prompting (center)
the proposed framework incorporates multiple modules as a iterativelyreasonsaseriesofintermediatestepsbeforethefinal
pipeline to process and interpret data, as shown in Figure 2. answerisgenerated.TheconceptualworkflowofCoTiteration
Specifically, once users’ questions are input into the ChatBot, isshowninFigure2.Thedesignedpromptscovercomprehen-
fivestepsareexecuted,eachofwhichisempoweredbyGPT’s sive descriptions in these aspects: 1) Instructions on what role
contextual and generative intelligence, to generate the final themodelisexpectedtoplayinperformingthetasksandhow
answer: 1) Task management: Since the fixed sequence of to generate the object responses; 2) In-depth descriptions of
implementation steps is not adaptable to resolving different database setup with an emphasis on table relations, column
questions, a customized execution plan is produced based on explanation, and data significance. 3) Domain knowledge in
the input. Meanwhile, each execution step is determined and transportation scenarios, especially those missing from GPT’s
managed flexibly by GPT’s decision-making capability. For commonsense, such as the definition of Traffic Performance
example,ifthequestionisrelevanttosearchingthetrafficdata, Scoreanditsmeasurementscale.4)Designatedoutputformat
database query generation and execution will be included in that is desired from GPT at the current step. An example
theplan;Otherwise,thechatfunctionwillbeactivatedtointer- prompt input is shown in Figure 2.
act with users based on the general knowledge base. 2) Query
generation: A query to extract relevant data in the database
D. Multi-Agent Strategy
is generated by understanding the input question. Constraints
on query statement specifics such as programming syntax and GPT achieves superior performance in solving individual
quantity of queried data are applied to the process. 3) Quality tasks following instructional logic in a dialogue. However,
check: An initial syntax check is performed on the generated highly sophisticated problems in real-world traffic analysis
query to inspect if it is beyond the designated constraints are challenging to tackle by directly adopting GPT’s general
like too many data instances are queried. 4) Query execution: intelligence. Here we introduce the multi-agent strategy in
The checked query is executed in the SQL database. While, transportation scenarios, an innovative collaborative workflow
an inspection of the execution log is conducted to debug incorporating GPT as different agents to work as a team,
any existing errors. 5) Data interpretation: The queried data which simulates the human’s real teamwork logistics in a
combined with the user’s question is analyzed and interpreted research lab or industrial company. The overall objective
into a traffic advisory report incorporating GPT’s wide range is to decompose the original complex tasks into sub-tasks
of transportation knowledge base. of different scopes and then assign each to a GPT agent
expertizedinthedesignateddomaintosolvebyastreamlined,
collaborative workflow. The interaction between agents is
C. Input Prompt Generation
enabledandachievedbyincorporatingapublicscratchpadfor
Prompt engineering is essential for LLMs to master pre- theagentteamtotracktheupdatedprogress.Agentsiteratively
liminary contextual information before a practical question is communicate with the GPT based on the CoT prompting
fed. Such integration can not only enable language models to dialogue in JSON format, illustrated in the previous section,
avoid the unnecessary repeating process of reasoning users’ to seek advisory and thoughts on task solutions and then
desiredresponsesineachrunwhenthecontexts(i.e.,database formulate the execution plan.
setup, question background) are constant, but also boost the The joint collaborative framework includes four virtual
ChatBotperformanceinthetransportationdomain.Inthepro- agents, shown in Figure 3, each of which conducts the des-
posed framework, the iterative interface with GPT is achieved ignated scope of tasks corresponding to a teamwork role. In
through the Chain of Thought (CoT) prompting [24], which detail,theprojectmanagercontrolstheentireteam’sworkflow5
Figure2.Thisapproachavoidsofferingirrelevantexamplesto
target work scope, thereby increasing the few-shot efficiency
and accuracy, as well as reducing the possibility of non-
objectiveexamplesmisleadingtheresponse.Theintegrationof
few-shot learning remarkably enhances ChatBot performance,
especiallywhenquestionsthataretypicallyaskedfallintothe
pre-defined scope. This also provides the potential for further
extensionofobjectiveresponsedomainbysimplyaddingmore
exemplar use cases.
F. Chat Memory
Fig.3:TP-GPTmulti-agentcollaborationframeworkfortraffic
data analysis To enhance interactivity, the ChatBot should retain a mem-
ory of past conversations, allowing it to infer and tailor
future responses based on previous interactions. In most use
by reasoning the next-step action based on the execution cases, if the initial generated response does not fulfill users’
resultsfromthelaststep;SQLengineergeneratessynthetically expectations, normally subsequent questions will follow, as-
correct queries for the desired data through contextual under- suming chat history is held. Thus, the ChatBot is integrated
standingoftheusers’inputdemand;Qualityanalystchecksif with the chat memory module by saving dialogue history in
asyntaxerrorexistsinthegeneratedqueryortheobtaineddata each conversation session. Specifically, only the part of chat
is not reasonably acceptable, then necessary inspections into recordsrelevanttothecurrentinputquestionisretrievedfrom
backend database execution log are implemented to figure out memoryforthemodel’sreferenceinresponse,whichprevents
the potential cause to the error in order to correct the query; from reading the whole lengthy dialogue list. Afterward, the
Finally, data analyst interprets the queried data to mobility generated answer, along with the question, is written into
analysis report based on pre-trained transportation domain memory. This simple yet effective implementation advances
knowledge, to answer the user’s demanded information in a the ChatBot towards more efficient and intelligent interaction
natural-language illustrative method with real-time data and experience.
detailed explanation involved. The multi-agent collaborative
strategy significantly improves the overall performance of the
IV. EXPERIMENTS
ChatBot in terms of accuracy, reliability, interpretability, and
flexibility. An online ChatBot with interactive web interface has been
developed using Streamlit framework in Python. An example
conversationbetweenuserandTP-GPTchatbotisvisualizedin
E. Few-shot Learning Prompting
Figure4.Furthermore,experimentalstudyhasbeenconducted
AlthoughLLMsexhibitimpressivezero-shotcapabilitiesfor
leveraging the ChatBot interface to evaluate and compare
general problems, they struggle to address complex problems
performance of TP-GPT with baseline models.
in the transportation domain using only a zero-shot approach.
Few-shotpromptingcanbeusedasatechniquetofacilitatein-
context learning by providing examples that guide the model
toward better performance. The demonstrations in the prompt
containexemplaruserquestionswiththeiridealqueriescrafted
bydevelopersthatthemodelisexpectedtofollowtogenerate
subsequent responses. In each attempt to resolve the input
question, the model can refer to several of the most relevant
examples, which not only enables the model to master the
contextual knowledge required to answer similar questions
accurately but also improves response efficiency by signifi-
cantlydecreasingreasoningtimeandtheriskofcausingerrors.
To further enhance response reliability leveraging few-shot
learning, an example repository covering multiple transporta-
Fig. 4: Online web application interface of TP-GPT ChatBot
tion scenarios is manually crafted, including real-time traffic
advisory, historical data statistical analysis, travel emission
inquiry, lane-based traffic performance inspection, and so
A. Experimental Setup
on, for providing comprehensive transportation evaluations.
In implementations, the input question is converted to text To demonstrate the model’s performance in solving
embedding to search the designated number of most similar transportation-specific domain problems, we introduce our
questions in the example repository. Then, these question- self-generated challenging traffic analysis benchmark named
queryexamplepairsareusedtopopulatethegeneratedprompt TransQuery based on the mobility database as mentioned in
formodelinference.Atypicalfew-shotpromptisdisplayedin Section III-A. The benchmark includes 50 manually designed6
complicatedtransportationsurveillanceandmanagementtasks barelygenerate15and7flawlessresponsesaccountingforthe
in real-world scenarios. Each instance is an input-output percentage of 30% and 14% respectively.
pair consisting of a question and its ground-truth response
(i.e., SQL query, target data, and natural language answer). TABLEIII:PerformancecomparisonofTP-GPTwithbaseline
These tasks cover a wide scope of traffic analysis challenges models on TransQuery traffic-analysis benchmark
including but not limited to: spatial-temporal traffic condition
Model ResponseMetrics Avg.Score
inquiry, traffic pattern recognition, peak traffic behavior anal-
Non-functional Runnablebutimperfect Flawless
ysis, real-time lane-based trip advisory, traffic counting statis-
tics, and vehicle emission evaluation. Some of the example PaLM2 31(62%) 4(8%) 15(30%) 0.34
problemsarelistedintheTableII.Todemonstratethevalidity SQLCoder 41(82%) 2(4%) 7(14%) 0.16
of employing TP-GPT framework, we use TransQuery bench- GPT-4Turbo 15(30%) 13(26%) 22(44%) 0.57
TP-GPT 3(6%) 7(14%) 40(80%) 0.87
mark to compare our method with GPT-4 Turbo [25], PaLM
2 [26], and SQLCoder [27] which are state-of-the-art baseline
LLMshavingexpertiseinthedatabasequerydomain.Standard This result not only demonstrates the complicacy and
prompting containing fundamental database information and challenge of TransQuery benchmark, but also indicates TP-
contextual knowledge is integrated into baseline models. GPT’s superior performance in resolving transportation sce-
nario problems and its effectiveness when GPT is integrated
TABLE II: Sample questions in TransQuery with our developed ChatBot system. A detailed comparison
between the models is narrated in the following.
Inputquestions
• PaLM 2: The most capable PaLM 2 model in code
ShowmethemostrecentloopdataonSR99Northboundsortedbylocations.
generation named Bison, is tested as one of the baselines
ComparetrafficperformanceofI-5betweenMondayandSundayinlastmonth.
WhatwasthetrafficconditionofSR-520duringtoday’seveningpeak? due to its compatibility with SQL query generation. This
ShouldIuseHOVorgeneralpurposelaneonI-5now? model has the fastest response time for its relatively
HowmanycarsareoneachsegmentofI-405onaveragenow? small model size. However, the performance struggles in
Whatisthedifferenceingreenhousegasemissionsbetweenweekdaysandweekendsinlastmonth?
dealing with complex tasks requiring overloaded tokens
thatfrequentlyexceedthelimit.Forexample,thequestion
To statistically compare the performance of each model
”Compare traffic performance of I-5 between Monday
on TransQuery, the execution result of each instance of the
and Sunday in last month.” requires querying a vast
benchmarkisevaluatedbythemetricswithcorrespondingrate
amountofminute-by-minutehistoricaldatainonemonth
scores: Non-functional: SQL query is non-executable due to
to summarize the traffic performance. This data volume
existingerrors,withratescoreof0;Runnablebutimperfect:
cannot be processed by Bison causing the token limit
SQL query can be executed successfully but not perfectly
error.
answers the input question, with rate score of 1; Flawless:
• SQLCoder: Defog’s SQLCoder-34B is implemented for
correct data can be queried, and interpretation of data can
text-to-SQLperformancecomparisonasoneofbaselines.
properly answer the input question, with rate score of 2. In
The model is constrained by its model size achieving
this way, an average performance score S is calculated for
the lowest solve rate. It could have been trained using
each model using the Equation 2:
the dataset that consists of MySQL-syntax samples other
(cid:80) n s thanMSSQLdialect,whichispotentiallythemajorfactor
S = i i i (2)
(cid:80) of disadvantageous performance. However, SQLCoder
s n
max i i
provides great possibilities of future fine tuning to adapt
whereirepresentsevaluationcategories(e.g.,non-functional),
to the specific domain due to its open-source origin.
nrepresentsthenumberofexperimentalinstancesclassifiedto
• GPT-4Turbo:InordertocompareTP-GPTwithamodel
eachcategory,andsrepresentstheircorrespondingratescores.
that is not limited by model size, we conducted experi-
mentsontheup-to-dateGPTmodel,GPT-4Turbo,which
B. Performance Comparison
has the 128,000 token limit. This model is commonly
In experiments, three baseline models are compared to regarded as the most powerful generative intelligence
TP-GPT using TransQuery benchmark that has 50 complex by the public for its extensive pre-trained knowledge
transportation problems. As shown in Table III, TP-GPT in various domains, absolutely including transportation
significantlyoutperformsotherbaselinemodelsonTransQuery and SQL query programming. In the experiment, the
with an average performance score of 0.87, surpassing GPT- fundamental prompt containing contextual information
4 Turbo of 0.57, PaLM 2 of 0.34, and SQLCoder of 0.16. such as description of the expected role to perform,
Specifically, out of total 50 questions, TP-GPT generates 40 database setup, and table schema is input to the model’s
flawless responses accounting for 80% of the whole dataset, memory. The result shows that GPT-4 Turbo achieves
7 runnable queries but imperfect responses, and only 3 non- a better performance than PaLM 2, especially regarding
functional queries. Whereas, the state-of-the-art LLM GPT- the lower non-functional rate. The strong reasoning and
4 Turbo only generates 22 flawless responses accounting for coding capabilities enable the model to generate more
44% of the whole dataset, even with superior pre-trained executable SQL queries. However, obtaining flawless
reasoning and contextual capability. PaLM 2 and SQLCoder answers with correctly queried data is still challenging7
foritslowsolveratecomparedtoTP-GPT.Afairportion • Few-shot learning: Few-shot learning has the least im-
of queries are runnable yet do not properly answer the pactonTP-GPTamongmodulesregardingresponsemet-
input questions. rics. This is because it only affects answering questions
• TP-GPT: Our innovative system TP-GPT, integrating highly similar to the exemplar. For most questions not
intelligence-boosting strategies to regular GPT-4 model relevant to example inventory, the responses are barely
without need of fine-tuning, achieves the highest per- impacted.
formance among powerful models. In experiments, TP- • Multi-agent strategy: Multi-agent collaboration plays
GPTiscapableofcraftinglengthySQLstatementswithin a vital role in TP-GPT, suggested by the decrease in
minutes and also interpreting data to answer tricky ques- flawless response rate to 44% when it is removed. Intu-
tions that require deduction and derivation. For example, itively, the problem-solving process has multiple stages,
TP-GPT tackles the question ”What is the difference in where reviewing outputs, generating feedback, and cre-
greenhouse gas emissions between weekdays and week- ating thoughts iteratively are fundamental to producing
ends in the last month?” by estimating emissions based accurate results. However, a sequential process with no
on Vehicle Miles of Travel (VMT) using a conceptual reflection loop in this variation frequently results in
formula. misunderstanding of contexts, causing syntax errors, or
Several observations are captured during experiments that extracting mistaken data.
potentially account for the superior performance of TP-GPT
over GPT-4 Turbo: 1) TP-GPT’s multi-agent strategy, espe-
cially the quality agent, enhances the query correctness by
checking syntax errors. However, GPT-4 Turbo suffers from
frequently executing incorrect queries without error checking.
It also struggles to recognize the correct column name of a
table even when the schema has been input; 2) TP-GPT’s
implementation of Chain of Thought enables the model to
revise the response in multiple rounds of iterative communi-
cation.Onthecontrary,GPT-4Turbomerelyprocessesdatain
asequentialflowwithoutthereviewloop,whichcausesalack
ofunderstandingofthedatabaseenvironmentandcontent.Itis
unable to inspect if the query will successfully extract correct
Fig.5:AblationstudyofTP-GPTonTransQuerybyremoving
data, having the potential to obtain empty results; 3) As TP-
prompts, few-shot learning, or multi-agent strategy
GPTispromptedbyfew-shotexamplesconsistingoftemplate
questions and queries, higher proficiency in generating cer-
tain SQL dialects (e.g., Microsoft SQL) is observed for the
V. CONCLUSIONS
prior knowledge leaned from few-shot learning in consistent
contexts. With the absence of this functional module, GPT- TheintegrationofLargeLanguageModels(LLMs)intothe
4 Turbo fluctuates its performance due to frequently caused domain of traffic management presents a transformative ap-
errors converting date from string to timestamp in Microsoft proach to addressing the complexities and challenges inherent
SQL syntax. in modern transportation systems. This paper has outlined the
development of an intelligent traffic performance chatbot, TP-
GPT, which leverages the power of real-time traffic data and
C. Ablation Study
the contextual understanding of LLMs to provide efficient,
This section examines the impact of removing the crafted accurate, and privacy-preserving transportation surveillance
prompt, few-shot learning, and multi-agent strategy from the and management.
proposedTP-GPTonTransQuery.Questionsinthebenchmark TP-GPT demonstrates a novel framework that can effec-
areinputtoTP-GPTanditsthreevariations,andeachresponse tivelynavigatethevastandintricatetrafficdatabaselandscapes
is evaluated as non-functional, runnable but imperfect, or to extract meaningful insights. It has great capabilities to
flawless as described in Section IV-A. The percentage of understand contextual information and respond to inquiries
responsesineachofthesethreecategoriesamongallresponses in the transportation domain by transforming input texts into
iscalculatedforthemodels,ofwhichtheresultsareshownin queries and converting data into detailed natural-language
Figure 5. By comparing the flawless response rates, TP-GPT analysis reports leveraging extensive prior knowledge. TP-
significantly outperforms the three variations, validating the GPT employs Chain of Thought prompting for iterative query
effectiveness of incorporating each of modules embedded in generation, a multi-agent strategy to optimize intermediate
TP-GPT. results, few-shot learning to enhance exemplar performance,
• Prompt: The prompt elaborately customized for and chat memory to improve interaction quality. The experi-
transportation-domain inquiry is essential to TP-GPT. mental study devices a challenging traffic-analysis benchmark
The removal of prompt leads to a sudden drop in TransQuery to compare the performance of TP-GPT with
flawless response rate from 80% to 26%, due to a lack state-of-the-art baseline LLMs. Quantitative results show that
ofcontextualknowledgeandperformingroledescription. TP-GPT significantly outperforms GPT-4 Turbo, PaLM 2,8
and SQLCoder, demonstrating its superior performance in [18] J.Li,B.Hui,G.Qu,J.Yang,B.Li,B.Li,B.Wang,B.Qin,R.Geng,
resolving real-world transportation inquiry tasks. The ablation N.Huo,X.Zhou,M.Chenhao,G.Li,K.Chang,F.Huang,R.Cheng,
and Y. Li, “Can LLM Already Serve as A Database Interface? A
study validates the effectiveness of employed modules in TP-
BIg Bench for Large-Scale Database Grounded Text-to-SQLs,” in Ad-
GPT. Furthermore, an intelligent online ChatBot empowered vancesinNeuralInformationProcessingSystems(A.Oh,T.Neumann,
by TP-GPT is launched with an interactive web interface. A. Globerson, K. Saenko, M. Hardt, and S. Levine, eds.), vol. 36,
pp.42330–42357,CurranAssociates,Inc.,2023.
Future studies will improve TP-GPT to have the ability
[19] Z. Hong, Z. Yuan, H. Chen, Q. Zhang, F. Huang, and X. Huang,
to predict future traffic conditions based on historical data, “Knowledge-to-sql: Enhancing sql generation with data expert llm,”
by possibly integrating traffic forecasting models. Besides, arXivpreprintarXiv:2402.11517,2024.
[20] Z.Cai,X.Li,B.Hui,M.Yang,B.Li,B.Li,Z.Cao,W.Li,F.Huang,
location recognition could be further enhanced leveraging the
andL.Si,“Star:Sqlguidedpre-trainingforcontext-dependenttext-to-
visual intelligence of language models in analyzing maps. sqlparsing,”arXivpreprintarXiv:2210.11888,2022.
[21] Z. Li and T. Xie, “Using llm to select the right sql query from
candidates,”arXivpreprintarXiv:2401.02115,2024.
[22] Z. Cui, M. Zhu, S. Wang, P. Wang, Y. Zhou, Q. Cao, C. Kopca, and
REFERENCES Y.Wang,“Starlabtrafficperformancescoreplatform,”2020.
[23] Z. Cui, M. Zhu, S. Wang, P. Wang, Y. Zhou, Q. Cao, C. Kopca, and
Y.Wang,“Trafficperformancescoreformeasuringtheimpactofcovid-
[1] Y.Chang,X.Wang,J.Wang,Y.Wu,L.Yang,K.Zhu,H.Chen,X.Yi, 19onurbanmobility,”2020.
C. Wang, and Y. Wang, “A survey on evaluation of large language [24] J.Wei,X.Wang,D.Schuurmans,M.Bosma,b.ichter,F.Xia,E.Chi,
models,” ACM Transactions on Intelligent Systems and Technology, Q.V.Le,andD.Zhou,“Chain-of-thoughtpromptingelicitsreasoningin
2023. Publisher:ACMNewYork,NY. largelanguagemodels,”inAdvancesinNeuralInformationProcessing
[2] M.FraiwanandN.Khasawneh,“AReviewofChatGPTApplicationsin Systems (S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho,
Education,Marketing,SoftwareEngineering,andHealthcare:Benefits, and A. Oh, eds.), vol. 35, pp. 24824–24837, Curran Associates, Inc.,
Drawbacks,andResearchDirections,”arXivpreprintarXiv:2305.00237, 2022.
2023. [25] OpenAI, “New models and developer products announced at devday,”
[3] S.Lai,Z.Xu,W.Zhang,H.Liu,andH.Xiong,“Largelanguagemodels 2023.
astrafficsignalcontrolagents:Capacityandopportunity,”arXivpreprint [26] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos,
arXiv:2312.16044,2023. S. Shakeri, E. Taropa, P. Bailey, Z. Chen, et al., “Palm 2 technical
[4] L. Wang, H. Jiang, P. Cai, D. Fu, T. Wang, Z. Cui, Y. Ren, H. Yu, report,”arXivpreprintarXiv:2305.10403,2023.
X.Wang,andY.Wang,“AccidentGPT:Accidentanalysisandprevention [27] R. S. Wong Jing Ping, Wendy Aw, “Open-sourcing sqlcoder2-15b and
from V2X Environmental Perception with Multi-modal Large Model,” sqlcoder-7b,”2023.
arXivpreprintarXiv:2312.13156,2023.
[5] Y.Ge,W.Hua,K.Mei,J.Tan,S.Xu,Z.Li,andY.Zhang,“Openagi:
When llm meets domain experts,” Advances in Neural Information
ProcessingSystems,vol.36,2024.
[6] O.TopsakalandT.C.Akinci,“Creatinglargelanguagemodelapplica-
tionsutilizinglangchain:Aprimerondevelopingllmappsfast,”vol.1,
pp.1050–1056,2023. Issue:1.
[7] X. Zhou, Z. Sun, and G. Li, “Db-gpt: Large language model meets
database,”DataScienceandEngineering,pp.1–10,2024.
[8] J.Wang,Z.Cai,Y.Chen,P.Yang,andB.Chen,“Anadvancedcontrol
strategyforconnectedautonomousvehiclesbasedonMicrosimulation
modelsatmultipleintersections,”PhysicaA:StatisticalMechanicsand
itsApplications,vol.623,p.128836,2023. Publisher:Elsevier.
[9] M. Villarreal, B. Poudel, and W. Li, “Can ChatGPT Enable ITS? The
Case of Mixed Traffic Control via Reinforcement Learning,” arXiv
preprintarXiv:2306.08094,2023.
[10] S. Zhang, D. Fu, W. Liang, Z. Zhang, B. Yu, P. Cai, and B. Yao,
“TrafficGPT:Viewing,processingandinteractingwithtrafficfoundation
models,”TransportPolicy,vol.150,pp.95–105,May2024.
[11] Y.Ren,Y.Chen,S.Liu,B.Wang,H.Yu,andZ.Cui,“TPLLM:ATraffic
Prediction Framework Based on Pretrained Large Language Models,”
arXivpreprintarXiv:2403.02221,2024.
[12] C. Sun, Y. Li, H. Li, and S. Hong, “TEST: Text prototype aligned
embedding to activate LLM’s ability for time series,” arXiv preprint
arXiv:2308.08241,2023.
[13] C.Liu,S.Yang,Q.Xu,Z.Li,C.Long,Z.Li,andR.Zhao,“Spatial-
temporal large language model for traffic prediction,” arXiv preprint
arXiv:2401.10134,2024.
[14] K. Rasul, A. Ashok, A. R. Williams, A. Khorasani, G. Adamopoulos,
R.Bhagwatkar,M.Bilosˇ,H.Ghonia,N.V.Hassen,andA.Schneider,
“Lag-llama: Towards foundation models for time series forecasting,”
arXivpreprintarXiv:2310.08278,2023.
[15] M. Jin, S. Wang, L. Ma, Z. Chu, J. Y. Zhang, X. Shi, P.-Y. Chen,
Y.Liang,Y.-F.Li,andS.Pan,“Time-llm:Timeseriesforecastingbyre-
programminglargelanguagemodels,”arXivpreprintarXiv:2310.01728,
2023.
[16] P. Wang, X. Wei, F. Hu, and W. Han, “TransGPT: Multi-modal
Generative Pre-trained Transformer for Transportation,” arXiv preprint
arXiv:2402.07233,2024.
[17] Y.Tang,Z.Wang,A.Qu,Y.Yan,K.Hou,D.Zhuang,X.Guo,J.Zhao,
Z. Zhao, and W. Ma, “Synergizing Spatial Optimization with Large
Language Models for Open-Domain Urban Itinerary Planning,” arXiv
preprintarXiv:2402.07204,2024.