PublishedasaconferencepaperatICLR2024
WHY IS SAM ROBUST TO LABEL NOISE?
ChristinaBaek ZicoKolter AditiRaghunathan
CarnegieMellonUniversity
{kbaek, zkolter, raditi}@andrew.cmu.edu
ABSTRACT
Sharpness-AwareMinimization(SAM)ismostknownforachievingstate-ofthe-art
performancesonnaturalimageandlanguagetasks. However,itsmostpronounced
improvements(oftensofpercent)isratherinthepresenceoflabelnoise. Under-
standingSAM’slabelnoiserobustnessrequiresadeparturefromcharacterizingthe
robustnessofminimaslyingin“flatter”regionsofthelosslandscape. Inparticular,
thepeakperformanceunderlabelnoiseoccurswithearlystopping,farbeforethe
lossconverges. WedecomposeSAM’srobustnessintotwoeffects: oneinduced
by changes to the logit term and the other induced by changes to the network
Jacobian. ThefirstcanbeobservedinlinearlogisticregressionwhereSAMprov-
ably up-weights the gradient contribution from clean examples. Although this
explicitup-weightingisalsoobservableinneuralnetworks,whenweinterveneand
modifySAMtoremovethiseffect,surprisingly,weseenovisibledegradationin
performance. WeinferthatSAM’seffectindeepernetworksisinsteadexplained
entirelybytheeffectSAMhasonthenetworkJacobian.Wetheoreticallyderivethe
implicitregularizationinducedbythisJacobianeffectintwolayerlinearnetworks.
Motivatedbyouranalysis,weseethatcheaperalternativestoSAMthatexplicitly
inducetheseregularizationeffectslargelyrecoverthebenefitsindeepnetworks
trainedonreal-worlddatasets.
1 INTRODUCTION
In recent years, there has been growing excitement about improving the generalization of deep
networks by regularizing the sharpness of the loss landscape. Among optimizers that explicitly
minimizesharpness,SharpnessAwareMinimization(SAM)(Foretetal.,2020)garneredpopularity
for achieving state-of-the-art performance on various natural image and language benchmarks.
Comparedtostochasticgradientdescent(SGD),SAMprovidesconsistentimprovementsofseveral
percentagepoints. Interestingly,alesswidelyknownfindingfromForetetal.(2020)isthatSAM’s
mostprominentgainslieelsewhere,inthepresenceofrandomlabelnoise. Infact,SAMismore
robusttolabelnoisethanSGDbytensofpercentagepoints, rivalingthecurrentbestlabelnoise
techniques(Jiangetal.,2017;Zhangetal.,2017;Arazoetal.,2019). InFigure1,wedemonstrate
thisfindinginCIFAR10with30%labelnoise,whereSAM’sbesttestaccuracyis17%higher. In
particular,wefindthattherobustnessgainsaremostprominentinaparticularversionofSAMcalled
1-SAMwhichappliestheperturbationsteptoeachsampleintheminibatchseparately.
Animportantcaveatabouttherandomlabelnoisesettingisthatthetestaccuracydoesnotimprove
withfurthertraining,butinsteadpeaksinthemiddle. Consequently,weargueunderstandingSAMin
thisregimerequiresadeparturefromreducingSAMtothesharpnessofitssolutionatconvergence,
butinsteadreasoningaboutSAM’s“earlylearning”behavior. Infact,eveninsettingswithaunique
minimum, the best test accuracy may change based on the optimization trajectory. Indeed, the
performanceachievedbySAMdoesnotdiminishwithunderparametrization,withthegainsabove
SGDsometimesincreasingwithmoredata(AppendixF).
Inthiswork,weinvestigatewhy1-SAMismorerobusttolabelnoisethanSGDatamoremechanistic
level. Decomposing the gradient of each example (“sample-wise” gradient) by chain rule into
∇ ℓ(f(w,x),y)=∂ℓ/∂f ·∇ f,weanalyzetheeffectofSAM’sperturbationontheterms∂ℓ/∂f
w w
(“logitscale”)and∇ f (“networkJacobian”),separately. Wemakethefollowingkeyconclusions
w
abouthowthesecomponentsimproveearly-stoppingtestaccuracy. Webeginourstudyinlinear
models,wheretheonlydifferencebetweenSAMandSGDisthelogitscaleterm. Here,weshow
thatSAMreducestoareweightingschemethatexplicitlyup-weightsthegradientcontributionof
1
4202
yaM
6
]GL.sc[
1v67630.5042:viXraPublishedasaconferencepaperatICLR2024
Test Accuracy Training Accuracy Training Loss
0.7 1.0
100
0.6 0.8
101 0.6
0.5
0.4 102
0.4
0.2 103
0.3
0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200 0 25 50 75 100 125 150 175 200
Epoch Epoch Epoch
SAM SGD Clean Noisy
Figure1:CIFAR10trainingaccuracyandlossincleanversusnoisydata. SAMachivesahigherclean
trainingaccuracybeforefittingthenoisydata,i.e.,whenaccuracyofnoisytrainingdatasurpasses
randomchance. Thiscorrespondswithahigherpeakintestaccuracy.
lowlosspoints. Thiseffectisparticularlyusefulinthepresenceofmislabeledexamples. When
trainingwithgradientdescent,correctlylabeledorcleanexamplesinitiallydominatethedirectionof
theupdateandasaresult,theircorrespondinglossdecreasesfirstbeforethatofmislabeledornoisy
examples(Liuetal.,2020;2023). Similartomanyexistinglabel-noisetechniques(Liuetal.,2020),
SAM’sexplicitup-weightingkeepsthegradientcontributionofcleanexampleslargeevenafterthey
arefit,slowingdowntherateatwhichnoisyexamplesarelearnedincomparison. Wehypothesize
thathigherpeaktestaccuracycorrespondswithachievinghighercleantrainingbeforeoverfittingto
noise. Empirically,thegapbetweenthetrainingaccuracyofcleanandnoisyexamplescloselytracks
thetestaccuracyinearlierepochs(Figure2).
In deep networks, SAM’s logit scale term empirically in-
ducesasimilareffectofexplicitlyup-weightingthegradients
Clean - Noisy Train Accuracy
oflowlossexamples. However,forthemagnitudeofSAM’s
SAM
perturbationutilizedinpractice,wefindthatSAM’slogit 0.6 SGD
scalehasnegligibleimpactonSAM’srobustness. Onthe
0.4
otherhand,justkeepingSAM’sperturbationonthenetwork
JacobiantermretainsnearlyidenticalperformancetoSAM. 0.2
Thissuggeststhatthereisafundamentallydifferentmech- 0.0
0 50 100 150 200
anismthatoriginatesinSAM’sJacobiantermthatresultsin Epoch
mostofSAM’slabelnoiserobustnessinthenonlinearcase. Figure2: SAMlearnscleanexamples
fasterthannoisyexamples.
Motivated by this finding, we analyze the Jacobian-only
SAMfora2-layerlinearnetwork,andshowthattheresulting
update decomposes into SGD with ℓ regularization on the final layer weights and intermediate
2
activations. Wearguethatthebenefitsofsuchregularizationcouldpartiallybereasonedaboutin
termsofgradientcontributionorasanimplicitup-weightingscheme. Namely,itkeepsthelossof
correctlyfitpointshighbyconstrainingthemagnitudeofthenetworkoutput. Furthermore,including
justthisregularizationfordeepnetworks,whilenotachievingthefullbenefitsofSAM,nonetheless
substantiallyimprovestheperformanceofSGD.Weemphasizethatthesemethodsaremeanttobe
illustrative,andnotintendedtobeastandalonemethodforlabelnoiserobustness. Butinthisvein,
ourfindingssuggestthatSAM’slabelnoiserobustnessmaynotcomeviasharpnesspropertiesat
convergence,butratherfromtheoptimizationtrajectorytaken.
2 PRELIMINARIES
2.1 PROBLEMSETUP
Model Weconsiderbinaryclassification. Thesignofthemodel’soutputf :Rd →Rmapsinputs
x ∈ X todiscretelabelst ∈ {−1,1}. Wewillstudytwokindsofmodels–alinearmodelanda
2-layerdeeplinearnetwork(DLN)inthiswork. Wedonotincludethebiasterm.
Linear: f(w,x)=⟨w,x⟩
(2.1)
DLN:f(v,W,x)=⟨v,Wx⟩whereW ∈Rd×h,v ∈Rh,
Letusdenotetheintermediateactivationasz =Wx∈Rh. Wewillabusethenotationtoalsorefer
toagenericparameterizedmodelasf(w,x)whenclearfromcontext.
2
ycaruccA ycaruccA
ysioNccA
naelCccA
ssoLPublishedasaconferencepaperatICLR2024
Objective Weconsiderthelogisticlossℓ(w,x,t) = −log(σ(t·f(w,x)))forsigmoidfunction
σ(z)= 1 . Givenntrainingpoints[(x ,t )]n sampledfromthedatadistributionD,our
1+exp(−z) i i i=1
trainingobjectiveis
n
1 (cid:88)
minL(w)whereL(w)= ℓ(w,x ,t ) (2.2)
w n i i
i=1
Bychainrule,wecanwritethesample-wisegradientwithrespecttothelogisticlossℓ(w,x,t)as
−∇ ℓ(x,t)=t·σ(−tf(w,x))∇ f(w,x) (2.3)
w w
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
logitscale Jacobian
Thelogitscale,whichisthemodel’sconfidencethatxbelongsintheincorrectclass−t,scalesthe
Jacobianterm. Thelogitscalegrowsmonotonicallywiththeloss. WerefertothenetworkJacobian
∇ f(w,x)asthe“Jacobianterm”.
w
Althoughourmathematicalanalysisisforbinaryclassification,thecross-entropylossformulticlass
classificationobservesasimilardecompositionforitssample-wisegradient:
−∇ ℓ(x,y)=⟨e −σ(f(w,x)),∇ f(w,x)⟩ (2.4)
w t w
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
logitscale Jacobian
whereσ(·)isthesoftmaxfunctionande istheone-hotencodingofthetargetlabel. Empirically,we
t
willobservethattheconclusionsfromourbinaryanalysistransfertomulti-class.
2.2 SHARPNESSAWAREMINIMIZATION
Sharpness-awareMinimization(SAM)(Foretetal.,2020)attemptstofindaflatminimumofthe
trainingobjective(Eq. 2.2)byminimizingthefollowingobjective
min max L(w+ϵ), (2.5)
w ∥ϵ∥2≤ρ
whereρisthemagnitudeoftheadversarialweightperturbationϵ.Theobjectivetriestofindasolution
thatliesinaregionwherethelossdoesnotfluctuatedramaticallywithanyϵ-perturbation. SAMuses
first-orderTaylorapproximationofthelosstoapproximateworst-caseϵwiththenormalizedgradient
ρ∇ L(w)/∥∇ L(w)∥.
w w
1-SAM However,thenaiveSAMupdatethatcomputesasingleϵdoesnotobserveperformance
gainsoverSGDinpracticeunlesspairedwithasmallbatchsize(Foretetal.,2020;Andriushchenko&
Flammarion,2022a). AlternativelyForetetal.(2020)proposeshardingtheminibatchandcalculating
SAM’sadversarialperturbationϵforeachshardseparately. Attheendofthisextremeis1-SAM
whichcomputesϵforthelossofeachexampleintheminibatchindividually. Formally,thiscanbe
writtenas
(cid:32) n (cid:33)
w =w−η
1 (cid:88)
∇ ℓ(w+ϵ ,x ,t ) whereϵ =ρ
∇ wℓ(x i,t i)
(2.6)
n w+ϵi i i i i ∥∇ ℓ(x ,t )∥
w i i 2
i=1
Inpractice,1-SAMistheversionofSAMthatachievesthemostperformancegain. Inthispaper,
we focus on understanding 1-SAM and will use SAM to refer to 1-SAM unless explicitly stated
otherwise.
2.3 HYBRID1-SAM
In our study of 1-SAM, we try to isolate the robustness effects of 1-SAM coming from how the
perturbationϵ affectsthelogitscalingtermandtheJacobiantermofeachsample-wiseupdate. To
i
doso,wewillalsopayattentiontothefollowingvariantsof1-SAM:
1-SAM: −∇ ℓ(w+ϵ ,x ,t )=tσ(−tf(w+ϵ ,x))∇ f(w+ϵ ,x)) (2.7)
w+ϵi i i i i w+ϵi i
LogitSAM:∆L-SAMℓ(w,x ,t )=tσ(−tf(w+ϵ ,x))∇ f(w ,x ) (2.8)
i i i w i
3PublishedasaconferencepaperatICLR2024
Test Accuracy Clean vs Noisy Grad Norm
0.75
1.1
0.70
1.0
0.65
0.9
0.60 0.8
0.7
0.55
0 200 400 600 800 1000 0 50 100 150 200
Timesteps Timesteps
(a)SAMtestaccuracyforρ∈[0,0.18].Bluercurves (b)Ratiooftheaveragesample-wisegradientnorm
denotelargerρ.Accuracyimproveswithlargerρ. ofcleanversusnoisypoints
Figure 3: Linear models trained on the toy Gaussian data using SAM. SAM’s preferential up-
weightingoflowlosspoints(right)correspondswithhigherearlystoppingtestaccuracy(left).
JacobianSAM:∆J-SAMℓ(w,x ,t )=tσ(−tf(w ,x ))∇ f(w+ϵ ,x ) (2.9)
i i i w+ϵi i i
Logit SAM(L-SAM) only applies theSAM perturbation to thelogit scale for each sample-wise
gradientwhileJacobianSAM(J-SAM)onlyappliestheSAMperturbationtotheJacobianterm. We
willobservethatindeepnetworks,J-SAMobservesclosetothesameperformanceas1-SAMwhile
L-SAMdoesnot.
3 LINEAR MODELS: SAM UP-WEIGHTS THE GRADIENTS OF LOW LOSS
EXAMPLES
Wefirststudytherobustnessaffectsofthelogitscaleterminlinearmodels.NotonlycanSAM’sboost
inearly-stoppingperformancebeobservedinlinearmodels,buttheJacobianterm∇ f(w,x)=x
w
forlinearmodelsandisindependentoftheweights. Thus,anyrobustnessobtainedbySAMinthis
settingisspecificallyduetoSAM’slogitscale.
3.1 EXPERIMENTALINVESTIGATIONINTOSAMEFFECTINLINEARMODELS
WetrainourlinearmodelsoverthefollowingtoyGaussiandatadistributionD :Rd×{−1,1}.
TrueLabel: y ∼{−1,1}byflippingfaircoin
(cid:20) (cid:18) γ2 (cid:19)(cid:21)
Input: x∼y· B ∈R,z ∼N 0, I (3.1)
d−1 d−1×d−1
Target: t=y·εwhereε∼{−1 w.p∆, 1 w.p(1−∆)}
In x, the first dimension contains the true signal yB while the remaining d − 1 dimensions is
uncorrelatedGaussiannoise. Wesampletrainingdatapoints(x,t)fromthisdistributionwherethe
randomlabelnoiseεcorrupts∆ofthetrainingdata. Weexpect∆<0.5meaningthemajorityofthe
dataisstillclean. Inthetestdata,weassumetherearenomislabeledexamples(t=y).
InFigure3,wecomparetheperformanceoffull-batchgradientdescentandSAMonourtoyGaussian
datawith40%labelnoise(SeeAppendixAforexperimentaldetails). Eveninthissimplesetting
SAMobservesnoticeablyhigherearlystoppingtestaccuracyoverSGD.Werunagridsearchforρ
between0and0.18andobservethattheearlystoppingtestaccuracymonotonicallyincreaseswithρ.
Correlatedwiththisimprovedperformance,whenweplottheaveragesample-wisegradientnorm
ofcleanexamplesversusnoisyexamplesalongtraining,weobservethatthisratiodecaysslower
withlargerρ. ThisleadsustosuspectthatSAM’ssuperiortestperformanceisduetocleanexamples
dominatingthegradientupdateforlonger. Previously,Liuetal.(2020)provedthatatthebeginning
ofgradientdescenttraininginlinearmodels,thecleanmajoritydominatesthegradient,causingthe
cleanexamplestobefitfirst. However,thedynamicquicklychangesastrainingprogresses. Asthe
lossofcleanpointsquicklydecays,thecontributionofnoisyoutlierstothegradientupdatebeginsto
outweighthatofthecleanexamples. Thisresultsinthetestaccuracydroppingbackdown.
4
ycaruccA
mroN
ysioN/mroN
naelCPublishedasaconferencepaperatICLR2024
Inthenextsection,wewillprovethatSAM,byvirtueofitsadversarialweightperturbation,preferen-
tiallyup-weightsthegradientsignalfromlow-losspoints. Thiskeepsthegradientcontributionfrom
cleanpointshighintheearlytrainingepochs. Consequently,SAMachieveshighertestaccuracyby
prioritizingfittingmorecleantrainingdatabeforeoverfittingtonoise.
3.2 ANALYSISOFSAM’SLOGITSCALE
Recallthat1-SAMupdateofdatapoint(x ,t )isthegradientevaluatedattheweightsperturbedby
i i
thenormalizedsample-wisegradient. Inlinearmodels,theperturbationissimplytheexamplescaled
bythelabelϵ i =−ρt i∥x xi i∥. Evaluatingthegradientatthisperturbedweight,SAM’supdatereduces
toaconstantadjustmentofthelogitscalebythenormofthedatapoint.
−∇ ℓ(w+ϵ ,x ,y )=t σ(−t <w,x >+ ρ∥x ∥ )x (3.2)
w+ϵi i i i i i i i 2 i
(cid:124) (cid:123)(cid:122) (cid:125)
constantadjustment
Since the sigmoid function σ increases monotonically, the extra positive constant increases the
gradientnormofalltrainingpoints. However,amongpointsofthesamenorm∥x ∥,thisadjustment
i
causeslowlosspointswheretheconfidencetowardstheincorrectclassσ(−t ⟨w,x ⟩)issmalltobe
i i
up-weightedbyalargercoefficientthanhighlosspointswheretheincorrectclassconfidenceishigh,
asproveninthefollowinglemma.
Lemma3.1(Preferentialup-weightingoflowlosspoints) Considerthefollowingfunction.
σ(−z+C) 1+exp(z)
f(z)= = (3.3)
σ(−z) 1+exp(z−C)
ThisfunctionisstrictlyincreasingifC >0.
Proof Thefirstderivativeisnon-negative.
df exp(z) (1+exp(z))exp(z−C)
= − (3.4)
dz 1+exp(z−C) (1+exp(z−C))2
exp(z)(1−exp(−C)
= >0 ∀z ∈R (3.5)
(1+exp(z−C))2
Wecaninterpret1-SAMasagradientreweightingscheme,wheretheweightforexamplex issetto
i
∥∇ ℓ(w+ϵ ,x ,t )∥ σ(−t ⟨w,x ⟩+ρ∥x ∥)
w+ϵi i i i = i i i . (3.6)
∥∇ ℓ(w,x ,t )∥ σ(−t ⟨w,x ⟩)
w i i i i
WedirectlyapplyLemma3.1bysettingz =t ⟨w,x ⟩andC =ρ∥x ∥ toprovethatacrosspointsof
i i i 2
thesamenorm,lowlosspointsaremoreaffectedbytheup-weightingthanhighlosspoints. Sincethe
lossdecreasesfasterforcleanpointsovernoisypoints,SAMpreferentiallyup-weightsthegradients
ofcleanpoints.
ThefactthattheresultingearlystoppingtestaccuracyofSAMishigherthangradientdescentfollows
byinduction. Forourtoydatadistribution,wenotethattestaccuracysaturatesasρgoestoinfinity.
Inparticular,1-SAMconvergestotheBayesoptimalsolution. Furthermore,consistentwithprevious
literature,wefindthateveninthislinearsetting,1-SAMbehavesverydifferentlyfromthenaive
SAMupdate(Section2.2);thelatterdoesnotachieveanyperformancegainsfromSGD.Wefurther
discusstheasymptoticbehaviorof1-SAMandthebehaviorofn-SAMinAppendixB.
4 NEURAL NETWORKS: SAM’S ROBUSTNESS COMES FROM THE JACOBIAN
From our linear analysis, we learned that SAM’s logit scale term preferentially up-weights the
gradientsoflow-losspoints. WeverifythatthiseffectofSAMisalsovisibleinneuralnetworks.
Whiletheeffectpersists,wewillshowthat,onthecontrary,1-SAM’slogitscalecomponentisnotthe
mainreasonforSAM’simprovedrobustnessindeepnetworks. Inparticular,justapplyingthe1-SAM
perturbationtotheJacobianterm(J-SAM)recoversmostof1-SAM’sgainswhilelogitreweighting
alone (L-SAM) cannot. Finally, we find that a large proportion of the performance gains can be
recoveredthroughasimplerregularizationthatmimicstheeffectofthe1-SAMJacobianonjustthe
lastlayerweightsandactivations.
5PublishedasaconferencepaperatICLR2024
DLN SAM Logit Scale Norm 2NN SAM Logit Scale Norm 1.5×100 ResNet SAM Logit Scale Norm
2.2 1.08 1.4×100
2.0
1.06 1.8
1.3×100
1.6 1.2×100
1.04 1.4
1.1×100
1.2
1.02
1.0 100
0 50 100 150 200 0 50 100 150 200 0 50 100 150
Epoch Epoch Epoch
Clean Noisy
DLN Test Accuracy 2NN Test Accuracy ResNet Test Accuracy
0.40 0.6 0.7
0.39 0.5 0.6
0.38 0.5
0.4
0.37 0.4
0.36 0.3 0.3
0.35 0.2 0.2
0.1
0 50 100 150 200 0 50 100 150 200 0 50 100 150 200
Epoch
SAM J-SAM SGD L-SAM
Figure4: In2-layerdeeplinearnetworks(DLN),2-layerMLPwithReLUactivation(2NN),and
ResNet18trainedonnoisyCIFAR10,weobservethatSAM’sperturbationtothelogitscalepreferen-
tiallyupweightsthegradientnormforcleanexamples(toprow). Yet,J-SAMi.e. SAMabsentthe
explicitreweightingeffect,preservesSAM’slabelnoiserobustness(bottomrow).
4.1 EXPLICITREWEIGHTINGDOESNOTFULLYEXPLAINSAM’SGAINS
Wefirstnotethattheupweightingoflow-losspointscanbeobservedeveninmulti-classclassification
withneuralnetworks. Recallthegeneralformofthegradientformulti-classcross-entropylossis
∇ ℓ(x,t)=⟨σ(f(w,x))−e ,∇ f(w,x)⟩whereσisthesoftmaxfunctionande istheone-hot
w t w t
encodingofthelabelt. Theanalogous“logitscale”componentformulticlassclassificationistheK
dimensionalvectorg(w,x,t)=σ(f(w,x))−e whoseℓ normscaleswiththeloss. InFigure4,we
t 2
measurethechangeinthenormofthisquantityforeachexamplex intheminibatchaftertheSAM
i
perturbation,i.e.,∥g(w+ϵ ,x ,t )∥/∥g(w,x ,t )∥. InResNet18trainedonCIFAR10usingSAM,
i i i i i
weindeedobservethatthisquantityishigherincleanexamplesthannoisyexamples(Figure4).
ThisresultaloneseeminglyimpliesthattheexplicitreweightingalsoexplainsSAM’srobustnessin
deepermodels. However,whenweisolatethiseffectbyapplyingSAMtojustthelogitscaleterm,
i.e.,L-SAM,withtheoptimalperturbationmagnitudefoundforSAMρ=0.01,weobservemarginal
early-stoppingperformancegainsaboveSGD(Figure4). Alternatively,J-SAMrecoversalmostallof
thegainsofSAM.ThissuggeststhatSAM’sreweightingofthelogitscaleisnotthemaincontributor
to SAM’s robustness in neural networks. We also find that this observation does not require an
arbitrarilydeepnetworkandalsoholdstrueinsimple2-layerdeeplinearnetworks(DLN)andReLU
MLP’s(2NN)trainedonflattenedCIFAR10imageswith30%labelnoise. Asimilaranalysisof
SAMunderlabelnoiseinlinearmodelswasconductedbyAndriushchenko&Flammarion(2022b),
howevertheyattributethelabelnoiserobustnessinneuralnetworkstologitscaling. Weclaimthe
opposite: thatthedirectionorthenetworkJacobianofSAM’supdatebecomesmuchmoreimportant.
4.2 ANALYSIS
Motivatedbythis,westudytheeffectofperturbingtheJacobianinasimple2-layerDLN.Inthe
linearcase,theJacobiantermwasconstant,butinthenonlinearcasetheJacobianisalsosensitive
toperturbation. Inparticular,for2-layerDLNs,J-SAMregularizesthenormoftheintermediate
activationsandlastlayerweights,asprovenbythefollowingproposition.
Proposition4.1 Forbinaryclassificationina2-layerdeeplinearnetworkf(v,W,x) = ⟨v,Wx⟩,
J-SAMapproximatelyreducestoSGDwithL2normpenaltyontheintermediateactivationsandlast
layerweights.
6
lanigirO
/
debrutreP
lanigirO
/
debrutreP
lanigirO
/
debrutreP
ycaruccAPublishedasaconferencepaperatICLR2024
Figure5: WhentrainingResNet18withSAM,thenormofthefinalintermediateactivationsandlast
layerweightsdecreasessignificantly,consistentwith2-layerDLNanalysis.
Proof WewritetheformoftheJ-SAMupdateforthefirstlayerW ofthedeeplinearnetwork:
(cid:16) ρ (cid:17)
−∇ ℓ(w+ϵ,x,t)=σ(−tf(w,x)) tv− z x⊤ (4.1)
W+ϵ(1) J
ρσ(−tf(w,x))
=−∇ ℓ(w,x,t)− zx⊤ (4.2)
W J
(cid:112)
where z = Wx is the intermediate activation and J = ∥∇f(x)∥ = ∥z∥2+∥x∥2∥v∥2 is a
normalizationfactor. Inthesecondlayer,thegradientwithrespecttovis
(cid:18) ρ∥x∥2 (cid:19)
−∇ ℓ(w+ϵ,x,t)=σ(−tf(w,x)) tz− v (4.3)
v+ϵ(2) J
ρσ(−tf(w,x))∥x∥2
=−∇ ℓ(w,x,t)− v (4.4)
v J
FromEquation4.1,notethatSAMaddsanactivationnormregularizationtothefirstlayerzx⊤ =
∇ 1∥z∥2scaledbysomescalardependentonρ,f(w,x),andJ. Similarly,fromEquation4.3,note
W2 2
thatSAMaddsaweightnormpenaltytothesecondlayerweightsv = ∇ 1∥v∥2 alsomultiplied
v2 2
bysomescalar. ThenormalizationfactorJ scalestheregularizationbeclosertothenormthanthe
squarednorm.
Empirically,weseethatasimilareffectispresentinSAMfordeepernetworks. Asnetworksbecome
deeper,theexactJ-SAMupdatebecomesmorecomplicatedthansimplySGDwithweightandfeature
normregularization. However,whenwetrainResNet18withSAMandmeasurethenormofthe
final intermediate activations and last layer weights, the norms still decrease significantly in this
setting(Figure5).
4.3 EXPERIMENTS
Inspired by our analysis, we train deep networks using SGD with ℓ regularization on the last
2
layer weights and last hidden layer intermediate activations. Namely, we simplify down SAM’s
regularizationtothefollowingobjective
N
1 (cid:88)
minL(w)+γ ∥z∥ +γ ∥v∥2 (4.5)
w
zN 2 v 2
i=1
wherevisthelastlayerweightsandzisthelasthiddenlayerintermediateactivation.
WeconductourexperimentsonCIFAR10withResNet18. 1-SAMleadstounstableoptimization
with batch normalization as it requires passing through the datapoints individually through the
network. Thus, we replace all batch normalization layers with layer normalization. Keeping all
otherhyperparameterssuchaslearningrate,weightdecay,andbatchsizethesame,wecompare
theperformanceofSGD,1-SAM,L-SAM,J-SAM,andtheregularizedSGD(Eq. 4.5). Although
regularizedSGDdoesnotachieveexactlythesametestaccuracyasSAM,thegapissignificantly
closedfrom17%to9%. Ontheotherhand,undernolabelnoise,regularizedSGDhasaneglible
improvementof1%withhyperparametersearch,whileSAMstillachieves8%highertestaccuracy
thanSGD(Figure9). Thus,whileinsufficienttoexplainallofSAM’sgeneralizationbenefits,we
suspectthatasimilarregularizationoffinallayersinSAMisparticularlyimportantforgeneralization
underheavylabelnoise.
7PublishedasaconferencepaperatICLR2024
Algorithm BestTestAccuracy
1-SAM(ρ=0.01) 69.47%
L-SAM(ρ=0.01) 54.13%
J-SAM(ρ=0.01) 69.17%
SGD 52.48%
SGDw/ProposedReg 60.8%
Table 1: Adding our proposed regularization on the last layer weights and logits boosts SGD
performanceby8%.
4.4 CONNECTIONBETWEENLOGITSCALEANDJACOBIANTERMS
WehypothesizethatthebenefitsofregularizingtheJacobiancouldalsopartiallybereasonedabout
intermsofgradientcontributionorasanimplicitup-weightingscheme. Byregularizingthenormof
theweightsandfeatures,themagnitudeofthenetworkoutputremainssmall. Given∥f(x)∥ ≤C,
2
notethatthelossofanysingledatapointislowerandupperboundedbylog(1+(K−1)exp(−C))
andlog(1+(K−1)exp(C)),respectively. BykeepingC small,cleantraininglossmayremain
non-negligibleevenasthecleantrainingaccuracyincreases,andthelossofnoisymisfitexamplesis
capped. Indeed,ascanbeobservedinFigure1and7,thelossofcleanexamplesismuchhigherin
SAMthanSGDforthesamegapbetweencleanandnoisytrainingaccuracy. Inlinearmodels,weight
decayandSAM’slogitscaleadjustmenthaveequivalenteffects(AppendixB).Asmallweightnorm
andSAM’slogitscalebothbalancethegradientcontributionofexamples. Furthermore,weightdecay
andsmallinitializationiswell-knowntoimproverobustnessinoverparametrizedsettings(Advani&
Saxe,2017).
5 RELATED WORK
5.1 IMPLICITBIASOFSAM
WhiletheoverallmechanicsofSAMremainspoorlyunderstood,severalpapershaveindependently
triedtoelucidatewhytheper-exampleregularizationof1-SAMmaybeimportant. Andriushchenko
&Flammarion(2022a)showthatinsparseregressionondiagonallinearnetworks,1-SAMismore
biasedtowardssparserweightsthannaiveSAM.Wenetal.(2022)differentiate1-SAMandnaive
SAMbyprovingthattheexactnotionof“flatness”thateachalgorithmregularizesisdifferent. Our
observationoffeatureregularizationisofcloseconnectiontoAndriushchenkoetal.(2023a)which
showthatSAMdrivesdownthenormoftheintermediateactivationsina2layerReLUnetwork
andthisimplicitlybiasestheactivationstobelowrank. Mengetal.(2023)analyzetheper-example
gradientnormpenalty,whichalsoeffectivelyminimizessharpnessandshowthatifthedatahaslow
signal-to-noiseratio,penalizingtheper-examplegradientnormdampensthenoiseallowingmore
signallearning. RecentworksalsomakeconnectionsbetweennaiveSAMandgeneralization(Behdin
&Mazumder,2023;Chenetal.,2024). Ouranalysisdifferfrompreviousworksaswefocuson
understanding1-SAM’srobustnesstorandomlabelnoise,whichrequiresreasoningaboutthebest
notfinaltestaccuracy. AlthoughthisistheregimewhereSAM’sachievementsarethemostnotable,
ithasnotbeenthoroughlystudiedinpreviousworkswhichfocusonSAM’ssolutionatconvergence.
5.2 CONNECTIONBETWEENSHARPNESSANDGENERALIZATION
Sharpnessofthelosslandscapehasusedindeeplearningasageneralizationmeasure(Hochreiter&
Schmidhuber,1997;Dziugaite&Roy,2017;Bartlettetal.,2017;Neyshaburetal.,2017). Afamily
ofoptimizationchoicesincludingminibatchnoiseinstochasticgradientdescent(SGD),largeinitial
learningrate,anddropouthaveshowntoimplicitlyregularizethemodeltowardssolutionslyingin
flatbasinsorareasoflowcurvature(Keskaretal.,2016;Dziugaite&Roy,2017;Cohenetal.,2021;
Damianetal.,2022;Nar&Sastry,2018;Weietal.,2020;Damianetal.,2021;Orvietoetal.,2022;
Jastrzebskietal.,2021). However,thenegativecorrelationbetweengeneralizationandsharpnessis
notuniversallytrue,butspecifictocertainoptimizationchoices(Jiangetal.,2019). Forexample,
strongdataaugmentationmayimprovegeneralizationbutsharpenthelandscape(Andriushchenko
etal.,2023b). Sharpnessregularizationalsoappearsinadversarialweightrobustness(Wuetal.,2020;
8PublishedasaconferencepaperatICLR2024
Zhengetal.,2021). However,adversarialweightrobustnesshasnodirectconnectiontothelabel
noiserobustnesswestudyinourwork.
5.3 LEARNINGWITHLABELNOISE
MislabeleddataisapersistentproblemevenincommonbenchmarkssuchasCIFARandImageNet
(Mu¨ller&Markert,2019)andtheycanhavesignificantimpactonmodelperformance(Nakkiran
etal.,2021;Rolnicketal.,2018;Gunasekaretal.,2023;Westetal.,2021). Ourconclusions,which
reducegeneralizationtohowmanymorecleanexamplesarelearnedfirst,istiedtopreviousliterature
onthefasterlearningtimeofcleanversusnoisyexamplesingradientbasedoptimization. Liuetal.
(2020)firstprovethisinlinearmodels. Morerecently,Liuetal.(2023)showedthatasimilareffect
isobservableinneuralnetworks,wherethegradientovercleanandnoisyexampleshavenegative
cosinesimilarityatthebeginningoftraining,andwemayreasonaboutearlylearningofcleanpoints
bythemagnitudeoftheircontributiontotheaveragegradient. Manymetricsinpracticealsouse
learningtimetoidentifyexamplesthatarememorized(Zhang&Sabuncu,2018;Leeetal.,2019;
Chen et al., 2019; Huang et al., 2019; Jiang et al., 2017; 2020a; Carlini et al., 2019; Jiang et al.,
2020b;Arazoetal.,2019;Liuetal.,2022).
6 DISCUSSION, LIMITATIONS, AND CONCLUSION
AlthoughtheSAMoptimizerhasprovenverysuccessfulinpractice,thereisanotabledividebetween
theestablishedmotivationforSAMoffindingaflatminimum, andtheempiricalgainsachieved.
Fundamentally,theworkwepresenthereaimstojustifytheusageofSAMbyappealingtoavery
differentsetofprinciplesthanthoseusedtooriginallyderivethealgorithm. Specifically,weshow
thatinthelinearandnonlinearcases,thereisanextenttowhichSAM“merely”actsbylearning
morecleanexamplesbeforefittingnoisyexamples. Thisprovidesanaturalperspectiveuponwhich
toanalyzethestrongperformanceofSAM,especiallyinthesettingoflabelnoise.
Inthelinearsetting,weidentifiedthatSAMexplicitlyup-weightsthegradientsignalfromlowloss
points. Thisisquitesimilartowellknownlabelnoiserobustnessmethods(Liuetal.,2022;2020)
whichalsoutilizelearningtimeasaproxyfordistinguishingbetweencleanandnoisyexamples. In
thenonlinearsetting,weidentifyarguablyamoreinterestingphenomena–howcleanexamplesare
fitcanaffectthelearningtimeofnoisyexamples. WeshowthatSAMregularizesthenormofthe
intermediateactivationsandfinallayerweightsthroughouttrainingandthisimproveslabelnoise
robustness. Ultimately,theeffectmaybesimilartolabelnoiserobustnessmethodsthatregularizeor
clipthenormofthelogits(Weietal.,2023).
Finally,weemphasizethatdespitetheircloseconnection,SAMhasbeensurprisinglyunderexploredin
thelabelnoisesetting.Theresearchcommunityhasdevelopedanumberofmethodsforunderstanding
andadjustingtolabelnoise,andithassofarbeenamysteryastohowSAMmanagestounintentionally
matchtheperformanceofsuchmethods. Empiricallyhowever,wefindthatsimulatingevenpartial
aspectsofSAM’sregularizationofthenetworkJacobiancanlargelypreserveSAM’sperformance.
Asasecondaryeffectofthisresearch,wehopeourconclusionscaninspirelabel-noiserobustness
methodsthatmayultimatelyhavesimilarbenefitstoSAM(butideally,withouttheadditionalruntime
costincurredbyshardingthegradientsin1-SAM).
7 ACKNOWLEDGEMENTS AND DISCLOSURE OF FUNDING
WethankHosseinMobahi,BehnamNeyshabur,DaraBahri,ShankarKrishnanfortheirinsightsand
guidancewhichgreatlyshapedourunderstandingofSAM.WethankJeremyCohen,TengyuMa,
Maksym Andriushchenko, and Tanya Marwah for the many helpful discussions about sharpness.
We thank Jacob Springer, Pratyush Maini, and Alex Li for discussions about memorization, and
assistance with building our codebase. We thank Anna Bair and Sanket Mehta for their insights
abouthowSAMbehavesinpractice. ThisworkwassupportedinpartbyBoschCenterforArtificial
IntelligenceandtheAI2050programatSchmidtSciences(Grant#G2264481).
9PublishedasaconferencepaperatICLR2024
REFERENCES
MadhuS.AdvaniandAndrewM.Saxe. High-dimensionaldynamicsofgeneralizationerrorinneural
networks,2017.
MaksymAndriushchenkoandNicolasFlammarion. Towardsunderstandingsharpness-awaremini-
mization. InInternationalConferenceonMachineLearning,pp.639–668.PMLR,2022a.
MaksymAndriushchenkoandNicolasFlammarion. Understandingsharpness-awareminimization,
2022b. URLhttps://openreview.net/forum?id=qXa0nhTRZGV.
MaksymAndriushchenko,DaraBahri,HosseinMobahi,andNicolasFlammarion. Sharpness-aware
minimizationleadstolow-rankfeatures. arXivpreprintarXiv:2305.16292,2023a.
MaksymAndriushchenko,FrancescoCroce,MaximilianMu¨ller,MatthiasHein,andNicolasFlam-
marion. Amodernlookattherelationshipbetweensharpnessandgeneralization. arXivpreprint
arXiv:2302.07011,2023b.
EricArazo,DiegoOrtego,PaulAlbert,NoelO’Connor,andKevinMcGuinness. Unsupervisedlabel
noisemodelingandlosscorrection. InInternationalconferenceonmachinelearning,pp.312–321.
PMLR,2019.
PeterLBartlett,DylanJFoster,andMatusJTelgarsky. Spectrally-normalizedmarginboundsfor
neuralnetworks. Advancesinneuralinformationprocessingsystems,30,2017.
KayhanBehdinandRahulMazumder. Onstatisticalpropertiesofsharpness-awareminimization:
Provableguarantees,2023.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and
Qiao Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL
http://github.com/google/jax.
NicholasCarlini,UlfarErlingsson,andNicolasPapernot. Distributiondensity,tails,andoutliersin
machinelearning: Metricsandapplications. arXivpreprintarXiv:1910.13427,2019.
PengfeiChen,BenBenLiao,GuangyongChen,andShengyuZhang. Understandingandutilizing
deepneuralnetworkstrainedwithnoisylabels. InInternationalConferenceonMachineLearning,
pp.1062–1070.PMLR,2019.
ZixiangChen,JunkaiZhang,YiwenKou,XiangningChen,Cho-JuiHsieh,andQuanquanGu. Why
doessharpness-awareminimizationgeneralizebetterthansgd? AdvancesinNeuralInformation
ProcessingSystems,36,2024.
JeremyMCohen,SimranKaur,YuanzhiLi,JZicoKolter,andAmeetTalwalkar. Gradientdescenton
neuralnetworkstypicallyoccursattheedgeofstability. arXivpreprintarXiv:2103.00065,2021.
AlexDamian,TengyuMa,andJasonDLee. Labelnoisesgdprovablyprefersflatglobalminimizers.
AdvancesinNeuralInformationProcessingSystems,34:27449–27461,2021.
AlexDamian,EshaanNichani,andJasonDLee. Self-stabilization: Theimplicitbiasofgradient
descentattheedgeofstability. arXivpreprintarXiv:2209.15594,2022.
GintareKarolinaDziugaiteandDanielMRoy. Computingnonvacuousgeneralizationboundsfor
deep(stochastic)neuralnetworkswithmanymoreparametersthantrainingdata. arXivpreprint
arXiv:1703.11008,2017.
PierreForet,ArielKleiner,HosseinMobahi,andBehnamNeyshabur. Sharpness-awareminimization
forefficientlyimprovinggeneralization. arXivpreprintarXiv:2010.01412,2020.
SuriyaGunasekar,YiZhang,JyotiAneja,CaioCe´sarTeodoroMendes,AllieDelGiorno,Sivakanth
Gopi,MojanJavaheripi,PieroKauffmann,GustavodeRosa,OlliSaarikivi,etal. Textbooksareall
youneed. arXivpreprintarXiv:2306.11644,2023.
SeppHochreiterandJu¨rgenSchmidhuber. Flatminima. Neuralcomputation,9(1):1–42,1997.
10PublishedasaconferencepaperatICLR2024
JinchiHuang,LieQu,RongfeiJia,andBinqiangZhao. O2u-net: Asimplenoisylabeldetection
approachfordeepneuralnetworks. InProceedingsoftheIEEE/CVFinternationalconferenceon
computervision,pp.3326–3334,2019.
StanislawJastrzebski,DevanshArpit,OliverAstrand,GiancarloBKerg,HuanWang,CaimingXiong,
RichardSocher,KyunghyunCho,andKrzysztofJGeras. Catastrophicfisherexplosion: Early
phasefishermatriximpactsgeneralization. InInternationalConferenceonMachineLearning,pp.
4772–4784.PMLR,2021.
LJiang,ZZhou,TLeung,LJLi,andLFei-Fei. Mentornet: Regularizingverydeepneuralnetworks
oncorruptedlabels. arXivpreprintarXiv:1712.05055,2017.
Lu Jiang, Di Huang, Mason Liu, and Weilong Yang. Beyond synthetic noise: Deep learning on
controllednoisylabels. InInternationalconferenceonmachinelearning,pp.4804–4815.PMLR,
2020a.
YidingJiang,BehnamNeyshabur,HosseinMobahi,DilipKrishnan,andSamyBengio. Fantastic
generalizationmeasuresandwheretofindthem. arXivpreprintarXiv:1912.02178,2019.
Ziheng Jiang, Chiyuan Zhang, Kunal Talwar, and Michael C Mozer. Characterizing structural
regularitiesoflabeleddatainoverparameterizedmodels. arXivpreprintarXiv:2002.03206,2020b.
NitishShirishKeskar,DheevatsaMudigere,JorgeNocedal,MikhailSmelyanskiy,andPingTakPeter
Tang. Onlarge-batchtrainingfordeeplearning: Generalizationgapandsharpminima. arXiv
preprintarXiv:1609.04836,2016.
KiminLee,SukminYun,KibokLee,HonglakLee,BoLi,andJinwooShin. Robustinferencevia
generativeclassifiersforhandlingnoisylabels. InInternationalconferenceonmachinelearning,
pp.3763–3772.PMLR,2019.
ChaoyueLiu,AmirhesamAbedsoltan,andMikhailBelkin. Onemergenceofclean-prioritylearning
inearlystoppedneuralnetworks. arXivpreprintarXiv:2306.02533,2023.
ShengLiu,JonathanNiles-Weed,NargesRazavian,andCarlosFernandez-Granda. Early-learning
regularizationpreventsmemorizationofnoisylabels. Advancesinneuralinformationprocessing
systems,33:20331–20342,2020.
Sheng Liu, Zhihui Zhu, Qing Qu, and Chong You. Robust training under label noise by over-
parameterization. InInternationalConferenceonMachineLearning,pp.14153–14172.PMLR,
2022.
Xuran Meng, Yuan Cao, and Difan Zou. Per-example gradient regularization improves learning
signalsfromnoisydata. arXivpreprintarXiv:2303.17940,2023.
NicolasMMu¨llerandKarlaMarkert. Identifyingmislabeledinstancesinclassificationdatasets. In
2019InternationalJointConferenceonNeuralNetworks(IJCNN),pp.1–8.IEEE,2019.
PreetumNakkiran,GalKaplun,YaminiBansal,TristanYang,BoazBarak,andIlyaSutskever. Deep
doubledescent:Wherebiggermodelsandmoredatahurt.JournalofStatisticalMechanics:Theory
andExperiment,2021(12):124003,2021.
KamilNarandShankarSastry. Stepsizemattersindeeplearning. AdvancesinNeuralInformation
ProcessingSystems,31,2018.
BehnamNeyshabur,SrinadhBhojanapalli,DavidMcAllester,andNatiSrebro. Exploringgeneraliza-
tionindeeplearning. Advancesinneuralinformationprocessingsystems,30,2017.
AntonioOrvieto,HansKersting,FrankProske,FrancisBach,andAurelienLucchi. Anticorrelated
noiseinjectionforimprovedgeneralization. InInternationalConferenceonMachineLearning,pp.
17094–17116.PMLR,2022.
DavidRolnick,AndreasVeit,SergeBelongie,andNirShavit. Deeplearningisrobusttomassive
labelnoise,2018.
ColinWei,ShamKakade,andTengyuMa. Theimplicitandexplicitregularizationeffectsofdropout.
InInternationalconferenceonmachinelearning,pp.10181–10192.PMLR,2020.
11PublishedasaconferencepaperatICLR2024
Hongxin Wei, Huiping Zhuang, Renchunzi Xie, Lei Feng, Gang Niu, Bo An, and Yixuan Li.
Mitigating memorization of noisy labels by clipping the model prediction. In International
ConferenceonMachineLearning,pp.36868–36886.PMLR,2023.
Kaiyue Wen, Tengyu Ma, and Zhiyuan Li. How does sharpness-aware minimization minimize
sharpness? arXivpreprintarXiv:2211.05729,2022.
PeterWest,ChandraBhagavatula,JackHessel,JenaDHwang,LiweiJiang,RonanLeBras,Ximing
Lu,SeanWelleck,andYejinChoi.Symbolicknowledgedistillation:fromgenerallanguagemodels
tocommonsensemodels. arXivpreprintarXiv:2110.07178,2021.
DongxianWu,Shu-TaoXia,andYisenWang. Adversarialweightperturbationhelpsrobustgeneral-
ization. AdvancesinNeuralInformationProcessingSystems,33:2958–2969,2020.
HongyiZhang,MoustaphaCisse,YannNDauphin,andDavidLopez-Paz. mixup: Beyondempirical
riskminimization. arXivpreprintarXiv:1710.09412,2017.
ZhiluZhangandMertSabuncu. Generalizedcrossentropylossfortrainingdeepneuralnetworks
withnoisylabels. Advancesinneuralinformationprocessingsystems,31,2018.
Yaowei Zheng, Richong Zhang, and Yongyi Mao. Regularizing neural networks via adversarial
modelperturbation. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.8156–8165,2021.
12PublishedasaconferencepaperatICLR2024
A METHODS
AllcodewasimplementedinJAX(Bradburyetal.,2018),andweutilizetheFlaxneuralnetwork
library. Weutilizeanormallydistributedrandominitializationscheme. Ourexperimentswererunon
NVIDIAQuadroRTXA6000.
A.1 SYNTHETICLABELNOISE
We add synthetic label noise in the following manner. We randomly select ∆ proportion of the
training data to corrupt. For each datapoint (x,y), we select corrupted label t randomly from
t∈{i∈[k]K |i̸=y}.
k=1
A.2 EXPERIMENTSONCIFAR10
ForexperimentsconductedonCIFAR10,wetrainwith30%labelcorruption. Wehyperparameter
tuned the model for the best learning rate using SGD, and then hyperparameter tune 1-SAM’s
hyperparameterρutilizingthesamelearningrate. Wedonotutilizeanylearningratescheduleor
dataaugmentationotherthannormalizingtheimagechannelsusingmean(0.4914,0.4822,0.4465)
andstandarddeviation(0.2023,0.1994,0.2010).
ResNet18 WehavemodifiedtheResNet18architecturebyreplacingallBatchNormlayerswith
LayerNorm. Thisisnecessarytosafelyrun1-SAMwhichrequiresaseparateforwardpassforeach
datapointintheminibatch.
Parameter Value
Batchsize 128
Learningrate 0.01
Weightdecay 0.0005
Epochs 200
ρ(forSAM) 0.01
2LayerDLN/MLPwithReLU Wedonotincludebiasatanylayer. Thewidthoftheintermediate
layerissetto500. WeusethesamehyperparametersastheResNet18experiments.
A.3 EXPERIMENTSONTOYDATA
Linear Wesettheparametersofourtoydatadistributiontobethefollowing
Parameter Value
∆ 0.4
B 2
γ 1
d 1000
Trainingsamples 500
Testsamples 1000
Thereisnoweightdecay. Learningrateissetto0.01.
13PublishedasaconferencepaperatICLR2024
B LINEAR MODEL ANALYSIS
ToyDataSetting WetrainourlinearmodelsoverthefollowingtoyGaussiandatadistribution
D :Rd×{−1,1}.
TrueLabel: y ∼{−1,1}byflippingfaircoin
(cid:20) (cid:18) γ2 (cid:19)(cid:21)
Input: x∼y· B ∈R,z ∼N 0, I (B.1)
d−1 d−1×d−1
Target: t=y·εwhereε∼{−1 w.p∆, 1 w.p(1−∆)}
Thetestdataisgeneratedfromarelateddistributionwherethetargetisnoiselesst=y.
TestAccuracy Theexpectedaccuracyoverthetestdistributioncanbewrittenas
(cid:104) (cid:104) (cid:105)(cid:105) (cid:16) (cid:17)
Acc(w)=E 1 y(w⊤x)>0 =P y(w⊤x)>0 (B.2)
x,y∼Dtest
√
(cid:18) (cid:19) (cid:18) (cid:19)
γ B d−1w
=P √ w⊤ z>−w B =1−Φ − 1 (B.3)
z∼N(0,I) d−1 1+ 1 γ∥w 1+∥
wherew ∈Rd−2denotesthevectorconsistingoftheentriesinwexcludingthefirst. Therefore,
1+
theaccuracymonotonicallyincreaseswithw /∥w ∥andoptimallinearclassifierinthissettingis
1 1+
proportionaltothefirstelementaryvector:
w∗ ∝e (B.4)
1
B.1 1-SAMASYMPTOTICS
Weareinterestedinassessingtheearly-stoppingtestaccuracyofnaiveSAM(n-SAM)versus1-SAM.
AswecanobserveinFigure3,thebesttestaccuracymonotonicallyincreaseswithρ. Weanalyze
1-SAM in the limit as the perturbation magnitude converges to limit ρ → ∞ and observe that it
converges to the optimal classifier. In this regime, the 1-SAM update for each example simply
becomes
∇ ℓ(w+ε ,x ,y )= lim −t σ(−t ⟨w ,x ⟩+ρ∥x ∥ )x =−t x (B.5)
w+εi i i i ρ→∞ i i i i i 2 i i i
and the ratio between the magnitude of any two points converges to 1, specifically for any two
datapointsx andx
i j
∥∇ ℓ(w+ε ,x ,y )∥ σ(−t ⟨w,x ⟩+ρ∥x ∥))
lim (cid:13) w+εi i i i (cid:13) = lim i i i (B.6)
ρ→∞(cid:13)∇ w+εjℓ(w+ε j,x j,y j)(cid:13) ρ→∞ σ(−t j⟨w,x j⟩+ρ∥x j∥)
1+exp(t ⟨w,x ⟩−ρ∥x ∥)
= lim i i i =1 (B.7)
ρ→∞1+exp(t j⟨w,x j⟩−ρ∥x j∥)
Asaresult,eachgradientupdateof1-SAMispreciselyequaltheempiricalmeanofthedatascaledby
thelabeluˆ =X⊤t. Saythatwearegivenfixedtrainingexamples[x ]n independentlysampled
D i i=1
fromDwhere∆iscorrupted. Notethatasdorngrows(and∆<0.5),uˆ convergestotheBayes
D
optimalclassifier. Notably,
(cid:34) n (cid:35)
(cid:88) γ d,n→∞
uˆ = (1−2∆)B, √ z −−−−−→(1−2∆)Be (B.8)
D i 1
n d−1
i=1
Notethattheridgeregressionalsoconvergestotheempiricalmeanscaledbythelabelinthelimit
(XX⊤+λI)−1X⊤t−λ −→ −−∞ →X⊤t (B.9)
ThisgivesusreasontobelievethatSAMandweightdecayhavesimilarregulatoryproperties.
14PublishedasaconferencepaperatICLR2024
n-SAM Test Accuracy
0.70 Gradient Descent
0.75 Test Accuracy 0.68 100000 n-SAM
0.66
0.70 0.64
0.62
0.65
0.60
0.60 0.58
0.56
0.55
0 200 400 600 800 1000 0 250 500 750 1000 1250 1500 1750 2000
Timesteps Timesteps
Hessian Trace 1-SAM Clean / Mislabeled Train Loss
1.2
1-SAM
200 n-SAM
1.0
150
0.8
100 0.6
50 0.4
0.2
0
0 250 500 750 1000 1250 1500 1750 2000 0 250 500 750 1000 1250 1500 1750 2000
Timesteps Timesteps
Figure6: Wetrainlinearmodelsonourtoymodelusing1-SAM.Bluercurvesdenotelargerρ. We
randomlyinitializethemodel,althoughweobservesimilarcurveswheninitializedattheorigin.
B.2 ANALYSISOFN-SAM
WealsoanalyzethenaiveSAM(n-SAM)update
(cid:32) n (cid:33)
1 (cid:88)
w =w−η ∇ ℓ(w+ϵ,x ,y )+λw (B.10)
n w+ϵ i i
i=1
whereϵ = ρ ∇wL(w) andL(w) = 1 (cid:80)n ℓ(w,x ,y )andρisahyperparameter. Theoriginal
∥∇wL(w)∥2 n i=1 i i
andfollowupworksForetetal.(2020);Andriushchenko&Flammarion(2022a)showthatn-SAM
doesnothaveanyadvantageovergradientdescentinpractice. Weobservethateveninthelinear
settingwithtoyGaussiandata,n-SAMobserveslittleearlystoppingimprovementsunlessρisscaled
updramatically. InFigure4forexample,wesetρ=100000forn-SAM.
Comparing1-SAMandn-SAMinthelinearsetting,wefindthattheyhavefundamentallydifferent
effects.n-SAM’sperturbationisnotaconstantthatisonlyafunctionofthedatanorm.Themagnitude
oftheperturbationisproportionaltotheloss, soSAMdoesnotpreferentiallyupweightlowloss
points except for the early training steps. Let us consider that the noise z of the datapoints are
i
orthogonalforeaseofanalysis. Then
(cid:18) (cid:19)
∇ L(w)
∇n−SAMℓ(w,x ,y )=−t σ −t ⟨w+ρ w ,x ⟩ x (B.11)
w i i i i ∥∇ L(w)∥ i i
w
(cid:18) ρ (cid:18) γ2∥z ∥σ(−t ⟨w,x ⟩)(cid:19)(cid:19)
=−t σ −t ⟨w,x ⟩+ CB2+ i i i x (B.12)
i i i n∥∇ L(w)∥ d−1 i
w
forsomescalarCthatremainsconstantacrosstheexamples. Assuminggradientdescentstartingat
w =0,classbalance,andthesamenumberofmislabeleddatapointsineachclass,n-SAMateach
iteration,perturbseachpointproportionaltoσ(−t⟨w,x ⟩)whichissmallerforlowlosspointsand
i
higherforhighlosspoints. Thisdoesnotguaranteepreferentialup-weighting. Wedoobservein
Figure3(seeorangecurve)thatifρissufficientlylarge,weareabletoseesomegainswithn-SAM
inthefirstcoupletimesteps. Finally,wedonotseeanycorrelationbetweentestaccuracyandflatness
measuredbytheHessiantrace. Infact,n-SAMwithlargeρachievessmallerHessiantracebythe
endoftrainingbutlowertestaccuracy.
15
ycaruccA
ecarT
ycaruccA
ssoLPublishedasaconferencepaperatICLR2024
C IMPLICIT UP-WEIGHTING
Previously, we showed that the gap between the training accuracy of clean and noisy examples
correlates highly with the test accuracy (See Figure 7). This led us to reason about improved
generalizationwithlearningcleanexamplesfasterthannoisyexamples. Inparticular,weusethe
“sample-wiseupdatenorm”asaproxyforhowmucheachcleanandnoisyexamplecontributetothe
averageupdate. Forexample,withSGD,theupdateisthegradientevaluatedatthemodelparameters
w,andforSAM,theupdateisthegradientevaluatedattheperturbedw+ϵ. Weshowedthatinthe
linearsetting,thelogitscaletermofSAMup-weightstheupdatenormofcleanexamples. Next,for
non-linearsetting,weshowedthatSAM’sJacobiantermregularizesthemagnitudeofthefunction
output,andwesuspectthatthishasasimilarimpliciteffectofbalancingthegradientsofcleanand
noisyexamplesbykeepingthelossofanyexamplewithinanarrowrange.
Below,weplottheaveragecleanversusnoisyratiooftheloss,sample-wiseupdatenorm,andlogit
scalenormfromSection4.1. Wedesirethesequantitiestobehighevenastheclean-noisyaccuracy
gapincreases. Forthesameclean-noisyaccuracygap,SAMandJ-SAM’sratioofclean/noisylossis
higher(acrossthetrainingtrajectory,beforetheaccuracygappeaks). Asaresult,theclean/noisy
updatenormratioisalsohigherwithSAMandJ-SAM,meaningcleanexampleshavehigherinfluence
on the direction of the update . Notably, this up-weighting effect is implicit, not due to explicit
upweighting.
Clean - Noisy Train Accuracy Gap
SGD
0.6 SAM
L-SAM
0.4 J-SAM
0.2
0.0
0 50 100 150 200
Epoch
Loss Ratio vs Train Accuracy Gap Update Norm Ratio vs Train Accuracy Gap Logit Scale Ratio vs Train Accuracy Gap
0.9 SGD 1.0
0.8 S L-A SM AM 0.9 0.9
J-SAM
0.7 0.8 0.8
0.6 SGD SGD 0.7 SAM 0.7 SAM 0.5 L-SAM L-SAM
0.6 J-SAM 0.6 J-SAM
0.4
0.3 0.5 0.5
0.2 0.4 0.4
0.0 0.2 0.4 0.6 0.0 0.2 0.4 0.6 0.0 0.2 0.4 0.6
Clean - Noisy Accuracy Gap Clean - Noisy Accuracy Gap Clean - Noisy Accuracy Gap
Figure7: Weplottheclean/noisyratiooftheloss,sample-wisegradientnorm,andlogitscalenorm
versustheclean-noisyaccuracygap. TheratiosarehigherwithSAMandJ-SAMthanSGDand
L-SAMforthesameaccuracygap. Orangemarkerdenotesthebeginningofthetrainingtrajectory
andpurpledenotestheend.
16
NL:CL
ysioNccA
naelCccA
NL : CL
N)w(g
:C)w(gPublishedasaconferencepaperatICLR2024
D EVIDENCE OF LABEL NOISE ROBUSTNESS ON OTHER DATASETS
Figure8: SAMobserveshighertrueaccuracyofnoisyexamplesandthiscorrespondswithbettertest
accuracyonTiny-ImageNetwith30%labelnoiseandFlower102with20%labelnoise. Donotethat
contrarytotrendsinCIFAR10,inalowdatasettingssuchasFlowers102,thetestaccuraciesofSAM
andSGDdonotdropevenwhenthemodelstartstooverfit. ModelsareResNet18.
E REGULARIZED SGD UNDER NO LABEL NOISE
Figure9: SAM(ρ=0.01)andSGDhaveasmaller8%differenceinperformance(lessincompar-
isontothe20%differencewith30%labelnoise). Ourweightandfeaturenormpenaltyobserves
improvementsbutonlybyasmallfactorof1%andtheperformancedegradesovertime.
17PublishedasaconferencepaperatICLR2024
F ABLATION STUDIES
We observe that the improved benefits of SAM in the label noise setting does not dimish with
underparametrization. Furthermore,SGDwithlargelearningratecannotmatchSAM’sperformance.
Test Accuracy LR=0.01 N=100 Test Accuracy LR=0.01 N=300 Test Accuracy LR=0.01 N=500
0.75 0.75 0.75
0.70 0.70 0.70
0.65 0.65 0.65
0.60 0.60 0.60
0.55 0.55 0.55
0.50 0.50 0.50
0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000
Timesteps Timesteps Timesteps
Test Accuracy LR=0.01 N=700 Test Accuracy LR=0.01 N=2000 Test Accuracy LR=0.01 N=3000
0.75 0.75 0.75
0.70 0.70 0.70
0.65 0.65 0.65
0.60 0.60 0.60
0.55 0.55 0.55
0.50 0.50 0.50
0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000
Timesteps Timesteps Timesteps
Figure 10: Behavior with number of training examples for linear model/toy Gaussian data
with 40% label noise. Linear models trained with different subsets of the toy Gaussian data
observe different levels of benefit with SAM. ρ is scaled between 0.03 and 0.18, bluer curves
signifyinghigherρ. Thedatais1000dimensions. NotethatperformanceimproveswithSAMeven
inunderparametrizedregimes.
Test Accuracy LR=0.001 N=500 Test Accuracy LR=0.01 N=500 Test Accuracy LR=0.1 N=500
0.75 0.75 0.75
0.70 0.70 0.70
0.65 0.65 0.65
0.60 0.60 0.60
0.55 0.55 0.55
0.50 0.50 0.50
0 200 400 600 800 1000 0 200 400 600 800 1000 0 25 50 75 100 125 150 175 200
Timesteps Timesteps Timesteps
Test Accuracy LR=0.1 N=500
0.75
0.70
0.65
0.60
0.55
0.50
0 25 50 75 100 125 150 175 200
Timesteps
Figure11: Behaviorwithlearningrateforlinearmodel/toyGaussiandatawith40%labelnoise
Asthelearningrateincreases,wegenerallyobserveaslightimprovementinearlystoppingaccuracy
inbothSGDandSAM.
18
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccA
ycaruccAPublishedasaconferencepaperatICLR2024
Figure12: BehaviorwithnumberoftrainingexamplesforResNet18/CIFAR10with30%label
noiseWecompareSGDandSAMtrainedondifferentnumberoftrainingexamples(10,50,100%of
thetrainingdata). WeseethatthedifferencebetweenSAM(ρ = 0.01)andSGDincreasesasthe
datasetsizeincreases.
Figure13: BehaviorwithmodelwidthforResNet18/CIFAR10with30%labelnoiseResNet18
startswith64convolutionalfilters,andthefiltersdoubleeverytwoconvolutionallayers. Wereduce
thewidthofResNet18by1/2and1/4bystartingwith32and16initialnumberoffilters,respectively.
We see that SAM (ρ = 0.01) and SGD both improve in terms of performance as model width
increases.
Figure14: BehaviorwithlearningrateforResNet18/CIFAR10with30%labelnoise. Foreach
learning rate, we choose the best ρ for SAM found by hyperparameter search. As learning rate
increases,weobservethatbothSAMandSGDbothimproveintermsofperformanceaslearning
rateincreases. Forsmalllearningrate0.001,wefounditdifficultforSAMtoobservesignificant
improvementsuponSGDunlesstrainedforatleastdoublethetime.
19