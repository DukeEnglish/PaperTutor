A Multi-Agent Rollout Approach for Highway Bottleneck
Decongenston in Mixed Autonomy
Lu Liu, Maonan Wang, Man-On Pun and Xi Xiong
Abstract
Theintegrationofautonomousvehicles(AVs)intotheexistingtransportationinfrastructureoffersapromising
solutiontoalleviatecongestionandenhancemobility.Thisresearchexploresanovelapproachtotrafficoptimiza-
tion by employing a multi-agent rollout approach within a mixed autonomy environment. The study concentrates
on coordinating the speed of human-driven vehicles by longitudinally controlling AVs, aiming to dynamically
optimize traffic flow and alleviate congestion at highway bottlenecks in real-time. We model the problem as a
decentralized partially observable Markov decision process (Dec-POMDP) and propose an improved multi-agent
rollout algorithm. By employing agent-by-agent policy iterations, our approach implicitly considers cooperation
among multiple agents and seamlessly adapts to complex scenarios where the number of agents dynamically
varies. Validated in a real-world network with varying AV penetration rate and traffic flow, the simulations
demonstrate that the multi-agent rollout algorithm significantly enhances performance, reducing average travel
time on bottleneck segments by 9.42% with a 10% AV penetration rate.
I. INTRODUCTION
The potential of autonomous vehicles (AVs) in enhancing road capacity, reducing stop-and-go traffic, and
optimizingoveralltrafficflowhaspositionedthemaskeyplayersinintelligenttransportationsystems[1].Withthe
acceleration of urbanization, traffic congestion has emerged as a critical challenge, imposing substantial economic
and productivity costs. Congestion often occurs near bottlenecks, which are identified as primary contributors to
the Capacity Drop [2], [3], due to their relatively low capacity and susceptibility to traffic overloading. Thus,
by controlling the number of vehicles prior to bottlenecks, traffic congestion can be alleviated. Although various
control methods have been proposed, ramp control is the most widely applied solution [4]. However, traffic flow
on congested highway segments often originates from multiple entrances, ramp control models are often tailored
to local segments and may not be effective. The potential of AVs in intelligent traffic management offers new
opportunities for addressing traffic congestion [5].
In this paper, we tackle the challenge of mitigating traffic congestion in dynamic environments by controlling
the speed of AVs to coordinate the timing of upstream vehicles approaching bottlenecks. Considering the scenario
with mixed autonomy traffic flow depicted in Fig. 1, where a bottleneck occurs as three lanes merge into two,
blue vehicles represent AVs, and white vehicles represent human-driven vehicles (HDVs). In uncontrolled areas,
AVs emulate the driving behavior of HDVs. Upon entering the controlled coordination zone, AVs utilize onboard
cameras and satellite navigation systems (e.g., GPS) to gather self-information, including their position, speed,
and distance to the bottleneck exit. Furthermore, to enhance situational awareness, the controlled coordination
zone is subdivided into multiple edges, allowing AVs to access the vehicle count on each lane of every edge
through connectivity established with roadside cameras via vehicular communication systems. Based on these
This work was supported in part by NSFC Project 72371172 and Fundamental Research Funds for the Central Universities.
L.LiuandX.XiongarewiththeKeyLaboratoryofRoadandTrafficEngineering,MinistryofEducation,TongjiUniversity,Shanghai,
China. M. Wang and Man-On Pun are with the School of Science and Engineering, the Chinese University of Hong Kong, Shenzhen,
China (Emails: luliu0720@tongji.edu.cn, maonanwang@link.cuhk.edu.cn, simonpun@cuhk.edu.cn, xi_xiong@tongji.edu.cn,)
4202
yaM
6
]AM.sc[
1v23130.5042:viXradata, strategic decision-making collaboration among AVs ensues, determining which vehicle should accelerate to
traverse the bottleneck promptly and which should decelerate and wait to pass, thereby reducing a reduction in
the average travel time of vehicles on the segment.
Fig. 1. Illustration of congestion where three lanes merge into two, with congestion originating downstream (right) and propagating
upstream (left).
Existing research that exploits the speed control of upstream AVs has demonstrated potential in mitigating
congestion [6]. The methods of inter-vehicle communication (IVC) and road-vehicle communication (RVC) are
widely used to manage AVs [7]. However, this relies on existing infrastructure and cannot guarantee the success
of communication. Li et al. [8] proposed a model-based predictive control framework to reduce the travel time
of vehicles in a network. The model-based control methods require rigorous and explicit models of traffic flow
dynamics to ensure optimal control. Most of the literature uses model-free approaches to model the problem
of mitigating traffic congestion using AVs. Maske et al. [9] introduced a distributed approach, with each AV
independently responsible for its control area. While relying solely on distributed control often overlooks the
intricacies of multi-vehicle cooperation. Consequently, multi-agent RL algorithms [10] have been proposed and
implemented for the coordinated control of AVs [11]. Although these algorithms allow AVs to interact, they
struggle to scale with the increasing number of agents, often leading to computational bottlenecks and suboptimal
performance in real-world scenarios.
To address this gap, our study formulates the control tasks of multiple AVs as a decentralized partially
observable Markov decision process (Dec-POMDP) and presents a novel multi-agent rollout approach. This
approach reframes the multi-agent decision-making problem into a sequential decision-making paradigm using
the agent-by-agent policy iteration (A2PI), and updates each AV’s policy via a Proximal Policy Optimization
(PPO) algorithm [12]. By enabling the actions of preceding AVs to inform the state inputs of subsequent AVs,
this method fosters an inherently adaptive cooperative interaction model. Crucially, our approach is designed to
flexibly accommodate dynamic fluctuations in the number of AVs, ensuring robust performance in evolving traffic
conditions. The main contributions of this article can be summarized as follows:
• We model the multi-agent problem as a Dec-POMDP to capture the intricate interactions among agents and
provide a unified framework for decision-making in multi-agent systems.
• A multi-agent rollout algorithm is developed, utilizing A2PI to convert the multi-agent problem into a
sequential decision-making problem. It is demonstrated that this iterative approach consistently enhances the
policy, effectively promoting collaboration among agents and adapting smoothly to changes in the number
of agents.
• The approach has been tested in mixed autonomy traffic flow on a real-world network. Experimental results
indicate a significant 9.42% improvement in the average travel time of vehicles on bottleneck segments with
a 10% penetration rate of AVs.
The remainder of this paper is organized in the following manner. Section II introduces the model for thescenario depicted in Fig. 1. Section III details the enhanced multi-agent rollout approach. Section IV analyzes
the performance of the proposed approach through simulations based on real traffic scenarios. Finally, Section V
summarizes the main findings and outlines several directions for future work.
II. MODELING AND FORMULATION
In this section, the methodology for optimizing traffic flow by incorporating AVs is presented within a Dec-
POMDP framework. The objective is to minimize travel costs for all vehicles in the network. Consider a highway
segment with a bottleneck, as illustrated in Fig. 1, where both AVs and HDVs converge, leading to a reduction in
the highway’s capacity. In the Dec-POMDP model, each AV is limited to its own sensor data and the information
disseminated by the infrastructure, thus possessing a partial view of the overall traffic situation. Prior to the
bottleneck, AVs undertake control actions, which in this study are confined to longitudinal speed adjustments to
simplify computational demands.
The traffic management challenge for a fleet of N AVs approaching a bottleneck is structured within the
Dec-POMDP paradigm. This paradigm is characterized by the tuple (S,{A } ,{O } ,T,R,γ,τ), where
i i∈N i i∈N
N represents the number of the agents, S represents the set of global states of the environment, inaccessible
to the AVs, {A } is the set of actions available to each AV i, and A = × A defines the joint action space.
i i i
Similarly, {O } denotes the set of observations for each AV, with O = × O representing the joint observation
i i i
space. The state transition probability function T : S ×A×S → [0,1] governs the likelihood of transitioning
between states given the actions taken by the AVs. The reward function R : S×A → R measures the immediate
utility of actions, while γ ∈ [0,1] is the discount factor, and τ specifies the planning horizon. The goal is to
optimize the cumulative long-term reward by coordinating AV acceleration and deceleration actions.
At each discrete time step t, AV i receives an observation ot ∈ O , and all AVs execute a joint action
i i
at = {at,··· ,at } ∈ A, resulting in a state transition according to T(st,at,st+1). Subsequently, each AV i
1 N
receives a collective reward rt is issued based on R. The total reward is the discounted expected sum over an
infinite horizon, with the optimal policy derived from the Bellman equation:
J∗(st) = max(cid:2) rt(st,at)+γϕ∗(st+1)(cid:3) , (1)
at∈A
where ϕ∗(st+1) = (cid:80) T(st,at,st+1)J∗(st+1) encapsulates the expected return from the next state, thus
st+1∈S
connecting the present decision-making to future rewards.
For effective vehicle control before the bottleneck, the specific Dec-POMDP representation is as follows. At
any given time step t, each AV agent, indexed by i where i = 1,2,...,N, acquires kinematic data including its
velocity vt and the distance dt to the bottleneck’s exit. To incorporate global traffic information, road segment
i i
data is encapsulated by the distribution of vehicles across edges and lanes, with nt denoting the number of
e,l
vehicles on edge e ∈ {1,2,...,E} and lane l ∈ {1,2,...,L}, where L and E represent the total number of
lanes and edges. The observation state for vehicle i is thus formalized as:
(cid:34) (cid:35)
vt,dt nt ,nt ,...,nt
ot = i i 1,1 1,2 E,L . (2)
i (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
AVinformation Roadsegmentinformation
Agent i selects actions at in the form of acceleration commands ηt, which are bounded within the operational
i i
limits [η ,η ]. The reward function rt is designed to reflect the average speed v of the vehicles within the
min max b
control zone, thereby aligning individual agent objectives with the overarching goal of enhancing traffic flow andminimizing congestion. Importantly, the reward incorporates a time penalty to discourage AVs from reducing
speed or halting in unregulated areas solely to accumulate rewards.
III. MULTI-AGENT ROLLOUT APPROACH FOR TRAFFIC FLOW OPTIMIZATION
In this section, a modified multi-agent rollout method based on A2PI is delineated for optimizing traffic flow
within environments of mixed autonomy. Initially, the concept of A2PI is elucidated, and it is substantiated
that iterative refinement of agent strategies enhances their performance. Subsequently, the multi-agent rollout
algorithm combining A2PI with PPO [12] is explained, which is used to fine-tune traffic flow optimization.
A. Agent-by-agent policy iteration
In the proposed A2PI approach, a joint policy vector π = (π ,π ,...,π ) governs the behavior of all AVs.
1 2 N
Each AV, indexed by i where i ∈ {1,2,3,...,N}, is initially equipped with a policy π0, which is a stochastic
i
strategy generated at random, with the superscript k = 0 denoting the iteration index. As the iteration process
unfolds, the policy π of a singular AV is selected for refinement, while the policies of its counterparts are
i
maintained unchanged. Such a sequential training methodology is designed to alleviate the instability that is
frequently encountered in multi-agent reinforcement learning paradigms.
TofacilitateeffectivecooperationamongmultipleAVs,thepolicyπ ofanAVisoptimizedusingthecumulative
i
rewardofallAVs,whichpromotestheenhancementofthepolicythroughiterativeupdates.Thisapproachenables
an AV to predict and adapt to the collective behavior of the group, which is crucial for achieving collaborative
objectives.
Fig. 2. Sequential policy update in multi-agent decentralized control.
Fig. 2 illustrates the update process for a single AV’s policy. It is noted that AVs undergo policy iteration in
a sequential manner. For the k-th iteration of AV i at time t, the policies of AVs 1 to i−1 have undergone k
iterations, whereas the policies of AVs i to N have completed k−1 iterations. The joint policy configuration at
this stage can be represented as follows, where πk(i−1) indicates the policy for AVs 1 through i−1 after the
k-th iteration. Once all AV policies have undergone K iterations, the configuration will be denoted as πk(i), and
for brevity, as πk:
πk(i−1) = {πk,...,πk ,πk−1,...,πk−1}. (3)
1 i−1 i N
Based on their respective observations ot for j ∈ {1,2,··· ,N}, AV i chooses the action at to maximize the
j i
reward, and other AVs select actions according to their strategies: ak,t for j ∈ {1,2,...,i−1} and ak−1,t for
j j
j ∈ {i + 1,...,N}. This selection results in a collective reward rt(st,ak,t ,at,ak−1,t ), where ak,t is the
1:i−1 i i+1:N 1:i−1
action tuple (ak,t,...,ak,t ) and ak−1,t is (ak−1,t,...,ak−1,t). During this process, only the policy model πk
1 i−1 i+1:N i+1 N i
is updated, with the parameters of other strategies remaining unchanged. Upon completion of the update, πk is
i
obtained. Subsequently, AV i+1 proceeds to update its policy, and this process continues until the policies of allagents have been updated, culminating in the optimization of the overall system’s performance. The effectiveness
of this approach is substantiated in the following proposition.
Proposition 1. For the states st ∈ S, the joint policy πk+1 obtained in iteration k +1 is not inferior to the
policy πk from iteration k, i.e.,
J (st) ≤ J (st), ∀st ∈ S. (4)
πk πk+1
Proof: To demonstrate the validity of Proposition 1, consider a simplified multi-agent system comprising two
agents (i.e., N = 2). The expected reward for state st under the joint policy πk is given by:
(cid:104) (cid:105)
J (st) = E r(st,ak,t,ak,t)+γϕ (st+1)
πk 1 2 (πk,πk)
1 2
(cid:104) (cid:105)
≤ max E r(st,at,ak,t)+γϕ (st+1)
1 2 (πk,πk)
at∈A 1 2
1 1
(cid:104) (cid:105)
= E r(st,ak+1,t,ak,t)+γϕ (st+1)
1 2 (πk+1,πk)
1 2
(cid:104) (cid:105)
≤ max E r(st,ak+1,t,ak)+γϕ (st+1)
1 2 (πk+1,πk)
ak∈A 1 2
2 2
(cid:104) (cid:105)
= E r(st,ak+1,t,ak+1,t)+γϕ (st+1)
1 2 (πk+1,πk+1)
1 2
= J (st). (5)
πk+1
In the above derivation, the expected reward under the joint policy πk is first established. Subsequently, the
policy for the first agent is improved, as indicated by the maximization operation, ensuring that the expected
reward following the policy enhancement for the first agent is at least as favorable as the previous one. The
new policy for the first agent is then adopted. The subsequent steps mirror the process for the second agent,
culminating in the validation of Proposition 1.
It is thereby demonstrated that in a two-agent system, the expected reward obtained when agents employ the
optimal policies is at least equivalent to the expected reward under the original policies. This substantiates the
efficacy of the proposed approach in augmenting the average velocity at traffic bottlenecks. The proof can be
extended analogously for any positive integer number of agents N.
B. Multi-agent rollout algorithm
Building upon the foundation of the A2PI approach, we integrate the PPO in the multi-agent rollout algorithm
to refine the policy of each AV within the mixed traffic flow. PPO stands out for its stable and efficient policy
updates, which are critical in a policy gradient approach that aims to maximize expected returns. It achieves this
by limiting the magnitude of policy modifications, a feature that is well-suited to the multi-agent rollout method
based on A2PI. This compatibility is due to PPO’s ability to facilitate gradual and consistent policy enhancement
for each agent, thereby ensuring that the collective system progresses towards optimal policy convergence without
introducing instability to the learning trajectory.
Incorporating the PPO algorithm with the A2PI framework involves a meticulous synchronization of the agent-
wisepolicyupdateswiththePPO’sobjective.Foragentiatthek-thiteration,thelocalenvironmentalinformation
isobtainedthroughobservationso ,andthepolicyπk isrefinedbymaximizingthePPOobjectivefunction,which
i i
is defined as:
(cid:104) (cid:16) (cid:17)(cid:105)
L(πk) = Eˆ min r (πk)Aˆ ,clip(cid:0) r (πk),1−ϵ,1+ϵ(cid:1) Aˆ , (6)
i t t i t t i twhere r t(π ik) is the probability ratio r t(π ik) = ππ k−ik( 1a (at|o |ot i) t), signifying the likelihood of selecting action a
t
under
the new policy relative to the old policy. The advi antagt ei function estimate at time t is represented by Aˆ , and ϵ is
t
a hyperparameter controlling the clipping range. The empirical average over a finite batch of samples is indicated
by
Eˆ
. This clipped objective allows for a controlled policy update that maintains the benefits of policy iteration
t
while mitigating the risks of policy performance collapse.
The PPO algorithm is further adapted to the multi-agent rollout approach by updating the policies of the agents
sequentially, as depicted in the A2PI process. Each policy πk is improved by collecting a set of trajectories under
i
the current policy configuration πk, computing advantage estimates, and then applying the PPO update rule. The
sequential update is crucial for maintaining the stability of the learning process in the multi-agent environment,
as it allows each agent’s policy to adapt gradually to any new behaviors emerging from the collective policy
changes of all agents. The pseudocode for the multi-agent rollout algorithm is shown in Algorithm 1.
Algorithm 1: Multi-Agent Rollout for Traffic Flow Optimization
Input: Initial policies π0 for all AVs i ∈ {1,2,...,N}.
i
Output: Converged policies π∗ for all AVs i.
i
1 Initialize policies π i0 for all AVs i ∈ {1,2,...,N}.
2 for k = 0,1,2,... to convergence do
3 for each agent i ∈ {1,2,...,N} do
4 Collect set of trajectories D i under πk.
5 Compute advantage estimates Aˆ t for each step in D i.
6 for epoch = 1,2,...,M do
7 Update π ik by maximizing L(π ik) using D i and Aˆ t.
8 Update π ik to π ik+1.
In this algorithm, the collection of trajectories D is performed by sampling actions from the current policy
i
πk and interacting with the environment. The advantage function estimates Aˆ are computed to quantify the
i t
benefit of the selected actions over the baseline, which guides the policy update. The policy update itself is
performed for M epochs to ensure sufficient optimization of the PPO objective within the trust region defined
by the clipping mechanism. This iterative process is repeated until the policies converge to an optimal solution,
thereby optimizing the traffic flow and reducing congestion within the mixed autonomy environment.
The multi-agent rollout algorithm presented in this study embodies a decentralized control strategy. Each AV
selects its optimal action through a process of distributed learning, which involves implicit cooperation among
all agents. Our method’s robustness is highlighted by its scalability to various numbers of agents; it adapts to
changes in agent populations without the need for re-engineering the decision-making framework. Moreover,
the policy iteration for each agent is conducted in a manner where the other agents’ policies remain stationary,
promoting a stable learning backdrop that is essential for the convergence of individual training processes. This
stability is a significant advantage, addressing the frequent convergence difficulties faced in multi-agent learning
algorithms due to environmental complexity.IV. NUMERICAL RESULTS
A. Experiment setting
This study conducted experiments utilizing the Simulation of Urban Mobility platform(SUMO). RL-based AV
optimization was facilitated by the Adam optimizer. The configuration parameters were set as follows: a learning
rate of 1×10−4, a discount factor γ of 0.9, and a clipping hyperparameter ϵ of 0.2. The evaluation metric was
the average travel time (Avg.TT), where lower values indicate better performance.
(a) The map of the selected segments on the Shenhai Highway
(b) Simulation of the Shenhai Highway bottleneck segment
Fig. 3. Traffic flow optimization at a bottleneck on the Shenhai Highway.
A real-world traffic scenario on the Shenhai Highway in Shanghai, characterized by a segment where three
lanes merge into two, was selected to assess the proposed method’s efficacy in alleviating congestion at the
bottleneck. The chosen coordination zone, extending over 1,000 m and depicted in Fig. 3, illustrates both the
map of the selected areas on the Shenhai Highway and the simulation of the bottleneck segment in SUMO.
Two traffic scenarios were set up, a congestion scenario with a simulation lasting 700 s, where the traffic flow
is 2400 veh/hour including 6 AVs, and another scenario with a simulation lasting 7200 s where traffic flow
varies randomly from 0 to 4000 veh/hour and 10% of the vehicles are AVs. Maximum speeds were established
at 10 m/s for HDVs and 12 m/s for AVs. The action space of an AV was defined as a set of accelerations
from η =−5 m/ss to η =2.5 m/ss. Vehicles were initially released randomly from the start of a designated
min max
roadway, and allowed to travel 400 m freely. By controlling AVs’ speed to enable vehicles in specific lanes to
decelerate and allow others to overtake, the overall waiting time is reduced.
B. Compared methods
The performance of the algorithm proposed in this paper is evaluated against three benchmark methods:
• WithoutControl:Inthismethod,thevehiclesdeterminetheirspeedwithintheSUMOsimulationenvironment
by considering the distance to the vehicle ahead, the desired headway, and so on [13]. In this paper, without
control is used as a baseline methodology.
• MATD3: While the Twin Delayed DDPG (TD3) algorithm is designed for single-agent tasks, it has been
adapted to multi-agent systems [14] by providing a separate TD3 instance for each agent.
• MAPPO: The design of the Multi-Agent Proximal Policy Optimization (MAPPO) algorithm [15], which
utilizes a global value function to evaluate the collective state of all agents, illustrates its positive role in
coordinating cooperation among multiple agents.C. Performance comparison
Table I presents the performance of various methods across two traffic scenarios. In the first scenario, charac-
terized by congestion with a traffic flow of 2400 veh/hour including 6 AVs, the multi-agent rollout algorithm
reducestheaveragetraveltimeby45.13%aftertwoiterationscomparedtoscenarioswithoutcontrol.Additionally,
the final total reward obtained by the multi-agent rollout algorithm is higher than those achieved by MATD3
and MAPPO. In the second scenario, where traffic flow varies randomly from 0 to 4000 veh/hour, including
a 10% AV penetration, the algorithm demonstrates a 9.42% improvement in average travel time compared to
uncontrolled conditions. Here too, the final reward of our method surpasses those of MATD3 and MAPPO.
TABLE I
PERFORMANCECOMPARISONSOFDIFFERENTALGORITHMS
Scenario Algorithm Reward Avg.TT (s) Perf (%)
Without Control -20537 563.46 –
MATD3 -4248 347.34 38.36
T=700s
MAPPO -3405 311.16 44.78
MARollout -3273 309.17 45.13
Without Control -41540 168.72 –
MATD3 -26691 157.95 6.38
T=7200s
MAPPO -22315 154.67 8.33
MARollout -20826 152.82 9.42
a T denotes simulation time; Perf denotes performance improvement;
MARollout denotes multi-agent rollout algorithm
Fig.4. Theperformancecurvesofthealgorithmsduringtraininginthefirstscenario.MARollout(iter.1)andMARollout(iter.2)represent
the results of 1-step and 2-step iteration, using the multi-agent rollout approach, respectively.
Fig. 4 illustrates the reward curves of different algorithms in the first traffic scenario. It is observed that after
two iterations, the MARollout achieves the highest reward. The evolution of rewards over the two iterations of the
multi-agent rollout algorithm indicates the development of cooperation among agents. Initially, the total reward
for all agents begins at a relatively low baseline within a stochastic environment. By the second iteration, there
is a significant increase in the initial reward, exceeding the final reward of the first iteration. This improvement
suggests that the agents developed effective policies during the first iteration to coordinate their actions. Fig. 5
illustrates this cooperative. We selected four vehicles near the bottleneck at a movement and depicted their actionvariations in trajectory form over the first six time steps. Among the three AVs, AV 2 was passing through
the bottleneck unaffected by other AVs, with its acceleration gradually increasing. Through observation, AV 1
identified only one vehicle ahead in its lane and has no competition with AV 2, so executing acceleration actions.
Due to AV 3 observing the presence of AV 2 at the bottleneck, it decelerated in advance to avoid waiting.
Fig. 5. An illustration of the cooperative actions of AVs near bottlenecks.
Although the performance of the multi-agent rollout algorithm was comparable to MAPPO, the MAPPO
assumes the number of agents is fixed at the start of training, which makes it powerless in dynamic scenarios
with agents entering and exiting the system frequently. Moreover, as the number of agents increases and policies
vary, the environment is unstable, which makes it difficult for MAPPO to converge. In contrast, utilizing a policy
iteration approach, the multi-agent rollout algorithm adeptly addresses these challenges.
D. Sensitivity analysis
This subsection examines the sensitivity of average travel time to key parameters, including traffic inflow and
AV penetration rate. We have conducted a sensitivity analysis of inflow at 5%, 10%, and 20% penetration rates
of AVs, respectively. As shown in Fig. 6(a), the average travel time increases with inflow for all penetrations.
Nevertheless,thepresenceofAVssignificantlyreducestheaveragetraveltime,especiallywhentheinflowexceeds
2,000 veh/hour, underscoring the AVs’ capability to preemptively traffic conditions at the bottleneck and slow
down to avoid congestion.
(a) Average travel time with the inflow (b) Average travel time with the penetration rate
Fig. 6. Sensitivity analysis of average travel time to changes in autonomous vehicles penetration rate and traffic inflow.
Subsequently, the influence of AV penetration rate on average travel time is shown in Fig. 6(b). The results
illustrate that a penetration rate range of 0% to 40% yields significant average travel time reductions across all
traffic flow. Notably, beyond the 10% penetration rate, the travel time begins to a stabilized value, indicating that
the penetration benefits of AVs may be nearing saturation.V. CONCLUSIONS
In this paper, we demonstrate the effectiveness of coordinated control of autonomous vehicles in alleviating
traffic congestion at bottlenecks within mixed autonomy traffic flow. By modeling the problem as a Dec-POMDP
and utilizing the multi-agent rollout approach based on A2PI, the task of multi-agent control is transformed into
sequential decision-making distributed control. With the AV penetration rate of 10%, the average travel time at
bottlenecks is reduced by 9.42%. This method not only enhances cooperation among agents but also addresses
the challenge of adapting to changes in agent numbers, a common limitation of multi-agent algorithms.
This work can be extended in several directions. Firstly, incorporating vehicles’ lane-changing behavior could
reflect real-world traffic flow more accurately. Secondly, applying this method to larger-scale networks would
allow for a deeper investigation into how multiple agents impact traffic flow within complex networks. Thirdly,
optimizing the policy update process by training multiple agents in parallel would have faster optimization.
REFERENCES
[1] T.-H.ChangandI.-S.Lai,“Analysisofcharacteristicsofmixedtrafficflowofautopilotvehiclesandmanualvehicles,”Transportation
Research Part C-emerging Technologies, vol. 5, pp. 333–348, 1997.
[2] P. W. Shaikh, M. El-Abd, M. Khanafer, and K. Gao, “A review on swarm intelligence and evolutionary algorithms for solving the
traffic signal control problem,” IEEE Transactions on Intelligent Transportation Systems, vol. 23, pp. 48–63, 2022.
[3] K.Chung,J.Rudjanakanoknad,andM.J.Cassidy,“Relationbetweentrafficdensityandcapacitydropatthreefreewaybottlenecks,”
Transportation Research Part B-methodological, vol. 41, pp. 82–95, 2007.
[4] S. Li, H. Yang, M. Li, J. Dai, and P. Wang, “A highway on-ramp control approach integrating percolation bottleneck analysis and
vehicle source identification,” Sustainability, vol. 15, no. 16, 2023.
[5] A. Adler, D. Miculescu, and S. Karaman, “Optimal policies for platooning and ride sharing in autonomy-enabled transportation,”
[6] W.Nie,Y.You,V.C.S.Lee,andY.Duan,“Variablespeedlimitcontrolforindividualvehiclesonfreewaybottleneckswithmixed
humanandautomatedtrafficflows,”2021IEEEInternationalIntelligentTransportationSystemsConference(ITSC),pp.2492–2498,
2021.
[7] X. Xiong, J. Sha, and L. Jin, “Optimizing coordinated vehicle platooning: An analytical approach based on stochastic dynamic
programming,” Transportation Research Part B: Methodological, vol. 150, pp. 482–502, 2021.
[8] D.Li,X.Zhao,andP.Cao,“Anenhancedmotorwaycontrolsystemformixedmanual/automatedtrafficflow,”IEEESystemsJournal,
vol. 14, no. 4, pp. 4726–4734, 2020.
[9] H. Maske, T. Chu, and U. Kalabic´, “Large-scale traffic control using autonomous vehicles and decentralized deep reinforcement
learning,” in 2019 IEEE Intelligent Transportation Systems Conference (ITSC), pp. 3816–3821, 2019.
[10] T. Rashid, M. Samvelyan, C. Witt, G. Farquhar, J. Foerster, and S. Whiteson, “Qmix: Monotonic value function factorisation for
deep multi-agent reinforcement learning,” 03 2018.
[11] S. Wang, Z. Wang, R. Jiang, F. Zhu, R. Yan, and Y. Shang, “A multi-agent reinforcement learning-based longitudinal and lateral
control of cavs to improve traffic efficiency in a mandatory lane change scenario,” Transportation Research Part C: Emerging
Technologies, 2024.
[12] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” ArXiv,
vol. abs/1707.06347, 2017.
[13] M.Treiber,A.Hennecke,andD.Helbing,“Congestedtrafficstatesinempiricalobservationsandmicroscopicsimulations,”Physical
review. E, Statistical physics, plasmas, fluids, and related interdisciplinary topics, vol. 62 2 Pt A, pp. 1805–24, 2000.
[14] E.Vinitsky,N.Lichtlé,K.Parvate,andA.Bayen,“Optimizingmixedautonomytrafficflowwithdecentralizedautonomousvehicles
and multi-agent reinforcement learning,” ACM Trans. Cyber-Phys. Syst., vol. 7, apr 2023.
[15] C.Yu,A.Velu,E.Vinitsky,Y.Wang,A.M.Bayen,andY.Wu,“Thesurprisingeffectivenessofmappoincooperative,multi-agent
games,” ArXiv, vol. abs/2103.01955, 2021.