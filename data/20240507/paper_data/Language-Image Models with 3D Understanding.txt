Language-Image Models with 3D Understanding
Jang Hyun Cho1,2, Boris Ivanovic2, Yulong Cao2, Edward Schmerling2,
Yue Wang2, Xinshuo Weng2, Boyi Li2, Yurong You2,
Philipp Kr¨ahenbu¨hl1,⋆, Yan Wang2,⋆, and Marco Pavone2,⋆
1 UT Austin
2 NVIDIA Research
janghyuncho7@utexas.edu
{bivanovic,yulongc,eschmerling,yuewang,xweng,
boyil,yurongy,yanwan,mpavone}@nvidia.com
Abstract. Multi-modal large language models (MLLMs) have shown
incredible capabilities in a variety of 2D vision and language tasks. We
extend MLLMs’ perceptual capabilities to ground and reason about
images in 3-dimensional space. To that end, we first develop a large-
scale pretraining dataset for 2D and 3D called LV3D by combining
multiple existing 2D and 3D recognition datasets under a common task
formulation: as multi-turn question-answering. Next, we introduce a new
MLLM named Cube-LLM and pre-train it on LV3D. We show that
pure data scaling makes a strong 3D perception capability without 3D
specific architectural design or training objective. Cube-LLM exhibits
intriguing properties similar to LLMs: (1) Cube-LLM can apply chain-
of-thought prompting to improve 3D understanding from 2D context
information.(2)Cube-LLMcanfollowcomplexanddiverseinstructions
and adapt to versatile input and output formats. (3) Cube-LLM can
be visually prompted such as 2D box or a set of candidate 3D boxes
from specialists. Our experiments on outdoor benchmarks demonstrate
that Cube-LLM significantly outperforms existing baselines by 21.3
pointsofAP ontheTalk2Cardatasetfor3Dgroundedreasoningand
BEV
17.7 points on the DriveLM dataset for complex reasoning about driving
scenarios, respectively. Cube-LLM also shows competitive results in
general MLLM benchmarks such as refCOCO for 2D grounding with
(87.0) average score, as well as visual question answering benchmarks
such as VQAv2, GQA, SQA, POPE, etc. for complex reasoning. Our
project is available at https://janghyuncho.github.io/Cube-LLM.
Keywords: Multi-modal Large Language Models · 3D Scene Under-
standing · Foundation Models · Autonomous Driving
1 Introduction
Internet-scale visual data have brought forth the advent of multi-modal large
languagemodels(MLLMs).Richanddiversevisualsupervisionalignspre-trained
⋆ Equal advising
4202
yaM
6
]VC.sc[
1v58630.5042:viXra2 JH Cho et al.
Input Text Output Text
Provide 3D bounding box of the region this (x, y, z, w, h, r) = (0.68, 0.68, 12.32, 4.70,
sentence describes: Black Audi on left. 1.68, 1.94, -2.14)
Input Image Output 3D Box (projected)
Fig.1:Theoverviewof Cube-LLMfor3D-groundedreasoning.Thetaskrequires
a model to take an image, understand the input text prompt (e.g., “Black Audi
on left.”) and ground it in 3-dimensional space.
large language models with billions of parameters to visual modality. The best
MLLMs can recognize, understand, and reason about images and videos far
better than any of specially designed architectures and algorithms [1,52]. The
decades worth of computer vision datasets —image classification, captioning,
object detection, grounding, document parsing, optical character recognition
(OCR)— fuels the powerful MLLMs through jointly training as a next token
predictiontask.Introducingtheabilityto“ground”in2-dimensionalspace(image
coordinates) bridges the low-level perception to high-level reasoning about visual
input, much like human cognition. However, one critical difference is that we
perceive the world in 3-dimensional space (view coordinates). This 3-dimensional
grounding allows us to perceive and reason about the visual input closer to the
actual world, which the current state of MLLMs has not explored yet.
In this work, our goal is to develop a framework to train a MLLM capable
of reasoning in both 2D and 3D spaces. We demonstrate that pure data scaling
can achieve our goal without any 3D specific architectural design or training
objective. We instead focus on careful data curation to address one question:
what tasks will induce 2D to 3D generalization? To this end, we introduce a
large scale language-image pretraining dataset for 2D and 3D, called LV3D.
We start with combining a diverse collection of 2D and 3D vision datasets for
indoors and outdoors and standardize labels to follow the consistent format
across datasets. We blend in the vision datasets with instruction-following data
of MLLM training as a series of question-answer pairs (§ 3.1). Next, we augment
our blended datasets by decomposing the vision labels into easier tasks (e.g.,
3D box → 2D point, depth, size, orientation). This trains our model to adapt
to versatile input and output formats, and connects the underlying 2D and 3D
structure (§ 3.2). Most importantly, we mix in a series of QA pairs about an
objectfor“step-by-step”reasoning,fromeasier(e.g., 2D box)toharder(e.g., 3D
box) task. This directly induces 2D to 3D generalization due to autoregressive
redocnE
noisiV
rezinekoT
)L-
2v
ONID(
MLL
)B7
anuciV(Cube-LLM 3
Open-vocabulary 3D Grounding
“Lily.” “Santa Clause.” “Christmas tree.” “Wooden dog.”
“Skateboard.” “Espresso machine.” “Printer.”
Complex Reasoning
”Where do I go to sleep?” ”Where do I do my homework?” ”Where do I sit down?”
”What should I use to ”Where should I wash ”What should I move to
cool down the room?” my hands?” block sunlight?”
3D Spatial Understanding
”Car right next to the cyclists.” ”Left cyclist.”
”Car farthest from me” ”Car closest to me.” ”Right cyclist.”
”Which car is closest to me?” ”Which car is right behind the ”Which car is right next to the
white hatchback?” forklift?”
Fig.2: Qualitative results of Cube-LLM 3D grounding in 3 aspects: open-
vocabulary understanding (top), complex reasoning (middle), and 3D spatial
understanding (bottom). Best viewed in color, zoomed.4 JH Cho et al.
nature of MLLMs (§ 3.3). Finally, we train a MLLM on LV3D as a single “next
token prediction” task, called Cube-LLM (§ 3.4).
Cube-LLM exhibits a number of intriguing properties. First, Cube-LLM
can self-improve its 3D reasoning performance by prompting with its own 2D
predictions. This visual chain-of-thought reasoning resembles the well-known
behavior of LLMs [59]. Second, Cube-LLM can adapt to versatile input and
output formats and questions, which follows instruction following ability of
LLMs [58]. Finally, Cube-LLM can be prompted with any specialist models for
any additional modalities (e.g., LiDAR) by simply adding their predictions to
the question. Cube-LLM shows remarkable improvement with data-scaling in
both 2D and 3D, for indoor and outdoor scene grounding as well as complex
reasoning tasks such as QA in driving scenarios.
We evaluate our model’s performance in both 3D grounding and 3D complex
reasoning tasks on various indoor and outdoor datasets as well as a standard
MLLM benchmark, and show qualitative results in 3D grounding in non-driving
scenes (Fig. 2). For 3D grounding on the Talk2Car dataset [18], Cube-LLM
surpasses the baselines by 21.3 in Bird’s Eye View (BEV) AP (71.4 vs 50.1) and
by 18.7 in 3D AP (64.1 vs 45.4). Additionally, our training framework improves
the performance of Cube-LLM on the DriveLM [48] dataset, nearly doubling
the performance in the BEV AP (66.0 vs 33.2) for 3D grounding from a baseline.
We also test Cube-LLM on complex reasoning benchmark of driving scenarios
(DriveLM), and improve the overall score by 17.7 (50.1 vs 32.4) compared to
DriveLM baseline [48]. Furthermore, we show that Cube-LLM performs the
state-of-the-art in 2D referring expression comprehension, achieving the average
score of 87.0 on refCOCO/+/g. Finally, we show that Cube-LLM maintains
competitive performance in various MLLM benchmarks including VQAv2, GQA,
etc., confirming that our 3D reasoning capability is an expansion, not a trade-off.
2 Related Work
Vision Language Models. By scaling up pre-training on the internet-scale
dataset, there has been significant progress for VLMs in the 2D vision-language
domain, showing strong capabilities in few-shot generalization. VLBRRT [50]
and ViLBERT [36] capitalized on a BERT-style framework for image-text co-
embedding. CLIP [44] embedded images and text captions into a shared feature
spaceviacontrastivelearningandpioneeredzero-shotvisiontasks.BLIP[31]and
BLIP2 [30] further improved CLIP by leveraging extra pseudo-labeled data and
betterimage/languageencoders.Flamingo[3]anditsopen-sourceimplementation
Open-Flamingo [4] proposed a fast adaptation approach to enable in-context
few-shot learning on novel visual-language tasks. GPT4V [53] and Gemini [52]
furtherdemonstratedstate-of-the-arthuman-levelvisualreasoningabilitythrough
scaling. LLaVA [35] pioneered instruction fine-tuning in the multimodal field.
These works have predominantly focused on the 2D vision and language tasks.
On the other hand, we aim to adapt these MLLMs to enhance their capabilities
for complex 3D reasoning and scene understanding tasks.Cube-LLM 5
Image-grounded Reasoning. With the advancement of multi-modal large
language models, image-grounded reasoning (referring and grounding) has shown
a great progress in 2D space. Image-grounded reasoning requires a model to
localize an object or a region that an input prompt enquires, or describe about a
region of interest. VisionLLM [57] adapts 2D object detector to align with an
LLM, and GPT4-ROI [69] employs hierarchical feature modeling of detectors to
reason about input visual prompt (ROI). Kosmos-2 [43] and Shikra [10] have
shown pure transformer-based visual encoder can surpass using 2D detectors
with data scaling. Recently, Ferret [65] has shown remarkable image-grounded
reasoning from both free-form visual prompt and text prompt. These works
reason in 2D space (image coordinate). To the best of our knowledge, our work
is the first to expand the reasoning capability of a MLLM to 3D.
Reasoning with Language Models in Autonomous Driving. Reasoning is
a long-standing problem in autonomous driving. GPT-Driver [40] incorporated
reasoning into autonomous driving by reformulating motion planning as a lan-
guage modeling problem and introducing fine-tuned LLMs as a motion planner.
DriveGPT4 [62] proposed an end-to-end driving approach that leverages a vision
language model to directly predict actions from sensory inputs. DiLu [60] capi-
talized on a knowledge-driven approach with large language models to improve
reasoning capability. Agent-Driver [41] proposed an LLM-based embodied agent,
achieving state-of-the-art decision-making performance and making it transpar-
ent. Dolphins [39] proposed an instruction fine-tuning pipeline for MLLM-based
autonomous driving. DriveLM [48] opted to use graph visual question answering
totacklereasoningproblemsinautonomousdriving.LMDrive[46]tackledclosed-
loop autonomous driving with language models. Our method is closely related
to these prior works since we attempt to solve visual reasoning problems in
autonomous driving. In contrast to prior work, Cube-LLM can directly reason
in the 3D space for complex AV perception scenarios, and can be trained in an
end-to-end fashion.
3 Unified Language-Image Pretraining for 2D and 3D
Our goal is to expand the capabilities of vision-language models to reason in
3-dimensional space. We propose a unified training framework to learn from both
2D and 3D perceptual data as well as standard image-text pairs. In this section,
we first discuss the data standardization to train a vision-language model at
scale (Sec. 3.1), task scaling to understand perceptual information in versatile
I/O format (Sec. 3.2), visual chain-of-thought reasoning for 3D grounding and
questionansweringtasks(Sec.3.3),andfinally,wepresentCube-LLM,thefinal
model of our unified training framework built on LLaVA-1.5 [34] (Sec. 3.4).
3.1 Data-scaling for Image-based 3D Reasoning
Our goal is to train a single 2D + 3D MLLM from all data sources available. To
standardize many different 2D and 3D grounding tasks into one, we standardize6 JH Cho et al.
the data, phrase all tasks as next token prediction, and format 3D reasoning as a
multi-turn conversation.
Data standardization. We consider points and boxes as our main spatial
representation for 2D and 3D reasoning. We convert every label to either a point
o2D =[xˆ,yˆ] or a bounding box o2D =[xˆ,yˆ,xˆ′,yˆ′]. Similarly, we convert every
point box
3D label to either o3D = [x,y,z] or o3D = [x,y,z,w,h,l,r ,r ,r ] where r ,
point box 1 2 3 1
r , r are Euler angles. We first standardize image-based 3D datasets by unifying
2 3
camera parameters. We follow the procedure of Omni3D [7]; define a virtual
camerawithafixedfocallengthf andtransformdepthz accordingtotheoriginal
camera parameters and the target image size. Since all 3D labels are unified to a
consistent camera intrinsic, we can now convert all x and y coordinates to 2D
projected coordinates (xˆ,yˆ). Consequently, we can represent all label formats to
naturally follow 2D to 3D per-object token sequence:
o2D =[xˆ,yˆ] (1)
point
o2D =[xˆ,yˆ,xˆ′,yˆ′] (2)
box
o3D =[xˆ,yˆ,z] (3)
point
o3D =[xˆ,yˆ,z,w,h,l,r ,r ,r ] (4)
box 1 2 3
whereeachvalueisrepresentedasashortsequenceoftexttokens(3for3-decimal
numbers). This allows the model to predict consistent ordering of token sequence
from 2D to 3D, which improves the understanding of the underlying structure.
With autoregressive models, we first localize objects in image coordinates (xˆ,yˆ),
then infer depth (z), and then infer the size and orientation (w,h,l,r ,r ,r ).
1 2 3
3D reasoning as multi-turn conversations.Now,wecombinethe2Dand3D
data with language-image instruction tuning data of visual language models [35].
For each image and a set of object labels pair, we construct a multi-turn conver-
sational question-answer data (Q , A , Q , A , ..., Q , A ). Each question
1 1 2 2 n n
refers an object with one property b and enquires b :
q a
b ,b ∈{box ,caption,box } (5)
q a 2D 3D
Eachobjectpropertyhasasetofpromptspredefined,suchas‘‘Provide the 3D
bounding box of the region this sentence describes: <caption>’’ for
b = caption and b = box . We combine the meta information of objects
q a 3D
(e.g., attribute, physical state, etc.) with the class name to enrich the textual
information.
3.2 Task-scaling for Versatile I/O Format
We are interested in a generalist model that accepts input and generates output
in versatile formats. Users may want to supplement 2D points or boxes as visual
prompt during inference, or may only want the metric depth of an object instead
of a complete 3D location. This interest in versatile I/O format shares the sameCube-LLM 7
Fig.3:Task-scaling for versatile I/O format.Decomposingtheexistinglabel
formats for 3D grounding task. A complete 3D location can be decomposed into
a center point ([x, y, z]), a depth ([z]), a (projected) 2D point ([x , y ]),
c c
and a (projected) 2D box ([x1, y1, x2, y2]). We define various tasks that
connect among these to train versatile I/O formats. Left: available (decomposed)
annotations. Right: various tasks for training.
spirit of instruction tuning in 2D-based visual language models [35,15,3]. To this
end, we define multiple relevant tasks for a model to adapt to wider spectrum of
similar tasks in 2D and 3D. We start by decomposing the existing label formats
to easier tasks as illustrated in Figure 3. After, we have expanded set of object
properties to construct question-answer pairs:
b ∈{point ,box ,caption,point ,box } (6)
q 2D 2D 3D 3D
b ∈{point ,box ,caption,depth,point ,box } (7)
a 2D 2D 3D 3D
We construct up to n=30 question answer pairs (Qbq,A ) sampled at random
ba ba
for each data. We combine a collection of 2D and 3D vision datasets (LV3D),
summarized in Table 1, and jointly train with this expanded set of tasks.
3.3 Visual Chain-of-Thought Prompting
One of the most intriguing properties of large language models is its emergent
ability to improve reasoning with intermediate steps [59]. This mostly attributes
to vast corpus of rich text data with numerous step-by-step question answering
samples [58]. We artificially supplement this step-by-step reasoning of 3D by
interleaving multiple questions of the same object from easy-to-hard order (the8 JH Cho et al.
Fig.4:Cube-LLM inference with prompting.Left:VisualChain-of-Thought
Prompting to reason in 3D step-by-step. Right: Incorporating specialist models
to further improve localization of Cube-LLM. Our model can either predict
directly from text prompt, or with visual chain-of-thought prompting, or with
specialist predictions as prompt.
left part of Figure. 4):

p(A |Qcaption) question 1
 box2D box2D
maximize p(A |Qcaption,A ,Qcaption) question 2 (8)

...box3D box2D box2D box3D
Furthermore,weallowtest-timeadaptationtoanyspecialistmodelsbymixingin
candidate objectsasasystemprompt(therightpartofFigure.4).Thiseffectively
alleviates the problem of localizing in 3D to “choosing the appropriate box from
candidates”,
maximize p(A |S ,Qcaption) (9)
box3D box3D box3D
where S is a set of candidate boxes, which can be provided by any specialist
box3D
models (depending on available input modalities) at inference. During training,
we use the ground truth boxes with a prompt ‘‘Here is the list of 3DCube-LLM 9
bounding boxes of all objects around the camera:’’andourmodeldoes
not bind with any particular specialist model.
3.4 Cube-LLM
WeintroduceCube-LLM,amulti-modallargelanguagemodelbasedonLLaVA-
1.5 architecture trained to reason in both 2D and 3D. Although we maintain
the generality of model architecture, we make simple yet critical changes to the
original LLaVA. We first replace the CLIP visual encoder with DINOv2 [42], and
undergo the same alignment step of the original LLaVA. Although DINOv2 is
not a text-aligned visual encoder like CLIP, we found minimal degradation in
the standard visual language model benchmarks while significantly improving
3D-related tasks. Then, we finetune the language model (Vicuna-7B [13]) while
freezing the visual encoder and jointly on LLaVA instruction-following data and
the 2D part of LV3D following Sec. 3.1, 3.2 and 3.3. We use low image resolution
(336×336)andtrainwithalargebatchsize.Then,weaddanadditionalfinetuning
stageforbothvisualandlanguagemodelswithhighresolutionimages(672×672)
of the full LV3D. For 3D representation, we use log-scale for depth and all others
remain unchanged. For 2D, we normalize image coordinates between 0 and 999.
For 3D, we we filter-out all annotations outside X and X (different for
min max
each xˆ,yˆ,z,w,h,l,r ,r ,r , more in supplementary), and normalize between 0
1 2 3
and 999.
4 Experiments
We evaluate the effectiveness of Cube-LLM in three aspects: (1) 3D-grounded
reasoning for indoor and outdoor scenes, (2) complex reasoning in 3D, and (3)
standard vision-language benchmarks.
4.1 Implementation Details
We use LLaVA-1.5 [34] with Vicuna-7B as our base model. We replace the CLIP
visual encoder with ViT-L/14 [19] based DINOv2. For all localization outputs,
we use 3 decimal places with text tokens, and keep 3 tokens per value (e.g.,
[021,521, ...]). Accordingly, we pre-process all LLaVA instruction-following
data to reflect this change. We follow the same alignment step to train the
MLP projection layers with the same training setup in [34]. For 2D and 3D
pretraining,weuserandomsamplingfollowingthesamplingrateinTable1.Each
data sample (image-annotation pair) is converted to the multi-turn conversation
format (Fig. 3) sampled at random. During pretraining, we use 8×8 A100s with
a batch size of 1024 and train the model with a learning rate lr =2×10−5 on
images with 336×336 resolution. Then, we fine-tune all parameters including
the visual encoder on a higher resolution 672×672 with 8×8 A100s and a batch
size of 256 with 4 gradient accumulation steps (effective batch size of 1024) and
a learning rate lr =2×10−5.10 JH Cho et al.
Table 1: 2D and 3D Language-Image Pretraining Dataset (LV3D). Sum-
mary of components detailing the number of images, tasks, availability of 2D and
3D labels, the number of QAs and objects, and their multiples during training
(stage 1 and stage 2). ⋆: Only used 2D bounding box.
dataset images labels labels captions #QAs stage1stage2
2D 3D
LLaVAdata[35] 80K ✓ ✗ ✓ 158K 1 0.5
refCOCO/+/g[67] 67K ✓ ✗ ✓ 154K 1 0.5
GRIT(subset)[43]4M ✓ ✗ ✓ 6.9M 1 0.3
AS(filtered)[56] 3.7M ✓ ✗ ✓ 13.2M 1 0.5
COCO[33] 118K ✓ ✗ ✗ 860K 1 0.5
Objects365[47] 600K ✓ ✗ ✗ 25.4M 0.3 0.2
SUN-RGBD[49] 5K ✓ ✓ ✗ 41K 1⋆ 5
Hypersim[45] 67K ✓ ✓ ✗ 2M 1⋆ 5
ArkitScenes[6] 53K ✓ ✓ ✗ 420K 1⋆ 5
Objectron[2] 37K ✓ ✓ ✗ 43K 1⋆ 5
KITTI[22] 4K ✓ ✓ ✗ 25K 1⋆ 5
NuScenes[8] 40K ✓ ✓ ✗ 1.1M 1⋆ 2
Lyft[25] 105K ✓ ✓ ✗ 723K 0 2
Argoverse2[61] 79K ✓ ✓ ✗ 915K 0 4
Waymo[51] 680K ✓ ✓ ✗ 5.1M 0 0.4
Total 9.6M ✓ ✓ ✓ 40.9M 0.87 0.52
4.2 Datasets
We pre-train Cube-LLM on LV3D, and then fine-tune it on the training split of
the target datasets, Talk2Car and DriveLM.
Talk2Car [18] is a 3D referring expression comprehension dataset of various
driving scenarios. It consists of 8,349 training samples and 1,163 validation
samples with images and LiDAR data. It provides rich question-answer pairs
groundedtoanobjectintheimage.Eachobjectislabeledwithasituationaltext
that uniquely identifies the object (e.g., “Wow hold on! That looks like my stolen
bike over there! Drop me off next to it.”). The original benchmark [18] evaluates
the 2D grounding performance with the AP metric. MSSG [12] extends the
0.5
task to 3D grounding and evaluates on both BEV AP and 3D AP.
DriveLM [48] is a recently released question-answering dataset for autonomous
driving based on the nuScenes dataset [8]. It consists of various driving scenes
with multi-view images and LiDAR point clouds, as well as frame-level question-
answering data, and has a total of 4,871 frames. Each frame contains 91.4
question-answer pairs on average, covering core autonomous driving tasks such
as perception, prediction, and planning, as well as a short description and 2D
boxes of important objects. To evaluate Cube-LLM, we construct another 3D
grounding benchmark based on the DriveLM dataset, which we call DriveLM-
Grounding. We associate the 2D boxes with the nuScenes 3D bounding boxes
by computing the IoU between 2D boxes projected from 3D labels and the 2D
boxes of labeled important objects, and only keep those with a IoU greater than
0.35. After association, DriveLM-Grounding has a total of 13,287 images, about
one annotation per image. We also use the DriveLM-QA data from the originalCube-LLM 11
Table 2: Talk2Car Benchmark for 2D and 3D Grounding. We denote C as
CameraandLasLiDAR.†:weusethetop-30predictedboxesofCenterPoint[64]
as visual prompt as illustrated in Figure 4. AP and AP follow MSSG [12] that
A B
apply different IoU threshold for each category.
2D BEV 3D
Method Input
AP0.5APAAPBAPAAPB
2DSpecialist
Talk2Car-2D[18] C 50.5 - - - -
VL-Bert[50] C 63.1 - - - -
ViLBERT[36] C 68.9 - - - -
CMRT[38] C 69.1 - - - -
StackedVLBert[14] C 71.0 - - - -
FA[17] C 73.5 - - - -
Cube-LLM(zero-shot) C 46.3 32.0 19.5 22.3 9.8
Cube-LLM C 79.2 46.3 30.1 34.7 18.2
3DSpecialist
Talk2Car-3D[18] L+C - 30.6 24.4 27.9 19.1
MSSG[12] L+C - 50.1 35.7 45.4 23.7
Cube-LLM† L+C 76.3 71.461.264.139.8
Table 3: DriveLM-Grounding benchmark for 3D grounding. LV3D (2D)
indicates that only 2D data in the pre-train dataset is included. We finetune
Cube-LLM and LLaVA-1.5 [34] on the DriveLM-Grounding dataset. Cube-
LLM pre-trained with LV3D achieves a 99% improvement compared to LLaVA-
1.5 on the APBEV metric.
A
BEV 3D
Method Pre-trainData
APBEVAPBEVAP3DAP3D
A B A B
LLaVA-1.5[34]LLaVAdata 33.2 16.3 21.7 7.7
Cube-LLM LLaVAdata 39.6 21.7 25.8 10.5
Cube-LLM LV3D(2D) 50.5 31.2 32.5 17.3
Cube-LLM LV3D 66.0 52.1 56.2 40.5
DriveLM to fine-tune Cube-LLM for complex reasoning tasks. The original
training split has 696 scenes in total. We use 600 scenes for training and 96
scenes for validation, which we include the DriveLM provided scenes for sample
evaluation and Talk2Car validation split scenes. We evaluate 3D grounding with
the same BEV AP and 3D AP metric as those in Talk2Car.
4.3 3D-Grounded Reasoning
Our results for 3D grounding on the Talk2Car dataset are detailed in Table
2, which is structured according to the input modalities used for 3D ground-
ing. The baselines that rely solely on camera inputs are only evaluated on
2D grounding, whereas those incorporating both camera and LiDAR inputs
are evaluated on both 2D and 3D grounding. Cube-LLM is pre-trained on
LV3D and fine-tuned on Talk2Car with resolution 672×672. We apply visual12 JH Cho et al.
Table 4: Inodoor 3D Grounding Benchmark. Here we compare Cube-LLM
trained on “small” subset of LV3D and the full LV3D. Although the subset and
full LV3D share the same indoor datasets, the added 2D data and outdoor 3D
data translate to better indoor 3D grounding result.
Objectron[2] ArkitScenes[6] SUN-RGBD[49]
Pre-trainData
mAPcls mAPcls+loc mAPcls mAPcls+loc mAPcls mAPcls+loc
3D 3D 3D 3D 3D 3D
LV3D-small 56.7 36.1 21.6 28.3 25.5 25.5
LV3D 69.8 45.4 23.5 31.8 29.7 28.8
∆ 13.1 9.3 1.9 3.5 4.2 3.3
Table 5: Referring Expression Comprehension Benchmark. We compare
Cube-LLM with other MLLMs for general 2D grounding tasks. Cube-LLM
consistently performs best in all data splits in refCOCO.
RefCOCO RefCOCO+ RefCOCOg
Models Size
val testAtestB val testAtestB val test Avg.
Specialist
MAttNet[66] 76.4 80.4 69.3 64.9 70.3 56.0 66.7 67.0 68.9
OFA-L[55] 80.0 83.7 76.4 68.3 76.0 61.8 67.6 67.6 72.7
TransVG[16] 81.0 82.7 78.4 64.8 70.7 56.9 68.7 67.7 71.4
UNITER[11] 81.4 87.0 74.2 75.9 81.5 66.7 74.0 68.7 76.2
VILLA[20] 82.4 87.5 74.8 76.2 81.5 66.8 76.2 76.7 77.8
UniTAB[63] 86.3 88.8 80.6 78.7 83.2 69.5 80.0 80.0 80.6
MDETR[27] 86.8 89.6 81.4 79.5 84.1 70.6 81.6 80.9 81.8
Generalist
LLaVA-1.5[34] 7B 75.6 82.1 66.9 65.5 76.2 53.9 68.9 69.1 69.8
VisionLLM-H[57] 7B 86.7 - - - - - - - -
Shikra[10] 7B 87.0 90.6 80.2 81.6 87.4 72.1 82.3 82.2 82.9
Ferret[65] 7B 87.5 91.4 82.5 80.8 87.4 73.1 83.9 84.8 83.9
MiniGPT-v2[9] 7B 88.7 91.7 85.3 80.0 85.1 74.5 84.4 84.7 83.8
LLaVA-G[68] 7B 89.2 - - 81.7 - - 84.8 - -
Qwen-VL[5] 7B 88.6 92.3 84.5 82.8 88.6 76.8 86.0 86.3 85.7
Cube-LLM 7B 90.9 92.6 87.9 83.9 89.2 77.4 86.6 87.2 87.0
Table 6: MLLM Benchmarks. We compare Cube-LLM with other MLLMs
for various visual question-answering tasks.
Model SizeVQAv2 [23]GQA[26]VizWiz[24]SQAI [37]POPE[32]
BLIP-2[30] 13B 41.0 41.0 19.6 61.0 85.3
InstructBLIP[15] 7B - 49.2 34.5 60.5 -
InstructBLIP[15] 13B - 49.5 33.4 63.1 78.9
IDEFICS[29] 9B 50.9 38.4 35.5 - -
Shikra[10] 13B 77.4 - - - -
Qwen-VL[5] 7B 78.8 59.3 35.2 67.1 -
Qwen-VL(chat)[5] 7B 78.2 57.5 38.9 68.2 -
miniGPT-v2[9] 7B - 60.1 53.6 - -
LLaVA-1.5[34] 7B 78.5 62.0 50.0 66.8 85.9
Cube-LLM 7B 78.3 62.4 51.0 69.2 87.1
Chain-of-Thought when predicting the 3D grounding. Remarkably, our camera-Cube-LLM 13
Table 7: Improvements of data scaling. This table illustrates how the in-
clusion of different datasets affects performance on Talk2Car 3D grounding
benchmark. Note that we do not perform visual chain-of-thought prompting
nor high-resolution finetuning for better comparison. We evaluate the model on
image resolution 336×336.
LLaVArefC.COCOTalk2CarNus.O365DriveLMOmni3DAPBEVAPBEVAP3DAP3D
A B A B
✓ 19.7 9.4 10.3 3.3
✓ ✓ 25.9 11.3 12.1 3.9
✓ ✓ ✓ 27.0 14.0 14.8 5.7
✓ ✓ ✓ ✓ ✓ 33.6 20.8 23.4 13.3
✓ ✓ ✓ ✓ ✓ ✓ ✓ 42.4 27.7 31.0 16.2
✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ 44.7 28.7 32.0 16.5
Table 8: Visual Chain-of-Thought ablations. We evaluate Cube-LLM with
and without visual Chain-of-thoughts (VCoT) prompting during inference on the
Talk2Car dataset for the 3D grounding task.
VCoTAPBEVAPBEVAP3DAP3D
A B A B
43.6 28.2 32.7 15.9
✓ 46.3 30.1 34.7 18.2
only Cube-LLM significantly surpasses the state-of-the-art model FA [17] by
5.7 points on 2D AP . Surprisingly, Cube-LLM also outperforms the cam-
0.5
era+LiDAR baseline, Talk2Car-3D [18], by 15.7 points on the BEV AP metric.
A
Our camera-only Cube-LLM is only 3.8 points behind the state-of-the-art cam-
era+LiDAR baseline MSSG [12]. MSSG [12] utilized the LiDAR point encoder
similar to CenterPoint [64] as well as image and text encoders for predicting 3D
grounding. Similarly, we leverage the LiDAR modality by using the predictions
from CenterPoint [64], selecting the top-30 boxes with the highest confidence
as a visual prompt. We observe a substantial 25.1 points improvements in AP ,
A
outperforming MSSG [12] by 21.3 points. Furthermore, We observe a similar
trendontheDriveLM-Groundingdataset,showninTable3.SimilartoTalk2Car,
Cube-LLM shows significant improvements compared to directly finetuning
fromLLaVA-1.5,resultingina32.8pointsimprovementontheBEVAP metric.
A
4.4 Complex Reasoning in 3D
To show the effectiveness of 3D reasoning capability, we finetune Cube-LLM on
DriveLM-QA dataset. The dataset comprises questions about perception (e.g.,
“what are the objects worth noting in the current scenario?”), prediction (e.g.,
“wheremightthevan,thesedan,andthepedestrianmoveinthefuture?),planning
(e.g., “what are the safe actions of the ego car considering those objects?”) and
behavior (e.g., “what would the ego vehicle’s next action would be?”). We
compareCube-LLMwithLLaVA-1.5[34]toshowtheimpactofourpretraining,
as well as the official baseline [48] that has been recently released. All models14 JH Cho et al.
Table 9: DriveLM QA Benchmark. All models are finetuned on a subset of
DriveLM from held-out scenes. †: we finetune LLaVA-1.5 on a subset of DriveLM
train split. DriveLM baseline finetunes LLaMA Adapter V2 [21] on a subset of
DriveLM train split. Top rows show models evaluated on the “same split” as the
DriveLM baseline. Bottom rows shows models evaluated on a larger test split
(10 random scenes) held-out from all training. ‡: DriveLM result on test data
reported by authors.
Method Acc. ChatGPT Match BLEU1 ROUGEL CIDEr Overall
baselinesplit
DriveLMbaseline[48] 0.0 65.1 28.3 5.0 8.4 9.9 32.4
LLaVA-1.5[34]† 38.5 53.5 26.1 15.8 14.3 30.3 36.1
Cube-LLM 38.5 89.4 39.0 16.3 20.4 31.3 50.1
oursplit
DriveLMbaseline[48]‡ 0.0 67.8 18.8 23.8 19.9 0.7 32.8
LLaVA-1.5[34]† 24.1 75.6 36.4 13.2 16.7 25.5 43.8
Cube-LLM 32.4 74.0 39.2 13.3 17.9 25.6 45.4
use 7-B scale LLM (Vicuna-7B [13] or LLaMA-7B [54]) and are fine-tuned on
subset of DriveLM train split. Top rows are the result on scenes held out by
the authors and bottom rows are our additional split to evaluate models on a
larger testset. The evaluation metric is based on accuracy, match (localization),
BLEU/ROUGE /CIDEr, and ChatGPT score for favorable text generation. In
L
both setting, Cube-LLM show competitive result consistently.
4.5 General MLLM Benchmarks
We show the performance of Cube-LLM on general MLLM benchmarks. In
Table5,wecompareCube-LLMtothestate-of-the-artsinReferringExpression
Comprehension (REC) benchmark on refCOCO/+/g [67] dataset. We compare
Cube-LLM to specialist models such as MDETR [27] and UniTAB [63] which
employs detection-specific architecture, and generalist models of same size such
as Ferret [65], Qwen-VL [5] and MiniGPT-v2 [9]. In all test splits, Cube-LLM
consistently outperforms with average score of 87.0. In Table 6, we compare
Cube-LLM with other competitive MLLMs of same model size on VQAv2 [23],
GQA [26], VizWiz [24], ScienceQA-Image [37], and POPE [32]. The first row has
modelswithfullyzero-shotevaluation,andbottomrowshasmodelsthathasseen
images from some of the datasets. Compared to LLaVA-1.5 [34], miniGPT-v2 [9]
and Qwen-VL [5], Cube-LLM maintain competitive result, validating that our
3D understanding does not degrade general reasoning capability of MLLM.
4.6 Ablation Study
Our work consists of three key contributions, including a large-scale language-
visual pre-training dataset LV3D, visual chain-of-thought prompting, and special-
ist prompting. We provide comprehensive ablation studies for each of our design
choices for Cube-LLM."Stop in front of the man in the yellow hat."
Cube-LLM 15
"Pull up behind the guy wearing a white shirt. He is my uncle."
Visual Chain-of-Thought Prompting
“Turn“T luerfnt lheeft rhee, rpe,a prka rbk ebheihnindd t hthee wwhhiittee vvaann, ,l eleft fot fo thf eth bela cbkla ccakr. ”car.”
Specialist Model Prompting
“Stop close to thi“sP barikk eb eshoi nId c tahnis sreede icfa irt. ”is my friend’s or not.”
Fig.5:Cube-LLM inference with prompting.Top:VisualChain-of-Thought
Promptingtoreasonin3Dstep-by-step.Bottom:Incorporatingspecialistmodels
tofurtherimprovelocalizationof Cube-LLM.Blue3Dboxesarethepredictions
of CenterPoint on corresponding LiDAR points.
Data-scaling effect.LV3Dcom“Sbtoipn neeasr sJoeev ienn thed tarutcak.s”ets,cumulativelyencompassing
1.1 million images. To illustrate the impact of data-scaling on the performance,
datasetsareincrementallyintegratedduringpre-trainingstage.WeemployCube-
LLM without Visual Chain-of-Thought (VCoT) and fine-tune it at the image
resolution336×336.TheresultsarepresentedinTable7.Weobserveaconsistent
improvement in performance, where AP grows from 19.7 to 44.7, improving by
25.0 points. It is also noteworthy that the incorporation of 2D data during
pre-training benefits the m“Puolld aeloln,gseidvee thne ftirsht obaurrgiehr ont hthee rifighnt.”e-tuning is performed for
3D grounding tasks - we observe a significant 6.3 points performance gain after
including refCOCO and COCO. Furthermore, in Table 4 we show the effect of
increasing the pretraining data on indoor object grounding benchmarks. We
evaluate Cube-LLM pre-trained on “small” subset of LV3D and the full LV3D,
on Objectron [2], ArkitScenes [6], and SUN-RGBD [49] datasets. The “small”
LV3D and full LV3D contain the same amount of 3D indoor data (more details
inthe supplementarymaterials). Weuse theclass name astext toprompt Cube-
LLM, or class name and 3D location (left,center,right,close,far). We describe
the prompt generation process in more detail in our supplementary materials.
We average the precision at IOU thresholds τ ∈[0.15,0.25,0.5].
3D
Visual Chain-of-thoughts (VCoT) prompting during inference. We
evaluateCube-LLMonTalk2CarwithandwithoutVCoT.AsshowninTable8,
employingVCoTpromptingyieldssignificantenhancementsinperformance,with
a improvement of (2.7, 1.9, 2.0, 2.3) points observed. It demonstrates that our
VCoT is able to effectively bridge the gap between 2D semantic reasoning and
3D geometry reasoning compared to directly reasoning for 3D from text prompt.
Figure 5 (top) visualizes the process.16 JH Cho et al.
Impact of specialist prompting during inference. Specialist prompting
can leverage new input modality, such as LiDAR. As demonstrated in Table 2
on Talk2Car dataset, employing CenterPoint [64] predictions as visual prompts
significantly improves the performance of Cube-LLM with gains of 25.1, 30.1,
29.4, 21.6 points in 3D grounding metrics. Note that Cube-LLM needs neither
CenterPoint nor LiDAR points during training, which means user can choose
any specialist model based on the available input modality. Figure 5 (bottom)
visualizes the process.
5 Conclusion
In this paper, we present Cube-LLM, a multi-modal language model that can
reason in both 2D and 3D. We provide a collection of dataset (LV3D) and a
training framework to effectively scale MLLM training for 3D understanding. We
evaluate Cube-LLM in 2D and 3D grounded reasoning and VQA tasks, and
show competitive results. We also show that Cube-LLM exhibits the behaviors
of LLMs such as chain-of-thought prompting to further improve 3D localization
of our model. Finally, we show that our model can adapt any specialist models
during inference by prompting their predictions as visual prompts. We examine
that pure transformer-based MLLM with minimal inductive bias can learn about
3D understanding solely by data scaling.
References
1. Openai chat. https://openai.com/research/gpt-4v-system-card, accessed:
2023-10-20 2
2. Ahmadyan, A., Zhang, L., Ablavatski, A., Wei, J., Grundmann, M.: Objectron:
A large scale dataset of object-centric videos in the wild with pose annotations.
CVPR (2021) 10, 12, 15, 2
3. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K.,
Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han,
T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock,
A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O.,
Zisserman, A., Simonyan, K.: Flamingo: a visual language model for few-shot
learning. ArXiv abs/2204.14198 (2022), https://api.semanticscholar.org/
CorpusID:248476411 4, 7
4. Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe,
K., Bitton, Y., Gadre, S.Y., Sagawa, S., Jitsev, J., Kornblith, S., Koh, P.W.,
Ilharco, G., Wortsman, M., Schmidt, L.: Openflamingo: An open-source framework
for training large autoregressive vision-language models. ArXiv abs/2308.01390
(2023), https://api.semanticscholar.org/CorpusID:261043320 4
5. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.:
Qwen-vl: A versatile vision-language model for understanding, localization, text
reading, and beyond. arXiv preprint arXiv:2308.12966 (2023) 12, 14
6. Baruch, G., Chen, Z., Dehghan, A., Dimry, T., Feigin, Y., Fu, P., Gebauer, T.,
Joffe, B., Kurz, D., Schwartz, A., Shulman, E.: ARKitscenes - a diverse real-world
dataset for 3d indoor scene understanding using mobile RGB-d data. In: NeurIPS
Datasets and Benchmarks Track (Round 1) (2021) 10, 12, 15, 2Cube-LLM 17
7. Brazil, G., Kumar, A., Straub, J., Ravi, N., Johnson, J., Gkioxari, G.: Omni3D: A
large benchmark and model for 3D object detection in the wild. In: CVPR. IEEE,
Vancouver, Canada (June 2023) 6, 1
8. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A.,
Pan, Y., Baldan, G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous
driving. arXiv preprint arXiv:1903.11027 (2019) 10
9. Chen,J.,Zhu,D.,Shen,X.,Li,X.,Liu,Z.,Zhang,P.,Krishnamoorthi,R.,Chandra,
V.,Xiong,Y.,Elhoseiny,M.:Minigpt-v2:largelanguagemodelasaunifiedinterface
for vision-language multi-task learning. arXiv preprint arXiv:2310.09478 (2023) 12,
14
10. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleashing
multimodalllm’sreferentialdialoguemagic.arXivpreprintarXiv:2306.15195(2023)
5, 12
11. Chen, Y.C., Li, L., Yu, L., El Kholy, A., Ahmed, F., Gan, Z., Cheng, Y., Liu, J.:
Uniter: Universal image-text representation learning. In: European conference on
computer vision. pp. 104–120. Springer (2020) 12
12. Cheng,W.,Yin,J.,Li,W.,Yang,R.,Shen,J.:Language-guided3dobjectdetection
in point cloud for autonomous driving. arXiv preprint arXiv:2305.15765 (2023) 10,
11, 13
13. Chiang, W.L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S.,
Zhuang, Y., Gonzalez, J.E., Stoica, I., Xing, E.P.: Vicuna: An open-source chatbot
impressing gpt-4 with 90%* chatgpt quality (March 2023), https://lmsys.org/
blog/2023-03-30-vicuna/ 9, 14
14. Dai, H., Luo, S., Ding, Y., Shao, L.: Commands for autonomous vehicles by
progressivelystackingvisual-linguisticrepresentations.In:ECCVWorkshops(2020)
11
15. Dai,W.,Li,J.,Li,D.,Tiong,A.M.H.,Zhao,J.,Wang,W.,Li,B.A.,Fung,P.,Hoi,
S.C.H.: Instructblip: Towards general-purpose vision-language models with instruc-
tiontuning.ArXivabs/2305.06500(2023),https://api.semanticscholar.org/
CorpusID:258615266 7, 12, 4
16. Deng, J., Yang, Z., Chen, T., gang Zhou, W., Li, H.: Transvg: End-to-end visual
grounding with transformers. 2021 IEEE/CVF International Conference on Com-
puter Vision (ICCV) pp. 1749–1759 (2021), https://api.semanticscholar.org/
CorpusID:233296838 12
17. Deruyttere, T., Grujicic, D., Blaschko, M.B., Moens, M.F.: Talk2car: Predicting
physical trajectories for natural language commands. IEEE Access (2022) 11, 13
18. Deruyttere,T.,Vandenhende,S.,Grujicic,D.,VanGool,L.,Moens,M.F.:Talk2car:
Taking control of your self-driving car. In: Proceedings of the 2019 Conference on
EmpiricalMethodsinNaturalLanguageProcessingandthe9thInternationalJoint
Conference on Natural Language Processing (EMNLP-IJCNLP). pp. 2088–2098
(2019) 4, 10, 11, 13
19. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N.:
An image is worth 16x16 words: Transformers for image recognition at scale. ICLR
(2021) 9
20. Gan, Z., Chen, Y.C., Li, L., Zhu, C., Cheng, Y., Liu, J.: Large-scale adversar-
ial training for vision-and-language representation learning. Advances in Neural
Information Processing Systems 33, 6616–6628 (2020) 12
21. Gao,P.,Han,J.,Zhang,R.,Lin,Z.,Geng,S.,Zhou,A.,Zhang,W.,Lu,P.,He,C.,
Yue, X., Li, H., Qiao, Y.: Llama-adapter v2: Parameter-efficient visual instruction
model. arXiv preprint arXiv:2304.15010 (2023) 1418 JH Cho et al.
22. Geiger, A., Lenz, P., Urtasun, R.: Are we ready for autonomous driving? the kitti
vision benchmark suite. In: CVPR (2012) 10
23. Goyal,Y.,Khot,T.,Summers-Stay,D.,Batra,D.,Parikh,D.:Makingthevinvqa
matter: Elevating the role of image understanding in visual question answering. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 6904–6913 (2017) 12, 14
24. Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J., Bigham,
J.P.: Vizwiz grand challenge: Answering visual questions from blind people. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 3608–3617 (2018) 12, 14
25. Houston, J., Zuidhof, G., Bergamini, L., Ye, Y., Chen, L., Jain, A., Omari, S.,
Iglovikov, V., Ondruska, P.: One thousand and one hours: Self-driving motion
prediction dataset. In: Conference on Robot Learning. pp. 409–418. PMLR (2021)
10, 2
26. Hudson, D.A., Manning, C.D.: Gqa: A new dataset for real-world visual reasoning
andcompositionalquestionanswering.In:ProceedingsoftheIEEE/CVFconference
on computer vision and pattern recognition. pp. 6700–6709 (2019) 12, 14
27. Kamath, A., Singh, M., LeCun, Y., Synnaeve, G., Misra, I., Carion, N.: Mdetr-
modulated detection for end-to-end multi-modal understanding. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision. pp. 1780–1790
(2021) 12, 14
28. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In: Proceedings
of the IEEE/CVF International Conference on Computer Vision. pp. 4015–4026
(2023) 1
29. Lauren¸con, H., Saulnier, L., Tronchon, L., Bekman, S., Singh, A., Lozhkov, A.,
Wang, T., Karamcheti, S., Rush, A.M., Kiela, D., Cord, M., Sanh, V.: Obelics: An
open web-scale filtered dataset of interleaved image-text documents (2023) 12
30. Li, J., Li, D., Savarese, S., Hoi, S.: BLIP-2: bootstrapping language-image pre-
trainingwithfrozenimageencodersandlargelanguagemodels.In:ICML(2023) 4,
12
31. Li, J., Li, D., Xiong, C., Hoi, S.: Blip: Bootstrapping language-image pre-training
for unified vision-language understanding and generation. In: ICML (2022) 4
32. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.R.: Evaluating object
hallucinationinlargevision-languagemodels.In:The2023ConferenceonEmpirical
MethodsinNaturalLanguageProcessing(2023),https://openreview.net/forum?
id=xozJw0kZXF 12, 14
33. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll´ar, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision–
ECCV2014:13thEuropeanConference,Zurich,Switzerland,September6-12,2014,
Proceedings, Part V 13. pp. 740–755. Springer (2014) 10
34. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning
(2023) 5, 9, 11, 12, 13, 14
35. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: NeurIPS (2023) 4,
6, 7, 10
36. Lu,J.,Batra,D.,Parikh,D.,Lee,S.:Vilbert:Pretrainingtask-agnosticvisiolinguis-
tic representations for vision-and-language tasks. Advances in neural information
processing systems 32 (2019) 4, 11
37. Lu,P.,Mishra,S.,Xia,T.,Qiu,L.,Chang,K.W.,Zhu,S.C.,Tafjord,O.,Clark,P.,
Kalyan, A.: Learn to explain: Multimodal reasoning via thought chains for scienceCube-LLM 19
question answering. In: The 36th Conference on Neural Information Processing
Systems (NeurIPS) (2022) 12, 14
38. Luo, S., Dai, H., Shao, L., Ding, Y.: C4av: Learning cross-modal representations
from transformers. In: ECCV Workshops (2020) 11
39. Ma, Y., Cao, Y., Sun, J., Pavone, M., Xiao, C.: Dolphins: Multimodal language
model for driving. arXiv preprint arXiv:2312.00438 (2023) 5
40. Mao, J., Qian, Y., Zhao, H., Wang, Y.: Gpt-driver: Learning to drive with gpt.
arXiv preprint arXiv:2310.01415 (2023) 5
41. Mao,J.,Ye,J.,Qian,Y.,Pavone,M.,Wang,Y.:Alanguageagentforautonomous
driving. arXiv preprint arXiv:2311.10813 (2023) 5
42. Oquab, M., Darcet, T., Moutakanni, T., Vo, H.V., Szafraniec, M., Khalidov, V.,
Fernandez,P.,HAZIZA,D.,Massa,F.,El-Nouby,A.,Assran,M.,Ballas,N.,Galuba,
W.,Howes,R.,Huang,P.Y.,Li,S.W.,Misra,I.,Rabbat,M.,Sharma,V.,Synnaeve,
G., Xu, H., Jegou, H., Mairal, J., Labatut, P., Joulin, A., Bojanowski, P.: DINOv2:
Learning robust visual features without supervision. Transactions on Machine
Learning Research (2024), https://openreview.net/forum?id=a68SUt6zFt 9
43. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-
2: Grounding multimodal large language models to the world. arXiv preprint
arXiv:2306.14824 (2023) 5, 10, 1, 2
44. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
natural language supervision. In: International conference on machine learning. pp.
8748–8763. PMLR (2021) 4
45. Roberts, M., Ramapuram, J., Ranjan, A., Kumar, A., Bautista, M.A., Paczan, N.,
Webb, R., Susskind, J.M.: Hypersim: A photorealistic synthetic dataset for holistic
indoor scene understanding. In: ICCV (2021) 10
46. Shao, H., Hu, Y., Wang, L., Waslander, S.L., Liu, Y., Li, H.: Lmdrive: Closed-loop
end-to-end driving with large language models. arXiv preprint arXiv:2312.07488
(2023) 5
47. Shao,S.,Li,Z.,Zhang,T.,Peng,C.,Yu,G.,Zhang,X.,Li,J.,Sun,J.:Objects365:
A large-scale, high-quality dataset for object detection. In: 2019 IEEE/CVF Inter-
national Conference on Computer Vision (ICCV) (2019) 10
48. Sima, C., Renz, K., Chitta, K., Chen, L., Zhang, H., Xie, C., Luo, P., Geiger,
A., Li, H.: Drivelm: Driving with graph visual question answering. arXiv preprint
arXiv:2312.14150 (2023) 4, 5, 10, 13, 14, 2
49. Song, S., Lichtenberg, S.P., Xiao, J.: Sun rgb-d: A rgb-d scene understanding
benchmark suite. In: CVPR (2015) 10, 12, 15, 2
50. Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.: Vl-bert: Pre-training of
generic visual-linguistic representations. In: International Conference on Learning
Representations (2020), https://openreview.net/forum?id=SygXPaEYvH 4, 11
51. Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo,
J., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H.,
Timofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J.,
Chen, Z., Anguelov, D.: Scalability in perception for autonomous driving: Waymo
open dataset. In: Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR) (June 2020) 10, 2
52. Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.B., Yu, J., Soricut, R.,
Schalkwyk, J., Dai, A.M., Hauth, A., et al.: Gemini: a family of highly capable
multimodal models. arXiv preprint arXiv:2312.11805 (2023) 2, 4
53. team, O.: Gpt-4 technical report (2023), https://api.semanticscholar.org/
CorpusID:257532815 420 JH Cho et al.
54. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient
foundation language models. arXiv preprint arXiv:2302.13971 (2023) 14
55. Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou,
J., Yang, H.: Ofa: Unifying architectures, tasks, and modalities through a simple
sequence-to-sequence learning framework. CoRR abs/2202.03052 (2022) 12
56. Wang, W., Ren, Y., Luo, H., Li, T., Yan, C., Chen, Z., Wang, W., Li, Q., Lu, L.,
Zhu, X., et al.: The all-seeing project v2: Towards general relation comprehension
of the open world. arXiv preprint arXiv:2402.19474 (2024) 10, 1, 2
57. Wang,W.,Chen,Z.,Chen,X.,Wu,J.,Zhu,X.,Zeng,G.,Luo,P.,Lu,T.,Zhou,J.,
Qiao, Y., et al.: Visionllm: Large language model is also an open-ended decoder for
vision-centric tasks. Advances in Neural Information Processing Systems 36 (2024)
5, 12
58. Wei, J., Bosma, M., Zhao, V., Guu, K., Yu, A.W., Lester, B., Du, N., Dai, A.M.,
Le, Q.V.: Finetuned language models are zero-shot learners. In: International
ConferenceonLearningRepresentations(2022),https://openreview.net/forum?
id=gEZrGCozdqR 4, 7
59. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,
D., et al.: Chain-of-thought prompting elicits reasoning in large language models.
Advances in Neural Information Processing Systems 35, 24824–24837 (2022) 4, 7
60. Wen, L., Fu, D., Li, X., Cai, X., Ma, T., Cai, P., Dou, M., Shi, B., He, L., Qiao,
Y.: Dilu: A knowledge-driven approach to autonomous driving with large language
models (2023) 5
61. Wilson, B., Qi, W., Agarwal, T., Lambert, J., Singh, J., Khandelwal, S., Pan, B.,
Kumar,R.,Hartnett,A.,Pontes,J.K.,Ramanan,D.,Carr,P.,Hays,J.:Argoverse2:
Nextgenerationdatasetsforself-drivingperceptionandforecasting.In:Proceedings
of the Neural Information Processing Systems Track on Datasets and Benchmarks
(NeurIPS Datasets and Benchmarks 2021) (2021) 10, 2
62. Xu, Z., Zhang, Y., Xie, E., Zhao, Z., Guo, Y., Wong, K.K., Li, Z., Zhao, H.:
Drivegpt4: Interpretable end-to-end autonomous driving via large language model.
arXiv preprint arXiv:2310.01412 (2023) 5
63. Yang, Z., Gan, Z., Wang, J., Hu, X., Ahmed, F., Liu, Z., Lu, Y., Wang, L.: Unitab:
Unifying text and box outputs for grounded vision-language modeling. In: ECCV
(2022) 12, 14
64. Yin, T., Zhou, X., Kra¨henbu¨hl, P.: Center-based 3d object detection and tracking.
CVPR (2021) 11, 13, 16
65. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F.,
Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity. arXiv
preprint arXiv:2310.07704 (2023) 5, 12, 14
66. Yu,L.,Lin,Z.,Shen,X.,Yang,J.,Lu,X.,Bansal,M.,Berg,T.L.:Mattnet:Modular
attention network for referring expression comprehension. In: Proceedings of the
IEEEconferenceoncomputervisionandpatternrecognition.pp.1307–1315(2018)
12
67. Yu, L., Poirson, P., Yang, S., Berg, A.C., Berg, T.L.: Modeling context in referring
expressions. In: Computer Vision–ECCV 2016: 14th European Conference, Ams-
terdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14 (2016) 10,
14
68. Zhang, H., Li, H., Li, F., Ren, T., Zou, X., Liu, S., Huang, S., Gao, J., Zhang, L.,
Li, C., Yang, J.: Llava-grounding: Grounded visual chat with large multimodal
models (2023) 12Cube-LLM 21
69. Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Chen, K., Luo,
P.: Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv
preprint arXiv:2307.03601 (2023) 5Language-Image Models with 3D Understanding -
Supplementary Materials
Jang Hyun Cho1,2, Boris Ivanovic2, Yulong Cao2, Edward Schmerling2,
Yue Wang2, Xinshuo Weng2, Boyi Li2, Yurong You2,
Philipp Kr¨ahenbu¨hl1,⋆, Yan Wang2,⋆, and Marco Pavone2,⋆
1 UT Austin
2 NVIDIA Research
janghyuncho7@utexas.edu
{bivanovic,yulongc,eschmerling,yuewang,xweng,
boyil,yurongy,yanwan,mpavone}@nvidia.com
1 Experiment Details
In this section, we provide more details of our experiments.
LV3D. Each data in LV3D is an image and annotation pair. Each annotation
consists of a list of objects present in each image. Each object has a list of
question and answer pairs as described in Section 3.2 of the main paper. If the
dataisfrom2Ddataset(e.g.,COCO),thequestionanswerpairsinclude“text→
2D box”, “2D center→2D box”, “2D box→text”, etc. Similarly, if the data
is from 3D dataset (e.g., NuScenes), the question includes “text → 3D box”,
“2D center → 3D box”, “2D center → depth”, “2D box → text”, etc., as
discussed in the Section 3 of the main paper. To supplement text information,
we leverage metadata from each dataset for each object class, such as object
attribute in NuScenes dataset (“pedestrian” → “a walking pedestrian.”).
ForGRIT[43],weusedthesubsetofthefirst500folders,whichisabout 1.AS[56]
3
is a collection of VQA datasets as well as some machine-generated 2D grounding
datafromasubsetofSegmentAnything-1B[28].Theoriginalannotationscontain
a substantial amount of noise with duplicate answers. We simply remove the
question-answer pairs of exactly identical and irrelevant answers. We also convert
all the bounding boxes to follow the same format as Cube-LLM. For data
standardization,wefollowOmni3D[7]andconvertalldatasetstofollowavirtual
camera of focal length f =512.
Cube-LLM pre-training undergoes pretraining stage and finetuning stage.
The pretraining is done on LV3D with the dataset multiples specified in Table 1
of the main paper. In this stage, all object depth z are transformed to align with
the virtual camera (same practice as Omni3D [7]) and converted to log-scale. For
each (x,y,z,w,h,l,r ,r ,r ), we normalize x and y in image coordinate from 0
1 2 3
to 999. For z, we set z =−4 and z =5 (after log-scale) and rescale in 0
min max
and999.Similarly,w =0,w =15,h =0,h =15,l =0,l =15.
min max min max min max
All euler angles are normalized between 0 and 2π. We train all 3 Euler angles in
⋆ Equal advising2 JH Cho et al.
“yaw”, “pitch”, and “roll” order. Such ordering of angles in pretraining ensures
the consistent sequential ordering before and after finetuning. To support flexible
question formats during inference, we prepare a set of question templates and
randomly sample one per object during training (e.g., “Provide 3D bounding
box of the region in the image that this sentence describes: <>”or
“What is the 3D box of the <>?”). For datasets where text does not contain
orientation-specific information, we apply random horizontal flip data augmen-
tation. We shuffle object order randomly, and use all objects even if there are
duplicatequestions,andcutoffthetrainingtokensequencebythecontextlength
of the language model (4096). We pre-train with 336×336 image size with frozen
image-encoder, and 672×672 with full training.
Cube-LLM fine-tuning undergoes a few change. Since finetuning benchmarks
are all for outdoor scenes, we finetune z to be in meter (i.e., no log-scale), and
set z = 0,z = 140. We also ignore “pitch” and “roll” and only train for
min max
“yaw”: (x,y,z,w,h,l,r ). We finetune on Talk2Car, DriveLM-grounding, and
1
NuScenes dataset altogether for 10 epochs. We randomly prompt ground-truth
boxes in the system prompt to allow specialist prompting at inference. We also
randomlysampletoqueryeither2Dboundingbox,3Dboundingbox,or2D-to-3D
multi-turn question answering.
Indoor 3D grounding benchmark. We use the testset of Objectron [2], Ark-
itScenes [6], and SUN-RGBD [49] to evaluate the 3D grounding performance
of Cube-LLM. In particular, we show the impact of data scaling with smaller
subset of out pre-training dataset, LV3D-small. In LV3D-small, we remove the
GRIT subset [43], AS-filtered [56], Waymo [51], Lyft [25], Argoverse2 [61], while
bothLV3DandLV3D-smallhavethesameamountofindoordatasets.Toevaluate
grounding performance, we measure precision at τ where τ ∈ [0.15,0.25,0.5].
When an image contains more than one objects associated to the input text
prompt, we consider the max IOU. To augment object location to the text
prompt, we add ‘‘<object> close to camera’’ if the depth is less than 0.8m.
Weadd‘‘<object> on the left’’or‘‘<object> on the right’’iftheob-
ject center is within the left/right 20 % of the image and the distance from
camera is 1/4/10 me away for small/medium/large objects. We define object
as small/medium/large by the max dimension (w,h,l), with threshold of 0.5,
2, 3m. Similarly, we add ‘‘<object> at the center’’ if the object center is
within the center 20 % and the distance from camera is 1/4/10 m away for
small/medium/large objects.
DriveLM-QA training. We aim to be consistent with the baseline training
recipe[48].WepreprocessDriveLMquestionsandanswerstofollowthebounding
box format of Cube-LLM; 3 decimal places, normalized between 0 and 1. For
both LLaVA and Cube-LLM, we train on DriveLM-QA for 5 epochs. For both
LLaVAandCube-LLM,weuseimageresolutionof336×336andsimplyfedthe
6 images independently to the vision encoder and concatenated before feeding to
the language model. The number of vision tokens are 576×6 for each frame. We
do not use any additional input (e.g., previous frames or point cloud) in order toCube-LLM 3
compare to the baselines although Cube-LLM can enhance 3D perception with
specialists. We hold out scene IDs:
"64a3a2d22172406c848f2a92275808ba", "08be42eb2186411d8e2201225329f1c6",
"4b5bf3f4668d44fea9a676e9c4a8a79e", "0e247ba64b9d4a34a7256b6c173b1b5d",
"dbd9183e1278475ea54761297e004b04", "4098aaf3c7074e7d87285e2fc95369e0",
"9f3c8453d03d4df5946444757376b826", "2fc3753772e241f2ab2cd16a784cc680",
"d0880a386b6d434bb5cd13c134af7a3e", "01c3f5e39956402da3e37845632fadca"
in our split evaluation.
2 Talk2Car Grounding with VCoT.
Figure1visualizesourvisualchain-of-thoughtpromptinginferenceonTalk2Carimages.
For each image and text prompt, we first ask with question:
‘‘Please provide 2D bounding box of the region this sentence describes:
<caption>.’’.
Then, with the model prediction, we construct the second question as:
‘‘Please provide 2D bounding box of the region this sentence describes:
<caption>.’’
<2D bounding box>
‘‘Please provide 3D bounding box of the region this sentence describes:
<caption>.’’ This simulates multi-turn conversation and the model can attend to the
tokens of the previous conversation to infer the final output. We witness that as text
prompt becomes more complicated, the guidance of 2D bounding box helps more.
3 DriveLM-QA Visualization
Figure 4, 5, and 6 show various types of DriveLM questions. A large portion of the
questionsaskaboutaparticularobjectspecifiedin<object ID, camera name, x, y>
format. Cube-LLM is capable of reasoning about the surrounding environment from
the input multi-view images. When the Cube-LLM and the ground truth do not
align (e.g., Figure 4 top and 6 bottom), it is evident that Cube-LLM understands the
overall layout of surrounding objects relative to the ego vehicle. Figure 7, 8 and 9 are
the QA samples specifically for grounding important objects nearby. Notable points
are that some of objects that Cube-LLM predicts that do not align with the ground
truth (colored in red) are still important in each driving scenario. For example, in
Figure 7 Cube-LLM predicts a traffic sign (warning sign for cross road), in Figure 8
Cube-LLMpredictsawhitesedaninfrontrightcamerathattheegomayneedtopay
attention to, and in Figure 9 Cube-LLM predicts a white sedan in back camera.
4 Failure Cases
In Figure 10 and 11, we show some failure cases of Cube-LLM grounding result
on DriveLM testset. Cube-LLM makes incorrect prediction mainly in two reasons:
inaccurate depth and semantic mismatch. Figure 10 shows three examples of inaccurate
depth errors and Figure 11 shows three examples of semantic mismatch. Notably, for
the inaccurate depth cases, the projected 3D boxes show accurate 2D localization in
image coordinate. This is because Cube-LLM trains to connect its 2D understanding4 JH Cho et al.
to 3D, as described in Section 3.3 of the main paper. For the semantic mismatch cases,
Cube-LLM struggles in correctly recognizing attributes when two similar objects
are next to each other (e.g., silver sedan vs. white sedan, gray SUV vs. white SUV).
Similarly, Figure 13 and Figure 12 show the failure cases of Cube-LLM on Talk2Car
testset. Again, Cube-LLM still able to predict accurate size and projected 2D box
region. Figure 12 show that Cube-LLM struggles recognizing the correct color of the
car under the shade, the physical status of the black car (moving vs parked), and does
not understand “closest to the curb.”
5 Limitations
Cube-LLMhasseverallimitations.First,Cube-LLMdoesnotemployanyresampling
methods [15,3] to reduce the number of vision tokens. This will limit the model to
increasetheinputresolutiontoevenlargerthanthecurrent672×672(e.g.,1344×1344).
Cube-LLM currently only supports a single frame input. However, video input is
critical in order to correctly recognize the dynamics of the environment. As a result,
Cube-LLMtendstofailcorrectlypredictingwhetheranobjectisstationaryormoving,
or rely on the location of object in the scene and infer the object’s dynamics (e.g., a
car inside a parking space is most likely stationary). We leave these limitations for the
future work.Cube-LLM 5
"Stop in front of the man in the yellow hat."
"Pull up behind the guy wearing a white shirt. He is my uncle."
“Turn left here, park behind the white van, left of the black car.”
“Stop close to this bike so I can see if it is my friend’s or not.”
“Stop near Joe in the truck.”
“Pull alongside the first barrier on the right.”
Fig.1: Cube-LLM visual chain-of-thought prompting inference. First
column is input image, the second column is the 2D bounding box prediction,
and the third column is the final 3D bounding box prediction prompted with the
2D prediction and text.6 JH Cho et al.
“The first table.” “The middle table.” “The furthest table from camera.”
“The chair closest to the camera.” “The chair furthest from the camera.”
A white SUV. “The traffic light closest to the white SUV.”
“The moving truck in front of me.” “The grey sedan next to the moving truck.”
“Where do I work mostly?” “Where should I pour my water?” “It’s too loud! How can I lower the
volume?”
“Where can I find my old files?” “Where can I store my objects in?”
Fig.2: More visualization of 3D grounding. Cube-LLM is capable of
grounding object with spatial cues and understand complex questions.Cube-LLM 7
“Kitchen sink.” “Kitchen paper towel.” “Window curtain.”
“Beam projector.” “Box under the projector.”
“Chair.” “Calendar on the wall.”
“Ladder to the second floor of the double bed.” “Drying rack.”
Fig.3: More visualization of 3D grounding. Cube-LLM is capable of
grounding open-vocabulary category names.8 JH Cho et al.
front left front front right
back right back back left
Question: What actions taken by the ego vehicle can lead to a collision with
<c1,CAM_FRONT_RIGHT,190.8,660.8>?
Cube-LLM: Slight right turn.
Ground Truth: Moderate right turn.
front left front front right
back right back back left
Question: What is the moving status of object <c1,CAM_BACK_LEFT,740.8,428.3>? Please
select the correct answer from the following options: A. Back up. B. Turn left. C. Stopped. D.
Turn right.
Cube-LLM: B. Turn left.
Ground Truth: B
Fig.4: Cube-LLM prediction on DriveLM-QA. Green marks are the refer-
ence marks and the corresponding bounding box in the question. Orange marks
are predicted 2D points by Cube-LLM. Blue marks are the reference marks
and the corresponding bounding box in the ground truth answers.Cube-LLM 9
front left front front right
back right back back left
Question: Predict the behavior of the ego vehicle. Please select the correct answer from the
following options: A. The ego vehicle is going straight. The ego vehicle is driving very fast. B.
The ego vehicle is steering to the left. The ego vehicle is driving very fast. C. The ego vehicle
is slightly steering to the right. The ego vehicle is driving fast. D. The ego vehicle is going
straight. The ego vehicle is driving with normal speed.
Cube-LLM: D. The ego vehicle is going straight. The ego vehicle is driving with normal speed.
Ground Truth: D
front left front front right
back right back back left
Question: What actions taken by the ego vehicle can lead to a collision with
<c2,CAM_FRONT_LEFT,327.2,684.3>?
Cube-LLM: Slight left turn.
Ground Truth: Slight left turn.
Fig.5: Cube-LLM prediction on DriveLM-QA. Green marks are the refer-
ence marks and the corresponding bounding box in the question. Orange marks
are predicted 2D points by Cube-LLM. Blue marks are the reference marks
and the corresponding bounding box in the ground truth answers.10 JH Cho et al.
front left front front right
back right back back left
Question: What is the moving status of object <c2,CAM_FRONT_RIGHT,1068.3,591.7>?
Please select the correct answer from the following options: A. Turn right. B. Stopped. C.
Reverse parking. D. Going ahead.
Cube-LLM: D. Going ahead.
Ground Truth: D
front left front front right
back right back back left
Question: What actions taken by the ego vehicle can lead to a collision with
<c2,CAM_BACK,1285.8,596.7>?
Cube-LLM: Reversing to the left rear can lead to a collision with
<c2,CAM_BACK,1286.4,596.7>.
Ground Truth: Changing to the left lane.
Fig.6: Cube-LLM prediction on DriveLM-QA. Green marks are the refer-
ence marks and the corresponding bounding box in the question. Orange marks
are predicted 2D points by Cube-LLM. Blue marks are the reference marks
and the corresponding bounding box in the ground truth answers.Cube-LLM 11
front left front front right
back right back back left
Question: What object should the ego vehicle notice first when the ego vehicle is getting to
the next possible location? What is the state of the object that is first noticed by the ego
vehicle and what action should the ego vehicle take? What object should the ego vehicle
notice second when the ego vehicle is getting to the next possible location? What is the state
of the object perceived by the ego vehicle as second and what action should the ego vehicle
take? What object should the ego vehicle notice third? What is the state of the object
perceived by the ego vehicle as third and what action should the ego vehicle take?
Cube-LLM: Firstly, notice that <c3,CAM_FRONT,1257.6,656.1>. The object is a traffic sign, so
the ego vehicle should slow down and proceed. Secondly, notice that
<c1,CAM_FRONT,227.2,506.7>. The object is going ahead, so the ego vehicle should
continue going ahead at the same speed. Thirdly, notice that <c2,CAM_BACK,1115.2,557.1>.
The object is going ahead, so the ego vehicle should continue going ahead at the same
speed.
Ground Truth: Firstlynotice that <c1,CAM_FRONT,232.5,528.3>. The object is going ahead,
so the ego vehicle should keep going ahead at the same speed. Secondly notice that
<c2,CAM_BACK_LEFT,1228.3,611.7>. The object is stationary, so the ego vehicle should
keep going ahead at the same speed. Thirdly notice that <c3,CAM_BACK,1126.7,549.2>. The
object is going ahead, so the ego vehicle should keep going ahead at the same speed.
Fig.7: Cube-LLM prediction on DriveLM-QA. Green marks are the refer-
ence marks and the corresponding bounding box in the question. Orange marks
are predicted 2D points by Cube-LLM. Blue marks are the reference marks
and the corresponding bounding box in the ground truth answers. Red circle is
the predicted object that do not agree with the ground truth.12 JH Cho et al.
front left front front right
back right back back left
Question: What are the important objects in the current scene? Those objects will be
considered for the future reasoning and driving decision.
Cube-LLM: There is a white sedan to the back of the ego vehicle, a man wearing a white shirt
to the front left of the ego vehicle, and a white sedan to the front right of the ego vehicle. The
IDs of these objects are <c1,CAM_BACK,742.4,521.1>,
<c2,CAM_FRONT_LEFT,673.6,609.3>, and <c3,CAM_FRONT_RIGHT,294.4,410.4>.
Ground Truth: There is a white car to the back of the ego vehicle, a white clothes pedestrian
to the front left of the ego vehicle, a white car to the back of the ego vehicle, and a black car to
the back of the ego vehicle. The IDs of these objects are <c1,CAM_BACK,731.7,525.8>,
<c2,CAM_FRONT_LEFT,654.9,585.7>, <c3,CAM_BACK,120.0,539.2>, and
<c4,CAM_BACK,655.0,529.2>.
Fig.8: Cube-LLM prediction on DriveLM-QA. Green marks are the refer-
ence marks and the corresponding bounding box in the question. Orange marks
are predicted 2D points by Cube-LLM. Blue marks are the reference marks
and the corresponding bounding box in the ground truth answers. Red circle is
the predicted object that do not agree with the ground truth.Cube-LLM 13
front left front front right
back right back back left
Question: What are the important objects in the current scene? Those objects will be
considered for the future reasoning and driving decision.
Cube-LLM: There is a white sedan car to the back of the ego vehicle, a black sedan to the
front of the ego vehicle, and a red car to the back of the ego vehicle. The IDs of these objects
are <c1,CAM_BACK,828.8,550.8>, <c2,CAM_FRONT,1091.2,458.1>, and
<c3,CAM_BACK,680.0,526.5>.
Ground Truth:There is a red car to the back of the ego vehicle, a black sedan to the front of
the ego vehicle, and a black SUV to the front of the ego vehicle. The IDs of these objects are
<c1,CAM_BACK,825.0,565.0>, <c2,CAM_FRONT,220.8,465.8>, and
<c3,CAM_FRONT,1098.3,455.8>.
Fig.9: Cube-LLM prediction on DriveLM-QA. Green marks are the refer-
ence marks and the corresponding bounding box in the question. Orange marks
are predicted 2D points by Cube-LLM. Blue marks are the reference marks
and the corresponding bounding box in the ground truth answers. Red circle is
the predicted object that do not agree with the ground truth.14 JH Cho et al.
“Elderly person in a floral shirt, moving.”
“White pickup truck, stationary.”
“Blue and white truck, stationary.”
Fig.10: Failure cases of DriveLM-Grounding images. The error mainly
attributes to incorrect depth. Each row has the original image (left), projected
3D box prediction and ground truth (middle), and BEV image (right). Blue box
is the ground truth and Orange box is the prediction. In BEV images, Green box
is the ground truth and red box is the prediction.Cube-LLM 15
“White car, stationary.”
“Silver sedan, stationary.”
“WhiteSUV,moving.”
Fig.11: Failure cases of DriveLM-Grounding images. The error mainly
attributestosemanticmismatch.Eachrowhastheoriginalimage(left),projected
3D box prediction and ground truth (middle), and BEV image (right). Blue box
is the ground truth and Orange box is the prediction. In BEV images, Green box
is the ground truth and red box is the prediction.16 JH Cho et al.
“Once the light turns green, turn left behind the silver car.”
“There is a red truck parked in a parking lot on the left handside. Getoverthere.”
“Stop next to my colleague who is standing on the right side of the road.”
Fig.12: Failure cases of Talk2Car images. The error mainly attributes
to incorrect depth. Each row has the original image (left), projected 3D box
prediction and ground truth (middle), and BEV image (right). Blue box is the
ground truth and Orange box is the prediction. In BEV images, Green box is
the ground truth and red box is the prediction.Cube-LLM 17
“Once the light turns green, turn left behind the silver car.”
“Switch to right lane and park on right behind parked black car.”
“My friend is the guy standing closest to the curb, next to that car in front of us. Pull over so he can get in.”
Fig.13: Failure cases of Talk2Car images. The error mainly attributes to
semantic mismatch. Each row has the original image (left), projected 3D box
prediction and ground truth (middle), and BEV image (right). Blue box is the
ground truth and Orange box is the prediction. In BEV images, Green box is
the ground truth and red box is the prediction.