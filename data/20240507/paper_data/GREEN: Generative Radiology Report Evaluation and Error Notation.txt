GREEN: Generative Radiology Report Evaluation and Error Notation
SophieOstmeier JustinXu ZhihongChen
sostm@stanford.edu xujustin@stanford.edu zhihongc@stanford.edu
MayaVarma LouisBlankemeier
mvarma2@stanford.edu lblankem@stanford.edu
ChristianBluethgen ArneEdwardMichalson MichaelMoseley
bluethgen@stanford.edu arne64@stanford.edu moseley@stanford.edu
CurtisLanglotz AkshaySChaudhari* Jean-BenoitDelbrouck*
langlotz@stanford.edu akshaysc@stanford.edu jbdel@stanford.edu
Abstract 1 Introduction
Machine learning has enabled great progress in the
automatic interpretation of images, where vision lan-
guagemodels(VLMs)translatefeaturesofimagesinto
text (Radford et al., 2021; Liu et al., 2024). In the
medicaldomain, patientimagesareinterpretedbyra-
Evaluatingradiologyreportsisachallenging
diologists,whichisreferredtoasradiologyreportgen-
problemasfactualcorrectnessisextremelyim-
eration(RRG).Automatedandhigh-qualityRRGhas
portant due to the need for accurate medical
the potential to greatly reduce the repetitive work of
communicationaboutmedicalimages. Exist-
radiologists, alleviate burdens arising from shortage
ing automatic evaluation metrics either suf-
of radiologists, generally improve clinical communi-
fer from failing to consider factual correct-
cation(KahnJretal.,2009),andincreasetheaccuracy
ness (e.g., BLEU and ROUGE) or are lim-
ofradiologyreports(RajpurkarandLungren,2023).
itedintheirinterpretability(e.g.,F1CheXpert
CommonlyusedevaluationmetricsinRRGlitera-
andF1RadGraph). Inthispaper,weintroduce
ture(Lin,2004;Zhangetal.,2019;Smitetal.,2020;
GREEN(GenerativeRadiologyReportEvalu-
Delbrouck et al., 2022) seek to evaluate a generated
ation and Error Notation), a radiology report
radiology report against a reference report written by
generationmetricthatleveragesthenaturallan-
a radiologist by leveraging simple n-grams overlap,
guage understanding of language models to
general language similarity, pathology identification
identify and explain clinically significant er-
withinspecificimagingmodalitiesanddiseaseclasses,
rors in candidate reports, both quantitatively
andcommercially-availablelargelanguagemodels. To
and qualitatively. Compared to current met-
achieveperformanceonparwithradiologists,evalua-
rics, GREEN offers: 1) a score aligned with
tionmetricsmustbeadeptwiththeradiologylanguage
expertpreferences,2)humaninterpretableex-
inordertoaccuratelyassessfactualcorrectnessandlev-
planations of clinically significant errors, en-
elsofuncertainties. Additionally,RRGmetricsshould
ablingfeedback loopswithend-users, and3)
beinterpretableinascalablefashiontoenableafeed-
alightweightopen-sourcemethodthatreaches
backloopbetweenthegeneratedreportsandtheexperts
the performance of commercial counterparts.
whoreviewthem. Moreover,thesemetricsshouldbe
WevalidateourGREENmetricbycomparing
open-sourcetoallowforassessmentofprivatedatasets
ittoGPT-4,aswellastoerrorcountsof6ex-
thatrequirethesafeguardingofpatientinformation.
pertsandpreferencesof2experts. Ourmethod
CurrentRRGmetricsfallshortofcapturingthenu-
demonstratesnotonlyhighercorrelationwith
ancedandmultifacetednatureofradiologyreports. To
experterrorcounts,butsimultaneouslyhigher
mitigatethecurrentgapsinappropriatemetricsforRRG,
alignmentwithexpertpreferenceswhencom-
paredtopreviousapproaches." *co-seniorauthorship
1
4202
yaM
6
]LC.sc[
1v59530.5042:viXraReference : “pleural effusion present”
BLEU Evaluation ROUGE-L Evaluation
Candidate1: Candidate2: Candidate1: Candidate2:
pleural effusion ispresent. pleural effusion notpresent. pleural effusion ispresent. pleural effusion notpresent.
= =
0.75 0.75 0.57 0.57
BERTScoreEvaluation F1RadGraph Evaluation
Candidate1: Candidate2: Candidate1: Candidate2:
pleural effusion ispresent. pleural effusion notpresent. pleural effusion ispresent pleural effusion notpresent.
0.85 0.75 1.0 0.5
OurProposedGREEN Evaluation
Candidate1: Candidate2:
pleural effusion ispresent. pleural effusion notpresent.
1.0 0.0
GenerativeExplanation:Pleuraleffusionismarkedas GenerativeExplanation:Pleuraleffusionismarkedas
positiveinbothreferenceandcandidatereports. positiveinreferencebutnegativeincandidate.
ErrorNotation:Clinically significant errors: 0. Matched ErrorNotation:Clinically significant errors: 1. pleural
Findings: 1.pleural effusion is present. effusion should be present.Matched Findings: 0.
Figure1: MotivationofGREEN.
weintroduceGREEN(GenerativeRadiologyEvalua- for use in confidential datasets without patient
tionandErrorNotation). TheGREENmetricintro- privacyconcerns.
ducesfivemajorcontributions:
• Multimodality: GREEN is designed to under-
• Score: Weintroduceandvalidateascore,which standawidearrayofpathologies,linguisticstyles,
ranges from 0 for the weakest assessment, to 1, andterminologies. WedemonstratethatGREEN
marking the highest score achievable. We show exhibitsageneralizedunderstandingofmedical
thatGREENisadeptwithradiologylanguageand language that spans various imaging modalities
canaccuratelyassessfactualcorrectnessandlevels and anatomical structures on out-of-distribution
ofuncertaintiesthatsurpasspriorapproaches. (OOD)data, specificallybyexaminingitsappli-
cationtoabdominalcomputedtomography(CT)
• Interpretable Evaluation Summary: We pro- reportsinazero-shotfashion.
videamethodtogenerateaclear,human-readable
evaluation summary independent of the test set • Datasets: Lastly, we share the dataset used to
size. By providing detailed error categorization develop our models. This dataset encompasses
withexplanations,GREENenablesmachinelearn- 100,000annotationsfromGPT-4relatedtochest
ingpractitionersandexpertstopinpointareasfor X-rays(spanningvariousdatasets)and50,000an-
improvementintheirtrainedsystems. notationsacrossadiversesetofimagingmodali-
ties.Bymakingtheseresourcesavailable,wehope
• Practicability: We open-source the GREEN tofacilitatefurtherresearchandimprovementin
modelthatleveragesa<7Bparameterlanguage theaccuracyandreliabilityofautomatedradiology
modelwithsimilar reportevaluationabilitiesas reportgenerationsystems.
largercounterparts. ThisapproachdecreasesGPU
requirementsandenhancesprocessingspeed. 2 RelatedWork
• Applicability: Leveraging high-performing Theliteraturedemonstratesvariousadvancesingenerat-
commercially-available large language model ing radiology reports from medical images (Ramesh
(LLM) services typically requires a de- et al., 2022; Jeong et al., 2024; Li et al., 2023; Yang
identification procedure and institutional review et al., 2022; Nguyen et al., 2021; Chen et al., 2024;
board approval for protected health information Chavesetal.,2024). Forinstance,asetofevaluation
(PHI).GREENisfree,open-source,anddesigned metricsarecommonlyutilizedtoassessthequalityof
2the generated reports and focus on lexical similarity Step1:DatasetGenerationw/GPT-4
(e.g.,ROUGE-L(Lin,2004)andBLEU(Papinenietal.,
Prompt+ +
2002))andfactualcorrectness(e.g.,F1CheXbert(Smit GreenAnalysis
etal.,2020)andF1RadGraph(Delbroucketal.,2022)). Candidate Reference GPT-4
F1CheXbertassessestheaccuracyofidentifieddisease Supervise
Step2:Training(DistillingtheknowledgetoasmallLLM)
labelsinreportsagainstanarrowreference, covering
only 14 CheXbert classes of common-but-specific Prompt+ +
GreenAnalysis
chest x-ray findings. F1RadGraph enhances factual
Candidate Reference GREEN
correctness evaluations by comparing the agreement
on anatomical and observational entities between
Figure2: TrainingprocedureoftheGREENmodel.
candidatereportsandreferencereports,usingagraph
model trained on human annotations. However, the
correlation of F1RadGraph with manual evaluations inAppendixA.1totaskGPT-4toidentifyandcatego-
by radiologists is low, leading to the development of rizedifferencesinnaturallanguageacrosssixunique
more closely-aligned metrics such as RadCliQ (Yu clinically-definedcategoriesdetailedpreviouslyYuetal.
et al., 2023a). RadCliQ consists of an ensemble of (2023b).
ROUGE,BLEU,CheXbertembeddingsimilarities,and The pairing process used five different heuristics,
RadGraph to form a composite metric which aims to generating 20,000 unique pairs for each heuristic: i)
matchexpert-generatederrorcounts. WhileRadCliQ randomlymatchingcandidatesandreferences,ii)modi-
iseffectiveinmirroringtheseerrorcounts, ithaslow fyingthecandidatebyremovingandshufflingsentences
interpretability as the individual metric weights are fromtheoriginalreport,iii)usingatrainedRRGmodel
unknownandthesinglenumericalscoreisinadequate tocreatethecandidatebasedonthereferencedimage(to
forclinicalintegration(Figure1). bridgethegapnotedinliteraturebetweenactualreports
and those generated by baseline models), iv) pairing
Our approach stands out from previous metrics candidateswiththeclosestsemanticallysimilarreport
byemphasizingclinicalrelevanceandinterpretability, assessedusingBERTScore(Zhangetal.,2019),andv)
showinghigheralignmentwithexperterrorcountsand creating candidates through RadGraph (named-entity
expertpreferenceswhilestillleveraginganopen-source recognitiondataset)(Jainetal.,2021)permutationsof
<7Bparametermodel. thereferencereports,incorporatingchangestothepres-
ence of findings or by making modifications to size,
3 GREEN severity,orlocationthroughoutthereports. Thenumber
ofRadGraphpermutationsandtheBERTScoredistribu-
GREEN(GenerativeRadiologyEvaluationandError tionofthepairsarepresentedinFigure3. Additionally,
Notation)involvesthreeprimarycomponents. a sample GPT-4 response of a pair with a candidate
thatincludesexactlyoneRadGraphpermutationcanbe
First, wedescribetheconstructionofourgenera- foundinAppendixA.2.
tivelanguagemodel,whichisdesignedtoidentifyand
classifyerrorsinradiologyreportsintosixcategories
(Section3.1). Thissectionissubdividedintothecollec-
tionoftrainingdata(Section3.1.1)anddetailsofthe
trainingprocess(Section3.1.2). Second,weelaborate
ontheGREENscoreinSection3.2,includingitsratio-
naleandsignificance. Third,weexplaintheGREEN
summaryanditsusefulness.
We then outline the steps we took to validate the Figure 3: Number of RadGraph permutations among
effectivenessandrelevanceofGREEN,bothquantita- thecandidatesfor20,000pairs(left)andBERTScore
tivelyandqualitatively,inSection3.4. distributionacross20,000pairs(right).
3.1 GenerativeLargeLanguageModel To maintain uniqueness across the dataset, once
pairs were formed using one heuristic, they were ex-
3.1.1 ChestX-raysDataCollection
cludedfromconsiderationinothers,ensuringeachof
To compile our training dataset, we selected 100,000
the100,000pairsisdistinct. Overall,174,329unique
reference and generated candidate report pairs
reportswereutilizedeitherasreferencesorcandidates
from six publicly-available de-identified chest X-
inthisstudy.
ray datasets: MIMIC-CXR (Johnson et al., 2019),
MIMIC-PRO(Rameshetal.,2022),CandidPTX(Feng 3.1.2 ModelArchitectureandTraining
etal.,2021),PadChest(Bustosetal.,2020),BIMCV- To enhance performance with medical data, we pre-
covid19 (Vayá et al., 2020) and OpenI (Demner- trainedLLaMA-2andPhi-2usingacomprehensiveset
Fushmanetal.,2012). Weemployedthepromptshown ofdomain-specificdatasetstoformRadLLaMA-2and
3GREENSummary
[Summary]:
Green score: mean 0.23 std 0.04
[Clinically Significant Errors]:
(a) False report of a finding in the candidate: 0.9
[Small right pleural effusion]
(b) Missing a finding present in the reference: 0.7
[Underlying chronic upper lobe scarring.]
(c) Misidentification of a finding's anatomic location/position: 0.4
[The opacity is in the right lower lobe, not the right upper lobe.]
(d) Misassessment of the severity of a finding: 0.8
[Bilateral pleural effusion]
(e) Mentioning a comparison that isn't in the reference: 0.7
[The candidate report mentions a discussion between doctors,
which is not present in the reference report]
(f) Omitting a comparison detailing a change from a prior study: 0.5
[The candidate report does not mention the absence of disease progression]
Figure 4: Sample GREEN summary. For each error subcategory, we provide the most representative error
explanations,enablinguserstopinpointareasforimprovementfortheirtrainedsystems.
RadPhi-2.ThesedatasetsincludeMIMIC-IVRadiology theGREENmetric.
Reports(Johnsonetal.,2023),MIMIC-IVDischarge
3.2 GREENScore
Summaries, MIMIC-CXR Radiology Reports, and a
varietyofsourcesfromPubMed(AbstractsandPatient Weemployedregularexpressions(regex)toparsethe
Reports). Specialized datasets such as Wiki Medical countsoferrorsfromthemodel’soutput. Specifically,
Terms1andMedicalGuidelines2(Vashishthetal.,2021) wedenotedthecountofeachtypeoferroras#error s,i,
werealsoused. wheretheerror’sclinicalsignificances∈{sig.,insig.}
To obtain a local RRG evaluator for the GREEN andsubcategoryi∈{(a),(b),...,(f)}.
metric model, we opted to train open-source models To calculate the GREEN score, we prioritized
insteadofrelyingonAPI-basedmodels. Specifically, #error sig.,i (errors with the potential to alter clinical
wefurtherfine-tunedRadLLaMA-2andRadPhi-2,as decision-makingprocesses)alongsidethecountsofac-
well as other models of different sizes, architectures, curatematchedfindings,#matchedfindings,forinver-
andpre-trainingdatasets,suchasLLaMA-2(Touvron sion. The formula for the GREEN score is then ex-
etal.,2023),Phi-2(JavaheripiandBubeck,2023),and pressedas:
Mistral-v0.1(Jiangetal.,2023)(Figure2).Modelswere #matchedfindings
trainedon8xNVIDIAA100TensorCoreGPUswith GREEN=
(f)
40GBVRAMusingtheHuggingfaceframeworkwith #matchedfindings+ (cid:80) #error
sig.,i
FlashAttention2,DeepSpeedStage3,andtheAdamW i=(a)
optimizer. An effective batch size of 2,048 was used (1)
for12epochs,aswellasabaselearningrateof1e-4,a
if # matched findings > 0, otherwise 0. Thus, the
warm-upratioof0.05,andaweightdecayof0.1. Train-
GREENscore(↑)isboundedbetween0and1.
ingfor7B-parametermodelsaveraged40GPUhours,
while the 2.7B-parameters models averaged 28 GPU 3.3 GREENSummary
hours.Forfastandreliableinference,weemployeddata
We present a method for a detailed analysis of error
parallelismanddeterministicsamplingwithamaximum
explanationpererrorsubcategoryi. Initially,thepro-
tokenlengthof2,048toensurethereproducibilityof
cessinvolvesclusteringtheembeddingsoftheexplana-
tionsentence,e ∈ E ,intok clustersusingSentence
1www.huggingface.co/datasets/gamino/wiki_medical_ i i
terms Transformers (Ni et al., 2021). Subsequent steps in-
2www.huggingface.co/datasets/epfl-llm/guidelines cludequantifyingthesizeofthesekclusters,wherek
4isdeterminedbythesilhouettedistance(Shahapureand
Nicholas, 2020). We then identify the largest cluster
andisolatethetop-3membersexhibitingtheclosestdis-
tancetothecluster’smeanasdeterminedthroughcosine
distancemeasures. Thetop-3membersarethennamed
foreacherrorcategoryintheGREENsummary(Ap-
pendixA.5). Forexample,inFigure4,subcategory(a)
has"(Smallrightpleuraleffusion,Smallrightpleuralef-
fusion,Smallrightpleuraleffusion)"asthethreeclosest
memberstothelargestcluster,whichindicatessignif-
icanthallucinationsofa"smallrightpleuraleffusion".
Thisopensupthepossibilityfortargeteddetectionof
databiasesorqualityissues, aswellasspecificareas
formodelimprovement.
Figure5: Mean-expertandinter-expertcorrelationma-
3.4 Validation
trix(Kendall’sTau)forfine-grainederrorcountsonthe
3.4.1 ExpertErrorCountsDataset
externalvalidationset(RexVal(Yuetal.,2023b)).
TovalidatetheGREENscoreinaclinicalsetting,we
utilizedthepublicly-availableReXValdataset, which
includesassessmentsfromsixboard-certifiedradiolo- challenging.Thequalityofthetwocandidatesisconsid-
gistson200pairsofgeneratedradiologyreportsfrom eredtobeequalwhenexpertshaddifferingpreferences,
50casesoftheMIMIC-CXRtestset(Yuetal.,2023b). henceimplyingnopreference. Wethenexcludedsuch
Each radiologist counted the occurrences of six spe- casesfromconsideration.
cificerrortypesdenotedearlier,distinguishingbetween
errorsofclinicalsignificanceandthoseconsideredin- Table 1: Difference between ReXVal experts and the
significant. GREENmodelmeasuredusingmeanabsoluteerrorof
significanterrorcounts.
3.4.2 ExpertPreferenceDataset
Sincewelackedaground-truthscoreforevaluatingra- 0 1 2 3 4 5 GREEN
diologyreports,weturnedtoaradiologistpreference 0 − 0.505 0.835 0.675 0.495 1.130 1.160
1 0.505 − 1.100 0.870 0.660 1.365 1.485
dataset. This helped to address the shortcomings of
2 0.835 1.100 − 0.730 0.770 0.725 0.715
comparingerrorscountsintheprevioussection. Inpar- 3 0.675 0.870 0.730 − 0.570 0.965 0.895
ticular,whenradiologistscomparetworeports,theydo 4 0.495 0.660 0.770 0.570 − 1.025 1.005
5 1.130 1.365 0.725 0.965 1.025 − 0.930
sowithanintuitiveweightingofmatchedfindingsand GREEN 1.160 1.485 0.715 0.895 1.005 0.930 −
significantandinsignificanterrors. Assuch,theprefer-
encedatasetenabledustodeterminewhichmetricmost
effectivelyreplicatesexpertevaluationsandallowedus
toassessGREENasapreferencegenerator (Rafailov 4 Experiments
etal.,2024;Ethayarajhetal.,2024).
Wecollected100pairwisepreferencesbytwoboard- 4.1 Inter-ExpertAnalysis
certified radiologists (with over 5 and 25 years of ex-
The baseline for model performance was established
perience). The dataset comprised of 50 cases of the
bycomparingthecorrelationbetweenexpertstoeach
ReXValdataset(Yuetal.,2023b),supplementedbyan
otherfromtheReXValdatasetusingKendall’sTauco-
additional50casesrandomlyselectedfromtheMIMIC-
efficient(Yuetal.,2023b). Thecorrelationbetweenthe
CXRtestset. Thetworadiologistswerepresentedwith
6expertswaslessthantheaveragecorrelationacross
achest X-rayalongsidetwo correspondingcandidate
experts,whichspansfrom0.41to0.60(Figure5). Ad-
reportsgeneratedbyanimage-captioningmodelfine-
ditionally,weassessedthediscrepancybetweenexperts
tunedontheMIMICCXRtrainingset3. Theprimary
by computing the mean absolute error of significant
taskfortheradiologistwastoselectthecandidatereport
errorcounts, resultinginameandifferenceof0.83±
theypreferredandtoquantifytheirconfidenceinthis
0.13(Table1). Theseinter-expertmeasuresserveasup-
selection on a scale ranging from 1 to 10. The inten-
perboundfortheGREENmodelperformanceoutlined
tionbehindthiswastocategorizethecomplexityofthe
below.
task. Inessence,whenradiologistshaveahighdegree
ofconfidenceintheirchosenreportforagivencase,itis 4.2 PerformanceonTrainingDataDistribution
anticipatedthattheautomatedpreferencegeneratorwill
WemeasuredperformanceoftheGREENmetricmodels
showthehighestconcordance,asthetaskisdeemedless
bysamplingdeterministicallyandcomparingthemean
3https://huggingface.co/nlpconnect/ absoluteerrorsandclassicallexicalmetricsagainstref-
vit-gpt2-image-captioning erence labels from GPT-4. We found that RadPhi-2
5Table2: Resultsontheinternaltestset(10,000examples)whencomparedtoGPT-4errorcounts. 1Meanabsolute
error±standarddeviation,2Averageerrorcount.
MAE±STD1 ∆GREEN↓
LanguageModel ∆Sig. ErrorCount↓ ∆Insig. ErrorCount↓ ∆MatchedFindings↓
3.1±2.62 0.15±0.522 2.07±1.842
Mistral-v0.1(7B) 0.97±1.18 0.22±0.58 0.44±0.70 0.11±0.17
LLaMA-2(7B) 1.35±1.40 0.15±0.52 1.62±1.67 0.29±0.26
Phi-2(2.7B) 0.84±1.14 0.20±0.58 0.34±0.59 0.09±0.14
RadLLaMA-2(7B) 0.70±0.99 0.20±0.57 0.29±0.54 0.08±0.13
RadPhi-2(2.7B) 0.63±0.99 0.18±0.57 0.26±0.53 0.06±0.12
andRadLLaMA-2exhibitthelowestmeanabsolutedif- GREEN-GPT-4andGREENdemonstratedsimilarand
ferenceforclinicallysignificanterrorsof0.63±0.99 strongercorrelationscomparedtoclassicalmetrics. We
(Table2)andthehighestclassicallexicalmetricswith notedthattheGREENcorrelationsignificantlyoutper-
ameanBERTScoreof0.84±0.10(Table3). Wemea- formsthatofRadGraph,despiteRadGraphbeingtrained
sured clustering and summary consistency with lan- onhumanannotations(RadGraph: 0.47(95%CI,-0.55
guagesimilaritymetricslikeBERTScore. Consistent 0.39)vs. GREEN:0.63(95%CI,0.690.56)).
with the quantitative results, we found that RadPhi-2 Comparedtotheinter-expertcorrelation,GREEN
and RadLLaMA-2 yielded the best natural language exhibitsacompetitivedegreeofcorrelationat0.63com-
agreementwithGPT-4(Table2). paredtotherangefrom0.48to0.64onthesameexam-
ples(Figure5).
Table3: ResultsontheInternaltestset(10,000exam- Additionally, weobservedthatcorrelatingtheun-
ples)whencomparedtoGPT-4responses. weightedsummederrorcountsofGREEN(clinically
sig. errors + insig. errors) yields a correlation coeffi-
Lexical cientof0.79(95%CI,0.740.83),whichmayexceed
LanguageModel
BERTScore↑ ROUGE-L↑ BLEU↑ theperformanceofGPT-4-basedG-Rad(Chavesetal.,
Mistral-v0.1(7B) 0.80±0.11 0.68±0.18 0.54±0.22 2024).
LLaMA-2(7B) 0.78±0.12 0.65±0.19 0.53±0.21
We further used expert preferences to determine
Phi-2(2.7B) 0.80±0.11 0.70±0.18 0.54±0.23
whetherthesummederrorcountsortheGREENscore
RadLLaMA-2(7B) 0.83±0.24 0.73±0.17 0.59±0.23
RadPhi-2(2.7B) 0.84±0.10 0.76±0.17 0.64±0.23 bestmimicsexpertevaluation. Thisapproachisbased
ontheassumptionthatclinicallysignificanterrors,in-
significanterrors,andmatchedfindingscarrydifferent
4.3 PerformanceonValidationDataDistribution weightsindeterminingthequalityofacandidatereport.
To analyze the performance upper bound of GREEN,
4.3.2 ExpertPreferences
weinferredGREENscoresfromGPT-4responseson
Theaccuracyofthegeneratedpreferenceswasmeasured
thevalidationset,andreferredtoitasGREEN-GPT-4.
byhowoftentheexpert-preferredreportmatchedthe
4.3.1 ExpertErrorCounts report that had a higher score from GREEN, a lower
summederrorcount,orthepreferenceofGPT-4. The
To quantitatively validate GREEN, we measured the
promptthatwasusedforGPT-4preferencesisshown
meanabsolutedifferenceandaccuracyrelativetothe
inAppendixA.3).
averageradiologist,asdetailedinSection4.1.Wefound
WeobservedthehighestmeanaccuracyforGREEN
that,overall,RadLLaMA-2exhibitsthelowestdiffer-
and GREEN-GPT-4, which both outperforms the
ences to the mean radiologist’s error counts (1.54 ±
summederrorcountapproach. ThepreferencesofGPT-
1.36sig. error difference), whichapproaches theper-
4 exhibited an accuracy of 0.23 (95% CI, 0.13 0.36)
formanceofGPT-4(1.51±1.29sig. errordifference).
(Table6).
Comparedtoallexpertsindividually,RadLLaMA-2ex-
Uponexaminingtheimpactofvaryingconfidence
hibits an average difference of 1.02 ± 0.27, which is
levels(Figure6),weobservedthatGREEN’spreference
withintheboundariesoftheaverageinter-expertdiffer-
alignmentimprovesinconjunctionwithincreasedradi-
enceof0.83±0.13. Drawingfromthesequantitative
ologistconfidence,distinguishingitfromtheapproach
results,alongwiththefindingspresentedinSection3.4,
ofusingjustthesumoferrorcountsorthedirectGPT-4
weselectedRadLLaMA-2astheGREENmodelforall
preferencewithlowaccuracy.
futureexperiments.
TovalidateGREENagainstexistingmetrics,weas-
sessedthecorrelationbetweenthetotalerrorcountby
radiologistsandtheclassicalmetrics,alongsideGPT-4
andGREEN,usinganexternaldataset(Table5). Both
6Table4: Resultsontheexternalvalidationset(200examples)comparedtoReXValhumanexperts. 1MAE:Mean
AbsoluteErrorofthesumofSig. ErrorsandInsig. Errors,2STD:StandardDeviation,3Averageerrorcount. (a)-(f)
Accuracyforeachsignificanterrorcategory: (a)Falsereportofafindinginthecandidate,(b)Missingafinding
presentinthereference,(c)Misidentificationofafinding’sanatomiclocation/position,(d)Misassessmentofthe
severityofafinding,(e)Mentioningacomparisonthatisn’tinthereference,and(f)Omittingacomparisondetailing
achangefromapriorstudy.
MAE1±STD2 Accuracy↑
LanguageModel
∆Sig.Error↓ ∆Insig.Error↓ (a) (b) (c) (d) (e) (f)
7.03±1.163 0.47±0.523
Comparedto Mistral-v0.1(7B) 2.60±1.91 0.87±0.94 0.13 0.31 0.62 0.59 0.48 0.67
AverageExpert LLaMA-2(7B) 2.62±1.25 0.47±0.52 0.10 0.23 0.65 0.59 0.68 0.70
Phi-2(2.7B) 2.10±1.39 0.65±0.70 0.34 0.08 0.65 0.57 0.66 0.53
RadLLaMA-2(7B) 1.54±1.36 0.51±0.54 0.34 0.38 0.60 0.54 0.65 0.68
RadPhi-2(7B) 2.08±1.15 0.55±0.61 0.19 0.18 0.62 0.57 0.62 0.61
GPT-4 1.51±1.29 0.52±0.55 0.32 0.40 0.65 0.59 0.68 0.70
Table5: Correlationanalysisbetweenmetrics,GREEN
score,andGREENscoreinferredfromGPT-4(GREEN-
GPT-4)toaveragetotalerrorcountof6radiologistsin
theReXValdataset(200examples).
Metrics Kendall’sTau↑
BLEU 0.33(95%CI,0.420.23)
BERTScore 0.44(95%CI,0.520.35)
F1RadGraph 0.47(95%CI,0.550.39)
ROUGE-L 0.53(95%CI,0.610.45)
RadCliQ-v1 0.58(95%CI,0.510.64)
GREEN(ours) 0.63(95%CI,0.690.56)
GREENGPT-4(ours) 0.64(95%CI,0.700.57)
ErrorcountsGREEN 0.79(95%CI,0.740.83)
Figure6: Radiologistconfidencevs. accuracyofprefer-
ErrorcountsGPT-4 0.79(95%CI,0.750.83)
encelabeling. Astheconfidenceoftheexpertsintheir
preferencesincreases,theGREENscoredemonstrates
Table6: Accuracieswith95%CIofvariouspreferences thehighestalignmentwithexpertpreferencesascom-
whencomparedtoexpertpreferences. paredtotheapproachofusingjustthesummederror
counts. Thisdifferencewasquantifiedusingaccuracy
Accuracy (greenlines). Ofnote,ifGPT-4isaskeddirectlyabout
apreference,italignspoorlywiththeexpertpreference.
PreferenceGPT-4 0.23(95%CI,0.130.36)
However,whentheGREENscoreformulaisapplied,
ErrorCountGREEN 0.57(95%CI,0.430.70) ahigheraccuracyisshownevenatlowerexpertconfi-
ErrorCountGPT-4 0.60(95%CI,0.470.74) dencelevels. DetailedresultscanbefoundinTable6.
GREEN(ours) 0.62(95%CI,0.490.75)
GREENGPT-4(ours) 0.68(95%CI,0.550.79)
imagingmodalitiesbeyondchestX-rays,wecreateda
dataset analogous to the training dataset used for the
GREENchestX-ray(Section3.1),butwithoutaccess
5 MultimodalityGeneralizability toRRGmodelstogeneratecandidatereportsforevery
modality. Wedidthistovalidateourmethodonarange
Wenowdemonstratehowthismethodcanbeappliedto ofimagingmodalitiesforwhichRRGmodelsmaynot
variousotherimagingmodalities. yetexist.
ThisnewdatasetisalsobasedonMIMIC-IVRadi-
5.1 Out-of-ChestX-rayDataset
ologyReports,whichincludes2,321,355de-identified
RecentworksextendedRRGcapabilitiesofVLMsto radiology reports from 237,427 patients. It covers a
other imaging modalities (Hamamci et al., 2024; Bai varietyofimagingmodalitiessuchasX-ray,computed
et al., 2024). To extend the GREEN model to new tomography, magnetic resonance imaging, and ultra-
7Table 7: Adapting GREEN to any imaging modality: Performances on Out-of-Chest X-ray and OOD Data in
Zero-ShotandTrainedConditions. 1Meanabsoluteerror±standardvariation,2AverageError,3Modalitiesinclude
X-ray,computedtomography,magneticresonanceimaging,andultrasound.
MAE±STD1 ∆GREEN↓
Evaluation Training
∆Sig. Error ∆Insig. Error ∆Matched
data data
Count↓ Count↓ Findings↓
2.24±2.282 0.16±0.452 1.83±2.252
MIMIC-IV-Notes3
CXRdataset 1.05±1.51 0.20±0.51 0.49±1.00 0.10±0.17
CXR+Out-ofCXRdataset 0.61±0.99 0.19±0.48 0.34±1.04 0.07±0.15
9.19±3.812 0.06±0.242 8.56±2.742
AbdominalCT, CXRdataset 5.31±2.82 0.06±0.24 4.09±2.74 0.21±0.17
OOD CXR+Out-ofCXRdataset 3.12±2.03 0.19±0.53 3.56±3.28 0.17±0.23
Table8:AdaptingGREENtoanyimagingmodality:PerformanceonOut-of-ChestX-rayandOODdatadistribution
inZero-ShotandTrainedConditionsbasedonLexicalMetrics. 1ModalitiesincludeX-ray,computedtomography,
magneticresonanceimaging,andultrasound.
Evaluation Training Lexical
data data
BERTScore↑ ROUGE-L↑ BLEU↑
MIMIC-IV-Notes1
CXRdataset 0.74±0.12 0.62±0.18 0.45±0.21
CXR+Out-ofCXRdataset 0.81±0.09 0.73±0.15 0.60±0.18
AbdominalCT, CXRdataset 0.68±0.12 0.58±0.15 0.41±0.13
OOD CXR+Out-ofCXRdataset 0.71±0.06 0.58±0.07 0.45±0.06
sound,asreferencedin(Johnsonetal.,2023). 5.3 Out-of-ChestX-rayExperiments
Wefirstuniformlysampledreportstomaintainadis- Wefirstevaluatedzero-shotperformanceoftheGREEN
tributionofcasessimilartothatdescribedin(Johnson modelontheOut-of-ChestX-raydataset(1.05±1.51
et al., 2023). Secondly, we used 4 methods to mod- sig. errorcountdifference)andontheexternalOOD
ifytheradiologyreportstogenerate50,000candidate (5.31±2.82sig. errorcountdifference). Wefine-tuned
reports:i)re-arrangingtheorderofsentences,ii)remov- thebestcheckpointoftheGREENmodelontheOut-
ingsentences,iii)randomlypairingsentences,andiv) of-Chest X-ray dataset with a batch size of 80 for 8
modifyingthereportbysamplingrandomcombinations epochsandthesamehyperparametersasmentionedin
oferrorcategoriesandaskingGPT-4toincorporateer- Section3.1.2. Weusedthesameevaluationexperiments
rorsintothereportstogenerateacandidatereport(ifno asintheprevioussection. Wefoundthatfurtherfine-
errorcategoriesaresampled,GPT-4isaskedtorephrase tuning on multimodality data improves the sig. error
thereportwiththesamemeaningbychangingasmall countdifferenceandthetextsimilaritymetricsforboth
numberofwords)(AppendixA.4). thein-distributionandout-of-distributiondata(0.61±
0.99and3.12±2.03sig. errorcountdifference).
WethenpromptedGPT-4toevaluatethedifferences
with the same prompt design as with the chest X-ray
data. Wefurthersplitthese50,000reportsintotraining, 6 Conclusion
validation,andtestsetsaccordingtothesame80/10/10
Inthisstudy,weintroducedGREEN(GenerativeRadi-
ratio and combined them with the initial chest X-ray
ologyReportEvaluationandErrorNotation),anovel
dataset.
metricaimedatenhancingtheevaluationofradiology
reports. GREENoutperformsexistingmetricsbyalign-
5.2 OODExternalAbdominalCTdataset ingcloselywiththenuancedrequirementsofmedical
diagnosticsthroughitspreciseassessmentoffactualcor-
Werandomlychose15pairsofreferenceandcandidate rectnessanduncertainties. Thescore’shighcorrelation
reportsfromanabdominalCTdataset. Thedatasetorig- withexpertevaluationsunderscoresitseffectiveness.
inatedfromtheStanfordUniversityMedicalCenter’s The open-source nature of GREEN supports
radiology dataset, which includes examinations from widespreaduseandcollaborativeimprovementswith-
December2012toOctober2018. outcompromisingdataprivacy. Itslightweightdesign
8ensures practicality across diverse settings, reducing modelforchestx-rayinterpretation. arXivpreprint
computationaldemands. Additionally,GREEN’sadapt- arXiv:2401.12208.
abilityacrossdifferentimagingmodalitiesandextensive
Jean-Benoit Delbrouck, Pierre Chambon, Christian
datasetsencouragebroaderapplicabilityandresearchin
Bluethgen, Emily Tsai, Omar Almusa, and Cur-
medicalartificialintelligence.
tisLanglotz.2022. Improvingthefactualcorrect-
TheGREENmetric’sabilitytomaintainrobustper- nessofradiologyreportgenerationwithsemanticre-
formanceonOODdatafurthersignifiesitsversatility wards. InFindingsoftheAssociationforComputa-
andpotentialasastandardforfuturedevelopmentsin tionalLinguistics: EMNLP2022,pages4348–4360,
automatedradiologyreporting. AbuDhabi,UnitedArabEmirates.Associationfor
ComputationalLinguistics.
7 Limitations
DinaDemner-Fushman,SameerAntani,MatthewSimp-
son, and George R Thoma. 2012. Design and de-
Analyzingeachsampletakesroughly3.75secondson
velopmentofamultimodalbiomedicalinformation
one A100 GPU. However, using batching can accel-
retrievalsystem. JournalofComputingScienceand
erate the processing to four samples in 4.22 seconds
Engineering,6(2):168–177.
(equivalenttoabout1.06secondspersample). Dueto
itscomplexity,itisslowercomparedtoROUGE,atap- Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff,
proximately 0.015secondspersample,butfasterthan DanJurafsky,andDouweKiela.2024. Kto: Model
GPT-4,atupto22.0secondspersample). alignmentasprospecttheoreticoptimization. arXiv
preprintarXiv:2402.01306.
WeintroduceOODmetricsandsuggestastrategy
toadjusttheGREENmodeltodifferentimagingtech- SijingFeng,DamianAzzollini,JiSooKim,Cheng-Kai
niques,evenintheabsenceofaninitialRRGmodelfor Jin,SimonPGordon,JasonYeoh,EveKim,Mina
eachtechnique. Nonetheless,fine-tuningGREENfor Han,AndrewLee,AakashPatel,etal.2021. Cura-
newimagingmodalitiesmightberequiredinsubsequent tionofthecandid-ptxdatasetwithfree-textreports.
studiestoensuresatisfactoryperformance. Radiology: ArtificialIntelligence,3(6):e210136.
Although the model operates deterministically to
Ibrahim Ethem Hamamci, Sezgin Er, Furkan Al-
ensurereproducibleoutputs,theerrorquantificationre-
mas, Ayse Gulnihan Simsek, Sevval Nil Esirgun,
mains,tosomeextent,uncontrollable,whichintroduces Irem Dogan, Muhammed Furkan Dasdelen, Bas-
adegreeofrandomnesstothecountingoferrors. This tianWittmann,EnisSimsar,MehmetSimsar,etal.
randomnessmaystemfrominherentuncertaintiesinthe 2024. A foundation model utilizing chest ct vol-
task,asevidencedbythedisagreementamongexperts umes and radiology reports for supervised-level
onfine-grainederrorcounts(Section4.1).Thisisachar- zero-shotdetectionofabnormalities. arXivpreprint
acteristicthathasbeenpreviouslyobservedandnoted arXiv:2403.17834.
ininter-expertagreementanalyses(Irvinetal.,2019).
JeremyIrvin,PranavRajpurkar,MichaelKo,YifanYu,
Silviana Ciurea-Ilcus, Chris Chute, Henrik Mark-
lund,BehzadHaghgoo,RobynBall,KatieShpan-
References skaya, etal.2019. Chexpert: Alargechestradio-
graph dataset with uncertainty labels and expert
FanBai,YuxinDu,TiejunHuang,MaxQ-HMeng,and
comparison. In Proceedings of the AAAI confer-
BoZhao.2024. M3d: Advancing3dmedicalimage
ence on artificial intelligence, volume 33, pages
analysis with multi-modal large language models.
590–597.
arXivpreprintarXiv:2404.00578.
SaahilJain, AshwinAgrawal, AdrielSaporta, Steven
Aurelia Bustos, Antonio Pertusa, Jose-Maria Salinas,
Truong,DuNguyenDuong,TanBui,PierreCham-
and Maria De La Iglesia-Vaya. 2020. Padch-
bon,YuhaoZhang,MatthewLungren,AndrewNg,
est: Alarge chestx-rayimagedatasetwithmulti-
CurtisLanglotz,PranavRajpurkar,andPranavRa-
label annotated reports. Medical image analysis,
jpurkar.2021. Radgraph:Extractingclinicalentities
66:101797. and relations from radiology reports. In Proceed-
ingsoftheNeuralInformationProcessingSystems
Juan Manuel Zambrano Chaves, Shih-Cheng Huang,
TrackonDatasetsandBenchmarks,volume1.
Yanbo Xu, Hanwen Xu, Naoto Usuyama, Sheng
Zhang,FeiWang,YujiaXie,MahmoudKhademi, Mojan Javaheripi and Sébastien Bubeck. 2023. Phi-
ZiyiYang,etal.2024. Trainingsmallmultimodal 2: Thesurprisingpowerofsmalllanguagemodels.
models to bridge biomedical competency gap: A MicrosoftResearchBlog.
case study in radiology imaging. arXiv preprint
arXiv:2403.08002. JaehwanJeong,KatherineTian,AndrewLi,SinaHar-
tung,SubathraAdithan,FardadBehzadi,JuanCalle,
ZhihongChen, MayaVarma, Jean-BenoitDelbrouck, DavidOsayande,MichaelPohlen,andPranavRa-
Magdalini Paschali, Louis Blankemeier, Dave jpurkar.2024. Multimodalimage-textmatchingim-
Van Veen, Jeya Maria Jose Valanarasu, Alaa provesretrieval-basedchestx-rayreportgeneration.
Youssef,JosephPaulCohen,EduardoPontesReis, InMedicalImagingwithDeepLearning,pages978–
et al. 2024. Chexagent: Towards a foundation 990.PMLR.
9AlbertQJiang,AlexandreSablayrolles,ArthurMensch, 2024. Direct preference optimization: Your lan-
Chris Bamford, Devendra Singh Chaplot, Diego guagemodelissecretlyarewardmodel. Advances
de las Casas, Florian Bressand, Gianna Lengyel, inNeuralInformationProcessingSystems,36.
Guillaume Lample, Lucile Saulnier, et al. 2023.
Mistral7b. arXivpreprintarXiv:2310.06825. PranavRajpurkarandMatthewPLungren.2023. The
currentandfuturestateofaiinterpretationofmed-
AJohnson,TPollard,SHorng,LACeli,andRMark. ical images. New England Journal of Medicine,
2023. Mimic-iv-note: Deidentifiedfree-textclinical 388(21):1981–1990.
notes(version2.2).physionet.
VignavRamesh,NathanAChi,andPranavRajpurkar.
AlistairEWJohnson,TomJPollard,SethJBerkowitz, 2022. Improvingradiologyreportgenerationsys-
NathanielRGreenbaum,MatthewPLungren,Chih- tems by removing hallucinated references to non-
ying Deng, Roger G Mark, and Steven Horng. existent priors. In Machine Learning for Health,
2019. Mimic-cxr,ade-identifiedpubliclyavailable pages456–473.PMLR.
databaseofchestradiographswithfree-textreports.
Ketan Rajshekhar Shahapure and Charles Nicholas.
Scientificdata,6(1):317.
2020. Clusterqualityanalysisusingsilhouettescore.
In2020IEEE7thinternationalconferenceondata
CharlesEKahnJr,CurtisPLanglotz,ElizabethSBurn-
scienceandadvancedanalytics(DSAA),pages747–
side,JohnACarrino,DavidSChannin,DavidM
748.IEEE.
Hovsepian, and Daniel L Rubin. 2009. Toward
best practices in radiology reporting. Radiology,
AkshaySmit,SaahilJain,PranavRajpurkar,AnujPa-
252(3):852–856.
reek,AndrewYNg,andMatthewLungren.2020.
Combining automatic labelers and expert annota-
MingjieLi,BingqianLin,ZicongChen,HaokunLin,
tions for accurate radiology report labeling using
XiaodanLiang,andXiaojunChang.2023. Dynamic
bert. In Proceedings of the 2020 Conference on
graphenhancedcontrastivelearningforchestx-ray
EmpiricalMethodsinNaturalLanguageProcessing
reportgeneration. InProceedingsoftheIEEE/CVF
(EMNLP),pages1500–1519.
ConferenceonComputerVisionandPatternRecog-
nition,pages3334–3343.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,
AmjadAlmahairi,YasmineBabaei,NikolayBash-
Chin-YewLin.2004. Rouge: Apackageforautomatic
lykov, Soumya Batra, Prajjwal Bhargava, Shruti
evaluation of summaries. In Text summarization
Bhosale, et al. 2023. Llama 2: Open founda-
branchesout,pages74–81.
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.
HaotianLiu,ChunyuanLi,QingyangWu,andYongJae
Lee.2024. Visualinstructiontuning. Advancesin
Shikhar Vashishth, Denis Newman-Griffis, Rishabh
neuralinformationprocessingsystems,36.
Joshi,RitamDutt,andCarolynPRosé.2021. Im-
provingbroad-coveragemedicalentitylinkingwith
HoangTNNguyen,DongNie,TaivanbatBadamdorj,
semantic type prediction and large-scale datasets.
Yujie Liu, Yingying Zhu, Jason Truong, and
Journalofbiomedicalinformatics,121:103880.
Li Cheng. 2021. Automated generation of
accurate\& fluent medical x-ray reports. arXiv Maria De La Iglesia Vayá, Jose Manuel Saborit,
preprintarXiv:2108.12126. Joaquim Angel Montell, Antonio Pertusa, Aure-
liaBustos,MiguelCazorla,JoaquinGalant,Xavier
Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gus-
Barber,DomingoOrozco-Beltrán,FranciscoGarcía-
tavo Hernández Ábrego, Ji Ma, Vincent Y Zhao,
García,etal.2020. Bimcvcovid-19+: alargean-
YiLuan,KeithBHall,Ming-WeiChang,etal.2021.
notateddatasetofrxandctimagesfromcovid-19
Large dual encoders are generalizable retrievers.
patients. arXivpreprintarXiv:2006.01174.
arXivpreprintarXiv:2112.07899.
ShuxinYang,XianWu,ShenGe,SKevinZhou,and
KishorePapineni,SalimRoukos,ToddWard,andWei- LiXiao.2022. Knowledgematters:Chestradiology
JingZhu.2002. Bleu: amethodforautomaticeval- reportgenerationwithgeneralandspecificknowl-
uationofmachinetranslation. InProceedingsofthe edge. Medicalimageanalysis,80:102510.
40thannualmeetingoftheAssociationforCompu-
tationalLinguistics,pages311–318. Feiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan,
AndyTsai, EduardoPontesReis, EduardoKaiser
AlecRadford,JongWookKim,ChrisHallacy,Aditya Ururahy Nunes Fonseca, Henrique Min Ho Lee,
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish ZahraShakeriHosseinAbad,AndrewYNg,etal.
Sastry,AmandaAskell,PamelaMishkin,JackClark, 2023a. Evaluatingprogressinautomaticchestx-ray
et al. 2021. Learning transferable visual models radiologyreportgeneration. Patterns,4(9).
fromnaturallanguagesupervision. InInternational
conferenceonmachinelearning,pages8748–8763. Feiyang Yu, Mark Endo, Rayan Krishnan, Ian Pan,
PMLR. Andy Tsai, Eduardo Pontes Reis, EKU Fonseca,
Henrique Lee, Zahra Shakeri, Andrew Ng, et al.
RafaelRafailov,ArchitSharma,EricMitchell,Christo- 2023b. Radiologyreportexpertevaluation(rexval)
pherDManning,StefanoErmon,andChelseaFinn. dataset.
10TianyiZhang,VarshaKishore,FelixWu,KilianQWein-
berger, and Yoav Artzi. 2019. Bertscore: Eval-
uating text generation with bert. arXiv preprint
arXiv:1904.09675.
11A Appendix
A.1 GPT-4PromptTemplateforGenerationofTrainingData
ThefollowingpromptwasusedinGPT-4togeneratetheGREENmodeltrainingdata. **ReferenceReport**and
**CandidateReport**fieldsarereplacedwiththeirrespectiveactualreports.
GPT-4Prompt
Objective:
Evaluate the accuracy of a candidate radiology report in comparison to a reference
radiology report composed by expert radiologists.
Process Overview:
You will be presented with:
1. The criteria for making a judgment.
2. The reference radiology report.
3. The candidate radiology report.
4. The desired format for your assessment.
1. Criteria for Judgment:
For each candidate report, determine:
- The count of clinically significant errors.
- The count of clinically insignificant errors.
Errors can fall into one of these categories:
a) False report of a finding in the candidate.
b) Missing a finding present in the reference.
c) Misidentification of a finding's anatomic location/position.
d) Misassessment of the severity of a finding.
e) Mentioning a comparison that isn't in the reference.
f) Omitting a comparison detailing a change from a prior study.
Note: Concentrate on the clinical findings rather than the report's writing style.
Evaluate only the findings that appear in both reports.
2. Reference Report:
**Reference Report**
3. Candidate Report:
**Candidate Report**
4. Reporting Your Assessment:
Follow this specific format for your output, even if no errors are found:
```
[Explanation]:
<Explanation>
[Clinically Significant Errors]:
(a) <Error Type>: <The number of errors>. <Error 1>; <Error 2>; ...; <Error n>
....
(f) <Error Type>: <The number of errors>. <Error 1>; <Error 2>; ...; <Error n>
[Clinically Insignificant Errors]:
(a) <Error Type>: <The number of errors>. <Error 1>; <Error 2>; ...; <Error n>
....
(f) <Error Type>: <The number of errors>. <Error 1>; <Error 2>; ...; <Error n>
[Matched Findings]:
<The number of matched findings>. <Finding 1>; <Finding 2>; ...; <Finding n>
```
12A.2 GPT-4ResponseusedasTrainingData
ThefollowingdepictsasampleGPT-4responsetothepromptinAppendixA.1forapairofreportsselectedfroma
’RadGraphPermutation’. Specifically,thecandidatereportcontainsexactlyoneRadGraphmodifierchange.
Reference: "Examination. Faintinfiltratesintheuppermiddlerightfieldanddoubtfulretrocardiacsuggestive
ofrespiratoryinfection. Costophrenicsinusesareclear. Noothernotablefindings."
Candidate: "Examination . Subpleural infiltrates in the upper dorsal right field and doubtful retrocardiac
suggestiveofrespiratoryinfection. Costophrenicsinusesareclear. Noothernotablefindings."
GPT-4Response
[Explanation]:
The candidate report misidentifies the anatomic location of the infiltrates.
The reference report mentions "upper middle right field" while the candidate report
mentions "upper dorsal right field".
[Clinically Significant Errors]:
(c) Misidentification of a finding's anatomic location/position: 1. The infiltrates
are in the upper middle right field, not the upper dorsal right field.
[Clinically Insignificant Errors]:
(a) False report of a finding in the candidate: 0.
(b) Missing a finding present in the reference: 0.
(c) Misidentification of a finding's anatomic location/position: 0.
(d) Misassessment of the severity of a finding: 0.
(e) Mentioning a comparison that isn't in the reference: 0.
(f) Omitting a comparison detailing a change from a prior study: 0.
[Matched Findings]:
3. Doubtful retrocardiac suggestive of respiratory infection;
Costophrenic sinuses are clear; No other notable findings.
```
13A.3 GPT-4PromptTemplateforPreferenceEvaluation
ThefollowingpromptwasusedinGPT-4fortheevaluationofpreferences. **ReferenceReport**,**Candidate
Report1**,and**CandidateReport2**fieldsarereplacedwiththeirrespectiveactualreports.
GPT-4Prompt
We would like to request your feedback on the radiology reports generated by two AI
assistants by comparing them to the reference report written by radiologists.
[Reference Report]
**Reference Report**
[Assistant 1]
**Candidate Report 1**
[Assistant 2]
**Candidate Report 2**
[Requirements]
1. The length of the reports is not important.
2. The style of the reports is not important.
3. The clinical accuracy is important especially for positive findings (i.e., diseases).
Therefore, please focus on clinical accuracy instead of the length and style.
Please compare the accuracy of their generated reports. You should tell me whether Assistant 1
is "better than", "worse than", or "equal to" Assistant 2.
Please first compare the generated reports with the reference report to analyze which one is
more in line with the given requirements.
In the last line, please output a single line containing only a single label selecting from
"Assistant 1 is better than Assistant 2", "Assistant 1 is worse than Assistant 2", and
"Assistant 1 is equal to Assistant 2".
14A.4 AlgorithmforModifyingRadiologyReports
WeemployedthisalgorithmtoproducepromptsforGPT-4tomodifycandidatereportsfornewimagingmodalities
thatlackRRGmodels.
1 def get_prompt(self , report):
2 error_types = self .get_error_combination(report)
3 # randomly choose if subtle or not , if subtle add "sentence"
4 subtle_change = ""
5 if random.random() > 0.5:
6 subtle_change = "Aim for subtlety , adjusting only one word where feasible . "
7 if not error_types == "no errors":
8 return f"[Objective]: Create a candidate radiology report that subtly
integrates specific errors based on the provided reference report.
9 Process Overview: You will be presented with:
10 1. Style of errors.
11 2. A reference radiology report to base your candidate report on.
12 3. The desired format for your candidate report. Note: Be short in your
response!
13
14 Style of errors:
15 Introduce errors related to {error_types}. The errors should be woven into the
report as if they were genuine observations from a medical image, without any meta−
commentary on their accuracy. {subtle_change}
16
17 Reference Report: \n{report}\n Desired format for your candidate report: \n\n
[Candidate]: <Candidate Report>"
18
19 return f"[Objective]: Create a candidate radiology report that has the same
20 clinical meaning but is slightly rephrased.
21 Process Overview: You will be presented with: \n 1.A reference radiology report to
base your candidate report on. \n 2. The desired format for your candidate report.
Note: Be short in your response! \n\n Reference radiology report: \n{report}\n\n
Desired format for your candidate report: \n\n [Candidate]: <Candidate Report>"
15A.5 VisualizationoftheGREENSummaryClusteringTechnique
Visualization(t-SNE)oftheclusteringtechniqueusedintheGREENsummary. Sentenceswereclusteredforeach
errorsubcategory.
16