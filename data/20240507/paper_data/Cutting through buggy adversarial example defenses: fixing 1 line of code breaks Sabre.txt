Cutting through buggy adversarial example defenses:
Sabre
fixing 1 line of code breaks
Nicholas Carlini
Google DeepMind
Abstract
Sabre is a defense to adversarial examples that was accepted at IEEE S&P 2024. We first reveal
significantflawsintheevaluationthatpointtoclearsignsofgradientmasking. Wethenshowthecause
of this gradient masking: a bug in the original evaluation code. By fixing a single line of code in the
original repository, we reduce Sabre’s robust accuracy to 0%. In response to this, the authors modify
the defense and introduce a new defense component not described in the original paper. But this fix
containsasecondbug;modifyingonemorelineofcodereducesrobustaccuracytobelow baselinelevels.
1 Introduction
Sabre [5] is a defense to adversarial examples that claims to be 3× more robust to attack than the current
state-of-the-art[?],bringingthebestattacksonCIFAR-10atadistortionofε=8/255from29%successrate
down to just 11%. In this paper we begin with a critique of the Sabre paper, and show it contains multiple
evaluationflaws. Forexample,thepaperclaimsthatSabreismoreaccuratewhenanadversaryisattacking
thedefensethanwhennotunderattack,claimsmathematicallyimpossiblelevelsofrobustness,anddoesnot
follow standard recommended best practices [3] when evaluating adversarial robustness—including omitting
any analysis against adaptive attacks [7].
Nevertheless, Sabre was accepted at IEEE S&P 2024, one of the top computer security conferences.
(Readers may be forgiven for mistaking this defense for a different defense—also with a flawed evaluation—
accepted at last year’s IEEE S&P 2023. [2])
Inthisshortpaperweshowthatbychangingasinglelineofcodetofixabuginthepublishedevaluation
code, the attack success rate increases to 100%. The authors responded to our attack by modifying the
published defense code to introduce a new defense component that was not present in the original paper,
thereby changing the defense to a different defense than was initially proposed. Unfortunately, this modifi-
cationcontainsasecondbug,andbymodifyingasecondlineofcodeweagaincompletelybreakthedefense.
The authors claim to have had a fix to this second attack for six weeks, but we have been unable to verify
these claims because the authors have not published code for this fix, nor have they described the necessary
changes. Whentheauthorsprovidethisinformationwewillupdateourpaperwithananalysisofanyfuture
modifications or corrections to the currently published defense implementation.
2 Background & Sabre
We assume familiarity with evaluating the robustness of adversarial example defenses [3]. We encourage
readers not familiar with this literature to read the related work before returning to this paper.
Sabre [5] is a preprocessor defense: given an input x, it first pre-processes the input with a function p,
and then classifies the resulting pre-processed image with a standard (adversarially trained) neural network
f; the resulting defended classifier is thus fSabre = f ◦p. Because our attack is completely independent of
the method by which Sabre works, we do not describe it in any more detail. Interested readers can refer to
the original paper [5] for a complete description of the methodology.
1
4202
yaM
6
]RC.sc[
1v27630.5042:viXra3 Mathematically Impossible
Sabre makes a number of claims that are impossible for any correct evaluation. The impossibility of the
claims it makes are sufficiently well understood that, instead of describing the errors ourselves, we prompt
GPT-4 with a copy of the checklist in [3], a screenshot of the defense paper [5], and then ask the question
“Give one thing that is concerning about this image that suggests the paper’s evaluation might be flawed.
Focus on [some paragraph or figure].”
• Nontrivial accuracy at ε=0.5. “Given the estimated value of approximately 92.66% accuracy at a
perturbationbound(ε)of0.5forthe.“Ours”defense,thisresultappearstocontradicttheexpectations
set forth by Carlini et al. They stated that for an ℓ distortion of 0.5, model accuracy should not be
∞
better than random guessing because any image can be converted into a solid gray picture, making it
impossible for the model to accurately predict the original content.”
• Attacking the model makes it more accurate. “In the SPEECH-COMMANDS row evaluated
with SABRE (the proposed defense), there is a noticeable discrepancy: the performance under the
“No Attack” condition is 83.57, which is lower than the performance under several of the adversarial
attacks such as FGSM (91.21), PGD 7 steps (95.40), and PGD 20 steps (95.68). This is concerning
becausewewouldgenerallyexpectamodeltoperformbestintheabsenceofanyattack,asadversarial
examples are specifically designed to degrade performance.
The fact that the model performs better under attack than when there is no attack challenges the
typicalexpectationsandcouldsuggestissuessuchasoverfittingtotheadversarialexamples,aproblem
with the evaluation methodology, or that the reported numbers may not accurately reflect the true
robustness of the model. This anomaly warrants a closer examination of the experimental setup and
the robustness evaluation to ensure the validity of the results.”
In both of these cases, the language model has correctly identified the flaws in the original paper, and the
author of this paper has checked that the model’s response is accurate and true.
4 Additional evaluation flaws
Sabre also deviates from recommended practices in evaluating adversarial robustness. Specifically, it does
not follow the widely accepted best practices in evaluating adversarial robustness [3] in the following ways:
• [3](§5.2) “Verify iterative attacks perform better than single-step attacks” In the original evaluation,
the robust accuracy at 100 iterations of PGD is higher than the robust accuracy at 1 step of FGSM.
• [3](§5.4) “Verify adaptive attacks perform better than any other” The paper performs no adaptive
attack. (See the following section.)
• [3](§4.4) “Try at least one gradient-free attack and one hard-label attack” The paper performs no
gradient-free attacks or hard-label attack.
• [3](§5.1) “Compare against prior work and explain important differences” The paper includes a com-
parison to PGD, but vastly under-reports the robustness of PGD as 30% accuracy at ε = 0.3 instead
of 90%+. (It also omits comparisons to Feature Squeezing; see the following section.)
• [3](§4.11)“Attack with random noise of the correct norm” Thepaperdoesnotattemptthisevaluation.
• [3](§4.13) “Perform ablation studies with combinations of defense components removed” The paper
does not do this. (See the following section.)
• [3](§4.3) “Applying many nearly-identical attacks is not useful” The paper reports the robustness of
multiple nearly-identical attacks instead of performing strong adaptive attacks.
25 Significant flaws not observable from the paper
We make a number of other comments on the Sabre defense that can not be observed by reading the paper
alone, and can only be noticed by reading the published code or corresponding with the authors.
Sabre is not evaluated against adaptive attacks. An adaptive attack is one that is “adapted to the
specificdetailsofthedefenseandattempt[s]toinvalidatetherobustnessclaimsthataremade”[3]“Adaptive
attacks have (rightfully) become the de facto standard for evaluating defenses to adversarial examples.” [7]
As the authors of [3] say, “Defending against non-adaptive attacks is necessary but not sufficient. It is our
firm belief that an evaluation against non-adaptive attacks is of very limited utility” (emphasis
not added here, and was present in original text).
Despite the fact that the paper writes Sabre “is virtually as robust against adaptive attacks as it is
against weak attacks,” this was not actually evaluated. The authors do not attempt an adaptive attack of
Sabre, and in personal communication, the authors state that Sabre is not intended to be robust against
future attacks—only those specific attacks considered in the paper. When the paper writes it is robust
against adaptive attacks, the author’s intent was to say that that AutoAttack [4], one particular adversarial
example generation method, does not succeed more often than PGD.
But this is not what an adaptive attack means. No attack can be said to be adaptive isolation—indeed,
thephrase“adaptiveattack”neverappearsintheAutoAttackpaper[4]. AutoAttackwasnotmeanttobea
replacement for an adaptive attack where the adversary designs the attack to specifically target the defense:
they even write “we [...] see AutoAttack and adaptive attacks as complementary. Claiming robustness to
adaptive attackers without actually performing an adaptive attack misleads the reader.
Sabre includes baselines against known-insecure defenses. The paper compares the proposed de-
fense to ME-Net [9], a defense published in December 2019 that was broken two months later in February
2020 [7]. ME-Net is indeed a similar defense, in that it also pre-processes inputs in a similar manner to
Sabre, and so one might expect that Sabre would evaluate against a similar attack strategy as the one
that was used to break ME-Net, but this was not done.
Moreover, the authors state that they were aware that ME-Net was previously broken, and yet (1) did
not mention this fact in their paper, and (2) did not attempt to break their defense with similar attacks to
the ones that broke ME-Net. Papers should not compare to prior insecure defenses without considering the
attack that broke them, because it is likely that the attack that broke the prior paper also will break this
one. And as we will show, the type of gradient obfuscation ME-Net suffered from is very similar to the type
of gradient obfuscation Sabre suffers from.
Sabreimplementspriorbaselinesincorrectly. TheSabrepapercontainsacomparisontoadversarial
training [6], the leading methodology behind the current state-of-the-art adversarial example defenses. Un-
fortunately, Sabre incorrectly implements adversarial training. On MNIST, instead of adversarial training
reducing attack success rate to just 11% on at a distortion of ε = 0.3, the paper reports that the attack
success rate against adversarial training is over 70%. This makes Sabre appear to be a much larger im-
provement compared to prior work than is actually the case. While qualified reviewers should know the
correct baseline numbers to which Sabre should be compared, an under-informed reader not familiar with
prior work might (incorrectly) believe that Sabre has improved significantly on prior work when, in fact,
the baseline is instead incorrect.
(Sabre also omits any comparisons to defenses that have since improved on adversarial training since it
was published in 2018; further reducing the gap between the papers state-of-the-art MNIST claims and the
true state-of-the-art.)
36 Our attack
We break Sabre twice.
6.1 Our first break
The paper repeatedly reports that Sabre is completely differentiable, but the defense is evaluated by wrap-
ping the preprocessing step with BPDA [1], a method that replaces the gradient of the preprocessor with an
estimate that f′(x) = x. But Sabre is already differentiable; wrapping it with BDPA removes the useful
gradient and replaces it with one that is much less useful. And so to break Sabre all we have to do is
remove the unnecessary BPDA wrapping.
1 diff --git a/core/defenses/sabre.py b/core/defenses/sabre.py
2 index fe509e6..bf13629 100644
3 --- a/core/defenses/sabre.py
4 +++ b/core/defenses/sabre.py
5 @@ -165,7 +165,7 @@ class SabreWrapper(nn.Module):
6 model = Sabre(eps=eps, wave=wave, use_rand=use_rand, n_variants=n_variants)
7 self.core = model
8 self.base_model = base_model
9 - self.transform = BPDAWrapper(lambda x, lambda_r: model.transform(x, lambda_r).float())
10 + self.transform = (lambda x, lambda_r: model.transform(x, lambda_r).float())
11 @property
12 def lambda_r(self):
With this change, after running the original evaluation code, the defense achieves <5% accuracy on the
MNIST dataset with a perturbation bound of 0.3. By increasing the number of attack iterations from 7
to 100, the defense accuracy drops to 0%. On the CIFAR-10 dataset we can achieve 0% accuracy at a
perturbation bound of 8/255 (as originally considered in the paper) with the same bugfix.
6.2 A fix to our first break
The authors acknowledge that our break effectively reduces Sabre’s robust accuracy to 0%. In response,
the authors modify the defense and introduce a new component that is not described in the original paper.
Specifically, the modified Sabre first discretizes inputs to a fixed number of digits of precision as follows:
1 def precision_blend(self, x):
2 if self.eps > 0:
3 precision = max(min(-int(math.floor(math.log10(abs(1.25 * self.eps)))) - 1, 1), 0)
4 x = self.diff_round(x, decimals=precision)
5 return x
6
7 def diff_round(self, x, decimals=1):
8 scale_factor = (10 ** decimals)
9 x = x * scale_factor
10 diff = (1 + self.error_coefficient) * x - torch.floor(x)
11 x = x - diff + torch.where(diff >= 0.5, 1, 0)
12 x = x / scale_factor
13 return x
Nevertheless, theauthorsclaimthatthisdefensecomponentwasintendedaspartoftheoriginaldefense.
In personal communication with the authors, they assert this defense component was not described in the
paper because it is not an important detail of the overall defense. (This is despite the fact that the entire
reason this component was introduced in the first place was to prevent the attack we described above.)
In further communication with the authors, they informed us that hyperparmaters should be set to
discretizeMNISTimagessothateverypixelissettoeither0or1(i.e.,zerodecimalsofprecisioninthecode
above), and discretize CIFAR-10 images to one decimal digit of precision.
4On differentiability. Throughout the paper, Sabre claims to be “end-to-end differentiable to avoid
gradientmasking”, claims“innatedifferentiabilityof[their]defensemechanism”, andclaimsto“ensureend-
to-end differentiability” [5]. But with this new changed proposed by the authors, this is no longer the case:
discretization is the quinticential example of nondifferentiability. Indeed, gradient-based attacks are only
effectivewhenmodelsareusefullydifferentiable. Andyet,thepaperstatesitverifies“theabsenceofgradient
obfuscation”.
And this is the reason why diff round is not just
asinglefunctioncalltotorch.round: thediff round
function is designed to implement a differentiable 0.4
roundingoperation. Unfortunatelythisfunctionisnot
correct, and has a significant bug.
0.3
Figure 1 shows a plot of this diff round function
when varying the input from 0.0 to 0.3, and then a
zoomed in view of the near-constant region. Observe 0.2
that the gradient of the function is negative almost
everywhere. And so, therefore, when we perform gra-
0.1
dient descent it will actually move away from the ad-
versarial examples!
And so while it is technically true that this func- 0.0
tion is “differentiable” in the sense that ∇f(x) is de- 0.00 0.05 0.10 0.15 0.20 0.25 0.30
fined almost everywhere, Sabre contains the single x
most extreme form of gradient masking the author of
this paper has ever seen. Not only does the gradient Figure 1: Buggy differentiable rounding operation.
hide the true direction of the adversarial examples, it
actually points directly away from them!
6.3 Our second break
Previously we broke Sabre by removing an unnecessary BPDA wrapper. We now insert a necessary one.
1 diff --git a/core/defenses/sabre.py b/core/defenses/sabre.py
2 index 319aebd..69fec01 100644
3 --- a/core/defenses/sabre.py
4 +++ b/core/defenses/sabre.py
5 @@ -61,7 +61,7 @@ class Sabre(nn.Module):
6 def precision_blend(self, x):
7 if self.eps > 0:
8 precision = max(min(-int(math.floor(math.log10(abs(1.25 * self.eps)))) - 1, 1), 0)
9 - x = self.diff_round(x, decimals=precision)
10 + x = BPDAWrapper(lambda x: self.diff_round(x, decimals=precision))(x)
11 return x
OnMNIST,wherethedefensebinarizesinputstoeither0or1,weachieveanattacksuccessrateof21%.
This is significantly higher than the 13% attack success rate of discretization alone, as reported by [8] which
was first published in 2017. Thus Sabre with binarization is worse than binarization alone. In order to
demonstrateconclusivelythatitisthebinarizationalonethathascausedthisrobustnessincrease,ifweadjust
thenumberofdecimalsfrom0to1(andthereforeroundeachinputpixeltooneof{0,0.1,0.2,...,0.9,1.0}),
then we again see that our one-line diff again increases the attack success rate to 100%.
On CIFAR-10, where the defense sets the number of decimals to 1 and so again discretizes each input
pixel to {0,0.1,0.2,...,0.9,1.0}, we achieve an attack success rate of 100%. Unfortunately, this CIFAR-10
result is for a model we have trained ourselves (but using the code published by the authors), and not for a
model trained by the authors themselves. When we informed the authors of this second attack, they stated
that this fix was incomplete for CIFAR-10, and that the defense must be adjusted differently for this model.
Howevertheauthorshavebeenunabletoprovideadescriptionofthenecessarychanges, animplementation
of the code for the fix, or a pointer to where the necessary adjustments are described in the original paper.
5
)x(dnuor_ffid7 Author Response
We shared an advance copy of this paper describing the evaluation flaws from Sections 3-5 with the authors
of Sabre. They provided the following response, which we quote verbatim:
7.1 Claims
• Use of GPT-4: Generative models should not be used to judge research papers.
Among the many known issues of such models are their tendency to produce fac-
tually incorrect information and having their responses biased by their inputs.
• Attackingthemodelmakesitmoreaccurate: Theproposedframeworkisprimarily
designed around the notion of noise reduction, such that the higher the amount of
noiseobserved,thestrongerthefiltering. Therefore,theobservationthatattacking
samples results in higher classification accuracies than those of benign samples is
simply indicative of the presence of noise in the benign samples. Through the
denoisingofSABRE,noiseisremovedbothfromtheattackedandbenignsamples,
allowing classifiers to be less susceptible to noise, thereby also improving their
generalization.
• Nontrivial accuracy at ε = 0.5: The evaluation results reported by the paper for
an attack performed with ε = 0.5 assume the attacks are performed on a model
robustly trained using the SABRE defense. While the expectations set forth by
Carlini et al. are theoretically sound, this does not guarantee that any attack
method would find such perturbations.
7.2 Evaluation
Theitemsbelowaddressthepointsmaderegardingthestandardevaluationprocedures
highlighted in the critique:
• (§5.2)“Verify iterative attacks perform better than single-step attacks” Thiswould
bethecaseiftheiterativeattacksonlyappliedperturbationswhenthesamplesare
correctlyclassified. Thisishowevernotthecasewiththebaselineimplementations
typically used for benchmarking.
• (§5.4) “Verify adaptive attacks perform better than any other” We considered
“adaptive attacks” to be attacks that choose the most appropriate attack per
samplegiventhedefense,possiblyoutofmultipleoptions. Whilethecritiquecon-
siders “adaptive attacks” to be attacks specifically designed against the defense,
we believed designing such attacks to be outside the scope of the paper and left it
for future work.
• (§4.4) “Try at least one gradient-free attack and one hard-label attack” Gradient-
free and hard-label attacks were not in the context of our threat model.
• (§5.1) “Compare against prior work and explain important differences” The paper
includes comparisons against several, widely used, prior work, with parameters
set to allow fair comparison of different defenses within our evaluation settings.
Unlikethereportedrobustaccuracyofmodelsthathavebeenadversariallytrained
with a 40-step PGD attack, our evaluations aim to assess the robustness and
generalization of models that have been adversarially trained with a 7-step PGD
attack, which explains the drop in robustness accuracy.
• (§4.11) “Attack with random noise of the correct norm” The threat model consid-
ered in the paper only considers gradient-based (whitebox) attacks.
6• (§4.13) “Perform ablation studies with combinations of defense components re-
moved” Appendix A in the paper provides ablation studies of SABRE with and
without non-core components of the proposed defense.
• (§4.3) “Applying many nearly-identical attacks is not useful” The paper includes
five distinct types of adversarial attacks widely used in the literature, namely:
FGSM, PGD, EoT-PGD, CW, and AutoAttack.
7.3 SABRE Details
• Evaluation against adaptive attacks. We considered “adaptive attacks” to
be attacks that choose the most appropriate attack per sample given the defense,
possibly out of multiple options. While the critique considers “adaptive attacks”.
to be attacks specifically designed against the defense, we believed designing such
attacks to be outside the scope of the paper and left it for future work.
• UseofBPDA.BPDAwasusedforthesimplepurposeofensuringdifferentiability
regardless of the technical specificities of the implementation.
• Comparison against a known-insecure defenseThepaperusestheME-NET
defense as a baseline because even though it has been broken, the approach is
somewhatsimilartoSABREandprovideshighlevelsofrobustnesswhenevaluated
asproposedintheoriginalpaper. ThepaperreferencesME-Netforbenchmarking
and does not to suggest it as a secure standalone solution. The ME-Net defense
was broken by the AutoAttack method, which has also been used to evaluate the
robustness of SABRE.
• Implementation of Adversarial Training. The paper uses a publicly avail-
able implementation of Adversarial Training, which is used in the literature for
benchmarking. Unlike the reported robust accuracy of models that have been
adversarially trained with a 40-step PGD attack, the SABRE paper assesses the
robustness and generalization of models that have been adversarially trained with
a 7-step PGD attack, explaining the drop in robustness accuracy.
7.4 Our response
We have several disagreements with the above response, but for brevity, limit our reply to two items:
• On “adaptive attacks” and robustness to future attacks. As we write throughout this paper,
the phrase “adaptive attack” has a standard definition in the adversarial machine learning literature:
they are ones that are “adapted to the specific details of the defense and attempt to invalidate the
robustness claims that are made” [3].
Nowhere in the original paper do the authors state their (new) definition of adaptive attacks (running
a set of fixed attacks and reporting the minimum among these), or state their proposed defense is
only intended to be robust against the specific attacks considered in this paper. Moreover, it would be
extremely abnormal for a security paper to ever make this claim because any defense, once published,
will be subject to scrutiny. If robustness to fixed attacks is all that is desired, hundreds of papers
already satisfy this goal.
• On threat models. The authors claim gradient-free attacks (and attacks that just add noise of the
correct norm) are not “in scope”. This is also incorrect: any attacker who can compute gradients
can just throw away the gradient and only look at the loss. The threat model of white-box attacks
encompasses the threat model of black-box attacks.
78 Conclusion
Sabre is the second defense to be accepted at IEEE S&P in as many years that had clear signs of a
flawed evaluation which should have been easily identified by qualified reviewers. As we have shown, these
evaluation flaws are caused by one (or two) bugs in the original evaluation that can be fixed by modifying
one (or two) lines of code.
There is no doubt that defending against adversarial examples remains an interesting and important
problem. And reviewers are attracted to papers making bold claims: it is much more tempting to accept a
paperthatclaimsnear-perfectrobustaccuracythanapaperthatclaimsamarginalimprovementinaccuracy.
But it is of utmost importance that papers with errors such as this are identified during review, so as to not
suggest that incorrectly evaluating defenses is tolerable.
Where do we go from here? Adversarial machine learning is no longer a field that only studies toy
problems to develop a better scientific understanding of the robustness of machine learning models. There
are now production systems that depend on the robustness of the underlying machine learning models.
If,asacommunity,wecannottellthedifferencebetweenadefensethatclaims90%+robustnessandone
that actually gives 0% robustness—on the simplest datasets and with glaring evaluation flaws—what hope
do we have for evaluating the robustness of defenses designed to protect real, complex, production systems?
References
[1] Athalye, A., Carlini, N., and Wagner, D. Obfuscated gradients give a false sense of security:
Circumventingdefensestoadversarialexamples. InInternationalconferenceonmachinelearning (2018),
PMLR, pp. 274–283.
[2] Carlini, N. A llm assisted exploitation of ai-guardian. arXiv preprint arXiv:2307.15008 (2023).
[3] Carlini, N., Athalye, A., Papernot, N., Brendel, W., Rauber, J., Tsipras, D., Good-
fellow, I., Madry, A., and Kurakin, A. On evaluating adversarial robustness. arXiv preprint
arXiv:1902.06705 (2019).
[4] Croce, F., and Hein, M. Reliable evaluation of adversarial robustness with an ensemble of diverse
parameter-free attacks. In International conference on machine learning (2020), PMLR, pp. 2206–2216.
[5] Diallo, A. F., and Patras, P. Sabre: Cutting through adversarial noise with adaptive spectral
filteringandinputreconstruction.In2024IEEESymposiumonSecurityandPrivacy(SP)(LosAlamitos,
CA, USA, may 2024), IEEE Computer Society, pp. 75–75.
[6] Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning
models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083 (2017).
[7] Tramer, F., Carlini, N., Brendel, W., andMadry, A. Onadaptiveattackstoadversarialexample
defenses. Advances in neural information processing systems 33 (2020), 1633–1645.
[8] Xu, W., Evans, D., and Qi, Y. Feature squeezing: Detecting adversarial examples in deep neural
networks. arXiv preprint arXiv:1704.01155 (2017).
[9] Yang, Y., Zhang, G., Katabi, D., and Xu, Z. Me-net: Towardseffectiveadversarialrobustnesswith
matrix estimation. arXiv preprint arXiv:1905.11971 (2019).
8