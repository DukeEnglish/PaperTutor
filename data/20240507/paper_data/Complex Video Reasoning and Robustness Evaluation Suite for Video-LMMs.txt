Complex Video Reasoning and Robustness Evaluation
Suite for Video-LMMs
MuhammadUzairKhattak1 MuhammadFerjadNaeem2 JameelHassan1
MuzammalNaseer1 FedericoTombari3,4 FahadShahbazKhan1,5 SalmanKhan1,6
1MohamedBinZayedUniversityofAI 2ETHZurich 3Google
4TUMunich 5LinköpingUniversity 6AustralianNationalUniversity
Abstract
RecentadvancementsinLargeLanguageModels(LLMs)haveledtothedevel-
opment of Video Large Multi-modal Models (Video-LMMs) that can handle a
wide range of video understanding tasks. These models have the potential to
be deployed in real-world applications such as robotics, AI assistants, medical
imaging,andautonomousvehicles. ThewidespreadadoptionofVideo-LMMsin
ourdailylivesunderscorestheimportanceofensuringandevaluatingtheirrobust
performance in mirroring human-like reasoning and interaction capabilities in
complex,real-worldcontexts. However,existingbenchmarksforVideo-LMMs
primarilyfocusongeneralvideocomprehensionabilitiesandneglectassessing
their reasoning capabilities over complex videos in the real-world context, and
robustnessofthesemodelsthroughthelensofuserpromptsastextqueries. Inthis
paper,wepresenttheComplexVideoReasoningandRobustnessEvaluationSuite
(CVRR-ES),anovelbenchmarkthatcomprehensivelyassessestheperformanceof
Video-LMMsacross11diversereal-worldvideodimensions. Weevaluate9recent
models,includingbothopen-sourceandclosed-sourcevariants,andfindthatmost
oftheVideo-LMMs,especiallyopen-sourceones,strugglewithrobustnessand
reasoningwhendealingwithcomplexvideos. Basedonouranalysis,wedevelopa
training-freeDual-StepContextualPrompting(DSCP)techniquetoenhancethe
performanceofexistingVideo-LMMs. Ourfindingsprovidevaluableinsightsfor
buildingthenextgenerationofhuman-centricAIsystemswithadvancedrobust-
ness and reasoning capabilities. Our dataset and code are publicly available at:
mbzuai-oryx.github.io/CVRR-Evaluation-Suite/.
1 Introduction
Recently,LargeLanguageModels(LLMs)[Touvronetal.,2023,Zhengetal.,2023,Jiangetal.,2024]
havedemonstratedimpressivereasoningandplanningcapabilitieswhilesimultaneouslyhandling
awiderangeofNLPtasks[Weietal.,2022a,Brownetal.,2020]. Consequently,theirintegration
withthevisionmodality,specificallyforvideounderstandingtasks,hasgivenrisetoVideoLarge
Multi-modal Models(Video-LMMs) [Li et al.,2023b]. These modelsact as visual chatbotsthat
acceptbothtextandvideoasinputandhandleadiversesetoftasks,includingvideocomprehension
[Maazetal.,2023],detailedvideounderstanding[Linetal.,2023],andactiongrounding[Zhangetal.,
2023]. Asthesemodelsdirectlycapturevideodata,theyholdsubstantialpotentialfordeploymentin
real-worldapplicationssuchasrobotics,surveillance,medicalsurgery,andautonomousvehicles.
However,asthesemodelsassumeanexpandingroleinoureverydaylives,assessingtheirperformance
incomprehendingcomplexvideosanddemonstratingreliablereasoningandrobustnesscapabilities
acrossdiversereal-worldcontextsbecomesessential. Video-LMMswithsuchcapabilitieswillbe
4202
yaM
6
]VC.sc[
1v09630.5042:viXraTextual Complex Inthewild Contextual MultipleTemporalOrder Table 1: Comparison of
Benchmark
RobustnessReasoning (OOD) DependencyActions &Fine-grained
CVRR-ESwithexistingbench-
MSVD-QA[Xuetal.,2017]
MSRVTT-QA[Xuetal.,2017] marks for video QA. The
TGIF-QA[Jangetal.,2017] CVRR-ES benchmark repre-
ActivityNet-QA[Yuetal.,2019]
VideoChat-GPT[Maazetal.,2023] sentsaninitialefforttoassess
MVBench[Lietal.,2023c] Video-LMMsinthecontextof
SEED-Bench[Lietal.,2023a]
theirapplicabilityandsuitabil-
CVRR-ES(ours)
ityinreal-worldapplications.
non-eN xo in st-e ex ni ts ste cn et n ea c dt eio pn ics tw ioi nth s. Time order understanding. Human 96.67%
Mult i ap l se in a gc lt ei o vn ids ein o. 6.0%6.33% U en md oe tr is ot na an ld cin og n to ef x t. GP GT e4 mV i( nis i-io Pn ro) 53.2% 70.78%
13.25% 12.17%
TimeChat 32.89%
VideoChat 25.78%
action unF din ee rs- tg ar na din ine gd . 9.58% EvaluC aV tiR oR n Suite 11.67% I n sote cr iap lr e ct oa nt tio en xt o .f Video-ChatGPT 24.96%
Video-LLaMA-2 21.62%
8.58% 7.92% LLaMA-VID 16.46%
Partial actions. Unusual and Physically MovieChat 16.41%
eN xo in st-e ex ni ts ste cn et n ea c dt eio pn ics tw io Ii nnth ts e. r5 p. r7 e5 ta% tion1 o1 f. 38% 7.38% C Oo bn jt ein ctu i It ny sA tan ano ndm c ea l Co ou us n a tc .tivities. Video LLaVa 0 1 25 0.92% 40 60 80 100
visual context. Accuracy % (averaged over 11 video dimensions)
Figure1: Left: CVRR-EScomprisesof11diversecomplexvideoevaluationdimensionsencompass-
ingavarietyofcomplex,real-worldcontexts. Right: OverallperformanceofVideo-LMMsonthe
CVRR-ESbenchmark. ResultsforeachVideo-LMMareaveragedacross11videodimensions.
moreeffectivewhenintegratedintoourdailylivesforsolvingperceptiontasksandwillbeapromising
steptowardsbuildinghuman-centricAI-assistivesystems.
SeveralattemptsinliteraturehavebeenmadetobenchmarkVideo-LMMs. SEED-Bench[Lietal.,
2023a]curatedaMCQ-basedbenchmarkingdatasetincluding3evaluationdimensionsforvideos.
Similarly, MV-Bench [Li et al., 2023c] constructed the Video-LMM benchmark and assembled
20challengingvideotasksforevaluatingthespatialandtemporalunderstandingofthesemodels.
WhilethesemethodsaimatbenchmarkingVideo-LMMs,theypredominantlyevaluatevideoand/or
temporalcomprehensionabilitiesandoverlookthecomplexreasoningaspectsofVideo-LMMsfor
real-worldcontext,andtheirrobustnesstowardsuserinputtextqueries;bothofwhicharecrucialto
ensuretheirresponsibleengagementwithhumansinvariousreal-worldsituationsinthewild. While
somestudieshaveexploredsimilarareassuchashallucinationsinimage-basedLLMs[Liuetal.,
2023a,Qianetal.,2024],nosuchcomprehensivestudyexistsforthecaseofVideo-LMMs.
Motivatedbythewide-scaleapplicationsofVideo-LMMsandthelackofworld-centriccomplex
videobenchmarkingefforts,wepresentanewbenchmark,ComplexVideoReasoningandRobustness
Evaluation Suite (CVRR-ES), to comprehensively assess the performance of Video-LMMs. As
showninTab. 1,CVRR-ESevaluatesVideo-LMMsonkeyaspectsofrobustnessandreasoningin
videos,encompassingvideodomainsthatmoreaccuratelytestmodelsinreal-worldscenariossuchas
videoshavingcontextualdependencyandin-the-wildaspects. CVRR-ESisanopen-endedvideoQA
benchmarkcomprising11real-worldvideocategorydimensions(Fig. 1,left)thatencompassdiverse
evaluationaspects. Thesedimensionsspanfromcontext-dependent(e.g., social, emotional, etc.)
categoriestoonesthatoftentakeplaceinthewildsuchasvideoscontainingphysicallyanomalous
activities. We comprehensively evaluate a representative set of 9 recent Video-LMMs (Fig. 1,
right)includingbothopen-sourceandclosed-sourcemodelsontheCVRR-ESbenchmarkusinga
LLM-assistedautomaticevaluationframework[Maazetal.,2023,Caietal.,2023].
TheperformanceofVideo-LMMsontheCVRR-ESbenchmarkrevealsthatthesemodelsstruggle
to correctly comprehend complex videos indicating their weak reasoning and lack of robustness
tothetextualuserqueries(Fig. 2). Forinstance,state-of-the-artVideo-LLaVA[Linetal.,2023]
achievesonly15.92%performanceaveragedacross11videodimensionsofCVRR-ES.Incontrast,
closed-sourcemodelsincludingGPT4V(vision)[OpenAI,2023]andGemini-Vision-Pro[Google,
2023]exhibitrelativelystrongerperformancebutstilllagbehindtheperformanceofhumans. Using
CVRR-ES benchmark, we extensively perform quantitative and qualitative analysis formulating
importantinsightsintotheseVideo-LMMsbasedontheirfailurecasesandindividualperformances
acrossthediversevideodimensions.
2
sMML
oediVH reo tw rie m vea n ity e min sd i fv roid mua tl hs ee x trit u t nh ke ? vehicle to C cao rr r te oc rt e R tre iesp veo n bs lae c: kT h br ae ge s p fre oo mp l te h e ex tit r ut nh ke . Two individuals exit the vehicle to retrieve items from the trunk.
Based on the visual cues provided in the Two individuals exit the Two individuals exit the In the video, we can see a
sequence of images, it appears that four 2. vehicle to retrieve items vehicle to retrieve items young man and woman
individuals exit the vehicle. from the trunk. from the trunk. exiting the car …
Correct Response: one man uses his back A man is seen standing in a In the video, a man is seen
muscles to crack eggs placed by another kitchen with his hands on his standing in a kitchen and
What is happening in the video? man into a bowl behind him. The process hips. Another man is standing washing dishes. He then
is amusing and ends with a celebratory behind him and pouring puts on a white robe and
turn by the man holding the eggs. something into a bowl. continues washing dishes.
B o i w un nna hvs s o iae ucln hd v so pe io t s ephn csoe tn iur… ne ri gan i, n k d po g ii l en v ya re i d s py op u oe ne a sll 'ir l o st.s i wo T o bhn n a le ceii q ks dp u p ar bia d te e n r whff k ro i ao nsr im sdm e te ti lamn h e g eb vs e o a t lw o .p lr ,a nk … b a bpe o ph wA ei lnn a ddo r o st th wh te oe nr b f tm i e hrs a eptn o fm ii u rs sa r th in n mo g…l a d t ni hn t 'eh sg e c sa hos b e ono c t re tow snn .l t do s f m o f fao tno h d e a b T h b ha i ag mc ceu k k y p p ,f e o ci as r auc ss ui ront s sa ng iw nn s s gd ao t i a t hmn en ig r me d ow o i n tnni ogt e hh b s i e hsh el isi h vs e i e n. r d . I y w pn o la a ut t sh n e te irg c fv m bri od oa me wno lp a, . ow bue ur cis n ke ege t a to a
B we af so vre is it bh le e m inu tg h ew sa cs e m neo ?ved, what other object C w mo a or s vr ee vi dc st i i b nR l e e ts h bp eeo vfn o ids re e e: ot N h . eo mot uh ge r w o ab sj ect A t wh at ee s sn mcn e oi ns v eb e a dbl .el fw oa res v this eib mle u i gn B bae lf lo wre a sth ve is m ibu leg i nw a ths em so cv ee nd e, . a tennis
B bee cfo ar me et h ve is m iblu eg u w na ds e rm no ev ae thd , i ta . tennis ball A t mh oy ee v sl el co dew .ne t e bn en fois r eb a thll ew mas u v gi s wib al se in A ba t le l.nnis I h Bn o e t fld oh i re n e gb t e hag emi n mun ugin gwg w io t ahf s t a h m te e o v n vi n ed die s ,o b a, a t wlh l he o inp te e a r v s t aao sbn el ew w. a as s
visible in the scene.
W c ve idh le ey b o i r ?s a t th ine g g io nl f te hr e C i hn io s tr tt ir e ne a gc d t t h hR e ee s i gsp o o e lfxn sps te r ie c: s kT s h ii nne tg og h o ti hl sf e e a r n g i rgs oe n uro nat dn c .de l fe rub sra trt ain tig o nan bd y T h hh e oe lh e g a .o s l jf ue sr t i s h ic te tl he eb r ba ati ln l ig n tb oe c thau e se T hh ee h g ao s l jf ue sr t i s m c ae dle eb ar a ht oin leg ib ne oc nau e,s e …
Based on … the golfer is celebrating because they The golfer is celebrating The golfer is celebrating in the The golfer is celebrating
successfully hit the golf ball from a sand bunker because he made a hole-in-one. video because he is because he hit the ball into
onto the green and possibly close to the hole… A hole-in-one is … successfully teeing off and … hole on his first attempt.
GPT4(Vision) Gemini-Pro-V Video-LLaVA LLaMA-VID MovieChat TimeChat
Figure2: WeobservethatmostVideo-LMMsstruggletoreasonovercomplexvideos(rows1-3)andexhibit
weakrobustnessandrectificationcapabilitieswhenpromptedtogenerateanswersforuserquestionsthatcan
sometimesbeconfusing(row4).TheQApairsinComprehensiveVideoReasoningandRobustnessEvaluation
Suite(CVRR-ES)benchmarkassesstheperformanceofVideo-LMMsbeyondgeneralvideocomprehension.
Basedonouranalysis,weobservethatstandardpromptingofVideo-LMMsstrugglesinsteering
theirfocusforcomplexvideounderstanding. Additionally,theirlimitationsinreasoningandrobust
videounderstandingofreal-worldscenariosaredominantlydrivenbythequalityoftextualinputs
(i.e., user questions). Based on these insights, we develop a training-free Dual-Step Contextual
Prompting(DSCP)technique,whicheffectivelysteersthemodel’sbehaviorduringinferencetoelicit
video-specificreasoningandimprovedrobustnesswithinVideo-LMMs. WithDSCP,Video-LMMs
showsubstantialimprovementsonourbenchmark,suggestingthepotentialofpromptingtechniques
forVideo-LMMs. Ourmaincontributionscanbesummarisedasfollows:
• WepresenttheComplexVideoRobustnessandReasoningEvaluationsuite(CVRR-ES),
aVideoQuestionAnsweringbenchmarkdesignedtoassessthereasoningandrobustness
capabilitiesofVideo-LMMsacross11diverseworld-centriccomplexvideodimensions.
• We comprehensively evaluate both open-source and closed-source Video-LMMs on the
CVRR-ESbenchmarkandfindthatmostmodelsexhibitweakperformance,highlighting
theirlimitedreasoningincomplexvideosandlackofrobustnesstowardsusertextqueries.
• WeconductextensiveanalysisandformulateimportantconclusionsaboutVideo-LMMs
basedontheirfailurecasesandperformanceontheCVRR-ESbenchmark. Ourfindings
providevaluableinsightsforbuildingthenextgenerationofhuman-centricAIsystemswith
improvedrobustnessandreasoningcapabilities.
• ToimproveVideo-LMMs’reasoningandrobustnessabilities,weformulateamodel-agnostic
andtraining-freepromptingtechniquethateffectivelyenhancestheirperformance.
32 RelatedWorks
VideoLargeMulti-modalmodels(Video-LMMs). Video-LMMs[Linetal.,2023,Lietal.,2023d,
Zhang et al., 2023] are advanced visual chatbots capable of performing a wide range of video
understandingtasks,includingvideocomprehensionandcaptioning,videoquestion-answering,and
actiongrounding. Thesemodelsacceptbothvideoandtextualinputsandgeneratetextualresponses.
Fromanarchitecturalperspective, Video-LMMstypicallycombinepre-trainedvisionbackbones
[Radfordetal.,2021,Fangetal.,2023,Wangetal.,2022b]withlargelanguagemodels[Touvron
etal.,2023,Zhengetal.,2023]usingconnectormodulessuchasMLPadapters,Q-former[Daietal.,
2023],andgatedattention[Alayracetal.,2022].VideoChat[Lietal.,2023b]andVideoChat-GPT[Li
etal.,2023d]presentedinitialopen-sourceeffortsinthisdirectionandweretrainedwithtwostages
ofalignmentandvideo-instructionfollowingobjectives. Recently,moreadvancedVideo-LMMs
haveemergedinthefield,withsomemodelsfocusingonimprovingmodelarchitectures[Lietal.,
2023d], expandingtonewtasks[Munasingheetal.,2023], andenablingsupportforlongvideos
[Songetal.,2023,Renetal.,2023]. Inthiswork,weaimtodevelopacomprehensivebenchmarking
evaluationframeworktoassessthereasoningandrobustnesscapabilitiesofVideo-LMMsanddevelop
atraining-freepromptingtechniquetoimprovetheirperformanceonthesefronts.
BenchmarkingVideo-LMMs. WiththegrowingnumberofVideo-LMMsemergingintheresearch
community,severalworkshavepresentedevaluationframeworkstoassessandquantifythesemodels
for benchmarking and analysis purposes. SEED-Bench [Li et al., 2023a] evaluates the visual
capabilities inboth imageand Video-LMMs across12 unique dimensions. MV-Bench [Liet al.,
2023c]curates20challengingvideotaskstoevaluatespatialandtemporalunderstandingofVideo-
LMMs. Video-ChatGPT[Maazetal.,2023]developsaquantitativeevaluationframeworktoassess
modelunderstandingacrossfiveaspectsofgeneralvideocomprehension,suchasthecorrectnessand
consistencyofmodelcaptions. Whiletheseevaluationframeworksprovideeffectiveinsights,their
assessmentsdonotextendbeyondgeneralvideo-comprehensionmetricstomoreadvancedaspectsof
reasoningandrobustness,particularlyforreal-worldcontextcases. Incontrast,ourworkfocuseson
providingacomplexvideoreasoningandrobustnessbenchmarkacross11diversereal-world-centric
evaluationtypesandoffersamorethoroughassessmentofVideo-LMMsinpracticalapplications.
Training-freePromptingTechniques. Steeringmodelbehavioratinferencetimeusingprompting
hasbecomeacommonparadigmintheNLPdomain. Prompting[Weietal.,2022b,Wangetal.,
2022a]referstothesetofinstructionsgivenasaprefixtothelanguagemodeltobetteralignmodel
responseswithhumanintentwithouttheneedfortask-specificfine-tuning. Promptingtechniques
canbeassimpleasasinglesentence(e.g., "Let’sthinkstepbystep")suchaszero-shotchainof
thought [Wei et al., 2022b] prompting, to more detailed techniques such as combining chain-of-
thoughtpromptingwithfew-shotlearning[Brownetal.,2020]andself-consistencychainofthought
prompting[Wangetal.,2022a]. Surprisingly,training-freepromptingtechniquesforVideoLarge
Multi-modal Models (Video-LMMs) have been minimally explored. In this work, we develop a
dual-steppromptingtechniquebasedonprincipledpromptinstructionsspecificallydesignedtosteer
themodel’sbehaviorforimprovedreasoningandrobustnessovercomplexvideos.
3 ComplexVideoReasoningandRobustnessEvaluationSuite
AsVideo-LMMsaretouchingnewreal-worldapplications,itisessentialtoensurerobustperformance
touserinputs,comprehendthevisualworld,andexhibithuman-likereasoningcapabilities. Inthis
work, our goal is to establish a comprehensive benchmark that specifically assess the robustness
andreasoningcapabilitiesofVideo-LMMsinavarietyofcomplexandcontextualvideoscovering
diversescenarios. Tothisend,wepresentComplexVideoReasoningandRobustnessEvaluation
Suite(CVRR-ES).WefirstprovideaholisticoverviewofCVRR-ESbenchmarkbelowanddetailthe
videoevaluationdimensionsinSec. 3.1. Subsequently,wepresenttheCVRR-EScreationprocessin
Sec. 3.2. WeprovidedetailsonthedatasetqualityandhumanevaluationinAppendixB.
OverviewofCVRR-ESBenchmark. CVRR-ESencompassesevaluationdimensionsthatcover
diversevideocategoriesrelatedtoreal-worldscenarios,rangingfromcontext-dependent(e.g.,social,
emotional)categoriestovideotypesthatoftentakeplaceinthewild(e.g., anomalousactivities).
Specifically,wehavecompiled11videoevaluationdimensionsandcurated2,400high-qualityopen-
endedquestion-answer(QA)pairs,spanning217high-qualityvideos. Theaveragevideoduration
is22.3seconds,withmaximumandminimumdurationsof183and2seconds,respectively. InFig.
4Figure3: CVRR-ESBenchmarkStatistics. Left: Frequencydistributionofthetypeofquestions. Right:
Illustrationofthemostfrequentkeywordsintheanswer-setofCVRR-ESbenchmark.
3 (left), we quantify the distribution of different question types present in our benchmark. This
diversesetofquestionsaimstocomprehensivelycapturethemodel’sansweringcapabilitiesbased
onreasoningandrobustnesscriteria. Weshowthewordcloudplotbasedonthefrequencyofkey
wordsintheanswersetofCVRR-ESinFig. 3(right). Thefrequentwordscorrespondtoobjectsand
attributeswithwhichVideo-LMMscouldmostlikelyinteractwhendeployedinpracticalscenarios.
3.1 CVRR-ESVideoCategorydefinitions.
ToassesstherobustnessandreasoningcapabilitiesofVideo-LMMsintheCVRR-ESbenchmark,
wecarefullycurate11diversebenchmarkevaluationcategories. AsshowninFig. 1(left), these
categoriesencompassawiderangeofreal-worldcomplexandcontextualvideoswithineachcategory.
Below,wedefineeachvideoevaluationdimensionoftheCVRR-ESbenchmarkindetail.
1)Multipleactionsinasinglevideo. Thiscategoryincludesvideosthatcontainmultipleactivities
withinasinglevideo. Thenumberofactivitiesvariesfrom2to4inthesevideos,mostlyfeaturing
humansperformingmultipleactivities.WecurateQApairsinthiscategoryaimingtoidentifywhether
themodelcanreasonoverchallengingquestionsconcerningmultipleactionsandunderstandthe
interrelationbetweendifferentactionswithinavideo.
2)Fine-grainedactionunderstanding. Wegathervideosampleswithfine-grainedactions. These
actionsencompassvariousfine-grainedactivitiesperformedbyhumans,includingpushing,opening,
closing,spreading,sitting,etc. Thiscategorypresentsachallengetothemodel’scomprehensionof
subtleandfine-grainedactionsthroughcarefullycraftedquestions.
3)Partialactions. BasedonourobservationsthatVideo-LMMspredominantlygeneratecontentthat
maybecontextuallyrelevantandlikelytoco-occurwiththedepictedsceneinthevideo,wecompile
videosfeaturingactionsthathaveahighprobabilityofbeingfollowedbysubsequentactionsbutare
notexecutedinthevideo. Forinstance,anactionsuchascrackinganegginakitchensettingoften
anticipatesthesubsequentactionoffrying/cookingtheegg.
4)Timeorderunderstanding. Accuratelyrecognizingthetemporalsequenceofactivitiesinvideos
iscrucialfordistinguishingbetweenatomicactions,suchaspushingandpulling. Wecollectvideos
offine-grainedactionsoccurringinaparticulartemporaldirectionandcuratechallengingquestions.
5) Non-existent actions with existent scene depictions. This category examines the model’s
robustnessandreasoningbehaviorinscenarioswhereweintroducenon-existentactivitiesintothe
videowithoutalteringthephysicalandspatialscenesorenvironmentaldetailsinit.
6)Non-existentactionswithnon-existentscenedepictions. Inthisevaluationcategory,wemake
theQAtaskmorechallengingbycreatingquestionsthatincludebothnon-existentactivitiesand
non-existentscenecomprehension. Non-existentscenecomprehensioninvolveschangingtheobjects,
attributes of objects, and background scene description. This evaluates the model’s reliability to
correctmisleadingquestionsandavoidgeneratingimaginarycontent.
7)Continuityandobjectinstancecount. Thiscategorycontainsvideos(bothrealandsimulations)
designedtotestthemodels’abilitytoaccuratelyrecognizethenumberofinstancesofobjects,people,
etc.,anddistinguishbetweenexistingobjectsandnewonesintroducedinthesamevideoscene.
8)Unusualandphysicallyanomalousactivities. Thiscategoryconsistsofvideoswithunconven-
tionalactivitiesandphysicalphenomenathatseeminglydefythelawsofphysics. Wemeticulously
5collectrelevantvideosfromvarioussourcesontheinternet,focusingoncapturingunusualactivities
suchasapersonfloatingintheairordrivingamotorbikeonarunningriver. Webelievethatassessing
Video-LMMsinsuchscenariosiscrucial,asitallowsustodeterminewhethertheycangeneralizeto
understandactionsinout-of-distributionvideosthatcanoccurinpracticalsituations.
9)Interpretationofsocialcontext. Intherealworld,humanactionsareofteninfluencedbysocial
contextintheirsurroundings. Forinstance,apersonmightbehelpinganelderlyindividualcrossthe
road. ThiscategoryevaluatesVideo-LMMsonsuchscenariostodeterminetheirabilitytoaccurately
infertherationalebehindactionsbasedonthedepictedsocialcontext. Wegatherdiversevideosfrom
theinternetandcreatechallengingquestionsthatencompassthesocialcontextdimension.
10)Understandingofemotionalcontext. Similartosocialcontext,humanscanaccuratelyunder-
standandinterpreteachother’sactionsbyconsideringtheemotionalcontext. Forexample,aperson
beingemotionallymovedandcryinginagatheringcouldbeahappymomentifitisonestemming
fromsuccess/joy. Wecollectvideosandcuratechallengingreasoningquestionsaimedatrecognizing
thenatureofactionssolelybasedonemotionalcontextforevaluatingVideo-LMMs.
11)Interpretationofvisualcontext. Thisdimensionfocusesonassessingthemodel’sreasoning
abilitiestorecognizetheactionsbyleveragingtheoverallvisualcontextualcuesinthevideo. We
curatespecificvideoscontainingactionswhereactivityidentificationandreasoningrequirevisual
contextualcues. Forexample,toidentifythenumberofpeoplepresentbasedonthepresenceof
shadows,onemustutilizethevisualcontextfromtheshadowstoreasonaboutthequestion.
QualitativeExamples. Fig.2showsexamplesofcollectedvideosfortheCVRR-ESbenchmark. The
curatedvideosarecarefullyselectedtobediverseandcontainrichspatio-temporalcontent,aligned
withtheproposedvideoevaluationdimensions.
3.2 BuildingCVRR-ESBenchmark
After defining the video evaluation dimensions, we now proceed toward building the CVRR-ES
benchmarkwhichconsistsofthreestages. Wepresenteachstageindetailbelow.
Stage1: DatacollectionandAnnotation. Wefirstcollecthigh-qualityvideosandannotateeach
video using human assistance. To ensure that each evaluation dimension captures the relevant
attributesandinformation,wemeticulouslyselectvideosthatarerepresentativeofspecificcharac-
teristicsassociatedwiththatdimension. Acrossthe11dimensions,214uniquevideosareselected
for the benchmark with around 20 videos per evaluation category. Around 60% of these videos
arecollectedfrompublicacademicdatasets. Tointroducediversityinthebenchmarkdistribution,
weincorporatevideosamplesfrommultipleacademicdatasetsincludingSomething-Something-v2
[Goyal et al., 2017], CATER [Girdhar and Ramanan, 2020], Charades [Sigurdsson et al., 2016],
ActivityNet[CabaHeilbronetal.,2015],HMDB51[Kuehneetal.,2011],YFCC100M[Thomee
etal.,2016]. Theremaining40%ofvideosarecollectedfromtheinternet.
Followingthevideocollectionprocess,twoexperiencedhumanannotatorsareassignedtogenerate
captionsforeachvideo. Forvideoswhereinitialcaptionsormetadataareavailablefromacademic
datasets,thecaptionsaregeneratedbytheannotatorsbasedonthem. Forvideoscollectedfromthe
internet,captionsareentirelygeneratedbyhumanannotators. Toensureconsistencyandhighquality,
weprovideannotationinstructionstoannotators,whogeneratecaptionsaccordingly. Personalized
annotationguidelinesareusedforeachvideocategory. RefertoadditionaldetailsinAppendixB.
Stage2:Question-AnswerGeneration.Thefirstchallengeistoselectanevaluationsettingtoassess
Video-LMMs. Humans typically engage in free-form conversation to interact with each other in
day-to-daylife. Inspiredbythis,weaimtosimulateasimilarstyleofinteractionwithVideo-LMMs
bycuratingopen-endedQApairstoevaluatethesemodelsforrobustnessandreasoning. Wefeed
detailedground-truthvideocaptionstoGPT-3.5LLM,whichareutilizedtogenerateopen-ended
questionscoveringbothreasoningandrobustnessaspects.
ReasoningQApairs: WithVideo-LMMsbeginningtointeractmoredirectlywithhumansinour
lives, it’scrucialtovalidatethereasoningabilitiesofVideo-LMMsformorereliableHuman-AI
interaction.WhenevaluatingthereasoningcapabilitiesofVideo-LMMs,weaimtodeterminewhether
thesemodelscanunderstandtheinputvideonotonlybyanalyzingspatialcontentbutalsobygrasping
theunderlyingrationalebehindtheoccurringactivitiesandtheirrelationshipswiththesurrounding
context. Thisinvolvescreatingquestionsthatgobeyondsimplevideocomprehensionandscene
6descriptionandrequirethemodeltoengageincomplexlogicalinference,contextualunderstanding,
andreasoningaboutcounterfactualandhypotheticalscenarios.
RobustnessQApairs: InadditiontoevaluatingthereasoningcapabilitiesofLLMs,itisimportant
toassessVideo-LMMstoensuretheirrobustandresponsibleperformanceinreal-worldscenarios. In
thecontextofVideo-LMMs,robustnesscanbeevaluatedfrombothvisual(videoinput)andtextual
interfaces. Our focus in this work lies on textual interface robustness by particularly testing the
model’scomprehensionwhenposedwithmisleadingorconfusingquestions. Thisscenariomirrors
realisticsituationswhereusers,basedontheirexpertiselevels,mayposeirrelevant,misleading,or
confusingquestions. Itiscrucialformodelstodemonstratereliabilityandrobustnessinhandling
suchqueriesandavoidgeneratingunrealorhallucinatedcontentforinputvideos.
WecuratespecificpromptsforeachevaluationdimensiontoinstructLLMingeneratingQApairs.
ExamplepromptsusedasaninstructiontoLLMsforcuratingQApairsforrobustnessandreasoning
aspectsareprovidedinFig. 14intheAppendix.
Stage 3: QA Pairs Filtration. After generating QA pairs, a manual filtration step is employed,
with human assistance to verify each generated QA pair. Approximately 30% of the QA pairs
generatedbyGPT-3.5arefoundtobenoisy, containingquestionsthatareunrelatedtothevideo
evaluationdimensionsorunanswerablebasedontheprovidedground-truthcaptions. Additionally,
manyquestionscontainanswerswithinthequestionitself. Therefore,anexhaustivefilteringprocess
isconductedwhichinvolvesQArectificationandremovingthosesampleswhicharenotrelevantto
thevideoorevaluationtype. Thisprocessresultsinafinalsetof2400high-qualityQApairsforthe
CVRR-ESbenchmark. ExamplesofQApairsareshowninTab. 4intheAppendix.
Stage4: EvaluationProcedure. Previousmethodsintheliterature[Maazetal.,2023,Caietal.,
2023,Liuetal.,2023a,Qianetal.,2024]haveexploredusingLLMmodelsasjudgesforquantifying
resultsinopen-endedQAbenchmarks. WeadoptasimilarapproachandinstructLLMstoactas
teacherstoassessthecorrectnessofpredictedresponsesfromVideo-LMMscomparedtoground-truth
answers. Wegenerateopen-endedpredictionsfromVideo-LMMsbyprovidingvideo-questionpairs
asinputsandthenpresentthemodelpredictionsandtheircorrespondingground-truthresponsesto
theLLMJudgealongsidetheevaluationprompt. TheJudgedetermineswhetherthepredictionis
correctorincorrectthroughabinaryjudgment,assignsascorefrom1to5representingthequalityof
theprediction,andprovidesareasoningtoexplainitsdecision. OurablativeanalysisintheAppendix.
Ddemonstratesthatreasoning-constrainedLLM-basedevaluationalignswellwithhuman-based
judgment. TheevaluationpromptisshowninFig. 13intheAppendixA.
4 Dual-StepContextualPromptingforVideo-LMMs.
Giventheirwide-scalepotentialinpracticaldownstreamapplications,newVideo-LMMsarefre-
quentlyintroducedbytheresearchcommunity. DespitetheavailabilityofnumerousVideo-LMMs,
themajorityofthemaretrainedusingonlypositiveexamplesandvideo-conversationaltemplates
thatareprimarilylimitedtotaskssuchasvideo-captioningandvideoquestionanswering. Thisleads
tohighlyover-affirmativebehaviorandalackofself-rectificationabilitiesinthesemodels(Sec. 5.4).
Additionally,thetemplateshaveminimalfocusonen-
Dual Step Contextual Prompting for Video-LMMs
hancingreasoningandrobustnesscapabilitiesthrough
Retrieving Contextual reasoning information (Step 1)
reasoning-basedinstruction-tuningpairs,resultingin
weakperformanceofsuchmodelsagainstrobustness As an intelligent video comprehension model, focus on these guidelines:
1. Differentiate recurring objects, count accurately, and identify
andreasoningQAevaluationsintheCVRR-ESbench- movements and poses.
2. Understand directional movements and temporal order.
mark. Furthermore,curatingreasoning-basedinstruc- 3. Pay attention to fine-grained actions with precision.
4. Assess incomplete actions without assuming completion.
tionfine-tuningdatasetsrequiresmeticulousdatacura- 5. Detect emotional, social, and visual cues.
6. Capture and analyze all relevant actions.
tionsteps,andretrainingthesemodelsiscomputation- 7. Identify unusual actions accurately.
allyexpensive[Lietal.,2023d,Renetal.,2023]. 8. Disagree with incorrect information given in question.
9. If you do not find the evidence in the frames, you can give a definite
answer by assuming that the asked action/attribute is not present.
Alternatively, training-free prompting techniques in 10. Provide to the point and concise response.
Now, proceed with answering the following question faithfully while
NLPliteraturehaveshowneffectivenessinelicitingrea- keeping above guidelines in mind:
Question: What is happening in the video?
soningabilitiesinLLMssuchaschainofthoughtand
Context conditioned question-answering (Step 2)
self-consistencyprompting[Weietal.,2022b,Wang
et al., 2022a]. Inspired by these approaches, we in- Context for the given video is: {step 1 response}. Now answer a
question truthfully based on the video and the provided context.
troduceapromptingtechniquecalledDualStepCon- Question: {User question}
Figure4: Principledpromptinstructionsin
textualPrompting(DSCP),whichaimstosteerVideo-
ourDSCPmethodforimprovingreasoning
LMMfocusforenhancedreasoningwhilesimultane-
androbustnessinVideo-LMMs.
ously encouraging the models to provide robust and
7How many different wigs I can see the young girl wearing a black wig, The young girl tried on three The young girl tried on three different
with different colors did the and a brunette wig is in the background. different wigs with different wigs with different colors before settling
young girl try before settling The girl is wearing a pink shirt in the colors before settling on the black on the black one.
on the black one? beginning of the video one.
Improved video reasoning and robustness with our DSCP Method
I s hca on wn to ht e g div ie ff ea rn e a nc t c wu ir ga st e th a an ts tw he er y a os u t nh ge g v ii rd l e tro ie d do .es not T n oh u ne m t hv b ei ed bre lo o a f cd w ko oie g ns s e n tho et p yr oo uv nid ge g i in rlf o trr im eda t bi eo fn o a reb o seu tt t lt ih ne g T t th rh ie ee d nv u bid m ee fbo oe rd ero o se efs td tn li ifo nft e g rp e or n no t v t i w hd eie g bsin lt af ho ce krm y oo na ut ei n .o gn ga ib rlo ut
Is the atmosphere in the Yes, the atmosphere in the dressing room is The video does not provide any The video does not provide any
dressing room tense and tense and uncomfortable as the girl information about the information about the atmosphere in the
uncomfortable as the girl receives her new hair. atmosphere in the dressing room. dressing room. However, it is possible …
receives her new hair? Improved video reasoning and robustness with our DSCP Method No, the atmosphere in the dressing room
… un, c ob mut f oth re ta v bi ld ee oo r d to ene ss e n wot h s ilu eg rg ee cs et i vth ina gt t hh ee r g ni er wl is h air. N o inro d u, i cnt ah c te o in ma gt f m o thro ats a tp b sh l hee e.r Te ish i en e n gt jh i ore yl iid nsr gle a ts u hs gi en h g ein xr g po eao rnm ied n i ss cm n eo .ilit n t ge n …se , i w ts h on em o wt a ot ne m bn ars nue s ia h nn i n td hg u e t n h cc heo a hm ira f i io r s r i st s ma sb m ill ie ni. l gi nT ah g s,e a …nd
Can we see visitors petting the Sure, in this video we can see a baby Yes, we can see visitors petting Yes, we can see visitors petting the white
w bah ri st ?e tiger through the cage p ara en d ala s oin osi td he e ra … cage with bars. There t bh ae rs w inh i tt he e t i vg ie dr e t oh .rough the cage tiger through the cage bars.
Improved video reasoning and robustness with our DSCP Method
No, we do not see visitors petting the white tiger No, we cannot see visitors petting the white tiger No, we cannot see visitors petting the white
through the cage bars in the video. through the cage bars in the video. tiger through the cage bars in the video.
D t vo eo o he l iss c ltt eoh i e nr e c tph hai em ir v p t ih da en e oz ze ?oe o u se Y ue ss in, gth te o oc lh si m top fa ixn tz he ee zin o oth ve e z ho ico le is . seen Y a ve i dts o e, o ot lh . te o c fh ixim thp ea n veze he ic i ls e s inee tn h eu sing Y toe s r, e t ph ae i rc th him e zp oa on z ve ee h ii cs l ese ie nn t hu esi vn ig d ea ot .ool
Improved video reasoning and robustness with our DSCP Method No, the chimpanzee does not use tools to
The chimpanzee does not use tools to repair the zoo No, the chimpanzee does not use tools to repair the repair the zoo vehicle in the video. … the
vehicle in the video. It can be seen driving the car … zoo vehicle in the video. chimpanzee driving the car …
Video-LLaVA LLaMA-VID MovieChat
Figure5: QualitativeresultsofDSCPpromptingmethod. UsingourDSCPapproach,Video-LMMs
demonstrateenhancedrobustnessandreasoningcapabilitiesovercomplexvideos.
grounded answers. DSCP is a two-step prompting method that 1) ensures that the model com-
prehendsthevideowhilereasoningovercrucialaspectsofcomplexvideounderstandingsuchas
contextualinformationanddecodingthecomplexrelationshipsbetweenobjectsandmotions,etc.,
and2)encouragesrobustnessbygeneratingtheresponseagainstthequestionwhileconditioningboth
onvideoandthecontextretrievedinthefirststep. BelowwediscusseachstepofDSCPindetail.
Step 1: Reasoning over the video. We first guide Video-LMMs using principled prompts to
interpretvideocontentfromareasoningperspective. AsshowninFig. 4(inblue),weformulateten
principledreasoning-basedinstructionsforprompting,P ,whichdirectsVideo-LMMstonot
reason
onlycomprehendthegeneralvideocontentbutalsosteersthemtoreasonovertherationalebehind
occurringactivitiesandtheirrelationshipswiththesurroundingcontext. Thesepromptinstructions
includespecificconsiderationslikecontextualpriors,thetemporalorderofactions,instancecount,
andattributes. Additionally,thepromptingtechniqueincorporatesinstructionstoensureconciseness
andfactuality, aimingtomitigatehallucinations. GivenaVideo-LMMF andinputvideoV, we
retrievecontextualreasoninginformationI byprovidingprincipledreasoningpromptP
context reason
alongwiththevideototheLMM,I =F(P |V).Thecontextualinformationisutilizedin
context reason
thesecondstepofDSCPtogenerateamoregroundedresponsetotheuserquestion.
Step2: Contextconditionedquestionanswering. Asdiscussedearlier,Video-LMMsareprimarily
trainedwithpositiveexamplestoanswerquestions,withlimitedemphasisonreasoningandrobust-
nessaspects. Consequently, enablingdirectinteractionofVideo-LMMswithusersinreal-world
scenarioscanresultinundesiredresponseswhentheuserquestionisconfusinganddeceivingdue
totheirextremeover-affirmativebehavior. Toaddressthesechallenges,weproposeincorporating
an additional inference step in Video-LMMs before answering the user’s question. We note that
Video-LMMsoftenpossessfactualknowledgeaboutthevideocontentbutmaybecomedistracted
andproducehallucinationswhenpromptedwithconfusingormisleadingquestions(moredetailsin
AppendixC).Specifically,wedeviseapromptingmethodthatconditionsthemodeltofirstcompre-
hendthevideoindetailwithoutattendingtotheuserquestion,therebyeliminatingtheinfluenceof
thequestion. ThecomplexvideocomprehensioninformationreferstoI formulatedinstep1.
context
Subsequently,weposetheuserquestioninthesecondstepusingpromptP whichcombinesuser
user
questionandthecontextualreasoninginformation(Fig. 4,ingreen)whileconditioningthemodel
onboththevideoandthecontextualreasoninginformationI . Concretely,Final response=
context
F(P |V),whereP =[question;I ].
user user context
8Table 2: Evaluation results of Video LLMs across various video-evaluation categories on the CVRR-ES
benchmark. Wepresentresultsforbothopen-sourceandclosed-sourcemodels,alongsidehumanevaluation
resultswhichservesastheupperboundonthebenchmark.
BenchmarkCategory
Video-LLaMA-2
VideoChat
Video-ChatGPT
Video-LLaVA
MovieChat
LLaMA-VID
TimeChat
Gemini-VPro
GPT4V Human
MultipleActionsin
16.98 23.90 27.67 15.72 12.58 17.92 28.30 43.08 57.55 93.40
singlevideo.
Fine-grainedaction
29.57 33.48 26.96 25.22 23.48 26.09 39.13 51.61 77.39 95.65
understanding.
Partial
24.76 33.01 22.82 13.59 21.36 14.56 49.51 67.48 73.79 98.54
actions.
Timeorder
16.45 31.58 27.63 21.05 16.45 19.74 34.21 45.39 57.89 97.37
understanding.
Non-existentactionswith
10.14 15.22 23.19 5.07 5.07 2.90 23.19 57.25 71.01 97.10
existentscene.
Non-existentactionswith
13.19 14.58 17.36 3.47 11.81 6.94 13.89 49.64 75.00 100.00
non-existentscene.
ContinuityandObject
28.25 24.29 28.41 21.47 19.77 24.86 34.46 36.16 62.71 96.49
instanceCount.
UnusualandPhysically
18.95 18.42 18.95 15.79 17.89 16.32 27.37 60.00 74.74 96.84
Anomalousactivities.
Interpretationof
25.00 31.07 32.50 18.93 17.14 13.93 39.29 64.29 79.64 97.51
socialcontext.
Understandingof
21.92 23.63 21.23 15.07 13.70 14.73 27.40 47.26 66.44 95.55
emotionalcontext.
Interpretationof
32.60 34.43 27.84 19.78 21.25 23.08 45.05 63.00 82.42 94.87
visualcontext.
Average 21.62 25.78 24.96 15.92 16.41 16.46 32.89 53.20 70.78 96.67
Intuitively,thefactualcontentgeneratedinthefirststepwillguidethemodeltowardsarobustresponse
inthesecondsteptoproducefactualandcorrectresponses,eveninthepresenceofnoisy/misleading
userquestions.WeillustratethequalitativeresultsoftheDSCPmethodinFig. 5.Thisapproachleads
toresponsesthatarebettergroundedwiththeactualvideocontentandarerobustagainstpotential
lesser-quality user queries. As we will later show, the DSCP technique effectively enhances the
performanceofVideo-LMMsontheCVRR-ESbenchmark.
5 EvaluationExperimentsonCVRR-ES.
Video-LMMs. Bothopen-sourceandclosed-sourcemodelsareselectedfortheevaluation. Among
theopen-sourcemodels, weevaluate7recentVideo-LMMs, includingVideo-LLaVA[Linetal.,
2023],TimeChat[Renetal.,2023],MovieChat[Songetal.,2023],LLaMA-ViD[Lietal.,2023d],
VideoChat [Li et al., 2023b] Video-ChatGPT [Maaz et al., 2023], and Video-LLaMA-2 [Zhang
etal.,2023]. Forevaluatingclosed-sourcemodels,weuseGemini-Pro-Vision[Google,2023]and
GPT-4V(vision)[OpenAI,2023]. RefertotheAppendixAforimplementationdetails.
5.1 MainExperimentsonCVRR-ES.
InTab. 2,wepresenttheevaluationresultsofVideo-LMMsonthe11dimensioncategoriesofthe
CVRR-ESbenchmark. Below,wepresentseveralkeyfindings.
OpenSourceVideo-LMMsstrugglesonCVRR-ESbenchmark. Allopen-sourceLMMsshow
inferiorperformanceacrossthedifferentevaluationdimensionsofCVRR-ES.Interestingly,someof
theearlierdevelopedopen-sourceVideo-LMMs,likeVideo-LLaMA,VideoChat,andVideo-ChatGPT,
exhibithigherperformancecomparedtomorerecentmodelssuchasVideo-LLaVA,MovieChat,and
LLaMA-VID.Overall,TimeChatachievesthehighestperformanceof32.89%averagedacrossthe11
evaluationdimensionsamongopen-sourceLMMs,followedbyVideoChatwithascoreof25.78%.
HumansrankhighestinCVRR-ESbenchmark. Humanstudiesachievethehighestperformance
ontheCVRR-ESbenchmark,withover95%accuracyacrossallevaluationdimensions. Furthermore,
theseresultssuggestthattheCVRR-ESQApairsareanswerableandsuitableforbenchmarking.
ClosedsourcemodelsperformcompetitivelyonCVRR-ES.AsshowninTab. 2,bothGeminiand
GPT4Vsurpasstheperformanceofopen-sourcemodelsandachievehighgainsacrossallevaluation
dimensions. ThecompetitiveresultsofGPT4VandGeminioncomplexvideoevaluationdimensions
suchaspartialactions,non-existentaction/scenedepiction,andcontext-dependentcategoriesshow
9Table 3: Prompting meth-
PromptingMethod VideoChatVideo-LLaVAMovieChatLLaMA-VIDTimeChat
ods. DSCPstage1usesonly
Standardprompting 25.78 15.92 16.41 16.46 32.89
ChainofThought(CoT)prompting 22.44 25.87 15.89 29.68 39.57 the principled instructions de-
signedinstep1, whileDSCP
DSCP(Stage1) 38.07 28.40 28.05 26.41 33.04
DSCP(Bothstages) 47.92 37.93 35.87 46.85 39.45 (Bothstages)usesthecomplete
dual-steppromptingtechnique.
thatthesemodelshaveamoresophisticatedunderstandingofthecomplexvisualcontentsofvideos
andhavestrongcapabilitiestorectifymisleadingandconfusinguserquestions. Overall,GTP4V
improvesoverGeminiby17.58%andprovidesanaverageaccuracyof70.78%onCVRR-ES.
5.2 EffectivenessofDSCPmethodforimprovingVideo-LMMsperformance
We next integrate DSCP technique with Video-
LMMsandpresentresultsontheCVRR-ESbench- Gemini-Pro +5.02
mark in Fig. 6. The results indicate that DSCP TimeChat +6.56
improvesthemodel’sperformancecomparedwith VideoChat +22.14
models that use standard prompting (i.e., using Video-ChatGPT +8.93
onlythequestionitself). Theseresultssuggestthat Video-LLaMA-2 +16.15
prompting techniques in Video-LMMs can better LLaMA-VID +30.39
guidemodelsforimprovedreasoningandrobustness. MovieChat +19.46
WithDSCP,initiallylow-performingVideo-LMMs
Video LLaVa +22.01
suchasVideo-LLaVa,MovieChat,andLLaMA-Vid
0 10 20 30 40 50 60
showmuchbetterrelativegainsandbecomecom- Accuracy % (averaged over 11 video dimensions)
Figure6: Video-LMMswithDSCPtechnique
petitivewithothermodels. Thehighestrelativegain
effectivelyimprovestheirperformance(gains
of184%isachievedbyLLaMA-ViD,whichmoves
areshowningreen)onCVRR-ESbenchmark.
from7thplaceintheleaderboardto2ndamongthe
open-sourcemodelsafterutilizingDSCPprompting. Weobservesimilaroverallpositivetrendsof
usingDSCPwithclosed-sourcemodelGemini,whichimprovesonthebenchmarkbyanabsolute
overallgainof5.02%. WeprovidemoredetailedresultscomparisonsinAppendixA.
5.3 Differentpromptingtechniques.
WestudythecontributionofeachstepofDSCPandcompareitwithchain-of-thoughtprompting
[Weietal.,2022b]. Theresultsforthetop5performingVideo-LMMsareshowninTab. 3. Chain-
of-thoughtpromptingimprovesoverthestandardpromptingtechniquein3outof5Video-LMMs,
suggestingthatpromptingtechniquesfromNLPliteraturecaneffectivelyguidemulti-modalVideo-
LMMstoenhancereasoningandrobustness. Next,weablateonthefirststepofDSCPprompting,
whichusestheprincipledinstructionsofDSCPstep1asaprefixalongsidetheactualuserquestion.
UsingthefirststeppromptingtechniqueofDSCPsubstantiallyimprovesmodelperformanceonall
Video-LMMs,suggestingtheeffectivenessoftheprincipledpromptinstructionsdesignedspecifically
forVideomodels.DSCPwithbothsteps,whichintegratesanadditionalthinkingstepintheprompting
step,furtherimprovestheresultsandprovidesthehighestresultson4outof5Video-LMMs.
5.4 MainfindingsandQualitativeResults
BasedontheresultsofVideo-LMMsonCVRR-ES,wedrawkeyfindingsandshowqualitativeresults.
TheseinsightscanserveasvaluableguidancefordevelopingthenextgenerationofVideo-LMMs,
aimingtomakethemmorerobustandreliablewhendeployedinreal-worldapplications.
ModelsexcellingatstandardVQAbenchmarksstruggleonCVRR-ESbenchmark. Ouranalysis
inSec. 5.1revealsthatthelatestopen-sourceVideo-LMMs,suchasVideo-LLaVA,MovieChat,and
LLaMA-VID,performlesseffectivelyontheCVRR-ESbenchmarkcomparedtoVideo-LMMsthat
wereintroducedearlierinthecommunity,suchasVideoChatandVideo-ChatGPT.Interestingly,the
samerecentmodelsdemonstratesuperiorperformanceongeneralvideocomprehensionbenchmarks
[Liuetal.,2023b]. ThisdiscrepancysuggeststhatcurrentVQAbenchmarks,likeActivityNet-QA
[Yu et al., 2019] and MSRVTT [Xu et al., 2017], do not adequately correlate with the complex
videoreasoningandrobustnessscenarioshighlightedinourbenchmark. Consequently, thisalso
indicatesthatmostnewerVideo-LMMsareheavilytrainedtoexcelongeneralvideocomprehension
benchmarkswhilereducingtheirgeneralizability,reasoning,androbustnesscapabilities.
Over-affirmativebehaviorofopen-sourceVideo-LMMs. Anotherimportantobservationabout
open-sourcemodelsistheirtendencytoexhibitexcessivelypositiveandaffirmativeresponses. As
showninFig. 7,open-sourceVideo-LMMsconsistentlyrespondwith"Yes,"evenwhenfacedwith
10
PCSD
htiw
sMML
oediVconfusingquestionsthatdescribenon-existentactionsandobjects.Thishighlightsthevulnerabilityof
thesemodelswheninteractingwithusersinreal-worldscenarios. InourCVRR-ESbenchmark,open-
sourcemodelsareparticularlyvulnerabletoourevaluationdimensionsof"Non-existentactionswith
theexistentscene"and"Non-existentactionswiththenon-existentscene"comparedtoclosed-source
models. Thesemodelslacknegationandself-rectificationcapabilities,especiallywhenusersprovide
misleadingorconfusingquestions. Weconjecturethatsuchbehaviorarisesduetotheabsenceof
negativeinstructiontuningpairsduringthetrainingofVideo-LMMs.
Tendencytowardsactivitycompletion. Mostopen-sourceVideo-LMMshaveshownweakperfor-
manceontheevaluationdimensionofpartialactionsinCVRR-ES,whichcontainsvideosfocusing
onincompleteoratomicactions. Tofurtheranalyzethemodels’behavior,weshowqualitativeresults
onsuchvideosinFig.8. Itcanbeobservedthatmostopen-sourcemodelstendtocompleteactions,
evenwhenonlypartoftheactionisprovidedinthevideo. Forinstance,Video-LLaVAstrugglesto
reasonoverthevideoanddescribesthemanaskickingthesoccerball,whiletheactioninthevideo
stopsatthepointofthemanplacinghisfootbesidetheball. Weobservesimilarbehaviorinother
Video-LMMs. Uponexaminingthefine-tuningstrategies[Maazetal.,2023,Liuetal.,2023b],we
findthatalmostallmodelsaretrainedonend-to-endactions-basedinstruction-tuningdata,causing
themtogeneratecompleteactiondescriptionsatinference. Thistendencyhighlightsthevulnerability
ofVideo-LMMsafterdeployment,asreal-worldscenariosofteninvolveatomic,sub-atomic,and
generalactionsalike. ToimprovetheperformanceofVideo-LMMs,itiscrucialtoincorporatediverse
actiontypesduringtraining,includingpartialandincompleteactions.
WeakGeneralizationtoextremeOODvideos. Theevaluationdimensionofunusualandphysically
anomalousactivitiesresemblesextremeout-of-distributionvideoexamples. Withtheexceptionof
GPT4VandGemini,Video-LMMsstrugglewiththisdimension,indicatingweakgeneralizability
towardsOODvideoscontainingthecoexistenceofunusualobjectsandactivitiesthatareextremely
rareintypicalvideos. Forinstance,Video-LLaVAinFig. 9describesapersonfallingonthestreet,
whilethevideoactuallyshowsthepersonperforminganopticalillusion. Toberesponsiblydeployed
in real-world applications, where OOD actions occur more frequently, Video-LMMs need to be
trained to perform more robustly on OOD samples. This may involve incorporating diverse and
atypicalexamplesinthetrainingdatatoimprovethemodel’sabilitytohandleunusualsituations.
Limitedunderstandingoftemporalorderincomplexvideos. TheCVRR-ESbenchmarkresults
showthatVideo-LMMsperformrelativelybetteronthefine-grainedactiondimensioncomparedto
thetime-orderunderstandingdimension. Whilethesemodelscanaccuratelyidentifyfine-grained
actions,theystrugglewithcomprehendingthecorrecttemporalorderoftheseactionswithinavideo.
Thislimitationcanleadtomisinterpretationsoftheunderlyinginformation. Wepresentfailurecases
relatedtothisdimensioninFig. 10. Forbuildingmoreadvancedworld-centricVideo-LMMs,itis
crucialtoenhancetheirabilitytoprocessandinterpreteventsequencesaccurately.
Video-LMMsstrugglesinunderstandingtheemotionalandsocialcontext. Formorereliable
interactionbetweenVideo-LMMsandhumansinpracticalscenarios,thesemodelsshouldcomprehend
thespatio-temporalsceneswithsocialandcontextualreasoningcapabilitiessimilartohumans. The
lowerperformanceofVideo-LMMsonsocialandemotionalcontextualdimensionsinCVRR-ES
highlightstheirlimitationsandlackofunderstandingofscenesbasedoncontextualcues.Forinstance,
asshowninFig. 11(bottom),GPT-4Vstrugglestocomprehendascenewhereaworkerisattempting
topreventshoesfromgettingwetduetotherainbymovingthemundertheshade. Instead,GPT-4V
providesaresponsethatcontradictsthesocialcuespresentinthevideo.
6 Conclusion
GiventheexpandingroleofVideo-LMMsinpracticalworld-centricapplications,itisvitaltoensure
thatthesemodelsperformrobustlyandexhibithuman-likereasoningandinteractioncapabilities
acrossvariouscomplexandreal-worldcontexts.Inthiswork,wepresenttheCVRR-ESbenchmarkfor
Video-LMMs,aimingtoevaluateVideo-LMMsontheseveryfronts. Throughextensiveevaluations,
wefindthatVideo-LMMs,especiallyopen-sourceones,exhibitlimitedrobustnessandreasoning
capabilitiesovercomplexvideosinvolvingreal-worldcontexts. Basedonouranalysis,weformulate
atraining-freepromptingtechniquethateffectivelyimprovestheperformanceofVideo-LMMsacross
variousevaluationdimensionsoftheCVRR-ESbenchmark. Furthermore,weanalyzeandinvestigate
thefailurecasesofVideo-LMMsontheCVRR-ESbenchmarkanddeduceseveralimportantfindings.
We hope that the CVRR-ES benchmark, accompanied by our extensive analysis, will contribute
towardsbuildingthenextgenerationofadvancedworld-centricvideounderstandingmodels.
11A bare c kt gh re or ue n o dt h we hr i lp el a ty he er s p ep rla soyi nn g p ii cn k sth ue p the ball? Y pe las y, it nh ge ir ne ta hr ee bo ath cke gr rp ol uay ne dr .s .. Y p the la es y, pit n eh g re sir one n ta h pr ee ic bo ka sth c uke pgr r tp o hl ua ey n be d ar s w ll s …hh ilo ew n Y pe las y, it nh ge ir ne ta hr ee bo ath cke gr rp ol uay ne dr s .. .shown
N bao c, kt gh re or ue n a dr e w n ho il eo t th he er p p el ra sy oe nrs p s ich ko sw un p p tl ha ey i bn ag l li .n the N plo a, y t inh ge r ie n a thre e n bo a co kt gh re or u p nl da .y .ers shown Y b cae a ns c, k bt gh ere o sr u ee n
e
i d ns w …a hfo eo retb oa tll h f eie r l pd l ain y et rh se yes.
Does the video feature a hand Yes, the video features the hand Yes, the video shows the hand Yes, the video shows the hand pouring
filling the cup with liquid? filling the cup with liquid. filling the cup with liquid. water into the cup and then pouring it out.
The video does not feature the hand filling the cup
with liquid. a liquid is visible on the surface … liquid No. Yes, the video shows a man using a plastic Yes.
was spilled from the cup rather than poured into it. bottle to fill a red glass cup with a pink liquid.
GPT4(Vision) Gemini-Pro-V Video-ChatGPT Video-LLaVA LLaMA-VID MovieChat TimeChat
Figure7: Overaffirmativebehaviour.Mostopen-sourceVideo-LMMsexhibitoverlyaffirmativebehaviorby
consistentlyagreeingwithuserquestions,evenwhenthequestionsareconfusingorinaccurate.
D cao re es
s
st ih ne
g
w it?oman mount the camel after Y cae ms, eth
l
e
af
w teo
r
m caa rn
e
sm sio nu gn it ts
.
the Y w ae ro os m,
u
a naf dnte tmr
h
c eoa u pr ne ets nss
.
i in t g a nth de r ic da em s e itl , the Y we os m, a af nte mr c oa ur ne ts ss i in t.g the camel, the
… woman is seen interacting with the camel, where she No, the woman does not mount the Yes, at the end of the video, the
a thp ep e imar as g t eo s b she oc war e hs es ri n mg o o ur n p tia nt gti n thg e i t c. aH mo ew l.ever, none of camel after caressing it. w cao rem sa sin n i gs im t.ounting the camel after Yes.
… a woman is seen standing on a In the video, a man is seen kicking a
W vidh ea ot ?is happening in the I bn
a
t llh oe
n
v aid fe ieo l,
d
a
.
…ma n
a
nis
d
s ie se sn
e
ek nic k ki in ckg
i
na
g
s o …ccer f Si he eld
t
a hn ed
n
k pi rc ok cin eg
e
da
s
b ta ol l
k
a icr ko u thn ed .
b all
s so hic rc te ar
n
b da bll lo acn
k
a
s
f hie ol rd
ts
w .hile wearing a red
The video appears to be a tutorial A man is demonstrating how to In the given video, we see a young man In the video, a man is
d …e m ao pn ps rt or aa cti hn ig
n
gt h te
h
et e bc ah ln
l
fiq rou me o af
n
k aic nk gi ln eg p thla ec be
a
y llo wur
h
p el na n sht ofo oo tit
n
b ge as ide p
a
l ga ry ein eg
n
s fo iec lc de
.
r
T
w heit rh
e
a isn
a
o gra ren eg ne ss oo cc cc ee rr bb aa ll ll on s sh kio llw
s
bin yg
d
o of if
n
h gi s
v
ab ra il ol uc so n mtr oo vl
e s
a jun sd t bp ela fn ot ri en mg t ah ke inir g f o tho et b kie cs ki .de the ball soccer ball. v bi ls ai cb kle s, h a irn td p w lae y ic na gn s a ol cs co e s r.ee a man wearing a on a field of grass.
GPT4(Vision) Gemini-Pro-V Video-ChatGPT Video-LLaVA LLaMA-VID MovieChat TimeChat
Figure8: Actioncompletiontendency.Mostopen-sourceVideo-LMMstendtogeneratecaptionscorrespond-
ingtocompleteactionsandstrugglewithdeterminingincompleteorpartialactions.
12a woman is seen lying on the floor with a woman is seen lying on the floor with her arms and
What is a woman is seen doing a workout on a a barbell on her stomach. She then lifts legs crossed. She then lifts her head and legs
h tha ep p vie dn ei on ?g in w we ei ig gh ht
t
m ana dc h pi en re fo. rS mhe
in
i gs s ae ve an
r
il eif tt yin og
f
t …he t bh ee
fo
b ra er lb ye inll
g
u bp
a
a cn
k
d
d
od wow n.n several times s bi em fou rl eta dn re oo pu ps il ny
g
a tn hd
e
h mo bld as
c
t kh de om
w
u np
.
for a few seconds
… performing a lifting exercise known as A woman is lying on a yoga mat with a we see a young woman working A woman is lying on
the bench press. The individual is lying on a barbell resting on her lower abdomen. She out on a mat with a black bar and the floor and
bench, pushing a barbell with weights then lifts her legs and places her feet on a gym. She lifts the bar over her performing squats
upward and then lowering it back towards the barbell and proceeds to do a series of head and continues to do so for while holding a
their chest in a controlled manner. leg and hip exercises. several minutes. heavyweight barbell.
W shh oa wt n is i nth te h eu n viu ds eu oa ?l aspect T t sh kh aee t m eu bn a ou n as d ru doa .il n a gs p ae hc at ns dh so tw ann d i n o nth ae video is … a hb o i l l di in t iy n t gth o ae nb v ri ued mae ko bd ri a es n lt lc ah e .e w …m ha in le' s T t sh ih dee e m wun aau ln ks iu wsa hdl iaa lens cp hie n oc g ldt a io nnf gd t ah ju ne m uvi mpd ie n bo g re i o ls ln at .h tha et
T t w hoh oh e lb e d eu r ine n a g u t p h as e eu nr ya s u l o s ma ens e b p p m re e e c r lt lft o ao s r b ah m neo diw fn l aogn a hai tn ai\ n tt " gh l he e aiv n siv t mi fad at ie li d ln eo -g na a \ i rp o" p nwtr te hi oa c i k lr es T t gh rh aae vt iu t thn y eu b s m yu la a el n a a nas ipp npe gec bt a aro s cf k tt woh ae b r ev d i sdd e ae f to y a ii ns n g T p anh e de o pv wli ed h e id to ere s ssh hso e irw d ts s i n d a ob g il nr ao gcu k a p s uo if t s T t ehh laee b v ou i rn d au e tos eu i da s a l a na m csp ea e rnc o t d u s o th ii nno egw an n i n
the ground. … impossible angle. dance on a busy city street. using a large and heavy cart.
GPT4(Vision) Gemini-Pro-V Video-ChatGPT Video-LLaVA LLaMA-VID MovieChat TimeChat
Figure9: WeakgeneralizationonOODvideos.Open-sourceVideo-LMMsstruggletocorrectlyreasonover
videoscontainingrareandunusualactions.
Is the hand movement from left to right in the Yes, the hand is moving from Yes, the hand movement in the Yes, the hand movement in the
video? left to right in the video.. video is from left to right in the … video is from left to right. The ....
I rt ig a hp tp te oa lr es f tt ,h ra at t t hh ee r h tha an nd fm roo mve lem fte n tot s rh igo hw t.n is from yes. Y Te hs e, hth ae n dh a an pd p em ao rsv e tom be en t p i on i nth tie n gv i ad te to h eis ff oro om d. left to right. yes.
I c as a
w
t m ah yee r fp a roe or mrs o r tun hn er nu cin n an mgi n i eng
r a
tt h ?o ew da ir rd es c tt ih oe n T cah me ep re ar .son is running towards the T thh ee cp ae mrs eo rn a .is running towards The person is running towards the camera.
The person in the images is running backwards, away The person is running In the given video, the person is running towards
from the camera. You can tell because they are … backwards toward the camera. running towards the camera. the camera
GPT4(Vision) Gemini-Pro-V Video-ChatGPT Video-LLaVA LLaMA-VID MovieChat TimeChat
Figure 10: Limited temporal understanding. Most Video-LMMs struggle to accurately determine the
temporalorderofactionsinvideos.Thebottomvideoshowsamanrunningbackwardalongatrack.
13I pd ue tn tit nif gy ti hf eth be a m ll ia nn t c ha el m holy le w ?alk away after Y ae ws a, yt h ae ft m era pn u c ta til nm gl y th w ea blk as ll … Y w the a es l, k ht s oh a le ew .m aya n a fi tn e t rh pe u v ttid ine go t c ha el m baly ll in Y ae fts e, rt h pe u tm tina gn tc ha elm bl ay l lw ina l tk hs e a hw oa ly e .
the man does not calmly walk away after putting No, he takes the flag out of the hole The man is walking calmly after
t eh xe p eb ra iell n in ci nth ge f rh uo sl te r. a T tih oe n s ae fq teu re tn hc ee p s uh to tw ; …s him and throws it. He then walks away. p a u gt ot li fn gg a t mhe e .b Ta hll e in st co e nth ee s h ho ol we s d …uring yes.
W unh ifa ot r mis t dh oe i np ge r as no dn win h o y?range T s at ph a pe n edp aie n rr sgs t o o on n b i an e st tth ar le e k e io ntr g a c n o tog rn e se ou r mn ai enfo odr n m e. i s … T c smlh ee a a n lp l ie wnr g hs o it thn ee i c n s l oio d tr e ha w .n …ag le k u wn ii tf ho r am is T ph ice k ip ne gr uso pn t rin as t hh e o no r ta hn eg se id u en oif fo tr hm e i rs o ad.
The person in the orange uniform The person in orange uniform The person in orange
appears to be a worker involved in is a street sweeper. She is … It is not clear what the person in the uniform is sweeping the
some sort of maintenance or cleaning sweeping up the sidewalk and orange uniform is doing, but it appears to be street and removing trash.
activity. Looking at the … putting the trash into a cart. standing and observing the street corner. …
GPT4(Vision) Gemini-Pro-V Video-ChatGPT Video-LLaVA LLaMA-VID MovieChat TimeChat
Figure11: Limitedcontextualunderstanding.MostVideo-LMMsexhibitaweakunderstandingofcomplex
videosthatcontainemotional(e.g.,anangryplayerinthetopvideo)andsocialcues(e.g.,apersonsavingshoes
fromgettingwetduetoraininthebottomvideo).
References
Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,Arthur
Mensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguagemodelforfew-shot
learning. 2022. 4
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
Advancesinneuralinformationprocessingsystems,33:1877–1901,2020. 1,4
FabianCabaHeilbron,VictorEscorcia,BernardGhanem,andJuanCarlosNiebles. Activitynet:Alarge-scale
videobenchmarkforhumanactivityunderstanding. InProceedingsoftheieeeconferenceoncomputervision
andpatternrecognition,pages961–970,2015. 6
Rizhao Cai, Zirui Song, Dayan Guan, Zhenhao Chen, Xing Luo, Chenyu Yi, and Alex Kot. Benchlmm:
Benchmarkingcross-stylevisualcapabilityoflargemultimodalmodels. arXivpreprintarXiv:2312.02896,
2023. 2,7
WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,BoyangLi,
PascaleFung,andStevenHoi.Instructblip:Towardsgeneral-purposevision-languagemodelswithinstruction
tuning. arXiv:2305.06500,2023. 4
Yuxin Fang, Quan Sun, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva-02: A visual
representationforneongenesis. arXiv:2303.11331,2023. 4
KanishkGandhi,Jan-PhilippFränken,TobiasGerstenberg,andNoahGoodman. Understandingsocialreasoning
inlanguagemodelswithlanguagemodels. AdvancesinNeuralInformationProcessingSystems,36,2024. 17
RohitGirdharandDevaRamanan. CATER:AdiagnosticdatasetforCompositionalActionsandTEmporal
Reasoning. InICLR,2020. 6
Google. Gemini,2023. URLhttps://blog.google/technology/ai/google-gemini-ai/. 2,9
RaghavGoyal,SamiraEbrahimiKahou,VincentMichalski,JoannaMaterzynska,SusanneWestphal,Heuna
Kim,ValentinHaenel,IngoFruend,PeterYianilos,MoritzMueller-Freitag,etal.The"somethingsomething"
videodatabaseforlearningandevaluatingvisualcommonsense. InICCV,2017. 6
YunseokJang,YaleSong,YoungjaeYu,YoungjinKim,andGunheeKim. Tgif-qa: Towardspatio-temporal
reasoninginvisualquestionanswering. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages2758–2766,2017. 2
14Albert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford,
DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,etal. Mixtralofexperts.
arXivpreprintarXiv:2401.04088,2024. 1
HildegardKuehne,HueihanJhuang,EstíbalizGarrote,TomasoPoggio,andThomasSerre. Hmdb: alarge
videodatabaseforhumanmotionrecognition. In2011Internationalconferenceoncomputervision,pages
2556–2563.IEEE,2011. 6
BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-bench: Benchmarking
multimodalllmswithgenerativecomprehension. arXivpreprintarXiv:2307.16125,2023a. 2,4
KunChangLi,YinanHe,YiWang,YizhuoLi,WenhaiWang,PingLuo,YaliWang,LiminWang,andYuQiao.
Videochat:Chat-centricvideounderstanding. arXivpreprintarXiv:2305.06355,2023b. 1,4,9
KunchangLi,YaliWang,YinanHe,YizhuoLi,YiWang,YiLiu,ZunWang,JilanXu,GuoChen,PingLuo,etal.
Mvbench:Acomprehensivemulti-modalvideounderstandingbenchmark. arXivpreprintarXiv:2311.17005,
2023c. 2,4
YanweiLi,ChengyaoWang,andJiayaJia. Llama-vid:Animageisworth2tokensinlargelanguagemodels.
arXivpreprintarXiv:2311.17043,2023d. 4,7,9
Bin Lin, Bin Zhu, Yang Ye, Munan Ning, Peng Jin, and Li Yuan. Video-llava: Learning united visual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023. 1,2,4,9
FuxiaoLiu,KevinLin,LinjieLi,JianfengWang,YaserYacoob,andLijuanWang. Aligninglargemulti-modal
modelwithrobustinstructiontuning. arXivpreprintarXiv:2306.14565,2023a. 2,7
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. 2023b. 10,11,20
MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan.Video-chatgpt:Towardsdetailed
videounderstandingvialargevisionandlanguagemodels. arXivpreprintarXiv:2306.05424,2023. 1,2,4,7,
9,11,20
ShehanMunasinghe,RusiruThushara,MuhammadMaaz,HanoonaAbdulRasheed,SalmanKhan,Mubarak
Shah, and Fahad Khan. Pg-video-llava: Pixel grounding large video-language models. arXiv preprint
arXiv:2311.13435,2023. 4
OpenAI. GPT-4V(ision)SystemCard,2023. URLhttps://cdn.openai.com/papers/GPTV_System_
Card.pdf. 2,9
YusuQian,HaotianZhang,YinfeiYang,andZheGan.Howeasyisittofoolyourmultimodalllms?anempirical
analysisondeceptiveprompts. arXivpreprintarXiv:2402.13220,2024. 2,7
AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,GirishSastry,
AmandaAskell,PamelaMishkin,JackClark,etal.Learningtransferablevisualmodelsfromnaturallanguage
supervision. 2021. 4
ShuhuaiRen,LinliYao,ShichengLi,XuSun,andLuHou. Timechat: Atime-sensitivemultimodallarge
languagemodelforlongvideounderstanding. arXivpreprintarXiv:2312.02051,2023. 4,7,9
GunnarASigurdsson,GülVarol,XiaolongWang,AliFarhadi,IvanLaptev,andAbhinavGupta. Hollywoodin
homes:Crowdsourcingdatacollectionforactivityunderstanding. InComputerVision–ECCV2016:14th
EuropeanConference,Amsterdam,TheNetherlands,October11–14,2016,Proceedings,PartI14,pages
510–526.Springer,2016. 6
EnxinSong,WenhaoChai,GuanhongWang,YuchengZhang,HaoyangZhou,FeiyangWu,XunGuo,Tian
Ye, Yan Lu, Jenq-Neng Hwang, et al. Moviechat: From dense token to sparse memory for long video
understanding. arXivpreprintarXiv:2307.16449,2023. 4,9,20
BartThomee,DavidAShamma,GeraldFriedland,BenjaminElizalde,KarlNi,DouglasPoland,DamianBorth,
andLi-JiaLi. Yfcc100m:Thenewdatainmultimediaresearch. CommunicationsoftheACM,59(2):64–73,
2016. 6
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,ArmandJoulin,Edouard
Grave,andGuillaumeLample. Llama:Openandefficientfoundationlanguagemodels. arXiv:2302.13971,
2023. 1,4
15XuezhiWang,JasonWei,DaleSchuurmans,QuocLe,EdChi,SharanNarang,AakankshaChowdhery,and
DennyZhou. Self-consistencyimproveschainofthoughtreasoninginlanguagemodels. arXivpreprint
arXiv:2203.11171,2022a. 4,7
YiWang,KunchangLi,YizhuoLi,YinanHe,BingkunHuang,ZhiyuZhao,HongjieZhang,JilanXu,YiLiu,
ZunWang,etal. Internvideo:Generalvideofoundationmodelsviagenerativeanddiscriminativelearning.
arXivpreprintarXiv:2212.03191,2022b. 4
JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,DaniYogatama,Maarten
Bosma,DennyZhou,DonaldMetzler,etal. Emergentabilitiesoflargelanguagemodels. arXivpreprint
arXiv:2206.07682,2022a. 1
JasonWei,XuezhiWang,DaleSchuurmans,MaartenBosma,FeiXia,EdChi,QuocVLe,DennyZhou,etal.
Chain-of-thoughtpromptingelicitsreasoninginlargelanguagemodels. Advancesinneuralinformation
processingsystems,35:24824–24837,2022b. 4,7,10
DejingXu,ZhouZhao,JunXiao,FeiWu,HanwangZhang,XiangnanHe,andYuetingZhuang. Videoquestion
answeringviagraduallyrefinedattentionoverappearanceandmotion. InProceedingsofthe25thACM
internationalconferenceonMultimedia,pages1645–1653,2017. 2,10
ZhouYu,DejingXu,JunYu,TingYu,ZhouZhao,YuetingZhuang,andDachengTao. Activitynet-qa:Adataset
forunderstandingcomplexwebvideosviaquestionanswering. InProceedingsoftheAAAIConferenceon
ArtificialIntelligence,volume33,pages9127–9134,2019. 2,10
HangZhang,XinLi,andLidongBing. Video-llama: Aninstruction-tunedaudio-visuallanguagemodelfor
videounderstanding. arXivpreprintarXiv:2306.02858,2023. 1,4,9
LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,ZiLin,Zhuohan
Li,DachengLi,Eric.PXing,HaoZhang,JosephE.Gonzalez,andIonStoica. Judgingllm-as-a-judgewith
mt-benchandchatbotarena. arXiv:2306.05685,2023. 1,4
16Appendix
Inthefollowingsections,weprovideadditionalinformationforthepaper: ComplexVideoReasoningand
RobustnessEvaluationSuiteforVideo-LMMs.Thecontentsareorganizedinthefollowingorder.
• Implementationdetails(AppendixA)
• AdditionaldetailsonCVRR-ESBenchmark(AppendixB)
• AnalysisandadditionalresultsforDSCPtechnique(AppendixC)
• AdditionalAblationExperiments(AppendixD)
A Implementationdetails
Foropen-sourcemodels,wefollowtheirdefaultbestinferencesettingsandhyperparameters.ToevaluateGemini
andGPT-4V,weutilizetheirofficialAPIs. FullvideosaredirectlypassedtoGeminiVision-Pro,asitsAPI
(usingGoogleCloudvertexaiframework)inherentlysupportsvideoinputs. However,asGPT-4Vdoesnot
inherentlysupportvideos,weuniformlysample8framesforeachvideowhicharepassedintoGPTAPIalong
withuserquestions.Foreachmodelunderevaluation,wegenerateresponsestothequestionsindependentlyand
withoutretainingthechathistory.FortheevaluationresultsofVideo-LMMsontheCVRR-ESQApairs,we
utilizeGPT-3.5asajudgeinallofourexperiments.
B AdditionaldetailsonCVRR-ESBenchmark.
Moredetailsonannotationprocess. Experthumanannotatorsareassignedtoannotatethevideosofthe
CVRR-ESbenchmark.Toensureconsistencyandhighquality,weprovideannotationinstructionstoannotators,
whogeneratecaptionsaccordingly.Forinstance,whenannotatingvideosforthecategoryofnon-existentactions
withnon-existentscenedepictions,annotatorsareinstructedtoincludeinformationaboutallactionsandattribute
informationaboutobjects.Thisensuresthateachcaptionprovidessufficientinformationtobeeffectivelyused
inthenextstageoftheQAgenerationprocess. Toverifythequalityandcorrectnessofvideocaptions,we
performtwoseparateiterationsofverificationandrectification(ifapplicable)ofeachvideocaptioncuratedin
thepreviousiteration.
Question-Answergenerationprocess.WeuseLLMassistedquestion-answergenerationprocess,tocurate
question-answerpairsusingground-truthvideocaptionsintheCVRR-ESbenchmark.Anillustrationofthis
processisshowninFig.14.
QualityofQApairs.WepresentexamplesofQApairsfromtheCVRR-ESbenchmarkinTable4.OurQApairs
areofhighqualityandaimtocomprehensivelytesttheunderstandingofVideo-LMMsagainstreasoningand
robustnesscriteriaacrossmultipleevaluationdimensions.Toquantitativelyassessthequalityofthebenchmark,
weestablishaqualityassessmentproceduresimilartotheonein[Gandhietal.,2024].Werandomlysample
1120QApairs,whichencompassallvideosoftheCVRR-ESbenchmark,andrequesthumanexpertstoevaluate
thequalityofeachQApairbyansweringthefollowingquestions:(1)"DoestheQApaircorrectlyrepresent
theevaluationdimensioncategoryunderwhichitfalls?"(possibleanswers:"Yes","No")(2)Canthequestion
becorrectlyansweredgivenonlythevideocontent? (possibleanswers: "Agree","Disagree")and(3)Isthe
correspondingpairedground-truthanswercorrect? (whichwillbeusedduringevaluationasgroundtruth)
(possibleanswers:"Yes","No").Onaverage,theanswerofexpertsforthefirstquestionwas"Yes"for98.84%
ofthetimes. Forthesecondandthirdquestions,theaveragedanswerwas"Agree"and"Yes"for100%and
99.91%ofthetimes,respectively.
HumanEvaluation.ToverifythattheQApairsintheCVRR-ESbenchmarkarereasonablyanswerableandto
establishabenchmarkforhumanperformance,weconductahumanevaluation.Twohumanexperts(authors)
areinstructedtowatchthevideocorrespondingtoeachquestionandprovideafree-formanswer.Thepredictions
ofthehumanexpertsforallQApairsareassessedusinganLLM-assistedevaluation.Individualfinalscores
areaveragedtomitigatepotentialbiasfromasinglehumanevaluator.Theresultsofthehumanevaluationare
presentedinthemainpaperintheexperimentssection(Sec.5.1).
C FurtherAnalysisandAdditionalResultsforDSCPMethod.
C.1 DiscussiononDSCPmethod.
WenotethatVideo-LMMsareoftenabletocorrectlycomprehendthevideocontentandproducefactualresponses.
However,theyareextremelysensitivetousertextualpromptinputsandexhibithighlyover-affirmativebehavior.
17Table4: Examplesofthequestion-answerpairsintheCVRR-ESbenchmarkforvariouscomplex
videoevaluationdimensions.
EvaluationDimensions SampleQuestion-Answerpairs
Q.Doesthepersonstanduptowelcomethecatorremainseatedthroughouttheirinteraction?
1.Multipleactionsin A.Thepersonremainsseatedthroughouttheirinteractionwiththecat.
asinglevideo Q.Whatisthenextactionperformedbythepersonafterusingthelaptop?
A.Theactiondirectlyafterusingthelaptopisplacingabagintherefrigerator.
Q.Atanypointinthevideo,doesthemanusethethreadtosewfabric?
2.Fine-grainedaction A.No,themanusesthethreadtocreateloopsanddemonstratetyingaknot;thereisnodepictionofsewingfabric.
understanding Q.Whatactionisperformedbytheperson’shandsinthevideo?
A.Theperson’shandsareshownpluggingablackUSBchargingcableintothechargingport.
Q.Whatishappeninginthevideo?
A.Thevideoshowsthedoorofaredcarandaperson’shandreachingtothehandleofthecar...
3.Partialactions
Q.Doesthevideoincludeamomentwherethesnackisreplacedtoitsoriginalpositionontheright?
A.No,thevideoconcentratesontheinitialactionofmovingthesnackfromtherighttotheleft,without...
Q.Isthevideoshowingtheactivityoftakingoutliquidfromthesodacan?
A.No,thevideodoesnotshowtheactivityoftakingouttheliquidfromthesodacan.Thevideoshows...
4.Timeorderunderstanding
Q.Isthepersonrunninginclockwisedirectionoranticlockwisedirectionontheracetrack?
A.Thepersonisrunninginanticlockwisedirectioninthevideo.
Q.Aftergoingthroughthebag,doesthepersonmeticulouslycleantheareaaroundthesink?
5.Non-existentactionswith
A.No,thepersondoesnotcleantheareaaroundthesinkaftergoingthroughthebag.Thevideofocuses...
existentscene
Q.Whatisthereactionoftheaudiencewhenthekeynotespeakerdelivershisspeech?
depictions
A.Thescenedoesnotincludeamomentwhereakeynotespeakerisdeliveringaspeech...
Q.Howdothechildreninteractwiththeflowersinthevideo?
6.Non-existentactionswith
A.Therearenochildreninteractingwiththeflowersdepictedinthevideo.Thefootageiscommittedtodisplaying...
non-existentscene
Q.Whatisthereactionofthechildplayinginthecornerwhenthedogrunspast?
depictions
A.Thereisnochildplayinginthecorneroranyreactiontothedogruningpast...
Q.Howmanyuniquesunglassesappearthroughoutthevideo?
7.ContinuityandObject A.Asthereare4personsinthecarwearingthesunglasses,thenumberofuniquesunglassesis4.
InstanceCount Q.Didtheattireofbothmenremainthesameuponre-enteringtheframethesecondtime?
A.No,theattireofbothmendidnotremainthesameuponre-entering...
Q.Isthepersonshowcasingwalkingorrunningmovementstoreachanelevatedpositioninthevideo?
8.UnusualandPhysically A.No,thepersondidnotwalkorrun;theyascendedandfloatedintheairthroughwhat...
Anomalousactivities Q.Howthepersonisabletoflyoverthewater?
A.Thepersonisusingaflyboardsystemattachedtohisshoesusingwhichheisflyingoverthewater.
Q.Whatwastheresponseofthecrowdwhenthegirllandedthewaterbottlevertically?
9.Interpretationof A.thecrowdapplaudedtoshowcaseappreciationforherperseveranceandsuccess.
socialcontext Q.Whatistheprimaryreasontheboytouchestheashesbeforeplacinghishandonthegoat?
A.Theboyusestheashestowarmthegoat,indicatinghisprimarymotiveiscareandprovidingwarmth.
Q.Identifyiftheemotionalcontextofthevideoisnegative,basedonthedescribedactionsandreactions?
10.Understandingof A.Theemotionalcontextofthevideoisnotnegative;itisoverwhelminglypositive.Theindicatorsofhappiness,...
emotionalcontext Q.Identifythenatureoftheinteractionbetweenthetwoindividuals.Isitprofessional,hostile,orfriendly?
A.Theinteractionisfriendly.Thisisevidencedbythewarmhugandthehandshake,...
Q.Doesthepersoninthevideoundergoarealphysicaltransformation?
11.Interpretationof A.No,...Theysimplyremovearubbermaskthatmadethemlooklikeaman,revealingthattheyareactuallyawoman.
visualcontext Q.Identifytheunusualbehaviordepictedbetweenapredatoranditsusualpreyinthevideo.
A.Acatplaysandsleepswithchicksinsteadofhuntingthem.Thisshowcasesanunusualpeace...
TheseattributesofVideo-LMMscanleadtohallucinationsandwrongoutputresponses,especiallywhenthe
userasksreasoning-based,low-quality,confusing,ormisleadingquestions.
OurDual-StepContextualPromptingtechniqueaimstoaddresstheselimitationsofVideo-LMMsbyexplicitly
delineatingthecontextualreasoninginformationretrievalfromtheuserquestionansweringusingatwo-step
promptingtechnique.Thisstrategyeffectivelyeliminatestheinfluenceofthequestion,leadingtomoregrounded
overallresponses.
InFig.12,weshowthesensitivityofVideo-LMMstotextualpromptsandtheimpactofeachstepintheDSCP
promptingtechnique. Itcanbeobservedthatpromptingthemodelwithsimplequestions,suchas’Describe
thevideocontent’or’Whatishappeninginthevideo?’leadstocorrectresponses.However,astheuserasks
areasoning-basedquestionoratrickyquestion,themodelstrugglestoreasonproperlyandhallucinatesdue
toanover-affirmativeresponse. Finally, wegeneratetheresponseusingtheDSCPmethod. Thefirststep
independentlyretrievescontextualreasoninginformationusingprincipledpromptinstructions,followedby
askingtheuseraquestionconditionedonboththefactualinformationretrievedearlierandtheinputvideo.
WeobservethatintegratingbothstepsofDSCPpromptinginjectsimprovedreasoningandself-rectification
capabilitiesintoVideo-LMMs.
C.2 Detailedcomparisonresults.
Inthemainpaper,wepresentedoverallresultscomparisonsbetweenVideo-LMMsutilizingtheDual-StepCon-
textualPrompting(DSCP)technique.Here,weshowtheperevaluationdimensionperformanceofVideo-LMMs
18In the video, two men are sitting at a bar, The … two men sitting in a bar, smoking
Simple What is happening in smoking cigarettes and having a conversation. Describe the cigarettes and engaging in conversation.
Questions the video? One of the men is seen smoking a cigarette video One of the men is seen smoking a cigarette
while the other is holding a drink. while the other is holding a drink . …
Simple
Reasoning Is there a cat shown Yes, a cat is shown in the video. It is seen Is the person in Yes, the person in red dress is having a
Questions in the video? sitting on a chair in the background while … r ae pd h d or ne es s c ah la l?ving phone call.
What kind of
C M Qo uisn elef su a tis odin ning sg/ W in h tha et i vs i dth ee o ?cat doing T wh he il eca tt h i es ts witt oin mg eo nn aa r ec h sa mir o i kn i nth g e a nb da c tk ag lkro inu gn .d d p pa e en r rs fc ooe rn mi s o it n nh gre ?i ght T tah ne g p oe dr aso nn ce o .n the right is performing a
Improved video reasoning and robustness with our DSCP Method
Simple
Reasoning Is there a cat shown No, there is no cat shown in the video. The Is the person in No, the person in red dress is not having a
Questions in the video? v smid oeo ki no gn l cy i gs ah ro ew tts e stw ao n dm de rn in s ki it nti gn .g in a bar, r ae pd h d or ne es s c ah la l?ving phone call.
C M Qo uisn elef su a tis odin ning sg/ W in h tha et i vs i dth ee o ?cat doing T v smih de oe o kca i not g ni s l cy n i gso aht ro ed w to ts ein stg w a a o nn dmy ht eh ani vn isg nit gi tn i an t g ch oe in n v vai ed b re sao ar. , t iT oh ne . I r as e pdth h de or nep ese sr c s aho la ln ?v ii nn g T a smnh ye o d kp iae nnr gs c o e an . c H io gen a rit s eh tje u te sle .t f st i ti ts i nn go t i np e thrf eo brm ari n ag n d
Figure12: EffectofdifferentpromptsonVideo-LLaVA.Row1: Video-LLaVAoftenprovidesfactualand
correctinformationabouttheinputvideowhenpromptedwithsimpleandclearquestions.Row2&3:However,
themodelstrugglestoremainfactualwhenthequestionbecomesreasoning-based,confusing,ormisleading,
mainlyduetoitsover-affirmativebehavior. Row4&5: OurDSCPmethodutilizescontextualreasoning
informationinthefirststepprompting,independentoftheuserquestion,andusesitasconditioninginformation
inthesecondstep,leadingtomoregroundedandfactualresponsestouserquestions.
whenutilizingDSCPtechniqueinTab.5.TheresultsindicatethatVideo-LMMswithDSCPtechniqueprovide
substantialperformanceimprovementsacrossvariousevaluationdimensionsintheCVRR-ESbenchmark.
WhileDSCPpromptingreducestheperformancefortheevaluationdimensionoftime-orderunderstanding
forafewVideo-LMMssuchasVideoChat, Video-ChatGPT,andGemini, theoverallrelativeperformance
improvementsarenotableforthemajorityofthemodels.DSCPtechniqueimprovestheperformanceofVideo-
LMMsacrossmostevaluationdimensions. Inparticular,DSCPshowsthehighestgainsfortheevaluation
dimensionsofphysicallyanomalous,contextualvideos,fine-grainedactions,andpartialactions,demonstrating
themodel’simprovedreasoningcapabilitieswithoutanyadditionaltraining.Forevaluationdimensionsinvolving
explicit misleading user questions, such as non-existent actions with non-existent scene depiction, DSCP
substantiallyimprovesthemodel’sperformance. Forinstance,VideoChatimprovesfrom14.38%to58.33%
onthesameevaluationdimension,correspondingtorelativegainsofover300%. ThissuggeststhatDSCP
promptingactsasanadditionalfilterlayerthatguidesthemodeltowardsrobustandgroundedbehavior.
TheoverallperformanceimprovementsofVideo-LMMswithDSCPsuggestthatpromptingtechniquescan
effectivelysteerthebehaviorofVideo-LMMsforenhancedreasoningandrobustnessovervideos.Although
DSCPshowspromisingresults,thenetperformanceofVideo-LMMsisstillfarfromsatisfactory,whichdemands
moreadvancedtechniquestofurtherenhancetheircapabilities,especiallyforopen-sourcemodels.
D AblationStudies.
OurCVRR-ESevaluationbenchmarkutilizeskeydesignchoices.Inthissection,wepresentseveralablation
studiestovalidatetheeffectivenessofthesedesignchoices.
AlignmentofLLMastheJudgewithHumanevaluators.
WeutilizeLLMssuchasGPT-3.5asajudgeforevaluatingVideo-LMMsontheCVRR-ESbenchmark.Inthis
study,wecomparehowcloselyLLMaccuracyscoresalignwithhumanevaluations.Weassigntwoexperthuman
evaluatorstoindependentlyevaluatehumanperformancebymanuallyevaluatingandscoringeachcandidate’s
answer.WeobservethatthehumanevaluationresultsbyLLMhaveanalignmentpercentageof95.36%.This
meansthatfor4.64%ofQApairs,therewasamismatchbetweenLLMjudgmentandhumanjudgment.The
95%+alignmentratewithGPT-3.5isencouraging,andweconjecturethatfutureLLMswillexhibitfurther
alignmentwithhumanevaluations.
LLMJudgementimprovesbygeneratingexplanations.OurdefaultevaluationpromptasshowninFig.13
requirestheJudgeLLMtogenerateacorrect/incorrectflag,ananswerqualityscore(rangingfrom0to5),and
therationalebehindthequalityscoreandthecorrect/incorrectflag.Thealignmentscorewithhumanevaluators
forthisinstructionpromptis95.36%.Previously,weutilizedtheLLMJudgeinstructionpromptbasedonprior
19Table5: VideoLMMsevaluationresultsusingourDual-StepContextualPrompting(DSCP)Technique.Video
LMMs with DSCP technique effectively improves their reasoning and robustness capabilities on complex
video-evaluationdimensionsinCVRR-ES.Absolutegainsoverthestandardpromptingareshowningreen.
BenchmarkCategory Video-LLaMA2 VideoChat Video-ChatGPT Video-LLaVA MovieChat LLaMA-VID TimeChat Gemini-VPro
MultipleActionsin 32.39 38.99 32.70 37.74 27.36 39.62 32.08 49.37
singlevideo. (+15.41) (+15.09) (+5.03) (+22.01) (+14.78) (+21.70) (+3.77) (+6.29)
Fine-grainedaction 35.65 39.57 28.26 33.48 41.74 41.74 40.87 51.15
understanding. (+6.09) (+6.09) (+1.30) (+8.26) (+18.26) (+15.65) (+1.74) (-0.46)
Partial 39.32 50.49 34.95 47.57 33.98 52.91 55.34 61.17
actions. (+14.56) (+17.48) (+12.14) (+33.98) (+12.62) (+38.35) (+5.83) (-6.31)
Timeorder 28.29 28.95 23.68 30.26 23.68 31.58 32.24 43.42
understanding. (+11.84) (-2.63) (-3.95) (+9.21) (+7.24) (+11.84) (-1.97) (-1.97)
Non-existentactionswith 39.86 65.94 31.16 47.10 39.13 51.45 30.43 68.12
existentscene. (+29.71) (+50.72) (+7.97) (+42.03) (+34.06) (+48.55) (+7.25) (+10.87)
Non-existentactionswith 40.97 58.33 30.56 42.36 35.42 56.94 29.17 71.94
non-existentscene. (+27.78) (+43.75) (+13.19) (+38.89) (+23.61) (+50.00) (+15.28) (+22.30)
ContinuityandObject 31.07 38.42 31.64 32.77 35.59 37.85 38.98 46.33
instanceCount. (+2.82) (+14.12) (+3.23) (+11.30) (+15.82) (+12.99) (+4.52) (+10.17)
UnusualandPhysically 38.95 50.00 33.16 31.58 40.53 40.53 37.89 65.26
Anomalousactivities. (+20.00) (+31.58) (+14.21) (+15.79) (+22.63) (+24.21) (+10.53) (+5.26)
Interpretationof 47.50 58.21 48.93 43.93 44.29 64.29 52.86 72.14
socialcontext. (+22.50) (+27.14) (+16.43) (+25.00) (+27.14) (+50.36) (+13.57) (+7.86)
Understandingof 35.27 41.10 30.14 24.66 32.88 37.67 33.56 50.68
emotionalcontext. (+13.36) (+17.47) (+8.90) (+9.59) (+19.18) (+22.95) (+6.16) (+3.42)
Interpretationof 47.50 58.21 48.93 43.93 44.29 64.29 52.86 72.14
visualcontext. (+13.55) (+22.71) (+19.78) (+26.01) (+18.68) (+37.73) (+5.49) (-2.20)
Average 37.77 47.92 33.89 37.93 35.87 46.85 39.45 58.22
(+16.15) (+22.14) (+8.93) (+22.01) (+19.46) (+30.39) (+6.56) (+5.02)
Evaluation Prompt to LLM as a Judge
You are an intelligent chatbot designed for evaluating the correctness of AI assistant predictions for
question-answer pairs.
Your task is to compare the predicted answer with the ground-truth answer and determine if the predicted
answer is correct or not. Here's how you can accomplish the task:
------
##INSTRUCTIONS:
- Focus on the correctness and accuracy of the predicted answer with the ground-truth.
- Consider predictions with less specific details as correct evaluation, unless such details are explicitly
asked in the question.
Please evaluate the following video-based question-answer pair:
Question: {CVRR-ES Question}
Ground truth correct Answer: {CVRR-ES GT answer}
Predicted Answer: {Video LMM prediction}
Provide your evaluation as a correct/incorrect prediction along with the score where the score is an
integer value between 0 (fully wrong) and 5 (fully correct). The middle score provides the percentage of
correctness.
Please generate the response in the form of a Python dictionary string with keys 'pred', 'score' and
'reason', where value of 'pred' is a string of 'correct' or 'incorrect', value of 'score' is in INTEGER, not STRING
and value of 'reason' should provide the reason behind the decision.
Only provide the Python dictionary string.
For example, your response should look like this: {'pred': 'correct', 'score': 4.8, 'reason': reason}.
Figure 13: Prompt used to instruct LLM as a judge for evaluating Video-LMM responses on CVRR-ES
benchmark.WeemployGPT-3.5turboasthechoiceofLLM.Thesystempromptisshowninbluewhilethe
mainpromptisshowningreen.
works[Maazetal.,2023,Liuetal.,2023b,Songetal.,2023],whichdonotrequestthemodeltoprovidethe
decisionrationale.Withtheirprompt,weobservethattheJudge’salignmentwithhumanevaluatorsis89.63%.
ThissuggeststhatrequiringLLMJudgedecisionstobeaccompaniedbycorrespondingreasonsyieldsmore
reliableevaluationresults.
20Evaluation dimension category: Understanding of emotional context.
Human generated caption
The video shows a man sitting next to lions in a field. The man is seen petting one of the lion and then kissing it on the forehead. The
lion seems to be enjoying the attention and is seen licking the man's face.The man then stands up and walks away from the lion. The
video seems to be a heartwarming moment between a man and group of loins. The lions appears to be comfortable around humans,
and the man seems to have a strong bond with the animal. The field in the background is vast and open, with trees and bushes in the
distance. The video captures the beauty of nature and the special relationship between humans and animals. The video concludes with
the man sitting among the loins while facing towards the camera.
Question-Answer Generation Process "Q": "Was the man attacked by the lion in the
video?"
Given a video containing actions dependent on "A": "No, the man was not attacked by the lion in
emotional context, with the following detailed caption the video. In fact, ..... "
explaining the events: The caption is: {Human "Q": "What emotional connection can be inferred
generated caption}. Manual between the man and the lion, based on the
1) Formulate 10 diverse misleading questions to test, filteration actions performed by both?"
whether the model can correctly identify the actions "A": "A strong bond of affection and trust can be
based on the emotional context in the video or not. inferred because the lion allowed the man to pet
2) Additionally, these inquiries should assess the ....."
system under test's ability to accurately identify the "Q": "Did the interaction happen in a closed
actions in accordance with the emotional context being space like a zoo enclosure?" depicted in the video. "A": "No, the interaction took place in an open
3) Generate questions that comprise both interrogative field, not in an enclosed space."
and declarative sentences, utilizing different language .
styles, and provide an explanation for each. More QA pairs ...
Figure14:AnillustrationoftheQApairgenerationprocessusingLLMsforourCVRR-ESbenchmark.Human-
generatedvideocaptionsareinputtoLLMswhichareinstructedtogeneratediverseQApairsencompassing
bothtextualrobustnessandreasoningdimensions.
21
)MLL(
ledoM
egaugnaL
egraL