A New Robust Partial p-Wasserstein-Based Metric for Comparing
Distributions
SharathRaghvendra* PouyanShirzadian† KaiyiZhang†
Abstract
The2-Wassersteindistanceissensitivetominorgeometricdifferencesbetweendis-
tributions,makingitaverypowerfuldissimilaritymetric. However,duetothissensi-
tivity,asmalloutliermasscanalsocauseasignificantincreaseinthe2-Wassersteindis-
tancebetweentwosimilardistributions.Similarly,samplingdiscrepancycancausethe
empirical 2-Wasserstein distance on n samples in R2 to converge to the true distance
atarateofn−1/4,whichissignificantlyslowerthantherateofn−1/2for1-Wasserstein
distance.
Weintroduceanewfamilyofdistancesparameterizedbyk ≥0,calledk-RPW,that
is based on computing the partial 2-Wasserstein distance. We show that (1) k-RPW
satisfiesthemetricproperties,(2)k-RPWisrobusttosmalloutliermasswhileretaining
thesensitivityof2-Wassersteindistancetominorgeometricdifferences,and(3)when
k is a constant, k-RPW distance between empirical distributions on n samples in R2
convergestothetruedistanceatarateofn−1/3,whichisfasterthantheconvergence
rateofn−1/4forthe2-Wassersteindistance.
Usingthepartial p-Wassersteindistance,weextendourdistancetoany p ∈ [1,∞].
By setting parameters k or p appropriately, we can reduce our distance to the total
variation, p-Wasserstein, and the Lévy-Prokhorov distances. Experiments show that
ourdistancefunctionachieveshigheraccuracyincomparisontothe1-Wasserstein,2-
Wasserstein,andTVdistancesforimageretrievaltasksonnoisyreal-worlddatasets.
1 Introduction
Given two probability distributions µ and ν with supports A and B, let, for any (a,b) ∈
A × B, d(a,b) be the cost of moving a unit mass from a to b. A transport plan γ is a
couplingofµandν,i.e.,ajointdistributionoverthesupportA×Bwhosefirstandsecond
marginals are µ and ν. For p ≥ 1, consider the case where the support of µ and ν lie in
a metric space (X,c) with a unit diameter, i.e., for any pair (a,b) ∈ X ×X,c(a,b) ≤ 1
and the cost of moving unit mass from a to b is given by d(a,b) = c(a,b)p. Let γ∗ be
an optimal transport plan with respect to the cost function d(·,·). Then, the p-Wasserstein
distancebetweenµandνisgivenby:
(cid:18)(cid:90) (cid:19)1/p
W (µ,ν) := c(a,b)pdγ∗(a,b) .
p
X×X
The p-Wasserstein distance is a powerful metric for measuring similarities between
probabilitydistributions. Duetoitsnumerousmathematicalproperties,the p-Wasserstein
*DepartmentofComputerScience,NorthCarolinaStateUniversity.
†DepartmentofComputerScience,VirginiaTech.
4202
yaM
6
]GL.sc[
1v46630.5042:viXradistance has found diverse applications including in machine learning [6, 9, 14, 20, 27,
31, 38], computer vision [3, 18, 24], and natural language processing [1, 19, 39]. One
can estimate the p-Wasserstein distance between two unknown distributions µ and ν by
simplytakingnsamplesfromeachµandνandthencomputingthe p-Wassersteindistance
between the discrete distributions over these samples (each sample point is assigned
a mass of 1/n). For p ∈ [1,∞), it is well-known that as n → ∞ , this empirical p-
Wassersteindistanceconvergestothetrue p-Wassersteindistance. Duetothislawofweak
convergence, the p-Wasserstein distance is used as a loss function in training generative
models[2,17,35].
The p-Wassersteindistanceissensitivetogeometricdissimilaritiesbetweenthedistri-
butions. Consider two distributions µ and ν = (1−δ)µ+δν′ that differ only by a mass
of δ. The p-Wasserstein distance between µ and ν can be as high as δ1/pW (µ,ν′). Thus,
p
as p increases, the W (µ,ν) increases by a rate of δ1/p, making W more sensitive to such
p p
differences for larger values of p. The higher sensitivity of p-Wasserstein distance makes
itanattractivechoiceasadissimilaritymetricbetweendistributions. Consequently,itcan
beusedinclustering[13,40]andbarycentercomputation[10,11,37].
The higher sensitivity of p-Wasserstein distance also makes it susceptible to noise of
two types: outliers and sampling discrepancy. Consider µ and ν = 0.99µ+0.01ν′ and
W (µ,ν′) = 1, i.e., we add an outlier mass of δ = 0.01 that is placed at a distance 1 from
p
µ. In this case, µ and ν differ in only 1% of mass and yet, the distance between µ and ν
is 0.1 when p = 2, 0.21 when p = 3, and 1 when p = ∞ . Thus, for p ≥ 2, outliers can
disproportionatelyincreasethedistancebetweendistributions.
Similar to outliers, sampling discrepancies in empirical distributions can also con-
tribute disproportionately to the overall p-Wasserstein distance. As a result, in 2-
dimensions, the convergence rate of the empirical p-Wasserstein distance to the true dis-
tance drops to n−1/2p and for p = ∞ , the empirical distance does not even converge to
the real one [16]. To understand this phenomenon better, consider p = 2 and a discrete
distribution µ having two points a and b in its support, each assigned a probability mass
of 1/2. Let c(a,b) = 1. Consider now two sets X and Y of n samples drawn from µ and
letµ (resp. µ )bethediscretedistributionswithpointsof X (resp. Y)asthesupportand
X Y √
amassof1/nateachpointinthesupport. NotethatE[|µ (a)−µ (a)|] = Θ(1/ n)and
X Y
therefore, W (µ ,µ ) ≈ n−1/4 and W (µ ,µ ) ≈ n−1/2p. Thus, the rate of convergence
2 X Y p X Y
for p ≥ 2 is slower than for the case with p = 1. Therefore, one needs significantly more
samplestogetanaccurateestimateofthetrue2-Wassersteindistance. Thisrestrictstheuse
of2-Wassersteindistance(andalsootherhighervaluesof p)asalossfunctioninlearning
tasks.
Onewaytoovercometheimpactofnoisefromoutliersorsamplingdiscrepancyisby
usingthepartial p-Wassersteindistance. Forα-partial p-Wassersteindistance,onewishesto
computethecheapestcostofatransportplanthattransportsαmassbetweendistributions
µ and ν. Such transport plan is referred to as α-optimal partial transport plan (or simply α-
partial OT plan). Given two distributions µ and ν = (1−δ)µ+δµ′, and under reasonable
assumptionsontheoutlierdistributionµ′,onecanshowthatthetransportplanassociated
witha(1−δ)-partial p-Wassersteindistancewilltransportmassonlyfromtheinliers. This
observationwasusedtoeliminatetheimpactofoutliersintwodistributionsandapplied
to many ML tasks [8, 25, 28, 30]. Most of these applications assume that the value of δ is(i)Distributionsµandν (ii)∥µ−ν∥ (iii)W (µ,ν) (iv)Π (µ,ν)
TV p p,k
Figure1: Interpretationsofdifferentdistancefunctions.
given;see[5,7,15,29]. Recently,Phataketal.[33]introducedtheideaofOT-profile,which
is a function that maps any α ∈ [0,1] to the α-partial p-Wasserstein distance between µ
andν. Theyshowedthatthisfunctionisanon-decreasingfunction1,whichcanbeusedto
also identify the value of δ. All existing works that use partial p-Wasserstein distance to
identify outliers are described for pairs of distribution. It is not clear how one can apply
thisdistanceonasetcontainingnoisydistributions.
Additionally, there are two major drawbacks of using (1−δ)-partial p-Wasserstein
distanceasadissimilaritymeasureonsetsofprobabilitydistributions.
• The (1−δ)-partial p-Wasserstein distance does not satisfy the triangle inequality,
and
• For two distributions µ and ν that differ by a mass less than δ, the (1−δ)-partial
p-Wasserstein distance will be 0, i.e., this cost is not sensitive to minor geometric
differencesindistributions.
Animportantopenquestionisthefollowing:
Can we design a new metric that retains the sensitivity of p-Wasserstein distance to minor
geometricdifferencesinthedistributionsbutisrobusttonoise?
Our Results: For any k ≥ 0, we introduce a partial p-Wasserstein distance-based
metriccalled(p,k)-RPWandwedenoteitbyΠ (·,·). Ourdistanceissimplythesmallest
p,k
εsuchthatthe(1−ε)-partial p-Wassersteindistanceisatmostkε.
Our distance combines the total variation distance with the p-Wasserstein distance.
Recollect that the total variation distance between µ and ν is the mass that remains
after all the co-located mass is transported. In Figure 1 (ii), the mass inside the green
region represents the TV distance between two distributions. The p-Wasserstein distance
measures the cost of the cheapest transport plan that leaves no mass behind. In Figure 1
(iii),thecostofmovingallthemassfromtheredregiontotheblueregionrepresentsthe p-
Wassersteindistance. Ourdistancebalancesthetwo,i.e.,wefindanεsuchthatatransport
plan that leaves ε mass behind has a cost of kε. In Figure 1 (iv), our distance balances
the cost of moving mass from the red region to the blue region with the mass remaining
inside the green region. The robustness of our distance follows from the observation that
noisymasswillbepartofthegreenregion(i.e.,massthatisnottransported)andtherefore,
1Theirfunctionmapsαtothepthpoweroftheα-partialp-Wassersteindistance.Inthispaper,however,weassumethat
thefunctionmapsαtothepartialp-Wassersteindistanceandnotitspthpower.Figure2: InterpretationofdistancesbasedontheOT-profile.
cannotcontributedisproportionatelytothecost. Weestablishthefollowingpropertiesfor
ourdistancefunction:
• Metric Property: For any choice of p ≥ 1 and k ≥ 0, the distance (p,k)-RPW is
a metric. Unlike the (1−δ)-partial p-Wasserstein distance, our distance function
satisfiesthetriangleinequality. Furthermore,unlikethe(1−δ)-partial p-Wasserstein
distance,wheretwodistributionsµandνcanhaveacostof0eveniftheydifferbya
massofδ,foranytwodistributionsµandνwithµ ̸= ν,Π (µ,ν) > 0. SeeSection2.
p,k
• Robust to Outliers: Given two distributions µ and ν, adding a mass of δ to ν will
change Π (µ,ν) byatmost ±δ. Inotherwords, anoutliermassof δ = 0.01cannot
p,k
increasetheΠ (µ,ν)bymorethan0.01. Recollectthatthiscanbeashighas0.1for
p,k
2-Wassersteindistanceand0.21for3-Wassersteindistance. SeeSection3.1.
• RobusttoSamplingDiscrepancy: In2d,the(p,1)-RPWbetweenempiricaldistributions
− p
converges to the true (p,1)-RPW distance at a rate of n 4p−2. In contrast, the 2-
Wassersteindistanceconvergesatarateofn−1/2p. SeeSection3.2.
Alternatively, in Figure 2, suppose point (x∗,y∗) is the intersection point of the line
y = k(1−x) with the OT-profile. Then, our distance is simply (1−x∗). Note that when
k = 0, our distance becomes the total variation distance. When we set k to be sufficiently
large, our distance becomes W (µ,ν)/k. In this sense, our distance interpolates between
p
the total variation distance and the p-Wasserstein distance. By choosing the parameters k
or pcorrectly,wecanreduceourdistancetoseveralwell-knowndistances.
• Relation to total variation distance: When k = 0 and for any p ≥ 1, (p,k)-RPW
betweenanytwodistributionsµandνisequaltothetotalvariationdistancebetween
µandν. SeeLemma4.2.
• Relation to p-Wasserstein distance: For any p ≥ 1, as k → ∞ , k × Π (µ,ν)
p,k
approachesthe p-Wassersteindistance. SeeLemma4.3.
• Relation to Lévy-Prokhorov distance: (∞ ,1)-RPW between any two distributions µ
andνisequaltotheirLévy-Prokhorovdistance. SeeLemma4.1.In our experiments, we use our distance from a query image to rank images in a
database of noisy images. Using this, we identify the top ten images in our database that
are similar to the query. For the MNIST and CIFAR10 datasets, our distance produces
a higher accuracy in comparison to the accuracy produced by the 1-Wasserstein, 2-
Wasserstein,andtheTVdistances.
1.1 Notations. For any distribution µ defined over a compact set X, let M(µ) :=
(cid:82)
dµ(x)denotethemassofµ. Foranypairofdistributionsµandνdefinedoverametric
X
space (X,c) and parameters p ≥ 1 and α ∈ [0,1], let W (µ,ν) denote the α-partial p-
p,α
Wasserstein distance between µ and ν. For any transport plan γ from µ to ν, define the
(cid:16)(cid:82) (cid:17)1/p
costofγasw (γ) := c(x,y)pdγ(x,y) .
p X×X
2 RobustPartial p-WassersteinMetric
For any parameters p ≥ 1 and k ≥ 0, we define the (p,k)-Robust Partial p-Wasserstein
distanceorsimply(p,k)-RPW betweenµandν,denotedbyΠ (µ,ν),tobetheminimum
p,k
valueε ≥ 0suchthatthe(1−ε)-partial p-Wassersteindistancebetweenµandνisatmost
kε;moreprecisely,
Π p,k(µ,ν) = inf{ε ≥ 0 | W p,1−ε(µ,ν) ≤ kε}. (2.1)
Alternatively,letP = (x∗,y∗)betheintersectionpointoftheOT-profilecurvewiththeline
y = k(1−x). Then,(p,k)-RPWbetweenµandνwouldbeΠ (µ,ν) = 1−x∗.
p,k
We show that (p,k)-RPW distance satisfies all the metric properties. The triangle
inequality is the only property for which the proof is non-trivial. We provide a sketch
oftheproofbelow;seeSectionA.1fordetails.
For any three probability distributions µ, ν, and κ, suppose Π (µ,κ) = ε and
p,k 1
Π (κ,ν) = ε . Let γ (resp. γ ) denote a (1−ε )-partial OT plan (resp. (1−ε )-partial
p,k 2 1 2 1 2
OTplan)fromµtoκ (resp. fromκ toν). InFigure3,theblobsintheleft,middle,andright
showthedistributionsµ,κ,andν,respectively,andtheblue(resp. red)arrowscorrespond
to the transport plan γ (resp. γ ). Let κ (resp. κ ) be the mass of κ that is transported
1 2 1 2
fromµ(resp. toν)byγ (resp. γ )(showninFigure3bytheblue(resp. red)regioninside
1 2
thedistributionκ). Defineκ tobethedistributionofmassofκ thatiscommontobothκ
c 1
andκ (thepurpleregioninsidethedistributionκ inFigure3). Notethatthetotalmassof
2
κ thatisnottransportedbyγ isatmostε ;therefore,
1 2 2
M(κ ) ≥ M(κ )−ε = 1−ε −ε . (2.2)
c 1 2 1 2
Defineµ (resp. ν )tobethedistributionwhosemassistransportedto(resp. from)κ inγ
c c c 1
(resp. γ ). InFigure3,thedistributionµ (resp. ν )isdepictedbythepurpleregioninside
2 c c
distributionµ(resp. ν). FromEquation(2.2),
M(µ ) = M(ν ) = M(k ) ≥ 1−ε −ε (2.3)
c c c 1 2
Therefore,wehave
W p,1−ε1−ε2(µ,ν) ≤ W p(µ c,ν c) ≤ W p(µ c,κ c)+W p(κ c,ν c) ≤ k(ε 1+ε 2). (2.4)
The second inequality follows from the triangle inequality of p-Wasserstein distances
and the third inequality follows from the definition of our distance. Furthermore, sinceFigure3: ThetriangleinequalityoftheRPWdistancefunction.
Π p,k(µ,ν) is the minimum ε with W p,1−ε(µ,ν) ≤ kε, from Equation (2.4), Π p,k(µ,ν) ≤
ε +ε ,asdesired.
1 2
LEMMA 2.1. Given a metric space (X,c) with a unit diameter and any parameters p ≥ 1 and
k ≥ 0, the (p,k)-RPW distance function Π (·,·) for all probability distributions defined over
p,k
(X,c)isametric.
The following lemma highlights a useful feature of our metric, which is used in
derivingitsimportantproperties.
LEMMA 2.2. Given two distributions µ and ν defined over a metric space (X,c) with a unit
diameterandparameters p ≥ 1and k ≥ 0, supposeW p,1−α(µ,ν) = kβ forsome α,β ≥ 0. Then,
Π (µ,ν) ≤ max{α,β}. Furthermore,ifk ̸= 0,thenmin{α,β} ≤ Π (µ,ν).
p,k p,k
3 RobustnessProperties
InSection3.1,weshowthatanoutliermassofδcannotincreasetheRPWdistancebymore
than δ, i.e., theRPWdistanceisrobusttooutliers. InSection3.2, weshowthattherateof
convergence of the empirical RPW distance to the real RPW distance is asymptotically
smaller than the rate of convergence for p-Wasserstein distance. Thus, we show that
the RPW distance is more robust to outlier as well as sampling discrepancies than the
p-Wassersteindistance.
3.1 Robustness to Outlier Noise. For δ ∈ (0,1), let ν˜ := (1− δ)ν + δν′ be a noisy
distribution obtained from ν contaminated with δ mass from a noise distribution ν′. In
Lemma 3.1, we show that by distorting the noise distribution ν′, an adversary cannot
arbitrarilychangethe(p,k)-RPWdistancebetweenµandν˜.
LEMMA 3.1. For any distributions µ,ν, and ν′ defined over a metric space (X,c) with a unit
diameterandparameters p ≥ 1,k ≥ 0,andδ ∈ (0,1),letν˜ = (1−δ)ν+δν′. Then,
Π (µ,ν)−δ ≤ Π (µ,ν˜) ≤ (1−δ)Π (µ,ν)+δ.
p,k p,k p,k
For a distribution µ and a noisy distribution µ˜ that differs from µ by only a δ fraction
ofmass(i.e.,∥µ−µ˜∥ = δ),considerthefollowingassumption:
TV
(A1) The(1− δ )-partial p-Wassersteindistancebetweenµandµ˜ isatleast 1W (µ,µ˜).
10 2 pAssuming(A1),inthefollowinglemma,weshowthatthe(p,k)-RPWdistancebetweenµ
(cid:8) (cid:9)
andµ˜ isproportionatetomin δ, 1W (µ,µ˜) .
k p
LEMMA 3.2. For a distribution µ defined over a metric space (X,c) with a unit diameter and
δ > 0,letµ˜ beadistributionthatdiffersfromµbyaδfractionofmasssatisfyingassumption(A1).
Then,foranyparameters p ≥ 1andk > 0,
(cid:18) (cid:26) (cid:27)(cid:19)
1
Π (µ,µ˜) = Θ min δ, W (µ,µ˜) .
p,k p
k
Intuitively,ifµ˜ isonlyslightlydifferentfromµ,i.e.,W (µ,µ˜) ≤ kδ,thenthesensitivity
p
of our metric would be similar to that of the p-Wasserstein distance. On the other hand,
if µ˜ is far from µ (i.e., the δ fraction of the mass of µ˜ that is different from µ is an outlier
noiseanddisproportionatelyincreasesthe p-Wassersteindistance), thenthesensitivityof
(p,k)-RPWisboundedbyδ.
3.2 Robustness to Sampling Discrepancies. Next, we show that in the 2-dimensional
Euclidean space, the rate of convergence of the empirical (p,1)-RPW to the true distance
is
O˜(n− 4pp
−2), which is significantly faster than the convergence rate of
O˜(n− 21
p) of the
empirical p-Wasserstein distance [16]2. In particular, for p = ∞ , the convergence rate
of our metric isO˜(n−1 4), whereas the empirical p-Wasserstein distance does not converge
to the true distance. For simplicity in presentation, we restrict our analysis to p = 2. Our
bounds for any p ≥ 1 and d ≥ 2 are stated in Corollary 3.1 and its proof is provided in
AppendixA.2.
LEMMA 3.3. Foranytwodistributions µ and ν definedoverametricspacewithaunitdiameter,
suppose µ and ν are two empirical distributions of µ and ν, respectively. Then, with a high
n n
probability,
|Π 2,1(µ,ν)−Π 2,1(µ n,ν n)| = O˜(n−1 3).
Notethatforanypairofdistributionsµandνandtheirempiricaldistributionsµ and
n
ν ,bythetriangleinequality,
n
|Π (µ,ν)−Π (µ ,ν )| ≤ Π (µ,µ )+Π (ν,ν ).
2,1 2,1 n n 2,1 n 2,1 n
Therefore,toproveLemma3.3,webound(2,1)-RPWofanydistributionµtoitsempirical
distributionµ inthefollowinglemma.
n
LEMMA 3.4. Given a continuous distribution µ in the 2-dimensional Euclidean space and an
empiricaldistributionµ
n
ofµ,Π 2,1(µ,µ n) = O˜(n−1 3)withahighprobability.
WebeginbydefiningasetofnotationsthatassistinprovingLemma3.4. LetGbeagrid
with cell side length n−α inside the unit square. For any cell □ ∈ G, let µ(□) denote the
massofµinside□ . Definetheexcessmassofacell□ asExc (□) := max{0,µ(□)−µ (□)}
µ n
and the excess of the grid G, denoted by Exc (G), as the total excess of all cells of G, i.e.,
µ
Exc µ(G) := ∑ □∈GExc µ(□). Whenµisclearfromcontext,wesimplifynotationanddenote
theexcessofG byExc(G). Lemma3.5boundstheexcessofG.
2O˜()hidespoly(d,logn)fromtheconvergencerate.(a) (b) (c)
Figure 4: (a) A distribution µ (shaded gray area) and an empirical distribution µ (red
n
dots), (b) γ transports as much mass as possible inside each cell of G (the red regions
1 1
show the mass of µ that is transported to µ inside each cell of G ), (c) for the remaining
n 1
mass, γ transports as much mass as possible inside the cells of G (red regions show the
2 2
massofµ1 thatistransportedtoµ1 byγ ).
n 2
LEMMA 3.5. For any distribution µ inside the unit d-dimensional hypercube, an empirical
distribution µ
n
of µ, and a grid G with cell side length n−α, Exc µ(G) = O˜(nd 2α−1 2) with a high
probability.
TobetterexplainourproofforLemma3.4,wefirstpresentaweakerboundofO˜(n− 13 0).
Intheappendix,weimproveouranalysisandobtainarateofO˜(n−1 3).
A weaker bound for Lemma 3.4. In the following, we construct a transport plan γ
that transports all except O˜(n− 13 0) mass with a cost of O˜(n− 13 0). Having computed such
transportplan,wethenuseLemma2.2toconcludethatΠ p,k(µ,ν) = O˜(n− 13 0). Thedetails
areprovidednext.
Define G
1
(resp. G 2) to be a grid with cell side length O(n−α1) (resp. O(n−α2)) for
α := 3 (resp. α := 1). ThegridsG andG areconstructedinawaythattheirboundaries
1 10 2 5 1 2
arealignedwitheachother. Letγ beapartialtransportplanthatarbitrarilytransports,for
1
any cell □ ∈ G , a mass of min{µ(□),µ (□)} from µ to µ. Define µ1 (resp. µ1) to be the
1 n n n
distributionofmassofµ(resp. µ )thatisnottransportedbyγ . Letγ beatransportplan
n 1 2
that transports, for any cell □ ∈ G , a mass of min{µ1(□),µ1(□)} from µ1 to µ1. Define
2 n n
γ = γ +γ . This completes the construction of γ. In the appendix, instead of two grids,
1 2
weconsiderO(loglogn)gridsandobtaintheboundclaimedinLemma3.4.
The transport plan γ transports as much mass as possible inside each cell of G ;
2
therefore, the total mass that is not transported by γ is equal to the excess of the grid
G ,whichfromLemma3.5is
2
1−M(γ) = Exc(G 2) = O˜(nα2−1 2) = O˜(n− 13 0). (3.5)
Next, we show that the cost w 2(γ) of the transport plan γ is O˜(n− 13 0). Recall that
γ = γ
1
+γ 2. In our analysis, we first show that w 22(γ 1) = O(n−3 5) and then show that
w 22(γ 2) = O(n− 53). Using these two bounds, we then conclude that w 22(γ) = O(n−3 5), orequivalently,w 2(γ) = O(n− 13 0).
Since all mass transportation in γ
1
has a squared cost of O(n−2α1) = O(n−3 5), we
get w 22(γ 1) = O(n− 53). Furthermore, by Lemma 3.5, with a high probability, M(µ1) =
O˜(nα1− 21) = O˜(n−1 5). Sincethetransportplanγ
2
transportsatmostM(µ1)mass,eachata
squaredcostofO(n−2α2) = O(n−2 5),
w 22(γ 2) = O˜(cid:0) M(µ1)×n−2α2(cid:1) = O˜(n−3 5).
Asaresult,thecostofγis
(cid:113)
w 2(γ) = w 22(γ 1)+w 22(γ 2) = O˜(n− 13 0). (3.6)
NotethatγisatransportplanthattransportsallexceptO˜(n− 13 0)mass(Equation(3.5))and
hascostO˜(n− 13 0)(Equation(3.6)). Hence,
W 2,1−α(µ,µ n) ≤ w 2(γ) = O˜(n− 13 0).
PluggingintoLemma2.2,Π 2,1(µ,µ n) = O˜(n− 13 0).
Intheappendix,weuseanidenticalapproachtoobtaintherateofconvergenceforany
dimensiond ≥ 1andanyparameter p ≥ 1. Thefollowinglemmasummarizestheresults.
LEMMA 3.6. Givenacontinuousdistributionµinthed-dimensionalEuclideanspace,anempiri-
caldistributionµ ofµ,andparameters p ≥ 1andk > 0constant,withahighprobability,
n
(cid:40)
O˜(n− d1), p ≤ d,
Π p,k(µ,µ n) =
O˜(n− p(d+p
2)−d), p >
d2
.
2
Foranypairofdistributionsµandνandtheirempiricaldistributionsµ andν ,bythe
n n
triangleinequality,
|Π (µ,ν)−Π (µ ,ν )| ≤ Π (µ,µ )+Π (ν,ν ).
p,k p,k n n p,k n p,k n
CombinedwithLemma3.6,wegetthefollowingcorollary.
COROLLARY 3.1. Fortwodistributionsµandνdefinedoverametricspacewithaunitdiameter,
suppose µ and ν are two empirical distributions of µ and ν, respectively. Then, for any p ≥ 1
n n
andk > 0constant,withahighprobability,
(cid:40)
O˜(n−1 d), p ≤ d,
|Π p,k(µ,ν)−Π p,k(µ n,ν n)| = O˜(n− p(d+p 2)−d), p > 2 d.
2
4 RelationtoOtherDistances
In this section, we discuss the relation of the (p,k)-RPW metric with three other well-
known distance functions, namely (i) Lévy-Prokhorov distance, (ii) total variation, and
(iii) p-Wasserstein distance. In particular, we first show that the (∞ ,1)-RPW is the same
as the Lévy-Prokhorov distance. We next show that for any p ≥ 1, the (p,k)-RPW metric
isaninterpolationbetweentotalvariationandthe p-Wassersteindistance. Moreprecisely,
Π (·,·)isthesameasthetotalvariationdistance,andforlargevaluesofk,the(p,k)-RPW
p,0
willbecloseto 1W (·,·).
k pLévy-Prokhorov distance. For any two distributions µ and ν defined over a set X in
the d-dimensional Euclidean space, let π(µ,ν) denote the Lévy-Prokhorov distance of µ
and ν. Inthefollowinglemma, weshowthatthe (∞ ,1)-RPWmetricisequaltotheLévy-
Prokhorovdistance. Theproofofthislemma,whichisprovidedintheappendix,issimilar
totheapproachdescribedbyLahnetal.[23].
LEMMA 4.1. Foranypairofprobabilitydistributionsµandνinametricspace(X,c)withaunit
diameter,Π ∞,1(µ,ν) = π(µ,ν).
Total Variation. For any pair of distributions µ and ν, let ∥µ−ν∥ denote the total
TV
variation of µ and ν. In Lemma 4.2, we show that for any p ≥ 1, the (p,0)-RPW distance
between µ and ν is equal to their total variation. Intuitively, the (p,0)-RPW distance
measures the maximum amount of mass that can be transported from µ to ν at a 0 cost,
i.e., (p,0)-RPWdistanceistheamountofmassof µ and ν thatoverlap, whichisthesame
astheirtotalvariation.
LEMMA 4.2. Foranytwodistributionsµandνinametricspace(X,c)withaunitdiameterand
anyparameter p ≥ 1,Π (µ,ν) = ∥µ−ν∥ .
p,0 TV
p-Wassersteindistance. Finally,weshowthatforlargeenoughvaluesof k,the (p,k)-
RPWmetricwouldbecloseto 1W (µ,ν).
k p
LEMMA 4.3. For any two distributions µ and ν over a metric space (X,c) with a unit diameter
and any parameters p ≥ 1 and k > 0, Π (µ,ν) ≤ 1W (µ,ν). Furthermore, if k ≥ 1, then
p,k k p
1 kW p(µ,ν) ≤ Π
p,k(µ,ν)+k−p+ p1
.
5 AlgorithmstoCompute(p,k)-RPW
In this section, we describe two approximation algorithms for computing the (p,k)-
RPW distance between two discrete distributions defined over supports of n points.
The first algorithm uses a binary search on the value of Π (µ,ν) and computes a δ-
p,k
additive approximation of (p,k)-RPW (or simply δ-close (p,k)-RPW) for any δ > 0 in
O(n3lognlogδ−1) time. Our second algorithm relies on the algorithm by [22] (LMR
algorithm)toapproximatetheOT-profileandcomputesaδ-additiveapproximationofour
metricinO(n2 + n )time.
δp δ2p
Note that when k = 0, as discussed in Lemma 4.2, the (p,0)-RPW is simply the total
variationdistanceandcanbecomputedondiscretedistributionsinlineartime. Hence,in
thefollowing,weassumek > 0.
Highly-Accurate Algorithm. For any ε ∈ [0,1], computing the (1 − ε)-partial p-
Wasserstein distance for discrete distributions can be done using a standard OT solver
in an augmented space [7], which takes O(n3logn) time [12, 32]. Therefore, one can use
asimplebinarysearchonthevalueof(p,k)-RPWtoobtainaδ-additiveapproximationin
O(n3lognlogδ−1)time. ThedetailsareprovidedinAppendixA.4.
ComputingThroughanApproximateOT-Profile. InLemmaA.3intheappendix,we
showthatcomputingourmetricusingaδ′-additiveapproximationoftheOT-profileleads
to a
2δ′
-close (p,k)-RPW. Phatak et al. [33] showed that for any α ∈ [0,1] and parameter
kδ′ > 0, the LMR algorithm computes a (δ′)1/p-additive approximation of the OT-profile.
Therefore, to compute a δ-close (p,k)-RPW distance function, we use the LMR algorithm
withanerrorparameterδ′ = (kδ)p toapproximatetheOT-profileandtocomputeaδ-close
2
Π (µ,ν)inO(n2 + n )time.
p,k δp δ2p
6 ExperimentalResults
In this section, we conduct experiments to show that our distance is robust to noise from
outliersandsamplediscrepancies.
Inourfirstexperiment,weusetheTVdistance,1-Wassersteindistance,2-Wasserstein
distance, (2,1)-RPW, and (2,0.1)-RPW to rank images from the MNIST [26] and CIFAR-
10 [21] datasets and measure the accuracy of the results. In our second experiment, we
measuretheconvergencerateofempirical(2,k)-RPWdistancetothetrue(2,k)-RPWand
compare it with the convergence rate for 2-Wasserstein distance for synthetic data sets.
For both experiments, we compute an additive approximation of the RPW metric using
theLMRalgorithm[22].
6.1 ImageRetrieval. FollowingtheexperimentalsetupintroducedbyRubneretal.[34],
weconductexperimentsonretrievingimagesusing(2,1)-RPWand(2,0.1)-RPWdistances
andcomparetheiraccuracyagainstthe1-Wasserstein,2-Wasserstein,andTVdistances. In
this experiment, given a dataset of labeled images and a set of unlabeled query images,
the goal is to retrieve, for each query image, a set of m similar images from the labeled
dataset. The accuracy of a distance function in the image retrieval task is then computed
as the ratio of the retrieved images with the correct label, averaged over all retrievals for
allqueryimages. Inourexperiments,wevarythevalueofmfrom1to100.
Datasets. Weconducttheexperimentsusingtwodatasets,namely,MNISTandCIFAR-
10. Inourexperimentsoneachdataset,werandomlyselect2kimagesasthelabeleddataset
andrandomlyselect50imagesasquery. Foreachdataset,weintroducethreescenariosof
perturbation:
(i) (noise in datasets) In this scenario, we add random noise to the images in the
datasets. FortheMNISTdataset,foreachimage,weaddarandomamountofnoise
between 0% and 10% to a random pixel in the image. For CIFAR-10, we replaced
randomlyselected10%ofpixelswithwhitepixelsineachimage,
(ii) (shiftindatasets)Inthisscenario,weshiftupeachlabeledimageby2pixelsforthe
MNIST dataset and increase the intensity of a random RGB channel by 20 in each
imageoftheCIFAR-10dataset,and,
(iii) (noise and shift in datasets) In the last scenario, we introduce both random noise
(scenario(i))andrandomshift(scenario(ii))totheimagesindatasets.
Results. TheresultsofthisexperimentareprovidedinFigure5. Thetop(resp. bottom)
row corresponds to our experiments on the MNIST (resp. CIFAR-10) dataset in the three
scenarios described above. In each plot, the horizontal axis is the number m of retrieved
images for each query, and the vertical axis is the accuracy of the retrieved images. The
resultsofourexperimentssuggestthatourmetricperformsbetterthanthe1-Wasserstein,
2-Wasserstein,andtheTVdistancesforthetaskofimageretrievalwithperturbations.(i)Noiseindatasets (ii)Shiftindatasets (iii)Noiseandshiftindatasets
Figure 5: The results of our experiments on image retrieval on (top row) MNIST dataset
and(bottomrow)CIFAR-10dataset.
First,forexperimentsonMNISTdataset,weobservethatfordatasets(ii)and(iii),the
accuracy achieved by the 2-Wasserstein distance is better than that of the 1-Wasserstein
distance. This,webelieveisduetothehighersensitivityof2-Wassersteindistance,which
enables it to detect minor differences effectively. However, for dataset (i), the presence of
noisedistorts2-Wassersteindistances,causingittounderperform.
Results for MNIST dataset (i): The TV distance, 1-Wasserstein distance, and (2,1)-RPW
distance are more robust to noise and therefore, perform better than the 2-Wasserstein
distance. The (2,1)-RPW distance outperforms 1-Wasserstein distance. This is because
it retains the higher sensitivity of 2-Wasserstein distance, which enables it to effectively
detectminordifferences.
ResultsforMNISTdataset(ii): TVdistanceisknowntobesensitivetoshifts. Asaresult,
theaccuracyofTV-distancedropssignificantly. Incontrast,the2-Wassersteindistanceand
the(2,1)-RPWdistancehandlesuchshiftsmoreeffectively.
Results for MNIST dataset (iii): The 2-Wasserstein distance is sensitive to noise and
the TV distance is sensitive to shifts. Therefore, both these distances produce lower
accuracy results. Recollect that the (2,1)-RPW distance combines the TV-distance and
the 2-Wasserstein distance and therefore, can handle both shifts and noise effectively.
Therefore,itoutperformsalldistancesinthissetting.
Results for CIFAR-10 dataset: In CIFAR-10 dataset, images that have the same labels
mayalreadyhavesignificantvariationsandshifts. Duetothesevariations, 2-WassersteinFigure6: Theconvergencerateofdifferentmetricson(left)2-pointdistributionand(right)
griddistribution.
andTVdistancesachieveloweraccuracyincomparisontothe1-Wassersteindistance. The
(2,0.1)-RPWdistance,however,outperformsthe1-Wassersteindistanceforthisdataset.
6.2 Rate of Convergence We conduct numerical experiments to compare the conver-
gence rate of the empirical (2,k)-RPW metric with that of the 2-Wasserstein distance and
TVdistanceondiscretedistributions.
Datasets. We compute the convergence rate of each metric by drawing two sets of n
i.i.dsamplesfromadiscretedistributionandcomputetheempiricaldistancebetweenthe
corresponding empirical distributions. We employ two synthetic 2-dimensional discrete
distributions,namely(i)(2-pointdistribution)adiscretedistributiondefinedover2points
aandbeachwithaprobability1/2,where∥a−b∥ = 1,and(ii)(griddistribution)adiscrete
distributiondefinedover16pointsthatareplacedinagridof4×4,whereeachpointhas
aprobabilityof 1 .
16
Inourexperiments, wevarythesamplesize n from10to106. Foreachvalueof n, we
conducttheexperiment10timesandtakethemeandistanceamongall10executions.
Results. As shown in Figure 6, experiments suggest that for both distributions, the
empirical (2,1)-RPW distance converges to 0 significantly faster than the 2-Wasserstein
distance. We also observe that for a small value of k (e.g., k = 0.1), (2,k)-RPW is close to
theTVdistance,whereasforalargevalueofk(e.g.,k = 10),the(2,k)-RPWdistancevalues
aresimilarto 1W (·,·). TheseresultsareinlinewithourtheoreticalboundsinSection4.
k 2
7 Conclusion
Inthispaper,wedesignedanewpartial p-Wassersteinbasedmetriccalledthe(p,k)-RPW
thatisrobusttooutliernoiseaswellassamplingdiscrepancies,butretainsthesensitivity
of p-Wasserstein distance in capturing the minor geometric differences in distributions.
We showed that our distance interpolates between the p-Wasserstein and TV distances
and inherits robustness to both noise and shifts in distribution. We also showed that, for
p = ∞ ,ourmetricisthesameastheLévy-Prokhorovdistance.
The main contribution of this paper is in introducing a new metric and deriving its
useful properties. Experiments suggest that the new distance is a promising alternative
to TV and p-Wasserstein distances. Including this distance into many potential machinelearninguse-cases,includingasalossfunctioninGANsorforcomputingbarycentersofa
setofdistributions,remainsanopenquestion.
Acknowledgement
We would like to acknowledge Advanced Research Computing (ARC) at Virginia Tech,
which provided us with the computational resources used to run the experiments. Re-
search presented in this paper was funded by NSF CCF-1909171 and NSF CCF-2223871.
Wethanktheanonymousreviewersfortheirusefulfeedback.
References
[1] DavidAlvarez-MelisandTommiSJaakkola. Gromov-Wassersteinalignmentofword
embeddingspaces. arXivpreprintarXiv:1809.00013,2018.
[2] Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative ad-
versarial networks. In Proc. 34th International Conference on Machine Learning, pages
214–223,2017.
[3] ArtursBackurs, Yihe Dong, Piotr Indyk, Ilya Razenshteyn, andTal Wagner. Scalable
nearestneighborsearchforoptimaltransport. InProc.37thInternationalConferenceon
MachineLearning,pages497–506,2020.
[4] MohitBansilandJunKitagawa. W∞-transportwithdiscretetargetasacombinatorial
matchingproblem. ArchivderMathematik,117(2):189–202,2021.
[5] Luis A Caffarelli and Robert J McCann. Free boundaries in optimal transport and
Monge-Ampereobstacleproblems. AnnalsofMathematics,pages673–730,2010.
[6] Wanxing Chang, Ye Shi, and Jingya Wang. Csot: Curriculum and structure-aware
optimaltransportforlearningwithnoisylabels.ArXivpreprintarXiv:2312.06221,2023.
[7] Laetitia Chapel, Mokhtar Z Alaya, and Gilles Gasso. Partial optimal tranport with
applicationsonpositive-unlabeledlearning. AdvancesinNeuralInformationProcessing
Systems,33:2903–2913,2020.
[8] Jaemoo Choi, Jaewoong Choi, and Myungjoo Kang. Generative modeling through
the semi-dual formulation of unbalanced optimal transport. Advances in Neural
InformationProcessingSystems,36,2024.
[9] Ching-Yao Chuang, R Devon Hjelm, Xin Wang, Vibhav Vineet, Neel Joshi, Antonio
Torralba, Stefanie Jegelka, and Yale Song. Robust contrastive learning against noisy
views. InProc.IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
16670–16681,2022.
[10] SebastianClaici,EdwardChien,andJustinSolomon. Stochasticwassersteinbarycen-
ters. InInternationalConferenceonMachineLearning,pages999–1008.PMLR,2018.
[11] Marco Cuturi and Arnaud Doucet. Fast computation of wasserstein barycenters. In
InternationalConferenceonmachinelearning,pages685–693.PMLR,2014.[12] Jack Edmonds and Richard M Karp. Theoretical improvements in algorithmic
efficiencyfornetworkflowproblems. J.oftheACM,19(2):248–264,1972.
[13] NabilElMalki,RobinCugny,OlivierTeste,andFranckRavat. Decwa: Density-based
clustering using wasserstein distance. In Proc. 29th ACM International Conference on
Information&KnowledgeManagement,pages2005–2008,2020.
[14] Peyman Mohajerin Esfahani and Daniel Kuhn. Data-driven distributionally robust
optimization using the Wasserstein metric: Performance guarantees and tractable
reformulations. Math.Prog.,171(1):115–166,2018.
[15] AlessioFigalli. Theoptimalpartialtransportproblem. ArchiveforRationalMechanics
andAnalysis,195(2):533–560,2010.
[16] Nicolas Fournier and Arnaud Guillin. On the rate of convergence in Wasserstein
distanceoftheempiricalmeasure. Probabilitytheoryandrelatedfields,162(3-4):707–738,
2015.
[17] Aude Genevay, Gabriel Peyre, and Marco Cuturi. Learning generative models with
Sinkhorndivergences. InInternationalConferenceonArtificialIntelligenceandStatistics,
page1608–1617,2018.
[18] Rishi Gupta, Piotr Indyk, and Eric Price. Sparse recovery for earth mover distance.
InProc.48thAnnualAllertonConferenceonCommunication,Control,andComput.,pages
1742–1744.IEEE,2010.
[19] Gao Huang, Chuan Guo, Matt J Kusner, Yu Sun, Fei Sha, and Kilian Q Weinberger.
Supervised word mover’s distance. Advances in neural information processing systems,
29,2016.
[20] Hicham Janati, Marco Cuturi, and Alexandre Gramfort. Wasserstein regularization
for sparse multi-task regression. In 22nd International Conference on Artificial Intelli-
genceandStatistics,pages1407–1416.PMLR,2019.
[21] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from
tinyimages. 2009.
[22] NathanielLahn, DeepikaMulchandani, andSharathRaghvendra. Agraphtheoretic
additive approximation of optimal transport. In Advances in Neural Information
ProcessingSystems,pages13813–13823,2019.
[23] NathanielLahn,SharathRaghvendra,andJiachengYe.Afastermaximumcardinality
matching algorithm with applications in machine learning. Advances in Neural
InformationProcessingSystems,34:16885–16898,2021.
[24] Zhengfeng Lai, Chao Wang, Sen-ching Cheung, and Chen-Nee Chuah. Sar: Self-
adaptive refinement on pseudo labels for multiclass-imbalanced semi-supervised
learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages4091–4100,2022.[25] KhangLe,HuyNguyen,QuangMNguyen,TungPham,HungBui,andNhatHo. On
robust optimal transport: Computational complexity and barycenter computation.
AdvancesinNeuralInformationProcessingSystems,34:21947–21959,2021.
[26] Yann LeCun. The mnist database of handwritten digits. http://yann. lecun.
com/exdb/mnist/,1998.
[27] GiuliaLuise,AlessandroRudi,MassimilianoPontil,andCarloCiliberto. Differential
properties of Sinkhorn approximation for learning with Wasserstein distance. Ad-
vancesinNeuralInformationProcessingSystems,31,2018.
[28] Debarghya Mukherjee, Aritra Guha, Justin M Solomon, Yuekai Sun, and Mikhail
Yurochkin. Outlier-robust optimal transport. In International Conference on Machine
Learning,pages7850–7860.PMLR,2021.
[29] SloanNietert,ZivGoldfeld,andRachelCummings. Outlier-robustoptimaltransport:
Duality, structure, and statistical analysis. In International Conference on Artificial
IntelligenceandStatistics,pages11691–11719.PMLR,2022.
[30] Sloan Nietert, Ziv Goldfeld, and Soroosh Shafiee. Outlier-robust wasserstein dro.
ArXivpreprintarXiv:2311.05573,2023.
[31] MaximeOquab, TimothéeDarcet, Théo Moutakanni, HuyVo, MarcSzafraniec, Vasil
Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby,
et al. Dinov2: Learning robust visual features without supervision. arXiv preprint
arXiv:2304.07193,2023.
[32] James Orlin. A faster strongly polynomial minimum cost flow algorithm. In Proc.
20thAnnualACMSympos.TheoryofComput.,pages377–387,1988.
[33] AbhijeetPhatak,SharathRaghvendra,ChittaranjanTripathy,andKaiyiZhang. Com-
putingalloptimalpartialtransports. InTheEleventhInternationalConferenceonLearn-
ingRepresentations,2022.
[34] YossiRubner, CarloTomasi, andLeonidasJGuibas. Theearthmover’sdistanceasa
metricforimageretrieval. InternationalJ.ofComput.Vision,40(2):99,2000.
[35] Tim Salimans, Han Zhang, Alec Radford, and Dimitris Metaxas. Improving GANs
usingoptimaltransport. InInternationalConferenceonLearningRepresentations,2018.
[36] HaykSedrakyanandNairiSedrakyan.TheHM-GM-AM-QMInequalities,pages21–43.
SpringerInternationalPublishing,2018.
[37] TomasVaskeviciusandLénaïcChizat. Computationalguaranteesfordoublyentropic
wasserstein barycenters. In Proc. 37th Conference on Neural Information Processing
Systems,2023.
[38] Cédric Vincent-Cuaz, Rémi Flamary, Marco Corneli, Titouan Vayer, and Nicolas
Courty. Semi-relaxed gromov-wasserstein divergence with applications on graphs.
arXivpreprintarXiv:2110.02753,2021.[39] Mikhail Yurochkin, Sebastian Claici, Edward Chien, Farzaneh Mirzazadeh, and
Justin M Solomon. Hierarchical optimal transport for document representation.
Advancesinneuralinformationprocessingsystems,32,2019.
[40] Yubo Zhuang, Xiaohui Chen, and Yun Yang. Wasserstein k-means for clustering
probabilitydistributions. AdvancesinNeuralInformationProcessingSystems,35:11382–
11395,2022.A MissingProofsandDetails
A.1 Missing Proofs of Section 2. In this section, we provide the proofs for Lemmas 2.1
and2.2.
LEMMA 2.1. Given a metric space (X,c) with a unit diameter and any parameters p ≥ 1 and
k ≥ 0, the (p,k)-RPW distance function Π (·,·) for all probability distributions defined over
p,k
(X,c)isametric.
Proof. To prove this lemma, we show that (p,k)-RPW satisfies all four properties of the
metricspaces,namely(i)identity,(ii)positivity,(iii)symmetry,and(iv)triangleinequality,
andconcludethatitismetric.
For any probability distribution µ defined over the support X, W (µ,µ) = 0 and
p
therefore,ε = 0satisfiestheconditioninEquation(2.1);hence,Π (µ,µ) = 0andproperty
p,k
(i)holds. Furthermore, if ν isanotherprobabilitydistributionover X thatisdistinctfrom
µ, then W (µ,ν) > 0. Hence, ε = 0 does not satisfy the condition in Equation (2.1),
p
and Π p,k(µ,ν), which is the smallest ε ≥ 0 with W p,1−ε(µ,ν) ≤ kε, will be positive and
property (ii) holds. Additionally, for any ε ≥ 0, the (1−ε)-partial p-Wasserstein distance
issymmetric,i.e.,W p,1−ε(µ,ν) = W p,1−ε(ν,µ). Therefore,fromEquation(2.1),RPWisalso
symmetric,i.e.,Π (µ,ν) = Π (ν,µ)andproperty(iii)holdsaswell.
p,k p,k
Finally, we show that RPW satisfies the triangle inequality. For any three probability
distributionsµ,ν,andκ,supposeΠ (µ,κ) = ε andΠ (κ,ν) = ε . Inthefollowing,we
p,k 1 p,k 2
showthatΠ (µ,ν) ≤ ε +ε andconcludeproperty(iv).
p,k 1 2
Note that if ε +ε ≥ 1, then the triangle inequality holds trivially since Π (µ,ν) ≤
1 2 p,k
1 ≤ ε +ε . Therefore, we assume that ε +ε < 1. Let γ denote a (1−ε )-partial OT
1 2 1 2 1 1
plan from µ to κ and define κ := γ #µ to be the mass of κ that is transported from µ by
1 1
γ ; here, # denotes the push-forward operation. Similarly, let γ denote a (1−ε )-partial
1 2 2
OT plan from κ to ν and define κ := (γ )−1#ν to be the mass of κ that is transported to
2 2
ν by γ . Then, both κ and κ are distributions over X that are dominated by κ and have
2 1 2
massesM(κ ) = 1−ε andM(κ ) = 1−ε .
1 1 2 2
Define κ to be the distribution of mass of κ that is common to both κ and κ ; more
c 1 2
precisely,foreach x ∈ X,
κ (x) = min{κ (x),κ (x)}.
c 1 2
Notethatthetotalmassofκ thatisnottransportedbyγ isatmostε ;therefore,
1 2 2
M(κ ) ≥ M(κ )−ε = 1−ε −ε . (A.1)
c 1 2 1 2
Define µ := (γ )−1#κ and ν := γ #κ tobedistributionswhosemassaretransportedto
c 1 c c 2 c
κ inγ andγ ,respectively. FromEquation(A.1),
c 1 2
M(µ ) = M(ν ) = M(k ) ≥ 1−ε −ε (A.2)
c c c 1 2
Bythetriangleinequalityofthe p-Wassersteindistance,
W (µ ,ν ) ≤ W (µ ,κ )+W (κ ,ν ). (A.3)
p c c p c c p c c
Furthermore,
W (µ ,κ ) ≤ w (γ ) ≤ kε . (A.4)
p c c p 1 1Similarly,
W (κ ,ν ) ≤ w (γ ) ≤ kε . (A.5)
p c c p 2 2
CombiningEquations(A.2),(A.3),(A.4),and(A.5),
W p,1−ε1−ε2(µ,ν) ≤ W p(µ c,ν c) ≤ W p(µ c,κ c)+W p(κ c,ν c) ≤ k(ε 1+ε 2). (A.6)
SinceΠ p,k(µ,ν)istheminimumεwithW p,1−ε(µ,ν) ≤ kε,fromEquation(A.6),Π p,k(µ,ν) ≤
ε +ε ,asdesired.
1 2
LEMMA 2.2. Given two distributions µ and ν defined over a metric space (X,c) with a unit
diameterandparameters p ≥ 1and k ≥ 0, supposeW p,1−α(µ,ν) = kβ forsome α,β ≥ 0. Then,
Π (µ,ν) ≤ max{α,β}. Furthermore,ifk ̸= 0,thenmin{α,β} ≤ Π (µ,ν).
p,k p,k
Proof. Letδ := Π (µ,ν). Weprovethislemmabyconsideringtwocases:
p,k
• If α ≤ β, then W p,1−β(µ,ν) ≤ W p,1−α(µ,ν) = kβ; hence, β satisfies the condition
in Equation (2.1), and Π (µ,ν) ≤ β = max{α,β}. Furthermore, if k > 0, then
p,k
δ ≥ α = min{α,β}becauseotherwise,ifδ < α,then
W p,1−δ(µ,ν) ≥ W p,1−α(µ,ν) = kβ ≥ kα > kδ,
whichisacontradiction.
• Otherwise, α > β and W p,1−α(µ,ν) = kβ < kα; hence, α satisfies the condition
in Equation (2.1), and Π (µ,ν) ≤ α = max{α,β}. Additionally, if k > 0, then
p,k
δ ≥ β = min{α,β},sinceotherwise,ifδ < β,then
W p,1−δ(µ,ν) ≥ W p,1−β(µ,ν) ≥ W p,1−α(µ,ν) = kβ > kδ,
whichisacontradiction.
A.2 MissingProofsandDetailsofSection3
A.2.1 RobustnesstoOutlierNosie
LEMMA 3.1. For any distributions µ,ν, and ν′ defined over a metric space (X,c) with a unit
diameterandparameters p ≥ 1,k ≥ 0,andδ ∈ (0,1),letν˜ = (1−δ)ν+δν′. Then,
Π (µ,ν)−δ ≤ Π (µ,ν˜) ≤ (1−δ)Π (µ,ν)+δ.
p,k p,k p,k
Proof. First, note that by the triangle inequality, Π (µ,ν˜) + Π (ν˜,ν) ≥ Π (µ,ν).
p,k p,k p,k
Furthermore, by the definition of ν˜, W (ν,ν˜) = 0. Therefore, by Lemma 2.2,
p,1−δ
Π (ν,ν˜) ≤ δand
p,k
Π (µ,ν˜) ≥ Π (µ,ν)−Π (ν,ν˜) ≥ Π (µ,ν)−δ.
p,k p,k p,k p,kDefineα := Π (µ,ν)andletγbea(1−α)-partialOTplanfromµtoν. Sinceν˜ isdefined
p,k
as (1−δ)ν+δν′, the transport plan γ′ := (1−δ)γ can be seen as a (1−δ)(1−α)-partial
transportplanfromµtoν˜;therefore,
W p,(1−δ)(1−α)(µ,ν˜) ≤ w p(γ′) = (1−δ)1 pw p(γ) ≤ k(1−δ)1 pα,
wherethelastinequalityholdsbythedefinitionoftheRPWdistance. UsingLemma2.2,
(cid:110) (cid:111)
Π (µ,ν˜) ≤ max 1−(1−δ)(1−α),(1−δ)1 pα = (1−δ)α+δ = (1−δ)Π (µ,ν)+δ.
p,k p,k
ThefollowinglemmahelpsinprovingLemma3.2.
LEMMA A.1. For two distributions µ and µ˜ defined over a metric space (X,c) with a unit
diameter,parameters p ≥ 1,k > 0,andaconstantα ∈ (0,1),letδ := ∥µ−µ˜∥ . Then,
TV
(cid:26) (cid:27) (cid:26) (cid:27)
1 1
min δ(1−α), W (µ,µ˜) ≤ Π (µ,µ˜) ≤ min δ, W (µ,µ˜) .
k
p,1−δ(1−α) p,k
k
p
Proof. UsingLemma2.2ondistributionsµandµ˜,
(cid:26) (cid:27)
1
Π (µ,µ˜) ≥ min δ(1−α), W (µ,µ˜) .
p,k
k
p,1−δ(1−α)
Next,sinceδ = ∥µ−µ˜∥ ,wegetW (µ,µ˜) = 0. PluggingintoLemma2.2,
TV p,1−δ
Π (µ,µ˜) ≤ δ. (A.7)
p,k
Furthermore,sinceW p,1−0(µ,µ˜) = W p(µ,µ˜),byLemma2.2,
1
Π (µ,µ˜) ≤ W (µ,µ˜). (A.8)
p,k p
k
CombiningEquations(A.7)and(A.8),
(cid:26) (cid:27)
1
Π (µ,µ˜) ≤ min δ, W (µ,µ˜) ,
p,k p
k
asclaimed.
Assuming that the assumption (A1) holds for the distributions µ and µ˜, by plugging
α = 0.9inLemmaA.1,wecanderivethefollowinglemma.
LEMMA 3.2. For a distribution µ defined over a metric space (X,c) with a unit diameter and
δ > 0,letµ˜ beadistributionthatdiffersfromµbyaδfractionofmasssatisfyingassumption(A1).
Then,foranyparameters p ≥ 1andk > 0,
(cid:18) (cid:26) (cid:27)(cid:19)
1
Π (µ,µ˜) = Θ min δ, W (µ,µ˜) .
p,k p
kA.2.2 RobustnesstoSamplingDiscrepancies
LEMMA 3.5. For any distribution µ inside the unit d-dimensional hypercube, an empirical
distribution µ
n
of µ, and a grid G with cell side length n−α, Exc µ(G) = O˜(nd 2α−1 2) with a high
probability.
Proof. First, note that if α ≥ 1, then nd 2α−1 2 ≥ 1, and the lemma statement holds trivially.
d
Therefore,fromnowon,weassumeαtobelessthan 1. Foranycell□ ofthegridG,define
d
p□ := µ(□) tobethetotalprobabilitymassof µ inside□ , i.e., theprobabilitythatapoint
drawnfrom µ liesinside□ . Anycell□ ∈ G isconsideredasparsecellif p□ ≤ 9logn , and
n
a dense cell otherwise. Let G (resp. G ) denote the subset of sparse (resp. dense) cells
S D
of G. For each sparse cell □ , Exc µ(□) ≤ p□ ≤ 9lo ngn ; therefore, using α < 1 d, the total
contributionofsparsecellstotheexcessofG isatmost
logn logn
O(|G S|× ) = O(ndα× ) = O˜(ndα−1) = O˜(nd 2α−1 2).
n n
Next, we analyze the excess of the dense cells. Let X = (x ,...,x ) denote the n
1 n
samplesdrawnfromµthatwereusedtoconstructtheempiricaldistributionµ . Foreach
n
□
dense cell , let Y□ be a random variable denoting the number of samples in X that lie
□
inside . UsingtheChernoffbound,
Pr(cid:2) Y□ ≤ np□−3(cid:112) np□logn(cid:3) ≤ n− 29 .
In other words, for each □ ∈ G D, Exc µ(□) = O( n1(cid:112) np□logn) with probability at least
1−n− 29 . Therefore,withprobabilityatleast(1−n−9/2)|G D| ≥ (1−n−9/2)n ≥ 1−n−7 2,the
totalexcessofthedensecellswouldbe
(cid:32) (cid:114) (cid:33) (cid:32)(cid:114) (cid:33)
∑ Exc µ(□) = O ∑ p□logn = O logn ∑ √ p□
n n
□∈G □∈G □∈G
D D D
(cid:32)(cid:114) (cid:33)
logn (cid:113)
= O × |G D| = O˜(nd 2α−1 2),
n
√ (cid:114)
where the third equality holds since ∑ □∈ |GG D|p□ ≤ ∑ □ |∈ GG DD |p□ [36] and ∑ □∈G
D
p□ ≤ 1, and
thelastequalityholdssince|G | ≤ ndα.
D
Improved proof of Lemma 3.4. We improve our bounds for the convergence rate of
the empirical (2,1)-RPW by extending our approach and considering O(loglogn) grids
instead of two grids. In the following, we construct a transport plan γ that transports all
exceptO˜(n−1 3)masswithacostofO˜(n−1 3). WethenconcludethatΠ p,k(µ,ν) = O˜(n−1 3).
Without loss of generality, assume n = 22h for some integer h > 0. Let β := logn .
3logn−2
Define a set of h grids ⟨G 1,...,G h⟩, where each grid G
i
has a side length O(n−αi) for
(cid:0) (cid:1)
α := 1 − β 1− 1 . We construct the grids in a way that their boundaries are aligned
i 2 2i
witheachother.Let µ0 := µ and µ0 := µ . Starting from i = 1, we compute a partial transport plan
n n
γ from µi−1 to µi−1 that transports as much mass as possible inside each cell of G . We
i n i
then define µi (resp. µi) as the distribution of the mass of µi−1 (resp. µi−1) that is not
n n
transported by γ, set i ← i+1, and continue the same process until we process the last
i
gridG . Defineγ := ∑h γ. Byourconstruction,thetransportplanγtransportsasmuch
h i=1 i
massaspossibleinsideeachcellofG . Therefore,thetotalmassthatisnottransportedby
h
γisequaltotheexcessExc(G ),whichbyLemma3.5,withahighprobability,is
h
O˜(nαh−1 2) = O˜(n−β(1− 21 h) ) = O˜(n31 l− oglo ng −n 2) = O˜(n−1 3+ 3(3log1 n−2)) = O˜(n−1 3). (A.9)
Next,weanalyzethecostof γ bycomputingthecostofeachtransportplan γ separately.
i
Forγ 1,sinceeachmasstransportationhasasquaredcostofO((n−α1)2),
w 22(γ 1) = O(n−2α1) = O(n−1+β) = O(n− 32 ll oo ggn n− −2 2) = O(n−2 3+ 3(3log2 n−2)) = O(n−2 3).
Furthermore, for each i > 1, the transport plan γ transports a total mass of at most
i
M(µi−1), which is equal to the excess of the grid G i−1, and by Lemma 3.5 is O˜(nαi−1−1 2).
Since γ transports mass between points inside the same cell of G , each mass transporta-
i i
tioninγ
i
hasasquaredcostofO(n−2αi),andtherefore,
w 22(γ i) = O˜(nαi−1−1 2−2αi) = O˜(n−1+β) = O˜(n−2 3).
Therefore,
(cid:118)
w 2(γ) = (cid:117) (cid:117) (cid:116)∑h w 22(γ i) = O˜((cid:113) hn−2 3) = O˜(n−1 3). (A.10)
i=1
By Equations (A.9) and (A.10), we have computed a transport plan γ from µ to µ that,
n
withahighprobability,transportsallexceptO˜(n−1 3) masswithacostO˜(n−1 3). Therefore,
Π 2(µ,µ n) = O˜(n−1 3)withahighprobability.
Weextendthesameapproachtoanydimensiond ≥ 2andany p ≥ 1inLemma3.6.
LEMMA 3.6. Givenacontinuousdistributionµinthed-dimensionalEuclideanspace,anempiri-
caldistributionµ ofµ,andparameters p ≥ 1andk > 0constant,withahighprobability,
n
(cid:40)
O˜(n− d1), p ≤ d,
Π p,k(µ,µ n) =
O˜(n− p(d+p
2)−d), p >
d2
.
2
Proof. In Lemma 4.3, we show that for any p ≥ 1 and k > 0, Π (µ,ν) ≤ 1W (µ,ν).
p,k k p
Therefore, for any constant k > 0, the convergence rate of the empirical (p,k)-RPW is
upper-boundedbytheconvergencerateoftheempirical p-Wassersteindistance. Fournier
and Guillin [16] showed that when p ≤ d, the p-Wasserstein distance achieves a
2
convergence rate of O˜(n−1 d). Therefore, our metric also achieves a convergence rate of
O˜(n− d1) inthiscase, provingtheboundclaimedinthelemmastatementfor p ≤ d. Inthe
2
remainingofthisproof,weproveourboundsfor p > d.
2Let h = log log n. Weassumethat h isaninteger. Let β := logn . Definea
2 dp 2 (p+2 dp−1)logn−p
setofhgrids⟨G 1,...,G h⟩,whereeachgridG
i
hasacellsidelengthofn−αi for
(cid:18) (cid:19)
1 2p d
α := − β 1−( )i .
i d d2 2p
FollowingtheapproachdiscussedinSection3.2,weconstructhtransportplansγ ,...,γ
1 h
anddefineatransportplanγ := ∑h γ tobeatransportplanthattransportstotalmassof
i=1 i
min{µ(□),µ (□)}insideeachcell□ ∈ G . ByLemma3.5,thetotalfreemasswithrespect
n h
toγwouldbe
(cid:32) (cid:33)
1−M(γ) = Exc(G h) = O˜ (cid:18) ndα 2h−1 2(cid:19) = O˜ (cid:16) n21− dpβ+ dpβ·( 2d p)h−1 2(cid:17) = O˜ n− (d+2l −og d p2 )n l− og1 n−d
 
1− d
= O˜  n− d+21 −d p+ (d+2−d d p+ )2 l− ogd p n−d  = O˜ (cid:16) n− (d+2p )p−d(cid:17) . (A.11)
 
Next, we bound the cost of γ by analyzing the cost of each transport plan γ,i ∈ [1,h]
i
separately. Forγ ,eachmasstransportationisinsideacellofG andhasa pthpowercost
1 1
ofO˜(n−pα1);hence,
w pp(γ 1) = O˜(n−pα1) = O˜ (cid:18) n− dp+2 dp 22 β(cid:16) 1− 2d p(cid:17)(cid:19) = O˜ (cid:18) n− (d+2p )2 p−d(cid:19) . (A.12)
Finally,foreachi > 1,thetotalmasstransportedbyγ isequaltotheexcessofG ,which
i i−1
byLemma3.5isO˜(ndαi 2−1−1 2). Eachmasstransportationin γ
i
isbetweenpointswithinthe
samecellofG
i
andthushasa pthpowercostofO˜(n−pαi). Therefore,
(cid:18) (cid:19)
w pp(γ i) = O˜(ndαi 2−1−1 2−pαi) = O˜ n− (d+2p )2 p−d . (A.13)
Addingthecostofalltransportplans,
w p(γ) = (cid:18) ∑h w pp(γ i)(cid:19)1/p = O˜(cid:0) (n− (d+2p )2 p−d loglogn)1/p(cid:1) = O˜(n− (d+2p )p−d). (A.14)
i=1
CombiningEquations(A.11)and(A.14),Π p(µ,µ n) = O˜(n− (d+2p )p−d).
A.3 Missing Proofs of Section 4. To prove Lemma 4.1, we begin by showing in
Lemma A.2 that when µ and ν are discrete distributions, Π ∞,1(µ,ν) = π(µ,ν). We then
useLemmaA.2toshowthatthesamealsoholdsforcontinuousdistributions.
LEMMA A.2. For any pair of discrete probability distributions µ and ν in a metric space (X,c)
withaunitdiameter,Π ∞,1(µ,ν) = π(µ,ν).Proof. To prove this lemma, we first show that Π ∞,1(µ,ν) ≤ π(µ,ν). We then show that
π(µ,ν) ≤ Π ∞,1(µ,ν)andconcludethelemmastatement.
Let δ := π(µ,ν) and suppose A and B denote the support of µ′ and ν′, respectively.
Definetheδ-discgraph G betweenpointsin Aand Btobeabipartitegraphwhereevery
δ
pointa ∈ Aisconnectedtoallpointsb ∈ Bwithc(a,b) ≤ δ. ForanysetSofverticesofG ,
δ
let N(S) denote the set of neighbors of S in G . By the definition of the Lévy-Prokhorov
δ
distance,foranysetofpointsS ⊆ A,µ(S) ≤ ν(N(S))+δ. Similarly,foranysubsetT ⊆ B,
ν(T) ≤ µ(N(T))+δ.
WeprovethatΠ ∞,1(µ,ν) ≤ π(µ,ν)byshowingthatthemaximumtransportplanγon
theδ-discgraphG transportsatotalmassofatleast1−δ. Inthiscase,sincealledgesofγ
δ
hasacostofatmostδ,w∞(γ) ≤ δ. Hence,the(1−δ)-partial∞ -Wassersteindistancefrom
µtoνwouldbeatmostδandΠ ∞,1(µ,ν) ≤ δ,asclaimed.
ConsiderabipartitegraphG′ obtainedfromG byaddingfakeverticesa′andb′,where
δ δ
a′ (resp. b′) has a mass of δ and is connected to all points of B (resp. A) with a cost δ. For
any subset S ⊆ A (resp. T ⊆ B), let N′(S) (resp. N′(T)) denote the set of neighbors
of S (resp. T) in G′ and suppose µ′(S) (resp. ν′(T)) denotes the total mass of points in
δ
S (resp. T) for any subset S ⊆ A∪{a′} (resp. T ⊆ B∪{b′}). By construction, for any
subset S ⊆ A, µ′(S) ≤ ν′(N′(S)) andsimilarly,foranysubset T ⊆ B, ν′(T) ≤ µ′(N′(T));
hence, by the extension of Hall’s marriage theorem [4], there exists a transport plan γ′ on
the graph G′ that transports all mass of the points in A to the points in B∪{b′}. Let γ
δ
denotethetransportplanobtainedfrom γ′ afterremovingthefakevertex b′ andallmass
transportation to b′. The transport plan γ transports at least 1−δ mass and has a cost
w∞(γ′) ≤ δ. Therefore,ifγtransportsatotalmassof1−δ′ forsomeδ′ ≤ δ,
W∞,1−δ(µ,ν) ≤ W∞,1−δ′(µ,ν) ≤ w∞(γ) ≤ δ,
which means that Π ∞,1(µ,ν) ≤ δ = π(µ,ν). We next show that Π ∞,1(µ,ν) ≥ π(µ,ν) in a
similarwayandconcludethatΠ ∞,1(µ,ν) = π(µ,ν).
Letδ := Π ∞,1(µ,ν). Letγbea(1−δ)-partialOTplanfromµtoνandletG
δ
beaδ-disk
graph on A∪B. Since w∞(γ) ≤ δ, all mass transportation by γ has a cost at most δ, i.e.,
all edges carrying a positive mass in γ are present in G . For any subset S ⊆ A, let µ be
δ S
the distribution of the mass of µ on the points in S. Define ν := γ#µ to be the subset of
S S
mass of ν that is transported from µ according to γ, and let T ⊆ B be the support of ν .
S S S
RecallthatalledgescarryingapositivemassinγarepresentinG ;therefore,allpointsin
δ
T areneighborsof S in G ,i.e., T ⊆ N(S) and ν(T ) ≤ ν(N(S)). Furthermore,since γ is
S δ S
a(1−δ)-partialOTplan,thetotalmassofµ thatisnottransportedbyγisatmostδ,and
S
hence,
µ(S) ≤ ν(T )+δ ≤ ν(N(S))+δ.
S
One can also show that for each subset T ⊆ B, ν(T) ≤ µ(N(T))+δ using an identical
argument. Therefore, by the definition of the Lévy-Prokhorov distance, π(µ,ν) ≤ δ =
Π ∞,1(µ,ν).
Inthefollowing,weshowthatforanypairof(continuous)probabilitydistributionsµ
andνandanyε > 0,
|Π ∞,1(µ,ν)−π(µ,ν)| ≤ ε (A.15)and conclude that Π ∞,1(µ,ν) = π(µ,ν) for any pair of probability distributions (discrete
orcontinuous).
Define □ to be a unit d-dimensional hypercube containing the set X. Let G be a grid
of cell side length √ε that partitions □ into smaller cells. Using the grid G, we construct
4 d
twodiscretedistributions µε and νε asfollows. Let G denotethesubsetofcellsof G that
X
intersectsthesetX. Foreachcellξ ∈ G ,wepickanarbitrarypointr insideξ∩X asthe
X ξ
representativepointofξ. LetR := (cid:83) {r }. Defineµε (resp. νε)asadiscretedistribution
ξ∈G ξ
overRthatassigns,foreachξ ∈ G ,amassofµ(ξ)(resp. ν(ξ))toitsrepresentativepoint
X
r . Thiscompletestheconstructionofµε andνε. NotethatbyLemma4.1,
ξ
Π ∞,1(µε,νε) = π(µε,νε). (A.16)
Furthermore, W∞(µ,µε) ≤ ε, since there is a transport plan that transports the mass of µ
4
inside each cell ξ ∈ G to the mass of µε at r and each mass transportation has a cost at
ξ
most 4ε. FromLemma2.2,Π ∞,1(µ,µε) ≤ 4ε. Similarly,Π ∞,1(ν,νε) ≤ 4ε. Therefore,usingthe
triangleinequality,
ε
|Π ∞,1(µ,ν)−Π ∞,1(µε,νε)| ≤ . (A.17)
2
Onecanalsoshowinasimilarwaythat
ε
|π(µ,ν)−π(µε,νε)| ≤ . (A.18)
2
WeconcludeEquation(A.15)bycombiningEquations(A.16),(A.17),and(A.18).
LEMMA 4.2. Foranytwodistributionsµandνinametricspace(X,c)withaunitdiameterand
anyparameter p ≥ 1,Π (µ,ν) = ∥µ−ν∥ .
p,0 TV
Proof. WeprovethislemmabyfirstshowingthatΠ (µ,ν) ≤ ∥µ−ν∥ andthenshowing
p,0 TV
that∥µ−ν∥ ≤ Π (µ,ν).
TV p,0
Let P(X) denote the set of all probability measures defined over the compact set X.
Nietert et al. [30] showed that one can rewrite the (1−ε)-partial p-Wasserstein distance
betweenµandνas
W p,1−ε(µ,ν) = inf W p(µ′ ,ν). (A.19)
µ′∈P(X):∥µ−µ′∥ TV≤ε
Defineδ = ∥µ−ν∥ . Pluggingε = δinEquation(A.19),
TV
W (µ,ν) = 0. (A.20)
p,1−δ
Therefore,byLemma2.2,
Π (µ,ν) ≤ max{0,δ} = δ = ∥µ−ν∥ . (A.21)
p,0 TV
Next,let δ′ = Π p,0(µ,ν). Bydefinitionofthe (p,0)-RPW,W p,1−δ′(µ,ν) ≤ 0×δ′ = 0(since
the parameter k is set to 0), and since the partial p-Wasserstein distance is non-negative,
W p,1−δ′(µ,ν) = 0. Therefore,
0 = W p,1−δ′(µ,ν) = inf W p(µ′ ,ν). (A.22)
µ′∈P(X):∥µ−µ′∥ TV≤δ′Let µ∗ be the distribution realizing the infimum in Equation (A.22). Then, W (µ∗,ν) = 0,
p
andbythemetricpropertiesofthe p-Wassersteindistance,µ∗ = ν;hence,
∥µ−ν∥ = ∥µ−µ∗∥ ≤ δ′ = Π (µ,ν). (A.23)
TV TV p,0
CombiningEquations(A.21)and(A.23),Π (µ,ν) = ∥µ−ν∥ .
p,0 TV
LEMMA 4.3. For any two distributions µ and ν over a metric space (X,c) with a unit diameter
and any parameters p ≥ 1 and k > 0, Π (µ,ν) ≤ 1W (µ,ν). Furthermore, if k ≥ 1, then
p,k k p
1 kW p(µ,ν) ≤ Π
p,k(µ,ν)+k−p+ p1
.
Proof. Letδ′ := W (µ,ν). Inthiscase,
p
δ′
W p,1−min{1,δ′ }(µ,ν) ≤ W p(µ,ν) = k× k.
k
Therefore, by Lemma 2.2, Π (µ,ν) ≤ max{min{1, δ′ }, δ′ } = 1W (µ,ν). We next show
p,k k k k p
that 1 kW p(µ,ν) ≤ Π
p,k(µ,ν)+k−p+ p1
for any k ≥ 1. Let δ = Π p,k(µ,ν). Since the (1− 1 k)-
partial p-Wassersteindistanceisatmost1,
1
W (µ,ν) ≤ 1 = k× .
p,1
k k
Therefore,δ ≤ 1. Letγbea(1−δ)-partialOTplan. Sincetheunderlyingmetricspacehas
k
aunitdiameter,theremaining δ massof µ and ν withrespectto γ canbetransportedata
costatmostδ;therefore,
W (µ,ν) ≤ (cid:0) wp(γ)+δ(cid:1)1/p ≤ (cid:0) (kδ)p+ 1(cid:1)1/p ≤ kδ+k−1/p.
p p
k
Equivalently, 1 kW p(µ,ν) ≤ Π
p,k(µ,ν)+k−p+ p1
.
A.4 MissingDetailsofSection5. Inthissection,weprovidethedetailsofthealgorithms
mentionedinSection5.
Highly-Accurate Algorithm. In this algorithm, we obtain an approximation of our
metric by a simple guessing procedure as follows. Starting from an initial guess g = 0.5
1
for the value of our metric, at any step i of our algorithm and for any guess value g ≥ 0,
i
define w
i
:= W p,1−gi(µ,ν). If w
i
≤ kg i, then by Lemma 2.2, w
i
≤ Π p,k(µ,ν) ≤ g i, i.e., our
guess value is large and we decrease our guess g by 2−(i+1). Otherwise, w > kg, and
i i i
in this case, by Lemma 2.2, g ≤ Π (µ,ν) < w, i.e., our guess value is small and we
i p,k i
increaseourguess g by2−(i+1). Notethatatanystepi, |g −Π (µ,ν)| ≤ 2−i. Therefore,
i i p,k
to obtain a δ-additive approximation of the (p,k)-RPW, the algorithm returns the guess
value g when2−i ≤ δ. Thiscompletesthedescriptionofouralgorithm.
i
We next analyze the running time of this algorithm. Computing the (1− g )-partial
i
p-WassersteindistancecanbedoneusingastandardOTsolverinanaugmentedspace[7],
which takes O(n3logn) time [12, 32]. The total number of iterations of our algorithm is
O(logδ−1)andtherefore,ouralgorithmrunsinO(n3lognlogδ−1)time.Computing Through an Approximate OT-Profile. In this part, we show that an
approximationoftheOT-profilecanbeusedtoapproximateourmetric. Wethenconclude
that one can use the LMR algorithm to obtain such approximations of the OT-profile and
toobtainaδ-additiveapproximationoftheRPWdistanceinO(n2 + n )time.
δp δ2p
For a δ′ ∈ (0,1], p ≥ 1, and α ∈ [0,1], let W (µ,ν) denote a δ′-close α-partial p-
p,α
Wassersteindistance,i.e.,W (µ,ν) ≤ W (µ,ν) ≤ W (µ,ν)+δ′. Define
p,α p,α p,α
Π p,k(µ,ν) = min{ε ≥ 0 | W p,1−ε(µ,ν) ≤ kε}
to be the (p,k)-RPW distance function when computed using the approximate partial p-
Wasserstein distances. In the following lemma, we show that Π (µ,ν) is a 2δ′ -additive
p,k k
approximationofΠ (µ,ν).
p,k
LEMMA A.3. Foranypairofdistributions µ and ν inametricspace (X,c) withaunitdiameter
andanyparameters p ≥ 1,k > 0,andδ′ > 0,
2δ′
Π (µ,ν) ≤ Π (µ,ν) ≤ Π (µ,ν)+ .
p,k p,k p,k
k
Proof. Letδ := Π (µ,ν). Bydefinition,
p,k
W (µ,ν) ≤ W (µ,ν) ≤ kδ. (A.24)
p,1−δ p,1−δ
Therefore,Π (µ,ν) ≤ δ = Π (µ,ν). Next,letδ := Π (µ,ν). Bydefinition,
p,k p,k p,k
W (µ,ν) ≤ W (µ,ν)+δ′ ≤ kδ+δ′ . (A.25)
p,1−δ p,1−δ
Bypropertiesofthepartial p-Wassersteindistance,
W p,1−δ−2ε(µ,ν) ≤ W p,1−δ−2δ′(µ,ν)+δ′ ≤ W p,1−δ(µ,ν)+δ′ ≤ W p,1−δ(µ,ν)+δ′ . (A.26)
k k
CombiningEquations(A.25)and(A.26),
2δ′
W p,1−δ−2δ′(µ,ν) ≤ W p,1−δ(µ,ν)+δ′ ≤ k(δ+
k
).
k
Therefore,Π (µ,ν) ≤ δ+ 2δ′ = Π (µ,ν)+ 2δ′ .
p,k k p,k k