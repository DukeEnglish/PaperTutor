Pose Priors from Language Models
SanjaySubramanian1 EvonneNg1 LeaMu¨ller1 DanKlein1 ShiryGinosar2
TrevorDarrell1
1UniversityofCalifornia,Berkeley
2GoogleResearch
{sanjayss,evonne ng,mueller,klein,trevordarrell}@berkeley.edu
shiry@google.com
“Their faces are touching as “The yogi reaches their hands back
they lean into each other” to touch their heels.”
from LMM… … to pose from LMM… … to pose
Figure1:Optimizingcontactsin3Dhumanpose.OurapproachleveragesthesemanticpriorsofaLargeMul-
timodalModel(LMM)byconvertingnaturallanguagedescriptionsofindividualsinanimageintomathematical
constraints.Wecanthenoptimizethe3Dposeestimatesusingtheseconstraints.Theseexamplesshowimage
descriptionsgeneratedbyanLMMandcorrespondingrefinedposeestimates.
Abstract
Wepresentazero-shotposeoptimizationmethodthatenforcesaccuratephysical
contactconstraintswhenestimatingthe3Dposeofhumans.Ourcentralinsightis
thatsincelanguageisoftenusedtodescribephysicalinteraction,largepretrained
text-basedmodelscanactaspriorsonposeestimation.Wecanthusleveragethis
insight to improve pose estimation by converting natural language descriptors,
generatedbyalargemultimodalmodel(LMM),intotractablelossestoconstrain
the3Dposeoptimization.Despiteitssimplicity,ourmethodproducessurprisingly
compelling pose reconstructions of people in close contact, correctly capturing
the semantics of the social and physical interactions. We demonstrate that our
method rivals more complex state-of-the-art approaches that require expensive
human annotationof contact pointsand training specializedmodels. Moreover,
unlikepreviousapproaches,ourmethodprovidesaunifiedframeworkforresolving
self-contactandperson-to-personcontact.
1 Introduction
Whichsocialsignalsshouldwemodelwhenbuildingmodelsofhumanbehavior?Thispaperfocuses
on touch, one of the strongest signals of social validation and rejection. As physical contact is
4202
yaM
6
]VC.sc[
1v98630.5042:viXraa universal human social signal, humans developed extensive terminology for its particularities.
Detaileddescriptionsoftouchindifferentcontextsarewidelydiscussedintextsthatrangefrom
love-songlyricssuchasPaulAnka’s“Putyourheadonmyshoulder”toShakespeare’s“Seehowshe
leanshercheekuponherhand.”(RomeoandJuliet).Ittouchesonsubjectsfromlovetomeditative
poses.
Ourgoalinthisworkistoenablecomputerstocorrectlyperceivephysicalcontact(SeeFigure1).
Specifically, we aim to build a system that takes as input a single view of people during close
physical interaction or one person in a pose that involves self-contact and produces accurate 3D
meshreconstructionsofeachpersonasoutput.Thissettingischallengingforstate-of-the-artpose
regressionmodels,assomebodypartsarefrequentlyoccludedbyotherones,andalsochallengingfor
poseoptimizationmethodsrelyingon2Dkeypoints,whichdonotconveycontactpoints.Previously
proposedapproachesaddresstheseissuesbycuratingtask-specificdatasetsviamotioncaptureor
human-annotatedpointsofcontactbetweenbodyparts[28,10,29].
Ourmaininsightisthatsincewrittenlanguagediscussesourphysicalinteractions(hugs,kisses,fist
fights,yogaposes,etc.)atgreatlength,weshouldbeabletoextractasemanticprioronhumans’
poses from a pretrained large multimodal model (LMM) [1, 26, 5]. Just like a prior trained on
motioncapturedata,thislanguage-basedpriorcantelluswhichcontactsaremostlikelyinposes
and interactions. Through this approach, we avoid the time-consuming and expensive collection
oftrainingdatainvolvingmotioncaptureorannotatedselfandcross-personcontactsthatprevious
refinementmethodsrequire.
This insight leads us to a simple zero-shot framework, which we call ProsePose.1 We prompt a
pre-trainedLMM,withtheimageandrequestasoutputaformattedlistofcontactconstraintsbetween
bodyparts.Wethenconvertthislistofconstraintsintoalossfunctionthatcanbeoptimizedjointly
withothercommonlosses,suchas2Dkeypointloss,torefinetheinitialestimatesofaposeregression
model.Thepromptprovidesanintuitivewayforthesystemdesignertoadaptthegeneratedconstraints
totheirsetting(e.g.iftheywanttofocusonyogaordance).
We show in experiments on three 2-person interaction datasets and one dataset of complex yoga
posesthatProsePoseproducesmoreaccuratereconstructionsthanpreviouszero-shotapproaches.
TheseresultsindicatethatLMMs,withoutanyadditionalfinetuning,offerausefulpriorforpose
reconstruction.
In summary, (1) we show that LMMs have implicit semantic knowledge of poses that is useful
for pose estimation, and (2) we formulate a novel zero-shot framework that converts free-form
naturallanguageresponsesfromtheLMMintotractablelossfunctionsthatcanbeusedforpose
optimization.
2 RelatedWork
2.1 3Dhumanposereconstruction
Reconstructing3Dhumanposesfromsingleimagesisanactiveareaofresearch.Priorworkshave
explored using optimization-based approaches [30, 12, 22, 31, 34] or pure regression [18, 2, 13,
17,21]toestimatethe3Dbodyposegivenasingleimage.HMR2[11]isarecentstate-of-the-art
regressionmodelinthislineofwork.Buildingonthesemonocularreconstructionapproaches,some
methodshavelookedintoreconstructingmultipleindividualsjointlyfromasingleimage.These
methods [44, 16, 36] use deep networks to reason about multiple people in a scene to directly
outputmulti-person3Dposepredictions.BEV[37]accountsfortherelativeproximityofpeople
explicitlyusingrelativedepthannotationstoreasonaboutproxemicswhenpredictingandplacing
eachindividualsinthescene(e.g.depthofpeoplewithrespecttooneanother).However,approaches
in both categories generally do not accurately capture physical contact between parts of a single
personorbetweenpeople[29,28].
1Projectpage:prosepose.github.io
22.2 Contactinferencein3Dposereconstruction
3Dposereconstructionisespeciallychallengingwhenthereisself-contactorinter-personcontact.
Thishasmotivatedalineofworkonposereconstructionapproachestailoredforthissetting. [28]
focusesonpredictingselfcontactregionsfor3Dposeestimationbyleveragingadatasetwithcollected
contactannotationstomodelcomplexposessuchasarmonhiporcrossedarms. [9]introducesthe
firstdatasetwithhand-annotatedgroundtruthcontactlabelsbetweentwopeople.REMIPS[10]and
BUDDI[29]trainmodelsontheperson-to-personcontactmapsinthisdatainordertoimprove3D
poseestimationofmultiplepeoplefromasingleimage.However,contactannotations,whichare
crucialfortheseapproaches,aredifficultandexpensivetoacquire.Ourmethoddoesnotrequire
anytrainingonsuchannotations.Instead,weleverageanLMM’simplicitknowledgeaboutposeto
constrainposeoptimizationtocapturebothself-andperson-to-personcontact.
2.3 Languagepriorsonhumanpose
There exists a plethora of text to 3D human pose and motion datasets [33, 14, 32], which have
enabledworkfocusedongenerating3Dmotionsequencesofasinglepersonperformingageneral
action [38, 15, 45]. This line of work has been extended to generating the motion of two people
conditionedontext[35,25].
PoseScript[6]isamethodforgeneratingasingleperson’sposefromfine-graineddescriptions.They
leveragealibraryofpredefinedposedescriptors,whichtheystitchtogethertoformdetailedtextual
annotationsforeachmotioncaptureposeintheirdataset.Bytrainingamodelontheselanguage
annotationsandassociatedposes,theycangeneratevariousplausibleposes.
PoseFix[7]considerstheproblemofmodifyingaposegivenafine-graineddescriptionofthedesired
change,andintroducesadatasetoftriplets,eachconsistingofaninitialpose,adifferentbutrelated
pose, and the description of the change. The PoseFix method then trains a model on this data to
predictthemodifiedposegiventheinitialposeanddescription.
PoseGPT [8], like our work, focuses on the problem of monocular 3D reconstruction of people.
PoseGPTisaposeregressorthatuseslanguageaspartofitstrainingdata.However,PoseGPTdoes
notproducebetterposeestimatesthanpreviousstate-of-the-artregressors(i.e.regressorsthatdonot
uselanguage)andappliesonlytotheone-personsetting.
Ourworkdiffersfrompreviousworkonlanguageandposeinseveralways.First,whereasallprior
work trains a model on data with pairs of language and pose, which is expensive to collect, our
methodleveragestheexistingknowledgeinanLMMtoreasonaboutpose.Second,priorworkon
languageandposefocusesoneithertheone-personorthetwo-personsetting.Incontrast,ourwork
unifiesbothinasingleframeworkwhichweusetoreasonaboutphysicalcontactswithinorbetween
poses.Finally,insceneswithphysicalcontact,weshowthatourlanguage-basedmethodimproves
theposeestimatesofstate-of-the-artregressors.
3 GuidingPoseOptimizationwithanLMM
Givenanimage,ourgoalistoestimatethe3Dbodyposeofindividualsintheimagewhilecapturing
theselfandcross-personcontactpoints.Whilewecannottriviallyusenaturallanguageresponses
(hug,kiss)todirectlyoptimize3Dbodyposes,weleveragethekeyinsightthatLMMsunderstand
howtoarticulateagivenpose(armsaroundwaist,lipstouching).Weproposeamethodtostructure
thesearticulationsintoconstraintsandconvertthemintolossfunctions.
Moreconcretely,ourframework,illustratedbyFigure2,takesasinputtheimageI andthebounding
boxes B of the subjects of interest. In the first stage, the image is passed to a pose regressor to
obtainaroughestimateofthe3DposeXpforeachindividualpintheimage.Inthesecondstage,
we prompt a LMM with the image and a set of instructions in order to generate a list of self- or
inter-personcontactconstraints,whichwethenconvertintoalossfunction(Sec.3.4).Finally,inthe
thirdstage,wejointlyoptimizethegeneratedlossfunctionwithseveralotherpre-definedlossterms
(Sec.3.4).WerefertoourframeworkasProsePose.
3Prompt: “Identify all pairs of Constraint Generation
body parts of Person 1 and
Person 2 that are touching.” def lmm_loss(...):
Arm, Waist (front) shoulder_waist_loss = ...
Hand, Waist (front) arm_waist_loss = ...
LMM Waist(front), Butt h ta on td a_ ls _t lo om sa s c =h _ .l .o .ss = ...
Reference Image: Leg, Butt
3D Pose Regressor Constrained Optimization
Output:
Initialization:
Figure2:LMM-guidedPoseEstimationOurmethodtakesasinputanimageofoneortwopeopleincontact.
Wefirstobtaininitialposeestimatesforeachpersonfromaposeregressor.ThenweuseanLMMtogenerate
contactconstraints,eachofwhichisapairofbodypartsthatshouldbetouching.Thislistofcontactsisconverted
intoalossfunctionL .WeoptimizetheposeestimatesusingL andotherlossestoproducearefined
LMM LMM
estimateofeachperson’sposethatrespectsthepredictedcontacts.
3.1 Preliminaries
Whileourapproachscalesinprincipletoanarbitrarynumberofindividuals,wefocusourdescription
onthetwo-personcasetokeeptheexpositionsimple.Wealsodemonstrateresultsontheone-person
case,whichissimplyanextensionofthetwo-personcase.Inparticular,weapplyourmethodto
theone-personcasebysettingX0 =X1.Pleasesee§7fordetailsonthedifferencesbetweenthe
two-personandone-personcases.
LargeMultimodalModelsAnLMMisamodelthattakesasinputanimageandatextpromptand
producestextoutputthatanswersthepromptbasedontheimage.Ourframeworkisagnostictothe
architectureoftheLMM.LMMsaretypicallytrainedtorespondtowidevarietyofinstructions[26,5],
butatthesametime,LMMsarepronetohallucination[23,24].Handlingcasesofhallucinationis
akeychallengewhenusingLMMs,andwemitigatethisissuebyaggregatinginformationacross
severalsamplesfromtheLMM.
Poserepresentation.Weuseahumanbodymodel[30]torepresenteachpersonp∈{0,1}.Thebody
modeliscomposedofaposeparameterthatdefinesthejointrotationsθ ∈Rdθ×3,whered
θ
isthe
numberofjoints,andashapeparameterβ ∈Rdβ,whered
β
isthedimensionsoftheshapeparameter.
WecanapplyaglobalrotationΦ ∈ R3 andtranslationt ∈ R3 toplaceeachpersonintheworld
coordinatespace.ThefullsetofparametersforeachpersonisdenotedbyXp = [θp,βp,Φp,tp].
Forsimplicity,werefertotheparameterset(X0,X1)asX.
Theseparameterscanbepluggedintoadifferentiablefunctionthatmapstoameshconsistingof
d
v
vertices V ∈ Rdv×3. From the mesh, we can obtain a subset of the vertices representing the
3Dlocationsofthebody’sjointsJ ∈Rdj×3.Fromthesejoints,wecancalculatethe2Dkeypoints
K byprojectingthe3Djointsto2DusingthecameraintrinsicsΠpredictedfrom[30].
proj
K =Π(J)∈Rdj×2. (1)
proj
Vertexregions.Inordertodefinecontactconstraintsbetweenbodyparts,wedefineasetofregions
ofvertices.Priorworkoncontacthaspartitionedthebodyintofine-grainedregions [9].However,
sinceourconstraintsarespecifiedbyaLMMtrainedonnaturallanguage,thereferencedbodyparts
areoftencoarseringranularity.Wethereforeupdatethesetofregionstoreflectthislanguagebias
bycombiningthesefine-grainregionsintolarger,morecommonlyreferencedbodypartssuchas
arm,shoulder(front&back),back,andwaist(front&back).Pleasesee§7.2foravisualizationofthe
coarseregions.Formally,wewriteR∈Rdr×3todenotearegionwithd
r
vertices,whichispartof
thefullmesh(R⊂V).
4Constraint definition. A contact constraint
specifies which body parts from two meshes
shouldbetouching.Usingthesetofcoarsere-
gions,wedefinecontactconstraintsaspairsof
coarseregionsc=(R ,R )betweenaregion
a b
R ofonemeshandR oftheothermesh,as
a b
showninFigure3.Forinstance,(“hand”,“arm”)
indicatesahandshouldtouchanarm.
3.2 PoseInitialization
Weobtainaroughinitialestimateofthe3Dpose
fromaregression-basedmethod.Theregressor
takesasinputtheimageI andoutputsestimates
Figure3:Notation.GivenanimageI,wecanlifteach
forthebodymodelparametersθ,β,r,andtfor
individualintocorresponding3DmeshesV.Wedefine
eachsubject.
contact constraints c as pairs of regions (R ,R ) in
a b
contact. The loss is defined in terms of the distance
3.3 ConstraintgenerationwithaLMM betweenthevertices(v a,v b)onthemesh.
Our method strives to enforce contact con-
straints for the estimated 3D poses. Our key
insight is to leverage a LMM to identify regions of contact between different body parts on the
humanbodysurface.AsshowninFigure2(top),weprompttheLMMwithanimageandaskit
tooutputalistofallplausibleregionsthatareincontact.However,wecannotsimplyusenatural
languagedescriptionstodirectlyoptimizea3Dmesh.Assuch,weproposeaframeworktoconvert
theseconstraintsintoalossfunction.
LMM-basedconstraintgeneration.GiventheimageI,wefirstusetheboundingboxesBtocrop
thepartcontainingthesubjects.Wethenuseanimagesegmentationmodeltomaskanyextraneous
individuals.Whilecroppingandmaskingtheimagemayremoveinformation,wefindtheLMMsare
relativelyrobusttomissingcontext,andmoreimportantly,thisallowsustoindicatewhichindividuals
tofocuson.Giventhesegmentedimage,weasktheLMMtogenerateasetC ={c ,...c }ofall
1 m
pairsofbodypartsthataretouching,wheremisthetotalnumberofconstraintstheLMMgenerates
fortheimage.
In the prompt, we specify the full set of coarse regions to pick from. We find that LMMs fail to
reliablyreferencetheleftandrightlimbscorrectlyorconsistently,sowedesignedthissetofcoarse
regionssuchthattheydonotdisambiguatethechiralityofthehands,arms,legs,feet,andshoulders.
Instead,thetwohandsaregroupedtogether,thetwoarmsaregroupedtogether,etc.Nevertheless,if
theLMMuses“left”or“right”toreferencearegion,despitetheinstructiontonotdoso,wedirectly
usethepartoftheregionwiththespecifiedchiralityratherthanconsideringbothpossibilities.
Motivatedbythechain-of-thoughttechnique,whichhasbeenshowntoimprovelanguagemodel
performanceonreasoningtasks[41],weasktheLMMtowriteitsreasoningordescribethepose
beforelistingtheconstraints.Forthefullpromptusedineachsetting,pleasereferto§7.
WesampleN responsesfromtheLMM,yieldingN setsofconstraints{C ,C ,...,C }.Thenext
1 2 N
stepistoconverteachconstraintsetC ,wherej ∈{1,2,...N},intoalossterm.
j
Lossfunctiongeneration.Wefirstfilteroutcontactpairsthatoccurfewerthanf timesacrossall
constraintsets,wheref isahyperparameter.Thenforeachcontactpairc = (R ,R )inC ,we
a b j
definedist(c)astheminimumdistancebetweenthetworegions:
dist(c)=min∥v −v ∥ ∀v ∈R ,∀v ∈R (2)
a b 2 a a b b
where{v ,v }∈R3.Inpractice,thenumberofverticesineachregioncanbeverylarge.Tomake
a b
thiscomputationtractable,wefirsttakearandomsampleofverticesfromR andfromR before
a b
computing distances between pairs of vertices in these samples. Furthermore, since the ordering
of the people in the LMM constraints is unknown (i.e. does R come from the mesh defined by
a
parameterX0or X1),wecomputetheoveralllossforbothpossibilitiesandtaketheminimum.We
usec⊤ =(R ,R )todenotetheflippedordering.WethensumoverallconstraintsinthelistC :
b a j
5 
(cid:88) (cid:88)
dist sum(C j)=min dist(c), dist(c⊤) (3)
c∈Cj c∈Cj
EachconstraintsetsampledfromtheLMMislikelytocontainnoiseorhallucination.Tomitigatethe
effectofthis,weaverageoverallN lossescorrespondingtoeachconstraintsettoobtaintheoverall
LMMloss.Thistechniqueissimilartoself-consistency[40],whichiscommononlyusedforcode
generationtasks.Concretely,theoverallLMMlossisdefinedas
N
1 (cid:88)
L = dist (C ) (4)
LMM N sum j
j=1
If a constraint set C is empty (i.e. the LMM does not suggest any contact pairs), then we set
j
dist (C )=0.Ifthereareseveralsuchconstraintsets,weinferthattheLMMhaslowconfidence
sum j
aboutthecontactpoints(ifany)intheimage.Tohandlethesecases,wesetathresholdtandifthe
numberofemptyconstraintsetsisatleastaslargeast,wegracefullybackofftotheappropriate
baselineoptimizationprocedure(describedinSections4.1and4.2foreachsetting).
3.4 Constrainedposeoptimization
Drawingfrompreviousoptimization-basedapproaches[29,3,31],weemployseveraladditional
lossesintheoptimization.Wethenminimizethejointlosstoobtainarefinedsubsetofthebody
modelparametersX′ =[θ′,β′,t′]:
[θ′,β′,t′]=argmin(λ L +λ L
LMM LMM GMM GMM
+λ L +λ L
β β θ θ
+λ L +λ L ) (5)
2D 2D P P
Following[29],wedividetheoptimizationintotwostages.Inthefirststage,weoptimizeallthree
parameters.Inthesecondstage,weoptimizeonlyθandt,keepingtheshapeβfixed.Here,wedetail
alloftheremaininglossesusedintheoptimization.
Poseandshapepriors.WecomputealossL basedontheGaussianMixtureposepriorof[3]
GMM
andashapelossL =∥β∥2,whichpenalizesextremedeviationsfromthebodymodel’smeanshape.
β 2
Initialposeloss.Toensurewedonotstraytoofarfromtheinitialization,wepenalizelargedeviations
fromtheinitialposeL =||θ′−θ||2.
θ 2
2Dkeypointloss.SimilartoBUDDI[29],foreachpersonintheimage,weobtainpseudoground
truth2DkeypointsandtheirconfidencesfromOpenPose[4]andViTPose[42].Giventhispseudo
groundtruth,wemergeallthekeypointsintoK ∈Rdj×2,andtheircorrespondingconfidencesinto
γ ∈Rdj.FromthepredictedX′,wecancomputethe2Dprojectionofeach3Djointlocationusing
Equation3.1.Then,the2Dkeypointlossisdefinedas:
dj
(cid:88)
L = γ(K −K)2 (6)
2D proj
j=1
Interpenetrationloss.Topreventpartsofonemeshfrombeingintheinterioroftheother,weadd
aninterpenetrationloss.Generically,giventwosetsofverticesV andV ,weusewindingnumbers
0 1
tocomputethesubsetofV thatintersectsV ,whichwedenoteasV .Similarly,V isthesubset
0 1 0,1 1,0
ofV thatintersectsV .Theinterpenetrationlossisthendefinedas
1 0
L = (cid:88) min ∥x−v ∥2+ (cid:88) min ∥y−v ∥2 (7)
P 1 2 0 2
x∈V0,1v1∈V1 y∈V1,0v0∈V0
Due to computational cost, this loss is computed on low-resolution versions of the two meshes
(roughly1000verticespermesh).
6Table1:Two-personResults.JointPA-MPJPE(lowerisbetter)andAvg.PCC(higherisbetter).ForFlickrCI3D,
PA-MPJPEiscomputedusingthepseudo-ground-truthfits.Boldindicatesbestzero-shotmethodineachcolumn.
Hi4D FlickrCI3D CHI3D
PA-MPJPE PA-MPJPE PCC PA-MPJPE PCC
↓ ↓ ↑ ↓ ↑
Zero-shot
BEV[37] 144 106 64.8 96 71.4
Heuristic 116 67 77.8 105 74.1
ProsePose 93 58 79.9 100 75.8
Supervised
BUDDI[29] 89 65.9 81.9 68 78.6
4 Experiments
Weconductexperimentsonseveraldatasetsinthetwo-personandone-personsettings.Inthissection,
wefirstprovideimportantimplementationdetailsandadescriptionofthequantitativemetricsthat
weusetoevaluateourmethodandpreviousapproaches.Wethenpresentquantitativeandqualitative
resultsdemonstratingthatProsePoserefinesposeestimatestocapturesemanticallyrelevantcontact
ineachsetting.
Implementationdetails.Followingpriorworkontwo-personposeestimation[29],weuseBEV[37]
toinitializetheposessinceitwastrainedtopredictboththebodyposeparametersandtheplacement
ofeachpersoninthescene.However,onthesinglepersonyogaposes,wefindthattheposeparameter
estimatesofHMR2[11]aremuchhigherquality,soweinitializethebodyposeusingHMR2.
WeusetheSMPL-X[30]bodymodelandGPT4-V[1]astheLMMwithtemperature=0.7when
sampling from it.2 We report results with LLaVA as the LMM in § 8.4. We use the Segment
Anything[20]asthesegmentationmodel,usedtoremoveextraneouspeopleintheimage(weonly
applythisstepforFlickrCI3D,sinceotherdatasetsarefrommotioncapture).WesetN =20samples
inallofourexperiments.Forallofour2-personexperiments,f =1,whilef =10inthe1-person
setting.Wesett=2fortheexperimentontheCHI3Ddatasetandt=N forallotherexperiments.
Wesetλ =1000inthe2-personexperiments,andλ =10000inthe1-personsetting.Inthe
LMM LMM
two-personcase,allotherlosscoefficientsaretakendirectlyfrom[29].Intheone-personcase,we
findthatremovingtheGMMposeprioranddoublingtheweightontheinitialposelossimproves
optimizationdramatically,likelybecausethecomplexyogaposesareoutofdistributionfortheGMM
prior.Thesehyperparametersandourpromptswerechosenbasedonexperimentsonthevalidation
sets.Furthermore,following[29],werunbothoptimizationstagesforatmost1000steps.Weusethe
Adamoptimizer[19]withlearningrate0.01.
For other implementation details such as prompts, the list of coarse regions in each setting, and
additionaldifferencesbetweenthe1-and2-personcases,pleasereferto§7.
Metrics.Asisstandardintheposeestimationliterature,wereportper-personProcrustes-aligned
MeanPerJointPositionError(PA-MPJPE)inmillimeterstoevaluatethedifferencebetweenthe
ground-truth(orpseudo-ground-truth)poseandpredictedpose.Thismetricfindsthebestalignment
betweentheestimatedandground-truthposebeforecomputingthejointerror.Inthetwo-person
setting,wefocusonthejointPA-MPJPE,asthisevaluationincorporatestherelativetranslationand
orientationofthetwopeople.Fortheper-personPA-MPJPE,pleasereferto§8.2.
We also include the percentage of correct contact points (PCC) metric introduced by [29]. This
metriccapturesthefractionofground-truthcontactpairsthatareaccuratelypredicted.Foragiven
radiusr,apairisclassifiedas“incontact”ifthetworegionsarebothwithinthespecifiedradius.
Weusethesetoffine-grainedregionsdefinedin[9]tocomputePCC.Themetricisaveragedover
r ∈ 0,5,10,15,...,95mm.PleasenotethatsincetheseregionsaredefinedontheSMPL-Xmesh
topology,weconverttheregressionbaselines–BEVandHMR2–fromtheSMPLmeshtopologyto
SMPL-Xtocomputethismetric.Pleasesee§8.1formoredetailsontheregionsandonthemesh
conversion.
2We access GPT4-V, specifically the gpt-4-vision-preview model, via the OpenAI API: plat-
form.openai.com.Weusethe“high”detailsettingforimageinput.
7.
Table2:Two-personPCC.Percentofcorrectcontactpoints(PCC)forfivedifferentradiir inmm. Bold
indicatesthebestzero-shotscoreineachcolumn.Attheground-truthcontactpoints,ourmethodbringsthe
meshesclosertogetherthantheotherzero-shotmethods.
PCC @ronFlickrCI3D PCC @ronCHI3D
↑ ↑
5 10 15 20 25 5 10 15 20 25
Zero-shot
BEV[37] 3.6 6.3 10.8 17.1 28.6 5.8 17.4 32.5 47.3 61.9
Heuristic 14.6 33.9 49.3 60.8 70.3 11.1 28.0 45.3 55.3 64.4
ProsePose 15.6 39.9 57.1 67.9 75.8 13.5 35.2 52.5 61.3 68.4
Supervised
BUDDI[29] 18.5 44.2 61.8 73.1 80.8 15.7 39.4 57.1 68.8 78.0
4.1 Two-personPoseRefinement
DatasetsInthetwo-personsetting,weevaluateonthreedatasets,andourdatasetprocessinglargely
follows[29].
Hi4D[43]isamotioncapturedatasetofpairsofpeopleinteracting.Eachsequencehasasubsetof
framesmarkedascontactframes,andwetakeeveryfifthcontactframe.Weusetheimagesfroma
singlecamera,resultinginroughly247images.
FlickrCloseInteractions3D(FlickrCI3D)[9]isacollectionofFlickrimagesofmultiplepeople
incloseinteraction.Thedatasetincludesmanualannotationsofthecontactmapsbetweenpairsof
people.[29]usedthesecontactmapstocreatepseudo-groundtruth3Dmeshesandcuratedaversion
ofthetestsettoexcludenoisyannotations,whichhasroughly1403images.
CHI3D [9] is a motion capture dataset of pairs of people interacting. We present results on the
validationset.Thereare126differentsequences,eachofwhichhasasingledesignated“contact
frame.”Eachframeiscapturedfrom4cameras,sothereareroughly504imagesinthisset.
To develop our method, we experimented on the validation sets of FlickrCI3D and Hi4D, and a
sampleofthetrainingsetfromCHI3D.Forourexperiments,wecancomputethePCConFlickrCI3D
andCHI3D,whichhaveannotatedground-truthcontactmaps.SinceallbaselinesalsouseBEVfor
initialization,weexcludeimageswhereBEVfailstodetectoneofthesubjectsintheinteractionpair.
BaselinesWecompareourestimatedposestothefollowing:
• BEV[37]Multi-person3Dposeestimationmethod.Usesrelativedepthtoreasonaboutthe
spatialplacementofindividualsinthescene.WeuseBEVtoinitializeourmethodandother
baselines.
• Heuristic A contact heuristic which includes the auxiliary losses in Section 3.4 as well
asatermthatminimizestheminimumdistancebetweenthetwomeshes.Thisheuristic
wasintroducedby[29],andweusetheirhyperparametersforthisheuristic.Pleasenote,
thisbaselineisusedasthedefaultwhenthenumberofemptyconstraintsetsisatleastthe
thresholdt.
• BUDDI [29] A method using a learned diffusion prior, which was trained on pairs of
interactingpeople,toconstraintheoptimization.WestressthatBUDDIrequiresalarge
amountofannotatedtrainingdataonpairsofinteractingbodies,whichisnotusedinour
method.
QuantitativeResultsTable1providesquantitativeresultsonthethreedatasets.
Acrossdatasets,ProsePoseconsistentlyimprovesoverthestrongestzero-shotbaseline,Heuristic.
On the Hi4D dataset, ProsePose reduces 85% of the gap in PA-MPJPE between Heuris-
tic and the fully supervised BUDDI. On the FlickrCI3D and CHI3D datasets, ProsePose
narrows the gap in the average PCC between Heuristic and BUDDI by more than one-
third. (While ProsePose achieves a better PA-MPJPE than BUDDI on FlickrCI3D, for this
dataset, we rely primarily on PCC since PA-MPJPE is computed on pseudo-ground-truth fits.)
8Input ProsePose BUDDI[29] Heuristic Input ProsePose BUDDI[29] Heuristic
Arm, Waist (front) ×15 Hand, Shoulder (front) ×12
Back, Shoulder (front) ×13 Hand, Shoulder (back) ×2
Back, Head ×4 Hand, Leg ×1
Hand, Shoulder (front) ×21 Hand, Hand ×20
Hand, Hand ×17
Arm, Shoulder (front) ×4
Figure 4: Two-person results We show qualitative results from ProsePose , BUDDI [29], and the contact
heuristic.Undereachexample,weshowthetop3constraintspredictedbyGPT4-Vandthenumberoftimes
eachconstraintwaspredictedacrossall20samples.Ourmethodcorrectlyreconstructspeopleinavarietyof
interactions,andthepredictedconstraintsgenerallyalignwiththeinteractiontypeineachexample.
On CHI3D, ProsePose outperforms Heuris-
tic but underperforms BEV in terms of PA-
MPJPE. We find that on the subset of images
wherewedonotdefaulttotheheuristic(i.e.on
images where GPT4-V predicts enough non-
emptyconstraintsets),thePA-MPJPEforPros-
ePose and BEV is 86 and 87, respectively. In
otherwords,inthecaseswhereourmethodis # of samples # of samples
actuallyused,thejointerrorisslightlylessthan Figure5:Moresamplesimproveposeestimation.On
that of BEV. As a result, we can attribute the theFlickrCI3Dvalidationset,takingmoresamplesfrom
worseoverallerrortothepoorerperformanceof the LMM and averaging the resulting loss functions
theheuristic.Overall,ourmethodimprovesover improvesbothjointPA-MPJPE(left)andaveragePCC
the other zero-shot methods in terms of both (right).
jointerrorandPCC. .
Table3:AblationsonHi4D.JointPA-MPJPE(lower
Table2showsthePCCforeachmethodatvari-
isbetter).Weevaluatetheimpactofeachlossinour
ousradii.TheresultsshowthatProsePosebrings
optimization on the Hi4D by removing one loss at a
the meshes closer together at the correct con-
time.Forallexperiments,weusethesamesettings.The
tactpoints.OnboththeFlickrCI3DandCHI3D
setofcaseswherewedefaulttothebaseline(Heuristic)
datasets,ProsePoseoutperformstheotherzero- isalsokeptthesame.
shotbaselines,closingmuchofthegapbetween
thefullysupervisedmethodandthebestcom- PA-MPJPE
↓
petingbaselineHeuristic.
AllLosses 81
Next,weablateimportantaspectsofProsePose w/o.L LMM 138
.InFigure5,weshowthattakingseveralsam- w/o.L GMM 85
w/o.L 91
plesfromtheLMMandaveragingtheresulting β
w/o.L 84
loss functions improves performance, mitigat- θ
w/o.L 130
ing the effect of LMM hallucination. Table 3 2D
w/o.L 78
P
presents an ablation of all the losses involved
inouroptimizationontheHi4Dvalidationset.
9
CCP
egarevA
EPJPM-AP.
Table4:One-personResults.PA-MPJPE(lowerisbetter)andAvg.PCC(higherisbetter).Ourmethodcaptures
ground-truthcontactsbetterthanthebaselinemethods,asshownbythePCC.
PCC @r
↑
PA-MPJPE PCC 5 10 15 20 25
↓ ↑
HMR2[11] 84 83.0 34.2 55.2 69.5 78.4 83.9
HMR2+opt 81 85.2 47.7 65.5 74.6 80.9 86.2
ProsePose 82 87.8 54.2 73.8 81.4 86.5 91.3
Hand, Foot
Input ProsePose HMR2[11] HMR2-opt Input ProsePose HMR2[11] HMR2-opt
Hand, Foot ×21 Hand, Hand ×14
Hand, Foot ×21 Hand, Foot ×18
Figure6:Single-personresultsWeshowqualitativeresultsfromProsePose,HMR2[11],andHMR2-optimon
complexyogaposes.EachexamplealsoshowstheconstraintsthatarepredictedbytheLMMatleastf =10
times(andarethususedtocomputeL )withtheircounts.ProsePosecorrectlyidentifiesself-contactpoints
LMM
andoptimizestheposestorespectthesecontacts.
L andL havethegreatestimpact,indi-
LMM 2D
cating that our LMM-based loss is crucial for
thelargeimprovementinjointerror.
QualitativeResultsFigure4showsexamplesofreconstructionsfromProsePose,Heuristic,and
BUDDI.Beloweachofourpredictions,welistthemostcommonconstraintspredictedbyGPT4-V
for the image. The predicted constraints correctly capture the semantics of each interaction. For
instance,itisinherentthatintango,oneperson’sarmshouldtouchtheother’sback.Inarugbytackle,
a player’s arms are usually wrapped around the other player. Using these constraints, ProsePose
correctly reconstructs a variety of interactions, such as tackling, dancing, and holding hands. In
contrast,theheuristicstrugglestoaccuratelypositionindividualsand/orpredictlimbplacements,
oftenresultinginawkwarddistances.
4.2 One-personposerefinement
DatasetsNext,weevaluateProsePoseonasingle-personsetting.Forthissetting,weevaluateon
MOYO[39],amotioncapturedatasetwithvideosofasinglepersonperformingvariousyogaposes.
Thedatasetprovidesviewsfrommultipledifferentcameras.Wepickasinglecamerathatshows
thesideviewforevaluation.Foreachvideo,wetakesingleframefromthemiddleasitgenerally
showsthemainpose.Thereisnoofficialtestset,andtheofficialvalidationsetconsistsofonly16
poses.Therefore,wecreatedourownsplitbypicking79arbitraryexamplesfromthetrainingsetto
formourvalidationset.Wethencombinetheremainingexamplesinthetrainingsetwiththeofficial
validationsettoformourtestset.Intotal,ourtestsetiscomposedof76examples.Sincethisdataset
doesnothaveannotatedregioncontactpairs,wecomputethepesudo-ground-truthcontactmaps
usingtheEuclideanandgeodesicdistancefollowing[28].
BaselinesWecompareagainstthefollowingbaselines:
• HMR2[11]State-of-the-artposeregressionmethod.Weusethisbaselinetoinitializeour
poseestimatesforoptimization.
10• HMR2+optOptimizationprocedurethatisidenticaltoourmethodwithoutL .Weuse
LMM
thisbaselineasthedefaultwhenthenumberofemptyconstraintsetsisatleastthethreshold
t.
Boththequantitativeandqualitativeresultsechothetrendsdiscussedinthe2-personsetting.Table4
providesthequantitativeresults.ThePCCmetricsshowthatourLMMlossimprovesthepredicted
self-contact in complex yoga poses relative to the two baselines. Figure 6 provides a qualitative
comparisonofposespredictedbyProsePoseversusthetwobaselines.Beloweachofourpredictions,
welistthecorrespondingconstraintspredictedbyGPT4-V.Ineachcase,thepredictedconstraint
capturesthecorrectself-contact,whichisreflectedinthefinalposeestimates.Withtheadditionof
thesemanticallyguidedloss,ProsePoseeffectivelyrefinestheposetoensurepropercontactbetween
hand-footorhand-hand,animportantdetailconsistentlyoverlookedbythebaselines.
4.3 Limitations
WhileProsePoseconsistentlyimprovescontactacrosssettingsanddatasets,ithassomelimitations.
First,thoughwemitigateitthroughaveraging,LMMhallucinationofincorrectconstraintsmaylead
toanunexpectedoutput.Second,whentakingtheminimumlossacrossthepossiblechiralitiesof
limbs,theposeinitializationmayleadtoasuboptimalchoice.Weshowin§8.3examplesoffailure
caseslikethese.WealsonotethattheLMMmaybebiasedtowardposescommonincertaincultures
duetoitstrainingdata.Inaddition,wefindthatGPT4-Vperformsworsewithsomeofthecamera
anglesintheMOYOdataset(e.g.frontaloraerial),perhapsbecauseinphotosyogaposesaremost
oftencapturedfromasideview.
5 Conclusion
We present ProsePose , a zero-shot framework for refining 3D pose estimates to capture touch
accuratelyusingtheimplicitsemanticknowledgeofposesinLMMs.Ourkeynoveltyisthatwe
generate structured pose descriptions from LMMs and convert them into loss functions used to
optimizethepose.SinceProsePosedoesnotrequiretraining,weeliminatetheneedfortheexpensive
contactannotationsusedinpriorworktotrainpriorsforcontactestimation.Ourframeworkapplies
in principle to an arbitrary number of people, and our experiments show in both one-person and
two-person settings, ProsePose improves over previous zero-shot baselines. More broadly, this
workprovidesevidencethatLMMsarepromisingtoolsfor3Dposeestimation,whichmayhave
implicationsbeyondtouch.
6 Acknowledgements
SS,EN,andTDweresupportedinpartbytheNSF,DoD,and/ortheBerkeleyArtificialIntelligence
Research(BAIR)industrialallianceprogram.
11References
[1] Achiam, O.J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F.L., Almeida, D.,
Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V.,
Baltescu,P.,Bao,H.,Bavarian,M.,Belgum,J.,Bello,I.,Berdine,J.,Bernadett-Shapiro,G.,
Berner,C.,Bogdonoff,L.,Boiko,O.,Boyd,M.,Brakman,A.L.,Brockman,G.,Brooks,T.,
Brundage,M.,Button,K.,Cai,T.,Campbell,R.,Cann,A.,Carey,B.,Carlson,C.,Carmichael,
R.,Chan,B.,Chang,C.,Chantzis,F.,Chen,D.,Chen,S.,Chen,R.,Chen,J.,Chen,M.,Chess,
B.,Cho,C.,Chu,C.,Chung,H.W.,Cummings,D.,Currier,J.,Dai,Y.,Decareaux,C.,Degry,
T.,Deutsch,N.,Deville,D.,Dhar,A.,Dohan,D.,Dowling,S.,Dunning,S.,Ecoffet,A.,Eleti,
A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S.P., Forte, J., Fulford, I., Gao,
L.,Georges,E.,Gibson,C.,Goel,V.,Gogineni,T.,Goh,G.,Gontijo-Lopes,R.,Gordon,J.,
Grafstein,M.,Gray,S.,Greene,R.,Gross,J.,Gu,S.S.,Guo,Y.,Hallacy,C.,Han,J.,Harris,J.,
He,Y.,Heaton,M.,Heidecke,J.,Hesse,C.,Hickey,A.,Hickey,W.,Hoeschele,P.,Houghton,B.,
Hsu,K.,Hu,S.,Hu,X.,Huizinga,J.,Jain,S.,Jain,S.,Jang,J.,Jiang,A.,Jiang,R.,Jin,H.,Jin,
D.,Jomoto,S.,Jonn,B.,Jun,H.,Kaftan,T.,Kaiser,L.,Kamali,A.,Kanitscheider,I.,Keskar,
N.S.,Khan,T.,Kilpatrick,L.,Kim,J.W.,Kim,C.,Kim,Y.,Kirchner,H.,Kiros,J.R.,Knight,M.,
Kokotajlo,D.,Kondraciuk,L.,Kondrich,A.,Konstantinidis,A.,Kosic,K.,Krueger,G.,Kuo,
V.,Lampe,M.,Lan,I.,Lee,T.,Leike,J.,Leung,J.,Levy,D.,Li,C.M.,Lim,R.,Lin,M.,Lin,S.,
Litwin,M.,Lopez,T.,Lowe,R.,Lue,P.,Makanju,A.A.,Malfacini,K.,Manning,S.,Markov,
T.,Markovski,Y.,Martin,B.,Mayer,K.,Mayne,A.,McGrew,B.,McKinney,S.M.,McLeavey,
C.,McMillan,P.,McNeil,J.,Medina,D.,Mehta,A.,Menick,J.,Metz,L.,Mishchenko,A.,
Mishkin,P.,Monaco,V.,Morikawa,E.,Mossing,D.P.,Mu,T.,Murati,M.,Murk,O.,M’ely,D.,
Nair,A.,Nakano,R.,Nayak,R.,Neelakantan,A.,Ngo,R.,Noh,H.,Long,O.,O’Keefe,C.,
Pachocki,J.W.,Paino,A.,Palermo,J.,Pantuliano,A.,Parascandolo,G.,Parish,J.,Parparita,
E., Passos, A., Pavlov, M., Peng, A., Perelman, A., de Avila Belbute Peres, F., Petrov, M.,
deOliveiraPinto,H.P.,Pokorny,M.,Pokrass,M.,Pong,V.H.,Powell,T.,Power,A.,Power,
B.,Proehl,E.,Puri,R.,Radford,A.,Rae,J.,Ramesh,A.,Raymond,C.,Real,F.,Rimbach,
K.,Ross,C.,Rotsted,B.,Roussez,H.,Ryder,N.,Saltarelli,M.D.,Sanders,T.,Santurkar,S.,
Sastry,G.,Schmidt,H.,Schnurr,D.,Schulman,J.,Selsam,D.,Sheppard,K.,Sherbakov,T.,
Shieh,J.,Shoker,S.,Shyam,P.,Sidor,S.,Sigler,E.,Simens,M.,Sitkin,J.,Slama,K.,Sohl,
I.,Sokolowsky,B.D.,Song,Y.,Staudacher,N.,Such,F.P.,Summers,N.,Sutskever,I.,Tang,
J.,Tezak,N.A.,Thompson,M.,Tillet,P.,Tootoonchian,A.,Tseng,E.,Tuggle,P.,Turley,N.,
Tworek,J.,Uribe,J.F.C.,Vallone,A.,Vijayvergiya,A.,Voss,C.,Wainwright,C.,Wang,J.J.,
Wang,A.,Wang,B.,Ward,J.,Wei,J.,Weinmann,C.,Welihinda,A.,Welinder,P.,Weng,J.,
Weng,L.,Wiethoff,M.,Willner,D.,Winter,C.,Wolrich,S.,Wong,H.,Workman,L.,Wu,S.,
Wu,J.,Wu,M.,Xiao,K.,Xu,T.,Yoo,S.,Yu,K.,Yuan,Q.,Zaremba,W.,Zellers,R.,Zhang,C.,
Zhang,M.,Zhao,S.,Zheng,T.,Zhuang,J.,Zhuk,W.,Zoph,B.:Gpt-4technicalreport(2023),
https://api.semanticscholar.org/CorpusID:257532815 2,7
[2] Arnab,A.,Doersch,C.,Zisserman,A.:Exploitingtemporalcontextfor3dhumanposeestima-
tioninthewild.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.pp.3395–3404(2019) 2
[3] Bogo,F.,Kanazawa,A.,Lassner,C.,Gehler,P.,Romero,J.,Black,M.J.:Keepitsmpl:Auto-
maticestimationof3dhumanposeandshapefromasingleimage.In:ComputerVision–ECCV
2016:14thEuropeanConference,Amsterdam,TheNetherlands,October11-14,2016,Proceed-
ings,PartV14.pp.561–578.Springer(2016) 6
[4] Cao,Z.,HidalgoMartinez,G.,Simon,T.,Wei,S.,Sheikh,Y.A.:Openpose:Realtimemulti-
person2dposeestimationusingpartaffinityfields.IEEETransactionsonPatternAnalysisand
MachineIntelligence(2019) 6
[5] Dai, W., Li, J., Li, D., Tiong, A.M.H., Zhao, J., Wang, W., Li, B.A., Fung, P., Hoi, S.C.H.:
Instructblip:Towardsgeneral-purposevision-languagemodelswithinstructiontuning.ArXiv
abs/2305.06500(2023),https://api.semanticscholar.org/CorpusID:258615266 2,
4
[6] Delmas,GingerandWeinzaepfel,PhilippeandLucas,ThomasandMoreno-Noguer,Francesc
andRogez,Gre´gory:PoseScript:3DHumanPosesfromNaturalLanguage.In:ECCV(2022) 3
[7] Delmas,GingerandWeinzaepfel,PhilippeandMoreno-Noguer,FrancescandRogez,Gre´gory:
PoseFix:Correcting3DHumanPoseswithNaturalLanguage.In:ICCV(2023) 3
12[8] Feng,Y.,Lin,J.,Dwivedi,S.K.,Sun,Y.,Patel,P.,Black,M.J.:Posegpt:Chattingabout3dhuman
pose. ArXiv abs/2311.18836 (2023), https://api.semanticscholar.org/CorpusID:
265506071 3
[9] Fieraru,M.,Zanfir,M.,Oneata,E.,Popa,A.I.,Olaru,V.,Sminchisescu,C.:Three-dimensional
reconstructionofhumaninteractions.In:ProceedingsoftheIEEE/CVFConferenceonCom-
puterVisionandPatternRecognition.pp.7214–7223(2020) 3,4,7,8
[10] Fieraru,M.,Zanfir,M.,Szente,T.,Bazavan,E.,Olaru,V.,Sminchisescu,C.:Remips:Physically
consistent3dreconstructionofmultipleinteractingpeopleunderweaksupervision.Advances
inNeuralInformationProcessingSystems34,19385–19397(2021) 2,3
[11] Goel,S.,Pavlakos,G.,Rajasegaran,J.,Kanazawa*,A.,Malik*,J.:Humansin4D:Reconstruct-
ingandtrackinghumanswithtransformers.In:InternationalConferenceonComputerVision
(ICCV)(2023) 2,7,10
[12] Guan,P.,Weiss,A.,Balan,A.O.,Black,M.J.:Estimatinghumanshapeandposefromasingle
image.In:2009IEEE12thInternationalConferenceonComputerVision.pp.1381–1388.IEEE
(2009) 2
[13] Guler,R.A.,Kokkinos,I.:Holopose:Holistic3dhumanreconstructionin-the-wild.In:Pro-
ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
10884–10894(2019) 2
[14] Guo,C.,Zou,S.,Zuo,X.,Wang,S.,Ji,W.,Li,X.,Cheng,L.:Generatingdiverseandnatural3d
humanmotionsfromtext.In:ProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition(CVPR).pp.5152–5161(June2022) 3
[15] Jiang,B.,Chen,X.,Liu,W.,Yu,J.,Yu,G.,Chen,T.:Motiongpt:Humanmotionasaforeign
language.arXivpreprintarXiv:2306.14795(2023) 3
[16] Jiang, W., Kolotouros, N., Pavlakos, G., Zhou, X., Daniilidis, K.: Coherent reconstruction
of multiple humans from a single image. In: Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition.pp.5579–5588(2020) 2
[17] Joo,H.,Neverova,N.,Vedaldi,A.:Exemplarfine-tuningfor3dhumanmodelfittingtowards
in-the-wild3dhumanposeestimation.In:2021InternationalConferenceon3DVision(3DV).
pp.42–52.IEEE(2021) 2
[18] Kanazawa,A.,Black,M.J.,Jacobs,D.W.,Malik,J.:End-to-endrecoveryofhumanshapeand
pose.In:ComputerVisionandPatternRecognition(CVPR)(2018) 2
[19] Kingma,D.P.,Ba,J.:Adam:Amethodforstochasticoptimization.CoRRabs/1412.6980(2014),
https://api.semanticscholar.org/CorpusID:6628106 7
[20] Kirillov,A.,Mintun,E.,Ravi,N.,Mao,H.,Rolland,C.,Gustafson,L.,Xiao,T.,Whitehead,S.,
Berg,A.C.,Lo,W.Y.,Dolla´r,P.,Girshick,R.:Segmentanything.arXiv:2304.02643(2023) 7
[21] Kolotouros,N.,Pavlakos,G.,Black,M.J.,Daniilidis,K.:Learningtoreconstruct3dhuman
poseandshapeviamodel-fittingintheloop.In:ProceedingsoftheIEEE/CVFinternational
conferenceoncomputervision.pp.2252–2261(2019) 2
[22] Lassner,C.,Romero,J.,Kiefel,M.,Bogo,F.,Black,M.J.,Gehler,P.V.:Unitethepeople:Closing
theloopbetween3dand2dhumanrepresentations.In:ProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition.pp.6050–6059(2017) 2
[23] Leng,S.,Zhang,H.,Chen,G.,Li,X.,Lu,S.,Miao,C.,Bing,L.:Mitigatingobjecthalluci-
nationsinlargevision-languagemodelsthroughvisualcontrastivedecoding.arXivpreprint
arXiv:2311.16922(2023),https://arxiv.org/abs/2311.16922 4
[24] Li,Y.,Du,Y.,Zhou,K.,Wang,J.,Zhao,W.X.,Wen,J.R.:Evaluatingobjecthallucinationin
largevision-languagemodels.arXivpreprintarXiv:2305.10355(2023) 4
[25] Liang,H.,Zhang,W.,Li,W.,Yu,J.,Xu,L.:Intergen:Diffusion-basedmulti-humanmotion
generationundercomplexinteractions.arXivpreprintarXiv:2304.05684(2023) 3
[26] Liu,H.,Li,C.,Wu,Q.,Lee,Y.J.:Visualinstructiontuning.In:NeurIPS(2023) 2,4,5
[27] Lu,Y.,Jiang,D.,Chen,W.,Wang,W.,Choi,Y.,Lin,B.Y.:Wildvisionarena:Benchmarkingmul-
timodalllmsinthewild(February2024),https://huggingface.co/spaces/WildVision/
vision-arena/ 6
13[28] Muller,L.,Osman,A.A.,Tang,S.,Huang,C.H.P.,Black,M.J.:Onself-contactandhumanpose.
In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.
9990–9999(2021) 2,3,10
[29] Mu¨ller,L.,Ye,V.,Pavlakos,G.,Black,M.,Kanazawa,A.:Generativeproxemics:Apriorfor
3dsocialinteractionfromimages.arXivpreprintarXiv:2306.09337(2023) 2,3,6,7,8,9,5
[30] Pavlakos,G.,Choutas,V.,Ghorbani,N.,Bolkart,T.,Osman,A.A.A.,Tzionas,D.,Black,M.J.:
Expressivebodycapture:3Dhands,face,andbodyfromasingleimage.In:ProceedingsIEEE
Conf.onComputerVisionandPatternRecognition(CVPR).pp.10975–10985(2019) 2,4,7
[31] Pavlakos,G.,Choutas,V.,Ghorbani,N.,Bolkart,T.,Osman,A.A.,Tzionas,D.,Black,M.J.:
Expressivebodycapture:3dhands,face,andbodyfromasingleimage.In:Proceedingsofthe
IEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.10975–10985(2019) 2,
6
[32] Plappert,M.,Mandery,C.,Asfour,T.:TheKITmotion-languagedataset.BigData4(4),236–
252(dec2016).https://doi.org/10.1089/big.2016.0028,http://dx.doi.org/10.1089/big.
2016.0028 3
[33] Punnakkal,A.R.,Chandrasekaran,A.,Athanasiou,N.,Quiros-Ramirez,A.,Black,M.J.:BA-
BEL:Bodies,actionandbehaviorwithenglishlabels.In:ProceedingsIEEE/CVFConf.on
ComputerVisionandPatternRecognition(CVPR).pp.722–731(Jun2021) 3
[34] Rempe,D.,Birdal,T.,Hertzmann,A.,Yang,J.,Sridhar,S.,Guibas,L.J.:Humor:3dhuman
motion model for robust pose estimation. In: Proceedings of the IEEE/CVF international
conferenceoncomputervision.pp.11488–11499(2021) 2
[35] Shafir,Y.,Tevet,G.,Kapon,R.,Bermano,A.H.:Humanmotiondiffusionasagenerativeprior.
arXivpreprintarXiv:2303.01418(2023) 3
[36] Sun,Y.,Bao,Q.,Liu,W.,Fu,Y.,Black,M.J.,Mei,T.:Monocular,one-stage,regressionof
multiple3dpeople.In:ProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision.pp.11179–11188(2021) 2
[37] Sun,Y.,Liu,W.,Bao,Q.,Fu,Y.,Mei,T.,Black,M.J.:Puttingpeopleintheirplace:Monocular
regressionof3dpeopleindepth.In:ProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition.pp.13243–13252(2022) 2,7,8,5
[38] Tevet, G., Raab, S., Gordon, B., Shafir, Y., Cohen-or, D., Bermano, A.H.: Human motion
diffusionmodel.In:TheEleventhInternationalConferenceonLearningRepresentations(2023),
https://openreview.net/forum?id=SJ1kSyO2jwu 3
[39] Tripathi,S.,Mu¨ller,L.,Huang,C.H.P.,Omid,T.,Black,M.J.,Tzionas,D.:3Dhumanpose
estimationviaintuitivephysics.In:ConferenceonComputerVisionandPatternRecognition
(CVPR).pp.4713–4725(2023),https://ipman.is.tue.mpg.de 10
[40] Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., Zhou, D.:
Self-consistencyimproveschainofthoughtreasoninginlanguagemodels.arxiv.Preprintposted
onlineMarch21,10–48550(2022) 6
[41] Wei,J.,Wang,X.,Schuurmans,D.,Bosma,M.,hsinChi,E.H.,Xia,F.,Le,Q.,Zhou,D.:Chain
ofthoughtpromptingelicitsreasoninginlargelanguagemodels.ArXivabs/2201.11903(2022),
https://api.semanticscholar.org/CorpusID:246411621 5
[42] Xu,Y.,Zhang,J.,Zhang,Q.,Tao,D.:ViTPose:Simplevisiontransformerbaselinesforhuman
poseestimation.In:AdvancesinNeuralInformationProcessingSystems(2022) 6
[43] Yin, Y., Guo, C., Kaufmann, M., Za´rate, J.J., Song, J., Hilliges, O.: Hi4d: 4d instance seg-
mentationofclosehumaninteraction.2023IEEE/CVFConferenceonComputerVisionand
Pattern Recognition (CVPR) pp. 17016–17027 (2023), https://api.semanticscholar.
org/CorpusID:257766362 8
[44] Zanfir,A.,Marinoiu,E.,Sminchisescu,C.:Monocular3dposeandshapeestimationofmultiple
peopleinnaturalscenes-theimportanceofmultiplesceneconstraints.In:Proceedingsofthe
IEEEConferenceonComputerVisionandPatternRecognition.pp.2148–2157(2018) 2
[45] Zhang,J.,Zhang,Y.,Cun,X.,Huang,S.,Zhang,Y.,Zhao,H.,Lu,H.,Shen,X.:T2m-gpt:Gen-
eratinghumanmotionfromtextualdescriptionswithdiscreterepresentations.In:Proceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR)(2023) 3
14AppendixforZero-ShotPosePriorsfromLanguage
Inthisappendix,weprovideadditionaldetailsaboutourmethod(Section7),detailsaboutmetrics
(Section8.1),additionalquantitativeresults(Section8.2),examplesoffailurecases(Section8.3),
experimentswithadifferentLMM(Section8.4),andmorequalitativecomparisons(Section8.5).
7 AdditionalMethodDetails
7.1 LMMPrompts
Theboxbelowcontainsourpromptforthetwo-personexperiments.
Youareahelpfulassistant.Youfollowalldirectionscorrectlyandprecisely.
Foreachimage,identifyallpairsofbodypartsofPerson1andPerson2thataretouching.
WritealloftheseinaMarkdowntablewherethefirstcolumnis”Person1BodyPart”and
thesecondcolumnis”Person2BodyPart”.
YoucanpickwhichisPerson1andwhichisPerson2.
Thelistofpossiblebodypartsis:head,neck,chest,stomach,waist(back),waist(front),back,
shoulder(back),shoulder(front),arm,hand,leg,foot,butt.
Donotincludeleft/right.
ListALLpairsyouareconfidentabout.
Ifyouarenotconfidentaboutanypairs,outputanemptytable.
Carefullywriteyourreasoningfirst,andthenwritetheMarkdowntable.
Theboxbelowcontainsourpromptfortheone-personexperiment.
Youareahelpfulassistant.Youanswerallquestionscarefullyandcorrectly.
Identifywhichbodypartsoftheyogiaretouchingeachotherinthisimage(ifany).
WriteeachpairinaMarkdowntablewithtwocolumns.
EachbodypartMUSTbefromthislist:
head,back,shoulder,arm,hand,leg,foot,stomach,butt,ground
Donotwrite”left”or”right”.
Describeandnametheyogapose,andthenwritetheMarkdowntable.
Notethattheposemaydifferfromthestandardversion,sopaycloseattention.
Onlylistapartifyou’recertainaboutit.
Ineachsetting,thepromptisgivenasthe“systemprompt”totheGPT-4API,andtheonlyother
messagegivenasinputcontainstheinputimagewiththe“high”detailsetting.
7.2 CoarseRegions
Figure 7 illustrates the coarse regions referenced in the prompt in our two-person experiments.
Figure8illustratesthecoarseregionsreferencedinthepromptinourone-personexperiments.In
theone-personcase,thepromptdoesnotmentionthe“chest,”“neck,”or“waist”regions,sincethey
tendtobelessimportantforcontactsinyogaposes,andthefront/backshouldersaremergedintoone
region,sincethedistinctiontendstobelessimportantforcontactsinyogaposes.
7.3 ConvertingConstraintstoLossesin1vs.2PersonCases
OurimplementationoftheconversionfromconstraintsoutputbytheLMMtolossfunctionsdiffers
slightlybetweenthetwo-personandone-personcases.
7.3.1 Two-person
SinceweasktheVLMnottodifferentiatebetween“left”and“right”limbs,whenthereshouldbe
aconstraintonbothlimbs(e.g.bothhands),takingtheminimumdistanceindependentlyforeach
constraintpairmayleadtoaconstraintononlyonelimb.Consequently,ifthesamebodypart(e.g.
1Figure7:Color-codedcoarseregionsinthetwo-personprompt:head,neck,chest,stomach,waist(back),waist
(front),back,shoulder(back),shoulder(front),arm,hand,leg,foot,butt.Notethatsomeoftheseregionsoverlap.
Forinstance,the“back”includesthe“waist(back)”and“shoulder(back)”regionsasasubset.
Figure8:Color-codedcoarseregionsintheone-personprompt:head,stomach,back,shoulder,arm,hand,leg,
foot,butt.Notethatthe“chest,”“neck,”and“waist(front)”regionsarenotcoveredbytheregionsintheprompt,
sincetheytendtohavelessimportanceforcontactsinyogaposes.
2“hand”)ismentionedinatleasttwoseparaterowsofthetableoutputbytheLMM(withoutany“left”
or“right”prefix),weenforcethatboththeleftandrightlimbsofthistypemustparticipateintheloss.
We also handle some variations in how the LMM references body parts. First, we check for the
followingtermsinadditiontothecoarseregionsnamedintheprompt:lefthand,righthand,leftarm,
rightarm,leftfoot,rightfoot,leftleg,rightleg,leftshoulder,rightshoulder,leftshoulder(front),
rightshoulder(front),leftshoulder(back),rightshoulder(back),waist.“waist”correspondstothe
unionof“waist(front)”and“waist(back).”Eachofthesetermsismappedtothecorrespondingset
offine-grainedregions,similartothecoarseregionsshowninFigure7.AsstatedinSection3.3of
themainpaper,ifa“left”or“right”partisexplicitlynamedbytheLMM’soutput,thispartofthe
coarseregionisdirectlyusedwithoutconsideringtheotherpart.
Second,wefindtherearesomecaseswheretheLMMexpressesuncertaintybetweenregionsusinga
delimiterlike“/”(e.g.“hand/arm”).SowespliteachentryintheMarkdownTable’soutputbythe
delimiter“/”andwecomputethelossforeachpossibleregionthatislisted;wethensumallofthese
losses.
7.3.2 One-person
Intheone-personexperiment,wedonotmakeuseoftheconstraintsinvolvingthe“ground”thatthe
LMMoutputs.Similartothetwo-personcase,thecodeforconvertingtheLMM’soutputtoaloss
functionchecksforthefollowingtermsinadditiontothebodyregionslistedintheprompt:lefthand,
righthand,leftarm,rightarm,leftfoot,rightfoot,leftleg,rightleg,leftshoulder,rightshoulder,left
shoulder(front),rightshoulder(front),leftshoulder(back),rightshoulder(back),waist.Eachof
thesetermsismappedtothecorrespondingsetoffine-grainedregions,similartothecoarseregions
showninFigure7.
7.4 BoundingBoxesandCropping
AsstatedinSection3ofthemainpaper,wetakeboundingboxesofthesubjectsofinterestasinput
andusethemtocroptheimageinordertoisolatetheperson/peopleofinterestwhenpromptingthe
LMM.ForFlickrCI3D,weusetheground-truthboundingboxesofthetwosubjectsofinterest.For
theotherdatasets,weusekeypointsdetectedbyViTPose/OpenPosetocreatetheboundingboxes.For
thesingle-personMOYOdataset,wemanuallycheckthattheboundingboxesfromthekeypointsand
theselectedHMR2outputscorrespondtothecorrectpersonintheimage.Wenotethatthebaseline
HMR2+optalsobenefitsfromthismanualchecking,sinceHMR2+optalsodependsontheHMR2
outputsandaccuratekeypoints.
8 Experiments
8.1 PCCCalculation
Figure9illustratesthe75fine-grainedregionsusedforPCCcalculation,whicharethesameasthose
usedin[9].WeoptedtocomputePCConthefine-grainedregionsratherthanonthecoarseones
sincepriorworkusesthefine-grainedregions[29]andsincewewanttomeasurecontactcorrectness
atafinergranularity(e.g.uppervs.lowerthighvs.knee).SincetheregressorsBEVandHMR2use
theSMPLmeshwhilethefine-grainedregionsaredefinedontheSMPL-Xmesh,weuseamatrix
M ∈Rnumverticessmplx×numverticessmpltoconverttheSMPLmeshestoSMPL-Xinordertocompute
PCC.
8.2 Per-personPA-MPJPE
Table5showstheper-personPA-MPJPEforeachofthedatasetsusedinourtwo-personexperiments.
8.3 Failurecases
Figure10showsexamplesoftwotypesofLingoPosefailures:(1)incorrectchirality(examplea)and
(2)hallucination(examplesbandc).Inexample(a),thetopconstraintsarecorrectbutwithoutthe
chiralityspecified.Theoptimizationthenbringsbothhandsofonepersontoroughlythesamepoint
ontheotherperson’swaist,ratherthanpositioningonehandoneachhip.Similarly,bothhandsofthe
3Figure9:Color-coded75fine-grainedregionsusedforPCCcalculation
Original Image Subject(s) ProsePose
Arm, Waist (Back) × 9
(a) Arm, Back × 8
Hand, Waist (Back) × 7
Hand, Back × 8
(b) Arm, Back × 6
Arm, Waist (Back) × 5
Hand, Foot × 16
(c)
Figure10:FailurecasesWeshowexamplesinwhichProsePosefailstooutputasemanticallycorrectpose.
Theconstraintsshownarethetop3constraints(orthetotalnumberofconstraints,whicheverissmaller)that
meetthethresholdf alongwiththeircounts(f =1fortwo-personexperimentsandf =10fortheone-person
experiment).
4Table5:Two-personResults.Per-personPA-MPJPE(lowerisbetter).ForFlickrCI3D,PA-MPJPEiscomputed
usingthepseudo-ground-truthfits.
Hi4D FlickrCI3D CHI3D
PA-MPJPE PA-MPJPE PA-MPJPE
↓ ↓ ↓
Zero-shot
BEV[37] 76 71 51
Heuristic 65 31 48
ProsePose 65 31 49
Supervised
BUDDI[29] 70 43 47
otherpersonarepositionedonthesameshoulderofthefirstperson.Examples(b)and(c)bothshow
casesofhallucination.Inexample(b),thehandispredictedtotouchthebackratherthanthehand.
Inexample(c),thehandispredictedtotouchthefootratherthantheleg.Interestingly,intheyoga
example,GPT4-Vcorrectlypredictsthenameoftheyogaposeinall20samples(“ParivrttaJanu
Sirsasana”).However,itoutputsaconstraintbetweenahandandafoot,whichistrueinthestandard
formofthisposebutnotinthedisplayedformofthepose.Consequently,theoptimizationbringsthe
lefthandclosertotherightfootthantotherightknee.
8.4 DifferentMultimodalModel
Inthissection,weevaluateProsePosewhenusingadifferentLMM.WeuseLLaVA-NeXT34B(i.e.
LLaVAv1.6)[26]astheLMM.Wefindthatthemodeldoesnotperformwellindirectlygenerating
thetableofconstraintsfromtheimage.Thisispresumablyaresultofaweakerlanguagemodelin
LLaVAcomparedtoGPT4Therefore,weinsteadgenerateacaptionfromtheLMM,andwefeedthe
captionalonetoGPT4inordertoconvertitintoatableofconstraints.Weevaluatedafewdifferent
prompts on the validation sets and chose the prompts with the best performance therein. For the
two-personexperiments,weusethefollowingpromptforLLaVA:
Describetheposeofthetwopeople.
WethenusethefollowingpromptwithGPT4torewritethecaptionsothatitdoesnotmentionleft
andrighttorefertolimbs,sincewefindthattheLMMisnotreliablycorrectindoingso:
Rewritethecaptionbelowsothatitdoesn’tmention”left”or”right”todescribeanyhand,
arm,foot,orleg.Therevisedcaptionshouldotherwisebeidentical.Writeonlytherevised
captionandnoothertext.
WethenusethefollowingpromptwithGPT4tocreatetheformattedtable.
Youareahelpfulassistant.YouwillfollowALLrulesanddirectionsentirelyandprecisely.
GivenadescriptionofPerson1andPerson2whoarephysicallyincontactwitheachother,
createaMarkdowntablewiththecolumns”Person1BodyPart”and”Person2BodyPart”,
listingthebodypartsofthetwopeoplethatareguaranteedtobeincontactwitheachother,
fromthefollowinglist.ALLbodypartsthatyoulistmustbefromthislist.Youcanchoose
which person is Person 1 and which is Person 2. Body parts: ”chest”, ”stomach”, ”waist
(front)”,”waist(back)”,”shoulder(front)”,”shoulder(back)”,”back”,”hand”,”arm”,”foot”,
”leg”,”head”,”neck”,”butt”Notethat”back”includestheentireareaoftheback.
Include all contact points that are directly implied by the description, not just those that
are explicitly mentioned. If there are no contact points between these body parts that the
descriptionimplicitlyorexplicitlyimplies,yourtableshouldcontainonlythecolumnnames
andnootherrows.
First,writeyourreasoning.ThenwritetheMarkdowntable.
Fortheone-personcase,weusethefollowingpromptforLLaVA:
5Table6:LLaVAResults.ErrdenotesJointPA-MPJPEforthetwo-persondatasets(Hi4D,FlickrCI3D,CHI3D)
andPA-MPJPEforMOYO.LowerisbetterforErr,andhigherisbetterforAvg.PCC.Boldindicatesbest
zero-shotmethodineachcolumn.
Hi4D FlickrCI3D CHI3D MOYO
Err Err PCC Err PCC Err PCC
↓ ↓ ↑ ↓ ↑ ↓ ↑
Heuristic 116 67 77.8 105 74.1 – –
HMR2+opt – – – – – 81 85.2
GPT4-V 93 58 79.9 100 75.8 82 87.8
LLaVA+GPT4 95 60 79.7 101 75.2 82 85.2
Describetheperson’spose.
Weusethesamepromptasabovetorewritethecaption.Wethenusethefollowingprompttocreate
theformattedtable:
Youareahelpfulassistant.YouwillfollowALLrulesanddirectionsentirelyandprecisely.
Givenadescriptionofayogapose,createaMarkdowntablewiththecolumns”BodyPart1”
and”BodyPart2”,listingthebodypartsofthepersonthatareguaranteedtobeincontact
witheachother,fromthefollowinglist.ALLbodypartsthatyoulistmustbefromthislist.
Body parts: ”head”, ”back”, ”shoulder”, ”arm”, ”hand”, ”leg”, ”foot”, ”stomach”, ”butt”,
”ground”Notethat”back”includestheentireareaoftheback.
Include all contact points that are directly implied by the description, not just those that
are explicitly mentioned. If there are no contact points between these body parts that the
descriptionimplicitlyorexplicitlyimplies,yourtableshouldcontainonlythecolumnnames
andnootherrows.
First,writeyourreasoning.ThenwritetheMarkdowntable.
Weusethegpt-4-0125-previewversionofGPT4viatheOpenAIAPI(weobtainedbetterresults
usingthismodelthangpt-4-1106-preview).Thelatencyofthisapproachismuchhigherthan
thesingle-stageapproachusedwithGPT4-V,sincewemustfeedeachcaptionindividuallytothe
OpenAIAPI.Therefore,wesetN = 5fortheseexperiments.SincewechangeN,wealsoneed
toselectappropriatethresholdsf andt.AsintheexperimentswithGPT4-V,wesett=N forall
datasetsexceptCHI3D.ForCHI3D,wefindonthevalidationsetthatt=2worksbetterthant=1,
sowesett=2.AsintheexperimentswithGPT4-V,wesetf =1forthe2-persondatasets,andwe
setf =3forMOYO,toapproximatetheratiof/N usedintheGPT4-Vexperiments.Finally,when
convertingtheconstraintpairstolossfunctions,wefoundthatonasmallnumberofexamples,the
pipelineproducedalargenumberofconstraints,leadingtoveryslowlossfunctions.Therefore,we
discardedlossfunctionsthatarelongerthan10000characters.
Table6showstheresults.Onthe2-persondatasets,theLLaVA+GPT4approachperformsbetterthan
thecontactheuristicbutnotaswellasGPT4-V.Thisisinlinewithholisticmultimodalevaluations
that indicate that GPT4-V performs better than LLaVA [27]. On the 1-person yoga dataset, the
performanceofLLaVA+GPT4iscomparablewiththatofthebaseline(HMR2+opt).Thereason
thatLLaVAperformsworsethanGPT4-VinthissettingmaybethatLLaVAdoesnothaveenough
trainingdataonyogatoprovideusefulconstraints.
8.5 AdditionalQualitativeResults
Figures 11, 12, 13, and 14 show additional, randomly selected examples from the multi-person
FlickrCI3Dtestset.Figures15,16,17,and18showthesameexamplescomparingProsePosewith
thepseudo-groundtruthfits.Figures19,20,and21showadditional,randomlyselectedexamples
fromtheHi4Dtestset.Figures22and23showadditional,randomlyselectedexamplesfromthe
CHI3Dvalidationset(whichweuseasthetestsetfollowing[29]).Figures24and25showadditional,
randomlyselectedexamplesfromthe1-personyogaMOYOtestset.
6Figure11:Non-curatedexamplesfromtheFlickrCI3Dtestset.Theyarerandomlyselectedfromtheexamples
forwhichthereisatleastonenon-emptyconstraintset.
7
]82[
IDDUB
citsirueH
esoPesorPFigure12:Non-curatedexamplesfromtheFlickrCI3Dtestset.Theyarerandomlyselectedfromtheexamples
forwhichthereisatleastonenon-emptyconstraintset.
8
]82[
IDDUB
citsirueH
esoPesorPFigure13:Non-curatedexamplesfromtheFlickrCI3Dtestset.Theyarerandomlyselectedfromtheexamples
forwhichthereisatleastonenon-emptyconstraintset.
9
]82[
IDDUB
citsirueH
esoPesorPFigure14:Non-curatedexamplesfromtheFlickrCI3Dtestset.Theyarerandomlyselectedfromtheexamples
forwhichthereisatleastonenon-emptyconstraintset.
10
]82[
IDDUB
citsirueH
esoPesorPFigure15:Non-curatedexamplesfromtheFlickrCI3Dtestset,comparingProsePosewiththepseudo-ground
truthfits.Theyarerandomlyselectedfromtheexamplesforwhichthereisatleastonenon-emptyconstraintset.
11
hturT
dnuorG-oduesP
esoPesorPFigure16:Non-curatedexamplesfromtheFlickrCI3Dtestset,comparingProsePosewiththepseudo-ground
truthfits.Theyarerandomlyselectedfromtheexamplesforwhichthereisatleastonenon-emptyconstraintset.
12
hturT
dnuorG-oduesP
esoPesorPFigure17:Non-curatedexamplesfromtheFlickrCI3Dtestset,comparingProsePosewiththepseudo-ground
truthfits.Theyarerandomlyselectedfromtheexamplesforwhichthereisatleastonenon-emptyconstraintset.
13
hturT
dnuorG-oduesP
esoPesorPFigure18:Non-curatedexamplesfromtheFlickrCI3Dtestset,comparingProsePosewiththepseudo-ground
truthfits.Theyarerandomlyselectedfromtheexamplesforwhichthereisatleastonenon-emptyconstraintset.
14
hturT
dnuorG-oduesP
esoPesorPFigure19:Non-curatedexamplesfromtheHi4Dtestset.Theyarerandomlyselectedfromtheexamplesfor
whichthereisatleastonenon-emptyconstraintset.
15
]82[
IDDUB
citsirueH
esoPesorPFigure20:Non-curatedexamplesfromtheHi4Dtestset.Theyarerandomlyselectedfromtheexamplesfor
whichthereisatleastonenon-emptyconstraintset.
16
]82[
IDDUB
citsirueH
esoPesorPFigure21:Non-curatedexamplesfromtheHi4Dtestset.Theyarerandomlyselectedfromtheexamplesfor
whichthereisatleastonenon-emptyconstraintset.
17
]82[
IDDUB
citsirueH
esoPesorPFigure 22: Non-curated examples from the CHI3D validation set (which we use as the test set). They are
randomlyselectedfromtheexamplesforwhichthereareatleastnineteennon-emptyconstraintsets(sincewe
sett=2forCHI3D).
18
]82[
IDDUB
citsirueH
esoPesorPFigure 23: Non-curated examples from the CHI3D validation set (which we use as the test set). They are
randomlyselectedfromtheexamplesforwhichthereareatleastnineteennon-emptyconstraintsets(sincewe
sett=2forCHI3D).
19
]82[
IDDUB
citsirueH
esoPesorPFigure24:Non-curatedexamplesfromtheMOYOtestset.Theyarerandomlyselectedfromtheexamplesfor
whichthereisatleastonenon-emptyconstraintset.
20
]11[
2RMH
mitpo+2RMH
esoPesorPFigure25:Non-curatedexamplesfromtheMOYOtestset.Theyarerandomlyselectedfromtheexamplesfor
whichthereisatleastonenon-emptyconstraintset.
21
]11[
2RMH
mitpo+2RMH
esoPesorP