Towards A Human-in-the-Loop LLM Approach to
Collaborative Discourse Analysis
Clayton Cohn1[0000−0003−0856−9587], Caitlin Snyder1[0000−0002−3341−0490], Justin
Montenegro2, and Gautam Biswas1[0000−0002−2752−3878]
1 Vanderbilt University, Nashville, TN 37240, USA
clayton.a.cohn@vanderbilt.edu
2 Martin Luther King, Jr. Academic Magnet High School, Nashville, TN 37203, USA
Abstract. LLMshavedemonstratedproficiencyincontextualizingtheir
outputsusinghumaninput,oftenmatchingorbeatinghuman-levelper-
formanceonavarietyoftasks.However,LLMshavenotyetbeenusedto
characterize synergistic learning in students’ collaborative discourse. In
thisexploratorywork,wetakeafirststeptowardsadoptingahuman-in-
the-looppromptengineeringapproachwithGPT-4-Turbotosummarize
and categorize students’ synergistic learning during collaborative dis-
course. Our preliminary findings suggest GPT-4-Turbo may be able to
characterize students’ synergistic learning in a manner comparable to
humans and that our approach warrants further investigation.
Keywords: LLM · Collaborative Learning · Human-in-the-Loop · Dis-
course Analysis · K12 STEM.
1 Introduction
Computational modeling of scientific processes has been shown to effectively
foster students’ Science, Technology, Engineering, Mathematics, and Comput-
ing (STEM+C) learning [5], but task success necessitates synergistic learning
(i.e., the simultaneous development and application of science and computing
knowledge to address modeling tasks), which can lead to student difficulties
[1]. Research has shown that problem-solving environments promoting synergis-
tic learning in domains such as physics and computing often facilitate a bet-
ter understanding of physics and computing concepts and practices when com-
pared to students taught via a traditional curriculum [5]. Analyzing students’
collaborative discourse offers valuable insights into their application of both
domains’ concepts as they construct computational models [8]. Unfortunately,
manually analyzing students’ discourse to identify their synergistic processes is
time-consuming, and programmatic approaches are needed.
Inthispaper,wetakeanexploratoryfirststeptowardsadoptingahuman-in-
the-loop LLM approach from previous work called Chain-of-Thought Prompting
+ Active Learning [3](detailedinSection3)tocharacterizethesynergisticcon-
tent in students’ collaborative discourse. We use a large language model (LLM)
to summarize conversation segments in terms of how physics and computing
4202
yaM
6
]LC.sc[
1v77630.5042:viXra2 C. Cohn et al.
concepts are interwoven to support students’ model building and debugging
tasks.WeevaluateourapproachbycomparingtheLLM’ssummariestohuman-
produced ones (using an expert human evaluator to rank them) and by qualita-
tively analyzing the summaries to discern the LLM’s strengths and weaknesses
alongside a physics and computer science teacher (the Educator) with experi-
enceteachingtheC2STEMcurriculum(seeSection3.1).Withinthisframework,
we analyze data from high school students working in pairs to build kinematics
modelsandanswerthefollowingresearchquestions:RQ1)Howdoesthequality
of human- and LLM-generated summaries and synergistic learning characteri-
zations of collaborative student discourse compare?, and RQ2) What are the
LLM’sstrengths,andwheredoesitstruggle,insummarizingandcharacterizing
synergistic learning in physics and computing?
As this work is exploratory, due to the small sample size, we aim not to
present generalizable findings but hope that our results will inform subsequent
researchasweworktowardsforgingahuman-AIpartnershipbyprovidingteach-
erswithactionable,LLM-generatedfeedbackandrecommendationstohelpthem
guide students in their synergistic learning.
2 Background
RoschelleandTeasley[7]definecollaborationas“a coordinated, synchronous ac-
tivity that is a result of a continuous attempt to construct and maintain a shared
conception of a problem.” This development of a shared conceptual understand-
ingnecessitatesmulti-facetedcollaborativediscourseacrossmultipledimensions:
social (e.g., navigating the social intricacies of forming a consensus [12]), cogni-
tive(e.g.,thedevelopmentofcontext-specificknowledge[8]),andmetacognitive
(e.g., socially shared regulation [4]). Researchers have developed and leveraged
frameworkssituatedwithinlearningtheorytoclassifyandanalyzecollaborative
problem solving (CPS) both broadly (i.e., across dimensions [6]) and narrowly
(i.e.,byfocusingononeCPSaspecttogainin-depthinsight,e.g.,argumentative
knowledge construction [12]). In this paper, we focus on one dimension of CPS
that is particularly important to the context of STEM+C learning: students’
cognitive integration of synergistic domains.
Leveraging CPS frameworks to classify student discourse has traditionally
been done through hand-coding utterances. However, this is time-consuming
and laborious, leading researchers to leverage automated classification methods
suchasrule-basedapproaches,supervisedmachinelearningmethods,and(more
recently)LLMs[10].UtilizingLLMscanhelpextendpreviousworkonclassifying
synergisticlearningdiscourse,whichhasprimarilyreliedonthefrequencycounts
of domain-specific concept codes [8,5]. In particular, the use of LLMs can help
address the following difficulties encountered while employing traditional meth-
ods: (1) concept codes are difficult to identify programmatically, as rule-based
approaches like regular expressions (regex) have difficulties with misspellings
and homonyms; (2) the presence or absence of concept codes is not analyzed in
a conversational context; and (3) the presence of cross-domain concept codes isTowards A HITL LLM Approach to Collaborative Discourse Analysis 3
not necessarily indicative of synergistic learning, as synergistic learning requires
students to form connections between concepts in both domains.
Recent advances in LLM performance capabilities have allowed researchers
tofindnewandcreativewaystoapplythesepowerfulmodelstoeducationusing
in-contextlearning (ICL)[2](i.e.,providingtheLLMwithlabeledinstancesdur-
ing inference) in lieu of traditional training that requires expensive parameter
updates. One prominent extension of ICL is chain-of-thought reasoning (CoT)
[11], which augments the labeled instances with “reasoning chains” that explain
therationalebehindthecorrectanswerandhelpguidetheLLMtowardsthecor-
rect solution. Recent work has found success in leveraging CoT towards scoring
and explaining students’ formative assessment responses in the Earth Science
domain [3]. In this work, we investigate this approach as a means to summarize
and characterize synergistic learning in students’ collaborative discourse.
3 Methods
This paper extends the previous work of 1) Snyder et al. on log-segmented dis-
course summarization defined by students’ model building segments extracted
from their activity logs [9], and 2) Cohn et al. on a human-in-the-loop prompt
engineering approach called Chain-of-Thought Prompting + Active Learning [3]
(the Method) for scoring and explaining students’ science formative assessment
responses. The original Method is a three-step process: 1) Response Scoring,
where two human reviewers manually label a sample of students’ formative as-
sessment responses and identify disagreements (i.e., sticking points) the LLM
may similarly struggle with; 2) Prompt Development, which employs few-shot
CoT prompting to address the sticking points and help align the LLM with
the humans’ scoring consensus; and 3) Active Learning, where a knowledgeable
human (e.g., a domain expert, researcher, or instructor) acts as an “oracle” and
identifiestheLLM’sreasoningerrorsonavalidationset,thenappendsadditional
few-shot instances that the LLM struggled with to the prompt and uses CoT
reasoning to help correct the LLM’s misconceptions. We illustrate the Method
in Figure 1. For a complete description of the Method, please see [3].
In this work, we combine log-based discourse segmentation [9] and CoT
prompting [3] to generate more contextualized summaries of students’ discourse
segmentstostudystudents’synergisticlearningprocessesbylinkingtheirmodel
constructionanddebuggingactivitieswiththeirconversationsduringeachprobl-
em-solvingsegment.WeprovideSupplementaryMaterials3 thatinclude1)addi-
tionalinformationaboutthelearningenvironment,2)methodapplicationdetails
(including our final prompt and few-shot example selection methodology), 3) a
moreindepthlookatourconversationwiththeEducator,and4)amoredetailed
analysis of the LLM’s strengths and weaknesses while applying the Method.
3 https://github.com/oele-isis-vanderbilt/AIED24_LBR4 C. Cohn et al.
3.1 STEM+C Learning Environment, Curriculum, and Data
OurworkinthispapercentersontheC2STEMlearningenvironment[5],where
students learn kinematics by building computational models of the 1- and 2-D
motion of objects. C2STEM combines block-based programming with domain-
specific modeling blocks to support the development and integration of science
and computing knowledge as students create partial or complete models that
simulate behaviors governed by scientific principles. This paper focuses on the
1-D Truck Task, where students use their knowledge of kinematic equations to
model the motion of a truck that starts from rest, accelerates to a speed limit,
cruises at that speed, then decelerates to come to a stop at a stop sign.
Our study, approved by our university Institutional Review Board, included
26 consented high school students (aged 14-15) who completed the C2STEM
kinematics curriculum. Students’ demographic information was not collected as
part of this study (we began collecting it in later studies). Data collection in-
cludedloggedactionsintheC2STEMenvironment,savedprojectfiles,andvideo
and audio data (collected using laptop webcams and OBS software). Our data
analysis included 9 dyads (one group had a student who did not consent to
data collection, so we did not analyze that group; and we had technical issues
with audio data from other groups). The dataset includes 9 hours of discourse
transcripts and over 2,000 logged actions collected during one day of the study.
Student discourse was transcribed using Otter.ai and edited for accuracy.
3.2 Approach
We extend the Method, previously used for formative assessment scoring and
feedback,topromptGPT-4-Turbotosummarizesegmentsofstudents’discourse
andidentifytheDiscourseCategory (definedmomentarily)byansweringthefol-
lowing question: “Given a discourse segment, and its environment task context
and actions, is the students’ conversation best characterized as physics-focused
(i.e., the conversation is primarily focused on the physics domain), computing-
focused (i.e., the conversation is primarily focused on the computing domain),
physics-and-computing-synergistic (i.e., students discuss concepts from both do-
mains, interleaving them throughout the conversation, and making connections
Fig.1. Chain-of-Thought Prompting + Active Learning, identified by the green box,
whereeachbluediamondisastepintheMethod.Yellowboxesrepresenttheprocess’s
application to the classroom detailed in prior work [3].Towards A HITL LLM Approach to Collaborative Discourse Analysis 5
between them), or physics-and-computing-separate (i.e., students discuss both
domains but do so separately without interleaving)?” We use the recently re-
leasedGPT-4-TurboLLM(gpt-4-0125-preview)becauseitprovidesanextended
context window (128,000 tokens).
We selected 10 training instances and 12 testing instances (10 additional
segments were used as a validation set to perform Active Learning) prior to
Response Scoring, using stratified sampling to approximate a uniform distribu-
tion across Discourse Categories for both the train and test sets. Note that the
student discourse was segmented based on which element of the model the stu-
dents were working on (identified automatically via log data). During Response
Scoring, the first two authors of this paper (Reviewers R1 and R2, respectively)
independently evaluated the training set segments, classifying each segment as
belonging to one of the four Discourse Categories. For each segment the Re-
viewers disagreed on, the reason for disagreement was noted as a sticking point,
and the segment was discussed until a consensus was reached on the specific
Discourse Category for that segment. R1 and R2 initially struggled to agree on
segments’ Discourse Categories (Cohen’s k = 0.315). This is because segments
often contained concepts from both domains that may or may not have been
interwoven, so it was not always clear which Discourse Category a segment be-
longed to. Because of this, the Reviewers ultimately opted to label all segments
via consensus coding.
DuringPromptDevelopment,weprovidedtheLLMwithexplicittaskinstruc-
tions,curricularandenvironmentcontext,andgeneralguidelines(e.g.,instruct-
ing the LLM to cite evidence directly from the students’ discourse to support
its summary decisions and Discourse Category choice). We supplemented the
prompt with extensive contextual information not found in previous work [9],
includingtheDiscourseCategories,C2STEMvariablesandtheirvalues,physics
andcomputingconceptsandtheirdefinitions,andstudents’actionsinthelearn-
ing environment (derived from environment logs). Four labeled instances were
initiallyappendedtothepromptasfew-shotexamples(oneperDiscourseCate-
gory).ActiveLearningwasperformedforatotaloftworoundsover10validation
set instances, at the end of which one additional few-shot instance was added.
Before testing, R1 wrote summaries (and labeled Discourse Categories) for
the12testinstances.R2thencomparedthehuman-generatedsummariestotwo
LLMs’ summaries: GPT-4-Turbo and GPT-4. We compare GPT-4 to GPT-4-
Turbo to see which LLM is most promising for use in future work. To evaluate
RQ1, R2 used “ranked choice” to rank the three summaries from best to worst
foreachtestsetinstancewithoutknowledgeofwhetherthesummariesweregen-
eratedbyahuman,GPT-4-Turbo,orGPT-4(theCompetitors).Threerankings
were used for the scoring: (1) Wins (the number of times each Competitor was
ranked higher than another Competitor across all instances, i.e., the best Com-
petitor for an individual segment receives two “wins” for outranking the other
twoCompetitorsforthatsegment);(2)Best (thenumberofinstanceseachCom-
petitor was selected as the best choice); and (3) Worst (the number of instances
each Competitor was selected as the worst choice). To answer RQ1, we used6 C. Cohn et al.
theWilcoxonsigned-ranktesttodetermineifthedifferenceinrankingsbetween
the Human and GPT-4-Turbo’s summaries was statistically significant. We also
qualitatively compared the differences between the summaries. To answer RQ2,
we performed qualitative analysis using the constant comparative method and
interviewed the Educator to derive GPT-4-Turbo’s strengths and weaknesses
using the Method for our task.
4 Findings
To answer RQ1, we first present the test results for the Wins, Best, and Worst
rankingsforthe12testsetinstancesforallthreeCompetitorsinTable1.Forall
threemetrics,thehumanoutperformedGPT-4-TurboandGPT-4-Turbooutper-
formedGPT-4,asevaluatedbyR2.Whileranking,R2remarkedonseveralocca-
sionsthatGPT-4-Turbo’sresponsesstoodoutasbeingthemostdetailedandin-
formative regarding the students’ problem-solving processes. GPT-4-Turbo also
correctly identified a segment’s Discourse Category and explained why it did
not belong to another category even though the distinction was nuanced. Con-
versely, GPT-4 suffered from hallucinations in a number of instances and failed
to produce a Best summary. For example, GPT-4 included a physics concept
in its summary that was not part of the discourse and cited irrelevant evidence
(e.g., it cited a student who said “I got it” as evidence of a computing concept).
R2 also remarked GPT-4 was prone to generating summaries that lacked depth
and detail. There were no discernible trends in the LLMs’ abilities to classify
segments across different Discourse Categories.
To quantify our answer for RQ1, we
testedthedifferencesbetweenthethreeCom-
R1GPT-4-TurboGPT-4 petitors’ raw rankings via Wilcoxon signed-
Wins 17 12 7 rank tests, which yielded p = 0.519 and
Best 8 4 0 p = 0.266 comparing the Human to GPT-4-
Worst 3 4 5 Turbo and GPT-4-Turbo to GPT-4, respec-
tively; and p = 0.052 comparing the Human
Table 1. Competitors’ rankings to GPT-4, implying the ranking differences
across all test set instances. The
were not significant at the p = 0.05 level
best-performing Competitor for
forallthreecomparisons.Futureworkwitha
each segment is in bold.
largersamplesizeisnecessarytodetermineif
humansoutperformGPT-4-TurboorGPT-4,
as our study cannot rule this out (especially
given the low p-value comparing the Human to GPT-4). Contrary to the quan-
titative findings, our qualitative analysis revealed that GPT-4-Turbo exhibited
several strengths relative to the Human, such as providing greater detail and
explaining why a Discourse Category was not appropriate for a given segment.
This is especially useful in supporting classroom instructors and generating au-
tomated,adaptivescaffoldingforstudents.ToanswerRQ1,theMethodenables
GPT-4-Turbotoperformsimilarlytohumansforthistaskanddatasetbutboth
Competitors exhibit nuances that warrant further investigation.Towards A HITL LLM Approach to Collaborative Discourse Analysis 7
We answer RQ2 by analyzing GPT-4-Turbo’s test set generations via the
constant comparative method to discern its strengths and weaknesses and by
integrating insights from our conversation with the Educator. The LLM con-
sistently followed prompt instructions, cited relevant discourse pieces similarly
to humans (often citing the exact same discourse pieces as R1), adhered to
the CoT reasoning chains outlined in the few-shot examples, and selectively
extracted relevant information from the segments for summarization. These re-
sults corroborate previous findings [3]. Notably, the LLM seamlessly integrated
the additional context (see Section 3.2) and exhibited accurate coreference res-
olution, correctly identifying entities like physics and computing concepts when
ambiguous pronouns such as “it” or “that” were used. The LLM effectively rec-
ognizedambiguoussegments,andexplainedwhenmultipleDiscourseCategories
maybeapplicableandwhythelessrelevantonewasnotchosen.Themodelalso
showcased adept zero-shot identification of concepts defined in the prompt but
not used in the reasoning chains.
The Educator highlighted the LLM’s ability to pinpoint specific items the
human may have missed. In one instance, the Educator was shown an LLM-
generatedsegmentsummarythatheinitiallybelievedtobephysics-and-comput-
ing-synergistic, but he later agreed with the LLM that the segment was best
categorized as physics-and-computing-separate, as the students were merely dis-
cussing the domains sequentially and not interleaving and forming connections
betweenthecross-domainconcepts.TheEducatoralsovaluedtheLLM’sability
to highlight when students may be in need of teacher assistance and provided
several ideas for enhancing the human-AI partnership by using the LLM’s sum-
maries to generate actionable insight to support students’ STEM+C learning
(e.g., using the LLM’s summaries to create a graphical timeline to capture stu-
dents’ conceptual understanding).
DespiteGPT-4-Turbo’scapabilities,therearenotableareasforimprovement.
The autoregressive nature of LLMs introduces challenges related to reliance on
keywordsandphrases.AstheLLMconsiderseverytokengeneratedpreviouslyin
subsequentiterations,anyhallucinations(ormisinterpretationsofthepromptor
its own generation) can propagate forward and compromise the overall integrity
ofitsresponse.AninstanceofthisoccurredwhentheLLMfixatedontwophysics
concepts during summarization when the segment was almost entirely focused
on the computing domain. GPT-4-Turbo’s initial focus on the physics concepts
causedittolabelthesegmentasphysics-and-computing-synergistic,eventhough
both Reviewers and GPT-4 all considered the segment to be computing-focused.
Additionally,theLLM’sabilitytointegrateenvironmentactionsinitssummaries
was limited, often addressing them superficially without connecting them to the
broaderdiscoursecontext.TheEducatoralsosuggestedtheLLMshouldconsider
thetemporalityofsegmentsbyincorporating“pause” identificationandduration
from prosodic audio via timestamps and suggested highlighting instances where
students expressed uncertainty by saying things like “Um...” or “I’m still a little
stumped”, as both may help teachers identify students’ difficulties.8 C. Cohn et al.
5 Conclusion, Limitations, and Future work
The primary limitation of this exploratory study is its small test sample size
(12 segments). Additionally, only one researcher ranked the three sets of test
setsummaries(onehuman-generatedandtwoLLM-generated).Theconstraints
we faced were the time-cost of manually labeling, summarizing, analyzing, and
evaluating the individual segments and summaries (up to two hours and ≈256
tokens per segment summary). While these results cannot be generalized, we
have demonstrated the Method’s potential for summarizing and characterizing
students’ synergistic discourse in a manner that can deepen educators’ insights
into students’ cross-domain conceptual understandings. In future work, we will
conduct an extensive evaluation to test the generalizability of our approach,
including evaluating our Method’s performance across various learning tasks.
Acknowledgments. ThisworkissupportedunderNationalScienceFoundationawar-
ds DRL-2112635 and IIS-2327708.
Disclosure of Interests. The authors have no competing interests to declare.
References
1. Basu, S., et al.: Identifying middle school students’ challenges in computational
thinking-based science learning. Research and practice in technology enhanced
learning 11(1), 13 (2016)
2. Brown, T.B., et al.: Language Models are Few-Shot Learners. arXiv e-prints
arXiv:2005.14165 (May 2020)
3. Cohn,C.,Hutchins,N.,Le,T.,Biswas,G.:Achain-of-thoughtpromptingapproach
with llms for evaluating students’ formative assessment responses in science. Pro-
ceedings of the AAAI Conference on Artificial Intelligence 38(21) (Mar 2024)
4. Hadwin, A., et al.: Self-regulated, co-regulated, and socially shared regulation of
learning.Handbookofself-regulationoflearningandperformance30,65–84(2011)
5. Hutchins, N., et al.: C2STEM: a system for synergistic learning of physics and
computational thinking. Journal of Science Education and Technology 29 (2020)
6. Meier, A., Spada, H., Rummel, N.: A rating scheme for assessing the quality of
computer-supported collaboration processes. IJCSCL 2, 63–86 (2007)
7. Roschelle,J.,Teasley,S.D.:Theconstructionofsharedknowledgeincollaborative
problem solving. In: Computer supported collaborative learning (1995)
8. Snyder,C.,etal.:Analyzingstudents’synergisticlearningprocessesinphysicsand
ct by collaborative discourse analysis. In: CSCL (2019)
9. Snyder, C., Hutchins, N.M., Cohn, C., Fonteles, J.H., Biswas, G.: Analyzing stu-
dents collaborative problem-solving behaviors in synergistic stem+c learning. In:
Proceedings of the 14th Learning Analytics and Knowledge Conference (2024)
10. Suraworachet, W., Seon, J., Cukurova, M.: Predicting challenge moments from
students’ discourse: A comparison of gpt-4 to two traditional natural language
processing approaches. arXiv preprint arXiv:2401.01692 (2024)
11. Wei, J., et al.: Chain-of-Thought Prompting Elicits Reasoning in Large Language
Models. arXiv e-prints arXiv:2201.11903 (2022)
12. Weinberger, A., Fischer, F.: A framework to analyze argumentative knowledge
construction in computer-supported collaborative learning. Computers & educa-
tion 46(1), 71–95 (2006)