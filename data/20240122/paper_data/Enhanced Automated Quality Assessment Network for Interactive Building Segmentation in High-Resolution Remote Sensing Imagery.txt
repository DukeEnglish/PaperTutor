ENHANCED AUTOMATED QUALITY ASSESSMENT NETWORK FOR INTERACTIVE
BUILDING SEGMENTATION IN HIGH-RESOLUTION REMOTE SENSING IMAGERY
Zhili Zhang1, Xiangyun Hu1,2 *, and Jiabo Xu1
1School of Remote Sensing and Information Engineering, Wuhan University, Wuhan 430079, P. R.
China.
2Hubei Luojia Laboratory, Wuhan University, Wuhan 430079, P. R. China.
ABSTRACT focusing primarily on two aspects: qualitative analysis of
segmentation outcomes and the impact of sample quality on
In this research, we introduce the enhanced automated deep learning-based model. Qualitative evaluations typically
quality assessment network (IBS-AQSNet), an innovative employ empirical and rule-based designs. For example,
solution for assessing the quality of interactive building methods like LabelMe [2] utilize annotation scoring
segmentation within high-resolution remote sensing imagery. functions based on control point counts, but these often
This is a new challenge in segmentation quality assessment, overlook image context, leading to a certain unreliability.
and our proposed IBS-AQSNet allievate this by identifying Vittayakorn and Hays [3] introduced more comprehensive
missed and mistaken segment areas. First of all, to acquire scoring functions, incorporating factors such as annotation
robust image features, our method combines a robust, pre- size and edge detection. The second aspect includes
trained backbone with a lightweight counterpart for selecting high-quality samples for model training to improve
comprehensive feature extraction from imagery and their segmentation performance such as diverse labeling
segmentation results. These features are then fused through a schemes [4], and neural network applications for map
simple combination of concatenation, convolution layers, refinement [5]. Despite these efforts, a significant gap
and residual connections. Additionally, ISR-AQSNet remains in the direct, quantitative evaluation of
incorporates a multi-scale differential quality assessment segmentation sample quality.
decoder, proficient in pinpointing areas where segmentation
result is either missed or mistaken. Experiments on a newly-
built EVLab-BGZ dataset, which includes over 39,198
buildings, demonstrate the superiority of the proposed
method in automating segmentation quality assessment,
thereby setting a new benchmark in the field.
Index Terms—Remote sensing imagery, interactive
building segmentation, pre-trained backbone, deep learning,
segmentation quality assessment.
1. INTRODUCTION
Annotation quality aSsessment (AQS) is a critical work (a) (b) (c)
in practical applications, focusing on identifying missed or Fig.1. Case studies of quality assessment for interactive
mistaken areas in target or category annotations [1]. In this
building segmentation: (a) images with point or bounding
research, we conduct a segmentation quality assessment
box prompts; (b) interactive segmentation results; (c) ground
(SQA) study on the results from an interactive segmentation
truth of SQA, with mistaken areas in red and missed areas in
model, as illustrated in Figure 1. This study evaluates the
Green.
interactive segmentation results of two types of prompts:
point-based and bounding box-based. The ground truth of
Our recent research focuses on assessing the quality of
SQA highlights missed areas in green and mistaken areas in
annotated samples for categories like buildings and water
red. Given the demand for real-time and precise
bodies in a single remote sensing image (RSI)[1]. This paper
segmentation during interactive tasks, exploring an efficient,
expands this scope to real-time SQA of interactive building
lightweight approach for SQA is both challenging and
segmentation, categorizing the results into missed and
essential.
mistaken types at the instance level. Given the variable
In the past decades, research on SQA has been limited,performance of interactive segmentation models such as complemented by a lightweight backbone for rapid feature
Interformer [6], SAM [7], and PGR-Net [8] across different extraction from images and segmentation results. These
data sources and tasks, SQA for building segmentation features are fused through techniques like convolutional
encounters several challenges: conducting instance-level layers, concatenation, sampling, and residual concatenation.
segmentation quality analysis, ensuring real-time predictive The SQA result is then predicted by using a multiscale
efficiency of the models, and maintaining robustness. To feature difference quality assessment module. The proposed
alleviate these, we employ the SAM model for building IBS-AQSNet is able to realize a fast and stable quality
interaction segmentation. Our proposed approach utilizes assessment of the interaction segmentation results.
SAM's backbone for robust feature extraction,
Fig.2. The flowchart of the proposed IBS-AQSNet.
2. METHODOLOGY based backbone is utilized for extracting rich semantic
In this section, the proposed IBS-AQSNet is described in features from images across four stages, without
detail. The flowchart of the proposed IBS-AQSNet is shown involvement in training. In contrast, the ResNet18-based
in Figure 2. Firstly, the segmentation results and their image backbone is adapted for rapid learning on interaction
features provided by the interactive segmentation model are segmentation results and images. This adaptation includes
used as inputs to the IBS-AQSNet. The proposed method expanding the input channels from 3 to 4 and removing the
first extracts the features of the segmentation result by taking classifier, thus retaining spatially detailed feature maps
the segmentation result and image as the input of a across the four stages. This modified backbone facilitates
lightweight backbone. Then image and segmentation easier training and ensures stable feature map integration
features are simply fused by convolution, sampling and with the pre-trained ViT-based backbone.
concatenation operations. Finally, a multiscale differential The process involves fusing feature maps from both
quality assessment decoder is utilized to obtain the quality backbones across their respective four stages. To align
assessment results. spatial resolutions, the first three stages of the ViT-based
backbone's feature maps are resized to correspond with
2.1. Robust feature extraction and feature fusion those from ResNet18. Meanwhile, the fourth-stage feature
Figure 2 illustrates our method employs dual backbones maps of ResNet18 are modified to align with the fourth
for robust image feature extraction: a Vision Transformer stage of the ViT-based backbone. To keep pre-trained
(ViT)-based backbone from SAM-b, trained on extensive features from the ViT-basd backbone, a 1×1 convolution
datasets for robust feature extraction [7], and a modified, kernel is employed to standardize the channel dimensions to
lightweight backbone based on ResNet18 [9]. The ViT- those of ResNet18 across all four stages. Following this, thefeature maps from each stage are concatenated in a which is adept at pinpointing both missed and incorrect
sequential manner. Specifically, the fourth-stage fused areas within the segmentation results.
feature maps are processed using ASPP [10], then subjected 2.3. Loss fuction and accuracy assessment
to stage-by-stage up-sampling and concatenation with the During the training phase, the supervision of the
preceding stage's feature maps, along with processing via auxiliary output and SAQ results is accomplished using two
convolutional layers incorporating residual concatenation. loss functions: the cross entropy loss function (CE) and the
This simple strategy results in the generation of fused feature dice loss function [11]. The loss computation formula for the
maps at three distinct scales, as depicted in the 'Neck' of segmentation result (Ouput) is as follows:
Figure 2. Aligning with our recent work [1], the last layer of   (1)
the fused features is directed to the classifier as an auxiliary
Here, the parameters γ and γ are set to 0.5, the GT means
1 2
output. The final fused feature maps maintain sizes of 1/4,
the corresponding ground truth, and loss means the dice
D
1/8, 1/16, and 1/16 relative to the original image, with
loss function.
respective channel counts of 64, 128, 256, and 512.
Furthermore, we employ four common metrics to
evaluate the accuracy of our proposed method: F1-score,
2.2. The AQS Decoder
recall, precision, and overall accuracy (OA). These metrics
Following our recent AQSNet [1], we continuously use
are determined by comparing the prediction map against its
this advanced multi-scale feature difference quality
ground truth (GT), and involve the calculation of true
assessment module, referred to as the AQS decoder, for
positives (TP), false positives (FP), true negatives (TN), and
assessing segmentation results. The process begins with
false negatives (FN). The corresponding equations for these
rapid feature learning using two convolutional layers, with a
metrics are presented in equations (2) - (5).
3×3 kernel and a stride of 2. The next step involves
(2)
subtracting the fusion features from the segmentation
features across three scales. At each stage, the CSAM (3)
module from AQSNet is utilized for an in-depth analysis of
these differentiated features, thereby enhancing the detection (4)
of missed and mistaken areas in the segmentation results.
Subsequently, these enhanced differential features are TPTN (5)
OA
uniformly resized to align with the scale of the largest TPFPTNFN
feature map. The culmination of this process involves
integrating these concatenated features into a classifier,
Table 1. Quantitative analysis of IBS-AQSNet under different experimental setups on the EVLab-BGZ. PIF (using pre-
trained image features from SAM-b’s backbone), AQSD (the AQS decoder), M (Million), G (Billion).
Params Flops Missed-areas Mistaken-areas
Methods OA
(M) (G) Precision Recall F1-score Precision Recall F1-score
Baseline 32.66 81.05 34.940 70.326 46.685 64.691 52.308 57.844 98.370
Baseline + PIF 43.12 99.06 51.196 62.748 56.386 55.497 60.698 57.981 98.934
Baseline + PIF + AQSD 41.62 95.70 51.376 63.734 56.892 59.383 61.132 60.245 98.951
(a) (b) (c) (d) (e) (f)
Fig.3. IBS-AQSNet performance visualization on EVLab-BGZ under different experimental setups. (a) images & prompts, (b)
interactive segmentation results, (c) SQA ground truth, (d) Baseline, (e) Baseline with PIF, (f) Baseline with PIF and
AQSD.Key: Red for mistaken areas, Green for missed areas.
lo s s 
1
 lo s s
C E
 O
F
u
P
1
p
r

u t ,G T 
e c is io n 
T
R e c a ll 
2  P r e c is
P r e c is io
 lo s s2
D
T P
P  F PT
P
T P  F Nio
n  R e c a ll
n  R e c a ll
O u p u t ,G T 3. EXPERIMENTAL RESULTS AND ANALYSIS is further enhanced by the addition of pre-training features.
In point prompt interaction segmentation, our approach
To evaluate the effectiveness of our proposed IBS-AQSNet, demonstrates similar enhancements. These findings
we create a novel instance-level building extraction dataset, underscore the effectiveness of our method in enabling
called EVLab-BGZ, primarily located in Guangzhou city. automatic quality assessment for building interaction
This dataset includes both aerial and satellite images with segmentation. We have integrated this approach into our
resolutions of 0.2 and 0.5, captured in 2019, each measuring interaction segmentation software, enhancing its capacity for
512 × 512 pixels (refer to Figure 4). It comprises 2,825 innovative building interactions and delivering superior
aerial and 1,006 satellite images, encompassing a total of segmentation outcomes. More visualization results are
39,198 building instances. In these buildings, each building available here1.
is labeled independently and multiple buildings are rarely
labeled together. The dataset is partitioned into 3,000 4. CONCLUSION
images for training and 831 for testing. Additionally, we
have made this dataset publicly available. The building This paper introduces the IBS-AQSNet, a noval network
density within the dataset varies significantly, ranging from a designed for segmentation quality assessment (SQA) of
single building to as many as 51 buildings per image, with interactive building segmentation in high-resolution remote
each building occupying over 2,500 pixels. Given these sensing imagery. The proposed IBS-AQSNet integrates a
attributes, the dataset presents a considerable challenge for robust, pre-trained backbone with a lightweight counterpart,
building instance segmentation tasks. enabling efficient and comprehensive feature extraction. The
network employs a straightforward fusion of these features
and integrates a multi-scale differential quality assessment
decoder, adept at identifying errors, including missed and
mistaken areas, in SQA results. The proposed method
effectively addresses the challenge of building segmentation
quality assessment. Experimental validation on the newly
(a) (b) (c) built EVLab-BGZ dataset confirms that the IBS-AQSNet
Fig.4. Some building samples from EVLab-BGZ: (a-b) excels in the building SQA task, representing a progression
Aearial imagery; (b) Satelite imagery. in remote sensing image analysis.
The designed methods are validated on the EVLab-BGZ. 5. ACKNOWLEDGMENT
Both quantitative and qualitative analyses are conducted to
evaluate their performance. As shown in Table 1, the This work was supported by the Special Fund of Hubei
baseline, which incorporates a ResNet18 backbone and an Luojia Laboratory under grant 220100028 and 230700006,
UperNet decoder [12], achieves an F1 score of 46.685% for and the Fundamental Research Funds for the Central
missed areas and 57.844% for mistaken areas, with an OA of Universities, China under grant 2042022dx0001.
98.370%. These findings highlight the relative simplicity in
recognizing background areas in SQA, in contrast to the 6. REFERENCES
more complex task of identifying missed and mistaken areas.
The use of pre-trained image features leads to a significant [1] Z. Zhang, Q. Zhang, X. Hu, M. Zhang, and D. Zhu,
9.71% increase in the F1 score for missed areas, while only "On the automatic quality assessment of annotated
marginally improving the detection performance for sample data for object extraction from remote
mistaken areas. The precision for mistaken areas declines, sensing imagery," ISPRS Journal of
and although recall increases, the F1 score remains Photogrammetry and Remote Sensing, vol. 201, pp.
unchanged, suggesting possible underutilization of the used 153-173, 2023.
pre-training features. The introduction of AQS decoder [2] B. C. Russell, A. Torralba, K. P. Murphy, and W. T.
gives an improvement in detecting both mistaken and missed Freeman, "LabelMe: a database and web-based tool
areas, indicating the necessity for specially designed for image annotation," International journal of
modules in tasks such as building SQA. computer vision, vol. 77, pp. 157-173, 2008.
Figure 3 presents the visualization of our proposed [3] S. Vittayakorn and J. Hays, "Quality Assessment
methods through two prompts for building interactive for Crowdsourced Object Annotations," in BMVC,
segmentation, illustrating that our method closely aligns with 2011, pp. 1-11.
the SQA ground truth. Columns (d) - (f) of Figure 3 depict [4] A. Pilch and H. Maciejewski, "Labeling Quality
that our method, utilizing the AQS decoder, more accurately Problem for Large-Scale Image Recognition," in
identifies missed regions than the baseline model, which
often misidentifies such areas. The precision of the baseline 1 https://github.com/zhilyzhang/IBS-AQSNetInternational Conference on Dependability and
Complex Systems, 2022, pp. 206-216: Springer.
[5] S. Zorzi, K. Bittner, and F. Fraundorfer, "Map-
repair: Deep cadastre maps alignment and temporal
inconsistencies fix in satellite images," in IGARSS
2020-2020 IEEE International Geoscience and
Remote Sensing Symposium, 2020, pp. 1829-1832:
IEEE.
[6] Y. Huang et al., "InterFormer: Real-time interactive
image segmentation," in Proceedings of the
IEEE/CVF International Conference on Computer
Vision, 2023, pp. 22301-22311.
[7] A. Kirillov et al., "Segment anything," arXiv
preprint arXiv:.02643, 2023.
[8] Z. Shu, X. Hu, and H. Dai, "Progress Guidance
Representation for Robust Interactive Extraction of
Buildings from Remotely Sensed Images," Remote
Sensing, vol. 13, no. 24, p. 5111, 2021.
[9] K. He, X. Zhang, S. Ren, and J. Sun, "Deep
residual learning for image recognition," in
Proceedings of the IEEE conference on computer
vision and pattern recognition, 2016, pp. 770-778.
[10] L.-C. Chen, G. Papandreou, F. Schroff, and H.
Adam, "Rethinking atrous convolution for semantic
image segmentation," arXiv preprint arXiv:.05587,
2017.
[11] F. Milletari, N. Navab, and S.-A. Ahmadi, "V-net:
Fully convolutional neural networks for volumetric
medical image segmentation," in 2016 fourth
international conference on 3D vision (3DV), 2016,
pp. 565-571: Ieee.
[12] T. Xiao, Y. Liu, B. Zhou, Y. Jiang, and J. Sun,
"Unified perceptual parsing for scene
understanding," in Proceedings of the European
conference on computer vision (ECCV), 2018, pp.
418-434.