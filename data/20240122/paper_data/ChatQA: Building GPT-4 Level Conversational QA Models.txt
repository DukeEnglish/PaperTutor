ChatQA: Building GPT-4 Level Conversational QA Models
ZihanLiu1 WeiPing1 RajarshiRoy1 PengXu1 MohammadShoeybi1 BryanCatanzaro1
Abstract level accuracy, through a proposed two-stage instruction
tuningrecipe,anenhancedretrieverforretrieval-augmented
Inthiswork,weintroduceChatQA,afamilyof
generation(RAG)inconversationalQA,andcarefuldata
conversationalquestionanswering(QA)models,
curationprocess.
thatobtainGPT-4levelaccuracies. Specifically,
weproposeatwo-stageinstructiontuningmethod Specifically,wemakethefollowingcontributions:
thatcansignificantlyimprovethezero-shotcon-
versationalQAresultsfromlargelanguagemod- 1. Weproposeatwo-stageinstructiontuningmethod
els(LLMs). Tohandleretrievalinconversational and a dataset curation recipe that can largely en-
QA, we fine-tune a dense retriever on a multi- hanceLLM’scapabilityofintegratinguserprovided
turnQAdataset,whichprovidescomparablere- orretrievedcontextforzero-shotconversationalQA
sultstousingthestate-of-the-artqueryrewriting tasks. Wedemonstratethatourmethodsignificantly
model while largely reducing deployment cost. outperforms regular instruction tuning or RLHF-
Notably,ourChatQA-70BcanoutperformGPT-4 basedrecipes(e.g.,Llama-2-Chat).
in terms of average score on 10 conversational
2. ForRAGinconversationalQA,weshowthatfine-
QAdatasets(54.14vs. 53.90),withoutrelyingon
tuningstate-of-the-artsingle-turnqueryretrievers
anysyntheticdatafromOpenAIGPTmodels.
onhuman-annotatedmulti-turnQAdatasetworks
aswellasutilizingthestate-of-the-artLLM-based
1.Introduction queryrewritingmodel,i.e.,GPT-3.5-turbo(OpenAI,
2022).
Most recently, ChatGPT (OpenAI, 2022) and its follow
ups(OpenAI,2023;Anthropic,2023b;Google,2023)have 3. We build a family of ChatQA models based on
led to the paradigm shift of building question answer- Llama2-7B, Llama2-13B, Llama2-70B (Touvron
ing (QA) models in production and research community. et al., 2023) and a in-house 8B pretrained GPT.
Inparticular,thefollowingaspectsoftheQAmodelsare We conduct comprehensive study on 10 conver-
preferred in real-world applications: i) The users can in- sational QA datasets, including 5 datasets with
teract with the QA models in a conversational way, thus long documents that need retrieval and 3 datasets
onecaneasilyraisefollow-upquestions. ii)Thegeneralist with tables. In terms of average score, our
modelscangenerateanswersinzero-shotmannerwithout ChatQA-70Bmodel(54.14)canoutperformGPT-
dataset-specificfine-tuning,whilematchingtheaccuracies 3.5-turbo(50.37)andGPT-4(53.90)withoututiliz-
offine-tunedexpertmodels.iii)TheQAmodelsarecapable inganysyntheticdatafromChatGPTmodels.
of integrating retrieved chunks of evidence in both open-
4. Westudythe“unanswerable”scenario,wherethe
domainorlongdocumentsettings,wheretheprovidedcon-
desired answer is not included in the provided or
textismuchlongerthanthecontextwindowofLLM(e.g.,
retrievedcontext,thustheLLMcaneasilyhalluci-
Anthropic,2023a;Xuetal.,2023b). Tothisend,wefocus
nate. Wedemonstratethataddingasmallamount
ontheconversationalQAcoveringthesethreeaspects.
of“unanswerable”samplesininstructiontuningcan
However, building a conversational QA model, that can steerthemodeltogenerate“cannotanswer”output
matchtheaccuracyofthestate-of-the-artblack-boxmodel, whenitisnecessary,thuslargelyreducehallucina-
i.e.,GPT-4(OpenAI,2023),isstillagrandchallengeforthe tion. OurChatQA-70BoutperformsGPT-3.5-turbo
researchcommunity. Inthiswork,weintroduceChatQA- inthisregard,whilestillhasaslightgapcompared
70B, a white-box conversational QA model with GPT-4 toGPT-4(around3.5%).
1NVIDIA. Correspondence to: Zihan Liu <zihanl@
We organize the rest of the paper as follows. We discuss
nvidia.com>, WeiPing<wping@nvidia.com>.
relatedworkin§2. Weintroducethetwo-stageinstruction
Copyright2024bytheauthor(s). tuning method for ChatQA in § 3, and study retrieval in
1
4202
naJ
81
]LC.sc[
1v52201.1042:viXraChatQA:BuildingGPT-4LevelConversationalQAModels
conversational QA in § 4. We present the experimental haryetal.,2019;Chuetal.,2020;Quetal.,2020;Anantha
setupin§5resultsin§6andconcludethepaperin§7. etal.,2021;Brabantetal.,2022),alongsidemultiplepro-
posedqueryrewritingmethods(Ishiietal.,2022;Yuetal.,
2020;Wuetal.,2022;DelTredicietal.,2021;Chenetal.,
2.RelatedWork
2022b;Galimzhanovaetal.,2023). Forexample,Wuetal.
2.1.ConversationalQA (2022)andChenetal.(2022b)proposedtousereinforce-
ment learning methods for the query rewriting. Yu et al.
Question answering in conversational way naturally im-
(2020)investigatedfew-shotgenerativemodelslikeGPT-2
provesuserexperiencesbyaddressingfollowupquestions.
for query rewriting. Galimzhanova et al. (2023) studied
Themodelcanalsoraiseclarificationquestionsforusers
instructiontunedGPT-3.5-turboandshowedthatitachieved
ifnecessary, whichcanreducehallucination. Thus, itbe-
state-of-the-artresultsforconversationalqueryrewriting.
comesthedefaultformatofdeployingQAmodelsinproduc-
tion(e.g.OpenAI,2022;Google,2023;Anthropic,2023b).
2.2.2.FINE-TUNINGRETRIEVERFORMULTI-TURNQA
Inrecentyears,manyconversationalQAdatasetshavebeen
Somepreviousworkfine-tuneasingle-turnqueryretriever
introduced,wherethemodelsareaskedtoanswerquestions
onin-domainconversationalqueryandcontextpairs(Feng
based on provided context or documents. The provided
et al., 2020; Gao et al., 2022; Adlakha et al., 2022; Wu
context or documents can be: i) text-only from various
etal.,2023),soitcandirectlytakeaconcatenationofdialog
domains (Feng et al., 2020; Anantha et al., 2021; Saeidi
historyandcurrentqueryasinput.Inthiswork,wefocuson
etal.,2018;Adlakhaetal.,2022;Aliannejadietal.,2021;
thezero-shotevaluation. Wefine-tuningasingle-turnquery
Reddyetal.,2019;Quetal.,2020;Wuetal.,2023;Deng
retriever on a high-quality multi-turn dataset. Then, we
et al., 2022; Guo et al., 2021; Choi et al., 2018; Campos
evaluatezero-shotcapabilityofthefine-tunedretrieveron
etal.,2020),orii)plaintextalongwithtables(Pasupat&
fivebenchmarkdatasets. Surprisingly,wefindthissimple
Liang,2015;Nakamuraetal.,2022;Chenetal.,2022a).
approach can obtain comparable zero-shot results as the
IncontrasttothelatestLLM-basedgeneralistsolution(e.g., state-of-the-artqueryrewritingmodel,i.e.,GPT-3.5-turbo.
OpenAI,2022),mostofpreviousstudiesfocusonfine-tuned
expertmodelsonspecificdomainsordatasets(Fengetal., 2.3.InstructionTuning
2020;Izacard&Grave,2021;Chenetal.,2022a;Gaoetal.,
ThegoalofinstructiontuningistoequipLLMswiththe
2022;Nakamuraetal.,2022;Adlakhaetal.,2022;Wuetal.,
capabilitytofollownaturallanguageinstructions(Weietal.,
2023).
2022a; Sanh et al., 2022; Mishra et al., 2022; Iyer et al.,
2022; Du et al., 2022; Ouyang et al., 2022; Wang et al.,
2.2.RetrievalforMulti-TurnQA
2023b;Zhangetal.,2023;Gaoetal.,2023;Chungetal.,
Conversational QA involves retrieval-augmented genera- 2022; Muennighoff et al., 2022; Xu et al., 2023a; Wang
tion(RAG)inopen-domainsetting,orwhentheprovided etal.,2022c;Zhouetal.,2023). Therehasbeenasurgein
documentsarelongerthanthecontextwindowofLLM.The thedevelopmentofhigh-qualityinstructiontuningdatasets,
denseretrieversareusuallytrainedtoretrievethetop-krele- includingFLAN(Chungetal.,2022),Self-Instruct(Wang
vantchunksgivenasinglequestion(e.g.,Linetal.,2023a; etal.,2022b),unnaturalInstructions(Honovichetal.,2022),
Wangetal.,2022a;Izacardetal.,2022). Inconversational Dolly (Conover et al., 2023b), and OpenAssistant (Ko¨pf
QA,thefollow-upquestions(e.g.,withpronounsreferring etal.,2023).
toentitiesmentionedinthepreviousconversation)mayhave
Althoughnumerousresearchaboutinstructiontuninghas
insufficient information for retrieval, while feeding them
been conducted, a few work focused on improving RAG
alongwiththedialoguehistorycanberedundantandlead
orcontextawarenessgenerationforQA.Linetal.(2023b)
tosub-optimalresults.
introducedaretrieval-augmentedinstructiontuningmethod,
whichappendstop-kretrievedchunksforLLMfine-tuning.
2.2.1.CONVERSATIONALQUERYREWRITING
Wangetal.(2023a)appliedinstructiontuningafterretrieval-
Mostofprevioussolutionsarequeryrewritingmethods.The augmentedpretraining. Incontrast,weproposeatwo-stage
latestturnofquestionisrewrittentobeastandalonequery instruction tuning method to improve generation with re-
withoutadditionalinformationfrompreviousdialoguehis- trival or provided context. We find that appending top-k
tory (Vakulenko et al., 2021a; Ye et al., 2023; Mo et al., retrievedchunksforLLMfine-tuningdoesnothelpfora
2023), so it can be directly used by retrieval model to re- widerangeofconversationQAtasks(see§6.3fordetails).
trieverelevantcontext(Vakulenkoetal.,2021b;Meleetal.,
AfterthereleaseofChatGPT(OpenAI,2022),instruction
2021;Raposoetal.,2022;Moetal.,2023). Manydatasets
tuning becomes an indispensable ingredient to build the
havebeencollectedtofacilitatethislineofresearch(Elgo-
state-of-the-artdialogueagentwhichhasastonishingzero-
2model 1/4/24, 8:10 PM
ChatQA:BuildingGPT-4LevelConversationalQAModels
Stage-1: Supervised Fine-tuning Stage-2: Context-Enhanced Instruction Tuning
NarrativeQA
Foundation Soda, ELI5 SFT DROP, Quoref
FLAN, Dolly ROPES, SQuAD ChatQA
LLM OpenAssistant Model NewsQA, TAT-QA
ConversationalQA
Figure1. Two-stageinstructiontuningframeworkforChatQA.
shotcapabilitytoawiderangeoftasks. Theconversational 3.2.Stage-2: Context-EnhancedInstructionTuning
QAcapabilityplaysacrucialroleinadialogueagent,yet
Tofurtherenhancethemodel’sconversationalQAcapability
therehasbeenlimitedresearchdedicatedtothisvitalaspect.
overagivencontext,weconductasecondstageinstruction
tuning,whichintegratecontextualizedQAdatasetsintothe
3.ChatQA
instructiontuningblend.Specifically,thestage-2instruction
tuningdatasetsconsistofablendofcontextualizedsingle-
Inthissection,weproposeatwo-stageinstructiontuning
turnQAandconversationalQAdatasets. Wepresentfurther
methodforChatQA.SeeFigure1foranillustration. Our
detailsforthestage-2instructiontuningdatasetsbelow.
methodstartswithanpretrainedLLMfoundationmodel.At
stage-1,w eapplysupervisedfine-tuning(SFT)asinOuyang
3.2.1.HUMANANNOTATEDDATA
etal.(2022)onablendofinstruction-followinganddialog
datasets. Afterthat,ourmodelexhibitsgoodcapabilityto Inadditiontothepublicavailabledatasets,oneofthekey
followinstructionsasaconversationalagent. However,its elementsforstage-2istoobtainahigh-qualitydocument-
capabilityforcontextualizedorRAG-basedQAremainslim- groundedconversationalQAdataset. Wecreateahuman-
ited.Hence,weintroduceasubsequentstage,calledcontext- annotatedconversationalQA(calledHumanAnnotatedCon-
enhancedinstructiontuning,whichisdesignedspecifically vQA)datasetonlyconsistingof7kdialogues. Tobuildthis
forenhancingourmodel’scapabilityforcontext-awareor dataset, wefirstcollected7kdocumentscoveringdiverse
retrieval-augmentedgenerationinconversationalQA. topics from Internet. Then, we instruct the annotator to
actbothasacurioususeraskingquestions(andfollow-up
3.1.Stage-1: SupervisedFine-tuning questions) about the documentand as a agent to give the
answers. Wecreateamulti-turnconversationforeachdocu-
To construct a large and comprehensive supervised fine-
ment,resultinginatotalof7kconversationalQAdialogues
tuning(SFT)dataset, wefollowXuetal.(2023b);Wang
withanaverageof5user-agentturnsperdialogue. Details
etal.(2023a)andgatheracombinedsetof128KSFTsam-
ofdatacollectionguidelinecanbefoundinAppendixG.
plesfromhigh-qualityinstructiontuningdatasets. Itcon-
sistsof1)asocialdialoguedatasetSoda(Kimetal.,2022), Inaddition,toreducehallucinatedanswersinunanswerable
2) a long-form QA dataset ELI5 containing elaborate an- cases, we aim to empower our model to explicitly indi-
swers (Fan et al., 2019), 3) FLAN and chain-of-thought cateitwhentheanswercannotbefoundwithinthegiven
datasets (Wei et al., 2022b; Chung et al., 2022; Longpre context. To obtain these unanswerable data samples, we
etal.,2023),4)LLMsyntheticinstructionstuningdatasets: requestedannotatorstoprovideallcontextlocationstothe
Self-Instruct (Wang et al., 2022b) and Unnatural Instruc- userquestion. Hence,itenabledustoconstructunanswer-
tions(Honovichetal.,2022),and5)aprivatecrowd-sourced ablescenariosbydeletingthetextfromthecorresponding
conversationaldataset,aswellastwopublichuman-written locations in the context. After deleting the relevant text
conversation datasets: OpenAssistant (Ko¨pf et al., 2023), tothequestion,weuseasentence,“Sorry. I cannot
https://www.mathcha.io/editor# Page 1 of 2
andDolly(Conoveretal.,2023a). find the answer based on the context”, as
theresponsefortheunanswerablequestions. Finally, we
WeunifythestructureofalltheSFTdatainaconversational
constructanother1.5kuser-agentturnswithunanswerable
format. We first add a “System” role at the beginning to
annotations,whichprovidesagoodtrade-offofanswerable
setupageneralinstructionguidingLLMtoprovidepolite
andunanswerablecases(see§6.5fordetails).
andhelpfulanswers. Wealsoadd“User”and“Assistant”
rolestoincorporateinstructionandresponsepairsfromthe
3.2.2.SYNTHETICDATAGENERATION
instructiontuningdatasets. Weapplyfine-tuningusingthis
unifiedformatonanLLMfoundationmodel. TovalidatethequalityoftheHumanAnnotatedConvQA,we
leverageGPT-3.5-turbotogeneratesyntheticconversational
3retrieval 12/20/23, 3:49 PM
ChatQA:BuildingGPT-4LevelConversationalQAModels
QA dataset given its powerful instruction-following and Multi-Turn Queries
Contrastive
textgenerationcapability. Notethat,large-scalesynthetic Finetuning
User: is brandy melville a person? Query
dataforconversationalQAhasalsobeenexploredinDai Assistant: No. It is an Italian brand Encoder
User: I haven't heard of it. How do
etal.(2022). Inthiswork,wefocusonmidsizehigh-quality they advertise it?
q3
syntheticdataforLLMfine-tuning.
Corresponding
Contextqs1q2
Eq1 Eq2 Eq3
The instruction for GPT-3.5-turbo comprises three parts:
... ... It does not employ traditional Context Ec1 pos neg neg
1)systemroleguidingthemodeltoprovidehelpfulanswer, advertising techniques. However, they Encoder
depend heavily on social media for Ec2 neg pos neg
2)examplesofconversationalQAindicatingtherequired their advertising ... ...
datatypes,and3)adocumentthatdirectsthemodeltogen- c2c3 Ec3 neg neg pos
c1
erateconversationalQAbasedonitscontent. Wecollect7k
Figure2.Illustrationoffine-tuningretrieverformulti-turnQA.
documents(average∼1kwordsperdocument)fromcom-
mon crawl, which cover a wide range of domains. Each
documentisusedforgenerationofasingleconversational
QAsample,whichleadstoatotalof7kmulti-turnQAdia-
QA (Zhu et al., 2021), and 3) All of SFT datasets from
logueswithanaverageof4.4user-agentturnsperdialogue
stage-1.
(calledSyntheticConvQA).
We follow the similar template as in stage-1 to unify all
SimilartotheHumanAnnotatedConvQA,weconstructan-
single-turn QA and conversational QA datasets. The dif-
other1.5kuser-agentturnswithunanswerableannotations
ferencesareintwoparts: 1)Followingthesystemrole,we
in this synthetic dataset. Since there is no annotations of
appendrelevantcontextforthesingle-turnquestionorthe
the context location for the agent’s answer, we construct
multi-turnconversation;and2)Weintegratefurtherinstruc-
synthetic unanswerable samples from SyntheticConvQA.
tionjustbeforethesingle-turnquestionormulti-turncon-
Specifically,wefirstcutthedocument(foreachdialogue)
versationbasedontheanswertypesofdifferentQAdatasets
intodifferentchunks. Then,weconsideritasavalidunan-
(e.g.,shortanswer,longanswer,arithmeticcalculation). We
swerablesampleonlywhentherearechunksthathave“high
usetheformatforSFTdatasetfromstage-1.2
overlaps” with the agent’s answer to be removed and the
rest of the chunks shows “low overlaps” with the agent’s
answer. Weusethe4-gramrecallscorebetweeneachchunk 4.RetrievalforMulti-TurnQA
andagent’sanswer(itmeasurestheratiosoftheanswer’s
In conversational QA tasks, when a document becomes
4-gramphrasesarewithineachchunk)asthemetrictomea-
suretheiroverlaps,andconsiderithigherthan0.5ash“ttphs:i//gwwhw.mathchtao.ioo/ediltoerngthytofeeddirectlyintoLLMs,aretrieverthatcan Page 1 of 2
handleconversationalquerybecomesessential. Thiscon-
overlaps”andlowerthan0.1as“lowoverlaps”.
versationalretrieverencodestheconcatenationofdialogue
historyandcurrentquery,andthenretrieverelevantcontext
3.2.3.TRAININGBLENDS
fromdocuments. Afterthat,onlyrelevantcontextwillbe
Inthispart,weintroducethedetailsoftrainingblendsfor usedasinputsforLLMs.Thestate-of-the-artretrievers,e.g.,
stage-2instructiontuning. ToboosttheQAcapabilityin Dragon (Lin et al., 2023a), are optimized for single-turn
handlingtabulardocumentsandarithmeticcalculation,we queries,resultinginalimitedgeneralizationcapabilityfor
addtheTAT-QAdataset(Zhuetal.,2021)whichcontains multi-turn conversational queries. In Figure 2, we depict
both elements. In addition, we integrate contextualized ourretrieverfine-tuningmethodtoalleviatethisissue. We
single-turnQAdatasetstofurtherstrengthentheQAcapa- proposetouseconversationalqueryandcontextpairsfor
bilityofourmodel. Furthermore,westillkeepthestage-1 furtherfine-tuningasingle-turnretrievertobettercopewith
SFTdatasetinthetrainingblendtomaintainthemodel’s conversationalinputs.
instruction-followingcapability.
An alternative solution is conversational query rewriting
Finally, the training blend for stage-2 consists of: 1) A methodwhichusesaqueryrewritertorewritethecurrent
conversational QA dataset: HumanAnnotatedConvQA questionbasedontheconversationalhistory. Therewritten
or SyntheticConvQA, 1 2) single-turn QA datasets: query is then directly used as the input to a single-turn
DROP (Dua et al., 2019), NarrativeQA (Kocˇisky` et al., queryretrieverforretrievingrelevantcontext. Inadditionto
2018), Quoref (Dasigi et al., 2019), ROPES (Lin et al., theembeddingandsearchcost,thequeryrewritingmodel
2019),SQuAD1.1(Rajpurkaretal.,2016),SQuAD2.0(Ra- introducesalargeamountofextracomputationalexpense
jpurkaretal.,2018),NewsQA(Trischleretal.,2017),TAT- togeneratetherewrittenquery.
1Unlessspecifiedotherwise,theexperimentsuseHumanAnno- 2Detailsofthetemplatesforbothstage-1andstage-2instruc-
tatedConvQAasthedefaultsetting. tiontuningaswellasthesyntheticdatagenerationcanbefoundin
theAppendixA.
4ChatQA:BuildingGPT-4LevelConversationalQAModels
Average Doc2Dial QuAC QReCC TopiOCQA INSCIT
Models
top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5 top-5* top-20* top-5* top-20*
Adlakhaetal.(2022) - - - - - - - - - 70.40△ - -
Wuetal.(2023) - - - - - - - - - - - 71.10△
E5-unsupervised†(Wangetal.,2022a) 31.56 59.22 23.02 55.33 43.49 77.68 44.71 84.99 26.25 37.67 20.32 40.44
E5-unsupervised+Rewrite‡ 33.23 61.02 25.56 58.00 46.00 80.01 45.50 85.89 27.58 39.15 21.53 42.04
E5-unsupervised+Fine-tune† 47.79 75.00 45.28 80.96 46.52 80.74 53.37 89.91 41.01 51.07 52.79 72.31
Dragon†(Linetal.,2023a) 46.29 73.09 43.33 75.61 56.80 82.86 46.17 81.96 57.68 78.80 27.49 46.22
Dragon+Rewrite‡ 54.46 80.13 47.60 80.60 47.10 77.15 51.73 85.78 73.07 88.19 52.79 68.92
Dragon+Fine-tune† 52.72 80.67 48.94 83.01 52.64 81.95 50.73 87.17 67.86 86.28 43.43 64.94
-SyntheticConvQA♢ 52.98 81.15 48.64 83.47 54.75 83.23 49.63 86.70 64.48 85.24 47.41 67.13
Table1.Retrievalresultsacrossfivemulti-turnQAdatasetswiththeaveragetop-1andtop-5recallscores. Comparedtorewriting,
fine-tuningperformsmuchbetteronE5-unsupervisedandcomparableonDragon.*SincetheaveragecontextlengthinTopiOCQAand
INSCITaresmallerthanotherdatasets,wereporttop-5andtop-20toroughlymatchthecontextlengthsoftop-1andtop-5,respectively,
inotherdatasets.†Theinputsofthesetwomodelsareconcatenationofthedialoguehistoryandcurrentquery.‡Theinputsofthismodel
istherewrittenquery.♢denotesthattheHumanAnnotatedConvQAdatasetisreplacedwiththeSyntheticConvQAforfine-tuning.△The
numbersarenotapple-to-applecomparison(e.g.,theyusetrainingsetforfine-tuning).
4.1.Fine-tuningRetrieverforMulti-turnQA unsupervised(Wangetal.,2022a),whichisnotfinetuned
on MS MACRO (Nguyen et al., 2016). In terms of the
Tobuildahigh-qualityfine-tuningdataset,weleveragethe
experimentsonDragon,wefindthatfine-tuningperforms
conversationalQAdatasetfromeithertheHumanAnnotat-
marginallyworsethanqueryrewritinginaveragetop-1re-
edConvQAortheSyntheticConvQAtoconstructconversa-
callby1.74%, whileitachievesbetterresultsonaverage
tionalqueryandcontextpairs.
top-5recallby0.54%. Itdemonstratestheeffectivenessof
FortheHumanAnnotatedConvQA,wedirectlytakethean- the fine-tuning approach for the conversational retrieval.
notationsoftheconversationalqueryandcontextpairs,and In addition, we observe that the results are comparable
usethemtofurtherfine-tuneasingle-turnqueryretriever. between using HumanAnnotatedConvQA and Synthetic-
FortheSyntheticConvQA,wefirstcuteachdocumentin ConvQAforfine-tuning. Thishighlightsthatourhuman-
theconversationalQAdatasetintodifferentchunks. Then, annotateddatasetisinhigh-quality,andwedonotrelyon
wecalculatethe4-gramrecallscorebetweenagent’sanswer ChatGPTmodelsforbuildingthestate-of-the-artmulti-turn
and each chunk. After that, we consider the chunk that queryretriever.
hasthehighestrecallscoreasthegoldchunkforthecur-
Surprisingly, fine-tuning performs significant better than
rentuser’squestion. Finally,theconstructedconversational
rewritingonE5-unsupervised. WeconjecturethatsinceE5-
queryandcontextpairsareusedtofine-tuneasingle-turn
unsuperviseddoesnotusehuman-annotatedqueryandcon-
queryretriever.
textpairsinthepre-trainingstage,leadingtoaweakgeneral-
izationtothehigh-qualityrewrittenquery.Incontrast,using
4.2.ConversationalQueryRewriting
high-quality dataset to fine-tune E5-unsupervised brings
Tobuildpowerfulconversationalqueryrewritingmodel,we agiantboostwithmorethan15%improvementsonboth
takeGPT-3.5-turboastherewritergiventhatGalimzhanova averagetop-1andtop-5recallscores.
etal.(2023)demonstratedthestate-of-the-artqueryrewrit-
Therefore,fine-tuningagoodsingle-turnretrieveronhigh-
ingresultsusingGPT-3.5-turbo. SimilartoGalimzhanova
quality conversational query context pairs performs on
etal.(2023),wenotonlyprovideGPT-3.5-turbowiththe
parwithleveragingthestate-of-the-artrewriter. However,
rewritingtaskinstruction,butalsogiveitfew-shotrewriting
rewritingmethodrequiresextracomputationaltimeforau-
examplestoenhancethequalityofrewritingresults. More
toregressivegenerationprocessandprobablyalsoAPIcost
detailscanbefoundinAppendixB.1.
forusingpowerfulmodelslikeGPT-3.5-turbo. Incontrast,
ourproposedmulti-turnfine-tuningbypassestheseissues.
4.3.Comparisons FortheQAevaluationsacrossthesefivedatasets,wecon-
sistentlyusetheretrievedtop-5resultsfromthefine-tuning
InTable1,wecomparethequerrewritingandfine-tuning
approach for all the QA models. We put more results on
methodsacrossfivedatasetsinthezero-shotsetting. More
comparisonsbetweenrewritingandfine-tuningmethodsin
detailsaboutthesedatasetscanbefoundin§5.2.1. Wecon-
theAppendixB.2.
ductexperimentsonastate-of-the-artretriever,Dragon(Lin
et al., 2023a), and a strong unsupervised retriever, E5-
5ChatQA:BuildingGPT-4LevelConversationalQAModels
5.ExperimentalSetup 5K words, with a maximum document size of 20K
words.
Inthissection,wepresentthedetailsofourexperimental
setupfortheconversationalquestionansweringtask. • TopiOCQA(Adlakhaetal.,2022)isgroundedonthe
wholeWikipedia. Itincorporatestopicswitchingand
5.1.Baselines requirestheagenttosearchtheentireWikipediafor
answerstouserquestions.
We conduct experiments on different model sizes. First,
to show the effectiveness of stage-2 context-enhanced in- • INSCIT(Wuetal.,2023)isalsogroundedonthewhole
struction tuning, we compare against the Llama2-SFT- Wikipedia. Itstudiesthecasewhereuserquestionsare
7B/13B/70B,whichistheLlama2-7B/13B/70Bfoundation under-specifiedandrequireclarification.
modelafterthestage-1supervisedfine-tuning(SFT).Sec-
ond,wecompareagainstLlama2-Chat-7B/13B/70Bsince ForDoc2Dial,QuAC,andQReCC,wesegmentdocuments
Llama2-Chatmodelsareshowntopossessstronginstruc- intoaround300-wordchunks,andweretrievetop-5relevant
tionfollowingandconversationalQAcapabilities(Touvron chunksascontextforeachuserquestion. ForTopioCQA
etal.,2023). AsidefromLlama2models,wealsoconduct andINSCIT,wefollowtheiroriginalsegmentation,result-
experiments on our in-house GPT-8B foundation model, inginsmallerchunks. Hence,weretrievedtop-20chunks
andwecompareagainstitsstage-1SFTbaseline(GPT-8B- toobtainsimilarcontextlengthtothefirstthreedatasets.
SFT).Finally,wecompareagainsttwoverystrongOpenAI
models: GPT-3.5-turbo (4k) and GPT-4 (8k). For fair
5.2.2.SHORTDOCUMENTDATASETS
comparison,weusethesamecontextasinputsforbothour
Toincreasethediversityofdocumentlengths, wecollect
modelsandbaselines. Notethatwehavecarefullytunedthe
fiveconversationalQAdatasetswithshortdocuments(less
instructionsforallthebaselinestoensuretheyachieveas
than 1.5K words). On average, 1 word will be tokenized
goodaspossibleresults.3
into 1.5tokens. Hence,thedocumentcanbedirectlyfitted
intoLLMswithasequencelengthof4Ktokens.
5.2.EvaluationBenchmarks
• CoQA (Reddy et al., 2019) is a conversational QA
5.2.1.LONGDOCUMENTDATASETS
dataset with each dialogue grounded on a short pas-
WecollectfiveconversationalQAdatasetswithlongdoc- sage.Theanswersaregenerallyshort,andthepassages
uments which cannot be directly fitted into LLMs with a coverawiderangeofdomainslikechildren’sstories,
sequencelengthof4Ktokens. Hence,werunamulti-turn literature,mid/highschoolexams,news,Wikipedia.
retrievertogettop-5relevantchunksastheinputs(experi-
• DoQA (Campos et al., 2020) covers three domains:
mentscanbefoundin§4.3).
cooking,travel,andmoviescollectedfromactiveStack
Exchange4forums. Thedatasetcontainsunanswerable
• Doc2Dial(Fengetal.,2020)isadocument-grounded
caseswhereanswerscannotbefoundwithinthegiven
conversational QA dataset covering four domains:
document.
DMV,SSA,VA,andStudentAid. Eachsamplecom-
prisesadialoguewhereauserposesqueriesregarding • ConvFinQA(Chenetal.,2022a)isbasedontheFinan-
thedocument,andanagentrespondsthosequestions. cialdomain.Eachdocumentcontainsasinglefinancial
Theaveragedocumentlengthisaround101Kwords. reporttablealongwithrelevanttextsurroundingthe
table. Thisdatasetinvolvesarithmeticcalculationand
• QuAC(Choietal.,2018)isbasedonWikipediadocu-
complexnumericalreasoning.
ments. Originally,thedocumentisshort. Sinceeach
dialogue is linked to multiple Wikipedia URLs, we • SQA (Pasupat & Liang, 2015) is grounded on docu-
extracttextfromtheselinkstoincreasethedocument mentswhichcontainonlyasingleTablewithoutany
size to approximately an average of 15K words. It surroundingtext. Thedocumentsarecollectedfrom
containsunanswerablecaseswhereanswerscannotbe Wikipedia,andthequestionsarehighlycompositional,
foundwithinthegivencontext. whichrequiresthemodelwithrobusttablecomprehen-
sionabilitytogivecorrectanswers.
• QReCC(Ananthaetal.,2021)isanopen-domaincon-
versationalQAdatasetacrossmultiplesources.Similar • HybridDial(Nakamuraetal.,2022)isaconversational
toQuAC,eachdialoguealsohascorrespondingURLs. QAdatasetgroundedondocumentscontainingboth
WeextracttextfromthoseURLstoconstructthedocu- Wikipediatabularandtextualdata. Thequestionsare
ments. Intheend,theaveragedocumentsizeisaround complexwhichrequiresreasoningoverthedocuments.
3ThepromptsforthesebaselinescanbefoundinAppendixC. 4https://stackexchange.com/
6ChatQA:BuildingGPT-4LevelConversationalQAModels
Models Average Doc2Dial QuAC QReCC CoQA DoQA ConvFinQA SQA TopiOCQA HybridDial INSCIT
GPT-8B-SFT 34.46 31.03 20.07 37.69 59.24 21.72 15.44 40.06 38.17 52.29 28.86
ChatQA-8B 49.36 36.76 33.95 45.54 77.90 44.65 61.68 60.74 47.03 53.81 31.50
Llama2-7B-SFT 34.81 30.26 19.21 37.55 62.75 21.76 34.43 32.18 32.88 48.96 28.16
Llama2-7B-Chat 38.86 33.27 25.83 46.02 72.28 33.15 36.58 26.14 36.68 47.02 31.67
ChatQA-7B 47.71 37.88 29.69 46.97 76.61 41.57 51.61 61.87 45.45 54.51 30.96
Llama2-13B-SFT 37.69 30.68 21.59 38.25 69.52 21.70 41.14 37.85 35.26 52.22 28.73
Llama2-13B-Chat 40.34 34.74 27.89 47.19 72.50 32.60 41.54 25.39 39.25 49.82 32.52
ChatQA-13B 50.86 38.05 34.28 48.06 77.23 43.31 65.44 66.41 48.88 56.19 30.79
Llama2-70B-SFT 43.22 34.42 25.65 41.88 73.04 28.21 46.64 58.90 37.20 55.52 30.71
Llama2-70B-Chat 45.21 36.87 32.47 49.40 80.41 38.97 46.85 37.62 44.31 50.35 34.88
ChatQA-70B 54.14 38.90 41.82 48.05 78.57 51.94 73.69 69.14 50.98 56.44 31.90
-SyntheticConvQA♢ 54.08 39.19 38.33 48.73 79.83 48.65 76.44 68.63 51.30 55.68 33.98
-w/ostage-1† 52.18 38.43 37.52 46.08 73.51 49.42 72.15 72.08 51.28 50.74 30.56
-w/osingle-turn* 52.25 38.30 37.89 47.08 76.74 46.43 72.42 67.41 49.85 53.16 33.18
GPT-3.5-turbo(4k) 50.37 34.83 37.17 50.46 79.33 41.11 73.15 60.63 44.30 47.42 35.27
GPT-4(8k) 53.90 34.16 40.29 52.01 77.42 43.39 81.28 79.21 45.09 49.81 36.34
Table2.Zero-shotconversationalQAresultsacross10datasets. ♢ denotesthattheHumanAnnotatedConvQAisreplacedwiththe
SyntheticConvQA.†denotesthatthestage-1(SFT)isremovedandonlyChatQAstage-2tuningisapplied. *denotesthatthesingle-
turnQAdatasetsareremovedfromtheChatQAstage-2trainingblends. Both“w/ostage-1”and“w/osingle-turn”settingsusethe
SyntheticConvQAdata.Intermsofaveragescores,ourChatQAmodelsgreatlysurpassSFTandChatcounterparts,andourbestmodel
ChatQA-70BslightlyoutperformsGPT-4.
Over all the 10 datasets, ConvFinQA, SQA, and Hybrid- model’sconversationalQAcapability. Intermsofaveraged
Dialdatasetscontaintabulardatainthedocuments,while scores, Llama2-Chat models surpass SFT model counter-
documentsoftheremainingdatasetsaretext-only.5 partsbyasmallmargin,whileourChatQAmodelsachieve
aroundorover10pointofabsoluteimprovementoverSFT
5.3.EvaluationMetrics andChatcounterparts.Forexample,ChatQA-13Bimproves
onLlama2-13B-SFTandLlama2-13B-Chatby13.17(from
GiventhatF1scoreisthemostcommonlyusedautomatic
37.69 to 50.86) and 10.52 (from 40.34 to 50.86), respec-
metrictoassessQAmodels, weuseitforalldatasetsex-
tively. This is because context-enhanced instruction fine-
ceptforConvFinQA.InConvFinQA,wefollowChenetal.
tuningenablesthemodeltolearnhowtoeffectivelyextract
(2022a)touseexactmatchmetricsincetheanswersinCon-
usefulinformationfromretrievedorrelevantcontext.
vFinQAareaboutextractingnumbersfromdocumentsas
well as arithmetic calculations. Hence, the answer only ComparedtoOpenAImodels,ourbestmodelChatQA-70B
makes sense when it is exactly the same as the answer. surpassesGPT-3.5-turboby3.77averagescoreandslightly
Whenmodelsgeneratethearithmeticformula,wewillcal- outperformsGPT-4by0.24averagescore. Inaddition, a
culateitsfinalresultbasedonacalculatorandcompareit much smaller size of ChatQA-13B is able to marginally
withthegoldanswer. outperformGPT-3.5-turbobyanaveragescoreof0.49.
Inaddition,wealsoconducthumanevaluationstoassessthe
6.1.2.IMPORTANCEOFSTAGE-1SFT
correctnessofgeneratedanswersbetweenourbestmodel
andGPT-4. In Table 2, we conduct ablation study on the importance
of stage-1 SFT, which enhance the instruction following
capabilityofChatQA.Weremovethestage-1SFTfromthe
6.Results
fine-tuningstagesandonlyapplystage-2context-enhanced
6.1.MainResults instructiontuningontopoffoundationLLM.Wefindthat
theaveragescoredrops1.9(from54.08to52.18). Aside
6.1.1.OVERVIEW
fromtheSQA,removingstage-1makesthemodelperform
InTable2,wecomparedifferentmodelvariantsandOpenAI consistentlyworseonotherdatasets. Resultsindicatethat
modelsacross10conversationalQAdatasets. stage-1 still plays an important role, even through all of
SFTdatasetsofstage-1areblendedinstage-2instruction
We find that our ChatQA method greatly improves the tuningaswell. Wefigurethatbuildinginstruction-following
capabilityfirstisbeneficialforstage-2tuning.
5DetailsofthesebenchmarkdatasetsareintheAppendixC.
7ChatQA:BuildingGPT-4LevelConversationalQAModels
OursWin Tie GPT-4Win Models Avg-text Avg-table Avg-ret Avg-nonret
Average 13.81% 69.09% 17.10% ChatQA-13B 45.80 62.68 40.01 61.72
ChatQA-70B 48.88 66.42 42.33 65.96
Doc2Dial 14.29% 68.00% 17.71%
GPT-3.5-turbo(4k) 46.07 60.40 40.41 60.33
QuAC 11.67% 73.33% 15.00%
GPT-4(8k) 46.96 70.10 41.58 66.22
QReCC 11.11% 77.22% 11.67%
CoQA 7.78% 80.00% 12.22%
DoQA 22.78% 57.78% 19.44% Table4.Fine-grainedstudiesonaveragescoresofdifferentdataset
ConvFinQA 16.67% 67.78% 15.55% types. Avg-textcoversdatasetswherethedocumentsonlyhave
SQA 11.11% 61.67% 27.22% text,includingDoc2Dial,QuAC,QReCC,CoQA,DoQA,Topi-
TopiOCQA 19.31% 60.69% 20.00% OCQA,andINSCIT.Avg-tablecoversdatasetshavingtableinthe
HybridDial 7.78% 78.33% 13.89% documents,includingConvFinQA,SQA,andHybridDial.Avg-ret
INSCIT 15.56% 66.11% 18.33% coversdatasetshavinglongdocumentsrequiringretrieval,includ-
ing Doc2Dial, QuAC, QReCC, TopiOCQA, and INSCIT. Avg-
nonretcoversdatasetshavingshortdocumentswhichdonotre-
Table3.Humanevaluation(A/Btesting)comparingourChatQA-
quireretrieval,includingCoQA,DoQA,ConvFinQA,SQA,and
70BtoGPT-4over10datasets. Intermsofaveragescores,our
HybridDial.
modelandGPT-4aretiemostofthetime(69.09%),andGPT-4
achievesslightlyhigherwinrate(∼3.3%)thanours.
6.1.5.HUMANEVALUATION
DespiteF1scoresarethemostcommonlyusedmetricsfor
evaluatingthequalityofQAmodels,thereareoftenmulti-
6.1.3.EFFECTIVENESSOFSINGLE-TURNDATA
plewaystoanswerquestions,whichmakestheautomatic
Toinvestigatehowsingle-turnQAdatasetsaffectmodel’s metricslessthanperfect. Therefore,weusehumanevalua-
multi-turn QA capability, we conduct ablation study by tionstofurthercompareourChatQA-70BwithGPT-4. In
removing them from the ChatQA-70B training blends in thishumanevaluation,weaskannotatorstoverifythefacts
stage-2. AsshowninTable2,incorporatingsingle-turnQA inChatQA-70BandGPT-4’soutputsanddeterminewhich
datasetsinthestage-2trainingblends(ChatQA-70B)gener-
modelprovidesamoreaccurateresponsetothequestion6.
allymakethescoresincreaseacrossallbenchmarkdatasets, Thehumanevaluationresultsoverthe10datasetsareshown
whichleadstoanaverageimprovementof1.83score. Inter- in Table 3. We first find that our ChatQA-70B and GPT-
estingly,weobserveimprovementinConvFinQA,SQA,and 4 are tie most of the time (69.09%), and GPT-4 achieves
HybridDial(table-baseddatasets),despitetheaddedsingle- slightlyhigherwinrate(∼3.3%)thanours.Thisfurthercon-
turnQAdatasetsdonohavetabulardatainthedocuments. firmsourmodelhaspowerfulcapabilitytoproducecorrect
Theseresultsalignwithourintuitions. Addingsingle-turn answers. Second,wefindthatourmodelachievesslightly
datasetsimprovesthemodel’scapabilitytoextractanswers betterwinratethanGPT-4inConvFinQA,whichindicates
fromcontext,resultinginbetterscoresinconversationalQA thestrongarithmeticcalculationabilityofourmodel. Third,
datasets. wefindthatGPT-4achievessignificantlybetterwinratein
SQAtask,whichsuggeststhatthereisstillagapbetween
6.1.4.HUMANANNOTATEDDATAVS. GPT-3.5-TURBO ourmodelandGPT-4ontabularreasoningtask.
SYNTHETICDATA
6.2.Fine-grainedAnalyses
InTable2,wealsocompareourChatQAmodelsusingthe
7kGPT-3.5-Turbosyntheticdataset(SyntheticConvQA)and InTable4,wefurthercomparedourmodelsandOpenAI
ourcollected7khuman-annotateddataset(HumanAnnotat- modelsacrossdifferentdatasettypesinourconversational
edConvQA). First, we find that both achieve comparable QAbenchmarks. InthecomparisonbetweenChatQA-70B
resultsintermsofaveragescores,whichsuggeststhatwe and GPT-4, ChatQA-70B achieves better results in text-
do not need to rely on synthetic data from OpenAI mod- onlydocuments(avg-text),whichdemonstrateitssuperior
elstobuildthestate-of-the-artconversationalQAmodels. textunderstandingability. While,GPT-4showsbetterQA
Second,wefindthatusinghuman-annotateddataachieved capabilityintabulardatagiventhecomparisonsinavg-table.
significant improvements on QuAC and DoQA datasets. Asforthedatasetsthatrequireordonotrequireretrieval,
Thiscanbeattributedtothefactthatthehuman-annotated ChatQA-70BandGPT-4arecomparable(gapsarewithin
datahavehigherqualityonunanswerablecaseswhichexists anaveragescoreof1).
in QuAC and DoQA datasets. Eventually, it leads to the
InthecomparisonbetweenChatQA-13BandGPT-3.5-turbo,
overallimprovementsonthesetwodatasets. Detailresults
andanalysesonunanswerablecasescanbefoundin§6.5). 6MorehumanevaluationsetupcanbefoundintheAppendixD.
8ChatQA:BuildingGPT-4LevelConversationalQAModels
Models Avg-all Avg-ret Avg-nonret constructedNarrativeQAtoreplacetheoriginaloneforthe
stage-2instructiontuning.
ChatQA-70B 54.14 42.33 65.96
-w/“top-5”chunks 54.04 42.91 65.16
In Table 5, we find that using “top-5” chunks as the con-
textfortrainingleadstoimprovementsonthedatasetsthat
Table5.Ablationstudyonusing“top-5”retrievedchunkasthe
needretrieval. Butitdowngradestheperformanceonnon-
contextforthestage-2instructiontuning.Wereportaveragescores
retrievaldatasets. Overall,thesetwomodelsperformcom-
onalldatasets(Avg-all),fiveretrievaldatasets(Avg-ret)andfive
non-retrievaldatasets(Avg-nonret). parable.Itisbecauseincorporating“top-5”retrievedchunks
instage-2tuningalignswiththeinferencestagewherere-
trieval is needed and hence, improves the Avg-ret score.
Models Avg. D2D QuAC QReCC TopiO INSCIT
However,mixingcontinuousanddiscontinuousdocuments
ChatQA-70B 42.31 39.19 38.33 48.73 51.30 33.98
couldmakethestage-2tuninglessstable, leadingtosub-
-#ofctx:top-3 41.91 37.20 38.35 48.94 52.78 32.27
-#ofctx:top-10 40.71 37.06 36.95 47.61 49.40 32.53 optimalresultsonnon-retrievaldatasets. Webelievemore
-ctxreverseordering 42.48 39.08 38.85 49.63 51.16 33.69 futureworkcanbeconductedregardingthebalanceofin-
-ctxswingordering 42.30 39.35 38.09 49.09 50.98 33.99
corporatingcontinuouscontextandtop-kretrievedchunks
-ctxrandomordering 42.01 39.32 38.28 48.79 50.13 33.51
-DragonRetrieval 40.50 37.92 38.44 47.88 50.39 27.87 inthestage-2tuning.
Table6.Ablationstudiesoninputcontextacrossdatasetsthatneed 6.4.AblationStudiesforInferenceStage
retrieval.AllmodelsareusingtheSyntheticConvQA.D2Ddenotes
In Table 6, we show ablation studies on how the number
Doc2Dial,andTopiOdenotesTopiOCQA.Westudynumberof
ofretrievedcontext/chunks,contextordering,anddifferent
contextsusedininputs(#ofctx),contextordering(reverse,swing,
retrieversaffecttheconversationalQAresults.
random), andusingretrievedcontextfromoriginalDragon. In
comparison, ChatQA-70B (default setting) is using “Dragon +
First, we find that using more contexts as inputs do not
Fine-tune”retrievedtop-5contexts,andthesequentialordering
alwaysimprovetheresults. Utilizingtop-5contextsasin-
fromfirsttofifthcontextintop-5.
putyieldsbetterresultscomparedtousingeithertop-3or
top-10contexts. Intuitively,morecontextshavehigherprob-
abilitytocontaincorrectanswers(betterrecallscores). As
ChatQA-13B demonstrates better tabular QA capability
a result, using top-5 contexts achieves better results than
given the scores in Avg-table. ChatQA-13B also shows
using top-3. However, as the number of contexts further
betterscoresindatasetswheredocumentsthatdonotneed
increases,themodelmaysufferfrom“lostinthemiddle”
retrieval,whileitisonparwithGPT-3.5-turbointext-only
phenomenon(Liuetal.,2023)andthedifficultyofextract-
documentsanddocumentsthatneedretrieval.
inganswersfromtheprovidedcontextcouldalsoincrease,
whichleadstoinferiorresultsbyusingtop-10contexts.
6.3.Top-kChunksforStage-2InstructionTuning
Second, we study how using different orderings of top-5
Forallthedatasetsusedbystage-2tuning,thecontextare
contextsaffectstheresults. Wecomparesequentialordering
providedascontinuousparagraphsordocumentsthatcon-
(from1stcontextto5thcontext)toreverseordering(from
taintheanswer. Incontrast,themodelneedstohandlethe
5thto1stcontext), swingordering(giventhe“lostinthe
top-kretrievedchunksatinferenceforlongdocuments. To
middle”phenomenon,wearrangethemostrelevantcontext
reducesuchtrain/testmismatch,weinvestigatewhetherre-
toappearatthebeginningandtheendoftheinputcontext.
placing some continuous paragraphs with retrieved top-k
Hence, the ordering becomes {1st, 3rd, 5th, 4th, 2nd}),
chunkswillenhancethemodel’srobustness.
andrandomordering(randomshufflethetop-5contexts).
WeuseNarrativeQAforthisstudy,sinceeachquestionhas Wefindusingsequentialorderingiscomparabletousing
acorrespondinglongdocument. ForNarrativeQA,weorigi- reverseandswingorderings,andrandomshufflingisslightly
nallyuseasummaryofthelongdocumentasthecontext, worse. Resultsindicatethatourmodelexcelsinextracting
whichcontainstheanswer. Toincorporatediscontinuous thecorrectanswerfromlengthycontexts,regardlessofthe
contexts, we first cut the long document into 300-word answer’s location. It is because during the ChatQA fine-
chunks. Then, we use Dragon retriever to retrieve top-4 tuning, theanswer’slocationoccursrandomlywithinthe
chunks to the question as the additional context. Finally, context.
wetaketheretrievedfourchunksandthesummaryofthe
Third, weobservethatwhenwereplace“Dragon+Fine-
long document as the “top-5” chunks. 7 We use this re-
tune”withtheoriginalnon-finetunedDragonretriever,the
7Note that, we did not directly use top-5 retrieved chunks averagescoredropsby1.81(from42.31to40.50). Inaddi-
fortraining,becausetheymaynotcontaintheanswer. Insuch tion,thescoredropssignificantlyinINSCITdataset(from
cases,fine-tuningthemodeltogenerateanswercouldencourage 33.98to27.87)duetothelargeperformancegapbetween
hallucination.
9ChatQA:BuildingGPT-4LevelConversationalQAModels
Models Avg-Both Avg-QuAC QuAC(no*) QuAC(yes*) Avg-DoQA DoQA(no*) DoQA(yes*) Avg-CQA
ChatQA-70B
-1kunanswerable† 76.88 80.89 75.10 86.67 72.88 64.49 81.26 54.16
-1.5kunanswerable† 77.25 80.76 77.66 83.85 73.74 68.81 78.67 54.14
-2kunanswerable† 77.10 80.82 77.59 84.05 73.38 67.95 78.80 53.86
-2.5kunanswerable† 75.87 78.81 73.76 83.85 72.93 66.54 79.31 53.78
-SyntheticConvQA♢ 69.84 72.92 55.38 90.42 66.77 45.09 88.45 54.08
GPT-3.5-turbo(4k) 73.27 78.34 61.91 94.76 68.21 51.99 84.43 50.37
GPT-4(8k) 80.73 87.42 83.45 91.38 74.05 74.28 73.82 53.90
Table7.AccuraciesonanswerableandunanswerablesamplesacrossQuACandDoQAdatasets.Avg-Bothistheaveragedscorebetween
QuACandDoQA.♢denotesthattheHumanAnnotatedConvQAisreplacedwiththeSyntheticConvQA.*“no”denotestheunanswerable
samples,while“yes”denotestheanswerablesamples.†Weconductablationstudiesintermsofthenumberofunanswerablesamplesin
HumanAnnotatedConvQA.Avg-CQAistheaveragescoreacrossthe10conversationalQAdatasets.Weuse1.5kunanswerablesamples
forourfinalChatQA-70B,asitproducesbothhigh-qualitygenerationandlesshallucination.
thetworetrievers(asshowninTable1). Basically,when comparedtousingSyntheticConvQA,usingHumanAnno-
the quality of retrieval improves, it directly enhances the tatedConvQAsignificantlyincreasetheaverageaccuracies
performanceofquestionanswering. onbothQuACandDoQA.Itisbecausetheunanswerable
annotationsareinhigherqualityforhuman-annotateddata,
6.5.EvaluationofUnanswerableCase whichsignificantlyimprovetheaccuracyforunanswerable
cases. Second,OpenAImodelsshowpowerfulcapabilityin
6.5.1.EVALUATIONSETUP
thistask,especiallyforGPT-4. Comparedtothem,ourbest
Inthissection,westudyanotheraspectofthemodel’sca- modelachievedsignificantlybetteraverageaccuracythan
pability,whichistodiscernifaquestioncanbeanswered GPT-3.5-turbo,whilewestillhasaslightgapcomparedto
withintheprovidedcontext. Generatingananswerinunan- GPT-4(around3.5%). Third,wefindthatmodelsachieving
swerablecasewillleadtohallucination. Toallowthiseval- higheraccuracyonunanswerablesamplestendstogetlower
uation,werequirethemodeltoindicateitwhennoanswer accuracyonanswerablesamples,andviceversa. Wespec-
canbefoundinthegivencontext. ulatethatwhenamodeltendstobe“aggressive”andoffer
somewhatrelevantanswerstothoseunanswerablequestions,
WeuseQuACandDoQAdatasetswhichhavesuchunan-
itwillboosttheaccuracyforanswerablecases,butreduces
swerablecasestoevaluatesuchcapability. Specifically,for
accuracyforunanswerableones. Conversely,whenamodel
unanswerablecase,weconsiderthemodelindicatingthat
ismore“conservative”andstrictlycheckifthequestioncan
thequestioncannotbeansweredascorrect8,andasforan-
beansweredwillresultintheoppositeeffects.
swerablecases, weconsiderthemodelnotindicatingthe
question is unanswerable as correct (i.e., the model giv- Weconductablationstudiesintermsofthenumberofunan-
ing an answer). Note that for answerable cases, we only swerable samples in HumanAnnotatedConvQA. We find
selectthesampleswherecorrectcontextisretrieved. Our thatusingasmallamountofunanswerablesamples(e.g.,
modelcheckpoints(ChatQA-70Bw/1.5kunanswerableand 1.5k)isabletoachieveremarkableresultsontheunanswer-
ChatQA-70Bw/SyntheticConvQA)aresameonesreported able evaluation, as well as the overall conversational QA
inTable2. score (i.e., Avg-CQA). Interestingly, incorporating more
unanswerablesamplesdoesnotnecessarilyleadtohigher
Intheend,wecalculatetheaverageaccuracyscoreofunan-
accuracyscoresintheunanswerableevaluation.
swerable and answerable cases as the final metric. We
considerthisaverageaccuracyasareliablemetricsinceitis
6.6.CaseStudy
inthesamespiritofF1metricwhichmeasurestheharmonic
meanofprecisionandrecallscores. In Table 8, we show four examples of ChatQA-70B and
GPT-4outputs.9
6.5.2.RESULTS
Thefirstexample(aboutBoardhearing)isasimpleinfor-
In Table 7, we compare our models with OpenAI mod- mationseekingproblem,andbothChatQA-70BandGPT-4
els across QuAC and DoQA datasets. First, we find that answercorrectly. Inthesecondexample(aboutArya),the
model needs to find implicit information (highlighted in
8Wecollectasetofheuristicmatchingpatternsfromallgener-
blue) for giving the answer. GPT-4 tends to be safe on
atedsamplestodetermineifthemodelsuggeststhatthequestion
isunanswerable.MoredetailscanbefoundintheAppendixE. 9MoreexamplescanbefoundinAppendixF.
10ChatQA:BuildingGPT-4LevelConversationalQAModels
Context Conversation AssistantResponses
ChatQA-70B:Ifyou’reappealingaVAdecisionyoureceivedonor
TheprocessforBoardhearingshaschanged.Ifyou’reappealingaVAdecision User:WhatcanIlearnatmyBoardhearing?
afterFebruary19,2019,you’llfollowthenewprocess.
underthenewprocess,keepreadingbelowtolearnhowtorequestandprepare Assistant:Doyouwanttolearnexactlywhattoexpect?
forahearing.You’llfollowthenewprocessifyou’re:AppealingaVAdecision ...
GPT-4:You’llfollowthenewprocessforaBoardhearingifyou’re
youreceivedonorafterFebruary19,2019.ParticipatingintheRapidAppeals User:IsitnecessaryformetoconsiderrequestingaBoardhearing?
appealingaVAdecisionyoureceivedonorafterFebruary19,2019,
ModernizationProgramOptingintothenewprocessusingVAForm10182If Assistant:Youcanrequestavideoconferencehearingifyouwant
participatingintheRapidAppealsModernizationProgram,oropting
youreusingVAForm9toappealaVAdecisionyoureceivedbeforeFebruary toprovidetestimonytoajudgeaboutyourappeal.
intothenewprocessusingVAForm10182.
19,2019,gototheBoardofVeterans’AppealsBVAHearingpage...TheBoard User:whenwillIfollowthenewprocessforaBoardhearing?
ofVeteransAppealsscheduleshearingsindocketordertheorderinwhichthey
GroundTruth:YouwillfollowthenewprocessifappealingaVA
receiverequests... Assistant:
decisionifyoureceiveditonorafterFebruary19,2019.
User:HowdidAryasurvivethisconfrontationunscathed?
title:HowdidAryasurvivethisconfrontationunscathed?
ChatQA-70B:Thechildappearedtobeababy.
TherewasnotimeorintentiontoturnherintoWhiteWalker.Letuslookatthe
Assistant:Aryawasquickenoughtoreact,dropthedaggerand
scenefromS04E04.Awhitewalkerbringsababyholdinginhislapandputsit
dotherest. GPT-4:Thetextdoesnotprovideinformationontheexactageofthe
onanicetable(orwhateveritis)forthearrivaloftheNightKing....WhenArya
childthatwaslaidontheicealtar.
dropsthedagger,NightKinglookssurprised.Thisshowshedoesn’tknowwhat
User:Howoldisthechildthattheylayonthealtarofice?
wouldhappennext.Therewasnotimetothinkandturnherintoawhitewalker
GroundTruth:ababy
bytouch.TherewasnoplanninginthisscenelikeS04E04....
Assistant:
Massif Typeofnaturereserve Preservedarea User:whichoftheforestsisacarpathianbiospherereserve?
Chornohora CarpathianBiosphere 2476.8ha ChatQA-70B:Uholka/WideMeadow
Uholka/WideMeadow CarpathianBiosphere 11860ha Assistant:Chornohora,Uholka/WideMeadow,Svydovets,
Svydovets CarpathianBiosphere 3030.5ha Maramoros,Kuziy/Trybushany GPT-4:Chornohora,Uholka/WideMeadow,Svydovets,Maramoros
Maramoros CarpathianBiosphere 2243.6ha
Rozok PresovPreserved 67.1ha User:whichofthosecoversmorethan3100ha? GroundTruth:Uholka/WideMeadow
Jasmund JasmundNationalPark 492.5ha
... Assistant:
title:Summaryofenemyaircraftdamaged/destroyed(JohnB.England)
Date Location Number Type Status User:whatwereallofthedatesinwhichjohnb.englanddestroyedor
24April1944 Munich,Germany 3 Me-110 Destroyed damagedenemyplanes? ChatQA-70B:6October1944,13September1944,27November1944
27May1944 Strasbourg,France 1 Me-109 Damaged (missing24April1944)
18August1944 Paris,France 0.5 Me-109 Destroyed Assistant:24April1944,27May1944,18August1944,13September
13September1944 Nordhausen,Germany 2.5 Me-109 Destroyed 1944,6October1944,27November1944,14January1945... GPT-4:24April1944,13September1944,6October1944,
6October1944 Berlin,Germany 1 Me-109 Damaged 27November1944
6October1944 Berlin,Germany 2 Me-109 Destroyed User:onwhichofthosedatesdidjohnb.englanddestroyordamage
27November1944 Magdeburg,Germany 4 FW-190 Destroyed morethanoneenemyplane? GroundTruth:24April1944,13September1944,6October1944,
14January1945 Berlin,Germany 1 Me-109 Destroyed 27November1944
... Assistant:
Table8.CasestudiesonChatQA-70BandGPT-4.Weusebluecolortohighlighttherelevantcontextforthelastuserquestion.Andwe
usegreenandredcolortohighlightthecorrectandincorrectpartsintheassistantresponses.
providing an answer, and it replies that the context does cansignificantlyenhanceourmodel’scapabilitytohandle
notprovideexactinformationabouttheagewhichisalso scenarioswhereanswersareunavailable. Theunanswerable
correct. caseevaluationhighlightsthatourbestmodelChatQA-70B
onlyhasaslightgapcomparedtoGPT-4.
Boththethirdandfourthexamplesrequirethemodeltohave
good tabular understanding and reasoning ability. In the
thirdexample(aboutMassif),ChatQA-70Bgivescorrect References
answer by comparing the size of preserved area against
Adlakha, V., Dhuliawala, S., Suleman, K., de Vries, H.,
3100ha,whileGPT-4failstodoso. Inthefourthexample
andReddy, S. Topiocqa: Open-domainconversational
(aboutJohnB.England),ChatQA-70Bcorrectlyliststhree
questionansweringwithtopicswitching. TACL,2022.
datesbutmissesonedate,whileGPT-4correctlyanswers
thequestion. Aliannejadi,M.,Kiseleva,J.,Chuklin,A.,Dalton,J.,and
Burtsev, M. Building and evaluating open-domain di-
alogue corpora with clarifying questions. In EMNLP,
7.Conclusion
2021.
Inthispaper,webuildafamilyofChatQAmodels,vary-
Anantha,R.,Vakulenko,S.,Tu,Z.,Longpre,S.,Pulman,S.,
ing in model sizes from 7B to 70B. Comprehensive eval-
andChappidi,S. Open-domainquestionansweringgoes
uations on 10 conversational QA datasets show that our
conversationalviaquestionrewriting. InNAACL,2021.
bestChatQA-70BmodelcanremarkablyoutperformsGPT-
3.5-turbo and perform on par with GPT-4 without using Anthropic. Introducing100kcontextwindows,2023a.
anysyntheticdatafromChatGPTmodels. Inaddition,we
demonstrate that fine-tuning a single-turn query retriever Anthropic. IntroducingClaude,2023b.
using our curated conversational QA data performs com-
Brabant,Q.,Lecorve´,G.,andBarahona,L.M.R. Coqar:
parabletothestate-of-the-artLLM-basedqueryrewriting
Questionrewritingoncoqa. InLREC,2022.
model,withouttheneedofextracomputationaltimeandpo-
tentialAPIcostfromrewriting. Furthermore,weshowthat Campos,J.A.,Otegi,A.,Soroa,A.,Deriu,J.M.,Cieliebak,
incorporatingasmallamountof“unanswerable”samples M.,andAgirre,E. Doqa-accessingdomain-specificfaqs
viaconversationalqa. InACL,2020.
11ChatQA:BuildingGPT-4LevelConversationalQAModels
Chen, Z., Li, S., Smiley, C., Ma, Z., Shah, S., andWang, Du,Z.,Qian,Y.,Liu,X.,Ding,M.,Qiu,J.,Yang,Z.,and
W.Y. Convfinqa: Exploringthechainofnumericalrea- Tang,J. Glm: Generallanguagemodelpretrainingwith
soninginconversationalfinancequestionanswering. In autoregressiveblankinfilling. InACL,2022.
EMNLP,2022a.
Dua,D.,Wang,Y.,Dasigi,P.,Stanovsky,G.,Singh,S.,and
Chen,Z.,Zhao,J.,Fang,A.,Fetahu,B.,Rokhlenko,O.,and Gardner,M. Drop: Areadingcomprehensionbenchmark
Malmasi,S. Reinforcedquestionrewritingforconversa- requiringdiscretereasoningoverparagraphs. InNAACL,
tionalquestionanswering. InEMNLP,2022b. 2019.
Choi,E.,He,H.,Iyyer,M.,Yatskar,M.,Yih,W.-t.,Choi,Y., Elgohary, A., Peskov, D., and Boyd-Graber, J. Can you
Liang,P.,andZettlemoyer,L. Quac: Questionanswering unpackthat? learningtorewritequestions-in-context. In
incontext. InEMNLP,2018. EMNLP,2019.
Chu,Z.,Chen,M.,Chen,J.,Wang,M.,Gimpel,K.,Faruqui, Fan,A.,Jernite,Y.,Perez,E.,Grangier,D.,Weston,J.,and
M.,andSi,X. Howtoaskbetterquestions? alarge-scale Auli,M. Eli5: Longformquestionanswering. InACL,
multi-domaindatasetforrewritingill-formedquestions. 2019.
InAAAI,2020.
Feng,S.,Wan,H.,Gunasekara,C.,Patel,S.,Joshi,S.,and
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., Lastras,L.doc2dial:Agoal-orienteddocument-grounded
Fedus,W.,Li,Y.,Wang,X.,Dehghani,M.,Brahma,S., dialoguedataset. InEMNLP,2020.
Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X.,
Chowdhery, A., Castro-Ros, A., Pellat, M., Robinson, Galimzhanova,E.,Muntean,C.I.,Nardini,F.M.,Perego,
K.,Valter,D.,Narang,S.,Mishra,G.,Yu,A.,Zhao,V., R., and Rocchietti, G. Rewriting conversational ut-
Huang,Y.,Dai,A.,Yu,H.,Petrov,S.,Chi,E.H.,Dean, terances with instructed large language models. In
J.,Devlin,J.,Roberts,A.,Zhou,D.,Le,Q.V.,andWei, IEEE/WICInternationalConferenceonWebIntelligence
J. Scalinginstruction-finetunedlanguagemodels. arXiv andIntelligentAgentTechnology,2023.
preprintarXiv: 2210.11416,2022.
Gao,C.,Zhang,W.,andLam,W. Unigdd: Aunifiedgen-
Conover, M., Hayes, M., Mathur, A., Meng, X., Xie, J., erativeframeworkforgoal-orienteddocument-grounded
Wan,J.,Shah,S.,Ghodsi,A.,Wendell,P.,Zaharia,M., dialogue. InACL,2022.
etal. Freedolly: Introducingtheworld’sfirsttrulyopen
instruction-tunedllm,2023a. Gao,Y.,Xiong,Y.,Gao,X.,Jia,K.,Pan,J.,Bi,Y.,Dai,Y.,
Sun,J.,andWang,H. Retrieval-augmentedgeneration
Conover,M.,Hayes,M.,Mathur,A.,Xie,J.,Wan,J.,Shah, for large language models: A survey. arXiv preprint
S.,Ghodsi,A.,Wendell,P.,Zaharia,M.,andXin,R. Free arXiv:2312.10997,2023.
Dolly:Introducingtheworld’sfirsttrulyopeninstruction-
tunedllm,2023b. Google. Introducingbard,2023.
Dai,Z.,Chaganty,A.T.,Zhao,V.,Amini,A.,Green,M., Guo, M., Zhang, M., Reddy, S., and Alikhani, M. Abg-
Rashid, Q., and Guu, K. Dialog inpainting: Turning coqa: Clarifying ambiguity in conversational question
documentstodialogs. InICML,2022. answering. InAKBC,2021.
Dasigi,P.,Liu,N.F.,Marasovic´,A.,Smith,N.A.,andGard- Honovich,O.,Scialom,T.,Levy,O.,andSchick,T. Unnat-
ner,M. Quoref: Areadingcomprehensiondatasetwith uralinstructions: Tuninglanguagemodelswith(almost)
questionsrequiringcoreferentialreasoning. InEMNLP, nohumanlabor. arXivpreprintarXiv:2212.09689,2022.
2019.
Ishii,E.,Xu,Y.,Cahyawijaya,S.,andWilie,B. Canques-
DelTredici, M., Barlacchi, G., Shen, X., Cheng, W., and tionrewritinghelpconversationalquestionanswering?
deGispert,A. Questionrewritingforopen-domaincon- InProceedingsoftheThirdWorkshoponInsightsfrom
versationalqa: Bestpracticesandlimitations. InCIKM, NegativeResultsinNLP,2022.
2021.
Iyer, S., Lin, X. V., Pasunuru, R., Mihaylov, T., Simig,
Deng, Y., Lei, W., Zhang, W., Lam, W., and Chua, T.- D., Yu, P., Shuster, K., Wang, T., Liu, Q., Koura, P.S.,
S. Pacific: Towards proactive conversational question etal. Opt-iml: Scalinglanguagemodelinstructionmeta
answering over tabular and textual data in finance. In learningthroughthelensofgeneralization.arXivpreprint
EMNLP,2022. arXiv:2212.12017,2022.
12ChatQA:BuildingGPT-4LevelConversationalQAModels
Izacard,G.andGrave,E´. Leveragingpassageretrievalwith flan collection: Designing data and methods for effec-
generativemodelsforopendomainquestionanswering. tiveinstructiontuning. arXivpreprintarXiv:2301.13688,
InProceedingsofthe16thConferenceoftheEuropean 2023.
ChapteroftheAssociationforComputationalLinguistics,
Mele,I.,Muntean,C.I.,Nardini,F.M.,Perego,R.,Tonel-
2021.
lotto,N.,andFrieder,O. Adaptiveutterancerewritingfor
Izacard,G.,Caron,M.,Hosseini,L.,Riedel,S.,Bojanowski, conversationalsearch. InformationProcessing&Man-
P.,Joulin,A.,andGrave,E. Unsuperviseddenseinfor- agement,2021.
mationretrievalwithcontrastivelearning. Transactions
Mishra,S.,Khashabi,D.,Baral,C.,andHajishirzi,H.Cross-
onMachineLearningResearch,2022.
taskgeneralizationvianaturallanguagecrowdsourcing
Kim, H., Hessel, J., Jiang, L., Lu, X., Yu, Y., Zhou, instructions. InACL,2022.
P., Bras, R. L., Alikhani, M., Kim, G., Sap, M.,
Mo,F.,Mao,K.,Zhu,Y.,Wu,Y.,Huang,K.,andNie,J.-Y.
et al. Soda: Million-scale dialogue distillation with
Convgqr: Generativequeryreformulationforconversa-
social commonsense contextualization. arXiv preprint
tionalsearch. arXivpreprintarXiv:2305.15645,2023.
arXiv:2212.10465,2022.
Muennighoff,N.,Wang,T.,Sutawika,L.,Roberts,A.,Bi-
Kocˇisky`,T.,Schwarz,J.,Blunsom,P.,Dyer,C.,Hermann,
derman,S.,Scao,T.L.,Bari,M.S.,Shen,S.,Yong,Z.-X.,
K.M.,Melis,G.,andGrefenstette,E. Thenarrativeqa
Schoelkopf,H.,etal.Crosslingualgeneralizationthrough
readingcomprehensionchallenge. TACL,2018.
multitaskfinetuning. arXivpreprintarXiv:2211.01786,
2022.
Ko¨pf, A., Kilcher, Y., von Ru¨tte, D., Anagnostidis, S.,
Tam,Z.-R.,Stevens,K.,Barhoum,A.,Duc,N.M.,Stan-
Nakamura,K.,Levy,S.,Tuan,Y.-L.,Chen,W.,andWang,
ley, O., Nagyfi, R., etal. Openassistantconversations–
W.Y. Hybridialogue: Aninformation-seekingdialogue
democratizing large language model alignment. arXiv
datasetgroundedontabularandtextualdata. InFindings
preprintarXiv:2304.07327,2023.
ofACL,2022.
Ko¨pf,A.,Kilcher,Y.,vonRu¨tte,D.,Anagnostidis,S.,Tam,
Nguyen,T.,Rosenberg,M.,Song,X.,Gao,J.,Tiwary,S.,
Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley,
Majumder,R.,andDeng,L. Msmarco: Ahumangen-
O., Nagyfi, R., ES, S., Suri, S., Glushkov, D., Dantu-
eratedmachinereadingcomprehensiondataset. choice,
luri,A.,Maguire,A.,Schuhmann,C.,Nguyen,H.,and
2016.
Mattick,A. Openassistantconversations-democratizing
largelanguagemodelalignment. arXivpreprintarXiv: OpenAI. IntroducingChatGPT,2022.
2304.07327,2023.
OpenAI. GPT-4,2023.
Lin, K., Tafjord, O., Clark, P., and Gardner, M. Reason-
Ouyang,L.,Wu,J.,Jiang,X.,Almeida,D.,Wainwright,C.,
ingoverparagrapheffectsinsituations. InProceedings
Mishkin,P.,Zhang,C.,Agarwal,S.,Slama,K.,Ray,A.,
ofthe2ndWorkshoponMachineReadingforQuestion
et al. Training language models to follow instructions
Answering,2019.
withhumanfeedback. NeurIPS,2022.
Lin,S.-C.,Asai,A.,Li,M.,Oguz,B.,Lin,J.,Mehdad,Y.,
Pasupat,P.andLiang,P. Compositionalsemanticparsing
Yih,W.-t.,andChen,X. Howtotrainyourdragon: Di-
onsemi-structuredtables. InACL,2015.
verseaugmentationtowardsgeneralizabledenseretrieval.
arXivpreprintarXiv:2302.07452,2023a. Qu,C.,Yang,L.,Chen,C.,Qiu,M.,Croft,W.B.,andIyyer,
M. Open-retrievalconversationalquestionanswering. In
Lin,X.V.,Chen,X.,Chen,M.,Shi,W.,Lomeli,M.,James,
SIGIR,2020.
R., Rodriguez, P., Kahn, J., Szilvasy, G., Lewis, M.,
etal.Ra-dit:Retrieval-augmenteddualinstructiontuning. Rajpurkar,P.,Zhang,J.,Lopyrev,K.,andLiang,P. Squad:
arXivpreprintarXiv:2310.01352,2023b. 100,000+questionsformachinecomprehensionoftext.
InEMNLP,2016.
Liu, N. F., Lin, K., Hewitt, J., Paranjape, A., Bevilac-
qua, M., Petroni, F., and Liang, P. Lost in the middle: Rajpurkar,P.,Jia,R.,andLiang,P. Knowwhatyoudon’t
Howlanguagemodelsuselongcontexts. arXivpreprint know: Unanswerablequestionsforsquad. InACL,2018.
arXiv:2307.03172,2023.
Raposo,G.,Ribeiro,R.,Martins,B.,andCoheur,L. Ques-
Longpre, S., Hou, L., Vu, T., Webson, A., Chung, H.W., tion rewriting? assessing its importance for conversa-
Tay,Y.,Zhou,D.,Le,Q.V.,Zoph,B.,Wei,J.,etal. The tionalquestionanswering. InECIR,2022.
13ChatQA:BuildingGPT-4LevelConversationalQAModels
Reddy,S.,Chen,D.,andManning,C.D. Coqa: Aconver- Wei,J.,Bosma,M.,Zhao,V.Y.,Guu,K.,Yu,A.W.,Lester,
sationalquestionansweringchallenge. TACL,2019. B.,Du,N.,Dai,A.M.,andLe,Q.V. Finetunedlanguage
modelsarezero-shotlearners. InICLR,2022a.
Saeidi,M.,Bartolo,M.,Lewis,P.,Singh,S.,Rockta¨schel,
T.,Sheldon,M.,Bouchard,G.,andRiedel,S. Interpreta- Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F.,
tionofnaturallanguagerulesinconversationalmachine Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought
reading. InEMNLP,2018. prompting elicits reasoning in large language models.
NeurIPS,2022b.
Sanh,V.,Webson,A.,Raffel,C.,Bach,S.H.,Sutawika,L.,
Alyafeai,Z.,Chaffin,A.,Stiegler,A.,Scao,T.L.,Raja, Wu, Z., Luan, Y., Rashkin, H., Reitter, D., Hajishirzi, H.,
A.,etal. Multitaskpromptedtrainingenableszero-shot Ostendorf,M.,andTomar,G.S. Conqrr: Conversational
taskgeneralization. InICLR,2022. queryrewritingforretrievalwithreinforcementlearning.
InEMNLP,2022.
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Wu,Z.,Parish,R.,Cheng,H.,Min,S.,Ammanabrolu,P.,
Bhosale,S.,etal. Llama2: Openfoundationandfine- Ostendorf, M., and Hajishirzi, H. Inscit: Information-
tuned chat models. arXiv preprint arXiv:2307.09288, seekingconversationswithmixed-initiativeinteractions.
2023. TACL,2023.
Trischler, A., Wang, T., Yuan, X., Harris, J., Sordoni, A., Xu,C.,Sun,Q.,Zheng,K.,Geng,X.,Zhao,P.,Feng,J.,Tao,
Bachman,P.,andSuleman,K. Newsqa: Amachinecom- C.,andJiang,D. Wizardlm: Empoweringlargelanguage
prehensiondataset. InProceedingsofthe2ndWorkshop modelstofollowcomplexinstructions. arXivpreprint
onRepresentationLearningforNLP,2017. arXiv:2304.12244,2023a.
Vakulenko,S.,Longpre,S.,Tu,Z.,andAnantha,R. Ques- Xu,P.,Ping,W.,Wu,X.,McAfee,L.,Zhu,C.,Liu,Z.,Sub-
tionrewritingforconversationalquestionanswering. In ramanian,S.,Bakhturina,E.,Shoeybi,M.,andCatanzaro,
WSDM,2021a. B. Retrievalmeetslongcontextlargelanguagemodels.
arXivpreprintarXiv:2310.03025,2023b.
Vakulenko,S.,Voskarides,N.,Tu,Z.,andLongpre,S. A
comparisonofquestionrewritingmethodsforconversa- Ye,F.,Fang,M.,Li,S.,andYilmaz,E. Enhancingconver-
tionalpassageretrieval. InECIR,2021b. sationalsearch: Largelanguagemodel-aidedinformative
queryrewriting. InEMNLP,pp.5985–6006,2023.
Wang, B., Ping, W., McAfee, L., Xu, P., Li, B., Shoeybi,
M., and Catanzaro, B. Instructretro: Instruction tun- Yu, S., Liu, J., Yang, J., Xiong, C., Bennett, P., Gao, J.,
ingpostretrieval-augmentedpretraining. arXivpreprint and Liu, Z. Few-shot generative conversational query
arXiv:2310.07713,2023a. rewriting. InSIGIR,2020.
Wang, L., Yang, N., Huang, X., Jiao, B., Yang, L., Zhang,S.,Dong,L.,Li,X.,Zhang,S.,Sun,X.,Wang,S.,
Jiang,D.,Majumder,R.,andWei,F. Textembeddings Li,J.,Hu,R.,Zhang,T.,Wu,F.,etal. Instructiontuning
by weakly-supervised contrastive pre-training. arXiv for large language models: A survey. arXiv preprint
preprintarXiv:2212.03533,2022a. arXiv:2308.10792,2023.
Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Zhou,C.,Liu,P.,Xu,P.,Iyer,S.,Sun,J.,Mao,Y.,Ma,X.,
Khashabi,D.,andHajishirzi,H. Self-instruct: Aligning Efrat, A., Yu, P., Yu, L., et al. Lima: Less is more for
languagemodelwithselfgeneratedinstructions. arXiv alignment. arXivpreprintarXiv:2305.11206,2023.
preprintarXiv:2212.10560,2022b.
Zhu, F., Lei, W., Huang, Y., Wang, C., Zhang, S., Lv, J.,
Wang, Y., Mishra, S., Alipoormolabashi, P., Kordi, Y., Feng,F.,andChua,T.-S. Tat-qa: Aquestionanswering
Mirzaei,A.,Naik,A.,Ashok,A.,Dhanasekaran,A.S., benchmarkonahybridoftabularandtextualcontentin
Arunkumar,A.,Stap,D.,etal. Super-naturalinstructions: finance. InACL,2021.
Generalizationviadeclarativeinstructionson1600+nlp
tasks. InEMNLP,2022c.
Wang,Y.,Ivison,H.,Dasigi,P.,Hessel,J.,Khot,T.,Chandu,
K.R.,Wadden,D.,MacMillan,K.,Smith,N.A.,Beltagy,
I., et al. How far can camels go? exploring the state
ofinstructiontuningonopenresources. arXivpreprint
arXiv:2306.04751,2023b.
14ChatQA:BuildingGPT-4LevelConversationalQAModels
A.ChatQAInstructionTuning
A.1.Stage-1: SupervisedFine-tuning
TheformattemplateofLLMinputsinstage-1isasfollows:
System: This is a chat between a user and an artificial intelligence assistant.
The assistant gives helpful, detailed, and polite answers to the user’s questions
based on the context. The assistant should also indicate when the answer cannot be
found in the context.
User: {Question 1}
Assistant: {Answer 1}
...
User: {Latest Question}
Assistant:
Weusethe{Latest Answer}fromAssistantasthesupervisionformodeloutputs.
A.2.Stage-2: Context-EnhancedInstructionTuning
Based on the stage-1 format template, the LLM inputs in stage-2 adds {Context for Latest Question} and
{Instruction}fromUser,asdepictedbelow:
System: This is a chat between a user and an artificial intelligence assistant.
The assistant gives helpful, detailed, and polite answers to the user’s questions
based on the context. The assistant should also indicate when the answer cannot be
found in the context.
{Context for Latest Question}
User: {Instruction} + {Question 1}
Assistant: {Answer 1}
...
User: {Latest Question}
Assistant:
SameasStage-1,weusethe{Latest Answer}fromAssistantasthesupervisionformodeloutputs.
Asforthe{Instruction},weusedifferentinstructionsfordifferentdatasetsbasedontheanswertypes. Detailscanbe
foundbelow:
“Please give a full and complete answer for the question.” This is for datasets with long an-
swers. WeuseitfortheHumanAnnotatedConvQAorSyntheticConvQA.
“Answer the following question with a short span. The answer needs to be just in
a few words.” Thisisfordatasetswithshortanswers. WeuseitforSQuAD1.1,SQuAD2.0,NarrativeQA,DROP,
ROPES,NewsQA,andQuoref.
“Answer the following question with a number from context or the math arithmetic
15ChatQA:BuildingGPT-4LevelConversationalQAModels
using +, -, *, or /.” Thisisfordatasetsusingarithmeticcalculationorextractingnumbersfromthecontext. We
useitfortheTAT-QAdatasetwherequestionsneedthearithmeticcalculationorextractingnumbersfromthecontext.
“Answer the following question with a short span, or a full and complete answer.”
Thisisfordatasetswithbothshortandlonganswers. Themodelneedstotellwhethertogenerateshortorlonganswers
basedonthequestions. WeuseitfortheTAT-QAdatasetwherequestionsdonotneedthearithmeticcalculation,sincewe
observethelengthsofthecorrespondinganswerscanbebothshortandlong.
Forthestage-1SFTdatausedinstage-2instructiontuning,wekeepthesameformattemplateasinstage-1. Inotherwords,
the{Context for Latest Question}and{Instruction}areempty.
A.3.PromptsforSyntheticDataGeneration
Weuseatwo-steppromptingonGPT-3.5-turboforobtainingahigh-qualitymulti-turnQAconversation. Instep-1,
wepromptGPT-3.5-turbotogenerateamulti-turnQAconversationbasedonagivendocument. However,wefindthatthe
generatedconversationlackspronounsorcommonnounstoreferbacktowordsorphrasesindialoguehistory. Hence,in
step-2,wepromptGPT-3.5-turbotorevisethegeneratedconversationbyaddingpronounsorcommonnouns. Forboth
stages,wegiveGPT-3.5-turbofew-shotexamplestoensureahigh-qualitygeneration.
Thein-contextfew-shotpromptinstep-1isasfollows(aimstogeneratemulti-turnQAconversation):
You are a helpful, pattern-following assistant.
role-user:
PersonA has access to information in the following passage:
###There are certain important aspects of college preparation. The student preparing
for college should define their goals and interests. The student should also plan
financially and academically. We have a checklist that helps students get ready
financially and academically.###
Generate a conversation of around four turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:
role-assistant:
PersonU: I need to know how to prepare for college.
PersonA: You should first define your goals and interest and be sure to know the
costs and how to plan financially and academically for college.
PersonU: Is there something I can use as a guide to get ready for it?
PersonA: you can use our checklist to get ready financially and academically.
PersonU: Any other tip?
PersonA: Be sure to check out the reasons to attend a College or Career schools.
PersonU: Ok, that sounds good. Can you give me more tips?
PersonA: Another thing to take into consideration is to explore your career options.
role-user:
PersonA has access to information in the following passage:
###There are several disability compensation benefits. One of the benefits is the
annual clothing allowance. This allowance covers the cost of purchasing new clothes
up to a limit of 200 dollars annually.###
Generate a conversation of around two turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
16ChatQA:BuildingGPT-4LevelConversationalQAModels
are follow-up or continuations of previous questions.
Conversation:
role-assistant:
PersonU: I heard about a disability benefit for clothing. What is it about?
PersonA: This is a disability compensation benefit known as an annual clothing
allowance.
PersonU: I am quite interested about such benefits.
PersonA: Would you like to find out if you are eligible?
role-user:
PersonA has access to information in the following passage:
###Vascepa (icosapent ethyl) is a prescription fish oil medication approved for
treating adults with extremely high triglycerides, defined as a triglyceride level
of 500 mg/dL or higher. While using this medicine, you may need frequent blood tests
at your doctor’s office. Vascepa is only part of a treatment program that may also
include diet, exercise, and weight control. Follow your doctor’s instructions very
closely. You should not use Vascepa if you are allergic to icosapent. To make sure
Vascepa is safe for you, tell your doctor if you have: 1 liver disease; 2 diabetes;
3 a thyroid disorder; 4 problems with your pancreas; 5 a bleeding or blood-clotting
disorder; 6 if you are allergic to fish or shellfish; or. 7 if you drink large
amounts of alcohol, Vascepa (icosapent) works in the liver and bloodstream to reduce
very low-density triglycerides.###
Generate a conversation of around three turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:
role-assistant:
PersonU: I heard about a medication called Vascepa. What is it used for?
PersonA: Vascepa is a prescription fish oil medication used for treating adults with
extremely high triglyceride levels.
PersonU: Are there any side effects or precautions I should be aware of?
PersonA: While using Vascepa, you may need frequent blood tests, and it’s important
to follow your doctor’s instructions closely. It’s important to talk to your doctor
about any allergies or existing conditions you have.
PersonU: Is it effective in lowering cholesterol levels?
PersonA: Vascepa is used together with a low-fat diet and other treatments to lower
total cholesterol in people with severe high cholesterol.
role-user:
PersonA has access to information in the following passage:
{Document}
Generate a conversation of around three turns between PersonA and PersonU where
PersonU needs information from PersonA that is in the passage. Use only information
found in the passage.
PersonU starts the conversation and only asks questions. Most of PersonU’s questions
are follow-up or continuations of previous questions.
Conversation:
17ChatQA:BuildingGPT-4LevelConversationalQAModels
role-assistant:
Thein-contextfew-shotpromptinstep-2isasfollows(aimstorevisethemulti-turnQAconversationbyaddingpronouns
andcommonnouns):
You are a helpful, pattern-following assistant.
role-user:
Given the following conversation between PersonU and PersonA:
###PersonU: I’ve heard about adobo sauce in Mexican cuisine. Can you tell me more
about it?
PersonA: Adobo sauce is a dark red sauce made from ground chiles, herbs, and
vinegar. It’s used as a marinade and cooking sauce in Mexican and Southwestern
cuisines.
PersonU: What are the main ingredients in adobo sauce?
PersonA: The main ingredients are typically chile peppers, garlic, and vinegar.
However, there can be variations in the recipe.
PersonU: How can I make adobo sauce at home?
PersonA: One way to make adobo sauce at home is by using smoked jalapeno peppers
called chipotles. You can stew the chipotles in a sauce with tomatoes, garlic,
vinegar, salt, and spices. It’s also possible to find canned adobo sauce in the
Hispanic foods section of many grocery stores.
PersonU: Are there any other uses for adobo sauce?
PersonA: Adobo sauce can be used as a condiment or as a marinade for various dishes.
It enhances the flavor of meats and can be used as a cooking and serving sauce.
Additionally, adobo is also used in a stew-like presentation of meat prepared in
this sauce.
PersonU: Is adobo sauce easy to preserve?
PersonA: Yes, you can preserve adobo sauce by freezing or canning it when you’re
done making it. This allows you to store it for up to a year and enjoy its flavors
whenever you want.###
Modify PersonU’s questions to use pronouns or common nouns to refer back to words
or phrases in PersonU’s previous questions or PersonA’s previous responses.
role-assistant:
PersonU: I’ve heard about adobo sauce in Mexican cuisine. Can you provide more
information about it?
PersonA: Adobo sauce is a dark red sauce made from ground chiles, herbs, and vinegar.
It’s used as a marinade and cooking sauce in Mexican and Southwestern cuisines.
PersonU: What are the main ingredients in the sauce?
PersonA: The main ingredients are typically chile peppers, garlic, and vinegar.
However, there can be variations in the recipe.
PersonU: How can I make the sauce at home?
PersonA: One way to make it at home is by using smoked jalapeno peppers called
chipotles. You can stew them in a sauce with tomatoes, garlic, vinegar, salt, and
spices. It’s also possible to find canned adobo sauce in the Hispanic foods section
of many grocery stores.
PersonU: Are there any other uses for it?
PersonA: It can be used as a condiment or as a marinade for various dishes. It
enhances the flavor of meats and can be used as a cooking and serving sauce.
Additionally, it is also used in a stew-like presentation of meat prepared in
this sauce.
PersonU: Is it easy to preserve?
PersonA: Yes, you can preserve it by freezing or canning it when you’re done
18ChatQA:BuildingGPT-4LevelConversationalQAModels
making it. This allows you to store it for up to a year and enjoy its flavors
whenever you want.
role-user:
Given the following conversation between PersonU and PersonA:
###PersonU: I’ve heard about a test called urinalysis. What is it used for?
PersonA: Urinalysis is a test that evaluates a sample of your urine. It is used to
detect and assess a wide range of disorders, such as urinary tract infection,
kidney disease, and diabetes. It examines the appearance, concentration, and content
of urine.
PersonU: Can it help diagnose bladder cancer?
PersonA: Yes, urinalysis can be used as a screening and diagnostic tool for
bladder cancer. One way to test for bladder cancer is to check for blood in the
urine, which is called hematuria. This can be done during a urinalysis by examining
the urine cells under a special microscope.
PersonU: What other conditions or diseases can be detected through urinalysis?
PersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections,
evaluate causes of kidney failure, screen for the progression of chronic conditions
like diabetes mellitus and high blood pressure, and it can be used in combination
with other tests to diagnose certain diseases. It is a versatile test that helps
detect abnormalities in the urinary tract.
PersonU: Where can I get a urinalysis done?
PersonA: Urinalysis can be performed in various healthcare settings, including
doctors’ offices, urgent care facilities, laboratories, and hospitals. You can
provide a urine sample in a specimen cup, and the sample can be analyzed on-site
or sent to a laboratory for testing.
PersonU: How much urine is typically needed for a urinalysis?
PersonA: Usually, only small amounts of urine, ranging from 30 to 60 ml, are required
for urinalysis testing. It’s a non-invasive and relatively simple test that provides
valuable information about your health.###
Modify PersonU’s questions to use pronouns or common nouns to refer back to words
or phrases in PersonU’s previous questions or PersonA’s previous responses.
role-assistant:
PersonU: I’ve heard about a test called urinalysis. What is it used for?
PersonA: Urinalysis is a test that evaluates a sample of your urine. It’s used to
detect and assess a wide range of disorders, such as urinary tract infection,
kidney disease, and diabetes. It examines the appearance, concentration, and
content of urine.
PersonU: Can it help diagnose bladder cancer?
PersonA: Yes, urinalysis can be used as a screening and diagnostic tool for bladder
cancer. One way to test for bladder cancer is to check for blood in the urine, which
is called hematuria. This can be done during a urinalysis by examining the urine
cells under a special microscope.
PersonU: What other conditions or diseases can be detected through this test?
PersonA: Urinalysis is commonly used to diagnose urinary tract or kidney infections,
evaluate causes of kidney failure, screen for the progression of chronic conditions
like diabetes mellitus and high blood pressure, and it can be used in combination
with other tests to diagnose certain diseases. It is a versatile test that helps
detect abnormalities in the urinary tract.
PersonU: Where can I go to get this test done?
PersonA: Urinalysis can be performed in various healthcare settings, including
doctors’ offices, urgent care facilities, laboratories, and hospitals. You can
provide a urine sample in a specimen cup, and the sample can be analyzed on-site
19ChatQA:BuildingGPT-4LevelConversationalQAModels
or sent to a laboratory for testing.
PersonU: How much urine is typically needed for the test?
PersonA: Usually, only small amounts of urine, ranging from 30 to 60 ml, are
required for urinalysis testing. It’s a non-invasive and relatively simple test
that provides valuable information about your health.
role-user:
Given the following conversation between PersonU and PersonA:
{multi-turn QA conversation}
Modify PersonU’s questions to use pronouns or common nouns to refer back to words
or phrases in PersonU’s previous questions or PersonA’s previous responses.
role-assistant:
B.MoreDetailsandResultsforRetrievalinConversationalQA
B.1.QueryRewritingPromptsforGPT-3.5-turbo
Thein-contextfew-shotqueryrewritingpromptweuseforGPT-3.5-turboisasfollows:
You are a helpful, pattern-following assistant.
role-user:
Given the following conversation between PersonU and PersonA:
PersonU: Hello, I would like to know what to do if I do not agree with any decision.
PersonA: disagree with our decision about your monthly income adjustment amounts?
PersonU: no. Where can I find my SHIP contact information?
PersonA: You can find your local SHIP contact information in the back of your
Medicare & You 2020 Handbook online.
PersonU: and how do they calculate the adjustments?
Instead of having this entire conversation, how can PersonU get what he or she is
looking for using a single question? Respond with that question.
role-assistant:
How is the calculation for adjustments made by SHIP determined?
role-user:
Given the following conversation between PersonU and PersonA:
PersonU: I need to know how to prepare for college.
PersonA: You should first define your goals and interest and be sure to know the
costs and how to plan financially and academically for college.
PersonU: Is there something I can use as a guide to get ready for it?
Instead of having this entire conversation, how can PersonU get what he or she is
looking for using a single question? Respond with that question.
role-assistant:
What resources or guides can I use to help me prepare for college?
role-user:
Given the following conversation between PersonU and PersonA:
20ChatQA:BuildingGPT-4LevelConversationalQAModels
{Dialogue History + Latest Question}
Instead of having this entire conversation, how can PersonU get what he or she is
looking for using a single question? Respond with that question.
role-assistant:
B.2.MoreResultsforRetrievalinConversationalQA
Average Doc2Dial QuAC QReCC TopiOCQA INSCIT
Models
top-1 top-5 top-1 top-5 top-1 top-5 top-1 top-5 top-5* top-20* top-5* top-20*
Dragon(w/dialoghistory) 46.29 73.09 43.33 75.61 56.8 82.86 46.17 81.96 57.68 78.80 27.49 46.22
Dragon+Rewrite(w/dialoghistory) 47.57 74.12 44.54 76.98 57.23 83.04 46.45 82.60 60.94 81.74 28.69 46.22
Dragon+Rewrite(w/singlequeryonly) 54.46 80.13 47.60 80.60 47.10 77.15 51.73 85.78 73.07 88.19 52.79 68.92
Dragon+Fine-tune(w/dialoghistory) 52.72 80.67 48.94 83.01 52.64 81.95 50.73 87.17 67.86 86.28 43.43 64.94
Dragon+Fine-tune+Rewrite(w/dialoghisotry) 53.17 80.84 49.30 84.64 55.04 83.23 51.23 87.99 60.50 81.03 49.80 67.33
Table9. Comprehensivemulti-turnretrievalresultsacrossfivedatasets.
InTable9,weshowcomprehensivecomparisonsbetweenfine-tuningandrewritingmethods.
Interestingly,wefindthatcomparedtoonlyusingtherewrittenqueryasinput(Dragon+Rewrite(w/singlequeryonly)),
givingadditionaldialoghistory(Dragon+Rewrite(w/dialoghistory))makestheaveragescoressignificantlydrop. This
isbecauseDragonisoriginallypretrainedonsingle-turnqueries,itwillnaturallyhavebettergeneralizationabilitywhen
asingle-turnrewrittenqueryinsteadofamulti-turnconversationisprovided. Andtherewrittenqueryalreadycontains
sufficientinformationfromthedialoghistory.
Inaddition,weobservethat“Dragon+Fine-tune”performsonparwith“Dragon+Fine-tune+Rewrite”. Inotherwords,
forthemulti-turnfine-tuningmethod,replacingoriginalquerywithrewrittenqueryasinputsyieldscomparableresults.
Thisisbecauserewrittenquerywillnotprovidemuchadditionalinformationforthemodelsincethedialoghistoryhas
alreadybeenprovided,anditmightevencausenegativeeffects(e.g.,resultsontheTopiOCQAdataset)sinceitmakesthe
wholeconversationinputnotnatural. Thisonceagaindemonstrateshoweffectivethefine-tuningmethodisinequippingthe
modelwiththecapabilitytocomprehendthemulti-turncontext.
C.ConversationalQABenchmarks
C.1.DataStatistics
Doc2Dial WeusethetestsetofDoc2Dialfortheevaluation. Itconsistsof719dialogswith3939user-agentturns.
QuAC WeusethevalidationsetofQuACfortheevaluationsinceitstestsetcannotbedirectlyobtained. Itsvalidation
set consists of 1000 dialogs with 7354 user-agent turns. Among these 7354 user-agent turns, there are 1486 (around
20.2%) unanswerable questions. For the combined evaluation of answerable and unanswerable questions, we set the
ground truth response for the unanswerable question as “Sorry. I cannot find the answer based on
the context.”,sameastheonewesetinourstage-2tuning. Forthefaircomparison,wereplacetheunanswerable
responsesinthebaselinemodels(i.e.,Llama2-SFT/Chat,GPT-3.5-turbo,GPT-4)withthesamesentence(Detailscanbe
foundinAppendixE).
QReCC WeusethetestsetofQReCCfortheevaluation. Itstestsetincludessomedialoguesamplessourcefromthe
QuACdataset. Toavoidoverlapwithotherbenchmarkdatasets,weremovethoseQuAC-sourcedsamples,resultingin2805
user-agentturns.
TopiOCQA WeusethevalidationsetofTopiOCQAsinceitstestsetisnotavailableyet. Itsvalidationsetconsistsof
205dialogswith2514user-agentturns. Eachquestionhasatotaloffourhuman-writtenanswers. Ithasunanswerable
cases. However,wedonotincludethisdatasetinunanswerablecaseevaluationbecauseweonlyfind46questionswithno
21ChatQA:BuildingGPT-4LevelConversationalQAModels
answerbeingfound,whichisnotsufficientenoughfortheunanswerablecaseevaluation. Giventhatthisdatasetusethe
wholeWikipediaastheretrievalcorpus,weleverageitstopicinformationformulti-turnretrievalsincewefocusedmoreon
conversationalQAoverdocumentswhichtypicallynarrowdowntoaspecifictopicordomain.
INSCIT WeusethevalidationsetofTopiOCQAsinceitstestsetisnotavailableyet. Itsvalidationsetconsistsof86
dialogueswith502user-agentturns. Eachquestionhasanaverageof1.9human-writtenanswers. Ithasunanswerable
response for a few questions. However, we do not include this dataset in unanswerable case evaluation because every
questionalwayshasatleastoneanswerableresponse,whichmeanseveryquestionisanswerable. INSCITalsousethe
wholeWikipediaastheretrievalcorpus. Hence,wealsoleverageitstopicinformationformulti-turnretrievalforthesame
reasonmentionedinTopiOCQA.
CoQA WeusethevalidationsetofCoQAsinceitstestsetcannotbedirectlyobtained. Itsvalidationsetconsistsof500
dialogueswith7983user-agentturns. Eachquestionhasatotaloffourhuman-writtenanswers. Ithasunanswerablecases.
However,wedonotincludethisdatasetinunanswerablecaseevaluationforthesamereasonasintheTopiOCQAdataset.
Weonlyfind13questionswithnoanswerbeingfound,whichisnotsufficientenoughfortheunanswerablecaseevaluation.
DoQA WeusethetestsetofDoQAfortheevaluation. Itstestsetconsistsof1200dialogueswith5394user-agentturns
acrossCooking,TravelandMoviedomains. Among5394user-agentturns,thereare1479(around27.4%)unanswerable
questions. WeusethesamestrategyastheonementionedinQuACdatasettoincorporateunanswerablesamplesintothe
evaluation.
ConvFinQA WeusethevalidationsetofCoQAsinceitstestsetcannotbedirectlyobtained. Itsvalidationsetconsistsof
421dialogueswith1490user-agentturns.
SQA WeusethetestsetofSQAfortheevaluation. Itstestsetconsistsof1025dialogueswith3100user-agentturns.
HybridDial WeusethetestsetofHybridDialfortheevaluation. Itstestsetconsistsof243dialogueswith1111user-agent
turns.
C.2.PromptsfortheBenchmarks
Weusethepromptformatmentionedin§A.2forbenchmarksinChatQA,Llama2-Chat,GPT-3.5-turboandGPT-4. We
keepthe{Context for Latest Question}thesameforallmodels,whileweadjustthe{Instruction}forthe
baselines(i.e.,Llama2-Chat,GPT-3.5-turboandGPT-4)toensuretheydonotgetsub-optimalresults. Weusedifferent
instructionsfortestbenchmarkswithdifferentanswertypes(e.g.,longanswer,shortanswer,arithmeticcalculation). The
{Instruction}forallmodelsonthebenchmarksareasfollows:
C.2.1.CHATQA
Wekeepthe{Instruction}consistentbetweenthedatablendsinstage-2andtestbenchmarksbasedondifferentanswer
types.
Weuse“Please give a full and complete answer for the question.” forDoQA,INSCIT,Hybrid-
Dial,Doc2Dial,QuAC,andQReCC,sincethesedatasetsgenerallyhavelonganswersforthequestions.
We use “Answer the following question with a short span, or a full and complete
answer.” forSQAandTopiOCQA,sincethesedatasetshavebothshortandlonganswersbasedonthequestions.
We use “Answer the following question with a short span. The answer needs to be
just in a few words.” forCoQA,sinceitgenerallyhasshortanswersforthequestions.
We use “Answer the following question with a number from context or the math
arithmetic using +, -, *, or /.” for ConvFinQA, since this dataset requires the model to either ex-
tractnumbersfromthecontextordoarithmeticcalculation. Wewillcalculatethenumberbasedonthearithmeticformula
themodelgeneratesandcompareitwiththegoldanswer.
22ChatQA:BuildingGPT-4LevelConversationalQAModels
C.2.2.LLAMA2-CHAT
NotethatwetriedoriginalLlama2-Chatprompttemplate10,whichgivesslightlyworseresultscomparedtousingtheonein
§A.2. Wehavetriedseveral{Instruction}forLlama2-Chat. Wefindtheonesbelowworksthebest.
We use “Please give an answer in just one sentence.” for DoQA, INSCIT, HybridDial, Doc2Dial,
QuAC,andQReCC,sincethesedatasetsgenerallyhavelonganswersbutwithinonesentence. Wenoticethatitisimportant
togiveLlama2-Chatspecificinstructionlike“onesentence”insteadof“fullandcompleteanswer”topreventthemodel
fromgeneratingverylonganswers.
We use “Answer the following question with a short span, or one sentence.” for Topi-
OCQA,sincethisdatasethasbothshortandlonganswersbasedonthequestions,andthelonganswersaregenerallywithin
onesentence.
Weuse“Answer the following questions with one or a list of entities.” forSQA,sincethe
answerforthisdatasetalwaysconsistsofoneoralistofentitiesfromthecontext.
We use “Answer the following question with a short span. The answer needs to be
just in a few words.” forCoQA,sinceitgenerallyhasshortanswersforthequestions.
We use “Answer the following question with just a number from context or just the
math arithmetic using +, -, *, or /.” for ConvFinQA, since this dataset requires the model to either
extractnumbersfromthecontextordoarithmeticcalculation. Weextractthearithmeticformulageneratedbythemodeland
useacalculatortogetitsfinalresult.
C.2.3.GPT-3.5-TURBO&GPT-4
Wehavetriedseveral{Instruction}forGPT-3.5-turboandGPT-4,wefindtheonesbelowworksthebest(appliedfor
bothGPT-3.5-turboandGPT-4).
We use “Please give an answer in just one sentence.” for DoQA, INSCIT, HybridDial, Doc2Dial,
QuAC,andQReCC,sincethesedatasetsgenerallyhavelonganswersbutwithinonesentence. SimilartoLlama2-Chat,we
alsofindOpenAImodelstendtogeneratequitelonganswersgiventheinstructionof“fullandcompleteanswer”. Hence,
wemaketheinstructionmorespecific(i.e.,“onesentence”)topreventthemodelfromgeneratingverylonganswers.
We use “Answer the following questions in JUST a few words or one sentence.” for Topi-
OCQA,sincethisdatasethasbothshortandlonganswersbasedonthequestions,andthelonganswersaregenerallywithin
onesentence.
We use “Answer the following questions with one or a list of entities. Do not give
a detailed explanation. Answer needs to be as short as possible.” forSQA.Wefindthat
OpenAImodelsoftengivedetailedexplanationforSQAdatasetunlessspecificallyinstructednotto.
We use “Answer the following question with a short span. The answer needs to be
just in a few words.” forCoQA,sinceitgenerallyhasshortanswersforthequestions.
We use “Answer the following questions with just a number from context or just the
math arithmetic using +, -, *, or /.” for ConvFinQA, since this dataset requires the model to either
extractnumbersfromthecontextordoarithmeticcalculation. Weextractthearithmeticformulageneratedbythemodeland
useacalculatortogetitsfinalresult.
D.HumanEvaluation
Weconducthumanevaluationacrossthetentestbenchmarkdatasets. Werandomlyselect60samplesforeachdataset,and
eachsampleislabelledbythreeannotators,whichresultsinatotalof1800annotations.
Weasktheannotatorstoverifythefactsinmodels’outputsanddeterminewhichmodelprovidesamoreaccurateresponse
tothequestion. WeuseAmazonMechanicalTurkplatformtoconductthehumanevaluation. Thedetailsofourhuman
evaluationinstructionsandtheinterfaceshowntoannotatorsareillustratedinFigure3.
10https://gpus.llm-utils.org/llama-2-prompt-template/
23ChatQA:BuildingGPT-4LevelConversationalQAModels
Figure3. Humanevaluationinstructionsandtheinterfaceforannotators.
E.UnanswerableCaseEvaluation
Forunanswerablecase,weconsiderthemodelindicatingthatthequestioncannotbeansweredascorrect. Foranswerable
cases,weconsiderthemodelnotindicatingthequestionisunanswerableascorrect(i.e.,themodelgivingananswer). We
findthatChatQAalwaysgenerates“Sorry. I cannot find the answer based on the context”when
itfindsthequestionisunanswerable,sinceweusethissentenceasaresponseforunanswerablequestionsinthestage-2
tuning. AsforLlama2-Chat,GPT-3.5-turbo,andGPT-4,theiroutputsforunanswerablequestionsusuallyfollowsome
specificpatterns. Below,welisttheheuristicmatchingpatternsderivedfromallgeneratedsamplesinDoQAandQuAC
datasetstodetermineifthemodelsuggeststhatthequestionisunanswerable. Whenwefindthegeneratedoutputcontains
thepatternlistedbelow,weinterpretitasanindicationthatthequestionisunanswerable,andwewillreplaceitsanswer
with“Sorry. I cannot find the answer based on the context.” fortheF1evaluation.
i’m not sure, cannot find, does not provide, cannot provide, cannot answer,
cannot be found, cannot be determined, don’t have information, do not have
information, couldn’t find, no information in the context, does not mention,
not explicitly mentioned, i don’t have any, i do not have any, does not
specify, doesn’t provide, not able to, unable to, doesn’t specify, there is
no information, there is no mention, not mentioned, i don’t have enough
information, there is no specific information, there is no specific mention,
no information found, I don’t have that information
F.CaseStudy
MoreexamplesonChatQA-13B,ChatQA-70B,GPT-3.5-turbo,andGPT-4canbefoundinTable10.
24ChatQA:BuildingGPT-4LevelConversationalQAModels
Context Conversation AssistantResponses
User:whatracersareshown?
Pos Rider Time/Retired
1 LorisCapirossi 38:04.730 Assistant:LorisCapirossi,ValentinoRossi,
2 ValentinoRossi +0.180 JeremyMcWilliams,TohruUkawa,...,
3 JeremyMcWilliams +0.534 HenkVanDeLagemaat,MauriceBolwerk,
4 TohruUkawa +0.537 RobertoRolfo,MarcellinoLucchi.
5 ShinyaNakano +0.742 ChatQA-13B:LorisCapirossi
6 RalfWaldmann +7.019 User:ofthose,whattimeswererecorded?
7 FrancoBattaini +20.889 ChatQA-70B:+1Lap
8 StefanoPerugini +20.891 Assistant:38:04.730,+0.180,+0.534,
9 JasonVincent +21.310 +0.537,...,+1:40.635,+1Lap,Retirement. GPT-3.5-turbo:38:04.730
10 AnthonyWest +26.816
11 AlexHofmann +26.933 User:ofthosetimes,whichwerenot GPT-4:38:04.730
... actualtimes?
23 ArnoVisscher +1:40.635 GroundTruth:+1Lap
24 HenkVanDeLagemaat +1Lap Assistant:+1Lap,Retirement
Ret MauriceBolwerk Retirement
Ret AndreRomein Retirement User:whichofwasanactualfinish?
Ret RobertoRolfo Retirement
Ret MarcellinoLucchi Retirement Assistant:
User:whatwastheaveragebacklogin2015?
2014comparedto2013mst2019snetsales
decreased$305million,or3%in2014as
Assistant:17400
comparedto2013...spacesystems2019
operatingresultsincludedthefollowing
User:whatwasitin2014?
(inmillions):
ChatQA-13B:17400+20300+21400
netsales backlogatyear-end Assistant:20300
2015 $9105 $17400 ChatQA-70B:17400+20300+21400
2014 $9202 $20300 User:whatisthesumofthoseyears?
2013 $9288 $21400 GPT-3.5-turbo:9288+9202+9105
Assistant:17400+20300
2015comparedto2014spacesystems2019
GPT-4:17400+20300+21400
netsalesin2015decreased$97million,or
User:whatwastheaveragebacklogin2013?
1%,comparedto2014.thesedecreaseswere
GroundTruth:17400+20300+21400
partiallyoffsetbyhighernetsalesof
Assistant:21400
approximately$235millionforbusinesses
acquiredin2014;andapproximately$75
User:whatisthetotalsumforall3years?
millionfortheorionprogramdueto
increasedvolume.
Assistant:
ChatQA-13B:Thereare8Gymnastics
participatedinthechampionship.
ChatQA-70B:Thereare8gymnasts*
TheXVIWorldRhythmicGymnastics
User:WhatistheXVIWorldRhythmic participatedinthechampionship.
ChampionshipswereheldinBrussels,
GymnasticsChampionships?
Belgium,onNovember20-22,1992.
GPT-3.5-turbo:Therewereatotalof8
Rank Gymnast Country Point Assistant:TheXVIWorldRhythmic gymnasts*whoparticipatedin
1 OxanaKostina Russia 9.775 GymnasticsChampionshipswereheldin thischampionship.
2 MariaPetrova Bulgaria 9.700 Brussels,Belgium,onNovember
3 DianaPopova Bulgaria 9.625 20-22,1992 GPT-4:Thecontextdoesnotprovide
4 CarmenAcedo Spain 9.625 thetotalnumberofgymnastswho
5 IrinaDeleanu Romania 9.550 User:HowmanyGymnasticsparticipated participatedintheXVIWorld
6 EkaterinaS. Ukraine 9.525 thischampionship? RhythmicGymnasticsChampionships.
7 ViktoriyaYani Ukraine 9.475
8 MariaSansaridou Greece 9.425 Assistant: GroundTruth:Thereare8gymnasts
participatedthischampionship.
*BothmodelscorrecttheUser’stypoof
“Gymnastics”to“gymnasts”
25ChatQA:BuildingGPT-4LevelConversationalQAModels
Context Conversation AssistantResponses
The3000metressteeplechaseor3000-meter
User:Hello,canyoutellmeaboutthe3000
steeplechaseisthemostcommondistanceforthe
metressteeplechase?
steeplechaseintrackandfield.Itisanobstacle
raceoverthedistanceofthe3000metres,which
Assistant:The3000metressteeplechaseisthe
derivesitsnamefromthehorseracingsteeplechase. ChatQA-13B:HeraninBerlin,Adelaide,andStockholm.
mostcommondistanceforthesteeplechasein
Year Athlete Location trackandfieldthatgoesoverthedistanceof
ChatQA-70B:KerryO’BrienraninAdelaideandBerlin.
1968 JoukoKuha(FIN) Stockholm the3000metres,whichderivesitsnamefrom
1969 VladimirasDudinas(URS) Kiev thehorseracingsteeplechase. GPT-3.5-turbo:KerryO’BrienraninBerlin,Adelaide,
1970 KerryO’Brien(AUS) Berlin andChampaign.
1971 KerryO’Brien(AUS) Adelaide User:Canyoutellmethementhatarepartofit?
1972 AndersGa¨rderud(SWE) Helsinki GPT-4:KerryO’BrienraninBerlinandAdelaide.
1973 BenJipcho(KEN) Helsinki Assistant:SomeofthemenareHenryRonoand
1974 AndersGa¨rderud(SWE) Helsinki MichaelKarst. GroundTruth:HeranforbothBerlinandAdelaide.
1975 AndersGa¨rderud(SWE) Stockholm
1976 AndersGa¨rderud(SWE) Montre´al User:WhichlocationsdidKerryO’Brienranfor?
1977 MichaelKarst(FRG) Stockholm
... Assistant:
ChatQA-13B:Thegroundsforrefusalofthatvisaare
strictlylimited.Theseprovisionsarealsodependent
title:Non-EUfamilymembertravelingtoSchengen
User:CanItravelfromRomaniatotheSchengen onyourtravelingwithorjoiningyourspouse.In
countries.source:Youcantravelvisa-freeonlyifyou
areavisa-free? caseyouarenot,you’llneedaregularSchengenvisa.
havea“residence”cardofafamilymemberofaunion
citizen”issuedunderarticle10ofthefreemovement
Assistant:Youcantravelvisa-freeonlyifyou ChatQA-70B:Sorry.Icannotfindtheanswerbasedon
directive(orifyouareacitizenofacountrywhose
havea“residencecardofafamilymemberofa thecontext.
nationalsdon’trequirevisasintheSchengenarea).A
unioncitizen”issuedunderarticle10ofthefree
residencecardissuedbyRomaniatothespouseofa
movementdirective GPT-3.5-turbo:Toobtainanarticle10card,youwould
Romaniancitizenisnotnecessarilyanarticle10card.
needtomeettherequirementssetoutinthefree
Ifyoudonothaveanarticle10card,EUcountries
User:Ihaveatemporaryresidencecardalso,can movementdirectiveandapplyforitthroughthe
otherthanRomaniamustissueyouavisafreeofcharge
wetraveltogethervisa-free? appropriatechannelsinRomania.
andonthebasisofanexpeditedprocedure.Thegrounds
forrefusalofthatvisaarestrictlylimited.These
Assistant:Ifyoudonothaveanarticle10card,EU GPT-4:Thetextdoesnotprovideinformationonhow
provisionsarealsodependentonyourtravelingwithor
countriesotherthanRomaniamustissueyouavisa toobtainanarticle10card.*
joiningyourspouse.Incaseyouarenot,you’llneeda
freeofchargeandonthebasisofanexpedited
regularSchengenvisa.AsRelaxedpointedoutina
procedure. GroundTruth:Sorry.Icannotfindtheanswerbased
comment,itappearsthatyourconcernstemsinpart
onthecontext.
fromthefactthatRomaniaisnotyetafullparticipant
User:HowdoIgetaarticle10card?
intheSchengenagreement.Theserulesapplytoall
*Intheevaluation,wewilldetectthisunanswerable
EUcountries,notjustSchengencountries,because
Assistant: patternandreplaceitwith“Sorry.Icannotfindthe
theyarebasedontheEUrightoffreedomofmovement.
answerbasedonthecontext”tomatchwiththe
groundtruth.
title:Bengalfox.“Vulpesbengalensis”isarelatively
ChatQA-13B:Theearshavethesamecolourasthe
smallfoxwithanelongatedmuzzle,long,pointed User:whatisredfox?
napeormaybedarker,butnothavingadarkpatch
ears,andabushytailabout50to60%ofthelength
asin“V.vulpes”.Extensivevariationincoatcolour
oftheheadandbody.Itsdorsalpelageisveryvariable, Assistant:Itisthelargestofthetruefoxesand
existsacrosspopulationsandseasonallywithin
butmostlygrayishandpalerventrally;itslegstend oneofthemostwidelydistributedmembersof
populations,butgenerallyvariesfromgreytopale
tobebrownishorrufous.Itismoredaintilybuiltthan theorderCarnivora.
brown.
“Vulpesvulpes”.Thetailisbushywithaprominent
blacktipwhichdistinguishesitfrom”V.vulpes”.Back User:wherediditoriginate?
ChatQA-70B:“Vulpesbengalensis”isarelatively
ofearsaredarkbrownwithblackmargin.Itsrhinarium
smallfoxwithanelongatedmuzzle,long,pointed
isnakedandthelipsareblack,withsmallblackhair Assistant:ThespeciesisEurasianinorigin.
ears,andabushytailabout50to60%ofthelength
patchesonupperpartofnuzzle(shaft)infrontofeyes.
oftheheadandbody.
User:howdiditevolve?
title:Redfox.Theredfoxisconsideredamore
GPT-3.5-turbo:I’msorry,butIcouldn’tfind
specialisedformof”Vulpes”thantheAfghan,corsac Assistant:Itisconsideredamorespecialisedform
anyinformationaboutathirdtypeoffoxinthe
andBengalfoxesinthedirectionofsizeandadaptation of“Vulpes”thantheAfghan,corsacandBengal
givencontext.
tocarnivory;theskulldisplaysfarfewerneotenous foxesinthedirectionofsizeandadaptation
traitsthaninotherspecies... tocarnivory.
GPT-4:Thecontextdoesnotprovideinformationon
athirdtypeoffox.
title:Bengalfox.Theearshavethesamecolourasthe User:whatistheappearanceofthisthirdtype
napeormaybedarker,butnothavingadarkpatchasin offox?
GroundTruth:Itisarelativelysmallfoxwithan
“V.vulpes”.Extensivevariationincoatcolourexists
elongatedmuzzle,long,pointedears,andabushytail
acrosspopulationsandseasonallywithinpopulations, Assistant:
about50to60%ofthelengthoftheheadandbody.
butgenerallyvariesfromgreytopalebrown...
Table10.CasestudyforChatQA-13B,ChatQA-70B,GPT-3.5-turbo,andGPT-4.Weusebluecolortohighlighttherelevantcontextfor
thelastuserquestion.Andweusegreenandredcolortohighlightthecorrectandincorrectpartsintheassistantresponses.
26ChatQA:BuildingGPT-4LevelConversationalQAModels
G.GuidelinesforConversationalQADataCollection
In this section, we demonstrate the guidelines we provide for the conversational QA data collection. We request the
annotatorstoactasbothuserandagenttoconstructtheconversationalQAsamplebasedonthegivendocument,which
makestheannotationmoreefficient. Theguidelinesconsistsofthreeparts: 1)WhatdoesconversationalQAsampleslook
like;2)WhatkindsofconversationalQAsamplesweneed;and3)Whatweneedtoannotate.
G.1.WhatdoesconversationalQAsampleslooklike
OneconversationalQAsampleisbasedonadocumentthatweprovide. Theuserandagentbehaviorsareasfollows:
• Userbehavior: 1)Askquestionstoagentbasedonthegivendocument;2)Answerquestionsfromagentwhenagent
wantstoclarifysomething.
• Agentbehavior: 1)Answerquestionsfromuserbasedonthedocument;2)Askquestionstotheuserwhentheuser’s
questionisnotclear,ortoogeneral/broad.
G.2.Whatkindsofmulti-turnQAsamplesweneed
Welisttherequirementsforuser’squestionsandagent’sresponsesbelow.
User’sQuestions
• User’squestionscanrefertohis/herprevious(ormultipleturnsbefore)question.
• User’squestionscanalsorefertotheprevious(ormultipleturnsbefore)answerfromtheagent.
• Trytousepronounsorcommonnounstoreplacetheentitiesmentionedbefore.
• Trytomaketheuser’squestionsdiverse. Forthesametypeofquestions,trytousedifferentwaystoexpressitin
differentdialogueannotations.
• Otheruserquestiontypesweneed
– Giventheanswerfromagent,askforextrainformation(e.g.,Whatelse...;anyother...;istheremore...).
– Switchthetopicandstartinganewthreadinconversation.
– Asktwoquestionsatthesametime.
Agent’sResponse
• Trytomaketheagent’sanswerwithin1-2sentences. Iftheanswerhastobelong,trytomakeitconcise.
• Trynottodirectlycopythewholerelevantcontextfromthedocumentastheanswer. Instead,trytoconstructthe
answerbyparaphrasingthechosencontext.
• Trytocomeupwithasmallproportionofcaseswhereagentasksquestionstousertoclarifysomething. Specifically,
whentheuser’squestionistoobroadornotclear,theagentneedstonarrowdownthescopebyaskingclarification
questionstoseewhichspecificaspectsthattheusercaresmoreabout.
G.3.Whatweneedtoannotate
Welistwhatweneedtoannotateforeachconversationbelow.
• Foreachdocument,weneedtoannotatetheuser’squestionsandcorrespondingagent’sresponses. Theaveragenumber
ofuser-agentturnsperconversationneedstobearoundfive.
• Foreachuser’squestion,weneedtoannotatealltherelevantcontextwithinthedocument.
27