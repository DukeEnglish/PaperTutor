RAP-SAM : Towards Real-Time All-Purpose Segment Anything
ShilinXu1,4* HaoboYuan2 QingyuShi1 LuQi3 JingboWang4 YiboYang5
YiningLi4 KaiChen4 YunhaiTong1 BernardGhanem5 XiangtaiLi2,4 † Ming-HsuanYang3,6
1PekingUniversity 2NanyangTechnologicalUniversity 3UC,Merced, 4ShanghaiAILaboratory
5KAUST 6GoogleResearch
ProjectPage: https://xushilin1.github.io/rap_sam/
Abstract
Advanced by transformer architecture, vision founda-
tionmodels(VFMs)achieveremarkableprogressinperfor-
manceandgeneralizationability. SegmentAnythingModel
(SAM) is one remarkable model that can achieve general-
izedsegmentation.However,mostVFMscannotruninreal-
time, which makes it difficult to transfer them into several
products. On the other hand, current real-time segmenta-
tion mainly has one purpose, such as semantic segmenta-
tion on the driving scene. We argue that diverse outputs
areneededforrealapplications. Thus,thisworkexploresa
newreal-timesegmentationsetting,namedall-purposeseg-
mentation in real-time, to transfer VFMs in real-time de-
ployment. It contains three different tasks, including in-
teractive segmentation, panoptic segmentation, and video
segmentation. We aim to use one model to achieve the
abovetasksinreal-time. Wefirstbenchmarkseveralstrong
baselines. Then, we present Real-Time All Purpose SAM
(RAP-SAM). It contains an efficient encoder and an effi-
cient decoupled decoder to perform prompt-driven decod-
ing. Moreover, wefurtherexploredifferenttrainingstrate-
gies and tuning methods to boost co-training performance
further. Our code and model are available at https:
//github.com/xushilin1/RAP-SAM/.
Figure 1. We present real-time all-purpose segmentation to seg-
ment and recognize objects for image, video, and interactive in-
1.Introduction puts. Inadditiontobenchmarking,wealsoproposeasimpleyet
effectivebaseline,namedRAP-SAM,whichachievesthebestac-
Recent advancements in computer vision, propelled by curacyandspeedtrade-offamongthreedifferenttasks.Bothreal-
transformer architectures [2, 13, 75], focus on develop- timepanopticsegmentationandvideoinstancesegmentationare
ing a singular, large-scale model versatile enough for var- shownatthebottom.
ious tasks, including detection and segmentation, with a
general-purpose design. Specifically, several studies [74,
80] adopting identical architectures have demonstrated su-
sively for specific tasks. Furthermore, numerous stud-
perior performance compared to models designed exclu-
ies [30, 34] investigate the foundation model design us-
ing extensive data. A notable example, the Segment Any-
*TheworkisdoneduringtheinternshipatShanghaiAILaboratory.
Thefirsttwoauthorsshareequaltechnicalcontributions.†ProjectLeader. thing Model (SAM) [34], introduces an interactive model
1
4202
naJ
81
]VC.sc[
1v82201.1042:viXradriven by visual prompts for versatile segmentation. Con- queries.Viaextensiveexperiments,weadoptpooling-based
currently,forsemantic-levelsegmentation,numerousstud- dynamicconvolutiontoreplaceper-pixelcross-attentionto
ies [10, 29, 46, 47, 93] employing a universal design have achievebetteraccuracyandspeedtrade-off. Tobalancein-
surpassed previous models tailored for specific image and teractiveandsemantic-levelsegmentation,wepresentanef-
videosegmentationtasks. fectivedualadapterdesignthatbetteradaptssharedknowl-
However, the majority of these studies face challenges edgeofthesamedecoder. Finally,wetermourbestmodel
in achieving real-time performance or compatibility with asReal-timeAll-PurposeSAM(RAP-SAM).Tothebestof
mobile devices, primarily due to their heavy encoders and ourknowledge,itisthefirstreal-timeall-purposesegmen-
cascaded decoders. Thisconstraint significantly limits the tationmodel.Moreover,duetothesemanticlevelandinter-
applicability of these advanced architectures in practical activelevelsegmentationability,ourmodelcanalsocreate
vision applications, although they can achieve high per- morenewapplications,suchasinteractivevideosegmenta-
formance. On the contrary, various studies explore real- tion. Extensive experiments show the effectiveness of our
time segmentation using distinctive designs. Yet, these design. Themaincontributionsofthisworkare:
efforts predominantly concentrate on singular application • Weintroduceall-purposesegmentation,amulti-taskseg-
purposes,includingautonomousdrivingscenes,theCOCO mentationthataimstosegmentobjectsforimage,video,
dataset for panoptic segmentation (PS) [50], and real-time andinteractiveinputsinreal-time.
video instance/object segmentation (VIS) [81]. Currently, • We benchmark several real-time transformer-based seg-
no studies investigate real-time, all-purpose segmentation, mentationapproachesforthenewsettings.
wherein a model is capable of performing universal seg- • WepresentasimpleyetfastbaselinenamedRAP-SAM.
mentation tasks, encompassing image segmentation, video It contains a lightweight feature extractor, a unified de-
instance/videosegmentation,andSAM-likeinteractiveseg- coder,andtwoasymmetricadapters.
mentation. • Extensive experiments demonstrate that RAP-SAM
achieves the best speed and accuracy trade-off in the
Inspiredbytheaforementionedchallenges,thisstudyin-
proposed benchmark and regular real-time semantic and
vestigatesthenovelissueofreal-time,all-purposesegmen-
panopticsegmentationbenchmarks. Wealsoshowscala-
tation. Weposeacriticalinquiry: giventhelimitedcompu-
bilityacrossdatasetsandapplicationdemos.
tationalresourcesandmodelcapacity,howcanwedevelop
an efficient, all-purpose segmentation model? This entails
2.RelatedWork
creating a single model capable of segmenting, tracking,
and classifying each pixel in real-time, akin to performing
Universal Segmentation. Prior research has focused on
interactivesegmentationsimilartoSAM.
designing segmentation models tailored for specific tasks.
Initially, wedefine this researchproblem by integrating Recent developments in this field [2, 10, 13, 17, 39, 42,
existing benchmarks. Specifically, we use the COCO [50] 43,45,49,52,85–87,93]haveshiftedtowardsemploying
and YouTube-VIS 2019 [81] datasets for joint co-training, mask classification architectures, coupled with an end-to-
employing identical hyper-parameters. Beyond the se- endsetpredictionobjective,tofacilitateuniversalsegmen-
mantic queries used for semantic-level segmentation in tation. This approach has outperformed specialized mod-
prior research, our approach additionally incorporates vi- els[4,6,7,20,27,33,40,41,48,88,96]acrossvariousseg-
sual prompt queries in SAM for interactive segmentation. mentation tasks, including those involving images, videos,
However, there are still several challenges: 1), No previ- and point clouds [9, 32, 46, 47, 67, 78, 79]. Specifically,
ous works explore joint co-training for three different pur- Mask2Former[10]usesamasked-attentionmechanismand
poses.Thus,extensiveexperimentsareneededtosearchfor surpassestheperformanceofpriorspecificimagesegmen-
abettermeta-architecture. 2),Interactivesegmentationand tation models. Similarly, Tube-Link [46] outperforms ear-
semantic-level segmentation need better strategies to bal- liermodels[40,47,51]tailoredforspecificvideosegmenta-
ance each other. 3), Under the real-time setting, several tiontasks. Semantic-SAM[35],builtonMask-DINO[36],
previousarchitectures[42,80]formulti-tasksegmentation employs a hybrid query design to facilitate both panop-
maynotbepractical. ticandinteractivesegmentationwithinasingleframework.
To address these challenges, we benchmark several re- However, despite the strong performance of these works,
centworks[10,25,85]inareal-timesetting. Then,weini- their complexity, stemming from the heavier encoder and
tially investigate a range of meta-architectures suitable for the cascaded decoder designs, poses significant challenges
all-purposesegmentation. Unlikepreviousworks,weavoid forintegrationintoactualproducts.Theseareprimarilyhin-
cascaded architecture and only use the pyramid feature deredbyslowprocessingspeedsandsubstantialparameter
once when performing dynamic convolution. We explore overhead.
four different architectures in the case of various decoder Vision Foundation Model. Recent advancements in the
designs, where we adopt a single decoder with multiple vision research community have been propelled by the
2development of Vision Foundation Models (VFMs), in- Table1. Real-TimeSegmentationSettings. Ourall-purposeseg-
cluding pure-visual pre-training [19], vision-language pre- mentationsupportsmoretasksinoneframework.
training[15,37],andmulti-modalmodelsemployingvisual
prompting[1,75]. SAM[34]standsasaseminalworktar- Property ImageSeg VideoSeg SAM-Like Ours
ImageMasks ✓ ✗ ✓ ✓
geting general interactive segmentation. Subsequent stud-
VideoMasks ✗ ✓ ✗ ✓
ies[3,12,77]haveadapted SAMfordiverseapplications, Interactive ✗ ✗ ✓ ✓
predominantly employing it as a segmentation tool. How- SemanticLabels ✓ ✓ ✗ ✓
ever,theseVisionFoundationModelstypicallyemploysub- Multitasks ✗ ✗ ✗ ✓
stantial visual encoders, like ViT-base [13], which poses
challenges for deployment on real-time or mobile devices
can achieve more flexible input and output, which is more
duetotheirsize.
challenging. Combined with interactive segmentation and
EfficientModelDesign. Thisresearchdirectionprimarily
video segmentation, we can also achieve interactive video
concentratesonthedevelopmentofefficientCNNs[18,23,
segmentationasvideoobjectsegmentation.
24, 28, 56, 66, 94], transformers [57, 58, 64], and hybrid
Datasets. For image segmentation and interactive seg-
architectures[8,38,55,90,100],aimedatadvancingvisual
mentation, we use the well-known COCO dataset [50] for
representation learning. Most research investigates back-
benchmarking. WeadoptpartiallySAMdata[34]fortest-
bone design within the constraints of real-time processing
ing. Due to the limited resources and limited model ca-
and parameter efficiency. These studies are orthogonal to
pacity in real-time settings, we do not use SAM data for
ourown,whichprimarilyexplorestheimpactofdata,task,
training. We take COCO data as SAM data for training
and decoder design. Additionally, our work employs effi-
and testing, where the boxes and random points sampled
cientmodelsasencodersandpresentsdetailedbenchmarks
from masks are used for visual prompting. For video seg-
andanalysesoftheencoder’seffects.
mentation, we adopt the widely used YouTube-VIS 2019
Efficient Segmentation. Previous studies [22, 25, 44, 59,
dataset [81] for training. We do not use the SAM data for
71, 83, 84, 95] on efficient segmentation have predomi-
our benchmarking. There are several reasons: (1), SAM
nantlyconcentratedonclosed-setandspecificdomains. In
data is not well annotated as the COCO dataset. (2), We
particular,asignificantportionofthisresearch[25,44,60]
only focus on object-level and scene-level segmentation,
is dedicated to driving scenarios. Recently, various stud-
while SAM data contain more granularity. (3), SAM data
ies[71,92,99]havedevelopedefficientsegmentationtech-
has no semantics, while our goal is to predict mask labels
niques that facilitate model execution on mobile devices.
forrealapplications.
Mobile SAM [89] introduces a streamlined encoder distil-
lation method. Fast SAM [97] employs a single-stage in-
4.ProposedMethod
stancesegmentationframeworkthatdirectlydecodesclass-
agnosticmasks. Recently, multiplestudieshavebeencon-
Motivation. Employingthesameformulation, wefirstre-
ducted on efficient panoptic segmentation [25, 63, 69] and
visitthreetasks:panopticsegmentation,videoinstanceseg-
rapid video instance segmentation [82, 91]. However,
mentation, and interactive segmentation. When combined
these real-time segmentation methods are limited to spe-
with a query-based mask transformer, object queries can
cific tasks. We contend that an all-purpose model capable
representallentitieswithinthescene,encompassingimage-
ofreal-timeperformancewouldhavewide-rangingapplica-
levelobjects,video-leveltrackedobjects,andprompt-based
tions in editing, tracking, and segmentation functionalities
specificobjects. Eachtask’sformulationisasfollows:
withinvariousproducts.
For panoptic segmentation [33], given an input image
I ∈RH×W×3,theobjectiveistogenerateagroupofmasks
3.ProblemSetup
{y }N ={(m ,c )}N wherec denotesthegroundtruth
i i=1 i i i=1 i
Settings. Ourproposedreal-timeall-purposesegmentation class label of the binary mask m i and N is the number of
containsthreevisualinputs,includingimage,video,andvi- masks,H ×W indicatesthespatialdimensions.
sual prompts (boxes and points). It outputs corresponding Forvideoinstancesegmentation[81],givenavideoclip
masks,labels,andinstanceID.Weadoptpanopticsegmen- input V ∈ RT×H×W×3, where T denotes the number of
tation for image segmentation, which unifies semantic and frames,theprocessaimstogenerateaninstancemasktube
instance segmentation. For video segmentation, we adopt {y i}N
i=1
={(m i,c i,d i)}N i=1,whereN representsthenum-
videoinstancesegmentation,whereweaimtosegmentand berofthetubemasksm ∈{0,1}T×H×W,c indicatesthe
i i
track each foreground instance. For interactive segmenta- classlabelofthetubem ,andd identifiestheinstanceID
i i
tion, we follow the SAM-like setting, where we perform foreachtube.
class-agnostic segmentation according to the user’s inputs. ForSAM-likeinteractivesegmentation[34],boththeim-
Compared with previous settings, as shown in Tab. 1, we age I and visual prompts P ∈ RK×4 (box prompts are
3object / prompt quries
Visual Prompts Three-Stage Decoder
prompt quries prompt quries
IoU add & norm
Prompt Prompt
Encoder Adaptor Mask FFN
F
add & norm
Object
Mask
Adaptor self-attention
Imageor Video object quries object quries Class
add & norm
Linear project prompt query(!
Backbone Lite Neck F Innerproduct object query )! coD ny vn oa lum tii oc
n
Prompt Adaptor *)*&")+
VI im dea og e
F
eF ae ta ut ru er e
M
M apap
!
$! !%!" :# ': ×# #× ×% %× ×&
&
Object Adaptor *&'(
feature map object / prompt quries
Figure2.RAPSAMoverview.Ourmethodcontainsthreevisualinputs:image,video,andvisualprompts.Utilizingpositionalencoding,
wegeneratepromptqueriesfromthesevisualprompts.Thelearnableobjectqueries,alongsidethepromptqueriesandthefeaturemapF,
aredirectedtothemulti-stagedecoder.Thisprocessgeneratesmulti-stagepredictionsandrefinedqueries.Theserefinedqueriesengagein
cross-attentionwithF,resultinginthefinalprediction.
Table 2. Comparison of Segmentation Methods. Our proposed 4.1.RAP-SAMArchitecture
RAP-SAMsupportsvarioussegmentationtasks,anditcanrunin
realtime. OverallArchitecture. OurRAP-SAMisasimpleencoder
and decoder architecture. As shown in Fig. 2, it contains
Methods SS PS VIS Interactive Multi-TaskinOneModel RealTime a backbone, a lightweight neck, and a shared multitask
ICNet[95] ✓ ✗ ✗ ✗ ✗ ✓ decoder. Visual prompts P are also the input of the de-
Bi-Seg[84] ✓ ✗ ✗ ✗ ✗ ✓
YOSO[25] ✓ ✓ ✗ ✗ ✗ ✓ coder. FollowingSAM,wealsoadoptthepromptencoder
Mobilie-VIS[91] ✗ ✗ ✓ ✗ ✗ ✓
toencodevisualpromptsintoaquery. Weadoptthesame
SAM[34] ✗ ✗ ✗ ✓ ✗ ✗
Mask2Former[10] ✓ ✓ ✗ ✗ ✗ ✗ decoder for both visual prompts and initial object queries
VideoK-Net[47] ✗ ✓ ✓ ✗ ✗ ✗
OneFormer[29] ✓ ✓ ✗ ✗ ✓ ✗ to share more computation and parameters. However, the
RAP-SAM(Ours) ✓ ✓ ✓ ✓ ✓ ✓ goalsofbotharedifferent. Theformerpaysmoreattention
tothelocaldetails,whilethelatteralsoconsidersthescene
andtemporalfeatures. Tobetterbalancetheresultsforin-
teractive segmentation and image/video segmentation, we
used here) are taken as inputs. These prompts, which in- designapromptadapterandanobjectadapterintheendof
cludepointsandboxes,leadtotheoutputsofcorresponding thedecoder.
binary image masks {y i}K i=1 = {m i ∈ {0,1}H×W}K i=1, Lite Feature Extractor. Due to computation cost limi-
where K denotes the number of visual prompts. Each vi- tation, we avoid the design of previous methods, such as
sual prompt is encoded into one object query, which nat- a large backbone and heavier transformer encoder. We
urally can be the input of the decoder, as implemented in explore lightweight backbones, including ResNet18 [21],
[10,34]. STDC-v1 [14], and SeaFormer [71]. We adopt feature
pyramid networks with deformable convolutions to fuse
Single Object Query For All Segmentation. As men-
multi-scalefeaturesandobtainmorealignedfeaturerepre-
tioned above, by combining all three settings into one, we
sentation, motivated by previous works [44] on real-time
canrepresentalltheoutputsegmentationentitiesusingthe
semantic segmentation. For video input, we extract the
same query-based mask classification framework. In par-
spatial-temporal features. For simplicity, we use F ∈
ticular, oneobjectquerycorrespondstoonemaskm , one img
labelc ,andoneIDd .Weaimtodesignasimple,effii cient, RH 4×W 4 ×d for image inputs and F vid ∈ RT×H 4×W 4 ×d for
i i
videoinputs.
andparameter-sharingframeworktosupportthesetasks.
Unified Dynamic Convolution Decoder. As in previous
Compared Baselines. In Sec. 5.1, we benchmark several works,thegoalofthedecoderistorefinetheobjectquery.
baselines for our all-purpose segmentation. We use the However,manyoftheseapproachesareimpracticalforreal-
prompt encoder (proposed in SAM [34]) to encode the vi- timesettingsduetotheirrelianceonheavilycascadedlay-
sual prompt and jointly learn with object query for both ersandpixel-wisecross-attentionmechanisms. Incontrast,
PS and VIS on a shared decoder. Although we adopt the ourmethodemploysapooling-baseddynamicconvolution
lightweight design to speed up, their performance is still framework[47,93]toenhancethedecoder’sefficiency.
limitedornotbalanced,asshowninTab3. GivenobjectqueryQ ∈ RN×d andpromptqueryP ∈
i i
4
esab-gnilooP
noitulovnoc
cimanyd
esab-gnilooP
noitulovnoc
cimanyd
esab-gnilooP
noitulovnoc
cimanydRK×d, the initial masks M are obtained by dot product whereMHSAmeansMultiHeadSelfAttention,FFNisthe
i
withF orF . idenotestheindexofthedecoderstage. FeedForwardNetworkcommonlyusedinthecurrentver-
img vid
According to the inputs, the masks can be image masks sionofTransformers[2,13]. Finally,therefinedmasksare
for panoptic segmentation M ipan ∈ RN×H 4×W 4 and inter- obtainedviadotproductbetweentherefinedqueriesQp ian,
active segmentation M iiter ∈ RK×H 4×W 4 or tube masks and the input features F p, F s. For mask classification, we
M itube ∈ RN×T×H 4×W 4 . Then, we can obtain the corre- adoptseveralfeed-forwardlayersonQp iananddirectlyout-
sponding query features Xpan ∈ RN×d, Xiter ∈ RK×d, put the class scores. Then, we perform the inner product
X itube ∈RN×dviamaskpoi oling: i betweenthelearnedqueriesQp ian,Qi iter,andtheirfeatures
F and F . For the video segmentation task, the only
vid img
W H difference is that we predict the tube mask rather than the
(cid:88)(cid:88)
Xpan = Mpan(u,v)·F (u,v). (1) imagemask. Theentireprocess(Equ.1,Equ.2,Equ.3and
i i−1 img
u v Equ.5)isrepeatedthreetimesintotal.
Moreover, we also explore various decoupled designs,
For video tasks, we group the spatial-temporal features as
asshowninFig.3. Inparticular,aspreviousworkssuggest,
follows:
we also explore decoupled decoders and their correspond-
ingadapters. Viaourdetailedexperiments, wedonotfind
T W H
(cid:88)(cid:88)(cid:88)
Xtube = Mtube(t,u,v)·F (t,u,v), (2) extra gains in the cases of decoupled decoders that intro-
i i−1 vid
ducemorecomputationandparametercosts.Pleasereferto
t u v
Sec.5.2forthedetaileddecoderdesign.
where u, v are the indices of spatial location, t is index of
Light-Weight Decoupled Adapter. After the shared de-
inputframe. Sincethedynamicconvolutionperformssim-
coder, we also add two lightweight adapters, A and
ilarlyforeachtask,weusepanopticsegmentationXpan as obj
i A prompttoadapttheshareddecoder’sknowledgebetter. In
an illustration. For interactive segmentation, we adopt the
particular, we use the asymmetric design. A uses the
obj
sameequation(Equ.1). Inourimplementation,thelearned
samedynamicconvolutiondesigntorefinetheobjectquery
queryissharedforbothQpanandQtube.
i i further, while A prompt uses pixel-wise cross-attention de-
In particular, we adopt the design [42, 68, 93] to refine sign. Our key insights are: (1) For panoptic segmentation
inputqueriesQp ian withfeaturesX ipan whicharegrouped and video segmentation, the interaction of the scene fea-
fromtheirmasks, ture is necessary, but not for interactive segmentation. (2)
despitethedecoderbeingshared, thedetailsarestillmiss-
Qˆpan =DynamicConv(Xpan,Qpan), (3)
i i i−1 ingforinteractivesegmentationduetothepoolingeffectin
Equ.1,whereweevenfindinferiorresultsafteradoptinga
where the dynamic convolution uses the query features
pooling-basedadapter. Moredetailedadapterdesigncanbe
Xpan to generate parameters to weight input queries
i foundinSec.5.2.
Qpan. Specifically, DynamicConv uses input query fea-
i−1
tures Xpan to generate gating parameters via a multilayer 4.2.TrainingandInference
i
perceptron (MLP) and multiply back to the original query
input Qpan. We adopt the same design [68, 93] by learn- Joint Image and Video Segmentation Co-training. Our
i−1
ing gating functions to update the refined queries. The goal is to train all segmentation tasks only once jointly.
DynamicConvoperationisbasedon: All training targets are one entity label and mask for all
three different cases. The entity can be thing, stuff, class-
Qˆpan =Gate (Xpan)Xpan+Gate (Xpan)Qpan, (4) agnostic masks, and their corresponding labels. Note that
i x i i q i i−1
theinstancemaskswiththesameIDdformthetubemasks.
where Gate is implemented with a fully connected (FC) Duringtraining,weapplyHungarianmatchingbetweenthe
layer followed by Layer Norm (LN) and a sigmoid layer. predicted and ground-truth entity masks to assign object
Weadopttwodifferentgatefunctions,includingGate and queriestovideo/imageentitiesandthensupervisetheirpre-
x
Gate . They are implemented by MLP. The former is to dicted masks and classification. The classifier is replaced
q
weigh the query features, while the latter is to weigh cor- by CLIP text embedding to avoid cross-dataset taxonomy
respondingqueries. Next,weadoptoneself-attentionlayer conflicts.
with feed-forward layers [70, 72] to learn the correspon- Following previous works [10, 46], the final loss func-
denceamongeachqueryandupdatethemaccordingly.This tionisgivenasL=λ L +λ L +λ L .
cls mcls ce mce dice mdice
operationleadstothefullcorrelationamongqueries,shown Here,L istheCross-Entropy(CE)lossformaskclas-
mcls
asfollows: sification, and L and L are mask Cross-Entropy
mce mdice
(CE) loss and Dice loss [62, 73] for segmentation, respec-
Q =FFN(MHSA(Qˆ )+Qˆ ), (5) tively. Bydefault,wesetλ =2,λ =5,λ =5.
i i i cls ce dice
5Table3. Real-TimeAll-PurposeSegmentationbenchmark. OurproposedRAP-SAMachievesthebestaccuracyandspeedtrade-offon
threetasks.
COCO-Panoptic COCO-SAM YouTube-VIS2019
Method Backbone Flops Parameters FPS
PQ SQ PQ th PQ st mIoU mAP
Mask2Former[10] R18 35.6 77.5 39.0 30.3 54.7 38.6 89.8G 18.6M 31.2
Mask2Former[10] R50 42.9 79.8 47.6 35.6 58.0 42.1 153G 45.2M 26.6
MaskFormer[11] R18 31.0 76.0 33.2 27.7 55.6 34.1 79.7G 18.5M 38.0
MaskFormer[11] R50 37.4 77.3 41.2 31.8 58.8 40.0 143.0G 45.2M 34.3
MaskFormer[11] TopFormer[92] 31.6 75.9 33.5 28.6 56.1 38.2 60.0G 12.8M 29.9
kMaX-DeepLab[85] R18 27.8 71.0 30.3 24.1 16.9 23.1 87.1G 18.7M 15.0
kMaX-DeepLab[85] R50 36.9 75.7 42.7 28.4 20.1 26.4 280.0G 51.4M 14.9
YOSO[25] R18 31.6 76.8 36.4 24.4 45.7 31.4 57.3G 18.7M 41.0
YOSO[25] R50 37.3 76.9 42.7 29.2 49.2 40.7 119.0G 45.1M 36.3
YOSO[25] TopFormer[92] 31.0 75.9 33.0 26.9 45.1 27.9 36.8G 12.9M 31.1
RAP-SAM R18 39.9 78.6 43.3 34.8 52.7 38.7 60.5G 22.8M 40.3
RAP-SAM R50 46.9 80.8 51.6 39.8 57.9 46.2 123.0G 47.2M 35.1
RAP-SAM TopFormer[92] 34.6 77.0 37.8 29.8 53.3 41.7 40.1G 15.0M 30.7
Inference. we follow the same inference procedure of Re-implemented Baselines. Since no previous works ex-
Mask2Former[10]forimagesegmentation. Wemergethe plore joint co-training for image, video, and interactive
thingsandstuffforPSaccordingtothesortedscores. The segmentation in one framework. We re-implement sev-
scores are generated by CLIP text embedding. For video eral representative baselines using the same codebase, in-
instance segmentation, we use query matching rather than cluding non-real-time models (Mask2Former [10], kMaX-
introducingextratrackingcomponentstogenerateinstance DeepLab [85]) for reference. In particular, we benchmark
ID,followingpreviouswork[26,46,47]. Weadoptanear- over ten different methods, including different backbones
online method for inference. For interactive segmentation anddecoders.
tasks, we follow the origin SAM [34], by providing box Implementation Details. We implement our models and
andpointpromptsandobtainingthebinarymasks. Allthe all other baselines in PyTorch [65] with the MMDetection
parametersaresharedacrossthreedifferenttasks. toolbox[5]. Weusethedistributedtrainingframeworkwith
ExtensionandApplication. Sinceourmodelcanperform 16A100GPUs. Eachmini-batchhastwoimagesperGPU.
various segmentation tasks, RAP-SAM can be extended In particular, we adopt pseudo video training on COCO
to more datasets and tasks, including semantic segmenta- by moving image masks with random directions. All the
tion on ADE20k [98] or video panoptic segmentation on modelsaretrainedwith24epochs. Fordataaugmentation,
VIPSeg [61]. Moreover, our method can support prompt- we adopt large-scale jitter as previous works [10] to build
drivenvideoobjectsegmentationbycombininginteractive strong baselines. For all models, we adopt training steps
segmentation with a video instance segmentation frame- and optimizers. Refer to the supplementary for more de-
work. We put the details of the extension and application tails.
forRAP-SAMinthesupplementary.
5.1.MainResults
5.Experiments
OurBenchmarkResults. Welistourbenchmarkresultsin
Datasets. Our benchmark uses COCO [50] and YouTube- Tab. 3. We benchmark recent state-of-the-art methods us-
VIS2019[81]datasetsforbenchmarking. COCO-SAMis ingthesametrainingandtestsettings. Fromthetable,our
constructed by using ground truth boxes and masks as vi- proposedRAP-SAMachievesthebestspeed andaccuracy
sualpromptinputs,followingtheSAM-1Bdataset[34]. In trade-off on all three different visual segmentation tasks,
addition,toverifytheeffectivenessandgeneralityofRAP- under the various backbones. Mask2Former [10] achieves
SAM,wealsouseotherdatasets,includingADE-20K[98] similarorpartiallystrongerresultsthanourmethod. How-
andVIP-Segdataset[61]inSec.5.1. ever, their speed is still limited, which is 7-8 FPS slower
Evaluation Metrics and Devices. For image segmenta- than our methods. YOSO [25] runs as fast as our method.
tion, we adopt panoptic quality (PQ), which can be fur- However,theperformancegapisstillsignificant. Thus,our
therdecomposedintothesegmentationquality(SQ)andthe proposed RAP-SAM is a simple yet effective baseline for
recognitionquality(RQ).FortheVIStask,mAPisadopted. real-timeandall-purposesettings.
Wefurtherreportthesemanticsegmentationresults(mIoU) PromptableResultsComparedwithSAM.InTab.4,we
and video panoptic segmentation results (STQ [76] and verify our trained model as a promptable segmenter, like
VPQ[31]). WeadoptoneA100GPUforspeedtesting. SAM.WecompareourmethodwithSAM-Huge, whichis
6Table4. ComparisonwithSAM[34]asapromptablesegmenter Table7.Ablationonshareddecoderdesign.
onvariousdetectorsandinstancesegmenters.
Setting COCO-Panoptic COCO-SAM FPS
Method Detectors mAP AP50 AP75 Parameters Flops Per-PixelCross-Attention[10] 45.0 55.3 28.0
Pooling+DynamicConvolution[68] 43.8 55.7 36.2
SAM-Huge Mask-RCNN(R50) 35.6 54.9 38.4 641M 3000G
Pooling+DynamicConvolutionwithGate[93] 44.6 56.7 34.5
Ours Mask-RCNN(R50) 26.7 43.0 27.9 47M 188G
SAM-Huge Mask-RCNN(X-101-64x4d) 39.0 60.5 42.1 641M 3000G
Ours Mask-RCNN(X-101-64x4d) 28.5 46.4 29.9 47M 188G
Table8.Ablationonmeta-architecture.
Table 5. Comparison with real-time video segmentation method
Setting COCO-Panoptic COCO-SAM Parameters
onVIP-Segvalidationset.
Fig.3(a) 43.8 54.2 46.3M
Fig.3(b) 44.0 55.2 53.6M
Method backbone VPQ1 VPQ2 VPQ4 VPQ6 VPQ STQ FPS Fig.3(c) 44.6 56.7 47.3M
Clip-PanoFCN[61] R50 24.3 23.5 22.4 21.6 22.9 31.5 8 Fig.3(d) 45.2 56.2 54.6M
VideoK-Net[47] R50 29.5 26.5 24.5 23.7 26.1 33.1 10
Tube-Link[46] STDCv1 32.1 31.3 30.1 29.1 30.6 32.0 14
Tube-Link[46] STDCv2 33.2 31.8 30.6 29.6 31.4 32.8 12 Table 9. Ablation on adapter design. CA: cross-attention. DC:
Ours R18 34.2 32.1 30.8 30.2 32.5 33.7 30
dynamicconvolution.
Table 6. Comparison with real-time panoptic segmentation
methodonADE-20kvalidationset. A obj A prompt COCO-Panoptic COCO-SAM
- - 44.2 53.2
Method Backbone PQ PQth PQst Parameters Flops CA CA 42.6 54.3
DC DC 44.7 52.1
PanSegFormer[49] R50 36.4 35.3 38.6 51M 214G
DC CA 44.6 56.7
MaskFormer[11] R50 34.7 32.2 39.7 45.2M 181G
Mask2Former[10] R50 39.7 39.0 40.9 45.2M 226G CA DC 44.5 56.2
YOSO[25] R50 38.0 37.3 39.4 45.1M 176G
RAP-SAM R50 38.3 36.6 39.7 47.2M 179G
Table10. Ablationonjointco-training. (a),COCO-Panoptic. (b),
YT-VIS2019.(c),COCO-SAM.
muchlarger. Despitetherebeingagap,ourmethodcanrun
Setting COCO-Panoptic YT-VIS2019 COCO-SAM
fastandwithmuchlessGFlops.
a 36.6 - -
Comparison with Specific Design Models on VIP-Seg.
b - 21.5 -
We also verify the effectiveness of RAP-SAM on a more a+b 36.2 36.0 50.7
a+b+c 35.7 35.3 50.9
challenging video segmentation task, video panoptic seg-
mentation on the VIP-Seg dataset. Compared with recent
works[46,61],RAP-SAMalsoarchivesthebestspeedand
have more capacity. We argue that since our feature ex-
accuracytrade-off.
tractorsareweakerthanthoseworksandtherepresentation
Comparison with Specific Design Models on ADE-20k.
power is far from the heavier backbone such as Swin [52]
WefurthertransferRAP-SAMonADE-20kdatasets. Since
orViT-large[13],addingmorecapacityinthedecodermay
our method is real-time model, we only list several repre-
notboostthefinalperformance.
sentativeworksforreference. Again,comparedwithrecent
Ablation on Adapter Design. Then, we explore the
work[25],ourmethodstillachievesstrongerresults.
adapterdesigninTab.9. Inparticular,weuseapre-trained
modelwithoutadaptersforinitializationtoseetheeffectof
5.2.AblationStudyandVisualAnalysis
differentadapters. Inparticular, wefind usingasymmetric
Ablation on Shared Decoder Design. In Tab. 7, we find adaptersworkswellforbalancedresultsforobjectqueries
usingsimplepooling-baseddynamicconvolutionperforms and prompt queries, since the goals of the two queries are
well under real-time settings, where we adopt R18 as the different. Theformerneedsthespatio-temporalandscene-
backbone and keep the adapter unchanged. In particular, level context, while the latter only cares about the region
thereisnodifferenceinpanopticsegmentationresultsanda contextwiththeinputpositionastheguidance.
slightgapforinteractivesegmentation. However,per-pixel EffectivenessofJointCo-training. Wefurtherexplorethe
cross-attentionintroducesextracomputationcosts. effect of each dataset in Tab. 10. We find joint co-training
Ablation on Meta-Architecture Design. In Tab. 8, we with image and video data leads to better performance for
further explore the meta-architecture design, as shown in video instance segmentation. Adding COCO-SAM train-
Fig. 3. We use R50 as the backbone and keep the adapter ing leads to a few performance drops. This is because the
unchanged. Fromthetable,wefindusingashareddecoder learningtargetsaredifferentforobjectqueriesandprompt
architecture(inTab.7)achievesthebestparameterandper- queries.
formancetrade-off. Thesefindingsmayconflictwithrecent Visualization. In Fig. 4, we give the three different vi-
multi-task works [42] since the decoupled decoders may sualization results on the YouTube-VIS 2019 dataset and
7(a) Object quries (b) Object quries (c) Object quries (d) object quries Adaptor
Mask Mask Mask Mask
Adaptor
Updated quries Class Decoder Updated quries Class Updatedquries Class Decoder Adapted quries Class
FPN Feature Decoder FPN Feature Decoupled FPN Feature Decoder FPN Feature Decoupled
Mask Decoder Mask Mask Decoder Mask
Adaptor
Class Class Class Class
Prompt quries Updated quries Prompt quries Updated quries Prompt quries Updatedquries prompt quries Adaptor Adapted quries
Figure3. Meta-architectureexploration. (a),Simpleshareddecoderdesign. (b),Decoupleddecoderdesignwithtwoheads. (c),Shared
decoderwithdecoupledadapter.(d),Decoupleddecoderwiththedecoupledadapters.Bestviewedincolorandzoomin.
Frame 1 Frame 2 Frame 3 Frame 4 Frame 5
Figure4. ThevisualizationresultsofourmodelontheYouTube-VIS2019datasetandtheCOCOdataset. Thefirstthreerowsvisualize
fiveframesofinputs.Thesameinstancesareshownwiththesamecolor.Thefourthrowshowstheinteractivesegmentationresultswitha
single-pointprompt(showningreencolor).Thelastrowshowsthepanopticsegmentationresults.
theCOCOdataset. Wegeneratethesevisualizationresults segmentation results with a single-point prompt (shown in
using only a single model thanks to the joint co-training greencolor),andthelastrowshowsthepanopticresults.
strategy. The first three rows in Fig. 4 show the video in-
stancesegmentation. Thefourthrowshowstheinteractive
86.Conclusion thelearningrateissetto1e-4forallmethods. Wewarmup
the learning rate in the first 500 iterations using a linearly
In this work, we explore a challenging real-time setting,
increased strategy and decay the learning rate at 8 and 11
all-purpose segmentation, to transfer more segmentation
epochsbyafactorof10. Fordataaugmentation,weusethe
tasks into one model. To solve this problem, we intro-
large-scalejittering(LSJ)augmentation[16]witharandom
duceRAP-SAM,areal-time segmentation model, thatcan
scalesampledfromtherangeof0.1to2.0andfollowedby
segment and track objects in image, video, and interac-
afixedsizecropto1024×1024.
tive modes. We design a simple shared decoder with two
More Inference Details. For video instance segmenta-
key designs: shared dynamic convolution and asymmetric
tion, we adopt simple object query matching [26, 47] for
adapters, which lead to the best trade-off between speed
all methods to keep the fair comparison. For interactive
and accuracy on three segmentation tasks. We hope our
segmentation,wemainlyusepointprompts,whichismore
proposed benchmark inspires future research on this new
challengingthanboxprompts. Forpanopticsegmentation,
setting.
all hyperparameters are the same. The FPS is obtained on
Overview. Inthissupplementary,wefirstpresentmorede-
oneA100cardforboththemainpaperandsupplementary.
tailed results to support both the benchmark and our pro-
posedRAP-SAMinSec.A.Then, inSec.B,wepresenta
B.VisualComparison
moredetailedvisualcomparisonusingourmodel. Finally,
inSec.C,wediscussseveralfailurecasesandpotentialfu- Comparison on Youtube-VIS dataset. In Fig. 5, we
turework. compare our RAP-SAM (right) with a strong baseline
Mask2Former (left) with the same ResNet50 backbone.
A.MoreExperimentResults Ourmethodsachievemoreconsistenttrackingandsegmen-
tationresultsintheseexamples.
Benchmark More Methods. In Tab. 11, we benchmark
ComparisononCOCOPanopticSegmentationdataset.
moremodelsandbackbones[57,58,93]asthesupplemen-
In Fig. 6, We demonstrate that our RAP-SAM model sur-
tary to main paper. Again, for various backbones and dif-
passes K-Net in panoptic segmentation tasks. RAP-SAM
ferent heads, our model can achieve better or comparable
exhibits enhanced accuracy in processing complex scenes
results across three different tasks, while running in real-
and maintaining fine details, especially in segmenting ob-
time. In particular, compared with the dynamic kernel-
jectedgesandmanagingoverlappingobjects.
basedmethod,K-Net,ourapproachachievesbetterperfor-
More Interactive Segmentation Results. In Fig. 7, We
manceontheR18andR50backbonesandcomparablere-
present further visualizations of interactive segmentation,
sultsonthelightweightbackbones.
showcasingsegmentationoutcomesfor’things’and’stuff’.
ScaleUpJointCo-training. InthebottomofTab.11,we
In these visualizations, green dots signify manually speci-
furtherscaleupourmodelonalargebackbone,ConvNeXt
fiedprompts.Ourmodelexhibitsprecisesegmentationabil-
large[53].Fromthetable,wefindtherearesignificantgains
ities for ’things’ and ’stuff’. Notably, even when prompt
in all settings while still running with 8 FPS. This means
points are situated on object boundaries, our model accu-
our proposed all-purposed segmentation still has room to
ratelyidentifiesandsegmentstheobject.
balancetheaccuracyandinferencespeed.
Explore adapter Design on More Methods. We
C.ChallengesandFutureWork
explore the effectiveness of our adapter design on
Mask2Former [10] and add the adapter module to the last Failure Cases Analysis. We also analyze failure cases in
decoder layer of Mask2Former. As shown in Tab. 12, video/image segmentation and identify two typical failure
Mask2Former with adapter will improve 1.1 mIoU score modesofRAP-SAM.Firstly, asillustratedinFig.8, when
comparedwithoriginMask2Formerforinteractivesegmen- faced with multiple objects with high overlap, RAP-SAM
tationandonlydrop0.3PQforpanopticsegmentation.This strugglestofullyrecognizethem,resultingintheomission
resultindicatesthattheadaptermodulenotonlyappliesto of several masks. Secondly, in crowded scenarios, RAP-
ourmethodbutcanalsobegeneralizedtoothermethods SAM faces challenges in recognizing and segmenting all
More Implementation Details. We implement our pro- instanceswithalimitednumberofinstancekernels.
posed method and benchmark methods using MMDetec- Future Work Discussion. There are several directions to
tion toolbox [5], and we train all of these methods in the explore for real-time all-purpose segmentation. The first
sameenvironment. ForkMaX-DeepLab[85],weexcluded direction is to achieve better-balanced performance on im-
theauxiliarysemanticsegmentationloss,instancediscrim- age,video,andinteractivesegmentation(sincethereisstill
ination loss, and preserving PQ-style loss alignment with alot ofroom forperformance, as showninthe lastrow of
Mask2former [10] for a fair comparison. We utilize the Tab.11). Theseconddirectionistospeedupthemodeland
AdamW [54] optimizer with a weight decay of 0.05, and deploythemodelontheedgedevice, suchasiPhone. The
9Table11.BenchmarkMoreModelsonReal-TimeAll-PurposeSegmentation.TheGFlopsareobtainedwith1333×800inputs.
COCO-Panoptic COCO-SAM YouTube-VIS2019
Method Backbone Flops Parameters FPS
PQ SQ PQ th PQ st mIoU mAP
YOSO[25] EdgeNexT[57] 31.8 74.5 35.6 26.1 47.4 35.4 40G 15.3M 28.6
K-Net[93] R18[21] 33.1 75.7 36.8 27.4 53.6 34.8 124G 25.6M 30.4
K-Net[93] R50[21] 39.2 78.2 43.9 31.6 56.3 41.7 171G 38.5M 20.9
K-Net[93] EdgeNexT[57] 38.0 76.7 42.5 31.4 55.7 44.0 108G 19.5M 30.5
Mask2Former[10] EdgeNexT[57] 39.8 78.8 43.6 34.0 54.2 - 73G 12.0M 25.6
RAP-SAM R18 39.9 78.6 43.3 34.8 52.7 38.7 60.5G 22.8M 40.3
RAP-SAM R50 46.9 80.8 51.6 39.8 57.9 46.2 123.0G 47.2M 35.1
RAP-SAM EdgeNexT[57] 38.0 77.7 42.1 31.9 54.8 46.3 44G 14.3M 31.5
RAP-SAM ConvNeXt-L[53] 52.3 82.5 58.6 42.8 61.1 55.6 700G 239M 8.6
RAP-SAM Mask2Former
Figure 5. When compared to the Mask2Former(right) on the YouTube-VIS 2019 dataset, our RAP-SAM(left) demonstrates superior
performanceinrecognizingandsegmentingobjectsincertainscenarios,anditalsoholdsanadvantageindetailingedges.
Table12.ExploreadapterDesignonMoreMethods. 2020. 1,2,5
[3] Cheng Chen, Juzheng Miao, Dufan Wu, Zhiling Yan,
Method Backbone PQ mIoU Sekeun Kim, Jiang Hu, Aoxiao Zhong, Zhengliang Liu,
Mask2Former R50 43.2 57.0 Lichao Sun, Xiang Li, Tianming Liu, Pheng-Ann Heng,
Mask2Former+adapter R50 43.0 58.1 andQuanzhengLi. Ma-sam:Modality-agnosticsamadap-
tationfor3dmedicalimagesegmentation. arXivpreprint
arXiv:2309.08842,2023. 3
[4] KaiChen,JiangmiaoPang,JiaqiWang,YuXiong,Xiaox-
thirddirectionistoexploredifferentknowledgedistillation iao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping
approachestotransferthevisionfoundationmodelinreal- Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin.
time all-purpose models. Moreover, more visual prompts Hybridtaskcascadeforinstancesegmentation. InCVPR,
canbeexplored,suchasmaskpromptsorboxprompts.Our 2019. 2
methodcurrentlyonlysupportspointprompts. Thiswillbe [5] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu
Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei
ourfuturework.
Liu,JiaruiXu,etal. Mmdetection: Openmmlabdetection
toolboxandbenchmark. arXivpreprint,2019. 6,9
References [6] Liang-Chieh Chen, George Papandreou, Florian Schroff,
andHartwigAdam. Rethinkingatrousconvolutionforse-
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An- manticimagesegmentation. arXiv:1706.05587,2017. 2
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur [7] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Flo-
Mensch, Katherine Millican, Malcolm Reynolds, et al. rian Schroff, and Hartwig Adam. Encoder-decoder with
Flamingo: avisuallanguagemodelforfew-shotlearning. atrousseparableconvolutionforsemanticimagesegmenta-
NeurIPS,2022. 3 tion. InECCV,2018. 2
[2] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nico- [8] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen
las Usunier, Alexander Kirillov, and Sergey Zagoruyko. Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. Mobile-
End-to-endobjectdetectionwithtransformers. InECCV,
10RAP-SAM
K-Net
Figure6.WhencomparedwithK-Net(bottom)ontheCOCOdataset,ourRAP-SAM(top)modelshowsasignificantadvantage,achieving
moreaccuratesegmentationofforegroundandbackground,withedgesthataresmoother.
former: Bridging mobilenet and transformer. In CVPR, ZongxinYang,WenguanWang,andYiYang.Segmentand
2022. 3 trackanything. arXivpreprintarXiv:2305.06558,2023. 3
[9] Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexan- [13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
der Kirillov, Rohit Girdhar, and Alexander G Schwing. Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mask2formerforvideoinstancesegmentation. arXivpre- Mostafa Dehghani, Matthias Minderer, Georg Heigold,
print,2021. 2 Sylvain Gelly, et al. An image is worth 16x16 words:
[10] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Transformersforimagerecognitionatscale.arXivpreprint
Alexander Kirillov, and Rohit Girdhar. Masked-attention arXiv:2010.11929,2020. 1,2,3,5,7
mask transformer for universal image segmentation. In [14] MingyuanFan,ShenqiLai,JunshiHuang,XiaomingWei,
CVPR,2022. 2,4,5,6,7,9,10 Zhenhua Chai, Junfeng Luo, and Xiaolin Wei. Rethink-
[11] BowenCheng,AlexanderG.Schwing,andAlexanderKir- ingbisenetforreal-timesemanticsegmentation. InCVPR,
illov. Per-pixelclassificationisnotallyouneedforseman- 2021. 4
ticsegmentation. InNeurIPS,2021. 6,7 [15] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,
[12] Yangming Cheng, Liulei Li, Yuanyou Xu, Xiaodi Li, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue
11Things
Stuff
Figure7. Hereareadditionalresultsofinteractivesegmentation. Forpointpromptsorboxprompts, RAP-SAMsegmentsoutthecor-
responding object based on the spatial location of the prompt. We have provided both segmentation results for ’things’ and ’stuff’,
demonstratingthatRAP-SAMiscapableofsegmentingbothwhilemaintainingstrongperformance.
Figure8.FailuremodesofRAP-SAMinimage/videosegmentation.
Cao. Eva: Exploringthelimitsofmaskedvisualrepresen- Deep residual learning for image recognition. In CVPR,
tationlearningatscale. arXivpreprintarXiv:2211.07636, 2016. 4,10
2022. 3 [22] WeixiangHong,QingpeiGuo,WeiZhang,JingdongChen,
[16] Golnaz Ghiasi, Yin Cui, A. Srinivas, Rui Qian, Tsung-Yi and Wei Chu. Lpsnet: A lightweight solution for fast
Lin,EkinDogusCubuk,QuocV.Le,andBarretZoph.Sim- panopticsegmentation. InCVPR,2021. 3
plecopy-pasteisastrongdataaugmentationmethodforin- [23] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh
stancesegmentation. CVPR,2020. 9 Chen,BoChen,MingxingTan,WeijunWang,YukunZhu,
[17] Xiuye Gu, Yin Cui, Jonathan Huang, Abdullah Rashwan, RuomingPang,VijayVasudevan,etal. Searchingformo-
XuanYang, XingyiZhou, GolnazGhiasi, WeichengKuo, bilenetv3. InICCV,2019. 3
HuizhongChen,Liang-ChiehChen,etal.Dataseg:Taming [24] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry
a universal multi-dataset multi-task segmentation model. Kalenichenko, Weijun Wang, Tobias Weyand, Marco An-
arXivpreprintarXiv:2306.01736,2023. 2 dreetto,andHartwigAdam.Mobilenets:Efficientconvolu-
[18] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing tionalneuralnetworksformobilevisionapplications.arXiv
Xu, andChangXu. Ghostnet: Morefeaturesfromcheap preprintarXiv:1704.04861,2017. 3
operations. InCVPR,2020. 3 [25] Jie Hu, Linyan Huang, Tianhe Ren, Shengchuan Zhang,
[19] KaimingHe,XinleiChen,SainingXie,YanghaoLi,Piotr Rongrong Ji, and Liujuan Cao. You only segment once:
Dolla´r,andRossGirshick. Maskedautoencodersarescal- Towardsreal-timepanopticsegmentation. InCVPR,2023.
ablevisionlearners. CVPR,2022. 3 2,3,4,6,7,10
[20] KaimingHe,GeorgiaGkioxari,PiotrDolla´r,andRossGir- [26] De-AnHuang,ZhidingYu,andAnimaAnandkumar. Min-
shick. Maskr-cnn. InICCV,2017. 2 vis: A minimal video instance segmentation framework
[21] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. withoutvideo-basedtraining. InNeurIPS,2022. 6,9
12[27] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Learning a unified model for panoptic part segmentation.
Huang,YunchaoWei,andWenyuLiu. Ccnet: Criss-cross InECCV,2022. 2,5,7
attentionforsemanticsegmentation. InICCV,2019. 2 [43] Xiangtai Li, Shilin Xu, Yibo Yang, Haobo Yuan, Guan-
[28] Forrest N Iandola, Song Han, Matthew W Moskewicz, gliang Cheng, Yunhai Tong, Zhouchen Lin, Ming-Hsuan
Khalid Ashraf, William J Dally, and Kurt Keutzer. Yang,andDachengTao. Panopticpartformer++: Aunified
Squeezenet: Alexnet-level accuracy with 50x fewer pa- anddecoupledviewforpanopticpartsegmentation. arXiv
rameters and¡ 0.5 mb model size. arXiv preprint preprintarXiv:2301.00954,2023. 2
arXiv:1602.07360,2016. 3 [44] Xiangtai Li, Ansheng You, Zeping Zhu, Houlong Zhao,
[29] JiteshJain,JiachenLi,MangTikChiu,AliHassani,Nikita MaokeYang, KuiyuanYang, andYunhaiTong. Semantic
Orlov, andHumphreyShi. OneFormer: OneTransformer flowforfastandaccuratesceneparsing.InECCV,2020.3,
toRuleUniversalImageSegmentation. CVPR,2023. 2,4 4
[30] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana [45] XiangtaiLi,HaoboYuan,WeiLi,HenghuiDing,SizeWu,
Parekh, HieuPham, QuocLe, Yun-HsuanSung, ZhenLi, Wenwei Zhang, Yining Li, Kai Chen, and Chen Change
and Tom Duerig. Scaling up visual and vision-language Loy. Omg-seg: Isonemodelgoodenoughforallsegmen-
representation learning with noisy text supervision. In tation? arXiv,2024. 2
ICML,2021. 1 [46] Xiangtai Li, Haobo Yuan, Wenwei Zhang, Guangliang
[31] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Cheng,JiangmiaoPang,andChenChangeLoy. Tube-link:
Kweon. Videopanopticsegmentation. InCVPR,2020. 6 Aflexiblecrosstubebaselineforuniversalvideosegmen-
[32] Dahun Kim, Jun Xie, Huiyu Wang, Siyuan Qiao, Qi- tation. InICCV,2023. 2,5,6,7
hang Yu, Hong-SeokKim, Hartwig Adam, InSo Kweon, [47] Xiangtai Li, Wenwei Zhang, Jiangmiao Pang, Kai Chen,
andLiang-ChiehChen. Tubeformer-deeplab: Videomask Guangliang Cheng, Yunhai Tong, and Chen Change Loy.
transformer. InCVPR,2022. 2 Video k-net: A simple, strong, and unified baseline for
[33] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten videosegmentation. InCVPR,2022. 2,4,6,7,9
Rother,andPiotrDolla´r. Panopticsegmentation. InCVPR, [48] YanweiLi, HengshuangZhao, XiaojuanQi, LiweiWang,
2019. 2,3 Zeming Li, Jian Sun, and Jiaya Jia. Fully convolutional
[34] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi networksforpanopticsegmentation. CVPR,2021. 2
Mao,ChloeRolland,LauraGustafson,TeteXiao,Spencer [49] Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima
Whitehead,AlexanderCBerg,Wan-YenLo,etal.Segment Anandkumar, Jose M. Alvarez, Tong Lu, and Ping Luo.
anything. ICCV,2023. 1,3,4,6,7 Panopticsegformer:Delvingdeeperintopanopticsegmen-
[35] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong tationwithtransformers,2021. 2,7
Liu,JianweiYang,ChunyuanLi,LeiZhang,andJianfeng [50] Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Gao. Semantic-sam: Segment and recognize anything at Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and
anygranularity. arXivpreprintarXiv:2307.04767,2023. 2 CLawrenceZitnick. Microsoftcoco: Commonobjectsin
[36] FengLi,HaoZhang,Huaizhexu,ShilongLiu,LeiZhang, context. InECCV,2014. 2,3,6
LionelM.Ni,andHeung-YeungShum. MaskDINO:To- [51] Haotian Liu, Rafael A. Rivera Soto, Fanyi Xiao, and
wardsaunifiedtransformer-basedframeworkforobjectde- Yong Jae Lee. Yolactedge: Real-time instance segmenta-
tectionandsegmentation. InCVPR,2023. 2 tionontheedge(jetsonagxxavier:30fps,rtx2080ti:170
[37] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. fps). arXivpreprintarXiv:2012.12259,2020. 2
Blip: Bootstrapping language-image pre-training for uni- [52] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei,
fied vision-language understanding and generation. In ZhengZhang,StephenLin,andBainingGuo. Swintrans-
ICML,2022. 3 former: Hierarchicalvisiontransformerusingshiftedwin-
[38] Jiashi Li, Xin Xia, Wei Li, Huixia Li, Xing Wang, Xue- dows. ICCV,2021. 2,7
feng Xiao, Rui Wang, Min Zheng, and Xin Pan. Next- [53] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeicht-
vit: Next generation vision transformer for efficient de- enhofer,TrevorDarrell,andSainingXie.Aconvnetforthe
ployment in realistic industrial scenarios. arXiv preprint 2020s. CVPR,2022. 9,10
arXiv:2207.05501,2022. 3 [54] IlyaLoshchilovandFrankHutter.Decoupledweightdecay
[39] XiangtaiLi, HenghuiDing, WenweiZhang, HaoboYuan, regularization. arXivpreprint,2017. 9
GuangliangCheng,PangJiangmiao,KaiChen,ZiweiLiu, [55] Hailong Ma, Xin Xia, Xing Wang, Xuefeng Xiao, Jiashi
andChenChangeLoy. Transformer-basedvisualsegmen- Li,andMinZheng. Mocovit: Mobileconvolutionalvision
tation:Asurvey. arXivpre-print,2023. 2 transformer. arXivpreprintarXiv:2205.12635,2022. 3
[40] XiangtaiLi,HaoHe,YiboYang,HenghuiDing,Kuiyuan [56] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian
Yang,GuangliangCheng,YunhaiTong,andDachengTao. Sun. Shufflenet v2: Practical guidelines for efficient cnn
Improvingvideoinstancesegmentationviatemporalpyra- architecturedesign. InECCV,2018. 3
midrouting. T-PAMI,2022. 2 [57] Muhammad Maaz, Abdelrahman Shaker, Hisham
[41] Xiangtai Li, Zhao Houlong, Han Lei, Tong Yunhai, and Cholakkal,SalmanKhan,SyedWaqasZamir,RaoMuham-
YangKuiyuan. Gff: Gatedfullyfusionforsemanticseg- mad Anwer, and Fahad Shahbaz Khan. Edgenext: ef-
mentation. InAAAI,2020. 2 ficiently amalgamated cnn-transformer architecture for
[42] Xiangtai Li, Shilin Xu, Yibo Yang, Guangliang Cheng, mobilevisionapplications. 2022. 3,9,10
Yunhai Tong, and Dacheng Tao. Panoptic-partformer: [58] SachinMehtaandMohammadRastegari.Mobilevit:Light-
13weight,general-purpose,andmobile-friendlyvisiontrans- forin-contextvisuallearning. InCVPR,2023. 1
former. InICLR,2022. 3,9 [75] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang,
[59] Sachin Mehta, Mohammad Rastegari, Anat Caspi, Linda Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting
Shapiro,andHannanehHajishirzi.Espnet:Efficientspatial everythingincontext. InICCV,2023. 1,3
pyramidofdilatedconvolutionsforsemanticsegmentation. [76] M. Weber, J. Xie, M. Collins, Yukun Zhu, P. Voigtlaen-
InECCV,2018. 3 der,H.Adam,B.Green,A.Geiger,B.Leibe,D.Cremers,
[60] Sachin Mehta, Mohammad Rastegari, Linda Shapiro, and AljosaOsep,L.Leal-Taixe´,andLiang-ChiehChen. Step:
HannanehHajishirzi. Espnetv2: Alight-weight,poweref- Segmentingandtrackingeverypixel. NeurIPS,2021. 6
ficient,andgeneralpurposeconvolutionalneuralnetwork. [77] Junde Wu, Rao Fu, Huihui Fang, Yuanpei Liu, Zhaowei
InCVPR,2019. 3 Wang, Yanwu Xu, Yueming Jin, and Tal Arbel. Medical
[61] Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, samadapter: Adaptingsegmentanythingmodelformedi-
Yunchao Wei, and Yi Yang. Large-scale video panoptic calimagesegmentation. arXivpreprintarXiv:2304.12620,
segmentationinthewild:Abenchmark.InCVPR,2022.6, 2023. 3
7 [78] Zeqi Xiao, Wenwei Zhang, Tai Wang, Chen Change Loy,
[62] FaustoMilletari,NassirNavab,andSeyed-AhmadAhmadi. Dahua Lin, and Jiangmiao Pang. Position-guided point
V-Net: Fullyconvolutionalneuralnetworksforvolumetric cloud panoptic segmentation transformer. arXiv preprint,
medicalimagesegmentation. In3DV,2016. 5 2023. 2
[63] Rohit Mohan and Abhinav Valada. Efficientps: Efficient [79] Shilin Xu, Xiangtai Li, Jingbo Wang, Guangliang Cheng,
panopticsegmentation. IJCV,2021. 3 YunhaiTong,andDachengTao. Fashionformer:Asimple,
[64] JuntingPan,AdrianBulat,FuwenTan,XiatianZhu,Lukasz effectiveandunifiedbaselineforhumanfashionsegmenta-
Dudziak, Hongsheng Li, Georgios Tzimiropoulos, and tionandrecognition. ECCV,2022. 2
BraisMartinez. Edgevits:Competinglight-weightcnnson [80] BinYan,YiJiang,JiannanWu,DongWang,ZehuanYuan,
mobiledeviceswithvisiontransformers. ECCV,2022. 3 PingLuo,andHuchuanLu. Universalinstanceperception
[65] AdamPaszke, SamGross, FranciscoMassa, AdamLerer, asobjectdiscoveryandretrieval. InCVPR,2023. 1,2
JamesBradbury,GregoryChanan,TrevorKilleen,Zeming [81] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: an segmentation. InICCV,2019. 2,3,6
imperativestyle,high-performancedeeplearninglibrary.In [82] ShushengYang,YuxinFang,XinggangWang,YuLi,Chen
NeurIPS,2019. 6 Fang, Ying Shan, Bin Feng, and Wenyu Liu. Crossover
[66] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey learning for fast online video instance segmentation. In
Zhmoginov,andLiang-ChiehChen.Mobilenetv2:Inverted ICCV,2021. 3
residualsandlinearbottlenecks. InCVPR,2018. 3 [83] Changqian Yu, Changxin Gao, Jingbo Wang, Gang Yu,
[67] Jonas Schult, Francis Engelmann, Alexander Hermans, ChunhuaShen,andNongSang. Bisenetv2: Bilateralnet-
Or Litany, Siyu Tang, and Bastian Leibe. Mask3D: work with guided aggregation for real-time semantic seg-
MaskTransformerfor3DSemanticInstanceSegmentation. mentation. IJCV,2021. 3
ICRA,2023. 2 [84] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao,
[68] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng GangYu,andNongSang. Bisenet: Bilateralsegmentation
Xu,WeiZhan,MasayoshiTomizuka,LeiLi,ZehuanYuan, network for real-time semantic segmentation. In ECCV,
ChanghuWang,andPingLuo. SparseR-CNN:End-to-end 2018. 3,4
objectdetectionwithlearnableproposals. CVPR,2021. 5, [85] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,
7 YukunZhu,HartwigAdam,AlanYuille,andLiang-Chieh
[69] ShuyangSun,WeijunWang,QihangYu,AndrewHoward, Chen. k-meansmasktransformer. InECCV,2022. 2,6,9
PhilipTorr,andLiang-ChiehChen. Remax: Relaxingfor [86] HaoboYuan, XiangtaiLi, YiboYang, GuangliangCheng,
better training on efficient panoptic segmentation. arXiv JingZhang,YunhaiTong,LefeiZhang,andDachengTao.
preprintarXiv:2306.17319,2023. 3 Polyphonicformer: Unifiedquerylearningfordepth-aware
[70] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob videopanopticsegmentation. ECCV,2022.
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, [87] Haobo Yuan, Xiangtai Li, Chong Zhou, Yining Li, Kai
andIlliaPolosukhin. Attentionisallyouneed. InNIPS, Chen,andChenChangeLoy. Open-vocabularysam: Seg-
2017. 5 ment and recognize twenty-thousand classes interactively.
[71] Qiang Wan, Zilong Huang, Jiachen Lu, Gang Yu, and Li arXivpreprint,2024. 2
Zhang.Seaformer:Squeeze-enhancedaxialtransformerfor [88] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-
mobilesemanticsegmentation. InICLR,2023. 3,4 contextual representations for semantic segmentation. In
[72] HuiyuWang,YukunZhu,HartwigAdam,AlanYuille,and ECCV,2020. 2
Liang-Chieh Chen. Max-deeplab: End-to-end panoptic [89] ChaoningZhang,DongshenHan,YuQiao,JungUkKim,
segmentationwithmasktransformers. InCVPR,2021. 5 Sung-Ho Bae, Seungkyu Lee, and Choong Seon Hong.
[73] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, Fastersegmentanything:Towardslightweightsamformo-
and Lei Li. Solo: Segmenting objects by locations. In bileapplications. arXivpreprintarXiv:2306.14289,2023.
ECCV,2020. 5 3
[74] XinlongWang, WenWang, YueCao, ChunhuaShen, and [90] Jiangning Zhang, Xiangtai Li, Jian Li, Liang Liu, Zhu-
TiejunHuang.Imagesspeakinimages:Ageneralistpainter cunXue, BoshenZhang, ZhengkaiJiang, TianxinHuang,
14Yabiao Wang, and Chengjie Wang. Rethinking mobile
blockforefficientattention-basedmodels. InICCV,2023.
3
[91] RenhongZhang, TianhengCheng, ShushengYang, Haoyi
Jiang, Shuai Zhang, Jiancheng Lyu, Xin Li, Xiaowen
Ying, Dashan Gao, Wenyu Liu, et al. Mobileinst: Video
instance segmentation on the mobile. arXiv preprint
arXiv:2303.17594,2023. 3,4
[92] Wenqiang Zhang, Zilong Huang, Guozhong Luo, Tao
Chen, Xinggang Wang, Wenyu Liu, Gang Yu, and Chun-
huaShen. Topformer: Tokenpyramidtransformerformo-
bilesemanticsegmentation. InCVPR,2022. 3,6
[93] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and
Chen Change Loy. K-net: Towards unified image
segmentation. InNeurIPS,2021. 2,4,5,7,9,10
[94] XiangyuZhang,XinyuZhou,MengxiaoLin,andJianSun.
Shufflenet:Anextremelyefficientconvolutionalneuralnet-
workformobiledevices. InCVPR,2018. 3
[95] HengshuangZhao,XiaojuanQi,XiaoyongShen,Jianping
Shi,andJiayaJia. Icnetforreal-timesemanticsegmenta-
tiononhigh-resolutionimages. InECCV,2018. 3,4
[96] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang
Wang, and Jiaya Jia. Pyramid scene parsing network. In
CVPR,2017. 2
[97] Xu Zhao, Wenchao Ding, Yongqi An, Yinglong Du, Tao
Yu, MinLi, MingTang, andJinqiaoWang. Fastsegment
anything,2023. 3
[98] BoleiZhou, HangZhao, XavierPuig, SanjaFidler, Adela
Barriuso,andAntonioTorralba.Semanticunderstandingof
scenesthroughtheADE20Kdataset. CVPR,2017. 6
[99] ChongZhou,XiangtaiLi,ChenChangeLoy,andBoDai.
Edgesam:Prompt-in-the-loopdistillationforon-devicede-
ploymentofsam. arXivpreprintarXiv:2312.06660,2023.
3
[100] QianyuZhou,XiangtaiLi,LuHe,YiboYang,Guangliang
Cheng, Yunhai Tong, Lizhuang Ma, and Dacheng Tao.
Transvod: End-to-endvideoobjectdetectionwithspatial-
temporaltransformers. TPAMI,2022. 3
15