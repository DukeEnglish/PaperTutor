AUTOFT: Robust Fine-Tuning by Optimizing Hyperparameters on OOD Data
CarolineChoi∗1 YoonhoLee∗1 AnnieChen1 AllanZhou1 AditiRaghunathan2
ChelseaFinn1
1StanfordUniversity 2CarnegieMellonUniversity
{cchoi1, yoonho}@cs.stanford.edu
Abstract
Near-Optimal Parameters
for Each Data Distribution
Foundationmodelsencoderichrepresentationsthatcan
Pre-Trained
beadaptedtoadesiredtaskbyfine-tuningontask-specific
Parameters OOD 1
data. However,fine-tuningamodelononeparticulardata
d mi ast nr cib eu oti non oto hf ete rn dc iso tm ribp uro tim onis se .s Cth ue rrm eo nd te ml’ es tho ori dg sin foa rlp roer bf uo sr t-
Standard
AutoFT
Fine-Tuning
fine-tuningutilizehand-craftedregularizationtechniquesto
constrainthefine-tuningprocesstowardsthebasefounda-
tionmodel. Yet,itishardtopreciselyspecifywhatcharac- OOD Val
teristicsofthefoundationmodeltoretainduringfine-tuning,
OOD 2
as this depends on how the pre-training, fine-tuning, and ID
evaluationdatadistributionsrelatetoeachother. Wepro-
pose AUTOFT, a data-driven approach for guiding foun- Figure1.OverviewofAUTOFT:amethodforrobustlyfine-tuning
dation model fine-tuning. AUTOFT optimizes fine-tuning foundationmodels.Whilefine-tuningwithin-distribution(ID)data
hyperparameterstomaximizeperformanceonasmallout- (blue), AUTOFT optimizeshyperparametersforpost-adaptation
of-distribution(OOD)validationset. Toguidefine-tuningin performance on a small out-of-distribution validation set (red).
agranularway,AUTOFTsearchesahighlyexpressivehyper- Thisvalidationsetservesasaproxyforperformanceondifferent
parameterspacethatincludesweightcoefficientsformany
distributions(greenandpurple),encouragingAUTOFTtolearna
robustfine-tuningprocedure.
differentlosses,inadditiontolearningrateandweightdecay
values. WeevaluateAUTOFTonninenaturaldistribution
shiftswhichincludedomainshiftsandsubpopulationshifts.
(OOD) data. This indicates that conventional fine-tuning
OurexperimentsshowthatAUTOFTsignificantlyimproves
strategiescanfailtoutilizethepriorknowledgeembedded
generalization to new OOD data, outperforming existing
inthefoundationmodel.
robustfine-tuningmethods. Notably,AUTOFTachievesnew
Thisissueofconventionalfine-tuningdistortingbenefi-
state-of-the-artperformanceontheWILDS-iWildCamand
cialfoundationmodelpriorshasdrivenrecentresearchon
WILDS-FMoWbenchmarks,outperformingthepreviousbest
developingrobustfine-tuningmethods.Suchmethodsaimto
methodsby6.0%and1.5%,respectively.
produceanadaptedmodelthatachievesgoodperformance
underdistributionshiftsbypreservingthepriorknowledge
embeddedinthefoundationmodel. Priorworkshavepro-
1.Introduction
posed various regularization techniques for this purpose,
Foundationmodelshaveemergedasapowerfultoolinma- suchasensemblingmodelsbeforeandafteradaptation[92]
chinelearning,demonstratingunprecedentedperformance orinitiallyfittingonlythelastlayer[50]. However,asthese
acrossawidevarietyofdatadistributions [40,41,72]. By methodsareprimarilybasedonhumanintuition,theymay
pre-trainingonlargeanddiversedatasets,thesemodelslearn not fully account for the complex interplay between the
representationsthatcanserveasrichcommon-sensepriors foundationmodelpriorsandtheadaptationprocess.
that complement task-specific data. We thus expect fine- WeintroduceAUTOFT,anovelmethodforrobustfine-
tuning to enhance the generalization capabilities of foun- tuningthataimstofindtherighttradeoffbetweentheprior
dation models. However, fine-tuning often degrades the andthefine-tuningdatathroughhyperparameteroptimiza-
performance of foundation models on out-of-distribution tion.Ourkeyinsightisthatwecanlearnwhatcharacteristics
1
4202
naJ
81
]VC.sc[
1v02201.1042:viXraStandard Fine-Tuning with OOD Evaluation AutoFT
ID Dist OOD Dist
Val Data Val Data
ID Dist
 ID Dist

Train Data Train Data
Evaluate on
 Evaluate on
Hyperparameter
 Hyperparameter

Unseen Dists Unseen Dists
Optimization Optimization
Fine-Tuning Fine-Tuning
Figure2. Asummaryofourdataassumptionsandevaluationprotocol.Thestandardapproachistooptimizehyperparametersonavalidation
datasetdrawnfromthesamedistributionasthetrainingdata.Incontrast,AutoFTemploysasmallout-of-distribution(OOD)validataionset
forhyperparameteroptimization,enhancingthegeneralizabilityofthefinalmodel.Weevaluateallfine-tunedmodelsondatafromunseen
distributionshifts(greenandpurple).
ofthefoundationmodeltopreserveduringfine-tuningus- 2.RelatedWork
ingadata-drivenapproach. Likeexistingrobustfine-tuning
AutoML and hyperparameter optimization. Our work
methods,wefine-tuneafoundationmodelontask-specific
leverages high-level ideas from the broader literature on
data,andthenevaluatetheresultingmodelonasetofOOD
meta-learningandhyperparameteroptimization. Suchmeth-
distributions. However, we additionally leverage a small
odshaveproposedtooptimizedifferentpartsofthetraining
OODvalidationsetwithupto1000labeledexamplesfrom
pipeline,includinggeneralhyperparameters[11,26,39,58,
oneunseendistribution; weoptimizefine-tuninghyperpa-
59],networkarchitectures[62,74,93,100,101],augmenta-
rametersforpost-adaptationperformanceonthisOODvali-
tionpolicies[19,20,34,60],optimizers[4,10,15,65,89],
dationset. Importantly,theOODvalidationsetisonlyused
andobjectivefunctions[8,46,68,97]. However, mostof
forhyperparameteroptimization,notfine-tuning,anddoes
these works optimize for generalization within the train-
notfollowthesamedistributionastheOODtestsets. We
ingdistributionanddonotconsiderrobustnesstodistribu-
illustratetheintuitionbehindourapproachinFigure1and
tion shifts. Existing works that optimize a training proce-
ourdataassumptionsinFigure2.
dureforOODgeneralizationconsiderastructuredfew-shot
Wemaketwokeyalterationstostandardhyperparame- adaptationsetting[56,99],limitingtheirscalabilitytolarge
teroptimization,whichwefindtobecriticalforautomatic datasets.
robustfine-tuning. First,asmentionedabove,weoptimize
Transferlearning. Whileearlyresearchdemonstratedthe
hyperparameterswithrespecttoanOODvalidationsetrather
effectivenessofusingfeatureslearnedfrompre-trainingon
thananIDvalidationset. Second,weuseabroaderdefini-
largedatasetsfornewtasks,transferlearninghasevolved
tionof“hyperparameter”:beyondtheusualhyperparameters
to focus on optimizing performance in limited data sce-
suchaslearningrate,weparameterizethefine-tuningobjec-
narios[69,81,96]. Commontransferlearningtechniques
tiveitselfthroughweightcoefficientsforseveraldifferent
include fine-tuning with regularization [1, 29, 42, 43, 52,
loss functions and regularizers. This larger hyperparame-
57,82,94,98]andselectivelyfreezingpre-trainedparame-
tersearchspacegivesAUTOFTmoregranularcontrolover
ters[17,22–24,33,45,51,53,54,63,73,77,85]. However,
adaptation.
aspre-trainedmodelsarefine-tunedforaspecificdistribu-
tion,theireffectiverobustnessdecreasesatconvergence[3].
WerigorouslyevaluateAUTOFTonawidearrayofreal-
Weintroduceatransferlearningframeworkthatpreserves
world datasets and consider various types of distribution
the robustness of the pre-trained model while adapting to
shifts,includingsubpopulationanddomainshift.Ourexperi-
newtasks.
mentsshowthatourapproachresultsinbettergeneralization
to unseen OOD data. With at most 1000 datapoints from Out-of-distributiongeneralization. Maintaininggoodper-
anOODdistribution,AUTOFToutperformsexistingrobust formanceondatathatdeviatesfromthetrainingdistribution
fine-tuningmethodsacrossallbenchmarks. Thesegainsin iscrucialinmanyreal-worldapplications,wheremodelsmay
robustnessareachievedwithminimaladditionalcompute, facedatafromunfamiliarenvironments[28,32,35,47]. Nu-
requiringatmost5%moretotalcomputecomparedtostan- merousstudieshaveinvestigatedhowtoensurerobustnessto
dardfine-tuning. Amongotherresults, AUTOFT achieves variousdistributionshifts[3,5,14,18,36,55,61,80,86,90].
newstate-of-the-artperformanceonthechallengingiWild- Some works have shown that despite the myriad of ways
CamandFMoWbenchmarks [9,16,47],outperformingthe inwhichdatadistributionscanchange,naturallyoccurring
priorbestmethodsby6.0%and1.5%,respectively. distributionshiftshaveasurprisinglypredictableeffecton
2model performance [6, 66, 83], suggesting that it may be mizationproblemas
possibletolearnhowtoberobusttounseennaturaldistri-
butionshifts. Goyaletal.[31]isrelatedtoourwork: they ValidationSetPerformance
dynamicallyadaptmodelstodistributionshiftsviaameta- ϕ∗ =argmaxE(cid:2) P(cid:122) erf(LearnAl(cid:125) g(cid:124) (ϕ,D ),D (cid:123) )(cid:3) . (1)
tr val
learned test-time adaptation loss applied to unlabeled test ϕ∈Φ (cid:124) (cid:123)(cid:122) (cid:125)
LearnedParameters
data. AUTOFT instead focuses on fine-tuning foundation
models and does not require any test data during training. Here,theexpectationistakenoveranyrandomnessinthe
AUTOFTimprovesgeneralizationtonoveldistributionshifts learningalgorithmLearnAlg,suchasinputdatashufflingor
afterfine-tuningusingonlyasmallOODvalidationset. random initialization. Theoptimized hyperparameters ϕ∗
aresubsequentlyusedtotrainthemodel.
Robust fine-tuning. Foundation models trained on mas-
Priorhyperparameteroptimizationmethodstypicallystart
sive datasets encode a broad range of general knowledge,
withrandomlyinitializedmodelparametersanduseaval-
enabling robust performance across various data distribu-
idation set D , drawn from the same distribution as the
tions,includingOODscenarios[13,72]. Whileinprinciple, val
training data, to adjust hyperparameters. The problem of
foundation models should serve as a useful prior for fur-
robustfine-tuning,however,beginswithpre-trainedmodel
therfine-tuning,empiricalevidenceshowsthatfine-tuning
parameters and aims to achieve high performance on test
foundation models on a new task often leads to a signif-
data that diverges from the original training data’s distri-
icant drop in OOD performance [3]. Recent works have
bution. In the next section, we describe how we modify
proposedmodificationstothebasicfine-tuningprocedureto
thestandardhyperparameteroptimizationloopforrobustly
improveOODgeneralization[30,50,67,91,92]. Instead
fine-tuningpre-trainedfoundationmodels.
ofhand-designingaregularizationtechnique,weproposea
data-drivenapproachtolearnamorenuancedfine-tuning
procedure. In fact, some prior works [30, 54, 94] can be
4. AUTOFT:RobustFine-TuningviaHyperpa-
seenasspecialcasesofAUTOFTsinceourhyperparameter rameterOptimization
searchspaceencompassesthesefine-tuningalgorithms. Our
experimentsinSec.6demonstratethatAUTOFTconsistently
Inthissection,wepresentAUTOFT,adata-drivenmethod
outperformspriorrobustfine-tuningmethodsonOODdata.
for robustly fine-tuning large pre-trained models. AUT-
OFT aims to find hyperparameters for fine-tuning a pre-
3.HyperparameterOptimization trainedmodelthatstrikeabetterbalancebetweenthegeneral-
purposeknowledgeencodedinthepre-trainedmodeland
We begin by formalizing hyperparameter optimization, a
thetask-specificknowledgeinthetrainingdata. Toachieve
procedurethatweextendforrobustlyfine-tuningfoundation
this, AUTOFT optimizes hyperparameters with respect to
modelsinSec.4. Hyperparametersarepredefinedproper-
aperformancemetricthatissensitivetodegradationunder
tiesofthelearningalgorithmwhicharenotlearnedduring
distributionshift.Specifically,weleverageasmallvalidation
training, such as network architecture, learning rate, and
setfromadifferentdistributionfromthefine-tuningdata;
regularizationstrength. Sincehyperparameterssignificantly
performanceonthisOODvalidationsetservesasaproxy
impacttheperformanceofthefinalmodel, itiscrucialto
forgeneralOODperformance.
choosetherighthyperparametersforanylearningproblem.
Itisdifficulttoknowtheoptimalhyperparametersapriori,
4.1.ProblemSetting: RobustFine-Tuning
astheyareinfluencedbymanypropertiesoftheproblemset-
ting,includingthedatadistributionanddesiredperformance Duringhyperparameteroptimization,weassumeaccessto
metric. two datasets: (1) a large fine-tuning dataset D from the
tr
Formally,wedenotethelearningalgorithmasLearnAlg trainingdistributionP ,and(2)asmallheld-outOODval-
tr
anditshyperparametersasϕ∈Φ,whereΦisthehyperpa- idationsetD fromadifferentdistributionP . Thevali-
val val
rameterspace. Wealsodenotethetrainingandvalidation dationsetD ismuchsmallerthanD ,andisusedexclu-
val tr
datasetsasD andD ,respectively. Thesedatasetsaredis- sivelyforhyperparameteroptimization,notforfine-tuning.
tr val
jointandaretypicallydrawnfromthesamedistribution. We Afterhyperparameteroptimization,wefine-tuneonD with
tr
denotetheresultingmodelasLearnAlg(ϕ,D ),toexplicitly thebestfoundhyperparametersandthentestthefine-tuned
tr
represent the dependence on both hyperparameters ϕ and modelonseveralOODdistributionsP1 ,P2 ,.... These
ood ood
trainingdataD . Thegoalofhyperparameteroptimization testdistributionsaredifferentfrombothP andP ,andare
tr tr val
istofindhyperparametersthatmaximizesomeperformance notshowntothemodelduringeitherfine-tuningorhyper-
metricPerf(f,D )whichdependsonthemodelf andthe parameteroptimization. Wevisualizethisdataassumption
val
validationdatasetD . Examplesofperformancemetrics inFigure2.
val
includetop-1accuracy, macroF1score, andworst-region Let f denote a pre-trained model with parameters θ,
accuracy. We can formally state the hyperparameter opti- which we adapt to the task at hand by fine-tuning on D .
tr
3initialfoundationmodelwithhyperparametersϕ∗,andeval-
Algorithm1 AUTOFT
uateitsperformanceonnovelOODdistributionstoassess
InputHyperparameteroptimizerHPO itsgeneralizationcapabilities. Wesummarizethisprocedure
InputIDtrainingdataD tr,OODvalidationdataD val inAlgorithm1.
forϕ←HPO.Sample()do
f ←LearnAlg(D ,ϕ) //Fine-tunemodel Hyperparameterspace. Weconsideralargerhyperparame-
ft tr
p←Perf(f ,D ) //Evaluate terspaceΦthanwhatistypicallyconsidered.Thisisbecause
ft val
HPO.Update(ϕ,p) //HPOUpdate forrobustfine-tuning,thealgorithmmustselectivelyignore
endfor someaspectsofthetrainingdistributioninordertobetter
ϕ∗ ←HPO.Best() //Getbesthyperparameters generalizetonovelOODdistributions. Toenablethismore
θ∗ ←LearnAlg(D ,ϕ∗) //Fine-tunefinalmodel nuancedbehavior,weincreasetheexpressivityofthehyper-
tr
parameteroptimizationprocessbyexpandingthehyperpa-
rameterspaceΦ. Themainnoveltyofourhyperparameters
Weconsiderfoundationmodelsf thatpossessrich,general- spaceisthatweallowthehyperparameterspaceΦtoexpress
purposeknowledgefrombeingpretrainedonlarge,diverse thefine-tuningobjectiveitselfbylearningtoweightbetween
datasets. Letϕ∈Φrepresentthehyperparametersforfine- differentlossfunctionsandregularizers. Forexample,the
tuning, andletLearnAlg(ϕ,D tr)denotethefine-tuningal- cross-entropyandhingelossessimilarlyguidethemodel’s
gorithmwhichproducesadaptedmodelparameters. Inthis predictions towards the correct label, but with a different
work,thefine-tuningalgorithmLearnAlgisstochasticgra- amount of penalty for severely incorrect samples. As an-
dientdescent. TheperformancemetricPerf differsacross otherexample,contrastivelossonimage-textpairs[30,72]
theexperimentsweconsider: itistop-1accuracyformost alsoguidesthemodel’spredictionstowardsthecorrectlabel,
settings. butinawaythatincorporateslanguageembeddingswhich
Weassumethatalldatadistributionspertaintothesame isentirelydifferentfromhowthecross-entropyandhinge
task,withdifferencesarisingfromdomainshiftscausedby lossesoperate. Similarly,differentlossesforreducingover-
naturalvariationinthedatacollectionprocess. Forexample, confidenceorconstrainingmodelparameterstowardstheir
all data may be for the same task of recognizing animals initialvalueshavedifferenteffectsonthemodel’sadaptation
fromcameratrapimages,witheachdistributioncorrespond- process.
ingtoadifferentsetoflocations[9,47]. Wepositthatusing Ashyperparameters,weconsiderweightcoefficientsfor
onesmallOODvalidationsetD val canguidethemodelto ninedifferentlossfunctionsandregularizers: cross-entropy
generalize better to various new OOD distributions. This loss,hingeloss,image-textcontrastiveloss,entropy,confi-
validationsetreflectsthetypeofdistributionshiftsthatthe denceminimization,L1norm,L2norm,L1andL2distance
modelmightencounter. Performanceonthissetcanserve topretrainedmodelparameters. Weselecttheselossestoad-
asaneffectiveproxyforperformanceonnovelOODdistri- dressvariousaspectsofmodellearningandgeneralization,
butions. OurexperimentsinSec.6confirmthat AUTOFT such as classification accuracy, decision confidence, and
cansuccessfullyleverageonesmallOODvalidationsetto preventionofoverfitting,ensuringanuancedandeffective
improvegeneralizationtonoveldistributions,outperforming modeladaptation. Wedenotetheselossweightcoefficients
state-of-the-artmethodsforrobustfine-tuning. asW ={w ,w ,...,w }. Denotingthei-thlossfunction
1 2 9
or regularizer as L , the total loss L is the weighted sum
i
4.2.HyperparameterOptimizationLoop L=(cid:80)9
w L . Toenablefinercontroloverthespeedand
i=1 i i
stability of adaptation during fine-tuning, we additionally
Formulation. Weformalizeourhyperparameteroptimiza-
learnhyperparametersforthefine-tuningoptimizer:learning
tionasfollows. GivenahyperparameterspaceΦforafine-
rateηandweightdecayδ. Foranevenlargerhyperparame-
tuningalgorithmLearnAlg,wesolvetheoptimizationprob-
terspace,weoptionallyconsiderper-layerhyperparameters
lemin(1)tofindhyperparametersϕ∗suchthatthefine-tuned
for the learning rate, weight decay, and parameter norm
modelf′ = LearnAlg(ϕ∗,D )performswellontheOOD
tr weights;wecallthisvariant“per-layer”inourexperiments
validationsetD . Practically,thisinvolvesfine-tuningthe
val inAppendixA.Ourcompletesetofhyperparametersisthus
samefoundationmodelmultipletimes,eachtimewithdif-
ϕ=(W,η,δ).
ferenthyperparametersϕ1,ϕ2,...toobtaindifferentfinal
fine-tunedmodelsf1 = LearnAlg(ϕ1,D ),f2 = ···. We Hyperparameteroptimizationalgorithm. Findingeffec-
tr
then judge the quality of each set of hyperparameters ϕi tivehyperparametersforrobustfine-tuningrequiresanopti-
bytheperformanceoffi ontheOODvalidationset. The mizerthatcanefficientlysearchourlarge,high-dimensional
hyperparameteroptimizationalgorithmaggregatestheinfor- hyperparameterspaceΦ. WeusetheTree-structuredParzen
mationfromthesedifferentfine-tuningrunsandestimates Estimator [12, TPE] as the hyperparameter optimization
thebesthyperparametersϕ∗. Attesttime,wefine-tunethe algorithm, as its design allows for efficient processing in
4Good Prior Misspecified Prior Fine-tuning Learning Curves Learned Weights
105 105
Dim-wise Weight
Global Weight 104
10 2
104 104
103
Dim-wise Avg
103 103
102 G Dil mob -a wl
ise 10 3
0 20 40 0 20 40 0 5 10 15
Hyperparameter Trials Hyperparameter Trials Fine-tuning Steps ID OOD Both None
Figure3. Didacticexperiment. Lefttwo: learningcurvesforfittingaGaussiandistributiontotoydata. Hyperparameteroptimization
withalargerhyperparameterspace(red)leadstobetterOODperformanceonlywhenthepriorisinformative.Right:fine-tuninglearning
curveswithlearnedhyperparameters.Averagingthedimension-wiseweights(yellow)resultsinmassivelyoverfittingtotheIDdata(solid)
andunderperformingOOD(dashed).Thedimension-wisehyperparameters(red)showthebestgeneralization.Farright:visualizationof
learnedlossweightsw foreachdimension.ThelearnedweightsarelowestforthedimensionswhereIDalonediffersfromtheprior(red),
i
indicatingthatthemodellearnstoignoretheIDdatainthesedimensions.
high-dimensionalspacesandyieldseffectiveresultsatthe prior,weconsiderasimpleexperiment. Wefocusonsyn-
orderofhundredsofevaluations. TPEcreatesaprobabilis- theticdatasetsofvectorsdrawnfromzero-meanGaussian
tic model from previous hyperparameter evaluations, and distributionswithdifferentvariances. Weassumethatthe
differentiatesbetweenbetterandworsehyperparametercon- priorunitvariancedistributionishelpfulbutthatthedata
figurationsbycomparingtheirlikelihoodundertwoseparate exhibitsdeviationfromthisprioralongafewdimensions.
models. We use the TPE implementation of the optuna A detailed summary of our data and training setup is in
library[2]withnofurthermodifications. AppendixB.1. Weconsider“fine-tuning”asstartingfrom
AsthepriordistributionforTPE,weuseanappropriately the prior distribution. In this toy model, dimension-wise
scaled distribution for each hyperparameter, operating in weighthyperparametersw dictatethedegreeofinfluence
i
the log-space for the weight coefficients and the learning ofthepriorversusthetrainingdataoneachdimension,anal-
ratesincetheoptimalvaluescanvaryoverseveralordersof ogoustohowthefine-grainedhyperparametersinAUTOFT
magnitude: balancethefoundationmodelpriorandtrainingdataalong
differentaspectsoffine-tuning. AsinAUTOFT,weusethe
w ∼LogUniform(w ,w ),
i min max TPEalgorithmtooptimizetheweighthyperparametersw
i
η ∼LogUniform(η , η ),δ ∼Uniform(0.0,1.0).
min max tomaximizethelog-likelihoodofthevalidationdataD .
val
We find that in practice, (w ,w ) = (10−4,10) is an ResultsinFigure3showtheeffectofstandardhyperpa-
min max
effective range for all w . Given a model’s conventional rameter tuning (global weight) versus considering an ex-
i
learningrateη∗,weset(η ,η )=(10−2η∗,102η∗). panded hyperparameter space as in AUTOFT (dim-wise
min max
weight). Learningdimension-wiseweightsiseffective,but
Computationalcost. ThehyperparametertuningofAUT-
onlywhenthepriordistributioniswell-specifiedandpro-
OFTintroducesminimalcomputationaloverhead,requiring
videsabettersignalforOODgeneralizationthanthetrain-
atmost5%additionalcomputecomparedtoonestandard
ing set along some dimensions, as foundation models do
fine-tuningrun. Eachevaluationofhyperparametersonthe
forgeneralfine-tuningtasks. Thisadditionalexpressivityis
OOD validation set requires only a few inner loop gradi-
necessarytospecifyabetteradaptationprocedure: naively
ent steps on the fine-tuning dataset. For example, on the
averagingthelearneddim-wiseparameterscausesthemodel
iWildCamdataset,AUTOFTusesonly500hyperparameter
tooverfittheIDdata. Finally,throughinspectingthelearned
evaluations,eachrequiringonly10gradientstepsonthefine-
weights,weseethatthelearneddim-wiseweightsimprove
tuningdata. With1.3milliontrainingexamplesandabatch
OODgeneralizationbydownweightingtheinfluenceofthe
sizeof256,AUTOFTrequiresonly5000additionalgradient
IDdatainthedirectionswhereonlytheIDdatadiffersfrom
steps, compared to approximately 100,000 gradient steps
theprior(nottheOODdata).
forfinalfine-tuning. AUTOFTisthereforecomputationally
inexpensive,andhighlypracticalforevenlargefine-tuning
problemsettings. 5.ExperimentalSetup
4.3.IntuitionandDidacticExperiment
Distributionshifts. Inouranalysis,wefocusonnaturaldis-
Toprovideintuitionforhowhyperparameteroptimization tributionshifts,definedbyTaorietal.[83]asshiftsarising
can lead to more effective utilization of a partially useful fromreal-worldvariations,suchaschangesinlighting,ge-
5
)ataD
DOO(
LLN
)ataD
DOO(
LLN
thgieWography,andimagestyles. Thisapproach,inlinewithprior Method Architecture ID OOD
works on robust fine-tuning [30, 50, 72, 92], emphasizes
GroupDRO ResNet50 37.5(1.9) 23.8(2.0)
shiftsthatarerepresentativeofreal-worldscenarios.
ABSGD ResNet50 47.5(1.6) 33.0(0.6)
Ourmainresultsincludefourdistributionshifts,includ-
Copy-Paste ResNet50 50.2(1.6) 36.5(0.9)
ingreal-worlddistributionshiftsarisinginwildliferecogni-
ERM PNASNet 52.8(1.4) 38.5(0.6)
tionandsatelliteimagery,andtwoCIFAR-10-deriveddis-
ERM ViTL 55.8(1.9) 41.4(0.5)
tributionshifts. TheWILDS-iWildCamdataset[9,47,79]
ModelSoups ViTL 57.6(1.9) 43.3(1.0)
presentsdistributionshiftsarisingfromvariationsincam-
FLYP ViTL-336px 59.9(0.7) 46.0(1.3)
eratraplocations,lightingconditions,andanimalbehaviors
across various geographic regions. The WILDS-FMoW AUTOFT ViTL-336px 63.5(0.5) 52.0(0.4)
dataset[16,47,79]presentsdistributionshiftsarisingfrom
changesintime,geographiclocations,andlanduse. Finally, Table1. iWildCamresults. AUTOFT withweightensembling
we evaluate on two CIFAR-10 derived distribution shifts: attainsstate-of-the-artOODperformanceontheWILDS-iWildCam
CIFAR-10toCIFAR-10.1andCIFAR-10.2,whichinvolve benchmarkwithaViT-L/14-336pxbackbone,surpassingall
subtlechangesinimagecharacteristicsandcomposition. priorentriesontheWILDSleaderboard[47].
Foundation models and CLIP. We fine-tune pre-trained
Method Architecture ID OOD
CLIP models [72], including those provided in the
open-cliprepository[40]. WeusetheCLIPViT-B/16 GroupDRO DenseNet121 51.2(0.4) 31.1(1.7)
model from OpenAI as our default model, unless spec- LISA DenseNet121 52.8(1.2) 35.5(0.8)
ified otherwise. Evaluation on CIFAR-10 uses the ERMw/aug DenseNet121 55.5(0.4) 35.7(0.3)
CLIP ViT-L/14 model from OpenAI, in line with [92]. DFR DenseNet121 53.4(0.4) 42.8(0.4)
Our SoTA results on WILDS-iWildCam use the CLIP ERM ViTL 66.9(0.2) 46.1(0.6)
ViT-L/14-336pxmodelfromOpenAI.Weusetexttem- ModelSoups ViTL 69.5(0.1) 47.6(0.3)
platesusedinpriorwork[72,92]togeneratezero-shotfinal Freeze-Embed ViTL-336px 68.3(0.4) 50.3(1.1)
layerweightsforalldatasets.
AUTOFT ViTL-336px 72.1(0.1) 51.8(0.4)
Effectiverobustnessandweightensemblingcurves. We
usetheeffectiverobustnessframeworkbyTaorietal.[83] Table2.FMoWresults.AUTOFTwithweightensemblingattains
toevaluatemodelrobustnessbasedonaccuracyexceeding state-of-the-artOODperformanceontheWILDS-FMoWbench-
markwithaViT-L/14-336pxbackbone,surpassingallprior
a baseline trained only on the reference distribution. Lin-
entriesontheWILDSleaderboard[47].
early interpolating the weights of a fine-tuned model and
apretrainedmodel(WiSE-FT)hasbeenshowntoimprove
both ID and OOD performance [91]. Hence, we include
anorthogonalwaytootherrobustfine-tuningmethods.
weightensemblingasanadditionalpointofcomparison,and
interpolatetheweightsofmodelsfine-tunedbyeachmethod Trainingprotocol. Wecloselyfollowthetrainingdetails
with10mixingcoefficientsα. ofGoyaletal.[30]andWortsmanetal.[92]. Allmethods
fine-tunemodelswithanAdamWoptimizer,cosinelearning
Baselines. We compare AUTOFT against several meth-
ratescheduler,andabatchsizeof512forImageNetand256
ods for adapting pretrained models. We include two stan-
forallotherdatasets. Allbaselinehyperparameters,suchas
dardtransferlearningmethodsthatminimizecross-entropy
learningrate,weightdecay,andwarmuplength,aretuned
loss: linear probing (LP) and full fine-tuning (FT). We
throughgridsearch. Allmethods,includingAUTOFT,per-
alsocomparewithrecentworksinrobustfine-tuning: L2-
formearlystoppingbasedonin-distribution(ID)validation
SP [59], which fine-tunes with an L2 regularization term
accuracy. We provide a comprehensive breakdown of the
towardspretrainedweights;LP-FT[50],whichperformslin-
hyperparametersweepsinthesupplementarymaterial. We
earprobingfollowedbyfullfine-tuning;Freeze-Embed[51],
emphasizethatnoneofthesemethods,includingAUTOFT,
whichfreezestheembeddinglayerduringfine-tuning;and
observeanyofthetestOODdistributionsduringtraining.
FLYP[30],whichfine-tuneswiththeCLIPpretrainingloss
Finally, wereportmetricsaveragedover5runswith95%
– a contrastive loss between image embeddings and class-
confidenceintervals.
descriptive prompt embeddings. As we are interested in
OODperformance,wealsocompareagainstOODgeneral-
6.Results
izationmethodsincludingGroupDRO[78],ABSGD[70],
LISA [95], DFR [44], and Copy-Paste [27]. We addition- Inthissection,wepresentthemainexperimentalfindingsfor
ally evaluate all methods with weight ensembling (WiSE- AUTOFT.First,weshowthatAUTOFTimprovestheperfor-
FT)[92],whichisshowntoimproveOODperformancein manceoffine-tunedmodelsonseverallarge-scale,synthetic
6ImageNet iWILDCam FMoW
WithoutEnsembling WithEnsembling WithoutEnsembling WithEnsembling WithoutEnsembling
Methods ID OOD ID OOD ID OOD ID OOD ID OOD
Zeroshot 68.3(-) 58.7(-) 68.3(-) 58.7(-) 8.7(-) 11.0(-) 8.7(-) 11.0(-) 20.4(-) 18.7(-)
LP 79.9(0.0) 57.2(0.0) 80.0(0.0) 58.3(0.0) 44.5(0.6) 31.1(0.4) 45.5(0.6) 31.7(0.4) 48.2(0.1) 30.5(0.3)
FT 81.4(0.1) 54.8(0.1) 82.5(0.1) 61.3(0.1) 48.1(0.5) 35.0(0.5) 48.1(0.5) 35.0(0.5) 68.5(0.1) 39.2(0.7)
L2-SP 81.6(0.1) 57.9(0.1) 82.2(0.1) 58.9(0.1) 48.6(0.4) 35.3(0.3) 48.6(0.4) 35.3(0.3) 68.6(0.1) 39.4(0.6)
LP-FT 81.8(0.1) 60.5(0.1) 82.1(0.1) 61.8(0.1) 49.7(0.5) 34.7(0.4) 50.2(0.5) 35.7(0.4) 68.4(0.2) 40.4(1.0)
FLYP 82.6(0.0) 60.2(0.1) 82.9(0.0) 63.2(0.1) 52.2(0.6) 35.6(1.2) 52.5(0.6) 37.1(1.2) 68.6(0.2) 41.3(0.8)
AUTOFT 81.8(0.1) 61.5(0.1) 82.4(0.1) 64.3(0.1) 51.0(0.5) 38.3(0.5) 51.3(0.5) 39.3(0.5) 67.1(0.3) 42.3(0.5)
Table3. AUTOFToutperformsallbaselinesbothwithandwithoutensembling.Withoutensembling,AUTOFTimprovesOODperformance
by 1.3% on ImageNet, 3.7% on WILDS-iWildCam, and 6.1% on WILDS-FMoW. These improvements are preserved with weight
ensembling.
ImageNet iWildCam FMoW
0.40
0.67 0.43
0.35
0.30 0.38
0.62 0.25 0.33
0.20 0.28
0.57
0.15 0.23
0.18
5 0 5 0 5 3 8 3 8 3 8 3 8 3 4 9 4 9 4 9 4 9 4 9
6 7 7 8 8 1 1 2 2 3 3 4 4 5 2 2 3 3 4 4 5 5 6 6
0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
ID Avg. Acc. on 5 Distribution Shifts ID Macro F1 ID Worst Region Accuracy
AutoFT FT Best Ensembled Model
FLYP L2-SP Fine-tuned Model Without Ensembling
LP-FT Weight Ensembling Curves Zero-shot Model
Figure4. AUTOFToutperformsexistingmethods,bothwithandwithoutweightensembling[92].Here,weshowtheID-OODperformance
curvesobtainedbylinearlyinterpolatingthefine-tunedmodelweightswiththezero-shotweights.
andnaturaldistributionshifts,includingImageNet,WILDS- respectively. Tab.3containsresultsonImageNet,WILDS-
iWildCam,WILDS-FMoW,andCIFAR.Then,wepresent FMoW,andWILDS-iWildCam;resultsonCIFAR-10arein
additionalexperimentsinthelow-dataandtransferlearning Tab.9. AUTOFTconsistentlyoutperformsallbaselineson
regimes. Finally,weinvestigatethetransferabilityofhyper- novelOODdistributions. ThesegainsinOODperformance
parameterslearnedbyAUTOFTacrossfine-tuningdataset aremaintainedwhenensemblingzero-shotandfine-tuned
andbackbone,andtheeffectofthechoiceofOODvalidation models,followingWortsmanetal.[92]. Wereportweight
distributionforhyperparameteroptimization. Thesefindings ensemblingresultswiththemixturecoefficientthatyields
highlighttheeffectivenessofAUTOFTforrobustfine-tuning thehighestIDvalidationaccuracy.
inavarietyofsettings.
State-of-the-artperformanceonWILDS-iWildCamand
6.1.EvaluationUnderDistributionShifts WILDS-FMoW.Toassesswhetherperformancegainsby
AUTOFT continue to hold on larger foundation models,
Improvements on ImageNet, WILDS, and CIFAR dis- we evaluate AUTOFT with a ViT-L/14@336px model.
tribution shifts. We evaluate AUTOFT on nine natural AsshowninTabs.1and2, AUTOFT achievessignificant
distribution shifts: five from ImageNet, WILDS-FMoW, gainsof6.0%and3.6%inOODandIDmacro-F1,respec-
WILDS-iWildCam,andtwosubtleCIFAR-10distribution tively, over the current leader on the WILDS-iWildCam
shifts. For the OOD validation sets, we use up to 1000 benchmark[47],FLYP[30]. OntheWILDS-FMoWbench-
examplesfromImageNet-C,theOODvalidationsplitsof mark, AUTOFT alsooutperformsthecurrentleader,SGD
WILDS-FMoWandWILDS-iWildCam,andCIFAR-10-C, (Freeze-Embed) [51], by 1.5% and 5.3% in OOD and ID
7
stfihS
noitubirtsiD
5
no
.ccA
.gvA
DOO
1F
orcaM
DOO
ycaruccA
noigeR
tsroW
DOOPatchCamelyon SST2
k(shots) 4 16 32 4 16 32
Zeroshot 56.5(-) 56.5(-) 56.5(-) 60.5(-) 60.5(-) 60.5(-)
LP 60.4(4.0) 64.4(3.7) 67.0(4.4) 60.8(1.8) 61.9(1.4) 62.9(1.3)
FT 63.1(5.5) 71.6(4.6) 75.2(3.7) 61.1(0.7) 62.4(1.6) 63.4(1.9)
LP-FT 62.7(5.3) 69.8(5.3) 73.9(4.6) 60.9(2.4) 62.9(1.9) 63.6(1.4)
FLYP 66.9(5.0) 74.5(2.0) 76.4(2.4) 61.3(2.7) 65.6(2.1) 68.0(1.7)
AUTOFT 68.1(5.1) 76.8(2.9) 79.5(2.0) 65.0(3.8) 67.5(1.1) 69.0(1.1)
Table4. AUTOFTshowssuperiorperformanceinbinaryfew-shotclassification. AUTOFToutperformsFLYPby3.1%andfullfine-tuning
by4.3%in32-shotclassificationonPatchCamelyon.
Methods CalTech-101 StanfordCars Flowers-102
Model Soups [91], which ensembles more than 70 mod-
Zeroshot 87.7(-) 64.4(-) 71.2(-) elsfine-tunedwithLP-FTanddifferentaugmentationsand
LP 94.8(0.0) 83.1(0.0) 95.9(0.0) hyperparameters. AUTOFTalsooutperformsLP-FT,astate-
FT 97.2(0.1) 84.4(0.3) 90.4(0.5) of-the-artbaselineforfine-tuning.
LP-FT 96.9(0.6) 89.4(0.1) 97.9(0.1)
6.2.Few-ShotClassification
FLYP 97.6(0.1) 89.6(0.3) 97.7(0.1)
Inmanyreal-worldapplications,limitedamountsoflabeled,
AUTOFT 99.0(0.1) 89.6(0.2) 97.6(0.2)
task-specific data are available for fine-tuning. Few-shot
Table5. Inadditiontoenhancingrobustness, AUTOFT canim- classificationthusservesasanimportantbenchmarkforeval-
proveperformanceinIIDsettings,wherethereisnodistribution uatingtheutilityoffine-tuningapproaches. Few-shotbinary
shift. Here, AUTOFT uses ID validation examples to optimize classificationisaparticularlychallengingtaskforadaptation,
hyperparameters. AUTOFTachievesthehighestIDperformance giventhesmallnumberoftrainingexamples.Weevaluateon
onCalTech-101andmatchesthebestbaselineonStanfordCars. 4,16,and32shotbinaryclassificationtasksfromthePatch-
CamelyonandRendered-SST2datasets,followingRadford
MNIST CIFAR10.2 CIFAR10.1 etal.[72]andGoyaletal.[30].PatchCamelyoncontainsdig-
italpathologyimagesforthedetectionofmetastatictissue.
Method ID OOD ID OOD ID OOD
Rendered-SST2focusesonopticalcharacterrecognitionfor
FT 99.8 62.3 97.9 96.2 98.3 93.4 classifyingtextsentimentaspositiveornegative.
AUTOFT 99.8 61.1 98.2 96.3 98.4 93.4 AUTOFTdemonstratesstronggeneralizationcapabilities
withlimiteddata,outperformingallbaselinesonallfew-shot
Table6.Performancecomparisonofmodelsfine-tunedonnewdis-
tasksinTab.4. Forexample,AUTOFToutperformsFLYP
tributionswithhyperparameterslearnedbyAUTOFTonadifferent
by3.7%andfullfine-tuningby3.9%inachallenging4-shot
dataset,CIFAR-10.Hyperparameterstransferbettertodatasetsthat
classificationtaskonRendered-SST2.
aremoresimilartotheoriginalfine-tuningdistribution.
6.3.TransferLearning
ValidationDataset CIFAR-10.1 CIFAR-10.2
Instandardtransferlearningsettings,wherenodistribution
CIFAR-10-C 97.4(0.2) 93.5(0.2) shifts exist between train and test data, AUTOFT shows
CIFAR-10 96.9(0.2) 93.2(0.1) strong performance. The primary goal here is to achieve
CIFAR-10.1 96.8(0.3) 92.8(0.2) strongin-distribution(ID)performanceonthetestset.Inthis
CIFAR-10.2 96.7(0.5) 93.8(0.0) setting,AUTOFTusesasmallIDvalidationsettooptimize
hyperparameters.
Table7. PerformanceofAUTOFTwithdifferentvalidationdatasets
WecompareAUTOFTagainstotherbaselinesonseveral
forhyperparameterevaluation.AllAUTOFTrunsaboveoptimize
transferlearningdatasets: CalTech-101,StanfordCars,and
hyperparametersforfine-tuningonCIFAR-10. AUTOFTproduces
Flowers-102 in Tab. 5. AUTOFT matches or outperforms
a robust fine-tuned model with all validation datasets, with the
allbaselineson2ofthe3transferlearningdatasets,outper-
bestresultsachievedwhenthevalidationsetisfromadifferent
distributionthanthefine-tuningdataset,CIFAR-10. formingFLYPby1.4%onCalTech-101.
6.4.AblationStudies
worst-regionaccuracy,respectively. Onbothbenchmarks, Hyperparametertransfer.Arethehyperparameterslearned
AUTOFT additionally outperforms the compute-intensive byAUTOFTforonefine-tuningdistributioneffectivewhen
8appliedtonewfine-tuningdistributions? WefirsttrainAUT- withinAUTOFTcouldofferfurtherinsights. Lastly,explor-
OFT on CIFAR-10, using 100 examples from corrupted ing the possibility of identifying a single set of hyperpa-
CIFAR-10forhyperparameteroptimization. Wethenfine- rametersthatworkseffectivelyacrossmultiplefine-tuning
tuneamodelwiththeselearnedhyperparametersonthree distributionspresentsaninterestingdirectionforfuturework.
newdistributions: MNIST,CIFAR-10.1,andCIFAR-10.2.
Finally, we evaluate the fine-tuned model’s performance 8.Acknowledgements
onboththefine-tuningdistributionandseveraldistribution
WethankKyleHsu,LukasHaas,andothermembersofthe
shifts. ForMNIST,weevaluatethefine-tunedmodel’sper-
IRISlabforhelpfulfeedbackanddiscussions.Wealsothank
formance on three distribution shifts: corrupted MNIST,
Sachin Goyal for help with ImageNet experiments. This
EMNIST, and rotated MNIST. Similarly, we evaluate the
workwassupportedbyKFASandNSF.A.R.issupported
modelfine-tunedonCIFAR-10.1onCIFAR-10.2,andvice
bySchmidtFutures,Google,Apple,andOpenPhilanthropy.
versa. FurtherexperimentaldetailsareinAppendixA.Re-
C.F.issupportedbyONRgrantN00014-21-1-2685andthe
sults in Tab. 6 show model performance after fine-tuning
NSFCAREERaward.
withthesetransferredhyperparameters. OnCIFAR-10.1and
CIFAR-10.2,AUTOFToutperformsstandardfine-tuningon
IDandOODaccuracy. However,onMNIST,AUTOFTre-
sultsinslightlyworseOODperformancethanfine-tuning.
ThissuggeststhatthehyperparametersoptimizedbyAUT-
OFTtransfermoreeffectivelytodatasetsthataresimilarto
theoriginalfine-tuningdistribution.
Effectofthehyperparameteroptimizationdataset. How
doesthechoiceofdatasetforhyperparameteroptimization
affecttheperformanceofthefine-tunedmodel? AUTOFT
optimizes hyperparameters using a validation set distinct
fromboththeIDfine-tuningdistributionandOODtestsets.
In Table 7, we assess the performance of AUTOFT using
differentdatasetsforhyperparameteroptimization.
The results suggest that AUTOFT yields strong OOD
performanceacrossarangeofvalidationdatasets,withbest
OOD performance achieved when the validation set used
for hyperparameter optimization is different from the ID
dataset.
7.Conclusion
We introduce AUTOFT, a novel data-driven approach for
robustfine-tuningthatoptimizesanexpressivesetofhyper-
parametersusingasmallOODvalidationset. AUTOFTonly
requiresasmallamountofdatafromonenaturallyoccurring
OOD distribution—such data is often readily available or
ispossibletogatheratasimilarcosttothatoftheoriginal
ID training data. Our empirical results demonstrate that
AUTOFTconsistentlyoutperformsexistingapproachesand
achievesanewstate-of-the-artforrobustfine-tuning. We
hopethatourworkwillinspirefutureresearchondata-driven
approachesforrobustfine-tuning.
Futuredirections. Whileourexperimentsshowstrongre-
sultsinimageclassification,wehavenotyetverifiedtheef-
fectivenessofAUTOFToutsidethisproblemsetting. Future
researchcouldexploreextendingthegeneralworkflowof
AUTOFTtootherareassuchasimagesegmentation,object
detection, and natural language processing (NLP). More-
over,investigatingalternativehyperparametersearchspaces
9References [14] JonathonByrdandZacharyLipton. Whatistheeffectof
importanceweightingindeeplearning? InInternational
[1] ArmenAghajanyan,AkshatShrivastava,AnchitGupta,Na-
ConferenceonMachineLearning,pages872–881.PMLR,
man Goyal, Luke Zettlemoyer, and Sonal Gupta. Better
2019.
fine-tuning by reducing representational collapse. arXiv
[15] Xiangning Chen, Chen Liang, Da Huang, Esteban Real,
preprintarXiv:2008.03156,2020.
KaiyuanWang,YaoLiu,HieuPham,XuanyiDong,Thang
[2] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru
Luong, Cho-Jui Hsieh, et al. Symbolic discovery of op-
Ohta,andMasanoriKoyama.Optuna:Anext-generationhy-
timization algorithms. arXiv preprint arXiv:2302.06675,
perparameteroptimizationframework.InProceedingsofthe
2023.
25thACMSIGKDDinternationalconferenceonknowledge
[16] Gordon Christie, Neil Fendley, James Wilson, and Ryan
discovery&datamining,pages2623–2631,2019.
Mukherjee. Functionalmapoftheworld. InProceedings
[3] AndersAndreassen,YasamanBahri,BehnamNeyshabur,
oftheIEEEConferenceonComputerVisionandPattern
andRebeccaRoelofs. Theevolutionofout-of-distribution
Recognition,pages6172–6180,2018.
robustness throughout fine-tuning. arXiv preprint
[17] NivCohen,RinonGal,EliAMeirom,GalChechik,and
arXiv:2106.15831,2021.
Yuval Atzmon. " this is my unicorn, fluffy": Personaliz-
[4] Marcin Andrychowicz, Misha Denil, Sergio Gomez, ingfrozenvision-languagerepresentations. arXivpreprint
MatthewWHoffman,DavidPfau,TomSchaul,Brendan arXiv:2204.01694,2022.
Shillingford,andNandoDeFreitas. Learningtolearnby
[18] ElliotCreager,Jörn-HenrikJacobsen,andRichardZemel.
gradientdescentbygradientdescent. Advancesinneural
Environmentinferenceforinvariantlearning. InInterna-
informationprocessingsystems,29,2016.
tionalConferenceonMachineLearning,pages2189–2200.
[5] MartinArjovsky,LéonBottou,IshaanGulrajani,andDavid PMLR,2021.
Lopez-Paz. Invariant risk minimization. arXiv preprint
[19] EkinDCubuk,BarretZoph,DandelionMane,VijayVasude-
arXiv:1907.02893,2019.
van,andQuocVLe. Autoaugment:Learningaugmentation
[6] ChristinaBaek,YidingJiang,AditiRaghunathan,andJZico strategiesfromdata. InProceedingsoftheIEEE/CVFcon-
Kolter. Agreement-on-the-line:Predictingtheperformance ferenceoncomputervisionandpatternrecognition,pages
of neural networks under distribution shift. Advances in 113–123,2019.
NeuralInformationProcessingSystems,35:19274–19289,
[20] EkinDCubuk,BarretZoph,JonathonShlens,andQuocV
2022.
Le. Randaugment: Practical automated data augmenta-
[7] AndreiBarbu,DavidMayo,JulianAlverio,WilliamLuo, tion with a reduced search space. In Proceedings of the
ChristopherWang,DanGutfreund,JoshTenenbaum,and IEEE/CVFconferenceoncomputervisionandpatternrecog-
BorisKatz. Objectnet:Alarge-scalebias-controlleddataset nitionworkshops,pages702–703,2020.
for pushing the limits of object recognition models. Ad-
[21] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
vancesinneuralinformationprocessingsystems,32,2019.
andLiFei-Fei. Imagenet:Alarge-scalehierarchicalimage
[8] Sarah Bechtle, Artem Molchanov, Yevgen Chebotar, Ed- database. In2009IEEEconferenceoncomputervisionand
ward Grefenstette, Ludovic Righetti, Gaurav Sukhatme, patternrecognition,pages248–255.Ieee,2009.
andFranziskaMeier. Metalearningvialearnedloss. In [22] CianEastwood,IanMason,ChristopherKIWilliams,and
202025thInternationalConferenceonPatternRecognition
Bernhard Schölkopf. Source-free adaptation to measure-
(ICPR),pages4161–4168.IEEE,2021. mentshiftviabottom-upfeaturerestoration. arXivpreprint
[9] Sara Beery, Arushi Agarwal, Elijah Cole, and Vighnesh arXiv:2107.05446,2021.
Birodkar. Theiwildcam2021competitiondataset. arXiv [23] CianEastwood,IanMason,andChristopherKIWilliams.
preprintarXiv:2105.03494,2021. Unit-level surprise in neural networks. In I (Still) Can’t
[10] SamyBengio, YoshuaBengio, JocelynCloutier, andJan BelieveIt’sNotBetter!WorkshopatNeurIPS2021,pages
Gescei. Ontheoptimizationofasynapticlearningrule. In 33–40.PMLR,2022.
Optimality in Biological and Artificial Networks?, pages [24] Utku Evci, Vincent Dumoulin, Hugo Larochelle, and
281–303.Routledge,2013. MichaelCMozer. Head2toe: Utilizingintermediaterep-
[11] James Bergstra and Yoshua Bengio. Random search for resentationsforbettertransferlearning. InInternational
hyper-parameteroptimization. Journalofmachinelearning ConferenceonMachineLearning,pages6009–6033.PMLR,
research,13(2),2012. 2022.
[12] JamesBergstra,RémiBardenet,YoshuaBengio,andBalázs [25] LiFei-Fei,RobFergus,andPietroPerona. Learninggener-
Kégl. Algorithms for hyper-parameter optimization. In ativevisualmodelsfromfewtrainingexamples:Anincre-
AdvancesinNeuralInformationProcessingSystems.Curran mentalbayesianapproachtestedon101objectcategories.In
Associates,Inc.,2011. ComputerVisionandPatternRecognitionWorkshop.IEEE,
[13] RishiBommasani,DrewAHudson,EhsanAdeli,RussAlt- 2004.
man,SimranArora,SydneyvonArx,MichaelSBernstein, [26] MatthiasFeurer,AaronKlein,KatharinaEggensperger,Jost
JeannetteBohg,AntoineBosselut,EmmaBrunskill,etal. Springenberg,ManuelBlum,andFrankHutter. Efficient
Ontheopportunitiesandrisksoffoundationmodels. arXiv androbustautomatedmachinelearning. Advancesinneural
preprintarXiv:2108.07258,2021. informationprocessingsystems,28,2015.
10[27] Irena Gao, Shiori Sagawa, Pang Wei Koh, Tatsunori [40] GabrielIlharco,MitchellWortsman,RossWightman,Cade
Hashimoto,andPercyLiang. Out-of-domainrobustnessvia Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
targetedaugmentations. arXivpreprintarXiv:2302.11861, VaishaalShankar,HongseokNamkoong,JohnMiller,Han-
2023. nanehHajishirzi,AliFarhadi,andLudwigSchmidt. Open-
[28] RobertGeirhos,Jörn-HenrikJacobsen,ClaudioMichaelis, clip,2021. Ifyouusethissoftware,pleaseciteitasbelow.
RichardZemel,WielandBrendel,MatthiasBethge,andFe- [41] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh,
lixAWichmann. Shortcutlearningindeepneuralnetworks. HieuPham,QuocLe,Yun-HsuanSung,ZhenLi,andTom
NatureMachineIntelligence,2(11):665–673,2020. Duerig. Scalingupvisualandvision-languagerepresenta-
[29] HenryGouk,TimothyHospedales,andmassimilianopon- tionlearningwithnoisytextsupervision. InInternational
til. Distance-basedregularisationofdeepnetworksforfine- conferenceonmachinelearning,pages4904–4916.PMLR,
tuning. InInternationalConferenceonLearningRepresen- 2021.
tations,2021.
[42] HaomingJiang, PengchengHe, WeizhuChen, Xiaodong
[30] SachinGoyal,AnanyaKumar,SankalpGarg,ZicoKolter,
Liu,JianfengGao,andTuoZhao. Smart: Robustandef-
and Aditi Raghunathan. Finetune like you pretrain: Im-
ficientfine-tuningforpre-trainednaturallanguagemodels
provedfinetuningofzero-shotvisionmodels.arXivpreprint
throughprincipledregularizedoptimization. arXivpreprint
arXiv:2212.00638,2022.
arXiv:1911.03437,2019.
[31] SachinGoyal,MingjieSun,AditiRaghunathan,andJZico
[43] NeeravKarani,ErtuncErdil,KrishnaChaitanya,andEnder
Kolter. Testtimeadaptationviaconjugatepseudo-labels.
Konukoglu. Test-timeadaptableneuralnetworksforrobust
Advances in Neural Information Processing Systems, 35:
medicalimagesegmentation. MedicalImageAnalysis,68:
6204–6218,2022.
101907,2021.
[32] IshaanGulrajaniandDavidLopez-Paz. Insearchoflost
[44] Polina Kirichenko, Pavel Izmailov, and Andrew Gordon
domaingeneralization. arXivpreprintarXiv:2007.01434,
Wilson. Lastlayerre-trainingissufficientforrobustness
2020.
tospuriouscorrelations. arXivpreprintarXiv:2204.02937,
[33] YunhuiGuo,HonghuiShi,AbhishekKumar,KristenGrau-
2022.
man,TajanaRosing,andRogerioFeris. Spottune:transfer
learningthroughadaptivefine-tuning. InProceedingsof [45] JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,Joel
theIEEE/CVFconferenceoncomputervisionandpattern Veness, Guillaume Desjardins, Andrei A Rusu, Kieran
recognition,pages4805–4814,2019. Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-
[34] RyuichiroHataya,JanZdenek,KazukiYoshizoe,andHideki Barwinska, et al. Overcoming catastrophic forgetting in
Nakayama. Fasterautoaugment: Learningaugmentation neuralnetworks. Proceedingsofthenationalacademyof
strategies using backpropagation. In Computer Vision– sciences,114(13):3521–3526,2017.
ECCV 2020: 16th European Conference, Glasgow, UK, [46] LouisKirsch, SjoerdvanSteenkiste, andJürgenSchmid-
August23–28,2020,Proceedings,PartXXV16,pages1–16. huber. Improving generalization in meta reinforce-
Springer,2020. ment learning using learned objectives. arXiv preprint
[35] Dan Hendrycks and Thomas Dietterich. Benchmarking arXiv:1910.04098,2019.
neuralnetworkrobustnesstocommoncorruptionsandper- [47] Pang Wei Koh, Shiori Sagawa, Henrik Marklund,
turbations. ProceedingsoftheInternationalConferenceon SangMichaelXie,MarvinZhang,AkshayBalsubramani,
LearningRepresentations,2019. WeihuaHu,MichihiroYasunaga,RichardLanasPhillips,
[36] Dan Hendrycks, Mantas Mazeika, Saurav Kadavath, and IrenaGao,etal. Wilds: Abenchmarkofin-the-wilddis-
DawnSong. Usingself-supervisedlearningcanimprove tributionshifts. InInternationalConferenceonMachine
modelrobustnessanduncertainty. Advancesinneuralinfor- Learning,pages5637–5664.PMLR,2021.
mationprocessingsystems,32,2019.
[48] JonathanKrause,MichaelStark,JiaDeng,andLiFei-Fei.
[37] DanHendrycks,StevenBasart,NormanMu,SauravKada- 3d object representations for fine-grained categorization.
vath,FrankWang,EvanDorundo,RahulDesai,TylerZhu, In Proceedings of the IEEE International Conference on
SamyakParajuli,MikeGuo,etal.Themanyfacesofrobust- ComputerVisionWorkshops,pages554–561,2013.
ness:Acriticalanalysisofout-of-distributiongeneralization.
[49] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiple
InProceedingsoftheIEEE/CVFInternationalConference
layersoffeaturesfromtinyimages. 2009.
onComputerVision,pages8340–8349,2021.
[50] AnanyaKumar,AditiRaghunathan,RobbieMatthewJones,
[38] DanHendrycks,KevinZhao,StevenBasart,JacobStein-
TengyuMa,andPercyLiang. Fine-tuningcandistortpre-
hardt,andDawnSong. Naturaladversarialexamples. In
trainedfeaturesandunderperformout-of-distribution. InIn-
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
ternationalConferenceonLearningRepresentations,2022.
sionandPatternRecognition,pages15262–15271,2021.
[39] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. [51] AnanyaKumar,RuoqiShen,SébastienBubeck,andSuriya
Sequentialmodel-basedoptimizationforgeneralalgorithm Gunasekar. Howtofine-tunevisionmodelswithsgd,2022.
configuration. In Learning and Intelligent Optimization: [52] Cheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.
5thInternationalConference,LION5,Rome,Italy,January Mixout:Effectiveregularizationtofinetunelarge-scalepre-
17-21,2011.SelectedPapers5,pages507–523.Springer, trainedlanguagemodels. arXivpreprintarXiv:1909.11299,
2011. 2019.
11[53] JaejunLee,RaphaelTang,andJimmyLin.Whatwouldelsa [67] JishnuMukhoti,YarinGal,PhilipHSTorr,andPuneetK
do? freezinglayersduringtransformerfine-tuning. arXiv Dokania. Fine-tuningcancrippleyourfoundationmodel;
preprintarXiv:1911.03090,2019. preserving features may be the solution. arXiv preprint
[54] YoonhoLee,AnnieSChen,FahimTajwar,AnanyaKumar, arXiv:2308.13320,2023.
HuaxiuYao,PercyLiang,andChelseaFinn. Surgicalfine- [68] JunhyukOh,MatteoHessel,WojciechMCzarnecki,Zhong-
tuning improves adaptation to distribution shifts. arXiv wenXu, HadoPvanHasselt, SatinderSingh, andDavid
preprintarXiv:2210.11466,2022. Silver. Discoveringreinforcementlearningalgorithms. Ad-
[55] YoonhoLee,HuaxiuYao,andChelseaFinn. Diversifyand vancesinNeuralInformationProcessingSystems,33:1060–
disambiguate: Learningfromunderspecifieddata. arXiv 1070,2020.
preprintarXiv:2202.03418,2022. [69] MaximeOquab,LeonBottou,IvanLaptev,andJosefSivic.
[56] Da Li, Yongxin Yang, Yi-Zhe Song, and Timothy Learningandtransferringmid-levelimagerepresentations
Hospedales. Learningtogeneralize:Meta-learningfordo- usingconvolutionalneuralnetworks. InProceedingsofthe
maingeneralization. InProceedingsoftheAAAIconference IEEEconferenceoncomputervisionandpatternrecognition,
onartificialintelligence,2018. pages1717–1724,2014.
[70] Qi Qi, Yi Xu, Rong Jin, Wotao Yin, and Tianbao Yang.
[57] HaoLi,PratikChaudhari,HaoYang,MichaelLam,Avinash
Attentionalbiasedstochasticgradientforimbalancedclassi-
Ravichandran,RahulBhotika,andStefanoSoatto. Rethink-
fication. arXivpreprintarXiv:2012.06951,2020.
ingthehyperparametersforfine-tuning. InInternational
[71] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
ConferenceonLearningRepresentations,2020.
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
[58] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Ros-
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
tamizadeh, and Ameet Talwalkar. Hyperband: A novel
Krueger,andIlyaSutskever. Learningtransferablevisual
bandit-basedapproachtohyperparameteroptimization. The
modelsfromnaturallanguagesupervision,2021.
journal of machine learning research, 18(1):6765–6816,
[72] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
2017.
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
[59] LiamLi,KevinJamieson,AfshinRostamizadeh,Ekaterina
AmandaAskell,PamelaMishkin,JackClark,etal. Learn-
Gonina, Moritz Hardt, Ben Recht, and Ameet Talwalkar.
ingtransferablevisualmodelsfromnaturallanguagesuper-
Massivelyparallelhyperparametertuning. 2018.
vision. InInternationalconferenceonmachinelearning,
[60] SungbinLim,IldooKim,TaesupKim,ChiheonKim,and
pages8748–8763.PMLR,2021.
SungwoongKim. Fastautoaugment. AdvancesinNeural
[73] Vinay V Ramasesh, Ethan Dyer, and Maithra Raghu.
InformationProcessingSystems,32,2019.
Anatomyofcatastrophicforgetting:Hiddenrepresentations
[61] EvanZLiu,BehzadHaghgoo,AnnieSChen,AditiRaghu-
andtasksemantics. arXivpreprintarXiv:2007.07400,2020.
nathan, PangWeiKoh, ShioriSagawa, PercyLiang, and
[74] EstebanReal,AlokAggarwal,YanpingHuang,andQuocV
ChelseaFinn. Justtraintwice:Improvinggrouprobustness
Le. Regularizedevolutionforimageclassifierarchitecture
withouttraininggroupinformation. InInternationalCon-
search. InProceedingsoftheaaaiconferenceonartificial
ferenceonMachineLearning, pages6781–6792.PMLR,
intelligence,pages4780–4789,2019.
2021.
[75] BenjaminRecht,RebeccaRoelofs,LudwigSchmidt,and
[62] Hanxiao Liu, Karen Simonyan, and Yiming Yang.
VaishaalShankar. Docifar-10classifiersgeneralizetocifar-
Darts: Differentiable architecture search. arXiv preprint
10? 2018.
arXiv:1806.09055,2018.
[76] BenjaminRecht,RebeccaRoelofs,LudwigSchmidt,and
[63] YuhanLiu,SaurabhAgarwal,andShivaramVenkataraman. VaishaalShankar. Doimagenetclassifiersgeneralizetoima-
Autofreeze:Automaticallyfreezingmodelblockstoacceler- genet? InInternationalConferenceonMachineLearning,
atefine-tuning. arXivpreprintarXiv:2102.01386,2021. pages5389–5400.PMLR,2019.
[64] ShangyunLu,BradleyNott,AaronOlson,AlbertoTodes- [77] AmélieRoyerandChristophLampert. Aflexibleselection
chini,HosseinVahabi,YairCarmon,andLudwigSchmidt. schemeforminimum-efforttransferlearning. InProceed-
Harder or different? a closer look at distribution shift in ingsoftheIEEE/CVFWinterConferenceonApplicationsof
datasetreproduction. InICMLWorkshoponUncertainty ComputerVision,pages2191–2200,2020.
andRobustnessinDeepLearning,page15,2020. [78] ShioriSagawa,PangWeiKoh,TatsunoriBHashimoto,and
[65] LukeMetz,JamesHarrison,CDanielFreeman,AmilMer- PercyLiang. Distributionallyrobustneuralnetworksfor
chant,LucasBeyer,JamesBradbury,NamanAgrawal,Ben groupshifts:Ontheimportanceofregularizationforworst-
Poole,IgorMordatch,AdamRoberts,etal. Velo:Training casegeneralization. arXivpreprintarXiv:1911.08731,2019.
versatilelearnedoptimizersbyscalingup. arXivpreprint [79] Shiori Sagawa, Pang Wei Koh, Tony Lee, Irena Gao,
arXiv:2211.09760,2022. Sang Michael Xie, Kendrick Shen, Ananya Kumar, Wei-
[66] John P Miller, Rohan Taori, Aditi Raghunathan, Shiori huaHu,MichihiroYasunaga,HenrikMarklund,SaraBeery,
Sagawa, Pang Wei Koh, Vaishaal Shankar, Percy Liang, EtienneDavid,IanStavness,WeiGuo,JureLeskovec,Kate
YairCarmon,andLudwigSchmidt. Accuracyontheline: Saenko,TatsunoriHashimoto,SergeyLevine,ChelseaFinn,
onthestrongcorrelationbetweenout-of-distributionand and Percy Liang. Extending the WILDS benchmark for
in-distributiongeneralization. InInternationalConference unsupervisedadaptation. InInternationalConferenceon
onMachineLearning,pages7721–7735.PMLR,2021. LearningRepresentations,2022.
12[80] Hadi Salman, Andrew Ilyas, Logan Engstrom, Ashish InProceedingsoftheIEEE/CVFConferenceonComputer
Kapoor, andAleksanderMadry. Doadversariallyrobust VisionandPatternRecognition,pages7959–7971,2022.
imagenetmodelstransferbetter? AdvancesinNeuralInfor- [93] YuhuiXu,LingxiXie,XiaopengZhang,XinChen,Guo-Jun
mationProcessingSystems,33:3533–3545,2020. Qi,QiTian,andHongkaiXiong. Pc-darts:Partialchannel
[81] Ali Sharif Razavian, Hossein Azizpour, Josephine Sulli- connectionsformemory-efficientarchitecturesearch. arXiv
van, andStefan Carlsson. Cnnfeatures off-the-shelf: an preprintarXiv:1907.05737,2019.
astoundingbaselineforrecognition. InProceedingsofthe [94] LIXuhong,YvesGrandvalet,andFranckDavoine. Explicit
IEEEconferenceoncomputervisionandpatternrecognition inductivebiasfortransferlearningwithconvolutionalnet-
workshops,pages806–813,2014. works. InInternationalConferenceonMachineLearning,
[82] ZhiqiangShen,ZechunLiu,JieQin,MariosSavvides,and pages2825–2834.PMLR,2018.
Kwang-TingCheng. Partialisbetterthanall: Revisiting [95] HuaxiuYao,YuWang,SaiLi,LinjunZhang,WeixinLiang,
fine-tuningstrategyforfew-shotlearning. InProceedings JamesZou,andChelseaFinn.Improvingout-of-distribution
of the AAAI Conference on Artificial Intelligence, pages robustnessviaselectiveaugmentation. InInternationalCon-
9594–9602,2021. ferenceonMachineLearning,pages25407–25437.PMLR,
[83] RohanTaori,AchalDave,VaishaalShankar,NicholasCar- 2022.
lini, Benjamin Recht, and Ludwig Schmidt. Measuring [96] JasonYosinski,JeffClune,YoshuaBengio,andHodLipson.
robustnesstonaturaldistributionshiftsinimageclassifica- Howtransferablearefeaturesindeepneuralnetworks? Ad-
tion. AdvancesinNeuralInformationProcessingSystems, vancesinneuralinformationprocessingsystems,27,2014.
33:18583–18599,2020. [97] TianheYu,ChelseaFinn,AnnieXie,SudeepDasari,Tian-
[84] Antonio Torralba, Rob Fergus, and William T. Freeman. haoZhang, PieterAbbeel, andSergeyLevine. One-shot
80milliontinyimages:Alargedatasetfornonparametric imitationfromobservinghumansviadomain-adaptivemeta-
objectandscenerecognition. IEEETransactionsonPat- learning. arXivpreprintarXiv:1802.01557,2018.
ternAnalysisandMachineIntelligence,30(11):1958–1970, [98] Jeffrey O Zhang, Alexander Sax, Amir Zamir, Leonidas
2008. Guibas, and Jitendra Malik. Side-tuning: a baseline for
[85] HugoTouvron,MatthieuCord,AlaaeldinEl-Nouby,Jakob networkadaptationviaadditivesidenetworks. InEuropean
Verbeek, and Hervé Jégou. Three things everyone ConferenceonComputerVision,pages698–714.Springer,
should know about vision transformers. arXiv preprint 2020.
arXiv:2203.09795,2022. [99] MarvinZhang,HenrikMarklund,NikitaDhawan,Abhishek
[86] EricTzeng,JudyHoffman,NingZhang,KateSaenko,and Gupta, Sergey Levine, and Chelsea Finn. Adaptive risk
TrevorDarrell. Deepdomainconfusion: Maximizingfor minimization:Learningtoadapttodomainshift. Advances
domaininvariance. arXivpreprintarXiv:1412.3474,2014. inNeuralInformationProcessingSystems,34:23664–23678,
[87] Bastiaan S Veeling, Jasper Linmans, Jim Winkens, Taco 2021.
Cohen, and Max Welling. Rotation equivariant cnns for [100] BarretZophandQuocVLe.Neuralarchitecturesearchwith
digitalpathology. arXivpreprintarXiv:1806.03962,2018. reinforcementlearning. arXivpreprintarXiv:1611.01578,
[88] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P 2016.
Xing. Learningrobustglobalrepresentationsbypenalizing [101] BarretZoph,VijayVasudevan,JonathonShlens,andQuocV
localpredictivepower. InAdvancesinNeuralInformation Le. Learningtransferablearchitecturesforscalableimage
ProcessingSystems,pages10506–10518,2019. recognition. In Proceedings of the IEEE conference on
[89] Olga Wichrowska, Niru Maheswaranathan, Matthew W. computervisionandpatternrecognition,pages8697–8710,
Hoffman,SergioGomezColmenarejo,MishaDenil,Nando 2018.
deFreitas,andJaschaSohl-Dickstein. Learnedoptimizers
thatscaleandgeneralize. 2017.
[90] Olivia Wiles, Sven Gowal, Florian Stimberg, Sylvestre
Alvise-Rebuffi, Ira Ktena, Taylan Cemgil, et al. A fine-
grained analysis on distribution shift. arXiv preprint
arXiv:2110.11328,2021.
[91] MitchellWortsman,GabrielIlharco,SamirYaGadre,Re-
becca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos,
Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon
Kornblith,etal. Modelsoups:averagingweightsofmulti-
plefine-tunedmodelsimprovesaccuracywithoutincreasing
inferencetime. InInternationalConferenceonMachine
Learning,pages23965–23998.PMLR,2022.
[92] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim,
MikeLi,SimonKornblith,RebeccaRoelofs,RaphaelGon-
tijo Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok
Namkoong,etal. Robustfine-tuningofzero-shotmodels.
13A.AdditionalExperiments loss:
A.1.ImageNetDistributionShifts 10
(cid:88)
argminD (q ||p)+ w NLL (q ,D). (2)
KL θ i i θ
A.1.1 MainResults θ
i=1
WeprovidedetailedresultsforeachImageNet-deriveddistri- B.2.Datasets
butionshiftinTab.8. AutoFToutperformsallbaselineson
Below, we summarize the datasets we use for evaluation,
9outof10evaluationsettings. FLYP,thepriorstate-of-the-
includingthefine-tuningdataset(ID),thevalidationdataset
art,hasstrongIDaccuracy: 82.6%and82.9%withoutand
forhyperparameteroptimization,andthetestOODdatasets.
withensembling,whereasAUTOFThas81.8%and82.4%.
Despite its high ID accuracy, the model fine-tuned with • CIFAR-10[49]contains60,000imagesacross10classes.
FLYPhassubstantiallyworseOODaccuracythanAUTOFT: We use CIFAR-10 for fine-tuning, 100 examples from
60.2%and63.2%withoutandwithensembling,compared CIFAR-10-Cforvalidation,andtheCIFAR-10.1[75,84]
to 61.5% and 64.3% for AUTOFT. We observe a similar andCIFAR-10.2[64]asOODtestsets.
effect with all other points of comparison, indicating that
thesemethodsforrobustfine-tuningwere“overfitting”to • ImageNet [21] contains over a million images in 1000
theIDdistributioninawaythatAUTOFTdoesnot. categories.WeuseImageNetasourIDdistribution,15000
examples from ImageNet-C for validation, and five Im-
ageNet variations for the OOD datasets following prior
A.1.2 Few-ShotClassification
works [30, 50, 71, 92]: ImageNet-V2 [76], ImageNet-
Figure5showstheaverageOODaccuracyonfiveImageNet- R[37],ImageNet-A[38],ImageNet-Sketch[88],andOb-
deriveddistributionshiftsagainstIDaccuracyonImageNet jectNet[7].
for4,16,and32shotclassification. AUTOFToutperforms
• WILDS-iWildCam[9,47,79]isananimalcameratrap
baselines in effective robustness in all three settings. In
image classification dataset where the label y is one of
32-shotclassification,AUTOFTimprovesaverageOODac-
182 animal species. The ID and OOD datasets consist
curacyby2.2%comparedtotheleadingbaseline,FLYP.
ofphotosfromdisjointsetsofcameratraps,makingthe
imagesdifferincameraspecificationsandattributessuch
A.2.CIFARDistributionShifts
as background and lighting. We use the official splits
AUTOFToutperformspriorapproachesonthesubtleCIFAR- fromKohetal.[47]andusetheIDtrainsetforfine-tuning,
10.1andCIFAR-10.2distributionshifts. theOODvalidationsetforhyperparameteroptimization,
andtheOODtestsetforevaluation.
A.3.ExpressivityofHyperparameterSpace
• WILDS-FMoW[16,47,79]containsremotesensingim-
Weinvestigatetheeffectofexpressivityofthehyperparam-
ageryfromsatellites. Eachimageistobeclassifiedinto
eter search space. We compare AUTOFT against a layer-
one among 62 categories, including labels like “impov-
wisevariantofAUTOFTthatlearnsper-layerlearningrates,
erished settlement” and “hospital.” The ID and OOD
weightdecays,andL1/L2normsandregularizationterms
datasetsdifferinyearofacquisitionandgeographicloca-
in Tab. 10. The layerwise variant of AUTOFT results in
tion. We use the official splits from Koh et al. [47] and
worseIDandOODperformance,suggestingthatanoverly
usetheIDtrainsetforfine-tuning,theOODvalidationset
expressivehyperparameterspaceoverfitstothevalidation
forhyperparameteroptimization,andtheOODtestsetfor
set.
evaluation.
B.ExperimentalDetails Inallofthetransferlearningdatasetsdescribedbelow,we
use a subset of the ID validation set for hyperparameter
B.1.ToyExperimentDetails
optimization. In other words, we do not use an external
We provide further details for the toy experiment in “OOD”set.
Sec. 4.3 and Figure 3. The prior distribution is a unit
• Caltech101[25]containsimagesofobjectsfrom101dif-
Gaussian, i.e. it has variance [1.0,1.0,...]. Train-
ferent categories, including “dragonfly,” “grand piano,”
ing data D is drawn from a distribution with variance
tr and“saxophone.”
[10−3,10−3,10−3,10−3,1.0,1.0,...],whilethevalidation
data D is drawn from a distribution with variance • StanfordCars [48] features a collection of car images
val
[10−3,10−3,10−3,1.0,10−3,1.0,...]. We fit the parame- categorizedbymodel,make,andyear,wherethetaskisto
tersofaGaussiandistributionq tothetrainingdatausing classifythemintooneof196types,suchas“FordMustang
θ
avariationalBayesobjectivebyminimizingthefollowing Convertible1967”or“ToyotaPriusHatchback2009.”
14WithoutEnsembling WithEnsembling
Methods ID Im-V2 Im-R Im-A Sketch ObjectNet Avg.OOD ID Im-V2 Im-R Im-A Sketch ObjectNet Avg.OOD
Zeroshot 68.3 61.9 77.7 50.0 48.3 55.4 58.7 68.3 61.9 77.7 50.0 48.3 55.4 58.7
LP 79.9 69.8 70.8 46.4 46.9 52.1 57.2 80.0 70.3 72.4 47.8 48.1 52.8 58.3
FT 81.3 71.2 66.1 37.8 46.1 53.3 54.9 82.5 72.8 74.9 48.1 51.9 59.0 61.3
L2-SP 81.7 71.8 70.0 42.5 48.5 56.2 57.8 82.2 72.9 75.1 48.6 51.4 58.9 61.4
LP-FT 81.7 72.1 73.5 47.6 50.3 58.2 60.3 82.1 72.8 75.3 50.1 51.7 59.2 61.8
FLYP 82.6 73.0 71.4 48.1 49.6 58.7 60.2 82.9 73.5 76.0 53.0 52.3 60.8 63.1
AUTOFT 81.8 73.1 72.4 48.8 49.8 63.5 61.5 82.4 73.6 76.4 53.1 52.6 65.6 64.3
Table8. AUTOFTconsistentlyoutperformsallbaselinesoneachImageNetdistributionshift.
4-Shot ImageNet 16-Shot ImageNet 32-Shot ImageNet
60.4
61.5
59.9 61.2
59.4 60.5
60.2
58.9
59.5
59.2
58.4
7 2 7 2 7 2 2 2 2 2 2 2 2 2 2 2 2 2 2
8. 9. 9. 0. 0. 1. 9. 0. 1. 2. 3. 4. 9. 0. 1. 2. 3. 4. 5.
6 6 6 7 7 7 6 7 7 7 7 7 6 7 7 7 7 7 7
In-Distribution (ID) Accuracy In-Distribution (ID) Accuracy In-Distribution (ID) Accuracy
AutoFT FT Fine-tuned Model Without Ensembling
FLYP Weight Ensembling Curves Zero-shot Model
LP-FT Best Ensembled Model
Figure5. AUTOFTenhanceseffectiverobustnessoverexistingmethods,indicatedbyverticaldistance. AUTOFTresultsinbetterOOD
accuracyatagivenIDaccuracyacrossvaryingdegreesofweightinterpolationbetweenthefine-tunedandzero-shotmodels.
Method CIFAR-10.1 CIFAR-10.2 iWildCam
Zero-shot 92.5 88.8 Method ID OOD
Fine-tuning 95.9 91.3
AUTOFT 51.0 38.3
AUTOFT 97.5 93.5 LayerwiseAUTOFT 47.8 34.6
WiSE-FT(bestα) 98.0 94.4
AUTOFT(bestα) 98.3 95.0
Table10. AUTOFTyieldsbetterIDandOODperformancethan
itsvariantwithper-layerlearningrates,weightdecays,andL1/L2
distancestopretrainedparameters,indicatingthatmoreexpressive
Table9. AUTOFT outperformsfine-tuningby2.2%onCIFAR-
hyperparameterspacesoverfittothevalidationset,degradingper-
10.2andby1.4%onCIFAR-10.1,usingonly100samplesfrom
formance.
CIFAR-10-C.AUTOFTadditionallyoutperformsWiSE-FTwith
weightensembling.
• Rendered SST2 [72] is a dataset for optical character
recognition,wherethetaskistoclassifytextsentimentas
• Flowers102[87]consistsofflowerimagesfromtheUK,
“positive”or“negative.”
withtheobjectiveofclassifyingeachimageintooneof
102species,suchas“oxeyedaisy”or“hibiscus.”
• PatchCamelyon[87]providesdigitalpathologyimages
for binary classification, with the goal of identifying
metastatictumortissues.
15
)DOO(
stfihS
.tsiD
5
no
.ccA
.gvA
)DOO(
stfihS
.tsiD
5
no
.ccA
.gvA
)DOO(
stfihS
.tsiD
5
no
.ccA
.gvAB.3.TrainingDetails ofhyperparametersacross5separateAUTOFTruns,based
onperformanceonthek-shotvalidationset.
B.3.1 Baselines
Transferlearning. Wenotethatthetransferlearningexper-
We closely follow the training details in Goyal et al. [30].
imentsonFlowers102andStanfordCarsdonotuseOOD
For all datasets excluding ImageNet, we use a batch size
dataforhyperparameteroptimization;here,AUTOFTopti-
of 256 and conduct a hyperparameter sweep across five
mizeshyperparametersonaheld-outIDvalidationset.
learningrates{10−6,10−5,...,10−2}andfiveweightde-
cayvaluesintherange{0.0,0.1,...,0.4}. OnImageNet, WILDS-iWildCamandWILDS-FMoWstate-of-the-art
we use a larger batch size of 512 and perform a hyperpa- (SoTA). For the iWildCam and FMoW SoTA results
rametersweepthreelearningrates{10−6,10−5,10−4}and in Tab. 1, we fine-tune the ViT-L/14@336px model
twoweightdecays{0,0.1}. ForL2-SP,wetunetheweight withlossweightslearnedonthesmallerViT-B/16back-
of the regularization term λ ∈ {10−4,10−3,10−2,10−1}. bone with AUTOFT. Applying AUTOFT directly to the
WeselectbaselinehyperparametersbasedonIDvalidation ViT-L/14@336pxmodelmayimproveperformancefur-
performance. Fordatasetswithoutastandardvalidationset, ther,althoughatthecostofmorecompute.
wesplitthetrainingdataintoan80:20ratiotocreateone.
Inthekfew-shotsettingwithk ∈{4,16,32},weselect
k training and k validation examples from each class. To
accountforvariancefromthesmalltrainingandvalidation
sets,weaverageover50runs.
B.3.2 AUTOFT
Hyperparameter search space. As described in Sec. 4,
AUTOFT learnsweightsforninedifferentlossesonalog-
uniformrange[10−4,10]. AUTOFT additionallysearches
forlearningrateinthelog-uniformrange[10−2·η∗,102·η∗],
whereη∗istheconventionallearningrateusedinpriorworks
onfine-tuning[30,50,92],andweightdecayvaluesinthe
log-uniformrange[0.0,1.0].
Optuna settings. Below, we summarize the number of
inner-loopgradientsteps,Optunatrials,andvalidationset
sizeforeachdataset. Allofthesevalueswereselectedbased
onperformanceonanIDvalidationset:
Dataset InnerSteps Trials N
val
iWildCam 10 500 1000
FMoW 10 500 1000
CIFAR-10 10 100 100
Flowers 50 500 500
Cars 50 500 1000
ImageNet 100 500 1000
Few-shotclassification. Forthek few-shotclassification
setting,wherek ∈ {4,16,32},weuseak-shotvalidation
set for hyperparameter optimization. On all k-shot SST2
andPatchCamelyonexperiments,werunwith10inner-loop
gradientstepsand50Optunatrials. WerunallImageNet
few-shotexperimentswith100Optunatrialsand5,20,and
50innerstepsfor4-shot,16-shot,and32-shotclassification,
respectively. Duetoincreasedvariancefromtheverysmall
trainingandvalidationsetsinfew-shotbinaryclassification
(e.g.,SST2andPatchCamelyon),weselectthesinglebestset
16