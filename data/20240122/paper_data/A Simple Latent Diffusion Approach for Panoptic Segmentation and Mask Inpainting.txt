A Simple Latent Diffusion Approach for Panoptic Segmentation and Mask
Inpainting
WouterVanGansbeke1,2 BertDeBrabandere2
1 INSAIT 2 Segments.ai
Abstract Image
Panopticandinstancesegmentationnetworksareoften
trainedwithspecializedobjectdetectionmodules,complex
lossfunctions,andad-hocpost-processingstepstohandle
thepermutation-invarianceoftheinstancemasks. Thiswork
buildsuponStableDiffusionandproposesalatentdiffusion
approachforpanopticsegmentation,resultinginasimple
architecturewhichomitsthesecomplexities. Ourtraining
processconsistsoftwosteps: (1)trainingashallowautoen-
Diffusion Model
codertoprojectthesegmentationmaskstolatentspace;(2)
Noisy Label Label
trainingadiffusionmodeltoallowimage-conditionedsam-
Iterative Denoising
plinginlatentspace. Theuseofagenerativemodelunlocks
theexplorationofmaskcompletionorinpainting,whichhas Figure1.Wepresentasimplegenerativeapproachforthesegmentation
applicationsininteractivesegmentation. Theexperimental taskthatbuildsuponStableDiffusion[59].Thekeyideaistoleveragethe
diffusionprocesstobypasscomplexdetectionandsegmentationmodules,
validation yields promising results for both panoptic seg-
andtounlockmaskinpainting.Thegenerativeprocessisconditionedon
mentation and mask inpainting. While not setting a new RGBimagestopredictpanopticmasksinaniterativefashion.
state-of-the-art,ourmodel’ssimplicity,generality,andmask
completion capability are desirable properties. The code
thenecessityforlabels[18,70–72,76,77],butlikewisere-
andmodelswillbemadeavailable.1
quirehighly-specializedmodules,suchasregionproposal
networksorclustering. Differently,weseektoleveragegen-
erativemodelstobypasstheaforementionedcomponents.
1.Introduction
Thisobservationalignswithrecentworks[10–12,41,49]
advocating for general computer vision models in and at-
The image segmentation task [47, 51] has gained a lot of
tempttounifythefield. Thesepioneeringworkslimitthe
attentionintheliterature,encompassingthreepopularsub-
adoptionoftask-specificcomponentsandinsteaduseagen-
fields: semantic,instance,andpanopticsegmentation. Over
erativeprocess,suchasautoregressivemodeling,rendering
theyears,segmentationtoolshaveproventheirusefulness
itsdesignapplicabletodifferenttasksand/ormodalities. In
forawiderangeofapplications,suchasautonomousdriv-
asimilarvein,wetakeinspirationfromrecenttext-to-image
ing [19], medical imaging [50], agriculture [17], and aug-
diffusion models [20, 31, 32, 52, 56, 57, 59] to tackle the
mentedreality[1,26]. Currentmethodsarebuiltuponcon-
segmentationtaskinagenerativefashion. Inadditiontothe
volutionalnetworks[28]andtransformers[21,73]tolearn
diffusionmodel’sgeneralarchitecture,wefurthermotivate
hierarchical image representations, and to simultaneously
this decision as follows: (i) diffusion models are capable
leveragelarge-scaledatasets[45,85]. Earliersegmentation
ofgeneratingimageswithhighphotorealismanddiversity;
approachesreliedonspecializedarchitectures[7,29,67,75],
(ii)theyperformonparwithautoregressivepriorswhilebe-
suchasregionproposalnetworks[58]anddynamicconvo-
ingmorecomputationallyefficient[57,63];(iii)theylearn
lutions [34]. More recent approaches [8, 16] advocate an
high-qualityspatialrepresentations,advantageousfordense
end-to-end strategy but introduce complex loss functions,
prediction tasks; (iv) they naturally exhibit image editing
i.e.,bipartitematchingusingobjectqueriestoseparatein-
capabilities. Webuildfurtheruponthesepropertiesbyhar-
stances. Someworkshaveshownpromisingresultswithout
nessing the generative power of Latent Diffusion Models
1https://github.com/segments-ai/latent-diffusion-segmentation (LDMs)[59]forimagesegmentationapplications.
1
4202
naJ
81
]VC.sc[
1v72201.1042:viXraTorealizethisobjective,weintroduceLDMSeg,asim- totrainaseparatevisionmodel. Chenetal.[11]simplified
pleLatentDiffusionModelforSegmentation,visualizedin thisprocedurebyframingseveralvisiontasksaslanguage
figure1. Insummary,ourcontributionsarethreefold: modelingtasks,withinasinglemodel. Luetal.[49]also
• Generative Framework: This paper proposes a fully follow this route and show promising results for a large
generative approach based on Latent Diffusion Models varietyofvisionandlanguagetasksusingaunifiedframe-
(LDMs)forpanopticsegmentation. Ourapproachbuilds work. Eachworkpresentstheinputasasequenceofdiscrete
uponStableDiffusion[59]tostriveforsimplicityandto tokenswhicharesubsequentlyreconstructedviaautoregres-
easecompute.Wefirststudytheclass-agnosticsetuptolib- sivemodeling. Otherworks[4,78]leveragemaskedimage
eratepanopticsegmentationfrompredefinedclasses[40]. modelingtotrainasinglemodelformultiplevisiontasks.
• General-PurposeDesign:Ourapproachcircumventsspe- Differently,weleveragethedenoisingprocessincontinuous
cializedarchitectures,complexlossfunctions,andobject latentspace,whichiswell-suitedtohandledenseprediction
detectionmodules,presentinthemajorityofprevailing taskswithhigh-dimensionalinputs[12,32,59].
methods. Here,thedenoisingobjectiveomitsthenecessity
forobjectqueries,regionproposals,andHungarianmatch-
DenoisingDiffusionModels. Denoisingdiffusionmod-
ing[42]. Thissimpleandgeneralapproachpavestheway
els[32,62,63]wereintroducedasanewclassofgenerative
forfutureextensionstoawiderangeofdenseprediction
models. Recentstrategies[20,31,52,57]additionallylever-
tasks,e.g.,depthprediction,saliencyestimation,etc.
agetextasguidance,e.g.,viaCLIPembeddings,toachieve
• MaskInpainting: Wesuccessfullyapplyourapproachto
resultswithimpressiverealismandcontrol. Buildingupon
scene-centricdatasetsanddemonstrateitsmaskinpainting
itssuccess,afewdiffusion-basedsolutionsappearedinthe
capabilitiesfordifferentsparsitylevels.
segmentation literature. However, they have undesirable
2.RelatedWork properties: (i)theinabilitytodifferentiatebetweeninstances
inanimage[2,3,5],(ii)thenecessityforspecializedarchi-
Panoptic Image Segmentation. Panoptic segmenta- tecturesandlossfunctions[27,80],or(iii)thedependence
tion [38] has lately gained popularity as it combines se- onobjectdetectionweightsandbitdiffusion[12,13]. The
manticandinstancesegmentation. Inparticular,itsgoalis closestrelatedworkfromChenetal.[12],presentsaframe-
todetectandsegmentbothstuff-like(e.g.,vegetation,sky,
workforpanopticsegmentationbyleveragingthediffusion
mountains,etc.) andthing-like(e.g.,person,cat,car,etc.)
processinpixel-space. Incontrast,werigorouslyfollowla-
categories. Earlier works modified instance segmentation tentdiffusionmodelsbyrelyingoncontinuouslatentcodes
architecturestoadditionallyhandle(amorphous)stuff cat- withoutthenecessityforobjectdetectionmodules. Impor-
egories as they are hard to capture with bounding boxes. tantly,wecaneffortlesslyleveragepublicimage-diffusion
For instance, Kirillov et al. showed promising results by weights[59]aswekeepthearchitecturetask-agnostic.
makingindependentpredictionsusingsemanticandinstance
segmentationarchitectures[38],andlaterbyintegratinga
3.Method
semanticsegmentationbranchwithafeaturepyramidnet-
work(FPN)intoMaskR-CNN[37]. Otherworks[14,79] Preliminaries. Theaimofthispaperistotrainafullygen-
extendthisideabyrelyingonspecializedarchitecturesand erativemodelforpanopticsegmentationviathedenoising
lossfunctionsfrombothsegmentationfields. Morerecent diffusion paradigm [32]. While this generative approach
works [8, 15, 16, 74, 81, 84] handle things and stuff cate- resultsinlongersamplingtimesthandiscriminativemeth-
gories in a unified way via object queries and Hungarian ods, wejustifythisdecisionthroughfourdifferentlenses:
matching [42]. These works generally depend on special- (i)easeofuse–diffusionmodelsomitspecializedmodules,
izedmodules–suchasanchorboxes[58],non-maximum suchasnon-maximumsuppressionorregionproposalnet-
suppression[6,33],mergingheuristics[38,79],orbipartite works,arestabletotrain,andexhibitfastersamplingthan
matchingalgorithms[8,16,74],etc. –togeneratepanoptic autoregressivemodels[68];(ii)compositionality–thisgen-
masks.Instead,weproposeatask-agnosticgenerativeframe- erativeapproachcapturescomplexscenecompositionswith
worktobypassthesecomponents. Consequently,werefrain highrealismanddiversity,whilealsoenablingimageedit-
fromusingtask-specificaugmentations,suchaslarge-scale ing[57]; (iii)dataset-agnostic–werelyonspatiallystruc-
jitteringorcopy-pasteaugmentations[25]. turedrepresentationsthatarenottiedtopredefinedclassesor
taxonomies;(iv)computationalcost–latentdiffusionmod-
General-purpose Frameworks. Similar to our work, els[59]reducethecomputationalrequirementsbymodeling
a number of task-agnostic solutions have been sug- latentsinsteadofpixelswithanautoencoder[36,69]. Mo-
gested that cast vision problems as a generative process. tivatedbythesepoints,wecreateaframeworkthatbuilds
Kolesnikovetal.[41]minimizedtask-specificknowledgeby uponlatentdiffusionmodels’generativepower[57,59]for
learningtask-specificguidingcodes,usingalanguagemodel, thesegmentationtask.
2Sec. 3.1
Sec. 3.2
Image Descriptor
Figure2.OverviewofLDMSeg.Inspiredbylatentdiffusionmodels,wepresentasimplediffusionframeworkforsegmentationandmaskinpainting.The
approachconsistsoftwostages:(i)learncontinuouscodesztwithashallowautoencoderonthelabels(Sec.3.1);(ii)learnadenoisingfunctionconditioned
onimagelatentszi(Sec.3.2).Inthesecondstage,theerrorbetweenthepredictednoiseϵˆandtheappliedGaussiannoiseϵisminimized.Duringinference,
wetraversethedenoisingprocessbystartingfromGaussiannoise.Themodelsftandfirespectivelyencodethelabelsandimages.Whilewerelyonthe
imageencoderfifromStableDiffusion[59],wefocusonftandgforsegmentation.Ouraimistoprioritizegeneralitybylimitingtask-specificcomponents.
Problem Setup. The considered segmentation task re- guidedbyimagefeaturesz . Wemakeasimilarderivation
i
quires a dataset of images X = {x ,...,x } and corre- as[57]butconditionthegenerativeprocessonimages. For-
1 n
spondinggroundtruthpanopticsegmentationmasksY = mally, this two-step procedure allows us to construct the
{y ,...,y }.Wedon’tmakeanydistinctionbetweenthings conditionaldistributionp(y|x)viathechainrule:
1 n
orstuff classesbuthandleallclassesidentically. Particularly,
p(y|x)=p(y|z ,z )·p(z ). (1)
we first only predict the individual panoptic IDs and not t i t
theindividualclasses,renderingourapproachclass-agnostic. Both terms p(z ) and p(y|z ,z ) are reflected in our
t t i
Wewillalsoshowresultsintheexperimentswhenpredicting framework as two separate training stages (see figure 2).
class-labels. Section 3.1 discusses the learning of the prior via autoen-
Assume that all images are of size H ×W. We train codingandsection3.2leveragesthispriortotrainalatent
asegmentationmodeltorealizethemappingR3 H W → diffusionmodelforsegmentation.
× ×
RN H W. For each pixel the model performs a soft as-
× × 3.1.Stage1: CompressSegmentationTargets
signmentovertheinstances{1,...,N}. Letthelatentrep-
resentations z and z respectively refer to the image and In the first step, we train a network to compress the task-
i t
targetfeaturesaftertheprojectiontoD-dimensionallatents specific targets into latent codes. While we will focus on
RD H/f W/f,wheref ∈Ndenotestheresizingfactor. panopticsegmentationmasks,wenotethatasimilarstrategy
× ×
Fromahigh-levelperspective,ourmethodhastwokey isapplicabletootherdensepredictiontargets,suchasdepth
components: learningtheprioroversegmentationlatentsz orsurfacenormalsestimation.
t
andlearningaconditionaldiffusionprocessinlatentspace.
First, we train a shallow autoencoder to capture the prior Motivation. Themotivationtodesignashallowautoen-
distributionp(z )thatlearnstocompressthelabelsintocom- coderstemsfromtheobservationthatsegmentationmaps
t
pactlatentcodesz . Second,wetrainadiffusionprocess– differfundamentallyfromimagesastheyarelowerinen-
t
conditionedontheimageandtargetfeaturesp(y|z ,z ).This tropy. First,thesemaskstypicallycontainonlyasmallnum-
t i
componentisresponsiblefordecodingnoisytargetfeatures, berofuniquevaluesastheyonlycapturetheobject’sgeneral
3shapeandlocationinthescene. Second,neighbouringpix- avalidchoice,weoptforacategoricallossduetosegmenta-
elsarestronglycorrelatedandoftenidentical,resultingin tiontask’sdiscretenature.Thereconstructionlosscomprises
largelyspatiallyredundantinformation. Weconcludethat twolossterms:(i)thecross-entropylossL enforcesunique
ce
thesegmentationtaskonlynecessitatesashallownetworkto andconfidentpredictionsforeachpixel;(ii)themaskloss
efficientlycompressthetask’stargetsandtoreliablycapture L furtherrefinesthesegmentationmasksbytreatingeach
m
the prior distribution p(z ). Interestingly, this conclusion instanceindividually,alleviatingtheneedforexhaustively
t
holdsforamyriadofdensepredictiontasks,suchaspanop- labeledimages. ThistermisimplementedviatheBCEand
ticsegmentation,depthprediction,saliencyestimationetc. Dicelosses[29,66]. Notethatthisautoencodingstrategy
Thisalsojustifieswhywerefrainfromusingmoreadvanced preventstheneedforHungarianmatching[42].
autoencodersthatrelyoncomputationallydemandingarchi- Typically,latentdiffusionmodelsincorporateapenalty
tectureswithadversarialorperceptuallosses[35,83],e.g., term Ω into the loss formulation to align the bottleneck
VQGAN[23],typicallyusedtoencodeimages[59]. latentswithastandardGaussiandistributionN(0,I). This
termgenerallytakestheformofaKLdivergence,resulting
inavariationalautoencoder[36]. However,weempirically
Encoding. Weanalyzeseveralencodingstrategiestorep-
observedthatweightdecayregularizationsufficestokeep
resenttheinputsegmentationmapy,latentsz andoutputyˆ.
t the weights w, and by extension the latents z , bounded.
1. Inputy: Let N denote the maximum number of in- t
Consequently,themagnitudeofthemodel’sweights||w||
stancesperimage.RGB-encoding(3channels),bit-encoding 2
ispenalized,resultinginthefinalloss:
(log N channels), one-hot-encoding (N channels), or
2
positional-encoding[73]arealljustifiableoptions. Wefol-
L (w;y)=L (w;yˆ,y)+Ω(z ,w)
low[12]inrepresentingtheinstanceIDsasbits,whichisa AE rec t (2)
naturalchoicegivendiscretesegmentationmaps. =L ce(w;yˆ,y)+L m(w;yˆ,y)+λ||w||2 2,
2. Latentcodez : No vector quantization or dimensional-
t
whereyˆdenotesthereconstructedsegmentationmap. Fur-
ityreductionisappliedtothelatentcodes. Weempirically
thermore, we follow PointRend [39] to select logits that
foundthatdirectlyusingthecontinuouslatentsresultedina
correspondwithuncertainregions. Thisstrategylimitsthe
simplearchitecturaldesignwithasmallmemoryfootprint
memoryconsumptionaswellasthetotaltrainingtime.
(shallow)whilesimultaneouslydemonstratingencouraging
segmentationresultsforbothreconstructionandgeneration.
3.2.Stage2: TrainaDenoisingDiffusionModel
3. Outputyˆ: Theoutputisone-hotencodedtotraintheau-
toencoder with a standard cross-entropy loss. In practice, Image-ConditionedDiffusionProcess. Thesecondstage
wedefineN outputchannelstoreconstructN differentin- of the framework models the function h via a conditional
stancesinascene,e.g.,128. diffusionprocess. Inparticular,weaimtolearnadiffusion
processoverdiscretetimestepsbyconditioningthemodel
onimagesandnoisysegmentationmasksforeachindivid-
ArchitecturalDesign. Thearchitectureoftheautoencoder
ualtimestep. WefurtherfollowtheformulationofStable
follows a simple and shallow design. It comprises an en-
Diffusion[59]. Hence,thisprocessiscarriedoutinajoint
coderf anddecoderg,yieldingtheoverallfunctiong◦f .
t t latentspace–usingourtrainedsegmentationencoderf and
Theencoderf includesonlyafewstridedconvolutionsto t
t givenimageencoderf [59]–inordertoprojectthetargets
compressthetargetsandisinspiredbyControlNet[82]. For i
andimagestotheirrespectivelatentrepresentationsz and
instance,targetswithsize512×512canberesizedefficiently t
z . Wediscussthetrainingandinferenceproceduresnext.
to64×64,inordertoleveragethelatentspaceofStableDif- i
Duringtrainingwelinearlycombinethenoiseϵwiththe
fusion[59]bystacking3convolutionsofstride2. Similarly,
latentsz forarandomlysampledtimestepj ∈[1,T]:
thedecodergconsistsofoneormoretransposeconvolution t
toupscalethemasksandminimizealossatpixel-level. Asa z˜j =(cid:112) α¯ z +(cid:112) 1−α¯ ϵ, (3)
consequence,thenumberoftrainableparametersisatleast2 t j t j
ordersofmagnitudessmallerthantheamountofparameters
whereα¯ isdefinedbythenoiseschedule,followingRom-
in the diffusion model h (≈ 2M vs. 800M). The model’s j
bachetal.[59]. Thelatentsaresubsequentlyfused{z˜j,z }
shallowdesignbringsseveraladvantagestothetable: fast t i
viachannel-wiseconcatenationasz ∈R2D H/f W/f,be-
training,goodgeneralizationacrossdatasets,andapplicable c × ×
fore feeding them to the UNet [59, 60] h , parameterized
toinpaintingwithoutarchitecturalchanges. θ
with weights θ. Despite the examination of various fus-
ingtechniques(i.e.,fusingintermediatefeaturesviacross-
LossFunction. Theautoencoderaimstominimizethere- attentionorviamodality-specificbranches)wefoundthat
constructionerrorL betweentheoutputsyˆandtheone-hot straightforward concatenation at the input works surpris-
rec
encodedsegmentationmasksy. Whileregressionlossesare inglywell. Attheoutput,thereconstructionerrorbetween
4Denoising steps
Image 1 2 3 4 5 6 7 Ts = 8
Figure3.DiffusionProcessandSNR. (1)Duringtrainingwerandomlysampleatimestepfrom[1,T]inthedenoisingprocess.WecanincreasetheRGB
image’simportancebystrengtheningthenoise:(i)Following[12,59],wedownscalethelatentszcusingscalingfactors∈Randdemonstrateitsimpact–
Row1(s=1.0)isclearlyeasiertodecodethanrow2(s≈0.18[59]).(ii)Lossesfortimestepsnear0arefurtherdownscaledtoavoidoverfitting.Both
strategiesenforcesthemodeltofocusontheRGBimageingeneratingplausiblesegmentationmaps.Notethatwedon’tapplyexplicitconstraintstotheprior
distributionp(zt),e.g.,matchastandardGaussianN(0,1).(2)DuringsamplingthedenoisingprocessistraversedfromrighttoleftinTsiterations.
the added Gaussian noise ϵ ∼ N(0,I) and the predicted progressively adds more details to the segmentation map
noiseϵˆisminimized[32]as as controlled by the input image. We rely on the DDIM
L (θ;ϵ)=E
(cid:2)
||ϵ−h (z
,j)||2(cid:3)
, (4)
scheduler[63]toapplythisdenoisingprocessoverasmall
LDMSeg zc,ϵ ∼N(0,I),j θ c 2 numberofsamplingtimestepsT
s
<<T. Here,theprevious
where each timestep j is uniformly sampled from [1,T]. sampleiscomputedas
To reduce training time, we downscale the loss for small √
t ti om -ne ost ie seps r, ai t. ie o. (j S< NR2 )5 a% nd·T ar. eT th he us se rl ea lt ae tn ivts eh lyav ee asa yh ti ogh des nig on ia sel- z˜ tj −1 = √α¯ α¯j − j1 (z˜ tj −(cid:112) 1−α¯ j)·ϵˆ+(cid:112) 1−α¯ j −1·ϵˆ, (5)
withoutmodelingsemantics. Wefinallyrefertoalgorithm1
whichfollowsdirectlyfromequation3. Recallthatthisstrat-
foranoverviewoftheforwardpassduringtraining.
egyallowsustomodelp(y|z ,z ),asdefinedinequation1.
t i
Finally, algorithm 2 provides the details of the sampling
Algorithm1ForwardpassinPytorch.
processandfigure3servesasanillustration.
# f_i, f_t: image encoder, segmentation encoder
# h: denoising UNet
# x: images of size [bs, 3, H, W]
# y: bit maps of size [bs, log(N), H, W] Image Descriptors. Complementary, we briefly experi-
# s, T: scaling factor, number of train steps
mentedwithaddingguidanceviaStableDiffusion’scross-
y = 2 * y - 1 # bit maps to range [-1, 1]
x = x / 127.5 - 1 # images to range [-1, 1] attentionlayerstoimprovesamplequality. Thismechanism
z_t = f_t(y) * s # encode bit maps and rescale
z_i = f_i(x) * s # encode images and rescale wasinitiallyintendedfortextembeddings,butcanbeusedto
feedanyimagedescriptor. Wedidn’tobserveimprovements
j = torch.randint((bs,) 0, T)
noise = torch.randn_like(z_t) withself/weakly-supervisedpriors,likeimageembeddings
z_t = scheduler.add_noise(z_t, noise, j) # Eq. 3
z_c = torch.cat([z_t, z_i], dim=1) from CLIP [55], or when using captions from BLIP [43].
noise_pred = h(z_c, j)
Wecurrentlybypassthecross-attentionlayerswithskipcon-
loss = torch.sum((noise_pred - noise)**2, dim=[1,2,3])
loss = torch.mean(loss * scheduler.weights[j]) nections,butthisisapromisingdirectiontoexplorefurther.
scheduler.weights:attribute-arrayoflengthTwithlossweights
3.3.SegmentationMaskInpainting
Algorithm2SamplingprocessinPytorch.
# f_i, g: image encoder, segmentation decoder Setup. Our image-conditioned diffusion model is well-
# h: denoising UNet
# x: image of size [3, H, W] suitedtocompletepartialsegmentationmasks. Thisispo-
# T, Ts: number of training steps, inference steps tentiallyusefulfor(i)completingsparse2Dsegmentation
x = x / 127.5 - 1 # image to range [-1, 1]
z_i = f_i(x) * s # encode image and rescale masksobtainedfromprojecting3Dpointcloudsegmentation
z_t = torch.randn_like(z_i) # Gaussian noise
for j in scheduler.inference_steps: labelsontoimages;(ii)refiningnoisysegmentationlabels
z_c = torch.cat([z_t, z_i], dim=0) thatwerepropagatedfromoneframetothenextusingop-
noise_pred = h(z_c, j)
j_prev = j - T // Ts ticalflow;(iii)interactiveimagelabelingwithroughbrush
# apply Eq. 5
z_t = scheduler.step(z_t, noise_pred, j, j_prev) strokes. Incontrasttoimageediting[59],theconsideredseg-
y_pred = g(z_t) # decode segmentation latents mentationmapscontainemptyregions,whichweinitialize
scheduler.inferencesteps:attribute-chosensamplingtimesteps
scheduler.step:function-predictprevioussample
with0’s. Wesimulatedifferentsparsitiesinsection4.2.
Existingstate-of-the-artapproachesarenotdesignedfor
Duringsampling,thedenoisingprocessistraversedfrom theseapplicationsastheydecoupletheoutputfromtheinput
righttoleft(seefigure3). ItstartsfromGaussiannoiseand viabipartitematching[8,16]. Theyrequireadditionalsteps
5tomatchpredictionswiththegivenpartialsegmentationIDs activations.Thisresultsinaresizingfactorf of8andlatents
(e.g.,viamajorityvoting). Incontrast,diffusionmodelsact withsize4×64×64. Werelyontheimageencoderf from
i
onthecorruptedmasksbydefault. Rombachetal.[59](VAE)toconverttheimagextolatents
of the same size. We adopt Stable Diffusion’s pretrained
InpaintingProcess. Ideally,wecantackleinpaintingprob- weightstoinitializetheUNeth, anditsrescalingfactors
lems out-of-the-box, i.e., without finetuning. To achieve tolowertheSNRass·z c [59]. Notably,4zero-initialized
thisgoal,theinferenceloop,previouslydiscussedinalgo- channelsareappendedtothefirstconvolutionallayerofh,
rithm 2, is modified. Assume that we have a dataset of allowing us to operate on the concatenated input z c. We
pairs(y,m). Eachpaircontainsasparsesegmentationmask additionallyincorporateself-conditioning[13]toimprove
y ∈{0,1}log 2N ×H ×W,representedasabitmap,andavalid sample quality. Our segmentation decoder g consists of
maskm∈{0,1}H W,representedasabooleanmask.Now, 2 transpose convolutions, resulting in an upscaling factor
×
thediffusionprocessshouldfillinthemissingregionsiny, of4. Werandomlysamplej from1000discretetimesteps
determinedbythezerosinm. Ateachstepofthedenoising and linearly decay the loss for the bottom 25% to lower
process,thelatentscorrespondingtothegivenpixelsinm the impact of samples with high SNR. The AdamW [48]
arefixed. Thekeydifferenceswiththesamplingprocessare optimizerisadoptedwithalearningrateof1e −4andweight
highlightedinalgorithm3. decay of 5e −2. Finally, the first stage is trained for 60k
iterationswithabatchsizeof16whilethesecondstageis
trained for 50k iterations with a batch size of 256 unless
Algorithm3InpaintingprocessinPytorch.
statedotherwise.
# f_i, f_t: image encoder, segmentation encoder
# g, h: segmentation decoder, denoising UNet
# x: image of size [3, H, W]
# y: sparse bit map of size [log(N), H, W] InferenceSetupandEvaluationProtocol. Duringinfer-
# m: boolean mask indicating valid pixels, size [H, W]
# T, Ts: number of train steps, inference steps ence,theDDIMscheduler[64]generatessampleswith50
y = 2 * y - 1 # bit maps to range [-1, 1] equidistanttimestepsinlatentspace.Attheendofthedenois-
y[:, m] = 0 # set invalid regions to 0
x = x∼ / 127.5 - 1 # image to range [-1, 1] ingprocess,wedecodeandupscalethesegmentationlogits
z_i = f_i(x) * s # encode image and rescale (outputofg)withafactorof2usingbilinearinterpolation.
z_t_masked = f_t(y) * s # encode bit map and rescale
Theargmaxoperatorproducesthefinal(discrete)per-pixel
m = interpolate(m, size=z_i.shape[-2:])
z_t = torch.randn_like(z_i) segmentationmasks. Webenchmarkourapproachwiththe
for j in scheduler.inference_steps:
PanopticQuality(PQ)evaluationprotocol,definedbyKir-
z_c = torch.cat([z_t, z_i], dim=0)
noise_pred = h(z_c, j) illovetal.[38]. PQistheproductoftwoqualitymetrics:
# apply Eq. 3 (solve for z_t)
thesegmentationqualitywhichmeasurestheintersection-
z_t = scheduler.remove_noise(z_t, noise_pred, j)
z_t[:, m] = z_t_masked[:, m] # keep latents over-unionofmatchedsegments(IoU)andtherecognition
j_prev = j - T // Ts qualitywhichmeasurestheprecisionandrecall(F-score).
# apply Eq. 3
z_t = scheduler.add_noise(z_t, noise_pred, j_prev)
4.2.PanopticSegmentationResults
y_pred = g(z_t) # decode segmentation latents
scheduler.inferencesteps:attribute-chosensamplingtimesteps Image-conditionedMaskGeneration. Wevisualizepre-
scheduler.removenoise:function-removenoisefromsample
scheduler.step:function-predictprevioussample dictionsofLDMSeginfigure4and5onafewvalidationim-
agesfromCOCO[45]andCitysapes[19]respectively. The
4.Experiments modeldividesanimageintosemanticallymeaningfulgroups
and captures different objects in the scene, e.g., persons,
4.1.ExperimentalSetup
horses,cats,cars,etc. Notably,LDMSegdirectlygenerates
Dataset. We conduct the bulk of our experiments on non-overlappinginstancemasks,requiredforpanopticseg-
COCO[45]. Thestandardtrainandvalsetsareadopted mentation. Additionally,figure10visualizesthepredictions
fortraining(118kimages)andevaluation(5kimages)respec- whensamplingfromourgenerativemodel(seefirstcolumn)
tively. Duringtraining,weusethepanopticmasks–contain- with different seeds. Different Gaussian noise maps lead
ingbothstuff andthingsclasses.Fortheclass-agnosticsetup, to different segmentation IDs. Compared to other frame-
weonlyaimtopredictinstances,withouttheircategories. works, the sampled noise resembles the object queries in
Mask2FormerandDETR[8,16],ortheregionsproposals
ArchitectureandTrainingSetup. Themaximumnumber inMaskR-CNN[29].
ofdetectablesegmentsN issetto128.Weresizetheinputto
512×512,applyrandomhorizontalflipping,andrandomly MaskInpainting. Figure6demonstrates theinpainting
assign segmentation IDs to integers in [0,N − 1]. The performancefortwodifferentgranularitylevelsinmaskm.
segmentationencoderf processesthepanopticmaskyby We mask random regions in the ground truth to simulate
t
leveraging3convolutionallayerswithstride2andSiLU[22] globalandlocalcompletiontasks. Theresultsdemonstrate
6Figure4.QualitativeResultsonCOCO. ThefiguredisplaysresultsonCOCOval2017.Wefollowtheinferencesetup(section3.2)tosamplefromour
model. Onlytheargmaxoperatorisappliedforpost-processing. Keyobservation:ourmodeldisentanglesoverlappinginstancesinchallengingscenes
withoutcomplexmodulesorpost-processing.Tovisualize,segmentsareassignedtorandomcolors,andmissing(VOID)pixelsinthegroundtruthareblack.
Global Mask Inpainting Sparse Mask Inpainting
Figure5.QualitativeResultsonCityscapes. Thefiguredisplayspredic-
tionsonCityscapes’svalidationset.Keyobservation:themodelproduces
panopticmaskscoveringvariousvisualconcepts. Thinstructures,e.g., Figure6.MaskInpainting.Thefigurevisualizesgeneratedsamplesfor
poles,aredifficulttocaptureduetothediffusionprocessinlatentspace. differentgranularitylevelsbyfollowingsection3.3.Keyobservation:the
modelisabletofillinmissingregionsbypropagatingthepartiallygiven
thatLDMSegisabletofillpanopticmasksviathediffusion segmentationIDsusinganimage-conditioneddiffusionprocess. Global
maskinpainting(left)worksout-of-the-boxwhilesparsemaskinpainting
process,andwithoutrequiringadditionalcomponents.
(right)showsinaccuracies.Wehypothesizethatthiscanbeaddressedby
Further,figure9displaysthePQmetricforawiderange furtherfinetuningLDMsegonsparseinpaintingmasks.
ofdropratesdandblocksizesB.Whileourmodelperforms
wellforglobalinpainting,thereisstillroomforimprovement 50.8%PQonCOCO’svalidationset. However,thereisstill
whenconsideringverysparseinpaintingtasks(e.g.,dropping agapwithspecializedmethods,i.e.,MaskFormer[15,16]
16×16 pixels with a drop rate of 90%). Particularly, for whichobtains59.0%PQ.
dropratesabove70%andsmallblocksizes,thereduction Tocomparewithstate-of-the-artmethods,wetrainour
rate(negativeslope)inPQincreases. Furtherimprovements method for 200k iterations, as well as a classifier with a
canbeobtainedbyfinetuningormodifyingtheautoencoder. simplesegmentationhead[9]ontopoftheproposals. We
For instance, shallower encoders f could adapt better to comparetheobtainedpanopticsegmentationintable2. Our
t
sparseinputs,inexchangeforlowerreconstructionquality. generativeapproachcanalmostmatchseveralspecializedap-
proaches(43.3%vs.44.1%)[37,79]. LDMSegoutperforms
State-of-the-artComparison. Table1comparesLDMSeg Painter[78]whilenotrequiringdifferentencodingstrategies
withpriorartfortheclass-agnosticsetup. Ourmodelobtains forthingsandstuff categoriesorNMS(43.3%vs.41.3%).
7
gninoitidnoc
rof
egamI
hturt
dnuorG
selpmaS
detareneG
gninoitidnoc
rof
egamI
hturt
dnuorG
selpmaS
detareneG
gninoitidnoc
rof
egamI
hturT
dnuorG
ksaM
laitraP
neviG
selpmaS
detareneG82.0% 80 80
80
70
70 70
62.0% 60
60 60
50.8% 50
50
40 50 B=256
40 B=128
PQ 30 PQ 40 B=64
30 SQ SQ B=32
RQ 20 RQ B=16
30
0 5 10 15 20 25 30 35 40 45 50 1e−7 1e−6 1e−5 1e−4 0.1 0.3 0.5 0.7 0.9
Numberofsamplingsteps-Ts KLlossweight Sparsity-droprated
Figure7.AblationSamplingSteps.Thefigure Figure8.AblationKLLoss.Thefigureshows Figure 9. Sparsity Experiment. The figure
showstheimpactofthenumberofsamplingsteps theimpactofaKLlossonthePQ,SQ,andRQ showsthePQfordifferentdropratesandblock
onthePQ,SQ,andRQmetrics(COCOval). metrics(COCOval). sizes(B×Bpixls)inmaskm(COCOval).
Method Backbone PQ↑
Specialistapproaches:
PanopticFPN[37] ResNet[28] 44.1
DETR[8] ResNet[28] 45.6
MaskFormer[15] ResNet[28] 46.5
Mask2Former[16] Swin-L[46] 57.8
Generalistapproaches:
Painter[78] ViT[21] 41.3
UViM[41] ViT[21] 45.8
Pix2Seq-D†[12] ResNet[28] 50.3
LDMSeg(Ours) UNet[60] 43.3
Table2. State-of-the-artComparison. Thetablepresentsthepanop-
Figure10. ImpactofGaussianNoise. WeshowpredictionsonCOCO ticsegmentationresultsonCOCOval2017. (LDMSegvs.specialists)
val2017whensamplingdifferentnoisemaps.Theimagesandthegen- Ourapproachalmostmatchestheperformanceofseveralspecialistson
eratedmasksarevisualizedinrow1androws2&3respectively. All thepanopticsegmentationtask. (LDMSegvs.generalists)LDMSegout-
predictionsinthesamerowweregeneratedfromthesamenoise,visualized performsPainter, butnotPix2Seq-D andUViM.NotethatPix2Seq-D
incolumn1. Keyobservation:differentinitializationsgeneratepanoptic leveragesadditionalannotatedtrainingdata(†)fromObjects365[61]and
maskswithdifferentIDs(seerow2vs.row3).TheGaussiannoiseimplic- UViMissensitivetotheadoptedguidingcodes,necessitatingspecialmech-
itlyfunctionsasobjectqueries[8,15]orregionproposals[29]. anismslikecodedropoutandoptimalcodelength[41].Incontrast,wecan
performmaskinpaintingout-of-the-box.
Method Backbone PQ↑
Pix2Seq-D[12]achieveshigherPQbyadoptingadditional
Specialistapproaches:
MaskFormer[15] ViT[21] 54.1 trainingdata[61]andapixel-levelobjective. Weexpectthat
Mask2Former[16] ViT[21] 56.5 largerdatasetsandahigherresolutionwillhelpinclosing
Mask2Former⋆[16] ViT[21] 59.0
thisgap.
Generalistapproaches:
LDMSeg†(Ours) UNet[60] 50.9 Incontrasttopriorart[41,78],LDMSegcantacklethe
LDMSeg(Ours) UNet[60] 50.8 task of mask inpainting out-of-the-box, and without spe-
cificprocessingtechniques. AsUViM[41]isnotcapable
Table1.ComparisonwithSpecialists(class-agnostic).Theresultsarere-
portedonCOCOval2017.Wedifferentiatebetweenspecialistandgener- ofmaskinpaintingbydesign,it’sunclearhowPainter[78]
alistmethodsandtrainfor50kiterations.TrainingdetailsforMask2Former and Pix2Seq-D [12] should be modified: (i) Painter [78]
areprovidedinthesupplementarysinceweuseasimplifiedversion,similar relies on a predefined color palette that differs for things
toSAM[40]withMAE[30]orDINOv2[53]initialization(see⋆). (†)
and stuff classes. Due to this fixed encoding (e.g., center
denotestheadoptionofStableDiffusion’sVAEwith84Mtrainableparam-
eters,resultingin20×moreparameterscomparedtoourstandardsetup ofmassforthingscategories),itisunabletocompletethe
(84Mvs.2M)forthesameperformance(seesection3.1). given (random) segmentation IDs. Furthermore, its hefty
post-processingisnotguaranteedtopreservetheIDsinthe
While UViM [41] obtains better results, it’s arguably less partialinputmask,whichisdetrimentalformaskinpainting.
robustduetoitsstrongrelianceonspecificcodelengthsand Ourmethodisconditionedonlatentsthatarenotconfined
codedropout,inordertotrainitsautoregressivelanguage toatask-specificencoding,whichpreservesthegiveninput
model(LM).Incontrast,weleveragelatentcodescentered masks. (ii) Future research could explore if our inpaint-
around zero using a shallow autoencoder, which benefits ingstrategy(seesection3.3)canbeextendedtoBitDiffu-
robustness. Toincreasethedenoisingdifficulty,wesimply sion[13],usedinPix2Seq-D[12]. However,theunreleased
dropthescalingfactors(seesection3.2). Unsurprisingly, weightsposesachallengeinadoptingthisparadigm.
8
]%[ytilauQ
0 deeS
1 deeS
gninoitidnoc
rof egamI
]%[ytilauQ
]%[QPAblation Studies. We ablate the number of inference scheduler. ThecodeisdevelopedinPytorch[54]andwill
timestepsinfigure7andobservethatthePQmetricstarts bemadeavailableaswellasourmodels.
plateauingat20iterations. Longerinferenceschedulescan
furtherimprovetherecognitionquality(i.e.,areductionin
Baseline. Mask2Former[16]isaspecializedsegmentation
falsepositivesandnegatives),whilekeepingthesegmenta-
frameworkthatproducesexcellentresultsforthepanoptic
tionqualityconstant(∼ 80%). Wereportourbestresults
segmentationtask. WefollowthetrainingrecipefromViT-
with50iterations.
Det[44]andSAM[40]toleverageplainViTbackbones[21]
Figure 8 demonstrates the impact of adding a KL loss
withMAEpretrainedweights[30]. Specifically,themodel
to the autoencoding objective in equation 2. It aligns the
consistsofthevisiontransformerbackbone,ashallowneck,
latentswithastandardGaussiandistribution.Unsurprisingly,
andamaskdecoder. Thelattercontains6maskedattention
increasingitslossweightbeyond1e 5 isnotbeneficialas
− decoderlayersand128objectqueries,following[16]. The
ithurtsthereconstructionquality. Weconcludethatweight
lossrequiresHungarianmatching[42]tohandlethepermu-
decaysufficestokeepthesegmentationlatentsbounded.
tationinvarianceofthepredictionsduringtraining. Toreport
Table1verifiesthehypothesisthatpanopticmasksdon’t
theresults,wefollowitspost-processingstrategytocombine
requireapowerfulVAE.Inparticular,wefinetunedStable
the classification and mask branches. We adopt the same
Diffusion’s VAE [59] instead of our shallow autoencoder,
augmentationsasinthemainpaper,i.e.,squareresizingand
as discussed in section 3.1. However, this results in the
randomhorizontalflipping. Thisbaselinestrikesagoodbal-
samefinalPQ(50.9%vs.50.8%). Ourshallowautoencoder
ancebetweenperformance,complexity,andtrainingspeed.
contains 20× less parameters, which reduces the training
Additionally,weprovideresultsbyrelyingonthebackbone
timeofstage2by20%(seesection3.2). Thisobservations
andpretrainedweightsofDINOv2[53],aswefoundthisto
likelyholdsforothervisiontasksaswell.
outperformMAEpretrainedweightsforaViT-Bbackbone.
Wetrainthesemodelswithabatchsizeof32andalearning
5.Conclusion rateof1.5e 4for50kiterationson8×16GBV100GPUs.
−
In summary, this paper presented LDMSeg, a simple yet
powerfullatentdiffusionapproachforsegmentationandin- EvaluationProcedure. Ourmodelproducesexcellentpre-
painting. It builds upon Stable Diffusion [59] to learn an dictions by simply relying on the argmax operator. No
image-conditioneddiffusionprocessforsegmentationinla- additionalprocessingisusedforthevisualizations(seerow
tentspace. Theexperimentalresultsarepromising: (i)our 3infiguresS2andS3).InordertoreportthefinalPQmetric,
frameworkbypassesspecializedobjectdetectionmodules, however,weeliminatenoisebythresholdingthepredictions
such as region proposal networks and bipartite matching, at 0.5 (after applying softmax) and filtering out segments
togeneratepanopticmasks;(ii)ourmodelunlockssparse withanareasmallerthan512. Theseresultsareshowninthe
panopticmaskcompletionwithoutfinetuning. Hence, we lastrowoffiguresS2andS3. NoticethatMask2Former’s
hopethatthisworkwillsparkfurtherinterestintodesigning training objective does not impose exclusive pixel assign-
general-purposeapproachesfordensepredictiontasks. Due ments,henceitneedsadditionalpost-processingsteps.
tothesimpleandgeneraldesignofLDMSeg,thereisstill
roomforimprovementintermsofaccuracyandsampling C.AdditionalResults
speed. Evidentfuturedirectionsinclude: trainingthepre-
We show the panoptic segmentation results on COCO
sentedmethodonlargerdatasetsandextendingittoother
val2017[45]infigureS1using50timesteps.Additionally,
densepredictiontasks,e.g.,depthprediction.
weshow(class-agnostic)masksinfiguresS2andS3. The
inputimagesareresizedto3×512×512duringtraining
A.SupplementaryMaterial
andthediffusionprocessactsonlatentsofsize4×64×64.
We discuss the implementation details in section B, addi- Tovisualizethemasks,weassigneachsegmenttoarandom
tional results in section C, limitations in section D, and color. Overall, we conclude that the model is capable of
broaderimpactinsectionE. generatinghigh-qualitypanopticmasks.
FigureS4displaystheresultsfordifferenttimestepsinthe
B.ImplementationDetails denoisingprocess. Longersamplingbenefitsthegeneration
ofdetails,suchascapturingsmallobjectsinthebackground
Model Card. Our best model is trained for 200k itera- oranobject’sedges. Thisapproachnecessitates10to50it-
tionsonCOCOwithmixedprecisiontrainingon8×40GB erationstoproducehigh-qualitysegmentationmasks,which
NVIDIAA100GPUsusingtheGoogleCloud. Werelyon isinlinewithlatentdiffusionmodelsforimages[59]. Fur-
the pretrained Stable Diffusion [59] weights provided by thermore,asthemodelwasforcedtodistinguishbetween
HuggingFace[24]. Wealsoadoptitssettingsforthenoise differentinstancesduringtraining,it’sunlikelythatdifferent
9FigureS1.PanopticSegmentation-QualitativeResults. ThefiguredisplaysthepanopticsegmentationforseveralimagesintheCOCOvalset.
instances will be grouped during inference. Interestingly, D.LimitationsandFutureWork
the model iteratively improves the predictions while not
Undoubtedly,ourmodelhasseverallimitationsdespiteits
reinforcingmistakesduringthegenerativeprocess.
generaldesign. Wediscusstwolimitations: (i)themodel
Table 1 provides the inference times for different sam-
canmisssmallbackgroundobjectsduetotheprojectionto
plingdurations. Unsurprisingly,diffusionmodelsareatleast
latentspace;(ii)themodelisslowerduringinferencethan
anorderofmagnitudeslowerthanspecializedmethods(com-
specializedsegmentationmodelsduetotheadoptionofadif-
pare0.47svs.0.02s). However,recentresearch[65]looks
fusionprior. Inexchange,ourmethodissimple,general,and
promisingtogeneratehigh-qualitymasksinasinglestep.
unlocksout-of-the-boxmaskinpainting. Finally,increasing
Finally,wenotethatloweringthelossforsmalltimesteps
theresolutionofthelatents,enablingopen-vocabulary[55]
(e.g.,j <25%)isnotcrucial,butspeeds-uptrainingby0.3
detection, andincludingadditionaldensepredictiontasks
to0.5%PQ.Weaimtoremovethisinfuturework.
areexcitingdirectionstoexplorefurther.
Method #Iters Time[s] PQ[%] E.BroaderImpact
Mask2Former 1 0.02 59.0
LDMSeg 10 0.24 50.0 Thepresentedapproachreliesonpretrainedweightsfrom
LDMSeg 20 0.47 50.7
Stable Diffusion. Consequently, our model is subject to
LDMSeg 50 1.14 50.8
thesamedatasetandarchitecturalbiases. Theusershould
beawareofthesebiasesandtheirimpactonthegenerated
Table1.Inferencetimetogenerateanimage-conditionedmaskona4090
GPU.Thetableprovidestheresultsforadifferentamountofdenoising masks.Forinstance,thesetypesof(foundation)modelshave
steps.WecomparetoMask2Former. theabilitytohallucinatecontent.
10FigureS2.ExamplesonCOCO(1). ThefiguredisplaysthegeneratedmasksontheCOCOvalset.
FigureS3.ExamplesonCOCO(2). ThefiguredisplaysmoregeneratedmasksontheCOCOvalset.
11
gninoitidnoc
rof
egamI
hturT
dnuorG
selpmaS
detareneG
yalrevO
gninoitidnoc
rof
egamI
hturT
dnuorG
selpmaS
detareneG
yalrevOFigureS4.Resultsfordifferenttimesteps. Thefigurevisualizestheimage-conditionedsamplesforthetimesteps1,5,10,20,and50inthediffusion
process.Longersamplingisrequiredtocapturemoredetails,whichisbeneficialforcomplexscenes(e.g.,carsinthebackgroundincolumn4).
12
segamI
pets
1
spets
5
spets
01
spets
02
spets
05References [15] BowenCheng,AlexanderG.Schwing,andAlexanderKir-
illov. Per-pixelclassificationisnotallyouneedforsemantic
[1] Hassan Abu Alhaija, Siva Karthik Mustikovela, Lars
segmentation. InAdvancesinNeuralInformationProcessing
Mescheder,AndreasGeiger,andCarstenRother. Augmented
Systems(NeurIPS),2021. 2,7,8
realitymeetscomputervision:Efficientdatagenerationfor
[16] BowenCheng,IshanMisra,AlexanderG.Schwing,Alexan-
urban driving scenes. International Journal of Computer
derKirillov,andRohitGirdhar. Masked-attentionmasktrans-
Vision(IJCV),2018. 1
formerforuniversalimagesegmentation. InConferenceon
[2] TomerAmit,TalShaharbany,EliyaNachmani,andLiorWolf.
ComputerVisionandPatternRecognition(CVPR),2022. 1,
Segdiff:Imagesegmentationwithdiffusionprobabilisticmod-
2,5,6,7,8,9
els. arXivpreprintarXiv:2112.00390,2021. 2
[17] MangTikChiu,XingqianXu,YunchaoWei,ZilongHuang,
[3] EmmanuelBrempongAsiedu,SimonKornblith,TingChen,
Alexander G Schwing, Robert Brunner, Hrant Khacha-
NikiParmar,MatthiasMinderer,andMohammadNorouzi.
trian,HovnatanKarapetyan,IvanDozier,GregRose,etal.
Decoder denoising pretraining for semantic segmentation.
Agriculture-vision:Alargeaerialimagedatabaseforagricul-
arXivpreprintarXiv:2205.11423,2022. 2
turalpatternanalysis. InConferenceonComputerVisionand
[4] AmirBar,YossiGandelsman,TrevorDarrell,AmirGlober- PatternRecognition(CVPR),2020. 1
son,andAlexeiEfros. Visualpromptingviaimageinpaint-
[18] Jang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath
ing. InAdvancesinNeuralInformationProcessingSystems
Hariharan. Picie:Unsupervisedsemanticsegmentationusing
(NeurIPS),2022. 2 invarianceandequivarianceinclustering. InCVPR,2021. 1
[5] DmitryBaranchuk,IvanRubachev,AndreyVoynov,Valentin [19] MariusCordts,MohamedOmran,SebastianRamos,Timo
Khrulkov,andArtemBabenko. Label-efficientsemanticseg- Rehfeld,MarkusEnzweiler,RodrigoBenenson,UweFranke,
mentationwithdiffusionmodels.InInternationalConference StefanRoth,andBerntSchiele. Thecityscapesdatasetfor
onLearningRepresentations(ICLR),2022. 2 semanticurbansceneunderstanding. InConferenceonCom-
[6] NavaneethBodla,BharatSingh,RamaChellappa,andLarryS puterVisionandPatternRecognition(CVPR),2016. 1,6
Davis. Soft-nms–improvingobjectdetectionwithoneline [20] Prafulla Dhariwal and Alexander Nichol. Diffusion mod-
of code. In Conference on Computer Vision and Pattern els beat gans on image synthesis. In Advances in Neural
Recognition(CVPR),2017. 2 InformationProcessingSystems(NeurIPS),2021. 1,2
[7] ZhaoweiCaiandNunoVasconcelos. Cascader-cnn:Delving [21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
intohighqualityobjectdetection.InConferenceonComputer Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
VisionandPatternRecognition(CVPR),2018. 1 MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
[8] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas vainGelly,etal. Animageisworth16x16words:Transform-
Usunier,AlexanderKirillov,andSergeyZagoruyko. End-to- ersforimagerecognitionatscale.InInternationalConference
endobjectdetectionwithtransformers. InEuropeanConfer- onLearningRepresentations(ICLR),2021. 1,8,9
enceonComputerVision(ECCV),2020. 1,2,5,6,8 [22] Stefan Elfwing, Eiji Uchibe, and Kenji Doya. Sigmoid-
[9] Liang-ChiehChen,GeorgePapandreou,FlorianSchroff,and weighted linear units for neural network function approx-
HartwigAdam. Rethinkingatrousconvolutionforsemantic imation in reinforcement learning. Neural networks, 107:
imagesegmentation. arXivpreprintarXiv:1706.05587,2017. 3–11,2018. 6
7 [23] PatrickEsser,RobinRombach,andBjornOmmer. Taming
[10] Ting Chen, Saurabh Saxena, Lala Li, David J Fleet, and transformersforhigh-resolutionimagesynthesis. InConfer-
GeoffreyHinton. Pix2seq:Alanguagemodelingframework enceonComputerVisionandPatternRecognition(CVPR),
forobjectdetection.InInternationalConferenceonLearning 2021. 4
Representations(ICLR),2022. 1 [24] Hugging Face. Compvis/stable-diffusion-v1-4, 2023. Re-
[11] TingChen,SaurabhSaxena,LalaLi,Tsung-YiLin,DavidJ trievedSeptember15,2023. 9
Fleet,andGeoffreyEHinton.Aunifiedsequenceinterfacefor [25] GolnazGhiasi,YinCui,AravindSrinivas,RuiQian,Tsung-
visiontasks. InAdvancesinNeuralInformationProcessing YiLin,EkinDCubuk,QuocVLe,andBarretZoph. Simple
Systems,2022. 2 copy-pasteisastrongdataaugmentationmethodforinstance
[12] TingChen,LalaLi,SaurabhSaxena,GeoffreyHinton,and segmentation.InConferenceonComputerVisionandPattern
DavidJFleet. Ageneralistframeworkforpanopticsegmen- Recognition(CVPR),2021. 2
tationofimagesandvideos. InInternationalConferenceon [26] KristenGrauman,AndrewWestbury,EugeneByrne,Zachary
ComputerVision(ICCV),2023. 1,2,4,5,8 Chavis,AntoninoFurnari,RohitGirdhar,JacksonHamburger,
[13] TingChen,RuixiangZhang,andGeoffreyHinton. Analog HaoJiang,MiaoLiu,XingyuLiu,etal. Ego4d:Aroundthe
bits: Generatingdiscretedatausingdiffusionmodelswith world in 3,000 hours of egocentric video. arXiv preprint
self-conditioning. InInternationalConferenceonLearning arXiv:2110.07058,2021. 1
Representations(ICLR),2023. 2,6,8 [27] Zhangxuan Gu, Haoxing Chen, Zhuoer Xu, Jun Lan,
[14] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, ChanghuaMeng,andWeiqiangWang. Diffusioninst: Dif-
ThomasSHuang, HartwigAdam, andLiang-ChiehChen. fusion model for instance segmentation. arXiv preprint
Panoptic-deeplab: A simple, strong, and fast baseline for arXiv:2212.02773,2022. 2
bottom-uppanopticsegmentation. InConferenceonCom- [28] KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun.
puterVisionandPatternRecognition(CVPR),2020. 2 Deepresiduallearningforimagerecognition. InConference
13onComputerVisionandPatternRecognition(CVPR),2016. [45] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
1,8 PietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence
[29] KaimingHe,GeorgiaGkioxari,PiotrDolla´r,andRossGir- Zitnick. Microsoft coco: Common objects in context. In
shick. Maskr-cnn. InInternationalConferenceonComputer EuropeanConferenceonComputerVision(ECCV),2014. 1,
Vision(ICCV),2017. 1,4,6,8 6,9
[30] KaimingHe,XinleiChen,SainingXie,YanghaoLi,Piotr [46] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng
Dolla´r,andRossGirshick. Maskedautoencodersarescal- Zhang, StephenLin, andBainingGuo. Swintransformer:
ablevisionlearners. InConferenceonComputerVisionand Hierarchicalvisiontransformerusingshiftedwindows. In
PatternRecognition(CVPR),2022. 8,9 InternationalConferenceonComputerVision(ICCV),2021.
[31] Jonathan Ho and Tim Salimans. Classifier-free diffusion 8
guidance. arXivpreprintarXiv:2207.12598,2022. 1,2 [47] JonathanLong,EvanShelhamer,andTrevorDarrell. Fully
[32] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu- convolutionalnetworksforsemanticsegmentation. InCon-
sionprobabilisticmodels. InAdvancesinNeuralInformation ferenceonComputerVisionandPatternRecognition(CVPR),
ProcessingSystems(NeurIPS),2020. 1,2,5 2015. 1
[33] JanHosang,RodrigoBenenson,andBerntSchiele. Learning [48] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
non-maximum suppression. In Conference on Computer regularization. arXivpreprintarXiv:1711.05101,2017. 6
VisionandPatternRecognition(CVPR),2017. 2 [49] JiasenLu,ChristopherClark,RowanZellers,RoozbehMot-
[34] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc taghi,andAniruddhaKembhavi. Unified-io:Aunifiedmodel
VanGool. Dynamicfilternetworks. InAdvancesinNeu- forvision,language,andmulti-modaltasks. InInternational
ralInformationProcessingSystems(NeurIPS),2016. 1 ConferenceonLearningRepresentations(ICLR),2023. 1,2
[35] JustinJohnson,AlexandreAlahi,andLiFei-Fei. Perceptual [50] Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree
lossesforreal-timestyletransferandsuper-resolution. In Kalpathy-Cramer,KeyvanFarahani,JustinKirby,YuliyaBur-
EuropeanConferenceonComputerVision(ECCV),2016. 4 ren, NicolePorz, JohannesSlotboom, RolandWiest, etal.
[36] DiederikPKingmaandMaxWelling. Auto-encodingvaria- Themultimodalbraintumorimagesegmentationbenchmark
tionalbayes. InInternationalConferenceonLearningRepre- (brats). IEEETransactionsonPatternAnalysisandMachine
sentations(ICLR),2014. 2,4 Intelligence(T-PAMI),2014. 1
[37] AlexanderKirillov, RossGirshick, KaimingHe, andPiotr [51] Shervin Minaee, Yuri Y Boykov, Fatih Porikli, Antonio J
Dolla´r. Panopticfeaturepyramidnetworks. InProceedings Plaza,NasserKehtarnavaz,andDemetriTerzopoulos. Image
oftheIEEE/CVFconferenceoncomputervisionandpattern segmentationusingdeeplearning:Asurvey. IEEETransac-
recognition,2019. 2,7,8 tionsonPatternAnalysisandMachineIntelligence(T-PAMI),
[38] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten 2021. 1
Rother, andPiotrDolla´r. Panopticsegmentation. InCon- [52] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
ferenceonComputerVisionandPatternRecognition(CVPR), Shyam,PamelaMishkin,BobMcGrew,IlyaSutskever,and
2019. 2,6 MarkChen. Glide:Towardsphotorealisticimagegeneration
[39] AlexanderKirillov,YuxinWu,KaimingHe,andRossGir- andeditingwithtext-guideddiffusionmodels. InInterna-
shick. Pointrend:Imagesegmentationasrendering. InCon- tionalConferenceonMachineLearning(ICML),2022. 1,
ferenceonComputerVisionandPatternRecognition(CVPR), 2
2020. 4 [53] MaximeOquab,Timothe´eDarcet,The´oMoutakanni,HuyVo,
[40] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao, MarcSzafraniec,VasilKhalidov,PierreFernandez,Daniel
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite- Haziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2:
head,AlexanderCBerg,Wan-YenLo,etal. Segmentany- Learningrobustvisualfeatureswithoutsupervision. arXiv
thing. arXivpreprintarXiv:2304.02643,2023. 2,8,9 preprintarXiv:2304.07193,2023. 8,9
[41] Alexander Kolesnikov, Andre´ Susano Pinto, Lucas Beyer, [54] Adam Paszke, Sam Gross, Soumith Chintala, Gregory
XiaohuaZhai,JeremiahHarmsen,andNeilHoulsby. Uvim: Chanan, Edward Yang, Zachary DeVito, Zeming Lin, Al-
Aunifiedmodelingapproachforvisionwithlearnedguiding banDesmaison,LucaAntiga,andAdamLerer. Automatic
codes.InAdvancesinNeuralInformationProcessingSystems differentiationinpytorch. 2017. 9
(NeurIPS),2022. 1,2,8 [55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[42] HaroldWKuhn. Thehungarianmethodfortheassignment Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
problem. Navalresearchlogisticsquarterly,2(1-2):83–97, AmandaAskell,PamelaMishkin,JackClark,etal. Learning
1955. 2,4,9 transferablevisualmodelsfromnaturallanguagesupervision.
[43] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip: InInternationalConferenceonMachineLearning(ICML),
Bootstrappinglanguage-imagepre-trainingforunifiedvision- 2021. 5,10
language understanding and generation. In International [56] AdityaRamesh, MikhailPavlov, GabrielGoh, ScottGray,
ConferenceonMachineLearning(ICML),2022. 5 ChelseaVoss,AlecRadford,MarkChen,andIlyaSutskever.
[44] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Zero-shottext-to-imagegeneration. InInternationalConfer-
Exploringplainvisiontransformerbackbonesforobjectde- enceonMachineLearning(ICML),2021. 1
tection.InEuropeanConferenceonComputerVision(ECCV), [57] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
2022. 9 andMarkChen. Hierarchicaltext-conditionalimagegenera-
14tionwithcliplatents. arXivpreprintarXiv:2204.06125,2022. [72] Wouter Van Gansbeke, Simon Vandenhende, and Luc
1,2,3 Van Gool. Discovering object masks with transformers
[58] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. for unsupervised semantic segmentation. arXiv preprint
Fasterr-cnn:Towardsreal-timeobjectdetectionwithregion arXiv:2206.06363,2022. 1
proposalnetworks. InAdvancesinNeuralInformationPro- [73] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
cessingSystems(NeurIPS),2015. 1,2 reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
[59] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Polosukhin. Attentionisallyouneed. InAdvancesinNeural
Patrick Esser, and Bjo¨rn Ommer. High-resolution image InformationProcessingSystems(NeurIPS),2017. 1,4
synthesis with latent diffusion models. In Conference on [74] HuiyuWang,YukunZhu,HartwigAdam,AlanYuille,and
ComputerVisionandPatternRecognition(CVPR),2022. 1, Liang-ChiehChen. Max-deeplab:End-to-endpanopticseg-
2,3,4,5,6,9 mentationwithmasktransformers. InConferenceonCom-
[60] OlafRonneberger,PhilippFischer,andThomasBrox. U-net: puterVisionandPatternRecognition(CVPR),2021. 2
Convolutionalnetworksforbiomedicalimagesegmentation. [75] XinlongWang,RufengZhang,TaoKong,LeiLi,andChun-
InMedicalImageComputingandComputer-AssistedInter- hua Shen. Solov2: Dynamic and fast instance segmenta-
vention,2015. 4,8 tion. InAdvancesinNeuralInformationProcessingSystems
(NeurIPS),2020. 1
[61] ShuaiShao,ZemingLi,TianyuanZhang,ChaoPeng,Gang
[76] Xinlong Wang, Zhiding Yu, Shalini De Mello, Jan Kautz,
Yu,XiangyuZhang,JingLi,andJianSun. Objects365: A
large-scale,high-qualitydatasetforobjectdetection. InIn- Anima Anandkumar, Chunhua Shen, and Jose M Alvarez.
ternationalConferenceonComputerVision(ICCV),2019. Freesolo: Learningtosegmentobjectswithoutannotations.
InConferenceonComputerVisionandPatternRecognition
8
(CVPR),2022. 1
[62] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,
[77] XudongWang,RohitGirdhar,StellaXYu,andIshanMisra.
and Surya Ganguli. Deep unsupervised learning using
Cutandlearnforunsupervisedobjectdetectionandinstance
nonequilibriumthermodynamics.InInternationalConference
segmentation.InConferenceonComputerVisionandPattern
onMachineLearning(ICML),2015. 2
Recognition(CVPR),2023. 1
[63] JiamingSong,ChenlinMeng,andStefanoErmon. Denoising
[78] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and
diffusionimplicitmodels. InInternationalConferenceon
TiejunHuang. Imagesspeakinimages:Ageneralistpainter
LearningRepresentations(ICLR),2021. 1,2,5
forin-contextvisuallearning. InConferenceonComputer
[64] YangSongandStefanoErmon.Improvedtechniquesfortrain-
VisionandPatternRecognition(CVPR),2023. 2,7,8
ingscore-basedgenerativemodels. InAdvancesinNeural
[79] YuwenXiong,RenjieLiao,HengshuangZhao,RuiHu,Min
InformationProcessingSystems(NeurIPS),2020. 6
Bai,ErsinYumer,andRaquelUrtasun. Upsnet: Aunified
[65] YangSong,PrafullaDhariwal,MarkChen,andIlyaSutskever.
panopticsegmentationnetwork. InConferenceonComputer
Consistencymodels.InInternationalConferenceonMachine
VisionandPatternRecognition(CVPR),2019. 2,7
Learning(ICML),2023. 10
[80] JiaruiXu,SifeiLiu,ArashVahdat,WonminByeon,Xiaolong
[66] Carole H Sudre, Wenqi Li, Tom Vercauteren, Sebastien
Wang,andShaliniDeMello. Open-vocabularypanopticseg-
Ourselin,andMJorgeCardoso. Generaliseddiceoverlapas
mentationwithtext-to-imagediffusionmodels.InConference
adeeplearninglossfunctionforhighlyunbalancedsegmen-
onComputerVisionandPatternRecognition(CVPR),2023.
tations. InDeepLearninginMedicalImageAnalysisand
2
MultimodalLearningforClinicalDecisionSupport,2017. 4
[81] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,
[67] ZhiTian,ChunhuaShen,andHaoChen. Conditionalconvo-
YukunZhu,HartwigAdam,AlanYuille,andLiang-Chieh
lutionsforinstancesegmentation. InEuropeanConference Chen. k-meansmasktransformer. InEuropeanConference
onComputerVision(ECCV),2020. 1 onComputerVision(ECCV),2022. 2
[68] Aa¨ron Van Den Oord, Nal Kalchbrenner, and Koray [82] LvminZhang,AnyiRao,andManeeshAgrawala. Adding
Kavukcuoglu. Pixel recurrent neural networks. In Inter- conditional control to text-to-image diffusion models. In
national Conference on Machine Learning (ICML), 2016. InternationalConferenceonComputerVision(ICCV),2023.
2 4
[69] AaronvandenOord,OriolVinyals,andkoraykavukcuoglu. [83] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,
Neuraldiscreterepresentationlearning. InAdvancesinNeu- andOliverWang. Theunreasonableeffectivenessofdeep
ralInformationProcessingSystems(NeurIPS),2017. 2 featuresasaperceptualmetric. InConferenceonComputer
[70] WouterVanGansbeke,SimonVandenhende,StamatiosGeor- VisionandPatternRecognition(CVPR),2018. 4
goulis,andLucVanGool. Revisitingcontrastivemethodsfor [84] WenweiZhang,JiangmiaoPang,KaiChen,andChenChange
unsupervisedlearningofvisualrepresentations. InAdvances Loy. K-net: Towardsunifiedimagesegmentation. In Ad-
inNeuralInformationProcessingSystems(NeurIPS),2021. vancesinNeuralInformationProcessingSystems(NeurIPS),
1 2021. 2
[71] WouterVanGansbeke,SimonVandenhende,StamatiosGeor- [85] BoleiZhou,HangZhao,XavierPuig,TeteXiao,SanjaFi-
goulis,andLucVanGool. Unsupervisedsemanticsegmenta- dler,AdelaBarriuso,andAntonioTorralba. Semanticunder-
tionbycontrastingobjectmaskproposals. InInternational standingofscenesthroughtheade20kdataset. International
ConferenceonComputerVision(ICCV),2021. JournalofComputerVision(IJCV),2019. 1
15