Harnessing Density Ratios for Online Reinforcement Learning
Philip Amortila∗ Dylan J. Foster Nan Jiang
philipa4@illinois.edu dylanfoster@microsoft.com nanjiang@illinois.edu
Ayush Sekhari Tengyang Xie
sekhari@mit.edu tengyangxie@microsoft.com
Abstract
The theories of offline and online reinforcement learning, despite having evolved in parallel, have
begun to show signs of the possibility for a unification, with algorithms and analysis techniques for one
setting often having natural counterparts in the other. However, the notion of density ratio modeling, an
emerging paradigm in offline RL, has been largely absent from online RL, perhaps for good reason: the
very existence and boundedness of density ratios relies on access to an exploratory dataset with good
coverage, but the core challenge in online RL is to collect such a dataset without having one to start.
In this work we show—perhaps surprisingly—that density ratio-based algorithms have online coun-
terparts. Assuming only the existence of an exploratory distribution with good coverage, a structural
condition known as coverability (Xie et al., 2023), we give a new algorithm (Glow) that uses density
ratio realizability and value function realizability to perform sample-efficient online exploration. Glow
addressesunboundeddensityratiosviacarefuluseoftruncation,andcombinesthiswithoptimismtoguide
exploration. Glow is computationally inefficient; we complement it with a more efficient counterpart,
HyGlow, for the Hybrid RL setting (Song et al., 2023) wherein online RL is augmented with additional
offline data. HyGlow is derived as a special case of a more general meta-algorithm that provides a
provable black-box reduction from hybrid RL to offline RL, which may be of independent interest.
1 Introduction
A fundamental problem in reinforcement learning (RL) is to understand what modeling assumptions and
algorithmic principles lead to sample-efficient learning guarantees. Investigation into algorithms for sample-
efficient reinforcement learning has primarily focused on two separate formulations: Offline reinforcement
learning,wherealearnermustoptimizeapolicyfromloggedtransitionsandrewards,andonline reinforcement
learning, where the learner can gather new data by interacting with the environment; both formulations
share the common goal of learning a near-optimal policy. For the most part, the bodies of research on
offline and online reinforcement have evolved in parallel, but they exhibit a number of curious similarities.
Algorithmically, many design principles for offline RL (e.g., pessimism) have online counterparts (e.g.,
optimism), and statistically efficient algorithms for both frameworks typically require similar representation
conditions (e.g., ability to model state-action value functions). Yet, the frameworks have notable differences:
online RL algorithms require exploration conditions to address the issue of distribution shift (Russo and
VanRoy,2013;Jiangetal.,2017;Sunetal.,2019;Wangetal.,2020c;Duetal.,2021;Jinetal.,2021a;Foster
et al., 2021), while offline RL algorithms require conceptually distinct coverage conditions to ensure the data
logging distribution sufficiently covers the state space (Munos, 2003; Antos et al., 2008; Chen and Jiang, 2019;
Xie and Jiang, 2020, 2021; Jin et al., 2021b; Rashidinejad et al., 2021; Foster et al., 2022; Zhan et al., 2022).
Recently, Xie et al. (2023) exposed a deeper connection between online and offline RL by showing that
coverability—that is, existence of a data distribution with good coverage for offline RL—is itself a sufficient
conditionthatenablessample-efficientexplorationinonlineRL,evenwhenthelearnerhasnopriorknowledge
of said distribution. This suggests the possibility of a theoretical unification of online and offline RL, but the
∗Authorslistedinalphabeticalorder.
1
4202
naJ
81
]GL.sc[
1v18690.1042:viXrapicture remains incomplete, and there are many gaps in our understanding. Notably, a promising emerging
paradigm in offline RL makes use of the ability to model density ratios (also referred to as marginalized
importance weights or simply weight functions) for the underlying MDP. Density ratio modeling offers an
alternative to classical value function approximation (or, approximate dynamic programming) methods
(Munos, 2007; Munos and Szepesvári, 2008; Chen and Jiang, 2019), as it avoids instability and typically
succeedsunderweakerrepresentationconditions(requiringonlyrealizabilityconditionsasopposedtoBellman
completeness-type assumptions). Yet despite extensive investigation into density ratio methods for offline
RL—both in theory (Liu et al., 2018; Uehara et al., 2020; Yang et al., 2020; Uehara et al., 2021; Jiang
and Huang, 2020; Xie and Jiang, 2020; Zhan et al., 2022; Chen and Jiang, 2022; Rashidinejad et al., 2023;
Ozdaglar et al., 2023) and practice (Nachum et al., 2019; Kostrikov et al., 2019; Nachum and Dai, 2020;
Zhang et al., 2020; Lee et al., 2021)—density ratio modeling has been conspicuously absent in the online
reinforcement learning. This leads us to ask:
Can online reinforcement learning benefit from the ability to model density ratios?
Adapting density ratio-based methods to the online setting with provable guarantees presents a number of
conceptual and technical challenges. First, since the data distribution in online RL is constantly changing,
it is unclear what densities one should even attempt to model. Second, most offline reinforcement learning
algorithms require relatively stringent notions of coverage for the data distribution. In online RL, it is
unreasonable to expect data gathered early in the learning process to have good coverage, and naive
algorithms may cycle or fail to explore as a result. As such, it may not be reasonable to expect density ratio
modeling to benefit online RL in the same fashion as offline.
Our contributions. We show that in spite of these challenges, density ratio modeling enables guarantees
for online reinforcement learning that were previously out of reach.
• Density ratios for online RL.Weshow(Section3)thatforanyMDPwithlowcoverability, densityratio
realizability and realizability of the optimal state-action value function are sufficient for sample-efficient
onlineRL.Thisresultisobtainedthroughanewalgorithm,Glow,whichaddressestheissueofdistribution
shift via careful use of truncated density ratios, which it combines with optimism to drive exploration.
This complements Xie et al. (2023), who gave sample complexity guarantees for coverable MDPs under
a stronger Bellman completeness assumption for the value function class.
• Density ratios for hybrid RL. Our algorithm for online RL is computationally inefficient. We com-
plement it (Section 4) with a more efficient counterpart, HyGlow, for the hybrid RL framework (Song
et al., 2023), in which the learner has access to additional offline data that covers a high-quality policy.
• Hybrid-to-offline reductions. To achieve the result above, we investigate a broader question: when
can offline RL algorithms be adapted as-is to online settings? We provide a new meta-algorithm, H O,
2
which reduces hybrid RL to offline RL by repeatedly calling a given offline RL algorithm as a black box.
We show that H O enjoys low regret whenever the black-box offline algorithm satisfies certain conditions,
2
and demonstrate that these conditions are satisfied by a range of existing offline algorithms, thus lifting
them to the hybrid RL setting.
While our results are theoretical in nature, we are optimistic that they will lead to further investigation into
the power of density ratio modeling in online RL and inspire practical algorithms.
Paper organization. Section 2 contains necessary background, introducing density ratio modeling and the
notion of coverability. Section 3 presents our main results for the online reinforcement learning framework,
and Section 4 presents our main results for the hybrid framework. We conclude with discussion in Section 5.
Proofs, examples, and additional discussion are deferred to the appendix.
1.1 Preliminaries
Markov Decision Processes. We consider an episodic reinforcement learning setting. A Markov Decision
Process(MDP)isatupleM=(X,A,P,R,H,d ),whereX isthe(large/potentiallyinfinite)statespace,Ais
1
the action space, H ∈N is the horizon, R={R }H is the reward function (where R :X ×A→∆([0,1])),
h h=1 h
2P ={P } isthetransitiondistribution(whereP :X×A→∆(X)),andd istheinitialstatedistribution.
h h≤1 h 1
Arandomizedpolicyisasequenceoffunctionsπ ={π :X →∆(A)}H . Whenapolicyisexecuted,itgener-
h h=1
atesatrajectory(x ,a ,r ),...,(x ,a ,r )viatheprocessa ∼π (x ),r ∼R (x ,a ),x ∼P (x ,a ),
1 1 1 H H h h h h h h h h h+1 h h h
initialized at x ∼ d (we use x to denote a deterministic terminal state with zero reward). We write
1 1 H+1
Pπ[·] and Eπ[·] to denote the law and corresponding expectation for the trajectory under this process.
For a policy π, the expected reward for is given by J(π):=Eπ(cid:2)(cid:80)H r (cid:3), and the value functions given by
h=1 h
(cid:34) H (cid:35) (cid:34) H (cid:35)
(cid:88) (cid:88)
Vπ(x):=Eπ r |x =x , and Qπ(x,a):=Eπ r |x =x,a =a .
h h′ h h h′ h h
h′=h h′=h
We write π⋆ ={π⋆}H to denote an optimal deterministic policy, which maximizes Vπ at all states. We let
h h=1
T denote the Bellman (optimality) operator for layer h, defined via
h
(cid:104) (cid:105)
[T f](x,a)=E r +maxf(x ,a′)|x =x,a =a
h h h+1 h h
a′
for f :X ×A→R.
Online RL. In the online reinforcement learning framework, the learner repeatedly interacts with an
unknown MDP by executing a policy and observing the resulting trajectory. The goal is to maximize total
reward. Formally, theprotocolproceedsinN rounds, whereateachroundt∈[N], thelearnerselectsapolicy
π(t) = {π(t)}H in the (unknown) underlying MDP M⋆ and observes the trajectory {(x(t),a(t),r(t))}H .
h h=1 h h h h=1
Our results are most naturally stated in terms of PAC guarantees. Here, after the N rounds of interaction
conclude, the learner can use all of the data collected to produce a final policy π, with the goal of minimizing
(cid:98)
Risk:=E [J(π⋆)−J(π)], (1)
π(cid:98)∼p (cid:98)
where p∈∆(Π) denotes a distribution that the algorithm can use to randomize the final policy.
Offline RL. In offline reinforcement learning, the learner does not directly interact with M⋆, and is
instead given a dataset of tuples (x ,a ,r ,x ) collected i.i.d. according to (x ,a )∼µ , r ∼R (x ,a ),
h h h h+1 h h h h h h h
x ∼ P (x ,a ), where µ is the offline data distribution for layer h. Based on the dataset, the offline
h+1 h h h h
algorithmproducesapolicyπ whoseperformanceismeasuredbyitsrisk, asinEquation(1); wewriteRisk
(cid:98) off
when we are in the offline interaction protocol.
Additional definitions and assumptions. We assume that rewards are normalized such that (cid:80)H r ∈
h=1 h
[0,1] almost surely for all trajectories (Jiang and Agarwal, 2018; Wang et al., 2020a; Zhang et al., 2021;
Jin et al., 2021a). To simplify presentation, we assume that X and A are countable; we expect that our
results extend to handle continuous variables with an appropriate measure-theoretic treatment. We define
the occupancy measure for policy π via dπ(x,a):=Pπ[x =x,a =a].
h h h
2 Problem Setup: Density Ratio Modeling and Coverability
To investigate the power of density ratio modeling in online RL, we make use of function approximation, and
aim to provide sample complexity guarantees with no explicit dependence on the size of the state space. We
begin by appealing to value function approximation, a standard approach in online and offline reinforcement
learning, and assume access to a value-function class F ⊂(X ×A×[H]→[0,1]) that can realize the optimal
value function Q⋆.
Assumption 2.1 (Value function realizability). We have Q⋆ ∈F.
For f ∈F, we define the greedy policy π via π (x)=argmax f (x,a), with ties broken in an arbitrary
f f,h a h
consistent fashion. For the remainder of the paper, we define Π:={π |f ∈F} as the policy class induced
f
by F, unless otherwise specified.
3Density ratio modeling. While value function approximation is a natural modeling approach, prior works
in both online (Du et al., 2020; Weisz et al., 2021; Wang et al., 2021) and offline RL (Wang et al., 2020b;
Zanette, 2021; Foster et al., 2022) have shown that value function realizability alone is not sufficient for
statistically tractable learning in many settings. As such, value function approximation methods in online
(Zanette et al., 2020; Jin et al., 2021a; Xie et al., 2023) and offline RL (Antos et al., 2008; Chen and Jiang,
2019) typically require additional representation conditions that may not be satisfied in practice, such as the
stringent Bellman completeness assumption (i.e., T F ⊆F ).
h h+1 h
In offline RL, a promising emerging paradigm that goes beyond pure value function approximation is to
model density ratios (or, marginalized important weights), which typically take the form
dπ(x,a)
h (2)
µ (x,a)
h
for a policy π, where µ denotes the offline data distribution. A recent line of work (Xie and Jiang, 2020;
h
Jiang and Huang, 2020; Zhan et al., 2022) shows, that given access to a realizable value function class and
a weight function class W that can realize the ratio (2) (typically either for all policies π, or for the optimal
policy π⋆), one can learn a near-optimal policy offline in a sample-efficient fashion; such results sidestep the
need for stringent value function representation conditions like Bellman completeness. To explore whether
density ratio modeling has similar benefits in online RL, we make the following assumption.
Assumption 2.2 (Density ratio realizability). The learner has access to a weight function class W ⊂
(X ×A×[H]→R ) such that for any policy pair π,π′ ∈Π, and h∈[H], we have1
+
wπ;π′
(x,a):=
dπ h(x,a)
∈W.
h dπ′(x,a)
h
Assumption 2.2 does not assume that the density ratios under consideration are finite. That is, we do not
assume boundedness of the weights, and our results do not pay for their range; our algorithm will only access
certain clipped versions of the weight functions (in fact, it is sufficient to only realize certain “clipped” weight
functions; cf. Remark B.1).
Comparedtodensityratioapproachesintheofflinesetting, whichtypicallyrequireeitherrealizabilityofdπ h/µh
for all π ∈Π or realizability of dπ h⋆ /µh, where µ is the offline data distribution, we require realizability of dπ h/dπ h′
for all pairs of policies π,π′ ∈Π. This assumption is natural because it facilitates transfer between historical
data (which is algorithm-dependent) and future policies. Notably, it is weaker than assuming realizability
of dπ h/νh for all π ∈ Π and any fixed distribution ν (Remark B.2), and is also weaker than model-based
realizability. We refer the reader to Appendix B for a detailed comparison to alternative assumptions.
Coverability. In addition to realizability assumptions, online RL methods require exploration conditions
(Russo and Van Roy, 2013; Jiang et al., 2017; Sun et al., 2019; Wang et al., 2020c; Du et al., 2021; Jin
et al., 2021a; Foster et al., 2021) that allow deliberately designed algorithms to control distribution shift or
extrapolate to unseen states. Towards lifting density ratio modeling from offline to online RL, we make use of
coverability (Xie et al., 2023), an exploration condition inspired by the notion of coverage in the offline setting.
Definition 2.1 (Coverability coefficient (Xie et al., 2023)). The coverability coefficient C >0 for a policy
cov
class Π is given by
C cov := µ1,...,µHin ∈f ∆(X×A)π∈Πs ,u hp
∈[H](cid:13)
(cid:13) (cid:13)
(cid:13)µdπ
h
h(cid:13)
(cid:13) (cid:13)
(cid:13)
∞.
We refer to the distribution µ⋆ that attains the minimum for h as the coverability distribution.
h
Coverability is a structural property of the underlying MDP, and can be interpreted as the best value one can
achieve for the concentrability coefficient C conc(µ):=sup π∈Π,h∈[H]∥dπ h/µh∥
∞
(a standard coverage parameter
in offline RL (Munos, 2007; Munos and Szepesvári, 2008; Chen and Jiang, 2019)) by optimally designing
the offline data distribution µ. However, in our setting the agent has no prior knowledge of µ⋆ and no way to
1Weadopttheconventionthatx/0=+∞whenx>0and0/0=1.
4explicitly search for it. Examples that admit low coverability include tabular MDPs and Block MDPs (Xie
et al., 2023), linear/low-rank MDPs (Huang et al., 2023), and analytically sparse low-rank MDPs (Golowich
et al., 2023); see Appendix C for further examples.
Concretely, we aim for sample complexity guarantees scaling as poly(H,C ,log|F|,log|W|,ε−1), where ε
cov
is the desired bound on the risk in Eq. (1). Such a guarantee complements Xie et al. (2023), who achieved
similar sample complexity under the Bellman completeness assumption, and parallels the fashion in which
density ratio modeling allows one to remove completeness in offline RL. To simplify presentation as much as
possible, we assume finiteness of F and W, but our results extend to infinite classes via standard uniform
convergence arguments. Likewise, we do not require exact realizability, and an extension to misspecified
classes is given in Appendix E.
Additional notation. For n∈N, we write [n]={1,...,n}. For a countable set Z, we write ∆(Z) for the
set of probability distributions on Z. We adopt standard big-oh notation, and use O(cid:101)(·) and Ω(cid:101)(·) to suppress
factors polylogarithmic in H, T, ε−1, log|F|, log|W|, and other problem parameters. For each h∈[H], we
define F = {f | f ∈ F} and W = {w | w ∈ W}. For any function u : X ×A (cid:55)→ R and distribution
h h h h
ρ∈∆(X ×A), we define the norms ∥u∥ =E [|u(x,a)|] and ∥u∥ =(cid:112)E [u2(x,a)].
1,ρ (x,a)∼ρ 2,ρ (x,a)∼ρ
3 Online RL with Density Ratio Realizability
This section presents our main results for the online RL setting. We first introduce our main algorithm,
Glow (Algorithm 1), and explain the intuition behind its design (Section 3.1). We then show (Section 3.2)
that Glow obtains polynomial sample complexity guarantees (Theorems 3.1 and 3.2) under density ratio
realizability and coverability, and conclude with a proof sketch (Section 3.3).
3.1 Algorithm and Key Ideas
Our algorithm, Glow (Algorithm 1), is based on the principle of optimism in the face of uncertainty. For
each iteration t ≤ T ∈ N, the algorithm uses the density ratio class W to construct a confidence set (or,
version space) F(t) ⊆F with the property that Q⋆ ∈F(t). It then chooses a policy π(t) =π based on the
f(t)
value function f(t) ∈F(t) with the most optimistic estimate E[f (x ,π (x ))] for the initial value. Then, it
1 1 f,1 1
uses the policy π(t) to gather K ∈N trajectories, which are used to update the confidence set for subsequent
iterations.
Within the scheme above, the main novelty to our approach lies in the confidence set construction. Glow
appeals to global optimism (Jiang et al., 2017; Zanette et al., 2020; Du et al., 2021; Jin et al., 2021a; Xie
et al., 2023), and constructs the confidence set F(t) by searching for value functions f ∈F that satisfy certain
Bellman residual constraints for all layers h∈[H] simultaneously. For MDPs with low coverability, previous
such approaches (Jin et al., 2021a; Xie et al., 2023) make use of constraints based on squared Bellman error,
which requires Bellman completeness. The confidence set construction in Glow (Eq. (4)) departs from this
approach, and aims to find f ∈F such that the average Bellman error is small for all weight functions. At
the population level, this (informally) corresponds to requiring that for all h∈[H] and w ∈W,2
E d¯(t)(cid:2) w h(x h,a h)(f h(x h,a h)−[T hf h+1](x h,a h))(cid:3) −α(t)·E d¯(t)(cid:2) (w h(x h,a h))2(cid:3) ≤β(t). (3)
where d¯(t) := 1 (cid:80) dπ(t) is the historical data distribution and α(t) > 0 and β(t) > 0 are algorithm
h t−1 i<t h
parameters; this is motivated by the fact that the optimal value function satisfies
Eπ(cid:2) w (x ,a )(Q⋆(x ,a )−(cid:2) T Q⋆ (cid:3) (x ,a ))(cid:3) =0
h h h h h h h h+1 h h
wfo hr ia cl hlf au ln loc wti son ts ow tra an nd sfep rol bic oie us nπ ds. O onur than ea oly ffs -i ps ou lis ce yst Bh ea lt lmE aq n.( e3 r) rh orold fos rfo thr eth he isw te oi rg ih cat lfu dn isc tt rio ibn uw tih( ot) n:=
d¯
(td )π
h
t(t o) /d t¯ h( ht) e,
on-policy Bellman error for π(t).
2AverageBellmanerrorwithout weight functions isusedinalgorithmssuchatOlive(Jiangetal.,2017)andBiLin-UCB
andDuetal.(2021). Withoutweighting,thisapproachisinsufficienttoderiveguaranteesbasedoncoverability;seediscussion
inXieetal.(2023).
5Algorithm 1 Glow: Global Optimism via Weight Function Realizability
input: Value function class F, Weight function class W, Parameters T,K ∈N, γ ∈[0,1].
1: //For Theorem 3.1 , set T =Θ(cid:101)((H2Ccov/ε2)·log(|F||W|/δ)), K=1, and γ=(cid:112) Ccov/(Tlog(|F||W|/δ)).
2: //For Theorem 3.2 , set T =Θ(cid:101)(H2Ccov/ε2), K=Θ(cid:101)(Tlog(|F||W|/δ)), and γ=(cid:112) Ccov/T.
3: Set γ(t) =γ·t, α(t) =8/γ(t) and β(t) =(36γ(t)/K(t−1))·log(6|F||W|TH/δ).
4: Initialize D(1) =∅ for all h≤H.
h
5: for t=1,...,T do
Define confidence set based on (regularized) minimax average Bellman error:
6:
(cid:40) (cid:41)
F(t) = f ∈F |∀h: sup E (cid:98) D(t)(cid:104)(cid:0) [∆(cid:98)hf](x h,a h,r h,x′ h+1)(cid:1) ·wˇ h(x h,a h)−α(t)·(cid:0) wˇ h(x h,a h)(cid:1)2(cid:105) ≤β(t) , (4)
w∈Wh h
where wˇ :=clip γ(t)[w] and [∆(cid:98)hf](x h,a h,r h,x′ h+1):=f h(x h,a h)−r−max a′f h+1(x′ h+1,a′).
Compute optimistic value function and policy:
7:
f(t) :=a fr ∈g Fm (ta )xE (cid:98)
x1∼D
1(t)[f 1(x 1,π f(x 1))], and π(t) :=π f(t). (5)
8: //Online data collection.
9: Initialize D(t+1) ←D(t) for h∈[H].
h h
10: for k =1,...,K do
11: Collect a trajectory (x 1,a 1,r 1),...,(x H,a H,r H) by executing π(t).
12: Update D h(t+1) ←D h(t+1)∪{(x h,a h,r h,x h+1)} for each h∈[H].
13: output: policy π (cid:98) =Unif(π(1),...,π(T)). //For PAC guarantee only.
Remark 3.1. Among density ratio-based algorithms for offline reinforcement learning (Jiang and Huang,
2020; Xie and Jiang, 2020; Zhan et al., 2022; Chen and Jiang, 2022; Rashidinejad et al., 2023), the constraint
(3) is most directly inspired by the Minimax Average Bellman Optimization (Mabo) algorithm (Xie and
Jiang, 2020), which uses a similar minimax approximation to the average Bellman error.
Partial coverage and clipping. Compared to the offline setting, much extra work is required to handle
the issue of partial coverage. Early in the learning process, the ratio w h(t) := dπ h(t) /d¯( ht) may be unbounded,
which prevents the naive empirical approximation to Eq. (3) from concentrating. To address this issue, Glow
carefully truncates the weight functions under consideration.
Definition 3.1 (Clipping operator). For any w :X ×A→R∪{∞} and γ ∈R, we define the clipped weight
function (at scale γ) via
clip [w](x,a):=min{w(x,a),γ}.
γ
Within Glow, we replace the weight functions in Eq. (3) with clipped counterparts given by wˇ(x,a) :=
clip [w](x,a), where γ(t) :=γ·t for a parameter γ ∈[0,1].3 For a given iteration t, clipping in this fashion
γ(t)
may render Eq. (3) a poor approximation to the on-policy Bellman error. The crux of our analysis is to
show—via coverability—that on average across all iterations, the approximation error is small.
An important difference relative to Mabo is that the weighted Bellman error in Eq. (3) incorporates a
quadraticpenalty−α(t)·E d¯(t)[(w h(x h,a h))2]fortheweightfunction. Thisisnotessentialtoderivepolynomial
sample complexity guarantees, but is critical to attain the 1/ε2-type rates we achieve under our strongest
realizability assumption. Briefly, regularization is beneficial because it allows us to appeal to variance-
dependent Bernstein-style concentration; our analysis shows that while the variance of the weight functions
under consideration may not be small on a per-iteration basis, it is small on average across all iterations
(again, via coverability). Interestingly, similar quadratic penalties have been used within empirical offline RL
algorithms based on density ratio modeling (Yang et al., 2020; Lee et al., 2021), as well as recent theoretical
results (Zhan et al., 2022), but for considerations seemingly unrelated to concentration.
3Allofourresultscarryovertothealternateclippingoperatorclip [w](x,a)=w(x,a)I{w(x,a)≤γ}.
γ
63.2 Main Result: Sample Complexity Bound for Glow
We now present the main sample complexity guarantees for Glow. The first result we present, which gives
the tightest sample complexity bound, is stated under a form of density ratio realizability that strengthens
Assumption 2.2. Concretely, we assume that the class W can realize density ratios for certain mixtures of
policies. For t∈N, we write π(1:t) as a shorthand for a sequence of policies (π(1),··· ,π(t)), where π(i) ∈Π,
and let dπ(1:t) := 1(cid:80)t dπ(i).
t i=1
Assumption 2.2′ (Density ratio realizability, mixture version). Let T be the parameter to Glow (Algo-
rithm 1). For all h∈[H], π ∈Π, t≤T, and π(1:t) ∈Πt, we have
wπ;π(1:t)
(x,a):=
dπ h(x,a)
∈W.
h dπ(1:t)(x,a)
h
This assumption directly facilitates transfer from the algorithm’s historical distribution d¯(t) := 1 (cid:80) dπ(t)
h t−1 i<t h
to on-policy error. Naturally, it is implied by the stronger-but-simpler-to-state assumption that we can realize
density ratios dπ h/dρ
h
for all π ∈Π and all mixture √policies ρ∈∆(Π). Under Assumption 2.2′, we show that
Glow obtains 1/ε2-PAC sample complexity and T-regret.
Theorem3.1(RiskboundforGlowunderstrongdensityratiorealizability). Letε>0begiven, andsuppose
that Assumptions 2.1 and 2.2′ hold. Then, Glow, with hyperparameters T =Θ(cid:101)(cid:0) (H2Ccov/ε2)·log(|F||W|/δ)(cid:1) ,
(cid:112)
K = 1, and γ = Ccov/(Tlog(|F||W|/δ)) returns an ε-suboptimal policy π
(cid:98)
with probability at least 1−δ after
collecting
(cid:18) H2C (cid:19)
N =O(cid:101) cov log(|F||W|/δ) (6)
ε2
trajectories. Additionally, for any T ∈N, with the same choice for K and γ as above, Glow enjoys the regret
bound
T
Reg:=
(cid:88) J(π⋆)−J(π(t))=O(cid:101)(cid:0) H(cid:112)
C covT
log(|F||W|/δ)(cid:1)
.
t=1
Next, we provide our main result, which gives a sample complexity guarantee under density ratio realizability
for pure policies (Assumption 2.2). To obtain the result, we begin with a class W that satisfies Assumption
2.2, then expand it to obtain an augmented class W that satisfies mixture realizability (Assumption 2.2′).
This reduction increases log|W| by a T factor, which we offset by increasing the batch size K; this leads to a
polynomial increase in sample complexity.4
Theorem 3.2 (RiskboundforGlow underweakdensityratiorealizability). Let ε>0 be given, and suppose
that Assumptions 2.1 and 2.2 hold for the classes F and W. Then, Glow, when executed with a modified
class W defined in Eq. (34) in Appendix E, with hyperparameters T =Θ(cid:101)(H2Ccov/ε2), K =Θ(cid:101)(T log(|F||W|/δ)),
(cid:112)
and γ = Ccov/T, returns an ε-suboptimal policy π
(cid:98)
with probability at least 1−δ after collecting N trajectories,
for
(cid:18) H4C2 (cid:19)
N =O(cid:101) cov log(|F||W|/δ) . (7)
ε4
Theorems 3.1 and 3.2 show for the first time that value function realizability and density ratio realizability
alone are sufficient for sample-efficient online RL under coverability. In particular, the sample complexity and
regret bound in Theorem 3.1 match the coverability-based guarantees obtained in Xie et al. (2023, Theorem
1) under the complementary Bellman completeness assumption, with the only difference being that they
scale with log(|F||W|) instead of log|F|; as discussed in Xie et al. (2023), this rate is tight for the special
case of contextual bandits (H =1). Interesting open questions include (i) whether the sample complexity
√
4ThisreductionalsopreventsusfromobtainingaregretbounddirectlyunderAssumption2.2,thougha(slower-than- T)
regretboundcanbeattainedusinganexplore-then-commitstrategy.
7for learning with density ratio realizability for pure policies can be improved to 1/ε2, and (ii) whether value
realizability and coverability alone are sufficient for sample-efficient RL. Extensions to Theorems 3.1 and 3.2
under misspecification are given in Appendix E. We further refer to Appendix C for examples instantiating
these results. In particular, our results establish a positive result for a generalized class of Block MDPs
with coverable latent spaces, while only requiring (for the first time) function approximation conditions that
concern the latent space (Example C.2).
Like other algorithms based on global optimism (Jiang et al., 2017; Zanette et al., 2020; Du et al., 2021; Jin
et al., 2021a; Xie et al., 2023), Glow is not computationally efficient. As a step toward developing practical
online RL algorithms based on density ratio modeling, we give a more efficient counterpart for the hybrid RL
model in the Section 4.
Remark 3.2 (Connection to Golf). Prior work (Xie et al., 2023) analyzed the Golf algorithm of Jin et al.
and established positive results under coverability and Bellman completeness. We remark that by allowing
for weight functions that take negative values,5 Glow can be viewed as a generalization of Golf, and can
be configured to obtain comparable results. Indeed, given a value function class F that satisfies Bellman
completeness, the weight function class W :={f −f′ |f,f′ ∈F} leads to a confidence set construction at
least as tight as that of Golf. To see this, observe that if we set γ ≥ 2 so that no clipping occurs, our
construction for F(t) (Eq. (4)) implies (after standard concentration arguments) that Q⋆ ∈ F(t) and that
in-sample squared Bellman errors are small with high probability. These ingredients are all that is required to
repeat the analysis of Golf from Xie et al. (2023).
3.3 Proof Sketch
We now give a proof sketch for Theorem 3.1, highlighting the role of truncated weight functions in addressing
partial coverage. We focus on the regret bound; the sample complexity bound in Eq. (6) is an immediate
consequence.
By design, the constraint in Eq. (4) ensures that Q⋆ ∈F(t) for all t≤T with high probability. Thus, by a
standard regret decomposition for optimistic algorithms (Lemma D.4 in the appendix), we have
T T H
Reg=(cid:88) J(π⋆)−J(π(t))≲(cid:88)(cid:88) E (cid:2) f(t)(x ,a )−[Tf(t) ](x ,a )(cid:3) , (8)
d(t) h h h h+1 h h
h
t=1 t=1h=1(cid:124) (cid:123)(cid:122) (cid:125)
On-policyBellmanerrorforf(t) underπ(t)
uptolower-orderterms,whereweabbreviated(t) =dπ(t). Defining[∆ f(t)](x,a):=f(t)(x,a)−[T f(t) ](x,a),
h h h h+1
itremainstoboundtheon-policyexpectedbellmanerrorE [[∆ f(t)](x ,a )]. Todoso, anaturalapproach
d(t) h h h
h
is to relate this quantity to the weighted off-policy Bellman error under d¯ (t) := 1 (cid:80) dπ(i) by introducing
t−1 i<t
the weight function d(t)/d¯(t) ∈W:
(cid:20) d(t)(x ,a )(cid:21)
E [[∆ f(t)](x ,a )]≈E [∆ f(t)](x ,a )· h h h .
d(t) h h h d¯(t) h h h d¯(t)(x ,a )
h h h h h
Unfortunately, this equality is not true as-is because the ratio d(t)/d¯(t) can be unbounded. We address this by
replacing d¯ (t) by d¯ (t+1) throughout the analysis (at the cost of small approximation error), and work with the
weight function w h(t) :=d( ht)/d¯( ht+1) ∈W, which is always bounded in magnitude t. However, while boundedness
is a desirable property, the range t is still too large to obtain non-vacuous concentration guarantees. This
motivates us to introduce clipped/truncated weight functions via the following decomposition.
E [[∆ f(t)](x ,a )]≤E (cid:2) [∆ f(t)](x ,a )·clip (cid:2) w(t)(cid:3) (x ,a )(cid:3) +E (cid:2)I(cid:8) w(t)(x ,a )≥γ(t)(cid:9)(cid:3) .
d(t) h h h d¯(t+1) h h h γ(t) h h h d(t) h h h
h h h
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
On-policyBellmanerror (At): Clippedoff-policyBellmanerror (Bt): Lossduetoclipping
Recall that wˇ(t) :=clip (cid:2) w(t)(cid:3) (x ,a ). As w(t) ∈W, it follows from the constraint in Eq. (4) and Freedman-
h γ(t) h h h
type concentration that the clipped Bellman error in term (A t) has order α(t)·E d¯(t+1)(cid:2) (wˇ h(t))2(cid:3) +β(t), so that
5Inthiscontext,W canbethoughtofmoregenerallyasaclassoftest functions.
8(cid:80)T t=1A
t
≤ (cid:80)T t=1α(t)·E d¯(t+1)(cid:2) (wˇ h(t))2(cid:3) +β(t). Sincewecliptoγ(t) =γt,wehave(cid:80)T t=1β(t) ≲γ·T log(|F||W|/δ);
bounding the sum of weight functions requires a more involved argument that we defer for a moment.
We now focus on bounding the terms (B ). Each term (B ) captures the extent to which the weighted
t t
off-policy Bellman errorat iteration t fails toapproximatethe true Bellmanerror due toclipping. This occurs
when d¯ (t+1) has poor coverage relative to d(t), which happens when π(t) visits a portion of the state space not
previously covered. We begin by applying Markov’s inequality (I{u≥v}≤u/v for u,v ≥0) to bound
B ≤ 1 E (cid:2) w(t)(x ,a )(cid:3) = 1 E (cid:20) d( ht)(x h,a h) (cid:21) = 1 E (cid:34) d( ht)(x h,a h) (cid:35) , (9)
t γ(t) d( ht) h h h γ(t) d( ht) d¯( ht+1)(x h,a h) γ d h(t) d(cid:101)( ht+1)(x h,a h)
where the equality uses that γ(t) := γ ·t and d(cid:101)(t+1) := d¯ (t+1) ·t. Our most important insight is that even
though each term in Eq. (9) might be large on a given iteration t (if a previously unexplored portion of the
state space is visited), coverability implies that on average across all iterations the error incurred by clipping
must be small. In particular, using a variant of a coverability-based potential argument from Xie et al. (2023)
(Lemma D.5), we show that
(cid:88)T
E
(cid:34) d( ht)(x h,a h) (cid:35)
≤O(C ·log(T)),
t=1
d( ht)
d(cid:101)( ht+1)(x h,a h)
cov
so that (cid:80)T t=1B
t
≤O(cid:101)(C cov/γ). To conclude the proof, we use an analogous potential argument to show the
sum of weight functions in our bound on (cid:80)T t=1A
t
also satisfies (cid:80)T t=1α(t)·E d¯(t+1)(cid:2) (wˇ h(t))2(cid:3) ≤O(cid:101)(C cov/γ). The
intuition is similar: the squared weight functions (corresponding to variance of the weighted Bellman error)
may be large in a given round, but cannot be large for all rounds under coverability. Altogether, combining
the bounds on A and B gives
t t
(cid:18) (cid:18) (cid:19)(cid:19)
C
Reg=O(cid:101) H cov +γ·T log(|F||W|HTδ−1) . (10)
γ
The final result follows by choosing γ >0 to balance the terms.
We find it interesting that the way in which this proof makes use of coverability—to handle the cumulative
loss incurred by clipping—is quite different from the analysis in Xie et al. (2023), where it more directly
facilitates a change-of-measure argument.
4 Efficient Hybrid RL with Density Ratio Realizability
Ourresultsintheprequelshowthatdensityratiorealizabilityandcoverabilitysufficeforsample-efficientonline
RL. However, like other algorithms for sample-efficient exploration under general function approximation
(Jiang et al., 2017; Du et al., 2021; Jin et al., 2021a), Glow is not computationally efficient. Toward
overcoming the challenges of intractable computation in online exploration, a number of recent works show
that including additional offline data in online RL can lead to computational benefits in theory (e.g., Xie
et al., 2021b; Wagenmaker and Pacchiano, 2023; Song et al., 2023; ?) and in practice (e.g., Cabi et al., 2020;
Nair et al., 2020; Ball et al., 2023; Song et al., 2023; ?). Notably, combining offline and online data can enable
algorithms that provably explore without having to appeal to optimism or pessimism, both of which are
difficult to implement efficiently under general function approximation.
Song et al. (2023) formalize a version of this setting—in which online RL is augmented with offline data—as
hybrid reinforcement learning. Formally, in hybrid RL, the learner interacts with the MDP online (as in
Section 1.1) but is additionally given an offline dataset D collected from a data distribution ν. The data
off
distributionν istypicallyassumedtoprovidecoveragefortheoptimalpolicyπ⋆ (formalizedinDefinition4.4),
butnotonallpolicies,andthusadditionalonlineexplorationisrequired(seeRemark4.2forfurtherdiscussion).
94.1 H O: A Provable Black-Box Hybrid-to-Offline Reduction
2
Interestingly, many of the above approaches for the hybrid setting simply apply offline algorithms (with
relatively little modification) on a mixture of online and offline data (e.g., Cabi et al., 2020; Nair et al.,
2020; Ball et al., 2023). This raises the question: when can we use a given offline algorithm as a black box
to solve the problem of hybrid RL (or, more generally, of online RL?). To answer this, we give a general
meta-algorithm, H O, which provides a provable black-box reduction to solve the hybrid RL problem by
2
repeatedly invoking a given offline RL algorithm on a mixture of offline data and freshly gathered online
trajectories. We instantiate the meta-algorithm using a simplified offline counterpart to Glow as a black
box to obtain HyGlow, a density ratio-based algorithm for the hybrid RL setting that improves upon the
computational efficiency of Glow (Section 4.2). To present the result, we first describe the class of offline
RL algorithms with which it will be applied.
Offline RL and partial coverage. We refer to a collection of distributions µ = {µ }H , where µ ∈
h h=1 h
∆(X ×A), as a data distribution, and we say that a dataset D = {D }H has H ·n samples from data
h h=1
distributions µ(1),...,µ(n) if D = {(x(i),a(i),r(i),x(i) )}n where (x(i),a(i)) ∼ µ(i), r(i) ∼ R (x(i),a(i)),
h h h h h+1 i=1 h h h h h h h
x(i) ∼P (x(i),a(i)). We denote the mixture distribution via µ(1:n) ={µ(1:n)}H , where µ(1:n) := 1 (cid:80)n µ(i).
h+1 h h h h h=1 h n i=1 h
Definition 4.1 (OfflineRLalgorithm). AnofflineRLalgorithmAlg takesasinputadatasetD ={D }H
off h h=1
of H ·n samples from µ(1),...µ(n) and outputs a policy π ={π }H .6 We allow µ(1),...µ(n) to be adaptively
h h=1
chosen, i.e. each µ(i+1) may be a function of the samples generated from µ(1)...µ(i).7
An immediate problem with directly invoking offline RL algorithms in the hybrid model is that typical
algorithms—particularly, those that do not make use of pessimism (e.g., Xie and Jiang, 2020)—require
relatively uniform notions of coverage (e.g., coverage for all policies as opposed to just coverage for π⋆) to
provide guarantees, leading one to worry that their behaviour might be completely uncontrolled when applied
with non-exploratory datasets. Fortunately, we will show that for a large class algorithms whose risk scales
with a measure of coverage we refer to as clipped concentrability, this phenomenon cannot occur. Below, for
any distribution ρ∈∆(X ×A), we write ∥·∥ and ∥·∥ for the L (ρ) and L (ρ) norms.
1,ρ 2,ρ 1 2
Definition 4.2 (Clipped concentrability coefficient). The clipped concentrability coefficient (at scale γ ∈R )
+
for π ∈Π relative to a data distribution µ={µ }H , where µ ∈∆(X ×A), is defined as
h h=1 h
CC
h(π,µ,γ):=(cid:13)
(cid:13)
(cid:13) (cid:13)clip
γ(cid:20) µdπ h(cid:21)(cid:13)
(cid:13)
(cid:13)
(cid:13)
.
h 1,dπ
h
This coefficient should be thought of as a generalization of the standard (squared) L (µ) concentrability
2
c ao lge offi ric ti he mnt sC (ec2 o .gnc .,,2 F,h a( rπ a, hµ m) a:= nd∥ ed tπ h/ aµ lh .,∥2 2 2, 0µ 1h 0=
),
b∥ udπ h t/µ inh c∥ o1 r,d pπ ho, raa tefu sn cd lia pm pie nn gta tl oo bb ej te tc et ri hn anth de lea pn aa rl ty is ai ls co of veo rffl agin ee
.
WRL
e
consider offline RL algorithms with the property that for any offline distribution µ, the algorithm’s risk can
be bounded by the clipped concentrability coefficients for (i) the output policy π, and (ii) the optimal policy
(cid:98)
π⋆. For the following definition, we recall the notation γ(n) :=γ·n.
Definition 4.3 (CC-bounded offline RL algorithm). We say that an offline algorithm Alg is CC-bounded
off
at scale γ ∈R under an assumption Assumption(·) if there exists scalars a ,b such that for all n∈N and
+ γ γ
data distributions µ(1),...,µ(n), Alg outputs a distribution p∈∆(Π) satisfying
off
H
Risk =E [J(π⋆)−J(π)]≤(cid:88) a γ (CC (π⋆,µ(1:n),γ(n))+E [CC (π,µ(1:n),γ(n))])+b (11)
off π(cid:98)∼p (cid:98) n h π(cid:98)∼p h (cid:98) γ
h=1
withprobabilityatleast1−δ,whengivenadatasetofH·nsamplesfromµ(1),...,µ(n) suchthatAssumption(µ(1:n),M⋆)
is satisfied.
6Alg doesnothaveanyparameters;whenparametersareneeded,Alg shouldinsteadbethoughtofasthealgorithmfor
off off
afixedchoiceofparameter. Likewise,wetreatF andW (orotherinputfunctionclasses)aspartofthealgorithm.
7GuaranteesforofflineRLinthei.i.d. settingcanoftenbeextendedtotheadaptivesetting(Section4.2).
10This definition does not automatically imply that the offline algorithm has low offline risk, but simply that
the risk can be bounded in terms of clipped coverage (which may be large if the dataset has poor coverage).
In the sequel, we will show that many natural offline RL algorithms have this property (Appendix F.1).
Examples of assumptions for Assumption include value function completeness (e.g., for Fqi (Chen and Jiang,
2019)) and realizability of value functions and density ratios (e.g., for Mabo (Xie and Jiang, 2020)).
Offline RL algorithms based on pessimism (Jin et al., 2021b; Rashidinejad et al., 2021; Xie et al., 2021a)
typically enjoy risk bounds that only require coverage for π⋆. Crucially, by allowing the risk bound in
Definition 4.3 to scale with coverage for π in addition to π⋆, we can accommodate non-pessimistic offline
(cid:98)
RL algorithms such as Fqi and Mabo that are weaker statistically, yet more computationally efficient.
Remark 4.1. While the bound in Eq. (11) might seem to suggest a 1/n-type rate, it will typically lead to a
1/√ n-type rate after choosing γ >0 to balance the a nγ and b
γ
terms.
The H O algorithm. Our reduction, H O, is given in Algorithm 2. For any dataset D, we will write
2 2
D(cid:12) (cid:12) for the subset consisting of its first t elements. The algorithm is initialized with an offline dataset
1:t
D = {D }H , and at each iteration t ∈ [T] invokes the black-box offline RL algorithm Alg with a
off off,h h=1 off
dataset D ={D }H that mixes the first t elements of D with all of the online data gathered
hybrid hybrid,h h=1 off,h
so far. This produces a policy π(t), which is executed to gather trajectories that are then added to the online
dataset and used at the next iteration.
H O is inspired by empirical methods for the hybrid setting (e.g., Cabi et al., 2020; Nair et al., 2020; Ball
2
et al., 2023). The total computational cost is simply that of running the base algorithm Alg for T rounds,
off
and in particular the meta-algorithm is efficient whenever Alg is.
off
Algorithm 2 H O: Hybrid-to-Offline Reduction
2
input: Parameter T ∈N, offline algorithm Alg , offline datasets D ={D } each of size T.
off off off,h h
1: Initialize D(1) =D(1) =∅ for all h∈[H].
on,h hybrid,h
2: for t=1,...,T do
3: Get policy π(t) from Alg
off
on dataset D h(t y)
brid
={D h(t y) brid,h} h.
4: Collect trajectory (x 1,a 1,r 1),...,(x H,a H,r H) using π(t); D o(t n+ ,h1) :=D o(t n) ,h∪{(x h,a h,r h,x h+1)}.
5: Aggregate offline and online data: D h(t y+ b1 r)
id,h
:=D off,h(cid:12) (cid:12) 1:t∪D o(t n+ ,h1) for all h∈[H].
6: output: policy π (cid:98) =Unif(π(1),...,π(T)).
Main sample complexity bound for H O. We now present the main result for this section: a risk
2
bound for the H O reduction. Our bound depends on the coverability parameter for the underlying MDP, as
2
well as the quality of the offline data distribution ν, quantified by single-policy concentrability.
Definition 4.4 (Single-policy concentrability). A data distribution ν ={ν }H satisfies C -single-policy
h h=1 ⋆
concentrability if
m hax(cid:13) (cid:13) (cid:13) (cid:13)d νπ h h⋆(cid:13) (cid:13) (cid:13)
(cid:13)
∞
≤C ⋆.
Theorem 4.1 (Risk bound for H O). Let T ∈ N be given, let D consist of H ·T samples from data
2 off
distribution ν, and suppose that ν satisfies C -single-policy concentrability. Let Alg be CC-bounded at scale
⋆ off
γ ∈(0,1) under Assumption(·), with parameters a and b . Suppose that for all t∈[T] and π(1),...,π(t) ∈Π,
γ γ
Assumption(µ(t),M⋆) holds for µ(t) := {1/2(ν h+1/t(cid:80)t i=1dπ h(i))}H h=1. Then, with probability at least 1−δT,
the risk of H O (Algorithm 2) with inputs T, Alg , and D is bounded as
2 off off
(cid:18) (cid:18) (cid:19)(cid:19)
a (C +C )
Risk≤O(cid:101) H γ ⋆
T
cov +b
γ
. (12)
Forthealgorithmsweconsider,onecantakea
γ
∝a/γ andb
γ
∝bγ forscalar-valuedproblemparametersa,b>
0, so that choosing γ optimally gives Risk≤O(cid:101)(cid:0) H(cid:112) (C⋆+Ccov)ab/T(cid:1) sample complexity of O(cid:101)(cid:0) H2(C⋆+Ccov)ab/ε2(cid:1)
11to find an ε-optimal policy (see Corollary F.1).8
The basic idea behind the proof of Theorem 4.1 is as follows: using a standard regret decomposition based
on average Bellman error, we can bound the risk of H O by the average of the two clipped concentrability
2
terms in (11) across all iterations. Coverage for π⋆ is automatically handled by Definition 4.4, and we use
a potential argument similar to the online setting (cf. Section 3.3) to show that the π-coverage terms can
(cid:98)
be controlled by coverability. This is similar in spirit to the analysis of Song et al. (2023), with coverability
taking the place of bilinear rank (Du et al., 2021).
Our result is stated as a bound on the risk to the optimal policy π⋆, but extends to give a bound on the risk
of any comparator πc with Definition 4.3 and Definition 4.4 replaced by coverage for πc. This is a special case
of a more general result, Theorem F.5, which handles the general case where ν need not satisfy single-policy
concentrability.
4.2 Applying the Reduction: HyGlow
We now apply H O to give a hybrid counterpart to Glow (Algorithm 1), using a variant of Mabo (Xie
2
and Jiang, 2020) as the black box offline RL algorithm Alg in H O. Further examples, which apply
off 2
Fitted Q-Iteration (Fqi) and Model-Based Maximum Likelihood Estimation as the black box, are deferred to
Appendix F.1.
As discussed in Section 3, the construction for the confidence set of Glow (Eq. (4)) bears some resemblance
to the Mabo algorithm (Xie and Jiang, 2020) from offline RL, save for the important additions of clipping
and regularization. For our main example, the offline RL algorithm we consider is a variant of Mabo that
incorporates clipping and regularization in the same fashion, which we call Mabo.cr. Our algorithm takes
as input a dataset D ={D } with H ·n samples, has parameters consisting of a value function class F, a
h
weight function class W, and a clipping scale γ, and computes the following estimator:
f(cid:98)∈ar fg ∈m Fin wm ∈a Wx(cid:88)H (cid:12) (cid:12) (cid:12)E (cid:98)Dh(cid:104) wˇ h(x h,a h)[∆(cid:98)hf](x h,a h,r h,x′ h+1)(cid:105)(cid:12) (cid:12) (cid:12)−α(n)E (cid:98)Dh(cid:2) wˇ h2(x h,a h)(cid:3) , (13)
h=1
where α(n) := 8/γ(n) and wˇ
h
:= clip γ(n)[w h]. We will show that this algorithm is CC-bounded under Q⋆-
realizability and a density ratio realizability assumption.
Theorem 4.2 (Mabo.crisCC-bounded). Let D ={D }H consist of H·n samples from µ(1),...,µ(n). For
h h=1
any γ ∈R , the Mabo.cr algorithm (Eq. (13)) with parameters F, augmented class W defined in Eq. (38)
+
in Appendix F.1.1, and γ is CC-bounded at scale γ under the Assumption that Q⋆ ∈F and that for all π ∈Π
and h∈[H], dπ h/µ( h1:n) ∈W.
We remark that, following the same arguments in Remark B.1, it suffices to instead only realize the clipped
density ratios for the optimal scale γ. By instantiating H O with this algorithm, we obtain a density
2
ratio-based algorithm for the hybrid RL setting that is statistically efficient and improves the computational
efficiency of Glow by removing the need for optimism. We call the end-to-end hybrid algorithm HyGlow,
and the full pseudocode can be found in Algorithm 3.
Corollary 4.1 (Risk bound for HyGlow). Let ε > 0 be given, let D consist of H ·T samples from
off
data distribution ν, and suppose that ν satisfies C -single-policy concentrability. Suppose that Q⋆ ∈ F
⋆
and that for all t ∈ [T], π ∈ Π, and h ∈ [H], we have dπ h/µ h(t) ∈ W, where µ( ht) := 1/2(ν
h
+1/t(cid:80)t i=1dπ h(i)).
Then, HyGlow with inputs T = Θ(cid:101)((H4(Ccov+C⋆)/ε2)·log(|F||W|/δ)), F, augmented W defined in Eq. (38),
(cid:16)(cid:112) (cid:17)
γ =Θ(cid:101) (C⋆+Ccov)/TH2log(|F||W|/δ) , and D
off
returns an ε-suboptimal policy with probability at least 1−δT
after collecting
(cid:18) H2(C +C ) (cid:19)
N =O(cid:101) cov ⋆ log(|F||W|/δ)
ε2
trajectories.
8Note that H 2O does not depend on the clipping scale γ, and thus if the algorithm is CC-bounded for multiple scales
simultaneouslywecaninfacttaketheminimumoftheright-hand-sideoverallsuchγ.
12Algorithm 3 HyGlow: H O + Mabo.cr
2
input: Parameter T ∈N, value function class F, weight function class W, parameter γ ∈[0,1], offline
datasets D ={D } each of size T.
off off,h h
1: //For Corollary 4.1 , set T =Θ(cid:101)((H4(Ccov+C⋆)/ε2)·log(|F||W|/δ)) , and
γ=Θ(cid:101)(cid:16)(cid:112) (C⋆+Ccov)/TH2log(|F||W|/δ)(cid:17)
.
2: Set γ(t) =γ·t, and α(t) =8/γ(t).
3: Initialize D(1) =D(1) =∅ for all h∈[H].
on,h hybrid,h
4: for t=1,...,T do
5: Compute value function f(t) such that
f(t) ∈ar fg ∈m Fin wm ∈a Wx(cid:88)H (cid:12) (cid:12) (cid:12) (cid:12)E (cid:98) D h(t y) brid,h(cid:104) wˇ h(x h,a h)[∆(cid:98)hf](x h,a h,r h,x′ h+1)(cid:105)(cid:12) (cid:12) (cid:12) (cid:12)−α(t)E (cid:98) D h(t y) brid,h(cid:2) wˇ h2(x h,a h)(cid:3) , (14)
h=1
where wˇ :=clip γ(t)[w] and [∆(cid:98)hf](x,a,r,x′):=f h(x,a)−r−max a′f h+1(x′,a′).
6: Compute policy π(t) ←π f(t).
7: Collect trajectory (x 1,a 1,r 1),...,(x H,a H,r H) using π(t); D o(t n+ ,h1) :=D o(t n) ,h∪{(x h,a h,r h,x h+1)}.
8: Aggregate offline and online data: D h(t y+ b1 r)
id,h
:=D off,h(cid:12) (cid:12) 1:t∪D o(t n+ ,h1) for all h∈[H].
9: output: policy π (cid:98) =Unif(π(1),...,π(T)).
The realizability assumptions parallel those required that Glow to obtain the analogous Ccov/ε2-type sample
complexity for the purely online setting (cf. Assumption 2.2′). While the sample complexity matches
that of Glow, the computational efficiency is improved because we remove the need for global optimism.
More specifically, note that when clipping and the absolute value signs are removed from Eq. (13), the
optimization problem is concave-convex in the function class F and weight function class W, a desirable
property shared by standard density-ratio based offline algorithms (Xie and Jiang, 2020; Zhan et al., 2022).
Thus, if F and W are parameterized as linear functions, (i.e., F = {(x,a)(cid:55)→⟨ϕ(x,a),θ⟩|θ ∈Θ } and
F
W ={(x,a)(cid:55)→⟨ψ(x,a),θ⟩|θ ∈Θ } for feature maps ϕ and ψ) it can be solved in polynomial time using
W
standard tools for minimax optimization (e.g., Nemirovski, 2004). To accommodate clipping and the absolute
value signs efficiently, we note that by reparameterizing, Eq. (13) can be written as a convex-concave program
in which the max player optimizes over the set W(cid:102)γ where W(cid:102)γ,h :={±clip γn[w h]|w
h
∈W h}. While this set
may be complex, the result continues to hold if the max player optimizes over any expanded weight function
class W′ that satisfies W(cid:102)γ ⊆W′ and ∥w∥
∞
≤γn for all w ∈W′, thus allowing for the use of, e.g., convex
relaxations or alternate parameterizations. We defer the details to Appendix F.1.2.
Remark4.2(ComparisontoofflineRL). Itisinstructivetocomparetheperformanceof HyGlow toexisting
results for purely offline RL, which assume access to a data distribution ν with single-policy concentrability
(Definition 4.4). Let us write wπ := dπ/ν, w⋆ := dπ⋆ /ν and V⋆ for the optimal value function. The most
relevant work is the Pro-Rl algorithm of Zhan et al. (2022). Their algorithm is computationally efficient
and enjoys a polynomial sample complexity bound under the realizability of certain regularized versions of w⋆
and V⋆. By contrast, our result requires Q⋆-realizability and the density ratio realizability of wπ for all π ∈Π,
but for the unregularized problem. These assumptions are not comparable, as either may hold without the
other. Thus, these results are best thought of as complementary.9 However, our approach requires additional
online access, while their algorithm does not.
To the best of our knowledge, all other algorithms for the purely offline setting that only require single-policy
concentrability either need stronger representation conditions (such as value-function completeness (Xie et al.,
2021a)), or are not known to be computationally efficient in the general function approximation setting due
to the need for implementing pessimism (e.g., Chen and Jiang, 2022).
9ThesamplecomplexityboundinZhanetal.(2022)isalsoslightlylarger,scalingroughlyasO(cid:101)(cid:18) H6C ε⋆4 6C⋆2 ,ε(cid:19)
,whereC⋆,ε is
thesingle-policyconcentrabilityfortheregularizedproblem,asopposedtoourO(cid:101)(cid:0) H2(C⋆+Ccov)/ε2(cid:1)
.
134.3 Generic Reductions from Online to Offline RL?
Our hybrid-to-offline reduction H O and the CC-boundedness definition also shed light on the question of
2
when offline RL methods can be lifted to the purely online setting. Indeed, observe that any offline algorithm
which satisfies CC-boundedness (Definition 4.3) with only a π-coverage term, namely which satisfies an offline
(cid:98)
risk bound of the form
H
Risk ≤(cid:88) a γ E [CC (π,µ(1:n),γn)]+b , (15)
off n π(cid:98)∼p h (cid:98) γ
h=1
can be repeatedly invoked within H 2O (with D
off
=∅) to achieve a small (cid:112) Ccov/T-type risk bound for the
purely online setting, with no hybrid data. This can be seen immediately by inspecting our proof for the
hybrid setting (Theorem F.5 and Theorem 4.1).
We can think of algorithms satisfying Eq. (15) as optimistic offline RL algorithms, since their risk only scales
with a term depending on their own output policy; this is typically achieved using optimism. In particular, it
is easy to see, that Glow and Golf (Jin et al., 2021a; Xie et al., 2023) can be interpreted as repeatedly
invoking such an optimistic offline RL algorithm within the H O reduction. This class of algorithms has
2
not been considered in the offline RL literature since they inherit both the computational drawbacks of
pessimistic algorithms and the statistical drawbacks of “neutral” (i.e. non-pessimistic) algorithms (at least,
when viewed only in the context of offline RL).
In more detail, as with pessimism, optimism is often not computationally efficient, although it furthermore
requires all-policy concentrability (as opposed to single-policy concentrability) to obtain low offline risk. On
the other hand, neutral (non-pessimistic) algorithms such as Fqi (Chen and Jiang, 2019) and Mabo (Xie
and Jiang, 2020) also require all-policy concentrability, but are more computationally efficient. However,
our reduction shows that these algorithms might merit further investigation. In particular, it uncovers
that they can automatically solve the online setting (without hybrid data) under coverability and when
repeatedly invoked on datasets generated from their previous policies. We find that this reduction advances
the fundamental understanding of sample-efficient algorithms in both the online and offline settings, and
are optimistic that this understanding can be used for future algorithm design.
5 Discussion
Our work shows for the first time that density ratio modeling has provable benefits for online reinforcement
learning, and serves as step in a broader research program that aims to clarify connections between online
and offline reinforcement learning. To this end, we highlight some exciting directions for future research.
Realizability. Whileourresultsshowthatdensityratiorealizabilityallowsforsamplecomplexityguarantees
based on coverability that do not require Bellman completeness, the question of whether value function
realizability alone is sufficient still remains.
Generic reductions from online to offline RL. Our hybrid-to-offline reduction, H O, sheds light on
2
the question of when and how existing offline RL methods can be adapted as-is to the hybrid setting. Are
there more general principles under which offline RL methods can be adapted to online settings?
Practical and efficient online algorithms. Beyond the theoretical directions above, we are excited to
explore the possibility of developing practical and computationally efficient online reinforcement learning
algorithms based on density ratio modeling.
Acknowledgements
AS thanks Sasha Rakhlin for useful discussions. AS acknowledges support from the Simons Foundation and
NSF through award DMS-2031883, as well as from the DOE through award DE- SC0022199. Nan Jiang
acknowledges funding support from NSF IIS-2112471 and NSF CAREER IIS-2141781.
14References
Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire. Taming the
monster: A fast and simple algorithm for contextual bandits. In International Conference on Machine
Learning, pages 1638–1646, 2014.
András Antos, Csaba Szepesvári, and Rémi Munos. Learning near-optimal policies with bellman-residual
minimization based fitted policy iteration and a single sample path. Machine Learning, 71(1):89–129, 2008.
Philip J Ball, Laura Smith, Ilya Kostrikov, and Sergey Levine. Efficient online reinforcement learning with
offline data. In International Conference on Machine Learning, pages 1577–1594. PMLR, 2023.
SerkanCabi, SergioGómezColmenarejo, AlexanderNovikov, KseniaKonyushkova, ScottE.Reed, RaeJeong,
Konrad Zolna, Yusuf Aytar, David Budden, Mel Vecerík, Oleg Sushkov, David Barker, Jonathan Scholz,
Misha Denil, Nando de Freitas, and Ziyu Wang. Scaling data-driven robotics with reward sketching and
batch reinforcement learning. In Robotics: Science and Systems XVI, Virtual Event / Corvalis, Oregon,
USA, July 12-16, 2020, 2020.
Jinglin Chen and Nan Jiang. Information-theoretic considerations in batch reinforcement learning. In
International Conference on Machine Learning, 2019.
Jinglin Chen and Nan Jiang. Offline reinforcement learning under value and density-ratio realizability: the
power of gaps. In Uncertainty in Artificial Intelligence, pages 378–388. PMLR, 2022.
Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John Langford. Provably
efficient RL with rich observations via latent state decoding. In International Conference on Machine
Learning, pages 1665–1674. PMLR, 2019.
Simon S Du, Sham M Kakade, Ruosong Wang, and Lin F Yang. Is a good representation sufficient for sample
efficient reinforcement learning? In International Conference on Learning Representations, 2020.
Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun, and Ruosong Wang.
Bilinear classes: A structural framework for provable generalization in RL. International Conference on
Machine Learning, 2021.
Amir-massoud Farahmand, Csaba Szepesvári, and Rémi Munos. Error propagation for approximate policy
and value iteration. Advances in Neural Information Processing Systems, 23, 2010.
Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. The statistical complexity of interactive
decision making. arXiv preprint arXiv:2112.13487, 2021.
Dylan J Foster, Akshay Krishnamurthy, David Simchi-Levi, and Yunzong Xu. Offline reinforcement learning:
Fundamentalbarriersforvaluefunctionapproximation. InConferenceonLearningTheory,pages3489–3489.
PMLR, 2022.
Noah Golowich, Dhruv Rohatgi, and Ankur Moitra. Exploring and learning in sparse linear mdps without
computationally intractable oracles. arXiv preprint arXiv:2309.09457, 2023.
Audrey Huang, Jinglin Chen, and Nan Jiang. Reinforcement learning in low-rank mdps with density features.
arXiv preprint arXiv:2302.02252, 2023.
Nan Jiang and Alekh Agarwal. Open problem: The dependence of sample complexity lower bounds on
planning horizon. In Conference On Learning Theory, pages 3395–3398. PMLR, 2018.
NanJiangandJiaweiHuang. Minimaxvalueintervalforoff-policyevaluationandpolicyoptimization. Neural
Information Processing Systems, 2020.
Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire. Contextual
decision processes with low bellman rank are pac-learnable. In International Conference on Machine
Learning, pages 1704–1713. PMLR, 2017.
Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. Provably efficient reinforcement learning with
linear function approximation. In Conference on Learning Theory, pages 2137–2143, 2020.
15Chi Jin, Qinghua Liu, and Sobhan Miryoosefi. Bellman eluder dimension: New rich classes of RL problems,
and sample-efficient algorithms. Neural Information Processing Systems, 2021a.
Ying Jin, Zhuoran Yang, and Zhaoran Wang. Is pessimism provably efficient for offline RL? In International
Conference on Machine Learning, pages 5084–5096. PMLR, 2021b.
Ilya Kostrikov, Ofir Nachum, and Jonathan Tompson. Imitation learning via off-policy distribution matching.
In International Conference on Learning Representations, 2019.
AkshayKrishnamurthy,AlekhAgarwal,andJohnLangford.PACreinforcementlearningwithrichobservations.
In Advances in Neural Information Processing Systems, pages 1840–1848, 2016.
Jongmin Lee, Wonseok Jeon, Byungjun Lee, Joelle Pineau, and Kee-Eung Kim. Optidice: Offline policy
optimization via stationary distribution correction estimation. In International Conference on Machine
Learning, pages 6120–6130. PMLR, 2021.
Gen Li, Wenhao Zhan, Jason D Lee, Yuejie Chi, and Yuxin Chen. Reward-agnostic fine-tuning: Provable
statistical benefits of hybrid reinforcement learning. arXiv preprint arXiv:2305.10282, 2023.
Fanghui Liu, Luca Viano, and Volkan Cevher. Provable benefits of general coverage conditions in efficient
online rl with function approximation. International Conference on Machine Learning, 2023.
Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: Infinite-horizon
off-policy estimation. Advances in neural information processing systems, 31, 2018.
Zakaria Mhammedi, Dylan J Foster, and Alexander Rakhlin. Representation learning with multi-step inverse
kinematics: An efficient and optimal approach to rich-observation rl. International Conference on Machine
Learning (ICML), 2023.
Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. Kinematic state abstraction and
provably efficient rich-observation reinforcement learning. arXiv preprint arXiv:1911.05815, 2019.
Rémi Munos. Error bounds for approximate policy iteration. In International Conference on Machine
Learning, 2003.
Rémi Munos. Performance bounds in ℓ -norm for approximate value iteration. SIAM Journal on Control
p
and Optimization, 2007.
RémiMunosandCsabaSzepesvári. Finite-timeboundsforfittedvalueiteration. Journal of Machine Learning
Research, 2008.
Ofir Nachum and Bo Dai. Reinforcement learning via fenchel-rockafellar duality. arXiv preprint
arXiv:2001.01866, 2020.
Ofir Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice: Policy
gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.
Ashvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online reinforcement
learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.
Arkadi Nemirovski. Prox-method with rate of convergence O(1/t) for variational inequalities with Lipschitz
continuous monotone operators and smooth convex-concave saddle point problems. SIAM Journal on
Optimization, 15(1):229–251, 2004.
GergelyNeuandNnekaOkolo. Efficientglobalplanninginlargemdpsviastochasticprimal-dualoptimization.
In International Conference on Algorithmic Learning Theory, pages 1101–1123. PMLR, 2023.
Gergely Neu and Ciara Pike-Burke. A unifying view of optimism in episodic reinforcement learning. Advances
in Neural Information Processing Systems, 33:1392–1403, 2020.
Asuman E Ozdaglar, Sarath Pattathil, Jiawei Zhang, and Kaiqing Zhang. Revisiting the linear-programming
framework for offline rl with general function approximation. In International Conference on Machine
Learning, pages 26769–26791. PMLR, 2023.
16Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, and Stuart Russell. Bridging offline reinforcement
learning and imitation learning: A tale of pessimism. Advances in Neural Information Processing Systems,
34:11702–11716, 2021.
PariaRashidinejad,HanlinZhu,KunheYang,StuartRussell,andJiantaoJiao. Optimalconservativeofflinerl
with general function approximation via augmented lagrangian. In The Eleventh International Conference
on Learning Representations, 2023.
Daniel Russo and Benjamin Van Roy. Eluder dimension and the sample complexity of optimistic exploration.
In Advances in Neural Information Processing Systems, pages 2256–2264, 2013.
Yuda Song, Yifei Zhou, Ayush Sekhari, J Andrew Bagnell, Akshay Krishnamurthy, and Wen Sun. Hybrid
RL: Using both offline and online data can make RL efficient. International Conference on Learning
Representations, 2023.
Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. Model-based RL in
contextual decision processes: PAC bounds and exponential improvements over model-free approaches. In
Conference on learning theory, pages 2898–2933. PMLR, 2019.
Masatoshi Uehara, Jiawei Huang, and Nan Jiang. Minimax weight and Q-function learning for off-policy
evaluation. In International Conference on Machine Learning, 2020.
Masatoshi Uehara, Masaaki Imaizumi, Nan Jiang, Nathan Kallus, Wen Sun, and Tengyang Xie. Finite
sampleanalysisofminimaxofflinereinforcementlearning: Completeness,fastratesandfirst-orderefficiency.
arXiv:2102.02981, 2021.
Andrew Wagenmaker and Aldo Pacchiano. Leveraging offline data in online reinforcement learning. In
International Conference on Machine Learning, pages 35300–35338. PMLR, 2023.
RuosongWang, SimonSDu, LinYang, andShamKakade. Islonghorizonrlmoredifficultthanshorthorizon
rl? Advances in Neural Information Processing Systems, 33:9075–9085, 2020a.
Ruosong Wang, Dean Foster, and Sham M Kakade. What are the statistical limits of offline RL with linear
function approximation? In International Conference on Learning Representations, 2020b.
Ruosong Wang, Russ R Salakhutdinov, and Lin Yang. Reinforcement learning with general value function
approximation: Provably efficient approach via bounded eluder dimension. Advances in Neural Information
Processing Systems, 33, 2020c.
Yuanhao Wang, Ruosong Wang, and Sham M Kakade. An exponential lower bound for linearly-realizable
MDPs with constant suboptimality gap. Neural Information Processing Systems (NeurIPS), 2021.
Gellért Weisz, Philip Amortila, and Csaba Szepesvári. Exponential lower bounds for planning in MDPs
with linearly-realizable optimal action-value functions. In Algorithmic Learning Theory, pages 1237–1264.
PMLR, 2021.
Tengyang Xie and Nan Jiang. Q* approximation schemes for batch reinforcement learning: A theoretical
comparison. In Conference on Uncertainty in Artificial Intelligence, 2020.
Tengyang Xie and Nan Jiang. Batch value-function approximation with only realizability. In International
Conference on Machine Learning, pages 11404–11413. PMLR, 2021.
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism
for offline reinforcement learning. Advances in neural information processing systems, 34:6683–6694, 2021a.
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-
efficient offline and online reinforcement learning. Advances in neural information processing systems, 34:
27395–27407, 2021b.
Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, and Sham M Kakade. The role of coverage in online
reinforcement learning. International Conference on Learning Representations, 2023.
17Mengjiao Yang, Ofir Nachum, Bo Dai, Lihong Li, and Dale Schuurmans. Off-policy evaluation via the
regularized lagrangian. Advances in Neural Information Processing Systems, 33:6551–6561, 2020.
Andrea Zanette. Exponential lower bounds for batch reinforcement learning: Batch RL can be exponentially
harder than online RL. In International Conference on Machine Learning, 2021.
AndreaZanette,AlessandroLazaric,MykelKochenderfer,andEmmaBrunskill. Learningnearoptimalpolicies
with low inherent bellman error. In International Conference on Machine Learning, pages 10978–10989.
PMLR, 2020.
Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offline reinforcement learning with
realizability and single-policy concentrability. In Conference on Learning Theory, pages 2730–2775. PMLR,
2022.
Ruiyi Zhang, Bo Dai, Lihong Li, and Dale Schuurmans. Gendice: Generalized offline estimation of stationary
values. arXiv preprint arXiv:2002.09072, 2020.
Xuezhou Zhang, Yuda Song, Masatoshi Uehara, Mengdi Wang, Alekh Agarwal, and Wen Sun. Efficient
reinforcement learning in block mdps: A model-free representation learning approach. In International
Conference on Machine Learning, pages 26517–26547. PMLR, 2022.
Zihan Zhang, Xiangyang Ji, and Simon Du. Is reinforcement learning more difficult than bandits? a
near-optimal algorithm escaping the curse of horizon. In Conference on Learning Theory, pages 4528–4531.
PMLR, 2021.
18Contents of Appendix
A Additional Related Work 19
B Comparing Weight Function Realizability to Alternative Realizability Assumptions 20
C Examples for Glow 21
D Technical Tools 23
D.1 Reinforcement Learning Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
E Proofs from Section 3 (Online RL) 24
E.1 Supporting Technical Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
E.2 Main Technical Result: Bound on Cumulative Suboptimality for Glow . . . . . . . . . . . . 29
E.3 Proof of Theorem 3.1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
E.4 Proof of Theorem 3.2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
F Proofs and Additional Results from Section 4 (Hybrid RL) 36
F.1 Examples for H O . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
2
F.2 Proofs for H O (Theorem 4.1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
2
F.3 Proofs for H O Examples (Appendix F.1) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
2
A Additional Related Work
Online reinforcement learning. Xie et al. (2023) introduce the notion of coverability and provide regret
bounds for online reinforcement learning under the assumption of access to a value function class F satisfying
Bellman completeness. Liu et al. (2023) extend their result to more general coverage conditions under the
same Bellman completeness assumption. Our work complements these results by providing guarantees based
on coverability that do not require Bellman completeness.
To the best of our knowledge, our work is the first to provide provable sample complexity guarantees for
online reinforcement learning that take advantage of the ability to model density ratios, but a number of
closely related works bear mentioning. Recent work of Huang et al. (2023) considers the low-rank MDP
model and provides an algorithms which takes advantage of a form of occupancy realizability. Occupancy
realizability, while related to density ratio realizability, is stronger assumption in general: For example,
the Block MDP model (Krishnamurthy et al., 2016; Du et al., 2019; Misra et al., 2019; Zhang et al., 2022;
Mhammedi et al., 2023) admits a density ratio class of low complexity, but does not admit a small occupancy
class. Overall, however, their results are somewhat complementary, as they do not require any form of value
function realizability. A number of other recent works in online reinforcement learning also make use of
occupancy measures, but restrict to linear function approximation (Neu and Pike-Burke, 2020; Neu and
Okolo, 2023). Lastly, a number of works apply density ratio modeling in online settings with an empirical
focus (?Nachum et al., 2019), but do not address the exploration problem.
Offline reinforcement learning. Within the literature on offline reinforcement learning theory, density
ratio modeling has been widely used as a means to avoid strong Bellman completeness requirements, with
theoretical guarantees for policy evaluation (Liu et al., 2018; Uehara et al., 2020; Yang et al., 2020; Uehara
et al., 2021) and policy optimization (Jiang and Huang, 2020; Xie and Jiang, 2020; Zhan et al., 2022; Chen
and Jiang, 2022; Rashidinejad et al., 2023; Ozdaglar et al., 2023). A number of additional works investigate
density ratio modeling with an empirical focus, and do not provide finite-sample guarantees (Nachum et al.,
2019; Kostrikov et al., 2019; Nachum and Dai, 2020; Zhang et al., 2020; Lee et al., 2021).
Hybrid reinforcement learning. Song et al. (2023) were the first to show, theoretically, that the hybrid
reinforcement learning model can lead to computational benefits over online and offline RL individually. Our
reduction, H O, can be viewed as generalization of their Hybrid Q-Learning algorithm, with their result
2
corresponding to the special case in which Fqi is applied as a the base algorithm. Our guarantees under
19coverability complement their guarantees based on bilinear rank. Other recent works on hybrid reinforcement
learning in specialized settings (e.g., tabular MDPs or linear MDPs) include Wagenmaker and Pacchiano
(2023); Li et al. (2023); ?.
B Comparing Weight Function Realizability to Alternative Realiz-
ability Assumptions
In this section, we compare the density ratio realizability assumption in Assumption 2.2 to a number of
alternative realizability assumptions.
Comparison to Bellman completeness. A traditional assumption in the analysis of value function
approximation methods is to assume that F satisfies a representation condition called Bellman completeness,
which asserts that T F ⊆F ; this assumption is significantly stronger than just assuming realizability of
h h+1 h
Q⋆ (Assumption 2.1), and has been used throughout offline RL (Antos et al., 2008; Chen and Jiang, 2019),
and online RL (Zanette et al., 2020; Jin et al., 2021a; Xie et al., 2023).
Bellmancompletenessisincomparabletoourdensityratiorealizabilityassumption. Forexample,thelow-rank
MDP model (Jin et al., 2020) in which P (x′ |x,a)=⟨ϕ (x,a),ψ (x′)⟩ satisfies Bellman completeness when
h h h
the feature map ϕ is known to the learner even if ψ is unknown (but may not satisfy it otherwise), and
satisfies weight function realizability when the feature map ψ is known even if ϕ is unknown (but may not
satisfy it otherwise). Examples C.1 and C.2 in Appendix C give further examples that satisfy weight function
realizability but are not known to satisfy Bellman completeness.
Comparison to model-based realizability. Weight function realizability is strictly weaker than model-
based realizability (e.g., Foster et al., 2021), in which one assumes access to a model class M of MDPs that
contains the true MDP.10 Since each MDP induces an occupancy for every policy, it is straightforward to see
that given such a class, we can construct a weight function class W that satisfies Assumption 2.2 with
log|W|≤O(log|M|+log|Π|),
as well as a realizable value function class with log|F| ≤ O(log|M|). On the other hand, weight function
realizability does not imply model-based realizability; a canonical example that witnesses this is the Block
MDP.
Example B.1 (BlockMDP). Inthewell-studiedBlockMDPmodel(Krishnamurthyetal.,2016;Jiangetal.,
2017; Du et al., 2019; Misra et al., 2019; Zhang et al., 2022; Mhammedi et al., 2023), there exists realizable
value function class F and weight function class W with log|F|,log|W|≲poly(|S|,|A|,log|Φ|), where S is
the latent state space and Φ is a class of decoder functions. However, there does not exist a realizable model
class with bounded statistical complexity (see discussion in, e.g., Mhammedi et al. (2023)). ◁
Alternative forms of density ratio realizability. The following remarks concern slight variants of the
density ratio realizability assumption.
Remark B.1 (Clipped density ratio realizability). Since Glow only accesses the weight function class W
through the clipped weight functions, we can replace Assumption 2.2 with the assumption that for all π,π′ ∈Π,
t≤T, and h∈[H], we have
(cid:104) (cid:105)
clip
wπ;π′
∈W ,
γt h h
where T ∈N and γ ∈[0,1] are chosen as in Theorem 3.2. Likewise, we can replace Assumption 2.2′ with the
assumption that for all π ∈Π, and for all t≤T, h∈[H], and π(1:t) ∈Πt, we have
(cid:104) (cid:105)
clip
wπ;π(1:t)
∈W ,
γt h h
for T ∈N and γ ∈[0,1] chosen as in Theorem 3.1.
10UptoH factors,thisisequivalenttoassumingaccesstoaclassoftransitiondistributionsthatcanrealizethetruetransition
distribution,andaclassofrewarddistributionsthatcanrealizethetruerewarddistribution.
20Remark B.2 (Density ratio realizability relative to a fixed reference distribution). Assumption 2.2 is weaker
than assuming access to a class W that can realize the ratio dπ h/νh (or alternatively νh/dπ h) for all π ∈ Π,
where ν is an arbitrary fixed distribution (natural choices might include ν = µ⋆ or ν = dπ⋆). Indeed,
h h h h h
given access to such a class, the expanded class W′ :={w/w′ |w,w′ ∈W} satisfies Assumption 2.2, and has
log|W′|≤2log|W|.
C Examples for Glow
In this section, we give two new examples in which our main results for Glow, Theorems 3.1 and 3.2, can be
applied: MDPs with low-rank density features and general value functions, and a class of MDPs we refer to
as Generalized Block MDPs.
Example C.1 (Realizable Q⋆ with low-rank density features). Consider a setting in which (i) Q⋆ ∈ F,
and (ii), there is a known feature map ψ : X ×A → Rd with ∥ψ (x,a)∥ ≤ 1 such that for all π ∈ Π,
h h 2
dπ(x,a)=⟨ψ (x,a),θπ⟩ for an unknown parameter θπ ∈Rd with ∥θπ∥ ≤1. This assumption is sometimes
h h 2
referred to as low occupancy complexity (Du et al., 2021), and has been studied with and without known
features. In this case, we have C ≤ d, and one can construct a weight function class W that satisfies
cov
Assumption 2.2′ with log|W| ≤ O(cid:101)(dH) (Huang et al., 2023).11 As a result, Theorem 3.1 gives sample
complexity O(cid:101)(cid:0) H3d2log|F|/ε2(cid:1). Note that while this setup requires that the occupancies themselves have
low-rank structure, the class F can consist of arbitrary, potentially nonlinear functions (e.g., neural networks).
We remark that when the feature map ψ is not known, but instead belongs to a known class Ψ, the result
continues to hold, at the cost of expanding W to have size log|W|≤O(cid:101)(cid:0) dH +log|Ψ|(cid:1).
This example is similar to but complementary to Huang et al. (2023), who give guarantees for reward-free
exploration under low-rank occupancies. Their results do not require any form of value realizability, but
require a low-rank MDP assumption (which, in particular, implies Bellman completeness).12 ◁
Our next example concerns a generalization of the well-studied Block MDP framework (Krishnamurthy et al.,
2016; Jiang et al., 2017; Du et al., 2019; Misra et al., 2019; Zhang et al., 2022; Mhammedi et al., 2023) that
we refer to as the Generalized Block MDP.
DefinitionC.1(GeneralizedBlockMDP). AGeneralizedBlockMDPM=(X,S,A,P ,R ,q,H,d )
latent latent 1
is comprised of an observation space X, latent state space S, action space A, latent space transition kernel
P :S×A→∆(S), and emission distribution q :S →∆(X). The latent state space evolves based on
latent
the agent’s action a ∈A via the process
h
r ∼R (s ,a ), s ∼P (·|s ,a ), (16)
h latent h h h+1 latent h h
with s ∼ d ; we refer to s as the latent state. The latent state is not observed directly, and instead we
1 1 h
observe observations x ∈X generated by the emission process
h
x ∼q(·|s ). (17)
h h
We assume that the emission process satisfies the decodability property:
suppq(·|s)∩suppq(·|s′)=∅, ∀s′ ̸=s∈S. (18)
Decodability implies that there exists a (unknown to the agent) decoder ϕ : X →S such that ϕ (x )=s
⋆ ⋆ h h
a.s. for all h ∈ [H], meaning that latent states can be uniquely decoded from observations. Prior work on
the Block MDP framework (Krishnamurthy et al., 2016; Jiang et al., 2017; Du et al., 2019; Misra et al.,
2019; Zhang et al., 2022; Mhammedi et al., 2023) assumes that the latent space S and action space A are
finite, but allow the observation space X to be large or potentially infinite. They provide sample complexity
guarantees that scale as poly(|S|,|A|,H,log|Φ|,ε−1), where Φ is a known class of decoders that contains ϕ .
⋆
11Tobeprecise,W isinfinite,andthisresultrequiresacoveringnumberbound. Weomitaformaltreatment,andreferto
Huangetal.(2023)fordetails.
12Thelow-rankMDPassumptionisincomparabletotheassumptionweconsiderhere,asitimpliesthatdπ(x)=⟨ψ (x),θπ⟩,
h h
butdoesnotnecessarilyimplythatthestate-action occupancies(i.e. dπ(x,a))arelow-rank.
h
21We use the term Generalized Block MDP to refer to Block MDPs in which the latent space not tabular, and
can be arbitrarily large.
Example C.2 (Generalized Block MDPs with coverable latent states). We can use Glow to give sample
complexityguaranteesforGeneralizedBlockMDPsinwhichthelatentspaceislarge, buthaslowcoverability.
Let Π = (S ×[H] → ∆(A)) denote the set of all randomized policy that operate on the latent space.
latent
Assume that the following conditions hold:
• We have a value function class F such that Q⋆ ∈F, where Q⋆ is the optimal Q-function
latent latent latent
for the latent space.
• We have access to a class of latent space density ratios W such that for all h ∈ [H], and all
latent
π,π′ ∈Π ,
latent
dπ (s,a)
wπ,π′ (s,a):= latent,h ∈W ,
latent,h dπ′ (s,a) latent
latent,h
where dπ =Pπ(s =s,a =a) is the latent occupancy measure.
latent,h h h
• The latent coverability coefficient is bounded:
(cid:13)dπ (cid:13)
C cov,latent := µ1,...,µHin ∈f ∆(S×A)π∈Πlas teu ntp ,h∈[H](cid:13) (cid:13)
(cid:13)
la µte hnt,h(cid:13) (cid:13)
(cid:13)
∞.
We claim that whenever these conditions hold, analogous conditions hold in observation space (viewing the
Generalized BMDP as a large MDP), allowing Glow and Theorem 3.2 to be applied. Namely, we have:
• ThereexistsaclassF satisfyingAssumption2.1inobservationspacesuchthatlog|F|≤O(log|F |+log|Φ|).
latent
• There exists a weight function class W satisfying Assumption 2.2 in observation space such that
log|W|≤O(log|W |+log|Φ|).
latent
• We have C ≤C .
cov cov,latent
Asaresult,Glowattainssamplecomplexitypoly(C ,H,log|F |,log|W |,log|Φ|,ε−1). Thisgen-
cov.latent latent latent
eralizesexistingresultsforBlockMDPswithtabularlatentstatespaces,whichhavelog|F |,log|W |,C =
latent latent cov,latent
poly(|S|,|A|,H). ◁
22D Technical Tools
Lemma D.1 (Azuma-Hoeffding). Let M ∈N and (Y ) be a sequence of random variables adapted to a
m m≤M
filtration (F ) . If |Y |≤R almost surely, then with probability at least 1−δ,
m m≤M m
(cid:12) (cid:12)
(cid:12)(cid:88)M (cid:12) (cid:112)
(cid:12) Y −E [Y ](cid:12)≤R· 8Mlog(2δ−1).
(cid:12) m m−1 m (cid:12)
(cid:12) (cid:12)
m=1
Lemma D.2 (Freedman’s inequality (e.g., Agarwal et al., 2014)). Let M ∈N and (Y ) be a real-valued
m m≤M
martingale difference sequence adapted to a filtration (F ) . If |Y | ≤ R almost surely, then for any
m m≤M m
η ∈(0,1/R), with probability at least 1−δ,
(cid:12) (cid:12)
(cid:12) (cid:12)(cid:88)M
Y
(cid:12)
(cid:12)≤η
(cid:88)M
E (cid:2) (Y )2(cid:3) +
log(2δ−1)
.
(cid:12) m(cid:12) m−1 m η
(cid:12) (cid:12)
m=1 m=1
The following lemma is a standard consequence of Lemma D.2 (e.g., Foster et al., 2021).
Lemma D.3. Let M ∈N and (Y ) be a sequence of random variables adapted to a filtration (F ) .
m m≤M m m≤M
If 0≤Y ≤R almost surely, then with probability at least 1−δ,
m
M M
(cid:88) 3 (cid:88)
Y ≤ E [Y ]+4Rlog(2δ−1),
m 2 m−1 m
m=1 m=1
and
M M
(cid:88) (cid:88)
E [Y ]≤2 Y +8Rlog(2δ−1).
m−1 m m
m=1 m=1
D.1 Reinforcement Learning Preliminaries
Lemma D.4 (Jiang et al. (2017, Lemma 1)). For any value function f =(f ,...,f ),
1 H
H
(cid:88)
E x1∼d1[f 1(x 1,π f1(x 1))]−J(π f)= E dπf[f h(x h,a h)−[T hf h+1](x h,a h)].
h
h=1
Lemma D.5 (Per-state-action elliptic potential lemma; Xie et al. (2023, Lemma 4)). Let d(1),...,d(T) be an
arbitrary sequence of distributions over a set Z, and let µ∈∆(Z) be a distribution such that d(t)(z)/µ(z)≤C
for all z ∈Z and t∈[T]. Then, for all z ∈Z, we have
T
(cid:88) d(t)(z)
≤2log(1+T).
(cid:80)
d(m)(z)+Cµ(z)
t=1 i<t
23E Proofs from Section 3 (Online RL)
This section of the appendix is organized as follows:
• Appendix E.1 provides supporting technical results for Glow, including concentration guarantees.
• Appendix E.2 presents our main technical result for Glow, Lemma E.4, which bounds the cumulative
suboptimalityoftheiteratesπ(1),...,π(T)producedbythealgorithmforgeneralchoicesoftheparameters
T, K, and γ >0.
• Finally, in Appendices E.3 and E.4, we invoke with specific parameter choices to prove Theorems 3.1
and 3.2, as well as more general results (Theorems 3.1′ and 3.2′) that allow for misspecification error.
E.1 Supporting Technical Results
For x,x′ ∈X, a∈A, r ∈[0,1], and h∈[H], recall the notation
[∆(cid:98)hf](x,a,r,x′)=f h(x,a)−r−maxf h(x′,a′),
a′
[∆ f](x,a)=f (x,a)−[T f ](x,a),
h h h h+1
wˇ (x,a)=clip [w ](x,a).
h γ(t) h
Lemma E.1 (Basic concentration for Glow). Let γ(t) ≥0 for t∈[T]. With probability at least 1−δ, all of
the following inequalities hold for all f ∈F, w ∈W, t∈[T] and h∈[H]:
(a) (cid:12) (cid:12) (cid:12)E (cid:98) D(t)(cid:104) [∆(cid:98)hf](x h,a h,r h,x′ h+1)·wˇ h(x h,a h)(cid:105) −E d¯(t)[[∆ hf](x h,a h)·wˇ h(x h,a h)](cid:12) (cid:12) (cid:12)≤ 3γ1 (0
t)
E d¯(t)(cid:2) (wˇ h(x h,a h))2(cid:3) + β 1( 2t) ,
h h h
(b) γ(1
t)
E d¯(t)(cid:2) wˇ h2(x h,a h)(cid:3) ≤ γ(2 t)E (cid:98) D(t)(cid:2) wˇ h2(x h,a h)(cid:3) + 2β 9(t) ,
h h
(c) γ(1 t)E (cid:98) D(t)(cid:2) wˇ h2(x h,a h)(cid:3) ≤ 2γ3
(t)
E d¯(t)(cid:2) wˇ h2(x h,a h)(cid:3) + β 9(t) ,
h h
(cid:113)
(d) J(π⋆)−E x1∼d1[f 1(x 1,π f1(x 1))]≤E (cid:98)
x1∼D
1(t)[max aQ⋆ 1(x 1,a 1)−f 1(x 1,π f1(x 1))]+ 8log(6 K|F (| t| −W 1| )TH/δ),
where wˇ :=clip [w ] and β(t) :=
36γ(t)
log(6|F||W|TH/δ).
h γ(t) h K(t−1)
ProofofLemmaE.1. Fixanyh∈[H]andt∈[T]. LetM =K(t−1)andrecallthatthedatasetDt consists
h
of M tuples of the form {(x(m),a(m),r(m),x(m) )} where x(m) ∼P(·|x(m),a(m)), and a(m) =π (x(m))
h h h h+1 m≤M h+1 h h h τ(m) h
where τ(m)=⌈m/K⌉. Fix any f ∈F and w ∈W.
Proof of (a). For each m∈[M], define the random variable
Y
m
=[∆(cid:98)hf](x( hm),a( hm),r h(m),x( hm +) 1)·wˇ h(x( hm),a( hm))−E d(τ(m))[[∆ hf](x h,a h)·wˇ h(x h,a h)]
h
Clearly, E [Y ]=0 and thus {Y } is a martingale difference sequence with
m−1 m m m≤M
|Y |≤3 sup|wˇ (x ,a )|≤3γ(t),
m h h h
xh,ah
since |[∆(cid:98)hf](x( hm),a( hm),r h(m),x( hm +) 1)|≤2 and |[∆ hf](x h,a h)|≤1. Furthermore,
M M M
(cid:88) (cid:88) (cid:88)
Y
m
= [∆(cid:98)hf](x( hm),a( hm),r h(m),x( hm +) 1)·wˇ h(x( hm),a( hm))− E d(τ(m))[[∆ hf](x h,a h)·wˇ h(x h,a h)]
h
m=1 m=1 m=1
t−1
(cid:104) (cid:105) (cid:88)
=K(t−1)E (cid:98) D(t) [∆(cid:98)hf](x h,a h,r h,x′ h+1)·wˇ h(x h,a h) −K E d(τ)[[∆ hf](x h,a h)·wˇ h(x h,a h)]
h h
τ=1
24(cid:104) (cid:105)
=K(t−1)E (cid:98) D(t) [∆(cid:98)hf](x h,a h,r h,x′ h+1)·wˇ h(x h,a h) −K(t−1)E d¯(t)[[∆ hf](x h,a h)·wˇ h(x h,a h)].
h h
Additionally, we also have that
E m−1(cid:2) (Y m)2(cid:3) ≤2E m−1(cid:104) ([∆(cid:98)hf](x( hm),a( hm),r h(m),x( hm +) 1)·wˇ h(x( hm),a( hm)))2(cid:105)
(cid:20)(cid:16) (cid:17)2(cid:21)
+2E E [[∆ f](x ,a )·wˇ (x ,a )]
m−1 d(τ(m)) h h h h h h
h
(cid:104) (cid:104) (cid:105)(cid:105)
≤E 8wˇ (x(m),a(m))2 +2E ([∆ f](x ,a )·wˇ (x ,a ))2
m−1 h h h d(τ(m)) h h h h h h
h
(cid:104) (cid:104) (cid:105)(cid:105)
≤E 8wˇ (x(m),a(m))2 +2E (wˇ (x ,a ))2
m−1 h h h d(τ(m)) h h h
h
(cid:104) (cid:105)
=10E (wˇ (x ,a ))2
d(τ(m)) h h h
h
where the second line follows since |[∆(cid:98)hf](x( hm),a( hm),r h(m),x( hm +) 1)|≤2 and by using Jensen’s inequality, and
the third line uses |[∆ f](x ,a )|≤1.
h h h
Thus, using Lemma D.2 with η =1/3γ(t), we get that with probability at least 1−δ′,
(cid:12) (cid:12)
(cid:12)(cid:88)M (cid:12) (cid:12) (cid:104) (cid:105) (cid:12)
(cid:12) (cid:12) Y m(cid:12) (cid:12)=K(t−1)(cid:12) (cid:12)E (cid:98) D(t) [∆(cid:98)hf](x h,a h,r h,x′ h+1)·wˇ h(x h,a h) −E d¯(t)[[∆ hf](x h,a h)·wˇ h(x h,a h)](cid:12) (cid:12)
(cid:12) (cid:12) h h
m=1
t−1
≤ 10K (cid:88) E (cid:2) (wˇ (x ,a ))2(cid:3) +3γ(t)log(2/δ′)
3γ(t) d( hτ) h h h
τ=1
= 10K(t−1) E (cid:2) (wˇ (x ,a ))2(cid:3) +3γ(t)log(2/δ′).
3γ(t) d¯( ht) h h h
The above bound implies that
(cid:12) (cid:104) (cid:105) (cid:12)
(cid:12) (cid:12)E (cid:98) D(t) [∆(cid:98)hf](x h,a h,r h,x′ h+1)·wˇ h(x h,a h) −E d¯(t)[[∆ hf](x h,a h)·wˇ h(x h,a h)](cid:12) (cid:12)
h h
≤ 10 E (cid:2) (wˇ (x ,a ))2(cid:3) + 3γ(t) log(2/δ′).
3γ(t) d¯( ht) h h h K(t−1)
Plugging in the value of β(t) gives the desired bound. The final result follows by setting δ′ =δ/3|F||W|TH, and
taking another union bound over the choice of f,w,t and h.
Proof of (b) and (c). For each m∈[M], define the random variable
Y
=(cid:0)
wˇ
(x(m),a(m))(cid:1)2
.
m h h h
Clearly,thesequence{Y } isadaptedtoanincreasingfiltration,withY ≥0and|Y |=|(wˇ (x(m),a(m)))2|≤
m m≤M t t h h h
(γ(t))2. Furthermore,
M t−1
(cid:88) E [Y ]=K(cid:88) E (cid:2) (wˇ (x ,a ))2(cid:3) =K(t−1)E (cid:2) (wˇ (x ,a ))2(cid:3) ,
m−1 m d(τ) h h h d¯(t) h h h
h h
m=1 τ=1
and,
M
(cid:88) Y m = (cid:88) (wˇ h(x h,a h))2 =K(t−1)E (cid:98) D(t)(cid:2) (wˇ h(x h,a h))2(cid:3) .
h
m=1 (x,a)∈D(t)
h
25Thus, by Lemma D.3, we have that with probability at least 1−δ′,
E d¯(t)(cid:2) (wˇ h(x h,a h))2(cid:3) ≤2E (cid:98) D(t)(cid:2) (wˇ h(x h,a h))2(cid:3) + 8(γ( Kt)) (2 tl −og 1(2 )/δ′) ,
h h
and
E (cid:98) D(t)(cid:2) (wˇ h(x h,a h))2(cid:3) ≤ 3 2E d¯(t)(cid:2) (wˇ h(x h,a h))2(cid:3) + 4(γ( Kt)) (2 tl −og 1(2 )/δ′) .
h h
The final result follows by setting δ′ =δ/3|F||W|TH, and taking another union bound over the choice of f,w,t
and h.
Proof of (d). For each m∈[M], define the random variable
Y =maxQ⋆(x(m),a)−f (x(m),π (x(m))).
m
a
1 1 1 1 f1 1
Clearly, |Y |≤1. Thus, using Lemma D.1, we get that with probability at least 1−δ′,
m
M M
(cid:88) (cid:88)(cid:16) (cid:17) (cid:112)
E [Y ]≤ maxQ⋆(x(m),a)−f (x(m),π (x(m))) + 8Mlog(2/δ′).
m−1 m
a
1 1 1 1 f1 1
m=1 m=1
Setting M =K(t−1) and noting that E [Y ]=J(π⋆)−E [f (x ,π (x )] since x(m) ∼d for any
m−1 m x1∼d1 1 1 f1 1 1 1
m∈[M], we get that
(cid:115)
(cid:104) (cid:105) 8log(2/δ′)
J(π⋆)−E [f (x ,π (x )]≤E maxQ⋆(x ,a)−f (x ,π (x )) +
x1∼d1 1 1 f1 1 x∼D 1(t) a 1 1 1 1 f1 1 K(t−1)
The final result follows by setting δ′ =δ/3|F||W|TH, and taking another union bound over the choice of f,w,t
and h.
Lemma E.2 (Properties of Glow confidence set). Let γ(t) ≥0 for t∈[T]. With probability at least 1−δ,
all of the following events hold:
(a) For all t≥1, Q⋆ ∈F(t)
(b) For all t≥2, h∈[H], f ∈F(t), and w ∈W, we have
E [[∆ f](x ,a )·wˇ (x ,a )]≤ 20 E (cid:2) (wˇ (x ,a ))2(cid:3) + 7β(t) .
d¯( ht) h h h h h h γ(t) d¯( ht) h h h 18
Furthermore,
E [[∆ f](x ,a )·wˇ (x ,a )]≤ 40 E (cid:2) (wˇ (x ,a ))2(cid:3) + 7β(t) + γ(t) ,
d¯( ht+1) h h h h h h γ(t) d¯( ht+1) h h h 9 160t2
(c) For all t≥2, we have
(cid:115)
(cid:104) (cid:105) 8log(6|F||W|TH/δ)
E maxQ⋆(x ,a)−f(t)(x ,π(t)(x )) ≤ ,
x1∼d1 a 1 1 1 1 1 1 K(t−1)
where wˇ :=clip [w ] and β(t) =
36γ(t)
log(6|F||W|TH/δ).
h γ(t) h K(t−1)
26Proof of Lemma E.2. Using Lemma E.1, we have that with probability at least 1−δ, for all f ∈ F,
w ∈W, t∈[T] and h∈[H],
(cid:12) (cid:104) (cid:105) (cid:12)
(cid:12) (cid:12)E (cid:98) D(t) [∆(cid:98)hf](x h,a h,r h,x′ h+1)·wˇ h(x h,a h) −E d¯(t)[[∆ hf](x h,a h)·wˇ h(x h,a h)](cid:12) (cid:12)
h h
≤ 10 E (cid:2) (wˇ (x ,a ))2(cid:3) + β(t) , (19)
3γ(t) d¯ h(t) h h h 12
γ1 (t) E d¯( ht)(cid:2) (wˇ h(x h,a h))2(cid:3) ≤ γ2 (t)E (cid:98) D h(t)(cid:2) (wˇ h(x h,a h))2(cid:3) + 2β 9(t) , (20)
γ1 (t)E (cid:98)
D
h(t)(cid:2) (wˇ h(x h,a h))2(cid:3) ≤ 2γ3
(t)
E
d¯
h(t)(cid:2) (wˇ h(x h,a h))2(cid:3) + β 9(t) , (21)
and,
(cid:104) (cid:105)
J(π⋆)−E d1[f 1(x 1,π f1(x 1)]≤E (cid:98) D 1(t) m aaxQ⋆ 1(x 1,a)−f 1(x 1,π f1(x 1))
(cid:115)
8log(6|F||W|TH/δ)
+ . (22)
K(t−1)
For the rest of the proof, we condition on the event in which (19-22) hold.
Proof of (a). Consider any t∈[T], and observe that the optimal state-action value function Q⋆ satisfies
for any w ∈W,
h
(cid:104) (cid:105)
E (Q⋆(x ,a )−r −maxQ⋆ (x′ ,a′))·wˇ (x ,a ) =0,
d¯( ht) h h h h
a′
h+1 h+1 h h h
where wˇ :=clip [w ]. Using the above relation with (19), we get that
h γ(t) h
E (cid:98) D h(t)(cid:104) (Q⋆ h(x h,a h)−r h−m aa ′xQ⋆ h+1(x′ h+1,a′))·wˇ h(x h,a h)(cid:105) ≤ 31 γ0 (t) E d¯( ht)(cid:2) (wˇ h(x h,a h))2(cid:3) + β 1( 2t)
≤ 4 E (cid:2) (wˇ (x ,a ))2(cid:3) + β(t)
γ(t) d¯ h(t) h h h 12
≤ γ8 (t)E (cid:98)
D
h(t)(cid:2) (wˇ h(x h,a h))2(cid:3) +β(t),
where the second-last inequality follows from (20).
Plugging in the values of α(t) and β(t), rearranging the terms, we get that
(cid:104) (cid:105)
E (cid:98) D h(t) (Q⋆ h(x h,a h)−r h−m aa ′xQ⋆ h+1(x′ h+1,a′))·wˇ h(x h,a h)−α(t)(wˇ h(x h,a h))2 ≤β(t).
Since the above inequality holds for all w ∈W, we have that Q⋆ ∈F(t).
Proof of (b). Fix any t, and note that by the definition of F(t), any f ∈F(t) satisfies for any w ∈W, the
bound
E (cid:98)
D
h(t)(cid:104) (f h(x h,a h)−r h−m aa ′xf h+1(x′ h+1,a′))·wˇ h(x h,a h)(cid:105) ≤ γ1 (0 t)E (cid:98)
D
h(t)(cid:2) (wˇ h(x h,a h))2(cid:3) +β(t).
Using the above bound with (19), we get that
E d¯( ht)[[∆ hf](x h,a h)·wˇ h(x h,a h)]≤ 31 γ0 (t) E d¯( ht)(cid:2) (wˇ h(x h,a h))2(cid:3) + γ1 (0 t)E (cid:98) D h(t)(cid:2) (wˇ h(x h,a h))2(cid:3) + 1 13 2β(t).
27Plugging the bound from (21) for the second term above, we get that
E [[∆ f](x ,a )·wˇ (x ,a )]≤ 20 E (cid:2) (wˇ (x ,a ))2(cid:3) + 7β(t) .
d¯( ht) h h h h h h γ(t) d¯( ht) h h h 18
Finally, noting that d¯ (t+1) = (t−1)d¯(t)+d(t), we can further upper bound as:
t
E [[∆ f](x ,a )·wˇ (x ,a )]
d¯(t+1) h h h h h h
h
(cid:18) (cid:19)
≤ t−1 20 E (cid:2) (wˇ (x ,a ))2(cid:3) + 7β(t) + 1 E [[∆ f](x ,a )·wˇ (x ,a )]
t γ(t) d¯( ht) h h h 18 t d h(t) h h h h h h
(cid:18) (cid:19)
≤2 20 E (cid:2) (wˇ (x ,a ))2(cid:3) + 7β(t) + 1 E [|wˇ (x ,a )|]
γ(t) d¯( ht) h h h 18 t d( ht) h h h
≤ 40 E (cid:2) (wˇ (x ,a ))2(cid:3) + 7β(t) + 40 E (cid:2) wˇ (x ,a )2(cid:3) + γ(t)
γ(t) d¯( ht) h h h 9 γ(t) d h(t) h h h 160t2
= 40 E (cid:2) (wˇ (x ,a ))2(cid:3) + 7β(t) + γ(t) ,
γ(t) d¯( ht+1) h h h 9 160t2
where the second-to-last line follows from an application of AM-GM inequality.
Proof of (c). Plugging in f =f(t) in (22) and noting that max Q⋆(x ,a)=Q⋆(x,π⋆(x)) for any x∈X,
a 1 1 1
we get that
(cid:115)
(cid:104) (cid:105) (cid:104) (cid:105) 8log(6|F||W|TH/δ)
J(π⋆)−E x1∼d1 f 1(t)(x 1,π f 1(t)(x 1)) ≤E (cid:98) x1∼D 1(t) Q⋆ 1(x 1,π 1⋆(x 1))−f 1(t)(x 1,π f 1(t)(x 1)) + K(t−1) .
Howe (cid:104)ver, note that by definition, f(t) ∈ (cid:105)argmax fE (cid:98)[f 1(x 1,π f1(x 1)], and using part-(a), Q⋆ ∈ F(t). Thus,
E (cid:98) D(t) Q⋆ 1(x 1,π 1⋆(x 1))−f 1(t)(x 1,π f(t)(x 1)) ≤0, which implies that
1 1
(cid:115)
(cid:104) (cid:105) 8log(6|F||W|TH/δ)
J(π⋆)−E f(t)(x ,π (x )) ≤ .
x1∼d1 1 1 f(t) 1 K(t−1)
1
Lemma E.3 (Coverability potential bound). Let d(1),...,d(T) be an arbitrary sequence of distributions over
X ×A, such that there exists a distribution µ∈∆(X ×A) that satisfies ∥d(t)/µ∥
∞
≤C for all (x,a)∈X ×A
and t∈[T]. Then,
T (cid:34) (cid:35)
(cid:88) d(t)(x,a)
E ≤5Clog(T),
(x,a)∼d(t)
d(cid:101)(t+1)(x,a)
t=1
where recall that d(cid:101)(t+1) :=
(cid:80)t
d(t) for all t∈[T].
s=1
Proof of Lemma E.3. Let τ(x,a)=min{t|d(cid:101)(t+1)(x,a)≥Cµ(x,a)}. With this definition, we can bound
T (cid:34) (cid:35) T (cid:34) (cid:35)
(cid:88) d(t)(x,a) (cid:88) d(t)(x,a)
E = E ·I{t<τ(x,a)}
d(t) d(t)
d(cid:101)(t+1)(x,a) d(cid:101)(t+1)(x,a)
t=1 t=1
T (cid:34) (cid:35)
(cid:88) d(t)(x,a)
+ E ·I{t≥τ(x,a)}
d(t)
d(cid:101)(t+1)(x,a)
t=1
T T (cid:34) (cid:35)
(cid:88) (cid:88) d(t)(x,a)
≤ E [I{t<τ(x,a)}]+ E ·I{t≥τ(x,a)} ,
d(t) d(t)
d(cid:101)(t+1)(x,a)
t=1 t=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(I):burn-inphase (II):stablephase
28where the second line uses that d(t)(x,a)/d(cid:101)(t+1)(x,a)≤1.
For the burn-in phase, note that
T
(cid:88) (cid:88) (cid:88) (cid:88) (cid:88)
E d(t)[I{t<τ(x,a)}]= dt(x,a)= d(cid:101)(τ(x,a))(x,a)≤ Cµ(x,a)=C,
t=1 x,a t<τ(x,a) x,a x,a
where the last inequality uses that by definition, d˜t(x,a)≤Cµ(x,a) for all t≤τ(x,a).
For the stable phase, whenever t≥τ(x,a), by definition, we have d(cid:101)(t+1)(x,a)≥Cµ(x,a) which implies that
d(cid:101)(t+1)(x,a)≥ 1(d(cid:101)(t+1)(x,a)+Cµ(x,a)). Thus,
2
T (cid:34) (cid:35)
(cid:88) d(t)(x,a)
(II)≤2 E (23)
d(t)
d(cid:101)(t)(x,a)+Cµ(x,a)
t=1
T
(cid:88)(cid:88) d(t)(x,a)·d(t)(x,a)
=2
d(cid:101)(t)(x,a)+Cµ(x,a)
x,a t=1
(cid:32) T (cid:33)
(cid:88) (cid:88) d(t)(x,a)
≤2 max d(t′)(x,a)max .
t′∈[T] x,a d(cid:101)(t)(x,a)+Cµ(x,a)
x,a t=1
Using the per-state elliptical potential lemma (Lemma D.5) in the above inequality, we get that
(cid:88) (cid:88)
(II)≤4log(T +1) max d(t′)(x,a)≤4Clog(T +1) µ(x,a)=4Clog(1+T), (24)
t′∈[T]
x,a x,a
(cid:13) (cid:13)
where the second inequality follows from the fact that (cid:13)d(t)(cid:13) ≤C (by definition), and the last equality uses
(cid:13) µ (cid:13)
that (cid:80) µ(x,a)=1. Combining the above bound, we get∞ that
x,a
T (cid:34) (cid:35)
(cid:88) d(t)(x,a)
E ≤5Clog(T).
d(t)
d(cid:101)(t+1)(x,a)
t=1
E.2 Main Technical Result: Bound on Cumulative Suboptimality for Glow
In this section we prove a key technical lemma, Lemma E.4, which gives a bound on the cumulative
suboptimalityofthesequenceofpoliciesgeneratedbyGlow. BoththeproofsofTheorem3.2andTheorem3.1
build on this result. To facilitate more general sample complexity bounds that allow for misspecification error
in W. In particular, for each t∈[T], we define ξ as the misspecification error of the clipped density ratio
t
d( ht)/d¯( ht+1) in class W h, defined as
ξ t := hs ∈u [Hp ]wi ∈n Wf hπsu ∈p Π(cid:13) (cid:13) (cid:13) (cid:13)clip γ(t)(cid:20) d¯d h(t( h +t) 1)(cid:21) −clip γ(t)[w h](cid:13) (cid:13) (cid:13)
(cid:13)
1,d¯π, (25)
h
where recall that for any function u : X ×A (cid:55)→ R and distribution d ∈ ∆(X ×A), the norm ∥u∥ :=
1,d
E [|u(x,a)|]. Note that under Assumption 2.2 or 2.2′, ξ =0 for all t∈[T].
(x,a)∼d t
Lemma E.4 (Bound on cumulative suboptimality). Let π(1),...,π(T) be the sequence of policies generated by
Glow, when executed on classes F and W with parameters T,K and γ. Then the cumulative suboptimality
of the sequence of policies {π(t)} is bounded as
t∈[T]
(cid:88)T J(π⋆)−J(π(t))=O(cid:32) H(cid:32) C covlog(1+T)
+
γT log(|F||W|HTδ−1) +(cid:88)T
ξ
+γlog(T)(cid:33)(cid:33)
.
γ K t
t=1 t=1
29Proof of Lemma E.4. Fix any t≥2. We begin by establishing optimism as follows:
(cid:104) (cid:105)
J(π⋆)−J(π(t))=E maxQ⋆(x ,a) −J(π(t))
x1∼d1
a
1 1
=E (cid:104) maxQ⋆(x ,a)−f(t)(x ,π(t)(x ))(cid:105) +E (cid:2) f(t)(x ,π(t)(x ))(cid:3) −J(π(t))
x1∼d1
a
1 1 1 1 1 x1∼d1 1 1 1
(cid:115)
≤ 8log(6|F||W|TH/δ) +E (cid:2) f(t)(x ,π(t)(x ))(cid:3) −J(π(t)),
K(t−1) x1∼d1 1 1 1
where the last line follows from Lemma E.2-(c). Using Lemma D.4 for the second term, we get that
(cid:115)
H
8log(6|F||W|TH/δ) (cid:88)
J(π⋆)−J(π(t))≤ + E [[∆ f(t)](x ,a )],
K(t−1) (xh,ah)∼d( ht) h h h
h=1
where recall that [∆ f(t)](x ,a ):=f(t)(x ,a )−[T f(t) ](x ,a ). Thus,
h h h h h h h h+1 h h
T T
(cid:88) (cid:88)
J(π⋆)−J(π(t))≤J(π⋆)−J(π(1))+ J(π⋆)−J(π(t))
t=1 t=2
(cid:115)
T T H
(cid:88) 8log(6|F||W|TH/δ) (cid:88)(cid:88)
≤1+ + E [[∆ f(t)](x ,a )], (26)
K(t−1) (xh,ah)∼d h(t) h h h
t=2 t=2h=1
where the second inequality uses that J(π⋆)≤1 and J(π(1))≥0.
We next bound the expected Bellman error terms that appear in the right-hand-side above. Consider any
t≥2 and h∈[H], and note that via a straightforward change of measure,
(cid:20) d(t)(x ,a ) (cid:21)
E [[∆ f(t)](x ,a )]=E [∆ f(t)](x ,a )· h h h
d(t) h h h d¯(t+1) h h h d¯(t+1)(x ,a )
h h h h h
Since u≤min{u,v}+uI{u≥v} for any u,v, we further decompose as
(cid:20) (cid:26) d(t)(x ,a ) (cid:27)(cid:21)
E [[∆ f(t)](x ,a )]≤E [∆ f(t)](x ,a )·min h h h ,γ(t)
d(t) h h h d¯(t+1) h h h d¯(t+1)(x ,a )
h h h h h
(cid:124) (cid:123)(cid:122) (cid:125)
(A):ExpectedclippedBellmanerror
(cid:20) (cid:26) d(t)(x ,a ) (cid:27)(cid:21)
+E I h h h ≥γ(t) ,
d(t) d¯(t+1)(x ,a )
h h h h
(cid:124) (cid:123)(cid:122) (cid:125)
(B):clippingviolation
where in the second term we have changed the measure back to d(t) and used that |[∆ f(t)](x ,a )|≤1. We
h h h h
bound the terms (A) and (B) separately below.
Bound on expected clipped Bellman error. Let w(t)(x ,a ) ∈ W denote a weight function which
h h h
satisfies
su πp(cid:13) (cid:13) (cid:13) (cid:13)clip γ(t)(cid:20) d¯d
(
hth( +t) 1)(cid:21) −clip γ(t)(cid:2) w h(t)(cid:3)(cid:13) (cid:13) (cid:13)
(cid:13)
1,dπ
≤ξ t, (27)
h
which is guaranteed to exist by the definition of ξ . Then, we have
t
(cid:20) (cid:20) d(t)(x ,a ) (cid:21)(cid:21)
(A)=E [∆ f(t)](x ,a )·clip h h h
d¯(t+1) h h h γ(t) d¯(t+1)(x ,a )
h h h h
≤E d¯( ht+1)(cid:2) [∆ hf(t)](x h,a h)·clip γ(t)(cid:2) w h(t)(x h,a h)(cid:3)(cid:3) +(cid:13) (cid:13) (cid:13) (cid:13)clip γ(t)(cid:20) d¯d ( hth( +t) 1)(cid:21) −clip γ(t)(cid:2) w h(t)(cid:3)(cid:13) (cid:13) (cid:13) (cid:13)
1,d¯(t+1)
h
30≤E (cid:2) [∆ f(t)](x ,a )·clip (cid:2) w(t)(x ,a )(cid:3)(cid:3) +ξ ,
d¯(t+1) h h h γ(t) h h h t
h
where the second line uses that |[∆ f(t)](x ,a )|≤1, and the last line plugs in (27). Next, using Lemma E.2-
h h h
(b) in the above inequality, we get that
(A)≤ 40 E (cid:104)(cid:0) clip (cid:2) w(t)(x ,a )(cid:3)(cid:1)2(cid:105) + 7β(t) + γ(t) +ξ . (28)
γ(t) d¯( ht+1) γ(t) h h h 9 160t2 t
Further splitting the first term, and using that (a+b)2 ≤2a2+2b2 , we have that
E (cid:104)(cid:0) clip (cid:2) w(t)(x ,a )(cid:3)(cid:1)2(cid:105)
d¯(t+1) γ(t) h h h
h
≤2E d¯( ht+1)(cid:34)(cid:18) clip γ(t)(cid:20) d¯d ( ht( h +t) 1( )x (xh h, ,a ah h) )(cid:21)(cid:19)2(cid:35) +2(cid:13) (cid:13) (cid:13) (cid:13)clip γ(t)(cid:20) d¯d ( hth( +t) 1)(cid:21) −clip γ(t)(cid:2) w h(t)(cid:3)(cid:13) (cid:13) (cid:13) (cid:13)2
2,d¯(t+1)
h
≤2E d¯( ht+1)(cid:34)(cid:18) clip γ(t)(cid:20) d¯d ( ht( h +t) 1( )x (xh h, ,a ah h) )(cid:21)(cid:19)2(cid:35) +2γ(t)(cid:13) (cid:13) (cid:13) (cid:13)clip γ(t)(cid:20) d¯d h(t( h +t) 1)(cid:21) −clip γ(t)(cid:2) w h(t)(cid:3)(cid:13) (cid:13) (cid:13) (cid:13)
1,d¯(t+1)
h
(cid:34)(cid:18) (cid:20) d(t)(x ,a ) (cid:21)(cid:19)2(cid:35)
≤2E clip h h h +2γ(t)ξ ,
d¯(t+1) γ(t) d¯(t+1)(x ,a ) t
h h h h
where the second line holds since ∥w∥2 ≤∥w∥ ∥w∥ and clip [w]≤γ(t) for any w, and the last line is
2,d ∞ 1,d γ(t)
due to (27). Using the above bound in (28), we get that
80 (cid:34)(cid:18) (cid:26) d(t)(x ,a ) (cid:27)(cid:19)2(cid:35) γ(t)
(A)≤ E min h h h ,γ(t) +4ξ +10β(t)+
γ(t) d¯( ht+1) d¯( ht+1)(x h,a h) t 80t2
80 (cid:20) d(t)(x ,a ) (cid:21) γ(t)
≤ E h h h +4ξ +10β(t)+
γ(t) d( ht) d¯( ht+1)(x h,a h) t 80t2
(cid:34) (cid:35)
80 d(t)(x ,a ) γ
= E h h h +4ξ +10β(t)+ ,
γ d( ht) d(cid:101)( ht+1)(x h,a h) t 80t
where the second line simply follows from a change of measure, and the last line holds since γ(t) =γt, and
d(cid:101)(t+1) =td¯ (t+1).
Bound on clipping violation. Since I{u≥v}≤ u for any u,v ≥0, we get that
v
1 (cid:20) d(t)(x ,a ) (cid:21) 1 (cid:34) d(t)(x ,a ) (cid:35)
(B)≤ E h h h = E h h h ,
γ(t) d( ht) d¯( ht+1)(x h,a h) γ d( ht) d(cid:101)( ht+1)(x h,a h)
where the last line holds since γ(t) =γt.
Combining the bounds on the terms (A) and (B) above, and summing over the rounds t=2,...,T, we get
(cid:88)T
E [[∆ f(t)](x ,a )]≤
81(cid:88)T
E
(cid:34) d h(t)(x h,a h) (cid:35) +10(cid:88)T β(t)+4(cid:88)T
ξ +
γ
, (29)
t=2
d( ht) h h h γ
t=2
d( ht) d(cid:101) h(t+1)(x h,a h)
t=2 t=2
t 80
For the first term, using Lemma E.3, along with the bound (cid:13) (cid:13)d( ht)/µ⋆ h(cid:13) (cid:13)
∞
≤C cov, we get that
(cid:88)T
E
(cid:34) d h(t)(x h,a h) (cid:35)
≤5C log(1+T).
t=2
d( ht)
d(cid:101) h(t+1)(x h,a h)
cov
For the second term, we have
(cid:88)T (cid:88)T 36γtlog(6|F||W|HTδ−1) 72γT log(6|F||W|HTδ−1)
β(t) = ≤ .
K(t−1) K
t=2 t=2
31Combining these bounds, we get that
(cid:88)T
E [[∆ f(t)](x ,a
)]=O(cid:32) C covlog(1+T)
+
γT log(|F||W|HTδ−1) +(cid:88)T
ξ
+γlog(T)(cid:33)
, (30)
d(t) h h h γ K t
h
t=2 t=1
Plugging this bound in to (26) for each h∈[H] gives the desired result.
E.3 Proof of Theorem 3.1
In this section, we prove a generalization of Theorem 3.1 that accounts for misspecification error when the
class W can only approximately realize the density ratios of mixed policies. Formally, we make the following
assumption on the class W:
Assumption 2.2† (Density ratio realizability, mixture version, with misspecification error). Let T be the
parameter to Glow (Algorithm 1). For all h ∈ [H], π ∈ Π, t ∈ [T], and π(1:t) = (π(1),...,π(t)) ∈ Π, there
exists a weight function
wπ;π(1:t)
(x,a)∈W such that
h h
(cid:13) (cid:13)
sup(cid:13)
(cid:13)
dπ
h
−wπ;π(1:t)(cid:13)
(cid:13) ≤ε .
π(cid:101)∈Π(cid:13) (cid:13)dπ h(1:t) h (cid:13)
(cid:13)
1,dπ h(cid:101)
apx
Note that setting ε =0 above recovers Assumption 2.2′ given in the main body.
apx
Theorem 3.1′. Let ε>0 be given, and suppose that Assumption 2.1 holds. Further, suppose that Assumption
2.2† (above) holds with ε
apx
≤ε/18H. Then, Glow, when executed on classes F and W with hyperparameters
T
=Θ(cid:101)(cid:0) (H2Ccov/ε2)·log(|F||W|/δ)(cid:1)
, K =1, and γ
=(cid:112)
Ccov/(Tlog(|F||W|/δ)) returns an ε-suboptimal policy π
(cid:98)
with
probability at least 1−δ after collecting
(cid:18) H2C (cid:19)
N =O(cid:101) cov log(|F||W|/δ) (31)
ε2
trajectories. In addition, for any T ∈N, with the same choice for K and γ as above, the algorithm enjoys a
regret bound of the form
T
Reg:= (cid:88) J(π⋆)−J(π(t))=O(cid:101)(cid:0) H(cid:112) C covT log(|F||W|/δ)+HTε apx(cid:1) . (32)
t=1
Clearly, setting the misspecification error ε =0 above recovers Theorem 3.1.
apx
Proof of Theorem 3.1′. First note that by combining Assumption 2.2† with the fact that clip [z] is
γ
1-Lipschitz for any γ >0, we have that for any h∈[H] and t∈[T], there exists a weight function w(t) ∈W
h h
such that
sup(cid:13) (cid:13) (cid:13) (cid:13)clip γ(t)(cid:20) d¯d (th( +t) 1)(cid:21) −clip γ(t)(cid:2) w h(t)(cid:3)(cid:13) (cid:13) (cid:13)
(cid:13)
≤ε apx. (33)
π∈Π h 1,dπ
h
Using this misspecification bound, and setting K =1 in Lemma E.4, we get that with probability at least
1−δ,
T (cid:18) (cid:19)
Reg=(cid:88)
J(π⋆)−J(π(t))=O
HC covlog(1+T)
+γHT log(6|F||W|HTδ−1)+HTε .
γ apx
t=1
32Further setting γ =(cid:112) Ccov/(Tlog(6|F||W|HTδ−1)) implies that
(cid:16) (cid:112) (cid:17)
Reg≤O H C T log(T)log(6|F||W|HTδ−1)+HTε .
cov apx
For the sample complexity bound, note that the returned policy π is chosen via π ∼Unif({π(1),...,π(T)}),
(cid:98) (cid:98)
and thus
T (cid:32) (cid:114) (cid:33)
E[J(π⋆)−J(π)]= 1 (cid:88) J(π⋆)−J(π(t))≤O H C cov log(T)log(6|F||W|HTδ−1)+Hε .
(cid:98) T T apx
t=1
(cid:16) (cid:17)
Hence, when ε apx ≤O(ε/H), setting T =Θ(cid:101) H2 εC 2cov log(6|F||W|HTδ−1) implies that the returned policy π (cid:98)
satisfies
E[J(π⋆)−J(π)]≤ε.
(cid:98)
The total number of trajectories collected to return an ε-suboptimal policy is given by
(cid:18) H2C (cid:19)
T ·K ≤O(cid:101) cov log(6|F||W|Hδ−1) .
ε2
E.4 Proof of Theorem 3.2
In this section, we prove a generalization of Theorem 3.2 in Theorem 3.2′ (below) which accounts for
misspecification error when the class W can only approximately realize the density ratios of pure policies.
Formally, we make the following assumption on the class W.
Assumption 2.2‡ (Density ratio realizability, with misspecification error). For any policy pair π ,π ∈Π
1 2
and h∈[H], there exists some weight function w(π1,π2) ∈W such that
h h
su πp(cid:13) (cid:13) (cid:13) (cid:13)d dπ h
π
h1
2
−w h(π1,π2)(cid:13) (cid:13) (cid:13)
(cid:13)
1,dπ
≤ε apx.
h
Setting ε =0 above recovers Assumption 2.2 given in the main body.
apx
Note that Assumption 2.2‡ only states that density ratios of pure policies are approximately realized by
W . On the other hand, the proof of Lemma E.4, our key tool in sample complexity analysis, requires
h
(approximate) realizability for the ratio d(t)/d¯(t+1) in W h, which involves a mixture of occupancies. We fix this
problem by running Glow on a larger class W that is constructed using W and has small misspecification
error for d(t)/d¯(t+1) for all t≤T. Before delving into the proof of Theorem 3.2′, we first describe the class W.
Construction of the class W. Define an operator Mixture that takes in a sequence of weight functions
{w(1),...,w(t)} and a parameter t ≤ T, and outputs a function [Mixture(w(1),...,w(t);t)] such that for any
x,a∈X ×A,
1
[Mixture(w(1),...,w(t);t)] h(x,a):=
E (cid:2)
w(s)(x,a)(cid:3).
s∼Unif([t]) h
Using the operator Mixture, we define W(t) via
W(t) ={Mixture(w(1),...,w(t);t)|w(1),...,w(t) ∈W},
33and then define
W =∪ W(t). (34)
t≤T
As a result of this construction, we have that
|W|≤(|W|+1)T ≤(2|W|)T. (35)
In addition, we define W =(cid:8) w |w ∈W(cid:9). The following lemma shows that W has small misspecification
h h
error for density ratios of mixture policies.
Lemma E.5. Let t ≥ 0 be given, and suppose Assumption 2.2‡ holds. For any sequence of policies
π(1),...,π(t) ∈Π, and h∈[H], there exists a weight function w¯ ∈W(t) such that for any γ >0,
h h
su πp(cid:13) (cid:13) (cid:13) (cid:13)clip γ(cid:20) d¯d
(
hth( +t) 1)(cid:21) −clip γ[w¯ h](cid:13) (cid:13) (cid:13)
(cid:13)
1,dπ
≤γ2ε apx,
h
where recall that d¯(t+1) = 1(cid:80)t d(s).
h t s=1 h
The following theorem, which is our main sample complexity bound under misspecification error, is obtained
by running Glow on the weight function class W.
Theorem 3.2′. Let ε>0 be given, and suppose that Assumption 2.1 holds. Further, suppose that Assumption
2.2‡ holds with ε apx ≤O(cid:101)(ε5/C c3 ovH5). Then, Glow, when executed on classes F and W (defined in Eq. (34))
(cid:112)
with hyperparameters T = Θ(cid:101)(H2Ccov/ε2), K = Θ(cid:101)(T log(|F||W|/δ)), and γ = Ccov/T returns an ε-suboptimal
policy π with probability at least 1−δ after collecting
(cid:98)
(cid:16)H4C2 (cid:17)
N =O(cid:101) cov log(|F||W|/δ) .
ε4
trajectories.
Setting the misspecification error ε =0 above recovers Theorem 3.2 in the main body.
apx
Proof of Theorem 3.2′. Using the misspecification bound from Lemma E.5 in Lemma E.4 implies that
with probability at least 1−δ,
(cid:88)T J(π⋆)−J(π(t))=O(cid:32) HC covlog(1+T)
+
γHT log(6|F|(cid:12) (cid:12)W(cid:12) (cid:12)HTδ−1)
+Hγ2T3ε
+γHlog(T)(cid:33)
.
γ K apx
t=1
Using the relation in Eq. (35), we get that |W|≤(2|W|)T and thus
(cid:88)T J(π⋆)−J(π(t))=O(cid:32) HC covlog(1+T)
+
γHT log(6|F|(cid:12) (cid:12)W(cid:12) (cid:12)HTδ−1)
+Hγ2T3ε
+γHlog(T)(cid:33)
.
γ K apx
t=1
(cid:113)
Setting K =2T log(6|F||W|HTδ−1) and γ = Ccov in the above bound, we get
T
T
(cid:88) (cid:16)(cid:16) (cid:112) (cid:17)(cid:17)
J(π⋆)−J(π(t))≤O H C T log(T)+HC T2ε .
cov cov apx
t=1
Finally, observing that the returned policy π ∼Unif({π(1),...,π(T)}), we get
(cid:98)
T (cid:32)(cid:32) (cid:114) (cid:33)(cid:33)
(cid:88) J(π⋆)−J(π(t))≤O H C cov log(T)+HC T2ε .
T cov apx
t=1
34Thus, when ε apx ≤O(cid:101)(ε5/C c3 ovH5), setting T =Θ(cid:101)(cid:0)C εc 2ov log2(C εc 2ov)(cid:1) in the above bound implies that
E[J(π⋆)−J(π)]≤ε.
(cid:98)
The total number of trajectories collected to return ε-suboptimal policy is given by:
(cid:18) H4C2 (cid:19)
T ·K =O cov log(|F||W|HTδ−1) .
ε4
Proof of Lemma E.5. Fix any h∈[H] and t∈[T]. Using Assumption 2.2‡, we have that for any s≤t,
there exists a function w(s,t) ∈W such that
h h
sup(cid:13) (cid:13) (cid:13) (cid:13)d d( h (s t)) −w h(s,t)(cid:13) (cid:13) (cid:13)
(cid:13)
≤ε apx. (36)
π∈Π h 1,dπ
h
Let w¯
h
=(cid:2) Mixture(w(1,t),...,w(t,t);t)(cid:3)
h
∈W h, and recall that d¯ (t+1) =E s∼Unif([t])(cid:2) d( hs)(cid:3). For any x,a∈X ×A,
define
(cid:12)   (cid:12)
(cid:12) (cid:12)  1  (cid:26) 1 (cid:27)(cid:12) (cid:12)
ζ(x,a):=(cid:12) (cid:12) (cid:12)min E s∼Unif([t])(cid:104) d( hs)(x,a)/d h(t)(x,a)(cid:105),γ −min E s∼Unif([t])[w(s,t)(x,a)],γ (cid:12) (cid:12) (cid:12).
Using that the function g(z)=min(cid:8)1,γ(cid:9) = 1 is γ2-Lipschitz, we get that
z max{z,γ−1}
ζ(x,a)≤γ2·(cid:12) (cid:12) (cid:12) (cid:12)E s∼Unif([t])(cid:20) d d( h (s t)) (( xx ,, aa ))(cid:21) −E s∼Unif([t])(cid:2) w h(s,t)(x,a)(cid:3)(cid:12) (cid:12) (cid:12)
(cid:12)
h
≤γ2E s∼Unif([t])(cid:20)(cid:12) (cid:12) (cid:12) (cid:12)d d( h (s t)) (( xx ,, aa )) −w h(s,t)(x,a)(cid:12) (cid:12) (cid:12) (cid:12)(cid:21) ,
h
where the last inequality follows from the linearity of expectation, and by using Jensen’s inequality.
Thus,
πsu ∈p ΠE xh,ah∼dπ h(cid:20)(cid:12) (cid:12) (cid:12) (cid:12)min(cid:26) d¯d
(
ht( h +t) 1( )x (xh h, ,a ah h) ),γ(cid:27) −min{w¯ h(x h,a h),γ}(cid:12) (cid:12) (cid:12) (cid:12)(cid:21)
= supE [ζ(x ,a )]
π∈Π
xh,ah∼dπ
h
h h
≤γ2supE s∼Unif([t])(cid:34)(cid:13) (cid:13) (cid:13) (cid:13)d d( h (s t)) −w h(s,t)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:35)
π∈Π h 1,dπ
h
≤γ2E s∼Unif([t])sup(cid:34)(cid:13) (cid:13) (cid:13) (cid:13)d d( h (s t)) −w h(s,t)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:35)
π∈Π h 1,dπ
h
≤γ2ε ,
apx
wherethesecond-to-lastlineisduetoJensen’sinequality,andthelastlinefollowsfromtheboundin(36).
35F Proofs and Additional Results from Section 4 (Hybrid RL)
This section is organized as follows:
• Appendix F.1 gives additional details and further examples of offline algorithms with which we can
apply H O, including an example that uses FQI as the base algorithm.
2
• Appendix F.2 contains the proof of the main result for H O, Theorem 4.1.
2
• Appendix F.3 contains supporting proofs for the offline RL algorithms (Appendix F.1) we use within
H O.
2
F.1 Examples for H O
2
This section contains additional examples of base algorithms that can be applied within the H O reduction.
2
F.1.1 HyGlow Algorithm
Forcompleteness,westatefullpseudocodefortheHyGlowalgorithmdescribedinSection4.2asAlgorithm3.
As described in the main body, this algorithm simply invokes H O with a clipped and regularized variant of
2
the Mabo algorithm (Mabo.cr, Eq. (13)) as the base algorithm.
We will invoke Mabo.cr with a certain augmented weight function class W, which we now define. For any
w ∈W, let w(h) ∈(X ×A×[H]→R∪{+∞}) be defined by
(cid:40)
w (x,a) if h=h′
w(h)(x,a):= h , (37)
h′ 0 if h̸=h′
and let W(h) ={w(h) |w ∈W}. Define the set
W :=W ∪(−W)∪ (W(h)∪(−W(h))). (38)
h∈[H]
Note that the size satisfies W is (cid:12) (cid:12)W(cid:12) (cid:12)≤2(H +1)|W|≤4H|W|.
We recall that the Mabo.cr algorithm with dataset D and parameters γ,F,W is defined via
f(cid:98)∈ar fg ∈m Fin wm ∈a Wx(cid:88)H (cid:12) (cid:12) (cid:12)E (cid:98)Dh(cid:104) wˇ h(x h,a h)[∆(cid:98)hf](x h,a h,r h,x′ h+1)(cid:105)(cid:12) (cid:12) (cid:12)−α(n)E (cid:98)Dh(cid:2) wˇ h2(x h,a h)(cid:3) , (39)
h=1
where α(n) :=8/γ(n) and wˇ
h
:=clip γ(n)[w h].
In Appendix F.3.2 we will prove the following result.
Theorem 4.2 (Mabo.crisCC-bounded). Let D ={D }H consist of H·n samples from µ(1),...,µ(n). For
h h=1
any γ ∈R , the Mabo.cr algorithm (Eq. (13)) with parameters F, augmented class W defined in Eq. (38)
+
in Appendix F.1.1, and γ is CC-bounded at scale γ under the Assumption that Q⋆ ∈F and that for all π ∈Π
and h∈[H], dπ h/µ( h1:n) ∈W.
The risk bound for HyGlow will follow.
Corollary 4.1 (Risk bound for HyGlow). Let ε > 0 be given, let D consist of H ·T samples from
off
data distribution ν, and suppose that ν satisfies C -single-policy concentrability. Suppose that Q⋆ ∈ F
⋆
and that for all t ∈ [T], π ∈ Π, and h ∈ [H], we have dπ h/µ h(t) ∈ W, where µ( ht) := 1/2(ν
h
+1/t(cid:80)t i=1dπ h(i)).
Then, HyGlow with inputs T = Θ(cid:101)((H4(Ccov+C⋆)/ε2)·log(|F||W|/δ)), F, augmented W defined in Eq. (38),
(cid:16)(cid:112) (cid:17)
γ =Θ(cid:101) (C⋆+Ccov)/TH2log(|F||W|/δ) , and D
off
returns an ε-suboptimal policy with probability at least 1−δT
after collecting
(cid:18) H2(C +C ) (cid:19)
N =O(cid:101) cov ⋆ log(|F||W|/δ)
ε2
trajectories.
36F.1.2 Computationally efficient implementation for HyGlow
In the following, we expand on the discussion after Theorem 4.2 regarding computationally efficient imple-
mentation of the optimization problem in Eq. (13) and Eq. (14) via reparameterization. Let γ >0 and n>0
be given to the learner, and suppose that in addition to the class W, the learner has access to a function
class W(γ,n) that satisfies the following assumption.
Assumption F.1. The function class W(γ,n) satisfies
(a) For all w ∈W(γ,n) and h∈[H], ∥w ∥ ≤γn.
h ∞
(b) For all h∈[H], {±clip [w]|w ∈W}⊆W(γ,n).
γn
(c) For all h∈[H], {±clip [w(h)]|w ∈W}⊆W(γ,n), where w(h) is defined as in Eq. (37).
γn
Note that W(γ,n) also satisfies the density ratio realizability required by Mabo.cr (cf. Theorem 4.2). We
claim that optimizing directly over the class W(γ,n), which does not involve explicitly clipping, leads to the
same guarantee as solving Eq. (13). In more detail, consider the following offline RL algorithm, which given
offline datasets {D } , returns
h h≤H
H
f(cid:98)∈ar fg ∈m Fin w∈m Wa (x γ,n)(cid:88) E (cid:98)Dh(cid:104) w h(x h,a h)[∆(cid:98)hf](x h,a h,r h,x′ h+1)(cid:105) −α(n)E (cid:98)Dh(cid:2) w h2(x h,a h)(cid:3) . (40)
h=1
Using the next lemma, we show that the function obtained by solving Eq. (40) leads to the same offline RL
guarantee as Mabo.cr when |log(W(γ,n))|=O(log(|W|)+poly(H)). In particular, substituting the bound
from Lemma F.1 in place of the corresponding bound from Lemma F.6 in the proof of Theorem 4.2, while
keeping rest of the analysis same, shows that the above described computationally efficient implementation of
Mabo.cr is also CC-bounded. Using this fact with Theorem 4.1 implies the desired performance guarantee
for HyGlow (similar to Corollary 4.1).
Lemma F.1. Suppose F and W satisfy Assumption 2.1 and Assumption F.4. Additionally, let γ ∈(0,1) and
n>0 be given constants, and for h∈[H], let D be datasets of size n sampled from the offline distribution
h
µ(1:n). Furthermore, let W(γ,n) be a reparameterized function class that satisfies Assumption F.1 w.r.t. W.
h
Then, the function f(cid:98)returned by Eq. (40), when executed with datasets {D h} h≤H, weight function class
W(γ,n), and parameters α(n) = 8 , satisfies with probability at least 1−δ,
γn
(cid:88)H (cid:12)
(cid:12) (cid:12)E
µ(1:n)(cid:104)
[∆ hf(cid:98)](x h,a h)·wˇ h(x h,a
h)(cid:105)(cid:12)
(cid:12)
(cid:12)≤O(cid:32) (cid:88)H γ1
nE
µ(1:n)(cid:104)
(wˇ h(x h,a
h))2(cid:105) +H2β(n)(cid:33)
,
h h
h=1 h=1
(cid:16) (cid:17)
for all w ∈W, where β(n) =O γn log(24|F||W|H2/δ) .
n−1
Proof of Lemma F.1. Repeating the same arguments as in the proof of Lemma E.2 with K = 1 (we
avoid repeating the arguments for conciseness), along with the fact that ∥w′∥ ≤ γn for all h ∈ [H] and
h ∞
w′ ∈W(γ,n), we get that the returned function f(cid:98)satisfies for all w′ ∈W(γ,n),
H (cid:32) H (cid:33)
(cid:88)
E
µ(1:n)(cid:104)
[∆ hf(cid:98)](x h,a h)·w h′(x h,a
h)(cid:105)
≤O
(cid:88) γ1
nE
µ(1:n)(cid:104)
(w h′(x h,a
h))2(cid:105)
+
nH −γn
1log(|F||W(γ,n)|H/δ) .
h h
h=1 h=1
However, as in the proof of Lemma F.6, since for any w ∈ W we have that both clip [w(h)] ∈ W(γ,n) and
γn h
−clip [w(h)]∈W(γ,n) for every h∈[H], the above inequality immediately implies that for every w ∈W,
γn h
(cid:88)H (cid:12)
(cid:12) (cid:12)E
µ(1:n)(cid:104)
[∆ hf(cid:98)](x h,a h)·wˇ h(x h,a
h)(cid:105)(cid:12)
(cid:12)
(cid:12)≤O(cid:32) (cid:88)H γ1
nE
µ(1:n)(cid:104)
(wˇ h(x h,a
h))2(cid:105)
+
H n2 −γ 1n log(|F||W(γ,n)|H/δ)(cid:33)
,
h h
h=1 h=1
where we used that the RHS is independent of the sign. The final statement follows by plugging in that
|log(W(γ,n))|=O(log(|W|)+poly(H)).
37F.1.3 Fitted Q-Iteration
In this section, we apply H O with the Fitted Q-Iteration (Fqi) algorithm (Munos, 2007; Munos and
2
Szepesvári, 2008; Chen and Jiang, 2019) as the base algorithm. For an offline dataset D and value function
class F, the Fqi algorithm is defined as follows:
Algorithm F.1 (Fitted Q-Iteration (Fqi)).
1. Set f(cid:98)H+1(x,a)=0 for all (x,a)
2. For h=H,...,1:
(cid:104) (cid:105)
f(cid:98)h ∈a fr hg ∈m FhinE (cid:98)Dh (f h(x h,a h)−r h−m aa ′xf(cid:98)h+1(x h+1,a′)))2 .
3. Output π =π .
(cid:98) f(cid:98)
We analyze Fqi under the following standard Bellman completeness assumption.
Assumption F.2 (Bellman completeness). For all h∈[H], we have that
T F ⊆F
h h+1 h
Theorem F.2. The Fqi algorithm is CC-bounded under Assumption F.2 with scaling functions a = 6 and
γ γ
b =1024log(2n|F|)γ, for all γ >0 simultaneously. As a consequence, when invoked within H O, we have
γ 2
Risk≤O(cid:101)(cid:0) H(cid:112) (C⋆+Ccov)log(|F|δ−1)/T(cid:1)
with probability at least 1−δT .
The proof can be found in Appendix F.3.3.
The full pseudocode for H O with Fqi is essentially identical to the Hy-Q algorithm of Song et al. (2023),
2
except for a slightly different data aggregattion strategy in Line 8. Thus, Hy-Q can be interpreted as a
special case of the H O algorithm when instantiated with Fqi as a base algorithm. The risk bounds in Song
2
et al. (2023) are proven under an a structural condition known as bilinear rank (Du et al., 2021), which
is complementary to coverability. Our result recovers a special case of a risk bound for Hy-Q given in the
follow-up work of Liu et al. (2023), which analyzes Hy-Q under coverability instead of bilinear rank.
F.1.4 Model-Based MLE
In this section we apply H O with Model-Based Maximum Likelihood Estimation (MLE) algorithm as the
2
base algorithm. The algorithm is parameterized by a model class M = {M }H , where M ⊂ {M :
h h=1 h h
X ×A → ∆(R×X)}. Each model M = {M }H ∈ M has the same state space, action space, initial
h h=1
distribution, and horizon, and each M ∈M is a conditional distribution over rewards and next states for
h h
layer h. For a dataset D, the algorithm proceeds as follows.
Algorithm F.3 (Model-Based MLE).
• For h∈[H]:
– Compute the maximum likelihood estimator for layer h as
(cid:88)
M(cid:99)h =argmax log(M h(r h,x
h+1
|x h,a h)) (41)
Mh∈Mh
(xh,ah,rh,xh+1)∈Dh
• Output π⋆ , the optimal policy for M(cid:99)={M(cid:99)h}
h
M(cid:99)
We analyze Model-Based MLE under a standard realizability assumption.
Assumption F.3 (Model realizability). We have that M⋆ ∈M.
38Theorem F.4. The model-based MLE algorithm is CC-bounded under Assumption F.3 for all γ > 0
simultaneously, with scaling functions a = 6 and b =8log(|M|H/δ)γ. As a consequence, when invoked
γ γ γ
within H 2O, we have Risk≤O(cid:101)(cid:0) H(cid:112) (C⋆+Ccov)log(|M|Hδ−1)/T(cid:1) with probability at least 1−δT.
The proof can be found in Appendix F.3.4.
F.2 Proofs for H O (Theorem 4.1)
2
The following theorem is a slight generalization of Theorem 4.1. In the sequel, we prove Theorem 4.1 as a
consequence of this result.
Theorem F.5. Let T ∈N be given, let D consist of H ·T samples from data distribution ν. Let Alg
off off
be CC-bounded at scale γ ∈ [0,1] under Assumption(·), with parameters a and b . Suppose that for all
γ γ
t∈[T] and π(1),...,π(t) ∈Π, Assumption(µ(t),M⋆) holds for µ(t) :={1/2(ν h+1/t(cid:80)t i=1dπ h(i))}H h=1. Then, with
probability at least 1−δT, the risk of H O (Algorithm 2) with inputs T, Alg , and D is bounded as
2 off off
H T (cid:18) (cid:18) (cid:19)(cid:19)
Risk≤
2 Ta
γ
(cid:88)(cid:88)1
tCC h(π⋆,ν,γ(t))+O(cid:101) H
C
c
Nova
γ +b
γ
. (42)
h=1t=1
(cid:124) (cid:123)(cid:122) (cid:125)
=:erroff
Proof of Theorem F.5. Recall the definitions d(t) := dπ(t), d(cid:101)(t) := (cid:80)t−1d(t), and d¯(t) := 1 d(cid:101)(t).
h h h s=1 h h t−1 h
Furthermore, letd⋆ :=dπ⋆ forallh≤H. NotethatthedatadistributionforD(t) isµ(t) ={µ(t)}H where
h h hybrid h h=1
µ(t) = 1(ν +d¯(t)), where ν is the offline distribution. As a result of Definition 4.3, the offline algorithm
h 2 h h h
Alg invoked on the dataset D(t) outputs a distribution p ∼∆(Π) that satisfies the bound:
off hybrid t
H
E [J(π⋆)−J(π(t))]≤(cid:88) a γ (cid:0) CC (π⋆,µ(t),γt)+E [CC (π(t),µ(t),γt)](cid:1) +b , (43)
π(t)∼pt t h π(t)∼pt h γ
h=1
with probability at least 1−δ, where a and b are the scaling functions for which Alg is CC-bounded at
γ γ off
scale γ. By taking a union bound over T, the number of iterations, we have that the event in Eq. (43) occurs
for all t≤T with probability greater than 1−δT.
Plugging in the definition for the clipped concentrability coefficient above and summing over t from 1,...,T,
we get that
(cid:34) T (cid:35)
(cid:88)
Reg=E J(π⋆)−J(π(t))
t=1
≤(cid:88)H (cid:32) (cid:88)T a tγ (cid:13) (cid:13) (cid:13) (cid:13)clip γt(cid:20) d µπ h (t⋆ )(cid:21)(cid:13) (cid:13) (cid:13)
(cid:13)
+(cid:88)T a tγ E π(t)∼pt(cid:34)(cid:13) (cid:13) (cid:13) (cid:13)clip γt(cid:20) µd h( (t t) )(cid:21)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:35)(cid:33) +HTb γ.
h=1 t=1 h 1,d⋆ h t=1 h 1,d( ht)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(I) (II)
For each h∈[H], we bound the two terms I and II separately below.
Term (I). Note that µ(t)(x,a)≥ν (x,a)/2 for any x,a. Thus,
h h
(I)≤2a γ(cid:88) t=T
1
1
t
(cid:13) (cid:13) (cid:13) (cid:13)clip γt(cid:20) d νπ h h⋆(cid:21)(cid:13) (cid:13) (cid:13)
(cid:13) 1,dπ h⋆
=2a γ(cid:88) t=T
1
1 tCC h(π⋆,ν,γt).
39Term (II). We bound this term uniformly for any π(t) ∼p . So, fix π(t) and note that
t
(cid:88)T 1 (cid:20) (cid:26) d(t)(x,a) (cid:27)(cid:21)
(II)=a E min h ,γt
γ t d(t) µ(t)(x,a)
t=1 h h
(cid:32) T (cid:20) (cid:26) (cid:27)(cid:21) T (cid:20) (cid:26) (cid:27)(cid:21)(cid:33)
(cid:88)1 d(t)(x,a) d(t)(x,a) (cid:88)1 d(t)(x,a)
=a E I ≤γt + E γt·I >γt ,
γ t d(t) µ(t)(x,a) µ(t)(x,a) t d(t) µ(t)(x,a)
t=1 h h h t=1 h h
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(II.A) (II.B)
where the second line holds since min{u,v}=uI{u≤v}+vI{v <u} for all u,v ∈R.
In order to bound the two terms appearing above, we use certain properties of coverability, similar to the
analysis of Glow (Appendix E). For a parameter λ∈(0,1), let us define a burn-in time
(cid:26)
C
·µ⋆(x,a)(cid:27)
τ(λ)(x,a)=min t|d(cid:101)(t)(x,a)≥ cov h , (44)
h h λ
and observe that
T
(cid:88) E [I(cid:8) t<τ(λ)(x,a)(cid:9) ]=(cid:88) (cid:88) d(t)(x,a)≤ 2C cov, (45)
d(t) h h λ
h
t=1 x,a t<τ(λ)(x,a)
h
which holds for any λ∈(0,1). This bound can be derived by noting that
(cid:88) (cid:88) d( ht)(x,a)=(cid:88) d˜(τh(λ)(x,a))(x,a)
x,a t<τ(λ)(x,a) x,a
h
=(cid:88) d˜(τh(λ)(x,a)−1)(x,a)+(cid:88) d(τh(λ)(x,a))(x,a)
x,a x,a
≤(cid:88)C covµ⋆(x,a)+(cid:88)
C µ⋆(x,a)
λ h cov h
x,a x,a
(cid:18) (cid:19)
1 2C
≤C +1 ≤ cov.
cov λ λ
We also recall the follow bound, which is a corollary of the elliptical potential lemma (Lemma D.5):
(cid:88)T (cid:88) d(t)(x,a)d( ht)(x,a) I(cid:8) t>τ(λ)(x,a)(cid:9) ≤(cid:88)T (cid:88) d(t)(x,a)d h(t)(x,a) I(cid:8) t>τ(1)(x,a)(cid:9)( ≤i) 5log(T)C . (46)
h h h h cov
d(cid:101)(t)(x,a) d(cid:101)(t)(x,a)
t=1 x,a t=1 x,a
The inequality (i) can be seen derived by noting that, under the event in the indicator, we have d(cid:101)(t)(x,a)≥
h
C covµ⋆ h(x,a) and thus d(cid:101)( ht)(x,a)≥ 1 2(C covµ⋆ h(x,a)+d(cid:101)(t)(x,a)). This gives
(cid:88)T (cid:88) d(t)(x,a)d( ht)(x,a) I(cid:8) t>τ(1)(x,a)(cid:9) ≤2(cid:88)T (cid:88)
d(t)(x,a)
d( ht)(x,a)
,
t=1 x,a
h d(cid:101)(t)(x,a) h
t=1 x,a
h d(cid:101)(t)(x,a)+C covµ⋆ h(x,a)
from which we can repeat the steps from Eq. (23) to Eq. (24).
Term (II.A). To bound this term, we introduce a split according to the burn-in time τ(1)(x,a), i.e.
h
(II.A)=(cid:88)T 1 E (cid:20) d( ht)(x,a) I(cid:26) d( ht)(x,a) ≤γt(cid:27) (cid:0)I(cid:8) t≤τ(1)(x,a)(cid:9) +I(cid:8) t>τ(1)(x,a)(cid:9)(cid:1)(cid:21) .
t d(t) µ(t)(x,a) µ(t)(x,a) h h
t=1 h h h
40The first term is bounded via
(cid:88)T 1 E (cid:20) d( ht)(x,a) I(cid:26) d( ht)(x,a) ≤γt(cid:27) I(cid:8) t≤τ(1)(x,a)(cid:9)(cid:21) ≤γ(cid:88)T E (cid:2)I(cid:8) t≤τ(1)(x,a)(cid:9)(cid:3)
t d(t) µ(t)(x,a) µ(t)(x,a) h d(t) h
t=1 h h h t=1 h
≤2γC ,
cov
by Eq. (45) with λ=1. The second term is bounded via:
(cid:88)T 1 E (cid:20) d( ht)(x,a) I(cid:26) d(t)(x,a) ≤γt(cid:27) I(cid:8) t>τ(1)(x,a)(cid:9)(cid:21) ≤2(cid:88)T 1 E (cid:20) d( ht)(x,a) I(cid:8) t>τ(1)(x,a)(cid:9)(cid:21)
t d(t) µ(t)(x,a) µ(t)(x,a) h t d(t) d¯(t)(x,a) h
t=1 h h h t=1 h h
≤2(cid:88)T
E
(cid:34) d( ht)(x,a)
I(cid:8)
t>τ(1)(x,a)(cid:9)(cid:35)
d( ht) d(cid:101)(t)(x,a) h
t=1 h
≤10log(T)C ,
cov
by using that µ(t)(x,a)≥
d¯( ht)(x,a)
and Equation (46).
h 2
Adding these two terms together gives us the upper bound (II.A)≤2γC +10log(T)C .
cov cov
Term (II.B). We have
(cid:88)T 1 (cid:20) (cid:26) d(t)(x,a) (cid:27)(cid:21) (cid:88)T (cid:20) (cid:26) d(t)(x,a) (cid:27)(cid:21)
E γtI h >γt =γ E I h >γt
t d(t) µ(t)(x,a) d(t) µ(t)(x,a)
t=1 h h t=1 h h
( ≤i) γ(cid:88)T
E
(cid:34) I(cid:40) C covµ⋆ h(x,a)
>
γ(cid:41)(cid:35)
d h(t) d(cid:101)(t)(x,a) 2
t=1 h
T
( =ii) γ(cid:88) E (cid:2)I(cid:8) t≤τ(γ/2)(x,a)(cid:9)(cid:3)
d(t) h
h
t=1
4C
≤γ· cov =4C ,
γ cov
where the inequality (i) follows from applying the upper bounds d(t)(x,a) ≤ C µ⋆(x,a) and µ(t)(x,a) ≥
h cov h h
1d¯(t)(x,a), and the inequality (ii) follows from the definition of the burn-in time (Eq. (44)) with λ=γ/2.
2 h
Combining all the bounds above, we get that
(II)≤2a (2γC +10log(T)C +2C )
γ cov cov cov
=4a C (γ+10log(T)+1).
γ cov
Adding together the terms so far, we can conclude the regret bound:
H T
(cid:88)(cid:88)1
Reg≤2a CC (π⋆,ν,γt)+H(4a C (γ+10log(T)+1)+Tb ).
γ t h γ cov γ
h=1t=1
It follows that the policy π =Unif(π(1),...,π(T)) satisfies the risk bound
(cid:98)
H T (cid:18) (cid:19)
Risk≤
2a
γ
(cid:88)(cid:88)1
CC (π⋆,ν,γt)+H
4a γC
cov (γ+10log(T)+1)+b
T t h T γ
h=1t=1
(cid:124) (cid:123)(cid:122) (cid:125)
:=erroff
H T (cid:18) (cid:18) (cid:19)(cid:19)
=
2 Ta
γ
(cid:88)(cid:88)1
tCC h(π⋆,ν,γt)+O(cid:101) H
C
co
Tva
γ +b
γ
,
h=1t=1
41with probability at least 1−δT, where in the last line we have used that γ ∈[0,1].
We now prove Theorem 4.1 as a consequence of Theorem F.5.
Theorem 4.1 (Risk bound for H O). Let T ∈ N be given, let D consist of H ·T samples from data
2 off
distribution ν, and suppose that ν satisfies C -single-policy concentrability. Let Alg be CC-bounded at scale
⋆ off
γ ∈(0,1) under Assumption(·), with parameters a and b . Suppose that for all t∈[T] and π(1),...,π(t) ∈Π,
γ γ
Assumption(µ(t),M⋆) holds for µ(t) := {1/2(ν h+1/t(cid:80)t i=1dπ h(i))}H h=1. Then, with probability at least 1−δT,
the risk of H O (Algorithm 2) with inputs T, Alg , and D is bounded as
2 off off
(cid:18) (cid:18) (cid:19)(cid:19)
a (C +C )
Risk≤O(cid:101) H γ ⋆
T
cov +b
γ
. (12)
Proof of Theorem 4.1. Under the assumptions in the theorem statement, Theorem F.5 implies that
H T (cid:18) (cid:19)
Risk≤
2a
γ
(cid:88)(cid:88)1
CC (π⋆,ν,γt)+H
4a γC
cov (γ+10log(T)+1)+b .
T t h T γ
h=1t=1
Since max hmax x,a(cid:12) (cid:12) (cid:12)d νπ h h⋆ (( xx ,, aa ))(cid:12) (cid:12) (cid:12)≤C ⋆, we have CC h(π⋆,ν,γt)≤C ⋆, so we can simplify the first term above to
H T T
2a
γ
(cid:88)(cid:88)1
CC (π⋆,ν,γt)≤
2a γC ⋆H(cid:88)1
≤
6a γC
⋆Hlog(T),
T t h T t T
h=1t=1 t=1
using the bound on the harmonic number (cid:80)T t=11/t ≤ 3log(T). Combining with the remainder of the risk
bound in Theorem F.5 gives us
(cid:18) (cid:18) (cid:19)(cid:19)
Ha a (C +C )
Risk≤ Tγ(6C ⋆log(T)+4C cov(γ+10log(T)+1))+Hb
γ
=O(cid:101) H γ ⋆
T
cov +b
γ
,
where we have used the fact that γ ∈[0,1].
Corollary F.1. Let T ∈N and D consist of H ·T samples from data distribution ν. Suppose that for all
off
t∈[T] and π(1),...,π(t) ∈Π, Assumption(µ(t),M⋆) holds for µ(t) :={1/2(ν h+1/t(cid:80)t i=1dπ h(i))}H h=1. Let Alg
off
be CC-bounded at scale γ ∈[0,1] under Assumption(·) and with parameters a = a and b =bγ. Consider
γ γ γ
the H O algorithm with inputs T, Alg , and D . Then,
2 off off
• If Alg
off
is CC-bounded at scale γ
=Θ(cid:101)(cid:16)(cid:112) aCcov/bT(cid:17)
and T is such that γ ∈[0,1], we have
(cid:114) H T (cid:32) (cid:114) (cid:33)
Risk≤2
TCab (cid:88)(cid:88)1
tCC h(π⋆,ν,γ(t))+O(cid:101) H
C
co
Tvab
.
cov
h=1t=1
with probability greater than 1−δT.
• If ν satisfies C ⋆-single-policy concentrability and Alg
off
is CC-bounded at scale γ
=Θ(cid:101)(cid:16)(cid:112) a(C⋆+Ccov)/bT(cid:17)
and T is such that γ ∈[0,1], then
(cid:16) (cid:112) (cid:17)
Risk≤O(cid:101) H (C⋆+Ccov)ab/T ,
with probability greater than 1−δT.
Proof of Corollary F.1. We start with the first case. We recall the definition of err appearing in
off
Theorem F.5:
(cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:19)
4a C C a
err
off
:=H γ
T
cov (γ+8log(T)+1)+b
γ
=O(cid:101) H co Tv γ +b
γ
.
42and the risk bound from Theorem F.5.
H T
Risk≤
2a
γ
(cid:88)(cid:88)1
CC (π⋆,ν,γ(t))+err . (47)
T t h off
h=1t=1
Plugging in a = a and b =bγ into err gives
γ γ γ off
(cid:18) (cid:19)
a
err =H 4 C (γ+8log(T)+1)+bγ .
off γT cov
(cid:113)
The above is optimized by picking γ =2 aCcov(8log(T)+1). Plugging this in gives us
b T
(cid:32) (cid:114) (cid:33) (cid:32) (cid:114) (cid:33)
abC (8log(T)+1) aC abC
err
off
=H 4 cov
T
+4 Tcov =O(cid:101) H Tcov ,
as desired. For the second result, recall from Theorem 4.1 that the risk bound when ν satisfied C -policy-
⋆
concentrability is
Ha
Risk≤ γ(6C log(T)+4C (γ+8log(T)+1))+Hb .
T ⋆ cov γ
Plugging in a = a and b =bγ gives us
γ γ γ
Ha
Risk≤ (6C log(T)+4C (γ+8log(T)+1))+Hbγ.
Tγ ⋆ cov
(cid:113)
This expression is optimized by γ = a(6C⋆log(T)+4Ccov(γ+8log(T)+1)), which when substituted gives us the
Tb
risk bound
(cid:114) (cid:32) (cid:32)(cid:114) (cid:33)(cid:33)
ab(6C log(T)+4C (γ+8log(T)+1)) (C +C )ab
Risk≤2H ⋆ cov =O(cid:101) H ⋆ cov ,
T T
as desired.
F.3 Proofs for H O Examples (Appendix F.1)
2
F.3.1 Supporting Technical Results
Lemma F.2 (Telescoping Performance Difference (Xie and Jiang (2020, Theorem 2); Jin et al. (2021b,
Lemma 3.1))). For any f ∈F, we have that
H
(cid:88)
J(π⋆)−J(π f)≤ E dπ h⋆[T hf h+1(x h,a h)−f h(x h,a h)]+E dπ hf[f h(x h,a h)−T hf h+1(x h,a h)].
h=1
This bound follows from a straightforward adaptation of the proof of Xie and Jiang (2020, Theorem 2) to the
finite horizon setting.
Lemma F.3. For all policy π ∈Π, value function f ∈F, timestep h∈[H], data distribution µ={µ }H
h h=1
where µ ∈∆(X ×A), and γ ∈R , we have
h +
(cid:20) (cid:20) dπ(x ,a )(cid:21)(cid:21) (cid:20) dπ(x ,a ) (cid:21)
E [[∆ f](x ,a )]≤E [∆ f](x ,a )·clip h h h +2Pπ h h h >γ .
dπ h h h h µh h h h γ µ h(x h,a h) µ h(x h,a h)
Similarly,
(cid:20) (cid:20) dπ(x ,a )(cid:21)(cid:21) (cid:20) dπ(x ,a ) (cid:21)
E [−[∆ f](x ,a )]≤E (−[∆ f](x ,a ))·clip h h h +2Pπ h h h >γ ,
dπ h h h h µh h h h γ µ h(x h,a h) µ h(x h,a h)
where recall that [∆ f](x,a):=f (x,a)−[T f ](x,a).
h h h h+1
43Proof of Lemma F.3. Inthefollowing,weprovethefirstinequality. Thesecondinequalityfollowssimilarly.
Using that |[∆ f](x,a)|≤1 for any x,a∈X ×A, we have
h
E [[∆ f](x ,a )]≤E [[∆ f](x ,a )·I{µ (x ,a )̸=0}]+E [I{µ (x ,a )=0}].
dπ h h h dπ h h h h h h dπ h h h
h h h
For the second term, for any γ >0,
(cid:20) (cid:26) dπ(x ,a ) (cid:27)(cid:21) (cid:20) dπ(x ,a ) (cid:21)
E [I{µ (x ,a )=0}]≤E I h h h >γ =Pπ h h h >γ .
dπ h h h h dπ h µ h(x h,a h) µ h(x h,a h)
For the first term, using that u≤min{u,v}+uI{u≥v} for all u,v ≥0, and that |[∆ f](x,a)|≤1 for any
h
x,a∈X ×A, we get that
E [[∆ f](x ,a )·I{µ (x ,a )̸=0}]
dπ h h h h h h
h
(cid:20) dπ(x ,a ) (cid:21)
=E [∆ f](x ,a )· h h h I{µ(x ,a )̸=0}
µh h h h µ (x ,a ) h h
h h h
(cid:20) (cid:20) dπ(x ,a )(cid:21) (cid:21)
≤E [∆ f](x ,a )·clip h h h I{µ (x ,a )̸=0}
µh h h h γ µ (x ,a ) h h h
h h h
(cid:20) dπ(x ,a ) (cid:26) dπ(x ,a ) (cid:27) (cid:21)
+E h h h I h h h >γ I{µ (x ,a )̸=0}
µh µ (x ,a ) µ (x ,a ) h h h
h h h h h h
(cid:20) (cid:20) dπ(x ,a )(cid:21) (cid:21) (cid:20) (cid:26) dπ(x ,a ) (cid:27)(cid:21)
=E [∆ f](x ,a )·clip h h h I{µ (x ,a )̸=0} +E I h h h >γ .
µh h h h γ µ h(x h,a h) h h h dπ h µ h(x h,a h)
Furthermore, also note that
(cid:20) (cid:20) dπ(x ,a )(cid:21) (cid:21)
E [∆ f](x ,a )·clip h h h I{µ (x ,a )=0}
µh h h h γ µ (x ,a ) h h h
h h h
=
(cid:88)
µ (x ,a )·[∆ f](x ,a )·clip
(cid:20) dπ h(x h,a h)(cid:21)
=0.
h h h h h h γ µ (x ,a )
h h h
(xh,ah) s.t. µh(xh,ah)=0
The final bound follows by combining the above three terms.
Lemma F.4. For any policy π, data distribution µ = {µ }H where µ ∈ ∆(X ×A), scale γ ∈ R and
h h=1 h +
horizon h∈[H], we have
Pπ(cid:20) dπ h(x h,a h) >γ(cid:21)
≤
2 (cid:13) (cid:13)
(cid:13)clip
(cid:20) dπ h(cid:21)(cid:13) (cid:13)
(cid:13) .
µ (x ,a ) γ (cid:13) γ µ (cid:13)
h h h h 1,dπ
h
Proof of Lemma F.4. Note that
(cid:20) dπ(x ,a ) (cid:21) (cid:20) (cid:26) dπ(x ,a ) (cid:27)(cid:21)
Pπ h h h >γ =E I h h h >γ
µ h(x h,a h)
dπ
h µ h(x h,a h)
(cid:20) (cid:26) dπ(x ,a ) γ(cid:27)(cid:21)
≤E I h h h >
dπ h µ h(x,a)+γ−1dπ h(x h,a h) 2
2 (cid:20) dπ(x ,a ) (cid:21)
≤ E h h h
γ dπ h µ h(x h,a h)+γ−1dπ h(x h,a h)
≤ γ2 E dπ h(cid:20) clip γ(cid:20) µdπ h h( (x xh h, ,a ah h) )(cid:21)(cid:21) = γ2 (cid:13) (cid:13) (cid:13) (cid:13)clip γ(cid:20) µdπ h h(cid:21)(cid:13) (cid:13) (cid:13)
(cid:13)
1,dπ,
h
where the first inequality follows from
1
γ−1dπ(x,a)>µ (x,a) =⇒ γ−1dπ(x,a)> (µ (x,a)+γ−1dπ(x,a)),
h h h 2 h h
and the second inequality follows by Markov’s inequality.
44Lemma F.5. For any policy π, data distribution µ={µ }H where µ ∈∆(X ×A), scale γ ∈R , and
h h=1 h +
horizon h∈[H], we have
(cid:13)
(cid:13)
(cid:13)clip
(cid:20) dπ h(cid:21)(cid:13)
(cid:13)
(cid:13)2 ≤2(cid:13)
(cid:13)
(cid:13)clip
(cid:20) dπ h(cid:21)(cid:13)
(cid:13)
(cid:13) .
(cid:13) γ µ (cid:13) (cid:13) γ µ (cid:13)
h 2,µh h 1,dπ
h
Proof of Lemma F.5. Beginning with the left-hand side, we have,
(cid:34) (cid:26) dπ(x ,a ) (cid:27)2(cid:35) (i) (cid:34)(cid:18) dπ(x ,a )(cid:19)2 (cid:26) dπ(x ,a ) (cid:27)(cid:35)
E min h h h ,γ ≤ 2E h h h I h h h ≤γ
µh µ (x ,a ) µh µ (x ,a ) µ (x ,a )
h h h h h h h h h
(cid:20) dπ(x ,a ) (cid:26) dπ(x ,a ) (cid:27)(cid:21)
+2E γ· h h h ·I h h h >γ
µh µ (x ,a ) µ (x ,a )
h h h h h h
(ii) (cid:20) dπ(x ,a ) (cid:26) dπ(x ,a ) (cid:27)(cid:21) (cid:20) (cid:26) dπ(x ,a ) (cid:27)(cid:21)
≤ 2E h h h I h h h ≤γ +2E γ·I h h h >γ
dπ
h µ h(x h,a h) µ h(x h,a h)
dπ
h µ h(x h,a h)
(iii) (cid:20) (cid:26) dπ(x ,a ) (cid:27)(cid:21)
≤ 2E min h h h ,γ .
dπ
h µ h(x h,a h)
√
In(i),wehaveusedthatforallu,v ∈R+,min{u,v}≤uI{u≤v}+ uvI{v <u}andthat(u+v)2 ≤2(u2+v2),
thus that min{u,v}2 ≤2(u2I{u≤v}+uvI{v <u}). In (ii), we have done a change of measure from µ to
h
dπ. In (iii), we have used that min{u,v}=uI{u≤v}+vI{v <u}.
h
F.3.2 Proofs for Mabo.cr (Proof of Theorem 4.2)
Suppose the dataset D ={D } is sampled from the offline distribution µ(1:n). In this section, we analyze
h h≤H
our regularized and clipped variant of Mabo (Eq. (13)). We analyze Mabo.cr under the following density
ratioassumption. Recallforanysequenceofdatadistributionsµ(1),...µ(n),wedenotethemixturedistribution
by µ(1:n) ={µ(1:n)}H , defined by µ(1:n) := 1 (cid:80)n µ(i).
h h=1 h n i=1 h
Assumption F.4. For a given sequence of data distributions µ(1),...µ(n), we have that for all π ∈Π, and
for all h∈[H],
dπ
h ∈W.
µ(1:n)
h
We first note the following bound for the hypothesis f(cid:98)returned by Mabo.cr.
Lemma F.6. Let D ={D }H be a dataset consisting of H ·n samples from µ(1),...,µ(n). Suppose that F
h h=1
satisfies Assumption 2.1 and that W satisfies Assumption F.4 with respect to µ(1:n). Let f(cid:98)be the function
returnedby Mabo.cr,giveninEq.(13),whenexecutedon{D } withparametersγ,F,andtheaugmented
h h≤H
weight function class W defined in Eq. (38). Then, with probability at least 1−δ, we have
(cid:88)H (cid:12)
(cid:12) (cid:12)E
µ(
h1:n)(cid:104)
[∆ hf(cid:98)](x h,a h)·wˇ h(x h,a
h)(cid:105)(cid:12)
(cid:12)
(cid:12)≤(cid:88)H γ2 (0
n)
E
µ(
h1:n)(cid:104)
(wˇ h(x h,a
h))2(cid:105)
+
17
8H2β(n), (48)
h=1 h=1
for all w ∈W, where β(n) := 36γ(n) log(24|F||W|H2/δ).
n−1
Proof of Lemma F.6. Repeating the argument of Lemma E.2 (a), we can establish that Q⋆ satisfies the
following bound for all h∈[H],w ∈W:
E (cid:98)Dh(cid:104)(cid:0) [∆(cid:98)hQ⋆](x h,a h,r h,x′ h+1)(cid:1) ·wˇ h(x h,a h)(cid:105) ≤E (cid:98)Dh(cid:104) α(n)·(cid:0) wˇ h(x h,a h)(cid:1)2(cid:105) +β(n), (49)
45withprobabilityatleast1−δ,whereα(n) =8/γ(n)andβ(n) =36γ(n)/n−1log(6|F||W|H/δ)≤36γ(n)/n−1log(24|F||W|H2/δ).
Going forward, we condition on the event that this holds. Now, since the right-hand side of Eq. (49) is
independent of the sign of wˇ , we can apply this to −wˇ ∈W to conclude that
h
(cid:12) (cid:12) (cid:12)E (cid:98)Dh(cid:104)(cid:0) [∆(cid:98)hQ⋆](x h,a h,r h,x′ h+1)(cid:1) ·wˇ h(x h,a h)(cid:105)(cid:12) (cid:12) (cid:12)≤E (cid:98)Dh(cid:104) α(n)·(cid:0) wˇ h(x h,a h)(cid:1)2(cid:105) +β(n).
Summing over h∈[H] and taking the max over w ∈W, we can conclude that:
max(cid:88)H (cid:12) (cid:12) (cid:12)E (cid:98)Dh(cid:104)(cid:0) [∆(cid:98)hQ⋆](x h,a h,r h,x′ h+1)(cid:1) ·wˇ h(x h,a h)(cid:105)(cid:12) (cid:12) (cid:12)−E (cid:98)Dh(cid:104) α(n)·(cid:0) wˇ h(x h,a h)(cid:1)2(cid:105) ≤Hβ(n).
w∈W
h=1
By Assumption 2.1 and the definition of the hypothesis f(cid:98)returned by Mabo.cr, we have:
max(cid:88)H (cid:12) (cid:12) (cid:12)E (cid:98)Dh(cid:104)(cid:0) [∆(cid:98)hf(cid:98)](x h,a h,r h,x′ h+1)(cid:1) ·wˇ h(x h,a h)(cid:105)(cid:12) (cid:12) (cid:12)−E (cid:98)Dh(cid:104) α(n)·(cid:0) wˇ h(x h,a h)(cid:1)2(cid:105) ≤Hβ(n),
w∈W
h=1
and in particular the bound that for all w ∈W
H H
(cid:88) E (cid:98)Dh(cid:104)(cid:0) [∆(cid:98)hf(cid:98)](x h,a h,r h,x′ h+1)·wˇ h(x h,a h)(cid:1)(cid:105) ≤(cid:88) E (cid:98)Dh(cid:104) α(n)·(cid:0) wˇ h(x h,a h)(cid:1)2(cid:105) +Hβ(n).
h=1 h=1
Repeating the argument for Lemma E.2 (b), we can conclude that for all w ∈W
H H
(cid:88) E
µ(
h1:n)(cid:104) [∆ hf(cid:98)](x h,a h)·wˇ h(x h,a h)(cid:105) ≤(cid:88) γ2 (0
n)
E
µ(
h1:n)(cid:2) (wˇ h(x h,a h))2(cid:3) + 7H 1β 8(n) .
h=1 h=1
Applying this to w(h) ∈W and to −w(h) ∈W, and again noting that the right-hand side is independent of
the sign of wˇ, we can conclude that for each h∈[H],w ∈W:
(cid:12) (cid:12) (cid:12)E
µ(
h1:n)(cid:104) [∆ hf(cid:98)](x h,a h)·wˇ h(x h,a h)(cid:105)(cid:12) (cid:12) (cid:12)≤ γ2 (0
n)
E
µ(
h1:n)(cid:2) (wˇ h(x h,a h))2(cid:3) + 7H 1β 8(n) ,
Summing over h∈[H] gives the desired bound.
Theorem 4.2 (Mabo.crisCC-bounded). Let D ={D }H consist of H·n samples from µ(1),...,µ(n). For
h h=1
any γ ∈R , the Mabo.cr algorithm (Eq. (13)) with parameters F, augmented class W defined in Eq. (38)
+
in Appendix F.1.1, and γ is CC-bounded at scale γ under the Assumption that Q⋆ ∈F and that for all π ∈Π
and h∈[H], dπ h/µ( h1:n) ∈W.
Proof of Theorem 4.2. Let π =π . Using the performance difference lemma (Lemma F.2), we note that
(cid:98) f(cid:98)
H H
(cid:88) (cid:88)
J(π⋆)−J(π (cid:98))≤ E
dπ
h⋆[−[∆ hf(cid:98)](x h,a h)]+ E
dπ
h(cid:98)[[∆ hf(cid:98)](x h,a h)]. (50)
h=1 h=1
However, note that due to Lemma F.3 and Lemma F.4, we have that
E dπ h(cid:98)[([∆ hf(cid:98)](x h,a h))]≤E (cid:124)µ( h1:n)(cid:20) ([∆ hf(cid:98)](x h,a h) (cid:123)) (cid:122)clip γn(cid:20) µd ( h1π h(cid:98) :n( )x (xh, ha ,h a) h)(cid:21)(cid:21) (cid:125)+ γ4 n(cid:13) (cid:13) (cid:13) (cid:13)clip γn(cid:20) µd ( h1π h(cid:98) :n)(cid:21)(cid:13) (cid:13) (cid:13) (cid:13)
1,dπ h(cid:98)
, (51)
(I)
and,
E dπ h⋆[(−[∆ hf(cid:98)](x h,a h))]≤E µ( h1:n)(cid:20) (−[∆ hf(cid:98)](x h,a h))clip γn(cid:20) µd ( hπ h 1:⋆ n( )(x xh h, ,a ah h) )(cid:21)(cid:21) + γ4 n(cid:13) (cid:13) (cid:13) (cid:13)clip γn(cid:20) µd ( hπ h 1:⋆ n)(cid:21)(cid:13) (cid:13) (cid:13) (cid:13)
1,dπ⋆
. (52)
(cid:124) (cid:123)(cid:122) (cid:125) h
(II)
46We bound the terms (I) and (II) separately below. Before we delve into these bounds, note that using
Lemma F.6, we have with probability at least 1−δ,
wm ∈a Wx(cid:88)H (cid:18)(cid:12) (cid:12) (cid:12)E
µ(
h1:n)(cid:104) wˇ h([∆ hf(cid:98)](x h,a h))(cid:105)(cid:12) (cid:12) (cid:12)− γ2 (0
n)
E
µ(
h1:n)(cid:2) (wˇ h(x h,a h))2(cid:3)(cid:19) ≤ 17 8H2β(n), (53)
h=1
where β(n) := 36γnlog(24|F||W|H2/δ). Moving forward, we condition on the event under which Eq. (53)
n−1
holds.
(cid:20) (cid:21)
Bound on Term (I). Define w via w
h
:= µ(d 1π h(cid:98)
:n)
and wˇ
h
:=clip
γn
µ(d 1π h(cid:98)
:n)
, and note that due to Assumption
h h
F.4, we have that w ∈W. Thus, using (53), we get that
(cid:88)H (cid:88)H (cid:12) (cid:12)
E µ(1:n)[(−[∆ hf(cid:98)](x h,a h))wˇ h(x h,a h)]≤ (cid:12) (cid:12)E µ(1:n)[(−[∆ hf(cid:98)](x h,a h))wˇ h(x h,a h)](cid:12)
(cid:12)
h h
h=1 h=1
H
≤(cid:88) 20 E (cid:2) (wˇ (x ,a ))2(cid:3) + 7 H2β(n)
γ(n) µ h(1:n) h h h 18
h=1
H
=(cid:88) γ2 (0 n)(cid:13) (cid:13)clip γn[w h](cid:13) (cid:13)2
2,µ( h1:n)
+ 17 8H2β(n)
h=1
H
≤(cid:88) γ4 (0 n)(cid:13) (cid:13)clip γn[w h](cid:13) (cid:13)
1,dπ h(cid:98)
+ 17 8H2β(n),
h=1
where the last step follows by Lemma F.5 and the definition of w .
h
(cid:20) (cid:21)
Bound on Term (II). Definew⋆ := dπ h⋆ andwˇ⋆ :=clip dπ h⋆ ,andagainnotethatduetoAssumption
h µ(1:n) h γn µ(1:n)
h h
F.4, we have that w ∈W . Thus, using (53), and repeating the same arguments as above, we get that
h h
H
(II)≤(cid:88) γ4 (0 n)(cid:13) (cid:13)clip γn[w h⋆](cid:13) (cid:13) 1,dπ h⋆ + 17 8H2β(n),
h=1
Plugging the two bounds above in (55) and (56), and then further in (54), and using the definitions for w
h
and w⋆, we get that with probability at least 1−δ,
h
J(π⋆)−J(π (cid:98))≤(cid:88)H γ4 n0 (cid:13) (cid:13) (cid:13) (cid:13)clip γn(cid:20) µd (π h 1:⋆ n)(cid:21)(cid:13) (cid:13) (cid:13)
(cid:13)
+(cid:88)H γ5 n(cid:13) (cid:13) (cid:13) (cid:13)clip γn(cid:20) µd (1π h(cid:98) :n)(cid:21)(cid:13) (cid:13) (cid:13)
(cid:13)
+ 11 84 H2β(n)
h=1 h 1,dπ h⋆ h=1 h 1,dπ h(cid:98)
=(cid:88)H 40 (cid:32)(cid:13) (cid:13) (cid:13)clip (cid:20) dπ h⋆ (cid:21)(cid:13) (cid:13) (cid:13) +(cid:13) (cid:13) (cid:13)clip (cid:20) dπ h(cid:98) (cid:21)(cid:13) (cid:13) (cid:13) (cid:33) + 14 H2β(n)
h=1γ(n) (cid:13) γn µ h(1:n) (cid:13)
1,dπ h⋆
(cid:13) γn µ( h1:n) (cid:13)
1,dπ h(cid:98)
18
H
(cid:88) 40 n
= (CC (π⋆,µ(1:n),γn)+CC (π,µ(1:n),γn))+28H2γ log(24|F||W|H2/δ)
γ(n) h h (cid:98) n−1
h=1
H
(cid:88) 40
≤ (CC (π⋆,µ(1:n),γn)+CC (π,µ(1:n),γn))+56H2γlog(24|F||W|H2/δ),
γn h h (cid:98)
h=1
which establishes that the algorithm is CC-bounded for scale γ, with scaling functions a = 40 and b =
γ γ γ
56H2γlog(24|F||W|H2/δ).
47Corollary 4.1 (Risk bound for HyGlow). Let ε > 0 be given, let D consist of H ·T samples from
off
data distribution ν, and suppose that ν satisfies C -single-policy concentrability. Suppose that Q⋆ ∈ F
⋆
and that for all t ∈ [T], π ∈ Π, and h ∈ [H], we have dπ h/µ h(t) ∈ W, where µ( ht) := 1/2(ν
h
+1/t(cid:80)t i=1dπ h(i)).
Then, HyGlow with inputs T = Θ(cid:101)((H4(Ccov+C⋆)/ε2)·log(|F||W|/δ)), F, augmented W defined in Eq. (38),
(cid:16)(cid:112) (cid:17)
γ =Θ(cid:101) (C⋆+Ccov)/TH2log(|F||W|/δ) , and D
off
returns an ε-suboptimal policy with probability at least 1−δT
after collecting
(cid:18) H2(C +C ) (cid:19)
N =O(cid:101) cov ⋆ log(|F||W|/δ)
ε2
trajectories.
Proof of Corollary 4.1. This follows by combining Theorem 4.2 with Corollary F.1.
F.3.3 Proofs for Fitted Q-Iteration (Fqi)
In this section we prove Theorem F.2.
We quote the following generalization bound for least squares regression in the adaptive setting.
Lemma F.7 (Least squares generalization bound; Song et al. (2023, Lemma 3)). Let R > 0,δ ∈ (0,1),
and H : X (cid:55)→ [−R,R] a class of real-valued functions. Let D = {(x ,y )...(x ,y )} be a dataset of T
1 1 T T
points where x ∼ρ (x ,y ) and y =h⋆(x )+ε for some realizable h⋆ ∈H and ε is conditionally
t t 1:t−1 1:t−1 t t t t
mean-zero, i.e. E[y | x ] = h⋆(x ). Suppose max |y | ≤ R and max |h⋆(x)| ≤ R. Then the least squares
t t t t t x
solution (cid:98)h∈argmin h∈H(cid:80)T t=1(h(x t)−y t)2 satisfies that with probability at least 1−δ,
T
(cid:88) (cid:104) (cid:105)
E
x∼ρt
((cid:98)h(x)−h⋆(x))2 ≤256R2log(2|H|/δ).
t=1
Using the above theorem, we can show the following concentration result for Fqi.
Lemma F.8 (Concentration bound for Fqi). With probability at least 1−δ, we have that for all h∈[H],
(cid:104) (cid:105) 1 (cid:88)n (cid:20)(cid:16) (cid:17)2(cid:21)
E
µ( h1:n)
([∆ hf(cid:98)](x h,a h))2 =
n
E
(xh,ah)∼µ( hi)
f(cid:98)h(x h,a h)−[T hf(cid:98)h+1](x h,a h)
i=1
log(2|F|H/δ)
≤1024 .
n
Proof of Lemma F.8. Fix h + 1. Consider the regression problem induced by the dataset D =
h
{(z h(i),y h(i))}n
i=1
where z h(i) =(x( hi),a( hi))∼µ( hi) and y h(i) =r(i)+max a′f(cid:98)h+1(x( hi +) 1,a′). This problem is realizable
viatheregressionfunctionE[y h(i) |z h(i)]=h⋆(z h(i))=Tf(cid:98)h+1(z h(i))∈F,andsatisfiesthat|y h(i)|≤2,|h⋆(z h(i))|≤2.
In this regression problem, the least squares solution from Lemma F.7 is precisely the Fqi solution, so by
Lemma F.7 we have
n
(cid:16) (cid:17)2 1 (cid:88) (cid:16) (cid:17)2
E
(xh,ah)∼µ( h1:n)
f(cid:98)h(x h,a h)−[Tf(cid:98)h+1](x h,a h)) =
n
E
(xh,ah)∼µ( hi)
f(cid:98)h(x h,a h)−[Tf(cid:98)h+1](x h,a h)
i=1
log(2|F|/δ)
≤1024 ,
n
with high probability. Taking a union bound over h∈[H] gives the desired result.
Theorem F.2. The Fqi algorithm is CC-bounded under Assumption F.2 with scaling functions a = 6 and
γ γ
b =1024log(2n|F|)γ, for all γ >0 simultaneously. As a consequence, when invoked within H O, we have
γ 2
Risk≤O(cid:101)(cid:0) H(cid:112) (C⋆+Ccov)log(|F|δ−1)/T(cid:1)
with probability at least 1−δT .
48Proof of Theorem F.2. Let π =π . Using the performance difference lemma (given in Lemma F.2), we
(cid:98) f(cid:98)
note that
H H
(cid:88) (cid:88)
J(π⋆)−J(π (cid:98))≤ E
dπ
h⋆[−[∆ hf(cid:98)](x h,a h)]+ E
dπ
h(cid:98)[[∆ hf(cid:98)](x h,a h)]. (54)
h=1 h=1
However, note that due to Lemma F.3 and Lemma F.4, we have that
E dπ h(cid:98)[([∆ hf(cid:98)](x h,a h))]≤E (cid:124)µ( h1:n)(cid:20) ([∆ hf(cid:98)](x h,a h) (cid:123)) (cid:122)clip γn(cid:20) µd ( h1π h(cid:98) :n( )x (xh, ha ,h a) h)(cid:21)(cid:21) (cid:125)+ γ4 n(cid:13) (cid:13) (cid:13) (cid:13)clip γn(cid:20) µd ( h1π h(cid:98) :n)(cid:21)(cid:13) (cid:13) (cid:13) (cid:13)
1,dπ h(cid:98)
, (55)
(I)
and,
E dπ h⋆[(−[∆ hf(cid:98)](x h,a h))]≤E µ( h1:n)(cid:20) (−[∆ hf(cid:98)](x h,a h))clip γn(cid:20) µd ( hπ h 1:⋆ n( )(x xh h, ,a ah h) )(cid:21)(cid:21) + γ4 n(cid:13) (cid:13) (cid:13) (cid:13)clip γn(cid:20) µd ( hπ h 1:⋆ n)(cid:21)(cid:13) (cid:13) (cid:13) (cid:13)
1,dπ⋆
. (56)
(cid:124) (cid:123)(cid:122) (cid:125) h
(II)
Bound on Term (I). Note that
(cid:118)
E µ(1:n)(cid:20) ([∆ hf(cid:98)](x h,a h))clip γn(cid:20) µd (1π h(cid:98) :n( )x (xh,a ,h a) )(cid:21)(cid:21) ≤(cid:117) (cid:117) (cid:116)E µ(1:n)(cid:20)(cid:16) [∆ hf(cid:98)](x h,a h)(cid:17)2(cid:21) E µ(1:n)(cid:34)(cid:18) clip γn(cid:20) µd (1π h(cid:98) :n( )x (xh,a ,h a) )(cid:21)(cid:19)2(cid:35)
h h h h h h h h h
≤(cid:114) 2048log(2|F|H)(cid:13) (cid:13)
(cid:13)clip
(cid:20) dπ h(cid:98) (cid:21)(cid:13) (cid:13)
(cid:13) ,
n (cid:13) γn µ(1:n) (cid:13)
h 2,µ(1:n)
h
wherethesecondlinefollowsfromCauchy-SchwarzandthelastlinefollowsbyLemmaF.8. AM-GMinequality
implies that
(cid:114) 2048log(2|F|H)(cid:13) (cid:13)
(cid:13)clip
(cid:20) dπ h(cid:98) (cid:21)(cid:13) (cid:13)
(cid:13) ≤
1(cid:32) 1 (cid:13) (cid:13)
(cid:13)clip
(cid:20) dπ h(cid:98) (cid:21)(cid:13) (cid:13) (cid:13)2 +2048log(2|F|H)γ(cid:33)
n (cid:13) n µ(1:n) (cid:13) 2 γn(cid:13) γn µ(1:n) (cid:13)
h 2,µ(1:n) h 2,µ(1:n)
h h
≤
1(cid:32) 4 (cid:13) (cid:13)
(cid:13)clip
(cid:20) dπ h(cid:98) (cid:21)(cid:13) (cid:13)
(cid:13)
+2048log(2|F|H)γ(cid:33)
,
2 γn(cid:13) γn µ(1:n) (cid:13)
h 1,dπ h(cid:98)
where the last line is due to Lemma F.5.
Bound on Term (II). Repeating the same argument above for π⋆, we get that
(II)≤ 2 (cid:13) (cid:13) (cid:13)clip (cid:20) dπ h⋆ (cid:21)(cid:13) (cid:13) (cid:13) +1024log(2|F|H)γ.
γn(cid:13) γn µ(1:n) (cid:13)
h 1,dπ⋆
h
Combining the above bounds implies that
J(π⋆)−J(π (cid:98))≤(cid:88)H γ2 n(cid:32)(cid:13) (cid:13) (cid:13) (cid:13)clip γn(cid:20) µd (π h 1:⋆ n)(cid:21)(cid:13) (cid:13) (cid:13)
(cid:13)
+(cid:13) (cid:13) (cid:13) (cid:13)clip γn(cid:20) µd (1π h(cid:98) :n)(cid:21)(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:33) +2048log(2|F|H)γ
h=1 h 1,dπ h⋆ h 1,dπ h(cid:98)
H
(cid:88) 2
= (CC (π⋆,µ(1:n),γ)+CC (π,µ(1:n),γ))+2048log(2|F|H)γ,
γn h h (cid:98)
h=1
which shows that Fqi is CC-bounded at scale γ under Assumption F.2, with scaling functions a = 2 and
γ γ
b =2048log(2|F|H)γ.
γ
It remains to show the stated risk bound when Fqi is applied within H O. This follows by applying Corol-
2
lary F.1.
49F.3.4 Proofs for Model-Based MLE
In this section we prove Theorem F.4. We quote the following generalization bound for maximum likelihood
estimation (MLE) in the adaptive setting.
Lemma F.9 (MLE generalization bound; ?, Theorem 18). Consider a sequential conditional probability
estimation setting with an instance space Z and a target space Y and conditional density p(y |z). We are
given F : (Z ×Y) → R. We are given a dataset D = {(z ,y )}T where z ∼ D = D (z ,y ) and
t t t=1 t t t 1:t−1 1:t−1
y ∼p(·|z ). Fix δ ∈(0,1), assume |F|<∞ and there exists f⋆ ∈F such that f⋆(y |z)=p(y |z). Then,
t t
with probability at least 1−δ,
(cid:88)T (cid:13) (cid:13)2
E z∼ρt(cid:13) (cid:13)f(cid:98)(·|z)−f⋆(·|z)(cid:13)
(cid:13)
tv
≤2log(|F|/δ),
t=1
where
T
(cid:88)
f(cid:98)∈argmax logf(y
t
|z t).
f∈F
t=1
Recall the notation that each model M ∈ M defines a conditional probability distribution of the form
M(r,x′ |x,a). We apply the above generalization bound to our setting to obtain the following.
Corollary F.2. Under Assumption F.3, we have that the model-based maximum likelihood estimator of
Equation (41) satisfies
(cid:20)(cid:13) (cid:13)2 (cid:21) log(|M|H/δ)
∀h∈[H]: E
µ( h1:n)
(cid:13) (cid:13)M(cid:99)h(·|x h,a h)−M h⋆(·|x h,a h)(cid:13)
(cid:13) tv
≤2
n
.
Note that we have taken an extra union bound over H so that the MLE succeeds at each layer.
This is easily seen to imply an error bound between the associated Bellman operators.
Corollary F.3. Let [T(cid:98)f](x,a)=E (r,x′)∼M(cid:99)(·|x,a)[r+max a′f(x′,a′)] denote the Bellman optimality operator
of M(cid:99), and T denote the Bellman optimality operator of M⋆. Then we have that for all h∈[H] and for all
f :X ×A→[0,1],
(cid:20)(cid:16) (cid:17)2(cid:21)
log(|M|H/δ)
E
µ(1:n)
[T(cid:98)hf](x h,a h)−[T hf](x h,a h) ≤8
n
.
h
Proof of Corollary F.3. Notice that
(cid:104) (cid:105) (cid:104) (cid:105)
[T(cid:98)hf](x,a)−[T hf](x,a)=E
(r,x′)∼M(cid:99)(x,a)
r+m aa ′xf(x′,a′) −E
(r,x′)∼M⋆(x,a)
r+m aa ′xf(x′,a′)
(cid:13) (cid:13)
≤2(cid:13) (cid:13)M(cid:99)h(·|x,a)−M h⋆(·|x,a)(cid:13)
(cid:13)
.
tv
Theorem F.4. The model-based MLE algorithm is CC-bounded under Assumption F.3 for all γ > 0
simultaneously, with scaling functions a = 6 and b =8log(|M|H/δ)γ. As a consequence, when invoked
γ γ γ
within H 2O, we have Risk≤O(cid:101)(cid:0) H(cid:112) (C⋆+Ccov)log(|M|Hδ−1)/T(cid:1) with probability at least 1−δT.
Proof of Theorem F.4. We note that Corollary F.2 implies a squared Bellman error bound for Q⋆ , the
M(cid:99)
optimal value function for M(cid:99), since
(cid:16) (cid:17)2 (cid:16) (cid:17)2
∀x,a: [T(cid:98)hQ⋆ M(cid:99),h+1](x,a)−[T hQ⋆ M(cid:99),h+1](x,a) = Q⋆ M(cid:99),h(x,a)−[T hQ⋆ M(cid:99),h+1](x,a) ,
50by the optimality equation for Q⋆ . Thus we have
M(cid:99)
(cid:20)(cid:16) (cid:17)2(cid:21)
log(|M|H/δ)
∀h∈[H]: E Q⋆ (x ,a )−[T Q⋆ ](x ,a ) ≤8 . (57)
µ( h1:n) M(cid:99),h h h h M(cid:99),h+1 h h n
This is enough to repeat the proof of CC-boundedness for Fqi (Theorem F.2), with Q⋆ taking the place of
M(cid:99)
the Fqi solution f(cid:98). Indeed, the only algorithmic property that we used for Fqi was Lemma F.8, which also
holds for Q⋆ by Equation (57). Tracking the slightly different constants resulting gives us the desired values
for a and bM(cid:99).
γ γ
51