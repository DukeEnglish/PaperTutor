Traffic Smoothing Controllers for Autonomous Vehicles Using Deep
Reinforcement Learning and Real-World Trajectory Data
Nathan Lichtle´1,2†∗, Kathy Jang1†, Adit Shah1†, Eugene Vinitsky3†,
Jonathan W. Lee1,4, and Alexandre M. Bayen1,4
Abstract—Designingtraffic-smoothingcruisecontrollersthat potential to yield a pronounced effect on the state of traffic,
canbedeployedontoautonomousvehiclesisakeysteptowards addressing the problem of energy usage from another angle.
improvingtrafficflow,reducingcongestion,andenhancingfuel
This paper focuses on the problem of energy usage in
efficiency in mixed autonomy traffic. We bypass the common
issue of having to carefully fine-tune a large traffic micro- transportation and explores the potential of AVs to alleviate
simulator by leveraging real-world trajectory data from the this issue. In this era of mixed autonomy traffic, in which a
I-24 highway in Tennessee, replayed in a one-lane simulation. percentage of vehicles are AVs with special control capabil-
Using standard deep reinforcement learning methods, we train
ities, a rich amount of work has been produced that shows
energy-reducing wave-smoothing policies. As an input to the
that even a small percentage of intelligent agents in traffic
agent, we observe the speed and distance of only the vehicle in
front, which are local states readily available on most recent arecapableofachievingenergysavingsandtrafficreduction
vehicles,aswellasnon-localobservationsaboutthedownstream via wave dampening or jam-absorption driving of stop-
state of the traffic. We show that at a low 4% autonomous and-go waves and bottlenecks [5]–[8]. Many studies have
vehicle penetration rate, we achieve significant fuel savings of
shown how longitudinal control of an AV can significantly
over 15% on trajectories exhibiting many stop-and-go waves.
impact global and local metrics such as total velocity or
Finally, we analyze the smoothing effect of the controllers
and demonstrate robustness to adding lane-changing into the fuel economy; for instance, controlled vehicles are capable
simulation as well as the removal of downstream information. of completely dissipating traffic waves in a ring setting [9],
[10]. To our knowledge, there have no been no large-scale
I. INTRODUCTION
tests conducted with the goal of smoothing traffic flow.
Transportation accounts for a large share of energy usage
However, finding an effective way to design controllers
worldwide, with the U.S. alone attributing 28% of its total
for these settings remains an open question due to the
energyconsumptionin2021tomovingpeopleandgoods[1].
partially observed, hybrid, multi-agent nature of traffic flow.
Itisthesinglelargestsectorofenergyconsumption,ranking
In recent years, reinforcement learning (RL) has emerged
overothermajorcontributorssuchastheindustrialsector[2].
as a powerful approach for traffic control, leveraging its
Advances in technology have paved the way for improve-
ability to capture patterns from unstructured data. RL is
ments in CO emissions and fuel economy via consumer
2
responsible for producing quality controllers across a wide
shifts toward hybrid and electric vehicles (EVs), as well
range of domains, from robotics [11] to mastering gameplay
as innovation in vehicle technology such as turbocharged
over human experts such as with Starcraft or Go [12], [13].
engines or cylinder deactivation [3]. Meanwhile, as au-
Within the domain of traffic, RL has been used to derive
tonomousvehicles(AVs)becomeincreasinglymoreavailable
a variety of state-of-the-art controllers for improving traffic
inroadways,withtheInsuranceInstituteforHighwaySafety
flow metrics such as throughput and energy efficiency [7],
predicting there to be 3.5 million vehicles with autonomous
[14], [15].
capabilities on U.S. roads by 2025 [4], so too do their
With a particular focus on the real-world impacts of RL
This work was supported in part by the C3.ai Digital Transformation on traffic, we discuss the RL-based controller we developed
InstituteunderGrantNumber053483.NathanLichtle´ issupportedinpart
by the International Emerging Actions project SHYSTRA (CNRS). This for a stretch of the I-24 highway near Nashville, Tennessee.
materialisbaseduponworksupportedbytheNationalScienceFoundation Our controllers take into account the requirements of real-
under Grants CNS-1837244 (K. Jang), CNS-2146752 (A. Bayen), CNS- world deployment, utilizing observations that are accessible
2135579 (A. Bayen, J. Lee). This material is based upon work supported
by the U.S. Department of Energy’s Office of Energy Efficiency and viaradarandcameras.Despitelimitedaccesstothefullstate
Renewable Energy (EERE) under the Vehicle Technologies Office award space, our RL-based approach achieves notable fuel savings
numberCIDDE–EE0008872.Theviewsexpressedhereindonotnecessarily even with low penetration rates.
representtheviewsoftheU.S.DepartmentofEnergyortheUnitedStates
Government. The contributions of this article are:
1 Department of Electrical Engineering and Computer Sciences, UC
Berkeley • The introduction of a single-agent RL-based controller
2 CERMICS,E´coledesPontsParisTech developed using real traffic trajectories with advanced
3 DepartmentofMechanicalEngineering,UCBerkeley telemetry-enabled downstream information,
4 InstituteforTransportationStudies,UCBerkeley • Numerical results of the controller’s performance
demonstrating significant fuel savings of over 15% in
† Theseauthorscontributedequallytothiswork.
∗ Correspondingauthor:nathanlct@berkeley.edu scenariosexhibitinglarge-amplitudestop-and-gowaves.
4202
naJ
81
]YS.ssee[
1v66690.1042:viXraII. PROBLEMFORMULATION the average speed of traffic in each segment. For training,
we retrieved the INRIX data matching the time when the
In this paper, we consider the problem of decreasing
trajectories were collected on the highway (which includes
energy consumption in highway traffic by attempting to
delay). In the absence of such historical data, one could also
smooth out stop-and-go waves. These waves are a phe-
generate synthetic traffic data for each dataset trajectory by
nomenon where high vehicle density causes vehicles to start
artificially and averaging the trajectory speeds accordingly.
and stop intermittently, creating a wave-like pattern that can
On top of this raw INRIX data, we use a speed planner
propagate upstream and be highly energy-inefficient due to
developed in [19] that provides a profile that the controller
frequent accelerating and braking [16]. We insert a small
should approximately track. The speed planner takes in the
percentage of autonomous vehicles (AVs) equipped with
data and interpolates on the individual data points to create
reinforcementlearning-basedcontrollersintothefluxoftraf-
a continuous and less noisy speed profile of the whole
fic. Leveraging a data-driven, one-lane simulator previously
highway. It then uses kernel smoothing to create a target
introduced in [17], we simulate real-world highway trajec-
speed profile, which is intended as an estimate of the speed
tories. This approach is considerably more time-efficient
todriveatinordertosmoothoutthetrafficwaves.However,
than comprehensive traffic micro-simulations and is able to
due to delay and noisy estimates, driving exactly at this
partiallymodeltheintricatestop-and-gobehaviorsthatoccur
speed is insufficient to guarantee smoothing or reasonable
in traffic, although it overlooks complex dynamics such as
gaps. This target speed profile, sampled at different points,
lane-changing, which we rectify by incorporating a lane-
isfinallyfedasaninputtoourcontroller,whichcanbeused
changing model that we calibrate on data. The following
as an indication of where the free-flow and the congested
subsections introduce the simulation as well as the different
regions might be.
modules that it integrates.
A. Dataset and Simulation C. Energy function
WeusetheI-24TrajectoryDataset[18]introducedin[17] Since we aim to optimize energy consumption, we need
alongwithaone-lanesimulatorthatreplayscollectedtrajec- a model that can be used to compute consumed energy
tory data. A vehicle, which we call the trajectory leader, is in our cost functions. We use a model of instantaneous
placed at the front of the platoon and replays the real-world fuel consumption, a polynomial fitted on a Toyota RAV4
I-24 trajectory. Behind it, we place an arbitrary number of model that is similar to the model in [20], but with updated
AVsandhumanvehicles(introducedinSec.II-D).Typically coefficients. The function depends on the speed of the AV
during training, the platoon behind the trajectory leader and its instantaneous acceleration, as well as road grade,
consists of one RL-controlled AV, followed by 24 human which we assume to be zero in this work.
vehicles. The goal of the AV is to absorb the perturbations
D. Human Driver Behavior
in the leader trajectory, so that the energy consumption of
thefollowinghumanvehiclesimprovescomparedtothecase To model human drivers in simulation, we use the In-
where the AV is not present. telligent Driver Model (IDM) [21], a time-continuous car-
Instead of training on the whole dataset, we only train following model which is widely used in traffic applica-
on a selected set of four trajectories containing different tions. We pick the IDM parameters such that the model is
patterns of waves alternating between low and high speeds. unstable below 18m, meaning that stop-and-go waves will
s
This allows us to optimize directly on the dynamics we are propagate backward in the flux of traffic and grow instead
interested in improving, without having training diluted by of just dissipating. Numerous results demonstrate the string-
free-flow trajectories containing no waves for the controller unstable qualities of human-driver behavior, both via real
to smooth. We observed that this made training faster while human drivers in real life and via models such as IDM in
still yielding a controller able to generalize to unseen tra- simulation [10].
jectories. As the trajectories are quite long (between 5000
E. Lane-changing model
and 12000 time steps, where a time step is 0.1s), each
simulation randomly samples a chunk of size 500 (or 50s) We use a lane-changing model to enable more complex
within a trajectory. At evaluation time, we consider a set multi-lane dynamics for evaluation only. The model consists
of six trajectories distinct from the training trajectories and of a cut-in probability function P in(h,v lead) and a cut-out
simulate the whole trajectories. probabilityfunctionP out(v lead),wherehisthespacegapand
v is the speed of the leading vehicle. Both are piecewise
lead
B. Speed planner
second-orderpolynomialswhosecoefficientswerecalibrated
Weusereal-timedataaboutthestateoftrafficallalongthe usingdatacollectedontheI-24highway,whichhasnotbeen
I-24 in order to equip the RL control with some knowledge publishedyet.Ateachtimestept,andforeachegovehiclein
about the downstream state of traffic. The data is provided thesimulation,P givestheprobabilitythatavehiclecutsin
in
to us by INRIX, which broadcasts it in real time with front of the ego vehicle, while P gives the probability that
out
an approximately 1-minute update interval and a 3-minute the leading vehicle cuts out. If a cut-in happens, a vehicle
delay. The highway is divided into segments of 0.5 miles on is inserted such that the ratio between the space gap of the
average (with significant variance), and the data consists of insertedvehicleandthespacegapoftheegovehicleafterthecut-infollowsanormaldistribution(alsofittodata),clipped where the AV velocity is slightly exaggerated to ensure
to ensure safety after insertion. This model lets us measure robustness at both low and high speeds. The actual numbers
therobustnessofthecontrol,allowinghumanvehiclestocut are chosen heuristically: for instance, if both AV and leader
in front of the AV as it tries to open larger gaps to smooth driveat30m,thefailsafewillensureaminimumgapof30m.
s
out traffic waves. The failsafe triggers if the time to collision ever goes below
6seconds,inwhichcasetheRLoutputisoverriddenandthe
III. CONTROLLERDESIGN
vehicle will brake at its maximum allowed deceleration. We
In this section, we formally define the problem in the thus have hmin = 6vdiff. The final RL acceleration is given
t t
context of reinforcement learning and discuss the structure by:
and design of the controller. 
−3 if ∆TTC ≤6 (⇔h ≤hmin)
 t t t
A. Defining the POMDP aout = 1.5 if ∆TTC >6 and h ≥hmax
t t t t
We use the standard RL formalism of maximizing the a otherwise
t
discounted sum of rewards for a finite-horizon partially-
which is further clipped to ensure that the speed vav remains
observed Markov decision process (POMDP). We can for- t
within the boundaries [0,35]m. Note that the free-flow be-
mally define this POMDP as the tuple (S,A,T,R,γ,Ω,O) s
havior due to the gap-closing wrapper will be to drive at the
where S is a set of states, A represents the set of actions,
T : S ×A×S → R represents the conditional probability speed limit in the absence of a leader.
distributionoftransitioningtostates′givenstatesandaction 3) Optimization criterion: At the core, we aim to mini-
a, R : S ×A → R is the reward function, and γ ∈ (0,1] mizetheoverallenergyconsumptionofthetraffic.However,
assparserewardsarehardertooptimize,weemployproxies
is the discount factor used when calculating the sum of
that can be minimized at each time step. We mainly aim
rewards. The final two terms are included since the state
to minimize the instantaneous energy consumption of the
is hidden: Ω is the set of observations of the hidden state,
and O : S ×Ω → R represents the conditional observation AV and a platoon of vehicles behind it. For comfort, and
as another proxy for energy savings, we also minimize
probability distribution.
squaredaccelerationamplitudes.Sinceoptimizingforenergy
1) Observation space: The primary observation space (at
time t) consists of the ego vehicle speed vav, the leader and acceleration can be done by stopping or maintaining
t
vehicle speed vlead, and the space gap (bumper-to-bumper unreasonably large or small gaps for comfort, we penalize
t
gapsoutsideofacertainrange;thisalsopenalizestheuseof
distance) h between the two vehicles. (Note that all dis-
t
tances are in m, velocities in m, and accelerations in m.) failsafe and gap-closing interventions. To further discourage
Two gap thresholds are also ins cluded: hmin, which is s t2 he large gaps within this allowed range, the final term adds a
t
penalty proportional to the time gap (space gap divided by
failsafethresholdbelowwhichthevehiclewillalwaysbrake,
and hmax, the gap-closing threshold above which the vehicle ego speed). This is formalized as the reward function r t,
t
which is given by:
will always accelerate. We also include the history of the
ego vehicle’s speed over the last 0.5 seconds. Finally, the n
observationspaceincludestrafficinformationfromthespeed r t =−c 1n1 (cid:88) E ti−c 2(ao tut)2−c 31(cid:2) h t ∈/ [hm tin,hm tax](cid:3)
planner.Thisconsists ofthecurrenttargetspeed vsp,aswell i=1
t
h
asthetargetspeeds200m,500m,and1kmdownstreamofthe −c t 1[h >10∧vav >1]
vehicle’scurrentposition.NotethattheAVonlyobservesits
4vav t t
t
leading vehicle and that there is no explicit communication where Ei is the instantaneous energy consumption of
t
between AVs. All observations provided to the RL agent are
vehicle i at time t, where index i = 1 corresponds to
rescaled to the range [−1,1].
the AV, and indexes i = 2 to i = n correspond to the
2) Actionspace: Theactionspaceconsistsofaninstanta- followingn−1IDMvehicles.1isdefinedsuchthat1[P]=
neous acceleration a ∈[−3,1.5]. After the RL output, gap-
t 1 if P is true, 0 otherwise.
closing and failsafe wrappers are then enforced. We define
the gap-closing wrapper such that hmax = max(120,6vav), B. Training algorithm
t t
meaning that the AV will be forced to accelerate if its space We use single-agent Proximal Policy Optimization [22]
gapbecomeslargerthan120moritstimegaplargerthan6s. (PPO) with an augmented value function as our training
This result is then wrapped within a failsafe, which enforces algorithm. PPO is a policy gradient algorithm, a class of RL
safe following distances and prevents collisions. We define techniques that optimize for cumulative, discounted reward
the time to collision ∆T tTC of the ego and lead vehicles as: via shifting the parameters of the neural net directly. More
 h explicitly, policy gradient methods represent the policy as
 t if vdiff >0
∆TTC = vdiff t π θ(a|s), where θ are the parameters of the neural net.
t +t
∞ otherwise
During training, we give additional observations that are
available in simulation as an input to the value network.
(cid:20) (cid:18) (cid:19) (cid:21)
4 This includes the cumulative miles traveled and gallons of
vdiff = vav 1+ +1 −vlead
t t 30 t gasconsumed,whichallowsforestimatingenergyefficiency.Fig. 1. Evolution of the speed of the trajectory leader and the AVs in a platoon of 200 vehicles. The 8 AVs are equally spaced at a 4% penetration
rate.ThefirstAVintheplatoonisshowninblue,thefollowingonesaredisplayedbydecreasingopacityandthelastoneisingreen,demonstratingthe
smoothingeffectoftheAVsontheleadertrajectory.Inparticular,onecanseehowthefirstAV(inblue)alreadysmoothesthetrajectoryleader(inred),
doesn’tslowdownasmuchoraccelerateasfast,andthussavesenergy.
C. Experiment details
We run the PPO experiments using the implementation
provided in Stable Baselines 31 version 1.6.2. We train the
model for 2500 iterations, which takes about 6 hours on 90
CPUs. We use a training batch size of 9000, a batch size of
3000,alearningrateof3·10−4anddo5epochsperiteration.
Thesimulationhorizonissetto500,andwedo10simulation
steps per environment step, ie. each action is repeated 10
times. Given that the simulation time step is dt = 0.1s,
this means that the action changes only every second during
training. This allows us to artificially reduce the horizon so
that it only is 50, meaning that each training batch contains
100 simulation rollouts. The agent’s policy is modeled as a
fully-connected neural network with 4 hidden layers of 64
neurons each and tanh linearities, with continuous actions.
The value network has the same architecture as the policy
network. We set the discount factor γ to 0.999, the GAE
value λ to 0.99, and the other training and PPO parameters
are left to their default values.
We train with a platoon of n=25 vehicles (not including
theleadingtrajectoryvehicle)consistingofoneAVfollowed
by 24 IDM vehicles. For our reward function, we used
coefficients c = 0.06, c = 0.02, c = 0.6, c = 0.005.
1 2 3 4
Both the model parameters and training hyperparameters
are determined through grid search, with each experiment
Fig.2. Trajectoriesusedforevaluation,numberedfrom1to6.Thefirst conducted using 4 to 8 distinct random seeds to overcome
one corresponds to free flow, while the five others contain both low and
instances of the agent getting trapped in local optima.
high speeds, including sharp breaking, sharp accelerating or stop-and-go
behaviors.
IV. RESULTS
In this section, we analyze the performances of our RL
controller in simulation, in terms of energy savings, wave-
smoothing, behavior and robustness.
The smoothing effect of the AVs is illustrated in Fig. 1,
where one can see the speeds of all the AVs in a platoon of
This augmented observation space also includes the follow-
200vehiclesasafunctionoftimeontrajectory6(seeFig.2).
ing metadata: the size of the finite horizon, an identifier
One can observe how the speed profiles become smoother
for the specific trajectory chunk being trained on, and the
and smoother after each AV.
vehicle’s progress (in space and time) within this chunk.
Fig. 3 illustrates the smoothing performed by the RL
The additional information removes some of the partial
agents on trajectory 6 in a different way. The top-left
observability from the system, allowing the value function
to more accurately predict the factors that influence reward. 1https://github.com/DLR-RM/stable-baselines3Fig.3. Time-spacediagrams,eachrepresentingasimulationof200vehicles.Eachvehicletrajectoryisplottedasalineinposition-by-timespace,with
colorrepresentingthespeedofthatvehicle.Onecanobservethewave-smoothingeffectoftheRL-controlledAVsovertime.Horizontallinesdisplaythe
throughputofthetrafficflowatthatparticularposition.Alsonotethatasawarm-up,allAVsbehaveashumanswhentheirpositionisnegative.Topleft:
all200vehiclesareIDMs.Trafficwavesareillustratedbytheredandblackcolors,whilebrightgreenrepresentsfreeflow.Topright: 20equally-spaced
RL-controlledAVs(10%penetrationrate,1AVevery10vehicles).OnecanseethelargergapsopenedbytheAVsasthewhitelinesbetweenplatoons.
Bottom left: 8 equally-spaced RL-controlled AVs (4% penetration rate, 1 AV every 25 vehicles). Bottom right: 8 equally-spaced RL-controlled AVs
(4%penetrationrate)withthelane-changingmodelenabled(notethatitisdisabledinall3othersubfigures).
Index 10%w/oLC 10%w/LC 4%w/oLC 4%w/LC
time-space diagram shows that the humans don’t smooth
1 +7.33% +8.39% +4.29% +6.12%
any waves; on the contrary, they even create some due to
2 +10.87% +12.63% +7.04% +9.42%
the string-unstable nature of the IDM we use. At a 10% 3 +14.65% +14.48% +9.02% +7.23%
penetration rate, most of the waves get smoothed out, less 4 +15.02% +13.19% +9.23% +8.54%
5 +22.58% +15.77% +17.05% +8.37%
at 4%, and even less with lane-changing. However, in all
6 +28.98% +18.55% +19.96% +15.40%
3 cases, the diagrams clearly demonstrate the improvement
TABLEI
over the baseline. As expected, AVs opening larger gaps
WERUNSIMULATIONSWITH200VEHICLES,ANDSHOWTHE
also leads to decreased throughput, and the best throughput
IMPROVEMENTINSYSTEMMPGWHENWECONTROL10%(LEFT)OR
is achieved when the lane-changing model is enabled and
4%(RIGHT)OFTHEVEHICLES,COMPAREDTOWHENALLVEHICLES
humanvehicles fillinthegaps. Thiscomesdownto atrade-
BEHAVEASIDMS,WITHANDWITHOUTLANE-CHANGINGINBOTH
off betweenthroughput reduction and energysavings, which
CASES.THETRAJECTORIESUSEDFOREVALUATIONCANBESEENIN
can be tuned by varying the penetration rate.
FIG.2,WITHCORRESPONDINGINDEXES.
Table I shows the energy savings that our controller
achievesontheevaluationtrajectories(showninFig.2)when
deployed on AVs at two different penetration rates, with and
without lane-changing enabled. The percentages correspond the energy consumption on trajectory 6 by over 15%, while
to how much the average system miles-per-gallon (MPG) only reducing throughput by 5%.
valueincreaseswhenAVsuseourRLcontroller,comparedto
We also note that the controller appears robust to not
whentheyallbehaveashumans.TheaverageMPGisdefined
havingaccesstothespeedplanner.Forexampleata4%pen-
asthesumofthedistancestraveledbyallthevehiclesinthe
etration rate and without lane-changing, the control achieves
simulation,dividedbythesumoftheirenergyconsumptions.
+16.87% energy improvement on trajectory 6 without the
Wecanobservetheenergysavingsvaryingalotdepending speed planner (compared to +19.96% with the speed plan-
on the trajectories, which is expected since trajectories that ner), and the trend is similar on the other trajectories.
are mostly free flow (like trajectory 1) cannot be improved Finally, in Fig. 4, we illustrate the gaps opened by the
much, while one with a lot of stop-and-go waves (like AV on trajectory 6, along with the failsafe and gap-closing
trajectory6)hasalotofpotentialtobesmoothed.Asonecan thresholds. The gap-closing threshold allows the AV to open
expect, energy savings decrease as the AV penetration rate larger gaps and consequently absorb abrupt braking from its
decreases or as lane-changing is enabled, but even at 4% leader while ensuring that these gaps are not overly large.
penetration and with lane-changing, the controller reduces As can be expected, we have observed that the larger the[4] “Autonomous Vehicles — content.naic.org,” https://content.naic.org/
cipr-topics/autonomous-vehicles,[Accessed04-Mar-2023].
[5] W. Beaty, “Traffic “experiments” and a cure for waves & jams,”
http://amasci.com/amateur/traffic/trafexp.html, 1998, [Accessed 15-
Oct-2006].
[6] F. Wu, R. E. Stern, S. Cui, M. L. Delle Monache, R. Bhadani,
M.Bunting,M.Churchill,N.Hamilton,B.Piccoli,B.Seiboldetal.,
“Tracking vehicle trajectories and fuel rates in phantom traffic jams:
Methodology and data,” Transportation Research Part C: Emerging
Technologies,vol.99,pp.82–109,2019.
[7] K.Jang,E.Vinitsky,B.Chalaki,B.Remer,L.Beaver,A.Malikopou-
los,andA.Bayen,“Simulationtoscaledcity:zero-shotpolicytransfer
for traffic control via autonomous vehicles,” in 2019 International
ConferenceonCyber-PhysicalSystems,Montreal,CA,2018.
Fig.4. SpacegapofthefirstAVintheplatoon(inblue)bytime,onthe [8] E. Vinitsky, K. Parvate, A. Kreidieh, C. Wu, and A. Bayen, “La-
samescenarioasinFig.1.Theorangelineshowsthegap-closingthreshold grangian control through deep-rl: Applications to bottleneck decon-
hmax and the green line shows the failsafe threshold hmin, introduced in gestion,”in201821stInternationalConferenceonIntelligentTrans-
Set c.III-A.2. t portationSystems(ITSC). IEEE,2018,pp.759–765.
[9] R. E. Stern, S. Cui, M. L. Delle Monache, R. Bhadani, M. Bunting,
M. Churchill, N. Hamilton, H. Pohlmann, F. Wu, B. Piccoli et al.,
“Dissipation of stop-and-go waves via control of autonomous vehi-
maximumgapweallow,thebettertheAVperformsinterms
cles: Field experiments,” Transportation Research Part C: Emerging
ofenergysavings.However,alargermaximumgapisusually Technologies,vol.89,pp.205–221,2018.
accompanied by a decrease in throughput, which is again a [10] S. Cui, B. Seibold, R. Stern, and D. B. Work, “Stabilizing traffic
flow via a single autonomous vehicle: Possibilities and limitations,”
trade-off. The failsafe threshold mostly ensures safety and
inIntelligentVehiclesSymposium(IV),2017IEEE. IEEE,2017,pp.
comfort for the driver, although it is worth noting that when 1336–1341.
deploying the controller, we integrate an additional explicit [11] S. Gu, E. Holly, T. Lillicrap, and S. Levine, “Deep reinforcement
learning for robotic manipulation with asynchronous off-policy up-
safety wrapper, as detailed in [17].
dates,”inRoboticsandAutomation(ICRA),2017IEEEInternational
Conferenceon. IEEE,2017,pp.3389–3396.
V. CONCLUSION
[12] O.Vinyals,I.Babuschkin,W.M.Czarnecki,M.Mathieu,A.Dudzik,
In this work, we developed RL policies that incorporate J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev et al.,
“Grandmaster level in starcraft ii using multi-agent reinforcement
both accessible local observations and downstream traffic
learning,”Nature,vol.575,no.7782,pp.350–354,2019.
information, achieving substantial energy savings in simu- [13] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang,
lation. There are several avenues for future research. Given A.Guez,T.Hubert,L.Bakeretal.,“Masteringthegameofgowithout
humanknowledge,”Nature,vol.550,no.7676,p.354,2017.
thatallthetrainingisconductedinasimulatedenvironment,
[14] C.Wu,A.Kreidieh,K.Parvate,E.Vinitsky,andA.M.Bayen,“Flow:
it would be beneficial to train the agent with more realistic Architecture and benchmarking for reinforcement learning in traffic
dynamics by enhancing the accuracy of the various models control,”arXivpreprintarXiv:1710.05465,p.10,2017.
[15] E.Vinitsky,A.Kreidieh,L.LeFlem,N.Kheterpal,K.Jang,C.Wu,
that make up this simulation, like human driving behavior,
F. Wu, R. Liaw, E. Liang, and A. M. Bayen, “Benchmarks for
lane-changing dynamics, and energy consumption metrics. reinforcementlearninginmixed-autonomytraffic,”inConferenceon
Additionally, it would be interesting to explore multi-agent RobotLearning. PMLR,2018,pp.399–409.
[16] Y. Sugiyama, M. Fukui, M. Kikuchi, K. Hasebe, A. Nakayama,
RL to help the model be robust to interactions between AVs
K. Nishinari, S.-i. Tadaki, and S. Yukawa, “Traffic jams without
and potentially enable cooperation between them. bottlenecks—experimentalevidenceforthephysicalmechanismofthe
While our simulation process has a distinct speed advan- formationofajam,”Newjournalofphysics,vol.10,no.3,p.033001,
2008.
tageoverlargemicro-simulators,itcouldbenefitsignificantly
[17] N. Lichtle´, E. Vinitsky, M. Nice, B. Seibold, D. Work, and A. M.
from vectorization. Moreover, despite our technical capacity Bayen, “Deploying traffic smoothing cruise controllers learned from
to deploy our controller safely onto a real-world vehicle, trajectory data,” in 2022 International Conference on Robotics and
Automation(ICRA). IEEE,2022,pp.2884–2890.
gathering results is challenging and necessitates further field
[18] M. Nice, N. Lichtle´, G. Gumm, M. Roman, E. Vinitsky,
tests.Anotherdirectionofresearchweareexploringconsists S. Elmadani, M. Bunting, R. Bhadani, K. Jang, G. Gunter et al.,
of training and deploying adaptive cruise control (ACC)- “The i-24 trajectory dataset,” Sep. 2021. [Online]. Available:
https://doi.org/10.5281/zenodo.6456348
based controllers, where the policy outputs a desired set-
[19] Z.Fu,A.R.Kreidieh,H.Wang,J.W.Lee,M.L.D.Monache,and
speed instead of an acceleration. By design of the ACC, the A.M.Bayen,“Cooperativedrivingforspeedharmonizationinmixed-
control would be safe and smooth, and easily deployable trafficenvironments,”2023.
[20] J.W.Lee,G.Gunter,R.Ramadan,S.Almatrudi,P.Arnold,J.Aquino,
at a large scale simply by augmenting the onboard ACC
W. Barbour, R. Bhadani, J. Carpio, F.-C. Chou et al., “Integrated
algorithm to use the RL control as a set-speed actuator. framework of vehicle dynamics, instabilities, energy models, and
sparseflowsmoothingcontrollers,”inProceedingsoftheWorkshopon
REFERENCES Data-DrivenandIntelligentCyber-PhysicalSystems,2021,p.41–47.
[21] A.Kesting,M.Treiber,andD.Helbing,“Enhancedintelligentdriver
[1] “Use of energy for transportation - u.s. energy information ad- model to access the impact of driving strategies on traffic capacity,”
ministration (eia) — eia.gov,” https://www.eia.gov/energyexplained/ Philosophical Transactions of the Royal Society A: Mathematical,
use-of-energy/transportation.php,[Accessed04-Mar-2023]. Physical and Engineering Sciences, vol. 368, no. 1928, pp. 4585–
[2] “U.S. energy facts explained - consumption and production - U.S. 4605,2010.
Energy Information Administration (EIA) — eia.gov,” https://www. [22] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov,
eia.gov/energyexplained/us-energy-facts/,[Accessed04-Mar-2023]. “Proximal policy optimization algorithms,” arXiv preprint
[3] “Highlights of the Automotive Trends Report — US arXiv:1707.06347,2017.
EPA — epa.gov,” https://www.epa.gov/automotive-trends/
highlights-automotive-trends-report,[Accessed04-Mar-2023].