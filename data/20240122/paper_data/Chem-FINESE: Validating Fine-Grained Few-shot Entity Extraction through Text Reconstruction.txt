Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction
through Text Reconstruction
QingyunWang, ZixuanZhang, HongxiangLi, XuanLiu,
JiaweiHan, HengJi, HuiminZhao
UniversityofIllinoisatUrbana-Champaign
{qingyun4,zixuan11,hanj,hengji,zhao5}@illinois.edu
Abstract Input
Through application of ligand screening, we describe the first examples of Pd-
catalyzed Suzuki–Miyaura reactions using aryl sulfamates at room
Fine-grainedfew-shotentityextractioninthe
temperature.
chemicaldomainfacestwouniquechallenges.
Ground Truth
First,comparedwithentityextractiontasksin
ligand <Ligands>, Pd-catalyzed Suzuki-Miyaura reactions <Coupling
thegeneraldomain,sentencesfromchemical
reactions>, aryl sulfamates <Aromatic compounds>, room temperature
papers usually contain more entities. More- <Thermodynamic properties>
over,entityextractionmodelsusuallyhavedif- Sentence Reconstructed from Ground Truth
ficultyextractingentitiesoflong-tailedtypes.
Ligands play a crucial role in Pd-catalyzed Suzuki-Miyaura reactions, which
In this paper, we propose Chem-FINESE, a are coupling reactions that enable the synthesis of diverse organic compounds
such as aryl sulfamates at room temperature, exploiting their favorable
novel sequence-to-sequence (seq2seq) based thermodynamic properties.
few-shotentityextractionapproach,toaddress InBoxBART Entity Extraction Results
thesetwochallenges. OurChem-FINESEhas ligand screening <Ligands>, Pd-catalyzed Suzuki-Miyaura reactions
twocomponents: aseq2seqentityextractorto <Coupling reactions>, aryl sulfamates <Catalysts> [Missing:
room temperature <Thermodynamic properties>]
extractnamedentitiesfromtheinputsentence
Sentence Reconstructed from Name Tagging Results
andaseq2seqself-validationmoduletorecon-
Ligand screening is conducted to identify suitable ligands for Pd-catalyzed
structtheoriginalinputsentencefromextracted Suzuki-Miyaura reactions, which are coupling reactions known for their
efficacy in the synthesis of aryl sulfamates, acting as catalysts in the
entities. Inspiredbythefactthatagoodentity
process. [Missing: room temperature <Thermodynamic properties>]
extractionsystemneedstoextractentitiesfaith-
fully,ournewself-validationmoduleleverages
Figure 1: Comparison of sentence reconstruction re-
entityextractionresultstoreconstructtheorigi-
sultsfromgroundtruthandInBoXBART(Parmaretal.,
nalinputsentence. Besides,wedesignanew
2022). WehighlightCompleteCorrect,MissedEntity,
contrastive loss to reduce excessive copying
andPartiallyCorrectPredictionwithdifferentcolor.
duringtheextractionprocess. Finally, were-
leaseChemNER+,anewfine-grainedchemical
entityextractiondatasetthatisannotatedbydo-
thelimitedreadingabilityofhumans. Therefore,in-
mainexpertswiththeChemNERschema. Ex-
formationextraction,especiallyentityextractionof
perimentsinfew-shotsettingswithbothChem-
NER+ and CHEMET datasets show that our fine-grainedscientificentitytypes,becomesacru-
newlyproposedframeworkhascontributedup cialsteptoautomaticallycatchupwiththenewest
to 8.26% and 6.84% absolute F1-score gains researchfindingsinthechemicaldomain.
respectively1.
Despitesuchapressingneed,fine-grainedentity
extraction in the chemical domain presents three
1 Introduction
distinctiveandnon-trivialchallenges. First,there
are very few publicly available benchmarks with
Millions of scientific papers are published an-
nually2, resulting in an information overload high-qualityannotationsonfine-grainedchemical
entitytypes. Forexample,ChemNER(Wangetal.,
(VanNoorden,2014;Landhuis,2016). Duetosuch
2021a)developedthefirstfine-grainedchemistry
anexplosionofresearchdirections,itisimpossible
entity extraction dataset. However, their dataset
forscientiststofullyexplorethelandscapedueto
isnotreleasedpublicly. Toaddressthisissue,we
1Theprograms,data,andresourcesarepubliclyavailable collaboratewithdomainexpertstoannotateChem-
forresearchpurposesat:https://github.com/EagleW/Ch NER+, a new chemical entity extraction dataset
em-FINESE.
based on the ChemNER ontology. Besides, we
2https://esperr.github.io/pubmed-by-year/abou
t.html constructanothernewfine-grainedentityextraction
4202
naJ
81
]LC.sc[
1v98101.1042:viXradatasetbasedonanexistingentitytypingdataset Chemical FINe-grained Entity extraction with
CHEMET(Sunetal.,2021). SElf-validation(Chem-FINESE).Specifically,our
Chem-FINESE has two parts: a seq2seq entity
extractor to extract named entities from the in-
1.0 Dataset
ChemNER+ putsentenceandaseq2seqself-validationmodule
0.8 CHEMET to reconstruct the original input sentence based
0.6 on the extracted entities. First, we employ a
seq2seq model to extract entities from the input
0.4
sentence,sinceitdoesnotrequireanytask-specific
0.2 component and explicit negative training exam-
ples (Giorgi et al., 2022). We generate the entity
0.0
extractionresultsasaconcatenationofpairs,each
0 10 20 30 40 50
Relative Rank of Types consistingofanentitymentionanditscorrespond-
ingtype,asshowninFigure1.
Figure 2: Type distributions for the training sets of
Onecriticalissueforseq2seqentityextraction
ChemNER+andCHEMETdatasets. TheY-axisrepre-
isthatthelanguagemodeltendstomissimportant
sentsthenumberofmentionsnormalizedbythemen-
tionsofthemostfrequenttype. TheX-axisrepresents entitiesorexcessivelycopyoriginalinput. Forex-
therankoftypes. ample,theseq2seqentityextractionresultsmissed
thetypethermodynamicpropertiesandgenerated
Inaddition,currententityextractionsystemsin “ligandscreening”inFigure1. However,thegoalof
few-shot settings face two main problems: miss- information extraction isto provide factual infor-
ing mentions and incorrect long-tail predictions. mationandknowledgecomprehensively. Inother
One primary reason for missing mentions is that words, if the model extracts knowledge precisely,
the sentences in scientific papers typically cover readers should be able to faithfully reconstruct
moreentitiesthansentencesinthegeneraldomain. theoriginalsentenceusingtheextractionresults.
For example, there are 3.1 entities per sentence Inspired by such a goal, to evaluate whether the
inourChemNER+dataset,whichismuchhigher seq2seqentityextractorhasfaithfullyextractedim-
thanthe1.5entitiesinthegeneraldomaindataset portant information, we propose a novel seq2seq
CONLL2003 (Tjong Kim Sang and De Meulder, self-validationmoduletoreconstructtheoriginal
2003). As a result, it is more difficult for entity sentences based on entity extraction results. As
extractionmodelstocoverallmentionsintheinput showninFigure1,thesentencereconstructedfrom
sentences. As shown in Figure 1, since the in- thegroundtruthisclosertotheoriginalinputthan
puthasalreadyincludedfourchemicalentities,In- the sentence reconstructed from entity extraction
BoXBARTmodel(Parmaretal.,2022)completely results, which misses the reaction condition and
missestheentity“roomtemperature”. introducesadditionalinformationthattreatedthe
Furthermore, entity distributions in the chemi- “arylsulfamates”ascatalysts. Additionally,wein-
cal domain are highly imbalanced. As shown in troduce a new entity decoder contrastive loss to
Figure2,weobservethattheentitytypedistribu- control the mention spans. We treat text spans
tionsofChemNER+andCHEMETexhibitsimilar containingentitymentionsashardnegatives. For
long-tailpatterns. Infew-shotsettings,entitieswith instance,giventhegroundtruthentity“arylsulfa-
long-tailtypesareextremelydifficulttoextractdue mates”, we will treat “aryl sulfamates at room
toinsufficienttrainingexamples. Forexample,as temperature”asahardnegative.
shown in Figure 1, InBoXBART mistakenly pre- Ourextensiveexperimentsdemonstratethatour
dicts the entity “aryl sulfamates” as catalyst, be- proposedframeworksignificantlyoutperformsour
cause its type has a frequency forty times lower baseline model by up to 8.26% and 6.84% abso-
than the predicted type (i.e., 4 vs 136). More- luteF1-scoregainsonChemNER+andCHEMET
over, the diverse representation nature of chemi- datasetsrespectively. Ouranalysisalsoshowsthat
cal entities—such as trade names, trivial names, Chem-FINESEcaneffectivelylearntoselectcor-
and semi-systematic names (e.g., THF, iPrMgCl, rect mentions and improve long-tail entity type
8-phenylring)—makesitevenharderformodels performance. Toevaluatethegeneralizationabil-
togeneralizeontheselong-tailentities. ityofourproposedmethod,wealsoevaluateour
Toaddressthesechallenges,weproposeanovel frameworkonCrossNER(Liuetal.,2021),which
snoitneM
fo
rebmuN
evitaleRis based on Wikipedia. Our Chem-FINESE still Through application of ligand screening, we
describe the first examples of Pd-catalyzed
outperformsotherbaselinesinallfivedomains.
Suzuki–Miyaura reactions using aryl
Ourcontributionsarethreefold: sulfamates at room temperature.
Entity Extraction
1. We propose two few-shot chemical fine-
Encoder A
grained entity extraction datasets, based on
Entity Decoder
Decoder A
human-annotatedChemNER+andCHEMET. Contrastive Loss
ligand <Ligands>, Pd-
2. Weproposeanewframeworktoaddressthe
Reconstruction catalyzed Suzuki-Miyaura Supervised
mentioncoverageandlong-tailedentitytype Loss reactions <Coupling Loss
reactions>, ...
problemsinchemicalfine-grainedentityex-
tractiontasksthroughanovelself-validation Self-Validation
Encoder B
module and a new entity extractor decoder
contrastiveobjective. Ourmodeldoesnotre- Decoder B
quireanyexternalknowledgeordomainadap-
tivepretraining. Through application of ligand screening, we
describe the first examples of Pd-catalyzed
Suzuki–Miyaura reactions using aryl
3. Ourextensiveexperimentsonbothchemical sulfamates at room temperature.
few-shotfine-graineddatasetsandtheCross-
Figure3: Architectureoverview. Weusetheexamplein
NER dataset justify the superiority of our
Figure1asawalking-throughexample.
Chem-FINESEmodel.
2 TaskFormulation
copying. The entiremodel istrainedwith acom-
binationofthesupervisedloss,thereconstruction
Following Giorgi et al. (2022), we formulate en-
loss,andtheentitydecodercontrastiveloss.
tityextractionasasequence-to-sequence(seq2seq)
generationtaskbytakingasourcedocumentS as
3.2 EntityExtractionModule
input. ThemodelgeneratesoutputY,atextconsist-
Our entity extraction module follows a seq2seq
ingofaconcatenationofnfine-grainedchemical
setup (Yan et al., 2021; Giorgi et al., 2022). For-
entitiesE ,E ,...,E . EachmentionE includes
1 2 n i
mally, we use the state-of-the-art coarse-grained
the mention µ in the source document S and its
i
chemical entity extractor InBoXBART (Parmar
entitytypeρ ∈ P,whereP isasetcontainingall
i
etal.,2022)asthebackbone. Wemodelthecondi-
entitytypes. Specifically,weproposethefollowing
tionalprobabilityofextractingentitiesfromsource
outputlinearizationschema: giventheinputS,the
sequenceS as
output is Y = µ < ρ >,µ < ρ >,...,µ <
1 1 2 2 n
ρ n >. Wefurtherillustratedthiswithanexample: T
(cid:89)
S:Throughapplicationofligandscreening,wedescribethe p(Y|S) = p(y t|S,y <t), (1)
first examples of Pd-catalyzed Suzuki–Miyaura reactions t=1
usingarylsulfamatesatroomtemperature.
wheretheoutputY hasalengthofT,andy isthe
t
Y: ligand <Ligands>, Pd-catalyzed Suzuki–Miyaura
predictedtokenattimetintheoutputY.
reactions<Couplingreactions>,arylsulfamates<Aromatic
Wesupervisetheentityextractionusingthestan-
compounds>,roomtemperature<Thermodynamicproperties>
dardcross-entropyloss:
T
(cid:88)
3 Method L = logp(y |S,y ). (2)
gen t <t
t=1
3.1 ModelArchitecture
3.3 Self-validationModule
The overall framework is illustrated in Figure
3. Given the source document S, we first use a Sinceagoodinformationextractionsystemneeds
seq2seqmodeltoextractfine-grainedchemicalen- to extract entities faithfully, we propose a self-
tities. Then,weproposeanewself-validationmod- validationmoduletoreconstructtheoriginalsen-
uletoreconstructtheoriginalinputbasedonentity tencefromtheextractedentitiestocheckwhether
extractionresults. Finally,weintroduceanewen- the model overlooks any entities. Different from
tity decoder contrastive loss to reduce excessive previousduallearningarchitectures(Iovineetal.,2022), which use dual cycles or reinforcement whereH¯+ andH¯− aredecoderhiddenstatesfrom
i
learning to provide feedback, we use Gumbel- the positive and i-th negative samples, W is a
x
softmax(GS)estimator(Jangetal.,2017)toavoid learnable parameter, τ is the temperature, and
the non-differentiable issue in explicit decoding. Avg(∗)denotestheaveragepoolingfunction.
Specifically,basedonInBoXBART(Parmaretal.,
3.5 TrainingObjective
2022), we first pretrain a seq2seq self-validation
module that takes in the entity extraction results Wejointlyoptimizethecross-entropyloss,recon-
Y and generates a reconstructed sentence Sˆ. We structionloss,andentitydecodercontrastiveloss:
useourtrainingsettopretraintheself-validation
module. We fix the weight of the self-validation L = L gen+αL recon+βL cl, (6)
moduleafterpretraining. Inthetrainingstage,the
inputembeddingH oftheself-validationmodule where α,β are hyperparameters that control the
t
isgivenby: weightsofthereconstructionlossandcontrastive
lossrespectively.
H = GS(p(y |S,y ))·E , (3)
t t <t v
whereE isthevocabularyembeddingmatrixand Dataset Split #Pair #Token #Entity
v
GS is the Gumbel-softmax estimator. The total Train 542 32.9 3.10
ChemNER+ Valid 100 39.9 4.57
inputembeddingsfortheself-reconstructionmodel
Test 100 39.4 4.61
isH = [H ;H ;...;H ].
1 2 T Train 6,561 37.8 1.57
Thereconstructionlossis: CHEMET Valid 520 31.6 2.15
Test 663 36.6 1.95
Tˆ
(cid:88)
L recon = logp(sˆ tˆ|H,sˆ <tˆ), (4) Table1: Statisticsofourdataset. #Tokendenotesav-
tˆ=1 eragenumberofwordspersentence. #Entitydenotes
averagenumberofentitiespersentence.
wherethereconstructedsentenceSˆhasalengthof
Tˆ,andsˆ isthepredictedtokenattimetˆinSˆ.
tˆ
4 BenchmarkDataset
3.4 ContrastiveEntityDecodingModule
Entityextractiondatasetsinthescientificdomain 4.1 DatasetCreation
usually contain more entities for each sentence. ChemNER+ Dataset. Since the annotation of
Fromtheinitialexperiments,wefoundthattheen-
ChemNER dataset is not fully available online,
tityextractionmoduletendstogenerateincorrect
we decide to create our own dataset, Chem-
mentionsbyassociatingitwithunrelatedcontexts
NER+, basedon availablesentences fromChem-
to help the reconstruction of the self-validation
NER(Wangetal.,2021a)dataset. Followingthe
module. For example, given the example in Fig-
schemaofChemNER,weasktwoChemistryPh.D.
ure1,thebaselinemodelgenerates“ligandscreen-
students to annotate a new dataset, covering 59
ing”insteadof“ligand”. Therefore,weintroduce fine-grainedchemistrytypeswith742sentences3.
anewdecodingcontrastivelossinspiredbyWang
et al. (2023a) to suppress excessive copying. We CHEMET Dataset. We construct a new fine-
constructnegativesamplesbycombiningmentions grained entity extraction dataset based on
withsurroundingunrelatedcontexts. Forexample, CHEMET (Sun et al., 2021). For any entity in
we will consider “ligand screening, we describe the training set that overlaps with the validation
the first examples” as a negative of entity “lig- and testing sets, we replace its multi-labels with
and”. Wetreattheoriginalmentiontypepairsas the most frequent types that appear in the valida-
the ground truth and maximize their probability tionandtestingsets. Forotherentities,wereplace
withInfoNCEloss(Oordetal.,2018): theremainingtypeswiththeirmostfrequenttypes
that appeared in the training set. We merge the
entity types with the same subcategory name in
exp(x+/τ)
L cl = (cid:80) exp(cid:0) x−/τ(cid:1) +exp(x+/τ), CHEMET (Sun et al., 2021). The final dataset
i i consistsof30fine-grainedorganicchemicaltypes.
(5)
x+ = σ(Avg(W xH¯++b x)), Table1showsthedetaileddatastatistics.
x− = σ(Avg(W H¯−+b )),
i x i x 3HumanannotationdetailsareinAppendixE.k-shot 6 9 12 15 18 uments,seq2seqmodelsgenerallyachievehigher
performance in low-resource settings with fewer
RoBERTa 8.09 7.98 8.00 16.22 7.94
PubMedBERT 5.48 5.12 5.77 5.46 5.88 parameters as shown in Table 11. These results
ScholarBERT 23.96 29.82 27.65 31.48 32.76
demonstratethatseq2seqmodelshaveabettergen-
InBoXBART 26.23 27.89 28.83 33.64 30.39
eralizationabilityinfew-shotsettings.
+Valid 32.40 31.13 33.64 35.31 36.44
+Valid+CL 33.11 32.75 34.75 37.89 38.65
k-shot 6 9 12 15 18
Table2: micro-F1(%)scoresforChemNER+withfew-
InBoXBART 36.96 38.22 38.34 47.91 42.84
shot settings. Valid is a model with a self-validation +Valid 45.07 45.28 41.56 48.15 46.15
module. CLisamodelwithadecodercontrastiveloss. +Valid+CL 45.58 44.03 45.25 51.68 47.88
Table4: Mentionmicro-F1(%)scoresforChemNER+
k-shot 6 9 12 15 18
withfew-shotsettings.
RoBERTa 4.91 4.16 4.79 4.83 4.81
PubMedBERT 4.07 4.67 3.87 4.47 3.96
ScholarBERT 17.00 33.63 29.65 29.72 32.52
InBoXBART 29.93 29.57 31.76 36.16 37.52 k-shot 6 9 12 15 18
+Valid 32.74 34.09 33.30 40.81 38.37
InBoXBART 46.74 42.07 44.32 47.58 52.90
+Valid+CL 33.81 36.41 36.11 40.52 39.94
+Valid 47.87 46.01 44.18 50.55 50.50
+Valid+CL 48.96 49.83 47.03 50.61 54.10
Table3: micro-F1(%)scoresforCHEMETwithfew-
shotsettings. Table5: Mentionmicro-F1 (%)scoresfor CHEMET
withfew-shotsettings.
4.2 Few-shotSetup
Additionally, the self-validation variants sig-
For each dataset, we randomly sample a subset nificantly outperform the baseline InBoXBART,
basedonthefrequencyofeachtypeclass. Specif- showing the benefit of the self-validation mod-
ically, given a dataset, we first set the number of ule in capturing mentions. Moreover, our self-
maximumentitymentionsk forthemostfrequent validationmodulecaneffectivelyenhancetheper-
entitytypeinthedataset. Wethenrandomlysample formance of the entity extraction module in ex-
othertypesandensurethatthedistributionofeach tremely low-resource settings. In 6-shot scenar-
typeremainsthesameasintheoriginaldataset. We ios for both ChemNER+ and CHEMET datasets,
choose the values 6,9,12,15,18 as the potential ourmodelachievesimpressiveperformancecom-
maximumentitymentionsfork. TheChemNER+ paredtoScholarBERT,whichfurtherverifiesthe
andCHEMETfew-shotdatasetscontain52and28 effectivenessoftheself-validationmodule. Finally,
typesrespectively. adding decoder contrastive loss helps the model
performsignificantlybetterinTable2,suggesting
5 Experiments thatcontrastivelearningfurtherhelpsthemention
extractionqualitybyreducingexcessivecopying.
5.1 Baselines
Interestingly,weobservethatdecodercontrastive
We compare our model with (1) state-of-the- learningimproveslessinTable3thaninTable2,
artpretrainedencoder-basedmodelsincluding becausetheCHEMETcontainsfewerentitiesper
RoBERTa (Liu et al., 2019) and models with do- sentencecomparedtotheChemNER+.
mainadaptivetraining,suchasPubMedBERT(Gu
etal.,2021)andScholarBERT(Hongetal.,2023). ChemNER+ CHEMET
2.6 1.35
Since we use InBoXBART (Parmar et al., 2022) 2.4 1.30 1.25
as our backbone, we also include (2) baselines 2.2 1.20
for ablation. The hyperparameters, training and 6 8 10 k-s1 h2 ot 14 16 18 6 8 10 k-s1 h2 ot 14 16 18
InBoXBART +Valid +Valid+CL
evaluationdetailsarepresentedinAppendixA.
Figure 4: Average tokens in each mention for Chem-
5.2 OverallPerformance NER+andCHEMETdatasetswithfew-shotsettings.
Tables2,3showthatourmodelsoutperformbase-
linesforfew-shotsettingsbyalargemargin. Com- PerformanceofMentionExtraction. Wecalcu-
paredtothebestpretrainedencoder-basedScholar- late the mention F1 scores in Tables 4 and 5. In
BERT,pretrainedon221Btokensofscientificdoc- addition,wealsotestafullyunsupervisedmention
snekoT
egarevAextraction based on AMR-Parser (Fernandez As- examplesindicatesthatourmodelachievessatisfac-
tudilloetal.,2020)4. TheF1-scoresare38.22and toryperformanceforlong-tailentities,evenwitha
45.33forChemNER+andCHEMET,respectively. limitedtrainingsample.
Theseresultsimplythattheself-validationmodel
generally improves the mention extraction accu- 6 Analysis
racy. Moreover, adding decoder contrastive loss
6.1 QualitativeAnalysis
generallyfurtherbolstersthementionF1scoreby
reducingthenumberoftokensthatappearineach Table 8 shows two typical examples from the 18-
mention,asshowninFigure4. shotChemNER+datasetthatillustratehowincor-
poratingaself-validationmoduleanddecodercon-
k-shot 6 9 12 15 18 trastivelosscanimprovethementioncoverageand
long-tailentityperformance.
RoBERTa 2.04 2.05 2.05 0.00 2.05
PubMedBERT 2.05 0.00 0.00 2.13 0.00 Inthefirstexample, theInBoXBARTbaseline
ScholarBERT 0.00 9.28 4.71 0.00 6.90 failstoidentifyboth“cyclophanes”and“polycy-
InBoXBART 8.33 11.36 15.22 17.14 7.69
cles”, probably because the input sentence con-
+Valid 10.81 12.24 10.26 9.76 23.81
+Valid+CL 26.19 23.91 23.26 19.05 25.00 tains too many entities. With the help of the
self-validation module, the InBoXBART+Valid
Table6: micro-F1(%)scoresforlong-tailentitytypes
model successfully captures the first entity “cy-
ChemNER+withfew-shotsettings.
clophanes”. However, it still cannot recognize
“polycycles”. Additionally, both the baseline and
theInBoXBART+Validmodelmistakenlytreatthe
k-shot 6 9 12 15 18
entity“Suzukicross-couplingandmetathesis”and
RoBERTa 0.00 0.00 0.00 0.00 0.00
theentity“metathesis”,becausethosemodelsex-
PubMedBERT 0.00 0.00 0.00 0.00 0.00
ScholarBERT 0.00 0.00 0.00 0.00 0.00 cessivelycopyfromtheoriginalsentence. Incon-
InBoXBART 4.90 7.55 4.55 5.05 12.26 trast,byaddingthedecodercontrastiveloss,which
+Valid 8.72 13.10 4.55 16.96 20.83
usesthementionswithsurroundingunrelatedcon-
+Valid+CL 7.07 11.32 8.33 5.15 23.01
textsasnegatives,themodelsuccessfullyseparates
Table7: micro-F1(%)scoresforlong-tailentitytypes theentity“Suzukicross-couplingandmetathesis”
CHEMETwithfew-shotsettings. Theencoder-based fromtheentity“metathesis”.
modelsfailtoextractlong-tailentitytypesforallfew- In the second example, both the baseline and
shot settings. Compared to encoder-based models,
theInBoXBART+Validmodelpredictaverylong
seq2seqmodelscanutilizelabelsemanticsinthegen-
textspanthattreatsthreeentitiesasasingleentity.
eration procedure. Therefore, encoder-based models
They also fail to capture “asymmetric catalysis”
requiremoretrainingdataunderfew-shotsettings.
and “highly enantioselective process” as entities
becausetheirtypeshavelowfrequencyinthetrain-
Performance of Long-tail Entity. To evaluate ingset. Withthehelpofdecodercontrastiveloss,
theperformanceoflong-tailentities,wefirstrank the model reduces the excessive copying of the
entitytypesbytheirfrequency. Wethenselectthe entity extraction module while trying to capture
entitytypesthatappearinthelower50%andcal- importantentitiesasaccuratelyaspossible. There-
culate the F1 scores of those types5. The results fore,themodelsuccessfullyclassifies“asymmetric
areinTables6and7. Notably,ourproposedmeth- catalysis”asCatalysiscorrectlyandalsopredicts
odsgreatlyoutperformtheencoder-basedbaselines. “enantioselectiveprocess”asanentity.
Boththeself-verificationmoduleandthedecoder
contrastivelossaidtheentityextractionmodulein 6.2 CompatiblewithOtherFew-shot
focusingonlong-tailentitiesbycreatingamorebal- Datasets?
anceddistributionofentitytypes. Themajorreason
CrossNER Dataset. In the above experiments,
fortherelativelylowperformanceinTable7isthat
wefocusonthefew-shotsettingsforchemicalpa-
thedifferencesbetweenthetypesinCHEMETare
pers and prove the effectiveness of our proposed
notsignificant. Therelativelystableperformance
framework. Toevaluatethegeneralizationability
ofourmodelinTable6acrossincreasingfew-shot
ofourproposedframeworkonotherdomains,we
4ImplementationdetailsareinAppendixA. conductexperimentsontheCrossNERdataset(Liu
5EntityfrequencyandselectedtypesareinAppendixB. etal.,2021). ThedetailedstatisticsareinTable9.InBoXBART Severalcyclophanes,polycycles,... havebeensynthesizedbyemployingacombinationofSuzukicross-
couplingandmetathesis .
Couplingreactions
+Valid Severalcyclophanes ,polycycles,...havebeensynthesizedbyemployingacombinationof
Heterocycliccompounds
Suzukicross-couplingandmetathesis .
Organicreactions
+Valid+CL Severalcyclophanes ,polycycles ,...havebeensynthesizedbyemployingacombi-
Heterocycliccompounds Biomolecules
nationofSuzukicross-coupling andmetathesis .
Couplingreactions Chemicalproperties
Ground Severalcyclophanes ,polycycles ,...havebeensynthesizedbyemployingacombi-
Aromaticcompounds Organicpolymers
Truth nationofSuzukicross-coupling andmetathesis .
Couplingreactions Substitutionreactions
InBoXBART ...withtheadvantagesofasymmetriccatalysis(stepandatomeconomy)inarareexampleofanenantioselec-
tivecrosscouplingofaracemicelectrophilebearinganoxygenleavinggroup ...theidentificationofa
Catalysis
highlyenantioselectiveprocess.
+Valid ...withtheadvantagesofasymmetriccatalysis(stepandatomeconomy)inarareexampleofanenantiose-
lectivecrosscouplingofaracemicelectrophilebearinganoxygenleavinggroup ...the
Organometalliccompounds
identificationofahighlyenantioselectiveprocess
+Valid+CL ...with the advantages of asymmetric catalysis (step and atom economy) in a rare example of an
Catalysis
enantioselectivecrosscouplingofaracemicelectrophilebearinganoxygenleavinggroup ...the
Functionalgroups
identificationofahighlyenantioselectiveprocess .
Chemicalproperties
Ground ... with the advantages of asymmetric catalysis ( step and atom economy ) in a rare example of
Catalysis
Truth anenantioselectivecrosscoupling ofaracemicelectrophile bearinganoxygen
Couplingreactions Organiccompounds
leavinggroup ...theidentificationofahighlyenantioselectiveprocess .
Functionalgroups Catalysis
Table8: Examplesshowinghowtheself-validationmoduleandentitydecodercontrastivelossimprovesthemodel
performance. WehighlightCompleteCorrect,MissedEntity,andPartiallyCorrectPredictionwithdifferentcolor.
Comparedtootherbaselines,our+Valid+CLsuccessfullycapturesentitieswhereotherbaselinesmiss.
Weremovesentenceswithoutanyentity. Because that the model achieves the largest gain for the
theCrossNERdatasetisbasedonWikipediaarti- AI domain and the smallest gain for the politics
cles, we choose RoBERTa and ScholarBERT as domain. The major reason behind this is that AI
encoder-based baselines. Additionally, we select domaincontainsthemostinformativeentitytypes,
BART-base (Lewis et al., 2020) as the backbone whichcoverthekeypointsofthesentence,includ-
forourablationvariations. ingalgorithm, task, etc. Onthecontrary, thepol-
itics domain contains many names of politicians
Dom. Train Valid Test #Type #Token #Entity and locations, which require background knowl-
AI 100 350 430 14 31.5 4.42 edgefortheself-verificationmoduletoidentify.
Lit. 99 400 416 12 37.6 5.39
Mus. 100 380 465 13 41.4 7.05
6.3 RemainingChallenges
Pol. 200 541 651 9 43.5 6.46
Sci. 200 450 543 17 35.8 5.62
MisleadingSubwords. Weobservethatthemen-
tion text can sometimes mislead the type predic-
Table 9: Statistics of CrossNER. Dom. denotes the
tions, especially if the type contains a subword
domainofthedataset.
from the mention. As a result, the model fails to
identifythetypecorrectly. Forexample,giventhe
mention“unnaturalaminoacidderivatives”,our
Model AI Lit. Mus. Pol. Sci.
modelfocusesontheword“acid”andpredictsthe
RoBERTa 60.88 67.51 59.07 63.79 60.96
ScholarBERT 56.99 59.35 52.26 57.15 57.01 entitytobeOrganicacidsinsteadofOrganonitro-
BART-base 59.20 66.90 62.78 67.99 62.18 gencompounds. Thepotentialreasonbehindthis
+Valid 61.84 67.97 60.94 67.22 62.40
isthattheBARTmodelincorrectlyassociatesthe
+Valid+CL 62.48 68.22 63.39 68.03 62.87
“acid” in the mention with Organic acids. Such
Table10: F1(%)scoresforCrossNER. typeerrorsmightbeincorporatedintothedecoder
contrastivelearningasadditionalhardnegatives.
Results. As shown in Table 10, our model con- Fine-grained Type Classification. The model
sistentlyproducesthebestF1scoresacrossallfive tendstopredictgenericentitytypesinsteadofmore
domainsofCrossNERwithoutanyexternalknowl- fine-grainedentitytypes. Forinstance,themodel
edgeordomainadaptivepretraining. Weobserve predictsthemention“Cs2CO3”asInorganiccom-pounds instead of Inorganic carbon compounds. achievescompetitiveperformancewithoutexternal
Thisissuemightcomefromannotationambiguity knowledgeordomainadaptivetraining.
in the training set. Additionally, the model pre-
Cycle Consistency. Cycle consistency, namely
dictstypesthatarenotinthepredefinedontology.
structural duality, leverages the symmetric struc-
Forinstance,themodellabels“GK”asGenecyclic
ture of tasks to facilitate the learning process. It
compounds instead of Enzymes. This error can
hasemergedasaneffectivewaytodealwithlow-
possiblybesolvedbyconstraintdecoding.
resourcetasksinnaturallanguageprocessing. First
7 RelatedWork introducedinmachinetranslation(Heetal.,2016;
Cheng et al., 2016; Lample et al., 2018; Mohiud-
Scientific Entity Extraction. Entity extraction din and Joty, 2019; Xu et al., 2020) to deal with
for scientific papers has been widely exploited thescarcityofparalleldata,cycleconsistencyhas
in the biomedical domain (Nguyen et al., 2022; beenexpandedtoothernaturallanguageprocessing
Labrak et al., 2023; Cao et al., 2023; Li et al., tasks,includingsemanticparsing(Caoetal.,2019;
2023b; Hiebel et al., 2023) and the computer sci- Yeetal.,2019),naturallanguageunderstanding(Su
encedomain(Luanetal., 2018;Jainetal.,2020; et al., 2019; Tseng et al., 2020; Su et al., 2020),
Viswanathan et al., 2021; Shen et al., 2021; Ye and data-to-text generation (Dognin et al., 2020;
et al., 2022; Jeong and Kim, 2022; Hong et al., Guo et al., 2020; Wang et al., 2023b). Recently,
2023). Despite this, fine-grained scientific entity Iovine et al. (2022) successfully apply the cycle
extraction (Wang et al., 2021a) in the chemical consistencytoentityextractionbyintroducingan
domainreceiveslessattentionduetothescarcity iterativetwo-stagecycleconsistencytrainingproce-
ofbenchmarkresources. Mostbenchmarksinthe dure. Despitetheseefforts,thenon-differentiability
chemical (Krallingeretal.,2015;Kimetal.,2015) of the intermediate text in the cycle remains un-
only provide coarse-grained entity types. In this solved, leading to the inability to propagate the
paper, we address this problem by releasing two lossthroughthecycle. Toaddressthisissue,Iovine
new datasets for chemical fine-grained entity ex- etal.(2022)andWangetal.(2023b)alternatively
traction based on the ChemNER schema (Wang freezeoneofthetwomodelsintwoadjacentcycles.
et al., 2021a) and CHEMET dataset (Sun et al., Onthecontrary,weintroducethegumbel-softmax
2021). estimatortoavoidthenon-differentiableissue. Ad-
ditionally, we reduce the dual cycle training into
Few-shot Entity Extraction. Few-shot learn-
end-to-endtrainingtosavetimeandcomputation
ing attracts growing interest, especially for low-
resources.
resource domains. Previous improvements for
few-shotlearningcanbedividedintoseveralcat-
8 ConclusionandFutureWork
egories: domain-adaptivetrainingbytrainingthe
model in the same or similar domains (Liu et al., Inthispaper,weintroduceanovelframeworkfor
2021;Ohetal.,2022),prototypelearningbylearn- chemical fine-grained entity extraction. Specifi-
ing entity type prototypes (Ji et al., 2022; Oh cally,wetargettwouniquechallengesforfew-shot
etal.,2022;Maetal.,2023),prompt-basedmeth- fine-grained scientific entity extraction: mention
ods (Lee et al., 2022; Xu et al., 2023; Nookala coverageandlong-tailentityextraction. Webuilda
etal.,2023;Yangetal.,2023;Chenetal.,2023b), newself-validationmoduletoautomaticallyproof-
data-augmentation(Caietal.,2023;Ghoshetal., read the entity extraction results and a novel de-
2023), code generation (Li et al., 2023a), meta- coder contrastive loss to reduce excessive copy-
learning (de Lichy et al., 2021; Li et al., 2022; ing. Experimentalresultsshowthatourproposed
Ma et al., 2022), knowledge distillation (Wang model achieves significant performance gains on
etal.,2021c;Chenetal.,2023a),contrastivelearn- two datasets: ChemNER+ and CHEMET. In the
ing (Das et al., 2022), and external knowledge future, weplantoexploreincorporatinganexter-
including label definitions (Wang et al., 2021b), nalknowledgebasetofurtherimprovethemodel’s
AMRgraph(Zhangetal.,2021),andbackground performance. Specifically, we plan to inject type
knowledge(Laietal.,2021). Incontrasttothese definition into the representation to facilitate the
methods,ourapproachformulatesthetaskinatext- entityextractionprocedure. Wewillalsocontinue
to-textframework. Inaddition,weintroduceanew exploringtheuseofconstraintdecodingtofurther
simplebuteffectiveself-validationmodule,which improveentityextractionquality.9 Limitations References
JiongCai,ShenHuang,YongJiang,ZeqiTan,Pengjun
9.1 LimitationsofDataCollections
Xie,andKeweiTu.2023. Graphpropagationbased
Both ChemNER+ and CHEMET are based on dataaugmentationfornamedentityrecognition. In
Proceedings of the 61st Annual Meeting of the As-
papers about Suzuki Coupling reactions from
sociationforComputationalLinguistics(Volume2:
PubMed6. Our fine-grained entity extraction
ShortPapers),pages110–118,Toronto,Canada.As-
datasets are biased towards the topics and ontol- sociationforComputationalLinguistics.
ogyprovidedbyChemNER+andCHEMET.For
JiarunCao,NielsPeek,AndrewRenehan,andSophia
example, CHEMET only focuses on the organic
Ananiadou.2023. Gaussiandistributedprototypical
compounds. Thenumberofavailablesentencesis
networkforfew-shotgenomicvariantdetection. In
limitedbytheoriginaldatasetandourannotation The22ndWorkshoponBiomedicalNaturalLanguage
efforts. WecurrentlyonlyfocusontheEnglishsen- ProcessingandBioNLPSharedTasks,pages26–36,
Toronto,Canada.AssociationforComputationalLin-
tences. Weonlytestourmodelonchemicalpapers
guistics.
(i.e., ChemNER+ and CHEMET) and Wikipedia
(CrossNER). In the future, we aim to adapt our Ruisheng Cao, Su Zhu, Chen Liu, Jieyu Li, and Kai
modelforcategoriesinotherlanguages. Yu.2019. Semanticparsingwithduallearning. In
Proceedingsofthe57thAnnualMeetingoftheAsso-
ciationforComputationalLinguistics,pages51–64,
9.2 LimitationsofSystemPerformance
Florence,Italy.AssociationforComputationalLin-
Our few-shot learning framework currently re- guistics.
quires defining the entity ontology and few-shot
Jiawei Chen, Yaojie Lu, Hongyu Lin, Jie Lou, Wei
examplesbeforeperforminganytrainingandtest- Jia,DaiDai,HuaWu,BoxiCao,XianpeiHan,and
ing. Therefore,duetopatternsinthepretrainingset, Le Sun. 2023a. Learning in-context learning for
ourmodelmightproducementiontypesthatdon’t namedentityrecognition. InProceedingsofthe61st
AnnualMeetingoftheAssociationforComputational
alignwithourpredefinedontology. Forinstance,it
Linguistics(Volume1: LongPapers),pages13661–
maygenerateCyclopentadienylcompoundsinstead
13675,Toronto,Canada.AssociationforComputa-
ofthepredefinedtypeCyclopentadienylcomplexes. tionalLinguistics.
Furthermore, the pretrained model might empha-
Yanru Chen, Yanan Zheng, and Zhilin Yang. 2023b.
sizelanguagemodelingoveraccuratelyidentifying
Prompt-based metric learning for few-shot NER.
entirechemicalphrases. Forexample,itmightrec- In Findings of the Association for Computational
ognize Pd in the catalyst Pd(OAC)2 simply as a Linguistics: ACL2023,pages7199–7212,Toronto,
Canada.AssociationforComputationalLinguistics.
transitionmetal.
YongCheng,WeiXu,ZhongjunHe,WeiHe,HuaWu,
Acknowledgement MaosongSun,andYangLiu.2016. Semi-supervised
learningforneuralmachinetranslation. InProceed-
This work is supported by the Molecule Maker ingsofthe54thAnnualMeetingoftheAssociationfor
LabInstitute: anAIresearchinstituteprogramsup- ComputationalLinguistics(Volume1: LongPapers),
pages1965–1974,Berlin,Germany.Associationfor
portedbyNSFunderawardNo. 2019897,andby
ComputationalLinguistics.
DOECenterforAdvancedBioenergyandBioprod-
uctsInnovationU.S.DepartmentofEnergy,Office Sarkar Snigdha Sarathi Das, Arzoo Katiyar, Rebecca
ofScience,OfficeofBiologicalandEnvironmen- Passonneau, and Rui Zhang. 2022. CONTaiNER:
Few-shot named entity recognition via contrastive
talResearchunderAwardNumberDESC0018420.
learning. InProceedingsofthe60thAnnualMeet-
The views and conclusions contained herein are ingoftheAssociationforComputationalLinguistics
thoseoftheauthorsandshouldnotbeinterpreted (Volume1: LongPapers),pages6338–6353,Dublin,
asnecessarilyrepresentingtheofficialpolicies,ei- Ireland.AssociationforComputationalLinguistics.
therexpressedorimpliedof,theNationalScience
CypriendeLichy,HadrienGlaude,andWilliamCamp-
Foundation, the U.S. Department of Energy, and bell.2021. Meta-learningforfew-shotnamedentity
the U.S. Government. The U.S. Government is recognition. InProceedingsofthe1stWorkshopon
MetaLearningandItsApplicationstoNaturalLan-
authorizedtoreproduceanddistributereprintsfor
guageProcessing,pages44–58,Online.Association
governmentalpurposesnotwithstandinganycopy-
forComputationalLinguistics.
rightannotationtherein.
Pierre Dognin, Igor Melnyk, Inkit Padhi, Cicero
Nogueira dos Santos, and Payel Das. 2020. Du-
6https://pubmed.ncbi.nlm.nih.gov/ alTKB:ADualLearningBridgebetweenTextandKnowledgeBase. InProceedingsofthe2020Con- Linguistics: ACL2023,pages1270–1283,Toronto,
ferenceonEmpiricalMethodsinNaturalLanguage Canada.AssociationforComputationalLinguistics.
Processing(EMNLP),pages8605–8616,Online.As-
sociationforComputationalLinguistics. Andrea Iovine, Anjie Fang, Besnik Fetahu, Oleg
Rokhlenko,andShervinMalmasi.2022. Cyclener:
RamónFernandezAstudillo,MiguelBallesteros,Tahira Anunsupervisedtrainingapproachfornamedentity
Naseem, Austin Blodgett, and RaduFlorian. 2020. recognition. InProceedingsoftheACMWebConfer-
Transition-basedparsingwithstack-transformers. In ence2022,WWW’22,page2916–2924,NewYork,
FindingsoftheAssociationforComputationalLin- NY,USA.AssociationforComputingMachinery.
guistics: EMNLP 2020, pages 1001–1007, Online.
AssociationforComputationalLinguistics. Sarthak Jain, Madeleine van Zuylen, Hannaneh Ha-
jishirzi,andIzBeltagy.2020. SciREX:Achallenge
Alyson Gamble. 2017. Pubmed central (pmc). The datasetfordocument-levelinformationextraction. In
CharlestonAdvisor,19(2):48–54. Proceedingsofthe58thAnnualMeetingoftheAsso-
ciationforComputationalLinguistics,pages7506–
SreyanGhosh,UtkarshTyagi,MananSuri,SonalKu- 7516, Online. Association for Computational Lin-
mar,RamaneswaranS,andDineshManocha.2023. guistics.
ACLM:Aselective-denoisingbasedgenerativedata
augmentation approach for low-resource complex Eric Jang, Shixiang Gu, and Ben Poole. 2017. Cate-
NER. In Proceedings of the 61st Annual Meeting goricalreparameterizationwithgumbel-softmax. In
of the Association for Computational Linguistics Poceedingsof5thInternationalConferenceonLearn-
(Volume1: LongPapers),pages104–125,Toronto, ingRepresentations.
Canada.AssociationforComputationalLinguistics.
YunaJeongandEunhuiKim.2022. Scideberta: Learn-
John Giorgi, Gary Bader, and Bo Wang. 2022. A ing deberta for science technology documents and
sequence-to-sequenceapproachfordocument-level fine-tuninginformationextractiontasks. IEEEAc-
relationextraction. InProceedingsofthe21stWork- cess,10:60805–60813.
shoponBiomedicalLanguageProcessing,pages10–
25,Dublin,Ireland.AssociationforComputational BinJi,ShashaLi,ShaoduoGan,JieYu,JunMa,Huijun
Linguistics. Liu, and Jing Yang. 2022. Few-shot named entity
recognition with entity-level prototypical network
YuGu,RobertTinn,HaoCheng,MichaelLucas,Naoto enhancedbydispersedlydistributedprototypes. In
Usuyama,XiaodongLiu,TristanNaumann,Jianfeng Proceedings of the 29th International Conference
Gao,andHoifungPoon.2021. Domain-specificlan- on Computational Linguistics, pages 1842–1854,
guagemodelpretrainingforbiomedicalnaturallan- Gyeongju, Republic of Korea. International Com-
guageprocessing. ACMTrans.Comput.Healthcare, mitteeonComputationalLinguistics.
3(1).
Sun Kim, Rezarta Islamaj Dogan, Andrew Chatr-
QipengGuo,ZhijingJin,XipengQiu,WeinanZhang, Aryamontri,MikeTyers,WJohnWilbur,andDon-
DavidWipf,andZhengZhang.2020. CycleGT:Un- aldCComeau.2015. Overviewofbiocreativevbioc
supervisedgraph-to-textandtext-to-graphgeneration track. InProceedingsoftheFifthBioCreativeChal-
viacycletraining. InProceedingsofthe3rdInter- lenge Evaluation Workshop, Sevilla, Spain, pages
nationalWorkshoponNaturalLanguageGeneration 1–9.
from the Semantic Web (WebNLG+), pages 77–88,
Dublin,Ireland(Virtual).AssociationforComputa- Martin Krallinger, Obdulia Rabal, Florian Leitner,
tionalLinguistics. MiguelVazquez,DavidSalgado,ZhiyongLu,Robert
Leaman,YananLu,DonghongJi,DanielMLowe,
Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai etal.2015. Thechemdnercorpusofchemicalsand
Yu, Tie-Yan Liu, and Wei-Ying Ma. 2016. Dual drugsanditsannotationprinciples. Journalofchem-
learning for machine translation. In Advances in informatics,7(1):1–17.
NeuralInformationProcessingSystems,volume29.
CurranAssociates,Inc. YanisLabrak,AdrienBazoge,RichardDufour,Mick-
aelRouvier,EmmanuelMorin,BéatriceDaille,and
Nicolas Hiebel, Olivier Ferret, Karen Fort, and Au- Pierre-AntoineGourraud.2023. DrBERT:Arobust
rélie Névéol. 2023. Can synthetic text help clini- pre-trainedmodelinFrenchforbiomedicalandclini-
calnamedentityrecognition? astudyofelectronic caldomains. InProceedingsofthe61stAnnualMeet-
healthrecordsinFrench. InProceedingsofthe17th ing of the Association for Computational Linguis-
Conference of the European Chapter of the Asso- tics(Volume1: LongPapers),pages16207–16221,
ciationforComputationalLinguistics,pages2320– Toronto,Canada.AssociationforComputationalLin-
2338,Dubrovnik,Croatia.AssociationforComputa- guistics.
tionalLinguistics.
TuanLai,HengJi,ChengXiangZhai,andQuanHung
Zhi Hong, Aswathy Ajith, James Pauloski, Eamon Tran. 2021. Joint biomedical entity and relation
Duede,KyleChard,andIanFoster.2023. Thedimin- extraction with knowledge-enhanced collective in-
ishingreturnsofmaskedlanguagemodelstoscience. ference. In Proceedings of the 59th Annual Meet-
In Findings of the Association for Computational ingoftheAssociationforComputationalLinguisticsandthe11thInternationalJointConferenceonNatu- Pascale Fung. 2021. Crossner: Evaluating cross-
ralLanguageProcessing(Volume1: LongPapers), domain named entity recognition. In Proceedings
pages6248–6260,Online.AssociationforComputa- of the AAAI Conference on Artificial Intelligence,
tionalLinguistics. volume35,pages13452–13460.
GuillaumeLample,AlexisConneau,LudovicDenoyer, Ilya Loshchilov and Frank Hutter. 2017. SGDR:
andMarc’AurelioRanzato.2018. Unsupervisedma- stochasticgradientdescentwithwarmrestarts. In5th
chine translation using monolingual corpora only. International Conference on Learning Representa-
In the Sixth International Conference on Learning tions,ICLR2017,Toulon,France,April24-26,2017,
Representations. ConferenceTrackProceedings.OpenReview.net.
EstherLandhuis.2016. Scientificliterature: Informa- Ilya Loshchilov and Frank Hutter. 2019. Decoupled
tionoverload. Nature,535(7612):457–458. weightdecayregularization. InProceedingsofthe
7thInternationalConferenceonLearningRepresen-
Dong-HoLee,AkshenKadakia,KangminTan,Mahak tations.
Agarwal, Xinyu Feng, Takashi Shibuya, Ryosuke
Mitani,ToshiyukiSekiya,JayPujara,andXiangRen. YiLuan, LuhengHe, MariOstendorf, andHannaneh
2022. Goodexamplesmakeafasterlearner: Simple Hajishirzi.2018. Multi-taskidentificationofentities,
demonstration-basedlearningforlow-resourceNER. relations, andcoreferenceforscientificknowledge
In Proceedings of the 60th Annual Meeting of the graphconstruction. InProceedingsofthe2018Con-
AssociationforComputationalLinguistics(Volume ferenceonEmpiricalMethodsinNaturalLanguage
1: LongPapers),pages2687–2700,Dublin,Ireland. Processing, pages 3219–3232, Brussels, Belgium.
AssociationforComputationalLinguistics. AssociationforComputationalLinguistics.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ruotian Ma, Zhang Lin, Xuanting Chen, Xin Zhou,
Ghazvininejad,AbdelrahmanMohamed,OmerLevy, JunzheWang,TaoGui,QiZhang,XiangGao,and
Veselin Stoyanov, and Luke Zettlemoyer. 2020. YunWenChen.2023. Coarse-to-finefew-shotlearn-
BART:Denoisingsequence-to-sequencepre-training ing for named entity recognition. In Findings of
fornaturallanguagegeneration,translation,andcom- theAssociationforComputationalLinguistics: ACL
prehension. InProceedingsofthe58thAnnualMeet- 2023,pages4115–4129,Toronto,Canada.Associa-
ingoftheAssociationforComputationalLinguistics, tionforComputationalLinguistics.
pages7871–7880,Online.AssociationforComputa-
tionalLinguistics. Tingting Ma, Huiqiang Jiang, Qianhui Wu, Tiejun
Zhao,andChin-YewLin.2022. Decomposedmeta-
Jing Li, Billy Chiu, Shanshan Feng, and Hao Wang. learningforfew-shotnamedentityrecognition. In
2022. Few-shotnamedentityrecognitionviameta- FindingsoftheAssociationforComputationalLin-
learning. IEEETransactionsonKnowledgeandData guistics: ACL2022,pages1584–1596,Dublin,Ire-
Engineering,34(9):4245–4256. land.AssociationforComputationalLinguistics.
PengLi,TianxiangSun,QiongTang,HangYan,Yuan- TasnimMohiuddinandShafiqJoty.2019. Revisiting
bin Wu, Xuanjing Huang, and Xipeng Qiu. 2023a. adversarialautoencoderforunsupervisedwordtrans-
CodeIE: Large code generation models are better lationwithcycleconsistencyandimprovedtraining.
few-shot information extractors. In Proceedings InProceedingsofthe2019ConferenceoftheNorth
of the 61st Annual Meeting of the Association for AmericanChapteroftheAssociationforComputa-
ComputationalLinguistics(Volume1: LongPapers), tionalLinguistics: HumanLanguageTechnologies,
pages15339–15353,Toronto,Canada.Association Volume1(LongandShortPapers),pages3857–3867,
forComputationalLinguistics. Minneapolis,Minnesota.AssociationforComputa-
tionalLinguistics.
Yueling Li, Sebastian Martschat, and Simone Paolo
Ponzetto. 2023b. Multi-source (pre-)training for NgocDangNguyen,LanDu,WrayBuntine,Changyou
cross-domainmeasurement,unitandcontextextrac- Chen, and Richard Beare. 2022. Hardness-guided
tion. In The 22nd Workshop on Biomedical Natu- domain adaptation to recognise biomedical named
ralLanguageProcessingandBioNLPSharedTasks, entities under low-resource scenarios. In Proceed-
pages1–25,Toronto,Canada.AssociationforCom- ingsofthe2022ConferenceonEmpiricalMethods
putationalLinguistics. inNaturalLanguageProcessing,pages4063–4071,
AbuDhabi,UnitedArabEmirates.Associationfor
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man- ComputationalLinguistics.
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019. Venkata Prabhakara Sarath Nookala, Gaurav Verma,
Roberta: A robustly optimized bert pretraining ap- SubhabrataMukherjee,andSrijanKumar.2023. Ad-
proach. Computation and Language Repository, versarialrobustnessofprompt-basedfew-shotlearn-
arXiv:1907.11692. ingfornaturallanguageunderstanding. InFindings
of the Association for Computational Linguistics:
Zihan Liu, Yan Xu, Tiezheng Yu, Wenliang Dai, Zi- ACL2023,pages2196–2208,Toronto,Canada.As-
weiJi,SamuelCahyawijaya,AndreaMadotto,and sociationforComputationalLinguistics.Jaehoon Oh, Sungnyun Kim, Namgyu Ho, Jin-Hwa Proceedingsofthe58thAnnualMeetingoftheAsso-
Kim,HwanjunSong,andSe-YoungYun.2022. Un- ciationforComputationalLinguistics,pages1795–
derstanding cross-domain few-shot learning based 1807,Online.AssociationforComputationalLinguis-
ondomainsimilarityandfew-shotdifficulty. InAd- tics.
vancesinNeuralInformationProcessingSystems.
RichardVanNoorden.2014. Globalscientificoutput
Aaron van den Oord, Yazhe Li, and Oriol Vinyals. doubleseverynineyears. Naturenewsblog.
2018. Representation learning with contrastive
predictive coding. Machine Learning Repository, VijayViswanathan,GrahamNeubig,andPengfeiLiu.
arXiv:1807.03748. 2021. CitationIE:Leveragingthecitationgraphfor
scientific information extraction. In Proceedings
Mihir Parmar, Swaroop Mishra, Mirali Purohit, Man of the 59th Annual Meeting of the Association for
Luo,MuradMohammad,andChittaBaral.2022. In- ComputationalLinguisticsandthe11thInternational
BoXBART:Getinstructionsintobiomedicalmulti- JointConferenceonNaturalLanguageProcessing
tasklearning. InFindingsoftheAssociationforCom- (Volume 1: Long Papers), pages 719–731, Online.
putationalLinguistics:NAACL2022,pages112–128, AssociationforComputationalLinguistics.
Seattle,UnitedStates.AssociationforComputational
Linguistics. Qingyun Wang, Manling Li, Hou Pong Chan, Lifu
Huang,JuliaHockenmaier,GirishChowdhary,and
YongliangShen,XinyinMa,YechunTang,andWeim-
HengJi.2023a. Multimediagenerativescriptlearn-
ingLu.2021. Atrigger-sensememoryflowframe- ingfortaskplanning. InFindingsoftheAssociation
workforjointentityandrelationextraction. InPro- for Computational Linguistics: ACL 2023, pages
ceedings of the Web Conference 2021, WWW ’21,
986–1008,Toronto,Canada.AssociationforCompu-
page1704–1715,NewYork,NY,USA.Association
tationalLinguistics.
forComputingMachinery.
XuanWang,VivianHu,XiangchenSong,ShwetaGarg,
Pontus Stenetorp, Sampo Pyysalo, Goran Topic´,
Jinfeng Xiao, and Jiawei Han. 2021a. ChemNER:
TomokoOhta,SophiaAnaniadou,andJun’ichiTsujii.
Fine-grained chemistry named entity recognition
2012. brat: aweb-basedtoolforNLP-assistedtext
with ontology-guided distant supervision. In Pro-
annotation. In Proceedings of the Demonstrations
ceedingsofthe2021ConferenceonEmpiricalMeth-
atthe13thConferenceoftheEuropeanChapterof
ods in Natural Language Processing, pages 5227–
theAssociationforComputationalLinguistics,pages
5240,OnlineandPuntaCana,DominicanRepublic.
102–107,Avignon,France.AssociationforCompu-
AssociationforComputationalLinguistics.
tationalLinguistics.
YaqingWang,HaodaChu,ChaoZhang,andJingGao.
Shang-YuSu,Chao-WeiHuang,andYun-NungChen.
2021b. Learningfromlanguagedescription: Low-
2019. Dualsupervisedlearningfornaturallanguage
shotnamedentityrecognitionviadecomposedframe-
understandingandgeneration. InProceedingsofthe
work. InFindingsoftheAssociationforComputa-
57thAnnualMeetingoftheAssociationforComputa-
tionalLinguistics: EMNLP2021,pages1618–1630,
tionalLinguistics,pages5472–5477,Florence,Italy.
Punta Cana, Dominican Republic. Association for
AssociationforComputationalLinguistics.
ComputationalLinguistics.
Shang-YuSu,Chao-WeiHuang,andYun-NungChen.
Yaqing Wang, Subhabrata Mukherjee, Haoda Chu,
2020. Towardsunsupervisedlanguageunderstanding
YuanchengTu,MingWu,JingGao,andAhmedHas-
andgenerationbyjointduallearning. InProceedings
san Awadallah. 2021c. Meta self-training for few-
of the 58th Annual Meeting of the Association for
shotneuralsequencelabeling. InProceedingsofthe
ComputationalLinguistics,pages671–680,Online.
27thACMSIGKDDConferenceonKnowledgeDis-
AssociationforComputationalLinguistics.
covery&DataMining,KDD’21,page1737–1747,
C. Sun, W. Li, J. Xiao, N. Parulian, C. Zhai, and New York, NY, USA. Association for Computing
H. Ji. 2021. Fine-grained chemical entity typing Machinery.
withmultimodalknowledgerepresentation. In2021
ZhuoerWang,MarcusCollins,NikhitaVedula,Simone
IEEE International Conference on Bioinformatics
Filice,ShervinMalmasi,andOlegRokhlenko.2023b.
and Biomedicine (BIBM), pages 1984–1991, Los
Faithfullow-resourcedata-to-textgenerationthrough
Alamitos,CA,USA.IEEEComputerSociety.
cycle training. In Proceedings of the 61st Annual
Erik F. Tjong Kim Sang and Fien De Meulder. Meeting of the Association for Computational Lin-
2003. IntroductiontotheCoNLL-2003sharedtask: guistics(Volume1: LongPapers),pages2847–2867,
Language-independentnamedentityrecognition. In Toronto,Canada.AssociationforComputationalLin-
ProceedingsoftheSeventhConferenceonNatural guistics.
LanguageLearningatHLT-NAACL2003,pages142–
147. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi,Pier-
Bo-Hsiang Tseng, Jianpeng Cheng, Yimai Fang, and ricCistac,TimRault,RemiLouf,MorganFuntow-
DavidVandyke.2020. Agenerativemodelforjoint icz,JoeDavison,SamShleifer,PatrickvonPlaten,
naturallanguageunderstandingandgeneration. In Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu,Teven Le Scao, Sylvain Gugger, Mariama Drame, A TrainingandEvaluationDetails
QuentinLhoest,andAlexanderRush.2020. Trans-
formers:State-of-the-artnaturallanguageprocessing.
InProceedingsofthe2020ConferenceonEmpirical Avg.runtime #ofParameters
Methods in Natural Language Processing: System
RoBERTa 16min 125M
Demonstrations,pages38–45,Online.Association
PubMedBERT 18min 109M
forComputationalLinguistics.
ScholarBERT 19min 355M
InBoXBART 58min 139M
Weijia Xu, Xing Niu, and Marine Carpuat. 2020.
+Valid 56min 279M
Dualreconstruction: aunifyingobjectiveforsemi- +Valid+CL 59min 279M
supervisedneuralmachinetranslation. InFindings
of the Association for Computational Linguistics:
Table11: Runtimne(excludeCrossNER)andNumber
EMNLP2020,pages2006–2020,Online.Association
ofModelParameters
forComputationalLinguistics.
YuanyuanXu,ZengYang,LinhaiZhang,DeyuZhou, OurbaselinesandmodelarebasedontheHug-
Tiandeng Wu, and Rong Zhou. 2023. Focusing, gingfaceframework(Wolfetal.,2020)7. Ourmod-
bridgingandpromptingforfew-shotnestednamed
els are trained on a single NVIDIA A100 GPU.
entity recognition. In Findings of the Association
for Computational Linguistics: ACL 2023, pages All hyperparameter settings are listed below. We
2621–2637,Toronto,Canada.AssociationforCom- optimizeallmodelsbyAdamW(Loshchilovand
putationalLinguistics. Hutter,2019). Theruntimeandnumberofparame-
tersislistedinTable11.
Hang Yan, Tao Gui, Junqi Dai, Qipeng Guo, Zheng
Zhang,andXipengQiu.2021. Aunifiedgenerative
RoBERTa. We train a RoBERTa-base model
frameworkforvariousNERsubtasks. InProceedings
of the 59th Annual Meeting of the Association for with100epochsandabatchsize32. Thelearning
ComputationalLinguisticsandthe11thInternational rateis2×10−5withϵ = 1×10−6. Weusealinear
JointConferenceonNaturalLanguageProcessing
schedulerfortheoptimizer.
(Volume1: LongPapers),pages5808–5822,Online.
AssociationforComputationalLinguistics.
PubMedBERT. ThePubMedBERThasthesame
model architecture as BERT-base with 12 trans-
Li Yang, Qifan Wang, Jingang Wang, Xiaojun Quan,
FuliFeng,YuChen,MadianKhabsa,SinongWang, formerlayers. Theoriginalcheckpointispretrained
Zenglin Xu, and Dongfang Liu. 2023. MixPAVE: on PubMed abstracts and full-text articles. We
Mix-prompt tuning for few-shot product attribute
trainamicrosoft/BiomedNLP-PubMedBERT-base-
valueextraction. InFindingsoftheAssociationfor
uncased-abstract-fulltext model with100 epochs
ComputationalLinguistics: ACL2023,pages9978–
9991, Toronto, Canada. Association for Computa- andabatchsize32. Thelearningrateis2×10−5
tionalLinguistics. withϵ = 1×10−6. Weusealinearschedulerfor
theoptimizer.
Deming Ye, Yankai Lin, Peng Li, and Maosong Sun.
2022. Packedlevitatedmarkerforentityandrelation
ScholarBERT. The ScholarBERT is based on
extraction. InProceedingsofthe60thAnnualMeet-
ingoftheAssociationforComputationalLinguistics thesamearchitectureasBERT-large. Theoriginal
(Volume1: LongPapers),pages4904–4917,Dublin, checkpointispretrainedon5,496,055articlesfrom
Ireland.AssociationforComputationalLinguistics. 178,928journals. Thepretrainingcorpushas45.3%
articles about biomedicine and life sciences. We
HaiYe,WenjieLi,andLuWang.2019. Jointlylearning
semanticparserandnaturallanguagegeneratorvia train a globuslabs/ScholarBERT model with 100
dualinformationmaximization. InProceedingsof epochs and a batch size 32. The learning rate is
the57thAnnualMeetingoftheAssociationforCom- 2 × 10−5 with ϵ = 1 × 10−6. We use a linear
putationalLinguistics,pages2090–2101,Florence,
schedulerfortheoptimizer.
Italy.AssociationforComputationalLinguistics.
InBoXBART. The InBoXBART is an
Zixuan Zhang, Nikolaus Parulian, Heng Ji, Ahmed
Elsayed, Skatje Myers, and Martha Palmer. 2021. instructional-tuning language model for 32
Fine-grained information extraction from biomedi- biomedical NLP tasks based on BART-base. We
calliteraturebasedonknowledge-enrichedAbstract
trainthecogint/in-boxbartmodelwith100epochs
MeaningRepresentation. InProceedingsofthe59th
and a batch size 16. The learning rate is 10−5
AnnualMeetingoftheAssociationforComputational
Linguisticsandthe11thInternationalJointConfer- with ϵ = 1 × 10−6. During decoding, we use
ence on Natural Language Processing (Volume 1: beam-searchtogenerateresultswithabeamsize5.
LongPapers),pages6261–6270,Online.Association
forComputationalLinguistics. 7https://github.com/huggingface/transformersWe use cosine annealing warm restarts schedule ands, Organophosphorus compounds, Re-
(LoshchilovandHutter,2017)fortheoptimizer. active intermediates, Substitution reactions,
Inorganic carbon compounds, Organonitro-
InBoXBART+Valid. We first pretrain the self-
gen compounds, Biomolecules, Coordina-
validation model, which is based on cogint/in-
tion compounds, Halogens, Chemical ele-
boxbart, on the training set. The learning rate
ments, Chlorides, Eliminationreactions, Or-
for the self-validation module is 1 × 10−5 with
ganic redox reactions, Inorganic phospho-
ϵ = 1 × 10−6. We use BLUE and ROUGE to
rus compounds, Organic polymers, Macro-
selectthebestmodel. Wethentraintheentityex-
cycles, Cyclopentadienyl complexes, Sub-
tractionmodelandtheself-validationmodeljointly
stituents,Namereactions,Spirocompounds,
with cross-entropy L loss and reconstruction
gen Chemicalkinetics,Organometallicchemistry,
lossL . ThefinallossisL = L +5·L .
recon gen recon Catalysis, Organosulfur compounds, Ring
Thelearningrateis5×10−5 withϵ = 1×10−6.
forming reactions, Noble gases, Protecting
Duringdecoding,weusebeam-searchtogenerate
groups, Addition reactions, Carbenes, Inor-
resultswithabeamsize5. Weusecosineanneal-
ganicnitrogencompounds,Non-coordinating
ingwarmrestartsschedule(LoshchilovandHutter,
anions, Polymerization reactions, Carbon-
2017)fortheoptimizer.
carbon bond forming reactions, Isomerism,
InBoXBART+Valid+CL. The final model is Enzymes,Oxoacids,Hydrogenationcatalysts
similar to InBoXBART+Valid. We retain the self-
• CHEMET: Acyl Groups, Alkanes, Alkenes,
validation module and add a new decoder con-
Alkynes, Amides, Amines, Aryl Groups,
trastiveloss. ThefinallossisL = L +0.2·L +
gen cl
Carbenes, Carboxylic Acids, Esters, Ethers,
5·L . Werandomlychoose5negativesamples
recon
Heterocyclic Compounds, Ketones, Ni-
for each instance. The learning rate is 5×10−5
triles, Nitro Compounds, Organic Polymers,
with ϵ = 1 × 10−6. During decoding, we use
Organohalides,OrganometallicCompounds,
beam-searchtogenerateresultswithabeamsize
Other Aromatic Compounds, Other Hydro-
5. Weusecosineannealingwarmrestartsschedule
carbons, Other Organic Acids, Other Or-
(LoshchilovandHutter,2017)fortheoptimizer.
ganic Compounds, Other Organonitrogen
AMR-based Mention Extraction. We use Compounds,OtherOrganophosphorusCom-
AMR-parser(FernandezAstudilloetal.,2020)to pounds, Phosphinic Acids And Derivatives,
extract mentions. We treat all text spans that are Phosphonic Acids, Phosphonic Acids And
linkabletoWikipediaasmentions. Derivatives,PolycyclicOrganicCompounds,
SulfonicAcids,Thiols
EvaluationMetrics. Weuseentity-levelmicro-
F1 for all experiments. We use the library from Thefrequencyforeachtypeinthetrainingdata
nereval https://github.com/jantrienes/ne of both ChemNER+ and CHEMET are listed be-
reval.
low:
B DatasetDetails • ChemNER+: Organic compounds: 183,
Coupling reactions: 171, Aromatic com-
We list the entity types of ChemNER+ and
pounds: 136, Functional groups: 120, Het-
CHEMETbelow:
erocyclic compounds: 106, Catalysts: 70,
• ChemNER+: Transition metals, Organic Biomolecules: 66, Chemical elements: 64,
acids,Heterocycliccompounds,Organometal- Organohalides: 63, Transition metals: 56,
lic compounds, Reagents for organic chem- Chemical properties: 55, Ligands: 55, Or-
istry, Inorganic compounds, Thermody- ganicacids: 48, Thermodynamicproperties:
namic properties, Aromatic compounds, 43, Inorganic compounds: 43, Coordina-
Metalhalides,Organicreactions,Alkylating tion compounds: 37, Stereochemistry: 33,
agents, Organic compounds, Coupling reac- Organometalliccompounds: 33,Reagentsfor
tions, Functional groups, Inorganic silicon organic chemistry: 28, Coordination chem-
compounds,Stereochemistry,Organohalides, istry: 27,Organonitrogencompounds: 26,Or-
Chemical properties, Catalysts, Free radi- ganic reactions: 23, Organic polymers: 23,
cals,Alkaloids,Coordinationchemistry,Lig- Substitutionreactions: 21,Catalysis: 20,Or-ganicredoxreactions: 18,Reactiveintermedi- bons: 236,OtherOrganicAcids: 194,Other
ates: 13,Substituents: 13,Halogens: 12,Ad- Organophosphorus Compounds: 97, Acyl
ditionreactions: 8,Chlorides: 6,Ringform- Groups: 78, Nitriles: 77, Carboxylic Acids:
ingreactions: 6,Inorganiccarboncompounds: 62,SulfonicAcids: 37,NitroCompounds: 26,
6,Enzymes: 6,Alkaloids: 4,Organophospho- Carbenes: 9,PhosphonicAcidsAndDeriva-
ruscompounds: 4,Organosulfurcompounds: tives: 4,Thiols: 2
4,Oxoacids: 4,Eliminationreactions: 3,Car-
benes: 3, Inorganic phosphorus compounds: C EvaluationonWholeDataset
2,Chemicalkinetics: 2,Macrocycles: 2,No-
ble gases: 2, Organometallic chemistry: 2,
Model Precision Recall F1
Hydrogenation catalysts: 2, Metal halides:
In-BoXBART 55.73 43.28 48.72
1,Cyclopentadienylcomplexes: 1,Inorganic
+Valid 57.49 45.77 50.97
nitrogen compounds: 1, Protecting groups: +Valid+CL 57.41 46.20 51.10
1,Alkylatingagents: 1,Polymerizationreac-
tions: 1 Table 12: micro-F1 for ChemNER+ with the whole
trainingset.
• CHEMET:OtherOrganicCompounds: 1705,
Ethers: 934, Other Aromatic Compounds:
Weconductfullysupervisedtrainingonalltrain-
882, Heterocyclic Compounds: 792, Alka-
ing sets. The results are listed in Table 12 and
nes: 528, Amides: 516, Other Organonitro-
13. We observe that the self-validation module
genCompounds: 501,OrganometallicCom-
still improves the performance of the original In-
pounds: 495,Esters: 440,Amines: 431,Ke-
BoXBARTfortwodatasets. Weobservethatthe
tones: 406,PolycyclicOrganicCompounds:
decodercontrastivelossfurtherimprovesthemodel
375,ArylGroups: 363,Organohalides: 312,
performance on ChemNER+. However, adding
Alkynes: 281, Alkenes: 266, Organic Poly-
the entity decoder contrastive loss slightly de-
mers: 255,OtherHydrocarbons: 236,Other
creasesit. Becausethereare6561sentencesinthe
OrganicAcids: 194,OtherOrganophosphorus
CHEMETdataset,whichislargerthantheChem-
Compounds: 97, Acyl Groups: 78, Nitriles:
NER+dataset,themodelwiththeself-validation
77,CarboxylicAcids: 62,SulfonicAcids: 37,
modulealreadyperformsverywell. Additionally,
NitroCompounds: 26,Carbenes: 9,Phospho-
sincetheCHEMETmodelcontainsfewerentities
nicAcidsAndDerivatives: 4,Thiols: 2
persentencethantheChemNER+datasetandthese
Weconsiderthefollowingtypesaslong-tailen- entitiesareallorganiccompoundsseparatedaway
titytypesforChemNER+andCHEMET.Welist fromeachother,theentitydecodercontrastiveloss
boththeentitytypeanditsfrequency: might introduce noise into the generation results,
consequentlydecreasingtheperformance.
• ChemNER+: Reactiveintermediates: 13,Sub-
stituents: 13, Halogens: 12, Addition reac-
Model Precision Recall F1
tions: 8, Chlorides: 6, Ring forming reac-
In-BoXBART 64.94 41.62 50.73
tions: 6,Inorganiccarboncompounds: 6,En-
+Valid 70.09 42.16 52.65
zymes: 6, Alkaloids: 4, Organophosphorus +Valid+CL 68.50 41.31 51.15
compounds: 4,Organosulfurcompounds: 4,
Oxoacids: 4, Elimination reactions: 3, Car- Table13: micro-F1forCHEMETwiththewholetrain-
ingset.
benes: 3, Inorganic phosphorus compounds:
2,Chemicalkinetics: 2,Macrocycles: 2,No-
ble gases: 2, Organometallic chemistry: 2,
D ScientificArtifacts
Hydrogenation catalysts: 2, Metal halides:
1,Cyclopentadienylcomplexes: 1,Inorganic
Welistthelicensesofthescientificartifactsused
nitrogen compounds: 1, Protecting groups:
inthispaper: PMCOpenAccessSubset(Gamble,
1,Alkylatingagents: 1,Polymerizationreac- 2017)8 (CCBY-NC,CCBY-NC-SA,CCBY-NC-
tions: 1
NDlicenses),HuggingfaceTransformers(Apache
• CHEMET: Alkynes: 281, Alkenes: 266,
8https://www.ncbi.nlm.nih.gov/pmc/tools/openf
Organic Polymers: 255, Other Hydrocar- tlist/License2.0),ChemNER(nolicense),CHEMET9 datasetcanonlybeusedfornon-commercialpur-
(MIT license), RoBERTa (cc-by-4.0), PubMed- posesbasedonPMCOpenAccessTermsofUse.
BERT(MITlicense),ScholarBERT(apache-2.0),
BLEU10,ROUGE11,InBoXBART(MITlicense),
brat(MITlicense),andnereval(MITlicense). Our
usageofexistingartifactsisconsistentwiththeir
intendeduse.
E HumanAnnotation
The instructions for human annotations can be
foundinthesupplementarymaterial. Humananno-
tators are required to annotate the chemical com-
poundentitiesmentionedeitherinnaturallanguage
or chemical formulas and other chemical related
terms including reactions, catalysts, etc. We re-
cruittwoseniorPh.D.studentsfromtheChemistry
departmentinouruniversitytoperformhumanan-
notations. Weusebrat(Stenetorpetal.,2012)for
allhumanannotations.
F EthicalConsideration
TheChem-FINESEmodelandcorrespondingmod-
els we have designed in this paper are limited to
thechemicaldomain,andmightnotbeapplicable
tootherscenarios.
F.1 UsageRequirement
OurChem-FINESEsystemprovidesinvestigative
leadsforfew-shotfine-grainedentityextractionfor
the chemical domain. Therefore, thefinal results
are not meant to be used without any human re-
view. However, domain experts might be able to
usethistoolasaresearchassistantinscientificdis-
covery. Inaddition,oursystemdoesnotperform
fact-checking or incorporate any external knowl-
edge, which remains as future work. Our model
is trained on PubMed papers written in English,
whichmightpresentlanguagebarriersforreaders
whohavebeenhistoricallyunderrepresentedinthe
NLP/Chemicaldomain.
F.2 DataCollection
Our ChemNER+ sentences are based on papers
from PMC Open Access Subset. Our annotation
is approved by the IRB at our university. All an-
notatorsinvolvedinthehumanevaluationarevol-
untary participants and receive a fair wage. Our
9https://github.com/chenkaisun/MMLI1
10https://github.com/cocodataset/cocoapi/blob/
master/license.txt
11https://github.com/cocodataset/cocoapi/blob/
master/license.txt