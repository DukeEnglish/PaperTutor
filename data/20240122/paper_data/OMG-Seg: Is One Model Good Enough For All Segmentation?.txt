OMG-Seg : Is One Model Good Enough For All Segmentation?
XiangtaiLi1 HaoboYuan1 WeiLi1 HenghuiDing1 SizeWu1 WenweiZhang1
YiningLi2 KaiChen2 ChenChangeLoy1
1S-Lab,NanyangTechnologicalUniversity 2ShanghaiArtificialIntelligenceLaboratory
ProjectPage: https://lxtgh.github.io/project/omg_seg
xiangtai94@gmail.com ccloy@ntu.edu.sg
Video Semantic Seg Video Instance Seg Video Panoptic Seg OV Video Seg
Interactive Seg Semantic Panoptic OV Interactive Seg
Video
OMG-Seg
Image
Close Set Open Set
Semantic Seg Instance Seg Panoptic Seg OV Seg
Figure1.OMG-Segcanhandleovertendifferentsegmentationtasksinoneframework,includingimage-levelandvideo-levelsegmentation
tasks,interactivesegmentation,andopen-vocabularysegmentation.Toourknowledge,thisisthefirstmodeltounifythesefourdirections.
Abstract all these tasks in one model and achieve satisfactory per-
formance. We show that OMG-Seg, a transformer-based
In this work, we address various segmentation tasks, encoder-decoderarchitecturewithtask-specificqueriesand
each traditionally tackled by distinct or partially unified outputs, can support over ten distinct segmentation tasks
models. We propose OMG-Seg, One Model that is Good and yet significantly reduce computational and parameter
enough to efficiently and effectively handle all the seg- overheadacrossvarioustasksanddatasets. Werigorously
mentation tasks, including image semantic, instance, and evaluate the inter-task influences and correlations during
panoptic segmentation, as well as their video counter- co-training. Code and models are available at https:
parts, open vocabulary settings, prompt-driven, interac- //github.com/lxtGH/OMG-Seg.
tive segmentation like SAM, and video object segmenta-
tion. To our knowledge, this is the first model to handle
1
4202
naJ
81
]VC.sc[
1v92201.1042:viXra1.Introduction unifiedsegmentationmodeldesignedtodelivercompetitive
performance across a broad spectrum of visual segmenta-
Visual segmentation that aims to understand semantics at
tion tasks. Unlike previous unified models that typically
thepixellevelhasbeenalongstandingproblem[55,67]in
employ a shared visual backbone but several task-specific
thevisioncommunity,fuelingadvancementsindiverseap-
branches, OMG-Seg adopts a shared encoder-decoder ar-
plicationssuchasrobotics, autonomousvehicles, andaug-
chitecture. In particular, we unify all the task outputs as
mented/virtualrealitysystems. Overthepastdecade,ow-
a unified query representation. One query can represent a
ingtothetremendousprogressindeeplearning[10,11,36,
masklabel, animageortubemask, auniqueID,andavi-
45, 46, 48, 60, 100], this fundamental problem has been
sual prompt. Then, we can adopt a shared decoder to pro-
significantlytransformedintoadiversesetoftasksforim-
cessalltypesofquerieswiththeirfeatures.Thissetupfacil-
age and video data, including basic semantic object / in-
itatesgeneraltrainingandinferenceprocessesthatunifyall
stance segmentation, panoptic segmentation, and the more
visual-only segmentation tasks, capitalizing on the exten-
recent prompt-driven interactive segmentation [38]. Con-
sive parameter sharing across tasks. Through co-training
sequently, a plethora of task-specific deep segmentation
on combined image and video datasets, OMG-Seg, once
models (e.g., Mask-RCNN [30], Mask2Former [18], and
trained, is capable of handling up to ten diverse segmen-
SAM [38]), along with different benchmarks, have been
tationtasksacrossdifferentdatasets.
proposed. Thelateststudies[26,28,82,95]strivetoextend
OMG-Segachievescomparableresultsonimage,video,
these standard close-set segmentation models to more dy-
open-vocabulary,andinteractivesegmentationsettingsover
namic,real-worldscenarios. Thisinvolvesintegratingpre-
eight different datasets, including COCO [52], ADE-
trainedvision-languagefoundationmodels,e.g.,CLIP[66],
20k [98], VIPSeg [58], Youtube-VIS-2019 [90], Youtube-
into deep segmentation frameworks, enabling visual seg-
VIS-2021, and DAVIS-17 [5], based on one single shared
mentationthroughopen-vocabularytextdescriptions.
model. To the best of our knowledge, we are the first to
Mostexistingdeepsegmentationmodelsoftenfocuson
achievefourdifferentsettingsinonesinglemodel.
a single specific task. In many scenarios, a generalizable
model capable of handling a broader spectrum of segmen-
2.RelatedWork
tation tasks is highly desired. A unified model of this
nature would eliminate the necessity for task-specific de-
Universal Image/Video Segmentation. The advent of vi-
signs,whileprovidingaversatilesolutiontoawiderangeof
siontransformers[6,23,53]hasledtoawaveofinnovation
segmentationtasksthroughasingleandcohesivearchitec-
inuniversalsegmentation.Recentworks[18,47,70,92,97]
ture. This approach benefits significantly from leveraging
have developed mask classification architectures grounded
largeandvarieddatacorpora, whichenhancesthemodel’s
in an end-to-end set prediction approach, outperforming
adaptabilityandeffectivenessacrossdifferentsegmentation
specialized models [8, 20–22, 30, 37, 48, 50] in both im-
tasks.
age and video segmentation tasks [35, 49, 51]. Despite
Unifying diverse segmentation tasks within a single theseadvancements,mostexistingmethodsstillrelyondis-
modelisnon-trivialbecauseeachtasktypicallycomeswith tinct models for different segmentation tasks and datasets.
itsownuniquemodeldesign. Theemergenceoftransform- Recently, there has been a shift towards training a single
ers [6, 23, 53] has catalyzed several segmentation mod- model [27, 33, 88, 89] across diverse datasets and tasks,
els based on the Detection Transformer (DETR) architec- reaping the benefits of parameter sharing. For instance,
ture [18, 33, 35, 49, 51, 97], yielding notable successes in OneFormer [33] integrates three image segmentation tasks
performance and task integration. Concurrently, there are within a single model, while UNINEXT [89] concentrates
alsomodels[2,27,75,86,89,91,103]thatemployasimi- on unifying instance-level tasks. Similarly, TarVIS [2]
larframeworktomergeopen-vocabularyandmulti-dataset combines various video segmentation tasks using target
segmentationwithinaunifiedarchitecture. Yet,thesemod- prompts. However, none of these existing works has thor-
els often fall short in generalizing to video or interactive oughly investigated the joint training of image, video, and
segmentation,bothessentialforbroaderapplications.Some prompt-drivendatawithinonecomprehensivesegmentation
recentstudies[71,76,77]aimtounifyallvisiontasksunder model. Our work stands as the first attempt in this direc-
one single framework with segmentation included. How- tion,stretchingthepotentialofco-trainingacrossthesedo-
ever, these more generalized models still lag behind task- mains. Foramorein-depthcomparisonofmodelcapabili-
specificsegmentationmodelsintermsofperformance. ties,pleaserefertoTab.1.
In this study, we demonstrate that one model is good Visual Foundation Models. Recent studies in vi-
enough for all segmentation1 by introducing OMG-Seg, a sual foundation models have exhibited a diversification
1Thisincludesprimarilypurevisualand2Dsegmentationtasks, ex- tation [56]. Nonetheless, these could be seamlessly integrated into our
cludingspecifictaskslikemedicalsegmentation[7]andreferringsegmen- OMG-Segframeworkwithappropriateinputadaptations.
2Table1.SettingComparisonForDifferentModels.Weincludeseveralrepresentativemethodshere.OurproposedOMG-Segcanperform
varioussegmentationtasksinonemodel.
Methods SS IS PS VSS VIS VPS VOS Open-Set Multidatasettraining Interactive Sharedmodel
DeeplabV3+[11] ✓
MaskRCNN[30] ✓
PanopticFPN[36] ✓
DERT[6] ✓
DetectorRS[64] ✓ ✓
TCB[58] ✓
VisTR[78] ✓
VPSNet[34] ✓
STM[61] ✓
K-Net[97] ✓ ✓ ✓
Mask2Former[18] ✓ ✓ ✓
VideoK-Net[51] ✓ ✓ ✓
Tube-Link[49] ✓ ✓ ✓
TubeFormer[35] ✓ ✓ ✓
OneFormer[33] ✓ ✓ ✓ ✓
TarViS[2] ✓ ✓ ✓ ✓ ✓
MSeg[40] ✓ ✓ ✓
UNINEXT[89] ✓ ✓ ✓ ✓
OpenSeg[26] ✓ ✓ ✓ ✓
SAM[38] ✓ ✓
Semantic-SAM[42] ✓ ✓ ✓ ✓ ✓ ✓ ✓
SEEM[104] ✓ ✓ ✓ ✓ ✓ ✓ ✓
OPSNet[14] ✓ ✓
FreeSeg[65] ✓ ✓ ✓ ✓ ✓
OMG-Seg ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
in optimization techniques, encompassing various learn- ity has inspired several research initiatives that use a com-
ing paradigms. These include vision-only pre-training mon transformer framework for different domains. No-
strategies [29, 44, 85], joint vision-language pre-training tably, efforts in the realm of vision generalists have been
approaches[24, 43], and multi-modal frameworks that in- directed toward unifying disparate tasks within the vision
corporatevisualprompting[1,63,77]. Anotableexample, domain. For instance, the Pix2Seq series [12, 13] ap-
SAM [38], demonstrates the generalizability and scalabil- proach task unification through auto-regressive token pre-
ityofextensivetraininginachievinggeneralsegmentation. diction. Similarly,Unified-IO[54]implementsasequence-
Building on this, Semantic-SAM [42] augments the SAM to-sequence pipeline, converting diverse inputs and out-
model by adding semantic labels and increased levels of putsintodiscretetokensequences. Furthermore,recentad-
granularity. However,despitetheirimpressivecapabilities, vancements [3, 4, 25, 73, 76, 77] have explored visual in-
thesevisualfoundationmodelstypicallyfallshortinvideo context learning as the means to combine various vision
segmentationtasks,necessitatingfurtherrefinementforop- tasks. These methods predominantly target task unifica-
timalperformanceinmoredynamiccontexts. tion across domains. However, bridging the performance
gapbetweenunifiedsegmentationmodelsandpurpose-built
OpenVocabularySegmentation. Thislineofvisualseg-
segmentationmodelsremainsanopenproblem.
mentationresearch[41,72]aimstorecognizeandsegment
novelobjectsbeyondthelimitedclosed-setvisualconcepts.
3.Methodology
Leveraging the transferrable representations offered by vi-
sion language models (VLMs), many studies [26, 28, 79,
Motivation and Overview. Our OMG-Seg is a single yet
81, 82, 84, 87, 93–95, 99] explore the alignment between
versatile model—with reduced task-specific customization
region and text representations during training. At the in-
andmaximalparametersharing—thatcansupportadiverse
ferencestage,detectorscanrecognizenewclassesusingthe
setofsegmentationtasks,makingitonemodelforallseg-
text embeddings derived from VLMs. Our model follows
mentation. Ourgoalisnottopursuestate-of-the-artresults
this notion to achieve open vocabulary segmentation. In
for each task but to increase the modeling capacity of one
particular, we use frozen VLMs to serve both as a feature
generalizablesegmentationmodelwhileallowingextensive
extractorandclassifier. Thisstrategyallowsforaseamless
knowledgesharingbetweentasks.
transitionintotheopenvocabularysetting.
The main idea of our approach is to leverage object
Unified Modeling. The adaptable nature of the trans- queriesforrepresentingdistinctentities,encompassingvar-
former architecture [23, 69] facilitates the sharing of fun- ious mask types and their respective video formats. In
damentalmodulesacrossvariousmodalities. Thisversatil- Sec. 3.1, we begin by reexamining the definitions of im-
3Figure2.OMG-Segmeta-architecture.(a)OMG-SegfollowsthearchitectureofMask2Former[18],containingabackbone(CLIPVisual
Encoder),apixeldecoder,andamaskdecoder.Thedifferentpartsareasharedmaskdecoderforbothimageandvideosegmentationanda
visualpromptencoder. Weusetwotypesofmaskqueries,i.e.,semanticqueries,forinstance/semanticmasksormasktubes,andlocation
queriesthatencodeboxorpointprompts.(b)OnedecoderlayerintheMaskDecoder.Thelocationqueriesskiptheself-attentionoperation
astheyareonlyconditionedontheimagecontentandthelocationprompts. (c)TheforwardpassofOMG-Segintrainingandinference.
WeuseCLIP’stextencodertorepresentcategorynamesandclassifymasksbycalculatingcosinesimilaritybetweenmaskfeaturesand
textembeddings.
age, video, interactive, and open vocabulary segmentation 3.1.UnifiedTaskRepresentation
settings. In this exploration, we show that the target out-
Image Segmentation. Given an input image I ∈
putsofthesevariedsettingscanbeeffectivelytransformed
RH×W×3, the goal of image segmentation is to output a
into a unified query representation. Specifically, a single
groupofmasks{y }G ={(m ,c )}G wherec denotes
querycanencapsulateamasklabel,animageortubemask, i i=1 i i i=1 i
the class label of the binary mask m and G is the num-
auniqueidentifier,oravisualprompt. i
ber of masks, H × W are the spatial size. According to
For example, video segmentation tasks require only an the scope of class labels and masks, we report the results
additionalIDcomparedtoimagesegmentation, whichcan of three different segmentation tasks, including semantic
be adapted from the query ID. This allows us to employ a segmentation(SS),instancesegmentation(IS),andpanop-
shareddecodertoprocesseachqueryanditsassociatedfea- ticsegmentation(PS).PSistheunificationofbothSSand
turesinastreamlinedmanner,withtheprimarydistinction IS,whichcontainscountablethingclassesanduncountable
beingthespecificfeatureinputsusedincross-attentionlay- stuffclasses. Forallthreetasks,weadoptmaskclassifica-
ers. Inthecontextofimagetasks,wefollowtheestablished tionarchitecture[19,97],whereeachmaskcorrespondsto
designofMask2former[18],enablingqueriesandfeatures asemanticlabel.
to engage in masked-cross attention. For video tasks, we
Video Segmentation. Given a video clip input as V ∈
incorporatetemporalfeatureswith3Dpositionembeddings
RT×H×W×3, where T represents the frame number, the
andfocusonpredictingtubemasksforobjectsacrossshort
goal of video segmentation is to obtain a mask tube
videoclips. Forinteractivesegmentationtasks,weemploy
{y }N = {(m ,c ,d )}N , where N is the number of
thesamedecoderasimagetasksbutskiptheself-attention i i=1 i i i i=1
the tube masks m ∈ {0,1}T×H×W. c denotes the class
operation to condition mask prediction only on the visual i i
labelofthetubemaskm whiled denotestheinstanceID
promptsandimagecontents,asdetailedinSec.3.2. i i
of each tube mask. Each tube mask can be classified into
In addition, to circumvent class taxonomy conflicts, we a countable thing class or uncountable stuff class, where
adopt CLIP embeddings for mask classification. We em- the thing classes are also assigned a unique ID. For stuff
ploythefrozenCLIPvisualencoderasthebackbone,whose masks, the tracking is zero by default. When N = C and
features are shared by the pixel decoder and the open- the task only contains stuff classes, and all thing classes
vocabulary mask classification. This design enables ef- have no IDs, VPS turns into video semantic segmentation
ficient open-vocabulary inference without incurring addi- (VSS). If {y }N overlap and C only contains the thing
i i=1
tional costs. The training and inference pipelines built on classes and all stuff classes are ignored, VPS turns into
suchafrozenbackbonearedescribedinSec.3.3. videoinstancesegmentation(VIS).VideoObjectSegmen-
4tation(VOS)aimstotrackthefirstframeworkmaskswith- the ConvNeXt architecture [53] from the OpenCLIP [32].
outperformingclassification. Motivatedbyimagesegmen- Givenimage/videoinputs,theVLMencoderextractsmulti-
tation, wealsoadoptthetubemaskclassificationarchitec- scalefrozenfeature{Ffrozen}3 ,forfurtherprocess.
j j=1
ture[35,49]totrainandlinkshorttubesalongthetemporal Pixel Decoder as Feature Adapter. The pixel decoder is
dimension. For VOS, we adopt class-agnostic tube-wised the same as Mask2Former, which contains multi-stage de-
training,whichissimilartoVPSandVIS. formable attention layers. It transforms the frozen feature
Interactive Segmentation. The interactive segmentation {Ffrozen}3 ,intothefusedfeature{Ffuse}3 ,withthe
j j=1 j j=1
in SAM [38] framework takes both image I and visual samechanneldimension,wherej isthelayerindexoffea-
prompts P ∈ RN×{2,4}, such as points and boxes, as in- ture. j =3isthehighest-resolutionfeature.
puts, anditoutputsthecorrespondingbinaryimagemasks Combined Object Queries. As analyzed above, each ob-
{y i}N i=1 = {m i ∈ H × W}N i=1 N is the number of vi- ject query represents one type of mask output. However,
sualprompts. Eachvisualpromptisencodedintoanobject fromthefunctionalityperspective,image,video,andinter-
query,whichnaturallycanbetheinputofthedecoder,like activemodesrepresentdifferentproperties.Forimages,ob-
in [18,38]. Inourexperiments,weusetheshareddecoder jectqueriesfocusonobject-levellocalizationandrecogni-
foralldifferenttaskqueries. tion. For video, object queries may involve temporal con-
Open-VocabularyandMulti-DatasetSegmentation. The sistency,suchasthesameobjectlongdifferentframes. For
task formulation is the same as the previous image and interactivesegmentation,objectqueriesareforcedtolocate
video segmentation. However, this setting goes beyond specific regions. For image and video input, we adopt ob-
fixedlabelspace. Inparticular,itrequiresopen-setrecogni- jectqueriestorepresentimagemasksortrackedtubemasks.
tiononvariousdatasets. Meanwhile,multi-datasetsegmen- Sincebothneedsemanticlabels. Wetermthemassemantic
tationrequiresonemodeltosegmentmoreconceptsunder queries, Qs . For interactive mode, following SAM [38],
obj
different datasets. As a common practice, we adopt CLIP we adopt the prompt encoder to encode the various visual
textembeddingasthemaskclassifier,whichavoidstaxon- prompts into the same shape of object queries. We term
omyconflictsandachievesopen-setrecognitionatthesame themaslocationqueries,Ql .Thus,wecansharethesame
obj
time. As a result, we measure the distance between the interfaceforthetransformerdecoder.
visual query feature and class embeddings rather than the Shared Multi-Task Decoder. Its main operation is cross-
learnedclassifier. attention,whichtakesinthecombinedobjectqueries(Qs
obj
All the Things are in Queries. As mentioned above, by and Ql ) and the image/video feature Ffuse, and out-
obj j
combining all different settings, we can represent all the
puts refined object queries. The final masks are obtained
output segmentation entities using the same query-based
via dot-product of refined queries and high-resolution fea-
mask classification framework. In particular, one object ture Ffuse. For image semantic level tasks, we adopt
3
querycorrespondstoonemaskm i,labelc i,andIDd i. De- the same procedure of Mask2Former. In particular, Qs
obj
pending on different task settings, the formats and ranges
perform masked cross-attention [18] with multi-scale fea-
of m i, c i, and d i are different. However, the formats and tures Ffuse. Qs is Query while Ffuse are the Key and
j obj j
rangesofm ,c ,andd aresimilar.Thus,itisnaturaltoput
i i i Value. Then, a multi-head self-attention (MHSA) layer is
allthesetasksintoonesharedencoder-and-decoderframe-
appliedtotherefinedqueries.Therefinedqueriesandhigh-
work and co-train one model for all segmentation tasks
resolutionfeaturesareusedto
Thus, it is natural to put all these tasks into one shared
For video tasks, we adopt the same cross-attention de-
encoder-and-decoderframeworkandco-trainonemodelfor sign. The only difference is the pyramid features Ffuse
allsegmentationtasks. j
are contacted along the temporal dimension with 3D posi-
tion embeddings, which are the default setting as previous
3.2.OMG-SegArchitecture
works [16, 49]. The combined video features and refined
Overview. OMG-Seg follows the architecture design of queriesareusedtopredictthetubemask.
Mask2Former[18]. AsshowninFig.2,itcontainsaback- For interactive segmentation, we carry out the same
bone, a pixel decoder, and a mask decoder. The differ- cross-attentiondesign. However,weskiptheself-attention
ence lies in the following aspects, including frozen back- to avoid interaction between mask queries in the MHSA
bone design, combined object queries which contain both layer, since the interactive segmentation only cares about
objectqueryandvisualprompt,andasharedmulti-taskde- theinputvisualpromptregions. Afterobtainingtherefined
coder.Givendifferenttasksettings,thedecoderoutputscor- object query, it is passed through a prediction FFN, which
respondingmasksandlabels. We typicallyconsistsofa3-layerperceptronwithaReLUacti-
VLM Encoder as Frozen Backbone. To enable open- vationlayerandalinearprojectionlayer.Allthequeriesare
vocabularyrecognition,forthebackbonepart,weadoptthe supervisedbymaskclassificationlossandmaskprediction
frozen CLIP visual model as a feature extractor. We use loss.Thedecodingprocessisinacascadedmanner,inthree
5stagesforeachfeaturepyramid. the training. COCO-SAM is created by using the ground
truthboxes,andmaskcenterpointsarevisualprompts. The
3.3.TrainingandInference
annotationsareobtainedbyCOCOpanopticmasks. More-
JointImageVideoDatasetCo-training. Ratherthanfirst over,wealsoincludethemulti-datasetsettingsinTab.3to
pre-trained on image datasets, our goal is to train all seg- verify the effectiveness of multi-dataset co-training of our
mentation tasks only once jointly. All training targets are OMG-Seg. In addition to Tab. 2, we add more datasets,
oneentitylabelandmaskforallthreedifferentcases. The including ADE-20k and YT-VIS21 for joint co-training.
entity can be thing, stuff, class-agnostic masks, and their Weusethecorrespondingmetricsforeachdataset,includ-
correspondinglabels. Notethattheinstancemaskswiththe ing PQ [37], mask mAP [52], VPQ [34], tube mAP [90],
same ID d form the tube masks. During training, we ap- J&F[5],andmIoU[98].
plyHungarianmatchingbetweenthepredictedandground- ImplementationDetails.Weimplementourmodelsandall
truth entity masks to assign object queries to video/image otherbaselinesinMMDetection[9]. Weusethedistributed
entities,andthensupervisetheirpredictedmasksandclassi- training framework with 32 A100 GPUs. Each mini-batch
fication. TheclassifierisreplacedbyCLIPtextembedding has one image per GPU. For data augmentation, we adopt
to avoid cross-dataset taxonomy conflicts. The final loss large-scalejitteraspreviousworks[18,49]tobuildstrong
function is given as L = λ L +λ L +λ L . baselines. Forallmodelsineachtable, weadoptthesame
cls cls ce ce dice dice
Here,L istheCross-Entropy(CE)lossformaskclassifi- trainingsteps.WeuseOpenCLIP[66]toinitializetheback-
cls
cation,andL andL aremaskCrossEntropy(CE)loss bonenetworkandreplacelearnedclassifierswiththeircor-
ce dice
andDiceloss[59,74]forsegmentation,respectively. responding text embeddings. For image inputs, we treat
Universal Inference. For image segmentation, we follow them as pseudo videos by concatenating two images and
the same inference procedure of Mask2Former [18]. For theirmasksintoone. Weadoptdifferentsamplingratesto
example, for PS, we merge the things and stuff according balance the training examples for each dataset. We report
to the sorted scores. The scores are generated by CLIP resultsofbothfrozenandtrainedbackbonesforreference.
textembedding. Forvideosegmentationtasks,forVISand Welistmoredetailsinthesupplementarymaterial.
VPS,togenerateinstanceID,followingpreviouswork,we
use query matching rather than introducing extra tracking 4.1.MainResults
components. ForVOStasks, weadoptmaskmatchingbe-
System-level Comparison. In Tab. 2, we present a com-
tweenthefirstframeandtheremainingframes.Forinterac-
parative analysis of our OMG-Seg against recent method-
tive segmentation tasks, we follow the original SAM [38],
ologies across a variety of settings. A significant high-
byprovidingboxandpointprompts,andobtainthebinary
light of our work is its unique capability to deliver sub-
masks. For open vocabulary segmentation, since we have
stantialresultsinallscenariosusingasinglemodelframe-
a frozen CLIP encoder, we merge mask pooled score and
work. In the realm of specific image and video segmenta-
learnedscorewiththeopen-vocabularyembeddings.
tion models, OMG-Seg demonstrates performance on par
Combining Tasks For More Applications. Since our
with leading approaches like Mask2Former [18], Tube-
model can perform various segmentation tasks, combining
Link [49], and TarViS [2]. While it exhibits a slight de-
interactive,openvocabularyandimage/videosegmentation
crease in performance on the COCO image segmentation
taskscanleadtoseveralnewapplications. Forexample,we
benchmark, it achieves near state-of-the-art results on the
cancombineinteractiveandvideosegmentation,leadingto
VIPSegdatasets, showcasingitsrobustnessandversatility.
flexible prompt-driven video object segmentation. Or we
Furthermore, when benchmarked against open vocabulary
cancombineinteractivesegmentationwithanopenvocab-
methodssuchasFCCLIP[91]andODISE[86],OMG-Seg
ulary setting, which results in open vocabulary interactive
notonlycompetesfavorablybutalsooutperformsODISEin
segmentation. More examples are provided in Sec. 4 and
certain scenarios. This is particularly evident in the realm
supplementary.
of open vocabulary video segmentation on YT-VIS-21, as
detailedinthe7thcolumnofthetable. Thesefindingsun-
4.Experiments
derscoretheeffectivenessandadaptabilityofourOMG-Seg
Datasets and Metrics. Unlike regular settings, we aim to approach in handling a wide array of segmentation chal-
explore co-training on multiple datasets as much as pos- lenges.
sible. In Tab. 2, we use COCO panoptic [52], COCO- In addition, our method has been benchmarked against
SAM,VIPSeg[57],andYoutube-VIS-2019[90](YT-VIS- recent unified models, revealing insightful comparisons.
19) as training datasets. In addition to the closed-set test- When compared with vision generalists such as that
ing,weincludetheopenvocabulary(OV)inferencebyus- described in [76], our approach, OMG-Seg, demon-
ing Youtube-VIS-2021, ADE-20k [98], and DAVIS-2017 strates superior performance. However, in comparison
datasets [5], where their annotations are not used during with several specialized segmentation models, including
6Table2. ExperimentresultsofOMG-Segonimage, video, open-vocabulary, andSAM-likesettings. *denotesmodelsarepre-trained
ontheObject365dataset[68]. Weonlylistrepresentativemethodsduetothepagelimit. Refertothesupplementarymaterialformore
methods.Ourresultsaretheaveragedresultsoffivedifferentexperiments.
COCO-PS Cityscapes-PS COCO-IS VIPSeg-VPS YT-VIS-19 YT-VIS-21-OV ADE-OV DAVIS-17-VOS-OV COCO-SAM ShareModel
Methods Backbone
PQ PQ mAP VPQ mAP mAP PQ J&F mIoU -
DetectorRS[64] ResNet50 - - 42.1 - - - - - - -
HTC[8] ResNet50 - - 38.4 - - - - - - -
STM[62] ResNet101 - - - - - - - 79.2 - -
K-Net[97] ResNet50 47.1 - 38.6 - - - - - - -
Mask2Former[18] ResNet50 51.9 62.1 43.7 - - - - - - -
Mask2Former[18] Swin-Large 57.8 66.6 50.1 - - - - - - -
k-MaxDeeplab[92] ResNet50 53.0 64.3 - - - - - - - -
k-MaxDeeplab[92] ConvNeXt-Large 58.1 68.4 - - - - - - - -
SeqFormer[80] ResNet50 - - - - 47.4 - - - - -
IDOL[83] Swin-Large - - - - 64.3 - - - - -
MinVIS[31] Swin-Large - - - - 61.6 - - - - -
VideoK-Net[51] ResNet50 - - - 26.1 40.5 - - - - -
Tube-Link[49] ResNet50 - - - 41.2 52.8 - - - - -
Tube-Link[49] Swin-base - - - 54.5 - - - - - -
OneFormer[33] Swin-Large 58.0 67.2 49.2 - - - - - - ✓
TarViS[2] Swin-Large - - - 48.0 - - - - - ✓
fc-clip[91] ConvNeXt-Large 54.4 - 44.6 - - - 26.8 - - ✓
ODISE[86] ViT-Large 55.4 - 46.0 - - - 22.6 - - ✓
DaTaSeg[27] ViT-L 53.5 - - - - - - - - ✓
X-Decoder[103] DaViT 56.9 - 46.7 - - - 21.8 - - ✓
SEEM[104]* DaViT 57.5 - 47.7 - - - - 58.9 83.4 ✓
UNINEXT[89]* ConvNeXt-L - - 49.6 - 64.3 - - 77.2 - ✓
HIPIE[75]* ViT-H 58.0 - 51.9 - - - 20.6 - - ✓
OpenSeeD[96]* Swin-L 59.5 - 53.2 - - - 19.7 - - ✓
SAM[38] ViT-H - - - - - - - - 55.3 ✓
Semantic-SAM[42] Swin-T 55.2 - 47.4 - - - - - 53.0 ✓
Painter[76] ViT-L 43.4 - - - - - - - - ✓
OMG-Seg ConvNeXt-Large(frozen) 53.8 65.7 44.5 49.8 56.4 50.5 27.9 74.3 58.0 ✓
OMG-Seg ConvNeXt-XX-Large(frozen) 55.4 65.3 46.5 53.1 60.3 55.2 27.8 76.9 59.3 ✓
Table3.ExperimentresultsofOMG-Segonmultipledatasetsettings.Weusefivedifferentdatasetsforbalancedjointco-trainingforonly
12epochs.Wealsoimplementcomparedbaselinesinthesamecodebase.
Methods/Settings Backbone COCO-PS COCO-IS ADE-PS VIPSeg-VPS YT-VIS-19 YT-VIS-21 Params(M) ShareModel
K-Net[97] ConvNeXt-Large(trained) 50.5 42.3 40.2 - - - -
Mask2Former[18] ConvNeXt-Large(trained) 53.2 45.2 43.2 - - - -
Mask2Former-VIS[16] ConvNeXt-Large(trained) - - - - 45.8 42.3
singledatasetbaseline ConvNeXt-Large(frozen) 52.5 45.6 41.2 42.3 45.3 44.3 1326 -
OMG-Seg ConvNeXt-Large(frozen) 52.9 44.3 28.2 46.9 48.8 46.2 221 ✓
OMG-Seg ConvNeXt-Large(trained) 55.0 45.3 36.8 45.8 47.2 45.2 221 ✓
UNINEXT [89] and Wang et al. [75], we observe a dis- son in the same setting, we reimplemented two key base-
cernible performance discrepancy in the COCO datasets, lines: K-Net[97]andMask2Former[18]. Ourfindingsin-
notably in panoptic and instance segmentation tasks. This dicatethatjointco-traininggenerallyenhancesperformance
gap, we argue, can be partially attributed to our train- across most video segmentation datasets, leading to sub-
ing regime, which spans only 24 epochs, and also we stantialmodelparameterreduction(from1326Mto221M).
keep the backbone frozen. Furthermore, the integration of This improvement is consistent across three VPS and VIS
video segmentation and interactive segmentation datasets datasets, irrespective of whether the backbones are frozen
for joint co-training presents a more formidable challenge or not. However, it is noteworthy that the performance on
compared to previous works. This is primarily because the ADE-20k dataset significantly diminishes under joint
learningspatial-temporalandlocalization-sensitivefeatures co-training. We hypothesize that this is largely due to the
fromimagedataisinherentlymorecomplex, giventhedi- challengesposedbyscalevarianceandtheunevendistribu-
versityofthelearningtargets. tion of classes within the dataset. Interestingly, when us-
ing a pre-trained backbone, we observe an uplift in image
Despite these challenges, it is noteworthy that no other
segmentationperformance,albeitatthecostofaminorde-
existing models offer the comprehensive segmentation ca-
clineinvideosegmentationefficacy. Thistrade-offcanbe
pabilities that OMG-Seg does. This ability to effectively
attributed to the unbalanced nature of samples that pursue
handleallformsofsegmentation,despitethesmallperfor-
differentoptimizationobjectives,essentiallycausingatug-
mance gaps noted, reinforces our assertion that OMG-Seg
of-war over the representational capacity of the backbone.
isarobustandversatilemodelsuitablefordiversesegmen-
Such a scenario suggests that incorporating a greater vol-
tationscenarios.
ume of video training examples could potentially address
Multi-dataset Setting. In Tab. 3, we extend our investi- thisissue.
gation to multi-dataset settings. To ensure a fair compari-
7Table 4. Ablation on joint co-training. (a), COCO-PS. (b), Table6.Ablationonwhetherusingextraadapters.
VIPSeg-VPS.(c).YT-VIS-19.
Setting epoch COCO-PS VIPSeg-VPS Params(M) GFlops(G)
Setting COCO-PS VIPSeg-VPS YT-VIS-19 ADE-OV YT-VIS-21-OV baseline 12 53.0 48.5 221 868
+Adapter[15] 12 53.5 49.2 +11 +103
a 53.4 32.2 34.2 25.5 30.3 +MorePixelDecoderLayer[102] 12 53.6 49.4 +21 +60
a+b 52.9 49.0 45.2 26.2 39.6 baseline 36 54.8 50.1 221 868
a+b+c 53.0 48.5 56.8 26.1 50.3 +Adapter[15] 36 54.6 49.6 +11 +103
+MorePixelDecoderLayer[102] 36 54.7 50.2 +21 +60
Table5.Ablationonshareddecoderdesign.
Table7.AblationondifferentCLIPs.
Setting COCO-PS VIPSeg-VPS Param GFlops Backbone epoch COCO-PS VIPSeg-VPS ADE-OV Params(M) Flops(G)
shared 53.0 48.5 221 868 ResNet50 12 44.8 42.0 18.2 59.5 340
ConvNeXtLarge 12 53.0 48.5 26.8 221 868
decoupledimage/video 53.6 46.2 243 868
ConvNeXtXX-Large 12 54.3 53.2 27.2 820 2854
ConvNeXtXX-Large 24 55.5 53.3 27.8 820 2854
ConvNeXtXX-Large 36 56.0 53.0 26.7 820 2854
Qualitative Result. In Fig. 3, we show the effectiveness
of our OMG-Seg model using a ConvNeXt-Large model thattheadapter[15]boostsperformancewithfewertraining
across five different tasks. The first two rows demonstrate epochs,butitseffectivenessagainstthebaselinedisappears
the model’s high-quality image segmentation capabilities inextendedtrainingscenarios. Inaddition, weexperiment
on the COCO dataset. In the VIS and VPS tasks, OMG- with increasing the neck capacity by duplicating attention
Seg shows proficiency in segmenting and tracking fore- layers in the pixel decoder, observing similar outcomes to
groundobjects. Notably,inthelastrow,weshowanopen- the adapter implementation. Consequently, we opt not to
vocabulary video instance segmentation on Youtube-VIS, incorporate additional adapters, maintaining a cleaner and
successfully identifying the “lizard” class, which was not simplerframework.
includedinthetrainingset. Ablation on Other CLIPs. In Tab. 7, following the ap-
proach of prior open vocabulary research [39, 91], we pri-
4.2.AblationStudyandAnalysis
marilyemployconvolution-basedCLIPmodelsduetotheir
Inthissection, weuseCOCO,VIPSeg, andYoutube-VIS- spatialinformationhandlingandadaptabilitytoscalevari-
19 for ablation studies of our OMG-Seg. All experiments ations across different datasets. As we scale up the CLIP
usefrozenConvNeXt-Largeasthebackboneandthesame modelsizeandextendtrainingsteps,weobserveimprove-
dataaugmentationwith12epochstrainingbydefault. ments across all three datasets. Notably, model conver-
EffectofTrainingDataset. InTab.4,weevaluatetheim- genceisachievedat24epochs,fasterthaninpreviousstud-
pact of various datasets on model performance. As indi- ies[18]. Thisacceleratedconvergencemaybeattributedto
cated in the first row, using only the COCO dataset yields themodel’slimitedcapacity,suggestingthatlargermodels
satisfactory zero-shot results across other datasets, largely couldfurtherelevateperformance.
attributedtotheemploymentoffrozenCLIPvisualfeatures
5.Conclusion
for zero-shot region feature classification. Upon integra-
tion of the VIPSeg dataset, a slight dip in performance on
Inthisstudy,weintroducethefirstjointco-trainingframe-
the COCO dataset is observed. However, this is counter-
work for image, video, open-vocabulary, and interactive
balanced by significant improvements in both the VIPSeg
segmentation. Oursolution,OMG-Seg,isanovelyetsim-
andYoutube-VISdatasets. Incorporatingallthreedatasets,
pleframeworkthatusesaunifiedqueryrepresentationand
COCO, VIPSeg, and Youtube-VIS, results in an optimal
a shared decoder for diverse tasks. For the first time, it
performance balance across all datasets, establishing this
is possible to train a single segmentation model capable
combinationasourpreferredanddefaultconfiguration.
of performing across ten different tasks with competitive
Ablation on Shared Decoder Design. In Tab. 5, we ex-
performance compared to task-specific models. This ap-
plore the efficacy of a shared decoder design. Employing
proachsignificantlyreducesboththeparametersizeandthe
a separate decoder head for video segmentation tasks re-
need for specialized engineering in model design for vari-
sultsinaslightperformancedecrease. Thisoutcomeisin-
ousapplications.Weenvisionthatourefficientandversatile
fluencedbyouruseofpseudo-videosamplesduringimage
frameworkwillserveasarobustbaselineformulti-taskand
datasettraining. Bysharingthedecoder,wealigntheopti-
multi-datasetsegmentation.
mization objectives more closely, which particularly bene-
fitsthevideodatasetswithshortclips[90].
6.Appendix
Ablation on Extra Adapter. In Tab. 6, we assess the ad-
ditionofanextraadaptertothefrozenCLIPbackbone,en- Overview. Inthisappendix, we firstpresentmoremethod
hancingthecapacityofOMG-Seg. Ourexperimentsreveal details in Sec. A. Then, we present more experiment re-
8Panoptic Segmentation
COCO-dataset
Interactive Segmentation
COCO-dataset
Video Instance Segmentation
Youtube-VIS-2019-dataset
Video PanpoticSegmentation
VIP-Seg-dataset
Open-Vocabulary Video
Instance Segmentation
Youtube-VIS-2021 dataset
Figure3. FunctionalVisualizationofOMG-Segmodel. Welistfivedifferenttasksfromfourdatasetsasexamples. Ourmethodachieves
high-qualitysegmentation,tracking,andaswellasinteractivesegmentationinonesharedmodel.
sultsinSec.B.Finally,weshowmoreimage,video,open- vocabularysegmentation. ComparedwithOneFormer[33],
vocabulary,andinteractivesegmentationdemosinSec.C. wecanachievevideo,open-vocabulary,andinteractiveseg-
mentation. Compared with TarVS [2], we can keep image
A.MoreMethodDetails segmentation without specific fine-tuning. Compared with
recent FreeSeg [65], we can achieve both video segmenta-
More Detailed Comparison with Recent Works. Due
tionandinteractivesegmentationinonemodel.
to the page limitation, we only select several representa-
Implementation Details of OMG-Seg. We use balanced
tiveworksforsettingcomparison. Comparedwithspecific
training for our model. In particular, for two different
models [17, 64], our method achieves extreme parameter
setting of Tab.2 and Tab.3 in the main paper, we bal-
sharing and performs various tasks that these models can-
ance each dataset sample according to the COCO dataset
notperform.
size. Then, we choose the same data augmentation as
Compared with video segmentation and unified video
Mask2Former[18]. Forthetextembeddinggeneration,we
segmentation [2, 49], our method can also achieve open-
followthestandardopen-vocabularydetectionandsegmen-
vocabulary and interactive segmentation, as well as good
tationsetting[82,101]. Wegeneratemultipletextprompts
enough performance on image segmentation. This is
withtheclassnamesandkeepthetextembeddingfixedfor
because our model is jointly co-trained on both image
both training and inference. In this way, we can achieve
and video segmentation datasets without introducing task-
multi-datasetandopen-vocabularysegmentation.
specifictuningonvideosegmentationdatasets. Inaddition,
duetothefrozenCLIPbackbone,ourmethodcanalsoper- More Detailed Inference Process. Our model has vari-
form video open vocabulary segmentation without any ar- ous inference modes. For image segmentation on various
chitecturemodification. datasets, we simply follow the Mask2Former to obtain the
Compared with recent partial unified models, our corresponding mask and labels. For video segmentation,
method achieves all related visual segmentation in one we adopt simple query matching [31, 51] without learn-
model. For example, compared with Semantic-SAM [42], ingtheextratrackingqueryembedding. Webelieveadding
ourmodelcanachievebothvideosegmentation(VIS,VSS, suchcomponentswillimprovethevideosegmentation. For
VPS) and open-vocabulary segmentation. Compared with open-vocabularysegmentation,wefusethefrozenCLIPvi-
UNINEXT [89], our method can perform interactive seg- sualscopeandpredictedscopetoboostthenovelclassseg-
mentation, panoptic segmentation (VPS, PS), and open- mentation. Forinteractivesegmentation,wemainlyusethe
9Table8.ResultsusingResNet50backbone. aregoodenough,whileintroducingtheglobalinformation
willbringnoisetothequerylearning.
Method Backbone COCO-PS VIPSeg-VPS Youtube-VIS-2019
Mask2Former[18] ResNe50 52.0 - -
C.MoreVisualizationExample
Mask2Former-VIS[16] ResNe50 - - 46.4
OMG-Seg ResNe50 49.9 42.3 46.0
OMG-Seg ConvNext-L 54.5 50.5 56.2 MoreVisualResultsonMoreTasks. InFig.4,wepresent
morevisualexamplesfortwoadditionaltasks.Oneisopen-
Table9.ResultsusingViTbackbone.
vocabularypanopticsegmentationonADE-20k. Asshown
inthetoprow,ourmethodcanachievegoodzero-shotseg-
Backbone COCO-PS Youtube-VIS-2019 VIP-Seg
mentation quality. In the second row, we also provide in-
ViT-L(frozen) 34.5 23.2 34.5
teractivesegmentationontheImageNet-1kdataset. Weadd
ViT-L(learned) 52.2 54.3 48.2
theclasslabelsthatarefromthesimpleCLIPscore. Tothis
ConvNext-L(frozen) 54.5 56.2 50.5
end,weachieveopen-vocabularyinteractivesegmentation.
LimitationandFutureWork. Onelimitationofourwork
Table10. Ablationonself-attentionmodeforinteractivesegmen-
tation tasks. We use ResNet50 as the backbone. The masks are is the capacity of our model. Since we use the frozen ar-
usedtofilteroutthecorrelationofeachqueryduringself-attention. chitecturetokeeptheopen-vocabularyability,whichleads
to inferior results for one specific dataset or task. How-
Setting COCO-PS COCO-SAM ever, webelieve addingmoredatasetco-training[38] with
thelearnedbackbonewillimproveourmodelperformance.
SelfAttentionwithoutmasks 45.2 40.7
With the aid of more text-image pairs or classification
SelfAttentionwithmasks 49.9 52.2
datasets, we also achieve open-vocabulary segmentation
ability while keeping the performance improved on close
sets. Thisisourfutureworktoscaleupourmodel. More-
point prompts to evaluate despite the box prompts, which
over,wecanalsoaddatextpathtosupportlanguage-driven
canalsobeusedasSAM[38]. Moreover,sinceourmodel
segmentationtasks,suchasreferringimage/videosegmen-
adopts the frozen CLIP features, we can freely label the
tation or even with large language models (LLMs) to per-
prompt-driven segmentation masks, where we can achieve
formjointreasoningandsegmentationinoneframework.
open-vocabulary interactive segmentation. The GFlops of
themainpaperarecalculatedwith1200×800bydefault.
References
B.MoreExperimentResults
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Inadditiontothemainpaper,wealsoprovidemoreablation
Mensch, Katherine Millican, Malcolm Reynolds, et al.
studiesandexperimentresultshere.
Flamingo: avisuallanguagemodelforfew-shotlearning.
Results Using ResNe50 backbone. In Tab. 8, we re-
InNeurIPS,2022. 3
port our model using ResNet50 backbone. We jointly co- [2] AliAthar,AlexanderHermans,JonathonLuiten,DevaRa-
train our model with 24 epochs. Compared with specific manan, andBastianLeibe. Tarvis: Aunifiedarchitecture
Mask2Formerfor50epochtraining,ourmodelcanachieve fortarget-basedvideosegmentation. InCVPR,2023. 2,3,
considerableresultsbutwithlessparametercosts. 6,7,9
Exploration on ViT-based CLIP backbone. In Tab. 9, [3] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir
Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and
we explore the CLIP-ViT backbone. We find using frozen
Alexei A Efros. Sequential modeling enables scal-
CLIP-ViTleadstoinferiorresults. Thisisbecausethepo-
able learning for large vision models. arXiv preprint
sition embedding of ViT is fixed (224 by default), and a
arXiv:2312.00785,2023. 3
simple bilinear upsampling operation hurts the origin rep-
[4] AmirBar,YossiGandelsman,TrevorDarrell,AmirGlober-
resentation. Thus,inthesecondrow,weadoptthelearned son,andAlexeiEfros.Visualpromptingviaimageinpaint-
architecture. However,westillfindperformancegapswith ing. InNeurIPS,2022. 3
convolution-basedCLIP.Moreover,sincethereisnofrozen [5] Sergi Caelles, Alberto Montes, Kevis-Kokitsi Maninis,
CLIPandtheopen-vocabularyabilityislostduringthefine- Yuhua Chen, Luc Van Gool, Federico Perazzi, and Jordi
tuning. Pont-Tuset. The2018davischallengeonvideoobjectseg-
mentation. arXivpreprintarXiv:1803.00557,2018. 2,6
Interactive Segmentation with Masked Self-Attention.
[6] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nico-
Ininteractivemode,wesetthequeryinvisible(achievethis
las Usunier, Alexander Kirillov, and Sergey Zagoruyko.
by masking) to each other during the cross-attention pro-
End-to-endobjectdetectionwithtransformers. InECCV,
cess. Ifnot,asshowninTab.10,wefindasignificantper-
2020. 2,3
formance drop for both COCO-SAM and COCO-PS. This [7] JienengChen,YongyiLu,QihangYu,XiangdeLuo,Ehsan
is because, for interactive segmentation, the local features Adeli,YanWang,LeLu,AlanLYuille,andYuyinZhou.
10Open-Vocabulary
Panoptic Segmentation
ADE-20k dataset
Open-Vocabulary Interactive
Segmentation
dog
ImageNet dataset dog bear python
Figure4.MorefunctionalVisualizationofOMG-Segmodel.Inadditiontofivedifferenttasksofthemainpaper,wealsovisualizetheopen-
vocabularysegmentationresults: open-vocabularypanopticsegmentationresultsonADE-20k,open-vocabularyinteractivesegmentation
resultsonImageNet1kdataset.
Transunet: Transformers make strong encoders for medi- bottom-uppanopticsegmentation. InCVPR,2020. 9
calimagesegmentation. arXivpreprintarXiv:2102.04306, [18] Bowen Cheng, Ishan Misra, Alexander G. Schwing,
2021. 2 Alexander Kirillov, and Rohit Girdhar. Masked-attention
[8] KaiChen,JiangmiaoPang,JiaqiWang,YuXiong,Xiaox- mask transformer for universal image segmentation. In
iao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping CVPR,2022. 2,3,4,5,6,7,8,9,10
Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. [19] BowenCheng,AlexanderG.Schwing,andAlexanderKir-
Hybridtaskcascadeforinstancesegmentation. InCVPR, illov. Per-pixelclassificationisnotallyouneedforseman-
2019. 2,7 ticsegmentation. InNeurIPS,2021. 4
[9] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu [20] HenghuiDing,ChangLiu,ShutingHe,XudongJiang,and
Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Chen Change Loy. MeViS: A large-scale benchmark for
Liu,JiaruiXu,etal. MMdetection:Openmmlabdetection video segmentation with motion expressions. In ICCV,
toolboxandbenchmark. arXivpreprint,2019. 6 2023. 2
[10] Liang-Chieh Chen, George Papandreou, Florian Schroff, [21] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang,
andHartwigAdam. Rethinkingatrousconvolutionforse- PhilipHSTorr, andSongBai. MOSE:Anewdatasetfor
manticimagesegmentation. arXiv:1706.05587,2017. 2 video object segmentation in complex scenes. In ICCV,
[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Flo- 2023.
rian Schroff, and Hartwig Adam. Encoder-decoder with [22] Henghui Ding, Chang Liu, Suchen Wang, and Xudong
atrousseparableconvolutionforsemanticimagesegmenta- Jiang. VLT: Vision-language transformer and query gen-
tion. InECCV,2018. 2,3 erationforreferringsegmentation. IEEETPAMI,2023. 2
[12] TingChen,SaurabhSaxena,LalaLi,DavidJFleet,andGe- [23] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
offreyHinton. Pix2seq: Alanguagemodelingframework Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
for object detection. arXiv preprint arXiv:2109.10852, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
2021. 3 Sylvain Gelly, et al. An image is worth 16x16 words:
[13] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, Transformersforimagerecognitionatscale.arXivpreprint
DavidJ.Fleet,andGeoffreyHinton.Aunifiedsequencein- arXiv:2010.11929,2020. 2,3
terfaceforvisiontasks. arXivpreprintarXiv:2206.07669, [24] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,
2022. 3 Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue
[14] XiChen,ShuangLi,Ser-NamLim,AntonioTorralba,and Cao. Eva: Exploringthelimitsofmaskedvisualrepresen-
Hengshuang Zhao. Open-vocabulary panoptic segmenta- tationlearningatscale. arXivpreprintarXiv:2211.07636,
tionwithembeddingmodulation. ICCV,2023. 3 2022. 3
[15] ZheChen,YuchenDuan,WenhaiWang,JunjunHe,Tong [25] ZhongbinFang,XiangtaiLi,XiaLi,JoachimMBuhmann,
Lu,JifengDai,andYuQiao.Visiontransformeradapterfor ChenChangeLoy,andMengyuanLiu. Explorein-context
densepredictions. arXivpreprintarXiv:2205.08534,2022. learningfor3dpointcloudunderstanding. arXivpreprint
8 arXiv:2306.08659,2023. 3
[16] Bowen Cheng, Anwesa Choudhuri, Ishan Misra, Alexan- [26] GolnazGhiasi,XiuyeGu,YinCui,andTsung-YiLin.Scal-
der Kirillov, Rohit Girdhar, and Alexander G Schwing. ingopen-vocabularyimagesegmentationwithimage-level
Mask2formerforvideoinstancesegmentation. arXivpre- labels. InECCV,2022. 2,3
print,2021. 5,7,10 [27] Xiuye Gu, Yin Cui, Jonathan Huang, Abdullah Rash-
[17] BowenCheng,MaxwellDCollins,YukunZhu,TingLiu, wan, XuanYang, XingyiZhou, GolnazGhiasi, Weicheng
ThomasSHuang,HartwigAdam,andLiang-ChiehChen. Kuo,HuizhongChen,Liang-ChiehChen,etal. DaTaseg:
Panoptic-deeplab: A simple, strong, and fast baseline for Taming a universal multi-dataset multi-task segmentation
11model. InNeurIPS,2023. 2,7 CVPR,2023. 3
[28] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. [45] XiangtaiLi, HenghuiDing, WenweiZhang, HaoboYuan,
Open-vocabularyobjectdetectionviavisionandlanguage GuangliangCheng,PangJiangmiao,KaiChen,ZiweiLiu,
knowledgedistillation. InICLR,2021. 2,3 andChenChangeLoy. Transformer-basedvisualsegmen-
[29] KaimingHe,XinleiChen,SainingXie,YanghaoLi,Piotr tation:Asurvey. arXivpre-print,2023. 2
Dolla´r,andRossGirshick. Maskedautoencodersarescal- [46] Xiangtai Li, Xia Li, Li Zhang, Guangliang Cheng, Jian-
ablevisionlearners. InCVPR,2022. 3 ping Shi, Zhouchen Lin, Shaohua Tan, and Yunhai Tong.
[30] KaimingHe,GeorgiaGkioxari,PiotrDolla´r,andRossGir- Improvingsemanticsegmentationviadecoupledbodyand
shick. Maskr-cnn. InICCV,2017. 2,3 edgesupervision. InECCV,2020. 2
[31] De-AnHuang,ZhidingYu,andAnimaAnandkumar. Min- [47] Xiangtai Li, Shilin Xu, Yibo Yang, Guangliang Cheng,
vis: A minimal video instance segmentation framework Yunhai Tong, and Dacheng Tao. Panoptic-partformer:
withoutvideo-basedtraining. InNeurIPS,2022. 7,9 Learning a unified model for panoptic part segmentation.
[32] GabrielIlharco,MitchellWortsman,RossWightman,Cade InECCV,2022. 2
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, [48] Xiangtai Li, Ansheng You, Zhen Zhu, Houlong Zhao,
VaishaalShankar,HongseokNamkoong,JohnMiller,Han- MaokeYang, KuiyuanYang, andYunhaiTong. Semantic
nanehHajishirzi,AliFarhadi,andLudwigSchmidt. Open- flowforfastandaccuratesceneparsing. InECCV,2020. 2
clip,July2021. 5 [49] Xiangtai Li, Haobo Yuan, Wenwei Zhang, Guangliang
[33] JiteshJain,JiachenLi,MangTikChiu,AliHassani,Nikita Cheng,JiangmiaoPang,andChenChangeLoy. Tube-link:
Orlov, andHumphreyShi. OneFormer: OneTransformer Aflexiblecrosstubebaselineforuniversalvideosegmen-
toRuleUniversalImageSegmentation. InCVPR,2023. 2, tation. InICCV,2023. 2,3,5,6,7,9
3,7,9 [50] Xiangtai Li, Jiangning Zhang, Yibo Yang, Guangliang
[34] Dahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Cheng,KuiyuanYang,YuTong,andDachengTao. Sfnet:
Kweon. Videopanopticsegmentation. InCVPR,2020. 3, Fasterandaccuratedomainagnosticsemanticsegmentation
6 viasemanticflow. IJCV,2023. 2
[35] Dahun Kim, Jun Xie, Huiyu Wang, Siyuan Qiao, Qi- [51] Xiangtai Li, Wenwei Zhang, Jiangmiao Pang, Kai Chen,
hang Yu, Hong-SeokKim, Hartwig Adam, InSo Kweon, Guangliang Cheng, Yunhai Tong, and Chen Change Loy.
andLiang-ChiehChen. Tubeformer-deeplab: Videomask Video k-net: A simple, strong, and unified baseline for
transformer. InCVPR,2022. 2,3,5 videosegmentation. InCVPR,2022. 2,3,7,9
[36] AlexanderKirillov,RossGirshick,KaimingHe,andPiotr [52] Tsung-Yi Lin, Michael Maire, Serge Belongie, James
Dolla´r.Panopticfeaturepyramidnetworks.InCVPR,2019. Hays, Pietro Perona, Deva Ramanan, Piotr Dolla´r, and
2,3 CLawrenceZitnick. Microsoftcoco: Commonobjectsin
[37] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten context. InECCV,2014. 2,6
Rother,andPiotrDolla´r. Panopticsegmentation. InCVPR, [53] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeicht-
2019. 2,6 enhofer,TrevorDarrell,andSainingXie.Aconvnetforthe
[38] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi 2020s. InCVPR,2022. 2,5
Mao,ChloeRolland,LauraGustafson,TeteXiao,Spencer [54] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh
Whitehead,AlexanderCBerg,Wan-YenLo,etal.Segment Mottaghi, and Aniruddha Kembhavi. Unified-io: A uni-
anything. InICCV,2023. 2,3,5,6,7,10 fiedmodelforvision,language,andmulti-modaltasks. In
[39] WeichengKuo,YinCui,XiuyeGu,A.J.Piergiovanni,and ICLR,2023. 3
AneliaAngelova. F-VLM:Open-vocabularyobjectdetec- [55] JitendraMalik,SergeBelongie,ThomasLeung,andJianbo
tion upon frozen vision and language models. In ICLR, Shi. Contourandtextureanalysisforimagesegmentation.
2023. 8 IJCV,2001. 2
[40] JohnLambert, ZhuangLiu, OzanSener, JamesHays, and [56] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana
Vladlen Koltun. Mseg: A composite dataset for multi- Camburu, Alan L Yuille, and Kevin Murphy. Generation
domainsemanticsegmentation. InCVPR,2020. 3 andcomprehensionofunambiguousobjectdescriptions.In
[41] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen CVPR,2016. 2
Koltun, and Rene´ Ranftl. Language-driven semantic seg- [57] Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang,
mentation. InICLR,2022. 3 Yunchao Wei, and Yi Yang. Large-scale video panoptic
[42] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong segmentationinthewild:Abenchmark. InCVPR,2022. 6
Liu,JianweiYang,ChunyuanLi,LeiZhang,andJianfeng [58] JiaxuMiao,YunchaoWei,YuWu,ChenLiang,Guangrui
Gao. Semantic-sam: Segment and recognize anything at Li, and Yi Yang. Vspw: A large-scale dataset for video
anygranularity.arXivpreprintarXiv:2307.04767,2023.3, sceneparsinginthewild. InCVPR,2021. 2,3
7,9 [59] FaustoMilletari,NassirNavab,andSeyed-AhmadAhmadi.
[43] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. V-Net: Fullyconvolutionalneuralnetworksforvolumetric
Blip: Bootstrapping language-image pre-training for uni- medicalimagesegmentation. In3DV,2016. 6
fied vision-language understanding and generation. In [60] Shervin Minaee, Yuri Y Boykov, Fatih Porikli, Antonio J
ICML,2022. 3 Plaza,NasserKehtarnavaz,andDemetriTerzopoulos. Im-
[44] WeiLi, JiahaoXie, andChenChangeLoy. Correlational age segmentation using deep learning: A survey. PAMI,
imagemodelingforself-supervisedvisualpre-training. In 2021. 2
12[61] SeoungWugOh,Joon-YoungLee,NingXu,andSeonJoo Chunhua Shen, and Tiejun Huang. Seggpt: Segmenting
Kim. Videoobjectsegmentationusingspace-timememory everythingincontext. InICCV,2023. 2,3
networks. InICCV,2019. 3 [78] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua
[62] SeoungWugOh,Joon-YoungLee,NingXu,andSeonJoo Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-
Kim. Videoobjectsegmentationusingspace-timememory to-end video instance segmentation with transformers. In
networks. InICCV,2019. 7 CVPR,2021. 3
[63] ZhiliangPeng,WenhuiWang,LiDong,YaruHao,Shaohan [79] Zhenyu Wang, Yali Li, Xi Chen, Ser-Nam Lim, Antonio
Huang,ShumingMa,andFuruWei. Kosmos-2: Ground- Torralba, HengshuangZhao, andShengjinWang. Detect-
ingmultimodallargelanguagemodelstotheworld. arXiv ingeverythingintheopenworld:Towardsuniversalobject
preprintarXiv:2306.14824,2023. 3 detection. InCVPR,2023. 3
[64] SiyuanQiao, Liang-ChiehChen, andAlanYuille. Detec- [80] JunfengWu,YiJiang,SongBai,WenqingZhang,andXi-
tors: Detectingobjectswithrecursivefeaturepyramidand angBai. Seqformer: Sequentialtransformerforvideoin-
switchableatrousconvolution. InCVPR,2021. 3,7,9 stancesegmentation. InECCV,2022. 7
[65] JieQin,JieWu,PengxiangYan,MingLi,RenYuxi,Xue- [81] Jianzong Wu, Xiangtai Li, Henghui Ding, Xia Li, Guan-
fengXiao,YitongWang,RuiWang,ShileiWen,XinPan, gliang Cheng, Yunhai Tong, and Chen Change Loy. Be-
etal. Freeseg: Unified,universalandopen-vocabularyim- trayedbycaptions:Jointcaptiongroundingandgeneration
agesegmentation. InCVPR,2023. 3,9 foropenvocabularyinstancesegmentation.InICCV,2023.
[66] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya 3
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, [82] JianzongWu,XiangtaiLi,ShilinXu,HaoboYuan,Henghui
AmandaAskell,PamelaMishkin,JackClark,etal. Learn- Ding, YiboYang, XiaLi, JiangningZhang, YunhaiTong,
ingtransferablevisualmodelsfromnaturallanguagesuper- Xudong Jiang, Bernard Ghanem, and Dacheng Tao. To-
vision. InICML,2021. 2,6 wardsopenvocabularylearning:Asurvey.arXivpre-print,
[67] Florian Schroff, Antonio Criminisi, and Andrew Zisser- 2023. 2,3,9
man. Objectclasssegmentationusingrandomforests. In [83] Junfeng Wu, Qihao Liu, Yi Jiang, Song Bai, Alan Yuille,
BMVC,2008. 2 andXiangBai. Indefenseofonlinemodelsforvideoin-
[68] ShuaiShao,ZemingLi,TianyuanZhang,ChaoPeng,Gang stancesegmentation. InECCV,2022. 7
Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: [84] SizeWu, WenweiZhang, LuminXu, ShengJin, Xiangtai
Alarge-scale,high-qualitydatasetforobjectdetection. In Li, Wentao Liu, and Chen Change Loy. Clipself: Vision
ICCV,2019. 7 transformerdistillsitselfforopen-vocabularydensepredic-
[69] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob tion. arXivpreprintarXiv:2310.01403,2023. 3
Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, [85] JiahaoXie,WeiLi,XiaohangZhan,ZiweiLiu,YewSoon
andIlliaPolosukhin. Attentionisallyouneed. InNIPS, Ong,andChenChangeLoy. Maskedfrequencymodeling
2017. 3 forself-supervisedvisualpre-training. InICLR,2023. 3
[70] HuiyuWang,YukunZhu,HartwigAdam,AlanYuille,and [86] Jiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xi-
Liang-Chieh Chen. Max-deeplab: End-to-end panoptic aolong Wang, and Shalini De Mello. Open-Vocabulary
segmentationwithmasktransformers. InCVPR,2021. 2 PanopticSegmentationwithText-to-ImageDiffusionMod-
[71] Wenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, els. InCVPR,2023. 2,6,7
XizhouZhu,GangZeng,PingLuo,TongLu,JieZhou,Yu [87] Shilin Xu, Xiangtai Li, Size Wu, Wenwei Zhang, Yin-
Qiao, et al. Visionllm: Large language model is also an ing Li, Guangliang Cheng, Yunhai Tong, Kai Chen, and
open-endeddecoderforvision-centrictasks.arXivpreprint ChenChangeLoy. Dst-det: Simpledynamicself-training
arXiv:2305.11175,2023. 2 for open-vocabulary object detection. arXiv preprint
[72] Weiyao Wang, Matt Feiszli, Heng Wang, and Du Tran. arXiv:2310.01393,2023. 3
Unidentifiedvideoobjects: Abenchmarkfordense,open- [88] ShilinXu,HaoboYuan,QingyuShi,LuQi,JingboWang,
worldsegmentation. InICCV,2021. 3 Yibo Yang, Yining Li, Kai Chen, Yunhai Tong, Bernard
[73] XinshunWang,ZhongbinFang,XiaLi,XiangtaiLi,Chen Ghanem, Xiangtai Li, and Ming-Hsuan Yang. Rap-sam:
Chen, and Mengyuan Liu. Skeleton-in-context: Unified Towards real-time all-purpose segment anything. arXiv
skeletonsequencemodelingwithin-contextlearning.arXiv preprint,2024. 2
preprintarXiv:2312.03703,2023. 3 [89] BinYan,YiJiang,JiannanWu,DongWang,ZehuanYuan,
[74] Xinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, PingLuo,andHuchuanLu. Universalinstanceperception
and Lei Li. Solo: Segmenting objects by locations. In asobjectdiscoveryandretrieval. InCVPR,2023. 2,3,7,9
ECCV,2020. 6 [90] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance
[75] Xudong Wang, Shufan Li, Konstantinos Kallidromitis, segmentation. InICCV,2019. 2,6,8
Yusuke Kato, Kazuki Kozuka, and Trevor Darrell. Hier- [91] Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and
archicalopen-vocabularyuniversalimagesegmentation. In Liang-Chieh Chen. Convolutions die hard: Open-
NeurIPS,2023. 2,7 vocabulary segmentation with single frozen convolutional
[76] XinlongWang, WenWang, YueCao, ChunhuaShen, and clip. InNeurIPS,2023. 2,6,7,8
TiejunHuang.Imagesspeakinimages:Ageneralistpainter [92] Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins,
forin-contextvisuallearning. InCVPR,2023. 2,3,6,7 YukunZhu,HartwigAdam,AlanYuille,andLiang-Chieh
[77] Xinlong Wang, Xiaosong Zhang, Yue Cao, Wen Wang, Chen. k-meansmasktransformer. InECCV,2022. 2,7
13[93] Haobo Yuan, Xiangtai Li, Chong Zhou, Yining Li, Kai
Chen,andChenChangeLoy. Open-vocabularysam: Seg-
ment and recognize twenty-thousand classes interactively.
arXivpreprint,2024. 3
[94] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and
ChenChangeLoy. Open-vocabularydetrwithconditional
matching. InECCV,2022.
[95] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and
Shih-Fu Chang. Open-vocabulary object detection using
captions. InCVPR,2021. 2,3
[96] HaoZhang,FengLi,XueyanZou,ShilongLiu,Chunyuan
Li, Jianwei Yang, and Lei Zhang. A simple framework
foropen-vocabularysegmentationanddetection. InICCV,
2023. 7
[97] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and
Chen Change Loy. K-net: Towards unified image
segmentation. InNeurIPS,2021. 2,3,4,7
[98] BoleiZhou, HangZhao, XavierPuig, SanjaFidler, Adela
Barriuso,andAntonioTorralba.Semanticunderstandingof
scenesthroughtheADE20Kdataset. InCVPR,2017. 2,6
[99] HaoZhou,TianchengShen,XuYang,HaiHuang,Xiangtai
Li, Lu Qi, and Ming-Hsuan Yang. Rethinking evaluation
metrics of open-vocabulary segmentaion. arXiv preprint
arXiv:2311.03352,2023. 3
[100] Tianfei Zhou, Fatih Porikli, David J Crandall, Luc
Van Gool, and Wenguan Wang. A survey on deep learn-
ingtechniqueforvideosegmentation. PAMI,2023. 2
[101] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Phillip
Kra¨henbu¨hl, andIshanMisra. Detectingtwenty-thousand
classesusingimage-levelsupervision. InECCV,2022. 9
[102] XizhouZhu,WeijieSu,LeweiLu,BinLi,XiaogangWang,
andJifengDai. Deformabledetr:Deformabletransformers
forend-to-endobjectdetection. InICLR,2020. 8
[103] XueyanZou*,Zi-YiDou*,JianweiYang*,ZheGan,Linjie
Li, Chunyuan Li, Xiyang Dai, Jianfeng Wang, Lu Yuan,
NanyunPeng,LijuanWang,YongJaeLee*,andJianfeng
Gao*.Generalizeddecodingforpixel,imageandlanguage.
InCVPR,2023. 2,7
[104] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie
Li, JianfengGao, andYongJaeLee. Segmenteverything
everywhereallatonce. InNeurIPS,2023. 3,7
14