1
Maximal-Capacity Discrete Memoryless
Channel Identification
Maximilian Egger, Rawad Bitar, Antonia Wachter-Zeh, Deniz Gündüz and Nir Weinberger
Abstract
The problem of identifying the channel with the highest capacity among several discrete memoryless channels
(DMCs) is considered. The problem is cast as a pure-exploration multi-armed bandit problem, which follows the
practical use of training sequences to sense the communication channel statistics. A capacity estimator is proposed
and tight confidence bounds on the estimator error are derived. Based on this capacity estimator, a gap-elimination
algorithm termed BestChanID is proposed, which is oblivious to the capacity-achieving input distribution and
is guaranteed to output the DMC with the largest capacity, with a desired confidence. Furthermore, two additional
algorithmsNaiveChanSeland MedianChanEl,thatoutputwith certainconfidencea DMC with capacity close
to the maximal, are introduced. Each of those algorithms is beneficial in a different regime and can be used as
a subroutine in BestChanID. The sample complexity of all algorithms is analyzed as a function of the desired
confidence parameter, the number of channels, and the channels’ input and output alphabet sizes. The cost of best
channelidentificationis shownto scale quadraticallywith the alphabetsize, and a fundamentallower boundfor the
required number of channel senses to identify the best channel with a certain confidence is derived.
Index Terms
Best-arm identification, Capacity estimation, Channel identification, Discrete Memoryless Channels, Multi-
Armed Bandits, Pure-exploration,Sample complexity
I. INTRODUCTION
We consider an information-theoretic instance of a pure exploration problem in the setting of noisy channel
coding. From a communication system perspective, exploration of the channel state is an important task, which
must be routinely performed in order to maintain the highest possible communication rate based on the current
channel conditions [2, Ch. 10] [3, Ch. 9]. Typically, in such systems, before actual data is being transmitted, or
whenever an uncertainty of the channel conditions arises, the transmitter sends a training sequence that is agreed
upon in advance. Based on the received signal and the knowledge of the training sequence, the receiver estimates
certain statistical characteristics of the channel, and then may adapt its various components accordingly, such as
equalizers and decoders. In many cases, the receiver can also communicate its acquired knowledge back to the
transmitter (e.g., in a two-way link). In this case, the transmitter can also adapt its operation, e.g., its encoding
rate, its codebook, physical parameters of the modulator, and so on. The duration of the training phase is a natural
performance criterion in wireless systems, along with the quality of the obtained learned components and the
reliability of the decision.
Aprototypicalandsimpleinstanceofthisgeneralchannelestimation/learningproblemisthatofchannelselection.
Consider a transmitter that can communicate to the receiver using one of k parallel channels, whose statistical
properties are unknown in advance. In this scenario, the adaptation problem becomes a simple decision problem
between k options. For instance, orthogonal-frequency-division-multiplexing (OFDM) systems use k different
frequency bands. The system may choose which of the available frequency bands to allocate to a user, typically
those with the strongest frequency response and hence, the largest capacity [4]. After choosing the active sub-
channels, the system may optimize its power allocation via the water-pouring rule [5, Ch. 9], or even both rate-
and power-allocation [6, Ch. 9]. Similarly, in multiple-input multiple-output (MIMO) systems [7], [8], the receiver
may select the transmit-antenna with the maximal fading gain.
M.E.,R.B.andA.W-Z.arewiththeTechnicalUniversityofMunich.Emails:{maximilian.egger,rawad.bitar,antonia.wachter-zeh}@tum.de.
D.G. is with Imperial College London. Email: d.gunduz@imperial.ac.uk. N.W. is at Technion — Israel Institute of Technology. Email:
nirwein@technion.ac.il.
This project has received funding from the German Research Foundation (DFG) under Grant Agreement Nos. BI 2492/1-1 and WA
3907/7-1. The work of N.W. was partly supported by the Israel Science Foundation (ISF), grant no. 1782/22. Parts of the results were
presented at IEEE International Symposium on Information Theory (ISIT),2023 [1].
4202
naJ
81
]TI.sc[
1v40201.1042:viXra2
In the case of Gaussian channels, the objective reduces to parameter estimation, i.e., estimating the channel gain
orthechannelmatrix in thecaseofMIMO; sincethesameGaussianinputdistribution maximizesthecapacityofall
the channels.Instead, we considerthis problem in the caseof discrete memoryless channels(DMCs). To the best of
our knowledge, and to some extent, to our surprise, this problem has not been studied in the literature. Specifically,
we assume that communication may take place over one of k possible DMCs with the same finite input and output
alphabets. During the training phase, the transmitter may transmit training symbols over each channel and sample
their output. Based on these samples, the receiver’s goal is to decide which channels have the maximal capacity,
which can be achieved upon optimizing the input distribution. From the perspective of multi-armed bandit (MAB)
problems, this falls into the category of pure-exploration [9, Ch. 33]. In the standard MAB problem, the agent
sequentially chooses (pulls) one of k possible arms to maximize its cumulative reward. To achieve this without
knowingthe rewardofeacharmin advance,the agenthasto balanceexplorationandexploitation.In contrast,in the
pure-exploration problem of best-arm identification in the fixed-confidence setting [10], the only goal is to find the
arm with maximal mean reward, with a desired reliability. In contrast to the standard MAB problem, the potential
loss during exploration is not accumulated, and the agent’s decision only has to comply with the target reliability
level while exploiting a minimal number of training symbols. The fundamental difficulty of such a problem is
quantified by lower bounds that provide insights on the minimum number of required arm pulls to determine the
best arm with a certain confidence, e.g., [11]–[13].
One variant of the best-arm identification problem relaxes the objective from outputting the maximal-reward arm
to only outputting an arm whose reward is ε-close to the maximal expected reward under a given target reliability
level,say,with probability atleast1 δ. Thisnaturally reducesthenumberof roundsrequired fora givenreliability.
−
Such algorithms are termed (ε,δ)-PAC, abbreviated for probably approximately correct (PAC) [14].
Exploration in wireless communication networks has been formulated as a MAB problem by various authors,
e.g., [15]–[24]. The problem of estimating Shannon-theoretic measures is well-explored from various perspectives,
starting from [25], [26], and then extended to large alphabets [27]–[29], to the rate-distortion function [30], to
analog sources [31], to mutual information [32], [33] and more. A MAB problem, in which the instantaneous
reward is based on self-information, was recently considered in [34], [35]. A DMC input distribution adaptation
method was proposed and analyzed in [36]. The problem of best channel identification through MABs with pure
exploration was first introduced in an earlier version of this paper [1].
a) Contributions: We formulate the problem of identifying the DMC with the maximal capacity among a set
of k DMCs as a MAB problem in the pure-exploration regime with fixed confidence. We propose a method to
estimate the capacity of a DMC by sampling its output with a given sequenceof known input symbols. We provide
confidenceintervals for the error of the estimator, of a sub-Gaussiannature, that is, an interval length whose size is
roughly O( log(1/δ)/n), for n samples and confidencelevel δ. To obtain this confidence interval for the capacity
estimation error, we invoke the minimax characterization of capacity [37], [38], which expresses the capacity as
p
the minimization over output distributions, cf. (1), rather than maximization over input distributions. This allows
us to define a pseudo-capacity, which replaces the minimization of the output alphabet to a strict interior of the
simplex; concretely, the probability of each output letter is restricted to be larger than η > 0. The resulting pseudo-
capacityapproximatesthetruecapacity,andthevalueofη controlsthebias-variancetrade-offintheestimationerror
analysis. We utilize the confidence interval bound on the capacity estimation error and propose a gap-elimination
algorithm for identifying the DMC with the maximal capacity from k possible candidates, with high probability.
We introduce two PAC algorithms that are optimal in different parameter regimes and can be used as standalone
or in conjunction with the proposed gap-elimination algorithm. These algorithms identify an ε-best DMC whose
capacity is, at a high level, at most ε far from that of the best channel. We analyze the sample complexity of the
proposed algorithms both theoretically and experimentally. Given a set of candidate DMCs parameterized by their
transition matrices, we derive a fundamental lower bound on the number of channel senses required to identify the
one with the highest capacity with a probability of at least 1 δ.
−
b) Outline: InSectionII,weintroducethesystemmodelforexploringDMCswithunknowncapacitiesthrough
sensing. In Section III, we establish a confidence bound for capacity estimation under deterministic sensing. In
Section IV, we introduce BestChanID, a round-based elimination algorithm for best channel identification in the
context of MABs with fixed confidence, which relies on the existence of algorithms that output a channel with
capacity being close to the capacity of the best channel. In Section V, we investigate such algorithms. Lastly, in3
Section VI, we establish a fundamental converse result of a lower bound on the number of required channel senses
for best channel identification. We conclude the paper in Section VII. For a smooth flow of the paper, the majority
of the proofs are deferred to the appendix.
II. PRELIMINARIES AND SYSTEM MODEL
Throughout the paper, random variables are denoted by typesetter letters, e.g., Z, and sets by calligraphic letters,
e.g., . The probability distribution of a random variable X is denoted by PX, and the probability simplex
Z ∈ X
over the alphabet is denoted by ( ). For integers r ,r N such that r < r , [r ] := 1,...,r and
1 2 + 1 2 1 1
X P X ∈ { }
[r ,r ]:= r ,...,r . All logarithms are taken to the natural base unless stated otherwise. The Kullback-Leibler
1 2 1 2
{ }
(KL) divergence between two distributions PX and QX is denoted by D(PX QX), the binary entropy function by
k
h b(t):= −tlog(t) −(1 −t)log(1 −t), and the total variation distance by d TV(PX,QX) :=
x
|PX(x) −QX(x) |.
Let alphabets and be given. The mutual information functional between the input distribu∈tXion PX over and
X Y P X
the output distribution over induced by channel W
j
is denoted by I(PX;W j), and the capacity of the DMC W
j
Y
is denoted by C(W ).
j
Let W be a collection of k DMCs, W : . The goal of the learner is to identify the index j⋆
j j [k] j
of the { DMC} w∈ith the largest capacity, i.e., j⋆ := argX m→ ax Y C(W ), which we assume to be unique (otherwise,
j [k] j
identifying the best channel is impossible), and that the ma∈ximal channel capacity is finite. The suboptimality gap
∆ of a certain channel j is defined as the difference between its capacity and the capacity of the best channel,
j
i.e.,
∆ := C(W ) C(W ).
j j⋆ j
−
To identify the best channel, the learner is given limited access to sense the DMCs, such that for each channel
j she can obtain a set of input-output samples X ,Y , where Y W ( X ) are independently and identically
i i i j i
{ } ∼ · |
distributed (i.i.d).
Specifically, at time t N , the learner chooses a symbol X and a channel index j and observes the
+ t t
∈ ∈ X
output Y W ( X ). The action of the learner is thus the pair A = (X ,j ), and is a function of the past
t j t t t t
∼ t · |
fa rc ot mion ts hean hd isto ob rs yervati to on as nH at ct: i= on,{( AX s =,j πs,Y (s) } )s .∈[t −1] (and thus is random). A policy π = (π t) t ∈N + is a mapping
t t t t
H H
Given a fixed confidence level δ > 0, the learner is required to find a policy π and an associated stopping time
τ that is adapted to the filtration = ( t) t N , where t = σ( t). The policy determines which channel the
F F ∈ + F H
learner should sample at each time instance t [τ] before stopping. In addition, the learner is required to find
∈
an -measurable selection function ψ for which Pr[τ < and ψ( ) = j⋆] δ, that also achieves the minimal
τ τ
F ∞ H 6 ≤
E[τ]. In other words, the figure of merit of a policy π is its expected stopping time. Assume that the learner has
decided to stop (because t = τ( )) such that for each channelshe is given n input-output samples X ,Y .
t j i i i [n ]
Since the channels are
independH
ent, the optimal decision function φ uses the samples from each
resp{
ective
c} ha∈nnj
el
to obtain an estimate Cˆn j(W j) of its capacity C(W j), and outputs the channel with the highest estimate.
III. CAPACITY ESTIMATOR AND A CONFIDENCE INTERVAL
Inthissection,weproposeanestimatorforthecapacityofaDMCW:
X →
Y,givenbyC(W)= max PXI(PX;W),
andstateaconfidenceintervalontheestimationerror.Weomitthechannelindexj inthissectionforeaseofnotation.
To estimate the capacity, the learner is provided with n samples of the channel output obtained for her choice of
inputs. A straightforward approach to capacity estimation is to decompose the mutual information I(PX;W) as
I(PX;W)= H(PY) H PYX PX ,andthenseparatelyestimateeachoftheentropyterms.Confidenceintervalsfor
the entropy terms
w−
ill
then|lea|
d to a confidenceinterval for the mutual information for a fixed PX. Such confidence
(cid:0) (cid:1)
intervals for entropy estimation were originally derived in [25] (see also [32], [34], [39]). Nonetheless, obtaining a
confidenceinterval for the capacity of a DMC is more challengingsince the capacity is the maximumof the mutual
information over all possible input distributions. To address this issue, we utilize the minimax characterization of
capacity [37], [38] to obtain uniform convergence bounds. This characterization will also justify our choice to
allocate an equal number of samples to each input letter in .
X4
In this section, it will be convenient to refer to W as a collection of conditional distributions WYX=x
x
.
With this notation, we may recall that the minimax capacity theorem [37], [38] states that { | } ∈X
C(W)= min maxD WYX=x QY , (1)
QY ( ) x | k
∈P Y ∈X
(cid:0) (cid:1)
where ( ) is the probability simplex over . A simple consequence of the proof of (1) is that the minimizer Q⋆
Y
P Y Y
is in fact the capacity-achieving output distribution P⋆, i.e., the output distribution induced by a capacity achieving
Y
input distribution P⋆. With a slight abuse of notation, let us denote the strict interior of ( ), specifically, the set
X
P Y
of output distributions for which any symbol has a probability mass larger than η > 0, as
η( ) := QY ( ): minQY(y) η .
P Y { ∈ P Y y ≥ }
∈Y
We may then define the pseudo-capacity
C η(W):= min maxD WYX=x QY , (2)
QY ∈Pη( Y) x
∈X
| k
(cid:0) (cid:1)
which is obtained by replacing the minimization over ( ) with a minimization over ( ) ( ). It should
η
P Y P Y ⊂ P Y
be noted that the solution Q⋆ of the minimization problem in (2) may not lead to a valid output distribution, in
Y
eth xe amse pn lese
,
tt hh eat et rh ae sr ue rem sa yy mb be oln fo orin apu bt ind ais rytrib eru at sio un reP cX haf no nr ew
l
h wi ic th
h
erax s∈uXreP pX r( ox b) aW biY li|tX y(y η| /x 2) c co anrr ne os tpo hn ad vs et po roQ b⋆ Y ab. iF lio tyr
P
larger than η, no matter what the input distribution is. Nonetheless, the value of C (W) will serve as an upper
η
bound to the true capacity.
By continuity of the KL divergence in the interior of the simplex, it can be noted that C(W)= lim C (W).
η 0 η
↓
The following lemma is a refinement of this observation:
Lemma 1. For any DMC V : , it holds that
X → Y
0 C (V) C(V) 2η .
η
≤ − ≤ |Y|
Proof. To prove Lemma 1, we rely on the following result that bounds from above the minimum KL divergence
between a reference distribution P ( ) and any Q ( ).
Y
∈ P Y
∗Y
∈
Pη
Y
Lemma 2. Let η (0,1/(2 )). For any P ( ), we have
Y
∈ |Y| ∈ P Y
min D(P Q ) 2η .
Y Y
Q ( ) k ≤ |Y|
Y∈Pη Y
Proof. The proof is provided in Appendix A.
We now prove Lemma 1. It is obvious that C (V) C(V), since ( ) ( ). Furthermore,
η η
≥ P Y ⊂ P Y
C (V)= min maxD(V Q )
η Y X=x Y
Q ( ) x | ||
Y∈Pη Y ∈X
= min max D(V Q P )
Y X Y X
Q ( )P ( ) | || |
Y∈Pη Y X∈P X
(a)
= max min D(V Q P )
Y X Y X
P X∈P( X)Q Y∈Pη( Y) | || |
(b)
max I(P ;V)+ min D(P Q )
X Y Y
≤ P ( ) Q ( ) ||
X∈P X (cid:20) Y∈Pη Y (cid:21)
= C(V)+ max min D(P Q )
Y Y
P ( )Q ( ) ||
X∈P X Y∈Pη Y
(c)
C(V)+2η ,
≤ |Y|
where (a) follows from the minimax theorem ( ( ) and ( ) are convex sets, and the average KL divergence
η
P X P Y
is linear in P , hence concave, and convex in Q ); in (b) P is the marginal induced by the input P and V,
X Y Y X
P (y) = P (x)V (y x); and lastly, (c) follows since Lemma 2 holds uniformly for any P .
Y x
∈X
X Y |X | Y
P5
Consider the following procedure for estimating the capacity of W, given a total budget of n channel sensing
operations.Eachinputsymbolx isfedintothechannel n times(ignoringforsimplicitytheintegerconstraints
as they do not substantially affe∈ ctX the algorithm or its ana|lXys|is), and the empirical conditional distribution of Y
od uen tpo ute td sab my pW lˆ esY |cX o= rx rei ss pc oo nm dip nu gte tod a ins pW uˆ tY x|X(y |x .) T= he|X n c| apai c∈it[n y/ i|Xs|t] h1 e{ nY ei s= timy a| teX d= inx a} n, aw tuh re ar le wY ai, yi a∈
s
[n/ |X|], are the
∈ X P
Cˆn(W):= C(Wˆ )= max I PX;Wˆ ,
PX ( )
∈P X (cid:16) (cid:17)
and the chosen input distribution is any Pˆ X⋆
∈
argmax
PX ∈P(
X)I PX;Wˆ . The uniform allocation of samples to
input symbols is justified by the minimax formulation of capacity, which suggests to target similar worst-case
(cid:16) (cid:17)
estimation errors for every row of the transition matrix (for every x ). Works such as [40] further show that
∈ X
under some conditions the uniform distribution is a reasonable prior for every channel. However, note that we only
use a uniform allocation of the input symbols during the sensing period and optimize overthe input distribution for
determining the capacity of a channel. Otherwise, we would suffer a non-negligible additive loss in the capacity
calculation for each channel. For clarity of exposition, we define
g(α) := 4 log( /α) and
|X||Y| |X|
e2
t(α) := 5 |Y| .
s log |X|
|X| α
Given those two functions, we state in Proposition 1 the confidence bound for capacity estimation.
Proposition 1. Let α 1 be given, then for
≤
5 g(α)log(n t(α)) g(α)
ε= · + ,
4 √n n
p
we have
Pr Cˆn(W) C(W) ε 1 α.
− ≤ ≥ −
h(cid:12) (cid:12) i
Assuming
|X|
=
|Y| ≥
2 and 0
≤
α
≤
1/2(cid:12) (cid:12)so that log |X α| >(cid:12) (cid:12)1, the statement for ε simplifies by bounding t(α) by
a small constant t(α) 1.1.
≤
We provide a sketch of the proof for brevity. The full proof is given in Appendix B.
Sketch of Proof. We use the dual formulation of capacity in (1) and the triangle inequality to express the difference
between C(Wˆ ) and C(W) by the bias of the pseudo-capacities C (W) and C (Wˆ ), which can be bounded as
η η
in Lemma 1, and a probabilistic bound on the difference between C (W) and C (Wˆ ). The latter follows from
η η
expressing the difference of capacities using the minimax characterization in (1) decomposed into entropy and
cross-entropy, which can be individually bounded from above. Setting η = 1/n provides a trade-off between the
different bounds and leads to the statement above.
Remark1. Weusethedualformulationofcapacityfortworeasons.First,thecapacityachievingoutputdistribution
is unique, and so optimizing over this distribution is more natural. By contrast, the capacity achieving input
distribution may not be unique, and optimal input distributions may even have completely disjoint supports. Thus,
the support may not be stable with respect to estimation errors. Second, our confidence interval is obtained by
approximating the capacity with pseudo-capacity, and then estimating the pseudo-capacity. The definition of the
pseudo-capacity is natural in the dual formulation, because it is based on an explicit constraint on the output
probability. The primal formulation of the pseudo-capacity would constrain the input distributions to ones whose
output distribution satisfies that constraint,and thus is less explicit. Without any assumptionson the input or output
distributions, it is unclear how to provide a tight confidence interval for capacity estimation.
Given the confidence interval bound in Proposition 1, we state in Lemma 3 the number of samples n required
to achieve a certain confidence level α for the capacity estimate Cˆn(W). For ease of notation, we ignore ceiling6
operators as their impact vanishes throughout the asymptotic analysis. Let c := 1525 be a constant and t˜(α) :=
1 4·
25t(α) be a scaled version of t(α).
4
Lemma 3. For n satisfying (3) and a given ε > 0, the estimated capacity has an error of at most ε with probability
at least 1 α:
−
c g(α) t˜(α)g(α) 2g(α)
n max 1 log2 , . (3)
≥ ε2 ε2 ε
(cid:26) (cid:18) (cid:19) (cid:27)
For ε 1, the sufficient number of samples is bounded by n 15c 1g(α) log2 t˜(α)g(α) .
≤ ≥ ε2 ε2
Proof. The proof is given in Appendix C. (cid:16) (cid:17)
This result will be used throughout the paper to determine the number of channel senses used by our algorithms
to determine the best channels with a certain probability.
IV. BEST CHANNEL IDENTIFICATION
We now introduce a policy BestChanID whose output jˆ⋆ corresponds to the channel with the highest capacity
withprobabilityatleast1 δ,i.e.,Pr(jˆ⋆ = j⋆) 1 δ.WegiveinTheorem1thestoppingtimeofBestChanIDre-
flectedbythetotalnumbe− rofchannelsensesan≥ dre− ferredtoasthesamplecomplexity.OuralgorithmBestChanID
proceeds in rounds and gradually removes channels that are unlikely to be the best channel from the set of possible
future actions. In the context of MABs, this falls into the class of action elimination algorithms. In particular, this
algorithm is an adaptation of the exponential gap-elimination algorithm proposed in [41].
Let R be the maximum number of rounds for which BestChanID is run, and let be a collection
r r [R]
of nested sets with monotonically decreasing cardinality such that jˆ⋆ = {C } ∈ = [k], i.e.,
R R 1 1
{ } C ⊂ C − ⊂ ··· ⊂ C
the algorithm proceeds until only one channel is left. Denote by τ the time at which round r starts. For all
r
t [τ ,τ 1] the actions allowed by BestChanID are of the form A = (X ,j ) where j .
r r+1 t t t t r
∈ − ∈ C
At every round r [R], the algorithm BestChanID senses each of the channels in for a pre-specified
r
∈ C
number of times n by sending each input symbol x an equal number of times (up to integer rounding), and
r
observes the corresponding channel outputs to obtain∈ anX estimate Cˆn r(W j) of the capacity C(W j) for all j r,
∈ C
as explained in Section III.
After each round, assuming that the best channelis within , BestChanID uses an (ε,δ)-PAC subroutine with
r
C
parameters (ε /2,δ ) computed as in Algorithm 1 that senses the channels in and outputs a channelj that
r r r r r
C ∈ C
satisfiesPr ∆ j
r ≤
ε 2r
≥
1 −δ r. We will later providetwo subroutinessuited for approximatechannelidentification
and analyze their sample complexities. The difference between the subroutines is that one has a better asymptotic
(cid:0) (cid:1)
(in the number of channels) sample complexity. In contrast, the other has a better sample complexity for a small
number of channels.
Having j , the set is constructed by comparing the estimates of the capacities for all j to that of j
r r+1 r r
and discarding all theC channels whose estimated capacity is less than Cˆn r(W
j
) ε r, i.e., ∈ C
r −
= j : Cˆn r(W ) <Cˆn r(W ) ε .
r+1 r r j j r
C C \ ∈ C r −
BestChanID is summarized in Algorithn m 1. The number of times each chano nel j is sampled (sensed) in
r
∈ C
round r is a function of δ (and implicitly of , ) given by
r
|X| |Y|
n 4c g(δ )/ε2log2 4t˜(δ )g(δ )/ε2 , (4)
r ≥ 1 r r r r r
where ε = 2 r/4 and δ = δ/(50r3). The choice of n i(cid:0)s determined by(cid:1)the results for capacity estimation in
r − r r
Section III. The correctness of BestChanID and its stopping time reflected by its sample complexity are stated
in Theorem 1.7
Algorithm 1 BestChanID Best Channel Identification
Require: 0 < δ < 1
Initialize: [k], r = 1, n as in (4)
1 r
C ←
while > 1 do
r
|C |
ε = 2 r/4, δ =δ/(50r3)
r − r
Sample each channel j
r
for n
r
times and let Cˆn r(W j) be the resulting capacity estimate
∈ C
j (ε /2,δ )-PAC subroutine with channels
r r r r
← C
r+1 r
j
r
: Cˆn r(W j) < Cˆn r(W
j
) ε
r
C ← C \ ∈ C r −
Set r r+1n o
←
end while
Output: jˆ⋆ =
r
{ } C
Theorem 1. For an appropriately chosen PAC subroutine, the policy BestChanID described in Algorithm 1
outputs the channel j⋆ with the largest capacity, i.e., j⋆ = argmax C(W ), with probability at least 1 δ. The
j [k] j
sample complexity of BestChanID required for determining the b∈est channel with confidence 1 δ is g− iven by
−
1
∆ 2log log ∆ 1
O
−j
δ
−j
j X∈[k] (cid:18) (cid:16)
1
(cid:17)(cid:19)
log2log log ∆ 1 +log2 ∆ 1 .
δ
−j −j
!
(cid:18) (cid:18) (cid:19) (cid:19)
(cid:16) (cid:17) (cid:16) (cid:17)
additionaltermcomparedtorewardsin[0,1]asin[41]
The dependence of the sample compl|exity on the alphab{ezt sizes and } is hidden in the -notation
|X| |Y| O
(Bachmann–Landaunotation). However, it is important to note that the dominating factor of the sample complexity
scales with . In comparison to the well-known gap-elimination algorithm of [41] for arms with bounded
|X||Y|
rewards in [0,1], the cost of estimating capacity in terms of channel senses is the additional multiplicative term
highlighted in Theorem 1.
Toanalyzetheactualnon-asymptoticsamplecomplexity,weplotinFig.1numericalsimulationsofBestChanID
for a random set of binary DMCs and different levels of confidence. Out of 1024 DMCs, we create four settings
characterized by the combination of 10 or 20 DMCs and different minimal suboptimality gaps of ∆ 0.08 or
min
≈
∆ 0.13, where ∆ := min ∆ . We use two different values for ∆ to represent the hardness of
min min j [k] j⋆ j min
the pro≈ blem. It can be seen that incre∈asi\ng k or decreasing ∆ results in an increasing number of channel senses,
min
where doubling k from 10 to 20 is less harmful than decreasing ∆ by less than a factor of 2.
min
Proof of Theorem 1. We start by proving the sample complexity in Theorem 1. The proof of correctness is given
in Appendix D. To that end, we need Definition 1 and the intermediate results stated in Lemmas 4 to 6. We prove
Theorem 1 afterward.
Definition 1. Let ∆ := min ∆ be the minimum suboptimality gap and ∆ := max ∆ the maximum.
min j:∆ =0 j max j j
j6
Then, we partition the channels for all integers log (1/∆ ) s log (1/∆ ) into sets defined as
⌊ 2 max ⌋ ≤ ≤ ⌈ 2 min ⌉ As
:= j [k] : 2 s ∆ < 2 s+1 .
s − j −
A ∈ ≤
We further define a := and := (cid:8) . (cid:9)
s s r,s r s
|A | C C ∩A
Lemma 4. Choose n to satisfy (4) with equality. We have
r
s 1
− s s s
n = s24slog +4slog log2log .
r
O δ δ δ
Xr=1 (cid:16) (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)
Proof. The proof is given in Appendix E.
Lemma 5. Given an integer s 1 and n chosen to satisfy (4) with equality, we have
r+s
≥
r+1
∞ 1 s s s
n = s24slog +4slog log2log .
r+s
8 O δ δ δ
Xr=0(cid:18) (cid:19)
(cid:16) (cid:16) (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)8
1010
·
2.5 ∆ =0.13,k =10 ∆ =0.13,k =20 min min
∆ =0.08,k =10 ∆ =0.08,k =20
min min
2.0
1.5
1.0
0.5
0.5 0.6 0.7 0.8 0.9 1.0
Confidence Level 1 δ
−
Fig. 1: We simulate BestChanID with two different numbers k 10,20 of randomly generated binary DMCs
∈ { }
(solid and dashed lines) and minimum suboptimality caps ∆ = min ∆ 0.08,0.13 (blue and orange)
min j [k] j⋆ j
for different values of δ between 0.5 and 1. ∈ \ ∈ { }
Proof. The proof is given in Appendix F.
Lemma 6. Let n be chosen to satisfy (4) with equality, then
r
r 1
∞ 1 n− = log 1 log2log 1 .
r
8 O δ δ
r=1(cid:18) (cid:19) (cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
X
Proof. The proof is given in Appendix G.
We are now ready to prove Theorem 1, for which we follow [41, Lemma 3.5] to bound the number of channel
sensing operations. To prove the asymptotic sample complexity, we use as a PAC-subroutine in Algorithm 1 the
MedianChanEl policy introduced and analyzed in the following section. The number of operations required by
the subroutine (cf. Theorem 3) are in the order of n (cf. (4)), and henceforth can be ignored for the analysis.
r
For s 1, the number of times T the learner senses channels from can be bounded by using the fact that
s s
≥ A
from round s onward, the cardinality of the candidate set will decrease (w.h.p.) by a fraction of 1 per round,
Cr,s 8
hence the number of per-round senses, accordingly. Up until round s, we assume the worst-case scenario of a
non-decreasing candidate set described by . In particular, we can write
s
A
s 1
∞ − ∞
T = n n + n
s r,s r s r r,s r
|C | ≤ |A | |C |
r=1 r=1 r=s
Xs 1 X r+1 X
− ∞ 1
a n +a n ,
s r s r+s
≤ 8
r=1 r=0(cid:18) (cid:19)
X X
which we can bound using Lemmas 4 and 5. Fors < 1, meaning for large suboptimality gaps,the cardinality of the
candidate set will decrease (w.h.p.) from the first round onward. Hence, we have T s = a s ∞r=1 81 r −1 n r, which
we can bound by the help of Lemma 6. We can further observe that for all channels j , by Definition 1 we
s
∈ AP (cid:0) (cid:1)
have 2s < 2 . Hence, from Lemmas 4 to 6, we obtain T for all s as
∆ s
j
s s
T = a 4slog log2log +s2
s s
O δ δ
(cid:16) (cid:16) (cid:17)(cid:16) 1 (cid:16) (cid:17) (cid:17)(cid:17)
= ∆ 2log log ∆ 1
O
−j
δ
−j
j X∈As (cid:18)
1
(cid:16) (cid:17)(cid:19)
log2log log ∆ 1 +log2 ∆ 1 .
δ
−j −j
!
(cid:18) (cid:18) (cid:19) (cid:19)
(cid:16) (cid:17) (cid:16) (cid:17)
Summing over all (all channels) concludes the proof.
s
A
sesneS
lennahC
fo
rebmuN
latoT9
Algorithm 2 (ε,δ)-PAC NaiveChanSel
Require: 0 < δ < 1, ε> 0, Set of channels , n as in (5)
Sample each channel j for n times anC d let Cˆn(W ) be the resulting capacity estimates
r j
Output: ˆj = argma∈ x C Cˆn(W )
{ ε } j ∈C j
Remark 2. If a capacity-achievinginput distribution P⋆ ( ) is known for each channel j (or only for the best
X
∈ P X
channel j⋆), or even fixed due to some constraints, then the problem of finding the channel with maximum capacity
boils down to determiningthe channelwith the largestmutual information for P⋆. In this case,the requirednumber
X
of channel senses is obtained by replacing g(α) in (3) by g˜(α) := 2(3 +2)2log(4/α). This follows from using
|X|
the confidence bound for mutual information estimation established in [32] and following the same derivations as
for obtaining Theorem 1.
V. APPROXIMATE BEST CHANNEL IDENTIFICATION
By approximate best channel identification, we consider the problem of identifying with high probability an
ε-best channel j j : ∆ ε such that the identified channel ˆj satisfies
ε j j [k] ε
∈ { ≤ } ∈
Pr ∆ ε 1 δ.
ˆj
ε ≤ ≥ −
(cid:16) (cid:17)
An algorithm that fulfills this property is called (ε,δ)-PAC. A variety of such algorithms have been proposed
and analyzed, most notably in [14]. However, the required assumptions on the bandits’ reward distributions are not
satisfiedin thespecificcaseofbestchannelidentification.In this section,we will focuson two improvedalgorithms
studied in works subsequent to [14], and adapt them to the channel identification problem. After this modification,
each of these algorithms can be used either as a standalone (ε,δ)-PAC policy, or as a subroutine in Algorithm 1.
Tobuild anintuition abouthowthis classofalgorithms work,we will firstintroducea naivesamplingstrategyfor
channel identification, which has been studied for rewards with bounded support in [14]. Showing the correctness
and the sample complexity of this algorithm is simple. The algorithm, which we term NaiveChanSel, senses
each channelj for n times to estimate Cˆn(W ) as explainedin Section III. The policy then outputs the channel
j
∈C
ˆj that maximizes Cˆn(W ) and is shown to be ε-close to the best channel j⋆ with probability at least 1 δ, i.e.,
ε j
Pr ∆ ε 1 δ. NaiveChanSel is summarized in Algorithm 2. The number of required channe− l senses
ˆj
ε ≤ ≥ −
(per channel) follows from Lemma 3 and is given by
(cid:0) (cid:1)
4c g δ 4t˜( δ )g δ 2g δ
n max 1 2k log2 2k 2k , 2k . (5)
≥ ε2 ε2 ε
( (cid:0) (cid:1) (cid:0) (cid:1)! (cid:0) (cid:1))
We continue to analyze the properties of Algorithm 2, i.e., the correctness and the sample complexity, which are
summarized in Theorem 2.
Theorem2. The(ε,δ)-PAC NaiveChanSeldescribedin Algorithm 2 outputsa channelˆj with ∆ ε 2
ε
∈ C
ˆj
ε ≤ ≤
with probability at least 1 δ. The total required sample complexity is given by
−
1
|C| log |C| log2 log |C| .
O ε2 δ ε2 δ
(cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:19)
We assume ε 2 since we will use NaiveChanSel as a subroutine in BestChanID, ensuring that ε 1.
≤ ≤
Proof. Consider the NaiveChanSel described in Algorithm 2. To prove the correctness of the algorithm, let
W be a channel with ∆ > ε. The algorithm chooses n to satisfy (5). We have from Lemma 3 that
j j
∈ C
Pr Cˆn(W )> C(W )+ε/2 δ/(2 ),
j j
≤ |C|
Pr Cˆn(W )< C(W ) ε/2 δ/(2 ).
(cid:0) j⋆ j⋆ (cid:1)
− ≤ |C|
Since ∆ = C(W ) C(W ) > ε, it (cid:0)thus follows that (cid:1)
j j⋆ j
−
Pr Cˆn(W ) < Cˆn(W ) δ/ .
j⋆ j
≤ |C|
(cid:0) (cid:1)10
Algorithm 3 (ε,δ)-PAC MedianChanEl
Require: 0 < δ < 1, ε> 0, Set of channels
C
[k], ε = ε/4, δ = δ/2, r = 1, n as in (6)
1 1 1 r
C ←
while > 1 do
r
|C |
Sample each channel j for n times
r r
Let Cˆn(W ) be the resu∈ ltiC ng capacity estimates
j
Let c be the channel with the median estimate
r
r+1 r
j
r
: Cˆn r(W j) < Cˆn r(W
c
)
C ← C \ ∈ C r
ε r+1 = 3ε r/4n, δ r+1 = δ r/2, r r+1 o
←
end while
Output: ˆj = argmax Cˆn(W ), an ε-best channel with probability 1 δ.
ε j j
{ } ∈C −
By the union bound, we have Pr j :∆ > ε,Cˆn(W )< Cˆn(W ) δ, which proves the correctness.
j j⋆ j
∃ ≤
Let now ε 2, then for ν := 4 a straightforward calculation bounds the total number T of channel senses
≤ |(cid:0)X||Y| (cid:1)
by
T
4c 1ν |C|log 2 |C δ||X|
log2
4t˜(α)ν |C|log 2 |C δ||X|
.
≤ ε2 ε2
(cid:0) (cid:1) (cid:18) (cid:0) (cid:1)(cid:19)
This leads to the complexity stated in Theorem 2.
Such naive strategies sample the candidates several times and select the most promising candidate. They are
known to achieve good empirical results whenever the candidate set (i.e., the number of arms or channels) is small
[42]. However, for a larger set of candidates,the MedianEliminationalgorithm proposed in [14] is commonly
used. This algorithm was shown to be an asymptotically optimal PAC-algorithm. For (ε,δ)-PAC problems with a
reasonably small set of candidate arms (channels in our case) whose rewards are bounded in the interval [0,1], [42]
shows that a naive sampling strategy as in Algorithm 2 is indeed to be preferred over MedianElimination.
Furthermore, the authors of [42] also propose improved sampling strategies that are beneficial over naive sampling
only when the number of arms is larger than 105.
Indeed, we will show that an adaptation of the well-known MedianElimination[14] has a better asymptotic
complexity than NaiveChanSel in terms of -notation, but incurs large multiplicative overheads. We will
O
introduce and analyze a modified version suited for our problem, which will also be used in the asymptotic sample
complexity analysis of BestChanID. However, since the number of channels here is likely to be rather small,
following the same line of argumentation of [42], we expect that for small k, NaiveChanSel leads to smaller
actual sample complexity, despite a slightly worse asymptotic complexity statement. We will verify this statement
both theoretically and experimentally in the sequel.
We call the modified version of MedianElimination MedianChanEl. In the terminology of our paper,
this round-based algorithm discards half of the candidate channels each round which are likely not to be ε-optimal.
We refer to that algorithm as MedianChanEl and provide a summary in Algorithm 3. To fulfill the requirements
of an (ε,δ)-PAC policy, the number of channel senses per round is required to be
4c g(δ /3) 4t˜(α)g(δ /3) 2g(δ /3)
n max 1 r log2 r , r . (6)
r ≥ ε2 · ε2 ε2
(cid:26) r (cid:18) r (cid:19) r (cid:27)
Theorem 3 further summarizes the most important properties of MedianChanEl.
Theorem3. The(ε,δ)-PAC MedianChanEldescribedin Algorithm 3 outputsa channelˆj with ∆ ε 4
ε
∈ C
ˆj
ε ≤ ≤
with probability at least 1 δ. The total required sample complexity is given by
−
1 1 1
|C| log log2 log .
O ε2 δ ε2 δ
(cid:18) (cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:19)(cid:19)
Proof. The proof is given in Appendix H.
One can observe that the asymptotic statement of sample complexity is better than that of NaiveChanSel by
the lack of the factors in both log terms. To verify the hypothesis of NaiveChanSel performing better for a
|C|11
1011
MedianChanEl
NaiveChanSel
1010 Break-Even
109
(ǫ=0.1,δ =0.1)
108
(ǫ=0.1,δ =0.7)
(ǫ=0.3,δ =0.1)
107
101 102 103
Number of Channels
Fig. 2: Comparison of the proposed (ε,δ)-PAC algorithms tested on different parameter sets (ε,δ) with 10 binary
DMCs with random transition matrices W ,j [10]. Solid lines mean (ε = 0.1,δ = 0.1), dashed dotted lines
j
∈
(ε = 0.1,δ = 0.7) and dashed lines (ε = 0.3,δ = 0.1).
small set of channels, we simulate the algorithms with a random set of binary DMCs for different (ε,δ) values and
plot the resulting actual number of required channel senses in Fig. 2. We use (ε = 0.1,δ = 0.1) as a baseline and
observethe impact of increasing either the former to ε= 0.3 or the latter to δ = 0.7. As expectedfrom Theorems 2
and 3, one can find that increasing ε has a much larger impact on the number of channel senses than increasing δ.
Further, while increasing ε does not impact the break-even point at which MedianChanEl gets beneficial over
NaiveChanSel, increasing δ shifts the break-even point to the left, i.e., towards a smaller number of channels.
VI. AN INFORMATION-THEORETIC LOWER BOUND
This section establishes a lower bound on the expected sample complexity of any best channel identification
algorithm with confidence1 δ. To that end, we consider a bandit model described by the set of channels specified
−
byconditionaldistributionsw := (W ,...,W ,W ,W ,...W ),whereforallj [k]thereexistsaprobability
1 a 1 a a+1 k
density W , W (Y X) for which we require − that W , where is a suitable p∈ robability measure. To derive
j j j
| ∈ W W
a lower bound, we consider a relaxed version of the best channel identification problem introduced above. In
particular, we assume the learner knows the transition matrices of all channels W beforehand; and hence, must
j
correctly assign the channels to their transition matrices. Such a treatment of the problem facilitates enhanced
sampling strategies that utilize the given knowledge of the environment, and provides a bound on the problem
studied in this paper. To prove the lower bound, we rely on a change of measure argument that is based on altered
versionsofdistributionw,whereanelementa [k]isreplacedbyanalternativechannel.Letthealtereddistribution
∈
for any a [k] be w := (W ,...,W ,W ,W ,...W ). The channel W is chosen such that it changes the
desired pro∈ bability
ofa′
the
eve1
nt of
idena
−
ti1 fyinga′ thea b+ e1
st
chank
nel j⋆ under
distriba′
ution w of at least 1 δ to at most
−
δ under distribution w . For this to happen, the best channel under w should not be j⋆. The following assumption
a′ a′
formally describes the requirements for such altered distributions.
Assumption 1. Consider a model w. Assume that for any a [k] j⋆ there exists a model w with W
∈ \
a′ a′
∈ W
such that for every input symbol x it holds that D(W ( x) W ( x)) < D(W ( x) W ( x)) <
∈ X
a
· | k
j⋆
· |
a
· | k
a′
· |
D(W ( x) W ( x))+ζ and that C(W )> C(W ) for any ζ > 0. For a = j⋆, we further assume that there
a
· | k
j⋆
·|
a′ j⋆
existsW suchthatforthesuboptimalchannelj [k] j⋆ andthecorrespondinginputsymbolx that
j′⋆
∈ W ∈ \{ }
j
∈ X
maximizes D(W ( x ) W ( x )) it holds that D(W ( x ) W ( x )) < D W ( x ) W ( x ) <
j⋆
· |
j
k
j
· |
j j⋆
· |
j
k
j
· |
j j⋆
· |
j
k
j′⋆
· |
j
D(W j⋆(
·
|x j) kW j(
·
|x j))+ζ, where C(W j′⋆)< C(W j) for any ζ > 0. (cid:16) (cid:17)
Informally, the above assumption ensures that for every suboptimal channel j [k] j⋆ , there exists an
∈ \ { }
alternative channel close to j⋆ with a larger capacity than j⋆. Similarly, for the best channel j⋆, there exists an
alternative channel that is close to the furthest suboptimal channel j (in terms of KL-divergence) with a smaller
capacity than j. With this at hand, we are now ready to state the best-case sample complexity.
sesneS
lennahC
fo
rebmuN
latoT12
Theorem 4. Considering a model w satisfying Assumption 1, the minimum number of required channel senses to
identify the capacity-maximizing channel with probability at least 1 δ is
−
log(1/2.4δ)
E [σ]
w
≥ max D W (πa πa(x)) W ( x)
a ∈X[k] \j⋆ x ∈X a y | x k j⋆ · |
(cid:0) log(1/2.4δ) (cid:1)
+ ,
max D W ( x) W (πa πa(x))
j ∈[k] \j⋆,x ∈X j⋆ · | k j y | x
where σ is the stopping time of the algorithm, and πa,π(cid:0)a are the permutations of i(cid:1)nput and output symbols for
x y
each suboptimal channel a [k] j⋆ that maximize the lower bound on E [σ].
w
∈ \
While the lower bound depends on the explicit statistical properties of the channels under consideration (i.e.,
their transition matrices), the sample complexities proved by our algorithms are functions of the suboptimality
gaps. This discrepancy introduces a natural gap between the lower bound and the sample complexities stated
above. Bounding the number of channel senses in terms of the suboptimality gaps ∆ from below would be a
j
desired step but requires an upper bound on the quantity min max D(W (π x) W ( x)), and hence,
π ,x′ x a y ′ j⋆
y ∈X | k · |
on max D(W ( x) W ( x)) for some permuted W that minimizes the suboptimality gap. We detail in
x
∈X
j′
· | k
j⋆
· |
j′
the following the lack of a direct relation between the maximal KL divergenceand the suboptimality gap, and show
that expressing our lower bound using suboptimality gap is impossible (at least, without additional assumptions).
Tothatend,considerasthebestamongallDMCsin the candidateset anapproximationofa binaryasymmetric
C
channel (Z-channel) characterized by probability q > 0 and transition matrix
1 ε ε
W j⋆ = − q 1 q, ,
(cid:18) − (cid:19)
where ε 0 is a vanishing constant. In addition, let channel with index j = j⋆ be a binary symmetric channel
→ 6
(BSC) with crossover probability p > 0 and transition matrix
p 1 p
W j = 1 p − p .
(cid:18) − (cid:19)
Hence,whenε 0,thecapacityofthebestchannelj⋆ approachesthecapacityofaZ-channel,i.e.,lim C(W )=
ε 0 j⋆
→ q →
log 1+(1 q)1−q (0,log(2)]. Recall the capacity of the BSC j being C(W j) = 1 h b(p) [0,log(2)).
− ∈ − ∈
By the continuity of these two capacity formulas, for any q and ε 0, there exists a p parameterizing a
(cid:16) (cid:17)
→
suboptimal DMC j with vanishing suboptimality gap, i.e., lim ∆ = 0 for some pair (p,q). At the same
ε 0 j
→
time, the measure lim min max D(W (π x) W ( x)) = goes to infinity. Since naturally the
ε 0 π ,x′ x a y ′ j⋆
→ y ∈X | k · | ∞
upper bound on min max D(W (π x) W ( x)) should increase with ∆ , the existence of such two
π ,x′ x a y ′ j⋆ j
y ∈X | k · |
channels contradicts the existence of a general bound that relates the divergence of individual rows in the transition
matrices of the best and a suboptimal channel to the suboptimality gap of the latter. This finding raises the question
of which figure of merit actually determines the difficulty of identifying channels with high capacity.
Proof of Theorem 4. We follow the proof method of [12]. First, recall that the action at time t is A = (X ,j ),
t t t
i.e., the pair of a selected channel and an input symbol to be transmitted, and consider a certain a [k]. We next
∈
define the following log-likelihood ratio of observing a certain set of actions and rewards under model w and w ,
a′
where j [k] a , we set W = W :
∀ ∈ \{ }
j′ j
L = L (A ,...,A ,Z ,...,Z )
t t 1 t 1 t
k t
W (Z x)
:= 1 A = (x,j) log j l |
l
{ } W (Z x)
j=1 l=1x
j′ l
|
!
XXX∈X
k N x,j W (Z x)
j x,j,l
= log | .
W (Z x)
j=1x l=1
j′ x,j,l
|
!
XX∈X X13
The last equality holds from the i.i.d. assumption of the channel model considering N as the number of times
x,j
channel j has been sensed with input symbol x and (Z ) as the sequence of subsequently observed symbols
x,j,l
from that channel j. Next, applying Wald’s Lemma [43] to the log-likelihood ratio L at stopping time σ yields
σ
k
W (Z x)
E [L ] = E [N ]E log j x,j,l |
w t w x,j w
W (Z x)
j=1x
" j′ x,j,l
|
!#
XX∈X
k
= E [N ]D W ( x) W ( x)
w x,j j
· | k
j′
·|
j=1x
XX∈X (cid:0) (cid:1)
= E [N ]D W ( x) W ( x) ,
w x,a a
· | k
a′
· |
x
X∈X (cid:0) (cid:1)
where the last equality holds since W = W for all j = a. To relate the expected value of the log-likelihood
j j′
6
ratio considering the two different models w and w to the corresponding probabilities of a certain event upon
a′
E
stopping time σ, we make use of the following lemma from [12]:
Lemma 7 ([12, Lemma 19]). For every event , we have
σ
E ∈ F
E [L ] d (Pr( ),Pr( )),
w t b
≥ w E w′ E
where d (x,y) is the binary relative entropy, and Pr ( ) and Pr ( ) are the probabilities of the event under
b w w′
E E E
the models w and w , respectively.
′
We use the result of Lemma 7 with the event of selecting the best channel j⋆ under w as the best channel.
By the requirements for every algorithm, the probability of this happening under model w is bounded from below
by 1 δ and for w bounded from above by δ. This is because, under the latter model, j⋆ is no longer the best
−
a′
channel. We start with bounding the number of channel senses of all suboptimal channels j [k] j⋆ . Bounding
∈ \{ }
the KL-divergence terms by the maximum over all x to later obtain a bound on E [N ], we simplify
∈ X x w x,a
to ∈X
P
E [N ]maxD W ( x) W ( x)
w x,a
x
a
· | k
a′
· |
x ∈X
X∈X (cid:0) (cid:1)
E [N ]D W ( x) W ( x)
≥
w x,a a
· | k
a′
· |
x
X∈X (cid:0) (cid:1)
sup d (Pr( ),Pr( )) d (1 δ,δ) log(1/2.4δ).
b b
≥ w E w′ E ≥ − ≥
E∈Fσ
Now using Assumption 1 and letting ζ go to zero, we get for every suboptimal channel a [k] j⋆ that
∈ \
log(1/2.4δ)
E [N ]
w x,a
≥ max D(W ( x) W ( x))
x X∈X
x
∈X
a
· | k
a′
· |
log(1/2.4δ)
≥ max D(W ( x) W ( x))+ζ
x a j⋆
∈X · | k · |
ζ 0 log(1/2.4δ)
→= .
max D(W ( x) W ( x))
x a j⋆
∈X ·| k · |
To tighten the lower bound, we can decouple the action A = (X ,j ) from the assignment of an input symbol
t t t
x for the selected channel j , i.e., for each channel there exists a bijective function that maps the action X to
′t t t
an input symbol x . We choose that function to maximize the lower bound, which is equivalent to considering all
′t
row permutations πa of W . Similarly, we can choose a mapping from observation Y to an output symbol Y
x a t t′
to maximize the lower bound, which can be done by optimizing over all possible column permutations πa of the
y
channel matrix W . Considering that we are optimizing over the choice of the input symbol, the optimization over
a
a permutation of input symbols translates to optimizing over a specific choice of the symbol. For any choice of πa
x
and πa, which we detail in the sequel, we get
y
log(1/2.4δ)
E [N ] . (7)
w x,a
≥ max D W (πa πa(x)) W ( x)
x X∈X x ∈X a y | x k j⋆ · |
(cid:0) (cid:1)14
Similarly to above, for the uniquely optimal channel a = j⋆, we consider the most impactful change of measure
with respect to the pair of channel and corresponding input symbol that maximizes the KL-divergence compared
with j⋆. This is captured by Assumption 1. In a similar way to which the previous bound was derived, we obtain
log(1/2.4δ)
E [N ]
w x,j⋆
≥
max D W ( x) W ( x)
x
X∈X
x
∈X
j⋆
· | k
j′⋆
· |
lo(cid:16)g(1/2.4δ) (cid:17)
≥ max max D(W ( x) W ( x))+ζ
j [k] j⋆ x j⋆ j
∈ \ ∈X · | k · |
ζ 0 log(1/2.4δ)
→= .
max D(W ( x) W ( x))
j [k] j⋆,x j⋆ j
∈ \ ∈X · | k · |
To tighten this lower bound, we apply a similar argument as above. Considering any good choice of row and
column permutations πa and πa of W for all a [k] j⋆, we have the following bound:
x y a ∈ \
log(1/2.4δ)
E [N ] (8)
w x,j⋆
≥ max D W ( x) W (πa πa(x))
x X∈X a ∈[k] \j⋆,x ∈X j⋆ · | k j y | x
We combine the result for all suboptimal channels in (7) wi(cid:0)th the result for the optimal(cid:1)channel in (8) and obtain
E [σ] E [N ]+ E [N ]. (9)
w w x,a w x,j⋆
≥
a [k] j⋆x x
∈X\ X∈X X∈X
It remains to find a good choice of the permutations πa,πa that maximizes the overall lower bound. This
can be done by maximizing (9) with respect to all
p{ ossx ibley} pa e∈rm[k] u\tj a⋆
tions. In particular, we have
1
πa,⋆,πa,⋆ = argmax
{ x y }a ∈[k] \j⋆
{π xa,π ya
}a∈[k]\j⋆max
a ∈[k] \j⋆,x
∈X
D W j⋆(
· |
x) kW j(π ya
|
π xa(x))
1
+ (cid:0) (cid:1)
max D W (πa πa(x)) W ( x)
a ∈X[k] \j⋆ x ∈X a y | x k j⋆ · |
Using the above choice of πa,⋆ ,πa,⋆ yields the statement in T(cid:0) heorem 4. (cid:1)
x y a [k] j⋆
{ } ∈ \
VII. CONCLUSION
We considered the problem of identifying the channel with the maximal capacity out of k DMCs by using their
input-output samples. To this end, we established a confidence interval bound for capacity estimation using the
minimax characterization of capacity. We formulated the problem as best arm identification in the MAB context,
proposed algorithms in the fixed confidence setting, and analyzed their sample complexity both theoretically and
through numerical simulations. We further introduced an information-theoretic explicit lower bound on the sample
complexity of every best channel identification problem based on the channel properties. Our proposed capacity
estimation algorithm uniformly distributes the samples across the channel’s input letters . Nonetheless, a small
X
subset of the input alphabet may only support the capacity-achieving input distribution. By identifying this subset
throughout the algorithm, the learner may instruct the transmitter to refrain from sampling input letters not in this
subset.Analyzingsuchalgorithms is challengingbecausethe capacity-achievinginputdistribution is notnecessarily
unique, and standard capacity computation algorithms such as Blahut-Arimoto [44], [45] tend to produce fully
supportedinputdistributions.Evenmore,thesupportofthecapacity-achievinginputdistribution mightnotbestable
with respectto estimation errors ofthe channel.This direction is thusleft for future research.Further analysisofthe
figures of merit that reflect the problem’s difficulty and finding a lower bound based on the suboptimality gaps to
bridge the gap between the achieved sample complexities and the lower bound is also left for future investigations.15
APPENDIX
In Appendix A we prove the result of Lemma 2, which was stated in Section III and is used in the proof
of Lemma 1. In Appendix B, we prove the confidence bound for capacity estimation stated in Proposition 1. In
Appendix C, we prove the result for the number of samples that suffice to achieve a certain confidence level when
estimating the capacity of a DMC, which was stated in Lemma 3 and is required to prove Theorems 1 to 3.
In Appendix D, we prove the correctness of BestChanID. In Appendices E to G, we provide the proofs for
Lemmas 4 to 6, which we used in Section IV to prove the sample complexity given in Theorem 1. Lastly, in
Appendix H, we prove the sample complexity of MedianChanEl.
A. Proof of Lemma 2
Proof. We want to prove that for every P (y) in the simplex ( ), there exists a distribution Q in the simplex
Y
P Y
∗Y
( ) such that D(P Q ) is bounded by 2 η 1. Let Q be a modified distribution of P (y) constructed as
Pη
Y
Y
k
∗Y
|Y| ≤
∗Y Y
1
Q = max P (y),ηξ ,
∗Y
ξ {
Y
}
with a normalization constant ξ that satisfies
ξ = max P (y),ξη
Y
{ }
y
X∈Y
(P (y)+ξη) = 1+ξη .
Y
≤ |Y|
y
X∈Y
Consequently, we have ξ 1 . Using that result and the above definition, we can write
≤ 1 η
− |Y|
min D(P Q ) D(P Q )
Q ( )
Y
k
Y
≤
Y
k
∗Y
Y∈Pη Y
P (y)
Y
= P (y)log
Y
Q
y
∗Y
X∈Y
ξP (y)
Y
= P (y)log
Y
max P (y),ξη
Y
y { }
X∈Y
P (y)
Y
= log(ξ)+ P (y)log
Y
max P (y),ξη
Y
y { }
X∈Y
P (y)
Y
= log(ξ)+ P (y)log
Y
ξη
y :ξη>P (y)
∈Y X Y
1 η
log(ξ) log |Y|
≤ ≤ 1 η ≤ 1 η
(cid:18) − |Y|(cid:19) (cid:18) − |Y|(cid:19)
2η .
≤ |Y|
This concludes the proof.
B. Proof of Proposition 1
To prove Proposition 1 we will need the following lemma, which is a standard bound on the empirical total
variation distance, and is derived here for completeness and convenience.
Lemma 8. Let Y P be independent and identically distributed, and Pˆn be the empirical distribution based on
i ∼ Y Y
n observed samples. Then,
4 log 1
d Pˆn,P |Y| α
TV Y Y ≤ s n
(cid:16) (cid:17)16
with probability 1 α.
−
Proof. The total variation distance d (Pˆn,P ) satisfies a bounded difference inequality with constant 1/n as a
TV Y Y
function of (Y ,...,Y ), and so by McDiarmid’s inequality [46, Theorem 3.11]
1 n
Pr d (Pˆn,P ) E d (Pˆn,P ) ǫ e 2nǫ2,
TV Y Y − TV Y Y ≥ ≤ −
or h h i i
log 1
0 d Pˆn,P E d (Pˆn,P ) + α
≤ TV Y Y ≤ TV Y Y s 2n
(cid:16) (cid:17) h i
with probability 1 α. Furthermore,
−
E d (Pˆn,P )
TV Y Y
h i
= E Pˆn(y) P (y)
 Y − Y 
y X∈Y(cid:12)
(cid:12)
(cid:12)
(cid:12)
(a)  (cid:12) (cid:12) 2
E Pˆn(y) P (y)
≤ s Y − Y
y X∈Y (cid:20)(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:21)
(b) 1 (cid:12) (cid:12)
= P (y)(1 P (y))
Y Y
n −
r
y
X∈Y
(c) P (y)
Y
(1 P (y))
v Y
≤ u n  − 
u y y
u
X∈Y X∈Y
t  
1
= |Y|− |Y|,
n ≤ n
r r
where (a) follows from Jensen’s inequality; (b) follows from the variance of an unbiased estimator for a Bernoulli
random variable; and (c) follows from Cauchy-Schwarz inequality. The result then follows since √a + √b
≤
2(a+b) and so
p
log 1 log 1 +2
α + |Y| α |Y|
s 2n n ≤ s n
r
4 log 1
|Y| α.
≤ s n
Proof of Proposition 1. The idea is to relate C(Wˆ ) to the estimate Cn(Wˆ ) of the pseudo-capacity for a properly
η
chosenη = 1/n. The value of η controls the bias-variancetrade-off. The bias is large for large η, yet the estimation
variance is lower.
To this end, our first goal is to obtain a high probability bound on the estimation error of C (W) from
η
Cˆn(W)= min maxD(Wˆ Q ).
η Q ( ) x Y |X=x || Y
Y∈Pη Y ∈X
From Lemma 8, it holds for any x that n/ samples lead to
∈ X |X|
d Wˆ (y x),W (y x) 4 |X||Y|log |X α| =:κ, (10)
TV Y |X | Y |X | ≤ s n
(cid:16) (cid:17)17
with probability larger than 1 α/ . By the union bound, the same holds simultaneously for all x with
probability 1 α. We next ass− ume | tX ha| t this event holds, and evaluate the error C (Wˆ ) C (W). To∈ begX in, we
η η
− | − |
note that
D(W Q )
Y X=x Y
| ||
= H(W ) W (y)logQ (y),
Y X=x Y X=x Y
− | − |
y
X∈Y
and so for any x and Q := ( ) the triangle inequality implies
Y η η
∈ X ∈ Q P Y
D(Wˆ Q ) D(W Q )
Y X=x Y Y X=x Y
| || − | ||
(cid:12) (cid:12)
(cid:12) H(Wˆ ) H(W ) (cid:12)
(cid:12)≤ Y |X=x − Y |X=x (cid:12)
(cid:12) (cid:12)
(cid:12)+ Wˆ (y) W (cid:12) (y) logQ (y) . (11)
(cid:12) Y |X=x − Y |X=(cid:12)x | Y |
y X∈Y(cid:12)
(cid:12)
(cid:12)
(cid:12)
The first term in (11) is an entropy diffe(cid:12)rence term, and can be bo(cid:12)unded from the TV bound on the entropy
difference in [47, Problem 3.10]. In the following, we use for any x,y the notation x y := max x,y . Note the
∨ { }
elementary inequality on the binary entropy function
1 1
h (t) := tlog +(1 t)log
b
t − (1 t)
−
1
tlog +t, (12)
≤ t
which holds for any t [0,1]. Using the bound in [48], [49] (Equations (4) and (11), respectively) implies that for
any Wˆ ,W ∈ ( )
Y X Y X
| | ∈ P Y
H(Wˆ ) H(W )
Y X Y X
| − |
(cid:12) 1 (cid:12)
(cid:12) d Wˆ ,W (cid:12)log( 1)
≤(cid:12) 2 TV Y |X Y |X (cid:12) |Y|−
(cid:16) 1 (cid:17)
+h d Wˆ ,W
b TV Y X Y X
2 | |
(cid:18) (cid:19)
(cid:16) (cid:17)
(a) 1
d Wˆ ,W log( 1)
TV Y X Y X
≤ 2 | | |Y|−
1 (cid:16) (cid:17) 1
+ d Wˆ ,W log
TV Y X Y X
2 | | 1d Wˆ ,W
(cid:16) (cid:17) 2 TV Y X Y X
| |
1 (cid:16) (cid:17)
+ d Wˆ ,W
TV Y X Y X
2 | |
(b) κ (cid:16) κ (cid:17) 2 κ
log( 1)+ log +
≤ 2 |Y|− 2 κ 2
(c) κ 2 κ
log |Y| + , (13)
≤ 2 κ 2
(cid:18) (cid:19)
where (a) follows from the bound in (12), (b) follows since d (Wˆ ,W ) κ defined in (10) and noticing
TV Y X Y X
that total variation is always bounded by 2. | | ≤
Remark 3. A tighter bound could be found by considering the relation between local variation and total variation
distance as established in [50]. However, the benefits thereof are limited in the regime of interest for this paper.
Remark 4. Alternatively to bound the entropy difference using a Fano-type inequality as above, similarly to
[32], we could utilize McDiramid’s inequality to bound the maximum change a single replacement in the mea-
surements could carry to the entropy estimation. Taking into account the estimation bias, one can show that18
H(Wˆ ) H(W ) κ log(n/ )+ κ2 with probability at least 1 α/ , which is a looser bound than
Y |X − Y |X ≤ √2 |X| 2 − |X|
(cid:12)the one in (13).
(cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
Next, the second term in (11) is bounded for any Q as
Y η
∈ Q
Wˆ (y) W (y) logQ (y)
Y X=x Y X=x Y
| − | | |
y X∈Y(cid:12)
(cid:12)
(cid:12)
(cid:12)1 1
d(cid:12) Wˆ ,W log(cid:12) κlog . (14)
TV Y X=x Y X=x
≤ | | η ≤ η
(cid:16) (cid:17)
Hence, inserting (13) and (14) to (11) results
D(Wˆ Q ) D(W Q )
Y X=x Y Y X=x Y
| || − | ||
(cid:12) κ 2e (cid:12)
(cid:12) log |Y| .(cid:12)
(cid:12) ≤ 2 η2κ (cid:12)
(cid:18) (cid:19)
Consequently, the pseudo-capacity estimation error is similarly bounded as
C (Wˆ ) C (W)
η η
−
(cid:12) (cid:12)
(cid:12)= min maxD((cid:12)W Q ) min maxD(Wˆ Q )
(cid:12) (cid:12) Y X=x Y Y X=x Y
(cid:12)Q Y∈Qη x ∈X | || −Q Y∈Qη x ∈X | || (cid:12)
(cid:12) (cid:12)
(cid:12)max max D(W Q ) D(Wˆ Q ) (cid:12)
≤ (cid:12)Q
Y∈Qη
x
∈X(cid:12)
Y |X=x || Y − Y |X=x || Y
(cid:12)
(cid:12)
κ 2e (cid:12) (cid:12)
log |Y(cid:12)| . (cid:12) (15)
≤ 2 η2κ
(cid:18) (cid:19)
Recalling that t(α) := 5 |Y|e2 , by the triangle inequality
log|X|
r|X| α
C(Wˆ ) C(W) =
−
(cid:12) (cid:12)
(cid:12)= C(Wˆ ) C ((cid:12)Wˆ )+C (Wˆ ) C (W)+C (W) C(W)
η η η η
(cid:12) − (cid:12) − −
(cid:12) (cid:12)
(cid:12)C(Wˆ ) C (Wˆ ) + C (Wˆ ) C (W) + C (W) C(W(cid:12) )
η η η η
≤ (cid:12) − − | − (cid:12) |
(a)(cid:12)
κ
2(cid:12)
e
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)2η + log (cid:12) |Y(cid:12)| +2η (cid:12)
≤ |Y| 2 η2κ |Y|
(cid:18) (cid:19)
(b) 5
κlog(t(α)n)+4|Y|
≤ 4 n
(c) 5 4 log |X|log2(t(α)n) 4 log |X|
|X||Y| α + |X||Y| α ,
≤ 4s n n
where (a) holds from (15) and utilization of Lemma 1 twice (for V Wˆ and then for V = W), (b) follows by
≡
using the definition of κ and setting η = 1 resulting in
n
κ 4e2 2 κ n e2
log |Y| = log |Y|
4 (cid:18) η4κ2 (cid:19) 4 η4 |X|log |X α|!
κ n5 e2 5κ e2
= log |Y| = log n 5 |Y| ,
4 log |X|! 4 s log |X|!
|X| α |X| α
and (c) holds by replacing the value of κ, and slightly relaxing the inequality using the fact that 4 |Y| κ2, which
n ≤ 2
holds since 2 and log( /α) > 1 by the choice of α.
|X|≥ |X|19
C. Proof of Lemma 3
To prove Lemma 3 and thereby bound the number of samples n required to achieve a certain confidence for
estimating the capacity associated with a channel, we need a refinement of Lemma 15 from [35], which we state
and prove in the following.
Lemma 9 (Refinement of [35, Lemma 15]). If n 15log2(ρ/y)/y, then it holds that log2(ρn) y.
≥ n ≤
Proof. The proof is a refinement of [35] for the case where r = 2. For n 1,
log2(ρn)
y has a unique maximum
≥ n ≤
of 4ρ at n = e2. For y [0, 4ρ], by using n = 15log2(ρ/y)/y, we get
e2 ρ ∈ e2
log2(ρn) log2((ρ/y)15log2(ρ/y))
= y
n · 15log2(ρ/y)
log2((ρ/y)15log2(ρ/y))
y sup
≤ · 15log2(ρ/y)
y [0,4ρ]
∈ e2
y′:=y/ρ log2((1/y )15log2(1/y ))
′ ′
y sup
≤ · 15log2(1/y )
y′= [0, 4 ] ′
∈ e2
y,
≤
with the value of the supremum determined numerically. Note that this result can be generalized to arbitrary values
of r in
logr(ρn)
y.
n ≤
Proof of Lemma 3. To make sure the confidence bound holds, we require that each of the two summands in
Proposition 1 is lower or equal to ε/2. Recall that g(α) := 4 log( /α). We first consider the left term, i.e.,
|X||Y| |X|
we require that
5 g(α)log(t(α)n) ε
4 √n ≤ 2
p
(Equivalently)
4ε2 log2(t(α)n)
. (16)
25g(α) ≥ n
Applying Lemma 9 to (16) with y = 4ε2 and ρ =t(α) leads to the first argument in Lemma 3, i.e., we require
25g(α)
that n 15 25g(α) log2(t(α)25g(α) ). Imposing the same requirement on the right term, i.e., g(α) ε, yields the
≥ · 4ε2 4ε2 n ≤ 2
second argument in Lemma 3. Since both requirements have to hold, we take the maximum of the two results,
which proves Lemma 3.
D. Proof of Theorem 1
The correctness of Algorithm 1 as stated in Theorem 1 is proved following the same lines as in [41]. We provide
it here for completeness, while adapting it to our problem formulation. First, we need the following result that was
proven in [41] for a different type of concentration inequalities.
Lemma 10. Assume a set of channels that contains the best channel j⋆. Let now j be an (ε /2,δ )-best
r r r r r
C ∈ C
arm. Then, for estimations based on n samples according to (4), we have with probability at least 1 2δ that
r r
−
Cˆn r(W j⋆) Cˆn r(W
j
) ε r. (17)
≥ ε −
Proof. The proofs follows the same lines as in [41]. We first observe that C(W ) C(W ). Hence, from
j j⋆
Lemma 3 we have that Cˆn r(W
j
) C(W
j
) + ε r/2 C(W j⋆) + ε/2 with probar bili≤ ty at least 1 δ r. Thus,
r ≤ r ≤ −
Pr Cˆn r(W
j
) C(W j⋆)+ε r/2 δ r. From Lemma 3, we further have Pr C(W j⋆) Cˆn r(W j⋆)+ε r/2 δ.
r ≥ ≤ ≥ ≤
Applying the union bound yields
(cid:16) (cid:17) (cid:16) (cid:17)
Pr Cˆn r(W
j
) Cˆn r(W j⋆)+ε r/2 2δ r.
r ≥ ≤
(cid:16) (cid:17)20
Taking the complementary event concludes the proof.
Lemma 11 ( [41, Lemma 3.4]). Consider Algorithm 1, then Pr > 1 24δ for any r s 1.
|Cr,s
|
8|Cr −1,s
| ≤
r
≥ ≥
Proof. The proof of [41] holds verbatim, since our subroutin(cid:0)e outputs an (ε /(cid:1)2,δ )-best channel from either
r r
NaiveChanSel or MedianChanEl.
We are now ready to prove the correctness of Algorithm 1 stated in Theorem 1.
Proof of Theorem 1. Consider Algorithm 1 with confidence level 1 δ. We utilize Lemma 3 with confidence level
α = δ r = 50δ r3 and a maximal deviation of ε= ε 2r = 2− 8r, to obtain− that
4c 4c g(δ )
n 1 log2 2 r .
r ≥ ε2 ε2
r (cid:18) r (cid:19)
By that choice of n , we have by Lemma 10 that Algorithm 1 removes a best channel from the set in any of
r r
C
the rounds with probability at most 2δ . From a union bound over all rounds r with δ = δ/(50r2), the probability
r r
that the best channel gets eliminated in any of the rounds is at most ∞r 2δ r = 2 6π 52 0δ
≤
δ/5. We apply Lemma 11
to prove a decreasing cardinality of as r grows. By union bound, the probab·ility that the cardinality of
r r,s
decreases by less than a factor of 8 inC all of the rounds r > s is at mP ost ∞r=124δ
r
= 624 5π 02 rδ
2 ≤
4δ/5. Hence,C the
algorithm terminates and outputs j⋆ with probability at least 1 δ, which concludes the p·roof.
− P
E. Proof of Lemma 4
For notational convenience, we denote the quantities ν := 4 , ζ := 50 and γ := 4 3 > √3 ζ. We
|X||Y| |X| |X|
then further denote f(r) := log ζr3 . Throughout the analysis of the sample complexity, we use c := t˜(1) =
δ p 2
5 |Y lo| ge2 as a constant upper (cid:16)boun(cid:17)d for t˜(α), i.e., c 2
≤
t˜(α) for all α> 0.
|X| |X|
Pqroof of Lemma 4. We make use of the following inequality that holds for any a,b R:
∈
(a+b)2 2(a2+b2). (18)
≤
We first bound rs −=1 1n
r
as
P s −1
n =
s −1 4c 1g(δ r)
log2
4c 2g(δ r)
r ε2 ε2
r=1 r=1 r (cid:18) r (cid:19)
X X
( =a)
s −1
64c ν 4rlog
ζr3
log2 64c ν 4rlog
ζr3
1 2
· δ · δ
r=1 (cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:19)
X
s 1
(b) −
64c ν 4rf(r) 2 r2log2(4)+log2(64c νf(r))
1 2
≤ · ·
r=1
X s 1 (cid:0) (cid:1)
(c) − γr
< 384c ν 4rlog r2log2(4)
1
· δ
Xr=1 (cid:16) (cid:17)
γr
+log2 192c νlog ,
2
δ
!
(cid:16) (cid:16) (cid:17)(cid:17)
where (a) is obtained by plugging the values of g(δ ) and ε ; (b) is obtained by using (18), and (c) follows from
r r
f(r)= log ζr3 log γr 3 = 3log γr and yields Lemma 4.
δ ≤ δ δ
(cid:16) (cid:17)
(cid:0) (cid:1) (cid:0) (cid:1)21
F. Proof of Lemma 5
To prove the result of Lemma 5, we need the definitions from Appendix E, the results of Lemmas 10 and 11
and the following intermediate results.
Claim 1. We have the following upper bounds
∞ r2 ∞ log(r) ∞ r2log(r)
= 6, 1 and 8.
2r 2r ≤ 2r ≤
r=0 r=1 r=1
X X X
Proposition 2. Let r,s be numbers in N+, γ,ζ > 0 such that γ > √3 ζ and 0 δ 1, then
≤ ≤
ζ(r+s)3 2γs
f(r+s) = log 3log +3log(r).
δ ≤ δ
(cid:18) (cid:19) (cid:18) (cid:19)
Proof. We bound the given quantity as
ζ(r+s)3 γ(r+s) 2 γrs
log 3log 3log · ,
δ ≤ δ ≤ δ
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
and split the terms to conclude the proof.
We are now ready to prove Lemma 5.
Proof of Lemma 5. Tobound ∞r=0(1 8)r+1n r+s, wefirstsplitthesuminto 1 8n
s
and ∞r=1(1 8)r+1n r+s.Wecontinue
to bound the latter and will later bound the former.
P P
r+1 r+1
∞ 1
n =
∞ 1 4c 1νlog( |X|/δ r+s)
log2
4c 2νlog( |X|/δ r+s)
8 r+s 8 ε2 ε2
r=1(cid:18) (cid:19) r=1(cid:18) (cid:19) r+s (cid:18) r+s (cid:19)
X X
r+1
( =a) ∞ 1 64c ν 4r+sf(r+s)log2 4r+s 64c νf(r+s)
1 2
8 · ·
r=1(cid:18) (cid:19)
X (cid:0) (cid:1)
(b) ∞ 64c 1ν2 −r4s
f(r+s)2 log2 4r+s +log2(64c νf(r+s))
2
≤ 8
r=1
X (cid:0) (cid:0) (cid:1) (cid:1)
=
4s ·64c 1ν ∞
2 −rf(r+s) (r+s)2log2(4)+log2(64c 2νf(r+s)) , (19)
4
Xr=1 (cid:16) term1 term2 (cid:17)
where (a) holds because 4r = 22r = 1 , and (b) follo|ws fro{mz (18).}Nex|t, for ea{sez of nota}tion, we define
8r+1 23r 8 82r
c := 96c ν. Then, term 1 of (19) is·bounde·d using Proposition 2 and (18) as
3 1
4s
∞
64c ν 2 r f(r+s) (r+s)2log2(4)
1 −
4 · ·
r=1
X (cid:0) (cid:1)
2γs ∞ (r+s)2 ∞ log(r)(r+s)2
c 4s log +
≤ 3 δ 2r 2r
!
(cid:18) (cid:19)r=1 r=1
X X
(a) 2γs ∞ r2 s2 ∞ log(r)r2 log(r)s2
2c 4s log + +
≤ 3 δ 2r 2r 2r 2r
!
(cid:18) (cid:19)r=1(cid:18) (cid:19)r=1(cid:18) (cid:19)
X X
( =b) 2c 4s log 2γs 6+s2 + 8+s2
3
δ
(cid:18) (cid:18) (cid:19) (cid:19)
s (cid:0) (cid:1) (cid:0) (cid:1)
= s24slog , (20)
O δ
(cid:16) (cid:16) (cid:17)(cid:17)
where (a) follows from (18) and (b) follows using that ∞x=r 21
r
= 1 and by applying the results of Claim 1.
To bound term 2 in (19), we first establish two bounds. To this end, we will need the following intermediate
P
result.22
Proposition 3. Let c >0 be a constant and r,s N+, then we have for some γ > √3 ζ the following inequality:
∈
ζ(r+s)3
log2(cf(r+s)) =log2 clog
δ
(cid:18) (cid:18) (cid:19)(cid:19)
2γs
<2log2 3clog +2r2
δ
(cid:18) (cid:18) (cid:19)(cid:19)
Proof. We use Proposition 2 and the fact that log(a+b) = log(a (1+b/a)) and log(a) a 1. Hence, we obtain
· ≤ −
ζ(r+s)3 2γrs
log2 clog log2 3clog
δ ≤ δ
(cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:18) (cid:19)(cid:19)
2γs
= log2 3c log +log(r)
δ
(cid:18) (cid:18) (cid:18) (cid:19) (cid:19)(cid:19)
2γs rδ
= log2 3clog 1+log
δ · 2γs
(cid:18) (cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:19)(cid:19)
2γs
log2 3clog (1+log(r))
≤ δ ·
(cid:18) (cid:18) (cid:19) (cid:19)
2γs
log2 3clog r
≤ δ ·
(cid:18) (cid:18) (cid:19) (cid:19)
2γs
2log2 3clog +2log2(r)
≤ δ
(cid:18) (cid:18) (cid:19)(cid:19)
2γs
< 2log2 3clog +2(r 1)2, (21)
δ −
(cid:18) (cid:18) (cid:19)(cid:19)
and observing that r 1< r and r 1 concludes the proof.
− ≥
We apply Proposition 3 with c= 64c ν together with results form Claim 1 to derive the relation
2
2γs ∞ 1 ζ(r+s)3
log log2 clog
δ 2r δ
(cid:18) (cid:19)r=1 (cid:18) (cid:18) (cid:19)(cid:19)
X
2γs ∞ 1 2γs
log 2log2 3clog +2r2
≤ δ 2r δ
(cid:18) (cid:19)r=1 (cid:18) (cid:18) (cid:18) (cid:19)(cid:19) (cid:19)
X
2γs 2γs ∞ 1 ∞ r2
= log 2log2 3clog +2
δ δ 2r 2r
!
(cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:19)r=1 r=1
X X
2γs 2γs
log 2log2 3clog +12 . (22)
≤ δ δ
(cid:18) (cid:19)(cid:18) (cid:18) (cid:18) (cid:19)(cid:19) (cid:19)
Again using the result from Proposition 3 with c = 64c ν together with results from Claim 1, we have that
2
∞ log(r) ζ(r+s)3
log2 clog
2r δ
r=1 (cid:18) (cid:18) (cid:19)(cid:19)
X
∞ log(r) 2γs
2log2 3clog +2r2
≤ 2r δ
r=1 (cid:18) (cid:18) (cid:18) (cid:19)(cid:19) (cid:19)
X
2γs ∞ log(r) ∞ r2log(r)
= 2log2 3clog +2
δ 2r 2r
(cid:18) (cid:18) (cid:19)(cid:19)r=1 r=1
X X
2γs
2log2 3clog +16. (23)
≤ δ
(cid:18) (cid:18) (cid:19)(cid:19)23
With c = 64c ν and using the results from (22), (23) and Proposition 2, we obtain for the second term in (19)
2
that
4s ·64c 1ν ∞
2 −r f(r+s)log2 64c 2νlog
ζ(r+s)3
4 · δ
r=1 (cid:18) (cid:18) (cid:19)(cid:19)
X
=
4s192c 1ν
log
2γs ∞ 1
log2 64c νlog
ζ(r+s)3
4 δ 2r 2 δ
(cid:18) (cid:19)r=1 (cid:18) (cid:18) (cid:19)(cid:19)
X
∞ log(r) ζ(r+s)3
+ log2 64c νlog
2r · 2 δ
!
r=1 (cid:18) (cid:18) (cid:19)(cid:19)
X
2γs 2γs
4s 48ν log 2log2 3clog +12
≤ · · δ δ
(cid:18) (cid:18) (cid:19)(cid:18) (cid:18) (cid:18) (cid:19)(cid:19) (cid:19)
2γs
+2log2 3clog +16
δ
(cid:18) (cid:18) (cid:19)(cid:19) (cid:19)
s s
= 4slog log2log . (24)
O δ δ
To conclude the proof, we boun(cid:16) d 1n (cid:16) n(cid:17) as (cid:16) (cid:17)(cid:17)
8 s ≤ s
ζs3 ζs3
n = 64c ν 4slog log2 64c ν 4rlog
s 1 2
· δ · δ
(cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:19)
64c ν 4sf(s) 2 s2log2(4)+log2(64c νf(s))
1 2
≤ · ·
γs
< 384ν 4slog (cid:0) s2log2(4) (cid:1)
· δ
(cid:16) (cid:17) γs
+log2 192c νlog
2
δ
!
s (cid:16)s (cid:16)s (cid:17)(cid:17)
= s24slog +4slog log2log . (25)
O δ δ δ
Putting (20), (24) and (25) togethe(cid:16)r, we obta(cid:16)in(cid:17)the result(cid:16)sta(cid:17)ted in Lem(cid:16)m(cid:17)a(cid:17)5.
G. Proof of Lemma 6
Proof. We choose n r to fulfill (3) with equality. Given that, we bound ∞r=1 1 8 r −1 n r from above as
r 1 r 1
∞ 1 − n = ∞ 1 − 4c 1g(δ r) log2 4cP2g(δ r(cid:0)) (cid:1)
8 r 8 ε2 ε2
r=1(cid:18) (cid:19) r=1(cid:18) (cid:19) r (cid:18) r (cid:19)
X X
( =a) ∞ 1 r − 61 4c ν 4rlog ζr3 log2 64c ν 4rlog ζr3
1 2
8 · δ · δ
r=1(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:19)
X
(b) ∞ 8
64c νf(r) 2 r2log2(4)+log2(64c νf(r))
≤ 2r · 1 · 2
r=1
X (cid:0) (cid:1)
(c) ∞ 8 γr
< 384c νlog r2log2(4)
2r · 1 δ
Xr=1 (cid:16) (cid:17)
γr
+log2 192c νlog ,
2
δ
!
(cid:16) (cid:16) (cid:17)(cid:17)
(d) ∞ 8 γr
< 384c νlog r2log2(4)
2r · 1 δ
Xr=1 (cid:16) (cid:17)
γ
+2log2 192c νlog +2r2
2
δ
!
(cid:16) (cid:16) (cid:17)(cid:17)
( =e) log 1 log2log 1 ,
O δ δ
(cid:18) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)24
where (a) is obtained by plugging the values of g(δ ) and ε , (b) is obtained by using that 4r = 822r = 8 and
r r 8r−1 2·3r 2r
(18), and (c) follows from f(r) = log ζr3 log γr 3 = 3log γr , (d) is obtained by the same arguments as
δ ≤ δ δ
the proof of Proposition 3, and (e) foll(cid:16)ows(cid:17)from Claim 1. This concludes the proof.
(cid:0) (cid:1) (cid:0) (cid:1)
H. Proof of Theorem 3
Proof. The proof of correctness follows similar lines to the proof in [14], and holds since our sampling strategy
guarantees the required target probabilities and worst-case estimation errors. This suffices to assure the correctness
ofthealgorithm. ToprovethesamplecomplexitystatedinTheorem3,werequirethesimplificationofthefollowing
function b(δ ,ε ).
r r
4c g(δ /3) 4c g(δ/3/2r)
b(δ ,ε ) = log2 2 r = log2 2
r r ε2 ((3/4)r 1ε/4)2
(cid:18) r (cid:19) (cid:18) − (cid:19)
= log (16/9)r 1 +log 16 4c g(δ/3/2r)/ε2 2
− 2
·
2 (r 1)2log2(16/9)+log2 16 4c g(δ/3/2r)/ε2
(cid:0) (cid:0) (cid:1) (cid:0) 2 (cid:1)(cid:1)
≤ − ·
256c
C(cid:0)(r 1)2 +2log2 2 |X(cid:0)||Y| log(2r3 /δ) (cid:1)(cid:1)
≤ 5 − ε2 |X|
(cid:18) (cid:19)
C 1
C (r 1)2 +2log2 3 log(2r)+log(3 )+log
≤ 5 − ε2 |X| δ
(cid:18) (cid:18) (cid:18) (cid:19)(cid:19)(cid:19)
C 1
C (r 1)2 +2log2 3 log rC +C
≤ 5 − ε2 δ 1′ 1
(cid:18) (cid:18) (cid:19) (cid:19)
C 1 (cid:0) (cid:1)
C (r 1)2 +4log2 3 log +4log2 rC +C
≤ 5 − ε2 δ 1′ 1
(cid:18) (cid:18) (cid:19)(cid:19)
C 1 (cid:0) (cid:1)
C (r 1)2 +4log2 3 log +8log2 rC +C
≤ 2 − ε2 δ 1′ 4
(cid:18) (cid:18) (cid:19)(cid:19)
C 1 (cid:0) (cid:1)
C (r 1)2 +4log2 3 log +16(r 1)2 +C
≤ 2 − ε2 δ − 6
(cid:18) (cid:18) (cid:19)(cid:19)
C 1
C (r 1)2 +4log2 3 log +C ,
≤ 5 − ε2 δ 6
(cid:18) (cid:18) (cid:19)(cid:19)
where C = 2log2(16/9), C = 256c , C = 8log2(C ), C = 16 +C , C = 16log2(C )+C . Now
2 3 2
|X||Y|
4 1 5 2 6 1′ 4
assuming that ε 4 such that r :ε 1, we have
r
≤ ∀ ≤
log (k)
2 k 4c g(δ /3)
1 r
b(δ ,ε )
2r 1 ε2 · r r
r=1 − r
X
log (k)
2 k 4c g(δ/3/2r)
1
= b(δ ,ε )
2r 1((3/4)r 1ε/4)2 · r r
r=1 − −
X
=
64c 1k
log 2(k)
8
r −1
g(δ/3/2r) b(δ ,ε )
ε2 9 · r r
r=1 (cid:18) (cid:19)
X
=
64c 1k
log 2(k)
8
r −1
4 log(2r3 /δ) b(δ ,ε )
ε2 9 |X||Y| |X| · r r
r=1 (cid:18) (cid:19)
X
C k
log 2(k)
8
r −1
1
= · rlog(2)+log(3 )+log b(δ ,ε )
ε2 9 |X| δ r r
r=1 (cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:19)
X25
C k
log 2(k)
8
r −1
1
≤
ε·
2 9
rC 1′ +C 1+log
δ ·
r=1 (cid:18) (cid:19) (cid:18) (cid:18) (cid:19)(cid:19)
X
C 1
C (r 1)2 +4log2 3 log +C
5 − ε2 δ 6
(cid:18) (cid:18) (cid:18) (cid:19)(cid:19) (cid:19)
r 1
= k log 1 ∞ 8 − r3+rlog2 1 log 1
O ε2 δ 9 ε2 δ
!
(cid:18) (cid:19)r=1(cid:18) (cid:19) (cid:18) (cid:18) (cid:18) (cid:19)(cid:19)(cid:19)
X
k 1 1 1
= log log2 log ,
O ε2 δ ε2 δ
(cid:18) (cid:18) (cid:19)(cid:18) (cid:18) (cid:18) (cid:19)(cid:19)(cid:19)(cid:19)
where C = 256c , C = log(3 ) and C = log(2). This concludes the proof.
1
|X||Y|
1
|X|
1′
REFERENCES
[1] M. Egger, R. Bitar, A. Wachter-Zeh, D. Gündüz, and N. Weinberger, “Maximal-capacity discrete memoryless channel identification,”
in IEEE International Symposium on Information Theory (ISIT), 2023.
[2] J. G. Proakis and M. Salehi, Digital communications. McGraw-hill New York, 2001, vol. 4.
[3] J. R. Barry, E. A. Lee, and D. G. Messerschmitt, Digital communication. Springer Science & Business Media, 2012.
[4] Y.Liu,Z.Tan,H.Hu,L.J.Cimini,andG.Y.Li,“ChannelestimationforOFDM,”IEEECommunicationsSurveys&Tutorials,vol.16,
no. 4, pp. 1891–1908, 2014.
[5] T. M. Cover and J. A. Thomas, Elements of information theory. John Wiley & Sons, 2012.
[6] A. Goldsmith, Wireless communications. Cambridge university press, 2005.
[7] D. Tse and P. Viswanath, Fundamentals of wireless communication. Cambridge university press, 2005.
[8] M.BigueshandA.Gershman,“Training-basedMIMOchannel estimation:astudyofestimatortradeoffsandoptimaltrainingsignals,”
IEEE Transactions on Signal Processing, vol. 54, no. 3, pp. 884–893, 2006.
[9] T. Lattimore and C. Szepesvári, Bandit algorithms. Cambridge University Press, 2020.
[10] K. Jamieson and R. Nowak, “Best-armidentification algorithms for multi-armed bandits inthe fixedconfidence setting,”in2014 48th
Annual Conference on Information Sciences and Systems (CISS), 2014, pp. 1–6.
[11] J.-Y.AudibertandS.Bubeck, “Bestarmidentificationinmulti-armedbandits,”inConferenceonLearningTheory,Jun.2010, p.13p.
[12] E. Kaufmann, O. Cappé, and A. Garivier, “On the complexity of best arm identification in multi-armed bandit models,” Journal of
Machine Learning Research, vol. 17, pp. 1–42, 2016.
[13] A. Garivier and E. Kaufmann, “Optimal best arm identification with fixed confidence,” in Annual Conference on Learning Theory,
vol. 49, Jun 2016, pp. 998–1027.
[14] E. Even-Dar, S. Mannor, and Y. Mansour, “Action elimination and stopping conditions for the multi-armed bandit and reinforcement
learning problems,” Journal of Machine Learning Research, vol. 7, no. 39, pp. 1079–1105, 2006.
[15] P.Coucheney,K.Khawam,andJ.Cohen,“Multi-armedbanditfordistributedinter-cellinterferencecoordination,”inIEEEInternational
Conference on Communications (ICC). IEEE, 2015, pp. 3323–3328.
[16] J. Lunden, V. Koivunen, and H. V. Poor, “Spectrum exploration and exploitation for cognitive radio: Recent advances,” IEEE signal
processing magazine, vol. 32, no. 3, pp. 123–140, 2015.
[17] P. Blasco and D. Gündüz, “Multi-access communications with energy harvesting: A multi-armed bandit model and the optimality of
the myopic policy,” IEEE Journal on Selected Areas in Communications, vol. 33, no. 3, pp. 585–597, 2015.
[18] J. Oksanen, “Machine learning methods for spectrum exploration and exploitation,” Ph.D. dissertation, Aalto University, 2016.
[19] S.Maghsudi andE.Hossain,“Multi-armedbanditswithapplicationto5Gsmallcells,”IEEEWirelessCommunications,vol.23,no.3,
pp. 64–73, 2016.
[20] B. Nikfar and A. H. Vinck, “Relay selection in cooperative power line communication: A multi-armed bandit approach,” Journal of
Communications and Networks, vol. 19, no. 1, pp. 1–9, 2017.
[21] B. Yang, X. Wang, and Z.Qian, “A multi-armed bandit model-based vertical handoff algorithmfor heterogeneous wirelessnetworks,”
IEEE Communications Letters, vol. 22, no. 10, pp. 2116–2119, 2018.
[22] S.Boldrini,L.DeNardis,G.Caso,M.T.Le,J.Fiorina,andM.-G.DiBenedetto, “muMAB:Amulti-armedbanditmodel forwireless
network selection,” Algorithms, vol. 11, no. 2, p. 13, 2018.
[23] L.Besson, E.Kaufmann, and C.Moy, “Aggregation of multi-armedbandits learning algorithmsfor opportunistic spectrumaccess,” in
2018 IEEE Wireless Communications and Networking Conference (WCNC). IEEE, 2018, pp. 1–6.
[24] R. Bonnefoi, L. Besson, C. Moy, E. Kaufmann, and J. Palicot, “Multi-armed bandit learning in IoT networks: Learning helps even in
non-stationary settings,” in Cognitive Radio Oriented Wireless Networks. Springer, 2018, pp. 173–185.
[25] A. Antos and I. Kontoyiannis, “Convergence properties of functional estimates for discrete distributions,” Random Structures &
Algorithms, vol. 19, no. 3-4, pp. 163–193, 2001.
[26] L. Paninski, “Estimation of entropy and mutual information,” Neural Computation, vol. 15, no. 6, pp. 1191–1253, Jun. 2003.
[27] J. Jiao, K. Venkat, Y. Han, and T. Weissman, “Minimax estimation of functionals of discrete distributions,” IEEE Transactions on
Information Theory, vol. 61, no. 5, pp. 2835–2885, 2015.
[28] ——, “Maximum likelihood estimation of functionals of discrete distributions,” IEEE Transactions on Information Theory, vol. 63,
no. 10, pp. 6774–6798, 2017.26
[29] Y. Wu and P. Yang, “Minimax rates of entropy estimation on large alphabets via best polynomial approximation,” IEEE Transactions
on Information Theory, vol. 62, no. 6, pp. 3702–3720, 2016.
[30] M. T. Harrison and I. Kontoyiannis, “Estimation of the rate–distortion function,” IEEE Transactions on Information Theory, vol. 54,
no. 8, pp. 3757–3762, 2008.
[31] Q.Wang,S.R.Kulkarni,S.Verdúetal.,“Universalestimationofinformationmeasuresforanalogsources,”FoundationsandTrends®
in Communications and Information Theory, vol. 5, no. 3, pp. 265–353, 2009.
[32] O. Shamir, S. Sabato, and N. Tishby, “Learning and generalization with the information bottleneck,” Theoretical Computer Science,
vol. 411, no. 29, pp. 2696–2711, 2010.
[33] A.G.Stefani,J.B.Huber,C.Jardin,andH.Sticht,“Confidenceintervalsforthemutualinformation,”InternationalJournalofMachine
Intelligence and Sensory Signal Processing, vol. 1, no. 3, pp. 201–214, 2014.
[34] N. Weinberger and M. Yemini, “Upper confidence interval strategies for multi-armed bandits with entropy rewards,” in IEEE
International Symposium on Information Theory (ISIT), 2022, pp. 1647–1652.
[35] ——,“Multi-armedbanditswithself-informationrewards,”IEEETransactionsonInformationTheory,vol.69,no.11,pp.7160–7184,
2023.
[36] S. Tridenski and R. Zamir, “Channel input adaptation via natural type selection,” IEEE Transactions on Information Theory, vol. 66,
no. 4, pp. 2078–2090, 2019.
[37] I.Csiszár,“Aclassofmeasuresofinformativityofobservationchannels,”PeriodicaMathematicaHungarica,vol.2,no.1,pp.191–213,
Mar. 1972.
[38] J. H. B. Kemperman, “On the Shannon capacity of an arbitrary channel,” Indagationes Mathematicae (Proceedings), vol. 77, no. 2,
pp. 101–115, 1974.
[39] S.-W. Ho and R. W. Yeung, “The interplay between entropy and variational distance,” IEEE Transactions on Information Theory,
vol. 56, no. 12, pp. 5906–5929, 2010.
[40] N. Shulman and M. Feder, “The uniform distribution as a universal prior,” IEEE Transactions on Information Theory, vol. 50, no. 6,
pp. 1356–1362, 2004.
[41] Z. Karnin, T. Koren, and O. Somekh, “Almost optimal exploration in multi-armed bandits,” in International Conference on Machine
Learning, vol. 28, no. 3, 17–19 Jun 2013, pp. 1238–1246.
[42] A.Hassidim,R.Kupfer,andY.Singer,“Anoptimaleliminationalgorithmforlearningabestarm,”inAdvancesinNeuralInformation
Processing Systems, vol. 33, 2020, pp. 10788–10798.
[43] A. Wald, “On Cumulative Sums of Random Variables,” The Annals of Mathematical Statistics, vol. 15, no. 3, pp. 283 – 296, 1944.
[44] R. Blahut, “Computation of channel capacity and rate-distortion functions,” IEEETransactions on Information Theory, vol. 18, no. 4,
pp. 460–473, 1972.
[45] S. Arimoto, “An algorithm for computing the capacity of arbitrary discrete memoryless channels,” IEEE Transactions on Information
Theory, vol. 18, no. 1, pp. 14–20, 1972.
[46] R. van Handel, “Probability in high dimension,” Princeton Uiversity New Jersey, Tech. Rep., 2014.
[47] I. Csiszár and J. Körner, Information Theory: Coding Theorems for Discrete Memoryless Systems. Cambridge, U.K.: Cambridge
University Press, 2011.
[48] Z. Zhang, “Estimating mutual information via Kolmogorov distance,” IEEE Transactions on Information Theory, vol. 53, no. 9, pp.
3280–3282, 2007.
[49] K.M.R.Audenaert,“AsharpcontinuityestimateforthevonNeumannentropy,”Journal ofPhysicsA:MathematicalandTheoretical,
vol. 40, no. 28, p. 8127, Jun 2007.
[50] I.Sason,“Entropyboundsfordiscreterandomvariablesviacoupling,”inIEEEInternationalSymposiumonInformationTheory,2013,
pp. 414–418.