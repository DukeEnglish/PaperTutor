PublishedasaconferencepaperatICLR2024
QUERYING EASILY FLIP-FLOPPED SAMPLES
FOR DEEP ACTIVE LEARNING
SeongJinCho1,2, GwangsuKim3, JunghyunLee4, JinwooShin4, ChangD.Yoo1
1DepartmentofElectricalEngineering,KAIST,2KoreaInstituteofOrientalMedicine,
3DepartmentofStatistics,JeonbukNationalUniversity,4KimJaechulGraduateSchoolofAI,KAIST
{ipcng00,jh lee00,jinwoos,cd yoo}@kaist.ac.kr, s88012@jbnu.ac.kr
ABSTRACT
Activelearningisamachinelearningparadigmthataimstoimprovetheperfor-
mance of a model by strategically selecting and querying unlabeled data. One
effectiveselectionstrategyistobaseitonthemodel’spredictiveuncertainty,which
can be interpreted as a measure of how informative a sample is. The sample’s
distancetothedecisionboundaryisanaturalmeasureofpredictiveuncertainty,
butitisoftenintractabletocompute,especiallyforcomplexdecisionboundaries
formedinmulticlassclassificationtasks. Toaddressthisissue,thispaperproposes
theleastdisagreemetric(LDM),definedasthesmallestprobabilityofdisagree-
mentofthepredictedlabel,andanestimatorforLDMproventobeasymptotically
consistentundermildassumptions. Theestimatoriscomputationallyefficientand
canbeeasilyimplementedfordeeplearningmodelsusingparameterperturbation.
TheLDM-basedactivelearningisperformedbyqueryingunlabeleddatawiththe
smallest LDM. Experimental results show that our LDM-based active learning
algorithmobtainsstate-of-the-artoverallperformanceonallconsidereddatasets
anddeeparchitectures.
1 INTRODUCTION
Machinelearningofteninvolvestheburdensometaskofannotatingalargeunlabeleddatasetthatis
generallyabundantandreadilyavailable. Activelearning(Cohnetal.,1996)alleviatesthisburden
by selecting the most informative unlabeled samples. Of the various active learning algorithms,
uncertainty-basedsampling(Ashetal.,2020;Zhaoetal.,2021b;Woo,2023)ispreferredforits
simplicity and relatively low computational cost. Uncertainty-based sampling selects unlabeled
samplesthataremostdifficulttopredict(Settles,2009;Yangetal.,2015;Sharma&Bilgic,2017).
Themainfocusofuncertainty-basedsamplingisquantifyingtheuncertaintyofeachunlabeledsample
givenapredictor. Untilnow,variousuncertaintymeasures,aswellascorrespondingactivelearning
algorithms,havebeenproposed(Nguyenetal.,2022;Houlsbyetal.,2011;Jungetal.,2023);see
AppendixAforamorecomprehensiveoverviewoftherelatedwork. However,manyofthemarenot
scalableorinterpretableandoftenrelyonheuristicapproximationslackingtheoreticaljustifications.
Thispaperfocusesonaconceptuallysimpleapproachbasedonthedistancebetweensamplesandthe
decisionboundary.
Indeed,theunlabeledsamplesclosesttothedecisionboundaryareconsideredthemostuncertain
samples(Kremeretal.,2014;Ducoffe&Precioso,2018;Raj&Bach,2022). Ithasbeentheoretically
establishedthatselectingunlabeledsampleswiththesmallestmarginleadstoexponentialperfor-
manceimprovementoverrandomsamplinginbinaryclassificationwithlinearseparators(Balcan
etal.,2007;Kpotufeetal.,2022). However,inmostcases,thesample’sdistancetothedecision
boundaryisnotcomputationallytractable,especiallyfordeepneuralnetwork-basedmulticlasspre-
dictors. Various(approximate)measureshavebeenproposedforidentifyingtheclosestsamplesto
thedecisionboundary(Ducoffe&Precioso,2018;Moosavi-Dezfoolietal.,2016;Mickischetal.,
2020),butmostlackconcretejustification.
Thispaperproposesanewparadigmofclosenessmeasurethatquantifieshoweasilysampleprediction
canbeflip-floppedbyasmallperturbationinthedecisionboundary,departingfromtheconventional
Euclidean-based distance. Therefore, samples identified to be closest by this measure are most
1
4202
naJ
81
]GL.sc[
1v78790.1042:viXraPublishedasaconferencepaperatICLR2024
uncertaininpredictionand,thus,arethemostinformative. Themaincontributionsofthispaperare
asfollows:
• Thispaperdefinestheleastdisagreemetric(LDM)asameasureofthesample’scloseness
tothedecisionboundary.
• This paper proposes an estimator of LDM that is asymptotically consistent under mild
assumptionsandasimplealgorithmtoempiricallyevaluatetheestimator,motivatedfrom
thetheoreticalanalyses. ThealgorithmperformsGaussianperturbationcenteredaroundthe
hypothesislearnedbystochasticgradientdescent(SGD).
• ThispaperproposesanLDM-basedactivelearningalgorithm(LDM-S)thatobtainsstate-of-
the-artoverallperformanceonsixbenchmarkimagedatasetsandthreeOpenMLdatasets,
testedovervariousdeeparchitectures.
2 LEAST DISAGREE METRIC (LDM)
This section defines the least disagree metric (LDM) and proposes an asymptotically consistent
estimatorofLDM.Then,motivatedfromaBayesianperspective,apracticalalgorithmforempirically
evaluatingtheLDMisprovided.
2.1 DEFINITIONOFLDM
Let X and Y be the instance and label space with |Y| ≥ 2, and H be the hypothesis space of
h:X →Y. LetDbethejointdistributionoverX ×Y,andD betheinstancedistribution. The
X
LDMisinspiredbythedisagreemetricdefinedbyHanneke(2014). Thedisagreemetricbetween
twohypothesesh andh isdefinedasfollows:
1 2
ρ(h ,h ):=P [h (X)̸=h (X)] (1)
1 2 X∼DX 1 2
whereP istheprobabilitymeasureonX inducedbyD . Foragivenhypothesisg ∈Hand
X∼DX X
x
0
∈ X, letHg,x0 := {h ∈ H | h(x 0) ̸= g(x 0)}bethesetofhypothesesdisagreeingwithg in
theirpredictionofx . Basedontheabovesetanddisagreemetric,theLDMofasampletoagiven
0
hypothesisisdefinedasfollows:
Definition1. Forgiveng ∈Handx ∈X,theleastdisagreemetric(LDM)isdefinedas
0
L(g,x ):= inf ρ(h,g). (2)
0
h∈Hg,x0
Throughoutthispaper,wewillassumethatallthehypothesesareparametricandtheparametersthat
definethehypotheseshandgrespectivelyarew,v ∈Rp.
ToillustratethedefinitionofLDMmoreclearly,
weprovideasimpleexample. Consideratwo-
dimensionalbinaryclassificationwithasetof
linearclassifiers,
H={h:h(x)=sign(xTw),w ∈R2}
wherexisuniformlydistributedonX = {x :
∥x∥ ≤ 1} ⊂ R2. Leth beahypothesislead-
θ
ing to decision boundary that forms an angle
(radian) of θ ∈ ((cid:57)π,π) with that of g, then
we have that ρ(h ,g) = |θ|. For given g
and x 0, let θ
0
∈ (θ 0,π 2) be thπ e angle between Figure1:AnexampleofLDMofx 0forgivengin
the decision boundary of g and the line pass- binaryclassificationwiththelinearclassifier. Here
(i (cid:57)n πg ,t (cid:57)h πro +ug θh 0)x ∪0, (θt 0h ,e πn )}H ;g s,x ee0 F= igu{ reh θ 1.| Tθ hu∈
s,
x disi as gu ren ei sfo wrm ithly gd fi os rtr xib 0u wte hd enon θ<X (cid:57)π⊂ +R
θ
02. orT θh 0e <h θθ
,
L(g,x 0)=inf hθ∈Hg,x0 ρ(h θ,g)= |θ π0|. thusL(g,x 0)=inf hθ∈Hg,x0 ρ(h θ,g)= |θ π0|.
Conceptually,asamplewithasmallLDMindicatesthatitspredictioncanbeeasilyflip-floppedeven
byasmallperturbationinthepredictor. Precisely,supposethatahypothesishissampledwithits
parametergivenbyw ∼N(v,Iσ2),thenitisexpectedthat
L(g,x )<L(g,x )⇔P [h(x )̸=g(x )]>P [h(x )̸=g(x )].
1 2 w 1 1 w 2 2
2PublishedasaconferencepaperatICLR2024
ThesamplewiththesmallestLDMisthemostuncertainand,thus,mostinformative. Thisintuition
is rigorously verified for the two-dimensional binary classification with linear classifiers and is
empiricallyverifiedonbenchmarkdatasetswithdeepnetworks(seeAppendixC.1).
2.2 ANASYMPTOTICALLYCONSISTENTESTIMATOROFLDM
Inmostcases,LDMisnotcomputableforthefollowingtworeasons: 1)ρisgenerallyintractable,
especiallywhenD andharebothcomplicated,e.g.,neuralnetworksoverreal-worldimagedatasets,
X
and2)oneneedstotakeaninfimumoverH,whichisusuallyaninfiniteset.
Toaddresstheseissues,weproposeanestimatorfortheLDMbasedonthefollowingtwoapprox-
imations: 1)PinthedefinitionofρisreplacedbyanempiricalprobabilitybasedonM samples
(Monte-Carlomethod),and2)HisreplacedbyafinitehypothesissetH ofcardinalityN. Our
N
estimator,denotedbyL (g,x ),isdefinedasfollows:
N,M 0
(cid:40) M (cid:41)
L (g,x ):= inf ρ (h,g)≜ 1 (cid:88) I(cid:2) h(X )̸=g(X )(cid:3) , (3)
N,M 0 h∈Hg,x0 M M i i
N i=1
whereH Ng,x0 := {h∈H
N
|h∈Hg,x0}, I[·]isanindicatorfunction, andX 1,...,X
M
i. ∼i.d. D X.
Here,M isthenumberofMonteCarlosamplesforapproximatingρ,andN isthenumberofsampled
hypothesesforapproximatingL.
Oneimportantpropertyofanestimatoris(asymptotic)consistency,i.e.,inourcase,wewantthe
LDMestimatortoconvergeinprobabilitytothetrueLDMasM andN increaseindefinitely.
WestartwithtwoassumptionsonthehypothesisspaceHandthedisagreemetricρ:
Assumption1. HisaPolishspacewithmetricd (·,·).
H
Thisassumptionallowsustoavoidanycomplications1thatmayarisefromuncountability,especially
aswewillconsideraprobabilitymeasureoverH;seeChapter1.1ofvanderVaart&Wellner(1996).
Assumption2. ρ(·,·)isB-LipschitzforsomeB >0,i.e.,ρ(h,g)≤Bd (h,g), ∀h,g ∈H.
H
If ρ is not Lipschitz, then the disagree metric may behave arbitrarily regardless of whether the
hypothesesare“close”ornot. Thiscanoccurinspecific(arguablynotsorealistic)cornercases,such
aswhenD isamixtureofDiracmeasures.
X
Fromhereonandforth,wefixsomeg ∈Dandx ∈X.
0
ThefollowingassumptionintuitivelystatesthatH ismorelikelytocoverregionsofHwhoseLDM
N
estimatorisε-closetothetrueLDM,asthenumberofsampledhypothesesN increases.
Assumption3(CoverageAssumption). Thereexisttwodeterministicfunctionsα:N×R →R
≥0 ≥0
andβ :N→R thatsatisfieslim α(N,ε)=lim β(N)=0foranyε∈(0,1),and
>0 N→∞ N→∞
 
1
P  h∗∈i Hnf
g,x0
h∈m Hi gn ,x0d H(h∗,h)≤ Bα(N,ε)≥1−β(N), ∀ε∈(0,1), (4)
ρ(h∗,g)−L(g,x0)≤ε N
wherewerecallthatHg,x0 ={h∈H|h(x 0)̸=g(x 0)}.
Here,weimplicitlyassumethatthereisarandomizedprocedurethatoutputsasequenceoffinitesets
H ⊆H ⊆···⊂H. α(N,ε)istheratedescribingtheoptimalityofapproximatingHusingH in
1 2 N
thathowcloseistheε-optimalsolutionis,and1−β(N)istheconfidencelevelthatconvergesto1.
In the following theorem, whose proof is deferred to Appendix B, we show that our proposed
estimator,L ,isasymptoticallyconsistent:
N,M
Theorem 1. Let g ∈ H, x ∈ X, and δ > 0 be arbitrary. Under Assumption 1, 2, and 3, with
0
M > 8 log(CN),wehavethatforanyε∈(0,1),
δ2
1
P[|L (g,x )−L(g,x )|≤2δ+α(N,ε)+ε]≥1− −β(N).
N,M 0 0 CN
1Theusualmeasurabilityandusefulpropertiesmaynotholdfornon-separablespaces(e.g.,Skorohodspace).
3PublishedasaconferencepaperatICLR2024
P
Furthermore,asmin(M,N)→∞with2M =ω(log(CN)),wehaveL (g,x )→L(g,x ).
N,M 0 0
For our asymptotic guarantee, we require M = ω(log(CN)), while the guarantee holds w.p. at
least 1− 1 −β(N). For instance, when β(N) scales as 1/N, the above implies that when N
CN
scalesexponentially(indimensionorsomeotherquantity),theguaranteeholdswithoverwhelming
probabilitywhileM onlyneedstobeatleastscalinglinearly,i.e.,M needsnotbetoolarge.
OneimportantconsequenceisthattheorderingoftheempiricalLDMispreservedinprobability:
Corollary1. AssumethatL(g,x ) < L(g,x ). UnderthesameassumptionsasTheorem1, the
i j
followingholds: foranyε>0,
lim P[L (g,x )>L (g,x )+ε]=0.
N,M i N,M j
min(M,N)→∞
M=ω(log(CN))
2.3 EMPIRICALEVALUATIONOFLDM
Motivation. Assumption3mustbesatisfiedforourLDMestimatortobeasymptoticallyconsistent,
asonlythencouldwehopeto“cover”thehypothesisthatyieldsthetrueLDMwithhighprobability
asN →∞. Todothat,weconsiderGaussiansamplingaroundthetargethypothesisgtoconstructa
sufficientlylarge,finitecollectionofN hypothesesH . Indeed,weshowinAppendixC.3thatthis
N
isthecasefor2Dbinaryclassificationwithlinearclassifiers.
Remark1. ItisknownthatSGDperformsBayesianinference(Mandtetal.,2017;Chaudhari&
Soatto,2018;Mingardetal.,2021),i.e.,gcanbethoughtofasasamplefromaposteriordistribution.
CombinedwiththeBernstein-vonMisestheorem(Hjortetal.,2010),whichstatesthattheposterior
ofaparametricmodelconvergestoanormaldistributionundermildconditions,Gaussiansampling
aroundgcanbethoughtofassamplingfromaposteriordistribution,inaninformalsense.
AlgorithmDetails. Algorithm1empiricallyevaluatestheLDMofxforgiveng. Whensampling
the hypotheses, we use a set of variances {σ2}K such that σ < σ . The reason for this is
k k=1 k k+1
two-fold. First,withtoosmallσ2,thesampledhypothesishisexpectedtonotsatisfyh(x)̸=g(x).
Ontheotherhand,itmaybethatwithtoolargeσ2,theminimumofρ (h,g)overthesampledh’s
M
istoofarawayfromthetrueLDM,especiallywhenthetrueLDMiscloseto0. Bothreasonsare
basedontheintuitionthatlargerσ2impliesthatthesampledhypothesishisfurtherawayfromg. In
AppendixC.2,weprovethatE [ρ (h,g)]ismonotoneincreasinginσ2for2Dbinaryclassification
w M
withlinearclassifiers,andalsoshowempiricallythatthisholdsformorerealisticscenarios. The
algorithmproceedsasfollows: foreachk,hissampledwithw ∼N(v,Iσ2),andifh(x)̸=g(x)
k
thenupdateL asmin{L ,ρ (h,g)}. WhenL doesnotchangestimesconsecutively,moveonto
x x M x
k+1. Thisiscontinuedwhilek <K.
Small s is Sufficient. The remaining question is how to determine an appropriate s. In binary
classificationwiththelinearclassifierdescribedinFigure1,theevaluatedLDMreachesthetrue
LDMevenwithasmalls. However,alargesisrequiredfordeeparchitecturestomaketheevaluated
LDMconverge. Thisiscomputationallyprohibitiveastheevaluationrequiresahugeruntimefor
samplingalargenumberofhypotheses. Fortunately,weobservedempiricallythatforallconsidered
valuesofs,therankorderofsamples’LDMsispreservedwithahighrank-correlationcoefficient
close to 1. As the ordering induced by the LDMs is enough for the LDM-based active learning
(describedinSection3),asmallsissufficient. Experimentaldetailsandfurtherdiscussionsonthe
stopconditionsaredeferredtoAppendixF.1.
3 LDM-BASED ACTIVE LEARNING
ThissectionintroducesLDM-S,theLDM-basedbatchsamplingalgorithmforpool-basedactive
learning. Inpool-basedactivelearning,wehaveasetofunlabeledsamples,U,andwesimultaneously
query q samples from randomly sampled pool data P ⊂ U of size m. In our algorithm, a given
hypothesisgisobtainedbytrainingonlabeledsamples,andvistheparameterofg.
2Fortheasymptoticanalyses,wewritef(n)=ω(g(n))iflim f(n) =∞.
n→∞ g(n)
4PublishedasaconferencepaperatICLR2024
Algorithm1EmpiricalEvaluationofLDM Algorithm2ActiveLearningwithLDM-S
Input: Input:
x:targetsample L ,U :Initiallabeledandunlabeledsamples
0 0
g:targethypothesisparameterizedbyv m,q:poolandquerysize
M:numberofsamplesforapproximation T :numberofacquisitionsteps
{σ2}K :setofvariances
k k=1
s:stopconditionforparametersampling fort=0toT −1do
ObtainvbytrainingonL
L =1 t
x RandomlysampleP ⊂U with|P|=m
fork=1toKdo t
EvaluateL ofx∈PforvbyAlgorithm1
c=0 x
Computeγ(x)usingEqn.5
whilec<sdo
Samplehwithw∼N(v,Iσ2) Q 1 ←{x 1}wherex 1 =argmin x∈PL x
k forn=2toqdo
c=c+1
ifh
L
c(
x
=x)
←
0̸= ρg M(x (h) ,a gn )dL x >ρ M(h,g)then Sp( ax m) pl= exγ n(x ∈)∗ Pm wi .n p.x′ P∈ (Q xn n− )1 =d co (cid:80)s( xz
jp
∈x
(x
P, nz p)x
(2
x′)
j)2
endif Q ←Q ∪{x }
n n−1 n
endwhile endfor
endfor L ←L ∪{(x ,y )} , U ←U \Q
t+1 t i i xi∈Qq t+1 t q
return:L endfor
x
3.1 LDM-SEEDING
Onena¨ıveapproachtoactivelearningwithLDMistoselectq sampleswiththesmallestLDMs,
i.e.,mostuncertainsamples. However,asshowninAppendixC.4,thisalgorithmmaynotleadto
goodperformance. Uponfurtherinspection,weobservecaseswherethereisasignificantoverlap
ofinformationintheselectedbatches,andsampleswithlargerLDMsaremorehelpful. Thisissue,
oftenreferredtoassamplingbias,isprevalentinuncertainty-basedactivelearning(Dasgupta,2011).
Onepopularapproachtomitigatethisisconsideringdiversity,whichhasbeendoneviak-means++
seeding(Ashetal.,2020),submodularmaximization(Weietal.,2015),clustering(Citovskyetal.,
2021;Yangetal.,2021),jointmutualinformation(Kirschetal.,2019),andmore.
Thispaperincorporatesdiversityviaamodificationofthek-means++seedingalgorithm(Arthur&
Vassilvitskii,2007),whichiseffectiveatincreasingbatchdiversitywithoutintroducingadditional
hyperparameters(Ashetal.,2020). Intuitively,thek-means++seedingselectscentroidsbyiteratively
samplingpointsproportionaltotheirsquareddistancefromthenearestcentroidthathasalreadybeen
chosen,whichtendstoselectadiversebatch. Ourproposedmodificationusesthecosinedistance
betweenthelastlayerfeaturesofthedeepnetwork,motivatedbythefactthattheperturbationis
appliedtotheweightsofthelastlayer,andthescalesoffeaturesdonotmatterinthefinalprediction.
We introduce the seeding methods based on the principle of querying samples with the smallest
LDMswhilepursuingdiversity. ForqueryingsampleswiththesmallestLDMs,thispaperconsiders
amodifiedexponentiallydecayingweightingwithrespecttoLDMinanEXP3-typemanner(Auer
etal.,2002);suchweightingschemehasbeensuccessfullyappliedtoactivelearning(Beygelzimer
etal.,2009;Ganti&Gray,2012;Kim&Yoo,2022). Tobalanceselectingsampleswiththesmallest
LDMsversusincreasingdiversity,P ispartitionedasP andP ,thenthetotalweightsofP andP
q c q c
aresettobeequal,whereP isthesetofsampleswithsmallestLDMofsizeq (=querysize)and
q
P =P \P . Precisely,theweightsofx∈P aredefinedasfollows: foreachx∈P ,
c q i
(cid:16) (cid:17)
exp −(Lx−Lq)+
γ(x)=
Lq
, (5)
(cid:16) (cid:17)
(cid:80) exp −(Lx−Lq)+
x∈Pi Lq
wherei∈{q,c}istheindicesofthepartitionofP,(·) =max{0,·},andL =max L .
+ q x∈Pq x
LDM-SeedingstartsbyselectinganunlabeledsamplewiththesmallestLDMinP. Thenextdistinct
unlabeledsampleissampledfromP bythefollowingprobability:
p(x)2
P(x)= , p(x)=γ(x)∗ min d (z ,z ) (6)
(cid:80) xj∈Pp(x j)2 x′∈Q cos x x′
5PublishedasaconferencepaperatICLR2024
(a) (b) (c)
Figure2: Thecomparisonofselectingsample(s). Theblackcrossesandcirclesarelabeled,andthe
graydotsareunlabeledsamples. (a)SamplesselectedbyLDM-based,entropy-based,andrandom
samplinginbinaryclassificationwiththelinearclassifier. (b)Thetestaccuracywithrespecttothe
numberoflabeledsamples. (c)Thet-SNEplotofselectedbatchsamplesin3-classclassification
withadeepnetworkonMNISTdataset.
whereQisthesetofselectedsamples,andz ,z arethefeaturesofx,x′. Notethattheselection
x x′
probabilityisexplicitlyimpactedbyLDMswhilehavingdiversity. TheeffectivenessofLDMis
showninFigure12ofSection4.1.
3.2 LDM-S:ACTIVELEARNINGWITHLDM-SEEDING
WenowintroduceLDM-SinAlgorithm2,theLDM-basedseedingalgorithmforactivelearning.
LetL andU bethesetoflabeledandunlabeledsamplesatstept,respectively. Foreachstept,
t t
thegivenparametervisobtainedbytrainingonL ,andthepooldataP ⊂ U ofsizemisdrawn
t t
uniformlyatrandom. Thenforeachx∈P,L forgivenvandγ(x)areevaluatedbyAlgorithm1
x
andEqn.5,respectively. Thesetofselectedunlabeledsamples,Q ,isinitializedas{x }where
1 1
x = argmin L . Forn = 2,...,q,thealgorithmsamplesx ∈ P withprobabilityP(x)in
1 x∈P x n
Eqn.6andappendsittoQ . Lastly,thealgorithmqueriesthelabely ofeachx ∈ Q ,andthe
n i i q
algorithmcontinuesuntilt=T −1.
4 EXPERIMENTS
ThissectionpresentstheempiricalresultsoftheeffectivenessofLDMintheproposedalgorithm,
aswellasacomprehensiveperformancecomparisonwithvariousuncertainty-basedactivelearning
algorithms. WeevaluateourapproachonthreeOpenML(OML)datasets(#6(Frey&Slate,1991),
#156 (Vanschoren et al., 2014), and #44135 (Fanty & Cole, 1990)) and six benchmark image
datasets(MNIST(Lecunetal.,1998),CIFAR10(Krizhevsky,2009),SVHN(Netzeretal.,2011),
CIFAR100(Krizhevsky,2009),TinyImageNet(Le&Yang,2015),FOOD101(Bossardetal.,2014)),
andImageNet(Russakovskyetal.,2015). MLP,S-CNN,K-CNN(Cholletetal.,2015),Wide-ResNet
(WRN-16-8;(Zagoruyko&Komodakis,2016)),andResNet-18(Heetal.,2016)areusedtoevaluate
theperformance. Allresultsareaveragedover5repetitions(3forImageNet). Precisedetailsfor
datasets,architectures,andexperimentalsettingsarepresentedinAppendixD.
4.1 EFFECTIVENESSOFLDMINSELECTING(BATCHED)UNCERTAINSAMPLES
ToinvestigatehowLDMworks,weconductasimpleactivelearningexperimentinasettingwhere
trueLDMismeasurable.Weconsidertwo-dimensionalbinaryclassificationwithH={h:h (x)=
w
I[ℓ (x) > 0.5]} where ℓ (x) := 1/(1+e−(xTw)) and w ∈ R2 is the parameter of h and x is
w w
uniformlydistributedonX = {x : ∥x∥ ≤ 1} ⊂ R2. Threeinitiallabeledsamplesarerandomly
selectedfromeachclass,andonesampleisqueriedateachstep. LDM-basedactivelearning(LDM)
iscomparedwithentropy-baseduncertaintysampling(Entropy)andrandomsampling(Random).
Figure2ashowsexamplesofthesamplesselectedbyeachalgorithm. Theblackcrossesandcircles
arelabeledsamples,andthegraydotsareunlabeled. BothLDMandEntropyselectsamplesclose
tothedecisionboundary. Figure2bshowstheaveragetestaccuracyof100replicateswithrespect
tothenumberoflabeledsamples. LDMperformslikeEntropy,whichismuchbetterthanRandom.
6PublishedasaconferencepaperatICLR2024
(a) (b) (c)
(d) (e) (f)
Figure3: Theimprovedtestaccuracybylabelingthekth batchofsizeq frompooldatasortedin
ascendingorderofLDMwhenthenumberoflabeledsamplesis100(a)or300(d),andt-SNEplots
ofthefirstandeighthbatchesforeachcase(b-c,e-f)onMNIST.
In binary classification, entropy is a strongly decreasing function of the distance to the decision
boundary. Thatis,theentropy-basedalgorithmselectsthesampleclosesttothedecisionboundary
inthesamewayasthemargin-basedalgorithm,andtheeffectivenessofthismethodhasbeenwell-
proventheoreticallyandempiricallyinbinaryclassification. Wealsocomparedthebatchsamples
selectedbyLDM,Entropy,Coreset,andRandomin3-classclassificationwithadeepnetworkonthe
MNISTdataset. Figure2cshowsthet-SNEplotoftheselectedbatchsamples. Thegrayandblack
pointsarepooldataandlabeledsamples,respectively. TheresultsshowthatLDMandEntropyselect
samplesclosetothedecisionboundary, whileCoresetselectsmorediversesamples. Forfurther
ablations,we’veconsideredvaryingbatchsizes(AppendixF.3),seedingalgorithmswithoutuseof
LDM(AppendixF.2),andvanillaseedingalgorithmwithotheruncertaintymeasures(AppendixG.1),
allofwhichshowtheeffectivenessofournewlyintroducedLDM.
4.2 NECESSITYOFPURSUINGDIVERSITYINLDM-S
Here,weprovideablationsontheeffectofdiversesampling(andwhyweneedtoconsiderdiversity)
byconsideringthescenarioinwhichwequerysampleswiththesmallestLDMsonlyWequerythe
kthbatchofsizeq =20fork ∈[50]fromMNISTsortedinascendingorderofLDMandcompare
theimprovementsintestaccuracy. AsshowninFigure3awhere100samplesarelabeled,samples
withthesmallestLDMsleadtothebestperformance,whereasinFigure3dwhere300samplesare
labeled, that is not the case. To see why this is the case, we’ve plotted the t-SNE plots (van der
Maaten&Hinton,2008)ofthefirstandeighthbatchesforeachcase. Inthefirstcase,asshownin
Figure3b–3c,thesamplesofthefirstandeighthbatchesareallspreadout,sothereisnooverlapof
informationbetweenthesamplesineachbatch;takingacloserlook,itseemsthatsmallerLDMleads
tothesamplesbeingmorespreadout. However,inthesecondcase,asshowninFigure3e–3f,the
samplesofthefirstbatchareclosetooneanother,i.e.,thereisasignificantoverlapofinformation
between the samples in that batch. Surprisingly, this is not the case for the eighth batch, which
consistsofsamplesoflargerLDMs. Thisleadsustoconcludethatdiversityshouldbeconsideredin
thebatchselectionsetting,evenwhenusingtheLDM-basedapproach. InAppendixC.4,weprovide
ageometricalintuitionofthisphenomenonin2Dbinaryclassificationwithlinearclassifiers.
4.3 COMPARINGLDM-STOBASELINEALGORITHMS
WenowcomparetheperformanceofLDM-Swiththebaselinealgorithms. Wefirstdescribethe
baselinealgorithmswecompare,thenprovidecomparisonsacrossandpertheconsidereddatasets.
7PublishedasaconferencepaperatICLR2024
Table1: Themean±standarddeviationoftherepetition-wiseaveragedperformance(testaccuracy)
differences (%), relative to Random, over the entire steps. The positive value indicates higher
performance than Random, and the asterisk (∗) indicates that p < 0.05 in paired sample t-test
betweenLDM-Sandothers. (bold+underlined: bestperformance,bold: second-bestperformance)
LDM-S Entropy Margin Coreset ProbCov DBAL ENS BADGE BAIT
(ours) (1948) (2006) (2018) (2022) (2017) (2018) (2020) (2021)
OML#6 4.76±0.36 -2.46±0.70* 4.11±0.24 1.14±0.33* 0.19±0.40* 0.10±0.22* 4.43±0.31 2.98±0.28* 2.57±0.30*
OML#156 1.18±0.32 -1.29±0.51* 0.64±0.39* -19.53±3.32* -0.08±0.51* -14.40±1.08* 0.61±0.35* 1.06±0.39 -7.66±1.12*
OML#44135 4.36±0.27 1.84±0.36* 4.11±0.32 0.27±0.68* 3.55±0.28* 1.61±0.45* 2.47±0.37* 2.48±0.44* 2.55±0.41*
MNIST 3.33±0.43 2.36±0.84* 3.01±0.45 -0.04±1.23* 2.92±0.41 1.68±0.80* 2.98±0.36 3.01±0.45 3.37±0.43
CIFAR10 1.34±0.19 0.00±0.21* 0.43±0.27* -3.71±0.56* 0.04±0.37* -0.15±0.31* 0.58±0.28* 0.90±0.21* 0.60±0.13*
SVHN 2.53±0.22 1.52±0.19* 1.98±0.18* -1.66±0.51* 0.88±0.14* 2.46±0.21 2.08±0.22* 2.18±0.23* 2.18±0.23*
CIFAR100 0.98±0.44 0.37±0.60 0.57±0.45 0.89±0.49 0.74±0.60 0.55±0.77 0.03±0.41* 0.64±0.48 -
T.ImageNet 0.55±0.16 -0.61±0.28* 0.28±0.29 -0.20±0.46* 0.45±0.23 0.27±0.19* -0.15±0.35* 0.12±0.40* -
FOOD101 1.27±0.34 -0.86±0.20* 0.34±0.25* 1.30±0.16 0.50±0.22* 1.18±0.35 -0.15±0.46* 0.71±0.43* -
ImageNet 0.96±0.23 -0.21±0.58* 0.14±0.54 -0.62±0.20* 0.30±0.22* - 0.57±0.21 0.71±0.81 -
Table2: Themeanofruntime(min)foreachalgorithmandeachdataset. WeobservethatLDM-S
operatesasfastasEntropyonalmostalldatasets.
LDM-S Entropy Margin Coreset ProbCov DBAL ENS BADGE BAIT
OML#6 7.1 6.0 5.9 6.4 5.8 5.8 28.1 6.2 628.8
OML#156 4.5 4.2 3.7 4.4 4.0 4.2 17.5 4.2 60.2
OML#44135 5.2 4.0 4.6 4.2 6.2 4.2 18.2 4.4 319.8
MNIST 17.6 10.4 11.3 11.3 10.4 12.1 49.6 12.5 27.9
CIFAR10 106.0 99.6 101.0 106.1 97.5 108.0 496.0 102.2 6,436.5
SVHN 68.9 65.1 66.6 70.9 64.8 105.8 324.8 70.5 6,391.6
CIFAR100 405.9 395.2 391.4 429.7 405.5 447.8 1,952.1 445.3 -
T.ImageNet 4,609.5 4,465.9 4,466.1 4,706.8 4,621.4 4,829.2 19,356.4 5,152.5 -
FOOD101 4,464.8 4,339.8 4,350.3 4,475.7 4,471.0 4,726.8 18,903.3 4,703.9 -
Baselinealgorithms Eachbaselinealgorithmisdenotedasfollows:‘Rand’:randomsampling,‘En-
tropy’:entropy-baseduncertaintysampling(Shannon,1948),‘Margin’:margin-basedsampling(Roth
&Small,2006),‘Coreset’: core-setselection(Sener&Savarese,2018),‘ProbCov’: maximizing
probabilitycoverage(Yehudaetal.,2022),‘DBAL’:MC-dropoutsamplingwithBALD(Galetal.,
2017),‘ENS’:ensemblemethodwithvariationratio(Beluchetal.,2018),‘BADGE’:batchactive
learningbydiversegradientembeddings(Ashetal.,2020),and‘BAIT’:batchactivelearningvia
informationmetrics(Ashetal.,2021). ForDBAL,weuse100forwardpassesforMC-dropout,and
forENS,weuseanensembleconsistingof5identicalbutrandomlyinitializednetworks. DBALcan
notbeconductedonResNet-18,andBAITcannotbeconductedwithourresourcesonCIFAR100,
TinyImageNet,FOOD101,andImageNetsinceitrequireshundredstothousandsoftimeslongerthan
CIFAR10. ForthehyperparametersofourLDM-S,wesets=10,M tobethesamesizeasthepool
size,andσ =10βk−5whereβ =0.1andk ∈{1,2,··· ,51}inconvenient(seeAppendixF.4).
k
Performancecomparisonacrossdatasets Itisn’teasytofindanalgorithmthatperformsbest
in all benchmark datasets, but there is a clear winner that performs best on average on the nine
benchmarkdatasetsconsideredinthepaper. Theperformanceprofile(Dolan&More´,2002)and
penaltymatrix(Ashetal.,2020)areexaminedtoprovidecomprehensivecomparisonsacrossdatasets.
ThedetailsoftheperformanceprofileandpenaltymatrixaredescribedinAppendixE.Figure4a
showstheperformanceprofilew.r.t. δ. Overall,itisclearthatLDM-SretainsthehighestR (δ)over
A
allconsideredδ’s. WealsoobservethatR (0)=35%whiletheotheralgorithmshaveavalue
LDM-S
lessthan15%. ThisclearlyshowsthatourLDM-Soutperformstheotherconsideredalgorithms.
Figure4bshowstheresultingpenaltymatrix. First,notethatinthefirstrow,LDM-Soutperformsall
theotheralgorithmsuptostatisticalsignificance,sometimesmorethanhalfthetime. (Recallthatthe
largestentryofthepenaltymatrixis,atmost,9,thetotalnumberofdatasets.) Also,lookingatthe
firstcolumn,itisclearthatnootheralgorithmbeatsLDM-Soverallruns. Lookingatthebottomrow,
itisclearthatLDM-Sobtainsthebestoverallperformance.
8PublishedasaconferencepaperatICLR2024
(a) (b)
Figure4: Theperformancecomparisonacrossdatasets(a)Dolan-More´ plotamongthealgorithms
acrossallexperiments. AUCistheareaunderthecurve. (b)Thepairwisepenaltymatrixoverall
experiments. Element P corresponds roughly to the number of times algorithm i outperforms
i,j
algorithmj. Column-wiseaveragesatthebottomshowoverallperformance(lowerisbetter).
Performancecomparisonperdatatset Table1presentsthemeanandstandarddeviationofthe
repetition-wiseaveragedperformancedifferences,relativetoRandom,overtheentiresteps. The
positivevalueindicateshigherperformancecomparedtoRandom,andtheasterisk(∗)indicatesthe
p-valueislessthan0.05inpairedsamplet-testforthenullofnodifferenceversusthealternativethat
theLDM-Sisbetterthaneachofthecomparedalgorithm. WeobservethatLDM-Seitherconsistently
performsbestoriscomparablewithotheralgorithmsforalldatasets. Incontrast,theperformance
of the algorithms except LDM-S varies depending on the datasets. For instance, ‘Entropy’ and
‘Coreset’showpoorperformancecomparedtootheruncertainty-basedalgorithms,includingours,on
OpenML,MNIST,CIFAR10,andSVHN,while‘Coreset’performsatparwithoursonCIFAR100and
FOOD101. ‘Margin’performscomparablywithLDM-Sonrelativelysmalldatasetsbutshowspoor
performanceonlargedatasets. ‘ENS’,althoughcomparabletootheralgorithms,stillunderperforms
comparedtoLDM-Sonalldatasets. Asimilartrendcanalsobeobservedfor‘ProbCov’,‘DBAL’,
‘BADGE’,and‘BAIT’.Thedetailsoftestaccuracieswithrespecttothenumberoflabeledsamples
foreachdatasetarepresentedinAppendixG.3.
Runtimecomparisonperdatatset Table2presentsthemeanofruntime(min)toperformactive
learningforeachalgorithmandeachdataset. Overall, comparedtoEntropy, whichisoneofthe
fastestactivelearningalgorithms,theruntimeofLDM-Sincreasedbyonly3∼6%onCIFAR10,
SVHN,CIFAR100,TinyImageNet,andFOOD101,anditisevenalittlefasterthanMC-BALD,
Coreset,andBADGE.AlthoughtheruntimeofLDM-Sincreasedby20∼70%comparedtoEntropy
onOpenMLandMNISTdatasets,thisisprimarilyattributedtotherelativelysmalltrainingtime
comparedtoacquisitiontime,whichisduetothesimplicityofdatasetsandnetworks. ENS-VarR
requiresabout5timesmorecomputationalloadthanEntropy,asallnetworksintheensembleare
individuallytrainedinthatmethod. TherunningtimeofBAITisproportionaltod2C2qwhered,C,
andqisthefeaturedimension,numberofclasses,andquerysize;thus,evenformanageabletasks
suchasOpenMLdatasets,itrequiresalotoftime. ItisexpectedtotakeseveralyearstorunBAIT
onlargedatasetssinceittakeshundredstothousandsoftimeslongerthanonCIFAR10. Thus,with
comparableruntime,LDM-Soutperformsorisonparwiththeconsideredalgorithmsanddatasets.
5 CONCLUSION
Thispaperproposestheleastdisagreemetric(LDM),whichmeasuressampleuncertaintybyperturb-
ingthepredictorandanasymptoticallyconsistentestimator. Thispaperthenproposesahypothesis
samplingmethodforapproximatingtheLDMandanLDM-basedactivelearningalgorithmtoselect
unlabeledsampleswithsmallLDMwhilepursuingbatchdiversity. Theproposedalgorithmeither
consistentlyperformsbestoriscomparablewithotherhigh-performingactivelearningalgorithms,
leadingtostate-of-the-artperformanceacrossalltheconsidereddatasets.
9PublishedasaconferencepaperatICLR2024
OneimmediatefuturedirectionisobtainingarigoroussamplecomplexityguaranteeforourLDM-S
algorithm, which we currently lack. Also, incorporating scalable and simple posterior sampling
frameworksinsteadofthecurrentGaussiansamplingschemeisanexcitingdirection.RecentlyKirsch
etal.(2023)showedthatbatchacquisitionstrategiessuchasrank-basedstrategiesoftenoutperform
thetop-Kstrategy. CombiningthiswithourLDM-basedapproachwouldbeaninterestingdirection
thatmayleadtofurtherimprovementinperformance.
REFERENCES
DavidJ.Aldous. Coveringacompactspacebyfixed-radiusorgrowingrandomballs. LatinAmerican
JournalofProbabilityandMathematicalStatistics,19:755–767,2022.
David Arthur and Sergei Vassilvitskii. K-Means++: The Advantages of Careful Seeding. In
ProceedingsoftheEighteenthAnnualACM-SIAMSymposiumonDiscreteAlgorithms,SODA’07,
pp.1027–1035,USA,2007.SocietyforIndustrialandAppliedMathematics.
JordanAsh,SurbhiGoel,AkshayKrishnamurthy,andShamKakade. GoneFishing: NeuralActive
Learning with Fisher Embeddings. In Advances in Neural Information Processing Systems,
volume34,pp.8927–8939.CurranAssociates,Inc.,2021.
JordanT.Ash,ChichengZhang,AkshayKrishnamurthy,JohnLangford,andAlekhAgarwal. Deep
BatchActiveLearningbyDiverse,UncertainGradientLowerBounds. InInternationalConference
onLearningRepresentations,2020.
PeterAuer,NicoloCesa-Bianchi,YoavFreund,andRobertESchapire.Thenonstochasticmultiarmed
banditproblem. SIAMJournalonComputing,32(1):48–77,2002.
Maria-FlorinaBalcan,AndreiBroder,andTongZhang. Marginbasedactivelearning. InNaderH.
BshoutyandClaudioGentile(eds.),LearningTheory–COLT2007,pp.35–50,Berlin,Heidelberg,
2007.SpringerBerlinHeidelberg.
WilliamH.Beluch,TimGenewein,AndreasNurnberger,andJanM.Kohler.ThePowerofEnsembles
forActiveLearninginImageClassification. In2018IEEE/CVFConferenceonComputerVision
andPatternRecognition,pp.9368–9377,2018.
AlinaBeygelzimer,SanjoyDasgupta,andJohnLangford. ImportanceWeightedActiveLearning. In
Proceedingsofthe26thAnnualInternationalConferenceonMachineLearning,ICML’09,pp.
49–56,NewYork,NY,USA,2009.AssociationforComputingMachinery.
LukasBossard,MatthieuGuillaumin,andLucVanGool. Food-101–miningdiscriminativecompo-
nentswithrandomforests. InComputerVision–ECCV2014,pp.446–461,Cham,2014.Springer
InternationalPublishing.
G.E.P.BoxandMervinE.Muller. ANoteontheGenerationofRandomNormalDeviates. The
AnnalsofMathematicalStatistics,29(2):610–611,1958.
E. Burnaev, P. Erofeev, and A. Papanov. Influence of resampling on accuracy of imbalanced
classification. InEighthInternationalConferenceonMachineVision(ICMV2015),volume9875,
pp.987521.InternationalSocietyforOpticsandPhotonics,SPIE,2015a.
E. Burnaev, P. Erofeev, and D. Smolyakov. Model selection for anomaly detection. In Eighth
InternationalConferenceonMachineVision(ICMV2015),volume9875,pp.987525.International
SocietyforOpticsandPhotonics,SPIE,2015b.
RazvanCaramalau,BinodBhattarai,andTae-KyunKim. SequentialGraphConvolutionalNetwork
forActiveLearning. In2021IEEE/CVFConferenceonComputerVisionandPatternRecognition
(CVPR),pp.9578–9587,2021.
PratikChaudhariandStefanoSoatto. Stochasticgradientdescentperformsvariationalinference,con-
vergestolimitcyclesfordeepnetworks. InInternationalConferenceonLearningRepresentations,
2018.
Franc¸oisCholletetal. Keras. https://keras.io,2015.
10PublishedasaconferencepaperatICLR2024
Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin
Rostamizadeh, and Sanjiv Kumar. Batch Active Learning at Scale. In Advances in Neural
InformationProcessingSystems,volume34,pp.11933–11944.CurranAssociates,Inc.,2021.
DavidA.Cohn,ZoubinGhahramani,andMichaelI.Jordan. ActiveLearningwithStatisticalModels.
JournalofArtificialIntelligenceResearch,4(1):129–145,Mar1996.
SanjoyDasgupta. Twofacesofactivelearning. TheoreticalComputerScience,412(19):1767–1781,
2011. AlgorithmicLearningTheory(ALT2009).
Elizabeth D. Dolan and Jorge J. More´. Benchmarking optimization software with performance
profiles. MathematicalProgramming,91(2):201–213,Jan2002.
MelanieDucoffeandFredericPrecioso. AdversarialActiveLearningforDeepNetworks: aMargin
BasedApproach. arXivpreprintarXiv:1802.09841,2018.
R.M.Dudley. RealAnalysisandProbability. CambridgeSeriesinAdvancedMathematics.Cam-
bridgeUniversityPress,2002.
JuanElenter,NavidNaderializadeh,andAlejandroRibeiro.ALagrangianDualityApproachtoActive
Learning. InAdvancesinNeuralInformationProcessingSystems,volume35,pp.37575–37589.
CurranAssociates,Inc.,2022.
Mark Fanty and Ronald Cole. Spoken Letter Recognition. In Advances in Neural Information
ProcessingSystems,volume3.Morgan-Kaufmann,1990.
PeterWFreyandDavidJSlate. LetterrecognitionusingHolland-styleadaptiveclassifiers. Machine
learning,6(2):161–182,1991.
Alexander Freytag, Erik Rodner, and Joachim Denzler. Selecting influential examples: Active
learningwithexpectedmodeloutputchanges. InComputerVision–ECCV2014,pp.562–577,
Cham,2014.SpringerInternationalPublishing.
YarinGalandZoubinGhahramani. DropoutasaBayesianApproximation: RepresentingModel
UncertaintyinDeepLearning. InProceedingsofThe33rdInternationalConferenceonMachine
Learning,volume48ofProceedingsofMachineLearningResearch,pp.1050–1059,NewYork,
NewYork,USA,20–22Jun2016.PMLR.
Yarin Gal, Riashat Islam, and Zoubin Ghahramani. Deep Bayesian Active Learning with Image
Data. InProceedingsofthe34thInternationalConferenceonMachineLearning,volume70of
ProceedingsofMachineLearningResearch,pp.1183–1192.PMLR,06–11Aug2017.
RaviGantiandAlexanderGray. UPAL:UnbiasedPoolBasedActiveLearning. InProceedings
oftheFifteenthInternationalConferenceonArtificialIntelligenceandStatistics,volume22of
ProceedingsofMachineLearningResearch,pp.422–431,LaPalma,CanaryIslands,21–23Apr
2012.PMLR.
BinGu,ZhouZhai,ChengDeng,andHengHuang. EfficientActiveLearningbyQueryingDiscrimi-
nativeandRepresentativeSamplesandFullyExploitingUnlabeledData. IEEETransactionson
NeuralNetworksandLearningSystems,32(9):4111–4122,2021.
Steve Hanneke. Theory of Disagreement-Based Active Learning. Foundations and Trends® in
MachineLearning,7(2-3):131–309,2014.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. DelvingDeepintoRectifiers: Surpassing
Human-LevelPerformanceonImageNetClassification. In2015IEEEInternationalConference
onComputerVision(ICCV),pp.1026–1034,2015.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image
Recognition. In2016IEEEConferenceonComputerVisionandPatternRecognition(CVPR),pp.
770–778,2016. doi: 10.1109/CVPR.2016.90.
Nils Lid Hjort, Chris Holmes, Peter Mu¨ller, and Stephen G Walker. Bayesian Nonparametrics.
Number28inCambridgeSeriesinStatisticalandProbabilisticMathematics.CambridgeUniversity
Press,2010.
11PublishedasaconferencepaperatICLR2024
NeilHoulsby,FerencHusza´r,ZoubinGhahramani,andMa´te´Lengyel. BayesianActiveLearningfor
ClassificationandPreferenceLearning. arXivpreprintarXiv:1112.5745,2011.
AjayJ.Joshi,FatihPorikli,andNikolaosPapanikolopoulos. Multi-classactivelearningforimage
classification. In2009IEEEConferenceonComputerVisionandPatternRecognition,pp.2372–
2379,2009.
SeohyeonJung, SanghyunKim, andJuhoLee. Asimpleyetpowerfuldeepactivelearningwith
snapshotsensembles. InTheEleventhInternationalConferenceonLearningRepresentations,
2023.
GwangsuKimandChangD.Yoo. BlendingQueryStrategyofActiveLearningforImbalancedData.
IEEEAccess,10:79526–79542,2022.
Yoon-YeongKim,KyungwooSong,JoonHoJang,andIl-chulMoon. LADA:Look-AheadData
Acquisition via Augmentation for Deep Active Learning. In Advances in Neural Information
ProcessingSystems,volume34,pp.22919–22930.CurranAssociates,Inc.,2021.
AndreasKirsch,JoostvanAmersfoort,andYarinGal. BatchBALD:EfficientandDiverseBatch
AcquisitionforDeepBayesianActiveLearning. InAdvancesinNeuralInformationProcessing
Systems,volume32.CurranAssociates,Inc.,2019.
AndreasKirsch,SebastianFarquhar,ParmidaAtighehchian,AndrewJesson,Fre´de´ricBranchaud-
Charron, and Yarin Gal. Stochastic Batch Acquisition: A Simple Baseline for Deep Active
Learning. TransactionsonMachineLearningResearch,2023. ExpertCertification.
SamoryKpotufe,GanYuan,andYunfanZhao. Nuancesinmarginconditionsdeterminegainsin
activelearning.InInternationalConferenceonArtificialIntelligenceandStatistics,pp.8112–8126.
PMLR,2022.
PLKrapivsky. Randomsequentialcovering. JournalofStatisticalMechanics: TheoryandExperi-
ment,2023(3):033202,mar2023.
Jan Kremer, Kim Steenstrup Pedersen, and Christian Igel. Active learning with support vector
machines. WIREsDataMiningandKnowledgeDiscovery,4(4):313–326,2014.
Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical Report 0,
UniversityofToronto,Toronto,Ontario,2009.
Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. Technical report, CS231N,
StanfordUniversity,2015.
Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. ProceedingsoftheIEEE,86(11):2278–2324,1998.
DavidD.LewisandWilliamA.Gale. ASequentialAlgorithmforTrainingTextClassifiers. InSIGIR
’94,pp.3–12,London,1994.SpringerLondon.
Rafid Mahmood, Sanja Fidler, and Marc T Law. Low-Budget Active Learning via Wasserstein
Distance: AnIntegerProgrammingApproach. InInternationalConferenceonLearningRepresen-
tations,2022.
StephanMandt,MatthewD.Hoffman,andDavidM.Blei. StochasticGradientDescentasApproxi-
mateBayesianInference. JournalofMachineLearningResearch,18(134):1–35,2017.
PascalMassart. Someapplicationsofconcentrationinequalitiestostatistics. AnnalesdelaFaculte´
dessciencesdeToulouse: Mathe´matiques,Ser.6,9(2):245–303,2000.
DavidMickisch,FelixAssion,FlorensGreßner,WiebkeGu¨nther,andMarieleMotta. Understand-
ing the Decision Boundary of Deep Neural Networks: An Empirical Study. arXiv preprint
arXiv:2002.01810,2020.
ChrisMingard,GuillermoValle-Pe´rez,JoarSkalse,andArdA.Louis. IsSGDaBayesiansampler?
Well,almost. JournalofMachineLearningResearch,22(79):1–64,2021.
12PublishedasaconferencepaperatICLR2024
MohamadAminMohamadi,WonhoBae,andDanicaJSutherland. MakingLook-AheadActive
LearningStrategiesFeasiblewithNeuralTangentKernels. InAdvancesinNeuralInformation
ProcessingSystems,volume35.CurranAssociates,Inc.,2022.
Seyed-MohsenMoosavi-Dezfooli,AlhusseinFawzi,andPascalFrossard. DeepFool: ASimpleand
AccurateMethodtoFoolDeepNeuralNetworks. In2016IEEEConferenceonComputerVision
andPatternRecognition(CVPR),pp.2574–2582,2016.
YuvalNetzer,TaoWang,AdamCoates,AlessandroBissacco,BoWu,andAndrewY.Ng. Reading
digitsinnaturalimageswithunsupervisedfeaturelearning. InNIPSWorkshoponDeepLearning
andUnsupervisedFeatureLearning2011,2011.
Vu-LinhNguyen,MohammadHosseinShaker,andEykeHu¨llermeier. Howtomeasureuncertainty
inuncertaintysamplingforactivelearning. MachineLearning,111(1):89–122,2022.
Mathew D. Penrose. Random Euclidean coverage from within. Probability Theory and Related
Fields,185(3):747–814,2023.
RobertPinsler, JonathanGordon, EricNalisnick, andJose´ MiguelHerna´ndez-Lobato. Bayesian
Batch Active Learning as Sparse Subset Approximation. In Advances in Neural Information
ProcessingSystems,volume32.CurranAssociates,Inc.,2019.
AnantRajandFrancisBach. ConvergenceofUncertaintySamplingforActiveLearning. InProceed-
ingsofthe39thInternationalConferenceonMachineLearning,volume162ofProceedingsof
MachineLearningResearch,pp.18310–18331.PMLR,17–23Jul2022.
A.ReznikovandE.B.Saff. TheCoveringRadiusofRandomlyDistributedPointsonaManifold.
InternationalMathematicsResearchNotices,2016(19):6065–6094,122015.
DanRothandKevinSmall. Margin-basedactivelearningforstructuredoutputspaces. InMachine
Learning: ECML 2006: 17th European Conference on Machine Learning Berlin, Germany,
September18-22,2006Proceedings17,pp.413–424.Springer,2006.
OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,ZhihengHuang,
AndrejKarpathy,AdityaKhosla,MichaelBernstein,AlexanderC.Berg,andLiFei-Fei. ImageNet
LargeScaleVisualRecognitionChallenge. InternationalJournalofComputerVision(IJCV),115
(3):211–252,2015. doi: 10.1007/s11263-015-0816-y.
OzanSenerandSilvioSavarese. ActiveLearningforConvolutionalNeuralNetworks: ACore-Set
Approach. InInternationalConferenceonLearningRepresentations,2018.
BurrSettles. Activelearningliteraturesurvey. Technicalreport,UniversityofWisconsin-Madison
DepartmentofComputerSciences,2009.
C.E.Shannon. Amathematicaltheoryofcommunication. TheBellSystemTechnicalJournal,27(3):
379–423,1948.
ManaliSharmaandMustafaBilgic. Evidence-baseduncertaintysamplingforactivelearning. Data
MiningandKnowledgeDiscovery,31(1):164–202,Jan2017.
WeishiShiandQiYu. IntegratingBayesianandDiscriminativeSparseKernelMachinesforMulti-
classActiveLearning. InAdvancesinNeuralInformationProcessingSystems,volume32,pp.
2285–2294.CurranAssociates,Inc.,2019.
SamrathSinha,SaynaEbrahimi,andTrevorDarrell. VariationalAdversarialActiveLearning. In
2019IEEE/CVFInternationalConferenceonComputerVision(ICCV),pp.5971–5980,2019.
C.Spearman. ”GeneralIntelligence,”ObjectivelyDeterminedandMeasured. TheAmericanJournal
ofPsychology,15(2):201–292,1904.
SimonTongandEdwardChang. SupportVectorMachineActiveLearningforImageRetrieval. In
ProceedingsoftheNinthACMInternationalConferenceonMultimedia,MULTIMEDIA’01,pp.
107–118,NewYork,NY,USA,2001.AssociationforComputingMachinery.
13PublishedasaconferencepaperatICLR2024
Evgenii Tsymbalov, Maxim Panov, and Alexander Shapeev. Dropout-based active learning for
regression. InAnalysisofImages,SocialNetworksandTexts,pp.247–258,Cham,2018.Springer
InternationalPublishing.
EvgeniiTsymbalov,SergeiMakarychev,AlexanderShapeev,andMaximPanov. DeeperConnections
betweenNeuralNetworksandGaussianProcessesSpeed-upActiveLearning.InProceedingsofthe
Twenty-EighthInternationalJointConferenceonArtificialIntelligence,IJCAI-19,pp.3599–3605.
InternationalJointConferencesonArtificialIntelligenceOrganization,72019.
LaurensvanderMaatenandGeoffreyHinton. VisualizingDatausingt-SNE. JournalofMachine
LearningResearch,9(86):2579–2605,2008.
AadvanderVaart.Asymptoticstatistics,volume3ofCambridgeSeriesinStatisticalandProbabilistic
Mathematics. Cambridgeuniversitypress,2000.
AadvanderVaartandJonA.Wellner.WeakConvergenceandEmpiricalProcesses:WithApplications
toStatistics. SpringerSeriesinStatistics(SSS).SpringerNewYork,1996.
JoaquinVanschoren,JanNVanRijn,BerndBischl,andLuisTorgo. Openml: networkedsciencein
machinelearning. ACMSIGKDDExplorationsNewsletter,15(2):49–60,2014.
MartinJ.Wainwright. High-DimensionalStatistics: ANon-AsymptoticViewpoint. Number48in
CambridgeSeriesinStatisticalandProbabilisticMathematics.CambridgeUniversityPress,2019.
Haonan Wang, Wei Huang, Andrew Margenot, Hanghang Tong, and Jingrui He. Deep Active
Learning by Leveraging Training Dynamics. In Advances in Neural Information Processing
Systems,volume35.CurranAssociates,Inc.,2022.
KaiWei,RishabhIyer,andJeffBilmes. Submodularityindatasubsetselectionandactivelearning.
InInternationalconferenceonmachinelearning,pp.1954–1963.PMLR,2015.
Jae Oh Woo. Active Learning in Bayesian Neural Networks with Balanced Entropy Learning
Principle. InTheEleventhInternationalConferenceonLearningRepresentations,2023.
Yazhou Yang, Xiaoqing Yin, Yang Zhao, Jun Lei, Weili Li, and Zhe Shu. Batch Mode Active
LearningBasedonMulti-SetClustering. IEEEAccess,9:51452–51463,2021.
YiYang, ZhigangMa, FeipingNie, XiaojunChang, andAlexanderG.Hauptmann. Multi-Class
ActiveLearningbyUncertaintySamplingwithDiversityMaximization. InternationalJournalof
ComputerVision,113(2):113–127,Jun2015.
Ofer Yehuda, Avihu Dekel, Guy Hacohen, and Daphna Weinshall. Active Learning Through a
Covering Lens. In Advances in Neural Information Processing Systems, volume 35. Curran
Associates,Inc.,2022.
DonggeunYooandInSoKweon. LearningLossforActiveLearning. In2019IEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR),pp.93–102,2019.
SergeyZagoruykoandNikosKomodakis. WideResidualNetworks. InProceedingsoftheBritish
MachineVisionConference(BMVC),pp.87.1–87.12.BMVAPress,September2016.
BeichenZhang,LiangLi,ShijieYang,ShuhuiWang,Zheng-JunZha,andQingmingHuang. State-
RelabelingAdversarialActiveLearning. In2020IEEE/CVFConferenceonComputerVisionand
PatternRecognition(CVPR),pp.8753–8762,2020.
GuangZhao,EdwardDougherty,Byung-JunYoon,FrancisAlexander,andXiaoningQian. Efficient
ActiveLearningforGaussianProcessClassificationbyErrorReduction. InAdvancesinNeural
InformationProcessingSystems,volume34,pp.9734–9746.CurranAssociates,Inc.,2021a.
Guang Zhao, Edward Dougherty, Byung-Jun Yoon, Francis Alexander, and Xiaoning Qian.
Uncertainty-aware Active Learning for Optimal Bayesian Classifier. In International Confer-
enceonLearningRepresentations,2021b.
14PublishedasaconferencepaperatICLR2024
CONTENTS
1 Introduction 1
2 LeastDisagreeMetric(LDM) 2
2.1 DefinitionofLDM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
2.2 AnAsymptoticallyConsistentEstimatorofLDM . . . . . . . . . . . . . . . . . . 3
2.3 EmpiricalEvaluationofLDM . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3 LDM-basedActiveLearning 4
3.1 LDM-Seeding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
3.2 LDM-S:ActiveLearningwithLDM-Seeding . . . . . . . . . . . . . . . . . . . . 6
4 Experiments 6
4.1 EffectivenessofLDMinSelecting(Batched)UncertainSamples . . . . . . . . . . 6
4.2 NecessityofPursuingDiversityinLDM-S . . . . . . . . . . . . . . . . . . . . . . 7
4.3 ComparingLDM-StoBaselineAlgorithms . . . . . . . . . . . . . . . . . . . . . 7
5 Conclusion 9
A RelatedWork 17
B ProofofTheorem1: LDMEstimatorisConsistent 17
B.1 SupportingLemmasandProofs. . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.2 ProofofCorollary1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C TheoreticalVerificationsofIntuitionsfor2DBinaryClassificationwithLinearClassifiers 20
C.1 LDMasanUncertaintyMeasure . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.2 Varyingσ2inAlgorithm1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.3 Assumption3HoldswithGaussianSampling . . . . . . . . . . . . . . . . . . . . 23
C.4 EffectofDiversityinLDM-S . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D Datasets,NetworksandExperimentalSettings 25
D.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.2 DeepNetworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
D.3 ExperimentalSettings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
E PerformanceProfileandPenaltyMatrix 27
E.1 PerformanceProfile . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
E.2 PenaltyMatrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
F AblationStudy 27
F.1 ChoiceofStopConditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
15PublishedasaconferencepaperatICLR2024
F.2 EffectivenessofLDM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
F.3 EffectofBatchSize . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
F.4 EffectofHyperparametersinLDM-S . . . . . . . . . . . . . . . . . . . . . . . . 30
G AdditionalResults 30
G.1 ComparingwithOtherUncertaintyMethodswithSeeding . . . . . . . . . . . . . 30
G.2 ComparisonacrossDatasetswithBAIT . . . . . . . . . . . . . . . . . . . . . . . 31
G.3 ComparisonperDatasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
16PublishedasaconferencepaperatICLR2024
APPENDIX
A RELATED WORK
There are various active learning algorithms such as uncertainty sampling (Lewis & Gale, 1994;
Sharma & Bilgic, 2017), expected model change (Freytag et al., 2014; Ash et al., 2020), model
outputchange(Mohamadietal.,2022),expectederrorreduction(Yoo&Kweon,2019;Zhaoetal.,
2021a),trainingdynamics(Wangetal.,2022),uncertaintyreduction(Zhaoetal.,2021b),core-set
approach(Sener&Savarese,2018;Mahmoodetal.,2022;Yehudaetal.,2022),clustering(Yang
etal.,2021;Citovskyetal.,2021),Bayesianactivelearning(Pinsleretal.,2019;Shi&Yu,2019),
discriminativesampling(Sinhaetal.,2019;Zhangetal.,2020;Guetal.,2021;Caramalauetal.,
2021),constrainedlearning(Elenteretal.,2022),anddataaugmentation(Kimetal.,2021).
Fortheuncertainty-basedapproach,variousformsofuncertaintymeasureshavebeenstudied. En-
tropy(Shannon,1948)basedalgorithmsqueryunlabeledsamplesyieldingthemaximumentropyfrom
thepredictivedistribution. Thesealgorithmsperformpoorlyinmulticlassastheentropyisheavily
influencedbyprobabilitiesoflessimportantclasses(Joshietal.,2009).Marginbasedalgorithms(Bal-
canetal.,2007)queryunlabeledsamplesclosesttothedecisionboundary. Thesample’scloseness
tothedecisionboundaryisoftennoteasilytractableindeeparchitecture(Mickischetal.,2020).
Mutual information based algorithms such as DBAL (Gal et al., 2017) and BatchBALD (Kirsch
etal.,2019)queryunlabeledsamplesyieldingthemaximummutualinformationbetweenpredictions
and posterior of model parameters. Both works use MC-dropout (Gal & Ghahramani, 2016) for
deepnetworkstoevaluateBALD(Houlsbyetal.,2011). DBALdoesnotconsiderthecorrelation
inthebatch,andBatchBALD,whichapproximatesbatch-wisejointmutualinformation,isnotap-
propriateforlargequerysizes. Disagreementbasedquery-by-committee(QBC)algorithms(Beluch
etal.,2018)queryunlabeledsamplesyieldingthemaximumdisagreementinlabelspredictedbythe
committee. Itrequiresahighcomputationalcostforindividualtrainingofeachcommitteenetwork.
GradientbasedalgorithmBADGE(Ashetal.,2020)queriesunlabeledsamplesthatarelikelyto
inducelargeanddiversechangestothemodel,andFisherInformation(vanderVaart,2000)based
algorithmBAIT(Ashetal.,2021)queriesunlabeledsamplesforwhichtheresultingMAPestimate
hasthelowestBayesriskusingFisherinformationmatrix. BothBADGEandBAITrequireahigh
computationalcostwhenthefeaturedimensionorthenumberofclassesislarge.
B PROOF OF THEOREM 1: LDM ESTIMATOR IS CONSISTENT
We consider multiclass classification, which we recall here from Section 2. Let X and Y be the
instance and label space with Y = {e }C , where e is the ith standard basis vector of RC (i.e.
i i=1 i
one-hotencodingofthelabeli),andHbethehypothesisspaceofh:X →Y.
Bythetriangleinequality,wehavethat
(cid:12) (cid:12) (cid:12) (cid:12)
|L
N,M
−L|≤(cid:12) (cid:12)L
N,M
−L(cid:101)N(cid:12) (cid:12)+(cid:12) (cid:12)L(cid:101)N −L(cid:12) (cid:12),
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
≜∆1(N,M) ≜∆2(N)
wherewedenoteL(cid:101)N :=min h∈Hg,x0 ρ(h,g).
N
Wedealwith∆ (N,M)first. Bydefinition,
1
(cid:12) (cid:12)
(cid:12) (cid:12)
∆ (N,M)=(cid:12) inf ρ (h,g)− inf ρ(h,g)(cid:12)
1 (cid:12) (cid:12)h∈Hg,x0 M h∈Hg,x0 (cid:12) (cid:12)
N N
(cid:12) (cid:12)
(cid:12) 1 (cid:88)M (cid:12)
=(cid:12) inf I[h(X )̸=g(X )]− inf E [I[h(X)̸=g(X)]](cid:12)
(cid:12) (cid:12)h∈Hg,x0 M i i h∈Hg,x0 X∼DX (cid:12) (cid:12)
N i=1 N
As∆ (N,M)isadifferenceofinfimumsofasequenceoffunctions,weneedtoestablishauniform
1
convergence-type result over a sequence of sets. This is done by invoking “general” Glivenko-
Cantelli(Lemma1)andourgenericbound(Lemma2)ontheempiricalRademachercomplexityon
17PublishedasaconferencepaperatICLR2024
ourhypothesisclassF ={f(x)=I[h(x)̸=g(x)]|h∈Hg,x0},i.e.,foranyscalarδ ≥0,wehave
N
(cid:114)
2log(CN)
sup |ρ (h,g)−ρ(h,g)|≤2 +δ. (7)
h∈Hg,x0 M M
N
(cid:16) (cid:17)
withD -probabilityatleast1−exp −Mδ2 .
X 8
(cid:113)
Nowletδ >0bearbitrary,andforsimplicity,denoteδ′ =2 2log(CN) +δ. AsHg,x0 isfinite,the
M N
infimumsinthestatementareactuallyachievedbysomeg ,g ∈Hg,x0,respectively. Thendueto
1 2 N
theuniformconvergence,wehavethatwithprobabilityatleast1−e−M 8δ2 ,
inf ρ(h,g)=ρ(g ,g)>ρ (g ,g)−δ′ > inf ρ (h,g)−δ′
2 M 2 M
h∈Hg,x0 h∈Hg,x0
N N
and
inf ρ (h,g)=ρ (g ,g)>ρ(g ,g)−δ′ > inf ρ(h,g)−δ′,
M M 1 1
h∈Hg,x0 h∈Hg,x0
N N
andthus,
(cid:12) (cid:12) (cid:114)
(cid:12) (cid:12) 2log(CN)
∆ (N,M)=(cid:12) inf ρ (h,g)− inf ρ(h,g)(cid:12)≤2 +δ.
1 (cid:12) (cid:12)h∈Hg,x0 M h∈Hg,x0 (cid:12) (cid:12) M
N N
ChoosingM > 8 log(CN),wehavethat∆ (N,M)≤2δwithprobabilityatleast1− 1 .
δ2 1 CN
Wenowdealwith∆ 2(N). Recallingitsdefinition,wehavethatforanyh∗ ∈Hg,x0 withρ(h∗,g)−
L(g,x )<ε,
0
(cid:12) (cid:12)
(cid:12) (cid:12)
∆ (N)=(cid:12) min ρ(h,g)− inf ρ(h,g)(cid:12)
2 (cid:12) (cid:12)
(cid:12)h∈Hg,x0 h∈Hg,x0 (cid:12)
N
(cid:12) (cid:12)
(cid:12) (cid:12)
≤(cid:12) min ρ(h,g)−ρ(h∗,g)(cid:12)+|ρ(h∗,g)−L(g,x )|
(cid:12) (cid:12) 0
(cid:12)h∈Hg,x0 (cid:12)
N
≤ min |ρ(h,g)−ρ(h∗,g)|+ε
h∈Hg,x0
N
≤ min ρ(h,h∗)+ε
h∈Hg,x0
N
(∗)
≤ B min d (h,h∗)+ε,
H
h∈Hg,x0
N
where(∗)followsfromAssumption2. Takingtheinfimumoverallpossibleh∗ onbothsides,by
Assumption3,wehavethat∆ (N)≤α(N,ε)+εw.p. atleast1−β(N).
2
Usingtheunionbound,wehavethatwithM > 8 log(CN),
δ2
P[L (g,x )−L(g,x )≤2δ+α(N,ε)+ε]≥P[∆ (N,M)+∆ (N)≤2δ+α(N,ε)+ε]
N,M 0 0 1 2
1
≥1− −β(N).
CN
Forthelastpart(convergenceinprobability),westartbydenotingZ =|L (g,x )−L(g,x )|.
δ,N N,M 0 0
(RecallthatM isafunctionofδ,accordingtoourparticularchoice). Undertheprescribedlimitsand
arbitraritiesandourassumptionthatα(N),β(N)→0asN →∞,wehavethatforanysufficiently
smallδ′ >0sufficientlylargeN,
P[Z ≤2δ′]≥1−2δ′.
δ′,s
Tobeprecise, givenarbitraryθ ,∆,ε > 0, letδ′ > 0besuchthat2δ+ε < δ′ . Thenitsuffices
0 2
(cid:16) (cid:16) (cid:17) (cid:16) (cid:17)(cid:17)
to choose N > max α−1 δ′;ε , 2 +β−1 δ′ , where α−1(·;ε) is the inverse function of
2 Cδ′ 2
α(·,ε)w.r.t. thefirstargument,andβ−1istheinversefunctionofβ.
18PublishedasaconferencepaperatICLR2024
Thisimpliesthatd (Z ,0) ≤ 2δ′,whered (X,Y) = inf{δ′ ≥ 0 | P[|X −Y| ≥ δ′] ≤ δ′}
KF δ′,s KF
is the Ky-Fan metric, which induces a metric structure on the given probability space with the
convergenceinprobability(seeSection9.2of(Dudley,2002)). Asθ ,∆,εisarbitraryandthussois
0
P
δ′,thisimpliesthat|L (g,x )−L(g,x )|→0.
N,M 0 0
Remark2. WebelievethatAssumption3canberelaxedwithaweakerassumption. Thisseemstobe
connectedwiththenotionofϵ-net(Wainwright,2019)aswellasrecentworksonrandomEuclidean
coverage(Reznikov&Saff,2015;Krapivsky,2023;Aldous,2022;Penrose,2023). Althoughthe
aforementionedworksandourlastassumptioniscommoninthattherandomsetisconstructedfrom
i.i.d. samples(hypotheses),onemaindifferenceisthatforourpurpose,insteadofcoveringtheentire
space,onejustneedstocoveracertainportionatwhichtheinfimumis(approximately)attained.
B.1 SUPPORTINGLEMMASANDPROOFS
Lemma1(Theorem4.2ofWainwright(2019)). LetY ,··· ,Y
i. ∼i.d.PforsomedistributionPover
1 n
X. Foranyb-uniformlyboundedfunctionclassF,anypositiveintegern≥1andanyscalarδ ≥0,
wehave
(cid:12) (cid:12)
(cid:12)1 (cid:88)n (cid:12)
sup(cid:12) f(Y )−E[f(Y)](cid:12)≤2R (F)+δ
(cid:12)n i (cid:12) n
f∈F(cid:12) (cid:12)
i=1
(cid:16) (cid:17)
withP-probabilityatleast1−exp −nδ2 . Here,R (F)istheempiricalRademachercomplexity
8b n
ofF.
Recall that in our case, n = M, b = 1, and F = {f(x)=I[h(x)̸=g(x)]|h∈Hg,x0}. The
N
followinglemmaprovidesagenericboundontheempiricalRademachercomplexityofF:
(cid:113)
Lemma2. R (F)≤ 2log(CN).
M M
Proof. Forsimplicity,wedenoteE≜E ,wheretheexpectationisw.r.t.X ∼D i.i.d.,and
{Xi}M i=1,σ i X
σistheM-dimensionalRademachervariable. Also,letl:[M]→[C]bethelabelingfunctionfor
fixedsamples{X }M i.e. l(i)=argmax [g(X )] . Asgoutputsone-hotencodingforeachi,
i i=1 c∈[C] i c
l(i)isuniqueandthuswell-defined. Thus,wehavethatf(X )=I[h(X )̸=g(X )]=1−h (X ),
i i i l(i) i
wherewedenoteh=(h ,h ,··· ,h )withh :X →{0,1}.
1 2 C j
Bydefinition,
(cid:34) M (cid:35)
1 (cid:88)
R (F)=E sup σ f(X )
M M i i
f∈F
i=1
(cid:34) M (cid:35)
1 (cid:88)
=E sup σ (1−h (X ))
h∈Hg N,x0 M
i=1
i l(i) i
(cid:34) M (cid:35)
1 (cid:88)
=E sup σ h (X )
h∈Hg N,x0 M
i=1
i l(i) i
(cid:34) M (cid:35)
1 (cid:88)
≤E sup σ h (X )
h∈Hg N,x0,l∈[C]M
i=1
i l i
=R ({h |h∈Hg,x0,l∈[C]})
M l N
(cid:114)
2log(CN)
≤ ,
M
wherethelastinequalityfollowsfromMassart’sLemma(Massart,2000)andthefactthatHg,x0 isa
N
finitesetofcardinalityatmostN.
19PublishedasaconferencepaperatICLR2024
(a) (b) (c)
(d) (e) (f)
Figure5: ExamplesofnegativeSpearman’srankcorrelationbetweenLDMorderanduncertainty
orderonMNIST(a),CIFAR10(b),SVHN(c),CIFAR100(d),TinyImageNet(e),andFOOD101(f).
B.2 PROOFOFCOROLLARY1
P
Withthegivenassumptions,weknowthatL (g,x ) → L(g,x )fork ∈ {i,j}. Fornotation
M,N k k
simplicitywedenoteL(g,x )asL(k),L (g,x )asL(k)andlim aslim . For
k M,N k N min(M,N)→∞ N→∞
M=ω(log(CN))
arbitraryε>0,wehave
(cid:104) (cid:105) (cid:104) ε ε(cid:105)
lim P L(i) >L(j)+ε = lim P L(i) >L(j)+ε ∧ |L(i)−L(i)|< ∧ |L(j)−L(j)|<
N→∞ N N N→∞ N N N 2 N 2
(cid:104) ε ε (cid:105)
≤ lim P L(i)+ >L(j)− +ε
N→∞ 2 2
(cid:104) (cid:105)
=P L(i) >L(j) =0.
C THEORETICAL VERIFICATIONS OF INTUITIONS FOR 2D BINARY
CLASSIFICATION WITH LINEAR CLASSIFIERS
(a) (b)
Figure6: ProofofProposition1.
20PublishedasaconferencepaperatICLR2024
Here,weconsidertwo-dimensionalbinaryclassificationwithasetoflinearclassifiers,H = {h :
h(x)=sgn(xTw)}wherew ∈R2istheparameterofh. Weassumethatxisuniformlydistributed
onX ={x:∥x∥≤1}⊂R2. Foragiveng ∈H,letvbetheparameterofg.
C.1 LDMASANUNCERTAINTYMEASURE
Recall from Section 2.1 that the intuition behind a sample with a small LDM indicates that its
predictioncanbeeasilyflip-floppedevenbyasmallperturbationinthepredictor. Wetheoretically
provethisintuition
Proposition1. Supposethathissampledwithw ∼N(v,Iσ2). Then,
L(g,x )<L(g,x )⇐⇒P[h(x )̸=g(x )]>P[h(x )̸=g(x )].
1 2 1 1 2 2
Figure5showsexamplesofSpearman’srankcorrelation(Spearman,1904)betweentheorderof
LDManduncertainty,whichisdefinedastheratiooflabelpredictionsbyasmallperturbationin
thepredictor,onMNIST,CIFAR10,SVHN,CIFAR100,TinyImageNet,andFOOD101datasets.
SampleswithincreasingLDMoruncertaintyarerankedfromhightolow. Theresultsshowthat
LDMorderanduncertaintyorderhaveastrongnegativerankcorrelation,andthusasamplewith
smallerLDMisclosertothedecisionboundaryandismoreuncertain.
ProofofProposition1. Oneimportantobservationisthatbythedualitybetweenw andx(Tong
& Chang, 2001), in R2, w is a point and x is represented by the hyperplane, l = {w ∈ R2 :
x
sgn(xTw)=0}. Supposethathissampledwithw ∼N(v,Iσ2),andletθ betheangleofv,θ
v x
betheanglebetweenl andpositivex-axis,andWv,xbethehalf-planedividedbyl whichdoes
x x
notcontainv:
Wv,x ={w′ ∈W |h′(x)̸=g(x)}
asinFigure6a. Then,L(g,x)=|θ −θ |/πandP[h(x)̸=g(x)]=P[w ∈Wv,x].
x v
Letd ,d bethedistancesbetweenvandl ,l respectively,and
1 2 x1 x2
W =Wv,x1 \Wv,x2, W =Wv,x2 \Wv,x1
1 2
asinFigure6b. Supposethatd <d ,then|θ −θ |<|θ −θ |sinced =∥w∥sin|θ −θ |,
1 2 x1 v x2 v i xi v
and
P[w ∈Wv,x1]−P[w ∈Wv,x2]=P[w ∈W ]−P[w ∈W ]>0
1 2
bythefollowings:
Wv,x1 =W ∪(Wv,x1 ∩Wv,x2), Wv,x2 =W ∪(Wv,x1 ∩Wv,x2)
1 2
whereW 1,W 2,andWv,x1∩Wv,x2 aredisjoint.NotethatW 1andW 2areone-to-onemappedbythe
symmetryatorigin,buttheprobabilitiesaredifferentbythebiasedlocationofv,i.e,ϕ(w |v,σ2)>
1
ϕ(w |v,σ2)forallpairsof(w ,w )∈W ×W thataresymmetricattheorigin. Hereϕ(·|v,σ2)
2 1 2 1 2
istheprobabilitydensityfunctionofthebivariatenormaldistributionwithmeanvandcovariance
σ2I. Thus,
L(g,x )<L(g,x )⇐⇒d <d ⇐⇒P[h(x )̸=g(x )]>P[h(x )̸=g(x )].
1 2 1 2 1 1 2 2
C.2 VARYINGσ2 INALGORITHM1
RecallfromSection2.2thattheintuitionbehindusingmultipleσ2 isthatitcontrolsthetrade-off
betweentheprobabilityofobtainingahypothesiswithadifferentpredictionthanthatofgandthe
scaleofρ (h,g). Theoretically,weshowthefollowingfortwo-dimensionalbinaryclassification
S
withlinearclassifiers:
Proposition2. Supposethathissampledwithw ∼N(v,Iσ2)wherew,v ∈W areparametersof
h,grespectively,thenE [ρ(h,g)]iscontinuousandstrictlyincreasingwithσ.
w
Figure7showstherelationshipbetweenρ¯ (h,g)andlogσforMNIST,CIFAR10,SVNH,CIFAR100,
S
TinyImageNet,andFOOD101datasetswherehissampledwithw ∼ N(v,Iσ2)andρ¯ (h,g)is
S
the mean of ρ (h,g) for sampled hs. The ρ¯ (h,g) is monotonically increasing with logσ in all
S S
experimentalsettingsforgeneraldeeplearningarchitectures.
21PublishedasaconferencepaperatICLR2024
(a) (b) (c)
(d) (e) (f)
Figure7: TherelationshipbetweenthedisagreemetricandperturbationstrengthforMNIST(a),
CIFAR10(b),SVHN(c),CIFAR100(d),TinyImageNet(e),andFOOD101(f)datasets. ρ¯ (h,g)is
S
monotonicallyincreasingwiththeperturbationstrengthinallexperimentalsettings.
(a) (b)
Figure8: ProofofProposition2.
ProofofProposition2. Bythedualitybetweenw andx,inW,w isapointandxisrepresented
by the hyperplane, l = {w ∈ W : sgn(xTw) = 0}. Let h be a sampled hypothesis with
x
w ∼ N(v,Iσ2), θ be the angle of v = (v ,v )T, i.e., tanθ = v /v , θ be the angle of w =
v 1 2 v 2 1
(w ,w )T,i.e.,tanθ =w /w ,andθ betheanglebetweenl andpositivex-axis. Here,θ,θ ∈
1 2 2 1 x x x
[−π+θ ,π+θ ]inconvenience. Whenθ orπ+θ isbetweenθandθ ,h(x)̸=g(x),otherwise
v v x x v
h(x)=g(x). Thus,ρ(h,g)=|θ−θ |/π.
v
UsingBox-Mullertransform(Box&Muller,1958),wcanbegeneratedby
(cid:112) (cid:112)
w =v +σ −2logu cos(2πu ), w =v +σ −2logu sin(2πu )
1 1 1 2 2 2 1 2
√
whereu andu areindependentuniformrandomvariableson[0,1].Then,∥w−v∥=σ −2logu
1 2 1
and(w −v )/(w −v )=tan(2πu ),i.e.,theangleofw−vis2πu . Here,
2 2 1 1 2 2
(cid:112)
∥v∥sin(θ−θ )=σ −2logu sin(2πu −θ) (8)
v 1 2
byusingtheperpendicularlinefromvtothelinepassingthroughtheoriginandw(seetheFigure8
for its geometry), and Eq. 8 is satisfied for all θ. For given u and u , θ is continuous and the
1 2
derivativeofθwithrespecttoσis
√
dθ
=
−2logu 1sin2(2πu 2−θ)
, thus
(cid:26) dd σθ >0, u
2
∈( 2θ πv,π+ 2πθv)
.
dσ ∥v∥sin(2πu 2−θ v) dd σθ <0, u
2
∈[0,1]\[ 2θ πv,π+ 2πθv]
22PublishedasaconferencepaperatICLR2024
(a) (b)
Figure9: ProofofProposition3.
Then,
dρ(h,g) dθ (cid:110)θ π+θ (cid:111)
=sgn(θ−θ ) >0 where u ∈/ v, v .
dσ v dσ 2 2π 2π
T ρ(h hu ,s g, )ρ( =h,g F) (i σs ,c uon ,t uin )u ,ou ths ea nn Ed st [r ρi (c htl ,y gi )n ]cr =eas (cid:82)in Fg (w σ,it uh ,σ uw )h fe (n uu )2 f(̸=
u
)2θ dπv uan dd uu 2 wh̸= ereπ+ 2 fπθ (v u. )L =et
1 2 w 1 2 1 2 1 2 i
I[0<u <1]. For0<σ <σ ,
i 1 2
E [ρ(h,g)]−E [ρ(h,g)]
w∼N(v,Iσ2) w∼N(v,Iσ2)
2 1
(cid:90)
= (F(σ ,u ,u )−F(σ ,u ,u ))f(u )f(u )du du >0.
2 1 2 1 1 2 1 2 1 2
C.3 ASSUMPTION3HOLDSWITHGAUSSIANSAMPLING
Proposition3. Assumethatthegivensamplex formsanangleθ ∈(0,π/2)w.r.t. thex-axis,and
0 0
considerε ∈ (0,1)suchthatθ +πε < π. Then,forbinaryclassificationwithlinearhypotheses
0 2
and H comprising of N i.i.d. random samples from N((1,0),σ2I ) for some fixed σ2 > 0,
N 2
(cid:16) (cid:17) (cid:16) √ (cid:17)
Assumption3holdswithα(N,ε)=O √1 andβ(N)=O e− N .
N
Proof. We want to show that there exists α(N,·) and β(N) with lim α(N,·) =
N→∞
lim β(N)=0suchthat
N→∞
(cid:34) (cid:35)
1
P inf min ∥w∗−w∥ ≥ α(N,ε) ≤β(N),
w∗:θ(w∗)∈(θ0,θ0+πε)w∈Hg N,x0 2 B
where θ(w∗) is the angle made between w∗ and the x-axis. Note that Hg,x0 is random as well.
N
Thus,wemakeuseofapeelingargumentasfollows: conditionedontheeventthat|Hg,x0|=S for
N
S ∈[N],wehavethatforany∆∈(0,sinθ ),
0
(cid:34) (cid:12) (cid:35)
(cid:12)
P inf min ∥w∗−w∥ ≥∆(cid:12)|Hg,x0|=S =(1−p(θ ,∆,ε))S,
w∗:θ(hw∗)∈(θ0,θ0+πε)w∈H Ng,x0 2 (cid:12) (cid:12) N 0
where p(θ ,∆,ε) is the measure of the region enclosed by the red boundary in Fig. 9a w.r.t.
0
N((1,0),σ2I ). Also,wehavethatforS ∈[N]
2
(cid:18) (cid:19)
N
P[|Hg,x0|=S]= q(θ )S(1−q(θ ))N−S,
N S 0 0
whereq(θ )isthemeasureofthe(light)redregioninFig.9bw.r.t. N((1,0),σ2I ).
0 2
Thus,wehavethat
(cid:34) (cid:35)
P inf min ∥w∗−w∥ ≥∆
2
w∗:θ(w∗)∈(θ0,θ0+πε)w∈H Ng,x0
23PublishedasaconferencepaperatICLR2024
(cid:88)N (cid:34) (cid:12) (cid:12) (cid:35)
= P inf min ∥w∗−w∥ ≥∆(cid:12)|Hg,x0|=S P[|Hg,x0|=S]
S=0
w∗:θ(w∗)∈(θ0,θ0+πε)w∈Hg N,x0 2 (cid:12) (cid:12) N N
N (cid:18) (cid:19)
= (cid:88) N q(θ )S(1−q(θ ))N−S(1−p(θ ,∆,ε))S
S 0 0 0
S=0
=((1−q(θ ))+q(θ )(1−p(θ ,∆,ε)))N
0 0 0
=(1−p(θ ,∆,ε)q(θ ))N
0 0
≤e−Np(θ0,∆,ε)q(θ0),
wherethelastinequalityfollowsfromthesimplefactthat1+x≤exforanyx∈R.
Nowitsufficestoobtainnon-vacuouslowerboundsofp(θ ,∆,ε)andq(θ ).
0 0
Lower bounding q(θ ). By rotational symmetry of N((1,0),σ2I ) and the fact that rotation
0 2
preserves Euclidean geometry, this is equivalent to finding the probability measure of the lower
half-planeundertheGaussiandistributionN((0,sinθ ),σ2I ),whichisasfollows:
0 2
1 (cid:90) 0 (cid:90) ∞ (cid:18) x2+(y−sinθ )2(cid:19)
q(θ )= exp − 0 dxdy
0 2πσ2 2σ2
−∞ −∞
1 (cid:90) −sinθ0 (cid:18) y2 (cid:19)
= √ exp − dy
2πσ 2σ2
−∞
1 1 (cid:90) sinθ0 (cid:18) y2 (cid:19)
= − √ exp − dy
2 2πσ 2σ2
0
(∗) 1 1 (cid:90) sinθ0(cid:18) 2σ2 (cid:18) (cid:18) (sinθ )2(cid:19)(cid:19) (cid:19)
≥ − √ 1−exp − 0 y+1 dy
2 2πσ
0
(sinθ 0)2 2σ2
1 1
(cid:18) (cid:18) (cid:18)
(sinθ
)2(cid:19)(cid:19) (cid:19)
= − √ σ2 1−exp − 0 +sinθ
2 2πσ 2σ2 0
1 1
(cid:18) (cid:18) (cid:18)
(sinθ
)2(cid:19)(cid:19)
sinθ
(cid:19)
= − √ σ 1−exp − 0 + 0 ,
2 2π 2σ2 σ
where(∗)followsfromthesimpleobservationthatex ≤ 1−e−ax+1forx∈[−a,0]anda>0.
a
Lowerboundingp(θ ,∆,ε). Viasimilarrotationalsymmetryargumentsandgeometricdecompo-
0
sitionoftheregionenclosedbytheredboundary(seeFig.9a),wehavethat
p(θ ,∆,ε)
0
≥A +A +A (seeFig.9a)
1 2 3
1 (cid:90) ∆(cid:90) ∞ (cid:18) x2+(y−sinθ )2(cid:19)
≥ exp − 0 dxdy
2πσ2 2σ2
0 0
1 (cid:90) 0 (cid:90) ∞ (cid:18) x2+(y−sin(θ +πε))2(cid:19)
+ exp − 0 dxdy
2πσ2 2σ2
−∆ 0
1 (cid:90) θ0+πε(cid:90) ∞ (cid:18) (rcosθ−1)2+(rsinθ)2(cid:19)
+ exp − rdrdθ
2πσ2 2σ2
θ0 0
1 (cid:90) ∆−sinθ0 (cid:18) y2 (cid:19) 1 (cid:90) ∆+sin(θ0+πε) (cid:18) y2 (cid:19)
= √ exp − dy+ √ exp − dy
2 2πσ 2σ2 2 2πσ 2σ2
−sinθ0 sin(θ0+πε)
1 (cid:90) θ0+πε(cid:90) ∞ (cid:18) r2−2rcosθ+1(cid:19)
+ exp − rdrdθ
2πσ2 2σ2
θ0 0
∆
(cid:18)
(sinθ
)2(cid:19)
∆
(cid:18)
(∆+sin(θ
+πε))2(cid:19)
≥ √ exp − 0 + √ exp − 0
2 2πσ 2σ2 2 2πσ 2σ2
ε (cid:90) ∞ (cid:18) r2+1(cid:19)
+ rexp − dr
2σ2 2σ2
0
24PublishedasaconferencepaperatICLR2024
Figure10: TheeffectofLDM-basedseeding. ConsideringonlyLDM,samplesareselectedonlyinA
orB,butsamplesinCorDarealsoselectediftheseedingmethodisapplied.
∆
(cid:18)
(sinθ
+1)2(cid:19)
ε
(cid:18)
1
(cid:19)
≥ √ exp − 0 + exp − .
2πσ 2σ2 2 2σ2
(cid:16) (cid:17)
Bychoosing∆=O √1 ,thepropositionholds.
N
C.4 EFFECTOFDIVERSITYINLDM-S
Here,weprovidemoreintuitiononwhysamplesselectedintheorderofthesmallestempiricalLDM
maynotbethebeststrategy,andthuswhyneedtopursuediversityviaseeding,whichwasverifiedin
realisticscenariosinSection4.2. Again,letusconsiderarealizablebinaryclassificationwithasetof
linearclassifiers,i.e.,thereexistsh∗whosetesterroriszero(SeeFigure10). Lettheredcirclesand
bluecrossesbethelabeledsamplesandgbethegivenhypotheseslearnedbythelabeledsamples. In
thiscase,samples’LDMingroupsAorBaresmall,andthoseingroupsCorDarelarge. Thus,the
algorithmwillalwayschoosesamplesingroupsAorBifwedonotimposediversityandchoose
onlysampleswiththesmallestLDM.However,thesamplesingroupsCorDarethesamplesthat
aremorehelpfultousinthiscase,i.e.,providemoreinformation. Therefore,pursuingdiversityis
necessarytoprovidethechancetoquerysamplesinCandD.
D DATASETS, NETWORKS AND EXPERIMENTAL SETTINGS
D.1 DATASETS
OpenML#6(Frey&Slate,1991)isaletterimagerecognitiondatasetwhichhas20,000samplesin
26classes. Eachsamplehas16numericfeatures. Inexperiments,itissplitintotwoparts: 16,000
samplesfortrainingand4,000samplesfortest.
OpenML#156(Vanschorenetal.,2014)isasynthesizeddatasetforrandomRBFwhichhas100,000
samplesin5classes. Eachsamplehas10numericfeatures. Inexperiments,asubsetissplitintotwo
parts: 40,000samplesfortrainingand10,000samplesfortest.
OpenML#44135(Fanty&Cole,1990)isaisolatedletterspeechrecognitiondatasetwhichhas7,797
samplesin26classes. Eachsamplehas614numericfeatures. Inexperiments,itissplitintotwo
parts: 6,237samplesfortrainingand1,560samplesfortest.
MNIST(Lecunetal.,1998)isahandwrittendigitdatasetwhichhas60,000trainingsamplesand
10,000testsamplesin10classes. Eachsampleisablackandwhiteimageand28×28insize.
CIFAR10andCIFAR100(Krizhevsky,2009)aretinyimagedatasetswhichhas50,000training
samplesand10,000testsamplesin10and100classesrespectively. Eachsampleisacolorimage
and32×32insize.
25PublishedasaconferencepaperatICLR2024
Table3: Settingsfordataandacquisitionsize. Acquisitionsizedenotesthenumberofinitiallabeled
samples+querysizeforeachstep(thesizeofpooldata)→thenumberoffinallabeledsamples.
#ofparameters Datasize
Dataset Model Acquisitionsize
sampled/total train/validation/test
OpenML#6 MLP 3.4K/22.0K 16,000/-/4,000 200 +200(2K) →4,000
OpenML#156 MLP 0.6K/18.6K 40,000/-/10,000 100 +100(2K) →2,000
OpenML#44135 MLP 3.4K/98.5K 6,237/-/1,560 100 +100(2K) →2,000
MNIST S-CNN 1.3K/1.2M 55,000/5,000/10,000 20 +20(2,000) →1,020
CIFAR10 K-CNN 5.1K/2.2M 45,000/5,000/10,000 200 +400(4,000) →9,800
SVHN K-CNN 5.1K/2.2M 68,257/5,000/26,032 200 +400(4,000) →9,800
CIFAR100 WRN-16-8 51.3K/11.0M 45,000/5,000/10,000 5,000 +2,000(10,000) →25,000
TinyImageNet WRN-16-8 409.8K/11.4M 90,000/10,000/10,000 10,000 +5,000(20,000) →50,000
FOOD101 WRN-16-8 206.9K/11.2M 60,600/15,150/25,250 6,000 +3,000(15,000) →30,000
ImageNet ResNet-18 513K/11.7M 1,153,047/128,120/50,000 128,120 +64,060(256,240) →384,360
Table4: Settingsfortraining.
Batch LearningRateSchedule
Dataset Model Epochs Optimizer LearningRate
size ×decay[epochschedule]
OpenML#6 MLP 100 64 Adam 0.001 -
OpenML#156 MLP 100 64 Adam 0.001 -
OpenML#44135 MLP 100 64 Adam 0.001 -
MNIST S-CNN 50 32 Adam 0.001 -
CIFAR10 K-CNN 150 64 RMSProp 0.0001 -
SVHN K-CNN 150 64 RMSProp 0.0001 -
CIFAR100 WRN-16-8 100 128 Nesterov 0.05 ×0.2[60,80]
TinyImageNet WRN-16-8 200 128 Nesterov 0.1 ×0.2[60,120,160]
FOOD101 WRN-16-8 200 128 Nesterov 0.1 ×0.2[60,120,160]
ImageNet ResNet-18 100 128 Nesterov 0.001 ×0.2[60,80]
SVHN (Netzer et al., 2011) is a real-world digit dataset which has 73,257 training samples and
26,032testsamplesin10classes. Eachsampleisacolorimageand32×32insize.
TinyImageNet(Le&Yang,2015)isasubsetoftheILSVRC(Russakovskyetal.,2015)dataset
whichhas100,000samplesin200classes. Eachsampleisacolorimageand64×64insize. In
experiments,TinyImageNetissplitintotwoparts: 90,000samplesfortrainingand10,000samples
fortest.
FOOD101(Bossardetal.,2014)isafine-grainedfoodimagedatasetwhichhas75,750training
samplesand25,250testsamplesin101classes. Eachsampleisacolorimageandresizedto75×75.
ImageNet (Russakovsky et al., 2015) is an image dataset organized according to the WordNet
hierarchywhichhas1,281,167trainingsamplesand50,000validationsamples(weusethevalidation
samplesastestsamples)in1,000classes.
D.2 DEEPNETWORKS
MLPconsistsof[128dense-dropout(0.3)-128dense-dropout(0.3)-#classdense-softmax]
layers,anditisusedforOpenMLdatasets.
S-CNN(Cholletetal.,2015)consistsof[3×3×32conv−3×3×64conv−2×2maxpool−dropout
(0.25)−128dense−dropout(0.5)−#classdense−softmax]layers,anditisusedforMNIST.
K-CNN(Cholletetal.,2015)consistsof[two3×3×32conv−2×2maxpool-dropout(0.25)−
two3×3×64conv−2×2maxpool-dropout(0.25)−512dense−dropout(0.5)−#classdense-
softmax]layers,anditisusedforCIFAR10andSVHN.
WRN-16-8(Zagoruyko&Komodakis,2016)isawideresidualnetworkthathas16convolutional
layersandawideningfactor8,anditisusedforCIFAR100,TinyImageNet,andFOOD101.
ResNet-18(Heetal.,2016)isaresidualnetworkthatisa72-layerarchitecturewith18deeplayers,
anditisusedforImageNet.
26PublishedasaconferencepaperatICLR2024
D.3 EXPERIMENTALSETTINGS
Theexperimentalsettingsforactivelearningregardingdataset,architecture,numberofparameters,
datasize,andacquisitionsizearesummarizedinTable3. Trainingsettingsregardinganumberof
epochs,batchsize,optimizer,learningrate,andlearningrateschedulearesummarizedinTable4.The
modelparametersareinitializedwithHenormalinitialization(Heetal.,2015)forallexperimental
settings. Forallexperiments,theinitiallabeledsamplesforeachrepetitionarerandomlysampled
accordingtothedistributionofthetrainingset.
E PERFORMANCE PROFILE AND PENALTY MATRIX
E.1 PERFORMANCEPROFILE
Theperformanceprofile,knownastheDolan-More´plot,hasbeenwidelyconsideredinbenchmarking
activelearning(Tsymbalovetal.,2018;2019),optimizationprofiles(Dolan&More´,2002),and
evengeneraldeeplearningtasks(Burnaevetal.,2015a;b). TointroducetheDolan-More´ plot,let
accD,r,t bethetestaccuracyofalgorithmAatstept∈[T ],fordatasetDandrepetitionr ∈[R],
A D
and∆D,r,t =max (accD,r,t)−accD,r,t. Here,T isthenumberofstepsfordatasetD,andRis
A A′ A′ A D
thetotalnumberofrepetitions. Then,wedefinetheperformanceprofileas
R (δ):=
1
(cid:88)(cid:34)(cid:80) r,tI(∆D A,r,t ≤δ)(cid:35)
,
A n RT
D D D
wheren isthenumberofdatasets. Intuitively,R (δ)isthefractionofcaseswheretheperformance
D A
gapbetweenalgorithmAandthebestcompetitorislessthanδ. Specifically,whenδ =0,R (0)is
A
thefractionofcasesonwhichalgorithmAperformsthebest.
E.2 PENALTYMATRIX
The penalty matrix P = (P ) is evaluated as done in Ash et al. (2020): For each dataset, step,
ij
and each pair of algorithms (A , A ), we have 5 test accuracies {accr}5 and {accr}5 re-
i j √ i r=1 j r=1
spectively. We compute the t-score as t = 5µ¯/σ¯, where µ¯ = 1(cid:80)5 (accr − accr) and
5 r=1 i j
(cid:113)
σ¯ = 1(cid:80)5 (accr−accr−µ¯)2. Thetwo-sidedpairedsamplet-testisperformedforthenull
4 r=1 i j
thatthereisnoperformancedifferencebetweenalgorithms: A issaidtobeatA whent>2.776
i j
(thecriticalpointofp-valuebeing0.05), andvice-versawhent < −2.776. ThenwhenA beats
i
A ,weaccumulateapenalty3of1/T toP whereT isthenumberofstepsfordatasetD,and
j D i,j D
vice-versa. Summingacrossthedatasetsgivesusthefinalpenaltymatrix.
F ABLATION STUDY
F.1 CHOICEOFSTOPCONDITIONs
Figure11ashowstheempiricallyevaluatedLDMbyAlgorithm1inbinaryclassificationwiththe
linearclassifierasdescribedinFigure1. Intheexperiment,thetrueLDMofthesampleissetto
0.01. TheevaluatedLDMisveryclosetothetrueLDMwhens = 10andreachesthetrueLDM
whens≥20withthegapbeingroughly10−4. Thissuggeststhatevenwithamoderatevalueofs,
Algorithm1canapproximatethetrueLDMwithsufficientlylowerror.
Figure11bshowstheempiricallyevaluatedLDMsofMNISTsamplesforafour-layeredCNNwhere
M issettobethetotalnumberofsamplesinMNIST,whichis60000. Wedenotex astheithsample
i
orderedbythefinalevaluatedLDM.ObservethattheevaluatedLDMsaremonotonicallydecreasing
assincreases,andtheyseemtoconvergewhilemaintainingtherankorder. Inpractice,alarges
forobtainingvaluesclosetothetrueLDMiscomputationallyprohibitiveasthealgorithmrequires
hugeruntimetosamplealargenumberofhypotheses. Forexample,whens=50000,ouralgorithm
samples∼50Mhypothesesandtakesroughly18hourstorunandevaluateLDM.Therefore,based
3Thischoiceofpenaltyensuresthateachdatasetcontributesequallytotheresultingpenaltymatrix.
27PublishedasaconferencepaperatICLR2024
(a) (b) (c)
(d) (e)
Figure11:EmpiricallyevaluatedLDMsbyAlgorithm1withrespecttothestopconditions. (a)Here,
weconsiderthetwo-dimensionalbinaryclassificationwiththelinearclassifier(seeFigure1). The
evaluatedLDMisclosetothetrueLDMevenwhens=10andreachesthetrueLDMwhens≥20.
(b)EvaluatedLDMsofMNISTsampleswithafour-layeredCNN.ObservethattheevaluatedLDM
monotonicallydecreasesassincreases,andtherankorderiswellmaintained. (c)Inthesamesetting,
therankcorrelationcoefficientoftheevaluatedLDMsofarangeofs’stothatats=50000. Note
thatalreadyats=10,therankcorrelationcoefficientis0.998,suggestingthats=10suffices. (d-e)
ThenumberofsampledhypothesesandruntimeforevaluatingLDMarealmostlinearlyproportional
tothestopcondition.
upontheobservationthattherankorderispreservedthroughoutthevaluesofs,wefocusontherank
orderoftheevaluatedLDMsratherthantheirtruevalues.
Figure11cshowstherankcorrelationcoefficientoftheevaluatedLDMsofarangeofs’stothatat
s =50000. Evenwhens =10,therankcorrelationcoefficientbetweens =10ands =50000is
already0.998. WeobservedthatthesamepropertiesoftheevaluatedLDMsw.r.t. thestopcondition
salsoholdforotherdatasets,i.e.,thepreservationofrankorderholdsingeneral.
Figure11dand11eshowthenumberofsampledhypothesesandruntimewithrespecttothestop
conditionwhenLDMisevaluated.Botharealmostlinearlyproportionaltothestopconditionandthus
weshouldsetthestopconditionassmallaspossibletoreducetherunningtimeinLDMevaluation.
F.2 EFFECTIVENESSOFLDM
ToisolatetheeffectivenessofLDM,weconsiderthreeothervariantsofLDM-S.‘LDM-smallest’
selectbatcheswiththesmallestLDMswithouttakingdiversityintoaccount,‘Seeding(cos)’and
‘Seeding(ℓ )’aretheunweightedk-means++seedingmethodsusingcosineandℓ distance,respec-
2 2
tively. NotethatthelasttwodonotuseLDMinanyway. Wehaveexcludedbatchdiversitytofurther
clarifytheeffectivenessofLDM.
Figure12showsthetestaccuracywithrespecttothenumberoflabeledsamplesonMNIST,CIFAR10,
andCIFAR100datasets.Indeed,weobserveasignificantperformanceimprovementwhenusingLDM
andafurtherimprovementwhenbatchdiversityisconsidered. Additionalexperimentsareconducted
tocomparethek-means++seedingwithFASS(Weietal.,2015)orCluster-Margin(Citovskyetal.,
2021),whichcanbeabatchdiversitymethodforLDM.AlthoughFASShelpsLDMslightly,itfalls
shortofLDM-S.Cluster-MarginalsodoesnothelpLDMand,surprisingly,degradestheperformance.
WebelievethisisbecauseCluster-Marginstronglypursuesbatchdiversity,diminishingtheeffectof
LDMasanuncertaintymeasure. Specifically,Cluster-MarginconsiderssamplesofvaryingLDM
scoresfromthebeginning(asomewhatlargeportionofthemarethusnotsouseful). Incontrast,
28PublishedasaconferencepaperatICLR2024
(a) (b) (c)
Figure12: TheeffectofdiversesamplinginLDM-SonMNIST(a),CIFAR10(b),andCIFAR100
(c)datasets. ‘LDM-smallest’: selectingbatchwiththesmallestLDM,‘Seeding(cos)’: unweighted
seedingusingcosinedistance,‘Seeding(ℓ )’: unweightedseedingusingℓ -distance,‘LDM+FASS’:
2 2
thecombinationofLDMandFASS,‘LDM+C-Margin’:thecombinationofLDMandCluster-Margin.
LDM-Sleadstosignificantperformanceimprovementcomparedtothosewithoutbatchdiversity,
withFASS,orwithCluster-Margin.
(a) (b) (c)
Figure13: Performancecomparisonwhenthebatchsizesare1(a),10(b),and40(c)onMNIST.
(a) (b) (c)
Figure14: Performancecomparisonwhenthebatchsizeis5KonCIFAR10(a), SVHN(b), and
CIFAR100(c).
ouralgorithmsignificantlyweightsthesampleswithsmallLDM,biasingoursamplestowardsmore
uncertainsamples(andthusmoreuseful).
F.3 EFFECTOFBATCHSIZE
ToverifytheeffectivenessofLDM-Sforbatchmode,wecomparetheactivelearningperformance
withrespecttovariousbatchsizes. Figure13showsthetestaccuracywhenthebatchsizesare1,
10, and 40 on the MNIST dataset. Figure 14 shows the test accuracy when the batch size is 5K
onCIFAR10,SVHM,andCIFAR100datasets. Overall,LDM-Sperformswellcomparedtoother
algorithms,evenwithsmallandlargebatchsizes. Therefore,theproposedalgorithmisrobustto
thebatchsize,whileotherbaselinealgorithmsoftendonot,e.g.,BADGEperformswellwithbatch
size10or40butispoorwithbatchsize1onMNIST.Notethatwehaveaddedadditionalresults
29PublishedasaconferencepaperatICLR2024
(a) (b) (c)
Figure15: Performancevshyperparameters. Thetestaccuracywithrespecttostopconditions(a),
thenumberofMonteCarlosamplesforapproximatingρ(b),andsigmas’interval(c).
(a) (b) (c)
Figure16: TheperformancecomparisonofLDM-Swiththestandarduncertaintymethodstowhich
weighted seeding is applied on MNIST (a), CIFAR10 (b), and CIFAR100 (c). Even if weighted
seedingisappliedtothestandarduncertaintymethods,LDM-Sperformsbetter.
forBatchBALDinFigure13andCluster-Margin(C-Margin)inFigure14. Forbothsettings,we’ve
matchedthesettingasdescribedintheiroriginalpapers.
F.4 EFFECTOFHYPERPARAMETERSINLDM-S
Therearethreehyperparametersintheproposedalgorithm: stopconditions,thenumberofMonte
CarlosamplesM,andthesetofvariances{σ2}K .
k k=1
The stop condition s is required for LDM evaluation. We set s = 10, considering the rank
correlation coefficient and computing time. Figure 15a shows the test accuracy with respect to
s∈{1,5,10,100,1000},andthereisnosignificantperformancedifference.
ThenumberofMonteCarlosamplesM issetforapproximatingρ. Theproposedalgorithmaims
todistinguishLDMsofpooldata,thus,wesetM tobethesamesizeasthepoolsize. Figure15b
shows the test accuracy with respect to M ∈ {10,100,1000,10000}, and there is no significant
performancedifferenceexceptwhereM isextremelysmall,e.g.,M =10.
Thesetofvariances{σ2}K issetforhypothesissampling. Figure7inAppendixC.2showsthe
k k=1
relationshipbetweenthedisagreemetricandσ2. ToproperlyapproximateLDM,weneedtosample
hypotheseswithawiderangeofρ,andthusweneedawiderangeofσ2. Toefficientlycoverawide
rangeofσ2,wemaketheexponentequallyspacedsuchasσ =10βk−5whereβ >0andsetσthe
k
have10−5to1. Figure15cshowsthetestaccuracywithrespecttoβ ∈{0.01,0.05,0.1,0.5,1},and
thereisnosignificantperformancedifference.
G ADDITIONAL RESULTS
G.1 COMPARINGWITHOTHERUNCERTAINTYMETHODSWITHSEEDING
ToclarifywhetherthegainsofLDM-Soverthestandarduncertaintymethodsareduetoweighted
seedingorduetothesuperiorityofLDM,theperformanceofLDM-Siscomparedwiththosemethods
30PublishedasaconferencepaperatICLR2024
(a) (b)
Figure17: ComparisonwithBAITacrossOpenML#6,#156,#44135,MNIST,CIFAR10,andSVHN
datasets. (a)Dolan-More´ plot. (b)Thepenaltymatrix.
towhichweightedseedingisapplied. Figure16showsthetestaccuracywithrespecttothenumber
oflabeledsamplesonMNIST,CIFAR10,andCIFAR100datasets. Overall,evenwhenweighted
seedingisappliedtothestandarduncertaintymethods,LDM-Sstillperformsbetteronalldatasets.
Therefore,theperformancegainsofLDM-ScanbeattributedtoLDM’ssuperiorityoverthestandard
uncertaintymeasures.
G.2 COMPARISONacrossDATASETSWITHBAIT
TheperformanceprofileandpenaltymatrixbetweenLDM-SandBAIT,areexaminedonOpenML#6,
#156,#44135,MNIST,CIFAR10,andSVHNdatasetswheretheexperimentsforBAITareconducted.
Figure17ashowstheperformanceprofilew.r.t. δ. ItisclearthatLDM-SretainsthehighestR (δ)
A
overallconsideredδ’s,whichmeansthatourLDM-Soutperformstheotheralgorithmsincluding
BAIT.Figure17bshowsthepenaltymatrix. Itis,also,clearthatLDM-Sgenerallyoutperformsall
otheralgorithmsincludingBAIT.
G.3 COMPARISONperDATASETS
Table5showsthemean±stdofthetestaccuracies(%)w.r.t. thenumberoflabeledsamplesfor
OpenML#6,#156,#44135withMLP;MNISTwithSCNN;CIFAR10andSVHNwithK-CNN;
CIFAR100,TinyImageNet,FOOD101withWRN-16-8;andImageNetwithResNet-18. Overall,
LDM-Seitherconsistentlyperformsbestorisatparwithotheralgorithmsforalldatasets,whilethe
performanceofthealgorithmsexceptLDM-Svariesdependingondatasets.
31PublishedasaconferencepaperatICLR2024
32
)ecnamrofreptseb-dnoces
:dlob,ecnamrofreptseb
:denilrednu+dlob(
.selpmasdelebalforebmuneht
.t.r.w)%(seicaruccatsetehtfodts±naemehT
:5elbaT
TIAB
EGDAB
SNE
LABD
voCborP
teseroC
nigraM
yportnE
dnaR
S-MDL
|L|
6#LMnepO
10.0±34.16
10.0±56.16
20.0±69.06
20.0±46.85
10.0±00.16
20.0±32.16
30.0±22.16
10.0±29.65
10.0±46.06
10.0±24.26
004
00.0±55.37
10.0±31.57
10.0±35.57
10.0±95.07
00.0±01.37
10.0±66.37
10.0±49.47
10.0±83.66
00.0±15.27
10.0±92.67
008
10.0±25.87
10.0±58.97
10.0±03.18
10.0±89.57
10.0±17.77
10.0±46.87
00.0±75.08
20.0±26.07
00.0±94.77
10.0±26.18
002,1
00.0±48.18
00.0±06.28
10.0±42.48
00.0±70.97
10.0±51.08
10.0±62.18
10.0±19.38
20.0±16.37
00.0±02.08
00.0±47.48
006,1
10.0±93.48
00.0±18.48
00.0±28.68
00.0±37.18
10.0±12.28
10.0±71.38
00.0±14.68
20.0±94.77
00.0±60.28
00.0±50.78
000,2
10.0±37.68
00.0±40.78
10.0±47.88
10.0±02.48
10.0±10.48
10.0±00.58
10.0±95.88
10.0±08.18
10.0±26.38
00.0±21.98
004,2
00.0±44.88
00.0±13.88
10.0±01.09
10.0±00.68
00.0±40.58
00.0±22.68
00.0±67.98
10.0±21.58
10.0±77.48
00.0±53.09
008,2
00.0±67.98
00.0±81.98
00.0±69.09
10.0±34.78
00.0±59.58
00.0±22.78
00.0±38.09
00.0±73.78
10.0±69.58
00.0±60.19
002,3
00.0±75.09
00.0±12.09
00.0±07.19
00.0±38.88
10.0±09.68
10.0±58.78
00.0±05.19
00.0±75.88
00.0±77.68
00.0±08.19
006,3
00.0±82.19
00.0±53.19
00.0±26.29
00.0±96.98
00.0±27.78
10.0±94.88
00.0±63.29
00.0±45.98
10.0±00.88
00.0±35.29
000,4
651#LMnepO
30.0±26.76
20.0±53.17
20.0±79.76
40.0±01.46
10.0±20.07
30.0±51.16
30.0±59.76
20.0±32.56
20.0±03.07
20.0±87.96
002
20.0±40.67
20.0±47.38
20.0±91.28
10.0±87.07
10.0±86.28
30.0±29.36
20.0±85.18
30.0±24.67
10.0±72.38
20.0±80.38
004
10.0±86.77
10.0±86.68
10.0±83.68
20.0±60.27
00.0±56.58
40.0±64.56
10.0±57.68
10.0±11.38
10.0±02.68
10.0±03.78
006
20.0±74.87
00.0±79.78
00.0±71.88
20.0±88.27
00.0±97.68
50.0±04.56
00.0±41.88
10.0±59.58
00.0±90.78
00.0±06.88
008
10.0±98.87
00.0±49.88
00.0±12.98
20.0±28.27
10.0±65.78
50.0±95.56
00.0±38.88
10.0±56.78
10.0±36.78
00.0±23.98
000,1
10.0±92.97
00.0±95.98
00.0±88.98
30.0±55.27
00.0±15.88
50.0±09.76
00.0±38.98
00.0±31.98
00.0±13.88
00.0±80.09
002,1
10.0±54.08
00.0±99.98
00.0±32.09
20.0±78.27
00.0±30.98
20.0±41.96
00.0±04.09
00.0±88.98
00.0±87.88
00.0±34.09
004,1
10.0±07.08
00.0±82.09
00.0±84.09
10.0±60.37
00.0±64.98
40.0±20.07
00.0±76.09
00.0±41.09
00.0±11.98
00.0±07.09
006,1
20.0±58.18
00.0±94.09
00.0±57.09
20.0±69.37
00.0±96.98
50.0±30.07
00.0±58.09
00.0±73.09
00.0±24.98
00.0±59.09
008,1
20.0±71.48
00.0±37.09
00.0±10.19
30.0±29.37
00.0±50.09
50.0±29.07
00.0±90.19
00.0±26.09
00.0±76.98
00.0±91.19
000,2
53144#LMnepO
10.0±28.87
10.0±33.87
20.0±61.87
20.0±65.67
10.0±07.18
00.0±29.57
10.0±33.18
10.0±76.57
20.0±44.77
10.0±82.28
002
10.0±05.98
00.0±06.88
00.0±06.88
10.0±65.68
10.0±95.19
10.0±29.48
00.0±01.29
20.0±81.68
10.0±12.68
10.0±56.29
004
00.0±92.29
10.0±60.29
00.0±81.29
00.0±36.09
00.0±15.39
10.0±84.98
00.0±21.49
10.0±71.19
10.0±81.98
00.0±15.49
006
10.0±94.39
00.0±24.39
10.0±07.39
00.0±56.29
00.0±96.49
10.0±92.19
00.0±03.59
00.0±62.39
10.0±50.19
10.0±43.59
008
10.0±14.49
10.0±82.49
10.0±28.49
00.0±87.39
10.0±20.59
00.0±74.29
00.0±30.69
00.0±14.49
00.0±09.19
00.0±68.59
000,1
00.0±78.49
00.0±21.59
10.0±92.59
10.0±75.49
00.0±22.59
00.0±30.39
00.0±82.69
00.0±31.59
00.0±53.29
00.0±31.69
002,1
00.0±34.59
00.0±35.59
10.0±35.59
00.0±62.59
00.0±75.59
00.0±16.39
00.0±83.69
00.0±26.59
00.0±18.29
00.0±72.69
004,1
10.0±46.59
10.0±88.59
00.0±85.59
00.0±84.59
00.0±37.59
10.0±81.49
10.0±44.69
00.0±39.59
00.0±92.39
00.0±15.69
006,1
00.0±48.59
00.0±89.59
10.0±27.59
00.0±36.59
00.0±97.59
00.0±14.49
00.0±83.69
10.0±48.59
10.0±24.39
00.0±06.69
008,1
00.0±56.59
00.0±62.69
00.0±31.69
00.0±12.69
10.0±69.59
00.0±95.49
00.0±12.69
00.0±04.69
10.0±37.39
00.0±26.69
000,2
egaptxennodeunitnoCPublishedasaconferencepaperatICLR2024
33
egapsuoiverpmorfdeunitnoC–5elbaT
TIAB
EGDAB
SNE
LABD
voCborP
teseroC
nigraM
yportnE
dnaR
S-MDL
|L|
TSINM
10.0±70.78
20.0±34.58
10.0±43.58
10.0±95.87
10.0±16.68
30.0±92.97
10.0±68.48
40.0±76.08
20.0±23.18
10.0±80.78
021
10.0±99.29
10.0±35.29
10.0±07.19
10.0±34.88
10.0±44.29
20.0±73.78
10.0±71.29
10.0±38.09
10.0±80.88
00.0±29.29
022
00.0±39.49
00.0±86.49
00.0±03.49
00.0±30.39
00.0±03.49
20.0±09.09
00.0±76.49
00.0±59.39
10.0±99.09
00.0±88.49
023
00.0±90.69
10.0±46.59
00.0±28.59
00.0±22.59
00.0±34.59
20.0±48.29
00.0±78.59
00.0±53.59
10.0±54.29
00.0±10.69
024
00.0±46.69
00.0±33.69
00.0±64.69
00.0±08.59
00.0±31.69
10.0±07.39
00.0±43.69
00.0±00.69
10.0±15.39
00.0±05.69
025
00.0±11.79
00.0±28.69
00.0±49.69
00.0±08.69
00.0±36.69
10.0±77.49
00.0±39.69
00.0±87.69
00.0±22.49
00.0±69.69
026
00.0±82.79
00.0±20.79
00.0±63.79
00.0±59.69
00.0±89.69
10.0±12.59
00.0±51.79
00.0±21.79
00.0±87.49
00.0±33.79
027
00.0±94.79
00.0±64.79
00.0±65.79
00.0±13.79
00.0±60.79
10.0±83.59
00.0±64.79
00.0±14.79
00.0±81.59
00.0±85.79
028
00.0±56.79
00.0±16.79
00.0±36.79
00.0±15.79
00.0±44.79
00.0±79.59
00.0±86.79
00.0±96.79
00.0±44.59
00.0±77.79
029
00.0±79.79
00.0±87.79
00.0±57.79
00.0±77.79
00.0±36.79
10.0±48.59
00.0±48.79
00.0±88.79
00.0±88.59
00.0±59.79
020,1
01RAFIC
00.0±84.94
10.0±32.05
10.0±39.94
10.0±15.94
10.0±64.94
10.0±44.74
10.0±53.94
10.0±69.84
10.0±61.94
00.0±03.05
004,1
10.0±53.65
10.0±47.65
10.0±93.65
10.0±35.55
10.0±90.65
10.0±59.25
10.0±15.65
10.0±53.55
10.0±40.65
10.0±10.75
006,2
10.0±93.06
10.0±45.06
10.0±43.06
10.0±34.95
00.0±99.95
10.0±03.65
00.0±94.06
10.0±86.95
00.0±99.95
10.0±03.16
008,3
00.0±00.36
10.0±39.36
00.0±23.36
00.0±13.26
10.0±37.26
00.0±07.85
10.0±18.26
00.0±06.26
10.0±96.26
00.0±90.46
000,5
00.0±23.56
00.0±03.56
10.0±99.46
10.0±00.46
10.0±83.46
10.0±80.16
00.0±79.46
00.0±24.46
10.0±65.46
00.0±97.56
002,6
10.0±28.66
10.0±11.76
10.0±87.66
10.0±21.66
10.0±82.66
10.0±48.26
10.0±57.66
00.0±92.66
00.0±78.56
10.0±82.76
004,7
00.0±99.76
00.0±24.86
10.0±11.86
10.0±49.76
00.0±06.76
10.0±05.36
10.0±78.76
10.0±77.76
10.0±05.76
00.0±38.86
006,8
10.0±24.96
00.0±80.96
10.0±32.96
10.0±28.86
00.0±54.86
10.0±44.46
10.0±31.96
00.0±26.86
10.0±15.86
00.0±01.07
008,9 NHVS
00.0±19.77
10.0±13.87
10.0±94.77
10.0±24.87
10.0±65.77
10.0±48.57
10.0±49.77
10.0±52.67
10.0±22.77
10.0±20.97
004,1
00.0±31.38
00.0±94.38
10.0±32.38
10.0±72.38
10.0±55.28
10.0±26.08
10.0±16.38
00.0±33.28
10.0±55.18
00.0±67.38
006,2
00.0±45.58
00.0±86.58
10.0±58.58
00.0±00.68
00.0±95.48
10.0±59.18
10.0±82.58
00.0±33.58
00.0±16.38
00.0±80.68
008,3
00.0±94.78
10.0±23.78
00.0±32.78
00.0±94.78
00.0±29.58
00.0±21.38
00.0±11.78
00.0±87.68
00.0±50.58
00.0±85.78
000,5
10.0±16.88
00.0±83.88
00.0±03.88
00.0±19.88
00.0±98.68
00.0±12.48
00.0±11.88
00.0±90.88
00.0±89.58
00.0±46.88
002,6
00.0±92.98
00.0±02.98
10.0±13.98
00.0±16.98
00.0±75.78
00.0±37.48
00.0±30.98
00.0±48.88
00.0±95.68
00.0±65.98
004,7
00.0±70.09
00.0±09.98
00.0±40.09
00.0±43.09
00.0±22.88
00.0±52.58
00.0±26.98
00.0±56.98
00.0±82.78
00.0±40.09
006,8
00.0±25.09
00.0±01.09
00.0±23.09
00.0±77.09
00.0±08.88
00.0±31.68
00.0±54.09
00.0±61.09
00.0±27.78
00.0±16.09
008,9
egaptxennodeunitnoCPublishedasaconferencepaperatICLR2024
34
egapsuoiverpmorfdeunitnoC–5elbaT
TIAB
EGDAB
SNE
LABD
voCborP
teseroC
nigraM
yportnE
dnaR
S-MDL
|L|
001RAFIC
00.0±98.03
10.0±90.13
10.0±08.03
10.0±67.13
10.0±64.13
10.0±81.13
10.0±47.03
10.0±82.13
00.0±58.13
000,7
10.0±96.63
00.0±24.63
10.0±23.63
10.0±86.73
10.0±06.73
20.0±38.63
00.0±03.63
10.0±49.63
00.0±16.73
000,9
10.0±43.04
00.0±40.04
10.0±42.04
10.0±34.14
10.0±33.14
10.0±33.04
10.0±19.93
20.0±04.04
10.0±88.04
000,11
10.0±44.34
10.0±08.24
10.0±23.34
10.0±20.44
00.0±51.44
10.0±15.34
10.0±61.34
00.0±31.34
10.0±68.34
000,31
10.0±02.64
10.0±35.54
10.0±11.64
10.0±55.64
10.0±85.64
00.0±01.64
10.0±77.54
10.0±77.54
10.0±95.64
000,51
-
00.0±64.84
10.0±85.74
10.0±32.84
10.0±42.84
10.0±96.84
00.0±63.84
10.0±68.74
10.0±57.74
10.0±18.84
000,71
10.0±02.94
10.0±25.84
10.0±99.84
10.0±28.84
10.0±82.94
10.0±69.84
00.0±66.84
10.0±53.84
10.0±14.94
000,91
10.0±45.05
10.0±01.05
10.0±06.05
00.0±43.05
10.0±17.05
10.0±65.05
10.0±94.05
10.0±78.94
00.0±49.05
000,12
10.0±60.25
10.0±47.15
00.0±92.25
00.0±20.25
00.0±39.15
10.0±40.25
10.0±01.25
10.0±92.15
00.0±84.25
000,32
10.0±36.55
10.0±30.55
10.0±27.55
10.0±07.55
10.0±51.55
00.0±25.55
10.0±37.55
10.0±15.45
00.0±00.65
000,52
teNegamIyniT
10.0±07.32
10.0±32.32
10.0±17.32
00.0±60.42
10.0±78.32
10.0±08.32
00.0±31.32
10.0±27.32
00.0±79.32
000,51
00.0±56.72
10.0±92.72
10.0±07.72
10.0±22.82
10.0±76.72
10.0±93.72
00.0±27.62
00.0±56.72
00.0±67.72
000,02
00.0±12.13
10.0±28.03
10.0±03.13
10.0±46.13
00.0±81.13
00.0±09.03
10.0±03.03
00.0±22.13
10.0±15.13
000,52
00.0±02.43
00.0±37.33
10.0±72.43
10.0±45.43
10.0±58.33
00.0±27.33
10.0±83.33
10.0±32.43
10.0±93.43
000,03
-
10.0±88.63
10.0±11.63
00.0±88.63
10.0±97.63
10.0±83.63
00.0±87.63
00.0±61.63
10.0±77.63
10.0±34.73
000,53
10.0±09.83
10.0±34.83
10.0±80.93
20.0±20.93
10.0±63.83
10.0±15.93
00.0±43.83
10.0±95.83
10.0±93.93
000,04
10.0±07.04
10.0±37.04
20.0±29.04
00.0±37.04
10.0±79.93
00.0±05.14
10.0±40.04
20.0±52.04
10.0±23.14
000,54
10.0±10.24
10.0±09.24
20.0±95.24
10.0±07.24
20.0±61.14
10.0±10.34
10.0±04.14
10.0±37.14
10.0±56.24
000,05
101DOOF
10.0±55.52
00.0±45.52
10.0±60.62
00.0±19.52
10.0±94.62
10.0±36.52
00.0±24.52
10.0±68.52
00.0±23.62
000,9
10.0±38.92
10.0±65.92
10.0±44.03
00.0±04.03
00.0±30.13
10.0±77.92
00.0±01.92
10.0±98.92
10.0±38.03
000,21
10.0±23.43
10.0±17.33
10.0±21.53
10.0±07.43
10.0±04.53
10.0±13.43
10.0±10.33
00.0±91.43
10.0±80.53
000,51
10.0±88.73
10.0±28.63
00.0±84.83
10.0±00.83
10.0±26.83
10.0±86.73
10.0±81.63
10.0±51.73
10.0±05.83
000,81
-
10.0±96.14
10.0±66.04
10.0±61.24
10.0±06.14
10.0±73.24
10.0±35.14
10.0±17.93
00.0±47.04
10.0±62.24
000,12
10.0±71.54
10.0±39.34
10.0±02.54
10.0±85.44
10.0±42.54
10.0±84.44
10.0±42.34
10.0±18.34
10.0±64.54
000,42
10.0±93.84
00.0±94.74
10.0±96.84
00.0±76.74
10.0±07.84
10.0±57.74
10.0±46.64
10.0±73.74
00.0±88.84
000,72
10.0±39.05
10.0±21.05
10.0±25.15
10.0±27.94
00.0±22.15
00.0±02.05
10.0±76.94
00.0±42.05
00.0±04.15
000,03
teNegamI
10.0±09.14
00.0±29.14
10.0±09.14
00.0±72.14
10.0±26.14
00.0±52.14
10.0±54.14
00.0±99.14
081,291
10.0±25.64
10.0±94.64
00.0±33.64
00.0±14.54
10.0±10.64
10.0±85.54
00.0±09.54
00.0±26.64
042,652
-
-
10.0±88.94
10.0±27.94
10.0±33.94
10.0±83.84
10.0±62.94
10.0±68.84
10.0±80.94
00.0±41.05
003,023
10.0±59.25
00.0±64.25
00.0±68.15
00.0±69.05
10.0±61.25
00.0±81.25
00.0±11.25
00.0±35.35
063,483