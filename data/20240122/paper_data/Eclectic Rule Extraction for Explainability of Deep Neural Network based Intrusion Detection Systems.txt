Eclectic Rule Extraction for Explainability
of Deep Neural Network based
Intrusion Detection Systems
Jesse Ables∗, Nathaniel Childers∗, William Anderson∗,
Sudip Mittal∗, Shahram Rahimi∗, Ioana Banicescu∗, and Maria Seale†
∗ Department of Computer Science & Engineering
Mississippi State University, Mississippi, USA,
(email: {jha92, nac294, wha41}@msstate.edu, {mittal, rahimi, ioana}@cse.msstate.edu)
† U.S Army Engineer Research and Development Center
Vicksburg, Mississippi, USA, (email: maria.a.seale@erdc.dren.mil)
Abstract—This paper addresses trust issues created from the global explanations for neural networks but are themselves
ubiquity of black box algorithms and surrogate explainers in blackboxes.Byusingthesetechniques,wetakeastepbackin
ExplainableIntrusionDetectionSystems(X-IDS).WhileExplain-
explainability.Ifonecannottrustablackboxmodelbecauseit
able Artificial Intelligence (XAI) aims to enhance transparency,
is opaque, how can one trust a black box surrogate explainer?
black box surrogate explainers, such as Local Interpretable
Model-Agnostic Explanation (LIME) and SHapley Additive ex- Onesolutiontothisproblemistheuseofpedagogical,white
Planation(SHAP),aredifficulttotrust.Theblackboxnatureof box explanations for neural networks. Pedagogical algorithms
thesesurrogateexplainersmakestheprocessbehindexplanation take a similar approach to black box surrogate explainers.
generation opaque and difficult to understand. To avoid this
They use the neural network inputs and outputs to create
problem, one can use transparent white box algorithms such
an approximate model [5], [6]. A popular technique is to
as Rule Extraction (RE). There are three types of RE algo-
rithms: pedagogical, decompositional, and eclectic. Pedagogical train Decision Trees (DT) as a surrogate model. Pedagogical
methods offer fast but untrustworthy white-box explanations, approaches have the benefit of being fast and scalable. This
whiledecompositionalREprovidestrustworthyexplanationswith method, however, is also lacking with regard to trust. Since
poor scalability. This work explores eclectic rule extraction,
pedagogical methodsdo notuse theblack box neuralnetwork
which strikes a balance between scalability and trustworthiness.
weights, they cannot create a trustworthy surrogate model.
By combining techniques from pedagogical and decompositional
approaches, eclectic rule extraction leverages the advantages of DecompositionalRuleExtraction(RE)canalleviatethisissue.
both, while mitigating some of their drawbacks. The proposed By training DTs using the weights from each layer [5], [6],
Hybrid X-IDS architecture features eclectic RE as a white box wecancreatetrustworthyrulesforblackboxneuralnetworks.
surrogateexplainerforblackboxDeepNeuralNetworks(DNN).
While decompositional RE gains in trustability, it loses in
The presented eclectic RE algorithm extracts human-readable
scalability. A major issue with this type of algorithm is its
rulesfromhiddenlayers,facilitatingexplainableandtrustworthy
rulesets.EvaluationsonUNSW-NB15andCIC-IDS-2017datasets exponential scaling due to its need to stitch each layer’s rules
demonstrate the algorithm’s ability to generate rulesets with together from input to output.
99.9% accuracy, mimicking DNN outputs. The contributions of Anotheroptionistouseeclecticruleextraction.EclecticRE
thisworkincludethehybridX-IDSarchitecture,theeclecticrule
uses techniques from both pedagogical and decompositional
extraction algorithm applicable to intrusion detection datasets,
algorithms [6]. Eclectic algorithms offer a middle ground
and a thorough analysis of performance and explainability,
demonstrating the trade-offs involved in rule extraction speed between the scalability of pedagogical techniques and the
and accuracy. trustworthiness of decompositional techniques. It trains one
or more DTs for each hidden layer which are used to extract
I. INTRODUCTION
rules for a ruleset. Due to this, eclectic rule extraction scales
The ubiquity of black box algorithms and black box sur- polynomially with respect to the number of layers. This
rogate explainers create trust issues for Explainable Intrusion scaling issue can be mitigated using the eclectic algorithm’s
Detection Systems (X-IDS) [1]. Explainable Artificial Intel- customizability.ForlargerDeepNeuralNetworks(DNN),one
ligence (XAI) was created as a means to increase the trans- can extract rules from a subset of layers rather than all layers.
parency of these black box approaches [2]. Historically, white Additionally, the eclectic rule extractor gains the benefit of
boxtechniqueswereusedtocreateexplanationsforblackbox trust from generating rulesets using weights from the black
models.Morerecently,theuseofsurrogateexplainers,suchas boxneuralnetwork.Thismakesitmoretrustworthythanblack
Local Interpretable Model-Agnostic Explanations (LIME) [3] box surrogates and pedagogical approaches.
or SHapley Additive exPlanations (SHAP) [4], have become X-IDS heavily benefits from explainability and trust [7].
more common. These techniques are used to create local and Explainability allows security experts to understand how and
4202
naJ
81
]RC.sc[
1v70201.1042:viXrawhy their IDS is making predictions. Experts can use eclectic A. Intrusion Detection Systems (IDS)
RE as a means to understand their IDS by generating global,
The authors of [8] and [9] define an intrusion as actions
explainable rulesets. Using this information, security experts
thatobtainunauthorizedaccesstoanetworkorsystem.These
areabletomakemodificationstotheirIDSinordertoincrease
intrusions can be categorized by their ability to affect a
its accuracy. Additionally, experts have other tasks that they
system’s Confidentiality, Integrity, and Availability (CIA) [7].
needtoperformtoprotecttheirsystems.Havingmoretrustin
IDS are tasked with defending a network or host system from
the IDS can help experts perform tasks in a more timely and
intrusion, thereby protecting the CIA principles of security. In
confidentmanner.Thisleadstomorereliablenetworkdefense.
the past, intrusion detection methods took a signature-based
In this work, we present a hybrid X-IDS architecture that
approach. The effectiveness of signature-based approaches is
uses white box eclectic RE to generate explanations. The
determined by the software’s ability to establish, and store,
proposed solution is a white box surrogate explainer that
valid malware signatures and check inputs against those sig-
utilizes the DNN’s hidden layers to generate an explainable
natures [10]. This method, however, has difficulty detecting
ruleset. DNN models are trained using the UNSW-NB15 and zero-dayattacks.Thealternativetosignature-basedmethodsis
CIC-IDS-2017 datasets, and explainable rulesets are created
anomaly-baseddetection.Ratherthanrelyingonadatabaseof
usingtheeclecticREalgorithm.WefindthattheREalgorithm
knownsignatures,anomaly-basedmethodsuseMLalgorithms
is able to generate rulesets that mimic the models’ outputs at
tolearnobservedbehaviorsindata.Anybehaviorsthatdeviate
an accuracy of 99.9%. Additionally, the rulesets have similar
from the learned behavior is considered an anomaly [10].
accuracy to the DNN models when compared to the datasets’
Many modern IDS systems and research use anomaly-
ground truth labels.
based detections in order to achieve higher accuracy. These
Major contributions presented in this work are -
approaches can be divided into black box and white box
• A hybrid X-IDS architecture using a black box DNN [7]. Black box algorithms are generally considered difficult to
predictorandawhiteboxsurrogateexplainer.EclecticRE explainandhaveanopaquedecisionprocess[11],[12].These
isusedtogeneratehuman-readablerulesfromthehidden properties make it difficult to discern how and why the model
neurons of a DNN. RE creates a global, explainable links certain inputs with predicted outputs. Examples of black
ruleset that can be used to help users understand how boxIDSusedintheliteratureincludeExtremeGradientBoost
and why their model makes predictions. (XGBoost) [13], Support Vector Machines (SVM) [14], auto-
• An eclectic rule extraction algorithm that can be run on encoder+LSTMneuralnetwork[15],and1-DimensionalCNN
intrusion detection datasets. This algorithm can be run [16]. White box algorithms, on the other hand, are easy to
for both binary and categorical predictions. An eclectic understandandtransparentintheirdecisionprocess.However,
RE algorithm gives the user flexibility when determining theyareoftenlessaccuratethanblackboxmethodsduetothe
how much of the model they would like to explain. This simplicityintheirdesign.ExamplesofwhiteboxIDSareSelf
can increase ruleset extraction speed. Rulesets generated Organizing Maps (SOM) [8] and decision trees [17].
using this algorithm are highly similar to the DNNs’
B. Explainable Artificial Intelligence (XAI)
predictions.
• A performative and explanatory analysis of our architec- DARPA defines explainable systems as models that can
ture using modern datasets. Our model is tested against explain their reasoning to a human, describe the model’s
the CIC-IDS-2017 and UNSW-NB15 datasets. Using strengths and weaknesses, and convey a sense of future
these datasets, we train and test our DNN and create behavior [18]. Using these criteria, explainable models can
accurate rulesets. Rulesets are able to mimic the DNNs’ be created that promote model fairness, privacy, reliability,
outputswithanaccuracyof99.9%.Wediscussthetrade- causality, and trustability [8].
off of speed and performance and detail the rulesets’ Similar to IDS and other AI applications, XAI methods fall
explainability. into black box and white box explanation techniques. Many
This paper is outlined as follows - Section II details related modern XAI-enabled neural networks feature the use of black
works and background information in IDS, XAI, and X-IDS. box explanation modules. Black box explainers such as Local
In Section III, we describe rule extraction techniques and the Interpretable Model-agnostic Explanations (LIME) [3], SHap-
algorithm used for our experiments. Section IV explains our ley Additive exPlanations (SHAP) [4], and Layer-wise Rele-
hybrid X-IDS architecture. Next, Section V we discuss our vance Propagation (LRP) [19]. These methods form associa-
experiment and experimental results. Finally, we detail future tionsbetweeninputdataandpredictedoutputs,typicallyusing
works and conclude the paper in Section VI. feature importance as a means of explanation. Unfortunately,
thesemethodsufferfromalackoftrustability[20].Whitebox
II. RELATEDWORKANDBACKGROUND
explainers can be used to avoid this problem. As mentioned
Here we present various related works and necessary back- previously, their transparent and simple-to-understand nature
groundinformationpertainingtoXAIandX-IDS.Thissection allowsthemtocreatetrustworthyexplanations.Inourprevious
aims to summarize the current state of explainability in IDS- works [8], [20], we utilize self organizing maps, growing
related materials and contextualize the work described in the selforganizingmaps,andgrowinghierarchicalselforganizing
following sections. maps in order to create accurate, highly explainable intrusiondetection systems. Explanations are innate to these models, starting from the output by connecting it to the final hidden
unlikeblackboxneuralnetworks.Inordertoexplainblackbox layer. It then proceeds in reverse by connecting each hidden
neural networks with white box techniques, one must delve layer to the next one, finally connecting the first hidden layer
into hybrid X-IDS. to the model’s input layer. A substitution algorithm is used
to create a ruleset from this chain of rules. Conversely, the
C. Hybrid XAI and X-IDS
pedagogicalapproachmaintainsthemodel’sblackboxnature.
Hybrid X-IDS are systems that use black box models to Itmapsmodelinputstooutputsandusesthismappingtotrain
make predictions and white box models to generate expla- aDT.Notably,thisapproachtypicallytrainsonlyasingletree.
nations. One method found in the literature is to use the Lastly, there is the eclectic approach. Eclectic algorithms use
inputs and outputs of a NN to train a Decision Tree (DT) a mixture of decompositional and pedagogical techniques. In
classifier. Explainer algorithms such as TREEPAN [21] and this type of algorithm, each hidden layer is used to generate
HYPINV [22] can be used to generate DTs from trained its own textual ruleset. These rulesets can be concatenated
NNs. The authors of [2] use a similar approach to these togethertoexplainthewholeofthenetwork.Inthiswork,we
algorithms to create an X-IDS. They train a set of DTs using chose to use an eclectic-type RE algorithm. The customizable
the inputs to the NN. Then, the outputs of the NN are used nature of the algorithm allows us to make design choices that
to generate explanations by finding the best DT. Notably, benefit IDS datasets.
using these methods to explain a NN comes with a significant Eclectic algorithms lead themselves to be more useful for
downside.OneismaintainingtheblackboxnatureoftheNN. intrusion detection and its large datasets. Decompositional
Additionally, the proposed methods are designed using white algorithmssuchasDeepRed[23]haveanexponentialruntime
boxalgorithms,whiletheyareusedasiftheyareblackboxes. complexity[24].Thiscomplexityisduetohowthisalgorithm
SomeapproachesusetheNN’sweightsinordertogenerate substitutes rules from the input layer to the output layer.
DTs or rulesets. DeepRED [23] and ECLAIRE [24] are both These algorithms lack flexibility. One must create a ruleset
extraction algorithms that “open” the black box to generate by linking the output to the input, greatly increasing the
explanations. Both of these algorithms are designed to work number of possible substitutions needed to create a ruleset.
on DNNs and can generate accurate DTs. Rulesets can then This in combination with large IDS datasets can cause ruleset
be extracted from the DTs that can be read by humans. generation time to become unfeasible for intrusion detection.
Since these algorithms utilize the NN’s inputs, weights, and The eclectic algorithm, however, has the ability to mitigate
outputs, one may be able to conclude that these algorithms this problem. Rather than generating rules by connecting all
generatemoretrustworthyexplanations.Theseexplainersalso layers, the eclectic algorithm can use a subset or selection of
work in a white box manner making the explainers them- layers. Additionally, the size of the DTs that are created with
selves trustworthy. The authors of [25] use DeepRED to this method can be adjusted to speed up rule generation.
generateexplanationsforvariousIDSdatasets.Theynotethat
DeepREDrequiresalargeamountofdatatogenerateaccurate Algorithm 1 DNN Rule Extraction
rulesets.ThecreatorsofECLAIRE[24]testtheiragainstmany Input: Dataset (X), Model (M), Decision Tree Algorithm
smaller datasets. They show that their algorithm can generate (DT )
alg
explainablerulesets.However,theirimplementationisnotwell Output: Final Ruleset (R)
suitedforuseonlargeIDSdatasets.Additionally,theseworks BEGIN
compare the rulesets accuracy to the dataset’s labels. DTs 1: R=set()
and rulesets are generated to explain the NN. Our work adds 2: Y′ =M.predict(X)
a metric that compares the model’s outputs to the dataset’s 3: for hidden layer h i in M do
outputs which demonstrates that our extraction algorithm is 4: X′ =h i(X)
able to mimic the DNN’s thought process. 5: hidden dt =DT alg(X′,Y′)
6: Rules hidden =ExtractRules(hidden dt)
III. RULEEXTRACTIONININTRUSIONDETECTION 7: Yˆ =Rules hidden(X′)
Inthissection,wedetaildifferentruleextractiontechniques. 8: input dt =DT alg(X,Yˆ)
Rule extraction, being a white box surrogate technique, has 9: R.add(ExtractRules(input dt))
some useful benefits that make it a good alternative to black 10: end for
box surrogate explainers. 11: return R
END
A. Rule Extraction Techniques
Rule Extraction (RE) algorithms are a family of techniques The pseudo-code for the algorithm we implemented can
usedtogeneratetextualrulesetsfromneuralnetworks.REcan be found in Algorithm 1. The algorithm takes input of a
becategorizedintothreefamilies:decompositional,pedagogi- dataset(X),aDNNmodel(M),andadecisiontreealgorithm
cal, and eclectic [26]. The decompositional approach opens (DT ). It trains two decision trees per layer that we can
alg
up the black box and uses neuron weights and activations extract rules from to generate a ruleset. The algorithm begins
to generate rules. This approach generates rules layer-to-layer byinitializinganemptyset(R)thatwillbeusedtostorefutureTerm Label
FeatureThreshold
sinpkt > 0.48 && sttl > 0.24 && smean <= 0.01 && ct_srv_src > 0.02 && synack <= 0.01 == benign
Rule
Fig. 1: The composition of a rule. A rule consists of terms that are concatenated together to form a rule. Rules can then be
combined in a set to form a ruleset.
rules. We then used the trained model to generate predicted A. Pre Modeling
labels for our dataset (Y′). We then loop through each hidden
The first phase in the architecture is Pre Modeling. Here,
layer (h ). For each hidden layer, we generate a new dataset
i we construct high-quality datasets, from the input data, and
(X′). This dataset is generated by obtaining the hidden values
determine model hyper-parameters. This work uses the CIC-
created by the hidden neurons on each layer. We can then
IDS-2017 [27] and UNSW-NB15 [28] datasets. There are a
use our hidden value dataset (X′) and our predicted labels
number of reasons why we chose these two datasets. First,
(Y′) to train our hidden value decision tree (hidden ). After
dt these datasets use more modern attacks when compared to
the decision tree is trained, we can extract the rules from the
older datasets. CIC-IDS-2017 and UNSW-NB15 were devel-
treeusingadepth-firstsearchapproach(ExtractRules()).Once
oped in 2017 and 2015, respectively. Both of these datasets
we have our rules extracted, we use the rules to generate a
were created to offer ‘up-to-date’ attacks. Although these
list of labels (Yˆ). These labels are used in conjunction with
datasets are six years old, they can be used to give a good
the original dataset to create the final input-to-output decision
impression of how our model will work with real-world data.
tree (input ). The same depth-first search algorithm is used
dt Second,thesedatasetscontainmillionsofsamples.Thelarger
to extract the rules which are then added to the ruleset (R).
datasets can show how the algorithm can be used with real
More information about our specific implementation can be
world data and can demonstrate the algorithms’ scalability.
found in Section IV-C.
Understandingthescalabilityofourmodel,whileexplainable,
B. Rules and Rulesets is a crucial factor for intrusion detection. In addition, we are
abletocreatelargevalidationandtestingdatasetsthatcontain
As discussed earlier, RE algorithms basically take a model
data which may not appear in the training dataset.
and a dataset as an input, and produce a ruleset. Rulesets
Weuseafewtechniquestopreprocessthedatasets.First,we
consist of rules that are used to categorize training or testing
begin by removing samples with empty values. Then we can
samples. Figure 1 demonstrates the composition of rules and
one hot encode the categorical features. Finally, we normalize
rulesets. Rules are a conjunction of terms. A rule returns true
the values. During this phase, we can also choose to feature
if all of the terms return a true value. When a rule returns
engineer the datasets. Lastly, we tune model parameters to
true, a categorical label is given to the sample. As the name
achieve higher accuracies. This includes many common NN
implies, a ruleset is a set of rules. All rules in a ruleset have
options. The number of hidden layer neurons and the number
unique term compositions. Similarly, all terms in a rule are
of training iterations are the primary options we change to
unique. Terms, in short, are nodes (decisions) on a DT. Rules
create an accurate IDS model. The general make-up of the
are paths through the decision tree that are labeled with a leaf
NN can be found in the following section.
node.
IV. SURROGATEEXPLANATIONX-IDSARCHITECTURE B. Modeling
One important goal of X-IDS is to aid users in under- The next phase is Modeling. In this phase, we train the
standing predictions that can help them with certain tasks black box neural network and record various quality and
such as recognition of possible false-positives. To help CSoC performativemetrics.ToconstructtheNN,weuseTensorflow
(Cyber Security Operation Center) security analysts in their [29]. It consists of an input layer, two r hidden layers, and an
tasks and to protect their networks, we propose a hybrid X- output layer. The two hidden layers each contain 64 neurons
IDS architecture that uses a DNN to create predictions and and use the ReLU activation function. The output layer uses
eclecticREtechniquestocreateexplanations.Thearchitecture the Sigmoid activation function for binary classification. To
isdividedintofourphases.First,thedatasetsarepreprocessed optimize the model, we selected the Adam optimizer. After
and model parameters are tuned in the Pre-Modeling phase. themodelhasbeentrained,itcanbetestedforvariousquality
Second, the DNN are trained and various quality metrics are and performative metrics. Table I shows the DNN parameters
recorded. Third, rules are extracted from the model to form we used during training.
rulesets. Fourth, the rulesets are tested for various statistical Quality Metrics: For our experiments, we record many
measures. The architecture diagram for our X-IDS can be traditional quality metrics. These include accuracy, F1-score,
found in Figure 2. precision,recall,FalsePositiveRate(FPR),andFalseNegativePre-Modeling Modeling Rule Extraction Post-Extraction Statistics
Neural Neural
Dataset N Met ow do er lk N Met ow do er lk Dataset Ruleset
Model Tuning and
Feature Engineering Quality Metrics Statistics
Eclectic Rule
P Sa er la em cte iote nr A Fc 1c -Su cra oc rey P Fr PRe Rec c /i Fs a Ni lo l Rn E Ax lgtr oa rc itt hio mn AR cu cl ue rs ae ct
y
Feature Individual
Scaling Performative Metrics Rule Acc
Feature Rule Usage
Selection T Tr ea sin tii nn gg TT ii mm ee Pr Se pd eic et dion Ruleset Count
Fig. 2: Architecture for a surrogate explainer X-IDS. It features four total phases. In the Pre-Modeling phase, the datasets are
feature engineered to be compatible with the neural network, and model parameters can be selected. The model is trained and
tested in the Modeling phase. Here we record important quality and performative metrics. The trained model and dataset can
then be used to extract a ruleset. Lastly, we generate statistics for the ruleset and rules to aid the user in their understanding.
ParameterName ParameterValue C. Rule Extraction
Hidden Layers 2 In this stage, we use the trained model and the training
Hidden Layer
64 dataset to extract rules from the model’s hidden layers. We
Neuron Count
outline the eclectic rule extraction algorithm in Section III.
Output Layer
Neuron Count 2 Rules generated from the hidden layer can be concatenated
Hidden Layer Rectified Linear together to form an explainable ruleset. This ruleset can be
Activation Function Unit (ReLU) used by the user to understand the potential decisions the
Output Layer Softmax model is making when determining if a sample is benign or
Activation Function
malicious.
Bactch Size 64
The rule extraction algorithm depends on the use of a
Training Iterations 100
Early Stopping DecisionTree(DT).WechosetousetheScikit-learnDecision
5
(Validation Loss) Tree Classifier. However, there are other DT classifiers that
are available, such as C5.0, that offer varying functionality
TABLEI:Theparametersusedfortrainingourneuralnetwork
and scalability. Scikit-learn’s DT offers the benefit of speed
model. All layers are fully connected and use the ReLU
and ease of use, which is the reason why we chose their
activation function. The eclectic rule extraction algorithm
implementation. In addition, the DTs come with varying
we implemented requires there to be at least two output
hyper-parametersthatcanbeusedtoalterthetrainingprocess.
neurons.Twooutputneuronscaneffectivelybeusedforbinary
Notably, the ‘max depth’ and ‘max leaves’ hyper-parameters
classification with the softmax activation function.
can be used to limit the size and number of rules generated.
We can modify these parameters to find the optimal trade-
off between speed and ruleset accuracy. Lastly, it is possible
to modify the DT training process by changing the amount of
Rate (FNR). Accuracy compares the number of true positives
datatheytrainon.Themainscalabilityfactorwiththiseclectic
and negatives to the number of false positives and negatives.
RE algorithm is dataset size. One could decide to only use a
Thisgivesageneralideaofhowwellthemodelperformsasa
subset of the original training dataset in order to speed up the
whole.However,itsusemaybemisleadingduetoimbalanced
rule extraction process. This comes with its own downsides,
datasets.TheF1-score,ontheotherhand,accountsforthisby
however. By limiting the amount of data the DTs are trained
using precision and recall to define its score. This helps to
on, one may be leaving out vital information for an accurate
minimize the effects of an imbalanced dataset. The last two
ruleset.
metrics are FPR and FNR. These detail how often the model
There are some additional notes that can be made about
mislabelsanomalousdataasnormalandvice-versa.Theseare
our specific implementation. First, the algorithm is designed
importantmetricsforanIDSasitdenoteshowoftenanattack
to work with multiclass datasets. This means that NNs need
goes unnoticed or a benign user is prevented from using a
to have at least two output neurons. Multiclass classification
service.
can abstractly predict binary classes by using two output
Performative Metrics: There are also performance-based neurons and the softmax activation function. The user will
metrics that are important to note for an IDS. These include need to One Hot Encode their binary class dataset. Secondly,
training, testing, and prediction times. The speed at which an we did not implement multiprocessing. As mentioned later in
IDS can be trained and tested can be vital for a network. Section V, the bottleneck for our implementation is Python’sdefault single thread. Implementing the ability to use more truth accuracy compares the ruleset’s accuracy to the testing
than one CPU core should drastically increase the speed of dataset’s true labels. We define high accuracy for model
the algorithm. prediction accuracy as >99% and ground truth accuracy as
with 1% of the model’s accuracy. Rulesets are only extracted
D. Post-Extraction Statistics
once; however, the rulesets are randomized and tested five
Finally, we can compare the dataset and model predictions times. Randomizing the rulesets before testing shows that
to the ruleset to obtain useful statistical data. Useful informa- the greedy rule selection approach is accurate. Because of
tion includes ruleset accuracy, individual rule accuracy, and this, we also record the standard deviation to demonstrate
rule usage. Using these statistics, we can aid the user in thealgorithm’svolatility.Theexperimentsaimtodemonstrate
understanding the ruleset and model. The first step in this the trade-off of accuracy to speed and to determine potential
process is ruleset evaluation. optimal parameters.
There are some major design decisions that can be made
A. Ruleset Experiments
whenevaluatingaruleset.First,onecanopttotakeacompre-
hensiveorgreedyapproachtoevaluatingtheruleset.Weopted Our ruleset experiments are divided into a few different
to use a greedy rule comparison approach. The difference categories. First, the RE algorithm creates rulesets using
between these approaches is their stopping criteria for each default,unboundedparameters.ThisallowsallDTstogenerate
testingsample.Inthecomprehensiveapproach,eachsampleis asmanylayersandleavesastheyneed.Additionally,wetrain
comparedtoallrules.Sincethereisapotentialforcollision,an the RE algorithm using the full training dataset. Second, we
additionalstepwouldneedtobemadetodeterminewhichrule create rulesets by limiting the number of layers the DTs can
is more accurate. The second method is a greedy approach. generate.Thislowersthetotalnumberandsizeofrules.Third,
Ratherthancompareasampletoallpossiblerules,westopas we limit the number of leaves the DTs can generate. This
soon as we find a matching rule. The benefit of this approach can allow the DTs to grow as many layers as they want but
is that it speeds up the evaluation on average by a factor of 2. reduces the number of total rules that can be created. Fourth,
This is due to the fact that on average a sample should only welimittheREalgorithmtoeachlayer.Sinceourmodelshave
need to be tested against half the rules. This, however, does two hidden layers, we will test how accurate the rulesets are
not change the potential maximum runtime. when they are generated from each layer. Last, we will train
A potential problem with RE is the number of rules it the RE using subsets of the training dataset. Although each
extracts. Some extraction algorithms can extract 10,000 rules. experiment only changes one parameter, optimal parameters
Our algorithm has extracted close to 2400 rules. These large could be a mixture of the above changes.
number of rules are an unreasonable amount for a human to
B. Unbounded Eclectic Rule Extraction
process. To mitigate this, we can assign a usage counter and
accuracytoeachrule.Duringrulesetevaluation,thesestatistics As mentioned, our first experiment used default DT pa-
can be saved in order to assist the user in understanding the rameters and the full training dataset. This allows the DTs
ruleset. Higher used and higher accuracy rules can be used to be trained unbounded. It can generate as many layers and
to understand the general composition of benign or malicious leaves as it needs. Rulesets generated this way were nearly
samples. as accurate as their DNN and had high model prediction
accuracy. However, a major downside to creating rulesets
V. EXPERIMENTALRESULTS
this way is the amount of time needed and the size of the
We evaluate our X-IDS’s eclectic rule extraction technique rules. The UNSW-NB15 model took 1610 seconds to extract
against a trained DNN. The DNN is trained up to 100 epochs and, on average, 723 seconds to test. Testing had a standard
with early stopping. Training ends early when the validation deviation of 78.6 seconds. UNSW had a model prediction
loss does not improve with a patience of 5 epochs. Early accuracy of 99.04% and a ground truth accuracy of 93.6%.
stoppingisusedasameanstospeeduptrainingtimeafterthe CIC-IDS-2017 was extracted in 9504 seconds (2.6 hours) and
model has reached a minimum. In general, the models would testedin6962seconds(1.9hours).Thismodel’stestingspeed
trainanaverageof25to30epochsbeforestopping.Wetrained had a standard deviation of 1511 seconds (.4 hours). It had
the DNN on two datasets: CIC-IDS-2017 and UNSW-NB15. a model prediction accuracy of 99.96% and a ground truth
These datasets are separated into training (60%), validation accuracy of 93.1%. Both datasets’ prediction accuracy and
(20%), and testing (20%) sets. The datasets are preprocessed true label accuracy are within one percentage point, which
for binary classification. However, the label datasets are One is considered acceptable for this study. As one can see, there
Hot Encoded to work with our RE implementation (See isalargedeviationintestingtimes.Thereareseveralpotential
Section IV-C). Our models achieved an accuracy of 93.1% on reasons behind this. First, the ruleset can accurately separate
CIC-IDS-2017 and 93.7% on UNSW-NB15. These accuracies data so that few samples have overlapping rules. This can
are important to show how effective the eclectic rule extractor mean many rules must test many samples before finding their
is. Extracted rulesets are tested for model prediction and match. Additionally, some rules tend to be champions for
ground truth accuracy. Model prediction accuracy compares each label. Depending on where these rules are ordered in the
the model’s predictions to the ruleset’s predictions. Ground rulesetcandrasticallychangethespeedatwhichthealgorithmUNSW-NB15
Num. Ground Truth Model Prediction Average Longest Extraction Testing Testing
Experiment
Rules Accuracy Accuracy Terms Rule Time (s) Time (s) Std (s)
Unbounded 2380 93.6% 99.1% 15.8 30 1610 723 79
2000 Leaves 2421 93.6% 99.0% 15.7 30 1600 805 140
1000 Leaves 1684 93.7% 99.1% 14.7 25 1603 549 95
500 Leaves 946 93.6% 99.0% 13.1 25 993 278 65
100 Leaves 184 93.5% 98.5% 4.2 14 212 51 7
10 Leaves 19 90.9% 94.7% 4.2 5 60 6.6 .5
20 Layers 2278 93.6% 99.0% 20 20 1532 606 98
10 Layers 502 93.4% 98.4% 10 10 430 89 10
5 Layers 38 91.0% 95.1% 5 5 77 11 2
CIC-IDS-2017
Num. Ground Truth Model Prediction Average Longest Extraction Testing Testing
Experiment
Rules Accuracy Accuracy Terms Rule Time (s) Time (s) Std (s)
Unbounded 1686 93.1% 99.9% 14.2 27 9504 6882 1321
2000 Leaves 1815 93.1% 99.9% 14.1 27 9782 6211 566
1000 Leaves 1701 93.1% 99.9% 14.3 27 8969 4950 879
500 Leaves 1000 93.1% 99.9% 12.2 25 7276 3305 719
100 Leaves 188 93.0% 99.8% 8.6 18 1792 554 129
10 Leaves 20 91.5% 97.6% 4.7 6 664 67 11
20 Layers 1675 93.1% 99.9% 13.2 20 4401 4659 1409
10 Layers 601 93.0% 99.6% 9.1 10 2060 1242 301
5 Layers 57 91.0% 97.0% 4.9 5 760 113 30
TABLE II: This table shows the results from the unbounded, leaves, and layers tests. Tests were run on DNN that achieved
an accuracy of 93.7% for UNSW-NB15 and 93.1% for CIC-IDS-2017. The unbounded experiment allows the rule extraction
algorithm to generate trees with unlimited leaves and layers. The leaves and layers experiments limit the number of leaves and
layers the decision tree algorithm is allowed to produce. This speeds the rule extraction up at the cost of accuracy. Accuracy
is measured in two ways: ground truth accuracy and model prediction accuracy. Ground truth accuracy compares the ruleset’s
prediction ability to the testing dataset’s true labels. Model prediction accuracy compares the ruleset’s prediction accuracy to
the model’s predictions.
runs. Lastly, longer rules that are checked before finding the 100 leaves. Likely, this is due to the large training dataset
matching rule will also increase runtime. size.Figures3,demonstratethetrade-offofaccuracytospeed.
We define speedup as the combination of unbounded training
C. Limited Leaves and average testing time divided by the limited experiment’s
The next set of experiments limits the number of leaves training and average testing time. Depending on the datset,
that DTs are allowed to generate. This limits the rulesets in a onecan see5to10 timesspeedupbefore losing 1%accuracy.
few ways. First, there is a maximum amount of rules that are
allowed to be generated. This number is ≤n∗2, where n is
D. Limited Layers
thelimitofleaves.ThisphenomenacanbeseeninII.Forboth
datasets, the 500 leaves and below are only to generate less Table II also shows the results from the limited layers test.
than or equal to the maximum allowed rules. Some of these This limit strictly affects the number of layers and implicitly
experiments do not reach their maximum due to the second restricts the number of rules. These experiments demonstrate
hidden layer creating duplicate rules. Second, although there the ability to speed up the algorithms by limiting ruleset
isnoexplicitlimitonthenumberoflayers,limitingtheleaves creation. The total number of rules for the unbounded and
can limit the number of layers. This effect can be seen in the 20layerexperimentsaresimilar.However,theextractiontime
100and10leaveslimitedexperiments.Weseethataccuracyis isreducedbyafactorof2.Thisislikelyduetothedecreaseof
associatedwiththenumberofrules;however,thereisanupper theaveragelengthandthelongestrule.UNSW-NB15isableto
limit on the number of rules needed for high accuracy. Users maintainhighaccuracyusingthe20layerlimitation.However,
are able to limit the RE algorithm greatly before accuracy it loses 0.6% model prediction accuracy when limited to 10
begins to degrade below our 1% criteria. UNSW-NB15 can layers.CIC-IDS-2017followsasimilartrendasintheprevious
be limited to 500 leaves and still maintain high accuracy. Its experiment. It is able to have high accuracy even with the
speed can be increased further if only 100 leaves are used, more limited parameters. Even with only 5 layers, it is able
but its ability to mimic model output degrades by 0.5%. CIC- to mimic the model’s predictions with an accuracy of 97%.
IDS-2017 is able to maintain high accuracy when limited to Again, its ability to maintain high accuracy when comparedUNSW-NB15
Num. Ground Truth Model Prediction Average Longest Extraction Testing Testing
Experiment
Rules Accuracy Accuracy Terms Rule Time (s) Time (s) Std (s)
Unbounded 2380 93.6% 99.1% 15.8 30 1610 723 79
80% Dataset 2029 93.6% 98.9% 15.6 28 1106 535 44
60% Dataset 1735 93.6% 98.8% 14.9 28 695 589 89
40% Dataset 1377 93.4% 98.6% 14.4 27 358 324 23
20% Dataset 841 93.3% 98.3% 13.2 23 110 281 76
First Hidden 1391 93.6% 99.0% 15.5 29 768 561 93
Second Hidden 1396 93.6% 99.0% 15.5 30 828 471 63
CIC-IDS-2017
Num. Ground Truth Model Prediction Average Longest Extraction Testing Testing
Experiment
Rules Accuracy Accuracy Terms Rule Time (s) Time (s) Std (s)
Unbounded 1686 93.1% 99.9% 14.2 27 9504 6882 1321
80% Dataset 1474 93.1% 99.9% 14.2 25 6964 4388 1599
60% Dataset 1389 93.1% 99.9% 13.5 25 4244 4630 1573
40% Dataset 990 93.1% 99.9% 13.1 24 2416 3208 1348
20% Dataset 826 93.1% 99.9% 12.1 23 877 2295 753
First Hidden 905 93.1% 99.9% 14.1 27 4625 4184 1138
Second Hidden 909 93.1% 99.9% 14.1 27 4837 3509 636
TABLE III: This table shows the results from the unbounded, training data subsets, and hidden layer tests. Tests were run on
DNN that achieved an accuracy of 93.7% for UNSW-NB15 and 93.1% for CIC-IDS-2017. The unbounded experiment allows
the rule extraction algorithm to generate trees with unlimited leaves and layers. The data subset tests limit the amount of
training data used to extract rules. This speeds up the training time linearly but does not necessarily affect testing time. The
hidden layer limitation extracts rules from each hidden layer. Extraction time is cut in half when compared to the unbounded
test. Testing time is also increased due to fewer rules extracted. Accuracy is measured in two ways: ground truth accuracy and
model prediction accuracy. Ground truth accuracy compares the ruleset’s prediction ability to the testing dataset’s true labels.
Model prediction accuracy compares the ruleset’s prediction accuracy to the model’s predictions.
to UNSW-NB15 is likely due to the larger amount of training to cut the extraction time in half using this limitation while
samples. also maintaining high accuracy. We see a similar trend for
CIC-IDS-2017. This raises the question: “Is creating high
E. Training Data Subsets accuracy rulesets, by extracting from only part of the DNN,
Thetrainingdatasubsetexperimentsseektoimprovespeed less explainable?” Unfortunately, there is no definitive answer
bylimitingtheamountoftrainingdatausedtocreaterulesets. to this question. The answer is likely subjective with respect
Generally, we see linear increases in extraction and testing to individual users.
time with respect to dataset size. Here we see that training
G. Explainability Discussion
dataset size is an important factor for model prediction ac-
curacy. Although minor, we see an immediate degradation Due to the size of some of the rulesets, it is important to
of model prediction accuracy for UNSW-NB15. Ground truth discuss the usability of eclectic rule extraction. Additionally,
accuracy is able to maintain high accuracy, but we begin to itisimportanttodiscusstheexplainabilityandtrustworthiness
lose model explainability. On the other hand, CIC-IDS-2017 of certain limited rulesets. Our RE algorithm created as many
is able to maintain high accuracy throughout all the subset as 2400 rules when unbounded. Additionally, the unbounded
experiments. Have in mind that 20% of the CIC-IDS-2017 rulesets generated rules with an average of 15.8 terms and a
dataset is still larger than the UNSW-NB15 dataset. maxof30terms.Thesetwofactscombinetomakeitadifficult
task for humans to parse rulesets. By limiting the algorithm
F. Limited DNN Hidden Layers
in the various ways above, we are able to decrease the size
Thelastsetofexperimentstestshowrulesetsgeneratedfrom of the rules and rulesets. This makes the rulesets easier for
each layer perform. Using this parameter cuts the number of users to parse but potentially lowers the ruleset’s accuracy.
rules in half and greatly increases the algorithm’s speed as With this in mind, we should ask a few questions. First,
the algorithm scales exponentially with respect to the number “does limiting the DTs decrease the ruleset’s explainability
of hidden layers. However, using this, debatably, limits the and trustworthiness?” Second, “is model prediction accuracy
ruleset’s explainability. Rather than explaining the full model, directly related to explainability and trustworthiness?” Third,
one is only explaining part of the model. In our case, we “what methods can users use to understand rulesets?”
are only explaining half of the model. UNSW-NB15 is able The first and second questions are interlinked. The answers
to maintain 99% model prediction accuracy. A user is able to these questions are likely subjective and open to debate.(a)UNSW-NB15layerspeedupcomparedtoaccuracy (b)UNSW-NB15leavesspeedupcomparedtoaccuracy
loss loss
0.000 0.000
0.00u 5nlimited20 20001000 500
10 0.005 100
0.010
0.010
0.015
0.020 0.015
0.025 5 0.020 10
0.030 G Mr oo du en l d P rT eru dt ich t iA oc nc Aur ca cc uy racy G Mr oo du en l d P rT eru dt ich t iA oc nc Aur ca cc uy racy
0.025
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 0.75 1.00 1.25 1.50 1.75 5 10 15 20 25
Speed Up Speed Up
(c)CIC-IDS-2017 layer speed up compared to accuracy loss (d)CIC-IDS-2017 leaves speed up compared to accuracy loss
Fig. 3: These charts compare the speed up versus accuracy loss for the UNSW-NB15 and CIC-IDS-2017 rulesets. Ground
truth accuracy is the rulesets label versus the testing datasets labels. Model prediction accuracy is the rulesets labels versus
the models’ predicted outputs. Figures (a)(b)(d) are split into two charts with different scales. Points on each of the figures
are labeled with their experiment name.
One user may value model prediction accuracy and ground way DT algorithms train to their advantage. Scikit-learn’s DT
truthaccuracysimilarityoverallothermetrics.Thisisbecause mainly focuses on information gain. This means that higher-
they are the only concrete statistics that one can use to leveltermswilltypicallyhavemorevariance.Thesetermswill
compare DNN model and ruleset. Limiting ruleset creation appear in more rules meaning they are more significant than
would only decrease explainability when accuracy begins to other terms and features. Lastly, it may be possible to use an
degrade.Thequestionthenbecomes“howmuchcanaccuracy algorithm to summarize the rulesets. This could be useful on
degrade before this type of user no longer trusts the ruleset?” larger rulesets, but it may run into the issue of explaining a
Anotherusermayvalueinformationasameansofdetermining white-box with a black-box.
trustworthiness. Longer rules and rulesets may seem more
explainable, especially because these typically correlate with
VI. CONCLUSIONANDFUTUREWORKS
higher accuracy.
The third question can have a more concrete answer. Rule- In this paper, we created an X-IDS architecture that uses
sets are able to record how many and how accurate they are eclectic rule extraction to generate explanations for a DNN.
with the testing dataset. Rules can then be sorted by the most Our X-IDS created rulesets that were up to 99.9% accurate
used or the most correct. This is applicable for both ground when compared to our models’ outputs. Our rulesets also had
truth and model prediction accuracy. Users can view the most a similar accuracy to the DNN models when compared to the
used rules and their labels. These rules can be used to form a testing datasets true labels. The experiments run demonstrate
general, global understanding of the DNN model. Users may theeclecticREalgorithm’sscalabilityandcustomizability.By
beabletodeterminewhichfeaturesallowforhigheraccuracy. limiting ourrule extraction algorithm, we cangreatly increase
Using this information, the user may be able to determine itsspeed.However,itsaccuracycanbegintodegradethemore
which features should be removed from the dataset to make thealgorithmislimited.Thisgivestheuserthechoicebetween
for more accurate predictions. Additionally, one can use the accuracy, explainability, and speed.
atleD
ycaruccA
atleD
ycaruccAPotentialfutureworksincludeextendingtheeclecticruleex- [14] ManjiriVKotpalliwarandRakhiWajgi. Classificationofattacksusing
tractionalgorithmtorecurrentneuralnetworksorotherhighly support vector machine (svm) on kddcup’99 ids database. In 2015
FifthInternationalConferenceonCommunicationSystemsandNetwork
accurateAImodels.ForX-IDSarchitecturetobetrusted,both
Technologies,pages987–990.IEEE,2015.
the model and the explainer need to be accurate. Another [15] EarumMushtaq,AneelaZameer,MuhammadUmer,andAsimaAkber
potential future work could involve translating extracted rules Abbasi. Atwo-stageintrusiondetectionsystemwithauto-encoderand
lstms. AppliedSoftComputing,121:108768,2022.
into directly useful firewall rules. Rather than giving the user
[16] Emad Ul Haq Qazi, Abdulrazaq Almorjan, and Tanveer Zia. A one-
a set of rules, the rulesets themselves could be explained by dimensionalconvolutionalneuralnetwork(1d-cnn)baseddeeplearning
creating firewall rules. systemfornetworkintrusiondetection. AppliedSciences,12(16):7986,
2022.
[17] Love Allen Chijioke Ahakonye, Cosmas Ifeanyi Nwakanma, Jae-Min
ACKNOWLEDGEMENT
Lee,andDong-SeongKim.Scadaintrusiondetectionschemeexploiting
the fusion of modified decision tree and chi-square feature selection.
This work by Mississippi State University was financially
InternetofThings,21:100676,2023.
supported by the U.S. Department of Defense (DoD) High [18] David Gunning and David Aha. Darpa’s explainable artificial intelli-
Performance Computing Modernization Program, through gence(xai)program. AImagazine,40(2):44–58,2019.
[19] Gre´goireMontavon,AlexanderBinder,SebastianLapuschkin,Wojciech
the US Army Engineer Research and Development Center
Samek,andKlaus-RobertMu¨ller. Layer-wiserelevancepropagation:an
(ERDC) (#W912HZ-21-C0058) and National Science Foun- overview. ExplainableAI:interpreting,explainingandvisualizingdeep
dationAward#2234515.Theviewsandconclusionscontained learning,pages193–209,2019.
[20] Jesse Ables, Thomas Kirby, Sudip Mittal, Ioana Banicescu, Shahram
hereinarethoseoftheauthorsandshouldnotbeinterpretedas
Rahimi, William Anderson, and Maria Seale. Explainable intrusion
necessarily representing the official policies or endorsements, detectionsystemsusingcompetitivelearningtechniques. arXivpreprint
either expressed or implied, of the U.S. Army ERDC or the arXiv:2303.17387,2023.
[21] Mark Craven and Jude Shavlik. Extracting tree-structured representa-
U.S. DoD.
tions of trained networks. Advances in neural information processing
systems,8,1995.
REFERENCES [22] Emad W Saad and Donald C Wunsch II. Neural network explanation
usinginversion. Neuralnetworks,20(1):78–93,2007.
[1] Krishna Keerthi Chennam, Swapna Mudrakola, V Uma Maheswari, [23] JanRubenZilke,EneldoLozaMenc´ıa,andFrederikJanssen. Deepred–
Rajanikanth Aluvalu, and K Gangadhara Rao. Black box models ruleextractionfromdeepneuralnetworks. InDiscoveryScience:19th
for explainable artificial intelligence. Explainable AI: Foundations, International Conference, DS 2016, Bari, Italy, October 19–21, 2016,
MethodologiesandApplications,232:1,2022. Proceedings19,pages457–473.Springer,2016.
[2] MateuszSzczepan´ski,MichałChoras´,MarekPawlicki,andRafałKozik. [24] MateoEspinosaZarlenga,ZohrehShams,andMatejaJamnik. Efficient
Achievingexplainabilityofintrusiondetectionsystembyhybridoracle- decompositionalruleextractionfordeepneuralnetworks.arXivpreprint
explainerapproach. In2020InternationalJointConferenceonNeural arXiv:2111.12628,2021.
Networks(IJCNN),pages1–8.IEEE,2020. [25] Samah Almutlaq, Abdelouahid Derhab, Mohammad Mehedi Hassan,
[3] MarcoTulioRibeiro,SameerSingh,andCarlosGuestrin. ”whyshould and Kuljeet Kaur. Two-stage intrusion detection system in intelligent
itrustyou?”explainingthepredictionsofanyclassifier. 2016. transportation systems using rule extraction methods from deep neural
[4] Scott M. Lundberg and Su In Lee. A unified approach to interpreting networks. IEEE Transactions on Intelligent Transportation Systems,
modelpredictions. volume2017-December,2017. 2022.
[5] Robert Andrews, Joachim Diederich, and Alan B Tickle. Survey and [26] Tameru Hailesilassie. Rule extraction algorithm for deep neural net-
critique of techniques for extracting rules from trained artificial neural works:Areview. arXivpreprintarXiv:1610.05267,2016.
networks. Knowledge-basedsystems,8(6):373–389,1995. [27] RanjitPanigrahiandSamarjeetBorah.Adetailedanalysisofcicids2017
[6] M Gethsiyal Augasta and T Kathirvalavakumar. Rule extraction from datasetfordesigningintrusiondetectionsystems. InternationalJournal
neural networks—a comparative study. In International Conference ofEngineering&Technology,7:479–482,32018.
onPatternRecognition,InformaticsandMedicalEngineering(PRIME- [28] Nour Moustafa and Jill Slay. Unsw-nb15: a comprehensive data set
2012),pages404–408.IEEE,2012. fornetworkintrusiondetectionsystems(unsw-nb15networkdataset).
[7] SubashNeupane,JesseAbles,WilliamAnderson,SudipMittal,Shahram In 2015 military communications and information systems conference
Rahimi, Ioana Banicescu, and Maria Seale. Explainable intrusion (MilCIS),pages1–6.IEEE,2015.
detectionsystems(x-ids):Asurveyofcurrentmethods,challenges,and [29] Mart´ınAbadi,AshishAgarwal,PaulBarham,EugeneBrevdo,Zhifeng
opportunities. IEEEAccess,10:112392–112415,2022. Chen,CraigCitro,GregS.Corrado,AndyDavis,JeffreyDean,Matthieu
[8] JesseAbles,ThomasKirby,WilliamAnderson,SudipMittal,Shahram Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey
Rahimi, Ioana Banicescu, and Maria Seale. Creating an explainable Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser,
intrusion detection system using self organizing maps. In 2022 IEEE Manjunath Kudlur, Josh Levenberg, Dandelion Mane´, Rajat Monga,
Symposium Series on Computational Intelligence (SSCI), pages 404– Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon
412.IEEE,2022. Shlens,BenoitSteiner,IlyaSutskever,KunalTalwar,PaulTucker,Vin-
[9] DorothyEDenning. Anintrusion-detectionmodel. IEEETransactions centVanhoucke,VijayVasudevan,FernandaVie´gas,OriolVinyals,Pete
onsoftwareengineering,(2):222–232,1987. Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang
[10] AnsamKhraisat,IqbalGondal,PeterVamplew,andJoarderKamruzza- Zheng. TensorFlow: Large-scale machine learning on heterogeneous
man. Survey of intrusion detection systems: techniques, datasets and systems,2015. Softwareavailablefromtensorflow.org.
challenges. Cybersecurity,2(1):1–22,2019.
[11] OctavioLoyola-Gonzalez.Black-boxvs.white-box:Understandingtheir
advantagesandweaknessesfromapracticalpointofview.IEEEaccess,
7:154096–154113,2019.
[12] Riccardo Guidotti, Anna Monreale, Salvatore Ruggieri, Franco Turini,
FoscaGiannotti,andDinoPedreschi. Asurveyofmethodsforexplain-
ing black box models. ACM computing surveys (CSUR), 51(5):1–42,
2018.
[13] Bhoopesh Singh Bhati, Garvit Chugh, Fadi Al-Turjman, and
Nitesh Singh Bhati. An improved ensemble based intrusion detection
techniqueusingxgboost.Transactionsonemergingtelecommunications
technologies,32(6):e4076,2021.