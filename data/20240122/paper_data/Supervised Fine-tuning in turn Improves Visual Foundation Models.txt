Supervised Fine-tuning in turn Improves Visual Foundation Models
XiaohuJiang1,2,YixiaoGe2,3(cid:66),YuyingGe3,
ChunYuan1(cid:66),YingShan2,3,
1ShenzhenInternationalGraduateSchool,TsinghuaUniversity
2ARCLab,TencentPCG 3TencentAILab
jiangxh21@mails.tsinghua.edu.cn,
{yixiaoge,yuyingge,yingsshan}@tencent.com,
yuanc@sz.tsinghua.edu.cn
Abstract (a) Paradigm in NLP (b) ViSFT
Image-text training like CLIP has dominated the pre-
LLMs Vision Foundation Model
training of vision foundation models in recent years. Sub- Pretraining Pretraining
sequent efforts have been made to introduce region-level
visual learning into CLIP’s pretraining but face scalabil-
ity challenges due to the lack of large-scale region-level
datasets. Drawing inspiration from supervised fine-tuning
Supervised Fine-tuning: Supervised Fine-tuning:
(SFT) in natural language processing such as instruction e.g., Instruction Tuning ViSFT
tuning, weexplorethepotentialoffine-grainedSFTinen-
hancing the generation of vision foundation models after
their pretraining. Thus a two-stage method ViSFT is pro-
posedtounleashthefine-grainedknowledgeofvisionfoun-
dation models. In ViSFT, the vision foundation model is Evaluation on Evaluation on
Out-of-Domain Tasks Out-of-Domain Tasks
enhanced by performing visual joint learning on some in-
domaintasksandthentestedonout-of-domainbenchmarks.
With updating using ViSFT on 8 V100 GPUs in less than
2 days, a vision transformer with over 4.4B parameters Figure1.DrawinginspirationfromthetrainingparadigminNLP,
shows improvements across various out-of-domain bench- we perform ViSFT on vision foundation models after their pre-
marksincludingvisionandvision-linguisticscenarios. trainingandsubsequentlyevaluatethemonout-of-domaintasks.
downstreamvisiontasks. However,theseeffortsfacescala-
1.Introduction bilitychallengesduetothelackoflarge-scaleregion-level
datasets.
Trainingofvisionfoundationmodelshaswitnessedsignifi- In the realm of natural language processing, the afore-
cantprogressinrecentyears[5,12,22,31,51,57,66,67]. mentionedchallengeisaddressedbyemployingsupervised
Among these developments, the image-text representation fine-tuning (SFT) following the pretraining of large lan-
learning,exemplifiedbymodelssuchasCLIP[57],hasbe- guage models, such as through instruction tuning [29, 46,
come the mainstream approach for training vision founda- 60, 72, 83]. By generating detailed task descriptions as
tion models, achieving state-of-the-art performance across instructions, the model undergoes SFT to understand and
variousvisionandvision-languagetasks. Furthermore, ef- followtheinstructions. DrawinginspirationfromtheNLP
forts like GLIP [41] and RegionCLIP [82] aim to extend SFT, we investigate the potential of implementing pure
CLIP’scapabilitiesbylearningregion-levelvisualrepresen- VisionSFT(whichwetermViSFT)toenhancethegener-
tations during pretraining, thereby facilitating fine-grained alizationcapabilitiesofvisionfoundationmodelsasshown
inFigure1.
∗ThisworkwasdonewhenXiaohuJiangwasinterningatARCLab,
Ourfindingssuggestthattherepresentationandgeneral-
Tencent PCG. Code shall be released at https://github.com/
TencentARC/ViSFT.(cid:66)Correspondingauthor. ization of the visual transformer within a CLIP model can
1
4202
naJ
81
]VC.sc[
1v22201.1042:viXraindeedbeimprovedfollowingViSFT.Inessence,ViSFTis improvements across various benchmarks in both visual
abletounleashfine-graineddetailswithinthevisualtrans- andvision-linguisticscenarioswithlightweighttraining.
former that may have been overlooked during image-text
pretraining. Wespeculatethatthismethodassiststhevision 2.RelatedWork
transformerinidentifyingamoreoptimalsubspace.
In ViSFT, we incorporate the visual transformer as the Pretraining of Vision Foundation Models has experi-
backbone network connected to the heads of various in- encedconsiderableprogressinrecentyears. Followingthe
domain vision tasks for joint learning. We opt for object- introduction of the Vanilla Vision Transformer (ViT) [12],
level tasks on COCO [42], including detection, segmenta- numerous pretraining paradigms have been explored for
tion,andcaptioning. ResearcherstypicallytrainLoRA[26] vision transformers, including supervised pretraining on
forvarioustasksandthenchoosethecorrespondingLoRA large-scaleimagedatasets[11,65],self-supervisedlearning
weights during inference, meaning that different LoRA strategies [5, 51], masked image modeling techniques [22,
weightsstoretheirowntask-specificknowledge. Similarly, 53], and more. Notably, image-text pretraining meth-
we use LoRA weights to preserve the unleashed informa- ods [31, 57, 78] such as CLIP have emerged as the pre-
tion. AnotherbenefitofLoRAtuningisitslightweightna- dominantapproachfortrainingfoundationalvisionmodels.
ture,whichlowerstrainingcosts. Thismethodleveragesextensiveimage-textdatatopretrain
models,aimingtolearnthecorrespondencebetweenimages
ViSFT differs from previous multi-task training ap-
andtext.
proaches [6, 10, 27, 38, 76, 77], which fine-tune on in-
domaintasktrainingsplitsandthenmaximizeperformance Moreover, efforts like GLIP [41] and RegionCLIP [82]
onvalidationsplits.Ourgoalistoobtainfine-grainedinfor- intendtointroduceregion-levelvisualrepresentationlearn-
mation through the joint learning of in-domain tasks (e.g., ing into CLIP’s pretraining process, thereby enhancing
detection,segmentation),therebydevelopingavisiontrans- the performance of fine-grained downstream vision tasks.
former backbone with superior representation, and then However, these endeavors encounter challenges in scaling
evaluate the model on out-of-domain benchmarks (e.g., upthemodelsizeduetothescarcityoflarge-scaleregion-
OCR, GOI [40]) as illustrated in Figure 2. Since we do level detection and grounding data. As a result, CLIP
notneedtomaximizetheperformanceonin-domaintasks, remains the prevailing paradigm in visual representation
there is no requirement to design intricate task heads for learning,supportedbyextensiveimage-textdatasets.
ViSFT, such as multi-task mechanisms for resolving task Recent EVA-CLIP series [13, 14, 66] achieve state-of-
conflicts[15,37,84],makingViSFTmoreflexible. the-artperformanceonseveralzero-shotbenchmarks. EVA
firstperformsmaskedimagemodelingonscratch-basedvi-
Another challenge lies in ensuring that knowledge
sion transformers to reconstruct the features of a CLIP’s
learned from in-domain tasks can be effectively trans-
vision encoder. Then, the vision encoder of CLIP is re-
ferred to the vision transformer backbone, rather than be-
placed with the trained vision transformers for image-text
ingtrappedintaskheads. Toaddressthis,wedivideViSFT
pretraining. EVAsuccessfullyscalesthevisiontransformer
intotwostages.Inthefirststage,wetrainthecorresponding
toover4.4billionparameters. WhileBLIP-2[39]employs
in-domaintaskheadswhilekeepingthevisiontransformer
a bridge model (q-former) to integrate EVA-CLIP-G with
backbonefrozen. Inthesecondstage, weintroduceLoRA
large language models (LLMs), achieving state-of-the-art
parameters to the vision transformer backbone and freeze
performance on various visual-language benchmarks. Our
thetaskheads,enablingtheknowledgetobetransferredex-
ViSFThasexploredthepotentialoffine-grainedsupervised
clusivelytotheLoRAparameters.
fine-tuning in enhancing the generalization capabilities of
OurexperimentsdemonstratethatbyundergoingViSFT
bothEVA-CLIPandBLIP-2.
updatingon8V100-SXM2-32GBGPUsinlessthan2days,
aCLIPvisiontransformerwithamodelsizeexceeding4.4B Visual-Linguistic Instruction Tuning represents a sim-
exhibits improvements across 6 different benchmarks, in- ple yet effective supervised fine-tuning (SFT) strategy for
cluding vision and vision-linguistic scenarios (despite not enhancingthegeneralizabilityoffoundationalmodels. No-
performingSFTontheCLIP’stextencoder). Ourcontribu- tably, natural language processing (NLP) instruction tun-
tionscanbesummarizedasfollows: ing [29, 46, 60, 72, 83] has achieved promising results in
• We showcase the potential of fine-grained supervised zero-shotlearningbyutilizingasmallnumberofexamples
fine-tuning (SFT) in enhancing the generalization capa- andasetofnaturallanguageinstructionstoguidethemodel
bilitiesofvisionfoundationmodels. in learning new tasks. There are generally two methods
• Atwo-stageViSFTprocessisproposedtoeffectivelyun- for constructing instruction datasets: data integration from
leash the fine-grained knowledge of vision foundation annotated natural language datasets [46, 60] and generat-
models. ing outputs using LLMs [71, 75]. Based on the collected
• Theperformanceofvisualfoundationmodelshasshown IT dataset, a pre-trained model can be directly fine-tuned
2in a fully-supervised manner. Among these techniques, Tasks Annotations Heads
HINT [29] adopts a hypernetwork to convert instructions ObjectDetection boundingboxeswith80objectcategories Detr
into adapter and prefix parameters, which is akin to how InstanceSegmentation per-instancesegmentationmasks Mask2former
ImageCaptioning naturallanguagedescriptionsoftheimages LSTM
ViSFTstoresfine-grainedinformationinLoRAparameters.
PanopticSegmentation fullscenesegmentationwiththingandstuff Mask2former
Besides text-only domains, instruction tuning has been PoseEstimation personinstanceslabeledwithkeypoints VitPose
applied in multimodal domains [3, 17, 43, 74, 81]. MUL-
TIINSTRUCT [74] is a multimodal instruction tuning Table1.AnoverviewoftaskcategoriesandannotationsinCOCO,
dataset comprising 62 diverse tasks in a unified seq-to- alongwiththeirassociatedtaskheadsforimplementation. Anno-
seq format. LLaVA (13B) [43] is a large multimodal tationsexcludedfromourproposedsolutionaredenotedinGray.
modeldevelopedbyconnectingthevisualencoderofCLIP
Model Layers Hiddensize Patchsize MLPsize Heads Params
(400M)[57]withthelanguagedecoderLLaMA(7B)[68].
EVA-ViT-G[66] 40 1408 14 6144 16 1B
GPT-4 is employed to convert image-text pairs into an ap-
EVA-ViT-E[66] 64 1792 14 15360 16 4.4B
propriateinstruction-followingformatforLLaVA’sdataset.
Whiletheabovestudieshaveachievedsuccessintext-only
Table 2. Details of EVA-ViT model variants employed in our
and multimodal domains, the vision-only domain SFT has experiments: EVA-ViT-GandEVA-ViT-E,bothwithover1Bil-
notyetbeenextensivelyexplored. lionparameters,arederivedfromEVA-CLIP-GandEVA-CLIP-E
models,respectively.
Multi-Task Training employs foundation models as the
backbone,coupledwithmultipletask-specificheads. Typi-
cally,multi-tasktraininginvolvesfine-tuningthebackbone notationsenablefine-grainedlearningforeveryimage.
andtask-specificheadsconcurrentlyondownstreamtasks’ FollowingtheablationstudiesinSec4.4,weultimately
training splits and maximizing performance on validation selected object detection, instance segmentation, and im-
splits,whicharein-domain. agecaptioningasthein-domaintasks. Moreover, taskson
There has been extensive development in multi-task COCOofferavarietyofoff-the-shelftaskheads,obviating
trainingacrossvision[21,63,64,79,80],language[20,44, theneedtodevelopnewtaskheads.
45, 59, 61], and multimodal domains [32, 35, 56]. Recent
3.2.ModelDetails
efforts aim to perform multi-task training using a single,
genericmodel[32,38,76,77,84]. However,suchattempts Inthissection,weoutlinetheprocessofconductingViSFT
oftenfacechallengesduetotaskanddomainconflicts,lead- on the vision foundation model as illustrated in Figure 2.
ing to the development of domain alignment methods and The entire model training procedure is divided into two
mechanismstomitigatetaskconflicts. stages. During the first stage, we employ the pre-trained
ViSFT departs from traditional multi-task training ap- vision transformer from an EVA-CLIP model to serve as
proaches by obtaining fine-grained information through the backbone network and freeze it. Detection, segmenta-
joint learning of in-domain tasks while evaluating perfor- tion, and caption heads are then independently connected
mance on out-of-domain tasks. Additionally, rather than forfine-tuning. Thisstepaimstoobtaintaskheadsthatare
tuning LoRA and task heads simultaneously, ViSFT is di- compatiblewiththevisiontransformerfeatures. Inthesec-
vided into two stages. Since there is no need to maxi- ondstage,thevisiontransformerisaugmentedwithLoRA
mize performance on in-domain tasks, ViSFT does not re- weights, and all task heads are connected for fine-tuning.
quiredomainalignmentmethodsortaskconflictalleviation AsidefromtheaddedLoRAweights,othermoduleswillre-
mechanisms, making it more flexible and easier to imple- mainfrozen. Thisapproachensuresthatfine-grainedinfor-
ment. mation obtained through joint learning is directed towards
theLoRAparameters.
3.Method
EVAVisionTransformer.Weselectthevisiontransformer
3.1.TasksandDatasets fromEVA-CLIP[66]asthevisionfoundationmodel,given
its state-of-the-art performance and the architecture that is
ToensurethatViSFTremainsbothsimpleandfine-grained
basically consistent with the vanilla ViT [12]. As demon-
whileeliminatingtheneedtocreatenewdatasets,weopted
strated in Table 2, we conducted experiments using two
to train our model using the COCO [42] dataset. This
modelsizes: EVA-ViT-GandEVA-ViT-E.
datasetprovidesadiverserangeofannotationsforeachim-
age, including bounding boxes, instance-specific segmen- LoRA Update Matrices. For a pre-trained weight matrix
tation masks, natural language descriptions, and panoptic W q/v ∈ Rd×k within the query and value embedding lay-
segmentationmasks(acombinationofinstanceandseman- ers of EVA-ViT, we impose a constraint on their updates
tic segmentation). Additionally, 250k-person instances are byintroducingalow-rankdecomposition: W q/v +∆W =
annotatedwithkeypoints. AsdepictedinTable1,thesean- W +BA, where B ∈ Rd×r and A ∈ Rr×k, and rank
q/v
3(a)Vision Foundation Model Pretraining: CLIP, … (b)Performing ViSFT on In-Domain Tasks (c)Evaluation on Out-of-Domain Tasks
Losses
Texts OCR: Available GOI: apple …
A dog playing
with a sports
Text Encoder ball on the grass
Add & Norm
Contrastive
Loss
Add & Norm Feed Forward
x L
Feed Forward
Add & Norm
Add & Norm x L
Feed Forward Add & Norm + MulG−Head
Self AIenGon
x L Multi-Head P wre et igra hi tn se d LoRA
Self Attention
Add & Norm W!/# △W
X
Multi-Head …
Self Attention
Images
Figure2. Anoverviewofourproposedmethodisasfollows: (a)First, avisionfoundationmodelispretrainedsuchasCLIP-ViT.(b)
Next,weexecuteViSFTtoupdatetheLoRAweightsandretainthefine-grainedinformationthroughjointlearningofin-domaintasks.(c)
Finally,inconjunctionwiththeupdatedLoRAweights,evaluationsonmultipleout-of-domaintasksexhibitconsiderableenhancement.
“OCR”referstotheopticalcharacterrecognitiontask,while“GOI”denotesthegroundedobjectidentificationtask.
r < min(d,k). During the second stage of training, the tionstogroundtruthboxes.
weight matrices W and W are frozen, preventing them
q v Segmentation Head. We utilize Mask2former [9] as the
from receiving gradient updates, while A and B contain
segmentation head. As a unified framework for segmen-
trainableparameters. Forh = W x,theforwardpass
q/v q/v tation tasks, Mask2former is capable of handling both
yields:
instance segmentation and panoptic segmentation tasks,
therebyprovidingconvenienceforexperimentingwithvari-
h =W x+∆Wx=W x+BAx (1)
q/v q/v q/v
oussegmentationannotations.Tofacilitatetheuseofvision
transformers as the backbone, we have modified the input
Detection Head. Among the available detection heads,
featurelevelsofMask2formerto1.
Detr[4]isthefirsttoincorporatetransformers,whichsim-
Mask2formeralsogeneratesafixednumberofqueryem-
plifies the detection head design, eliminates the need for
beddings. The segmentation mask representations are de-
intricatepost-processingtechniquessuchasnon-maximum
rivedfromthedotproductbetweenthedecoder’sfinal-layer
suppression, and supports single-scale feature input from
hidden state of the j-th embedding and a per-pixel feature
visiontransformers. WhileDetrexhibitsslowconvergence,
map:
itisimportanttonotethatwedon’tpursuesuperiorperfor-
manceonthesein-domaintaskheads. Instead, weemploy qmask =Upsample(cid:16) MLP(q )⊙R(cid:0) G(F )+H(Fenc)(cid:1)(cid:17) ,
thesetaskheadsasabridgetorestorefine-grainedinforma- i i 0 1
(2)
tionofthevisiontransformer.
whereG isa1×1convolutionlayerfollowedbyaGroup
Detr generates a fixed number of learnable query em-
Normalization(GN),Hisa1×1convolutionfollowedbya
beddings,whichserveasinputtotheimagedecoder. These
GNandabilinearupsampling,andRisa3×3convolution
queries interact with one another via self-attention and in-
followed by a GN, a ReLU, and a 1×1 convolution. F
teractwithflattenedimagefeaturesthroughcross-attention 0
andFenc representtheper-pixelfeaturemapsproducedby
layers. Subsequently, MLP and linear heads are employed 1
thebackboneandencoder,respectively.
forboundingboxandlabelprediction,respectively.Finally,
a bi-partite matching mechanism is used to assign predic- Captioning Head. Following [73], we employ a classic
4
Visual
Transfomrer
Visual
Transfomrer
Visual
TransfomrerLongShort-TermMemory(LSTM)networkthatgenerates variants (ImageNet-A [25], ImageNet-R [24], ImageNet-
a caption by producing one word at each time step, condi- Sketch[69]),aswellasotherclassificationdatasets[16,23,
tioned on a context vector, the previous hidden state, and 36,62].
thepreviouslygeneratedwords. (4)Image-TextRetrieval: Weexaminethezero-shotre-
trieval performance on COCO [8] and Flickr30K [55] for
i t  σ  E  bothEVA-CLIP-EandBLIP-2,inwhichthevisionencoder


f
o
tt

=
 
σ
σ

 T
D+m+n,nhy
zt
ˆ−t− 11
 (3)
isr (e 5p )la Vce isd ub ay
l
QEV ueA s- tV ioi nT- AE na sn wd eE riV nA g:-V AiT f- tG er,r fie ns ep -e tc ut niv ine gly.
the
g tanh t visual encoder of BLIP-2, we conduct a quantitative eval-
t
uation of the zero-shot visual question answering task on
c =f ⊙c +i ⊙g VQAv2[18],GQA[28],andOK-VQA[49].
t t t−1 t t
(4)
(6) Captioning: Captioning performance on the unseen
h =o ⊙tanh(c )
t t t
NoCapsdataset[1]isalsoevaluated.
Here,i ,f ,o ,g ,andh representtheinput,forget,mem-
t t t t t
ory, output, and hidden states of the LSTM, respectively. 4.2.ImplementationDetails
Thecontextvectordenotedaszˆ∈ RD,capturesthevisual
Duringthefirststageoftraining,Detr[4]servesasthede-
information associated with a specific input location. The
tection head, featuring six encoder layers and six decoder
embedding matrix E ∈ Rm×K is also considered. Let m
layers. Theencoderdimensionis128, thedecoderdimen-
andnrepresenttheembeddingandLSTMdimensionality,
sion is 256, and the MLP dimension is 1024. For the seg-
respectively,whileσ and⊙denotethelogisticsigmoidac-
mentation head, Mask2former [9] consists of six encoder
tivationandelement-wisemultiplication,respectively.
layers and nine decoder layers. The encoder dimension is
Trainable Parameters. The trained parameters comprise 256, the encoder MLP dimension is 512, the decoder di-
twoparts:inthefirststage,theparametersofeachtaskhead mension is 256, and the decoder MLP dimension is 1024.
aretrained,whileinthesecondstage,theweightsofLoRA Both Detr and Mask2former share the following settings:
aretrained.Intermsofparametersizesettings,takingEVA- the number of attention heads is 8, the number of input
ViT-E as an example, the total parameter size of all task queryembeddings is100, the batchsizeis 1 perGPU, the
heads amounts to 36.8M. We set the size of the two parts numberoffeaturelevelsis1,andthelearningrateis5e−5.
to be roughly equal, thus setting the rank of LoRA to 64, Bothmodelsaretrainedfor150kiterations.
resultinginaparametersizeof29.4M.Subsequentablation With respect to the captioning head, we primarily ad-
experiments in sec 4.4 demonstrate that the size of LoRA heretothesettingspresentedin [73]. TheLSTMencoder
parametershasminimalimpactontheresults. and decoder dimensions are both 384, the batch size is 32
per GPU, the learning rate is 4e−4, and the training pro-
4.Experiments ceeds for 100k iterations. All task head training utilizes
theAdamWoptimizer[47],embracesacosinelearningrate
4.1.EvaluationBenchmarks
strategy, and incorporates a warmup of 2k iterations. The
We focus on performance on out-of-domain tasks and trainingforeachtaskheadisexecutedusing8NvidiaVolta
datasetsthatarenotincludedaspartofthesupervisedvision V100-SXM2-32GB GPUs. The training of various task
finetuning, encompassing both visual and visual-linguistic heads can be conducted concurrently, with the first stage
benchmarks: oftrainingrequiringlessthan3daystofinish.
(1)OpticalCharacterRecognition(OCR):Afterfreezing During the second stage of training, we jointly train
thevisiontransformeranditscorrespondingLoRAweights, EVA-ViTonmultipletasks. Ateachiteration,werandomly
wefollowtheapproachin [2]totrainalightweightheadfor select a task to fill a batch of samples. We simply assign
opticalcharacterrecognition.Utilizingthefrozenbackbone a comparable sampling probability for each task (0.4 for
weights, we employ the MJSynth [30] and SynthText [19] captioning, 0.3 for both detection and segmentation). In
datasetsfortrainingandevaluatetheperformanceonacom- our implementation, we employ 8 NVIDIA Volta V100-
bined set of multiple OCR datasets, including IC03 [48], SXM2-32GB GPUs (batch size 1 per GPU for detection
IC13[33],IC15[34],SVTP[54],SVT[70],andIIIT[50]. andsegmentation, batchsize8perGPUforcaptioning)in
(2) Grounded Object Identification: We evaluate the a distributed manner, using PyTorch [52]. To alleviate the
model’s performance on the M3IT dataset [40], which in- CUDA memory pressure, we have enabled optimizer state
volvesclassifyinganobjectspecifiedinanimage. sharding. ItusestheZeROoptimizerstateshardingmethod
(3) Image Classification: We replace EVA-CLIP’s as described in [58]. Additionally, gradient checkpoint-
visual encoder with the fine-tuned EVA-ViT and per- ing[7]isactivated. TheAdamWoptimizerisutilizedwith
form zero-shot classification on ImageNet-1K [11] and its alearningrateof1e−5andawarm-upcosinelearningrate
5Model Params Iters Accuracy M3IT[40]val
Model Params Iters
Top-1Acc Top-5Acc
EVA-ViT-G 1.0B 0k 44.4
EVA-ViT-G 1.0B 5k 46.9(+2.5) EVA-ViT-G 1.0B 0k 52.3 87.3
ViSFT
EVA-ViT-G ViSFT 1.0B 15k 47.6(+3.2) EVA-ViT-G ViSFT 1.0B 5k 52.9(+0.6) 87.5(+0.2)
EVA-ViT-G 1.0B 15k 52.9(+0.6) 87.7(+0.4)
ViSFT
Table3. Evaluationofopticalcharacterrecognitionperformance
before and after Vision SFT implementation. “Accuracy” repre- EVA-ViT-E 4.4B 0k 54.9 88.3
sents the ratio of correct word instances to the total number of EVA-ViT-E ViSFT 4.4B 5k 55.2(+0.3) 88.7(+0.4)
wordinstances(%). “Iters”referstothenumberofiterationsup-
Table4.Performanceofgroundedobjectidentificationundervar-
datedduringthesecondstage.
iousconditions.WereporttheTop-1andTop-5accuracies(%)on
M3IT’svalidationsetwithimprovementsdenotedinbrackets,e.g.,
schedule(using2000warm-upiterations).
(+0.6). “Iters”referstothenumberofiterationsupdatedduring
The training process continues for 50k iterations, with
thesecondstage.
checkpointssavedevery5kiterations. Thesecondstageof
training requires less than 2 days to complete. We denote
the model after 5k iterations as the default ViSFT setting,
asitshowsimprovementonthemajorityofbenchmarks.
4.3.MainResults
Optical Character Recognition (OCR). Optical Charac- EVA-CLIP-E[66] 82.1 94.5 71.6 82.0 65.8 99.3 93.1 67.7 90.5
ter Recognition (OCR) aims to extract textual information EVA-CLIP-E 82.4 94.6 71.7 82.1 67.1 99.4 93.2 67.8 90.6
ViSFT
fromimages,posingafine-grainedandchallengingtaskdue
Table 5. Zero-shot image classification results on ImageNet-1K
tothevariabilityinfonts, colors, sizes, andorientationsof
anditsvariants,aswellasadditionalclassificationdatasets. Top-
the text within images. Consequently, OCR serves as an
1accuracy(%)onvalidationsetsisreported. Resultsexhibiting
effective benchmark to evaluate the capability of a visual
notable improvements are emphasized in Bold. The number of
foundationmodelincapturingthefine-grainedandseman- iterationsupdatedduringthesecondstageinthiscaseis5k.
ticinformationofanimage.
Inlinewiththemethodologyproposedin [2],weimple- fewerparametersandaremorepronetolosingfine-grained
ment a vision transformer as the backbone of our model, informationduringimage-textpretraining.
freezing both the backbone and its corresponding LoRA
weights. Following this, we train a 4-layer lightweight ImageClassification. InTable5,wefurtherexhibittheef-
transformer head specifically designed for the OCR task. fectiveness and robustness of our approach across 9 zero-
To evaluate the effectiveness of our approach, we perform shot image classification benchmarks. We conduct zero-
experiments on a diverse collection of OCR datasets [33, shotclassificationonEVA-CLIP-Ebeforeandaftervisually
34, 48, 50, 54, 70] and report the average accuracy. The supervised fine-tuning, observing improvements across all
resultspresentedinTable3demonstratethatafterapplying 9 datasets. Notable enhancements are evident on datasets
theViSFT,theperformanceofopticalcharacterrecognition consistingofadversarialandunmodifiedexamples,suchas
canbeimprovedbyatleast2.5points,whichindicatesthat ImageNet-A [25] (increasing from 82.1% to 82.4%) and
thevisiontransformereffectivelyregainsfine-grainedinfor- EuroSAT[23](risingfrom65.8%to67.1%),indicatingthat
mation and is able to capture both the intricate details and fine-grainedinformationcanstrengthenthemodel’srobust-
semanticinformationoftheimage. nesstoreal-worldperturbations.
Grounded Object Identification. Grounded Object Iden- Image-Text Retrieval. Table 6 presents the zero-shot im-
tification(GOI)involvesclassifyingaspecifiedobjectinan age and text retrieval results on Flickr30K and COCO.
image using the [CLS] token feature of vision transform- UponimplementingViSFT,EVA-CLIP-Eexhibitsenhance-
ers.Thisfine-grainedtaskwasnotseenduringEVA-CLIP’s mentsinbothtextandimageretrieval, withamoresignif-
pretraining or our ViSFT. After probing the classification icant impact observed in image retrieval tasks. Notably, it
head for 30 epochs on the M3IT dataset, both EVA-ViT-G shows a 1.1% increase in image retrieval performance, as
and EVA-ViT-E exhibit an enhancement ranging from 0.3 assessedbyCOCO’sRecall@5metric. Thisisattributable
to 0.6 points, as depicted in Table 4. The improvement tothemodelisabletobetterunderstandandextractrelevant
is more pronounced for EVA-ViT-G, which is a smaller featuresfromimageswhenpairedwithcorrespondingtexts.
model. These results indicate that ViSFT can bolster the Wefurtherconductedevaluationsonparadigmsbeyond
model’s generalization performance, with more significant EVA-CLIP, such as BLIP-2. Owing to the constraints in
improvements observed in smaller models, which possess resources,wedidnotretrainaq-former. Instead,welever-
6
]52[A-teNegamI ]42[R-teNegamI ]96[S-teNegamI ]11[K1-teNegamI
]32[TASoruE
]63[01-RAFIC
]63[001-RAFIC
]26[BRSTG
]61[101-hcetlaCTextRetrieval ImageRetrieval Model Params Iters CIDEr
Model Iters Flickr30k COCO Flickr30k COCO
BLIP-2ViT-GOPTs[39] 3.8B 0k 100.9
R@5 R@5 R@5 R@5
BLIP-2ViT-GOPTs 3.8B 5k 101.3(+0.4)
ViSFT
EVA-CLIP-E[66] 0k 99.4 87.6 94.3 74.9
BLIP-2ViT-GOPTs 3.8B 15k 102.1(+1.1)
EVA-CLIP-E 5k 99.4 87.7(+0.1) 94.3 75.2(+0.3) ViSFT
ViSFT
EVA-CLIP-E ViSFT 50k 99.5(+0.1) 87.6 94.8(+0.5) 76.0(+1.1) Table 8. NoCaps caption performance. Results are reported us-
BLIP-2ViT-G[66] 0k 99.9 94.2 96.8 84.0 ingtheCIDErmetric,whichmeasuresthesimilaritybetweengen-
BLIP-2ViT-G 5k 99.9 94.3(+0.1) 96.9(+0.1) 84.1(+0.1) eratedcaptionsandground-truthcaptions(highervaluesarebet-
ViSFT
ter). ExperimentsareconductedonBLIP-2ViT-GOPT (des-
2.7B
Table 6. Comparison of image-text retrieval performance across ignatedasOPTs).Q-formerofBLIP-2isnotretrainedduetolim-
varioussettings.ResultsareassessedusingRecall@5(%).Perfor- itedresources. “Iters”referstothenumberofiterationsupdated
manceforbothFlickr30KandCOCOdatasetsarereported,with duringthesecondstage.
evaluations conducted on EVA-CLIP and BLIP-2. Notable im-
provementsarehighlightedinBold. Owingtotheconstraintsin COCO
resources,wedidnotretrainaq-formerofBLIP-2. “Iters”refers Rank TextRetrieval ImageRetrieval
tothenumberofiterationsupdatedduringthesecondstage. R@5 R@5
Model Params Iters VQAv2 OK-VQA GQA r=8 87.7 75.1
r=16 87.7 75.1
BLIP-2ViT-GOPTs[39] 3.8B 0k 51.9 31.5 32.6
r=32 87.8 75.0
BLIP-2ViT-GOPTs 3.8B 5k 52.0 31.5 32.6
ViSFT r=64 87.7 75.2
BLIP-2ViT-GOPTs 3.8B 20k 52.0 31.7 32.7
ViSFT
Table9. AblationanalysisofLoRAwithvaryingranks. Results
BLIP-2ViT-GOPTl[39] 7.8B 0k 55.1 35.4 35.3
arepresentedfortextretrieval(R@5),andimageretrieval(R@5).
BLIP-2ViT-GOPTl 7.8B 5k 55.2 35.5 35.3
ViSFT
BLIP-2ViT-GOPTl 7.8B 20k 55.3 35.7 35.5
ViSFT is able to enhance captioning performance on the unseen
Table7. Zero-shotvisualquestionansweringresults. Metricsin- dataset. ResultsareprovidedinTable8.
cludeaccuracyforVQAv2,OK-VQA,andGQA(%).Evaluations
are conducted on BLIP-2 ViT-G OPT (designated as OPTs) 4.4.AblationStudies
2.7B
andBLIP-2ViT-GOPT (designatedasOPTl).Duetolimited
6.7B
Inthesubsequentsections,weexaminethecriticaldesigns
resources,wedidnotretrainaq-formerofBLIP-2.
of our ViSFT in conjunction with EVA-CLIP-E. Unless
explicitly stated, image-text retrieval performance on the
aged the pre-trained weights of BLIP-2 from its first stage
COCOdatasetisevaluated.
toperformazero-shotevaluation. AsillustratedinTable6,
after implementing fine-grained tuning on BLIP-2’s visual Effects of LoRA Rank. In the rank configuration for
encoder,weobservedphenomenasimilartothoseofEVA- LoRA,asmentionedbefore,weemployedthedefaultvalue
CLIP,whichfurthersubstantiatesourconclusions. of r = 64, which results in comparable parameter sizes
for LoRA and task heads within our experimental setup.
Visual Question Answering. We assessed the zero-shot
Table 9 demonstrates that LoRA exhibits competitive per-
visual question answering performance of BLIP-2 ViT-G
formance across various rank settings. Consequently, we
OPT and BLIP-2 ViT-G OPT using benchmarks
2.7B 6.7B
maintain the original default configuration, and the addi-
such as VQAv2 [18], GQA [28], and OK-VQA [49]. As
tionalcostsincurredcomparedtosmallerranksettingsare
depictedinTable7,themodelsperformingViSFTontheir
negligible.
visualencodereithermaintainorenhancetheirperformance
acrossallthreebenchmarks.Theimprovementisabitmore Training Data Size. Table 10 indicates that utilizing the
pronounced on OK-VQA, suggesting that ViSFT provides fulltrainingdatasetyieldsslightlymorecompetitiveperfor-
benefits for out-of-domain datasets. Moreover, the perfor- mance,suggestingthattheremayberoomforimprovement
mance improvement is slightly more evident when scaling ifwecanleveragemoredataannotatedsimilarlytoCOCO.
the language model from 2.7B to the larger 6.7B version, We defer this exploration to future work, as the impact of
indicatingthatourViSFTcanpreservethevisual-linguistic training data size is marginal, for instance, increasing the
alignmentwhenthelanguagemodelisscaledupevenwith- trainingdatasizefrom25%to100%onlyenhancestheper-
outretrainingtheq-former. formanceby0.1%intheimage-textretrievaltask.
ImageCaptioning.Weevaluatedtheimagecaptioningper- Training Strategies. In the second stage, there are three
formance of our Vision SFT in conjunction with BLIP-2 potential strategies for performing vision fine-tuning. The
ViT-G OPT on the NoCaps dataset, which was not used classic approach involves fine-tuning both the task heads
duringthetrainingphase. OurfindingsindicatethatViSFT and the backbone simultaneously. However, as Table 11
7COCO
DataSize TextRetrieval ImageRetrieval
R@5 R@5
25% 87.6 75.1
50% 87.6 75.1
100% 87.7 75.2
(a)w/oViSFT (b)w/ViSFT
Table 10. Ablation of training data size on image-text retrieval
tasks:K%indicatestheuseofK%oftheavailabletrainingdata. Figure 3. Visualization of [CLS] token’s attention distribution.
Experiments are conducted on the last layer of EVA-ViT-G. At-
COCO tendedimagepatchesarehighlighted.
HeadLR TextRetrieval ImageRetrieval
R@5 R@5
ViTLR×0 87.7 75.2 segmentation on COCO. To analyze the effects of vari-
ViTLR×0.1 87.6 75.1 ous tasks, we conduct experiments by either adding new
ViTLR×1 87.6 75.0 tasks, such as pose estimation, replacing instance segmen-
tationwithpanopticsegmentation,orindependentlyremov-
Table11.Ablationanalysisofemployingvarioustaskheadlearn-
ingeachtaskfromthejoint-trainingtasks. Forposeestima-
ing rates in the second training stage: ”ViT LR ×0” indicates
freezingthetaskheads,”ViTLR×1”denotessimultaneousfine- tion,weemploytheViTPosetaskhead,whichutilizesavi-
tuningofLoRAweightsandtaskheads,and”ViTLR×0.1”rep- siontransformerasthebackboneandrequiresonlyasingle-
resentsfine-tuningtaskheadswithalearningratethatis0.1times scaleinputfeature. Forpanopticsegmentation,whichcom-
smallerthanthelearningrateappliedtoViT’sLoRAweights. binesinstancesegmentationandsemanticsegmentation,we
maintain the use of the mask2former head to ensure a fair
ImageNet-1K COCO
comparison.
Setting Classification TextRetrieval ImageRetrieval
Table 12 demonstrates that adding a new task, such as
Top-1 R@5 R@5
pose estimation, does not yield further performance im-
Default 82.1 87.7 75.2 provements. Thisisreasonable,asnotallimagesinCOCO
w/pose 82.1 87.7 75.1↓ containpersoninstancesthatwouldbenefitfromposekey-
r/panoptic 82.1 87.8↑ 75.1↓ pointannotations. Asimilarphenomenoncanbeobserved
in instruction tuning [72]: not all task clusters benefit the
82.0↓ 87.9↑ 75.2
w/odetection
foundationmodel,andminimalimpactisobservedfromthe
82.0↓ 87.8↑ 75.1↓
w/osegmentation
sentimentanalysiscluster.
82.0↓ 87.8↑ 75.2
w/ocaption
Theresultsforinstancesegmentationandpanopticseg-
mentation are competitive, as semantic annotations are
Table12. Ablationanalysisoftasktypeselection. Evaluationfo-
more coarse-grained than instance annotations. This indi-
cusesonzero-shotimageclassificationandimage-textRetrieval.
Defaultsettingincorporatesobjectdetection, instancesegmenta- catesthatinstanceannotationspossesssufficientgranularity
tion and image captioning. “ ” denotes “with”, “ ” signifies foreffectivelyperformingourViSFT.
w/ w/o
“without”and“ ”represents“instancesegmentationisre- Uponremovinganyofthethreetasks,thezero-shotim-
r/panoptic
placedbypanopticsegmentation”. Arrowsareusedtorepresent ageclassificationperformancedeteriorates,despiteexhibit-
theincreaseordecreaserelativetothedefaultsetting. ing competitive results in text retrieval. This aligns with
observationsfrominstructiontuning[72],emphasizingthe
demonstrates, this method yields suboptimal performance. importance of task diversity for executing supervised fine-
Aspreviouslymentioned,fine-grainedinformationlearned tuning.Whenthenumberoffine-tuningtasksislimited,the
from different annotations can be trapped within the task model’sgenerativecapabilitiesarealsoconstrained.
heads. Consequently,analternativesolutionistominimize
thelearningrateofthetaskheads,forexample,settingitto 4.5.Visualization
1/10 of the backbone’s learning rate. Nonetheless, as ob-
Tofurthersubstantiatetheefficacyofourapproach,wehave
servedinTable11,theperformanceremainsunsatisfactory,
conducted a visualization of ViSFT. The image patches of
suggestingthatfine-grainedinformationisindeedproneto
EVA-ViT-Garereshapedintoa2Dconfigurationfollowing
betrappedinthetaskheads. Therefore, weproposefreez-
the insertion of the [CLS] token, and we visualize the at-
ingthetaskheads,andtheresultsindicatethatthisstrategy
tentiondistributionofthe[CLS]tokenacrossthesepatches.
performsbetter.
As depicted in Figure 3, after applying ViSFT, the [CLS]
Selection of Task Types. In our default configuration, tokennotonlyattendstonearbypatches(highlightedatthe
we adopt object detection, image captioning, and instance topoftheimages)butalsofocusesonmoredistantobjects.
8This suggests that ViSFT assists vision foundation models [10] Michael Crawshaw. Multi-task learning with deep neural
incapturingfine-grainedinformationfromimagepatches. networks:Asurvey.arXivpreprintarXiv:2009.09796,2020.
2
5.Conclusion [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,
andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage
Drawing inspiration from natural language processing, we database. In2009IEEEconferenceoncomputervisionand
explorethepotentialoffine-grainedsupervisedfine-tuning patternrecognition,pages248–255.Ieee,2009. 2,5,6,1
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
(SFT)toenhancethegeneralizationandrepresentationca-
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
pabilitiesofvisionfoundationmodelsafterpretraining. We
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
proposeatwo-stagemethod,termed”ViSFT,”toeffectively
vain Gelly, et al. An image is worth 16x16 words: Trans-
unleashthefine-grainedknowledgeembeddedwithinthese
formers for image recognition at scale. arXiv preprint
models. Throughourlightweighttrainingprocess,theper-
arXiv:2010.11929,2020. 1,2,3
formance of vision foundation models exhibits improve- [13] YuxinFang,QuanSun,XinggangWang,TiejunHuang,Xin-
mentsacrossawiderangeofout-of-domainbenchmarksin longWang, andYueCao. Eva-02: Avisualrepresentation
bothvisualandvision-linguisticscenarios. forneongenesis. arXivpreprintarXiv:2303.11331,2023. 2
[14] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,
References Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue
Cao. Eva:Exploringthelimitsofmaskedvisualrepresenta-
[1] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, tionlearningatscale.InProceedingsoftheIEEE/CVFCon-
RishabhJain,MarkJohnson,DhruvBatra,DeviParikh,Ste- ferenceonComputerVisionandPatternRecognition,pages
fanLee,andPeterAnderson.Nocaps:Novelobjectcaption- 19358–19369,2023. 2
ingatscale. InProceedingsoftheIEEE/CVFinternational [15] William Fedus, Barret Zoph, and Noam Shazeer. Switch
conferenceoncomputervision,pages8948–8957,2019. 5 transformers:Scalingtotrillionparametermodelswithsim-
[2] Rowel Atienza. Vision transformer for fast and efficient pleandefficientsparsity. TheJournalofMachineLearning
scenetextrecognition. InInternationalConferenceonDoc- Research,23(1):5232–5270,2022. 2
ument Analysis and Recognition, pages 319–334. Springer, [16] LiFei-Fei,RobFergus,andPietroPerona. Learninggener-
2021. 5,6 ative visualmodels from fewtraining examples: An incre-
[3] TimBrooks,AleksanderHolynski,andAlexeiAEfros. In- mentalbayesianapproachtestedon101objectcategories.In
structpix2pix:Learningtofollowimageeditinginstructions. 2004conferenceoncomputervisionandpatternrecognition
In Proceedings of the IEEE/CVF Conference on Computer workshop,pages178–178.IEEE,2004. 5,6,1
VisionandPatternRecognition,pages18392–18402,2023. [17] Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang,
3 MiaoZheng,QianZhao,KuikunLiu,WenweiZhang,Ping
Luo, and Kai Chen. Multimodal-gpt: A vision and lan-
[4] NicolasCarion,FranciscoMassa,GabrielSynnaeve,Nicolas
guage model for dialogue with humans. arXiv preprint
Usunier,AlexanderKirillov,andSergeyZagoruyko.End-to-
arXiv:2305.04790,2023. 3
endobjectdetectionwithtransformers. InEuropeanconfer-
[18] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBa-
enceoncomputervision,pages213–229.Springer,2020. 4,
tra,andDeviParikh. Makingthevinvqamatter: Elevating
5
the role of image understanding in visual question answer-
[5] MathildeCaron,HugoTouvron,IshanMisra,Herve´ Je´gou,
ing. In Proceedings of the IEEE conference on computer
JulienMairal,PiotrBojanowski,andArmandJoulin.Emerg-
visionandpatternrecognition,pages6904–6913,2017. 5,
ingpropertiesinself-supervisedvisiontransformers.InPro-
7,1
ceedingsoftheIEEE/CVFinternationalconferenceoncom-
[19] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman.
putervision,pages9650–9660,2021. 1,2
Syntheticdatafortextlocalisationinnaturalimages.InPro-
[6] Rich Caruana. Multitask learning. Machine learning, 28:
ceedingsoftheIEEEconferenceoncomputervisionandpat-
41–75,1997. 2
ternrecognition,pages2315–2324,2016. 5,1
[7] TianqiChen,BingXu,ChiyuanZhang,andCarlosGuestrin. [20] KazumaHashimoto,CaimingXiong,YoshimasaTsuruoka,
Training deep nets with sublinear memory cost. arXiv and Richard Socher. A joint many-task model: Growing
preprintarXiv:1604.06174,2016. 5 a neural network for multiple nlp tasks. arXiv preprint
[8] XinleiChen,HaoFang,Tsung-YiLin,RamakrishnaVedan- arXiv:1611.01587,2016. 3
tam,SaurabhGupta,PiotrDolla´r,andCLawrenceZitnick. [21] KaimingHe,GeorgiaGkioxari,PiotrDolla´r,andRossGir-
Microsoft coco captions: Data collection and evaluation shick.Maskr-cnn.InProceedingsoftheIEEEinternational
server. arXivpreprintarXiv:1504.00325,2015. 5 conferenceoncomputervision,pages2961–2969,2017. 3
[9] BowenCheng,IshanMisra,AlexanderGSchwing,Alexan- [22] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
der Kirillov, and Rohit Girdhar. Masked-attention mask Dolla´r,andRossGirshick.Maskedautoencodersarescalable
transformerforuniversalimagesegmentation. InProceed- visionlearners. InProceedingsoftheIEEE/CVFconference
ings of the IEEE/CVF conference on computer vision and on computer vision and pattern recognition, pages 16000–
patternrecognition,pages1290–1299,2022. 4,5 16009,2022. 1,2
9[23] Patrick Helber, Benjamin Bischke, Andreas Dengel, and reading. In201513thinternationalconferenceondocument
DamianBorth. Eurosat: Anoveldatasetanddeeplearning analysisandrecognition(ICDAR),pages1156–1160.IEEE,
benchmarkforlanduseandlandcoverclassification. IEEE 2015. 5,6
Journal of Selected Topics in Applied Earth Observations [35] Douwe Kiela, Alexis Conneau, Allan Jabri, and Maximil-
andRemoteSensing,12(7):2217–2226,2019. 5,6,1 ianNickel. Learningvisuallygroundedsentencerepresenta-
[24] DanHendrycks, StevenBasart, NormanMu, SauravKada- tions. arXivpreprintarXiv:1707.06320,2017. 3
vath,FrankWang,EvanDorundo,RahulDesai,TylerZhu, [36] AlexKrizhevsky,GeoffreyHinton,etal. Learningmultiple
SamyakParajuli,MikeGuo,etal.Themanyfacesofrobust- layersoffeaturesfromtinyimages. 2009. 5,6
ness:Acriticalanalysisofout-of-distributiongeneralization. [37] DmitryLepikhin,HyoukJoongLee,YuanzhongXu,Dehao
In Proceedings of the IEEE/CVF International Conference Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam
onComputerVision,pages8340–8349,2021. 5,6,1 Shazeer, andZhifengChen. Gshard: Scalinggiantmodels
[25] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein- withconditionalcomputationandautomaticsharding. arXiv
hardt, and Dawn Song. Natural adversarial examples. In preprintarXiv:2006.16668,2020. 2
ProceedingsoftheIEEE/CVFConferenceonComputerVi- [38] HaoLi,JinguoZhu,XiaohuJiang,XizhouZhu,Hongsheng
sionandPatternRecognition,pages15262–15271,2021. 5, Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang,
6,1 WenhaiWang,etal. Uni-perceiverv2: Ageneralistmodel
[26] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen- forlarge-scalevisionandvision-languagetasks.InProceed-
Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen. ingsoftheIEEE/CVFConferenceonComputerVisionand
Lora: Low-rankadaptationoflargelanguagemodels. arXiv PatternRecognition,pages2691–2700,2023. 2,3
preprintarXiv:2106.09685,2021. 2 [39] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
[27] RonghangHuandAmanpreetSingh.Unit:Multimodalmul- Blip-2: Bootstrapping language-image pre-training with
titask learning with a unified transformer. In Proceedings frozen image encoders and large language models. arXiv
oftheIEEE/CVFInternationalConferenceonComputerVi- preprintarXiv:2301.12597,2023. 2,7
sion,pages1439–1449,2021. 2 [40] Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang,
[28] DrewAHudsonandChristopherDManning. Gqa: Anew Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu
dataset for real-world visual reasoning and compositional Sun, et al. M3it: A large-scale dataset towards multi-
questionanswering. InProceedingsoftheIEEE/CVFcon- modal multilingual instruction tuning. arXiv preprint
ference on computer vision and pattern recognition, pages arXiv:2306.04387,2023. 2,5,6,1
6700–6709,2019. 5,7 [41] LiunianHaroldLi,PengchuanZhang,HaotianZhang,Jian-
[29] HamishIvison, AkshitaBhagia, Yizhong Wang, Hannaneh wei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu
Hajishirzi, and Matthew Peters. Hint: Hypernetwork in- Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded
structiontuningforefficientzero-shotgeneralisation. arXiv language-image pre-training. In Proceedings of the
preprintarXiv:2212.10315,2022. 1,2,3 IEEE/CVF Conference on Computer Vision and Pattern
[30] MaxJaderberg,KarenSimonyan,AndreaVedaldi,andAn- Recognition,pages10965–10975,2022. 1,2
drew Zisserman. Synthetic data and artificial neural net- [42] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
works for natural scene text recognition. arXiv preprint PietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence
arXiv:1406.2227,2014. 5,1 Zitnick. Microsoft coco: Common objects in context. In
[31] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh, ComputerVision–ECCV2014: 13thEuropeanConference,
HieuPham, QuocLe, Yun-HsuanSung, ZhenLi, andTom Zurich, Switzerland, September 6-12, 2014, Proceedings,
Duerig. Scaling up visual and vision-language representa- PartV13,pages740–755.Springer,2014. 2,3,1
tion learning with noisy text supervision. In International [43] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.
conferenceonmachinelearning,pages4904–4916.PMLR, Visualinstructiontuning. arXivpreprintarXiv:2304.08485,
2021. 1,2 2023. 3
[32] Lukasz Kaiser, Aidan N Gomez, Noam Shazeer, Ashish [44] Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. Adversar-
Vaswani, Niki Parmar, Llion Jones, and Jakob Uszko- ialmulti-tasklearningfortextclassification. arXivpreprint
reit. One model to learn them all. arXiv preprint arXiv:1704.05742,2017. 3
arXiv:1706.05137,2017. 3 [45] Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng
[33] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Gao. Multi-taskdeepneuralnetworksfornaturallanguage
Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles understanding. arXivpreprintarXiv:1901.11504,2019. 3
Mestre,JoanMas,DavidFernandezMota,JonAlmazanAl- [46] Shayne Longpre, Le Hou, Tu Vu, Albert Webson,
mazan,andLluisPereDeLasHeras.Icdar2013robustread- HyungWonChung,YiTay,DennyZhou,QuocVLe,Barret
ingcompetition. In201312th internationalconference on Zoph,JasonWei,etal. Theflancollection: Designingdata
documentanalysisandrecognition,pages1484–1493.IEEE, andmethodsforeffectiveinstructiontuning. arXivpreprint
2013. 5,6 arXiv:2301.13688,2023. 1,2
[34] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos [47] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
Nicolaou,SumanGhosh,AndrewBagdanov,MasakazuIwa- regularization. arXivpreprintarXiv:1711.05101,2017. 5
mura,JiriMatas,LukasNeumann,VijayRamaseshanChan- [48] SimonMLucas,AlexPanaretos,LuisSosa,AnthonyTang,
drasekhar,ShijianLu,etal.Icdar2015competitiononrobust ShirleyWong,RobertYoung,KazukiAshida,HirokiNagai,
10Masayuki Okamoto, Hiroaki Yamamoto, et al. Icdar 2003 naud Stiegler, Teven Le Scao, Arun Raja, et al. Multi-
robustreadingcompetitions: entries, results, andfuturedi- taskpromptedtrainingenableszero-shottaskgeneralization.
rections. International Journal of Document Analysis and arXivpreprintarXiv:2110.08207,2021. 1,2
Recognition(IJDAR),7:105–122,2005. 5,6,1 [61] AndersSøgaardandYoavGoldberg. Deepmulti-tasklearn-
[49] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and ingwithlowleveltaskssupervisedatlowerlayers. InPro-
Roozbeh Mottaghi. Ok-vqa: A visual question answering ceedingsofthe54thAnnualMeetingoftheAssociationfor
benchmark requiring external knowledge. In Proceedings ComputationalLinguistics(Volume2: ShortPapers),pages
oftheIEEE/cvfconferenceoncomputervisionandpattern 231–235,2016. 3
recognition,pages3195–3204,2019. 5,7,1 [62] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and
[50] AnandMishra,KarteekAlahari,andCVJawahar.Scenetext ChristianIgel. Manvs.computer: Benchmarkingmachine
recognition using higher order language priors. In BMVC- learningalgorithmsfortrafficsignrecognition. Neuralnet-
Britishmachinevisionconference.BMVA,2012. 5,6,1 works,32:323–332,2012. 5,6
[51] Maxime Oquab, Timothe´e Darcet, The´o Moutakanni, Huy [63] TrevorStandley,AmirZamir,DawnChen,LeonidasGuibas,
Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, JitendraMalik,andSilvioSavarese. Whichtasksshouldbe
DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. learned together in multi-task learning? In International
Dinov2:Learningrobustvisualfeatureswithoutsupervision.
ConferenceonMachineLearning,pages9120–9132.PMLR,
arXivpreprintarXiv:2304.07193,2023. 1,2 2020. 3
[64] Gjorgji Strezoski, Nanne van Noord, and Marcel Worring.
[52] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,
Manytasklearningwithtaskrouting. InProceedingsofthe
James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
IEEE/CVF International Conference on Computer Vision,
Lin,NataliaGimelshein,LucaAntiga,etal.Pytorch:Anim-
pages1375–1384,2019. 3
perativestyle, high-performancedeeplearninglibrary. Ad-
[65] ChenSun, AbhinavShrivastava, SaurabhSingh, andAbhi-
vancesinneuralinformationprocessingsystems,32,2019.
navGupta. Revisitingunreasonableeffectivenessofdatain
5
deeplearningera. InProceedingsoftheIEEEinternational
[53] ZhiliangPeng,LiDong,HangboBao,QixiangYe,andFuru
conferenceoncomputervision,pages843–852,2017. 2
Wei.Beitv2:Maskedimagemodelingwithvector-quantized
[66] QuanSun,YuxinFang,LedellWu,XinlongWang,andYue
visualtokenizers.arXivpreprintarXiv:2208.06366,2022.2
Cao.Eva-clip:Improvedtrainingtechniquesforclipatscale.
[54] Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan
arXivpreprintarXiv:2303.15389,2023. 1,2,3,6,7
Tian,andChewLimTan. Recognizingtextwithperspective
[67] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
distortioninnaturalscenes. InProceedingsoftheIEEEin-
Massa,AlexandreSablayrolles,andHerve´ Je´gou. Training
ternationalconferenceoncomputervision,pages569–576,
data-efficient image transformers & distillation through at-
2013. 5,6
tention. In International conference on machine learning,
[55] Bryan A Plummer, Liwei Wang, Chris M Cervantes,
pages10347–10357.PMLR,2021. 1
Juan C Caicedo, Julia Hockenmaier, and Svetlana Lazeb-
[68] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
nik. Flickr30k entities: Collecting region-to-phrase corre-
Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste
spondences for richer image-to-sentence models. In Pro-
Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
ceedingsoftheIEEEinternationalconferenceoncomputer
Llama: Open and efficient foundation language models.
vision,pages2641–2649,2015. 5,1
arXivpreprintarXiv:2302.13971,2023. 3
[56] SubhojeetPramanik,PriyankaAgrawal,andAmanHussain.
[69] Haohan Wang, Songwei Ge, Zachary Lipton, and Eric P
Omninet: Aunifiedarchitectureformulti-modalmulti-task
Xing. Learningrobustglobalrepresentationsbypenalizing
learning. arXivpreprintarXiv:1907.07804,2019. 3
localpredictivepower.AdvancesinNeuralInformationPro-
[57] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya cessingSystems,32,2019. 5,6,1
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, [70] KaiWang,BorisBabenko,andSergeBelongie. End-to-end
AmandaAskell,PamelaMishkin,JackClark,etal.Learning scenetextrecognition. In2011Internationalconferenceon
transferable visual models from natural language supervi- computervision,pages1457–1464.IEEE,2011. 5,6
sion.InInternationalconferenceonmachinelearning,pages
[71] YizhongWang,YeganehKordi,SwaroopMishra,AlisaLiu,
8748–8763.PMLR,2021. 1,2,3
NoahASmith,DanielKhashabi,andHannanehHajishirzi.
[58] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Self-instruct: Aligninglanguagemodelwithselfgenerated
YuxiongHe. Zero: Memoryoptimizationstowardtraining instructions. arXivpreprintarXiv:2212.10560,2022. 2
trillion parameter models. In SC20: International Confer- [72] Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu,
enceforHighPerformanceComputing,Networking,Storage AdamsWeiYu,BrianLester,NanDu,AndrewMDai,and
andAnalysis,pages1–16.IEEE,2020. 5 QuocVLe. Finetunedlanguagemodelsarezero-shotlearn-
[59] Victor Sanh, Thomas Wolf, and Sebastian Ruder. A hier- ers. arXivpreprintarXiv:2109.01652,2021. 1,2,8
archical multi-task approach for learning embeddings from [73] KelvinXu,JimmyBa,RyanKiros,KyunghyunCho,Aaron
semantictasks. InProceedingsoftheAAAIConferenceon Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua
ArtificialIntelligence,pages6949–6956,2019. 3 Bengio. Show, attend and tell: Neural image caption gen-
[60] VictorSanh,AlbertWebson,ColinRaffel,StephenHBach, erationwithvisualattention. InInternationalconferenceon
Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Ar- machinelearning,pages2048–2057.PMLR,2015. 4,5
11[74] ZhiyangXu,YingShen,andLifuHuang. Multiinstruct:Im-
provingmulti-modalzero-shotlearningviainstructiontun-
ing. arXivpreprintarXiv:2212.10773,2022. 3
[75] FXue,ZZheng,andYYou. Instructioninthewild:Auser-
basedinstructiondataset,2023. 2
[76] HanrongYeandDanXu. Invertedpyramidmulti-tasktrans-
formerfordensesceneunderstanding. InEuropeanConfer-
enceonComputerVision,pages514–530.Springer,2022.2,
3
[77] Hanrong Ye and Dan Xu. Taskprompter: Spatial-channel
multi-taskpromptingfordensesceneunderstanding. InThe
EleventhInternationalConferenceonLearningRepresenta-
tions,2022. 2,3
[78] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,Mo-
jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
captionersareimage-textfoundationmodels. arXivpreprint
arXiv:2205.01917,2022. 2
[79] Amir R Zamir, Alexander Sax, William Shen, Leonidas J
Guibas, Jitendra Malik, and Silvio Savarese. Taskonomy:
Disentanglingtasktransferlearning. InProceedingsofthe
IEEE conference on computer vision and pattern recogni-
tion,pages3712–3722,2018. 3
[80] AmirRZamir,AlexanderSax,NikhilCheerla,RohanSuri,
ZhangjieCao,JitendraMalik,andLeonidasJGuibas. Ro-
bustlearningthroughcross-taskconsistency.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPat-
ternRecognition,pages11197–11206,2020. 3
[81] Bo Zhao, Boya Wu, and Tiejun Huang. Svit: Scaling up
visualinstructiontuning. arXivpreprintarXiv:2307.04087,
2023. 3
[82] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chun-
yuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou,
Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: Region-
based language-image pretraining. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages16793–16803,2022. 1,2
[83] ChuntingZhou,PengfeiLiu,PuxinXu,SriniIyer,JiaoSun,
Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu,
et al. Lima: Less is more for alignment. arXiv preprint
arXiv:2305.11206,2023. 1,2
[84] Jinguo Zhu, Xizhou Zhu, Wenhai Wang, Xiaohua Wang,
Hongsheng Li, Xiaogang Wang, and Jifeng Dai. Uni-
perceiver-moe: Learningsparsegeneralistmodelswithcon-
ditionalmoes. AdvancesinNeuralInformationProcessing
Systems,35:2664–2678,2022. 2,3
12Supervised Fine-tuning in turn Improves Visual Foundation Models
Supplementary Material
A.ViSFTProcedure ImageNet-A [25] is subject to the ImageNet-A terms of
use[88].
TheViSFTtrainingprocesscanbedescribedinAlgorithm1
and Algorithm 2, which obtain compatible in-domain task ImageNet-R [24] is subject to the ImageNet-R terms of
headT∗andlearnedLoRAweights∆W∗,respectively. use[89].
n
UponacquiringthelearnedLoRAweights∆W∗,evalu-
ImageNet-Sketch [69] is subject to the ImageNet-Sketch
ationsonout-of-domainbenchmarkscanbeoutlinedinAl- termsofuse[90].
gorithm3.
EuroSAT[23]issubjecttotheEuroSATtermsofuse[86].
Algorithm1Stage1Training Caltech-101 [16] is subject to the Caltech-101 terms of
Require: Training dataset D(x,y); Pretrained vision use[94].
foundationmodelM
IC03[48]issubjecttotheICDAR2003termsofuse[96].
1: Initialize an in-domain task head T n, for n ∈
IIIT[50]issubjecttotheIIIT5k-wordtermsofuse[87].
{1,...,N}andfreezeM
2: fori=1,2,...do ▷Canbeexecutedinparallel MJSynth[30]issubjecttotheMJSynthtermsofuse[97].
3: Extractfeaturef=M(x)forinputx
SynthText[19]issubjecttotheSynthTexttermsofuse[98].
4: MinimizeL n(y,T n(f))onDtoobtainT n∗
5: endfor M3IT[40]issubjecttotheM3ITtermsofuse[91].
COCO[42]issubjecttotheCOCOtermsofuse[85].
Flickr30K[55]issubjecttotheFlickrtermsofuse[95].
Algorithm2Stage2Training
Require: In-domaintaskheadT∗;Pretrainedvisionfoun- VQAv2[18]issubjecttotheVQAv2termsofuse[92].
n
dation model M; Sampling probability α , n ∈
n OK-VQA[49]issubjecttotheOK-VQAtermsofuse[93].
{1,...,N}
1: Initialize LoRA weights ∆W, freeze M and T∗, n ∈
n
{1,...,N}
2: fori=1,2,...do
3: Selectanin-domaintaskT n∗accordingtoP(α n)
4: Extractfeaturef′ =M(x;∆W)forinputx
5: MinimizeL′ n(y,T n(f′))onDtoobtain∆W∗
6: endfor
Algorithm3Evaluation
Require: PretrainedvisionfoundationmodelM;Learned
LoRA weights ∆W∗; Out-of-domain benchmark T ;
o
EvaluationdatasetE (x,y),o∈{1,...,O}
o
1: InitializeresultslistR o
2: forxinE o(x)do
3: Extractfeaturef∗ =M(x;∆W∗)forinputx
4: PredictingR o =[R o,T o(f∗)]
5: endfor
6: Accumulateresults: Metric(E o(y),R o)onE o
B.LicensesofDatasets
ImageNet-1k[11]issubjecttotheImageNettermsofuse
[99].
1AppendixReferences
[85] Coco terms & conditions of use. https://
cocodataset.org/#termsofuse.
[86] Eurosat terms & conditions of use. https://github.
com/phelber/EuroSAT/blob/master/LICENSE.
[87] Iiit5k-word terms & conditions of use. https:
//cvit.iiit.ac.in/images/Projects/
SceneTextUnderstanding / IIIT5Kfiles /
README.txt.
[88] Imagenet-a terms & conditions of use. https:
//github.com/hendrycks/natural-adv-
examples/blob/master/LICENSE,.
[89] Imagenet-r terms & conditions of use. https://
github.com/hendrycks/imagenet-r/blob/
master/LICENSE,.
[90] Imagenet-sketch terms & conditions of use. https:
//github.com/HaohanWang/ImageNet-Sketch/
blob/master/LICENSE,.
[91] M3it terms & conditions of use. https:
//huggingface.co/datasets/MMInstruction/
M3IT-80.
[92] Vqav2terms&conditionsofuse. https://visualqa.
org/terms.html.
[93] AllenAI. Ok-vqa terms & conditions of use. https://
allenai.org/terms.
[94] Lab. Caltech. Caltechdata terms & conditions of
use. https://library.caltech.edu/search/
caltechdata#terms.
[95] Inc.Flickr. Flickrterms&conditionsofuse. https://
www.flickr.com/help/terms.
[96] Hideaki Goto. Icdar 2003 terms & conditions of use.
http://www.imglab.org/db/files/README-
ICDAR2003-SceneTrialTrain-GT4.txt.
[97] Oxford. Mjsynth terms & conditions of use. https://
www.robots.ox.ac.uk/˜vgg/data/text/#sec-
synth,.
[98] Oxford. Synthtextterms&conditionsofuse. https://
www.robots.ox.ac.uk/˜vgg/terms/dataset-
group-2-access.html,.
[99] Princeton University and Stanford University. Imagenet
terms&conditionsofuse. https://image-net.org/
download.
2