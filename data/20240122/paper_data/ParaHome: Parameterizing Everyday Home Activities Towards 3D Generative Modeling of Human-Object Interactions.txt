ParaHome: Parameterizing Everyday Home Activities
Towards 3D Generative Modeling of Human-Object Interactions
JeonghwanKim* JisooKim* JeonghyeonNa HanbyulJoo
SeoulNationalUniversity
{roastedpen,jlogkim,prom317,hbjoo}@snu.ac.kr
https://jlogkim.github.io/parahome
Figure1.OurParaHomesystemenablestoparameterizethedetailed3Dmovementsofthehumanbody,hands,anddiverseobjectstowards
thegenerativemodelingofthehuman-objectinteractions
Abstract scenarioswithcorrespondingdescriptionsintexts;(3)in-
cluding articulated objects with multiple parts expressed
Toenablemachinestolearnhowhumansinteractwith withparameterizedarticulations.Buildinguponourdataset,
thephysicalworldinourdailyactivities,itiscrucialtopro- we introduce new research tasks aimed at building a gen-
viderichdatathatencompassesthe3Dmotionofhumansas erativemodelforlearningandsynthesizinghuman-object
wellasthemotionofobjectsinalearnable3Drepresenta- interactionsinareal-worldroomsetting.
tion.Ideally,thisdatashouldbecollectedinanaturalsetup,
capturingtheauthenticdynamic3Dsignalsduringhuman-
objectinteractions.Toaddressthischallenge,weintroduce 1.Introduction
theParaHomesystem,designedtocaptureandparameterize
dynamic3Dmovementsofhumansandobjectswithinacom- Ourdailyroutinesinvolveaseriesofactionswherewe
monhomeenvironment.Oursystemconsistsofamulti-view interact with various objects in various ways. As humans,
setupwith70synchronizedRGBcameras,aswellaswear- weeffortlesslyforeseetheconsequencesofouractions;for
ablemotioncapturedevicesequippedwithanIMU-based instance,pullingtherefrigeratorhandleopensthedoor,and
bodysuitandhandmotioncapturegloves.Byleveragingthe tilting a kettle towards a cup pours water into it. As seen,
ParaHomesystem,wecollectanovellarge-scaledatasetof thereisaclearandrobustcorrelationbetweenouractions
human-objectinteraction.Notably,ourdatasetofferskeyad- andhowtheenvironmentresponds,whichcannotbeeasily
vancementoverexistingdatasetsinthreemainaspects:(1) understoodormimickedbymachinesandrobots.Akeyob-
capturing3Dbodyanddexteroushandmanipulationmotion stacleinmakingmachinestounderstandsuchsophisticated
alongside 3D object movement within a contextual home human-objectinteractionsliesinthescarcityoflarge-scale
environmentduringnaturalactivities;(2)encompassinghu- datasetscapturedinnaturalandcasualsettings,including
man interaction with multiple objects in various episodic the 3D motions of both humans and objects occurring in
causalinteractions.Existingdatasetsprimarilyfocusonlim-
*Equalcontribution itedaspectsofthesechallenges.Earlyapproachescapture
1
4202
naJ
81
]VC.sc[
1v23201.1042:viXrathe human motion without objects [20,30,31,37,42,55]. wealsorepresentdiversearticulatedmotionsofobjectsvia
Morerecentapproachesfocusonhand-objectinteractions object-specificparametricmodels.Ourdatasetcontainsdata
instaticposturesorrelativelysimpleandshortinteractions from30participants,22objects,101scenarios,and440min-
suchasgraspingorpickingup[2,6,16,23,27,58].While utesofsequencesinclocktime.Wedesignthecapturesce-
fewrecentdatasetsintroducescenarioswherehumansper- nariotoreflectvariousnaturalactivitiesthatarecommonly
formhandmanipulationswitharticulatedobjects[13,36], observedinourdailylives,whereeachparticipantperforms
they still consider less natural scenarios by capturing the interactionswithmultipleobjectsintheirownuniquestyles
interactionsinasimpletablesetupandshortatomicactions givenacommonverbalguidanceineachscenario.Allcap-
withasingleobjectateachtime.Notably,noneoftheprior tureddatawillbepubliclyavailable.Ourcontributionsare
datasets comprehensively capture the entire body, dexter- summarizedas:(1)presentinganewhomeactivitycapture
ousfingermotions,and3Dmovementsofdiverseobjects systemcombiningmultiplecamerasandwearablemotion
suchasarticulatedobjects,electronicdevices,andfurnitures capturedevices;(2)collectinganewHOIdatasetcapturing
inaroom-scaleenvironment.Assuch,existinggenerative bothdexteroushumanactionsand3Dmovementsofobjects
modelingmethodsbasedonthesedatasetsalsoshowlimi- inacommonparameterizedspace;(3)introducingnewfu-
tationsintheircapability.Previousmodelseitherprimarily tureresearchtaskstounderstandhuman-objectinteractions
focusonhumanmotionsynthesiswithoutconsideringob- withmultipleobjectsandsequentialactions.
jectinteractions[21,22,49,50],considerrelativelysimple
andatomicHOIbetweenthehumanbodyandrigidobject 2.RelatedWork
motions[9,34,48,59,60,64,67–70]orhumanactioninthe
3D Human Body and Hand Datasets. There has been a
scenewithoutobjectarticulation[5,26,41,65].
longhistoryofworkinanalyzinghumanmotionorbehaviors
Toaddresssuchlimitations,wepresentanovelcapture
viamulti-camerasystems.PioneeredbytheworkofKanade
system,namedParaHome,alongwithalarge-scaledataset
etal.[32],anumberofsystemsweresubsequentlyproposed
includingdiverseandnaturalhuman-objectinteractions.Our
toreconstructhumanbehaviors[19,39,40,47].Markerless-
ParaHomesystemallowsparticipantstofreelyinteractwith
motioncapturesystemhasbeentackledbyproducing3D
multipleobjectsinaroomenvironment,asshowninFig.1
skeletal structures over time similar to the marker-based
andFig.8.Theprimaryobjectiveofoursystemanddatasets
counterparts[3,4,7,8,10,14,15,18,33,51,57,61,62].
istocapturecommonandrealisticactions,includingauthen-
Capturing3Dhandsisoftenmorechallengingthancap-
ticsignalsfromthedetailedanddexteroushumanmotions
turingthe3Dbodymotionsduetotheinherentsevereocclu-
atthefingerlevel,aswellaspreciseobjecttrackingandar-
sionsandsmallscale.Theearlyworktacklescapturing3D
ticulatedmovements.Obtainingsuchsignalsischallenging,
handposesonly[44,56,63,66,71],whichmainlypursuesto
since the system should handle multiple scales (big body
providesupervisionforthehandposeestimationalgorithms
movementsacrosstheroomandsubtlehandmotions)and
fromsingleRGBimage.Duetothechallengesincollecting
severeocclusionsduringinteractions.Toachievethisgoal,
real-worlddata,someapproachesalsoproposesynthetic3D
wedesignourParaHomesystembycombiningamultiview
handdataset,includingSynthHands[45],RendererdHand
camerasystemequippedwith70synchronizedcamerasand
Pose[71]andGanHand[9].
wearablemotioncaptureequipment,anIMU-basedmotion
Human-Object Interaction Dataset. A direction of ap-
capturesuitandmotioncapturegloves.Themulti-camera
proaches pursue to capture relatively natural RGB videos
systemtracksthe3Dmovementsofobjects,capturingboth
of hands and object interaction, including HO-3D [23],
rigid and articulated motions, as well as the global rigid
H2O-3D [24], HOI4D [36], DexYCB [6], FreiHand [72],
transformationsofthehumanbodyandhandsinthecam-
and Assembly101 [54]. To handle severe occlusions dur-
eraspace.Thehumanbodyandhandmotionsarecaptured
inginteractions,oftenmultiviewsystems(from2-8views)
usingwearablemotioncaptureequipmentthatisocclusion-
are used, where either manual 2D annotations from mul-
free.Giventhefactthatspatiallyandtemporallycombining
tiview or optimization approaches are used to reconstruct
heterogeneoussystemsischallenging,weproposemultiple
3D hands and objects. Another line of approaches sacri-
hardwareandalgorithmicsolutionstorobustlyreconstruct
fices the naturalism in the images and uses marker-based
human motions and object movements in a common 3D
motioncapturesystem[25,58],magneticsensors[16],or
coordinatesystem.
motioncapturegloves[11].Theseapproachescanprovide
Leveraging our ParaHome system, we collect a large- more accurate 3D hand poses, but the RGB images may
scale human-object interaction dataset in the home activ- notbeusedintrainingmodelsorevaluations.Alternatively,
ityscenarios.Theresultingdatarepresentsdetailedhuman- inOBMAN[27],asyntheticdatasetforhand-objectinter-
objectinteractionsintheparametricspace,viahumanpose action scenes is also proposed, and there exists a method
parameters and object parameters, enabling new opportu- inbuildingpseudo-GTdatafromin-the-wildimages[72].
nities for human-object interaction studies. In particular, Some datasets also contain 6D object annotations, along
2Figure2.(Center)ReconstructedsceneofParaHomefromtopview.Picturesadjacenttotherenderingweretakenfromthecenterofthe
room,headedtowardsthecorrespondingblackdotsinthescene.(Right)PicturesofRGBcamera,Manusmotioncapturegloves,andXsens
motioncapturesuitwithattachedbodymarkers.
withhandannotations[2,6,13,23,24,27,29,36].Among ahumansubjectattimet,andtheenvironmentstatusS (t)
e
them,onlyafewdatasetscontainarticulatedobjects[13,36]. representsthecurrentstatusofsurroundingobjects.Thehu-
While the early work contains the interaction from a sin- manstatusS (t)={S (t),S (t),S (t)}iscomposedof
p b lh rh
gle hand, more recent approach considers the interaction thebodyS (t),left-handS (t),andright-handparameters
b lh
with both hands [2,24,35,58]. A few approaches, includ- S (t), where each of them can be represented as mocap
rh
ing GRAB [58], ARCTIC [13], BEHAVE [28], Action- outputsviaagloballocationandlocaljointorientations.The
Sense[11],InterCap[29]considerbothhandinteractions environment status is represented as S (t) = {Sj(t)}N ,
e e j=1
andwholebodycaptures.Notably,existingapproachesfocus where Sj(t) is the status of the j-th object, assuming we
e
onsubpartproblems:(1)bycapturing onlybodymotions consider N different object instances. The object status
withoutobjectmotions,(2)bycapturingrelativelyshortand Sj(t) = {lj(t),θj(t),ϕj(t)} is represented by 3D trans-
e e e e
simpleinteractions(grasping,pickingup)withasingletarget lationlj(t)∈R3,3Dorientationθj(t)∈SO(3),andobject
e e
object;(3)inafixedsmallspaceoratablesetup. specificparametersϕj(t).Thelengthofϕj(t)variesbased
e e
Comparedtothepreviousdatasets[1,13,29],ourdataset onthetypeandthedimensionofmovablepartsoftheob-
capturesmorenaturalhuman-objectinteractions,covering: jects(e.g.ϕlaptop(t)andϕdrawer(t)containoneandtwopart
e e
(1)bothhandandbodymotioncaptures;(2)articulatedob- parameterseach).
ject modeling and tracking; (3) interactions with multiple To learn the characteristic correlations between the en-
objects;(4)navigationandmanipulationsinaroomenviron- vironmentS (t)changesandthehumanactionsS (t),pa-
e p
ment;and(5)sequentialcorrelatedactions.Wesummarize rameterizing both in a common spatio-temporal space is
thekeyaspectsofexistingdatasetandoursinsupplementary crucial,whichmotivatesustobuildourParaHomesystem.
material. Onedirectiontowardsgenerativemodelingforhuman-object
interactionistoprobabilisticallymodelthedistributionsof
3.ModelingParameterizedEpisodicHOI possibleconfigurationsofhumansandobjectsinthispara-
metric3Dspace:P(S(t : t+w)),whereweconsiderthe
Toeffectivelymodelhuman-objectinteractions,weem-
humanandobjectmovementsinafixedtimewindowfrom
ploy a parameterized 3D space that captures the nuanced
ttot+w.Intuitively,thisformulationcancapturethelike-
relationshipbetweenhumanmotionsandobjectmovements.
lihood of plausible human and object configurations and
Forthehumanparts,wefocusonthemotionofthewhole
dynamics.Wecanalternativelyformulateitasaprediction
bodyanddexteroushandmovementsastheessentialcom-
problemas:
ponents.Rigidobjectsarerepresentedthrough6DoFrigid
motions(3Dtranslationandorientation).Additionally,we S (t:t+w)=F(S (t:t+w)) (1)
o i
incorporate object-specific dynamics such as opening the
laptoporturningtheknobsofagasstove.Formally,werep- , where the input S (t:t + w) is a subset of full states
i
resentthestatusofahumanandtheenvironmentattimetas S(t:t+w)andS (t:t+w)isthereconstructedunseencues
o
S(t)={S (t),S (t)},whereS (t)isthecurrentstatusof predictedbythemodel.Forinstance,wecanbuildthemodel
p e p
3Figure 3. Marker solutions to track and localize objects and its
parts.
Figure4.Scanned3DmodelofobjectsinParaHomesystem.
toinfer3Dobjectmovementsfromthehumanbodymotions, 4.2.3DArUcoMarkerDesignandPlacement
ortoinferfingermotionsfromdesiredobjectmovements.
Forreliabletrackingofobjectsandtheirarticulatedmove-
Moreover,wecanalsoformulatetheHOIunderstandingas
ments,weattachArUcomarkers[17]toallfacesofthe3D
afuturestatusforecastingtask:
cubespanning6cminspiredby[73].Examplesofourmark-
ers are shown in Fig. 3. The major motivation of our 3D
S (t+1:t+w)=F(S (t−w:t)). (2)
o i markersolutionliesinitsrobusttrackingadvantageduring
complicatedobjectmanipulationscenarios,whereoftenob-
Alltheseformulationscanbeinterestingfutureresearchdi-
jectsareseverlyoccludedbyhandgrasping.Inparticular,we
rectionsasawaytounderstandandlearnspatio-temporal
findattachingmarkersontheobjectsurfaceasinprevious
relationsofhuman-objectinteractionsincasualandnatural
approaches [13] is sub-optimal, often requiring laborious
activitysetups.Crucially,thesestudiesrequirealarge-scale
manualpost-processing.Wequantitativelydemonstratethe
dataset capturing the human-object interactions S(t) in a
strength and robustness of our 3D marker solution in our
naturalsetup.ByleveragingourParaHomesystemandcol-
experiments.
lecteddataset,wepursuetoopenupnewpotentialsforthese
impactfulresearchtasks. 5.DataAcquisition
5.1.ModelingandTracking3DObjects
4.3DParametricHomeCaptureSystem
ComputingTransformsbetweenObjectand3DMarkers.
In contrast to the existing datasets [1,13,23,35,72]
Weobtainhigh-quality3Dmeshscansofallobjectsplaced
that captures only subparts of human-object interactions
inoursystemviaanEinstar3Dscanner[12].Wescaneach
in a limited or small-scale setup, our ParaHome system
objectatleasttwicetoreducetheunscannedareasorholesby
is designed to capture all essential signals. Such signal,
changingtheorientationsoftheobjects(e.g.,up-side-down),
S(t) = {S (t),S (t)}, contains body motion, finger mo-
p e andfusethescannedmeshesviamanualalignments.Objects
tion,and3Dobjectmovementsinaresidentialstudioapart-
arescaledasametricscale(inmeters)tobeconsistentwith
ment.Thisrequiressolvingmultiplehardwareandsoftware
ourcamerasystemspace.Ourscannedobjectsareshownin4.
challenges.
Forbettervisibility,weattachedoneormultiple3DArUco
markers and flat-style markers to each object part. Then,
4.1.HardwareSystemandArchitecture
objecttrackingisperformedbydetectingandtrackingthe
ArUcomarkersineveryframe.Thepositionandorientation
Our ParaHome environment encompasses an area of
12.4m2, as shown in Fig. 2. Capturing 3D movements of ofeachobjectareobtainedasfollows:
thehumanbody,fingers,andobjectsduringinteractionsina
T (t)=T T (t) (3)
obj mar→obj mar
spaciousroomenvironmentischallenging,evenfortheelab-
oratecapturesystems[13,25,58].Inourpaper,wepropose where T is a pre-computed fixed transformation
mar→obj
anewsystembycombiningbothamulti-camerasystemand fromthemarkertotheobject,andT (t)isatransforma-
mar
anIMU-basedwearablemotioncapturesuitandglovesasa tionforthemarkersfromtheobjectcanonicalspacetothe
solution.Tocovertheentirevolumeoftheroomandreduce currentposeinthecamerasystemspaceatcapturetimet.
occlusionissues,weinstall70RGBindustrialcameras.Also T iscomputedmanuallybyselectingseveralcorre-
mar→obj
fortrackinghumanbodyandhandmovements,weutilize spondingpointsbetweenobjectscanandthecaptureinthe
Xsensmotionsuit[43]andManushandgloves[38].Forfur- camerasystemspace.
therdetailsofoursystemsetups,pleasechecksupplementary ModelingObjectArticulations.Sincethewholepartsof
material. anobjectarescannedasasinglechunk,wemanuallysep-
4Figure6.(Left)BeforeBodyCalibration(Right)AfterBodyCali-
bration,Orange:forwardkinematicoutput,Blue:RGBTriangulated
Result
Figure5.Articulationstateof3Dscannedobjects.Bluebarsshow
theobjectspecificparameterssj(t)foreachobjectpart.Assj(t)
e e
changes,correspondingmovablepartsoftheobjectsshowdifferent
articulationstates
measurement. As such, directly transferring the output of
wearablecapturesintothecamerasystemspacecannotfulfill
arate each rigidly moving part and build a parameterized ourgoalofcapturingprecisehand-objectinteractions.Inthis
structureforarticulatedmodels.Weassumeeitherarevolute section,wepresentamethodtospatiallyaligntwosystems.
orslidingjoint.Bothtypesofjointsrequirearticulationaxis
a ∈ R3 and in case it is revolute, pivot point p ∈ R3 AligningWearableMocapinMulti-CameraSystem.To
e e
spatiallyalignthebodymotioncapturewithourmulti-view
additionally.Thus,wedefinethejointstatusofj-thobject
asϕj(t)={τj}n whereτj ={aj ,pj ,sj (t)}.Here, camera system, we need to provide correspondences be-
e i i=1 i e,i e,i e,i tweenthetwosystems.Forthispurpose,weattach3or4
nrepresentsthenumberofpartsandsj (t)∈Rdenotesa
e,i ArUcomarkerstoeachof11near-rigidbodyparts(torso,
relativepartstate(eitherradianforrevolutejointormeterfor
hands,upperarms,lowerarms,upperlegs,lowerlegs),as
slidingjoint)frompartstatusintheobjectcanonicalspace.
showninFig.2andinoursupplementarymaterial.Wede-
ExamplesareshowninFig.5.Seesupplementarymaterial
note the four corner points of the j-th marker attached to
regardingtheprocessofcomputingarticulationaxisaj eand thei-thbodypartasMb ∈R4×3,wherethepositionsare
pivotpointpj foreachobject. i,j
e defined in the local joint coordinate w.r.t the correspond-
Capturing3DObjectMotions.Wetrackthestatusofeach
ingbodypart.Specifically,thegoalofouralignmentpro-
objectattimet,S (t)={l (t),θ (t),ϕ (t)},viaourmulti-
e e e e cessis:(1)toobtainauthenticbodyskeletonconfiguration
viewsystem.The3Drigidtransformation(translationl (t)
e B = {O}, which represents the offsets of the child joints
and3Dorientationθ e(t))iscomputedbyspecifyingtheat- fromtheparentjoints,and(2)bodymarkerlocationsMb.
tachedmarkercornersateachtimet,bytriangulatingNnum-
Giventheseparametersandthejointanglemeasurements
berof2Dmarkerpoints{m i(t)}N i=0into3Dmarkerpoints provided by themocap suit θt ∈ R23×3 at time t, wecan
{M (t)}N .Giventhe3Dmarkercorners{M (t)}N and
i i=0 i i=0 transformthebody-attachedmarkersintotheperson-centric
thecorrespondingcorners{Mˆ i}N i=0 intheobjectcanocial coordinate denoted as Mb (t) via forward kinematics
mocap
space,wecomputeT (t),viaawell-knownKabschal-
mar functionK as:Mb (t)=K (Mb,θt,B).Atthesame
gorithm. Then, the object transformation T (t) can be b mocap b
obj time, the body-attached markers can be reconstructed via
computedasEq.3.
our multi-view system, which is denoted as Mb (t) de-
Theobject-specificdynamicstatusϕ (t)issimilarlycom- cam
e finedinthecamerasystemspace.Then,therigidtransfor-
putedbycomputingthetransformationsfromthe3Dmarkers
mationtotransferthemocapdataintothecamerasystem
attachedtothebaseparttotheonesforthemovableparts.
space can be computed with the marker correspondence:
Tcam(t) = T(Mb (t),Mb (t)), where only visible
5.2.Capturing3DHumanMotions b mocap cam
markers in Mb (t) are considered for the computation.
cam
Leveraging two heterogeneous systems, multi-camera Notethatwecancomputetherigidtransformationaslongas
systemsandwearablemotionsuitsandgloves,ischallenging, wecanseeatleastonebody-attachedmarkerinMb (t),
cam
sincetheydonotshareacommonspatialworldcoordinate. providingrobustnesstothemarkerocclusionsincomputing
Specifically,wearablemotioncapturesystemssufferfrom thegloballocationoftheactor.Weperformsuchtransforma-
the drift issue in localizing the global root position, and tionfortheentireframesofbodyalignmentcapturewhich
morecriticallytheyemployanimperfectassumptionofthe consistsofmotionsthatencompassvariousbodyposes.We
body and hand skeleton scale that differs from the actual setbelowobjectivefunctiontogetoptimizedBandMb:
5whereTk isthehand-to-cameratransformationatk-thtime
h
instance,andK istheforwardkinematicsfunctiontotrans-
h
formthei-thfingertip.Asanadditionalconstraint,wein-
cludeL whichpenalizesthedistancebetweenthewrist
wrist
locationsfromthebodymocapandfromthehandmarker
toenforcehandsarerigidlyconnectedtothebodymocap.
WealsoincludeL topreventthefingertipsfrompenetrat-
pen
ingthesurfaceofthecalibrationcube.Seesupplementary
materialfordetails.
Figure7.(Left)HandCalibrationProtocoland(Right)Beforeand
AfterCalibrationProtocol
5.3.Post-ProcessingToEnhanceQualities
Asanadvantageoftheheterogeneousnatureofthesys-
tems, we can leverage cues from both systems to reduce
T
(cid:88) jittersandhandletrackingfailures.
min λ Lt +λ Lt . (4)
Mb,B b body f foot EnhancingHandTracking.Theglobalhandlocationesti-
t=1
matedbytheattached3Dmarkermaysufferfrommotion
TheL isthemean-squared-errorbetweencorresponding
body jitterortrackingfailuresduetoocclusionsorlowcamera
markersas:
visibility.Asasolutiontoenhancethetrackingquality,we
Lt = (cid:88) (cid:13) (cid:13)Tcam(t)mmocap−mcam(cid:13) (cid:13). alsoleveragethecuesfromthewearablemocap.Specifically,
body b i i
wearegiventwomeasurementsregardingthewrists,from
mb∈Mb ,mcam∈Mb
i mocap i cam hand attached markers and from the body mocap. If both
(5)
measurementsareavailable,weperformaweightedinterpo-
WeuseaconstrainttermL toenforcethefootpartstobe
foot
lationtoreducemotionjitters.Inthecaseoftrackingfailure
closetowardthegroundwithoutpenetrationorfloating.The
ofthehand-attached3Dmarker,werelyonthecuesfrom
accuracyofourbodyalignmentprocessisshowninFig.6,
thebodymotionsuitsinceit’socclusion-free.Wedemon-
showingthealignedmarkersaftertheoptimization.
stratetherobustnessofmethodinourexperiments.Seeour
CalibratingHandMocap.Thehandmocapoutputsfrom
supplementarymaterialfordetails.
theglovesalsosufferfromsimilarissueswhichareunknown
skeletonlengthsofactualhandandlocalizingintothecam- Enhancing Object Tracking. Although we use multiple
era system space. Since a subtle error may cause a large cameraswithour3Dmarkersystem,objecttrackingfailure
deviationinthehandinteractionscene,suchissuesarecriti- may still happen due to severe occlusions during interac-
calinobtaininghighqualitydata.Asasolution,wepresenta tions.Asasolution,weleveragethemocapmeasurement
newhandalignmentprotocolofusingacalibrationstructure. from our mocap system to enhance object tracking quali-
Wespecifytheordered3Dcornerverticesofthestructure, ties.Specifically,wefirstinferwhethercurrentlyanobject
C = {c ∈ R3}6 , which we localize in camera system isclosetowardthehumanactorbycheckingthedistances
i i=1
spaceviatriangulation.Duringthealignmentprotocol,the betweenthehumanjointpositionsandobject’ssurface.Ifan
participanttouchesthecalibrationstructure’sknowncorner objecthasbeenmovedbytheactor,weassumetheobject
locationswiththeirfingertips,fromwhichwecanapproxi- isnear-rigidlyconnectedtotheclosebodyjoints,andapply
matethedesiredlocationsoffingertips.Examplesofhand thetemporaltransformationofthebodyjointstointerpolate
calibrationprotocolsandthehandcalibrationstructureare themissingobjecttrajectoriesfromtheneighboringtimes.
showninFig.7andoursupplementaryvideoaswell. Inpractice,weapplythismethodforbothreducingjitters
Similartothebodycalibrationthegoalofhandcalibration andrecoveringthetrackingfailures.
istoobtaintheauthentichandskeletonconfigurationH=
{Sh ∈ R20×1,Oh ∈ R20×3}andpositionsof3Dmarkers 6.3DHomeActivityDataset
inthelocalhand-centriccoordinateMh ∈R3×4×3,where
Sh and Oh each denotes scales of hand skeleton and per- Leveraging our ParaHome system, we capture a large-
scalehumanobjectinteractiondataset.Thecaptureddatain-
joint skeletal offsets. We perform an optimization via the
cludesthehumanbodyandfingermotions,andthedynamic
followingobjectivefunction:
changesofarticulatedobjectsthatparticipantsinteractwith.
min λ tL tip+λ wL wrist+λ pL pen. (6) Ourdatasetisintendedtocapturethediverseandrealistic
Mh,H
behaviorsofhumansinaroomenvironment,wherehumans
L
tip
= (cid:80) k∈o(cid:13) (cid:13)T hkK hk(H) i−ck j(cid:13) (cid:13)penalizesthedifference can freely move around interacting with multiple objects
betweenafingertipandthecorrespondingcornerofthecal- which could have inter-relationships. Due to the diversity
ibrationcubeck (odenotestheorderofcorrespondences), ofhumanactivities,wefocusonseveralkeysetofactions
i
6AverageTracked DuringManipulation
AverageCameraNumber 8.94 8.59
Reproj.Error[pixel] 1.021 1.016
Ours 4 7 10 20 40
TrackingSuccessRatio 1.0 0.76 0.79 0.86 0.90 0.93
Table1.Evaluationresultsonsystemsettings(Top)AverageRe-
projection error detected in the scene and during manipulation
byhumans.(Bottom)Averagenumberoftrackedobjectratioon
multiplesampledwindows.Numbersintheupperrowrepresent
numberofvirtualpassivemarkersattachedtothesurfaceofthe
targetobject.
Figure8.Renderedscenesfromexamplescenario.(Top)Aperson
7.Evaluations
poursfromkettleanddrinksfromacup.(Middle)Apersonopens
alaptop.(Bottom)Apersonchopsonthecuttingboardusingknife
Inthissection,weevaluatetherobustnessandaccuracy
andpourintothepotafteropeningapotlid.
ofoursysteminvariousways.Fortheevaluation,wecapture
longtestsequences(4-5minutes)includingvarioushuman
wherehumaninteractwithoneormultipleobjectsfordaily objectinteractions.
activities.
7.1.SystemandCaptureEvaluation
6.1.CaptureScenarios ImpactofCameraNumber.Wedemonstratetheadvantage
of using all 70 cameras against the alternative solutions
Thesequenceofactionsthatparticipantsperformiscom-
ofusingfewercameras.Toquantifytheimpactofcamera
posedofsmallatomicactionssuchas“movethelaptopto
number, we count the number of detected marker corners
thetable”,whichinvolvesthemanipulationofoneortwo
by simulating systems with varying numbers of cameras
objects.Atotalof35distinctatomicactionsareperformed
withsubsetsfromtheentirecameraset.AsseeninFig.9
byparticipants.20ofthemarecooking-relatedactionssuch
(TopLeft),thedetectedratiotendstoincreaselinearlywith
as“turnonthegasstove”,andtheremaining15aresmall
the addition of cameras to the subsets. This observation
actionsthatcanoccurinaroomenvironment,suchas“type
impliesthatwetakefulladvantageofoursystemwithout
thelaptop”.
experiencingredundancy.
Eachparticipantperforms2to4scenarios.Asinglesce- Advantageof3DMarkerCube.Wecompareour3Dcube
narioconsistsof12to15ofnon-cookingactions,and9to14 markersolutionagainstanalternativesolutionofattaching
actionsfromcooking-relatedactions.Theseatomicactions markersonthesurfaceoftheobjectbody(e.g.,IRmarkersas
areplacedinsemi-arbitraryorderwithcorrespondingverbal in [13]).TosimulatesuchIRmarkersystem,wesampleaset
instructionfortheactionsthatarerecordedandannotatedto ofpointsontheobjectmeshsurface,assumingthesampled
eachcapturedata.Checkoursupplementarymaterialforthe surface points as virtual markers. We choose a complex
fulllistofactionsandmoredetails. interactionsequenceof4minlong,whereour3Dmarker-
based object tracking is fully successful. Then, we assess
6.2.DatasetStatistics the tracking ratio of virtual surface markers, considering
varyingnumbersofmarkerattachedasshowninTab.1.As
We capture the data from 30 participants (15 females aprotocoltodeterminethetrackingratio,weconsiderthe
and15males).Tothisend,our3Dcommondailymotion actorinthesceneasameshmodelasinFig.1additionally
datasetcontains440minutesofactionsinteractingwith22 attached with a box shaped mesh to represent body torso
objects.Eachscenariothattheparticipantperformsconsists andassumeamarkertrackingfailsifthedetectednumberof
of21to29individualactions.Duetothestoragelimits,we cameraofthemarkerislessthan3.Weconsider300frames
divideonescenariointo2sessionsofcaptures.Durationof of sliding windows, and assume an object is successfully
eachsessionrangesfrom64to280seconds,withanaverage trackedifatleast4markersoftheobjectarefullytracked
of 145 seconds per session. Each participant performs 2 alongthewindowwithoutanytrackingfailure.Finally,we
to 4 scenarios, and total of 101 different scenarios were computetheaveragetrackingsuccessratioineachmarker
captured.Afterdividingeachscenariointoindividualatomic setups,asshowninTab.1.Asseen,attachingmarkersonthe
actions, we captured a total of 2284 atomic actions. For objectsurfacesuffersfromtrackingfailuresmainlybecause
furtherdetails,seesupplementarymaterial. theactorstendtotouchtheobjectduringinteractionsand
7duetotheexistenceofmultipleobjectsinthescene.
3DMarkerTriangulationQuality.Tomeasurethequality
oftrackingobjectsviaArUcomarkers,wereportaverage
reprojectionerroroftriangulatedArUcomarkercornersfor
alltrackedscenesandfortheinteractiontargetparts.Along
witheachreprojectionerror,wealsoreporttheaveragenum-
berofcameras(visibility)usedincornertriangulationfor
eachcase,whenincontactandaveragedinallscenes.Check
Tab.1forthedetails.
TrackingQualityviaAssessingTemporalJitters.Wecom-
parethequalityoftrackingofourtwoheterogeneoussys-
tems, a multi-view system and the IMU-based wearable
mocapsuits.Asawaytoverifythequality,weassessthe
temporalmotionjittersofbothsystem,byplottingthederiva-
Figure9.(TopLeft)Ratioofmarkersbeingdetectedwithrespectto
tiveofaccelerationforacertainintervaloftime.Forthistest,
70camerastosamplednumberofcameras(TopRight)Acceleration
wecaptureadedicatedsequence,interactingwithsmalland
derivativeofobjectandinteractinghandwristjoint(BottomLeft)
movableobjectssuchasacupandakettlebymovingthem
Comparisonofdifferencebetweeenoriginaltranslationandhole-
into various poses in multiple areas. The result is shown filledversions.(BottomRight)Comparisonofdifferencebetweeen
inFig.9(TopRight),wherebothsystemsshowsimilarjit- originaloriginalrotation(in6D)andhole-filledversions
ter levels. Given that we use the commercial expert-level
wearablemotioncapturesystemequippedwithhigh-quality failures.
IMUs, this particular result demonstrates that our object EvaluatingHandTrackingEnhancement.Totestthero-
trackingqualityfromcamerasiscomparabletothewearable bustnessofoursystem,werecoverundetectedmarkersof
capturesystem,enablingsubtleinteractioncaptures. thedroppedframeswithourpostprocessingalgorithm.We
HandAlignmentEvaluation.Toquantifythequalityofour measuredL2distancebetweenhandjointscomputedusing
handalignmentprocedureshowninSec.7.1, weperform recoveredmarkers positionandjoint positionsof original
a validation capture, where a participant touches random outputandgetL2loss9mm.
marker corners attached to the objects in various areas in
8.Discussion
thestudio.Then,wemeasuretheaveragedistancebetween
the fingertip and the contacted marker corner point at the
WepresentaParaHomesystem,specificallydesignedto
contacttiming.TheAveragePositionError(APE)is11mm
capturehumanmotions,fingermovements,andobjectdy-
with86fingertoucheswithvariousfingertips.Considering
namicsin3Dduringnaturalhuman-objectinteractionsina
thepossiblebiascausedduringthetouchduetothefinger
studioapartmentsetting.Ourinnovativesystemincorporates
width(15-20mm),ourresultdemonstratesprecisespatial
multiplehardwareandsoftwaresolutions.Leveragingour
alignmentquality.
system,wecollectanewHOIdataset,whichofferskeyim-
provementsoverexistingdatasets,including:(1)capturing
7.2.PostProcessingValidation
3Dbodyandhandmotionalongside3Dobjectmovement
We validate the performance of our post-processing within a contextual home environment during natural ac-
method.Asawaytoquantifytheperformance,wesimulate tivities;(2)encompassinghumaninteractionwithmultiple
thetrackingfailurecasesbyintentionallydroppingcertain objects;(3)includingarticulatedobjectswithmultipleparts.
intervalsofframesfromsuccessfullytrackedsequences,and Weexpectourdatasetcanprovideseveralnewopportunities
applyingourpost-processingmethodtorecoverthedropped inexploringthegenerativemodelingofHOIinaparame-
frames.Bycomparingtheoriginallytrackedcueswiththe terized3Dspace.Weinvesteffortstoexpandourdatasetby
recoveredcuesfromthepost-processing,wecanassessthe involving more participants with more scenarios. All pro-
qualityofourpost-processingoutputs. cesseddatawillbemadeavailabletothepublic,facilitating
EvaluatingObjectTrackingEnhancement.Weperform furtherresearchinthefield.
ourobject-trackingenhancementprocessusingthecuesfrom There exist limitations in our system, intriguing inter-
handtrackingandcompareitsqualitybyanaiveinterpola- estingfutureresearchdirections.First,theRGBvideosin
tion method based on linear functions (e.g. lerp for trans- our dataset are heavily biased due to the wearable suits
lation and slerp for rotation). The quantitative results are and markers used in the system, and cannot be used in
showninFig.9(BottomLeft),(BottomRight),whereour trainingtheRGBimage-basedmodels.Weplantoenhance
method powered with mocap device observations demon- our system toward a markerless system by eliminating
stratesmuchbetterperformanceinrecoveringthetracking artificialmarkers,oncewecanbuildarobustpriormodel
8for human object interactions learned from our current workforhumanactivitiesusingwearablesensorsinakitchen
dataset.Second,tomakethedatasetmoregeneralizedand environment. NeurIPS,35:13800–13813,2022. 2,3
unbiased,itisnecessarytoincludemorediversefurniture, [12] Einstar,2023. https://www.einstar.com/. 4,15
electronic devices, and items, as well as diverse room [13] ZicongFan,OmidTaheri,DimitriosTzionas,Muhammed
layouts.Towardthisgoal,weplantoreplicateoursystemin Kocabas,ManuelKaufmann,MichaelJBlack,andOtmar
differentroomsettings,withmorediverseobjectsensuring Hilliges. Articulatedobjectsinfree-formhandinteraction.
CVPR,2023. 2,3,4,7,12
bettergeneralizability.
[14] Y.FurukawaandJ.Ponce. Dense3dmotioncapturefrom
synchronizedvideostreams. CVPR,2008. 2
Acknowledgement.ThisworkwassupportedbySamsung
[15] Juergen Gall, Carsten Stoll, Edilson De Aguiar, Christian
Electronics MX Division, NRF grant funded by the Ko-
Theobalt,BodoRosenhahn,andHans-PeterSeidel. Motion
reangovernment(MSIT)(No.2022R1A2C2092724andNo.
captureusingjointskeletontrackingandsurfaceestimation.
RS-2023-00218601),andIITPgrantfundedbytheKorean
CVPR,2009. 2
government(MSIT)(No.2021-0-01343).H.Jooisthecorre-
[16] GuillermoGarcia-Hernando,ShanxinYuan,SeungryulBaek,
spondingauthor.
andTae-KyunKim.First-personhandactionbenchmarkwith
rgb-dvideosand3dhandposeannotations. CVPR,2018. 2,
References 12
[17] SergioGarrido-Jurado,RafaelMun˜oz-Salinas,FranciscoJose´
[1] BharatLalBhatnagar,XianghuiXie,IlyaAPetrov,Cristian
Madrid-Cuevas, and Manuel Jesu´s Mar´ın-Jime´nez. Auto-
Sminchisescu, Christian Theobalt, and Gerard Pons-Moll.
matic generation and detection of highly reliable fiducial
Behave:Datasetandmethodfortrackinghumanobjectinter-
markersunderocclusion. PatternRecognition,47(6):2280–
actions. CVPR,2022. 3,4,12
2292,2014. 4
[2] Samarth Brahmbhatt, Chengcheng Tang, Christopher D
[18] D. Gavrila and LS Davis. Tracking of humans in action:
Twigg,CharlesCKemp,andJamesHays. Contactpose:A
A3-Dmodel-basedapproach. ARPAImageUnderstanding
datasetofgraspswithobjectcontactandhandpose. ECCV,
Workshop,1996. 2
2020. 2,3,12
[19] Markus Gross, Stephan Wu¨rmlin, Martin Naef, Edouard
[3] Christoph Bregler, Jitendra Malik, and Katherine Pullen.
Lamboray,ChristianSpagno,AndreasKunz,EstherKoller-
Twistbasedacquisitionandtrackingofanimalandhuman
Meier, Tomas Svoboda, Luc Van Gool, Silke Lang, Kai
kinematics. IJCV,2004. 2
Strehlke,AndrewVandeMoere,andOliverStaadt. Blue-c:A
[4] ThomasBrox,BodoRosenhahn,JuergenGall,andDaniel spatiallyimmersivedisplayand3dvideoportalfortelepres-
Cremers. Combinedregionandmotion-based3Dtrackingof ence. SIGGRAPH,2003. 2
rigidandarticulatedobjects. TPAMI,2010. 2 [20] RalphGrossandJianboShi.Thecmumotionofbody(mobo)
[5] ZheCao,HangGao,KarttikeyaMangalam,Qi-ZhiCai,Minh database. 2001. 2
Vo,andJitendraMalik. Long-termhumanmotionprediction [21] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,
withscenecontext. ECCV,2020. 2 XingyuLi,andLiCheng. Generatingdiverseandnatural3d
[6] Yu-WeiChao,WeiYang,YuXiang,PavloMolchanov,Ankur humanmotionsfromtext. CVPR,2022. 2
Handa,JonathanTremblay,YashrajSNarang,KarlVanWyk, [22] Ikhsanul Habibie, Daniel Holden, Jonathan Schwarz, Joe
UmarIqbal,StanBirchfield,etal. Dexycb:Abenchmarkfor Yearsley,andTakuKomura. Arecurrentvariationalautoen-
capturinghandgraspingofobjects. CVPR,2021. 2,3,12 coderforhumanmotionsynthesis. BMVC,2017. 2
[7] KongManCheung,SimonBaker,andTakeoKanade. Shape- [23] ShreyasHampali,MahdiRad,MarkusOberweger,andVin-
from-silhouette across time part i: Theory and algorithms. centLepetit. Honnotate:Amethodfor3dannotationofhand
IJCV,2005. 2 andobjectposes. CVPR,2020. 2,3,4,12
[8] StefanoCorazza,LarsMu¨ndermann,EmilianoGambaretto, [24] ShreyasHampali,SayanDebSarkar,MahdiRad,andVincent
GiancarloFerrigno,andThomasP.Andriacchi. Markerless Lepetit. Keypointtransformer:Solvingjointidentification
Motion Capture through Visual Hull, Articulated ICP and inchallenginghandsandobjectinteractionsforaccurate3d
SubjectSpecificModelGeneration. IJCV,2010. 2 poseestimation. CVPR,2022. 2,3,12
[9] EnricCorona,AlbertPumarola,GuillemAlenya,Francesc [25] ShangchenHan,BeibeiLiu,RobertWang,YutingYe,Christo-
Moreno-Noguer,andGre´goryRogez. Ganhand:Predicting pherDTwigg,andKenrickKin. Onlineopticalmarker-based
humangraspaffordancesinmulti-objectscenes.CVPR,2020. handtrackingwithdeeplabels. ACMTOG,37(4):1–10,2018.
2 2,4
[10] EdilsondeAguiar,CarstenStoll,ChristianTheobalt,Naveed [26] MohamedHassan,ParthaGhosh,JoachimTesch,Dimitrios
Ahmed, Hans-Peter Seidel, and Sebastian Thrun. Perfor- Tzionas,andMichaelJBlack. Populating3dscenesbylearn-
mancecapturefromsparsemulti-viewvideo. SIGGRAPH, inghuman-sceneinteraction. CVPR,2021. 2
2008. 2 [27] YanaHasson,GulVarol,DimitriosTzionas,IgorKalevatykh,
[11] JosephDelPreto,ChaoLiu,YiyueLuo,MichaelFoshey,Yun- MichaelJBlack,IvanLaptev,andCordeliaSchmid.Learning
zhu Li, Antonio Torralba, Wojciech Matusik, and Daniela joint reconstruction of hands and manipulated objects. In
Rus.Actionsense:Amultimodaldatasetandrecordingframe- CVPR,2019. 2,3,12
9[28] Chun-HaoPHuang,HongweiYi,MarkusHo¨schle,Matvey [46] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani,
Safroshkin,TsvetelinaAlexiadis,SenyaPolikovsky,Daniel TimoBolkart,AhmedAAOsman,DimitriosTzionas,and
Scharstein, and Michael J Black. Capturing and inferring MichaelJBlack.Expressivebodycapture:3dhands,face,and
densefull-bodyhuman-scenecontact. CVPR,2022. 3 bodyfromasingleimage. InProceedingsoftheIEEE/CVF
[29] YinghaoHuang,OmidTaheri,MichaelJBlack,andDimitrios conferenceoncomputervisionandpatternrecognition,pages
Tzionas. Intercap:Jointmarkerless3dtrackingofhumans 10975–10985,2019. 16
andobjectsininteraction. GCPR,2022. 3,12 [47] Benjamin Petit, Jean-Denis Lesage, Edmond Boyer, and
[30] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian BrunoRaffin. VirtualizationGate. SIGGRAPHEmerging
Sminchisescu. Human3.6m:Largescaledatasetsandpredic- Technologies,2009. 2
tivemethodsfor3dhumansensinginnaturalenvironments. [48] IlyaAPetrov,RiccardoMarin,JulianChibane,andGerard
IEEEtransactionsonpatternanalysisandmachineintelli- Pons-Moll. Objectpop-up:Canweinfer3dobjectsandtheir
gence,36(7):1325–1339,2013. 2 posesfromhumaninteractionsalone? CVPR,2023. 2
[31] Hanbyul Joo, Hao Liu, Lei Tan, Lin Gui, Bart Nabbe, [49] MathisPetrovich,MichaelJBlack,andGu¨lVarol. Action-
IainMatthews,TakeoKanade,ShoheiNobuhara,andYaser conditioned3dhumanmotionsynthesiswithtransformervae.
Sheikh. Panopticstudio:Amassivelymultiviewsystemfor ICCV,2021. 2
socialmotioncapture. CVPR,2015. 2 [50] MathisPetrovich,MichaelJBlack,andGu¨lVarol. Temos:
[32] TakeoKanade,PeterRander,andP.J.Narayanan. Virtualized Generatingdiversehumanmotionsfromtextualdescriptions.
reality:Constructingvirtualworldsfromrealscenes. IEEE ECCV,2022. 2
Multimedia,1997. 2
[51] RalfPlankersandPascalFua. ArticulatedSoftObjectsfor
[33] Roland Kehl and Luc Van Gool. Markerless tracking of
Multi-ViewShapeandMotionCapture. TPAMI,2003. 2
complexhumanmotionsfrommultipleviews.CVIU,2006.2
[52] JavierRomero,DimitriosTzionas,andMichaelJBlack. Em-
[34] NileshKulkarni,DavisRempe,KyleGenova,AbhijitKundu,
bodiedhands:Modelingandcapturinghandsandbodiesto-
JustinJohnson,DavidFouhey,andLeonidasGuibas. Nifty:
gether. SIGGRAPHASIA,2017. 16
Neural object interaction fields for guided human motion
[53] JohannesLSchonbergerandJan-MichaelFrahm. Structure-
synthesis. arXivpreprintarXiv:2307.07511,2023. 2
from-motionrevisited. InCVPR,2016. 12
[35] TaeinKwon,BugraTekin,JanStu¨hmer,FedericaBogo,and
[54] FadimeSener,DibyadipChatterjee,DanielShelepov,Kun
MarcPollefeys. H2o:Twohandsmanipulatingobjectsfor
He,DipikaSinghania,RobertWang,andAngelaYao. As-
firstpersoninteractionrecognition. ICCV,2021. 3,4,12
sembly101:Alarge-scalemulti-viewvideodatasetforunder-
[36] YunzeLiu,YunLiu,CheJiang,KangboLyu,WeikangWan,
standingproceduralactivities. CVPR,2022. 2
HaoShen,BoqiangLiang,ZhoujieFu,HeWang,andLiYi.
[55] LeonidSigal,AlexandruOBalan,andMichaelJBlack. Hu-
Hoi4d: A 4d egocentric dataset for category-level human-
maneva:Synchronizedvideoandmotioncapturedatasetand
objectinteraction. CVPR,2022. 2,3,12
baselinealgorithmforevaluationofarticulatedhumanmotion.
[37] NaureenMahmood,NimaGhorbani,NikolausFTroje,Ger-
IJCV,87(1-2):4,2010. 2
ard Pons-Moll, and Michael J Black. Amass: Archive of
[56] TomasSimon,HanbyulJoo,IainMatthews,andYaserSheikh.
motioncaptureassurfaceshapes. ICCV,2019. 2
Handkeypointdetectioninsingleimagesusingmultiview
[38] Manus,2023. https://www.manus-meta.com/. 4,
bootstrapping. CVPR,2017. 2
12
[57] CarstenStoll,NilsHasler,JuergenGall,Hans-PeterSeidel,
[39] T.MatsuyamaandT.Takai. Generation,visualization,and
andChristianTheobalt.Fastarticulatedmotiontrackingusing
editingof3dvideo. 3DPVT,2002. 2
asumsofgaussiansbodymodel. ICCV,2011. 2
[40] WojciechMatusik,ChrisBuehler,RameshRaskar,StevenJ.
[58] OmidTaheri,NimaGhorbani,MichaelJBlack,andDimitrios
Gortler,andLeonardMcMillan. Image-basedvisualhulls.
Tzionas. Grab:Adatasetofwhole-bodyhumangraspingof
SIGGRAPH,2000. 2
objects. ECCV,2020. 2,3,4,12
[41] Aymen Mir, Xavier Puig, Angjoo Kanazawa, and Gerard
Pons-Moll.Generatingcontinualhumanmotionindiverse3d [59] PurvaTendulkar,D´ıdacSur´ıs,andCarlVondrick. Flex:Full-
scenes. arXivpreprintarXiv:2304.02061,2023. 2 bodygraspingwithoutfull-bodygrasps. CVPR,2023. 2
[42] GyeongsikMoon,Shoou-IYu,HeWen,TakaakiShiratori, [60] DylanTurpin,LiquanWang,EricHeiden,Yun-ChunChen,
andKyoungMuLee. Interhand2.6m:Adatasetandbaseline MilesMacklin,StavrosTsogkas,SvenDickinson,andAni-
for 3d interacting hand pose estimation from a single rgb meshGarg. Grasp’d:Differentiablecontact-richgraspsyn-
image. ECCV,2020. 2 thesisformulti-fingeredhands. ECCV,2022. 2
[43] Movella,2023. https://base.xsens.com/. 4,12 [61] Daniel Vlasic, Ilya Baran, Wojciech Matusik, and Jovan
[44] FranziskaMueller,FlorianBernard,OleksandrSotnychenko, Popovic´. Articulatedmeshanimationfrommulti-viewsilhou-
DushyantMehta,SrinathSridhar,DanCasas,andChristian ettes. TOG,2008. 2
Theobalt. Ganeratedhandsforreal-time3dhandtracking [62] Daniel Vlasic, Ilya Baran, Wojciech Matusik, and Jovan
frommonocularrgb. CVPR,2018. 2 Popovic´. Articulatedmeshanimationfrommulti-viewsilhou-
[45] FranziskaMueller,DushyantMehta,OleksandrSotnychenko, ettes. SIGGRAPH,2008. 2
SrinathSridhar,DanCasas,andChristianTheobalt. Real- [63] DonglaiXiang,HanbyulJoo,andYaserSheikh. Monocular
timehandtrackingunderocclusionfromanegocentricrgb-d totalcapture:Posingface,body,andhandsinthewild.CVPR,
sensor. ICCV,2017. 2 2019. 2
10[64] Sirui Xu, Zhengyuan Li, Yu-Xiong Wang, and Liang-Yan
Gui. Interdiff:Generating3dhuman-objectinteractionswith
physics-informeddiffusion. ICCV,2023. 2
[65] Sifan Ye, Yixing Wang, Jiaman Li, Dennis Park, C Karen
Liu,HuazheXu,andJiajunWu.Scenesynthesisfromhuman
motion. SIGGRAPHAsia,2022. 2
[66] ShanxinYuan,QiYe,BjornStenger,SiddhantJain,andTae-
KyunKim.Bighand2.2mbenchmark:Handposedatasetand
stateoftheartanalysis. CVPR,2017. 2
[67] HuiZhang,SammyChristen,ZicongFan,LuochengZheng,
JeminHwangbo,JieSong,andOtmarHilliges. Artigrasp:
Physicallyplausiblesynthesisofbi-manualdexterousgrasp-
ingandarticulation. arXivpreprintarXiv:2309.03891,2023.
2
[68] HeZhang,YutingYe,TakaakiShiratori,andTakuKomura.
Manipnet:Neuralmanipulationsynthesiswithahand-object
spatialrepresentation. ACMTOG,40(4),2021. 2
[69] JuntianZheng,QingyuanZheng,LixingFang,YunLiu,and
LiYi.Cams:Canonicalizedmanipulationspacesforcategory-
levelfunctionalhand-objectmanipulationsynthesis. CVPR,
2023. 2
[70] KeyangZhou,BharatLalBhatnagar,JanEricLenssen,and
Gerard Pons-Moll. Toch: Spatio-temporal object-to-hand
correspondenceformotionrefinement. ECCV,2022. 2
[71] ChristianZimmermannandThomasBrox. Learningtoes-
timate3dhandposefromsinglergbimages. ICCV,2017.
2
[72] ChristianZimmermann,DuyguCeylan,JimeiYang,Bryan
Russell,MaxArgus,andThomasBrox. Freihand:Adataset
formarkerlesscaptureofhandposeandshapefromsingle
rgbimages. ICCV,2019. 2,4,12
[73] Gaspard Zoss, Derek Bradley, Pascal Be´rard, and Thabo
Beeler. An empirical rig for jaw animation. ACM TOG,
37(4):1–12,2018. 4
11Dataset frame# subject# object# body hand# contact obj.6d. obj.artic. multiobj. environment
Freihand [72] 37K 32 27 ✗ 1 - ✗ ✗ - tablesetup
Obman [27] 154K 20 2772 ✗ 1 ✗ ✓ ✗ ✗ synthetic
GRAB [58] - 10 51 ✓ 2 ✓ ✓ ✗ ✗ standingsetup
DexYCB [6] 582K 10 20 ✗ 1 ✗ ✓ ✗ ✗ tablesetup
HO3D [23] 78K 10 10 ✗ 1 ✗ ✓ ✗ ✗ tablesetup
ContactPose [2] 3M 50 25 ✗ 2 ✓ ✓ ✗ ✗ standingsetup
BEHAVE [1] 15K 8 20 ✓ - ✓ ✓ ✗ ✗ portablesetup
InterCap [29] 67K 10 10 ✓ 2 ✓ ✓ ✗ ✗ portablesetup
FHPA [16] 105K 6 26 ✗ 1 ✗ ✓ ✗ ✗ roomsetup
H2O [35] 571K 4 8 ✗ 2 ✗ ✓ ✗ ✗ tablesetup
H2O-3D [24] 76K 5 10 ✗ 2 ✗ ✓ ✗ ✗ tablesetup
HOI4D [36] 2.4M 9 800 ✗ 1 ✗ ✓ ✓ ✗ roomsetup
ARCTIC [13] 2.1M 10 11 ✓ 2 ✓ ✓ ✓ ✗ standingsetup
Ours 56M 30 22 ✓ 2 ✓ ✓ ✓ ✓ roomsetup
Table2.Comparisonofexistinghuman-objectinteractiondatasets
fabricsplacedinoursystem.Weprovidepre-calibratedini-
tialintrinsicparametersforthethreetypesoflensesderived
from2or3samplesoflensesforbetterconvergenceincam-
eraposeestimation.Wescalethecalibrated3Dspaceinto
areal-worldmetric(inmeters)bylocatingcheckerboards
withknownsizesduringcameracalibration.
All cameras, the motion capture suit, and gloves are
syncedand gen-locked viaacommon squarewave signal
Figure10.SystemDevices.(a)RGBcameraswith3typesoflenses.
thatcomesfromthemotioncapturedevicetosynchronize
(b)Signaldistributers(c)Desktopmachines(d)LEDLights(e)
twoheterogeneoussystems,whichiscrucialtopreciseHOI
NASstoragesystems
captures. To deliver the sync signals to a large number of
A.DatasetComparison cameras,weutilized11signaldistributorsinahierarchical
manner,eachofwhichcanbeconnectedto8camerasvia
In Tab. 2, we present a comprehensive comparison be-
GPIO cables. We use 1 master and 18 slave desktop ma-
tweenourParaHomedatasetandotherexistingdatasetsthat
chinestocontrolthecamerasandprocesscapturedrecords.
capturehuman-objectinteractions.Asshown,ourParaHome
Eachslavemachineisconnectedto3or4camerasviaEth-
datasetisthefirstdatasettocaptureallessentialcomponents
ernetcablesandequippedwitha4-port1Gethernetboard,
indynamicHOIscenariosinanaturalroomenvironment,
and2SSDswithacapacityof500GBand1TBeach.We
containingbodymotions,handmotions,andthemovements
alsoinclude2NASstoragesystemseachholding96TBof
of all visible objects in the scenes including object artic-
storage,connectedtoallmachinestosaveprocesseddata.
ulations. Notably, our dataset contains one to two orders
15LEDlights(4500lm)areinstalledtoprovidesufficient
ofmagnitudelargerframenumbersthanexistingdatasets.
illumination. Pictures of our system devices are shown in
DetailsofthecomparisonareinTab.2.
Fig.10.
Tocapturebothbodymotionandsubtlehandmotions,
B.SystemDetails
weuseIMU-basedmotioncaptureequipment,Xsensmotion
Inordertocovertheentirevolumeoftheroomandtore- suit [43] and Manus hand gloves [38]. The body motion
duceocclusionissues,weinstall70RGBindustrialcameras, systemcapturesthemotionsat60Hz.
BFLY-31S4C-C. The cameras capture videos at 30Hz in
C.DataAcquisition
2048×1536resolution.Wesettheexposuretimeat3msec,
whichshowsagoodbalancebetweenlow-motionblurand
C.1.ModelingObjectArticulations.
sufficient brightness. We use three types of lenses (thirty
3mmlenses,twenty5mmlenses,andtwenty6mmlenses), Tocapturethemovementofarticulatedobjects,wemodel
where the wide-angle lens (3mm) is helpful in capturing eachobjectasaparametric3Dmodelbydefiningtheobject-
widearea.WecalibratethecamerasusingStructure-from- specific articulated motion parameters. This modeling re-
MotionviaCOLMAP[53]withmultiplerandomlypatterned quiresscanningindividualpartsseparatelyandcompositing
12Object Part1 Part2
Sink revolute revolute
Laptop revolute -
Drawer sliding sliding
Gasstove revolute revolute
Microwave revolute -
Trashbin revolute -
Washingmachine revolute -
Refrigerator revolute revolute
Table3.Partinformationofarticulatedobjects
theminacanonicalspacewithdefiningaxisdirection,pivot
points,revolutejoints,andsoon,basedontheobjecttypes.
DuringHOIcaptures,wetrackthemotionofeachpartvia Figure11.Thehandcalibrationstructuretopreciselymeasurehand
ourmarkersystem(e.g.monitorofalaptopandthebase), skeletonconfigurationandtofindtherelativelocationsofhand
fromwhichwecomputethearticulatedmotionparameters. markersattachedtowristinhand-centriccoordinate
Inthissubsection,wedescribetheprocessofmodelingar-
ticulatedobjectsasparametric3Dmodels.Articulationin-
formation of each object with multiple parts is shown in
C.2.BodyAlignmentDetail(Sec5.2inMainPaper)
Tab.3.
Tofindaxisa andpivotpointp ofthearticulatedob-
e e
jects, we capture markers attached to each object part at
Inthissubsection,weprovideadditionaldetailsofour
differentpartstatesseparatelyandacquireeachmarkercor-
spatialalignmentprocessbetweenamultiviewcamerasys-
nersintheParaHomespaceas{m (t)}n .Priortoapplying
i t=1 temandwearablemotioncapturesystems,describedinSec.
algorithm, we transform marker corners {m (t)}n back
i t=1 5.2inourmainmanuscript.
toobjectcanonicalspacewithT−1 andutilizetrans-
mar→obj
formedmarkercornersinthecanonicalspace{m′ i(t)}n t=1. Toresolvetheissueofimperfectbodyandhandskeleton
Fortheslidingjoint,axisa ecaneasilybecalculatedusing scalefromthewearablemotioncapturesystem,weattach
markercornersattimetandt′as:
3or4ArUcomarkerstoeachnear-rigidbodypart(torso,
m′(t)−m′(t′) hands, upper arms, lower arms, upper legs, lower legs) to
a e = ∥mi ′(t)−mi ′(t′)∥ assigncorrespondences.Duringalignmentcapture,partic-
i i ipantsperformtherange-of-motionmovementbyrotating
Incaseobjectparthasarevolutejoint,westartinitializing theirarmsandlegswhilepinnedorbent,particularlytwisting
anaxisa andeachrelativestate∆s (t,t′)=|s (t)−s (t′)| theirwriststolocateeachhandwrist.Withthecaptureddata,
e e e e
betweentimetandt′ (forthetargetarticulatedobjectcap- weoptimizebodyskeletonconfigurationB ={O}andbody
tured at different n number of states, time t and time t′ markerslocationsMb viagradientdecentwithalearning
satisfies t ̸= t′ and t,t′ ∈ {1,2,··· ,n}). Then we apply rateof0.008for50epochs.Specificallyforweightsofbody
optimizationwithmarkercornerstowardallpossiblepairsof and foot, λ b = 100,λ f = 5000 are used. In the case that
timestandt′.Letf beamapdefiningrotationtransforma- alignmentisnotwelloptimized,weadditionallypenalize
tionwithrespecttopivotandgivenaxis-angleanddenoteas excessivelengthchangeinspinesanddifferenceinskeleton
T =f(a ,∆s (t,t′),p ).Thenforasetofallpossible lengthsbetweentheleftandrightsidesofthebodybyadding
t′→t e e e
time pairs P, the optimization target for axis a , relative anextraregularizationtermtotheoptimizationtarget.Once
e
state∆s andpivotp becomes: thealignmentprocedureisfinished,weremovemarkers(all
e e
fromtheupperlegs,oneforeachupperarm,lowerarm,and
argmin (cid:88) ∥m′(t)−T m′(t′)∥2 lowerleg)tominimizeinterferencewiththemovementsof
i t′→t i
ae,∆se(t,t′),pe(t,t′)∈P the participant. The selection of remaining markers is de-
terminedbasedontheirimportanceduringcaptures,where
Sinceinitialaxisa andpivotp areinitializedintheobject weassesstheirimportancebyevaluatingwhethertheirab-
e e
canonical space, we directly utilize acquired information sencewouldcompromisetheaccuracyofbodypositioning
toderivetransformationsfromdetectedmarkersfromeach inthecameraspace.Checkoursupplementaryvideoforan
capturesession. exampleofbodyalignmentmotion.
13Corner# HandSide Seq1 Seq2 Seq3 Seq4
1,2 Right 1,2 1,3 1,4 1,5
1,3 Right 1,2 1,3 1,4 1,5
2,4 Right 1,2 1,3 1,4 1,5
5,2 Right 1,2 1,3 1,4 1,5
6,2 Right 1,2 1,3 1,4 1,5
6,3,2 Right 1,2,3 1,3,4 1,4,5 -
2,1 Left 1,2 1,3 1,4 1,5
3,1 Left 1,2 1,3 1,4 1,5
4,2 Left 1,2 1,3 1,4 1,5
5,2 Left 1,2 1,3 1,4 1,5
6,2 Left 1,2 1,3 1,4 1,5
2,5,6 Left 1,2,3 1,3,4 1,4,5 -
Table4.HandCalibrationProtocol
C.3.HandCalibrationStructureandProtocol(Sec
5.2inMainPaper)
Ashumanusuallyhandleobjectswiththeirfingers,fin-
Figure12.HandSkeletonHierarchy
gertipsplayanimportantroleduringinteraction.Wemade
thecalibrationstructure,asshowninFig.11,tobetterlocate
fingertipsandfindthehandskeletonconfigurationsalong
avoidunnaturaldeformationofhandskeleton.Thelocation
withrelativelocationsbetweentheattachedhandmarkersto
of hand markers Mh is optimized through a total of 150
wrist.Webuildthehandcalibrationstructurebycombining
iterations.Theskeletonscaleandadditionaloffsetareopti-
threecubeswithArUcomarkersandspecifyingtheordered
mizedstartingfrom50and100iterationseach.Weusethree
3Dcorner verticesof thestructureC = {c ∈ R3}6 as
i i=1 losses, L to measure the Euclidean distance from hand
shown in Fig. 11. During the hand calibration procedure, tip
tipstopairedcorners,L tomeasurethedistancefrom
we request each participant to touch the calibration struc- wrist
wristlocationfrombodymotioncapturedeviceandthewrist
ture’scornerswiththeirfingertips.Weinstructthemtotouch
position computed by hand marker position and L to
specified multiple corners at each step using two or three pen
measurepenetrationofhandtothecalibrationstructure.The
fingertips.Aparticipantundergoes23stepsofsuchtouching
penetrationlossisacosinesimilaritybetweenthecalibra-
processesper-hand.TheTab.4compriseshandcalibration
tionstructure’snormalvectorandthetargetcorner-to-hand
instructionsforsubjectstofollow.Corner#isasetoftwo
tip vector. In summed loss λ L +λ L +λ L ,
orthreetargetcornernumbersofthecalibrationstructure t tip w wrist p pen
losses are weighted equally by λ = 1,λ = 1,λ = 1.
which the subject should contact with their fingertips. In t w p
Buttheyaremanuallyadjustedbasedonthetouchaccuracy
theTab.4,theordersoffingerstotouchthetargetcorners
andbodycalibrationaccuracyoftheoptimizedresult.After
are specified with numbers corresponding to each finger,
the alignment process, the average Euclidean distance be-
which are (1:Thumb, 2:Index, 3:Middle, 4:Ring, 5:Little).
tweenthecornerandthetargetfingertipresultsin0.83(in
Anexampleofthehandcalibrationprocedureisshownin
centimeters).
oursupplementaryvideo.
C.4.ImplementationDetailsonHandCalibration D.CaptureScenarioDetails
Here we describe the details of the hand calibration Actionsareplacedinanarbitraryordersothateachsce-
method. As described in 5.2, optimization parameters are nario has a different order of actions. However, because
handskeletonconfigurationH = {Sh,Oh}andpositions certainactionsetsnecessitateamorenaturalorderingsuch
of3Dmarkersinthelocalhand-centriccoordinateMh.We as“drinkwaterfromthecup”and“pourwaterfromthekettle
empiricallydecidethetargetrangeoftheoptimizationskele- intothecup”,wherethelatterneedstobeperformedbefore
tontothepalmareashowninFig.12andaddconstraints the former, we maintain the order for such actions sets to
thatlimittheskeletonscales(s )foreachskeletonsegment mirrorreal-lifehumanbehavior.Forcooking-relatedactions,
i
i between 0.8 ≤ s ≤ 1.2, and additional skeleton offset thereare7setsofdesignated,orderedactions,andoneof
i
value(δ )fortargetjointsj with|δ|≤0.01inmeterscaleto themisrandomlypickedtoconstructthescenario.Thesetof
j
14E.DatasetDetails
Non-cookingactions
Bringthecup E.1.DatasetContents
Drinkfromthecup
Scanned Object Mesh. 3D meshes of the objects in the
Placethecupinthesink
roomenvironmentareobtainedvia3Dscanner[12].They
Takethechairandsit
arescaledtoametricscaleandzero-centered.
Takethelaptoptotheseat
ObjectArticulationInformation.Objectswitharticulation
Openthelaptop
containaxisa .Iftheparthasarevolutejoint,weinclude
Typethekeyboardonthelaptop e
pivotpointp additionally.Thesearedefinedintheobject
Closethelaptop e
canonicalspaceandareutilizedingettingeachobjectpart-
Takethebookfrombookshelf
transformationtowardthecameraspace.
Bringthekettle
Object-MarkerTransformations.Since3Dmarkercubes
Pourwaterintothecup
attached to each object can fall apart from the object, we
Placethekettleinthesink
maintaintheinformationper-sequencecaptureandincaseit
Openthedrawerandtakeitemout
isdetached,were-calculatethetransformationagainafter
Puttrashinthetrashbin
attachingtothepartagain.
Openthewashingmachineandputlaundry
RelativeOrientationofHand/BodyJoints.Orientationof
Kitchen-relatedactions eachhandandbodyjointswithrespecttotheirparentjoints
arerecordedandoptimizedviaamotioncapturesystem.
Openthesinkandtakethecuttingboardandknifeout
3DHand/BodyJointPositionsintheCameraSpace.With
Opentherefrigeratorandtakefoodout
thepositionsofmarkersattachedtothebodyinthemocap
Cutthefoodonthecuttingboard
spaceacquiredviabodyalignmentprotocol,translationand
Takethepotandplaceitonthestove
orientationtocameraspaceareobtainedusingthepositions
Turnonthegasstove
ofcorrespondingmarkersincameraspaces.Andwecompute
Puttheslicedfoodintopot/pan
the positions of two hands and body using the obtained
Seasonwithsalt
translationandorientation.
Turnoffthegasstove
VerbalInstructionsforEachActions.Foreverycapture,
Placethepottothediningtable
participants are given instructions as to what actions they
Takethepanandplaceitonthestove
willbeperforming.Itisconveyedverballyandcontainswhat
Placethepantothediningtable
objecttointeractwith,andhowtointeractwiththeobject.
Openthemicrowave
The instructions are recorded and aligned with hand and
Opentherefrigerator,takefoodoutandputitinmicrowave
bodydata.
Closethemicrowave
Per-frameContactInformation.Ateachframewherecon-
Movetheheatedfoodfrommicrowavetopot
tactbetweenLeft/Right/Bodyandobjectoccurs,thecorre-
Movethefoodfrompottothebowl
spondingframeandobjectcategory/bodypartinformation
Placethebowltothediningtable
isrecorded.
Putthefoodintopot
Movethefoodfrompantopot E.2.SequenceVisualization
Movetheheatedfoodfrommicrowavetopan
Sampleddatafromourcollecteddatasetsareshownin
Fig.13.Correspondinginstructionsareprovidedunderthe
Table5.Listofactionsduringthecapture
caption.
E.3.ContactInformationandVisualization
Contactinformationbetweenthehandandobjectsurfaces
isacquiredbycalculatingSDFofobjectmeshandtransform-
cooking-relatedactionsisplacedinbetweenthearbitrarily inghumanjointsperframeincameraspacetowardeachob-
ordered non-cooking actions. To this end, every scenario jectcanonicalspacebyapplyingtransformationT−1 .
mar→obj
is composed in semi-arbitrary order to reflect diverse and Then,forthejointsthatarecloserthanthecontactthresh-
realistichumanbehaviorinastudioapartmentenvironment. olddistancefromtheobjectsurface,wedecidetheyarein
Notably,thecorrespondingverbalinstructionsfortheactions contactfromeachother.Wereportper-objectratioofcon-
arealsorecordedandannotatedtothecaptureddata.Alist tactedframesw.r.ttrackednumberofframesinTab.6where
ofactioninstructionsconveyedduringcapturesisshownin contacthasbeencountedforleftandrighthandseparately.
Tab.5. Alsoinordertovisualizecontactinformation,wefitMANO
15ObjectName LeftHand RightHand Total
Cup 0.039 0.042 0.075
Pan 0.036 0.070 0.100
Pot 0.060 0.068 0.080
Book 0.024 0.020 0.033
Bowl 0.101 0.069 0.145
Salt 0.043 0.050 0.086
Sink 0.084 0.068 0.123
Desk 0.037 0.033 0.054
Knife 0.019 0.080 0.096
Chair 0.023 0.022 0.037
Potlid 0.012 0.026 0.036
Kettle 0.027 0.040 0.062
Laptop 0.060 0.052 0.072
Drawer 0.004 0.011 0.014
Gasstove 0.006 0.018 0.024
Trashbin 0.008 0.005 0.011
Microwave 0.008 0.004 0.011
Bookshelf 0.001 0.001 0.002
Diningtable 0.016 0.016 0.026
Refrigerator 0.017 0.036 0.043
Cuttingboard 0.086 0.051 0.107
Washingmachine 0.011 0.009 0.017
Table6.Ratioofcontactedframesw.r.ttrackedframes
[52]andSMPLX[46]poseandshapeparametersincam-
eracoordinatespaceusingeachbodyjointpositions.Using
FittedMANOandSMPLXmodelper-frame,wecompute
SDFofbody,handsandobjectsandquerydistancesateach
meshverticesw.r.teachother.Thenthecontactedverticesat
eachmeshsurfaceareacquiredbythresholdingthedistance
value.Forthecontactedpoints,distancesarevisualizedas
intensityimagesinFig.15andFig.14.
16Figure13.(Column1)Opentherefrigerator,takefoodoutandputitinthemicrowave.Closethemicrowave.(Column2)Takethechairand
sit.Openthelaptop.Typeonthelaptop.Openthedrawerandtakeanitemout.(Column3)Opentherefrigeratorandtakefoodout.Cutthe
foodonthecuttingboard.Takethepotandplaceitonthestove.(Column4)Turnonthegas-stove.Seasonwithsalt.(Column5)Pourwater
intothecup.Drinkfromthecup.Takethechairandsit.Openthelaptop.
17Figure14.ExampleofHandsandObjectsContactVisualizationinCaptureddata.(Column1)Interactionscenewherethesubjectinteract
withmultipleobjectswhilenavigatingtheroomenvironment.(Column2)Close-upversionofthescene.(Column3,4,5,6)Contactintensity
foreachhandpalm(Left,Right),cupandkettle.
18Figure15.ExampleofBodyandObjectsContactVisualizationinCaptureddata.(Column1)Interactionscenewherethesubjectinteract
withmultipleobjectswhilenavigatingtheroomenvironment.(Column2)Close-upversionofthescene.(Column3,4,5)Contactintensityof
bodyandeachhandpalm(Left,Right).(Column6,7,8)Contactintensityofchair,cupandlaptop.
19