UnderreviewasaconferencepaperatICLR2024
AGENTMIXER: MULTI-AGENT CORRELATED POLICY
FACTORIZATION
Anonymousauthors
Paperunderdouble-blindreview
ABSTRACT
Centralizedtrainingwithdecentralizedexecution(CTDE)iswidelyemployedto
stabilizepartiallyobservablemulti-agentreinforcementlearning(MARL)byuti-
lizing a centralized value function during training. However, existing methods
typicallyassumethatagentsmakedecisionsbasedontheirlocalobservationsin-
dependently,whichmaynotleadtoacorrelatedjointpolicywithsufficientcoordi-
nation. Inspiredbytheconceptofcorrelatedequilibrium,weproposetointroduce
astrategymodificationtoprovideamechanismforagentstocorrelatetheirpoli-
cies. Specifically, we present a novel framework, AgentMixer, which constructs
thejointfullyobservablepolicyasanon-linearcombinationofindividualpartially
observablepolicies. Toenabledecentralizedexecution,onecanderiveindividual
policies by imitatingthe joint policy. Unfortunately, such imitation learning can
lead to asymmetric learning failure caused by the mismatch between joint pol-
icyandindividualpolicyinformation. Tomitigatethisissue,wejointlytrainthe
jointpolicyandindividualpoliciesandintroduceIndividual-Global-Consistency
to guarantee mode consistency between the centralized and decentralized poli-
cies. WethentheoreticallyprovethatAgentMixerconvergestoanϵ-approximate
Correlated Equilibrium. The strong experimental performance on three MARL
benchmarksdemonstratestheeffectivenessofourmethod.
1 INTRODUCTION
Cooperative multi-agent reinforcement learning (MARL) has attracted substantial attention in re-
cent years owing to its promise in solving many real-world tasks that naturally comprise multiple
decision-makers interacting at the same time, such as multi-robot control (Gu et al., 2023), traffic
signalcontrol(Ma&Wu,2020),andautonomousdriving(Shalev-Shwartzetal.,2016). However,
unlikethesingle-agentRLsettings,learninginmulti-agentsystems(MAS)posestwoprimarychal-
lenges: coordination,i.e.,agentsshouldworktogetherinordertoachieveacommongoalandlearn
optimal joint behavior, and partial observability, which limits each agent to her own local obser-
vationsandactions. Toaddressthesedifficulties, mostworksadoptapopularlearningframework
called Centralized Training Decentralized Execution (CTDE) (Lowe et al., 2017; Yu et al., 2022;
Rashidetal.,2020a)thatallowsagentstoaccessglobalinformationduringthetrainingphasewhile
remainingthelearnedpoliciesexecutedwithonlylocalinformationinadecentralizedway.
Toenhancecoordination, onelineofresearchistousevaluedecomposition(VD)(Sunehagetal.,
2017),e.g. QMIX(Rashidetal.,2020b)andQPLEX(Wangetal.,2021a),whichlearnsacentral-
izedjointactionvaluefunctionfactorizedbydecentralizedagentutilityfunctions. Withthestruc-
turalconstraintofIndividual-Global-Max(IGM)(Sonetal.,2019),itguaranteestheoptimalaction
consistencybetweenthecentralizedanddecentralizedpolicies. Ontheotherhand,multi-agentpol-
icy gradient (MAPG) methods(de Witt et al., 2020), such as MADDPG (Lowe et al., 2017) and
MAPPO(Yuetal.,2022),hasachievedremarkablesuccess. However,whilelearningacentralized
critic, previousworksarestillconstrainedbyassumingindependenceamongagentsduringexplo-
ration. Afewrecentworksfurtherproposeauto-regressivepoliciestoimposecoordinationamong
agentsbyallowingagentstoobserveotheragents’actions,eitherexplicitly(Fuetal.,2022;Wang
etal.,2023)orimplicitly(Lietal.,2023;Wenetal.,2022). InspiredbytheCorrelatedEquilibrium
(CE)(Maschleretal.,2013)ingametheory,MAVEN(Mahajanetal.,2019)andSIC(Chenetal.,
2022)introduceahierarchicalcontrolmethodwithanexternalsharedlatentvariableasadditional
informationforagentstocoordinateeachother. However, notethatexistingauto-regressivemeth-
1
4202
naJ
61
]AM.sc[
1v82780.1042:viXraUnderreviewasaconferencepaperatICLR2024
ods assume a pre-defined execution order. Moreover, most existing correlated policies violate the
requirementfordecentralizedexecution. ThispaperinsteadaimstoachieveCorrelatedEquilibrium
inafullydecentralizedwaywhichiscrucialforreal-worldapplications.
In order to mitigate the difficulty of learning under
partialobservability,CTDEexploitstruestateinfor-
mation,usuallyviaacentralizedcritic,totrainindi- 2 2
vidualpoliciesconditionedonthelocalobservation-
actionhistory.Whileitispossibletofirstlearnacen-
tralizedexpertpolicyandthentrainthedecentralized 1 2 1 2
agents to follow it (Lin et al., 2022), it may result
insuboptimalpartiallyobservablepoliciessincethe
omniscientcriticoragenthasnoknowledgeofwhat 2 2
thedecentralizedagentsdonotknow,referredtoas Figure 1: The partially observable bridge
the asymmetric learning failure (Warrington et al., crossingtask,wheretwoagents(bluesquare
2021). Consider a scenario where two agents of andorangesquare)withchangingphysiques
distinct physical shapes try to get to their opposite in different episodes (left and right figure)
destinations through two possible paths 1 and 2, as wanttoarriveattheirdestinations(starswith
showninFigure1. Successfulpoliciesshouldavoid corresponding colors) through passageways
collision as the body sizes of agents always change 1 or 2. In this task, naively learning from a
and each passage only permits two small agents or fullobservationexpertpolicywouldresultin
one big agent. In CTDE, we can learn the optimal suboptimal partially observable policies due
fullyobservablejointpolicyconditionedonagents’ totheasymmetriclearningfailure(Warring-
physiques, which would select the shorter path 1 tonetal.,2021).
whenbothagentsaresmall. However,naivelylearn-
ingfromsuchacentralizedagentcouldleadtoagentsjammingonthesamepassage,asthepartially
observable agents cannot access the other agent’s body size. In contrast, the optimal partially ob-
servablepoliciesshouldideallyensurethateachagentconsistentlyselectsdistinctpassagewaysto
avoid collision. This asymmetric learning failure is a prevalent issue in MARL due to the partial
observabilitynatureofMAS.Whileafewworkshavestudiedsimilarchallengesinthecontextof
single-agentRL(Walsmanetal.,2023),itisworthnotingthatthisissuewithintheMARLdomain
hasnotbeenthoroughlyinvestigatedtothebestofourknowledge.
In this paper, we propose correlated policy factorization, dubbed AgentMixer, to tackle the above
two challenges and achieve CE among agents in a fully decentralized way. Firstly, we propose a
novelframework,namedPolicyModifier(PM),tomodelthecorrelatedjointpolicy,whichtakesas
inputdecentralizedpartiallyobservablepoliciesandthestateinformationandoutputsthemodified
policies. Consequently,PMactsasanobserver fromtheCEperspectiveandthemodifiedpolicies
form a correlated joint policy. Further, to mitigate the asymmetric learning failure when learning
decentralizedpartiallyobservablepoliciesfromthecorrelatedjointfullyobservablepolicy,wethen
introduceanovelmechanismcalledIndividual-Global-Consistency(IGC),whichkeepsconsistent
modes between individual policies and joint policy while allowing correlated exploration in joint
policy. Theoretically, we prove that AgentMixer converges to ϵ-approximate Correlated Equilib-
rium. Experimentalresultsonvariousbenchmarksconfirmitsstrongempiricalperformanceagainst
currentstate-of-the-artMARLmethods.
2 RELATED WORK
Modelingcomplexcorrelationsamongagentshasbeenattractingagrowingamountofattentionin
recentyears. Thecentralizedtrainingdecentralizedexecution(CTDE)paradigmhasdemonstrated
its success in cooperative multi-agent domain (Lowe et al., 2017; Rashid et al., 2020a; Yu et al.,
2022). Centralizedtrainingwithadditionalglobalinformationmakesagentscooperatebetterwhile
decentralizedexecutionenablesdistributeddeployment.
Valuedecomposition. ValuedecompositionmethodsdecomposethejointQ-functionintoindivid-
ualutilityfunctionsfollowingdifferentinterpretationsofIndividual-Global-Maximum(IGM)(Son
et al., 2019), i.e., the consistency between optimal local actions and optimal joint action. VDN
(Sunehagetal.,2017)andQMIX(Rashidetal.,2020b)decomposesthejointaction-valuefunction
by additivity and monotonicity respectively. QTRAN (Son et al., 2019), WQMIX (Rashid et al.,
2UnderreviewasaconferencepaperatICLR2024
2020a)andQPLEX(Wangetal.,2021a)introduceadditionalcomponentstoenhancetheexpressive
capabilityofvaluedecomposition. Toenhancecoordination,MAVEN(Mahajanetal.,2019)intro-
ducescommittedexplorationamongagentsintoQMIX,whileDCG(Boehmeretal.,2020)models
theinteractionsbetweenagentswithacoordinationgraph. Recentworksdelveintoapplyingvalue
decomposition to actor-critic methods (Su et al., 2021; Zhang et al., 2021). VDACs (Su et al.,
2021), FACMAC(Pengetal.,2021)andDOP(Wangetal.,2021b)combinevaluedecomposition
to compute policy gradient with a centralized but factored critic. Zhang et al. (2021); Wang et al.
(2023)derivejointsoft-Q-functiondecompositionaccordingtoindependentandconditionalpolicy
factorizationrespectively.
Policyfactorization. Existingapproachescommonlyassumetheindependenceofagents’policies,
modeling the joint policy as the Cartesian Product of each agent’s fully independent policy (Yu
etal.,2022;Zhangetal.,2021;Kubaetal.,2021). However,suchanassumptionlacksinmodeling
complex correlations as it constrains the expressiveness of the joint policy and limits the agents’
capability to coordinate. In contrast, some recent works (Wang et al., 2023; Wen et al., 2022; Fu
etal.,2022)explicitlytakethedependencyamongagentsbypresentingthejointpolicyinanauto-
regressive form based on the chain rule (Box et al., 2015). MAT (Wen et al., 2022) casts MARL
intoasequencemodelingproblemandintroducesTransformer(Vaswanietal.,2017)togenerateac-
tions. Wangetal.(2023)extendsFOPZhangetal.(2021)withauto-regressivepolicyfactorization.
However, the lack of restrictions on dependent and independent policies may lead to inconsisten-
cies. ACE (Li et al., 2023) transforms multi-agent Markov Decision Process (MMDP) (Littman,
1994)intoasingle-agentMarkovDecisionProcess(MDP)(Feinberg&Shwartz,2012),whichim-
plicitlymodelstheauto-regressivejointpolicy. Despitethemeritsoftheauto-regressivemodel,the
fixedexecutionorderandexplicitlyconstrainedrepresentationlimitthefeasiblejointpolicyspace.
InspiredbyCorrelatedEquilibrium(Maschleretal.,2013),SIC(Chenetal.,2022)introducesaco-
ordinationsignaltoachievericherclassesofthejointpolicyandmaximizesthemutualinformation
(Kim et al., 2020) between the signal and the joint policy, which is close to MAVEN. Correlated
Q-learning(Greenwald&Hall,2003)generalizesNashQ-learning(Hu&Wellman,2003)basedon
CEandproposesseveralvariantstoresolvetheequilibriumselectionproblem(Samuelson,1997).
Similarly,SchroederdeWittetal.(2019)learnsahierarchicalpolicytreebasedonasharedrandom
seed. Sheng et al. (2023) and Wen et al. (2019) learn coordinated behavior with recursive reason-
ing. However,mostexistingworkfocusesonfullyobservablesettingsorviolatesthedecentralized
executionrequirement.
Moreover,existingapproachesrarelystudytheissuesarisingfromtheuseofasymmetricinforma-
tion(Warringtonetal.,2021)inCTDE,thatis,thejointfullyobservablecriticoragenthasaccessto
informationunavailabletothepartiallyobservableagents. Inthispaper,westudyhowtofactorize
thecorrelatedjointfullyobservablepolicyintodecentralizedpoliciesunderpartialobservability.
3 PRELIMINARIES
3.1 DECENTRALIZEDPARTIALLYOBSERVABLEMARKOVDECISIONPROCESSES
In this work, we model a fully cooperative multi-agent game with N agents as a decentralized
partially observable Markov decision process (Dec-POMDP) (Oliehoek & Amato, 2016), which
is formally defined as a tuple G = (N,S,O,O,A,T,Ω,R,γ,ρ ). N = {1,...,N} is a set of
0
agents, s ∈ S denotes the state of the environment and ρ is the distribution of the initial state.
0
A = (cid:81)N Ai is the joint action space, O = (cid:81)N Oi is the set of joint observations. At time
i=1 i=1
step t, each agent i receives an individual partial observation oi ∈ Oi given by the observation
t
function O : (a ,s ) (cid:55)→ P(o |a ,s ) where a ,s and o are the joint actions, states
t t+1 t+1 t t+1 t t+1 t+1
andjointobservationsrespectively. Eachagentiusesastochasticpolicyπi(ai|hi,ωi)conditioned
t t t
on its action-observation history hi = (oi,ai,...,oi ,ai ) and a random seed ωi ∈ Ω to
t 0 0 t−1 t−1 t t
choose an action ai ∈ Ai. A belief state b(s |h ) is a sufficient statistic for joint history h ,
t t t t
as an estimate of the underlying state s . Actions a drawn from joint policy π(a |s ,ω ) condi-
t t t t t
tioned on state s and joint random seed ω = (ω1,...,ωN) change the state according to transi-
t t t t
tion function T : (s ,a1,...,aN) (cid:55)→ P(s |s ,a1,...,aN). All agents share the same reward
t t t t+1 t t t
r = R(s ,a1,...,aN) based on s and a . γ is the discount factor for future rewards. The
gt oal of aget ntst is to mt aximize the ext pectedt total reward, J(π) = E [(cid:80)∞ γtr ], where
s0,a0,... t=0 t
s ∼ρ (s ),a ∼π(a |s ,ω ).
0 0 0 t t t t
3UnderreviewasaconferencepaperatICLR2024
3.2 EQUILIBRIUMNOTIONS
We first define a joint (potentially correlated) policy as π = π1 ⊙π2···⊙πN. We also denote
π−i = π1⊙···πi−1⊙πi+1⊙···⊙πN tobethejointpolicyexcludingtheith agent. Aproduct
policyisdenotedasπ =π1×π2···×πN ifthedistributionofdrawingeachseedωi fordifferent
t
agentsisindependent. WedefinethevaluefunctionVi (s)astheexpectedreturnsunderstates
πi,π−i
thatithagentwillreceiveifallagentsfollowjointpolicyπ =(πi,π−i):
Vi (s)=E [Σ∞ γtr |s =s]. (1)
πi,π−i ai 0:∞∼πi,a− 0:i ∞∼π−i,s1:∞∼T t=0 t 0
Astrategymodificationfortheith agentisamapfi : Ai (cid:55)→ Ai,whichmapsfromtheactionsetto
itself. Wecandefinetheresultingpolicybyapplyingthemaponπiasfi⋄πi.
Withthedefinitionabove,wecanaccordinglydefinethesolutionconcepts.
Definition 1 (ϵ-approximate Nash Equilibrium). A product policy π is an ϵ-approximate Nash
∗
Equilibrium(NE)ifforforalli∈N andanyϵ≥0:
Vi (s)≥maxVi (s)−ϵ. (2)
π ∗i,π∗−i πi πi,π∗−i
Definition2(ϵ-approximateCoarseCorrelatedEquilibrium). Ajointpolicyπ isanϵ-approximate
∗
CoarseCorrelatedEquilibrium(CCE)ifforforalli∈N andanyϵ≥0:
Vi (s)≥maxVi (s)−ϵ. (3)
π ∗i,π∗−i πi πi,π∗−i
TheonlydifferencebetweenDefinition1andDefinition2isthatanNEhastobeaproductpolicy
whileaCCEcanbecorrelated.
Definition3(ϵ-approximateCorrelatedEquilibrium). Ajointpolicyπ isanϵ-approximateCorre-
∗
latedEquilibrium(CE)ifforforalli∈N andanyϵ≥0:
Vi (s)≥maxVi (s)−ϵ. (4)
π ∗i,π∗−i fi (fi⋄π ∗i)⊙π∗−i
ItisalsoworthnotingthatanNEisalwaysaCE,andaCEisalwaysaCCE.
4 METHOD
In this work, we propose AgentMixer to achieve correlated policy factorization. The proposed
methodconsistsoftwomaincomponents: PolicyModifierthatmodelscorrelatedjointfullyobserv-
ablepolicyandIndividual-Global-Consistencythatleveragestheresultingjointpolicyforlearning
theindividualpolicieswhilemitigatingtheasymmetricinformationissue.
4.1 POLICYMODIFIER
Toefficientlyintroducecorrelationamongagents, weproposePolicyModifier, anovelframework
based entirely on multi-layer perceptrons (MLPs) (see Appendix A), which contains two types of
MLP layers (Tolstikhin et al., 2021): agent-mixing MLPs and channel-mixing MLPs. The agent-
mixing MLPs allow inter-agent communication; they operate on each channel of the feature inde-
pendently. The channel-mixing MLPs allow intra-agent information fusion; they operate on each
agent independently. These two types of layers are interleaved to enable the interaction among
agentsandthecorrelatedrepresentationofthejointpolicy. Specifically,agent-andchannel-mixing
canbewrittenasfollows:
H =H +W(2) σ(W(1) LayerNorm(H )),
agent input agent agent input
(5)
H =H +σ(W(1) LayerNorm(H ))W(2) ,
channel agent channel agent channel
whereH isaconcatenationofstatefeaturesandindividualpoliciesfeaturesandWdenotesfully
input
connected layers. Then, the output of PM will be combined with individual policies to generate
thecorrelatedjointpolicy,denotedasPM([πi]N ) = ((f1⋄π1),··· ,(fN ⋄πN)) = (f1⋄π1)⊙
i=1
(f2 ⋄π2)···⊙(fN ⋄πN), wheref denotesastrategymodification. Consequently,PMmapsthe
individualpoliciesintoacorrelatedjointpolicybyintroducingdependenciesamongagents.
4UnderreviewasaconferencepaperatICLR2024
Environment
Individual-Global-Consistency
channel-mixing
N x Policy Modifier
agent-mixing
Agent 1 Agent 2 Agent n
Figure 2: AgentMixer contains two components: 1) Policy Modifier takes the individual partially
observablepoliciesandstateasinputsandproducescorrelatedjointfullyobservablepolicyasout-
puts,and2)Individual-Global-Consistencykeepsthemodeconsistencyamongthejointpolicyand
individualpolicies.
4.2 INDIVIDUALGLOBALCONSISTENCY
With the resulting correlated joint fully observable policy generated by PM, we can easily adopt
differentsingle-agentalgorithmstogetan(sub-)optimalcorrelatedjointfullyobservablepolicy. To
fulfilldecentralizedexecution,wefurtheraskaquestion:
Question1: Canwejustderivethedecentralizedpartiallyobservablepoliciesbydistillingthe
learned(sub-)optimalcorrelatedjointfullyobservablepolicy?
Inthissection,wetakeseveralstepstoprovideanegativeanswertotheaboveresearchquestion.We
begin by defining the joint policy and product policy as π (a|s) and π (a|b) respectively. Let the
θ ϕ
jointoccupancy,ρπ(s,b),asthe(improper)marginalstate-beliefdistributioninducedbyapolicyπ:
ρπ(s,b) = (cid:80)∞ γtP(s = s,b = b|π). Then,themarginalstatedistributionandmarginalbelief
distributionindt= u0 cedbyπt aredet notedasρπ(s) = (cid:82) ρπ(s,b)dbandρπ(b) = (cid:82) ρπ(s,b)dsrespec-
b s
tively. Todistillthejointpolicyπ (a|s)intotheproductpolicyπ (a|b), previouswork(Yeetal.,
θ ϕ
2022) leverage imitation learning (Ross et al., 2011), i.e., optimizing the asymmetric distillation
objective:
E ρπβ(s,b)[D KL(π θ(a|s)∥π ϕ(a|b))],whereπ β(s,b)=βπ θ(a|s)+(1−β)π ϕ(a|b). (6)
π is a mixture of the joint policy π (a|s) and the product policy π (a|b). The coefficient β is
β θ ϕ
annealed to zero during training. This avoids compounding error which grows with time horizon
(Ross&Bagnell,2010).
Wethenshowthattheoptimalproductpolicydefinedbythisobjectivecanbeexpressedasposterior
inferenceoverstateconditionedonthejointpolicy:
Definition4(Implicitproductpolicy). Foranycorrelatedjointfullyobservablepolicyπ andany
θ
productpartiallyobservablebehavioralpolicyπ ,wedefineπˆψ astheimplicitproductpolicyofπ
ψ θ θ
underπ as:
ψ
πˆ θψ =E ρπψ(s|b)[π θ(a|s)], (7)
Suchposteriorinferencehasafixedpoint(Warringtonetal.,2021),i.e.,π = πˆψ,andwereferto
ψ θ
thisproductpolicyastheimplicitproductpolicyofπ ,denotedasπˆ .
θ θ
Implicitproductpolicyisdefinedasaposteriorinferenceprocedure,marginalizingtheconditional
occupancyρπψ(s|b). Sincetheobservations/beliefmaynotcontaininformationtodistinguishtwo
differentlatentstates,theρπψ(s|b)isastochasticdistribution,andtheimplicitproductpolicyisthe
average of the fully observable policy. Suppose a scenario where the agent learns to cross the ice
whileavoidingthepitsinthemiddleoftheice. Thefullyobservablepolicywhichcanobservethe
locationofthepitswillchoosesaferroutesthatavoidthepits,i.e.,bothsidesoftheice. However,
accordingto7,theimplicitpolicythatisnotinformedofthepitlocationswilltakeanaveragepath
5UnderreviewasaconferencepaperatICLR2024
of those safe routes, despite the danger of pits. The key insight is that directly imitating the fully
observablepolicywillcauseasymmetriclearningfailure.
Weshowthatthesolutiontotheasymmetricdistillationobjectivein6isequivalenttotheimplicit
product policy 7 in Appendix B. However, the implicit product policy requires marginalizing the
conditionaloccupancyρπ(s|b),whichisintractable. Therefore,wecanintroduceavariationalim-
plicit product policy, π , as a proxy to the implicit product policy, which can be learned by mini-
η
mizingthefollowingobjective:
E ρπψ(s,b)[D KL(π θ(a|s)∥π η(a|b))]. (8)
Under sufficient expressiveness and exact updates assumptions, by setting π = π , updating 8
ψ η
convergestothefixedpoint,i.e.,theimplicitproductpolicy(seeAppendixB).
We now reason about the asymmetric learning failure. In order to guarantee the optimal product
partially observable policy, the divergence between the joint policy and product policy should be
strictlyzero,whichwedenoteasidentifiability:
Definition 5 (Identifiable policy pair). Given a correlated joint fully observable policy π and a
θ
productpartiallyobservablepolicyπ ,wedefine{π ,π }asanidentifiablepolicypairifandonly
ϕ θ ϕ
ifE ρπϕ(s,b)[D KL(π θ(a|s)∥π ϕ(a|b))]=0.
Identifiablepolicypairsrequirethattheproductpartiallyobservablepolicycanexactlyrecoverthe
correlated joint fully observable policy. Identifiability then requires the optimal correlated joint
fullyobservablepolicyandthecorrespondingimplicitproductpolicytoformanidentifiablepolicy
pair. Usingidentifiability,wecanthenprovethat,givenanoptimalcorrelatedjointfullyobservable
policy,optimizingtheasymmetricdistillationobjectiveisguaranteedtorecoveranoptimalproduct
partiallyobservablepolicy:
Theorem 1 (Convergence of asymmetric distillation). Given an optimal correlated joint fully ob-
servablepolicyπ beingidentifiability,theiterationdefinedby:
θ∗
η
k+1
=argminE ρπηk(s,b)[D KL(π θ∗(a|s)∥π η(a|b))],
(9)
η
convergestoπ (a|b)thatdefinesanoptimalproductpartiallyobservablepolicy,ask →∞.
η∗
Proof. SeeAppendixBfordetailedproof.
Theorem 1 shows that identifiability of the optimal joint policy defines a sufficient condition to
guaranteethethoroughdistillationoftheoptimaljointfullyobservablepolicyintoproductpartially
observablepolicies.Unfortunately,theidentifiabilityimposesastronglimitationontheapplicability
ofasymmetricdistillation.Hereby,wecanconcludeanegativeanswertotheQuestion1.Therefore,
insteadofnaivelyapplyingdistillationonthelearnedjointpolicy,wesimultaneouslylearnthecor-
relatedjointfullyobservablepolicyanditsproductpartiallyobservablecounterpart. Wewillshow
that the interleaving of the two learning processes moves the product partially observable policy
closertoCorrelatedEquilibrium,i.e.,theoptimalproductpartiallyobservablepolicy.
We now use the insight from Theorem 1 and the definition of identifiability to define Individual-
Global-Consistency(IGC),whichkeepsconsistentmodesbetweenproductpartiallyobservablepol-
icyandcorrelatedjointfullyobservablepolicy.
Definition6(IGC). Foracorrelatedjointfullyobservablepolicyπ (a|s),ifthereexistproductpar-
θ
tiallyobservablepolicyπ (a|b)=π (a|h1)×π (a|h2)···×π (a|hN),suchthatthefollowing
ϕ ϕ1 ϕ2 ϕN
holds:
 
Mo(π )
ϕ1
.
Mo(π )= . , (10)
θ  . 
Mo(π )
ϕN
whereMo(·)denotesthemodeofdistribution. Then,wesaythatπ (a|b)satisfyIGC.
ϕ
IGCenablestheactionsthatoccurmostfrequentlyinthejointpolicyandtheproductpolicytobe
equivalent. Crucially,IGCminimizesthedivergencebetweenthetwopolicieswhileallowingcor-
relatedexplorationinthejointpolicy. Surprisingly,onemayfindthatIGCandIGMareequivalent
asmonotonicityandmodeconsistencyaresimilar.
6UnderreviewasaconferencepaperatICLR2024
4.2.1 IMPLEMENTATIONOFIGC
In order to preserve IGC, we adopt the method of disentanglement between exploration and ex-
ploitationtodecomposethejointpolicyintotwocomponents: oneforthemode(exploitation)and
theotherforthedeviation(exploration). Then,IGCcanbeenforcedthroughanequalityconstraint
ontherelationshipbetweenthemodeofjointpolicyandindividualpolicies. Basedonthisdisentan-
glement,agentsareabletocoordinatetheirexplorationthroughthecentralizedpolicy. Inpractice,
we divide the implementation of IGC into two categories: continuous action space and discrete
actionspace.
Continuous Case: In this case, we assume the continuous action policy of agent i as a Gaus-
sian distribution with mean µ and standard deviation σ : π (a|hi) = N(µ (hi),σ2 (hi)).
ϕi ϕi ϕi ϕi ϕi
Since the mode of a Gaussian distribution is equal to the mean, we set the mean of joint pol-
icy as the collection of individual policies while the standard deviation is generated by PM:
π (a|s)=N(([µ ]N ),σ2(s)).
θ ϕi i=1 θ
DiscreteCase: Inthiscase,wedenotethediscreteactionpolicyofagentiasacategoricaldistri-
butionparameterizedbyprobabilitiesα :
ϕi
K
(cid:88)
π (a|hi)=Cat(α (hi))=softmax(α (hi)), αk (hi)=1 (11)
ϕi ϕi ϕi ϕi
k=1
Themodeofacategoricaldistributionisthemostcommoncategory,thecategorywiththehighest
frequency. However,itistrickytopromotecooperativeexplorationwhilepreservingthemodecon-
sistency. Fortunately,Gumbel-Softmaxdistribution(Jangetal.,2017)providesanotherperspective,
whereweexplicitlydisentangleexplorationandmode. Specifically,wedefinethejointpolicyas:
 softmax((ϵ1+logα )/τ1) 
θ ϕ1
.
π = . , (12)
θ  . 
softmax((ϵN +logα )/τN)
θ ϕN
where τ is a temperature hyperparameter and ϵ is sampled using inverse transform sampling by
θ
generating u ∈ (0,1) with sigmoid function and computing ϵ = −log(−log(u )). Note that
θ θ θ
whenthetemperatureapproaches0,thejointpolicydegradestothecollectionofindividualpolicies.
4.3 CONVERGENCEOFAGENTMIXER
Together with PM, we can view the learning of the correlated joint fully observable policy as a
single-agentRLproblemwhereabundantsingle-agentmethodswiththeoreticalguaranteesofcon-
vergence and performance exist. Specifically, AgentMixer is trained end-to-end to maximize the
followingobjective:
(cid:34) ∞ (cid:35)
(cid:88)
J(π θ)=E s∼ρπθ,a∼πθ γtr t ,subjecttoIGC,whereπ θ =PM([π ϕi]N i=1). (13)
t=0
Theorem 2 (Convergence of AgentMixer). The product partially observable policy generated by
AgentMixerisaϵ-CE.
Proof. ForproofseeAppendixB.
WithTheorem3,wearereadytopresentthelearningframeworkofAgentMixer,asillustratedinFig-
ure2,whichconsistsoftwomaincomponents: PolicyModifierandIndividual-Global-Consistency.
Specifically, PM acts as an observer who takes a holistic view and recommends that each agent
follow her instructions. IGC then requires the agents to be obligated to follow the recommenda-
tions they receive. We provide the pseudo-code for AgentMixer in Appendix C. AgentMixer can
benefitfromavarietyofstrongsingle-agentalgorithms,suchasTD3(Fujimotoetal.,2018),PPO
(Schulman et al., 2017b), and SAC (Haarnoja et al., 2019). In this work, our implementation of
AgentMixerfollowsPPO(Schulmanetal.,2017b).
7UnderreviewasaconferencepaperatICLR2024
5 EXPERIMENTS
We compare our method with MAPPO (Yu et al., 2022), HAPPO (Kuba et al., 2021), MAT-Dec
(Wenetal.,2022),MAVEN(Mahajanetal.,2019)andMACPF(Wangetal.,2023). Anextensive
evaluation is performed on both an illustrative matrix game (Lauer & Riedmiller, 2000) and two
popularMARLbenchmarks,Multi-AgentMuJoCo(Pengetal.,2021)(MA-MuJoCo)withcontin-
uous action space and SMAC-v2 (Ellis et al., 2022) with discrete action space. More results and
experimentaldetailsonthesetasksareincludedinAppendixE.
5.1 CLIMBINGMATRIXGAME
The climbing matrix game (Lauer & Riedmiller,
2000) has the payoff shown in the left of Figure 3.
Climbing Game
In this task, there are two agents to select the col- 10 0 6 5
umn and row index of the matrix respectively. The
0
goalistoselectthemaximalelementinthematrix. -30 7 0 AengMixer
MAPPO
Although stateless and with simple action space, 10 HAPPO
MAVEN
Climbingisdifficulttosolveviaindependentlearn- 11 -30 0 MAT-Dec
20 MAT
ing,astheagentsneedtocoordinateamongtwoop- 0.5 1.0 1.5 2.0
Environment Steps 1e5
timaljointactions. TherightofFigure3showsthat
Figure 3: Left: the Climbing matrix game;
the almost compared baselines converge to a local
right: theperformancecomparison.
optimumwhileonlyAgentMixerandMATsuccess-
fullylearntheoptimalpolicy. Thisisreasonable,asinMAPPO,HAPPO,andMAT-Dec,agentsare
fully independent of each other when making decisions, they may fail to coordinate their actions,
which eventually leads to a sub-optimal joint policy. While with an explicit external coordination
signal, MAVEN only finds the optima by chance. For MAT, since it learns a centralized auto-
regressive policy, the second agent thus takes as input the first agent’s action. It is not a surprise
that MAT converges to the highest return due to using a centralized policy. In contrast, thanks to
theintroducedIGCmechanism,AgentMixersuccessfullylearnsfullydecentralizedoptimalpolicies
fromtheoptimalcorrelatedjointpolicygeneratedbythePolicyModifier(PM)module.
Ant-v2 2x4 Ant-v2 4x2 Ant-v2 8x1
2500 AengMixer 1600
MAPPO 2000
2000 H MA AP TPO 1400
1500 MAT-Dec 1500 1200
1000
1000 1000
800
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
1e7 1e7 1e7
Ant-v2 2x4d HalfCheetah-v2 6x1 Humanoid-v2 17x1
800
2500 3000
2000 600
2000
1500
1000 1000 400
500
0 200
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Environment Steps 1e7 Environment Steps 1e7 Environment Steps 1e7
Figure4: PerformancecomparisononmultipleMulti-AgentMuJoCotasks.
5.2 CONTINUOUSACTIONSSPACES: MA-MuJoCo
As in the full observation setting, previous methods have shown near-optimal performance in the
MA-MuJoCotasks(Kubaetal.,2021;Wenetal.,2022),weinsteadsetobsk = 0forallthetasks,
whichmeansthateachagentcanonlyobserveitsownjointinformationandsatisfiesbetterthepartial
observabilitynatureinMARL.WeshowtheperformancecomparisonagainstthebaselinesinFigure
4. WecanseethatAgentMixerenjoyssuperiorperformanceoverthosebaselines. Thesuperiority
ofourmethodishighlightedespeciallyinAnt-v2tasks,wherepartialobservabilityposesacritical
challengeasthelocalobservationsofeachagent(leg)oftheantarequitesimilarandmakeithard
toestimatethenecessarystateinformationforcoordination. Inthesetasks,whileotheralgorithms,
eventhecentralizedMAT,failtolearnanymeaningfuljointpolicies,AgentMixeroutperformsthe
8
nruteR
edosipE
nruteR
edosipE
nruteR
edosipEUnderreviewasaconferencepaperatICLR2024
baselinesbyalargemargin. TheseresultsshowthatAgentMixercaneffectivelyexploitasymmetric
informationtomitigatethechallengesincurredbyseverepartialobservability.
5.3 DISCRETEACTIONSPACES: SMAC-v2
Compared to the StarCraft Multi-Agent
Challenge (SMAC), we instead evalu-
ate our method on the more challeng- 10gen_zerg 5v5 10gen_zerg 10v10
ing SMAC-v2 benchmark which is de-
0.4 0.4
signed with higher randomness. As AengMixer
MAPPO
showninFigure5,wegenerallyobserve 0.2 H MA AP VP EO N 0.2
that AgentMixer achieves comparable M MA AT T-Dec
performance compared with the base- 0.0 MACPF 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
lines. Note that even centralized MAT Environment Steps 1e7 Environment Steps 1e7
performssimilarlytootherdecentralized Figure 5: Comparison of the mean test win rate on
counterparts. SMACv2.
5.4 ABLATIONRESULTS
Ant-v2 2x4 Ant-v2 4x2 Ant-v2 8x1 Ant-v2 2x4d
1600 2000 A M He AAn PPg PPM OOixer 2000 1400 2500
1500 M M AIA A LT T-Dec 1500 1200 2000
1000 1500
1000
1000 800 1000
500 600 500
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
1e7 1e7 1e7 1e7
4000 4000 4000 4000
3000 3000 3000 3000
2000 2000 2000 2000
1000 1000 1000 1000
0 0 0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Environment Steps 1e7 Environment Steps 1e7 Environment Steps 1e7 Environment Steps 1e7
Figure6: AblationsonAnt-v2. Thelargeperformancegapcanbeseenbetweentrainingandtesting
on AIL, which is caused by addressing asymmetric learning failure. Other baselines fail to learn
anyeffectivepolicies,whileAgentMixerobtainssuperiorperformance.
To examine the effectiveness of AgentMixer in addressing asymmetric learning failure, we per-
formablationexperimentsbyaddinganimitationlearningbaseline,asymmetricimitationlearning
(AIL)(Warringtonetal.,2021),whichusesPPO,conditionedonfullstateinformation,tosupervise
learning decentralized policies, conditioned on partial information. As shown in Figure 6, due to
asymmetriclearningfailure,AILperformspoorlyinevaluation, althoughitachievessuperiorper-
formance in training. In contrast, AgentMixer couples the learning of the centralized policy and
decentralized policies such that partially observed policies can perform consistently with the fully
observedpolicy.
6 CONCLUSION
In order to achieve coordination among partially observable agents, this paper presents a novel
framework named AgentMixer which enables correlated policy factorization and provably con-
verges to ϵ-approximate Correlated Equilibrium. AgentMixer consists of two key components: 1)
thePolicyModifierthattakesalltheinitialdecisionsfromindividualagentsandcomposestheminto
a correlated joint policy based on the full state information; 2) the Individual-Global-Consistency
which mitigates the asymmetric learning failure by preserving the consistency between individual
andjointpolicy. Surprisingly,IGCandIGMcanbeconsideredasparallelworksofpolicygradient-
based and value-based methods respectively. We will study the transformation between IGC and
IGM in future work. We extensively evaluate the proposed method on both an illustrative matrix
gameandtwopopularMARLbenchmarks. Theexperimentsdemonstratethatourmethodoutper-
formsstrongbaselinesinmosttasksandachievescomparableperformanceintherest.
9
nruteR
edosipE
lavE
nruteR
edosipE
niarT
etaR
niW
tseTUnderreviewasaconferencepaperatICLR2024
REFERENCES
WendelinBoehmer,VitalyKurin,andShimonWhiteson. Deepcoordinationgraphs. InHalDaume´
IIIandAartiSingh(eds.),Proceedingsofthe37thInternationalConferenceonMachineLearn-
ing,volume119ofProceedingsofMachineLearningResearch,pp.980–991.PMLR,13–18Jul
2020. URLhttps://proceedings.mlr.press/v119/boehmer20a.html.
G.E.P. Box, G.M. Jenkins, G.C. Reinsel, and G.M. Ljung. Time Series Analysis: Forecasting and
Control. Wiley Series in Probability and Statistics. Wiley, 2015. ISBN 9781118674925. URL
https://books.google.fi/books?id=rNt5CgAAQBAJ.
LihengChen,HongyiGuo,YaliDu,FeiFang,HaifengZhang,WeinanZhang,andYongYu. Signal
instructed coordination in cooperative multi-agent reinforcement learning. In Jie Chen, Je´roˆme
Lang, Christopher Amato, and Dengji Zhao (eds.), Distributed Artificial Intelligence, pp. 185–
205,Cham,2022.SpringerInternationalPublishing. ISBN978-3-030-94662-3.
Christian Schro¨der de Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip H. S.
Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft
multi-agent challenge? CoRR, abs/2011.09533, 2020. URL https://arxiv.org/abs/
2011.09533.
Benjamin Ellis, Skander Moalla, Mikayel Samvelyan, Mingfei Sun, Anuj Mahajan, Jakob N. Fo-
erster, and Shimon Whiteson. Smacv2: An improved benchmark for cooperative multi-agent
reinforcementlearning,2022.
E.A. Feinberg and A. Shwartz. Handbook of Markov Decision Processes: Methods and Ap-
plications. International Series in Operations Research & Management Science. Springer
US, 2012. ISBN 9781461508052. URL https://books.google.fi/books?id=
TpwKCAAAQBAJ.
Wei Fu, Chao Yu, Zelai Xu, Jiaqi Yang, and Yi Wu. Revisiting some common practices in coop-
erative multi-agentreinforcement learning. In KamalikaChaudhuri, StefanieJegelka, Le Song,
CsabaSzepesvari,GangNiu,andSivanSabato(eds.),Proceedingsofthe39thInternationalCon-
ference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp.
6863–6877. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/
fu22d.html.
Scott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in
actor-critic methods. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th In-
ternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning
Research, pp. 1587–1596. PMLR, 10–15 Jul 2018. URL https://proceedings.mlr.
press/v80/fujimoto18a.html.
Amy Greenwald and Keith Hall. Correlated-q learning. In Proceedings of the Twentieth Interna-
tional Conference on International Conference on Machine Learning, ICML’03, pp. 242–249.
AAAIPress,2003. ISBN1577351894.
Shangding Gu, Jakub Grudzien Kuba, Yuanpei Chen, Yali Du, Long Yang, Alois Knoll, and
Yaodong Yang. Safe multi-agent reinforcement learning for multi-robot control. Artifi-
cial Intelligence, 319:103905, 2023. ISSN 0004-3702. doi: https://doi.org/10.1016/j.artint.
2023.103905. URL https://www.sciencedirect.com/science/article/pii/
S0004370223000516.
TuomasHaarnoja,AurickZhou,KristianHartikainen,GeorgeTucker,SehoonHa,JieTan,Vikash
Kumar, Henry Zhu, Abhishek Gupta, Pieter Abbeel, and Sergey Levine. Soft actor-critic algo-
rithmsandapplications,2019.
JunlingHuandMichaelP.Wellman. Nashq-learningforgeneral-sumstochasticgames. J.Mach.
Learn.Res.,4(null):1039–1069,dec2003. ISSN1532-4435.
Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In
InternationalConferenceonLearningRepresentations,2017. URLhttps://openreview.
net/forum?id=rkE3y85ee.
10UnderreviewasaconferencepaperatICLR2024
WoojunKim,WhiyoungJung,MyungsikCho,andYoungchulSung. Amaximummutualinforma-
tionframeworkformulti-agentreinforcementlearning,2020.
Jakub Grudzien Kuba, Ruiqing Chen, Muning Wen, Ying Wen, Fanglei Sun, Jun Wang, and
Yaodong Yang. Trust region policy optimisation in multi-agent reinforcement learning. CoRR,
abs/2109.11251,2021. URLhttps://arxiv.org/abs/2109.11251.
Martin Lauer and Martin A. Riedmiller. An algorithm for distributed reinforcement learning in
cooperativemulti-agentsystems. InProceedingsoftheSeventeenthInternationalConferenceon
MachineLearning,ICML’00,pp.535–542,SanFrancisco,CA,USA,2000.MorganKaufmann
PublishersInc. ISBN1558607072.
Chuming Li, Jie Liu, Yinmin Zhang, Yuhong Wei, Yazhe Niu, Yaodong Yang, Yu Liu, and Wanli
Ouyang. Ace: Cooperative multi-agent q-learning with bidirectional action-dependency. Pro-
ceedingsoftheAAAIConferenceonArtificialIntelligence,37(7):8536–8544,Jun.2023. doi: 10.
1609/aaai.v37i7.26028. URL https://ojs.aaai.org/index.php/AAAI/article/
view/26028.
Sheng Li, Jayesh K Gupta, Peter Morales, Ross Allen, and Mykel J Kochenderfer. Deep implicit
coordination graphs for multi-agent reinforcement learning. arXiv preprint arXiv:2006.11438,
2020.
AlexTong Lin, MarkDebord, KatiaEstabridis, Gary Hewer, GuidoMontufar, andStanley Osher.
Decentralizedmulti-agentsbyimitationofacentralizedcontroller.InJoanBruna,JanHesthaven,
andLenkaZdeborova(eds.),Proceedingsofthe2ndMathematicalandScientificMachineLearn-
ingConference,volume145ofProceedingsofMachineLearningResearch,pp.619–651.PMLR,
16–19Aug2022. URLhttps://proceedings.mlr.press/v145/lin22a.html.
Michael L. Littman. Markov games as a framework for multi-agent reinforcement learning. In
William W. Cohen and Haym Hirsh (eds.), Machine Learning Proceedings 1994, pp. 157–
163. Morgan Kaufmann, San Francisco (CA), 1994. ISBN 978-1-55860-335-6. doi: https:
//doi.org/10.1016/B978-1-55860-335-6.50027-1. URL https://www.sciencedirect.
com/science/article/pii/B9781558603356500271.
Ryan Lowe, YI WU, Aviv Tamar, Jean Harb, OpenAI Pieter Abbeel, and Igor Mordatch. Multi-
agent actor-critic for mixed cooperative-competitive environments. In I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,
2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/
file/68a9750337a418a86fe06c1991a1d64c-Paper.pdf.
JinmingMaandFengWu. Feudalmulti-agentdeepreinforcementlearningfortrafficsignalcontrol.
In Amal El Fallah Seghrouchni, Gita Sukthankar, Bo An, and Neil Yorke-Smith (eds.), Pro-
ceedings of the 19th International Conference on Autonomous Agents and Multiagent Systems,
AAMAS ’20, Auckland, New Zealand, May 9-13, 2020, pp. 816–824. International Foundation
for Autonomous Agents and Multiagent Systems, 2020. doi: 10.5555/3398761.3398858. URL
https://dl.acm.org/doi/10.5555/3398761.3398858.
Anuj Mahajan, Tabish Rashid, Mikayel Samvelyan, and Shimon Whiteson. Maven: Multi-agent
variational exploration. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche´-Buc, E. Fox,
and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Cur-
ranAssociates,Inc.,2019.URLhttps://proceedings.neurips.cc/paper_files/
paper/2019/file/f816dc0acface7498e10496222e9db10-Paper.pdf.
M. Maschler, S. Zamir, and E. Solan. Game Theory. Cambridge University Press, 2013. ISBN
9781107005488. URLhttps://books.google.fi/books?id=lqwzqgvhwXsC.
Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs.
SpringerPublishingCompany,Incorporated,1stedition,2016. ISBN3319289276.
Bei Peng, Tabish Rashid, Christian Schroeder de Witt, Pierre-Alexandre Kamienny, Philip Torr,
Wendelin Boehmer, and Shimon Whiteson. Facmac: Factored multi-agent centralised policy
11UnderreviewasaconferencepaperatICLR2024
gradients. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan
(eds.), AdvancesinNeuralInformationProcessingSystems, volume34, pp.12208–12221.Cur-
ranAssociates,Inc.,2021.URLhttps://proceedings.neurips.cc/paper_files/
paper/2021/file/65b9eea6e1cc6bb9f0cd2a47751a186f-Paper.pdf.
Tabish Rashid, Gregory Farquhar, Bei Peng, and Shimon Whiteson. Weighted qmix: Expand-
ing monotonic value function factorisation for deep multi-agent reinforcement learning. In
H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neu-
ral Information Processing Systems, volume 33, pp. 10199–10210. Curran Associates, Inc.,
2020a. URL https://proceedings.neurips.cc/paper_files/paper/2020/
file/73a427badebe0e32caa2e1fc7530b7f3-Paper.pdf.
TabishRashid,MikayelSamvelyan,ChristianSchroederDeWitt,GregoryFarquhar,JakobFoerster,
andShimonWhiteson.Monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcement
learning. J.Mach.Learn.Res.,21(1),jan2020b. ISSN1532-4435.
Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Yee Whye
Teh and Mike Titterington (eds.), Proceedings of the Thirteenth International Conference on
Artificial Intelligence and Statistics, volume 9 of Proceedings of Machine Learning Research,
pp. 661–668, Chia Laguna Resort, Sardinia, Italy, 13–15 May 2010. PMLR. URL https:
//proceedings.mlr.press/v9/ross10a.html.
StephaneRoss, GeoffreyGordon, andDrewBagnell. Areductionofimitationlearningandstruc-
turedpredictiontono-regretonlinelearning. InGeoffreyGordon,DavidDunson,andMiroslav
Dud´ık (eds.), Proceedings of the Fourteenth International Conference on Artificial Intelligence
andStatistics,volume15ofProceedingsofMachineLearningResearch,pp.627–635,FortLaud-
erdale, FL, USA, 11–13 Apr 2011. PMLR. URL https://proceedings.mlr.press/
v15/ross11a.html.
LarrySamuelson. Evolutionarygamesandequilibriumselection,volume1. MITpress,1997.
Christian Schroeder de Witt, Jakob Foerster, Gregory Farquhar, Philip Torr, Wendelin Boehmer,
and Shimon Whiteson. Multi-agent common knowledge reinforcement learning. In H. Wal-
lach, H. Larochelle, A. Beygelzimer, F. d'Alche´-Buc, E. Fox, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.,
2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/
file/f968fdc88852a4a3a27a81fe3f57bfc5-Paper.pdf.
John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region
policyoptimization.InFrancisBachandDavidBlei(eds.),Proceedingsofthe32ndInternational
ConferenceonMachineLearning,volume37ofProceedingsofMachineLearningResearch,pp.
1889–1897, Lille, France, 07–09 Jul 2015. PMLR. URL https://proceedings.mlr.
press/v37/schulman15.html.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimization algorithms. CoRR, abs/1707.06347, 2017a. URL http://arxiv.org/abs/
1707.06347.
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. Proximalpolicy
optimizationalgorithms,2017b.
Shai Shalev-Shwartz, Shaked Shammah, and Amnon Shashua. Safe, multi-agent, reinforcement
learningforautonomousdriving. CoRR,abs/1610.03295,2016. URLhttp://arxiv.org/
abs/1610.03295.
Junjie Sheng, Wenhao Li, Bo Jin, Hongyuan Zha, Jun Wang, and Xiangfeng Wang. Negotiated
reasoning: Onprovablyaddressingrelativeover-generalization,2023.
KyunghwanSon,DaewooKim,WanJuKang,DavidEarlHostallero,andYungYi.QTRAN:Learn-
ing to factorize with transformation for cooperative multi-agent reinforcement learning. In Ka-
malikaChaudhuriandRuslanSalakhutdinov(eds.), Proceedingsofthe36thInternationalCon-
ference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp.
5887–5896. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/
son19a.html.
12UnderreviewasaconferencepaperatICLR2024
JianyuSu,StephenAdams,andPeterBeling. Value-decompositionmulti-agentactor-critics. Pro-
ceedings of the AAAI Conference on Artificial Intelligence, 35(13):11352–11360, May 2021.
doi: 10.1609/aaai.v35i13.17353. URL https://ojs.aaai.org/index.php/AAAI/
article/view/17353.
PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,Max
Jaderberg,MarcLanctot,NicolasSonnerat,JoelZ.Leibo,KarlTuyls,andThoreGraepel. Value-
decompositionnetworksforcooperativemulti-agentlearning,2017.
Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas
Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic,
and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision. In M. Ranzato,
A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neu-
ral Information Processing Systems, volume 34, pp. 24261–24272. Curran Associates, Inc.,
2021. URL https://proceedings.neurips.cc/paper_files/paper/2021/
file/cba0a4ee5ccd02fda0fe3f9a3e7b89fe-Paper.pdf.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. Von
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Ad-
vances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.,
2017. URL https://proceedings.neurips.cc/paper_files/paper/2017/
file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
AaronWalsman,MuruZhang,SanjibanChoudhury,DieterFox,andAliFarhadi. Impossiblygood
expertsandhowtofollowthem. InTheEleventhInternationalConferenceonLearningRepre-
sentations,2023. URLhttps://openreview.net/forum?id=sciA_xgYofB.
Jiangxing Wang, Deheng Ye, and Zongqing Lu. More centralized training, still decentralized ex-
ecution: Multi-agent conditional policy factorization. In The Eleventh International Confer-
ence on Learning Representations, 2023. URL https://openreview.net/forum?id=
znLlSgN-4S0.
JianhaoWang,ZhizhouRen,TerryLiu,YangYu,andChongjieZhang. {QPLEX}: Duplexdueling
multi-agentq-learning. InInternationalConferenceonLearningRepresentations, 2021a. URL
https://openreview.net/forum?id=Rcmk0xxIQV.
YihanWang,BeiningHan,TonghanWang,HengDong,andChongjieZhang. {DOP}: Off-policy
multi-agentdecomposedpolicygradients. InInternationalConferenceonLearningRepresenta-
tions,2021b. URLhttps://openreview.net/forum?id=6FqKiVAdI3Y.
AndrewWarrington,JonathanWLavington,AdamScibior,MarkSchmidt,andFrankWood.Robust
asymmetriclearninginpomdps. InMarinaMeilaandTongZhang(eds.),Proceedingsofthe38th
InternationalConferenceonMachineLearning,volume139ofProceedingsofMachineLearning
Research,pp.11013–11023.PMLR,18–24Jul2021. URLhttps://proceedings.mlr.
press/v139/warrington21a.html.
Muning Wen, Jakub Kuba, Runji Lin, Weinan Zhang, Ying Wen, Jun Wang, and Yaodong
Yang. Multi-agent reinforcement learning is a sequence modeling problem. In S. Koyejo,
S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neu-
ral Information Processing Systems, volume 35, pp. 16509–16521. Curran Associates, Inc.,
2022. URL https://proceedings.neurips.cc/paper_files/paper/2022/
file/69413f87e5a34897cd010ca698097d0a-Paper-Conference.pdf.
YingWen,YaodongYang,RuiLuo,JunWang,andWeiPan. Probabilisticrecursivereasoningfor
multi-agent reinforcement learning. In International Conference on Learning Representations,
2019. URLhttps://openreview.net/forum?id=rkl6As0cF7.
JianingYe,ChenghaoLi,JianhaoWang,andChongjieZhang. Towardsglobaloptimalityincoop-
erativemarlwithsequentialtransformation. arXivpreprintarXiv:2207.11143,2022.
13UnderreviewasaconferencepaperatICLR2024
Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and YI WU.
The surprising effectiveness of ppo in cooperative multi-agent games. In S. Koyejo, S. Mo-
hamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Infor-
mation Processing Systems, volume 35, pp. 24611–24624. Curran Associates, Inc., 2022.
URL https://proceedings.neurips.cc/paper_files/paper/2022/file/
9c1535a02f0ce079433344e14d910597-Paper-Datasets_and_Benchmarks.
pdf.
Tianhao Zhang, Yueheng Li, Chen Wang, Guangming Xie, and Zongqing Lu. Fop: Factorizing
optimal joint policy of maximum-entropy multi-agent reinforcement learning. In Marina Meila
andTongZhang(eds.),Proceedingsofthe38thInternationalConferenceonMachineLearning,
volume139ofProceedingsofMachineLearningResearch,pp.12491–12500.PMLR,18–24Jul
2021. URLhttps://proceedings.mlr.press/v139/zhang21m.html.
A DETAIL STRUCTURE OF POLICY MODIFIER
Figure 7 depicts the macro-structure of Policy Modifier. It accepts the state and policies of agents
asinput. Specifically,PolicyModifiertwoMLPblocks. Thefirstoneistheagent-mixingMLPs: it
actsoncolumnsofinput. Thesecondoneisthechannel-mixingMLP:itactsonrowsoftheoutput
ofagent-mixingMLPs.
Figure 7: Policy Modifier consists of policy embedding layer, agent-mixing MLP and channel-
mixingMLP.
B ADDITIONAL PROOFS
B.1 PROOFOFLEMMA1
Lemma1(Asymmetricdistillationsolution). theoremadForanycorrelatedjointfullyobservable
policy π and fixed product partially observable behavioral policy π , the implicit product policy
θ ψ
πˆψ,definedinDefinition4,minimizestheasymmetricdistillationobjective:
θ
πˆ θψ =argminE ρπψ(s,b)[D KL(π θ(a|s)∥π(a|b))]. (14)
π
14UnderreviewasaconferencepaperatICLR2024
Proof. Expandingtheright-handside:
argminE ρπψ(s,b)[D KL(π θ(a|s)∥π(a|b))]
π
(cid:20)(cid:90) (cid:90) π (a|s) (cid:21)
=argminE ρπψ(b) π θ(a|s)log( πθ
(a|b)
)daρπψ(s|b)ds ,
π s a
(cid:20)(cid:90) (cid:21) (cid:20)(cid:90) (cid:90) (cid:21)
=argminE ρπψ(b) H(π θ(·|s))ρπψ(s|b)ds −E ρπψ(b) π θ(a|s)log(π(a|b))daρπψ(s|b)ds ,
π s s a
whereH(·)istheentropyfunction,
(cid:20)(cid:90) (cid:21)
=argminconst−E ρπψ(b) πˆ θψ(a|b)log(π(a|b))da ,
π a
notethatwearefreetosettheconst,solongasitremainsindependentofπ,
(cid:20)(cid:90) (cid:90) (cid:21)
=argminE ρπψ(b) πˆ θψ(a|b)log(πˆ θψ(a|b))da− πˆ θψ(a|b)log(π(a|b))da ,
π a a
(cid:104) (cid:16) (cid:17)(cid:105)
=argminE ρπψ(b) D KL πˆ θψ(a|b)∥π(a|b) .
π
(15)
Henceweconcludetheproof.
B.2 PROOFOFCONVERGENCEOFITERATIVEVARIATIONALAPPROXIMATION
We first introduce an assumption which simply states that the variational family is sufficiently ex-
pressive such that the implicit product policy can be recovered, and the implicit product policy
is sufficiently expressive such that the optimal product partially observable policy can actually be
found.
Assumption 1 (Sufficiency of Variational Representations). We assume that for any product be-
havioral policy, π , the variational family is sufficiently expressive such that any implicit product
ψ
policy,πˆ ,canbeexactlyrecoveredundertheoccupancyinducedbytheproductbehavioralpolicy.
θ
Wealsoassumethatthereisanimplicitproductpolicy, πˆ , suchthatanoptimalproductpartially
θ
observablepolicycanberepresented,andthusthereisavariationalimplicitproductpolicythatcan
representtheoptimalproductpartiallyobservablepolicyunderρπψ(b).
Wethenintroducethelemmawhichshowsthatthesolutiontoaniterativeprocedureactuallycon-
vergestothesolutionofasingleequivalent“static”optimizationproblem. Thislemmaallowsusto
solvethechallengingoptimizationusingasimpleiterativeprocedure.
Lemma 2 (Convergence of Iterative Variational Approximation). Given the implicit product pol-
icy πˆ and the corresponding variational approximation to πˆ , π , then under Assumption 1, the
θ θ η
iterativeprocedure:
η
k+1
=argminE ρπηk(b)[D KL(πˆ θ(a|b)∥π η(a|b))],withk →∞,
(16)
η
convergestothesolutiontotheoptimizationproblem:
η∗ =argminE [D (πˆ (a|b)∥π (a|b))].
ρπη(b) KL θ η (17)
η
Proof. Webeginbyexpressingthetotalvariationbetweenρπη∗(b)andρπ ηk(b)atthekthiteration:
D TV(ρπη∗(b)∥ρπ ηk(b))=sup(cid:12) (cid:12)(cid:80)∞ t=0γtP(b
t
=b|π η∗)−(cid:80)∞ t=0γtP(b
t
=b|π ηk)(cid:12) (cid:12),
b
=sup|(cid:80)k γtP(b =b|π )+(cid:80)∞ γtP(b =b|π ) (18)
t=0 t η∗ t=k+1 t η∗
b
−(cid:80)k γtP(b =b|π )−(cid:80)∞ γtP(b =b|π )|.
t=0 t ηk t=k+1 t ηk
Wecanthennotethatatthekth iteration,themarginalbeliefdistributionsinducedbyπ andπ
η∗ ηk
overthefirstkiterationmustbeidenticalastheunderlyingdynamicsarethesameattheinitialstate
15UnderreviewasaconferencepaperatICLR2024
andbeliefstateandwehaveexactlyminimizedtheD (π ∥ π ). Withtheassumptionthatthe
KL η∗ η
maximumvariationbetweenthedensitiesisboundedbyC,wehave:
sup|(cid:80)k γtP(b =b|π )+(cid:80)∞ γtP(b =b|π )
t=0 t η∗ t=k+1 t η∗
b
−(cid:80)k γtP(b =b|π )−(cid:80)∞ γtP(b =b|π )|,
t=0 t ηk t=k+1 t ηk
=sup|(cid:80)∞ γt(P(b =b|π )−P(b =b|π ))|,
t=k+1 t η∗ t ηk
b
≤sup|(cid:80)∞ γtC|, (19)
t=k+1
b
1 1−γk+1
=C( − ),
1−γ 1−γ
γk+1
=C =O(γk).
1−γ
Hence, asγ ∈ [0,1), thetotalvariationbetweenπ andπ convergestozeroask → ∞. With
η∗ ηk
thisresultandtheexpressivenessassumption,wecompletetheproof.
SimilartotheproofofLemma2,wecanderivethefollowingresult:
argminE ρπψ(b)[D KL(πˆ θ(a|b)∥π η(a|b))]=argminE ρπˆθ(b)[D KL(πˆ θ(a|b)∥π η(a|b))].
(20)
η η
Thisresultallowsustoexchangethedistributionunderwhichwetakeexpectations.
B.3 PROOFOFTHEOREM1
WithAssumption1,Lemma2,andtheidentifiabilitycondition,wearereadytoverifytheconver-
genceofasymmetricdistillation.
Theorem 1 (Convergence of asymmetric distillation). Given an optimal correlated joint fully ob-
servablepolicyπ beingidentifiability,theiterationdefinedby:
θ∗
η
k+1
=argminE ρπηk(s,b)[D KL(π θ∗(a|s)∥π η(a|b))],
(9)
η
convergestoπ (a|b)thatdefinesanoptimalproductpartiallyobservablepolicy,ask →∞.
η∗
Proof. Webeginbyconsideringthelimitingbehaviorask →∞:
η∗ = lim argminE ρπηk(s,b)[D KL(π θ∗(a|s)∥π η(a|b))],
k→∞ η
= lim argminE ρπηk(b)[D KL(πˆ θ∗(a|b)∥π η(a|b))],(Lemma1)
k→∞ η
=argminE ρπη(b)[D KL(πˆ θ∗(a|b)∥π η(a|b))],(Lemma2)
(21)
η
=argminE [D (πˆ (a|b)∥π (a|b))],(exchangethedistribution)
ρπˆθ∗(b) KL θ∗ η
η
=argminE ρπϕ∗(b)[D KL(π ϕ∗(a|b)∥π η(a|b))].(identifiability)
η
Finally,underAssumption1,theexpectedKLdivergencecanbeexactlyzero,whichcompletesthe
proof.
B.4 PROOFOFTHEOREM2
Theorem 2 (Convergence of AgentMixer). The product partially observable policy generated by
AgentMixerisaϵ-CE.
Proof. SincePMmodifiesthedecentralizedpoliciesandgeneratesthecorrelatedjointpolicy, i.e.,
π = ((f1 ⋄ π ),··· ,(fN ⋄ π )), the RL procedure mentioned in 13 can be regarded as a
θ θ ϕ1 θ ϕN
single-agent RL problem. By leveraging Theorem 1 in TRPO (Schulman et al., 2015), we can
16UnderreviewasaconferencepaperatICLR2024
concludethatasequence(π )∞ ofjointpoliciesupdatedby13hasthemonotonicimprovement
θk k=1
property, i.e., J(π ) ≥ J(π ). According to Bolzano-Weierstrass Theorem, the sequence of
θk+1 θk
policies(π θk)∞
k=1
existsatleastonesub-optimalpointπ θ∗. Letπ θ¯betheoptimaljointpolicyand
ϵ=V (s)−V (s)≥0. Giventhevaluefunctiondefinedin1,wehave:
π θ¯ πθ∗
V (s)+ϵ≥V (s),∀π . (22)
πθ∗ πθ θ
Sinceoptimizingπ isactuallyoptimizingf ,thenwecanobtain:
θ θ
V (s)+ϵ≥maxV (s).
((f θ1 ∗⋄π ϕ1),···,(f θN ∗⋄π ϕN))
fθ
((f θ1⋄π ϕ1),···,(f θN⋄π ϕN)) (23)
ApplyingIGC,whichkeepsthemodeconsistencybetweenjointpolicyandproductpolicy,yields:
((f1 ⋄π ),··· ,(fN ⋄π ))=(π ,··· ,π ). (24)
θ∗ ϕ1 θ∗ ϕN ϕ1 ϕN
Finally,byplugging24into23:
V (s)≥maxV (s)−ϵ.
(π ϕ1,···,π ϕN)
fθ
((f θ1⋄π ϕ1),···,(f θN⋄π ϕN)) (25)
whichisexactlytheϵ-CEdefinedinDefinition3.
C PSEUDO-CODE FOR AGENTMIXER
Thepseudo-codeofourmethodisshowninAlgorithmC.
Algorithm1AgentMixer
INITIALIZE Decentralized partially observable policies {π ,...,π }, a single agent algo-
ϕ1 ϕN
rithmA.
//Constructthejointpolicy:
π =PM([π ]N ),subjectedtoIGC.
θ ϕi i=1
RunAonπ .
θ
RETURN{π ,...,π }.
ϕ1 ϕN
D BASELINES AND MORE EXPERIMENTS
We compare our method with the baselines below including both algorithms with state-of-the-art
performanceandmethodsdesignedspecificallytotacklethecoordinationproblems.
MAPPO(Yuetal.,2022)appliesPPO(Schulmanetal.,2017a)tomulti-agentsettingsandutilizes
CTDEtolearncriticsbasedontheglobalstateinordertostabilizethepolicygradientestimation.
Although with simple techniques, MAPPO has achieved tremendous empirical success in various
multi-agentdomainsandcanbeastrongbaselineforourmethod.
HAPPO (Kuba et al., 2021) performs sequential policy updates by utilizing other agents’ newest
policyundertheCTDEframeworkandprovablyobtainsthemonotonicpolicyimprovementguar-
anteeasinsingle-agentPPO.
MAT-Dec (Wen et al., 2022) is the decentralized version of MAT which models the multi-agent
decision process as a sequence-to-sequence generation problem with powerful transformer archi-
tecture (Vaswani et al., 2017). MAT-Dec relaxes the restriction of using other agents’ actions but
remains taking the full observations from other agents. Therefore, we remind that MAT-Dec uses
full state information in experiments while our method and other baselines are limited by partial
observation.
MAVEN (Mahajan et al., 2019) is proposed to improve the exploration of QMIX by introducing
a latent space for hierarchical control. Compared to QMIX, MAVEN takes further advantage of
CTDEthroughacommittedexplorationstrategy.
MACPF (Wang et al., 2023) extends SAC (Haarnoja et al., 2019) into multi-agent settings and
explicitlyintroducesauto-regressivedependencyamongagents.
17UnderreviewasaconferencepaperatICLR2024
AIL (Wang et al., 2023) naively distills partially observable agents’ policies from the fully ob-
servablecentralizedpolicy. Althoughshowingsignificanttrainingperformance,itsuffersfromthe
asymmetriclearningfailureproblemandfailstoperformwellduringexecutionwithpartialobser-
vation.
We summarize the different CTDE settings in Table 1. Note that although MAT and MAT-Dec
sometimes show better performance than other methods, they take the full state information even
duringexecution.
Algorithm MAPPO HAPPO MAT MAT-Dec MAVEN MACPF AIL Ours
P.Ob. (execution) ✓ ✓ ✗ ✗ ✓ ✓ ✓ ✓
C.Value(training) ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
C.Policy(training) ✗ ✗ ✗ ✗ ✗ ✗ ✓ ✓
Table1: ThistablecomparesdifferentsettingsofCTDEusedinbaselineswithourmethods,where
P.Ob. denotespartialobservation,C.ValuemeanscentralizedvalueandC.Policyrepresentscen-
tralizedpolicy. Notethatonlymethodswithpartialobservationduringexecutionarefairforcom-
parison. Although all the methods take advantage of CTDE by using a centralized value during
training,onlyourmethodandAILfurtheremploycentralizedpolicyfortraining. Ourmethodtack-
lestheasymmetriclearningfailureprobleminAILandhenceshowsbetterperformance.
D.1 MOREEXPERIMENTSONMA-MuJoCo
Inspiredbymujocotasksinthesingle-agentRLrealm,MA-MuJoCosplitsthejointsofrobotsinto
differentagentstoenabledecentralizedcontrolforMARLresearch. MA-MuJoCoallowsdifferent
observationsettingsbychangingtheparameterofobskwhichcontrolsthenumberofneighborjoints
eachagentcanobserve.
We show the experiment results on more MA-MuJoCo tasks in Figure 8. Extended ablation study
resultsareshowninFigure9.
HalfCheetah-v2 2x3 HalfCheetah-v2 3x2 Walker2d-v2 2x3
6000 2000
AengMixer
MAPPO
HAPPO 1500
4000 MAT 4000
MAT-Dec
1000
2000 2000
500
0 0 0
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
1e7 1e7 1e7
Walker2d-v2 3x2 Walker2d-v2 6x1 HumanoidStandup-v2 17x1
1250 800
1000 125000
600
750 100000
500 400 75000
250 200 50000
0 25000
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Environment Steps 1e7 Environment Steps 1e7 Environment Steps 1e7
Figure8: PerformancecomparisononmultipleMulti-AgentMuJoCotasks.
D.2 MOREEXPERIMENTSONSMAC-V2
The original StarCraft Multi-Agent Challenge (SMAC) has been shown to be not difficult enough,
asanopen-looppolicyconditionedonlyonthetimestepcanachievenon-trivialwinratesformany
scenarios. To address these shortcomings, a new benchmark, SMACv2 was proposed to address
SMAC’slackofstochasticity.
We compare a baseline, MAPPO FULL, conditioned on full state information during evaluation.
TheresultsinFig. 12showthatpartiallyobservablepoliciesachievesimilarperformanceasfully
18
nruteR
edosipE
nruteR
edosipEUnderreviewasaconferencepaperatICLR2024
HalfCheetah-v2 2x3 HalfCheetah-v2 3x2 HalfCheetah-v2 6x1 Walker2d-v2 2x3
AengMixer 6000 2000
MAPPO 3000
4000 H MA AP TPO 4000 1500
MAT-Dec 2000
AIL 1000
2000 2000 1000
500
0
0 0 0
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
1e7 1e7 1e7 1e7
Walker2d-v2 3x2 Walker2d-v2 6x1 Humanoid-v2 17x1 HumanoidStandup-v2 17x1
1250 800 800
1000 600 125000
750 600 100000
400
500 400 75000
250 200 50000
0 0 200 25000
0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00 0.00 0.25 0.50 0.75 1.00
Environment Steps 1e7 Environment Steps 1e7 Environment Steps 1e7 Environment Steps 1e7
Figure9: AblationsonmultipleMulti-AgentMuJoCotasks.
HalfCheetah-v2 2x3 HalfCheetah-v2 3x2 HalfCheetah-v2 6x1
6000 6000 4000
4000 AgentMixer 4000
M HAA PP PP OO 2000
2000 MAT 2000
MAT_DEC
0 A MI AL PPO_FULL 0 0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Environment Steps 1e7 Environment Steps 1e7 Environment Steps 1e7
Figure10: AblationsonmultipleMulti-AgentMuJoCotasks.
10gen_protoss 5v5 10gen_protoss 10v10 10gen_zerg 10v11 10gen_terran 5v5
0.6
0.4 0.3
0.4 AengMixer 0.3 0.2 0.4 0.2 M H MAA AP VP PP EOO N 0.2 0.1 0.2
MAT 0.1
MAT-Dec
0.0 MACPF 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Environment Steps 1e7 Environment Steps 1e7 Environment Steps 1e7 Environment Steps 1e7
Figure11: ComparisonofthemeantestwinrateonSMACv2.
19
nruteR
edosipE
nruteR
edosipE
etaR
niW tseT
nruteR
edosipEUnderreviewasaconferencepaperatICLR2024
observablepolicies. Thisdemonstratesthatglobalinformationisnotimportantforlearninginthe
SMACv2domain.
10gen_protoss 5v5 10gen_protoss 10v10 10gen_zerg 5v5
0.6
0.6 0.5
0.4
0.4 A Me An Pg PM Oixer 0.3 0.4
HAPPO
0.2 M MA ATVEN 0.2 0.2
MAT-Dec 0.1
MACPF
0.0 MAPPO_FULL 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
1e7 1e7 1e7
10gen_zerg 10v10 10gen_zerg 10v11 10gen_terran 5v5
0.5 0.3
0.4 0.4
0.3 0.2
0.2 0.2 0.1
0.1
0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Environment Steps 1e7 Environment Steps 1e7 Environment Steps 1e7
Figure12: AblationsdemonstratingtheeffectoffullstateinformationonSMACv2.
D.3 MOREEXPERIMENTSONPREDATOR-PREY
WeuseanenvironmentsimilartothatdescribedbyLietal.(2020)whereagentsarecontrolledto
capture prey. If a prey is captured, the agents receive a reward of 10. However, the environment
penalizes any single-agent attempt to capture prey with a penalty. Figure 13 shows the average
returnfortestepisodesforvaryingpenalties.
Predator-Prey Penalty = 0 Predator-Prey Penalty = -1
0.6
2 0.4
1 AengMixer 0.2
MAPPO
HAPPO
MAT 0.0
0 MAT-Dec
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Environment Steps 1e6 Environment Steps 1e6
Figure13: PerformancecomparisononPredator-Preywithdifferentpenaltiesforsingle-agentcap-
tureattempt.
E HYPER-PARAMETERS
Forafaircomparison,theimplementationofAgentMixerandthebaselinesarebasedontheimple-
mentationofMAPPO.Wekeepallhyper-parametersunchangedattheoriginbest-performingstatus.
Theproposedmethodandcomparedbaselinesareimplementedintoparameterindependentversion
exceptMATandMAT-Dec. Thecommonanddifferenthyper-parametersusedforthebaselinesand
AgentMixeracrossalldomainsarelistedinTable2-8respectively.
E.1 COMMONHYPER-PARAMETERS
Welistthecommonhyper-parametersacrossallthedomainsinTable2-5.
E.2 MATRIXGAMES
Welistthehyper-parametersusedinmatrixgamesinTable6.
20
etaR
niW
tseT
etaR
niW
tseT
nruteR
edosipEUnderreviewasaconferencepaperatICLR2024
Parameter Value
agent-mixinghiddendim 32
channel-mixinghiddendim 256
mixerlr 5e-5
Table2: Uniquehyper-parametersofAgentMixer.
Parameter Value
blocknumber 1
headnumber 1
Table3: Uniquehyper-parametersofMAT/MAT-Dec.
Parameter Value
noisedim 2
epsilonstart 1.0
epsilonend 1.0
targetupdateinterval 200
Table4: Uniquehyper-parametersofMAVEN.
Parameter Value
Training
optimizer Adam
optimizerepsilon 1e-5
weightdecay 0
maxgradnorm 10
datachunklength 1
Model
activation ReLU
PPO
ppo-clip 0.2
gamma 0.99
gaelambda 0.95
Table5: Commonhyper-parametersusedacrossalldomains.
E.3 SMACV2
Welistthehyper-parametersusedforeachmapofSMACv2inTable7.
E.4 MA-MUJOCO
Thehyper-parametersusedforeachtaskofMA-MuJoCoarelistedinTable8.
21UnderreviewasaconferencepaperatICLR2024
Parameter Value
Training
actorlr 5e-4
criticlr 5e-4
entropycoef 0.01
Model
hiddenlayer 1
hiddenlayerdim 64
PPO
ppoepoch 15
ppo-clip 0.2
nummini-batch 1
Sample
environmentsteps 200000
rolloutthreads 50
episodelength 200
Table6: Commonhyper-parametersusedinmatrixgames.
Parameter Value
Training
actorlr 5e-4
criticlr 5e-4
entropycoef 0.01
Model
hiddenlayer 1
hiddenlayerdim 64
PPO
ppoepoch 5
ppo-clip 0.2
nummini-batch 1
Sample
environmentsteps 10000000
rolloutthreads 50
episodelength 200
Table7: Commonhyper-parametersusedintheSMACv2.
Parameter Value
Training
actorlr 3e-4
criticlr 3e-4
entropycoef 0
Model
hiddenlayer 2
hiddenlayerdim 64
PPO
ppoepoch 5
ppo-clip 0.2
nummini-batch 1
Sample
environmentsteps 10000000
rolloutthreads 40
episodelength 100
Environment
agentobsk 0
Table8: Commonhyper-parametersusedintheMA-MuJoCo.
22