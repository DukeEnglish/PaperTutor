Beyond Reference-Based Metrics:
Analyzing Behaviors of Open LLMs on Data-to-Text Generation
ZdeneˇkKasner and OndrˇejDušek
CharlesUniversity,FacultyofMathematicsandPhysics
InstituteofFormalandAppliedLinguistics
Prague,CzechRepublic
{kasner,odusek}@ufal.mff.cuni.cz
Abstract public APIs
Weinvestigatetowhichextentopenlargelan-
guagemodels(LLMs)cangeneratecoherent
JSON JSON JSON CSV MD
andrelevanttextfromstructureddata. Topre-
weather product game time entity
vent bias from benchmarks leaked into LLM data data results series graph
trainingdata,wecollectQUINTD-1: anad-hoc
LLM
benchmarkforfivedata-to-text(D2T)genera-
tiontasks,consistingofstructureddatarecords
instandardformatsgatheredfrompublicAPIs.
Weleveragereference-freeevaluationmetrics
weather product game chart entity
andLLMs’in-contextlearningcapabilities,al- forecast description report caption description
lowing us to test the models with no human-
analyzing model token-level
writtenreferences. Ourevaluationfocuseson behavior error annotations
annotatingsemanticaccuracyerrorsontoken-
level,combininghumanannotatorsandamet- Figure 1: We experiment with using LLMs for gen-
ricbasedonGPT-4. Oursystematicexamina- erating text from structured data in various domains,
tion of the models’ behavior across domains analyzingmodelbehaviorandevaluatingtheiroutput
and tasks suggests that state-of-the-art open ontokenlevel.
LLMs with 7B parameters can generate flu-
ent and coherent text from various standard
data formats in zero-shot settings. However, ger issue is perhaps the lack of testing grounds.
we also show that semantic accuracy of the ThecurrentD2Tgenerationbenchmarksaresatu-
outputsremainsamajorissue: onourbench- ratedbyapproachesoptimizingtowardstraditional
mark,80%ofoutputsofopenLLMscontain
reference-based evaluation metrics, which were
a semantic error according to human annota-
showntocorrelatepoorlywithhumanjudgement
tors (91% according to GPT-4). Our code,
(Gehrmann et al., 2023; van der Lee et al., 2021;
data,andmodeloutputsareavailableathttps:
Novikovaetal.,2017). Whenitcomestothemod-
//d2t-llm.github.io.
els, using closed LLMs (OpenAI, 2023a,b) is in-
1 Introduction
creasinglyconsideredabadresearchpracticedueto
itsnon-reproducibility(Rogers,2023;Chenetal.,
Large language models (LLMs; Ouyang et al.,
2023), and potential data contamination further
2022; Touvron et al., 2023a,b; Jiang et al., 2023;
restricts the space for experiments (Golchin and
Tunstall et al., 2023) have already left a mark in
Surdeanu,2023;Aiyappaetal.,2023;Anonymous,
manyareasofnaturallanguageprocessing(NLP).
2023).
Suprisingly,theirapplicabilitytothetaskofdata-to-
Inthispaper,weproposehowtogetaroundthese
text(D2T)generation(ReiterandDale,1997;Gatt
issuesandmovetowardsbetterD2Tgenerationap-
andKrahmer,2018)remainsunderexplored,with
proacheswithmorerigorousevalution. Webegin
limitedevaluationonahandfulofwell-established
withtherealizationthatunlabeleddataareplenti-
benchmarks only (Axelsson and Skantze, 2023;
ful. Toleveragethedataforourexperiments, we
YuanandFärber,2023). Generatingtextfromstruc-
tureddataisarguablychallengingforLLMs,given
introduceQUINTD:1 atoolforcollectingstructured
datafromfivedomainsinstandardformats: JSON,
the specifics of D2T generation such as long in-
puts, complex non-linear structure, and strict re-
1QuintetofUnlabeledInputsforNaturalTasksinData-
quirementsonsemanticaccuracy. However,abig- to-text(pronouncedas“quintet”).
4202
naJ
81
]LC.sc[
1v68101.1042:viXraTaskId Domain TaskDescription Source Format
openweather Weather Generatingaweatherforecastfromweatherdata. OpenWeather JSON
gsmarena Technology Describingaproductbasedonitsattributes. GSMArena JSON
ice_hockey Sport Describinganoutcomeofanice-hockeygame. RapidAPI JSON
owid Health Generatingacaptionforatimeseries. OurWorldInData CSV
wikidata Worldfacts Desribingentitiesandrelationsinaknowledgegraph. Wikidata Markdown
Table1: ThedomainsandtasksincludedintheQUINTDdatacollectiontoolweusefortestingD2Tgenerationwith
LLM.Inourexperiments,wedownload100developmentand100testexamplesofinputdataforeachdomain.
CSV,andMarkdown. Wechoosethedomainsso acrosstasksandformats(§5). Ourmainfindings
thatthedatacanbedirectlyusedasinputforfive areasfollows:
distinct D2T generation tasks. Specifically, our
• Open LLMs can generate fluent outputs
tasks inlude generating weather forecasts, sports
from structured data in common formats
reports, product descriptions, chart captions, and
underzero-shotsettings.
entitydescriptions(seeTable1). Next,wecollecta
setof1000inputswithQUINTDandusetheinputs • Semantic accuracy is still a major obsta-
as an ad-hoc benchmark (called QUINTD-1) for cle: onaverage,80%ofLLMoutputscontain
testingabilitiesofLLMsforD2Tgeneration. We asemanticerroraccordingtohumanannota-
assumethatthedataformatsinQUINTD-1arecom- tors and 91% according to the GPT-4-based
monintheLLMs’pretrainingcorpora,sowespec- metric.
ifythetaskusinginstructions(insteadofstandard
• Longdatainputscausepracticalissues,in-
finetuningwithhuman-writtenoutputs),capitaliz-
cludingtheneedforlong-contextmodels,in-
ing on the in-context learning abilities of LLMs
creasedGPUmemoryrequirements,andun-
(§2).
availabilityoffew-shotapproaches.
We push towards better reproducibility by fo-
• Theoutputqualitycanbeaidedbyfollow-
cusing on open LLMs, which – apart from being
ingseveralrules-of-thumbonpreprocessing
moreaccessible–alsoachieveincreasinglybetter
the model input, such as including units, re-
resultsacrosstasks(Zhengetal.,2023;Beeching
moving unnecessary fields, or prefixing the
et al., 2023). For our experiments, we use three
modelanswer.
openLLMswith7Bparameters: Llama-2(Touvron
et al., 2023b; TogetherAI, 2023), Mistral (Jiang
2 Reference-FreeD2TGeneration
et al., 2023), and Zephyr (Tunstall et al., 2023).
WealsouseGPT-3.5(OpenAI,2023b)asaclosed
2.1 DataCollectionTool
modelbaselineforthefinalexperiments. Giventhe
Weintroduceatooldubbed QUINTDforcollecting
behavioral nature of the experiments with LLMs
ad-hoctestssetsusingpublicAPIsinfivedifferent
(Holtzmanetal.,2023),weputemphasisonreport-
domains,andwecollectonesuchset(QUINTD-1)
ingmodelbehaviorthroughouttheprocess(§3).
forourexperiments. Ourmainreasonsfordepart-
Another piece of the puzzle is reference-free
ingfromthetraditionalschemeofbenchmarking
evaluation: using the input data as a ground for
onwell-establisheddatasetsare:
comparison,insteadofhumanreferences(§4). For
evaluation,weusemanualannotationsfromhuman 1. Any published test sets may be potentially
crowdworkers(vanderLeeetal.,2021),alongwith includedinthetrainingdataofLLMs.
a customized automatic metric based on GPT-4
2. Publicsourcesofstructureddataofferenough
(Liuetal.,2023;ChiangandLee,2023;Kocmiand
resourcesforcreatingad-hoctestsets.
Federmann,2023a). Togetafine-grainedpicture
of model errors, we annotate semantic accuracy 3. Withouthumanreferences,ourdatacollection
errorsonthelevelofindividualtokens(Thomson schemeislightweightandreplicable.
andReiter,2020;Thomsonetal.,2023).
Given the available public sources of data, we
Basedonourresults,weprovidegeneralrecom- settledonthefivetaskswhicharedescribedinTa-
mendationsaboutD2TgenerationwithopenLLMs ble1(seeAppendixAformoredetails). ThetasksPrompt
usethesameprompttemplateP forallthedomains
andmodels.
Based on the given data:
ˋˋˋ For a set of preliminary experiments, we first
{DATA} wrote down the initial version of the prompt and
ˋˋˋ
used the data without further preprocessing. We
Your task is to write a brief, fluent,
and coherent single-paragraph {output_type} then iteratively improved our experimental setup
in natural language. The text should be
byobservingoutputsonthedevelopmentset. We
balanced and neutral. Make sure that all the
facts mentioned in the text can be derived describealltheobservationsandmodificationswe
from the input data, do *not* add any extra madebeforegeneratingthefinaloutputsonthetest
information.
setin§3.3.
Startwith
3.2 Models
Sure! Here is the {output_type}:
" For our experiments, we selected the following
LLMsavailableunderanopenlicense:
Figure 2: The prompt P and the model output prefix • Llama2(Touvronetal.,2023a;TogetherAI,
(“Startwith”)weusedfortheexperimentsinthispaper. 2023),
DATAisfilledwiththedatarecordxandoutput_type
togethercomputer/Llama-2-7B-32K-Instruct
isfilledaccordinglyforeachdomainD(cf. Table1).
• Mistral(Jiangetal.,2023),
mistralai/Mistral-7B-Instruct-v0.1
are based on structured data in common formats:
JSON,CSV,andMarkdown. • Zephyr(Tunstalletal.,2023).
HuggingFaceH4/zephyr-7b-beta
2.2 QUINTD-1Dataset
Themodelsareinstruction-tuned,operatewith
ThebenchmarkwecollectedusingQUINTDforour
32k context, and have good performance on re-
experimentsinthispaper(QUINTD-1)contains500
centbenchmarks. Allthemodelshave7Bparam-
examplesinthedevelopmentsetand500examples
eters and thus fit on a single NVIDIA A40 (48G
inthetestset(100examplesperdomainforeach
VRAM)in16-bitprecision. Themodelsareavail-
split). Wekeepthesizeofthedatasetmoderatefor
ablethroughHuggingFace(Wolfetal.,2020).
aquickexperimentalturnaround.
WeaccessedthemodelsviaanAPIprovidedby
We downloaded the data between November
thetext-generation-webuiframework2running
2023andJanuary2024. Notethatthedatasetcon-
tains only unlabeled data without any reference locally. Forthefinalexperiments,wealsoincluded
GPT-3.5 (OpenAI, 2023b) accessed through the
outputs(e.g. weatherdata,butnotatextualweather
OpenAIAPI.
forecast). Newversionsofthebenchmarkcanbe
easilycreatedwiththe QUINTDtoolweprovide.
3.3 ObservationsfromPreliminary
2.3 TaskDefinition Experiments
EachexampleinQUINTD-1consistsofastructured
Duringdevelopment,wemadeseveralobservations
datarecordxfromadomainD P{openweather,
whichwetookintoaccountforourfinalexperimen-
gsmarena,ice_hockey,owid,wikidata}. Given
talsetup:
x and a prompt P, the goal is to generate natural
languageoutputy faithfultothedatax,according Anyinputfieldmayappearintheoutput. The
totheinstructionsinthepromptP (seeFigure2). modelsdonotalwaysselectthemostrelevantfields
forthegivenoutput. Forexample,weobservedthat
3 Experiments
the models commonly mention identifiers, times-
3.1 ExperimentalProcess tamps,files,andothermetadata,leadingtounnatu-
raloutputs. Duetothis,wedecidednottoinclude
Ourgoalistoavoidextensivedatapreprocessing
theseirrelevantfieldsontheinput.
and prompt engineering, since these steps could
harmthereproducibilityandgeneralizabilityofour
2https://github.com/oobabooga/
experiments. Withthisgoalinmind,wedecidedto text-generation-webui"weather": [{ Llama2-7B-
"description": 32k-Instruct
"light rain",
" w} i] n, d": { + prompt Mistral-7B
"deg": 328, instruct
"gust": 3.92,
"speed": 2.27 Zephyr-7B-
}
(...) data beta
Mountain Brook, Alabama is expected to
have a mild and mostly cloudy start to
the week, with temperatures ranging from
4.78°C to 11.47°C. Rain is expected on
Monday and Tuesday (...)
output
GPT-4
✔
✘
human
annotators
Figure 3: Our experimental setup. We first generate the outputs using LLMs which are given raw data and a
task-specificprompt. Weannotatethetoken-levelsemanticerrorsintheLLMoutputswitha)anautomaticmetric
basedGPT-4whichmatchestheoutputtotherawdata,andb)humanannotators,whoannotatetheerrorsinthe
outputgiventhedatavisualization.
Unitsneedtobespecifiedexplicitly. Iftheunits ments since we could not robustly fit an addi-
arenotspecifiedinthedatarecord,themodelstend tional (x , y ) pair in the prompt. We
example example
toresorttotheirbestguess. Thismaygounnoticed attemptedincludingonlyy (makingthesetup
example
iftheunitisclearfromthecontext(e.g.,themodel “half-shot”),butweobservedthatthemodelsthen
will usually not report temperature in Fahrenheit tendedtouseentitiesfromtheexample(unrelated
instead of Celsius), but it may get problematic if totheactualinput)intheiroutputs. Therefore,we
thevalueisambiguous(e.g.,windspeedinkm/h decided not to to follow this line of experiments
versusm/s). Therefore,weexplicitlyaddunitsto (see§5.3fordiscussion).
alldatarecordswhereappropriate.
Deterministicdecodingandsamplingareonpar.
Understandablelabelsareenough. Ontheflip In our preliminary experiments, we observed a
side,wedecidenottoaddextradescriptionstothe roughly similar output quality for both determin-
keysifthekeywasunderstandablefromthelabel isticdecodingandsampling.3 Forthefinalexper-
(e.g. homeTeamordimensions). Asdiscussedby iments,wedecidedtousedeterministicdecoding
Kasner et al. (2023b), pretrained models tend to which is non-parametric and conceptually more
interpretthefieldscorrectlyaslongasthelabelis suitableforD2Tgeneration.
human-readable. Weonlydecidedtoincludechart
metadatafortheCSVfilesintheowiddomain. Prefixing the output makes parsing easier.
Evenwithvariationsofa“generateonlytheoutput”
Longinputscanbetroublesome. Theinputsin
instruction appended to the prompt, the models
some domains can easily get longer than 10-20k
(especiallyLlama2)tendedtofirstconfirmthere-
tokens. Thisissueisamplifiedbythefactthatthe
quest. For that reason, we decided to prefix the
evaluatedLLMstokenizenumbersintoindividual
input for all the models with “Sure! Here is the
digits. Because of the long inputs, we decided
{output_type}: "”. Theopeningquoteattheendof
togiveuptheofficialLlama-2-7B-Instructmodel
theprefixallowedustorobustlyparsethetextsim-
with4kcontext(Touvronetal.,2023b)infavorof
plybystrippingtheclosingquotefromthemodel
Llama2with32kcontext(TogetherAI,2023). With
output.
long inputs, the GPU memory consumption also
gets considerably higher, so we needed to down- Theoutputsarefluent,butinaccurate. Weob-
samplethedatainowidandopenweathertokeep servedthatthevastmajorityofmodeloutputswere
theirlengthunder~8ktokens. gramaticallyandstylisticallycorrect,capturingthe
outputtypespecifiedintheprompt. However,we
Few-shot experiments are infeasible. Due to
the context-length limitations described above,
3Weusedthetext-generation-webuidefaultdecoding
we were not able to conduct few-shot experi- parameters:temperature=0.7,top_p=0.9,andtop_k=20.
atad
war
noitazilausiv
atad
sMLL
nepo
srotatonna
noitareneg
noitaulavealso noticed that the outputs contained many fac- • E : human evaluation based on crowd-
hum
tual errors (even after emphasizing the focus on sourcing(§4.2).
factualaccuracyintheprompt,seeFigure2). This
Thesetwoschemesarebasedonsimilarinstruc-
observationleadustoevaluatethemodeloutputs
tionsandproduce(nearly)equivalentoutputs. The
usingtoken-levelannotationsfocusedonsemantic
main idea in introducing multiple schemes is to
accuracyerrors(ReiterandThomson,2020).
compensateforshortcomingsofeachapproachand
thusincreasethereplicabilityandrobustnessofour
3.4 FinalExperiments
results.
Taking the observations in §3.3 into account, we
4.1 GPT-4-basedEvaluation
proceededtogeneratetheoutputsonthetestsetof
QUINTD-1fortoken-levelerroranalysis. Wefirst LLM-basedmetricscanbecustomizedforapartic-
preprocessed the data as mentioned: we stripped ulartaskwithouttheneedfortrainingdata. Forour
outunnecessaryfields,addedunits,anddownsam- experiments, we employ a metric based on GPT-
pledthedatatofitthecontext. Forallthemodels 4(gpt-4-1106-preview,OpenAI,2023a),which
mentionedin§3.2,weusedthepromptinFigure2 wasshowntobesuperiorinfollowingfine-grained
anddeterministicdecodingwithamaximumlength instructions compared to other LLMs, and to be
of512tokens. havinghighcorrelationswithhumanjudgementon
Forcomparsion,wealsogeneratedoutputsfor evaluatinggeneratedtexts(Zhaoetal.,2023;Sot-
thesameinputsandidenticalpromptswithGPT-3.5 tanaetal.,2023;KocmiandFedermann,2023a,b).
(gpt-3.5-turbo-1106).4 Giventhatthedecoding E gpt isinstantiatedusingapromptandasystem
parametersprovidedbytheOpenAIAPIarefairly messagedescribingthetask. Weinstructthemodel
limited,weonlytriedtobringthesetupcloserto toproduceaJSONoutputwithsequentiallyordered
deterministic decoding by fixing the temperature errorsusingthefollowingformat:
andseedto0. {
"errors": [{
"reason": [REASON],
4 Evaluation
"text": [TEXT_SPAN],
"type": [ERROR_CATEGORY]
Forevaluation,wefocusonsemanticaccuracyer- },
...]
rors. Wecomparethegeneratedtextstotheinput
}.
data,lookingforpartsoftextswhicharenotfaith-
Note that we require that the model first gen-
fultotheinputdata. Weannotatetheerrorsonthe
erates the reason for the error. Besides that we
tokenlevel,consideringallthetokensintheoutput
getafree-formtextreasoningforeacherrorwith-
textaspotentialsourcesoferrors.
outexcessiveextracosts,ourcursoryobservations
Regardingtheerrortaxonomy,wesettledonfour
suggestthatthisleadstomorepreciseoutputs.
errorcategories: INCORRECT,NOT_CHECKABLE,MIS
Concerningthealignmentofthemodeloutputs
LEADING,andOTHER. Thetaxonomyisinspiredby
with the original text, we perform matching on
themethodologydiscussedinThomsonandReiter
TEXT_SPAN.Weensurethatthemodelresponseis
(2020)andThomsonetal.(2023). Tokeepthean-
a valid JSON using OpenAI’s response_format
notationtractable,wedecidednottodistinguishbe-
parameter. SeeAppendixBformoredetailsabout
tweenfine-grainedcategories(e.g. incorrectname
the metric, including the prompt and the system
vs. incorrect number). The descriptions of our
message.
errorcategories,aspresentedintheinstructionsfor
annotation,areincludedinTable2. 4.2 Human-basedEvaluation
We employ two complementary evaluation
AnautomaticmetricbasedonaclosedLLMmakes
schemes:
the evaluation potentially non-reproducibile and
• E : an automatic metric based on GPT-4 biased(KocmiandFedermann,2023a;Wangetal.,
gpt
(§4.1), 2023b), for which we compensate by obtaining
annotationsfromhumanannotators.
4WedidnotincludeGPT-3.5inourpreliminaryexperi- ForthehumanannotationmetricE ,wepre-
hum
mentssinceclosedmodelsarenotourfocus.Wealsodidnot
paredacustomwebinterfacebasedonTABGENIE
useGPT-4becausewereservethemodelforevaluation,see
§4.1. (Kasneretal.,2023a),whereanannotatorisableError Description
INCORRECT Thefactinthetextcontradictsthedata.
NOT_CHECKABLE Thefactinthetextcannotbecheckedgiventhedata.
MISLEADING Thefactinthetextismisleadinginthegivencontext.
OTHER Thetextisproblematicforanotherreason,e.g.grammaticallyorstylisticallyincorrect,irrelevant,or
repetitive.
Example
data Nokia3310|color:black,blue,grey|display:320x240px
text Nokia3310isproducedinFinlandandfeaturesa320x320display.Itisavailableinblackcolor.The
dataseemtoprovideonlypartialinformationaboutthephone.
Table 2: Categories of errors annotated in our evaluation and an example demonstrating the error types. See
AppendixCforexplanationaboutindividualerrorsintheexample.
to annotate a text span with a selected error cate- onebeingpresentinmorethan26%ofexamples
gory. We created custom visualizations for each accordingtobothmetrics.
dataformat. UnlikewithE , wedid notaskthe The results vary widely according to the do-
gpt
crowdworkers for free-form reasoning about the main(seeAppendixF).Forexample,theoutputsin
errorssincethatwouldmaketheannotationmore wikidatacontainmuchmoreNOT_CHECKABLEer-
complex. rorsonaverage(1.54peroutputaccordingtoE )
hum
WehiredannotatorsontheProlificcrowdsourc- thanINCORRECTerrors(0.12peroutputaccording
ing platform.5 In total, we hired 100 annotators, to E ), suggesting that with simpler inputs, the
hum
eachannotating20examples(4modeloutputsfor modelstendtowardsextrinsichallucinations. The
each of the 5 domains). We selected annotators openweather domain seems to be the most com-
withatleast10completedtasksand100%approval plexwiththelongestoutputs(~164tokens),more
rate,havingEnglishastheirprimarylanguage. We than8errorsintheoutputonaverageand>90%of
paid the annotators £9 per hour, according to the outputscontaininganerror.
platform’srecommendations. Themediantimefor ThedifferencesbetweentheopenLLMsarenot
completing the annotations was 47 minutes. See major. OutoftheopenLLMs,Zephyrhasthebest
AppendixCfortheinstructionsfortheannotators resultsacrosscategoriesandmetrics,followedby
and the annotation interface and Appendix E for Llama2. However,theoutputsofMistralarelonger
thedatavisualizations. on average, leaving more space for errors. GPT-
3.5(whichweconsiderseparately)doesgenerally
5 ResultsandDiscussion better according to both E and E , although
gpt hum
stillmakesanerrorin60-75%ofexamples(2errors
A summary of the token-level annotations is in per example on average). In general, the results
Table 3 and 4, with detailed results per domain showthatLLMsmaketoomanysemanticerrorsto
providedinAppendixF. be usable in practice for D2T generation in zero-
shotsetting.
5.1 HowAccurateAretheModelOutputs?
5.2 DoEvaluationMethodsAgree?
Dependingonthemodel,between74-85%ofexam-
plescontainanerroraccordingtoE ,suggesting To quantify the agreement of our evaluation met-
hum
thatopenLLMsmakesemanticerrorsveryoften. rics, we computed the Pearson correlation coef-
AccordingtoE ,thenumberisashighas88-93%. ficient between the error counts on the level of
gpt
tokens, examples, and domains (see Appendix D
ThemostcommonerrortypeisINCORRECT.As
fordetails). Thecorrelationonthelevelofdomains
showninTable3, alltheopenLLMsmakemore
(r “ 0.92)showsthatE andE correlate
thantwostatementscontradictingthedataper domain gpt hum
wellforhigh-levelrankingofthemodels. However,
output on average. The NOT_CHECKABLE errors
thecorrelationisweakeronthelevelofindividual
are also relatively common: more than one per
examples (r “ 0.52), and even weaker on
output on average according to E , and at least example
hum
theleveloftokens(r “ 0.25).
token
5https://prolific.com In Table 5, we show the percentage of tokensIncorrect NotCheckable Misleading Other Allcategories
E E E E E E E E E E #Tok.
gpt hum gpt hum gpt hum gpt hum gpt hum
Llama2 2.46 1.50 0.90 1.21 0.20 0.23 0.13 0.10 3.70 3.05 83.8
Mistral 2.80 1.95 0.52 1.12 0.37 0.44 0.11 0.25 3.80 3.76 114.9
Zephyr 2.50 1.41 0.40 0.79 0.39 0.20 0.06 0.16 3.35 2.56 98.0
GPT-3.5 1.57 0.63 0.32 0.51 0.42 0.17 0.02 0.07 2.32 1.38 84.9
Table3: Theaveragenumbersoferrorsperoutput(lowerisbetter)basedonGPT-4(E )andhumanannotators
gpt
(E ). Wealsoincludetheaveragenumberoftokensperoutputintherightmostcolumn. Theresultsofthebest
hum
openLLMareemphasized.
Incorrect NotCheckable Misleading Other Allcategories
E E E E E E E E E E
gpt hum gpt hum gpt hum gpt hum gpt hum
Llama2 0.78 0.52 0.46 0.57 0.15 0.17 0.09 0.08 0.92 0.85
Mistral 0.78 0.53 0.32 0.49 0.23 0.20 0.08 0.13 0.93 0.80
Zephyr 0.77 0.46 0.26 0.42 0.27 0.16 0.04 0.11 0.88 0.74
GPT-3.5 0.64 0.37 0.21 0.29 0.26 0.12 0.02 0.06 0.75 0.60
Table 4: The ratio of outputs containing at least one error (lower is better) based on GPT-4 (E ) and human
gpt
annotators(E ). TheresultsofthebestopenLLMareemphasized.
hum
E E E +E sible,canpotentiallybesolvedbyusingmoreef-
gpt hum gpt hum
ficient long-context models equipped with Flash
Incorrect 0.135 0.099 0.040
Notcheckable 0.039 0.076 0.016 Attention (Dao et al., 2022) and fast inference li-
Misleading 0.022 0.022 0.001 braries such as exllama6 or llama.cpp.7 A po-
Other 0.008 0.018 0.001
Allcategories 0.204 0.214 0.059 tentially suitable open LLM in this respect is the
long-contextLlama2(Xiongetal.,2023;modelto
Table5: Theratiooftokensmarkedaserrorneousby bereleased).
GPT-4(E ),humanannotators(E ),andbothmetrics
gpt hum
atthesametime(E +E ). Test the models in the wild. Except for using
gpt hum
an ad-hoc dataset of real-world data as we did in
our work, ecological validity of D2T evaluation
markedbyindividualmetrics. Whilebothmetrics canbealsoensuredbycontinousevaluationwith
mark around 21% of tokens as errorneous, they human users (Zheng et al., 2023) and evaluating
agreeon thespecifictokensinless than6%, sug- real-worldimpactofthesystems(Reiter,2023).
gestingthattheerrorspansmaybesubjecttointer-
Multilingualityisanopportunity. Withthere-
pretation. Weconcludewhilethedetailsregarding
cent efforts in extending D2T generation to low-
error spans and categories may vary, the metrics
resourcelanguages(Cripwelletal.,2023),multi-
generallyagreeonaccuracyofmodeloutputsfora
lingualD2TgenerationwithopenLLMsseemsa
givensetofexamples.
promising direction. Although we did not go be-
5.3 RecommendationsandDirections yond English, initial steps were already done by
workssuchasLorandiandBelz(2023).
Forget fluency, solve accuracy. The output of
LLMsismostlysatisfactoryregardingthestyle,for- Be careful about subtle bugs. During our pre-
mat,andpurposeofthetext. However,theamount liminaryexperiments,weuncoveredsubtlebugsin
of semantic errors remains very high. Improving APIcallssuchasincorrectinstructiontemplates8
semantic accuracy of the models (Li et al., 2022; orinvoluntaryinputtruncation. Withtheapparent
Schmidtová,2023),alongwithnewmodel-based easeofAPIaccessandrobustnessofLLMs,such
evaluationmetrics(Liuetal.,2023;Xuetal.,2023), bugscouldgounnoticedandartificiallyskewthe
couldthushelptobringimproveLLM-basedD2T modelperformance.
generationsystemswhereitismostneeded.
6https://github.com/turboderp/exllamav2
7https://github.com/ggerganov/llama.cpp
Use efficient models. The memory issues with
8https://huggingface.co/docs/transformers/
longcontext,makingfew-shotexperimentsinfea- chat_templating6 RelatedWork entitydescriptionsfromWikipediainfotables(Le-
bretetal.,2016).
6.1 D2TGenerationTasks
Weather Forecasts First attempts for generat- 6.2 D2TGenerationwithLLMs
ingweatherforecastsincludetemplate-basedand
Recentworkshavefocusedonexploringcapabil-
statistical approaches (Belz, 2005, 2008; Angeli
itiesofclosedLLMsonexistingD2Tgeneration
et al., 2010) for the Sumtime-meteo and Weath-
datasets. AxelssonandSkantze(2023)evaluated
erGov datasets (Sripada et al., 2002; Liang et al.,
GPT-3.5(OpenAI,2023b)onWebNLG,alongwith
2009). More recently, Balakrishnan et al. (2019)
YuanandFärber(2023),whoalsotestedthemodel
introduced a weather forecast dataset with tree-
ontheAGENDAdataset(Koncel-Kedziorskietal.,
structuredmeaningrepresentations. Ourweather
2019). Both works found out that regardless of
forecastsarelessstructuredandbasedona5-day
potential data contamination, the LLMs rank be-
weatheroutlook.
hindstate-of-the-artfinetunedmodelsonautomatic
metrics. Zhao et al. (2023) tested closed models
ProductDescriptions Ourphonespecifications
on modified table-to-text generation datasets and
are closest to Wen et al. (2015, 2016) who intro-
foundoutthatintermsoffaithfulness,GPT-4can
ducedadatasetforgeneratingdescriptionsoflap-
outperformstate-of-the-artmodels.
topsandTVs. Theirsolutionwasbasedonrecur-
rentneuralnetworks,althoughtemplatesremained
6.3 BeyondReference-BasedMetrics
ago-toapproachforthetask(Wangetal.,2017).
Recently,Shaoetal.(2021)andKotoetal.(2022) Manyworkshaverecentlyinvestigatedthepoten-
also proposed specialized architectures based on tial of using LLMs for automatic reference-free
pretrainedlanguagemodelsforthedatafrombig evaluation of generated text, generally achieving
e-commerceplatforms. high correlations with human judgement (Zhao
etal.,2023;Sottanaetal.,2023;KocmiandFeder-
SportReports AlltheD2Tgenerationdatasets
mann,2023a,b;ChiangandLee,2023;Wangetal.,
from the Rotowire family (Wiseman et al.,
2023a;Fuetal.,2023). However,theyalsovoice
2017;Wang,2019),includingSportSett:Basketball
concernsaboutitsnon-reproducibility(Kocmiand
(Thomson et al., 2021), and ESPN-NBA (Nie
Federmann,2023a)andpotentialbiasofthesemod-
etal.,2018)focusongeneratingbasketballreports.
els(Wangetal.,2023b).
AlongwithMLB(Puduppullyetal.,2019b),these
Holtzmanetal.(2023)suggestthattheresearch
datasetsbelongamongthemostchallengingD2T
onLLMsshouldmoveawayfromreportingbench-
datasets,attractingvariousneural-basedsolutions
markscores,investigatingmodelbehaviorsinstead.
(Puduppully et al., 2019a, 2022; Puduppully and
Inthisvein,UpadhyayandMassie(2022)analysed
Lapata,2021;Rebuffeletal.,2020). Weuseinstead
theabilityofmodelsofproducingdifferenttypes
simplerdatacoveringicehockeygamesummaries.
of content in D2T generation. Regarding human
ChartCaptions Followingtheearlyrule-based evaluation,ThomsonandReiter(2020)proposeda
approaches (Demir et al., 2008, 2012), the ap- protocolforreference-freetoken-levelannotation
proachesforchartcaptioningrecentlytacklelarge- ofcomplexD2Tgenerationoutput.
scaledatasetsfromdataanalyticinstitutions(Obeid
andHoque,2020;Kantharajetal.,2022). Wefo- 7 Conclusion
cusononeofthetasksfromSharmaetal.(2021),
We provided an exploratory study into D2T gen-
whichisgeneratingdescriptionsoftimeseriesin
erationwithopenLLMs. Weproposednewdirec-
thehealthdomain.
tionsforD2Tgeneration,includingusingad-hoc
EntityDescriptions Thetaskofgeneratingde- testsets, dataincommonformats, andreference-
scriptionsforaknowledgegraphhasbeencovered freeevaluation. ByacombinationofGPT-4-based
extensivelyinD2Tgeneration(Gardentetal.,2017; metric and human evaluation, we evaluated the
Ferreira et al., 2020; Agarwal et al., 2021; Chen performanceofLLMsonfivedomains,providing
et al., 2020; Ribeiro et al., 2020, inter alia). Our token-level annotations of model outputs across
task is to describe an entity provided a list of its fivedomains,andrecommendationsforfuturedi-
properties, which is closely related to generating rectionsinD2Tgeneration.Acknowledgements RachithAiyappa,JisunAn,HaewoonKwak,andYong-
yeolAhn.2023. CanwetrusttheevaluationonChat-
ThisworkwassupportedbytheEuropeanResearch GPT? InProceedingsofthe3rdWorkshoponTrust-
worthy Natural Language Processing (TrustNLP
Council (Grant agreement No. 101039303, NG-
2023),pages47–54,Toronto,Canada.
NLG)andCharlesUniversityprojectSVV260698.
It used resources of the LINDAT/CLARIAH-CZ Gabor Angeli, Percy Liang, and Dan Klein. 2010. A
SimpleDomain-independentProbabilisticApproach
ResearchInfrastructure(CzechMinistryofEduca-
toGeneration. InProceedingsofthe2010Confer-
tion,Youth,andSportsprojectNo. LM2018101).
ence on Empirical Methods in Natural Language
Processing,EMNLP2010,9-11October2010,MIT
Limitations StataCenter,Massachusetts,USA,AmeetingofSIG-
DAT, a Special Interest Group of the ACL, pages
Inourwork,wedonotincludecomparisontoother 502–512.
D2T generation approaches. The main reason is Anonymous.2023. Leak,Cheat,Repeat:DataContami-
thatourbenchmarkisreference-free, whilelarge nationandEvaluationMalpracticesinClosed-source
majority of prior approaches are based on mod- LLMs. OpenReview. https://openreview.net/
forum?id=exbPWKOyzF.
els finetuned on reference outputs. However, we
believe that our work still satisties our main goal Agnes Axelsson and Gabriel Skantze. 2023. Using
LargeLanguageModelsforZero-shotNaturalLan-
ofprovidinginsightsintobehaviorsofopenLLM
guageGenerationfromKnowledgeGraphs. CoRR,
modelsonD2Tgeneration.
abs/2307.07312.
Weacknowledgethatreference-freemetricscur-
AnushaBalakrishnan,JinfengRao,KartikeyaUpasani,
rently have various shortcomings, including re-
MichaelWhite,andRajenSubba.2019. Constrained
lianceonclosedmodelsorspecifichumanannota-
DecodingforNeuralNLGfromCompositionalRep-
tionprotocols,leadingtolimitedreplicability,and resentationsinTask-orientedDialogue. InProceed-
high price of execution. None of the approaches ings of the 57th Conference of the Association for
Computational Linguistics, ACL 2019, Volume 1:
also produces flawless outcomes and have only
LongPapers,pages831–844,Florence,Italy.
moderatecorrelationswitheachother. Webelieve
thattheseshortcomingswillbeadressedinthefu- Edward Beeching, Clémentine Fourrier, Nathan
Habib, Sheon Han, Nathan Lambert, Nazneen
turewithopenmodel-basedmetrics.
Rajani, Omar Sanseviero, Lewis Tunstall, and
Ourchoiceofmodelsislimitedto7B-parameter
Thomas Wolf. 2023. Open LLM Leader-
open LLMs due to our limited computational re- board. https://huggingface.co/spaces/
sources. Also, unlike some other LLMs such as HuggingFaceH4/open_llm_leaderboard.
GPT-Neo (Black et al., 2022) or BLOOM (Big- AnjaBelz.2005. Corpus-drivengenerationofweather
Science Workshop et al., 2022), the models we forecasts. In Proc. 3rd Corpus Linguistics Confer-
useddonotdisclosethedatatheyweretrainedon. ence.
Forthisreason,wefinditevermoreimportantto Anja Belz. 2008. Automatic generation of weather
testthemodelsonbenchmarkswhoselabelscould forecast texts using comprehensive probabilis-
havenotbeenincludedintheirtrainingdata. tic generation-space models. Nat. Lang. Eng.,
14(4):431–455.
The approaches based on LLMs may produce
factuallyincorrectinformation. Anytextproduced BigScience Workshop, Teven Le Scao, Angela Fan,
ChristopherAkiki,ElliePavlick,SuzanaIlic´,Daniel
by the LLMs therefore need to be carefully ex-
Hesslow, Roman Castagné, Alexandra Sasha Luc-
amined, andnodecisionsshouldbebasedonthe
cioni,FrançoisYvon,etal.2022. Bloom: A176b-
generatedtextalone. parameteropen-accessmultilinguallanguagemodel.
arXivpreprintarXiv:2211.05100.
SidneyBlack,StellaBiderman,EricHallahan,Quentin
References
Anthony, Leo Gao, Laurence Golding, Horace He,
ConnorLeahy,KyleMcDonell,JasonPhang,etal.
OshinAgarwal,HemingGe,SiamakShakeri,andRami
2022. Gpt-neox-20b: An open-source autoregres-
Al-Rfou.2021. KnowledgeGraphBasedSynthetic
sivelanguagemodel. InProceedingsofBigScience
Corpus Generation for Knowledge-enhanced Lan-
Episode#5–WorkshoponChallenges&Perspectives
guage Model Pre-training. In Proceedings of the
inCreatingLargeLanguageModels,pages95–136.
2021 Conference of the North American Chapter
of the Association for Computational Linguistics: Armand Boschin and Thomas Bonald. 2019. Wiki-
HumanLanguageTechnologies,NAACL-HLT2021, datasets: standardized sub-graphs from wikidata.
pages3554–3565,Online. arXivpreprintarXiv:1906.04536.Lingjiao Chen, Matei Zaharia, and James Zou. 2023. AlbertGattandEmielKrahmer.2018. Surveyofthe
How is ChatGPT’s behavior changing over time? State of the Art in Natural Language Generation:
CoRR,abs/2307.09009. Coretasks,applicationsandevaluation. J.Artif.In-
tell.Res.,61:65–170.
Wenhu Chen, Yu Su, Xifeng Yan, and William Yang
Wang. 2020. KGPT: Knowledge-grounded Pre-
SebastianGehrmann,ElizabethClark,andThibaultSel-
trainingforData-to-textGeneration. InProceedings
lam. 2023. Repairing the Cracked Foundation: A
of the 2020 Conference on Empirical Methods in
SurveyofObstaclesinEvaluationPracticesforGen-
NaturalLanguageProcessing,EMNLP2020,pages
eratedText. J.Artif.Intell.Res.,77:103–166.
8635–8648,Online.
Shahriar Golchin and Mihai Surdeanu. 2023. Time
DavidCheng-HanChiangandHung-yiLee.2023. Can
Travel in LLMs: Tracing Data Contamination in
Large Language Models Be an Alternative to Hu-
LargeLanguageModels. CoRR,abs/2308.08493.
manEvaluations? InProceedingsofthe61stAnnual
Meeting of the Association for Computational Lin-
guistics(Volume1: LongPapers),ACL2023,pages AriHoltzman,PeterWest,andLukeZettlemoyer.2023.
15607–15631,Toronto,Canada. GenerativeModelsasaComplexSystemsScience:
How can we make sense of large language model
Liam Cripwell, Anya Belz, Claire Gardent, Albert behavior? CoRR,abs/2308.00189.
Gatt, Claudia Borg, Marthese Borg, John Judge,
MichelaLorandi,AnnaNikiforovskaya,andWilliam AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen-
Soto Martinez. 2023. The 2023 WebNLG Shared sch,ChrisBamford,DevendraSinghChaplot,Diego
Task on Low Resource Languages. Overview and de Las Casas, Florian Bressand, Gianna Lengyel,
EvaluationResults(WebNLG2023). InProceedings Guillaume Lample, Lucile Saulnier, Lélio Re-
oftheWorkshoponMultimodal,MultilingualNatu- nard Lavaud, Marie-Anne Lachaux, Pierre Stock,
ralLanguageGenerationandMultilingualWebNLG TevenLeScao,ThibautLavril,ThomasWang,Timo-
Challenge (MM-NLG 2023), pages 55–66, Prague, théeLacroix,andWilliamElSayed.2023. Mistral
CzechRepublic. 7B. CoRR,abs/2310.06825.
Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra,
ShankarKantharaj,RixieTiffanyKoLeong,XiangLin,
andChristopherRé.2022. FlashAttention: Fastand
AhmedMasry,MeghThakkar,EnamulHoque,and
Memory-efficientExactAttentionwithIO-awareness.
ShafiqR.Joty.2022. Chart-to-Text: ALarge-scale
InAdvancesinNeuralInformationProcessingSys-
Benchmark for Chart Summarization. In Proceed-
tems35: AnnualConferenceonNeuralInformation
ingsofthe60thAnnualMeetingoftheAssociationfor
Processing Systems 2022, NeurIPS 2022, New Or-
ComputationalLinguistics(Volume1: LongPapers),
leans,LA,USA.
ACL2022,pages4005–4023,Dublin,Ireland.
SenizDemir,SandraCarberry,andKathleenF.McCoy.
2008. GeneratingTextualSummariesofBarCharts. Zdeneˇk Kasner, Ekaterina Garanina, Ondˇrej Plátek,
InINLG2008-ProceedingsoftheFifthInternational and Ondˇrej Dušek. 2023a. TabGenie: A Toolkit
NaturalLanguageGenerationConference,June12- forTable-to-textGeneration. InProceedingsofthe
14,2008,SaltFork,Ohio,USA. 61stAnnualMeetingoftheAssociationforCompu-
tational Linguistics: System Demonstrations, ACL
SenizDemir,SandraCarberry,andKathleenF.McCoy. 2023,pages444–455,Toronto,Canada.
2012. SummarizingInformationGraphicsTextually.
Comput.Linguistics,38(3):527–574. Zdeneˇk Kasner, Ioannis Konstas, and Ondˇrej Dušek.
2023b. Mind the Labels: Describing Relations in
ThiagoFerreira,ClaireGardent,NikolaiIlinykh,Chris
KnowledgeGraphsWithPretrainedModels. InPro-
vanderLee,SimonMille,DiegoMoussallem,and
ceedings of the 17th Conference of the European
Anastasia Shimorina. 2020. The 2020 Bilingual,
Chapter of the Association for Computational Lin-
Bi-DirectionalWebnlg+SharedTaskOverviewand
guistics,EACL2023,Dubrovnik,pages2390–2407,
Evaluation Results (Webnlg+ 2020). In Proceed-
Croatia.
ings of the 3rd International Workshop on Natu-
ral Language Generation from the Semantic Web
TomKocmiandChristianFedermann.2023a. GEMBA-
(WebNLG+).
MQM: Detecting Translation Quality Error Spans
withGPT-4. InProceedingsoftheEighthConference
JinlanFu,See-KiongNg,ZhengbaoJiang,andPengfei
onMachineTranslation,WMT2023,pages768–775,
Liu. 2023. GPTScore: Evaluate as You Desire.
Singapore.
CoRR,abs/2302.04166.
ClaireGardent,AnastasiaShimorina,ShashiNarayan, Tom Kocmi and Christian Federmann. 2023b. Large
andLauraPerez-Beltrachini.2017. TheWebNLG LanguageModelsAreState-of-the-artEvaluatorsof
Challenge: Generating Text from RDF Data. In TranslationQuality. InProceedingsofthe24thAn-
Proceedingsofthe10thInternationalConferenceon nual Conference of the European Association for
NaturalLanguageGeneration,INLG2017,Santiago MachineTranslation,EAMT2023,pages193–203,
deCompostela,pages124–133,Spain. Tampere,Finland.Rik Koncel-Kedziorski, Dhanush Bekal, Yi Luan, JasonObeidandEnamulHoque.2020. Chart-to-Text:
MirellaLapata,andHannanehHajishirzi.2019. Text GeneratingNaturalLanguageDescriptionsforCharts
Generation from Knowledge Graphs with Graph by Adapting the Transformer Model. In Proceed-
Transformers. In Proceedings of the 2019 Confer- ingsofthe13thInternationalConferenceonNatural
enceoftheNorthAmericanChapteroftheAssoci- LanguageGeneration,INLG2020,pages138–147,
ation for Computational Linguistics: Human Lan- Dublin,Ireland.
guageTechnologies,NAACL-HLT2019,Minneapo-
OpenAI. 2023a. GPT-4 Technical Report. CoRR,
lis, MN,Volume1(LongandShortPapers), pages
abs/2303.08774.
2284–2293,USA.
OpenAI. 2023b. Introducing ChatGPT. https://
FajriKoto,JeyHanLau,andTimothyBaldwin.2022.
openai.com/blog/chatgpt. AccessedonJanuary
CanPretrainedLanguageModelsGeneratePersua-
9,2024.
sive,Faithful,andInformativeAdTextforProduct
Descriptions? InProceedingsoftheFifthWorkshop
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
on E-Commerce and NLP (ECNLP 5), pages 234–
Carroll L. Wainwright, Pamela Mishkin, Chong
243,Dublin,Ireland.
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,
JohnSchulman,JacobHilton,FraserKelton,Luke
RémiLebret,DavidGrangier,andMichaelAuli.2016.
Miller,MaddieSimens,AmandaAskell,PeterWelin-
NeuralTextGenerationfromStructuredDatawith
der,PaulF.Christiano,JanLeike,andRyanLowe.
ApplicationtotheBiographyDomain. InProceed-
2022. Training languagemodelsto followinstruc-
ingsofthe2016ConferenceonEmpiricalMethodsin
tionswithhumanfeedback. InAdvancesinNeural
NaturalLanguageProcessing,EMNLP2016,pages
InformationProcessingSystems35: AnnualConfer-
1203–1213,Austin,Texas,USA.
enceonNeuralInformationProcessingSystems2022,
WeiLi,WenhaoWu,MoyeChen,JiachenLiu,Xinyan NeurIPS2022,NewOrleans,LA,USA.
Xiao, and Hua Wu. 2022. Faithfulness in Natural
RatishPuduppully,LiDong,andMirellaLapata.2019a.
LanguageGeneration: ASystematicSurveyofAnal-
Data-to-textGenerationwithContentSelectionand
ysis,EvaluationandOptimizationMethods. CoRR,
Planning. InTheThirty-ThirdAAAIConferenceon
abs/2203.05227.
ArtificialIntelligence,AAAI2019,TheThirty-First
PercyLiang,MichaelI.Jordan,andDanKlein.2009. InnovativeApplicationsofArtificialIntelligenceCon-
LearningSemanticCorrespondenceswithLessSu- ference,IAAI2019,TheNinthAAAISymposiumon
pervision. InACL2009,Proceedingsofthe47thAn- EducationalAdvancesinArtificialIntelligence,EAAI
nualMeetingoftheAssociationforComputational 2019,pages6908–6915,Honolulu,Hawaii,USA.
Linguistics and the 4th International Joint Confer-
RatishPuduppully,LiDong,andMirellaLapata.2019b.
enceonNaturalLanguageProcessingoftheAFNLP,
Data-to-text Generation with Entity Modeling. In
2-7August2009,pages91–99,Singapore.
Proceedingsofthe57thConferenceoftheAssocia-
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, tionforComputationalLinguistics,ACL2019,Vol-
Ruochen Xu, and Chenguang Zhu. 2023. G-Eval: ume 1: Long Papers, pages 2023–2035, Florence,
NLG Evaluation using Gpt-4 with Better Human Italy.
Alignment. InProceedingsofthe2023Conference
RatishPuduppully,YaoFu,andMirellaLapata.2022.
onEmpiricalMethodsinNaturalLanguageProcess-
Data-to-textGenerationwithVariationalSequential
ing,EMNLP2023,pages2511–2522,Singapore.
Planning. Trans.Assoc.Comput.Linguistics,10:697–
715.
Michela Lorandi and Anja Belz. 2023. Data-to-text
GenerationforSeverelyUnder-resourcedLanguages
RatishPuduppullyandMirellaLapata.2021. Data-to-
withGPT-3.5: ABitofHelpNeededfromGoogle
textGenerationwithMacroPlanning. Trans.Assoc.
Translate (WebNLG 2023). In Proceedings of the
Comput.Linguistics,9:510–527.
WorkshoponMultimodal,MultilingualNaturalLan-
guageGenerationandMultilingualWebNLGChal- ClémentRebuffel,LaureSoulier,GeoffreyScoutheeten,
lenge(MM-NLG2023),pages80–86. andPatrickGallinari.2020. AHierarchicalModel
for Data-to-text Generation. In Advances in Infor-
FengNie,JinpengWang,Jin-GeYao,RongPan,and
mationRetrieval-42ndEuropeanConferenceonIR
Chin-YewLin.2018. Operation-guidedNeuralNet-
Research,ECIR2020,Lisbon,Portugal,April14-17,
works for High Fidelity Data-To-text Generation.
2020,Proceedings,PartI,volume12035ofLecture
In Proceedings of the 2018 Conference on Empir-
NotesinComputerScience,pages65–80.
icalMethodsinNaturalLanguageProcessing,pages
3879–3889,Brussels,Belgium. Ehud Reiter. 2023. We should evaluate real-world
impact! https://ehudreiter.com/2023/11/13/
Jekaterina Novikova, Ondˇrej Dušek, Amanda Cercas evaluate-real-world-impact/. AccessedonJan-
Curry,andVerenaRieser.2017. WhyWeNeedNew
uary11,2024.
EvaluationMetricsforNLG. InProceedingsofthe
2017ConferenceonEmpiricalMethodsinNatural EhudReiterandRobertDale.1997. Buildingapplied
Language Processing, EMNLP 2017, pages 2241– natural language generation systems. Nat. Lang.
2252,Copenhagen,Denmark. Eng.,3(1):57–87.EhudReiterandCraigThomson. 2020. Shared Task HugoTouvron,ThibautLavril,GautierIzacard,Xavier
onEvaluatingAccuracy. InProceedingsofthe13th Martinet,Marie-AnneLachaux,TimothéeLacroix,
InternationalConferenceonNaturalLanguageGen- BaptisteRozière,NamanGoyal,EricHambro,Faisal
eration,INLG2020,pages227–231,Dublin,Ireland. Azhar,AurélienRodriguez,ArmandJoulin,Edouard
Grave, and Guillaume Lample. 2023a. LLaMA:
Leonardo F. R. Ribeiro, Martin Schmitt, Hinrich Open and Efficient Foundation Language Models.
Schütze, and Iryna Gurevych. 2020. Investigating CoRR,abs/2302.13971.
PretrainedLanguageModelsforGraph-to-textGen-
eration. CoRR,abs/2007.08426. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Anna Rogers. 2023. Closed AI Models Make Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
BadBaselines. https://hackingsemantics.xyz/ Bhosale,DanBikel,LukasBlecher,CristianCanton-
2023/closed-baselines/. Accessed on January Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
11,2024. JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
CynthiaGao,VedanujGoswami,NamanGoyal,An-
PatríciaSchmidtová.2023. SemanticAccuracyinNat-
thonyHartshorn,SagharHosseini,RuiHou,Hakan
ural Language Generation: A Thesis Proposal. In
Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
Proceedings of the 61st Annual Meeting of the As-
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
sociation for Computational Linguistics: Student
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
Research Workshop, ACL 2023, pages 352–361,
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
Toronto,Canada.
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
HuajieShao,JunWang,HaohongLin,XuezhouZhang,
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Aston Zhang, Heng Ji, and Tarek F. Abdelzaher.
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
2021. ControllableandDiverseTextGenerationin
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
E-commerce. In WWW ’21: The Web Conference
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
2021, pages 2392–2401, Virtual Event / Ljubljana,
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Slovenia.
Melanie Kambadur, Sharan Narang, Aurélien Ro-
Mandar Sharma, John S. Brownstein, and Naren Ra- driguez,RobertStojnic,SergeyEdunov,andThomas
makrishnan.2021. TCube: Domain-agnosticNeural Scialom. 2023b. Llama 2: Open Foundation and
Time-seriesNarration. CoRR,abs/2110.05633. Fine-tunedChatModels. CoRR,abs/2307.09288.
AndreaSottana,BinLiang,KaiZou,andZhengYuan. Lewis Tunstall, Edward Beeching, Nathan Lambert,
2023. EvaluationMetricsintheEraofGPT-4: Re- Nazneen Rajani, Kashif Rasul, Younes Belkada,
liably Evaluating Large Language Models on Se- Shengyi Huang, Leandro von Werra, Clémentine
quence to Sequence Tasks. In Proceedings of the Fourrier,NathanHabib,NathanSarrazin,OmarSan-
2023ConferenceonEmpiricalMethodsinNatural seviero,AlexanderM.Rush,andThomasWolf.2023.
Language Processing, EMNLP 2023, pages 8776– Zephyr: DirectDistillationofLMAlignment. CoRR,
8788,Singapore. abs/2310.16944.
SomayajuluSripada,EhudReiter,JimHunter,andJin AshishUpadhyayandStewartMassie.2022. Content
Yu.2002. Sumtime-meteo: Parallelcorpusofnatu- TypeProfilingofData-to-textGenerationDatasets.
rallyoccurringforecasttextsandweatherdata. Com- InProceedingsofthe29thInternationalConference
putingScienceDepartment,UniversityofAberdeen, onComputationalLinguistics,COLING2022,pages
Aberdeen,Scotland,Tech.Rep.AUCS/TR0201. 5770–5782,Gyeongju,RepublicofKorea.
CraigThomsonandEhudReiter.2020. AGoldStan- ChrisvanderLee,AlbertGatt,EmielvanMiltenburg,
dardMethodologyforEvaluatingAccuracyinData- and Emiel Krahmer. 2021. Human evaluation of
To-textSystems. InProceedingsofthe13thInterna- automatically generated text: Current trends and
tionalConferenceonNaturalLanguageGeneration, best practice guidelines. Comput. Speech Lang.,
INLG2020,pages158–168,Dublin,Ireland. 67:101151.
CraigThomson,EhudReiter,andSomayajuluSripada. HongminWang.2019. RevisitingChallengesinData-
2021. SportSett:Basketball - A Robust and Main- to-textGenerationwithFactGrounding. InProceed-
tainable Dataset for Natural Language Generation. ingsofthe12thInternationalConferenceonNatural
page9. LanguageGeneration,INLG2019,pages311–322,
Tokyo,Japan.
CraigThomson,EhudReiter,andBarkaviSundararajan.
2023. Evaluatingfactualaccuracyincomplexdata- JiaanWang,YunlongLiang,FandongMeng,Haoxiang
to-text. ComputerSpeech&Language,80:101482. Shi,ZhixuLi,JinanXu,JianfengQu,andJieZhou.
2023a. Is ChatGPT a Good NLG Evaluator? A
TogetherAI. 2023. Preparing for the era of 32K con- PreliminaryStudy. CoRR,abs/2303.04048.
text: Early learnings and explorations. https:
//www.together.ai/blog/llama-2-7b-32k. Ac- Jinpeng Wang, Yutai Hou, Jing Liu, Yunbo Cao, and
cessedonJanuary2,2024. Chin-Yew Lin. 2017. A Statistical Framework forProductDescriptionGeneration. InProceedingsof YilunZhao,HaoweiZhang,ShengyunSi,LinyongNan,
theEighthInternationalJointConferenceonNatu- XiangruTang,andArmanCohan.2023. Investigat-
ralLanguageProcessing,IJCNLP2017,Volume2: ingTable-to-TextGenerationCapabilitiesofLLMs
ShortPapers,pages187–192,Taipei,Taiwan. inReal-WorldInformationSeekingScenarios.
PeiyiWang,LeiLi,LiangChen,DaweiZhu,Binghuai LianminZheng,Wei-LinChiang,YingSheng,Siyuan
Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,
Sui. 2023b. Large Language Models are not Fair ZhuohanLi,DachengLi,EricP.Xing,HaoZhang,
Evaluators. CoRR,abs/2305.17926. JosephE.Gonzalez,andIonStoica.2023. Judging
LLM-as-a-judgewithMT-benchandChatbotArena.
Tsung-HsienWen,MilicaGašic,NikolaMrkšic,LinaM
CoRR,abs/2306.05685.
Rojas-Barahona, Pei-Hao Su, David Vandyke, and
SteveYoung.2015. Towardmulti-domainlanguage
A QUINTD Data
generationusingrecurrentneuralnetworks. InNIPS
WorkshoponMachineLearningforSpokenLanguage
Here we describe the data sources we include in
UnderstandingandInteraction.
the QUINTD collection tool and the procedure of
Tsung-Hsien Wen, Milica Gasic, Nikola Mrksic, collectingtheQUINTD-1benchmark. Toreplicate
Lina Maria Rojas-Barahona, Pei-Hao Su, David
the data collection, please refer to the scripts we
Vandyke,andSteveJ.Young.2016. Multi-domain
provide.
Neural Network Language Generation for Spoken
DialogueSystems. InNAACLHLT2016,The2016
Conference of the North American Chapter of the A.1 SelectionofDataSources
AssociationforComputationalLinguistics: Human
When selecting the data sources, we had the fol-
LanguageTechnologies,pages120–129,SanDiego
lowingdesiderata:
California,USA.
Sam Wiseman, Stuart M. Shieber, and Alexander M. • Dataneedstobepubliclyavailable.
Rush.2017. ChallengesinData-to-documentGen-
• Dataneedstorepresentacommondata-to-text
eration. InProceedingsofthe2017Conferenceon
task.
Empirical Methods in Natural Language Process-
ing,EMNLP2017,pages2253–2263,Copenhagen,
• Data needs to be in a common format (or
Denmark.
straightforwardlytransformabletoone).
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien
Chaumond,ClementDelangue,AnthonyMoi,Pier- WesettledonthedatasourcesdescribedinAp-
ricCistac,TimRault,RémiLouf,MorganFuntowicz, pendixA.2. Allthesourcescanbeaccessedusing
JoeDavison,SamShleifer,PatrickvonPlaten,Clara an API. Note that some of the APIs have access
Ma,YacineJernite,JulienPlu,CanwenXu,TevenLe
limits,eitherfortherequestsmadefromasingleac-
Scao, Sylvain Gugger, Mariama Drame, Quentin
countperday,orforanumberofrequestsfroman
Lhoest,andAlexanderM.Rush.2020. Transform-
ers: State-of-the-art Natural Language Processing. IPaddresswithinatimewindow. However,these
InProceedingsofthe2020ConferenceonEmpirical limitsdonotseverelylimitthedatacollectionpro-
Methods in Natural Language Processing: System
cessonthescaleweusehere.
Demonstrations,EMNLP2020-Demos,pages38–
45,Online.
A.2 DataCollection
WenhanXiong,JingyuLiu,IgorMolybog,HejiaZhang,
Table6summarizesthesourcesofdataandoutput
Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi
typesforeachdomain.
Rungta,KarthikAbinavSankararaman,BarlasOguz,
MadianKhabsa,HanFang,YasharMehdad,Sharan
A.2.1 OpenWeather
Narang,KshitizMalik,AngelaFan,ShrutiBhosale,
SergeyEdunov,MikeLewis,SinongWang,andHao OpenWeather(OpenWeatherMap.org)isanonline
Ma.2023. EffectiveLong-contextScalingofFoun- servicewhichprovidesglobalweatherdataviaweb
dationModels. CoRR,abs/2309.16039.
interface and API. The API responses are in the
WendaXu,DanqingWang,LiangmingPan,Zhenqiao JSON format documented at the official website.
Song,MarkusFreitag,WilliamYangWang,andLei Forourexperiments,weusedtheforecast5API
Li. 2023. INSTRUCTSCORE: Explainable Text
which allows to download a 5-day forecast with
GenerationEvaluationwithFinegrainedFeedback.
3-hourresolutionforanylocationspecifiedbyits
ShuzhouYuanandMichaelFärber.2023. Evaluating GPScoordinates.
GenerativeModelsforGraph-to-textGeneration. In
Thefreetierislimitedto1,000APIcallsperday,
Proceedingsofthe14thInternationalConferenceon
whichisenoughtodownloadourwholetestsetin
Recent Advances in Natural Language Processing,
RANLP2023,pages1256–1264,Varna,Bulgaria. onebulk. However,atthetimeofexperiments,theDomainId Source Outputtype
openweather OpenWeather five-dayweatherforecast
gsmarena GSMArena productdescription
ice_hockey RapidAPI icehockeygamesummary
owid OurWorldInData chartcaption
wikidata Wikidata entitydescription
Table6: ThesourcesofdataandoutputtypesforindividualdomainsinQUINTD.
freeAPIonlyallowedtodownloadthedataforthe allowsonly50requestsperday,butthatdoesnot
time when the request was made. At the time of limitthedatacollectionsincetheendpointreturns
writing,OpenWeatherispushinganewOneCall all the games played on a particular day in a sin-
API3.0whichallowstodownloadweatherdatafor glerequest. Wedownloadedthegamesplayedon
anytimestamp,butonly4daysahead(insteadof5). 27 November 2023 for the development set (184
Theserestrictionssomehowlimitthereplicability games)and29November2023forthetestset(216
of our QUINTD-1 dataset (at least with the free games), taking a random sample of 100 for each
API),butdonotlimitdownloadinganewbatchof split.
datawithasimilarformat.
A.2.4 OurWorldInData
For the QUINTD-1 dataset, we randomly sam-
OurWorldInDataisapublicdatabaseandwebinter-
pled100citiesforeachsplitfromthelistofcities
facefordataaboutworlddevelopmentsinvarious
withapopulationover1000andusedtheircoordi-
domainsandsources. WeusedtheofficialAPI(cur-
natesinthequeriestoOpenWeatherAPI.Allthe
rently experimental) which is accessible through
dataforecastsweredownloadedonJan3,2024.
thePythonpackageowid-catalog. Thepackage
A.2.2 GSMArena allowstoaccessindividualCSVtablesasPandas
GSMArena is a website providing specifications dataframes.
andreviewsformobiledevices. Fordownloading For our data collection, we decided to limit
thedata,weusedtheunofficialgsmarena-apitool ourselves to time series, i.e. a single column
whichreturnsthedatainaJSONformat. Notethat with values changing over time. Besides the
GSMArenaimposeslimitationsonthenumberof simplicity of visualizing such a chart (which
requestsperIPaddress,whichmayinducedelays is used by human annotators for checking the
whendownloadingalargeramountofdata. correctness of the output), there is also a clear
Tocreateabalancedsample,wedownloadedde- goal for the target chart description: describing
tailedspecificationof10productsfromeachavail- the developments of a value over time. We
ablebrand,andrandomlyselected100productsfor also limited ourselves to the health domain. In
eachsplitfromthedownloadedset. particular, we selected the tables COVID data
(columns new_cases_smoothed_per_million,
A.2.3 RapidAPIIceHockey
new_tests_smoothed_per_thousand, people_
RapidAPIisaservicewhichprovidesAPIaccess
vaccinated_per_hundred, reproduction_
to data from multiple domains, including sport,
rate, and positive_rate) and Life expectancy
finance, entertainment, and others. Most of the
data(columnlife_expectancy_0).
APIsareprovidedinafreemiummode,i.e. witha
We downloaded the data for all countries with
limitednumberofAPIcallsperday.
non-empty entries in the table, taking a random
For QUINTD, we selected the IceHockeyAPI
sampleof100examplesforeachsplit. Onmodel
(popularity9.1/10)whichprovidesaccesstoice
input,weformattedthedataforeachtimeseriesas
hockeygamesfromworldtopleagues. Ourchoice
two-column CSV, including the title, the descrip-
was influenced by our own personal preferences,
tion,andtheunitforeachexampleasacomment
combined with the desire to cover a sport which
(#)atthebeginningoftheinput.
has not been covered previously in sports report
generation. A.2.5 Wikidata
We used the matches endpoint which returns Wikidataisalargeopen-sourceknowledgegraph,
high-leveldetailsaboutagame. NotethattheAPI containing factual information about entities andtheirproperties. Wikidataprovidesaccessthrough pieceoftextasanerror,ordidnotmanagetocopy
anofficialAPI,butweinsteaddecidedtoextract theinputtextverbatim.
our data using the wikidatasets (Boschin and
C HumanEvaluation
Bonald,2019)Pythonlibrary,whichprovidesac-
cess to preprocessed properties of entities from Asdescribedin§4.2,wesetupthehumanevalua-
particulardomains. Itallowedustoavoidcrawling tioncampaignonProlific. Tomakethedatamore
and filtering the knowledge graph and its offline accessible to the annotators, we created custom
processingmadethedatacollectionfaster.9
datavisualizationsforeachdomain. Forthedatain
For our dataset, we selected the entities from openweatherandowid,weusedinteractivegraphs
the companies, countries, films, and humans fromHighcharts.com,andwemanuallycreatedthe
domains. Foreachentity,werandomlyextracted tablesforotherdomains. Youcanfindthefullin-
between2to10propertiesintheknowledgegraph. structionsforhumanannotatorsinFigure5andthe
Weextractedupto100subgraphsforeachdomain examplesofdatavisualizationsinAppendixE.
and took a random sample of 100 subgraphs for
each split. On model input, we formatted each D MetricCorrelation
subgraph as a simple Markdown-formatted text
The Pearson correlation coefficients (§5.2) were
snippet,usingtheentityasatitleandincludinga
computedusingtwolists(forE andE )asfol-
hum gpt
bulletpointforeachkey-valuepair.
lows(notethateacherrorcategorywasconsidered
separately):
B GPT-4Evaluation
• Forr ,weconcatenatedtheaverageerror
domain
We used the prompt in Figure 4 for instantiating
countsperdomain(seeTable12).
the GPT-4-based metric.10 We ensured that the
• Forr ,weconcatenatedthecountofer-
outputisavalidJSONusingarecentlyintroduced example
rorsperexample.
parameter response_format in the OpenAI API.
Atthepriceof$0.01per1kinputtokensand$0.03 • Forr token,weconcatenatedthebinaryindica-
per1kgeneratedtokens,theevaluationprocesscost torsmarkinganerrorpertoken.
approximately$45intotal.
E Examples
B.1 AligningtheErrors
Herewepresentanexampleofinputsandmodel
For aligning the errors with the original text, we outputs(alongwithannotations)foreachdomain:
performstringmatchingonthetextspandecoded
• openweather: Figure7(in)andTable7(out),
by GPT-4 in the TEXT_SPAN field. In our prelimi-
• gsmarena: Figure8(in)andTable7(out),
naryexperiments,thismethodprovedtobemore
robustthaneitheraskingforstartandendindices • ice_hockey: Figure9(in)andTable9(out),
oftheerrorspan(whichwouldrelyonthemodel’s
• owid: Figure10(in)andTable10(out),
abilitytocountcharacters)orperformingsequence
• wikidata: Figure11(in)andTable11(out).
taggingonthecopyoftheinput(whichwouldrely
onthemodel’sabilitytoperfectlycopytheinput). Notethatthegraphsforopenweatherandowid
We tried to respect monotonic ordering of text areinteractivewhenaccessedthroughthewebin-
spansbutfellbacktofull-textsearchifthespanis terface.
notfoundfollowingthepreviousone. Weconsider
F FullResults
thisapproachsuccessfulsincematchingcompletely
failedonlyinaminorityofcases(137outof6927).
Hereweincludethetableswithresultsforindivid-
Basedonourmanualexamination,thesemostlyin-
ualdomains:
cludecaseswhereGPT-4triedtosuggestamissing
• Table12presentstheaveragenumbersofer-
9Alltheentitiesandpropertiesarelinkedwithanidentifier rors per output separately for each domain
totheWikidatadatabase,makingtheprocessalsoreplicable
(theaggregatedresultsareinTable3),
throughtheofficialAPI.
10Note that the example in the prompt differs from the • Table13presentstheratioofoutputscontain-
exampleusedforhumanannotators(seeFigure5).Werevised
ingatleastoneerrorseparatelyforeachdo-
theexampletobemoreinstructivebutwewerenotableto
re-runtheGPT-4evaluationduetoourlimitedbudget. main(theaggregatedresultsareinTable4).SystemMessage
You are an expert data-to-text error annotation system. You undestand structured data and you
can correcly operate with units and numerical values. You are designed to output token-level
annotations in JSON.
Prompt
Given the data:
ˋˋˋ
data
ˋˋˋ
Annotate all the errors in the following text:
ˋˋˋ
text
ˋˋˋ
Output the errors as a JSON list "errors" in which each object contains fields "reason", "text",
and "type". The value of "text" is the text of the error. The value of "reason" is the reason
for the error. The value of "type" is one of 0, 1, 2, 3 based on the following list:
- 0: Incorrect fact: The fact in the text contradicts the data.
- 1: Not checkable: The fact in the text cannot be checked in the data.
- 2: Misleading: The fact in the text is misleading in the given context.
- 3: Other: The text is problematic for another reason, e.g. grammatically or stylistically
incorrect, irrelevant, or repetitive.
The list should be sorted by the position of the error in the text.
*Example:*
data:
ˋˋˋ
[ [ "Aditi Bhagwat", "occupation", "television actor" ], [ "Aditi Bhagwat", "date of birth", "18
January 1981" ] ]
ˋˋˋ
text:
ˋˋˋ
Aditi Bhagwat, born on January 18, 1991, used to be a popular Indian television actor. The data
comes from a knowledge graph.
ˋˋˋ
output:
ˋˋˋ "errors": ["reason": "The data mentions that the actor was born on 1981", "text": "1991",
"type": 0, "reason": "Misleadingly suggests that the actor is not alive", "text": "used to
be", type: 2, "reason": "Popularity is not mentioned in the data", "text": "popular", type: 1,
"reason", "Nationality is not mentioned in the data", "text": "Indian", type: 1, "reason": "The
note is superfluous", "text": "The data comes from a knowledge graph.", type: 3]
ˋˋˋ
Note that some details may not be mentioned in the text: do not count omissions as errors. Also
do not be too strict: some facts can be less specific than in the data (rounded values, shortened
or abbreviated text, etc.), do not count these as errors. If there are no errors in the text,
"errors" will be an empty list.
Figure4: ThepromptweusedfortheGPT-4evaluationmetric.In this task, you will annotate 20 examples in total. For each example, you will see data on the left side and the
correspondinggeneratedtextontherightside.Yourtaskistoannotateerrorsinthetextwithrespecttothedata.
Therearefivetypesoferrorsthatyoucanmarkinthegeneratedtext:
1. Incorrectfact:Thefactinthetextcontradictsthedata.
2. Notcheckable:Thefactinthetextcannotbecheckedgiventhedata.
3. Misleading:Thefactinthetextismisleadinginthegivencontext.
4. Other: Thetextisproblematicforanotherreason, e.g. grammaticallyorstylisticallyincorrect, irrelevant, or
repetitive.
Howtomarkandsubmittheannotations?
Useyourmousetohighlightspecificpartsofthetextcontainingtheerrors.Toswitchbetweenerrorcategories,repeatedly
clickonthehighlightedtext(thelastclickremovesthehighlight).Notethathighlightingfromtherighttoleftcanwork
betterforlongerspans.
Onceyouthinkyouhavemarkedalltheerrorspresentinthetext,clicktheMarkexampleascompletebutton(youcan
stillupdatetheannotationlater).Youwillbeabletosubmittheannotationsoncetheyareallaremarkedascomplete.
HowshouldIdecideontheerrors?
• Eacherrorspanshouldincludeallthewordsrelatedtotheerror(butnothingelse).
• Ifyouthinkthefactisprobablytrue,butcannotbederivedfromthedata,markitasnotcheckable.
• Ifyouarenotreallysureifthefactshouldbemarkedasanerror,leaveitunmarked.
Anexampleoftheannotatedoutput
Anexampleofthedatainputandthecorrespondingtextannotatedwitherrors:
data
Nokia3310
• color:black,blue,grey
• display:320x240px
text(productdescription)
Nokia3310isproducedinFinlandandfeaturesa320x320display.Itis availableinblackcolor.Thedataseemtoprovide
onlypartialinformationaboutthephone.
explanation
• producedinFinland:Thecountrywherethephoneisproducedisnotmentionedinthedata.
• 320x320:Thedatamentionsthatthedisplayhasresolution320x240px.
• availableinblackcolor:Misleadinglysuggeststhatthephoneisnotavailableinothercolors.
• The data seem to provide only partial information about the phone.: The note is irrelevant for the phone
description.
Notethatforthesakeofbrevity,thisparticularexampleisbasedonasmalldatainputandcontainsmanyerrors,which
maynotalwaysbethecase.
WhatkindsofdataandtextcanIencounter?
Youcanencounterthefollowingkindsoftexts:
• a5-dayweatherforecastgeneratedfromweatherdata,
• adescriptionofaproductgeneratedfromproductspecifications
• anicehockeygamereportgeneratedfrominformationaboutthegame,
• acaptionofahealth-relatedchart,
• adescriptionofanentity(human,country,film,orcompany)basedonitsproperties.
Whatisthesourceofthedataandthetexts?
Thedataisdownloadedfrompublicsources(openweathermap.org,gsmarena.com,rapidapi.com,ourworldindata.com,
wikidata.com).Thetextsaregeneratedbylargelanguagemodels.
Whatisthepurposeoftheresearch?
TheresearchisconductedbytheNLPresearchgroupatCharlesUniversityinPrague.TheresearchisapartoftheERC
projectNext-GenerationNaturalLanguageGenerationandwasapprovedbytheethicalcommitteeoftheproject.
Bycontinuing, youagreethattheanonymousannotationswillbemadeavailabletotheresearchcommunitytohelp
evaluatingandimprovingdata-to-textgenerationsystemsbasedonlargelanguagemodels.
Figure5: Theinstructionsgiventothehumanannotators.Figure6: Theannotationinterfaceforhumanannotators.
Figure7: Anexampleofaninputfromopenweather(seetheoutputsinTable7).Model Humanannotations(E ) GPT-4annotations(E )
hum gpt
Llama2 TheweatherinNiemegk,Germany,willbecloudy TheweatherinNiemegk,Germany,willbecloudy
andrainyforthenextfewdays.OnJanuary3rd,the andrainyforthenextfewdays.OnJanuary3rd,the
temperaturewillbearound9.55°C,withafeels-like temperaturewillbearound9.55°C,withafeels-like
temperature of 6.59°C. The weather will be light temperature of 6.59°C. The weather will be light
rain,withawindspeedof6.24m/sandagustof rain,withawindspeedof6.24m/sandagustof
12.1 m/s. The rain will last for 3 hours, and the 12.1 m/s. The rain will last for 3 hours, and the
pressurewillbe991hPa. pressurewillbe991hPa.
Mistral FromJanuary3rdtoJanuary7th,thecityofNiemegk FromJanuary3rdtoJanuary7th,thecityofNiemegk
in Germany will experience a range of weather in Germany will experience a range of weather
conditions. On January 3rd, the temperature will conditions. On January 3rd, the temperature will
be9.55°Cwithafeelsliketemperatureof6.59°C. be9.55°Cwithafeelsliketemperatureof6.59°C.
Therewillbelightrainwitha3-hourprecipitation Therewillbelightrainwitha3-hourprecipitation
of1.06mm. Thewindspeedwillbe6.24m/swith of1.06mm. Thewindspeedwillbe6.24m/swith
agustof12.1m/s.OnJanuary4th,thetemperature agustof12.1m/s.OnJanuary4th,thetemperature
willbe8.2°Cwithafeelsliketemperatureof5.64°C. willbe8.2°Cwithafeelsliketemperatureof5.64°C.
Therewillbelightrainwitha3-hourprecipitationof Therewillbelightrainwitha3-hourprecipitationof
1.94mm.Thewindspeedwillbe4.31m/swithagust 1.94mm.Thewindspeedwillbe4.31m/swithagust
of11.6m/s. OnJanuary5th, thetemperaturewill of11.6m/s. OnJanuary5th, thetemperaturewill
be6.37°Cwithafeelsliketemperatureof2.25°C. be6.37°Cwithafeelsliketemperatureof2.25°C.
Therewillbelightrainwitha3-hourprecipitation Therewillbelightrainwitha3-hourprecipitation
of1.13mm. Thewindspeedwillbe7.07m/swith of1.13mm. Thewindspeedwillbe7.07m/swith
agustof15.51m/s. OnJanuary6th, thetempera-
agustof15.51m/s. OnJanuary6th, thetempera-
turewillbe2.17°Cwithafeelsliketemperatureof
turewillbe2.17°Cwithafeelsliketemperatureof
-3.55°C.Therewillbeovercastcloudswitha3-hour
-3.55°C.Therewillbeovercastcloudswitha3-hour
precipitationof0.31mm. Thewindspeedwillbe
precipitationof0.31mm. Thewindspeedwillbe
3.54m/swithagustof7.11m/s.OnJanuary7th,the
3.54m/swithagustof7.11m/s.OnJanuary7th,the
temperaturewillbe-0.89°Cwithafeelsliketemper-
temperaturewillbe-0.89°Cwithafeelsliketemper-
atureof-0.89°C.Therewillbeovercastcloudswith
atureof-0.89°C.Therewillbeovercastcloudswith
a3-hourprecipitationof0.31mm.Thewindspeed
a3-hourprecipitationof0.31mm.Thewindspeed
willbe0.63m/swithagustof0.7m/s.
willbe0.63m/swithagustof0.7m/s.
Zephyr Over the next five days, the weather in Niemegk, Over the next five days, the weather in Niemegk,
Germanywillbecharacterizedbyfluctuatingtem- Germanywillbecharacterizedbyfluctuatingtem-
peraturesandvaryingprecipitation. OnThursday, peraturesandvaryingprecipitation. OnThursday,
January3,lightrainandalowtemperatureof9.55°C January3,lightrainandalowtemperatureof9.55°C
canbeexpected,withachanceofraincontinuing canbeexpected,withachanceofraincontinuing
intoFriday. SnowfallispossibleonFriday,witha intoFriday. SnowfallispossibleonFriday,witha
lowtemperatureof-0.89°C.Saturdaywillbeover- lowtemperatureof-0.89°C.Saturdaywillbeover-
castwithalowtemperatureof0.03°C,andlightrain castwithalowtemperatureof0.03°C,andlightrain
isforecastedforSaturdayevening.Sundaywillbe isforecastedforSaturdayevening.Sundaywillbe
overcastaswell,withalowtemperatureof-2.23°C. overcastaswell,withalowtemperatureof-2.23°C.
Mondaywillseeadropintemperatures,withalow Mondaywillseeadropintemperatures,withalow
of-4.07°Candscatteredclouds.Temperatureswill of-4.07°Candscatteredclouds.Temperatureswill
continuetofallthroughouttheweek,withlowsof continuetofallthroughouttheweek,withlowsof
-5.19°Cand-9.51°ConTuesdayandWednesday,re- -5.19°Cand-9.51°ConTuesdayandWednesday,re-
spectively.Windspeedswillvaryfrom3.1to15.51 spectively.Windspeedswillvaryfrom3.1to15.51
meterspersecond,withgustsreachingupto15.51 meterspersecond,withgustsreachingupto15.51
meterspersecondonMonday. meterspersecondonMonday.
GPT-3.5 Over the next five days, the weather in Niemegk, Over the next five days, the weather in Niemegk,
Germany is expected to be quite varied. On Jan- Germany is expected to be quite varied. On Jan-
uary4th,therewillbelightrainwithtemperatures uary4th,therewillbelightrainwithtemperatures
rangingfrom2.17°Cto9.55°C.Thefollowingday, rangingfrom2.17°Cto9.55°C.Thefollowingday,
January5th,therewillbelightsnowwithtempera- January5th,therewillbelightsnowwithtempera-
turesdroppingtoaslowas-0.89°C.OnJanuary6th, turesdroppingtoaslowas-0.89°C.OnJanuary6th,
theweatherwillbeovercastwithtemperaturesrang- theweatherwillbeovercastwithtemperaturesrang-
ingfrom-0.32°Cto1.69°C.Thetrendcontinueson ingfrom-0.32°Cto1.69°C.Thetrendcontinueson
January7thwithovercastcloudsandtemperatures January7thwithovercastcloudsandtemperatures
droppingtoaslowas-2.51°C. Finally,onJanuary droppingtoaslowas-2.51°C. Finally,onJanuary
8th,therewillbescatteredcloudswithtemperatures 8th,therewillbescatteredcloudswithtemperatures
rangingfrom-5.19°Cto-3.67°C.Throughoutthis rangingfrom-5.19°Cto-3.67°C.Throughoutthis
period,windspeedswillvarybetween0.63m/sand period,windspeedswillvarybetween0.63m/sand
8.12m/s,andthepressurewillrangefrom991hPa 8.12m/s,andthepressurewillrangefrom991hPa
to1034hPa. to1034hPa.
Table7: Anexampleofthemodeloutputsonopenweather(seetheinputinFigure7).Figure8: Anexampleofaninputfromgsmarena(seetheoutputsinTable8).Model Humanannotations(E ) GPT-4annotations(E )
hum gpt
Llama2 The Gigabyte GSmart Mika M3 is a smartphone The Gigabyte GSmart Mika M3 is a smartphone
thatwasreleasedin2014.Ithasa5-inchIPSLCD thatwasreleasedin2014.Ithasa5-inchIPSLCD
displaywitharesolutionof720x1280pixelsanda displaywitharesolutionof720x1280pixelsanda
13MPmaincamera. Thedevicealsohasan8MP 13MPmaincamera. Thedevicealsohasan8MP
frontcameraandispoweredbyaMediatekMT6582 frontcameraandispoweredbyaMediatekMT6582
chipsetwith1GBofRAMand8GBofinternalstor- chipsetwith1GBofRAMand8GBofinternalstor-
age.IthasdualSIMsupportandisavailableinNavy
age.IthasdualSIMsupportandisavailableinNavy
Blue.
Blue.
Mistral TheGSmartMikaM3isasmartphonedeveloped TheGSmartMikaM3isasmartphonedeveloped
by Gigabyte. It was released in November 2014 by Gigabyte. It was released in November 2014
andispoweredbytheAndroid4.4.2operatingsys- andispoweredbytheAndroid4.4.2operatingsys-
tem. Thedevicefeaturesa5.0-inchIPSLCDdis- tem. Thedevicefeaturesa5.0-inchIPSLCDdis-
playwitharesolutionof720x1280pixels,which playwitharesolutionof720x1280pixels,which
translatesto294pixelsperinch.Thesmartphoneis translatesto294pixelsperinch.Thesmartphoneis
equippedwitha13-megapixelrearcamerawithauto- equippedwitha13-megapixelrearcamerawithauto-
focusandLEDflash,aswellasan8-megapixelfront- focusandLEDflash,aswellasan8-megapixelfront-
facingcamera.ItispoweredbyaMediatekMT6582 facingcamera.ItispoweredbyaMediatekMT6582
chipsetwithaquad-core1.3GHzCortex-A7CPU chipsetwithaquad-core1.3GHzCortex-A7CPU
andaMali-400MP2GPU.Thedevicehas8GBof andaMali-400MP2GPU.Thedevicehas8GBof
internalstorageand1GBofRAM,withtheoptionto internalstorageand1GBofRAM,withtheoptionto
expandstorageviaamicroSDHCcard.TheGSmart expandstorageviaamicroSDHCcard.TheGSmart
MikaM3alsofeaturesWi-Fi,Bluetooth,GPS,and MikaM3alsofeaturesWi-Fi,Bluetooth,GPS,and
NFCconnectivity,aswellasa3.5mmheadphone NFCconnectivity,aswellasa3.5mmheadphone
jackandamicroUSB2.0portforcharginganddata jackandamicroUSB2.0portforcharginganddata
transfer.ThedeviceisavailableinNavyBlueandis transfer.ThedeviceisavailableinNavyBlueandis
poweredbya1900mAhLi-Ionbattery,whichpro- poweredbya1900mAhLi-Ionbattery,whichpro-
videsupto150hoursofstand-bytimeandupto4 videsupto150hoursofstand-bytimeandupto4
hoursand30minutesoftalktime. hoursand30minutesoftalktime.
Zephyr TheGigabyteGSmartMikaM3isasmartphonethat TheGigabyteGSmartMikaM3isasmartphonethat
boasts a 5.0-inch IPS LCD display with a resolu- boasts a 5.0-inch IPS LCD display with a resolu-
tionof720x1280pixels.ItrunsonAndroid4.4.2 tionof720x1280pixels.ItrunsonAndroid4.4.2
(KitKat) and is powered by a quad-core 1.3 GHz (KitKat) and is powered by a quad-core 1.3 GHz
Cortex-A7processorwithaMali-400MP2GPU.The Cortex-A7processorwithaMali-400MP2GPU.The
devicecomeswith8GBofinternalstorageand1GB devicecomeswith8GBofinternalstorageand1GB
of RAM, which can be expanded via a dedicated of RAM, which can be expanded via a dedicated
microSDHCcardslot.Theprimarycameraisa13- microSDHCcardslot.Theprimarycameraisa13-
megapixelautofocuslenswithanLEDflash,while megapixelautofocuslenswithanLEDflash,while
thesecondarycameraisan8-megapixellens.Con- thesecondarycameraisan8-megapixellens.Con-
nectivity options include Wi-Fi, Bluetooth, GPS, nectivity options include Wi-Fi, Bluetooth, GPS,
FMradio,andmicroUSB2.0. Thesmartphoneis FMradio,andmicroUSB2.0. Thesmartphoneis
equippedwitha1900mAhremovableLi-Ionbattery equippedwitha1900mAhremovableLi-Ionbattery
thatprovidesupto4hoursoftalktimeandupto thatprovidesupto4hoursoftalktimeandupto
150hoursofstandbytime.Thedeviceisavailablein 150hoursofstandbytime.Thedeviceisavailablein
NavyBlueandsupportsdualSIMcards(Mini-SIM/
NavyBlueandsupportsdualSIMcards(Mini-SIM/
Micro-SIM).
Micro-SIM).
GPT-3.5 The GSmart Mika M3, also known as the Giga- The GSmart Mika M3, also known as the Giga-
byte GSmart Mika M3, is a smartphone that was byte GSmart Mika M3, is a smartphone that was
announcedinOctober2014andreleasedinNovem- announcedinOctober2014andreleasedinNovem-
berofthesameyear.Itfeaturesa5.0-inchIPSLCD berofthesameyear.Itfeaturesa5.0-inchIPSLCD
displaywitharesolutionof720x1280pixelsand displaywitharesolutionof720x1280pixelsand
ispoweredbyaQuad-core1.3GHzCortex-A7pro- ispoweredbyaQuad-core1.3GHzCortex-A7pro-
cessor. Thephonecomeswith1GBofRAMand cessor. Thephonecomeswith1GBofRAMand
8GB of internal storage, which can be expanded 8GB of internal storage, which can be expanded
via a microSDHC card slot. The main camera is via a microSDHC card slot. The main camera is
13MP with LED flash, while the selfie camera is 13MP with LED flash, while the selfie camera is
8MP. The phone runs on Android 4.4.2 (KitKat) 8MP. The phone runs on Android 4.4.2 (KitKat)
andisequippedwitha1900mAhremovablebattery. andisequippedwitha1900mAhremovablebattery.
OtherfeaturesincludedualSIMcapability,Wi-Fi, OtherfeaturesincludedualSIMcapability,Wi-Fi,
Bluetooth,GPS,FMradio,andmicroUSB2.0.The Bluetooth,GPS,FMradio,andmicroUSB2.0.The
GSmartMikaM3isavailableinthecolorNavyBlue GSmartMikaM3isavailableinthecolorNavyBlue
andhasasleekandcompactdesign,measuring144 andhasasleekandcompactdesign,measuring144
x70.5x8.3mmandweighing146g. x70.5x8.3mmandweighing146g.
Table8: Anexampleofthemodeloutputsongsmarena(seetheinputinFigure8).Figure9: Anexampleofaninputfromice_hockey(seetheoutputsinTable9).Model Humanannotations(E ) GPT-4annotations(E )
hum gpt
Llama2 IntheVHL23/24season,KristallSaratovhosted IntheVHL23/24season,KristallSaratovhosted
HC Norilsk at their home arena. Kristall Saratov HC Norilsk at their home arena. Kristall Saratov
won the game with a final score of 1-5. Kristall wonthegamewithafinalscoreof1-5.KristallSara-
Saratov’s home team scored one goal in the first tov’shometeamscoredonegoalinthefirstperiod,
period, whileHCNorilskscoredtwogoalsinthe whileHCNorilskscoredtwogoalsinthesecond
secondandthirdperiods. Thegamelastedfor60 andthirdperiods. Thegamelastedfor60minutes,
minutes,withKristallSaratov’shometeamleading withKristallSaratov’shometeamleadingthegame
thegamefortheentireduration. fortheentireduration.
Mistral OnNovember28th, 2023, theVHL23/24season OnNovember28th, 2023, theVHL23/24season
gamebetweenKristallSaratovandHCNorilsktook gamebetweenKristallSaratovandHCNorilsktook
place. Thegamewasplayedatthehomearenaof place. Thegamewasplayedatthehomearenaof
KristallSaratov,withthehometeamwinning1-0. KristallSaratov,withthehometeamwinning1-0.
Thegamewasplayedinthreeperiods,withthefirst Thegamewasplayedinthreeperiods,withthefirst
periodendingina0-0tie,thesecondperiodending periodendingina0-0tie,thesecondperiodending
ina2-3scoreinfavorofHCNorilsk,andthethird ina2-3scoreinfavorofHCNorilsk,andthethird
period ending in a 1-0 score in favor of Kristall periodendingina1-0scoreinfavorofKristallSara-
Saratov. The game ended in normal time, with a tov. Thegameendedinnormaltime,withatotal
totalscoreof3-1infavorofHCNorilsk.
scoreof3-1infavorofHCNorilsk.
Zephyr IntheVHL23/24season,KristallSaratovhosted IntheVHL23/24season,KristallSaratovhostedHC
HCNorilsk. Thefirstperiodsawtwoquickgoals Norilsk.Thefirstperiodsawtwoquickgoalsfrom
fromNorilsk,withoneinthesecondandthirdpe- Norilsk,withoneinthesecondandthirdperiodsas
riods as well, resulting in a dominant 5-0 victory. well,resultinginadominant5-0victory. Kristall
KristallSaratovmanagedtoscoreasinglegoalin Saratovmanagedtoscoreasinglegoalinthethird
thethirdperiod,butitwasnotenoughtoovercome period,butitwasnotenoughtoovercomeNorilsk’s
Norilsk’slead. Thefinalscorewas1-5infavorof lead.Thefinalscorewas1-5infavorofHCNorilsk.
HCNorilsk.
GPT-3.5 In a match from the VHL 23/24 season, Kristall In a match from the VHL 23/24 season, Kristall
Saratov faced off against HC Norilsk. The game Saratov faced off against HC Norilsk. The game
tookplaceonNovember28,2023,at15:30.Despite tookplaceonNovember28,2023,at15:30.Despite
agoalfromKristallSaratovinthethirdperiod,HC agoalfromKristallSaratovinthethirdperiod,HC
Norilskemergedvictoriouswithafinalscoreof5-1. Norilskemergedvictoriouswithafinalscoreof5-1.
Theawayteamdominatedthegame,scoring2goals Theawayteamdominatedthegame,scoring2goals
inthefirstperiodand3goalsinthesecondperiod. inthefirstperiodand3goalsinthesecondperiod.
KristallSaratovstruggledtokeepup,resultingina KristallSaratovstruggledtokeepup,resultingina
clearwinforHCNorilsk. clearwinforHCNorilsk.
Table9: Anexampleofthemodeloutputsonice_hockey(seetheinputinFigure9).Figure10: Anexampleofaninputfromowid(seetheoutputsinTable10).
Model Humanannotations(E ) GPT-4annotations(E )
hum gpt
Llama2 LifeexpectancyatbirthinDjiboutihasincreased LifeexpectancyatbirthinDjiboutihasincreased
significantlyoverthepastfewdecades,withano- significantlyoverthepastfewdecades,withano-
tableincreasefrom40.5665yearsin1950to62.3049 tableincreasefrom40.5665yearsin1950to62.3049
yearsin2021. yearsin2021.
Mistral This chart displays the life expectancy at birth in This chart displays the life expectancy at birth in
Djiboutifrom1950to2021.Thedatashowsasteady Djiboutifrom1950to2021.Thedatashowsasteady
increaseinlifeexpectancyovertheyears,withasig- increaseinlifeexpectancyovertheyears,withasig-
nificantjumpinthe1970sand1980s.In2021,the nificantjumpinthe1970sand1980s.In2021,the
averagelifeexpectancyatbirthinDjiboutiwas62.3 averagelifeexpectancyatbirthinDjiboutiwas62.3
years. years.
Zephyr Thislinegraphillustratestheevolutionoflifeex- Thislinegraphillustratestheevolutionoflifeex-
pectancy at birth in Djibouti from 1950 to 2021. pectancy at birth in Djibouti from 1950 to 2021.
Initially, the average number of years that a new- Initially, the average number of years that a new-
borncouldexpecttolivewasaround40.5yearsin borncouldexpecttolivewasaround40.5yearsin
1950,anditgraduallyincreasedoverthefollowing 1950,anditgraduallyincreasedoverthefollowing
decades, reaching 62.3 years in 2021. The trend decades, reaching 62.3 years in 2021. The trend
showsasteadyimprovement,withsomefluctuations
showsasteadyimprovement,withsomefluctuations
inthe1980sand1990s.Thedatapresentedhereis
inthe1980sand1990s.Thedatapresentedhereis
basedonthesex-andage-specificdeathratespre-
basedonthesex-andage-specificdeathratespre-
vailingatthetimeofbirthforaspecificyearina
vailingatthetimeofbirthforaspecificyearina
givencountry,territory,orgeographicarea.
givencountry,territory,orgeographicarea.
GPT-3.5 ThechartshowsthelifeexpectancyatbirthinDji- ThechartshowsthelifeexpectancyatbirthinDji-
boutifrom1950to2021. Thelifeexpectancyhas boutifrom1950to2021. Thelifeexpectancyhas
steadilyincreasedovertheyears,from40.57yearsin steadilyincreasedovertheyears,from40.57yearsin
1950to62.30yearsin2021,withsomefluctuations 1950to62.30yearsin2021,withsomefluctuations
inbetween.Thedatareflectstheaveragenumberof inbetween.Thedatareflectstheaveragenumberof
yearsanewborncouldexpecttoliveifexposedto yearsanewborncouldexpecttoliveifexposedto
theprevailingdeathratesatthetimeofbirth. theprevailingdeathratesatthetimeofbirth.
Table10: Anexampleofthemodeloutputsonowid(seetheinputinFigure10).Figure11: Anexampleofaninputfromwikidata(seetheoutputsinTable11).
Model Humanannotations(E ) GPT-4annotations(E )
hum gpt
Llama2 Driven Brands is an American franchise holding Driven Brands is an American franchise holding
company that owns, operates, and franchises au- companythatowns,operates,andfranchisesauto-
tomotive services businesses in the United States. motiveservicesbusinessesintheUnitedStates.The
Thecompanywasfoundedin1996andisheadquar- companywasfoundedin1996andisheadquartered
teredinCharlotte,NorthCarolina.Itoperatesseveral in Charlotte, North Carolina. It operates several
brands, including Maaco, Meineke, Econo Lube,
brands,includingMaaco,Meineke,EconoLube,and
andCARSTAR.
CARSTAR.
Mistral DrivenBrandsisabusinessoperatingintheautomo- DrivenBrandsisabusinessoperatingintheautomo-
tiveservicesindustryintheUnitedStatesofAmer- tiveservicesindustryintheUnitedStatesofAmer-
ica. ica.
Zephyr DrivenBrandsisabusinessoperatingintheauto- DrivenBrandsisabusinessoperatingintheauto-
motiveservicesindustrybasedintheUnitedStates motiveservicesindustrybasedintheUnitedStates
of America. As an instance of the broader cate- of America. As an instance of the broader cate-
goryofbusinesses,DrivenBrandsprovidesgoods goryofbusinesses,DrivenBrandsprovidesgoods
orservicestocustomersinexchangeforfinancial orservicestocustomersinexchangeforfinancial
remuneration. remuneration.
GPT-3.5 Driven Brands is a business based in the United Driven Brands is a business based in the United
StatesofAmerica,operatingwithintheautomotive StatesofAmerica,operatingwithintheautomotive
servicesindustry. servicesindustry.
Table11: Anexampleofthemodeloutputsonwikidata(seetheinputinFigure11).Incorrect NotCheckable Misleading Other Allcategories
E E E E E E E E E E #Tok.
gpt hum gpt hum gpt hum gpt hum gpt hum
openweather 5.77 4.02 1.94 2.06 0.34 0.52 0.43 0.16 8.48 6.76 166.7
gsmarena 1.88 0.65 0.37 0.89 0.33 0.19 0.11 0.09 2.69 1.82 90.3
Llama2 ice_hockey 1.82 1.45 0.85 0.96 0.04 0.09 0.06 0.12 2.77 2.62 63.3
owid 2.18 1.25 0.22 0.58 0.22 0.28 0.01 0.07 2.63 2.18 60.3
wikidata 0.65 0.15 1.13 1.58 0.08 0.09 0.06 0.07 1.92 1.89 38.5
openweather 7.18 5.30 0.59 2.52 0.48 1.35 0.09 0.53 8.34 9.70 193.5
gsmarena 1.86 0.51 0.80 1.10 0.54 0.29 0.05 0.12 3.25 2.02 146.3
Mistral ice_hockey 1.91 1.44 0.72 0.89 0.13 0.14 0.05 0.13 2.81 2.60 92.4
owid 2.67 2.39 0.18 0.40 0.41 0.29 0.04 0.07 3.30 3.15 91.1
wikidata 0.40 0.10 0.32 0.71 0.27 0.13 0.32 0.38 1.31 1.32 51.0
openweather 6.46 4.09 0.42 1.11 0.38 0.34 0.01 0.15 7.27 5.69 130.9
gsmarena 1.72 0.34 0.93 1.04 0.85 0.23 0.03 0.10 3.53 1.71 142.8
Zephyr ice_hockey 1.49 0.88 0.19 0.60 0.08 0.10 0.02 0.07 1.78 1.65 83.1
owid 2.51 1.63 0.20 0.52 0.43 0.22 0.01 0.10 3.15 2.47 85.2
wikidata 0.33 0.09 0.24 0.70 0.23 0.11 0.21 0.36 1.01 1.26 48.1
openweather 3.88 1.48 0.17 0.65 0.96 0.34 0.00 0.05 5.01 2.52 112.8
gsmarena 1.30 0.20 1.13 0.80 0.63 0.21 0.03 0.14 3.09 1.35 129.5
GPT-3.5 ice_hockey 0.72 0.81 0.12 0.48 0.05 0.07 0.00 0.08 0.89 1.44 84.4
owid 1.81 0.63 0.03 0.25 0.35 0.17 0.00 0.01 2.19 1.06 62.2
wikidata 0.14 0.05 0.13 0.35 0.11 0.06 0.06 0.05 0.44 0.51 35.7
Table12: Theaveragenumbersoferrorsperoutputforeachdomain(lowerisbetter). Wealsoincludetheaverage
numberoftokensperoutputintherightmostcolumn. SeeTable3foraggregatedresults.
Incorrect NotCheckable Misleading Other Allcategories
E E E E E E E E E E
gpt hum gpt hum gpt hum gpt hum gpt hum
openweather 0.96 0.74 0.70 0.70 0.26 0.29 0.25 0.12 1.00 0.88
gsmarena 0.83 0.38 0.28 0.48 0.21 0.18 0.06 0.05 0.90 0.74
Llama2 ice_hockey 0.77 0.69 0.49 0.50 0.04 0.07 0.05 0.10 0.93 0.88
owid 0.92 0.68 0.18 0.37 0.19 0.24 0.01 0.06 0.93 0.87
wikidata 0.42 0.12 0.63 0.79 0.07 0.09 0.06 0.05 0.83 0.87
openweather 0.99 0.75 0.23 0.60 0.23 0.37 0.06 0.17 1.00 0.91
gsmarena 0.89 0.33 0.45 0.53 0.33 0.18 0.05 0.11 0.99 0.72
Mistral ice_hockey 0.83 0.73 0.52 0.60 0.13 0.12 0.04 0.09 0.96 0.87
owid 0.91 0.74 0.17 0.27 0.29 0.24 0.04 0.06 0.98 0.85
wikidata 0.29 0.08 0.25 0.43 0.19 0.10 0.20 0.24 0.70 0.66
openweather 0.99 0.80 0.27 0.47 0.30 0.23 0.01 0.13 1.00 0.90
gsmarena 0.87 0.23 0.53 0.47 0.45 0.19 0.02 0.08 0.99 0.60
Zephyr ice_hockey 0.79 0.54 0.16 0.38 0.08 0.10 0.02 0.06 0.84 0.75
owid 0.93 0.68 0.14 0.31 0.30 0.20 0.01 0.07 0.96 0.83
wikidata 0.28 0.06 0.20 0.48 0.21 0.07 0.15 0.22 0.60 0.64
openweather 0.96 0.61 0.15 0.33 0.53 0.21 0.00 0.05 0.99 0.74
gsmarena 0.69 0.15 0.68 0.42 0.34 0.13 0.03 0.11 0.93 0.56
GPT-3.5 ice_hockey 0.52 0.64 0.09 0.30 0.05 0.07 0.00 0.07 0.56 0.76
owid 0.90 0.42 0.02 0.13 0.30 0.14 0.00 0.01 0.92 0.57
wikidata 0.12 0.04 0.11 0.26 0.10 0.06 0.06 0.05 0.33 0.37
Table13: Theratioofoutputscontainingatleastoneerror foreachdomain(lowerisbetter). SeeTable4for
aggregatedresults.