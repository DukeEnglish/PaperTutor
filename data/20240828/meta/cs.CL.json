[
    {
        "title": "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations",
        "authors": "Yucheng JiangYijia ShaoDekun MaSina J. SemnaniMonica S. Lam",
        "links": "http://arxiv.org/abs/2408.15232v1",
        "entry_id": "http://arxiv.org/abs/2408.15232v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15232v1",
        "summary": "While language model (LM)-powered chatbots and generative search engines\nexcel at answering concrete queries, discovering information in the terrain of\nunknown unknowns remains challenging for users. To emulate the common\neducational scenario where children/students learn by listening to and\nparticipating in conversations of their parents/teachers, we create\nCollaborative STORM (Co-STORM). Unlike QA systems that require users to ask all\nthe questions, Co-STORM lets users observe and occasionally steer the discourse\namong several LM agents. The agents ask questions on the user's behalf,\nallowing the user to discover unknown unknowns serendipitously. To facilitate\nuser interaction, Co-STORM assists users in tracking the discourse by\norganizing the uncovered information into a dynamic mind map, ultimately\ngenerating a comprehensive report as takeaways. For automatic evaluation, we\nconstruct the WildSeek dataset by collecting real information-seeking records\nwith user goals. Co-STORM outperforms baseline methods on both discourse trace\nand report quality. In a further human evaluation, 70% of participants prefer\nCo-STORM over a search engine, and 78% favor it over a RAG chatbot.",
        "updated": "2024-08-27 17:50:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15232v1"
    },
    {
        "title": "LLM Defenses Are Not Robust to Multi-Turn Human Jailbreaks Yet",
        "authors": "Nathaniel LiZiwen HanIan StenekerWillow PrimackRiley GoodsideHugh ZhangZifan WangCristina MenghiniSummer Yue",
        "links": "http://arxiv.org/abs/2408.15221v1",
        "entry_id": "http://arxiv.org/abs/2408.15221v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15221v1",
        "summary": "Recent large language model (LLM) defenses have greatly improved models'\nability to refuse harmful queries, even when adversarially attacked. However,\nLLM defenses are primarily evaluated against automated adversarial attacks in a\nsingle turn of conversation, an insufficient threat model for real-world\nmalicious use. We demonstrate that multi-turn human jailbreaks uncover\nsignificant vulnerabilities, exceeding 70% attack success rate (ASR) on\nHarmBench against defenses that report single-digit ASRs with automated\nsingle-turn attacks. Human jailbreaks also reveal vulnerabilities in machine\nunlearning defenses, successfully recovering dual-use biosecurity knowledge\nfrom unlearned models. We compile these results into Multi-Turn Human\nJailbreaks (MHJ), a dataset of 2,912 prompts across 537 multi-turn jailbreaks.\nWe publicly release MHJ alongside a compendium of jailbreak tactics developed\nacross dozens of commercial red teaming engagements, supporting research\ntowards stronger LLM defenses.",
        "updated": "2024-08-27 17:33:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15221v1"
    },
    {
        "title": "Classifying populist language in American presidential and governor speeches using automatic text analysis",
        "authors": "Olaf van der VeenSemir DzeboLevi LittvayKirk HawkinsOren Dar",
        "links": "http://arxiv.org/abs/2408.15213v1",
        "entry_id": "http://arxiv.org/abs/2408.15213v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15213v1",
        "summary": "Populism is a concept that is often used but notoriously difficult to\nmeasure. Common qualitative measurements like holistic grading or content\nanalysis require great amounts of time and labour, making it difficult to\nquickly scope out which politicians should be classified as populist and which\nshould not, while quantitative methods show mixed results when it comes to\nclassifying populist rhetoric. In this paper, we develop a pipeline to train\nand validate an automated classification model to estimate the use of populist\nlanguage. We train models based on sentences that were identified as populist\nand pluralist in 300 US governors' speeches from 2010 to 2018 and in 45\nspeeches of presidential candidates in 2016. We find that these models classify\nmost speeches correctly, including 84% of governor speeches and 89% of\npresidential speeches. These results extend to different time periods (with 92%\naccuracy on more recent American governors), different amounts of data (with as\nfew as 70 training sentences per category achieving similar results), and when\nclassifying politicians instead of individual speeches. This pipeline is thus\nan effective tool that can optimise the systematic and swift classification of\nthe use of populist language in politicians' speeches.",
        "updated": "2024-08-27 17:19:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15213v1"
    },
    {
        "title": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?",
        "authors": "Kristina GligorićTijana ZrnicCinoo LeeEmmanuel J. CandèsDan Jurafsky",
        "links": "http://arxiv.org/abs/2408.15204v1",
        "entry_id": "http://arxiv.org/abs/2408.15204v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15204v1",
        "summary": "Large language models (LLMs) have shown high agreement with human raters\nacross a variety of tasks, demonstrating potential to ease the challenges of\nhuman data collection. In computational social science (CSS), researchers are\nincreasingly leveraging LLM annotations to complement slow and expensive human\nannotations. Still, guidelines for collecting and using LLM annotations,\nwithout compromising the validity of downstream conclusions, remain limited. We\nintroduce Confidence-Driven Inference: a method that combines LLM annotations\nand LLM confidence indicators to strategically select which human annotations\nshould be collected, with the goal of producing accurate statistical estimates\nand provably valid confidence intervals while reducing the number of human\nannotations needed. Our approach comes with safeguards against LLM annotations\nof poor quality, guaranteeing that the conclusions will be both valid and no\nless accurate than if we only relied on human annotations. We demonstrate the\neffectiveness of Confidence-Driven Inference over baselines in statistical\nestimation tasks across three CSS settings--text politeness, stance, and\nbias--reducing the needed number of human annotations by over 25% in each.\nAlthough we use CSS settings for demonstration, Confidence-Driven Inference can\nbe used to estimate most standard quantities across a broad range of NLP\nproblems.",
        "updated": "2024-08-27 17:03:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15204v1"
    },
    {
        "title": "Infusing Acoustic Pause Context into Text-Based Dementia Assessment",
        "authors": "Franziska BraunSebastian P. BayerlFlorian HönigHartmut LehfeldThomas HillemacherTobias BockletKorbinian Riedhammer",
        "links": "http://arxiv.org/abs/2408.15188v1",
        "entry_id": "http://arxiv.org/abs/2408.15188v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15188v1",
        "summary": "Speech pauses, alongside content and structure, offer a valuable and\nnon-invasive biomarker for detecting dementia. This work investigates the use\nof pause-enriched transcripts in transformer-based language models to\ndifferentiate the cognitive states of subjects with no cognitive impairment,\nmild cognitive impairment, and Alzheimer's dementia based on their speech from\na clinical assessment. We address three binary classification tasks: Onset,\nmonitoring, and dementia exclusion. The performance is evaluated through\nexperiments on a German Verbal Fluency Test and a Picture Description Test,\ncomparing the model's effectiveness across different speech production\ncontexts. Starting from a textual baseline, we investigate the effect of\nincorporation of pause information and acoustic context. We show the test\nshould be chosen depending on the task, and similarly, lexical pause\ninformation and acoustic cross-attention contribute differently.",
        "updated": "2024-08-27 16:44:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15188v1"
    }
]