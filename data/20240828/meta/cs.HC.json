[
    {
        "title": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?",
        "authors": "Kristina GligorićTijana ZrnicCinoo LeeEmmanuel J. CandèsDan Jurafsky",
        "links": "http://arxiv.org/abs/2408.15204v1",
        "entry_id": "http://arxiv.org/abs/2408.15204v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15204v1",
        "summary": "Large language models (LLMs) have shown high agreement with human raters\nacross a variety of tasks, demonstrating potential to ease the challenges of\nhuman data collection. In computational social science (CSS), researchers are\nincreasingly leveraging LLM annotations to complement slow and expensive human\nannotations. Still, guidelines for collecting and using LLM annotations,\nwithout compromising the validity of downstream conclusions, remain limited. We\nintroduce Confidence-Driven Inference: a method that combines LLM annotations\nand LLM confidence indicators to strategically select which human annotations\nshould be collected, with the goal of producing accurate statistical estimates\nand provably valid confidence intervals while reducing the number of human\nannotations needed. Our approach comes with safeguards against LLM annotations\nof poor quality, guaranteeing that the conclusions will be both valid and no\nless accurate than if we only relied on human annotations. We demonstrate the\neffectiveness of Confidence-Driven Inference over baselines in statistical\nestimation tasks across three CSS settings--text politeness, stance, and\nbias--reducing the needed number of human annotations by over 25% in each.\nAlthough we use CSS settings for demonstration, Confidence-Driven Inference can\nbe used to estimate most standard quantities across a broad range of NLP\nproblems.",
        "updated": "2024-08-27 17:03:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15204v1"
    },
    {
        "title": "Crossing Rays: Evaluation of Bimanual Mid-air Selection Techniques in an Immersive Environment",
        "authors": "DongHoon KimDongyun HanSiyeon BakIsaac Cho",
        "links": "http://arxiv.org/abs/2408.15199v1",
        "entry_id": "http://arxiv.org/abs/2408.15199v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15199v1",
        "summary": "Mid-air navigation offers a method of aerial travel that mitigates the\nconstraints associated with continuous navigation. A mid-air selection\ntechnique is essential to enable such navigation. In this paper, we consider\nfour variations of intersection-based bimanual mid-air selection techniques\nwith visual aids and supporting features: Simple-Ray, Simple-Stripe,\nPrecision-Stripe, and Cursor-Sync. We evaluate their performance and user\nexperience compared to an unimanual mid-air selection technique using two tasks\nthat require selecting a mid-air position with or without a reference object.\nOur findings indicate that the bimanual techniques generally demonstrate faster\nselection times compared to the unimanual technique. With a supporting feature,\nthe bimanual techniques can provide a more accurate selection than the\nunimanual technique. Based on our results, we discuss the effect of selection\ntechnique's visual aids and supporting features on performance and user\nexperience for mid-air selection.",
        "updated": "2024-08-27 17:00:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15199v1"
    },
    {
        "title": "Regaining Trust: Impact of Transparent User Interface Design on Acceptance of Camera-Based In-Car Health Monitoring Systems",
        "authors": "Hauke SandhausMadiha Zahrah ChoksiWendy Ju",
        "links": "http://arxiv.org/abs/2408.15177v1",
        "entry_id": "http://arxiv.org/abs/2408.15177v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15177v1",
        "summary": "Introducing in-car health monitoring systems offers substantial potential to\nimprove driver safety. However, camera-based sensing technologies introduce\nsignificant privacy concerns. This study investigates the impact of transparent\nuser interface design on user acceptance of these systems. We conducted an\nonline study with 42 participants using prototypes varying in transparency,\nchoice, and deception levels. The prototypes included three onboarding designs:\n(1) a traditional Terms and Conditions text, (2) a Business Nudge design that\nsubtly encouraged users to accept default data-sharing options, and (3) a\nTransparent Walk-Through that provided clear, step-by-step explanations of data\nuse and privacy policies. Our findings indicate that transparent design\nsignificantly affects user experience measures, including perceived creepiness,\ntrust in data use, and trustworthiness of content. Transparent onboarding\nprocesses enhanced user experience and trust without significantly increasing\nonboarding time. These findings offer practical guidance for designing\nuser-friendly and privacy-respecting in-car health monitoring systems.",
        "updated": "2024-08-27 16:21:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15177v1"
    },
    {
        "title": "Interactive dense pixel visualizations for time series and model attribution explanations",
        "authors": "Udo SchlegelDaniel A. Keim",
        "links": "http://arxiv.org/abs/2408.15073v1",
        "entry_id": "http://arxiv.org/abs/2408.15073v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15073v1",
        "summary": "The field of Explainable Artificial Intelligence (XAI) for Deep Neural\nNetwork models has developed significantly, offering numerous techniques to\nextract explanations from models. However, evaluating explanations is often not\ntrivial, and differences in applied metrics can be subtle, especially with\nnon-intelligible data. Thus, there is a need for visualizations tailored to\nexplore explanations for domains with such data, e.g., time series. We propose\nDAVOTS, an interactive visual analytics approach to explore raw time series\ndata, activations of neural networks, and attributions in a dense-pixel\nvisualization to gain insights into the data, models' decisions, and\nexplanations. To further support users in exploring large datasets, we apply\nclustering approaches to the visualized data domains to highlight groups and\npresent ordering strategies for individual and combined data exploration to\nfacilitate finding patterns. We visualize a CNN trained on the FordA dataset to\ndemonstrate the approach.",
        "updated": "2024-08-27 14:02:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15073v1"
    },
    {
        "title": "Constraining Participation: Affordances of Feedback Features in Interfaces to Large Language Models",
        "authors": "Ned CooperAlexandra Zafiroglu",
        "links": "http://arxiv.org/abs/2408.15066v1",
        "entry_id": "http://arxiv.org/abs/2408.15066v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15066v1",
        "summary": "Large language models (LLMs) are now accessible to anyone with a computer, a\nweb browser, and an internet connection via browser-based interfaces, shifting\nthe dynamics of participation in AI development. This paper examines the\naffordances of interactive feedback features in ChatGPT's interface, analysing\nhow they shape user input and participation in LLM iteration. Drawing on a\nsurvey of ChatGPT users and applying the mechanisms and conditions framework of\naffordances, we demonstrate that these features encourage simple, frequent, and\nperformance-focused feedback while discouraging collective input and\ndiscussions among users. We argue that this feedback format significantly\nconstrains user participation, reinforcing power imbalances between users, the\npublic, and companies developing LLMs. Our analysis contributes to the growing\nbody of literature on participatory AI by critically examining the limitations\nof existing feedback processes and proposing directions for their redesign. To\nenable more meaningful public participation in AI development, we advocate for\na shift away from processes focused on aligning model outputs with specific\nuser preferences. Instead, we emphasise the need for processes that facilitate\ndialogue between companies and diverse 'publics' about the purpose and\napplications of LLMs. This approach requires attention to the ongoing work of\ninfrastructuring - creating and sustaining the social, technical, and\ninstitutional structures necessary to address matters of concern to groups\nimpacted by AI development and deployment.",
        "updated": "2024-08-27 13:50:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15066v1"
    }
]