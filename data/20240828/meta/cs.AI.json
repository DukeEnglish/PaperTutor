[
    {
        "title": "The Mamba in the Llama: Distilling and Accelerating Hybrid Models",
        "authors": "Junxiong WangDaniele PaliottaAvner MayAlexander M. RushTri Dao",
        "links": "http://arxiv.org/abs/2408.15237v1",
        "entry_id": "http://arxiv.org/abs/2408.15237v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15237v1",
        "summary": "Linear RNN architectures, like Mamba, can be competitive with Transformer\nmodels in language modeling while having advantageous deployment\ncharacteristics. Given the focus on training large-scale Transformer models, we\nconsider the challenge of converting these pretrained models for deployment. We\ndemonstrate that it is feasible to distill large Transformers into linear RNNs\nby reusing the linear projection weights from attention layers with academic\nGPU resources. The resulting hybrid model, which incorporates a quarter of the\nattention layers, achieves performance comparable to the original Transformer\nin chat benchmarks and outperforms open-source hybrid Mamba models trained from\nscratch with trillions of tokens in both chat benchmarks and general\nbenchmarks. Moreover, we introduce a hardware-aware speculative decoding\nalgorithm that accelerates the inference speed of Mamba and hybrid models.\nOverall we show how, with limited computation resources, we can remove many of\nthe original attention layers and generate from the resulting model more\nefficiently. Our top-performing model, distilled from Llama3-8B-Instruct,\nachieves a 29.61 length-controlled win rate on AlpacaEval 2 against GPT-4 and\n7.35 on MT-Bench, surpassing the best instruction-tuned linear RNN model.",
        "updated": "2024-08-27 17:56:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15237v1"
    },
    {
        "title": "Into the Unknown Unknowns: Engaged Human Learning through Participation in Language Model Agent Conversations",
        "authors": "Yucheng JiangYijia ShaoDekun MaSina J. SemnaniMonica S. Lam",
        "links": "http://arxiv.org/abs/2408.15232v1",
        "entry_id": "http://arxiv.org/abs/2408.15232v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15232v1",
        "summary": "While language model (LM)-powered chatbots and generative search engines\nexcel at answering concrete queries, discovering information in the terrain of\nunknown unknowns remains challenging for users. To emulate the common\neducational scenario where children/students learn by listening to and\nparticipating in conversations of their parents/teachers, we create\nCollaborative STORM (Co-STORM). Unlike QA systems that require users to ask all\nthe questions, Co-STORM lets users observe and occasionally steer the discourse\namong several LM agents. The agents ask questions on the user's behalf,\nallowing the user to discover unknown unknowns serendipitously. To facilitate\nuser interaction, Co-STORM assists users in tracking the discourse by\norganizing the uncovered information into a dynamic mind map, ultimately\ngenerating a comprehensive report as takeaways. For automatic evaluation, we\nconstruct the WildSeek dataset by collecting real information-seeking records\nwith user goals. Co-STORM outperforms baseline methods on both discourse trace\nand report quality. In a further human evaluation, 70% of participants prefer\nCo-STORM over a search engine, and 78% favor it over a RAG chatbot.",
        "updated": "2024-08-27 17:50:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15232v1"
    },
    {
        "title": "Fundus2Video: Cross-Modal Angiography Video Generation from Static Fundus Photography with Clinical Knowledge Guidance",
        "authors": "Weiyi ZhangSiyu HuangJiancheng YangRuoyu ChenZongyuan GeYingfeng ZhengDanli ShiMingguang He",
        "links": "http://arxiv.org/abs/2408.15217v1",
        "entry_id": "http://arxiv.org/abs/2408.15217v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15217v1",
        "summary": "Fundus Fluorescein Angiography (FFA) is a critical tool for assessing retinal\nvascular dynamics and aiding in the diagnosis of eye diseases. However, its\ninvasive nature and less accessibility compared to Color Fundus (CF) images\npose significant challenges. Current CF to FFA translation methods are limited\nto static generation. In this work, we pioneer dynamic FFA video generation\nfrom static CF images. We introduce an autoregressive GAN for smooth,\nmemory-saving frame-by-frame FFA synthesis. To enhance the focus on dynamic\nlesion changes in FFA regions, we design a knowledge mask based on clinical\nexperience. Leveraging this mask, our approach integrates innovative knowledge\nmask-guided techniques, including knowledge-boosted attention, knowledge-aware\ndiscriminators, and mask-enhanced patchNCE loss, aimed at refining generation\nin critical areas and addressing the pixel misalignment challenge. Our method\nachieves the best FVD of 1503.21 and PSNR of 11.81 compared to other common\nvideo generation approaches. Human assessment by an ophthalmologist confirms\nits high generation quality. Notably, our knowledge mask surpasses supervised\nlesion segmentation masks, offering a promising non-invasive alternative to\ntraditional FFA for research and clinical applications. The code is available\nat https://github.com/Michi-3000/Fundus2Video.",
        "updated": "2024-08-27 17:30:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15217v1"
    },
    {
        "title": "Can Unconfident LLM Annotations Be Used for Confident Conclusions?",
        "authors": "Kristina GligorićTijana ZrnicCinoo LeeEmmanuel J. CandèsDan Jurafsky",
        "links": "http://arxiv.org/abs/2408.15204v1",
        "entry_id": "http://arxiv.org/abs/2408.15204v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15204v1",
        "summary": "Large language models (LLMs) have shown high agreement with human raters\nacross a variety of tasks, demonstrating potential to ease the challenges of\nhuman data collection. In computational social science (CSS), researchers are\nincreasingly leveraging LLM annotations to complement slow and expensive human\nannotations. Still, guidelines for collecting and using LLM annotations,\nwithout compromising the validity of downstream conclusions, remain limited. We\nintroduce Confidence-Driven Inference: a method that combines LLM annotations\nand LLM confidence indicators to strategically select which human annotations\nshould be collected, with the goal of producing accurate statistical estimates\nand provably valid confidence intervals while reducing the number of human\nannotations needed. Our approach comes with safeguards against LLM annotations\nof poor quality, guaranteeing that the conclusions will be both valid and no\nless accurate than if we only relied on human annotations. We demonstrate the\neffectiveness of Confidence-Driven Inference over baselines in statistical\nestimation tasks across three CSS settings--text politeness, stance, and\nbias--reducing the needed number of human annotations by over 25% in each.\nAlthough we use CSS settings for demonstration, Confidence-Driven Inference can\nbe used to estimate most standard quantities across a broad range of NLP\nproblems.",
        "updated": "2024-08-27 17:03:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15204v1"
    },
    {
        "title": "Automatic 8-tissue Segmentation for 6-month Infant Brains",
        "authors": "Yilan DongVanessa KyriakopoulouIrina GrigorescuGrainne McAlonanDafnis BatalleMaria Deprez",
        "links": "http://arxiv.org/abs/2408.15198v1",
        "entry_id": "http://arxiv.org/abs/2408.15198v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15198v1",
        "summary": "Numerous studies have highlighted that atypical brain development,\nparticularly during infancy and toddlerhood, is linked to an increased\nlikelihood of being diagnosed with a neurodevelopmental condition, such as\nautism. Accurate brain tissue segmentations for morphological analysis are\nessential in numerous infant studies. However, due to ongoing white matter (WM)\nmyelination changing tissue contrast in T1- and T2-weighted images, automatic\ntissue segmentation in 6-month infants is particularly difficult. On the other\nhand, manual labelling by experts is time-consuming and labor-intensive. In\nthis study, we propose the first 8-tissue segmentation pipeline for\nsix-month-old infant brains. This pipeline utilizes domain adaptation (DA)\ntechniques to leverage our longitudinal data, including neonatal images\nsegmented with the neonatal Developing Human Connectome Project structural\npipeline. Our pipeline takes raw 6-month images as inputs and generates the\n8-tissue segmentation as outputs, forming an end-to-end segmentation pipeline.\nThe segmented tissues include WM, gray matter (GM), cerebrospinal fluid (CSF),\nventricles, cerebellum, basal ganglia, brainstem, and hippocampus/amygdala.\nCycle-Consistent Generative Adversarial Network (CycleGAN) and Attention U-Net\nwere employed to achieve the image contrast transformation between neonatal and\n6-month images and perform tissue segmentation on the synthesized 6-month\nimages (neonatal images with 6-month intensity contrast), respectively.\nMoreover, we incorporated the segmentation outputs from Infant Brain Extraction\nand Analysis Toolbox (iBEAT) and another Attention U-Net to further enhance the\nperformance and construct the end-to-end segmentation pipeline. Our evaluation\nwith real 6-month images achieved a DICE score of 0.92, an HD95 of 1.6, and an\nASSD of 0.42.",
        "updated": "2024-08-27 16:58:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15198v1"
    }
]