[
    {
        "title": "Exploiting Approximate Symmetry for Efficient Multi-Agent Reinforcement Learning",
        "authors": "Batuhan YardimNiao He",
        "links": "http://arxiv.org/abs/2408.15173v1",
        "entry_id": "http://arxiv.org/abs/2408.15173v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15173v1",
        "summary": "Mean-field games (MFG) have become significant tools for solving large-scale\nmulti-agent reinforcement learning problems under symmetry. However, the\nassumption of exact symmetry limits the applicability of MFGs, as real-world\nscenarios often feature inherent heterogeneity. Furthermore, most works on MFG\nassume access to a known MFG model, which might not be readily available for\nreal-world finite-agent games. In this work, we broaden the applicability of\nMFGs by providing a methodology to extend any finite-player, possibly\nasymmetric, game to an \"induced MFG\". First, we prove that $N$-player dynamic\ngames can be symmetrized and smoothly extended to the infinite-player continuum\nvia explicit Kirszbraun extensions. Next, we propose the notion of\n$\\alpha,\\beta$-symmetric games, a new class of dynamic population games that\nincorporate approximate permutation invariance. For $\\alpha,\\beta$-symmetric\ngames, we establish explicit approximation bounds, demonstrating that a Nash\npolicy of the induced MFG is an approximate Nash of the $N$-player dynamic\ngame. We show that TD learning converges up to a small bias using trajectories\nof the $N$-player game with finite-sample guarantees, permitting symmetrized\nlearning without building an explicit MFG model. Finally, for certain games\nsatisfying monotonicity, we prove a sample complexity of\n$\\widetilde{\\mathcal{O}}(\\varepsilon^{-6})$ for the $N$-agent game to learn an\n$\\varepsilon$-Nash up to symmetrization bias. Our theory is supported by\nevaluations on MARL benchmarks with thousands of agents.",
        "updated": "2024-08-27 16:11:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15173v1"
    },
    {
        "title": "Low-Budget Simulation-Based Inference with Bayesian Neural Networks",
        "authors": "Arnaud DelaunoyMaxence de la Brassinne BonardeauxSiddharth Mishra-SharmaGilles Louppe",
        "links": "http://arxiv.org/abs/2408.15136v1",
        "entry_id": "http://arxiv.org/abs/2408.15136v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15136v1",
        "summary": "Simulation-based inference methods have been shown to be inaccurate in the\ndata-poor regime, when training simulations are limited or expensive. Under\nthese circumstances, the inference network is particularly prone to\noverfitting, and using it without accounting for the computational uncertainty\narising from the lack of identifiability of the network weights can lead to\nunreliable results. To address this issue, we propose using Bayesian neural\nnetworks in low-budget simulation-based inference, thereby explicitly\naccounting for the computational uncertainty of the posterior approximation. We\ndesign a family of Bayesian neural network priors that are tailored for\ninference and show that they lead to well-calibrated posteriors on tested\nbenchmarks, even when as few as $O(10)$ simulations are available. This opens\nup the possibility of performing reliable simulation-based inference using very\nexpensive simulators, as we demonstrate on a problem from the field of\ncosmology where single simulations are computationally expensive. We show that\nBayesian neural networks produce informative and well-calibrated posterior\nestimates with only a few hundred simulations.",
        "updated": "2024-08-27 15:19:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15136v1"
    },
    {
        "title": "The Benefits of Balance: From Information Projections to Variance Reduction",
        "authors": "Lang LiuRonak MehtaSoumik PalZaid Harchaoui",
        "links": "http://arxiv.org/abs/2408.15065v1",
        "entry_id": "http://arxiv.org/abs/2408.15065v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15065v1",
        "summary": "Data balancing across multiple modalities/sources appears in various forms in\nseveral foundation models (e.g., CLIP and DINO) achieving universal\nrepresentation learning. We show that this iterative algorithm, usually used to\navoid representation collapse, enjoys an unsuspected benefit: reducing the\nvariance of estimators that are functionals of the empirical distribution over\nthese sources. We provide non-asymptotic bounds quantifying this variance\nreduction effect and relate them to the eigendecays of appropriately defined\nMarkov operators. We explain how various forms of data balancing in contrastive\nmultimodal learning and self-supervised clustering can be interpreted as\ninstances of this variance reduction scheme.",
        "updated": "2024-08-27 13:48:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15065v1"
    },
    {
        "title": "Data-driven Effective Modeling of Multiscale Stochastic Dynamical Systems",
        "authors": "Yuan ChenDongbin Xiu",
        "links": "http://arxiv.org/abs/2408.14821v1",
        "entry_id": "http://arxiv.org/abs/2408.14821v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14821v1",
        "summary": "We present a numerical method for learning the dynamics of slow components of\nunknown multiscale stochastic dynamical systems. While the governing equations\nof the systems are unknown, bursts of observation data of the slow variables\nare available. By utilizing the observation data, our proposed method is\ncapable of constructing a generative stochastic model that can accurately\ncapture the effective dynamics of the slow variables in distribution. We\npresent a comprehensive set of numerical examples to demonstrate the\nperformance of the proposed method.",
        "updated": "2024-08-27 07:03:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14821v1"
    },
    {
        "title": "General targeted machine learning for modern causal mediation analysis",
        "authors": "Richard LiuNicholas T. WilliamsKara E. RudolphIván Díaz",
        "links": "http://arxiv.org/abs/2408.14620v1",
        "entry_id": "http://arxiv.org/abs/2408.14620v1",
        "pdf_url": "http://arxiv.org/pdf/2408.14620v1",
        "summary": "Causal mediation analyses investigate the mechanisms through which causes\nexert their effects, and are therefore central to scientific progress. The\nliterature on the non-parametric definition and identification of mediational\neffects in rigourous causal models has grown significantly in recent years, and\nthere has been important progress to address challenges in the interpretation\nand identification of such effects. Despite great progress in the causal\ninference front, statistical methodology for non-parametric estimation has\nlagged behind, with few or no methods available for tackling non-parametric\nestimation in the presence of multiple, continuous, or high-dimensional\nmediators. In this paper we show that the identification formulas for six\npopular non-parametric approaches to mediation analysis proposed in recent\nyears can be recovered from just two statistical estimands. We leverage this\nfinding to propose an all-purpose one-step estimation algorithm that can be\ncoupled with machine learning in any mediation study that uses any of these six\ndefinitions of mediation. The estimators have desirable properties, such as\n$\\sqrt{n}$-convergence and asymptotic normality. Estimating the first-order\ncorrection for the one-step estimator requires estimation of complex density\nratios on the potentially high-dimensional mediators, a challenge that is\nsolved using recent advancements in so-called Riesz learning. We illustrate the\nproperties of our methods in a simulation study and illustrate its use on real\ndata to estimate the extent to which pain management practices mediate the\ntotal effect of having a chronic pain disorder on opioid use disorder.",
        "updated": "2024-08-26 20:31:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.14620v1"
    }
]