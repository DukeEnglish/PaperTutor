[
    {
        "title": "Drone-assisted Road Gaussian Splatting with Cross-view Uncertainty",
        "authors": "Saining ZhangBaijun YeXiaoxue ChenYuantao ChenZongzheng ZhangCheng PengYongliang ShiHao Zhao",
        "links": "http://arxiv.org/abs/2408.15242v1",
        "entry_id": "http://arxiv.org/abs/2408.15242v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15242v1",
        "summary": "Robust and realistic rendering for large-scale road scenes is essential in\nautonomous driving simulation. Recently, 3D Gaussian Splatting (3D-GS) has made\ngroundbreaking progress in neural rendering, but the general fidelity of\nlarge-scale road scene renderings is often limited by the input imagery, which\nusually has a narrow field of view and focuses mainly on the street-level local\narea. Intuitively, the data from the drone's perspective can provide a\ncomplementary viewpoint for the data from the ground vehicle's perspective,\nenhancing the completeness of scene reconstruction and rendering. However,\ntraining naively with aerial and ground images, which exhibit large view\ndisparity, poses a significant convergence challenge for 3D-GS, and does not\ndemonstrate remarkable improvements in performance on road views. In order to\nenhance the novel view synthesis of road views and to effectively use the\naerial information, we design an uncertainty-aware training method that allows\naerial images to assist in the synthesis of areas where ground images have poor\nlearning outcomes instead of weighting all pixels equally in 3D-GS training\nlike prior work did. We are the first to introduce the cross-view uncertainty\nto 3D-GS by matching the car-view ensemble-based rendering uncertainty to\naerial images, weighting the contribution of each pixel to the training\nprocess. Additionally, to systematically quantify evaluation metrics, we\nassemble a high-quality synthesized dataset comprising both aerial and ground\nimages for road scenes.",
        "updated": "2024-08-27 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15242v1"
    },
    {
        "title": "GenRec: Unifying Video Generation and Recognition with Diffusion Models",
        "authors": "Zejia WengXitong YangZhen XingZuxuan WuYu-Gang Jiang",
        "links": "http://arxiv.org/abs/2408.15241v1",
        "entry_id": "http://arxiv.org/abs/2408.15241v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15241v1",
        "summary": "Video diffusion models are able to generate high-quality videos by learning\nstrong spatial-temporal priors on large-scale datasets. In this paper, we aim\nto investigate whether such priors derived from a generative process are\nsuitable for video recognition, and eventually joint optimization of generation\nand recognition. Building upon Stable Video Diffusion, we introduce GenRec, the\nfirst unified framework trained with a random-frame conditioning process so as\nto learn generalized spatial-temporal representations. The resulting framework\ncan naturally supports generation and recognition, and more importantly is\nrobust even when visual inputs contain limited information. Extensive\nexperiments demonstrate the efficacy of GenRec for both recognition and\ngeneration. In particular, GenRec achieves competitive recognition performance,\noffering 75.8% and 87.2% accuracy on SSV2 and K400, respectively. GenRec also\nperforms the best class-conditioned image-to-video generation results,\nachieving 46.5 and 49.3 FVD scores on SSV2 and EK-100 datasets. Furthermore,\nGenRec demonstrates extraordinary robustness in scenarios that only limited\nframes can be observed.",
        "updated": "2024-08-27 17:59:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15241v1"
    },
    {
        "title": "Generative Inbetweening: Adapting Image-to-Video Models for Keyframe Interpolation",
        "authors": "Xiaojuan WangBoyang ZhouBrian CurlessIra Kemelmacher-ShlizermanAleksander HolynskiSteven M. Seitz",
        "links": "http://arxiv.org/abs/2408.15239v1",
        "entry_id": "http://arxiv.org/abs/2408.15239v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15239v1",
        "summary": "We present a method for generating video sequences with coherent motion\nbetween a pair of input key frames. We adapt a pretrained large-scale\nimage-to-video diffusion model (originally trained to generate videos moving\nforward in time from a single input image) for key frame interpolation, i.e.,\nto produce a video in between two input frames. We accomplish this adaptation\nthrough a lightweight fine-tuning technique that produces a version of the\nmodel that instead predicts videos moving backwards in time from a single input\nimage. This model (along with the original forward-moving model) is\nsubsequently used in a dual-directional diffusion sampling process that\ncombines the overlapping model estimates starting from each of the two\nkeyframes. Our experiments show that our method outperforms both existing\ndiffusion-based methods and traditional frame interpolation techniques.",
        "updated": "2024-08-27 17:57:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15239v1"
    },
    {
        "title": "Learning-based Multi-View Stereo: A Survey",
        "authors": "Fangjinhua WangQingtian ZhuDi ChangQuankai GaoJunlin HanTong ZhangRichard HartleyMarc Pollefeys",
        "links": "http://arxiv.org/abs/2408.15235v1",
        "entry_id": "http://arxiv.org/abs/2408.15235v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15235v1",
        "summary": "3D reconstruction aims to recover the dense 3D structure of a scene. It plays\nan essential role in various applications such as Augmented/Virtual Reality\n(AR/VR), autonomous driving and robotics. Leveraging multiple views of a scene\ncaptured from different viewpoints, Multi-View Stereo (MVS) algorithms\nsynthesize a comprehensive 3D representation, enabling precise reconstruction\nin complex environments. Due to its efficiency and effectiveness, MVS has\nbecome a pivotal method for image-based 3D reconstruction. Recently, with the\nsuccess of deep learning, many learning-based MVS methods have been proposed,\nachieving impressive performance against traditional methods. We categorize\nthese learning-based methods as: depth map-based, voxel-based, NeRF-based, 3D\nGaussian Splatting-based, and large feed-forward methods. Among these, we focus\nsignificantly on depth map-based methods, which are the main family of MVS due\nto their conciseness, flexibility and scalability. In this survey, we provide a\ncomprehensive review of the literature at the time of this writing. We\ninvestigate these learning-based methods, summarize their performances on\npopular benchmarks, and discuss promising future research directions in this\narea.",
        "updated": "2024-08-27 17:53:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15235v1"
    },
    {
        "title": "DCT-CryptoNets: Scaling Private Inference in the Frequency Domain",
        "authors": "Arjun RoyKaushik Roy",
        "links": "http://arxiv.org/abs/2408.15231v1",
        "entry_id": "http://arxiv.org/abs/2408.15231v1",
        "pdf_url": "http://arxiv.org/pdf/2408.15231v1",
        "summary": "The convergence of fully homomorphic encryption (FHE) and machine learning\noffers unprecedented opportunities for private inference of sensitive data. FHE\nenables computation directly on encrypted data, safeguarding the entire machine\nlearning pipeline, including data and model confidentiality. However, existing\nFHE-based implementations for deep neural networks face significant challenges\nin computational cost, latency, and scalability, limiting their practical\ndeployment. This paper introduces DCT-CryptoNets, a novel approach that\nleverages frequency-domain learning to tackle these issues. Our method operates\ndirectly in the frequency domain, utilizing the discrete cosine transform (DCT)\ncommonly employed in JPEG compression. This approach is inherently compatible\nwith remote computing services, where images are usually transmitted and stored\nin compressed formats. DCT-CryptoNets reduces the computational burden of\nhomomorphic operations by focusing on perceptually relevant low-frequency\ncomponents. This is demonstrated by substantial latency reduction of up to\n5.3$\\times$ compared to prior work on image classification tasks, including a\nnovel demonstration of ImageNet inference within 2.5 hours, down from 12.5\nhours compared to prior work on equivalent compute resources. Moreover,\nDCT-CryptoNets improves the reliability of encrypted accuracy by reducing\nvariability (e.g., from $\\pm$2.5\\% to $\\pm$1.0\\% on ImageNet). This study\ndemonstrates a promising avenue for achieving efficient and practical\nprivacy-preserving deep learning on high resolution images seen in real-world\napplications.",
        "updated": "2024-08-27 17:48:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.15231v1"
    }
]