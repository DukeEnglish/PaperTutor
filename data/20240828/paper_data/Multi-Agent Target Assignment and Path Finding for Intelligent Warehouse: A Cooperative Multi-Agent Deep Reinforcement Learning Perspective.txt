Multi-Agent Target Assignment and Path Finding for Intelligent Warehouse: A
Cooperative Multi-Agent Deep Reinforcement Learning Perspective
QiLiu,JianqiGao,DongjieZhu,XizhengPang,PengbinChen,JingxiangGuo,YanjieLi
DepartmentofControlScienceandEngineering,HarbinInstituteofTechnology(Shenzhen),Shenzhen518055,China.
Abstract
Multi-agent target assignment and path planning (TAPF) are two key problems in intelligent warehouse. However,
mostliteratureonlyaddressesoneofthesetwoproblemsseparately. Inthisstudy,weproposeamethodtosimulta-
neouslysolvetargetassignmentandpathplanningfromaperspectiveofcooperativemulti-agentdeepreinforcement
learning (RL). To the best of our knowledge, this is the first work to model the TAPF problem for intelligent ware-
house to cooperative multi-agent deep RL, and the first to simultaneously address TAPF based on multi-agent deep
RL.Furthermore,previousliteraturerarelyconsidersthephysicaldynamicsofagents. Inthisstudy,thephysicaldy-
namicsoftheagentsisconsidered. Experimentalresultsshowthatourmethodperformswellinvarioustasksettings,
whichmeansthatthetargetassignmentissolvedreasonablywellandtheplannedpathisalmostshortest. Moreover,
ourmethodismoretime-efficientthanbaselines.
Keywords:
Intelligentwarehouse,Taskassignment,Pathfinding,Multi-agentreinforcementlearning
1. Introduction chutes in the center of warehouse. The approach pro-
posedinthisstudyisbasedonthelogisticssortingcen-
Withtherapiddevelopmentoflogisticsrelatedindus- ter.
tries, new opportunities and challenges have been pro-
Taskassignmentandpathfinding(TAPF)aretwoim-
posed for intelligent warehousing [1, 2]. Traditional
portant processes in intelligent warehousing. The sys-
warehouse technologies use conveyor belts and other
tem first assigns a specified task to each agent accord-
equipmenttocompletematerialhandling. Theyarein-
ingtotheorderrequirements. Thentheagenttransports
flexibleandnoteasilyextensible. Intelligentwarehous-
thegoodsfromtheorigintothedestinationandensures
ingutilizesamulti-agentsystemtodelivergoodstodes-
that the path does not conflict with other agents. The
ignated locations, greatly improving warehousing effi-
taskassignmentandpathplanningproblemistypically
ciency. Intelligentwarehousesystemisdividedmainly
NP-hard[5], whichhasalargesearchspace. Thus, di-
intotheorderfulfillmentsystem,suchasthekivasystem
rectly solving this problem is difficult. In general, two
[3] and the logistics sorting center [4]. In order fulfill-
stepsarerequiredtosolvetheTAPFproblem. Thefirst
mentsystems,mobileagentsmoveinventorypodstoin-
stepismulti-agenttaskassignment(MATA),assigning
ventorystations. Then,workerstakegoodsfrominven-
tasks to agents without considering possible path con-
tory pods, and agents move the inventory pods to their
flicts between agents. The second step is path plan-
original locations. In logistics sorting centers, agents
ning for all agents by using multi-agent path finding
take goods from loading stations and deliver them to
(MAPF)algorithms. Numerousstudiesintheliterature
study MATA and MAPF; we describe these methods
∗Correspondingauthor:YanjieLi,autolyj@hit.edu.cn in detail in the next section. Although solving TAPF
Emailaddress:19B953036@stu.hit.edu.cn, separately reduces the difficulty of total problem, this
autolyj@hit.edu.cn, gaojianqi205a@stu.hit.edu.cn, type of method ignores the mutual influence between
190320304@stu.hit.edu.cn,20S153158@stu.hit.edu.cn, taskassignmentandpathplanning. Reasonabletaskas-
845924343@qq.com, 210320826@stu.hit.edu.cn(QiLiu,
signmentcannotonlyeffectivelyreducethepathlength
JianqiGao,DongjieZhu,XizhengPang,PengbinChen,Jingxiang
Guo,YanjieLi) of warehouse agents and improve the operation effi-
PreprintsubmittedtoAppliedIntelligence August27,2024
4202
guA
52
]IA.sc[
1v05731.8042:viXraciency,butalsohelpavoidpathconflictsbetweendiffer- algorithm [12], genetic algorithm [10], etc.. In the
entagents. Therefore,itisnecessarytosolvetheTAPF distributed assignment class, each agent in the ware-
problems together. In TAPF problem, we assume that house plans its own task sequence according to tasks
each idle agent and inventory pods are homogeneous. andenvironmentalinformation[13]. Thisclassmethod
Thus,wecanassignanytasktoanyagent. effectively reduces the load of the central control sys-
Recently, deep reinforcement learning (RL) has re- tem and is more flexible and adaptable, but it may not
ceivedgreatattentionduetoitsremarkablesuccessesin findtheglobaloptimalsolution[14]. Furthermore, the
widelychallengingdomains,suchasroboticcontrol[6], distributedassignmentclassmainlyincludes: learning-
multi-agentproblems[7], andvideogames[8]. Inthis basedmethods[15]andmarketauctionmethods[16].
study,weintroducecooperativemulti-agentdeepRLto
solve the TAPF problem to simultaneously address the 2.2. Multi-AgentPathFinding
TAPFproblem. Inrecentyears,MAPFhasbecomeahotresearchdi-
Themaincontributionsofthisstudycanbesumma- rection in the fields of computer science and robotics.
rizedasfollows: ClassicalMAPFalgorithmscanbedividedintooptimal
andsub-optimaltypesaccordingtowhetherthesolution
• To our best knowledge, this is the first work to
resultsmeettheoptimality. TheoptimaltypicalMAPF
modelTAPFproblemforintelligentwarehouseto
algorithmsincludeA*-based[17],conflict-basedsearch
cooperativemulti-agentdeepRL,andthefirsttosi-
[18], increasing cost tree search based [19], and
multaneously address TAPF based on multi-agent
compilation-based[20]methods. Sub-optimalclassical
deepRL.
MAPFalgorithmsincludesearch-based[21],rule-based
• In this study, the physical dynamics of the agents [22], compilation-based [23], bounded conflict-based
search[24],andgeneticalgorithm-based[25]methods.
isconsideredinthepathplanningphase.
However, traditionalMAPFalgorithmshavepoorreal-
• Experimental results show that our method per- timeperformance. Therefore,numerousresearchersare
formswellinvariousoftasksettings,whichmeans beginning to study MAPF based on deep RL [26] to
thetargetassignmentissolvedreasonablyandthe solve this problem. PRIMAL [27] is a typical MAPF
planned path is almost shortest. Furthermore, our algorithm based on deep RL. However, PRIMAL still
methodismoretime-efficientthanthebaselines. solvesthemulti-agentproblembysingle-agentdeepRL
method[27]. Inthisstudy, weproposeamethodtosi-
The rest of this study is organized as follows. Sec-
multaneously address target assignment and path plan-
tion2discussesrelatedworkofTAPFinanintelligent
ningfromaperspectiveofcooperativemulti-agentdeep
warehouse.Section3introducesthepreliminariesofthe
RL. Furthermore, in the simulation environment, the
Markov Decision Process, RL, the cooperative multi-
above-mentionedvariousMAPFalgorithmsrarelycon-
agent deep RL and the definition of MAPF and TAPF
siderthephysicaldynamicsofagents. Inthisstudy,the
problem. Section 4 detailedly describes our method.
physicaldynamicsfactorsoftheagentareconsidered.
Section 5 provides experimental results to verify the
improved efficiency of our method. Section 6 presents
2.3. Cooperative Multi-Agent Deep Reinforcement
conclusionsandfuturework.
Learning
Cooperativemulti-agentRLdealswithsystemscon-
2. RelatedWork sistingofseveralagentswhichinteractinacommonen-
vironment. Cooperativemulti-agentRLaimstolearna
2.1. Multi-AgentTaskAssignment policy that can make all agents work together to com-
Throughmulti-agenttaskassignmentalgorithms,we pleteacommongoal.Multi-agentdeepRLhasreceived
can maximize the utilization of warehouse resources great attention due to its capability in allowing agents
[9]. Atpresent,multi-agenttaskassignmentalgorithms tolearntomakedecisionsinmulti-agentenvironments
inintelligentwarehousecanbeclassedintocentralized through interactions. Independent Q-learning (IQL)
and distributed classes according to the management [28] solves the multi-agent RL problem by decompos-
mode.Inthecentralizedtaskassignmentclass,acentral ing it into a collection of simultaneous single-agent
control system is set up, which is responsible for task RL problems that share the same environment. How-
assignment,assigningtasksassignedtoagentsforexe- ever,IQLcannotsolvethenonstationarity[29]problem
cution[10]. Classicalcentralizedtaskassignmentalgo- causedbythechangingpoliciesofotheragents. Value-
rithms include: Hungarian algorithm [11], tabu search DecompositionNetworks(VDN)[30]proposestolearn
2to linearly decompose a team value function into a se- 3.2. CooperativeMulti-AgentDeepRL
riesofper-agentvaluefunctions. Afterthat,QMIX[7]
Afullycooperativemulti-agentproblemcanbemod-
utilizes a mixing network architecture to approximate
eled as a Markov Decision Process [26], described by
allagents’jointstate-actionvalue. Themixingnetwork a tuple G = (S,A,P,r,O,N,γ,T). s ∈ S represents
non-linearly combines each agent’s state-action value, thetruestate. Ateachtimestept, eachagentn ∈ N :=
which is only a condition on each agent’s local obser-
{1,...,N} selects an action an ∈ A, then the joint ac-
vations. Furthermore,QMIXenforcesthejointstateac- t (cid:16) (cid:17)
tion a = {a1,a2,...,aN} is obtained. P s′ | s,a :
tionvaluemonotonicwitheachagentstate-actionvalue t t t t t+1 t t
S×AN ×S → [0,1]denotesthestatetransitionfunc-
by employing a network structural monotonicity con-
tion.r(s,a ):S×AN →Rdenotestherewardfunction
strainttothemixingnetwork. t t
shared by all agents, γ ∈ [0,1) represents the discount
However,theseabovealgorithmscannothandlecon-
factor, and T denotes the time horizon. Each agent n
tinuousactionspaceproblems. Thedeepdeterministic
hasan independentobservation o ∈ O anda historyof
policy gradient (DDPG) [6] is a representative method
actionobservationτn ∈T ≡(O×A). Astochasticpol-
inthecontinuousactiontaskforasingleagent. Multi-
icyπn(an |τn) : T ×A → [0,1]isbasedonthehistory
agent deep deterministic policy gradient (MADDPG)
of action-observation. Let πn denote agent n’ policy;
[31]extendedDDPGfromsingleagentsettingtomulti-
cooperativeagentsaimtomaximize:
agent setting by using centralised training and decen-
tralised execution paradigm [32]. MADDPG is a rep-  
resentativealgorithmformulti-agentcontinuousaction J(π)=E
s0:T−1,a1∼π1,...,aN∼πN(cid:88)T−1 γtr(cid:16)
s t,a1 t,...,a
tN(cid:17)
(1)
problems. Thus, in this study, we model the TAPF in t=0
intelligentwarehouseasacooperativemulti-agentdeep
RLproblemandintroduceMADDPGtosimultaneously Incooperativemulti-agentdeepRL,thetrainingpro-
addressthisproblem. cess is centralized, but the execution process is decen-
tralized [32]. This means that, in the learning process,
each agent has access to the global state s. However,
each agent’s decision only depends on its own action-
3. Preliminaries
observationhistoryτnintheexecutionprocess.
ThissectionprovidesasummaryofMarkovDecision 3.3. DefinitionofMAPFandTAPFproblem
Process, RL,cooperativemulti-agentdeepRL,andthe
MAPFProblem:
definitionofMAPFandTAPFproblem.
In the Multi-Agent Path Finding (MAPF) problem,
we consider a finite-horizon Markov Decision Process
definedbyatuple(S,A,P,r,γ,T):
3.1. MarkovDecisionProcessandRL
• S: Statespace.
This study considers a finite-horizon Markov Deci-
sion Process [26], defined by a tuple (S,A,P,r,γ,T). • A: Finiteactionspace.
S denotes the state space, A represents the finite ac-
tion space, P : S×A×S → [0,1] denotes the state • P(s t+1|s t,a t): Statetransitiondistribution.
transition distribution, r : S×A → R denotes the re-
• r(s,a): Rewardfunction.
ward function, γ ∈ [0,1) denotes the discount factor t t
and T is a time horizon. At each time step t, an ac- • γ: Discountfactor.
tion a ∈ A is chosen from a policy π. After transit-
t
ingintothenextstatebysamplingfromP(s t+1 | s t,a t), • T: Timehorizon.
the agent obtains a immediate reward r(s,a). The
t t
agentcontinuesperformingactionsuntilitentersater- Each agent n corresponds to an RL agent. At each
minal state or t reaches the time horizon T. RL aims time step t, agent n selects an action an ∈ A based on
t
to learn the policy π : S × A → [0,1] for decision- its policy πn. The state s represents the current po-
t
makingproblemsbymaximizingdiscountedcumulative sitions of all agents. The action an can involve mov-
rewards E [r ] = E
(cid:104)(cid:80)T−1γtr(s,a)(cid:105)
. In this
ingtoanadjacentpositionorstayint
gatthecurrentpo-
π,P 0:T−1 π,P 0 t t
study,wedonothaveaccesstotheenvironmentdynam- sition. The objective is to find a policy πn for each
icsP,whichmeansmodel-freedeepRLlearning. agent that maximizes the expected cumulative rewards
3(a) fiveagents-fivetasks (b) fiveagents-twentytasks
Figure1:ModelingTAPFasaMARLproblem
E [r ],wherer isthesumoftherewardsover 4. Target Assignment and Path Finding for Intel-
π,P 0:T−1 0:T−1
time. ligent Warehouse: A Cooperative Multi-Agent
ConflictsinMAPFtypicallyinvolvecollisionsinthe DeepRLPerspective
selection of actions between different agents, resulting
inconflictsofpositionsandmovements. InSection4.1,wedetailedlydescribeourmethodthat
TAPFProblem: modelstheTAPFproblemasacooperativemulti-agent
The Target Assignment for Multi-Agent Path Find- RL problem. In Section 4.2, we introduce the MAD-
ing (TAPF) problem is defined within the frame- DPGalgorithmtosolveTAPF.
work of a finite-horizon Markov Decision Process as
(S,A,P,r,γ,T):
4.1. ModelingTAPFasaMARLProblem
• S: Statespace. As shown in Figure 1, we provide two scenarios
to describe how we model the TAPF problem (de-
• A: Finiteactionspace. scribed in Section 3.3) as a cooperative MARL prob-
lem. As introduced in Section 3.2, cooperative multi-
agent deep RL can be modeled as a tuple G =
• P(s t+1|s t,a t): Statetransitiondistribution.
(S,A,P,r,O,N,γ,T). Inthisstudy,thestatetransition
(cid:16) (cid:17)
function P s′ | s,a : S×AN ×S → [0,1] is un-
• r(s,a): Rewardfunction. t+1 t t
t t known, whichmeanstheagentcannotgettheenviron-
ment dynamics. This is consistent with the real world
• γ: Discountfactor.
TAPF problem. The number of agents N in this study
can be set to any integer number. The discount factor
• T: Timehorizon. γ = 0.99, which is the most common setting in deep
RL.SincethemainelementsinmultiagentdeepRLare
In TAPF, each agent n corresponds to an RL agent. the observation space O (or state space S), the action
Thestates representsthecurrentpositionsofallagents. space A and the reward function r. Thus, we describe
t
A task set Tasks contains m goal positions to be as- theseelementsindetail.
signed. UnlikeMAPF,thegoalpositionforeachagent Observationspace: Anagent’sobservationcontains
nisnotpredefined. Instead,theproblemistoformulate thepositionandvelocityitself,therelativepositionsof
agoalassignmentscheme(policy)foreachagent. The all tasks, the relative positions of the other agents, and
objectiveistominimizethesumofpathlengthsforall the relative positions of neighboring obstacles. In this
agents,whichisequivalenttomaximizingtheexpected study, the number of other visible agents in an agent’s
cumulativerewardsE π,P[r 0:T−1]. observation can be set to equal to or less than N − 1.
InTAPF,conflictsarisewhenagents’assignedgoals Taking Figure 1 (a) as an example, the five purple cir-
leadtocollisionsorinefficientpathplanning. clesrepresentfiveagents,thefiveblackcirclesrepresent
4next position next position next position
agent goal
No dynamics
agent goal
Dynamics
Figure2:Thephysicaldynamicsofagents
(where m is the mass of the agent), we can get the
acceleration a ⃗ = Fdir⃗ ection. According to basic
direction m
physicsknowledge,wecangetthevelocityoftheagent
v ⃗ = v ⃗ +a ⃗ ∗∆t, where v is the
t(direction) 0(direction) direction 0
initialvelocityand∆tdenotesatimeinterval.
Action space: In this study, the action space of an
agent is continuous, representing the movements. As
shown in Figure 3, an agent obtains a velocity be-
tween 0.0m/s and 1.0m/s in each of the four cardinal
directions[move left(v⃗ ),move right(v⃗),move down
−x x
(v⃗ ),move up(v⃗)],thefinalaction(⃗v)iscalculatedby
−y y
thethevectorsumofthefourvelocities.
Rewardfunction: Wedefinetherewardfunctionas:
Figure3:Actionspaceofanagent
r=reward success+reward distance tasks to agents
+reward collision agents to obstacles
five tasks. For obstacles, we set the agent can percep-
tivetherelativepositions oftheadjacentobstacles, the +reward collision agents to agents
(2)
numberofadjacentobstaclescanbesettoanyinteger.
InFigure1(a)and(b),differentcolorcurvesdenotethe The detailed rewards are defined as follows (R is the
navigationtrajectoriesofdifferentagents. radiusofanagent):
The physical dynamics of agents: Before describ- • reward success=100∗n,
ing the action space, we first describe the dynamics of
wherendenotesthenumberofelementsthatsatisfy
agents.Notethatourworkisthefirsttoconsiderthedy-
∥P − P ∥<0.05.
tasks agents
namicsofagentsinTAPFfortheintelligentwarehouse.
AsshowninFigure2(a),the”Nodynamics”schematic • reward distance tasks to agents
diagramrepresentspreviousliterature[27,33]methods =-∥P − P ∥
tasks agents min
that only focus on the next position of an agent, but
don’t consider velocities and accelerations of agents. • reward collision agents to obstacles=-2∗n,
Further, the dynamics of agents in previous literature wherendenotesthenumberofelementsthatsatisfy
are solved by using traditional methods [34]. On the ∥P obstacles − P agents∥<R.
contrary, the ”Dynamics” schematic diagram in Figure
• reward collision agents to agents=-2∗n,
2 (b) represents our method that considers the dynam-
wherendenotesthenumberofelementsthatsatisfy
ics of agents. We can calculate agents’ velocities and
∥P − P ∥<2R.
accelerations. agents agents
The output of our policy network is the magni- Forreward success,∥P − P ∥representsthe
tasks agents
tude and direction of four cardinal directions force distance matrix of all tasks to all agents. Figure 4
(F⃗ x,F⃗ −x,F⃗ y,F⃗ −y) applied to the agent. According to shows a detailed distance matrix. If the distance of
Newton’ssecondlawofmotion,F ⃗ =m∗a ⃗ agent i to task i is less than 0.05m, we give a +100
direction direction
5Figure4:Distanscematrixoftasksandagents:∥Ptasks−Pagents∥.Theelementdis(ai,ti)representsthedistancebetweenagentiandtaski.
(cid:16) (cid:17)
positive reward to enforce agents to navigate to the and a = a1,...,aN . on is the observation received
t t t t
tasks. If there are n elements in ∥P − P ∥ by agent n at time step t. D denotes the replay buffer
tasks agents
less than 0.05m, we set reward success = +100 ∗ n. containingtuples(x,a,r ···).
t t t
Reward distance tasks to agents is created to provide Thecentralizedcriticparametersϕareoptimizedby
a dense reward to accelerate the training speed. In minimizingtheloss:
∥P − P ∥ , the subscript min means that we
tasks agents min (cid:104) (cid:105)
sumtheminimumdistanceofeachtasktoeveryagent. L(ϕ)=E (Q(x,a |ϕ)−y)2 (4)
(xt,at,rt,xt+1)∼D t t t
Reward collision agents to obstacles is aim to pun-
ishagentsfromcollisiontoobstacles. Thedefinitionof where r is the concatenation of rewards (Eq(2)) re-
t
∥P − P ∥issimilarwith∥P − P ∥, ceivedbyallagentsattimestept.Thetargetcriticvalue
obstacles agents tasks agents
∥P − P ∥denotesthedistancematrixofob- ynisdefinedas:
obstacles agents t
staclesandtasks. Ifthedistanceofanagenttoanobsta-
(cid:16) (cid:17)
c al re ei nsl ee ls es mt eh na tn sR in,a ∥Pne og bsa tt ai cv lee sr −ew Pa ard ge− nt2 s∥is leg si sve thn a. nIf Rt ,he wre
e
y t =r t+γQ− x t+1,a1 t+1,...,a tN +1 |ϕ− | an t+1=π(ot+1),n=1,.. (., 5N
)
setreward collision agents to obstacles=-2∗n. where Q− is a target Q-network parameterized by ϕ−.
Reward collision agents to agents is aim to punish ϕ−isoptimizedby:
agents from collision to agents. The definition of
∥P − P ∥ is similar with ∥P − P ∥, ϕ− =αϕ+(1−α)ϕ− (6)
agents agents tasks agents
∥P − P ∥ denotes the distance matrix of
agents agents
agentsandagents.Ifthedistanceofanagenttoanagent where α is a coefficient to trade-off the weight of Q-
is less than 2R, we give a negative reward −2. If there networkandtargetQ-network.
are n elements in ∥P − P ∥ less than 2R, we Thesharerewardmakingtheagentslearnpolicyco-
agents agents
setreward collision agents to agents=-2∗n. operatively.Intheexecutionphase,weonlyusethepol-
icyπwhoseinputsareo1,...,oN,respectively,andthe
t t
4.2. SolvingTAPFProblemviaCooperativeMARL outputs are force (F) applied to the agent. According
tothephysicaldynamicsofagentsdescribedinSection
In this study, we introduce MADDPG [31] to solve (cid:16) (cid:17)
4.1,wecangettheactions a = a1,...,aN . Notethat,
TAPF problem. Considering that agents are homoge- t t t
foragentn,theinputofπisonlytheobservationon re-
neous, thus the agents can share the same policy net- t
ceivedbyagentnattimesteptintheexecutionphase.
work. Thismakeslearningpolicymoreefficiently. Let
N denotethenumberofagents,πdenotethepolicypa-
rameterizedbyθ. ThecentralizedcriticQisparameter-
5. Experiments
izedbyϕ. Theparametersθforpolicyπcanbeupdated
iterativelybytheassociatedcriticQ:
Inthissection,weverifiedourmethodfromthreeas-
∇ J(θ)=E (cid:104) ∇ π(cid:0) on |θ(cid:1) pects. InSection5.1,weverifiedthetargetassignment
θ (xt,a (cid:16)t)∼D θ t (cid:17) (cid:105) (3) and path planning performances of our method. In all
∇ atQ x t,a1 t,...,a tN |ϕ | an t=π(on t|θ) theexperiments,thelocationsofagentsandtaskpoints
aregeneratedrandomly. InSection5.2,weverifiedthe
where x t and a t aretheconcatenationofall (cid:16)agents’ob (cid:17)- learnedcooperationability. InSection5.3, weverified
servation and action at time step t, x = o1,...,oN thetimeefficiency.
t t t
65.1. TargetAssignmentandPathPlanning 5.3. TimeEfficiency
Table 1 shows the time comparisons between tradi-
In this subsection, we verified our method in vari-
tionalmethods[35,36]andourmethod. Table1shows
ous of intelligent warehouse settings. We set five dif-
thatourmethodsimultaneouslyaddressesthetargetas-
ferent level scenarios to verify the performances: (1)
signment problem (TA) and the path planning (PF),
two agents - two tasks (2) two agents - four tasks (3)
however, the traditional method addresses the target
fiveagents-fivetasks(4)fiveagents-tentasks(5)five
assignment first and then performs the path planning.
agents-twentytasks.
Thus, theconsumedtimeinourmethodisTAPFtime;
Figure5showsthetrainingaveragereturncurves,in
all the different level scenarios, the average returns in- however,theconsumedtimeinthetraditionalmethodis
TA+PF.InTable1,ESmeanseachstep.Thereasonthat
crease monotonous. This verifies the stability of our
wecomparetheconsumedtimeofeachstepis: (1)Our
method.
methodhandlescontinuousactionspaceproblem;how-
Figure 6 to Figure 10 show the performances target
assignment and path finding in five different level sce- ever, traditional method handles discrete action space
problem. (2)Theconsumedtimeofpathfindingcanbe
narios. Experimentalresultsshowthatalthoughthedif-
ficulty of these five different level scenarios increasing influencedbymanyfactors,suchastheresolutionofthe
gridmapandthedistancetakenateachstepinthegrid
gradually,ourmethodperformswellinallthetasks.For
map. (3) previous literature [35, 36] did not consider
easytasks(twoagents-twotasks)inFigure6(a)(b)(c)
thephysicaldynamicsofagents. However,thephysical
(d),wecanseethatthetargetassignmentandpathfind-
dynamics of agents has been considered in this study.
ingwereaddressedwell. Fortargetassignment,results
Thus, forafaircomparison, weonlycomparethecon-
showthatthetaskassignmentisaddressedveryreason-
sumed time of each step, which means the consumed
able, because our method assigns the task to the agent
time from an agent obtain an observation to output the
thatisclosetothetask.Forpathfinding,itisshownthat
decision action. For traditional method, the consumed
theplannedpathsarealmostshortest.
Fordifficulttasks(fiveagents-twentytasks)inFigure timeoftargetassignmentiscomputedbythemethodin
[35], andtheconsumedtimeofeachstepinpathplan-
10(a)(b)(c)(d),wecanseethatthetargetassignment
ningiscomputedbythemethodin[36].
and path finding were also addressed well. For target
Table1showsthatineasytaskssuchtwoagents-two
assignment,itcanbeseenthateachagentisreasonably
tasksandtwoagents-fourtasks,theconsumedtimeof
assignedtoseveralnearbytasks. Forpathfinding, itis
target assignment in traditional method can be accept-
shown that the planned paths are usually shortest. The
samesuperiorperformancescanbeseeninotherdiffer-
able.However,withthedifficultyoftasksincreased,the
timeofsolvingtargetassignmentintraditionalmethod
ent level scenarios, such as two agents - four tasks in
increased rapidly, especially in the five agents - twenty
Figure 7, five agents - five tasks in Figure 8, and five
tasks scenario. However, our method provides policy
agents - ten tasks in Figure 9. This demonstrated the
efficiencyofourmethod. time-efficiently in all different level tasks (from two
agents - two tasks to five agents - twenty tasks). This
verified the time-efficiency ability of our method. As
5.2. CooperationAbility
we all know, time efficiency is an important factor in
We also designed a conflict scenario to verify the real-world TAPF problems. Therefore, the results of
learnedcooperationabilityofagents. AsshowninFig- theexperimentdemonstratedthepromisingengineering
ure11,thebigpurplecirclerepresentsagent-1,itstask practicalvalueofourmethod.NotethatFigure6toFig-
isdenotedbythesmallpurplecircle. Thebiggraycir- ure10hadshowntheperformancesoftargetassignment
cle represents agent-2, its task is denoted by the small andpathfindingofourmethodinfivedifferentlevelsce-
blackcircle. Wedeliberatelyblockotherroadstocreate narios. Fortargetassignment,theresultsshowthattask
theconflictenvironment. AsshowninFigure11,there assignmentisreasonablyaddressedbecauseourmethod
is bound to be a conflict between agent-1 and agent-2 assignsthetasktotheagentthatisclosetothetask. For
duringthenavigation. Thetrajectoriesgeneratedbyour pathfinding,theresultsshowthattheplannedpathsare
methodareshowninFigure11,theredcurveisthetra- almostshortest.
jectory of agent-1, and the cyan curve is the trajectory
ofagent-2. Resultsshowthatbothagent-1andagent-2
6. Conclusionandfuturework
hadlearnedtoavoideachotheratthepointofconflict,
and then navigated to their tasks. This verified the co- In this study, we propose a method to simultane-
operationabilityofourmethod. ously address the TAPF problem in intelligent ware-
7Agent:2 Task:2 Agent:2 Task:4
50 150
100
0
50
50
0
100 50
150 100
150
200
200
250
250
300 300
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Environment Steps 1e6 Environment Steps 1e6
(a) twoagents-twotasks (b) twoagents-fourtasks
Agent:5 Task:5 Agent:5 Task:10
5000
2000
4000
1000 3000
2000
0
1000
1000
0
2000 1000
0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4 1.6 1.8
Environment Steps 1e6 Environment Steps 1e6
(c) fiveagents-fivetasks (d) fiveagents-tentasks
Agent:5 Task:20
10000
8000
6000
4000
2000
0
2000
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5
Environment Steps 1e6
(e) fiveagents-twentytasks
Figure5:Averagereturn
8
nruteR
egarevA
nruteR
egarevA
nruteR
egarevA
nruteR
egarevA
nruteR
egarevA(a) (b)
(c) (d)
Figure6:Twoagents-twotasks
(a) (b)
(c) (d)
Figure7:Twoagents-fourtasks
9(a) (b)
(c) (d)
Figure8:Fiveagents-fivetasks
(a) (b)
(c) (d)
Figure9:Fiveagents-tentasks
10(a) (b)
(c) (d)
Figure10:Fiveagents-twentytasks
Table1:Timecomparisonsbetweenourmethodandtraditionalmethods
Scenarios Ours Traditionalmethods
twoagents-twotasks 0.0003221s(TAPF-ES) 0.02091s(TA)+0.0038107s(PF-ES)
twoagents-fourtasks 0.0003287s(TAPF-ES) 0.03887s(TA)+0.0036540s(PF-ES)
fiveagents-fivetasks 0.0003375s(TAPF-ES) 0.0498669s(TA)+0.0037150s(PF-ES)
fiveagents-tentasks 0.0003414s(TAPF-ES) 0.2918257s(TA)+0.0054604s(PF-ES)
fiveagents-twentytasks 0.0003505s(TAPF-ES) 14.66s(TA)+0.0038555s(PF-ES)
house from a perspective of cooperative multi-agent
deep RL. First, we model the TAPF problem as coop-
erativemulti-agentdeepRLproblem. Then,wesimul-
taneously address target assignment and path finding
by cooperative multi-agent deep RL algorithm. More-
over, previous literature rarely considers the physical
dynamicsofagents.Inthisstudy,thephysicaldynamics
of agents have been considered. Experimental results
show that our method performs well in various of task
settings, which means the target assignment is solved
reasonablyandtheplannedpathisalmostshortest. Fur-
thermore, our method is more time-efficient than base-
lines. For future work, we will apply our method to
Figure11:Thecooperationofagents
real-worldtargetassignmentandpathfindingproblem.
117. Acknowledgments [17] G. Wagner, H. Choset, Subdimensional expansion for multi-
robotpathplanning,ArtificialIntelligence219(2015)1–24.
ThisresearchwasfundedbyShenzhenbasicresearch [18] G.Sharon,R.Stern,A.Felner,N.R.Sturtevant,Conflict-based
programJCYJ20180507183837726andNationalNatu- search for optimal multi-agent pathfinding, Artificial Intelli-
gence219(2015)40–66.
ralScienceFoundationU1813206,61977019.
[19] G.Sharon,R.Stern,M.Goldenberg,A.Felner, Theincreasing
costtreesearchforoptimalmulti-agentpathfinding, Artificial
Intelligence195(2013)470–495.
References
[20] V.Nguyen,P.Obermeier,T.C.Son,T.Schaub,W.Yeoh,Gener-
alizedtargetassignmentandpathfindingusinganswersetpro-
[1] S.Adhau,M.L.Mittal,A.Mittal,Amulti-agentsystemfordis-
gramming, in: TwelfthAnnualSymposiumonCombinatorial
tributedmulti-projectscheduling:Anauction-basednegotiation
Search,2019.
approach,EngineeringApplicationsofArtificialIntelligence25
[21] D.Silver,Cooperativepathfinding,in:ProceedingsoftheAAAI
(2012)1738–1751.
conferenceonArtificialIntelligenceandInteractiveDigitalEn-
[2] A.Ayari,S.Bouamama,acd3gpso:automaticclustering-based
tertainment,volume1,2005,pp.117–122.
algorithm for multi-robot task allocation using dynamic dis-
[22] P. Surynek, A novel approach to path planning for multiple
tributeddouble-guidedparticleswarmoptimization, Assembly
robotsinbi-connectedgraphs,in:2009IEEEInternationalCon-
Automation(2019).
ferenceonRobotics andAutomation, IEEE,2009, pp.3613–
[3] P.R.Wurman, R.D’Andrea, M.Mountz, Coordinatinghun-
3619.
dredsofcooperative,autonomousvehiclesinwarehouses, AI
[23] P.Surynek,A.Felner,R.Stern,E.Boyarski, Modifyingopti-
Magazine29(2008)9–9.
malsat-basedapproachtomulti-agentpath-findingproblemto
[4] Q. Wan, C. Gu, S. Sun, M. Chen, H. Huang, X. Jia, Life-
suboptimalvariants,in:InternationalSymposiumonCombina-
long multi-agent path finding in a dynamic environment, in:
torialSearch,volume8,2017,pp.169–170.
2018 15th International Conference on Control, Automation,
[24] M.Barer,G.Sharon,R.Stern,A.Felner,Suboptimalvariantsof
RoboticsandVision(ICARCV),IEEE,2018,pp.875–882.
theconflict-basedsearchalgorithmforthemulti-agentpathfind-
[5] H.Ma,C.Tovey,G.Sharon,T.Kumar,S.Koenig, Multi-agent
ingproblem,in:SeventhAnnualSymposiumonCombinatorial
pathfindingwithpayloadtransfersandthepackage-exchange
Search,2014.
robot-routingproblem, in: ProceedingsoftheAAAIConfer-
[25] L.Peihuang,W.Xing,W.Jiarong,Pathplanningandcontrolfor
enceonArtificialIntelligence,volume30,2016.
multipleagvsbasedonimprovedtwo-stagetrafficscheduling,
[6] D.Silver,G.Lever,N.Heess,T.Degris,D.Wierstra,M.Ried-
InternationalJournalofAutomationTechnology3(2009)157–
miller, Deterministicpolicygradientalgorithms, in: Interna-
164.
tionalConferenceonMachineLearning(ICML),PMLR,2014,
[26] R.S.Sutton,A.G.Barto,Reinforcementlearning:Anintroduc-
pp.387–395.
tion,MITpress,2018.
[7] T.Rashid,M.Samvelyan,C.Schroeder,G.Farquhar,J.Foer-
[27] G.Sartoretti,J.Kerr,Y.Shi,G.Wagner,T.S.Kumar,S.Koenig,
ster, S. Whiteson, Qmix: Monotonic value function factori-
H.Choset, Primal:Pathfindingviareinforcementandimitation
sationfordeepmulti-agentreinforcementlearning, in: Inter-
multi-agentlearning, IEEERoboticsandAutomationLetters4
national Conference on Machine Learning, PMLR, 2018, pp.
(2019)2378–2385.
4295–4304.
[28] A.Tampuu, T.Matiisen, D.Kodelja, I.Kuzovkin, K.Korjus,
[8] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness,
J.Aru, J.Aru, R.Vicente, Multiagentcooperationandcom-
M.G.Bellemare, A.Graves, M.Riedmiller, A.K.Fidjeland,
petitionwithdeepreinforcementlearning, PloSone12(2017)
G. Ostrovski, et al., Human-level control through deep rein-
e0172395.
forcementlearning,Nature518(2015)529–533.
[29] P.Hernandez-Leal,M.Kaisers,T.Baarslag,E.M.deCote, A
[9] B.P.Gerkey,M.J.Mataric´,Aformalanalysisandtaxonomyof
survey of learning in multiagent environments: Dealing with
taskallocationinmulti-robotsystems,TheInternationalJournal
non-stationarity,arXivpreprintarXiv:1707.09183(2017).
ofRoboticsResearch23(2004)939–954.
[30] P.Sunehag, G.Lever, A.Gruslys, W.M.Czarnecki, V.Zam-
[10] C.Liu,A.Kroll, Acentralizedmulti-robottaskallocationfor
baldi, M. Jaderberg, M. Lanctot, N. Sonnerat, J. Z. Leibo,
industrialplantinspectionbyusingA*andgeneticalgorithms,
K.Tuyls,etal., Value-decompositionnetworksforcooperative
in: InternationalConferenceonArtificialIntelligenceandSoft
multi-agentlearningbasedonteamreward, in: Proceedingsof
Computing,Springer,2012,pp.466–474.
the17thInternationalConferenceonAutonomousAgentsand
[11] F.Glover,Tabusearch:Atutorial,Interfaces20(1990)74–94.
MultiAgentSystems,2018,pp.2085–2087.
[12] H.W.Kuhn,Thehungarianmethodfortheassignmentproblem,
[31] R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel,
NavalResearchLogisticsQuarterly2(1955)83–97.
I. Mordatch, Multi-agent actor-critic for mixed cooperative-
[13] S. Giordani, M. Lujak, F. Martinelli, A distributed multi-
competitiveenvironments,AdvancesinNeuralInformationPro-
agentproductionplanningandschedulingframeworkformobile
cessingSystems30(2017).
robots,Computers&IndustrialEngineering64(2013)19–30.
[14] G.Best,O.M.Cliff,T.Patten,R.R.Mettu,R.Fitch,Dec-mcts: [32] L.Kraemer, B.Banerjee, Multi-agentreinforcementlearning
asarehearsalfordecentralizedplanning, Neurocomputing190
Decentralizedplanningformulti-robotactiveperception, The
(2016)82–94.
InternationalJournalofRoboticsResearch38(2019)316–337.
[33] M.Damani,Z.Luo,E.Wenzel,G.Sartoretti,Primal2:Pathfind-
[15] D.S.Bernstein,R.Givan,N.Immerman,S.Zilberstein, The
ingviareinforcementandimitationmulti-agentlearning-life-
complexity of decentralized control of markov decision pro-
long, IEEERoboticsandAutomationLetters6(2021)2666–
cesses, MathematicsofOperationsResearch27(2002)819–
2673.doi:10.1109/LRA.2021.3062803.
840.
[34] D.H.MichaelFerguson,DavidV.Lu,Ros-planning/navigation,
[16] M.M.Zavlanos,L.Spesivtsev,G.J.Pappas,Adistributedauc-
https://github.com/ros-planning/navigation,2022.
tionalgorithmfortheassignmentproblem, in:200847thIEEE
[35] J.Gao,Y.Li,Y.Xu,S.Lv, Atwo-objectiveilpmodelofop-
Conference on Decision and Control, IEEE, 2008, pp. 1212–
matspforthemulti-robottaskassignmentinanintelligentware-
1217.
12house,AppliedSciences12(2022)4843.
[36] Y. Xu, Y. Li, Q. Liu, J. Gao, Y. Liu, M. Chen, Multi-agent
pathfinding with local and global guidance, in: 2021 IEEE
InternationalConferenceonNetworking,SensingandControl
(ICNSC),volume1,IEEE,2021,pp.1–7.
Appendix
Network Architecture: The actor network contains
four fully connected layers. As described in Section
4.1,theinputdimensionofthefirstlayerisequaltothe
observation dimension. The hidden dimension is 128.
The first three layers are followed by ReLU activation
function. Theoutputdimensionoftheforthlayeristhe
actiondimension.Thecriticnetworkcontainsfourfully
connectedlayers. Theinputdimensionofthefirstlayer
isequaltotheobservationdimensionplustheactiondi-
mension. Thehiddendimensionis128. Thefirstthree
layers are followed by ReLU activation function. The
output of the forth layer is the state-action value. Net-
worksaretrainedbyAdamoptimizer.
Hyperparameters: The discount factor is γ= 0.95 in
allexperiments. Thetotaltrainingstepis1M,thelearn-
ing rate of actor and critic network is 0.0004 and the
batch size=1024.
13