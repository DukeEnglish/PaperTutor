DCT-CryptoNets: Scaling Private
Inference in the Frequency Domain
ArjunRoy KaushikRoy
PurdueUniversity PurdueUniversity
roy208@purdue.edu kaushik@purdue.edu
Abstract
Theconvergenceoffullyhomomorphicencryption(FHE)andmachinelearning
offersunprecedentedopportunitiesforprivateinferenceofsensitivedata. FHE
enablescomputationdirectlyonencrypteddata,safeguardingtheentiremachine
learning pipeline, including data and model confidentiality. However, existing
FHE-basedimplementationsfordeepneuralnetworksfacesignificantchallenges
in computational cost, latency, and scalability, limiting their practical deploy-
ment. This paper introduces DCT-CryptoNets, a novel approach that leverages
frequency-domainlearningtotackletheseissues. Ourmethodoperatesdirectlyin
thefrequencydomain,utilizingthediscretecosinetransform(DCT)commonlyem-
ployedinJPEGcompression. Thisapproachisinherentlycompatiblewithremote
computingservices,whereimagesareusuallytransmittedandstoredincompressed
formats. DCT-CryptoNetsreducesthecomputationalburdenofhomomorphicop-
erations by focusing on perceptually relevant low-frequency components. This
isdemonstratedbysubstantiallatencyreductionofupto5.3×comparedtoprior
workonimageclassificationtasks,includinganoveldemonstrationofImageNet
inference within 2.5 hours, down from 12.5 hours compared to prior work on
equivalentcomputeresources. Moreover,DCT-CryptoNetsimprovesthereliabil-
ityofencryptedaccuracybyreducingvariability(e.g.,from±2.5%to±1.0%on
ImageNet). Thisstudydemonstratesapromisingavenueforachievingefficient
andpracticalprivacy-preservingdeeplearningonhighresolutionimagesseenin
real-worldapplications.
1 Introduction
Escalating concerns related to privacy and security in machine learning have fueled exploration
of private inference techniques. These approaches aim to protect both sensitive data and model
parameterswhilemaintainingahighqualityuserexperience,especiallywhenrelatedtoinference
latencyandaccuracy. Toaddressthesechallenges,privateinferencesolutionsbasedoncryptographic
principles,suchasfullyhomomorphicencryption(FHE)havebeenproposed. However,suchstrong
securityguaranteesoftencomeswithsignificantcomputationaloverheadandlatency,creatingbarriers
towidespreadadoption.
Earlyworkonfullyhomomorphicencryptedneuralnetworks(FHENNs)facedlimitationsinboth
latencyandaccuracy. Duetothelimitednativeoperationsofhomomorphicencryption,primarily
addition and multiplication, many prior methods resorted to approximating non-linear activation
functionsusingpolynomials[8,4,7]. However,thisapproachintroducesaccuracydegradationas
networksdeepen,duetothecumulativeeffectofapproximationerrors. Newerhomomorphicencryp-
tionschemeslikeTFHE(FHEovertheTorus)canhandlenon-linearactivationfunctionswithout
theneedforapproximations. Evenwithsuchfeatures,optimizingtheefficiencyofhomomorphic
Preprint.Underreview.
4202
guA
72
]RC.sc[
1v13251.8042:viXra1e5
5.3x
reduction
in latency
1e4
Method FHE Scheme.
Lee et al. (1) CKKS
1e3
Lee et al. (2) [1] CKKS
Kim et al. [1] CKKS
Rovida et al. CKKS
SHE TFHE
DCT-CryptoNets (ours) TFHE
1e2
ResNet-20 (274K) ResNet-18 (11M) ResNet-18 (11M)
CIFAR-10 CIFAR-10 ImageNet
Model (Parameters) & Dataset
Figure1: ScalabilityofstateoftheartimageclassificationmethodsinFHENN.DCT-CryptoNets
isabletoreducelatencyby5.3×comparedtoonlyothermethodabletoinferonImageNet(SHE
[21]). Toensureafaircomparison,SHElatencyvalueswerenormalizedtothesamecomputational
resourcesasDCT-CryptoNets. CKKS-basedmethodshavedifficultyscalingtolargernetworksand
datasetsduetotheirhighlyapproximatenature.1
operations(HOPs),whichincludeconvolutionsandnon-linearactivationsinthecontextofneural
networks,remainsakeyareaofresearch.
ToreducethecomputationalburdenofHOPsinFHENNs,previousworkhasinvestigatedstrategies
foroptimizingconvolutionsintheencrypteddomain[22,19,16,29,31]anddesigningarchitectures
thatminimizetheuseofnon-linearactivations[11,14]. Inthiswork,weprioritizeoptimizingnon-
linearactivationsbyoperatinginthefrequencydomain. Thisapproachisdrivenbythesignificant
computationalcostofnon-linearactivationsinFHE(e.g.,ReLUoperationsconsuming∼32.6%of
totalinferencetime[19]),andtheirheightenedsensitivitytospatialdimensions. Asdemonstrated
in[14],theseactivationsaredisproportionatelyconcentratedintheearlylayersofnetworkswhere
spatialdimensionalityisthelargest(e.g.,48%withinthefirstquarterofaResNet-18network).
Whilenotableadvancementshavebeenmade,manyFHENNoptimizationsremaintailoredtosmall
networksandimages(e.g.,32×32). ScalingFHENNtoImageNetandlargernetworkspresentsa
formidablechallenge.Increasingbothnetworksizeandimagedimensionsintroduceslatencyfromnot
onlyconvolutionsbutalsonon-linearactivations,whichare"free"inunencryptednetworks,andan
expensivehomomorphicbootstrappingoperation[10]. Thishomomorphicbootstrapisusedto"reset"
noiseintheciphertextthataccumulateforeveryencryptedoperation(additionandmultiplication),
whichcaneventuallycorrupttheciphertextmakingdecryptionimpossible. Approximately31.6%of
totalinferencetimeisspentonthisbootstrappingoperation[19]. Whileattemptshavebeenmadeto
implementFHENNonImageNet,theseeffortshighlightthesubstantialtrade-offsinvolved: either
prohibitivelatency(e.g., 2.5daysforResNet-18[21])orseverelylimitedencryptionscope(e.g.,
encryptingonly8layersinResNet-18[16]).
ThisworkintroducesDCT-CryptoNets,anovelframeworkthataddressesthecomputationalchal-
lengesoffullyhomomorphicencryptedneuralnetworks(FHENNs). WeutilizetheDiscreteCosine
Transform (DCT) to convert image tensors into frequency tensors with 8×8 filters, significantly
reducingspatialdimensionality. Thisnotonlyalignswiththehumanvisualsystem’sdifferential
sensitivitytoperceptuallyrelevantlow-frequencyinformation[23]butalsomitigatesthecompu-
tationalburdenofnon-linearactivationsby34.8%. Thisyieldstwoadditionaladvantages: (1)it
reducescomputationallyexpensivehomomorphicbootstrapping,asfewerReLUactivationsresultin
lessciphertextnoiseaccumulation,and(2)itimprovesthestabilityofencryptedaccuracythrough
the reduction of bootstrap operations which accumulate approximation errors. Additionally, the
1Leeetal. (2)[18]toscaletoaResNet-110(1.7Mparameters). Kimetal[16]scaletoaPlain-18network
(ResNet-18withoutskipconnections)butonlyencryptthelast8layerswhenrunningonImageNet.
2
)s(
ycnetaLemphasisonlow-frequencyinformationboostsaccuracycomparedtotraditionalRGB-basedCNNs
duetofocusingonvisuallysalientinformation. Overall,ourmethoddemonstratessubstantiallatency
improvementsinconvolution-basedFHENNsacrossarangeofimageclassificationdatasets,culmi-
natinginanoveldemonstrationofImageNetinferencewithin2.5hours–asignificantadvancement
overpriorworkthatrequiredanestimated12.5hoursonequivalentcomputeresources(previously
reportedas2.5days)[21]. TheseresultsunderscorethepotentialofDCT-CryptoNetsforaccelerating
privacy-preservingdeeplearningapplications.
Ourapproachmakesthefollowingcontributions:
• WeproposeDCT-CryptoNetstoachievesignificantlatencyimprovementsinFHENNs. Our
focusonlow-frequencycomponentsreducestheimpactofcomputationallyexpensivenon-
linearactivationsandhomomorphicbootstrapping,resultingina5.3×latencyreductionon
ImageNet,whilemaintainingorevenboostingaccuracycomparedtoRGB-basednetworks
duetothefocusonperceptuallysalientinformation.
• We show that DCT-CryptoNets also bolsters the reliability of encrypted accuracy. By
mitigatingtheneedforhomomorphicbootstrapoperations,theseoptimizationscurberror
accumulation. Thisresultsinanotablereductioninencryptedaccuracyvariability(e.g.,
from±2.5%to±1.0%onImageNet).
• WeshowthatDCT-CryptoNetsexhibitssuperiorscalabilitywithincreasingresolution. This
isevidentintheamplifiedreductionofhomomorphicoperations(HOPs)observedforlarger
images(e.g.,a33.1%vs. 44.1%reductioninHOPswhenapplyingDCT-basedfrequency
optimizationstoa224×224vs. 448×448imagerespectively).
2 BackgroundandRelatedWork
2.1 CrytographicProtocolsforPrivateInference
Homomorphicencryptionreliesonlattice-basedcryptography[30]andtheLearningwithErrors
(LWE)problem[24],leveragingthecomputationalhardnessofsolvingequationsonhigh-dimensional
lattices with added noise. Lattice-based cryptography has gained significant recognition, with 3
outofthe4finalistsintheNationalInstituteofStandardsandTechnology(NIST)post-quantum
cryptographystandardizationprocessbeingbasedonthisapproach.
Many state-of-the-art private inference techniques leverage hybrid, "interactive" approaches that
combine homomorphic encryption (HE) with other cryptographic protocols such as multi-party
computation(MPC)[15,26,17]. InMPC,multiplepartiesjointlycomputeafunctionontheirprivate
inputswithoutdisclosingthoseinputstooneanother. Whileensuringnopartylearnsmorethanthe
finalresult,MPCstillcanleakinformationascommunicationbetweenpartiesisnecessary. Although
robust,thesemethodsstillposeasecurityriskinlow-trustenvironments.
Incontrast,"non-interactive"FHE-onlymethodsoffersuperiordataconfidentiality. FHEensures
thatneitherrawdatanorintermediatevaluesareexposedinplaintext. ThismakesFHEparticularly
well-suitedforscenarioswithhighlysensitivedataandmodelparameters,especiallyinsituations
wherethereislimitedtrustbetweenparties. However,thisenhancedprivacycomesatthecostof
increasedcomputationaloverheadcomparedtoMPCandhybridapproaches.
2.2 LimitationsofExistingFHENNSchemes
FHEschemesserveasthefundamentalcryptographicbuildingblocksforvariousapplication-specific
methodologies. ThechoiceofanFHEschemeinvolvesacarefulevaluationofthetrade-offsinherent
toeachschemetoensureoptimalperformance. FHENNsbasedonearlierschemessuchasBFV[9]
andBGV[3]wereknownfortheirefficiencyforencryptedadditionandmultiplication. However,
thesemethodsoftenfacedscalabilitychallengesduetotheirrelianceonpolynomialapproximations
of activations (PAAs) and pooling operations. PAAs introduce accuracy degradation due to the
cumulativeeffectofapproximationerrorsindeepernetworks. Tomitigateaccuracydegradation,
techniques to increase the polynomial degree of PAAs have been introduced [13]. Despite this,
thecomputationalcostofPAA-basednon-linearactivationsincreasesexponentiallywithnetwork
depth, becoming a major bottleneck to latency in deeper networks [21]. A more recent scheme,
CKKS[5],offersacompellingadvantageforneuralnetworkapplicationsbyoperatingdirectlyon
3floating-pointnumbers. Nevertheless,CKKSinheritsthesamechallengesassociatedwithPAAsand
alsointroducesadditionalaccuracyconcernsduetoitsrelianceonapproximatearithmetic,whichcan
leadtosignificanterroraccumulationindeepnetworks.
Further efforts to optimize PAA-based methods have shown promise but remain limited. Lee et
al. [19] introduced a binary-tree implementation for ReLU in the CKKS scheme, yet scalability
isconfinedtosmallermodelslikeResNet-20. Falcon[22]introducedfasthomomorphicdiscrete
Fouriertransform(HDFT)forconvolutionsandfullyconnectedlayersinBFV,significantlyreducing
operations. However,theapproachstillsuffersfromlimitedaccuracy(76.5%onCIFAR-10). Kimet
al. [16]extendedHDFTtoCKKS,butonlythefinal8layersofaPlain-18networkareencrypted
when operating on ImageNet. Lee et al. [18] proposed removing the imaginary components of
PAAs,thusenablingscalabilityuptoResNet-110(1.7Mparameters). WhilePAA-basedmethods
haveshownpromiseforsmallernetworks,theirinherentconstraintsinhandlingincreaseddepthand
computationalcomplexityhindertheirscalabilitytodeepernetworks.
2.3 AdvantagesofTFHE
IncontrasttootherHEschemes,TFHE(FHEovertheTorus)operateswithinthedomainofthetorus
modulo1,representingciphertextsaselementsofthisstructure. TFHEexcelsinperformingfast
andexactbinaryoperationsonencryptedbits,utilizingintegerandlogicgates. Thefoundationfor
theseoperationsliesintheM-thcyclotomicpolynomial,denotedasΦ(X). Itsdegree,representedby
N,istypicallychosenasapowerof2forefficiencyreasons,andsimplifiesΦ(X)toXN +1. By
definingpolynomialringsoverrealcoefficientsR [X]:=R[X]/(XN +1)andintegercoefficients
N
Z [X]:=Z[X]/(XN +1),weestablishtheToruspolynomialringT [X]:=R [X]/Z [X]=
N N N N
T[X]/(XN +1)whichformsaZ [X]-module. ThisalgebraicstructureisfundamentaltoTFHE,
N
facilitatingbothadditionandexternalmultiplicationbypolynomialsofZ [X].
N
UnlikeHEschemesthatrelyonPAAs,TFHE’sabilitytodirectlyimplementnon-linearactivations
using Boolean/integer arithmetic is a key factor in scalability. Directly implementing non-linear
activationsavoidstheaccuracydegradationoftenassociatedwithPAAsindeepernetworks. SHE
(LouandJiang[21]),aTFHE-basedFHENN,isabletoshowscalabilitytoImageNet. Theydoso
byemployingabit-seriesrepresentationandtechniqueslikelogarithmicweightquantizationand
bit-shift-basedconvolutions. However,SHEstillsuffersfromprohibitivelatencyonImageNet(e.g.,
ResNet-18in2.5days,ShuffleNetin5hoursperimage). Furthermore,SHEutilizesaleveledTFHE
schemewhichnecessitateshighermultiplicativedepthsandlargerciphertextsfordeepernetworks.
Inourapproach,weapplyprogrammablebootstrappingmechanisms(PBS)[6]tosupportarbitrarily
deepneuralnetworks. PBSservestwocrucialpurposes: (1)betterciphertextnoisereductionby
reorderingrotationandkey-switchciphertextoperationsand(2)enablinghomomorphicevaluation
ofanyfunctionexpressibleasalookup-table. PBS’sabilitytohomomorphicallyevaluatefunctions
expressedaslookup-tablesmakesitwell-suitedforimplementingnon-linearactivationfunctions.
2.4 Hyper-quantizationBackground
Hyper-quantizationisalossycompressiontechniquethatimprovestheefficiencyofneuralnetwork
models by reducing the precision of numerical representations (weights and activations). This
notonlylowersmemoryrequirementsandacceleratescomputationbutalsonaturallyalignswith
theinteger-basedpolynomialrepresentationinTFHE,facilitatingefficientcomputationwithinthe
encrypteddomain. However,hyper-quantizationoftenleadstoatrade-offinaccuracyasthelimited
representationcanintroduceerrors.
Quantization-awaretraining(QAT)canhelpmitigatetheseaccuracytrade-offsbysimulatingthe
effectsofquantizationduringthetrainingprocess. Byconstrainingweightsandactivationstofixed-
pointrepresentations,QATintentionallyintroducesquantizationnoiseduringtraining,enablingthe
network to learn and adapt to these errors through gradient-based optimization. Crucially, "fake
quantization"isemployedtomaintainfull-precisionweightsforsmootherconvergence[27]. Inthe
contextofuniformquantization,weconvertarealvaluer ∈[α,β]intoab-bitintegerq =⌊r +Z⌋,
S
where S = β−α represents the quantization scale and Z is the zero-point. Dequantization is
2b−1
achieved by the inverse operation: r = S(q −Z). Previous efforts to quantize FHENNs have
produced promising results. Notably, [32] achieved successful inference on a VGG-9 network
usingan8-bitquantizedTFHEschemewithprogrammablebootstrapping(PBS).Theirapproach
4DCT Transformation for Pre-processing Network Architecture
DCT-
Channel Dim. 82 8x8 Filter? 64 Channels CryptoNets DCT-Net [35] RGB
Spatial Dim.(s) 8 Cx56x56 Cx56x56 3x224x224
Y 1x1 Conv, 1x1 Conv, 7x7 Conv,
64, /1 64, /1 64, /2
BN BN BN
ReLU ReLU
Pool, /2
Cr
Residual
Blocks
Cb
Pool
FC
Reshape Subselect
DCT Spatial & Low Concatenate
Transform Channel Frequency & Normalize Prediction
Dimensions Channels
Figure2: Frequencybasepre-processingproposedby[35]andDCT-CryptoNetsproposednetwork
architecture. Changesfrommethodtomethodareemphasizedindarkerpurple. GoingfromRGB-
basedResNetheadtoDCTResNetheadincludeschangestofirstconvolutionlayerandtheexclusion
ofthepoolinglayer. ChangesfromtheOriginalDCT-basedResNetheadtoDCT-CryptoNetsResNet
headincludepruningtheReLUoperatorafterthefirstconvolutionlayerasproposedby[11,14].
achieved87.5%accuracyonaVGG-9networkbutstillhadsignificantlatencyof18,000secondsfor
inferenceontheCIFAR-10dataset. Moreaggressivequantizationcanalsoyieldlatencyreductions,
suchasa40%decreaseinruntimewhenmovingfroman8-bittoa2-bitBFV-basedFHENNfor
CIFAR-10[20]. Quantization-awaretrainingfacilitatestheseamlessconversionofourmodelsintoa
TFHE-compatibleframework,enablingustomaintainhighaccuracywhilesignificantlyreducing
computationaloverhead.
2.5 FrequencyDomainBackground
Inadeparturefrompriorwork,ourmethodoperatesdirectlyinthefrequencydomain,utilizedthe
DiscreteCosineTransform(DCT)-akeycomponentofJPEGcompression,widelyusedforimage
transmissionandstorage. Thisallowsourmethodtobedirectlycompatiblewithmachinelearningas
aservice(MLaaS)systemsandotherapplicationswhereimagedataistransmittedincompressed
formatsforremoteanalysis.The1-DDCTtransformusesthefollowingequationwherex represents
n
asignalandX representtheDCTcoefficients. DCTin2-Disthesame1-Dequationappliedtothe
k
widthandheightdimensions.
N−1 (cid:20) (cid:18) (cid:19)(cid:21)
(cid:88) kπ 1
X = x cos n+ k =0,...,N −1. (1)
k n N 2
n=0
WhiletheDCTitselfislosslessandreversible,byselectivelyretainingonlylow-frequencycompo-
nentswecanreducedatarequirementswhilepreservingimageclassificationaccuracy[35]. This
approach leverages the human visual system’s differential sensitivity to various frequency com-
ponents,allowingmodelstofocusonthemostperceptibleelements. Adversariallyrobustobject
recognitionmodelsdesignedtoresistperceptibleperturbationshavebeenshowntoprimarilyrely
onthislow-frequencyinformation[33,23]. Additionally,utilizingDCTcomponentsdirectlycan
improvetheprivacypreservationofrecognitionsystemsagainstvariouswhite-boxandblack-box
attacks[34]. Whilepreviouswork[25]hasexploredDCTforhomomorphicimagecompressionand
decompression,ourmethoduniquelyretainstheencrypteddatainthefrequencydomainthroughout
theCNNinferenceprocess. Theshiftfromhigh-spatialRGBtolow-frequencyDCTcomponents
inourtrainingapproachoffersadualadvantage. Byprioritizingperceptuallyrelevantinformation,
itholdsthepotentialforimprovedimageclassification. Simultaneously,thespatialdimensionality
reductionoftheDCTsignificantlylowersthecomputationaldemandsofhomomorphicinference,
particularlybyreducingtheburdenofnon-linearactivationsandhomomorphicbootstrapoperations.
5Table1: Comparisonofmultiplyandaccumulate,non-linearactivation,programmablebootstrapand
overallhomomorphicoperationsforRGB-based(toprow)andDCT-based(middlerows)ResNet-18
network. WeseethatwhenmovingfromtheRGBtotheDCTdomainthereisonlyaslightreduction
in MACs but a larger reduction in ReLUs and HOPs. We also see the same reduction in ReLU
operationsregardlessonthenumberoffrequencychannelssub-selected,allowingustouseaDCT
representationthatprovidesthebesttaskspecificaccuracy. Weobserveanadditional11%reduction
inHOPswhenapplyingDCT-CryptoNetstoa224×224vs. 448×448imagerespectively,showing
thatthecomputationaleffectivenessofDCToptimizationsimproveasimageresolutionincreases.
ImageNet-sizedImages Large(er)Images
Dimension #MACs #ReLUs #PBS #HOPs Dimension #MACs #ReLUs #PBS #HOPs
3×2242 1.82G 2.31M 22.4M 1.85G 3×4482 7.29G 9.23M 89.3M 7.67G
6×562 1.70G 1.51M 15.5M 1.24G 6×1122 6.82G 6.02M 50.7M 4.29G
24×562 1.71G 1.51M 15.5M 1.26G 24×1122 6.83G 6.02M 50.9M 4.33G
48×562 1.71G 1.51M 15.6M 1.26G 48×1122 6.85G 6.02M 51.2M 4.35G
64×562 1.72G 1.51M 15.6M 1.27G 64×1122 6.86G 6.02M 51.3M 4.38G
192×562 1.74G 1.51M 15.7M 1.28G 192×1122 6.97G 6.02M 51.7M 4.41G
Max∆
-6.54% -34.8% -30.0% -33.1% -6.54% -34.8% -43.9% -44.1%
RGB→DCT
3 Methodology
In this paper we propose DCT-CryptoNets, a novel approach for homomorphic inference that
achievesscalabilityandlowlatencythroughatrainingmethodologyandnetworktopologyspecifically
designed for the frequency domain. Our approach is composed of two key components: (1) A
network architecture optimized for the Discrete Cosine Transform (DCT) domain with strategic
ReLUreductionsand(2)EncryptionofthetrainednetworkintoaTFHE-compatiblescheme.
3.1 NetworkTopologyintheDCTDomainwithReLUReductions
Following the methodology in [35], we first transform RGB images to YCrCb color space, then
applytheDiscreteCosineTransform(DCT)togeneratefrequencydomainrepresentations. Weadopt
standard JPEG compression’s 8×8 filtering scheme, yielding 64 frequency channels per YCrCb
colorspace. Thisprocesseffectivelydownsamplesspatialdimensions(H,W)byafactorof8while
increasingchanneldimensions(C)byafactorof82. Subsequently,wesub-selectafixedportion
ofthelow-frequencychannels,emphasizingluma(Y)componentsduetotheirhigherperceptual
relevancecompared tochroma(Cr, Cb). Theselectionadheres toa standard4:2:2luma-chroma
subsamplingratio. Notably,thetotalnumberofhomomorphicoperations(HOPs)remainsrelatively
consistentregardlessofthenumberofsub-selectedfrequencychannels,allowingforflexibilityin
choosinganetworkthatprovideshighaccuracy(seeTable1). TheDCT’s8×8filteringofspatial
informationintofrequencychannelsprovesparticularlyadvantageousasimagesizeincreases. We
observethatDCTbecomesincreasinglyvaluableforHOPoptimizationaswescaletoImageNet-
sizedimagesandbeyond. For4482 images,weobservea44.1%reductioninHOPscomparedto
a33.1%reductionfor2242 images. WhilethepercentageofReLUoperationsremovedremains
constant,thisamplifiedHOPreductionatlargerimagesizescanbeattributedtothedecreasedneed
forbootstrappingoperations,adirectconsequenceoffewerReLUactivations(seeTable1). This
highlightsthescalabilityofDCT-CryptoNetsinhandlinglargerinputs.
Giventhedistinctdimensionalityoffrequency-domainfeatures(reducedHandW,increasedC),
wemodifytheinitialstride-2convolutionlayer,commonlyfoundinCNNs,withastride-1layer.
The subsequent layer’s channel size is then adjusted to accommodate the transformed input (see
Figure2). ThisapproachminimallymodifiesexistingCNNarchitecturestoacceptfrequency-domain
input. FurtheroptimizationinvolvespruningtheReLUactivationsfollowingthechannel-widening
convolution layer. This reduction in ReLU operations does not significantly impact accuracy, as
earlier-layerReLUstypicallyexhibitlowerimportancethanthoseinlaterlayers[14]. Whiletheskip
connectionpruningproposedin[11]alsoreducesReLUcount,itincursamuchgreateraccuracycost
andalsoincreaseHOPs(seeAppendixA.2). Therefore,weoptedforanetworkarchitecturethat
specificallyincorporatesfirst-layerReLUpruningasshowninFig.2.
6Table2: EvaluatingperformanceonlowresolutionimagesbycomparingthetraditionalResNet-20
and our proposed DCT-CryptoNets ResNet-20 on CIFAR-10. For DCT-CryptoNets ResNet-20,
various combinations of retained frequency channels and spatial input sizes were explored. Our
findingsindicatethatretainingthetop24low-frequencychannels(outof48)andaspatialsizeof
16×16leadstoa4%accuracyimprovementoverthetraditionalResNet-20modelwhilemaintaining
comparablelatency.
Input
Model Accuracy Latency(s)
Dimension
3×322 TraditionalResNet-20 86.5% 603
RGB
3×322 DCT-CryptoNetsResNet-20 91.6% 1,339
24×82 DCT-CryptoNetsResNet-20 85.2% 137
48×82 DCT-CryptoNetsResNet-20 86.5% 144
DCT
24×162 DCT-CryptoNetsResNet-20 90.5% 565
48×162 DCT-CryptoNetsResNet-20 90.4% 567
WhileDCTofferssignificantbenefitsforlargerimages,itsdirectapplicationtolowerresolutions
canleadtoexcessivespatialdimensionreduction. Toaddressthischallenge,weintroduceasecond
specializedarchitecture, tailoredforsmallerinputs(e.g., 32×32pixels). Thisadaptationutilizes
a4×4DCTfilter, resultingin48frequencychannels, andincorporatesarchitecturaladjustments
tomaintainspatialresolutionandpreventinformationloss(seeAppendixA.3foranarchitecture
deep-dive). Despiteanincreasedparametercount,thisarchitectureachievescomparablelatencyand
a4%accuracyimprovementoveratraditionalResNet-20onCIFAR-10. Furthermore,evenwith
furtherspatialreductionfrom16×16to8×8,weobservesimilaraccuracytoatraditionalResNet-20
coupled with a 4.2× latency improvement (see Table 2). This demonstrates the effectiveness of
DCT-CryptoNetseveninlowresolutionsettings.
3.2 EncryptingintoaTFHECompatibleScheme
Wefirstemployaquantization-awaretraining(QAT)frameworkonthepreprocesseddataandchosen
networktopology. ToachieveanoptimalbalancebetweenaccuracyandlatencyforourFHENNs,we
systematicallyexploredvariousquantizationlevels(seeAppendixA.1). Ourapproachemploysa
4-bitsymmetricquantizationschemewithafixedzero-pointforalltasksexceptImageNet,where
5-bitquantizationisused. Thesechoicesstrikeabalancebetweenefficiencyandaccuracy,allowing
ustoperformtheentireneuralnetworkcomputationdirectlyonencryptednumberswithintheTFHE
framework.
To fully encrypt the trained neural network within the TFHE scheme, we employ an automated
optimization process [2]. This process balances three key hyperparameters: (1) the circuit bit-
width, defined as the minimum number of bits required to represent the largest integer arising
duringcomputation. Thisgenerallymimicsthe4-bitor5-bitintegerusedduringQAT;(2)precision
roundingformodelaccumulators,whichimplementsascalingfactorusedtoremoveleastsignificant
bits;and(3)thedesirederrorprobabilityforprogrammablebootstrapping(PBS)operations. Unlike
polynomialapproximation(PAAs)thatintroduceinherentapproximationerrors,PBSerrorprobability
isatunablehyperparameter. Itcontrolsthelikelihoodofasmallerrorvalueaddedtothelook-uptable
(LUT)operation. Bycarefullytuningthesehyperparameters,theoptimizationprocessdeterminesthe
cryptosystemparametersthatensureefficientexecution.
(cid:26)
T[x], withprobability(1−p )
LUT[x]= err (2)
T[x+k], withprobabilityp , k =(±1,±2,...)
err
Determiningoptimalcryptographichyperparametersinvolvesempiricalexplorationduetotheindirect
control over the underlying cryptosystem parameters. Decreasing the PBS error probability or
increasingtheprecisionroundingthresholdtoallowmorebitsnecessitatesadditionalhomomorphic
operations(HOPs). Thispresentsatrade-off: whileenhancingaccuracy,italsoincreaseslatency. We
noticethatforsimplertasks,aprecisionroundingof6andaPBSerrorprobabilityof0.01achievea
goodbalance. However,morecomplextaskslikeImageNetrequirefurtherhyperparametertuningto
maintainaccuracy. Forinstance,increasingprecisionroundingto7reducesbittruncationbutleadsto
ahighernumberofHOPs,thusincreasinglatency(seeAppendixA.4foradetailedanalysis).
7Table3:Top-1accuracyofdatasetswithvaryinglevelsofretainedlow-frequencycomponents,ranging
fromahighlylossy6-channelrepresentationtoalossless192-channelrepresentation. Accuracies
arereportedasunencryptedplusorminusa95%confidenceintervalgeneratedbytheencryption
processasexplainedinSection4.2. AsDCTmodelsperdatasetsharesimilarnumberofHOPsand
cryptographichyperparameters,theirconfidenceintervalsareconsistent.
Input CIFAR-10 mini-ImageNet Imagenette ImageNet
Model
Dimension (10classes) (100classes) (10classes) (1Kclasses)
RGB ResNet-18 3×2242 92.4±2.4% 88.9±2.4% 88.5±2.0% 66.1±2.5%
ResNet-18 6×562 89.7±0.6% 81.6±1.8% 84.3±0.4% 55.5±1.0%
ResNet-18 24×562 91.2±0.6% 90.0±1.8% 87.5±0.4% 65.8±1.0%
DCT ResNet-18 48×562 91.7±0.6% 90.1±1.8% 86.4±0.4% 66.1±1.0%
ResNet-18 64×562 92.4±0.6% 89.5±1.8% 86.5±0.4% 66.3±1.0%
ResNet-18 192×562 92.3±0.6% 89.7±1.8% 83.8±0.4% 64.1±1.0%
4 Results
4.1 ExperimentalSetup
DCT-CryptoNetsutilizesseveraltoolsandframeworks. ImageconversiontotheDCTdomainis
performedusinglibjpeg-turbo,Brevitas[28]facilitatesquantization-awaretraining,andConcrete-ML
[36]enablesencryptionwithintheTFHEscheme. NeuralnetworktrainingisconductedonanRTX
A40GPUandFHENNlatencymeasurementsareobtainedfromdualAMDRyzenThreadripper
PRO5965WXprocessors(96-threadstotal). EachnetworkistrainedonCIFAR-10,mini-ImageNet,
Imagenette,andImageNet,withvaryingconfigurationsofDCTcomponents. Dataaugmentation
strategiesincluderandomcropping,colorjitter,andrandomflipping. ToassessFHENNaccuracy,
weleverageConcrete-ML’ssimulationaccuracyfeature,allowingustoefficientlyestimateexpected
accuracyandgathervaluablemetricsonthecompiledFHENN.Latencymeasurementsareconducted
byprocessingasingleimageatatimeonadedicatedmachinewithnobackgroundtasksrunning,
ensuringmaximumutilizationofallavailablethreads.
4.2 AccuracyandReliabilityEnhancementswithLow-FrequencyInformation
Todemonstratetheimpactoffocusingonvisuallysalientlow-frequencyinformationweevaluated
modelaccuracyacrossCIFAR-10,mini-ImageNet,Imagenette,andImageNetonupsampled3×2242
RGBinputsandtheirDCT-basedrepresentationswithvaryingfrequencycomponents(seeTable3).
DuetothecomputationalcostofobtainingsimulationaccuracyresultsinConcrete-ML(approxi-
mately8secondsperImageNetimage),comprehensivevalidationonthefullImageNetvalidation
set was impractical. To estimate the impact of encryption on accuracy, we employed statistical
bootstrappingtogenerate95%confidenceintervals(CIs)foreachmodel. Thisinvolvedrepeatedly
(10Ktimes)analyzingrandomcombinationsofsubsets(20subsetseachwith200randomimages)to
determineaccuracyvariability. Theresultingdistributionofaccuraciesallowedustoestimatethe
95%CIforeachmodel,reflectingtheexpectedvariabilityduetoencryption.
DCT-basednetworksconsistentlymatchedoroutperformedtheaccuracyoftheirRGBcounterparts,
particularlyonlargerdatasets(mini-ImageNet,ImageNet),duetotheirfocusonperceptuallyrelevant
low-frequency information. Generally, retaining the top 64 low-frequency channels yielded the
bestaccuracy. Thissuggeststhatlosslessrepresentations(192channels)containunnecessaryhigh-
frequencycomponentsthathinderlearning. Thus,somedegreeof"lossy-ness"allowsthenetworkto
prioritizevisuallysalientinformation,furtherimprovingaccuracy. Furthermore,DCT-basedmethods
exhibitednarrowerconfidenceintervals,indicatinggreateraccuracystabilityintheencrypteddomain.
Thisimprovedreliabilitystemsfromthe-30%reductioninprogrammablebootstrap(PBS)operations
(seeTable1),whichindividuallycontributetoerroraccumulation. DCT-CryptoNetsmeetsorexceeds
theaccuracyofRGB-basedmethodswhilealsoenhancingthereliabilityofencryptedaccuracy.
4.3 EvaluationofAccuracyandLatency
DCT-CryptoNetsdemonstratescompetitiveaccuracywhencomparedtoexistingmethodsonboth
CIFAR-10andImageNet(seeTable4). WhileCKKS-basedmethodsdemonstrateslightlylower
8Table 4: Performance comparison of DCT-CryptoNets and prior work. We selected the 64×562
DCTrepresentationforImageNetand24×162 forCIFAR-10duetotheirsuperioraccuracy. The
RGBvariants,trainedon3×322or3×2242inputs,serveasbaselineswithoutDCToptimizationso
demonstratetheisolatedimpactofDCT-basedoptimizations. DCT-CryptoNetsshowcomparable
performanceonsmallermodels/datasetsbutexcelinscalingtolargerones,highlightingitspotential
forreal-worldapplications.
Normalized
Input
Method Dataset Model Scheme Accuracy Latency(s) Latency(s)
Dimension
(96-threads)
CryptoDL[12] CIFAR-10 3×322 CustomCNN BGV 91.4% 11,686 ∼
FasterCryptonets[7] CIFAR-10 3×322 CustomCNN FV-RNS 75.9% 3,240 ∼
SHE[21] CIFAR-10 3×322 CustomCNN TFHE 92.5% 2,258 470
Leeetal.(1)[19] CIFAR-10 3×322 ResNet-20 CKKS 92.4% 10,602 ∼
Leeetal.(2)[18] CIFAR-10 3×322 ResNet-20 CKKS 91.3% 2,271 ∼
Kimetal.[16] CIFAR-10 3×322 Plain-20 CKKS 92.1% 368 ∼
Ranetal.[29] CIFAR-10 3×322 ResNet-20 CKKS 90.2% 392 ∼
Rovidaetal.[31] CIFAR-10 3×322 ResNet-20 CKKS 91.7% 336 ∼
Benamiraetal.[1] CIFAR-10 3×322 VGG-9 TFHE 74.0% 570 48
Stoianetal.[32] CIFAR-10 3×322 VGG-9 TFHE 87.5% 18,000 3,000
DCT-CryptoNets CIFAR-10 3×322 ResNet-20 TFHE 91.6% 1,339 1,339
DCT-CryptoNets CIFAR-10 24×162 ResNet-20 TFHE 90.5% 565 565
SHE[21] CIFAR-10 3×322 ResNet-18 TFHE 94.6% 12,041 2,509
DCT-CryptoNets CIFAR-10 3×322 ResNet-18 TFHE 92.3% 1,746 1,746
DCT-CryptoNets CIFAR-10 24×162 ResNet-18 TFHE 91.2% 1,004 1,004
SHE[21] ImageNet 3×2242 ResNet-18 TFHE 66.8% 216,000 45,000
DCT-CryptoNets ImageNet 3×2242 ResNet-18 TFHE 66.1% 16,115 16,115
DCT-CryptoNets ImageNet 64×562 ResNet-18 TFHE 66.3% 8,562 8,562
latencyonsmallermodelsduetotheirutilizationofsingleinstructionmultipledata(SIMD)packing
andpolynomialapproximationsofactivationfunctions(PAAs), theinherentlimitationsofPAAs
hinderCKKS-basedmethodsfromscalingtolargernetworksandhigh-resolutionimageslikethose
inImageNet. It’scrucialtonotethatCKKS’sfocusonamortizedruntimethroughSIMDpacking,a
featureunavailableinTFHE,makesdirectlatencycomparisonsvianormalizationlessmeaningful.
ComparingDCT-CryptoNetstoSHE[21],theonlyothermethoddemonstratingscalabilitytoIma-
geNet,weobservecomparableaccuracyandlatencyforsmallermodelsandimages. It’sworthnoting
that SHE’s fastest model on CIFAR-10 is a custom CNN without skip connections, contributing
to its slight latency advantage over our ResNet-20 model. As network or image size increases,
DCT-CryptoNetsshowcasessuperiorscalability,withprogressivelygreaterlatencyimprovements
comparedtoSHEculminatingina5.3×speeduponImageNet. Furthermore,evenwithoutDCT
optimizations,largerDCT-CryptoNetsResNet-18modelsoutperformSHEonbothCIFAR-10and
ImageNet. ThisimprovementcanbeattributedtoSHE’srelianceonaleveledTFHEscheme,which
necessitateshighermultiplicativedepthsandlargerciphertextsleadingtolatencyamplificationsin
deepernetworks. Incontrast,byleveragingprogrammablebootstrapping,DCT-CryptoNetsmitigates
theselimitationsandenablesefficienthandlingoflargermodels.
Overall,ourresultsdemonstratethatDCT-CryptoNetsachievesperformanceonparwithprevious
workonsmallerimagesandnetworks,whileshowcasingsuperiorscalabilityandefficiencygains
whenappliedtolarger,morecomplextasks.
4.4 LimitationsandFutureDirections
A key limitation of our approach is that performance is highly dependent on the cryptographic
hyperparametersandthedegreeofbitprecisionduringquantization-awaretraining(QAT).Careful
tuningofprecisionrounding,programmableboostrapping(PBS)errorprobabilityandQATbit-level
isessentialtonavigatethetrade-offbetweenaccuracyandlatency. Futurestepsshouldbemadeto
improvethecryptographicandquantizationparameterselectionprocess.
Furthermore,theaccuracybenefitsfromDCToptimizationsarelesspronouncedonsmallerimages
(e.g., 32×32)comparedtolargerones. Thisobservationalignswiththeinherentnatureofsmall
images,whichoftencontainlesshigh-frequencyinformationthatDCTfilteringtypicallyremoves.
9Consequently,theimpactoffocusingonlow-frequencycomponentsislesssignificantforthesesmaller
inputs. FutureworkcouldexploretechniquestoenhancetheaccuracybenefitsofDCTforsmaller
images,potentiallythroughadaptivefilteringoralternativefrequency-domainrepresentations.
5 Conclusion
Inthispaper,wepresentedDCT-CryptoNets,anovelframeworkthataddressesthecriticalchallenges
ofscalabilityandcomputationalcostinfullyhomomorphicencryptedneuralnetworks(FHENNs),
particularly for high resolution images. By combining frequency-domain learning through the
DiscreteCosineTransform(DCT),DCT-CryptoNetssignificantlyreducesthecomputationalburden
ofnon-linearactivationsandhomomorphicbootstraps,enablingmoreefficientinferenceonlarge-
scaledatasetslikeImageNet.Ourapproachdemonstratessubstantiallatencyimprovements,achieving
a 5.3× speedup compared to the only prior work on ImageNet and a boost in the reliability of
encryptedaccuracyvariability. Furthermore,isolatingtheimpactofDCT-basedoptimizationsreveals
a latency improvement ranging from 1.7× to 2.4×. These advancements showcase the promise
offrequency-domaintechniquesforprivacy-preservingdeeplearning. Thisnotonlyrepresentsa
significantsteptowardspracticaldeploymentofFHEinreal-worldapplications,butalsoopensnew
avenuesforfutureresearchinoptimizingprivateinferencefordeeplearning.
Acknowledgements
This project was supported from the Purdue Center for Secure Microelectronics Ecosystem –
CSME#210205.
References
[1] A.Benamira,T.Gu’erand,T.Peyrin,andS.Saha. Tt-tfhe:atorusfullyhomomorphicencryption-friendly
neuralnetworkarchitecture. ArXiv,2023.
[2] L.Bergerat,A.Boudi,Q.Bourgerie,I.Chillotti,D.Ligier,J.-B.Orfila,andS.Tap. Parameteroptimization
andlargerprecisionfor(t)fhe. JournalofCryptology,2023.
[3] Z.Brakerski,C.Gentry,andV.Vaikuntanathan. (leveled)fullyhomomorphicencryptionwithoutboot-
strapping. InInformationTechnologyConvergenceandServices,2012.
[4] A.Brutzkus,O.Elisha,andR.Gilad-Bachrach. Lowlatencyprivacypreservinginference. InInternational
ConferenceonMachineLearning,2018.
[5] J.H.Cheon,A.Kim,M.Kim,andY.Song. Homomorphicencryptionforarithmeticofapproximate
numbers. InInternationalConferenceontheTheoryandApplicationofCryptologyandInformation
Security,2017.
[6] I.Chillotti,M.Joye,andP.Paillier. Programmablebootstrappingenablesefficienthomomorphicinference
ofdeepneuralnetworks. CyberSecurityCryptographyandMachineLearning,2021.
[7] E.Chou,J.Beal,D.Levy,S.Yeung,A.Haque,andL.Fei-Fei. Fastercryptonets:Leveragingsparsityfor
real-worldencryptedinference. ArXiv,2018.
[8] N.Dowlin, R.Gilad-Bachrach, K.Laine, K.E.Lauter, M.Naehrig, andJ.R.Wernsing. Cryptonets:
Applyingneuralnetworkstoencrypteddatawithhighthroughputandaccuracy.InInternationalConference
onMachineLearning,2016.
[9] J.FanandF.Vercauteren. Somewhatpracticalfullyhomomorphicencryption. IACRCryptol.ePrintArch.,
2012.
[10] C.Gentry. Computingarbitraryfunctionsofencrypteddata. CommunicationsoftheACM,2010.
[11] Z.Ghodsi,A.K.Veldanda,B.Reagen,andS.Garg. Cryptonas: Privateinferenceonarelubudget. In
AdvancesinNeuralInformationProcessingSystems,2020.
[12] E.Hesamifard,H.Takabi,andM.Ghasemi. Cryptodl:Deepneuralnetworksoverencrypteddata. arXiv
preprintarXiv:1711.05189,2017.
[13] E.Hesamifard,H.Takabi,andM.Ghasemi.Deepneuralnetworksclassificationoverencrypteddata.ACM
ConferenceonDataandApplicationSecurityandPrivacy,2019.
[14] N.K.Jha,Z.Ghodsi,S.Garg,andB.Reagen. Deepreduce: Relureductionforfastprivateinference.
InternationalConferenceonMachineLearning,2021.
10[15] C.Juvekar,V.Vaikuntanathan,andA.Chandrakasan. GAZELLE:Alowlatencyframeworkforsecure
neuralnetworkinference. In27thUSENIXSecuritySymposium(USENIXSecurity18),2018.
[16] D.KimandC.Guyot. Optimizedprivacy-preservingcnninferencewithfullyhomomorphicencryption.
IEEETransactionsonInformationForensicsandSecurity,2023.
[17] B.Knott,S.Venkataraman,A.Y.Hannun,S.Sengupta,M.Ibrahim,andL.vanderMaaten. Crypten:
Securemulti-partycomputationmeetsmachinelearning. AdvancesinNeuralInformationProcessing
Systems,2021.
[18] E.Lee,J.-W.Lee,J.Lee,Y.-S.Kim,Y.Kim,J.-S.No,andW.Choi. Low-complexitydeepconvolutional
neuralnetworksonfullyhomomorphicencryptionusingmultiplexedparallelconvolutions. International
ConferenceonMachineLearning,2022.
[19] J.-W.Lee,H.Kang,Y.Lee,W.I.Choi,J.Eom,M.A.Deryabin,E.Lee,J.Lee,D.Yoo,Y.-S.Kim,and
J.-S.No.Privacy-preservingmachinelearningwithfullyhomomorphicencryptionfordeepneuralnetwork.
IEEEAccess,10,2022.
[20] W.Legiest,J.-P.D’Anvers,F.Turan,M.V.Beirendonck,andI.M.R.Verbauwhede. Neuralnetwork
quantisationforfasterhomomorphicencryption. InternationalSymposiumonOn-LineTestingandRobust
SystemDesign,2023.
[21] Q.LouandL.Jiang. She: Afastandaccuratedeepneuralnetworkforencrypteddata. InAdvancesin
NeuralInformationProcessingSystems,2019.
[22] Q.Lou,W.jieLu,C.Hong,andL.Jiang. Falcon:Fastspectralinferenceonencrypteddata. InAdvances
inNeuralInformationProcessingSystems,2020.
[23] C.Luo,Q.Lin,W.Xie,B.Wu,J.Xie,andL.Shen. Frequency-drivenimperceptibleadversarialattackon
semanticsimilarity. 2022IEEE/CVFConferenceonComputerVisionandPatternRecognition,2022.
[24] V.Lyubashevsky,C.Peikert,andO.Regev.Onideallatticesandlearningwitherrorsoverrings.Association
forComputingMachinery,2010.
[25] A.Mertens,G.Nicolas,andS.Rovira. Convolution-friendlyimagecompressioninfhe. CryptologyePrint
Archive,2024.
[26] P.Mishra,R.Lehmkuhl,A.Srinivasan,W.Zheng,andR.A.Popa. Delphi: Acryptographicinference
serviceforneuralnetworks. In29thUSENIXSecuritySymposium(USENIXSecurity20),2020.
[27] M.Nagel,M.Fournarakis,R.A.Amjad,Y.Bondarenko,M.vanBaalen,andT.Blankevoort. Awhite
paperonneuralnetworkquantization. ArXiv,2021.
[28] A.Pappalardo. Xilinx/brevitas,2023. https://doi.org/10.5281/zenodo.3333552.
[29] R.Ran,X.Luo,W.Wang,T.Liu,G.Quan,X.Xu,C.Ding,andW.Wen.Spencnn:Orchestratingencoding
andsparsityforfasthomomorphicallyencryptedneuralnetworkinference. InInternationalConferenceon
MachineLearning,2023.
[30] O.Regev. Onlattices,learningwitherrors,randomlinearcodes,andcryptography. InSymposiumonthe
TheoryofComputing,2005.
[31] L. Rovida and A. Leporati. Encrypted image classification with low memory footprint using fully
homomorphicencryption. InternationalJournalofNeuralSystems,2024.
[32] A.Stoian,J.Fréry,R.Bredehoft,L.Montero,C.Kherfallah,andB.Chevallier-Mames. Deepneural
networksforencryptedinferencewithtfhe. InIACRCryptologyePrintArchive,2023.
[33] A.Subramanian,E.Sizikova,N.J.Majaj,andD.G.Pelli. Spatial-frequencychannels,shapebias,and
adversarialrobustness. InAdvancesinNeuralInformationProcessingSystems,2023.
[34] Y.Wang,J.Liu,M.Luo,L.Yang,andL.Wang. Privacy-preservingfacerecognitioninthefrequency
domain. ProceedingsoftheAAAIConferenceonArtificialIntelligence,2022.
[35] K.Xu,M.Qin,F.Sun,Y.Wang,Y.kuangChen,andF.Ren.Learninginthefrequencydomain.IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,2020.
[36] Zama. ConcreteML:aprivacy-preservingmachinelearninglibraryusingfullyhomomorphicencryption
fordatascientists,2022. https://github.com/zama-ai/concrete-ml.
11A Appendix
A.1 QuantizationAblationonImageNet
Table5: QuantizationimpactforResNet-18onImageNet
Quantization Accuracy Latency(s)
4-bit 60.6% 12,874
RGB
5-bit 66.1% 16,115
(3×2242)
8-bit 68.4% 75,674
4-bit 62.9% 6,961
DCT
5-bit 66.3% 8,562
(64×562)
8-bit 68.5% 41,998
Toexaminetheimpactofquantizationonperformance,anablationstudywasperformedvarying
quantizationlevelsonImageNet,asdetailedinTable5. Adopting5-bitquantization,consistentwith
theapproachinSHE[21],resultedinanotableaccuracyimprovementcomparedto4-bitquantization,
withamanageableincreaseinlatencyofapproximately25%.Furtherincreasingthequantizationlevel
to8-bityieldedonlymarginalaccuracygains,whileincurringasubstantialincreaseinlatency. These
resultssuggestthat5-bitquantizationstrikesaneffectivebalancebetweenaccuracyandefficiencyfor
large-scaledatasetslikeImageNet.
A.2 ReLUPruningTechniques
Table6: ImpactofdifferentReLUpruningtechniques(3×2242RGB→64×562DCT).
+FirstLayer +SkipConnections
∆: RGB→DCT DCT-Net[35] +BothPruned
Pruned[11] Pruned[11]
#ReLUs -26.1% -34.8% -58.7% -67.4%
#HOPs -21.7% -33.1% -24.3% -25.7%
CIFAR-10 +0.0% -0.4% -2.67% -2.54%
ImageNet +2.2% +2.3% ∼ ∼
DCT-Net [35] inherently reduces ReLU operations due to spatial dimension reduction. DCT-
CryptoNets further optimizes latency by pruning the first layer’s ReLU’s, with minimal impact
onaccuracy. However,skipconnectionReLUpruningisnotrecommendedasitincurssignificant
accuracy degradation and increases homomorphic operations even though the number of ReLU
operationsdecreases. ThisisbecauseremovingReLUaftertheadditionoftwoquantizedtensors
necessitatestheinsertionofaquantizationidentityfunction.
A.3 DCT-CryptoNetsforSmall(er)ImagesandNetworks
When applying DCT optimizations to smaller images and networks (e.g., ResNet-20 on 32×32
images),adaptationsarenecessarytoaddresstheexcessivespatialdimensionreductioncausedby
thestandard8×8DCTfilter. WemitigatethisbyreducingtheDCTfiltersizeto4×4,resultingin
48frequencychannels(16perYCrCbcomponent)insteadofthetypical192. Toaccommodatethis
change,weintroduceamodifiedResNet-20architecturewithtwokeyadjustments(seeFig.3):
• DownsamplingReduction: Onedownsamplingstepisremovedtopreservespatialresolution
throughoutthenetwork.
• ChannelExpansion: Theinitialchanneldimensionsareincreasedtomatchthe48-channel
DCToutput,preventinginformationlossduringthefirstconvolutionallayertransition. The
modifiedchanneldimensionsarenow[48,56,64].
Althoughthischannelexpansiondoublesthenumberofparameters,itenableseffectiveDCTuti-
lizationonsmallerimages. AsdemonstratedinTable2,DCT-CryptoNetsResNet-20achievesa4%
accuracyimprovementoverthetraditionalResNet-20whilemaintainingcomparablelatency.
12
.spO
.ccATraditional DCT-CryptoNets
ResNet-20 ResNet20
RGB: 3x32x32 3x3 Conv, 16 DCT: 48x16x16 3x3 Conv, 48
3x3 Conv, 16 3x3 Conv, 48
3x3 Conv, 16 3x3 Conv, 16 1x1 Conv, 48 3x3 Conv, 48
32 x 32 16 x 16
3x3 Conv, 16 3x3 Conv, 48
Residual 3x3 Conv, 16 Residual 3x3 Conv, 48
Blocks Blocks
3x3 Conv, 16 3x3 Conv, 48
3x3 Conv, 32, /2 3x3 Conv, 56
Pool 3x3 Conv, 32 Pool 3x3 Conv, 56
FC FC
3x3 Conv, 32 16 x 16 3x3 Conv, 56 16 x 16
Prediction 3x3 Conv, 32 Prediction 3x3 Conv, 56
3x3 Conv, 32 3x3 Conv, 56
3x3 Conv, 32 3x3 Conv, 56
3x3 Conv, 64, /2 3x3 Conv, 64, /2
3x3 Conv, 64 3x3 Conv, 64
3x3 Conv, 64 3x3 Conv, 64
8 x 8 8 x 8
3x3 Conv, 64 3x3 Conv, 64
3x3 Conv, 64 3x3 Conv, 64
3x3 Conv, 64 3x3 Conv, 64
Figure3: Traditionalvs. DCT-CryptoNetsResNet-20Architecture
A.4 SensitivityofCryptographicHyperparametersinFHENN
Table7: Comparingthechangeoflatency,top-1accuracyandciphertextmemoryrequirementsofa
3×2242modelon1000samplesfromImageNetwhenmodulatingtwoencryptionhyperparameters
(precisionroundingandPBSerrorprobability).
PrecisionRounding=6 PrecisionRounding=7 PrecisionRounding=8
PBSErr. ∆Latency ∆Acc. ∆Memory ∆Latency ∆Acc. ∆Memory ∆Latency ∆Acc. ∆Memory
0.05 -25.6% -36.8% -24.4% -10.7% -32.2% -17.9% +34.0% -35.7% +19.1%
0.01 -12.1% -21.3% -6.2% ⋆ ⋆ ⋆ +81.7% +0.4% +56.8%
0.005 -34.2% -9.4% -8.0% +62.3% +0.0% +5.5% +203.4% +1.2% +93.3%
Encryption of the neural network requires careful selection of cryptographic hyperparameters to
balanceaccuracyandlatency. Startingwithourpre-selectedvaluesforImageNet(precisionrounding
of7andPBSerrorprobabilityof0.01),asdetailedinTable7,wesystematicallyexploretheeffects
ofvaryingtheseparameters. Reducingeitherparameterleadstoasignificantdropintop-1accuracy.
Increasingtheseparametersgenerallyyieldsanoticeableincreaseinlatency, withonlymarginal
accuracygains. Notably, raisingthebit-precisionroundingto8andhalvingthePBSprobability
increasestop-1accuracyonImageNetsubsamplesbyupto1.2%butincursathreefoldincreasein
latency.
A.5 InthecontextofMLaaS
DCT-CryptoNetsleveragestheDiscreteCosineTransform(DCT)tooperatedirectlyinthefrequency
domain,aligningwiththeprevalentuseofDCTinimagecompressionstandardslikeJPEG.This
enablesseamlessintegrationwithexistingimageprocessingpipelinesandfacilitatesefficienttrans-
missionandprocessinginmachinelearningasaservice(MLaaS)systemsandotherapplications
whereimagedataissentforremoteanalysis. Inthisparadigm(Figure4),amodelistrainedlocally,
encrypted,andthendeployedinthecloud. Duringinference,userstransformtheirimagesintoDCT
representations(aprocessoftenalreadyperformedduetoJPEGcompression)andencryptthemusing
theirprivatekey. Theencryptedimageisthensenttothecloud-basedmodelforprocessing,andthe
resultingencryptedoutputisreturnedtotheuser. Toobtainthefinalprediction,theuserdecrypts
theoutputofthepenultimatelayer(priortothefully-connectedlayer)andprocessestheresulting
plaintexttensorthroughalocalclassifier.
13Quantization
DCT Aware Trained
Transform Training Model
Encypt
Model
DCT Encrypt DCT Trained & Decrypt
"Cat"
Transform Components Encrypted & Predict
Model
Client-side Server-side Client-side
Encoder "Cat"
Encrypted Frequency
Componants
Server-side Client-side
Figure4: MLaaSSystemwithDCT-CryptoNets.
A.6 TFHE-basedFHENNComputeResources
Table8: ComparisonofTFHE-basedmethodsbasedonavailablecomputeresources.
Normalized
Input Reported CPU
Method Dataset Model Accuracy Latency(s)
Dimension Latency(s) Threads
(96-threads)
SHE[21] CIFAR-10 3×322 CustomCNN 92.5% 2,258 20 470
Benamiraetal.[1] CIFAR-10 3×322 VGG-9 74.01% 570 8 48
Stoianetal.[32] CIFAR-10 3×322 VGG-9 87.5% 18,000 16 3,000
DCT-CryptoNets CIFAR-10 3×322 ResNet-20 91.6% 1,339 96 1,339
DCT-CryptoNets CIFAR-10 24×162 ResNet-20 90.5% 565 96 565
SHE[21] CIFAR-10 3×322 ResNet-18 94.62% 12,041 20 2,509
DCT-CryptoNets CIFAR-10 3×322 ResNet-18 92.3% 1,746 96 1,746
DCT-CryptoNets CIFAR-10 24×162 ResNet-18 90.9% 1,004 96 1,004
DCT-CryptoNets CIFAR-10 3×2242 ResNet-18 92.4% 11,097 96 11,097
DCT-CryptoNets CIFAR-10 64×562 ResNet-18 92.4% 6,313 96 6,313
SHE[21] ImageNet 3×2242 ResNet-18 69.4% 216,000 20 45,000
DCT-CryptoNets(4-bit) ImageNet 3×2242 ResNet-18 60.6% 12,874 96 12,874
DCT-CryptoNets(5-bit) ImageNet 3×2242 ResNet-18 66.1% 16,115 96 16,115
DCT-CryptoNets(4-bit) ImageNet 64×562 ResNet-18 62.9% 6,961 96 6,961
DCT-CryptoNets(5-bit) ImageNet 64×562 ResNet-18 66.3% 8,562 96 8,562
Since TFHE prioritizes individual inference latency, we normalize prior work’s latency values
to the 96-thread resource capacity of DCT-CryptoNets to ensure a fair comparison. This direct
normalizationbasedonCPU-threadsismotivatedbythefactthattheConcrete-MLlibraryutilizes
100%ofavailableCPUresources. WenoticedwhenrunningDCT-CryptoNetsona24-threadIntel
Corei9-12900KFtherewasexactly4×increaseinlatencycomparedtothe96-threaddualAMD
RyzenThreadripperPRO5965WX.ItisimportanttonotethatmanyCKKS-basedmethodsutilize
singleinstructionmultipledata(SIMD)packingtooptimizeamortizedruntime,afeaturenotavailable
inTFHE.Consequently,normalizinglatencyforCKKS-basedmethodswouldnotaccuratelyreflect
theirperformancecharacteristicsduetotheirinherentrelianceonbatchprocessing.
14
gniniarT
ecnerefnI
reifissalC