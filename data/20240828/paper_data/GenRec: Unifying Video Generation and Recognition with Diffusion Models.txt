GenRec: Unifying Video Generation and Recognition
with Diffusion Models
ZejiaWeng1,2, XitongYang3, ZhenXing1,2,ZuxuanWu1,2†, Yu-GangJiang1,2
1ShanghaiKeyLabofIntell. Info. Processing,SchoolofCS,FudanUniversity
2ShanghaiCollaborativeInnovationCenterofIntelligentVisualComputing
3DepartmentofComputerScience,UniversityofMaryland
Abstract
Videodiffusionmodelsareabletogeneratehigh-qualityvideosbylearningstrong
spatial-temporalpriorsonlarge-scaledatasets. Inthispaper, weaimtoinvesti-
gatewhethersuchpriorsderivedfromagenerativeprocessaresuitableforvideo
recognition,andeventuallyjointoptimizationofgenerationandrecognition. Build-
inguponStableVideoDiffusion, weintroduceGenRec, thefirstunifiedframe-
worktrainedwitharandom-frameconditioningprocesssoastolearngeneralized
spatial-temporalrepresentations. Theresultingframeworkcannaturallysupports
generationandrecognition,andmoreimportantlyisrobustevenwhenvisualinputs
containlimitedinformation. Extensiveexperimentsdemonstratetheefficacyof
GenRecforbothrecognitionandgeneration. Inparticular,GenRecachievescom-
petitiverecognitionperformance,offering75.8%and87.2%accuracyonSSV2
andK400,respectively. GenRecalsoperformsthebestclass-conditionedimage-
to-video generation results, achieving 46.5 and 49.3 FVD scores on SSV2 and
EK-100datasets. Furthermore,GenRecdemonstratesextraordinaryrobustnessin
scenariosthatonlylimitedframescanbeobserved.
1 Introduction
Diffusionmodelshaveachievedsignificantsuccessinthefieldofimageandvideogenerationover
thepastfewyears. Avarietyofgenerativetaskshavebeenrevolutionizedbyusingdiffusionmodels
trainedonInternet-scaledata,suchastext-to-imagegeneration[33,30],imageediting[23],andmore
recently,text-to-videogeneration[15,2,51]andtext&image-to-videogeneration[53,18,21]. The
excellentgenerativecapabilitiesofdiffusionmodelssuggestthatinformativerepresentationislearned
during the generative training and strong visual priors are captured by the backbone models [9,
43, 6]. Therefore, recent work has explored leveraging the image diffusion models for image
understandingtasks,includingimagerecognition[9,8],objectdetection[7,54],segmentation[52]
andcorrespondencemining[39]. However,thecapabilityofvideodiffusionmodelstoeffectively
capturespatial-temporalinformationisnotfullyunderstood,andtheirpotentialfordownstreamvideo
understandingtasksremainsunder-explored.
Inthispaper,westudythepotentialofvideodiffusionmodels[28,1,27],particularlytheuncondi-
tionedorimage-conditionedmodels,forvideounderstandingbyaddressingthethreekeyproblems:
(a)Doesthebackbonemodeltrainedforvideogenerationextracteffectivespatial-temporalrepresen-
tationsforsemanticvideorecognition? (b)Canweretainthevideogenerationcapabilitybyjointly
optimizinggenerationandrecognition? (c)Willsuchaunifiedtrainingframeworkfurtherbenefit
videounderstanding,especiallyinnoisyscenarioswhereonlylimitedframesareavailable[3,25].
Whileconceptuallyappealing,unifyingvideogenerationandrecognitionintoadiffusionframework
isnon-trivial. Priorworkeitherviewsthediffusionmodelsasfrozenfeatureextractors[9,52,39],
ordeconstructsthemfornewtaskswhilesacrificingtheiroriginalgenerationcapability[8]. One
4202
guA
72
]VC.sc[
1v14251.8042:viXraClassification Generation Classification Generation
Loss Loss Loss Loss
Classifier Generator GenRec
FullVideos NoisyVideos MaskedVideos NoisyVideos
𝑽 𝑽 𝑽
𝝈 𝑴 𝝈
𝑵(⋅,𝝈) 𝑴(⋅) 𝑵(⋅,𝝈)
Video𝑽 Video𝑽 Video𝑽
(a)Classification (b)DiffusionGeneration (c)GenRec
Figure1: Comparisonofclassicalpipelinesforvideoclassificationandgenerationtaskswithour
proposedGenRecmethod. (a)Classification: Typicalvideoclassificationfocusonunderstanding
completevideos. (b)DiffusionGeneration: Diffusionmodelslearnthenoisereductiontrajectory
fromvideoswithvaryinglevelsofnoise. Thesetwodistincttrainingparadigmspresentchallenges
fortaskunification. Tobridgethisgap,wepropose(c)GenRec: alearningframeworkthatprocesses
maskframesV usingamaskingfunctionM(·)andnoisevideosV withnoisesamplingN(·,σ),
M σ
aimingtosimultaneouslylearnvideounderstandingandcontentcompletionwiththesamepartially
observedvisualcontent.
majorchallengecomesfromtheirdistincttrainingandinferenceprocesses. Diffusionmodelsare
typicallyoptimizedusingcorruptedinputs,optionallyaugmentedwithasingleconditioningframe,to
achieveunconditionedorimage-conditionedgenerationduringinference[27,1]. Incontrast,video
recognition models require access to multiple frames to reason about temporal relationships and
expect clean inputs during inference [48, 50]. Consequently, training a recognition model using
corruptedvideosandsingle-imageconditionstendstosufferfrominferiormodeloptimizationanda
moresignificanttraining-inferencegap.
Tothisend,weproposeGenRec,aunifiedvideodiffusionmodelthatenablesjointoptimizationfor
videogenerationandrecognition. Ourmodelisbuiltupontheopen-source,image-conditionedStable
VideoDiffusionmodel(SVD)[1],whichencodesstrongspatial-temporalpriorsbypretrainingon
large-scaleimageandvideodata. However,insteadofconditioningonthesameimageacrossall
videoframes,weproposetoconditiononarandomsubsetofframeswhilemaskingtheremaining
ones (see Figure 2). This simple random-frame conditioning process effectively bridges the gap
betweenthelearningprocessesofthetwotasks. Ontheonehand,thegenerationcapabilityofSVD
isextendedtohandlearbitraryframeprediction, whichprovidesmoreflexibleandunambiguous
videogeneration. Ontheotherhand,conditioningonarandomsubsetofframesallowsthemodelto
learnmorediscriminativeandrobustfeaturesfortherecognitiontask. Asshownin Figure1,the
modelisjointlyoptimizedusingbothgenerativesupervision(i.e.,noiseprediction)andclassification
supervision.
WeconductextensiveexperimentstoevaluatetheperformanceofGenRecforbothrecognitionand
generation. Withoutsacrificingthegenerationcapabilities,GenRecdemonstratescompetitivevideo
recognition performance, offering 75.8% and 87.2% accuracy on SSV2 and K400, respectively.
Furthermore,GenRecdemonstratesextraordinaryrobustnessinscenariosthatonlylimitedframes
can be observed. For example, when only the front half of the video can be observed, GenRec
achievesthe57.7%accuracy,whichcorrespondsto76.6%oftheaccuracy(75.3%)whentheentire
videoisvisible,emonstratingahigheraccuracyretentionratiothanothermethods. Byleveraging
therecognitionmodelforclassifierguidance[11],GenRecalsoachievessuperiorclass-conditioned
image-to-video generation results, with FVD scores of 46.5 and 49.3 on the SSV2 and EK-100
datasets,respectively.
22 Preliminary
Representingthedatadistributionasp (z)withastandarddeviationofσ ,wecanobtainafamily
data data
ofsmootheddistributionsp(z;σ)byaddingindependentandidenticallydistributedGaussiannoise
withstandarddeviationσ. Inthespiritofdiffusionmodels,thegenerationprocessbeginswitha
noiseimagez ∼N(0,σ2 I)anditerativelydenoisesitatdecreasingnoiselevelsσ =σ >
N max N max
σ >...>σ =0. Thefinaldenoisedresultz isthusdistributedaccordingtotheoriginaldata.
N−1 0 0
IntheEDM[22]framework,theoriginalz willbediffusedas:
0
z =z +σ·N(0,I), (1)
σ 0
andthecorrespondingPF-ODE[34]follows:
dz =−σ·∇ logp (z )dσ, (2)
σ z σ σ
where∇ logp (z )isthescorefunction. Noisescheduleσ(t)issetastimestept. Thetraining
z t t
objectiveistominimizetheL2losswiththedenoisernetworkD fordifferentσ:
θ
E ||D (z )−z ||2, (3)
z0∼pdata θ σ 0 2
withtherelationbetweenD andthescorefunction∇ logp(z;σ)asfollows:
θ z
∇ logp(z;σ)=(D(z )−z)/σ2. (4)
z σ
SVD[31]utilizestheEDMframeworktoperformgenerativetrainingonlarge-scalevideodatasets,
resultinginahigh-qualityvideogenerationmodel. Animage-to-videogenerationmodelcapable
offorecastingfutureframesgiventhefirstframehasbeenreleased. FollowingSVDmethod,we
also process videos in latent space. Given an input video V ∈ RT×H×W×3, a pretrained VAE
encoderisusedtoprojectitintothelatentspaceframebyframe,resultinginthelatentrepresentation
z ∈RT×h×w×D. WethenbuildGenRecbasedonSVD,inheritingitsstrongspatial-temporalpriors
0
asfoundationforthesubsequentgenerationandclassificationtasks.
3 GenRec
WenowintroduceGenRec,asimpleyetefficientframework,thatcannotonlygeneratetemporally-
coherentvideosconditionedonanarbitrarynumberofprovidedframesbutalsoisabletorecognize
actionsandeventswiththehelpofencodedspatial-temporalpriors. Tothisend,GenRecexplores
thestrongspatial-temporalpriorslearnedbyavideodiffusionmodel. Inthiswork,weinstantiate
GenRecwiththepowerfulopen-sourceStableVideoDiffusionmodel(SVD)[1],whichispretrained
onlarge-scalevideodatasetsandisabletoproduceaphoto-realisticvideowhenprovidedasingle
frame. Then,forgeneration,GenRecfollowstheclassicalEDMframeworktolearnnoisereduction
trajectories. Forrecognition,ontheotherhand,GenRecoperatesonintermediatedecodedfeatures
usingarecognitionhead. Furthermore,togeneratevideosinamorefreefashion,i.e.anarbitrary
collectionofframesusedascondition,wedesignalatentmaskingstrategythat“interpolates”masked
frames. Suchastrategyalsobenefitsrecognitionbyeasingthetrainingprocess. Moreimportantly,
by doing so GenRec supports a multitude of downstream tasks, particularly when limited visual
informationisprovided.
3.1 PipelineOverview
Latentdiffusionandlatentmasking. Duringthediffusionprocess,theGaussiannoisewitha
certainnoiselevelisaddedtothelatentrepresentationz ,creatinganoisylatentrepresentationz˜
0 i
followingEquation(1). RecallthatwhileSVDcontainspowerfulspatial-temporalpriors,itcanonly
perform generation when the first frame is provided. To allow a more “free” generation with an
arbitrarynumberofframesasinputs,wedesignalatentmaskingstrategy. Morespecifically,we
applyarandommaskmtothelatentrepresentationz ,producingamaskedlatentrepresentation
0
z . Suchastrategyencouragesthemodeltoreconstructtheoriginalvideocontentfromincomplete
0
frames,whichisinasimilarspirittoMAE[19]. Notethatwhenonlythefirstlatentisavailable,it
degradestothesameasSVD;ifalllatentsaremaskedout,thisdegradestounconditionalgeneration.
Furthermore,doingsoalsobenefitsrecognitiontaskswhenlimitedvisualcluesareavailable. For
example,inscenarioswithlimitedbandwidthleadingtoreducedframerates,theabilityofvideo
3Training Process
Video Latent
Masked Latent
Figure2: Thepipelineofourproposedvideoprocessingmethod. Theinputvideoisfirstprocessed
byapretrainedencoderE toproducealatentrepresentationz ,thenundergoesdiffusiontogenerate
0
anoisylatentz˜ . Therandommaskmisusedtocreatethemaskedlatentz . Duringtraining,the
t 0
noisylatentisconcatenatedwiththemaskedlatentasconditionandfedintoaSpatial-TemporalUNet,
resultinginbothreconstructionandrecognitionoutputs. Thereconstructedlatentcanbedecodedby
thepretraineddecoderDtoproducethefinalgeneratedvideo.
framecomplementationenablesthemodeltobetterpredictandperceivecompletevideoinformation.
Inpractice,wesimulatesuchconditionsbyrandomlyerasinghalftoallvideoconditions,retaining
onaverageonlyaboutone-fourthoftheoriginalvideoinformation. Thistechniqueallowsthemodel
toeffectivelyfillinthemissinginformation,enhancingitsabilitytorecognizeandunderstandthe
videocontentdespitethereduceddataavailability.
Unifying generation and understanding. To unify generation with masked latents, GenRec
predicts pixel-level and semantic-level contents with the combination of the noisy latent z˜ and
i
themaskedlatentz gainedbytheaforementionedlatentdiffusionandlatentmasking. Thetwo
0
latentsarechannel-wiseconcatenated[z˜,z ]andarefedintoaSpatial-TemporalUNet,together
i 0
with features from observed frames, to learn spatial and temporal representations, following [1].
TheweightsoftheUNetareinitializedfrom[1]toobtainspatialandtemporalpriors,learnedon
large-scalevideodatasets.
For the generation task, the UNet aims to reconstruct the original latent representation from the
combinednoisyandmaskedinputs. RepresentingUNetasthemappingfunctionF ,itsgoalisto
θ
predictcleanlatent,which,accordingtotheEDMframework,takestheformofarepresentation
mappingasfollows:
D (z˜;z ,σ)=c (σ)z˜ +c (σ)F ([c (σ)z˜,z ]), (5)
θ i 0 skip i out θ in i 0
inwhichwesetthesameskipconnectionc ,scalingfactorc andc as[1].
skip out in
Fortherecognitiontask,webreakdowntheUNetmappingfunctionasF =F ·F . Andwe
tail head
considerF ([c
(σ)z˜,z])∈RT×h′ ×w′ ×D′
asthecompactvideorepresentationextractedfrom
head in
theintermediatelayeroftheUNetmodel,whichisthenfedintotheclassifierheadϕ ,consistingof
θ
anattentivepoolerandafullyconnectedlayertopredictvideocategories:
yˆ =ϕ(F ([c (σ)z˜,z ])). (6)
head in i 0
3.2 Optimization
WetrainGenRecwithbothgenerationandclassificationobjectives,encouragingthemodeltolearn
high-qualityvideogenerationandaccuratevideounderstanding.
ThegenerativelossusesaL2losstomeasurethedifferencebetweentheoriginallatentrepresentation
andthereconstructedoutputproducedbytheUNet,andisdefinedas:
L (z ,z˜,z ;σ)=λ(σ)∥D (z˜;z ,σ)−z ∥2, (7)
G 0 i 0 θ i 0 0
4where D (z˜;z ,σ) is the denoised output mentioned in Equation (5), and λ(σ) is a weighting
θ i 0
functionbasedonthenoiselevelσ referringto[1,22]. Whiletheclassificationlossusesacross-
entropy loss to measure the discrepancy between the true labels and the predicted labels, and is
definedas:
(cid:88)
L (y,yˆ)=− y log(yˆ ), (8)
D i i
i
where y denotes the ground truth labels, and yˆ represents the predicted labels referring to Equa-
tionEquation(6).
Tobalancethelearningofgenerativeandrecognitiontasks,wesetabalancingweightγ tocontrol
therelativeimportanceofeachlossintheoverallobjectivefunction. ThetotallossLisgivenby:
L=L +γL , (9)
D G
3.3 InferenceforDifferentDownstreamTasks
Withtheabovetrainingstrategies,wenowintroducehowGenReccanflexiblysupportdifferenttypes
ofgenerationandrecognitiontasks.
Videogenerationconditionedonframes. Oncetrained,GenRecisabletogeneratehigh-quality
videosconditionedonanarbitrarynumberofgivenframes,thankstothelatentmaskingstrategy.
Particularly,followingtheEDMstochasticsamplerframeworkandEquation(2),GenReciteratively
denoisesthevideoconditionedonthemaskedlatentz ,asshownbelow:
0
dzˆ
z =zˆ +ϵ (zˆ ;z )=zˆ +(t −tˆ) i (10)
i−1 i θ i 0 i i−1 i dtˆ
i
D (zˆ ;z ,σ)−zˆ
=zˆ +(t −tˆ)(−1) θ i 0 i, (11)
i i−1 i tˆ
i
wherezˆ isderivedfromz˜ addingaperturbation. Withaniterativelydenoisingprocess,wecan
i i
finallyobtainthedenoisedvideolatentz whichcanbedecodedasacompletevideo.
0
Videogenerationconditionedonclasses. Whenthenumberofvisibleframesisextremelylimited,
themotiontrajectorybecomesunpredictableandthusitwouldbehardtomakeareliableprediction
ofthefuture. Tomitigatethisissue,GenRecsupportsaddingcategoryinformationtoguidevideo
generationintheexpecteddesireddirection.
Formally,wesimplifyEquation(11)withEquation(4),andobtain:
D (zˆ ;z ,σ)−zˆ
ϵ (zˆ ;z )=(t −tˆ)(−1) θ i 0 i (12)
θ i 0 i−1 i tˆ
i
=(−1)(t −tˆ)tˆ∇ logp (zˆ ). (13)
i−1 i i zˆi θ i
Wecannowreplacetheoriginalscorefunctionwithp(zˆ )p(y|zˆ ),inwhichydenotestheconditional
i i
class,togettheconditionalversionofresidual,denotedasϵ∗(zˆ ;z ):
θ i 0
ϵ∗(zˆ ;z )=(−1)(t −tˆ)tˆ∇ logp(zˆ )p(y|zˆ ) (14)
θ i 0 i−1 i i zˆi i i
=(−1)(t −tˆ)tˆ[∇ logp(zˆ )+∇ logp(y|zˆ )] (15)
i−1 i i zˆi i zˆi i
=ϵ (zˆ ;z )−(t −tˆ)tˆ∇ logp(y|zˆ ) (16)
θ i 0 i−1 i i zˆi i
Consideringthescalingfactorofzˆ : c (σ)= √ 1 (following[22],andσ =t ),thatwould
i in i i
σ2+σdata
pre-scaletheinputasc(z )=c (t )·z beforemodelprocessing,theformulationcanbefurther
i in i i
transferredas:
(t −tˆ)tˆ
ϵ∗(zˆ ;z )=ϵ (zˆ ;z )− i−1 i i ∇ logp(y|c(zˆ )) (17)
θ i 0 θ i 0 (cid:113)
tˆ2
+σ
c(zˆi) i
i data
Following[11],wesharpenthedistributionofp(y|z)bymultiplyingascalingfactors>1,shownas
s·∇ logp(y|z)=∇ log 1p(y|z)swhereZ isanarbitraryconstant. Largerscalingvaluewould
z z Z
bringmoreattentiontothetargetcategory. Here,p(y|c(zˆ ))comesfromtheclassificationbranchin
i
GenRec. Finally,wecanusethesameEDMsamplingprocedurewiththederivedclassinformation
togeneratesamples.
5Standardvideorecognition. BasedonEquation(6),GenReccandotheclassicalvideorecognition
bysettingconstantno-mask,andthusz isreplacedbyz andthepredictionfollows:
0 0
yˆ =ϕ(F ([c (σ)z˜,z ])). (18)
head in i 0
Videorecognitionwithpartiallyobservedframes. BasedonEquation(6),GenReccanbeapplied
tovideorecognitionwithpartiallyobservedframes,e.g.,earlyactionpredictionthataimstopredict
futureeventsbasedontheinitialframes,sparsevideorecognitionwherevideosaresparselyencoded
andtransmittedduetobandwidthlimitations.Bymaskingtheinvisibleframestogetz˜,andreplacing
i
thenoisylatentwithrandomnoise∼obeyingGaussiandistribution,GenReccandotheprediction
forpartiallyvisiblevideos,following:
yˆ =ϕ(F ([∼,z ])). (19)
head 0
4 Experiments
4.1 ExperimentalSetup
Datasets. In our experiments, we use the following four datasets: Something-Something V2
(SSV2)[17],Kinetics-400(K400)[24],UCF-101[35]andEpic-Kitchen-100(EK-100)[10]. SSV2
datasetisdesignedforfine-grainedactionrecognitionanditcontains174actionclasses,220,847
shortvideoclipswithanaveragedurationof4seconds. K400contains400actionclasses,306,245
videoclipswithanaveragedurationof10seconds. TheUCF-101datasetcomprises13,320videos
from101actioncategoriesandiswidelyutilizedforhumanactionrecognition. TheEK-100dataset
focusesonegocentricvision. Itcontainsatotalof90,000annotatedactionsegments,encompassing
97verbclassesand300nounclasses.
Evaluationprotocols. GenRecperformsbothgenerationandrecognitiontasks. Forgeneration,
weusetheFréchetVideoDistance(FVD)[41]metrictoassessthequalityofthegeneratedvideos.
A lower FVD score indicates higher fidelity and realism. For recognition, we measure the top-1
accuracythatreflectstheportionofcorrectlyclassifiedvideos. Wevalidateourmodelperformance
informalvideorecognition,partialvideorecognition,class-conditionedimage-to-videogeneration
andframecompletionwiththeabovemetrics.
Implementationdetails. Weinitiallysetthelearningrateto1.0×10−5 andsetthetotalbatch
sizeas32. Onlygenerationlosswillberetainedformodeladaptationonspecificdatasets. Wetrain
200kstepsonEK-100andUCF,and300kstepsonSSV2andK400,respectively. Subsequently,we
finetuneGenRecwithbothgenerationandrecognitionlosses. Thelearningrateissetto1.25×10−5
anddecayedto2.5×10−7usingacosinedecayscheduler.Wewarmupmodelswith5epochs,during
whichthelearningrateisinitiallysetas2.5×10−7andlinearlyincreasestotheinitiallearningrate
1.25×10−5. Thelossbalanceratioγ issetto10,andthelearningratefortheclassifierheadisten
timeshigherthanthebaselearningrate. Wedropouttheconditions10%ofthetimeforsupporting
classifier-freeguidance[20],andwefinetuneonK400for40epochsand30epochsonotherdatasets.
Thetrainingisexecutedon8A100sandeachcontainsabatchof8samples. Wesample16frames
foreachvideo.
4.2 MainResults
Comparisontostate-of-the-artinvideorecognitionandvideogeneration. Wecomparewith
state-of-the-artmethodsintermsoftheirrecognitionaccuracyandgenerationquality. Theresultsare
summarizedinTable1. Thefirsttwoblocksofthetablepresentscurrentadvancedvideorecognition
models, while the third block demonstrates the performance of the diffusion-based class-guided
image-to-videogeneration.
Asshowninthetable,GenRecachievesoptimalresultsorperformsonparwiththestate-of-the-art
approaches. In terms of video recognition, GenRec achieves 75.8% accuracy on SSV2 dataset,
surpassing the majority of current state-of-the-art methods. On K400, GenRec achieves 87.2%
accuracy, which is on par with the performance of MVD-H (87.2%) and Hiera (87.3%, 87.8%),
andsurpassesotheradvancedmethods. Theseresultsindicatetheeffectivenessofourapproachin
videorecognition. Inaddition,GenRecshowsaslightperformancegapcomparedtothemethods
6Table1: PerformanceofVideoRecognitionandGenerationMethods. Weevaluateonvideorecog-
nition and class-conditioned image-to-video generation tasks. SEER† predicts 16 frames, while
otherspredict12frames. Top-1accuracyandFVDscoresarereported. BaselineIadaptsSVDto
datasetswithgenerativefine-tuningandthenusesattentive-probingforclassification. BaselineII
fullyfinetunesSVDwithclassificationsupervisiononlyintraditionalclassificationframework.
ClassificationAcc(↑) GenerationFVD(↓)
Method Resolution Param. SSV2 K400 SSV2 EK-100
w/omulti-modalalign.
VideoMAE-L[40] 224×224 305M 74.3 85.2 - -
VideoMAE-H[40] 224×224 633M - 86.6 - -
OmniMAE-H[16] 224×224 650M 75.5 85.4 - -
MVD-H[44] 224×224 633M 77.3 87.2 - -
Hiera-L[32] 224×224 214M 75.1 87.3 - -
Hiera-H[32] 224×224 673M - 87.8 - -
MaskFeat-L[47] 312×312 218M 75.0 86.4 - -
w/multi-modalalign.
InternVideo[45] 224×224 1.3B 77.2 91.1 - -
InternVideo2[46] 224×224 6B 77.4 92.1 - -
OmniVec[36] - - 85.4 91.1 - -
OmniVec-2[37] - - 86.1 93.6 - -
TATS[14] 128×128 - - - 428.1 920.0
MCVD[42] 256×256 >3.5B - - 1407 4804
SimVP[13] 64×64 - - - 537.2 1991
VideoFusion[28] 256×256 1.8B - - 163.2 349.9
Tune-A-Video[49] 256×256 >860M 291.4 365.0
SEER[18] 256×256 >860M - - 112.9 271.4
SEER†[18] 256×256 >860M - - 355.4 -
BaselineI 256×256 2.1B 63.7 82.0 50.3 53.6
BaslineII 256×256 1.9B 75.9 86.6 - -
GenRec 256×256 2.1B 75.8 87.2 46.5 49.3
inthesecondblock. Itisimportanttonotethattheseadvancedmethodsbenefitsignificantlyfrom
pretraining on large-scale multimodal alignment datasets, which provide extensive cross-modal
supervisionthatenhancestheirabilitytocapturesemanticrelationshipsacrossvideoframes.
Wefurtherconstructtwostrongbaselines. BaselineIadaptsSVDtotherespectivedatasetthrough
generativefine-tuning,followedbyattentive-probingforclassification,wherethebackboneisfrozen
andallframesareusedasinput. BaselineIIinvolvesfullyfine-tuningtheoriginalSVDmodelwith
classificationsupervisiononly,ensuringthatallframesarevisibleduringtraining. Comparedwith
them,GenRecperformsonparorbetter. GenRecperformsgoodinsupportingnotonlyclassification
butalsogeneration,demonstratingitscomprehensivecapabilityinhandlingbothtaskseffectively.
Intermsofvideogeneration,weevaluatethemodelonclass-conditionedimage-to-videogeneration
taskfollowing[18]. ComparingtheFVDscoresofSEER:112.9andSEER†:355.4,itcanbeinferred
thatgeneratinglongervideoswith16framesismoredifficultthangenerating12frames. GenRec
generates videos with 16 frames and achieves much lower FVD scores than the other methods,
demonstratingtheeffectivenessofourapproachinvideogeneration.
Itisworthhighlightingthat,currentresearchalwaystreatsvideorecognitionandgenerationtasks
inaseparatemanner,andmostoftheadvancedmethodsfocusprimarilyoneitherrecognitionor
generationtasks. Forinstance,SEERmethodexcelsinclass-conditionedimage-to-videogeneration,
butlackstheabilitytodovideorecognition. Whilecurrentresearchonrepresentationlearning,shown
asthefirstandsecondblocksinTable1,lackstheabilitytodovideogenerationtasks. Incontrast,
GenRecnotonlyunifiesthesetasks,butalsoachievescompetitiveresultscomparedtothespecialized
methods.
Comparison to state-of-the-art in video recognition with limited frames. GenRec supports
videorecognitionwhenonlypartialframescanbeobserved. WeevaluatethiscapabilityontheSSV2
andEK-100datasets. Ourevaluationincludestwotasks: anearlypredictiontask,wherethemodel
7Table 2: Early action prediction and limited interpolation problem on Something-Something V2
dataset, with one temporal crop. ρ denotes the visible ratio according to the whole video. Acc
denotestothetop-1accuracy. Ratiometricrepresentthepercentageofmaximumperformancethat
themodelcanmaintainatvariousframerates. The’w/oG’experimentreferstotheresultsobtained
byremovingthegenerativesupervisionfromourmethod.
EarlyActionPrediction(ρ) LimitedInter.Frames
Method Metric 0.1 0.3 0.5 0.7 1.0 2fs 3fs 4fs 16fs
Accuracy 20.5 28.6 41.2 47.1 66.3 - - - -
TemPr[38]
Retention 30.9% 43.1% 62.1% 71.3% 100% - - - -
Accuracy - - - - - 34.2 54.3 64.4 75.0
MVD[44]
Retention - - - - - 45.6% 72.4% 85.9% 100%
Accuracy 26.9 39.8 55.6 70.2 75.0 53.6 65.0 68.8 75.0
MVD†[44]
Retention 35.9% 53.1% 74.1% 93.2% 100% 71.5% 86.7% 91.7% 100%
w/oG Accuracy 27.3↓1.6 39.6↓2.3 56.3↓1.4 71.6↓0.8 75.0↓0.3 53.5↓2.2 65.8↓1.5 69.8↓1.0 75.0↓0.3
Accuracy 28.9 41.9 57.7 72.4 75.3 55.7 67.3 70.8 75.3
GenRec
Retention 38.4% 55.6% 76.6% 96.1% 100% 74.0% 89.4% 94.0% 100%
Figure3: EarlyactionpredictiononEK-100andUCF-100datasets,withonetemporalcrop.
hasaccessonlytopreviouscontinuousframesfollowingthesettingof[38],andarecognitiontask
wherevideosaresparselysampled,andthemodelisexpectedtomakecorrectpredictions. Forfair
comparisons, we construct two strong baselines. We first apply MVD [44] to directly deal with
therecognitiontaskbyconstructingadensevideothroughnearestneighborinterpolation. Wealso
construct another baseline similar to our training pipeline, where we apply frame dropout in the
trainingprocessofMVD[44]forbetterfittingontaskwithpartialframes,andisnamedasMVD†.
Inallsettings,thenumberoffullyobservedframesis16.
Table2showstheresultsunderthesesettings,inwhichρdenotesthevisibleratio(thereareatotal
of16frames). Intheearlyactionpredictiontask,GenRecachievesthehighestaccuracyandratio
metricsatallobservationlevels. Notably,GenRecandMVD†exhibitsimilarperformancewhen
all frames are observed, but as the number of observed frames decreases, GenRec demonstrates
higher accuracy. GenRec also shows superior performance when videos are sparsely sampled,
maintaininghighaccuracyevenwithfewerobservedframes(e.g.,55.7%for2framesand70.8%for
4frames),indicatingitsrobustnessinhandlingsparsedata. Moreover,wecomputetheratiometric
representingthepercentageofmaximumperformancethatthemodelcanmaintainatvariousframe
rates,mitigatingtheunfairnesscausedbydifferentbackbonenetworks. Inthisscenario,GenRecstill
achievesthebestperformance.
Wefurtherinvestigatethecontributionsofgenerationsupervisionforrecognition. Asseenin Table2,
removinggenerationsupervisionresultsinnoticeableperformancedegradationacrossvarioustasks,
especiallywhenthenumberofvisibleframesgetless. Forexample,intheearlypredictiontask,the
accuracydecreasesby0.3%atρ=1.0andby2.3%atρ=0.3. Theseresultssuggestthatgeneration
supervisionisessentialformaintaininghighperformance,particularlywhenthemodelhastomake
predictionswithlimitedvisualinformation. Byincorporatinggenerationsupervision,themodelcan
betterhandlescenarioswithincompletedata,improvingrobustnessandaccuracy.
8Table3: Relationbetweengenerationandrecognition. Table4: ChoiceofUNetlay-
ersforfeatureextraction.
EarlyFrames LimitedInter.Frames
Method Metric 2fs 5fs 8fs 12fs 2fs 3fs 4fs UpIndex 1 2 3
Acc↑ 28.9 41.9 57.7 72.4 55.7 67.3 70.8 SSV2 71.8 75.8 75.2
GenRec
FVD↓ 57.8 44.0 30.3 16.6 46.7 31.7 24.3
Table5: Ablationstudyondifferentmaskingstrategies.
Expectationof SSV2
Method maskingratio Acc(↑) FVD(↓)
75%(ourchoice) 75.8 46.5
GenRec
50% +0.3 +0.5
87.5% -0.9 -0.3
We also evaluate the early action prediction on EK-100 and UCF-101. EK-100 is a temporally
sensitivedatasetsimilartoSSV2,demandingintermsofthemodel’stemporalmodelingcapability,
whileUCF-101demandsmoreonappearancemodeling. Weconductearlypredictionevaluation
on them to further reveal the robustness of our GenRec. As shown in Figure 3, GenRec clearly
outperforms TemPr and MVD†. In particular, the improvement becomes more significant as the
numberofobservedframesdecreases. MoreevaluationresultscanbeseeninAppendixA.1.
TheseresultscollectivelydemonstratethatGenReceffectivelyhandlesmissingvideoframes. The
robustnessandhighaccuracyofGenRecacrossdifferentdatasetsandobservationratioshighlightits
potentialforreal-worldapplicationswherevideodatamightbeincompleteorsparselysampled.
Therelationshipsbetweengenerationandrecognition. Wefurtherinvestigatetheconsistency
betweenvideogenerationandrecognition,asshowninTable3.Weevaluatetheperformanceofvideo
recognitionandgenerationwithlimitedframesandfindthattherecognitionaccuracynotonlydepends
onthenumberofvisibleframesbutalsosignificantlyonthelocationoftheseframes. Interestingly,
uniformsamplingappearstofacilitatevideorecognitionbetterthandensesamplingfromthevideo
prefix. Specifically, with the same number of frames, early prediction consistently shows lower
accuracycomparedtouniformlysampledframes(e.g.,28.9%vs. 55.7%with2frames)andworse
FVDscores(e.g.,57.8vs. 46.7with2frames). Whenonlythreeinterpolatedframesarevisible,the
31.7FVDscoreiscomparabletothatofaneight-frameprefix(30.3),whileachievingmuchhigher
recognitionaccuracy. Theseresultshighlighttheimportanceofcompletestateobservationforaction
recognitionandalsosuggestthatvideogenerationperformancecanpotentiallyreflecttaskdifficulty.
ChoiceofUNetlayers. AsdescribedinSection3,theUNetmappingfunctionF isdecoupledinto
F ·F ,whereF servesasthefeatureextractorforvideorecognition. OurUNetmodel
tail head head
contains4mainup-samplingblocks. Weinvestigatewhichoneisbestsuitedforrecognition. As
showninTable4,usingthesecondup-samplingblock(UpIndex2)yieldsthebestperformancewith
anaccuracyof75.8%. Thethirdblock(UpIndex3)followedwith75.2%,whilethefirstblock(Up
Index1)hasthelowestaccuracy. Assuch,wechoosethesecondblockforfeatureextraction.
Exploretheinfluenceofthemaskingstrategy. Wealsoconductanablationstudyonthemasking
schemesusingdifferentexpectedmaskingratios, asshowninTable5. Theresultsshowthatthe
FVDscoresremainsimilaracrossdifferentratios,andalargermaskingratiomightbebeneficial
forgeneration,asitcloselyresemblesourclass-conditionedframepredictionscenariowithoneor
twogivenframes. However,anexcessivelylargemaskingratio(87.5%)negativelyimpactsaction
recognitionaccuracy,leadingtoa0.9%decreasecomparedtoourselectedratio.
5 RelatedWork
Videodiffusionmodelsforgeneration. Thegreatsuccessofdiffusionmodelsinimagegeneration
hasledtorapidadvancementsinvideogeneration,includingtext-to-videogeneration[15,2,51],
image&text-to-video generation [53, 18, 21], and video editing [4, 5, 26, 29, 12]. Many current
9works[51,18]adaptsthediffusionmodelsfromimagestovideosbyincorporatingtemporalconvolu-
tionsandattentionmechanisms. Onetypicalandexcellentwork,StableVideoDiffusion[1],follows
theabovedescriptionandhasprovidedvaluablefoundationsforgeneratinghigh-quality,diverse,and
temporallyconsistentvideos. Differentfromthepreviouswork,inourpaper,wepursuenotonlythe
qualityofgeneration,butalsotheunityofmodelgenerationcapabilityandclassificationability.
Diffusionmodelsforvisualunderstanding. Recently,researchersstarttouncoverthesignificance
ofdiffusionmodelsfordiscriminationtasks. Anotableapproachinvolvesutilizingpretrainedvisual
diffusionmodelsforvariousdownstreamtasks,suchasimagesegmentation[52]andvisualcontent
correspondence[39].Additionally,somestudiestreatdiffusionlearningasaself-supervisedmethodto
acquirevaluablefeaturerepresentations[8]. However,mostcurrentworkseitherusestablediffusion
networks as pretrained backbones for downstream tasks or completely destroy their generative
capabilities. Consequently,thepotentialbenefitsofintegratinggenerationandclassificationabilities
intoasinglemodelremainunder-explored,whichistheprimaryfocusofourpaper.
6 Conclusion
Inthiswork,wepresentedGenRec,aunifiedvideodiffusionmodelthatenablesjointoptimization
for both video generation and recognition. GenRec exploits the significant temporal modeling
powerembeddedinthediffusionmodel,allowingformutualreinforcementbetweengenerationand
recognitiontasks. ExtensiveexperimentswereconductedtoevaluatetheperformanceofGenRec,
demonstrateourapproachcontainsstronggenerationandrecognitioncapabilitiesatthesametime
indifferentkindsofscenarios,includingnormalorpartialvideorecognition,videocompletionand
class-conditioned image-to-video generation. Our findings highlight the potential of combining
generationandclassificationtaskswithinasingleunifiedmodel,providingvaluableinsightsintothe
developmentofmoresophisticatedandversatilevideoanalysismodels. Futureworkwillfocuson
furtherrefiningthisintegrationandexploringitsapplicationsacrossvariousreal-worldscenarios.
References
[1] AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian,Do-
minikLorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal. Stablevideodiffusion:
Scalinglatentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023.
[2] AndreasBlattmann, RobinRombach, HuanLing, TimDockhorn, SeungWookKim, Sanja
Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent
diffusionmodels. InCVPR,2023.
[3] YuCao,DanielBarrett,AndreiBarbu,SiddharthNarayanaswamy,HaonanYu,AaronMichaux,
YueweiLin,SvenDickinson,JeffreyMarkSiskind,andSongWang.Recognizehumanactivities
frompartiallyobservedvideos. InCVPR,2013.
[4] DuyguCeylan,Chun-HaoPHuang,andNiloyJMitra. Pix2video: Videoeditingusingimage
diffusion. InICCV,2023.
[5] WenhaoChai,XunGuo,GaoangWang,andYanLu. Stablevideo: Text-drivenconsistency-
awarediffusionvideoediting. InProceedingsoftheIEEE/CVFInternationalConferenceon
ComputerVision,pages23040–23050,2023.
[6] HuanranChen,YinpengDong,ZhengyiWang,XiaoYang,ChengqiDuan,HangSu,andJun
Zhu. Robustclassificationviaasinglediffusionmodel. arXivpreprintarXiv:2305.15241,2023.
[7] ShoufaChen,PeizeSun,YibingSong,andPingLuo. Diffusiondet: Diffusionmodelforobject
detection. InICCV,2023.
[8] XinleiChen,ZhuangLiu,SainingXie,andKaimingHe. Deconstructingdenoisingdiffusion
modelsforself-supervisedlearning. arXivpreprintarXiv:2401.14404,2024.
[9] KevinClarkandPriyankJaini. Text-to-imagediffusionmodelsarezeroshotclassifiers. In
NeurIPS,2024.
10[10] DimaDamen,HazelDoughty,GiovanniMariaFarinella,AntoninoFurnari,EvangelosKazakos,
JianMa,DavideMoltisanti,JonathanMunro,TobyPerrett,WillPrice,etal.Rescalingegocentric
vision: Collection,pipelineandchallengesforepic-kitchens-100. IJCV,2022.
[11] PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. In
NeurIPS,2021.
[12] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis
Germanidis. Structureandcontent-guidedvideosynthesiswithdiffusionmodels. InICCV,
2023.
[13] Zhangyang Gao, Cheng Tan, Lirong Wu, and Stan Z Li. Simvp: Simpler yet better video
prediction. InCVPR,2022.
[14] SongweiGe,ThomasHayes,HarryYang,XiYin,GuanPang,DavidJacobs,Jia-BinHuang,and
DeviParikh. Longvideogenerationwithtime-agnosticvqganandtime-sensitivetransformer.
InECCV,2022.
[15] SongweiGe, SeungjunNah, GuilinLiu, TylerPoon, AndrewTao, BryanCatanzaro, David
Jacobs,Jia-BinHuang,Ming-YuLiu,andYogeshBalaji. Preserveyourowncorrelation: A
noisepriorforvideodiffusionmodels. InICCV,2023.
[16] RohitGirdhar,AlaaeldinEl-Nouby,MannatSingh,KalyanVasudevAlwala,ArmandJoulin,
andIshanMisra. Omnimae: Singlemodelmaskedpretrainingonimagesandvideos. InCVPR,
2023.
[17] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne
Westphal,HeunaKim,ValentinHaenel,IngoFruend,PeterYianilos,MoritzMueller-Freitag,
etal. The"somethingsomething"videodatabaseforlearningandevaluatingvisualcommon
sense. InICCV,2017.
[18] XianfanGu,ChuanWen,WeiruiYe,JiamingSong,andYangGao. Seer: Languageinstructed
videopredictionwithlatentdiffusionmodels. arXivpreprintarXiv:2303.14897,2023.
[19] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDollár,andRossGirshick. Masked
autoencodersarescalablevisionlearners. InCVPR,2022.
[20] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598,2022.
[21] TobiasHöppe,ArashMehrjou,StefanBauer,DidrikNielsen,andAndreaDittadi. Diffusion
modelsforvideopredictionandinfilling. arXivpreprintarXiv:2206.07696,2022.
[22] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of
diffusion-basedgenerativemodels. InNeurIPS,2022.
[23] BahjatKawar,ShiranZada,OranLang,OmerTov,HuiwenChang,TaliDekel,InbarMosseri,
andMichalIrani. Imagic: Text-basedrealimageeditingwithdiffusionmodels. InCVPR,2023.
[24] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijaya-
narasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human
actionvideodataset. arXivpreprintarXiv:1705.06950,2017.
[25] TianLan,Tsung-ChuanChen,andSilvioSavarese. Ahierarchicalrepresentationforfuture
action prediction. In Computer Vision–ECCV 2014: 13th European Conference, Zurich,
Switzerland,September6-12,2014,Proceedings,PartIII13,2014.
[26] Jun Hao Liew, Hanshu Yan, Jianfeng Zhang, Zhongcong Xu, and Jiashi Feng. Magicedit:
High-fidelityandtemporallycoherentvideoediting. arXivpreprintarXiv:2308.14749,2023.
[27] HaoyuLu,GuoxingYang,NanyiFei,YuqiHuo,ZhiwuLu,PingLuo,andMingyuDing. Vdt:
General-purposevideodiffusiontransformersviamaskmodeling. InTheTwelfthInternational
ConferenceonLearningRepresentations,2023.
11[28] ZhengxiongLuo,DayouChen,YingyaZhang,YanHuang,LiangWang,YujunShen,DeliZhao,
JingrenZhou,andTieniuTan. Videofusion: Decomposeddiffusionmodelsforhigh-quality
videogeneration. InCVPR,2023.
[29] EyalMolad,EliahuHorwitz,DaniValevski,AlexRavAcha,YossiMatias,YaelPritch,Yaniv
Leviathan, and Yedid Hoshen. Dreamix: Video diffusion models are general video editors.
arXivpreprintarXiv:2302.01329,2023.
[30] AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,Mark
Chen,andIlyaSutskever. Zero-shottext-to-imagegeneration. InICML,2021.
[31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer.
High-resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,2022.
[32] ChaitanyaRyali,Yuan-TingHu,DanielBolya,ChenWei,HaoqiFan,Po-YaoHuang,Vaibhav
Aggarwal,ArkabandhuChowdhury,OmidPoursaeed,JudyHoffman,etal.Hiera:Ahierarchical
visiontransformerwithoutthebells-and-whistles. InICML,2023.
[33] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyLDenton,Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistic
text-to-imagediffusionmodelswithdeeplanguageunderstanding. InNeurIPS,2022.
[34] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,and
BenPoole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXiv
preprintarXiv:2011.13456,2020.
[35] KhurramSoomro,AmirRoshanZamir,andMubarakShah. Ucf101: Adatasetof101human
actionsclassesfromvideosinthewild. arXivpreprintarXiv:1212.0402,2012.
[36] SiddharthSrivastavaandGauravSharma. Omnivec: Learningrobustrepresentationswithcross
modalsharing. InWACV,2024.
[37] SiddharthSrivastavaandGauravSharma. Omnivec2-anoveltransformerbasednetworkfor
largescalemultimodalandmultitasklearning. InCVPR,2024.
[38] AlexandrosStergiouandDimaDamen. Thewisdomofcrowds: Temporalprogressiveattention
forearlyactionprediction. InCVPR,2023.
[39] LumingTang,MenglinJia,QianqianWang,ChengPerngPhoo,andBharathHariharan. Emer-
gentcorrespondencefromimagediffusion. InNeurIPS,2023.
[40] ZhanTong,YibingSong,JueWang,andLiminWang. Videomae: Maskedautoencodersare
data-efficientlearnersforself-supervisedvideopre-training. InNeurIPS,2022.
[41] ThomasUnterthiner,SjoerdVanSteenkiste,KarolKurach,RaphaelMarinier,MarcinMichalski,
andSylvainGelly. Towardsaccurategenerativemodelsofvideo: Anewmetric&challenges.
arXivpreprintarXiv:1812.01717,2018.
[42] VikramVoleti,AlexiaJolicoeur-Martineau,andChrisPal. Mcvd-maskedconditionalvideo
diffusionforprediction,generation,andinterpolation. InNeurIPS,2022.
[43] JinglongWang,XiaweiLi,JingZhang,QingyuanXu,QinZhou,QianYu,LuSheng,andDong
Xu. Diffusionmodelissecretlyatraining-freeopenvocabularysemanticsegmenter. arXiv
preprintarXiv:2309.02773,2023.
[44] RuiWang,DongdongChen,ZuxuanWu,YinpengChen,XiyangDai,MengchenLiu,LuYuan,
and Yu-Gang Jiang. Masked video distillation: Rethinking masked feature modeling for
self-supervisedvideorepresentationlearning. InCVPR,2023.
[45] YiWang,KunchangLi,YizhuoLi,YinanHe,BingkunHuang,ZhiyuZhao,HongjieZhang,
JilanXu,YiLiu,ZunWang,etal. Internvideo: Generalvideofoundationmodelsviagenerative
anddiscriminativelearning. arXivpreprintarXiv:2212.03191,2022.
12[46] YiWang, KunchangLi, XinhaoLi, JiashuoYu, YinanHe, GuoChen, BaoqiPei, Rongkun
Zheng,JilanXu,ZunWang,etal.Internvideo2:Scalingvideofoundationmodelsformultimodal
videounderstanding. arXivpreprintarXiv:2403.15377,2024.
[47] ChenWei,HaoqiFan,SainingXie,Chao-YuanWu,AlanYuille,andChristophFeichtenhofer.
Maskedfeaturepredictionforself-supervisedvisualpre-training. InCVPR,2022.
[48] ZejiaWeng,XitongYang,AngLi,ZuxuanWu,andYu-GangJiang. Open-vclip: Transforming
cliptoanopen-vocabularyvideomodelviainterpolatedweightoptimization. InICML,2023.
[49] JayZhangjieWu,YixiaoGe,XintaoWang,StanWeixianLei,YuchaoGu,YufeiShi,Wynne
Hsu,YingShan,XiaohuQie,andMikeZhengShou. Tune-a-video: One-shottuningofimage
diffusionmodelsfortext-to-videogeneration. InICCV,2023.
[50] ZuxuanWu,ZejiaWeng,WujianPeng,XitongYang,AngLi,LarrySDavis,andYu-Gang
Jiang. Buildinganopen-vocabularyvideoclipmodelwithbetterarchitectures,optimization
anddata. TPAMI,2024.
[51] ZhenXing,QiDai,HanHu,ZuxuanWu,andYu-GangJiang. Simda: Simplediffusionadapter
forefficientvideogeneration. arXivpreprintarXiv:2308.09710,2023.
[52] JiaruiXu,SifeiLiu,ArashVahdat,WonminByeon,XiaolongWang,andShaliniDeMello.
Open-vocabularypanopticsegmentationwithtext-to-imagediffusionmodels. InCVPR,2023.
[53] XiYeandGuillaume-AlexandreBilodeau. Stdiff: Spatio-temporaldiffusionforcontinuous
stochasticvideoprediction. InAAAI,2024.
[54] HuiZhang,ZhengWang,ZuxuanWu,andYu-GangJiang. Diffusionad: Denoisingdiffusion
foranomalydetection. arXivpreprintarXiv:2303.08730,2023.
13A Appendix/supplementalmaterial
A.1 EarlyPredictiononEK-100andUCF-101
MoredetailedevaluationresultsofthevideorecognitionwithlimitedframesonEK-100andUCF-101
canbeseenhere.
Table6: EarlyactionpredictiononEK-100.
VerbObs.Ratio(ρ) NounObs.Ratio(ρ) ActionObs.Ratio(ρ)
Method
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
TemPr[38] 21.4 34.6 54.2 63.8 67.0 22.8 32.3 43.4 49.2 53.5 7.4 15.4 28.9 37.3 40.8
MVD†[44] 49.3 60.0 64.7 68.8 71.3 35.7 44.7 49.1 53.4 55.2 22.5 32.2 37.5 42.3 44.5
GenRec 55.8 63.6 67.9 71.7 73.1 40.1 47.8 52.0 55.3 56.7 28.1 36.1 40.8 45.0 46.6
Table7: EarlyactionpredictiononUCFdataset.
ObservationRatio(ρ)
Method
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
TemPr[38] 88.6 93.5 94.9 94.9 95.4 95.2 95.3 96.6 96.2
MVD†[44] 90.8 94.2 94.8 95.7 95.5 96.2 96.5 96.8 96.9
GenRec 94.4 96.0 96.5 96.6 96.9 97.4 97.8 97.9 97.9
A.2 CaseStudyforClass-ConditionedImage-to-VideoGenerationandforVideo
Interpolation
WeshowthegeneratedvisualizationoftheGenRec. Themodelcansupportvideogenerationgiven
variousnumbersofframes,aswellascategory-guidedgeneration. Weshowtwoofthemostdifficult
generativescenarios,whichare: (1)giventhefirstframeanddifferentactioncategoriestoguidethe
videogeneration,and(2)giventhestartandendframes,themodelisexpectedtocomplementthe
video. WealsocompareourmethodswithSEER[18]withcasespickedfromitsofficialwebsite. The
generationresultscanbeseeninFigure4, Figure5andFigure6.
A.3 LimitationsandBroaderImpacts
Limitations and Future Work The objective of our paper is to unify the tasks of generation
andrecognition,achievingorevensurpassingthestate-of-the-artexperimentalperformanceacross
varioustasks. However,ourmethodisbasedonfine-tuningapretrainedvideodiffusionmodel,using
morepretrainingdataandhavingalargernumberofparameterscomparedtopreviousmethods. This
isanissueweneedtoaddressinthefuture,andexploringthedistillationofawell-pretrainedvideo
diffusionmodelintoasmallermodelisaworthwhilefutureendeavor.
Broader Impacts The broader impact of the GenRec framework extends into various fields,
enhancingcapabilitiesincontentcreation,security,andaccessibility. Inthemediaindustry,itallows
fortheautomatedgenerationoftailored,high-qualityvideos,reducingproductioncostsandfostering
creativity. For surveillance, its robustness in limited information scenarios improves monitoring
effectiveness,particularlyinchallengingenvironments. Additionally,advancementsofGenRecin
videopredictioncanaidindevelopingassistivetechnologies,makingdigitalcontentmoreaccessible
andinteractive,particularlyforindividualswithvisualimpairments.
14GT Video:Covering something with something
Covering something with something
Pulling something from left to right
Pulling something from right to left
Lifting up one end of something, then letting it drop down
Lifting up one end of something, without letting it drop down
Figure4: Videogenerationcasestudy. Wegeneratevideosgiventhefirstframetogetherwiththe
classifierguidanceforvariouscategories.
15GT
Gen
GT
Gen
GT
Gen
GT
Gen
Figure5: Videogenerationcasestudy. Wegeneratevideosgiventhefirstframeandthelastframe.
16Action:Pushing something from left to right.
GT
SEER
Ours
Action:Covering something with something.
GT
SEER
Ours
Action:Dropping something in front of something
GT
SEER
Ours
Figure6: Videogenerationcasestudy. WecompareourmethodswithSEER[18]inthesettingof
generatingvideosgiventhefirstframetogetherwiththeclassifierguidance. Casesarepickedfrom
theofficialwebsiteofSEER[18].
17