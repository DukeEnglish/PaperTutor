ZHANGETAL.:UC-GS 1
Drone-assisted Road Gaussian Splatting
with Cross-view Uncertainty
SainingZhang1,2* 1InstituteforAIIndustryResearch(AIR),
SAINING001@e.ntu.edu.sg TsinghuaUniversity,
BaijunYe1,3* Beijing,China
yebaijun52@gmail.com 2CollegeofComputingandData
XiaoxueChen1 Science,
chenxx21@mails.tsinghua.edu.cn NanyangTechnologicalUniversity,
Singapore
YuantaoChen1
yuantaochen973@gmail.com
3IIIS,
TsinghuaUniversity,
ZongzhengZhang1
Beijing,China
zzongzheng0918@gmail.com
ChengPeng1,4
4SchoolofComputerScienceand
Technology,
1120211642@bit.edu.cn
BeijingInstituteofTechnology,
YongliangShi1
Beijing,China
shiyongliang@air.tsinghua.edu.cn
HaoZhao1†
zhaohao@air.tsinghua.edu.cn
Abstract
Robustandrealisticrenderingforlarge-scaleroadscenesisessentialinautonomous
drivingsimulation.Recently,3DGaussianSplatting(3D-GS)hasmadegroundbreaking
progressinneuralrendering,butthegeneralfidelityoflarge-scaleroadscenerenderings
isoftenlimitedbytheinputimagery,whichusuallyhasanarrowfieldofviewandfocuses
mainlyonthestreet-levellocalarea.Intuitively,thedatafromthedrone’sperspectivecan
provideacomplementaryviewpointforthedatafromthegroundvehicle’sperspective,
enhancing the completeness of scene reconstruction and rendering. However, training
naivelywithaerialandgroundimages,whichexhibitlargeviewdisparity,posesasignif-
icantconvergencechallengefor3D-GS,anddoesnotdemonstrateremarkableimprove-
mentsinperformanceonroadviews. Inordertoenhancethenovelviewsynthesisof
roadviewsandtoeffectivelyusetheaerialinformation,wedesignanuncertainty-aware
trainingmethodthatallowsaerialimagestoassistinthesynthesisofareaswhereground
images have poor learning outcomes instead of weighting all pixels equally in 3D-GS
traininglikepriorworkdid. Wearethefirsttointroducethecross-viewuncertaintyto
3D-GSbymatchingthecar-viewensemble-basedrenderinguncertaintytoaerialimages,
weightingthecontributionofeachpixeltothetrainingprocess.Additionally,tosystem-
aticallyquantifyevaluationmetrics,weassembleahigh-qualitysynthesizeddatasetcom-
prisingbothaerialandgroundimagesforroadscenes. Throughcomprehensiveresults,
*Equalcontribution
†Correspondingauthor
©2024.Thecopyrightofthisdocumentresideswithitsauthors.
Itmaybedistributedunchangedfreelyinprintorelectronicforms.
4202
guA
72
]VC.sc[
1v24251.8042:viXra2 ZHANGETAL.:UC-GS
Figure1:QualitativeresultsofourDrone-assistedRoadGaussianSplattingwithCross-view
Uncertainty and several baseline methods. The dataset is 1.6m test set of New York City.
Thequalityimprovementishighlightedbyboxes.
weshowthat: (1)Jointlytrainingaerialandgroundimageshelpsimproverepresenta-
tion ability of 3D-GS when test views are shifted and rotated, but performs poorly on
held-outroadviewtest. (2)Ourmethodreducestheweaknessofthejointtraining,and
out-performsotherbaselinesquantitativelyonbothheld-outtestsandscenesinvolving
viewshiftingandrotationonourdatasets.(3)Qualitatively,ourmethodshowsgreatim-
provementsintherenderingofroadscenedetails,asshowninFig.1.Thecodeanddata
forthisworkwillbereleasedathttps://github.com/SainingZhang/UC-GS.
1 Introduction
Autonomous driving simulation serves as a critical platform for scaling up to real-world
deployment. Realisticrenderingandnovelviewsynthesis(NVS)forlarge-scaleroadscenes
have become increasingly important in autonomous driving simulation, as they enable the
synthesisofhigh-qualitytrainingandtestingdataatasignificantlylowercostcomparedto
usingreal-worlddata. Thiscapabilityalsobenefitsawiderangeofapplications, including
digitalcities[14,47],virtualreality[43],andembodiedAI[27].
Recently,NeRF[21,25,38,40,41,52]hasgreatlyenhancedfidelityofNVSbyparam-
eterizing the 3D scene as implicit neural fields but suffers from slow rendering due to ex-
haustiveper-pixelraysamplingprocessespeciallyinlarge-scaleroadscenes. 3DGaussian
Splatting (3D-GS) [12] achieves real-time rendering by rasterizing the learnable Gaussian
primitives.
However, the rendering quality for both NeRF and 3D-GS is highly dependent on the
inputviews.Onthecontrary,currentroadscenedatasets,suchasKITTI[9]ornuScenes[4],
onlycontaincar-viewimages,whicharelimitedbythefieldofviewandfocusonthestreet-
level local areas. The most related work may be MatrixCity [15], which offers both aerial
andstreet-levelcityviews,buttheaerialimagery’shighaltitudelimitsitsabilitytocapture
fine-grainedroaddetails.Thismismatchinscaleandgranularitybetweenglobalaerialviews
andlocalstreetviewsmakesMatrixCityunsuitablefordrone-assistedroadscenesynthesis.
To address the aforementioned issues, we dedicate to establish a new paradigm for
drone-assisted road scene synthesis, aiming to overcome the limitations of input views
byintegratinganaerialperspectivetoprovideacomprehensiveglobalviewofroadscenes,
in contrast to the localized perspective obtained from ground-level vehicle cameras. The
aerialperspectivecanbecapturedusingdevicessuchasdrones.
Sinceitisdifficulttocapturewell-alignedviewsynthesisgroundtruthdatasetforevalua-
tioninrealworld,wefirstcreateasynthesizeddataset(Fig.2)comprisingaerial-groundim-
agerywithviewpointsthathavesimilarlevelsofinformationgranularityacrosslarge-scale
road scenes. For the aerial perspective, we simulate drone flight trajectories and behavior
patterns using AirSim [29]. For the ground-view images, we sample them to simulate the
perspectiveandfieldofviewfromonboardcamerasonvehicles.ZHANGETAL.:UC-GS 3
Figure2: Generalviewofthesynthesizeddataset. 5°dmeans5degreesdownward.
Ontheotherhand,thetrainingprocessof3D-GSleadsittoanimperfectmodelingwhen
it comes to significant different viewpoints, such as aerial and ground views. This limited
generalizationcapabilityisexacerbatedinourspecificsetting, makingsimultaneousfitting
ofaerialandgrounddatachallenging. Forinstance,theinherentambiguityinaerial-ground
dataand the informationoutsideco-viewregions diluteusefulinformation. Naivelyincor-
poratingaerialdataintothejointtrainingwithgrounddataadverselyaffectstheconvergence
oftheGaussianfieldandcompromisesrenderingqualityforNVSwhenshiftingandrotating
viewsinautonomousdrivingscenes.
To overcome this problem, we introduce a novel uncertainty-aware training paradigm
thatenablesmoreeffectiveuseofaerialimageryandguides3D-GStofocusonchallenging
areaswheregrounddataalonemaystruggle. Byexcludingirrelevantportionsinaerialper-
spectives, such as the upper floors of buildings, which are less related to street scenes, we
successfully mitigate the ambiguity and improve the fidelity of NVS such as view shifting
androtationonstreet.
Theuncertaintyisfirstcomputedwithintheground-viewimagespacethroughanensemble-
basedmethod,andthenprojectedtotheaerialspacetoassistthetrainingof3D-GS,which
is named as cross-view uncertainty as a new concept. Extensive experiments demonstrate
that our uncertainty-aware training paradigm for drone-assisted road scene synthesis out-
performsthenaivejointtrainingwithaerialandgroundimages,aswellastrainingwithonly
groundimages,bothquantitativelyandqualitatively. Ourmethodofferssignificantbenefits
forapplicationslikeautonomousdrivingsimulation. Tosummarize,thecontributionsofour
workinclude:
• We formalize the problem of drone-assisted road scene synthesis and craft a high-
qualityandappropriatedatasetforthisnewandimportantproblem;
• Weproposeanuncertainty-awaretrainingstrategyandarethefirsttodemonstratethat
cross-view uncertainty can facilitate a pixel-weighted training paradigm while prior
worksuseallpixelsofimagesequallyfor3DGaussians’training;
• Through extensive experiments and evaluations, we demonstrate notably improved
performanceonbothheld-outtestsandscenariosinvolvingviewshiftingandrotation
onroadscenesynthesis.4 ZHANGETAL.:UC-GS
2 Related Works
2.1 3DSceneRepresentation
Asthefoundationfor3Dcomputervision, various3Drepresentationshavebeenproposed
to depict real-world scenes such as point cloud-based representation [22, 45], voxel-based
representation[19,50],orimplicitrepresentation[21,26]. Amongtheimplicitrepresenta-
tion,NeRF[21]standsoutasagroundbreakingneuralrenderingmethodthatrepresents3D
scenes as continuous radiance fields parameterized by neural networks, taking coordinates
and viewing directions as inputs. With the rise of NeRF, many efforts have been made to
enhanceitsqualityandefficiency[1,2,3,5,7,17,42,44,48,51]. Recently,3DGaussian
splatting(3D-GS)[12]hasbeenproposedasanovel3Dscenerepresentation,utilizingaset
of 3D positions, opacity, anisotropic covariance, and spherical harmonic (SH) coefficients
torepresent a3Dscene. Compared withNeRF,3D-GS basedmethods[6, 18, 35, 46, 49],
showssuperiorperformanceinrenderingspeed,fidelity,andtrainingtime. Inthiswork,we
also leverage 3D-GS as the scene representation to resolve the problem of drone-assisted
roadscenesynthesis.
2.2 UncertaintyModeling
Modelinguncertaintyhasbeenalong-standingproblemindeeplearning. Earlyworksusu-
allyresolveuncertaintyestimationthroughBayesianNeuralNetwork(BNN)[23,24].How-
ever,thesemethodscanbecomputationallyexpensiveandchallengingtoimplement. Later,
dropout-based methods [8, 20, 37] have emerged as a computationally efficient alternative
thataddsdropoutduringinferencetoestimateuncertainty.Besides,ensemble-basedmethods
[13,16]havebeenproposedtomodeluncertaintybymergingthepredictionfrommultiple
independentlytrainedneuralnetworks. Asforthefieldof3Dscenerepresentation,aseries
ofworks[10,31,32,33]havefocusedonquantifyinguncertaintyinthepredictionofNeRF.
Forexample,S-NeRF[31]employaprobabilisticmodeltolearnasimpledistributionover
radiancefields,whileCF-NeRF[32]learnsadistributionoverpossibleradiancefieldswith
latent variable modeling and conditional normalizing flows. With the emergence of 3D-
GS, SGS [28] first addresses uncertainty modeling in 3D-GS, and integrates a variational
inference-based method with the rendering pipeline of 3D-GS. CG-SLAM [11] also intro-
ducetheuncertainty-aware3D-GStoSLAM.Inthiswork,weintroduceanovelcross-view
uncertaintytrainingparadigmtofacilitatethetrainingof3DGaussiansonroadscenes.
3 Method
Fig.3depictstheoverviewofourmethod. InSec.3.1,webrieflyintroducethebasicprin-
ciples of original 3D-GS. Next, we construct the first drone-assisted road scene dataset
in Sec. 3.2. Then, Sec. 3.3 illustrates how we model cross-view uncertainty through an
ensemble-based rendering paradigm and an uncertainty projection module. Finally, by in-
corporating the uncertainty map into the loss function, we can build an uncertainty-aware
trainingmodule,whichfacilitatesthetrainingof3D-GS(Sec.3.4).ZHANGETAL.:UC-GS 5
Figure3:OverviewofDrone-assistedRoadGaussianSplattingwithCross-viewUncertainty.
We first adopt an ensemble-based rendering uncertainty to quantify the learning outcomes
of 3D Gaussians on ground images. Next, the ground uncertainty is projected to the air to
buildthecross-viewuncertainty. Subsequently, weintroducethecross-viewuncertaintyto
thetrainingof3DGaussiansasweightforeachpixelofaerialimagesinthelossfunction,
togetherwiththeoriginalrenderinglossof3D-GSforgroundimages.
3.1 Preliminaries
3D-GS[12]representsa3Dscenebyasetofdifferentiable3DGaussians, whichcouldbe
efficientlyrenderedtoimagesthroughtile-basedrasterization.
Specifically, initialized by a bunch of Structure-from-Motion (SfM) points, each 3D
Gaussianisdefinedas:
G(x)=e− 21(x−µ)TΣ−1(x−µ), (1)
wherex∈R3×1 isarandom3Dpositioninthescene, µ ∈R3×1 standsforthemeanvector
of the 3D Gaussian, and Σ∈R3×3 refers to its covariance matrix. In order to maintain its
positivesemi-definite,ΣisfurtherformulatedasΣ=RSSTRT,whereR∈R3×3istherotation
matrixandS∈R3×3isthescalingmatrix.
TorendertheGaussiansintotheimagespace,eachpixel piscoloredbyα-blendingN
sortedGaussiansoverlapping pas:
N i−1
c(p)=∑cα ∏(1−α ), (2)
i i j
i=1 j=1
whereα iscalculatedbymultiplyingthe2DGuassianprojectedfrom3DGaussianGin p
j
withtheopacityofG,andc isthecolorofG.Throughthedifferentiabletile-basedrasterizer
i
technique, all attributes of the Gaussians could be learnable and optimized end-to-end via
trainingviewreconstruction.
In this work, we utilize Scaffold-GS [18] as our baseline, as it represents the SOTA
among 3D-GS based methods in road scene synthesis tasks. However, we posit that our
proposedstrategyholdspromiseforapplicationacrossother3D-GSbasedmethodsaswell.
3.2 Drone-assistedRoadSceneSynthesizedDataset
Crucialforautonomousdrivingsimulation,high-fidelityviewsynthesisforlargeroadscenes
is often hindered by poor road rendering due to reliance on limited car-view imagery. To6 ZHANGETAL.:UC-GS
(a) (b) (c)
Figure4: Resultsfortrainingwithgroundoraerialandgroundimagesonvariousmodels.
(G),(A+G)aretrainingwithgroundoraerialandgroundimages.
address this, we introduce drone-assisted road scene synthesis, leveraging aerial images as
anadditionalinputforabetterscenereconstruction.
Wepresentanewbenchmarkforassessingaerialimagesinlarge-scaleroadscenesyn-
thesis,featuringbothaerialandgroundposedimages. UsingUnrealEngine,wecreatetwo
high-fidelity scenes to simulate real-world road imagery. AirSim [29] controls drones and
vehicles for precise trajectory generation, simulating real scenarios. With the trajectories
of drones, we employ AirSim to simulate the camera perspectives and render correspond-
ing image data through the Unreal Engine. For ground-view imagery, we utilize vehicle
trajectories to generate forward-facing images. As shown in Fig. 2, to replicate real-world
drivingconditions,wecapturefront-viewgroundimagesatheightsof1.5mand1.8m,while
aerial-viewimagesarecollectedattheheight20mwiththeangleof60°downwardfromthe
front view (based on some tests). Additional test data at 1.6m and 1.9m heights evaluate
perspectiveimpact. Eachsceneincludesatrainingsetof315groundand351aerialimages,
andatestsetof36groundimages,aimingtosimulatediversedrivingscenariosforamore
representativebenchmarkdataset.
3.3 Cross-viewUncertaintyModeling
PreliminaryExperimentsandMotivation. Fromthecomparisonbetweenthedottedand
solid lines of different colors in Fig. 4 (a/b/c), it is clearly that jointly training aerial and
groundimagesmitigatesthedeclineinmetricsduringroadviewshiftingandrotationcom-
pared with merely training with ground images. However, aerial images do not enhance
the result on the held-out test of road scene synthesis, as shown by the point in the held-
out column of Fig. 4 (a/b/c). Weighting all pixels from aerial and ground images equally
while training will let aerial images have same synthesis priority as road views do for 3D
Gaussians. The areas that are non-overlapped with road scene and the areas that have lit-
tlecontributiontotheroadscenesynthesisintheaerialimagesnotonlyfailtoenhancethe
effectivenessofroadreconstructionbutalsoposemorechallengesto3DGaussians’conver-
gence. Thisleadstopoorrenderingqualityinthegroundperspectivewhenjointlytraining
aerialandgroundimages.
BaselineandImplementation. Inordertoenhancetherenderingresultofroadviews,we
attempttoquantifythecontributionofeachpixelintheaerialimagetotheroadscenesyn-
thesis. However,undertakingsuchataskisverychallenging. Wedecidetoapproachfroma
differentanglebyquantifyingthequalityofthelearningoutcomesofgroundimagestoin-ZHANGETAL.:UC-GS 7
Figure5: Thefirstrowshowsthevisualizationofthecross-viewuncertainty,andthesecond
rowshowsthecorrespondingaerialdata.
fertheweighttoeachpixelintheaerialimageduringGaussians’training. Toconveniently
and plausibly compute the learning outcomes, we adopt an ensemble-based rendering un-
certainty[13]paradigmtoquantifytheuncertaintyofeachpixelinthegroundimagery. To
be more specific, we train an ensemble of M gaussian splatting(GS)s initialised from the
structurefrommotion(SfM)generatedbygroundimagery. Byinterpretingtheensembleas
auniformly-weightedmixturemodel,themembers’predictionsarecombinedthroughaver-
aging,andthepredictiveuncertaintyisexpressedasthevarianceovertheindividualmember
predictions. WithanensembleofGSs,theexpectedcolorofpixel pinasceneis
1 M
µ (p)= ∑c (p). (3)
RGB k
M
k=1
The predictive uncertainty can be expressed as the variance over the individual member
predictions:
1 M
σ2 (p)= ∑(µ(p)−c (p))2. (4)
RGB M k
k=1
µ and σ2 can be calculated very easily by rendering the M individual RGB images
RGB RGB
and calculating the mean and variance directly in pixel space. Both will be 3-vectors over
theRGBcolourchannels.
Wecombinethevariancesfromthecolourchannelsintoasingleuncertaintyvalueby:
1
u(p)= ∑ log(σ2 (p)+1), (5)
3 RGB,(c)
c∈{RGB}
where σ2 (p) indicates the variance associated with colour channel c, log is the loga-
RGB,(c)
rithmictransformationtosmoothandtightenthevaluesforfurthernormalizationprocess.
Cross-view Uncertainty Projection. To project the uncertainty map from ground-view to
aerial-view, we test several methods. Neural field-based methods like NeRF and 3D-GS
are prone to overfitting, so neural fields trained with ground uncertainty maps are unable
to render high-quality uncertainty in the air. Besides, recently appeared end-to-end dense
stereo model—DUSt3R [39] has set SoTAs on many 3D tasks, which could be used as a
2D-2D pixel matcher between aerial and ground images. In this way, uncertainty maps
fromgroundareprojectedtoairthroughmatchesbetweengroundandaerialimages,andby
averagingtheuncertaintiesatpixelswithmultiplematches,webuildreasonablecross-view
uncertaintymapsfortraining. Thevisualizationofthecross-viewuncertaintymapisshown
inFig.5.8 ZHANGETAL.:UC-GS
3.4 Uncertainty-awareTraining
In this section, we elaborate on how we introduce the cross-view uncertainty map to the
training process. U (x) presents the uncertainty value on the pixel position x of the k-th
k
aerialimage. Firstofall,wenormalizealltheuncertaintytotherange(0,1)likethis:
1
U′=( U k )n , (6)
k max(U ...U )−min(U ...U )
1 M 1 M
where n refers to a hyperparameter for taking the n-th root, with the purpose of enhancing
theimpactofnon-zerovalues.
Then,weintroducetheuncertaintymaptothecolorandSSIMlossasaweightmap:
1 HW
L = ∑U′(x)|Cˆ(x)−C(x)|, (7)
color
HW
x=1
L =mean(U′(1.0−SSIM_MAP(Cˆ,C))), (8)
SSIM
whereCˆ(x)andCˆ representground-truthcolor,H andW standforheightandwidthofthe
image,andSSIM_MAPisthestructuralsimilarityoftheinferenceandtheground-truth.The
finallossfunctionisgivenby:
L=(1.0−λ )L +λ L +λ L , (9)
SSIM color SSIM SSIM vol vol
whereL isthevolumeregularizationusedin[18]toencouragestheneuralGaussiansto
vol
besmallwithminimaloverlapping.
Thelossfunctionachievesthecross-viewuncertainty-awaretrainingwhichweightsthe
effectsofeachpixelofaerialimagestobetterassistintheroadscenesynthesis.
4 Experiments
4.1 ExperimentalSetup
DatasetandMetrics. Inordertoensuretheauthenticityofthesimulationdata,weusetwo
realisticcityscenemodel,hereafterasNewYorkCity(NYC)andSanFrancisco(SF),from
Kyrylo Sibiriakov [34] and Tav Shande [30] to collect the data as mentioned in Sec. 3.2.
Inadditiontothe960×480aerialandgroundimages, wealsocollected1280×720(HD)
aerial images for experiments. All models are trained on 1.5m and 1.8m ground images,
respectively,andtestedattheviewpointofthefrontand5°downwardat0.1meterabovethe
groundlevel. Allresultsaremeasuredbythreemetrics: PSNR,SSIMandLPIPS.
BaselineandImplementation. Throughpreliminaryexperimentsamongseveralmethods,
Scaffold-GS[18]isselectedasthebaselinesinceitsoutstandingperformance. Allmethods
aretrainedfor900kiterations. Furthermore,werecordtheresultsofotherSoTAmethodsin
NVSlikeNerfacto[36],3D-GS[12]andMip-Splatting[49].
Forhyperparametersineq.(9),λ =0.2andλ =0.001asin[18]. Fornineq.(6),
SSIM vol
thedefaultvalueis6andwewilldiscussitinSec.4.3.ZHANGETAL.:UC-GS 9
4.2 MainResults
From preliminary experiments (Fig. 4), it could be easily concluded that when testing the
view shifting and rotation, the metrics of all methods decline. The inclusion of aerial im-
ages helps to slow down this trend compared to training the ground data along, indicating
thataerialimagescanprovidemoreperspective-richinformationtomaintaintherendering
ability of the neural field during the view shifting and rotation. However, in the held-out
viewpoints testing, training with aerial images performs no better than merely training on
groundimages,thusfailingtodemonstratethesuperiorityofaerialimagesintermsofview
shiftingandrotation.
Tab. 1 reports comprehensive results of road view synthesis on our datasets. After the
implementation of the cross-view uncertainty, the paradigm makes a great progress. The
averagegrowthofPSNRontheheld-outtestsetis0.68(NYC),and0.41(SF)comparedwith
SoTAmethodstrainingwithgroundimages. TheSSIMandLPIPSalsomakeasignificant
improvement. When shifting views, the PSNR is about 0.90 (NYC) and 0.80 (SF) more
thantrainingwithgroundimages. TheSSIMandLPIPSexhibitsimilaradvancementtrends.
Allourresultsout-performspreviousSOTAsolutionsandFig.1showstheimprovementof
ourmethodsoncertaindetailsofroadscene. Inaword,ourmethodnotonlyenhancesthe
representationofGSfromgroundperspectivesbutalsoimprovesthequalityofroadscenes
synthesisduringtheviewshiftingandrotation.
Testset Held-out View(+0.1m) View(+0.1m5°down)
Method/Metrics PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
Nerfacto(G)[36] 23.54 0.719 0.245 20.19 0.663 0.258 19.69 0.632 0.300
3D-GS(G)[12] 23.71 0.706 0.363 20.54 0.688 0.346 20.01 0.646 0.387
Mip-Splatting(G)[49] 25.35 0.779 0.302 20.51 0.710 0.302 20.03 0.668 0.350
Scaffold-GS(G)[18] 25.64 0.790 0.265 22.19 0.746 0.281 21.55 0.705 0.326
Scaffold-GS(A+G) 25.66 0.782 0.273 22.56 0.744 0.286 22.10 0.709 0.328
Scaffold-GS(A*+G) 25.68 0.784 0.274 22.91 0.751 0.284 22.38 0.715 0.326
Ours 26.32 0.802 0.244 23.11 0.766 0.258 22.49 0.725 0.303
(a)
Testset Held-out View(+0.1m) View(+0.1m5°down)
Method/Metrics PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
Nerfacto(G)[36] 23.82 0.631 0.344 23.38 0.618 0.344 22.77 0.587 0.421
3D-GS(G)[12] 23.22 0.630 0.449 21.96 0.597 0.457 21.64 0.564 0.492
Mip-Splatting(G)[49] 24.95 0.690 0.381 23.38 0.654 0.391 22.90 0.613 0.434
Scaffold-GS(G)[18] 25.16 0.697 0.375 24.37 0.675 0.386 23.69 0.637 0.423
Scaffold-GS(A+G) 24.98 0.691 0.383 24.63 0.683 0.384 23.97 0.647 0.421
Scaffold-GS(A*+G) 24.99 0.689 0.386 24.70 0.684 0.385 23.88 0.649 0.422
Ours 25.57 0.723 0.337 25.18 0.715 0.338 24.55 0.678 0.376
(b)
Table1: ResultsonNYC(a)andSF(b). A*isHDaerialimages. (G),(A+G)aretraining
withgroundoraerialandgroundimages.
4.3 Ablationstudies
EfficacyofCross-viewUncertainty. Comparedwithequallytrainingallaerialandground
images Tab. 1, the cross-view uncertainty-aware training achieves a 0.66 (NYC) and 0.59
(SF)increaseinPSNRonheld-outtestset, andabout0.47(NYC)and0.57(SF)whenthe
view shifting and rotation. Moreover, our method also reverses the adverse effects of the
joint training on SSIM and LPIPS, resulting in improvements in both metrics. It is also
very impressive that our method performs even better than using HD aerial data when the
viewshiftingandrotation. Thisreflectstheeffectiveutilizationofaerialdatainroadscene10 ZHANGETAL.:UC-GS
Figure 6: Rendering results for the ablation study of cross-view uncertainty. NYC: 1.6m
(row 1), 1.6m 5° down (row2); SF: 1.9m (row 3), 1.9m 5° down (row4). A* is HD aerial
images. (G),(A+G)aretrainingwithgroundoraerialandgroundimages.
synthesis. FromFig.6,itisclearthatourmethodnotonlycontributestotherenderingeffect
ofroadtexturesbutalsoenhancestheclarityofroadsideobstacles,lanemarkingsandground
signs,whichwillgreatlyaidinautonomousdrivingsimulation.
EfficacyofHyperparametern. Tab.2presentstheexperimentalresultsfordifferentvalues
ofnineq.(6). Whennissetto1(i.e.,non),thereisnosignificantimprovementinthemet-
ricscomparedtoequallytrainingonallaerialandgroundimages. However,asnincreases
to 2 or greater, the metrics improve with the increment of n, and the results become stable
whenn≥6. Thisindicatesthatwhenn≥6,thepotentialofaerialimageryisfullyrealized.
Testset Held-out View(+0.1m) View(+0.1m5°down)
n/Metrics PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
1 25.64 0.748 0.310 23.71 0.723 0.320 23.11 0.685 0.360
2 25.72 0.754 0.301 23.83 0.729 0.309 23.24 0.690 0.353
3 25.80 0.758 0.298 23.93 0.733 0.305 23.39 0.694 0.347
4 25.85 0.759 0.296 24.04 0.735 0.303 23.47 0.698 0.344
6 25.94 0.762 0.291 24.14 0.741 0.298 23.52 0.701 0.339
8 25.89 0.761 0.291 24.13 0.739 0.298 23.47 0.700 0.339
10 25.91 0.763 0.290 24.20 0.742 0.296 23.55 0.704 0.337
Table2: Theresultsfortheablationstudyofhyperparametern. nisfortakingthen-throot
totheuncertaintymap. Metricsareaveragedovertestingontwodatasets.
5 Conclusion
In this work, we propose a novel drone assisted road Gaussian Splatting with cross-view
uncertainty.Tousetheglobalinformationfromimagesindrones’viewtoassistground-view
training,wearethefirsttointroducethecross-viewuncertaintyintothe3D-GSbasedmodel
for weighting pixels in aerial images during training. This method reduces the impact of
superfluousaerialinformationandeffectivelyutilizesaerialimagesforroadscenesynthesis.
Fromtheexperimentalresults, weachieveSoTAontwohigh-fidelitysynthesizeddatasets.
Ourmethodenhancesvariousmetricsforheld-outgroundviewsynthesiswhilemaintaining
therobustnessofaerial-groundtrainingduringtheviewshiftingandrotation.Thesuperiority
ofthemethodshowsagreatpotentialfortheimprovementofautonomousdrivingsimulation
inthenearfuture.ZHANGETAL.:UC-GS 11
Acknowledgement
ThisresearchissponsoredbyTsinghua-ToyotaJointResearchFund(20223930097).
References
[1] JonathanTBarron,BenMildenhall,MatthewTancik,PeterHedman,RicardoMartin-
Brualla,andPratulPSrinivasan.Mip-nerf:Amultiscalerepresentationforanti-aliasing
neural radiance fields. In Proceedings of the IEEE/CVF International Conference on
ComputerVision,pages5855–5864,2021.
[2] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hed-
man. Mip-nerf360: Unboundedanti-aliasedneuralradiancefields. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages5470–
5479,2022.
[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hed-
man. Zip-nerf: Anti-aliased grid-based neural radiance fields. In Proceedings of the
IEEE/CVFInternationalConferenceonComputerVision,pages19697–19705,2023.
[4] HolgerCaesar,VarunBankiti,AlexHLang,SourabhVora,VeniceErinLiong,Qiang
Xu,AnushKrishnan,YuPan,GiancarloBaldan,andOscarBeijbom.nuscenes:Amul-
timodaldatasetforautonomousdriving. InProceedingsoftheIEEE/CVFconference
oncomputervisionandpatternrecognition,pages11621–11631,2020.
[5] Xiaoxue Chen, Junchen Liu, Hao Zhao, Guyue Zhou, and Ya-Qin Zhang. Nerrf:
3d reconstruction and viewsynthesis for transparent and specular objectswith neural
refractive-reflectivefields. arXivpreprintarXiv:2309.13039,2023.
[6] Kai Cheng, Xiaoxiao Long, Kaizhi Yang, Yao Yao, Wei Yin, Yuexin Ma, Wenping
Wang, and XuejinChen. Gaussianpro: 3dgaussian splattingwith progressivepropa-
gation. arXivpreprintarXiv:2402.14650,2024.
[7] KangleDeng,AndrewLiu,Jun-YanZhu,andDevaRamanan. Depth-supervisednerf:
Fewerviewsandfastertrainingforfree. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages12882–12891,2022.
[8] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Repre-
senting model uncertainty in deep learning. In international conference on machine
learning,pages1050–1059.PMLR,2016.
[9] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets
robotics: The kitti dataset. The International Journal of Robotics Research, 32(11):
1231–1237,2013.
[10] Lily Goli, Cody Reading, Silvia Selllán, Alec Jacobson, and Andrea Tagliasacchi.
Bayes’ rays: Uncertainty quantification for neural radiance fields. arXiv preprint
arXiv:2309.03185,2023.12 ZHANGETAL.:UC-GS
[11] Jiarui Hu, Xianhao Chen, Boyin Feng, Guanglin Li, Liangjing Yang, Hujun Bao,
Guofeng Zhang, and Zhaopeng Cui. Cg-slam: Efficient dense rgb-d slam in a con-
sistentuncertainty-aware3dgaussianfield. arXivpreprintarXiv:2403.16095,2024.
[12] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d
gaussiansplattingforreal-timeradiancefieldrendering. ACMTransactionsonGraph-
ics(ToG),42(4):1–14,2023.
[13] BalajiLakshminarayanan, AlexanderPritzel, andCharlesBlundell. Simpleandscal-
ablepredictiveuncertaintyestimationusingdeepensembles. Advancesinneuralinfor-
mationprocessingsystems,30,2017.
[14] Ruilong Li, Sanja Fidler, Angjoo Kanazawa, and Francis Williams. Nerf-xl: Scaling
nerfswithmultiplegpus. arXivpreprintarXiv:2404.16221,2024.
[15] YixuanLi,LihanJiang,LinningXu,YuanboXiangli,ZhenzhiWang,DahuaLin,and
BoDai. Matrixcity: Alarge-scalecitydatasetforcity-scaleneuralrenderingandbe-
yond. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages3205–3215,2023.
[16] JeremiahLiu, JohnPaisley, Marianthi-AnnaKioumourtzoglou, andBrentCoull. Ac-
curate uncertainty estimation and decomposition in ensemble learning. Advances in
neuralinformationprocessingsystems,32,2019.
[17] JunchenLiu,WenboHu,ZhuoYang,JiantengChen,GuoliangWang,XiaoxueChen,
Yantong Cai, Huan-ang Gao, and Hao Zhao. Rip-nerf: Anti-aliasing radiance fields
with ripmap-encoded platonic solids. In ACM SIGGRAPH 2024 Conference Papers,
pages1–11,2024.
[18] TaoLu,MulinYu,LinningXu,YuanboXiangli,LiminWang,DahuaLin,andBoDai.
Scaffold-gs: Structured 3d gaussians for view-adaptive rendering. arXiv preprint
arXiv:2312.00109,2023.
[19] JiagengMao,YujingXue,MinzheNiu,HaoyueBai,JiashiFeng,XiaodanLiang,Hang
Xu,andChunjingXu. Voxeltransformerfor3dobjectdetection. InProceedingsofthe
IEEE/CVFinternationalconferenceoncomputervision,pages3164–3173,2021.
[20] DailyMilanés-Hermosilla,RafaelTrujilloCodorniú,RenéLópez-Baracaldo,Roberto
Sagaró-Zamora, Denis Delisle-Rodriguez, John Jairo Villarejo-Mayor, and José Ri-
cardoNúñez-Álvarez. Montecarlo dropoutfor uncertaintyestimationand motorim-
ageryclassification. Sensors,21(21):7241,2021.
[21] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ra-
mamoorthi,andRenNg. Nerf: Representingscenesasneuralradiancefieldsforview
synthesis. CommunicationsoftheACM,65(1):99–106,2021.
[22] Raúl Mur-Artal and Juan D. Tardós. ORB-SLAM2: an open-source SLAM system
for monocular, stereo and RGB-D cameras. IEEE Transactions on Robotics, 33(5):
1255–1262,2017. doi: 10.1109/TRO.2017.2705103.
[23] RadfordMNeal.Bayesianlearningforneuralnetworks,volume118.SpringerScience
&BusinessMedia,2012.ZHANGETAL.:UC-GS 13
[24] RichardENeapolitanetal. Learningbayesiannetworks,volume38. PearsonPrentice
HallUpperSaddleRiver,2004.
[25] JulianOst,FahimMannan,NilsThuerey,JulianKnodt,andFelixHeide. Neuralscene
graphsfordynamicscenes. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages2856–2865,2021.
[26] JeongJoonPark,PeterFlorence,JulianStraub,RichardNewcombe,andStevenLove-
grove. Deepsdf: Learning continuous signed distance functions for shape represen-
tation. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition,pages165–174,2019.
[27] Ri-Zhao Qiu, Yafei Hu, Ge Yang, Yuchen Song, Yang Fu, Jianglong Ye, Jiteng Mu,
Ruihan Yang, Nikolay Atanasov, Sebastian Scherer, and Xiaolong Wang. Learning
generalizablefeaturefieldsformobilemanipulation,2024.
[28] Luca Savant, Diego Valsesia, and Enrico Magli. Modeling uncertainty for gaussian
splatting. arXivpreprintarXiv:2403.18476,2024.
[29] ShitalShah,DebadeeptaDey,ChrisLovett,andAshishKapoor. Airsim: High-fidelity
visualandphysicalsimulationforautonomousvehicles.InFieldandServiceRobotics:
Resultsofthe11thInternationalConference,pages621–635.Springer,2018.
[30] TavShande. Artstationpagehttps://www.artstation.com/tavshande,2022.
[31] JianxiongShen,AdriaRuiz,AntonioAgudo,andFrancescMoreno-Noguer.Stochastic
neuralradiancefields: Quantifyinguncertaintyinimplicit3drepresentations. In2021
InternationalConferenceon3DVision(3DV),pages972–981.IEEE,2021.
[32] Jianxiong Shen, Antonio Agudo, Francesc Moreno-Noguer, and Adria Ruiz.
Conditional-flownerf: Accurate3dmodellingwithreliableuncertaintyquantification.
InEuropeanConferenceonComputerVision,pages540–557.Springer,2022.
[33] JianxiongShen,RuijieRen,AdriaRuiz,andFrancescMoreno-Noguer. Estimating3d
uncertainty field: Quantifying uncertainty for neural radiance fields. arXiv preprint
arXiv:2311.01815,2023.
[34] KirillSibiriakov. Artstationpagehttps://www.artstation.com/vegaart,2022.
[35] XiaoweiSong,JvZheng,ShiranYuan,Huan-angGao,JingweiZhao,XiangHe,Wei-
haoGu,andHaoZhao. Sa-gs: Scale-adaptivegaussiansplattingfortraining-freeanti-
aliasing. arXivpreprintarXiv:2403.19615,2024.
[36] Matthew Tancik, Ethan Weber, Evonne Ng, Ruilong Li, Brent Yi, Terrance Wang,
Alexander Kristoffersen, Jake Austin, Kamyar Salahi, Abhik Ahuja, et al. Nerfstu-
dio:Amodularframeworkforneuralradiancefielddevelopment. InACMSIGGRAPH
2023ConferenceProceedings,pages1–12,2023.
[37] Beiwen Tian, Liyi Luo, Hao Zhao, and Guyue Zhou. Vibus: Data-efficient 3d scene
parsingwithviewpointbottleneckanduncertainty-spectrummodeling. ISPRSJournal
ofPhotogrammetryandRemoteSensing,194:302–318,2022.14 ZHANGETAL.:UC-GS
[38] HaithemTurki,JasonYZhang,FrancescoFerroni,andDevaRamanan.Suds:Scalable
urban dynamic scenes. In Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition,pages12375–12385,2023.
[39] ShuzheWang,VincentLeroy,YohannCabon,BorisChidlovskii,andJeromeRevaud.
Dust3r: Geometric3dvisionmadeeasy. arXivpreprintarXiv:2312.14132,2023.
[40] YuxiWei,ZiWang,YifanLu,ChenxinXu,ChangxingLiu,HaoZhao,SihengChen,
andYanfengWang.Editablescenesimulationforautonomousdrivingviacollaborative
llm-agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,pages15077–15087,2024.
[41] Zirui Wu, Tianyu Liu, Liyi Luo, Zhide Zhong, Jianteng Chen, Hongmin Xiao, Chao
Hou,HaozheLou,YuantaoChen,RunyiYang,etal.Mars:Aninstance-aware,modular
andrealisticsimulatorforautonomousdriving. InCAAIInternationalConferenceon
ArtificialIntelligence,pages3–15.Springer,2023.
[42] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang
Wang.Sinnerf:Trainingneuralradiancefieldsoncomplexscenesfromasingleimage.
InComputerVision–ECCV2022: 17thEuropeanConference,TelAviv,Israel,October
23–27,2022,Proceedings,PartXXII,pages736–753.Springer,2022.
[43] LinningXu,VasuAgrawal,WilliamLaney,TonyGarcia,AayushBansal,ChangilKim,
Samuel Rota Bulò, Lorenzo Porzi, Peter Kontschieder, Aljaž Božicˇ, et al. Vr-nerf:
High-fidelity virtualized walkable spaces. In SIGGRAPH Asia 2023 Conference Pa-
pers,pages1–12,2023.
[44] Shiyao Xu, Caiyun Liu, Yuantao Chen, Zhenxin Zhu, Zike Yan, Yongliang Shi, Hao
Zhao, andGuyueZhou. Camerarelocalizationinshadow-freeneuralradiancefields.
arXivpreprintarXiv:2405.14824,2024.
[45] Wei Xu, Yixi Cai, Dongjiao He, Jiarong Lin, and Fu Zhang. Fast-lio2: Fast direct
lidar-inertialodometry. IEEETransactionsonRobotics,38(4):2053–2073,2022.
[46] RunyiYang,ZhenxinZhu,ZhouJiang,BaijunYe,XiaoxueChen,YifeiZhang,Yuan-
tao Chen, Jian Zhao, and Hao Zhao. Spectrally pruned gaussian fields with neural
compensation. arXivpreprintarXiv:2405.00676,2024.
[47] BaijunYe,CaiyunLiu,XiaoyuYe,YuantaoChen,YuhaiWang,ZikeYan,Yongliang
Shi,HaoZhao,andGuyueZhou. Blendingdistributednerfswithtri-stagerobustpose
optimization. arXivpreprintarXiv:2405.02880,2024.
[48] AlexYu, VickieYe, MatthewTancik, andAngjooKanazawa. pixelnerf: Neuralradi-
ancefieldsfromoneorfewimages. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages4578–4587,2021.
[49] Zehao Yu, Anpei Chen, Binbin Huang, Torsten Sattler, and Andreas Geiger. Mip-
splatting: Alias-free3dgaussiansplatting. arXivpreprintarXiv:2311.16493,2023.
[50] Chongjian Yuan, Wei Xu, Xiyuan Liu, Xiaoping Hong, and Fu Zhang. Efficient and
probabilisticadaptivevoxelmappingforaccurateonlinelidarodometry.IEEERobotics
andAutomationLetters,7(3):8518–8525,2022.ZHANGETAL.:UC-GS 15
[51] Shiran Yuan and Hao Zhao. Slimmerf: Slimmable radiance fields. In 2024 Interna-
tionalConferenceon3DVision(3DV),pages64–74.IEEE,2024.
[52] TianyuanYuan,YuchengMao,JiaweiYang,YichengLiu,YueWang,andHangZhao.
Presight: Enhancingautonomousvehicleperceptionwithcity-scalenerfpriors. arXiv
preprintarXiv:2403.09079,2024.