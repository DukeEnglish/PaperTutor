Low-Budget Simulation-Based Inference
with Bayesian Neural Networks
ArnaudDelaunoy∗ MaxencedelaBrassinneBonardeaux*
UniversityofLiège UniversityofLiège
SiddharthMishra-Sharma† GillesLouppe
MIT,IAIFI,Anthropic UniversityofLiège
Abstract
Simulation-basedinferencemethodshavebeenshowntobeinaccurateinthedata-
poor regime, when training simulations are limited or expensive. Under these
circumstances,theinferencenetworkisparticularlypronetooverfitting,andusing
itwithoutaccountingforthecomputationaluncertaintyarisingfromthelackof
identifiabilityofthenetworkweightscanleadtounreliableresults. Toaddressthis
issue,weproposeusingBayesianneuralnetworksinlow-budgetsimulation-based
inference,therebyexplicitlyaccountingforthecomputationaluncertaintyofthe
posteriorapproximation. WedesignafamilyofBayesianneuralnetworkpriors
thataretailoredforinferenceandshowthattheyleadtowell-calibratedposteriors
ontestedbenchmarks,evenwhenasfewasO(10)simulationsareavailable. This
opensupthepossibilityofperformingreliablesimulation-basedinferenceusing
very expensive simulators, as we demonstrate on a problem from the field of
cosmology where single simulations are computationally expensive. We show
thatBayesianneuralnetworksproduceinformativeandwell-calibratedposterior
estimateswithonlyafewhundredsimulations.
1 Introduction
Simulation-basedinferenceaimsatidentifyingtheparametersofastochasticsimulatorthatbest
explainanobservation. InitsBayesianformulation,simulation-basedinferenceapproximatesthe
posteriordistributionofthemodelparametersgivenanobservation. Thisapproximationusuallytakes
theformofaneuralnetworktrainedonsyntheticdatageneratedfromthesimulator. Inthecontextof
scientificdiscovery,Hermansetal.(2022)stressedtheneedforposteriorapproximationsthatare
conservative–notoverconfident–inordertomakereliabledownstreamclaims. Theyalsoshowed
thatcommonsimulation-basedinferencealgorithmscanproduceoverconfidentapproximationsthat
mayleadtoerroneousconclusions.
Inthedata-poorregime(Villaescusa-Navarroetal.,2020;ZhangandMikelsons,2023;Zengetal.,
2023),wherethesimulatorisexpensivetorunandonlyasmallnumberofsimulationsareavailable,
training a neural network to approximate the posterior can easily lead to overfitting. With small
amountsoftrainingdata,theneuralnetworkweightsareonlylooselyconstrained,leadingtohigh
computationaluncertainty(Wengeretal.,2022).Thatis,manyneuralnetworkscanfitthetrainingdata
equallywell,yettheymayhaveverydifferentpredictionsontestdata. Forthisreason,theposterior
approximationisuncertainand,intheabsenceofaproperquantificationofthisuncertainty,potentially
overconfident. Fortunately,computationaluncertaintyinaneuralnetworkcanbequantifiedusing
∗Equalcontribution.
†WorkperformedwhileatMIT/IAIFI.
Preprint.Underreview.
4202
guA
72
]LM.tats[
1v63151.8042:viXraBayesianneuralnetworks(BNNs)(Galetal.,2016),whichaccountfortheuncertaintyintheneural
network weights. Therefore, in the context of simulation-based inference, BNNs can provide a
principledwaytoquantifythecomputationaluncertaintyoftheposteriorapproximation.
Hermansetal.(2022)showedempiricallythatusingensemblesofneuralnetworks,acrudeapprox-
imation of BNNs (Lakshminarayanan et al., 2017), does improve the calibration of the posterior
approximation. AfewstudieshavealsousedBNNsasdensityestimatorsinsimulation-basedinfer-
ence(Cobbetal.,2019;Walmsleyetal.,2020;Lemosetal.,2023). However,thesestudieshave
remainedempiricalandlimitedintheirevaluation. Thislackoftheoreticalgroundingmotivatesthe
needforamoreprincipledunderstandingofBNNsforsimulation-basedinference. Inparticular,the
choiceofpriorontheneuralnetworkweightshappenstobecrucialinthiscontext,asitcanstrongly
influencetheresultingposteriorapproximation. Yet,arbitrarypriorsthatconveylittleorundesired
informationabouttheposteriordensityhavebeenusedsofar.
Our contributions are twofold. Firstly, we provide an improved understanding of BNNs in the
context of simulation-based inference by empirically analyzing their effect on the resulting pos-
terior approximations. Secondly, we introduce a principled way of using BNNs in simulation-
based inference by designing meaningful priors. These priors are constructed to produce cal-
ibrated posteriors even in the absence of training data. We show that they are conservative
in the small-data regime, for very low simulation budgets. The code is available at https:
//github.com/ADelau/low_budget_sbi_with_bnn.
2 Background
Simulation-basedinference Weconsiderastochasticsimulatorthattakesparametersθasinput
and produces synthetic observations x as ouput. The simulator implicitly defines the likelihood
p(xθ)intheformofaforwardstochasticgenerativemodelbutdoesnotallowfordirectevaluation
|
ofitsdensityduetotheintractabilityofthemarginalizationoveritslatentvariables. Inthissetup,
Bayesiansimulation-basedinferenceaimsatapproximatingtheposteriordistributionp(θ x)using
|
thesimulator. Amongpossibleapproaches,neuralsimulation-basedinferencemethodstrainaneural
networktoapproximatekeyquantitiesfromsimulateddata,suchastheposterior,thelikelihood,the
likelihood-to-evidenceratio,orascorefunction(Cranmeretal.,2020).
Recently,concernshavebeenraisedregardingthecalibrationoftheapproximateposteriorsobtained
withneuralsimulation-basedinference. Hermansetal.(2022)showedthat,unlessspecialcareis
taken, common inference algorithms can produce overconfident posterior approximations. They
quantifythecalibrationusingtheexpectedcoverage
EC(pˆ,α)=E [1(θ Θ (α))] (1)
p(θ,x) pˆ
∈
whereΘ (α)denotesthehighestposteriorcredibleregionatlevelαcomputedusingtheposterior
pˆ
approximatepˆ(θ x).Theexpectedcoverageisequaltoαwhentheposteriorapproximateiscalibrated,
|
lowerthanαwhenitisoverconfidentandhigherthanαwhenitisunderconfidentorconservative.
The calibration of posterior approximations has been improved in recent years in various ways.
Delaunoyetal.(2022,2023)regularizetheposteriorapproximationstobebalanced,whichbiases
themtowardsconservativeapproximations. Similarly,Falkiewiczetal.(2024)regularizedirectlythe
posteriorapproximationbypenalizingmiscalibrationoroverconfidence. Masseranoetal.(2023)use
NeymanconstructionstoproduceconfidenceregionswithapproximateFrequentistcoverage. Patel
etal.(2023)combinesimulation-basedinferenceandconformalpredictions. Schmittetal.(2023)
enforcetheself-consistencyoflikelihoodandposteriorapproximationstoimprovethequalityof
approximateinferenceinlow-dataregimes.
Bayesiandeeplearning Bayesiandeeplearningaimstoaccountforboththealeatoricandepistemic
uncertaintyinneuralnetworks. Thealeatoricuncertaintyreferstotheintrinsicrandomnessofthe
variablebeingmodeled,typicallytakenintoaccountbyswitchingfromapointpredictortoadensity
estimator. Theepistemicuncertainty,ontheotherhand,referstotheuncertaintyassociatedwith
the neural network itself and is typically high in small-data regimes. Failing to account for this
uncertaintycanleadtohighmiscalibrationasmanyneuralnetworkscanfitthetrainingdataequally
well,yettheymayhaveverydifferentpredictionsontestdata.
2Bayesiandeeplearningaccountsforepistemicuncertaintybytreatingtheneuralnetworkweightsas
randomvariablesandconsideringthefullposterioroverpossibleneuralnetworksinsteadofonly
themostprobableneuralnetwork(Papamarkouetal.,2024). Formally,letusconsiderasupervised
learningsettinginallgenerality,wherexdenotesinputs,youtputs,DadatasetofN pairs(x,y),
andwtheweightsoftheneuralnetwork. Thelikelihoodofagivensetofweightsis
N
(cid:89)
p(D w) p(y x ,w), (2)
| ∝ i| i
i=1
wherep(y x ,w)istheoutputoftheneuralnetworkwithweightswandinputsx . Theresulting
i| i i
posteriorovertheweightsis
p(D w)p(w)
p(wD)= | , (3)
| p(D)
wherep(w)istheprior. Onceestimated,theposteriorovertheneuralnetwork’sweightscanbeused
forpredictionsthroughtheBayesianmodelaverage
(cid:90) M
1 (cid:88)
p(y x,D)= p(y x,w)p(wD)dw p(y x,w ),w p(wD). (4)
| | | ≃ M | i i ∼ |
i=1
Inpractice,theBayesianmodelaveragecanbeapproximatedbyMonteCarlosampling,withM
samplesfromtheposteriorovertheweights. Thequalityoftheapproximationdependsonthenumber
ofsamplesM, whichshouldbechosenhighenoughtoobtainagoodenoughapproximationbut
smallenoughtokeepreasonablethecomputationalcostsofpredictions.
Estimatingtheposteriorovertheneuralnetworkweightsisachallengingproblemduetothehigh
dimensionalityoftheweights. Variationalinference(Blundelletal.,2015)optimizesavariational
family to match the true posterior, which is typically fast but requires specifying a variational
family that may restrict the functions that can be modeled. Markov chain Monte Carlo methods
(WellingandTeh,2011;Chenetal.,2014),ontheotherhand,arelessrestrictiveinthefunctions
thatcanbemodeledbutrequirecarefultuningofthehyper-parametersandaremorecomputationally
demanding. TheBayesianposteriorcanalsobeapproximatedbyanensembleofneuralnetworks
(Lakshminarayanan et al., 2017; Pearce et al., 2020; He et al., 2020). Laplace methods leverage
geometric information about the loss to construct an approximation of the posterior around the
maximumaposteriori(MacKay,1992). Similarly,Maddoxetal.(2019)usethetrainingtrajectoryof
stochasticgradientdescenttobuildanapproximationoftheposterior.
3 Bayesianneuralnetworksforsimulation-basedinference
Inthecontextofsimulation-basedinference,treatingtheweightsoftheinferencenetworkasrandom
variablesenablesthequantificationofthecomputationaluncertaintyoftheposteriorapproximation.
Consideringneuralnetworkstakingobservationsxasinputandproducingparametersθasoutput,
theposteriorapproximationpˆ(θ x)canbemodeledastheBayesianmodelaverage
| (cid:90)
pˆ(θ x)= p(θ x,w)p(wD)dw, (5)
| | |
wherep(θ x,w)istheposteriorapproximationparameterizedbytheweightswandevaluatedat
|
(θ,x),andp(wD)istheposteriorovertheweightsgiventhetrainingsetD.
|
Remainingisthechoiceofpriorp(w). Whileprogresshasbeenmadeindesigningbetterpriors
inBayesiandeeplearning(Fortuin,2022),wearguethatnoneofthosearesuitableinthecontext
of simulation-based inference. To illustrate our point, let us consider the case of a normal prior
p(w)= (0,σ2I)ontheweights,inwhichcase
N (cid:90)
pˆ (θ x)= p(θ x,w) (wµ=0,Σ=σ2I)dw. (6)
normalprior
| | N |
AsmentionedinSection2, adesirablepropertyforaposteriorapproximationistobecalibrated.
ThereforewewantEC(pˆ ,α) = α, α. Althoughitmightbepossibleforthispropertyto
normalprior
∀
be satisfied in particular settings, this is obviously not the case for all values of σ and all neural
networkarchitectures. Therefore,andasillustratedinFigure1,theBayesianmodelaverageisnot
evencalibratedaprioriwhenusinganormalpriorontheweights. AstheBayesianmodelaverageis
notcalibratedapriori,itcannotbeexpectedthatupdatingtheposterioroverweightsp(wD)witha
|
smallamountofdatawouldleadtoacalibratedaposterioriBayesianmodelaverage.
33.1 Functionalpriorsforsimulation-basedinference
Wedesignapriorthatinducesanapriori-calibratedBayesianmodelaverage. Toachievethis,we
workinthespaceofposteriorfunctionsinsteadofthespaceofweights. Weconsiderthespaceof
functionstakingapair(θ, x)asinputandproducingaposteriordensityvaluef(θ,x)asoutput.
Eachfunctionf isdefinedbythejointoutputsitassociateswithanyarbitrarysetofinputs,such
thataposterioroverfunctionscanbeviewedasadistributionoverjointoutputsforarbitraryinputs.
Formally,letusconsiderM arbitrarypairs(θ,x)representedbythematricesΘ=[θ ,...,θ ]and
1 M
X = [x ,...,x ]andletf = [f ,...,f ]bethejointoutputsassociatedwiththoseinputs. The
1 M 1 M
distributionp(f Θ,X)thenrepresentsadistributionoverposteriorsf =[p˜(θ x ),...,p˜(θ x )].
1 1 M M
| | |
ThefunctionalposteriordistributionoverposteriorsforparametersΘandobservationsX given
a training dataset D is then p(f Θ,X,D) and the Bayesian model average is obtained through
|
marginalization,thatis
(cid:90)
p(θ x ,D)= f p(f Θ,X,D)df, i. (7)
i i i
| | ∀
Computingtheposterioroverfunctionsrequiresthespecificationofaprioroverfunctions. Wefirst
observethattheprioroverthesimulator’sparametersisacalibratedapproximationoftheposterior.
Thatis,forthepriorfunctionp :(θ,x) p(θ),wehavethatEC(p ,α)=α, α(Delaunoy
prior prior
→ ∀
etal.,2023). ItnaturallyfollowsthattheaprioriBayesianmodelaveragewithaDiracdeltaprior
aroundtheprioronthesimulator’sparametersiscalibrated
(cid:90)
pˆ(θ x )= f δ([f =p (θ ,x )])df, i
i i i j prior j j
| ∀
(8)
(cid:90)
= f δ(f =p(θ ))df , i EC(pˆ,α)=α, α.
i i i i
∀ ⇒ ∀
However,thispriorhaslimitedsupport,andtheBayesianmodelaveragewillnotconvergetothe
posteriorp(θ x)asthedatasetsizeincreases. WeextendthisDiracpriortoincludemorefunctionsin
|
itssupportwhileretainingthecalibrationproperty,whichweproposedefiningasaGaussianprocess
centeredatp .
prior
AGaussianprocess(GP)definesajointmultivariatenormaldistributionoveralltheoutputsf given
theinputs(Θ,X). Itisparametrizedbyameanfunctionµthatdefinesthemeanvaluefortheoutputs
giventheinputsandakernelfunctionK thatmodelsthecovariancebetweentheoutputs. Ifwehave
accesstonodata,themeanandthekerneljointlydefineaprioroverfunctionsastheydefineajoint
prioroveroutputsforanarbitrarysetofinputs. Inorderforthisprioroverfunctionstobecentered
aroundthepriorp ,wedefinethemeanfunctionasµ(θ,x)=p(θ). ThekernelK,ontheother
prior
hand,definesthespreadaroundthemeanfunctionandthecorrelationbetweentheoutputsf. Its
specificationisapplication-dependentandconstitutesahyper-parameterofourmethodthatcanbe
exploitedtoincorporatedomainknowledgeonthestructureoftheposterior. Forexample,periodic
kernelscouldbeusedifperiodicityisobserved. Kernel’shyperparameterscanalsobechosensuchas
toincorporatewhatwouldbeareasonabledeviationoftheapproximatedposteriorfromtheprior. We
denotetheGaussianprocessprioroverfunctionoutputsasp (f µ(Θ,X),K(Θ,X)). Proposition
GP
|
1showsthatafunctionalpriordefinedinthiswayleadstoacalibratedBayesianmodelaverage.
Proposition1. TheBayesianmodelaverageofaGaussianprocesscenteredaroundtheprioronthe
simulator’sparametersiscalibrated. Formally,letp bethedensityprobabilityfunctiondefinedby
GP
aGaussianprocess,µitsmeanfunction,andK thekernel. LetusconsiderM arbitrarypairs(θ,x)
representedbythematricesΘ = [θ ,...,θ ]andX = [x ,...,x ]andrepresentbythevector
1 M 1 M
f =[f ,...,f ]thejointoutputsassociatedwiththoseinputs. TheBayesianmodelaverageonthe
1 M
ithpairisexpressed
(cid:90)
pˆ(θ x )= f p (f µ(Θ,X),K(Θ,X))df
i i i GP
| |
Ifµ(θ,x)=p(θ), θ,x,then,
∀
EC(pˆ,α)=α, α,
∀
forallkernelK.
4Proof. Asp is,bydefinitionofaGaussianprocess,amultivariatenormal,theexpectationsofthe
GP
marginalsareequaltothemeanparameters
pˆ(θ x )=µ(θ ,x )=p(θ ).
i i i i i
|
The joint evaluation of the Bayesian model average of the Gaussian process is hence equivalent
to the joint evaluation of the prior for any matrices Θ and X. We can therefore conclude that pˆ
isequivalenttop : (θ,x) p(θ). SinceEC(p ,α) = α, α(Delaunoyetal.,2023),then,
prior prior
→ ∀
EC(pˆ,α)=α, α.
∀
3.2 Fromfunctionaltoparametricpriors
Inthissection,wenowdiscusshowthefunctionalGPprioroverposteriordensityfunctionsproposed
inSection3.1canbeusedinthesimulation-basedinferencesetting. WefollowFlam-Shepherdetal.
(2017)andSunetal.(2018)formappingthefunctionalpriortoadistributionoverneuralnetwork
weights. We note that other methods for functional Bayesian deep learning have been proposed
in the literature, such as (Tran et al., 2022; Rudner et al., 2022; Kozyrskiy et al., 2023; Ma and
Hernández-Lobato,2021). Thesemethodscouldalsobeusedinoursettingandarediscussedin
AppendixA.Letusfirstobservethataneuralnetworkarchitectureandaprioronweightsjointly
defineaprioroverfunctions. Weparameterizetheprioronweightsbyϕanddenotethisprobability
densityoverfunctionoutputsby
(cid:90)
p (f ϕ,Θ,X)= p(f w,Θ,X)p(wϕ)dw
BNN
| | |
(9)
(cid:90)
= δ([f =p(θ x ,w)])p(wϕ)dw.
i i i
| |
To obtain a prior on weights that matches the target GP prior, we optimize ϕ such that
p (f ϕ,Θ,X)matchesp (f µ(Θ,X),K(Θ,X)). FollowingFlam-Shepherdetal.(2017),
BNN GP
given a| measurement set = θ| ,x M at which we want the distributions to match, the KL
M { i i }i=1
divergencebetweenthetwopriorscanbeexpressedas
KL[p (f ϕ, ) p (f µ( ),K( ))]
BNN GP
| M || | M M
(cid:90) p (f ϕ, )
= p (f ϕ, )log BNN | M dy (10)
BNN | M p (f µ( ),K( ))
GP
| M M
= H[p (f ϕ, )] E [logp (f µ( ),K( ))],
−
BNN
| M −
pBNN(f|ϕ,M) GP
| M M
where the second term E [logp (f µ( ),K( ))] can be estimated using Monte-
Carlo.
ThefirsttermH[ppBNN( (f f|ϕ, ϕM ,) )],hG oP
wev| er,iM sharderM
toestimateasitrequirescomputing
BNN
| M
logp (f ϕ, ),whichinvolvestheintegrationoftheoutputoverallpossibleweightscombi-
BNN
| M
nations. Tobypassthisissue,Sunetal.(2018)proposetouseSpectralSteinGradientEstimation
(SSGE)(Shietal.,2018)toapproximatethegradientoftheentropyas
H[p (f ϕ, )] SSGE(f ,...,f p (f ϕ, )). (11)
∇ BNN | M ≃ 1 N ∼ BNN | M
Wenotethatthemeasurementset canbechosenarbitrarilybutshouldcovermostofthesupportof
M
thejointdistributionp(θ,x). Ifdatafromthisjointdistributionareavailable,thosecanbeleveraged
to build the measurement set. To showcase the ability to create a prior with limited data, in this
work,wederiveboundariesofthesupportofeachmarginaldistributionanddrawparametersand
observationsindependentlyanduniformlyoverthissupport. Ifthesupportisknownapriori,this
procedurecanbeperformedwithout(expensive)simulations. Wedrawanewmeasurementsetat
eachiterationoftheoptimizationprocedure. Ifafixedmeasurementsetisavailable,asubsampleof
thismeasurementsetshouldbedrawnateachiteration.
Asanillustrativeexample,wechoseindependentnormaldistributionsasavariationalfamilyp(wϕ)
|
overtheweightsandminimize(10)w.r.t. w. InFigure1, weshowthecoverageoftheresulting
aprioriBayesianmodelaverageusingthetunedprior,p(w ϕ),andnormalpriorsforincreasing
|
standarddeviationsσ,fortheSLCPbenchmark. Weobservethatwhilenoneofthenormalpriorsare
calibrated,thetrainedpriorachievesnear-perfectcalibration. Thispriorhenceguidestheobtained
posteriorapproximationtowardsmorecalibratedsolutions,eveninlowsimulation-budgetsettings.
5Theattentivereadermighthavenoticedthatp (f ϕ,Θ,X)andp (f µ(Θ,X),K(Θ,X))do
BNN GP
| |
not share the same support, as the former distribution is limited to functions that represent valid
densitiesbyconstruction,whilethelatterincludesarbitrarilyshapedfunctions. Thisisnotanissue
hereasthesupportofthefirstdistributionisincludedinthesupportoftheseconddistribution,and
functionsoutsidethesupportofthefirstdistributionareignoredinthecomputationofthedivergence.
3 3 3
1.0
2 2 2
0.8
1 1 1
0.6
0 0 0
0.4
−1 −1 −1
0.2
−2 −2 −2
0.0
−3
−2 θ0
1
2
−3
−2 0 2
−3
−2 0 2 0.0 0.2 Cre0 d.4 ibility0. l6 evel0.8 1.0
(wµ=0,Σ=12I) (wµ=0,Σ=0.052I)
N | N |
(wµ=0,Σ=0.12I) (wµ=0,Σ=0.0012I)
N | N |
0.01 0.02 0.03 0.04 0.05 0.06 (wµ=0,Σ=0.012I) p(wφ)
N | |
Figure 1: Visualization of the prior tuned to match the GP prior on the SLCP benchmark. Left:
examplesofposteriorfunctionssampledfromthetunedprioroverneuralnetwork’sweights. Right:
expectedcoverageofthepriorBayesianmodelaveragewiththetunedpriorandnormalpriorsfor
varyingstandarddeviations.
4 Experiments
Inthissection,weempiricallydemonstratethebenefitsofreplacingaregularneuralnetworkwith
aBNNequippedwiththeproposedpriorforsimulation-basedinference. WeconsiderbothNeural
PosteriorEstimation(NPE)withneuralsplineflows(Durkanetal.,2019)andNeuralRatioEstimation
(NRE)(Hermansetal.,2020),alongwiththeirbalancedversions(BNREandBNPE)(Delaunoyetal.,
2022,2023)andensembles(Lakshminarayananetal.,2017;Hermansetal.,2022). BNNs-based
methods are trained using mean-field variational inference (Blundell et al., 2015). As advocated
byWenzeletal.(2020),wealsoconsidercoldposteriorstoachievegoodpredictiveperformance.
Morespecifically,thevariationalobjectivefunctionismodifiedtogivelessweighttothepriorby
introducingatemperatureparameterT,
(cid:34) (cid:35)
(cid:88)
E logp(θ x ,w) T KL[p(wτ) p(wϕ)], (12)
w∼p(w|τ) i i
| − | || |
i
where τ are the parameters of the posterior variational family and T is a parameter called the
temperaturethatweightsthepriorterm. Inthefollowing,wecallBNN-NPE,aBayesianNeural
Networkposteriorestimatortrainedwithouttemperature(T = 1),andBNN-NPE(T = 0.01),an
estimatortrainedwithatemperatureof0.01,assigningalowerweighttotheprior.
AdetaileddescriptionoftheGaussianprocessusedcanbefoundinAppendixA.Forsimplicity,
inthisanalysis,weuseanRBFkernelintheGPprior. Ifmoreinformationonthestructureofthe
targetposteriorisavailable,moreinformedkernelsmaybeusedtoleveragethispriorknowledge. A
descriptionofthebenchmarkscanbefoundinAppendixB,andthehyperparametersaredescribedin
AppendixC.Forclarity,onlyNPE-basedmethodsareshowninthissection;resultsusingNREcan
befoundinAppendixD.
FollowingDelaunoyetal.(2022),weevaluatethequalityoftheposteriorapproximationsbasedon
theexpectednominallogposteriordensityandtheexpectedcoverageareaunderthecurve(coverage
AUC).TheexpectednominallogposteriordensityE [logpˆ(θ x)]quantifiestheamountof
θ,x∼p(θ,x)
|
densityallocatedtothenominalparameterthatwasusedtogeneratetheobservation. Thecoverage
(cid:82)1
AUC (EC(pˆ,α) α)dαquantifiesthecalibrationoftheexpectedposterior. Acalibratedposterior
0 −
approximationexhibitsacoverageAUCof0. ApositivecoverageAUCindicatesconservativeness,
andanegativecoverageAUCindicatesoverconfidence.
BNN-basedsimulation-basedinference Figure2comparessimulation-basedinferencemethods
withandwithoutaccountingforcomputationaluncertaintyWeobservethatBNNsequippedwith
6
2θ
egarevocdetcepxESLCP TwoMoons LotkaVolterra SpatialSIR
0 4 −1.5
3 2.0
−2
2
−2.0
1.5
−4 1 −2.5 1.0
0 −3.0 0.5
−6 −1 −3.5 0.0
−8
101 102 103 104 105 106
−− 32
101 102 103 104 105 106
−4.0
101 102 103 104 105 106
−0.5
101 102 103 104 105 106
0.1 0.3 0.05 0.2
0.0 0.2 0.00 0.1
−−−− 0000 .... 4321
101 102 103 104 105 106
−−−
00000 ..... 32101
101 102 103 104 105 106
−−−−− 00000 ..... 22110 50505
101 102 103 104 105 106
−−
000 ... 210
101 102 103 104 105 106
Simulationbudget
NPE BNPE NPEensemble BNN-NPE BNN-NPE(T=0.01)
Figure 2: Comparison of different simulation-based inference methods through the nominal log
probabilityandcoverageareaunderthecurve. Thehigherthenominallogprobability, themore
performantthemethodis. AcalibratedposteriorapproximationexhibitsacoverageAUCof0. A
positivecoverageAUCindicatesconservativeness,andanegativecoverageAUCindicatesoverconfi-
dence. 3runsareperformed,andthemedianisreported.
our prior and without temperature show positive coverage AUC even for simulation budgets as
lowasO(10). ThecoveragecurvesarereportedinAppendixDandshowthatthiscorrespondsto
conservativeposteriorapproximations. WeconcludethatBNNscanhencebereliablyusedwhen
the simulator is expensive and few simulations are available. We also observe that the nominal
logposteriordensityisonparwithothermethodsforveryhighsimulationbudgetsbutthatmore
samples are required to achieve high values. Cold posteriors can help achieve high nominal log
posterior values with fewer samples at the cost of sometimes producing overconfident posterior
approximations.Fromtheseobservations,generalguidelinestosetthetemperatureincludeincreasing
T ifoverconfidenceisobservedanddecreasingitiflowpredictiveperformanceisobserved.
ExamplesofposteriorapproximationsobtainedwithandwithoutusingaBayesianneuralnetwork
areshowninFigure3. WideposteriorsareobservedforlowbudgetsforBNN-NPE,whileNPE
producesanoverconfidentapproximationandexcludesmostoftherelevantpartsoftheposterior. As
thesimulationbudgetincreases,BNN-NPEconvergesslowlytowardsthesameposteriorasNPE.
BNN-NPE(T =0.01)convergesfasterthanBNN-NPEbut,forlowsimulationbudgets,excludes
partsoftheregionthatshouldbeacceptedaccordingtohighbudgetposteriors. Yet,theposterior
approximate is still less overconfident than NPE’s. Finally, further results in Appendix D show
that BNN-NRE is more conservative than BNN-NPE. This comes at the cost of lower nominal
logposteriordensityforagivensimulationbudgetwhilestillreachingcomparablevaluesasthe
simulationbudgetsgrow.
Comparison of different priors on weights We analyze the effect of the prior on the neural
network’sweightsontheresultingposteriorapproximation. Theposteriorapproximationsobtained
usingourGPpriorarecomparedtotheonesobtainedusingindependentnormalpriorsonweights
withzeromeansandincreasingstandarddeviations.InFigure4,weobservethatwhenusinganormal
prior,carefultuningofthestandarddeviationisneededtoachieveresultsclosetothepriordesigned
forsimulation-basedinference. Theusageofaninappropriatepriorcanleadtobadcalibrationfor
lowsimulationbudgetsorcanpreventlearningifitistoorestrictive.
Uncertaintydecomposition Wedecomposetheuncertaintyquantifiedbythedifferentmethods.
FollowingDepewegetal.(2018),theuncertaintycanbedecomposedas
H[pˆ(θ x)]=E [H[pˆ(θ x,w)]]+I(θ,w), (13)
q(w)
| |
7
roiretsopgollanimoN
CUAegarevoCNPE BNN-NPE BNN-NPE(T=0.01)
3 3 3
2 2 2
1 1 1
0 0 0
−1 −1 −1
−2 −2 −2
−3
−3 −2 −1 θ0
1
1 2 3
−3
−3 −2 −1 0 1 2 3
−3
−3 −2 −1 0 1 2 3
256 4096 65536 1048576
Figure3: Examplesof95%highestposteriordensityregionsobtainedwithvariousalgorithmsand
simulationbudgetsontheSLCPbenchmarkforasingleobservation. Theblackstarrepresentsthe
groundtruthusedtogeneratetheobservation.
CoverageAUC Nominallogposterior
0.2
0.1 −2
0.0
−0.1
−4
−0.2 −6
−0.3
−0.4 −8
−0.5
101 102 103 104 105 106 101 102 103 104 105 106
Simulationbudget
(wµ=0,Σ=0.012I) (wµ=0,Σ=0.052I) (wµ=0,Σ=0.12I) p(wφ)
N | N | N | |
Figure4:ComparisonofposteriorapproximationsobtainedusingapriortunedtomatchtheGaussian
process-basedpriorandusingindependentnormalpriorsonweightswithzeromeansandvarious
standarddeviationsontheSLCPbenchmark. 3runsareperformed,andthemedianisreported.
where E [H[pˆ(θ x,w)]] quantifies the aleatoric uncertainty, I(θ,w) quantifies the epistemic
q(w)
|
uncertainty,andthesumofthosetermsisthepredictiveuncertainty.Figure5showsthedecomposition
ofthetwosourcesofuncertainty,inexpectation,ontheSLCPbenchmark. Otherbenchmarkscan
befoundinAppendixD.WeobservethatBNN-NPEandNPEensemblemethodsaccountforthe
epistemicuncertaintywhileothermethodsdonot.BNPEartificiallyincreasesthealeatoricuncertainty
tobebettercalibrated. TheepistemicuncertaintyofBNN-NPEisinitiallylowbecausemostofthe
modelsareslightvariationsofp . Theepistemicuncertaintythenincreasesasitstartstodeviate
Θ
fromthe priorand decreasesas thetraining setsize increases. BNN-NPE(T = 0.01)exhibitsa
higherepistemicuncertaintyforlowbudgetsastheeffectofthepriorislowered.
InferingcosmologicalparametersfromN-bodysimulations ToshowcasetheutilityofBayesian
deeplearningforsimulation-basedinferenceinapracticalsetting,weconsiderachallenginginference
problemfromthefieldofcosmology. WeconsiderQuijoteN-bodysimulations(Villaescusa-Navarro
etal.,2020)tracingthespatialdistributionofmatterintheUniversefordifferentunderlyingcosmo-
logicalmodels. Theresultingobservationsareparticleswithdifferentmasses,correspondingtodark
matterclumps,whichhostgalaxies. Weconsiderthecanonicaltaskofinferringthematterdensity
(denotedΩ )andtheroot-mean-squarematterfluctuationaveragedoverasphereofradius8h−1
m
Mpc(denotedσ )fromanobservedgalaxyfield. Robustlyinferringthevaluesoftheseparametersis
8
oneofthescientificgoalsofflagshipcosmologicalsurveys. Thesesimulationsareverycomputation-
allyexpensivetorun,withover35millionCPUhoursrequiredtogenerate44100simulationsata
relativelylowresolution. Generatingsamplesathigherresolutions,orasignificantlylargernumber
ofsamples,ischallengingduetocomputationalconstraints. Theseconstraintsnecessitatemethods
thatcanbeusedtoproducereliablescientificconclusionsfromalimitedsetofsimulations–when
fewsimulationsareavailable,notonlyistheamountoftrainingdatalow,butsoistheamountoftest
datathatisavailabletoassessthecalibrationofthetrainedmodel.
Inthisexperiment,weuse2000simulationsprocessedasdescribedinCuesta-LazaroandMishra-
Sharma(2023). Thesesimulationsformasubsetofthefullsimulationsuiterunwithauniformprior
8
2θPredictiveuncertainty Epistemicuncertainty Aleatoricuncertainty
0.6
2.5 2.5
0.5
2.0 2.0
0.4
1.5 0.3 1.5
1.0 0.2 1.0
0.5 0.1 0.5
0.0
101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106
Simulationbudget
NPE BNPE NPEensemble BNN-NPE BNN-NPE(T=0.01)
Figure5: QuantificationofthedifferentformsofuncertaintiescapturedbythedifferentNPE-based
methodsontheSLCPbenchmark. 3runsareperformed,andthemedianisreported.
1.00
1.0 4
0.95
1000 0.90 0.8 3
800
600 0.85 0.6 2
400 0.80
200
0 0.75 0.4 1
8001000 0.70 0.2
0 200 400 600 800 1000 0200400600 0.65 0.2 0.3 0.4 0.5 0.0 0.0 0.2 0.4 0.6 0.8 1.0 0 101 102 103
Ωm Credibilitylevel Simulationbudget
NPE10 BNN-NPE(T=0.01)10
NPE200 BNN-NPE(T=0.01)200 NPE BNN-NPE(T=0.01)
NPE1800 BNN-NPE(T=0.01)1800
Figure6: ComparisonoftheposteriorapproximationsobtainedwithandwithoutaBayesianneural
networkonthecosmologicalapplication. Firstplot: Anexampleobservation: particlesrepresenting
galaxies in a synthetic universe. Second plot: example of 95% highest posterior density regions
forincreasingsimulationbudgets. Theblackstarrepresentsthegroundtruthusedtogeneratethe
observation. Thirdplot: ExpectedcoveragewithandwithoutusingaBayesianneuralnetworkfor
increasingsimulationbudgets. Fourthplot: Thenominallogposterior.
overtheparametersofinterest. 1800simulationsareusedfortrainingand200arekeptfortesting.
Weusethetwo-pointcorrelationfunctionevaluatedat24distancebinsasasummarystatistic. The
observableis,hence,avectorof24features. Weobservedthatsettingatemperaturelowerthan1
wasneededtoreachreasonablepredictiveperformancewithBayesianneuralnetworksinthissetting.
Figure6comparestheposteriorapproximationsobtainedwithasingleneuralnetworkagainstthose
obtainedwithaBNNtrainedwithatemperatureof0.01. Weobservefromthecoverageplotsthat
whileasingleneuralnetworkcanleadtooverconfidentapproximationsinthedata-poorregime,the
BNNleadstoconservativeapproximations. BNN-NPEalsoexhibitshighernominallogposterior
probability. Additionally,weobservethatitprovidesposteriorapproximationsthatarecalibratedand
haveahighnominallogprobabilitywithonlyafewhundredsamples.
5 Conclusion
Inthiswork,weuseBayesiandeeplearningtoaccountforthecomputationaluncertaintyassociated
with posterior approximations in simulation-based inference. We show that the prior on neural
network’s weights should be carefully chosen to obtain calibrated posterior approximations and
developapriorfamilywiththisobjectiveinmind. Thepriorfamilyisdefinedinfunctionspace
asaGaussianprocessandmappedtoaprioronweights. Empiricalresultsonbenchmarksshow
that incorporating Bayesian neural networks in simulation-based inference methods consistently
yieldsconservativeposteriorapproximations,evenwithlimitedsimulationbudgetsof (10). As
O
Bayesian deep learning continues to rapidly advance (Papamarkou et al., 2024), we anticipate
thatfuturedevelopmentswillstrengthenitsapplicabilityinsimulation-basedinference,ultimately
9
8σ
egarevocdetcepxE roiretsopgollanimoNenablingmoreefficientandreliablescientificapplicationsindomainswithcomputationallyexpensive
simulators.
UsingBNNsforsimulation-basedinferencealsocomeswithlimitations. Thefirstobservedlimitation
isthattheBayesianneuralnetworkbasedmethodsmightneedmoresimulateddatainordertoreach
apredictivepowersimilartomethodsthatdonotuseBNNs,suchasNPE.Whileweshowedthat
thislimitationcanbemitigatedbytuningthetemperatureappropriately,thisissomethingthatmight
requiretrialsanderrors. Asecondlimitationisthecomputationalcostofpredictions. Whentraining
aBNNusingvariationalinference,thetrainingcostremainsonasimilarscaleasstandardneural
networks. Atpredictiontime,however,theBayesianmodelaveragedescribedinEquation4must
beapproximated,andthisrequiresaneuralnetworkevaluationforeachMonteCarlosampleinthe
approximation. ThecomputationalcostofpredictionsthenscaleslinearlywiththenumberM of
MonteCarlosamples.
AcknowledgmentsandDisclosureofFunding
The authors would like to thank Adrien Bolland for the valuable discussions we had during this
project. Arnaud Delaunoy would also like to thank the National Fund for Scientific Research
(F.R.S.-FNRS) for his scholarship. This work is supported by the National Science Foundation
underCooperativeAgreementPHY-2019786(TheNSFAIInstituteforArtificialIntelligenceand
FundamentalInteractions,http://iaifi.org/). SMispartlysupportedbytheU.S.DepartmentofEnergy,
OfficeofScience,OfficeofHighEnergyPhysicsofU.S.DepartmentofEnergyundergrantContract
NumberDE-SC0012567.
References
Blundell,C.,Cornebise,J.,Kavukcuoglu,K.,andWierstra,D.(2015). Weightuncertaintyinneural
network. InInternationalconferenceonmachinelearning,pages1613–1622.PMLR.
Chen, T., Fox, E., and Guestrin, C. (2014). Stochastic gradient hamiltonian monte carlo. In
Internationalconferenceonmachinelearning,pages1683–1691.PMLR.
Cobb,A.D.,Himes,M.D.,Soboczenski,F.,Zorzan,S.,O’Beirne,M.D.,Baydin,A.G.,Gal,Y.,
Domagal-Goldman,S.D.,Arney,G.N.,Angerhausen,D.,etal.(2019). Anensembleofbayesian
neuralnetworksforexoplanetaryatmosphericretrieval. Theastronomicaljournal,158(1):33.
Cranmer, K., Brehmer, J., and Louppe, G. (2020). The frontier of simulation-based inference.
ProceedingsoftheNationalAcademyofSciences,117(48):30055–30062.
Cuesta-Lazaro,C.andMishra-Sharma,S.(2023). Apointcloudapproachtogenerativemodelingfor
galaxysurveysatthefieldlevel. arXivpreprintarXiv:2311.17141.
Delaunoy, A., Hermans, J., Rozet, F., Wehenkel, A., and Louppe, G. (2022). Towards reliable
simulation-basedinferencewithbalancedneuralratioestimation. AdvancesinNeuralInformation
ProcessingSystems,35:20025–20037.
Delaunoy,A.,Miller,B.K.,Forré,P.,Weniger,C.,andLouppe,G.(2023). Balancingsimulation-
based inference for conservative posteriors. In Fifth Symposium on Advances in Approximate
BayesianInference.
Depeweg,S.,Hernandez-Lobato,J.-M.,Doshi-Velez,F.,andUdluft,S.(2018). Decompositionof
uncertaintyinbayesiandeeplearningforefficientandrisk-sensitivelearning. InInternational
conferenceonmachinelearning,pages1184–1193.PMLR.
Durkan,C.,Bekasov,A.,Murray,I.,andPapamakarios,G.(2019). Neuralsplineflows. Advancesin
neuralinformationprocessingsystems,32.
Falkiewicz,M.,Takeishi,N.,Shekhzadeh,I.,Wehenkel,A.,Delaunoy,A.,Louppe,G.,andKalousis,
A.(2024). Calibratingneuralsimulation-basedinferencewithdifferentiablecoverageprobability.
AdvancesinNeuralInformationProcessingSystems,36.
10Flam-Shepherd,D.,Requeima,J.,andDuvenaud,D.(2017). Mappinggaussianprocesspriorsto
bayesianneuralnetworks. InNIPSBayesiandeeplearningworkshop,volume3.
Fortuin, V.(2022). Priorsinbayesiandeeplearning: Areview. InternationalStatisticalReview,
90(3):563–591.
Gal,Y.etal.(2016). Uncertaintyindeeplearning.
Greenberg,D.,Nonnenmacher,M.,andMacke,J.(2019). Automaticposteriortransformationfor
likelihood-freeinference. InInternationalConferenceonMachineLearning,pages2404–2414.
PMLR.
He, B., Lakshminarayanan, B., and Teh, Y. W. (2020). Bayesian deep ensembles via the neural
tangentkernel. Advancesinneuralinformationprocessingsystems,33:1010–1022.
Hermans,J.,Begy,V.,andLouppe,G.(2020). Likelihood-freemcmcwithamortizedapproximate
ratioestimators. InInternationalconferenceonmachinelearning,pages4239–4248.PMLR.
Hermans,J.,Delaunoy,A.,Rozet,F.,Wehenkel,A.,Begy,V.,andLouppe,G.(2022). Acrisisin
simulation-basedinference?beware,yourposteriorapproximationscanbeunfaithful.Transactions
onMachineLearningResearch.
Kozyrskiy,B.,Milios,D.,andFilippone,M.(2023). Imposingfunctionalpriorsonbayesianneural
networks. InICPRAM2023,12thInternationalConferenceonPatternRecognitionApplications
andMethods.
Lakshminarayanan,B.,Pritzel,A.,andBlundell,C.(2017).Simpleandscalablepredictiveuncertainty
estimationusingdeepensembles. Advancesinneuralinformationprocessingsystems,30.
Lemos,P.,Cranmer,M.,Abidi,M.,Hahn,C.,Eickenberg,M.,Massara,E.,Yallup,D.,andHo,S.
(2023). Robustsimulation-basedinferenceincosmologywithbayesianneuralnetworks. Machine
Learning: ScienceandTechnology,4(1):01LT01.
Lotka,A.J.(1920). Analyticalnoteoncertainrhythmicrelationsinorganicsystems. Proceedingsof
theNationalAcademyofSciences,6(7):410–415.
Ma,C.andHernández-Lobato,J.M.(2021). Functionalvariationalinferencebasedonstochastic
processgenerators. AdvancesinNeuralInformationProcessingSystems,34:21795–21807.
MacKay,D.J.(1992). Bayesianinterpolation. Neuralcomputation,4(3):415–447.
Maddox,W.J.,Izmailov,P.,Garipov,T.,Vetrov,D.P.,andWilson,A.G.(2019). Asimplebaseline
forbayesianuncertaintyindeeplearning. Advancesinneuralinformationprocessingsystems,32.
Masserano,L.,Dorigo,T.,Izbicki,R.,Kuusela,M.,andLee,A.B.(2023). Simulator-basedinference
withwaldo: Confidenceregionsbyleveragingpredictionalgorithmsandposteriorestimatorsfor
inverseproblems. ProceedingsofMachineLearningResearch,206.
Papamakarios,G.,Sterratt,D.,andMurray,I.(2019). Sequentialneurallikelihood: Fastlikelihood-
free inference with autoregressive flows. In The 22nd international conference on artificial
intelligenceandstatistics,pages837–848.PMLR.
Papamarkou, T., Skoularidou, M., Palla, K., Aitchison, L., Arbel, J., Dunson, D., Filippone, M.,
Fortuin,V.,Hennig,P.,Hubin,A.,etal.(2024). Positionpaper: Bayesiandeeplearningintheage
oflarge-scaleai. arXivpreprintarXiv:2402.00809.
Patel,Y.,McNamara,D.,Loper,J.,Regier,J.,andTewari,A.(2023). Variationalinferencewith
coverageguarantees. arXivpreprintarXiv:2305.14275.
Pearce,T.,Leibfried,F.,andBrintrup,A.(2020). Uncertaintyinneuralnetworks: Approximately
bayesianensembling. InInternationalconferenceonartificialintelligenceandstatistics,pages
234–244.PMLR.
11Rudner, T. G., Chen, Z., Teh, Y. W., and Gal, Y. (2022). Tractable function-space variational
inference in bayesian neural networks. Advances in Neural Information Processing Systems,
35:22686–22698.
Schmitt, M., Habermann, D., Bürkner, P.-C., Köthe, U., and Radev, S. T. (2023). Leveraging
self-consistencyfordata-efficientamortizedbayesianinference. arXivpreprintarXiv:2310.04395.
Shi,J.,Sun,S.,andZhu,J.(2018).Aspectralapproachtogradientestimationforimplicitdistributions.
InInternationalConferenceonMachineLearning,pages4644–4653.PMLR.
Sun,S.,Zhang,G.,Shi,J.,andGrosse,R.(2018). Functionalvariationalbayesianneuralnetworks.
InInternationalConferenceonLearningRepresentations.
Tran,B.-H.,Rossi,S.,Milios,D.,andFilippone,M.(2022). Allyouneedisagoodfunctionalprior
forbayesiandeeplearning. TheJournalofMachineLearningResearch,23(1):3210–3265.
Villaescusa-Navarro, F., Hahn, C., Massara, E., Banerjee, A., Delgado, A. M., Ramanah, D. K.,
Charnock, T., Giusarma, E., Li, Y., Allys, E., et al. (2020). The quijote simulations. The
AstrophysicalJournalSupplementSeries,250(1):2.
Volterra,V.(1926). Fluctuationsintheabundanceofaspeciesconsideredmathematically. Nature,
118(2972):558–560.
Walmsley, M., Smith, L., Lintott, C., Gal, Y., Bamford, S., Dickinson, H., Fortson, L., Kruk, S.,
Masters,K.,Scarlata,C.,etal.(2020). Galaxyzoo: probabilisticmorphologythroughbayesian
cnnsandactivelearning. MonthlyNoticesoftheRoyalAstronomicalSociety,491(2):1554–1574.
Welling,M.andTeh,Y.W.(2011). Bayesianlearningviastochasticgradientlangevindynamics. In
Proceedingsofthe28thinternationalconferenceonmachinelearning(ICML-11),pages681–688.
Citeseer.
Wenger, J., Pleiss, G., Pförtner, M., Hennig, P., and Cunningham, J. P. (2022). Posterior and
computational uncertainty in gaussian processes. Advances in Neural Information Processing
Systems,35:10876–10890.
Wenzel, F., Roth, K., Veeling, B., Swiatkowski, J., Tran, L., Mandt, S., Snoek, J., Salimans, T.,
Jenatton,R.,andNowozin,S.(2020). Howgoodisthebayesposteriorindeepneuralnetworks
really? InInternationalConferenceonMachineLearning,pages10248–10259.PMLR.
Zeng,J.,Todd,M.D.,andHu,Z.(2023). Probabilisticdamagedetectionusinganewlikelihood-free
bayesianinferencemethod. JournalofCivilStructuralHealthMonitoring,13(2):319–341.
Zhang,Y.andMikelsons,L.(2023). Sensitivity-guidediterativeparameteridentificationanddata
generationwithbayesflowandpels-vaeformodelcalibration. AdvancedModelingandSimulation
inEngineeringSciences,10(1):9.
12A Priortuningdetails
Wetunetheparametersϕofavariationaldistributionoverneuralnetworkweightsp(wϕ). Thevari-
|
ationaldistributionischosentobeindependentnormaldistributions,withparametersϕrepresenting
themeansandstandarddeviationsofeachparameterofw. Thisvariationalfamilydefinesaprior
overfunctionoutputs
(cid:90)
p (f ϕ,Θ,X)= p(f w,Θ,X)p(wϕ)dw. (14)
BNN
| | |
TheparametersϕareoptimizedtoobtainaprioronweightsthatmatchesthetargetGaussianprocess
functionalpriorp (f µ(Θ,X),K(Θ,X)). Toachievethis,werepeatedlysampleameasurement
GP
set = θ ,x M | andN functionoutputsfromtheBNNpriorf ,...,f p (f ϕ, )
M { i i }i=1 1 N ∼ BNN | M
andperformastepofgradientdescendtominimizethedivergence
KL[p (f ϕ, ) p (f µ( ),K( ))]. (15)
BNN GP
| M || | M M
ThemeanfunctionµoftheGaussianprocessisselectedas:
µ(θ,x)=p(θ). (16)
ThekernelK isacombinationoftwoRadialBasisFunction(RBF)kernels
(cid:112) (cid:112)
K(θ ,θ ,x ,x )= RBF(θ ,θ ) RBF(x ,x ). (17)
1 2 1 2 1 2 1 2
∗
suchthatthecorrelationbetweenoutputsishighonlyifθ andθ aswellasx andx areclose.
1 2 1 2
TheRBFkernelisdefinedas
RBF(x ,x
)=σ2exp(cid:32)
1
(cid:88)N
(x
1,i
−x
2,i)2(cid:33)
, (18)
1 2 −N 2l2
i i
whereσisthestandarddeviationandl isthelengthscaleassociatedtotheithfeature.Thelengthscale
i
isderivedfromthemeasurementset.Todeterminel ,wequeryobservationsxfromthemeasurement
i
setandcomputethe0.1quantileofthesquareddistancebetweendifferentobservationsforeach
feature. Wethensetl suchthat2l2 equalsthisquantile. Allthebenchmarkshaveauniformprior
i i
overthesimulator’sparameters. ThemeanfunctionisthenequaltoaconstantC forallinputvalues.
ThestandarddeviationischosentobeC/2. Toensurestabilityduringtheinferenceprocedure,we
enforceallstandarddeviationsdefinedinϕtobeatleast0.001bysettinganyparametersbelowthis
thresholdtothisvalue.
Notethattherearevariousmethodsthatcanbeusedtoperforminferenceontheneuralnetwork’s
weights with our GP prior. Instead of minimizing the KL-divergence, the parameters ϕ can be
optimized using an adversarial training procedure by treating both priors as function generators
andtrainingadiscriminatorbetweenthetwo(Tranetal.,2022). Anotherapproachtoperforming
inferenceusingafunctionalprioristodirectlyuseitduringinferencebymodifyingtheinference
algorithmtoworkinfunctionspace. Variationalinferencecanbeperformedinthespaceoffunction
(Sunetal.,2018;Rudneretal.,2022). ThestochasticgradientHamiltonianMonteCarloalgorithm
(Chen et al., 2014) could also be modified to include a functional prior Kozyrskiy et al. (2023).
Alternatively,avariationalimplicitprocesscanbelearnedtoexpresstheposteriorinfunctionspace
(MaandHernández-Lobato,2021).
B Benchmarksdescription
SLCP TheSLCP(SimpleLikelihoodComplexPosterior)benchmark(Papamakariosetal.,2019)
is a fictive benchmark that takes 5 parameters as input and produces an 8-dimensional synthetic
observable. Theobservationcorrespondstothe2Dcoordinatesof4pointsthataresampledfromthe
samemultivariatenormaldistribution. Weconsiderthetaskofinferringthemarginalover2ofthe5
parameters.
TwoMoons TheTwoMoonssimulator(Greenbergetal.,2019)modelsafictiveproblemwith
2parameters. Theobservablexiscomposedof2scalars, whichrepresentthe2Dcoordinatesof
arandompointsampledfromacrescent-shapeddistributionshiftedandrotatedaroundtheorigin
dependingontheparameters’values. Thosetransformationsinvolvetheabsolutevalueofthesumof
theparametersleadingtoasecondcrescentintheposteriorand,hencemakingitmulti-modal.
13Lotka Volterra The Lotka-Volterra population model (Lotka, 1920; Volterra, 1926) describes
a process of interactions between a predator and a prey species. The model is conditioned on 4
parametersthatinfluencethereproductionandmortalityrateofthepredatorandpreyspecies. We
inferthemarginalposteriorofthepredatorparametersfromatimeseriesof2001stepsrepresenting
theevolutionofbothpopulationsovertime. ThespecificimplementationisbasedonaMarkovJump
Process,asinPapamakariosetal.(2019).
SpatialSIR TheSpatialSIRmodel(Hermansetal.,2022)involvesagridworldofsusceptible,
infected,andrecoveredindividuals. Basedoninitialconditionsandtheinfectionandrecoveryrate,
themodeldescribesthespatialevolutionofaninfection. Theobservableisasnapshotofthegrid
worldaftersomefixedamountoftime. Thegridusedisofsize50by50.
C Hyperparameters
AlltheNPE-basedmethodsuseaNeuralSplineFlow(NSF)(Durkanetal.,2019)with3transforms
of6layers,eachcontaining256neurons. Meanwhile,alltheNRE-basedmethodsemployaclassifier
consisting of 6 layers of 256 neurons. For the spatialSIR and Lotka Volterra benchmarks, the
observableisinitiallyprocessedbyanembeddingnetwork. LotkaVolterra’sembeddingnetworkis
a10layers1Dconvolutionalneuralnetworkthatleadstoanembeddingofsize512. Ontheother
hand,SpatialSIR’sembeddingnetworkisan8layers2Dconvolutionalneuralnetworkresultinginan
embeddingofsize256. Allthemodelsaretrainedfor500epochswhichweobservedtobeenough
toreachconvergence.
Bayesianneuralnetwork-basedmethodsuseindependentnormaldistributionsasavariationalfamily.
Duringinference,100neuralnetworksaresampledtoapproximatetheBayesianmodelaverage. En-
semblemethodsinvolvetraining5neuralnetworksindependently. Theexperimentswereconducted
onaprivateGPUcluster,andtheestimatedcomputationalcostisaround25,000GPUhours.
D Additionalexperiments
Inthissection,weprovidecomplementaryresults. Figure7illustratestheperformanceofthevarious
NREvariants. Figures8and9displaythecoveragecurves, demonstratingthatahigherpositive
coverageAUCcorrespondstocoveragecurvesabovethediagonalline. Figures10and11presentthe
uncertaintydecompositionofallmethodsonallthebenchmarks. Figures12and13illustratehowthe
performanceofthedifferentalgorithmsvariesacrossdifferentruns.
14SLCP TwoMoons LotkaVolterra SpatialSIR
4
−0.5 −1.50
−1.0 3 −1.75 2.0
−1.5 2 −2.00 1.5
−−−− 3322 .... 5050 −101 −−−−− 33222 ..... 20752 50505 001 ... 050
101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106
0.25 0.30
0.20
0.20 0.4 0.25
0.15
0.15 0.3 0.20
0.10
0.10 0.2 0.15
0.05
0.05 0.1 0.10
0.00 0.0 0.00 0.05
−0.05 −0.1
−0.05 0.00
101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106
Simulationbudget
NRE BNRE NREensemble BNN-NRE BNN-NRE(T=0.01)
Figure7: ComparisonofdifferentNREsimulation-basedinferencemethodsthroughthenominal
log probability and coverage area under the curve. The higher the nominal log probability, the
moreperformantthemethodis. AcalibratedposteriorapproximationexhibitsacoverageAUCof
0. A positive coverage AUC indicates conservativeness, and a negative coverage AUC indicates
overconfidence. 3runsareperformed,andthemedianisreported
15
roiretsopgollanimoN
CUAegarevoCSLCP TwoMoons LotkaVolterra SpatialSIR
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Credibilitylevel
16 256 4096 65536 1048576
64 1024 16384 262144
Figure8: CoverageofdifferentNPEsimulation-basedinferencemethods. Acalibratedposterior
approximation exhibits a coverage AUC of 0. A coverage curve above the diagonal indicates
conservativenessandacurvebelowthediagonalindicatesoverconfidence. 3runsareperformed,and
themedianisreported.
16
EPN
EPNB
elbmesneEPN
)10.0=T(EPN-NNB
EPN-NNB
egarevocdetcepxESLCP TwoMoons LotkaVolterra SpatialSIR
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Credibilitylevel
16 256 4096 65536 1048576
64 1024 16384 262144
Figure9: CoverageofdifferentNREsimulation-basedinferencemethods. Acalibratedposterior
approximation exhibits a coverage AUC of 0. A coverage curve above the diagonal indicates
conservativenessandacurvebelowthediagonalindicatesoverconfidence. 3runsareperformed,and
themedianisreported.
17
ERN
ERNB
elbmesneERN
)10.0=T(ERN-NNB
ERN-NNB
egarevocdetcepxESLCP TwoMoons LotkaVolterra SpatialSIR
2.5 1.0 2.4 0.25
2.0 0.5 2.2 0.00
0.0 2.0 −0.25
1.5 −0.5 1.8 −0.50
1.0 −− 11 .. 50 11 .. 46 −− 10 .. 07 05
0.5 −2.0 1.2 −1.25
−2.5 1.0 −1.50
101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106
0.6 0.6 0.30 0.30
0.5 0.5 0.25 0.25
0.4 0.4 0.20 0.20
0.3 0.3 0.15 0.15
0.2 0.2 0.10 0.10
0.1 0.1 0.05 0.05
0.0 0.0 0.00 0.00
101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106
2.5 1.0 2.4 0.25
0.5 2.2 0.00
2.0 0.0 2.0 −0.25
1.5 −0.5 1.8 −0.50
−1.0 1.6 −0.75
1.0 −1.5 1.4 −1.00
0.5 −− 22 .. 50 11 .. 02 −− 11 .. 52 05
101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106
Simulationbudget
NPE BNPE NPEensemble BNN-NPE BNN-NPE(T=0.01)
Figure10: QuantificationofthedifferentformsofuncertaintiescapturedbythedifferentNPE-based
methods. 3runsareperformed,andthemedianisreported.
SLCP TwoMoons LotkaVolterra SpatialSIR
2.5 1 2.2 0.0
2.0 0 2.0 −− 00 .. 42
11 .. 05 −− 21 111 ... 468 −−− 100 ... 086
0.5
−3
1.2
−− 11 .. 42
101 102 103 104 105 106
−4
101 102 103 104 105 106
1.0
101 102 103 104 105 106
−1.6
101 102 103 104 105 106
0.16
0.25 0.25 0.14 0.14
0.12
0.20 0.20 0.12
0.10
0.10
0.15 0.15 0.08 0.08
0.10 0.10 0.06 0.06
0.04 0.04
0.05 0.05
0.02 0.02
0.00 0.00 0.00 0.00
101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106
2.5 1 2.2 0.0
2.0 0 2.0 −− 00 .. 42
−1 1.8 −0.6
11 .. 05 −2 11 .. 46 −− 10 .. 08
0.5
−3
1.2
−− 11 .. 42
101 102 103 104 105 106
−4
101 102 103 104 105 106
1.0
101 102 103 104 105 106
−1.6
101 102 103 104 105 106
Simulationbudget
NRE BNRE NREensemble BNN-NRE BNN-NRE(T=0.01)
Figure11: QuantificationofthedifferentformsofuncertaintiescapturedbythedifferentNRE-based
methods. 3runsareperformed,andthemedianisreported.
18
ytniatrecnucirotaelA
ytniatrecnucimetsipE
ytniatrecnuevitciderP
ytniatrecnucirotaelA
ytniatrecnucimetsipE
ytniatrecnuevitciderPSLCP TwoMoons LotkaVolterra SpatialSIR
0 4 −1.5 2
−2 2 −2.0 1
−4 0 −2.5 0
−−− 1086 −−− 642 −−−− 4433 .... 5050 −−−−− 54321
101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106
0.2 0.3 0.2
0.1
0.1 0.2 0.1
0.0 0.1 0.0 0.0
−−−− 0000 .... 4321
−−−−
00000 ..... 43210
−− 00 .. 21
−−−− 0000 .... 4321
101 102 103 104 105 106 101 102 103 104 105 106 101 102 103 104 105 106 −0.5 101 102 103 104 105 106
Simulationbudget
NPE BNPE NPEensemble BNN-NPE BNN-NPE(T=0.01)
Figure12: ComparisonofdifferentNPEsimulation-basedinferencemethodsthroughthenominal
log probability and coverage area under the curve. The higher the nominal log probability, the
moreperformantthemethodis. AcalibratedposteriorapproximationexhibitsacoverageAUCof
0. A positive coverage AUC indicates conservativeness, and a negative coverage AUC indicates
overconfidence. 3runsareperformed. Themedianrunisreportedinplain, andtheshadedlines
representtheothertworuns.
SLCP TwoMoons LotkaVolterra SpatialSIR
−0.5 4 −1.50
2.0
−1.0 3 −1.75
−1.5
2
−2.00 1.5
−2.0 −2.25 1.0
−2.5 1 −2.50
0.5
−3.0 0 −2.75
−− 43 .. 05
101 102 103 104 105 106
−1
101 102 103 104 105 106
−− 33 .. 20 50
101 102 103 104 105 106
−00 .. 50
101 102 103 104 105 106
0.25 0.3
0.20 0.4 0.20
0.15 0.3 0.15 0.2 0.10 0.10
0.05 0.2 0.05 0.1
0.00 0.1 0.00
0.0
−0.05 0.0 −0.05
−− 00 .. 11 50
101 102 103 104 105 106
−0.1
101 102 103 104 105 106
−− 00 .. 11 50
101 102 103 104 105 106
−0.1
101 102 103 104 105 106
Simulationbudget
NRE BNRE NREensemble BNN-NRE BNN-NRE(T=0.01)
Figure13: ComparisonofdifferentNREsimulation-basedinferencemethodsthroughthenominal
log probability and coverage area under the curve. The higher the nominal log probability, the
moreperformantthemethodis. AcalibratedposteriorapproximationexhibitsacoverageAUCof
0. A positive coverage AUC indicates conservativeness, and a negative coverage AUC indicates
overconfidence. 3runsareperformed. Themedianrunisreportedinplain, andtheshadedlines
representtheothertworuns.
19
roiretsopgollanimoN
CUAegarevoC
roiretsopgollanimoN
CUAegarevoC