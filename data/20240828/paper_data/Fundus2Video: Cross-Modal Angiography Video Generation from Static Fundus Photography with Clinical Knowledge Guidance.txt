Fundus2Video: Cross-Modal Angiography Video
Generation from Static Fundus Photography with
Clinical Knowledge Guidance
Weiyi Zhang1 , Siyu Huang2, Jiancheng Yang3, Ruoyu Chen1, Zongyuan Ge4,
Yingfeng Zheng5, Danli Shi1 ((cid:66)), and Mingguang He1
1 The Hong Kong Polytechnic University, Kowloon, Hong Kong
2 Clemson University, South Carolina, USA
3 École Polytechnique Fédérale de Lausanne (EPFL), Lausanne, Switzerland
4 Monash University, Melbourne, Australia
5 Sun Yat-sen University, Guangzhou, China
danli.shi@polyu.edu.hk
Abstract. Fundus Fluorescein Angiography (FFA) is a critical tool for
assessing retinal vascular dynamics and aiding in the diagnosis of eye
diseases. However, its invasive nature and less accessibility compared
to Color Fundus (CF) images pose significant challenges. Current CF
to FFA translation methods are limited to static generation. In this
work, we pioneer dynamic FFA video generation from static CF im-
ages. We introduce an autoregressive GAN for smooth, memory-saving
frame-by-frame FFA synthesis. To enhance the focus on dynamic le-
sion changes in FFA regions, we design a knowledge mask based on
clinical experience. Leveraging this mask, our approach integrates inno-
vative knowledge mask-guided techniques, including knowledge-boosted
attention, knowledge-aware discriminators, and mask-enhanced patch-
NCE loss, aimed at refining generation in critical areas and addressing
thepixelmisalignmentchallenge.OurmethodachievesthebestFVDof
1503.21andPSNRof11.81comparedtoothercommonvideogeneration
approaches. Human assessment by an ophthalmologist confirms its high
generation quality. Notably, our knowledge mask surpasses supervised
lesionsegmentationmasks,offeringapromisingnon-invasivealternative
to traditional FFA for research and clinical applications. The code is
available at https://github.com/Michi-3000/Fundus2Video.
Keywords: Video Generation · Generative Adversarial Network · Au-
toregressive Generation · Retinal Fundus Photography · Fluorescence
Angiography.
1 Introduction
Fundus Fluorescein Angiography (FFA) is an essential examination in ophthal-
mologyclinics,providingadynamicviewofretinalbloodflowandlesionchanges.
It offers critical insights into retinal circulatory dynamics, aiding in the identifi-
cation of conditions such as diabetic retinopathy, hypertensive retinopathy, and
4202
guA
72
]VI.ssee[
1v71251.8042:viXra2 Zhang et al.
macular degeneration [13]. Unlike Color Fundus (CF) images, FFA videos cap-
ture the dynamic filling process and real-time changes in retinal vascular abnor-
malities with greater clarity and depth, thereby enhancing diagnostic precision
andfacilitatingadeeperunderstandingofdiseaseprogressionandtreatmentre-
sponse. However, due to its invasive nature and potential side effects, FFA’s use
is limited for certain individuals. In contrast, CF photography is non-invasive,
readilyavailable[27],andhasbeenutilizedinsomedeep-learningmethods[6,21]
fordiseasediagnosis.Therefore,generatingrealisticFFAvideosfromCFimages
holds significant research and application potential.
When considering the generative models for FFA synthesis, the majority of
existing methods [23,11,12,16,20] focus on specific phases, like the venous and
late phase, using various Generative Adversarial Networks (GAN). However,
they overlook the changes occurring throughout the entire FFA process, which
includes multiple phases. While some approaches [2] can generate multiple dis-
creteFFAimagesfromdifferentphasessimultaneously,theystillcannotcapture
the fully dynamic changes of retinal structures and lesions. Capturing lesional
changes accurately is another challenge in FFA generation. While using lesion
labels for conditional supervision could potentially enhance image details, the
manual annotation of these labels is highly time-consuming and impractical for
segmenting all possible lesion changes. Additionally, the time-consuming nature
of FFA procedures makes it difficult to align FFA images precisely with CF im-
ages in clinical practice, due to blinking and movement, even with good patient
cooperation [4,7]. This misalignment poses a significant challenge for pixel-to-
pixel-based video generation processes.
Totacklethesechallenges,weproposeamodelleveraginganimage-to-image
GANframework,specificallypix2pixHD[25],togeneratesmoothandstableFFA
videosfromsingleCFimagesautoregressively.Throughclinicalknowledgeanal-
ysis of ground-truth FFA series, regions with significant lesion changes during
the early and late FFA series examination are lesional changes, reflecting the
damage in vascular or retinal pigment epithelium structure [3,28]. The larger
the changes, the more important they are. Leveraging this insight, we design a
knowledge mask that requires no additional manual labeling and enhances the
generation of regions with high variability. Using this mask, we introduce novel
knowledge mask-guided techniques into the baseline model to guide the model
to focus more on key regions during learning and generation. Specifically, we
propose a mask-enhanced patchNCE loss to address the pixel misalignment is-
sue. This model holds the potential to generate FFA videos from CF images to
other modalities and improve downstream tasks [1,22,19].
Insummary,ourresearchcontributesasfollows:1.Wearethefirsttogener-
atedynamicFFAvideodirectlyfromCFimages,markingasignificantadvance-
mentinophthalmicimaging.Specifically,weintroduceFundus2Video,anautore-
gressiveGANarchitecturetailoredforframe-by-frameFFAvideosynthesisfrom
CF images. This architecture optimizes memory usage and ensures smooth out-
put.2.Weintroduceaknowledgemaskderivedfromclinicalinsightstoenhance
focus on regions undergoing significant changes during dynamic FFA processes.Fundus2Video 3
Fig.1. Proposed Fundus2Video. (a) The overall architecture. Generator G generates
oneframeatatime,takingtheoutputfromtheprevioustimestepandtheCFimage
as input. During the training phase, unsupervised knowledge masks guide the entire
network. (b) The design of the mask-enhanced patchNCE loss.
Thiseliminatesmanuallabelingandimprovesgenerationinareaslikelesionsand
blood vessels. 3. With this mask, we implement knowledge mask (KM)-guided
techniques.Weintroduceknowledge-boostedattentionandknowledge-awaredis-
criminators for specific supervision on regions of lesion regions. To address the
pixel misalignment challenge between CF images and ground-truth FFA series
in critical areas, we employ a newly designed mask-enhanced patchNCE loss.
2 Methods
2.1 Overview
We aim to generate a realistic FFA video Yˆ from a given CF image x, with
the ground-truth FFA video during training represented as Y. Considering the
temporal nature of FFA series, we adopt an autoregressive GAN architecture to
capture temporal dependencies and generate coherent video sequences. An au-
toregressive GAN generates image samples sequentially, conditioning each new
image on previously generated images and additional inputs. In our context of
generating FFA videos from CF images, our autoregressive GAN, named Fun-
dus2Video, based on the image-to-image translation GAN pix2pixHD, sequen-
tially generates each frame of the FFA video, incorporating the CF image itself
and the preceding frames. Building upon the generator, discriminator, and loss
designsofpix2pixHD,ourapproachincorporatesspecificmodificationstoenable
autoregressiveandsmoothgeneration.ThearchitectureisasshowninFig.1(a).
ToensuresmoothoutputinFundus2Video,weincorporatemulti-frameinput
andsmoothingtechniquesforlongertemporalconsiderations.Specifically,wein-
put three consecutive frames from the ground-truth FFA series to the model in4 Zhang et al.
Fig.2.Thedefinitionoftheknowledgemask.Left:Theunsupervisedprocessofobtain-
ing the mask. The knowledge mask covers the same pathological areas as the expert-
labeled mask. Right: Generated results with and without the knowledge mask.
a sliding window fashion to provide longer temporal context for each generated
frame. Instead of generating each frame independently, we aggregate the gen-
erated frames over a sliding window and perform triple-frame averaging. This
approachsmoothsoutabrupttransitionsbetweenframesandensurescontinuity
in the generated video sequence.
2.2 Unsupervised Clinically Supported Knowledge Mask
The baseline Fundus2Video can generate smooth and continuous FFA videos.
However, it falls short of accurately depicting details like lesions and critical
structures as shown in Fig. 2 right, which are of utmost clinical importance. To
address this, we leveraged clinical insights to analyze ground-truth FFA videos,
which tell us regions undergoing significant morphological changes during the
FFAprocessoftencorrespondedtocruciallesionsorretinalstructureareasthat
pose challenges for the model. The theoretical basis is from [3]:
– During the FFA process, as the fluorescent dye flows through retinal ves-
sels, significant leakage always occurs around the lesions, leading to visible
differences between early and late stages.
Buildinguponthisknowledge,wedevisedasimplebinarymaskbycomputingthe
difference between the first frame (representing the arterial phase) and the last
frame(representingthelatephase)andsettingaspecificthresholdδ determined
through comparative experiments, which can be formulated as m = δ(Y −
0
Y ), where Y represents the first frame of the ground-truth FFA video and Y
T 0 T
represents the last frame. The process is depicted in Fig. 2. Unlike supervised
lesion/structuresegmentationmasks,thisknowledgemaskrequiresnoadditional
manualannotationorsegmentationmodeltraining.Itcanbeeasilyderivedfrom
raw data, making it simple yet effective.
2.3 Knowledge Mask-Guided Video Generation
Knowledge-boostedAttention. Sometypesoflesionsmaybechallengingto
detectinCFimagesduetolowcontrast,leadingtosynthesizedFFAsliceslackingFundus2Video 5
details in these areas. To address this limitation and improve the generator’s
ability to capture specific regions, we introduce additional supervision into the
learning process. Our approach, termed knowledge-boosted attention, involves
guidingthenetwork’sattentiontowardfocalregionsduringtraining.Toquantify
this guidance, we define an attention loss L as follows:
Att
1 (cid:88)
L (A,m)= (Ai−mi)2. (1)
Att n
i
Here, m represents the knowledge mask described in Section 2.2. A denotes
the attention map obtained by element-wise multiplication of the semantic-rich
activation map f from the last convolutional layer l in the generator and the
l
mask m. We then apply a rectified linear operation to A, resulting in A =
ReLU(f ⊙m).
l
Mask-enhancedPatchNCELosses. Toaddresspixelmisalignmentinground-
truth FFA series and CF images caused by motion artifacts during acquisition,
weintroducethePatchNCEloss[14],inspiredbycontrastivelearningtechniques
known for boosting model robustness against label noise. However, we observed
thatthemodel’sprimaryfocusshouldbeonreducingjitterinclinicallyrelevant
regions,suchaslesionsandvasculature,whichareofgreaterclinicalsignificance.
To further tackle this issue, we propose mask-enhanced PatchNCE losses as a
replacement for traditional PatchNCE losses. This method extends traditional
PatchNCE losses by incorporating a knowledge mask m, highlighting critical
regions within the FFA series. Mathematically, the proposed mask-enhanced
PatchNCE losses are based on the InfoNCE loss, which is defined as:
 
esim(v,v+)
L InfoNCE(v,v+,v−)=−log
esim(v,v+)+(cid:80)N esim(v,v
j−). (2)
j=1
Here, v, v+, and v− represent the embeddings of the anchor, positive, and neg-
ative samples, respectively.
Themask-enhancedunsupervisedPatchNCE(UP)losscomparestheanchor
patch z in the generated output with a corresponding positive patch z from
Yˆ X
the input CF image and negative patches z−, under the guidance of knowledge
X
mask m. It is defined as:
L =L (m⊙z ,m⊙z ,m⊙z−), (3)
MaskedUP InfoNCE Yˆ X X
where⊙denoteselement-wisemultiplication.Incontrast,themask-enhancedsu-
pervisedPatchNCE(SP)lossensuresconsistencybetweengeneratedandground-
truth patches. It designates the corresponding patch in the ground-truth image
z aspositive,whilenon-correspondingpatchesz− areconsiderednegatives.It’s
Y Y
defined as:
L =L (m⊙z ,m⊙z ,m⊙z−). (4)
MaskedSP InfoNCE Yˆ Y Y6 Zhang et al.
The illumination is shown in Fig. 1 (b). By integrating the knowledge mask
into the PatchNCE loss, our method directs the model’s focus during training,
improving its ability to capture clinically significant features.
Knowledge-awareDiscriminators. Weemploy3discriminatorsD ={D ,D ,D }
1 2 3
[9,25] with the same patchGAN architecture [10] to evaluate images at scales of
1,0.5and0.25fordifferentreceptivefields.Thediscriminatorobjectivefunction
for D with generator G is given by:
k
L (a,b,G(a))=E [logD (a,b)]+E [log(1−D (a,G(a)))], (5)
Dk a,b k a k
where a, b and G(a) are the input, ground-truth, and generated images.
However, solely discriminating the entire image may not ensure the authen-
ticity of lesion regions in generated FFA frames. Hence, we introduce discrim-
ination guided by knowledge across scales. By combining knowledge masks m
with corresponding FFA images, we tailor inputs for the discriminators to focus
on lesions. According to Eq. 5, the combined discriminator loss L (G,D )
GAN k
for scale k is defined as:
L (G,D )=L (x,y,G(x))+L (x⊙m,y⊙m,G(x)⊙m), (6)
GAN k Dk Dk
where ⊙ denotes element-wise multiplication, and x and y are the input CF
images and ground-truth FFA images, respectively.
Consequently, the final loss function is as follows:
L=λ L +λ L +λ L +λ L . (7)
UP MaskedUP SP MaskedSP Att Att GAN GAN
3 Experiments
Dataset. Ourdatasetcomprises350CFimagesand18,180correspondingFFA
images from 350 anonymous patients sampled from a large paired dataset. The
FFA images were obtained using Zeiss FF450 Plus and Heidelberg Spectralis
systems, with a resolution of 768×768 pixels. Meanwhile, the CF images were
capturedbyTopconTRC-50XFandZeissFF450Plusinstruments,withresolu-
tionsrangingfrom1,110×1,467to2,600×3,200pixels.TheInstitutionalReview
Board approved the study.
Implementation Details. The final objective function (Eq. 7) was utilized to
train the generative model, with λ , λ , λ , and λ set to 1, 1, 4, and
UP SP Att GAN
2, respectively.The threshold δ for obtaining theknowledge maskwas setto 45.
Duringtraining,eachground-truthFFAseriesproduced12frames,with4slices
randomlyselectedfromthevascular,venous,andlatephases,respectively.Data
augmentationtechniquesincludingrandomcropping,scaling,andcoloraugmen-
tation. The input images were resized to 512×512. Additionally, the model was
trainedtorandomlyselecteithergeneratedorground-truthframesasinput,en-
hancingitsadaptabilityandrobustness.WeemployedtheAdamoptimizerwithFundus2Video 7
Knowledge
Input Frame1 Frame4 Frame7 Frame10 Frame12
Mask
GT
Seg2Vid
Med-
ddqm
ConsistI2V
Fundus2Video
w/o
KM
Fundus2Video
Fig.3.Qualitativecomparisonofthemethods.Framesaresampledfromthe12-frame
video. Areas in red boxes denote significant lesions. It can be observed that the KM-
guided Fundus2Video exhibits best performance in generating critical lesions.
beta = 0.5 and beta = 0.999, adjusting the learning rate every 50 iterations
1 2
using the PyTorch [17] lr-scheduler. The initial learning rate was set to 2e-3,
with a batch size of 1. Training was conducted for 50 epochs on an NVIDIA
GeForce RTX3090. For evaluation, 70% of the data was reserved for training at
the patient level, while the remaining data was evenly split into validation and
test sets.
Evaluation Criteria. OurvideoevaluationcriteriaincludeFréchetVideoDis-
tance (FVD) [24], Structural Similarity Index (SSIM) [26], Peak Signal-to-Noise
Ratio(PSNR)[8],andLearnedPerceptualImagePatchSimilarity(LPIPS)[29].
They measure feature distribution similarity, video structural similarity, recon-
struction quality, and perceptual similarity, respectively.
Model Comparisons. We evaluate Fundus2Video against existing image-to-
video translation methods, including the auto-encoder-based Seg2vid [15], and8 Zhang et al.
Table 1. Comparison of the methods. M stands for masks.
ProposedTechniques
Models MaskType Mask-enhancedKnowledge Knowledge FVD↓ SSIM↑PSNR↑LPIPS↓
PatchNCE
PatchNCEloss-boost -aware
lossLP
LMaskedP attentionLAttdiscriminators
Seg2Vid[15] - - - - - 2302.15 0.2930 10.23 0.2451
Med-ddpm[5] - - - - - 2410.54 0.2305 10.59 0.2513
ConsistI2V[18] - - - - - 2108.33 0.2662 10.71 0.2498
KnowledgeM ✗ ✗ ✗ ✗ 1804.25 0.3225 11.11 0.2213
KnowledgeM ✓ ✗ ✗ ✗ 1611.21 0.3625 11.41 0.2162
KnowledgeM ✗ ✓ ✗ ✗ 1527.94 0.3738 11.76 0.2093
Fundus2VideoKnowledgeM ✗ ✗ ✓ ✗ 1701.30 0.3694 11.20 0.2133
KnowledgeM ✗ ✗ ✗ ✓ 1664.42 0.3442 11.36 0.2166
GTLesionSegM ✓ ✓ ✓ ✓ 1586.35 0.3688 11.23 0.2136
KnowledgeM ✓ ✓ ✓ ✓ 1503.210.3814 11.81 0.2001
the diffusion model-based Med-ddpm [5] and ConsistI2V [18]. Table 1 shows
our model’s superior performance across all metrics. Qualitative comparison in
Fig. 3 reveals clearer images and discernible lesion areas in our approach versus
others.
Ablation Studies. Additionally, we conduct comprehensive ablation studies
to assess the effectiveness of our proposed knowledge mask and related tech-
niques, detailed in the latter part of Table 1 and Fig. 3. Firstly, we show that
our designed mask-enhanced patchNCE loss, knowledge-boost attention, and
knowledge-aware discriminators, when combined with mask information, out-
perform the baseline Fundus2Video. Moreover, our mask-enhanced patchNCE
loss yields better results than patchNCE loss alone. Secondly, by replacing the
knowledgemaskwiththeground-truthlesionsegmentationmaskforcomparison,
we observe that utilizing our KM-guided techniques can enhance performance
even with the lesion segmentation mask. However, our knowledge mask yields
better results without the need for additional training or labeling efforts.
Human Assessment. Anophthalmologistreviewedtheresultsofallmethods
in Table 1 and found that our Fundus2Video significantly outperformed oth-
ers. Then the ophthalmologist conducted a quality assessment of 50 randomly
selected FFA videos generated by Fundus2Video from the test set, evaluating
them based on their corresponding CF images and ground-truth FFA videos.
Theevaluationfocusedonvascularperfusion,lesiondynamics,overallcoherence,
stability, and presence of artifacts. Scores ranged from 1 to 5, with 1 indicating
excellent quality and 5 indicating very poor quality. Our model received a score
of 2.12 with a standard deviation of 1.07, indicating good overall quality of the
generated videos.
4 Conclusion
Inthisstudy,weproposeFundus2Video,whichpioneersdynamicFFAvideogen-
erationfromstaticCFimagesusinganautoregressiveGANarchitecture.WithaFundus2Video 9
knowledge mask derived from clinical experience, we enhance focus on dynamic
lesionregions,outperformingsupervisedlesionsegmentationmasks.Ourmethod
incorporatesknowledge-boostedattention,knowledge-awarediscriminators,and
mask-enhanced patchNCE loss to address challenges in lesion generation and
pixel misalignment. Fundus2Video emerges as a promising alternative to tradi-
tional FFA, surpassing recent state-of-the-art approaches with its non-invasive,
intuitive, and dynamic features.
Acknowledgments. The study was supported by the Global STEM Professorship
Scheme(P0046113)andtheStart-upFundforRAPsundertheStrategicHiringScheme
(P0048623) from HKSAR. The sponsors or funding organizations had no role in the
design or conduct of this research.
DisclosureofInterests. Apatenthasbeenfiledforthisinnovation(CN202410360491.4).
References
1. Chen,R.,Zhang,W.,Song,F.,Yu,H.,Cao,D.,Zheng,Y.,He,M.,Shi,D.:Trans-
lating color fundus photography to indocyanine green angiography using deep-
learningforage-relatedmaculardegenerationscreening.npjDigitalMedicine7(1),
34 (2024)
2. Chen, Y., He, Y., Li, W., Wang, J., Li, P., Xing, L., Zhang, X., Shi, G.: Series-
parallel generative adversarial network architecture for translating from fundus
structureimagetofluorescenceangiography.AppliedSciences12(20),10673(2022)
3. Comin,C.H.,Tsirukis,D.I.,Sun,Y.,Xu,X.:Quantificationofretinalbloodleakage
infundusfluoresceinangiographyinaretinalangiogenesismodel.ScientificReports
11(1), 19903 (2021)
4. DeCarlo,T.E.,Romano,A.,Waheed,N.K.,Duker,J.S.:Areviewofopticalcoher-
ence tomography angiography (octa). International journal of retina and vitreous
1, 1–15 (2015)
5. Dorjsembe,Z.,Pao,H.K.,Odonchimed,S.,Xiao,F.:Conditionaldiffusionmodels
for semantic 3d medical image synthesis. arXiv preprint arXiv:2305.18453 (2023)
6. Faust, O., Acharya U, R., Ng, E.Y.K., Ng, K.H., Suri, J.S.: Algorithms for the
automateddetectionofdiabeticretinopathyusingdigitalfundusimages:areview.
Journal of medical systems 36, 145–157 (2012)
7. Freeman,W.R.,Bartsch,D.U.,Mueller,A.J.,Banker,A.S.,Weinreb,R.N.:Simul-
taneous indocyanine green and fluorescein angiography using a confocal scanning
laser ophthalmoscope. Archives of Ophthalmology 116(4), 455–463 (1998)
8. Huynh-Thu, Q., Ghanbari, M.: Scope of validity of psnr in image/video quality
assessment. Electronics letters 44(13), 800–801 (2008)
9. Iizuka,S.,Simo-Serra,E.,Ishikawa,H.:Globallyandlocallyconsistentimagecom-
pletion. ACM Transactions on Graphics (ToG) 36(4), 1–14 (2017)
10. Isola,P.,Zhu,J.Y.,Zhou,T.,Efros,A.A.:Image-to-imagetranslationwithcondi-
tional adversarial networks. In: Proceedings of the IEEE conference on computer
vision and pattern recognition. pp. 1125–1134 (2017)
11. Kamran,S.A.,Hossain,K.F.,Tavakkoli,A.,Zuckerbrod,S.L.:Attention2angiogan:
Synthesizing fluorescein angiography from retinal fundus images using generative
adversarial networks. In: 2020 25th International Conference on Pattern Recogni-
tion (ICPR). pp. 9122–9129. IEEE (2021)10 Zhang et al.
12. Kamran, S.A., Hossain, K.F., Tavakkoli, A., Zuckerbrod, S.L., Sanders, K.M.,
Baker,S.A.:Rv-gan:Segmentingretinalvascularstructureinfundusphotographs
using a novel multi-scale generative adversarial network. In: Medical Image Com-
puting and Computer Assisted Intervention–MICCAI 2021: 24th International
Conference,Strasbourg,France,September27–October1,2021,Proceedings,Part
VIII 24. pp. 34–44. Springer (2021)
13. Kylstra, J.A., Brown, J.C., Jaffe, G.J., Cox, T.A., Gallemore, R., Greven, C.M.,
Hall, J.G., Eifrig, D.E.: The importance of fluorescein angiography in planning
laser treatment of diabetic macular edema. Ophthalmology 106(11), 2068–2073
(1999)
14. Li, F., Hu, Z., Chen, W., Kak, A.: Adaptive supervised patchnce loss for learning
h&e-to-ihcstaintranslationwithinconsistentgroundtruthimagepairs.In:Medical
Image Computing and Computer Assisted Intervention – MICCAI 2023. pp. 632–
641. Springer Nature Switzerland, Cham (2023)
15. Pan,J.,Wang,C.,Jia,X.,Shao,J.,Sheng,L.,Yan,J.,Wang,X.:Videogeneration
from single semantic label map. In: Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition. pp. 3733–3742 (2019)
16. Park, K.B., Choi, S.H., Lee, J.Y.: M-gan: Retinal blood vessel segmentation by
balancing losses through stacked deep fully convolutional networks. IEEE Access
8, 146308–146322 (2020)
17. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-
performancedeeplearninglibrary.Advancesinneuralinformationprocessingsys-
tems 32 (2019)
18. Ren, W., Yang, H., Zhang, G., Wei, C., Du, X., Huang, S., Chen, W.: Con-
sisti2v:Enhancingvisualconsistencyforimage-to-videogeneration.arXivpreprint
arXiv:2402.04324 (2024)
19. Shi, D., He, S., Yang, J., Zheng, Y., He, M.: One-shot retinal artery and vein
segmentationviacross-modalitypretraining.OphthalmologyScience4(2),100363
(2024)
20. Shi, D., Zhang, W., He, S., Chen, Y., Song, F., Liu, S., Wang, R., Zheng, Y., He,
M.: Translation of color fundus photography into fluorescein angiography using
deeplearningforenhanceddiabeticretinopathyscreening.OphthalmologyScience
3(4), 100401 (2023)
21. Sinthanayothin,C.,Boyce,J.F.,Williamson,T.H.,Cook,H.L.,Mensah,E.,Lal,S.,
Usher, D.: Automated detection of diabetic retinopathy on digital fundus images.
Diabetic medicine 19(2), 105–112 (2002)
22. Song, F., Zhang, W., Zheng, Y., Shi, D., He, M.: A deep learning model for gen-
erating fundus autofluorescence images from color fundus photography. Advances
in ophthalmology practice and research 3(4), 192–198 (2023)
23. Tavakkoli,A.,Kamran,S.A.,Hossain,K.F.,Zuckerbrod,S.L.:Anoveldeeplearn-
ing conditional generative adversarial network for producing angiography images
from retinal fundus photographs. Scientific Reports 10(1), 1–15 (2020)
24. Unterthiner,T.,vanSteenkiste,S.,Kurach,K.,Marinier,R.,Michalski,M.,Gelly,
S.: Fvd: A new metric for video generation (2019)
25. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-
resolution image synthesis and semantic manipulation with conditional gans. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 8798–8807 (2018)Fundus2Video 11
26. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assessment:
fromerrorvisibilitytostructuralsimilarity.IEEEtransactionsonimageprocessing
13(4), 600–612 (2004)
27. Yannuzzi,L.A.,Ober,M.D.,Slakter,J.S.,Spaide,R.F.,Fisher,Y.L.,Flower,R.W.,
Rosen, R.: Ophthalmic fundus imaging: today and beyond. American journal of
ophthalmology 137(3), 511–524 (2004)
28. Yannuzzi, L.A., Rohrer, K.T., Tindel, L.J., Sobel, R.S., Costanza, M.A., Shields,
W.,Zang,E.:Fluoresceinangiographycomplicationsurvey.Ophthalmology93(5),
611–617 (1986)
29. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: Proceedings of the IEEE
conference on computer vision and pattern recognition. pp. 586–595 (2018)