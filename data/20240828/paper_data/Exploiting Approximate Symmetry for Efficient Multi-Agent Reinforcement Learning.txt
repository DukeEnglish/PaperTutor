Exploiting Approximate Symmetry for Efficient
Multi-Agent Reinforcement Learning
BatuhanYardim NiaoHe
DepartmentofComputerScience DepartmentofComputerScience
ETHZürich ETHZürich
alibatuhan.yardim@inf.ethz.ch niao.he@inf.ethz.ch
Abstract
Mean-fieldgames(MFG)havebecomesignificanttoolsforsolvinglarge-scale
multi-agentreinforcementlearningproblemsundersymmetry. However,theas-
sumptionofexactsymmetrylimitstheapplicabilityofMFGs,asreal-worldsce-
narios often feature inherent heterogeneity. Furthermore, most works on MFG
assumeaccesstoaknownMFGmodel,whichmightnotbereadilyavailablefor
real-worldfinite-agentgames. Inthiswork,webroadentheapplicabilityofMFGs
by providing a methodology to extend any finite-player, possibly asymmetric,
gametoan“inducedMFG”.First,weprovethatN-playerdynamicgamescanbe
symmetrizedandsmoothlyextendedtotheinfinite-playercontinuumviaexplicit
Kirszbraunextensions. Next,weproposethenotionofα,β-symmetricgames,a
newclassofdynamicpopulationgamesthatincorporateapproximatepermutation
invariance.Forα,β-symmetricgames,weestablishexplicitapproximationbounds,
demonstratingthataNashpolicyoftheinducedMFGisanapproximateNashof
theN-playerdynamicgame. WeshowthatTDlearningconvergesuptoasmall
biasusingtrajectoriesoftheN-playergamewithfinite-sampleguarantees,permit-
tingsymmetrizedlearningwithoutbuildinganexplicitMFGmodel. Finally,for
r
certaingamessatisfyingmonotonicity,weproveasamplecomplexityofOpε´6q
fortheN-agentgametolearnanε-Nashuptosymmetrizationbias. Ourtheoryis
supportedbyevaluationsonMARLbenchmarkswiththousandsofagents.
1 Introduction
Competitivemulti-agentreinforcementlearning(MARL)hasfoundawiderangeofapplicationsin
therecentyears[51,58,47,44,35,34]. Simultaneously,MARLisfundamentallychallengingat
theregimewithmanyagentsduetoanexponentiallygrowingsearchspace[57],alsoknownasthe
curse-of-many-agents. Evenfindinganapproximatesolution(i.e. approximateNash)isPPAD-hard
[16],thuspotentiallyintractable. Forthesereasons,ithasbeenanactiveareaofresearchtoidentify
“islandsoftractability”,whereMARLcanbesolvedefficiently(seee.g. [33,42]).
Inthiswork,wedevelopatheoryofefficientlearningforMARLproblemsthatexhibitapproximate
symmetry building upon the theory of mean-field games (MFG). MFG is a common theoretical
frameworkforbreakingthecurseofmanyagentsunderperfectsymmetry. Initiallyproposedby[30]
and[25],MFGanalyzesN-playergameswithsymmetricagentswhenN islarge. Inthissetting,
theso-calledpropagationofchaospermitsthereductionoftheN-playergametoagamebetween
arepresentativeagentandapopulationdistribution. Thistheoreticalframeworkhasbeenwidely
studiedinmanyrecentworks[1,20,41,42,59,60].
However,worksonMFGexhibittwomajorbottleneckspreventingwiderapplicabilityinMARL.
Firstandforemost,theaforementionedworksonMFGallassumesomeformofexactsymmetry
betweenagents. Namely,intheMFGs,allagentsmusthavethesamerewardfunctionanddynamics
Preprint.Underreview.
4202
guA
72
]TG.sc[
1v37151.8042:viXraWork Symmetry Approximation Learning Learnw/omodel
Saldietal.,2018 Exact ✓(asymptotic) ✗ -
Yardimetal.,2024 Exact ✓(explicit) ✗ -
CuiandKoeppl,2021 Exact ✓(asymptotic) ✓(reg.) ✗
Zamanetal.,2023 Exact ✗ ✓(reg.) ✗
Yardimetal.,2023 Exact ✗ ✓(reg.) ✓
PariseandOzdaglar,2019 Graphon ✓(explicit) ✗ -
Zhangetal.,2023 Graphon ✗ ✓(mon.) ✗
Pérolatetal.,2022 Multi-pop. ✗ ✓(mon.) ✗
Ourwork α,β-symm. ✓(explicit) ✓(mon.) ✓
Table 1: Selected models of symmetric games studied in MF-RL works. (reg.: only Nash with
regularizationstrictlyboundedawayfromzero,mon.: monotonicityassumption)
mustbehomogeneous(orpermutationinvariant)amongagents. Suchperfectsymmetrybetween
agentsinMARListheoreticallyconvenientyetpracticallyinfeasible: Eveninapplicationswhere
symmetryispresumed,usually,thereareimperfectionsindynamicsthatbreakinvariance. Little
researchhasstudiedwhetherMFGscouldoffertractableapproximationstootherwiseintractable
gamesthatmightexhibitapproximatesymmetries. Secondly,manyworksonMFG(suchas[19,41])
implicitlyassumethatanexactmodeloftheMFGisknowntothealgorithmakintosolvingaknown
MDP.Inreal-worldapplications,anexactMFGmodelmightnotbereadilyavailable. MFGscan
potentiallyaddresssettingswhereonlyN-playerdynamics(possiblyincorporatingimperfections
andheterogeneity)canbesimulated;however,suchatheoryofMFGshasyettobedeveloped.
WeaddresstheseshortcomingsbydevelopingatheoreticallysoundMARLframeworkforscenarios
whenpermutationinvarianceholdsonlyapproximately.UnlikepreviousworkonMFG,ourtheoretical
approach is constructive: we show that given any MARL problem, one can construct an MFG
approximationthatpermitsefficientlearning.Wedefineanew,broadclassofgameswithapproximate
permutationinvariance,dubbedα,β-symmetricgames,forwhichapproximateNashequilibriacan
belearnedefficiently. Ourtheoreticalframeworkprovidesend-to-endlearningguaranteesforpolicy
mirrordescentcombinedwithTDlearning. Ourexperimentalfindingsfurtherdemonstratestrong
performanceimprovementsinMARLproblemswiththousandsofagents.
1.1 RelatedWork
We compare our work with selected past MFG results in Table 1, and also provide a detailed
commentaryinthissection.
Mean-fieldgamesandRL.MFGsrepresentaparticulartypeofcompetitivegamewhereplayers
exhibitstrongsymmetries. PastworkhasstudiedtheexistenceofMFGNashequilibriumaswell
asitsapproximationoffinite-playerNash[10,11,46]. TheconvergenceofRLalgorithmshasalso
beenwidelystudiedindiscrete-timeMFGassumingeithercontractivityinthestationaryequilibrium
setting[63,60,19,59,13]ormonotonicityinthefinite-horizonsetting[42,41,61,40,43]. These
modelshoweverassumeexacthomogeneitybetweenallparticipants.Furthermore,existingalgorithms
typicallyassumeknowledgeoftheexactMFGmodel[19,63],hinderingtheirreal-worldapplicability.
Multi-populationMFG(MP-MFG)canincorporatemultipletypesofpopulationsexposedtodifferent
dynamics[25,41,52,17,5,11,24]. However,withineachpopulationexactsymmetrymusthold
andthenumberoftypesmustbemuchsmallerthanthenumberofagents. Moreover,MP-MFGcan
beliftedtoanequivalentsingle-populationMFG[24]undercertainconstraints. Overall,allofthese
worksrequirevariationsofthesamestringentsymmetryassumptions,restrictingtheirapplicability.
AdetailedsurveyoflearninginMFGscanbefoundat[32].
GraphonMFG.Graphongames,proposedinitiallyby[39],canincorporateheterogeneitybetween
MFGagentsbyassuminggraphon-basedinteractions. Thesettinghasbeenanalyzedindiscrete-time
[14, 56] as well as in the continuous time setting [2, 7, 3]. Recently, policy mirror descent has
beenanalyzedinthissettingtoproduceconvergenceguaranteesundermonotonicityconditions[64].
However,theseworksongraphonmean-fieldgamesstillincorporateexactsymmetryintheformof
2thegraphon: namely,thetypesofagentsmustfollowasymmetricdistributionandinteractionsmust
bethroughasymmetricgraphon. Infact,graphonMFGscanstillbereducedtoregularMFGs[64].
Otherrelatedwork. Anotherclassofgameswherealargenumberofagentscanbetackledtractably
aretheso-calledpotentialgames[45],generalizedtoMarkovpotentialgamesincorporatingdynamics
[33]. ApproximatepotentialshavebeenstudiedinasimilarspiritonMarkovα-potentialgames[22]
andnearpotentialgames[8]. However,tothebestofourknowledge,approximatesymmetryhasnot
beenstudiedintheliteratureofMFGs.
1.2 OurContributions
Welistthefollowingasourcontributions,comparedtopastworksummarizedintheprevioussection.
1. WefirsttacklethefoundationalbutunderstudiedquestionforMFGs: whencanagivenN-agent
gamebemeaningfullyextendedtoaninfinite-playerMFG?Weconstructawell-definedMFG
approximationtoanarbitrary(possiblynon-symmetric)finite-playerdynamicalgame(DG)using
theideaoffunctionsymmetrizationandviaKirszbraunLipschitzextensions.
2. Usingourextension,wedefineanewclassofα,β-symmetricDGsforwhichitistractableto
findapproximateNash. α,β-symmetrygeneralizespermutationinvarianceindynamicgamesto
arbitraryMARLproblems,whereparametersα,β quantifydegreesofheterogeneityindynamics
andplayerrewardsrespectively.
3. WeprovethatthesolutionoftheinducedMFGisindeedanapproximateNashtotheoriginal
?
α,β-symmetricDGuptoabiasofOp1{ N`α`βq,demonstratingthatMFGapproximationis
robusttoheterogeneityandfinite-agenterrorsintheDG.
4. WeanalyzeTDlearningonthetrajectoriesofthefinite-agentDG.Weshowthatbyonlyusing
Opε´2qsamplesfromtheN-playerdynamicgame,policiescanbeapproximatelyevaluatedon
theabstractMFGuptosymmetrizationerror.
5. Finally,weshowthatundermonotonicityconditions,policymirrordescent(PMD)combinedwith
r
TDlearningconvergestoanapproximateNashequilibriumusingOpε´6qsampletrajectoriesof
theN-playerDG.Thisprovidesanend-to-endlearningguaranteeforMARLunderα,β-symmetry,
characterizinganovelclassofproblemsthatcanbesolvedefficientlywithMARL.
2 MainResults
Notation. ForK PN ,letrKs:“t1,...,Ku. Let∆ betheprobabilitysimplexoverX. Forany
ą0 X
N P N define∆ :“ tµ P ∆ |Nµpxq P N ,@x P Xu. Forx P XN,definetheempirical
ą0 X,N X ř ě0
distributionσpxqP∆
X,N
asσpxqpx1q“1{N N i“11 xi“x1. LetS
K
bethesetofpermutationsover
thesetrKs, soS :“ tg : rKs Ñ rKs|gbijectiveu. Forx “ px ,...,x q P XK andg P S ,
K 1 K K
definegpxq :“ px ,...,x q P XK. Definex´i P XK´1 asthevectorwithi-thentryofa
gp1q gpKq
removed,andpx,x´iqPXK asthevectorwherei-thcoordinateofxhasbeenreplacedbyxPX.
Weconsiderdiscretestate-actionsetsS,A. Wedenotethesetoftime-dependentpoliciesonS,Aas
Π:“tπ :S ˆrHsÑ∆ u. Weabbreviateπ pa|sq:“πps,hqpaq. Forp:S Ñ∆ andρP∆ ,
A h A S
wedefinepρ¨pqP∆ aspρ¨pqps,aq:“ρpsqppsqpaqforalls,aPSˆA. Finally,wedefineentropy
ř S ř
Hpuq:“´ upaqlogupaqforuP∆ . WedenoteD pu|vq:“ upaqlogupaq foru,v P∆ .
a A KL a vpaq A
2.1 Finite-HorizonDynamicGames
Firstly,wedefinefinite-horizondynamicgames,themainobjectofinterestofthiswork.
Definition 1 (N-player FH-DG). An N-player finite-horizon dynamic game (FH-DG) is a tuple
pS,A,ρ ,N,H,tPiuN ,tRiuN qwherethestateandactionssetsS,Aarediscrete,ρ P∆ ,the
0 i“1 i“1 0 S
numberofplayersN P N ,horizonlengthH P N ,andtransitiondynamicsandrewardsare
ą1 ą0
functionssuchthatPi :SˆAˆpSˆAqN´1 Ñ∆ andRi :SˆAˆpSˆAqN´1 Ñr0,1s.
S
TheabovedefinitiondiffersfromMarkovgames[50],whereacommonstateissharedbyallagents.
InFH-DG,eachagentonlyobservestheirownstateandthedynamicsdependonthestatevectorof
allN agents. Suchamodelcanbeespeciallyrealisticincaseswheregameshavenaturallocality,
thatis,thegamestateisnotgloballyavailabletoagents. Next,wedefinetheNashequilibrium.
3Definition2(FH-DGNashequilibrium). ForaFH-DGG “ pS,A,ρ ,N,H,tPiuN ,tRiuN q,
0 i“1 i“1
policytupleπππ “pπ1,...,πNqPΠN theexpectedtotalrewardofagentiPrNsisdefinedas
« ˇ ff
Jpiqpπππq:“E
Hÿ´1 Ripsi,ai,ρρρ´iqˇ
ˇ ˇ@j:sj 0„ρ0, aj h„π hjpsj hq
h h h ˇsj „Pjp¨|sj,aj,ρρρ´jq
h“0
h`1 h h h
whereρρρ
h
:“`psi h,ai hq˘N i“1. Theexploitabilityofagentiforpoliciesπππ isthendefinedasEpiqpπππq :“
max Jpiq π,πππ´i ´Jpiqpπππq. Ifmax Epiqpπππq“0,πππiscalledaNashequilibrium(NE)ofthe
πPΠ i
FH-DG.Ifmax Epiqpπππqďδ,πππiscalledaδ-Nashequilibrium(δ-NE)oftheFH-DG.
i
Ataδ-NE,theincentiveforanyselfishagenttodeviateissmall: hence,approximateNEisanatural
solution concept for FH-DG. However, the problem of finding δ-NE is challenging: not only is
itcomputationallyintractableingeneral[15], butforlargeN thesearchspaceofpoliciesgrows
exponentially. Thismotivatestheapproximationviasymmetrizationintheremainderofthework.
2.2 SymmetrizationandLipschitzExtension
Inordertodefineapproximatesymmetry,wefirstshowthatfinite-agentdynamicsofDefinition1can
beextendedtoinfinitelymanyplayers. Intheprocess,wetackleaquestionthatisrelevantforMFGs
beyondourwork:WhenandhowcanwebuildanMFGmodelonthecontinuum,givendynamicson
finitelymanyplayers? WewillusethenotionsofsymmetrizationandLipschitzextension.
Definition3(Symmetricfunction,symmetrization). Afunctionf :XK ÑY iscalledsymmetricif
fpgpxqq“fpxq,@xPXK,g PS . Forasymmetricf :XK ÑY,wedefineitspopulationlifted
K
s s
versionf :∆ ÑY asthewell-definedfunctionsuchthatfpµq“fpxqfor@xPXK satisfying
X,K
σpxq“µ.Givenf :XK ÑRD,wedefinethesymmetrizationSympfq:XK ÑY as
ÿ
1
Sympfqpxq“ fpgpaqq, @aPXK.
K!
gPS
K
Ę Ğ
WealsodenoteSympfq:“Sympfq.
We note that the terminology “symmetrization” is consistent as Sympfq is indeed a symmetric
function(asverifiedinSectionA).Furthermore,iff issymmetricthenSympfq“f asexpected.
Finally,toextendDGtotheinfinite-playercontinuum,wewillusethefollowingspecialcaseofthe
Kirszbraun-Valentinetheorem,whichconcernsLipschitzextensionsoffunctionsfromstrictsubsets
oftheEuclideanspacetotheentiretyofthespacepreservingtheirLipschitzmodulus.
Lemma1(Kirszbraun-Valentine[27,55]). Letd 1,d
2
PN ą0,andU ĂRd1. Letf :U ÑRd2 bean
L-LipschitzfunctionwithrespecttotheEuclideannorm}¨} 2.Then,thereexistsExtpfq:Rd1 ÑRd2
suchthatExtpfqisL-LipschitzandExtpfqpxq“fpxqforallxPU.
While Extpfq is not unique in general, it admits various explicit formulations [36, 53], and the
particularformulationisnotimportantinthiswork.
2.3 Mean-fieldGamesandα,β-SymmetricGames
Next,usingthedefinitionsfromtheprevioussection,weshowhowtheFH-DGcanbeextendedtoan
MFG.Weformalizethefinite-horizonMFG(FH-MFG),whichwillbethemainapproximationtool.
Definition4(Finite-horizonmean-fieldgame). Afinite-horizonmean-fieldgame(FH-MFG)isa
tuplepS,A,ρ ,H,P,RqwhereS,Aarediscrete,ρ P∆ ,H PN ,thetransitiondynamicsP is
0 0 S ą0
afunctionP :SˆAˆ∆ Ñ∆ ,andtherewardRisafunctionR:SˆAˆ∆ Ñr0,1s.
SˆA S SˆA
Compared to Definition 1, Definition 4 introduces two conceptual changes under the premise of
exact symmetry: (1) the dependency of dynamics to the states and actions of other agents have
beenreducedtoadependencyonapopulationdistributionon∆ ,and(2)N agentshavebeen
SˆA
implicitlyreplacedbyasinglerepresentativeagent. WenextextendthedefinitionNEtoMFGs.
4Definition5(Inducedpopulation,MFG-NE). ForaFH-MFGdefinedbythetuplepS,A,ρ ,H,P,Rq,
0
wedefinethepopulationupdateoperatorsΓ,Λas
ÿ
Γpµ,πqps1,a1q:“ µps,aqPps1|s,a,µqπpa1|s1q (1)
sPS,aPA
␣ (
H´1
Λpπq:“ Γp¨¨¨ΓpΓpρ ¨π ,π q¨¨¨ ,π qq . (2)
0 0 1 h´1 h“0
Forπ PΠandµ“tµ uH´1 P∆H ,theexpectedrewardisdefinedas
h h“0 SˆA
« ˇ ff
Hÿ´1 ˇ
ˇ
V pµ,πq:“E Rps ,a ,µ qˇs0„ρ0, ah„πhpshq . (3)
h h h ˇ sh`1„Ppsh,ah,µhq
h“0
WedefineMFGexploitabilityasEpπq:“max VpΛpπq,π1q´VpΛpπq,πqandFH-MFG-NEas:
π1PΠ
Policyπ˚ “tπ˚uH´1 PΠsuchthat Epπ˚q“0. (MFG-NE)
h h“0
Intuitively,theabovedefinitionofMFG-NErequiresthatthepolicyπisoptimalagainstthepopulation
flow it induces. Questions of existence [9, 4, 23] and approximation of the FH-DG under exact
symmetry[46]havebeenthoroughlystudiedintheliterature. Thatis,ifanN-playergameexhibits
exactsymmetry,thentheMFG-NEexistsandisanapproximateNEoftheFH-DG.
Takingaconstructiveapproach,weshowthattheFH-MFG-NEofanappropriatelyconstructedMFG
isalsoanapproximateNEforagivenFH-DGwithoutapriormodel. Thedefinitionbelowofan
“inducedMFG”demonstrateshowarbitrarynon-symmetricdynamicscanbeextendedtoanMFG.
Definition6(InducedFH-MFG). LetG “ pS,A,ρ ,N,H,tPiuN ,tRiuN qbeaFH-DG.The
0 i“1 i“1
MFGinducedbyG,denotedMFGpGq,isdefinedtobethepS,A,ρ ,H,P,Rq,whereP :SˆAˆ
0
∆ Ñ∆ andR:SˆAˆ∆ Ñr0,1saredefinedforallsPS,aPA,µP∆ as:
SˆA S SˆA SˆA
` ` ˘˘ ` ` ˘˘
ÿN Ext SĘ ym Pips,a,¨q pµq ÿN Ext SĘ ym Rips,a,¨q pµq
Pps,a,µq:“ , Rps,a,µq:“ .
N N
i“1 i“1
MFGpGq is well-defined due to Lemma 1. In words, the definition of MFGpGq consists of two
Ę
mainoperations: (1)symmetrize(Symp¨q)andextend(Extp¨q)Pi,Ri to∆ ,and(2)average
SˆA
symmetrizeddynamicsandrewardsforallplayers. Furthermore,inthespecialcasePi “Pj and
Ri “ Rj for`all˘i ‰ j a`nd˘Pips,a,¨q,Rips,a,¨q are symmetric, the MFGpGq has dynamics and
s s
rewardsExt P1 ,Ext R1 ,whicharesimplytheLipschitzextensionsofP1,R1tothecontinuum.
Remark1. Evenforexactsymmetricgames,Definition6isrelevant. TheavailabilityofanMFG
modelistypicallytakenforgranted, however, sincereal-worldalgorithmsmightonlybeableto
accessfinite-agentdynamicswithoutaknownMFGmodel,itisavalidresearchquestionwhenand
howagamecanbemeaningfullyextendedtoinfiniteplayers(answeredbyDefinition6).
Finally,weprovidethedefinitionofapproximateorα,β-symmetry.
Definition7(α,β-SymmetricDG). LetG “ pS,A,ρ ,N,H,tPiuN ,tRiuN qbeanN-player
0 i“1 i“1
FH-DG,inducingMFGpGq“pS,A,ρ ,H,P,Rq. Ifitholdsforα,β ě0that
0
max max }Pips,a,ρρρq´Pps,a,µq} ďα,
1
iPrNs,s,aPSˆAρρρPpSˆAqN´1
µP∆SˆA σpρρρq“µ
max max |Rips,a,ρρρq´Rps,a,µq|ďβ,
iPrNs,s,aPSˆAρρρPpSˆAqN´1
µP∆SˆA σpρρρq“µ
thentheFH-DGG iscalledα,β-symmetric.
Asexpected,anexactlysymmetricN-playergameisalso0,0-symmetric,andanydynamicgameGis
α,β-symmetricforsomeconstantsαď2,β ď1. Hence,Definition7generalizesexactpermutation
invariance. Gamesthatexhibitnear-exactsymmetrieswillhaveverysmallconstantsα,β,wewill
nextprovideapproximationandlearningguaranteesforsuchfinite-agentgames.
52.4 ApproximationofNEunderApproximateSymmetry
Inthissection,wewillprovethataNEoftheinducedMFGpGqisalsoanapproximateNEofthe
finite-agentgameG. Wewillprovideanexplicitboundontheapproximation,motivatingtheuseof
MFGsforsolvingFH-DG.
Wefirstintroducethenotionofκ-sparsedynamics. Inwords,withκ-sparsedynamicsanagentat
statesplayingactionaisimpactedonlybyotheragentsoccupyingasparsesetof“neighboring”
state-actions N Ă S ˆ A where |N | ď κ. For a subset U Ă X, we define the function
s,a s,a
p : X Ñ X YtKu as p pxq “ x if x P U and p pxq “K otherwise, where K is treated as a
U U U
placeholderelementsuchthatKRU.
Definition8(κ-sparsedynamics/rewards). Afunctionf : XM Ñ Y iscalled κ-sparse(onsome
U Ă X)if|U| ď κandfpxq “ fpyqwheneverp px q “ p py qforalli “ 1,...,M. Dynamics
U i U i
tPiuN (resp. rewardstRiuN )arecalledκ-sparseifallPips,a,¨q(resp. Rips,a,¨q)areκ-sparse
i“1 i“1
onsomeU ĂSˆAforallsPS,aPA(resp. U ĂSˆAforallsPS,aPA).
s,a s,a
SparsityiscommoninFH-DG,particularlywhenthereisspatialstructure. ManystandardMFG
problemssuchasthebeach-barproblem[42]andcrowdmodeling[63]areinfactpκ“1q-sparse,as
agentsareonlyaffectedbythepopulationdistributionattheircurrentstate.
Usingsparsity,weprovideanupperboundoftheLipschitzconstantsofmapsPps,a,¨q,Rps,a,¨qof
theinducedMFGonthecontinuum∆ ,demonstratingthatunlesstheFH-DGexhibitsdominant
SˆA
players,P,RhaveboundedLipschitzmoduliindependentofN.
Lemma 2 (Lipschitz extension bound). Let G be an FH-DG with dynamics and rewards
tPiuN ,tRiuN admitting the induced mean-field game MFGpGq with dynamics and rewards
i“1 i“1
P,R. AssumethattPiuN ,tRiuN areκ-sparseanditholdsthat
i“1 i“1
}Pips,a,ρρρq´Pips,a,pps1,a1q,ρρρ´jqq} ďC , |Rips,a,ρρρq´Rips,a,pps1,a1q,ρρρ´jqq|ďC ,
1 1 2
foranyi,j PrNs,i‰j,s,s1 PS,a,a1 PAandρρρPpSˆAqN´1forsomeconstantsC ,C . Then,
? 1 2
theinducedP,RhaveLipschitzmodulusatmostC NκandC N κrespectively,thatis,
1 2
?
}Pps,a,µq´Pps,a,µ1q} ďC Nκ}µ´µ1} , |Rps,a,µq´Rps,a,µ1q|ďC N κ}µ´µ1} ,
2 1 2 2 2
foranysPS,aPA,µ,µ1 P∆ .
SˆA
TheabovetheoremcharacterizesaconditionontheoriginalFH-DGfortheinducedFH-MFGtohave
smooth(Lipschitz)dynamics. Thetheoremsuggeststhatthegamemusthavenodominantplayersso
thattheeffectofeachagentonothersisupperboundedoforderOp1{Nq. Furthermore,bystandard
resultsinMFGliterature,ifthe“nodominantplayers”conditionofLemma2holds,thepopulation
updateΓisalsoLipschitzcontinuouswithsomemodulusL thatisindependentofN.
pop,µ
Finally,westatethemainapproximationresult,whichquantifieshowcloselythetrueN-playergame
Nashequilibriumcanbeapproximatedbythemean-fieldNashequilibriumofthesymmetrizedgame.
Theorem1. LetG “pS,A,ρ ,N,H,tPiuN ,tRiuN qbeanN-playerFH-DGandMFGpGq“
0 i“1 i“1
pS,A,ρ ,H,P,Rq. LettheLipschitzmodulusofthepopulationupdateoperatorΓinµbeL . If
0 pop,µ
π˚ PΠisaMFG-NEofMFGpGq,thenpπ˚,...,π˚qPΠN isanϵ-NEoftheFH-DG,where
˜ ¸
H2p1´LH q 1´LH
ϵ“O pop?,µ `αH2 pop,µ `βH .
p1´L pop,µq N 1´L pop,µ
Proof. (sketch) We show that (1) the empirical distribution of agent state-actions over S ˆ A
approximatestheinducedmean-fieldΛpπ˚q,(2)marginaldistributionsofstatesofanagentPrsi “¨s
h
in FH-DG are also approximated by the mean-field, and (3) explicitly bounding the difference
betweenV andJpiq. TheformalproofandexplicitupperboundarepresentedinSectionB.3.
Mostcritically,theapproximationboundprovesthattheMFGapproximationisrobusttosmallhet-
erogeneity: whenα,β aresmall,theinducedMFG-NEapproximatesthetrueNEwell. Furthermore,
theupperboundsuggeststhreedifferentasymptoticregimesdependingonΓbeingnon-expansive,
contractive,orexpansive. IfL ď1,theboundaboveispolynomial. IfL ą1,αą0might
pop,µ pop,µ
incuranexponentialdependencyonH,whereastheerrorduetoβ ą 0onlyscaleslinearlywith
6OpβHq. However,theexponentialworst-casedependenceofthebiasonH isgenerallyunavoidable
evenunderperfectsymmetry,asmatchinglowerboundsareknown[62]. Theorem1alsorecovers
theboundsknownforexactlysymmetricFH-DG(i.e. α“β “0,see[62]).
Finally,weemphasizethatTheorem1doesnotassumeanyparticularstructureontheFH-DG:the
resultsapplyforanyvaluesofα,β,althoughthequalityofapproximationwillvary.Furthermore,itis
knownthatforN ą2,findinganϵ-NEfortheFH-DGisPPAD-completeevenforacertainabsolute
constantϵ[18]. Hence,evenwhenα,βarenotcloseto0,theresultwillbeusefulinapproachingthe
PPAD-completelimitviamean-fieldapproximation.
Theresultssofaralreadysuggestalearningalgorithm: onecanestimate(e.g. vianeuralnetworks)
theinducedP,RandsolvetheMFGdirectlywithstandardalgorithms(e.g. [42,31]). However,this
methodcanbeprohibitivelyexpensiveasitinvolveslearningfunctionstoandfrom∆ . The
SˆA
remainderofthepaperwillbededicatedtoformulatingmoreefficientmethods.
2.5 PolicyEvaluationwithα,β-Symmetry
Inthissection,weanalyzeTDlearningforα,β-symmetricFH-DG.WhileDefinition6providesan
explicitconstructionofanMFG,weshowthatthisconstructionisnotneededforpolicyevaluation.
Namely,usingTDlearning,apolicyπ canbeevaluatedwithrespecttothe(induced)mean-field
ΛpπqonlythroughsamplingtrajectoriesoftheFH-DGG. WefirstdefineQfunctionsontheMFG.
Definition9(Mean-fieldQvalues). FortheMFGpS,A,ρ ,H,P,Rq,forτ ě0,h“0,...,H´1,
0
wedefine(entropyregularized)Q-valuesforeachh“0,...,H ´1andsPS,aPAas
« ˇ ff
Hÿ´1 ˇ
ˇ
Qτ,πps,aq:“E Rps ,a ,µ q`τHpπ p¨|s qqˇ sh“s,ah“a,s h1`1„Pps h1,a h1,µ h1q, .
h h1 h1 h1 h1 h1 ˇa h1„π h1`1ps h1`1q,µ h1:“Λpπq h1,@h1ěh
h1“h
Inotherwords,theQ-valuesofapolicyπ arecomputedwithrespecttotheMDPinducedbythe
population distributions Λpπq in the MFG. We note that the above definition does not match the
typicaldefinitionofQ-valuesinamulti-agentsetting,andratherisdefinedconcerninganabstract
MFG.WenotethatwewilloccasionallytreatQτ,π asanelementofthevectorspaceRSˆA.
h
Forthefinite-horizonproblem,wewillanalyzeTDlearning,whichisastandardmethodforpolicy
evaluationwithestablishedguaranteesbeyondMFGs[54]. WeformulateAlgorithm1,presentedfor
simplicityasperformingTDlearningonagent1.
Algorithm1TDLearningforα,β-symmetricgames
Input: FH-DGG,epochsM,learningratestη u ,policyπ PΠ,entropyregularizationτ ě0.
m m
p
1: Q0ps,aqÐ0, @hPt0,...H ´1u,sPS,aPA
h
2: formP0,1,...,M ´1do
3: Usingπforallagents,samplepathfromG: tρρρ m,h,r m,hu hH “´ 01 :“tsi m,h,ai m,h,r mi ,hu i,h.
4: PerformTDupdate:
`
Qpm`1 ÐQpm`η Qpm ps1 ,a1 q`r1 `τHpπ p¨|s1 qq
h h m h`1 m˘,h`1 m,h`1 m,h h m,h
´Qpmps1 ,a1 q e , @hăH ´1
h m,h m,h s1 ,a1
m,h m,h
Qpm`1 ÐQpm `η pτHpπ p¨|s1 qq`r1 ´Qpm ps1 ,a1 qqe
H´1 H´1 m H´1 m,H´1 m,H´1 H´1 m,h m,h s1 m,H´1,a1
m,H´1
5: ReturntQp MuH´1.
h h“0
Theorem 2 (TD learning for α,β-Symmetric Games). Let G be an N-player FH-DG and
MFGpGq be its induced MFG. Let π P Π be a policy such that Λpπq “ µ “ tµ u and
h h
δ :“ inf Prs1 “ s,a1 “ ss.AssumeAlgorithm1isrunwithπ forM ą 0
h,s,as.t. Prs1“s,a1“ssą0 h h
epochsforwithleah rningh ratesη :“ 2δ´1 . Then, theoutputtQp Mu ofAlgorithm1satisfies
” ř ı m` m`2δ´1 ˘ h h
E H´1 }Qp M ´Qτ,π}2 ď O 1 ` 1 `α2`β2 ,where}¨} isdefinedforp P ∆ as
h“0b řh h µh M N p SˆA
}q} :“ pps,aqqps,aq2.
p s,a
7Theorem2providesafinite-sampleguaranteeforTDlearning, abuildingblockofmanyMARL
andMFGalgorithms. Mostimportantly,itsuggeststhatinordertousemean-fieldgametheoryto
approximateNEofanFH-DGG,thereisnoneedtoexplicitlybuildamodelofMFGpGq. Instead,
TDlearningintheoriginalN-playergamewhenalltheagentspursuepolicyπallowstheevaluation
ofthemean-fieldQ-valuesofπefficiently. TherateofconvergencesuggestedbyTheorem2matches
theoptimalknownratesforTD-learninginasingle-agentsetting[28]. Inpractice,onecanusethe
trajectoriesofallN agentstofurtherimproveefficiency,insteadofonlyusingthatofagenti“1.
2.6 LearningNEunderα,β-Symmetry
Wecompleteourframeworkbyprovidingourkeytheoreticalresult: anyα,β-symmetricDGcanbe
solvedapproximatelyonlyusingsamplesfromtheN-playerDG,undermonotonicityassumptions.
OuralgorithmusesTDlearningasabuildingblock,withstochasticpolicyevaluationsusedforpolicy
mirrordescentupdates[29,60,64].
Definition10(MonotoneMFG[42,41]). AMFGwithdynařmicsP andrewardsRiscalledmonotone
if P is independent of µ, and for all µ,µ1 it holds that pRps,a,µq´Rps,a,µ1qqpµps,aq´
s,a
µ1ps,aqqă0. ADGG iscalledmonotoneextendableifMFGpGqismonotone.
Tomotivatethisdefinition,weprovidealargeclassofDGsthatarerelevantandmonotone-extendable.
Example1(Asymmetricdynamiccongestiongames). Foranyi P rNs,leth : S ˆAˆrNs Ñ
i
r0,1s,ri : S ˆA Ñ r0,1sbearbitraryfunctionssothath ps,a,¨qisnon-increasingforanys,a.
i
AssumePip¨|s,a,ρρρ´iqdoesnotdependonρρρ´i foranys,a,andRips,a,ρρρ´iqbe1-sparsesothat
ř
Rips,a,ρρρ´iq“h ps,a, N 1 q`r ps,aq. Suchgamescanbeseenasgeneralizationsof
i j“1 ρj“ps,aq i
congestion games [45] and congestion games with player-specific incentives [38], for which an
efficientsolutionisunknown. Weprovemonotoneextendabilityandcharacterizethevaluesofα,β
andLipschitzconstantsforsuchgamesinSectionD.1.
Algorithm2Policymirrordescentforα,β-symmetricgames(Symm-PMD)
Input: FH-DGG,epochsT,TDlearningepochsM,learningratestξ u ,entropyτ.
t t
1: Initializeuniformpolicy: π 0,hpa|sq“1{|A|, @hPt0,...H ´1u,sPS,aPA
2: fortP0,1,...,T ´1do ŹRunforT epochs
3: RunAlgorithm1forpolicyπ t,M epochs,entropyτ,tη mu masinTheorem2
4: ObtaintQp t huH h“´ 01, setqp htps,aq:“Qp t hps,aq´τHpπ t,hp¨|sqq.
5: PerformPMDupdate: (@sPS,h“0,...,H ´1)
« ff
ξ
πp p¨|sq:“argmax t qptps,¨qJu`τHpuq ´D pu|π p¨|sqq.
t`1,h 1´τξ h KL t,h
uP∆A t
´ ¯
6: Updatepolicies ř: π t`1,hp¨|sq:“ 1´ t`1
1
πp t`1,hp¨|sq` t`1 1Unifp¨q, @sPS.
7: Returnπ¯ :“t T`1
1
T t“0π t,huH h“´ 01.
Theorem3(ConvergenceofPMD). LetG beamonotoneextendableα,β-symmetricgame. Assume
Symm-PMD(Algorithm2)runswithlearningratesξ t “ ? t1 `1,entropyregularizationτ Pp0,1{2q,
r
withM ěOpε´2qTDiterationsforT ěOpε´4qepochs. Then,theoutputpolicyπ¯ isaOpετ´1`
ατ´1`βτ´1`τ´1{?
N`τq-NashequilibriumofG inexpectation.
Proof. Theproofisbasedon[64]withtheaddedcomplicationsoffinitelymanyagents,approximate
symmetry,andstochasticTDlearning. FullproofispresentedinSectionD.3.
r
Theorem3suggestsasamplecomplexityofOpε´6qtrajectoriesfromtheN-agentFH-DGinorder
tocomputeaε-NE(uptosymmetrizationbias). Infact,itis(tothebestofourknowledge)thefirst
finite-sampleguaranteeforcomputingapproximateNEforalargeclassofdynamicgameswithmany
agents. Mostimportantly,thenumberofagentsN doesnotappearinthecomplexity: hence,the
curseofmanyagentscanbeprovablycircumventedforα,β-symmetricgames. Evenintheexactly
symmetric case (α “ β “ 0), Theorem 3 is the first guarantee to the best of our knowledge for
learningFH-MFG-NEonlyobservingtrajectoriesoftheN-agentgame.
83 ExperimentalResults
WesupportourtheorybydeployingSymm-PMD(Algorithm2)onseverallarge-scaleα,β-symmetric
games. Forevaluations,wemodifythewell-knownbenchmarksfromMFGliterature(see[13])to
proposethreegameswithasymmetricincentives:A-RPS,A-SIS,andA-Taxi.A-RPSisanadaptation
ofRPS[13]toincorporateasymmetricrewardsforagents. A-SISmodelsdiseasepropagationina
largepopulationindividuallychoosingtoself-isolateorgoout,incorporatingasymmetricagentswith
individualsusceptibility/healingratesanduniqueaversionstoisolation. Finally,A-Taxisimulatesa
largepopulationoftaxisservingclientsinagrid,withindividualpreferencesforregionsandcrowd
aversion. Inourexperiments,weuseN “ 1000andN “ 2000agentsdemonstratingtheability
ofourframeworktohandlelargeMARLgames. A-Taxiincorporates|S| ą 230,H “ 128,hence
necessitatesneuralparameterization. OursetupisthoroughlydescribedinSectionE.
WedeploySymm-PMDontwodifferentDGs,withα“0,β «0.1,N “2000,H “10onA-RPS
and α « β « 0.1,N “ 1000,H “ 20 for A-SIS. We compare the symmetrized approach of
Symm-PMDtoitsasymmetriccounterpartindependentPMD(IPMD),whereaseparatepolicyis
learnedforeachagent. Thetrainingcurves,picturedinFigures1-(b,c)characterizetheexploitabilty
ofthelearnedpoliciesthroughouttraining. Inbothcases,whileIPMDhasnoapproximationbiasin
principle,itstrugglestoconvergepresumablysufferingfromthecurse-of-many-agents. Symm-PMD,
ontheotherhand,rapidlyconvergestoapolicyprofilewithlowexploitabilityandismuchmore
sample-efficient. Inbothcases,Symm-PMDconvergestoasolutionwithlowbias.
Wedemonstratethescalabilityofourapproachwithneuralpolicies. IntheA-Taxienvironment,we
usePPO[49]withsymmetrizedneuralpoliciesandcomparetothesettingsthepolicyhasaccessto
agentidentities(eitherone-hotencoded,inOH-NN,orasaninteger,inID-NN).Symmetrizedpolicies
outperformeitherbenchmarkbyconvergingfasterandtoabettersolution. Learningindependent
neuralpoliciesforeachof1000agents(Ind-NN)isextremelyexpensiveinthissetting: thisapproach
performstheworstandisordersofmagnitudecomputationallymoreexpensive.
Computationalefficiency. Wealsoemphasizethecomputationalefficiencyofsymmetrization: since
ouralgorithmneednotlearnseparatepoliciesforeachagent,itisdrasticallymorecomputationally
efficientcomparedtoindependentPMD.InA-SISandA-RPSbenchmarks,learningis 60%faster,
whereassymmetrizedneuralPPOinA-Taxiis>95%fasterthanitsindependentcounterpart.
(a) (b) (c)
Figure1: (a)Themeanrewardsthroughouttrainingofsymmetricpolicies(Sym-NN),policieswith
onehotencodingfori(OH-NN),policieswithnumericalencodingfori(Ind-NN)andindependent
policies(Ind-NN)inA-Taxi. (b,c)TheexploitabilitythroughoutmultipleepochsofSymm-PMD
(Algorithm2)andIPMD,forA-RPSwithβ “0.1in(b)andA-SISwithα“β “0.1in(c).
4 DiscussionandConclusion
We formulated a new class of competitive MARL problems (α,β-symmetric games) that can be
tractablysolved. Weconstructivelyshowedthateveryα,β-symmetricFH-DGcanbeefficiently
approximatedbyaninducedMFG.WeprovidedtheoreticalguaranteesforTDlearning,andunder
monotonicity, for PMD to approximate NE up to symmetrization bias. These results provide a
completetheoryoflearningunderapproximatesymmetry,supportedbynumericalexperiments.
9AcknowledgmentsandDisclosureofFunding
ThisprojectissupportedbySwissNationalScienceFoundation(SNSF)undertheframeworkof
NCCRAutomationandSNSFStartingGrant.
References
[1] B. Anahtarci, C. D. Kariksiz, and N. Saldi. Q-learning in regularized mean-field games.
DynamicGamesandApplications,pages1–29,2022.
[2] A. Aurell, R. Carmona, G. Dayanıklı, and M. Laurière. Finite state graphon games with
applicationstoepidemics. DynamicGamesandApplications,12(1):49–81,2022.
[3] A.Aurell,R.Carmona,andM.Lauriere. Stochasticgraphongames: Ii.thelinear-quadratic
case. AppliedMathematics&Optimization,85(3):39,2022.
[4] A.Bensoussan,J.Frehse,P.Yam,etal. Meanfieldgamesandmeanfieldtypecontroltheory,
volume101. Springer,2013.
[5] A.Bensoussan,T.Huang,andM.Lauriere. Meanfieldcontrolandmeanfieldgamemodels
withseveralpopulations. arXivpreprintarXiv:1810.00783,2018.
[6] Q.Cai,Z.Yang,C.Jin,andZ.Wang. Provablyefficientexplorationinpolicyoptimization. In
InternationalConferenceonMachineLearning,pages1283–1294.PMLR,2020.
[7] P. E. Caines and M. Huang. Graphon mean field games and the gmfg equations: ε-nash
equilibria. In2019IEEE58thconferenceondecisionandcontrol(CDC),pages286–292.IEEE,
2019.
[8] O.Candogan,A.Ozdaglar,andP.A.Parrilo. Near-potentialgames: Geometryanddynamics.
ACMTrans.Econ.Comput.,1(2),may2013. ISSN2167-8375. doi: 10.1145/2465769.2465776.
URLhttps://doi.org/10.1145/2465769.2465776.
[9] P.Cardaliaguet. Notesonmeanfieldgames. Technicalreport,Technicalreport,2010.
[10] R. Carmona and F. Delarue. Probabilistic analysis of mean-field games. SIAM Journal on
ControlandOptimization,51(4):2705–2734,2013.
[11] R.Carmona,F.Delarue,etal. ProbabilistictheoryofmeanfieldgameswithapplicationsI-II.
Springer,2018.
[12] S. Cayci, N. He, and R. Srikant. Linear convergence of entropy-regularized natural policy
gradientwithlinearfunctionapproximation. arXivpreprintarXiv:2106.04096,2021.
[13] K.CuiandH.Koeppl. Approximatelysolvingmeanfieldgamesviaentropy-regularizeddeep
reinforcementlearning. InInternationalConferenceonArtificialIntelligenceandStatistics,
pages1909–1917.PMLR,2021.
[14] K.CuiandH.Koeppl. Learninggraphonmeanfieldgamesandapproximatenashequilibria.
arXivpreprintarXiv:2112.01280,2021.
[15] C.Daskalakis,P.W.Goldberg,andC.H.Papadimitriou. Thecomplexityofcomputinganash
equilibrium. CommunicationsoftheACM,52(2):89–97,2009.
[16] C.Daskalakis,N.Golowich,andK.Zhang. Thecomplexityofmarkovequilibriuminstochastic
games. InTheThirtySixthAnnualConferenceonLearningTheory,pages4180–4234.PMLR,
2023.
[17] G.DayanikliandM.Lauriere. Multi-populationmeanfieldgameswithmultiplemajorplayers:
Applicationtocarbonemissionregulations. arXivpreprintarXiv:2309.16477,2023.
[18] P.W.Goldberg. Asurveyofppad-completenessforcomputingnashequilibria. arXivpreprint
arXiv:1103.2709,2011.
10[19] X. Guo, A. Hu, R. Xu, and J. Zhang. Learning mean-field games. Advances in Neural
InformationProcessingSystems,32,2019.
[20] X. Guo, A. Hu, R. Xu, and J. Zhang. A general framework for learning mean-field games.
MathematicsofOperationsResearch,2022.
[21] X.Guo,A.Hu,M.Santamaria,M.Tajrobehkar,andJ.Zhang. MFGLib: Alibraryformean
fieldgames. arXivpreprintarXiv:2304.08630,2023.
[22] X.Guo,X.Li,C.Maheshwari,S.Sastry,andM.Wu. Markovα-potentialgames: Equilibrium
approximationandregretanalysis. arXivpreprintarXiv:2305.12553,2023.
[23] J. Huang, B. Yardim, and N. He. On the statistical efficiency of mean field reinforcement
learningwithgeneralfunctionapproximation. arXivpreprintarXiv:2305.11283,2023.
[24] J.Huang,N.He,andA.Krause. Model-basedrlformean-fieldgamesisnotstatisticallyharder
thansingle-agentrl,2024.
[25] M.Huang,R.P.Malhamé,andP.E.Caines.Largepopulationstochasticdynamicgames:closed-
loopmckean-vlasovsystemsandthenashcertaintyequivalenceprinciple. Communicationsin
Information&Systems,6(3):221–252,2006.
[26] S.Huang,R.F.J.Dossa,C.Ye,J.Braga,D.Chakraborty,K.Mehta,andJ.G.AraÃšjo. Cleanrl:
High-qualitysingle-fileimplementationsofdeepreinforcementlearningalgorithms. Journalof
MachineLearningResearch,23(274):1–18,2022.
[27] M.Kirszbraun. Überdiezusammenziehendeundlipschitzschetransformationen. Fundamenta
Mathematicae,22(1):77–108,1934.
[28] G.Kotsalis,G.Lan,andT.Li. Simpleandoptimalmethodsforstochasticvariationalinequal-
ities,ii: Markoviannoiseandpolicyevaluationinreinforcementlearning. SIAMJournalon
Optimization,32(2):1120–1155,2022.
[29] G.Lan. Policymirrordescentforreinforcementlearning: Linearconvergence,newsampling
complexity,andgeneralizedproblemclasses. Mathematicalprogramming,198(1):1059–1106,
2023.
[30] J.-M.LasryandP.-L.Lions. Meanfieldgames. Japanesejournalofmathematics,2(1):229–260,
2007.
[31] M. Laurière, S. Perrin, S. Girgin, P. Muller, A. Jain, T. Cabannes, G. Piliouras, J. P’erolat,
R.Elie,O.Pietquin,andM.Geist. Scalabledeepreinforcementlearningalgorithmsformean
fieldgames. InInternationalConferenceonMachineLearning,2022.
[32] M. Laurière, S. Perrin, J. Pérolat, S. Girgin, P. Muller, R. Élie, M. Geist, and O. Pietquin.
Learninginmeanfieldgames: Asurvey,2024.
[33] S.Leonardos,W.Overman,I.Panageas,andG.Piliouras. Globalconvergenceofmulti-agent
policygradientinmarkovpotentialgames. arXivpreprintarXiv:2106.01969,2021.
[34] W.Mao,H.Qiu,C.Wang,H.Franke,Z.Kalbarczyk,R.Iyer,andT.Basar. Amean-fieldgame
approachtocloudresourcemanagementwithfunctionapproximation. InAdvancesinNeural
InformationProcessingSystems,2022.
[35] L. Matignon, G. J. Laurent, and N. Le Fort-Piat. Hysteretic q-learning: an algorithm for
decentralized reinforcement learning in cooperative multi-agent teams. In 2007 IEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems,pages64–69.IEEE,2007.
[36] E.J.McShane. Extensionofrangeoffunctions. 1934.
[37] J.Mei,C.Xiao,C.Szepesvari,andD.Schuurmans. Ontheglobalconvergenceratesofsoftmax
policygradientmethods. InInternationalConferenceonMachineLearning,pages6820–6829.
PMLR,2020.
11[38] I.Milchtaich. Congestiongameswithplayer-specificpayofffunctions. Gamesandeconomic
behavior,13(1):111–124,1996.
[39] F.PariseandA.Ozdaglar. Graphongames. InProceedingsofthe2019ACMConferenceon
EconomicsandComputation,pages457–458,2019.
[40] J. Perolat, B. Scherrer, B. Piot, and O. Pietquin. Approximate dynamic programming for
two-playerzero-summarkovgames. InInternationalConferenceonMachineLearning,pages
1321–1329.PMLR,2015.
[41] J.Pérolat,S.Perrin,R.Elie,M.Laurière,G.Piliouras,M.Geist,K.Tuyls,andO.Pietquin.
Scalingmeanfieldgamesbyonlinemirrordescent. InProceedingsofthe21stInternational
ConferenceonAutonomousAgentsandMultiagentSystems,pages1028–1037,2022.
[42] S.Perrin,J.Pérolat,M.Laurière,M.Geist,R.Elie,andO.Pietquin. Fictitiousplayformean
field games: Continuous time analysis and applications. Advances in Neural Information
ProcessingSystems,33:13199–13213,2020.
[43] S.Perrin,M.Laurière,J.Pérolat,R.Élie,M.Geist,andO.Pietquin. Generalizationinmean
fieldgamesbylearningmasterpolicies. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume36,pages9413–9421,2022.
[44] N.Rashedi,M.A.Tajeddini,andH.Kebriaei. Markovgameapproachformulti-agentcompeti-
tivebiddingstrategiesinelectricitymarket. IETGeneration,Transmission&Distribution,10
(15):3756–3763,2016.
[45] R. W. Rosenthal. A class of games possessing pure-strategy nash equilibria. International
JournalofGameTheory,2(1):65–67,1973.
[46] N. Saldi, T. Basar, and M. Raginsky. Markov–nash equilibria in mean-field games with
discountedcost. SIAMJournalonControlandOptimization,56(6):4256–4287,2018.
[47] M. Samvelyan, T. Rashid, C. Schroeder de Witt, G. Farquhar, N. Nardelli, T. G. J. Rudner,
C.-M.Hung,P.H.S.Torr,J.Foerster,andS.Whiteson. Thestarcraftmulti-agentchallenge.
InProc.ofthe18thInternationalConferenceonAutonomousAgentsandMultiagentSystems
(AAMAS2019),AAMAS’19,page2186–2188,Richland,SC,2019.InternationalFoundation
forAutonomousAgentsandMultiagentSystems.
[48] J.Schulman,P.Moritz,S.Levine,M.Jordan,andP.Abbeel. High-dimensionalcontinuous
controlusinggeneralizedadvantageestimation. arXivpreprintarXiv:1506.02438,2015.
[49] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov. Proximalpolicyoptimization
algorithms,2017.
[50] L.S.Shapley. Stochasticgames. Proceedingsofthenationalacademyofsciences, 39(10):
1095–1100,1953.
[51] A. Shavandi and M. Khedmati. A multi-agent deep reinforcement learning framework for
algorithmictradinginfinancialmarkets. ExpertSystemswithApplications,208:118124,2022.
[52] S.G.Subramanian,P.Poupart,M.E.Taylor,andN.Hegde.Multitypemeanfieldreinforcement
learning. In Proceedings of the 19th International Conference on Autonomous Agents and
MultiAgentSystems,AAMAS’20,page411–419,Richland,SC,2020.InternationalFoundation
forAutonomousAgentsandMultiagentSystems. ISBN9781450375184.
[53] A.Sukharev. Optimalmethodofconstructingbestuniformapproximationsforfunctionsof
acertainclass. USSRComputationalMathematicsandMathematicalPhysics,18(2):21–31,
1978.
[54] J.TsitsiklisandB.VanRoy. Analysisoftemporal-diffferencelearningwithfunctionapproxi-
mation. Advancesinneuralinformationprocessingsystems,9,1996.
[55] F.A.Valentine. Alipschitzconditionpreservingextensionforavectorfunction. American
JournalofMathematics,67(1):83–93,1945.
12[56] D.Vasal,R.K.Mishra,andS.Vishwanath. Masterequationofdiscretetimegraphonmean
fieldgamesandteams. arXivpreprintarXiv:2001.05633,2020.
[57] L.Wang,Z.Yang,andZ.Wang. Breakingthecurseofmanyagents: Provablemeanembedding
q-iteration for mean-field reinforcement learning. In International conference on machine
learning,pages10092–10103.PMLR,2020.
[58] M.A.Wiering.Multi-agentreinforcementlearningfortrafficlightcontrol.InMachineLearning:
ProceedingsoftheSeventeenthInternationalConference(ICML’2000),pages1151–1158,2000.
[59] Q. Xie, Z. Yang, Z. Wang, and A. Minca. Learning while playing in mean-field games:
Convergenceandoptimality. InInternationalConferenceonMachineLearning,pages11436–
11447.PMLR,2021.
[60] B.Yardim,S.Cayci,M.Geist,andN.He. Policymirrorascentforefficientandindependent
learninginmeanfieldgames. InInternationalConferenceonMachineLearning,pages39722–
39754.PMLR,2023.
[61] B.Yardim,S.Cayci,andN.He. Statelessmean-fieldgames: Aframeworkforindependent
learningwithlargepopulations. InSixteenthEuropeanWorkshoponReinforcementLearning,
2023.
[62] B.Yardim,A.Goldman,andN.He. Whenismean-fieldreinforcementlearningtractableand
relevant?,2024.
[63] M.A.U.Zaman,A.Koppel,S.Bhatt,andT.Basar. Oracle-freereinforcementlearninginmean-
fieldgamesalongasinglesamplepath. InInternationalConferenceonArtificialIntelligence
andStatistics,pages10178–10206.PMLR,2023.
[64] F.Zhang,V.Y.Tan,Z.Wang,andZ.Yang. Learningregularizedmonotonegraphonmean-field
games. arXivpreprintarXiv:2310.08089,2023.
A Preliminaries
Firstly,wepresentseveralbasicfactsregardingsymmetrizationandsymmetricfunctions.
Lemma3. Foranyf :XK ÑY,Sympfqisasymmetricfunction.
Proof. Foranyg1 PS ,wehave
K
ÿ ÿ
1 1
Sympfqpg1pxqq“ fpgpg1pxqqq“ fpgpxqq“Sympfqpxq,
K! K!
gPS
K
gPS
K
sincecompositionbyg1definesabijectionfromS ontoitself.
K
Lemma4. Foranysymmetricfunctionf :XK ÑY,Sympfq“f.
Proof. Bysimplecomputation:
ÿ ÿ
1 1
Sympfqpxq“ fpgpxqq“ fpxq“fpxq.
K! K!
gPS
K
gPS
K
Normedpolicyspace. Intheproofs,weequipthepolicyspaceΠwiththenorm}¨} definedas
1
}π´π1} :“sup}πpsq´π1psq} ,
1 1
sPS
foranyπ,π1 PΠ. Wepresentseveralusefulresults.
Lemma5. Letπ,π1 PΠandµ,µ1 P∆ bearbitrary. Then,
S
}µ¨π´µ1¨π1} ď}µ´µ1} `}π´π1} .
1 1 1
13Proof. Thelemmafollowsfromthetwoinequalities
ÿ
}µ¨π´µ¨π1} ď |µpsqπpa|sq´µpsqπ1pa|sq|
1
ÿs,a ÿ
ď µpsq |πpa|sq´π1pa|sq|ď}π´π1} ,
1
s a
andsimilarly:
ÿ
}µ¨π´µ1¨π} ď |µpsqπpa|sq´µ1psqπpa|sq|
1
ÿs,a ÿ
ď |µpsq´µ1psq| πpa|sq“}µ´µ1} .
1
s a
Lemma6(LemmaB.2of[60]). AssumeE afiniteset,g : E Ñ Rp avectorvaluefunction,and
ν,µtwoprobabilitymeasuresonE. Then,
› ›
›ÿ ÿ ›
› › λ
› gpeqµpeq´ gpeqνpeq› ď g }µ´ν} ,
› › 2 1
e e 1
whereλ :“sup }gpeq´gpe1q} .
g e,e1 1
Toestablishexplicitupperboundsontheapproximationrate,wewillusestandardconcentration
tools.
Lemma 7. Let x1,...,xN be N independent Bernoulli random variables taking values in the
discretesetX,withxi takingthevalue1withsomeprobabilitypi Pr0,1sforalliPrNs. Itholds
that
«ˇ ˇff »˜ ¸ fi
E
ˇ
ˇ ˇ 1
ÿN ppi´xiqˇ
ˇ ˇ ď ?1 , E– 1
ÿN
ppi´xiq
2
fl ď 1 .
ˇN ˇ N N N
i“1 i“1
Proof. Observingthatpxi´piqareindependentrandomvariables,
»˜ ¸ fi « ff
E– 1
ÿN
ppi´xiq
2
fl “ 1 E
ÿN ` pi´xi˘
2 ď 1 .
N N2 N
i“1 i“1
Furthermore,usingJensen’sinequality,
»g fi
«ˇ ˇff f˜ ¸
ˇ
ˇ 1
ÿN ˇ
ˇ
—f
e 1
ÿN 2
ffi
E ˇ ppi´xiqˇ ďE– ppi´xiq fl
ˇN ˇ N
i“1 i“1
g
f »˜ ¸ fi
ďf
f eE– 1
ÿN
ppi´xiq
2
fl
ď
?1
.
N N
i“1
Forcontrollingerrorsunderstochasticity,thefollowingsimplelemmawillbeuseful.
Lemma8(Harmonicpartialsumbound). Foranyintegerss,s¯suchthat1ďs¯ăsandp‰´1,it
holdsthat
1
ÿs
1 1
logs´logs¯` ď ď `logs´logs¯,
s n s¯
n“s¯
sp`1 s¯p`1
ÿs
sp`1 s¯p`1
´ `s¯p ď np ď ´ `sp, ifpě0
p`1 pp`1q p`1 p`1
n“s¯
sp`1 s¯p`1
ÿs
sp`1 s¯p`1
´ `sp ď np ď ´ `s¯p, ifpď0
p`1 p`1 p`1 p`1
n“s¯
14Proof. The proof follows from the the basic fact that if f : r1,8q Ñ R is a non-increasing
ě0
function,then
ż ż
s ÿs s
fpxqdx`fpsqď fpnqď fpxqdx`fps¯q,
s¯ n“s¯ s¯
andlikewiseforanon-decreasingfunctionf :r1,8qÑR ,itholdsthat
ě0
ż ż
s ÿs s
fpxqdx`fps¯qď fpnqď fpxqdx`fpsq.
s¯ n“s¯ s¯
Finally, we slightly generalize the definition of MFG-NE (Definition 11), as our approximation
theoremsaresomewhatmoregeneralthanwhatisstatedinthemainbodyofthepaper: weconsider
approximateMFG-NEratherthanonlyexactMFG-NE.
Definition 11 (δ-MFG-NE). A policy sequence πππ˚ P Π is called a δ-MFG-NE of the MFG
H
pS,A,ρ ,H,P,Rqifitholdsthat
0
Eptπ˚uH´1qďδ. (δ-MFG-NE)
h h“0
AremarkonextensionLemma1and∆ . ForgivenN ą0andmapP¯ :∆ Ñ∆ with
S SˆA,N S
LipschitzmodulusLon∆ S`ˆA˘,N,theKirszbraun-ValentineLemma(Lemma1)onlyguaranteesan
L-LipschitzextensionExt P¯ : ∆ Ñ RS. However, wecantriviallyside-stepthisissue
SˆA,N
w opit eh raa tom ro todi tfi he ed ca op np vl ei xca sti eo tn ∆of .K Fir os rzb anra yun ex-V tea nl se in ot nin Ee. xL t`e Pt ¯P˘ ,ro Pj r∆ oS
j
: R ˝S EÑ xt`∆ P¯S˘ b ise at lh se op aro vj ae lc idtio Ln
-
L Pi rp os jchit ˝z Eex xt te`n Ps ¯io˘n hath sa it mp ar ge eseS sr ev te cs oP n¯ tao inn et dhe ins ∆et∆
aS sˆ rA eq,N
uira es d.Proj ∆∆ SS isnon-expansive. Moreover,
∆S S
B ExtendedProofsonApproximation
B.1 ProofofLemma2
TheproofreliesonthepropertiesofsymmetrizationandLemma6. Fornconveniencewedenote
K :“N ´1inthisproof.
` ˘
Firstl`y, w˘e show that for any i,j P rNs,s P S,a P A, the functions Sym Pi ps,a,¨q and
Sym Ri ps,a,¨q also satisfy the bounded variation and sparsity assumptions. Assume that
ρρρPpSˆAqK,ps1,a1qPSˆAarbitrary,Then,bydefinition,
` ˘ ` ˘
}Sym Pi ps,a,ρρρq´Sym Pi ps,a,pps1,a1q,ρρρ´jqq}
› 2 ›
› ÿ ÿ ›
1 › ›
ď › Pips,a,gpρρρqq´ Pips,a,gpps1,a1q,ρρρ´jqq›
K!› ›
gPS
K
gPS
K 2
ÿ › ›
ď
1 › Pips,a,gpρρρqq´Pips,a,gpps1,a1q,ρρρ´jqq›
K! 2
gPS
K
ÿ
1
ď C ďC .
K! 1 1
gPS
K
Furthermore, assume Pips,a,¨q is κ-sparse on some set U Ă S ˆA where |U | ď κ. Let
s,a s,a
ρρρ,ρρρ1 PpSˆAqK betwovectorsagreeingintheirentriesinU ,i.e.,p pρρρq“p pρρρ1q. Then,
s,a Us,a Us,a
` ˘ ÿ
1
Sym Pi ps,a,ρρρq“ Pips,a,gpρρρqq
K!
gPS
K
ÿ
1
“ Pips,a,gpρρρqq
K!
gP`S
K˘
“Sym Pi ps,a,ρρρ1q,
15sincegpρρρ1qagreesw`ith˘gpρρρqonitselementsinU s,aaswell,asp Us,apgpρρρqq“p Us,apgpρρρ1qq.Therefore
weconcludeSym Pi isalsoκ-sparseonU . Bysimilarcomputation,
s,a
ˇ ` ˘ ` ˘ ˇ
ˇ Sym Ri ps,a,ρρρq´Sym Ri ps,a,pps1,a1q,ρρρ´jqqˇ ďC ,
2
` ˘
andSym Ri isalsoκ-sparse.
` ˘
Ę
Next, we establish that the lifted functions Sym Pi ps,a,¨q only depend on µpsq for s P U ,
s,a
that i`s, w˘e show that if µ`,µ1 ˘are such that µps1,a1q “ µ1ps1,a1q for all ps1,a1q P U s,a, then
Ę Ę
Sym Pi ps,a,µq “ Sym Pi ps,a,µ1q. Letµ,µ1 P ∆ besuchthatµps1,a1q “ µ1ps1,a1q
SˆA,K
forallps1,a1q P U . Takearbitraryρρρ,ρρρ1 suchthatσpρρρq “ µ,σpρρρ1q “ µ1. Itholdsthatforsome
s,a
permutationg1 PS thatg1pρρρ1qagreeswithρρρonallentriestakingvaluesinU ,asρρρ1 andρρρhave
K s,a
thesamecountofelementsinU . Then
s,a
` ˘ ÿ
SĘ ym Pi ps,a,µq“ 1 Pips,a,gpρρρqq
K!
gPS
K
ÿ
1
“ Pips,a,gpg1pρρρ1qqq
K!
gPS
K
ÿ ` ˘
“ 1 Pips,a,gpρρρ1qq“SĘ ym Pi ps,a,µ1q.
K!
gPS
K
` ˘
Ę
A s`imil˘ar argument` wo˘rks for Sym Ri ps,a,¨q, allowing us to conclude that
Ę Ę
Sym Pi ps,a,¨q,Sym Ri ps,a,¨qonlydependonµps1,a1qifps1,a1qPU .
s,a
ř ` ˘ ř ` ˘
Ę Ę
Finally,weanalyzetheLipschitzmodulusoftheliftedfunctions1{N iSym Pi ,1{N iSym Ri .
Let µ ,µ P ∆ and ρρρ “ tρiuK ,ρρρ “ tρiuK P pS ˆ AqK be such that σpρρρ q “
1 2 SˆA,K 1 1 i“1 2 2 i“1 1
µ ,σpρρρ q“µ . Then,
1 2 2
` ˘ ` ˘ ÿ
}SĘ ym Pi ps,a,ρρρ q´SĘ ym Pi ps,a,ρρρ q} ďC 1 .
1 2 1 1 ρi‰ρi
1 2
iPrKs
Takingtheminimumoversuchρρρ ,ρρρ ,wehavethat
1 2
` ˘ ` ˘
}SĘ ym Pi ps,a,µ q´SĘ ym Pi ps,a,µ q}
1 ÿ 2 1
ď min C 1 ďC K}µ ´µ } ,
1 ρi‰ρi 1 1 2 1
ρρρ1,ρρρ2PpSˆAqK
iPrKs
1 2
σpρρρ1q“µ1,σpρρρ2q“µ2
asρρρ 1,ρρρ
2
can differ at a minim`um˘ at K}µ
1
´µ 2}
1
coordinates. Finally, as concluded from the
Ę
arguments above, since Sym Pi ps,a,µ q only depends on µ ps1,a1q for s1,a1 P U , one can
1 1 s,a
chooseµ¯ ,µ¯ P∆ suchthatµ¯ ps1,a1q“µ ps1,a1qandµ¯ ps1,a1q“µ ps1,a1qforallps1,a1qP
1 2 SˆA 1 1 2 2
U andµ¯ ps2,a2q“µ¯ ps2,a2qwheneverps2,a2q‰U . Then,
s,a 1 2 s,a
` ˘ ` ˘ ` ˘ ` ˘
}SĘ ym Pi ps,a,µ q´SĘ ym Pi ps,a,µ q} “}SĘ ym Pi ps,a,µ¯ q´SĘ ym Pi ps,a,µ¯ q}
1 2 1 1 2 1
“C K}µ¯ ´µ¯ }
1 1ÿ 2 1
“C K |µ¯ ps1,a1q´µ¯ ps1,a1q|
1 1 2
ds1,a1PUs,a
ÿ
?
ďC K |µ¯ ps1,a1q´µ¯ ps1,a1q|2 κ
1 1 2
s1,a1PUs,a
? ?
ďC K κ}µ¯ ´µ¯ } ďC K κ}µ ´µ } ,
1 1 2 2 1 1 2 2
thusprovingLipschitzboundontheset∆ . Byanidenticalargument,itholdsthat
SˆA,K
|SĘ ym` Ri˘
ps,a,µ
q´SĘ ym` Ri˘
ps,a,µ q|ďC pN
´1q?
κ}µ ´µ } .
1 2 2 1 2 2
ř ` ˘
Ę
The řresult f`ollo˘ws from an application of Lemma 1 to extend 1{N iSym Ri ps,a,¨q and
Ę
1{N iSym Pi ps,a,¨qfrom∆
SˆA,N
to∆ SˆA,asthenormequivalence}¨}
2
ď}¨} 1holds.
16B.2 PopulationFlowsareLipschitzContinuous
Lemma9(LipschitzcontinuityofΓ). LetP : S ˆAˆ∆ Ñ ∆ besuchthatPps,a,µqis
SˆA S
Lipschitzcontinuousin}¨} normwithmodulusK ą0and
1 µ
› › › ›
› › › ›
K :“sup Pps,a,µq´Pps1,a,µq , K :“sup Pps,a,µq´Pps,a1,µq .
s 1 a 1
s,s1 a,a1
a,µ s,µ
Thenitholdsforallµ,µ1 P∆ ,π,π1 PΠthat:
SˆA
ˆ ˙
K `K
}Γpµ,πq´Γpµ1,πq} ď s a `K }µ´µ1} ,
1 2 µ 1
forallπ PΠ,µ,µ1 P∆ .
SˆA
Proof. The proof is inspired by [60], apart from the fact that in our case the population update
operatorisdefineddifferentlyas:
ÿ
Γpµ,πqps1,a1q:“ µps,aqPps1|s,a,µqπpa1|s1q
sPS,aPA
Wewillproveaslightlymoregeneralstatement,that
ˆ ˙
› ›
› Γpµ,πq´Γpµ1,π1q› ď}µ´µ1} K s`K a `K `}π´π1} .
1 1 2 µ 1
Firstly,weupperboundtheLipschitzmodulusofthefunctionwithrespecttoµ. Foranyµ,µ1 P
∆ ,itholdsthat:
SˆA
› ›
› ›
Γpµ,πq´Γpµ1,πq
ˇ 1 ˇ
ÿ ˇÿ ˇ
ˇ ˇ
ď ˇ pµps,aqPps1|s,a,µq´µ1ps,aqPps1|s,a,µ1qqπpa1|s1qˇ
ˇ ˇ
s1,a1ˇs,a ˇ
ÿ ˇÿ ˇ
ˇ ˇ
ď ˇ pµps,aq´µ1ps,aqqPps1|s,a,µqπpa1|s1qˇ
ˇ ˇ
s1,a1 s,ˇa ˇ
ÿ ˇ ÿ ˇ
ˇ ˇ
` ˇ µ1ps,aqpPps1|s,a,µq´Pps1|s,a,µ1qqπpa1|s1qˇ
ˇ ˇ
› s1,a1 sPS,aPA ›
›ÿ ›
› ›
ď› pµps,aq´µ1ps,aqqpPps,a,µq¨πq›
› ›
s,ÿa ÿ 1 ÿ
` µ1ps,aq |Pps1|s,a,µq´Pps1|s,a,µ1q| πpa1|s1q
s,a s1 a1
max}Pps,a,µq¨π´Ppss,sa,µq¨π}
ď}µ´µ1} 1 `K }µ´µ1}
1 2 µ 1
max}Pps,a,µq´Ppss,sa,µq}
ď}µ´µ1} 1 `K }µ´µ1} .
1 2 µ 1
wherethelasttwolinesfollowfromLemma6andLemma5. Since
}Pps,a,µq´Ppss,sa,µq} ďK `K ,
1 s a
wehavetheclaimedinequality
ˆ ˙
› ›
› Γpµ,πq´Γpµ1,πq› ď}µ´µ1} K s`K a `K .
1 1 2 µ
17Finally,theLipschitzconstantforthepolicyπiscomputedby:
ˇ ˇ
› › Γpµ,πq´Γpµ,π1q› › ď ÿ ˇ ˇ ˇÿ µps,aqPps1|s,a,µq` πpa1|s1q´π1pa1|s1q˘ˇ ˇ ˇ
1 ˇ ˇ
s1ÿ,a1 s,a ÿˇ ˇ
ˇ ˇ
ď µps,aqPps1|s,a,µq πpa1|s1q´π1pa1|s1q
s,a,s1 a1
ď}π´π1} .
1
B.3 ProofofTheorem1
ThemainideasoftheapproximationproofaresimilartosomeargumentsfromMFGliterature(e.g.
see [46]) with two major differences: (1) the dynamics of the finite player game are not exactly
symmetric,and(2)unlikesomestandardworksthedynamicsandrewardsdependonthedistribution
ofagentsoverstate-actionpairs,notjuststates.
ForgivenRandP definethefollowingconstants:
ˇ ˇ ˇ ˇ
ˇ ˇ ˇ ˇ
L “ sup Rps,a,µq´Rps1,a,µq , L “ sup Rps,a,µq´Rps,a1,µq ,
s a
s,s1,a,µ› › s,a,a1,µ › ›
› › › ›
K “ sup Pp¨|s,a,µq´Pp¨|s1,a,µq , K “ sup Pp¨|s,a,µq´Pp¨|s,a1,µq .
s 1 a 1
s,s1,a,µ s,a,a1,µ
WealsointroducetheshorthandnotationforanysPS,uP∆ ,µP∆ :
A SˆA
ÿ ÿ
Pp¨|s,u,µq“ upaqPp¨|s,a,µq, Rps,u,µq“ upaqRps,a,µq.
aPA aPA
By[60,LemmaC.1],itholdsthat
K
}Pp¨|s,u,µq´Pp¨|s1,u1,µ1q} ďK }µ´µ1} `K dps,s1q` a }u´u1} ,
1 µ 1 s 2 1
L
|Rps,u,µq´Rps1,u1,µ1q|ďL }µ´µ1} `L dps,s1q` a }u´u1} . (4)
µ 1 s 2 1
Wewilldefineanewoperatorfortrackingtheevolutionofthepopulationdistributionoverfinitetime
horizonsforatime-varyingpolicy@π “tπ uH´1 PΠ:
h h“0
Γhpµ,πq:“lΓopo.oo.o.ooΓoopoΓoopooµoo,oπoo0mq,ooπo1ooqoo.o.oo.o,ooπooho´oo1onq
htimes
sothatΓ0pµ,πq:“µ. Lemma9yieldstheLipschitzcondition:
}Γnpµ,tπ un´1q´Γnpµ1,tπ un´1q}
i i“0 i i“0 1
ďL }Γn´1pµ,tπ un´2q´Γn´1pµ1,tπ1un´2q} `}π ´π1 }
pop,µ i i“0 i i“0 1 n´1 n´1 1
nÿ´1
ďLn }µ´µ1} ` Ln´1´i}π ´π1} , (5)
pop,µ 1 pop,µ i i 1
i“0
whereL istheLipschitzconstantofΓinµ.
pop,µ
WealsodefineausefulfunctionΞ:pSˆAqN ˆΠN Ñ∆ suchthatforanyρρρ“tpsi,aiquN ,
SˆA i“1
1
ÿN
Ξpρρρ,πsq“ Pp¨|si,ai,σpρρρ´iqq¨πs.
N
i“1
Inotherwords,Ξistheaveragepopulationflowexpectedundersymmetrizeddynamicsandreference
policyπs.
Theproofwillproceedinfoursteps:
18• Step1. Boundingtheexpecteddeviationoftheempiricalpopulationdistributionfromthe
mean-fielddistributionEr}µp ´µ } sforanygivenpolicyπ.
h h 1
• Step2. Boundingtotalvariationdistance(orequivalentlyℓ distance)betweenthemarginal
1
distributionsPrs1 “¨sintheN-playergameandPrs “¨sinthemean-fieldgame,
h h
• Step3. BoundingdifferenceofN agentvaluefunctionJp1qandtheinfiniteplayervalue
functionV,whenalltheplayersexceptthefirstoneplaythesamepolicy,
• Step4. BoundingtheexploitabilityofanagentwheneachofN agentsareplayingthe
FH-MFG-NEpolicy.
Step1: Empiricaldistributionbound. Duetoitsrelevanceforageneralconnectionbetweenthe
FH-MFGandtheN-playergame,westatethisresultintheformofanexplicitbound. Inthisstep,we
willassumeN playersoftheFH-DGpursuepoliciestπiu “tπiu PΠN,andrandomvariables
i h i,h
f Furthermore,assumeπs “tπs u PΠarbitrary,andinducespopulationµ“Λpπsq“tµ u . We
h hř h h
alsodefinethequantity∆ :“ 1 N }πi ´πs } and∆s :“max ∆ .
h N i“1 h h 1 hPrHs h
Theproofwillproceedinductivelyoverh. First,fortimeh“0,wehave
«› › ff «› › ff
›
› 1
ÿN ›
›
›
› 1
ÿN ›
›
Er}µp ´µ } sďE ›µp ´ ρ ¨πi› `E › ρ ¨πi ´µ ›
0 0 1 › 0 N 0 0› ›N 0 0 0›
«ˇ i“1 1 i“1 ˇff 1
ÿ ˇ
ˇ 1
ÿN ˇ
ˇ
ď E ˇ p1 ´ρ psqπipa|sqqˇ
ˇN si 0“s,ai 0“a 0 0 ˇ
sPS i“1
aPA « ff
1
ÿN
` E }πi ´πs }
N 0 0 1
i“1
1
ď|S||A|? `∆ ,
h
N
wherethelastlineisduetoLemma7andthefactthat1 areindependent,boundedrandom
” ı si“s,ai“a
0 0
variables,andthatwehaveE 1 “ρ psqπps,aq“µ ps,aq.
si“s,ai“a 0 0
0 0
Next,denotingtheσ-algebrainducedbytherandomvariablesptsi,aiuq asF ,wehavethat:
h h i,h1ďh h
Er}µp h`1´µ h`1} 1|F hsďlE ooro}oµop ohoo`oo1oo´ooooE oorooµp mh`ooo1oo|oFoohoosoo}o1oo|oFooohons
p△q
`E looro}oE oooroµp oohoo`oo1oo|oFoohoosoo´omΞoopoρρρooho,ooπs ooho`oo1ooqo}oo1oo|oFoohons
p˝q
`lE ooro}oΞooopoρρρooho,ooπs ooho`oo1ooqoo´mΓooopoµp oohoo,oπs oohooqo}o1oo|oFooohons
p‹q
`lE ooro}oΓoopooµp ooho,ooπs oohooqm´ooµoohoo`oo1o}o1ooo|Fooohons (6)
p♡q
Weupperboundthefourtermsseparately. Forp△q,itholdsthat
p△q“Er}µp ´Erµp |F s} |F s
ÿh`1 h`1 h 1 h
“ Er|µp ps,aq´Erµp ps,aq|F s||F s
h`1 h`1 h h
sPS,aPA
1
ď|S||A|? ,
N
sinceeachµp psqisanaverageofN independentrandomvariables(specificallyN independent
h`1
Bernoullirandomvariables)givenF ,usingLemma7.
h
19Next,forthetermp˝q,
p˝q“Er}Erµp |F s´Ξpρρρ ,πs q} |F s
h`1 h h h`1 1 h
„› › ˇ ȷ
“ N1 E
›
›
›ÿN
Pip¨|si h,ai h,ρρρ´ hiq¨π hi `1´
ÿN
Pp¨|si h,ai h,σpρρρ´ hiqq¨πs
h`1›
› ›
ˇ
ˇ ˇF h
i“1 i“1 1
„ ˇ ȷ
ď N1 E
ÿN
}Pip¨|si h,ai h,ρρρ´ hiq´Pp¨|si h,ai h,σpρρρ´ hiqq}
1ˇ
ˇ ˇF
h
i“1
1
ÿN
` }πi ´πs } .
N h`1 h`1 1
i“1
Bytheα-symmetrycondition,itfollowsthatp˝qďα`∆ .
h`1
Forp‹q“}Ξpρρρ ,πs u q´Γpµp ,πs q} ,
h h i h h 1
p‹q“Er}Ξpρρρ ,πs q´Γpµp ,πs q} |F s
h h`1 h h`1 1 h
„› › ˇ ȷ
“E
›
› › N1
ÿN
Pp¨|si h,ai h,σpρρρ´ hiqq¨πs h`1´
ÿ
µp hps1,a1qPp¨|s1,a1,µp hq¨πs
h`1›
› ›
ˇ
ˇ ˇF h .
i“1 s1,a1 1
„› › ˇ ȷ
“ N1 E
›
›
›ÿN
Pp¨|si h,ai h,σpρρρ h´iqq¨πs h`1´
ÿN
Pp¨|si h,a1 h,µp hq¨πs
h`1›
› ›
ˇ
ˇ ˇF h .
i“1 i“1 1
ThevectorspN ´1qσpρρρ´iq,Nµp candifferbyonly1inonecomponentduetothei-thagentbeing
h h
excludedfromtheformer,itholdsthat
}Nσpρρρ´iq´Nµp } ď}pN ´1qσpρρρ´iq´Nµp } `}σpρρρ´iq} ď3,
h h 1 h h 1 h 1
thereforeforanys,a,
3K
}Pp¨|s,a,σpρρρ´iqq´Pp¨|s,a,µp q} ď µ
h h 1 N
almostsurelyandp‹qisfurtherupperboundedbyp‹qď 3Kµ.
N
Finally,thelasttermp♡qcanbeboundedusing:
“ ‰
p♡q“E }Γpµp ,πs q´Γpµπs ,πs q} |F ďL }µp ´µ } .
h h h h 1 h pop,µ h h 1
Toconclude,mergingtheboundsonthethreetermsinInequality(6)andtakingtheexpectationon
bothsides,bythelawofiteratedexpectationsweobtain:
1 3K
Er}µp ´µ } sďL Er}µp ´µ } s`|S||A|? ` µ `∆ `α.
h`1 h`1 1 pop,µ h h 1 N N h`1
Inductiononhyieldstheboundforallh:
Er}µp ´µ } s
h h 1
ˆ ˙
ÿh
|S||A| 3K
ď Lh´h1 ? ` µ `∆ `α
pop,µ N N h`1
h1“0 ˆ ˙
ď 1´Lh po` p1 ,µ |S ?||A| `∆s ` 3K µ `α , (7)
1´L pop,µ N N
1´Lh`1
whereweadopttheconvenientshorthand pop,µ :“hifL “1.
1´Lpop,µ pop,µ
Step 2: Marginal state-action distributions. In this step, we analyze the distributions Prsi “
h
¨,ai “ ¨s. For simplicity, assume each player i ‰ 1 follows policy π P Π, player 1 follows an
h
arbitrarypolicyπs PΠ:wedenotetheinducedrandomvariablesintheN-playergameGassi,ai,µp .
h h h
Assumethatinthemean-fieldgameMFGpGq,therepresentativeplayerinMFGpGqalsofollows
20policyπs,evaluatedagainstdistributionµ:“Λpπq: denotetheinducedrandomvariablesass ,a .
h h
Inthissetting,wewillinductivelyupperboundthequantity
}Prs “¨s´Prs1 “¨s} .
h h 1
Firstly,forh“0,itholdsbydefinitionthat
Prs “¨s“Prs1 “¨s“ρ ,
0 0 0
hence}Prs “¨s´Prs1 “¨s} “0.
0 0 1
Next,forthetimesteph`1,
}Prs “¨s´Prs1 “¨s}
› h`1 h`1 1 ›
›ÿ ÿ ›
ď› › P1ps,πs hpsq,ρρρqPrs1 h “s,ρρρ´ hi “ρρρs´ Pps,πs hpsq,µ hqPrs h “ss› ›
›s,ρρρ s 1›
›ÿ ÿ ›
ď› › Pps,πs hpsq,σpρρρqqPrs1 h “s,ρρρ´ hi “ρρρs´ Pps,πs hpsq,µ hqPrs h “ss› ›
s,ρρ›ρ s › 1
›ÿ ›
`› › rP1ps,πs hpsq,ρρρq´Pps,πs hpsq,σpρρρqqsPrs1
h
“s,ρρρ´ hi “ρρρs› ›
› s,ρρρ 1 ›
›ÿ ÿ ›
ď› › Pps,πs hpsq,σpρρρqqPrs1 h “s,ρρρ´ hi “ρρρs´ Pps,πs hpsq,µ hqPrs h “ss› › `α, (8)
s,ρρρ s 1
wherethelastlinefollowsfromtheα-symmetrycondition. Fortheremainingterm,wefirstobserve
theinequality:
› ›
›ÿ ›
› › rPps,πs hpsq,σpρρρqq´Pps,πs hpsq,µ hqsPrs1
h
“s,ρρρ´ hi “ρρρs› ›
s,ρρρÿ 1
ď K }σpρρρq´µ } Prs1 “s,ρρρ´i “ρρρs
µ h 1 h h
s,ρρρ
ďK Er}σpρρρq´i´µ } s
µ h h 1
3K
ďK Er}µp ´µ } s` µ
µ h h 1 N
as again }µp
h
´σpρρρ´ hiq}
1
ď 3{N almost surely, as Nµp
h
and pN ´1qσpρρρ´ hiq differ by one in one
coordinateonly. Then,applyingthetriangleinequalityandmarginalizingoverρρρinInequality(8),
}Prs “¨s´Prs1 “¨s}
› h`1 h`1 1 ›
›ÿ ÿ ›
ď› › Pps,πs hpsq,µ hqPrs1 h “ss´ Pps,πs hpsq,µ hqPrs h “ss› ›
s s 1
3
`α` `K Er}µp ´µ } s
N µ h h 1
3K
ď}Prs1 “ss´Prs “ss} `α` µ `K Er}µp ´µ } s
h h 1 N µ h h 1
whereinthelastlineweusedthefactthatMarkovkernelsarenon-expansiveinℓ norm,wherein
1
thiscasetheMarkovkernelisgivenbyPp¨|s,πs psq,µ qforallsPS.
h h
Inductivelyapplyingtherecursiveboundaboveweobtaintheinequalityforallh:
ˆ ˙
3K
hÿ´1
}Prs1 “¨s´Prs “¨s} ďph´1q α` µ `K Er}µp ´µ } s. (9)
h h 1 N µ h h 1
h“0
s
BytheresultinStep1(Inequality7),since∆ď2{N inthiscaseitholdsthat
ˆ ˙
1´Lh`1 |S||A| 5K
Er}µp ´µ } sď pop,µ ? ` µ `α ,
h h 1 1´L pop,µ N N
21forallh“0,...,N ´1. Mergingthetwoinequalities:
}Prs1 “¨s´Prs “¨s}
h h 1
ˆ ˙ ˆ ˙
hÿ´1 1´Lh1`1
|S||A| 5K 3K
ďL pop,µ ` µ `α `ph´1q α` µ . (10)
pop,µ 1´L N N N
pop,µ
h1“0
Step3. Boundingvaluefunction. Inthisstep,weboundthedifferencebetweentheexpectedreturns
ofaplayerintheN playergameandtheinducedMFG(denotedbyfunctionsJ,V respectively).
Namely,wewillupperboundthedeviation
|Jp1qpπs,π,...,πq´VpΛpπq,πsq|
foranytwopoliciesπs,π PΠ.
Asinstep1, assumeeachplayeri ‰ 1followspolicyπ P Π, andplayer1followspolicyπs: we
denote the induced random variables in the N-player game G as si,ai,µp . Assume that in the
h h h
mean-fieldgameMFGpGq,therepresentativeplayerinMFGpGqalsofollowspolicyπs,evaluated
againstdistributionµ:“Λpπq: denotetheinducedrandomvariablesass ,a .
h h
s
BytheresultsinStep1,2shownininequalities(7),(9),since∆ď2{N inthiscaseitholdsthat:
ˆ ˙
1´Lh`1 |S||A| 5K
Er}µp ´µ } sď pop,µ ? ` µ `α ,
h h 1 1´L pop,µ N N
ˆ ˙
3K
hÿ´1
}Prs1 “¨s´Prs “¨s} ďph´1q α` µ `K Er}µp ´µ } s,
h h 1 N µ h h 1
h“0
forallh“0,...,N ´1.
Atafixedtimesteph,theexpectedone-steprewarddifferencescanbedecomposedintofourterms:
|ErRps ,a ,µ qs´ErR1ps1,a1,ρρρ´iqs|
h h h h h
ď|ErRps ,a ,µ qs´ErRps1,a1,µ qs|
h h h h h h
`|ErRps1,a1,µ qs´ErRps1,a1,µp qs|
h h h h h h
`|ErRps1,a1,µp qs´ErRps1,a1,σpρρρ´iqqs|
h h h h h
`|ErRps1,a1,σpρρρ´iqqs´ErR1ps1,a1,ρρρ´iqs|,
h h h h
enumeratingthesetermsas(I),(II),(III),and(IV),weupperboundeachasfollows:
L
(I)ď a }Prs “¨,a “¨s´Prs1 “¨,a1 “¨s}
2 h h h h 1
L
ď a }Prs “¨s´Prs1 “¨s}
2 h h 1
(II)ďL Er}µp ´µ } s
µ h h 1
3
(III)ďL Er}µp ´σpρρρ´iq} sďL
µ h 1 µN
(IV)ďβ
wherethelastlineusestheα,β symmetrycondition.
Then,summingupovertheentiretimehorizon,weobtaintheresult
|Jp1qpπs,π,...,πq´VpΛpπq,πsq|
ˆ ˙
Hÿ´1
L 3HL
ď L Er}µp ´µ } s` a }Prs “¨s´Prs1 “¨s} `βH ` µ (11)
µ h h 1 2 h h 1 N
h“0
22orsubstitutingtheupperboundsestablishedbefore,
|Jp1qpπs,π,...,πq´VpΛpπq,πsq|
˜ ¸
Hÿ´1
L K
hÿ´1
ď L Er}µp ´µ } s` a µ Er}µp ´µ } s
µ h h 1 2 h1 h1 1
h“0 ˆ ˙h1“0
3HL 3K
`βH ` µ `H2 α` µ
N N
˜ ¸ ˆ ˙
Hÿ´1
L K
hÿ´1
3HL 3K
ď L Ephq` a µ Eph1q `βH ` µ `H2 α` µ (12)
µ 2 N N
h“0 h1“0
wherewedefinethequantity
ˆ ˙
1´Lh`1 |S||A| 5K
Ephq :“ pop,µ ? ` µ `α .
1´L pop,µ N N
Step4. Boundingexploitabilityfunction. Finally,weusetheresultsfromthepreviousstepsto
upperboundtheexploitabilityoftheMFG-NEpolicyπ˚intheFH-DG.Letµ˚ “Λpπ˚q. Letπ1be
arbitrary.
Jp1qpπ1,π˚,...,π˚q´Jp1qpπ˚,...,π˚qďVpΛpπ˚q,π1q´VpΛpπ˚q,π˚q
`|Jp1qpπ˚,π˚,...,π˚q´VpΛpπ˚q,π˚q|
`|Jp1qpπ1,π˚,...,π˚q´VpΛpπ˚q,π1q|.
ThelasttwotermsinthisinequalitycanbeboundedbyInequality(12)bychoosingπs “ π˚ and
πs “π1respectively,andthefirsttermsatisfies
VpΛpπ˚q,π1q´VpΛpπ˚q,π˚qďδ
asπisassumedtobeaδ-MFG-NE.
Then,themainstatementofthetheoremisobtainedbyobserving
` ˘
Epiqpπ˚q“maxJpiq π1,π˚,´i ´Jpiqpπ˚,...,π˚q
π1PΠ ˜ ¸ ˆ ˙
Hÿ´1
L K
hÿ´1
6HL 3K
ďδ`2 L Ephq` a µ Eph1q `2βH ` µ `2H2 α` µ ,
µ 2 N N
h“0 h1“0
whereonceagain
ˆ ˙
1´Lh`1 |S||A| 5K
Ephq :“ pop,µ ? ` µ `α .
1´L pop,µ N N
Namely,ifL “1,then
pop,µ
ˆ ˆ ˙˙
1
Ephq “O h α` ? ,
N
ifL ă1,then
pop,µ
ˆ ˙
1
Ephq “O α` ? ,
N
andfinallyifL ą1,then
pop,µ
ˆ ˆ ˙˙
1
Ephq “O Lh α` ? .
pop,µ
N
23C ExtendedResultsonTDLearning
Wewillmakeuseofthefollowingtechnicallemma.
Lemma10. LetG “pS,A,ρ ,N,H,tPiuN ,tRiuN qbeaFH-DGwhichinducesMFGpGq“
0 i“1 i“1
pS,A,N,H,P,Rq. Furthermore,assumethatthepopulationflowoperatorΓoftheinducedMFG
satisfiestheLipschitzcondition
}Γpµ,πq´Γpµ1,πq} ďL }µ´µ1} ,
2 pop,µ 2
forallpoliciesπandµ,µ1 P∆ . Then,ifallplayersintheFH-DGplaypolicyπ PΠ,itholds
SˆA
that
˜ ¸
“ ‰ 1´L2ph`1q |S||A| 18K2pH2`2q
E }µp ´µ }2 ďC pop,µ ` µ `2pH2`2qα2 ,
h h 2 1´L2 N N2
pop,µ
forsomeabsoluteconstantC ą0.
Proof. DuetoitsrelevanceforageneralconnectionbetweentheFH-MFGandtheN-playergame,
westatethisresultintheformofanexplicitbound. Inthisstep,wewillassumeN playersofthe
FH-DG pursue policies tπiu “ tπiu P ΠN such that πi “ π for some π P Π, and random
i h i,h
variablestsi,aiu ,ρρρ PpSˆAqN aregeneratedaccordingtothefiniteplayerdynamics.
h h i,h1ďh h
Theproofwillproceedinductivelyoverh. First,fortimeh“0,wehave
“ ‰ ” ı
E }µp ´µ }2 ďE }µp ´ρ ¨π }2
0 0 2 0 0 0 2
»˜ ¸ fi
ďÿ
E– 1
ÿN
1 ´pρ ¨π qps,aq
2
fl
N si 0“s,ai 0“a 0 0
s,a i“1
« ff
ÿ
1
ÿN ´ ¯
2
ď E 1 ´pρ ¨π qps,aq
N2 si 0“s,ai 0“a 0 0
s,a i“1
|S||A|
ď ,
N
due to the fact that 1 are independent, bounded random variables, and that we have
” ı si“s,ai“a
0 0
E 1 “ρ psqπps,aq“µ ps,aq.
si“s,ai“a 0 0
0 0
Next,denotingtheσ-algebrainducedbytherandomvariablestsi,aiu asF ,wehavethat:
h h i,h1ďh h
“ ‰
E }µp ´µ }2|F (13)
“h`1 h`1 2 h ‰ “ ‰
ďE }µp ´Erµp |F s}2|F `E }µ ´Erµp |F s}2|F (14)
“ h`1 h`1 h 2 h‰ h`1 h`1 h 2 h
ďE }µp ´Erµp |F s}2|F
h`1 “ h`1 h 2 h ‰ “ ‰
`p1`δ´1qE }Γpµp ,π q´Erµp |F s}2|F `p1`δ qE }Γpµp ,π q´µ }2|F
h h h h`1 h 2 h h h h h`1 2 h
(15)
“ ‰ “ ‰
ďlE ooo}ooµp ooho`oo1oo´ooooE oorooµp mh`ooo1oo|oFoohoosoo}o2 2oo|oFooohon`2p1`δ h´1qE looo}ooE oorooµp ooho`oo1oo|oFoohooosom´oΞooopoρρρooho,ooπoohoqoo}o2 2oo|oFoohoon
p△“q ‰ “ p˝q ‰
`2p1`δ h´1qE looo}oΞoopooρρρohoo,ooπoohoqoo´oomΓopoµp oohoo,oπoohooqo}o2 2oo|oFooohon`p1`δ hqlE ooo}oΓoopooµp ooho,ooπoohooqm´ooµooho`oo1oo}o2 2oo|oFooohon
p‹q p♡q
(16)
where inequalities 15 and 16 follow from applications of Young’s inequality, where δ ą 0 is a
h
positivevaluetobedeterminedlater. Weupperboundthefourtermsseparatelyasintheproofof
Theorem1. Forp△q,itholdsthat
“ ‰
|S||A|
p△q“E }µp ´Erµp |F s}2|F ď ,
h`1 h`1 h 2 h N
24sinceeachµp psqisanaverageofindependentsubgaussianrandomvariablesgivenF .Specifically,
h`1 h
eachindicatorisbounded1 Pr0,1salmostsurely.
si “s,ai “a
h`1 h`1
Next,forthetermp˝q,
“ ‰
p˝q“E }Erµp |F s´Ξpρρρ ,πs q}2|F
h`1 h h h`1 2 h
„› › ˇ ȷ
ď N1
2
E
›
›
›ÿN
Pip¨|si h,ai h,ρρρ´iq¨π h`1´
ÿN
Pp¨|si h,ai h,σpρρρ´iqq¨π
h`1›
›
›2ˇ
ˇ ˇF
h
„˜i“1 i“1
¸ ˇ ȷ
1
ď N1
2
E
ÿN
}Pip¨|si h,ai h,ρρρ´iq´Pp¨|si h,ai h,σpρρρ´iqq}
1
2ˇ
ˇ ˇF
h
i“1
Bytheα-symmetrycondition,itfollowsthatp˝qďα2.
Forp‹q“Er}Ξpρρρ ,π u q´Γpµp ,π q}2|F s,
“ h h i h h 2 h ‰
p‹qďE }Ξpρρρ ,πs q´Γpµp ,πs q}2|F
h h`1 h h`1 1 h
„› › ˇ ȷ
“E
›
› › N1
ÿN
Pp¨|si h,ai h,σpρρρ h´iqq¨πs h`1´
ÿ
µp hps1,a1qPp¨|s1,a1,µp hq¨πs
h`1›
›
›2ˇ
ˇ ˇF h
i“1 s1,a1 1
„› › ˇ ȷ
“ N1
2
E
›
›
›ÿN
Pp¨|si h,ai h,σpρρρ´ hiqq¨πs h`1´
ÿN
Pp¨|si h,a1 h,µp hq¨πs
h`1›
›
›2ˇ
ˇ ˇF
h
.
i“1 i“1 1
ThevectorspN ´1qσpρρρ´iq,Nµp candifferbyonly1inonecomponentduetothei-thagentbeing
h h
excludedfromtheformer,itholdsthat
}Nσpρρρ´iq´Nµp } ď}pN ´1qσpρρρ´iq´Nµp } `}σpρρρ´iq} ď3,
h h 1 h h 1 h 1
thereforeforanys,a,
3K
}Pp¨|s,a,σpρρρ´iqq´Pp¨|s,a,µp q} ď µ
h h 1 N
9K2
almostsurelyandp‹qisfurtherupperboundedbyp‹qď µ.
N2
Finally,thelasttermp♡qcanbeupperboundedusingtheLipschitzconditiononΓ,namely:
“ ‰
p♡q“E }Γpµp ,π q´Γpµπ,π q}2|F ďL2 }µp ´µ }2.
h h h h 2 h pop,µ h h 2
Toconclude,mergingtheboundsonthethreetermsinInequality(6)andtakingtheexpectationswe
obtain:
“ ‰ “ ‰
|S||A|
E }µp ´µ }2 ďp1`δ qL2 E }µp ´µ }2 `
h`1 h`1 2 h pop,µ h h 2 N
9K2
`2p1`δ´1q µ `2p1`δ´1qα2.
h N2 h
Inductiononhyieldstheboundforallh:
“ ‰
E }µp ´µ }2
h h 2 ˜ ¸˜ ¸
ÿh źh |S||A| 9K2
ď L2ph´h1q p1`δ q `2p1`δ´1q µ `2p1`δ´1qα2 .
pop,µ h2 N h1 N2 h1
h1“0 h2“h1`1
Here,wespecifyδ :“p1`h2q´1,forwhichitholdsthat
h
ź8 ź8
p1`δ q“ p1`p1`h2q´1qďC,
h
h“0 h“0
foranabsoluteconstantC ă10. Then,
“ ‰
E }µp ´µ }2
h h 2 ˜ ¸
ÿh |S||A| 9K2
ď L2ph´h1qC `2pH2`2q µ `2pH2`2qα2 .
pop,µ N N2
h1“0
25whichyieldsthestatedupperboundofthelemma
˜ ¸
“ ‰ 1´L2ph`1q |S||A| 18K2pH2`2q
E }µp ´µ }2 ď pop,µ C ` µ `2pH2`2qα2 , (17)
h h 2 1´L2 N N2
pop,µ
1´L2ph`1q
whereweadopttheconvenientshorthand pop,µ :“hifL “1.
1´L2 pop,µ
pop,µ
NotethatwhiletheLipschitzmodulususedinLemma10iswithrespecttothe}¨} norm,Lemma2
2
readilyguaranteesthatthiswillhold.
C.1 ExtendedProofofTheorem2
Letµ“tµ u :“Λpπq,andnotethatbytheproofofTheorem1,itholdsthat(Inequality10)
h h
A :“}Prs1 “¨,a1 “¨s´Prs “¨,a “¨s}
h h h h h 1
ď}Prs1 “¨s´Prs “¨s}
h h 1
ˆ ˙ ˆ ˙
hÿ´1 1´Lh1`1
|S||A| 5K 3K
ďL pop,µ ` µ `α `ph´1q α` µ .
pop,µ 1´L N N N
pop,µ
h1“0
Likewise,byLemma10,
B :“Er}µ ´σpρρρ´1 q}2s
h h m,h 1
:“Er}µ ´σpρρρ´1 q}2s|S||A|
h m,h˜2 ¸
1´L2ph`1q |S||A| 18K2pH2`2q 18|S||A|
ď pop,µ 2C ` µ `2pH2`2qα2 |S||A|` .
1´L2 N N2 N2
pop,µ
WillwillcommonlyutilizetheboundsQp m Pr0,Q s,Qτ,π Pr0,Q salmostsurelyforQ :“
h max h max max
Hp1`log|A|q,astheone-steprewardsareboundedinranger0,1sandthepolicyentropyhastrivial
upperboundlog|A|. Denotethemarginalprobabilitiesofs1 ,a1 (whichisi.i.d. forallm)as
m,h m,h
p P∆ ,whichclearlydoesnotdependonepochmasthesamepoliciesaredeployedateach
h SˆA
TDlearninground. Wealsodefinethequantity
qτ,πps,aq:“Qτ,πps,aq´τHpπ p¨|sqq,
h h h
whichcorrespondstothemorestandardentropyregularizedvaluefunctioninsomeworks.
Weoutlinetheproofstrategyintodifferentstepsasfollows:
• Step1. AnalyzethealgorithmfortheQ-valuesattimestepH ´1,thatis,showthatin
expectation}Qτ H,π ´1´Qp m H´1}2
pH´1
decreaseswithOp1{mqoverepochs,uptoasmallbias
term.
• Step2. AssumingthattheerroratsometimehdecreaseswithrateOp1{mq,showthatthe
error}Qτ h, ´π 1´Qp m h´1}2
pH´1
alsodecreaseswithrateOp1{mq,showingthatthemagnification
intheconstantsarenottoolarge.
• Step3. Concludethestatementofthetheorembyinduction.
p
Step1. WewillfirstanalyzetheevolutionofQm . Bydefinition,itholdsthat
H´1
Qτ,π ps,aq“Rps,a,µ q`τHpπ psqq.
H´1 H´1 H´1
Inotherwords,thereisnobootstrappingandthestochasticerrordoesnothaveadependenceon
future biased estimates. Firstly, if s1 “ s,a1 “ a,ρρρ “ ρρρ, then it holds almost
m,H´1 m,H´1 m,H´1
surelythat
Qτ,π ps,aq´Qpm`1ps,aq
H´1 H´1
“Qτ,π ps,aq´Qpm ps,aq´η pri `τHpπ psqq´Qpmps,aqq
H´1 H´1 m m,H´1 H´1 h
“p1´η qpQτ,π ps,aq´Qpm ps,aqq´η pri `τHpπ p¨|sqq´Qτ,π ps,aqq
m H´1 H´1 m m,H´1 H´1 H´1
“p1´η qpQτ,π ps,aq´Qpm ps,aqq´η ppR1ps,a,ρρρ´1 q´Rps,a,µ qq,
m H´1 H´1 m m,H´1 H´1
26astheentropytermHpπ p¨|sqqcancelsout. Denotetheσ-algebra
H´1
Fm :“Ftts ,a u ,s “s,a “au
s,a m1,h m1,h m1ăm m,H´1 m,H´1
p
foranyfixeds,a. Then,notingthatQm ps,aqisFm-measurable,wehavetheinequalities
H´1 s,a
ErpQτ,π ps,aq´Qpm`1ps,aqq2|Fms
„Hˆ´1 H´1 s,a ˙ ˇ ȷ
2ˇ
“E p1´η mqpQτ H,π ´1ps,aq´Qpm H´1ps,aqq´η mpR1ps,a,ρρρ´ m1 ,H´1q´Rps,a,µ H´1qq ˇ ˇF sm
,a
“p1´η q2pQτ,π ps,aq´Qpm ps,aqq2
m H´1 H´1
`2p1´η qη pQτ,π ps,aq´Qpm ps,aqqErR1ps,a,ρρρ´1 q´Rps,a,µ q|Fms
m m H´1 H´1 m,H´1 H´1 s,a
`η2 ErpR1ps,a,ρρρ´1 q´Rps,a,µ qq2|Fms
m m,H´1 H´1 s,a
WeuseYoung’sinequalityandusingthefactthatrewardsareboundedinr0,1s,
ErpQτ,π ps,aq´Qpm`1ps,aqq2|Fms
H´1 H´1 s,a
ďp1´2η qpQτ,π ps,aq´Qpm ps,aqq2`η pQτ,π ps,aq´Qpm ps,aqq2
m H´1 H´1 m H´1 H´1
`η ErpR1ps,a,ρρρ´1 q´Rps,a,µ qq2|Fms`p1`Q2 qη2
m m,H´1 H´1 s,a max m
ďp1´η qpQτ,π ps,aq´Qpm ps,aqq2`η Er2β2`2L2}σpρρρ´1 q´µ }2|Fms
m H´1 H´1 m µ m,H´1 H´1 2 s,a
`p1`Q2 qη2
max m
Wethentakeexpectationsandusethelawoftotalexpectationtoobtainthebound:
ErpQτ,π ps,aq´Qpm`1ps,aqq2s
H´1 H´1
ďp1´η qp ps,aqErpQτ,π ps,aq´Qpm ps,aqq2s
m H´1 H´1 H´1
`2η p ps,aqErβ2`L2}σpρρρ´1 q´µ }2|s “s,a “as
m H´1 µ m,H´1 H´1 2 m,H´1 m,H´1
`p ps,aqp1`Q2 qη2
H´1 max m
`p1´p ps,aqqErpQτ,π ps,aq´Qpm`1ps,aqq2s
H´1 H´1 H´1
ďp1´p ps,aqη qErpQτ,π ps,aq´Qpm ps,aqq2s
H´1 m H´1 H´1
`η p ps,aqEr2β2`2L2}σpρρρ´1 q´µ }2|s “s,a “as
m H´1 µ m,H´1 H´1 2 m,H´1 m,H´1
`p ps,aqp1`Q2 qη2
H´1 max m
Summingthisinequalityoverallstate-actionpairswithweightp ,weobtain
H´1
Er}Qτ,π ´Qpm`1}2 s
H´1 H´1 pH´1
ďp1´δη qEr}Qτ,π ´Qpm }2 s
ÿm H´1 H´1 pH´1
` η p ps,aqEr2β2`2L2}σpρρρ´1 q´µ }2|s “s,a “as
m H´1 µ m,H´1 H´1 2 m,H´1 m,H´1
s,a
`p1`Q2 qη2
max m
“p1´δη qEr}Qτ,π ´Qpm }2 s`p1`Q2 qη2
m H´1 H´1 pH´1 max m
`2η β2`2η L2 Er}µ ´σpρρρ´1 q}2s
m m µ H´1 m,H´1 2
ďp1´δη qEr}Qτ,π ´Qpm }2 s`p1`Q2 qη2 `2η pβ2`L2B q.
m H´1 H´1 pH´1 max m m µ H´1
27ś
Weexpandthisrecursiveinequalityasfollows. DefinetheshorthandnotationΠm1 :“ m1 p1´
m k“m
δη q. Then,foranyM ą0,
k
Er}Qτ,π ´QpM }2 s
H´1 H´1 pH´1
Mÿ´1 Mÿ´1
ďΠM´1`p1`2Q2 q η2 ΠM´1`2 η pβ2`L2B qΠM´1
0 max m m`1 m µ H´1 m`1
m“0 m“0
ďΠM´1`p1`2Q2 qη2 `2η pβ2`L2B q
0 max M´1 M´1 µ H´1
Mÿ´2 Mÿ´2
`p1`2Q2 q η2 ΠM´1`2 η pβ2`L2B qΠM´1. (18)
max m m`1 m µ H´1 m`1
m“0 m“0
We bound the multiplicative terms Πm1. Assuming that η is of the form η “ u , for any
m m m v`m
mďm1,wehavethat
źm1 ÿm1
Πm1 “ p1´δη qďexpt´δ η qu
m k k
k“m k“m
ÿm1
u m1`v
ďexpt´δ quďexpt´δulog u
v`k m`v´1
ˆ
k“m
˙
m`v´1 δu
ď
m1`v
usingLemma8. Takingthevaluesu“v “2δ´1,thisreducesto
ˆ ˙
m`u´1 2
Πm1 ď ,
m m1`u
PlacingthisinInequality18forthetwotermsappearingΠM´1,ΠM´1,weobtain
m`1 0
Er}Qτ,π ´QpM }2 s
ˆ
H´1 H´ ˙1 pH`1
ˆ ˙ ˆ ˙
u´1 2 u 2 u
ď `p1`2Q2 q `2 pβ2`L2B q
M `u´1 max M `u´1 M `u´1 µ H´1
ˆ ˙ ˆ ˙
Mÿ´2 u 2 m`u 2
`p1`2Q2 q
max m`u M `u´1
m“0
ˆ ˙ˆ ˙
Mÿ´2 u m`u 2
`2pβ2`L2B q
µ H´1 m`v M `u´1
m“0
C u
ď 1 `C upβ2`L2B q,
pM `u´1q 2 µ H´1
for some absolute constants C ,C . This inequality concludes the convergence result for the Q
1 2
valuesattimestepH ´1,showingarateofconvergenceOp1{MqoverM epochsuptoabiasof
Opβ2`α2`1{Nq.
Step2. Next,weanalyzethecasehăH´1. Againundertheobservationthatifs1 “s,a1 “
m,h m,h
a,s1 “s1,a1 “a1,thenitholdsalmostsurelythat
m,h`1 m,h`1
Qτ,πps,aq´Qpm`1ps,aq
h h
“Qτ,πps,aq´Qpmps,aq´η pQpm ps1,a1q`r1 `τHpπ p¨|sqq´Qpmps,aqq
h h m h`1 m,h h h
“p1´η qpQτ,πps,aq´Qpmps,aqq´η pQpm ps1,a1q`r1 `τHpπ p¨|sqq´Qτ,πps,aqq,
m h h m h`1 m,h h h
“p1´η qpQτ,πps,aq´Qpmps,aqq´η pQpm ps1,a1q`r1 ´qτ,πps,aqq,
m h h m h`1 m,h h
sinceagainastheentropytermsHpπ p¨|sqqcancel. Definingtheinducedσ-algebra
h
Fm :“Ftts ,a u ,s “s,a “au.
h m1,h m1,h m1ăm m,h m,h
28p p
NotethatwithrespecttoFm,Qmismeasurableforanyh1asQmonlydependsonepisodesprevious.
h h1 h1
wehavetheupperbound:
ErpQτ,πps,aq´Qpm`1ps,aqq2|Fms
h h h
“p1´η
q2ErpQτ,πps,aq´Qpmps,aqq2|Fms
m h h h
`pη q2ErpQpm ps1,a1q`r1 ´qτ,πps,aqq2|Fms
m h`1 m,h h h
´2η mpQτ h,πps,aq´Qpm hps,aqqErQlp oom ho`oo1ooposoo1 moo,oho`oo1oo,oaoo1 mooo,hoo`om1qoo`oooroom1 oo,ohooo´oooqohoτ o, oπ opooso,ooaonq|F hms
△
“p1´η q2pQτ,πps,aq´Qpmps,aqq2`η2 Q2
m h h m max
´2η
pQτ,πps,aq´Qpmps,aqqEr△|Fms,
m h h h
ďp1´2η qpQτ,πps,aq´Qpmps,aqq2`2η2 Q2
m h h m max
`η pQτ,πps,aq´Qpmps,aqq2`η Er△|Fms2,
m h h m h
ďp1´η qpQτ,πps,aq´Qpmps,aqq2`2η2 Q2 `η Er△|Fms2, (19)
m h h m max m h
p
as Qm is Fm measurable, using Young’s inequality in the last line. The last bias term due to
h h
bootstrappingandfinitepopulationbiasweboundseparately. Wedecomposep△qasfollowsusing
Young’sinequality.
ˆ ˙
Er△|Fms2 ď 1` 1 ErQpm ps1,a1q´Qτ,π ps1,a1q|Fms2
h pH ´h`1q2 h`1 h`1 h
ˇ ÿ ˇ
`2pH ´h`1q2ˇErQτ,π ps1,a1q|Fms´ Ppss,sa,µ qQτ,π pss,saqˇ2
h`1 h h h`1
ss,as
`2pH ´h`1q2|Err1 ´Rps,a,µ q|Fms|2
m,h h h
Thethreetermsareupper-boundedbytheinequalitiesinexpectation:
|ErQpm ps1 ,a1 q´Qτ,π ps1 ,a1 q|Fms|2
h`1 m,h`1 m,h`1 h`1 m,h`1 m,h`1 h
ďEr|Qpm ps1 ,a1 q´Qτ,π ps1 ,a1 q|2|Fms
h`1 m,h`1 m,h`1 h`1 m,h`1 m,h`1 h
ď}Qpm ´Qτ,π }2
ˇ
h`1 h`1 ph`1p¨|s,aq
ˇ
ˇ ÿ ˇ2
ˇ ˇErQτ h, `π 1ps1 m,h`1,a1 m,h`1q|F hms´ Ppss,sa|s,a,µ hqQτ h, `π 1pss,saqˇ ˇ
ss,as
Q2
ď max Er2α2`2K2}µ ´σpρρρ´1 q}2|Fms
4 µ h m,h 1 h
ˇ ˇ
ˇ ˇ2
ˇ ˇErr1 ´Rps,a,µq|Fmsˇ
ˇ
m,h h
“2|ErR1ps,a,ρρρ´1 q´Rps,a,σpρρρ´1 qq|Fms|2`2|ErRps,a,σpρρρ´1 qq´Rps,a,µ q|Fms|2
m,h m,h h m,h h h
ď2β2`2L2 Er}µ ´σpρρρ´1 q}2|Fms
µ h m,h 1 h
Therefore,weconcludebyanapplicationofYoung’sinequalitythatalmostsurely,
ˆ ˙
Er△|Fms2 ď 1` 1 }Qpm ´Qτ,π }2
h pH ´h`1q2 h`1 h`1 ph`1p¨|s,aq
`pH ´h`1q2Q2 rα2`β2`p2K2`4L2qEr}µ ´σpρρρ´1 q}2|Fmss.
max µ µ h m,h 1 h
WeplacethisresultinInequality19toobtain
ErpQτ,πps,aq´Qpm`1ps,aqq2|Fms
h h h
ďp1´η qpQτ,πps,aq´Qpmps,aqq2`2η2 Q2
mˆ h h˙ m max
`η 1` 1 }Qpm ´Qτ,π }2
m pH ´h`1q2 h`1 h`1 ph`1p¨|s,aq
`η pH ´h`1q2Q2 rα2`β2`p2K2`4L2qEr}µ ´σpρρρ´1 q}2|s “s,a “ass,
m max µ µ h m,h 1 m,h m,h
29as}µ ´σpρρρ´1 q}2isindependentoftheestimates. Then,takingexpectationsonbothsidesforany
h m,h 1
p ps,aqą0,andnotingthatp ps,aqěδ,
h „´ h ¯ ȷ
E Qτ,πps,aq´Qpm`1ps,aq 2
h h
ďp1´δη qErQτ,πps,aq´Qpmps,aqq2s`2η2 Q2
ˆm h ˙h m max
`η 1` 1 Er}Qpm ´Qτ,π }2 s
m pH ´h`1q2 h`1 h`1 ph`1
`η pH ´h`1q2Q2 pα2`β2q
m max
`η pH ´h`1q2Q2 p2K2`4L2qEr}µ ´σpρρρ´1 q}2|s “s,a “as
m max µ µ h m,h 1 m,h m,h
Finally,takingaweightedsumofbothsidesoftheinequalityovers,awithweightsp ,
” ı h
E }Qτ,π´Qpm`1}2 ďp1´δη qEr}Qτ,π´Qpm`1}2 s`2η2 Q2
h h ph ˆm h h ˙ph m max
`η 1` 1 Er}Qpm ´Qτ,π }2 s
m pH ´h`1q2 h`1 h`1 ph`1
`η pH ´h`1q2Q2 rα2`β2`p2K2`4L2qB s
m max µ µ h
ExpandingthisrecursiveinequalityandusingthesamenotationasinStep1forthemultiplicative
terms, also taking the inductive assumption that Er}Qp m h`1 ´Qτ h, `π 1}2 ph`1s ď 2δ´1G `1 m´1 `G 2 for
someG ,G whichdependsonproblemparametersbutnotonm,wehavethefinalinequality:
” 1 2 ı
E }Qτ,π´Qpm`1}2 ďΠM´1Er}Qτ,π´Qp0}2 s` C 3u
h h ph m“0 h h ph M ´2`v
Mÿ´2
`2 pη q2Q2 ΠM´1
m max m`1
m“0
Mÿ´2
` η pH ´h`1q2Q2 rα2`β2`p2K2`4L2qB sΠM´1
m max µ µ h m`1
m“0
ˆ ˙ˆ ˙
Mÿ´2
1 G
` η 1` 1 `G ΠM´1
m pH ´h`1q2 2δ´1`m´1 2 m`1
m“0
OnceagainasinStep1,fixingthevaluesu“v “2δ´1,
E” }Qτ,π´Qpm`1}2 ı ďQ2 ˆ u´1 ˙ 2 ` C 3u `Mÿ´2 Q2 8δ´2
h h ph max M `u´1 M ´2`v maxpM `u´1q2
m“0
Mÿ´2
2pm`uqδ´1
` pH ´h`1q2Q2 rα2`β2`p2K2`4L2qB s
max µ µ h pM `u´1q2
m“0
ˆ ˙ˆ ˙
Mÿ´2
1 G 2pm`uqδ´1
` 1` 1 `G
pH ´h`1q2 2δ´1`m´1 2 pM `u´1q2
ř
m“0
ř
Usingthefactthat M pm`uq«M2and M c“cM,forsomeabsoluteconstantswehave
m“1 m“1
that
” ı ˆ ˙
E }Qτ,π´Qpm`1}2 ď 2 Q2 maxδ´1 2 ` C 3δ´1 ` C 4Q2 maxδ´2
h h ph M `2δ´1´1 M ´2`2δ´1 M `δ´1´1
`C pH ´h`1q2Q2 rα2`β2`p2K2`4L2qB sδ´1
ˆ5 ma˙x ˆ µ µ h ˙
1 C δ´1G
` 1` 6 1 `C G δ´1 (20)
pH ´h`1q2 2δ´1`m´1 7 2
Step3. Finally,weconcludewiththeproofusingSteps1and2. ByusingInequality20,itreadily
followsthat ” ı ˆ ˙
E }Qτ,π´Qpm`1}2 “O 1 `α2`β2` 1 ,
h h ph M N
30for all h, as the bound in Step 1 established the rate for time step H ´1. We comment on the
constants: Inequality20showsthatintheworstcase,theremightbeanexponentialdependenceon
H,whichmightbefundamental.
D ExtendedResultsonMonotonicityandLearningNE
D.1 Example: AsymmetricCongestionGames
` ˘
Notethatbysymmetryinarguments,itfollowsthatSym Rips,a,¨q “Rips,a,¨qforanys,a,as
` ˘ ÿ
1
Sym Rips,a,¨q pρρρq“ Rips,a,gpρρρqq
pN ´1q!
fPS
N´1˜ ¸
1
ÿ ÿN
“ h ps,a, 1 q`r ps,aq
pN ´1q! i gpρρρqj“ps,aq i
˜ fPS N´1 ¸ j“1
ÿN
“h s,a, 1 `r ps,aq
i ρj“ps,aq i
j“1
“Rips,a,ρρρq
` ˘
Ę
Bysimplecomputation,thepopulationliftedrewardsSym Rips,a¨q aregivenby
` ˘
SĘ ym Rips,a¨q pµq“h ps,a,Nµps,aqq`r ps,aq, @µP∆ .
i i SˆA,N´1
Weprovideanextensiontothecontinuum∆ vialinearinterpolationinthiscase,whilemany
SˆA
r
otheralternativesarepossible. Takethefunctionh :SˆAˆr0,1sÑr0,1ssuchthat
i
r
h ps,a,uq:“pNu´tNuuqh ps,a,tNuuq`prNus´Nuqh ps,a,rNusq.
i i i
Thefunctionisclearlymonotonicallydecreasinginu. Furthermore,itisalsoLipschitzcontinuousin
u,asforanyu ąu ,
1 2
r r
h ps,a,u q´h ps,a,u qď|u ´u |.
i 1 i 2 1 2
Finally,theasymmetryduetorewardscanbeupperboundedby
β ďsupsup sup |h ps,a,kq´h ps,a,kq|.
i j
s,a i,j kPrNs
D.2 PreliminariesforLearningRegularizedMonotoneMFG
We present several results required to establish convergence under monotonicity. We define the
(entropyregularized)MFGvaluefunctionsforanarbitrarypopulationflowµP∆ andpolicy
SˆA
π PΠas
« ˇ ff
Hÿ´1 ˇ
ˇ
Vτps|µ,πq:“E Rps ,a ,µ q`τHpπ p¨|s qqˇs0“s, a h1„π h1ps h1q
h h1 h1 h1 h1 h1 ˇs h1`1„Pps h1,a h1,µ h1q
«h1“h ˇ ff
Hÿ´1 ˇ
ˇ
Qτps,a|µ,πq:“E Rps ,a ,µ q`τHpπ p¨|s qqˇsh“s,ah“a,s h1`1„Pps h1,a h1,µ h1q, .
h h1 h1 h1 h1 h1 ˇ a h1„π h1`1ps h1`1q,@h1ěh
h1“h
Wedefinetheregularizedvaluefunctionofthegamesimilarly:
« ˇ ff
Hÿ´1 ˇ
ˇ
Vτpµ,πq:“E Rps ,a ,µ q`τHpπ p¨|s qqˇs0„ρ0, ah„πhpshq .
h h h h h ˇ sh`1„Ppsh,ah,µhq
h“0
Asexpected,withthesedefinitionsitholdsthat
ÿ
Vτps|µ,πq“ π pa|sqQτps,a|µ,πq,
h h h
aÿPA
Vτpµ,πq“ ρ psqVτps|µ,πq.
0 0
sPS
31Wealsodefinethequantity
qτps,a|µ,πq:“Qτps,a|µ,πq´τHpπ p¨|sqq,
h h h
which corresponds to the more standard entropy regularized value function. Firstly, we provide
severalusefullemmasanddefinitions.
Definition 12 (Entropy regularized MFG-NE). For a given MFG pS,A,ρ ,H,P,Rq, a policy
0
π˚ PΠiscalledtheτ-entropyregularizedMFG-NEifitholdsthat
τ
maxVτpΛpπ˚q,π1q´VτpΛpπ˚q,π˚q. (RegularizedMFG-NE)
τ τ τ
π1PΠ
Whileentropyregularizationwillenabletheconvergenceofouralgorithm,itwillalsointroducea
biasintermsoftheoriginal(unregularized)MFG.Thenextlemmaquantifiesthisbias.
Lemma11(Regularizationbias). LetµP∆ andpolicyπ PΠbearbitrary. Then,itholdsthat
SˆA
|Vτpµ,πq´Vpµ,πq|ďτHlog|A|.
Furthermore,ifπ˚isaτ-entropyregularizedMFG-NE,thenitisa2τHlog|A|-MFG-NE,thatis,
τ
Epπ˚qď2τHlog|A|.
τ
Proof.
ˇ « ˇ ffˇ
ˇ
ˇ
Hÿ´1 ˇ
ˇ
ˇ
ˇ
|Vτpµ,πq´Vpµ,πq|“ˇE τHpπp¨|s qqˇs0„ρ0, ah„πhpshq ˇ
ˇ h ˇ sh`1„Ppsh,ah,µhq ˇ
« h“0 ˇ ff
Hÿ´1 ˇ
ˇ
ďE τ|Hpπp¨|s qq|ˇs0„ρ0, ah„πhpshq ď Hτlog|A|,
h ˇ sh`1„Ppsh,ah,µhq
h“0
since entropy is upper bounded by |Hpπp¨|s qq| ď log|A|. The bound for exploitability follows
h
from:
Epπ˚q“maxVpΛpπ˚q,π1q´VpΛpπ˚q,π˚q
τ τ τ τ
π1PΠ
“maxVpΛpπ˚q,π1q´VτpΛpπ˚q,π1q`VτpΛpπ˚q,π1q´VpΛpπ˚q,π˚q
τ τ τ τ τ
π1PΠ
ďτHlog|A|`maxVτpΛpπ˚q,π1q´VpΛpπ˚q,π˚q
τ τ τ
π1PΠ
ďτHlog|A|`maxVτpΛpπ˚q,π1q´VτpΛpπ˚q,π˚q`VτpΛpπ˚q,π˚q´VpΛpπ˚q,π˚q
τ τ τ τ τ τ τ
π1PΠ
ď2τHlog|A|`maxVτpΛpπ˚q,π˚q´VτpΛpπ˚q,π˚q
τ τ τ τ
π1PΠ
“2τHlog|A|.
Wenotethatinoursetting,amonotoneMFGexhibitsauniqueMFG-NE,infact,auniqueregularized
MFG-NEforanyvalueofτ [64]. Theabovelemmashowsthatthebiasintroducedduetoentropy
regularizationisoftheorderOpτqasexpected.
Finally,westateseveralstandardfactsaboutmonotoneMFG,adaptedfromvariousworks[64,42,41].
Lemma 12 (Monotone improvement lemma). Let µ,µr P ∆ P ∆H which are induced by
SˆA S,A
policiesπ,πr PΠrespectively,thatisΛpπrq“µrandΛpπq“µ. IftheMFGismonotone,thatis,if
Definition10issatisfied,thenitholdsthat:
Vτpµ,πq`Vτpµr,πrq´Vτpµ,πrq´Vτpµr,πqď0.
Proof. Let µ “ tµ uH´1,µr “ tµr uH´1. The proof follows [64], except for the absence of a
h h“0 h h“0
graphon. FormonotoneMFG,duetotheassumptionthatP doesnotdependonµ,itholdsthat
Vτpµ,πq´Vτpµr,πq“V pµ,πq´V pµr,πq,
Vτpµr,πrq´Vτpµ,πrq“V pµr,πrq´V pµ,πrq.
32Furthermore,itholdsthat
ÿÿ
V pµr,πrq´V pµ,πrq“ µr ps,aqpRps,a,µr q´Rps,a,µ qq,
h h h
ÿh ÿs,a
V pµ,πq´V pµr,πq“ µ ps,aqpRps,a,µ q´Rps,a,µr qq.
h h h
h s,a
Then,usingthemonotonicityassumptionontherewards,itholdsthat
Vτpµ,πq`Vτpµr,πrq´Vτpµ,πrq´Vτpµr,πq
“V pµ,πq`V pµr,πrq´V pµ,πrq´V pµr,πq
ÿÿ ÿÿ
“ µr ps,aqpRps,a,µr q´Rps,a,µ qq` µ ps,aqpRps,a,µ q´Rps,a,µr qq
h h h h h h
ÿh ÿs,a h s,a
“ pµr ps,aq´µ ps,aqqpRps,a,µr q´Rps,a,µ qqď0.
h h h h
h s,a
The next lemma is simply an adaptation of the standard MFG performance difference lemma in
single-agentRLtotheMFGsetting.
Lemma13(Performancedifferencelemma). ForanarbitraryMFG,letπ,πr PΠandµ“Λpπq.
« ˇ ff
Hÿ´1 ˇ
ˇ
V 0τps|µ,πrq´V 0τps|µ,πq`τE πr,µ D KLpπr hp¨|s hq|π hp¨|s hqqˇ ˇs 0 “s
« h“0 ff
Hÿ´1
“E xqτps ,¨|µ,πq´τlogπ p¨|s q,πr p¨|s q´π p¨|s qy|s “s .
π˜,µ h h h h h h h h 0
h“0
Proof. Seethestandardprooftechniquefortheperformancedifferencelemma,e.g. [37,64].
Finally,westatetwotechnicallemmasdueto[64].
Lemma14(LemmaI.3of[64]). Letp,p1 P∆ bearbitrary,andpp“p1´βqp`βUnifpAqfor
A
someβ Pp0,1q. Then,
|A|
D pp˚|ppqďlog ,
KL β
β
D pp˚|ppq´D pp˚|pqď .
KL KL 1´β
Lemma 15 (Lemma 3.3 in [6]). Let p,p˚ P ∆ , α ą 0 and g : A Ñ r0,Gs be arbitrary, and
A
q P∆ beadistributionsuchthatqp¨q9pp¨qexptαgp¨qu. Then,
A
αG2
xgp¨q,p˚p¨q´pp¨qyď `α´1rD pp˚|pq´D pp˚|qqs.
2 KL KL
D.3 ExtendedProofofTheorem3
As mentioned before, the proof is an adaptation of [64] to setting where learning occurs with N
potentiallyasymmetricagents. ThemaindifferenceswillbetheabsenceofanexplicitMFGandthe
factthatouralgorithmsareonlyallowedtousesamplesoffiniteagenttrajectories.
Definetherandomvariableµ :“tµ uH´1 :“Λpπ q,whichisthemean-fieldpopulationdistribu-
t t,h h“0 t
tioninducedbythepolicyatepocht. Wedenotetherandomvariablesduetoestimationerrorofthe
q-functionsatepocht,timestephandanarbitrarystatesas
ˇ ˇ
Es :“ˇ xqptps,¨q´qτps,¨|µ ,π q,π˚p¨|sq´π p¨|sqyˇ .
t,h h h t t h t,h
33Furthermore, let π˚ be the unique τ-regularized MFG-NE, and µ˚ :“ tµ˚uH´1 “ Λpπ˚q. We
h h“0
definethequantity
Hÿ´1
∆ :“ E rD pπ˚p¨|s q}π p¨|s qqs,
t sh„µ˚
h
KL h h t,h h
h“0
whichwillbethemainquantityoferrortobeboundedusingthetechniquesof[64]. Wedenotethe
mixingcoefficientsβ
t
:“ 1{t`1forgenerality. Finally, wealsodefinethedistributionmismatch
coefficients
µ˚ps,aq
C :“supsup h ,
dist t,h s,a µ t,hps,aq
whicharealwaysfinite(andbounded)inourentropy-regularizedsetting(seeforinstance[12]). Itis
well-knownthatthePMDupdatewithentropyregularizationcanbewrittenas
πp pa|sq9π pa|sq1´τξtexptξ qptps,aqu,
t`1,h t,h t h
withappropriatenormalization.
ByLemma 14,itholds(almostsurely)foranysPS that
β
D pπ˚p¨|sq|π p¨|sqqďD pπ˚p¨|sq|πp p¨|sqq` t .
KL h t`1,h KL h t`1,h 1´β
t
UsingLemma15andthefactthatπ pa|sqěβ forallps,aqPSˆA,
t t
D pπ˚p¨|sq|π p¨|sqq
KL h@ t`1,h D
ď´ξ qpt ps,¨q´τlogπ p¨|sq,π˚p¨|sq´π p¨|sq
t h h h ˆ t,h ˙
1 |A| 2 β
`D pπ˚p¨|sq|π p¨|sqq` ξ2 H `τHlog|A|`τlog ` t
KL h t,h 2 t β 1´β
t t
ď´ξ xqτps,¨|µ ,π q´τlogπ p¨|sq,π˚p¨|sq´π p¨|sqy
t h t t h ˆ h t,h ˙
1 |A| 2 β
`D pπ˚p¨|sq|π p¨|sqq` ξ2 H `τHlog|A|`τlog ` t `ξ Es ,
KL h t,h 2 t β 1´β t t,h
t t
Then,usingtheperformancedifferencelemma(Lemma13),
∆ ´∆
t`1 t
Hÿ´1
:“ E rD pπ˚p¨|s q|π p¨|s qq´D pπ˚p¨|s q}π p¨|s qqs
µ˚ KL h h t`1,h h KL h h t,h h
h
h“0
Hÿ´1
ďξ rVτpµ ,π q´Vτpµ ,π˚qs´τξ E rD pπ˚p¨|s q}π p¨|s qqs
t t t t t µ˚ KL h h t,h h
h
h“0
ˆ ˙
1 |A| 2 β Hÿ´1
` ξ2H H `τHlog|A|`τlog ` t H `2ξ E rE s
2 t β t´1 1´β t t
h“0
µ˚ h t,h
ˆ ˙
1 |A| 2 β Hÿ´1
ď´τξ ∆ ` ξ2H H `τHlog|A|`τlog ` t H `2ξ E rE s,
t t 2 t β t´1 1´β t t
h“0
µ˚ h t,h
wherethelastinequalityfollowsfromthemonotoneimprovementlemma(Lemma12)appliedtothe
policypairπ ,π˚. Rearrangingbothsides,
t
ˆ ˙
1 ξ |A| 2 β H
∆ ď p∆ ´∆ q` tH H `τHlog|A|`τlog ` t
t τξ t t`1 2τ β p1´β qτξ
t t´1 t t
2
ÿH
` E rε s.
τ µ˚ h h
h“1
34Summingthisinequalityfromt“1,...,T,weobtain
ˆ ˙
1 ÿT 1 ξ |A| 2 β H
∆ ď ∆ ` tH H `τHlog|A|`τlog ` t
T t Tτξ 1 2τ β p1´β qτξ
t“1 t t´1 t t
2
ÿH
` E rε s.
τ µ˚ h h
h“1
Giventhatξ
t
“1{? t`1andβ
t
“1{t`1,weobtainthebounds
” ı
ř ř
1 ÿT
∆ “
τ´1∆ 1`τ´1H2`τH ?log|A|`τlog2pT `1q
`
T t“1 H h“´ 01E µ˚
h
E ts ,h
,
T t T Tτ
t“1
andfinallyusingYoung’sinequalityonthelastterm,andanapplicationofPinsker’sinequality,
„ ȷ
1
ÿÿ
1 1
ÿT
E }π p¨|s q´π˚p¨|s q}2 ď ∆
T µ˚ h 2 t,h h h h 1 T t
t h t“1 ” ı
ř ř
ď
τ´1∆ 1`τ´1H2`τH ?log|A|`τlog2pT `1q
`
2 T t“1 H h“´ 01E µ˚
h
E ts ,h
,
T Tτ
ř ř “ ‰
ď
τ´1∆ 1`τ´1H2`τH ?log|A|`τlog2pT `1q
`
T t“1 H h“´ 01E µ˚
h
8}qp htps h,¨q´q hτps h,¨|µ t,π tq}2 2
,
T Tτ2
ř ř “ ‰
T H´1E }π p¨|s q´π˚p¨|s q}2
t“1 h“0 µ˚ t,h h h 2
` h .
4T
Rearrangingtheterms,
„ ȷ
ÿÿ
1 1
E }π p¨|s q´π˚p¨|s q}2
T µ˚ h 2 t,h h h h 1
t h
4τ´1∆ `4τ´1H2`4τHlog|A|`4τlog2pT `1q
ď 1 ?
T
ř ř “ ‰
T H´1E 32}qptps ,¨q´qτps ,¨|µ ,π q}2
t“1 h“0 µ˚ h h h h t t 2
` h .
Tτ2
Finally, not“ing that by Theorem ‰2, after taking expectations on both sides it holds
that E “r řE
µ˚
4}qp htps h,¨q´q hτps h,¨ ‰q}2
2
s “ Opε2 ` α2 ` β2 ` 1{Nq, if follows that
h
ErE
µ˚
h}πs hp¨|s hq´π h˚p¨|s hq}2
2
sďOpτ´2ε2`τ´2α2`τ´2β2`τ´21{Nq.Usingthestandard
h
Lipschitzcontinuityofexploitability(seee.g. [60]),theexploitabilityboundinexpectationholds.
UsingLemma11,weobtaintheupperboundinexpectationontheexploitabilityoftheoutputpolicy
πsintermsoftheoriginal(unregularized)DG.
E DetailsofExperiments
E.1 HardwareSetupforExperiments
ExcepttheA-Taxibenchmark,allourexperimentsareCPU-based. WeuseasingleAMDEPYC
7742CPU,equippedwith128GBRAM.FortrainingpolicyandvalueneuralnetworkswithPPO
intheA-Taxibenchmark,weuseasingleRTX3090GPU.Withthissetup,runningSymm-PMD
andIPMDintheA-SISandA-RPSbenchmarkstakesbetween5-20minutes,andrunningPPOon
A-Taxitakesapproximate2hours. EvaluatingexploitabilityforagivenpolicyonA-SISandA-RPS
takesaround2hours,asweemployabrute-forceUCB-typebanditalgorithmtoaccuratelyestimate
bestresponseinthissetting.
35E.2 ExtendedDescriptionsoftheExperimentalSetup
Forsimplifiednotation,denotethestate-actionmarginaldensities
ÿ
σ pρρρ,aq“ σpρρρqps1,aq,
actions
sÿ1PS
σ pρρρ,sq“ σpρρρqps,a1q.
states
s1PA
Modifiedrock-paper-scissors(A-RPS). Weformulateamodifiedpopulationrock-paper-scissors
gameinspiredbytheformulationof[13]. Ourversionincorporatesvaryingpreferencesbetween
agentsbetweenpossiblemovesaswellasacrowdednesspenalty.
A-RPSconsistsofthreestatesS :“tR,P,SuandthreeactionsA:“tR,P,Su. WeuseN “2000
playersandatimehorizonofH “10,thoughthesecanbeincreasedordecreasedarbitrarily. We
definetherewardsasfollows.
Rips“R,a,ρρρ´iq“´ciσ pρρρ,aq´ui σ pρρρ,Pq`vi σ pρρρ,Sq,
actions R states R states
Rips“P,a,ρρρ´iq“´ciσ pρρρ,aq´ui σ pρρρ,Sq`vi σ pρρρ,Rq,
actions P states P states
Rips“S,a,ρρρ´iq“´ciσ pρρρ,aq´uiσ pρρρ,Rq`viσ pρρρ,Pq.
actions S states S states
Thecoefficientsci,ui,viareuniqueforeachagentindicatingtheirownutilities/rewardsduetolosing,
winning,orindividualpenaltyduetocrowdedness. Thestatetransitionsaredeterministicandare
givenby:
Pips1|s,a,ρρρ´iq“1 .
s1“a
Wegeneratethefixedcoefficientsui,virandomlybyaddingboundednoisetocoefficientsfrom[13],
sothat
ui “2`εi , vi “1`εsi
R R R R
ui “4`εi , vi “2`εsi
P P P P
ui “6`εi, vi “3`εsi.
S S S S
Therefore,themagnitudesoftheplayer-specificadditivetermsdetermineβ. InthecaseofA-RPS,
α“0.
Infectionmodelingwithasymmetricagents(A-SIS).Thisbenchmark,inspiredbytheSISbench-
mark of [13], models a large population of infected or healthy agents that can choose to go out
orremaininisolation. UnliketheSISbenchmark,A-SISisformulatedasanN-playergameand
incorporatesindividualdifferencesinnaturalsusceptibilities,recoveryrates,andaversionofisolation
betweenagents. Weformalizethedynamicgameasfollows. Thegameconsistsofthestatespace
S “tI,Hu(I indicatinginfected,H indicatinghealthy),andactionspaceA“tD,Uu(Dindicat-
ingsocialdistancing,U indicatinggoingout). Theinitialstatessi aresampledi.i.d. fromauniform
0
distributionoverS. EachagentiPN hasafixedsusceptibilityparameterα Pr0,1s,afixedhealing
i
probabilityθ Pr0,1sandafixedaversiontoisolationparameterξ Pr0,1s.
i i
PipI|H,D,ρρρ´iq“0
PipI|H,U,ρρρ´iq“α ˚σpρρρqpI,Uq,
i
PipI|I,D,ρρρ´iq“1´θ ,
i
PipI|I,U,ρρρ´iq“1´θ .
i
Theprobabilitiesofstayinghealthyareofcoursealwaysdefinedby
PipH|s,a,ρρρ´iq:“1´PipI|s,a,ρρρ´iq.
Therewardsofeachagentaregivebythefollowingwhichincorporatesapenaltyforillnessandan
agentspecificpenaltyforisolation:
Rips,a,ρρρ´iq“´1 ´ξ 1 .
s“I i a“D
36Parameter Value
Initiallearningrate 2.5e´4
Learningrateschedule Linear
γ (discountfactor) 0.999
λ (see[48]) 0.95
GAE
Entropyregularization 6e´3
Valuelosscoefficient 0.9
Maximumgradientnorm 0.5
Clipcoefficient 0.2
Sampletrajectoriesperepoch 1
NNtrainingpassesperepoch 4
Minibatchsize 16384
Advantagenormalization Yes
Table2: HyperparametersofthePPOalgorithm.
Theagentparametersα ,θ ,ξ areasexpectedfixedthroughoutthegame,andaresampledtobe
i i i
close. InthecaseofA-SIS,wesolveN “1000agentswithatimehorizonofH “20.
Asymmetrictaxi(A-Taxi). Finally,asamorecomplicatedbenchmarkweadapttheTaxiAsin[13],
weusethefollowinglayoutofthecitymap,whereS indicatesthestartingcellofallagents,bothH
andS areimpenetrablebarriersandtherestofthecityisdividedinto2zones.
¨ ˛
1 1 1
˚ 1 1 1 ‹
˚ ‹
˚ 1 1 1 ‹
˚ ‹
˚ H S H ‹
˚ ‹
2 2 2
˝ ‚
2 2 2
2 2 2
TheactionspaceofeachagentisA:“tU,D,L,R,Wu,indicatingactionstomoveinfourdirections
(up,down,left,rightrespectively)andwaitatthecurrentlocation. Customerscanonlybepicked
upwhilewaiting,anddeliveredwhilewaiting. Eachcellinthegridgeneratesanewcustomerwith
probability0.2,whichtheagentscanobserve.Uponpickingupacustomer,arandomtargetcoordinate
isgeneratewithinthesamezone.Customerscanonlybeleftattheirtargetcells.Successfuldeliveries
ofcustomersinzone1generateabaserewardof1.1,whereassuccessfuldeliveriesofcustomersin
zone2generatealowerrewardof1.0. Furthermore,eachagenthasazonespecificrewardmultiplier
β1,β2 ą 0,sothatagentibydeliveringacustomerinzone1gainsreward1.1β1 andviceversa.
i i i
Thismodelsvaryingefficienciesoftaxidriversaswellasindividualpreferencestovariouszonesof
thecity.Furthermore,weincorporateacrowdednesspenalty:ařnagentiPrNsatstatesattimehwill
notmove(simulatingajammingeffect)withprobabilitymint w 1 ,0.7u,wheretw uN are
j j sj“s j j“1
h
playerspecificweightsindicatingtheircontributiontotrafficjams. Thisintendstosimulateunique
contributionsofeachdrivertotrafficjams,presumablyduetovehicletypes,drivingstyles,etc.
Thenumberofstatesinthegameisontheorderof230,makinganeuralnetworkapproximation
fundamental. Forthisreason,weusevalueandpolicynetworkswithtwohiddenlayerswith128
neuronseach,withaleakyReLUnonlinearity. WeadoptthePPOimplementationofCleanRL[26]
forourpurposes. ThehyperparametersusedareindicatedinTable2.
E.3 ExtendedExperimentalResults
Wereporttwoadditionalsetsofresultsregardingthesensitivityofouralgorithmstoα,β andthe
populationdistributionbehvaiourintheA-Taxienvironment.
InFigure2-(a),wereportthesensitivityoftheexactMFG-NE(computedviacodeprovidedin[21])
toheterogeneityparametersα,β intermsofexploitabilityintheN “ 1000playergame. While
keepingtheotherparameterconstantat0,wesweepthroughvariousvaluesofeachofα,β inthe
rangep0,1{4q. Whilearoundthe0.1threshold,theexploitabilityrisesasexpected,forsmallervalues
37ofα,β thebiasintroducedisverysmall,providinganempiricalanalysisoftheapproximationbound
ofTheorem1.
(a) (b)
Figure2: (a)ThesensitivityoftheMFG-NEtoheterogeneityparametersα,β intheA-SISenvi-
ronment,intermsofexploitability. (b)PercentageofvehiclesinZone1intheA-Taxienvironment
throughouttrainingepochsfor4benchmarkalgorithms.
In Figure 2-(b), we keep track of number of taxis choosing to operate in Zone 1 in the A-Taxi
environmentthroughouttraining. WhilerewardsinZone2arehigherinthisenvironment,congestion
effectsrequireamixedNashequilibrium: agentsmustrandomlychooseattheveryfirststeptoserve
eitherZone1or2. Thefiguredemonstratesthemainadvantageofpolicy-basedmethodsforlearning
Nash: unlikemostvalue-basedmethods,PPOcanlearnamixedstrategyinsteadofconvergingtoa
deterministicpolicy. Asanadditionalbenchmark,inthefigureweevaluateSymm-NNwithoutany
entropyregularization(τ “ 0,shownbythelineSymm-NN-NR).Inthiscase,thepolicyrapidly
convergestoadeterministicpolicy,indicatingthatanon-zeroentropyregularizermightbenecessary
forlearningaNashequilibrium.
38