The Benefits of Balance:
From Information Projections to Variance Reduction
LangLiu∗ RonakMehta∗ SoumikPal ZaidHarchaoui
UniversityofWashington,Seattle
May24,2024
Abstract
Databalancingacrossmultiplemodalities/sourcesappearsinvariousformsinseveralfoundationmodels(e.g.,
CLIPandDINO)achievinguniversalrepresentationlearning.Weshowthatthisiterativealgorithm,usuallyusedto
avoidrepresentationcollapse,enjoysanunsuspectedbenefit:reducingthevarianceofestimatorsthatarefunctionalsof
theempiricaldistributionoverthesesources.Weprovidenon-asymptoticboundsquantifyingthisvariancereduction
effectandrelatethemtotheeigendecaysofappropriatelydefinedMarkovoperators.Weexplainhowvariousformsof
databalancingincontrastivemultimodallearningandself-supervisedclusteringcanbeinterpretedasinstancesofthis
variancereductionscheme.
1 Introduction
Deepneuralnetworkshaveshownremarkablesuccessatlearningtask-specificrepresentationsofdatawhenprovided
supervisionfrommassiveamountsoflabeledtrainingexamples. Recenttrends,however,haveshiftedtowardtask-
agnostic,universalrepresentationsthatmaybeeasilyfine-tunedorevenhavezero-shotcapabilitiesout-of-the-box.
Supervisedlearning,strictosensu,istoolimitedaframeworkforthesebillion-parameter,data-hungrymodels,anda
questionattheheartofmodernmachinelearningislearningfromunlabelled,partiallylabeled,orweaklylabeleddata.
Thisneedhaspavedthewayforthecurrentgenerationofself-supervisedlearning(SSL)approachesthatcircumvent
theneedforlargeamountsofstronglabels. InSSL,amodelistrainedonagenericpseudo-taskthatcanbeperformed
onunlabelleddata,suchasrelatingthetwomodalitiesofanimage-captionpairortwoaugmentationsofthesame
image. DespiteseveralmodernfoundationmodelssuchasDINO(Caronetal.,2021;Oquabetal.,2024)andCLIP
(Radfordetal.,2021)beingtrainedinthisfashion,manyaspectsofSSLremainbaffling.
Inparticular,thetrainingprocessofself-supervisedmodelsoftenoutgrowsand“breakstherules”ofthestandard
empiricalriskminimization(ERM)toolkit. ERMcombinestwowell-understoodtechniques: minibatchsamplingand
gradient-basedoptimizationusingbackpropagation. SSL,ontheotherhand,addsclever,less-understoodtechniquesto
thetrainingpipeline. Toillustratethis,consideraminibatchM ={Z ,...,Z }oftrainingexamplesandletP be
n 1 n n
theempiricaldistributionoftheminibatch. Foramodelparameterizedbyθ ∈Rdwithassociatedlossfunctionℓ ,a
θ
standardstochasticsupervisedlearningalgorithm,e.g.,Adam,involvescomputingtheminibatchloss
n
1 (cid:88)
E [ℓ (Z)]= ℓ (Z ) (1)
Z∼Pn θ n θ i
i=1
andbackpropagatingthroughittoproduceaminibatchstochasticgradientestimate. Thealgorithmthenproceedswith
thestochasticgradienttraining,oravariantthereof.
Ontheotherhand,self-supervisedmethodsoftenmodifythisrecipebyinterveningontheoptimizationalgorithmin
aminibatch-specificway. Forexample,SwaV(Caronetal.,2020)passestheminibatchexamplesthroughthemodel’s
encoderandclustersoutputvectorstogeneratepseudo-labelsforapredictiontask. Inteacher-studentarchitectures
suchasBYOL(Grilletal.,2020)andDINO(Caronetal.,2021),thedataarepassedthroughtwonetworks,where
*Theseauthorscontributedequallytothiswork.
1
4202
guA
72
]LM.tats[
1v56051.8042:viXrathe“student”isupdatedviabackpropagationandthe“teacher”isupdatedbycloningthestudent’sweightsinregular
intervals.InCLIP(Radfordetal.,2021),amodeloptimizesthesumoftwocrossentropylossterms,wherethepredicted
classprobabilitiesonexampleiaregeneratedbycomparisontoallotherelementsoftheminibatch. Thesestepsare
oftenmotivatedasconstraintstoavoidrepresentationcollapse, whereinthepseudo-taskcanbesolvedbyatrivial
representation,e.g.mappingalldatatothezerovector. Conceptually,however,itisdifficulttoseewhatexactlyisbeing
optimizedwhenintroducingsuchinterventionsintotheprocedure.
Inthiswork,weaimtogainabettertheoreticalunderstandingoftheobjectivesandalgorithmsunderlyingthese
empiricallyeffectiverecipes. Inparticular,wewanttoshedatheoreticallightontheirmarginalbenefits. Weshowthat
suchrecipesenjoyanunsuspectedbenefit: variancereduction. Weshallmakethispreciseinthefollowingsectionsand
providehereahigh-levelsummary.
LetX andY betwosamplespaces–whichweshallcallsources–andleth :X ×Y →Rbealoss. Recalling
θ
M ,wevieweachstepofthetrainingalgorithmsaboveasexactlyorapproximatelyoptimizing
n
E [h (X,Y)], (2)
(X,Y)∼Pθ(Mn) θ
where P (M ) is a probability measure over X ×Y that depends on the model. This reduces to empirical risk
θ n
minimization when Z = (X,Y) (e.g. feature-label pairs) and P (M ) = P . We emphasize, however, that the
θ n n
“derived”pair(X,Y)isnotnecessarilythesameastheoriginaldatapointZ. UsingtheSSLexamplesabove,while
{Z ,...,Z }mayrepresentnimages,X maybethespaceofvectorrepresentationsandY thespaceofclusterlabels.
1 n
OnespecificexampleofP hasbeenappliedacrossvariousfamiliesofself-supervisedlearningaswedetailinSec.2,
θ
whichwerefertoasdatabalancingorsimplybalancing,theprimarysubjectofthiswork.
Given an initial probability measure R over X ×Y, and target marginal distributions P on X and P on Y,
X Y
balancingreferstomodifyingRbyrepeatedlyapplyingtheoperations
R=R ·R (cid:55)→P ·R and R=R ·R (cid:55)→P ·R , (3)
X Y|X X Y|X Y X|Y Y X|Y
where R and R are the marginal distributions of R on X and Y, and R and R denote the respective
X Y Y|X X|Y
conditionaldistributions. WhenX andY arefinitewith|X|=mand|Y|=l,theseoperationsreducetorescalingthe
rowsofan(m×l)-matrixbyP /R anditscolumnsbyP /R . Thisalgorithmhasadecades-oldhistory,andis
X X Y Y
knowninothercontextsastheSinkhorn-Knoppmatrixscalingalgorithm(Sinkhorn,1967),iterativeproportionalor
biproportionalfitting(JohnstonandPattie,1993),andraking-ratioestimation(Thompson,2000). ThemarginalsP
X
andP representauxiliary,sideinformation,orinductivebias,e.g.,thedesireforbalancedclusters,fromusers.
Y
As we describe in Sec. 2, both self-labeling and contrastive approaches in SSL embed a (learnable) balancing
operationintheirobjectives,wheretheobjectivedependslinearly,asin(2),ornonlinearlyonthebalancedmeasure
P (M ). To be more specific, the operations (3) are applied in an alternating fashion to generate a sequence
θ n
R(0) :=R,R(1),...,R(k)andreturnthelastiterateasthemeasureP (M ). Anaturalquestiontoconsideris: ifthe
θ n
marginalsoneusesaccuratelyrepresenttheonesofatrueprobabilitymeasureP governingthedata, arebalanced
quantities“betterbehaved”thantheirunbalancedcounterparts? Ifso,inwhatway?
Inspiredbythisobservation,weformalizetheproblemasfollows. LetX andY betwofinitesamplespaceson
whichthereisanunknownprobabilitymeasureP withknownmarginals(P ,P ). Weobserveindependentdata
X Y
(X ,Y ),...,(X ,Y )∼P,definingtheempiricalmeasureP = 1 (cid:80)m δ . LetP(0) =P andP(k)denote
1 1 n n n n i=1 (Xi,Yi) n n n
theoutputofk ≥1iterationsofdatabalancing(seeSec.3fortheprecisedefinition). Inotherwords,weconsiderthe
casewhentheinitialreferencemeasureR=P . Finally,lettingh:X ×Y →Rbeafunctionofinterest,wedefine
n
thepopulationparameterψandbalancedestimatorψ(k)by
n
ψ :=E [h(X,Y)] and ψ(k) :=E [h(X,Y)]. (4)
(X,Y)∼P n (X,Y)∼Pn(k)
Ourgoalistoestablishtheoreticalguaranteesonthemeansquarederror(MSE)E [(ψ(k)−ψ)2]ofestimatingψusing
P n
ψ(k),withaninformativedependenceonthesamplesizen,numberofiterationsk,targetmarginals(P ,P ),and
n X Y
testfunctionh. Weareparticularlyinterestedinitscomparisontothedirectestimatorbasedontheempiricalmeasure
ψ(0) = 1 (cid:80)n h(X ,Y ), as to quantify the effect of the auxiliary information (P ,P ). Our analysis uncovers
n n i=1 i i X Y
twosurprisingfacts. Firstly,whileoriginallyproposedforadifferentpurpose,balancingprovidesvariancereduction.
Secondly,whilebalancingoperatesinanonlinearfashionontheinitialmeasure,thevariancereductioncanbeprecisely
quantifiedusingthespectraldecayoftwolinearMarkovoperators: theconditionalmeansgivenX andY,respectively.
2Contributions. InSec.2,weclarifythemathematicalconnectionbetweentheclassicaldatabalancingmethodsand
themodernrepresentationlearningtechniquesmentionedabove. InSec.3,weproveanewupperboundontheMSE
ofthebalancingestimatorψ(k). ThebounddecomposesintoanO(n−1)first-ordervariancetermandanO(n−3/2)
n
second-orderterm. Thefirst-ordertermisshowntohaveastrictimprovementovertheempiricalmeasurebaseline
withafine-graineddependenceonthespectrumoftwoparticularMarkovoperators. Theprooftechniquereliesona
recursiondecompositionforbalancingestimators,whichmaybeofindependentinterest. InSec.4,weillustratehow
insightsfromtheanalysistranslatetovariantsoftheCLIPobjective.
RelatedWork. Self-supervisedlearninghaswitnessedasurgeofrecentinterestasdatasetsandcomputinghardware
allowforlarger,morecapablemodels(seeBalestrieroetal.(2023)andreferencestherein). Whiledatabalancingrelates
mosttothe“self-distillation”familyofapproaches(Grilletal.,2020;Caronetal.,2020;ChenandHe,2021;Oquab
etal.,2024),wehighlightinthispapertheconnectionstocontrastiveapproachesaswell(Radfordetal.,2021).
Historical motivations for data balancing include census or survey data, in which P is a cross-tabulation of
n
(alimitednumberof)pairedobservationsandthetargetmarginalswereestimatedfromlargeamountsofunpaired
observations(DemingandStephan,1940;IrelandandKullback,1968). Thissituationisnotunlikethepresentday
–yetatadifferentscale,ofcourse–inwhichtheamountofunstructuredsingle-modalitydata(suchasimages)still
dwarfstheamountofhigh-qualitymultimodaldata(Gadreetal.,2023). Bickeletal.(1998)provedclassicalasymptotic
resultsonbalancingestimators. LinearoperatorssimilartotheonesweuseinSec.3alsoappearintheiranalysis. More
recently,AlbertusandBerthet(2019)studiedsuchestimatorsfromanasymptoticempiricalprocessviewpoint. Our
theoreticalresultssignificantlyimproveonthosefromAlbertusandBerthet(2019)primarilyinthedependenceofthe
numberofiterationskonthesamplesizentoachieveconvergenceguarantees(fromlogarithmictopolynomial).
Matrixscalingisapopularalgorithmtosolveentropy-regularizedoptimaltransport(EOT).Wereferto(Peyréand
Cuturi,2019)forasurvey. Seealso(Courtyetal.,2017;Shenetal.,2018;Pengetal.,2019)forinterestingmethods
basedonEOTinmachinelearning. Entropy-regularizedoptimaltransportwasoneoftheoriginalinspirationsforSSL
techniquessuchasSwaV(seeSec.2). WhileEOTisitselfadeterministicoptimizationproblem,arelatedstatistical
problemisthelarge-samplelimitsofEOTsolutionswhenthemarginalmeasuresareestimatedfromdata(Menaand
Niles-Weed,2019;Genevayetal.,2019;Klattetal.,2020). Weemphasizethat,whilethislineofworksharesthematrix
scalingalgorithmwithoursetting,thestatisticalproblemisentirelydistinct;instatisticalEOT,thetargetmarginal
distributionsarecomputedfromobservationsofindependent,unpaireddata,andtheinitialmeasurecanbecomputed
fromthecostfunction. Inoursetting,thedataaredependent,formingtherandominitialmeasureP ,whereasP and
n X
P arefixedauxiliaryinformation.
Y
2 Data Balancing in Practice
In this section, we expand on the examples of data balancing applications mentioned in Sec. 1. To demonstrate a
preciseconnectionto(2), wedescribehowacollectionoftrainingexamplesM = {Z ,...,Z }observedinan
n 1 n
originaldataspaceZ (e.g.grayscaleimages)ismappedtoaprobabilitymeasureP (M ). Wethenspecifythederived
θ n
samplespacesX andY ofthesources,theirsizesmandl,andhowthemodelmapstheoriginaldataintothesespaces.
Themodelalsodefinestheinitial(a.k.a.reference)measureR(0)onX ×Y andwemayidentifythetargetmarginals
θ
(P ,P )forthismeasuretofit. Fromthispoint,P (M )isproducedbyapplying(3)inanalternatingfashionto
X Y θ n
generatethesequenceR(0),...,R(k)andreturningthelastiterate,wherekisspecifictoeachexample. Theexamples
θ θ
arenamedbythedatatypesofX andY andvisualizedinFig.1.
Example1: TrainingExamplesandClusterAssignments. Awell-knowninstanceofbalancingisself-labeling
viaclustering;see (Asanoetal.,2020;Caronetal.,2020;Jonesetal.,2022)forseveralvariants. Wedescribethe
swappedpredictiontaskofCaronetal.(2020)forconcretenessbutemphasizethatclusteringofthisformisusedas
anintermediatestep(orasthetaskitself)inmanySSLpseudo-tasks. Atahighlevel,thisapproachinvolvespassing
elements of a minibatch through two encoders to generate vector representations. These representations are then
clusteredseparately,andthefeaturesfromoneencoderpredicttheclusterlabelfromtheotherencoding. Here,welet
M ={Z }n beaminibatchofnimages,with
n i i=1
X ={Z ,...,Z } and Y ={1,...,l},
1 n
3Balanced Metadata Balanced Clusters Balanced Modalities
Metadata Clusters Modality 1
"The first
example
of ..."
Counts of
"the"
"A picture
of ..."
Metadata Curation Self-Supervised Clustering Contrastive Learning
Figure1: DataBalancingExamples: Eachpanelshowsadifferentcaseofthesamplespaces(X,Y)andtheinitial
measureR. ThebluehistogramsarethetargetmarginalP . Left: R(x,y)istheproportionofsubstringmatches
Y
betweenatextcaptionxandakeywordy. Center: R(x,y)istheaffinityofanimagexforclustery. Right: R(x,y)
isthesimilarityofanimagextoatextcaptiony.
wherem = nandtheelementsofY indexlearnableclusterrepresentationvectorsc ,...,c ∈ Rr. Theelements
1 l
of X pass through encoders f : Z → Rr and f : Z → Rr, colloquially known as the student and teacher
θs θt
networks,respectively.Thus,weconsidertheoverallparametervectortobeθ :=(θ ,θ ,c ,...,c ).Giventemperature
s t 1 l
hyperparametersϵ,τ >0,thereferencemeasureandtestfunctionaregivenbytheexpressions
R(0)(x,y)=
efθs(x)⊤cy/ϵ
and h (x,y)=log
efθt(x)⊤cy/τ
.
θ (cid:80)l y′=1efθs(x)⊤c y′/ϵ θ (cid:80)l y′=1efθt(x)⊤c y′/τ
Directlyoptimizing(cid:80) R(0)(x,y)h (x,y)withoutanyconstraintswouldleadtocollapse,motivatingthebalancing
x,y θ θ
ofR(0). ThetargetmarginalsP andP aregivenbythediscreteuniformmeasuresonX andY. Thisformulationis
θ X Y
oftenderivedbysolvinganoptimaltransportproblemwiththeSinkhorn-Knoppalgorithmtoassignsoftclusterlabels,
theiterativesolutionresultfromthisprocedureispreciselyR(k). Theintuitionbehindthechoiceofuniformmarginal
θ
P isthateachdatapointhasanequalamountofmasstoallottoeachcluster,whereasP capturesthatthecluster
X Y
sizesareequal. Thenumberofiterationskisselectedbasedonoptimizationconsiderations.
Example2: Image-CaptionPairs. ContrastiveLanguage-ImagePre-Training(Radfordetal.,2021),orCLIP,isan
architecturewithanimageencoderandatextencoderthatmaptoajointembeddingspace.Trainedusingimage-caption
pairs,thelosspromotesrepresentationssuchthatimagesandtextthatarepairedintheminibatchareclose,whereas
thosethatarenotpairedarefar. Thelatteraspect(promotingdissimilarityofunpairedimages/text)iswhatprevents
collapseinthisframework. ThisinterpretationoftheCLIPobjectiveasanimplicitdatabalancingprocedureisnovel,to
ourknowledge. Underthisinterpretation,theobjectiveisinfactanonlinearfunctionofP (M ),whereasitsgradient
θ n
willhavealinearformsimilarto(2). Inthiscase,eachZ =(X ,Y ),whereX isanimageandY isanassociated
i i i i i
caption. Wehavethat
X ={X ,...,X } and Y ={Y ,...,Y },
1 n 1 n
4
teS
gniniarT
hctabiniM
2
ytiladoMsothatm=l=n. Consideranimageencoderf :X (cid:55)→Rr andtextencoderf :Y (cid:55)→Rr withparametervector
θI θT
θ =(θ ,θ ). Anaturalreferencemeasurewecanuseis
I T
R
θ(0)(x,y)∝efθI(x)⊤fθT(y),
forall(x,y)∈X ×Y.
If we think that the marginals of the images and the text should be roughly uniform, we can apply the balancing
iterations(3)withthetargetmarginalsbeingtheuniformdistributionsoverX andY,respectively. Becausethereisno
preferenceforstartingtheiterationswiththeX orY dimensionfirst,wemayconsiderbothorderings. LetU(1)beone
θ
iterationofbalancingintheY dimensionandV(1). ThentheoriginalCLIPobjectiveLCLIP(θ)canberecoveredas
θ n
n (cid:34) (cid:35)
LCLIP(θ):=−1(cid:88)
log
1/n
R(0)(X ,Y )+log
1/n
R(0)(X ,Y ) −logn
n 2 R(0)(Y ) θ i i R(0) (X ) θ i i
i=1 θ,Y i θ,X i
n
=−1(cid:88)(cid:104)
logU(1)(X ,Y )+logV(1)(X ,Y
)(cid:105)
−logn, (5)
2 θ i i θ i i
i=1
whereR(0) andR(0) denotethemarginalmeasuresofR(0). Thus,wecanviewP (M )aseitherU(1)orV(1),where
θ,X θ,Y θ θ n θ θ
theobjectiveincorporatesbothbyaveraging. Thisisofteninterpretedasanaverageofcross-entropylossterms,each
representingthepredictionofonemodality’soriginalpairfromtheother. Inourformulation,LCLIP(θ)canalsobe
n
viewedasnegativelog-likelihoodunderthemeasureP (M ). ItisalsoofinteresttostudytheeffectofusingU(k)and
θ n θ
V(k)fork ≥0ingeneral,asweexploreinSec.4.
θ
InAppx.E,wetakeabroaderviewpointanddescribeanotherexampleofabalancingprocedureonanentiretraining
set(asshowninFig.1). Weexploittheseconnectionsinthenexttwosections;inSec.3wereturntothestatistical
problemoutlinedinSec.1byanalyzingbalancing-basedestimators. InSec.4weprovideempiricalexamplesofthe
balancingviewpointofmultimodalpairs.
3 Data Balancing for Variance Reduction
We now present theoretical guarantees on the mean squared error (MSE) of the data-balanced estimator ψ(k) and
n
highlightkeypointsintheproofs. NotethatthereferencemeasureR(0) ischosenastheempiricalmeasureP . For
n
readers’convenience,anotationtable(Tab.1)isinAppx.A.
Notation and Setup. Recall the setting explained in Sec. 1, in which we consider sample spaces (X,Y), along
with true and unknown joint distribution P on X ×Y with known marginals (P ,P ). For ease of presentation,
X Y
weassumethat|X|=|Y|=m,althoughtheargumentsdonotrelyonequalsupportsizes. Wemakethefollowing
assumptionthroughout,whichisalmostalwayssatisfiedbythedesiredmarginalsP andP ,suchasintheuniform
X Y
casesdiscussedinSec.2.
Assumption1. ThetargetmarginalsP (x)>0andP (y)>0forallx∈X andy ∈Y.
X Y
WedefineP(0) =P astheempiricalmeasureandfork ≥1construct
n n
P(k)(x,y):= argmin {Q:QX=PX}KL(Q∥P n(k−1))=
P
n(P
k
,X X−1)(x)P n(k−1)(x,y) kodd
, (6)
n argmin {Q:QY=PY}KL(Q∥P n(k−1))= P(P kY −1)(y)P n(k−1)(x,y) keven
n,Y
where P(k) denotes the X marginal of P(k) (with P(k) defined analogously). Note that the iterations in (6) are
n,X n n,Y
equivalent to those in (3), so the balancing iterations are exactly information projections onto sets with marginal
constraints. SeeFig.2foravisualizationofthisprocedure.
WeproveinAppx.Cthattheseiterationsarewell-definedforallkundertheevent
S :={Supp(P )=Supp(P )andSupp(P )=Supp(P )},
n,X X n,Y Y
5Figure2: DataBalancing. Alternatingapplicationof(3). ThebluesetcontainsdistributionsQwithmarginalequalto
P onX,whereasdistributionsintheorangesethavemarginalP onY.
X Y
i.e.,allrowcountsandcolumncountsarenon-empty. Givenafunctionh:X ×Y →R,theprecisequantitythatwe
analyzeis
ψ(k) :=ψ˜(k)1 +ψ˜(0)1 for ψ˜(k) =E [h(X,Y)]. (7)
n n S n Sc n (X,Y)∼Pn(k)
WeshowintheanalysisthatS occurswithhighprobability,inwhichcaseψ(k) =ψ˜(k). Inordertointroducelinear
n n
operatorsthatplayakeyroleinouranalysis,wedefineL2(P)tobethesetoffunctionsh:X ×Y →Rsatisfying
E (cid:2) h2(X,Y)(cid:3) < ∞. Let L2(P ) be the subspace of L2(P) containing functions that only depend on the first
P X
argumentx∈X anddefineL2(P )analogously.
Y
ConditionalExpectationOperatorsinL2(P). Themainresultsrelyonthespectralpropertiesoftheconditional
expectationoperatorsinducedbytheprobabilitydistributionP whichwedefinebelow. Letµ :L2(P)→L2(P )
X X
andµ :L2(P)→L2(P )bedefinedas
Y Y
[µ h](x,y):=E [h(X,Y)|X](x) and [µ h](x,y):=E [h(X,Y)|Y](y).
X P Y P
Analogoustotheinformationprojectionsin(6),theseoperatorsareorthogonalprojectionsontoL2(P )andL2(P ),
X Y
respectively,andcanberewrittenas
µ h:= argmin E (cid:2) (h(X,Y)−f(X))2(cid:3)
X P
f∈L2(PX)
µ h:= argmin E (cid:2) (h(X,Y)−g(Y))2(cid:3) .
Y P
g∈L2(PY)
Asprovenusingasingularvaluedecomposition(Prop.4)inAppx.B.1,thereexistsabasis{α }m ofL2(P ),abasis
j j=1 X
{β }m ofL2(P ),andrealvalues{s }m ,thatsatisfy
j j=1 Y j j=1
µ α =s β andµ β =s α forj ∈{1,...,m}. (8)
Y j j j X j j j
Furthermore,α =1 andβ =1 leadingtotheprojection⟨f,α ⟩ =E [f(X)]. Finally,s =1ands is
1 X 1 Y 1 L2(PX) PX 1 j
non-negativeandnon-increasinginj. Wealsoassumethefollowing.
Assumption 2. The operators µ and µ have a positive spectral gap, i.e., s < s . Note that this assumption
X Y 2 1
issatisfiedwhenP(x,y) > 0forall(x,y) ∈ X ×Y bythePerron–FrobeniusTheorem(HornandJohnson,2013,
Chapter8).
UnderAsm.2,thesingularvalues{s }m thatarestrictlylessthan1willdetermineageometricrateofdecayin
j j=2
theupcomingMSEbound,whereasthesingularfunctionsα :X →Randβ :Y →Rwilldefineausefulcoordinate
j j
systemtorepresentprojectionsofhwhenanalyzingψ(k).
n
6Main Results. Let I be the identity map, and define the debiasing operators by the orthogonal complements
C =I−µ andC =I−µ oftheconditionalexpectationoperators. Usingthese,defineC =C forkoddand
X X Y Y k X
C =C forkeven. ObservethatC istranslationinvariant,i.e.,C (h+c)=C hforanyh∈L2(P)andconstantc,
k Y k k k
soitholdsthatC h=C h¯ forh¯ =h−E [h]. Theerrorofψ(k)isdominatedbyσ2 where
k k P n k
σ2 :=Var(h)=E (cid:2) h¯2(cid:3) fork =0
0 P
σ2 :=Var(C ...C h)=E (cid:2) (C ...C h¯)2(cid:3) fork ≥1. (9)
k 1 k P 1 k
Notethattheempiricalmeanψ(0) = 1 (cid:80)n h(X ,Y )isunbiased,andsoitsMSEisequaltoσ2/n. Anotherquantity,
n n i=1 i i 0
whichessentiallymeasuresthenon-uniformityofthetargetmarginalsis
p :=min{minP (x),minP (y)}.
⋆ X Y
x y
Wehavethatp ispositivebyAsm.1WecannowstateThm.1.
⋆
Theorem1. Forasequenceofdata-balancingestimators(ψ(k)) asdefinedin(7),thereexistsanabsoluteconstant
k≥1
C >0suchthat,whenn≥C[log (2n/p )+mlog(n+1)]/p2,wehave
2 ⋆ ⋆
(cid:104) (cid:105) σ2 (cid:16) (cid:17)
E (ψ(k)−ψ)2 ≤ k +O n−3/2 . (10)
P n n
Furthermore,thereareconstantsσ2 ,C ≥0dependingonlyonhandP suchthat
gap gap
σ2−σ2 ≥σ2 −C s2k ≥0, fors <1fromAsm.2. (11)
0 k gap gap 2 2
ReturningtothequestionsposedinSec.1, Thm.1saysthattheeffectofdatabalancingusingauxiliarytarget
marginals(P ,P )isvariancereduction,whichisinterestinggiventhattheoriginalmotivationforsuchproceduresin
X Y
self-supervisedlearningisavoidingrepresentationcollapse. TheproofofThm.1isgiveninAppx.Dandfollowsby
Thm.24whichquantifiestheO(n−3/2)termintheformofanon-asymptoticbound. While(11)showsthegeometric
decayofσ2 downtoσ2−σ2 ,thelowerboundismeanttobequalitative;wefullyquantifythevariancereduction
k 0 gap
withequalityintermsoftheentirespectrums ,...,s (recallthats =1)intheupcomingProp.3,withtherelevant
2 m 1
quantitiesbeingstrictlypositiveinallnon-trivialcases. Weproceedtohighlighttheprooftechniquewhichmaybeof
independentinterest. Thebackboneoftheargumentisthattheestimationerrorψ(k)−ψcanbewrittenasafunction
n
of a quantity similar, but not equal, to ψ(k−1) −ψ. The difference will depend on a debiasing operator C or C
n X Y
dependingonwhetherkisevenorodd.
ProofTechnique:RecursiveDecomposition. Foreaseofpresentation,weassumethattheeventSistrueandfocuson
theerrorψ˜(k)−ψ. Weintroduceadditionalnotationthatiscommoninempiricalprocesstheorytoshowthedependence
n
onthefunctionh,namely
√ √
G(k)(h):= n(ψ˜(k)−ψ)= n(E [h(X,Y)]−E [h(X,Y)]). (12)
n n Pn(k) P
TheMSEofψ˜(k) isthengivenbyE [(G(k)(h))2]/n. InProp.2below,V(k−1) isalower-ordertermwhoseprecise
n P n n
definitionisgiveninAppx.D.1alongsidetheproof.
Proposition2. Let(P(k)) ,beasequencecomputedaccordingto(6). Theseiterationsarewell-definedunderthe
n k≥1
eventS,andforG(k)definedin(12),itholdsthat
n
√
G(k)(h)=G(k)(C h)+ nV(k−1)(C h). (13)
n n k n k
Usingthisresult,wecanunrolltherecursionforkstepstoget:
√
G(k)(h)=G(0)(C ···C h)+ n(cid:80)k V(ℓ−1)(C ...C h). (14)
n n 1 k ℓ=1 n ℓ k
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
first-orderterm higher-orderterm
To understand how this expression relates to the scaling with respect to n in Thm. 1, recall that the MSE is equal
to 1E [(G(k)(h))2]. Weshow(seeProp.20)thattheterm|V(ℓ−1)(C ...C )|willbeO(n−1)inexpectation. Thus,
n P n n ℓ k
takingthesquareandexpectationof(14)givestheleadingtermσ2 = E [(G(0)(C ···C h))2] = O(1)andacross
k P n 1 k
termoforderO(n−1/2). DividingtheexpressionbynrecoverstheresultofThm.1. Itremainstoshowexactlyhow
muchsmallerσ2 isthanσ2.
k 0
7CLIP Text Embeddings BERT Text Embeddings GPT-2 Text Embeddings
m=128 m=512 m=128 m=512 m=128 m=512
0.8 0.8 0.8 0.8 0.30
0.6 0.6 0.25 0.3
0.6
0.6
0.4 0.4 0.4 0.20 0.2
0.4 0.15
0.2 0.2 0.2
0.2 0.10 0.1
0.3 0.3 0.05
0.03
0.4 0.4 0.04
0.2 0.2
0.02 0.03
0.2 0.2 0.1 0.1 0.02
0.01
0.01
0.0 0.0
0.8 0.8
0.8 0.8 0.3
0.6 0.6 0.3
0.6
0.6
0.4 0.4 0.4 0.2 0.2
0.4
0.2 0.2 0.2 0.1 0.1
Iterations Iterations Iterations Iterations Iterations Iterations
No Balancing CLIP Balancing (k=1) Multi-CLIP (k=2)
Figure3: Zero-ShotPerformanceacrossVaryingTextEmbeddings,BatchSizes,andObjectives. Thethreevertical
panelsdescribedifferentchoicesofthetextencoderg whichincreasesinqualityfromlefttoright;thatis,pre-trained
θ
GPT-2,BERT,andCLIPembeddings,respectively. Withineachverticalpanel,examplesincludebatchsizesm=128
andm=512. RowsindicatevariousevaluationdatasetsfromCIFAR-10,CIFAR-100,andSTL-10. They-axisofeach
plotindicatesaverageper-classrecall,whereasthex-axisindicatestrainingiterationsatthegivenbatchsize.
QuantifyingtheReductioninVariance. Recallthesingularvaluess ,...,s andthebases{α }m and{β }m
1 m j j=1 j j=1
from(8),alongwiththeconditionalexpectationoperatorsµ andµ . Becauseµ h¯ ∈L2(P )andµ h¯ ∈L2(P ),
X Y X X Y Y
wemaydecompose
m m
µ h¯ =(cid:88) u α and µ h¯ =(cid:88) v β .
X j j Y j j
j=1 j=1
Prop.3belowrelatesthe(normalized)varianceσ2 ofthefirst-ordertermtotheoneofthesamplemeanψ(0). Infact,it
k n
showsthatthevariancereductionσ2−σ2 decaysgeometricallytothequantity
0 k
σ2
:=(cid:88)m (cid:34)
u2+
(v
j
−s ju
j)2(cid:35)
.
gap j 1−s2
j=2 j
Forsimplicity,weonlypresenttheresultforkeven,i.e.,σ2 .
2t
Proposition3. Thevariancereductionachievedbyt+1iterationsoftheC C operatorcanbequantifiedas
Y X
σ2−σ2 =σ2 −(cid:88)m s2 j(v j −s ju j)2 s4t =(cid:88)m (cid:34) u2+(1−s4t+2)(v j −s ju j)2(cid:35) .
0 2(t+1) gap 1−s2 j j j 1−s2
j=2 j j=2 j
Intuitively,theoperatorsC andC arethemainsourcesofthereductioninvarianceviaorthogonality. Since
X Y
α =1 ,wecanseethatthereductionwillalwaysbestrictlypositiveaslongasµ h¯ isnotaconstantfunction.
1 X X
4 Numerical Illustrations
Inthissection,wefurtherillustratetherelationshipbetweendatabalancingandtheexamplesmentionedinSec.2,with
aparticularfocusonthesecondexampleduetoitsunconventionalinterpretationasbalancingprocedures. Codeto
reproducethedataandexperimentscanbefoundathttps://github.com/ronakdm/balancing.
8
01-RAFIC
001-RAFIC
01-LTSU(0)(x) U(0)(y) U(1)(x) U(1)(y) U(2)(x) U(2)(y)
θ,X θ,Y θ,X θ,Y θ,X θ,Y
V(0)(x) V(0)(y) V(1)(x) V(1)(y) V(2)(x) V(2)(y)
θ,X θ,Y θ,X θ,Y θ,X θ,Y
x y x y x y
Figure4: EmpiricalMarginalsofCLIPContrastMatrix. DepictionoftheprobabilitymeasuresU(k)andV(k)as
θ θ
describedin(15)fromSec.2. Theorangebarscorrespondtothemarginalthatissettothetargetuniformdistribution
onthegiveniteration. Left: U(0)andV(0),whereneithermarginalissettouniform. Center: U(1)andV(1),which
θ θ θ θ
correspondstotheoriginalCLIPloss. Right: U(2) andV(2), whichcorrespondtotwoiterationsofthebalancing
θ θ
procedurewithintheloss. Thebluebarsareslightlynon-uniform.
Model,Datasets,andEvaluation. Throughout,weconsidertrainingvariantsofCLIPmodels(seeSec.2),which
requireadatasetofimage-captionpairs. Forthetrainingset,weusetheImageNet-Captionsdataset(Fangetal.,2013),
whichpairsimagesfromImageNet(Dengetal.,2009)thatweretakenfromFlickrwiththeiroriginalcaptions. Inthe
notationofSec.2,themodelisspecifiedbyselectinganimageencoderf andatextencoderf . Inallcases,weuse
θI θT
afixedimage/textencoderasabasevectorrepresentationandcomposeitwithatrainablefeed-forwardneuralnetwork,
i.e., f = fhead ◦fbase. We fix the base image encoder as CLIP ViT-B/32 architecture pre-trained on LAION-2B
θ θ
(Schuhmannetal.,2022),andvarythebasetextencoderacrossembeddingmodelsofvaryingquality: GPT-2(Radford
et al., 2019), BERT (Devlin et al., 2019), and CLIP-based encodings. When two CLIP encoders are used for the
baseimage/textvectorrepresentation,theyaretakenfromseparateCLIPmodels(i.e.thebaserepresentationsarenot
dependent). Weevaluatemodelsbasedonzero-shotperformanceusingthestandardCLIPinferenceprocedure: forany
imagex,alabelc∈{1,...,C}ispredictedbyassociatingtoeachcanaturallanguagecaptiony ,andpredictingthe
c
scoress(x)=(s (x),...,s (x)),with
1 C
e⟨fθI(x),fθT(yc)⟩/τ
s (x)=
c (cid:80)C c′=1e⟨fθI(x),fθT(y c′)⟩/τ
fortemperatureparameterτ. Multiplecaptioningstrategiescanbeuseddependingontheevaluationdataset,forwhich
weaveragescoresandselectthehighest-scoringlabel. WeusethepublicCLIPBenchmarkrepositoryforournumerical
illustrations,usingthedatasetsCIFAR-10,CIFAR-100,andSTL-10,alongwiththeirdefaultcaptionsets. SeeAppx.E
forspecificmodeltagsandfullexperimentaldetails.
Two-StepBalancedCLIP. WeconsideredinSec.2avariantoftheobjectiveinwhichzeroormultiplebalancing
iterationsareperformed(see(5)),viaoptimizing
n
L(k)(θ)=−1(cid:88)(cid:104)
logU(k)(X ,Y )+logV(k)(X ,Y
)(cid:105)
. (15)
n 2 θ i i θ i i
i=1
Thiscontraststhesingle-iterationvariantL(1)(θ)whichreducestotheoriginalCLIPloss. Becausetheseiterationsare
n
appliedintheobjective,backpropagationoccursthrougheachiteration. InFig.3,weplotthezero-shotperformance(in
termsofaverageper-classrecall)ofthevariantstrainedonL(0)(thenormalizedreferencemeasure),L(1)(theoriginal
n n
CLIPloss), andL(2) (thetwo-iterationCLIPloss). Wealsovarythequalityofthetextencoderf , observingan
n θT
overallaccuracytrendofGPT-2≺BERT≺CLIPacrossvariants,whichistobeexpectedgiventhebaserepresentation
qualityofeachmodel. Interestingly,thereisanimprovementofmultiplebalancingiterationsacrosschoicesofthetext
embedding,thebatchsizem,andtheevaluationdataset.
Tofurtherillustratehowtheiterativebalancingprocedureisbakedintothelosses,recallfrom(15)thattheobjectives
decomposeintotwoterms,whichdependonU(k)andV(k)whichdifferonlybasedonwhetherbalancingtofitP or
θ θ Y
tofitP isappliedfirst,respectively. Thus,foranymodelparameterizedbyθandanynumberofiterationsk,there
X
9arefourmarginaldistributionsofinterest: U(k),U(k),V(k),andV(k). Basedontheorderofiterations,wehavethat
θ,X θ,Y θ,X θ,Y
U(1) =V(2) =P ,andV(1) =U(2) =P . ThisisillustratedinFig.4. Weseethatafteronlyafewiterations,both
θ,Y θ,Y Y θ,X θ,X X
marginaldistributionsconvergetotheuniformdistribution.
5 Conclusion
Weshowedhowseveraldisparatetechniquesusedtowardsthetrainingoffoundationmodelsareinstancesofadata
balancing algorithm, which has the unsuspected benefit of reducing the variance of learning objectives involving
multiplesourcesofdata. Weprovedanewnon-asymptoticboundonthemean-squarederrorofbalancedestimatorsas
theyadjusttothegivenmarginals. Wealsohighlightthekeyrolesofconditionalexpectationoperatorsinquantifying
thatvariancereductioneffect. Futureworkincludesexploringvariantsofpriorinformationonthedataqualityboth
fromtheoreticalandfrompracticalviewpoints.
References
M.AlbertusandP.Berthet. Auxiliaryinformation: Theraking-ratioempiricalprocess. ElectronicJournalofStatistics,
13(1),2019.
Y.Asano,C.Rupprecht,andA.Vedaldi. Self-labellingviasimultaneousclusteringandrepresentationlearning. In
ICLR,2020.
R.Balestriero,M.Ibrahim,V.Sobal,A.Morcos,S.Shekhar,T.Goldstein,F.Bordes,A.Bardes,G.Mialon,Y.Tian,
A. Schwarzschild, A. G. Wilson, J. Geiping, Q. Garrido, P. Fernandez, A. Bar, H. Pirsiavash, Y. LeCun, and
M.Goldblum. ACookbookofSelf-SupervisedLearning. arXivpreprint,2023.
P.J.Bickel,C.A.Klaassen,Y.Ritov,andJ.A.Wellner. EfficientandAdaptiveEstimationforSemiparametricModels.
Springer,1edition,1998.
M.Caron,I.Misra,J.Mairal,P.Goyal,P.Bojanowski,andA.Joulin. Unsupervisedlearningofvisualfeaturesby
contrastingclusterassignments. InNeurIPS,2020.
M. Caron, H. Touvron, I. Misra, H. Jégou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging properties in self-
supervisedvisiontransformers. InICCV,2021.
X.ChenandK.He. ExploringSimpleSiameseRepresentationLearning. InCVPR,2021.
N. Courty, R. Flamary, A. Habrard, and A. Rakotomamonjy. Joint distribution optimal transportation for domain
adaptation. InNeurIPS,2017.
T.M.Cover. ElementsofInformationTheory. JohnWiley&Sons,1999.
W. E. Deming and F. F. Stephan. On a least squares adjustment of a sampled frequency table when the expected
marginaltotalsareknown. AnnalsofMathematicalStatistics,11,1940.
J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei. ImageNet: Alarge-scalehierarchicalimagedatabase. In
CVPR,2009.
J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova. BERT:Pre-trainingofdeepbidirectionaltransformersforlanguage
understanding. InACL,2019.
A.Fang, G.Ilharco, M.Wortsman, Y.Wan, V.Shankar, A.Dave, andL.Schmidt. Datadeterminesdistributional
robustnessincontrastivelanguage-imagepre-training(CLIP). InICML,2013.
S.Y.Gadre,G.Ilharco,A.Fang,J.Hayase,G.Smyrnis,T.Nguyen,R.Marten,M.Wortsman,D.Ghosh,J.Zhang,
E.Orgad,R.Entezari,G.Daras,S.M.Pratt,V.Ramanujan,Y.Bitton,K.Marathe,S.Mussmann,R.Vencu,M.Cherti,
R.Krishna,P.W.Koh,O.Saukh,A.Ratner,S.Song,H.Hajishirzi,A.Farhadi,R.Beaumont,S.Oh,A.Dimakis,
J.Jitsev,Y.Carmon,V.Shankar,andL.Schmidt. DataComp: Insearchofthenextgenerationofmultimodaldatasets.
InNeurIPS,2023.
10A.Genevay,L.Chizat,F.Bach,M.Cuturi,andG.Peyré. SampleComplexityofSinkhornDivergences. InAISTATS,
2019.
I.Gohberg,S.Goldberg,andM.Kaashoek. ClassesofLinearOperatorsVol.1. Springer,1990.
J.-B. Grill, F. Strub, F. Altché, C. Tallec, P. Richemond, E. Buchatskaya, C. Doersch, B. Avila Pires, Z. Guo,
M. Gheshlaghi Azar, B. Piot, k. kavukcuoglu, R. Munos, and M. Valko. Bootstrap your own latent: A new
approachtoself-supervisedlearning. InNeurIPS,2020.
R.A.HornandC.R.Johnson. MatrixAnalysis. CambridgeUniversityPress,2013.
C.T.IrelandandS.Kullback. ContingencyTableswithGivenMarginals. Biometrika,1968.
R.J.JohnstonandC.J.Pattie. Entropy-maximizingandtheiterativeproportionalfittingprocedure. TheProfessional
Geographer,45,1993.
C.Jones,V.Roulet,andZ.Harchaoui. Discriminativeclusteringwithrepresentationlearningwithanyratiooflabeled
tounlabeleddata. StatisticsandComputing,2022.
D.KingmaandJ.Ba. Adam: Amethodforstochasticoptimization. InICLR,2015.
M.Klatt,C.Tameling,andA.Munk. EmpiricalRegularizedOptimalTransport: StatisticalTheoryandApplications.
SIAMJournalonMathematicsofDataScience,2020.
G.MenaandJ.Niles-Weed. Statisticalboundsforentropicoptimaltransport: Samplecomplexityandthecentrallimit
theorem. InNeurIPS,2019.
M.Nutz. IntroductiontoEntropicOptimalTransport. Lecturenotes,ColumbiaUniversity,2021.
M. Oquab, T. Darcet, T. Moutakanni, H. V. Vo, M. Szafraniec, V. Khalidov, P. Fernandez, D. HAZIZA, F. Massa,
A.El-Nouby,M.Assran,N.Ballas,W.Galuba,R.Howes,P.-Y.Huang,S.-W.Li,I.Misra,M.Rabbat,V.Sharma,
G.Synnaeve,H.Xu,H.Jegou,J.Mairal,P.Labatut,A.Joulin,andP.Bojanowski. DINOv2: LearningRobustVisual
FeatureswithoutSupervision. TransactionsonMachineLearningResearch,2024.
X.Peng,Q.Bai,X.Xia,Z.Huang,K.Saenko,andB.Wang. MomentMatchingforMulti-SourceDomainAdaptation.
InICCV,2019.
G.PeyréandM.Cuturi. ComputationalOptimalTransport: WithApplicationstoDataScience. Foundationsand
TrendsinMachineLearning,11,2019.
A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,andI.Sutskever. Languagemodelsareunsupervisedmultitask
learners,2019.
A.Radford,J.W.Kim,C.Hallacy,A.Ramesh,G.Goh,S.Agarwal,G.Sastry,A.Askell,P.Mishkin,J.Clark,etal.
Learningtransferablevisualmodelsfromnaturallanguagesupervision. InICML,2021.
C.Schuhmann,R.Beaumont,R.Vencu,C.W.Gordon,R.Wightman,M.Cherti,T.Coombes,A.Katta,C.Mullis,
M.Wortsman,P.Schramowski,S.R.Kundurthy,K.Crowson,L.Schmidt,R.Kaczmarczyk,andJ.Jitsev.LAION-5B:
Anopenlarge-scaledatasetfortrainingnextgenerationimage-textmodels. InNeurIPS,2022.
J.Shen,Y.Qu,W.Zhang,andY.Yu. WassersteinDistanceGuidedRepresentationLearningforDomainAdaptation. In
AAAI,2018.
R.Sinkhorn. DiagonalEquivalencetoMatriceswithPrescribedRowandColumnSums. AmericanMathematical
Monthly,74(4),1967.
M.E.Thompson. TheoryofSampleSurveys. Chapman&Hall,2000.
H.Xu,S.Xie,X.Tan,P.-Y.Huang,R.Howes,V.Sharma,S.-W.Li,G.Ghosh,L.Zettlemoyer,andC.Feichtenhofer.
DemystifyingCLIPdata. InICLR,2024.
11Appendix
Table of Contents
A Notation 13
B LinearOperatorsandVarianceReduction 13
B.1 SingularValueDecomposition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
B.2 ProofofMainResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
C FromInformationProjectionstoDataBalancing 17
C.1 BalancingasInformationProjections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C.2 ProofofMainResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
D StatisticalAnalysisofBalancingEstimators 22
D.1 RecursionofEstimationError . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
D.2 TechnicalTools&IntermediateResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.3 AnalysisofHigher-OrderTerm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
D.4 ProofofMainResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
E ExperimentalDetails 34
E.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
E.2 ModelSpecificationandHyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
E.3 ComputeEnvironment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
E.4 AdditionalExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
12A Notation
Symbol Description
X,Y Samplespacesfortwodatasources.
Supportsizesm=|X|andl=|Y|.
m,l
Wesometimesassumem=lforeaseofpresentation
R MeasureonX ×Y,knownasthereference.
P ProbabilitymeasureonX ×Y(thedata-generatingdistribution).
n Samplesize.
(X ,Y ),...,(X ,Y ) IndependentandidenticallydistributedsamplefromP.
1 1 n n
P Empiricalmeasureof{(X,Y)}n .
n i i i=1
Q ,Q MarginalsofmeasureQonX ×Y,e.g.R ,P ,P ,etc.
X Y X Y n,X
Supp(Q) FormeasureQoverZ,thesetofvaluesz∈ZsuchthatQ(z)>0.
(P n(k))
k≥1
Sequenceofiterationsof(6).
k Iterationcountof(6).
S Theevent{Supp(P )=Supp(P )andSupp(P )=Supp(P )}.
n,X X n,Y Y
h Testfunctionh:X ×Y →Rofinterest.
ψ Theestimand(cid:80) h(x,y)P(x,y).
x,y
ψ˜ n(k) Theestimator(cid:80) x,yh(x,y)P n(k)(x,y).
ψ n(k) Theestimatorψ n(k):=ψ˜ n(k)1 S+ψ n(0)1 Sc.
√
G( nk)(h) Normalizederror n(ψ˜ n(k)−ψ).
V n(k)(h) RemainderdefinedinProp.2.
h¯ Centeredfunctionh−E [h].
P
σ2 VariancetermE (cid:2) (C ,...C h)2(cid:3) .
k P 1 k
p min{min P (x),min P (y)}.
⋆ x X y Y
L2(P) Functionsh:X ×Y →R(asX ×Yisfinite).
L2(P ),L2(P ) SubspacesofL2(P)containingfunctionsonlyofx∈X andy∈Y,respectively.
X Y
Conditionalexpectationoperators[µ h](x):=E [h(X,Y)|X](x)
X P
µ ,µ
X Y and[µ h](y):=E [h(X,Y)|Y](y).
Y P
C ,C Debiasing/centeringoperatorsC =I−µ andC =I−µ .
X Y X X Y Y
µ ,C (µ ,C )forkoddand(µ ,C )forkeven.
k k X X Y Y
{s }m SingularvaluesinProp.4.
j j=1
{α }m ,{β }m BasesforL2(P )andL2(P )inProp.4.
j j=1 j j=1 X Y
Table1: Notationusedthroughoutthepaper.
B Linear Operators and Variance Reduction
ThissectionisdedicatedtoestablishingthevariancereductionresultinProp.3byemployingpropertiesoftheMarkov
operatorsintroducedinSec.3. Inthefirstpart,weestablishProp.4,thesingularvaluedecompositionthatdefinesthe
quantitiesappearinginProp.3. Inthesecondpart,wequantifythedifferencebetweenσ2 andσ2 forevenandodd
0 k
iterationsofk.
13B.1 SingularValueDecomposition
Recalltheconditionalmeanoperatorsµ andµ fromSec.3,
X Y
[µ h](x):=E[h(X,Y)|X](x)and[µ h](y):=E[h(X,Y)|Y](y),
X Y
withthecorrespondingdebiasing(a.k.a.centering)operatorsdefinedbyC =I−µ andC =I−µ .
X X Y Y
Proposition4. Thereexistsabasis{α }m ofL2(P ),abasis{β }m ofL2(P ),andrealvalues{s }m ,which
j j=1 X j j=1 Y j j=1
satisfy:
µ α =s β andµ β =s α forj ∈{1,...,m}, (16)
Y j j j X j j j
α =1 ,β =1 ,s =1ands isnon-negativeandnon-increasinginj.
1 X 1 Y 1 j
Proof. Whenµ isrestrictedtoL2(P )andµ isrestrictedtoL2(P ),theseoperatorsareinfactadjointinL2(P),
X Y Y X
asbythetowerpropertywehavetherelation
⟨f,µ g⟩ =E[f(X)E[g(Y)|X]]=E[E[f(X)|Y]g(Y)]=⟨µ f,g⟩ .
X L2(PX) Y L2(PY)
Sinceµ :L2(P )→L2(P )isacompactlinearoperator,byGohbergetal.(1990,SectionIV.1Theorem1.1)and
Y X Y
Gohbergetal.(1990,SectionIV.1Corollary1.2),wehavethatµ admitsasingularvaluedecompositionsatisfying(16).
Y
Next,weshowthats ≤ 1andthat1 isaneigenvectorofµ µ : L2(P ) → L2(P )witheigenvalue1,which
1 X X Y X X
confirmsthats =1andα =1 bythedefinitionofsingularvalues(arguingsymmetricallyachievesβ =1 ). By
1 1 X 1 Y
thevariationalrepresentationofsingularvalues(Gohbergetal.,1990,SectionIV.1Equation(2)),wehavethat
sup ∥µ f∥ =s .
Y L2(PY) 1
f:∥f∥ L2(PX)=1
Consideranyf ∈L2(P )suchthat∥f∥ =1. DefinetheconditionalprobabilityP (x|y)=P(x,y)/P (y)
X L2(PX) X|Y Y
whichiswell-definedbyassumption. Then,bytheCauchy-SchwarzinequalityinL2(P ),
X|Y
(cid:32) (cid:33)2
∥µ f∥2 = (cid:88) (cid:88) f(x)P (x|y) P (y)
Y L2(PY) X|Y Y
y∈Y x∈X
(cid:88) (cid:88)
≤ f2(x)P (x|y)P (y)
X|Y Y
y∈Yx∈X
(cid:88) (cid:88)
= f2(x) P(x,y)
x∈X y∈Y
=∥f∥2 =1.
L2(PX)
Thisprovesthats ≤1. Forequality,noticethatµ µ 1 =µ 1 =1 ,completingtheproof.
1 X Y X X Y X
B.2 ProofofMainResults
FromProp.4,weestablishtwobases{α }m and{β }m ofL2(P )andL2(P ),respectively. Thesebasesspan
j j=1 j j=1 X Y
therangeoftheoperatorsµ andµ . WewillconsidertherepeatedapplicationoftheoperatorC C ,asequenceof
X Y Y X
twocenteringoperationsonsomefunctionh∈L2(P),andcompare
E(cid:2)
((C C
)th¯)2(cid:3) againstE(cid:2) h¯2(cid:3)
Y X
forh¯ = h−E [h]. Weestablishthemainresultbymeasuringthereductioninvariancefromasingleapplication,
P
intermsofthecoordinatesofthefunctionofinterestoneachofthetwosubspaces. Wewillthenobservehowthese
coordinateschangeiteration-to-iterationtogivethefinalresult.
14Lemma5. Foranyh∈L2(P)suchthatE [h]=0,let
P
m m
(cid:88) (cid:88)
µ h= u α andµ h= v β .
X j j Y j j
j=1 j=1
Then,wehavethat
m m
E(cid:2) (C C h)2(cid:3) =E(cid:2) h2(cid:3) −(cid:88) u2−(cid:88) (v −s u )2.
Y X j j j j
j=2 j=2
Proof. Byorthogonality,wehavethat
E(cid:2)
(C C
h)2(cid:3) =E(cid:2)
((I−µ )C
h)2(cid:3)
Y X Y X
=E(cid:2) (C h)2(cid:3) −2E[(C h)(µ C h)]+E(cid:2) (µ C h)2(cid:3)
X X Y X Y X
=E(cid:2) (C h)2(cid:3) −2P ((µ C h)2)+P ((µ C h)2)
X Y Y X Y Y X
=E(cid:2) (C h)2(cid:3) −P ((µ C h)2)
X Y Y X
=E(cid:2) h2(cid:3) −P ((µ h)2)−P ((µ C h)2).
X X Y Y X
BecauseP(h)=0,itholdsbythetowerpropertyofconditionalexpectationthatP (µ h)=0,whichimpliesthat
X X
m
(cid:88)
u =⟨µ h,α ⟩ =0 =⇒ P ((µ h)2)= u2.
1 X 1 L2(PX) X X j
j=2
Forthesecondterm,observethatP (C h)=0,soitholdsbythetowerpropertythatP (µ C h)=0,so
X X Y Y X
m
(cid:88)(cid:16) (cid:17)2
P ((µ C h)2)= ⟨µ C h,β ⟩ .
Y Y X Y X j L2(PY)
j=2
Next,wecomputetheterminthesquarebyapplyingProp.4:
⟨µ C h,β ⟩ =⟨µ h,β ⟩ −⟨µ µ h,β ⟩
Y X j L2(PY) Y j L2(PY) Y X j L2(PY)
(cid:42) m (cid:43)
(cid:88)
=v − µ u α ,β
j Y k k j
k=1 L2(PY)
(cid:42) m (cid:43)
(cid:88)
=v − u s β ,β
j k k k j
k=1 L2(PY)
=v −s u ,
j j j
whichcompletestheproof.
Lem.5ensuresthatwehavereductiononeachiteration,withaformulathatdependsonthecoordinatesofthe
functiononeachsubspace. Becausethesecoordinateschangeeveryiteration,wetracktheminthenextlemma. Define
h =h¯ andh =(C C )h ,alongwiththeconstants{u }m and{v }m givenby
0 t+1 Y X t t,j j=1 t,j j=1
m m
(cid:88) (cid:88)
µ h = u α andµ h = v β .
X t t,j j Y t t,j j
j=1 j=1
Wehavethefollowing.
Lemma6. Forallt≥0,itholdsthat
u =s2u −s v ,
t+1,j j t,j j t,j
v =0.
t+1,j
15Proof. Fixanyj ∈[m],anduseProp.4towrite
u =⟨µ C C h ,α ⟩
t+1,j X Y X t j L2(PX)
=⟨µ (I−µ −µ +µ µ )h ,α ⟩
X X Y Y X t j L2(PX)
=⟨µ µ µ h ,α ⟩ −⟨µ µ h ,α ⟩
X Y X t j L2(PX) X Y t j L2(PX)
(cid:42) m (cid:43) (cid:42) m (cid:43)
(cid:88) (cid:88)
= µ µ u α ,α − µ v β ,α
X Y t,k k j X t,k k j
k=1 L2(PX) k=1 L2(PX)
=s2u −s v ,
j t,j j t,j
whichprovesthefirstpartoftheclaim. Forthesecondpart,notethatµ C =0,so⟨µ C C h ,α ⟩ =0.
Y Y Y Y X t j L2(PY)
UsingLem.5andLem.6,wecansimplyaccumulatethereductionincurredoneveryiteration.
Proposition7. Definetheconstants(u )m and(v )m by
j j=1 j j=1
m m
µ h¯ =(cid:88) u α andµ h¯ =(cid:88) v β .
X j j Y j j
j=1 j=1
Then,wemayquantifythevariancereductionachievedbyt+1iterationsoftheC C operatoras
Y X
E(cid:2) h¯2(cid:3) −E(cid:2) ((C C )t+1h¯)2(cid:3)
=(cid:88)m (cid:40)
u2+(v −s u
)2(cid:34)
1+
s2 j(1−s4 jt)(cid:35)(cid:41)
Y X j j j j 1−s2
j=2 j
→(cid:88)m (cid:34)
u2+
(v
j
−s ju
j)2(cid:35)
j 1−s2
j=2 j
ast→∞.
Proof. ApplyLem.5(t+1)-timessothat
m t
E(cid:2) ((C C )t+1h¯)2(cid:3) =E(cid:2) h¯2(cid:3) −(cid:88)(cid:88)(cid:2) (1+s2)u2 +v2 −2s u v (cid:3)
Y X j τ,j τ,j j τ,j τ,j
j=2τ=0
m (cid:34) t (cid:35)
=E(cid:2) h¯2(cid:3) −(cid:88) v2 −2s u v +(cid:88) (1+s2)u2
0,j j 0,j 0,j j τ,j
j=2 τ=0
asbyLem.6,wehavethatv =0forτ >0. Next,weunrollthedefinitionofu sothat
τ,j τ,j
u =s2u −s v
τ,j j τ−1,j j τ−1,j
=s2(s2u −s v )−s v
j j τ−2,j j τ−2,j j τ−1,j
=s2τ−2(s2u −s v )
j j 0,j j 0,j
16forτ >0,yielding
E(cid:2) h¯2(cid:3) −E(cid:2)
((C C
)t+1h¯)2(cid:3)
Y X
m (cid:34) t (cid:35)
(cid:88) (cid:88)
= u2 +(v −s u )2+(1+s2)(s2u −s v )2 (s4)τ−1
0,j 0,j j 0,j j j 0,j j 0,j j
j=2 τ=1
m (cid:34) t−1 (cid:35)
(cid:88) (cid:88)
= u2 +(v −s u )2+(1+s2)(s2u −s v )2 (s4)τ
0,j 0,j j 0,j j j 0,j j 0,j j
j=2 τ=0
=(cid:88)m (cid:34)
u2 +(v −s u )2+
s2 j(1+s2 j)(v
0,j
−s ju 0,j)2(1−s4 jt)(cid:35)
0,j 0,j j 0,j 1−s4
j=2 j
=(cid:88)m (cid:34)
u2 +(v −s u )2+
s2 j(v
0,j
−s ju 0,j)2(1−s4 jt)(cid:35)
.
0,j 0,j j 0,j 1−s2
j=2 j
Substituteu =u andv =v tocompletetheproof.
0,j j 0,j j
Wealsopresentthecorrespondingresultfork odd. Theprooffollowssimilarlybyrepeatedapplicationofthe
operatorC C . However,theiterationswillbecomparedtoσ2 = E (cid:2) (C h¯)2(cid:3) ,asweconsiderC h¯ asthe“first”
Y X 1 P X X
iterationtothisprocess.
Proposition8. Definetheconstants(u )m by
j j=1
m
µ C h¯ =(cid:88) u β .
Y X j j
j=1
Then,wemayquantifythevariancereductionachievedbyt+1iterationsoftheC C operatoras
X Y
E(cid:2) (C h¯)2(cid:3) −E(cid:2) ((C C )t+1C h¯)2(cid:3)
=(cid:88)m (cid:40)
u2+(s u
)2(cid:34)
1+
s2 j(1−s4 jt)(cid:35)(cid:41)
X X Y X j j j 1−s2
j=2 j
→(cid:88)m (cid:32) 1+s2 j(cid:33)
u2
1−s2 j
j=2 j
ast→∞.
Inordertohavefullmonotonicity,wealsoneedthatσ2 ≥σ2. Thisfollowsbyorthogonality,as
0 1
σ2 =E(cid:2) h¯2(cid:3) =E(cid:2) (C h¯)2(cid:3) +E(cid:2) (µ h¯)2(cid:3) =σ2+E(cid:2) (µ h¯)2(cid:3) ≥σ2. (17)
0 X X 1 X 1
Thus,wecancombineProp.8and(17)tofullyquantifytherelationshipbetweenσ2andσ2 forkodd.
0 k
C From Information Projections to Data Balancing
Thissectionisdedicatedtoderivingthreerepresentationsofthebalancingprocedureasprojectionsinvariousstatistical
divergences,asshowninFig.2.
WeconsidertwosetsofprobabilitymeasuresdenotedbyΠ = {Q:Q =P }andΠ = {Q:Q =P }.
X X X Y Y Y
ThemarginalmatchingstepsarewrittenasprojectionsintermsofastatisticaldivergenceD(precisely,anf-divergence)
intheform
P P
X ⊗P(k−1) =argminD(Q∥P(k−1)), Y ⊗R=argminD(Q∥P(k−1)).
P n(k ,X−1) n Q∈ΠX n P n(k ,Y−1) Q∈ΠY n
WeprovidethederivationsforthreecommonchoicesofD: Kullback-Leibler(KL),reverseKL,andχ2. Usingthis
viewpoint,andsimplyassumingthepositivityofthemarginalmeasuresP andP ,wederiveanupperboundin
X Y
17Prop.15thatisconstantink. ThisisanimprovementovertherecentworkofAlbertusandBerthet(2019),inwhich
theyshowupperboundthatscalesexponentiallyink.
TheKLrepresentationwillbeusedintheproofofProp.15,which(recallingthesequence(P(k)) from(6)),
n k≥1
controlstheerrorbetweenP(k) andP forkoddandP(k) andP forkeven.
n,Y Y n,X X
C.1 BalancingasInformationProjections
C.1.1 ProjectioninKL-Divergence
Proposition9. AssumethatP ≪R andP ≪R ,anddefine
X X Y Y
Q⋆ :=argminKL(Q∥R), P⋆ :=argminKL(Q∥R). (18)
Q∈ΠX Q∈ΠY
Then,itholdsthat
(cid:40)
P (x)R (y|x) ifR (x)>0
Q⋆(x,y)= X Y|X X (19)
0 ifR (x)=0
X
and
(cid:40)
P (y)R (x|y) ifR (y)>0
P⋆(x,y)= Y X Y . (20)
0 ifR (y)=0
Y
Proof. InthecasethatQ(x,y)=0,weapplytheconventionthat0log0=0. ConsiderthecaseQ⋆,theprojectionof
RontoΠ . Write
X
KL(Q∥R)=
(cid:88) (cid:88) Q(x,y)logQY|X(y|x)QX(x)
RY|X(y|x)RX(x)
x∈Xy∈Y
 
= (cid:88) Q X(x)(cid:88) Q Y|X(y|x)logQ RY Y| |X X( (y y| |x x) )Q RXX (( xx )) 
x∈X y∈Y
 
= (cid:88) Q X(x)(cid:88) Q Y|X(y|x)logQ RYY || XX (( yy || xx )) +(cid:88) Q Y|X(y|x)logQ RXX (( xx )) 
x∈X y∈Y y∈Y
 
= (cid:88) Q X(x)(cid:88) Q Y|X(y|x)logQ RYY || XX (( yy || xx )) + (cid:88) Q X(x)logQ RXX (( xx ))
x∈X y∈Y x∈X
(cid:88)
= Q (x)KL(Q (·|x)∥R (·|x))+KL(Q ∥R )
X Y|X Y|X X X
x∈X
(cid:88)
= P (x)KL(Q (·|x)∥R (·|x))+KL(P ∥R ),
X Y|X Y|X X X
x∈X
wherethelastlineisduetothemarginalconstraintQ∈Π . Fortheabovetobewelldefined,weneedthatP ≪R
X X X
sothatKL(P ∥R ) < +∞. TheaboveisminimizedwhenQ (y|x) = R (y|x)forall(x,y) ∈ X ×Y such
X X Y|X Y|X
thatQ (x)=P (x)>0. ThecaseofP⋆followsanalogouslywhenusingthatP ≪R .
X X Y Y
C.1.2 ProjectioninReverseKL-Divergence
Proposition10. AssumethatP ≪R andP ≪R ,anddefine
Y X Y Y
Q⋆ :=argminKL(R∥Q), P⋆ :=argminKL(R∥Q). (21)
Q∈ΠX Q∈ΠY
18Then,itholdsthat
(cid:40)
P (x)R (y|x) ifR (x)>0
Q⋆(x,y)= X Y|X X (22)
0 ifR (x)=0
X
and
(cid:40)
P (y)R (x|y) ifR (y)>0
P⋆(x,y)= Y X Y . (23)
0 ifR (y)=0
Y
Proof. InthecasethatR(x,y)=0,weapplytheconventionthat0log0=0. NotethatminimizingKL(R∥Q)overQ
isequivalenttominimizing−(cid:80) R(x,y)logQ(x,y)(i.e.thecrossentropy). ConsiderthecaseQ⋆,theprojection
x,y
of R onto Π . Because R ≪ Q for KL(R∥Q) < +∞ to hold, we have that R(x) > 0 =⇒ Q(x) > 0, so that
X
Q (y|x)iswell-defined. Write
Y|X
(cid:88)
− R(x,y)logQ(x,y)
x,y
(cid:88) (cid:88) (cid:88)
=− R (x)logQ (x)− R(x) R (y|x)logQ (y|x)
X X Y|X Y|X
x∈X x∈X y∈Y
 
(cid:88) (cid:88) (cid:88)
=− R X(x)logP X(x)+ R X(x)− R Y|X(y|x)logQ Y|X(y|x).
x∈X x∈X y∈Y
ThesecondfirsttermdoesnotdependonQduetothemarginalconstraintQ∈Π . Thesecondtermistheexpectation
X
ofthecrossentropyfromR toQ overR ,whichisminimizedifR =Q . WehavespecifiedQ
Y|X Y|X X Y|X Y|X Y|X
andQ ,completingtheproof.
X
C.1.3 Projectioninχ2-Divergence
Let1denotethefunctionthatisidenticallyequalto1. Considerthefollowingoptimizationproblem,whichisthe
subjectofthesubsequentlemmas:
min ∥1−ξ∥2 , (24)
L2(R)
ξ∈AX
where
 
 (cid:88) 
A := f :X ×Y →Rsatisfying f(x,y)R(x,y)=P (x)foranyx∈X .
X X
 
y∈Y
Lemma11. AssumethatP ≪R ,anddefineTheproblem(24)isfeasible,anditssolutioncanbewrittenas
X X
ξ⋆ =CR(1−f)+f
X
foranyf ∈L2(R),wherethelinearoperatorCR isspecifiedby
X
(cid:88)
[CRg](x,y)=g(x,y)− g(x,y′)R (y′|x).
X Y|X
y′∈Y
Proof. First,weestablishfeasibilitybyletting
(cid:40)
P (x)/R (x) ifR (x)>0
f(x,y):= X X X .
1 otherwise
19Thisfunctiondoesnotdependonthesecondinputy. BecauseweassumedthatP ≪ R ,wehavethattheterms
X X
(cid:80)
off(x,y)forwhichR (x)=0donotaffectwhether f(x,y)R(x,y)=P (x),becauseP (x)=0inthese
X y∈Y X X
cases. Intheremainderofthisproof,wewillshowthat(24)isanaffineprojectionproblem,andfinditssolutionby
converting it to a subspace projection problem. Indeed, consider f ,...,f ∈ A , and α ,...,α ∈ R such that
1 r X 1 r
(cid:80)r
α =1. Then,
j=1 j
   
r r
(cid:88) (cid:88) (cid:88) (cid:88)
 α jf j(x,y)·R(x,y)= α j f j(x,y)R(x,y)=P X(x),
y∈Y j=1 j=1 y∈Y
indicatingthat(cid:80)r α f (x,y)∈A andA isanaffinesubsetofL2(R). Define
j=1 j j X X
 
 (cid:88) 
S := g :X ×Y →Rsatisfying g(x,y)R(x,y)=0foranyx∈X .
X
 
y∈Y
Then,foranyf ∈A ,wehavethatg ∈S ifandonlyifg+f ∈A . Takinganyf ∈A ,lettingϕ⋆bethesolution
X X X X
of
min ∥1−f −ϕ∥2 , (25)
L2(R)
ϕ∈SX
wewillhavethatϕ⋆+f willbethesolutionof(24). Theremainderoftheproofisshowingthatϕ⋆ =CR(1−f).
X
First, definetheoperatorµR by[µ g](x,y) = (cid:80) g(x,y′)R (y′|x), andnote(byfactoringoutR (x))
X X y′∈Y Y|X X
thatg ∈S ifandonlyifµRg =0. Inaddition,µRgislinearandidempotentasµRµRg =µRg,soitisaprojection
X X X X X X
operator in L2(R). Thus, S is the orthogonal complement of range(µR), and the solution of (25) is given by
X X
(I−µR)(1−f)=CR(1−f),becauseCR =I−µR. Theclaimisproved.
X X X X
Lemma12. AssumethatP ≪R . Define
X X
Q⋆ :=argminχ2(Q∥R). (26)
Q∈ΠX
andletξ⋆bethesolutionofproblem(24). Then,
(cid:40)
P (x)R (y|x) ifR (x)>0
Q⋆(x,y)=ξ⋆(x,y)R(x,y)= X Y|X X . (27)
0 ifR (x)=0
X
Proof. First,byreparametrizingtheproblem(26)asfindingξsuchthatQ(x,y)=ξ(x,y)R(x,y),wecancomputeits
solutionbysolving
min ∥1−ξ∥2 , (28)
L2(R)
ξ∈AX,ξ≥0
Noticethatwealsohaveanon-negativityconstraint, asopposedto (24). Ifξ⋆ solves(24)andhappenstobenon-
negative,thenwehavethatξ⋆solves(28)aswellandthefirstequalityof(27)issatisfiedbydefinition. Weshowthe
secondequalityof(27)bydirectcomputation,whichalsoestablishesthenon-negativityofξ⋆simultaneously.
ApplyLem.11with
(cid:40)
P (x)/R (x) ifR (x)>0
f(x,y):= X X X .
1 otherwise
sothat
ξ⋆(x,y)=CR(1−f)(x,y)+f(x,y)
X
(cid:34) (cid:35)
(cid:88)
= f(x,z)R (z|x)−f(x,y) +f(x,y)
Y|X
z∈Y
=f(x,y′)
20foranyy′ ∈Y. Thus,thelikelihoodratioofQ⋆withrespecttoRisamarginalreweighting. Accordingly,
(cid:40)
P (x)R (y|x) ifR (x)>0
Q⋆(x,y)=ξ⋆(x,y)R(x,y)= X Y|X X ,
0 ifR (x)=0
X
completingtheproof.
Proposition13. AssumethatP ≪R andP ≪R . Define
X X Y Y
Q⋆ :=argminχ2(Q∥R), P⋆ :=argminχ2(Q∥R). (29)
Q∈ΠX Q∈ΠY
Then,itholdsthat
(cid:40)
P (x)R (y|x) ifR (x)>0
Q⋆(x,y)= X Y|X X
0 ifR (x)=0
X
(cid:40)
P (y)R (x|y) ifR (y)>0
P⋆(x,y)= Y X|Y Y . (30)
0 ifR (y)=0
Y
Proof. The first equality of (30) follows by the claim of Lem. 12. The second equality follows by repeating the
argumentofLem.11andLem.12with(X,x)and(Y,y)swapped.
C.2 ProofofMainResults
Wemaynowcontroltheerrorsoftheratioofmarginalsusingtheprojectioninterpretationestablishedintheprevious
sections. RecalltheeventS asdefinedinTab.1. Thefollowingresult,themonotonicityofthemarginalviolationterms
intermsofKL,willbeusefulinthebound.
Proposition14. (Nutz,2021,Proposition6.10)UndertheeventS,itholdsthat
KL(P(0) ∥P )≥KL(P ∥P(1))≥KL(P(2) ∥P )≥...
n,X X Y n,Y n,X X
WegivethefollowingresultforX andtheanalogousclaimholdsonY.
Proposition15. AssumethatP (x)>0forallx∈X. Itholdsthat
n,X
(cid:12) (cid:12) (cid:40)
(cid:12) P (x) (cid:12) max{n−1,1} ifk =1
max(cid:12) X −1(cid:12)≤ (31)
x∈X (cid:12) (cid:12)P n(k ,X−1)(x) (cid:12) (cid:12) max{1/p2 ⋆−1,1} ifk >1.
Inaddition,wehavethat
(cid:12) (cid:12)  (cid:113)
m x∈a
Xx(cid:12)
(cid:12)
(cid:12)
(cid:12)PP
(kX
−1(x )()
x)
−1(cid:12)
(cid:12)
(cid:12)
(cid:12)≤ n
1
(cid:113)1
2
1K KL L(P (Pn,X∥ ∥P PX)
)
i if fk
k
>= 11
.
n,X p2 2 n,X X
⋆
Moreover,whenKL(P ∥P )≤p2/2,wehave
n,X X ⋆
(cid:12) (cid:12) (cid:114)
(cid:12) P (x) (cid:12) 2 1
max(cid:12) X −1(cid:12)≤ KL(P ∥P ). (32)
x∈X (cid:12) (cid:12)P(k−1)(x) (cid:12) (cid:12) p ⋆ 2 n,X X
n,X
Proof. WefirstshowthatP(k−1)(x) ≥ 1/nfork = 1andP(k−1)(x) ≥ p2 fork > 1. Inthecasethatk = 1,the
n n ⋆
resultfollowsdirectlyfromtheeventS. Fork >1suchthatkisodd,wehavethatforx∈X,
P(k−1)(x)=
(cid:88)
P(k−1)(x,y)=
(cid:88) P Y(y)
P(k−2)(x,y)
n n P(k−2)(y) n
y∈Y y∈Y n,Y
≥p (cid:88) P(k−2)(x,y)=p P(k−2)(x)=p P (x)≥p2.
⋆ n ⋆ n,X ⋆ X ⋆
y∈Y
21Theresultforkevencanbeprovensimilarly. Wenowproceedtoprovingtheinequalitiesgiveninthestatement,which
willrelyonthelowerboundabove.
Provingthefirstinequality. Then,foranyx∈X,
(cid:12) (cid:12) (cid:40) (cid:41) (cid:40)
(cid:12) P (x) (cid:12) P (x) P (x) max{n−1,1} ifk =1
(cid:12) X −1(cid:12)=max X −1,1− X ≤ ,
(cid:12) (cid:12)P n(k ,X−1)(x) (cid:12) (cid:12) P n(k ,X−1)(x) P n(k ,X−1)(x) max{1/p2 ⋆−1,1} ifk >1
whichisthedesiredresultforthefirstinequality.
Provingthesecondandthirdinequalities. Consideranoddk ≥1. Bythedefinitionoftotalvariationdistance,it
holdsthat
(cid:12) (cid:12)
max(cid:12)P (x)−P(k−1)(x)(cid:12)≤TV(P(k−1),P ).
(cid:12) X n,X (cid:12) n,X X
x∈X
(cid:113)
AccordingtoPinsker’sinequality,wehavethatTV(P(k−1),P )≤ 1KL(P(k−1)∥P ),andsowehavethat
n,X X 2 n,X X
(cid:114) (cid:114)
(cid:12) (cid:12) 1 1
max(cid:12)P (x)−P(k−1)(x)(cid:12)≤ KL(P(k−1)∥P )≤ KL(P(0) ∥P ),
x∈X (cid:12) X n,X (cid:12) 2 n,X X 2 n,X X
wherethelastinequalityfollowsbythemonotonicityofSinkhorniterationsgiveninProp.14. Weapplythelower
boundstowrite
(cid:12) (cid:12)  (cid:113)
m x∈a
Xx(cid:12)
(cid:12)
(cid:12)
(cid:12)PP
(kX
−1(x )()
x)
−1(cid:12)
(cid:12)
(cid:12)
(cid:12)≤ n
1
(cid:113)1
2
1K KL L(P (Pn,X∥ ∥P PX)
)
i if fk
k
>= 11
.
n,X p2 2 n,X X
⋆
(cid:113) (cid:12) (cid:12)
Finally,when 1KL(P ∥P )≤p /2,wehavethatmax (cid:12)P (x)−P(k−1)(x)(cid:12)≤p /2andthus
2 n,X X ⋆ x∈X (cid:12) X n,X (cid:12) ⋆
(cid:12) (cid:12) p
minP(k−1)(x)≥minP (x)−max(cid:12)P(k−1)(x)−P (x)(cid:12)≥ ⋆.
x∈X n,X x∈X X x∈X (cid:12) n,X X (cid:12) 2
Hence,
(cid:12) (cid:12)
max(cid:12) (cid:12)
(cid:12)
P X(x) −1(cid:12) (cid:12)
(cid:12)≤
max x∈X (cid:12) (cid:12)P n(k ,X−1)(x)−P X(x)(cid:12) (cid:12)
≤
2 (cid:114) 1
KL(P ∥P ).
x∈X (cid:12) (cid:12)P n(k ,X−1)(x) (cid:12) (cid:12) min x∈X P n(k ,X−1)(x) p ⋆ 2 n,X X
Now,forkeven,setk =2tfort≥0. Wehavethat
(cid:114)
(cid:12) (cid:12) 1
max(cid:12)P(2t−1)(y)−P (y)(cid:12)≤TV(P(2t−1),P )≤ KL(P ∥P(2t−1)).
y∈Y (cid:12) n,Y Y (cid:12) n,Y Y 2 Y n,Y
InvokeProp.14onceagaintoachieve
(cid:114) (cid:114)
1 1
KL(P ∥P(2t−1))≤ KL(P ∥P ),
2 Y n,Y 2 n,X X
whichcompletestheproof.
D Statistical Analysis of Balancing Estimators
Thissectioncontainstheproofofthemainresult,namelyThm.1. Wefirstintroducesomeadditionalnotationandthen
giveabroadoutlineoftheproofforreadability. LettheexpectationofafunctionhunderaprobabilitymeasureQon
X ×Y bydenotedby
(cid:88)
Q(h)= h(x,y)Q(x,y)
x∈X,y∈Y
22sothat
ψ(k) =P(k)(h), ψ =P(h),
n n
and
√ √
G(k)(h)= n[P(k)−P](h)= n(P(k)(h)−P(h)). (33)
n n n
RecallinginadditionthatC =C forkoddandC =C forkeven. Finally,theevent
k X k Y
S :={Supp(P )=Supp(P )andSupp(P )=Supp(P )}, (34)
n,X X n,Y Y
isusedasaconditioninmanyresults.
ProofOutline. Wefirstestablishthattherecursionformula
[P(k)−P](h)=[P(k−1)−P](C h)+V(k−1)(C h)
n n k n k
holdsinProp.2. Applyingthisresultrepeatedlytothebalancedsequence(P(k)) andunrollingtherecursion,we
n k≥1
seethatwhenkisodd,
[P(k)−P](h)=[P(k−1)−P](C h)+V(k−1)(C h)
n n X n X
=[P(k−2)−P](C C h)+V(k−2)(C C h)+V(k−1)(C h)
n Y X n Y X n X
=[P(0)−P](C ...C h)+(cid:80)k V(ℓ−1)(C ...C h) (35)
n 1 k ℓ=1 n ℓ k
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
first-orderterm higher-orderterm
Additionally,leth := C ...C h,sothatthefirst-ordertermcanbewrittenasP(0)(h )−P(h )higher-order
ℓ,k ℓ k n 1,k 1,k
termcanalsobewrittenas(cid:80)k V(ℓ−1)(h ). Becauseouroriginalgoalistoupperboundthemeansquarederror,
ℓ=1 n ℓ,k
weusetheexpansionabovetowrite
(cid:12) (cid:12)2 (cid:12) (cid:12)2
E(cid:12)P(k)(h)−P(h)(cid:12) ≤E(cid:12)P(0)(h )−P(h )(cid:12)
(cid:12) n (cid:12) (cid:12) n 1,k 1,k (cid:12)
(cid:12) (cid:12)(cid:12) (cid:12) (cid:12) (cid:12)2
+2E(cid:12)P(0)(h )−P(h )(cid:12)(cid:12)(cid:80)k V(ℓ−1)(h )(cid:12)+E(cid:12)(cid:80)k V(ℓ−1)(h )(cid:12)
(cid:12) n 1,k 1,k (cid:12)(cid:12) ℓ=1 n ℓ,k (cid:12) (cid:12) ℓ=1 n ℓ,k (cid:12)
(cid:12) (cid:12)2
Regarding the first term, we have that E(cid:12)P(0)(h )−P(h )(cid:12) = σ2/n, which is the dominant term in Thm. 1.
(cid:12) n 1,k 1,k (cid:12) k
Thus,theremainingchallengeoftheproofwillbetoupperboundthecrosstermandothersquaredtermandshow
its dependence on n. The dominant term of these two will be the cross term, as we will essentially show that
|P(0)(h )−P(h )|isO(n−1/2)withhighprobability,andthat|(cid:80)k V(ℓ−1)(h )|isinfactO(n−1)withhigh
n 1,k 1,k ℓ=1 n ℓ,k
probability. AsstatedinSec.3,akeyintermediateresultincontrollingthehigher-ordertermisProp.15,whoseproofis
giveninAppx.C.Theremainingsubsectionswalkthroughthesestepsindetail.
D.1 RecursionofEstimationError
Wefirstrecallthatthesequence(P(k)) canbecomputedwiththefollowingformula:
n k≥1
P(0)(x,y):=P (x,y)andP(k)(x,y):=  P n(P k ,X X−1)(x)P n(k−1)(x,y) kodd . (36)
n n n  P(P kY −1)(y)P n(k−1)(x,y) keven
n,Y
Prop.2establishestheconditionsunderwhichthesestepsarewell-defined(i.e.P(k−1)(x)>0andP(k−1)(y)>0).
n,X n,Y
Let
 (cid:18) (cid:19)
V
n(k−1)(h)=(cid:80)
x,y (cid:18)P
n(P
k
,X X−1)(x)−1 (cid:19)h(x,y)P n(k−1)(x,y) kodd
(37)
(cid:80)
x,y
P(P kY −1)(y)−1 h(x,y)P n(k−1)(x,y) keven.
n,Y
23Proposition2. Let(P(k)) ,beasequencecomputedaccordingto(6). Theseiterationsarewell-definedunderthe
n k≥1
eventS,andforG(k)definedin(12),itholdsthat
n
√
G(k)(h)=G(k)(C h)+ nV(k−1)(C h). (13)
n n k n k
Proof. First,assumethatP(k−1)(x) > 0andP(k−1)(y) > 0forallx ∈ X andy ∈ Y sothatwemayestablishthe
n,X n,Y
recursion,whichwewillshowbyinductiontowardtheendoftheproof.
Considerthefollowingstepsinthecasethatkisodd:
P(k)(h)
n
=(cid:88) h(x,y)P(k)(x,y)=(cid:88) h(x,y) P X (x)P(k−1)(x,y) by(36)forkodd
n P(k−1) n
x,y x,y n,X
(cid:34) (cid:35)
=(cid:88) 1·h(x,y)P(k−1)(x,y)+(cid:88) P X (x)−1 ·h(x,y)P(k−1)(x,y)
n P(k−1) n
x,y x,y n,X
=P(k−1)(h)+V(k−1)(h),
n n
wherewesubstituted
 (cid:18) (cid:19)
V
n(k−1)(h)=(cid:80)
x,y (cid:18)P
n(P
k
,X X−1)(x)−1 (cid:19)h(x,y)P n(k−1)(x,y) kodd
. (38)
(cid:80)
x,y
P(P kY −1)(y)−1 h(x,y)P n(k−1)(x,y) keven
n,Y
ArguinganalogouslyforkevenandsubtractingP(h)onbothsides,wehavethat
[P(k)−P](h)=[P(k−1)−P](h)+V(k−1)(h). (39)
n n n
Wecanthenestablishthefollowing“centered”recursionusingthefollowingdecompositioninthecaseofkodd.
[P(k)−P](h)
n
=[P(k)−P](C h)+[P(k)−P](µ h) h=C h+µ h
n X n X X X
=[P(k−1)−P](C h)+V(k−1)(C h)+[P(k)−P](µ h) apply(39)toC h
n X n X n X X
=[P(k−1)−P](C h)+V(k−1)(C h). P(k)(µ h)=P(µ h)
n X n X n X X
The last line follows because µ h is only a function on X, and due to the definition of the marginal rebalancing
X
iterations,P(k) =P . Thisgivesthedesiredformulabysubstituting(33).
n,X X
Weproceedtoshowthattheiterationsarewell-defined.WewillinfactshowthatP(k−1)(x)>0andP(k−1)(y)>0
n,X n,Y
forallx∈X andy ∈Y. Fork =1,P(0) (x)=P (x)>0andP(0)(y)=P (y)>0forallx∈X andy ∈Y
n,X n,X n,Y n,Y
thisholdsundertheeventS byassumption. Wearguebyinductionthatthisholdsforallk >1. Assumethattheclaim
istruefor{1,...,k−1},andthatkiseven. Then,
P(k−1)(x)=P (x)>0,
n,X X
P(k−1)(y)= (cid:88) P(k−1)(x,y)= (cid:88) P X (x)P(k−2)(x,y)
n,Y n P(k−2) n
x∈X x∈X n,X
P
≥min X (x)·P(k−2)(y)>0
x∈X P(k−2) n,Y
n,X
as P(k−2)(x) > 0 and P(k−2)(y) > 0 by the inductive hypothesis. Arguing analogously for k odd achieves the
n,X n,Y
claim.
24D.2 TechnicalTools&IntermediateResults
Havingestablishedthebackboneoftheargument,wecollectinthissubsectionsomeusefultoolsthatareusedinthe
remainderoftheproofs.
Thefollowingresultfollowsfromthemethodoftypesininformationtheory,andwillbehelpfulinderivingthe
dependenceofthehigher-ordertermonn.
Theorem16. (Cover,1999, Theorem11.2.1)Letµbeadiscreteprobabilitymeasuresupportedonmatoms. Let
i.i.d.
U ,...,U ∼ µandµ betheassociatedempiricalmeasure. Then,wehaveforanyϵ>0that
1 n n
P(KL(µ n∥µ)≥ϵ)≤2−n(ϵ−mlog( nn+1)) .
Wethenprovidearesultthatcountsthenumberoftermsthatappearwhenrepeatedlycenteringviatheoperators
C ,...,C . Thisformalizesthepattern
1 k
C =I−µ
X X
C C =I−µ −µ +µ µ
Y X X Y Y X
C C C =I−µ −µ +µ µ +µ µ −µ µ µ ,
X Y X X Y Y X X Y X Y X
andsoon. Thiswillbeusefulwhenboundingh uniformly.
ℓ,k
Lemma17. Foranyk ≥1andℓ∈{1,...,k},
(k−ℓ−1)/2 (k−ℓ−1)/2
(cid:88) (cid:88)
C ...C =I− (µ µ )τµ − (µ µ )τµ
ℓ k X Y X Y X Y
τ=0 τ=0
(k−ℓ)/2 (k−ℓ)/2
(cid:88) (cid:88)
+ (µ µ )τ + (µ µ )τ +(−1)k−ℓ+1µ ...µ ,
X Y Y X ℓ k
τ=1 τ=1
wherethesum(cid:80)j
is0wheni>j
andis(cid:80)⌊j⌋
whenj isnotanintegerbyconvention.
τ=i τ=i
Proof. Weprovetheclaimbybackwardinductiononℓ,forthecasethatkisodd. Inthecaseℓ=k,theclaimholds
becauseC =I−µ . Next,foranyℓ<k,assumethatthestatedresultholdsfor{ℓ+1,...,k}. Then,ifℓisalsoodd
k k
(sothatµ =µ ),
ℓ X
C ...C =C C ...C
ℓ k ℓ ℓ+1 k
(k−ℓ−2)/2 (k−ℓ−2)/2
(cid:88) (cid:88)
=I− (µ µ )τµ − (µ µ )τµ
X Y X Y X Y
τ=0 τ=0
(k−ℓ−1)/2 (k−ℓ−1)/2
(cid:88) (cid:88)
+ (µ µ )τ + (µ µ )τ +µ ... µ
X Y Y X Y X
(cid:124)(cid:123)(cid:122)(cid:125)
τ=1 τ=1 k−ℓterms
(k−ℓ−2)/2 (k−ℓ−2)/2
(cid:88) (cid:88)
−µ + (µ µ )τµ + µ (µ µ )τµ
X X Y X X Y X Y
τ=0 τ=0
(k−ℓ−1)/2 (k−ℓ−1)/2
(cid:88) (cid:88)
− (µ µ )τ − µ (µ µ )τ −(µ µ )(k−ℓ)/2µ
X Y X Y X X Y X
τ=1 τ=1
Theredtermsandbluetermscancelouttozero. Thisleaves
(k−ℓ−2)/2 (k−ℓ−2)/2
(cid:88) (cid:88)
C ...C =I− (µ µ )τµ − (µ µ )τµ
ℓ k X Y X Y X Y
τ=0 τ=0
(k−ℓ−1)/2
(cid:88)
+ (µ µ )τ +(µ µ )(k−ℓ)/2
Y X Y X
τ=1
(k−ℓ−2)/2
(cid:88)
+ µ (µ µ )τµ +(−1)k−ℓ+1µ ...µ
X Y X Y ℓ k
τ=0
25whereinwecombinetheredtermsandre-indexthebluetermstoget
(k−ℓ−2)/2 (k−ℓ−2)/2
(cid:88) (cid:88)
C ...C =I− (µ µ )τµ − (µ µ )τµ
ℓ k X Y X Y X Y
τ=0 τ=0
(k−ℓ)/2 (k−ℓ)/2
(cid:88) (cid:88)
+ (µ µ )τ + (µ µ )τ +(−1)k−ℓ+1µ ...µ .
Y X X Y ℓ k
τ=1 τ=1
Finally,becausek−ℓisevenwhenkisoddandℓisodd,wecansettheupperboundofthefirsttwosumsto(k−ℓ−1)/2
withoutchangingthenumberofterms. Thisprovesthedesiredresult. Theresultcanbeprovedsimilarlywhenℓiseven.
Asaresult,wehaveprovedtheclaimforanyoddkandℓ≤k. Similarargumentscanbeusedforthecaseofkeven
andℓ≤k.
D.3 AnalysisofHigher-OrderTerm
Returningtotheoutlineatthestartofthissection,wemaynowboundthehigher-orderremaindertermin(35),namely
k k
(cid:88) (cid:88)
V(ℓ−1)(h )= V(ℓ−1)(C ...C h),
n ℓ,k n ℓ k
ℓ=1 ℓ=1
dependsoncontrollingthequantityV(k−1)inthesummation,whichwerecallforconvenience:
n
 (cid:18) (cid:19)
V
n(k−1)(h)=(cid:80)
x,y (cid:18)P
n(P
k
,X X−1)(x)−1 (cid:19)h(x,y)P n(k−1)(x,y) kodd
. (40)
(cid:80)
x,y
P(P kY −1)(y)−1 h(x,y)P n(k−1)(x,y) keven
n,Y
BecausewehaveestablisheduniformcontroloverthefunctionsP /P(k−1)−1andP /P(k−1)−1,viaProp.15in
X n,X Y n,Y
Appx.CwecannowboundthefullremainderinProp.20.
Wealsomakeuseofthefollowingintermediateresult,whichcontrolshowlargetheℓ -normofthefunctionhcan
∞
growaftercentering.
Lemma18. ∥h ∥ ≤2(k−ℓ+1)∥h∥ .
ℓ,k ∞ ∞
Proof. ApplyLem.17andthetriangleinequality,sothatweonlyneedtocountthenumberoftermsthatappearinthe
sums,adding2forthefirstandlasttermintheexpression. Wesubtract1fromthetotal,asoneofeither(k−ℓ)/2or
(k−ℓ+1)/2willbeafraction. Thisyields2(k−ℓ+1)termstotal,thedesiredresult.
WeupperboundthesuminProp.20. Todoso,weintroducesomenotation. ConsiderB andB definedby
1 2
 (cid:12) (cid:12)
max x∈X (cid:12) (cid:12) (cid:12)P(P ℓX −1( )x ()
x)
−1(cid:12) (cid:12)
(cid:12)
ℓodd
B 1 :=M 1 and B 2 := max M ℓ for M ℓ := (cid:12) n,X (cid:12)
2≤ℓ≤k max y∈Y(cid:12) (cid:12) (cid:12)P(P ℓY −1(y )()
y)
−1(cid:12) (cid:12)
(cid:12)
ℓeven
n,Y
fork ≥1. WealsoenumeratethesamplespacesasX ={x ,...,x }andY ={y ,...,y },anddefinethefunction
1 m 1 m
(cid:40)
1{x=x } kodd
1 (x,y):= j .
jk 1{y =y } keven
j
Thisisanindicatorfunctiononthej-thelementofeitherX orY dependingonwhetherkisoddoreven. Finally,for
anyfunctionh,use(undertheeventS)recalltheempiricalprocessnotation
√ (cid:16) (cid:17)
G(k)(h):= n P(k)(h)−P(h) . (41)
n n
Usingthisnotation, wecanrewritetherecursionintermsofthequantityG(k)(h)itself. Thisisestablishedinthe
n
followinglemma.
26Lemma19. Forkodd,itholdsthat
m (cid:34) (cid:35)
G(k)(h)=G(k−1)(C
h)+(cid:88) P X(x j)
−1 G(k−1)(C h1 ),
n n X P(k−1)(x ) n X jk
j=1 n,X j
whereasforkeven,itholdsthat
m (cid:34) (cid:35)
G(k)(h)=G(k−1)(C
h)+(cid:88) P Y(y j)
−1 G(k−1)(C h1 ),
n n Y P(k−1)(y ) n Y jk
j=1 n,Y j
Proof. Wegivetheproofforkodd. BythesecondclaimofProp.2andbythedefinitionofG(k)(h),weneedonly
n
showthatP(C h1 )=0. Indeed,
X jk
(cid:40)
E[C h|X](x ) ifx=x
E[(C h1 |X](x)= X j j .
X jk
0 ifx̸=x
j
ButE[C h|X](x )=0bydefinitionofC . TakinganexpectationoverP givesthatP(h 1 )=0,whichimplies
X j X X ℓ,k jk
thedesiredresult. Theproofforkevenfollowssymmetrically.
Thehigher-ordertermin(35),canbeboundedusingProp.20.
Proposition20. Foranyk ≥1,thefollowingholdsundertheeventS:
√ (cid:12) (cid:12)(cid:88)k (cid:12) (cid:12) (cid:88)m (cid:32) (cid:12) (cid:12) (cid:88)k (cid:12) (cid:12)(cid:33)
n(cid:12) V(ℓ−1)(C ...C h)(cid:12)≤ B (cid:12)G(0)(h 1 )(cid:12)+B (cid:12)G(0)(h 1 )(cid:12)
(cid:12) n ℓ k (cid:12) 1(cid:12) n 1,k jℓ (cid:12) 2 (cid:12) n ℓ,k jℓ (cid:12)
(cid:12) (cid:12)
ℓ=1 j=1 ℓ=2
√
+mB ∥h∥ nk(k−1)[B +B (k+1)/3].
2 ∞ 1 2
Proof. First, foranyℓ ∈ {1,...,k}, recallthenotationh := C ...C h. BythesecondclaimofProp.2andby
ℓ,k ℓ k
Lem.19,wehavethatforℓodd,
√
nV(ℓ−1)(h
)=(cid:88)m (cid:34) P
X (x
)−1(cid:35)
G(ℓ−1)(h 1 ). (42)
n ℓ,k P(ℓ−1) j n ℓ,k jℓ
j=1 n,X
Usingthestatementabove,wehavethat
√ (cid:12) (cid:12) (cid:88)m (cid:12) (cid:12)
n(cid:12)V(ℓ−1)(h )(cid:12)≤M (cid:12)G(ℓ−1)(h 1 )(cid:12).
(cid:12) n ℓ,k (cid:12) ℓ (cid:12) n ℓ,k jℓ (cid:12)
j=1
Theboundaboveholdsforℓevenaswell. Then,usingthefirstclaimProp.2againalongwiththetriangleinequality,
wehavethatforℓ≥2,
[P(ℓ−1)−P](h 1 )=[P(ℓ−2)−P](h 1 )+V(ℓ−2)(h 1 )
n ℓ,k jℓ n ℓ,k jℓ n ℓ,k jℓ
whichimpliesthat
(cid:12) (cid:12) (cid:12) (cid:12) √ (cid:12) (cid:12)
(cid:12)G(ℓ−1)(h 1 )(cid:12)≤(cid:12)G(ℓ−2)(h 1 )(cid:12)+ n(cid:12)V(ℓ−2)(h 1 )(cid:12)
(cid:12) n ℓ,k jℓ (cid:12) (cid:12) n ℓ,k jℓ (cid:12) (cid:12) n ℓ,k jℓ (cid:12)
(cid:12) (cid:12) √ (cid:12) (cid:12) √ (cid:12) (cid:12)
≤(cid:12)G(0)(h 1 )(cid:12)+ n(cid:12)V(0)(h 1 )(cid:12)+...+ n(cid:12)V(ℓ−2)(h 1 )(cid:12)
(cid:12) n ℓ,k jℓ (cid:12) (cid:12) n ℓ,k jℓ (cid:12) (cid:12) n ℓ,k jℓ (cid:12)
(cid:12) (cid:12) √ √
≤(cid:12)G(0)(h 1 )(cid:12)+M nP(0)(|h |1 )+...+M nP(ℓ−2)(|h |1 )
(cid:12) n ℓ,k jℓ (cid:12) 1 n ℓ,k jℓ ℓ n ℓ,k jℓ
(cid:12) (cid:12) √
≤(cid:12)G(0)(h 1 )(cid:12)+2∥h∥ n[B +B (ℓ−1)](k−ℓ+1), (43)
(cid:12) n ℓ,k jℓ (cid:12) ∞ 1 2
27byLem.18andM ≤B andM ≤B forℓ≥2. Summingthesebounds,wehavethat
1 1 ℓ 2
√ (cid:88)k (cid:12) (cid:12)
n (cid:12)V(ℓ−1)(h )(cid:12)
(cid:12) n ℓ,k (cid:12)
ℓ=1
(cid:88)m (cid:12) (cid:12) (cid:88)k (cid:88)m (cid:12) (cid:12)
≤M (cid:12)G(0)(h 1 )(cid:12)+ M (cid:12)G(ℓ−1)(h 1 )(cid:12)
1 (cid:12) n 1,k jℓ (cid:12) ℓ (cid:12) n ℓ,k jℓ (cid:12)
j=1 ℓ=2 j=1
(cid:88)m (cid:12) (cid:12) (cid:88)k (cid:88)m (cid:12) (cid:12)
≤B (cid:12)G(0)(h 1 )(cid:12)+B (cid:12)G(ℓ−1)(h 1 )(cid:12)
1 (cid:12) n 1,k jℓ (cid:12) 2 (cid:12) n ℓ,k jℓ (cid:12)
j=1 ℓ=2j=1
(cid:88)m (cid:12) (cid:12)
≤B (cid:12)G(0)(h 1 )(cid:12) +
1 (cid:12) n 1,k jℓ (cid:12)
j=1
(cid:88)k (cid:88)m (cid:16)(cid:12) (cid:12) √ (cid:17)
B (cid:12)G(0)(h 1 )(cid:12)+2∥h∥ n[B +B (ℓ−1)](k−ℓ+1) apply(43)
2 (cid:12) n ℓ,k jℓ (cid:12) ∞ 1 2
ℓ=2j=1
(cid:88)m (cid:32) (cid:12) (cid:12) (cid:88)k (cid:12) (cid:12)(cid:33)
= B (cid:12)G(0)(h 1 )(cid:12)+B (cid:12)G(0)(h 1 )(cid:12) +
1(cid:12) n 1,k jℓ (cid:12) 2 (cid:12) n ℓ,k jℓ (cid:12)
j=1 ℓ=2
√ (cid:88)k
2mB ∥h∥ n [B +B (ℓ−1)](k−ℓ+1),
2 ∞ 1 2
ℓ=2
because|X|=m. Wesumthelastterm:
k k−1 k−1
(cid:88) (cid:88) (cid:88)
[B +B (ℓ−1)](k−ℓ+1)=B (k−ℓ)+B ℓ(k−ℓ)
1 2 1 2
ℓ=2 ℓ=1 ℓ=1
k(k−1)
= [B +B (k+1)/3].
2 1 2
completingtheproof.
D.4 ProofofMainResults
Wecannowshowthemainresultofthissection: theboundonthemeansquarederroroftherebalancedestimator.
Recalltheevent
S :={Supp(P )=Supp(P )andSupp(P )=Supp(P )} (44)
n,X X n,Y Y
asintroducedin(34). Toremindthereaderofthehigh-levelstepsoftheproof,wemaydecomposetheerroronthe
eventS weusedtheestimator
ψ˜(k) :=ψ(k)1 +ψ(0)1
n n S n Sc
sowedecomposeontheeventS towrite
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:104) (cid:105) (cid:20)(cid:16) (cid:17)2 (cid:21)
E P˜(k)(h)−P(h) =E (P (h)−P(h))21 +E P(k)(h)−P(h) 1 . (45)
P n P n Sc P n S
Then,weusetheupcomingProp.21toboundthefirstterm,whichwillinturnrequireshowingthatS occurswithhigh
probability. Asforthesecondterm,wewillapplyProp.2andthederivation(35)towrite
E (cid:20)(cid:16) P(k)(h)−P(h)(cid:17)2 1 (cid:21) =E (cid:2) T21 (cid:3) +2E [T T 1 ]+E (cid:2) T21 (cid:3) (46)
P n S P 1 S P 1 2 S P 2 S
28for
k
(cid:88)
T :=[P(0)−P](C ...C h)andT := V(ℓ−1)(C ...C h). (47)
1 n 1 k 2 n ℓ k
ℓ=1
Bydefinition,wehavethatE (cid:2) T21 (cid:3) ≤E (cid:2) T2(cid:3) =σ2/n. ItthenremainstoboundthecrosstermE [T T 1 ]and
P 1 S P 1 k P 1 2 S
squaredtermE (cid:2) T21 (cid:3) . ThisisaccomplishedbyLem.23andLem.22,respectively.
P 2 S
Proposition21. ItholdsthatP(Sc)≤2m(1−p )n. Moreover,foranyδ ∈(0,1),wehave
⋆
(cid:104) (cid:105) 2log(2/δ)
E (P (h)−P(h))21 ≤4∥h∥2 min{2m(1−p )n,δ}+ ∥h∥2 2m(1−p )n.
P n Sc ∞ ⋆ n ∞ ⋆
Proof. DefineF :={Supp(P )̸=Supp(P )}andF :={Supp(P )̸=Supp(P )},sothatSc =F ∪F .
X n,X X Y n,Y Y X Y
WefirstcontroltheprobabilityofF . LetF :={P (x )=0}forj ∈[m]. WethenobtainF =∪m F ,which
X j n,X j X j=1 j
impliesbytheunionboundthat
m m
(cid:88) (cid:88)
P(F )≤ P(F )= (1−P (x ))n ≤m(1−p )n.
X j X j ⋆
j=1 j=1
Similarly,wehavethatP(F )≤m(1−p )nandthusP(Sc)≤2m(1−p )n,whichgivesthefirstclaim.
Y ⋆ ⋆
Tocontroltheexpectation,consideranyδ >0,anddefinetheevent
(cid:40) (cid:114) (cid:41)
(cid:12) (cid:12) 2log(2/δ)
E := (cid:12)P(0)(h)−P(h)(cid:12)≤ ∥h∥ .
δ (cid:12) n (cid:12) n ∞
ByHoeffding’sinequality,itholdsthatP(E )≥1−δ. Furthermore,weget
δ
E[1 (P(0)(h)−P(h))2]=E[1 1 (P(0)(h)−P(h))2]+E[1 1 (P(0)(h)−P(h))2]
Sc n Sc E δc n Sc Eδ n
2log(2/δ)
≤4∥h∥2 E[1 1 ]+ ∥h∥2 E[1 1 ]
∞ Sc E δc n ∞ Sc Eδ
2log(2/δ)
≤4∥h∥2 min{P(Sc),P(Ec)}+ ∥h∥2 P(Sc)
∞ δ n ∞
2log(2/δ)
≤4∥h∥2 min{2m(1−p )n,δ}+ ∥h∥2 2m(1−p )n.
∞ ⋆ n ∞ ⋆
Inordertoboundthetermsappearingin(46),weintroducetheeventsEδ,Eδ,andEδ,definedby
1 2 3
(cid:26) (cid:27)
1 2 log(n+1)
Eδ := max{KL(P ∥P ),KL(P ∥P )}≤ log +m
1 n,X X n,Y Y n 2 δ n
(cid:110)(cid:12) (cid:12) (cid:112) (cid:111)
Fδ := (cid:12)G(0)(h 1 )(cid:12)≤ 2log(2mk/δ)2(k−ℓ+1)∥h∥ , ℓ=1,...,k, j =1,...,m
ℓ (cid:12) n ℓ,k jℓ (cid:12) ∞
k
(cid:92)
Eδ := Fδ
2 ℓ
ℓ=1
(cid:110)(cid:12) (cid:12) (cid:112) (cid:111)
Eδ := (cid:12)G(0)(h )(cid:12)≤ 2log(2/δ)2k∥h∥ .
3 (cid:12) n 1,k (cid:12) ∞
TheeventsareconstructedsuchthatP(Eδ)≥1−δ,P(Eδ)≥1−δ,andP(Eδ)≥1−δ,asweusedintheupcoming
1 2 3
proofsofLem.23,Lem.22,andThm.24.
Lemma22(Squaredtermbound). LetT bedefinedasin(47). Foranyδ > 0,assumingthatn ≥ 2[log (2/δ)+
2 2
mlog(n+1)]/p2,wehavethat
⋆
E (cid:2) T21 (cid:3) ≤
2∥h∥2 ∞m2k2
[log (2/δ)+mlog(n+1)]2−1{k=1} ×
P 2 S p2 2
⋆
 (cid:18) k−1(cid:18) k+1(cid:19)(cid:19)2
8
(cid:32)(cid:114)
2mk
(k−1)(k+4)(cid:33)2
 4n+ n+2+ δ+ 2log (k+1)+ .
p2 p2 n2 δ p2
⋆ ⋆ ⋆
29Proof. ThefollowingcomputationsaredoneundertheeventS. First,applyProp.20towrite
1 (cid:88)m (cid:32) (cid:12) (cid:12) (cid:88)k (cid:12) (cid:12)(cid:33)
|T |≤ √ B (cid:12)G(0)(h 1 )(cid:12)+B (cid:12)G(0)(h 1 )(cid:12) +
2 n 1(cid:12) n 1,k jℓ (cid:12) 2 (cid:12) n ℓ,k jℓ (cid:12)
j=1 ℓ=2
mB ∥h∥ k(k−1)[B +B (k+1)/3]. (48)
2 ∞ 1 2
WedecomposeontheeventEδ∩Eδ. NotethatbyThm.16,wehavethatP(Eδ)≥1−δ. ItfollowsfromHoeffding’s
1 2 1
inequality,theunionbound,andboundednessof∥h 1 ∥byLem.18thatP(Eδ)≥1−δAsaresult,P(Eδ∩Eδ)≥
ℓ,k jℓ 2 1 2
1−2δ.
Bound|T |undertheeventS\(Eδ∩Eδ). Inthiscase,weapply(31)fromProp.15togetB ≤nandB ≤1/p2,
2 1 2 1 2 ⋆
alongwiththeuniversalboundsfromLem.18:
1 (cid:12) (cid:12)
√ (cid:12)G(0)(h 1 )(cid:12)≤2∥h ∥ ≤4k∥h∥
n(cid:12) n 1,k jℓ (cid:12) 1,k ∞ ∞
1 (cid:88)k (cid:12) (cid:12) (cid:88)k (cid:88)k
√ (cid:12)G(0)(h 1 )(cid:12)≤2 ∥h ∥ ≤ 4(k−ℓ+1)∥h∥ =2k(k−1)∥h∥
n (cid:12) n ℓ,k jℓ (cid:12) ℓ,k ∞ ∞ ∞
ℓ=2 ℓ=2 ℓ=2
sothatbyplugginginto(48),
(cid:20) (cid:18) (cid:19)(cid:21)
k−1 k+1
|T |≤∥h∥ mk 4n+ n+2+ ,
2 ∞ p2 3p2
⋆ ⋆
andinturn,
(cid:104) (cid:105) (cid:20) k−1(cid:18) k+1(cid:19)(cid:21)2
E T21 ≤2∥h∥2 m2k2 4n+ n+2+ δ. (49)
P 2 S\(E 1δ∩E 2δ) ∞ p2 3p2
⋆ ⋆
Bound |T | under the event S ∩Eδ ∩Eδ. In this case, we may use that n ≥ 2[log (2/δ)+mlog(n+1)]/p2
2 1 2 2 ⋆
apply(32)fromProp.15toget
(cid:114)
2 1 1 (cid:112)
max{B ,B }≤ KL(P ∥P )≤ √ 2log (2/δ)+2mlog(n+1)
1 2 p 2 n,X X p n 2
⋆ ⋆
andtheboundsbasedonEδ whichgive
2
(cid:114)
(cid:12) (cid:12) 2mk
(cid:12)G(0)(h 1 )(cid:12)≤ 2log 2k∥h∥
(cid:12) n 1,k jℓ (cid:12) δ ∞
(cid:88)k (cid:12) (cid:12) (cid:88)k (cid:114) 2mk (cid:114) 2mk
(cid:12)G(0)(h 1 )(cid:12)≤ 2log 2(k−ℓ+1)∥h∥ ≤ 2log k(k−1)∥h∥ ,
(cid:12) n ℓ,k jℓ (cid:12) δ ∞ δ ∞
ℓ=2 ℓ=2
Byplugginginto(48),
(cid:112)
2m∥h∥ 2log(2mk/δ)[2log (2/δ)+2mlog(n+1)]
|T |≤ ∞ 2 k(k+1)+
2 np
⋆
m∥h∥ [2log (2/δ)+2mlog(n+1)]
∞ 2 k(k−1)(k+4)
3np2
⋆
4mk∥h∥ [log
(2/δ)+2mlog(n+1)]1−1{k=1}/2
≤ ∞ 2 ×
np2
⋆
(cid:104) (cid:112) (cid:105)
p 2log(2mk/δ)(k+1)+(k−1)(k+4) .
⋆
30Inturn,
(cid:104) (cid:105) 16∥h∥2 m2k2[log (2/δ)+mlog(n+1)]2−1{k=1}
E T21 ≤ ∞ 2 ×
P 2 S\(E 1δ∩E 2δ) n2p4
⋆
(cid:104) (cid:112) (cid:105)2
p 2log(2mk/δ)(k+1)+(k−1)(k+4) . (50)
⋆
Combiningtogetherboth(50)and(49)andusingthat[log (2/δ)+2mlog(n+1)]≥1,wehavethat
2
E (cid:2) T21 (cid:3) ≤
2∥h∥2 ∞m2k2
[log (2/δ)+mlog(n+1)]2−1{k=1} ×
P 2 S p2 2
⋆
(cid:34)(cid:18) k−1(cid:18) k+1(cid:19)(cid:19)2
8
(cid:18)
(cid:112)
(k−1)(k+4)(cid:19)2(cid:35)
4n+ n+2+ δ+ 2log(2mk/δ)(k+1)+ ,
p2 p2 n2 p2
⋆ ⋆ ⋆
theresultasdesired.
Lemma23(Crosstermbound). LetT andT bedefinedasin(47). Foranyδ >0,assumingthatn≥2[log (2/δ)+
1 2 2
mlog(n+1)]/p2,wehavethat
⋆
E [T T 1 ]
P 1 2 S
2mk2∥h∥2 (cid:112) 2log(2/δ)[log (2/δ)+2mlog(n+1)]1−1{k=1}/2
≤ ∞ 2 ×
p2
⋆
(cid:34) (cid:112) (cid:18) (cid:18) (cid:19)(cid:19) (cid:35)
p 2log(2mk/δ)(k+1)+(k−1)(k+4) k+1
⋆ +6 4np2+(k−1) n+2+ δ ,
n3/2 ⋆ p2
⋆
Proof. ThefollowingcomputationsaredoneundertheeventS. First,applyProp.20towrite
1 (cid:12) (cid:12)(cid:34) 1 (cid:88)m (cid:32) (cid:12) (cid:12) (cid:88)k (cid:12) (cid:12)(cid:33)
|T T |≤ √ (cid:12)G(0)(h )(cid:12) √ B (cid:12)G(0)(h 1 )(cid:12)+B (cid:12)G(0)(h 1 )(cid:12) +
1 2 n(cid:12) n 1,k (cid:12) n 1(cid:12) n 1,k jℓ (cid:12) 2 (cid:12) n ℓ,k jℓ (cid:12)
j=1 ℓ=2
(cid:35)
mB ∥h∥ k(k−1)[B +B (k+1)/3] . (51)
2 ∞ 1 2
WedecomposeontheeventEδ∩Eδ∩Eδ. NotethatbyThm.16andthatn≥log (2/δ)+mlog(n+1),wehavethat
1 2 3 2
P(Eδ)≥1−δ. ItfollowsbyHoeffding’sinequalityandtheunionboundthatP(Eδ)≥1−δ. Similarly,wealsohave
1 2
byHoeffding’sinequalitythatP(Eδ)≥1−δ. Asaresult,P(Eδ∩Eδ∩Eδ)≥1−3δ.
3 1 2 3
Bound |T | under the event S\(Eδ ∩Eδ ∩Eδ). In this case, we apply (31) from Prop. 15 to get B ≤ n and
2 1 2 3 1
B ≤1/p2,alongwiththeuniversalboundsfromLem.18:
2 ⋆
1 (cid:12) (cid:12)
√ (cid:12)G(0)(h )(cid:12)≤2∥h ∥ ≤4k∥h∥
n(cid:12) n 1,k (cid:12) 1,k ∞ ∞
1 (cid:12) (cid:12)
√ (cid:12)G(0)(h 1 )(cid:12)≤2∥h ∥ ≤4k∥h∥
n(cid:12) n 1,k jℓ (cid:12) 1,k ∞ ∞
1 (cid:88)k (cid:12) (cid:12) (cid:88)k (cid:88)k
√ (cid:12)G(0)(h 1 )(cid:12)≤2 ∥h ∥ ≤ 4(k−ℓ+1)∥h∥ =2k(k−1)∥h∥ ,
n (cid:12) n ℓ,k jℓ (cid:12) ℓ,k ∞ ∞ ∞
ℓ=2 ℓ=2 ℓ=2
sothatbyplugginginto(51),
(cid:20) (cid:18) (cid:19)(cid:21)
k−1 k+1
|T T |≤4k2∥h∥2 m 4n+ n+2+ ,
1 2 ∞ p2 3p2
⋆ ⋆
andinturn,
(cid:104) (cid:105) 12k2∥h∥2 m(cid:20) (cid:18) k+1(cid:19)(cid:21)
E T T 1 ≤ ∞ 4np2+(k−1) n+2+ δ. (52)
P 1 2 S\(E 1δ∩E 2δ∩E 3δ) p2 ⋆ 3p2
⋆ ⋆
31Bound|T T |undertheeventS∩Eδ∩Eδ∩Eδ. Inthiscase,wemayusethatn≥2[log (2/δ)+mlog(n+1)]/p2
1 2 1 2 3 2 ⋆
apply(32)fromProp.15toget
(cid:114)
2 1 1 1 (cid:112)
max{B ,B }≤ KL(P ∥P )≤ √ 2log (2/δ)+2mlog(n+1)
1 2 p 2 n,X X np 2
⋆ ⋆
andtheboundsbasedonEδ∩Eδ∩Eδ whichgive
2 2 3
(cid:12) (cid:12) (cid:112)
(cid:12)G(0)(h )(cid:12)≤ 2log(2/δ)2k∥h∥
(cid:12) n 1,k (cid:12) ∞
(cid:12) (cid:12) (cid:112)
(cid:12)G(0)(h 1 )(cid:12)≤ 2log(2mk/δ)2k∥h∥
(cid:12) n 1,k jℓ (cid:12) ∞
(cid:88)k (cid:12) (cid:12) (cid:88)k (cid:114) 2mk (cid:114) 2mk
(cid:12)G(0)(h 1 )(cid:12)≤ 2log 2(k−ℓ+1)∥h∥ ≤ 2log k(k−1)∥h∥ ,
(cid:12) n ℓ,k jℓ (cid:12) δ ∞ δ ∞
ℓ=2 ℓ=2
Byplugginginto(51),
(cid:112)
m∥h∥ 2log(2mk/δ)[2log (2/δ)+2mlog(n+1)]
|T |≤ ∞ 2 k(k+1)+
2 np
⋆
m∥h∥ [2log (2/δ)+2mlog(n+1)]
∞ 2 k(k−1)(k+4)
3np2
⋆
mk∥h∥ [log
(2/δ)+2mlog(n+1)]1−1{k=1}/2
≤ ∞ 2 ×
np2
⋆
(cid:104) (cid:112) (cid:105)
p 2log(2mk/δ)(k+1)+(k−1)(k+4)
⋆
2mk2∥h∥2 (cid:112) 2log(2/δ)[log (2/δ)+2mlog(n+1)]1−1{k=1}/2
|T T |≤ ∞ 2 ×
1 2 n3/2p2
⋆
(cid:104) (cid:112) (cid:105)
p 2log(2mk/δ)(k+1)+(k−1)(k+4) ,
⋆
Inturn,
(cid:104) (cid:105) 2mk2∥h∥2 (cid:112) 2log(2/δ)[log (2/δ)+2mlog(n+1)]1−1{k=1}/2
E T21 ≤ ∞ 2 ×
P 2 S\(E 1δ∩E 2δ∩E 3δ) n3/2p2
⋆
(cid:104) (cid:112) (cid:105)
p 2log(2mk/δ)(k+1)+(k−1)(k+4) , (53)
⋆
Combiningtogetherboth(53)and(52)andusingthat[log (2/δ)+2mlog(n+1)]≥1,wehavethat
2
E [T T 1 ]
P 1 2 S
2mk2∥h∥2 (cid:112) 2log(2/δ)[log (2/δ)+2mlog(n+1)]1−1{k=1}/2
≤ ∞ 2 ×
p2
⋆
(cid:34) (cid:112) (cid:18) (cid:18) (cid:19)(cid:19) (cid:35)
p 2log(2mk/δ)(k+1)+(k−1)(k+4) k+1
⋆ +6 4np2+(k−1) n+2+ δ ,
n3/2 ⋆ p2
⋆
theresultasdesired.
WenowcombinethepreviousresultstoproveThm.24.
Theorem24. Forasequenceofrebalanceddistributions(P(k)) ,thereexistsanabsoluteconstantC >0suchthat
k≥1
whenn≥C[log (2n/p )+mlog(n+1)]/p2,
2 ⋆ ⋆
σ2 CB
E [(P˜(k)(h)−P(h))2]≤ k + , (54)
P n n n3/2
where
(cid:112) log(2n/p )m2k4∥h∥2 (cid:18) 2n (cid:19)2−1{k} (cid:18) 2mkn (k−1)2(cid:19)
B = ⋆ ∞ log +mlog(n+1) log + .
p2 2 p p p2
⋆ ⋆ ⋆ ⋆
32Proof. Weapplythedecomposition(45),andsubsequentlyhandlethesecondtermusingboundsonthetermsin(46).
Setδ =p4/n4. WeapplyLem.22andLem.23withthischoiceofδ,sothatthereexistsanabsoluteconstantsC˜,C ,
⋆ 1
andC suchthat
2
E [T T 1 ]≤C ∥h∥2 ∞m2k3(cid:112) log(2n/p ⋆) [log (2n/p )+mlog(n+1)]1−1{k=1}/2 ×
P 1 2 S 1 n3/2p2 2 ⋆
⋆
(cid:18) (cid:19)
2mnk k−1
log +
p p2
⋆ ⋆
E (cid:2) T21 (cid:3) ≤C
∥h∥2 ∞m2k4
[log (2n/p )+mlog(n+1)]2−1{k=1} ×
P 2 S 2 n2p2 2 ⋆
⋆
(cid:18)
2mnk
(k−1)2(cid:19)
log + ,
p p2
⋆ ⋆
whenn≥C˜[log (2n/p )+mlog(n+1)]/p2. ThisthenimpliesthatthereisanabsoluteconstantC suchthat
2 ⋆ ⋆ 3
(cid:20)(cid:16) (cid:17)2(cid:21)
E P˜(k)(h)−P(h)
P n
(cid:20)(cid:16) (cid:17)2 (cid:21) σ2
≤E P(0)(h)−P(h) 1 + k +
P n Sc n
C ∥h∥2 m2k4(cid:112) log(2n/p )(cid:20) 2n (cid:21)2−1{k=1}(cid:18) 2mnk (k−1)2(cid:19)
3 ∞ ⋆ log +mlog(n+1) log + .
n3/2p2 2 p p p2
⋆ ⋆ ⋆ ⋆
Next, we apply Prop. 21 with the same choice of δ. Because 2[log (2/δ) = mlog(n + 1)] ≥ log(m/δ) and
2
−log(1−p )≥p ≥p2,wehavethatn≥log(δ/m)/log(1−p ),whichimpliesthatm(1−p )n ≤δ. Combining
⋆ ⋆ ⋆ ⋆ ⋆
withthedisplayabove,wehavethatthereexistsanabsoluteconstantC >0suchthat
(cid:20)(cid:16) (cid:17)2(cid:21) σ2 C∥h∥2 m2k4(cid:112) log(2n/p )
E P˜(k)(h)−P(h) ≤ k + ∞ ⋆
P n n n3/2p2
⋆
×[log
(2/δ)+mlog(n+1)]2−1{k=1}(cid:18)
log2mnk
+
(k−1)2(cid:19)
,
2 p p2
⋆ ⋆
whichistheclaimedresult.
Whilenotshowninthemaintext,similartechniquestothoseusedabovecanalsocontrolthebiasofP˜(k)(h)asin
n
Thm.25. Interestingly,thisbiasisoforderO(n−2)whichconfirmstheintuitionthateventhoughtP˜(k)(h)maybe
n
biased,thedominanttermisthevariance.
Theorem25. Forasequenceofrebalanceddistributions(P(k)) ,thereexistsanabsoluteconstantC >0suchthat
k≥1
whenn≥C[log (2n/p )+mlog(n+1)]/p2,
2 ⋆ ⋆
(cid:12) (cid:12)2 CB
(cid:12)E [P˜(k)(h)−P(h)](cid:12) ≤ , (55)
(cid:12) P n (cid:12) n2
whereBisasdefinedinThm.24.
Proof. First,applythedecomposition(45)sothat
(cid:12) (cid:104) (cid:105)(cid:12) (cid:12) (cid:104)(cid:16) (cid:17) (cid:105)(cid:12)
(cid:12)E P˜(k)(h)−P(h) (cid:12)≤|E [(P (h)−P(h))1 ]|+(cid:12)E P(k)(h)−P(h) 1 (cid:12).
(cid:12) P n (cid:12) P n Sc (cid:12) P n S (cid:12)
ByusingtheargumentofProp.21,wehavethat
(cid:114)
2log(2/δ)
|E [P (h)−P(h)]1 |≤2∥h∥ min{2m(1−p )n,δ}+ ∥h∥ 2m(1−p )n.
P n Sc ∞ ⋆ n ∞ ⋆
33Then,bytherecursionformulaEquation(35),wehavethat
√ (cid:12) (cid:104)(cid:16) (cid:17) (cid:105)(cid:12)
n(cid:12)E P(k)(h)−P(h) 1 (cid:12)
(cid:12) P n S (cid:12)
(cid:12) (cid:104) (cid:105)(cid:12) (cid:12) (cid:12) (cid:34) √ (cid:88)k (cid:35)(cid:12) (cid:12)
=(cid:12)E G(k)(h)1 (cid:12)=(cid:12)E (1−1 )G(0)(C ...C h)+ n1 V(ℓ−1)(C ...C h) (cid:12).
(cid:12) P n S (cid:12) (cid:12) P Sc n 1 k S n ℓ k (cid:12)
(cid:12) (cid:12)
ℓ=1
BecauseG(0)(C ...C h)haszeromean,itfollowsthat
n 1 k
√ (cid:12) (cid:104)(cid:16) (cid:17) (cid:105)(cid:12) (cid:12) (cid:104) (cid:105)(cid:12) √
n(cid:12)E P(k)(h)−P(h) 1 (cid:12)≤(cid:12)E 1 G(0)(C ...C h) (cid:12)+ n|E [1 T ]|
(cid:12) P n S (cid:12) (cid:12) P Sc n 1 k (cid:12) P S 2
√
WehavebyHoeffding’sinequalitythatP(Eδ) ≥ 1−δ, andthatbyLem.18thatG(0)(C ...C h) ≤ 4k n∥h∥
3 n 1 k ∞
universally. Asaresult,applyingProp.21onceagain,
(cid:12) (cid:104) (cid:105)(cid:12)
(cid:12)E 1 G(0)(C ...C h) (cid:12)
(cid:12) P Sc n 1 k (cid:12)
(cid:12) (cid:104) (cid:105)(cid:12) (cid:12) (cid:104) (cid:105)(cid:12)
≤(cid:12)E 1 1 G(0)(C ...C h) (cid:12)+(cid:12)E 1 1 G(0)(C ...C h) (cid:12)
(cid:12) P Sc Eδ n 1 k (cid:12) (cid:12) P Sc Eδ n 1 k (cid:12)
3 3
√ (cid:112)
≤4k n∥h∥ min{2m(1−p )n,δ}+ 2log(2/δ)2k∥h∥ 2m(1−p )n.
∞ ⋆ ∞ ⋆
UsingasimilarargumenttoLem.22,wehavethatunderS\(Eδ∩Eδ)(whichoccurswithprobabilitynomorethan2δ),
1 2
(cid:20) (cid:18) (cid:19)(cid:21)
k−1 k+1
|T |≤∥h∥ mk 4n+ n+2+ ,
2 ∞ p2 3p2
⋆ ⋆
andthatunderS∩Eδ∩Eδ (whichoccurswithprobabilityatleast1−2δ),
1 2
4mk∥h∥ [log
(2/δ)+2mlog(n+1)]1−1{k=1}/2
|T |≤ ∞ 2
2 np2
⋆
(cid:104) (cid:112) (cid:105)
p 2log(2mk/δ)(k+1)+(k−1)(k+4) .
⋆
Applyingthedecomposition|E P [1 ST 2]|≤(cid:12) (cid:12) (cid:12)E P (cid:104) 1 S\(E 1δ∩E 2δ)T 2(cid:105)(cid:12) (cid:12) (cid:12)+(cid:12) (cid:12) (cid:12)E P (cid:104) 1 S∩E 1δ∩E 2δT 2(cid:105)(cid:12) (cid:12) (cid:12)andsettingδ = np2 ⋆ 2 achieves
thedesiredresult.
E Experimental Details
E.1 Datasets
Pre-TrainingData. Thepre-trainingdatawastakenfromthepublicImageNet-Captionsdataset(Fangetal.,2013).
Wesubsetthedatasetbyselectingthe250classesthatweremostfrequentinthedataset,resultingin174,594images
andassociatedFlickrcaptions. Theexactimagesusedandtheirassociatedcaptionsaregiveninthecodesupplement.
EvaluationData. Weperformzero-shotevaluation(asdescribedinSec.4)withimageclassificationdatasets. Weused
thedefaultclasscaptionsfromtheCLIPBenchmarkrepo. Thedatasets(andassociatedtasks)usedwere:
• CIFAR-10: 32-by-32colorimages,labeledwithoneof10classes. Thetestsethas10,000images.
• CIFAR-100: 32-by-32colorimages,labeledwithoneof100classes. Thetestsethas10,000images.
• STL-10: 96-by-96colorimages,labelledwithoneof10classes. Thetestsethas80,000images.
Evaluationscriptsusingthevariousembeddingsmodels(describedbelow)areprovided.
34E.2 ModelSpecificationandHyperparameters
ArchitectureandImplementation. ThemodelsconsideredCLIPmodels(Radfordetal.,2021),andarespecifiedby
pairsofencoders(f ,g ),representingimagesandtext,respectively. Theencodersdecomposeintof =fhead◦fbase
θ θ θ θ θ
(similarlyforg )wherefbasedenotesabaseimageencoderandfheaddenotesatrainableheadmodel. Theheadmodels
θ θ θ
arefeed-forwardnetworkswithtwohiddenlayers,256hiddenunits,and128-dimensionaloutputrepresentations. Their
inputdimensionsmaybe512or768,dependingonwhetheraCLIPmodelorBERT/GPT-2modelisusedasthebase.
Fortheimagebase/foundationmodels,weusetheopen-sourceOpenCLIPimplementationoftheViT-B/32modelwith
thelaion2b_s34b_b79kmodeltag. Forthetextencoder,weusetheencoderofthevariantoftheViT-B/32with
tagdatacomp_xl_s13b_b90k. FortheothertextencoderstheHuggingfaceimplementationsofGPT-2andBERT
wereused.
Optimizer. Foroptimization,modelsweretrainedwithstochasticgradientdescent(SGD)withthelearningratetuned
alongthegrid(cid:8) 1−3,3−3,1−2,3−2,1−1(cid:9)
andafixedweightdecayparameterof0.01. Momentum-variantssuchas
Adam(KingmaandBa,2015)werenotusedtoisolatetheeffectvaryinglossesasdescribedinSec.4.
E.3 ComputeEnvironment
ExperimentswererunonaCPU/GPUworkstation12virtualcores,126Gofmemory,andfourNVIDIATITANXp
GPUswith12Gmemoryeach. ThecodewaswritteninPython3andweusePyTorchforautomaticdifferentiation.
TheOpenCLIPandCLIPBenchmarkreposwereusedforzero-shotevaluation.
E.4 AdditionalExperiments
Here,wedetailadditionalexperimentsinmetadatacuration,asathirdexampleofbalancingcontinuedfromSec.2.
Metadata Curation: Background. Beyond Sec.2, we take abroader perspectivethan batcheswithin stochastic
trainingmethods,andconsiderbalancingapproachesappliedtoanentiretrainingset. Asdescribedinrecentstudies
onlargemodelpre-training,carefuldatasetdesigncanbetheprimaryfactorthatdifferentiatesperformancebetween
modelsatthebillion-parameterscale(Fangetal.,2013;Xuetal.,2024;Gadreetal.,2023). Oneapproachthatis
usedinboththeoriginalCLIPdatasetdesign(Radfordetal.,2021)andanopen-sourcereplication(Xuetal.,2024)
ismetadatacuration,whereinanimage-captiondatasetM issynthesizedusingalistofmetadata(e.g.keywords)
n
{y ,...,y }sothat
1 l
X ={Z ,...,Z }, Y ={y ,...,y }.
1 n 1 l
ThekeywordsareusedtosearchforcaptionswithinX ofimage-captionpairsviasubstringmatching. Thissearch
generatesaninitialdatasetofN matches(X ,Y ),...,(X ,Y ). NotethatN isthenumberofmatchesbetween
1 1 N N
captionsandkeywords,andnotthenumberofdatapoints;agivencaptionmayberepeatedmultipletimesforevery
keyworditmatches. Furthermore,amongtheN matches,thedistributionofobservedkeywordsisinitiallylong-tailed
(seeXuetal.(2024,Figure2)). Forexample,thekeyword“the”ismatchedinalmosteverycaption,whereasless
commonkeywordsmayhavenomatchesatall.InbothRadfordetal.(2021)andXuetal.(2024),thedataareresampled
sothatthisdistributionofkeywordsovermatchesisuniform,i.e.keywordswithmanymatcheshavetheirassociated
captions downsampled during the dataset creation process. While the probability measure may not be computed
explicitly(duetoscale),thisadjustmentofthekeyworddistributioncanbeviewedasasingleiterationofrebalancing(3)
applied to the Y marginal. Indeed, the reference measure R(0) (not dependent on any model parameters) can be
computedbylettingR(0)(x,y)denotethenumberofsubstringmatchesbetweencaptionxandkeywordydividedby
N. WeelucidatethisconnectionbyapplyingdirectbalancingonasmallerscaleexampleoftheImageNet-Captions
datasetinSec.4,observingthedesiredbalancingeffectonthekeywords.
MetadataCuration: Experiments. Wealsoinstantiatetheexampleaboveexperimentally. ThetargetmarginalP
Y
isselectedasinXuetal.(2024),bychoosingathresholdforwhichhighprobabilitykeywordshavetheirprobability
masstruncated,andtheprobabilitymeasureisnormalizedtosumtoone. WeshowtheobservedmarginalP andthe
n,Y
targetmarginalP sortedinincreasingorderinFig.5(left). TheoriginalmarginalonY hasapproximately5orders
Y
35m=128 m=512 m=128 m=512
0.4 0.4 0.8 0.8
0.3
0.6 0.6
0.2
0.2
0.4 0.4
0.1
2000 4000 2000 4000 2000 4000 2000 4000
Iterations Iterations Iterations Iterations
CLIP MetaCLIP
Observed Marginal over Keywords Balanced Marginal over Keywords
−2
10
−3
10
−4
10
−5
10
−6
10
−7
10
Figure5: MetaCLIPbalancing. DepictionofMetaCLIPbalancing(Example2inSec.2)onImageNet-Captions
dataset,inwhichX representscaptions,andY representskeywords. Left: ObservedmarginalP (top)andP
n,Y Y
(bottom),whicharesortedbyorderofincreasingprobability. Right: Zero-shotevaluationofanembeddingmodel
trainedusingthestandardCLIPlossoriginalversusthebalancedtrainingset.
ofmagnitudeofdifferencebetweenthemostandleastprobablekeyword. Afterbalancing,thetargetmarginalhas
lessthan2ordersofdifference. Toseehowthisaffectsdownstreamperformance,weplotthezero-shotaccuracyover
trainingiterationsinFig.5(right),whenusingtheoriginaldataset(CLIP)andusingthemetadatabalanceddataset
(MetaCLIP).Weobservemoderateimprovementespeciallyinthesmallbatchregime(m=128)whencuratingthe
dataset.
36
001-RAFIC
01-LTS