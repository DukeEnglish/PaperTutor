DATA-DRIVEN EFFECTIVE MODELING OF MULTISCALE
STOCHASTIC DYNAMICAL SYSTEMS
YUAN CHEN AND DONGBIN XIU∗
Abstract. We present a numerical method for learning the dynamics of slow components of
unknown multiscale stochastic dynamical systems. While the governing equations of the systems
are unknown, bursts of observation data of the slow variables are available. By utilizing the ob-
servation data, our proposed method is capable of constructing a generative stochastic model that
can accurately capture the effective dynamics of the slow variables in distribution. We present a
comprehensivesetofnumericalexamplestodemonstratetheperformanceoftheproposedmethod.
Key words. Effectivemodel,Multiscalestochasticdynamicalsystem,Generativemodel
MSC codes. 60H10,60H35,62M45,65C30
1. Introduction. Manymathematicalmodelsareestablishedbasedoncomplex
dynamics with multiple time scales. Understanding and simulating such systems are
challenging due to the separation of the scales. One common approach is to estab-
lish effective equations, also known as averaged equations, for the slow processes by
averaging out the fast variables with respect to their invariant measure, cf. [23,34].
Another popular way resorts to identifying and modeling certain low-dimensional
quantities to represent the dynamics, for example, invariant manifolds [18,40], tran-
sition pathways [20], reaction coordinates [12], etc. More recently, efforts have been
made to directly model the observables using data driven approaches. See, for exam-
ple, [3,17,21,44,47].
The focus of this paper is on numerical modeling of the effective dynamics of
unknown slow-fast multiscale stochastic dynamical systems. More specifically, we
assume that observation data of the slow variables are available, while the governing
equations for the systems may not. The goal is to construct a numerical model that
can accurately capture the slow dynamics, which in turn enables system analysis and
prediction of the stochastic dynamics. The proposed method falls into a broader
area of discovery of dynamics from observational data, which has attracted growing
attention in recent years. Most of the methods focus on deterministic systems, cf.
[2,11,24,30,37,38,39]. For stochastic systems, the presence of noises poses new
challenges to the design of effective data-driven methods. Several methods have been
developedbyusingGaussianprocesses([1,13,29,45]),ordeepneuralnetworks(DNNs)
[6,7,15,43,46], etc. More recently, a framework of stochastic flow map learning ( [8])
wasproposed. Byusinggenerativemachinelearningmodels,theapproachwasshown
to be highly effective in modeling unknown stochastic dynamical systems.
The framework of sFML is an extension of FML (flow map learning, cf. [37]) for
modeling deterministic systems. The FML approach utilizes data to learn the flow
map between state variables recorded on consecutive time steps. Originally devel-
oped to learn unknown autonomous dynamical systems, it was extended to learning
nonautonomous dynamical systems [35], parametric systems [36], PDEs [10,41], as
well as systems when only a subset of state variables are observed [22]. For unknown
stochasticsystems,thesFMLapproachincorporatesagenerativemodelinthelearned
flow map operator, such that the operator becomes stochastic and a weak approxi-
∗E-mailaddresses: {chen.11050, xiu.16}@osu.edu. DepartmentofMathematics,TheOhioState
University, Columbus, OH 43210, USA. Funding: This work was partially supported by AFOSR
FA9550-22-1-0011.
1
4202
guA
72
]GL.sc[
1v12841.8042:viXra2 YUANCHENANDDONGBINXIU
mation (in distribution) to the true stochastic flow map. Different generative models
canbeincorporated,forexample,GenerativeAdversarialNetworks(GANs)[6],auto-
encoders [42], and normalizing flows [9].
In this work, we adopt the sFML approach for learning the dynamics of the
learning slow variables in multiscale stochastic dynamical systems. Once success-
fully constructed, the sFML model generates stochastic trajectories of the slow state
variables for arbitrarily given initial conditions. The learned sFML model is a weak
approximation (in distribution) to the true and unknown slow dynamics. Conse-
quently, ourmodelservesasaneffectivemodelfortheslowdynamics, constructedby
data of the slow variables but not the fast variables. Unlike [22,25] for deterministic
dynamicalsystemswithmissingvariables, oursFMLmodeldoesnotrequirememory
terms. This owes to the distinctive feature of multiscale stochastic dynamics — fast
variables are quickly incorporated into (”slaved to”) the dynamics of the slow vari-
ables, in the form of statistics, which independently contributes to the distribution of
slow variables. The reduced system formed by only the slow variables thus becomes
approximately Markovain. The impact of the missing dynamics of the fast variable is
diminishinglysmallandpresentsitself,atbest,asnoises. Forthegenerativemodelof
the sFML model, we utilize conditional normalizing flow (cf. [31]) model to approxi-
mate the stochasticity of the slow system. Other kind of generative models can also
be considered in the sFML model, as shown the [8,9,42].
2. Setup. Let Ω be an event space and T be a finite time horizon, we consider
a d-dimensional stochastic system uε :Ω [0,T] Rd. The state variables u can be
separatedintoslowvariablesx Rℓ andf× astvari(cid:55)→ abley Rd−ℓ, andaredrivenbyan
∈ ∈
unknown stochastic differential equations:
dxε =f(xε,yε)+σ(xε,yε)dW1,
t t t t t t
(2.1) 1 1
dyε = g(xε,yε)+ β(xε,yε)dW2,
 t ε t t √ε t t t
where f and σ, g andβ are drift and diffusion functions, W ti are (independent) m i-
dimensional Brownian motions, m +m 1. The parameter 0 < ε 1 measures
1 2
≥ ≪
the separation of time-scale in the system. We emphasize that the form of equations
as well as the Brownian motion noises are unknown.
2.1. Objective. Although (2.1) is unknown, in the sense that the functions f,
σ, g and β are unknown, we assume that historical data of the slow variable xε are
t
available. More specifically, let t < t < ... be discrete time instants. We observe
0 1
N 1 pieces of time history data of the slow variables xε:
T ≥ t
(2.2) xε(t(i)),xε(t(i)),...,xε(t(i)) , i=1,2,...
0 1 Li
(cid:16) (cid:17)
where for simplicity we assume a constant time lag ∆ = t t (1) and the
i i−1
− ∼ O
length of all pieces of data is the same, L = L, i. We remark that data of the fast
i
∀
variables are not required in this work.
Ourgoalistoconstructanumericalmodelinvolvingonly theslowvariablesxε for
t
thedynamicsoftheslowvariablesofthemultiscalesystem(2.1),withouttheinvolve-
ment of the fast variables yε. The model predictions x shall be an approximation, in
t
distribution, to the true (and unknown) slow dynamics, i.e.,
(2.3) x(t ;x ) d xε(t ;x ), i=(cid:98) 1,2,...
i 0 i 0
≈
for arbitrarily given initial condition x .
0
(cid:98)MODELLINGOFMULTISCALESDE 3
2.2. Related Work. The current work is an extension of the FML (flow map
learning) methodology for modeling deterministic unknown dynamical systems and
its stochastic extension sFML for modeling unknown stochastic dynamical systems.
For an unknown deterministic autonomous system, dx = f(x), x Rd, where
dt ∈
f is unknown. The FML framework seeks to approximate the unknown flow map
x =Φ (x ) by using historical trajectory data. Specifically, FML method fits a
n tn−ts s
model
x =Φ (x ),
n+1 ∆t n
where Φ Φ is a numerical approximation of the true flow map. Once trained,
∆t ∆t
≈ (cid:101)
theFMLmodelcanbeusedasatime-marchingschemetopredictthesystemresponse
under n(cid:101)ew initial conditions. This framework was proposed in [37], with extensions
toparametricsystems[36], partiallyobserveddynamicalsystems[22], aswellasnon-
autonomous deterministic system [35].
For learning stochastic dynamics, dx = f(x,ω(t)), where ω(t) represents an un-
dt
known stochastic process driving the system. The work of [8] developed stochastic
flowmaplearning(sFML).Assumingthesystemsatisfiestime-homogeneousproperty
P(x x ) = P(x x ), s 0 (c.f. [28]), the method uses observation data on the
s+∆t s ∆t 0
| | ≥
state variable x to construct a one-step generative model
x =G (x ;z),
n+1 ∆t n
where z is a random variable with known distribution (e.g., standard Gaussian). The
function G, termed stochastic flow map, approximates the conditional distribution
G (x ;z) P(x x ). Subsequently, the sFML model becomes a weak approxi-
∆t s s+∆t s
∼ |
mation, in distribution, to the true stochastic dynamics. Different generative models
can be employed under the sFML framework, e.g., generative adversarial networks
(GANs) [8], autoencoder [42], etc. Recently, it was extended to stochastic systems
subjected to external excitation using normalizing flow [9].
2.3. Contribution. Thecontributionofthecurrentworkisontheconstruction
ofdata-driveneffectivemodelfortheslowvariablesofthemultiscalestochasticsystem
(2.1). Under the assumption that the fast variables quickly reach their stationary
measure and become ”slaved” to the slow variables, we develop a sFML model that
involves only the slow variables for accurate predictions of the effective dynamics
of the original system. Such a modeling approach is highly useful, as in practice
one often only has observation data on the slow variables because the fast variables
are difficult, if not impossible, to observe. The proposed work can be considered as
an extension of the sFML modeling methodology from general stochastic system to
multiscale stochastic system.
3. Method Description. In this section, we describe the proposed method in
detail.
3.1. MathematicalMotivation. Letusconsidertheunknownfullsystem(2.1)
over a time interval [t ,t ], n 0,
n n+1
≥
tn+1 tn+1
(3.1) xε =xε + f(xε(s),yε(s))ds+ σ(xε(s),yε(s))dW1(s).
n+1 n n
(cid:90)tn (cid:90)tn
For the multiscale system with ε 1, there exists a time scale τ O(ε) such that
≪ ∼
for t>τ the fast variable y becomes slaved to the slow variable x, i.e.,
(3.2) yε(t) y(xε(t)) , t>τ,
x
≈ ∼P4 YUANCHENANDDONGBINXIU
where isits(conditioned)invariantmeasurethatisafunctionofxonly. Therefore,
x
P
on any time scale larger than τ O(ε), there exists an unknown mapping G such
∼
that
(3.3) xε G(xε,dW (∆)),
n+1 ≈ n n
where dW (s) = (dW1(t +s);dW2(t +s)) is the independent increment of the
n n n
Wiener process. The above argument is illustrated in Figure 1.
x
0
Gε (x 0,y(x
0))
xε
1 Gε(xε,y(xε))
xε
2
y 1 1
0
x
xε
P 0 P 1
O(ε)
Fig.1. Illustrationofhowthefastvariablesy quicklybecomeslavedtotheslowvariablesxin
multiscale system.
3.2. Stochastic Flow Map Learning. Since the slow variables x follow the
unknown mapping (3.3), we now seek to construct a stochastic flow map learning
(sFML) model in the form of a generative model
(3.4) x =G(x ,z ),
n+1 n n
wherez
n
∈Rnz isarandomvariab (cid:98)leofkn(cid:98)ow (cid:98)ndistribution,e.g.,astandardGaussian,
n is the dimension of the random input. Our objective is to ensure this is a weak
z
approximation, in distribution, to the unknown mapping (3.3). That is, given an
exact slow variable xε, the sFML model shall produce an approximation of the slow
n
variable t ,
n+1
(3.5) xε =G(xε,z ) d xε .
n+1 n n ≈ n+1
Note that the approximation (cid:101)is in dist(cid:98)ribution, a weak form of approximation.
3.2.1. LearningMethodandTrainingData. InordertoconstructthesFML
model (3.4) satisfying (3.5), we execute the sFML model for one time step over ∆,
(3.6) x =G(x ,z ),
1 0 0
and utilize the data set (2.2) to lea(cid:98)rn the(cid:98)u(cid:98)nknown operator G.
(cid:98)MODELLINGOFMULTISCALESDE 5
Toconstructthetrainingdata, wereorganizethetrajectorydata(2.2)intopairs,
separated by the constant time step ∆,
(3.7) xε(t(i) ),xε(t(i)) , k =1,...L, i=1,...,N ,
k−1 k T
(cid:16) (cid:17)
whichcontainatotalofM =N Lsuchdatapairs. Werenameeachpairusingindices
T
0 and 1, and re-organize the training data set into a set of such pairwise data entries,
(3.8) xε(i),xε(i) , i=1,...,M.
0 1
(cid:16) (cid:17)
In other words, the training data set is comprised of M numbers of very short tra-
jectories of only two entries. Each of the i-th trajectory, i=1,...,M, starts with an
“initial condition” xε(i) and ends a single time step ∆ later at xε(i). The one-step
0 1
sFML model (3.6) is then applied to this training data set to accomplish
(3.9) xε =G(xε,z ) d xε.
1 0 0 ≈ 1
Thepresenceoftherandomvaria (cid:101)blez 0(cid:98)enables(3.6)tobeagenerativemodelthatcan
producerandomrealizations. Severalmethodsexisttoconstructstochasticgenerative
models,e.g.,GANs,diffusionmodel,normalizingflow,autoencoder,etc. Inthispaper,
we adopt normalizing flow for (3.6).
3.2.2. Normalizing Flow Model. Normalizing flows are generative models
that can produce efficient and accurate sampling and density evaluation. A normal-
izing flow is designed to transform a simple probability distribution, e.g., a standard
Gaussian, into a more complex distribution by a sequence of diffeomorphisms. Let
Z RD be a random variable with a known and tractable distribution p . Let g be
Z
∈
a diffeomorphism, whose inverse is f = g−1, and Y = g(Z). By using the change of
variable formula, one obtains the probability of Y:
p (y)=p (f(y)) detDf(y) =p (f(y)) detDg(f(y))−1,
Y Z Z
| | | |
where Df(y) = ∂f/∂y is the Jacobian of f and Dg(z) = ∂g/∂z is the Jacobian of
g. When the target complex distribution p is given, usually as a set of samples
Y
of Y, one chooses to find g from a parameterized family g , where the parameter θ
θ
is optimized to match the target distribution. Also, to circumvent the difficulty of
constructingacomplicatednonlinearfunctiong,oneutilizesacompositionof(much)
simpler diffeomorphisms: g=g g g . It can be shown that g remains a
m m−1 1
◦ ◦···◦
diffeomorphism with its inverse f =f f f . There exists a large amount
1 m−1 m
◦···◦ ◦
of literature on normalizing flows. We refer interested readers to review articles such
as [32].
In our setting, we seek to construct the one-step generative model (3.6) by using
the training data (3.8) to accomplish (3.9). We choose z to be a ℓ-dimensional stan-
0
dardGaussiansincetheslowvariablesareinℓdimension. LetT beadiffeomorphism
θ
with a set of parameters θ Rnθ. Our objective is to find θ such that T θ(z 0) follows
∈
the distribution of xε given by the samples xε(i) M in (3.8).
1 { 1 }i=1
Since the distribution of xε clearly depends on the unknown dynamics starting
1
at xε, we constraint the choice of θ to be a function of xε, i.e.,
0 0
(3.10) θ =N(xε;Θ),
06 YUANCHENANDDONGBINXIU
where N is a DNN mapping with trainable hyper-parameters Θ. This effectively
defines
(3.11) xε =T (z ),
1 N(xε 0;Θ) 0
where the diffeomorphism T is parameterized by the trainable hyper-parameters Θ
of the DNN. Let S = T−1 be the inverse of T. We have z = S (xε). The
0 N(xε 0;Θ) 1
invertibility of T allows us to compute:
(3.12) p(xε xε;Θ)=p S (xε) detDT (S (xε)) −1 .
1| 0 z0 N(xε 0;Θ) 1 N(xε 0;Θ) N(xε 0;Θ) 1
ThehyperparametersΘar(cid:0)edeterminedb(cid:1)y(cid:12)
(cid:12)
maximizingtheexpectedlog-(cid:12)
(cid:12)likelihood
(3.12), which is accomplished by minimizing its negative as the loss function,
(Θ):= E log(p(xε xε;Θ)),
L − (xε 0,xε 1)∼pdata 1| 0
where p is the distribution from the training data set (3.8) and computed as
data
M
(3.13) (Θ)= log p(xε(j) xε(j);Θ) .
L − 1 | 0
(cid:88)j=1 (cid:16) (cid:17)
Several designs for the invertible map T have been developed and studied ex-
tensively in the literature. These include, for example, masked autoregressive flow
(MAF) [33], real-valued non-volume preserving (RealNVP) [16], neural ordinary dif-
ferentialequations(NeuralODE)[5],etc. Inthispaper,weadopttheMAFapproach,
where the dimension of the parameter θ in (3.10) is set to be n =2ℓ, where ℓ is the
θ
dimension of the slow variables. For the technical detail of MAF, see [33].
3.3. DNN Model Structure and System Prediction. An illustration of
the proposed sFML model structure can be found in Figure 2. This is in direct
correspondence of (3.11). Minimization of the loss function (3.13), using the data set
(3.8), results in the training of the DNN hyperparameters Θ. Once the training is
completed and Θ fixed, (3.11) effectively defines the one-step sFML model (3.6):
x =T (z )=G(x ,z ),
1 N(x0) 0 0 0
where we have suppressed the fixed parameter Θ(cid:98).
The sFML system then produces the system prediction iteratively, for a given
initial condition of the slow variable xε,
0
xε =xε,
0 0
(3.14)
(cid:40)x (cid:98)ε
n+1
=G(xε n,z n), n ≥0,
where z
n
are i.i.d. ℓ-dimens (cid:98)ional st(cid:98)and (cid:98)ard normal random variables. Courtesy of
the construction requirement (3.5), the sFML prediction is expected to be a weak
d
approximation in distribution to the true stochastic dynamics, i.e., xε xε, n 0.
n ≈ n ≥
We remark again that the sFML model (3.14) requires only the slow variables. The
fast variables are not required during both the training and the p(cid:98)rediction of the
model. Therefore, the sFML model (3.14) is an effective model for the evolution of
the slow variables of the original (unknown) multiscale stochastic dynamical system
(2.1).MODELLINGOFMULTISCALESDE 7
z
T x
1
x N
0
b
b
Fig. 2. The DNN model structure for the proposed normalizing flow sFML method (3.11).
4. Numerical Examples. In this section, we present several numerical tests
to demonstrate the performance of our proposed method. The examples include a
skew product SDE, an exponential mean OU process, followed by 3 3-dimensional
nonlinear SDEs. In all the examples, the true SDE systems are known. However, the
known SDEs are used only to generate the training data set (3.8). We solve the true
systems by Euler-Maruyama method with a time step 10−4. The “initial conditions”
x in (3.8) are sampled uniformly in a given domain, specified in each example.
0
InoursFMLmodel,Figure2,theDNNhas3layers,eachofwhichwith20nodes,
andutilizestanhactivationfunction. Weemploycycliclearningratewithabaserate
3 10−4 andamaximumrate5 10−4, γ =0.99999, andstepsize10,000. Thecycle
× ×
is set for every 40,000 training epochs and with a decay scale 0.5. In our examples,
the DNN training is usually conducted for 200,000 300,000 epochs and batch size
∼
is taken 30,000 40,000.
∼
4.1. A skew product SDE. We first consider the following 2-dimensional sto-
chastic dynamics mentioned in Section 10 of [34],
dx
= 1 y2 x,
dt −
(4.1) 
dy
=
(cid:0) α y+(cid:1) 2λdW
t
,
dt −ε ε dt
(cid:114)
where the parameters are
setas
α=1.0, λ=2.4, and ε=0.005 . The fast variable y
isanOrnstein-Uhlenbeck(OU)process,whichhasaGaussianstationarydistribution
withmean0andvarianceλ/α. Notably,thisdistributiondoesnotdependontheslow
variable x. The training data set (3.8) is generated by uniformly sampling (x ,y ) in
0 0
( 1.5,2) ( 1,1.6) and solved for up to T =1.0. A total of M =40,000 trajectory
− × −
pairs are used in the training data set (3.8), where the time step ∆ is taken 0.01. A
simulation of the full system is illustrated on the left of Figure 4.
Once the sFML model (3.4) is trained, we conduct system predictions up to
T = 4.0, which requires 400 time steps. In Figure 3, we compare some sample
trajectory paths produced by the ground truth (left) and the learned sFML model
(right), with an initial condition x =1.5. (For the ground truth, y is sampled from
0 08 YUANCHENANDDONGBINXIU
1.4 1.4
1.2 1.2
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
T T
Fig. 3. Sample trajectories of Example 4.1 slow variable with initial condition x0 = 1.5 and
y0 sampling from stationary distribution. Left: ground truth; Right: Simulation using the trained
sFML model.
6
1.4 x1(slow) 1.4 GroundTruthMean
1.2
y1(fast)
4 1.2
G Prr eo du in cd tioT nru Mth eaS ntd
1.0 1.0 PredictionStd
2
0.8 0.8
0
0.6 0.6
0.4 2 0.4
−
0.2
0.2
4
0.0 −
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
T T
Fig.4. Left: OnesimulatedsampleoffullsystemofExample4.1withinitialconditionx0=1.5
andy0 samplingfromstationarydistribution. Right: Comparisonformeanandstandarddeviation
(STD) of ground truth and learned sFL model for the slow variable.
thestationarydistribution. ForthesFMLmodel, y isnotrequired.) Weobservethe
0
two sets appear visually similar to each other. To further validate the accuracy of
sFMLmethod,wecomparethemeanandstandarddeviationofthesolutionaveraged
over10,000trajectories. ThesFMLmodelpredictionsareshownintherightofFigure
4, along with the reference ground truth. Good agreement is observed.
4.2. An exponential mean OU process. We consider the 2-dimensional sto-
chastic dynamics
dx
=1 x+y,
dt −
(4.2) 
dy
=
1
(e−x y)+
2dW
t .
dt ε − ε dt
(cid:114)
Conditionedontheslowvariablex,thefastvariableyisanOUprocessandeventually
converges to a Gaussian stationary distribution with a mean e−x and unit variance.
Thetrainingdatasetisgeneratedwith(x ,y )uniformlysampledfrom( 1.5,2)
0 0
− ×
( 4,4). Wesetε=0.001andsimulatethefullsystemwithterminationtimeT =1.0
−
with ∆=0.01. An illustration of the full system can be viewed on the left of Figure
6. A total of M =40,000 trajectory pairs are used in the training data set (3.8).
x yMODELLINGOFMULTISCALESDE 9
1.35
1.30
1.30
1.25
1.25
1.20
1.20
1.15
1.15
1.10
1.10
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
T T
Fig. 5. Sample trajectories of Example 4.2 slow variable with initial condition x0 = 1.5 and
y0 sampling from stationary distribution. Left: ground truth; Right: Simulation using the trained
sFML model.
3 1.300
1.30 2 1.275
1.250
1
1.25
1.225
0
1.200
1.20 1
−
1.175
2 GroundTruthMean
1.15 − 1.150 GroundTruthStd
x1(slow) −3
1.125
PredictionMean
1.10 y1(fast) 4 PredictionStd
− 1.100
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0
T T
Fig.6. Left: OnesimulatedsampleoffullsystemofExample4.2withinitialconditionx0=1.5
andy0 samplingfromstationarydistribution(conditionedonx0). Right: Comparisonformeanand
standard deviation (STD) of ground truth and learned sFL model for the slow variable.
With the well-trained sFML model, we compare some sample trajectory paths
produced by the ground truth (left) and the learned sFML model (right) in Figure
5. The two sets of samples are visually similar. The mean and STD of the solution
are shown in Figure 6. Good agreement is observed for sFML model and reference
ground truth.
4.3. A triad system. We consider the following 3-dimensional stochastic dy-
namics
dx 2
= y y ,
1 2
dt −ε

dy 1 1 σdW1
(4.3) 
dd
yt1 =
−ε
22y 1+ 1εxy 2+
σε
dWdtt 2,
2 = y + xy + t ,
where the parameters
aresed tt
as
σ− =ε2
1
2 andε
ε2
=1 0.0ε 01.dt
This system is studied as a
stochastic model for mean flow-wave interaction in barotropic-baroclinic turbulence
[14,26]. We are interested in the climate (slow) variable x, instead of the weather
(fast) variables y. We plot one trajectory sample of the full system on the left of
x y10 YUANCHENANDDONGBINXIU
Figure 7. According to the work of [19], the analytically dervied effective equation
for x is dx = 1x + √1 dW , which indicated that the limiting process of x is a OU
t −2 t 3 t
process.
Here we utilize the sFML approach to construct a data-driven effective model
for x. We generate the training data set up to T = 1 with initial condition (x ,y )
0 0
uniformly sampling from ( 2,3) ( 1,1)2 and time step with ∆ = 0.01. A total
− × −
60,000 trajectory pair data are utilized for training the sFML model.
x1(slow) y1(fast) y2(fast)
GroundTruthMean
1.4 2.0 1.0 GroundTruthStd
1.2 0.8 PredictionMean
1.5
PredictionStd
1.0 1.0 0.6
0.8 0.5 0.4
0.6 0.0 0.2
0.4 −0.5 0.0
0.2 −1.0
0.2
1.5 −
0.0 −
−2.0 −0.4
0 1 2 3 4 5 0 1 2 3 4 5
T T
Fig. 7. Left: One simulated sample of full system of Example 4.4 with initial condition x0 =
1 and y sampling from stationary distribution (conditioned on x0). Right: Mean and standard
deviation (STD) for slow variable.
Oncesuccessfullytrained,wegeneratetrajectorysamplesuptoT =5withinitial
condition x = 1 using the trained model and compare them with ground truth. It
0
takes 500 steps to generate data from sFML model. Visual comparison of the sample
paths for x is presented in Figure 8, where we observe qualitatively similar behavior.
Tofurthervalidatetheaccuracy,wepresentthecomparisonsofmeanandSTDonthe
right of Figure 7. We observe good agreement between the sFML model prediction
and the ground truth.
2.0
1.5
1.5
1.0
1.0
0.5
0.5
0.0 0.0
−0.5 −0.5
1.0
1.0 −
−
1.5
−
0 1 2 3 4 5 0 1 2 3 4 5
T T
Fig. 8. Sample trajectories of Example 4.3 slow variable with initial condition x0 =1 and y0
sampling from stationary distribution (conditioned on x0). Left: ground truth; Right: Simulation
of x using the trained sFML model.
x yMODELLINGOFMULTISCALESDE 11
4.4. A 3D nonlinear SDE. Weconsiderthefollowing3-dimensionalstochastic
dynamics inspired by [21],
dx dW1
1 =x +σ t ,
2 1
dt dt

(4.4)
d dx
t2 = −x
1
−x 2+y2+σ
2dW dtt2
,
dy 1 1 2dW3
= y x +σ t ,
1 3
dt −ε
(cid:18)
− 4
(cid:19)
(cid:114)ε dt
where the parameters are set as σ = 0.3, σ = 0.3, σ = 0.1, and ε = 0.001. The
1 2 3
one-dimensional fast variable y is driven by an OU process and evolves to a Gaussian
distribution with a mean 1x and variance σ2. We plot one trajectory sample of the
4 1 3
full system in Figure 9 for a visual illustration.
2.0
x (slow)
1
1.5 x (slow) 0.6
2
y (fast)
1
1.0 0.4
0.5 0.2
0.0
0.0
0.5
− 0.2
−
1.0
−
0.4
−
0 1 2 3 4 5 6 7 8
T
Fig.9. OnesimulatedsampleoffullsystemofExample4.4withinitialconditionx0=(1.5,1.0)
and y0 sampling from stationary distribution.
To train the sFML effective model, we generate the training data set up to
T =1withinitialcondition(x ,y )uniformlysamplingfrom( 1.5,2.5) ( 2,1.5)
0 0
− × − ×
( 0.6,1) and a time step ∆ = 0.01. A total of 120,000 trajectory pair data are uti-
−
lized for training the sFML model. Once successfully trained, we generate trajectory
samples up to T = 8 with initial condition x = (1.5,1.0) using the trained sFML
0
model and compare them with ground truth. Visual comparison of the sample paths
of the slow variables are shown 10. To further validate the accuracy, we present the
comparison of mean and STD in Figure 11. And in Figure 12, we also show the com-
parison of the probability distribution of the solution at T = 2,4,6,8. We observe
good agreement between the sFML model prediction and the ground truth.
x y12 YUANCHENANDDONGBINXIU
2.0 2.0
1.5 1.5
1.0 1.0
0.5 0.5
0.0 0.0
0.5 0.5
− −
1.0 1.0
− 0 1 2 3 4 5 6 7 8 − 0 1 2 3 4 5 6 7 8
T T
1.0 1.0
0.5 0.5
0.0 0.0
−0.5 −0.5
−1.0 −1.0
0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 8
T T
Fig. 10. Sample trajectories of Example 4.4 slow variable with an initial condition x0 =
(1.5,1.0). Leftcolumn: groundtruthofx1(top)andx2(bottom),withy0sampledfromitsstationary
distribution (conditioned on x0); Right column: sFML simulation of x1 (top) and x2 (bottom).
1.00
GroundTruthMean GroundTruthMean
1.5 GroundTruthStd 0.75 GroundTruthStd
PredictionMean 0.50 PredictionMean
PredictionStd PredictionStd
1.0 0.25
0.00
0.5 0.25
−
0.50
0.0 −
0.75
−
−0.5 −1.00
0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 8
T T
Fig.11. Example4.4: Meanandstandarddeviation(STD)fortheslowvariablesx1 (left)and
x2 (right), with an initial condition x0=(1.5,1.0).
4.5. Multiscale Stochastic Oscillator. We consider the following stochastic
oscillator,
dx dW1
1 =λx θx γx y+σ t ,
1 2 1
dt − − dt

dx dW2
(4.5)  dt2 =θx 1+λx
2
−γx 2y+σ dtt ,
dy 1 2dW3
= y x2 x2 +σ t ,
dt −ε
(cid:0)
− 1− 2
(cid:1)
(cid:114)ε dtMODELLINGOFMULTISCALESDE 13
1.0 G Ler ao ru nn ed dTruth 1.0 G Ler ao ru nn ed dTruth 1.0 G Ler ao ru nn ed dTruth 1.0 G Ler ao ru nn ed dTruth
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 −0.5 0.0 0.5 1.0 1.5 2.0 0.0 −1.5 −1.0 −0.5 0.0 0.5 1.0 0.0 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5 0.0 −1.5 −1.0 −0.5 0.0 0.5 1.0 1.5
11 .. 46 G Ler ao ru nn ed dTruth 1.2 G Ler ao ru nn ed dTruth 11 .. 24 G Ler ao ru nn ed dTruth 1.2 G Ler ao ru nn ed dTruth
1.2 1.0 1.0 1.0
1.0 0.8 0.8 0.8
0.8 0.6 0.6 0.6
0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 −1.75−1.50−1.25−1.00−0.75−0.50−0.25 0.00 0.25 0.0 −1.0 −0.5 0.0 0.5 1.0 0.0 −1.0 −0.5 0.0 0.5 1.0 1.5 0.0 −1.0 −0.5 0.0 0.5 1.0 1.5
Fig. 12. Example 4.4: Comparison of the distribution of the slow variables at T = 2,4,6,8
with an initial condition x0=(1.5,1.0). Top row: x1; Bottom row: x2.
where the parameters are set as λ = 1.0, θ = 1.0, γ = 1.0, σ = 0.1, and ε = 0.001.
Thissystemisstudiedin[4],whosedeterministicversionwasstudiedin[27]asalow-
dimensional reduced model for a flow past a circular cylinder. The one-dimensional
fast process y is an OU process and evolves to a Gaussian distribution with mean
(x2 +x2) and variance σ2. A trajectory sample path of the full system is shown in
1 2
Figure 13 for demonstration purposes.
x (slow) x (slow) y (fast)
1 2 1
1.0 2.0
1.8
0.5
1.6
1.4
0.0
1.2
1.0
0.5
−
0.8
0.6
1.0
−
0.4
0 1 2 3 4 5 6 7 8
T
Fig. 13. One simulated sample of full system of Example 4.5 with initial condition x0 =
(1.0,1.0) and y0 sampling from stationary distribution.
TotrainthesFMLeffectivemodel, wegeneratethetrainingdatasetuptoT =1
with initial condition (x ,y ) uniformly sampling from ( 1.5,1.5)2 (0.1,2.5) and
0 0
− ×
time step ∆=0.01. A total of 120,000 trajectory data pairs are utilized for training
the sFML model. Once the sFML model is obtained, we generate system prediction
foruptoT =20withinitialconditionx =(1.0,1.0)andcomparethemwithground
0
truth. Samples of the phase portraits of the slow variables are shown in Figure 14,
whereweobservegoodvisualcomparison. Tofurthervalidatetheaccuracy,wepresent
the comparison of mean and STD in Figure 15, and the probability distributions at
x y14 YUANCHENANDDONGBINXIU
time T = 4,8,12,16 in Figure 16. We observe good agreement between the sFML
model prediction and the ground truth.
1.0 1.0
0.5 0.5
0.0 0.0
−0.5 −0.5
1.0
− 1.0
−
1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0
− − x1 − − x1
Fig. 14. Example 4.5: Samples of phase portrait of the slow variables (x1,x2) with initial
condition x0 = (1.0,1.0). Left: ground truth (with y0 sampled from its stationary distribution);
Right: sFML model.
GroundTruthMean PredictionMean GroundTruthMean PredictionMean
GroundTruthStd PredictionStd GroundTruthStd PredictionStd
1.00
1.00
0.75 0.75
0.50 0.50
0.25 0.25
0.00 0.00
−0.25 −0.25
−0.50 −0.50
−0.75 −0.75
1.00 1.00
− −
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
T T
Fig.15. Example4.5: Comparisonofthemeanandstandarddeviation(STD)forslowvariables
with initial condition x0=(1.0,1.0). Left: x1; Right: x2
5. Conclusions. In this paper, we presented a numerical method for construct-
ing accurate effective numerical model of slow-fast multiscale stochastic dynamical
systems. The method utilizes short bursts of data of the slow variables and con-
structs the stochastic flow map of the slow dynamics. A conditional normalizing flow
modelisemployedtoensurethelearnedmodelisanaccurategenerativemodelindis-
tribution. Oncesuccessfullytrained,themodelcanserveasatimesteppertoproduce
predictions of the slow variables, with any given initial conditions. By using a com-
prehensivesetofnumericalexamples,wedemonstratedthattheproposedapproachis
effectiveandaccurateinmodelingavarietyofunknownmultiscalestochasticsystems.
REFERENCES
2x 2xMODELLINGOFMULTISCALESDE 15
12 .. 70 50 G Ler ao ru nn ed dTruth 22 .. 05 G Ler ao ru nn ed dTruth 45 G Ler ao ru nn ed dTruth 11 .. 02 G Ler ao ru nn ed dTruth
11 .. 25 50 1.5 3 0.8
1.00 0.6
0.75 1.0 2 0.4
00 .. 25 50 0.5 1 0.2
0.00 −0.6 −0.4 −0.2 0.0 0.2 0.4 0.6 0.8 0.0 −1.0 −0.8 −0.6 −0.4 −0.2 0.0 0.2 0 0.0 0.2 0.4 0.6 0.8 1.0 1.2 0.0 −1.00−0.75−0.50−0.25 0.00 0.25 0.50 0.75
67 G Ler ao ru nn ed dTruth 11 .. 57 05 G Ler ao ru nn ed dTruth 11 .. 02 G Ler ao ru nn ed dTruth 23 .. 50 G Ler ao ru nn ed dTruth
345 011 ... 702 505 00 .. 68 12 .. 50
2 0.50 0.4 1.0
1 0.25 0.2 0.5
0 −1.1 −1.0 −0.9 −0.8 −0.7 0.00 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00 0.0 −0.75 −0.50 −0.25 0.00 0.25 0.50 0.75 1.00 0.0 −1.2 −1.0 −0.8 −0.6 −0.4 −0.2 0.0 0.2 0.4
Fig. 16. Example 4.5: Comparasion of the distribution of the slow variables at T =4,8,12,16
with initial condition x0=(1.0,1.0). Top row: x1; Bottom row: x2.
[1] C. Archambeau, D. Cornford, M. Opper, and J. Shawe-Taylor, Gaussian process ap-
proximations of stochastic differential equations,inGaussianProcessesinPractice,N.D.
Lawrence, A. Schwaighofer, and J. Quin˜onero Candela, eds., vol. 1 of Proceedings of
Machine Learning Research, Bletchley Park, UK, 12–13 Jun 2007, PMLR, pp. 1–16,
https://proceedings.mlr.press/v1/archambeau07a.html.
[2] S. L. Brunton, J. L. Proctor, and J. N. Kutz,Discovering governing equations from data
by sparse identification of nonlinear dynamical systems, Proc. Natl. Acad. Sci. USA, 113
(2016),pp.3932–3937,https://doi.org/10.1073/pnas.1517384113.
[3] K.P.Champion,S.L.Brunton,andJ.N.Kutz,Discoveryofnonlinearmultiscalesystems:
sampling strategies and embeddings, SIAM J. Appl. Dyn. Syst., 18 (2019), pp. 312–333,
https://doi.org/10.1137/18M1188227,https://doi.org/10.1137/18M1188227.
[4] M.D.Chekroun,A.Tantet,H.A.Dijkstra,andJ.D.Neelin,Ruelle–pollicottresonances
ofstochasticsystemsinreducedstatespace.parti: Theory,JournalofStatisticalPhysics,
179(2020),pp.1366–1402.
[5] R. T. Q. Chen, Y. Rubanova, J. Bettencourt, and D. K. Duvenaud, Neural ordinary
differential equations, in Advances in Neural Information Processing Systems, S. Bengio,
H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, eds., vol. 31,
CurranAssociates,Inc.,2018,https://proceedings.neurips.cc/paper files/paper/2018/file/
69386f6bb1dfed68692a24c8686939b9-Paper.pdf.
[6] X.Chen,J.Duan,J.Hu,andD.Li,Data-drivenmethodtolearnthemostprobabletransition
pathway and stochastic differential equation, Phys. D, 443 (2023), pp. Paper No. 133559,
15,https://doi.org/10.1016/j.physd.2022.133559.
[7] X. Chen, L. Yang, J. Duan, and G. E. Karniadakis, Solving inverse stochastic problems
fromdiscreteparticleobservationsusingtheFokker-Planckequationandphysics-informed
neural networks, SIAM J. Sci. Comput., 43 (2021), pp. B811–B830, https://doi.org/10.
1137/20M1360153.
[8] Y.ChenandD.Xiu,Learningstochasticdynamicalsystemviaflowmapoperator,J.Comput.
Phys.,508(2024),p.PaperNo.112984,https://doi.org/10.1016/j.jcp.2024.112984,https:
//doi.org/10.1016/j.jcp.2024.112984.
[9] Y. Chen and D. Xiu, Modeling unknown stochastic dynamical system subject to external
excitation,arXivpreprintarXiv:2406.15747,(2024).
[10] Z. Chen, V. Churchill, K. Wu, and D. Xiu, Deep neural network modeling of unknown
partialdifferentialequationsinnodalspace,JournalofComputationalPhysics,449(2022),
p.110782.
[11] V.ChurchillandD.Xiu,Flowmaplearningforunknowndynamicalsystems: Overview,im-
plementation,andbenchmarks,JournalofMachineLearningforModelingandComputing,
4(2023),pp.173–201.
[12] R. R. Coifman, I. G. Kevrekidis, S. Lafon, M. Maggioni, and B. Nadler, Diffusion
maps, reduction coordinates, and low dimensional representation of stochastic systems,
MultiscaleModel.Simul.,7(2008),pp.842–864,https://doi.org/10.1137/070696325,https:
//doi.org/10.1137/070696325.
[13] M. Darcy, B. Hamzi, G. Livieri, H. Owhadi, and P. Tavallali, One-shot learning of sto-
chastic differential equations with data adapted kernels, Phys. D, 444 (2023), pp. Paper
No.133583,18,https://doi.org/10.1016/j.physd.2022.133583.16 YUANCHENANDDONGBINXIU
[14] T. DelSole and B. F. Farrell, The quasi-linear equilibration of a thermally maintained,
stochasticallyexcitedjetinaquasigeostrophicmodel,JournalofAtmosphericSciences,53
(1996),pp.1781–1797.
[15] F. Dietrich, A. Makeev, G. Kevrekidis, N. Evangelou, T. Bertalan, S. Reich, and
I. G. Kevrekidis, Learning effective stochastic differential equations from microscopic
simulations: Linking stochastic numerics to deep learning, Chaos: An Interdisciplinary
JournalofNonlinearScience,33(2023).
[16] L. Dinh, J. Sohl-Dickstein, and S. Bengio,Density estimation using real NVP,inInterna-
tional Conference on Learning Representations, 2017, https://openreview.net/forum?id=
HkpbnH9lx.
[17] C. J. Dsilva, R. Talmon, C. W. Gear, R. R. Coifman, and I. G. Kevrekidis,Data-driven
reduction for a class of multiscale fast-slow stochastic dynamical systems,SIAMJ.Appl.
Dyn.Syst.,15(2016),pp.1327–1351,https://doi.org/10.1137/151004896,https://doi.org/
10.1137/151004896.
[18] J. Duan, K. Lu, and B. Schmalfuss, Invariant manifolds for stochastic partial differen-
tial equations, Ann. Probab., 31 (2003), pp. 2109–2135, https://doi.org/10.1214/aop/
1068646380,https://doi.org/10.1214/aop/1068646380.
[19] W. E, D. Liu, and E. Vanden-Eijnden,Analysis of multiscale methods for stochastic differ-
ential equations,Comm.PureAppl.Math.,58(2005),pp.1544–1585,https://doi.org/10.
1002/cpa.20088,https://doi.org/10.1002/cpa.20088.
[20] W.EandE.Vanden-Eijnden,Metastability,conformationdynamics,andtransitionpathways
incomplexsystems,inMultiscalemodellingandsimulation,vol.39ofLect.NotesComput.
Sci.Eng.,Springer,Berlin,2004,pp.35–68,https://doi.org/10.1007/978-3-642-18756-8 3,
https://doi.org/10.1007/978-3-642-18756-8 3.
[21] L. Feng, T. Gao, M. Dai, and J. Duan, Learning effective dynamics from data-driven sto-
chastic systems, Chaos, 33 (2023), pp. Paper No. 043131, 15, https://doi.org/10.1063/5.
0126667,https://doi.org/10.1063/5.0126667.
[22] X. Fu, L.-B. Chang, and D. Xiu, Learning reduced systems via deep neural networks with
memory,J.MachineLearningModel.Comput.,1(2020),pp.97–118.
[23] R. Khasminskij, On the principle of averaging the itov’s stochastic differential equations.,
Kybernetika,4(1968),pp.260–279.
[24] Z. Li, N. B. Kovachki, K. Azizzadenesheli, B. liu, K. Bhattacharya, A. Stuart, and
A. Anandkumar, Fourier neural operator for parametric partial differential equations,
in International Conference on Learning Representations, 2021, https://openreview.net/
forum?id=c8P9NQVtmnO.
[25] C. Ma, J. Wang, and W. E, Model reduction with memory and the machine learning of
dynamicalsystems,Commun.Comput.Phys.,25(2019),pp.947–962,https://doi.org/10.
4208/cicp.oa-2018-0269,https://doi.org/10.4208/cicp.oa-2018-0269.
[26] A.J.Majda,I.Timofeyev,andE.VandenEijnden,Modelsforstochasticclimateprediction,
ProceedingsoftheNationalAcademyofSciences,96(1999),pp.14687–14691.
[27] B.R.Noack,K.Afanasiev,M.Morzyn´ski,G.Tadmor,andF.Thiele,Ahierarchyoflow-
dimensional models for the transient and post-transient cylinder wake, Journal of Fluid
Mechanics,497(2003),pp.335–363.
[28] B. Øksendal, Stochastic differential equations, in Stochastic differential equations, Springer,
2003,pp.65–84.
[29] M. Opper,Variational inference for stochastic differential equations,Ann.Phys.,531(2019),
pp.1800233,9,https://doi.org/10.1002/andp.201800233.
[30] H.Owhadi,Computationalgraphcompletion,ResearchintheMathematicalSciences,9(2022),
p.27.
[31] G.Papamakarios,E.Nalisnick,D.J.Rezende,S.Mohamed,andB.Lakshminarayanan,
Normalizingflowsforprobabilisticmodelingandinference,TheJournalofMachineLearn-
ingResearch,22(2021),pp.2617–2680.
[32] G.Papamakarios,E.Nalisnick,D.J.Rezende,S.Mohamed,andB.Lakshminarayanan,
Normalizingflowsforprobabilisticmodelingandinference,J.Mach.Learn.Res.,22(2021),
pp.PaperNo.57,64.
[33] G. Papamakarios, T. Pavlakou, and I. Murray, Masked autoregressive flow for density
estimation, in Advances in Neural Information Processing Systems, I. Guyon, U. V.
Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,eds.,vol.30,
CurranAssociates,Inc.,2017,https://proceedings.neurips.cc/paper files/paper/2017/file/
6c1da886822c67822bcf3679d04369fa-Paper.pdf.
[34] G. A. Pavliotis and A. Stuart,Multiscale methods: averaging and homogenization,vol.53,
SpringerScience&BusinessMedia,2008.MODELLINGOFMULTISCALESDE 17
[35] T. Qin, Z. Chen, J. D. Jakeman, and D. Xiu, Data-driven learning of nonautonomous
systems, SIAM J. Sci. Comput., 43 (2021), pp. A1607–A1624, https://doi.org/10.1137/
20M1342859.
[36] T.Qin,Z.Chen,J.D.Jakeman,andD.Xiu,Deeplearningofparameterizedequationswith
applications to uncertainty quantification, Int. J. Uncertain. Quantif., 11 (2021), pp. 63–
82, https://doi.org/10.1615/Int.J.UncertaintyQuantification.2020034123, https://doi.org/
10.1615/Int.J.UncertaintyQuantification.2020034123.
[37] T.Qin,K.Wu,andD.Xiu,Datadrivengoverningequationsapproximationusingdeepneural
networks, J.Comput.Phys., 395(2019), pp.620–635, https://doi.org/10.1016/j.jcp.2019.
06.042.
[38] M. Raissi, P. Perdikaris, and G. Karniadakis, Physics-informed neural networks: A deep
learning framework for solving forward and inverse problems involving nonlinear partial
differential equations, Journal of Computational Physics, 378 (2019), pp. 686–707, https:
//doi.org/10.1016/j.jcp.2018.10.045.
[39] M.Raissi,P.Perdikaris,andG.E.Karniadakis,Multistepneuralnetworksfordata-driven
discovery of nonlinear dynamical systems,arXivpreprintarXiv:1801.01236,(2018).
[40] B. Schmalfuss and K. R. Schneider, Invariant manifolds for random dynamical systems
with slow and fast variables, J. Dynam. Differential Equations, 20 (2008), pp. 133–164,
https://doi.org/10.1007/s10884-007-9089-7,https://doi.org/10.1007/s10884-007-9089-7.
[41] K.WuandD.Xiu,Data-drivendeeplearningofpartialdifferentialequationsinmodalspace,
JournalofComputationalPhysics,408(2020),p.109307.
[42] Z. Xu, Y. Chen, Q. Chen, and D. Xiu, Modeling unknown stochastic dynamical system via
autoencoder,arXivpreprintarXiv:2312.10001,(2023).
[43] L.Yang,C.Daskalakis,andG.E.Karniadakis,Generativeensembleregression: Learning
particle dynamics from observations of ensembles with physics-informed deep generative
models,SIAMJournalonScientificComputing,44(2022),pp.B80–B99,https://doi.org/
10.1137/21M1413018.
[44] F. X.-F. Ye, S. Yang, and M. Maggioni, Nonlinear model reduction for slow-fast stochas-
tic systems near unknown invariant manifolds, J. Nonlinear Sci., 34 (2024), pp. Pa-
per No. 22, 54, https://doi.org/10.1007/s00332-023-09998-8, https://doi.org/10.1007/
s00332-023-09998-8.
[45] C. Yildiz, M. Heinonen, J. Intosalmi, H. Mannerstrom, and H. Lahdesmaki, Learning
stochasticdifferentialequationswithgaussianprocesseswithoutgradientmatching,in2018
IEEE 28th International Workshop on Machine Learning for Signal Processing (MLSP),
IEEE,2018,pp.1–6.
[46] J.Zhang,S.Zhang,andG.Lin,Multiauto-deeponet: Amulti-resolutionautoencoderdeeponet
for nonlinear dimension reduction, uncertainty quantification and operator learning of
forward and inverse stochastic problems,arXivpreprintarXiv:2204.03193,(2022).
[47] P. a. Zielin´ski and J. S. Hesthaven, Discovery of slow variables in a class of mul-
tiscale stochastic systems via neural networks, J. Nonlinear Sci., 32 (2022), pp. Pa-
per No. 51, 34, https://doi.org/10.1007/s00332-022-09808-7, https://doi.org/10.1007/
s00332-022-09808-7.