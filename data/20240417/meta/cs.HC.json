[
    {
        "title": "A Systematic Survey of the Gemini Principles for Digital Twin Ontologies",
        "authors": "James Michael ToothNilufer TuptukJeremy Daniel McKendrick Watson",
        "links": "http://arxiv.org/abs/2404.10754v1",
        "entry_id": "http://arxiv.org/abs/2404.10754v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10754v1",
        "summary": "Ontologies are widely used for achieving interoperable Digital Twins (DTws),\nyet competing DTw definitions compound interoperability issues. Semantically\nlinking these differing twins is feasible through ontologies and Cognitive\nDigital Twins (CDTws). However, it is often unclear how ontology use bolsters\nbroader DTw advancements. This article presents a systematic survey following\nthe PRISMA method, to explore the potential of ontologies to support DTws to\nmeet the Centre for Digital Built Britain's Gemini Principles and aims to link\nprogress in ontologies to this framework. The Gemini Principles focus on common\nDTw requirements, considering: Purpose for 1) Public Good, 2) Value Creation,\nand 3) Insight; Trustworthiness with sufficient 4) Security, 5) Openness, and\n6) Quality; and appropriate Functionality of 7) Federation, 8) Curation, and 9)\nEvolution. This systematic literature review examines the role of ontologies in\nfacilitating each principle. Existing research uses ontologies to solve DTw\nchallenges within these principles, particularly by connecting DTws, optimising\ndecisionmaking, and reasoning governance policies. Furthermore, analysing the\nsectoral distribution of literature found that research encompassing the\ncrossover of ontologies, DTws and the Gemini Principles is emerging, and that\nmost innovation is predominantly within manufacturing and built environment\nsectors. Critical gaps for researchers, industry practitioners, and\npolicymakers are subsequently identified.",
        "updated": "2024-04-16 17:34:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10754v1"
    },
    {
        "title": "Bootstrapping Linear Models for Fast Online Adaptation in Human-Agent Collaboration",
        "authors": "Benjamin A NewmanChris PaxtonKris KitaniHenny Admoni",
        "links": "http://arxiv.org/abs/2404.10733v1",
        "entry_id": "http://arxiv.org/abs/2404.10733v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10733v1",
        "summary": "Agents that assist people need to have well-initialized policies that can\nadapt quickly to align with their partners' reward functions. Initializing\npolicies to maximize performance with unknown partners can be achieved by\nbootstrapping nonlinear models using imitation learning over large, offline\ndatasets. Such policies can require prohibitive computation to fine-tune\nin-situ and therefore may miss critical run-time information about a partner's\nreward function as expressed through their immediate behavior. In contrast,\nonline logistic regression using low-capacity models performs rapid inference\nand fine-tuning updates and thus can make effective use of immediate in-task\nbehavior for reward function alignment. However, these low-capacity models\ncannot be bootstrapped as effectively by offline datasets and thus have poor\ninitializations. We propose BLR-HAC, Bootstrapped Logistic Regression for Human\nAgent Collaboration, which bootstraps large nonlinear models to learn the\nparameters of a low-capacity model which then uses online logistic regression\nfor updates during collaboration. We test BLR-HAC in a simulated surface\nrearrangement task and demonstrate that it achieves higher zero-shot accuracy\nthan shallow methods and takes far less computation to adapt online while still\nachieving similar performance to fine-tuned, large nonlinear models. For code,\nplease see our project page https://sites.google.com/view/blr-hac.",
        "updated": "2024-04-16 17:05:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10733v1"
    },
    {
        "title": "Attention-Aware Visualization: Tracking and Responding to User Perception Over Time",
        "authors": "Arvind SrinivasanJohannes EllemosePeter W. S. ButcherPanagiotis D. RitsosNiklas Elmqvist",
        "links": "http://arxiv.org/abs/2404.10732v1",
        "entry_id": "http://arxiv.org/abs/2404.10732v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10732v1",
        "summary": "We propose the notion of Attention-Aware Visualizations (AAVs) that track the\nuser's perception of a visual representation over time and feed this\ninformation back to the visualization. Such context awareness is particularly\nuseful for ubiquitous and immersive analytics where knowing which embedded\nvisualizations the user is looking at can be used to make visualizations react\nappropriately to the user's attention: for example, by highlighting data the\nuser has not yet seen. We can separate the approach into three components: (1)\nmeasuring the user's gaze on a visualization and its parts; (2) tracking the\nuser's attention over time; and (3) reactively modifying the visual\nrepresentation based on the current attention metric. In this paper, we present\ntwo separate implementations of AAV: a 2D data-agnostic method for web-based\nvisualizations that can use an embodied eyetracker to capture the user's gaze,\nand a 3D data-aware one that uses the stencil buffer to track the visibility of\neach individual mark in a visualization. Both methods provide similar\nmechanisms for accumulating attention over time and changing the appearance of\nmarks in response. We also present results from a qualitative evaluation\nstudying visual feedback and triggering mechanisms for capturing and\nrevisualizing attention.",
        "updated": "2024-04-16 17:04:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10732v1"
    },
    {
        "title": "Cross-Language Evolution of Divergent Collective Memory Around the Arab Spring",
        "authors": "H. Laurie JonesBrian C. Keegan",
        "links": "http://arxiv.org/abs/2404.10706v1",
        "entry_id": "http://arxiv.org/abs/2404.10706v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10706v1",
        "summary": "The Arab Spring was a historic set of protests beginning in 2011 that toppled\ngovernments and led to major conflicts. Collective memories of events like\nthese can vary significantly across social contexts in response to political,\ncultural, and linguistic factors. While Wikipedia plays an important role in\ndocumenting both historic and current events, little attention has been given\nto how Wikipedia articles, created in the aftermath of major events, continue\nto evolve over years or decades. Using the archived content of Arab\nSpring-related topics across the Arabic and English Wikipedias between 2011 and\n2024, we define and evaluate multilingual measures of event salience,\ndeliberation, contextualization, and consolidation of collective memory\nsurrounding the Arab Spring. Our findings about the temporal evolution of the\nWikipedia articles' content similarity across languages has implications for\ntheorizing about online collective memory processes and evaluating linguistic\nmodels trained on these data.",
        "updated": "2024-04-16 16:30:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10706v1"
    },
    {
        "title": "MathWriting: A Dataset For Handwritten Mathematical Expression Recognition",
        "authors": "Philippe GervaisAsya FadeevaAndrii Maksai",
        "links": "http://arxiv.org/abs/2404.10690v1",
        "entry_id": "http://arxiv.org/abs/2404.10690v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10690v1",
        "summary": "We introduce MathWriting, the largest online handwritten mathematical\nexpression dataset to date. It consists of 230k human-written samples and an\nadditional 400k synthetic ones. MathWriting can also be used for offline HME\nrecognition and is larger than all existing offline HME datasets like\nIM2LATEX-100K. We introduce a benchmark based on MathWriting data in order to\nadvance research on both online and offline HME recognition.",
        "updated": "2024-04-16 16:10:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10690v1"
    }
]