[
    {
        "title": "COMBO: Compositional World Models for Embodied Multi-Agent Cooperation",
        "authors": "Hongxin ZhangZeyuan WangQiushi LyuZheyuan ZhangSunli ChenTianmin ShuYilun DuChuang Gan",
        "links": "http://arxiv.org/abs/2404.10775v1",
        "entry_id": "http://arxiv.org/abs/2404.10775v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10775v1",
        "summary": "In this paper, we investigate the problem of embodied multi-agent\ncooperation, where decentralized agents must cooperate given only partial\negocentric views of the world. To effectively plan in this setting, in contrast\nto learning world dynamics in a single-agent scenario, we must simulate world\ndynamics conditioned on an arbitrary number of agents' actions given only\npartial egocentric visual observations of the world. To address this issue of\npartial observability, we first train generative models to estimate the overall\nworld state given partial egocentric observations. To enable accurate\nsimulation of multiple sets of actions on this world state, we then propose to\nlearn a compositional world model for multi-agent cooperation by factorizing\nthe naturally composable joint actions of multiple agents and compositionally\ngenerating the video. By leveraging this compositional world model, in\ncombination with Vision Language Models to infer the actions of other agents,\nwe can use a tree search procedure to integrate these modules and facilitate\nonline cooperative planning. To evaluate the efficacy of our methods, we create\ntwo challenging embodied multi-agent long-horizon cooperation tasks using the\nThreeDWorld simulator and conduct experiments with 2-4 agents. The results show\nour compositional world model is effective and the framework enables the\nembodied agents to cooperate efficiently with different agents across various\ntasks and an arbitrary number of agents, showing the promising future of our\nproposed framework. More videos can be found at\nhttps://vis-www.cs.umass.edu/combo/.",
        "updated": "2024-04-16 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10775v1"
    },
    {
        "title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents",
        "authors": "Liyan TangPhilippe LabanGreg Durrett",
        "links": "http://arxiv.org/abs/2404.10774v1",
        "entry_id": "http://arxiv.org/abs/2404.10774v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10774v1",
        "summary": "Recognizing if LLM output can be grounded in evidence is central to many\ntasks in NLP: retrieval-augmented generation, summarization, document-grounded\ndialogue, and more. Current approaches to this kind of \"fact-checking\" are\nbased on verifying each piece of a model generation against potential evidence\nusing an LLM. However, this process can be very computationally expensive,\nrequiring many calls to LLMs to check a single response. In this work, we show\nhow to build small models that have GPT-4-level performance but for 400x lower\ncost. We do this by constructing synthetic training data with GPT-4, which\ninvolves creating realistic yet challenging instances of factual errors via a\nstructured generation procedure. Training on this data teaches models to check\neach fact in the claim and recognize synthesis of information across sentences.\nFor evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact,\ncollected from recent work on fact-checking and grounding LLM generations. Our\nbest system MiniCheck-FT5 (770M parameters) outperforms all systems of\ncomparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for\ndata synthesis, and models.",
        "updated": "2024-04-16 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10774v1"
    },
    {
        "title": "LaDiC: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation?",
        "authors": "Yuchi WangShuhuai RenRundong GaoLinli YaoQingyan GuoKaikai AnJianhong BaiXu Sun",
        "links": "http://arxiv.org/abs/2404.10763v1",
        "entry_id": "http://arxiv.org/abs/2404.10763v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10763v1",
        "summary": "Diffusion models have exhibited remarkable capabilities in text-to-image\ngeneration. However, their performance in image-to-text generation,\nspecifically image captioning, has lagged behind Auto-Regressive (AR) models,\ncasting doubt on their applicability for such tasks. In this work, we revisit\ndiffusion models, highlighting their capacity for holistic context modeling and\nparallel decoding. With these benefits, diffusion models can alleviate the\ninherent limitations of AR methods, including their slow inference speed, error\npropagation, and unidirectional constraints. Furthermore, we identify the prior\nunderperformance of diffusion models stemming from the absence of an effective\nlatent space for image-text alignment, and the discrepancy between continuous\ndiffusion processes and discrete textual data. In response, we introduce a\nnovel architecture, LaDiC, which utilizes a split BERT to create a dedicated\nlatent space for captions and integrates a regularization module to manage\nvarying text lengths. Our framework also includes a diffuser for semantic\nimage-to-text conversion and a Back&Refine technique to enhance token\ninteractivity during inference. LaDiC achieves state-of-the-art performance for\ndiffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2\nCIDEr, demonstrating exceptional performance without pre-training or ancillary\nmodules. This indicates strong competitiveness with AR models, revealing the\npreviously untapped potential of diffusion models in image-to-text generation.",
        "updated": "2024-04-16 17:47:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10763v1"
    },
    {
        "title": "N-Agent Ad Hoc Teamwork",
        "authors": "Caroline WangArrasy RahmanIshan DurugkarElad LiebmanPeter Stone",
        "links": "http://arxiv.org/abs/2404.10740v1",
        "entry_id": "http://arxiv.org/abs/2404.10740v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10740v1",
        "summary": "Current approaches to learning cooperative behaviors in multi-agent settings\nassume relatively restrictive settings. In standard fully cooperative\nmulti-agent reinforcement learning, the learning algorithm controls\n\\textit{all} agents in the scenario, while in ad hoc teamwork, the learning\nalgorithm usually assumes control over only a $\\textit{single}$ agent in the\nscenario. However, many cooperative settings in the real world are much less\nrestrictive. For example, in an autonomous driving scenario, a company might\ntrain its cars with the same learning algorithm, yet once on the road, these\ncars must cooperate with cars from another company. Towards generalizing the\nclass of scenarios that cooperative learning methods can address, we introduce\n$N$-agent ad hoc teamwork, in which a set of autonomous agents must interact\nand cooperate with dynamically varying numbers and types of teammates at\nevaluation time. This paper formalizes the problem, and proposes the\n$\\textit{Policy Optimization with Agent Modelling}$ (POAM) algorithm. POAM is a\npolicy gradient, multi-agent reinforcement learning approach to the NAHT\nproblem, that enables adaptation to diverse teammate behaviors by learning\nrepresentations of teammate behaviors. Empirical evaluation on StarCraft II\ntasks shows that POAM improves cooperative task returns compared to baseline\napproaches, and enables out-of-distribution generalization to unseen teammates.",
        "updated": "2024-04-16 17:13:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10740v1"
    },
    {
        "title": "Bootstrapping Linear Models for Fast Online Adaptation in Human-Agent Collaboration",
        "authors": "Benjamin A NewmanChris PaxtonKris KitaniHenny Admoni",
        "links": "http://arxiv.org/abs/2404.10733v1",
        "entry_id": "http://arxiv.org/abs/2404.10733v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10733v1",
        "summary": "Agents that assist people need to have well-initialized policies that can\nadapt quickly to align with their partners' reward functions. Initializing\npolicies to maximize performance with unknown partners can be achieved by\nbootstrapping nonlinear models using imitation learning over large, offline\ndatasets. Such policies can require prohibitive computation to fine-tune\nin-situ and therefore may miss critical run-time information about a partner's\nreward function as expressed through their immediate behavior. In contrast,\nonline logistic regression using low-capacity models performs rapid inference\nand fine-tuning updates and thus can make effective use of immediate in-task\nbehavior for reward function alignment. However, these low-capacity models\ncannot be bootstrapped as effectively by offline datasets and thus have poor\ninitializations. We propose BLR-HAC, Bootstrapped Logistic Regression for Human\nAgent Collaboration, which bootstraps large nonlinear models to learn the\nparameters of a low-capacity model which then uses online logistic regression\nfor updates during collaboration. We test BLR-HAC in a simulated surface\nrearrangement task and demonstrate that it achieves higher zero-shot accuracy\nthan shallow methods and takes far less computation to adapt online while still\nachieving similar performance to fine-tuned, large nonlinear models. For code,\nplease see our project page https://sites.google.com/view/blr-hac.",
        "updated": "2024-04-16 17:05:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10733v1"
    }
]