[
    {
        "title": "Laplace-HDC: Understanding the geometry of binary hyperdimensional computing",
        "authors": "Saeid PourmandWyatt D. WhitingAlireza AghasiNicholas F. Marshall",
        "links": "http://arxiv.org/abs/2404.10759v1",
        "entry_id": "http://arxiv.org/abs/2404.10759v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10759v1",
        "summary": "This paper studies the geometry of binary hyperdimensional computing (HDC), a\ncomputational scheme in which data are encoded using high-dimensional binary\nvectors. We establish a result about the similarity structure induced by the\nHDC binding operator and show that the Laplace kernel naturally arises in this\nsetting, motivating our new encoding method Laplace-HDC, which improves upon\nprevious methods. We describe how our results indicate limitations of binary\nHDC in encoding spatial information from images and discuss potential\nsolutions, including using Haar convolutional features and the definition of a\ntranslation-equivariant HDC encoding. Several numerical experiments\nhighlighting the improved accuracy of Laplace-HDC in contrast to alternative\nmethods are presented. We also numerically study other aspects of the proposed\nframework such as robustness and the underlying translation-equivariant\nencoding.",
        "updated": "2024-04-16 17:36:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10759v1"
    },
    {
        "title": "Randomized Exploration in Cooperative Multi-Agent Reinforcement Learning",
        "authors": "Hao-Lun HsuWeixin WangMiroslav PajicPan Xu",
        "links": "http://arxiv.org/abs/2404.10728v1",
        "entry_id": "http://arxiv.org/abs/2404.10728v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10728v1",
        "summary": "We present the first study on provably efficient randomized exploration in\ncooperative multi-agent reinforcement learning (MARL). We propose a unified\nalgorithm framework for randomized exploration in parallel Markov Decision\nProcesses (MDPs), and two Thompson Sampling (TS)-type algorithms, CoopTS-PHE\nand CoopTS-LMC, incorporating the perturbed-history exploration (PHE) strategy\nand the Langevin Monte Carlo exploration (LMC) strategy respectively, which are\nflexible in design and easy to implement in practice. For a special class of\nparallel MDPs where the transition is (approximately) linear, we theoretically\nprove that both CoopTS-PHE and CoopTS-LMC achieve a\n$\\widetilde{\\mathcal{O}}(d^{3/2}H^2\\sqrt{MK})$ regret bound with communication\ncomplexity $\\widetilde{\\mathcal{O}}(dHM^2)$, where $d$ is the feature\ndimension, $H$ is the horizon length, $M$ is the number of agents, and $K$ is\nthe number of episodes. This is the first theoretical result for randomized\nexploration in cooperative MARL. We evaluate our proposed method on multiple\nparallel RL environments, including a deep exploration problem (\\textit{i.e.,}\n$N$-chain), a video game, and a real-world problem in energy systems. Our\nexperimental results support that our framework can achieve better performance,\neven under conditions of misspecified transition models. Additionally, we\nestablish a connection between our unified framework and the practical\napplication of federated learning.",
        "updated": "2024-04-16 17:01:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10728v1"
    },
    {
        "title": "How Deep Networks Learn Sparse and Hierarchical Data: the Sparse Random Hierarchy Model",
        "authors": "Umberto TomasiniMatthieu Wyart",
        "links": "http://arxiv.org/abs/2404.10727v1",
        "entry_id": "http://arxiv.org/abs/2404.10727v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10727v1",
        "summary": "Understanding what makes high-dimensional data learnable is a fundamental\nquestion in machine learning. On the one hand, it is believed that the success\nof deep learning lies in its ability to build a hierarchy of representations\nthat become increasingly more abstract with depth, going from simple features\nlike edges to more complex concepts. On the other hand, learning to be\ninsensitive to invariances of the task, such as smooth transformations for\nimage datasets, has been argued to be important for deep networks and it\nstrongly correlates with their performance. In this work, we aim to explain\nthis correlation and unify these two viewpoints. We show that by introducing\nsparsity to generative hierarchical models of data, the task acquires\ninsensitivity to spatial transformations that are discrete versions of smooth\ntransformations. In particular, we introduce the Sparse Random Hierarchy Model\n(SRHM), where we observe and rationalize that a hierarchical representation\nmirroring the hierarchical model is learnt precisely when such insensitivity is\nlearnt, thereby explaining the strong correlation between the latter and\nperformance. Moreover, we quantify how the sample complexity of CNNs learning\nthe SRHM depends on both the sparsity and hierarchical structure of the task.",
        "updated": "2024-04-16 17:01:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10727v1"
    },
    {
        "title": "HiGraphDTI: Hierarchical Graph Representation Learning for Drug-Target Interaction Prediction",
        "authors": "Bin LiuSiqi WuJin WangXin DengAo Zhou",
        "links": "http://arxiv.org/abs/2404.10561v1",
        "entry_id": "http://arxiv.org/abs/2404.10561v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10561v1",
        "summary": "The discovery of drug-target interactions (DTIs) plays a crucial role in\npharmaceutical development. The deep learning model achieves more accurate\nresults in DTI prediction due to its ability to extract robust and expressive\nfeatures from drug and target chemical structures. However, existing deep\nlearning methods typically generate drug features via aggregating molecular\natom representations, ignoring the chemical properties carried by motifs, i.e.,\nsubstructures of the molecular graph. The atom-drug double-level molecular\nrepresentation learning can not fully exploit structure information and fails\nto interpret the DTI mechanism from the motif perspective. In addition,\nsequential model-based target feature extraction either fuses limited\ncontextual information or requires expensive computational resources. To tackle\nthe above issues, we propose a hierarchical graph representation learning-based\nDTI prediction method (HiGraphDTI). Specifically, HiGraphDTI learns\nhierarchical drug representations from triple-level molecular graphs to\nthoroughly exploit chemical information embedded in atoms, motifs, and\nmolecules. Then, an attentional feature fusion module incorporates information\nfrom different receptive fields to extract expressive target features.Last, the\nhierarchical attention mechanism identifies crucial molecular segments, which\noffers complementary views for interpreting interaction mechanisms. The\nexperiment results not only demonstrate the superiority of HiGraphDTI to the\nstate-of-the-art methods, but also confirm the practical ability of our model\nin interaction interpretation and new DTI discovery.",
        "updated": "2024-04-16 13:35:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10561v1"
    },
    {
        "title": "Analytical Approximation of the ELBO Gradient in the Context of the Clutter Problem",
        "authors": "Roumen Nikolaev Popov",
        "links": "http://arxiv.org/abs/2404.10550v1",
        "entry_id": "http://arxiv.org/abs/2404.10550v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10550v1",
        "summary": "We propose an analytical solution for approximating the gradient of the\nEvidence Lower Bound (ELBO) in variational inference problems where the\nstatistical model is a Bayesian network consisting of observations drawn from a\nmixture of a Gaussian distribution embedded in unrelated clutter, known as the\nclutter problem. The method employs the reparameterization trick to move the\ngradient operator inside the expectation and relies on the assumption that,\nbecause the likelihood factorizes over the observed data, the variational\ndistribution is generally more compactly supported than the Gaussian\ndistribution in the likelihood factors. This allows efficient local\napproximation of the individual likelihood factors, which leads to an\nanalytical solution for the integral defining the gradient expectation. We\nintegrate the proposed gradient approximation as the expectation step in an EM\n(Expectation Maximization) algorithm for maximizing ELBO and test against\nclassical deterministic approaches in Bayesian inference, such as the Laplace\napproximation, Expectation Propagation and Mean-Field Variational Inference.\nThe proposed method demonstrates good accuracy and rate of convergence together\nwith linear computational complexity.",
        "updated": "2024-04-16 13:19:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10550v1"
    }
]