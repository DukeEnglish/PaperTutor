[
    {
        "title": "MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents",
        "authors": "Liyan TangPhilippe LabanGreg Durrett",
        "links": "http://arxiv.org/abs/2404.10774v1",
        "entry_id": "http://arxiv.org/abs/2404.10774v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10774v1",
        "summary": "Recognizing if LLM output can be grounded in evidence is central to many\ntasks in NLP: retrieval-augmented generation, summarization, document-grounded\ndialogue, and more. Current approaches to this kind of \"fact-checking\" are\nbased on verifying each piece of a model generation against potential evidence\nusing an LLM. However, this process can be very computationally expensive,\nrequiring many calls to LLMs to check a single response. In this work, we show\nhow to build small models that have GPT-4-level performance but for 400x lower\ncost. We do this by constructing synthetic training data with GPT-4, which\ninvolves creating realistic yet challenging instances of factual errors via a\nstructured generation procedure. Training on this data teaches models to check\neach fact in the claim and recognize synthesis of information across sentences.\nFor evaluation, we unify pre-existing datasets into a benchmark LLM-AggreFact,\ncollected from recent work on fact-checking and grounding LLM generations. Our\nbest system MiniCheck-FT5 (770M parameters) outperforms all systems of\ncomparable size and reaches GPT-4 accuracy. We release LLM-AggreFact, code for\ndata synthesis, and models.",
        "updated": "2024-04-16 17:59:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10774v1"
    },
    {
        "title": "LaDiC: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation?",
        "authors": "Yuchi WangShuhuai RenRundong GaoLinli YaoQingyan GuoKaikai AnJianhong BaiXu Sun",
        "links": "http://arxiv.org/abs/2404.10763v1",
        "entry_id": "http://arxiv.org/abs/2404.10763v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10763v1",
        "summary": "Diffusion models have exhibited remarkable capabilities in text-to-image\ngeneration. However, their performance in image-to-text generation,\nspecifically image captioning, has lagged behind Auto-Regressive (AR) models,\ncasting doubt on their applicability for such tasks. In this work, we revisit\ndiffusion models, highlighting their capacity for holistic context modeling and\nparallel decoding. With these benefits, diffusion models can alleviate the\ninherent limitations of AR methods, including their slow inference speed, error\npropagation, and unidirectional constraints. Furthermore, we identify the prior\nunderperformance of diffusion models stemming from the absence of an effective\nlatent space for image-text alignment, and the discrepancy between continuous\ndiffusion processes and discrete textual data. In response, we introduce a\nnovel architecture, LaDiC, which utilizes a split BERT to create a dedicated\nlatent space for captions and integrates a regularization module to manage\nvarying text lengths. Our framework also includes a diffuser for semantic\nimage-to-text conversion and a Back&Refine technique to enhance token\ninteractivity during inference. LaDiC achieves state-of-the-art performance for\ndiffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2\nCIDEr, demonstrating exceptional performance without pre-training or ancillary\nmodules. This indicates strong competitiveness with AR models, revealing the\npreviously untapped potential of diffusion models in image-to-text generation.",
        "updated": "2024-04-16 17:47:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10763v1"
    },
    {
        "title": "Deep Learning and LLM-based Methods Applied to Stellar Lightcurve Classification",
        "authors": "Yu-Yang LiYu BaiCunshi WangMengwei QuZiteng LuRoberto SoriaJifeng Liu",
        "links": "http://arxiv.org/abs/2404.10757v1",
        "entry_id": "http://arxiv.org/abs/2404.10757v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10757v1",
        "summary": "Light curves serve as a valuable source of information on stellar formation\nand evolution. With the rapid advancement of machine learning techniques, it\ncan be effectively processed to extract astronomical patterns and information.\nIn this study, we present a comprehensive evaluation of deep-learning and large\nlanguage model (LLM) based models for the automatic classification of variable\nstar light curves, based on large datasets from the Kepler and K2 missions.\nSpecial emphasis is placed on Cepheids, RR Lyrae, and eclipsing binaries,\nexamining the influence of observational cadence and phase distribution on\nclassification precision. Employing AutoDL optimization, we achieve striking\nperformance with the 1D-Convolution+BiLSTM architecture and the Swin\nTransformer, hitting accuracies of 94\\% and 99\\% correspondingly, with the\nlatter demonstrating a notable 83\\% accuracy in discerning the elusive Type II\nCepheids-comprising merely 0.02\\% of the total dataset.We unveil StarWhisper\nLightCurve (LC), an innovative Series comprising three LLM-based models: LLM,\nmultimodal large language model (MLLM), and Large Audio Language Model (LALM).\nEach model is fine-tuned with strategic prompt engineering and customized\ntraining methods to explore the emergent abilities of these models for\nastronomical data. Remarkably, StarWhisper LC Series exhibit high accuracies\naround 90\\%, significantly reducing the need for explicit feature engineering,\nthereby paving the way for streamlined parallel data processing and the\nprogression of multifaceted multimodal models in astronomical applications. The\nstudy furnishes two detailed catalogs illustrating the impacts of phase and\nsampling intervals on deep learning classification accuracy, showing that a\nsubstantial decrease of up to 14\\% in observation duration and 21\\% in sampling\npoints can be realized without compromising accuracy by more than 10\\%.",
        "updated": "2024-04-16 17:35:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10757v1"
    },
    {
        "title": "Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study",
        "authors": "Shusheng XuWei FuJiaxuan GaoWenjie YeWeilin LiuZhiyu MeiGuangju WangChao YuYi Wu",
        "links": "http://arxiv.org/abs/2404.10719v1",
        "entry_id": "http://arxiv.org/abs/2404.10719v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10719v1",
        "summary": "Reinforcement Learning from Human Feedback (RLHF) is currently the most\nwidely used method to align large language models (LLMs) with human\npreferences. Existing RLHF methods can be roughly categorized as either\nreward-based or reward-free. Novel applications such as ChatGPT and Claude\nleverage reward-based methods that first learn a reward model and apply\nactor-critic algorithms, such as Proximal Policy Optimization (PPO). However,\nin academic benchmarks, state-of-the-art results are often achieved via\nreward-free methods, such as Direct Preference Optimization (DPO). Is DPO truly\nsuperior to PPO? Why does PPO perform poorly on these benchmarks? In this\npaper, we first conduct both theoretical and empirical studies on the\nalgorithmic properties of DPO and show that DPO may have fundamental\nlimitations. Moreover, we also comprehensively examine PPO and reveal the key\nfactors for the best performances of PPO in fine-tuning LLMs. Finally, we\nbenchmark DPO and PPO across various a collection of RLHF testbeds, ranging\nfrom dialogue to code generation. Experiment results demonstrate that PPO is\nable to surpass other alignment methods in all cases and achieve\nstate-of-the-art results in challenging code competitions.",
        "updated": "2024-04-16 16:51:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10719v1"
    },
    {
        "title": "Dual Modalities of Text: Visual and Textual Generative Pre-training",
        "authors": "Yekun ChaiQingyi LiuJingwu XiaoShuohuan WangYu SunHua Wu",
        "links": "http://arxiv.org/abs/2404.10710v1",
        "entry_id": "http://arxiv.org/abs/2404.10710v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10710v1",
        "summary": "Harnessing visual texts represents a burgeoning frontier in the evolution of\nlanguage modeling. In this paper, we introduce a novel pre-training framework\nfor a suite of pixel-based autoregressive language models, pre-training on a\ncorpus of over 400 million documents rendered as RGB images. Our approach is\ncharacterized by a dual-modality training regimen, engaging both visual data\nthrough next patch prediction with a regression head and textual data via next\ntoken prediction with a classification head. This study is particularly focused\non investigating the synergistic interplay between visual and textual\nmodalities of language. Our comprehensive evaluation across a diverse array of\nbenchmarks reveals that the confluence of visual and textual data substantially\naugments the efficacy of pixel-based language models. Notably, our findings\nshow that a unidirectional pixel-based model, devoid of textual data during\ntraining, can match the performance levels of advanced bidirectional\npixel-based models on various language understanding benchmarks. This work\nhighlights the considerable untapped potential of integrating visual and\ntextual information for language modeling purposes. We will release our code,\ndata, and checkpoints to inspire further research advancement.",
        "updated": "2024-04-16 16:36:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10710v1"
    }
]