[
    {
        "title": "COMBO: Compositional World Models for Embodied Multi-Agent Cooperation",
        "authors": "Hongxin ZhangZeyuan WangQiushi LyuZheyuan ZhangSunli ChenTianmin ShuYilun DuChuang Gan",
        "links": "http://arxiv.org/abs/2404.10775v1",
        "entry_id": "http://arxiv.org/abs/2404.10775v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10775v1",
        "summary": "In this paper, we investigate the problem of embodied multi-agent\ncooperation, where decentralized agents must cooperate given only partial\negocentric views of the world. To effectively plan in this setting, in contrast\nto learning world dynamics in a single-agent scenario, we must simulate world\ndynamics conditioned on an arbitrary number of agents' actions given only\npartial egocentric visual observations of the world. To address this issue of\npartial observability, we first train generative models to estimate the overall\nworld state given partial egocentric observations. To enable accurate\nsimulation of multiple sets of actions on this world state, we then propose to\nlearn a compositional world model for multi-agent cooperation by factorizing\nthe naturally composable joint actions of multiple agents and compositionally\ngenerating the video. By leveraging this compositional world model, in\ncombination with Vision Language Models to infer the actions of other agents,\nwe can use a tree search procedure to integrate these modules and facilitate\nonline cooperative planning. To evaluate the efficacy of our methods, we create\ntwo challenging embodied multi-agent long-horizon cooperation tasks using the\nThreeDWorld simulator and conduct experiments with 2-4 agents. The results show\nour compositional world model is effective and the framework enables the\nembodied agents to cooperate efficiently with different agents across various\ntasks and an arbitrary number of agents, showing the promising future of our\nproposed framework. More videos can be found at\nhttps://vis-www.cs.umass.edu/combo/.",
        "updated": "2024-04-16 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10775v1"
    },
    {
        "title": "Gaussian Opacity Fields: Efficient and Compact Surface Reconstruction in Unbounded Scenes",
        "authors": "Zehao YuTorsten SattlerAndreas Geiger",
        "links": "http://arxiv.org/abs/2404.10772v1",
        "entry_id": "http://arxiv.org/abs/2404.10772v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10772v1",
        "summary": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated impressive novel view\nsynthesis results, while allowing the rendering of high-resolution images in\nreal-time. However, leveraging 3D Gaussians for surface reconstruction poses\nsignificant challenges due to the explicit and disconnected nature of 3D\nGaussians. In this work, we present Gaussian Opacity Fields (GOF), a novel\napproach for efficient, high-quality, and compact surface reconstruction in\nunbounded scenes. Our GOF is derived from ray-tracing-based volume rendering of\n3D Gaussians, enabling direct geometry extraction from 3D Gaussians by\nidentifying its levelset, without resorting to Poisson reconstruction or TSDF\nfusion as in previous work. We approximate the surface normal of Gaussians as\nthe normal of the ray-Gaussian intersection plane, enabling the application of\nregularization that significantly enhances geometry. Furthermore, we develop an\nefficient geometry extraction method utilizing marching tetrahedra, where the\ntetrahedral grids are induced from 3D Gaussians and thus adapt to the scene's\ncomplexity. Our evaluations reveal that GOF surpasses existing 3DGS-based\nmethods in surface reconstruction and novel view synthesis. Further, it\ncompares favorably to, or even outperforms, neural implicit methods in both\nquality and speed.",
        "updated": "2024-04-16 17:57:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10772v1"
    },
    {
        "title": "RapidVol: Rapid Reconstruction of 3D Ultrasound Volumes from Sensorless 2D Scans",
        "authors": "Mark C. EidPak-Hei YeungMadeleine K. WyburdJoão F. HenriquesAna I. L. Namburete",
        "links": "http://arxiv.org/abs/2404.10766v1",
        "entry_id": "http://arxiv.org/abs/2404.10766v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10766v1",
        "summary": "Two-dimensional (2D) freehand ultrasonography is one of the most commonly\nused medical imaging modalities, particularly in obstetrics and gynaecology.\nHowever, it only captures 2D cross-sectional views of inherently 3D anatomies,\nlosing valuable contextual information. As an alternative to requiring costly\nand complex 3D ultrasound scanners, 3D volumes can be constructed from 2D scans\nusing machine learning. However this usually requires long computational time.\nHere, we propose RapidVol: a neural representation framework to speed up\nslice-to-volume ultrasound reconstruction. We use tensor-rank decomposition, to\ndecompose the typical 3D volume into sets of tri-planes, and store those\ninstead, as well as a small neural network. A set of 2D ultrasound scans, with\ntheir ground truth (or estimated) 3D position and orientation (pose) is all\nthat is required to form a complete 3D reconstruction. Reconstructions are\nformed from real fetal brain scans, and then evaluated by requesting novel\ncross-sectional views. When compared to prior approaches based on fully\nimplicit representation (e.g. neural radiance fields), our method is over 3x\nquicker, 46% more accurate, and if given inaccurate poses is more robust.\nFurther speed-up is also possible by reconstructing from a structural prior\nrather than from scratch.",
        "updated": "2024-04-16 17:50:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10766v1"
    },
    {
        "title": "RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting",
        "authors": "Ashkan MirzaeiRiccardo De LutioSeung Wook KimDavid AcunaJonathan KellySanja FidlerIgor GilitschenskiZan Gojcic",
        "links": "http://arxiv.org/abs/2404.10765v1",
        "entry_id": "http://arxiv.org/abs/2404.10765v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10765v1",
        "summary": "Neural reconstruction approaches are rapidly emerging as the preferred\nrepresentation for 3D scenes, but their limited editability is still posing a\nchallenge. In this work, we propose an approach for 3D scene inpainting -- the\ntask of coherently replacing parts of the reconstructed scene with desired\ncontent. Scene inpainting is an inherently ill-posed task as there exist many\nsolutions that plausibly replace the missing content. A good inpainting method\nshould therefore not only enable high-quality synthesis but also a high degree\nof control. Based on this observation, we focus on enabling explicit control\nover the inpainted content and leverage a reference image as an efficient means\nto achieve this goal. Specifically, we introduce RefFusion, a novel 3D\ninpainting method based on a multi-scale personalization of an image inpainting\ndiffusion model to the given reference view. The personalization effectively\nadapts the prior distribution to the target scene, resulting in a lower\nvariance of score distillation objective and hence significantly sharper\ndetails. Our framework achieves state-of-the-art results for object removal\nwhile maintaining high controllability. We further demonstrate the generality\nof our formulation on other downstream tasks such as object insertion, scene\noutpainting, and sparse view reconstruction.",
        "updated": "2024-04-16 17:50:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10765v1"
    },
    {
        "title": "LaDiC: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation?",
        "authors": "Yuchi WangShuhuai RenRundong GaoLinli YaoQingyan GuoKaikai AnJianhong BaiXu Sun",
        "links": "http://arxiv.org/abs/2404.10763v1",
        "entry_id": "http://arxiv.org/abs/2404.10763v1",
        "pdf_url": "http://arxiv.org/pdf/2404.10763v1",
        "summary": "Diffusion models have exhibited remarkable capabilities in text-to-image\ngeneration. However, their performance in image-to-text generation,\nspecifically image captioning, has lagged behind Auto-Regressive (AR) models,\ncasting doubt on their applicability for such tasks. In this work, we revisit\ndiffusion models, highlighting their capacity for holistic context modeling and\nparallel decoding. With these benefits, diffusion models can alleviate the\ninherent limitations of AR methods, including their slow inference speed, error\npropagation, and unidirectional constraints. Furthermore, we identify the prior\nunderperformance of diffusion models stemming from the absence of an effective\nlatent space for image-text alignment, and the discrepancy between continuous\ndiffusion processes and discrete textual data. In response, we introduce a\nnovel architecture, LaDiC, which utilizes a split BERT to create a dedicated\nlatent space for captions and integrates a regularization module to manage\nvarying text lengths. Our framework also includes a diffuser for semantic\nimage-to-text conversion and a Back&Refine technique to enhance token\ninteractivity during inference. LaDiC achieves state-of-the-art performance for\ndiffusion-based methods on the MS COCO dataset with 38.2 BLEU@4 and 126.2\nCIDEr, demonstrating exceptional performance without pre-training or ancillary\nmodules. This indicates strong competitiveness with AR models, revealing the\npreviously untapped potential of diffusion models in image-to-text generation.",
        "updated": "2024-04-16 17:47:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.10763v1"
    }
]