LAPLACE-HDC: UNDERSTANDING THE GEOMETRY OF BINARY
HYPERDIMENSIONAL COMPUTING
SAEIDPOURMAND†,WYATTDWHITING†*,ALIREZAAGHASI,ANDNICHOLASF.MARSHALL
Abstract. This paper studies the geometry of binary hyperdimensional computing (HDC), a com-
putational scheme in which data are encoded using high-dimensional binary vectors. We establish a
resultaboutthesimilaritystructureinducedbytheHDCbindingoperatorandshowthattheLaplace
kernel naturally arises in this setting, motivating our new encoding method Laplace-HDC, which im-
proves upon previous methods. We describe how our results indicate limitations of binary HDC in
encoding spatial information from images and discuss potential solutions, including using Haar con-
volutional features and the definition of a translation-equivariant HDC encoding. Several numerical
experiments highlighting the improved accuracy of Laplace-HDC in contrast to alternative methods
arepresented. Wealsonumericallystudyotheraspectsoftheproposedframeworksuchasrobustness
andtheunderlyingtranslation-equivariantencoding.
1. Introduction
Hyperdimensional computing (HDC) is a computational paradigm rooted in cognitive science and
inspired by the operation of the brain. With billions of neurons and trillions of synapses, the human
brainexhibitsstatesakintohigh-dimensionalarrays. Unlikeconventionalmachinelearningmodelsthat
work with floating-point operations, the brain’s processes engage in simpler “arithmetic” but across
significantly higher dimensions (such as the operations in the cerebral cortex). HDC aims to mimic
the brain’s operation by encoding data with high-dimensional vectors, called hypervectors, while using
simple operations, such as the XOR operation. Hypervectors are often defined randomly or pseudo-
randomly and can have entries that are binary, integer, real, or complex [18]; however, in this paper,
we restrict our attention to binary HDC models, which are the most common in practice. In contrast
with typical floating-point operations on data, the simplicity of binary operations makes binary HDC
computationally straightforward and amenable to hardware-level optimization [5].
Similar to the cognitive operations of the brain, HDC is robust to noise, heavily distributable, inter-
pretable, and energy efficient [10]. Additionally, HDC models can undergo single-pass training, where a
modelistrainedbyprocessingeachsampleinthedatasetonlyonce. Thesimplicityofarithmetictasks
and their parallel nature facilitate rapid inference for these models. Thanks to these attributes, HDC
is a well-suited framework for the Internet of Things (IoT) and edge devices [13, 17], where resilience
to noise and straightforward computations hold significant importance. Despite the simple underlying
arithmetic, HDC models are considered across various complex tasks, such as speech recognition [7],
written language classification [12], DNA pattern matching [14], robotics [21], image description tasks
[20, 22], and low energy computing [1, 3, 8, 11].
A significant challenge associated with HDC models is their relatively low accuracy. For instance,
as demonstrated in the experiments section below, standard HDC models attain an average accuracy
rate of 82% on the MNIST handwritten digit classification task, while several variants of deep neural
networks can achieve accuracies above 99.5% [2, 6, 9]. Standard HDC modeling consists of three main
steps: hyperdimensionalembedding,single-passtraining,andinference. Effortstoenhancetheaccuracy
Key words and phrases. Hyperdimensionalcomputing,Laplacekernel,efficientcomputing,kernelmethods.
†Theseauthorscontributedequally. *Correspondingauthor.
1
4202
rpA
61
]GL.sc[
1v95701.4042:viXra2 S.POURMAND,W.D.WHITING,A.AGHASI,ANDN.F.MARSHALL
of these models typically involve either refining one of these steps or proposing additional steps in the
pipeline. Forexample,astandardHDCmodelusesarecord-basedencoding,whichinvolvesthebinding
and bundling of random hypervectors associated with the feature positions and feature values, while to
boost the accuracy for temporal or spatial data, an N-gram based encoding may be considered, where
featurepositionsareencodedintothehypervectorsthroughrotationalpermutations[4]. Asanexample
ofchangingthenumberofstepsintheprocess,OnlineHD[5]isanHDCframeworkwhereanadditional
retrainingphaseisintegratedintothemodelingpipeline. Thisaugmentationbooststhemodelaccuracy
bydiscardingthemispredictedqueriesfromthecorrespondingmispredictedclassesandaddingthemto
the correct class.
1.1. Motivation. In this paper, we are primarily motivated by the work of Yu et al. [25], which, in
contrast to previous works that mainly focused on using either deterministic hypervectors or random
hypervectors with i.i.d. entries, considers constructions of random hypervectors with a prescribed co-
variancestructure. TheyrefertotheirapproachasRFF-HDCandempiricallyshowthatitoutperforms
previous HDC schemes. More precisely, given a desired covariance structure K ∈ Rm×m, [25] uses the
following algorithm to construct a matrix V ∈{−1,+1}N×m of m hypervectors.
Require: Similarity matrix K ∈Rn×n, hyperdimension N
W
←sin(cid:0)πK(cid:1)
(where the function sin is applied entrywise)
2
USUT ←W (eigendecomposition)
Generate G∈RN×m with i.i.d. standard Gaussian entries
V ←sign(GS1/2U) (where sign is applied entrywise and S sets negative entries to 0).
+ +
return V ∈{−1,+1}N×m
When S = S , that is, when W is positive semi-definite, it follows that V⊤V/N = K, see §1.4 for a
+
moreprecisestatementandmathematicaldescription. TheassumptionthatW ispositivesemi-definite
constrains the types of similarities that can be achieved using this algorithm. However, [25] shows that
some restriction is necessary by proving that there are covariance structures K which are impossible to
realize using binary hypervectors.
The current paper builds upon [25] by studying the geometry of HDC encodings resulting from con-
structions using hypervectors with a covariance structure. More precisely, we consider the similarity
structure of the embedding space induced by the HDC binding operation, see (1). We show that the
Laplace kernel naturally arises in this context, and our results provide heuristics for choosing effective
distributions of hypervectors. Moreover, we consider several modifications to the HDC pipeline, which
further boost the accuracy of HDC models. We demonstrate theoretically and empirically how spatial
information for images is lost in the similarity structure of certain HDC encoding schemes and present
methods of retaining this information, including the definition of a translation-equivariant HDC en-
coding scheme. In addition to conducting empirical experiments to assess the proposed framework’s
performancecomparedtostate-of-the-arttechniques,mathematicaltoolsareusedtoexploretheoretical
aspects.
We emphasize that some models we explore (as with previous work) involve using floating-point op-
erationsduringtheconstructionofthehypervectorsortrainingstagesofthemodels. Wewillemphasize
when this is the case. Moreover, as we will discuss, these models have variants that can ultimately be
fully represented and operated in a binary mode for inference, and results for binary inference will be
presented.
1.2. Preliminaries and Notation. While implementations of binary HDC use vectors x ∈ {0,1}N
(for some large N on the order of N = 104) equipped with entrywise XOR (denoted ⊕), we may
conceptualizethesevectorsasbeingin{−1,+1}N equippedwithentrywiseproduct(denoted⊙). TheseLAPLACE-HDC 3
two representations are isomorphic: if ϕ:{0,1}N →{−1,+1}N by x(cid:55)→ϕ(x)=1−2x then
ϕ(x(i)⊕y(i))=1−2(x(i)⊕y(i))=ϕ(x(i))⊙ϕ(y(i)),
for all i = 1,...,N, where x(i) denotes the i-th entry of x. In this paper, we use the ({−1,+1}N,⊙)
representation of binary HDC schemes.
For a given hypervector u ∈ {−1,+1}N, we denote its k-th entry by u(k). We write e to denote
k
the k-th standard basis vector whose k-th entry is equal to 1 and which is zero elsewhere. We use the
convention that hypervectors are column vectors such that the inner product can be expressed by
N
(cid:88)
u⊤v = u(k)v(k),
k=1
where u⊤ denotes the transpose of u, and write u⊙v to denote the entrywise product
(u⊙v)(k)=u(k)v(k),
for k =1,...,N. Given a set of d hypervectors v ,...,v , let
1 d
d
(cid:75)
v =v ⊙···⊙v ,
j 1 d
j=1
denote the entrywise product over all vectors in the set. For a matrix A ∈ RN×N, we write ∥A∥ to
0
denote the number of nonzero entries of A
∥A∥ :=#{(i,j)∈{1,...,N}2 :A(i,j)̸=0},
0
where A(i,j) denotes the (i,j)-th entry of A, and # denotes the counting measure.
1.3. Defining an Embedding. Assume that data X ⊂ {1,...,m}d are given, that is, each x ∈ X is
a d-dimensional vector whose entries are integers in the set {1,...,m}. Let m hypervectors
v ,...,v ∈{−1,+1}N,
1 m
be given (see §1.4 for a discussion of constructing these hypervectors). Further, let P be a set d
permutation matrices of size N ×N
P ={Π ,Π ,...,Π };
1 2 d
examples of families of permutation matrices of interest are discussed below. The binding operation
which maps X →{−1,+1}N is defined by
d
(cid:75)
(1) x(cid:55)→ψ = Π v .
x i x(i)
i=1
Fortheencodingψ tobemeaningful,someassumptionsmustbeimposedonthepermutationmatrices.
x
We make the following trace-orthogonality assumption.
Assumption 1.1 (Trace-orthogonal family of permutations). We say that a family of permutations
P ={Π ,Π ,...,Π } is trace-orthogonal if
1 2 d
(cid:16) (cid:17)
(2) ⟨Π ,Π ⟩=Tr Π⊤Π =0, ∀i,i′ ∈{1,...,d}, i̸=i′.
i i′ i i′
Itisstraightforwardtoconstructafamilyoftrace-orthogonalpermutationsusingcyclicshifts(under
the necessary assumption that d≤N).4 S.POURMAND,W.D.WHITING,A.AGHASI,ANDN.F.MARSHALL
Remark 1.1 (1D-Cyclicfamily). Fori∈{1,...,d}letT1D-Cyclic denotetheN×N permutationmatrix,
i
which acts on v ∈{−1,+1}N by
(3) (T1D-Cyclicv)(i′)=v(i+i′), for i′ ∈{1,...,N},
i
where the addition i+i′ is taken modulo N. That is, T1D-Cyclic can be defined entrywise by
i
(cid:26) 1 if j =j′+i mod N
T1D-Cyclic(j,j′)=
i 0 otherwise.
When i ̸= i′ and d ≤ N, the support of T1D-Cyclic and T1D-Cyclic are disjoint so the trace-orthogonal
i i′
property holds.
Remark 1.2 (1D-Block Cyclic family). Suppose that N = dM for some positive integer M. Then,
another family of trace-orthogonal permutations matrices {T1D-Block :i∈{1,...,d}} can be defined by
i
their action on v ∈{−1,+1}d×M by
(4) (T1D-Blockv)(i′,k)=v(i+i′,k), for (i′,k)∈{1,...,d}×{1,...,M},
i
where the addition i+i′ is taken modulo d. It is straightforward to verify that this 1D-Block Cyclic
family is also trace-orthogonal.
1.4. ConstructingHypervectors. WechoosehypervectorsusingthemethodofYuetal. [25]outlined
in§1.1above. Inthefollowing,wedescribethisconstructionindetailandproviderelatedmathematical
preliminaries. Given an affinity matrix K ∈Rm×m, the goal is to construct hypervectors v ,...,v ∈
1 m
{−1,+1}N such that
v⊤v
E i j =K(i,j).
N
Recall Grothendieck’s identity (see, for example, [24, page 63]).
Lemma 1.1 (Grothendieck’s identity). Let g be an n-dimensional vector with i.i.d. random standard
Gaussian entries. Then, for any fixed vectors u,v ∈Sn−1, we have
2
E(sign(g⊤u)sign(g⊤v))= arcsin(u⊤v),
π
where Sn−1 ={x∈Rn :∥x∥ =1}.
2
Suppose that an affinity kernel matrix K ∈Rm×m is given, and define W ∈Rm×m by
(cid:16)π (cid:17)
(5) W(i,j)=sin K(i,j) ,
2
for i,j = 1,...,m. The construction is effective when the following assumption, which restricts the
possible choices of K, is satisfied.
Assumption 1.2 (Admissible affinity kernel). We say that an affinity kernel K ∈Rm×m is admissible
if the matrix W defined by (5) is symmetric positive semi-definite and K(i,i)=1 for all i=1,...,m.
An example of a family of admissible affinity kernels is provided in Corollary 1.1; also see §1.2. For
now, we proceed under the assumption that W is a symmetric positive semi-definite matrix, which
implies W can be decomposed as
W =U⊤U,
where U is a real-valued n×n matrix. Note that if W is not positive semi-definite, it is possible to
truncatethenegativeeigenvaluestoachieveadecompositionofthisform,see§1.1;however,inthiscase,
the construction will not achieve hypervectors with the covariance structure of K.LAPLACE-HDC 5
Let G ∈ RN×m be a matrix whose entries are independent Gaussian random variables with mean 0
and variance 1. Define the matrix V ∈{−1,+1}N×m by
V =sign(GU),
where sign denotes the entrywise sign function with the convention that 0 has sign +1. Let v denote
k
the k-th column of V. Then, the hypervectors
v ,...,v ∈{−1,+1}N,
1 m
have the desired covariance structure in expectation. Indeed, we claim that
(6) Ev (k)v (k)=K(i,j), for all k ∈{1,...,N},
i j
which in turn implies
v⊤v
(7) E i j =K(i,j).
N
To show (6), we note that by the definition of v we have
j
Ev (k)v (k)=Esign(g⊤u )sign(g⊤u ),
i j k i k j
where g⊤ is the k-th row of G and u is the j-th column of U. Recall that in the definition of an
k j
admissible kernel, we assume K(i,i)=1. It follows that
(cid:16)π (cid:17)
∥u ∥2 =u⊤u =W(i,i)=sin K(i,i) =1,
i 2 i i 2
for i=1,...,m. Thus, Grothendieck’s Identity (see Lemma 1.1 above) can be applied to deduce that
2
(8) Esign(g⊤u )sign(g⊤u )= arcsin(u⊤u ).
k i k j π i j
By the definition of U we have
2 2
arcsin(u⊤u )= arcsin(W(i,j))=K(i,j),
π i j π
where the final equality follows from the definition of W.
1.5. MainAnalyticResult. LetdataX ⊂{1,...,m}dbegiven,andfixahyperdimensionN ≥d. Let
P ={Π ,Π ,...,Π } be a family of N ×N permutation matrices satisfying Assumption 1.1. Assume
1 2 d
that K ∈ Rm×m is an affinity kernel which is admissible in the sense of Assumption 1.2. Construct
v ,...,v ∈ {−1,+1}N using P and K in the procedure described in §1.4. Define the embedding
1 m
ψ ∈{−1,+1}N of a vector x∈X in terms of the binding operation
x
d
(cid:75)
(9) x(cid:55)→ψ = Π v .
x i x(i)
i=1
The following theorem is our main analytic result.
Theorem 1.1. Under the assumptions of §1.5, we have
(10)
S(x,y):=Eψ⊤ xψ
y
=(cid:89)d
K(x(i),y(i)),
N
i=1
and
(cid:32) (cid:33)
ψ⊤ψ 2γ
Var x y ≤ P(1−S(x,y)),
N N26 S.POURMAND,W.D.WHITING,A.AGHASI,ANDN.F.MARSHALL
where for the set P ={Π ,Π ,...,Π }:
1 2 d
(cid:13) (cid:13)
(cid:13)(cid:88)d (cid:88)d (cid:13)
(11) γ =(cid:13) Π Π⊤(cid:13) .
P (cid:13) i i′(cid:13)
(cid:13) (cid:13)
i′=1i=1 0
The proof of Theorem 1.1 is given in §A.1. The following corollary states this result for the case
where P is the 1D-Cyclic family of permutation matrices, and K is approximately the Laplace kernel.
Tosimplifytheexposition,wepresentaninformalversionofthisresulthereandprovideamoretechnical
statement in the following section.
Corollary 1.1 (Informal Statement). Let P = {Π ,...,Π } be the 1D-Cyclic family of permutation
1 d
matricesdefinedinRemark1.1,whichsatisfiesAssumption1.1. ItispossibletochooseK approximately
equal to the Laplace kernel K(i,j)≈exp(−λ|i−j|) such that Assumption 1.2 is satisfied. In this case,
(cid:32) (cid:33)
ψ⊤ψ
(12) S(x,y):=E x y ≈exp(−λ∥x−y∥ ).
N 1
Moreover, the variance satisfies
(cid:32) (cid:33)
ψ⊤ψ 4d−2
Var x y ≤ (1−S(x,y)).
N N
Forapreciseversionofthecorollary,seeTheorem1.2below. Interestingly,itisnotpossibletoreplace
the Laplace kernel with the Gaussian kernel in this statement, see §1.6
1.6. Choosing an Admissible Kernel. RecallthatK ∈Rm×m isadmissibleinthesenseofAssump-
tion 1.2 if K(i,i)=1 and if W defined by
(cid:16)π (cid:17)
W(i,j)=sin K(i,j)
2
is positive semi-definite. In §1.6.1, we conduct an informal study of admissible kernels. Subsequently,
in §1.6.2, we state a precise result that describes a family of admissible kernels K and the resulting
α
expected similarity S (x,y).
α
1.6.1. Heuristic Derivation. Given an admissible kernel K, Theorem 1.1 states that the resulting ex-
pected similarity S(x,y) has the form
d
(cid:89)
S(x,y)= K(x(i),y(i)).
i=1
This formula suggests that the values of K(i,j) should all be relatively close to 1 for all i,j since,
otherwise, taking the product will result in values close to zero; this observation motivates the ansatz
K(i,j)=1−F(i,j), where 0≤F(i,j)≤ε,
for some small ε>0. The series expansions
(cid:16)π (cid:17) π2x2
exp(x)=1+x+O(x2), and sin (1−x) =1− +O(x4),
2 8
as x→0 imply that the resulting matrix W satisfies
(cid:16)π (cid:17) (cid:18) π2 (cid:19)
W(i,j)=sin (1−F(i,j)) =exp − F(i,j)2 +O(ε4).
2 8
These calculations motivate choosing F(i,j) such that
(cid:18) π2 (cid:19)
W(i,j)≈exp − F(i,j)2
8LAPLACE-HDC 7
is positive semi-definite. One natural choice is setting F(i,j) = λ|i−j| such that K is approximately
equal to the Laplace kernel K(i,j) ≈ exp(−λ|i−j|) and W is approximately equal to the Gaussian
kernel
W(i,j)≈exp(cid:0) −|i−j|2/(2σ2)(cid:1)
,
where σ = 2λ/π. Both the Laplace kernel and the Gaussian kernel are positive definite kernels, so the
Laplace kernel K is one natural choice to use in this construction.
Interestingly, choosing F(i,j) = |i−j|2/(2σ2) such that K is approximately equal to the Gaussian
kernel K(i,j)≈exp(−λ|i−j|2/(2σ2) results in W of the form
W(i,j)≈exp(cid:0) −γ|i−j|4(cid:1)
,
forγ =π2/(32σ4),whichisnotapositivedefinitekernel. Thus,theGaussiankernelisnotanadmissible
choice of K for this construction.
1.6.2. Precise Description. In the following, we make the informal derivation of the previous section,
which says the Laplace kernel is a natural choice for K more rigorous and general. In particular, we
define a family of admissible kernels and derive the resulting expected similarity kernel.
Lemma 1.2. Fix α,λ>0, and consider the matrix W ∈Rm×m with elements
α
(13) W (i,j)=exp(−λ|i−j|α),
α
for all i,j =1,...,m. Then,
– For each α∈[0,2], W is positive semi-definite
α
– For each α∈(2,∞), W is not positive semi-definite
α
Proof of Lemma 1.2. ThisisadirectimplicationofSchoenberg’sclassicresult. Specifically, Corollary3
of [23] states that exp(−|x|α) is positive definite when 0<α≤2, and fails to become positive definite
when α > 2. This function being positive definite implies that for any selection of x ,...,x ∈ R, the
1 m
matrix with elements exp(−|x −x |α) is positive semi-definite. Narrowing down the choice of the test
i j
points to x =λ1/αi for i=1,...,m validates the claims of the lemma. □
i
Thefollowingtheoremdefinesafamilyofadmissiblekernelsand,motivatedbytheinformalderivation
of§1.6.1,derivestheresultingexpectedsimilaritykernel. Asubsequentremarkdescribestheconnection
to the Laplace kernel derived in the previous section.
Theorem 1.2. Let λ>0 and α∈(0,1]. Define K ∈Rm×m by
α
2 (cid:18) (cid:18) π2 (cid:19)(cid:19)
(14) K (i,j)= arcsin exp − λ|i−j|2α ,
α π 8
fori,j ∈{1,...,m}. Then,K isadmissibleinthesenseofAssumption1.2. Moreover,ifthebandwidth
α
parameter λ is set such that λ|i−j|α ≤ε, then
K (i,j)=exp(−λ|i−j|α)+O(ε2),
α
and the resulting expected similarity kernel S satisfies
α
S (x,y)=exp(−λ∥x−y∥α)(1+O(ε2d)),
α α
as ε→0.
TheproofofTheorem1.2isgivenin§A.2. Inthefollowing,wemakethreeremarksaboutthisresult.8 S.POURMAND,W.D.WHITING,A.AGHASI,ANDN.F.MARSHALL
Remark 1.3 (Laplace kernel). When α=1
K (i,j)=exp(−λ|i−j|)+O(ε2),
α
is the Laplace kernel, up to lower order terms, and
S (i,j)=exp(−λ∥x−y∥ )(1+O(ε2d)),
α 1
which recovers the informal motivating result from the previous section.
Remark 1.4 (Limitation of similarity structure). The result of Theorem 1.1 suggests a limitation of
binary HDC pipelines that involve binding data x (cid:55)→ ψ and then relying on the similarity structure
x
induced by inner products ψ⊤ψ /N. Namely, the expected similarity S(x,y) is invariant to global
x y
permutations of the elements of the data, that is,
S(x,y)=S(x◦σ,y◦σ),
where (x◦σ)(i) = x(σ(i)) for a permutation σ, see (10). For some data types, such as images, the
orderingofthedataelementsmaycontaininformation. Forexample,agroupofblackpixelsmayindicate
structure, such as a digit, while scattered black pixels may be noise. These spatial relationships can
be captured by using feature extraction methods such as convolutional filters, see §2.4. Alternatively,
the definition of the embedding could be modified so that spatial information is encoded via another
mechanism such as translation-equivariance, see §1.7
Remark 1.5(Settingthebandwidthparameterλ). WhenK isdefinedby(14),theformoftheresulting
α
expected similarity
S (x,y)≈exp(−λ∥x−y∥α),
α α
can be used to set the bandwidth parameter λ. Suppose a data set X = {x }n is given. A typical
i i=1
strategy for kernel methods is setting the bandwidth parameter so that K is invariant to transforma-
α
tions that preserve distances ∥x−y∥ up to a global constant (for example, global scaling of the data).
α
One way to achieve this is by setting
c
(15) λ= ,
median(D)
for some constant c>0, where
D(i,j)=∥x −x ∥α,
i j α
and median(D) is the median of the n2 numbers in the matrix D. When the number of data points n
is large, the matrix D in (15) could be replaced by a matrix D′ corresponding to a subset of the data
to reduce the required computation.
1.7. Translation Equivariant Encoding. Remark 1.4 describes a limitation of the method related
to encoding spatial information. In the following, we describe an alternate way to encode spatial
information by defining a translation equivariant binding operation for images. Suppose that x ∈
{1,...,m}L×L is an L×L image. For simplicity, initially assume N :=L2 (We will subsequently relax
this assumption to handle N > L2). We define a family of permutations P = {T :i,j =1,..., m},
i,j
which act on v ∈{−1,+1}L×L by
(T v)(i′,j′)=v(i+i′,j+j′),
i,j
where the addition i+i′ and j +j′ is taken modulo L. This family satisfies the Trace-Orthogonality
Assumption 1.1. Define the embedding
L
(cid:75)
(16) x(cid:55)→ψ = T v ,
x i,j x(i,j)
i,j=1LAPLACE-HDC 9
wherex(i,j)denotesthe(i,j)-thentryofx. Byconstruction,thisembeddingistranslation-equivariant.
That is,
T ψ =ψ ,
i,j x Ti,jx
see Figure 1.
Image space Embedding space
ψ
T T
i,j i,j
ψ
Figure 1. Communitive diagram for tanslation-equivariance of binding operation (16).
TheequivariantbindingoperationforthecaseN =L2 canbeextendedtoN >L2 whilemaintaining
exact translation-equivariance by using a block-based construction when N =ML2 by taking M copies
of the L×L construction. More precisely, we define the 2D-Block family of permutations matrices
{T2D-Block :i,j ∈{1,...,L}} which act on v ∈{−1,+1}L×L×M by
i,j
(cid:16) (cid:17)
(17) T2D-Blockv (i′,j′,k)=v(i+i′,j+j′,k), for (i′,j′,k)∈{1,...,L}×{1,...,L}×{1,...,M},
i,j
where the addition i+i′ and j+j′ is taken modulo L. Alternatively, one can consider N = M2 > L2
and define {T2D-Cyclic :i,j ∈{1,...,L}}
i,j
(cid:16) (cid:17)
(18) T2D-Cyclicv (i′,j′)=v(i+i′,j+j′), for (i′,j′)∈{1,...,M}2,
i,j
whereadditioni+i′ andj+j′ istakenmoduloM. Whenthe2D-Cyclicfamilyofpermutationsisusedin
thebindingoperation, theresultingencodingistranslation-equivariantwhenignoringboundaryeffects.
We demonstrate an application of the 2D-Cyclic family of permutations to visualize this equivariance
property §2.7.
2. Experiments
We present several numerical experiments on the binary HDC schemes described in this paper and
their limitations and extensions. Code for all presented methods is publically available at:
https://github.com/HDStat/Laplace-HDC
This section is organized as follows. In §2.1, we describe the linear classifiers that we use on the binary
encodings. In§2.2, wepresentresultsforthevanillaversionofLaplace-HDC.In§2.3, wepresentresults
for Laplace-HDC using singular value decomposition (SVD) features. In §2.4, we present results for
Laplace-HDC with Haar convolutional features. In §2.5, we provide a comparison of Laplace-HDC to
other methods. Beyond the model accuracy, we study robustness in §2.6, and conduct experiments
concerning the translation-equivariance property of the proposed encoding scheme in §2.7.10 S.POURMAND,W.D.WHITING,A.AGHASI,ANDN.F.MARSHALL
2.1. Classification Methods. Once the data x ∈ X has been embedded x (cid:55)→ ψ , a classifier may
x
be trained. Each classifier we consider uses an inner product to determine the final class. Suppose the
classes {1,2,...,c} are represented within X, and suppose ψ denotes the class representative for class
i
i, for i=1,2,...,c . We determine the class of y by a simple linear classifier
(cid:16) (cid:17)
(19) class(y)=argmax ψ⊤ψ .
y i
i=1,...,c
Below, we detail four methods we use for determining the class representatives {ψ }.
i
2.1.1. Float and Binary Majority Vote. The majority vote classifiers operate on a simple principle for
classification. First, we describe the binary flavor of this method. Consider the collection
C ={x∈X : class(x)=i},
i
and let #(C ) denote the number of elements in the collection. The representative for class i, denoted
i
ψ , is determined by a majority vote of ψ for x ∈ C . More precisely, we define the Binary Majority
i x i
Vote representative ψ by
i
(cid:40) (cid:80)
+1 ψ (k)>0
ψ (k)= x∈Ci x
i −1 otherwise,
The Float Majority Vote is largely the same, but we relax the condition that ψ must have entries in
i
−1,+1. Instead, the entries of ψ may be floating-point numbers. In this case, the representative ψ is
i i
determined entrywise by class averages
1 (cid:88)
ψ (k)= ψ (k).
i #(C ) x
i
x∈Ci
2.1.2. Float and Binary SGD. Wedefinetwoclassifiers,whichwecallFloatSGDandBinarySGD.The
Float SGD classifier determines the class representatives ψ by optimizing a cross-entropy loss function
i
using stochastic gradient descent; more precisely, we use Adam [15] with a learning rate parameter
α = 0.01, where the model takes ψ and outputs one of the c classes. We perform 3 epochs training
x
passes over the data in X in all experiments. The Binary HDC classifier operates in the same manner,
except during each training epoch, the weights of the model parameter are clamped to be in the range
[−1,1]. Oncealltrainingepochshavebeencompleted;thesignfunctionisappliedtothemodelweights,
making all negative weights −1 and all positive weights +1.
2.2. Laplace-HDC in its Basic Form. In this section, we define a Binary HDC scheme motivated
by Theorem 1.1, Corollary 1.1, and Theorem 1.2, which we call Laplace-HDC. Given a data set X =
{x ,...,x } where x ∈{1,...,m}d, Laplace-HDC consists of the following steps:
1 n i
1. Set the bandwidth parameter λ>0 using Remark 1.5 with α=1.
2. Define the kernel K ∈Rm×m using (14).
3. Construct hypervectors v ,...,v ∈{−1,+1}N using the method detailed in §1.4.
1 m
4. Choose a family of permutation matrices P = {Π ,...,Π } that are Trace-Orthogonal in the
1 d
sense of Assumption 1.1.
5. Encode each x (cid:55)→ψ using the binding operation (9).
j xj
6. Return {ψ ,...,ψ } where ψ ∈{−1,+1}N.
x1 xn xi
By construction, it follows from Theorem 1.2 that
ψ⊤ψ
E xi xj ≈exp(−λ∥x −x ∥ ).
N i j 1
In the following, we demonstrate the utility of Laplace-HDC in an application to image classification.
Weconsiderfourchoicesoftrace-orthogonalfamiliesofpermutationmatrices: 1D-Cyclicshift,1D-BlockLAPLACE-HDC 11
Cyclic, 2D-Block Cyclic, and 2D-Cyclic Shift, which are defined in (3), (4), (17), and (18), respectively.
Classification is performed by determining which class representative ψ gives the largest inner product
i
(19). The class representatives ψ are determined by two different methods: Float SGD and Binary
i
SGD, which are defined in §2.1.2.
In each case, we use the largest possible hyperdimensionality N ≤ 104. For example, the 1D-Block
familyofpermutationsrequiresthatN =dM forsomepositiveintegerM. Theimagedataweconsider
hasdimensiond=282,sowechooseM =⌊104/282⌋=12,whichresultsinN =9408. Whensettingthe
bandwidth parameter λ>0, we use (15) with c=1 except for binary SGD where c=4 provides better
performance, and we estimate the median of the ℓ -distances of the data using 1000 samples selected
1
uniformlyatrandomfromX. Theaccuracyforeachispresentedasthemean±onestandarddeviation
in all cases; see Table 1.
Table 1. Basic Laplace-HDC Performance
1D-Cyclic 2D-Cyclic 1D-Block 2D-Block
Float SGD - Fashion MNIST 86.06±0.74 86.03±0.73 87.86±0.40 87.41±0.15
Binary SGD - Fashion MNIST 83.60±1.35 83.87±0.63 84.72±0.23 83.51±0.57
Float SGD - MNIST 95.46±0.48 95.46±0.51 96.13±0.25 96.14±0.28
Binary SGD - MNIST 93.25±1.09 93.37±1.00 94.43±0.82 94.59±0.44
2.3. Laplace-HDC with SVD Features. Singular value decomposition is a popular pre-processing
toolinpredictivetasksinvolvingnumericalfeatures. Well-knowntechniquessuchasprincipalcomponent
regression use this decomposition to rotate the coordinate system so that the features are uncorrelated
intherotatedsystem. TheprocessnormallyinvolvespopulatingthenumericalfeaturesintoamatrixX
(wherethenumberofcolumnscorrespondstothenumberoffeaturesandthenumberofrowscorresponds
to the number of samples) and then performing a compact SVD of the form X = UΣV⊤ to acquire
the transformation matrix. The new feature matrix takes the form Xˆ = XV, which not only enjoys
uncorrelated features, also may have fewer columns than those in X thanks to the compact nature of
the SVD operation (instead of a full SVD). Truncating small singular values is a manual alternative to
reduce the number of features, which is also capable of denoising the feature matrix. This technique is
incorporated into the HDC pipeline by first mapping the data matrix X to Xˆ and then performing the
hyperdimensional encoding on Xˆ.
To evaluate the effects of SVD transformation, we applied it to the FashionMNIST data and used a
totalof8variationsofpermutationschemesandclassifierpairs. Themeantestaccuracy(inpercentage)
of different classifiers, plus and minus one standard deviation, is computed for 50 independent experi-
mentsafterSVDpreprocessinganddifferentpermutationschemes. TheresultsareavailableinTable2.
The hyperparameters λ and N are set in the same way described in §2.2.
Table 2. Laplace-HDC with SVD features
1D-Cyclic 1D-Block
Float SGD - Fashion MNIST 87.26±0.16 86.67±0.44
Binary SGD - Fashion MNIST 84.40±0.42 83.57±1.13
Float SGD - MNIST 95.63±0.30 96.15±0.30
Binary SGD - MNIST 94.03±0.35 94.66±0.3912 S.POURMAND,W.D.WHITING,A.AGHASI,ANDN.F.MARSHALL
2.4. Laplace-HDC with Haar Convolutional Features. The result of Theorem 1.1, Corollary 1.1,
and Theorem 1.2 suggest a limitation of using the inner product similarity structure of HDC encodings
when applied to images: the spatial relationship between pixels is lost. In images, the spatial relation
between pixels contains meaningful information. For example, a group of black pixels may indicate
structure, such as a digit, while scattered black pixels may be noise. Note that the ℓ -norm is invariant
1
to permutations
d d
(cid:88) (cid:88)
∥x∥ = |x(i)|= |x(σ(i))|=∥x◦σ∥ ,
1 1
i=1 i=1
for any fixed permutation σ of {1,...,d}. Thus, by Theorem 1.1, the expected similarity structure of
the HDC embedding is invariant to a global permutation of the pixels of all the images. One basic way
to encode spatial information is to use convolutional features. As a basic demonstration, we consider
the 9 Haar wavelet matrices of dimension 4×4; see Figure 2.
1.0
0.5
0.0
−0.5
−1.0
Figure 2. Collection of 9 Haar convolution matrices of dimension 4×4.
Convolving these 9 filters of dimension 4×4 with an L×L image with stride s creates
n=9((L−4)/s+1)2
convolutional Haar features, where the stride is the amount of each filter is shifted in each direction
when convolving over the data. Each feature coordinate is mapped to the interval [0,255] by an affine
transformation (determined from the training data), which is rounded to an integer in {0,1,...,255}.
These integer features are used in the same Laplce-HDC methodology described in §2.2; the results are
reported in Table 3.
Table 3. Laplace-HDC with Haar convolutional features
1D-Cyclic 1D-Block
Float SGD - Fashion MNIST 88.67±0.30 87.86±0.40
Binary SGD - Fashion MNIST 86.65±0.36 85.63±0.56
Float SGD - MNIST 96.40±0.28 96.22±0.29
Binary SGD - MNIST 95.17±0.44 94.85±0.50LAPLACE-HDC 13
More generally, using features from a trained convolutional neural network is possible and would
improve the accuracy even further; see §3 for further discussion.
2.5. Comparison to Other Methods. In this section, we implement our proposed framework and
compareittosomeoftherelevantworksintheliterature,suchasRFF-HDC,OnlineHD,and(Extended)
HoloGN.Inthesequel,wefirstbrieflyovervieweachofthesemethodsandthenreporttheirperformance
on standard datasets such as MNIST and FashionMNIST. To present the results in a reliable format,
the experiments are performed 50 times, and mean accuracies along with standard deviation of the
accuracies and the histograms are reported in Figure 3 and Table 4.
RFF-HDC. This method [25] is the most relevant baseline, as it is the basis for our work. While
traditionalhyperdimensionalcomputingmethods(herereferredtoasVanillaHDC)arefast, theysuffer
fromlowpredictionaccuracy. RFF-HDCutilizessimilaritymatricesinasimilarwaytoRandomFourier
Features(RFF)toconstructthehypervectors,whichhelpsoutperformthestate-of-the-artHDCschemes.
A pseudo-code that constructs the hypervectors for RFF-HDC was given in §1.1. Note that all the
floating point operations are performed during the construction of the hypervectors or learning the
models, and ultimately, the models are fully represented and operated in binary mode for inference.
OnlineHD. Performing iterative training rather than single-pass training is one approach to boost the
accuracy of HDC models, although it increases the time complexity and memory usage, which is costly.
Works such as [5] relate the low accuracy of single-pass models to the naive gathering of information
fromallhypervectorsthatbelongtothesameclass. Thisleadstothedominanceofthecommonpattern
whiledownplayingthemoreuncommonpatternsinthedata. OnlineHD[5]ispresentedasasingle-pass
remedy to this problem. Basically, if a hypervector is closely similar to the current state of the class
hypervector, then OnlineHD assigns a small weight to it while updating the model in order to decrease
its effect, and if the hypervector is distant, the weight increases.
Extended HoloGN. Holographic graph neuron (HoloGN) [16] is an approach designed for character
recognition over a dataset of small binary images. HoloGN assigns a randomly generated hypervector
to each pixel. Then, a circular shift occurs if the pixel color is white. After this stage, the remaining
procedure is similar to vanilla HDC bundling (Binary Majority vote, see §2.1.1). An extension of this
approach to operate with non-binary datasets such as MNIST was presented in [19], which is used as a
comparison baseline in our experiments.
The accuracies reported in Table 4 and the histograms depicted in Figure 3 show that Laplace-
HDC with convolutional features can outperform the state-of-the-art techniques in terms of the mean
accuracy. Intermsoftheaccuracystandarddeviation,Laplace-HDCoffersasignificantlylowerdeviation
compared to the RFF-HDC, with which it shares some foundations.
Table 4. The mean test accuracy (in percentage) of different methods discussed in
§2.5plusandminusonestandarddeviation,computedfor50independentexperiments.
The reported Laplace-HDC uses Haar convolutional features, 1D-Cyclic permutations,
and the Binary SGD classifier.
Vanilla HDC Ext. HoloGN OnlineHD RFF-HDC Laplace-HDC
MNIST 82.07±0.17 80.21±0.41 93.32±0.08 93.58±1.21 95.17±0.44
Fashion MNIST 69.07±0.19 62.44±0.19 82.55±0.12 81.19±1.81 86.65±0.36
In the next two sections, we explore some other aspects of the proposed binary HDC beyond the
accuracy.
2.6. Robustness to Corruptions. A notable characteristic of binary HDC encodings are their ro-
bustness to noise: these binary encodings can remain effective even in the presence of corrupted bits.14 S.POURMAND,W.D.WHITING,A.AGHASI,ANDN.F.MARSHALL
8
8
6
6
4
4
2 2
0 0
0.814 0.816 0.818 0.82 0.822 0.824 0.826 0.684 0.686 0.688 0.69 0.692 0.694 0.696 0.698
(a)
6 6
5 5
4 4
3 3
2 2
1 1
0 0
0.79 0.795 0.8 0.805 0.81 0.815 0.618 0.62 0.622 0.624 0.626 0.628 0.63 0.632
(b)
10 7
6
8
5
6 4
4 3
2
2
1
0 0
0.93 0.931 0.932 0.933 0.934 0.935 0.936 0.822 0.824 0.826 0.828 0.83
(c)
5
8
4
6
3
4
2
1 2
0 0
0.9 0.92 0.94 0.96 0.98 0.74 0.76 0.78 0.8 0.82 0.84 0.86 0.88
(d)
7 7
6 6
5 5
4 4
3 3
2 2
1 1
0 0
0.935 0.94 0.945 0.95 0.955 0.96 0.965 0.855 0.86 0.865 0.87 0.875 0.88
(e)
Figure 3. Accuracy histograms of the methods in Table 4 for 50 trials. The mean
accuracy and one-standard deviation interval are shown with dashed lines: (a) Vanilla
HDC, (b) Extended HoloGN, (c) OnlineHD, (d) RFF-HDC, (e) Laplace-HDCLAPLACE-HDC 15
To demonstrate this robustness property for Laplace-HDC, we perform an experiment where the classi-
fication task is stress-tested by randomly corrupting a proportion of the bits of the binary encoding.
In this experiment, for each x ∈ X the corresponding encoded hypervector ψ ∈ {−1,1}N is cor-
x
rupted by flipping k of the N bits of the encoded vector to generate a corrupted hypervector ψ(cid:101) . More
x
precisely, for each x ∈ X we choose a set {i ,...,i } from {1,...,N} independently and uniformly at
1 k
random without replacement and set
ψ(cid:101) x(i j)=−ψ x(i j), for j =1,...,k.
Afterthisstep, theclassofψ(cid:101) isinferredusingaclassifiermodelwhichwastrainedontheuncorrupted
x
data ψ . In this fashion, we are able to determine the degree to which corruption affects classification
x
accuracy. In this experiment, we use the Binary SGD classifier, see §2.1.2. We report results based on
the ratio k/N of corrupted bits to the bits in the encoding. When k/N = 0, the embedding ψ is not
x
altered; themethodandclassificationaccuracyarethesameasreportedin§2.2. Whenk/N =1/2,half
the bits are randomly corrupted, and the corrupted embedding ψ(cid:101) and original embedding ψ become
x x
uncorrelated. Figure 4 shows the degradation pattern of the Laplace-HDC accuracy as a function of
bit error rate. The experiment is performed for the FashionMNIST data for k = 0,2,...,5000, and
hyperparameters λ and N are set in the same way described in §2.2. Each experiment is performed
multiple times, and in addition to the mean accuracy, an uncertainty region of radius three standard
deviations is depicted around the mean accuracy plot. One can see that with almost up to a 25%
bit error rate (which corresponds to 50% of the worst possible corruption), the classification accuracy
confidently maintains a value above 80%.
RatioofRandomlyFlippedBits
Figure 4. The robustness of the proposed HDC formulation to noise: in each hyper-
vector, a portion of the bits are randomly flipped, and the accuracy is evaluated for
the noisy model. The narrow shaded region around the accuracy curve is ±3 times the
standard deviation of the accuracy. One can notice that up to almost 25% bit error
rate, the accuracy does not drop below 80%
2.7. TranslationEquivariance. Inthissection,wedescribehowthe2D-Cyclicfamilyofpermutations
defined in (18) encodes spatial information and leads to interesting visualizations. In particular, we
consider the encoding {1,...,m}L×L →{−1,+1}M×M by
M
(cid:75)
x(cid:55)→ψ = T2D-Cyclicv ,
x i,j x(i,j)
i,j=1
ycaruccAledoM16 S.POURMAND,W.D.WHITING,A.AGHASI,ANDN.F.MARSHALL
which is translation-equivariant up to boundary effects, see §1.7. Consider examples from the Fashion-
MNIST dataset; see Figure 5.
T-shirt/top Trouser Pullover Dress Coat
0 0 0 0 0
1.0
10 10 10 10 10
0.8
20 20 20 20 20
0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0.6
Sandal Shirt Sneaker Bag Ankle boot 0.4
0 0 0 0 0
10 10 10 10 10 0.2
20 20 20 20 20
0.0
0 10 20 0 10 20 0 10 20 0 10 20 0 10 20
Figure 5. One example from each class in Fashion MNSIT dataset
Let z denote the all zero image z(i,j)=0 for i,j =1,...,L, and ψ be the encoding of the all zero
z
image. To visualize the hypervector encodings of these images, we plot
ψ ⊙ψ ,...,ψ ⊙ψ ,
x1 z x10 z
see Figure 6.
T-shirt/top Trouser Pullover Dress Coat
0 0 0 0 0
1.0
20 20 20 20 20
40 40 40 40 40
60 60 60 60 60 0.8
80 80 80 80 80
0 50 0 50 0 50 0 50 0 50 0.6
Sandal Shirt Sneaker Bag Ankle boot 0.4
0 0 0 0 0
20 20 20 20 20
40 40 40 40 40 0.2
60 60 60 60 60
80 80 80 80 80
0.0
0 50 0 50 0 50 0 50 0 50
Figure 6. We plot ψ ⊙ψ for the class example images x ,...,x from Figure 5.
xj z 1 10
Informally speaking, this image consists of translated versions of thresholded versions of the original.
The images are overlapping, which captures some autocorrelation of the image with itself when inner
productsarecomputed. Wenotethatclassificationperformanceseemshigherwhenparametersaretuned
so that there is overlap. The number of images in the HDC encoding is controlled by the bandwidth
parameter λ. To make another visualization, we can look at the class averages of these images; see
Figure 7.LAPLACE-HDC 17
T-shirt/top Trouser Pullover Dress Coat
0 0 0 0 0
1.00
20 20 20 20 20
40 40 40 40 40 0.75
60 60 60 60 60
0.50
80 80 80 80 80
0.25
0 50 0 50 0 50 0 50 0 50
0.00
0 Sandal 0 Shirt 0 Sneaker 0 Bag 0 Ankle boot −0.25
20 20 20 20 20
−0.50
40 40 40 40 40
60 60 60 60 60 −0.75
80 80 80 80 80
−1.00
0 50 0 50 0 50 0 50 0 50
Figure 7. We average 1 (cid:80) ψ ⊙ψ where C is the j-th class, for j =1,...,10.
#(Cj) x∈Cj x z j
3. Discussion
ThispaperintroducesLaplace-HDC,abinaryHDCschememotivatedbythegeometryofhypervector
data encodings. We build upon the work of Yu et al. [25], by considering the inner product structure of
hypervectorencodingsresultingfromthebindingoperationforhypervectorswithacovariancestructure.
We show that the Laplace kernel (rather than the Gaussian kernel) is a natural choice for the covari-
ancestructureofthehypervectorsusedinthisconstruction. Inthiscase,weshowthattheinnerproduct
ofthehypervectorencodingsofdataisrelatedtoanℓ -normLaplacekernelbetweendatapoints,which
1
motivatesamethodforsettingthebandwidthparameterλ>0inthisconstruction. Theseobservations
lead to a practical binary HDC scheme, which we call Laplace-HDC.
Our results also indicate a limitation of binary HDC schemes of this type for image data: the spatial
relationship between pixels is lost. More precisely, our results show that the inner product structure
of hypervector encodings is invariant to global permutations of the data. We demonstrate that when
spatialrelationshipsareencoded,eveninanelementaryway,suchasthroughconvolutionalHaarfeatures
(which are not invariant to global permutations of the pixels), the accuracy of binary HDC improves
for image data.
Wenotethatmorecomplicatedfeatureextractionmethodscouldbeusedtoincreasetheperformance
further. For example, the features derived from the output of one or more layers of a convolutional
neural network trained on image data could be used. The fact that binary HDC schemes of this type
are invariant to global permutations of the pixels can be viewed both as a limitation or a feature of
binding-based binary HDC encoding schemes.
We emphasize that our theoretical results only say that spatial relationships are not encoded in the
inner product structure of the binding operation. It may be possible to recover spatial information via
another method. We illustrate such a method when we define a translation-equivariant binary HDC
encoding scheme for images, which is a potential direction for future work.
We note that the Trace-Orthongal assumption that we make on the families of permutation matrices
we consider may be overly restrictive. We performed some limited experiments using families of permu-
tations, which are each sampled independently and uniformly, which achieved similar accuracy to the
Trace-Orthogonal families of permutation we considered (1D-Cyclic, 1D-Block, 2D-Cyclic, 2D-Block).
However, it should be noted that each of the families of permutations we considered has efficient imple-
mentations that maintain memory locality when encoding batched images. In contrast, performing a
uniformlyrandompermutationisordersofmagnitudeslowerduetoalackofmemorylocality. However,18 S.POURMAND,W.D.WHITING,A.AGHASI,ANDN.F.MARSHALL
there may be pseudo-random permutations that can be efficiently implemented that are interesting to
consider.
Acknowledgements. The authors thank Peter Cowal for useful discussions about the paper.
References
[1] T. Basaklar, Y. Tuncel, S. Y. Narayana, S. Gumussoy, and U. Y. Ogras, Hypervector design for efficient
hyperdimensional computing on edge devices,arXivpreprintarXiv:2103.06709,(2021).
[2] A.Byerly,T.Kalganova,andI.Dear,Noroutingneededbetweencapsules,Neurocomputing,463(2021),pp.545–
553.
[3] Y.-C. Chuang, C.-Y. Chang, and A.-Y. A. Wu, Dynamic hyperdimensional computing for improving accuracy-
energy efficiency trade-offs,in2020IEEEWorkshoponSignalProcessingSystems(SiPS),IEEE,2020,pp.1–5.
[4] L. Ge and K. K. Parhi, Classification using hyperdimensional computing: A review, IEEE Circuits and Systems
Magazine,20(2020),pp.30–47.
[5] A. Herna´ndez-Cano, N. Matsumoto, E. Ping, and M. Imani,Onlinehd: Robust, efficient, and single-pass online
learning using hyperdimensional system, in 2021 Design, Automation, and Test in Europe Conference Exhibition
(DATE),IEEE,2021,pp.56–61.
[6] D. Hirata and N. Takahashi, Ensemble learning in cnn augmented with fully connected subnetworks, IEICE
TRANSACTIONSonInformationandSystems,106(2023),pp.1258–1261.
[7] M.Imani,D.Kong,A.Rahimi,andT.Rosing,Voicehd: Hyperdimensionalcomputingforefficientspeechrecogni-
tion,in2017IEEEInternationalConferenceonRebootingComputing(ICRC),2017,pp.1–8.
[8] M. Imani, J. Morris, J. Messerly, H. Shu, Y. Deng, and T. Rosing, Bric: Locality-based encoding for energy-
efficient brain-inspired hyperdimensional computing,inProceedingsofthe56thAnnualDesignAutomationConfer-
ence2019,2019,pp.1–6.
[9] H.D.Kabir,M.Abdar,A.Khosravi,S.M.J.Jalali,A.F.Atiya,S.Nahavandi,andD.Srinivasan,Spinalnet:
Deep neural network with gradual input,IEEETransactionsonArtificialIntelligence,(2022).
[10] P. Kanerva, Hyperdimensional computing: An introduction to computing in distributed representation with high-
dimensional random vectors,Cognitivecomputation,1(2009),pp.139–159.
[11] G.Karunaratne,M.LeGallo,G.Cherubini,L.Benini,A.Rahimi,andA.Sebastian,In-memoryhyperdimen-
sional computing,NatureElectronics,3(2020),pp.327–337.
[12] G. Karunaratne, A. Rahimi, M. L. Gallo, G. Cherubini, and A. Sebastian, Real-time language recognition
using hyperdimensional computing on phase-change memory array, in 2021 IEEE 3rd International Conference on
ArtificialIntelligenceCircuitsandSystems(AICAS),2021,pp.1–1.
[13] B. Khaleghi, H. Xu, J. Morris, and T. v. Rosing, tiny-hd: Ultra-efficient hyperdimensional computing engine
for iot applications, in 2021 Design, Automation and Test in Europe Conference and Exhibition (DATE), 2021,
pp.408–413.
[14] Y.Kim,M.Imani,N.Moshiri,andT.Rosing,Geniehd: Efficientdnapatternmatchingacceleratorusinghyperdi-
mensional computing,in2020Design,AutomationandTestinEuropeConferenceExhibition(DATE),IEEE,2020,
pp.115–120.
[15] D. P. Kingma and J. Ba,Adam: A method for stochastic optimization,2017.
[16] D.Kleyko,E.Osipov,A.Senior,A.I.Khan,andY.A.S¸ekerciogg˘lu,Holographicgraphneuron: Abioinspired
architectureforpatternprocessing,IEEEtransactionsonneuralnetworksandlearningsystems,28(2016),pp.1250–
1262.
[17] D. Kleyko, D. Rachkovskij, E. Osipov, and A. Rahimi, A survey on hyperdimensional computing aka vector
symbolic architectures, part ii: Applications, cognitive models, and challenges, ACM computing surveys, 55 (2023),
pp.1–52.
[18] D. Kleyko, D. A. Rachkovskij, E. Osipov, and A. Rahimi, A survey on hyperdimensional computing aka vector
symbolic architectures, part i: Models and data transformations,ACMcomputingsurveys,55(2023),pp.1–40.
[19] A. X. Manabat, C. R. Marcelo, A. L. Quinquito, and A. Alvarez, Performance analysis of hyperdimensional
computingforcharacterrecognition,in2019InternationalSymposiumonMultimediaandCommunicationTechnology
(ISMAC),2019,pp.1–5.
[20] P. Neubert and S. Schubert, Hyperdimensional computing as a framework for systematic aggregation of image
descriptors,inProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,2021,pp.16938–
16947.
[21] P. Neubert, S. Schubert, and P. Protzel, An introduction to hyperdimensional computing for robotics, KI-
Ku¨nstlicheIntelligenz,33(2019),pp.319–330.LAPLACE-HDC 19
[22] K. Schlegel, P. Neubert, and P. Protzel, A comparison of vector symbolic architectures, Artificial Intelligence
Review,55(2022),pp.4523–4555.
[23] I.J.Schoenberg,Metricspacesandpositivedefinitefunctions,TransactionsoftheAmericanMathematicalSociety,
44(1938),pp.522–536.
[24] R. Vershynin,High-dimensional probability: An introduction with applications in data science,vol.47,Cambridge
universitypress,2018.
[25] T.Yu,Y.Zhang,Z.Zhang,andC.M.DeSa,Understandinghyperdimensionalcomputingforparallelsingle-pass
learning,AdvancesinNeuralInformationProcessingSystems,35(2022),pp.1157–1169.
Appendix A. Proof of Analytic Results
A.1. Proof of Theorem 1.1.
Proof of Theorem 1.1. By the definition (1) of the map x(cid:55)→ψ we have
x
1 (cid:16) (cid:17)
S(x,y)= ETr ψ ψ⊤
N x y
(cid:32)(cid:32) d (cid:33)(cid:32) d (cid:33)(cid:33)
1 (cid:75) (cid:75)
= ETr Π v v⊤ Π⊤
N i x(i) y(i) i
i=1 i=1
(cid:32) d (cid:33)
1 (cid:75)
= ETr Π v v⊤ Π⊤
N i x(i) y(i) i
i=1
N (cid:32) d (cid:33)
1 (cid:88) (cid:89)
= E e⊤Π v v⊤ Π⊤e
N j i x(i) y(i) i j
j=1 i=1
N d
1 (cid:88)(cid:89) (cid:16) (cid:17)
= E e⊤Π v v⊤ Π⊤e ,
N j i x(i) y(i) i j
j=1i=1
N d
1 (cid:88)(cid:89)
= K(x(i),y(i))
N
j=1i=1
d
(cid:89)
(20) = K(x(i),y(i)).
i=1
In the chain of equalities above, the third equality holds since for arbitrary vectors u ,v ,RN,
i i
d d d
(cid:75) (cid:75) (cid:75)
u v⊤ = u v⊤,
i i′ i i
i=1 i′=1 i=1
(cid:74)
where the right-side represents a straightforward generalization of Hadamard product from vectors
to matrices. Moreover, by the construction of v , the elements v (j) and v (j′) are statistically
k k k′
independent whenever j ̸=j′. As a result, since the permutation matrices Π are non-overlapping, the
i
factors e⊤Π v v⊤ Π⊤e and e⊤Π v v⊤ Π⊤e become independent whenever i ̸= i′, which
j i x(i) y(i) i j j i′ x(i′) y(i′) i′ j
justifies the fifth equality. Finally, the sixth equality is a straightforward implication of (6).20 S.POURMAND,W.D.WHITING,A.AGHASI,ANDN.F.MARSHALL
Next, we bound the variance. First, we compute the second moment
E(cid:32) ψ⊤
x
Nψ y(cid:33)2
=
N1 2E (cid:88)N (cid:89)d
e⊤
j
Π iv x(i)v⊤ y(i)Π⊤
i
e
j 2
j=1i=1
  
N d N d
(21) = N1 2E (cid:88)(cid:89) e⊤ j Π iv x(i)v⊤ y(i)Π⊤ i e j(cid:88) (cid:89) e⊤ j′Π i′v x(i′)v⊤ y(i′)Π⊤ i′e j′
j=1i=1 j′=1i′=1
N N (cid:32) d d (cid:33)
1 (cid:88)(cid:88) (cid:89) (cid:89)
= E e⊤Π v v⊤ Π⊤e e⊤Π v v⊤ Π⊤e .
N2 j i x(i) y(i) i j j′ i′ x(i′) y(i′) i′ j′
j=1j′=1 i=1 i′=1
Define the N ×N matrix
d d
(cid:88)(cid:88)
Q:= Π Π⊤,
i i′
i′=1i=1
and accordingly, define
Ω:={(j,j′):Q(j,j′)>0},
where Q(j,j′) is the (j,j′)-th element of Q. One can split the summation above over Ω and Ωc as:
(cid:32)
ψ⊤ψ
(cid:33)2
(22) E x y =E +E ,
N Ω Ωc
where for X =Ω,Ωc:
d d
1 (cid:88) (cid:89) (cid:89)
E = E e⊤Π v v⊤ Π⊤e e⊤Π v v⊤ Π⊤e .
X N2 j i x(i) y(i) i j j′ i′ x(i′) y(i′) i′ j′
(j,j′)∈Xi=1 i′=1
The summand is clearly upper-bounded by 1, so the summation of terms over Ω is bounded by
(cid:80)
1 |Ω| γ
(23) E ≤ (j,j′)∈Ω = = P ,
Ω N2 N2 N2
where the final equality follows from the definition of γ . On the other hand, for (j,j′)∈Ωc we have
P
d d
(cid:89) (cid:89)
(24) E e⊤Π v v⊤ Π⊤e e⊤Π v v⊤ Π⊤e =S(x,y)2.
j i x(i) y(i) i j j′ i′ x(i′) y(i′) i′ j′
i=1 i′=1
To see why (24) holds, we start by showing that if (j,j′)∈Ωc, then
(25) e⊤Π ̸=e⊤Π , ∀i,i′ ∈{1,...,d}.
j i j′ i′
Suppose not, that is, suppose e⊤Π =e⊤Π , for some i ,i′ ∈{1,...,d}. Then,
j i0 j′ i′ 0 0 0
(cid:88)(cid:88)
0<e⊤Π Π⊤e ≤e⊤ Π Π⊤e =Q(j,j′),
j i0 i′
0
j′ j i i′ j′
i i′
which contradicts the fact that (j,j′) ∈ Ωc. Recall that, by construction, the elements v (j) and
k
v (j′) are statistically independent whenever j ̸=j′. By (25), it follows that e⊤Π v v⊤ Π⊤e and
k′ j i x(i) y(i) i j
e⊤Π v v⊤ Π⊤e are independent, for all i,i′ ∈{1,...,d}. Thus, we can take the expectation of
j′ i′ x(i′) y(i′) i′ j′
the product of i and i′ separately and use the fact that
d
(cid:89) (cid:16) (cid:17)
E e⊤Π v v⊤ Π⊤e =S(x,y),
j i x(i) y(i) i j
i=1LAPLACE-HDC 21
to deduce (24). Combining (22), (23), and (24) gives
E(cid:32)
ψ⊤ xψ
y(cid:33)2
≤ γ P + 1 (cid:88) S(x,y)2 = γ P + N2−γ PS(x,y)2.
N N2 N2 N2 N2
(j,j′)∈Ωc
Finally, we have
(cid:32)
ψ⊤ψ
(cid:33) (cid:32)
ψ⊤ψ
(cid:33)2 (cid:32)
ψ⊤ψ
(cid:33)2
Var x y =E x y − E x y
N N N
γ N2−γ
≤ P + PS(x,y)2−S(x,y)2
(26) N2 N2
γ
= P (1−S(x,y)2)
N2
2γ
≤ P(1−S(x,y)),
N2
where the final inequality follows from using the fact that 1+S(x,y)≤2. This completes the proof.
□
Remark A.1 (Sharpness of proof of Theorem 1.1). From (21) we have
E(cid:32) ψ⊤ xψ y(cid:33)2
=
1 (cid:88)N (cid:88)N E(cid:32) (cid:89)d
X
(cid:89)d
X
(cid:33)
,
N N2 j,i j′,i′
j=1j′=1 i=1 i′=1
where
X =e⊤Π v v⊤ Π⊤e .
j,i j i x(i) y(i) i j
In the following, we show this estimate can be refined leading to a more complicated statement. Let
(cid:40) d (cid:41)
(cid:88)
Ω = i∈{1,...,d}:e⊤Π Π⊤e >0 .
j,j′ j i i′ j′
i′=1
Using this notation, we have
(cid:32) d d (cid:33)  
(cid:89) (cid:89) (cid:89) (cid:89) (cid:89) (cid:89)
(27) E X j,i X j′,i′ = E(X j,i) E(X j′,i′)E  X j,i X j′,i′,
i=1 i′=1 i∈Ωc j,j′ i′∈Ωc j′,j i∈Ω j,j′ i′∈Ω j′,j
where we used the fact that X is independent of all other random variables in the product when
j,i
i ∈ Ωc , and likewise, by symmetry, X is independent of all other random variables in the product
j,j′ j,i′
when i′ ∈Ω . We have
j′,j
(cid:89) (cid:89) (cid:89) (cid:89)
EX EX = K(x(i),y(i)) K(x(i′),y(i′)),
j,i j′,i′
i∈Ωc i′∈Ωc i∈Ωc i′∈Ωc
j,j′ j′,j j,j′ j′,j
while the third term on the right-hand side of (27) is bounded by 1. In fact, this bound is sharp in
certain cases, for example, in the case when x and y are constant. In summary, we have
E(cid:32) ψ⊤ xψ y(cid:33)2
≤
1 (cid:88)N (cid:88)N (cid:89)
K(x(i),y(i))
(cid:89)
K(x(i′),y(i′)).
N N2
j=1j′=1i∈Ωc i′∈Ωc
j,j′ j′,j22 S.POURMAND,W.D.WHITING,A.AGHASI,ANDN.F.MARSHALL
Note that with this notation Ω is the set of “bad permutations”, which cause terms in the product
j,j′
of random variables to be dependent. When Ω =∅ and Ω =∅ we have
j,j′ j′,j
(cid:89) (cid:89)
K(x(i),y(i)) K(x(i′),y(i′))=S(x,y)2.
i∈Ωc i′∈Ωc
j,j′ j′,j
Alternatively, if there is only one bad permutation in each set Ω ={i } and Ω ={i′} we have
j,j′ 0 j′,j 0
(cid:89) (cid:89) S(x,y)2
K(x(i),y(i)) K(x(i′),y(i′))= .
K(x(i ),y(i ))K(x(i′),y(i′))
i∈Ωc i′∈Ωc 0 0 0 0
j,j′ j′,j
Previously, we performed a global analysis, which only looked at the number of j,j′ for which there are
any bad permutations (see Ω below). This refined analysis shows that the number of bad permutations
also matters. The block-based permutation schemes minimize the number of j,j′ for which Ω > 0,
j,j′
but do this bad trying to maximize |Ω | whenever Ω >0.
j,j′ j,j′
A.2. Proof of Theorem 1.2.
Proof of Theorem 1.2. If K is defined by (14), then
α
(cid:18) π2 (cid:19)
W (i,j)=exp − λ|i−j|2α ,
α 8
which is positive semi-definite by Lemma 1.2 and the fact that α∈(0,1]. Note that
√
2 arcsin(cid:0) exp(cid:0) −x2(cid:1)(cid:1) =1− 2 2x +O(x3),
π π
as x→0. If λ|i−j|α ≤ε, then it follows that the kernel K defined in (14) satisfies
α
(28) K (i,j)=1−λ|i−j|α+O(ε3).
α
Write
d (cid:32) d (cid:33)
(cid:89) (cid:88)
S (x,y)= K (x(i),y(j))=exp ln(K (x(i),y(j))) .
α α α
i=1 i=1
Using the series expansions
ln(1−x)=−x+O(x2), and exp(x)=1+O(x),
as x→0, together with (28) gives
(cid:32) d (cid:33) (cid:32) d (cid:33)
exp (cid:88) ln(K (x(i),y(j))) =exp −λ(cid:88) |x(i)−y(i)|α (cid:0) 1+O(ε2d)(cid:1) .
α
i=1 i=1
That is,
S
(x,y)=exp(−λ∥x−y∥α)(cid:0) 1+O(ε2d)(cid:1)
α α
which completes the proof.
□
Email address: pourmans@oregonstate.edu
School of Electrical Engineering and Computer Science, Oregon State University
Email address: whitinwy@oregonstate.edu
Department of Mathematics, Oregon State University
Email address: alireza.aghasi@oregonstate.eduLAPLACE-HDC 23
School of Electrical Engineering and Computer Science, Oregon State University
Email address: marsnich@oregonstate.edu
Department of Mathematics, Oregon State University