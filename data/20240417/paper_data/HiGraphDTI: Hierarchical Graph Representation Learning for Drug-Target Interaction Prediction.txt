HiGraphDTI: Hierarchical Graph Representation
Learning for Drug-Target Interaction Prediction
Bin Liu1, Siqi Wu1, Jin Wang1 ⋆, Xin Deng1, and Ao Zhou1
Key Laboratory of Data Engineering and Visual Computing,
Chongqing University of Posts and Telecommunications, China
{liubin, wangjin, dengxin}@cqupt.edu.cn, {yunning996, zacqupt}@gmail.com
Abstract. The discovery of drug-target interactions (DTIs) plays a
crucial role in pharmaceutical development. The deep learning model
achievesmoreaccurateresultsinDTIpredictionduetoitsabilitytoex-
tractrobustandexpressivefeaturesfromdrugandtargetchemicalstruc-
tures. However, existing deep learning methods typically generate drug
features via aggregating molecular atom representations, ignoring the
chemical properties carried by motifs, i.e., substructures of the molecu-
largraph.Theatom-drugdouble-levelmolecularrepresentationlearning
cannotfullyexploitstructureinformationandfailstointerprettheDTI
mechanism from the motif perspective. In addition, sequential model-
based target feature extraction either fuses limited contextual informa-
tion or requires expensive computational resources. To tackle the above
issues, we propose a hierarchical graph representation learning-based
DTIpredictionmethod(HiGraphDTI).Specifically,HiGraphDTIlearns
hierarchical drug representations from triple-level molecular graphs to
thoroughly exploit chemical information embedded in atoms, motifs,
and molecules. Then, an attentional feature fusion module incorporates
information from different receptive fields to extract expressive target
features. Last, the hierarchical attention mechanism identifies crucial
molecular segments, which offers complementary views for interpreting
interaction mechanisms. The experiment results not only demonstrate
thesuperiorityofHiGraphDTItothestate-of-the-artmethods,butalso
confirm the practical ability of our model in interaction interpretation
and new DTI discovery.
Keywords: drug-target interaction prediction · hierarchical graph rep-
resentation learning · feature fusion · attention mechanism
1 Introduction
Nowadays, pharmaceutical scientists still rely on existing drug-target interac-
tions (DTIs) to develop novel drugs [3]. Therefore, there is a pressing need
to accurately and efficiently discover new DTIs. Although traditional in vitro
wet-lab verification can obtain reliable DTIs, the complex experimental process
⋆ Corresponding author.
4202
rpA
61
]GL.sc[
1v16501.4042:viXra2 Bin Liu et al.
consumesconsiderabletimeandlabor,makingitchallengingtoscreenthrougha
largenumberofcandidatesrapidly[29].Thecomputationalmethodsreceivecon-
siderable focus, since they can significantly diminish the resources for screening
by predicting reliable DTI candidates [1]. Deep learning models have achieved
superior performances in DTI prediction, due to their ability to extract robust
and high-quality features from abundant drug and target structure information
[22,4]. Deep learning DTI prediction methods typically extract drug and tar-
get features from their chemical structures and integrate them to infer unseen
interactions [27].
Drugsarechemicalmolecules,representedbyeithertheSimplifiedMolecular
Input Line Entry System (SMILES) strings [2] or molecular graphs [19]. Convo-
lutional Neural network (CNN) [28] and Transformer [14,26] are utilized to gen-
eratedrugembeddingsviaencodingsequentialmolecularinformationinSMILES
strings.Ontheotherhand,themoleculargraphsexplicitlydepictatomrelations
in2-dimensionalgeometricspace,enablinggraphneuralnetworks(GNNs)toex-
tractmoreinformativedrugrepresentations[13,17].Motifs,molecularsubgraphs
composedofpartofatomsandtheirbonds,usuallycarryindicativeinformation
abouttheimportantmolecularpropertiesandfunctions[25].Nevertheless,exist-
ingGNN-baseddeeplearningmodelstypicallylearnatomnodeembeddingsand
aggregate them via readout or attention-weighted summation to derive molec-
ular representations, ignoring important functional characteristics expressed by
motifs.Furthermore,currentDTIpredictionmethodsonlyofferthecontribution
of each atom to the interaction between drug and target, failing to investigate
the biological interpretation of DTIs from the motif perspective.
Fortargets,DTIpredictionmethodsusesequentialmodels,suchasCNN[23,13],
RNN [12] and Transformer [8] to extract high-level features from their amino
acid sequences. They commonly select the last layer of the deep neural network
as final representations. However, CNN and RNN-based target features lack a
broad receptive field [23,13,12]. Although Transformer-based target representa-
tions fuse every amino acid embeddings, they suffer expensive computational
costs [8].
In this study, we propose a hierarchical graph representation learning-based
DTI prediction method (HiGraphDTI) to enrich the information involved in
drug and target features and enhance the interpretation of DTI mechanisms.
First, we employ hierarchical molecular graph representation to extract atom,
motif, and global-level embeddings, enabling atomic information aggregation
more orderly and reliable while incorporating more chemical properties. Then,
we develop an attentional target feature fusion module, which extends recep-
tive fields to improve the expressive ability of protein representations. Finally,
we design a hierarchical attention mechanism to capture the various level cor-
relations between drugs and targets, providing comprehensive interpretations of
DTIs from multiple perspectives. Experimental results on four benchmark DTI
datasetsillustratethatHiGraphDTIsurpassessixstate-of-the-artmethods.The
effectiveness of our method in providing valuable biological insights is verified
via case studies on multi-level attention weight visualizations.HiGraphDTI for DTI Prediction 3
2 Related Work
Predicting drug-target interactions (DTIs) is a crucial area of research in drug
development. In recent years, predominant computational approaches comprise
two categories: traditional machine learning and deep learning.
TraditionalmachinelearningDTIpredictionmethodstypicallyrelyonman-
ually crafted features, e.g., molecular descriptors for drugs and structural and
physicochemical property-based protein features [21]. In [15], the SVM classi-
fier utilizes different kernel functions to determine the similarity of compounds
and proteins, and combines chemical and genomic spaces via tensor products.
EBiCTR [20] is an ensemble of bi-clustering trees trained on the reconstructed
output space and dyadic (drug and target) feature space.
Deep learning approaches can alleviate the issue by their capability to learn
feature representations. DeepDTA [30] only leverages the sequence information
of drugs and targets to predict drug-target binding affinity. DeepConv-DTI [16]
employs convolution on amino acid subsequences of varying lengths to capture
local residue patterns in proteins, enriching the information of target features.
TransformerCPI [7] let target features serve as the output of the Transformer
encoder and the drug features serve as the input to the Transformer decoder
to catch the interactions between drugs and targets. MolTrans [14] introduces
a Frequent Consecutive Sub-sequence (FCS) mining algorithm, which utilizes
unlabeled data to learn contextual semantic information in SMILES strings and
amino acid sequences. The FCS algorithm enhances the expressive power of the
model and makes progress in exploiting other information. However, it merely
identifies patterns in SMILES strings, which may not correspond to the struc-
tural characteristics of drugs. IIFDTI [9] comprises four feature components:
target features extracted by convolutional neural networks, drug features ex-
tractedbygraphattentionnetworks,andtwointeractionfeaturesobtainedfrom
theTransformer.SimilartoMoLTrans,italsoincorporatessemanticinformation
of SMILES and amino acid sequences. DrugBAN [5] utilizes a bilinear attention
networkmoduletocapturelocalinteractionsbetweendrugsandtargetsforDTI
prediction.
Although the aforementioned methods have achieved excellent performance,
they still encounter issues: 1. They explore the structural information in drug
moleculesinadequately.2.Theytypicallyemploysummationoraveragingasthe
READOUTfunction,leadingtoanunorderedaggregationofinformationinthis
process. 3. They lack multi-level biological interpretation.
3 Method
In this section, we illustrate the proposed HiGraphDTI model that predicts in-
teractionsbetweendrugsandtargetsviahierarchicalgraphrepresentationlearn-
ing.Fig.1outlinesthearchitectureofHiGraphDTI,whichconsistsofthreemain
modules:4 Bin Liu et al.
– Hierarchical molecular graph representation that extracts drug features, en-
richingthechemicalstructurepropertiesandcharacteristicsexploitationand
making the information aggregation process more orderly and reliable.
– Attentional target feature fusion that adopts a broader receptive field to
protein sequence representation extraction.
– Hierarchical attention mechanism that captures the correlations between
drug and target features from various perspectives, providing comprehen-
sive explanations for DTI mechanisms.
3.1 Hierarchical Molecular Graph Representation
Hierarchicalgraphrepresentationfordrugscontainstwoparts:hierarchicalgraph
construction and message-passing. The molecular graph partition process is il-
lustrated in Fig. 2.
First, We transform the original drug molecules into a graph G = (V,E),
whereeachatomcorrespondstoanodev ∈V,andthebondsbetweenatomscor-
respond to bidirectional edges in E. G is the atom layer of the molecular graph.
Then,wedividethedrugmoleculesintomultiplefunctionalfragmentsusingthe
Breaking of Retrosynthetically Interesting Chemical Substructures (BRICS) al-
gorithm, which defines 16 rules and breaks strategic bonds in a molecule that
Fig.1. The overview architecture of HiGraphDTI.HiGraphDTI for DTI Prediction 5
match a set of chemical reactions [11]. Following the work [25], we supplement
an additional partition rule, i.e., disconnecting cycles and branches around min-
imum rings, to BRICS algorithm to get rid of excessively large fragments.
These obtained fragments, referred to as motifs, construct the second level
of the molecular graph. We create a node for each motif, and the collection of
nodes is defined as V . We connect each motif node with its involved atoms in
m
the atom layer, and the collection of these edges is defined as E . To avoid the
m
over-smoothing issue in graph neural networks and make message aggregation
morereasonable,theseedgesareunidirectional,pointingfromtheatomlayerto
the motif layer.
Finally, to aggregate the global information of drug molecules, we construct
a global node V , which is the graph-layer. We establish connections between it
g
and all motif nodes, and the collection of these edges is referred to as E . These
g
edges are also unidirectional, pointing from motif nodes to the global node V .
g
The final hierarchical graph is constructed as follows:
G¯ =(V¯,E¯),V¯ =(V,V ,V ),E¯ =(E,E ,E ) (1)
m g m g
Fig.2.Hierarchicalgraphrepresentationconstruction.Inthediagram,solidlinesrep-
resent bidirectional edges, and dashed lines represent unidirectional edges.
Given the triple-layer molecular graph, we employ Graph Isomorphism Net-
work(GIN)topropagatemessagesandlearnnodeembeddingsduetoitssuperior
expressive power demonstrated by Weisfeiler-Lehman (WL) test [24]. Specifi-
cally, the message-passing formula of GIN is:
(cid:88)
hl =MLPl(hl−1+ (hl−1+WlX )) (2)
v v v uv
u∈N(v)6 Bin Liu et al.
where MLPl represents a multi-layer perceptron (MLP) for node features uti-
lizedtoupdatenodes,X representstheedgeembeddingsbetweennodesuand
uv
v, Wl represents the embedding parameters of X for each layer and h0 =X
uv v v
istheinputnodefeatureofv ∈V¯.Aftermultipleiterationsofupdates,weobtain
the final embeddings of atom, motif, and global nodes, denoted as H ∈R|a|×d,
a
H ∈ R|m|×d and H ∈ R1×d respectively, where |a| is the number of atoms,
m g
|m| is the number of motifs. We adopt H as the representation of the whole
g
drug molecule.
3.2 Attentional Target Feature Fusion
Followingpreviouswork[23],wepartitionthetargetsequenceinto3-gramamino
acidstoobtaintheinitialvector,denotedasX ={x ,x ,...,x },wherex ∈Rd
P 1 2 l i
representstheembeddingofthei-thsegment,l isthenumberofthepartitioned
sequences, and d is the embedding dimension. To better aggregate critical fea-
tures in the protein vector representation, We design a one-dimensional (1D)
convolutional neural network with layer-wise decreasing channels, and the for-
mula for each layer is as follows:
X =Relu(BN (Conv1D (X ))) (3)
i i i i−1
whereX representsthefeaturerepresentationforthei-thlayer,andX =X ,
i 0 P
Conv1D represents the 1D convolution in the i-th layer with the kernel size of
15andtheoutputchannelsreducedbyhalf,RelurepresentstheReLunonlinear
activation function, BN represents the batch normalization in the i-th layer.
i
WeobtaintargetfeaturerepresentationsX ,X ,X atthreedifferentconvo-
1 2 3
lutionallayers.Toaggregatetargetinformation,weadapttheattentionalfeature
fusion (AFF) module [10] tailored to amino acid sequences. The process is de-
pictedinFig.3.WeperformtransposedconvolutiononX tomapittothesame
3
dimension as X and then put it into the AFF module. Next, Map the result to
2
the same dimension as X and put it into the AFF module to obtain the out-
1
come, denoted as H ∈ Rl×d, where l is the number of partitioned sequences,
P
is the embedding dimension.
The detailed illustration of AFF module are shown in Fig. 4. AFF module
receives two inputs, I and I , where I is the high-level feature after trans-
1 2 1
posed convolution, and I is the low-level feature. We combine the information
2
from those through element-wise summation as I and feed the result into mod-
ule M to extract additional information. Module M achieves channel attention
across multiple scales by changing the spatial pooling size. It mainly consists of
two parts: one for extracting global features and the other for extracting local
features, as illustrated in Eq. (4).
M(I)=σ(L(I)⊕L(MeanPooling(I))) (4)
whereσ istheSigmoidfunction,MeanPooling(I)=
1(cid:80)l
I[i,:]istheaverage
l i=1
poolingalongcolumns,⊕referstothebroadcastingadditionandL(I)isdefined
as:
L(I)=BN(PWConv (Relu(BN(PWConv (I))))) (5)
2 1HiGraphDTI for DTI Prediction 7
Fig.3. The overview architecture of feature fusion module for protein. The high-level
featureismappedtothesamedimensionasthelow-levelusingtransposedconvolution
andtheninputintotheAFFmoduleforfusion.Takingtheresultashigh-levelfeatures,
repeat the operation.
Fig.4. The architecture of AFF module, which utilizes operation M to compute the
attention matrix for weighted aggregation of inputs I and I .
1 28 Bin Liu et al.
In L(I), PWConv and PWConv refer to two point-wise 1D convolutions to
1 2
captureinformationfromdiversechannelsandmaintainthemodelaslightweight
as possible. After applying the module M, We obtain the attention matrix. To
get the final output, we perform the following operations.
O=M(I)⊗I +(1−M(I))⊗I (6)
1 2
where ⊗ denotes the element-wise multiplication. In Fig. 4, the black dashed
line denotes (1−M(I)). M(I) and 1−M(I) are real arrays in the range of 0 to
1, facilitating a weighted sum of I and I .
1 2
3.3 Hierarchical Attention Mechanism
Wedesignahierarchicalattentionmechanismtocapturethecorrelationbetween
triple-level drug features (H , H , H ) and target features (H ). Graph level
a m g P
drug features H aggregates the global molecular information after cross-level
g
message-passing. It consists of only a single vector, lacking robustness. Incorpo-
rating additional information could significantly decrease its express capacity.
Therefore,weonlyincorporatedrugfeaturesintothetargetembeddingH .We
P
calculate the attention between targets and different levels of drugs using the
following formula:
Attn =Relu(H⊤W H ) (7)
a P a a
Attn =Relu(H⊤W H ) (8)
m P m m
Attn =Relu(H⊤W H ) (9)
g P g g
where Attn ∈ Rl×|a|, Attn ∈ Rl×|m|, Attn ∈ Rl×1 represent attention
a m g
matrices between protein partitioned sequence and different levels (atom, motif,
and global) of the drug molecule. Next, we calculate the mean along the rows
for each attention matrix, resulting in three attention vectors A , A , A . The
a m g
summationofthesevectorsutilizedasweightsforupdatingH yieldstheprotein
P
representation enriched with drug information. Its formula is as follows:
F =H ·(SF(A )+SF(A )+SF(A )) (10)
P P a m g
where SF is the softmax function.
Finally, we concatenate H and F and then feed them into a multi-layer
g P
perceptron (MLP) model to derive the probability of drug-target interaction Yˆ.
The binary cross-entropy loss is utilized for training our model.
N
1 (cid:88)
L=− y ·log(yˆ)+(1−y )·log(1−yˆ) (11)
N i i i i
i
where y is the true label, yˆis the predicted label, N is the number of training
i i
samples.HiGraphDTI for DTI Prediction 9
4 Experiments
4.1 Experimental Setup
We select four benchmark datasets in the DTI field to evaluate our model, in-
cluding Human dataset [18], Caenorhabditis elegant (C.elegans) dataset [18],
BindingDB dataset [12], GPCR dataset [7]. Human and C.elegans datasets are
created using a systematic screening framework to obtain highly credible nega-
tivesamples[18].GPCRdatasetisconstructedthroughtheGLASSdatabase[6],
which uses scores to describe the drug-target affinity(DTA). To obtain samples
forDTIs,GPCRusesathresholdof6.0tocategorizepositiveandnegativesam-
ples. The BindingDB dataset [12] primarily focuses on the interactions of small
molecules, and it is well-divided into non-overlapping training, validation, and
test sets. Table 1 presents the statistics of the mentioned datasets.
Table 1. Statistics of datasets.
Datasets Targets Drugs Interactions Positive Negative
Human 852 1052 6738 3369 3369
C.elegans 2504 1434 8000 4000 4000
BindingDB 812 49745 61258 33772 27486
GPCR 356 5359 15343 7989 7354
FortheHumanandC.elegansdatasets,weemployafive-foldcross-validation
approach.Theyaredividedintotrainingset,validationsetandtestsetaccording
to the ratio of 8:1:1. For the BindingDB dataset, the training set, validation set
and test set are partitioned well [12]. For the GPCR dataset, the training set
and test set are divided well [7]. We randomly select the 20% of the training set
as the validation set.
We select six state-of-the-art DTI prediction methods for comparison: Deep-
DTA [30], DeepConv-DTI [16], MolTrans [14], TransformerCPI [7], IIFDTI [9],
and DrugBAN [5]. A brief introduction to the methods mentioned above is pro-
videdintheSupplementaryMaterials.ToadaptDeepDTA,adrug-targetaffinity
prediction model, to the DTI prediction task, we replace the loss function in its
last layer with binary cross-entropy loss.
We choose four metrics for evaluating our models: AUC (the area under the
receiver operating characteristic curve), AUPR (the area under the precision-
recall curve), Precision, and Recall. We execute all models ten times using dif-
ferent random seeds, calculating their averages to compare performance.
Foralldatasets,wesavethemodelparametersthatachievethehighestAUC
onthevalidationset.Then,weevaluateitsperformanceonthetestsettoobtain
results.Foreachdataset,weexecutetheexperimentstentimeswithdifferentten
seedsandcalculatetheiraverageandstandarddeviation(std)asthefinalresults
to compare. Details regarding dataset partitioning and model hyperparameter10 Bin Liu et al.
settings are available in the Supplementary Materials. The codes of our model
are available at https://anonymous.4open.science/r/HiGraphDTI-08FB.
4.2 Comparison Results
As shown in Table 2 and Table 3, HiGraphDTI outperforms the six baselines in
termsofAUCandAUPRonalldatasets.Weattributetheexcellentperformance
to three merits of HiGraphDTI. First, using hierarchical graph representation
allowsdrugstoaggregateinformationacrossdifferentlevels,enrichingthemolec-
ularstructurerepresentation.Second,employingfeaturefusionmodulesenables
targetstocaptureinformationfromdifferentreceptivefields,enhancingthepro-
teinsequencerepresentation.Thrid,Applyinghierarchicalattentionmechanisms
computesinteractiveattentionbetweendifferentlevelsofdrugsandtargets,aug-
menting the interaction information between drugs and targets.
IIFDTI ranks second on the Human, C. elegans and GPCR datasets. The
innovation of IIFDTI lies in its utilization of Word2Vec to separately extract
featurerepresentationsfromSMILESandaminoacidsequences.Itincorporates
textual information encoded in SMILES, while HiGraphDTI enriches hierarchi-
cal information in molecular graph representations. Compared to compressed
textual information, hierarchically aggregated information based on molecular
chemical properties is more expressive. At the same time, after hierarchical par-
titioning,ourmethodcancalculateattentionscoresbetweendifferentlevelsand
thetarget.Thatenrichestheinformationofinteractionfeaturesandallowsfordi-
versebiologicalinterpretationsatdifferentlevelsofDTI.HiGraphDTIsurpasses
IIFDTIinAUCandAUPR,especiallyontheGPCRdataset,withimprovements
of 1.3% and 0.8%, respectively. For the larger dataset BindingDB, DrugBAN is
the second-best in terms of AUC. DrugBAN utilizes graph neural networks and
convolutional neural networks to extract feature representations for drugs and
targets.ItemploysitsproposedBilinearAttentionNetworktoobtaininteraction
features. However, It does not incorporate additional information to enrich its
feature representation, resulting in its inferiority to HiGraphDTI. Furthermore,
HiGraphDTI also exhibits advantages over IIFDTI on the BindingDB dataset,
achieving improvements of 0.9% in AUC and 1.1% in AUPR. The results for
precision and recall are presented in the supplementary materials.
4.3 Ablation Experiment
TovalidatetheeffectivenessofeachmoduleinHiGraph,wedesignthefollowing
ablation experiments.
– HiGraphDTIw/oFF:Weremovethetargetfeaturefusionmoduleandretain
the last output convolutional layer as target representation.
– HiGraphDTIw/oHI:Weremoveallattentionsbetweendrugsandtargets.We
only concatenate the global-level features of drugs and the mean of target
features for prediction.HiGraphDTI for DTI Prediction 11
Table 2. Experiment results in terms of AUC, where the best and runner-up results
are highlighted in bold and underlined, respectively.
Dataset
Human C.elegans BindingDB GPCR
Model
DeepDTA 0.972 (0.001) 0.983 (0.001) 0.934 (0.007) 0.776 (0.006)
DeepConv-DTI 0.967 (0.002) 0.983 (0.002) 0.922 (0.003) 0.752 (0.011)
MolTrans 0.974 (0.002) 0.982 (0.003) 0.899 (0.006) 0.807 (0.004)
TransformerCPI 0.970 (0.006) 0.984 (0.002) 0.933 (0.011) 0.842 (0.007)
IIFDTI 0.984 (0.003) 0.991 (0.002) 0.944 (0.003) 0.845 (0.008)
DrugBAN 0.984 (0.001) 0.989 (0.001) 0.945 (0.007) 0.837 (0.010)
Ours 0.985 (0.001) 0.993 (0.001) 0.954 (0.003) 0.858 (0.004)
Table 3.ExperimentresultsintermsofAUPR,wherethebestandrunner-upresults
are highlighted in bold and underlined, respectively.
Dataset
Human C.elegans BindingDB GPCR
Model
DeepDTA 0.973 (0.002) 0.984 (0.007) 0.934 (0.008) 0.762 (0.015)
DeepConv-DTI 0.964 (0.004) 0.985 (0.001) 0.921 (0.004) 0.685 (0.010)
MolTrans 0.976 (0.003) 0.982 (0.003) 0.897 (0.010) 0.788 (0.009)
TransformerCPI 0.974 (0.005) 0.983 (0.003) 0.934 (0.015) 0.837 (0.010)
IIFDTI 0.985 (0.003) 0.992 (0.003) 0.945 (0.004) 0.842 (0.007)
DrugBAN 0.981 (0.001) 0.990 (0.002) 0.944 (0.005) 0.823 (0.013)
Ours 0.988 (0.001) 0.993 (0.001) 0.955 (0.003) 0.850 (0.003)
– HiGraphDTIw/oHC: We remove the hierarchical structure from the graph
representation and only use atom-level embeddings to construct drug fea-
tures.
– HiGraphDTIw/oML: We remove the motif-level nodes from the hierarchical
molecular graph and only utilize atom and global nodes to construct drug
features.
The experimental results on the GPCR dataset are shown in Fig. 5. The result
of HiGraphDTIw/oFF validates the importance of the feature fusion module
in constructing target features. Losing multiple receptive fields leads to a de-
crease in model performance. The result of HiGraphDTIw/oHI demonstrates
the validity of the hierarchical attention mechanisms. It comprehends the in-
teraction between drugs and targets from different perspectives, enhancing the
understanding and predictive capability of the model. Finally, the comparison
between HiGraphDTIw/oHC and HiGraphDTIw/oML confirms the superiority12 Bin Liu et al.
Fig.5. Ablation experiment results on the GPCR dataset
ofhierarchicalgraphrepresentationlearningmethodsindrugfeatureextraction.
The multi-layered structure enriches the expression of drug features.
4.4 Attention Interpretation
Thehierarchicalattentionmechanismnotonlyenhancesmodelperformancebut
alsoassistsusinunderstandingthedrug-targetinteractionfromvariousinsights.
In this part, we utilize the attention weights to interpret the effectiveness of
thehierarchicalattentionmechanism.Furthermore,weillustratethedrug-target
interaction from the atom and motif levels to offer valuable assistance for drug
discovery.
To better understand the interaction between drug and target, we choose
target PDB: 1N28 and drug (ligands) PDB: I3N (C H NO ) as a case study.
19 19 3
We use the hierarchical attention mechanism to calculate the attention vector
B =SF(A )+SF(A )+SF(A )∈Rl, which demonstrates the distribution
P a m g
ofaminoacidattentionweights.ThevaluesinB areallwithintherangeof0to
P
1.TheattentionweightsforeachaminoacidofPDB:1N28areshowninFig.6(a),
where different colors represent varying attention weights. The actual binding
sitesarerepresentedbyaminoacidletterswitharedbackground.FromFig.6(a),
wecanobservethatthemodelgiveshighattentiontosixamongthetotaleleven
binding sites. In addition, the model provides seven other positions (located at
30, 31, 32, 69, 70, 97, 98) with high attention weights, which could serve as
potential binding sites for future chemical experiments. Fig. 6(b) depicts the
3D visualization of the docking interaction of PDB: I3N and PDB: 1N28, where
red regions represent binding sites with high attention weights, yellow segments
indicate the binding site with low attention weights, green regions represent theHiGraphDTI for DTI Prediction 13
(a) Attention Weights for each amino acid of PDB: 1N28
(b) 3D visualization of docking interaction
of PDB: I3N with PDB: 1N28
Fig.6.VisualizationoftargetattentionweightsforinteractionofPDB:I3NandPDB:
1N28
high attention weighted amino acids that have not been recognized as binding
sites.
IntheprocessofcomputingB ,weobtainthreeattentionmatrices:Attn ∈
P a
Rl×|a|,Attn ∈Rl×|m|,andAttn ∈Rl×1.Wefurtheraverageeverycolumnof
m g
Attn ,Attn toobtaintheattentionvectorB ∈R|a|,B ∈R|m|,whereeach
a m a m
elementillustratestheimportanceofeachnodetotheinteraction.Visualization
of drug attention weights for the interaction of PDB: I3N and PDB: 1N28 are
shown in Fig. 7, where dashed lines of the same color connect motif and its
composed atoms. There are fifteen atoms interacting with at least one amino
acid, where 2/3 attention weights exceed 0.6. The corresponding motif nodes
also exhibit high attention weights. It can be observed that the 11-th atom (C)
is an active node in the docking simulation. While its atom attention weight is
nothigh,the3-rdmotifnodecontainingithasahighattentionweight,servingas
apowerfulsupplement.Thisvalidatesthatthehierarchicalgraphrepresentation
approach to constructing drug features permits the model to better discern the14 Bin Liu et al.
Fig.7. Visualization of drug attention weights for the interaction of PDB: I3N and
PDB:1N28
importance of nodes and ensures crucial nodes are not overlooked during the
drug development process.
5 Conclusion
In this paper, we propose a novel model to predict DTI, named HiGraphDTI.
Weutilizehierarchicalmoleculargraphrepresentationtoconstructdrugfeatures,
which possess more information about drug structures and a more reasonable
way to convey messages. To expand the receptive field of target features, we
design the attentional target feature fusion strategy to obtain more informative
proteinrepresentations.Furthermore,withthehierarchicalattentionmechanism,
we catch the interactive information between drugs and targets from multiple
views. To validate the effectiveness of our model, we compare it with six state-
of-the-art models on four datasets. The experimental results indicate that our
model outperforms comparing baselines in terms of AUC and AUPR metrics.
Finally,thevisualizationsofattentionweightsconfirmtheinterpretationability
of HiGraph to support new drug discovery.
References
1. Abbasi, K., Razzaghi, P., Poso, A., Ghanbari-Ara, S., Masoudi-Nejad, A.: Deep
learning in drug target interaction prediction: Current and future perspectives.
Current Medicinal Chemistry p. 2100–2113 (Apr 2021)HiGraphDTI for DTI Prediction 15
2. Anderson, E., Veith, G., Weininger, D.: Smiles: a line notation and computerized
interpreter for chemical structures. (Jan 1987)
3. Bagherian,M.,Sabeti,E.,Wang,K.,Sartor,M.A.,Nikolovska-Coleska,Z.,Najar-
ian, K.: Machine learning approaches and databases for prediction of drug-target
interaction: a survey paper. Briefings in Bioinformatics p. 247–269 (Jan 2021)
4. Bagherian,M.,Sabeti,E.,Wang,K.,Sartor,M.A.,Nikolovska-Coleska,Z.,Najar-
ian, K.: Machine learning approaches and databases for prediction of drug-target
interaction: a survey paper. Briefings in bioinformatics 22(1), 247–269 (2021)
5. Bai, P., Miljković, F., John, B., Lu, H.: Interpretable bilinear attention network
withdomainadaptationimprovesdrug–targetprediction.NatureMachineIntelli-
gence 5(2), 126–136 (Feb 2023)
6. Chan,W.K.B.,Zhang,H.,Yang,J.,Brender,J.R.,Hur,J.,Özgür,A.,Zhang,Y.:
Glass:acomprehensivedatabaseforexperimentallyvalidatedgpcr-ligandassocia-
tions. Bioinformatics p. 3035–3042 (Sep 2015)
7. Chen, L., Tan, X., Wang, D., Zhong, F., Liu, X., Yang, T., Luo, X., Chen, K.,
Jiang, H., Zheng, M.: Transformercpi: improving compound–protein interaction
prediction by sequence-based deep learning with self-attention mechanism and la-
bel reversal experiments. Bioinformatics p. 4406–4414 (Aug 2020)
8. Cheng, Z., Yan, C., Wu, F.X., Wang, J.: Drug-target interaction prediction using
multi-head self-attention and graph attention network. IEEE/ACM Transactions
on Computational Biology and Bioinformatics 19(4), 2208–2218 (2022)
9. Cheng, Z., Zhao, Q., Li, Y., Wang, J.: IIFDTI: predicting drug–target interac-
tionsthroughinteractiveandindependentfeaturesbasedonattentionmechanism.
Bioinformatics 38(17), 4153–4161 (07 2022)
10. Dai,Y.,Gieseke,F.,Oehmcke,S.,Wu,Y.,Barnard,K.:Attentionalfeaturefusion.
In: 2021 IEEE Winter Conference on Applications of Computer Vision (WACV)
(Jan 2021)
11. Degen, J., Wegscheid-Gerlach, C., Zaliani, A., Rarey, M.: On the art of compiling
andusing’drug-like’chemicalfragmentspaces.ChemMedChem3(10),1503–7(Oct
2008)
12. Gao,K.Y.,Fokoue,A.,Luo,H.,Iyengar,A.,Dey,S.,Zhang,P.:Interpretabledrug
targetpredictionusingdeepneuralrepresentation.In:ProceedingsoftheTwenty-
Seventh International Joint Conference on Artificial Intelligence (Jul 2018)
13. Hua, Y., Song, X.N., Feng, Z., Wu, X.J., Kittler, J., Yu, D.J.: Cpinformer for
efficient and robust compound-protein interaction prediction. IEEE/ACM Trans-
actions on Computational Biology and Bioinformatics p. 1–1 (Jan 2022)
14. Huang, K., Xiao, C., Glass, L.M., Sun, J.: Moltrans: Molecular interaction trans-
formerfordrugtargetinteractionprediction.Bioinformaticsp.830–836(May2021)
15. Jacob,L.,Vert,J.P.:Protein-ligandinteractionprediction:animprovedchemoge-
nomics approach. Bioinformatics 24, 2149 – 2156 (2008)
16. Lee, I., Keum, J., Nam, H.: Deepconv-dti: Prediction of drug-target interactions
via deep learning with convolution on protein sequences. PLOS Computational
Biology 15(6), 1–21 (06 2019)
17. Li, F., Zhang, Z., Guan, J., Zhou, S.: Effective drug–target interaction predic-
tion with mutual interaction neural network. Bioinformatics 38(14), 3582–3589
(06 2022)
18. Liu,H.,Sun,J.,Guan,J.,Zheng,J.,Zhou,S.:Improvingcompound–proteininter-
action prediction by building up highly credible negative samples. Bioinformatics
p. i221–i229 (Jun 2015)16 Bin Liu et al.
19. Nguyen,T.,Le,H.,Quinn,T.P.,Nguyen,T.,Le,T.D.,Venkatesh,S.:GraphDTA:
predictingdrug–targetbindingaffinitywithgraphneuralnetworks.Bioinformatics
37(8), 1140–1147 (10 2020)
20. Pliakos,K.,Vens,C.:Drug-targetinteractionpredictionwithtree-ensemblelearn-
ing and output space reconstruction. BMC bioinformatics 21, 1–11 (2020)
21. Sachdev, K., Gupta, M.K.: A comprehensive review of feature based methods for
drug target interaction prediction. Journal of biomedical informatics 93, 103159
(2019)
22. Sun, M., Zhao, S., Gilvary, C., Elemento, O., Zhou, J., Wang, F.: Graph convo-
lutional networks for computational drug development and discovery. Briefings in
bioinformatics 21(3), 919–935 (2020)
23. Tsubaki, M., Tomii, K., Sese, J.: Compound-protein interaction prediction with
end-to-end learning of neural networks for graphs and sequences. Bioinformatics
p. 309–318 (Jan 2019)
24. Xu,K.,Hu,W.,Leskovec,J.,Jegelka,S.:Howpowerfularegraphneuralnetworks?
CoRR abs/1810.00826 (2018)
25. Zhang,Z.,Liu,Q.,Wang,H.,Lu,C.,Lee,C.K.:Motif-basedgraphself-supervised
learning for molecular property prediction. Advances in Neural Information Pro-
cessing Systems 34, 15870–15882 (2021)
26. Zhao, Q., Duan, G., Zhao, H., Zheng, K., Li, Y., Wang, J.: Gifdti: Prediction of
drug-target interactions based on global molecular and intermolecular interaction
representation learning. IEEE/ACM Transactions on Computational Biology and
Bioinformatics 20(3), 1943–1952 (2023)
27. Zhao,Q.,Yang,M.,Cheng,Z.,Li,Y.,Wang,J.:Biomedicaldataanddeeplearn-
ingcomputationalmodelsforpredictingcompound-proteinrelations.IEEE/ACM
Transactions on Computational Biology and Bioinformatics p. 2092–2110 (Jul
2022)
28. Zhao, Q., Zhao, H., Zheng, K., Wang, J.: HyperAttentionDTI: improving
drug–proteininteractionpredictionbysequence-baseddeeplearningwithattention
mechanism. Bioinformatics 38(3), 655–662 (10 2021)
29. Zitnik, M., Nguyen, F., Wang, B., Leskovec, J., Goldenberg, A., Hoffman, M.M.:
Machinelearningforintegratingdatainbiologyandmedicine:Principles,practice,
and opportunities. Inf Fusion 50, 71–91 (Oct 2019), journal Article
30. Öztürk,H.,Özgür,A.,Ozkirimli,E.:DeepDTA:deepdrug–targetbindingaffinity
prediction. Bioinformatics 34(17), i821–i829 (09 2018)