N-Agent Ad Hoc Teamwork
Caroline Wang ∗ Arrasy Rahman
caroline.l.wang@utexas.edu arrasy@cs.utexas.edu
The University of Texas at Austin The University of Texas at Austin
Ishan Durugkar Elad Liebman Peter Stone
ishan.durugkar@sony.com eliebman@sparkcognition.com pstone@cs.utexas.edu
Sony AI SparkCognition The University of Texas at Austin
and Sony AI
Abstract
Currentapproachestolearningcooperativebehaviorsinmulti-agentsettingsassume
relatively restrictive settings. In standard fully cooperative multi-agent reinforce-
ment learning, the learning algorithm controls all agents in the scenario, while in
ad hoc teamwork, the learning algorithm usually assumes control over only a single
agent in the scenario. However, many cooperative settings in the real world are
much less restrictive. For example, in an autonomous driving scenario, a company
might train its cars with the same learning algorithm, yet once on the road, these
cars must cooperate with cars from another company. Towards generalizing the
class of scenarios that cooperative learning methods can address, we introduce N-
agent ad hoc teamwork, in which a set of autonomous agents must interact and
cooperatewithdynamicallyvaryingnumbersandtypesofteammatesatevaluation
time. This paper formalizes the problem, and proposes the Policy Optimization
with Agent Modelling (POAM) algorithm. POAM is a policy gradient, multi-agent
reinforcement learning approach to the NAHT problem, that enables adaptation
to diverse teammate behaviors by learning representations of teammate behaviors.
Empirical evaluation on StarCraft II tasks shows that POAM improves coopera-
tive task returns compared to baseline approaches, and enables out-of-distribution
generalization to unseen teammates.
1 Introduction
Cooperative multi-agent reinforcement learning (MARL) is a paradigm for learning agent teams
that solve a common task via interaction with each other and the environment (Littman, 1994).
A related paradigm for learning cooperative behavior is ad hoc teamwork (AHT). In contrast to
MARL, the objective of AHT is to create a single agent policy that can collaborate with unknown
teammates to solve a common task (Mirsky et al., 2022; Stone et al., 2010).
While impressive, these advances have largely been confined to scenarios in which either complete
control over all agents is assumed, or only a single agent is adapted for cooperation (Rashid et al.,
2018; Son et al., 2019; Lin et al., 2023; Baker et al., 2019). However, real-world collaborative
scenarios—e.g. search-and-rescue, or robot fleets for warehouses—might demand agent subteams
thatareabletocollaboratewithunfamiliarteammatesthatfollowdifferentcoordinationconventions.
Towards producing agent teams that are more flexible and applicable to realistic cooperative sce-
narios, this paper proposes the novel problem setting of N-agent ad hoc teamwork (NAHT), in
which a set of autonomous agents must interact with an uncontrolled set of teammates to perform
∗Correspondingauthor.
1
4202
rpA
61
]IA.sc[
1v04701.4042:viXraFigure 1: Left: MARL algorithms assume full control over all M agents in a cooperative scenario.
Center: AHTalgorithmsoftenassumethatonlyasingleagentiscontrolledbythelearningalgorithm,
while the other M −1 agents are uncontrolled and can have a diverse, unknown set of behaviors.
Right: NAHT, the paradigm proposed by this paper, assumes that N agents (where N can vary)
are controlled by the learning algorithm, while the remaining M −N agents are uncontrolled.
a cooperative task. When there is only a single ad hoc agent, NAHT is equivalent to AHT. On
the other hand, when all ad hoc agents were produced by some MARL algorithm and there are
no uncontrolled teammates, NAHT is equivalent to MARL. Thus, the proposed problem setting
generalizes both MARL and AHT.
Drawing from ideas in both MARL and AHT, we introduce Policy Optimization with Agent Mod-
elling (POAM). POAM is a policy-gradient based approach for learning cooperative multi-agent
team behaviors, in the presence of varying numbers and types of teammate behaviors. It consists of
(1) an agent modeling network that generates a vector characterizing teammate behaviors, and (2)
an independent actor-critic architecture, which conditions on the learned teammate vectors to en-
able adaptation to a variety of potential teammate behaviors. Empirical evaluation on StarCraft II
tasksshowsthatPOAMlearnstocoordinatewithachangingnumberofteammatesofvarioustypes,
with higher competency than MARL, AHT, and NAHT baseline approaches. An evaluation with
out-of-distribution teammates also reveals that POAM’s agent modeling module enables improved
generalization to out-of-distribution teammates, compared to baseline without agent modeling.
To summarize, this paper makes the following contributions:
• Itproposes,motivates,andformalizesthenovelproblemsettingofN-agentadhocteamwork.
• It introduces Policy Optimization with Agent Modelling (POAM), an algorithm that lever-
ages ideas from AHT and MARL to enable learning cooperative team behaviors whilst
dealing with diverse behaviors from uncontrolled teammates.
• It empirically demonstrates that POAM enables improved cooperative task returns over
baselines, and generalization to out-of-distribution teammates.
2 Background
The NAHT problem is situated in the framework of Decentralized Partially Observable
Markov Decision Processes,orDec-POMDPs(Bernsteinetal.,2002;Oliehoek&Amato,2016).
ADec-POMDPconsistsofM agents, ajointstatespaceS, jointactionspaceA, per-agentobserva-
tionspacesOi, transitionfunctionT :S×A7→∆(S)∗, commonrewardfunctionr :S×A7→∆(R)
(thus defining a cooperative task), discount factor γ ∈ [0,1] and horizon T ∈ Z, which represents
the length of an interaction, or episode. Each agent receives an observation from the environment
via its personal observation function Oi :S×A7→∆(Oi). The joint state space is structured such
that S = S ×···×S , where S for i ∈ {1···M} corresponds to the state space of each agent.
1 M i
Thejointactionspaceisdefinedanalogously. DenotingHi asitssetofallpossiblelocalobservation
and individual action history, agent i acts according to a policy, π :Hi 7→∆(A ). In the following,
i i
we overload r to refer to both the reward function, and the task defined by that reward function.
∗∆(S)denotesthespaceofprobabilitydistributionsoversetS.
2Where appropriate, we use r to denote the reward at timestep t. The notation −i will be used as
t
shorthand for all the agents other than agent i.
3 NAHT Problem Formulation
DrawingfromthegoalsofMARLandAHT(Stoneetal.,2010),thegoalofN-agentad-hocteamwork
isto create a set of autonomous agents that are able to efficiently collaborate with both
known and unknown teammates to maximize return on a task. The goal is formalized
below.
LetC denoteasetofad-hocagents. IfthepoliciesoftheagentsinC aregeneratedbyanalgorithm,
we say that the algorithm controls agents in C. Since our intention is to develop algorithms for
generating the policies of agents in C, we refer to agents in C as controlled. Let U denote a set of
uncontrolled agents, which we define as all agents in the environment not included in C.† We adopt
an analogous assumption to Stone et al. (2010)—namely, that agents in U satisfy a minimal level of
competency in a team setting.
We model an open system of interaction, in which a random selection of M agents from sets C
and U must coordinate to perform task r. For illustration, consider a warehouse staffed by robots
developed by companies A and B, where there is a box-lifting task that requires three robots to
accomplish. If Company A’s robots are ad-hoc agents (corresponding to C), then some robots from
CompanyAcouldcollaboratewithrobotsfromCompanyB(correspondingtoU)toaccomplishthe
task, rather than requiring that all three robots come exclusively from A or B. Motivated thus, we
introduce a team sampling procedure X. At the beginning of each episode, X samples a team of M
agents by first sampling N < M, then sampling N agents from the set C and M −N agents from
U. We define X more formally below.
Letg(k,µ,s)representafunctionforrandomlyselectingkelementsfromagenericsetµ,conditioned
on a state s∈S. For instance, g might be the distribution, Multinomial(k,|µ|,ϕ(s)), for selecting k
elements with replacement from the set µ, parameterized by the state-dependent probability logits
ϕ(s)∈[0,1]|µ|. Allowingg todependonaninitialstateenrichestherepresentableopeninteractions.
Returning to our robot warehouse example, a state-dependent g might enable us to model that
robots suitable for heavy loads are more likely to be present near the loading dock area. For s∈S,
X is a sampling procedure parameterized by the tuple (s,M,U,C,ϕ ,g ,g ), where ϕ ∈[0,1]M
M U C M
represents the logits of a categorical distribution:
1. Sample an integer N from Cat(ϕ ).
M
2. Sample N controlled agents via g from U.
U
3. Sample M −N uncontrolled agents via g from C.
C
Since teams consisting entirely of unknown agents are not of interest, we restrict consideration to
teams containing at least one controlled agent, i.e N ≥ 1. The agent sampling functions g ,g
U C
could employ sampling with replacement to model scenarios where two robots may use the same
policy, or sampling without replacement for scenarios where that is not possible (e.g. if all robots
are heterogeneous).
Without loss of generality, let C(θ) denote a set of agents parameterized by θ. Let the notation
π(M) ∼ X(U,C) indicate sampling a team of size M from U and C. The objective of the NAHT
problem is to find parameters θ, such that C(θ) maximizes the expected return in the presence of
teammates from U:
†TheoriginalAHTproblemstatementusedtheterms“known”and“unknown”agents. However,itiscommonfor
modernAHTlearningmethodstoassumesomeknowledgeaboutthe“unknown”teammatesduringtraining(Mirsky
etal.,2022). Thus,weinsteademploythetermscontrolledanduncontrolled.
3" T #!
X
max E γtr .
π(M)∼X(U,C(θ)) t
θ
t=0
The central challenges of the NAHT problem include (1) coordinating with potentially unknown
teammates (generalization), and (2) coping with an unknown number of uncontrolled teammates
(openness).
4 A Motivating Example
Having introduced the NAHT problem, a natural question to consider is whether AHT solutions
may optimally address NAHT problems as well. If so, then there would be little need to consider
the NAHT problem setting. For instance, as an AHT solution consists of a single teammate policy,
trained in the scenario illustrated in Figure 1, a simple yet reasonable approach consists of directly
copying the AHT policy as many times as required, in an NAHT problem.
To illustrate the limitations of such an approach, we consider a concrete example in which a single
adhocagentwouldfollowarationalpolicy, buttwoadhocagentsfollowingthatsamepolicywould
lead to a suboptimal outcome. Define the following simple game for M agents: at each turn, each
agent a picks one bit b ∈ {0,1}; at the end of each turn, all the bits are summed s = P b . The
i i i i
team wins if and only if the sum of the chosen bits is exactly 1.
Consider a scenario where there are M −1 (uncontrolled) agents who follow a deterministic policy.
For simplicity, suppose they always pick 0. We now introduce a new ad hoc agent a . Assume that
h
the ad hoc agent observes the M −1 always-zero agents play for one step, and the ad hoc agent a
h
is introduced into the game in the second step. Suppose the ad hoc agent selects actions according
to the following strategy:
1. Model teammate actions by observing the last step.
2. Select the best response action.
3. If no information is known about a teammate, assume they act according to the average of
expected actions of all the other observed agents.
Iftheadhocagenta actsbasedontheteammatemodelingschemedescribedabove,itwillobserve
h
that all the other agents picked 0 at the first step, and therefore pick 1.
Now, consider a scenario in which there are M −2 always-zero agents playing without the ad hoc
agent for one step, and the ad hoc agents a1 and a2 are introduced into the game in the second
h h
step. Following the same modeling policy, the two ad hoc agents a1 and a2 will both assume that
h h
theirteammatesfollowanalways-zeropolicy,andthereforelearntopickthe1action—asuboptimal
decision, sinces=2. Followingthesecondstep, botha1 anda2 willassumethattheircounterparts
h h
pick the 1 action. As a result, both agents will switch to the action they would consider the best
response, and would both pick the 0 action, meaning once again the joint action is not optimal, as
s=0. We observe that extending the agent modeling window beyond one step does not resolve this
inherent pattern (see Appendix A.4).
While the policy above leads to optimal behavior for a single ad hoc agent, it results in poor joint
performance in the presence of another ad hoc agent. A policy that explicitly accounts for the
potentially unpredictable behavior of other agents and strives to cooperate with other agents would
lead to better performance. In the example above, finding such a policy is as simple as introducing
ϵ-greedy exploration to the ad hoc agents’ action selection behavior. With that modification, both
agents will continue exploring until the optimal reward of s = 1 is achieved, in which case both
agents will converge to an optimal behavior. This is because with high probability, the two ad hoc
agents will learn to optimally coordinate by following a joint policy that selects one of the optimal
jointactionsof(0,1)or(1,0)atsometimestep,afterwhichbothagentswillrepeatthatjointaction
due to the single-step agent modeling.
4Figure 2: The training protocol for the proposed method, POAM. POAM adopts an independent
policy gradient architecture with parameter sharing. POAM trains a single policy network, which
characterizesthebehaviorofallcontrolledagents. Theyellowagentsarecontrolledbypoliciesdrawn
from U, the set of uncontrolled agents, while the green agents are controlled by the same POAM
policy. Data from both controlled and uncontrolled agents is used to train the POAM critic, V ,
π
while the actor π(θ) is trained on data from the controlled agents only. The actor and critic are
both conditioned on a learned team encoding vector, learned via an encoder-decoder architecture.
5 Policy Optimization with Agent Modeling (POAM)
This section describes the proposed Policy Optimization with Agent Modeling (POAM) method,
which trains a collection of NAHT agents that can adaptively deal with different collections of
unknown teammates. POAM relies on an agent modeling network to initially build an encoding
vector of the collection of teammates encountered during an interaction. Adaptive agent policies
that can maximize the controlled agents’ returns are then learned by POAM by training a policy
network, which receives environment observation and team encoding vector as input. Details of the
training process for both agent modeling and policy networks are described in Sections 5.1 and 5.2
respectively, while an illustration of how POAM trains NAHT agents is provided in Figure 2.
5.1 Agent Modeling Network
Designing adaptive policies that enable NAHT agents to achieve optimal returns against any team
of uncontrolled agents drawn from some set U, requires information on the encountered team’s
unknown behavior. However, in the absence of prior knowledge about uncontrolled teammates’
policies, local observations from a single timestep may not contain sufficient information regarding
the encountered team. To circumvent this lack of information, POAM’s agent modeling network
plays a crucial role in providing team encoding vectors that characterize the behavior of teammates
in the encountered team.
We identify two main criteria for desirable team encoding vectors. First, team encoding vectors
should identify information regarding the unknown state and behavior of other agents in the envi-
ronment. Second, team encoding vectors should ideally be computable from the sequence of local
observations and actions of the team. Fulfilling both requirements provides an agent with useful
information for decision-making in NAHT problems, even under partial observability.
POAM produces desirable team encoding vectors by training a model with an encoder-decoder
architecture, which is illustrated by red components in Figure 2. The encoder, fenc : Hi 7→ Rn, is
θe
parameterized by θe and processes the modeling agent’s history of local observations and actions
5up to timestep t, ht = {ok,ak−1}t , to compute a team encoding vector, et ∈ Rn. This reliance
i i k=1 i
on local observations helps ensure that the agent modeling network can operate without having
access to the environment state (i.e. under partial observability). We then train decoder networks,
fdec : Rn 7→ O−i and fdec : Rn 7→ ∆(A ), that predicts the observations and actions of the
θo θa −i
modelled agents at timestep t, (ot ,at ), to encourage et to contain relevant information for the
−i −i i
NAHT agents’ decision-making process. Formally, this decoder is trained to minimize the following
loss function to accurately predict the modelled agents’ observations and actions:
(cid:16) (cid:17)
L (ht)= ||fdec(fenc(ht))−ot ||2−log(p(at ;fdec(fenc(ht)))) .
θe,θo,θa θo θe −i −i θa θe
5.2 Policy and Value Networks
POAM relies on an actor-critic architecture to train agent policies, where the policy and networks
are both conditioned on the teammate embedding produced by the encoder-decoder architecture
described in Section 5.1.
The policy network, πi :Hi×Rn 7→∆(A ), uses the NAHT agent’s local observation, ot, and the
θp i i
team embedding from the encoder network, et, to compute a policy followed by the NAHT agents.
i
Conditioning the policy network on et allows an NAHT agent to change its behaviour based on the
i
inferred characteristics of encountered agents. When training the policy network, we also rely on a
value network V :Hi×Rn 7→R, which measures the expected returns given h , and et. The value
θc t i
network serves as a baseline to reduce the variance of the gradient updates and also conditions on
the learned teammate embeddings, for similar reasons to the policy.
POAMthentrainsthepolicyandvaluenetworksusinganapproachbasedofftheIndependentPPO
algorithm (Yu et al., 2022) (IPPO). IPPO is selected as the base MARL algorithm for two reasons.
First, using an independent MARL method circumvents the need to deal with the changing number
of agents resulting from environment openness. Second, it has been demonstrated to be effective on
various MARL tasks. POAM trains the value network to produce accurate state value estimates by
minimizing the following loss function:
L (ht)=
1(cid:16)
V
(ht,fenc(ht))−Vˆt(cid:17)2
,
θc 2 θc θe
where Vˆ t = PT γlrt+l. The policy network is trained to minimize the PPO loss function, where
l=0
the Generalized Advantage Estimator is employed (Schulman et al., 2015):
L (ht)=min(cid:16) rt(θp)Aˆt ,clip(cid:0) rt(θp),1−ϵ,1+ϵ(cid:1) Aˆt (cid:17) −αH(πi (.|ht,fenc(ht))),
θp GAE GAE θp θe
πi (at|ht,fenc(ht))
where rt(θp)= θp i θe .
πi (at|ht,fenc(ht))
θpold i θe
Leveraging data from uncontrolled agents In the NAHT setting, we assume access to the
joint observationsandactionsgeneratedbythecurrentteamdeployedintheenvironment,wherethe
team consists of a mix of controlled and uncontrolled agents. POAM leverages the data from both
controlled and uncontrolled agents to train the value network—in effect, treating the uncontrolled
agents as exploration policies. This is a significant departure from PPO, which is a fully on-policy
algorithm. However, onlydatafromthecontrolledagentsisusedtotrainthepolicynetwork, asthe
policy update is highly sensitive to off-policy data.
6 Experiments and Results
This section presents an empirical evaluation of POAM and baseline approaches across different
NAHTproblems. Wespecificallyinvestigatetwoquestions. First,whetherPOAMleadstoimproved
6solutionsforNAHToverbaselines,intermsofsampleefficiencyorasymptoptictaskreturn. Second,
whether agent modeling enables generalization to previously unseen teammates.
We start this section by introducing the main experimental procedures and baselines in Section 6.1.
Section6.2presentsourmainresultthatcomparesPOAMandthebaselinesinNAHTproblemssit-
uatedacrossdifferentenvironments. Sections6.3&6.4thenprovideadditionalanalysisontheteam
encoding vectors produced by POAM and its ability to generalize the learned policy to previously
unseen sets of teammate policies. Finally, see the Appendix for further results, hyperparameters,
and implementation details. Our code will be made available upon publication.
6.1 Experimental Design
This subsection describes the experimental design, including the particular NAHT instance consid-
ered in the experiments, training procedure, experimental domain, and baselines.
6.1.1 A Practical Instantiation of NAHT
Similar to AHT, the NAHT problem can be param-
eterized by the amount of knowledge that controlled
agents have about uncontrolled agents, and whether
uncontrolledagentscanadapttothebehaviorofcon-
trolled agents (Barrett & Stone, 2012). This sec-
tion considers a practical instantiation of the NAHT
problem, designed to assess whether the baseline al-
gorithmshavesuccessfullylearnedtocooperatewith
each of the uncontrolled agents under consideration.
Recall that U and C denote the sets of uncontrolled
and controlled agent policies, respectively, while M
denotes the total team size, and N the number of
agents sampled from C. For simplicity, we assume
that policies in U do not adapt to the controlled
agents’ behavior, agents are homogeneous (so that
they are interchangeable within a team), and that
agents cannot communicate. While in principle, the
Figure 3: A practical instantiation of the
team sampling procedure X could define any prob-
NAHT problem.
ability distribution over teams constructed from U
and C, we consider an X consisting of sampling
train
N uniformly from {1,··· ,M −1}, sampling N agents from C and M −N agents from U in a
uniform fashion (Figure 3). The sampling procedure described here takes place at the beginning
of each episode to select a team, which is then deployed in the environment. Data generated by
the deployed team (e.g. joint observations, joint actions, and rewards) are returned to the learning
algorithm. See Appendix A.1.2 for further details on the evaluation procedure.
6.1.2 Generating Uncontrolled Teammates
To generate a set of uncontrolled teammates U, the following MARL algorithms are used to train
agent teams: VDN (Sunehag et al., 2018), QMIX (Rashid et al., 2018), IQL (Tan, 1997), IPPO,
andMAPPO(Yuetal.,2022). Weverifythatthegeneratedteambehaviorsarediversebychecking
that (1) teams trained by the same algorithm learn non-compatible coordination conventions, and
(2) teams trained by different algorithms are not compatible (see Appendix A.2).
LetU denotethesetofuncontrolledteammatesusedtotrainPOAMandthebaselinemethods.
train
U consists of five teams, where each individual team is trained via VDN, QMIX, IQL, IPPO
train
and MAPPO, respectively. U consists of a set of holdout teams trained via the same MARL
test
7algorithms, but that have not been seen during training. The experimental results reported in 6.2
and 6.3 are computed with respect to U only, while the experimental results in 6.4 use U .
train test
6.1.3 Experimental Domain
ExperimentsareconductedontheStarCraftMulti-AgentChallenge(SMAC)benchmark(Samvelyan
et al., 2019). SMAC features a set of cooperative tasks, where a team of allied agents must defeat
a team of enemy agents controlled by the game server. It is a partially observable domain with a
continuous state space, where each agent can observe features about itself, enemy, and allied agents
within some radius. The action space is discrete—each agent can choose an enemy to attack, a
direction to move in, or to not perform any action. The SMAC tasks considered here are 5v6,
8v9, 10v11, 3s4z, 3s5z. Thefirstnumberindicatesthenumberofalliedagents,whilethesecond
indicates the number of enemy agents; full task names and descriptions are provided in App. A.1.
6.1.4 Baselines
As NAHT is a new problem proposed by this paper, there are no prior algorithms that are directly
designed for the NAHT problem. Therefore, we construct three baselines to contextualize the
performance of POAM.
• Naive MARL: various well-known MARL algorithms are considered, including both independent
and centralized training with decentralized execution algorithms (Hernandez-Leal et al., 2019).
The algorithms evaluated here include IQL (Claus & Boutilier, 1998), VDN (Sunehag et al.,
2018), QMIX Rashid et al. (2018), IPPO, and MAPPO (Yu et al., 2022). The MARL baselines
aretrainedinself-playandthenevaluatedintheNAHTsetting. Inthefollowing, wereportonly
the performance of the best naive MARL baseline.
• Independent PPO in the NAHT setting (IPPO-NAHT): IPPO is a policy gradient MARL algo-
rithm that directly generalizes PPO (Schulman et al., 2017) to the multi-agent setting. It was
found to be surpringly effective on a variety of MARL benchmarks by Yu et al. (2022). In con-
trast to the naive MARL baselines, IPPO-NAHT is trained using the proposed NAHT training
scheme. The variant considered here employs full parameter sharing, where the actor is trained
ondataonlyfromcontrolledagents, butthecriticistrainedusingdatafrombothcontrolledand
uncontrolledagents. ThelatterdetailisakeyalgorithmicfeaturewhichPOAMalsoemploys,but
is an extension from the most naive version of PPO (see Section 6.3). IPPO can be considered
an ablation of POAM, where the agent modeling module is removed.
• POAMintheAHTsetting(POAM-AHT):AsconsideredinSection4,anaturalbaselineapproach
totheNAHTproblemistodeployAHTpoliciesdirectlyintheNAHTsetting. Toexperimentally
evaluatetheintuitionthatAHTpoliciesdonotsufficefortheNAHTproblemsetting,weconsider
an AHT version of POAM. POAM-AHT is trained identically to POAM, but where the number
of controlled agents N is always equal to 1 during training.
Allmethodsemployfullparametersharing(Papoudakisetal.,2021b). SeeAppendixA.1fordetails
about architecture, hyperparameter tuning and algorithm implementation.
6.2 Main Results
TheprimaryquestionaddressedhereiswhetherPOAMimprovesperformanceontheNAHTproblem
setting, as compared to baselines. Figure 4 shows the learning curves of POAM and IPPO-NAHT
on the SMAC tasks, and the test returns achieved by the best naive MARL baseline and POAM-
AHT. See the Appendix for tables providing the performances of all naive MARL methods, across
all tasks. All learning curves consist of the mean test returns across three trials, while the shaded
regions reflect the 95% confidence intervals. All methods are trained for 20M time steps. We find
that for all tasks, POAM outperforms baselines in either asymptotic return (5v6, 3s5z), or sample
85v6 8v9 3s5z 10v11
20
15.0 20
15
12.5 15 15
10.0
10
7.5 10 10 poam
ippo
5.0 5 p naoa ivm e mah at rl
0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0 0.0 0.5 1.0 1.5 2.0
Timesteps 1e7 Timesteps 1e7 Timesteps 1e7 Timesteps 1e7
Figure 4: POAM consistently improves over the baselines of IPPO-NAHT, POAM-AHT, and the
best naive MARL baseline in all tasks, in either sample efficiency or asymptotic return. The perfor-
mance of POAM-AHT and naive MARL are inconsistent across tasks. Results are computed over
three trials, while the shaded region represents the 95% confidence interval.
efficiency (8v9, 10v11). While POAM initially has lower sample efficiency than IPPO-NAHT, this
trend is reversed after the first few million steps, after which POAM displays higher return. We
attribute this to the initial costs incurred by learning team encoding vectors, which once learned,
improves learning efficiency. IPPO-NAHT, which can be viewed as an ablation of POAM with no
agent modeling, is the next best performing method. Although IPPO-NAHT has generally poorer
sample efficiency than POAM, the method converges to the approximately the same returns on 8v9
and 10v11. While the agent modeling module of POAM provides team embedding vectors to the
policylearningprocess,theembeddingsarethemselvesproducedfromeachagent’sownobservation.
Since no additional information is provided to POAM agents, it is unsurprising that IPPO-NAHT
can converge to similar solutions as POAM, given enough training steps.
Finally, while the best naive MARL baseline
and POAM-AHT learn good solutions on some
Average MSE Over Episode Average Prob Over Episode
tasks,neithermethodconsistentlyperformswell 1.0
across all tasks. Overall, POAM discovers the 0.20
mostconsistentlyperformantpoliciescompared 0.15 0.8
totheothermethods. Weconcludethat(1)the 0.10 0.6
proposed learning scheme can learn effectively
0.05
in the presence of multiple types of agents, and 0.4
0.00
(2) agent modeling improves learning efficiency.
0 20 40 60 0 20 40 60
Episode Timestep Episode Timestep
train ts (rounded to nearest million)
6.3 A Closer Look at POAM 0m 4m 8m 12m 16m
2m 6m 10m 14m 18m
TwokeyaspectsofPOAMaretheuseofarecur- Figure 5: Evolution of the within-episode mean
rentteammatemodelingmodule,andtheuseof squared error (left) and probability of actions
datafromuncontrolledagentstotrainthecritic. that were actually taken by modelled teammates
Weperformfurtheranalysisofbothoftheseas- (right), computed by the POAM agent over the
pects on the 5v6 task. course of training. The agent modeling perfor-
mance improves over the course of an episode, as
Teammate modeling performance In the more data about teammate behavior is observed.
NAHTtraining/evaluationsetting, anewsetof
teammates is sampled at the beginning of each episode. Therefore, an important subtask for a
competentNAHTagentistorapidlymodelthedistributionandtypeofteammatesatthebeginning
of the episode, to enable the policy to exploit that knowledge as the episode progresses. This task
is especially challenging in the presence of partial observability (a property of the SMAC tasks).
To address the above challenges, POAM employs a recurrent encoder, which encodes the POAM
agent’s history of observations and actions to an embedding vector, and a (non-recurrent) decoder
network, which predicts the egocentric observations and actions for all teammates.
A natural question then, is whether the learned teammate encoding vectors actually improve over
the course of an episode. Figure 5 depicts the within-episode mean squared error (MSE) of the
9
naeM
nruteR
tseT
ESM
egarevA
borP
egarevAobservationpredictedbythedecoder,andthewithin-episodeprobability oftheactionactuallytaken
by the modelled teammates, according to the decoder’s modelled action distribution.
Each curve corresponds to a different training checkpoint. We observe the MSE rapidly converges
to a low value, while the predicted probability of the taken actions has converged to 1 by time step
20. Note that the MSE is initially 0 at timestep 0 because the initial position is not randomized
in SMAC. Next, we observe that the modeling performance improves over the course of training,
indicated by lower prediction error at later training steps. Thus, we conclude that POAM learns
accurate teammate models relatively early in the episode for the 5v6 task—successfully coping with
the challenges introduced by the sampled teammates and partial observability.
Impact of data from non-controlled agents Recall
thatbothPOAMandIPPO-NAHTupdatethevaluenet-
5v6
workusingdatafromboththecontrolledanduncontrolled
agents. AsFigure6shows,thisalgorithmicfeatureresults
15
in a significant performance gain over training using on-
policy data only, for both POAM and IPPO-NAHT.
10
6.4 Out of Distribution Generalization
5
Experiments in prior sections were conducted with in-
distribution teammates, i.e. teammates that the methods
0.0 0.5 1.0 1.5 2.0
Timesteps 1e7 interacted directly with during training. This section ex-
amines whether agent modeling improves generalization
poam ippo-naht
poam w/o ucd ippo-naht w/o ucd toout-of-distribution(OOD)teammates,wheretheOOD
teammatesaregeneratedbyusingMARLalgorithmswith
Figure 6: Learning curves of POAM
different random seeds to generate teammates that have
and IPPO, where the value net-
not been interacted with during training.‡
work is trained with and without
uncontrolled agents’ data (UCD). Figure 7 shows the mean and standard error of the test
return achieved by POAM, compared to IPPO-NAHT,
when the algorithm in question is paired with previously
unseen seeds of IPPO, IQL, MAPPO, QMIX, and VDN (computed across three trials). In two
out of four tasks (5v6, 3s5z), POAM has a significantly higher return than IPPO-NAHT, while
the remaining two tasks exhibit a smaller improvement. For all tasks, we observe that the standard
errorforPOAMislowerthanthatofIPPO-NAHT,showingthatPOAMperformsmoreconsistently
against a range of teammates and team configurations than IPPO-NAHT.
‡WedemonstrateinApp. A.2thatinSMACtasks,thesameMARLalgorithmwithdifferentseedstrainsteams
thathavedifferentcoordinationconventions. Hence,teamsfromunseenseedsdisplayOODbehavior.
OOD Evaluation
5v6 8v9 3s5z 10v11
20
15.0 20
12.5 15 15
15
10.0
7.5 10 10 10
5.0
5 5 5
2.5
ippo-naht poam
0.0 0 0 0
ippo iql mappoqmix vdn ippo iql mappoqmix vdn ippo iql mappoqmix vdn ippo iql mappoqmix vdn
Target Algorithms Target Algorithms Target Algorithms Target Algorithms
Figure7: MeanandstandarderroroftestreturnsachievedbyPOAMandIPPO-NAHT,whenpaired
with out-of-distribution teammates. POAM shows improved generalization to OOD teammates, as
compared to IPPO-NAHT, across all tasks.
10
naeM
nruteR
tseT
nruteR
tseT
naeM7 Related Works
This section summarizes related work in the areas most closely related to NAHT, namely, ad hoc
teamwork, zero-shot coordination, agent modeling, and cooperative MARL.
AdHocTeamwork&Zero-ShotCoordination. Priorworksinadhocteamwork(Stoneetal.,
2010)andzero-shotcoordination(ZSC)(Huetal.,2020)exploredmethodstodesignadaptiveagents
that can optimally collaborate with unknown teammates. While they both highly resemble the
NAHTproblem, existingmethodsforAHT(Rahmanetal.,2021;Mirskyetal.,2022)andZSC(Hu
et al., 2020; Lupu et al., 2021) have been limited to single-agent control scenarios. We argue that
direct,naiveapplicationsofAHTandZSCtechniquestoourproblemofinterestareineffective—see
the discussion in Section 4 and results in Section 6.
Recent research in AHT and ZSC utilizes neural networks to improve agent collaboration within
various team configurations. These recent works mostly focus on two approaches. The first ap-
proach trains the agent to adapt to unknown teammates by characterizing teammates’ behavior
as fixed-length vectors using neural networks and learning a policy network conditioned on these
vectors (Rahman et al., 2021; Papoudakis et al., 2021a; Zintgraf et al., 2021). The second designs
teammate policies that maximize the agent’s performance when collaborating with diverse team-
mates (Lupu et al., 2021; Rahman et al., 2021). Our work builds on the first category, extending it
tocontrolmultipleagents. WhilethisalsooffersapotentialpathforrobustNAHTagents,designing
teammate policies for training will be kept as future work.
Evaluating Agents’ Cooperative Capabilities. Beyond training agents to collaborate with
teammateshavingunknownpolicies, researchershavedevelopedenvironmentsandmetricstoassess
cooperativeabilities. TheMeltingPotsuite(Leiboetal.,2021;Agapiouetal.,2022)mostlyevaluates
controlled agents’ ability to maximize utilitarian welfare against unknown agents. However, this
evaluation suite focuses on mixed-motive games where agents may have conflicting goals. This
contrastswithourwork’sscopeoffullycooperativesettings,whereallagentssharethesamereward
function. MacAlpine&Stone(2017)alsoexploredalternativemetricstomeasureagents’cooperative
capabilities while disentangling the effects of their overall skills in drop-in RoboSoccer.
Agent Modeling. Agent modeling (Albrecht & Stone, 2018) helps us characterize other agents
basedontheiractions. Suchcharacterizationscouldattempttoinferimportantattributesofanother
agent’sdecisionmakingprocesssuchastheirgoals(Hannaetal.,2021)orpolicies(Papoudakisetal.,
2021a). POAM and most existing AHT techniques rely on agent modeling to provide important
teammate information for decision-making when collaborating with unknown teammates.
Cooperative MARL. Cooperative Multi-Agent Reinforcement Learning (MARL) explores algo-
rithms for training agent teams on fully cooperative tasks. Some existing methods focus on credit
assignment and decentralized control (Foerster et al., 2018; Rashid et al., 2018). Other works in co-
operativeMARLalsoleverageparametersharingandroleassignment(e.g.,Christianosetal.(2021);
Yangetal.(2022))todecideanoptimaldivisionoflaborbetweenagents. However,thesetechniques
assume control over all existing agents during training and evaluation. This limits these techniques’
effectiveness in settings with new teammates (NAHT problem) as shown in prior work (Vezhnevets
et al., 2020; Hu et al., 2020; Rahman et al., 2021).
8 Conclusion
This paper proposes and formulates the problem of N-agent ad-hoc teamwork (NAHT), a general-
izationofbothAHTandMARL.Itfurtherproposesamulti-agentreinforcementlearningalgorithm
totrainNAHTagentscalledPOAM,anddevelopsaproceduretotrainandevaluateNAHTagents.
POAMisapolicygradientapproachthatusesanencoder-decoderarchitecturetoperformteammate
modeling,andleveragesdatafromuncontrolledagentsforpolicyoptimization. Empiricalvalidation
11on StarCraft II tasks shows that POAM consistently improves over baseline NAHT approaches, in
terms of sample efficiency, asymptotic return, and generalization to out-of-distribution teammates.
WhilethisworkexploresteammatemodelingforNAHT,otheralgorithmicideasfromAHTprovide
a rich set of future directions. These include generating teammates for the NAHT setting, alter-
native teammate modeling schemes, and considering communication protocols for NAHT agents.
Limitations of POAM also suggest directions for future work. For instance, POAM is an inde-
pendent MARL method (albeit with parameter sharing)—how might one leverage knowledge from
the global state to generate improved NAHT policies? Further, POAM’s actor update is purely
on-policy, and therefore cannot leverage data generated by uncontrolled agents. Future work may
consider off-policy learning methods as a solution strategy.
Acknowledgments
This work has taken place in the Learning Agents Research Group (LARG) at UT Austin. LARG
researchissupportedinpartbyNSF(FAIN-2019844,NRT-2125858),ONR(N00014-18-2243),ARO
(E2061621), Bosch, LockheedMartin, andUTAustin’sGoodSystemsgrandchallenge. PeterStone
serves as the Executive Director of Sony AI America and receives financial compensation for this
work. The terms of this arrangement have been reviewed and approved by the University of Texas
at Austin in accordance with its policy on objectivity in research.
References
JohnPAgapiou,AlexanderSashaVezhnevets,EdgarADuéñez-Guzmán,JaydMatyas,YiranMao,
Peter Sunehag, Raphael Köster, Udari Madhushani, Kavya Kopparapu, Ramona Comanescu,
et al. Melting pot 2.0. arXiv preprint arXiv:2211.13746, 2022.
Stefano V Albrecht and Peter Stone. Autonomous agents modelling other agents: A comprehensive
survey and open problems. Artificial Intelligence, 258:66–95, 2018.
StefanoV.Albrecht, FilipposChristianos,andLukasSchäfer. Multi-Agent Reinforcement Learning:
Foundations and Modern Approaches. MIT Press, 2024. URL https://www.marl-book.com.
Bowen Baker, Ingmar Kanitscheider, Todor Markov, Yi Wu, Glenn Powell, Bob McGrew, and
Igor Mordatch. Emergent Tool Use From Multi-Agent Autocurricula. In International Confer-
ence on Learning Representations, September 2019. URL https://openreview.net/forum?id=
SkxpxJBKwS.
Samuel Barrett and Peter Stone. An analysis framework for ad hoc teamwork tasks. In Proceedings
of the 11th International Conference on Autonomous Agents and Multiagent Systems-Volume 1,
pp. 357–364, 2012.
Daniel S. Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The Complexity of
Decentralized Control of Markov Decision Processes. Mathematics of Operations Research, 27
(4):819–840, November 2002. ISSN 0364-765X. doi: 10.1287/moor.27.4.819.297. URL https:
//pubsonline.informs.org/doi/10.1287/moor.27.4.819.297.
FilipposChristianos,GeorgiosPapoudakis,MuhammadARahman,andStefanoVAlbrecht.Scaling
multi-agent reinforcement learning with selective parameter sharing. In International Conference
on Machine Learning, pp. 1989–1998. PMLR, 2021.
Caroline Claus and Craig Boutilier. The dynamics of reinforcement learning in cooperative mul-
tiagent systems. In Proceedings of the fifteenth national/tenth conference on Artificial intelli-
gence/Innovative applications of artificial intelligence, AAAI ’98/IAAI ’98, pp. 746–752, USA,
July 1998. American Association for Artificial Intelligence. ISBN 978-0-262-51098-1.
12Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson.
Counterfactual multi-agent policy gradients. In Proceedings of the AAAI conference on artificial
intelligence, volume 32, 2018.
Josiah P Hanna, Arrasy Rahman, Elliot Fosong, Francisco Eiras, Mihai Dobre, John Redford, Sub-
ramanian Ramamoorthy, and Stefano V Albrecht. Interpretable goal recognition in the presence
of occluded factors for autonomous vehicles. In 2021 IEEE/RSJ International Conference on
Intelligent Robots and Systems (IROS), pp. 7044–7051. IEEE, 2021.
Pablo Hernandez-Leal, Bilal Kartal, and Matthew E. Taylor. A Survey and Critique of Multia-
gent Deep Reinforcement Learning. Autonomous Agents and Multi-Agent Systems, 33(6):750–
797, November 2019. ISSN 1387-2532, 1573-7454. doi: 10.1007/s10458-019-09421-1. URL
http://arxiv.org/abs/1810.05587. arXiv:1810.05587 [cs].
Hengyuan Hu, Adam Lerer, Alex Peysakhovich, and Jakob Foerster. “other-play” for zero-shot
coordination. In International Conference on Machine Learning, pp. 4399–4410. PMLR, 2020.
Joel Z Leibo, Edgar A Dueñez-Guzman, Alexander Vezhnevets, John P Agapiou, Peter Sunehag,
Raphael Koster, Jayd Matyas, Charlie Beattie, Igor Mordatch, and Thore Graepel. Scalable
evaluationofmulti-agentreinforcementlearningwithmeltingpot. InInternational conference on
machine learning, pp. 6187–6199. PMLR, 2021.
Fanqi Lin, Shiyu Huang, Tim Pearce, Wenze Chen, and Wei-Wei Tu. TiZero: Mastering Multi-
AgentFootballwithCurriculumLearningandSelf-Play. In Proceedings of the 2023 International
Conference on Autonomous Agents and Multiagent Systems, AAMAS ’23, pp. 67–76, Richland,
SC, May 2023. International Foundation for Autonomous Agents and Multiagent Systems. ISBN
978-1-4503-9432-1.
Michael L Littman. Markov games as a framework for multi-agent reinforcement learning. In
Machine learning proceedings 1994, pp. 157–163. Elsevier, 1994.
Andrei Lupu, Brandon Cui, Hengyuan Hu, and Jakob Foerster. Trajectory diversity for zero-shot
coordination. In International conference on machine learning, pp. 7204–7213. PMLR, 2021.
Patrick MacAlpine and Peter Stone. Evaluating ad hoc teamwork performance in drop-in player
challenges. In Gita Sukthankar and Juan A. Rodriguez-Aguilar (eds.), Autonomous Agents and
Multiagent Systems, AAMAS 2017 Workshops, Best Papers, pp. 168–186. Springer International
Publishing, 2017. URL http://nn.cs.utexas.edu/?LNAI17-MacAlpine.
Reuth Mirsky, Ignacio Carlucho, Arrasy Rahman, Elliot Fosong, William Macke, Mohan Sridharan,
Peter Stone, and Stefano V Albrecht. A survey of ad hoc teamwork research. In European
Conference on Multi-Agent Systems, pp. 275–293. Springer, 2022.
Frans A. Oliehoek and Christopher Amato. A Concise Introduction to Decentralized POMDPs.
Springer Publishing Company, Incorporated, 1st edition, 2016. ISBN 3319289276.
GeorgiosPapoudakis,FilipposChristianos,andStefanoV.Albrecht. Agentmodellingunderpartial
observability for deep reinforcement learning. In Advances in Neural Information Processing
Systems, 2021a.
Georgios Papoudakis, Filippos Christianos, Lukas Schäfer, and Stefano V. Albrecht. Benchmarking
Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks. In Advances in
Neural Information Processing Systems, volume 34, 2021b.
Arrasy Rahman, Niklas Höpner, Filippos Christianos, and Stefano V. Albrecht. Towards Open Ad
Hoc Teamwork Using Graph-based Policy Learning. In Proceedings of the 38 th International
Conference on Machine Learning, volume 139. PMLR, June 2021.
13TabishRashid,MikayelSamvelyan,ChristianSchroeder,GregoryFarquhar,JakobFoerster,andShi-
monWhiteson. QMIX:Monotonicvaluefunctionfactorisationfordeepmulti-agentreinforcement
learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of
Proceedings of Machine Learning Research. PMLR, 2018.
Mikayel Samvelyan, Tabish Rashid, Christian Schroeder de Witt, Gregory Farquhar, Nantas
Nardelli, Tim G. J. Rudner, Chia-Man Hung, Philiph H. S. Torr, Jakob Foerster, and Shimon
Whiteson. The StarCraft Multi-Agent Challenge. CoRR, abs/1902.04043, 2019.
JohnSchulman,PhilippMoritz,SergeyLevine,MichaelI.Jordan,andP.Abbeel. High-dimensional
continuous control using generalized advantage estimation. CoRR, abs/1506.02438, 2015. URL
https://api.semanticscholar.org/CorpusID:3075448.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimization algorithms. ArXiv, abs/1707.06347, 2017. URL https://api.semanticscholar.
org/CorpusID:28695052.
Kyunghwan Son, Daewoo Kim, Wan Ju Kang, David Earl Hostallero, and Yung Yi. QTRAN:
Learning to Factorize with Transformation for Cooperative Multi-Agent Reinforcement Learning.
InProceedings of the 36th International Conference on Machine Learning,pp.5887–5896.PMLR,
May 2019. URL https://proceedings.mlr.press/v97/son19a.html.
Peter Stone, Gal Kaminka, Sarit Kraus, and Jeffrey Rosenschein. Ad Hoc Autonomous Agent
Teams: Collaboration without Pre-Coordination. In Proceedings of the AAAI Conference on
Artificial Intelligence, volume 24, pp. 1504–1509, July 2010. doi: 10.1609/aaai.v24i1.7529. URL
https://ojs.aaai.org/index.php/AAAI/article/view/7529.
D.J.Strouse,KevinR.McKee,MattBotvinick,EdwardHughes,andRichardEverett.Collaborating
with Humans without Human Data. January 2022. URL http://arxiv.org/abs/2110.08176.
arXiv:2110.08176 [cs].
PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,ViniciusZambaldi,Max
Jaderberg,MarcLanctot,NicolasSonnerat,JoelZ.Leibo,KarlTuyls,andThoreGraepel. Value-
decompositionnetworksforcooperativemulti-agentlearningbasedonteamreward.InProceedings
of the 17th International Conference on Autonomous Agents and Multi Agent Systems, AAMAS
’18, 2018.
Ming Tan. Multi-agent reinforcement learning: Independent versus cooperative agents. In In-
ternational Conference on Machine Learning, 1997. URL https://api.semanticscholar.org/
CorpusID:267858156.
Alexander Vezhnevets, Yuhuai Wu, Maria Eckstein, Rémi Leblond, and Joel Z Leibo. Options as
responses: Grounding behavioural hierarchies in multi-agent reinforcement learning. In Interna-
tional Conference on Machine Learning, pp. 9733–9742. PMLR, 2020.
Mingyu Yang, Jian Zhao, Xunhan Hu, Wengang Zhou, Jiangcheng Zhu, and Houqiang Li. Ldsa:
Learningdynamicsubtaskassignmentincooperativemulti-agentreinforcementlearning.Advances
in Neural Information Processing Systems, 35:1698–1710, 2022.
Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising
effectivenessofmappoincooperativemulti-agentgames. InProceedingsoftheNeuralInformation
Processing Systems Track on Datasets and Benchmarks, 2022.
LuisaZintgraf,SamDevlin,KamilCiosek,ShimonWhiteson,andKatjaHofmann. Deepinteractive
bayesian reinforcement learning via meta-learning. arXiv preprint arXiv:2101.03864, 2021.
14A Appendix
A.1 Experiment Details
A.1.1 SMAC Tasks
The SMAC tasks considered in this paper are described in more detail below:
• 5v6: stands for 5m vs 6m, five allied Marines versus six enemy Marines.
• 8v9: stands for 8m vs 9m, eight allied Marines versus nine enemy Marines.
• 10v11: stands for 10m vs 11m, ten allied Marines versus eleven enemy Marines.
• 3s5z: stands for 3s vs 5z, three allied Stalkers versus five enemy Zealots.
A.1.2 Evaluation Details
The experiments in the main paper consider a particular team sampling procedure called X ,
train
which uniformly samples N and uniformly samples agents from the sets U and C. While the
learning curves reported in the main paper are computed directly with X , we use an iterative
train
anddeterministicevaluationprocedurefortheresultspresentedinSection6.4andintheAppendix.
The purpose of the additional evaluation procedure presented here is to ensure that we thoroughly
evaluate the trained NAHT agents for all possible values of N and all possible teammates.
Given a set of uncontrolled agents U and a set of agents C generated by some NAHT method, we
compute the following M −N cross-play scores. Let N be the number of agents sampled from C,
such that N < M. For N ∈ {1,··· ,M −1}, construct the joint policy π(N) by selecting N agents
uniformlyfromC andM−N agentsfromU. EvaluatetheresultantteamonthetaskforE episodes.
This results in M −1 episode returns, on which we may compute summary statistics. If agents in
U and C are generated by algorithm A and B, respectively, then we call the thus computed average
return, the M −N cross-play score.
More concretely: each algorithm (VDN, QMIX, QMIX-NS, IQL, IQL-NS) is run k times with
different seeds, to generate k teams of teammates to act as uncontrolled agents. For each pair of
algorithms, we sample a subset of the possible seed pairs, and evaluate the teams that result from
merging said seed pairs. Note that each seed actually represents a team of agents. For example, if
VDN and QMIX have seeds 1 2, and 3, the cross-play evaluation might consider the seed pairings
(VDN 1, QMIX 2), (VDN 2, QMIX 3), (VDN 3, QMIX 1). Given a pair of seeds (e.g. (VDN 1),
(QMIX 2)), the M −N cross-play score is computed as the average return generated by sweeping
N ∈ {1,··· ,M −1} and evaluating the merged team that consists of selecting N agents from the
first team, and M −N agents for E episodes. In our experiments, E =128, and k =3.
Means and standard errors are computed over all seed pairings considered, as we treat each M−N
evaluation as an independent trial. For most experiments, three seed pairs are considered, where
seeds are paired to ensure that all seeds participate in at least one evaluation. This ensures that
the computational cost of the evaluation remains linear in k, rather than quadratic. However, note
that for the OOD experiments presented in Section 6.4, we consider all possible seed pairings for
the most comprehensive evaluation.
Self-play scores. For algorithm pairs that are the same, seeds are paired with themselves. For
example, the seed pairs for VDN versus VDN would be (1,1),(2,2),(3,3). The reason for this is
thatweareinterestedintheperformanceofagentswhenpairedwithknownteammates. Thisisthe
model self-play score, in contrast to the algorithm self-play score (Albrecht et al., 2024).
15A.1.3 Algorithm Implementation
The experiments in this paper use algorithm implementations from the ePyMARL codebase (Pa-
poudakis et al., 2021b). The value-based methods are used without modification (i.e. IQL, VDN,
QMIX), but we implement our own version of policy gradient methods (IPPO, MAPPO), based on
the implementation of (Yu et al., 2022).
All methods employ recurrent actors and critics, with full parameter sharing, i.e. all agents are
controlled by the same policy, where the agent id is input to the policy/critic networks, to allow
behavioral differentiation between agents. For policy gradient methods, the policy architecture is
two fully connected layers, followed by an RNN (GRU) layer, followed by an output layer. Each
layer has 64 neurons with ReLU activation units, and employs layer normalization. The critic
architecture is the same as the policy architecture. The value-based networks employ the same
architecture, except that there is only a single fully connected layer before the RNN layers, and
layer normalization is not used (following the ePyMARL implementation). Please consult the code
base for full implementation details.
Hyperparameters. Forthevalue-basedmethods,defaulthyperparametersareused. Wetunethe
hyperparametersofthepolicygradientmethodsonthe5v6task,andapplythoseparametersdirectly
to the remaining SMAC tasks. The hyperparameters considered for policy gradient algorithms are
given in Table 1. POAM adopts the same hyperparameters as IPPO where applicable. We also
tuned additional hyperparameters specific to POAM (see Table 2).
Algorithm Buffer size Epochs Minibatches Entropy Clip Clip
value loss
IPPO 128, 256, 512 1, 4, 10 1, 3 0.01, 0.01, 0.05, 0.1 no, yes
0.03,
0.05
MAPPO 64, 128, 256 4, 10 1, 3 0.01, 0.05, 0.1, 0.2 no, yes
0.03,
0.05, 0.07
Table 1: Hyperparameters evaluated for the policy gradient algorithms. Selected values are bolded
Algorithm ED epochs ED Minibatches ED LR
POAM 1, 5, 10 1, 2 0.0005, 0.005
Table2: AdditionalhyperparametersevaluatedforPOAM;notethatEDstandsforencoder-decoder.
Selected values are bolded.
A.2 Supplemental Results
A.2.1 On coordination conventions in StarCraft II
The experimental procedure detailed in Section 6.1 generates diverse teammates in SMAC by using
MARL algorithms to train multiple teams of agents. Two underlying assumptions of the procedure
are that (1) teams trained by the same algorithm learn non-compatible coordination conventions,
and (2) teams trained by different algorithms are not compatible. Both points are experimentally
verified in this section.
Self-play with MARL algorithms. For the SMAC tasks under consideration, teams trained
using the same MARL algorithm, but different seeds, can converge to distinct coordination conven-
tions. Figure8demonstratesthisbydepictingthereturnofteamstrainedtogether(matchedseeds)
versus those of teams that were not trained together (mismatched seeds), across all naive MARL
16Matched vs Mismatched Seed Self Play Evaluation
5v6 8v9 3s5z 10v11
17.5 20 20 20
15.0 15 15 15
12.5
10.0 10 10 10
7.5
5.0 5 5 5
matched
2.5 mismatched
0.0 0 0 0
ippo iql mappo qmix vdn ippo iql mappo qmix vdn ippo iql mappo qmix vdn ippo iql mappo qmix vdn
Algorithms Algorithms Algorithms Algorithms
Figure 8: Agent teams that were trained together using the same algorithm (matched seeds) have
higher returns than teams that were not trained together but were trained with the same algorithm
(mismatched seeds).
algorithms (IPPO, IQL, MAPPO, QMIX, VDN) and tasks considered. Overall, the returns of the
teams that were trained together are higher than those not trained together.
The general phenomenon has been previously observed and exploited by prior works in ad hoc
teamwork (Strouse et al., 2022). This paper takes advantage of this to generate a set of diverse
teammatesfortheStarCraftIIexperiments. Weselecttaskswheretheeffectissignificant,toensure
that there are distinct coordination behaviors for POAM to model. Tasks that were considered but
subsequently ruled out for this reason include 3m vs 3m, 8m vs 8m, and 6h vs 8z.
Cross-play with MARL algorithms Tables 9, 10, 11, and 12 display the full cross-play results
for all MARL algorithms on all tasks. Note that the tables are reflected across the diagonal axis
for viewing ease. The values on the off-diagonal (i.e., where the two algorithms are not the same)
are the cross-play score, while the values shown on the diagonal are self-play scores, computed as
described in App. A.1.2. The cross-play and self-play scores displayed are means and standard
errors. The rightmost column reflects the row average and standard deviation of the cross-play
(xp) scores corresponding to the test set of VDN, QMIX, IQL, IPPO, and MAPPO, excluding the
self-play score. Thus, the rightmost column reflects how well on average the row algorithm can
generalize to unfamiliar teams.
Overall, we find that MARL algorithms perform significantly better in self-play than cross-play.
We note that there are a few exceptions (e.g., IPPO vs QMIX on 8m), and that the cross-play and
self-play scores are much closer on 10v11, but overall the trend is consistent across tasks.
Figure 9: Cross-play table for the 5v6 task.
A.3 Cross-play performance as a function of the number of agents.
To provide more insight on how a non-NAHT team performance changes as the number of unseen
agents increases, Figure 13 plots QMIX and IPPO’s test scores on 5v6 when paired with various
methods, as a function of the number of unseen agents. Note that k = 0 corresponds to teams
entirely composed of the evaluated agents, while k = 5 corresponds to teams entirely composed of
the unseen teammates, and are actually self-play scores, whereas the other points are cross-play
scores. These points appear on the plot to contextualize the performance of the mixed teams.
17
nruteR
tseT
naeMFigure 10: Cross-play table for the 8m task.
Figure 11: Cross-play table for the 3s5z task.
QMIX and IPPO have similar returns in self-play, but comparing the two figures, we observe a
difference in the rate at which the cross-play returns decline. In particular, QMIX’s performance
declines most around k =3 across a variety of methods, while IPPO’s performance is minimized at
k =2.
A.4 Further Discussion of the Motivating Example
ThissectionconsidersanextensionofthemotivatingexamplediscussedinSection4;namely,whether
consideringlongeragentmodelingwindowsissufficienttoallowtheadhocagentstosolvethes=1
game without coordination.
Consider a modification of the ad hoc agents’ policies such that instead of picking the best response
to teammate actions in the last step, they consider the last t actions of each teammate, and pick
the best response with respect to the expected actions. In this extended scenario, assume there are
M −1 (uncontrolled) agents which always pick 0. Assume that the M −1 always-zero agents play
without the ad hoc agent for t steps. Then, the ad hoc agent a is introduced into the game in the
h
t+1-th step. In this scenario, a will learn that all the other agents always pick zero, and will pick
h
the optimal action 1 accordingly.
Now consider the setting where there are M −2 always-zero agents playing without the ad hoc
agent for t steps, and two ad hoc agents a1 and a2 are introduced into the game at the t+1-th
h h
step. Following the same modeling policy, the two ad hoc agents a1 and a2 will once again both
h h
learn that their teammates follow an always-zero policy, and therefore learn to pick the 1 action —
a suboptimal decision, as s=2.
Inthissetting, evenifa1 anda2 modeledteammatebehavioronaslidingwindowoflengthtrather
h h
thaneverytstepsinbatchform,theywillnotconvergetooptimalbehavior. Toseewhy,considerthe
actionsofa1 anda2 inanarbitrarywindowofsizew. Sincetheybegininunison, aredeterministic,
h h
and consider the same window size, their actions will be identical for all the t steps of this window.
For the first t steps, lacking history, both agents will pick action 1, as we have shown above. After
k steps, they will both switch to 0, as we have also shown. Now, however, if they are both modeling
agent action distributions using a rolling window of the same size t, after t/2+1 additional steps,
they will reach a point in which both agents have chosen 0 more than 50% of the time in the last t
timesteps.
18Figure 12: Cross-play table for the 10v11 task.
5v6, Evaluated Algorithm=QMIX 5v6, Evaluated Algorithm=IPPO
18 18
16 16
14 14
12
12
10
10
8
8
0 1 2 3 4 5 0 1 2 3 4 5
k Unseen Agents k Unseen Agents
qmix-vs-vdn qmix-vs-iql qmix-vs-ippo ippo-vs-vdn ippo-vs-iql ippo-vs-ippo
qmix-vs-qmix qmix-vs-mappo ippo-vs-qmix ippo-vs-mappo
Figure13: Meanandstd. errorofQMIX(left)andIPPO(right)testreturnson5v6,inthepresence
of uncontrolled agents generated by various MARL learning algorithms, as the number of unseen
agentsincreasesfromk =0toamaximumofk =5. Theunseenagentsaregeneratedbythesecond
method in the legend name.
At that point, both agents will once again simultaneously switch their actions to 1 to maximize the
likelihood of winning the game. As a result S would once again flip from 0 to 2. Using a longer
window for modeling action does not mitigate the problem of inadequately modeling other ad hoc
agents, which leads to an inability to cooperate.
A.5 Computing Infrastructure
All value-based algorithms (e.g. QMIX, VDN, IQL) were run without parallelized training, while
policy gradient algorithms were run with parallelized training. The servers used in our experiments
ran Ubuntu 18.04 with the following configurations:
• Intel Xeon CPU E5-2630 v4; Nvidia Titan V GPU.
• Intel Xeon CPU E5-2698 v4; Nvidia Tesla V100-SXM2 GPU.
• Intel Xeon Gold 6342 CPU; Nvidia A40 GPU.
• Intel Xeon Gold 6342 CPU; Nvidia A100 Gpu.
19
nruteR
naeM
nruteR
naeM