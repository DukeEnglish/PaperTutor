Bootstrapping Linear Models for Fast Online Adaptation in
Human-Agent Collaboration
BenjaminA.Newman ChrisPaxton
CarnegieMellonUniversity,Meta Meta
Pittsburgh,Pennsylvania,USA Pittsburgh,Pennsylvania,USA
newmanba@cmu.edu cpaxton@meta.com
KrisKitani HennyAdmoni
CarnegieMellonUniversity,Meta CarnegieMellonUniversity
Pittsburgh,Pennsylvania,USA Pittsburgh,Pennsylvania,USA
kkitani@cmu.edu henny@cmu.edu
ABSTRACT 1 INTRODUCTION
Agentsthatassistpeopleneedtohavewell-initializedpoliciesthat Agentsthatcollaboratewithpeopletocompleteaperson’spre-
canadaptquicklytoalignwiththeirpartners’rewardfunctions.Ini- ferredgoalcannotalwaysknowthispreferenceinadvanceofan
tializingpoliciestomaximizeperformancewithunknownpartners interaction.Thoughpeoplemayinitiallystatethesepreferences,
canbeachievedbybootstrappingnonlinearmodelsusingimitation theymaydrift,sometimeschangingentirely,overthecourseof
learningoverlarge,offlinedatasets.Suchpoliciescanrequirepro- multipleinteractionepisodes.Whiletheremaybenocontinued
hibitivecomputationtofine-tunein-situandthereforemaymiss explicitcommunicationbetweencollaborativepartners,people’s
criticalrun-timeinformationaboutapartner’srewardfunctionas in-situbehaviorsaregoal-drivenandthuscanrevealtheup-to-date
expressedthroughtheirimmediatebehavior.Incontrast,online preference.Thismeansthatupdatingagentpoliciesbasedonin-situ
logisticregressionusinglow-capacitymodelsperformsrapidinfer- behaviorsiscriticalforassistingpeopleduringcollaborations,i.e.
enceandfine-tuningupdatesandthuscanmakeeffectiveuseofim- ensuringthatrobotactionsaredeferentialtousergoals[21].
mediatein-taskbehaviorforrewardfunctionalignment.However, Muchcurrentresearchinhuman-agentcollaborationaimsto
theselow-capacitymodelscannotbebootstrappedaseffectively learnzero-shotcollaborationpoliciesfromofflinedatasetsthatare
byofflinedatasetsandthushavepoorinitializations.Wepropose eithercollectedfromhuman-humandemonstrations[8]orgen-
BLR-HAC,BootstrappedLogisticRegressionforHumanAgentCol- eratedsynthetically[28].Insteadofusinganindividual’sin-situ
laboration,whichbootstrapslargenonlinearmodelstolearnthe behaviortoupdateamodelonlinetoimproveperformancewithre-
parametersofalow-capacitymodelwhichthenusesonlinelogistic specttothatindividual’spreference,theseapproachestrainagents
regressionforupdatesduringcollaboration.WetestBLR-HACin offlineincollaborationwiththepopulationofpartneragentsrepre-
asimulatedsurfacerearrangementtaskanddemonstratethatit sentedbythetrainingdataset.Theythentargetgoodperformance
achieveshigherzero-shotaccuracythanshallowmethodsandtakes inaggregateontaskmetrics.Attesttime,theseapproachesassume
farlesscomputationtoadaptonlinewhilestillachievingsimilar thepreferencesandbehaviorofanewhumancollaboratorwill
performancetofine-tuned,largenonlinearmodels.Forcode,please fallwithinthedistributionofthecollaboratorsrepresentedbythe
seeourprojectpagehttps://sites.google.com/view/blr-hac trainingdata.Whiletheseapproacheshavebeenshowntobeef-
fectiveontaskmetricsingeneralcollaborationsettings,theydo
KEYWORDS notnecessarilytransfertothestrictercriteriaofassistivecollabo-
rationswheresuccessinataskisdictatedbyapersonalpreference
AssistiveRobotics;OnlineAssistance;Human-RobotInteraction;
andpeople’sgoalsandbehaviorscandriftawayfromthetraining
CollaborativeAssistance
distribution.
Furthermore,thepopulationofpersonalpreferencesissubstan-
ACMReferenceFormat:
tialanddiverse,makingitdifficulttoensuresufficientcoverage
BenjaminA.Newman,ChrisPaxton,KrisKitani,andHennyAdmoni.2024.
duringtrainingtime.Collectinglargedatasetsofhuman-human
BootstrappingLinearModelsforFastOnlineAdaptationinHuman-Agent
Collaboration.InProc.ofthe23rdInternationalConferenceonAutonomous dataistime-consumingandexpensive,whilecollaborationamong
AgentsandMultiagentSystems(AAMAS2024),Auckland,NewZealand,May populationsofprocedurallygeneratedagentscanyielddatathat
6–10,2024,IFAAMAS,10pages. donottightlymatchthedistributionofthehumanpopulation.Fur-
thermore,aspeoplerepeatedlyexecuteacollaborativetask,they
maydevelopnewpreferencesthatareunlikelytobecapturedby
thedistributionofcollaborationdatarepresentedinofflinedatasets.
ThisworkislicensedunderaCreativeCommonsAttribution
Weproposeamethodthattakesadvantageoftheseadvance-
International4.0License.
mentsinzero-shotcoordinationandappliesthemtoalgorithms
Proc.ofthe23rdInternationalConferenceonAutonomousAgentsandMultiagentSystems forfast,onlineadaptationfromin-situbehavior.Inthisway,we
(AAMAS2024),N.Alechina,V.Dignum,M.Dastani,J.S.Sichman(eds.),May6–10,2024, hopetoachievebothgoodinitialperformancewhenassistinga
Auckland,NewZealand.©2024InternationalFoundationforAutonomousAgentsand
MultiagentSystems(www.ifaamas.org).
4202
rpA
61
]IA.sc[
1v33701.4042:viXraH A H A
State
Action Action Corrective Action Action Action
Actions
select sd place on top place on bottom select sd place on bottom
Figure1:Onestepofanexamplesurfacerearrangementtask:cupboardorganization.Fromlefttoright:aperson(H)picksan
objecttoplaceinthedishwasher;theagent(A)initiallyplacesthisincorrectly;thepersoncorrectstheplacement.Fromthis,
theagentlearnsthattheuserlikestoplaceblueobjectsonthebottomshelfandcanplacethenext,similarobjectcorrectly.
newpartner,butalsotocontinuetoadapttotheirpreferenceover Whilesomepriorapproachestodevelopingautonomousassis-
continuedexposure. tantsforhouseholdtasksrelyonpeopleprovidingfulltaskdemon-
Decipheringpeople’sexactpreferencescanbedifficult,however, strationsinadvanceofacollaboration,BLR-HACaimstooperate
asthesepreferencesareoftennotexplicitlystatedandcanchange inrealtime,utilizinginformationfromeachactionasitistakenby
overthecourseofaninteraction.Fortunately,in-situbehaviorsare aperson.Furthermore,approachesrelyingonfulltaskdemonstra-
goal-directedandcanimplicitlyrevealinformationaboutaperson’s tionscanintroduceadditionalburdenonapersonandberedundant
currentpreferenceorgoal,evenwhenitisnotexpresslycommuni- tothegoal-directedbehaviorpeopleexhibitwhencompletingtasks
cated.Wesuggestthatagentsengaginginassistivecollaborations [6].Incontrast,trainingshallow,low-capacitymodelswithlogistic
utilize these goal-directed behaviors to infer and act towards a regressionthroughMaxEntIRLtoutilizein-situbehaviorhasbeen
person’scurrentgoal,therebyenablingpersonalizedassistance. showntoeffectivelyandquicklyadapttopeople’sobjectivesin
Tododevelopamodelthatcanutilizethesegoal-directedbe- areassuchasrobotteleoperation[16]andmotionplanning[18].
haviorsforcollaborativeassistance,weintroduceBLR-HAC:Boot- We test BLR-HAC in a simulated version of our surface re-
strappedLogisticRegressionforHumanAgentCollaboration.This arrangementtask.WefindthatBLR-HACoutperformsbaseline
model is trained using a two-stage approach: first, we pretrain low-capacitymodelsandlarge,nonlinearmodelstrainedwithbe-
atransformer[30]tolearntoproducetheparametersofashal- haviorcloninginzero-shotcoordination.WealsofindthatBLR-
low,parameterizedpolicythatsecond,isupdatedthroughouta HACachievessimilarperformancebutrequiresafractionofthe
human-agentcollaborationusingonlinelogisticregression.Totest computeofatransformerthatisfine-tunedonline.Thisfinding
BLR-HAC,wefirstintroduceaformalizationofaspecificinstance holdstruewhenconsideringbothpreferencesthatremainthesame
ofarearrangementtask,whichwecallassistivesurfacerearrange- overtime,i.e.arestationary,andthosethatdrift,i.e.arenonsta-
ment.WethencompareBLR-HAC’sperformanceinasimulated tionary.Takentogether,theseresultsshowhowBLR-HACisable
versionofthistaskagainsttwobaselines:1)atraditionaltrans- totakeadvantageofthestrengthsofbothzero-shotandfastonline
formertrainedwithbehaviorcloningand2)atraditionalshallow adaptationmethods.Itdoesthisbypretrainingalarge,nonlinear
policytrainedwithonlinelogisticregression. modeltolearntheparametersofashallowpolicythatcanbeup-
Ourchosendomainofsurfacerearrangementmodelshousehold datedwithonlinelogisticregression.Thisresultsinacollaborative
tasks, like dishwasher loading, which have complex, long-term agentthatisbothwell-initializedandhighlyadaptable.
dependenciesdeterminedbyacombinationofaperson’senviron- Inthispaperwemakethefollowingcontributions:
mentandtheirstronglyheldpreferences.Forexample,aperson
mayprefertoplacelargedishesbeforesmallonestomaximize
capacity.Suchhighdimensionalstateandpreferencespaceslead
toanalmostinfinitenumberofdiverseandequallyvalidsolutions • aformalizationofcommonhouseholdtasksascollaborative
forcompletinganygivenhouseholdchores.Forexample,choosing IRLtasks,whichwecallsurfacerearrangement,
toloadadishwasherbasedondishmaterialisjustasvalidasload- • anovelmodel,BLR-HAC,thatcombinesthestrengthsof
ingbasedondishsize;itisamatterofpersonalpreference.Given pretrainedlarge,nonlinearmodelswithlow-capacitymodels
thisdiversity,householdtasksmakeespeciallygoodtestbedsfor trainedonlinevialogisticregressionforefficientlearningin
studyingalgorithmsthatrequirealigningrobotpolicieswithpeo- human-robotcollaborations,and
ple’srewardfunctions,thusmimickingmanyusecasesforassistive • evidence from experiments in simulation that BLR-HAC
robotics. outperformsitscomponentmodels.2 RELATEDWORK device,suchasajoystick,arobotcanobserveuserinputcommands
First,wepresentworkinstateandaction-conditionedmodelsthat andinfertheuser’smostlikelygoalfromasetofpredetermined
donotexplicitlylearnabouttheirhumanpartnerduringtaskexecu- goals. The robot then assists the user by moving along a path
tion.Wefollowthisbyreviewingworkinadaptivecollaborations. towardsthepredictedgoal[16].MaxEntIRLcanalsobeusedto
interpret less direct forms of user behavior, such as physically
2.1 StateandAction-ConditionedCollaboration pushingarobotoutofthewaytodeterminewhichpaththeuser
preferstherobottotake,forexampletocarryacoffeemugaround
Priorapproachestosolvinglong-horizontaskswithcomplextem-
alaptopcomputerinsteadofoverit[18],usingnaturalisticeye
poraldependenciesandunderspecifiedsolutions,suchasthose
gazeincombinationwithjoysticksignalstocontrolarobotarm
presentinoursurfacerearrangementdomain,canrelyonresolv-
[4, 5, 22], or using corrective actions to learn about features of
ingambiguitiesthroughacombinationofteleoperationandpre-
theenvironmentthatrelatetoaperson’spreferencetoincrease
programmedroutines[12],orbysuggestingoptimal,predetermined
generalizabilityandsampleefficiency[24].Weareinterestedin
solutions[20].Solutionsfollowingtheformermethodcanplaceun-
adaptingonlineMaxEntIRLfordetermininghigh-leveltaskplans
dueburdenonapersontoexplicitlyexpresstheirpreferences,and
consistentwithuserpreferencesinhouseholdcollaborationsfrom
renderrobotactionredundantwhenademonstrationcompletes.
in-taskcorrectivebehavior.
Theyalsorequirepeopletocontinuallydemonstratetheirdesired
IRLhasalsobeenappliedtolearnrobotpoliciesinothertypes
solutionastheconstraintsofthetask,suchasaperson’spreference
ofhuman-robotinteractions.Forexample,tolearnpeople’sprefer-
ortheenvironment,vary.Methodsfollowingthelatterexampledo
encesfromobservationsofindependenttaskdemonstrations[31],
notallowforfullfreedomofexpressionfromtheuserandassume
orbylearningassistivesocialactionsfortherapybycombininga
allusershavethesame“optimal”solution.
therapists’expertisewithexpertdemonstrations[3],orforsocial
Zero-shotcoordinationisarecentfieldofresearchaimingto
health,suchasarobotreceptionistlearningtogivehygienead-
develop models that can successfully and immediately interact
viceinashoppingmall[11].Ourformulationlearnspreferences
withnovelpartners.Thiscanbedonebypretrainingmodelsin
fromin-situ,collaborativebehaviorforcollaborativerearrangement
simulationagainstagentsdesignedtomimichumanbehavior[8]
tasks.
oroveradiversepopulationofsimulatedagents[28].Usingthese
Finally,anotherimportantaspectofmaintainingassistivehuman-
methods,though,canleadtooverlyspecificsolutions.Othershave
agentcollaborationsistomaintaincollaborativefluency[15].Main-
usedlargelanguagemodelstrainedwithweb-scaledatatopropose
tainingprinciplesofcollaborativefluency,suchasminimizingagent
taskplansthatarethenexecutedbyrobots[2].Thesetaskplans
andhumanidletime,allowshuman-agentcollaborationstofunc-
arenotadaptedtoanindividualuser,whoserewardfunctionmay
tionsimilarlytohuman-humancollaborations,therebyreducing
ormaynotfitwellwithinthedistributionseenduringtraining.
frictiononpeopletointeractwithautonomousagents.Further-
Thesemethodsplacetheburdenonthepersontoeitheraccepta
more,robotsassistingpeopletocompletecollaborativetaskshas
lesspreferredrobotbehaviororcontinuetoprovideactionsthat
beenshowntoaffectaperson’sultimatedecision[23],makingit
increasethelikelihoodoftherobotbehaviorexhibitingbehavior
importanttocontinuallymonitorandassesspeople’sgoalsduring
inlinewiththeperson’spreferences.Inthiswork,wefocuson
collaboration.Inthiswork,wewillusetheseideasasjustification
combiningthesegoodinitializationswithonlineadaptation.
forourdesiretodevelopanalgorithmthatadaptstouserprefer-
Often,approachesrelyingsolelyonlarge,pretraineddeepneural
encesinreal-time.
networksrequirepeopletogenerateexplicitdescriptionsoftheir
preferenceswhichcanbedecodedbythemodelintorobotaction
[2].Actionsproducedfromthisprocessarenotguaranteedtoalign 3 METHODS
withaperson’staskobjective.Whiledeepnetworkscanpotentially
Weformalizethetaskofsurfacerearrangement,aspecificinstance
be adapted to meet individual preferences through fine-tuning
ofrearrangementproblems[7,29],asadecentralizedpartiallyob-
[10,14],doingsowithlargemodelscanleadtochallenging,unstable
servableMarkovdecisionproblem(DEC-POMDP).
learningthatresultsinvariableperformance[19].Inthiswork,we
focusondevelopinganalgorithmthatcanquicklyadapttopeople’s
naturallyexpressed,task-orientedbehavior. 3.1 DefiningSurfaceRearrangement
To study assistive collaborations, we introduce assistive surface
2.2 AdaptiveCollaborations
rearrangement,acollaborativepickandplacetaskwheretwoagents
Using IRL for robot control can be difficult, in part, due to the worktogethertoarrangeasetofobjects𝑂intoasetoflocations
ambiguitythatarisesfromtraditionalIRL[1].Maximumentropy 𝐿.Inthistask,theassistiveagentaimstohelpapersonrearrange
IRLfacilitatesthisbyusingtheprincipleofmaximumentropyto objectsintolocations.Importantly,theagent’sgoalistoachievethe
ordersolutionsaccordingtohowwelltheymatchobserveduser finalstatethatisdesiredbytheperson,whichisinitiallyunknown
behavior[32].Thissolutionhasalsobeenusedinbehavioralscience totheagent.
tomodelpeople’sabilitytoinferothers’goalsfromtheirbehavior Asingleepisodeofthistaskconsistsofanobjectrepositorycon-
asexhibitedduringgoal-directedplans[6]. tainingobjects𝑜 ∈𝑂.Theinitialstateoftheepisodeis𝐿randomly
Theseinsightshavebeenappliedtorobottrajectoryoptimization chosenobjectsfrom𝑂,and𝐿vacantlocations.Eachlocationhasa
for shared control. In the difficult task of teleoperating a high- capacityforasingleobject.Progressinthetaskismadebyplacing
degreeoffreedomrobotarmwithalow-degreeoffreedominput objects𝑜intolocations𝑙 ∈𝐿.AtaskiscompletedwhenallobjectsAlgorithm1SurfaceRearrangement estimateandmaximizetheperson’srewardfunction.We
R 1e :q 𝑠u 0ir ←e: e𝜋 n𝜃 v, .r𝜋 e𝜃ˆ s, ee tn ()v,𝑂,𝐿
•
𝛾th ,e are df io scr oe ua nss tiu nm ge faa cl tl oa rg .entshavethesamerewardfunction.
2: 𝜉 ← (cid:2)𝑠0(cid:3)
3:
while𝜉.length<𝐿do
54 :: 𝑎𝑎 ℎ 𝑟𝑡𝑡 ←← 𝜋𝜋 𝜃𝜃 ˆ(cid:16)(cid:0) ·· || 𝑎𝑠𝑡 ℎ𝑡− ,1 𝑠𝑡(cid:1) −1(cid:17) won eeG h(i abv vee ecn a nt u oh sa cet otw nh te e ra oos lt )s h ,u e tm hr iie sst paw rs oo s bua lmg eme en d rt ts eo da ubn ced eat sh p ta oet r aw so se in nar goe lv eo e arn gl wy eh no o tp s pt ei rm op bi oz ll ei in c mg y
,
6:
𝑠𝑡,𝑎 𝑐𝑡 ←env.step(cid:16) 𝑎 ℎ𝑡,𝑎 𝑟𝑡,𝑠𝑡−1(cid:17) allowing it to be decomposed to a POMDP. Since POMDPs are
computationallyintractabletosolveexactly,weusetheQMDP
7: 𝜉.append(cid:16)(cid:104) 𝑎 ℎ𝑡,𝑎 𝑟𝑡,𝑎 𝑐𝑡,𝑠𝑡(cid:105)(cid:17) approximation[17].Priorworkinonlinehumanrobotcollaboration
8: endwhile [18]hasshownhowaQMDPcanbesolvedonlineusingonline
gradientdescent,adaptedforourpurposeinAlg.3.
𝑜havebeenplacedintoalocation𝑙.Forsimplicity,weassumethat
𝑁 ≤ |𝐿|andthatplacing𝑜in𝑙 occursinstantaneously. 4 APPROACH
Twoagentsinteractinanepisodeinthefollowingway.The
Ourultimategoalistolearnanassistivepolicythatcollaborates
humanagent𝜋
𝜃
firstpicksanobjectgiventhecurrentstate𝑠𝑡−1.
with a person during a surface rearrangement task. Given that
Then,therobotagent𝜋 placesthisobjectintoalocation.The
𝜃ˆ wewantourpolicytobeassistive,itshouldtakeactionsthatare
environmentthenreturnsthenextstate𝑠𝑡
andthehumancorrects alignedwiththeperson’sunderlyingpreferenceforcompletingthe
therobot’saction,returning𝑎 𝑐𝑡 .Anepisode𝜉canberepresented
task.Weinterpretthisasaregretminimizationproblem,wherethe
asthefollowingtuple:(cid:16) 𝑠0,𝑎 ℎ1,𝑎 𝑟1,𝑎 𝑐1,𝑠1,...𝑎 ℎ𝐿,𝑎 𝑟𝐿,𝑎 𝑐𝐿,𝑠𝐿(cid:17) . policyaimstominimizetheregretofitsactionswithrespecttothe
actionsthatwouldbeexhibitedundertheperson’struepreference
3.2 FormalizingSurfaceRearrangement forcompletingthetask.Importantly,weassumethatthepolicy
does not have prior knowledge of this preference and that the
Giventhisdescription,wecanmodelassistivesurfacerearrange-
persondoesnotimmediatelyorexplicitlyrevealit.Additionally,
mentasadecentralizedpartiallyobservableMarkovdecisionprob-
weassumethatthespaceofpossiblepreferencesthepersoncould
lem(DEC-POMDP)whichisatupleof (𝑆,Π,𝐴,𝑇,𝑍,𝑂,𝑟,𝛾).Our
holdtobeextremelylarge,makingdisambiguationfromlimited
objectiveistotrainapolicy𝜋 𝑟 thatsolvesthisDEC-POMDP:
interactionwiththepersondifficult.
• Sisthesetofallpossiblestates.Asinpriorwork[18],we Under these conditions, we have two main ways to perform
assumethataparticularstate𝑠 ∈𝑆isatupleofobservable regretminimization.First,wecanensureourpolicytakesgood
andunobservablefeatures:𝑠 = (𝑥,{𝜃 𝑖}).Observablestate initialactionsthatarelikelytoalignwiththeperson’spreference,
featuresarerepresentedasatupleofallpossiblelocations oftenreferredtoaszero-shotperformance.Second,wecanadapt
andallpossibleobjects.Locationsarerepresentedbytheir thepolicyonlineasahistoryofbehaviorisaccumulated.
IDandtheircurrentoccupancy.Objectsarerepresentedby Actioninferenceandpolicyadaptationdonotoperatewithina
theirIDandthelocationtheycurrentlyoccupy,ifany.The vacuum,butratherwithinthecourseoftheinteraction.Thecom-
unobservable portion of the state,𝜃 𝑖 describes the learn- monmetricinhuman-robotinteractionofcollaborativefluency
ableparametersoftherewardfunctionconsistentwiththe [15],forexample,iscriticaltopeopleconsideringaninteraction
human’spreferenceinthetask. witharobottobe“good.”Animportantfacetofthismetricisre-
• Πisthesetofagents.Inourinitialversionofthisproblem, latedtotheamountoftimetherobotsitsidleduringtaskexecution.
weassumetwoagents:ahumanagentandanassistiveagent. Thismakesfrequentlyupdatinglargemodelsduringaninteraction
• 𝐴 𝑖 isthesetofactionsforaparticularagent𝑎 𝑖.Weassume challenging,asbothactioninferenceandpolicyupdatingrequire
thatthepersonbothselectsobjectsandcorrectsobjectplace- largeamountsofcomputation,leadingtohighrobotidletimes.We
ments,whiletherobotcanonlymakeobjectplacements. aimtodevelopamethodthatcantakeadvantageofthegoodperfor-
• 𝑍 𝑖 isthesetofobservationsusedtoinfer𝜃.Theassistive manceoflargenonlinearmodelswhilebeingabletoquicklyadapt
agent’sobservationspaceistheperson’sactionspace.Inthis touserpreferences,asexpressedthroughtheirin-situbehavior,
workweassumethatthehumandoesnotinfertherobot’s withoutcausingtherobottoidle.
preference. TolearnanassistivepolicythatsolvestheDEC-POMDPdis-
• 𝑇(𝑠𝑡−1,a𝑡−1,𝑠𝑡)denotesthetransitiondynamicsthatmodel cussedinSec.3.2,wefirstgenerateasimulateddatasetofdiverse,
theprobabilityofenteringaparticularstategiventhecurrent high-levelpreferences(Sec.4.1.1).Usingthesepreferences,wecol-
stateandbothagents’actions.Asinpriorwork[18],changes lectadatasetofcollaborativedemonstrationsinasimulatedsur-
in𝑇 aredictatedby𝜃.Weassumethistobeconstantand facerearrangementtaskoverarangeofdifficulties(Sec.4.1.2).We
deterministicwithinasingleepisode. thentrainourtwo-stagealgorithmbyfirst,learningtomimicthe
• 𝑂 𝑖(𝑠𝑡+1,𝑢 𝑖𝑡,𝑧𝑡+1),theobservationdistributionforagent𝜋 𝑖. collectedexpertdemonstrations(Sec.4.2)andsecond,usingthe
• 𝑟 𝑖(𝑠𝑡,{𝑎 𝑖}𝑡)istherewardfunctionfortheeachagent.We preferencerepresentationslearnedinSec.4.3toperformfast,online
assume an assistive setting where the agent is trying to adaptation.Input Feature Embedding Preference Estimation Action Estimation
Human Action Object Features
at-k-1:t 𝝓
h h
R ao cb to -t k A -2c :t ti -o 1n Locatio 𝝓n F reatures TrTarnasnf Pso rfr Beom fr Ble ome r lcer oe kn 𝟁cErc kn eE cnMocododedere lr θ 𝝅 a rt
State State Features
st-k-2:t-1 𝝓
s
Figure2:BLR-HACOverviewFromlefttoright,wefirstembedtheinputstateandactionsusing𝜙.Thesearethenconcatenated
andfedintothepreferenceestimator𝜓.Thislearnstooutputrewardparameters,𝜃 whichareusedtoinitializeanonline
learningpolicyusingthepolicy𝜋,whichdeterminestherobot’saction𝑎 𝑟.
Algorithm2ExpertDemonstrationCollection Algorithm3LearningPriorsforOnlineLinearRegression
Require: Θ,𝜋,env,𝑂,𝐿 Require: 𝐷,M,𝜙 ℎ,𝜙 𝑟,𝜙 𝑠
1: 𝐷 = [] 1: whiletrainingdo
2:
for𝜃 inΘdo
2:
for(𝑠,𝑎 ℎ,𝑘)in𝐷do
3: 𝜉 ←surfaceRearrangement(𝜋 𝜃,env,O,L) 3: 𝜃ˆ←M(𝜙 𝑠(𝑠),𝜙 ℎ(𝑎 ℎ),𝜙 𝑟 (𝑎 𝑐))
4: 𝐷.append(𝜉) 4: 𝑎 𝑟 ←argmax𝑎𝑟∈𝐴𝑟𝜙 ℎ(𝑎 ℎ𝑡)·𝜃ˆ·𝜙 𝑟(𝐴 𝑟)
5: endfor 5: loss←𝑝(𝑎 𝑐)log𝑞(𝑎 𝑟)
6:
training←M.update(loss)
7: endfor
8: endwhile
4.1 Datasets
4.1.1 ModelingaDiverseUserPopulation. Thetwokeyideasof
ourmethodtodevelopassistiverobotsforhouseholdcollaborations wecollect100demonstrationsfromeachpreferencegeneratedin
isthatthemethodshouldbeabletobotheffectivelyusealarge Sec.4.1.1.
populationofpreferencedatatopretraingoodinitializationsand
beabletoquicklyadapttoaparticularpreferencewhenpresented 4.2 LearningPreferencesinaDiverseUser
withinformationaboutthatpreference.
Population
Tocapturetheseideasinourexperiments,wedevelopasimu-
lateddatasetofpreferences.First,wesamplealargesetofpref- Thefirststepofourproposedalgorithmaimstominimizeregret
erences,representingapopulation,asencodedby𝜃.Weassume by achieving good zero-shot performance. Ultimately, we want
preferencesfromwithinthispopulationaredrawnnormallyfrom
tomodel𝑝(𝑎 𝑟|𝑠,𝑎 ℎ).Thisproblem,however,isill-posed,astwo
oneofseveralmodes,eachofwhichindicatesasubpopulationof policiesparameterizedbydifferentpreferenceswillcorrectlytake
similarpreferences.Wesamplethreepreferencedatasets:train,, twodifferentactions𝑎 𝑟 giventhesamestateandhumanaction.
andtest.Fromeachsetofpreferences,wesampleepisodesofsur-
Toaccountforthisambiguity,weincludeahistoryof𝑘priorstate
facerearrangementepisoderollouts,thuscreatingthreedatasets: and action pairs taken under the current preference and maxi-
𝐷 𝑡𝑟𝑎𝑖𝑛, 𝐷 𝑒𝑣𝑎𝑙, and𝐷 𝑡𝑒𝑠𝑡.𝐷 𝑡𝑟𝑎𝑖𝑛consistsof1000simulatedprefer- mize𝑝(𝑎 𝑟|𝑠𝑡−𝑘−2:𝑡−1,𝑎 ℎ𝑡−𝑘−1:𝑡,𝑎 𝑐𝑡−𝑘−2:𝑡−1).Forthesakeofbrevity,
ences,sampledfromfourmodes,with1000episodesperpreference. we will slightly abuse notation and refer to this distribution as
𝐷 𝑒𝑣𝑎𝑙, and𝐷 𝑡𝑒𝑠𝑡 eachcontain100simulatedpreferences,with20 𝑝(𝑎 𝑟|𝑠,𝑎 ℎ,𝑘).
episodesperpreference. Again,whentrainingassistiveagents,achievinglowzero-shot
performanceisnotouronlyobjective.Wealsoneedanagentthat
4.1.2 EnvironmentsforSurfaceRearrangement. Totesttheefficacy adaptsonlinetoincominguserbehaviorwhilemaintainingcollabo-
ofourapproachatvaryingdifficulties,wedevelopthreeenviron- rativefluency.Thismeansdevelopingalightweight,low-parameter
ments.Eachenvironmentscalesproblemdifficultybyincreasing modelcapableofperformingactioninferenceandpolicyadaptation
thesizeofthestatespace.Wehaveasmallenvironment,withfive inrealtime.
possibleobjectsandfivelocations,amediumenvironment,with To do this, instead of learning𝑝(𝑎 𝑟|𝑠,𝑎 ℎ,𝑘) directly, we first
tenobjectsandtenlocations,andfinallyalargeenvironment,with learnalatentspacethatcorrespondstotheweightsofalogistic
25objectsand25locations. regressionproblem.Theseweightsserveastheinputtothesecond
Tocollectademonstrationdatasetforeachenvironment,weuse stepofouralgorithm,Sec.4.3.Thus,wetrainourmodeltomaximize
Alg.2.Importantly,tocollectexpertdemonstrations,weset𝜃 =𝜃ˆ 𝑀 𝜙,𝜓(𝑠,𝑎 ℎ,𝑘,𝑡)=𝑝(𝜃|𝑠,𝑎 ℎ,𝑘,𝑡).Inthisway,weplaceaninductive
andusealinearpolicy𝜋 = 𝜙 ℎ(𝑎 ℎ) ·𝜃 ·𝜙 𝑟(𝐴 𝑟),whereall𝜙 are biasoverthelatentspaceofthemodel,enticingittolearnamatrix
implementedasone-hotembeddinglayers.Foreachenvironment ofsize𝑂×𝐿,thatcanbeusedastheweightsofanonlinelogisticregressionproblem.Wetreatthisasaclassificationproblemand intheinitialmodelweights.Ourintuition,though,isthat
minimizethecrossentropylossbetweenourmodel’spredictions sincedemonstrationsaredrawnfromalarge,diversepopu-
andthecollectedexpertdemonstrations:𝐿=𝑝(𝑎 𝑐)·log𝑞(𝑎 𝑟)where lationofpreferences,andthattherelationsbetweenprefer-
𝑞(𝑎 𝑟)=𝜙(𝑎 ℎ)·𝑀(𝑠,𝑎 ℎ,𝑘,𝑡)·𝜙(𝐴 𝑟),asshowninAlg.3. encesandpeoplearenotknownapriori,thisdisambigua-
tionwillbenefitfromanonlinearfunctionapproximator.We
4.3 BootstrappingShallowLinearModelsfor expectnonlinear,high-capacitymodelstooutperformthis
Fast,OnlineAdaptation baseline.
• DeepLinear.Sincethespaceofpreferencesisverylarge,
Thesecondstepofourproposedalgorithmaimstominimizeregret
itcouldsimplybethatincreasingmodelcapacitywithout
throughonlineadaptation.Usingtheoutputofthemodellearned
introducingnonlinearitymaycapturethepreferencedistri-
inSec.4.2,wecanemployonlinelogisticregression,whichhas
bution.Totestthis,weintroduceDeepLinear,whichsimply
beenshowntoworkwellforteachinghumanpreferencestoagents
addsadditionalmodelparametersinbothwidthanddepth.
throughcorrectivefeedbackinrobotcontroltasks.Importantly,
WeexpectthismodeltooutperformaShallowLinearmodel
sinceonlinelogisticregressionhasaverysimpleupdateruleto
estimate𝜃thatoperatesoveramuchsmallernumberofparameters butunderperformnonlinearmethods.
• Multi-LayerPerceptron.Totesttheimportanceofmodel-
thanalarge,nonlinearnetwork,wecanadaptthisinitialestimate
ingthepreferencedistributionwithanonlinearmodel,we
oftheperson’spreferencein-situwithoutriskinglargehumanor
introduceamulti-layerperceptronbaseline.Weexpectthis
robotidletime,therebymaintainingcollaborativefluency.
Toupdateourestimateof𝜃,weusealinearapproximationofthe modeltooutperformbothlinearmethodsbutunderperform
attention-basedmechanisms.
QMDPsolutiontotheDEC-POMDPinSection3.2andstochastic
• CausalTransformer.Finally,sincewearepassingahis-
gradientdescent,resultinginthefollowingupdaterule:
toryofbehaviortothemodelateverytimestep,wecan
𝜃ˆ=𝜃ˆ−𝛼(𝜙 ℎ(𝑎 ℎ)·𝜙 𝑟(𝑎 𝑟)−𝜙 ℎ(𝑎 ℎ)·𝜙 𝑟(𝑎 𝑐)) inferthecurrentpreferencefromthissequenceofbehaviors.
where𝛼 isthelearningrate. Attention-basedmechanisms,specificallycausaltransform-
ers,havebeenshowntoexcelatmodelingsequentialdata.
5 EXPERIMENTALDESIGN Totestthisweimplement𝜓 asatransformer,andexpectit
tooutperformallothermethods.
Totestouralgorithm,wedesignseveralexperiments.First,wevali-
datetheneedforlarge,nonlinearmodelstolearnthedistributionof Thesecondaxisofbaselineswedevelopcomparestheimpor-
preferencesembeddedinthedemonstrationdataset,Sec.5.1.Then tanceofintroducinganinductivebiasoverthelatentspaceinorder
weexplorehowouralgorithmfaresinitsintendedusecase:fast, tolearn𝜃.Wecompareanimplementationoftheabovemodels
onlineadaptation.Wetestthisintwoscenarios.Sec.5.2.1analyzes inwhicheachmodelminimizes𝐿=𝑝(𝑎 𝑟)·logM(𝑠,𝑎 ℎ,𝑘)toour
adaptationtoasinglepreferenceovertime,whileSec.5.2.2explores proposedinductivebias,whichminimizes𝐿 =𝑝(𝑎𝑟)·log𝜙(𝑎 ℎ)·
howwellouralgorithmfareswhenthepreferencegeneratingthe M(𝑠,𝑎 ℎ,𝑘)·𝜙(𝐴 𝑟).
behaviorchangeswithoutexplicitcommunicationtotherobot.
5.1.1 ImplementationDetails. Toimplementourmodelswemake
5.1 Zero-ShotCoordination thefollowingdecisions.Weperformaseparateparametersweep
foreachmodelandenvironmentforthefollowingparametersand
Weevaluateourmodelineachenvironmentoverthetestsetusing
ranges:learningrate (1𝑒−3,1𝑒−6),thedimensionalityofhidden
Alg.3.Whileweareinsearchofanalgorithmthatperformsregret
layers(25,28),andthenumberoflayersin𝜓 (3,5,7,10,12).Weset
minimization,thismetricisrelativetoaspecificpreference.To
thesizeoftheinputhistorytobe50,paddingwhennecessary.For
understandmodelperformanceinanabsolutesenseandcompare
eachmodel,weimplementall𝜙 asasingle,one-hotembedding
acrossenvironments,wereportaccuracyintermsofthenumberof
spaceofvocabularysize208,where0-7arespecialcharacters,8-107
correctrobotactionpredictions.Thismetricisinverselycorrelated
arelocationindices,and108-207areobjectindices.Toimplement
withregret.
𝜓,weusePyTorch[25]andbaseourimplementationofacausal
Wechooseourbaselinestoexaminetwokeyquestions:1)are
transformeronDecisionTransformer[9].Weimplement𝜋 asa
high-capacity,nonlinearmodelsnecessaryfordisambiguationbe-
simplelinearmodelforinductivebiasandasanMLPfornoin-
tweenpreferencesinahighlydiversepreferencespace,and2)how
ductivebias.Allmodelsaretrainedusingtheappropriatetraining
doesinducinganinductiveprioroverthelatentspaceaffectzero-
andevaluationsets,whichdonotoverlapwiththetestset,with10
shotperformance?
epochsofearlystopping.
Toanswerthesequestionsweintroducebaselinesacrosstwo
axes:modelcomplexityandmodelbias.Todeterminetheeffectof
high-capacitynonlinearmodelsonzero-shotperformancewecom- 5.2 Test-TimeAdaptation
parefourlevelsofmodelcomplexityintermsofhowweimplement Developingassistivepoliciesisnotonlyaboutachievinggoodzero-
𝜓 inFig.2: shotperformance,however.Thespaceofactualhumanpreferences
• ShallowLinear.TypicalonlineIRLsettingslearnashallow isalmostboundlessandlikelyimpossibletocaptureinadvance
modelfromscratchusingMaxEntIRL.Tobootstrapthispro- ofaninteraction.Therefore,itisimportanttodevelopalgorithms
cess,onecouldperformthesameprocessovertheoffline thatcanrapidlyalignthemselveswithpreferencesassociatedwith
dataset,therebyencodingthediversepreferencepopulation aperson’sin-situ behavior.Westudythisintwosettings.First,Small Medium Large
NoPrior Prior(ours) NoPrior Prior(ours) NoPrior Prior(ours)
ShallowLinear 0.413 0.665 0.215 0.518 0.096 0.289
DeepLinear 0.425 0.680 0.199 0.504 0.101 0.303
MLP 0.605 0.759 0.361 0.653 0.120 0.358
Transformer 0.729 0.771 0.603 0.673 0.160 0.412
Table1:Wecomparezero-shotperformanceonthetestsetofeachenvironment.Wehavetwoaxesofcomparison:model
complexityintherows,andinductivepriorinthecolumns.Resultsarereportedintermsofaccuracy.Wecanseethatthe
highestcapacity,attentionbasedmodeltrainedwithaninductiveprioroutperformsallothermodelsineveryenvironment.
weanalyzeouralgorithm’sabilitytoadapttoastationaryprefer- attention-based method trained with an inductive prior outper-
enceoverthecourseofmultipleepisodes.Then,weanalyzeour formsallothermethods,achieving77.1%,67.3%,and41.2%accu-
algorithm’sabilitytoadaptinscenarioswherepreferencesarenon- racyonthesmall,medium,andlargeenvironments,respectively.
stationary.Here,weareinterestedinanalgorithm’sabilityto1) Weseethatthedifferenceinperformancebetweenmodelstrained
maintaindecentperformanceinthefaceofthepreferencechange, withandwithouttheinductivepriorincreasesasthedifficultyof
and2)rapidlyrecoverafterthechangeinpreference. theproblemincreases.Additionally,weseethegeneraltrendthat
highercapacity,nonlinearmodelsoutperformlowercapacitylinear
5.2.1 Stationary Preferences. To test our algorithm’s ability to
models.Theseresultsempiricallyjustifyourdesiretouseahigh-
adapttostationarypreferences,weaveragetheperformanceof
capacitynonlinearmodeltobootstrapalinearmodelinanonline
ourbootstrappedonlineIRLalgorithmoverallpreferencesinthe
logisticregressionproblem.
testingsetover20episodesineachtestingenvironment.
OursecondsetofresultsisshowninFig.3.Here,weplotthetest-
Wecompareagainstalinearmodelthatlearnsfromscratchand
timeadaptationaccuracyforthreemodels:linear(inred),BLR-HAC
amethodthatoptimizesoveralltransformerparametersbetween
(ingreen),andanonlinetransformer(inyellow).Fromthesegraphs,
episodesbutkeepsinferencecomputationconstant.Wemeasure
wecanseesupportforourhypothesisthatbootstrapped,shallow
computationcostintermsofFLOPSandcalculatethesevalues
linearmodelstrainedwithIRLachievegoodaccuracywithlow
empiricallyusingFVCore.Weexpecttoseethatthebootstrapped
computation.WecanseethatBLR-HACandTransformerbothstart
onlineIRLalgorithmachievessimilarperformancetotheonline
withhigheraccuracythanLinearinallcasesandthatthisdifference
transformermethodbutatafractionofthecompute.
increasesastheproblemcomplexityincreases.Furthermore,wesee
5.2.2 Nonstationary Preferences. Similar to the stationary pref- howBLR-HACachievessimilarperformanceoverepisodesasthe
erencesexperiment,werunIRLover20episodes.Inthisanaly- transformermethod,butatafractionofthecomputation.While
sis,however,weswitchtoadifferentrandomobjectiveafter10
bothmethodshavesimilarinferencecompute,of𝑂𝑥𝐿FLOPS,BLR-
episodes.Again,wecompareagainstalinearmodellearningfrom
HACusesonly2𝑥𝑂𝑥𝐿FLOPS,whiletheTransformermethoduses
scratchandanonlinetransformerimplementation.Weexpectto
∼400𝑀FLOPSduringupdates.
seethatthelinearmethodstartswithpoorperformancebutadapts Finally,weseeinFig.4resultsfromtest-timeadaptionwith
quicklywhenexposedtoincomingbehavior.Weexpecttoseethat nonstationarypreferences.Theseresultsshowmixedsupportfor
thetransformermethodstartswithgoodperformanceandadapts ourhypothesisthatbootstrapped,shallowlinearmodelstrained
moreslowlyasbehaviordataisaccumulated.Finally,weexpect withIRLrecoverwellfromunexpectedshiftsinuserbehavior.In
ourmethodtoachievethebenefitsofboththelinearfromscratch eachgraph,episodes1-10showsimilarresultstothepreviousset
andthetransformermethods:itshouldstartoffwithreasonable ofexperiments.Atepisode10,however,thepreferenceshifts,and
performanceandadaptquicklyasdataisaggregated. allmodelssufferadropinperformance.Interestingly,inallcases,
BLR-HACsuffersthesmallestdropinperformance.Whilethisisa
5.2.3 ImplementationDetails. Forbothexperiments,wedoahy- positiveresult,wealsoseethatastheenvironmentbecomesmore
perparametersweepoverthelearningrateintherange(1𝑒−2,1𝑒−5)
complex, BLR-HAC suffers in its adaptation rate from episodes
forthetransformerand(1,5,10)forthelinearmodels.Inbothcases,
10-20.Whileitadaptsonparwiththelinearmethod(thoughstill
weusethemaximumlearningrateforallexperiments.Additionally, achieveshigherperformanceduetoitsbetterinitialperformance)
weusestochasticgradientdescentforoptimizationinbothcases. itadaptsslowerthanthetransformer-basedmethod.Thisislikely
Totrainthetransformermethod,weperformfivestepsofgradient duetothefactthatthetransformerisabletomakebetteruseof
descentbetweeneachepisode. thelargeramountsofdatathatarebeingaggregatedinthelarge
environment.
6 RESULTS
FromrunningtheexperimentsoutlinedinSec.5,wehavethree
mainresults.First,wefindsupportforourhypothesisthatnonlin-
ear,high-capacitymodelstrainedwithinductivebiasescanlearn
a diverse population of user preferences. In Tab. 1, we see theStationaryAdaptationin𝑆𝑚𝑎𝑙𝑙 StationaryAdaptationin𝑀𝑒𝑑𝑖𝑢𝑚 StationaryAdaptationin𝐿𝑎𝑟𝑔𝑒
1 1 1
0.5
Linear
0.5 0.5
BLR-HAC
Transf
0 0 0
1 5 10 15 20 1 5 10 15 20 1 5 10 15 20
EpisodeNumber EpisodeNumber EpisodeNumber
Figure3:StationaryTest-TimeAdaptation.Learningcurvesforeachtestenvironmentforeachalgorithm.Wereporttheaverage
accuracyovereachepisode.BLR-HACisabletoachievethelowzero-shotperformanceofthetransformermethod,andthefast
adaptationofthelinearmethod.Additionally,wecanseethatastheepisodelengthincreases,thesedifferencesinperformance
aremorenotable,withthelinearmethodfailingtocatchuptotheothertwomethodsoverthecourseof20episodes.
NonstationaryAdaptationin𝑆𝑚𝑎𝑙𝑙 NonstationaryAdaptationin𝑀𝑒𝑑𝑖𝑢𝑚 NonstationaryAdaptationin𝐿𝑎𝑟𝑔𝑒
1 1 1
0.5 0.5 0.5
Linear
BLR-HAC
Transf
0 0 0
1 5 10 15 20 1 5 10 15 20 1 5 10 15 20
EpisodeNumber EpisodeNumber EpisodeNumber
Figure4:NonstationaryTest-TimeAdaptation.Learningcurvesovereachtestenvironmentforeachalgorithm.Wereport
averageaccuracyoverepisodes.BLR-HACisabletoperformonparwiththetransformermethodinthesmallandmedium
environmentsandpartofthelargeenvironment.BLR-HACoutperformsallmethodsinallenvironmentsimmediatelyafter
thepreferenceswitch.Inthelargeenvironment,though,thetransformerrecoversmorequicklyasithasaccesstomoredata.
7 DISCUSSION,LIMITATIONS,ANDFUTURE MechanicalTurk[13]orProlific[26]wouldallowustopretrain
WORK BLR-HACwithrealdata.
Finally,ourmethodalsoassumesasingle,synchronizedmodal-
Wedeveloppoliciesforassistiveagentsthatarebothwell-initialized
ityofcorrectiveactions:directstatecorrections.Thismakesour
andhighly-adaptable.Throughsimulatedexperiments,ourmethod
learningproblemeasierbymaximizingthecorrelationbetweenthe
achievesboththegoodinitializationsoflarge,nonlinearmodels
leader’scorrectionsandtheirrewardfunction.Wewouldliketo
trainedwithbehaviorcloningandthefastadaptationtouserbehav-
extendourapproachtoaccountforothermodalitiesofcorrections
iorpresentinlow-capacitymodelstrainedwithonlineMaxEntIRL.
issuedasynchronously,suchasthoseexpressedinrealtimethrough
Importantly,BLR-HACinitializesbetterthanShallowLinearontest
verbalornonverbalcommunication.
datathatisfarfromtheinitialdistribution,meaningthatourap-
proachshouldideallyallowforfasteradaptationtopopulationsfor
8 CONCLUSION
whomitisdifficulttocollectdataforofflinepretraining.
FutureworkshouldexploreapplyingBLR-HACtouserstudies Inthiswork,welaidoutanargumentforwhyassistiveagents
withrealpeopletodeterminewhetherthebetterinitializationsand shouldbebothwell-initializedandhighly-adaptable.Weintroduced
fasteradaptationsofourmethodholdoutsideofsimulationand anovelformulationofassistivehuman-agentcollaborationascol-
arepreferred.Itisalsoimportanttostudyhowtheeffectofthesize laborativeinversereinforcementlearningandintroducedanalgo-
ofthesurfacerearrangementproblemontheseresults. rithmBLR-HACthattakesadvantageofsophisticatedpopulation-
Userstudiesalsoprovideanopportunitytoimproveourmethod. levelmodelingfoundindeepneuralnetworkswiththefastadapta-
Collectinginteractiondatathroughinteractivesimulators,such tionofshallow,low-capacityinversereinforcementlearningmeth-
as AI Habitat [27, 29], deployed on platforms such as Amazon ods. Finally, we verified these claims through simulated experi-
ments.
)tcerroc%(ycaruccA
)tcerroc%(ycaruccA9 ETHICSSTATEMENT
[10] SeanChen,JensenGao,SiddharthReddy,GlenBerseth,AncaD.Dragan,and
SergeyLevine.2022.ASHA:AssistiveTeleoperationviaHuman-in-the-LoopRein-
Weshowwecanuseofflinedatasetstobootstrapassistivecollab-
forcementLearning.In2022InternationalConferenceonRoboticsandAutomation
orations by pretraining assistive agents. This method, however, (ICRA).7505–7512. https://doi.org/10.1109/ICRA46639.2022.9812442
necessitatesusingspecificsubpopulationsofthelargerhumanpop- [11] ZhichaoChen,YutakaNakamura,andHiroshiIshiguro.2022. Androidasa
ReceptionistinaShoppingMallUsingInverseReinforcementLearning.IEEE
ulation,i.e.thoserepresentedbythedataset.Thisleadstoethical RoboticsandAutomationLetters7,3(2022),7091–7098. https://doi.org/10.1109/
questionssuchas:Arethepreferencespresentinthedatasetrepre- LRA.2022.3180042
[12] MateiCiocarlie,KaijenHsiao,AdamLeeper,andDavidGossow.2012.Mobile
sentativeofthelargerpopulation?Howdoesthisaffectpeoplewho
manipulationthroughanassistivehomerobot.In2012IEEE/RSJInternational
holdpreferencesoutsidethissubpopulation?Thesequestionsare ConferenceonIntelligentRobotsandSystems.5313–5320. https://doi.org/10.1109/
especiallypertinentinassistivesettings,whereagentsarelikelyto IROS.2012.6385907
[13] KevinCrowston.2012.AmazonMechanicalTurk:AResearchToolforOrganiza-
encounterout-of-distributionphenomenaattest-time.Itisques-
tionsandInformationSystemsScholars.InShapingtheFutureofICTResearch.
tionssuchasthesethatmotivatethiswork. MethodsandApproaches,AnolBhattacherjeeandBrianFitzgerald(Eds.).Springer
Weassumeacriticalpartofprovidingassistanceistoreduce BerlinHeidelberg,Berlin,Heidelberg,210–221.
[14] JerryZhi-YangHe,ZackoryErickson,DanielS.Brown,AditiRaghunathan,and
unnecessaryburdenplacedonindividualswhileactinginalign- AncaDragan.2022. LearningRepresentationsthatEnableGeneralizationin
mentwiththeirpreference.Whenaperson’spreferencesarewell AssistiveTasks.In6thAnnualConferenceonRobotLearning. https://openreview.
net/forum?id=b88HF4vd_ej
representedbythedataset,pretrainingnecessarilyminimizesa
[15] GuyHoffman.2019.EvaluatingFluencyinHuman–RobotCollaboration.IEEE
person’sburdentobringtheagentintoalignmentwiththeirprefer- TransactionsonHuman-MachineSystems49,3(2019),209–218. https://doi.org/
ence.Whenaperson’spreferencesarenotwellrepresentedbythe 10.1109/THMS.2019.2904558
[16] ShervinJavdani,HennyAdmoni,StefaniaPellegrinelli,SiddharthaS.Srini-
dataset,ourmethodalignstotheperson’spreferencequicklyby
vasa, and J. Andrew Bagnell. 2018. Shared autonomy via hindsight opti-
usingtheirin-situ,goal-directedbehavior.Thus,whilethemodel mizationforteleoperationandteaming. TheInternationalJournalofRobot-
doesnothaveaninitialrepresentationoftheseout-of-domainpref- icsResearch37,7(2018),717–742. https://doi.org/10.1177/0278364918776060
arXiv:https://doi.org/10.1177/0278364918776060
erences,itdoesknowhowtointerpretgoal-directedbehaviorsin [17] MichaelLLittman,AnthonyRCassandra,andLesliePackKaelbling.1995.Learn-
ordertolearnsucharepresentation. ingpoliciesforpartiallyobservableenvironments:Scalingup.InMachineLearn-
ingProceedings1995.Elsevier,362–370.
Webelievethereisampleopportunityforfutureworktocon-
[18] DylanPLosey,AndreaBajcsy,MarciaKO’Malley,andAncaDDragan.2022.
tinuetoexploresolutionstotheseethicaldilemmas,suchasto Physicalinteractionascommunication:Learningrobotobjectivesonlinefrom
learnmoregeneralizablefeaturesofpreferencesthatallowforbet- humancorrections.TheInternationalJournalofRoboticsResearch41,1(2022),
20–44.
terrepresentationsofhumanpreferences,orbyteachingagents
[19] MariusMosbach,MaksymAndriushchenko,andDietrichKlakow.2021. On
tolearntolearnpreferences,whichwouldimproveanassistive theStabilityofFine-tuning{BERT}:Misconceptions,Explanations,andStrong
agentsabilitytoadapttoout-of-distributionpreferences. Baselines. In International Conference on Learning Representations. https:
//openreview.net/forum?id=nzpLWnVAyah
[20] BenjaminNewman,KevinCarlberg,andRutaDesai.2020.OptimalAssistancefor
Object-RearrangementTasksinAugmentedReality. arXiv:2010.07358[cs.HC]
[21] BenjaminA.Newman,ReubenM.Aronson,KrisKitani,andHennyAdmoni.
REFERENCES 2022. HelpingPeopleThroughSpaceandTime:AssistanceasaPerspective
onHuman-RobotInteraction. FrontiersinRoboticsandAI 8(2022). https:
[1] PieterAbbeelandAndrewYNg.2004. Apprenticeshiplearningviainverse //doi.org/10.3389/frobt.2021.720319
reinforcementlearning.InProceedingsofthetwenty-firstinternationalconference [22] Benjamin A. Newman, Reuben M. Aronson, Siddhartha S. Srinivasa, Kris
onMachinelearning.1. Kitani, and Henny Admoni. 2022. HARMONIC: A multimodal dataset of
[2] MichaelAhn,AnthonyBrohan,NoahBrown,YevgenChebotar,OmarCortes, assistive human–robot collaboration. The International Journal of Robot-
ByronDavid,ChelseaFinn,KeerthanaGopalakrishnan,KarolHausman,Alex icsResearch41,1(2022),3–11. https://doi.org/10.1177/02783649211050677
Herzog,etal.2022. Doasican,notasisay:Groundinglanguageinrobotic arXiv:https://doi.org/10.1177/02783649211050677
affordances.arXivpreprintarXiv:2204.01691(2022). [23] BenjaminA.Newman,AbhijatBiswas,SarthakAhuja,SiddharthGirdhar,KrisK.
[3] AntonioAndriella,CarmeTorras,CarlaAbdelnour,andGuillemAlenyà.2022. Kitani,andHennyAdmoni.2020.ExaminingtheEffectsofAnticipatoryRobot
IntroducingCARESSER:Aframeworkforinsitulearningrobotsocialassistance AssistanceonHumanDecisionMaking.InSocialRobotics,AlanR.Wagner,David
fromexpertknowledgeanddemonstrations.UserModelingandUser-Adapted Feil-Seifer,KerstinS.Haring,SilviaRossi,ThomasWilliams,HongshengHe,and
Interaction(032022). https://doi.org/10.1007/s11257-021-09316-5 ShuzhiSamGe(Eds.).SpringerInternationalPublishing,Cham,590–603.
[4] ReubenM.AronsonandHennyAdmoni.2022.GazeComplementsControlInput [24] BenjaminA.Newman,ChristopherJasonPaxton,KrisKitani,andHennyAd-
forGoalPredictionDuringAssistedTeleoperation.Roboticsscienceandsystems moni.2023.TowardsOnlineAdaptationforAutonomousHouseholdAssistants.
(2022). https://par.nsf.gov/biblio/10327640 InCompanionofthe2023ACM/IEEEInternationalConferenceonHuman-Robot
[5] ReubenM.Aronson,ThiagoSantini,ThomasC.Kübler,EnkelejdaKasneci,Sid- Interaction(Stockholm,Sweden)(HRI’23).AssociationforComputingMachinery,
dharthaSrinivasa,andHennyAdmoni.2018. Eye-HandBehaviorinHuman- NewYork,NY,USA,506–510. https://doi.org/10.1145/3568294.3580136
RobotSharedManipulation.InProceedingsofthe2018ACM/IEEEInternational [25] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory
ConferenceonHuman-RobotInteraction(Chicago,IL,USA)(HRI’18).Association Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,AlbanDes-
forComputingMachinery,NewYork,NY,USA,4–13. https://doi.org/10.1145/ maison,AndreasKopf,EdwardYang,ZacharyDeVito,MartinRaison,Alykhan
3171221.3171287 Tejani,SasankChilamkurthy,BenoitSteiner,LuFang,JunjieBai,andSoumith
[6] ChrisLBaker,JoshuaBTenenbaum,andRebeccaRSaxe.2007.Goalinference Chintala.2019.PyTorch:AnImperativeStyle,High-PerformanceDeepLearn-
asinverseplanning.InProceedingsoftheAnnualMeetingoftheCognitiveScience ingLibrary. InAdvancesinNeuralInformationProcessingSystems32.Curran
Society,Vol.29. Associates,Inc.,8024–8035. http://papers.neurips.cc/paper/9015-pytorch-an-
[7] DhruvBatra,AngelXChang,SoniaChernova,AndrewJDavison,JiaDeng, imperative-style-high-performance-deep-learning-library.pdf
VladlenKoltun,SergeyLevine,JitendraMalik,IgorMordatch,RoozbehMot- [26] Prolific.2014Online.Prolific. https://www.prolific.co
taghi,etal.2020.Rearrangement:Achallengeforembodiedai.arXivpreprint [27] ManolisSavva,AbhishekKadian,OleksandrMaksymets,YiliZhao,ErikWijmans,
arXiv:2011.01975(2020). BhavanaJain,JulianStraub,JiaLiu,VladlenKoltun,JitendraMalik,etal.2019.
[8] MicahCarroll,RohinShah,MarkKHo,TomGriffiths,SanjitSeshia,PieterAbbeel, Habitat:Aplatformforembodiedairesearch.InProceedingsoftheIEEE/CVF
andAncaDragan.2019.Ontheutilityoflearningabouthumansforhuman-ai InternationalConferenceonComputerVision.9339–9347.
coordination.Advancesinneuralinformationprocessingsystems32(2019). [28] DJStrouse,KevinMcKee,MattBotvinick,EdwardHughes,andRichardEverett.
[9] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, 2021. Collaboratingwithhumanswithouthumandata. AdvancesinNeural
Michael Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. 2021. InformationProcessingSystems34(2021),14502–14515.
Decision Transformer: Reinforcement Learning via Sequence Modeling.
arXiv:2106.01345[cs.LG][29] AndrewSzot,AlexanderClegg,EricUndersander,ErikWijmans,YiliZhao, Luxburg,S.Bengio,H.Wallach,R.Fergus,S.Vishwanathan,andR.Garnett(Eds.),
JohnMTurner,NoahDMaestre,MustafaMukadam,DevendraSinghChap- Vol.30.CurranAssociates,Inc. https://proceedings.neurips.cc/paper/2017/file/
lot,OleksandrMaksymets,AaronGokaslan,VladimírVondruš,SameerDharur, 3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf
FranziskaMeier,WojciechGaluba,AngelXChang,ZsoltKira,VladlenKoltun, [31] BryceWoodworth,FrancescoFerrari,TeofiloE.Zosa,andLaurelD.Riek.2018.
JitendraMalik,ManolisSavva,andDhruvBatra.2021. Habitat2.0:Training PreferenceLearninginAssistiveRobotics:ObservationalRepeatedInverseRein-
HomeAssistantstoRearrangetheirHabitat.InAdvancesinNeuralInforma- forcementLearning.InProceedingsofthe3rdMachineLearningforHealthcareCon-
tionProcessingSystems,A.Beygelzimer,Y.Dauphin,P.Liang,andJ.Wortman ference(ProceedingsofMachineLearningResearch,Vol.85),FinaleDoshi-Velez,Jim
Vaughan(Eds.). https://openreview.net/forum?id=DPHsCQ8OpA Fackler,KenJung,DavidKale,RajeshRanganath,ByronWallace,andJennaWiens
[30] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones, (Eds.).PMLR,420–439. https://proceedings.mlr.press/v85/woodworth18a.html
AidanNGomez,ŁukaszKaiser,andIlliaPolosukhin.2017. AttentionisAll [32] BrianD.Ziebart,AndrewMaas,J.AndrewBagnell,andAnindK.Dey.2008.
youNeed.InAdvancesinNeuralInformationProcessingSystems,I.Guyon,U.Von MaximumEntropyInverseReinforcementLearning.InProc.AAAI.1433–1438.