Analytical Approximation of the ELBO Gradient
in the Context of the Clutter Problem
Roumen Nikolaev Popov
Sintience Ltd., Sofia 1618, Bulgaria
rpopov@sintience.com
Abstract
We propose an analytical solution for approximating the gradient of the Evidence Lower Bound
(ELBO) in variational inference problems where the statistical model is a Bayesian network con-
sisting of observations drawn from a mixture of a Gaussian distribution embedded in unrelated
clutter,knownastheclutterproblem. Themethodemploysthereparameterizationtricktomove
the gradient operator inside the expectation and relies on the assumption that, because the like-
lihood factorizes over the observed data, the variational distribution is generally more compactly
supported than the Gaussian distribution in the likelihood factors. This allows efficient local
approximation of the individual likelihood factors, which leads to an analytical solution for the
integral defining the gradient expectation. We integrate the proposed gradient approximation as
the expectation step in an EM (Expectation Maximization) algorithm for maximizing ELBO and
test against classical deterministic approaches in Bayesian inference, such as the Laplace approx-
imation, Expectation Propagation and Mean-Field Variational Inference. The proposed method
demonstratesgoodaccuracyandrateofconvergencetogetherwithlinearcomputationalcomplex-
ity.
1 Introduction
Variational inference provides a viable, deterministic alternative to stochastic sampling methods,
such as Markov chain Monte Carlo (MCMC), for approximating the generally intractable marginal
likelihood in Bayesian inference problems, as discussed in Blei et al. (2017). This is achieved by
introducingavariationaldistributionasapproximationtotheposteriordensityandthenminimizing
the Kullback-Leibler (KL) divergence between the two, thus turning the inference problem into an
optimization problem over the parameters of the variational distribution (Jordan et al. (1999),
Bishop (2006), Zhang et al. (2019)). The minimization of the KL divergence is realized through
maximizationofalowerboundonthelogmarginallikelihood, knownastheEvidenceLowerBound
(ELBO), and the aim is that ELBO might be tractable or easy to approximate where the marginal
likelihood is not. This is not always the case, however, and particularly for the clutter problem
Minka (2001b) points out that ELBO is neither tractable nor easy to approximate analytically .
The clutter problem is a toy Bayesian inference problem described in Minka (2001a) that has
a statistical model defined by a Bayesian network where the observations are generated from a
mixture of a Gaussian distribution with known covariance embedded in unrelated clutter. It can
also be viewed as a model for measuring a physical quantity where the measurements are corrupted
by Gaussian noise and outliers, and in that respect may be useful in applications at the edge or in
safety-critical applications such as self-driving cars where undetected outliers in the sensory data
have the potential to cause catastrophic failure of the system.
A classical approach for solving the intractability problem of ELBO is to use the mean-field
approximation as described in Bishop (2006), which is based on the assumption that the variational
distribution factorizes over the latent variables. However, in the case of the clutter problem the
assumption holds poorly, which results in low accuracy of the approximation, demonstrated in
Minka (2001a).
1
4202
rpA
61
]GL.sc[
1v05501.4042:viXraAnother approach, popularized more recently in Paisley et al. (2012) and Kingma and Welling
(2014)circumventstheintractabilityofELBOby employingstochasticapproximation oftheELBO
gradient rather than of ELBO itself. This is applicable in cases where computation of the marginal
likelihoodisnotrequiredandweareonlyinterestedinmaximizingELBOtooptimizetheparameters
ofthevariationaldistribution. SincethedefinitionofELBOcanberegardedasanexpectationover
the variational distribution, the central premise of this method is to convert the gradient of the
expectation into an expectation of a gradient and then stochastically approximate the expectation.
InPaisleyetal.(2012)theconversionisachievedbyusingtheidentity∇ q(θ|ψ)=q(θ|ψ)∇lnq(θ|ψ),
ψ
where q(θ|ψ) is the variational distribution, θ comprises the latent variables and ψ represents the
parameters of the variational distribution. This approach is known as the log-derivative trick or
the score function estimator and while broadly applicable, typically suffers from a high variance
of the estimated gradient (Jankowiak and Obermeyer (2018)). To avoid the high variance issue,
Kingma and Welling (2014) adopt a different approach and perform the conversion by employing a
reparameterizationofthevariationaldistributioninwhichthelatentvariableisexpressedintermsof
a differentiable transformation of an auxiliary random variable with independent distribution. This
approach is known as the reparameterization trick or the pathwise gradient estimator and while not
as broadly applicable as the score function estimator, generally exhibits lower variance (Jankowiak
and Obermeyer (2018)).
ThestochasticapproximationoftheELBOgradienthasaprimaryadvantageinthatitisbroadly
applicable and can work with high dimensional latent spaces (Ranganath et al. (2014)), which best
benefitsapplicationincomplex,offlineinferenceproblemswherethestochasticnatureofthemethod
and its usually high computational cost are not an issue. In contrast, real time applications at
the edge and safety-critical applications require fast and deterministic inference algorithms. To
address this need we propose a method that, in the context of the clutter problem and based on the
reparameterization trick, analytically approximates the ELBO gradient and provides a solution for
maximizingELBObyintegratingtheapproximationastheexpectationstepinanEM(Expectation
Maximization) algorithm. The proposed approximation is tested against classical deterministic
approachesinBayesianinference, suchastheLaplaceapproximation, ExpectationPropagationand
Mean-FieldVariationalInference,anddemonstratesgoodaccuracyandrateofconvergencetogether
with linear computational complexity.
2 Preliminaries
2.1 ELBO Definition
Let X be a set of observations and Z a set of latent variables with a joint distribution p(X,Z).
We are interested in approximating the posterior distribution p(Z|X) and the marginal likelihood
p(X). For any variational probability distribution q(Z) the log marginal likelihood can be written
as follows, as described in Bishop (2006):
lnp(X)=L(q(Z))+KL(q(Z)||p(Z|X)) (1)
(cid:90) (cid:26) p(X,Z)(cid:27)
L(q(Z))= q(Z)ln dZ (2)
q(Z)
(cid:90) (cid:26) p(Z|X)(cid:27)
KL(q(Z)||p(Z|X))=− q(Z)ln dZ (3)
q(Z)
Since KL(q(Z)||p(Z|X)) is the KL divergence and is always positive, it follows that maximizing
L(q(Z)) is equivalent to minimizing KL(q(Z)||p(Z|X)). As such, the quantity L(q(Z)) is a lower
bound on the log marginal likelihood and is known as ELBO.
22.2 Clutter Problem Definition
The clutter problem is a toy Bayesian inference problem described in Minka (2001a) that has a
statisticalmodeldefinedbyaBayesiannetworkwheretheobservationsaregeneratedfromamixture
of a Gaussian distribution with known covariance embedded in unrelated clutter. The observation
density is given by:
p(x|µ)=(1−w)N(x;µ,Σ )+wP (x) (4)
g c
where w is the probability of having a clutter sample and P (x) is the clutter distribution, both of
c
which are known. If we have a set of independent observations X = {x ,...,x } and p(µ) is the
1 n
prior distribution over µ, then the joint distribution of X and µ becomes:
(cid:89)
p(X,µ)=p(µ) p(x |µ) (5)
i
i
Since the joint distribution is a product of sums, we can not really use belief propagation because
the belief state for µ is a mixture of 2n Gaussians (Minka (2001a)), which makes the computational
complexity prohibitive in cases of more than a few observations.
ApplyingvariationalinferencetotheproblemproducesthefollowingexpressionforELBO,where
q(µ) is the variational distribution:
(cid:90) (cid:90) (cid:90)
(cid:88)
L(q(µ))= q(µ) lnL (µ|x )+ q(µ)lnp(µ)− q(µ)lnq(µ) (6)
i i
µ i µ µ
andL (µ|x )=(1−w)N(µ;x ,Σ )+wP (x )arethelikelihoodfactors. DuetothetermslnL (µ|x ),
i i i g c i i i
which are logarithms of sums, ELBO is not directly analytically tractable in this case and has to be
approximated.
2.3 The Reparameterization Trick
As described in Kingma and Welling (2014), the reparameterization trick involves expressing the
latent variable in terms of a differentiable transformation of an auxiliary random variable with
independent distribution, which results in moving the gradient operator inside the expectation. Let
z be a continuous latent variable having a distribution q (z), where ϕ are the parameters of the
ϕ
distribution. The latent variable can be expressed as z = g (ϵ), where g is the differentiable
ϕ ϕ
transformation and ϵ is the auxiliary random variable having an independent distribution q (ϵ).
0
Using the reparameterization trick, an expectation over q (z) can be reformulated as follows:
ϕ
(cid:90) (cid:90)
E [f(z)]= q (z)f(z)dz = q (ϵ)f(g (ϵ))dϵ (7)
q ϕ 0 ϕ
For the case of a univariate normal distribution z ∼q (z)=N(µ,σ2) a valid reparameterization is
ϕ
z =µ+σϵ(KingmaandWelling(2014)),whereϵistheauxiliaryrandomvariablehavingastandard
normal distribution ϵ∼q (ϵ)=N(0,1).
0
3 Analytical Approximation of the ELBO Gradient
Wedevelopthe proposedmethod in thecontextof the clutterproblem for onedimensional observa-
tiondata. InthiscaseELBOtakestheformasspecifiedinequation(6),wherex areonedimensional
i
and the Gaussian distribution in the likelihood factors is therefore univariate with variance v . We
g
usethereparameterizationtricktomovethegradientoperatorinsidetheexpectation, withthegoal
of eliminating the logarithms that are otherwise difficult to directly approximate analytically. The
methodthenreliesontheassumptionthat,becausethelikelihoodfactorizesovertheobserveddata,
the variational distribution is generally more compactly supported than the Gaussian distribution
3N(µ;x ,v )inthelikelihoodfactors. Thisallowsefficientlocalapproximationoftheindividuallike-
i g
lihood factors with exponentiated quadratics by employing Taylor series expansion. As a result, the
integral defining the expectation becomes tractable and can be solved analytically. It is important
to note that while the approximation of each likelihood factor is local, the resulting approximation
of the gradient is not because the local point for each likelihood factor is different. We use the
proposed gradient approximation to maximize ELBO by integrating it in the expectation step of an
EM algorithm (Dempster et al. (1977)), which is described in section 4. The rest of this section is
organized into three subsections that develop the main steps of the proposed solution (applying the
reparameterization trick, locally approximating the likelihood factors and analytically solving the
expectation integrals), a subsection where we briefly discuss possible extension to multidimensional
data and a subsection where we examine the applicability to non-Gaussian distributions. We start
by restricting the variational distribution q(µ) to the family of normal distributions N(µ ,v ) and
q q
assume a conjugate prior p(µ)=N(µ ,v ).
p p
3.1 Applying the Reparameterization Trick
Using the reparameterization trick for the case of a univariate normal distribition we have µ =
√
µ + v ϵ and the ELBO gradient can be expressed as follows:
q q
(cid:90) (cid:90) (cid:90)
(cid:88) √
∇L(q(µ))= q (ϵ) ∇lnL (ϵ|(x −µ )/ v )+∇ q(µ)lnp(µ)−∇ q(µ)lnq(µ) (8)
0 i i q q
ϵ i µ µ
where the gradient operator is over the parameters of the variational distribution µ and v , and
√ √ √ q q
L (ϵ|(x −µ )/ v ) = (1−w)N(ϵ;(x −µ )/ v ,v /v )/ v +wP (x ) are the reparameterized
i i q q i q q g q q c i
likelihoodfactors. Applyingthegradientoperatortothelogarithmsandcarryingouttheintegration
in the tractable terms the expression for the ELBO gradient becomes:
√
∇L(q(µ))=(cid:90) q 0(ϵ)(cid:88)∇ LL (i ϵ( |ϵ (| x(x i −− µµ q )/)/ √ vv q )) − 1 2∇(µ p−
v
µ q)2 + 21 ∇ln vv q − 1 2∇ vv q (9)
ϵ i i i q q p p p
Executing the gradient operator over µ and taking the sum outside the integral we obtain:
q
√
G =
∂L(q(µ)) =−√
v
(cid:88)(cid:90)
q (ϵ)π
(ϵ)ϵ−(x i−µ q)/ v q
+
(µ p−µ q)
(10)
µq ∂µ q 0 i v v
q i ϵ g p
where √ √
(1−w)N(ϵ;(x −µ )/ v ,v /v )/ v
i q q g q q
π i(ϵ)=
(1−w)N(ϵ;(x −µ
)/√
v ,v /v
)/√
v +wP (x )
(11)
i q q g q q c i
Performing the same for the gradient over v results in the expression:
q
(cid:34) √ (cid:35)
∂L(q(µ)) 1 (cid:88)(cid:90) ϵ(ϵ−(x i−µ q)/ v q) 1 1
G = =− q (ϵ)π (ϵ) − + (12)
vq ∂v 2 0 i v v v
q i ϵ g q p
3.2 Local Approximation of the Likelihood Factors
To construct an analytical expression for the ELBO gradient we need to be able to analytically
approximate the integrals in equation (10) and (12). If we can efficiently approximate the term
q (ϵ)π (ϵ) with a Gaussian then both integrals become tractable. Conveniently, q (ϵ) and the nu-
0 i 0
merator of π (ϵ) are both Gaussian and therefore their product is also Gaussian, which we denote
i
ρ (ϵ):
i
1
(cid:18)
1(x −µ
)2(cid:19)
ρ (ϵ)=(1−w) exp − i q N(ϵ;ϵ ,v ) (13)
i (cid:112) 2(v g+v q)π 2 v g+v q i i
40.15 0.15
0.1 0.1
0.05 0.05
0 0
0 5 10 0 5 10
Figure 1: Local approximation of the reparameterized likelihood factors
√
where ϵ = v (x −µ )/(v +v ) and v = v /(v +v ). Since we have assumed that q(µ) is
i q i q g q i g g q
more compactly supported than N(µ;x ,v ), it follows that q (ϵ) is also more compactly supported
√ i g 0
than N(ϵ;(x −µ )/ v ,v /v ) and therefore ρ (ϵ) is more compactly supported than N(ϵ;(x −
√ i q q g q i √i
µ )/ v ,v /v ) as well. Consequently, the reparameterized likelihood factors L (ϵ|(x −µ )/ v ),
q q g q i i q q
which are the denominators in π (ϵ), can be efficiently approximated locally with exponentiated
i
quadratics as illustrated in figure 1. The quadratics are determined by a second order Taylor series
expansionofthelogarithmofthefactorscalculatedatthepointϵ andtheapproximationisspecified
i
by the following expressions:
(cid:18) (cid:19)
1
L i(ϵ)≈L(cid:101)i(ϵ)=exp 2ln(L i(ϵ))′′| ϵ=ϵi(ϵ−ϵ i)2+ln(L i(ϵ))′| ϵ=ϵi(ϵ−ϵ i)+lnL i(ϵ i) (14)
ln(L (ϵ))′′| =π (ϵ )(cid:16) (1−π (ϵ ))(cid:0) ϵ −(x −µ )/√ v (cid:1)2 v /v −1(cid:17) v /v (15)
i ϵ=ϵi i i i i i i q q q g q g
ln(L (ϵ))′| =−π (ϵ )(cid:0) ϵ −(x −µ )/√ v (cid:1) v /v (16)
i ϵ=ϵi i i i i q q q g
√
where we have used L (ϵ) for short of L (ϵ|(x −µ )/ v ).
i i i q q
It is interesting to note that the approximations of the reparameterized likelihood factors look
remarkablysimilartotheapproximatetermst˜(θ)inMinka(2001b). Thisisnotacoincidencesince
i
both have the same functional form of exponentiated quadratic and both approximate likelihood
factors. However, t˜(θ) is a global approximation and is defined as a scaled ratio between the new
i
variational distribution and the old t˜ i(θ) = Zqnew(θ)/q(θ) Minka (2001b). In contrast, L(cid:101)i(ϵ) is a
local approximation and is determined by the second order Taylor series expansion of lnL (ϵ) at ϵ .
i i
Furthermore, the two are used in very different contexts and for different purpose.
3.3 Analytically Solving the Integrals
Since both ρ i(ϵ) and L(cid:101)i(ϵ) are exponentiated quadratics, the product ρ i(ϵ)/L(cid:101)i(ϵ) is also an expo-
nentiated quadratic and after some rearranging can be expressed as follows:
(cid:112)
q 0(ϵ)π i(ϵ)≈ρ i(ϵ)/L(cid:101)i(ϵ)=π i(ϵ i) vˆ iA iN(ϵ;ϵˆ i,vˆ i) (17)
5where
(cid:44)(cid:34) (cid:32) (cid:18)
x −µ
(cid:19)2 (cid:33) (cid:35)
vˆ = v (1−π (ϵ )) π (ϵ )v i q +1 v +v (18)
i g i i i i g v +v q g
g q
√ x −µ
ϵˆ = v (1−π (ϵ )vˆ) i q (19)
i q i i i v +v
g q
A
=exp(cid:34)
−1(cid:0)
1−π (ϵ
)2vˆ(cid:1)
v
(cid:18)
x i−µ
q(cid:19)2(cid:35)
(20)
i 2 i i i q v +v
g q
It can be verified that N(ϵ;ϵˆ,vˆ) is a proper Gaussian because vˆ is strictly positive.
i i i
Alternatively, the Laplace approximation can also be used to locally approximate q (ϵ)π (ϵ) by
0 i
findingitsmodeandcomputingthevarianceatthemode. However,findingthemodeofq (ϵ)π (ϵ)re-
0 i
quiresinitselfiterativeapproximation,whichwouldaddsignificantcomplexitytothewholemethod
because it has to be performed for each data point.
Next, to compute the integrals in equation (10) and (12) we substitute q (ϵ)π (ϵ) with its ap-
0 i
proximation from equation (17). Carrying out the integration over ϵ and rearranging we obtain:
G
µq
≈G(cid:101)µq =(cid:88) B ix i v−µ q + µ p v−µ q (21)
g p
i
(cid:34) (cid:35)
G
vq
≈G(cid:101)vq =
1
2
−(cid:88)
C
iv1 +(cid:88)
D
i(x i−
v
µ q)2
v
+1
v
+
v1
−
v1
(22)
g g g q q p
i i
B =π (ϵ )(cid:112) vˆA v g+π i(ϵ i)vˆ iv q (23)
i i i i i v +v
g q
(cid:112)
C =π (ϵ ) vˆA vˆ (24)
i i i i i i
D =(1−π (ϵ )vˆ)B (25)
i i i i i
Equations (21) and (22) represent an analytical approximation of the ELBO gradient over the
parameters of the variational distribution and are illustrated in figure 2.
3.4 Extension to Multidimensional Data
LetM bethedimensionalityofthedata. Sincethedataspacecanbescaledandrotatedarbitrarily,
wecanassumewithoutlossofgeneralitythatthemultivariateGaussianintheobservationdensityis
spherical. Consequently, the gradient over the mean of the variational distribution is separable and
canbebrokenintoM onedimensionalgradientsalongtheprincipalaxesoftheellipsoiddefiningthe
variational distribution. Similarly, the gradient over the diagonal elements of Σ is also separable
q
and can be broken into M one dimensional variance gradients along the principal axes. However,
efficiently determining the off-diagonal elements of Σ is not trivial and a possible approach can be
q
to adjust the principal axes by employing principal component analysis in an iterative optimization
scheme, essentially breaking the multidimensional problem into multiple single dimensional ones.
3.5 Applicability to Non-Gaussian Distributions
The main restriction for the applicability of the proposed approximation to non-Gaussian distribu-
tions comes from the reparameterization trick, which does not easily generalize to other common
distributions, such as the gamma or beta. While a number of methods have been proposed to cir-
cumvent that problem (Ruiz et al. (2016), Figurnov et al. (2018)), these have been developed with
stochastic approximation of the expectation in mind and are not readily applicable to analytical in-
tegration. A possible solution for this limitation can be to approximate the target distribution with
6Algorithm 1 EM with analytical approximation of the ELBO gradient
1. Input: data x , size n, w, Pc(x ), v , µ , v
i i g p p
2. Initialize µ =(cid:80) x /n, v =(cid:80) (x −µ )2/n+v , vˆ =max(2v ,v )
q i i q i i q g g q g
3. For t=1 to convergence or maximum number of iterations
(a) Expectation step: approximate the ELBO gradient and compute π (ϵ ), vˆ, A , B , C
i i i i i i
and D
i
(b) Maximization step: update the parameters of the variational distribution µ and v ac-
q q
cording to the update rules
(c) update vˆ =max(min(2v ,vˆ /2),v ), constrain v =min(v ,max(v ,vˆ /2))
g q g g q q g g
a Gaussian mixture and extend the proposed ELBO gradient approximation to handle Gaussian
mixtures.
4 Maximizing ELBO
To test the proposed analytical approximation of the ELBO gradient we integrate it in the expecta-
tion step of an EM algorithm for ELBO maximization, as specified in algorithm 1. The approxima-
tion allows the expectation step to be applied directly on the ELBO gradient rather than on ELBO
itself. In the maximization step we then derive update rules for optimization of the variational dis-
tribution parameters µ and v through fixed-point iteration. Our aim is to construct linear update
q q
rules by keeping constant, with respect to the current values of µ and v , all terms that are limited
q q
in range. For each update rule convergence is briefly examined by demonstrating that the update is
in the direction of the local maximum stationary point but we leave a detailed convergence analysis
for future work.
Since the approximation relies on the assumption that the variational distribution is more com-
pactly supported than the Gaussian distribution in the likelihood factors, to satisfy that condition
during the first few iterations when v is large we introduce a substitute for v , denoted vˆ , and
q g g
initializeittomax(2v ,v ). Thefactor2ischosenbecauseitprovidesacceptableworstcaseerrorof
q g
theapproximation. Thesubstitutevˆ isthengraduallyreducedwitheachiterationofthealgorithm
g
according to the rule vˆ = max(min(v ,vˆ /2),v ) until it reaches the value of v . The value of
g q g g g
vˆ is used in place of v in all expressions of the gradient approximation and the update rules but
g g
for clarity purposes we do not specify it explicitly. In addition, to prevent v from breaking the
q
assumption of compact support it is constrained by the rule v =min(v ,max(v ,vˆ /2)) at the end
q q g g
of each iteration.
4.1 Update Rule for the Mean
√
WestartbyexaminingthetermsthatmakeupB asdefinedinequation(23). Theseareπ (ϵ ), vˆ,
i i i i
A and (v +π (ϵ )vˆv )/(v +v ). If we assume that the probability for a non-clutter data point is
i g i i i q g q
greater than zero, it can be readily deduced from its definition in equation (11) that the term π (ϵ )
i i
is limited to the range (0,1]. It follows then that vˆ falls within the range (0,1] and therefore so
√ i
does vˆ. Since π (ϵ ) ∈ (0,1] and vˆ ∈ (0,1], it follows that (1−π (ϵ )2vˆ) ∈ [0,1) and therefore
i i i i i i i
A ∈(0,1] because it is an exponent with a negative argument. Since π (ϵ )∈(0,1] and vˆ ∈(0,1],
i i i i
it also follows that (v +π (ϵ )vˆv )/(v +v ) ∈ (0,1]. Consequently, since B is a product of the
g i i i q g q i
aboveterms, itfollowsthatB islimitedtotherange(0,1]. Therefore, toderivetheupdaterulefor
i
the mean µ we keep B constant with respect to the current value of µ . Setting the approximate
q i q
710
4
5
2
0
0
-5
-2
-10
-2 -1 0 1 2 3 0 0.2 0.4 0.6 0.8 1
Figure 2: AnalyticalapproximationoftheELBOgradientoverthemeanµ (leftpanel)andvariance
q
v (right panel) of the variational distribution
q
gradient G(cid:101)µq equal to zero in equation (21) we then solve the resulting equation for µ q:
(cid:80) B (µ(t))x /v +µ /v
µ(t+1) = i i q i g p p (26)
q (cid:80) B (µ(t))/v +1/v
i i q g p
where µ(t+1) is the updated value and µ(t) is the current value. The corresponding update function
q q
is presented in figure 2 and is defined by the linear expression:
G(cid:98)µq =(cid:88) B i(µ( qt))x i v−µ q + µ p v−µ q (27)
g p
i
To confirm that µ is updated in the direction of the local maximum we demonstrate that the slope
q
oftheupdatefunctionG(cid:98)µq isnegative. Letµ∗
q
bethelocalmaximumstationarypoint(G(cid:101)µq(µ∗ q)=0
and ∂G(cid:101)µq(µ q)/∂µ q|
µq=µ∗ q
<0). If G(cid:101)µq(µ( qt))>0 it follows that µ∗
q
>µ( qt), since G(cid:101)µq is the gradient.
Atthesametime, ifG(cid:101)µq(µ( qt))>0andtheslopeofG(cid:98)µq isnegativeitfollowsthatµ( qt+1) >µ( qt), and
therefore the update is in the direction of the local maximum. Similarly, if G(cid:101)µq(µ( qt))<0 it follows
that µ∗ < µ(t) and µ(t+1) < µ(t), and therefore the update is again in the direction of the local
q q q q
maximum. Since we already established that B is positive, it follows from equation (27) that the
i
slope of G(cid:98)µq is negative and therefore µ
q
is always updated in the direction of the local maximum.
4.2 Update Rule for the Variance
We start by multiplying equation (22) with v to eliminate v in the denominator. The resulting
q q
function G(cid:101)vqv
q
is illustrated in figure 2 and can be expressed as follows:
(cid:34) (cid:32) (cid:33) (cid:35)
G(cid:101)vqv
q
= 21 − (cid:88) C iv1 + v1 v q+(cid:88) D i(x i−
v
µ q)2
v
v +q
v
+1 (28)
g p g g q
i i
Next, we examine the terms C and D . Since we already established that π (ϵ )∈(0,1], vˆ ∈(0,1]
i i i i i
and A ∈(0,1], it follows from the definition of C in equation (24) as a product of these terms that
i i
81.2 100
100
1
0.8 10-1
0.6
10-1
0.4 10-2
0.2
0 10-2 10-3
-1 0 1 2 3 0 5 10 15 20 25 30 0 5 10 15 20 25 30
Figure 3: Testing the ELBO gradient analytical approximation (ELBO GAA) against classical de-
terministic approaches for a sample size of 20 data points
C ∈ (0,1]. Since π (ϵ ) ∈ (0,1], vˆ ∈ (0,1] and B ∈ (0,1], it follows that (1−π (ϵ )vˆ) ∈ [0,1)
i i i i i i i i
and consequently D ∈ [0,1). In addition, the term v /(v +v ) is also limited to the range [0,1).
i q g q
Therefore,toderivetheupdateruleforthevariancev wekeepthetermsB ,C ,D andv /(v +v )
q i i i q g q
constantwithrespecttothecurrentvalueofv q. SettingtheapproximategradientG(cid:101)vq equaltozero,
we then solve the resulting linear equation, obtaining the following update rule for the variance:
(cid:32) (cid:33)(cid:44)(cid:32) (cid:33)
v(t+1) =
(cid:88)
D
(v(t))(x i−µ q)2 v q(t)
+1
(cid:88)
C (v(t))
1
+
1
(29)
q
i
i q v g v g+v q(t)
i
i q v g v p
where v(t+1) is the updated value and v(t) is the current value. The corresponding update function
q q
is presented in figure 2 and is formulated by the linear expression:
(cid:34) (cid:32) (cid:33) (cid:35)
G(cid:98)vq =
1
2
−
(cid:88)
i
C i(v q(t))
v1
g
+
v1
p
v
q+(cid:88)
i
D i(v
q(t))(x i−
v
gµ q)2
v
gv +q(t v)
q(t)
+1 (30)
To confirm that v is updated in the direction of the local maximum we verify that the slope
q
of G(cid:98)vq is negative. Since we already established that C
i
is positive, it follows from equation (30)
that the slope of G(cid:98)vq is negative and therefore v
q
is always updated in the direction of the local
maximum.
Inaddition,wedemonstratethatequation(29)producesvalidupdates(v(t+1) >0)byexamining
q
the update function at v = 0. In that case the term ((cid:80) C (v(t))/v +1/v )v is equal to zero
q i i q g p q
because C (v(t)) is finite. Since D ∈ [0,1) and v /(v + v ) ∈ [0,1), it follows that the term
i q i q g q
((cid:80) iD i(v q(t))(x
i
−µ q)2/v g)v q(t)/(v
g
+v q(t)) ⩾ 0. Consequently, the update function G(cid:98)vq is strictly
positive at v = 0 and since, as already established, its slope is negative, it follows that the root
q
v(t+1) is strictly positive and therefore is a valid update for v .
q q
5 Comparative Testing
WetesttheproposedmethodforanalyticalapproximationoftheELBOgradientagainstclassicalde-
terministic approaches such as the Laplace approximation, expectation propagation and mean-field
9100 100 101
100
10-1 10-1
10-2
10-2 10-3
10-4
10-1 10-3 10-5
0 5 10 15 20 0 5 10 15 20 0 5 10 15 20
Figure 4: Testing the proposed approximation (ELBO GAA) against classical deterministic ap-
proaches for sample sizes of 5 (left), 10 (middle) and 100 (right) data points
variational inference. Our implementation of the Laplace approximation and mean-field variational
inferenceisbasedonthedescriptionofthealgorithmsinBishop(2006)andforexpectationpropaga-
tion we use Minka (2001a). The test data is generated from the clutter problem observation density
as defined in equation (4), for one dimensional data and using the same parameters as specified in
Minka (2001b): w = 0.5, Pc(x) = N(x;µ = 0,v = 10), Gaussian distribution in the observation
c c
density N(x;µ = 2,v = 1), number of data points n = 20 and we also assume the same prior
g
distribution p(µ)=N(µ;µ =0,v =100). The aim is to determine how well the proposed method
p p
approximatestheposteriordistributionp(µ|X)incomparisontotheotherthreemethods. Asamea-
sure of goodness of approximation we use the KL divergence between the variational distribution
and the posterior, KL(q(µ)||p(µ|X)) = lnp(X)−L(q(µ)). This is calculated by numerically evalu-
ating the log marginal likelihood lnp(X) and the evidence lower bound L(q(µ)) for each method.
In addition, a baseline of best achievable approximation is provided for reference by numerically
maximizing ELBO. We denote the mean of the baseline variational distribution µ¯ and use it to
q
define absolute error rates |µ −µ¯ | for the means of the tested variational distributions.
q q
The result of one test with 20 data points is presented in figure 3. A data sample with a
rather skewed posterior is selected to give the tested methods a challenge. To summarize the test
results, theproposedmethodscoressecondbehindEPintermsofKLdivergenceandabsoluteerror
of the mean, beating both mean-field variational inference and the Laplace approximation by a
significant margin. It also demonstrates a good rate of convergence, slightly worse than MF and
Laplace. Furthermore, in contrast to EP, which can fail to converge (Minka (2001a)) or in certain
circumstances can produce negative v (Minka (2001b)), the proposed method appears to not suffer
q
from problems with convergence and as demonstrated in section 4.2 is guaranteed to have strictly
positive v .
q
An interesting observation is that just before convergence is reached the KL divergence of the
proposed method increases slightly. This is easily explained. The optimization of the variational
distribution parameters is performed with respect to the local maximum stationary point defined
by the approximate ELBO gradient, which is slightly different from the stationary point of the real
ELBO gradient due to the imperfection of the approximation. As a result, when the parameters
are far away from the two stationary points, updating them towards the appproximate stationary
pointmeansalsoadecreaseinthedistancetotherealstationarypointandhenceareductionofthe
KL divergence. However, when the parameters are very close to the real stationary point updating
10themtowardstheappproximatestationarypointmayinsomecasesincreasethedistancetothereal
stationary point and hence increase the KL divergence.
Figure 4 illustrates the performance of the tested algorithms for 5, 10 and 100 data points. As
can be observed, the proposed method performs well for all three cases and for the case of 5 data
points even comes out the best in terms of KL divergence.
Intermsofcomputationalcomplexity,theproposedmethodislinearlydependentonthenumber
of data points just like the other three methods. However, the number of exponentiation operations
is 2n versus n for EP, MF and Laplace. Furthermore, there are also n square roots and the number
of arithmetic operations is generally larger.
6 Conclusion
We describe a method for analytical approximation of the ELBO gradient in the context of the
clutter problem. The approximation is integrated into the expectation step of an EM algorithm for
ELBOmaximizationandupdaterulesforoptimizingthevariationalparametersarederived. Wetest
the proposed method against classical deterministic approaches such as the Laplace approximation,
expectation propagation and mean-field variational inference, and the method demonstrates good
accuracyandrateofconvergenceforbothsmallandlargenumberofdatapoints. Thecomputational
complexity is linearly dependent on the number of data points, but the number of mathematical
operationsissomewhatlargercomparedtotheotherthreealgorithmsinthetest. Weprovidelimited
convergence analysis for the update rules of the developed EM algorithm and, while empirical data
suggests convergence is reliable, a detailed analytical proof of convergence is a primary research
task for the future. Another area where the proposed method requires further work is the extension
to multidimensional data observations and the handling of non-Gaussian distributions. While the
proposed method provides a solution for the maximization of ELBO by optimizing the variational
distribution parameters it does not offer a means to calculate the actual value of ELBO and as a
result can not be used for model selection.
References
C. M. Bishop. Pattern Recognition and Machine Learning. Springer New York, 2006.
D. M. Blei, A. Kucukelbir, and J. D. McAuliffe. Variational inference: A review for statisticians.
Journal of the American Statistical Association, 112:859–877, 2017.
A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via the
em algorithm. Journal of the Royal Statistical Society. Series B, 39:1–38, 1977.
M. Figurnov, S. Mohamed, and A. Mnih. Implicit reparameterization gradients. In Advances in
Neural Information Processing Systems (NIPS 2018), 2018.
M. Jankowiak and F. Obermeyer. Pathwise derivatives beyond the reparameterization trick. In
Proceedings of the 35th International Conference on Machine Learning, volume 80, pages 2235–
2244, 2018.
M.I.Jordan,Z.Ghahramani,T.S.Jaakkola,andL.K.Saul.Anintroductiontovariationalmethods
for graphical models. Machine Learning, 37:183–233, 1999.
D. P. Kingma and M. Welling. Auto-encoding variational bayes. In International Conference on
Learning Representations, ICLR 2014, 2014.
T. P. Minka. Expectation propagation for approximate bayesian inference. In Proceedings of the
Seventeenth Annual Conference on Uncertainty in Artificial Intelligence (UAI-2001), pages 362–
369, 2001a.
11T.P.Minka. A Family of Algorithms for Approximate Bayesian Inference. PhDthesis, Department
of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, 2001b.
J. Paisley, D. Blei, and M. Jordan. Variational bayesian inference with stochastic search. In Pro-
ceedings of the 29th International Conference on Machine Learning, ICML 2012, volume 2, 2012.
R. Ranganath, S. Gerrish, and D. Blei. Black box variational inference. In Proceedings of the
Seventeenth International Conference on Artificial Intelligence and Statistics, volume 33, pages
814–822, 2014.
F.R.Ruiz,M.K.Titsias,andD.M.Blei. Thegeneralizedreparameterizationgradient. InAdvances
in Neural Information Processing Systems (NIPS 2016), 2016.
C. Zhang, J. Bu¨tepage, H. Kjellstr¨om, and S. Mandt. Advances in variational inference. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 41(8):2008–2026, 2019.
Appendix A. Approximating the Terms q (ϵ)π (ϵ) with a Gaussian
0 i
Substituting π (ϵ) with its definition from equation (11) we have:
i
√ √
(1−w)N(ϵ;(x −µ )/ v ,v /v )/ v
i q q g q q
q 0(ϵ)π i(ϵ)=q 0(ϵ)
(1−w)N(ϵ;(x −µ
)/√
v ,v /v
)/√
v +wP (x )
(31)
i q q g q q c i
√ √
Since both q (ϵ) and (1−w)N(ϵ;(x −µ )/ v ,v /v )/ v are Gaussian, their product is conve-
0 i q q g q q
niently also Gaussian, wich we denote ρ (ϵ):
i
(cid:32) (cid:0) √ (cid:1)2(cid:33)
1 1(0− x i−µ q)/ v q
ρ (ϵ)=(1−w) exp − N(ϵ;ϵ ,v ) (32)
i (cid:112) 2(1+v g/v q)v qπ 2 1+v g/v q i i
√
whereϵ = v (x −µ )/(v +v )andv =v /(v +v ). Rearrangingandsimplifyingequation(32)
i q i q g q i g g q
we obtain:
1
(cid:18)
1(x −µ
)2(cid:19)
ρ (ϵ)=(1−w) exp − i q N(ϵ;ϵ ,v ) (33)
i (cid:112) 2(v g+v q)π 2 v g+v q i i
The denominator of π (ϵ) is the reparameterized likelihood factor L (ϵ) = (1−w)N(ϵ;(x −
√ √ i i i
µ )/ v ,v /v )/ v +wP (x ) and, because of the assumption for compact support of the varia-
q q g q q c i
tional distribution, it can be efficiently approximated locally at the point ϵ with an exponentiated
i
quadratic, where the quadratic is derived by a second order Taylor series expansion of lnL (ϵ) at ϵ :
i i
(cid:18) (cid:19)
1
L i(ϵ)≈L(cid:101)i(ϵ)=exp 2ln(L i(ϵ))′′| ϵ=ϵi(ϵ−ϵ i)2+ln(L i(ϵ))′| ϵ=ϵi(ϵ−ϵ i)+lnL i(ϵ i) (34)
ln(L (ϵ))′′| =π (ϵ )(cid:16) (1−π (ϵ ))(cid:0) ϵ −(x −µ )/√ v (cid:1)2 v /v −1(cid:17) v /v (35)
i ϵ=ϵi i i i i i i q q q g q g
ln(L (ϵ))′| =−π (ϵ )(cid:0) ϵ −(x −µ )/√ v (cid:1) v /v (36)
i ϵ=ϵi i i i i q q q g
Deriving ln(L (ϵ))′ is straightforward and for ln(L (ϵ))′′ we use the product rule of derivatives:
i i
(ln(L (ϵ))′)′ =−(cid:16) π (ϵ)′(cid:0) ϵ−(x −µ )/√ v (cid:1) +π (ϵ)(cid:0) ϵ−(x −µ )/√ v (cid:1)′(cid:17) v /v (37)
i i i q q i i q q q g
We then apply the product rule of derivatives also to π (ϵ)′ and rearrange:
i
π (ϵ)′ =−π (ϵ )(1−π (ϵ ))(cid:0) ϵ−(x −µ )/√ v (cid:1) v /v (38)
i i i i i i q q q g
12Replacing π (ϵ)′ back in equation (37) we obtain:
i
ln(L (ϵ))′′ =−(cid:16) −π (ϵ )(1−π (ϵ ))(cid:0) ϵ−(x −µ )/√ v (cid:1)2 v /v +π (ϵ)(cid:17) v /v (39)
i i i i i i q q q g i q g
ln(L (ϵ))′′ =π (ϵ)(cid:16) (1−π (ϵ))(cid:0) ϵ−(x −µ )/√ v (cid:1)2 v /v −1(cid:17) v /v (40)
i i i i q q q g q g
√
Next we substitute ϵ in the numerator of π (ϵ ) with its definition v (x − µ )/(v + v ) and
i i i q i q g q
simplify the resulting expression:
(1−w)
(cid:18)
(x −µ ) v
(cid:19)
(1−w)
(cid:32) 1(cid:18)
√ x −µ x −µ
(cid:19)2
v
(cid:33)
√
v
N ϵ i; i √
v
q , vg = (cid:112)
2v π
exp −
2
v qvi +vq − i √
v
q vq (41)
q q q g g q q g
(1−w)
(cid:18)
(x −µ ) v
(cid:19)
(1−w)
(cid:32) 1(cid:18)
v
(cid:19)2
(x −µ
)2(cid:33)
√
v
N ϵ i; i √
v
q , vg = (cid:112)
2v π
exp −
2 v
+q
v
−1 i
v
q (42)
q q q g g q g
(1−w)
(cid:18)
(x −µ ) v
(cid:19)
1
(cid:18)
1v (x −µ
)2(cid:19)
√ v N ϵ i; i √ v q , vg =(1−w) (cid:112) 2v π exp − 2 g (v i +v )q 2 (43)
q q q g g q
We then rewrite equation (34) by completing the square inside the exponent, where for simplicity
we have defined a =ln(L (ϵ))′′| and b =ln(L (ϵ))′| :
i i ϵ=ϵi i i ϵ=ϵi
(cid:32)
1(ϵ−(ϵ −b /a
))2(cid:33) (cid:18) 1b2(cid:19)
L(cid:101)i(ϵ)=exp
2
i
1/a
i i exp − 2ai L i(ϵ i) (44)
i i
The product ρ i(ϵ)/L(cid:101)i(ϵ) can be formulated then as follows:
ρ (ϵ) 1
(cid:18)
1(x −µ
)2(cid:19)
1
(cid:18)
1(ϵ−ϵ
)2(cid:19)
i =(1−w) exp − i q √ exp − i
(cid:112)
L(cid:101)i(ϵ) 2(v g+v q)π 2 v g+v q 2v iπ 2 v i
(cid:32)
1(ϵ−(ϵ −b /a
))2(cid:33) (cid:18) 1b2(cid:19)
exp − i i i exp i /L (ϵ ) (45)
2 1/a 2a i i
i i
√
Substituting v in 2v π with v /(v + v ) we cancell out the terms (v + v ) and substituting
i i g g q g q
exp(cid:0) −(1/2)(x −µ )2/(v +v )(cid:1) withexp(cid:0) −(1/2)(x −µ )2(v +v )/(v +v )2(cid:1) theexpressionfor
i q g q i q g q g q
the product ρ i(ϵ)/L(cid:101)i(ϵ) becomes:
ρ (ϵ) 1 1
(cid:18)
1v (x −µ
)2(cid:19) (cid:18)
1v (x −µ
)2(cid:19)
i =(1−w)√ exp − g i q exp − q i q
L(cid:101)i(ϵ) 2π(cid:112) 2v gπ 2 (v g+v q)2 2 (v g+v q)2
(cid:18)
1(ϵ−ϵ
)2(cid:19) (cid:32)
1(ϵ−(ϵ −b /a
))2(cid:33) (cid:18) 1b2(cid:19)
exp − i exp − i i i exp i /L (ϵ ) (46)
2 v 2 1/a 2a i i
i i i
Since (1−w)(cid:0) 1/(cid:112) 2v π(cid:1) exp(cid:0) −(1/2)v (x −µ )2/(v +v )2(cid:1) is the numerator of π (ϵ ) as defined
g g i q g q i i
inequation(43)andL i(ϵ i)isthedenominator,theproductρ i(ϵ)/L(cid:101)i(ϵ)issimplifiedtothefollowing:
ρ (ϵ) 1 (cid:18) 1v (x −µ )2 1b2(cid:19)
i =√ π (ϵ )exp − q i q + i
L(cid:101)i(ϵ) 2π i i 2 (v g+v q)2 2a i
(cid:18)
1(ϵ−ϵ
)2(cid:19) (cid:32)
1(ϵ−(ϵ −b /a
))2(cid:33)
exp − i exp − i i i (47)
2 v 2 1/a
i i
13We then carry out the multiplication of the two exponentiated quadratics:
ρ (ϵ) 1
(cid:18) 1(ϵ−ϵˆ)2(cid:19)
i = √ π (ϵ )A exp − i (48)
L(cid:101)i(ϵ) 2π i i i 2 vˆ i
vˆ =v (1/a )/(v +1/a ) (49)
i i i i i
ϵˆ =(ϵ /a +(ϵ −b /a )v )/(v +1/a ) (50)
i i i i i i i i i
(cid:20) 1(cid:18) v (x −µ )2 b2 b2 (cid:19)(cid:21)
A =exp − q i q − i + i (51)
i 2 (v +v )2 a a2(v +1/a )
g q i i i i
Expanding and rearranging equations (49), (50) and (51) we obtain:
vˆ =1/(a +1/v ) (52)
i i i
vˆ =v /(a v +v +v ) (53)
i g i g q g
(cid:46)(cid:104) (cid:16) (cid:0) √ (cid:1)2 (cid:17) (cid:105)
vˆ = v π (ϵ ) (1−π (ϵ )) ϵ −(x −µ )/ v v /v −1 v +v +v (54)
i g i i i i i i q q q g q q g
(cid:46)(cid:104) (cid:16) (cid:0) √ (cid:1)2 (cid:17) (cid:105)
vˆ = v (1−π (ϵ )) π (ϵ ) ϵ −(x −µ )/ v v /v +1 v +v (55)
i g i i i i i i q q q g q g
(cid:46)(cid:104) (cid:16) (cid:17) (cid:105)
vˆ = v (1−π (ϵ )) π (ϵ )(x −µ )2(v /(v +v )−1)2/v +1 v +v (56)
i g i i i i i q q g q g q g
vˆ = v (cid:14)(cid:2) (1−π (ϵ ))(cid:0) π (ϵ )v (x −µ )2/(v +v )2+1(cid:1) v +v (cid:3) (57)
i g i i i i g i q g q q g
ϵˆ =(ϵ /v +ϵ a −b )/(a +1/v ) (58)
i i i i i i i i
ϵˆ =ϵ −b vˆ (59)
i i i i
(cid:0) √ (cid:1)
ϵˆ =ϵ +π (ϵ )vˆ ϵ −(x −µ )/ v v /v (60)
i i i i i i i q q q g
(cid:0)√ (cid:0)√ √ (cid:1) (cid:1)
ϵˆ = v +π (ϵ )vˆ v −(v +v )/ v v /v (x −µ )/(v +v ) (61)
i q i i i q g q q q g i q g q
√
ϵˆ = v (1−π (ϵ )vˆ)(x −µ )/(v +v ) (62)
i q i i i i q g q
(cid:20) 1(cid:18) v (x −µ )2 b2/a2−b2(v +1/a )/a (cid:19)(cid:21)
A =exp − q i q + i i i i i i (63)
i 2 (v +v )2 v +1/a
g q i i
(cid:20) 1(cid:18) v (x −µ )2 b2v /a (cid:19)(cid:21)
A =exp − q i q − i i i (64)
i 2 (v +v )2 v +1/a
g q i i
(cid:20) 1(cid:18) v (x −µ )2 (cid:19)(cid:21)
A =exp − q i q −b2vˆ (65)
i 2 (v +v )2 i i
g q
A
i
=exp(cid:34)
−
21(cid:32) v
q
(v(x
i
+− vµ
)q
2)2
−π i(ϵ
i)2(cid:18)√ v vq(x +i−
v
µ q)
−
x
i
√− vµ q(cid:19)2
v q2/v g2vˆ
i(cid:33)(cid:35)
(66)
g q g q q
(cid:34) 1(cid:32) v (x −µ )2 (cid:18) v (cid:19)2 (cid:33)(cid:35)
A =exp − q i q −π (ϵ )2 q −1 v /v2vˆ(x −µ )2 (67)
i 2 (v +v )2 i i v +v q g i i q
g q g q
(cid:20) 1(cid:18) v (x −µ )2 v (x −µ )2(cid:19)(cid:21)
A =exp − q i q −π (ϵ )2vˆ q i q (68)
i 2 (v +v )2 i i i (v +v )2
g q g q
A
=exp(cid:34)
−1(cid:0)
1−π (ϵ
)2vˆ(cid:1)
v
(cid:18)
x i−µ
q(cid:19)2(cid:35)
(69)
i 2 i i i q v +v
g q
14Since0⩽π (ϵ )⩽1,itfollowsthat0<vˆ ⩽1andthereforeequation(48)representsaproperGaus-
i i i
sian which results in the final expression for ρ i(ϵ)/L(cid:101)i(ϵ) as a Gaussian approximation of q 0(ϵ)π i(ϵ):
(cid:112)
q 0(ϵ)π i(ϵ)≈ρ i(ϵ)/L(cid:101)i(ϵ)=π i(ϵ i) vˆ iA iN(ϵ;ϵˆ i,vˆ i) (70)
(cid:44)(cid:34) (cid:32) (cid:18)
x −µ
(cid:19)2 (cid:33) (cid:35)
vˆ = v (1−π (ϵ )) π (ϵ )v i q +1 v +v (71)
i g i i i i g v +v q g
g q
√ x −µ
ϵˆ = v (1−π (ϵ )vˆ) i q (72)
i q i i i v +v
g q
A
=exp(cid:34)
−1(cid:0)
1−π (ϵ
)2vˆ(cid:1)
v
(cid:18)
x i−µ
q(cid:19)2(cid:35)
(73)
i 2 i i i q v +v
g q
Appendix B. Solving the Integrals
To compute the integral in equation (10) we substitute q (ϵ)π (ϵ) with its approximation from
0 i
equation (17):
√
G
µq
≈G(cid:101)µq =−√ v q(cid:88) π i(ϵ i)(cid:112) vˆ iA i(cid:90) N(ϵ;ϵˆ i,vˆ i)ϵ−(x i− vµ q)/ v q + (µ p v−µ q) (74)
i ϵ g p
√ √
Modifying the term ϵ − (x − µ )/ v to (ϵ − ϵˆ) + (ϵˆ − (x − µ )/ v ) and recognizing that
(cid:82)
i q q √i i i q q
N(ϵ;ϵˆ,vˆ)(ϵ−ϵˆ)dϵ = 0 while (ϵˆ −(x −µ )/ v ) is a constant with regards to ϵ, we carry out
i i i i i q q
the integration:
√
G(cid:101)µq =−√ v q(cid:88) π i(ϵ i)(cid:112) vˆ iA iϵˆ i−(x i−
v
µ q)/ v q + (µ p v−µ q) (75)
g p
i
Next, we substitute ϵˆ with its definition from equation (19) and rearrange:
i
(cid:18) (cid:19)
G(cid:101)µq =−(cid:88) π i(ϵ i)(cid:112) vˆ iA
i
v q(1 v− +π i v(ϵ i)vˆ i) −1 x i v−µ q + (µ p v−µ q) (76)
g q g p
i
Further rearranging, we obtain the final expression for G(cid:101)µq:
G(cid:101)µq =(cid:88) B ix i v−µ q + (µ p v−µ q) (77)
g p
i
B =π (ϵ )(cid:112) vˆA v g+π i(ϵ i)vˆ iv q (78)
i i i i i v +v
g q
To compute the integral in equation (12) we substitute q (ϵ)π (ϵ) with its approximation from
0 i
equation (17):
(cid:34) √ (cid:35)
1 (cid:88) (cid:112) (cid:90) ϵ(ϵ−(x i−µ q)/ v q) 1 1
G
vq
≈G(cid:101)vq =−
2
π i(ϵ i) vˆ iA
i
N(ϵ;ϵˆ i,vˆ i)
v
−
v
+
v
(79)
i ϵ g q p
√ √
Modifyingthetermϵ(ϵ−(x −µ )/ v )to((ϵ−ϵˆ)+ϵˆ)((ϵ−ϵˆ)+(ϵˆ−(x −µ )/ v ))werecognizethat
(cid:82) Nϵˆ(ϵ;ϵˆ,vˆ)(ϵ−ϵˆ)2dϵ=i vˆ,(cid:82)q N(ϵ;q ϵˆ,vˆ)(ϵ−ϵˆi )dϵ=i 0,(cid:82) Ni (ϵ;ϵˆi ,vˆ)(i ϵ−ϵˆq )(ϵˆ−(q
x −µ
)/√
v )dϵ=0
i i i i √ i i i i i i i i i q q
and ϵˆ(ϵˆ −(x −µ )/ v ) is a constant with respect to ϵ. Carrying out the integration in equation
i i i q q
(79) then results in the following expression:
(cid:34) √ (cid:35)
1 (cid:88) (cid:112) (cid:88) (cid:112) ϵˆ i(ϵˆ i−(x i−µ q)/ v q) 1 1
G(cid:101)vq =−
2
π i(ϵ i) vˆ iA ivˆ i/v g+ π i(ϵ i) vˆ iA
i v
−
v
+
v
(80)
g q p
i i
15√
Next, wesubstituteϵˆ inϵˆ(ϵˆ −(x −µ )/ v )withitsdefinitionfromequation(19)andrearrange:
i i i i q q
(cid:18) (cid:19)
x −µ v (1−π (ϵ )vˆ)−v −v 1
ϵˆ i ϵˆ i− i √ v q =(1−π i(ϵ i)vˆ i) q i v i +i v g q(x i−µ q)2 v +v (81)
q g q g q
(cid:18) (cid:19)
x −µ v +π (ϵ )vˆv 1
ϵˆ i ϵˆ i− i √ v q =−(1−π i(ϵ i)vˆ i) g v i +vi i q(x i−µ q)2 v +v (82)
q g q g q
Substituting equation (82) back in equation (80) we obtain the final expression for G(cid:101)vq:
(cid:34) (cid:35)
G(cid:101)vq =
21 −(cid:88)
C
iv1 +(cid:88)
D
i(x i−
v
µ q)2
v
+1
v
+
v1
−
v1
(83)
g g g q q p
i i
(cid:112)
C =π (ϵ ) vˆA vˆ (84)
i i i i i i
D =(1−π (ϵ )vˆ)B (85)
i i i i i
16