MATHWRITING: A DATASET FOR HANDWRITTEN
MATHEMATICAL EXPRESSION RECOGNITION
PhilippeGervais*,AsyaFadeeva†,AndriiMaksai
GoogleResearch
{pgervais, fadeich, amaksai}@google.com
March2024
WeintroduceMathWriting,thelargestonlinehandwrittenmathematicalexpressiondatasettodate. Itconsistsof230k
human-written samples and an additional 400k synthetic ones. MathWriting can also be used for offline HME
recognitionandislargerthanallexistingofflineHMEdatasetslikeIM2LATEX-100K[6]. Weintroduceabenchmark
basedonMathWritingdatainordertoadvanceresearchonbothonlineandofflineHMErecognition.
72a078dfeb8e6027 c96d0c67f82ee512 fe938af2c772a57a
ThreeexamplesofHMEfromMathWriting. MoreexamplescanbefoundinappendixD.
Partialdataset(1.5MB):
https://storage.googleapis.com/mathwriting_data/mathwriting-2024-excerpt.tgz
Fulldataset(2.9GB):
https://storage.googleapis.com/mathwriting_data/mathwriting-2024.tgz
Associatedcode:
https://github.com/google-research/google-research/tree/master/mathwriting
1 Introduction
Onlinetextrecognitionmodelshaveimprovedalotinthepastfewyears,becauseofimprovementsinmodelstructure
andalsobecauseofbiggerdatasets. Mathematicalexpression(ME)recognitionisamorecomplextaskthathasnot
receivedasmuchattention. However,theproblemisdifferentfromtextrecognitioninanumberofinterestingways
whichcanpreventimprovementsononetransferingtotheother. ThoughMEssharewithtextmostoftheirsymbols,
they follow a more rigid structure which is also two-dimensional. Where text can be treated to some extent as a
one-dimensional problem amenable to sequence modeling, MEs cannot, because the relative position of symbols in
spaceismeaningful. Itisnotjustasegmentationproblemeitherbecausetheoutputofarecognizerhastocontainthe
relationship between symbols, serialized in some form (LATEX, a graph, InkML, etc.). Similarly to the case of text,
handwrittenMEs(HME)aremoredifficulttorecognizethanprintedonesbecausetheyaremoreambiguous,andfor
severalotherreasonsthatweelaborateonbelow.
Intermsofdata,handwrittensamplesarecostlytoobtainbecausetheyhavetobewrittenbyhumanbeings,whichis
compoundedinthecaseofonlinerepresentation(ink)bythenecessitytousededicatedhardware(touchscreen,digital
pen,etc.).BypublishingtheMathWritingdataset,wehopetoalleviatesomeoftheneedfordataforresearchpurposes.
∗Firstauthorship
†Correspondingauthor
4202
rpA
61
]VC.sc[
1v09601.4042:viXraMathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
Samplesincludealargenumberofhuman-writteninks, aswellassyntheticones. MathWritingcanreadilybeused
withotheronlinedatasetslikeCROHME[16]orDetexify[2]-wepublishthedatainInkMLformattofacilitatethis.
ItcanalsobeusedforofflineMErecognitionsimplybyrasterizingtheinks,usingcodeprovidedontheGithubpage3.
MathWriting is to the best of the authors’ knowledge the largest set of online HME published so far - both human-
writtenandsynthetic. ItsignificantlyexpandsthesetofsymbolscoveredbyCROHME[16], enablingmoresophis-
ticatedrecognitioncapabilities. Sinceinkscanberasterized,MathWritingcanalsobeenseenaslargerthanexisting
offlineHMEdatasets[6,17,1].Forthesereasonsweintroduceanewbenchmark,applicabletobothonlineandoffline
MErecognition.
Thiswork’smaincontributionsare:
• alargedatasetofHME
• groundtruthexpressionsinnormalizedformtosimplifytrainingandtomakeevaluationmorerobust
• anewbenchmarkbasedonthedataset,withseveralbaselines.
Thepresentpaperfocusesonthehigh-leveldescriptionofthedataset:creationprocess,postprocessing,train/testsplit,
groundtruthnormalization,statistics,andageneraldiscussionofthedatasetcontenttohelppractitionersunderstand
whatcanandcannotbeachievedwithit. Allthelow-leveltechnicalinformationlikefileformatscanbefoundinthe
readme.mdfilepresentattherootofthedatasetarchivelinkedabove. WealsoprovidecodeexamplesonGithub3,to
showhowtoreadthevariousfiles,processandrasterizetheinks,andtokenizetheLATEXgroundtruth.
2 Benchmark
2.1 DatasetOverview
MathWritingcontainsonlineHMEs,thatissequencesoftouchpoints(coordinates)obtainedfromatouchscreenor
digitalpen:
• 253k human-written expressions, with three splits: train, valid, test (respectively 230k, 15k, and 7k
samples.). Seesection3.3
• 6khuman-writtenisolatedsymbols,extractedfromsamplesfromthetrainsplitabove
• 396ksyntheticexpressions,obtainedbystitchingtogetherisolatedsymbolsinboundingboxesobtainedusing
theLATEXcompiler. SeeSection3.4.
MathWritingcovers244mathematicalsymbolsplus10syntactictokensthataredescribedinAppendixC,andalarge
varietyofstructures,includingmatrices. AllsampleshavealabelinnormalizedLATEXnotationtobeusedasground
truth(Section4). Theraw(un-normalized)LATEXlabelsarealsoprovided(seefigure1foranexample). Nosymbol-
levelsegmentationinformationisprovided. LATEXlabelscomepredominantlyfromWikipediawiththeexceptionofa
smallfractionthatweregeneratedwithcode(Section3.5).
MathWriting has been placed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 Interna-
tional4.
2.2 BenchmarkDefinition
WeproposethebenchmarkbasedontheMathWritingdatasetthatevaluatesthequalityofhandwritingmathexpression
recognition. Thisbenchmarkincludestwomajorparts:
• evaluationsamples: thetestsplitofMathWriting.
• metric: character error rate (CER) [11], where a "character" is a LATEX token as defined by the code in
AppendixB.
WeprovideareferenceimplementationoftheevaluationmetricattheGithubpage3. WeproposetheuseofCERasa
metrictomakeresultscomparabletootherrecognitiontasksliketextrecognition[4]. TheuseofLATEXtokensinstead
of unicode characters means that errors on a single non-latin letter (e.g. \alpha instead of a) will be counted as a
singleerrorinsteadofmultiple.
3https://github.com/google-research/google-research/tree/master/mathwriting
4https://creativecommons.org/licenses/by-nc-sa/4.0/
2MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
51b364eb9ba2185a
Figure1: Anexamplefromthetrainsplit,withitslabels:
Raw: f(x)=\frac1e\cdot \sum_{n=0}^{\infty}{n^{x}\over n!}
Normalized: f(x)=\frac{1}{e}\cdot\sum_{n=0}^{\infty}\frac{n^{x}}{n!}
2.3 BaselineRecognitionModels
InTable1weprovideresultsfordifferentmodelsontheMathWritingbenchmark. Allmodelsaretrainedexclusively
ontheMathWritingdatasetapartfromtheOCRAPIthatwastrainedonthemixtureoftasks. Thefollowingmodels
werepickedbasedonmostpopularapproachesinhandwritingrecognition.
CTC Transformer This model is encoder-only with a Connectionist Temporal Classification loss (CTC) [9]. We
trainedfromscratchandexclusivelyonMathWritingamodelsimilarto[4],replacingLSTMlayersbyTransformer
layers. It contains 11 layers with an embedding size of 512. More details about this baseline are provided in Ap-
pendixDof[7].
OCR ThisisGoogle’spubliclyavailableDocumentAIOCRAPI[8],whichprocessesbitmapimages. Ithasbeen
trainedpartlyonsamplesfromMathWriting.Wesentinksrenderedwithblackinkonawhitebackgroundandsearched
foroptimalimagesizeandstrokewidthtogetthebestevaluationresultfromthemodel.
VLM Wefine-tunedalargeVision-LanguageModelPaLI[5]onMathWritingdataset. Itreceivesasinputboththe
inkasasequence(likeCTCTransformer)anditsrasterizedversion(likeOCR).Resultscomefrom[7].
Table1:Recognitionresultsfordifferentmodels.Theevaluationmetricisreportedonboththevalidandtestsplits.
Model Input Parameters CERonvalid CERontest
OCR[8] Image - 6.50 7.17
CTCTransformer[7] Ink 35M 4.41 5.56
PaLI[7] Image+Ink 700M 4.47 5.95
Table 1 shows the evaluation comparison between the four models. The OCR model has no information about the
order of writing and speed (offline recognition), which explains its lower performance than methods that take time
informationintoaccount(onlinerecognition). Thetwoothermethods–PaLIandCTCTransformerperformsignifi-
cantlybetterthanOCR.TheseresultsshowthatourdatasetcanbeusedtotrainclassicalrecognitionmodelslikeCTC
transformeraswellasmorerecentarchitectureslikeVLM.
Figure 2 shows examples of mistakes that models make. Two of the main causes of mistakes are mixing up simi-
lar looking characters like “z” and “2”, and incorrectly nesting a subexpression like not placing it in a subscript or
superscript.
3 DatasetCreationProcess
3.1 InkCollection
Inks were obtained from human contributors through an in-house Android app. The task consisted in copying a
rendered mathematical expression (prompt) shown on the device’s screen using either a digital pen or a finger on
a touch screen. Mathematical expressions used as prompt were first obtained in LATEX format, then rendered into a
bitmap through the LATEX compiler (see Appendix A for the template used). 95% of MathWriting expressions were
3MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
Figure2: ExamplesofrecognitionmistakesfromCTCTransformermodel. Weobservesimilarmistakesfromother
models.
obtained from Wikipedia. The remaining ones were generated to cover underrepresented cases in Wikipedia, like
isolatedletterswithnestedsub/superscriptsorcomplicatedfractions(seeSection3.5).
ContributorswerehiredinternallyatGoogle. 6collectioncampaignswererunbetween2016and2019,eachlasting
between2to3weeks.
3.2 Postprocessing
Weappliednopostprocessingtothecollectedinksotherthandroppingentirelythosethatwerecompletelyunreadable
orhadstraymarks. Inksareprovidedintheiroriginalform,astheywererecordedwiththecollectionapp. Whatwe
didnotdowastodiscardsamplesthatwereveryhardtoreadorambiguous,becausewebelievethistypeofsampleto
beessentialintrainingahigh-qualitymodel.
Somecleanupwasperformedonthelabels(groundtruths). Thegoalwastomakethedatasetbettersuitedtotraining
anMLmodel,andeliminateunavoidableissuesthatoccurredduringthecollection.Aftertrainingsomeinitialmodels,
we manually reviewed samples for which they performed poorly. This helped identify a lot of unusable inks (near-
blank, lots of stray strokes, scribbles, etc.), and a lot of ink/label discrepancies. A fairly common occurrence was a
contributor forgetting to copy part of the prompted expression. We adjusted the label to what was actually written
unlesstheinkcontainedapartially-drawnsymbol,inwhichcasewediscardedthesampleentirely. Inthisprocesswe
eliminatedorfixedaround20ksamples.
Themostimportantpostprocessingstepwastonormalizethelabels: therearemanydifferentwaystowriteamath-
ematicalexpressioninLATEXformatthatwillrendertoimagesthatareequivalentinhandwrittenform. Weapplieda
seriesoftransformationstoeliminateasmanyvariationsaspossiblewhileretainingthesamesemantic. Thisgreatly
improved the performance of models and made their evaluation more precise. We publish both the normalized and
raw(unnormalized)labels,toenablepeopletoexperimentwithothernormalizationprocedures.
This normalization is similar to what [6] did, but pushed further because of the specifics of handwritten MEs. See
Section4formoredetail.
3.3 DatasetSplit
MathWritingiscomposedoffivedifferentsetsofsamples,whichwecall’splits’:train,valid,test,symbols,and
synthetic.
The valid and test splits are the result of multiple operations performed between 2016 and 2019. The first split
operation,performedonthedataavailablein2016,wasbasedonthecontributorid: anygivencontributor’ssamples
would not appear in more than one split (either train, valid, test). This is common practice for handwriting
recognitionsystems,totesthowtherecognizerperformsonunseenhandwritingstyles.
Experimentsthenshowedthatamoreimportantfactorthanthehandwritingstylewaswhetherthelabelhadalready
beenseenduringtraining.Subsequentdatacollectioncampaignsfocusedonincreasinglabelvariety,andnewsamples
wereaddedtovalidandtest,thistimesplitbylabel:agivennormalizedmathematicalexpressionwouldnotappear
in more than one split. In the published version, valid has a 55% (8.5k samples) intersection with train based
on unique normalized labels, and test has an 8% intersection (647 samples). We chose to have a low intersection
betweentrainandtestinordertocorrectlymeasuregeneralizationoftrainedmodelstounseenlabels.
4MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
3.4 SyntheticSamplesandIsolatedSymbols
Inordertofurtherincreasethelabeldiversityusedintrainingwecreatedsyntheticsamples. Thisalsoenabledcom-
pensating for another limitation of the human collection which was the limited length of expressions - because of
the screen size. We used LATEX expressions from Wikipedia that were not present in the data collection, with a 90
percentileofexpressionlengthincharactersof68characters,comparedto51intrain. Thisisespeciallyimportant
asdeepneuralnetsoftenfailtogeneralizetoinputslongerthantheirtrainingdata[3,14]. Usingsyntheticlonginks
togetherwiththeoriginalhuman-writteninkssuccessfullyeliminatedthatproblem[13].
The synthesis technique is as follows: starting from a raw LATEX mathematical expression, we computed a DVI file
using the LATEX compiler, from which we extracted bounding boxes. We then used those bounding boxes to place
handwrittenindividualsymbols,resultinginacompleteexpression.SeeFigure3foranexampleofextractedbounding
boxesandtheresultingsyntheticexample.
Figure3: exampleofasyntheticinkcreatedfromboundingboxeswithlabel((p+q)+(p-q))/2=q
Inks for those individual symbols are all from the symbols split. They have been manually extracted from inks in
train. Foreachsymbolwewantedtosupportwemanuallyselectedstrokescorrespondingtothissymbolfor20-30
distinctoccurrencesintrain,andusedthatinformationtogenerateasetofindividualsymbols.
Asignificantdifferencebetweensyntheticandhuman-writteninksisthestrokeorder. Forsyntheticinks,strokeorder
follows the order of the bounding boxes in the DVI file, which can be different from the usual order of writing for
mathematicalexpressions. However,thewritingorderwithinagivensymbolisconsistentwithhumanwriting.
Similarsynthesistechniqueshavebeenusedby[16]withinks,[6]and[1]withrasterimages.
3.5 AcquisitionofLATEXExpressions
The labels we publish mostly come from Wikipedia (95% of all samples have labels from Wikipedia). A small
part were generated, to cover deeply nested fractions, number-heavy expressions, and isolated letters with nested
superscriptsandsubscripts,whicharerareinWikipedia.
TheextractionprocessfromWikipediafollowedthesesteps:
• download an XML Wikipedia dump which provides Wikipedia’s raw textual content.
enwiki-20231101-pages-articles.xml was used for synthetic samples, older dumps for human-
writtenones
• extract all LATEX expressions from that file. This gives the list of all mathematical expressions in LATEX
notationfromWikipedia
• keepthosewhichcouldbecompiledusingthepackageslistedinAppendixA.Wikipediacontainsasignificant
numberofexpressionsthatarenotacceptedbytheLATEXcompiler,becauseofsyntaxerrorsorotherreasons
• keep only those which can be processed by our normalizer which only supports a subset of all LATEX com-
mandsandstructures
Forexpressionsusedforsynthesis,thefollowingextrastepswereperformed:
• keeponlytheexpressionswhosenormalizedformcontainsmorethanasingleLATEXtoken.Example:\alpha
isrejectedbut\alpha^{2}iskept. Thisstepisusefultoeliminatetrivialexpressionsthatwouldn’taddany
usefulinformation
5MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
Sub/superscriptareputinbraces,\overisreplacedby\frac
Raw: \overline{hu^2}+{1 \over 2}{k_{ap}g_zh^2}
Normalized: \overline{hu^{2}}+\frac{1}{2}k_{ap}g_{z}h^{2}
Subscriptsareputbeforesuperscripts,extraspaceisdropped
Raw: \int^a_{-a}f(x) dx=0
Normalized: \int_{-a}^{a}f(x)dx=0
Singlequotesarereplacedbyasuperscript
Raw: f’(\overline x)
Normalized: f^{\prime}(\overline{x})
Textformattingcommandslike\rmaredropped
Raw: ~A_{0}=\frac{ND}{\sigma_{\rm as}+\sigma_{\rm es}}~
Normalized: A_{0}=\frac{ND}{\sigma_{as}+\sigma_{es}}
Matrixenvironmentswithdelimiterslikebmatrixarereplacedbymatrixsurroundedbydelimiters
Commandslike\cosarereplacedbytheseriesofletters
Raw: \begin{bmatrix} -\sin t \\ \cos t \end{bmatrix}
Normalized: [\begin{matrix}-sint\\ cost\end{matrix}]
Delimitersizemodifierslike\bigaredropped
Raw: \big(\tfrac{a}{N}\big)
Normalized: (\frac{a}{N})
Figure4: Examplesofexpressionnormalization. SeeSection4fordetails.
• de-duplicateexpressionsbasedontheirnormalizedform. e.g. \frac12and\frac{1}{2}normalizetothe
samething,wekeptonlyoneoftheminrawform
• restrictthelistofexpressionstothesamesetoftokensusedinthetrainsplit: ifthenormalizedformofan
expressioncontainedatleastonetokenthatwasnotalsopresentsomewhereintrain,itwasdiscarded.
4 LabelNormalization
Allsamplesinthedatasetcomewithtwolabels: theLATEXexpressionthatwasusedduringthedatacollection(anno-
tationlabelintheInkMLfiles), andanormalizedversionofitmeantformodeltraining, whichisfreefromafew
sources of confusions for an ML model (annotation normalizedLabel). See figure 4 for some examples, and the
followingsectionsformoredetail.
Labelnormalizationcoversthreecategories:
• variations used in print that can’t be reproduced in handwriting - e.g. bold, italic, or that haven’t been
reproducedconsistentlybycontributors.
• non-uniquenessoftheLATEXsyntax. e.g. \frac{1}{2}and1\over 2areequivalent.
• visualvariationsthatcanreproducedinhandwritingbutcan’treliablybeinferredbyamodel. Thisincludes
sizemodifierslike\left,\right.
Weprovidetherawlabelstomakeitpossibletoexperimentwithalternativenormalizationschemes,whichcouldlead
tobetteroutcomesfordifferentapplications.
6MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
4.1 SyntacticVariations
ThereareseveralwaystochangeaLATEXstringwithoutchangingtherenderedoutputsignificantly. Thenormalization
weimplementeddoesthefollowing:
• allunnecessaryspaceisdropped
• allcommandargumentsareconsistentlyputincurlybraces
• superscripts and subscripts are put in curly braces and their order is normalized. e.g. a^2_1 becomes
a_{1}^{2}.
• redundantbracesaredropped
• infixcommandsarereplacedbytheirprefixversions. e.g. \overisreplacedby\frac
• a lot of synonyms are collapsed. e.g. \le and \leq, \longrightarrow and \rightarrow, etc. Some of
thesynonymsareonlysynonymsinhandwriting. Forexample\star(⋆)and∗aredifferentinprint(5-prong
and6-prongstars),butthedifferencewasnotexpressedinhandwritingbyourcontributors.
• functions commands like \sin are replaced by the sequence of letters of the function name (e.g. \sin is
replacedbysin).Thisreducestheoutputvocabulary,andeliminatesasourceofconfusionbecausewefound
thatLATEXexpressionsfromWikipediacomewithamixoffunctioncommandsandsequencesofletters.
• expansionofabbreviations. e.g. \cdots,\ldots,etc. havebeenreplacedbythecorrespondingsequenceof
characters.
• matrixenvironmentsarenormalizedtouseonlythe’matrix’environmentsurroundedbytheproperdelimiters
likebracketsorparentheses.
• \binomisturnedintoa2-elementcolumnmatrix.ExpressionsfromWikipediadidnotusethoseconsistently,
sowemadethechoicetonormalize\binomaway.
4.2 DifferencesBetweenPrintAndHandwriting
Thefollowingcharacteristicscannotberepresentedinhandwritingandhavebeennormalizedaway:
• color
• accuratespacing: e.g. ~,\quad.
• fontstyleandsize: e.g. \mathrm,\mathit,\mathbf,\scriptstyle.
There are others that can be represented in handwriting, but that are not consistent enough in MathWriting to be
preserved:
• font families: Fraktur, Calligraphic. In practice, only Blackboard (\mathbb) has been written consistently
enoughbycontributorsthatwewereabletokeepit: \mathcaland\mathfrakaredropped.
• somevariationslike\rightarrow→and\longrightarrow−→.
• somecharactervariations. e.g. \varrho,\varepsilon
• sizemodifierslike\left,\right,\big. Similarly,variable-widthdiacriticslike\widehat.
4.3 Limitations
The normalization process is purely syntactic, and can not cover cases where the meaning of the expression has to
be taken into account. For example, a lot of expressions from Wikipedia use cos instead of \cos. It is often clear
to a human reader whether the sequence of characters c,o,s represents the \cos command or simply three letters.
However, this can not be reliably inferred by a syntactic parser, for example in tacos vs ta\cos. An alternative
would be to update the raw labels, which we didn’t do because we wanted to keep the information that was used
duringthecollectionasuntouchedaspossible.
Similarly, cases like 10^{-1} usually mean {10}^{-1}, though they render exactlythe same. We made the choice
to normalize to the former because it’s the only option with a purely syntactic normalizer. It’s also better than not
removing these extra braces because it gives more consistent label structures, which simplifies the model training
problem.
7MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
5 DetailedDescription
In this section we describe the main properties of MathWriting dataset and compare it to the existing CROHME23
dataset[16].
Table2: StatisticsondifferentsubsetsofMathWritingdataset.
train synthetic validation test
#distinctinks 230k 396k 16k 8k
#distinctlabels 53k 396k 8k 4k
5.1 LabelStatistics
MathWriting contains 457k unique labels after normalization (Section 4). From Table 2 we see that most unique
expressionsarecoveredbythesyntheticportionofthedataset. However,theabsolutenumberofuniqueexpressions
inhuman-writtenpartisstillhigh. Thisunderlinestheimportanceofsyntheticdataasitallowsmodelstoseeamuch
biggervarietyofexpressions. Itisimportanttonotethatthesyntheticsplithasessentiallynorepeatedexpressions.
On the other hand, in real data multiple different writings of the same expression are quite common, see Figure 5.
Thisfactallowsustoseparatelyevaluatemodel’squalityonexpressionsthatwereobservedduringtrainingandthat
modelhasn’tseenbefore. AsseeninTable3thebiggestintersectioninexpressionsisbetweenvalidationandtrain.
Therefore,theperformancegapbetweenvalidationandtestonMathWritingdatasetshowshowwellmodelsgeneralize
toexpressionsthatwerenotseenintrain.
Figure5:Countsofinkscorrespondingtothesamenormalizedexpression,orderedbyincreasingcount.Eachposition
onthehorizontalcorrespondstoauniquenormalizedexpression. Almost5kuniqueexpressionshavebeenwritten10
timesormorebycontributors.
Table3: Countsofuniquelabelssharedbetweensplits
train synthetic valid test
train - 0 3.6k 355
synthetic 0 - 0 0
valid 3.6k 0 - 239
test 355 0 239 -
Themedianlengthofexpressionsincharactersis26whichiscomparabletooneofthemostpopularEnglishrecogni-
tiondatasetsIAMonDB[10],whichhasmedianof29characters. However,itisimportanttonotethatLATEXexpres-
sionshavesymbolsthatspanmultiplecharacterslike\frac. Themedianlengthofexpressionsintokens(providedin
AppendixC)is17,thusmakingtrainingamodelontokensratherthencharacterseasierduetoshortertargetlengths
[15,12]. WewanttoemphasizethatMathWritingcanbeusedwithadifferenttokenizationschemeandtokenvocab-
ulary from what we propose in Appendix C. In Figure 6 we show the number of occurrences for the most frequent
tokens. Tokens{and}arebyfarthemostfrequentastheyareintegraltotheLATEXsyntax.
8MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
Figure6: HistogramofthemostfrequenttokensinMathWriting.
10thpercentile median 90thpercentile
#strokes 5 14 39
#points 131 350 1069
writingtime(sec) 1.88 6.03 16.42
aspectratio 1.32 3.53 9.85
Table4: InkstatisticsofMathWritingdataset.
5.2 InkStatistics
EachinkinMathWritingdatasetisasequenceofstrokesI=[s ,...,s ],eachstrokes consistingofpoints. Apoint
0 n i
is represented as a triplet (x,y,t) where x and y are coordinates on the screen and t is a timestamp. In Table 4 we
providestatisticsonnumberofstrokes,pointsanddurationofwriting.
It’simportanttonotethat,asinkswerecollectedondifferentdevices,theabsolutecoordinatesvaluescanvaryalot.
Inhuman-writtendatathetimeinformationtalwaysstartsfrom0butitisnotalwaysthecaseinthesyntheticsplit.
Differentsamplesoftenhavedifferentsamplingratesduetotheuseofdifferentdevices. Samplingrateisthenumber
of points that a device has written in one second. This value varies across different inks as seen in Figure 7. One
consequenceisthefactthatthesameinkwrittenontwodifferentdevicescanresultininkswithadifferentnumberof
points.Forhuman-writteninks,thesamplingrateisconsistentbetweenstrokes,butitisnotthecaseforsyntheticones.
Inordertoaccommodateamodelandmakesequencesshorter, inkscanberesampledintimeasshowninFigure8.
Thework[7]resampledinkstoarateof20ms.
Figure7: Histogramofsamplingratesinhuman-writtendataofMathWritingdataset.
9MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
Figure 8: Examples of time resampling with different time periods. Larger periods result in shorter sequences of
points.
Devicetype Ink
GooglePixelBook 51k
GoogleNexus5X 28k
CoolpadMega2.5D 14k
OnePlusOne 13k
GoogleNexus5 11k
GoogleNexus6 11k
GoogleNexus6P 11k
CoolpadMega3 8k
LGOptimusL9 8k
GalaxyGrandDuos 7k
GooglePixelXL 6k
SamsungGalaxyS7 5k
Table 5: Top-12 devices used, with the number of samples obtained from each device. The bias towards Google
devicessimplyreflectstheconditionsinwhichinkswerecollected.
5.3 DevicesUsed
Around 150 distinct device types have been used by contributors. In most cases inks were written on smartphones
using a finger on a touchscreen. However, there are cases where tablets with styluses were used. The main device
used in this case is Google Pixelbook, which accounted for 51k inks total (see Table 5). Out of all device types,
37 contributed more than 1000 inks. Note that writing on a touchscreen with a finger or a stylus results in different
low-levelartifacts.
AlldeviceswererunningthesameAndroidapplicationforinkcollection,regardlessofwhethertheiroperatingsystem
wasAndroidorChromeOS.
5.4 ComparisonWithCROHME23
In this section we compare main dataset statistics of CROHME23 [16] and MathWriting. We normalize LATEX ex-
pressions in both datasets (details in Section 4) in order to estimate label intersection between MathWriting and
CROHME23.
Intermsofoverallsize,MathWritinghasnearly3.9timesasmanysamplesand4.5timesasmanydistinctlabelsafter
normalization, see Table 6. A significant number of the labels can be found in both datasets (47k), but the majority
isdataset-specific. Itisalsoimportanttonotethatbothdatasetsincludesyntheticdata. TheMathWritingdatasethas
morehuman-writteninksthanCROHME23asseeninTable7.
MathWritingcontainsamuchlargervarietyoftokensthanCROHME23. Ithas254distincttokensincludingallLatin
capitallettersandalmosttheentireGreekalphabet. Italsocontainsmatrices,whicharenotincludedinCROHME23.
Therefore,morescientificfieldslikequantummechanics,differentialcalculus,andlinearalgebracanberepresented
usingMathWriting.
10MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
MathWriting CROHME23 Common
Inks 650k 164k 0
Labels 457k 102k 47k
Vocabularytokens 254 105 104
Table6: Countsofinks, distinctlabelsanddistincttokensusedinMathWritingandCROHME23. Thesingletoken
presentinCROHME23butnotinMathwritingistheliteraldollarsign\$.
MathWriting CROHME23
Human-writteninks 253k 17k
Syntheticinks 396k 147k
Table7:Countofhuman-writtenandsyntheticinksforMathWritingandCROHME23. Human-writteninksrepresent
38%ofthetotalforMathWriting,and10%forCROHME23.
6 Discussion
We discuss in this section other aspects of MathWriting not mentioned above, to give a better idea of what can be
achievedwiththedata.
6.1 DifferencesinWritingStyle
The number of contributors was large enough that a variety of writing styles are represented in the dataset. Two
examples can be seen in Figures 9 and 10. Similar though less obvious differences exist for other letters. Style
differences also show through writing order. Figure 11 shows different ways of writing a simple fraction – writing
fractionstop-downorbottom-upislikelyinfluencedbythelanguageonespeaks.
6.2 SourcesofNoise
Theresultofanytaskperformedbyhumanswillcontainmistakes,andMathWritingisnoexception. We’vedoneour
besttoremovemostofthemistakes,butweknowthatsomeremain.
Stray strokes These do not carry any meaning and should be ignored by any recognizer. Since they also appear
in real applications, there could be some benefit in having some in the dataset to teach the model about them. That
said, it being usually easier to add noise rather than to remove it, we made the choice of discarding as many inks
containing stray strokes as possible. Not all inks with stray strokes have been found and removed though (e.g.
train/9e64be65cb874902.inkml that was discovered post-publication). The fraction of inks containing stray
strokesissignificantlylowerthan1%,andshouldnotbeanissuefortrainingamodel.
Incorrectgroundtruth Contributorsdidnotalwayscopythepromptperfectly,leadingtoavarietyofdifferences.
In most of the cases we spotted, we were able to fix the label to match what had actually been written. A short
manual review once the dataset was in its final state showed the rate of incorrect ground truth to be between 1%
and 2%. Most of the mistakes are very minor, usually a single token added, missing or incorrect. Errors here also
comefromambiguitiesormisuseoftheLATEXnotation:expressionscomingfromWikipediacontainsomemisuselike
using\Sigmawhere\sumwasmoreappropriate,\triangleinsteadof\Delta,\triangledowninsteadof\nabla,
d4e9c7b8f1ffb958 0dca307a1895512d 45af9641d8d2ed56
Figure9: Threewaysofwritingalowercase’r’. Fromanecdotalevidence,weassociatethestyleontheleftwithparts
ofEuropeandthestyleontherightwithIndia.
11MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
a933cd67f7891dc8 ecc157b89c3e344d
Figure10: Twowaysofwritinglowercases.
a751880b939d5a9a 8c95114b04a97aa2
0348238e894e8d62 ee557c63b5755a6f
Figure11: Examplesofvariouswritingordersfoundinthetrainingset. Redarrowsshowthemovementofthepen
betweenstrokes. Topleft: mostcommonwritingorder(top-down,fractionbardrawnleft-to-right),topright: fraction
barwrittenfirst,bottomleft: fractionbardrawnright-to-left,bottomright: fractionwrittenbottom-up. Anecdotally,
weassociatethislatterstylewiththeChineselanguagewherefractionsarereadbottom-up.
\begin{matrix}\end{matrix} instead of \binom, and also some handwriting-specific ambiguities like \dagger
vs\topvsT.Therearealsosomeinstanceswherereferencenumbersorextrapunctuationareincluded.
Aggressive normalization While the above sources of noise are unavoidable, normalization is a postprocessing
operation that can in theory be tweaked to perfection. In practice, it’s a compromise between reducing accidental
ambiguities(i.e. removingsynonyms),andremovinginformation. Examples: wemadethechoiceoftreating\binom
asasynonymfora2-elementmatrix.Whileitdoesimproverecognitionaccuracybymakingtheproblemeasier,italso
moves the burden of distinguishing between the two cases to downstream steps in the recognition pipeline. Similar
thingscanbesaidaboutremovingallcommandsthatindicatethattheircontentistextinsteadofmath(e.g. \mbox),
droppingsizemodifiers,rewritingfunctioncommands(e.g. \sin,\cos),etc. Usingadifferentnormalizationcould
provebeneficialdependingonthecontexttherecognizerisusedinpractice. However,forthepurposeofabenchmark
anyreasonablecompromiseisadequate.
6.3 RecognitionChallenges
MathWriting presents some inherent recognition challenges, which are typical of handwritten representations. For
example, it’s not really possible to distinguish these pairs from the ink alone: \frac{\underline{a}}{b} vs
\frac{a}{\overline{b}}, and \overline\omega vs \varpi. We’d like to point out that these ambiguities are
notanissueforhumansinpractice,becausetheyrelyoncontextualinformationtodisambiguate: aparticularwriting
idiosyncrasy,consistencywithnearbyexpressions,knowledgeofthescientificdomain,etc. SeeFigures12and13for
moreexamples.
12MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
357313f77d65b804 b8b8cff97f5c044a
Figure12: Left: characterambiguity. Isit1 ≤ x < x or1 ≤ n < n ? Right: whatisthefractionnesting
n n+1 η η+1
order?
96bf9fb2da96db9e e50b0275ef2c9549
Figure13: Left: \binomor2-elementmatrix? Right: pna =aorpna n=a?
pn p
6.4 DatasetApplicationsandFutureWork
Mathwriting can be used to train recognizers for a large variety of scientific fields, and is also large enough to en-
ablesynthesisofmathematicalexpressions. CombiningitwithotherlargedatasetslikeCROHME23wouldincrease
the variety of samples even further, both in terms of writing style and number of expressions, likely improving the
performanceofamodel.
Bounding box information for synthetic samples is provided to enable researchers to experiment with synthetic ink
generation. Synthetic samples published have been obtained through the straightforward process of pasting inks
of individual symbols exactly where bounding boxes were located. This give samples that reflect the very regular
structuregeneratedbyLATEX. Itispossibletoimprovethisprocessbymodifyingthelocation, sizeororientationof
boundingboxespriortogeneratingthesyntheticinks. ThiswouldsoftenLATEX’srigidstructureandmakesynthetic
dataclosertohumanhandwriting.
Another application of these bounding boxes would be to bootstrap a recognizer that would also return character
segmentationinformation. ThiskindofoutputiscriticalforsomeUIfeatures-forexample, editinganhandwritten
expression.
MathWritingcanalsobeimprovedbyvaryingthelabelnormalization.Changingitcanhavedifferentbenefitsdepend-
ingontheapplication,asmentionedabove. WeprovidethesourceLATEXstringforthatreason.
Pushing the recognition performance past some of the inherent ambiguities present in the data will require some
contextualinformation,likethescientificfield,someinformationaboutthewriter,etc. Someofthisinformationcould
be added post-hoc. A language model trained on a large set of mathematical expressions would be a step in that
direction.
Finally,exploringadifferenttrain/validation/testsplitcouldbringsomeimprovementstothebenchmark. Thevalid
andtestsplitsthatwepublisharetheresultofseveraldecisionsmadeoveralongperiodoftimeandmaynotbethe
optimal way to assess a model’s performance for practical applications. In particular, correctly assessing whether a
HMErecognizerproperlygeneralizesiscomplexandrequiresmoreresearch.
7 Conclusion
We introduced MathWriting, the largest dataset of online handwritten mathematical expressions to date, and a new
benchmark. We hope this will help research in online and offline mathematical expression recognition. We also
encourage data practitioners to build on the dataset. We intentionally chose a file format for MathWriting close to
theoneusedbyCROHMEtofacilitatetheircombineduse. Wealsoprovidedoriginalorintermediaterepresentations
13MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
(raw LATEX strings, bounding boxes) to enable experimentation with the data itself, and suggested a few directions
(Section6.4).
Acknowledgements
Wewarmlythankallthecontributorswithoutwhomthisdatasetwouldnotexist.WethankHenryRowleyandThomas
Deselaersfortheircontributiontoorganizingthedatacollectionandsupportingthiseffortformanyyears. Wethank
AshishMylesandMHJohnsonforrelatedcontributionsthatresultedinimportantdataandmodelimprovements. We
thankVojtaLetalandPedroGonnetfortheircontributionstotheCTCTransformermodelaswellastosyntheticdata.
WethankPeterGarstandJonathanBaccashfortheircontributiontothelabelnormalizer. WethankBlagojMitrevski
andHenryRowleyfortheirusefulsuggestionsregardingthetextofthepaper. WethankourproductcounselsJanel
ThamkulandRachelStiglerfortheirlegaladvice.
References
[1] Aida calculus math handwriting recognition dataset. https://www.kaggle.com/datasets/aidapearson/
ocr-data.
[2] Detexifydata. https://github.com/kirel/detexify-data.
[3] Anil,C.,Wu,Y.,Andreassen,A.,Lewkowycz,A.,Misra,V.,Ramasesh,V.,Slone,A.,Gur-Ari,G.,Dyer,E.,and
Neyshabur,B. Exploringlengthgeneralizationinlargelanguagemodels,2022.
[4] Carbune,V.,Gonnet,P.,Deselaers,T.,Rowley,H.A.,Daryin,A.,Calvo,M.,Wang,L.-L.,Keysers,D.,Feuz,S.,
andGervais,P. Fastmulti-languagelstm-basedonlinehandwritingrecognition,2020.
[5] Chen, X., Wang, X., Changpinyo, S., Piergiovanni, A., Padlewski, P., Salz, D., Goodman, S., Grycner, A.,
Mustafa, B., Beyer, L., Kolesnikov, A., Puigcerver, J., Ding, N., Rong, K., Akbari, H., Mishra, G., Xue, L.,
Thapliyal, A., Bradbury, J., Kuo, W., Seyedhosseini, M., Jia, C., Ayan, B. K., Riquelme, C., Steiner, A., An-
gelova, A., Zhai, X., Houlsby, N., and Soricut, R. Pali: A jointly-scaled multilingual language-image model,
2023.
[6] Deng,Y.,Kanervisto,A.,Ling,J.,andRush,A.M. Image-to-markupgenerationwithcoarse-to-fineattention,
2017.
[7] Fadeeva, A., Schlattner, P., Maksai, A., Collier, M., Kokiopoulou, E., Berent, J., and Musat, C. Representing
onlinehandwritingforrecognitioninlargevision-languagemodels,2024.
[8] GoogleCloud.Detecthandwritinginimage,2023.URLhttps://cloud.google.com/document-ai/docs/
enterprise-document-ocr#ocr_add_ons.
[9] Graves,A.,Fernández,S.,Gomez,F.,andSchmidhuber,J. Connectionisttemporalclassification: Labellingun-
segmentedsequencedatawithrecurrentneuralnetworks.InProceedingsofthe23rdInternationalConferenceon
MachineLearning,ICML’06,pp.369–376,NewYork,NY,USA,2006.AssociationforComputingMachinery.
ISBN1595933832. doi: 10.1145/1143844.1143891. URLhttps://doi.org/10.1145/1143844.1143891.
[10] Liwicki,M.andBunke,H. IAM-OnDB-anon-lineenglishsentencedatabaseacquiredfromhandwrittentexton
awhiteboard. InICDAR’05.IEEE,2005.
[11] Michael, J., Labahn, R., Grüning, T., andZöllner, J. Evaluatingsequence-to-sequencemodelsforhandwritten
textrecognition,2019.
[12] Neishi,M.andYoshinaga,N.Ontherelationbetweenpositioninformationandsentencelengthinneuralmachine
translation. InProceedingsofthe23rdConferenceonComputationalNaturalLanguageLearning(CoNLL),pp.
328–338, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/
K19-1031. URLhttps://aclanthology.org/K19-1031.
[13] Timofeev, A., Fadeeva, A., Afonin, A., Musat, C., and Maksai, A. DSS: Synthesizing Long Digital Ink Us-
ing Data Augmentation, Style Encoding and Split Generation, pp. 217–235. Springer Nature Switzerland,
2023. ISBN 9783031416859. doi: 10.1007/978-3-031-41685-9_14. URL http://dx.doi.org/10.1007/
978-3-031-41685-9_14.
[14] Varis, D. and Bojar, O. Sequence length is a domain: Length-based overfitting in transformer models. In
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for
Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.650. URL http://dx.doi.org/10.
18653/v1/2021.emnlp-main.650.
14MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
[15] Varis, D. and Bojar, O. Sequence length is a domain: Length-based overfitting in transformer models. pp.
8246–8257,012021. doi: 10.18653/v1/2021.emnlp-main.650.
[16] Xie,Y.,Mouchère,H.,Liwicki,F.,Rakesh,S.,Saini,R.,Nakagawa,M.,Nguyen,C.,andTruong,T.-N. ICDAR
2023CROHME:CompetitiononRecognitionofHandwrittenMathematicalExpressions,pp.553–565. 082023.
ISBN978-3-031-41678-1. doi: 10.1007/978-3-031-41679-8_33.
[17] Yuan, Y., Liu, X., Dikubab, W., Liu, H., Ji, Z., Wu, Z., and Bai, X. Syntax-aware network for handwritten
mathematical expression recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,pp.4553–4562,2022.
15MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
A LATEXtemplateforlabelrendering
Allthepackagesanddefinitionsthatarerequiredtocompileallthenormalizedandrawlabels:
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
B TokenizationCode
PythoncodeusedinthisworktotokenizeLATEXmathematicalexpressions.
import re
_COMMAND_RE = re.compile(
r’\\(mathbb{[a-zA-Z]}|begin{[a-z]+}|end{[a-z]+}|operatorname\*|[a-zA-Z]+|.)’)
def tokenize_expression(s: str) -> list[str]:
tokens = []
while s:
if s[0] == ’\\’:
tokens.append(_COMMAND_RE.match(s).group(0))
else:
tokens.append(s[0])
s = s[len(tokens[-1]):]
return tokens
C Tokens
Usingtheabovecodetocomputetokens,thesetofallsamplesinthedataset(human-written,synthetic,fromallsplits)
containthefollowingafternormalization:
• Syntactictokens: _^{}&\\space
• Latinlettersandnumbers: a-zA-Z0-9
• Blackboardcapitalletters\mathbb{A}-\mathbb{Z}\mathbb
• Latinpunctuationandsymbols: ,;: ! ? . ()[]\{\}*/+-\_\&\#\%|\backslash
• Greek letters: \alpha \beta \delta \Delta \epsilon \eta \chi \gamma \Gamma \iota \kappa
\lambda \Lambda \nu \mu \omega \Omega \phi \Phi \pi \Pi \psi \Psi \rho \sigma \Sigma \tau
\theta\Theta\upsilon\Upsilon\varphi\varpi\varsigma\vartheta\xi\Xi\zeta
• Mathematicalconstructs: \frac\sqrt\prod\sum\iint\int\oint
• Diacriticsandmodifiers-Notetheabsenceofthesingle-quotecharacter,whichisnormalizedto^{\prime}:
\hat\tilde\vec\overline\underline\prime\dot\not
• Matrixenvironment: \begin{matrix}\end{matrix}
• Delimiters: \langle\rangle\lceil\rceil\lfloor\rfloor\|
• Comparisons: \ge\gg\le\ll<>
• Equality,approximations: =\approx\cong\equiv\ne\propto\sim\simeq
• Set theory: \in \ni \notin \sqsubseteq \subset \subseteq \subsetneq \supset \supseteq
\emptyset
16MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
• Operators: \times \bigcap \bigcirc \bigcup \bigoplus \bigvee \bigwedge \cap \cup \div \mp
\odot\ominus\oplus\otimes\pm\vee\wedge
• Arrows: \hookrightarrow \leftarrow \leftrightarrow \Leftrightarrow \longrightarrow
\mapsto\rightarrow\Rightarrow\rightleftharpoons\iff
• Dots: \bullet\cdot\circ
• Other symbols: \aleph \angle \dagger \exists \forall \hbar \infty \models \nabla \neg
\partial\perp\top\triangle\triangleleft\triangleq\vdash\Vdash\vdots
D Examplesofinks
This section shows a few examples of rendered inks, so that the reader can get a feel for the kind of data that is in
MathWriting. All samples are from the training set. They have been manually picked to show a variety of sizes,
charactersandstructures.
D.1 Human-WrittenSamples
cf786356546d722c 658e3c257badd8cc 40b3844b5aeaec00 c28701c7369c22ba
97e829ac79e851fe fd676faba32f1cbb 5e0c81acf5ccdfd3 b3ed172628caafe9
6150c57b1a98b5ec bca00e3111b70212 e829d5eb7e7b3b68 44bfa5fc08eb5da5
d233aaf208bd7568 ee6f0bfd294aa209 6579f917f0ba236b cea67d239b9f8884
bb4bca53d0e6336d b14bca3fc2d2819a 2409d2feaa79b9d7 51486ff88b789d6d
02229a0c174d8dbe 478e10a15203fa3a ccb15825579a096b 7a4b95285de0caf0
17MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
ac25b4d053596ded 7d597c52bf8bdd1e acc5f6620fad1ce8 88c3551d373c72e5
5009d32d32f80324 068de3aad90c403c a3cf115524f0c55b 355f5df56a16913a
d9b2ce7aa3495888 adceb80fdadf9f6e 25892f7caeac8c36 41e1261a951c6f33
02a7f7f172671fb4 0d848d4b170d36b9 2adc4f10d42b641c 408f904038dcbba0
D.2 SyntheticSamples: ExpressionsfromWikipedia
133829b5a10b783f 11623165e9bab0e4 5ca38d17bf2bea0a 089650cc894c024b
8e88b75cb5f03bf4 5c1573b41e762307 68e9560a27a093c8 daab3bae071f8bc0
77602abaea39b774 eb817bcbfd11df18 3bd062b33ea5db6f 51ef5122de326151
f2a613fc323df342 cbd19abc03ba4098 1879aa5c882b445d 094c7e52a3f0934d
18MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
1cd654228d7ca6bb fc00050933165b70
60553a301aaf84a3 b2aeaf7a0fd30ed6
254dbc2b3843dcf8 0772aeaac09d3415
20b4ebf292cfa8d1 96613ae167f35f8a
4b1f5165e3698343 6f86778996d5f514
685bd5676bda74ce b79dfccd7f5b5cb4
D.2.1 SyntheticSamples: GeneratedFractions
4677b76acec23465 eef0cd70f8872a9c d9df5fffcfe81d07 244ad5e60c9fea92
88d5f862ad46cb47 9067ae238a278b32 ce12f955b6ba76a4 7fa1aa18a332b211
19MathWriting: ADatasetForHandwrittenMathematicalExpressionRecognition
1c21c51bc1319124 afd5254b25be1256 409a91ba03e3cc7e 486a38bd87b8ed97
05efec2565d6726e 3fdc553e580f2a78 b17c3206c2d610b9 9df33845897752ea
20