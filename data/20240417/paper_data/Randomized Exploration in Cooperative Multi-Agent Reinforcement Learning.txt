Randomized Exploration in Cooperative Multi-Agent
Reinforcement Learning
Hao-Lun Hsu∗‡, Weixin Wang†‡, Miroslav Pajic§, Pan Xu¶
April 17, 2024
Abstract
We present the first study on provably efficient randomized exploration in cooperative
multi-agent reinforcement learning (MARL). We propose a unified algorithm framework for
randomized exploration in parallel Markov Decision Processes (MDPs), and two Thompson
Sampling (TS)-type algorithms, CoopTS-PHE and CoopTS-LMC, incorporating the perturbed-
history exploration (PHE) strategy and the Langevin Monte Carlo exploration (LMC) strategy
respectively, which are flexible in design and easy to implement in practice. For a special class of
parallel MDPs where the transition is (approximately) linear, we theoretically prove that both
√
CoopTS-PHE and CoopTS-LMC achieve a O(cid:101)(d3/2H2 MK) regret bound with communication
complexity O(cid:101)(dHM2), where d is the feature dimension, H is the horizon length, M is the
number of agents, and K is the number of episodes. This is the first theoretical result for
randomized exploration in cooperative MARL. We evaluate our proposed method on multiple
parallelRLenvironments,includingadeepexplorationproblem(i.e.,N-chain),avideogame,and
areal-worldprobleminenergysystems. Ourexperimentalresultssupportthatourframeworkcan
achievebetterperformance,evenunderconditionsofmisspecifiedtransitionmodels. Additionally,
weestablishaconnectionbetweenourunifiedframeworkandthepracticalapplicationoffederated
learning.
1 Introduction
Multi-agent Reinforcement Learning (MARL) has emerged as a potent tool with wide-ranging
applications in diverse fields including robotics (Ding et al., 2020; Liu et al., 2019), gaming (Tsay
et al., 2011; Zhao et al., 2019; Ye et al., 2020), and numerous real-world systems (Bazzan, 2009;
Fei and Xu, 2022; Yeh et al., 2023). This is particularly evident in cooperative scenarios, where
the effectiveness of MARL is enhanced through the implementation of both direct and indirect
communication channels among agents. This requires MARL algorithms to adeptly and flexibly
coordinate communications to optimize the benefits of cooperation. One of the classical challenges
in MARL is to balance between exploration and exploitation, i.e., ensuring that agents not only
effectively utilize existing information but also acquire new knowledge. Recent literature, such as the
∗Duke University; email: hao-lun.hsu@duke.edu
†Duke University; email: weixin.wang@duke.edu
‡Equal contribution
§Duke University; email: miroslav.pajic@duke.edu
¶Duke University; email: pan.xu@duke.edu
1
4202
rpA
61
]GL.sc[
1v82701.4042:viXraworks on cooperative exploration strategies (Hao et al., 2023) and on dynamic exploitation tactics
(Rojas-Córdova et al., 2023) in MARL highlight the intricacies and importance of balancing such
trade-off. As indicated by Hao et al. (2023); Chalkiadakis and Boutilier (2003); Liu et al. (2023),
achieving this equilibrium is crucial for the practical deployment of MARL systems in real-world
scenarios, where unpredictability and the need for rapid adaptation are prevalent.
Optimism in the Face of Uncertainty (OFU) is a popular strategy to address the exploration-
exploitationproblem(Abbasi-Yadkorietal.,2011). OFUstrategyleadstonumerousupperconfidence
bound (UCB)-type algorithms in contextual bandits (Chu et al., 2011; Abbasi-Yadkori et al., 2011;
Li et al., 2017), single-agent reinforcement learning (Jin et al., 2020; Wang et al., 2020a), and more
recently multi-agent reinforcement learning (Dubey and Pentland, 2021; Min et al., 2023). These
algorithms compute statistical confidence regions for the model or the value function, given the
observed history, and perform the greedy policy with respect to these regions, or upper confidence
bounds. Though UCB-based methods give out strong theoretical results, they often have poor
performance in practice (Osband et al., 2013; Osband and Van Roy, 2017). For example, Wang
et al. (2020a) demonstrates that computing the confidence bonus necessitates advanced sensitivity
sampling and the expensive computation makes the practical applications inefficient. It is worth
noting that UCB is mostly constructed based on a linear structure (Chu et al., 2011; Jin et al.,
2020). NeuralUCB is a notable attempt at a nonlinear version while it is infeasible in terms of
computational complexity (Zhou et al., 2020; Xu et al., 2021).
Inspired by Thompson Sampling (TS) (Thompson, 1933), posterior sampling for reinforcement
learning (Agrawal and Jia, 2017; Zhou et al., 2019) involves maintaining a posterior distribution over
the parameters of the Markov Decision Processes (MDP) model parameters. Although conceptually
simple, most existing TS methods require the exact posterior or a good Laplacian approximation (Xu
et al., 2022). Recently, there have been advancements in randomized exploration with approximate
sampling. One important method is perturb-history exploration (PHE) strategy, which involves
introducing random perturbations in the action history of the agent (Kveton et al., 2019, 2020a;
Ishfaq et al., 2021). This randomized exploration approach diversifies the agent’s experience, aiding
in learning more robust strategies in environments with uncertainty and variability. Another effective
method is Langevin Monte Carlo (LMC) method (Xu et al., 2022; Ishfaq et al., 2024; Huix et al.,
2023; Karbasi et al., 2023; Mousavi-Hosseini et al., 2023). Notably, Ishfaq et al. (2024) maintains
the simplicity and scalability of LMC, making it applicable in deep Reinforcement Learning (RL)
algorithms by approximating the posterior distribution of the Q function.
Despite the aforementioned advancements of randomized exploration in bandits and single-agent
RL, there remains a scarcity of research on randomized exploration within cooperative MARL,
which motivates us to present the first investigation into provably efficient randomized exploration
in cooperative MARL, with both theoretical and empirical evidence. We specifically focus on the
applicability in parallel MDPs, aiming to facilitate faster learning and to improve policy optimization
with the same state and action spaces, allowing for leveraging similarities across MDPs. We
theoretically and empirically demonstrate that randomized exploration strategies can be extended to
the multi-agent setting and the benefit of randomized exploration instead of UCB can be significant
from single-agent to multi-agent setting.
In summary, our contributions are as follows:
• We propose a unified algorithm framework for learning parallel MDPs, and apply two TS-related
strategies PHE and LMC for exploration, which leads to the CoopTS-PHE and CoopTS-LMC
algorithms. Different from conventional TS where the computation of posterior is expensive
2Table 1: Comparison on episodic, non-stationary, linear MDPs. We define the average regret as
the cumulative regret divided by the total number of samples used by the algorithm. Here d is the
feature dimension, H is the episode length, K is the number of episodes, and M is the number of
agents in a multi-agent setting.
Randomized Generalizable Communication
Algorithm Regret Setting AverageRegret
Exploration toDeepRL Complexity
L L C CL AO CS MS o sP
o
oyV V o
o
oT C npI I
p
p- -- - --R LU P
T
TLLSL SHC S
S
SV VS VE
-
-BV
P
LI II( M(I ( H((IJ I MDs( si
E
ChZ n h u if na f b (ae a (n e Ot q
e
Oqe y
t
ut a ue et a ratl rte n.
l
s
s, a d
.
)a )e ,2 l lt 0 P 2. ., ,2 0ea 20 22 nl 0) 30 t. 2 )2, l1 42 a) )0 n2 d0 ,) 2021) O
O
O(cid:101)
(cid:101)
(cid:101)O OO O O(cid:101) (cid:101)(cid:101) (cid:101) (cid:101)(
(
(d
d
d( (( ( (d dd d d3 32
2
232 H
H
H3 32 3 2 2
3
2H H H H H2
2
2√
√
√5 2 2 2 2 2√ √ √ √√ M
M
MK K K K KK
K
K) ) ) ) ))
)
)
s s s s m
m
m
mi i i in n n n u
u
u
ug g g g l
l
l
lt
t
t
tl l l le e e e i
i
i
i-
-
-
-- - - - a
a
a
aa a a a g
g
g
gg g g g e
e
e
ee e e e n
n
n
nn n n n t
t
t
tt t t t O
O
OO (cid:101)
(cid:101)
(cid:101)(cid:101) O O O O(cid:101) (cid:101) (cid:101) (cid:101)(
(
(( d
d
d( ( ( (d d d d d3 2
3 2
3
23 2 H
H
H2 2 2 23 3 3 3H H H H H(cid:112)
(cid:112)
(cid:112)3 2 (cid:112) (cid:112) (cid:112) (cid:112)(cid:112) 1
1
1/
/
/1 1 1 11 M
M
M/ / / // K K K KK K
K
K) ) ) )) )
)
)
Y Y Y
Y
YN NNe e e
e
eo oos s s
s
s
Y Y
Y
YN N N Ne e
e
eo o oos s
s
s
dd
d
dHH
H
H– – – – MM
M
M3
2
2
2
and the Laplace approximation causes sampling errors (Riquelme et al., 2018; Kveton et al.,
2020b), our proposed algorithms only require adding standard Gaussian noises to the dataset
(CoopTS-PHE) or to the gradient (CoopTS-LMC) when performing Least-Square Value Iteration,
which is efficient in computation and avoids sampling bias due to the Laplace approximation.
Notably, both algorithms can be easily implemented with deep neural networks which are more
practical than UCB-based algorithms in deep MARL.
• WhenreducedtolinearparallelMDPs,wetheoreticallyprovethatbothCoopTS-PHEandCoopTS-
√ √ √
LMCwithlinearfunctionapproximationcanachievearegretboundO(cid:101)(cid:0) d3/2H2 M(cid:0) dMγ+ K(cid:1)(cid:1)
with communication complexity O(cid:101)(cid:0) (d+K/γ)MH(cid:1), where d is the feature dimension, H is the
horizon length, M is the number of agents, K is the number of episodes for each agent, and γ
is a parameter controlling the communication frequency. When γ = O(K/dM), our algorithms
√
attain O(cid:101)(cid:0) d3/2H2 MK(cid:1) regret with O(cid:101)(dHM2) communication complexity. This result matches
the best communication complexity in cooperative MARL (Min et al., 2023), and the best regret
bounds for randomized RL in the single-agent setting (M = 1) (Ishfaq et al., 2021, 2024). A
comprehensive comparison with baseline algorithms on episodic, non-sationary, linear MDPs is
presented in Table 1.
• We further extend our theoretical analysis to the misspecified setting where both the transition
and reward are approximately linear up to an error ζ and the MDPs could be heterogeneous
across agents, which is a generalized notion of misspecification (Jin et al., 2020). We theoretically
prove when ζ = O(cid:0)(cid:112) d/MK(cid:1), the cumulative regret for CoopTS-PHE matches the result in
the linear homogeneous MDP setting. Simultaneously, when ζ = O(cid:0)(cid:112) 1/MK(cid:1), the cumulative
regret for CoopTS-LMC matches the result in the linear homogeneous MDP setting. This result
indicates that CoopTS-PHE has a slightly higher tolerance on the model misspecification than
CoopTS-LMC.
• We conduct extensive experiments on various benchmarks with comprehensive ablation studies,
including N-chain that requires deep exploration, Super Mario Bros task in a misspecified setting,
and a real-world problem in thermal control of building energy systems. Through empirical
evaluation, we demonstrate that our randomized exploration strategies outperform existing deep
Q-network (DQN)-based baselines. We also show that our random exploration strategies in
cooperative MARL can be adapted to the existing federated RL framework when data transitions
are not shared.
3Notations We denote [n] := {1,2,...,n} for any positive integer n. We use I to denote the d×d
identity matrix. For any vector x ∈ Rd and positive semidefinite matrix Σ ∈ Rd×d, we denote
√
∥x∥ = x⊤Σx. For two positive sequences {a } and {b } with n = 1,2,..., we write a = O(b )
Σ n n n n
if there exists an absolute constant C > 0 such that a
n
≤ Cb
n
holds for all n ≥ 1. We use O(cid:101) to
further hide polylogarithmic terms. For positive semidefinite matrix A and B, we write A ≼ B if
A−B is positive semidefinite.
2 Related Work
Cooperative Multi-Agent Reinforcement Learning Cooperative MARL is closely intertwined
withthedomainofmulti-agentmulti-armedbandits,exemplifiedbydecentralizedalgorithmsfeaturing
communication across a network or hypergraphs (Landgren et al., 2016; Zhang et al., 2023; Jin et al.,
2024) and distributed settings (Hillel et al., 2013; Wang et al., 2020b). Cooperative MARL manifests
primarily in two categories: multi-agent MDPs (Boutilier, 1996; Zhang et al., 2018; Xie et al., 2020;
Dubey and Pentland, 2021) and parallel MDPs (Bernstein et al., 2002; Dubey and Pentland, 2021;
Lidard et al., 2022; Bernstein et al., 2002; Min et al., 2023). In the realm of cooperative multi-agent
robotics, the former is employed to formulate optimal multi-agent policies across the distributed
system (Yu et al., 2022, 2023). On the other hand, homogeneous parallel MDPs leverage inter-agent
communication to expedite learning processes (Kretchmar, 2002). Additionally, heterogeneous
parallel MDPs establish connections to heterogeneous federated learning (Li et al., 2020) and exhibit
improved generalizability in transfer learning scenarios (Taylor and Stone, 2009).
We focus on parallel MDPs in this paper, where agents interact with the environment simulta-
neously to tackle shared challenges within extensive and distributed systems (Kretchmar, 2002).
Recently, Dubey and Pentland (2021) proposed the Coop-LSVI algorithm, extending the LSVI-UCB
algorithm (Jin et al., 2020) in single-agent RL to MARL with linear MDPs. In a parallel RL setting
with asynchronous communication, Min et al. (2023) builds upon Coop-LSVI while relinquishing
compatibility with heterogeneous MDPs. Meanwhile, Lidard et al. (2022) focuses on fully decen-
tralized multi-agent UCB Q-learning in a tabular setting, maintaining polynomial space complexity
even as the number of agents increases. However, it is worth noting that neither of the previous
works (Dubey and Pentland, 2021; Min et al., 2023) in non-tabular cooperative MRAL provides
experimental validation for the efficacy of their proposed communication strategies. The gap arises
from their reliance on LSVI-UCB as the core algorithm, wherein optimism is instantiated through
UCB. Empirical evidence suggests that UCB-based approaches tend to underperform in practical
scenarios (Osband et al., 2013; Osband and Van Roy, 2017; Ishfaq et al., 2024). Moreover, the
computational demands of LSVI-UCB become untenable due to the necessity of recurrently com-
puting the feature covariance matrix for updating the UCB bonus function. Therefore, randomized
exploration in this work is critical to make these algorithm designs practical.
Randomized Exploration The roots of randomized exploration, particularly TS, can be traced
back to its success in bandit problems (Thompson, 1933). Randomized exploration strategies can
typically exhibit superior performance in practical applications due to avoidance of early convergence
to suboptimal actions (Jin et al., 2021, 2022b, 2023). Furthermore, these strategies demonstrate
robustnessinthefaceofnoiseanduncertainty,particularlywithinnon-stationaryenvironments(Wang
and Zhou, 2020; Bakshi et al., 2023). This success has extended to Langevin Monte Carlo Thompson
Sampling (LMCTS), which has been applied to various domains, including linear bandits, generalized
4linear bandits, and neural contextual bandits (Xu et al., 2022). The exploration of posterior sampling
techniques in RL has gained prominence, building upon the foundation laid by TS (Strens, 2000;
Agrawal and Jia, 2017). Randomized Least-Square Value Iteration (RLSVI) is an approach that
leverages random perturbations to approximate the posterior, with frequentist regret analysis applied
under the tabular MDP setting (Osband et al., 2016b), inspiring subsequent works focusing on
theoreticalanalysesaimedatimprovingworst-caseregretundertabularMDPs(Russo,2019;Agrawal
et al., 2021), with extensions to the linear setting (Zanette et al., 2020; Ishfaq et al., 2021; Dann et al.,
2021). In addition to theoretical advancements, several practical algorithms have been proposed
based on RLSVI to approximate posterior samples of Q functions in deep RL. These approaches
involve ensembles of randomly initialized neural networks (Osband et al., 2016a, 2018) and noise
injection into the parameters of the neural network (Fortunato et al., 2018; Li et al., 2022). With
the success of LMCTS (Xu et al., 2022) in bandit domains, the exploration of randomized methods
has expanded to alternative approaches like LMC in tabular RL (Karbasi et al., 2023) and linear
MDPs with neural network approximation (Ishfaq et al., 2024). Further works delve into the realm
of random exploration from the perspectives of delayed feedback (Kuang et al., 2023) and offline
RL (Nguyen-Tang and Arora, 2023).
While posterior sampling demonstrates superiority in various contexts, its theoretical foundations
in the multi-agent setting remain underexplored. Existing research predominantly focuses on two-
player zero-sum games, considering both Bayesian (Zhou et al., 2019; Jafarnia-Jahromi et al., 2023)
and frequentist regrets (Xiong et al., 2022; Qiu et al., 2023). There is no existing work studying
randomized exploration for cooperative multi-agent settings.
3 Preliminary
InparallelMarkovDecisionProcesses(MDPs), M agentsinteractindependentlywiththeirrespective
discrete-time MDPs. All agents have the same but independent state and action spaces. Each
agent might have its unique reward functions and transition probabilities. Specifically, for agent
m ∈ M, the associated MDP is defined by the tuple MDP(S,A,H,P ,r ). Here S and A are
m m
the state and action spaces respectively. The transition probabilities for agent m are denoted by
P = {P } , where P : S ×A → S is the transition probability at step h, and H is the
m m,h h∈[H] m,h
horizon length. r = {r } are the reward functions, with r : S×A → [0,1]. The policy for
m m,h h∈[H] m,h
agent m is represented by π = {π } , consisting of a set of H functions where π : S → A.
m m,h h∈[H] m,h
In each episode k = 1,2,..., each agent m ∈ M adopts a fixed policy πk = {πk } and
m m,h h∈[H]
begins in an initial state sk chosen arbitrarily by the environment. During each step h ∈ [H] in
m,1
this episode, the agent observes its current state sk , selects an action ak ∼ πk (·|sk ), receives
m,h m,h m,h m,h
a reward r (sk ,ak ), and then transitions to the next state sk based on the transition
m,h m,h m,h m,h+1
probability P (·|sk ,ak ). The reward defaults to 0 when the episode terminates at step H +1.
m,h m,h m,h
The effectiveness of any policy π in the mth MDP is evaluated by the value function Vπ (s) :
m,h
S → R, for any s ∈ S,h ∈ [H],m ∈ M, which is defined as follows
(cid:20) (cid:88)H (cid:12) (cid:21)
Vπ (s) := E r (s ,a )(cid:12)s = s .
m,h π m,h′ m,h′ m,h′ (cid:12) m,h
h′=h
Furthermore, the state-action value function Qπ (s,a) : S ×A → R, which calculates the total
m,h
expected reward from an action-state pair at step h for any (s,a) ∈ S ×A in the mth MDP, is
5defined as follows
(cid:20) (cid:88)H (cid:12) (cid:21)
Qπ (s,a) := E r (s ,a )(cid:12)s = s,a = a .
m,h π m,h′ m,h′ m,h′ (cid:12) m,h m,h
h′=h
The optimal policy for the mth MDP is denoted as π∗ , and we denote V∗ (s) = Vπ m∗ (s). The
m m,h m,h
cumulative group regret for total K episodes is defined as
K
Regret(K) = (cid:88) (cid:88)(cid:2) V∗ (cid:0) sk (cid:1) −Vπ m,k(cid:0) sk (cid:1)(cid:3) .
m,1 m,1 m,1 m,1
m∈Mk=1
4 Algorithm Design
In this section, we first present a unified algorithm framework for conducting randomized exploration
in cooperative MARL. Then we introduce two practical randomized exploration strategies. At the
end of this section, we discuss the instantiation of our algorithm in the linear structure.
4.1 Unified Algorithm Framework
A unified algorithm framework is presented in Algorithm 1. The core idea is that each agent
executes Least-Square Value Iteration (LSVI) in parallel and makes decisions based on collective
data obtained from communication between each agent and the server. Before we describe the details
of our algorithm, we first define notations about the datasets stored on each agent’s local machine
and the server.
Index notation We define k (k) (denoted as k when no ambiguity arises) as the last episode
s s
before episode k where synchronization happens. For episode k and step h, we define three datasets
as follows.
User(k) = (cid:8)(cid:0) sτ ,aτ ,sτ (cid:1)(cid:9) , (4.1a)
h n,h n,h n,h+1 n∈M,τ∈[ks]
Uloc (k) = (cid:8)(cid:0) sτ ,aτ ,sτ (cid:1)(cid:9)k−1 , (4.1b)
m,h m,h m,h m,h+1 τ=ks+1
(cid:91)
U (k) = User(k) Uloc (k). (4.1c)
m,h h m,h
By definition, User(k) is the dataset that is shared across all agents due to the latest synchronization
h
at episode k . Uloc (k) is the unique data collected by agent m since episode k . Then U (k) is the
s m,h s m,h
total dataset available for agent m at the current time. Let K(k) = |U (k)| be the total number of
m,h
data points. For the simplicity of notation, we also re-order the data points in U (k), and rename
m,h
the tuple (sτ ,aτ ,sτ ) as (sl,al,s′l) such that we have U (k) = (cid:83)K(k) (sl,al,s′l). In fact,
m,h m,h m,h+1 m,h l=1
this can be done by the following one-to-one mapping
(cid:40)
(τ −1)M +n τ ≤ k ,
l (n,τ) = s (4.2)
m,k
(M −1)k +τ k < τ ≤ k−1.
s s
Therefore, we use indices (s,a,s′) ∈ U (k) and l ∈ [K(k)] interchangeably for the summation over
m,h
set U (k).
m,h
6Algorithm 1 Unified Algorithm Framework for Randomized Exploration in Parallel MDPs
1: Initialization: set User(k),Uloc (k) = ∅.
h m,h
2: for episode k = 1,...,K do
3: for agent m ∈ M do
4:
Receive initial state sk .
m,1
5: Vk (·) ← 0.
m,H+1
6: {Qk (·,·)}H ←Randomized Exploration ◁ Algorithm 2 or Algorithm 3
m,h h=1
7: for step h = 1,...,H do
8: ak ← argmax Qk (sk ,a).
m,h a∈A m,h m,h
9: Receive sk
m,h+1
and r h.
10: Uloc (k) ← Uloc (k)(cid:83)(cid:0) sk ,ak ,sk (cid:1).
m,h m,h m,h m,h m,h+1
11: if Condition then
12: SYNCHRONIZE ← True.
13: end if
14: end for
15: end for
16: if SYNCHRONIZE then
17: for step h = H,...,1 do
18: ∀ AGENT: Send Uloc (k) to SERVER.
m,h
19: SERVER: Uloc(k) ← (cid:83) Uloc (k).
h m∈M m,h
20: SERVER: User(k) ← User(k)(cid:83) Uloc(k).
h h h
21:
SERVER: Send User(k) to each AGENT.
h
22: ∀ AGENT: Set Uloc (k) ← ∅.
m,h
23: end for
24: end if
25: end for
Algorithm interpretation At a high level, each episode k in Algorithm 1 consists of two stages.
The first stage (Lines 3-15) is parallelly executed by all agents and the second stage (Lines 16-24)
involves the communication among agents and the server.
In the first stage (Lines 3-15) of Algorithm 1, each agent m operates in two parts. The first part
(Line 6) updates estimated Q functions {Qk }H through LSVI with a randomized exploration
m,h h=1
strategy (Algorithm 2 or Algorithm 3, which will be introduced in Section 4.2). In particular, given
the estimated value functions Vk (·) = max Qk (·,a) at step h+1, we perform one step
m,h+1 a∈A m,h
robust backward Bellman update to obtain Vk (·) at step h. And we initialize Vk (·) to be 0
m,h m,H+1
(Line 5).
In the second part (Lines 7-14), after obtaining the estimated Q functions, in each step h we
execute the greedy policy with respect to Qk and collect new data points which are added to
m,h
the local dataset Uloc (k) (Lines 8-10). Then we verify the synchronization condition (Lines 11-13).
m,h
In this paper, we mainly use three types of synchronization rules. (1) We can synchronize every c
episode where c is a user-defined constant, which is easy to implement in practice. (2) We can also
synchronize at the episode of b1,b2,...,bn, with b representing the base of the exponential function.
This is guided by the intuition that agents require more transitions urgently at the early learning
stages. (3) Additionally, if we have a feature mapping ϕ(s,a) : S ×A → Rd, based on (4.1), we
7define the following empirical covariance matrices.
serΛk = (cid:88) ϕ(cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1)⊤ ,
h
(sl,al,s′l)∈User(k)
h
locΛk = (cid:88) ϕ(cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1)⊤ ,
m,h
(sl,al,s′l)∈Uloc (k)
m,h
Λk = serΛk + locΛk +λI.
m,h h m,h
We synchronize as long as the following condition is met,
det(cid:0)serΛk +locΛk +λI(cid:1)
γ
log h m,h ≥ , (4.3)
det(cid:0) serΛk h+λI(cid:1) (k−k s)
where γ is a communication control factor. In our experiments, we try all three rules and compare
their performance, which is discussed in detail in Appendix I.1.
The second stage (Lines 16-24) is executed only when the synchronization condition is satisfied.
First, all the agents upload their local transition set Uloc (k), i.e., the newly collected local data after
m,h
the last synchronization, to the server. Then, the server gathers all information together in User(k)
h
and sends it back to each agent. Finally, each agent resets the local transition set Uloc (k) ← ∅. Now
m,h
agent m can access the dataset U (k) = User(k)(cid:83) Uloc (k), which contains the historical data of
m,h h m,h
all agents up to last synchronization and its local dataset.
4.2 Randomized Exploration Strategies
When we update the model parameter and estimate Q functions in Algorithm 1 (Line 6), we use
exploration strategies to avoid suboptimal policies. Previous work adopted Upper Confidence Bound
(UCB) exploration in the linear function class (Dubey and Pentland, 2021; Min et al., 2023) to
estimate the Q function {Qk }H . Although UCB-based methods come with strong theoretical
m,h h=1
guarantees, they often perform poorly in practice (Chapelle and Li, 2011; Osband et al., 2013;
Osband and Van Roy, 2017). Moreover, UCB requires a precise computation of the confidence
set, which is usually hard to be implemented beyond the linear structure. In contrast, randomized
exploration strategies exhibit more robust performance, are flexible in design and easy to implement,
and do not require a linear structure.
WeapproximatetheQfunctionswiththefollowingfunctionclassF = {f : S×A → R|f (s,a) =
w w
f(w;ϕ(s,a))}, where w ∈ Rd is the parameter and ϕ ∈ Rd is a feature mapping associated with
state-action pairs. Now we define the loss function for estimating the Q functions.
K(k)
Lk (w) = (cid:88) L(cid:0) rl +Vk (s′l ),f(cid:0) w;ϕl(cid:1)(cid:1) +λ∥w∥2, (4.4)
m,h h m,h+1
l=1
where rl = r (cid:0) sl,al(cid:1), ϕl = ϕ(cid:0) sl,al(cid:1), and L is a user-specified loss function.
h h
Perturbed-History Exploration The first strategy we use in Algorithm 1 is calledthe perturbed-
history exploration (Kveton et al., 2019, 2020a; Ishfaq et al., 2021), displayed in Algorithm 2. We
8refer to the resulting algorithm as CoopTS-PHE. In particular, we optimize the following randomized
loss function, where we add random Gaussian noises to the rewards and regularizer in (4.4).
K(k)
L(cid:101)k,n (w) = (cid:88) L(cid:0)(cid:0) rl +ϵk,l,n(cid:1) +Vk (s′l ),f(cid:0) w;ϕl(cid:1)(cid:1) +λ∥w+ξk,n∥2, (4.5)
m,h h h m,h+1 h
l=1
where ϵk,l,n i. ∼i.d N(0,σ2), ξk,n ∼ N(0,σ2I), and n ∈ [N]. Then we obtain the following perturbed
h h
estimated parameter
w (cid:101)mk,n
,h
= argminL(cid:101)k m,n ,h(w). (4.6)
w∈Rd
Note that we repeat the above steps for n = 1,...,N to obtain independent copies of parameters,
which is referred to as the multi-sampling process (Ishfaq et al., 2021, 2024). Then we obtain the
estimated Q function Qk based on Line 7 in Algorithm 2. Finally, by maximizing Qk over action
m,h m,h
space A, we obtain the estimated value function Vk .
m,h
Algorithm 2 Perturbed-History Exploration
1: Input: multi-sampling number N ∈ N+, function class F = {f w : S × A → R|f w(s,a) =
f(w;ϕ(s,a))}.
2: for step h = H,...,1 do
3: for n = 1,...,N do
4: Sample {ϵk,l,n} i. ∼i.d N(0,σ2) and ξk,n ∼ N(0,σ2I) independently.
h l∈[K(k)] h
5: Solve w (cid:101)mk,n
,h
according to (4.6).
6: end for
7: Qk
m,h
← min(cid:8) max n∈[N]f(cid:0) w (cid:101)mk,n ,h;ϕ(cid:1) ,H −h+1(cid:9)+.
8: V mk ,h(·) ← max a∈AQk m,h(·,a).
9: end for
10:
Output: {Qk (·,·),Vk (·,·)}H .
m,h m,h h=1
Langevin Monte Carlo Exploration Next we introduce the Langevin Monte Carlo exploration
strategy (Xu et al., 2022; Ishfaq et al., 2024) (displayed in Algorithm 3), which stems from the
Langevin dynamics (Roberts and Tweedie, 1996; Bakry et al., 2014; Dalalyan, 2017; Xu et al.,
2018; Zou et al., 2021). Combining it with Algorithm 1 leads to our second proposed algorithm,
CoopTS-LMC. Specifically, we update the model parameter iteratively. For iterate j = 1,...,J ,
k
the update is given by
(cid:113)
wk,j,n = wk,j−1,n−η ∇Lk (cid:0) wk,j−1,n(cid:1) + 2η β−1 ϵk,j,n, (4.7)
m,h m,h m,k m,h m,h m,k m,k m,h
where Lk is defined in (4.4), ϵk,j,n ∈ Rd is a standard Gaussian noise, η is the learning rate,
m,h m,h m,k
and β is the inverse temperature parameter. We again use the multi-sampling trick to obtain N
m,k
independent estimators and similarly obtain the estimated Q function Qk by truncation based on
m,h
Line 10 in Algorithm 3.
9Algorithm 3 Langevin Monte Carlo Exploration
1: Input: multi-sampling number N ∈ N+, function class F = {f w : S × A → R|f w(s,a) =
f(w;ϕ(s,a))}, step sizes {η } , inverse temperature parameters {β } .
m,k m∈M,k∈[K] m,k m∈M,k∈[K]
2: for step h = H,...,1 do
3: for n = 1,...,N do
4: wk,0,n = wk−1,J k−1,n.
m,h m,h
5: for j = 1,...,J k do
6:
Sample ϵk,j,n i. ∼i.d N(0,I).
m,h
7:
Update wk,j,n by (4.7).
m,h
8: end for
9: end for
10: Qk ← min(cid:8) max f(cid:0) wk,J k,n;ϕ(cid:1) ,H −h+1(cid:9)+.
m,h n∈[N] m,h
11: V mk ,h(·) ← max a∈AQk m,h(·,a).
12: end for
13:
Output: {Qk (·,·),Vk (·,·)}H .
m,h m,h h=1
4.3 Instantiation in the Linear Function Class
In this section, we specifically discuss our TS-related algorithms in the linear structure, which is
under the assumption of linear function approximation and linear MDP setting. We first present the
definition of linear MDPs.
Definition 4.1 (Linear MDP (Jin et al., 2020)). An MDP(S,A,H,P ,r ) is a linear MDP with
m m
feature map ϕ : S ×A → Rd, if for any h ∈ [H], there exist d unknown measures µ = (µ1,...,µd)
h h h
over S and an unknown vector θ ∈ Rd such that for any (s,a) ∈ S ×A,
h
P (·|s,a) = (cid:10) ϕ(s,a),µ (·)(cid:11) , r (s,a) = (cid:10) ϕ(s,a),θ (cid:11) .
h h h h
Withoutlossofgenerality,weassumethatforall(s,a) ∈ S×A,∥ϕ(s,a)∥ ≤ 1andmax{∥µ (S)∥,∥θ ∥}
√ h h
≤ d.
Recall from the loss function in (4.4), here we choose L to be l loss and linear function class
2
f(w;ϕl) = w⊤ϕl. By solving this least-square regression problem, we obtain the unperturbed
regression estimator wk . In the linear setting, we have the closed-form solution
(cid:98)m,h
wk = (Λk )−1bk , (4.8)
(cid:98)m,h m,h m,h
where Λk and bk are defined as follows
m,h m,h
K(k)
Λk = (cid:88) ϕ(cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1)⊤ +λI,
m,h
l=1
K(k)
bk = (cid:88)(cid:2) r (cid:0) sl,al(cid:1) +Vk (cid:0) s′l(cid:1)(cid:3) ϕ(cid:0) sl,al(cid:1) .
m,h h m,h+1
l=1
10A natural way of doing randomized exploration is to add a noise N(0,σ2(Λk )−1) to wk
m,h (cid:98)m,h
and get the estimated parameter w¯k . Then we can construct estimated Q function Qk (·,·) =
m,h m,h
min{ϕ(·,·)⊤w¯k ,H − h + 1}+. We call this method as CoopTS, which is aligned with other
m,h
linear TS algorithms (Agrawal and Goyal, 2013a; Abeille and Lazaric, 2017). In what follows, we
theoretically show that our proposed algorithms are equivalent or approximately converge to the
CoopTS algorithm in the linear function approximation setting.
For CoopTS-PHE (Algorithm 1+Algorithm 2), let the function approximation in (4.5) be linear
and choose L to be the squared loss. By solving this least-square regression problem, we obtain
the perturbed regression estimator wk,n in CoopTS-PHE. The following proposition conveys that
(cid:101)m,h
CoopTS-PHE is actually equivalent to CoopTS.
Proposition 4.2 (Equivalent to CoopTS). The output wk,n by CoopTS-PHE is equivalent to
(cid:101)m,h
adding a Gaussian vector to the unperturbed regression estimator wk , i.e., wk,n = wk +ζk,n,
(cid:98)m,h (cid:101)m,h (cid:98)m,h m,h
where ζk,n ∼ N(0,σ2(Λk )−1).
m,h m,h
For CoopTS-LMC (Algorithm 1+Algorithm 3), let function approximation in (4.4) be linear
and choose L to be l loss to get the loss function. Then after finishing the LMC update, we
2
get the estimated parameter wk,J k,n and construct the model approximation of Q function. The
m,h
following proposition conveys that the distribution of wk,J k converges to the posterior distribution
m,h
of Thompson Sampling exploration. The proof of this proposition is given in Xu et al. (2022).
Proposition 4.3 (Approximately equivalent to CoopTS (Xu et al., 2022)). If the epoch length
J in Algorithm 3 is sufficiently large, the distribution of wk,J k converges to Gaussian distribution
k m,h
N(wk ,β−1 (Λk )−1 ).
(cid:98)m,h m,k m,h
Propositions 4.2 and 4.3 indicate that the results of our two randomized exploration strategies
are closely related to CoopTS. As we have mentioned above, in CoopTS, the estimated parameter
w¯k is sampled from the normal distribution N(wk ,σ2(Λk )−1). However, in practice, this
m,h (cid:98)m,h m,h
sampling is often executed in this way: we sample β ∼ N(0,I) first, then we calculate w¯k =
m,h
w (cid:98)mk
,h
+ σ(Λk m,h)−1 2β and obtain the estimated parameter. Nevertheless, computing (cid:0) Λk m,h(cid:1)−1 2
can be computationally expensive, often requiring at least O(d3) operations with the Cholesky
decomposition, making it impractical for high-dimensional machine learning challenges. Additionally,
the Gaussian distribution used in Thompson Sampling may not effectively approximate the posterior
distribution in more complex bandit models than the linear MDP due to their intricate structures.
Moreover, as pointed out by recent work (Chapelle and Li, 2011; Riquelme et al., 2018; Kveton
et al., 2020b; Xu et al., 2022), the Laplace approximation-based Thompson Sampling exhibits a
constantapproximationerrorintheestimationoftheposteriordistribution. Therefore, itnecessitates
a careful redesign of the covariance matrix to ensure effective performance.
Advantages of PHE and LMC As mentioned above, computing (cid:0) Λk (cid:1)−1 2 can be computation-
m,h
ally expensive. However, Perturbed-History exploration and Langevin Monte Carlo exploration can
avoid this. For PHE, by only adding i.i.d random Gaussian noise to perturb reward and regularizer,
its performance will be equivalent to TS. For LMC, by only performing noisy gradient descent, we
can do the randomized exploration, resulting in similar performance compared with TS. Additionally,
these two methods can easily be implemented to general function class while Thompson Sampling
11usually cannot be generalized except for the linear setting. In summary, these two methods are both
flexible in design and easy to implement in practice.
Communication cost We emphasize that agents can just send compressed statistics to the
server under the linear setting, which can largely reduce communication cost. In the linear function
class, we can calculate the closed-form solution of the regression problem (4.8). In this case, when
synchronization process is met, all the agents will only need to send their calculated local statistics
locΛk and locbk to help solve the regression problem. This communication cost is much smaller
m,h m,h
becauseΛisonlyad×dmatrixandbisonlyad-dimensionalvector, wheredisthefeaturedimension
in linear MDP assumption. This can also avoid privacy disclosure through communications.
Nevertheless, in the general function class setting, our proposed algorithms still require sharing
all the collected datasets, which will cause relatively large communication cost. Additionally, in
Appendix I.2, we also propose a federated setting algorithm Algorithm 4. In this setting, instead of
sharing collected datasets, agents can just share the weight of the collected estimated Q functions,
which can largely reduce the communication cost.
5 Theoretical Analysis
In this section, we provide theoretical analyses of our algorithms in the linear structure, which is
under the assumption of linear function approximation and linear MDP setting.
5.1 Homogeneous Parallel Linear MDPs
Throughout the analyses in this subsection, we assume all agents share the same linear MDP defined
in Definition 4.1. First, we have the following regret bound for CoopTS-PHE.
Theorem 5.1. In CoopTS-PHE (Algorithm 1+Algorithm 2), let N = C(cid:101)log(δ)/log(c 0) where
C(cid:101) = O(cid:101)(d) and c
0
= Φ(1), Φ(·) is the cumulative distribution function (CDF) of the standard normal
distribution. Let λ = 1 and 0 < δ < 1. Under the determinant synchronization condition (4.3), we
obtain the following cumulative regret
√ √
Regret(K) = O(cid:101)(cid:0) d3 2H2 M(cid:0)(cid:112) dMγ + K(cid:1)(cid:1) ,
with probability at least 1−δ.
Remark 5.2. Whenwechooseγ = O(K/dM)inthesynchronizationcondition(4.3),thecumulative
√
regret of CoopTS-PHE becomes O(cid:101)(d3/2H2 MK), which matches the re √sult of UCB exploration
(Dubey and Pentland, 2021). When M = 1, the regret becomes O(cid:101)(d3/2H2 K), which matches the
existing best single-agent result (Jin et al., 2020; Ishfaq et al., 2021, 2024). Note that if there is
no communication at all and agents act independently, with the same number of learning rounds
√
(or samples), the cumulative regret becomes O(cid:101)(M ·d3/2H2 K). By incorporating commun √ication,
our regret bound in Theorem 5.1 is lower than that of the independent setting by a factor M. A
similar strategy called rare-switching update with a determinant synchronization condition has also
been adopted in parallel bandit problems (Ruan et al., 2021; Chan et al., 2021).
Similarly, we have the following result for CoopTS-LMC.
12Theorem 5.3. In CoopTS-LMC (Algorithm 1+Algorithm 3), let N = C¯log(δ)/log(c′) where
√ √ 0
c′
0
= 1−1/2 2eπ and C¯ = O(cid:101)(d). Let 1/(cid:112) β
m,k
= O(cid:101)(cid:0) H d(cid:1) for all m ∈ M, λ = 1, and 0 < δ < 1.
Foranyepisodek ∈ [K]andagentm ∈ M,letthelearningrateη = 1/(cid:0) 4λ (cid:0) Λk (cid:1)(cid:1),theupdate
m,k max m,h
number J = 2κ log(4HKMd) where κ = λ (cid:0) Λk (cid:1) /λ (cid:0) Λk (cid:1) is the condition number of
k k k max m,h min m,h
Λk . Under the determinant synchronization condition (4.3), we have
m,h
√ √
Regret(K) = O(cid:101)(cid:0) d3 2H2 M(cid:0)(cid:112) dMγ + K(cid:1)(cid:1) ,
with probability at least 1−δ.
Remark 5.4. Note that CoopTS-PHE and CoopTS-LMC have the same order of regret. Hence the
discussion in Remark 5.2 also applies to CoopTS-LMC. We would also like to highlight that our
results are the first rigorous regret bounds of randomized MARL algorithms.
Fromtheperspectiveoftechnicalnovelty,ouranalysisofrandomizedMARLalgorithmsisdifferent
from that of UCB-based algorithms (Dubey and Pentland, 2021) because the model prediction
error here contains randomness, causing a more complex probability analysis and an additional
approximation error. We would also like to point out that in proofs for both CoopTS-LMC and
CoopTS-PHE we use a new ε-covering technique to prove that the optimism lemma holds for all
(s,a) ∈ S ×A instead of just the state-action pairs encountered by the algorithm, which is essential
for the regret analysis. This was ignored by previous works (Cai et al., 2020) and its follow-up works
(Zhong and Zhang, 2024; Ishfaq et al., 2024) that use the same regret decomposition technique.
Furthermore, the multi-agent setting and the communications from synchronization in our algorithms
also significantly increase the challenges in our analysis compared to randomized exploration in the
single-agent setting (Ishfaq et al., 2021, 2024).
Next we present the communication complexity of Algorithm 1 with synchronization condition
(4.3).
Lemma 5.5. The total number of communication rounds between the agents and the server in
Algorithm 1 is bounded by CPX = O(cid:101)((d+K/γ)MH).
Remark 5.6. We provide a refined analysis in Appendix A to get this improved result based on that
of Dubey and Pentland (2021), which studied the same communication procedure as ours. When we
choose γ = O(K/dM), the communication complexity reduces to O(cid:101)(dHM2). Note that Min et al.
(2023) studied the asynchronous setting where only one agent is active in each episode, giving out
√
the regret O(cid:101)(cid:0) d3/2H2 K(cid:1) with the communication complexity O(cid:101)(dHM2). It is interesting to see
that our algorithm, though in the synchronous setting, has the same communication complexity as
the asynchronous variant. This implies that the asynchronous algorithm can only circumvent current
communication by delaying it to the future but does not decrease the communication complexity.
In fact, the synchronous setting can learn the policy better in our work, which is indicated by
comparison of the average regret (the cumulative regret divided by the total number of samples
used by the algorithm) in Table 1. By achieving a matched communication complexity, we find that
synchronous and asynchronous settings have their own advantages and cannot replace each other.
This phenomenon can help us better understand the properties of these two communication schemes.
5.2 Misspecified Setting
In this part, we extend our theoretical analysis to the misspecified setting. In this setting, the
transition functions P and the reward functions r are heterogeneous across different MDPs,
m,h m,h
13which is slightly more complicated than the homogeneous setting. Moreover, instead of assuming
the transition and reward are linear, we only require each individual MDP is a ζ-approximate linear
MDP (Jin et al., 2020) where both the transition and reward are approximately linear up to an
controlled error ζ.
Definition 5.7 (Misspecified Parallel MDPs). For any 0 < ζ ≤ 1, and for any agent m ∈ M,
the corresponding MDP(S,A,H,P ,r ) is a ζ-approximate linear MDP with a feature map ϕ :
m m
S ×A → Rd, for any h ∈ [H], there exist d unknown (signed) measures µ = (cid:0) µ(1) ,...,µ(d)(cid:1) over S
h h h
and an unknown vector θ ∈ Rd such that for any (s,a) ∈ S ×A, we have
h
(cid:13) (cid:13)P m,h(· | s,a)−(cid:10) ϕ(s,a),µ h(·)(cid:11)(cid:13) (cid:13)
TV
≤ ζ,
(cid:12) (cid:12)
(cid:12)r m,h(s,a)−⟨ϕ(s,a),θ h⟩(cid:12) ≤ ζ,
where∥·∥ isthetotalvariationnorm,fortwodistributionsP andP ,wedefineitas: ∥P −P ∥ =
TV 1 2 1 2 TV
1 (cid:80) |P (x)−P (x)|. Withoutlossofgenerality,weassumethat∥ϕ(s,a)∥ ≤ 1forall(s,a) ∈ S×A,
2 x∈Ω 1 2 √
and max(cid:8) ∥µ (S)∥,∥θ ∥(cid:9) ≤ d for all h ∈ [H] and m ∈ M.
h h
Remark 5.8. Note that our misspecified setting defined in Definition 5.7 is a generalized notion of
misspecification in Jin et al. (2020). Moreover, our misspecified setting is also more general and cover
the small heterogeneous setting mentioned in Dubey and Pentland (2021). The triangle inequality
can easily be used to derive small heterogeneous setting from our misspecified setting, but not vice
versa.
Next we state our regret bound for CoopTS-PHE in the misspecified setting.
Theorem 5.9 (Misspecified Regret Bound for CoopTS-PHE). In CoopTS-PHE (Algorithm 1+Al-
gorithm 2), under Definition 5.7 and determinant synchronization condition (4.3), with the same
initialization with Theorem 5.1, we obtain the following cumulative regret
Regret(K) =
O(cid:101)(cid:16) d3 2H2√ M(cid:0)(cid:112)
dMγ
+√ K(cid:1) +dH2M√ K(cid:0)(cid:112)
dMγ
+√ K(cid:1) ζ(cid:17)
,
with probability at least 1−δ.
√ √
R √emark5.10. Whenwechooseζ = O(cid:0)(cid:112) d/MK(cid:1),thecumulativeregretbecomesO(cid:101)(cid:0) d3 2H2 M(cid:0) dMγ
+ K(cid:1)(cid:1). This matches the result of Theorem 5.1 in the linear MDP setting.
Similarly, we can have the following result for CoopTS-LMC.
Theorem 5.11 (Misspecified Regret Bound for CoopTS-LMC). In CoopTS-LMC (Algorithm 1+Al-
gorithm 3), under Definition 5.7 and determinant synchronization condition (4.3), with the same
√ √
initialization with Theorem 5.3 except that 1/(cid:112) β
m,k
= O(cid:101)(cid:0) H d + H MKdζ(cid:1), we obtain the
following cumulative regret
Regret(K) =
O(cid:101)(cid:16) d3 2H2√ M(cid:0)(cid:112)
dMγ
+√ K(cid:1) +d3 2H2M√ K(cid:0)(cid:112)
dMγ
+√ K(cid:1) ζ(cid:17)
,
with probability at least 1−δ.
√ √
R √emark5.12. Whenwechooseζ = O(cid:0)(cid:112) 1/MK(cid:1),thecumulativeregretbecomesO(cid:101)(cid:0) d23 H2 M(cid:0) dMγ
+ K(cid:1)(cid:1). This matches the result of Theorem 5.3 in the linear MDP setting. By comparing Theo-
√
rems 5.9 and 5.11, we find the result of CoopTS-LMC has an extra d factor worse than that of
√
CoopTS-PHE,causingthechosenζ inCoopTS-PHEhasanextra dorderoverthatinCoopTS-LMC.
This indicates that CoopTS-PHE has better performance tolerance for the misspecified setting.
146 Experiments
In this section, we present an empirical evaluation of our proposed randomized exploration strategies
(i.e., CoopTS-PHE and CoopTS-LMC) with deep Q-networks (DQNs) (Mnih et al., 2015) as
the core algorithm on varying tasks under multi-agent settings compared with several baselines:
vanilla DQN, Double DQN (Hasselt et al., 2016), Bootstrapped DQN (Osband et al., 2016a), and
Noisy-Net (Fortunato et al., 2018)).
Given that all experiments are conducted under multi-agent settings unless explicitly specified
as a single-agent or centralized scenario, we denote CoopTS-PHE as "PHE" and CoopTS-LMC
as "LMC" in both experimental contexts and figures. We run all our experiments on Nvidia RTX
A5000 with 24GB RAM.
10 10
8 8
6 6
4 PHE 4 PHE
LMC LMC
DQN DQN
2 Bootstrapped DQN 2 Bootstrapped DQN
NoisyNet DQN NoisyNet DQN
DDQN DDQN
0 0
0 1000 2000 3000 4000 5000 6000 0 2000 4000 6000 8000
Episode Episode
(a) m=2 (b) m=3
Figure 1: Comparison among different exploration strategies in N-chain with N = 25. All results
are averaged over 10 runs.
6.1 N-chain
The N-chain (Osband et al., 2016a) comprises a sequence of N states denoted as s , ∀1 ≤ l ≤ N.
l
Assuming the existence of m agents, all initiating their trajectories from s , this study explores
2
the dynamics of their movement within the chain. At each time step, agents face the decision to
move either left or right. Notably, each agent incurs a nominal reward of r = 0.001 upon reaching
state s , while a more substantial reward of r = 1 is obtained upon reaching the terminal state s .
1 N
The illustration of N-chain environment is shown in Appendix I.1. With a horizon length of N +9,
the optimal return is 10. We consider N = 25 with the communication among agents in Figure 1
following the synchronization approach in Algorithm 1. Note that the total training episodes in the
x-axis are shared among m agents. In Figure 1(a), we show that PHE and Bootstrapped DQN result
in better performance while LMC can also eventually converge to a similar reward.
Upon increasing the number of agents to m = 3, we show in Figure 1(b) that our randomized
exploration methods outperform all other baselines. Notably, the fluctuation in PHE is observed
to be less pronounced against LMC. This observation lends support to our theoretical framework
regarding performance tolerance in the misspecified setting, as detailed in Section 5.2. The complete
results for N-chain and ablation studies can be found in Appendix I.1.
15
draweR draweR900 900
800 800
700 700
600 600
500 500
400 400
PHE PHE
300 LMC 300 LMC
DQN DQN
200 200
Bootstrapped DQN Bootstrapped DQN
NoisyNet DQN NoisyNet DQN
100 100
DDQN DDQN
0 0
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
Episode Episode
(a) Mario (Parallel) (b) Mario (Federated)
10
20
30
40
50
Random PHE LMC DQN BootstrappedNoisyNet DDQN
Algorithm
(c) Tampa (hot humid) building energy system
Figure 2: (a)-(b): Comparison among different exploration strategies in Super Mario Bros. (c):
Evaluation performance at Tampa (hot humid) in building energy systems. All results are averaged
over 10 runs.
6.2 Super Mario Bros
Environmental heterogeneity, arising from various sources, is a prevalent challenge in practical
scenarios. InSection5.2, weillustratetheextensionofhomogeneousparallelMDPtothemisspecified
setting. In the Super Mario Bros task (Tsay et al., 2011), we examine a scenario where four agents,
denoted as m = 4, engage in learning within distinct environments. Despite these environments
sharing the same state space S, action space A, and reward function, their characteristics are
different described in Appendix I.2. The primary objective of the Super Mario Bros task is to train
an agent capable of advancing as far-right and rapidly as possible without collisions or falls. Utilizing
preprocessed images as input states, agents aim to select optimal actions from a set of 7 discrete
actions.
Figure 2(a) visually depicts that both randomized exploration strategies outperform other
baselines in cooperative parallel learning. Notably, We observe that the superiority of LMC gets
significant against PHE unlike the results in N-chain in Figure 1. In the case of PHE, Gaussian
noise is introduced to the reward before applying the Bellman update, which can be viewed as a
method empirically approximating the posterior distribution of the Q function using a Gaussian
16
draweR
nruteR
yliaD
draweRdistribution. However, it is crucial to note that in practical scenarios, unlike the N-chain setting,
Gaussian distributions may not always provide an accurate approximation of the true posterior of
the Q function (Ishfaq et al., 2024). Here, transitions are shared among the four agents whenever
the synchronization condition in (4.3) is met. We also conducted extra experiments in this task
extending our proposed method to federated learning shown in Figure 2(b). More details of the
corresponding experiments and discussion can be referred to Appendix I.2.
6.3 Thermal Control of Building Energy Systems
Finally, we assess the efficacy of our randomized exploration strategies through their application
to a practical task within a sustainable energy system, as outlined by Yeh et al. (2023). This
task involves addressing real-world physical constraints and accounting for environmental shifts
over time. BuildingEnv, a scenario in Yeh et al. (2023), is designed to manage the heating supply
in a multi-zone building. The objective is to meet user-defined temperature specifications while
simultaneously minimizing overall electricity consumption. We defer the details of the environment
details to Appendix I.3.
With the availability of different cities in varying weather types, we conduct experiments on
multiple cities in parallel and share their data following Algorithm 1 for each exploration strategy.
During the evaluation, we deploy those trained policies to the environment of each city/weather
respectively. We include all the methods as well as random action in Figure 2(c) for a fair comparison.
Specifically, we sample action randomly from action space for random action. We display the
distribution of the return with probability density in violin plots, indicating that our PHE and
LMC can perform better with a higher mean. Additional results for other cities can be found in
Appendix I.3.
7 Conclusion
Inthiswork, weproposedaunifiedalgorithmframeworkforperformingprovablyefficientrandomized
exploration in parallel MDPs. By combining this unified algorithm framework with two TS-type
randomized exploration strategies, PHE and LMC, we obtained two algorithms for parallel MDPs:
CoopTS-PHE and CoopTS-LMC. These two algorithms are both flexible in design and easy to
implement in practice. Under the setting of linear MDP, we derived the theoretical regret bounds
and communication complexities of CoopTS-PHE and CoopTS-LMC. This is the first result for
randomized exploration in cooperative MARL, and also matches the existing best regret bounds
for randomized exploration in single-agent RL (Ishfaq et al., 2021, 2024). We also extended our
theoretical analysis to the misspecified setting. Our experimental results on diverse RL parallel
environments verified that randomized exploration can achieve improved performance for balancing
exploration and exploitation in both homogeneous and heterogeneous settings. For future research,
it would be interesting to extend our randomized exploration algorithm to a fully decentralized or
advanced federated learning setting. Developing a more communication-efficient algorithm, which
may reduce the large communication cost in the general function class setting, can also be a future
direction.
17References
Abbasi-Yadkori, Y., Pál, D. and Szepesvári, C. (2011). Improved algorithms for linear
stochastic bandits. Advances in neural information processing systems 24. (pp. 2, 68, 69, and 71.)
Abeille, M. and Lazaric, A. (2017). Linear thompson sampling revisited. In Artificial Intelligence
and Statistics. PMLR. (p. 11.)
Abramowitz, M. and Stegun, I. A. (1968). Handbook of mathematical functions with formulas,
graphs, and mathematical tables, vol. 55. US Government printing office. (p. 70.)
Agrawal, P., Chen, J. and Jiang, N. (2021). Improved worst-case regret bounds for randomized
least-squares value iteration. In Proceedings of the AAAI Conference on Artificial Intelligence,
vol. 35. (p. 5.)
Agrawal, S. and Goyal, N. (2013a). Thompson sampling for contextual bandits with linear
payoffs. In International conference on machine learning. PMLR. (p. 11.)
Agrawal, S. and Goyal, N. (2013b). Thompson sampling for contextual bandits with linear
payoffs. In International Conference on Machine Learning. (p. 74.)
Agrawal, S. and Jia, R. (2017). Optimistic posterior sampling for reinforcement learning: worst-
case regret bounds. In Advances in Neural Information Processing Systems, vol. 30. (pp. 2
and 5.)
Bakry, D., Gentil, I., Ledoux, M. et al. (2014). Analysis and geometry of Markov diffusion
operators, vol. 103. Springer. (p. 9.)
Bakshi, N. A., Gupta, T., Ghods, R. and Schneider, J. (2023). Guts: Generalized uncertainty-
aware thompson sampling for multi-agent active search. arXiv preprint arXiv:2304.02075 . (p.
4.)
Bazzan, A. L. (2009). Opportunities for multiagent systems and multiagent reinforcement learning
in traffic control. Autonomous Agents and Multi-Agent Systems 18 342–375. (p. 1.)
Bernstein, D. S., Givan, R., Immerman, N. and Zilberstein, S. (2002). The complexity
of decentralized control of markov decision processes. Mathematics of Operations Research 27
819–840. (p. 4.)
Boutilier, C. (1996). Planning, learning and coordination in multiagent decision processes. In
Theoretical Aspects of Rationality and Knowledge. (p. 4.)
Cai, Q., Yang, Z., Jin, C. and Wang, Z. (2020). Provably efficient exploration in policy
optimization. In International Conference on Machine Learning. PMLR. (pp. 13, 27, and 28.)
Chalkiadakis, G. and Boutilier, C. (2003). Coordination in multiagent reinforcement learning:
A bayesian approach. In Proceedings of the second international joint conference on Autonomous
agents and multiagent systems. (p. 2.)
Chan, J., Pacchiano, A., Tripuraneni, N., Song, Y. S., Bartlett, P. and Jordan, M. I.
(2021). Parallelizing contextual bandits. arXiv preprint arXiv:2105.10590 . (p. 12.)
18Chapelle, O. and Li, L. (2011). An empirical evaluation of thompson sampling. Advances in
neural information processing systems 24. (pp. 8 and 11.)
Chu, W., Li, L., Reyzin, L. and Schapire, R. (2011). Contextual bandits with linear payoff
functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and
Statistics. JMLR Workshop and Conference Proceedings. (p. 2.)
Dalalyan, A. S. (2017). Theoretical guarantees for approximate sampling from smooth and
log-concave densities. Journal of the Royal Statistical Society Series B: Statistical Methodology 79
651–676. (p. 9.)
Dann, C., Mohri, M., Zhang, T. and Zimmert, J. (2021). A provably efficient model-free
posterior sampling method for episodic reinforcement learning. Advances in neural information
processing systems 34. (p. 5.)
Ding, G., Koh, J. J., Merckaert, K., Vanderborght, B., Nicotra, M. M., Heckman,
C., Roncone, A. and Chen, L. (2020). Distributed reinforcement learning for cooperative
multi-robot object manipulation. arXiv preprint arXiv:2003.09540 . (p. 1.)
Dubey, A. and Pentland, A. (2021). Provably efficient cooperative multi-agent reinforcement
learning with function approximation. arXiv preprint arXiv:2103.04972 . (pp. 2, 3, 4, 8, 12, 13,
14, 25, and 27.)
Fei, Y. and Xu, R. (2022). Cascaded gaps: Towards logarithmic regret for risk-sensitive reinforce-
ment learning. In International Conference on Machine Learning. PMLR. (p. 1.)
Fortunato, M., Azar, M. G., Piot, B. et al. (2018). Noisy networks for exploration. arXiv
preprint arXiv:1706.10295 . (pp. 5, 15, and 71.)
Hao, J., Yang, T., Tang, H., Bai, C., Liu, J., Meng, Z., Liu, P. and Wang, Z. (2023).
Exploration in deep reinforcement learning: From single-agent to multiagent domain. IEEE
Transactions on Neural Networks and Learning Systems . (p. 2.)
Hasselt, H. V., Guez, A. and Silver, D. (2016). Deep reinforcement learning with double
qlearning. In Annual AAAI Conference on Artificial Intelligence (AAAI). (pp. 15 and 71.)
Hillel, E., Karnin, Z. S., Koren, T., Lempel, R. and Somekh, O. (2013). Distributed
exploration in multi-armed bandits. Advances in Neural Information Processing Systems 26. (p.
4.)
Horn, R. A. and Johnson, C. R. (2012). Matrix analysis. Cambridge university press. (p. 69.)
Huix, T., Zhang, M. and Durmus, A. (2023). Tight regret and complexity bounds for thompson
sampling via langevin monte carlo. In Proceedings of The 26th International Conference on
Artificial Intelligence and Statistics (F. Ruiz, J. Dy and J.-W. van de Meent, eds.), vol. 206 of
Proceedings of Machine Learning Research. PMLR.
URL https://proceedings.mlr.press/v206/huix23a.html (p. 2.)
19Ishfaq, H., Cui, Q., Nguyen, V., Ayoub, A., Yang, Z., Wang, Z., Precup, D. and Yang, L.
(2021). Randomized exploration in reinforcement learning with general value function approxima-
tion. In International Conference on Machine Learning. PMLR. (pp. 2, 3, 5, 8, 9, 12, 13, 17, 69,
and 71.)
Ishfaq, H., Lan, Q., Xu, P., Mahmood, A. R., Precup, D., Anandkumar, A. and Azizzade-
nesheli, K. (2024). Provable and practical: Efficient exploration in reinforcement learning via
langevin monte carlo. In The Twelfth International Conference on Learning Representations.
URL https://openreview.net/forum?id=nfIAEJFiBZ (pp. 2, 3, 4, 5, 9, 12, 13, 17, 27, and 69.)
Jafarnia-Jahromi, M., Jain, R. and Nayyar, A. (2023). Learning zero-sum stochastic games
with posterior sampling. https://arxiv.org/abs/2109.03396 . (p. 5.)
Jin, C., Yang, Z., Wang, Z. and Jordan, M. I. (2020). Provably efficient reinforcement learning
with linear function approximation. In Conference on Learning Theory. PMLR. (pp. 2, 3, 4, 10,
12, 14, and 69.)
Jin, H., Peng, Y., Yang, W., Wang, S. and Zhang, Z. (2022a). Federated reinforcement
learning with environment heterogeneity. In International Conference on Artificial Intelligence
and Statistics. PMLR. (p. 78.)
Jin, T., Hsu, H.-L., Chang, W. and Xu, P. (2024). Finite-time frequentist regret bounds of
multi-agent thompson sampling on sparse hypergraphs. In Annual AAAI Conference on Artificial
Intelligence (AAAI). (p. 4.)
Jin, T., Xu, P., Shi, J., Xiao, X. and Gu, Q. (2021). Mots: Minimax optimal thompson sampling.
In International Conference on Machine Learning. PMLR. (p. 4.)
Jin, T., Xu, P., Xiao, X. and Anandkumar, A. (2022b). Finite-time regret of thompson sampling
algorithms for exponential family multi-armed bandits. Advances in Neural Information Processing
Systems 35 38475–38487. (p. 4.)
Jin, T., Yang, X., Xiao, X. and Xu, P. (2023). Thompson sampling with less exploration is fast
and optimal. In International Conference on Machine Learning. PMLR. (p. 4.)
Karbasi, A., Kuang, N. L., Ma, Y. and Mitra, S. (2023). Langevin thompson sampling
with logarithmic communication: Bandits and reinforcement learning. In Proceedings of the 40th
International Conference on Machine Learning (A. Krause, E. Brunskill, K. Cho, B. Engelhardt,
S. Sabato and J. Scarlett, eds.), vol. 202 of Proceedings of Machine Learning Research. PMLR.
URL https://proceedings.mlr.press/v202/karbasi23a.html (pp. 2 and 5.)
Kretchmar, R. M. (2002). Parallel reinforcement learning. In The 6th World Conference on
Systemics, Cybernetics, and Informatics. (p. 4.)
Kuang, N., Yin, M. et al. (2023). Posterior sampling with delayed feedback for reinforcement
learning with linear function approximation. Advances in neural information processing systems .
(p. 5.)
Kveton, B., Szepesvari, C., Ghavamzadeh, M. and Boutilier, C. (2019). Perturbed-history
exploration in stochastic multi-armed bandits. (pp. 2 and 8.)
20Kveton, B., Zaheer, M., Szepesvari, C., Li, L., Ghavamzadeh, M. and Boutilier, C.
(2020a). Randomized exploration in generalized linear bandits. In Proceedings of the Twenty Third
International Conference on Artificial Intelligence and Statistics (S. Chiappa and R. Calandra,
eds.), vol. 108 of Proceedings of Machine Learning Research. PMLR.
URL https://proceedings.mlr.press/v108/kveton20a.html (pp. 2 and 8.)
Kveton, B., Zaheer, M., Szepesvari, C., Li, L., Ghavamzadeh, M. and Boutilier, C.
(2020b). Randomized exploration in generalized linear bandits. In International Conference on
Artificial Intelligence and Statistics. PMLR. (pp. 3 and 11.)
Landgren, P., Srivastava, V. and Leonard, N. E. (2016). On distributed cooperative decision-
making in multiarmed bandits. In 2016 European Control Conference (ECC). IEEE. (p. 4.)
Li, L., Chu, W., Langford, J. and Schapire, R. E. (2010). A contextual-bandit approach to
personalized news article recommendation. In Proceedings of the 19th international conference on
World wide web. (p. 74.)
Li, L., Lu, Y. and Zhou, D. (2017). Provably optimal algorithms for generalized linear contextual
bandits. In International Conference on Machine Learning. PMLR. (p. 2.)
Li, T., Sahu, A. K., Zaheer, M., Sanjabi, M., Talwalkar, A.andSmith, V.(2020). Federated
optimization in heterogeneous networks. In Proceedings of Machine Learning and Systems, vol. 2.
(p. 4.)
Li, Z.,Li, Y.,Zhang, Y.,Zhang, T.andLuo, Z.-Q.(2022). Hyperdqn: Arandomizedexploration
method for deep reinforcement learning. In International Conference on Learning Representations.
(p. 5.)
Lidard, J., Madhushani, U. and Leonard, N. E. (2022). Provably efficient multi-agent rein-
forcement learning with fully decentralized communication. In 2022 American Control Conference
(ACC). IEEE. (p. 4.)
Liu, B., Wang, L. and Liu, M. (2019). Lifelong federated reinforcement learning: a learning
architecture for navigation in cloud robotic systems. IEEE Robotics and Automation Letters 4
4555–4562. (p. 1.)
Liu, Z., Zhang, J., Liu, Z., Du, H., Wang, Z., Niyato, D., Guizani, M. and Ai, B. (2023).
Cell-free xl-mimo meets multi-agent reinforcement learning: Architectures, challenges, and future
directions. arXiv preprint arXiv:2307.02827 . (p. 2.)
Min, Y., He, J., Wang, T. and Gu, Q. (2023). Cooperative multi-agent reinforcement learning:
asynchronous communication and linear function approximation. In International Conference on
Machine Learning. PMLR. (pp. 2, 3, 4, 8, and 13.)
Mnih, V., Kavukcuoglu, K., Silver, D. et al. (2015). Human-level control through deep
reinforcement learning. Nature 518 529–533. (pp. 15, 71, and 80.)
Mousavi-Hosseini, A., Farghly, T., He, Y., Balasubramanian, K. and Erdogdu, M. A.
(2023). Towards a complete analysis of langevin monte carlo: Beyond poincaré inequality. (p. 2.)
21Nguyen-Tang, T. and Arora, R. (2023). On sample-efficient offline reinforcement learning: Data
diversity, posterior sampling and beyond. Advances in neural information processing systems . (p.
5.)
Osband, I., Aslanides, J. and Cassirer, A. (2018). Randomized prior functions for deep
reinforcement learning. Advances in neural information processing systems 31. (p. 5.)
Osband, I., Blundell, C., Pritzel, A. and Roy, B. V. (2016a). Deep exploration via boot-
strapped dqn. Advances in neural information processing systems 29. (pp. 5, 15, and 71.)
Osband, I., Russo, D. and Van Roy, B. (2013). (more) efficient reinforcement learning via
posterior sampling. Advances in Neural Information Processing Systems 26. (pp. 2, 4, and 8.)
Osband, I. and Van Roy, B. (2017). Why is posterior sampling better than optimism for
reinforcement learning? In International conference on machine learning. PMLR. (pp. 2, 4, and 8.)
Osband, I., Van Roy, B. and Wen, Z. (2016b). Generalization and exploration via randomized
value functions. In International Conference on Machine Learning. PMLR. (p. 5.)
Qiu, S., Dai, Z., Zhong, H., Wang, Z., Yang, Z. and Zhang, T. (2023). Posterior sampling for
competitive rl: Function approximation and partial observation. Advances in neural information
processing systems . (p. 5.)
Riquelme, C., Tucker, G. and Snoek, J. (2018). Deep bayesian bandits showdown: An empirical
comparison of bayesian deep networks for thompson sampling. arXiv preprint arXiv:1802.09127 .
(pp. 3 and 11.)
Roberts, G. O. and Tweedie, R. L. (1996). Exponential convergence of langevin distributions
and their discrete approximations. Bernoulli 341–363. (p. 9.)
Rojas-Córdova, C., Williamson, A. J., Pertuze, J. A. and Calvo, G. (2023). Why one
strategy does not fit all: a systematic review on exploration–exploitation in different organizational
archetypes. Review of Managerial Science 17 2251–2295. (p. 2.)
Ruan, Y., Yang, J. and Zhou, Y. (2021). Linear bandits with limited adaptivity and learning
distributional optimal design. In Proceedings of the 53rd Annual ACM SIGACT Symposium on
Theory of Computing. (p. 12.)
Russo, D.(2019). Worst-caseregretboundsforexplorationviarandomizedvaluefunctions. Advances
in neural information processing systems 32 14410–14420. (p. 5.)
Strens, M. (2000). A bayesian framework for reinforcement learning. In International Conference
on Machine Learning. PMLR. (p. 5.)
Taylor, M. E. and Stone, P. (2009). Transfer learning for reinforcement learning domains: A
survey. Journal of Machine Learning Research 10 1633–1685. (p. 4.)
Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in
view of the evidence of two samples. Biometrika 25 285–294. (pp. 2 and 4.)
22Tsay, J.-J., Chen, C.-C. and Hsu, J.-J. (2011). Evolving intelligent mario controller by rein-
forcement learning. In International Conference on Technologies and Applications of Artificial
Intelligence. (pp. 1 and 16.)
Vershynin, R. (2018). High-dimensional probability: An introduction with applications in data
science, vol. 47. Cambridge university press. (p. 69.)
Wang, R., Salakhutdinov, R. R. and Yang, L. (2020a). Reinforcement learning with general
valuefunctionapproximation: Provablyefficientapproachviaboundedeluderdimension. Advances
in Neural Information Processing Systems 33 6123–6135. (p. 2.)
Wang, Y., Hu, J., Chen, X. and Wang, L. (2020b). Distributed bandit learning: Near-optimal
regret with efficient communication. In International Conference on Learning Representations. (p.
4.)
Wang, Z. and Zhou, M. (2020). Thompson sampling via local uncertainty. In International
Conference on Machine Learning. PMLR. (p. 4.)
Xie, Q., Chen, Y., Wang, Z. and Yang, Z. (2020). Learning zero-sum simultaneous-move markov
games using function approximation and correlated equilibrium. In Proceedings of Thirty Third
Conference on Learning Theory, vol. 125. PMLR. (p. 4.)
Xiong, W., Zhong, H., Shi, C., Shen, C. and Zhang, T. (2022). A self-play posterior sampling
algorithm for zero-sum markov games. In International Conference on Machine Learning. PMLR.
(p. 5.)
Xu, P., Chen, J., Zou, D. and Gu, Q. (2018). Global convergence of langevin dynamics based
algorithms for nonconvex optimization. Advances in Neural Information Processing Systems 31.
(p. 9.)
Xu, P., Wen, Z., Zhao, H. and Gu, Q. (2021). Neural contextual bandits with deep representation
and shallow exploration. In International Conference on Learning Representations. (p. 2.)
Xu, P., Zheng, H., Mazumdar, E. V., Azizzadenesheli, K. and Anandkumar, A. (2022).
Langevin monte carlo for contextual bandits. In International Conference on Machine Learning.
PMLR. (pp. 2, 5, 9, and 11.)
Ye, D., Chen, G., Zhang, W., Chen, S., Yuan, B., Liu, B., Chen, J., Liu, Z., Qiu, F., Yu,
H. et al. (2020). Towards playing full moba games with deep reinforcement learning. Advances
in Neural Information Processing Systems 33 621–632. (p. 1.)
Yeh, C., Li, V., Datta, R., Arroyo, J., Christianson, N., Zhang, C., Chen, Y., Hosseini,
M.,Golmohammadi, A.,Shi, Y.,Yue, Y.andWierman, A.(2023). Sustaingym: Abenchmark
suite of reinforcement learning for sustainability applications. In Thirty-seventh Conference on
Neural Information Processing Systems Datasets and Benchmarks Track. PMLR. (pp. 1, 17,
and 80.)
Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A. and Wu, Y. (2022). The
surprising effectiveness of PPO in cooperative multi-agent games. In Thirty-sixth Conference on
Neural Information Processing Systems Datasets and Benchmarks Track. (p. 4.)
23Yu, C., Yang, X., Gao, J., Chen, J., Li, Y., Liu, J., Xiang, Y., Huang, R., Yang, H.,
Wu, Y. and Wang, Y. (2023). Asynchronous multi-agent reinforcement learning for efficient
real-time multi-robot cooperative exploration. In Proceedings of the 2023 International Conference
on Autonomous Agents and Multiagent Systems, AAMAS. ACM. (p. 4.)
Zanette, A., Brandfonbrener, D., Brunskill, E., Pirotta, M. and Lazaric, A. (2020).
Frequentist regret bounds forrandomized least-squares value iteration. In International Conference
on Artificial Intelligence and Statistics. PMLR. (pp. 3 and 5.)
Zhang, K., Yang, Z., Liu, H., Zhang, T. and Başar, T. (2018). Fully decentralized multi-agent
reinforcement learning with networked agents. In International Conference on Machine Learning,
vol. 80. PMLR. (p. 4.)
Zhang, W., Zhou, D., Li, L. and Gu, Q. (2021). Neural thompson sampling. In International
Conference on Learning Representations. (p. 74.)
Zhang, Y., Qu, G., Xu, P., Lin, Y., Chen, Z. and Wierman, A. (2023). Global convergence
of localized policy iteration in networked multi-agent reinforcement learning. Proceedings of the
ACM on Measurement and Analysis of Computing Systems 7 1–51. (p. 4.)
Zhao, Y., Borovikov, I., Rupert, J., Somers, C. and Beirami, A. (2019). On multi-agent
learning in team sports games. arXiv preprint arXiv:1906.10124 . (p. 1.)
Zhong, H. and Zhang, T. (2024). A theoretical analysis of optimistic proximal policy optimization
in linear markov decision processes. Advances in Neural Information Processing Systems 36. (p.
13.)
Zhou, D., Li, L. and Gu, Q. (2020). Neural contextual bandits with ucb-based exploration. In
International Conference on Machine Learning. (pp. 2 and 74.)
Zhou, Y., Li, J. and Zhu, J. (2019). Posterior sampling for multi-agent reinforcement learning:
solving extensive games with imperfect information. In International Conference on Learning
Representations. (pp. 2 and 5.)
Zou, D., Xu, P. and Gu, Q. (2021). Faster convergence of stochastic gradient langevin dynamics
for non-log-concave sampling. In Uncertainty in Artificial Intelligence. PMLR. (p. 9.)
24A Analysis of the Communication Complexity of Algorithm 1
The proof of this lemma is largely inspired by that in Dubey and Pentland (2021). However, we
provide a refined analysis here, and thus obtain an improved communication complexity O(dHM2),
in contrast with the O(dHM3) complexity in their paper. We also discussed this in Remark 5.6 and
showed that our result matches that of a recently proposed asynchronous algorithm.
Proof of Lemma 5.5. We assume σ = {σ ,...,σ } as the synchronization episodes, where σ ∈ [K],
1 n i
we also denote σ = 0. To bound the number of synchronization n, we separate σ into two parts
0
with an undetermined term α
I = {i ∈ [n]|σ −σ ≤ α},
1 i i−1
I = {i ∈ [n]|σ −σ > α}.
2 i i−1
Then we have n = |I |+|I |. Note that
1 2
n
(cid:88) (cid:88)
K ≥ σ = (σ −σ ) ≥ (σ −σ ) > |I |α.
n i i−1 i i−1 2
i=1 i∈I2
Then we have |I | < K/α. Then note that
2
n (cid:18)det(Λσi )(cid:19) (cid:18)det(Λσi )(cid:19)
(cid:88) m,h (cid:88) m,h
log ≥ log
det(Λσi−1) det(Λσi−1)
i=1 m,h i∈I1 m,h
(cid:88) γ
≥
σ −σ
i i−1
i∈I1
γ
≥ |I | . (A.1)
1
α
Define ΛK = (cid:80) (cid:80)K ϕ(cid:0) zk (cid:1) ϕ(cid:0) zk (cid:1)⊤ +λI where zk = (cid:0) sk ,ak (cid:1). On the other hand,
h m∈M k=1 m,h m,h m,h m,h m,h
we have
n (cid:18)det(Λσi )(cid:19) (cid:18)det(Λσn )(cid:19)
(cid:88) m,h m,h
log = log
det(Λσi−1) det(Λσ0 )
i=1 m,h m,h
(cid:18) det(ΛK)(cid:19)
≤ log h
det(λI)
≤ dlog(1+MK/d), (A.2)
where the first inequality holds due to the trivial fact that A ≼ B ⇒ det(A) ≤ det(B), the second
inequality follow from Lemma H.2 and the fact that ∥ϕ(·)∥ ≤ 1. Combine (A.1) and (A.2), then we
2
have |I | ≤ dα/γlog(1+MK/d). Finally, we choose α = K/d, then we have
1
K dα (cid:16) MK(cid:17) (cid:16) K(cid:17) (cid:16) MK(cid:17)
n ≤ + log 1+ = d+ log 1+ .
α γ d γ d
When one synchronization occurs, communications between agents and the server will occur M
times because we have M agents in total. Recall from Lines 16-24 in Algorithm 1, also note that in
one synchronization episode, communications will happen H times between every agent and the
server. Finally, the upper bound of communication complexity is
(cid:0) (cid:1)
CPX = O(cid:101) (d+K/γ)MH .
This completes the proof.
25B Proof of the Regret Bound for CoopTS-LMC
The general framework for CoopTS-LMC and CoopTS-PHE is closely similar. To make the article
more concise, we first prove CoopTS-LMC completely, which is a bit more complicated. Then we
can simplify the following similar proof for CoopTS-PHE in Appendix E.
B.1 Supporting Lemmas
Before deriving the regret bound for CoopTS-LMC, we first provide the necessary technical lemmas
for our regret analysis. Note that the loop (Line 3-9) in Algorithm 3 is to do multi-sampling for N
times. To simplify the notations, we eliminate the index n before Lemma B.7 because the previous
lemmas have nothing to do with multi-sampling.
Definition B.1 (Model prediction error). For any (m,k,h) ∈ M×[K]×[H], we define the model
error associated with the reward r ,
h
lk (s,a) = r (s,a)+P Vk (s,a)−Qk (s,a).
m,h h h m,h+1 m,h
Definition B.2 (Filtration). For any (m,k,h) ∈ M×[K]×[H], we define the filtration F as
m,k,h
F = σ(cid:16) (cid:8)(cid:0) sτ ,aτ (cid:1)(cid:9) (cid:91)(cid:8)(cid:0) sk ,ak (cid:1)(cid:9) (cid:91)(cid:8)(cid:0) sk ,ak (cid:1)(cid:9) (cid:17) .
m,k,h n,i n,i (n,τ,i)∈M×[k−1]×[H] n,i n,i (n,i)∈[m−1]×[H] m,i m,i i∈[h]
PropositionB.3. InAlgorithm3,theparameterwk,J k satisfiestheGaussiandistributionN(cid:0) µk,J k,Σk,J k(cid:1),
m,h m,h m,h
where mean vector and the covariance matrix are defined as
k
µk,J k = AJ k...AJ1w1,0 +(cid:88) AJ k...AJi+1(cid:0) I−AJi(cid:1) wi ,
m,h k 1 m,h k i+1 i (cid:98)m,h
i=1
k
Σk,J k = (cid:88) 1 AJ k...AJi+1(cid:0) I−A2Ji(cid:1) (Λi )−1(I+A )−1AJi+1...AJ k,
m,h β k i+1 i m,h i i+1 k
m,i
i=1
where A = I−2η Λi for i ∈ [k].
i m,i m,h
Lemma B.4. For any (m,k,h) ∈ M×[K]×[H], the unperturbed estimated parameter wk
(cid:98)m,h
satisfies
(cid:13) (cid:13)w (cid:98)mk ,h(cid:13) (cid:13) ≤ 2H(cid:112) Mkd/λ.
Lemma B.5. Let λ = 1 in Algorithm 3. For any fixed 0 < δ < 1, with probability at least 1−δ2,
for any (m,k,h) ∈ M×[K]×[H] and for any (s,a) ∈ S ×A, we have
(cid:32) (cid:115) (cid:33)
(cid:12) (cid:12) 2dlog(1/δ) 4
(cid:12)ϕ(s,a)⊤wk,J k −ϕ(s,a)⊤wk (cid:12) ≤ 5 + ∥ϕ(s,a)∥ .
(cid:12) m,h (cid:98)m,h(cid:12) 3β
K
3 (Λk m,h)−1
Lemma B.6. Let λ = 1 in Algorithm 3. For any fixed 0 < δ < 1, with probability at least 1−δ,
for any (m,k,h) ∈ M×[K]×[H], we have
(cid:115)
(cid:13) (cid:13)w mk,J ,hk(cid:13) (cid:13) ≤ 1 36 Hd√ MK + 32 βK δd3/2 d =ef B δ,
K
26Lemma B.7. Let λ = 1 in Algorithm 3. For any fixed 0 < δ < 1, with probability at least 1−δ,
for all (m,k,h) ∈ M×[K]×[H], we have
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:2)(cid:0) Vk −P Vk (cid:1)(cid:0) sl,al(cid:1)(cid:3)(cid:13) (cid:13) ≤ 3H√ dC ,
(cid:13) m,h+1 h m,h+1 (cid:13) δ
(cid:13) (cid:13)
(sl,al,s′l)∈U (k) (Λk )−1
m,h m,h
√
where C =
(cid:104)
1 log(K
+1)+log(cid:16)
2 2KB
δ/2NMHK(cid:17)
+log
3(cid:105)1/2
and B is defined in Lemma B.6.
δ 2 H δ δ
Lemma B.8. Let λ = 1 in Algorithm 3. Under Definition 4.1, for any fixed 0 < δ < 1, with
probability at least 1−δ, for all (m,k,h) ∈ M×[K]×[H] and for any (s,a) ∈ S ×A, we have
(cid:12) (cid:12) √
(cid:12)ϕ(s,a)⊤wk −r (s,a)−P Vk (s,a)(cid:12) ≤ 5H dC ∥ϕ(s,a)∥ .
(cid:12) (cid:98)m,h h h m,h+1 (cid:12) δ (Λk )−1
m,h
Lemma B.9 (Errorbound). Letλ = 1inAlgorithm3. UnderDefinition4.1, foranyfixed0 < δ < 1,
with probability at least 1−δ−δ2, for any (m,k,h) ∈ M×[K]×[H] and for any (s,a) ∈ S ×A,
we have
(cid:32) √ (cid:115) 2dlog(cid:0)√ N/δ(cid:1) 4(cid:33)
−lk (s,a) ≤ 5H dC +5 + ∥ϕ(s,a)∥(cid:0) (cid:1) ,
m,h δ 3β 3 Λk −1
K m,h
where C is defined in Lemma B.7.
δ
Lemma B.10 (Optimism). Let λ = 1 in Algorithm 3 and c′ = 1− √1 . Under Definition 4.1,
0 2 2eπ
for any fixed 0 < δ < 1, with probability at least 1−|C(ε)|c′N −2δ where |C(ε)| ≤ (3/ε)d, for all
0
(m,h,k) ∈ M×[H]×[K] and for all (s,a) ∈ S ×A, we have
lk (s,a) ≤ α ε,
m,h δ
√ √
where α = MK(cid:0) 2H d+B (cid:1).
δ δ/NMHK
Remark B.11. Here we point out that in our proofs for both CoopTS-LMC and CoopTS-PHE, we
use a new ε-covering technique to prove that the optimism lemma holds for all (s,a) ∈ S×A instead
of just the state-action pairs encountered by the algorithm, which is essential in applying this lemma
to bound the term E [lk (s ,a )|s = sk ] in (B.2) in the regret analysis. This was ignored
π∗ m,h m,h m,h m,1 m,1
by previous works (Cai et al., 2020; Ishfaq et al., 2024) that use the same regret decomposition
technique in the single-agent setting.
The following lemma gives the upper bound of self-normalized term summation in the multi-agent
setting, which is first introduced by Lemma 9 in Dubey and Pentland (2021). To make our analysis
complete, we give out the proof in the Appendix C.9 where we make some necessary modifications
compared with Lemma 9 in Dubey and Pentland (2021).
Lemma B.12. Let Algorithm 2 run for any K > 0, M ≥ 1, and γ as the communication control
factor. Define ΛK = (cid:80) (cid:80)K ϕ(cid:0) sk ,ak (cid:1) ϕ(cid:0) sk ,ak (cid:1)⊤ +λI, then we have
h m∈M k=1 m,h m,h m,h m,h
(cid:115)
(cid:88) (cid:88)K (cid:13) (cid:13)ϕ(sk ,ak )(cid:13) (cid:13) ≤ (cid:18) log(cid:18) det(ΛK h )(cid:19) +1(cid:19) M√ γ +2 MKlog(cid:18) det(ΛK h )(cid:19) .
m,h m,h (Λk )−1 det(λI) det(λI)
m,h
m∈Mk=1
27The following lemma shows that we can decompose the regret of Algorithm 2 into three different
components. The proof of this lemma closely resembles Lemma 4.2 in Cai et al. (2020) for the
single-agent setting. When we fix the agent m ∈ M, it is totally same as Lemma 4.2 in Cai et al.
(2020).
Lemma B.13 (Lemma 4.2 in Cai et al. (2020)). Define the operators and the following terms:
(J f)(s) = (cid:10) f(s,·),π∗ (·|s)(cid:11) , (J f)(s) = (cid:10) f(s,·),πk (·|s)(cid:11) ,
m,h m,h m,k,h m,h
D = (cid:0)J (cid:0) Qk −Qπ m,k(cid:1)(cid:1)(cid:0) sk (cid:1) −(cid:0) Qk −Qπ m,k(cid:1)(cid:0) sk ,ak (cid:1) , (B.1)
m,k,h,1 m,k,h m,h m,h m,h m,h m,h m,h m,h
D = (cid:0)P (cid:0) Vk −Vπ m,k (cid:1)(cid:1)(cid:0) sk ,ak (cid:1) −(cid:0) Vk −Vπ m,k (cid:1)(cid:0) sk (cid:1) .
m,k,h,2 m,h m,h+1 m,h+1 m,h m,h m,h+1 m,h+1 m,h+1
Then we can decompose the regret into the following form:
K
Regret(K) = (cid:88) (cid:88) V∗ (cid:0) sk (cid:1) −Vπ m,k(cid:0) sk (cid:1)
m,1 m,1 m,1 m,1
m∈Mk=1
K H
= (cid:88) (cid:88)(cid:88) E (cid:2)(cid:10) Qk (s ,·),π∗ (·,|s )−πk (·|s )(cid:11) |s = sk (cid:3)
π∗ m,h m,h m,h m,h m,h m,h m,1 m,1
m∈Mk=1h=1
(cid:124) (cid:123)(cid:122) (cid:125)
(i)
K H
(cid:88) (cid:88)(cid:88)
+ (D +D )
m,k,h,1 m,k,h,2
m∈Mk=1h=1
(cid:124) (cid:123)(cid:122) (cid:125)
(ii)
K H
+ (cid:88) (cid:88)(cid:88)(cid:0)E (cid:2) lk (s ,a )|s = sk (cid:3) −lk (cid:0) sk ,ak (cid:1)(cid:1) .
π∗ m,h m,h m,h m,1 m,1 m,h m,h m,h
m∈Mk=1h=1
(cid:124) (cid:123)(cid:122) (cid:125)
(iii)
B.2 Regret Analysis
In this part, we give out the proof of Theorem 5.3, the regret bound for CoopTS-LMC.
Proof of Theorem 5.3. Based on the result from Lemma B.13, we do the regret decomposition first
K
Regret(K) = (cid:88) (cid:88) V∗ (cid:0) sk (cid:1) −Vπ m,k(cid:0) sk (cid:1)
m,1 m,1 m,1 m,1
m∈Mk=1
K H
= (cid:88) (cid:88)(cid:88) E (cid:2)(cid:10) Qk (s ,·),π∗ (·,|s )−πk (·|s )(cid:11) |s = sk (cid:3)
π∗ m,h m,h m,h m,h m,h m,h m,1 m,1
m∈Mk=1h=1
(cid:124) (cid:123)(cid:122) (cid:125)
(i)
K H
(cid:88) (cid:88)(cid:88)
+ (D +D )
m,k,h,1 m,k,h,2
m∈Mk=1h=1
(cid:124) (cid:123)(cid:122) (cid:125)
(ii)
28K H
+ (cid:88) (cid:88)(cid:88)(cid:0)E (cid:2) lk (s ,a )|s = sk (cid:3) −lk (cid:0) sk ,ak (cid:1)(cid:1) . (B.2)
π∗ m,h m,h m,h m,1 m,1 m,h m,h m,h
m∈Mk=1h=1
(cid:124) (cid:123)(cid:122) (cid:125)
(iii)
Next, we will bound the above three terms respectively.
Bounding Term (i) in (B.2): for the policy πk , we have
m,h
K H
(cid:88) (cid:88)(cid:88) E (cid:2)(cid:10) Qk (s ,·),π∗ (·,|s )−πk (·|s )(cid:11) |s = sk (cid:3) ≤ 0. (B.3)
π∗ m,h m,h m,h m,h m,h m,h m,1 m,1
m∈Mk=1h=1
This is because by definition πk is the greedy policy for Qk .
m,h m,h
Bounding Term (ii) in (B.2): note that 0 ≤ Qk ≤ H −h+1 ≤ H, based on (B.1), for any
m,h
(m,k,h) ∈ M×[K]×[H], we have |D | ≤ 2H and |D | ≤ 2H. Note that D is a
m,k,h,1 m,k,h,2 m,k,h,1
martingale difference sequence E[D |F ] = 0. By applying Azuma-Hoeffding inequality,
m,k,h,1 m,k,h
with probability at least 1−δ/3, we have
K H
(cid:88) (cid:88)(cid:88) (cid:112)
D ≤ 2 2MH3Klog(6/δ).
m,k,h,1
m∈Mk=1h=1
Note that D is also a martingale difference sequence. By applying Azuma-Hoeffding inequality,
m,k,h,2
with probability at least 1−δ/3, we have
K H
(cid:88) (cid:88)(cid:88) (cid:112)
D ≤ 2 2MH3Klog(6/δ).
m,k,h,2
m∈Mk=1h=1
By taking union bound, with probability at least 1−2δ/3, we have
K H K H
(cid:88) (cid:88)(cid:88) (cid:88) (cid:88)(cid:88) (cid:112)
D + D ≤ 4 2MH3Klog(6/δ). (B.4)
m,k,h,1 m,k,h,2
m∈Mk=1h=1 m∈Mk=1h=1
Bounding Term (iii) in (B.2): based on Lemma B.9 and Lemma B.10, by taking union bound,
with probability at least 1−|C(ε)|c′N −2δ′−MHK(δ′+δ′2), we have
0
K H
(cid:88) (cid:88)(cid:88)(cid:0)E (cid:2) lk (s ,a )|s = sk (cid:3) −lk (cid:0) sk ,ak (cid:1)(cid:1)
π∗ m,h m,h m,h m,1 m,1 m,h m,h m,h
m∈Mk=1h=1
K H
≤ (cid:88) (cid:88)(cid:88)(cid:0) α ε−lk (cid:0) sk ,ak (cid:1)(cid:1)
δ′ m,h m,h m,h
m∈Mk=1h=1
≤ HMKα δ′ε+ (cid:88) (cid:88)K (cid:88)H (cid:32) 5H√ dC δ′ +5(cid:115) 2dlog 3( β√ N/δ′) + 4 3(cid:33) (cid:13) (cid:13)ϕ(sk m,h,ak m,h)(cid:13) (cid:13) (Λk )−1
K m,h
m∈Mk=1h=1
= HMKα δ′ε+(cid:32) 5H√ dC δ′ +5(cid:115) 2dlog 3( β√ N/δ′) + 4 3(cid:33) (cid:88)H (cid:88) (cid:88)K (cid:13) (cid:13)ϕ(sk m,h,ak m,h)(cid:13) (cid:13) (Λk )−1
K m,h
h=1m∈Mk=1
29(cid:32) (cid:115) √ (cid:33)
√ 2dlog( N/δ′) 4
≤ HMKα ε+ 5H dC +5 +
δ′ δ′
3β 3
K
(cid:115)
(cid:88)H (cid:18) (cid:18) det(ΛK)(cid:19) (cid:19)
√
(cid:18) det(ΛK)(cid:19)
× log h +1 M γ +2 MKlog h
det(λI) det(λI)
h=1
(cid:32) (cid:115) √ (cid:33)
√ 2dlog( N/δ′) 4
≤ HMKα ε+ 5H dC +5 +
δ′ δ′
3β 3
K
(cid:16) √ (cid:112) (cid:17)
×H d(log(1+MK/d)+1)M γ +2 MKdlog(1+MK/d) .
The first inequality follows from Lemma B.10, the second inequality follows from Lemma B.9, the
third inequality follows from Lemma B.12, the last inequality holds due to Lemma H.2 and the fact
that ∥ϕ(·)∥ ≤ 1.
2 √
we
hH ae vr ee we choose ε = dH(cid:112) d/MK/α δ′ = O(cid:101)((cid:112) 1/dHM3K4N) and choose √ β1
K
= 20H dC δ′ + 1 36,
(cid:88) (cid:88)K (cid:88)H (cid:0)E π∗(cid:2) l mk ,h(s m,h,a m,h)|s
m,1
= sk m,1(cid:3) −l mk ,h(cid:0) sk m,h,ak m,h(cid:1)(cid:1) ≤ O(cid:101)(cid:0) dH2(cid:0) dM√ γ +√ dMK(cid:1)(cid:1) ,
m∈Mk=1h=1
(B.5)
occurs with probability at least 1−|C(ε)|c′N −2δ′−MHK(δ′+δ′2).
0
We set δ′ = δ/12(MHK +1) and choose N = C¯log(δ)/log(c′) where C¯ = O(cid:101)(d), then we have
0
1−|C(ε)|c′N −2δ′−MHK(δ′+δ′2
) ≥ 1−δ/3.
0
Combining Terms (i)(ii)(iii) together: Based on (B.3), (B.4) and (B.5). By taking union bound,
we get that the final regret bound for CoopTS-LMC is O(cid:101)(cid:0) dH2(cid:0) dM√ γ+√ dMK(cid:1)(cid:1) with probability
at least 1−δ.
C Proof of Supporting Lemmas in Appendix B
C.1 Proof of Proposition B.3
Recall from Algorithm 3, the LMC update rule is
(cid:113)
wk,j = wk,j−1−η ∇Lk (cid:0) wk,j−1(cid:1) + 2η β−1 ϵk,j ,
m,h m,h m,k m,h m,h m,k m,k m,h
where we have ∇Lk (cid:0) wk,j−1(cid:1) = 2(cid:0) Λk wk,j−1 −bk (cid:1). Plug in the above formula, then we can
m,h m,h m,h m,h m,h
calculate that
(cid:113)
wk,J k = wk,J k−1−2η (cid:0) Λk wk,J k−1−bk (cid:1) + 2η β−1 ϵk,J k
m,h m,h m,k m,h m,h m,h m,k m,k m,h
(cid:113)
= (cid:0) I−2η Λk (cid:1) wk,J k−1+2η bk + 2η β−1 ϵk,J k
m,k m,h m,h m,k m,h m,k m,k m,h
30J −1
= (cid:0) I−2η Λk (cid:1)J kwk,0 +
(cid:88)k
(I−2η Λk
)l(cid:16)
2η bk
+(cid:113)
2η β−1 ϵk,J
k−l(cid:17)
m,k m,h m,h m,k m,h m,k m,h m,k m,k m,h
l=0
J −1
= (cid:0) I−2η Λk (cid:1)J kwk,0 +2η
(cid:88)k
(cid:0) I−2η Λk (cid:1)l bk
m,k m,h m,h m,k m,k m,h m,h
l=0
J −1
+(cid:113)
2η β−1
(cid:88)k
(cid:0) I−2η Λk (cid:1)l ϵk,J k−l,
m,k m,k m,k m,h m,h
l=0
where the third equality follows from iteration. Denote that A = I−2η Λi . Moreover, we
i m,i m,h
choose the step size such that 0 < η < 1/(cid:0) 2λ (cid:0) Λi (cid:1)(cid:1). Thus we have
m,i max m,h
J −1 J −1
wk,J k = AJ kwk−1,J k−1 +2η
(cid:88)k
AlΛk wk
+(cid:113)
2η β−1
(cid:88)k
Alϵk,J k−l
m,h k m,h m,k k m,h(cid:98)m,h m,k m,k k m,h
l=0 l=0
J −1
= AJ kwk−1,J k−1 +(I−A )(cid:0) I+A +...+AJ k−1(cid:1) wk
+(cid:113)
2η β−1
(cid:88)k
Alϵk,J k−l
k m,h k k k (cid:98)m,h m,k m,k k m,h
l=0
J −1
= AJ kwk−1,J k−1 +(cid:0) I−AJ k(cid:1) wk
+(cid:113)
2η β−1
(cid:88)k
Alϵk,J k−l
k m,h k (cid:98)m,h m,k m,k k m,h
l=0
= AJ k...AJ1w1,0
+(cid:88)k
AJ k...AJi+1(cid:0) I−AJi(cid:1) wi
+(cid:88)k (cid:113)
2η β−1AJ
k...AJi+1J (cid:88)i−1
Alϵi,Ji−l,
k 1 m,h k i+1 i (cid:98)m,h m,i m,i k i+1 i m,h
i=1 i=1 l=0
where the first equality holds because bk = Λk wk and wk−1,J k−1 = wk,0 , the third equality
m,h m,h(cid:98)m,h m,h m,h
follows from the fact that I+A+...+An−1 = (I−An)(I−A)−1, and the fourth equality holds
because of iteration.
Note that ϵi,Ji−l ∼ N(0,I), based on the property of multivariate Gaussian distribution, we have
m,h
wk,J k ∼ N(cid:0) µk,J k,Σk,J k(cid:1). Then we can directly get the mean vector
m,h m,h m,h
k
µk,J k = AJ k...AJ1w1,0 +(cid:88) AJ k...AJi+1(cid:0) I−AJi(cid:1) wi .
m,h k 1 m,h k i+1 i (cid:98)m,h
i=1
(cid:113)
NextwewillcalculatethecovariancematrixΣk,J k. Forsimplicity,wedefineM = 2η β−1AJ k...AJi+1,
m,h i m,i m,i k i+1
thus we have
M
J (cid:88)i−1
Alϵi,Ji−l ∼
N(cid:32) 0,J (cid:88)i−1
M Al(cid:0) M
Al(cid:1)⊤(cid:33)
∼
N(cid:32)
0,M
(cid:32)J (cid:88)i−1 A2l(cid:33) M⊤(cid:33)
.
i i m,h i i i i i i i
l=0 l=0 l=0
Thus we get the covariance matrix Σk,J k,
m,h
Σk,J k
=(cid:88)k
M
(cid:32)J (cid:88)i−1 A2l(cid:33)
M⊤
m,h i i i
i=1 l=0
31=(cid:88)k
2η β−1AJ
k...AJi+1(cid:32)J (cid:88)i−1 A2l(cid:33)
AJi+1...AJ k
m,i m,i k i+1 i i+1 k
i=1 l=0
k
=(cid:88) 2η β−1AJ k...AJi+1(cid:0) I−A2Ji(cid:1) (I−A2)−1AJi+1...AJ k
m,i m,i k i+1 i i i+1 k
i=1
k
=(cid:88) 1 AJ k...AJi+1(cid:0) I−A2Ji(cid:1)(cid:0) Λi (cid:1)−1 (I+A )−1AJi+1...AJ k,
β k i+1 i m,h i i+1 k
m,i
i=1
where the third equality follows from the fact that I+A+...+An−1 = (I−An)(I−A)−1. Here
we complete the proof.
C.2 Proof of Lemma B.4
Proof. Note that wk = (cid:0) Λk (cid:1)−1 bk , we can calculate that
(cid:98)m,h m,k m,h
(cid:13) (cid:13)
(cid:13) (cid:13)w (cid:98)mk ,h(cid:13) (cid:13) = (cid:13) (cid:13)(cid:0) Λk m,h(cid:1)−1 bk m,h(cid:13)
(cid:13)
(cid:13) (cid:13)
= (cid:13) (cid:13)(cid:0) Λk (cid:1)−1 (cid:88) (cid:2) r (cid:0) sl,al(cid:1) +Vk (s′l )(cid:3) ϕ(cid:0) sl,al(cid:1)(cid:13) (cid:13)
(cid:13) m,h h m,h+1 (cid:13)
(cid:13) (cid:13)
(sl,al,s′l)∈U (k)
m,h
(cid:32) (cid:33)1/2
≤ √1 λ(cid:112) K(k) (cid:88) (cid:13) (cid:13)(cid:2) r h(cid:0) sl,al(cid:1) +V mk ,h+1(s′l )(cid:3) ϕ(cid:0) sl,al(cid:1)(cid:13) (cid:13)2
(Λk m,h)−1
(sl,al,s′l)∈U (k)
m,h
(cid:32) (cid:33)1/2
≤
√2H(cid:112)
K(k)
(cid:88) (cid:13) (cid:13)ϕ(sl,al)(cid:13) (cid:13)2
λ (Λk m,h)−1
(sl,al,s′l)∈U (k)
m,h
(cid:112)
≤ 2H K(k)d/λ
(cid:112)
≤ 2H Mkd/λ,
wherethefirstinequalityfollowsfromLemmaH.3,thesecondinequalityisdueto0 ≤ Vk ≤ H−h+1,
m,h
0 ≤ r ≤ 1 and ∥ϕ(s,a)∥ ≤ 1, the third inequality follows from Lemma H.4, and the last inequality
h
holds because K(k) = (M −1)k +k−1 ≤ Mk.
s
C.3 Proof of Lemma B.5
Proof. We separate the error into two terms and bound them respectively,
(cid:12) (cid:12) (cid:12) (cid:16) (cid:17)(cid:12) (cid:12) (cid:16) (cid:17)(cid:12)
(cid:12)ϕ(s,a)⊤wk,J k −ϕ(s,a)⊤wk (cid:12) ≤ (cid:12)ϕ(s,a)⊤ wk,J k −µk,J k (cid:12)+(cid:12)ϕ(s,a)⊤ µk,J k −wk (cid:12). (C.1)
(cid:12) m,h (cid:98)m,h(cid:12) (cid:12) m,h m,h (cid:12) (cid:12) m,h (cid:98)m,h (cid:12)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
I1 I2
Bounding Term I in (C.1): by Cauchy-Schwarz inequality, we have
1
(cid:12) (cid:16) (cid:17)(cid:12) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:12)ϕ(s,a)⊤ wk,J k −µk,J k (cid:12) ≤ (cid:13)ϕ(s,a)(cid:13) ·(cid:13)wk,J k −µk,J k(cid:13) .
(cid:12) m,h m,h (cid:12) (cid:13) (cid:13) Σk,Jk (cid:13) m,h m,h(cid:13) (Σk,Jk)−1
m,h m,h
32By choosing η ≤ 1/(4λ (Λk )) for all k and m, then we have
m,k max m,h
1
I ≼ A = I−2η Λk ≼ (1−2η λ (Λk ))I, (C.2)
2 k m,k m,h m,k min m,h
3
I ≼ I+A = 2I−2η Λk ≼ 2I.
2 k m,k m,h
Recall the definition of Σk,J k in Proposition B.3. By choosing β = β for all i ∈ [k] and m ∈ M,
m,h m,i K
then we have
k
ϕ(s,a)⊤Σk,Jkϕ(s,a)=(cid:88) 1 ϕ(s,a)⊤AJk...AJi+1(cid:0) I−A2Ji(cid:1)(cid:0) Λi (cid:1)−1 (I+A )−1AJi+1...AJkϕ(s,a)
m,h β k i+1 i m,h i i+1 k
m,i
i=1
k
≤ 2 (cid:88) ϕ(s,a)⊤AJk...AJi+1(cid:16)(cid:0) Λi (cid:1)−1 −AJi(cid:0) Λi (cid:1)−1 AJi(cid:17) AJi+1...AJkϕ(s,a)
3β k i+1 m,h i m,h i i+1 k
m,i
i=1
k−1
= 2 (cid:88) ϕ(s,a)⊤AJk...AJi+1(cid:16)(cid:0) Λi (cid:1)−1 −(cid:0) Λi+1(cid:1)−1(cid:17) AJi+1...AJkϕ(s,a)
3β k i+1 m,h m,h i+1 k
K
i=1
2
− ϕ(s,a)⊤AJk...AJ1(Λ1 )−1AJ1...AJkϕ(s,a)
3β k 1 m,h 1 k
K
2
+ ϕ(s,a)⊤(Λk )−1ϕ(s,a),
3β m,h
K
where the first inequality follows from (C.2). By the definition of Λi and Woodbury formula, we
m,h
have
(cid:32) (cid:33)−1
(cid:0) Λi (cid:1)−1 −(cid:0) Λi+1(cid:1)−1 = (cid:0) Λi (cid:1)−1 − Λi + (cid:88) ϕ(cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1)⊤
m,h m,h m,h m,h
(sl,al,s′l)∈U (k)
m,h
= (Λi )−1φ(I +φ⊤(Λi )−1φ)−1φ⊤(Λi )−1,
m,h n m,h m,h
where φ is a matrix with the dimension of d×n, n is the number difference of ϕ(cid:0) sl,al(cid:1) between
(cid:0) Λi (cid:1)−1 and (cid:0) Λi+1(cid:1)−1 (i.e. we concatenate all ϕ(cid:0) sl,al(cid:1) into the matrix φ). Note that n ≤ M,
m,h m,h
then we have
(cid:16) (cid:17)
ϕ(s,a)⊤AJ k...AJi+1 (cid:0) Λi (cid:1)−1 −(cid:0) Λi+1(cid:1)−1 AJi+1...AJ kϕ(s,a)
k i+1 m,h m,h i+1 k
(cid:16) (cid:17)
= ϕ(s,a)⊤AJ k...AJi+1 (cid:0) Λi (cid:1)−1 φ(I +φ⊤(cid:0) Λi (cid:1)−1 φ)−1φ⊤(cid:0) Λi (cid:1)−1 AJi+1...AJ kϕ(s,a)
k i+1 m,h n m,h m,h i+1 k
≤ ϕ(s,a)⊤AJ k...AJi+1(cid:0) Λi (cid:1)−1 φφ⊤(cid:0) Λi (cid:1)−1 AJi+1...AJ kϕ(s,a)
k i+1 m,h m,h i+1 k
= (cid:13) (cid:13)ϕ(s,a)⊤AJ k...AJi+1(cid:0) Λi (cid:1)−1 φ(cid:13) (cid:13)2
k i+1 m,h 2
≤ (cid:13) (cid:13)AJ k...AJi+1(Λi )−1/2ϕ(s,a)(cid:13) (cid:13)2 ·(cid:13) (cid:13)(Λi )−1/2φ(cid:13) (cid:13)2
k i+1 m,h 2 m,h F
k
≤ (cid:89) (cid:16) 1−2η λ (cid:0) Λj (cid:1)(cid:17)2Jj tr(cid:0) φ⊤(cid:0) Λi (cid:1)−1 φ(cid:1) ∥ϕ(s,a)∥2 ,
m,j min m,h m,h (Λi )−1
m,h
j=i+1
where ∥·∥
F
is Frobenius norm and the last inequality is due to ∥Λ−1 2X∥2
F
= tr(X⊤Λ−1X) and (C.2).
Thus we have
k k
(cid:13) (cid:13)ϕ(s,a)(cid:13) (cid:13)2
Σk,Jk
≤ 3β2 (cid:88) (cid:89) (cid:16) 1−2η m,jλ min(cid:0) Λj m,h(cid:1)(cid:17)2Jj tr(cid:0) φ⊤(cid:0) Λi m,h(cid:1)−1 φ(cid:1) ∥ϕ(s,a)∥2
(Λi )−1
m,h K m,h
i=1j=i+1
332
+ ∥ϕ(s,a)∥2 .
3β (Λk )−1
K m,h
√
Using the inequality a2+b2 ≤ a+b for a,b > 0, we get
(cid:114) (cid:18) k k
(cid:13) (cid:13)ϕ(s,a)(cid:13) (cid:13) Σk m,J ,hk ≤ 3β2
K
(cid:88) (cid:89) (cid:16) 1−2η m,jλ min(Λj m,h)(cid:17)Jj tr(φ⊤(Λi m,h)−1φ)1 2∥ϕ(s,a)∥ (Λi m,h)−1
i=1j=i+1
(cid:19)
+∥ϕ(s,a)∥
(Λk )−1
m,h
d =ef gk (ϕ(s,a)).
(cid:98)m,h
(cid:16) (cid:17)−1/2(cid:16) (cid:17)
Note that Σk,J k wk,J k −µk,J k ∼ N(0,I ). By the Gaussian concentration property, we
m,h m,h m,h d
have
P(cid:18)(cid:13) (cid:13)(cid:16) Σk,J k(cid:17)−1/2(cid:16) wk,J k −µk,J k(cid:17)(cid:13) (cid:13) ≥ (cid:112) 4dlog(1/δ)(cid:19) ≤ δ2.
(cid:13) m,h m,h m,h (cid:13)
Then we have
P(cid:16)(cid:12) (cid:12)ϕ(s,a)⊤wk,J k −ϕ(s,a)⊤µk,J k(cid:12) (cid:12) ≥ 2gk (ϕ(s,a))(cid:112) dlog(1/δ)(cid:17)
(cid:12) m,h m,h(cid:12) (cid:98)m,h
≤ P(cid:16)(cid:12) (cid:12)ϕ(s,a)⊤wk,J k −ϕ(s,a)⊤µk,J k(cid:12) (cid:12) ≥ 2(cid:112) dlog(1/δ)∥ϕ(s,a)∥ (cid:17)
(cid:12) m,h m,h(cid:12) Σk,Jk
m,h
≤ P(cid:16)(cid:13) (cid:13)ϕ(s,a)(cid:13) (cid:13) ·(cid:13) (cid:13)wk,J k −µk,J k(cid:13) (cid:13) ≥ 2(cid:112) dlog(1/δ)∥ϕ(s,a)∥ (cid:17)
(cid:13) (cid:13) Σk m,J ,hk (cid:13) m,h m,h(cid:13) (Σk m,J ,hk)−1 Σk m,J ,hk
= P(cid:16)(cid:13) (cid:13)(cid:0) Σk,J k(cid:1)−1/2(cid:0) wk,J k −µk,J k(cid:1)(cid:13) (cid:13) ≥ 2(cid:112) dlog(1/δ)(cid:17)
(cid:13) m,h m,h m,h (cid:13)
≤ δ2. (C.3)
Bounding Term I in (C.1): Recall from Proposition B.3, we have
2
k
µk,J k = AJ k...AJ1w1,0 +(cid:88) AJ k...AJi+1(cid:0) I−AJi(cid:1) wi
m,h k 1 m,h k i+1 i (cid:98)m,h
i=1
k−1
= AJ k...AJ1w1,0 +(cid:88) AJ k...AJi+1(wi −wi+1)−AJ k...AJ1w1 +wk
k 1 m,h k i+1 (cid:98)m,h (cid:98)m,h k 1 (cid:98)m,h (cid:98)m,h
i=1
k−1
= AJ k...AJ1(w1,0 −w1 )+(cid:88) AJ k...AJi+1(wi −wi+1)+wk .
k 1 m,h (cid:98)m,h k i+1 (cid:98)m,h (cid:98)m,h (cid:98)m,h
i=1
Then we can get
k−1
ϕ(s,a)⊤(µk,J k −wk ) = ϕ(s,a)⊤AJ k...AJ1(w1,0 −w1 )+ϕ(s,a)⊤(cid:88) AJ k...AJi+1(wi −wi+1).
m,h (cid:98)m,h k 1 m,h (cid:98)m,h k i+1 (cid:98)m,h (cid:98)m,h
(cid:124) (cid:123)(cid:122) (cid:125) i=1
I21 (cid:124) (cid:123)(cid:122) (cid:125)
I22
34In Algorithm 3, we choose w1,0 = 0 and w1 = (Λ1 )−1b1 = 0. Thus we have I = 0. To
m,h (cid:98)m,h m,h m,h 21
bound term I , we use the inequalities in (C.2) and Lemma B.4, we have
22
k−1
I ≤ (cid:12) (cid:12)(cid:88) ϕ(s,a)⊤AJ k...AJi+1(wi −wi+1)(cid:12) (cid:12)
22 (cid:12) k i+1 (cid:98)m,h (cid:98)m,h (cid:12)
i=1
k−1 k
≤ (cid:88) (cid:89) (cid:16) 1−2η λ (Λj )(cid:17)Jj ∥ϕ(s,a)∥(∥wi ∥+∥wi+1∥)
m,j min m,h (cid:98)m,h (cid:98)m,h
i=1j=i+1
k−1 k
≤ (cid:88) (cid:89) (cid:16) 1−2η λ (Λj )(cid:17)Jj ∥ϕ(s,a)∥(cid:0) 2H(cid:112) Mid/λ+2H(cid:112) M(i+1)d/λ(cid:1)
m,j min m,h
i=1j=i+1
k−1 k
≤ 4H(cid:112) MKd/λ(cid:88) (cid:89) (cid:16) 1−2η λ (Λj )(cid:17)Jj ∥ϕ(s,a)∥.
m,j min m,h
i=1j=i+1
Thus we get
k−1 k
ϕ(s,a)⊤(cid:0) µk,J k −wk (cid:1) ≤ 4H(cid:112) MKd/λ(cid:88) (cid:89) (cid:16) 1−2η λ (Λj )(cid:17)Jj ∥ϕ(s,a)∥. (C.4)
m,h (cid:98)m,h m,j min m,h
i=1j=i+1
Substituting (C.3) and (C.4) into (C.1), with probability at least 1−δ2, we have
(cid:12) (cid:12)
(cid:12)ϕ(s,a)⊤wk,J k −ϕ(s,a)⊤wk (cid:12)
(cid:12) m,h (cid:98)m,h(cid:12)
(cid:115)
k−1 k
≤ 4H(cid:112) MKd/λ(cid:88) (cid:89) (cid:16) 1−2η λ (Λj )(cid:17)Jj ∥ϕ(s,a)∥+2 2dlog(1/δ) ∥ϕ(s,a)∥
m,j min m,h 3β
K
(Λk m,h)−1
i=1j=i+1
(cid:115)
k k
+2 2dlog(1/δ) (cid:88) (cid:89) (cid:16) 1−2η λ (Λj )(cid:17)Jj tr(cid:0) φ⊤(cid:0) Λi (cid:1)−1 φ(cid:1) 21 ∥ϕ(s,a)∥
3β
K
m,j min m,h m,h (Λi m,h)−1
i=1j=i+1
d =ef W. (C.5)
Here we choose η = 1/(4λ (Λj )) and set κ = λ (cid:0) Λj (cid:1) /λ (cid:0) Λj (cid:1), then we have
m,j max m,h j max m,h min m,h
(cid:16)
1−2η λ (cid:0) Λj
(cid:1)(cid:17)Jj
= (1−1/2κ )Jj.
m,j min m,h j
We want to have (1−1/2κ j)Jj < ϵ, it suffices to choose J
j
such that
log(1/ϵ)
J ≥ .
j log(cid:0) 1 (cid:1)
1−1/2κj
Note that 1/2κ ≤ 1/2, we have log(1/(1−1/2κ )) ≥ 1/2κ because e−x > 1−x for 0 < x < 1.
j j j
Therefore, we only need to pick J ≥ 2κ log(1/ϵ).
j √ j
Also note that 1 ≥ ∥ϕ(s,a)∥ ≥ λ∥ϕ(s,a)∥ and tr(cid:0) φ⊤(cid:0) Λi (cid:1)−1 φ(cid:1) ≤ M due to the
(Λi )−1 m,h
m,h
fact that n ≤ M. By setting ϵ = 1/(4HMKd) and λ = 1, we obtain
(cid:115)
(cid:88)k−1
(cid:112)
2dlog(1/δ)(cid:18) (cid:88)k−1 √ (cid:19)
W ≤ ϵk−i4H MKd/λ∥ϕ(s,a)∥+2 ∥ϕ(s,a)∥ + ϵk−i M∥ϕ(s,a)∥
3β
K
(Λk m,h)−1
i=1 i=1
35k−1 √
(cid:88) (cid:112)
≤ ϵk−i4H MKd/λ MK∥ϕ(s,a)∥
(Λk )−1
m,h
i=1
(cid:115)
2dlog(1/δ)(cid:18) (cid:88)k−1 √ (cid:19)
+2 ∥ϕ(s,a)∥ + ϵk−iM K∥ϕ(s,a)∥
3β
K
(Λk m,h)−1 (Λk m,h)−1
i=1
(cid:115)
k−1 (cid:18) k−1 (cid:19)
(cid:88) 2dlog(1/δ) (cid:88)
≤ ϵk−i−1∥ϕ(s,a)∥ +2 ∥ϕ(s,a)∥ + ϵk−i−1∥ϕ(s,a)∥
(Λk m,h)−1 3β
K
(Λk m,h)−1 (Λk m,h)−1
i=1 i=1
(cid:115)
(cid:18) (cid:19)
2dlog(1/δ) 4
≤ 5 + ∥ϕ(s,a)∥ ,
3β
K
3 (Λk m,h)−1
√
wherethesecondinequalityfollowsfrom∥ϕ(s,a)∥ ≥ 1/(cid:112) K(k)+1∥ϕ(s,a)∥ ≥ 1/ MK∥ϕ(s,a)∥,
(Λk )−1
m,h
the fourth inequality follows from (cid:80)k−1ϵk−i−1 = (cid:80)k−2ϵi < 1/(1−ϵ) ≤ 4/3. Finally we have
i=1 i=0
(cid:115)
(cid:18)(cid:12) (cid:12) (cid:18) 2dlog(1/δ) 4(cid:19) (cid:19)
P (cid:12)ϕ(s,a)⊤wk,J k −ϕ(s,a)⊤wk (cid:12) ≤ 5 + ∥ϕ(s,a)∥
(cid:12) m,h (cid:98)m,h(cid:12) 3β
K
3 (Λk m,h)−1
(cid:16)(cid:12) (cid:12) (cid:17)
≥ P (cid:12)ϕ(s,a)⊤wk,J k −ϕ(s,a)⊤wk (cid:12) ≤ W
(cid:12) m,h (cid:98)m,h(cid:12)
≥ 1−δ2.
This completes the proof.
C.4 Proof of Lemma B.6
Proof. Recall that wk,J k ∼ N(cid:0) µk,J k,Σk,J k(cid:1). Let ξk,J k = wk,J k −µk,J k ∼ N(0,Σk,J k), thus we have
m,h m,h m,h m,h m,h m,h m,h
(cid:13) (cid:13)wk,J k(cid:13) (cid:13) = (cid:13) (cid:13)µk,J k +ξk,J k(cid:13) (cid:13) ≤ (cid:13) (cid:13)µk,J k(cid:13) (cid:13)+(cid:13) (cid:13)ξk,J k(cid:13) (cid:13). (C.6)
m,h m,h m,h m,h m,h
Bounding (cid:13) (cid:13)µk,J k(cid:13) (cid:13) in (C.6): Based on Proposition B.3, we have
m,h
k
(cid:13) (cid:13)µk m,J ,hk(cid:13) (cid:13) = (cid:13) (cid:13) (cid:13)AJ kk...AJ 11w m1,0 ,h+(cid:88) AJ kk...AJ i+i+ 11(cid:0) I−AJ ii(cid:1) w (cid:98)mi ,h(cid:13) (cid:13)
(cid:13)
i=1
k
≤ (cid:88)(cid:13) (cid:13)AJ kk...AJ i+i+ 11(cid:0) I−AJ ii(cid:1)(cid:13) (cid:13)
F
·(cid:13) (cid:13)w (cid:98)mi ,h(cid:13) (cid:13)
i=1
k
≤ 2H(cid:112) MKd/λ(cid:88)(cid:13) (cid:13)AJ k...AJi+1(cid:0) I−AJi(cid:1)(cid:13) (cid:13)
k i+1 i F
i=1
k
≤ 2Hd(cid:112) MK/λ(cid:88) ∥A k∥ 2J k...∥A i+1∥J 2i+1(cid:13) (cid:13)(cid:0) I−AJ ii(cid:1)(cid:13) (cid:13)
2
i=1
k k
≤ 2Hd(cid:112) MK/λ(cid:88) (cid:89) (cid:0) 1−2η m,jλ min(cid:0) Λj m,h(cid:1)(cid:1)Jj(cid:0) ∥I∥ 2+(cid:13) (cid:13)A i(cid:13) (cid:13)J 2i(cid:1)
i=1j=i+1
36k k
≤ 2Hd(cid:112) MK/λ(cid:88) (cid:89) (cid:0) 1−2η λ (cid:0) Λj (cid:1)(cid:1)Jj(cid:0) 1+(cid:0) 1−2η λ (cid:0) Λi (cid:1)(cid:1)Jj(cid:1) ,
m,j min m,h m,i min m,h
i=1j=i+1
where the second inequality holds from Lemma B.4, the third inequality follows from the fact that
rank(cid:0) AJ k...AJi+1(cid:0) I−AJi(cid:1)(cid:1) ≤ d and ∥X∥ ≤ ∥X∥ ≤ rank(X)∥X∥ where ∥X∥ = σ (X).
k i+1 i 2 F 2 2 max
Recall that in Lemma B.5, we set J ≥ 2κ log(1/ϵ) where κ = λ (cid:0) Λj (cid:1) /λ (cid:0) Λj (cid:1),
j j j max m,h min m,h
ϵ = 1/(4HMKd) and λ = 1, thus we get
k
(cid:13) (cid:13)µk,J k(cid:13) (cid:13) ≤ 2Hd(cid:112) MK/λ(cid:88) (ϵk−i+ϵk−i+1)
m,h
i=1
∞
(cid:112) (cid:88)
≤ 4Hd MK/λ ϵi
i=0
16 √
≤ Hd MK.
3
Bounding (cid:13) (cid:13)ξk,J k(cid:13) (cid:13) in (C.6): Note that ξk,J k ∼ N(cid:0) 0,Σk,J k(cid:1), using Gaussian concentration
m,h m,h m,h
Lemma H.5, we have
(cid:18) (cid:114) (cid:19)
P (cid:13) (cid:13)ξk,J k(cid:13) (cid:13) ≤ 1 tr(cid:0) Σk,J k(cid:1) ≥ 1−δ.
m,h δ m,h
Recall from Proposition B.3, we have
k
tr(cid:0) Σk,J k(cid:1) = (cid:88) 1 tr(cid:0) AJ k...AJi+1(cid:0) I−A2Ji(cid:1) (Λi )−1(I+A )−1AJi+1...AJ k(cid:1)
m,h β k i+1 i m,h i i+1 k
m,i
i=1
k
≤ (cid:88) 1 tr(cid:0) AJ k(cid:1) ...tr(cid:0) AJi+1(cid:1) tr(cid:0) I−A2Ji(cid:1) tr(cid:0)(cid:0) Λi (cid:1)−1(cid:1) tr(cid:0)(cid:0) I+A )−1(cid:1)
β k i+1 i m,h i
m,i
i=1
×tr(cid:0) AJi+1(cid:1) ...tr(cid:0) AJ k(cid:1) ,
i+1 k
where the inequality holds due to Lemma H.6. Recall from (C.2) that, when η ≤ 1/(4λ (Λk ))
m,k max m,h
for all k and m, we have AJ ii ≼ (1−2η m,kλ min(Λk m,h))JjI, set λ = 1, then we obtain
(cid:16) (cid:17)
tr(AJi) ≤ tr (cid:0) 1−2η λ (cid:0) Λk (cid:1)(cid:1)JjI ≤ d(cid:0) 1−2η λ (cid:0) Λk (cid:1)(cid:1)Jj ≤ dϵ ≤ 1.
i m,k min m,h m,k min m,h
Similarly, we have I−A2Ji ≼ (cid:0) 1− 1 (cid:1) I, then we get
i 22Ji
(cid:18) (cid:19)
1
tr(I−A2Ji) ≤ 1− d < d.
i 22Ji
Also, based on (I+A )−1 ≼ 2I, we have
i 3
tr(cid:0)
(I+A
)−1(cid:1)
≤
2
d.
i
3
37Note that λ (cid:0)(cid:0) Λi (cid:1)−1(cid:1) ≤ 1, we have
max m,h
tr(cid:0)(cid:0) Λi (cid:1)−1(cid:1) ≤ (cid:88) λ(cid:0)(cid:0) Λi (cid:1)−1(cid:1) ≤ d.
m,h m,h
Combine the above results together and choose β = β for all i ∈ [K] and m ∈ M, we have
m,i K
K
tr(cid:0) Σk,J k(cid:1) ≤ (cid:88) 1 · 2 ·d3 = 2 Kd3.
m,h β 3 3β
m,i K
i=1
Then we have
(cid:18) (cid:114) (cid:19) (cid:18) (cid:114) (cid:19)
P (cid:13) (cid:13)ξk,J k(cid:13) (cid:13) ≤ 1 · 2 Kd3 ≥ P (cid:13) (cid:13)ξk,J k(cid:13) (cid:13) ≤ 1 tr(cid:0) Σk,J k(cid:1) ≥ 1−δ.
m,h δ 3β m,h δ m,h
K
Combine above results together: with probability at least 1−δ, we have
(cid:115)
(cid:13) (cid:13)wk,J k(cid:13) (cid:13) ≤ 16 Hd√ MK + 2K d3/2.
m,h 3 3β δ
K
This completes the proof.
C.5 Proof of Lemma B.7
Proof. Based on Lemma B.6, for any fixed n ∈ [N], with probability at least 1 − δ, for any
(m,k,h) ∈ M×[K]×[H], we have
(cid:115)
(cid:13) (cid:13)wk,J k,n(cid:13) (cid:13) ≤ 16 Hd√ MK + 2K d3/2.
m,h 3 3β δ
K
By taking union over n,m,k,h, we have for all (m,k,h) ∈ M×[K]×[H] and for all n ∈ [N], with
probability 1−δ/2, we have
(cid:115)
(cid:13) (cid:13)w mk,J ,hk,n(cid:13) (cid:13) ≤ 1 36 Hd√ MK + 4N 3M βH δK2 d3/2 = B δ/2NMHK. (C.7)
K
Based on Lemma H.7 and Lemma H.9, we have that for any ε > 0 and δ > 0, with probability at
least 1−δ/2,
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:2)(cid:0) Vk −P Vk (cid:1)(cid:0) sl,al(cid:1)(cid:3)(cid:13) (cid:13)
(cid:13) m,h+1 h m,h+1 (cid:13)
(cid:13) (cid:13)
(sl,al,s′l)∈U (k) (Λk )−1
m,h m,h
(cid:18) (cid:20)
d
(cid:18) k+λ(cid:19) (cid:18)B (cid:19) 3(cid:21) 8k2ε2(cid:19)1/2
≤ 4H2 log +dlog δ/2NMHK +log +
2 λ ε δ λ
√
(cid:20)
d
(cid:18) k+λ(cid:19) (cid:18)B (cid:19) 3(cid:21)1/2
2 2kε
δ/2NMHK
≤ 2H log +dlog +log + √ .
2 λ ε δ λ
38Here we set λ = 1,ε = √H , with probability at least 1−δ/2, we have
2 2k
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:2)(cid:0) Vk −P Vk (cid:1)(cid:0) sl,al(cid:1)(cid:3)(cid:13) (cid:13)
(cid:13) m,h+1 h m,h+1 (cid:13)
(cid:13) (cid:13)
(sl,al,s′l)∈U (k) (Λk )−1
m,h m,h
√ (cid:20) 1 (cid:18)B (cid:19) 3(cid:21)1/2
δ/2NMHK
≤ 2H d log(k+1)+log +log +H
2 √H δ
2 2k
√
√ (cid:20) 1 (cid:18)2 2KB (cid:19) 3(cid:21)1/2
≤ 3H d log(K +1)+log δ/2NMHK +log . (C.8)
2 H δ
(cid:104)
By applying union bound between (C.7) and (C.8), and define that C = 1 log(K +1)+log 3 +
δ 2 δ
√
log(cid:16)
2 2KB
δ/2NMHK(cid:17)(cid:105)1/2
, finally we obtain that for all (m,k,h) ∈ M×[K]×[H],
H
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:2)(cid:0) Vk −P Vk (cid:1)(cid:0) sl,al(cid:1)(cid:3)(cid:13) (cid:13) ≤ 3H√ dC ,
(cid:13) m,h+1 h m,h+1 (cid:13) δ
(cid:13) (cid:13)
(sl,al,s′l)∈U (k) (Λk )−1
m,h m,h
with probability at least 1−δ.
C.6 Proof of Lemma B.8
Proof. We denote the inner product over S by ⟨·,·⟩ . Based on P (·|s,a) = (cid:10) ϕ(s,a),µ (·)(cid:11) in
S h h S
Definition 4.1, we have
P Vk (s,a) = ϕ(s,a)⊤(cid:10) µ ,Vk (cid:11)
h m,h+1 h m,h+1 S
= ϕ(s,a)⊤(cid:0) Λk (cid:1)−1(cid:0) Λk (cid:1)(cid:10) µ ,Vk (cid:11)
m,h m,h h m,h+1 S
(cid:32) (cid:33)
= ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1)⊤ +λI (cid:10) µ ,Vk (cid:11)
m,h h m,h+1 S
(sl,al,s′l)∈U (k)
m,h
(cid:32) (cid:33)
= ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:0)P Vk (cid:1)(cid:0) sl,al(cid:1) +λI(cid:10) µ ,Vk (cid:11) .
m,h h m,h+1 h m,h+1 S
(sl,al,s′l)∈U (k)
m,h
(C.9)
Here the last equality uses P (·|s,a) = (cid:10) ϕ(s,a),µ (·)(cid:11) again. Then we can separate the following
h h S
error into three parts,
ϕ(s,a)⊤wk −r (s,a)−P Vk (s,a)
(cid:98)m,h h h m,h+1
= ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) (cid:2) r (cid:0) sl,al(cid:1) +Vk (s′l )(cid:3) ϕ(cid:0) sl,al(cid:1) −r (s,a)
m,h h m,h+1 h
(sl,al,s′l)∈U (k)
m,h
(cid:32) (cid:33)
−ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:0)P Vk (cid:1)(cid:0) sl,al(cid:1) +λI(cid:10) µ ,Vk (cid:11)
m,h h m,h+1 h m,h+1 S
(sl,al,s′l)∈U (k)
m,h
39(cid:32) (cid:33)
= ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:2)(cid:0) Vk −P Vk (cid:1)(cid:0) sl,al(cid:1)(cid:3)
m,h m,h+1 h m,h+1
(sl,al,s′l)∈U (k)
m,h
(cid:124) (cid:123)(cid:122) (cid:125)
(i)
(cid:32) (cid:33)
+ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) r (cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1) −r (s,a)
m,h h h
(sl,al,s′l)∈U (k)
m,h
(cid:124) (cid:123)(cid:122) (cid:125)
(ii)
−λϕ(s,a)⊤(cid:0) Λk (cid:1)−1(cid:10) µ ,Vk (cid:11) . (C.10)
m,h h m,h+1 S
(cid:124) (cid:123)(cid:122) (cid:125)
(iii)
Here the first equality holds due to (C.9). We now provide an upper bound for each of the terms in
(C.10).
Bounding Term (i) in (C.10): using Cauchy-Schwarz inequality and Lemma B.7, with
probability at least 1−δ, for all (m,k,h) ∈ M×[K]×[H] and for any (s,a) ∈ S ×A, we have
(cid:32) (cid:33)
ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:2)(cid:0) Vk −P Vk (cid:1)(cid:0) sl,al(cid:1)(cid:3)
m,h m,h+1 h m,h+1
(sl,al,s′l)∈U (k)
m,h
(cid:13) (cid:13)
≤ (cid:13) (cid:13) (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:2)(cid:0) Vk −P Vk (cid:1)(cid:0) sl,al(cid:1)(cid:3)(cid:13) (cid:13) ∥ϕ(s,a)∥
(cid:13) m,h+1 h m,h+1 (cid:13) (Λk )−1
(cid:13) (cid:13) m,h
(sl,al,s′l)∈U (k) (Λk )−1
√ m,h m,h
≤ 3H dC ∥ϕ(s,a)∥ . (C.11)
δ (Λk )−1
m,h
Bounding Term (ii) in (C.10): we first note that
(cid:32) (cid:33)
ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) r (cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1) −r (s,a)
m,h h h
(sl,al,s′l)∈Um,h(k)
(cid:32) (cid:33)
=ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) r (cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1) −ϕ(s,a)⊤θ
m,h h h
(sl,al,s′l)∈Um,h(k)
(cid:32) (cid:33)
=ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) r (cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1) −Λk θ
m,h h m,h h
(sl,al,s′l)∈Um,h(k)
(cid:32) (cid:33)
=ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) r (cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1) − (cid:88) ϕ(cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1)⊤ θ −λIθ
m,h h h h
(sl,al,s′l)∈Um,h(k) (sl,al,s′l)∈Um,h(k)
(cid:32) (cid:33)
=ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) r (cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1) − (cid:88) ϕ(cid:0) sl,al(cid:1) r (cid:0) sl,al(cid:1) −λθ
m,h h h h
(sl,al,s′l)∈Um,h(k) (sl,al,s′l)∈Um,h(k)
=−λϕ(s,a)⊤(cid:0) Λk (cid:1)−1 θ , (C.12)
m,h h
where the first and fourth equality holds due to the definition r (s,a) = (cid:10) ϕ(s,a),θ (cid:11) from
h h
Definition 4.1, the third equality uses the definition of Λk . Next we can obtain that
m,h
−λϕ(s,a)⊤(cid:0) Λk (cid:1)−1 θ ≤ λ∥ϕ(s,a)∥ ∥θ ∥
m,h h (Λk )−1 h (Λk )−1
m,h m,h
40√
≤ λ∥ϕ(s,a)∥ ∥θ ∥
(Λk )−1 h
√ m,h
≤ λd∥ϕ(s,a)∥ , (C.13)
(Λk )−1
m,h
√
whereweusethefactthatλ (cid:0)(cid:0) Λk (cid:1)−1(cid:1) ≤ 1/λand∥θ ∥ ≤ dfromDefinition4.1. ByCombining
max m,h h
(C.12) and (C.13), we obtain
(cid:32) (cid:33)
√
ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) r (cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1) −r (s,a) ≤ λd∥ϕ(s,a)∥ . (C.14)
m,h h h (Λk )−1
m,h
(sl,al,s′l)∈U (k)
m,h
Bounding Term (iii) in (C.10): we have
λϕ(s,a)⊤(cid:0) Λk m,h(cid:1)−1(cid:10) µ h,V mk ,h+1(cid:11) S ≤ λ∥ϕ(s,a)∥ (Λk )−1(cid:13) (cid:13)(cid:10) µ h,V mk ,h+1(cid:11) S(cid:13) (cid:13) (Λk )−1
√ m,h m,h
≤ λ∥ϕ(s,a)∥ (Λk )−1(cid:13) (cid:13)(cid:10) µ h,V mk ,h+1(cid:11) S(cid:13) (cid:13)
√ m,h
≤ H λ∥ϕ(s,a)∥ ∥µ ∥
(Λk )−1 h
√ m,h
≤ H λd∥ϕ(s,a)∥ , (C.15)
(Λk )−1
m,h
where the second inequality holds due to the fact that λ (cid:0)(cid:0) Λk (cid:1)−1(cid:1) ≤ 1/λ, the third inequality
max m,h √
uses the fact that Vk ≤ H and the last inequality follows from ∥µ ∥ ≤ d in Definition 4.1.
m,h+1 h
Combine Terms (i)(ii)(iii) together: combine (C.11), (C.14) and (C.15), then set λ = 1,
with probability at least 1−δ, we get
(cid:12) (cid:12) (cid:16) √ √ (cid:17)
(cid:12)ϕ(s,a)⊤wk −r (s,a)−P Vk (s,a)(cid:12) ≤ 3HC + λd+H λd ∥ϕ(s,a)∥
(cid:12) (cid:98)m,h h h m,h+1 (cid:12) δ (Λk )−1
√ m,h
≤ 5H dC ∥ϕ(s,a)∥ ,
δ (Λk )−1
m,h
This completes the proof.
C.7 Proof of Lemma B.9
Proof. Recall from Definition B.1,
−lk (s,a) = Qk (s,a)−r (s,a)−P Vk (s,a)
m,h m,h h h m,h+1
(cid:110) (cid:111)+
= min max ϕ(s,a)⊤wk,J k,n,H −h+1 −r (s,a)−P Vk (s,a)
m,h h h m,h+1
n∈[N]
≤ max ϕ(s,a)⊤wk,J k,n−r (s,a)−P Vk (s,a)
m,h h h m,h+1
n∈[N]
= max ϕ(s,a)⊤wk,J k,n−ϕ(s,a)⊤wk +ϕ(s,a)⊤wk −r (s,a)−P Vk (s,a)
m,h (cid:98)m,h (cid:98)m,h h h m,h+1
n∈[N]
≤ max (cid:12) (cid:12)ϕ(s,a)⊤w mk,J ,hk,n−ϕ(s,a)⊤w (cid:98)mk ,h(cid:12) (cid:12)+(cid:12) (cid:12)ϕ(s,a)⊤w (cid:98)mk ,h−r h(s,a)−P hV mk ,h+1(s,a)(cid:12) (cid:12).
n∈[N]
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) I2
I1
41Bounding Term I : based on Lemma B.5, for any fixed n ∈ [N], for any (m,h,k) ∈ M×[H]×[K]
1
and for any (s,a) ∈ S ×A, with probability at least 1−δ2, we have
(cid:32) (cid:115) (cid:33)
(cid:12) (cid:12)ϕ(s,a)⊤w mk,J ,hk,n−ϕ(s,a)⊤w (cid:98)mk ,h(cid:12) (cid:12) ≤ 5 2dl 3o βg( K1/δ) + 4
3
∥ϕ(s,a)∥ (Λk m,h)−1.
By taking union bound over n, we have for all n ∈ [N], with probability 1−δ2, we have
(cid:32) (cid:115) (cid:0)√ (cid:1) (cid:33)
(cid:12) (cid:12)ϕ(s,a)⊤w mk,J ,hk,n−ϕ(s,a)⊤w (cid:98)mk ,h(cid:12) (cid:12) ≤ 5 2dlog
3β
KN/δ + 4
3
∥ϕ(s,a)∥ (Λk m,h)−1.
This indicates, for any (m,h,k) ∈ M×[H]×[K] and (s,a) ∈ S×A, with probability at least 1−δ2,
we have
(cid:32) (cid:115) (cid:0)√ (cid:1) (cid:33)
Term I 1 = nm ∈a [Nx ](cid:12) (cid:12)ϕ(s,a)⊤w mk,J ,hk,n−ϕ(s,a)⊤w (cid:98)mk ,h(cid:12) (cid:12) ≤ 5 2dlog
3β
KN/δ + 4
3
∥ϕ(s,a)∥ (Λk m,h)−1.
(C.16)
Bounding Term I : based on Lemma B.8, with probability at least 1−δ, for any (m,h,k) ∈
2
M×[H]×[K] and (s,a) ∈ S ×A, we have
√
(cid:12) (cid:12)ϕ(s,a)⊤w (cid:98)mk ,h−r hk(s,a)−P hV mk ,h+1(s,a)(cid:12) (cid:12) ≤ 5H dC δ∥ϕ(s,a)∥ (Λk )−1.
m,h
Combine the two result above, by taking union bound, with probability at least 1−δ−δ2, for any
(m,h,k) ∈ M×[H]×[K] and (s,a) ∈ S ×A, we have
(cid:32) √ (cid:115) 2dlog(cid:0)√ N/δ(cid:1) 4(cid:33)
−lk (s,a) ≤ 5H dC +5 + ∥ϕ(s,a)∥ .
m,h δ 3β
K
3 (Λk m,h)−1
This completes the proof.
C.8 Proof of Lemma B.10
Proof. Recall from Definition B.1,
lk (s,a) = r (s,a)+P Vk (s,a)−Qk (s,a).
m,h h h m,h+1 m,h
Note that
(cid:110) (cid:111)+
Qk (s,a) = min max ϕ(s,a)⊤wk,J k,n,H −h+1 ≤ max ϕ(x,a)⊤wk,J k,n.
m,h m,h m,h
n∈[N] n∈[N]
Note that ∥ϕ(s,a)∥ ≤ (cid:112) 1/λ∥ϕ(s,a)∥ ≤ 1 for all ϕ(s,a). Define C(ε) to be a ε-cover of
(Λk )−1
m,h
(cid:8) ϕ | ∥ϕ∥ ≤ 1(cid:9). Based on Lemma H.8, we have |C(ε)| ≤ (3/ε)d.
(Λk )−1
m,h
42First, for any fixed ϕ(s,a) ∈ C(ε), based on the results in Proposition B.3, we have that
(cid:16) (cid:17)
ϕ(s,a)⊤wk,J k,n ∼ N ϕ(s,a)⊤µk,J k,ϕ(s,a)⊤Σk,J kϕ(s,a) for any fixed n ∈ [N]. Now we define
m,h m,h m,h
r (s,a)+P Vk (s,a)−ϕ(s,a)⊤µk,J k
h h m,h+1 m,h
Z = .
k (cid:113)
ϕ(s,a)⊤Σk,J kϕ(s,a)
m,h
When |Z | < 1, by Gaussian concentration Lemma H.10, we have
k
(cid:16) (cid:17)
P ϕ(s,a)⊤wk,J k,n ≥ r (s,a)+P Vk (s,a)
m,h h h m,h+1
(cid:32) ϕ(s,a)⊤wk,J k,n−ϕ(s,a)⊤µk,J k r (s,a)+P Vk (s,a)−ϕ(s,a)⊤µk,J k(cid:33)
= P m,h m,h ≥ h h m,h+1 m,h
(cid:113) (cid:113)
ϕ(s,a)⊤Σk,J kϕ(s,a) ϕ(s,a)⊤Σk,J kϕ(s,a)
m,h m,h
(cid:32) ϕ(s,a)⊤wk,J k,n−ϕ(s,a)⊤µk,J
k
(cid:33)
= P m,h m,h ≥ Z
(cid:113) k
ϕ(s,a)⊤Σk,J kϕ(s,a)
m,h
1
≥ √ exp(−Z2/2)
k
2 2π
1
≥ √ .
2 2eπ
Consider the numerator of Z :
k
(cid:12) (cid:12)r h(s,a)+P hV mk ,h+1(s,a)−ϕ(s,a)⊤µk m,J ,hk(cid:12) (cid:12)
≤ (cid:12) (cid:12)r h(s,a)+P hV mk ,h+1(s,a)−ϕ(s,a)⊤w (cid:98)mk ,h(cid:12) (cid:12)+(cid:12) (cid:12)ϕ(s,a)⊤w (cid:98)mk ,h−ϕ(s,a)⊤µk m,J ,hk(cid:12) (cid:12).
Based on Lemma B.8, with probablity at least 1−δ, we have
√
|r (s,a)+P Vk (s,a)−ϕ(s,a)⊤wk | ≤ 5H dC ∥ϕ(s,a)∥ ,
h h m,h+1 (cid:98)m,h δ (Λk )−1
m,h
From (C.4), we have
k−1 k
ϕ(s,a)⊤(cid:0) µk,J k −wk (cid:1) ≤ 4H(cid:112) MKd/λ(cid:88) (cid:89) (cid:16) 1−2η λ (Λj )(cid:17)Jj ∥ϕ(s,a)∥.
m,h (cid:98)m,h m,j min m,h
i=1j=i+1
Recall the proof of Lemma B.5, we set η = 1/(4λ (Λj )),J ≥ 2κ log(1/ϵ), then we have for
m,j max m,h j j
all j ∈ [K], (1−2η m,jλ min(Λj m,h))Jj ≤ ϵ, set ϵ = 1/4HMKd and λ = 1, we have
√ k−1
(cid:12) (cid:12)ϕ(s,a)⊤w (cid:98)mk ,h−ϕ(s,a)⊤µk m,J ,hk(cid:12) (cid:12) ≤ 4H MKd(cid:88) ϵk−i∥ϕ(s,a)∥
i=1
(cid:88)k−1 1 √ √
≤ ϵk−i−1 4H MKd MK∥ϕ(s,a)∥
4MHKd (Λk m,h)−1
i=1
4
≤ ∥ϕ(s,a)∥ .
3 (Λk m,h)−1
43So, with probablity at least 1−δ, we have
(cid:12) (cid:12)r h(s,a)+P hV mk ,h+1(s,a)−ϕ(s,a)⊤µk m,J ,hk(cid:12) (cid:12) ≤
(cid:18) 5H√
dC δ +
34(cid:19)
∥ϕ(s,a)∥ (Λk m,h)−1. (C.17)
Consider the denominator of Z : recall from the definition of Σk,J k from Proposition B.3, then
k m,h
we have
k
ϕ(s,a)⊤Σk,Jkϕ(s,a)=(cid:88) 1 ϕ(s,a)⊤AJk...AJi+1(cid:0) I−A2Ji(cid:1)(cid:0) Λi (cid:1)−1 (I+A )−1AJi+1...AJkϕ(s,a)
m,h β k i+1 m,h i i+1 k
m,i
i=1
k
≥(cid:88) 1 ϕ(s,a)⊤AJk...AJi+1(cid:0) I−A2Ji(cid:1)(cid:0) Λi (cid:1)−1 AJi+1...AJkϕ(s,a),
2β k i+1 m,h i+1 k
m,i
i=1
where we used the fact that 1I ≼ (I+A )−1. Then we have
2 k
k
ϕ(s,a)⊤Σk,Jkϕ(s,a)≥(cid:88) 1 ϕ(s,a)⊤AJk...AJi+1(cid:0)(cid:0) Λi (cid:1)−1 −AJi(cid:0) Λi (cid:1)−1 AJi(cid:1) AJi+1...AJkϕ(s,a)
m,h 2β k i+1 m,h i m,h i i+1 k
m,i
i=1
k−1
= 1 (cid:88) ϕ(s,a)⊤AJk...AJi+1(cid:0)(cid:0) Λi (cid:1)−1 −(cid:0) Λi+1(cid:1)−1(cid:1) AJi+1...AJkϕ(s,a)
2β k i+1 m,h m,h i+1 k
K
i=1
− 1 ϕ(s,a)⊤AJk...AJ1(cid:0) Λ1 (cid:1)−1 AJ1...AJkϕ(s,a)
2β k 1 m,h 1 k
K
+ 1 ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 ϕ(s,a).
2β m,h
K
By the definition of Λi and Woodbury formula, we have
m,h
(cid:32) (cid:33)−1
(cid:0) Λi (cid:1)−1 −(cid:0) Λi+1(cid:1)−1 = (cid:0) Λi (cid:1)−1 − Λi + (cid:88) ϕ(cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1)⊤
m,h m,h m,h m,h
(sl,al,s′l)∈U (k)
m,h
= (Λi )−1φ(cid:0) I +φ⊤(Λi )−1φ(cid:1)−1 φ⊤(Λi )−1,
m,h n m,h m,h
where φ is a matrix with the dimension of d×n, n is the number difference of ϕ(cid:0) sl,al(cid:1) between
(cid:0) Λi (cid:1)−1 and (cid:0) Λi+1(cid:1)−1 (i.e. we concatenate all ϕ(cid:0) sl,al(cid:1) in to the matrix φ). Note that n ≤ M,
m,h m,h
we have
(cid:16) (cid:17)
ϕ(s,a)⊤AJ k...AJi+1 (cid:0) Λi (cid:1)−1 −(cid:0) Λi+1(cid:1)−1 AJi+1...AJ kϕ(s,a)
k i+1 m,h m,h i+1 k
(cid:16) (cid:17)
= ϕ(s,a)⊤AJ k...AJi+1 (cid:0) Λi (cid:1)−1 φ(I +φ⊤(cid:0) Λi (cid:1)−1 φ)−1φ⊤(cid:0) Λi (cid:1)−1 AJi+1...AJ kϕ(s,a)
k i+1 m,h n m,h m,h i+1 k
≤ ϕ(s,a)⊤AJ k...AJi+1(cid:0) Λi (cid:1)−1 φφ⊤(cid:0) Λi (cid:1)−1 AJi+1...AJ kϕ(s,a)
k i+1 m,h m,h i+1 k
= (cid:13) (cid:13)ϕ(s,a)⊤AJ k...AJi+1(cid:0) Λi (cid:1)−1 φ(cid:13) (cid:13)2
k i+1 m,h 2
≤ (cid:13) (cid:13)AJ k...AJi+1(Λi )−1/2ϕ(s,a)(cid:13) (cid:13)2 ·(cid:13) (cid:13)(Λi )−1/2φ(cid:13) (cid:13)2
k i+1 m,h 2 m,h F
k
≤ (cid:89) (cid:16) 1−2η λ (cid:0) Λj (cid:1)(cid:17)2Jj tr(cid:0) φ⊤(cid:0) Λi (cid:1)−1 φ(cid:1) ∥ϕ(s,a)∥2 ,
m,j min m,h m,h (Λi )−1
m,h
j=i+1
44where ∥·∥
F
is Frobenius norm and the last inequality is due to ∥Λ−1 2X∥2
F
= tr(X⊤Λ−1X) and (C.2).
Therefore, we have
k
ϕ(s,a)⊤Σk,J kϕ(s,a) ≥ 1 ∥ϕ(s,a)∥2 − 1 (cid:89)(cid:0) 1−2η λ (cid:0) Λi (cid:1)(cid:1)2Ji∥ϕ(s,a)∥2
m,h 2β (Λk )−1 2β m,i min m,h (Λ1 )−1
K m,h K m,h
i=1
k−1 k
− 1 (cid:88) (cid:89) (cid:0) 1−2η λ (cid:0) Λj (cid:1)(cid:1)2Jj tr(cid:0) φ⊤(cid:0) Λi (cid:1)−1 φ(cid:1) ∥ϕ(s,a)∥2 .
2β m,j min m,h m,h (Λi )−1
K m,h
i=1j=i+1
Similar to the proof of Lemma B.5, note that tr(cid:0) φ⊤(Λi )−1φ(cid:1) ≤ M, when we choose J ≥
m,h j
2κ log(3kM), we have
j
√
(cid:18) k−1 (cid:19)
1 ∥ϕ(s,a)∥ (cid:88) M
∥ϕ(s,a)∥ ≥ √ ∥ϕ(s,a)∥ − − ∥ϕ(s,a)∥
Σk m,J ,hk 2 β
K
(Λk m,h)−1 (3KM)k (3kM)k−i
i=1
(cid:18) (cid:19)
1 1 1
≥ √ ∥ϕ(s,a)∥ − √ ∥ϕ(s,a)∥− √ ∥ϕ(s,a)∥
2 β K
(Λk m,h)−1
3 kM 6 kM
1
≥ √ ∥ϕ(s,a)∥ , (C.18)
4 β
K
(Λk m,h)−1
√
where we used the fact that λ (cid:0)(cid:0) Λk (cid:1)−1(cid:1) ≥ 1/kM and ∥ϕ(s,a)∥ ≥ 1/ kM∥ϕ(s,a)∥.
min m,h (Λk )−1
m,h
Therefore, according to (C.17) and (C.18), with probablity at least 1−δ, it holds that
|Z | =
(cid:12) (cid:12) (cid:12)r h(s,a)+P hV mk ,h+1(s,a)−ϕ(s,a)⊤µk m,J ,hk(cid:12) (cid:12)
(cid:12)
k (cid:12) (cid:113) (cid:12)
(cid:12) ϕ(s,a)⊤Σk,J kϕ(s,a) (cid:12)
m,h
√
5H dC + 4
≤ δ 3,
√1
4 βK
√
which implies |Z | < 1 when √1 = 20H dC + 16.
k βK δ 3
Till now we have proved that for any fixed ϕ(s,a) ∈ C(ε) and for all (m,h,k) ∈ M×[H]×[K],
for any fixed n ∈ [N], with probablity at least 1−δ, we have
(cid:16) (cid:17) 1
P ϕ(s,a)⊤wk,J k,n−r (s,a)−P Vk (s,a) ≥ 0 ≥ √ .
m,h h h m,h+1 2 2eπ
By taking union bound over n ∈ [N], with probablity at least 1−δ, we have
P(cid:16) max (cid:8) ϕ(s,a)⊤wk,J k,n−r (s,a)−P Vk (s,a)(cid:9) ≥ 0(cid:17) ≥ 1−(cid:16) 1− √1 (cid:17)N = 1−c′N ,
n∈[N] m,h h h m,h+1 2 2eπ 0
where c′ = 1− √1 . Therefore, for any fixed ϕ(s,a) ∈ C(ε) and for all (m,h,k) ∈ M×[H]×[K],
0 2 2eπ
with probability at least (1−δ)(cid:0) 1−c′N(cid:1) > 1−δ−c′N, we have
0 0
max (cid:8) ϕ(s,a)⊤wk,J k,n−r (s,a)−P Vk (s,a)(cid:9) ≥ 0. (C.19)
m,h h h m,h+1
n∈[N]
45Next for any ϕ = ϕ(s,a), we can find ϕ′ ∈ C(ε) such that ∥ϕ − ϕ′∥ ≤ ε. We define
(Λk )−1
m,h
∆ϕ = ϕ−ϕ′. Recall from Definition 4.1, we have
r (s,a)+P Vk (s,a) = ϕ(s,a)⊤θ +ϕ(s,a)⊤(cid:10) µ ,Vk (cid:11) d =ef ϕ(s,a)⊤wk ,
h h m,h+1 h h m,h+1 S m,h
√
where wk = θ +(cid:10) µ ,Vk (cid:11) . Note that max{∥µ (S)∥,∥θ ∥} ≤ d and Vk ≤ H−h ≤ H,
m,h h h m,h+1 S h h m,h+1
thus we have
√ √ √
(cid:13) (cid:13)w mk ,h(cid:13) (cid:13) ≤ ∥θ h∥+(cid:13) (cid:13)(cid:10) µ h,V mk ,h+1(cid:11) S(cid:13) (cid:13) ≤ d+H d ≤ 2H d.
Then we define the regression error ∆wk = wk −wk,J k,n. Thus we have
m,h m,h m,h
max (cid:8) ϕ(s,a)⊤wk,J k,n−r (s,a)−P Vk (s,a)(cid:9) = max (cid:8) −ϕ(s,a)⊤∆wk (cid:9) .
m,h h h m,h+1 m,h
n∈[N] n∈[N]
Then by Cauchy-Schwarz inequality, we have
ϕ⊤∆wk = ϕ′⊤ ∆wk +∆ϕ⊤∆wk
m,h m,h m,h
≥ ϕ′⊤ ∆wk −∥∆ϕ∥·(cid:13) (cid:13)∆wk (cid:13) (cid:13)
m,h m,h
√
≥ ϕ′⊤ ∆wk − MKε(cid:13) (cid:13)∆wk (cid:13) (cid:13).
m,h m,h
By triangle inequality, with probability at least 1−δ, we have
√
(cid:13) (cid:13)∆w mk ,h(cid:13) (cid:13) ≤ (cid:13) (cid:13)w mk ,h(cid:13) (cid:13)+(cid:13) (cid:13)w mk,J ,hk,n(cid:13) (cid:13) ≤ 2H d+B δ/NMHK
√ √
Denote α = MK(cid:0) 2H d+B (cid:1). Then, for all (m,h,k) ∈ M×[H]×[K], with probability
δ δ/NMHK
at least 1−δ, we have
max (cid:8) ϕ⊤∆wk (cid:9) ≥ max (cid:8) ϕ′⊤ ∆wk (cid:9) −α ε.
m,h m,h δ
n∈[N] n∈[N]
Recall from (C.19), by taking union bound, with probability at least 1−|C(ε)|c′N −2δ, for all
0
(m,h,k) ∈ M×[H]×[K] and for all (s,a) ∈ S ×A, we have
max (cid:8) ϕ⊤∆wk (cid:9) ≥ −α ε.
m,h δ
n∈[N]
Finally, with probability at least 1−|C(ε)|c′N −2δ, for all (m,h,k) ∈ M×[H]×[K] and for all
0
(s,a) ∈ S ×A, we have
lk (s,a) ≤ α ε.
m,h δ
This completes the proof.
46C.9 Proof of Lemma B.12
Proof. For simplicity, we denote (sk ,ak ) as zk . Then we consider the following mappings
m,h m,h m,h
(ν ,ν ) : [MK] → [M]×[K],
M K
(cid:24) (cid:25)
τ
ν (τ) = τ(modM), ν = ,
M K
M
where we set ν (τ) = M if M|τ. Next, for any τ ≥ 0, we define
M
τM
Λ¯τ = λI+(cid:88) ϕ(cid:16) zνK(u) (cid:17) ϕ(cid:16) zνK(u) (cid:17)⊤ , for τ > 0,
h νM(u),h νM(u),h
u=1
Λ¯0 = λI, for τ = 0.
h
We denote σ = {σ ,...,σ } as the synchronization episodes, where σ ∈ [K], we also denote σ = 0.
1 n i 0
Then we separate the episodes k = 1,...,K into two groups based on the following condition,
det(Λ¯σi)
1 ≤ h ≤ 3. (C.20)
det(Λ¯σi−1)
h
Note that the left inequality always holds due to Λ¯σi−1 ≼ Λ¯σi and the trivial fact that A ≼ B ⇒
h h
det(A) ≤ det(B). Then we define that I = {k ∈ N+,k ∈ [σ ,σ ),∀i ∈ [n]|(C.20) is true} and
1 i−1 i
I = {k ∈ N+,k ∈ [σ ,σ ),∀i ∈ [n]|(C.20) is false}, then[K] = I ∪I ∪{K}. Foranyk ∈ [σ ,σ )
2 i−1 i 1 2 i−1 i
and k ∈ I , note that Λ¯σi−1 ≼ Λk ≼ Λ¯k ≼ Λ¯σi, thus for any m ∈ M, we have
1 h m,h h h
(cid:115)
(cid:13) (cid:13)ϕ(cid:0) z mk ,h(cid:1)(cid:13) (cid:13) (Λk )−1 ≤ (cid:13) (cid:13)ϕ(cid:0) z mk ,h(cid:1)(cid:13) (cid:13) (Λ¯k)−1 dd ete (t Λ(Λ¯ kk h) )
m,h h m,h
(cid:115)
≤ (cid:13) (cid:13)ϕ(cid:0) z mk ,h(cid:1)(cid:13) (cid:13) (Λ¯k)−1 dd ee t(t Λ( ¯Λ¯ σσ h i−i) 1)
h h
≤ 2(cid:13) (cid:13)ϕ(cid:0) z mk ,h(cid:1)(cid:13) (cid:13) (Λ¯k)−1, (C.21)
h
where the first inequality follows from Lemma H.12, the second inequality follows from the trivial
fact that A ≼ B ⇒ det(A) ≤ det(B), and the final inequality holds because k ∈ I . Then we will
1
bound the summation for k ∈ I and k ∈ I respectively.
1 2
(cid:115)
(cid:88) (cid:88) (cid:88) (cid:88)
∥ϕ(zk )∥ ≤ MK ∥ϕ(zk )∥2
m,h (Λk m,h)−1 m,h (Λk m,h)−1
k∈I1∪{K}m∈M m∈Mk∈I1∪{K}
(cid:115)
(cid:88) (cid:88)
≤ 2 MK ∥ϕ(zk )∥2
m,h (Λ¯k)−1
h
m∈Mk∈I1∪{K}
(cid:118)
(cid:117) K
(cid:117) (cid:88) (cid:88)
≤ 2(cid:116)MK ∥ϕ(zk )∥2
m,h (Λ¯k)−1
h
m∈Mk=1
(cid:115)
(cid:18) det(ΛK)(cid:19)
≤ 2 MKlog h ,
det(λI)
47where the first inequality follows from Cauchy-Schwarz inequality, the second inequality holds due to
(C.21),thefinalequalityfollowsfromLemmaH.1andΛK = (cid:80) (cid:80)K ϕ(cid:0) sk ,ak (cid:1) ϕ(cid:0) sk ,ak (cid:1)⊤ +
h m∈M k=1 m,h m,h m,h m,h
λI.
For any interval [σ ,σ ), define ∆ = σ −σ −1, we calculate that
i−1 i i i i−1
(cid:118)
σ (cid:88)i−1
(cid:13) (cid:13)ϕ(z mk ,h)(cid:13) (cid:13) (Λk )−1 ≤
(cid:117)
(cid:117) (cid:116)∆ i
σ (cid:88)i−1
(cid:13) (cid:13)ϕ(z mk ,h)(cid:13) (cid:13)2 (Λk )−1
m,h m,h
k=σi−1 k=σi−1
(cid:118)
(cid:117) (cid:32) det(Λσi−1)(cid:33)
(cid:117) m,h
≤ (cid:116)∆ log
i det(Λσi−1)
m,h
√
≤ γ,
where the last inequality follows from the synchronization condition (4.3).
Define R = (cid:108) log(cid:16) det(ΛK h)(cid:17)(cid:109) , note that σ ≤ K, then we can find that
h det(λI) n
(cid:18) det(Λ¯σn)(cid:19) (cid:88)n (cid:18) det(Λ¯σi) (cid:19)
R ≥ log h = log h .
h det(Λ¯σ0) det(Λ¯σi−1)
h i=1 h
We can claim that I has at most R synchronization episodes, otherwise
2 h
(cid:88)n (cid:18) det(Λ¯σi) (cid:19)
(cid:88)
(cid:18) det(Λ¯σi) (cid:19)
R ≥ log h ≥ log h ≥ R log3,
h det(Λ¯σi−1) det(Λ¯σi−1) h
i=1 h i∈{i|σi−1∈I2} h
which causes the contradiction. Thus I has at most R intervals, then we get
2 h
(cid:88) (cid:88) (cid:13) (cid:13)ϕ(z mk ,h)(cid:13) (cid:13)
(Λk )−1
≤ R hM√ γ ≤ (cid:18) log(cid:18) d de et t( (Λ λK h I))(cid:19) +1(cid:19) M√ γ.
m,h
k∈I2m∈M
Finally, we can bound the total summation,
K
(cid:88) (cid:88)(cid:13) (cid:13)ϕ(z mk ,h)(cid:13) (cid:13) (Λk )−1 ≤ (cid:88) (cid:88)(cid:13) (cid:13)ϕ(z mk ,h)(cid:13) (cid:13) (Λk )−1 + (cid:88) (cid:88) ∥ϕ(z mk ,h)∥ (Λk )−1
m,h m,h m,h
m∈Mk=1 m∈Mk∈I2 m∈Mk∈I1∪{K}
(cid:115)
(cid:18) (cid:18) det(ΛK)(cid:19) (cid:19)
√
(cid:18) det(ΛK)(cid:19)
≤ log h +1 M γ +2 MKlog h .
det(λI) det(λI)
This completes the proof.
D Proof of the Regret Bound for CoopTS-LMC in Misspecified
Setting
In this section, we prove the regret bound for CoopTS-LMC in the misspecified setting. The regret
analysis, the essential supporting lemmas and their corresponding proofs are almost same as what
we have presented in Appendix B and Appendix C. Here we mainly point out the differences of
proof between these two settings.
48D.1 Supporting Lemmas
Definition D.1 (Model prediction error). For any (m,k,h) ∈ M×[K]×[H], we define the model
error associated with the reward r ,
m,h
lk (s,a) = r (s,a)+P Vk (s,a)−Qk (s,a).
m,h m,h m,h m,h+1 m,h
Lemma D.2. Let λ = 1 in Algorithm 3. Under Definition 5.7, for any fixed 0 < δ < 1, with
probability at least 1−δ, for all (m,k,h) ∈ M×[K]×[H] and for any (s,a) ∈ S ×A, we have
(cid:12) (cid:12) √ √
(cid:12)ϕ(s,a)⊤wk −r (s,a)−P Vk (s,a)(cid:12) ≤ (cid:0) 5H dC +3Hζ MKd(cid:1) ∥ϕ(s,a)∥ +3Hζ,
(cid:12) (cid:98)m,h m,h m,h m,h+1 (cid:12) δ (Λk )−1
m,h
where C is defined in Lemma B.7.
δ
Proof of Lemma D.2. Recall from Definition 5.7, we have
(cid:12) (cid:12)P m,hV mk ,h+1(s,a)−ϕ(s,a)⊤(cid:10) µ h,V mk ,h+1(cid:11) S(cid:12) (cid:12) ≤ (cid:13) (cid:13)P m,h(· | s,a)−(cid:10) ϕ(s,a),µ h(·)(cid:11)(cid:13) (cid:13) 1∥V mk ,h+1∥ ∞
≤ 2H(cid:13) (cid:13)P m,h(· | s,a)−(cid:10) ϕ(s,a),µ h(·)(cid:11)(cid:13) (cid:13)
TV
≤ 2Hζ,
where the first inequality follows from Cauchy-Schwarz inequality, the second inequality follows from
the fact that ∥Vk ∥ ≤ H and P , ∥P −P ∥ = 1 (cid:80) |P (x)−P (x)| = 1∥P −P ∥ for two
m,h+1 ∞ 2 1 2 TV 2 x∈ω 1 2 2 1 2 1
distributionsP andP , notethathereweregarddistributionasinfinitedimensionalvector, thethird
1 2
inequality follows from Definition 5.7. Define ∆ = P Vk (s,a)−ϕ(s,a)⊤(cid:10) µ ,Vk (cid:11) ,
m,1 m,h m,h+1 h m,h+1 S
thus |∆ | ≤ 2Hζ. Then we have
m,1
P Vk (s,a) = ϕ(s,a)⊤(cid:10) µ ,Vk (cid:11) +∆
m,h m,h+1 h m,h+1 S m,1
(cid:32) (cid:33)
= ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1)⊤ +λI (cid:10) µ ,Vk (cid:11) +∆
m,h h m,h+1 S m,1
(sl,al,s′l)∈U (k)
m,h
(cid:32) (cid:33)
= ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1)⊤(cid:10) µ ,Vk (cid:11)
m,h h m,h+1 S
(sl,al,s′l)∈U (k)
m,h
+λϕ(s,a)⊤(cid:0) Λk (cid:1)−1(cid:10) µ ,Vk (cid:11) +∆
m,h h m,h+1 S m,1
(cid:32) (cid:33)
= ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:0)P Vk (cid:1)(cid:0) sl,al(cid:1)
m,h m,h m,h+1
(sl,al,s′l)∈U (k)
m,h
(cid:32) (cid:33)
−ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ∆ ϕ(cid:0) sl,al(cid:1)
m,h m,1
(sl,al,s′l)∈U (k)
m,h
+λϕ(s,a)⊤(cid:0) Λk (cid:1)−1(cid:10) µ ,Vk (cid:11) +∆ . (D.1)
m,h h m,h+1 S m,1
Based on (D.1), we can separate the following error into four parts,
ϕ(s,a)⊤wk −r (s,a)−P Vk (s,a)
(cid:98)m,h m,h m,h m,h+1
= ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) (cid:2) r (cid:0) sl,al(cid:1) +Vk (s′l )(cid:3) ϕ(cid:0) sl,al(cid:1) −r (s,a)
m,h m,h m,h+1 m,h
(sl,al,s′l)∈U (k)
m,h
49(cid:32) (cid:33)
−ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:0)P Vk (cid:1)(cid:0) sl,al(cid:1)
m,h m,h m,h+1
(sl,al,s′l)∈U (k)
m,h
(cid:32) (cid:33)
+∆ ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) −λϕ(s,a)⊤(cid:0) Λk (cid:1)−1(cid:10) µ ,Vk (cid:11) −∆
m,1 m,h m,h h m,h+1 S m,1
(sl,al,s′l)∈U (k)
m,h
(cid:32) (cid:33)
= ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:2)(cid:0) Vk −P Vk (cid:1)(cid:0) sl,al(cid:1)(cid:3)
m,h m,h+1 m,h m,h+1
(sl,al,s′l)∈U (k)
m,h
(cid:124) (cid:123)(cid:122) (cid:125)
(i)
(cid:32) (cid:33)
+ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) r (cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1) −r (s,a)
m,h m,h m,h
(sl,al,s′l)∈U (k)
m,h
(cid:124) (cid:123)(cid:122) (cid:125)
(ii)
−λϕ(s,a)⊤(cid:0) Λk (cid:1)−1(cid:10) µ ,Vk (cid:11)
m,h h m,h+1 S
(cid:124) (cid:123)(cid:122) (cid:125)
(iii)
(cid:32) (cid:33)
+∆ ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) −∆ (D.2)
m,1 m,h m,1
(sl,al,s′l)∈U (k)
m,h
(cid:124) (cid:123)(cid:122) (cid:125)
(iv)
We now provide an upper bound for each of the terms in (D.2).
Bounding Term (i) in (D.2): same as (C.11) in Appendix C.6, with probability at least 1−δ,
we have
√
|Term (i)| ≤ 3H dC ∥ϕ(s,a)∥ . (D.3)
δ (Λk )−1
m,h
Bounding Term (ii) + Term (iv) in (D.2): define ∆ = r (s,a)−ϕ(s,a)⊤θ , then we have
m,2 m,h h
|∆ | ≤ ζ due to Definition 5.7. Next we have
m,2
(cid:32) (cid:33)
ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) r (cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1) −r (s,a)
m,h m,h m,h
(sl,al,s′l)∈Um,h(k)
(cid:32) (cid:33)
=ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) r (cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1) −ϕ(s,a)⊤θ −∆
m,h m,h h m,2
(sl,al,s′l)∈Um,h(k)
(cid:32) (cid:33)
=ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) r (cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1) −Λk θ −∆
m,h m,h m,h h m,2
(sl,al,s′l)∈Um,h(k)
(cid:32) (cid:33)
=ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) r (cid:0) sl,al(cid:1) − (cid:88) ϕ(cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1)⊤ θ −λIθ −∆
m,h m,h h h m,2
(sl,al,s′l)∈Um,h(k) (sl,al,s′l)∈Um,h(k)
(cid:32) (cid:33)
=ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) ∆ −λIθ −∆
m,h m,2 h m,2
(sl,al,s′l)∈Um,h(k)
50(cid:32) (cid:33)
=−λϕ(s,a)⊤(cid:0) Λk (cid:1)−1 θ +∆ ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) −∆ , (D.4)
m,h h m,2 m,h m,2
(sl,al,s′l)∈Um,h(k)
(cid:124) (cid:123)(cid:122) (cid:125)
(v)
wherethethirdequalityusesthedefinitionofΛk . ByCombining(D.4)and(C.13)inAppendixC.6,
m,h
we obtain
√
|Term (ii)+Term (iv)| ≤ λd∥ϕ(s,a)∥ +|Term (iv)+Term (v)|. (D.5)
(Λk )−1
m,h
Then we calculate that
(cid:12) (cid:32) (cid:33) (cid:12)
|Term (iv)+Term (v)|=(cid:12) (cid:12)(∆ +∆ )ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) −(∆ +∆ )(cid:12) (cid:12)
(cid:12) m,1 m,2 m,h m,1 m,2 (cid:12)
(cid:12) (cid:12)
(sl,al,s′l)∈Um,h(k)
(cid:12) (cid:32) (cid:33)(cid:12)
≤|∆ +∆ |·(cid:12) (cid:12)ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) (cid:12) (cid:12)+|∆ +∆ |
m,1 m,2 (cid:12) m,h (cid:12) m,1 m,2
(cid:12) (cid:12)
(sl,al,s′l)∈Um,h(k)
≤3Hζ∥ϕ(s,a)∥ (Λk m,h)−1
(cid:88) (cid:13) (cid:13)ϕ(sl,al)(cid:13)
(cid:13) (Λk m,h)−1 +3Hζ
(sl,al,s′l)∈Um,h(k)
(cid:32) (cid:33)1
2
≤3Hζ∥ϕ(s,a)∥
(Λk m,h)−1
K(k)
(cid:88) (cid:13) (cid:13)ϕ(sl,al)(cid:13) (cid:13)2
(Λk m,h)−1
+3Hζ
√
(sl,al,s′l)∈Um,h(k)
≤3Hζ MKd∥ϕ(s,a)∥ +3Hζ, (D.6)
(Λk )−1
m,h
wherethesecondinequalityfollowsfromCauchy-Schwarzinequalityandthefactthat|∆ +∆ | ≤
m,1 m,2
|∆ |+|∆ | ≤ 2Hζ +ζ ≤ 3Hζ, the third inequality holds because of Cauchy-Schwarz inequality,
m,1 m,2
and the last inequality holds because K(k) ≤ MK and Lemma H.4. Substitute (D.6) into (D.5), we
have
√ √
|Term (ii)+Term (iv)| ≤ (cid:0) 3Hζ MKd+ λd(cid:1) ∥ϕ(s,a)∥ +3Hζ. (D.7)
(Λk )−1
m,h
Bounding Term (iii) in (D.2): same as (C.15) in Appendix C.6, we have
√
|Term (iii)| ≤ H λd∥ϕ(s,a)∥ . (D.8)
(Λk )−1
m,h
Combine all the terms in (D.2) together: by using triangle inequality in (D.2), we combine
(D.3), (D.7) and (D.8), then set λ = 1, with probability at least 1−δ, we get
(cid:12) (cid:12)
(cid:12)ϕ(s,a)⊤wk −r (s,a)−P Vk (s,a)(cid:12)
(cid:12) (cid:98)m,h m,h m,h m,h+1 (cid:12)
(cid:18) √ √ √ √ (cid:19)
≤ 3H dC + d+H d+3Hζ MKd ∥ϕ(s,a)∥ +3Hζ
δ (Λk )−1
m,h
√ √
(cid:0) (cid:1)
≤ 5H dC +3Hζ MKd ∥ϕ(s,a)∥ +3Hζ.
δ (Λk )−1
m,h
This completes the proof.
51Lemma D.3 (Errorbound). Letλ = 1inAlgorithm3. UnderDefinition5.7, foranyfixed0 < δ < 1,
with probability at least 1−δ−δ2, for any (m,k,h) ∈ M×[K]×[H] and for any (s,a) ∈ S ×A,
we have
(cid:32) √ √ (cid:115) 2dlog(cid:0)√ N/δ(cid:1) 4(cid:33)
−lk (s,a) ≤ 5H dC +3Hζ MKd+5 + ∥ϕ(s,a)∥ +3Hζ,
m,h δ 3β
K
3 (Λk m,h)−1
where C is defined in Lemma B.7.
δ
Proof of Lemma D.3. We do the same process as that in Appendix C.7, and we have
−l mk ,h(s,a) ≤ max (cid:12) (cid:12)ϕ(s,a)⊤w mk,J ,hk,n−ϕ(s,a)⊤w (cid:98)mk ,h(cid:12) (cid:12)+(cid:12) (cid:12)ϕ(s,a)⊤w (cid:98)mk ,h−r m,h(s,a)−P m,hV mk ,h+1(s,a)(cid:12) (cid:12).
n∈[N]
(cid:124) (cid:123)(cid:122) (cid:125)
(cid:124) (cid:123)(cid:122) (cid:125) (ii)
(i)
Bounding Term (i): based on (C.16), for any (m,h,k) ∈ M×[H]×[K] and (s,a) ∈ S ×A, with
probability at least 1−δ2, we have
(cid:32) (cid:115) (cid:0)√ (cid:1) (cid:33)
nm ∈a [Nx ](cid:12) (cid:12)ϕ(s,a)⊤w mk,J ,hk,n−ϕ(s,a)⊤w (cid:98)mk ,h(cid:12) (cid:12) ≤ 5 2dlog
3β
KN/δ + 34 ∥ϕ(s,a)∥ (Λk m,h)−1.,
Bounding Term (ii): based on Lemma D.2, for all (m,h,k) ∈ M×[H]×[K] and (s,a) ∈ S ×A,
we have
√ √
(cid:12) (cid:12)ϕ(s,a)⊤w (cid:98)mk ,h−r hk(s,a)−P hV mk ,h+1(s,a)(cid:12) (cid:12) ≤ (cid:0) 5H dC δ +3Hζ MKd(cid:1) ∥ϕ(s,a)∥ (Λk )−1 +3Hζ,
m,h
Combine the two result above, by taking union bound, with probability at least 1−δ−δ2, we have
(cid:32) √ √ (cid:115) 2dlog(cid:0)√ N/δ(cid:1) 4(cid:33)
−lk (s,a) ≤ 5H dC +3Hζ MKd+5 + ∥ϕ(s,a)∥ +3Hζ.
m,h δ 3β
K
3 (Λk m,h)−1
This completes the proof.
Lemma D.4 (Optimism). Let λ = 1 in Algorithm 3 and c′ = 1− √1 . Under Definition 5.7,
0 2 2eπ
for any fixed 0 < δ < 1, with probability at least 1−|C(ε)|c′N −2δ where |C(ε)| ≤ (3/ε)d, for all
0
(m,h,k) ∈ M×[H]×[K] and for all (s,a) ∈ S ×A, we have
lk (s,a) ≤ α ε+3Hζ,
m,h δ
√ √
where α = MK(cid:0) 2H d+B (cid:1).
δ δ/NMHK
Proof of Lemma D.4. This proof is similar to the proof in Appendix C.8, we just prove the part
that for fixed ϕ ∈ C(ε). Recall from Definition D.1,
lk (s,a) = r (s,a)+P Vk (s,a)−Qk (s,a).
m,h m,h m,h m,h+1 m,h
Note that
(cid:110) (cid:111)+
Qk (s,a) = min max ϕ(s,a)⊤wk,J k,n,H −h+1 ≤ max ϕ(x,a)⊤wk,J k,n.
m,h m,h m,h
n∈[N] n∈[N]
52Here we define
r (s,a)+P Vk (s,a)−ϕ(s,a)⊤µk,J k −(∆ +∆ )
m,h m,h m,h+1 m,h m,1 m,2
Z = ,
k (cid:113)
ϕ(s,a)⊤Σk,J kϕ(s,a)
m,h
where ∆ = P Vk (s,a)−ϕ(s,a)⊤(cid:10) µ ,Vk (cid:11) ,∆ = r (s,a)−ϕ(s,a)⊤θ . Based on
m,1 m,h m,h+1 h m,h+1 S m,2 m,h h
(cid:16) (cid:17)
theresultsinPropositionB.3,wehavethatϕ(s,a)⊤wk,J k,n ∼ N ϕ(s,a)⊤µk,J k,ϕ(s,a)⊤Σk,J kϕ(s,a) ,
m,h m,h m,h
for any fixed n ∈ [N]. When |Z | < 1, by Gaussian concentration Lemma H.10, we have
k
(cid:16) (cid:17)
P r (s,a)+P Vk (s,a)−ϕ(s,a)⊤wk,Jk,n ≤(∆ +∆ )
m,h m,h m,h+1 m,h m,1 m,2
(cid:16) (cid:17)
=P ϕ(s,a)⊤wk,Jk,n ≥r (s,a)+P Vk (s,a)−(∆ +∆ )
m,h m,h m,h m,h+1 m,1 m,2
(cid:32) ϕ(s,a)⊤wk,Jk,n−ϕ(s,a)⊤µk,Jk r (s,a)+P Vk (s,a)−(∆ +∆ )−ϕ(s,a)⊤µk,Jk(cid:33)
=P m,h m,h ≥ m,h m,h m,h+1 m,1 m,2 m,h
(cid:113) (cid:113)
ϕ(s,a)⊤Σk,Jkϕ(s,a) ϕ(s,a)⊤Σk,Jkϕ(s,a)
m,h m,h
(cid:32) ϕ(s,a)⊤wk,Jk,n−ϕ(s,a)⊤µk,Jk (cid:33)
=P m,h m,h ≥Z
(cid:113) k
ϕ(s,a)⊤Σk,Jkϕ(s,a)
m,h
1
≥ √ exp(−Z2/2)
k
2 2π
1
≥ √ .
2 2eπ
Consider the numerator of Z :
k
(cid:12) (cid:12)r m,h(s,a)+P m,hV mk ,h+1(s,a)−ϕ(s,a)⊤µk m,J ,hk −(∆ m,1+∆ m,2)(cid:12) (cid:12)
≤ (cid:12) (cid:12)r m,h(s,a)+P m,hV mk ,h+1(s,a)−ϕ(s,a)⊤w (cid:98)mk ,h−(∆ m,1+∆ m,2)(cid:12) (cid:12)+(cid:12) (cid:12)ϕ(s,a)⊤w (cid:98)mk ,h−ϕ(s,a)⊤µk m,J ,hk(cid:12) (cid:12).
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
I1 I2
(D.9)
Bounding Term I in (D.9): recall the proof of Lemma D.2, we do the almost same error
1
decomposition as (D.2) with the only difference of adding term (∆ +∆ )
m,1 m,2
ϕ(s,a)⊤wk −r (s,a)−P Vk (s,a)+(∆ +∆ )
(cid:98)m,h m,h m,h m,h+1 m,1 m,2
(cid:32) (cid:33)
= ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:2)(cid:0) Vk −P Vk (cid:1)(cid:0) sl,al(cid:1)(cid:3)
m,h m,h+1 m,h m,h+1
(sl,al,s′l)∈U (k)
m,h
(cid:124) (cid:123)(cid:122) (cid:125)
(i)
(cid:32) (cid:33)
+ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) r (cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1) −r (s,a)
m,h m,h m,h
(sl,al,s′l)∈U (k)
m,h
(cid:124) (cid:123)(cid:122) (cid:125)
(ii)
−λϕ(s,a)⊤(cid:0) Λk (cid:1)−1(cid:10) µ ,Vk (cid:11)
m,h h m,h+1 S
(cid:124) (cid:123)(cid:122) (cid:125)
(iii)
53(cid:32) (cid:33)
+∆ ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) +∆ (D.10)
m,1 m,h m,2
(sl,al,s′l)∈U (k)
m,h
(cid:124) (cid:123)(cid:122) (cid:125)
(iv)
We now provide an upper bound for each of the terms in (D.10).
Bounding Term (i) in (D.10): almost same as (C.11) in Appendix C.6 with the only difference
between P and P , with probability at least 1−δ, we have
h m,h
√
|Term (i)| ≤ 3H dC ∥ϕ(s,a)∥ . (D.11)
δ (Λk )−1
m,h
Bounding Term (ii) + Term (iv) in (D.10): we do the same calculation as that in the proof of
Lemma D.2, based on (D.4), we have
(cid:32) (cid:33)
Term (ii) = ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) r (cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1) −r (s,a)
m,h m,h m,h
(sl,al,s′l)∈U (k)
m,h
(cid:32) (cid:33)
= −λϕ(s,a)⊤(cid:0) Λk (cid:1)−1 θ +∆ ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) −∆ .
m,h h m,2 m,h m,2
(sl,al,s′l)∈U (k)
m,h
(cid:124) (cid:123)(cid:122) (cid:125)
(v)
(D.12)
By Combining (D.12) and (C.13) in Appendix C.6, we obtain
√
|Term (ii)+Term (iv)| ≤ λd∥ϕ(s,a)∥ +|Term (iv)+Term (v)|. (D.13)
(Λk )−1
m,h
Then we calculate that
(cid:12) (cid:32) (cid:33)(cid:12)
|Term (iv)+Term (v)| = |∆ +∆ |·(cid:12) (cid:12)ϕ(s,a)⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) (cid:12) (cid:12)
m,1 m,2 (cid:12) m,h (cid:12)
(cid:12) (cid:12)
(sl,al,s′l)∈U (k)
m,h
≤ 3Hζ∥ϕ(s,a)∥ (Λk )−1
(cid:88) (cid:13) (cid:13)ϕ(sl,al)(cid:13)
(cid:13) (Λk )−1
m,h m,h
(sl,al,s′l)∈U (k)
m,h
(cid:32) (cid:33)1
≤ 3Hζ∥ϕ(s,a)∥
(Λk )−1
K(k)
(cid:88) (cid:13) (cid:13)ϕ(sl,al)(cid:13) (cid:13)2
(Λk )−1
2
m,h m,h
(sl,al,s′l)∈U (k)
m,h
√
≤ 3Hζ MKd∥ϕ(s,a)∥ , (D.14)
(Λk )−1
m,h
where the first inequality follows from Cauchy-Schwarz inequality and the fact that |∆ +∆ | ≤
m,1 m,2
3Hζ, the second inequality holds because of Cauchy-Schwarz inequality, and the last inequality holds
because K(k) ≤ MK and Lemma H.4. Substitute (D.14) into (D.13), we have
√ √
|Term (ii)+Term (iv)| ≤ (cid:0) 3Hζ MKd+ λd(cid:1) ∥ϕ(s,a)∥ . (D.15)
(Λk )−1
m,h
Bounding Term (iii) in (D.10): same as (C.15) in Appendix C.6, we have
√
|Term (iii)| ≤ H λd∥ϕ(s,a)∥ . (D.16)
(Λk )−1
m,h
54Combine all the terms in (D.10) together: by using triangle inequality in (D.10), we combine
(D.11), (D.15) and (D.16), then set λ = 1, with probability at least 1−δ, we get
(cid:12) (cid:12)
(cid:12)ϕ(s,a)⊤wk −r (s,a)−P Vk (s,a)+(∆ +∆ )(cid:12)
(cid:12) (cid:98)m,h m,h m,h m,h+1 m,1 m,2 (cid:12)
√ √
(cid:0) (cid:1)
≤ 5H dC +3Hζ MKd ∥ϕ(s,a)∥ .
δ (Λk )−1
m,h
Bounding Term I in (D.9): same as the proof in Appendix C.8, we have
2
(cid:12) (cid:12)ϕ(s,a)⊤w (cid:98)mk ,h−ϕ(s,a)⊤µk m,J ,hk(cid:12) (cid:12) ≤ 4 3∥ϕ(s,a)∥ (Λk m,h)−1.
So, with probability at least 1−δ, we have
(cid:12) (cid:12)r m,h(s,a)+P m,hV mk ,h+1(s,a)−ϕ(s,a)⊤µk m,J ,hk(cid:12) (cid:12) ≤
(cid:18) 5H√
dC δ
+3Hζ√
MKd+
34(cid:19)
∥ϕ(s,a)∥ (Λk m,h)−1.
(D.17)
Consider the denominator of Z : same as the proof in Appendix C.8, with (C.18), we have
k
1
∥ϕ(s,a)∥ ≥ √ ∥ϕ(s,a)∥ , (D.18)
Σk m,J ,hk 4 β
K
(Λk m,h)−1
√
where we used the fact that λ (cid:0)(cid:0) Λk (cid:1)−1(cid:1) ≥ 1/k and ∥ϕ(s,a)∥ ≥ 1/ k∥ϕ(s,a)∥ . There-
min m,h (Λk )−1 2
m,h
fore, according to (D.17) and (D.18), with probability at least 1−δ, it holds that
|Z | =
(cid:12) (cid:12) (cid:12)r m,h(s,a)+P m,hV mk ,h+1(s,a)−ϕ(s,a)⊤µk m,J ,hk(cid:12) (cid:12)
(cid:12)
k (cid:12) (cid:113) (cid:12)
(cid:12) ϕ(s,a)⊤Σk,J kϕ(s,a) (cid:12)
m,h
(cid:16) √ √ (cid:17)
5H dC +3Hζ MKd+ 4 ∥ϕ(s,a)∥
δ 3 (Λk )−1
≤ m,h
√1 ∥ϕ(s,a)∥
√ 4 β √K (Λk m,h)−1
5H dC +3Hζ MKd+ 4
= δ 3,
√1
4 βK
√ √
which implies |Z | < 1 when √1 = 20H dC +12Hζ MKd+ 16.
k βK δ 3
Now we have already proved that, for any fixed n ∈ [N], with probability at least 1−δ, we have
(cid:16) (cid:17) 1
P r (s,a)+P Vk (s,a)−ϕ(s,a)⊤wk,J k,n ≤ (∆ +∆ ) ≥ √ .
m,h m,h m,h+1 m,h m,1 m,2 2 2eπ
By taking union bound over n ∈ [N], with probablity at least 1−δ, we have
(cid:16) (cid:17)
P max (cid:8) ϕ(s,a)⊤wk,J k,n−r (s,a)−P Vk (s,a)(cid:9) ≥ −(∆ +∆ )
m,h m,h m,h m,h+1 m,1 m,2
n∈[N]
(cid:16) 1 (cid:17)N
≥ 1− 1− √
2 2eπ
=
1−c′N
,
0
55where c′ = 1− √1 . Finally, with probability at least (1−δ)(cid:0) 1−c′N(cid:1), for all (s,a) ∈ S ×A, we
0 2 2eπ 0
have
lk (s,a) ≤ 3Hζ.
m,h
Till now we have completed the proof of fixed ϕ ∈ C(ε). Follow the proof in Appendix C.8, we can
get the final result.
D.2 Regret Analysis
In this part, we give out the proof of Theorem 5.11, the regret bound for CoopTS-LMC in the
misspecified setting.
Proof of Theorem 5.11. This proof is almost same as the proof in Appendix B.2. We do the same
regret decomposition (B.2) and obtain the same bound for Term(i) (B.3) and Term(ii) (B.4). Next
we bound Term (iii) with new lemmas in the misspecified setting.
Bounding Term (iii) in (B.2): based on Lemma D.3 and Lemma D.4, by taking union bound,
with probability at least 1−|C(ε)|c′N −2δ′−MHK(δ′+δ′2), we have
0
K H
(cid:88) (cid:88)(cid:88)(cid:0)E (cid:2) lk (s ,a )|s = sk (cid:3) −lk (cid:0) sk ,ak (cid:1)(cid:1)
π∗ m,h m,h m,h m,1 m,1 m,h m,h m,h
m∈Mk=1h=1
K H
≤ (cid:88) (cid:88)(cid:88)(cid:16) −lk (cid:0) sk ,ak (cid:1) +α ε+3Hζ(cid:17)
m,h m,h m,h δ′
m∈Mk=1h=1
≤ (cid:88) (cid:88)K (cid:88)H (cid:32)(cid:32) 5H√ dC δ′ +3Hζ√ MKd+5(cid:115) 2dlog 3(cid:0) β√ N/δ′(cid:1) + 4 3(cid:33) (cid:13) (cid:13)ϕ(sk m,h,ak m,h)(cid:13) (cid:13) (Λk )−1
K m,h
m∈Mk=1h=1
(cid:33)
+α ε+6Hζ
δ′
(cid:32) √ √ (cid:115) 2dlog(cid:0)√ N/δ′(cid:1) 4(cid:33)
= HMKα ε+6H2MKζ + 5H dC +3Hζ MKd+5 +
δ′ δ′
3β 3
K
H K
×(cid:88) (cid:88) (cid:88)(cid:13) (cid:13)ϕ(sk ,ak )(cid:13)
(cid:13)
m,h m,h (Λk )−1
m,h
h=1m∈Mk=1
(cid:32) √ √ (cid:115) 2dlog(cid:0)√ N/δ′(cid:1) 4(cid:33)
≤ HMKα ε+6H2MKζ + 5H dC +3Hζ MKd+5 +
δ′ δ′
3β 3
K
(cid:115)
(cid:88)H (cid:18) (cid:18) det(ΛK)(cid:19) (cid:19)
√
(cid:18) det(ΛK)(cid:19)
× log h +1 M γ +2 MKlog h
det(λI) det(λI)
h=1
(cid:32) √ √ (cid:115) 2dlog(cid:0)√ N/δ′(cid:1) 4(cid:33)
≤ HMKα ε+6H2MKζ + 5H dC +3Hζ MKd+5 +
δ′ δ′
3β 3
K
(cid:16) √ (cid:112) (cid:17)
×H d(log(1+MK/d)+1)M γ +2 MKdlog(1+MK/d)
56= O(cid:101)(cid:16) d3 2H2√ M(cid:0)(cid:112) dMγ +√ K(cid:1) +d3 2H2M√ K(cid:0)(cid:112) dMγ +√ K(cid:1) ζ(cid:17) . (D.19)
The first inequality follows from Lemma D.4, the second inequality follows from Lemma D.3, the
third inequality follows from Lemma B.12, the last inequality holds due to Lemma H.2 and the fact
√ √
t dh ea fit ne∥ϕ in(· L)∥ e2 m≤ m1 a, Dth .4e .last equality follows from √ β1
K
= 20H dC δ′ +12Hζ MKd+ 1 36, which we
The probability calculation is same as that in Appendix B.2. By combining Terms (i)(ii)(iii)
together, we get that the final regret bound for CoopTS-LMC in misspecified setting is
O(cid:101)(cid:16) d3 2H2√ M(cid:0)(cid:112)
dMγ
+√ K(cid:1) +d3 2H2M√ K(cid:0)(cid:112)
dMγ
+√ K(cid:1) ζ(cid:17)
,
with probability at least 1−δ. Here we finish the proof.
E Proof of the Regret Bound for CoopTS-PHE
Before getting the regret bound for CoopTS-PHE, we first present some essential technical lemmas
required for our analysis.
E.1 Supporting Lemmas
Proposition E.1. Thedifferencebetweentheperturbedestimatedparameterwk,n andunperturbed
(cid:101)m,h
estimated parameter wk satisfies the Gaussian distribution,
(cid:98)m,h
(cid:16) (cid:17)
ζk,n = wk,n −wk ∼ N 0,σ2(cid:0) Λk (cid:1)−1 ,
m,h (cid:101)m,h (cid:98)m,h m,h
(cid:16) (cid:17)
where wk = (cid:0) Λk (cid:1)−1 (cid:80) (cid:2) r + Vk (cid:0) s′l(cid:1)(cid:3) ϕ(cid:0) sl,al(cid:1) is the unperturbed esti-
(cid:98)m,h m,h (sl,al,s′l)∈U (k) h m,h+1
m,h
mated parameter.
Next we will define some good events that hold with high probability to help prove the critical
lemmas in this section.
Lemma E.2 (Good events). For any fixed 0 < δ < 1, with some constant c > 0, we define the
following random events
G mk ,h(ζ,δ) d =ef (cid:110) max (cid:13) (cid:13)ζ mk, ,n h(cid:13) (cid:13)
Λk
≤ c 1σ√ d(cid:111) ,
n∈[N] m,h
G(M,K,H,δ) d =ef (cid:92) (cid:92) (cid:92) Gk (ζ,δ),
m,h
m∈Mk≤Kh≤H
where c = c(cid:112) log(dNMKH/δ). Then the event G(M,K,H,δ) occurs with probability at least
1
1−δ.
Lemma E.3. Let λ = 1 in Algorithm 2. For any fixed 0 < δ < 1, conditioned on the event
G(M,K,H,δ), with probability 1−δ, for all (m,k,h) ∈ M×[K]×[H], we have
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:2)(cid:0) Vk −P Vk (cid:1)(cid:0) sl,al(cid:1)(cid:3)(cid:13) (cid:13) ≤ 3H√ dD ,
(cid:13) m,h+1 h m,h+1 (cid:13) δ
(cid:13) (cid:13)
(sl,al,s′l)∈U (k) (Λk )−1
m,h m,h
(cid:104) (cid:16) √ √ √ (cid:17) (cid:105)1/2
where we define D = 1 log(K +1)+log 6 2K(2H MKd+c1σ d) +log 1 .
δ 2 H δ
57Lemma E.4. Let λ = 1 in Algorithm 2. Under Definition 4.1, for any fixed 0 < δ < 1, conditioned
on the event G(M,K,H,δ), with probability 1−δ, for all (m,k,h) ∈ M×[K]×[H] and for any
(s,a) ∈ S ×A, we have
(cid:12) (cid:12) √
(cid:12)ϕ(s,a)⊤wk −r (s,a)−P Vk (s,a)(cid:12) ≤ 5H dD ∥ϕ(s,a)∥ . (E.1)
(cid:12) (cid:98)m,h h h m,h+1 (cid:12) δ (Λk )−1
m,h
Lemma E.5 (Optimism). Let λ = 1 in Algorithm 2 and set c = Φ(1). Under Definition 4.1,
0
conditionedontheeventG(M,K,H,δ),withprobabilityatleast1−|C(ε)|cN−δwhere|C(ε)| ≤ (3/ε)d,
0
for all (m,k,h) ∈ M×[K]×[H] and for all (s,a) ∈ S ×A, we have
lk (s,a) ≤ A ε,
m,h δ
√ √
where A
δ
= c 1σ d+5H dD
δ
= O(cid:101)(Hd).
Lemma E.6 (Error bound). Let λ = 1 in Algorithm 2. Under Definition 4.1, for any fixed 0 < δ < 1,
conditioned on the event G(M,K,H,δ), with probability 1−δ, for all (m,k,h) ∈ M×[K]×[H]
and for any (s,a) ∈ S ×A, we have
−lk (s,a) ≤ c Hd∥ϕ(s,a)∥ ,
m,h 2 (Λk )−1
m,h
where c
2
= O(cid:101)(1).
E.2 Regret Analysis
In this part, we give out the proof of Theorem 5.1, the regret bound for CoopTS-PHE.
Proof of Theorem 5.1. Based on the result from Lemma B.13, we do the regret decomposition first
K
Regret(K) = (cid:88) (cid:88) V∗ (cid:0) sk (cid:1) −Vπ m,k(cid:0) sk (cid:1)
m,1 m,1 m,1 m,1
m∈Mk=1
K H
= (cid:88) (cid:88)(cid:88) E (cid:2)(cid:10) Qk (s ,·),π∗ (·,|s )−πk (·|s )(cid:11) |s = sk (cid:3)
π∗ m,h m,h m,h m,h m,h m,h m,1 m,1
m∈Mk=1h=1
(cid:124) (cid:123)(cid:122) (cid:125)
(i)
K H
(cid:88) (cid:88)(cid:88)
+ (D +D )
m,k,h,1 m,k,h,2
m∈Mk=1h=1
(cid:124) (cid:123)(cid:122) (cid:125)
(ii)
K H
+ (cid:88) (cid:88)(cid:88)(cid:0)E (cid:2) lk (s ,a )|s = sk (cid:3) −lk (cid:0) sk ,ak (cid:1)(cid:1) . (E.2)
π∗ m,h m,h m,h m,1 m,1 m,h m,h m,h
m∈Mk=1h=1
(cid:124) (cid:123)(cid:122) (cid:125)
(iii)
Next, we will bound the above three terms respectively.
Bounding Term (i) in (E.2): for the policy πk , we have
m,h
K H
(cid:88) (cid:88)(cid:88) E (cid:2)(cid:10) Qk (s ,·),π∗ (·,|s )−πk (·|s )(cid:11) |s = sk (cid:3) ≤ 0. (E.3)
π∗ m,h m,h m,h m,h m,h m,h m,1 m,1
m∈Mk=1h=1
58This is because by definition πk is the greedy policy for Qk .
m,h m,h
Bounding Term (ii) in (E.2): note that 0 ≤ Qk ≤ H −h+1 ≤ H, based on (B.1), for any
m,h
(m,k,h) ∈ M×[K]×[H], we have |D | ≤ 2H and |D | ≤ 2H. Note that D is a
m,k,h,1 m,k,h,2 m,k,h,1
martingale difference sequence E[D |F ] = 0. By applying Azuma-Hoeffding inequality,
m,k,h,1 m,k,h
with probability at least 1−δ/3, we have
K H
(cid:88) (cid:88)(cid:88) (cid:112)
D ≤ 2 2MH3Klog(6/δ).
m,k,h,1
m∈Mk=1h=1
Note that D is also a martingale difference sequence. By applying Azuma-Hoeffding inequality,
m,k,h,2
with probability at least 1−δ/3, we have
K H
(cid:88) (cid:88)(cid:88) (cid:112)
D ≤ 2 2MH3Klog(6/δ).
m,k,h,2
m∈Mk=1h=1
By taking union bound, with probability at least 1−2δ/3, we have
K H K H
(cid:88) (cid:88)(cid:88) (cid:88) (cid:88)(cid:88) (cid:112)
D + D ≤ 4 2MH3Klog(6/δ). (E.4)
m,k,h,1 m,k,h,2
m∈Mk=1h=1 m∈Mk=1h=1
Bounding Term (iii) in (E.2): conditioned on the event G(M,K,H,δ′), based on Lemma E.6 and
Lemma E.5, by taking union bound, with probability at least 1−|C(ε)|cN −δ′−MHKδ′, we have
0
K H
(cid:88) (cid:88)(cid:88)(cid:0)E (cid:2) lk (s ,a )|s = sk (cid:3) −lk (cid:0) sk ,ak (cid:1)(cid:1)
π∗ m,h m,h m,h m,1 m,1 m,h m,h m,h
m∈Mk=1h=1
K H
≤ (cid:88) (cid:88)(cid:88)(cid:0) A ε−lk (cid:0) sk ,ak (cid:1)(cid:1)
δ′ m,h m,h m,h
m∈Mk=1h=1
K H
≤ HMKA δ′ε+ (cid:88) (cid:88)(cid:88) c 2dH(cid:13) (cid:13)ϕ(sk m,h,ak m,h)(cid:13) (cid:13)
(Λk )−1
m,h
m∈Mk=1h=1
(cid:115)
(cid:88)H (cid:18) (cid:18) det(ΛK)(cid:19) (cid:19)
√
(cid:18) det(ΛK)(cid:19)
≤ HMKA ε+c dH log h +1 M γ +2 MKlog h
δ′ 2
det(λI) det(λI)
h=1
(cid:16) √ (cid:112) (cid:17)
≤ HMKA ε+c dH ·H d(log(1+MK/d)+1)M γ +2 MKdlog(1+MK/d) .
δ′ 2
The first inequality follows from Lemma E.5, the second inequality holds due to Lemma E.6, the
third inequality follows from Lemma B.12, the last inequality holds due to Lemma H.2 and the fact
that ∥ϕ(·)∥ ≤ 1.
2
Here we choose ε = dH(cid:112) d/MK/A
δ′
= O(cid:101)((cid:112) d/MK). Conditioned on the event G(M,K,H,δ′),
we have
(cid:88) (cid:88)K (cid:88)H (cid:0)E π∗(cid:2) l mk ,h(s m,h,a m,h)|s
m,1
= sk m,1(cid:3) −l mk ,h(cid:0) sk m,h,ak m,h(cid:1)(cid:1) ≤ O(cid:101)(cid:0) dH2(cid:0) dM√ γ +√ dMK(cid:1)(cid:1) ,
m∈Mk=1h=1
(E.5)
59with probability at least 1−|C(ε)|cN −δ′−MHKδ′. Based on Lemma E.2, the event G(M,K,H,δ′)
0
occurs with probability at least 1−δ′. Therefore, (E.5) occurs with probability at least
(cid:0) 1−δ′(cid:1)(cid:0) 1−|C(ε)|cN −δ′−MHKδ′(cid:1) .
0
We set δ′ = δ/6(MHK +2) and choose N = C(cid:101)log(δ)/log(c 0) where C(cid:101) = O(cid:101)(d), then we have
(cid:0) 1−δ′(cid:1)(cid:0) 1−|C(ε)|cN −δ′−MHKδ′(cid:1) ≥ 1−δ/3.
0
Combining Terms (i)(ii)(iii) together: Based on (E.3), (E.4) and (E.5). By taking union bound,
we get that the final regret bound for CoopTS-PHE is O(cid:101)(cid:0) dH2(cid:0) dM√ γ +√ dMK(cid:1)(cid:1) with probability
at least 1−δ.
F Proof of Supporting Lemmas in Appendix E
F.1 Proof of Proposition E.1
Proof. Based on (4.5), we can calculate that
(cid:32) (cid:33)
wk,n = (cid:0) Λk (cid:1)−1 (cid:88) (cid:2)(cid:0) r (cid:0) sl,al(cid:1) +ϵk,l,n(cid:1) +Vk (cid:0) s′l(cid:1)(cid:3) ϕ(cid:0) sl,al(cid:1) −λξk,n
(cid:101)m,h m,h h h m,h+1 h
(sl,al,s′l)∈U (k)
m,h
(cid:32) (cid:33)
= wk +(cid:0) Λk (cid:1)−1 (cid:88) ϵk,l,nϕ(cid:0) sl,al(cid:1) −λξk,n , (F.1)
(cid:98)m,h m,h h h
(sl,al,s′l)∈U (k)
m,h
(cid:16) (cid:17)
where wk = (cid:0) Λk (cid:1)−1 (cid:80) (cid:2) r + Vk (cid:0) s′l(cid:1)(cid:3) ϕ(cid:0) sl,al(cid:1) is the unperturbed esti-
(cid:98)m,h m,h (sl,al,s′l)∈U (k) h m,h+1
m,h
mated parameter. Since ϵk,l,n ∼ N(0,σ2), for l ∈ [K(k)], based on the property of Gaussian
h
distribution, we have
(cid:16) (cid:17)
ϵk,l,nϕ(cid:0) sl,al(cid:1)
∼ N
0,σ2ϕ(cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1)⊤
,
h
Since ξk,n ∼ N(0,σ2I), we can calculate the covariance matrix of the second term in (F.1),
h
(cid:32) (cid:33)
(cid:0) Λk (cid:1)−1 Cov (cid:88) ϵk,l,nϕ(cid:0) sl,al(cid:1) −λξk,n (cid:0) Λk (cid:1)−1
m,h h h m,h
(sl,al,s′l)∈U (k)
m,h
(cid:32) (cid:33)
= (cid:0) Λk (cid:1)−1 σ2 (cid:88) ϕ(cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1)⊤ +λI (cid:0) Λk (cid:1)−1
m,h m,h
(sl,al,s′l)∈U (k)
m,h
= σ2(cid:0) Λk (cid:1)−1 Λk (cid:0) Λk (cid:1)−1
m,h m,h m,h
= σ2(cid:0) Λk (cid:1)−1 .
m,h
It is obvious that the mean of the second term in (F.1) is 0. Thus, we have
(cid:16) (cid:17)
ζk,n = wk,n −wk ∼ N 0,σ2(cid:0) Λk (cid:1)−1 .
m,h (cid:101)m,h (cid:98)m,h m,h
This completes the proof.
60F.2 Proof of Lemma E.2
Proof. Recall that in Proposition E.1, we have
(cid:16) (cid:17)
(cid:8) ζk,n(cid:9) ∼ N 0,σ2(cid:0) Λk (cid:1)−1 .
m,h m,h
By Lemma H.10, for fixed n ∈ [N], with probability at least 1−δ, we have
(cid:13) (cid:13)ζk,n(cid:13)
(cid:13) ≤
c(cid:112)
dσ2log(d/δ).
m,h Λk
m,h
By applying union bound over N samples, we have
P(cid:16)
max
(cid:13) (cid:13)ζk,n(cid:13)
(cid:13) ≤
c(cid:112) dσ2log(d/δ)(cid:17)
≥ 1−Nδ.
m,h Λk
n∈[N] m,h
Now we define c = c(cid:112) log(dNMKH/δ), and we define the event
1
G mk ,h(ζ,δ) d =ef (cid:110) max (cid:13) (cid:13)ζ mk, ,n h(cid:13) (cid:13)
Λk
≤ c 1σ√ d(cid:111) .
n∈[N] m,h
Thus for any fixed m, h and k, the event Gk (ζ,δ) occurs with a probability of at least 1−δ/MKH.
m,h
By taking union bound over all (m,h,k) ∈ M×[H]×[K], we have
(cid:32) (cid:33)
P(cid:0) G(M,K,H,δ)(cid:1) = P (cid:92) (cid:92) (cid:92) Gk (ζ,δ) ≥ 1−δ.
m,h
m∈Mk≤Kh≤H
This completes the proof.
F.3 Proof of Lemma E.3
Proof. Based on the result in Lemma B.4, for any (m,h,k) ∈ M×[H]×[K], we have
(cid:13) (cid:13)w (cid:98)mk ,h(cid:13) (cid:13) ≤ 2H(cid:112) Mkd/λ.
By recalling the construction of Λk , it is trivial to find that λ (cid:0) Λk (cid:1) ≥ λ. Conditioned on the
m,h min m,h
event G(M,K,H,δ), we have
√ √
λ(cid:13) (cid:13)ζ mk, ,n h(cid:13) (cid:13) ≤ (cid:13) (cid:13)ζ mk, ,n h(cid:13) (cid:13)
Λk
≤ c 1σ d.
m,h
Then by triangle inequality, for all n ∈ [N], we obtain the upper bound
(cid:13) (cid:13)w (cid:101)mk,n ,h(cid:13) (cid:13) = (cid:13) (cid:13)w (cid:98)mk ,h+ζ mk, ,n h(cid:13) (cid:13) ≤ 2H(cid:112) Mkd/λ+c 1σ(cid:112) d/λ.
Based on the result from Lemma H.7 and Lemma H.9, we have that, for any ε > 0, and for all
(m,k,h) ∈ M×[K]×[H], with probability at least 1−δ, we have
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:2)(cid:0) Vk −P Vk (cid:1)(cid:0) sl,al(cid:1)(cid:3)(cid:13) (cid:13)
(cid:13) m,h+1 h m,h+1 (cid:13)
(cid:13) (cid:13)
(sl,al,s′l)∈U (k) (Λk )−1
m,h m,h
61(cid:32) (cid:34) d (cid:18) k+λ(cid:19) (cid:32) 3(cid:0) 2H(cid:112) Mkd/λ+c σ(cid:112) d/λ(cid:1)(cid:33) 1(cid:35) 8k2ε2(cid:33)1/2
≤ 4H2 log +dlog 1 +log +
2 λ ε δ λ
(cid:34) d (cid:18) k+λ(cid:19) (cid:32) 3(cid:0) 2H(cid:112) Mkd/λ+c σ(cid:112) d/λ(cid:1)(cid:33) 1(cid:35)1/2 2√ 2kε
1
≤ 2H log +dlog +log + √ .
2 λ ε δ λ
Here we set λ = 1,ε = √H , with probability at least 1−δ, we have
2 2k
(cid:13) (cid:13)
(cid:13) (cid:13) (cid:88) ϕ(cid:0) sl,al(cid:1)(cid:2)(cid:0) Vk −P Vk (cid:1)(cid:0) sl,al(cid:1)(cid:3)(cid:13) (cid:13)
(cid:13) m,h+1 h m,h+1 (cid:13)
(cid:13) (cid:13)
(sl,al,s′l)∈U (k) (Λk )−1
m,h m,h
√ (cid:34) 1 (cid:32) 6√ 2K(cid:0) 2H√ MKd+c σ√ d(cid:1)(cid:33) 1(cid:35)1/2
1
≤ 2H d log(K +1)+log +log +H
2 H δ
√
≤ 3H dD ,
δ
(cid:104) (cid:16) √ √ √ (cid:17) (cid:105)1/2
where we define D = 1 log(K +1)+log 6 2K(2H MKd+c1σ d) +log 1 . Here we finish the
δ 2 H δ
proof.
F.4 Proof of Lemma E.4
Proof. This proof is almost same as the proof of Lemma B.8 in Appendix C.6. The only difference
is the Term (i) in (C.10). Here based on Lemma E.6, conditioned on the event G(M,K,H,δ), with
probability 1−δ, we have
√
Term (i) ≤ 3H dD ∥ϕ(s,a)∥ .
δ (Λk )−1
m,h
Finally,conditionedontheeventG(M,K,H,δ),withprobability1−δ,forall(m,k,h) ∈ M×[K]×[H]
and for any (s,a) ∈ S ×A, we have
(cid:12) (cid:12) √ √ √
(cid:12)ϕ(s,a)⊤wk −r (s,a)−P Vk (s,a)(cid:12) ≤ (cid:0) 3H dD +H d+ d(cid:1) ∥ϕ(s,a)∥
(cid:12) (cid:98)m,h h h m,h+1 (cid:12) δ (Λk )−1
√ m,h
≤ 5H dD ∥ϕ(s,a)∥ .
δ (Λk )−1
m,k
Here we finish the proof.
F.5 Proof of Lemma E.5
Proof. Recall from Definition 4.1, we have
r (s,a)+P Vk (s,a) = ϕ(s,a)⊤θ +ϕ(s,a)⊤(cid:10) µ ,Vk (cid:11) d =ef ϕ(s,a)⊤wk ,
h h m,h+1 h h m,h+1 S m,h
√
where wk = θ +(cid:10) µ ,Vk (cid:11) . Note that max{∥µ (S)∥,∥θ ∥} ≤ d and Vk ≤ H−h ≤ H,
m,h h h m,h+1 S h h m,h+1
thus we have
(cid:13) (cid:13)w mk ,h(cid:13) (cid:13) ≤ ∥θ h∥+(cid:13) (cid:13)(cid:10) µ h,V mk ,h+1(cid:11) S(cid:13) (cid:13)
62√ √
≤ d+H d
√
≤ 2H d.
Then we define the regression error ∆wk = wk −wk . For any (m,h,k) ∈ M×[H]×[K] and
m,h m,h (cid:98)m,h
any (s,a) ∈ S ×A, we have
lk (s,a) = r (s,a)+P Vk (s,a)−Qk (s,a)
m,h h h m,h+1 m,h
(cid:110) (cid:16) (cid:17)(cid:111)+
= r (s,a)+P Vk (s,a)−min H −h+1, max ϕ(s,a)⊤ wk +ζk,n
h h m,h+1 (cid:98)m,h m,h
n∈[N]
(cid:110) (cid:16) (cid:17)(cid:111)
≤ max ϕ(s,a)⊤wk −(H −h+1),ϕ(s,a)⊤wk − max ϕ(s,a)⊤ wk +ζk,n
m,h m,h (cid:98)m,h m,h
n∈[N]
(cid:110) (cid:111)
≤ max 0,ϕ(s,a)⊤∆wk − max ϕ(s,a)⊤ζk,n , (F.2)
m,h m,h
n∈[N]
where the last inequality holds because |r | ≤ 1 and Vk ≤ H − h, this indicates r (s,a) +
h m,h+1 h
P Vk (s,a) = ϕ(s,a)⊤wk ≤ H −h+1. Note that ∥ϕ(s,a)∥ ≤ (cid:112) 1/λ∥ϕ(s,a)∥ ≤ 1 for
h m,h+1 m,h (Λk )−1
m,h
all ϕ(s,a). Define C(ε) to be a ε-cover of (cid:8) ϕ | ∥ϕ∥ ≤ 1(cid:9). Based on Lemma H.8, we have
(Λk )−1
m,h
|C(ε)| ≤ (3/ε)d.
First, for any fixed ϕ(s,a) ∈ C(ε), we have
(cid:16) (cid:17)
(cid:8) ϕ⊤ζk,n(cid:9) ∼ N 0,σ2∥ϕ∥2 .
m,h (Λk )−1
m,h
Use the property of Gaussian distribution, we obtain
(cid:16) (cid:17)
P ϕ⊤ζk,n −σ∥ϕ∥ ≥ 0 = Φ(−1).
m,h (Λk )−1
m,h
By taking union bound over n ∈ [N], we obtain
(cid:16) (cid:17)
P max (cid:8) ϕ⊤ζk,n −σ∥ϕ∥ (cid:9) ≥ 0 ≥ 1−(1−Φ(−1))N = 1−Φ(1)N = 1−cN.
m,h (Λk )−1 0
n∈[N] m,h
By applying union bound over C(ε), with probability 1−|C(ε)|cN, for all ϕ ∈ C(ε), we have
0
max (cid:8) ϕ⊤ζk,n −σ∥ϕ∥ (cid:9) ≥ 0. (F.3)
m,h (Λk )−1
n∈[N] m,h
Then,foranyϕ = ϕ(s,a),wecanfindϕ′ ∈ C(ε)suchthat∥ϕ−ϕ′∥ ≤ ε. Define∆ϕ = ϕ−ϕ′,
(Λk )−1
m,h
we have
ϕ⊤ζk,n −ϕ⊤∆wk = ϕ′⊤ ζk,n −ϕ′⊤ ∆wk +∆ϕ⊤ζk,n −∆ϕ⊤∆wk
m,h m,h m,h m,h m,h m,h
≥ ϕ′⊤ ζ mk, ,n h−∥ϕ′∥ (Λk )−1(cid:13) (cid:13)∆w mk ,h(cid:13) (cid:13) Λk −∥∆ϕ∥ (Λk )−1(cid:13) (cid:13)ζ mk, ,n h(cid:13) (cid:13) Λk
m,h m,h m,h m,h
−∥∆ϕ∥ (Λk )−1(cid:13) (cid:13)∆w mk ,h(cid:13) (cid:13) Λk
m,h m,h
≥ ϕ′⊤ ζ mk, ,n h−∥ϕ′∥ (Λk )−1(cid:13) (cid:13)∆w mk ,h(cid:13) (cid:13) Λk −ε(cid:0)(cid:13) (cid:13)ζ mk, ,n h(cid:13) (cid:13) Λk +(cid:13) (cid:13)∆w mk ,h(cid:13) (cid:13) Λk (cid:1) .
m,h m,h m,h m,h
(F.4)
63Conditioned on the event G(M,K,H,δ), we have
√
(cid:13) (cid:13)ζ mk, ,n h(cid:13) (cid:13)
Λk
≤ c 1σ d.
m,h
For any vector x ∈ Rd, we have
x⊤∆wk = x⊤(cid:0) wk −wk (cid:1)
m,h m,h (cid:98)m,h
(cid:18) (cid:18) (cid:19)(cid:19)
= x⊤(cid:0) Λk (cid:1)−1 Λk wk − (cid:88) (cid:2) r +Vk (cid:0) s′l(cid:1)(cid:3) ϕ(cid:0) sl,al(cid:1)
m,h m,h m,h h m,h+1
(sl,al,s′l)∈U (k)
m,h
(cid:18)K(k) (cid:18)K(k) (cid:19)(cid:19)
= x⊤(cid:0) Λk (cid:1)−1 (cid:88) ϕ(cid:0) sl,al(cid:1) ϕ(cid:0) sl,al(cid:1)⊤ wk +λwk − (cid:88)(cid:2) r +Vk (cid:0) s′l(cid:1)(cid:3) ϕ(cid:0) sl,al(cid:1)
m,h m,h m,h h m,h+1
l=1 l=1
(cid:18) (cid:18)K(k) (cid:19)(cid:19)
= x⊤(cid:0) Λk (cid:1)−1 wk + (cid:88)(cid:2)P Vk −Vk (cid:0) s′l(cid:1)(cid:3) ϕ(cid:0) sl,al(cid:1) ,
m,h m,h h m,h+1 m,h+1
l=1
where the third equality holds due to the definition of Λk . We set x = Λk ∆wk . By using
m,h m,h m,h
Cauchy-Schwarz inequality, we have
(cid:18) (cid:18)K(k) (cid:19)(cid:19)
(cid:13) (cid:13)∆w mk ,h(cid:13) (cid:13)2
Λk
= ∆w mk ,h⊤ w mk ,h+ (cid:88)(cid:2)P hV mk ,h+1−V mk ,h+1(cid:0) s′l(cid:1)(cid:3) ϕ(cid:0) sl,al(cid:1)
m,h
l=1
(cid:13) (cid:18)K(k) (cid:19)(cid:13)
≤ (cid:13) (cid:13)∆w mk ,h(cid:13) (cid:13)
Λk m,h
·(cid:13) (cid:13) (cid:13)w mk ,h+ (cid:88)
l=1
(cid:2)P hV mk ,h+1−V mk ,h+1(cid:0) s′l(cid:1)(cid:3) ϕ(cid:0) sl,al(cid:1) (cid:13) (cid:13)
(cid:13) (Λk
m,h)−1.
This indicates that with probability at least 1−δ, for all (m,h,k) ∈ M×[H]×[K], we have
(cid:13)K(k) (cid:13)
(cid:13) (cid:13)∆w mk ,h(cid:13) (cid:13)
Λk m,h
≤ (cid:13) (cid:13)w mk ,h(cid:13) (cid:13)
(Λk m,h)− √1
+(cid:13) (cid:13) (cid:13)(cid:88)
l=1
(cid:2)P hV mk ,h+1−V mk ,h+1(cid:0) s′l(cid:1)(cid:3) ϕ(cid:0) sl,al(cid:1)(cid:13) (cid:13)
(cid:13) (Λk m,h)−1
≤ (cid:13) (cid:13)w mk ,h(cid:13) (cid:13)+3H dD
δ
√
≤ 5H dD ,
δ
where the second inequality holds because of Lemma E.3. Then for all (m,h,k) ∈ M×[H]×[K],
with probability at least 1−δ, (F.4) becomes
√ √
max (cid:8) ϕ⊤ζ mk, ,n h−ϕ⊤∆w mk ,h(cid:9) ≥ max (cid:8) ϕ′⊤ ζ mk, ,n h−∥ϕ′∥ (Λk )−1(cid:13) (cid:13)∆w mk ,h(cid:13) (cid:13) Λk (cid:9) −ε(cid:0) c 1σ d+5H dD δ(cid:1) .
n∈[N] n∈[N] m,h m,h
√ √
Now we choose σ = O(cid:101)(H d) and guarantee that σ > 5H dD δ ≥ (cid:13) (cid:13)∆w mk ,h(cid:13) (cid:13) Λk , this is achievable
√ √ m,h
through calculation. Define A
δ
= c 1σ d+5H dD
δ
= O(cid:101)(Hd). Then, for all (m,h,k) ∈ M×[H]×
[K], with probability at least 1−δ, we have
max (cid:8) ϕ⊤ζk,n −ϕ⊤∆wk (cid:9) ≥ max (cid:8) ϕ′⊤ ζk,n −σ∥ϕ′∥ (cid:9) −A ε.
m,h m,h m,h (Λk )−1 δ
n∈[N] n∈[N] m,h
64Recall from (F.3), by taking union bound, with probability at least 1 − |C(ε)|cN − δ, for all
0
(m,h,k) ∈ M×[H]×[K] and for all (s,a) ∈ S ×A, we have
max (cid:8) ϕ⊤ζk,n −ϕ⊤∆wk (cid:9) ≥ −A ε.
m,h m,h δ
n∈[N]
Finally, recall from (F.2), we have, with probability at least 1−|C(ε)|cN −δ, for all (m,h,k) ∈
0
M×[H]×[K] and for all (s,a) ∈ S ×A, we have
lk (s,a) ≤ A ε.
m,h δ
This completes the proof.
F.6 Proof of Lemma E.6
Proof. Recall the definition of model prediction error in Definition B.1, we get
−lk (s,a) = Qk (s,a)−r (s,a)−P Vk (s,a)
m,h m,h h h m,h+1
(cid:110) (cid:16) (cid:17) (cid:111)+
= min max ϕ(s,a)⊤ wk +ζk,n ,H −h+1 −r (s,a)−P Vk (s,a)
(cid:98)m,h m,h h h m,h+1
n∈[N]
(cid:16) (cid:17)
≤ max ϕ(s,a)⊤ wk +ζk,n −r (s,a)−P Vk (s,a)
(cid:98)m,h m,h h h m,h+1
n∈[N]
(cid:16) (cid:17)
= max ϕ(s,a)⊤ζk,n − r (s,a)+P Vk (s,a)−ϕ(s,a)⊤wk
m,h h h m,h+1 (cid:98)m,h
n∈[N]
(cid:12) (cid:12)
≤ (cid:12) (cid:12)r h(s,a)+P hV mk ,h+1(s,a)−ϕ(s,a)⊤w (cid:98)mk ,h(cid:12) (cid:12)+ max (cid:12) (cid:12)ϕ(s,a)⊤ζ mk, ,n h(cid:12) (cid:12).
n∈[N]
Based on Lemma E.4, conditioned on the event G(M,K,H,δ), with probability 1 − δ, for all
(m,k,h) ∈ M×[K]×[H] and for any (s,a) ∈ S ×A, we have
(cid:12) (cid:12) √
(cid:12)ϕ(s,a)⊤wk −r (s,a)−P Vk (s,a)(cid:12) ≤ 5H dD ∥ϕ(s,a)∥ (F.5)
(cid:12) (cid:98)m,h h h m,h+1 (cid:12) δ (Λk )−1
m,h
Conditioned on the event G(M,K,H,δ), for all (m,h,k) ∈ M×[H]×[K] and for any (s,a) ∈ S×A,
we have
√
max (cid:12) (cid:12)ϕ(s,a)⊤ζ mk, ,n h(cid:12) (cid:12) ≤ c 1σ d∥ϕ(s,a)∥ (Λk )−1. (F.6)
n∈[N] m,h
Combine (F.5) and (F.6), then use σ defined in Lemma E.5. Conditioned on the event G(M,K,H,δ),
with probability 1−δ, for all (m,h,k) ∈ M×[H]×[K] and for any (s,a) ∈ S ×A, we get
√ √
−lk (s,a) ≤ (cid:0) 5H dD +c σ d(cid:1) ∥ϕ(s,a)∥
m,h δ 1 (Λk )−1
m,h
≤ c Hd∥ϕ(s,a)∥ ,
2 (Λk )−1
m,h
where c
2
= O(cid:101)(1). Here we completes the proof.
65G Proof of the Regret Bound for CoopTS-PHE in Misspecified
Setting
In this section, we prove the regret bound for CoopTS-PHE in the misspecified setting. The regret
analysis, the essential supporting lemmas and their corresponding proofs are very similar to what we
have presented in Appendix E and Appendix F. Here we mainly point out the differences of proof
between these two settings.
G.1 Supporting Lemmas
Lemma G.1. Let λ = 1 in Algorithm 2. Under Definition 5.7, for any fixed 0 < δ < 1, conditioned
on the event G(M,K,H,δ), with probability 1−δ, for all (m,k,h) ∈ M×[K]×[H] and for any
(s,a) ∈ S ×A, we have
(cid:12) (cid:12) √ √
(cid:12)ϕ(s,a)⊤wk −r (s,a)−P Vk (s,a)(cid:12) ≤ (cid:0) 5H dD +3Hζ MKd(cid:1) ∥ϕ(s,a)∥ +3Hζ,
(cid:12) (cid:98)m,h h h m,h+1 (cid:12) δ (Λk )−1
m,h
(G.1)
where D is defined in Lemma E.3.
δ
Proof of Lemma G.1. This proof is almost same as the proof of Lemma D.2, with the only difference
in bounding Term(i) in (D.2). Here (D.3) becomes
√
|Term(i)| ≤ 3H dD ∥ϕ(s,a)∥ .
δ (Λk )−1
m,h
Finally we can get the desired result.
Lemma G.2 (Optimism). Let λ = 1 in Algorithm 2 and set c = Φ(1). Under Definition 5.7,
0
conditionedontheeventG(M,K,H,δ),withprobabilityatleast1−|C(ε)|cN−δwhere|C(ε)| ≤ (3/ε)d,
0
for all (m,k,h) ∈ M×[K]×[H] and for all (s,a) ∈ S ×A, we have
lk ≤ A ε+3Hζ,
m,h δ
√ √
where A
δ
= c 1σ d+5H dD
δ
= O(cid:101)(Hd).
Proof of Lemma G.2. This proof is similar to the proof in Appendix F.5. In the previous part, we
have defined
∆ = P Vk (s,a)−ϕ(s,a)⊤(cid:10) µ ,Vk (cid:11) ,
m,1 m,h m,h+1 h m,h+1 S
∆ = r (s,a)−ϕ(s,a)⊤θ ,
m,2 m,h h
where |∆ | ≤ 2Hζ and |∆ | ≤ ζ. Thus we have
m,1 m,2
r (s,a)+P Vk (s,a) = ϕ(s,a)⊤wk +∆ +∆ ,
m,h m,h m,h+1 m,h m,1 m,2
where wk = ⟨µ ,Vk (cid:11) +θ . Then we define ∆wk = wk −wk . For any (m,h,k) ∈
m,h h m,h+1 S h m,h m,h (cid:98)m,h
M×[H]×[K] and any (s,a) ∈ S ×A, we have
lk (s,a) = r (s,a)+P Vk (s,a)−Qk (s,a)
m,h m,h m,h m,h+1 m,h
66(cid:110) (cid:16) (cid:17)(cid:111)+
= r (s,a)+P Vk (s,a)−min H −h+1, max ϕ(s,a)⊤ wk +ζk,n
m,h m,h m,h+1 (cid:98)m,h m,h
n∈[N]
(cid:110) (cid:16) (cid:17)(cid:111)
≤ max ϕ(s,a)⊤wk −(H −h+1),ϕ(s,a)⊤wk − max ϕ(s,a)⊤ wk +ζk,n
m,h m,h (cid:98)m,h m,h
n∈[N]
+∆ +∆
m,1 m,2
(cid:110) (cid:111)
≤ max 0,ϕ(s,a)⊤∆wk − max ϕ(s,a)⊤ζk,n +3Hζ. (G.2)
m,h m,h
n∈[N]
In Appendix F.5, we have proved that with probability at least 1−|C(ε)|cN −δ, for all (m,h,k) ∈
0
M×[H]×[K] and for all (s,a) ∈ S ×A, we have
max (cid:8) ϕ⊤ζk,n −ϕ⊤∆wk (cid:9) ≥ −A ε.
m,h m,h δ
n∈[N]
Substitute it into (G.2), we can get the final result.
Lemma G.3 (Errorbound). Letλ = 1inAlgorithm2. UnderDefinition5.7, foranyfixed0 < δ < 1,
conditioned on the event G(M,K,H,δ), with probability 1−δ, for all (m,k,h) ∈ M×[K]×[H]
and for any (s,a) ∈ S ×A, we have
√
−lk (s,a) ≤ (cid:0) c Hd+3Hζ MKd(cid:1) ∥ϕ(s,a)∥ +3Hζ,
m,h 2 (Λk)−1
h
where c
2
= O(cid:101)(1) is same as that in Lemma E.6.
Proof of Lemma G.3. Similar to the proof in Appendix F.6, using (F.6) in Appendix F.6 and (G.1),
we have
(cid:12) (cid:12)
−l mk ,h(s,a) ≤ (cid:12) (cid:12)r h(s,a)+P hV mk ,h+1(s,a)−ϕ(s,a)⊤w (cid:98)mk ,h(cid:12) (cid:12)+ max (cid:12) (cid:12)ϕ(s,a)⊤ζ mk, ,n h(cid:12) (cid:12)
n∈[N]
√ √ √
(cid:0) (cid:1)
≤ 5H dD +3Hζ MKd+c σ d ∥ϕ(s,a)∥ +3Hζ
δ 1 (Λk)−1
√ h
(cid:0) (cid:1)
≤ c Hd+3Hζ MKd ∥ϕ(s,a)∥ +3Hζ,
2 (Λk)−1
h
where c
2
= O(cid:101)(1) is same as that in Lemma E.6. Here we completes the proof.
G.2 Regret Analysis
In this part, we give out the proof of Theorem 5.9, the regret bound for CoopTS-PHE in the
misspecified setting.
Proof of Theorem 5.9. This proof is almost same as the proof in Appendix E.2. We do the same
regret decomposition (E.2) and obtain the same bound for Term (i) (E.3) and Term (ii) (E.4).
Next we bound Term (iii) with new lemmas in misspecified setting.
Bounding Term (iii) in (E.2): conditioned on the event G(M,K,H,δ′), based on Lemma G.3
and Lemma G.2, by taking union bound, with probability at least 1−|C(ε)|c′N −δ′−MHKδ′, we
0
have
K H
(cid:88) (cid:88)(cid:88)(cid:0)E (cid:2) lk (s ,a )|s = sk (cid:3) −lk (cid:0) sk ,ak (cid:1)(cid:1)
π∗ m,h m,h m,h m,1 m,1 m,h m,h m,h
m∈Mk=1h=1
67K H
≤ (cid:88) (cid:88)(cid:88)(cid:16) −lk (cid:0) sk ,ak (cid:1) +A ε+3Hζ(cid:17)
m,h m,h m,h δ′
m∈Mk=1h=1
(cid:88) (cid:88)K (cid:88)H (cid:16) (cid:0) √ (cid:1) (cid:17)
≤ c dH +3Hζ MKd ∥ϕ(s,a)∥ +3Hζ +A ε+3Hζ
2 (Λk)−1 δ′
h
m∈Mk=1h=1
√ H K
= HMKA δ′ε+6H2MKζ +(cid:0) c 2dH +3Hζ MKd(cid:1)(cid:88) (cid:88) (cid:88)(cid:13) (cid:13)ϕ(sk m,h,ak m,h)(cid:13) (cid:13)
(Λk )−1
m,h
h=1m∈Mk=1
√
≤ HMKA ε+6H2MKζ +(cid:0) c dH +3Hζ MKd(cid:1)
δ′ 2
(cid:115)
(cid:88)H (cid:18) (cid:18) det(ΛK)(cid:19) (cid:19)
√
(cid:18) det(ΛK)(cid:19)
× log h +1 M γ +2 MKlog h
det(λI) det(λI)
h=1
√
≤ HMKA ε+6H2MKζ +(cid:0) c dH +3Hζ MKd(cid:1)
δ′ 2
(cid:16) √ (cid:112) (cid:17)
×H d(log(1+MK/d)+1)M γ +2 MKdlog(1+MK/d)
=
O(cid:101)(cid:16) d23 H2√ M(cid:0)(cid:112)
dMγ
+√ K(cid:1) +dH2M√ K(cid:0)(cid:112)
dMγ
+√ K(cid:1) ζ(cid:17)
.
The first inequality follows from Lemma G.2, the second inequality holds due to Lemma G.3, the
third inequality follows from Lemma B.12, the last inequality holds due to Lemma H.2 and the fact
that ∥ϕ(·)∥
2
≤ 1, and again we choose ε = dH(cid:112) d/MK/A
δ′
= O(cid:101)((cid:112) d/MK).
The probability calculation is same as that in Appendix E.2. By combining Terms (i)(ii)(iii)
together, we get that the final regret bound for CoopTS-PHE in misspecified setting is
Regret(K) =
O(cid:101)(cid:16) d3 2H2√ M(cid:0)(cid:112)
dMγ
+√ K(cid:1) +dH2M√ K(cid:0)(cid:112)
dMγ
+√ K(cid:1) ζ(cid:17)
,
with probability at least 1−δ. Here we finish the proof.
H Auxiliary Lemmas
Lemma H.1. (Abbasi-Yadkori et al., 2011, Lemma 11) Let {X }∞ be a sequence in Rd, V is
t t=1
d×d positive definite matrix and define V¯ = V+(cid:80)t X X⊤. Then, we have that
t s=1 s s
log(cid:18) det(V¯ n)(cid:19)
≤
(cid:88)n
∥X ∥2 .
det(V) t V¯−1
t−1
t=1
Further, if ∥X ∥ ≤ L for all t, then
t 2
n
(cid:88) min(cid:110) 1,∥X ∥2 (cid:111) ≤ 2(cid:0) logdet(V¯ )−logdetV(cid:1) ≤ 2(cid:0) dlog(cid:0)(cid:0) trace(V)+nL2(cid:1) /d(cid:1) −logdetV(cid:1) ,
t V¯−1 n
t−1
t=1
and finally, if λ (V) ≥ max(cid:0) 1,L2(cid:1) then
min
(cid:88)n
∥X ∥2 ≤ 2log
det(V¯ n)
.
t V¯−1 det(V)
t−1
t=1
68Lemma H.2. (Abbasi-Yadkori et al., 2011, Lemma 10) Suppose X ,X ,...,X ∈ Rd and for any
1 2 t
1 ≤ s ≤ t, ∥X ∥ ≤ L. Let V¯ = λI+(cid:80)t X X⊤ for some λ > 0. Then,
s 2 t s=1 s s
det(cid:0) V¯ (cid:1) ≤ (cid:0) λ+tL2/d(cid:1)d .
t
Lemma H.3. (Ishfaq et al., 2021, Lemma D.5) Let A ∈ Rd×d be a positive definite matrix where
its largest eigenvalue λ (A) ≤ λ. Let x ,...,x be k vectors in Rd. Then it holds that
max 1 k
(cid:13) k (cid:13) √ (cid:18) k (cid:19)1/2
(cid:13) (cid:13)A(cid:88) x i(cid:13) (cid:13) ≤ λk (cid:88) ∥x i∥2
A
.
(cid:13) (cid:13)
i=1 i=1
Lemma H.4. (Jin et al., 2020, Lemma D.1) Let Λ = λI+(cid:80)t ϕ ϕ⊤, where ϕ ∈ Rd and λ > 0.
t i=1 i i i
Then it holds that
t
(cid:88)
ϕ⊤(Λ )−1ϕ ≤ d.
i t i
i=1
Lemma H.5. (Ishfaq et al., 2024, Lemma D.1) Given a multivariate normal distribution X ∼
N (0,Σ), we have,
(cid:18) (cid:114) (cid:19)
1
P ∥X∥ ≤ tr(Σ) ≥ 1−δ.
δ
Lemma H.6. (Horn and Johnson, 2012) If A and B are positive semi-definite square matrices of
the same size, then
0 ≤ [tr(AB)]2 ≤ tr(cid:0) A2(cid:1) tr(cid:0) B2(cid:1) ≤ [tr(A)]2[tr(B)]2.
Lemma H.7. (Jin et al., 2020, Lemma D.4) Let {s }∞ be a stochastic process on state space
i i=1
S with corresponding filtration {F }∞ . Let {ϕ }∞ be an Rd-valued stochastic process where
i i=1 i i=1
ϕ ∈ F , and ∥ϕ ∥ ≤ 1. Let Λ = λI+(cid:80)k ϕ ϕ⊤. Then for any δ > 0, with probability at least
i i−1 i k i=1 i i
1−δ, for all k ≥ 0, and any V ∈ V with sup |V(s)| ≤ H, we have
s∈S
(cid:13) (cid:13) (cid:13)(cid:88)k ϕ i{V(s i)−E[V(s i) | F i−1]}(cid:13) (cid:13) (cid:13)2 ≤ 4H2(cid:20) d log(cid:18) k+λ(cid:19) +log N ε(cid:21) + 8k2ε2 ,
(cid:13) (cid:13) 2 λ δ λ
i=1
Λ− k1
where N is the ε-covering number of V with respect to the distance dist(V,V′) = sup |V(s)−
ε s∈S
V′(s)|.
Lemma H.8. (Vershynin, 2018, Covering number of Euclidean ball) For any ε > 0, N , the
ε
ε-covering number of the Euclidean ball of radius B > 0 in Rd satisfies
(cid:18) 2B(cid:19)d (cid:18) 3B(cid:19)d
N ≤ 1+ ≤ .
ε
ε ε
Lemma H.9. Let V denote a class of functions mapping from S to R with the following parametric
form
(cid:110) (cid:110) (cid:111)+(cid:111)
V(·) = max min max ϕ(·,a)⊤wn,H −h+1 ,
a∈A n∈[N]
69where the parameter wn satisifies ∥wn∥ ≤ B for all n ∈ [N] and for all (x,a) ∈ S × A, we
have ∥ϕ(x,a)∥ ≤ 1. Let N be the ε-covering number of V with respect to the distance dist
V,ε
(V,V′) = sup (cid:12) (cid:12)V(s)−V′(s)(cid:12) (cid:12). Then
s∈S
(cid:18) 3B(cid:19)d
N ≤ .
V,ε
ε
Proof. ConsideranytwofunctionsV ,V ∈ V withparameters{wn} and{wn} respectively.
1 2 1 n∈[N] 2 n∈[N]
Then we have
(cid:12) (cid:12)
dist(V ,V ) ≤ sup(cid:12) max ϕ(s,a)⊤wn− max ϕ(s,a)⊤wn(cid:12)
1 2 (cid:12) 1 2(cid:12)
s,a n∈[N] n∈[N]
(cid:12) (cid:16) (cid:17)(cid:12)
≤ sup(cid:12) max ϕ(s,a)⊤wn−ϕ(s,a)⊤wn (cid:12)
(cid:12) 1 2 (cid:12)
s,a n∈[N]
≤ sup max
(cid:12) (cid:12)ϕ⊤wn−ϕ⊤wn(cid:12)
(cid:12)
1 2
∥ϕ∥≤1n∈[N]
= max sup
(cid:12) (cid:12)ϕ⊤(wn−wn)(cid:12)
(cid:12)
1 2
n∈[N]∥ϕ∥≤1
≤ max sup
∥ϕ∥(cid:13) (cid:13)wn−wn(cid:13)
(cid:13)
1 2
n∈[N]∥ϕ∥≤1
≤ max
(cid:13) (cid:13)wn−wn(cid:13)
(cid:13).
1 2
n∈[N]
Let N denote the ε-covering number of {w ∈ Rd | ∥w∥ ≤ B}. Then, Lemma H.8 implies
w,ε
(cid:18) 2B(cid:19)d (cid:18) 3B(cid:19)d
N ≤ 1+ ≤ .
w,ε
ε ε
For any V ∈ V, we consider its corresponding parameters {wn} . For any n ∈ [N], we can find
1 1 n∈[N]
wn such that ∥wn−wn∥ ≤ ε, then we can get V ∈ V with parameters {wn} . Then we have
2 1 2 2 2 n∈[N]
dist(V ,V ) ≤ max ∥wn−wn∥ ≤ ε. Thus, we have,
1 2 n∈[N] 1 2
(cid:18) 2B(cid:19)d (cid:18) 3B(cid:19)d
N ≤ N ≤ 1+ ≤ .
V,ε w,ε
ε ε
This completes the proof.
Lemma H.10. (Abramowitz and Stegun, 1968) Suppose Z is a Gaussian random variable Z ∼
N(µ,σ2), where σ > 0. For 0 ≤ z ≤ 1, we have
P(Z > µ+zσ) ≥ √1 e− 2z2 , P(Z < µ−zσ) ≥ √1 e− 2z2 .
8π 8π
And for z ≥ 1, we have
e−z2/2 e−z2/2
√ ≤ P(|Z −µ| > zσ) ≤ √ .
2z π z π
70Figure 3: The N-Chain environment (Osband et al., 2016a).
Lemma H.11. (Ishfaq et al., 2021, Lemma D.2) Consider a d-dimensional multivariate normal
distribution N(cid:0) 0,AΛ−1(cid:1) where A is a scalar. Let η ,η ,...,η be N independent samples from
1 2 N
the distribution. Then for any δ > 0
(cid:18) (cid:19)
(cid:112)
P max ∥η ∥ ≤ c dAlog(d/δ) ≥ 1−Mδ,
j Λ
j∈[M]
where c is some absolute constant.
Lemma H.12. (Abbasi-Yadkori et al., 2011, Lemma 12) Let A, B and C be positive semi-definite
matrices such that A = B+C. Then we have that
x⊤Ax det(A)
sup ≤ .
x⊤Bx det(B)
x̸=0
I Additional Experimental Details
We conduct comprehensive experiments investigating the exploration strategies for DQN under a
multi-agent setting. For all the Q networks in our experiments, we use ReLU as our activation
function. Given that all experiments are conducted under multi-agent settings unless explicitly
specified as a single-agent or centralized scenario, we denote our methods: CoopTS-PHE as "PHE"
and CoopTS-LMC as "LMC" in both experimental contexts and figures. In addition to our methods,
the baselines we selected are either commonly used (DQN (Mnih et al., 2015), DDQN (Hasselt
et al., 2016)) or with competitive empirical performance (Bootstrapped DQN (Osband et al.,
2016a), NoisyNet DQN (Fortunato et al., 2018)). Both Bootstrapped DQN and NoisyNet DQN
are randomized exploration methods. Bootstrapped DQN uses finite ensembles to generate the
randomized value functions and views them as approximate posterior samples of Q-value functions.
NoisyNet DQN injects noise into the parameters of neural networks to aid efficient exploration.
I.1 N-chain
We commence by presenting the comprehensive results for N = 25 in Figure 4, illustrating that
our randomized exploration methods exhibit greater suitability in realistic scenarios characterized
by an increasing number of agents. This superiority is particularly evident under two potential
circumstances: (1) where there are more limitations on computation or data access from each source
in the real world, and (2) when parallel learning from multiple sources can significantly enhance
runtime efficiency.
7110 10 10
8 8 8
6 6 6
4 PHE 4 PHE 4 PHE
LMC LMC LMC
DQN DQN DQN
2 Bootstrapped DQN 2 Bootstrapped DQN 2 Bootstrapped DQN
NoisyNet DQN NoisyNet DQN NoisyNet DQN
DDQN DDQN DDQN
0 0 0
0 1000 2000 3000 4000 5000 6000 0 2000 4000 6000 8000 0 2000 4000 6000 8000
Episode Episode Episode
(a) m=2 (b) m=3 (c) m=4
Figure 4: Comparison among different exploration strategies in N-chain with N = 25. All results
are averaged over 10 runs.
10 10
8
8
6
6
4
4
m = 1, N = 10 m = 1, N = 10
2
m = 2, N = 10 m = 2, N = 10
2 m = 3, N = 10 m = 3, N = 10
m = 4, N = 10 m = 4, N = 10
0
0 500 1000 1500 2000 2500 3000 0 500 1000 1500 2000 2500 3000
Episode Episode
(a) PHE (b) LMC
Figure 5: Rewards with averaged over 10 independent runs for different numbers of agents among
algorithms without communication. Note that when m = 1, one agent indicates a centralized setting.
Subsequently, we provide a more comprehensive study to investigate the exploration capabilities
facilitated by parallel training. Preliminary experiments are conducted with a reduced state space,
specifically considering N = 10. The study aims to investigate exploration capabilities across varying
agent counts, specifically within the set m ∈ {1,2,3,4}.
Performance Consistency with Varying m In the investigation detailed in Figure 5, we explore
parallel learning without inter-agent communication. Consequently, while multiple agents engage in
simultaneous policy learning, each agent independently formulates its policies without the exchange
of transition information. The discernible trend in this scenario is that an increase in the number
of agents sharing the total episodes results in a slower rate of policy learning. Notably, despite
this temporal discrepancy, all learning trajectories eventually approximate convergence towards the
optimal dashed line.
72
draweR
draweR
draweR
draweR
draweR10 10 10
8 8 8
6 6 6
4 4 4
centralized, n = 10 centralized, n = 10 centralized, n = 10
no communication(2 agents), n = 10 no communication(3 agents), n = 10 no communication(4 agents), n = 10
2 linear(2 agents), n = 10 2 linear(3 agents), n = 10 2 linear(4 agents), n = 10
constant(2 agents), n = 10 constant(3 agents), n = 10 constant(4 agents), n = 10
exponential(2 agents), n = 10 exponential(3 agents), n = 10 exponential(4 agents), n = 10
0 0 0
0 250 500 750 1000 1250 1500 0 500 1000 1500 2000 0 500 1000 1500 2000 2500 3000
Episode Episode Episode
(a) m=2 (b) m=3 (c) m=4
10 10
10
8 8 8
6 6 6
4 4 4
centralized, n = 10 centralized, n = 10 centralized, n = 10
no communication(2 agents), n = 10 no communication(3 agents), n = 10 no communication(4 agents), n = 10
2 linear(2 agents), n = 10 2 linear(3 agents), n = 10 2 linear(4 agents), n = 10
constant(2 agents), n = 10 constant(3 agents), n = 10 constant(4 agents), n = 10
exponential(2 agents), n = 10 exponential(3 agents), n = 10 exponential(4 agents), n = 10
0 0 250 500 750 1000 1250 1500 1750 0 0 500 1000 1500 2000 0 0 500 1000 1500 2000 2500 3000
Episode Episode Episode
(d) m=2 (e) m=3 (f) m=4
Figure 6: Different number of agents m with different synchronization strategies as well as the
single-agent and no communication settings in N = 10. Top: PHE, Bottom: LMC
10 10 10
8 8 8
6 6 6
4 LMC 4 LMC 4 LMC
PHE PHE PHE
NeuralTS NeuralTS NeuralTS
2 NeuralUCB 2 NeuralUCB 2 NeuralUCB
LinTS LinTS LinTS
LinUCB LinUCB LinUCB
0 0 0
0 200 400 600 800 1000 0 500 1000 1500 2000 0 500 1000 1500 2000 2500 3000
Episode Episode Episode
(a) m=2 (b) m=3 (c) m=4
Figure 7: Performance with different number of agents m compared with bandit-inspired exploration
in N = 10.
73
draweR
draweR
draweR
draweR
draweR
draweR
draweR
draweR
draweRPHE
30 LMC
DQN
Bootstrapped DQN
25
NoisyNet DQN
DDQN
NeuralTS
20
NeuralUCB
15
10
5
32_2 32_3 64_2 64_3
Neural Network
Figure 8: Computation time with different exploration strategies.
Different Synchronization Conditions To further demonstrate the efficiency of parallel learning
with communication, we compare different synchronization conditions in Section 4.1. Specifically, we
denote synchronization (1) in every constant step as constant, (2) following exponential function
as exponential, and (3) based on (4.3) as linear. To have a fair comparison among different
synchronization conditions, we firstly record the empirical number of synchronization via linear
condition in average, and then we consider constant value for constant condition and select proper
base b for exponential condition with a similar number of synchronization. Figure 6 illustrates that
any synchronization condition can improve learning efficiency but still with centralized learning as
an upper bound.
Performance Compared with Bandit-inspired Methods Since one of our proposed random
exploration strategies, PHE is a variant of approximation TS, it is fair for us to investigate the
performance of other exploration methods from bandit algorithms with the integration of DQN. We
mainly compare both TS and UCB under neural network (i.e., NeuralTS (Zhang et al., 2021) and
NeuralUCB (Zhou et al., 2020)) and linear (i.e., LinTS (Agrawal and Goyal, 2013b) and LinUCB (Li
et al., 2010)) settings. We show that a performance gap exists between linear approaches and other
neural-based methods even in a small-scale exploration problem with N = 10 in Figure 7.
Computational Time We have demonstrated that both NeuralTS and NeuralUCB exhibit
convergence to performance levels comparable to our proposed randomized exploration strategies
(i.e., PHE and LMC) when considering the case of N = 10 with m = 4 under the synchronization
condition (linear), as outlined in (4.3). However, we argue that the scalability of both methods
is limited due to their associated computational costs. To substantiate this assertion, we conduct
experiments across all methods including DQN baselines with N = 10 and m = 4 over 104 steps
with varying neural network sizes, such as [32,32,32], which signifies three layers with 32 neurons in
74
)ces(
emiT
noitatupmoC10 10 inv temp 1e0
inv temp 1e2
inv temp 1e4
8 8 inv temp 1e6
inv temp 1e8
6 6
4 4
inv temp 1e0
inv temp 1e2
2 inv temp 1e4 2
inv temp 1e6
inv temp 1e8
0 0
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
Episode Episode
(a) m=1 (centralized), N=25 (b) m=2 (no communication), N=25
Figure 9: Hyper-parameter tuning of inverse temperature (inv temp) β for LMC with N = 25:
m,k
(a) centralized setting m = 1 (b) 2 agents without communication m = 2.
10 10
8 8
6 6
4 m = 1, N = 25, full buffer 4 m = 1, N = 25, full buffer
m = 1, N = 25, half buffer m = 1, N = 25, half buffer
m = 1, N = 25, less buffer m = 1, N = 25, less buffer
2 m = 2 (no communication), N = 25, full buffer 2 m = 2 (no communication), N = 25, full buffer
m = 2 (no communication), N = 25, half buffer m = 2 (no communication), N = 25, half buffer
m = 2 (no communication), N = 25, less buffer m = 2 (no communication), N = 25, less buffer
0 0
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
Episode Episode
(a) PHE (b) LMC
Figure 10: Different buffer size with N = 25 between single agent (centralized) and 2 agents (no
communication). Note that the full buffer indicates the size of the total episodes. Each agent in no
communication setting only occupies half of the total episodes. Therefore, two curves (full buffer,
half buffer) in no communication are consistent.
75
draweR
draweR
draweR
draweR10 10
8 8
6 6
4 4
m = 1 (centralized), N = 25 m = 1 (centralized), N = 25
m = 2 (no communication), N = 25 m = 2 (no communication), N = 25
2 m = 2 (linear), N = 25 2 m = 2 (linear), N = 25
m = 2 (constant), N = 25 m = 2 (constant), N = 25
m = 2 (exponential), N = 25 m = 2 (exponential), N = 25
0 0
0 1000 2000 3000 4000 5000 6000 0 1000 2000 3000 4000 5000 6000
Episode Episode
(a) PHE (b) LMC
Figure 11: Different synchronization strategies as well as the single-agent and no communication
settings in N = 25.
each layer. Importantly, the length of the chain N has no bearing on the running time.
In Figure 8, we show the computational time of all methods under different neural network sizes.
The solid lines represent the average computational time over 10 random seeds and the shaded area
represents the standard deviation. We observe that NeuralTS and NeuralUCB have heavy running
time consistently with varying network sizes. Although the computation time of LMC is still higher
than other remaining approaches, we observe that it maintains a similar computation time with
different neural network sizes, which can still be scaled up to more complex problems with larger
neural networks.
Hyper-parameter Tuning of Inverse Temperature β Subsequently, we scale the problem
m,k
to N = 25. Given the extended horizon, the demand for exploration intensifies, leading us to
conduct hyper-parameter tuning for the inverse temperature parameter β in LMC, as illustrated
m,k
in Figure 9. It is crucial to note that the efficacy of learning is significantly influenced by the
exploration capacity in both centralized learning and parallel learning without communication. Our
observations reveal a discernible gap between centralized and parallel learning, a departure from
the pattern observed in Figure 5. We posit that the disparity may stem from issues associated with
the replay buffer size in off-policy RL algorithms. Specifically, when the replay buffer exhausts its
capacity for new transitions, the incoming transition replaces the oldest one.
Hyper-parameter Tuning of Buffer Size Therefore, we present a performance comparison
between a solitary agent (m = 1) and a scenario involving two agents (m = 2) in Figure 10 with
different buffer sizes. Full buffer and half buffer indicate the replay buffer’s capacity to store the
complete set and half of the transitions during training, respectively. We observe that the learning
process is more efficient with less buffer size in a centralized setting because having an excessively
large replay buffer may potentially impede the efficiency of the learning process. Furthermore, the
gap between centralized setting and paralleling learning still exists among different buffer sizes.
Therefore, we focus on the setting of less buffer size with different synchronization conditions in
76
draweR draweR10
8
6
m = 1 (centralized), prioritized
4
m = 2 (no communication), prioritized
m = 2 (linear), prioritized
2 m = 1 (centralized)
m = 2 (no communication)
m = 2 (linear)
0
0 1000 2000 3000 4000 5000 6000
Episode
Figure12: Gapreductionimprovementwithprioritizedexperiencereplayforparallellearningwithout
communication. Note that the same settings with standard and prioritized experience replay are in
the same-ish color.
Figure 11. We conclude that linear condition results in competitive performance in both PHE
and LMC in the N-chain problem and we report all exploration strategies with linear condition in
Section 6.1.
Ablation Study of Sampling Mechanism Toreducetherewardgap,weadoptabettersampling
mechanism in the replay buffer with prioritized experience replay (PER). In Figure 12, parallel
learning without inter-agent communication can increase reward with PER. However, centralized
learning with PER improves faster convergence with similar performance and the trends for linear
condition curves are similar. Therefore, the gap between centralized and parallel learning without
communication is reduced with PER. Note that the main experimental results in Figure 1 are based
on standard experience replay because standard sampling in linear condition has similar performance
against PER with faster training time.
I.2 Super Mario Bros
While cooperative parallel learning enhances training efficiency through data sharing, challenges
emerge when handling data from devices capturing images or audio due to privacy concerns in real-
worldapplications. Inresponse,ourapproachextendsrandomizedexplorationstrategiestoafederated
reinforcement learning framework as shown in Algorithm 4, from Algorithm 1, which incorporates
parameter synchronization among Q neural networks rather than relying on the conventional practice
of sharing agents’ transitions in Line 14 in Algorithm 4. Note that the synchronization follows the
format as in Algorithm 1 to update Q functions with horizon h ∈ H. However, in practice, we can
directly update the weight of the neural network to reduce the communication cost.
77
draweR(a) SuperMarioBros-1-1-v0 (b) SuperMarioBros-1-2-v0 (c) SuperMarioBros-1-3-v0 (d) SuperMarioBros-1-4-v0
Figure 13: Illustrations of 4 different environments in Super Mario Bros task.
The training process unfolds within a federated reinforcement learning framework, wherein
local updates and global aggregations are iteratively executed (Jin et al., 2022a). Specifically, each
agent iterates through multiple local updates of its value function, followed by server-mediated
averaging of these functions across all agents, constituting a form of parameter sharing. Note that
the transitions are not accessible among agents, leading us to directly synchronize all agents with
parameter sharing every constant local iteration instead of synchronization condition in (4.3). We
use the same architecture for all the experiments in the Super Mario Bros task with the images as
the input states. Particularly, we construct 3 convolutional neural network layers with width [32, 64,
32], followed by 2 linear layers with the output of action space in the Q network.
Algorithm 4 Unified Algorithm Framework for Randomized Exploration in Federated Learning
1: for episode k = 1,...,K do
2: for agent m ∈ M do
3:
Receive initial state sk .
m,1
4: Vk (·) ← 0.
m,H+1
5: {Qk (·,·)}H ←Randomized Exploration ◁ Algorithm 2 or Algorithm 3
m,h h=1
6: for step h = 1,...,H do
7: ak ← argmax Qk (sk ,a).
m,h a∈A m,h m,h
8: Receive sk
m,h+1
and r h.
9: if Condition then
10: SYNCHRONIZE ← True.
11: end if
12: end for
13: end for
14: if SYNCHRONIZE then
15: for step h = H,...,1 do
16: Q¯k ← 1 (cid:80)M Qk
m M m=1 m,h
17: Qk ← Q¯k , ∀m
m,h m,h
18: end for
19: end if
20: end for
7815
10
20
20 25
30
30 35
40
40
45
50 50
Random PHE LMC DQN BootstrappedNoisyNet DDQN Random PHE LMC DQN BootstrappedNoisyNet DDQN
Algorithm Algorithm
(a) Tampa (hot humid) (b) Tucson (hot dry)
50
0
60
50
70
80 100
90
150
100
110 200
Random PHE LMC DQN BootstrappedNoisyNet DDQN Random PHE LMC DQN BootstrappedNoisyNet DDQN
Algorithm Algorithm
(c) Rochester (cold humid) (d) Great Falls (cold dry)
Figure 14: Evaluation performance at different cities in building energy systems
79
nruteR
yliaD
nruteR
yliaD
nruteR
yliaD
nruteR
yliaDI.3 Thermal Control of Building Energy Systems
BuildingEnv encompasses the regulation of heat flow in a multi-zone building to sustain a desired
temperature setpoint. We focus on one pre-defined building called "office small" in different cities
with varying weather types, i.e., Tampa (Hot Humid), Tucson (Hot Dry), Rochester (Cold Humid),
and Great Falls (Cold Dry). Each episode is designed to span a single day, comprising 5-minute
time intervals (H = 288, τ = 5/60 hours).
Observation Space The state at time step t, denoted as s(t) ∈ RM+4, encompasses the tem-
peratures T (t) of each zone, where i ∈ M, along with four additional properties: QGHI(t), Q¯p(t),
i
T (t), and T (t). Specifically, QGHI(t) represents the heat gain from solar irradiance, Q¯p(t) denotes
G E
the heat acquired from occupant activities, while T (t) and T (t) signify the ground and outdoor
G E
environment temperatures, respectively.
Action Space The continuous version of the action a(t) ∈ [−1,1]M controls the heating of M
zones. However, since our randomized exploration strategies use DQN (Mnih et al., 2015) as the
backbone, we adopt the multi-discrete action space defined in Yeh et al. (2023), which is a vector of
action spaces. Then we convert the multi-discrete action space to a single discrete action space with
action mapping.
Reward Function The primary objective is to minimize energy consumption while ensuring the
maintenance of temperature within a specified comfort range. Therefore, the reward is penalized
with both temperature deviations and HVAC energy consumption as follows:
r(t) = −(1−β)∥a(t)∥ −β∥Ttarget(t)−T(t)∥ ,
2 2
where Ttarget(t) = [Ttarget(t),Ttarget(t),...,Ttarget(t)] are the target temperatures and T(t) =
1 2 M
[T (t),T (t),...,T (t)] are the actual zonal temperatures. The parameter β is the trade-off be-
1 2 M
tween the energy consumption and temperature deviation penalties.
We execute experiments following the united framework in Algorithm 1, synchronizing every
constant number of steps across diverse weather conditions in varying cities. We choose a 2-hidden
layerneuralnetworkwithwidth[64,64]fortheQnetwork. Subsequently,weevaluatetheperformance
ofallmethodsindistinctcitiesrespectively,asillustratedinFigure14. Notably,ourproposedrandom
exploration strategies demonstrate a consistently higher mean return across all cities. However,
it is worth highlighting that DQN in Figure 14(c) and Noisy-Net in Figure 14(d) exhibit lower
returns compared to random actions. This outcome can be attributed to the discrete action space
configuration(Yehetal.,2023). Inaddition, weobservethatmaintainingthermalcontrolofbuildings
is more challenging in cold weather conditions compared to hot weather conditions.
80