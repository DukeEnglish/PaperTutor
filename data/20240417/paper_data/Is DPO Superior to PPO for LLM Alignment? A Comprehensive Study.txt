Is DPO Superior to PPO for LLM Alignment? A Comprehensive Study
ShushengXu1 WeiFu1 JiaxuanGao1 WenjieYe2 WeilinLiu2
ZhiyuMei1 GuangjuWang2 ChaoYu*1 YiWu*123
Abstract underscoredtheimportanceofaligningthesemodelswith
humanpreferences(Agrawaletal.,2023;Kadavathetal.,
ReinforcementLearningfromHumanFeedback 2022;Shietal.,2023;Liangetal.,2021;Shengetal.,2019).
(RLHF)iscurrentlythemostwidelyusedmethod
Variousmethodshavebeendevelopedforfine-tuningLLMs,
toalignlargelanguagemodels(LLMs)withhu-
withpopularapproachesincludingSupervisedFine-Tuning
man preferences. Existing RLHF methods can
(SFT)(Pengetal.,2023)andReinforcementLearningfrom
beroughlycategorizedaseitherreward-basedor
HumanFeedback(RLHF)(Ziegleretal.,2019;Stiennon
reward-free. NovelapplicationssuchasChatGPT
etal.,2020;Ouyangetal.,2022). Typically,fine-tuningin-
andClaudeleveragereward-basedmethodsthat
volvestwophases: SFTtoestablishabasemodel,followed
firstlearnarewardmodelandapplyactor-critic
byRLHFforenhancedperformance. SFTinvolvesimitat-
algorithms, such as Proximal Policy Optimiza-
inghigh-qualitydemonstrationdata, whileRLHFrefines
tion(PPO).However, inacademicbenchmarks,
LLMsthroughpreferencefeedback.
thestate-of-the-artresultsareoftenachievedvia
reward-freemethods,suchasDirectPreference WithinRLHF,twoprominentapproachesarereward-based
Optimization (DPO). Is DPO truly superior to and reward-free methods. Reward-based methods, pio-
PPO? WhydoesPPOperformpoorlyonthese neeredbyOpenAI(Ouyangetal.,2022;Ziegleretal.,2019;
benchmarks? Inthispaper,wefirstconductboth Stiennonetal.,2020),constructarewardmodelusingpref-
theoreticalandempiricalstudiesonthealgorith- erence data and then employ actor-critic algorithms like
micpropertiesofDPOandshowthatDPOmay Proximal Policy Optimization (PPO) to optimize the re-
havefundamentallimitations. Moreover,wealso wardsignal. Incontrast,reward-freemethods,includingDi-
comprehensivelyexaminePPOandrevealthekey rectPreferenceOptimization(DPO)(Rafailovetal.,2023),
factorsforthebestperformancesofPPOinfine- RRHF (Yuan et al., 2023), and PRO (Song et al., 2023),
tuningLLMs. Finally,webenchmarkDPOand eliminate the explicit use of a reward function. DPO, a
PPOacrossvariousacollectionofRLHFtestbeds, representative reward-free method, expresses the reward
ranging from dialogue to code generation. Ex- function in a logarithmic form of the policy and focuses
perimentresultsdemonstratethatPPOisableto solelyonpolicyoptimization.
surpassotheralignmentmethodsinallcasesand
Notably, the most successful applications like Chat-
achievestate-of-the-artresultsinchallengingcode
GPT(OpenAI,2022)andClaude(Antropic,2023)arepro-
competitions.
ducedbythereward-basedRLHFmethodPPO,whilestrong
performancesinacademicbenchmarksoftenresultfromthe
reward-freeRLHFmethodDPO(Rafailovetal.,2023;Mis-
1.Introduction tralAI, 2023). This discrepancy raises two fundamental
questions:1)IsDPOtrulysuperiortoPPOintheRLHFdo-
LargeLanguageModels(LLMs)derivetheirextensivelan-
main? and2)CantheperformanceofPPObesubstantially
guagepatternsandknowledgethroughpre-trainingonsub-
improvedincommonRLHFbenchmarks? Inthispaper,we
stantialtextualdatasets(Brownetal.,2020;OpenAI,2023;
delveintothesequestions. Throughtheoreticalandempir-
Touvron et al., 2023; Chowdhery et al., 2023; Anil et al.,
ical analysis, we uncover the fundamental limitations of
2023). ToleveragetheformidablecapabilitiesofLLMsin
DPOandexplorecriticalfactorsthatenhancethepractical
practical applications, a growing amount of research has
performanceofPPOinRLHF.
*Co-corresponding authors. 1Tsinghua University, Beijing
First,ourtheoreticalexaminationrevealsthatDPOmight
China2OpenPsiInc.3ShanghaiQiZhiInstitute,Shanghai,China.
find biased solutions that exploit out-of-distribution re-
Correspondenceto:ShushengXu<xssstory@gmail.com>,Chao
Yu<zoeyuchao@gmail.com>,YiWu<jxwuyi@gmail.com>. sponses. Empirically,wedemonstratethattheperformance
of DPO is significantly affected by the distribution shift
1
4202
rpA
61
]LC.sc[
1v91701.4042:viXraIsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
betweenthemodeloutputsandthepreferencedataset. Sec- andexplorethekeyfactorsforPPOtraining.
ond,weperformablationstudiesonthealgorithmiccom-
Concurrenteffortshavebeenundertaken toavoid reward
ponentsofPPOanddiscoveracollectionofcriticalfactors
modeloveroptimization(Rame´etal.,2024),facilitatealign-
forPPO’sbestRLHFperformances,includingadvantage
mentdatageneration(Leeetal.,2023;Yangetal.,2023),
normalization, large batch size, and exponential moving
andimplementresource-efficientRLHFsystems(Yaoetal.,
average update for the reference model. Finally, we vali-
2023;Santacroceetal.,2023).Theseworkscomplementour
dateourfindingsthroughextensiveexperiments,including
studyandcanbeseamlesslyintegratedintoourimplemen-
dialoguegenerationtasksandmorechallengingcodegen-
tation. Previousworkshaveexploredtheimplementation
erationtasks. Theseexperimentsfeaturediversefeedback
detailsofPPOforLLMs(Zhengetal.,2023;Ramamurthy
typesanddifficultylevels.TheresultsindicatethatPPOcon-
etal.,2023). Ourpaperextendsitsinvestigationswithad-
sistentlyoutperformsDPOacrossallexperiments. Particu-
ditionalRLHFtechniques,optimizingPPOperformanceto
larly,inthemostchallengingcodecompetitiontasks,PPO
surpassitsreward-freecounterpart,DPO.Thisisanaspect
achievesstate-of-the-artresults. Specifically,ontheCode-
notpreviouslyreportedintheliterature. Ourworkisalso
Contestdataset(Lietal.,2022),ourPPOmodelwith34B
closelyrelatedtostudiesonalgorithmimplementationin
parametersoutperformsAlphaCode-41B(Lietal.,2022),
theRLcommunity(Engstrometal.,2020;Andrychowicz
exhibitinga10@1kimprovementfrom16.4%to22.4%.
etal.,2021;Yuetal.,2022). However,ourfindingsprovide
furtherinsightsintofine-tuningLLMswithamodelsizeof
2.RelatedWork upto34Bparameters.
Large language models (LLMs) trained on large datasets
3.Preliminary
acquire surprising capabilities (Brown et al., 2020; Ope-
nAI,2023;Touvronetal.,2023;Chowdheryetal.,2023;
Language Model. We consider an LLM as a policy
Aniletal.,2023;Kaplanetal.,2020;Brownetal.,2020).
π (y | x) parameterized by θ. π is designed to follow
To leverage these capabilities to real applications, pre- θ θ
userinstructionsx∈X togenerateatextresponsey∈Y.
trained LLM is further fine-tuned on specific tasks (Rad-
We only consider single-round conversations to simplify
ford et al., 2019; Chung et al., 2022; Tay et al., 2023).
notations. Given a prompt x, the LLM π will generate
Throughfine-tuningwithpopularapproachessuchasSFT θ
responseyinanauto-regressivemanner:
andRLHF,LLMsdemonstrateimpressiveperformanceon
(cid:89)
established benchmarks (Touvron et al., 2023; OpenAI, π (y|x)= π (y |x,y ), (1)
θ θ t <t
2023),aligningfurtherwithhumanpreferencesandsocietal
t
well-being(Russell&Norvig,2020;Russell,2022).
wherey isthet-thtokenintheresponseandy istokens
t <t
ThispaperconcentratesonRLHFmethods,whichcanbe intheresponsebeforey t.
broadlycategorizedintoreward-basedandreward-freeap-
SFT.Asaninitialphaseofalignment,thepre-trainedmodel
proaches. Reward-basedmethodsentailtrainingareward
isenforcedtoimitatehigh-qualitydemonstrationdata(dia-
model on preference data in an initial phase (Gao et al.,
logue,summarization,etc.),whichisusuallyreferredtoas
2023; Ziegler et al., 2019; Stiennon et al., 2020; Ouyang
SupervisedFine-Tuning(SFT).
et al., 2022). Subsequently, this learned reward model is
utilizedtoprovidearewardsignalforonlineReinforcement RLHF. To further align the SFT model π θ with human
Learning (RL) algorithms such as PPO (Schulman et al., preference,priorworks(Ziegleretal.,2019;Ouyangetal.,
2017). Thereexistpreviousworksthathavestudiedthese 2022)proposedtheReinforcementLearningfromHuman
methodsthroughhyper-parametertuningandanalyzedthe Feedback(RLHF)procedure,whichmaximizesthefollow-
effects of the quality reward model quality (Zheng et al., ingobjective,
2023;Casperetal.,2023). Incontrast,reward-freemethods (cid:20) (cid:21)
π (y|x)
offerasimplertrainingprocedurebydirectlytrainingLLMs J r(π θ)=E x∼pdata,y∼πθ r(x,y)−βlog πθ (y|x) .
onpreferencedataorrankingdatatodistillhumanprefer- ref
(2)
ence(Yuanetal.,2023;Liuetal.,2023;Touvronetal.,2023;
whereristherewardfunctionreflectinghumanpreferences.
Rafailovetal.,2023;Songetal.,2023;Dongetal.,2023).
rtakesapromptandthecorrespondingresponseasinput
Among these reward-free methods, DPO (Rafailov et al.,
andoutputsascalarvalue. π isthereferencemodelused
ref
2023)hasdemonstratedstrongperformancesandbecome
forregularizingπ withKullback–Leiblerdivergence. β is
θ
popular in the community (MistralAI, 2023; Chen et al.,
aconstanttocontrolthedegreeofregularization.
2024;Yuanetal.,2024). Thispaperexaminesrepresenta-
tivesfrombothcategories,specificallyPPOandDPO.We In the rest of this section, we will introduce two repre-
analyzethelimitationsofDPOtheoreticallyandempirically sentative algorithms to optimize Eq. (2): a reward-based
approach,PPO,andareward-freeapproach,DPO.
2IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
PPO.Wecandirectlyadoptstandardreinforcementlearning Action y y y
1 2 3
methodsforEq.(2).Inthispaper,wechosePPOasthetrain-
π 0.5 0.5 0
ingalgorithm. Whenrisunknown,arewardmodelr ∈R ref
ϕ
D {(y =y ,y =y )}
is first learned from human-labeled data to approximate pref w 1 l 2
r. Acommonpracticeistocollectadatasetofpreference π 0.1 0.0 0.9
DPO
pairsD ={(x,y ,y )}. y andy areresponsestoxand
w l w l π 1 0 0
PPO
marked as “win” and “lose” by human respectively. The
distributionofthepreferencedatasetisassumedtofollow
Table1.A state-less counter-example with three actions when
theBradley-Terrymodel(Bradley&Terry,1952;Christiano
DPO can minimize the loss but produce an unexpected policy.
etal.,2017), i.e., theprobabilityofresponsey w isbetter PPOwillnotproduceπ DPObecauseπ ref enforcestheprobability
thany l isgivenby ofoutputtingy 3iszero.
exp(r (x,y ))
P (y ≻y |x)= ϕ w
ϕ w l exp(r (x,y ))+exp(r (x,y ))
ϕ w ϕ l WeremarkthatalthoughRafailovetal.(2023)performsa
=σ(r (x,y )−r (x,y )). (3) single-roundDPOoverthepreferencedataset,somerecent
ϕ w ϕ l
worksalsoadaptDPOtoaniterativevariantwithalearned
whereσisthesigmoidfunction. GivenD,r istrainedby
ϕ rewardmodel(Yuanetal.,2024). Wealsoinvestigatethe
minimizingthenegativelog-likelihoodofEq.(3):
performanceofiterativeDPO.
L (r )=−E [logσ(r (x,y )−r (x,y ))]
R ϕ (x,yw,yl)∼D ϕ w ϕ l
(4) 4.UnderstandingtheLimitationofDPO
Afterarewardmodelr isobtained,risreplacedwithr
ϕ ϕ In this section, we demonstrate that DPO may not be su-
andJ (π )couldbeexplicitlyoptimizedwithonlineRL
rϕ θ periortoPPO.Firstly,wetheoreticallydemonstrateissues
algorithms. Wenotethatthereexistcaseswhenaground-
withtheDPOtrainingobjective. Secondly,weillustratethat
truthrewardisavailable,andthusrewardmodelingbecomes
DPOismoresusceptibletoout-of-distribution(OOD)data
unnecessary(Zhangetal.,2020;Sellametal.,2020;Rama-
throughasyntheticexample.Lastly,throughexperimentson
murthyetal.,2023). Inthesecases,therewardfunctioncan
arealpreferencedataset,wevalidatethattheperformance
bedirectlyincorporatedintoEq.(2). Whileweacknowl-
ofDPOcanbeimprovedbymitigatingthedistributionshift
edgeotheractor-criticalgorithmscanalsobefeasible(Mnih
betweenthemodeloutputsandthepreferencedataset.
et al., 2016; Haarnoja et al., 2018), we follow the main-
streamwork(Ziegleretal.,2019;Stiennonetal.,2020)and
4.1.TheoreticalAnalysis
focusonPPO(Schulmanetal.,2017)forouranalysisin
thispaper. Itiswell-knownthatPPOcouldexploitpotentialfailures
inthelearnedrewardmodeltoachievehighrewardswith-
DPO.Insteadoflearningarewardmodel,DirectPreference
outmeetingtheactualhumanpreference,oftenmanifested
Optimization(DPO)(Rafailovetal.,2023)optimizesthe
as erroneous (Lewis et al., 2017) or overly complex out-
policy π over preference data. DPO derived the closed-
θ
puts (Singhal et al., 2023). We argue that, though DPO
form solution of Eq. (2), which reveals the relationship
avoidsrewardmodeling,DPOhasasimilargeneralization
betweentherewardr(x,y)andtheoptimallanguagemodel
π∗(y|x): issue. In the following theorem, we will show that any
solutionfoundbyPPOalsominimizestheDPOobjective
(cid:18) (cid:19)
π∗(y|x)= 1 π (y|x)exp 1 r(x,y) , (5) Eq.(7),andthus,anysolutionfoundbyPPOthatexploits
Z(x) ref β therewardmodelcanalsobefoundbyDPO.Furthermore,
DPOmightdiscoversolutionsexploitingout-of-distribution
where Z(x) is a partition function that only depends on
data,posingariskofdeviatingexcessivelyfromtherefer-
promptx. AccordingtoEq.(5),ifπ maximizesJ (π ),
θ rϕ θ
encepolicyevenwhenthereferencepolicyalignswellwith
theunderlyingrewardcanbederivedwith
humanpreferences.
π (y|x)
r (x,y)=βlog θ +C(x). (6) Theorem4.1. Givenaground-truthrewardrandaprefer-
ϕ π (y|x)
ref encedatasetD,letΠ betheclassofpoliciesinduced
PPO
whereC :X →Risascalarfunction. Thisenablesusto bytrainingrewardmodelr overD andrunningPPOto
ϕ
r de rp iva era tm heet Der Pi Oze lE osq s. t( h4 a) tw di it rh ect th le yp oo pl ti ic my izπ eθ s, πand ,it .h e.e ,nwecan optimizeJ rϕ(θ). LetΠ DPObetheclassofpoliciesinduced
θ byminimizingDPOobjectiveEq.(7).Wehavethefollowing
L DPO(π θ)=−E
(x,yw,yl)∼D
(7) conclusion: Π PPOisapropersubsetofΠ DPO.
(cid:20) (cid:18) (cid:18) (cid:19)(cid:19)(cid:21)
π (y |x) π (y |x)
logσ β log θ w −log θ l .
π ref(y w |x) π ref(y l |x) Proof. WefirstprovethatΠ PPO isasubsetofΠ DPO,i.e.
3IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
Π ⊆Π . LetRbetheclassofrewardmodelsthat
PPO DPO
minimizesrewardlearninglossEq.(4). Wenotethereis
anone-to-manymappingbetweenΠ andRaccording
PPO
toEq.(6). Withoutlossofgenerality, weomitthescalar
factorC(x)anddefinef tobeanone-to-onemappingfrom
apolicytoarewardfunctionf(π)(x,y)=βlog π(y|x) .
πref(y|x)
WewillshowthattheminimumDPOlossisthesameas
the minimum reward learning loss, i.e., min L (r) =
r R
min L (π). We can show that min L (π) =
π DPO π DPO
min L (f(π))≥min L (r). Andforaminimizerr∗of
π R r R
L (r),wecanconstructapolicyπ∗fromr∗byEq.(5)and
R
thenmin L (π) ≤ L (π∗) = min L (r). There-
π DPO DPO r R
foretherewardlearninglossachievedbyrewardmodelsin
RandDPOlossachievedbypoliciesinΠ arethesame,
DPO
i.e. ∀r ∈ R,π ∈ Π ,min L (r) = L (r ) =
ϕ DPO DPO r R R ϕ
L (π )=min L (π).
DPO DPO π DPO
ForanysolutionfoundbyPPO,π ∈Π ,thereward
PPO PPO
r∗ =f(π )satisfiesthatπ isamaximizerofJ (π)
PPO PPO r∗
andπ canberepresentedbyr∗with
PPO
(cid:18) (cid:19)
1 1
π (y|x)= π (y|x)exp r∗(x,y) . (8)
PPO Z(x) ref β
Substituting π with Eq. (8) in L (π ), we get
PPO DPO PPO
Figure1.Preferencedatasetcoverage,policyprobabilitydistribu-
L (π )=L (r ). Therefore,π alsominimizes
DPO PPO R ϕ PPO tionsofπ ,π ,π ,andthevalueoflearnedrewardsin
theDPOloss,whichimpliesπ ∈Π . ref PPO DPO
PPO DPO thesyntheticscenario. Inthefirstfigure, darkcolorrepresents
Next,weshowthatΠ isapropersubsetofΠ ,i.e. datapresentinpreferencedata,whilelightmeansthedatapoints
PPO DPO
Π ⊊ Π with a counter-example as shown in Ta- arenotincluded. Althoughdatapointsmarkedwithredcircles
PPO DPO
andorangecirclesarenotcoveredbythepreferencedataset,DPO
ble 1. In this counter-example, we will show that there
assignshigherprobabilitiesofthesedatapointscomparedwiththe
existsasolutionfoundbyDPO,π ∈Π ,thatdoes
DPO DPO referencemodel.PPOassignslowprobabilitytothemarkeddata
notmaximizetheRLobjectiveofPPOEq.(2). Considera
pointsandlearnstheoptimalpolicy.
simplestate-lesscasewiththreeactions,butthepreference
datasetonlycontainsasinglepaircomparisonbetweeny 1 favoringunseenresponses,directlyimpactingqualityofthe
andy 2. DenotetheprobabilityofDPOpolicyπ DPO out- learnedpolicy. Bycontrast,PPOcanleverageprompt-only
puttingthefirsttwoactionsasaandb. TheDPOlossinthis dataandgenerateresponsesbeyondthepreferencedataset
scenarioisgivenbyL DPO =log(1+( ab)β),whichcanbe distribution. During training, KL divergence between π θ
minimizedaslongasb=0. Apossibleoptimalpolicypro- andπ canprovideadditionalregularizationforPPOon
ref
ducedbyDPOisshowninthethirdrowofTable1,which thesegeneratedsamples.
hasa0.1probabilitytooutputy anda0.9probabilityto
1
outputy . ThispolicycannotbeproducedbyPPObecause
3 4.2.EmpiricalValidationinASyntheticScenario
π enforcesπ toassign0probabilitytoy according
ref PPO 3
toEq.(8). We design a synthetic scenario to validate Theorem 4.1
in practice. We create discrete spaces of prompts and re-
sponses,bothofsize8. Thepolicyπ andrewardmodelr
θ ϕ
aremodeledasMLPs,whichtakeaone-hotvectorasinput
Weremarkthattherootcauseofrewardmisspecificationis and output a categorical distribution of overall responses.
thenarrowdistributioncoverageofthepreferencedataset. Wemanuallyenforcetheoptimalresponsetobediagonal
Thelearnedrewardmodelmayassignahighvaluetoout- indices. Thepreferencedatasetisrandomlycreatedunder
of-distribution(OOD)samplesandhasthepotentialtobe thisconstraintandonlycoverslimitedpreferencepairsfor
exploitedduringtheRLprocess. AlthoughDPOavoids each input. The resulting policies of DPO and PPO are
trainingtherewardmodel,itstillsuffersfromthemis- showninFigure1. Wecanseethatinpractice,DPOand
specification issue on OOD samples but in a different thelearnedrewardmodelcanassignhighvaluestothere-
manner.Specifically,DPOcandevelopabiaseddistribution sponseoutofthedistributionofpreferencedataset,which
4IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
aremarkedusingcircles.InthecaseofDPO,thefinalmodel ∆Help.↑ Harm.↓ S.R.↑
mayassignhigherprobabilitiesthanthereferencemodelto
SFT(Alpaca) -2.62 1.50 41.6%
theseresponses,whichisnotdesirableasperformanceim-
PPO 1.69 -12.08 99.5%
provementonOODresponsescouldnotbeguaranteed. For
+(Safe) 4.47 -12.33 99.6%
example,intheredcircles,DPOincreasestheprobability
DPO -4.19 -0.97 55.4%
from0.11to0.23. Incontrast,thoughtherewardmodelhas
+SFT(Safe) -1.62 -3.50 71.8%
asimilarmisspecificationissue,PPOcanalleviatetheissue
+filterdual-unsafe 2.46 -4.88 80.8%
withexplicitKLregularizationw.r.t. thereferencemodel. +filterdual-safe -2.86 -6.82 95.8%
PracticalRemark: Fromtheanalysisinthissection,we DPOIter.1 -3.22 -5.23 86.7%
attempttoprovideinsightstounderstandtheperformanceof DPOIter.2 -3.27 -8.83 99.7%
DPOIter.3 -3.26 -10.21 99.9%
DPOinpractice—DPOispronetogeneratingabiased
DPOIter.4 -2.96 -11.07 99.9%
policythatfavorsout-of-distributionresponses,leading
tounpredictablebehaviors. Wewillfurthervalidatethese
Table2.TheimpactoftrainingdataonDPO.WefirsttrainLlama-
insightsthroughanexperimentalstudyinvolvingLLMson 2-7BontheAlpacaopen-sourcedatasetandobtainSFT(Alpaca).
realpreferencedatasets. ThentheSFTmodelistrainedwithDPOandPPO.DPOperforms
poorlyduetodistributionmismatchandnoises.Theseissuescan
4.3.ExperimentsonRealPreferenceDatasets beresolvedby(1)additionalSFTonthepreferencedataset(SFT
(Safe)),(2)filteringoutcontroversyandnoisypreferencepairs,and
Inthissection,weconductexperimentsonrealpreference (3)generatingnewresponsesandusingalearnedrewardmodelto
datasetsandinvestigatetwoaspectsthatmayinfluenceDPO labelthepreferencedataforiterativeDPOtraining.
performance,includingthebasemodelandpreferencedata
showninTable2,resolvingthedistributionshiftissueessen-
usedforDPOtraining.
tiallyincreasesthesafetyrateby16.4%andthehelpfulness
rewardfrom−4.19to−1.62.
ExperimentalSetup. Weperformourexperimentalanal-
Sensitivity to Preference Data. There exist pairs
ysisonSafeRLHFdataset(Daietal.,2023). Inthisdataset,
(x,y ,y )intheSafeRLHFdatasetwherebothy andy
preference pairs have the form (x,y ,y ,l ,l ,b ,b ), 1 2 1 2
1 2 h s 1 2 have the same safety label. After filtering out the dual-
wherel andl areapreferencelabelsovery andy in
h s 1 2 unsafe and dual-safe preference data in the dataset, the
termsofhelpfulnessandsafety,respectively,whichcouldbe
trainedmodelcouldobtainamuchhighersafetyrate. How-
either1or2. b andb arebinarysafetylabelsofthesetwo
1 2 ever,filteringthedual-safepreferencedatawouldlargely
responses.Withthisdataset,ourobjectiveistotrainanLLM
hurttheperformanceofhelpfulness. Theseresultssuggest
thatprioritizessafetyoverhelpfulnessincontentgeneration.
that while DPO may derive advantages from eliminating
Specifically,inconstructingthepreferencedataset,ourpref-
noiseorcontroversiesinthetrainingdata,excessivelydis-
erenceisforthemorehelpfulresponsewhenbothoptions
cardinghigh-qualitydatacouldbedetrimentaltoDPOper-
areconsideredsafe. However,ifsafetyvariesbetweenthe
formance.
responses, our preference shifts towards the safer option.
FollowingDaietal.(2023),thebasemodelistrainedonthe ImpactofPreferenceDataDistribution.Whilemitigating
Alpaca(Taorietal.,2023)open-sourcedatasetwithSFT, thedistributionshiftcanbedonewithadditionalSFT,we
denoted as SFT (Alpaca). We use the evaluation models alsoinvestigatewhethercollectingadditionaldatawiththe
releasedbytheofficialcodebasetoevaluatethehelpfulness basemodelcouldbringbenefit. Specifically,insteadofus-
andharmfulness. Weremarkthattheseofficialevaluation ingtheexistingpreferencedata,wegeneratenewresponses
modelsarenotinvolvedduringthePPOtraining. Ourex- withSFT(Safe)andusealearnedrewardmodelforprefer-
perimentalstudyisshowninTable2. Ourobservationsare encelabeling. Wefurtherrepeatthisprocessanditeratively
summarizedbelow. setthereferencemodelasthelatestDPOmodelinthelast
iteration. WedenotethismethodasDPO-Iter. Remarkably,
Impact of The Base Model. When using SFT (Alpaca)
DPO-IterachievesacomparablesafetyratewithPPO.This
as the base and reference model, we find that DPO per-
experimentagaindemonstratesthatDPOcouldbeimproved
formspoorly,producingonlya55.4%safetyrateandlow
bymitigatingthedistributionshift. However,italsoobtains
helpfulnessreward. Wehypothesizethatthisiscausedby
amuchlowerhelpfulnessrewardcomparedtoPPO.
thedistributionshiftbetweenthetrainingdataofthebase
model,i.e.,theAlpacadataset,andthepreferencedata,i.e., Practical Remark: The performance of DPO could be
the SafeRLHF dataset. To study the impact, we further improvedbymitigatingthedistributionshiftbetweenthe
fine-tuneSFT(Alpaca)ontheSafeRLHFdatasetwithsafe modelandthepreferencedataset. Toalleviatetheissueof
responses to obtain SFT (Safe). We then use SFT (Safe) distributionshiftandnoisydata,wesuggestadoptingthe
as the reference model to re-train DPO from scratch. As iterativeDPOmethod. Oneshouldcarefullyannotatethe
5IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
Task HH-RLHF APPS CodeContest
OpenAssaint Intro. Inter. Comp.
Metric pass@10 pass@100 pass@1k
Reward pass@5 pass@5 pass@5
SFT 0.532 38.6% 10.1% 3.9% 0.9% 4.3% 12.0%
baselinePPO 0.706 18.0% 2.4% 1.1% 4.3% 6.0% 7.7%
+Adv.Norm. 0.716 38.1% 11.4% 4.6% 6.8% 9.4% 15.4%
+Large.Batch. 0.716 42.3% 14.6% 7.5% 5.1% 12.8% 19.6%
+Ref.EMA 0.718 44.4% 18.0% 9.1% 6.8% 13.7% 21.4%
Table3.AblationstudyofPPOondifferenttasks.BaselinePPOistrainedwithabatchsizeof64.Specifically,fortheHH-RLHFtask,
thebasemodelemployedisLlama2-7B.InthecaseofAPPSandCodeContesttasks,thebasemodelutilizedisCodeLlama-34B.
iarySFTlossduringPPOtrainingbecauseofthelimited
amount of data. This implementation includes common
PPOtechniquessuchasvaluelossclipandgeneralizedad-
vantageestimation(GAE)(Schulmanetal.,2016). Welist
experimentdetailsinAppendixA.2.
ExperimentalSetup.OurablationexperimentsforPPOare
carriedoutonadialoguetaskHH-RLHF(Baietal.,2022)
as well as two code generation tasks: APPS (Hendrycks
etal.,2021)andCodeContest(Lietal.,2022). HH-RLHF
is a preference dataset in the form defined in Section 3
thataimstotrainahelpfulandharmlessLLM.APPSand
Figure2.PerformanceofPPOonAPPSdatasetunderdifferent
CodeContestdatasetsarecompetitiveprogrammingdatasets.
batch sizes. The base LLM is CodeLlma-13B. “Introductory”,
Given a problem, the LLM should output a piece of exe-
“Interview”and“Competition”representthreelevelsofdifficulty.
cutablecodetosolvethisproblem. Thecorrectnessisver-
ifiedbytestcasesinthedataset,whichcanthengenerate
model-generatedsampleseachtimeandthenproceedtothe rewardsignalsorpreferencepairsforPPOandDPOtrain-
next round of training. However, we will demonstrate in ing,respectively. Weremarkthatthesetwotypesoftasks
Sec.6thatevenwithanearlyperfectannotator,theperfor- feature different types of reward signals: preference and
manceofDPOremainsunsatisfactoryinchallengingtasks directrewardfeedback. Thecompleteexperimentalsetup
suchascodegeneration. islistedinSection6. Intheexperimentresult,wedenote
advantage normalization as Adv. Norm., large batch-size
trainingasLargeBatchandexponentialmovingaverageof
5.KeyFactorstoPPOforRLHF
referencemodelupdateasRef. EMA.
Inthissection,weinvestigatethekeyfactorstotheRLHF
Analysis. TheresultoftheablationstudyisshowninSec-
performance of PPO. We find three key techniques: (1)
tion4.3. InSection4.3,withasmallbatchsize,baseline
advantage normalization (Raffin et al., 2021), (2) large-
PPOimprovesovertheSFTmodelonHH-RLHFandCode-
batch-sizetraining(Yuetal.,2022),and(3)updatingthe
Contestdatasetbutshowssignificantperformancedegrada-
parametersofthereferencemodelwithexponentialmoving
tionontheAPPSdataset. Advantagenormalizationstabi-
average (Ouyang et al., 2022). The first two techniques
lizesPPOtrainingandimprovestheperformanceofPPO.
arewidelyadoptedbytheRLcommunitybutarenotwell-
Themostsignificantbenefitisbroughtbyusingalargebatch
studiedinthefieldofRLHF.Thethirdisatechniquethathas
size,especiallyoncodegenerationtasks. Lastly,usingthe
receivedlimiteddiscussionintheliterature,involvingthe
exponentialmovingaverageforthereferencemodelalso
gradualupdateofthereferencemodelthroughanexponen-
bringsadditionalbenefits. Theintuitionbehindthisisthat
tialmovingaverage(Ouyangetal.,2022). Thisparticular
whilethemainLLMofPPOisrapidlychanging,therefer-
approachhasthepotentialtoyieldadditionalperformance
encemodelshouldalsobeupdatedaccordingly. Otherwise,
enhancements.
thelearnedmodelmaybestronglyregularizedtobeclose
totheSFTmodel,whichcanhurtperformanceinchalleng-
Implementation Details. Our PPO implementation is
ingtasks. Figure2furtherdemonstratesthatincreasingthe
basedonDeepSpeed-Chat(Yaoetal.,2023),exceptthat(1)
batchsizeofPPOconsistentlyimprovestheperformance
weuseascalarrewardforeachresponseinsteadofdense
across all difficulty levels in the APPS dataset. We also
rewardsassignedoneachtokenand(2)weomittheauxil-
6IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
OpenAssistant TestedV.S.Chosen TestedV.S.SFT
Reward TestedWin↑ Tie ChosenWin↓ TestedWin↑ Tie SFTWin↓
RRHF 0.523 28 33 39 29 37 34
PRO 0.529 37 26 37 34 33 33
DPO 0.611 55 21 24 53 31 16
DPO-Iter 0.678 55 18 27 54 33 13
PPO 0.718 57 21 22 58 29 13
Table4.ResultsontheHH-RLHFtestset.TheevaluationmetricsincludetheOpenAssistantrewardsandthewinrateofmodelsagainst
thechosenresponsesandSFTmodeloutputs.TheOpenAssistantrewardmodelisnotusedduringthetrainingprocess.NotethatDPOis
trainedonthepreferencedatainthedataset,whileIter.DPOistrainedonself-generatedresponses,usingarewardmodelforlabeling.
PPOWin Tie DPOWin Model Method Intro. Inter. Comp.
PPOV.S.DPO 42 28 30 GPT-Neo2.7B SFT 5.6% 0.8% 0.0%
PPOV.S.DPO-Iter 36 36 28 Codex12B SFT 9.7% 0.5% 0.1%
CodeT5 CodeRL 16.4% 4.9% 2.8%
Table5.OnHH-RLHF,weuseGPT-4tocomparetheoutputsof AlphaCode1B 5@1k 14.4% 5.6% 4.6%
thePPOandDPOmodels. Fewshot 10.8% 2.0% 0.8%
CodeLlama SFT 30.0% 7.8% 2.8%
7B DPO-Iter 20.9% 3.4% 1.3%
highlight that utilizing a small batch size, such as 64, in PPO 29.4% 7.6% 2.4%
PPOtrainingcouldnegativelyimpacttheperformanceof
Fewshot 23.7% 5.6% 2.1%
thebaseSFTmodel,resultingina33.7%performancelevel
CodeLlama SFT 33.7% 8.7% 3.6%
ontheintroductoryscale. Weremarkthatourfindingsare 13B DPO-Iter 33.0% 8.0% 2.8%
consistentwiththosedevelopedintheRLcommunity(Yu PPO 36.4% 11.47% 4.6%
etal.,2022). Fewshot 32.8% 8.8% 2.9%
CodeLlama SFT 38.6% 10.1% 3.9%
34B DPO-Iter 34.2% 9.3% 3.7%
6.BenchmarkResults
PPO 44.4% 18.0% 9.1%
Inthissection,weconductexperimentalvalidationstoeval-
Table7.ResultsonAppstestset. Allthenumbersarepass@5
uatetheperformancesofbothDPOandPPO.Initially,our
exceptforAlphaCode.Where“5@1k”meansthismodelsamples
experiments focus on general dialogue tasks, specifically 1000timesforeachproblemand5sampledcodesthatpassthe
HH-RLHFandSafeRLHF.Theprimarygoalistoimprove publictestcases(intheproblemdescription)areselectedtobe
theeffectivenessofLLMbypromotingconstructiveinter- evaluatedonhiddentestcases.
actionsandmitigatingdetrimentalcomponentswithinthe
model. Additionally,ourinvestigationextendstodemand- onLlama2-7B.Weevaluatethetrainedmodelsusingthe
ingcodegenerationtasks,namelyAPPSandCodeContest. OpenAssistantrewardmodel1. Notethatthismodelisonly
usedforevaluationandisnotinvolvedduringtraining. In
LLM Method ∆Help.↑ Harm.↓ S.R.↑ addition,weadoptGPT-4tocomparetheresponsesofdif-
Beaver - -6.59 89.6% ferentmodels. Thepromptandevaluationdetailsarelisted
inAppenidxB.
SFT -2.26 0.78 46.5%
Llama1 DPO -2.70 -6.38 93.1% AsshowninTable4,exceptDPOandPPO,wealsoinves-
7B DPO-Iter -2.79 -11.86 100.0%
tigateotheralignmentmethodssuchasRRHF(Yuanetal.,
PPO +0.66 -10.22 98.6%
2023)andPRO(Songetal.,2023). Theresultsdemonstrate
SFT -2.12 0.00 52.1%
thatPPOandDPOaremuchmorepreferredbyGPT-4than
Llama2 DPO -2.86 -6.82 95.8%
the chosen responses in the dataset and SFT model out-
7B DPO-Iter -2.96 -11.07 99.9%
PPO +1.69 -12.08 99.5% puts,outperformingRRHFandPROacrossallmetrics. In
thispaper,wefocusmoreontheperformanceofDPOand
Table6.ResultsonSafeRLHF.“Beaver”istheofficiallyreleased
PPO.WeobservethatDPO-IterperformsbetterthanDPO
model. “∆Help.” denoteshelpfulnessrelativetoBeaver. “S.R.”
but worse than PPO. PPO consistently achieves a higher
denotessafetyrate.Thereportedresultsarebasedontheofficial
rewardandhigherwinrates. WealsouseGPT-4tocompare
evaluationmodel.
the outputs of DPO and PPO directly, and the results are
HH-RLHF (Bai et al., 2022) dataset consists of human
preferencesonAIassistantresponses,encompassing170k 1https://huggingface.co/OpenAssistant/oasst-rm-2-pythia-
comparisons. Inthisdataset,weconductexperimentsbased 6.9b-epoch-1
7IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
sizes. Incontrast,forPPO,asthemodelsizeincreases,the
Valid.Set TestSet
Model Method 10@1k 10@1k improvementismoreapparent.Weremarkthatthefeedback
usingtestcasesisnearlyperfect. However,theperformance
AlphaCode9B - 16.9% 14.3%
ofDPO-Iterremainsunsatisfactory.
- 16.9% 15.6%
AlphaCode41B CodeContest(Lietal.,2022)isamorechallengingcom-
+clustering 21.0% 16.4%
petitiveprogrammingdatasetconsistingofseveralprogram-
SFT 10.3% 15.2%
minglanguages. Here,weonlyusePythoncode. Weadopt
DPO 0.0% 0.0%
CodeLlama34B DPO-Iter 3.5% 3.2% a similar way to train PPO as in the APPS dataset. For
PPO 19.7% 22.4% DPOtraining,weconstructthepreferencedatasetbyusing
thecorrectandincorrectcodesprovidedbythedataset. To
Table8.PassrateonCodeContestsdataset.“10@1k”meansthat compare with previous work, we adopt k@n to evaluate
1000 samples will be evaluated on public tests in the problem the generated code, which means that n samples will be
description,andonly10ofthemwillbesubmittedforhiddentests. evaluatedonpublictestsintheproblemdescription,andk
WeonlyusedPythonforsolvingproblems,whileAlphaCode
ofthemwillbesubmittedforhiddentests.
usedbothPythonandC++.
TheresultsarelistedinTable8. Weobtainedsimilarcon-
listedinTable5,whichdemonstratesthatGPT-4prefersthe clusionsasinAPPS.PPOimprovestheSFTmodelsignifi-
responsesofPPO. cantly,whileDPOfailstogenerateanycorrectcodes. After
oneepochoftraining,thecodewrittenbytheDPOmodel
SafeRLHF(Daietal.,2023)datasetcomprisesover30k
has achieved a pass rate of 0, we observe that the DPO
entriesofexpertcomparisondata. Eachentryinthisdataset
modeloutputsmanymeaninglesscodesnippets. Theresults
containstworesponsestoaquestion. Inourexperiments,
alsodemonstratethatDPO-Iterperformsworsecompared
weconsolidatetwopreferencesasmentionedinSection4.3.
toSFT.WiththeassistanceofPPO,CodeLlama-34Bhas
For evaluation, we borrow the official reward model and
costmodel2,whicharetrainedtoevaluatehelpfulnessand surpassedthepreviousstate-of-the-artonthistask,outper-
formingAlphacodewith41billionparameters.
harmlessness,respectively.
TheresultsonSafeRLHFarelistedinTab6. Experiments
7.Conclusion
indicatethatafteralignment,bothDPOandPPOcangen-
erateresponseswithlessharm,whilePPO’sresponsesare In this paper, we uncover the fundamental limitations of
morehelpful. DPOandexplorecriticalfactorsthatenhancethepractical
performanceofPPOinRLHF.Throughtheoreticalandex-
APPS (Hendrycks et al., 2021) is a description-to-code
perimentalanalysis,weexplorethelimitationsofDPOand
generationbenchmarkfromcompetitiveprogrammingplat-
findthatDPOissensitivetothedistributionshiftbetween
forms. Foreachquestion,therearealsotestcasestoverify
thebasemodeloutputsandpreferencedata.Wesuggestthat
the accuracy of generated codes. We use thesetest cases
iterativeDPOisbetterthantrainingonstaticdata. However,
inthetrainingsettoprovidefeedback. ForPPOtraining,
wealsofindthatDPOfailstoimprovetheperformanceon
thefeedbackcouldbedirectlyusedasareward. Wesimply
challengingtaskssuchascodegeneration. Moreover, ac-
definetherewardas10ifthegeneratedcodepassesalltest
cordingtotheablationstudy,wesummarizethekeyfactors
cases. Otherwise,therewardis0. ForDPO,sincethereare
forPPOtraining,includingadvantagenormalization,large
nopreferencepairs, weadoptDPO-Iter. Specifically, we
batch size, and updating the parameters of the reference
usethebasemodeltosample5codesforeachpromptand
modelwithanexponentialmovingaverage. Withourpracti-
utilizethetestcasestolabelthecorrectnessofgenerated
caltuningguideline,PPOdemonstratesrobusteffectiveness
codes. Itisworthnotingthatformanyprompts,thebase
acrossdiversetasksandachievesstate-of-the-artresultsin
modelmayfailtosampleanycorrectanswer. Insuchcases,
challengingcodecompetitiontasks.
we use the correct solutions from the dataset as y . We
w
evaluatetheresultsusingpass@k,whichisdefinedasthe Therearealsolimitationsinourwork. Therewardmodel
proportionofproblemssuccessfullysolvedbyemployingk is significant in the training processes of both PPO and
generatedprogramsforeachproblem. DPO-Iter. However,inthispaper,wehavenotdelvedinto
thediscussionofhowtoeffectivelytrainarobustreward
AsshowninTable7. Weconductexperimentsondifferent
model. Forthecodecompetitiontask,weutilizetheground-
model sizes. In particular, when using CodeLlama-34B
truthrewardforPPOtrainingandthelabelingofDPO-Iter.
asthebasemodel,weachievedstate-of-the-artresultson
However,thisdoesnotaffecttheconclusionsdrawninour
the APPS dataset. We can observe that DPO-Iter fails to
paper,andweleaveitasfutureworks.
improve the SFT model performances on all the model
2https://github.com/PKU-Alignment/safe-rlhf
8IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
ImpactStatements Brown,T.B.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,
J.,Dhariwal,P.,Neelakantan,A.,Shyam,P.,Sastry,G.,
OurstudyinvestigatesacriticalchallengeinaligningLarge
Askell,A.,Agarwal,S.,Herbert-Voss,A.,Krueger,G.,
LanguageModels(LLMs)withhumanpreferences,empha-
Henighan,T.,Child,R.,Ramesh,A.,Ziegler,D.M.,Wu,
sizingitssocietalimpact,includingtheeliminationofbias
J.,Winter,C.,Hesse,C.,Chen,M.,Sigler,E.,Litwin,M.,
andthereductionofunfairness. Theuseofapublicdataset
Gray,S.,Chess,B.,Clark,J.,Berner,C.,McCandlish,S.,
ensurestransparency,mitigatingconcernsrelatedtoprivacy
Radford, A., Sutskever, I., and Amodei, D. Language
andethicalconsiderations. Thisresearchemphasizesour
models are few-shot learners. In Larochelle, H.,
dedicationtoresponsibleAIpractices,aimingtoimprove
Ranzato,M.,Hadsell,R.,Balcan,M.,andLin,H.(eds.),
societalwell-beingbyaligningLLMswithhumanvalues
AdvancesinNeuralInformationProcessingSystems33:
whileupholdingrigorousstandardsforprivacyandethics.
Annual Conference on Neural Information Processing
Systems 2020, NeurIPS 2020, December 6-12, 2020,
References virtual, 2020. URL https://proceedings.
neurips.cc/paper/2020/hash/
Agrawal, A., Mackey, L., and Kalai, A. T. Do language
1457c0d6bfcb4967418bfb8ac142f64a-Abstract.
models know when they’re hallucinating references?
html.
CoRR, abs/2305.18248, 2023. doi: 10.48550/ARXIV.
2305.18248. URLhttps://doi.org/10.48550/ Casper, S., Davies, X., Shi, C., Gilbert, T. K., Scheurer,
arXiv.2305.18248. J., Rando, J., Freedman, R., Korbak, T., Lindner, D.,
Freire, P., Wang, T., Marks, S., Se´gerie, C., Carroll,
Andrychowicz,M.,Raichuk,A.,Stanczyk,P.,Orsini,M.,
M., Peng, A., Christoffersen, P. J. K., Damani, M.,
Girgin,S.,Marinier,R.,Hussenot,L.,Geist,M.,Pietquin,
Slocum, S., Anwar, U., Siththaranjan, A., Nadeau, M.,
O., Michalski, M., Gelly, S., and Bachem, O. What
Michaud, E. J., Pfau, J., Krasheninnikov, D., Chen,
mattersforon-policydeepactor-criticmethods? Alarge-
X., Langosco, L., Hase, P., Biyik, E., Dragan, A. D.,
scalestudy. In9thInternationalConferenceonLearn-
Krueger,D.,Sadigh,D.,andHadfield-Menell,D. Open
ingRepresentations,ICLR2021,VirtualEvent,Austria,
problemsandfundamentallimitationsofreinforcement
May 3-7, 2021. OpenReview.net, 2021. URL https:
learningfromhumanfeedback. CoRR,abs/2307.15217,
//openreview.net/forum?id=nIAxjsniDzg.
2023. doi: 10.48550/ARXIV.2307.15217. URLhttps:
Anil,R.,Dai,A.M.,Firat,O.,Johnson,M.,Lepikhin,D., //doi.org/10.48550/arXiv.2307.15217.
Passos,A.,Shakeri,S.,Taropa,E.,Bailey,P.,Chen,Z.,
Chen,Z.,Deng,Y.,Yuan,H.,Ji,K.,andGu,Q. Self-play
Chu, E., Clark, J. H., Shafey, L. E., Huang, Y., Meier-
fine-tuningconvertsweaklanguagemodelstostronglan-
Hellstern, K., Mishra, G., Moreira, E., Omernick, M.,
guagemodels. arXivpreprintarXiv:2401.01335,2024.
Robinson, K., Ruder, S., Tay, Y., Xiao, K., Xu, Y.,
Zhang, Y., A´brego, G.H., Ahn, J., Austin, J., Barham, Chowdhery,A.,Narang,S.,Devlin,J.,Bosma,M.,Mishra,
P., Botha, J. A., Bradbury, J., Brahma, S., Brooks, K., G., Roberts, A., Barham, P., Chung, H. W., Sutton,
Catasta, M., Cheng, Y., Cherry, C., Choquette-Choo, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko,
C. A., Chowdhery, A., Crepy, C., Dave, S., Dehghani, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer,
M., Dev, S., Devlin, J., D´ıaz, M., Du, N., Dyer, E., N., Prabhakaran, V., Reif, E., Du, N., Hutchinson, B.,
Feinberg, V., Feng, F., Fienber, V., Freitag, M., Gar- Pope, R., Bradbury, J., Austin, J., Isard, M., Gur-Ari,
cia, X., Gehrmann, S., Gonzalez, L., and et al. Palm G., Yin, P., Duke, T., Levskaya, A., Ghemawat, S.,
2 technical report. CoRR, abs/2305.10403, 2023. doi: Dev,S.,Michalewski,H.,Garcia,X.,Misra,V.,Robin-
10.48550/ARXIV.2305.10403. URL https://doi. son, K., Fedus, L., Zhou, D., Ippolito, D., Luan, D.,
org/10.48550/arXiv.2305.10403. Lim, H., Zoph, B., Spiridonov, A., Sepassi, R., Do-
han,D.,Agrawal,S.,Omernick,M.,Dai,A.M.,Pillai,
Antropic. Claude, Jul 2023. URL https://claude.
T. S., Pellat, M., Lewkowycz, A., Moreira, E., Child,
ai/chats.
R., Polozov, O., Lee, K., Zhou, Z., Wang, X., Saeta,
Bai,Y.,Jones,A.,Ndousse,K.,Askell,A.,Chen,A.,Das- B., Diaz, M., Firat, O., Catasta, M., Wei, J., Meier-
Sarma,N.,Drain,D.,Fort,S.,Ganguli,D.,Henighan,T., Hellstern, K., Eck, D., Dean, J., Petrov, S., andFiedel,
etal. Trainingahelpfulandharmlessassistantwithrein- N. Palm: Scalinglanguagemodelingwithpathways. J.
forcementlearningfromhumanfeedback. arXivpreprint Mach.Learn.Res.,24:240:1–240:113,2023.URLhttp:
arXiv:2204.05862,2022. //jmlr.org/papers/v24/22-1144.html.
Bradley, R.A.andTerry, M.E. Rankanalysisofincom- Christiano,P.F.,Leike,J.,Brown,T.B.,Martic,M.,Legg,
pleteblockdesigns: I.themethodofpairedcomparisons. S.,andAmodei,D. Deepreinforcementlearningfrom
Biometrika,39(3/4):324–345,1952. human preferences. In Guyon, I., von Luxburg, U.,
9IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan, the 35th International Conference on Machine Learn-
S. V. N., and Garnett, R. (eds.), Advances in Neural ing, ICML 2018, Stockholmsma¨ssan, Stockholm, Swe-
InformationProcessingSystems30: AnnualConference den, July 10-15, 2018, volume 80 of Proceedings of
on Neural Information Processing Systems 2017, Machine Learning Research, pp. 1856–1865. PMLR,
December4-9,2017,LongBeach,CA,USA,pp.4299– 2018. URL http://proceedings.mlr.press/
4307, 2017. URL https://proceedings. v80/haarnoja18b.html.
neurips.cc/paper/2017/hash/
Hendrycks,D.,Basart,S.,Kadavath,S.,Mazeika,M.,Arora,
d5e2c0adad503c91f91df240d0cd4e49-Abstract.
A.,Guo,E.,Burns,C.,Puranik,S.,He,H.,Song,D.,etal.
html.
Measuringcodingchallengecompetencewithapps.arXiv
Chung, H. W., Hou, L., Longpre, S., Zoph, B., Tay, Y., preprintarXiv:2105.09938,2021.
Fedus,W.,Li,E.,Wang,X.,Dehghani,M.,Brahma,S.,
Kadavath,S.,Conerly,T.,Askell,A.,Henighan,T.,Drain,
Webson, A., Gu, S. S., Dai, Z., Suzgun, M., Chen, X.,
D.,Perez,E.,Schiefer,N.,Hatfield-Dodds,Z.,DasSarma,
Chowdhery, A., Narang, S., Mishra, G., Yu, A., Zhao,
N.,Tran-Johnson,E.,Johnston,S.,Showk,S.E.,Jones,
V.Y.,Huang,Y.,Dai,A.M.,Yu,H.,Petrov,S.,Chi,E.H.,
A., Elhage, N., Hume, T., Chen, A., Bai, Y., Bowman,
Dean,J.,Devlin,J.,Roberts,A.,Zhou,D.,Le,Q.V.,and
S., Fort, S., Ganguli, D., Hernandez, D., Jacobson, J.,
Wei, J. Scalinginstruction-finetunedlanguagemodels.
Kernion,J.,Kravec,S.,Lovitt,L.,Ndousse,K.,Olsson,
CoRR, abs/2210.11416, 2022. doi: 10.48550/ARXIV.
C.,Ringer,S.,Amodei,D.,Brown,T.,Clark,J.,Joseph,
2210.11416. URLhttps://doi.org/10.48550/
N., Mann, B., McCandlish, S., Olah, C., and Kaplan,
arXiv.2210.11416.
J. Language models (mostly) know what they know.
CoRR, abs/2207.05221, 2022. doi: 10.48550/ARXIV.
Dai, J., Pan, X., Sun, R., Ji, J., Xu, X., Liu, M., Wang,
2207.05221. URLhttps://doi.org/10.48550/
Y.,andYang,Y. Saferlhf: Safereinforcementlearning
arXiv.2207.05221.
fromhumanfeedback. arXivpreprintarXiv:2310.12773,
2023. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
Chess,B.,Child,R.,Gray,S.,Radford,A.,Wu,J.,and
Dong,H.,Xiong,W.,Goyal,D.,Pan,R.,Diao,S.,Zhang,
Amodei, D. Scaling laws for neural language models.
J., Shum, K., and Zhang, T. RAFT: reward ranked
CoRR,abs/2001.08361,2020. URLhttps://arxiv.
finetuning for generative foundation model alignment.
org/abs/2001.08361.
CoRR, abs/2304.06767, 2023. doi: 10.48550/ARXIV.
2304.06767. URLhttps://doi.org/10.48550/ Langley,P.Craftingpapersonmachinelearning.InLangley,
arXiv.2304.06767. P.(ed.),Proceedingsofthe17thInternationalConference
onMachineLearning(ICML2000),pp.1207–1216,Stan-
Engstrom,L.,Ilyas,A.,Santurkar,S.,Tsipras,D.,Janoos, ford,CA,2000.MorganKaufmann.
F.,Rudolph,L.,andMadry,A. Implementationmatters
in deep RL: A case study on PPO and TRPO. In 8th Lee, H., Phatale, S., Mansoor, H., Lu, K., Mesnard, T.,
InternationalConferenceonLearningRepresentations, Bishop,C.,Carbune,V.,andRastogi,A. RLAIF:scaling
ICLR2020, AddisAbaba, Ethiopia, April26-30, 2020. reinforcement learning from human feedback with AI
OpenReview.net,2020.URLhttps://openreview. feedback. CoRR,abs/2309.00267,2023. doi: 10.48550/
net/forum?id=r1etN1rtPB. ARXIV.2309.00267. URL https://doi.org/10.
48550/arXiv.2309.00267.
Gao, L., Schulman, J., and Hilton, J. Scaling laws for
Lewis,M.,Yarats,D.,Dauphin,Y.N.,Parikh,D.,andBatra,
reward model overoptimization. In Krause, A., Brun-
D. Dealornodeal? end-to-endlearningfornegotiation
skill, E., Cho, K., Engelhardt, B., Sabato, S., and
dialogues. arXivpreprintarXiv:1706.05125,2017.
Scarlett, J. (eds.), International Conference on Ma-
chine Learning, ICML 2023, 23-29 July 2023, Hon- Li,Y.,Choi,D.,Chung,J.,Kushman,N.,Schrittwieser,J.,
olulu, Hawaii, USA, volume 202 of Proceedings of Leblond,R.,Eccles,T.,Keeling,J.,Gimeno,F.,DalLago,
MachineLearningResearch,pp.10835–10866.PMLR, A.,etal. Competition-levelcodegenerationwithalpha-
2023.URLhttps://proceedings.mlr.press/
code. Science,378(6624):1092–1097,2022.
v202/gao23h.html.
Liang, P. P., Wu, C., Morency, L., and Salakhutdinov,
Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. R. Towards understanding and mitigating social bi-
Soft actor-critic: Off-policy maximum entropy deep ases in language models. In Meila, M. and Zhang,
reinforcement learning with a stochastic actor. In T. (eds.), Proceedings of the 38th International Con-
Dy, J. G. and Krause, A. (eds.), Proceedings of ference on Machine Learning, ICML 2021, 18-24 July
10IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
2021,VirtualEvent,volume139ofProceedingsofMa- Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Man-
chineLearningResearch,pp.6565–6576.PMLR,2021. ning, C. D., and Finn, C. Direct preference optimiza-
URLhttp://proceedings.mlr.press/v139/ tion: Your language model is secretly a reward model.
liang21a.html. CoRR, abs/2305.18290, 2023. doi: 10.48550/ARXIV.
2305.18290. URLhttps://doi.org/10.48550/
Liu,T.,Zhao,Y.,Joshi,R.,Khalman,M.,Saleh,M.,Liu,
arXiv.2305.18290.
P.J.,andLiu,J. Statisticalrejectionsamplingimproves
preferenceoptimization. CoRR,abs/2309.06657,2023. Raffin,A.,Hill,A.,Gleave,A.,Kanervisto,A.,Ernestus,
doi: 10.48550/ARXIV.2309.06657. URL https:// M.,andDormann,N. Stable-baselines3: Reliablerein-
doi.org/10.48550/arXiv.2309.06657. forcementlearningimplementations. JournalofMachine
Learning Research, 22(268):1–8, 2021. URL http:
MistralAI. Mistral7B—mistral.ai. https://mistral. //jmlr.org/papers/v22/20-1364.html.
ai/news/announcing-mistral-7b/, 2023.
[Accessed18-01-2024]. Ramamurthy, R., Ammanabrolu, P., Brantley, K., Hessel,
J.,Sifa,R.,Bauckhage,C.,Hajishirzi,H.,andChoi,Y.
Mnih, V., Badia, A. P., Mirza, M., Graves, A., Lilli- Isreinforcementlearning(not)fornaturallanguagepro-
crap, T. P., Harley, T., Silver, D., and Kavukcuoglu, cessing: Benchmarks,baselines,andbuildingblocksfor
K. Asynchronousmethodsfordeepreinforcementlearn- natural language policy optimization. In The Eleventh
ing. In Balcan, M. and Weinberger, K. Q. (eds.), Pro- InternationalConferenceonLearningRepresentations,
ceedings of the 33nd International Conference on Ma- ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenRe-
chine Learning, ICML 2016, New York City, NY, USA, view.net,2023. URLhttps://openreview.net/
June 19-24, 2016, volume 48 of JMLR Workshop and pdf?id=8aHzds2uUyB.
Conference Proceedings, pp. 1928–1937. JMLR.org,
Rame´,A.,Vieillard,N.,Hussenot,L.,Dadashi,R.,Cideron,
2016. URL http://proceedings.mlr.press/
G., Bachem, O., and Ferret, J. Warm: On the bene-
v48/mniha16.html.
fits of weight averaged reward models. arXiv preprint
OpenAI. Introducing chatgpt, Nov 2022. URL https: arXiv:2401.12187,2024.
//openai.com/blog/chatgpt.
Russell, S. Human-compatible artificial intelligence.
OpenAI. GPT-4technicalreport. CoRR,abs/2303.08774, In Muggleton, S. H. and Chater, N. (eds.), Human-
2023. doi: 10.48550/ARXIV.2303.08774. URLhttps: LikeMachineIntelligence,pp.3–23.OxfordUniversity
//doi.org/10.48550/arXiv.2303.08774. Press, 2022. doi: 10.1093/OSO/9780198862536.003.
0001. URL https://doi.org/10.1093/oso/
Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, 9780198862536.003.0001.
C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K.,
Russell, S. and Norvig, P. Artificial Intelligence: A
Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller,
Modern Approach (4th Edition). Pearson, 2020.
L., Simens, M., Askell, A., Welinder, P., Christiano,
ISBN 9780134610993. URL http://aima.cs.
P. F., Leike, J., and Lowe, R. Training language
berkeley.edu/.
models to follow instructions with human feedback.
In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, Santacroce, M., Lu, Y., Yu, H., Li, Y., and Shen, Y.
D., Cho, K., and Oh, A. (eds.), Advances in Neural Efficient RLHF: reducing the memory usage of PPO.
InformationProcessingSystems35: AnnualConference CoRR, abs/2309.00754, 2023. doi: 10.48550/ARXIV.
on Neural Information Processing Systems 2022, 2309.00754. URLhttps://doi.org/10.48550/
NeurIPS 2022, New Orleans, LA, USA, November 28 arXiv.2309.00754.
- December 9, 2022, 2022. URL http://papers.
nips.cc/paper_files/paper/2022/hash/ Schulman, J., Moritz, P., Levine, S., Jordan, M. I., and
b1efde53be364a73914f58805a001731-Abstract-ACobnbefeel,rPe.nHceig.h-dimensionalcontinuouscontrolusing
html. generalizedadvantageestimation. InBengio,Y.andLe-
Cun,Y.(eds.),4thInternationalConferenceonLearning
Peng,B.,Li,C.,He,P.,Galley,M.,andGao,J. Instruction Representations,ICLR2016,SanJuan,PuertoRico,May
tuningwithGPT-4. CoRR,abs/2304.03277,2023. doi: 2-4, 2016, ConferenceTrackProceedings, 2016. URL
10.48550/ARXIV.2304.03277. URL https://doi. http://arxiv.org/abs/1506.02438.
org/10.48550/arXiv.2304.03277.
Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Klimov, O. Proximal policy optimization algorithms.
Sutskever,I.,etal. Languagemodelsareunsupervised CoRR,abs/1707.06347,2017. URLhttp://arxiv.
multitasklearners. OpenAIblog,1(8):9,2019. org/abs/1707.06347.
11IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
Sellam, T., Das, D., and Parikh, A. P. BLEURT: learn- Taori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X.,
ing robust metrics for text generation. In Jurafsky, Guestrin,C.,Liang,P.,andHashimoto,T.B. Stanford
D., Chai, J., Schluter, N., and Tetreault, J. R. (eds.), alpaca: Aninstruction-followingllamamodel,2023.
Proceedings of the 58th Annual Meeting of the Asso-
Tay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Wei, J.,
ciation for Computational Linguistics, ACL 2020, On-
Wang,X.,Chung,H.W.,Bahri,D.,Schuster,T.,Zheng,
line, July 5-10, 2020, pp. 7881–7892. Association for
H. S., Zhou, D., Houlsby, N., and Metzler, D. UL2:
Computational Linguistics, 2020. doi: 10.18653/V1/
unifyinglanguagelearningparadigms. InTheEleventh
2020.ACL-MAIN.704.URLhttps://doi.org/10.
InternationalConferenceonLearningRepresentations,
18653/v1/2020.acl-main.704.
ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenRe-
view.net,2023. URLhttps://openreview.net/
Sheng, E., Chang, K., Natarajan, P., and Peng, N. The
pdf?id=6ruVLB727MC.
womanworkedasababysitter:Onbiasesinlanguagegen-
eration. InInui,K.,Jiang,J.,Ng,V.,andWan,X.(eds.),
Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,
Proceedingsofthe2019ConferenceonEmpiricalMeth-
A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,
odsinNaturalLanguageProcessingandthe9thInterna-
Bhosale, S., Bikel, D., Blecher, L., Canton-Ferrer, C.,
tionalJointConferenceonNaturalLanguageProcessing,
Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu,
EMNLP-IJCNLP 2019, Hong Kong, China, November
J.,Fu,W.,Fuller,B.,Gao,C.,Goswami,V.,Goyal,N.,
3-7,2019,pp.3405–3410.AssociationforComputational
Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas,
Linguistics, 2019. doi: 10.18653/V1/D19-1339. URL
M.,Kerkez,V.,Khabsa,M.,Kloumann,I.,Korenev,A.,
https://doi.org/10.18653/v1/D19-1339.
Koura,P.S.,Lachaux,M.,Lavril,T.,Lee,J.,Liskovich,
D.,Lu,Y.,Mao,Y.,Martinet,X.,Mihaylov,T.,Mishra,P.,
Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, Molybog,I.,Nie,Y.,Poulton,A.,Reizenstein,J.,Rungta,
E. H., Scha¨rli, N., and Zhou, D. Large language mod- R.,Saladi,K.,Schelten,A.,Silva,R.,Smith,E.M.,Sub-
els can be easily distracted by irrelevant context. In ramanian,R.,Tan,X.E.,Tang,B.,Taylor,R.,Williams,
Krause,A.,Brunskill,E.,Cho,K.,Engelhardt,B.,Sabato, A.,Kuan,J.X.,Xu,P.,Yan,Z.,Zarov,I.,Zhang,Y.,Fan,
S., and Scarlett, J. (eds.), International Conference on A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic,
MachineLearning,ICML2023,23-29July2023,Hon- R.,Edunov,S.,andScialom,T. Llama2: Openfounda-
olulu, Hawaii, USA, volume 202 of Proceedings of tionandfine-tunedchatmodels. CoRR,abs/2307.09288,
MachineLearningResearch,pp.31210–31227.PMLR, 2023. doi: 10.48550/ARXIV.2307.09288. URLhttps:
2023.URLhttps://proceedings.mlr.press/ //doi.org/10.48550/arXiv.2307.09288.
v202/shi23a.html.
Yang,K.,Klein,D.,Celikyilmaz,A.,Peng,N.,andTian,Y.
Singhal,P.,Goyal,T.,Xu,J.,andDurrett,G. Alongway RLCD:reinforcementlearningfromcontrastdistillation
to go: Investigating length correlations in rlhf. arXiv forlanguagemodelalignment. CoRR,abs/2307.12950,
preprintarXiv:2310.03716,2023. 2023. doi: 10.48550/ARXIV.2307.12950. URLhttps:
//doi.org/10.48550/arXiv.2307.12950.
Song,F.,Yu,B.,Li,M.,Yu,H.,Huang,F.,Li,Y.,andWang,
Yao,Z.,Aminabadi,R.Y.,Ruwase,O.,Rajbhandari,S.,Wu,
H. Preferencerankingoptimizationforhumanalignment.
X.,Awan,A.A.,Rasley,J.,Zhang,M.,Li,C.,Holmes,C.,
CoRR, abs/2306.17492, 2023. doi: 10.48550/ARXIV.
Zhou,Z.,Wyatt,M.,Smith,M.,Kurilenko,L.,Qin,H.,
2306.17492. URLhttps://doi.org/10.48550/
Tanaka,M.,Che,S.,Song,S.L.,andHe,Y. Deepspeed-
arXiv.2306.17492.
chat: Easy,fastandaffordableRLHFtrainingofchatgpt-
likemodelsatallscales. CoRR,abs/2308.01320,2023.
Stiennon, N., Ouyang, L., Wu, J., Ziegler, D. M.,
doi: 10.48550/ARXIV.2308.01320. URL https://
Lowe, R., Voss, C., Radford, A., Amodei, D., and
doi.org/10.48550/arXiv.2308.01320.
Christiano, P. F. Learning to summarize with human
feedback. In Larochelle, H., Ranzato, M., Hadsell, Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen,
R., Balcan, M., and Lin, H. (eds.), Advances in A. M., and Wu, Y. The surprising effectiveness of
Neural Information Processing Systems 33: Annual PPO in cooperative multi-agent games. In Koyejo,
Conference on Neural Information Processing Sys- S., Mohamed, S., Agarwal, A., Belgrave, D., Cho,
tems 2020, NeurIPS 2020, December 6-12, 2020, K., and Oh, A. (eds.), Advances in Neural Infor-
virtual, 2020. URL https://proceedings. mation Processing Systems 35: Annual Conference
neurips.cc/paper/2020/hash/ on Neural Information Processing Systems 2022,
1f89885d556929e98d3ef9b86448f951-Abstract.NeurIPS 2022, New Orleans, LA, USA, November 28
html. - December 9, 2022, 2022. URL http://papers.
12IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
nips.cc/paper_files/paper/2022/hash/
9c1535a02f0ce079433344e14d910597-Abstract-Datasets_
and_Benchmarks.html.
Yuan, W., Pang, R. Y., Cho, K., Sukhbaatar, S., Xu, J.,
andWeston,J. Self-rewardinglanguagemodels. CoRR,
abs/2401.10020,2024. URLhttps://arxiv.org/
pdf/2401.10020.pdf.
Yuan, Z., Yuan, H., Tan, C., Wang, W., Huang, S.,
and Huang, F. RRHF: rank responses to align lan-
guage models with human feedback without tears.
CoRR, abs/2304.05302, 2023. doi: 10.48550/ARXIV.
2304.05302. URLhttps://doi.org/10.48550/
arXiv.2304.05302.
Zhang, T., Kishore, V., Wu, F., Weinberger, K. Q., and
Artzi, Y. Bertscore: Evaluating text generation with
BERT. In 8th International Conference on Learning
Representations, ICLR 2020, Addis Ababa, Ethiopia,
April26-30,2020.OpenReview.net,2020. URLhttps:
//openreview.net/forum?id=SkeHuCVFDr.
Zheng,R.,Dou,S.,Gao,S.,Hua,Y.,Shen,W.,Wang,B.,
Liu,Y.,Jin,S.,Liu,Q.,Zhou,Y.,Xiong,L.,Chen,L.,Xi,
Z.,Xu,N.,Lai,W.,Zhu,M.,Chang,C.,Yin,Z.,Weng,R.,
Cheng,W.,Huang,H.,Sun,T.,Yan,H.,Gui,T.,Zhang,
Q., Qiu, X., and Huang, X. Secrets of RLHF in large
language models part I: PPO. CoRR, abs/2307.04964,
2023. doi: 10.48550/ARXIV.2307.04964. URLhttps:
//doi.org/10.48550/arXiv.2307.04964.
Ziegler,D.M.,Stiennon,N.,Wu,J.,Brown,T.B.,Radford,
A., Amodei, D., Christiano, P. F., and Irving, G. Fine-
tuninglanguagemodelsfromhumanpreferences. CoRR,
abs/1909.08593, 2019. URL http://arxiv.org/
abs/1909.08593.
13IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
A.ImplementationDetails
A.1.DPODetails
ForDPOtraining,weuseβ=0.1withalearningrateof1e-6. Wesweepthebatchsizeandreportthebestperformance. For
HH-RLHFandSafeRLHF,wetrainDPOfortwoepochs. Forcodegenerationtasks,wetrainDPOforasingleepoch,since
ithasledtoadeteriorationinperformance.
A.2.PPODetails
DuringthePPOtrainingphase,weseparatetheparametersofactorandcritic,andsetthelearningrateto1e-5fortheactor
modeland5e-6forthecriticmodel. Bydefault,wesettheglobalbatchsizeas512,and512roll-outsamplesaresplitinto
4mini-batchestoupdatetheactorandcriticmodels. Weconfigurethesamplingparameterstoincludeatemperatureof
1.0andatop-kvalueof200. TheadvantageestimationparameterλinGAEandtheRLdiscountfactorγ arefixedat1.
WesettheKLpenaltycoefficientβ as0.1,withaclippingvalueof20forrewardscores. Weadditionallyadoptadvantage
normalizationandvaluenormalizationtostabilizethetraining.
For HH-RLHF and SafeRLHF, we set the maximum generated tokens as 256 and adopted PPO training for 5 epochs.
ForAPPSandCodeContest,wesetthemaximumgeneratedtokensas1024,andadoptPPOtrainingfor16epochs. The
checkpointswiththehighestreward/pass@konthevalidationsetsareselected.
B.GPT-4Evaluation
Weadoptthesameevaluationpromptwith(Rafailovetal.,2023). Thepromptis:
For the following query to a chatbot, which response is more helpful?
Query: <the user query>
Response A:
<either the test method or baseline>
Response B:
<the other response>
FIRST provide a one-sentence comparison of the two responses and explain \
which you feel is more helpful. SECOND, on a new line, state only "A" or \
"B" to indicate which response is more helpful. Your response should use \
the format:
Comparison: <one-sentence comparison and explanation>
More helpful: <"A" or "B">
WhenusingGPT-4forevaluation,werandomlysampled100queriesfromthetestset. AndaskGPT-4tocomparethetwo
responses. Tominimizetheimpactofresponsepositiononcomparison,weswappedthepositionsofthetworesponsesand
evaluatedthemseparately. Iftheresultsofthetwoevaluationsareinconsistent,wesetthefinalresultasa“Tie”.
C.AdditionalExperiments
C.1.VaryingtheReferenceModel
Weconductexperimentstoassesstheimpactofdistributionshiftbyvaryingthereferencemodel. Theresultsarelistedin
Table9andTable10. Llama2-7B-SFT(Safe)andCodellama13B-SFTaremodelsthatareclosertothepreferencedatasetin
theSafe-RLHFandAPPSdataset,respectively. TheresultsindicatethatDPOismoreaffectedbythedistributionshiftthan
PPO.
14IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
Method ReferenceModel Avg.Pass@5
DPO Codellama-13B-Pretrain 0.24%
DPO Codellama-13B-SFT 12.8%
PPO Codellama-13B-Pretrain 13.8%
PPO Codellama-13B-SFT 15.1%
Table9.ResultsofchangingthereferencemodelonAPPSdataset.Codellama-13B-SFTisclosertothepreferencedatasetthanCodellama-
13B-Pretrain.DPOismoreaffectedbythedistributionshiftthanPPO.
Method ReferenceModel ∆Help.↑ Harm.↓ S.R.↑
DPO Llama2-7B-SFT(Alpaca) -4.19 -0.97 55.4%
DPO Llama2-7B-SFT(Safe) -1.62 -3.5 71.8%
PPO Llama2-7B-SFT(Alpaca) 1.69 -12.08 99.5%
PPO Llama2-7B-SFT(Safe) 4.47 -12.33 99.6%
Table10.ResultsofchangingthereferencemodelonSafe-RLHFdataset.Llama2-7B-SFT(Safe)isclosertothepreferencedatasetthan
Llama2-7B-SFT(Alpaca).DPOismoreaffectedbythedistributionshiftthanPPO.
C.2.Varyingβ
InTable11,Weexploretheimpactofβ ontheHH-RLHFandAPPSdatasets. OntheHH-RLHFdataset,weevaluatethe
modelusingtheOpenAssistantrewardmetric. OntheAPPSdataset, wereporttheaveragepass@5score. Theresults
indicatethathavingtoolargeβ mayharmtheperformanceofbothDPOandPPO.Aβ valueof0.1consistentlyperforms
wellacrossvariousmodelsandtasks.
β 0 0.05 0.1(default) 0.2
HH-RLHF,Llama-7B
PPO 0.705 0.720 0.718 0.629
DPO N/A 0.609 0.611 0.597
APPS,Codellama-13B
PPO 13.0% 14.1% 15.1% 14.9%
DPO N/A 12.56% 12.0% 12.32%
Table11.ResultsofchangingtheβparameteronHH-RLHFandAPPSdataset.Theresultsindicatethathavingtoolargeβmayharmthe
performanceofbothDPOandPPO.Aβvalueof0.1consistentlyperformswellacrossvariousmodelsandtasks.
C.3.VaryingPreferenceDataset
WetrainthemodelonasubsetoftheHH-RLHFpreferencedataset. TheresultsareshowninTable12. Theresultssuggest
thattheperformanceofbothPPOandDPOmaybeaffectedbytheextentofcoverageinthepreferencedataset. When
trainingonthehelpful-basesubset,theperformanceofDPOhasdroppedtobesimilartothatoftheSFTmodel.
WealsoevaluatePPOontheHH-RLHFdatasetbyfilteringdual-unsafeanddual-safepreferencepairs. Theresultsare
listedinTable13. WeobservethatPPOcouldalsobeaffectedbythecompositionofthepreferencedataset. Overall,PPO
maintainsasaferateofover92%crossallthesettings,whileDPOismoreaffectedbythepreferencedataset.
Whenfilteringdual-unsafesamples,thePPOmodelachievessignificantlyhigherhelpfulnessrewards. Wehypothesizethat
itisbecausetherewardmodelcandiscernhelpfulnessatamorenuancedlevel. Uponfurtherfilteringofdual-safesamples,
weobservethatthemodelbecomesconservative,oftendecliningtorespondtoquestionsaltogether. Thisphenomenon
occursbecause,afterfilteringbothdual-unsafeanddual-safesamples,therewardmodelfocusessolelyonsafety. And
refusingtorespondcouldalwaysbeasafeoption.
15IsDPOSuperiortoPPOforLLMAlignment?AComprehensiveStudy
PreferenceDataset helpful-baseset fullset(default)
SFT N/A 0.532
PPO 0.602 0.718
DPO 0.544 0.615
Table12.ResultsofchangingthecoveragelevelofpreferencedatasetonHH-RLHFdataset.Whentrainingonthehelpful-basesubset,
theperformanceofDPOhasdroppedtobesimilartothatoftheSFTmodel.
∆Help.↑ Harm.↓ S.R.↑
PPO 1.69 -12.08 99.5%
+filterdual-unsafe 5.88 -9.12 92.6%
+filterdual-safe -8.04 -4.51 94.9%
DPO -1.62 -3.50 71.8%
+filterdual-unsafe 2.46 -4.88 80.8%
+filterdual-safe -2.86 -6.82 95.8%
Table13. Theimpactoffilteringdual-safeanddual-unsafetrainingdataonPPOandDPOontheSafe-RLHFdataset.
C.4.HumanEvaluation
Wealsoincludehumanevaluationtovalidatethepreference-basedtasks. TheresultsarelistedinTable14. Weensurethat
eachreferencepairsareevaluatedby4differentpersons. HumanagreewithGPT-4evaluationsatarateof60%and61%,
respectively. Accordingtohumanevaluationresults,PPOoutperformsbothDPOandDPO-Iter.
PPOwin Tie DPOwin GPT4-Humanagree%
PPOV.S.DPO 45 26 29 60
PPOV.S.DPO-iter 38 29 33 61
Table14.ResultsofhumanevaluationonHH-RLHFdataset.GPT-4agreeswithhumanevaluationsatarateof60%and61%,respectively.
Accordingtohumanevaluationresults,PPOoutperformsbothDPOandDPO-Iter.
16