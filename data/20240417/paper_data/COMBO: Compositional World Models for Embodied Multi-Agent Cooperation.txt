COMBO: Compositional World Models for
Embodied Multi-Agent Cooperation
Hongxin Zhang1*, Zeyuan Wang2∗, Qiushi Lyu3∗, Zheyuan Zhang4
Sunli Chen2, Tianmin Shu5, Yilun Du6, Chuang Gan1,7
1 University of Massachusetts Amherst 2 IIIS, Tsinghua University
3 Peking University 4 University of Michigan 5 Johns Hopkins University
6 Massachusetts Institute of Technology 7 MIT-IBM Watson AI Lab
Abstract. Inthispaper,weinvestigatetheproblemofembodiedmulti-
agentcooperation,wheredecentralizedagentsmustcooperategivenonly
partial egocentric views of the world. To effectively plan in this setting,
in contrast to learning world dynamics in a single-agent scenario, we
must simulate world dynamics conditioned on an arbitrary number of
agents’ actions given only partial egocentric visual observations of the
world.Toaddressthisissueofpartialobservability,wefirsttraingener-
ative models to estimate the overall world state given partial egocentric
observations.Toenableaccuratesimulationofmultiplesetsofactionson
this world state, we then propose to learn a compositional world model
formulti-agentcooperationbyfactorizingthenaturallycomposablejoint
actions of multiple agents and compositionally generating the video. By
leveraging this compositional world model, in combination with Vision
Language Models to infer the actions of other agents, we can use a tree
search procedure to integrate these modules and facilitate online coop-
erativeplanning.Toevaluatetheefficacyofourmethods,wecreatetwo
challenging embodied multi-agent long-horizon cooperation tasks using
the ThreeDWorld simulator and conduct experiments with 2-4 agents.
The results show our compositional world model is effective and the
frameworkenablestheembodiedagentstocooperateefficientlywithdif-
ferent agents across various tasks and an arbitrary number of agents,
showing the promising future of our proposed framework. More videos
can be found at https://vis-www.cs.umass.edu/combo/.
Keywords: Multi-AgentPlanning·CompositionalWorldModel·Em-
bodied Intelligence · Generative Models
1 Introduction
Building cooperative embodied agents that can engage in and help humans in
multi-agenttasksisavaluableyetchallengingendeavor.Tocooperativelyplanin
a multi-agent scenario, in contrast to learning world dynamics in a single-agent
* denotes equal contribution.
4202
rpA
61
]VC.sc[
1v57701.4042:viXra2 H. Zhang, Z. Wang, Q. Lyu et al.
What may Bob do? What should I do?
Bob may pick I can pick up I can pick up
up the tomato the burger the brown
slice top bread
Then Bob may… Then Bob may…
I can… I can… I can… I can…
TDW-Cook … …
Then Bob may… Then Bob may…
I can… I can… I can… I can…
… … … …
TDW-Game
(a) (b)
Fig.1: (a) Two challenging embodied multi-agent planning tasks TDW-Cook and
TDW-Game, where 2-4 agents cooperate to finish some dishes according to the recipe
orfinishpuzzlesaccordingtothevisualclue.(b)Theagentneedstoinferotheragents’
intents, propose possible actions, and accurately simulate how the world may be af-
fected by multiple sets of actions to make efficient cooperation in the long run.
scenario,thereisanadditionalchallengetosimulateworlddynamicsconditioned
on an arbitrary number of agents’ actions given only partial egocentric observa-
tions of the world.
Large generative models have brought remarkable advances to various do-
mains, including language understanding and generation [9,11,47,65], image
understanding and generation [14,15,26,30,42,43,60], and even video genera-
tion [7,18,22,25,28,37,66,75,76]. Many have explored how to leverage these
powerful foundation models for embodied AI, [1,31,33,41,67,69] leverage Large
Language Models for decision-making, [80] incorporates LLMs to build modular
embodiedagentsforcooperationandcommunication,[14,36,70]usesMultimodal
Language Models to build capable vision agents, [17,38] use Video Models to
generatevisualplansforrobots,[10,16,19,76]exploresmodelingworlddynamics
with video models to improve single-agent planning. However, how to leverage
Multimodal Language Models and Video Models to help build embodied agents
capable of planning under a visual cooperation task is under-explored where
it’s important to accurately simulate the world dynamics conditioned on an ar-
bitrary number of agents’ actions given only partial egocentric observations at
each step for efficient cooperation.
Weproposetolearnacompositionalworldmodelformulti-agentcooperation
by factorizing the naturally composable joint actions of an arbitrary number of
agents and compositionally generating the video to enable accurate simulation
of multiple sets of actions on the world state. To address the partial observ-COMBO 3
ability issue, we leverage a learned generative model to estimate the overall
world state given partial egocentric observations. By leveraging the composi-
tional world model, in combination with Vision Language Models, we propose
COMBO, a novel Compositional wOrld Model-based emBOdied multi-agent
planningframeworktofacilitateonlinecooperativeplanning.COMBO firstes-
timatestheoverallworldstatefrompartialegocentricobservations,thenutilizes
VisionLanguageModelstoactasanActionProposertoproposepossibleactions,
an Intent Tracker to infer other agents’ intents, and an Outcome Evaluator to
evaluate the different possible outcomes. In combination with the compositional
worldmodeltosimulatetheeffectofjointactionsontheworldstate,COMBO
usesatreesearchproceduretointegratethesemodulesandempowersembodied
agents to imagine how different actions may affect the world with other agents
in the long run and plan more cooperatively.
We build two challenging embodied multi-agent long-horizon cooperation
tasks on ThreeDWorld [20]: TDW-Game and TDW-Cook where 2-4 decentral-
ized agents must cooperate to finish several puzzles according to the visual clue
ordishesaccordingtotherecipegivenonlypartialegocentricviewsoftheworld
as shown in Fig. 1, requiring extensive visual cooperation. The agents need to
estimatetheoverallworldstategivenpartialegocentricobservations,inferother
agents’ intents, and accurately simulate how the world state may be affected
by multiple sets of actions to make efficient cooperation in the long run. Our
extensive experiments show our learned compositional world model delivers ac-
curate video synthesis conditioned on multiple sets of actions from an arbitrary
number of agents and COMBO enables the embodied agents to cooperate ef-
ficiently with different agents across various tasks and an arbitrary number of
agents. In sum, our contribution includes:
– Weproposetolearnacompositionalworldmodelformulti-agentcooperation
by factorizing joint actions of an arbitrary number of agents and composi-
tionally generating the video to enable accurate simulation of multiple sets
of actions on the world state.
– We introduce COMBO, a Compositional wOrld Model-based emBOdied
multi-agent planning framework to empower the agents to imagine how dif-
ferent actions may affect the world with other agents in the long run and
plan more cooperatively.
– We create two challenging embodied multi-agent planning tasks with 2-4
agents,conductexperiments,andshowourframeworkenablestheembodied
agents to cooperate efficiently with different agents across various tasks and
an arbitrary number of agents.
2 Related Work
2.1 Multi-Agent Planning
Multi-agent planning has a long-standing history [62], with various tasks and
methods have been introduced [2–4,12,34,35,44,52,54,56,63,71,80]. For em-
bodied intelligence, [52,53] explored the social perception of the agents during4 H. Zhang, Z. Wang, Q. Lyu et al.
householdingtasks,[80]studiedcommunicationandcooperationoftwoagentsin
a multi-room house. However, these works didn’t dig into the explicit challenge
of modeling the world dynamics conditioned on an arbitrary number of agents’
actions given partial egocentric observations of the world, which is essential for
the agents to cooperate efficiently in long-horizon cooperation-extensive visual
tasks. In contrast, we learn a compositional world model for multi-agent coop-
eration and propose a compositional world model-based embodied multi-agent
planningframeworktoempowertheembodiedagentstoimagine andplanmore
cooperatively with only partial egocentric observations.
2.2 Large Generative Models for Embodied AI
With the recent advance of large generative models, numerous works have ex-
plored how they can help build better agents [64,68,72,77], especially in em-
bodied environments [39,40,45,48,57,59,73,74,81,82]. Specifically, [1,21,31–
33,41,49,50,55,58,67,69,79] leverage Large Language Models to help decision-
making. [8,14,36,51,70] use multimodal language models to facilitate embod-
ied agents to plan end-to-end in visual worlds. [6,29] use diffusion models for
decision-making. [16,17,19,38,76] use video models to help the robot make vi-
sual plans, however, they assume the observation is static and non-partial and
neglect the challenge of learning world dynamics conditioned on multiple sets
of actions of an arbitrary number of agents. We learn a compositional world
modelformulti-agentcooperationtoenableaccuratesimulationofmultiplesets
of actions on the world and leverage a learned generative model to estimate the
overall world state given partial egocentric observations to address the partial
observability issue.
3 Problem Statement
Embodiedmulti-agentcooperationproblemcanbeformalizedasadecentralized
partially observable Markov decision process (DEC-POMDP) [5,46,61], defined
by (n,S,{A },{O },T,G,h), where:
i i
– n denotes the number of agents;
– S is a finite set of states;
– A is the action set for agent i;
i
– O is the observation set for agent i, including partial egocentric visual ob-
i
servation the agent receives through its sensors;
– T(s,a,s′)=p(s′|s,a) is the joint transition model which defines the proba-
bility that after taking joint action a∈A=A ×···×A in s∈S, the new
1 n
state s′ ∈S is achieved;
– G is the final goal state;
– h is the planning horizon.
Starting from an initial state S ∈ S, n decentralized agents need to plan
0
their actions a ∈ A given partial egocentric RGBD observations o ∈ O each
i i i i
step to achieve the goal state G using a minimal number of steps.COMBO 5
Alice pick up the green-brown piece
Bob pick up the blue-brown piece
Charlie wait
David pick up the aqua-black piece
Compositional World Model
Alice pick up the
green−brown piece
=
(   ,  |  1)
=Bob pick up the
blue−brown piece
Charlie wait   (   ,  |  2)
=
David pick up the
(   ,  |  3)
aqua−black piece
=
(   ,  |  4) Generated Video
Fig.2: Compositional World Model. Given a text condition of a joint action of
multiple agents a, we first factorize it into several components corresponding to each
agenta ,thenthecompositionalworldmodelgeneratesmultiplescoresconditionedon
i
the text components, then composed to generate the final output video.
4 Compositional World Model
To cooperatively plan in a multi-agent scenario, in contrast to learning world
dynamics in a single-agent scenario, there is an additional challenge to simulate
world dynamics conditioned on an arbitrary number of agents’ actions. We pro-
posetolearnacompositionalworldmodelformulti-agentcooperationasshown
in Fig. 2 by factorizing the naturally composable joint actions of an arbitrary
number of agents and compositionally generating the video to enable accurate
simulation of multiple sets of actions on the world state. We first discuss how
we learn the composable video diffusion model to act as a compositional world
modelinSec.4.1,thenintroducetheAgent-DependentLossScalingwedesigned
forthechallengingsituationwhereweneedtoaccuratelymodelmultipleagents’
manipulating multiple objects to improve the video synthesis performance in
Sec. 4.2.
4.1 Composable Video Diffusion Models
Text-guided video diffusion models learn to model the distribution of videos
conditioned on text prompts P (x|a), where the text prompts a in our scenario
θ
correspondstothejointactionsofnagentsa=(a ,··· ,a )∈A=A ×···×A ,
1 n 1 n
which can be naturally factorized into n components a ,··· ,a corresponding
1 n
Compose6 H. Zhang, Z. Wang, Q. Lyu et al.
to the action of each agent. We propose to learn the text-guided video diffusion
modelasacompositionofindividualvideodiffusionmodelsconditionedoneach
text component a :
i
n
P (x|a)=P(x|a ,··· ,a )∝P
(x)(cid:89)P θ(x|a i)
(1)
θ 1 n θ P (x)
θ
i=1
Leveraging the interpretation that diffusion models are functionally similar
to Energy-Based Models [15,43], we can train the model to learn a set of score
functions ϵ(x ,t|c ) and compose them as in Equation 2, which corresponds to
t i
the production of the probability densities in Equation 1.
n
(cid:88)
ϵˆ(x ,t|a)=ϵ (x ,t)+ ϵ (x ,t|a )−ϵ (x ,t) (2)
t θ t θ t i θ t
i=1
Then the sampling process changes to
x ∼N(x −ϵˆ(x ,t|a),σ2I) (3)
t−1 t t t
Wetrainthiscomposablevideodiffusionmodelwithtwostages.Instageone,
we learn to model the distribution of P (x|a ) by training with only conditions
θ i
correspondingtoasingleagent’sactioncomponentusingthestandarddenoising
diffusion training objective
n
(cid:88)
L = ∥ϵ (x,t|a )−ϵ∥2 (4)
MSE θ i
i=1
Theninstagetwo,wefine-tunethemodeltospecificallylearncompositional
generationtomodelP (x|a)bytrainingwithconditionscontainingjointactions
θ
of multiple agents using the composed score function loss
(cid:13) (cid:13)1 (cid:88)n (cid:13) (cid:13)2
L =∥ϵ(x ,t|a)−ϵ∥2 =(cid:13) ϵ (x ,t|a )−ϵ(cid:13) (5)
Composed t (cid:13)n θ t i (cid:13)
(cid:13) (cid:13)
i=1
After the two-stage training, we can sample from the composed distribution
with the composed score function at inference time given conditions of joint
action a=(a ,··· ,a ) as
1 n
n
(cid:88)
ϵˆ(x ,t|a)=ϵ (x ,t)+ ω(ϵ (x ,t|a )−ϵ (x ,t)) (6)
t θ t θ t i θ t
i=1
where ω is the guidance weight controlling the temperature scaling on the
conditions. When the condition corresponds to a single action component, the
above sampling procedure reduces to the classifier-free guidance [27].COMBO 7
World State Estimation
Environment Other Agents
Reconstructed Simulator
Partial
Point Clouds
Egocentric RGBD
Observation place the black
Rendered bread onto the plate
Partial
Noisy
World State
place the black bread O
Diffusion Model Action onto the cutting board Compositional octu
Proposer World Model m
plac oe
n
t th oe
t
hb ela pck
la
b teread
vE
e
a
Inpainted Intent
aul
World State Tracker Bob may place the burger bottom onto the plate rot
(a) (b)
Fig.3: (a) Given partial egocentric RGBD observations, COMBO first reconstructs
andinpaintsthetop-downorthographicimageastheoverallworldstateestimation.(b)
COMBO thenleveragetheplanningsub-modulesbuiltwithVisionLanguageModels
to propose actions, infer other agents’ intents, and evaluate the outcomes simulated
with the compositional world model to plan online with a tree search procedure to
cooperate in the long run.
4.2 Agent-Dependent Loss Scaling
Theperformanceofstageonetrainingimpactsthefinalcompositionalgeneration
performancealotgiventheaccuratemodelingofP(x|a )isthebasisformodeling
i
P(x|a). We have employed a technique named Agent-Dependent Loss Scaling to
assist in stage one training.
Formally,wedefineanagent-dependentlosscoefficientmatrixC ∈Rn×H×W
for n agents and H ×W pixel, and change the Equation 4 to
n
(cid:88)
L = C ·∥ϵ (x,t|a )−ϵ∥2 (7)
MSE i θ i
i=1
We simply set the loss coefficient matrix based on each agent’s reachable
region in the image to supervise the model to focus more on the related pixels.
We have observed that even with this straightforward loss coefficient approach,
thereisasignificantimprovementinthemodel’saccuracyofmodelingP (x|a ).
θ i
5 Compositional World Model for Multi-Agent Planning
To plan efficiently in the multi-agent cooperation problem, we still need to ad-
dress the challenge of partial egocentric observation and model complex world8 H. Zhang, Z. Wang, Q. Lyu et al.
dynamicsinthelongrun.WeproposeCOMBO,anovelCompositionalwOrld
Model-basedemBOdiedmulti-agentplanningframework,showninFig.3.After
receiving the egocentric observations during the last execution step, COMBO
first estimates the overall world state to plan on, as discussed in Sec. 5.1.
COMBO then utilizes Vision Language Models to act as an Action Proposer
to propose possible actions, an Intent Tracker to infer other agents’ intents, and
anOutcomeEvaluatortoevaluatethedifferentpossibleoutcomes,asdetailedin
Sec. 5.2. In combination with the compositional world model to simulate the ef-
fect of joint actions on the world state s =CWM(s ,a) introduced in Sec. 4,
i+1 i
we discuss the tree search procedure to integrate these planning sub-modules in
Sec. 5.3. COMBO empowers embodied agents to imagine how different plans
may affect the world with other agents in the long run and plan cooperatively.
5.1 World State Estimation with Partial Egocentric Views
Directlyplanningbasedonpartialegocentricviewspresentsaconsiderablechal-
lenge. To address this, we initially reconstruct partial point clouds from multi-
pleegocentric RGBDviews o capturedfromdifferentperspectives.These point
i
clouds are then transformed into a unified top-down orthographic image repre-
sentation,servingastheworldstate.Thisrepresentationisconstructedbyover-
laying the views in chronological order. It is important to note that our world
is inherently dynamic, with other agents actively interacting, resulting in a top-
down orthographic image representation that is both noisy and incomplete, as
depictedinFig.3(a).Torefinethisrepresentation,weemployadiffusionmodel
to inpaint the partial and noisy orthographic image, thereby enhancing the es-
timation of the world state s for subsequent planning. This approach allows
i
us to effectively represent and rectify the world state, enabling more accurate
planning in multi-agent environments.
5.2 Planning Sub-modules with Vision Language Models
Action Proposer Given the estimateed overall world state s and the long
i
horizon goal G, the Action Proposer first searches over the potential action
spaces A , and then proposes multiple possible actions a = AP(s ,G) to
i i,1...p i
explore on. We implement this module by querying a VLM to generate the
possible actions in the text given the task goal and encoded image world state
as context. We finetune LLaVA on randomly collected rollouts with possible
actions labeled to construct this module.
Intent Tracker Inferring what other agents may do given observation history
is important for effective multi-agent cooperation. We implement this module
a = IT(s ,G) by keeping the estimated image world state from the last k
−i i
steps and then feed into the VLM together with the task goal and to query for
thepossibleactionsofotheragentsa .Weconstructthismodulebyfinetuning
−i
LLaVA on collected short rollouts with other agents’ actions labeled.
Outcome EvaluatorIt’svitaltohaveawaytoassessthevalueoftheachieved
state from different plans so the search can be effectively deepened utilizingCOMBO 9
pruning. We implement an Outcome Evaluator to fulfill this functionality v =
OE(s,G) by generating a heuristic score v for each image state s given the task
goal G. To construct this module, we finetune LLaVA to describe the state of
eachobjectintheimageandthecorrespondingheuristicscorerepresentingsteps
left to achieve the task goal considering all objects.
5.3 Planning Procedure with Tree Search
Algorithm 1 COMBO Planning Procedure for Agent i.
1: Input: Estimated world state s from o , task goal G
0 i
2: Sub-modules:ActionProposerAP(s,G),IntentTrackerIT(s,G),Compositional
World Model CWM(s,a), Outcome Evaluator OE(s,G)
3: Parameters: Action Proposes P, Planning Beams B, Rollout Depths D
4: plans ←[[s ]]
0
5: new_plans ←[[s ]]
0
6: for d=1...D do
7: plans ← new_plans[1...B] # Keeps Only B Plan Beams with Best Scores
8: new_plans ←[]
9: for plan in plans do
10: s←plan[−1] # Get the Last Image State in the Plan Beam
11: a ←AP(s,G) # Generate P Different Action Proposals
i,1:P
12: a ←IT(s,G) # Infer Other Agents’ Possible Actions
−i
13: for p=1...P do
14: a←(a ,a )
i,p −i
15: s ←CWM(s,a)#SimulateNextStateConditionedonJointActions
next
16: new_plans.append(plan + s )
next
17: end for
18: end for
19: new_plans ← sorted(new_plans, OE(s,G)) # Sort Plans by the Score of
the Final State
20: end for
21: plan ← new_plans[1] # Return the Plan with the Best Score
With powerful generative sub-modules implemented so far, we can use an
effectivetreesearchalgorithmtointegratethemintoachievingthebestplanning
performance.Formally,weneedtosearchforasequenceofactionsthatcompose
the plan to achieve the task goal with a minimal number of steps. To sample
the long-horizon plans, we can chain the sub-modules to expand future states
fromcurrentstates withs =CWM(s ,AP(s ,G),IT(s ,G)).Byrecursively
i i+1 i i i
deployingthesechainedmoduleswecanexpandtheplansuntilachievingthegoal
state. However this is impractical due to the large search space, we implement
a limited tree search procedure instead by always keeping the B best-scored
plan beams and exploring P action proposals at each plan step for a maximum
of D rollout steps. We show the complete COMBO planning procedure in
Algorithm 1.10 H. Zhang, Z. Wang, Q. Lyu et al.
Duetotheuncertaintyofotheragents,COMBO replansateverysteptore-
flectthefast-changingworldstate.Assimilarlyobservedin[16],whensearching
for the best plan, the Outcome Evaluator may leverage the irregular artifacts of
theCompositionalWorldModeltogetartificiallyhighscores,suchaswherekey
objectsareteleportedtodesiredpositions.Tomitigatethisartificialexploitation
issue,theOutcomeEvaluatorwillusethedefaultscorefromthelaststateifthe
newstategeneratedfromtheCompositionalWorldModelsuspiciouslyincreases
the score above a threshold.
6 Experiments
6.1 Experimental Setup
Webuildtwochallengingembodiedmulti-agentplanningtasksTDW-Game and
TDW-Cook onThreeDWorld[20],where2-4decentralizedagentsmustcooperate
to finish some long-horizon goals, as illustrated in Fig. 1. All the agents have an
observation space of egocentric 336×336 RGBD images and the corresponding
camera matrix. Both tasks have a maximum planning horizon of 60 steps. In
TDW-Game, 3-4 agents cooperate to pass and place 6-8 puzzle pieces scattered
randomly on the table into the correct puzzle box according to visual clues such
as the shape. The location of the puzzle box and pieces are randomly initialized
across different episodes. The action space includes
– wait
– pick up <obj>
– place <obj> onto <loc>
In TDW-Cook, 2 agents cooperate to pass, cut, and place 6-8 out of 10 pos-
sible food items randomly scattered on the table to make some dishes according
totherecipe.Thelocationofthefooditemsandtherecipearerandomlyinitial-
ized across different episodes. Specifically, there is only one cutting board where
agents can pass or cut objects, making cooperation vital for efficient plans. The
action space includes
– wait
– pick up <obj>
– cut <obj>
– place <obj> onto <loc>
Metrics We evaluate the agent’s cooperation ability by task success rate and
average steps of the successful episodes when cooperating with different agents
across 10 episodes. An episode is done when the goal state is reached and con-
sidered successful or the maximum planning horizon is reached and considered
failed. Specifically, we implement two different agent policies as cooperators on
both tasks to reliably evaluate the agents. In TDW-Game, Agent 1 takes the
policy of always passing the unwanted puzzle pieces in a clockwise manner,
while Agent 2 takes the policy of always passing the unwanted puzzle pieces inCOMBO 11
TDW-Game TDW-Cook
Cooperating withAgent 1Agent 2Agent 1Agent 2
Success Rate 0 0 0 0
Recurrent World Models [23]
Average Steps / / / /
Success Rate 0 0 0 0
MAPPO [78]
Average Steps / / / /
Success Rate 1.0 0.8 0.5 0.2
CoELA (Oracle Vision) [80]
Average Steps 17.8 26.1 38.2 29.5
Success Rate 0.9 1.0 1.0 0.9
LLaVA [42]
Average Steps 26.0 31.1 31.5 36.7
Success Rate 0.8 0.7 0.9 0.9
COMBO (w/o IT)
Average Steps 15.6 17.4 24.8 21.0
Success Rate 1.0 1.0 1.0 1.0
COMBO (Ours)
Average Steps 16.8 18.1 21.3 21.7
Oracle Cooperator Success Rate 1.0 1.0 1.0 1.0
(Oracle Vision) Average Steps 14.9 16.8 23.4 19.3
Table 1: Main results on TDW-Game and TDW-Cook. We report the mean
successrateandaveragestepsofthesuccessfulepisodesover10episodeshere.COMBO
(w/o IT) denotes COMBO without the Intent Tracker module. Oracle Cooperator
has access to the Oracle state of the world and other agents’ policies. Agent 1 and
Agent 2 are cooperators with different policies, as detailed in Sec. 6.1.
a counter-clockwise manner. In TDW-Cook, Agent 1 takes a “selfish” policy of
alwaysoperatingobjectsinitsownrecipefirst,whileAgent2takesan“altruism”
policy of always prioritizing operating objects in the cooperator’s recipe.
Baselines We compare our methods to several multi-agent planning methods:
– Recurrent World Models [23], using VAE to encode the visual observations
and a generative recurrent neural network to learn the world model. An
evolution-trained simple policy is used as a controller.
– MAPPO [78], a MARL baseline where agents with shared weight Actor and
Critic are trained jointly with PPO.
– CoELA (Oracle Vision) [80], an LLM-based agent with access to the Oracle
visionperceptionofthestatere-implementedduetothelackofgroundtruth
segmentation needed to train the Perception Module of CoELA.
– LLaVA [42], LLaVA-v1.5-7B fine-tuned with the same collected rollouts to
train the Action Proposer in our COMBO, directly learn a mapping from
the visual observation and task goal to action.
– Oracle Cooperator (Oracle Vision) has access to the Oracle state of the
environment and other agents’ policies, acting as a strong baseline.
Implementation detailsThevideodiffusionmodelofthecompositionalworld
model is built upon AVDC [38] codebase, with the architectural modification of
introducing a cross attention layer to the text condition in the ResNet block
and replacing the Perceiver with an MLP to enhance the text conditioning.12 H. Zhang, Z. Wang, Q. Lyu et al.
We use the base resolution 128×128 for the video diffusion model and train
a super-resolution diffusion model to get the final 336 × 336 images. We use
the T5-XXL encoder to pre-process all the text conditions for a better contex-
tual representation. All vision language models used in the main experiments
are LLaVA-v1.5-7B. The planning parameters are set to Action Proposes P=3,
Planning Beams B=3, and Rollout Depths D=3 unless other specified. More
details are in Appendix A.
×
Alice pick up the green-brown piece
√
Alice pick up the lettuce Bob pick up the blue-brown piece
Bob pick up the pickle slice Charlie wait
David pick up the aqua-black piece
AVDC
COMBO
w.o ADLS
COMBO
Fig.4: Joint action-conditioned video generation. Our Compositional World
Modelcansimulateworlddynamicsconditionedonthejointactionofmultipleagents
accurately while AVDC struggles with simulating which agents should act, and
COMBO w.o ADLS may simulate actions incorrectly.
6.2 Results
COMBO can cooperate efficiently with different cooperatorsAsshown
inTab.1,bothagentsofdifferentpoliciesachievethebestperformancewhenco-
operatingwithCOMBO onbothtasks,finishingallthetaskswithinthelimited
horizon and cost the least steps. Both Recurrent World Models and MAPPO
perform poorly due to the difficulty of the multi-agent long-horizon task with
onlyegocentricvisualobservations.ComparedtoLLMAgentCoELAandVLM
Agent LLaVA, COMBO with a compositional world model to explicitly model
the world dynamics and plan accordingly achieves more efficient cooperation.
Intent Tracker contributes to efficient cooperationComparingtheresults
of COMBO (w.o IT) and COMBO in Tab. 1, we can see the Intent TrackerCOMBO 13
TDW-Game TDW-Cook
SingleMultiple Plan SingleMultiple Plan
AVDC 65% 20% 29.7(80%) 85% 25% 34.5(90%)
COMBO w.o ADLS 70% 55% 26.9(100%) 85% 70% 28.3(100%)
COMBO 95% 75% 17.5(100%) 100% 100% 21.5(100%)
Table2:HumanevaluationofgeneratedvideosfromdifferentWorldModels
and the corresponding plan performance. Single denotes the accuracy of syn-
thesizedvideosconditionedonasingleactionofanagentamong20samples.Multiple
denotestheaccuracyofsynthesizedvideosconditionedonmultipleactionsofmultiple
agentsamong20samples.Plandenotestheaveragesteps(successrate)acrosscooper-
atingwithbothAgent1andAgent2over10episodes.Bestperformancesareinbold.
3-agent 4-agent
Action Rollout Plan SuccessAverage
ProposalsP DepthDBeamB Rate Steps LLaVA[42] 27.4(95%) 28.7(95%)
3 1 1 50% 27.8
COMBO 15.0 (100%)17.5 (100%)
2 2 2 70% 18.1
3 3 3 100% 17.5 Oracle 13.5(100%) 15.9(100%)
Table3:Planperformanceimproves Table 4: Results on TDW-Game
with more compute budgets. We re- with different number of agents. We
port the mean success rate and average reportthemeansuccessrateandaverage
steps of the successful episodes cooper- steps of the successful episodes cooperat-
ating with Agent 1 and Agent 2 over 5 ing with Agent 1 and Agent 2 over 10
episodes on TDW-Game here. episodes here.
module is an important module that empowers the agent to infer other agents’
intents and consider them during planning for better cooperation in the long
run.
Compositional World Model is crucial to make cooperative plans We
compared the generated video quality with AVDC [38], a video diffusion model
designed for robotics tasks, and ablate on the effect of the Agent-Dependent
Scaling Loss (COMBO w.o. ADSL) in Tab. 2 and Fig. 4. Compared to AVDC,
our compositional world model can accurately simulate which agents are acting
andwhatinteractionsaredescribedinthetextcondition,leadingtoalargeper-
formance gain in the accuracy of modeling agent actions on the world state.
Removing the Agent-Dependent Scaling Loss leads to considerable accuracy
degradation, showing the effectiveness of the introduced technique. Moreover,
with the compositional world model, the overall plan performance boosts to
17.5 steps for TDW-Game compared to 29.7 steps with AVDC.
More computation budgets lead to better Plan We study the effect of
computation budgets on the long-horizon plan quality in Tab. 3, where we can
see more computation budgets with more action proposals, larger plan beams,
and deeper rollout depth leads to higher success rate and less average steps. A
qualitative example of generated plans with different computation budgets is
shown in Fig. 5, where more searches can obtain better plans in the long run.14 H. Zhang, Z. Wang, Q. Lyu et al.
3 Action Proposals, 1 Rollout Depth, 1 Plan Beams
Step 5 Step 7 Step 8 Step 9 Step 12 Step 14
3 Action Proposals, 3 Rollout Depth, 3 Plan Beams
Step 5 Step 7 Step 8 Step 10 Step 12 Step 14
Fig.5: More Computation budgets leads to better plan. With increased com-
putationbudgets(lowerpart),COMBO cansearchforabetterplanwhereAlicefirst
clearsthecommonregionwithDavidsothathecanpassthenextpuzzlepiecetoAlice
instead of having to wait, leading to a better world state after same number of steps.
COMBO can generalize to an arbitrary number of agents COMBO
trained with only data of four agents playing TDW-Game can surprisingly gen-
eralize to three agents version well as shown in Tab. 4, showing the strong gen-
eralization and promising future of the compositional world model-based mod-
ularized planning framework for multi-agent cooperation.
7 Conclusion
In this work, we learn a compositional world model for embodied multi-agent
cooperationbyfactorizingthenaturallycomposablejointactionsofanarbitrary
number of agents and compositionally generating the video to enable accurate
simulation of multi-agent world dynamics. We then propose COMBO, a novel
Compositional World Model-based embodied multi-agent planning framework
to empower the agents to infer other agents’ intents and imagine different plan
outcomes in the long run. Our experiments on two challenging embodied multi-
agent cooperation tasks show COMBO can enable the embodied agents to
cooperate efficiently with different agents across various tasks and an arbitrary
number of agents.
Future Work Our method leverages tree search combined with Large Gener-
ative Models to devise long-horizon plans. The necessity for multiple inferences
using large models leads to a relatively slow inference speed, restricting the ap-
plicability of our method in scenarios demanding rapid response. Exploring the
development of more efficient models could potentially mitigate this drawback
and enhance the practicality of our approach in time-sensitive environments.COMBO 15
References
1. Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David, B., Finn, C.,
Gopalakrishnan, K., Hausman, K., Herzog, A., et al.: Do as i can, not as i say:
Groundinglanguageinroboticaffordances.arXivpreprintarXiv:2204.01691(2022)
2. Amato,C.,Konidaris,G.,Kaelbling,L.P.,How,J.P.:Modelingandplanningwith
macro-actions in decentralized pomdps. Journal of Artificial Intelligence Research
64, 817–859 (2019)
3. Baker, B., Kanitscheider, I., Markov, T., Wu, Y., Powell, G., McGrew, B., Mor-
datch,I.:Emergenttoolusefrommulti-agentautocurricula.In:InternationalCon-
ferenceonLearningRepresentations(2020),https://openreview.net/forum?id=
SkxpxJBKwS
4. Bard, N., Foerster, J.N., Chandar, S., Burch, N., Lanctot, M., Song, H.F.,
Parisotto, E., Dumoulin, V., Moitra, S., Hughes, E., et al.: The hanabi challenge:
A new frontier for ai research. Artificial Intelligence 280, 103216 (2020)
5. Bernstein,D.S.,Givan,R.,Immerman,N.,Zilberstein,S.:Thecomplexityofdecen-
tralized control of markov decision processes. Mathematics of operations research
27(4), 819–840 (2002)
6. Black,K.,Nakamoto,M.,Atreya,P.,Walke,H.R.,Finn,C.,Kumar,A.,Levine,S.:
Zero-shotroboticmanipulationwithpre-trainedimage-editingdiffusionmodels.In:
TheTwelfthInternationalConferenceonLearningRepresentations(2024),https:
//openreview.net/forum?id=c0chJTSbci
7. Blattmann,A.,Rombach,R.,Ling,H.,Dockhorn,T.,Kim,S.W.,Fidler,S.,Kreis,
K.:Alignyourlatents:High-resolutionvideosynthesiswithlatentdiffusionmodels.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 22563–22575 (2023)
8. Brohan, A., Brown, N., Carbajal, J., Chebotar, Y., Chen, X., Choromanski, K.,
Ding,T.,Driess,D.,Dubey,A.,Finn,C.,etal.:Rt-2:Vision-language-actionmod-
els transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818
(2023)
9. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan,A.,Shyam,P.,Sastry,G.,Askell,A.,etal.:Languagemodelsarefew-shot
learners.Advancesinneuralinformationprocessingsystems33,1877–1901(2020)
10. Bruce, J., Dennis, M., Edwards, A., Parker-Holder, J., Shi, Y., Hughes, E., Lai,
M.,Mavalankar,A.,Steigerwald,R.,Apps,C.,etal.:Genie:Generativeinteractive
environments. arXiv preprint arXiv:2402.15391 (2024)
11. Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J., Horvitz, E., Kamar, E.,
Lee,P.,Lee,Y.T.,Li,Y.,Lundberg,S.,Nori,H.,Palangi,H.,Ribeiro,M.T.,Zhang,
Y.: Sparks of artificial general intelligence: Early experiments with gpt-4 (2023)
12. Carroll, M., Shah, R., Ho, M.K., Griffiths, T., Seshia, S., Abbeel, P., Dragan, A.:
On the utility of learning about humans for human-ai coordination. Advances in
neural information processing systems 32 (2019)
13. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of deep
bidirectional transformers for language understanding. In: Burstein, J., Doran,
C., Solorio, T. (eds.) Proceedings of the 2019 Conference of the North Amer-
ican Chapter of the Association for Computational Linguistics: Human Lan-
guage Technologies, Volume 1 (Long and Short Papers). pp. 4171–4186. Associ-
ation for Computational Linguistics, Minneapolis, Minnesota (Jun 2019). https:
//doi.org/10.18653/v1/N19-1423, https://aclanthology.org/N19-142316 H. Zhang, Z. Wang, Q. Lyu et al.
14. Driess,D.,Xia,F.,Sajjadi,M.S.M.,Lynch,C.,Chowdhery,A.,Ichter,B.,Wahid,
A., Tompson, J., Vuong, Q., Yu, T., Huang, W., Chebotar, Y., Sermanet, P.,
Duckworth,D.,Levine,S.,Vanhoucke,V.,Hausman,K.,Toussaint,M.,Greff,K.,
Zeng, A., Mordatch, I., Florence, P.: Palm-e: An embodied multimodal language
model. In: arXiv preprint arXiv:2303.03378 (2023)
15. Du,Y.,Durkan,C.,Strudel,R.,Tenenbaum,J.B.,Dieleman,S.,Fergus,R.,Sohl-
Dickstein,J.,Doucet,A.,Grathwohl,W.S.:Reduce,reuse,recycle:Compositional
generationwithenergy-baseddiffusionmodelsandmcmc.In:Internationalconfer-
ence on machine learning. pp. 8489–8510. PMLR (2023)
16. Du, Y., Yang, M., Florence, P., Xia, F., Wahid, A., Ichter, B., Sermanet, P., Yu,
T., Abbeel, P., Tenenbaum, J.B., et al.: Video language planning. arXiv preprint
arXiv:2310.10625 (2023)
17. Du, Y., Yang, S., Dai, B., Dai, H., Nachum, O., Tenenbaum, J., Schuurmans, D.,
Abbeel,P.:Learninguniversalpoliciesviatext-guidedvideogeneration.Advances
in Neural Information Processing Systems 36 (2024)
18. Esser, P., Chiu, J., Atighehchian, P., Granskog, J., Germanidis, A.: Structure
and content-guided video synthesis with diffusion models. In: Proceedings of the
IEEE/CVF International Conference on Computer Vision. pp. 7346–7356 (2023)
19. Finn,C.,Goodfellow,I.,Levine,S.:Unsupervisedlearningforphysicalinteraction
through video prediction. Advances in neural information processing systems 29
(2016)
20. Gan,C.,Schwartz,J.,Alter,S.,Mrowca,D.,Schrimpf,M.,Traer,J.,Freitas,J.D.,
Kubilius, J., Bhandwaldar, A., Haber, N., Sano, M., Kim, K., Wang, E., Lingel-
bach,M.,Curtis,A.,Feigelis,K.T.,Bear,D.,Gutfreund,D.,Cox,D.D.,Torralba,
A., DiCarlo, J.J., Tenenbaum, J.B., Mcdermott, J., Yamins, D.L.: ThreeDWorld:
Aplatformforinteractivemulti-modalphysicalsimulation.In:Thirty-fifthConfer-
ence on Neural Information Processing Systems Datasets and Benchmarks Track
(Round 1) (2021), https://openreview.net/forum?id=db1InWAwW2T
21. Gramopadhye, M., Szafir, D.: Generating executable action plans with
environmentally-aware language models. arXiv preprint arXiv:2210.04964 (2022)
22. Gupta, A., Tian, S., Zhang, Y., Wu, J., Martín-Martín, R., Fei-Fei, L.: Maskvit:
Masked visual pre-training for video prediction. In: The Eleventh International
ConferenceonLearningRepresentations(2023),https://openreview.net/forum?
id=QAV2CcLEDh
23. Ha, D., Schmidhuber, J.: Recurrent world models facilitate policy evolution. Ad-
vances in neural information processing systems 31 (2018)
24. Hansen, N.: The cma evolution strategy: A tutorial. arXiv preprint
arXiv:1604.00772 (2016)
25. Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D.P.,
Poole, B., Norouzi, M., Fleet, D.J., et al.: Imagen video: High definition video
generation with diffusion models. arXiv preprint arXiv:2210.02303 (2022)
26. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in
neural information processing systems 33, 6840–6851 (2020)
27. Ho, J., Salimans, T.: Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598 (2022)
28. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., Fleet, D.J.: Video
diffusion models. arXiv:2204.03458 (2022)
29. Hong, M., Kang, M., Oh, S.: Diffused task-agnostic milestone planner. Advances
in Neural Information Processing Systems 36 (2024)COMBO 17
30. Huang, S., Dong, L., Wang, W., Hao, Y., Singhal, S., Ma, S., Lv, T., Cui, L.,
Mohammed,O.K.,Liu,Q.,etal.:Languageisnotallyouneed:Aligningperception
with language models. arXiv preprint arXiv:2302.14045 (2023)
31. Huang, W., Abbeel, P., Pathak, D., Mordatch, I.: Language models as zero-shot
planners: Extracting actionable knowledge for embodied agents. In: International
Conference on Machine Learning. pp. 9118–9147. PMLR (2022)
32. Huang,W.,Xia,F.,Shah,D.,Driess,D.,Zeng,A.,Lu,Y.,Florence,P.,Mordatch,
I., Levine, S., Hausman, K., et al.: Grounded decoding: Guiding text generation
with grounded models for robot control. arXiv preprint arXiv:2303.00855 (2023)
33. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tomp-
son, J., Mordatch, I., Chebotar, Y., et al.: Inner monologue: Embodied reasoning
through planning with language models. arXiv preprint arXiv:2207.05608 (2022)
34. Jaderberg, M., Czarnecki, W.M., Dunning, I., Marris, L., Lever, G., Castaneda,
A.G., Beattie, C., Rabinowitz, N.C., Morcos, A.S., Ruderman, A., et al.: Human-
level performance in 3d multiplayer games with population-based reinforcement
learning. Science 364(6443), 859–865 (2019)
35. Jain, U., Weihs, L., Kolve, E., Farhadi, A., Lazebnik, S., Kembhavi, A., Schwing,
A.:Acordialsync:Goingbeyondmarginalpoliciesformulti-agentembodiedtasks.
In: Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, Au-
gust 23–28, 2020, Proceedings, Part V 16. pp. 471–490. Springer (2020)
36. Jiang,Y.,Gupta,A.,Zhang,Z.,Wang,G.,Dou,Y.,Chen,Y.,Fei-Fei,L.,Anand-
kumar, A., Zhu, Y., Fan, L.: Vima: General robot manipulation with multimodal
prompts. In: Fortieth International Conference on Machine Learning (2023)
37. Khachatryan, L., Movsisyan, A., Tadevosyan, V., Henschel, R., Wang, Z.,
Navasardyan,S.,Shi,H.:Text2video-zero:Text-to-imagediffusionmodelsarezero-
shot video generators. arXiv preprint arXiv:2303.13439 (2023)
38. Ko, P.C., Mao, J., Du, Y., Sun, S.H., Tenenbaum, J.B.: Learning to act from
actionless videos through dense correspondences (2023)
39. Kolve,E.,Mottaghi,R.,Han,W.,VanderBilt,E.,Weihs,L.,Herrasti,A.,Deitke,
M.,Ehsani,K.,Gordon,D.,Zhu,Y.,etal.:Ai2-thor:Aninteractive3denvironment
for visual ai. arXiv preprint arXiv:1712.05474 (2017)
40. Li,C.,Zhang,R.,Wong,J.,Gokmen,C.,Srivastava,S.,Martín-Martín,R.,Wang,
C., Levine, G., Lingelbach, M., Sun, J., et al.: Behavior-1k: A benchmark for em-
bodiedaiwith1,000everydayactivitiesandrealisticsimulation.In:Conferenceon
Robot Learning. pp. 80–93. PMLR (2023)
41. Li, S., Puig, X., Paxton, C., Du, Y., Wang, C., Fan, L., Chen, T., Huang, D.A.,
Akyürek, E., Anandkumar, A., et al.: Pre-trained language models for interactive
decision-making. Advances in Neural Information Processing Systems 35, 31199–
31212 (2022)
42. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint
arXiv:2304.08485 (2023)
43. Liu, N., Li, S., Du, Y., Torralba, A., Tenenbaum, J.B.: Compositional visual gen-
eration with composable diffusion models. In: European Conference on Computer
Vision. pp. 423–439. Springer (2022)
44. Lowe,R.,Tamar,A.,Harb,J.,PieterAbbeel,O.,Mordatch,I.:Multi-agentactor-
critic for mixed cooperative-competitive environments. Advances in neural infor-
mation processing systems 30 (2017)
45. Misra, D., Bennett, A., Blukis, V., Niklasson, E., Shatkhin, M., Artzi, Y.: Map-
ping instructions to actions in 3d environments with visual goal prediction. arXiv
preprint arXiv:1809.00786 (2018)18 H. Zhang, Z. Wang, Q. Lyu et al.
46. Oliehoek, F.A., Amato, C., et al.: A concise introduction to decentralized
POMDPs, vol. 1. Springer (2016)
47. OpenAI: Gpt-4 technical report (2023)
48. Padmakumar, A., Thomason, J., Shrivastava, A., Lange, P., Narayan-Chen, A.,
Gella, S., Piramuthu, R., Tur, G., Hakkani-Tur, D.: Teach: Task-driven embodied
agentsthatchat.In:ProceedingsoftheAAAIConferenceonArtificialIntelligence.
vol. 36, pp. 2017–2025 (2022)
49. Pallagani, V., Muppasani, B., Murugesan, K., Rossi, F., Horesh, L., Srivastava,
B.,Fabiano,F.,Loreggia,A.:Plansformer:Generatingsymbolicplansusingtrans-
formers. arXiv preprint arXiv:2212.08681 (2022)
50. Park, J.S., O’Brien, J.C., Cai, C.J., Morris, M.R., Liang, P., Bernstein, M.S.:
Generative agents: Interactive simulacra of human behavior. arXiv preprint
arXiv:2304.03442 (2023)
51. Patel, D., Eghbalzadeh, H., Kamra, N., Iuzzolino, M.L., Jain, U., Desai, R.: Pre-
trained language models as visual planners for human assistance. arXiv preprint
arXiv:2304.09179 (2023)
52. Puig, X., Shu, T., Li, S., Wang, Z., Liao, Y.H., Tenenbaum, J.B., Fidler, S., Tor-
ralba,A.:Watch-and-help:Achallengeforsocialperceptionandhuman-aicollab-
oration. In: International Conference on Learning Representations (2021)
53. Puig, X., Shu, T., Tenenbaum, J.B., Torralba, A.: Nopa: Neurally-guided on-
line probabilistic assistance for building socially intelligent home assistants. arXiv
preprint arXiv:2301.05223 (2023)
54. Puig,X.,Undersander,E.,Szot,A.,Cote,M.D.,Yang,T.Y.,Partsey,R.,Desai,R.,
Clegg, A.W., Hlavac, M., Min, S.Y., et al.: Habitat 3.0: A co-habitat for humans,
avatars and robots. arXiv preprint arXiv:2310.13724 (2023)
55. Raman, S.S., Cohen, V., Rosen, E., Idrees, I., Paulius, D., Tellex, S.: Plan-
ning with large language models via corrective re-prompting. arXiv preprint
arXiv:2211.09935 (2022)
56. Samvelyan, M., Rashid, T., Schroeder de Witt, C., Farquhar, G., Nardelli, N.,
Rudner, T.G., Hung, C.M., Torr, P.H., Foerster, J., Whiteson, S.: The starcraft
multi-agent challenge. In: Proceedings of the 18th International Conference on
Autonomous Agents and MultiAgent Systems. pp. 2186–2188 (2019)
57. Savva,M.,Kadian,A.,Maksymets,O.,Zhao,Y.,Wijmans,E.,Jain,B.,Straub,J.,
Liu,J.,Koltun,V.,Malik,J.,etal.:Habitat:Aplatformforembodiedairesearch.
In: Proceedings of the IEEE/CVF international conference on computer vision.
pp. 9339–9347 (2019)
58. Sharma, P., Torralba, A., Andreas, J.: Skill induction and planning with latent
language. arXiv preprint arXiv:2110.01517 (2021)
59. Shridhar,M.,Thomason,J.,Gordon,D.,Bisk,Y.,Han,W.,Mottaghi,R.,Zettle-
moyer,L.,Fox,D.:Alfred:Abenchmarkforinterpretinggroundedinstructionsfor
everyday tasks. In: Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition. pp. 10740–10749 (2020)
60. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-
vised learning using nonequilibrium thermodynamics. In: Bach, F., Blei, D. (eds.)
Proceedings of the 32nd International Conference on Machine Learning. Proceed-
ings of Machine Learning Research, vol. 37, pp. 2256–2265. PMLR, Lille, France
(07–09 Jul 2015), https://proceedings.mlr.press/v37/sohl-dickstein15.
html
61. Spaan,M.T.,Gordon,G.J.,Vlassis,N.:Decentralizedplanningunderuncertainty
forteamsofcommunicatingagents.In:Proceedingsofthefifthinternationaljoint
conference on Autonomous agents and multiagent systems. pp. 249–256 (2006)COMBO 19
62. Stone, P., Veloso, M.: Multiagent systems: A survey from a machine learning per-
spective. Autonomous Robots 8, 345–383 (2000)
63. Suarez, J., Du, Y., Isola, P., Mordatch, I.: Neural mmo: A massively multiagent
game environment for training and evaluating intelligent agents. arXiv preprint
arXiv:1903.00784 (2019)
64. Sumers, T., Yao, S., Narasimhan, K., Griffiths, T.L.: Cognitive architectures for
language agents. arXiv preprint arXiv:2309.02427 (2023)
65. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)
66. Villegas, R., Babaeizadeh, M., Kindermans, P.J., Moraldo, H., Zhang, H., Saffar,
M.T.,Castro,S.,Kunze,J.,Erhan,D.:Phenaki:Variablelengthvideogeneration
from open domain textual descriptions. In: International Conference on Learning
Representations (2023), https://openreview.net/forum?id=vOEXS39nOF
67. Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., Anand-
kumar, A.: Voyager: An open-ended embodied agent with large language models.
arXiv preprint arXiv:2305.16291 (2023)
68. Wang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J.,
Chen, X., Lin, Y., et al.: A survey on large language model based autonomous
agents. arXiv preprint arXiv:2308.11432 (2023)
69. Wang, Z., Cai, S., Chen, G., Liu, A., Ma, X., Liang, Y.: Describe, explain, plan
and select: interactive planning with llms enables open-world multi-task agents.
In: Thirty-seventh Conference on Neural Information Processing Systems (2023)
70. Wang, Z., Cai, S., Liu, A., Ma, X., Liang, Y.: JARVIS-1: Open-world multi-task
agents with memory-augmented multimodal language models. In: Second Agent
LearninginOpen-EndednessWorkshop(2023),https://openreview.net/forum?
id=xzPkZyHlOW
71. Wen, M., Kuba, J., Lin, R., Zhang, W., Wen, Y., Wang, J., Yang, Y.: Multi-
agent reinforcement learning is a sequence modeling problem. Advances in Neural
Information Processing Systems 35, 16509–16521 (2022)
72. Xi,Z.,Chen,W.,Guo,X.,He,W.,Ding,Y.,Hong,B.,Zhang,M.,Wang,J.,Jin,
S., Zhou, E., et al.: The rise and potential of large language model based agents:
A survey. arXiv preprint arXiv:2309.07864 (2023)
73. Xia, F., Zamir, A.R., He, Z., Sax, A., Malik, J., Savarese, S.: Gibson env: Real-
world perception for embodied agents. In: Proceedings of the IEEE conference on
computer vision and pattern recognition. pp. 9068–9079 (2018)
74. Xiang, F., Qin, Y., Mo, K., Xia, Y., Zhu, H., Liu, F., Liu, M., Jiang, H., Yuan,
Y., Wang, H., et al.: Sapien: A simulated part-based interactive environment. In:
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecog-
nition. pp. 11097–11107 (2020)
75. Yan, W., Zhang, Y., Abbeel, P., Srinivas, A.: Videogpt: Video generation using
vq-vae and transformers. arXiv preprint arXiv:2104.10157 (2021)
76. Yang,S.,Du,Y.,Ghasemipour,S.K.S.,Tompson,J.,Kaelbling,L.P.,Schuurmans,
D., Abbeel, P.: Learning interactive real-world simulators. In: The Twelfth Inter-
national Conference on Learning Representations (2024), https://openreview.
net/forum?id=sFyTZEqmUY
77. Yang, S., Nachum, O., Du, Y., Wei, J., Abbeel, P., Schuurmans, D.: Foundation
modelsfordecisionmaking:Problems,methods,andopportunities.arXivpreprint
arXiv:2303.04129 (2023)20 H. Zhang, Z. Wang, Q. Lyu et al.
78. Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen, A., Wu, Y.: The sur-
prising effectiveness of ppo in cooperative multi-agent games. Advances in Neural
Information Processing Systems 35, 24611–24624 (2022)
79. Yuan, S., Chen, J., Fu, Z., Ge, X., Shah, S., Jankowski, C.R., Yang, D., Xiao, Y.:
Distilling script knowledge from large language models for constrained language
planning. arXiv preprint arXiv:2305.05252 (2023)
80. Zhang, H., Du, W., Shan, J., Zhou, Q., Du, Y., Tenenbaum, J.B., Shu, T., Gan,
C.: Building cooperative embodied agents modularly with large language models
(2023)
81. Zhou,Q.,Chen,S.,Wang,Y.,Xu,H.,Du,W.,Zhang,H.,Du,Y.,Tenenbaum,J.B.,
Gan, C.: Hazard challenge: Embodied decision making in dynamically changing
environments (2024)
82. Zhu, Y., Gordon, D., Kolve, E., Fox, D., Fei-Fei, L., Gupta, A., Mottaghi, R.,
Farhadi, A.: Visual semantic planning using deep successor representations. In:
ProceedingsoftheIEEEinternationalconferenceoncomputervision.pp.483–492
(2017)COMBO 21
A Additional Experiment Details
A.1 Task
burger burger brown black
top bottom bread slice bread slice
pickle lettuce patty
whole whole whole
onion tomato cheese
onion tomato cheese
slice slice slice
TDW-Cook TDW-Game
Fig.6: Objects of TDW-Cook and TDW-Game. Left: 13 food items that may
occuronthetablewhile10ofthem(exceptforthethirdrow)mayoccurintherecipe.
Right: 4 puzzles containing 3 to 4 pieces each with different colors and shapes.
We build two challenging embodied multi-agent planning tasks TDW-Game
and TDW-Cook on ThreeDWorld [20], where 2-4 decentralized agents must co-
operate to finish some long-horizon goals. The object assets of the two tasks are
illustrated in Fig. 6.
A.2 Compositional World Models
Dataset Collection We collect random rollouts with a scripted planner and gen-
erated 107k videos of TDW-Game and 50k videos of TDW-Cook.
Model Parameters The parameters of our inpainting diffusion model, super-
resolution diffusion model, and video diffusion model are shown in Tab. 5.
Agent-dependent Loss Scale We use a simple method to set the loss scale. If
agent i is located in the upper half of the image, then we set C = 2
i,1:H/2,1:W
and others are 1. Similarly, the same approach is applied to other situations or
conditions.22 H. Zhang, Z. Wang, Q. Lyu et al.
Inpainting Super-ResolutionVideo Diffusion Model
num_parameters 84M 38M 198M
input_resolution 336×336 128×128 128×128
output_resolution 336×336 336×336 128×128
base_channels 64 64 128
num_res_block 2 2 2
attention_resolutions (16,) (16,) (8, 16)
channel_mult (1, 2, 4, 6, 8) (1, 2, 3, 4, 5) (1, 2, 3, 4, 5)
Table 5: Model Parameters.
Compute We train the world model for 50k steps in the first stage with 384
batch-size on 192 V-100 GPUs in 1 day. And then fine-tune the model for 25k
stepsinthesecondstagewith120batch-sizeon120V-100GPUsin1day.Both
the inpainting model and the super-resolution model are trained for 60k steps
with 288 batch-size on 24 V-100 GPUs in 1 day.
Samping We use DDIM sampling across the experiments with guidance weight
5 for the text-guided video diffusion model.
A.3 Baselines
MAPPO Our task is multi-agent reinforcement learning with decentralized
observation and control, as different agents have disjoint observations to pro-
duce their actions. We followed the training procedure in [78]. Each agent is an
actor-critic network where the actor and critic share a common convolutional
backbone. We pre-define a set of all possible actions for each environment and
let the PPO agent choose among them. We design rewards to reflect the steps
lefttofinishtheepisodeinasimilarspirittotheOutcomeEvaluator’sheuristics
score design.
However, in our setting, the embodied multi-agent simulation samples are
relativelyslow,creatingahigherdemandforsampleefficiencyofthemethod.We
conductedagridsearchonhyper-parametersoflearningratein{1e−6,5e−5,1e−3},
batch size in {2,64}. However, the results are poor and the behavior either falls
to near-random or collapses to a single action, which verifies the challenge of
cooperative planning on ego-centric RGBD observations.
Recurrent World Models We followed the training procedure in [23], firstly
trainingavariationalautoencoder(VAE)onthesamedatacollectedfortraining
COMBO to encode the egocentric observations in the TDW-Game and TDW-
Cook environmentsintoa32-dimensionallatentvectorz ∈R32.Then,wetraina
mixturedensitynetworkcombinedwitharecurrentneuralnetwork(MDN-RNN)
for predicting the future latent vector given a textual action which is encoded
by BERT [13], the current egocentric observation and the current hidden stateCOMBO 23
of the RNN, which is modeling P(z |a ,z ,h ). Finally, we train two simple
t+1 t t t
controllers using the covariance-matrix adaptation evolution strategy (CMA-
ES)[24]tomaximizetheexpectedcumulativerewardofarolloutbyinteracting
with the actual environments.
Therecurrentworldmodelfailedtomodelworlddynamicsaccuratelycondi-
tioned on a single action, as shown in Fig. 7. Specifically, we predict the latent
code z from MDN-RNN and use the decoder of the VAE to generate the im-
t+1
age of the next time step. Additionally, the simple controller doesn’t work well
on a task that requires complex planning as it keeps collapsing to single actions
in our experiments.
Initial
Frame
pick up the green-
Action pick up the lettuce
brown piece
Generated
Frame
Fig.7: Recurrent world models failed to model world dynamics conditioned
on single actions accurately.