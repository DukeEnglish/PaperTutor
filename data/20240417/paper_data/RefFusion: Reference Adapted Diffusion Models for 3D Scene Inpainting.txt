RefFusion: Reference Adapted Diffusion Models for 3D Scene Inpainting
ASHKANMIRZAEI,NVIDIA,UniversityofToronto,Canada
RICCARDODELUTIO,NVIDIA,USA
SEUNGWOOKKIM,NVIDIA,SouthKorea
DAVIDACUNA,NVIDIA,Canada
JONATHANKELLY,UniversityofToronto,Canada
SANJAFIDLER,NVIDIA,UniversityofToronto,Canada
IGORGILITSCHENSKI,UniversityofToronto,Canada
ZANGOJCIC,NVIDIA,Switzerland
Input Gaussians Novel Views
Scene
Inpainting
Object
Insertion
Inpainted Gaussians Novel Views
Scene
Outpainting
Sparse View
Reconstruction
Fig.1. Thisworkpresentsanapproachfor3Dinpaintingbasedonthedistillationofthe2Dgenerativepriorsfromreferenceadapteddiffusionmodels.Left
Givenascenerepresentedusing3DGaussiansplattingandtheuser-definedmask,weusethescoredistillationobjectivebasedonapersonalizeddiffusion
modeltoinpaintthemissingcontentin3D.RightOurmethodisgeneralandcanbe,withoutanychanges,appliedtoothereditingtaskssuchasobject
insertion,outpainting,andsparseviewreconstruction.Pleasevisitourprojectpage.
Neuralreconstructionapproachesarerapidlyemergingasthepreferred generalityofourformulationonotherdownstreamtaskssuchasobject
representationfor3Dscenes,buttheirlimitededitabilityisstillposinga insertion,sceneoutpainting,andsparseviewreconstruction.
challenge.Inthiswork,weproposeanapproachfor3Dsceneinpainting—the
taskofcoherentlyreplacingpartsofthereconstructedscenewithdesired
content.Sceneinpaintingisaninherentlyill-posedtaskasthereexistmany
1 INTRODUCTION
solutionsthatplausiblyreplacethemissingcontent.Agoodinpainting
methodshouldthereforenotonlyenablehigh-qualitysynthesisbutalso Neuralreconstructionmethodsenableseamlessreconstructionof
ahighdegreeofcontrol.Basedonthisobservation,wefocusonenabling 3Dscenesfromasetofposedimages.Theirsimpleformulation,
explicitcontrolovertheinpaintedcontentandleverageareferenceimageas highvisualfidelity,andincreasinglyfastrenderingmakethemthe
anefficientmeanstoachievethisgoal.Specifically,weintroduceRefFusion, preferred representation for a variety of use cases from AR/VR
anovel3Dinpaintingmethodbasedonamulti-scalepersonalizationofan
applicationstoroboticssimulation.Whilethesemethodsenable
imageinpaintingdiffusionmodeltothegivenreferenceview.Thepersonal-
novel-viewsynthesis,theyarestillinherentlylimitedtothecontent
izationeffectivelyadaptsthepriordistributiontothetargetscene,resulting
capturedinthetrainingdata.Yet,tomakethemusefulinpractice,
inalowervarianceofscoredistillationobjectiveandhencesignificantly
itiscriticaltoimpartthemwitheditability.Oneofthekeydesirable
sharperdetails.Ourframeworkachievesstate-of-the-artresultsforobject
removalwhilemaintaininghighcontrollability.Wefurtherdemonstratethe manipulationoperationsistheabilitytoremovepartsofthescene
andsubstitutethemwithsomedesiredcontent.Thistask,generally
knownas3Dinpainting,involvessynthesizingplausiblecontent
Authors’addresses:AshkanMirzaei,NVIDIA,UniversityofToronto,Canada,ashkan@ thatcoherentlyblendswiththerestofthescenewhenviewedfrom
cs.toronto.edu;RiccardodeLutio,NVIDIA,USA,rdelutio@nvidia.com;SeungWook
Kim,NVIDIA,SouthKorea,seungwookk@nvidia.com;DavidAcuna,NVIDIA,Canada, anyangle.Inpaintingisaninherentlyill-posedproblemasthere
dacunamarrer@nvidia.com;JonathanKelly,UniversityofToronto,Canada,jkelly@ oftenexistmultipleviableapproachestocompletethescene.Agood
utias.utoronto.ca;SanjaFidler,NVIDIA,UniversityofToronto,Canada,sfidler@nvidia.
inpaintingmodelshouldthereforealsobecontrollable,suchthat
com;IgorGilitschenski,UniversityofToronto,Canada,igor@gilitschenski.org;Zan
Gojcic,NVIDIA,Switzerland,zan.gojcic@gmail.com. userscanchoosethesolutionthatbestfitstheirneeds.
4202
rpA
61
]VC.sc[
1v56701.4042:viXra2 • AshkanMirzaei,RiccardodeLutio,SeungWookKim,DavidAcuna,JonathanKelly,SanjaFidler,IgorGilitschenski,andZanGojcic
Masked Image LaMa SDXL Inpainting Ours #1 Ours #2
Fig.2. Comparisonof2Dimageinpaintingmethodsonmultipleviewsofthesamescene.LaMa[Suvorovetal.2022]yieldsrelativelyconsistentinpaintings
butlacksdetails.SDXL[Podelletal.2023]synthesizescontentwithhigh-quality,butlowmulti-viewconsistencyduetothehighdiversityofitsgenerations.
Bypersonalizingthediffusionmodeltothereferenceview,ourmethodachieveshigh-qualitygenerationswithsuperiormulti-viewconsistency.Ours#1and
Ours#2areadaptedtoSDXLoutputsshowninthesecondandthirdrowrespectively.
Withtheadventoflargediffusionmodels[Balajietal.2022;Rom- personalizationmethodforinpaintingdiffusionmodelsthatadapts
bachetal.2022a;Sahariaetal.2022a,b],2Dimageinpaintinghas themodeltothegivenreferenceview.Thiscontributionaloneen-
beengettingincreasinglyclosetoachievingtheseproperties.Unfor- ablesexplicitcontrolovertheinpaintedcontentandreducesthe
tunately,large-scale3Dgenerativemethodsstilllagfarbehind.This varianceofthescoredistillationobjective.ii)Weleveragetheex-
canbeaccreditedtothelackoflarge-scale3Ddatasetsandefficient plicitnatureof3DGaussiansplattingtoconsolidatenoisy2Dmasks
high-resolution3Ddatastructures.Asaconsequence,muchofthe anddirectthegradientsofdifferentlosstermstothepertinentre-
recentandconcurrentworkon3Dinpaintinghasresortedtolifting gions.iii)Weproposeacombinationofobjectivetermsthatenables
thepriorsof2Dinpaintingmodelsto3D.Suchliftingisperformed usingtheSDSoptimizationprocedureatthescenelevel.iv)We
eitherexplicitly,byindependentlyinpaintingoneormultiple2D proposeanewdatasetdesignedforevaluatingobjectremovaland
viewsandconsolidatingtheirmulti-viewinconsistenciesin3D[Liu 3Dinpainting,comprisingninesceneswithlargecameramotion.
etal.2022;Mirzaeietal.2023a,b;Wangetal.2023b].Alternatively, TheexperimentsinSection4demonstratethehighvisualquality,
itcanbeformulatedasacontinuousdistillationprocess[Pooleetal. controllability,anddiversityofourapproach.Inaddition,theappli-
2023]thatseeks3Dinpaintingsusinga2Ddiffusionmodelasa cationsinSection5showcasethegeneralityofourformulation.
priorforoptimization[Prabhuetal.2023].
Thesemethodsachievepromisingresultsbutmanychallenges 2 RELATEDWORK
remain:a)Trade-offbetweendiversityandmulti-viewconsistency.
2Dand3DInpaintingInpaintingistheprocessofreplacingmiss-
Deterministic2Dinpaintingmodels,e.g.[Suvorovetal.2022],yield
ingregionswithrealisticcontent.Forexample,in2Dthisinvolves
relatively multi-viewconsistentinpaintings thatcandirectly be
producingplausiblevaluesformissingpixelsofanimage.Asan
liftedto3Dusingperceptuallosses[Mirzaeietal.2023b;Wangetal.
inherentlygenerativetask,advancementsofgenerativemodelshave
2023b].Yet,thiscomesatthecostofdiversityandlimitedvisual
ledtoincreasedperformanceof2Dinpainting.GenerativeAdversar-
quality.Prompt-basedinpaintingdiffusionmodelscansynthesize
ialNetwork(GAN)[Goodfellowetal.2014]-basedapproaches[Li
high-quality,diverse,andcontrollableinpaintings,butduetothe
etal.2022;Liuetal.2021;Suvorovetal.2022;Yuetal.2019;Zhao
looseconstraintoftextguidance,theirinpaintingsarehighlymulti-
etal.2021;Zhengetal.2022]learntohallucinatethemissingpixels
viewinconsistentandthusdifficulttoliftto3D(seeFigure2).b)
byplayinganadversarialgamebetweenanimageinpainterand
Maintainingfidelitytotheobservedcontent.Byinpaintingthe3D
a discriminator. Recently, diffusion model (DM) [Ho et al. 2020;
contentusing(inconsistent)2Dmasks,thesemethodsignorethat
Sohl-Dicksteinetal.2015;Songetal.2021]-basedinpaintingmod-
themissingcontentin3Dispotentiallysmallerthanthatnaively
els[Avrahamietal.2022;Lugmayretal.2022;Mengetal.2022;
definedbythe2Dmasks.Forexample,inthesceneshowninFigure3,
Rombachetal.2022a;Xieetal.2023]haveachievedstate-of-the-art
thecontentbehindthevaseisobservedinotherviewsandtherefore
results.Theseoperatebygraduallyperturbingacleanimagetowards
doesnotneedtobesynthesized.Finally,c)Conflictinggradients.The
randomnoisewhiletrainingadenoisingnetworktoreconstructthe
inconsistenciesacrosstheinpaintedviewsmayleadtoconflicting
image,conditionedonitsmaskedversion.
gradients,whichinturnresultinsmoothed-outinpaintings.
In3Dinpainting,thegoalistosynthesizeplausiblecontentfor
Toaddresstheabovechallenges,weproposeRefFusion,anovel
themissingregionsin3Dscenes.Thisisasignificantlyhardertask
3Dinpaintingmethodbasedoncontinuousdistillationofareference
asthegeneratedcontentneedstobeconsistentacrossvariousviews.
adapteddiffusionmodel.Inparticular:i)Weproposeamulti-scale
Although3DgenerativemodelshaverecentlygatheredincreasedRefFusion:ReferenceAdaptedDiffusionModelsfor3DSceneInpainting • 3
3 METHOD
interest[Bautistaetal.2022;Kalischeketal.2022;Kimetal.2023;
Nicholetal.2022;Zengetal.2022],theirqualityisstilllimitedby Ourmethodimprovesthedistillationof2DDMpriorsfor3Din-
thescarcityof3Dtrainingdataandthedifficultyofselectingan paintingofscenesrepresentedusingGaussiansplatting[Kerbletal.
appropriateunderlyingrepresentation.Therefore,mostexisting3D 2023].Tothisend,weadapta2DinpaintingDMbyemploying
inpaintingmodels[Haqueetal.2023;Liuetal.2022;Mirzaeietal. multi-scalecropsderivedfromareferenceimage(Section3.3).The
2023a,b;Prabhuetal.2023;Wangetal.2023b;Weberetal.2023; adaptationlargelyreducesthevarianceofthescoredistillationob-
Wederetal.2023]stillrelyonliftingthepriorsfrom2Dinpainting jectiveandremovestheneedfortextguidance.Moreover,itallows
modelsto3D.Inparticular,theyfirstinpaintmaskedinputimages ustointroduceamulti-scalescoredistillationobjective(Section3.4)
andcomplementthereconstructionobjectivewithregularization thatencompassesbothglobalcontextandlocaldetails.Toguidethe
techniquestolessenthemulti-viewinconsistencies[Liuetal.2022; supervisionofthereconstructiontothepertinentregions,welever-
Mirzaeietal.2023a,b;Wangetal.2023b;Wederetal.2023].The agetheexplicitnatureoftheGaussianrepresentation(Section3.5).
problemofinconsistencyisamplifiedwhenusingrecentDM-based ImportantpreliminariesaresummarizedinSection3.1,whilethe
inpaintingmodelswithincreaseddiversity.Weaddressthechallenge implementationdetailsareprovidedinSection3.7.
withthescoredistillationobjective[Pooleetal.2023]asfollows.
Distilling2DDiffusionModelPriorsScoreDistillationSampling 3.1 Preliminaries
(SDS),firstintroducedby[Pooleetal.2023],hasrecentlybeenused
DiffusionModelsareafamilyofgenerativemodelsknownfor
togeneraterealistic3Dobjects[Chenetal.2023;Liangetal.2023;
theirabilitytotransformsamplesfromatractabledistribution,typi-
Linetal.2023;Wangetal.2023a]andeven4Dscenes[Lingetal. callyaGaussian,towardsthetargetdatadistribution𝑝(x)[Hoetal.
2023;Renetal.2023;Zhengetal.2023]bydistillingthepriorsof
2020;Sohl-Dicksteinetal.2015].Thesemodelsarebuiltaround
text-to-imageDMs[Balajietal.2022;Daietal.2023;Rombachetal.
twokeyprocesses.Aforwardprocess,whichgraduallyremoves
2022a;Sahariaetal.2022b].TheSDSformulationfor3Dscenes thestructurefromthedatasamplesx∼𝑝(x)byaddingnoise.And,
worksbybackpropagatinggradientsfromaDM’sdenoisertothe
a reverse process, which slowly removes this noise and reintro-
underlyingscenerepresentationsothattherenderingslookrealistic.
ducesthestructureintoanintermediatelatentvariabledenotedas
Byrunningthisoptimizationprocessovermanycameraviewsthis x𝑡 =𝛼 𝑡x+𝜎 𝑡𝝐,𝝐 ∼N(0,𝑰).Inthiscontext,𝛼 𝑡 and𝜎 𝑡 representpre-
resultsina3Dconsistentscenerepresentation. determinednoiseschedules,andthevariable𝑡 denotesthetimestep
Inthispaper,weproposetofollowthisparadigmanduseapre-
withahighervalueindicatingagreateramountofnoise.There-
trained2DinpaintingDMasapriortoguidetheoptimizationpro-
verseprocessistypicallyparameterizedbyaconditionalneural
cessusingSDS.ConcurrentworksInpaint3D[Prabhuetal.2023]
network𝝐 thatistrainedtopredictthenoise𝝐usingthefollowing
𝜽
andNeRFiller[Weberetal.2023]operateonasimilarscheme,either
simplifiedobjective[Hoetal.2020]:
byusingtheSDSobjectivedirectlyoritsiterativedatasetupdate
formulationproposedbyIN2N[Haqueetal.2023].Wefindthat E x∼𝑝(x),𝝐∼N(0,𝑰),𝑡∼𝑇[𝑤(𝑡)||𝝐 𝜽(x𝑡,𝑡,c)−𝝐||2 2], (1)
personalizingtheDMtothetargetsceneandreferenceimageiskey where𝒄representsacondition(e.g.text,image,etc.)thatallowscon-
toachievingsharpresultsandusercontrollability. trollingthegenerationprocess,𝑤(𝑡)representsatime-conditional
Personalizing Diffusion Models Adapting large-scale text-to- weighting,and𝑇 isasetcontainingaselectionoftimesteps.
imageDMstogenerateuser-specifiedcontentoffersanefficientway Latent Diffusion Models (LDMs), improve computational and
forobtainingahigh-qualitypersonalizedgenerativemodel.This memoryefficiencyovertraditionalDMsbyperformingthediffusion
principlehasbeensuccessfullyappliedtomanyapplications,suchas processinalowerdimensionallatentspace[Rombachetal.2022a].
preservingtheidentityofanobject[Ruizetal.2023]orrenderinga Thisdimensionalityreductionisachievedbyemployingapretrained
chromeballtoestimatelighting[Phongthaweeetal.2023].Different encoder–decoderarchitecture,wheretheencoderEmapssamples
personalizationapproacheshavebeenproposed;DreamBooth[Ruiz from the data distribution x ∼ 𝑝(x) into a latent space Z. The
etal.2023]finetunesaDMandTextualinversion[Galetal.2022] decoderDperformstheinverseoperation,suchthatD(E(𝒙))≈𝒙.
optimizesanewwordembeddingthatcangeneratetargetsthrough InLDMs,theDMoperatesonZ,therefore𝒙inEquation1isreplaced
apretrainedDM.LoRA[Huetal.2022;Shahetal.2023],originally byitslatentrepresentation𝒛:=E(𝒙).
proposedforlanguagemodels,injectstrainablerank-decomposed ScoreDistillationSamplingoriginallyproposedby[Pooleetal.
matricesintofrozenDMsforparameter-efficientpersonalization. 2023],leveragesapretrainedDMtoguidetheoptimizationofadif-
RealFill[Tangetal.2023]and[Charietal.2023]personalizeboth ferentiable,parametricimage-renderingfunction𝑔 𝝓(𝝅):=𝒙,where
thecontextencoderandDMfortargetimages. 𝝅 denotesthecameraposefromwhichtheimage𝒙 isrendered.
PersonalizedDMshavebeenpreviouslyusedfor2Dto3Ddistil- Specifically,theparameters𝝓areupdatedusingthegradient:
l ea tti ao ln .. 2P 02ro 3l ]ifi uc sD er Le oam RAer t[ oW la en arg net anal. e2 v0 o2 l3 va in] gan md oD dr ee la am nC dr ca oft m3D pu[ tS eun a ∇ 𝝓L SDS(𝝓,𝜽):=E 𝝐∼N(0,𝑰),𝑡∼𝑇[𝑤(𝑡)(𝝐ˆ 𝜽(𝒛𝑡,𝑡,𝒄)−𝝐)𝜕 𝜕𝒛 𝝓𝑡 ], (2)
modifiedSDSobjectivefortext-to-3Dgeneration.Similarly,weper-
sonalize a pretrained DM using LoRA, but we propose a robust
where𝒛=E(𝑔 𝝓(𝝅))and
pipelinetailoredfor3DinpaintingtoefficientlypersonalizetheDM
onaninpaintedreferenceviewanduseittoinpaintasceneby 𝝐ˆ 𝜽(𝒛𝑡,𝑡,𝒄):=(1+𝛼)𝝐 𝜽(𝒛𝑡,𝑡,𝒄)−𝛼𝝐 𝜽(𝒛𝑡,𝑡,∅). (3)
optimizinganSDSobjective. Here,𝝐ˆ denotestheclassifierfreeguidance(CFG)version[Ho
𝜽
andSalimans2021]of𝝐 usedintext-conditionedDMstoenable
𝜽4 • AshkanMirzaei,RiccardodeLutio,SeungWookKim,DavidAcuna,JonathanKelly,SanjaFidler,IgorGilitschenski,andZanGojcic
Inputs LoRA Adaptation 3D Inpainting
Training Views
Dataset of
Refer Cen roc pe s View sk“ sA [, p ch roo pto p eo df ] ” R Tra an indo inm g InR pu an in 2 tiD n g Monodepth
View
Masks
Inpainting
DM
LoRA Adapted
Global Crops Inpainting DM
(Pretrained)
Reference View Noise “A photo of
sks[, cropped]”
Masked
Gaussians Discriminator
Local Crops
Fig.3. Overviewoftheproposedapproach.RefFusiontakestrainingviews,masks,andthereferenceviewasinput(left).WeadapttheinpaintingLDMon
boththeglobalandlocalcropsofthereferenceview(middle).Then,wedistillthepriorsoftheadaptedLDMtothescene(right)byminimizingtheSDS
objective.Additionally,weuseadiscriminatorlosstomitigatepotentialartifactsinappearanceandadepthlosstoenhancegeometry.WetrackGaussians
representingthemaskedandunmaskedregions,andbackpropagatethegradientsofindividualtermsonlytothepertinentregions.
higherqualitygenerationviaaguidancescaleparameter𝛼.Intu- optimizationproblem.Asaresult,theoutputsproducedbythis
itively, CFG aims to trade diversity with quality and is a linear procedureoftenexhibitalackofdetailanddiversity.Inaddition,
combinationoftwo𝝐 terms,withtheconditionomittedinthe DMsaretypicallyguidedusingtextprompts,whicharenotsuited
𝜽
latter(representedherebythenullsymbol∅). todescribelarge-scalescenesandthereforeonlyprovideweakreg-
PersonalizationandParameterEfficientFinetuningDMscan ularization.Tomitigatetheseproblems,weproposetopersonalize
bepersonalizedtogenerateimagesalignedwithaparticularconcept, theDMbasedonasingle(ormultiple)referenceimageofthescene
subject,orstyle[Ruizetal.2023;Shahetal.2023],byfinetuning beforedistillingitspriorsto3D.Thispersonalizationadaptsthe
theirweightsonaselectnumberofimages.However,updatingall DMtowardsthereferenceimageandalleviatestheneedfortext
theparametersofapretrainedDMiscomputationallyexpensive. guidanceduringthedistillationprocess.
Instead,parameter-efficientfinetuningmethodslikeLoRA[Huetal. LetI𝑟
∈Rℎ×𝑤×3denoteareferenceimage,whichcanbegener-
2022],injecttrainablelow-rankdecompositionmatricesandaimto atedbyaninpaintingmodeloranactualimageofthescene.Weuse
learnonlythevariationsfromthepretrainedweights. LoRAfinetuningtopersonalizeapretrainedinpaintingLDMtoI𝑟 by
GaussianSplattingparameterizesthescenewithasetof3DGauss- minimizingtheobjectiveinEquation1.Specifically,ateachiteration,
ianparticlesG,whereeachparticleisrepresentedbyitsposition wefirstaugmentI𝑟 bycroppingoutarandomthinborderaroundit
𝝁 ∈R3,scale𝒔 ∈R3,rotation𝒓 ∈R4,opacity𝜎 ∈R,andspherical toobtainI𝑐 𝑟 ∈Rℎ′×𝑤′×3,whereℎ′ ≤ℎand𝑤′ ≤𝑤.Wethensample
harmonicscoefficients𝜷 ∈R48.Gaussianparticlescanbeefficiently arandomrectangularmask𝒎∈{0,1}ℎ′×𝑤′ thatmasksoutpartof
renderedusingthedifferentiablesplattingformulationproposed theimageandtasktheLDMtoinpaintthemissingcontent.The
in[Kerbletal.2023],andhence,optimizedfromasetofposedimages conditioning𝒄inEquation1isobtainedbyconcatenatingthelatent
usingareconstructionloss.Consequently,thesetofdifferentiable representationofthemaskedreferenceview𝒛=E(𝒎◦I𝑐 𝑟)andthe
parametersisdefinedas𝝓 :={𝝁𝑖,𝒔𝑖,𝒓𝑖,𝜎 𝑖,𝜷𝑖} 𝑖| =G 1| . mask𝒎,where◦denotestheelement-wiseproduct.Forthetextem-
3.2 ProblemSetup bedding𝒚=E text(𝐶 𝑇)weuseafixedprompt𝐶 𝑇 ="Aphotoofsks".
WeapplyLoRAtotheattentionlayersofboththetextencoderE
text
Givena3Dreconstructionofasceneandthecorrespondingposed andtheU-Netdenoiser𝝐 .
𝜽
imagesthatgeneratedit,weinvestigate3Deditingtechniquesthat Inpractice,thenativeimageresolutionofourLDMis512×512
enableausertoperformcertainmanipulations.Inthefollowing,we pixelswitha64×64dimensionallatentspace.Suchlowresolution
detailourcontributionsonthetaskof3Dinpainting,beforeshow- is often too coarse to capture the high-frequency details of the
ingresultsonotherapplicationsthatcanbetackledwiththesame scene.Especially,asthegradientsoftheSDSlossarecomputed
formulationSection5.Inalltheexperiments,weadoptGaussian inthelow-resolutionlatentspaceandthenbackpropagatedtothe
splatting[Kerbletal.2023]asourunderlying3Drepresentation. input(cf.Equation2).Toaddressthischallenge,weproposeamulti-
However,ourapproachcanalsobeappliedtoother3Drepresenta- scalepersonalizationstrategywhereinthediffusioninpainteris
tionssuchasNeRFs[Mildenhalletal.2020]. additionallyfinetunedonlocal512×512cropssampledaroundthe
3.3 Multi-ScalePersonalization maskedregionofI𝑟.ToenabletheDMtodiscernvariationsbetween
localandglobalcropsweuseadistincttextprompt,𝐶′ ="Aphoto
Ourapproachinvolvesdistillingthelearnedpriorsfrom2Dgen- 𝑇
ofsks,cropped"fortheselocalcrops.
erativemodelsintothe3Ddomaintoprovideeffectivegenerative
guidance.Distillingpriorsfrom2DimageDMsinto3Dusingascore
distillationobjectiveiscommonlyrecognizedasamode-seeking
slexiP
deksamnURefFusion:ReferenceAdaptedDiffusionModelsfor3DSceneInpainting • 5
3.4 Multi-ScaleSDSObjective
geometryandappearanceofthesynthesizedregion.Theoverall
Buildingonthemulti-scalepersonalization,weformulateamulti- objectiveis:
scaleSDSobjective: L:=L rec+𝜆 SDSL SDS+𝜆 depthL depth+𝜆 advL adv, (5)
L SDS:=L Sg Dlo Sbal+L Slo Dc Sal. (4) where𝜆 ∗ >0balancetherelativecontributionofeachlosstermand
L andL denotetheadversarialanddepthloss,respectively.
adv depth
Specifically,consideranimageˆI𝑖renderedfromarandomcamera
Depth Regularization We regularize the depth using the out-
posefromthetrainingsetalongwithitsmask,𝒎𝑖.Then,∇Lglobal is putsofamonoculardepthestimationmodel.Inparticular,after
SDS
computedusingthewholeˆI𝑖 andthetextguidance"Aphotoofsks", every𝑁 depth iterations,werenderanimageˆI𝑖 fromarandomly
whileL Slo Dc SalconsidersonlyarandompatchˆI 𝑖𝑝 aroundthemaskedre- selectedcameraposeinthetrainingset.WethenmaskˆI𝑖 withits
gionofˆI𝑖andisguidedusingthetextprompt"Aphotoofsks,cropped". correspondingmask𝒎𝑖 andinpaintitusingouradaptedinpaint-
Suchmulti-scaleformulationallowsustobalanceglobalcontext
ingmodelstartingfromarandomtimestep𝑡
depth
∈𝑇.Thisprocess
withlocaldetailsevenatthelowresolutionoftheDM’slatentspace.
yieldsaninpaintedviewˆIinpaintthatwefeedintoamonoculardepth
estimationmodeltoobtainthedepthmap𝑑˜.
3.5 SplittingtheGaussiansintoMasked/UnmaskedSets Note that𝑑˜ is only relative and ambiguous in terms of scale
Weassumethattheregionofthescenethatshouldberemovedand andoffset.Tofixtheambiguity,weoptimizeascale𝑠andoffset𝑜
inpaintediseithermaskeddirectlyin3Dorprovidedintheformof tominimizethe𝐿2errorbetweentherendereddepth,𝑑ˆ,andthe
2Dmasksforeachtrainingview.Thelattercanbegeneratedusing aligneddepth𝑑¯=𝑠𝑑˜+𝑜forthesetofunmaskedpixels𝑃 .
unmasked
anoff-the-shelf2Dsegmentationmethodorprovidedbytheuser. Finally,weemploythefastbilateralsolver[BarronandPoole2016]
Assuch2Dmaskscanbeinconsistentin3D,weproposeasimple toreducepotentialremainingmisalignmentsandcalculateL
depth
heuristicthatconsolidatesthem. onthemaskedpixels,𝑃 ,as:
masked
3DConsistentMasksToconsolidateinconsistent2Dmasks,we
1 ∑︁
firstdetermineifeachGaussianparticleshouldbemaskedorun- L depth:= |𝑃 | ∥𝑑ˆ(𝑝)−𝑑¯(𝑝)∥2 2. (6)
masked,bycountinghowoftenitcontributestothevolumerender- masked 𝑝∈𝑃 masked
ingofmaskedandunmaskedpixelsacrossthetrainingviews.Ifthe AdverserialObjectiveTomitigateanycolormismatchesandar-
ratiomasked/unmaskedisaboveathreshold𝜏 maskthe3Dparticle tifactsontheboundaryofthehallucinatedregion,weemploya
islabeledasmasked.Afterparticlelabelling,werasterizethemback discriminatorD 𝝃 parameterizedby𝝃.Thisdiscriminatoristrained
tothetrainingviewsandthresholdtherenderedsemanticvalues todifferentiatebetweenrealpatchesthataresampledaroundthe
using𝜏 m′ asked,producingasetof3Dconsistent2Dbinarymasks. maskedregionofthereferenceviewI𝑟andthefakepatchesrendered
ConfiningtheLossFunctionstoPertinentRegionsDuringthe from𝑔 𝝓(𝝅)aroundthemaskofthetrainingviews.Wepropagate
optimization,weusetheper-Gaussianmaskstodirectthegradients thegradientsoftheadversariallosstoallsphericalharmonicscoeffi-
ofindividuallossterms.Specifically,thereconstructionlossfromun- cients𝜷𝑖 ∈𝝓,whilekeepingotherparametersfixed.FollowingGaN-
maskedpixelsonlyupdatestheparametersofunmaskedGaussians, eRF[Roessleetal.2023],weemploythefollowingregularizedver-
whereasthegradientsfromotherlosstermsareonlypropagated sionoftheGANlossL advwithan𝑅 1gradientpenalty[Mescheder
tothemaskedGaussians.Thisdifferentiationintomaskedandun- etal.2018]onD 𝝃,controlledbyabalancingscalar𝜆 gp:
maskedGaussiansiscrucialtopreventguidancelosses(e.g.,SDS
loss)fromundesirablyinfluencingtheregionsthatdonotrequire min𝜷max𝜉E(cid:104) 𝑓(D𝜉(ˆI𝑃 fake))+𝑓(−D𝜉(I𝑃 real))−𝜆 gp∥∇D𝜉(ˆI𝑃 fake)∥2 2(cid:105) , (7)
hallucination.But,italsorequiresustoadaptthedensificationand
where𝑓(𝑥):=−log(1+exp(−𝑥)),andˆI𝑃 andI𝑃
correspondto
pruningheuristicsproposedin[Kerbletal.2023].Specifically,if fake real
thesampledfakeandrealpatches,respectively.
aGaussianissplitorclonedwetransferitsmaskvaluetoitschil-
dren.Additionally,ifaGaussiantransitionsbetweentheregions,
3.7 ImplementationDetails
wepruneittomaintainregion-specificfidelity.
Reference-GuidedInitializationAtthestartoftheinpainting Forunprojectionandreprojectionofthemasks,weset𝜏 maskedand
process, we remove Gaussians in the masked scene region and
𝜏′ to1and0.3,respectively.Ineachiteration,wecompute
masked
replacethemusingthereferenceviewI𝑟.Specifically,wefirstpredict Lglobal usinganimagerenderedfromarandomcameraposefrom
thedepth𝑑˜ 𝑟 ofthereferenceimageI𝑟.Wethencompensatethe thS eD tS rainingset,whileaveragingtheLlocal acrosstwo512×512
scaleandoffsetambiguitiestoderivethealigneddepth𝑑ˆ 𝑟 (similar patchessampledfromtheboundingboxS oD fS themaskedregion.Fol-
towhatwillfollowinourdepthregularization)andunprojectit lowingGaussiansplatting[Kerbletal.2023],weresizeeachtraining
to3D.Weempiricallyobservethatreference-guidedinitialization viewtohavethelargersideoftheimagesbe1600pixels.Thedepth
outperformsrandominitializationTable3. lossiscalculatedevery8thiteration.Fortheadversarialloss,we
sample64realand64fake64×64patcheseveryiteration.Thearchi-
3.6 LossesandTraining
tectureofthediscriminatorfollowsStyleGAN2[Karrasetal.2020],
WetrainourmethodusingthecombinationofthemultiscaleSDSob- andthehyperparametersoftheadversariallossareconsistentwith
jectiveSection3.4andthereconstructionlossL rec(𝐿1andD-SSIM). GANeRF[Roessleetal.2023].Thelossweightsofindividualloss
Additionally,weemploytworegularizationtermsthatimprovethe termsaresetto𝜆 SDS := 0.001,𝜆 depth := 0.0625,and𝜆 adv := 0.03,6 • AshkanMirzaei,RiccardodeLutio,SeungWookKim,DavidAcuna,JonathanKelly,SanjaFidler,IgorGilitschenski,andZanGojcic
Table1. QuantitativeevaluationofobjectremovalonSPIn-NeRFdataset.
respectively.WeadopttheGaussianoptimizationparametersfrom
[Kerbletal.2023],butchangethedensificationandcloningheuristic Method LPIPS↓
topreventoverdensificationofGaussiansduetolargergradientsof
NeRF+LaMa(2D) 0.5369
theSDSloss.TogeneratetherefenceviewsweuseSDXL[Podell
ObjectNeRF[Yangetal.2021] 0.6829
etal.2023]inpaintingmodelduetoitshigherresolutionandqual-
MaskedNeRF[Mildenhalletal.2020] 0.6030
ity.However,tospeedupthedistillation,wepersonalizeStable-
NeRF-In[Liuetal.2022] 0.4884
Diffusion-2-Inpainting[Rombachetal.2022b]astheadaptedLDM.
SPIn-NeRF-SD[Mirzaeietal.2023b] 0.5701
ThesameLoRAoptimizationparametersasRealFill[Tangetal.
SPIn-NeRF-LaMa[Mirzaeietal.2023b] 0.4654
2023]areemployed;wetraintheDMfor2000iterations,withLoRA
Inpaint3D[Prabhuetal.2023] 0.5150
ranksetto8,𝛼 setto16,resolutionsetto512,andlearningrates
Reference-guidedNeRF[Mirzaeietal.2023a](SDV2) 0.4532
2𝑒−4and4𝑒−5fortheU-Netandthetextencoder,respectively.We
Reference-guidedNeRF[Mirzaeietal.2023a](SDXL) 0.4453
settheLoRAdropoutto0.1.NotethattheadaptedLDMexpects
Ours 0.4283
512×512images;thus,forcalculatingtheglobalSDSloss,wefirst
bilinearlydownsampletherenderedviewsto512×512. Table2. UserstudyofobjectremovalonSPIn-NeRFdataset.Foreach
methodwereportthepercentageofratersthatpereferreditoverours.
4 EXPERIMENTS
Method Quality(%) Removal(%)
DatasetandMetricsFollowingrelatedwork,weperformmost
SPIn-NeRF-LaMa 18.38 29.32
experiments on the SPIn-NeRF [Mirzaei et al. 2023b] dataset. It
Inpaint3D 11.87 11.52
consistsof10scenesoriginallydesignedforobjectremovalevalua-
Reference-guidedNeRF(SDXL) 23.64 43.65
tion.Eachsceneinthedatasetincludes60imageswithanunwanted
object(trainingviews)and40imageswithoutit(testviews).Human-
annotatedmasksoftheobjectregionareavailableforbothtraining BaselinesWecompareRefFusiontotwonaivebaselines,Masked
andtestviews.Formoreinformationaboutthedatasetpleaserefer NeRFandNeRF+LaMa.Intheformer,thereconstructionlossisonly
to[Mirzaeietal.2023b].Toperformaquantitativecomparisonofour calculatedontheunmaskedpixels,whilethelatterusesLaMa[Su-
methodtotheselectedbaselines,weuse40ground-truthimagesof vorov et al. 2022] to independently inpaint rendered views. We
eachscenewithouttheobject.Specifically,werendercorresponding furthercomparetoawiderangeofexisting3Dinpaintingmeth-
viewsfromeachmodelandcomputetheaveragelearnedperceptual ods:ObjectNeRF[Yangetal.2021],NeRF-In[Liuetal.2022],SPIn-
imagepatchsimilarity(LPIPS)[Zhangetal.2018]1.Inlinewiththe NeRF[Mirzaeietal.2023b],andconcurrentworkInpaint3D[Prabhu
relatedwork[Mirzaeietal.2023b]wecalculatethemetricsaround etal.2023].Forallthebaselines,weusedthesourcecodeorthe
themaskedregionbyconsideringtheboundingboxofthemask renderedimagesprovidedbytheauthorsoftherespectiveworks.
anddilatingtheboxineverydirectionby10%.Toaddressthelimi- ObjectRemovalWefirstprovideevaluationsbasedonthestan-
tationsofSPIn-NeRFdatasetindemonstratingmethodbehaviors dard3Dinpaintingbenchmark,theSPIn-NeRFdataset[Mirzaeietal.
onsceneswithmoresignificantcameramotionforinpainting,we 2023b].Toensureafaircomparison,weusethehuman-annotated
introduceourdatasetcomprisingninescenes.Thisdatasetencom- masks from the SPIn-NeRFdataset for all methods. As depicted
passeswide-baselineforward-facingscenesaswellas360-degree inTable1ourmethodsurpassesallbaselinesintermsofLPIPS.Fur-
scenes.Foreachscene,ourdatasetincludesimagesbothwithand thermore,wealsocomparefavourablyinaqualitativecomparison
withoutunwantedobjects,alongwithhuman-annotatedmasksfor asseeninFigure4.NotethattheperceptuallossusedinSPIn-NeRF
everyscene. doesn’tfullyresolvethemulti-viewinconsistenciesintroducedby
WefurtherperformedauserstudyusingAmazonMechanical inpaintingeachtrainingviewseparately,thereforeresultinginvi-
Turk.Specifically,raterswereshownvideosdepictingtheresultsof sualartifacts.Reference-guidedNeRFperformswellintheremoval
twomethodsrenderedfromthesamenoveltrajectories(ourmethod region,howeverthequalityoftheoverallreconstructionquality
andabaselineinrandomorder),alongwiththeinputvideoand suffers.Finally,Inpaint3Dalsodistillsapretrained2Dinpainting
imageshighlightingtheunwantedobject.Raterswerethenaskedto DMusinganSDSobjective,howeverthelackofpersonalization
voteforthemethodthatproducedthebestvideointermsofoverall resultsinafuzzyinpainting.Wesummarizetheresultsoftheuser
qualityandbestobjectremoval.Toimprovethequalityofresponses, studyinTable2,RefFusionoutperformsallbaselinesintermsof
weintroducedanattentionchecktask.Foreachscene,weadded overallqualityandobjectremoval.
aquestionwherethevideosdisplayedforbothmethodswerethe TheSPIn-NeRFdataset[Mirzaeietal.2023b]exclusivelycom-
same(outputsofourmodel)andaddedanoptionsuggestingthat prisessceneswithminimalcameramovements.Toremedythiscon-
bothofthevideosareidentical.Wesetathresholdof50%accuracy straint,weintroduceourdatasetfeaturingninedistinctsceneswith
ontheattentioncheckquestionstostrikeabalancebetweenthe broadercamerabaselines,includingwide-baselineforward-facing
quantityandqualityoftheresponses.Intotal,thisresultedin32 scenesand360-degreescenes.InFigure5,wepresentqualitative
userscomparingourmethodtothreebaselinesacross10scenesand comparisonsbetweentheoutcomesofourapproachandReference-
answering2questionsperexample. guidedNeRFappliedtoscenesfromtheMipNeRF360dataset[Barron
etal.2022](depictedinthefirstandsecondscenesoftheleftcol-
1Priorworkalsoreportsper-sceneFIDscore,butduetothesmallsizeofthedataset umn),aswellasourownscenes.Theresultsnotablyillustratethat
(40images)FIDvaluesareunrepresentative.WethusomitthemandonlyreportLPIPS. Reference-guidedNeRF,reliantsolelyonasingleinpaintedviewRefFusion:ReferenceAdaptedDiffusionModelsfor3DSceneInpainting • 7
Masked Image SPIn-NeRF Reference-guided NeRF Inpaint3D Ours
Fig.4. QualitativeobjectremovalresultsontheSPIn-NeRFdataset.RefFusionconsistentlyoutperformsthebaselines,yieldingsharperreconstructionand
moreplausibleinpainting.
Table3. AblationstudyofobjectremovalonSPIn-NeRFdataset.
additionallyusetheGTviewsforsupervisionusingreconstruction
Method LPIPS↓ loss—Ours(LoRA+recon).Figure6showsthatgenerativepriorscan
guidethereconstructioninasparseviewsetting2,especiallywhen
Oursw/opersonalization 0.5719
onlyasmallnumberofviewsareavailable.Indeed,Ours(LoRA+
Oursw/osplittingtheGaussians 0.5128
recon)consistentlyoutperforms3DGSintermsofLPIPSandPSNR.
Oursw/olocalSDS 0.5093
Figure11depictsaqualitativecomparisonwith3DGS.
Oursw/oreference-guidedinitialization 0.4680
ObjectInsertionFigure7illustratesthecapacityof RefFusionto
Oursw/oadversarialloss 0.4326
insertobjectsintoascene.Wedemonstratethatbyusingareference
Oursw/odepthloss 0.4299
viewwithanaddedobjectinthemaskedregion,obtainedusinga
Ours 0.4283
text-to-imageinpaintingdiffusionmodel.Ourmethodsucceedsin
distillingthespecifiedobjectintothescenewithhighvisualfidelity.
forprojectioninto3Dspace,encounterschallengesinextrapolating SceneOutpaintingInFigure8,wefollowtheprocedureproposed
informationfromthereferencetofurtherviews.Conversely,our in[Prabhuetal.2023]andgenerateinversemasksbyplacinga
methodconsistentlyyieldssharperandmoreplausibleinpainting sphereatafixeddistancealongtheopticalaxisandcheckingfor
resultswithoutvisualartifacts.Notethatinthethirdexample,we ray-sphereintersection.ExamplemaskisshowninFigure8(left).
removetwodistinctobjectsbyconcurrentlyincorporatingbothinto Giventhismask,wetaskourmethodtooutpaintthesceneusing
themasks. thesameformulationandhyperparametersusedforobjectremoval.
AblationonDesignChoicesWeperformablationstudiesonour Ourmethodcompletesthesceneinaplausiblemanner,butthe
keydesignchoicestohighlighttheirsignificance.Table3andFig- outpaintedregionslackvisualfidelityandhigh-frequencydetails.
ure10depictquantitativeandqualitativeresults.Thelargestdrop This hints that special treatment is required to obtain the same
inperformanceisobservedwhenremovingpersonalizationfrom qualityofresultswhenperformingoutpainting.
ourmethod.Thisconfirmsourintuitionthatreferenceadaptation
iscrucialforunlockingthepotentialofSDS-basedoptimizationat 6 CONCLUSIONS,LIMITATIONSANDFUTUREWORK
thescenelevel.Otherablationsshowthatremovinganycompo-
WeintroducedRefFusion,a3Dinpaintingframeworkthatachieves
nentsleadstoadverseeffectsonthefinaloutcomes.Whilewefound
state-of-the-artresults,whileofferingimprovedcontrollabilityover
thedepthlossnottobecrucialfortheobjectremovaltask,itstill
theinpaintedcontent.Weachievethisbypersonalizinganinpait-
contributestoasubtleimprovement.Moreover,itisparticularly
ningLDMtothegivenreferenceimageanddistillingtheadapted
beneficialforotherapplicationssuchasobjectinsertion.
diffusionpriortothe3DsceneWedemonstratethegeneralityofour
5 APPLICATIONS methodacrossadiversesetofdownstreamapplicationsincluding
objectinsertion,3Doutpainting,andsparseviewreconstruction.
In this section, we demonstrate the general applicability of our Despiteachievingnotableresults,therearemanyavenuestoim-
methodtovariousdownstream3Dapplications. proveRefFusion.Theprimarychallengeliesinremovinglarge
SparseViewReconstructionToinvestigatethebenefitsofour objectsthatsubstantiallycoverpartsofthereferenceimage.Fine-
methodforguidingthesparseviewreconstruction,weconsidera tuningthediffusionmodelonthetargetdatasetcouldhelpbridge
scenewithanunwantedoccluderwhereonlyasmallsetofcleanim- thisgap.Additionally,althoughLoRAimprovestheefficiencyofthe
agesfromthescene(GTviews)isavailable.Specifically,weconsider adaptationprocesstothetargetscene,thisprocessstillrequiresa
twosettings:a)weonlyusetheGTviewsforLoRAfinetuningand
donotusethemduringthereconstruction—Ours(LoRA),orb)we 2Asimilarobservationwasconcurrentlyshownby[Wuetal.2023].8 • AshkanMirzaei,RiccardodeLutio,SeungWookKim,DavidAcuna,JonathanKelly,SanjaFidler,IgorGilitschenski,andZanGojcic
Masked Image Reference-guided NeRF Ours Masked Image Reference-guided NeRF Ours
Fig.5. Qualitativeobjectremovalresultsonsceneswithlargercameramovements(MipNeRF360dataset[Barronetal.2022]andscenesfromourproposed
dataset).RefFusionconsistentlyoutperformstheReference-guidedNeRF.
considerableamountoftime.Furtheradvancementsinparameter-
efficientfinetuningandpersonalizationofdiffusionmodelsareex-
pectedtofurtherspeedupandenhanceourapproach.
Finally,recentadvancesinmulti-viewawareandvideodiffusion
modelscouldsignificantlyenhancethemulti-viewqualityofour
results.Especially,astheremainingartifactsaremostlyvisiblewhen
thecameramoves.
REFERENCES
Fig.6. ResultsofthesparseviewreconstructiononSPIn-NeRFdataset.
OmriAvrahami,DaniLischinski,andOhadFried.2022.Blendeddiffusionfortext-driven
UsingthesparseGTviewsonlyforpersonalizationOurs(LoRA)already editingofnaturalimages.InCVPR.
yieldscompetitiveresults.Whencombinedwiththereconstructionloss YogeshBalaji,SeungjunNah,XunHuang,ArashVahdat,JiamingSong,Qinsheng
Ours(LoRA+recon)consistentlyoutperforms3DGS[Kerbletal.2023], Zhang,KarstenKreis,MiikaAittala,TimoAila,SamuliLaine,BryanCatanzaro,
TeroKarras,andMing-YuLiu.2022.eDiff-I:Text-to-ImageDiffusionModelswith
showcasingthepotentialofgenerativepriorstoguide3Dreconstruction.
EnsembleofExpertDenoisers.arXiv(2022).
JonathanT.Barron,BenMildenhall,DorVerbin,PratulP.Srinivasan,andPeterHedman.
Masked View Reference View Novel View #1 Novel View #2 2022.Mip-NeRF360:UnboundedAnti-AliasedNeuralRadianceFields.CVPR(2022).
JonathanTBarronandBenPoole.2016.Thefastbilateralsolver.InECCV.
MiguelÁngelBautista,PengshengGuo,SamiraAbnar,WalterTalbott,AlexanderT
Toshev,ZhuoyuanChen,LaurentDinh,ShuangfeiZhai,HanlinGoh,DanielUlbricht,
AfshinDehghan,andJoshuaM.Susskind.2022. GAUDI:ANeuralArchitectfor
Immersive3DSceneGeneration.InNeurIPS.
PradyumnaChari,SizhuoMa,DaniilOstashev,AchutaKadambi,GurunandanKrishnan,
JianWang,andKfirAberman.2023.PersonalizedRestorationviaDual-PivotTuning.
arXiv(2023).
RuiChen,YongweiChen,NingxinJiao,andKuiJia.2023.Fantasia3D:Disentangling
GeometryandAppearanceforHigh-qualityText-to-3DContentCreation.InICCV.
XiaoliangDai,JiHou,Chih-YaoMa,SamTsai,JialiangWang,RuiWang,Peizhao
Zhang,SimonVandenhende,XiaofangWang,AbhimanyuDubey,MatthewYu,
Fig.7. Sampleobjectinsertionresults. AbhishekKadian,FilipRadenovic,DhruvMahajan,KunpengLi,YueZhao,Vladan
Petrovic,MiteshKumarSingh,SimranMotwani,YiWen,YiwenSong,Roshan
Masked View Reference View Novel View
Sumbaly,VigneshRamanathan,ZijianHe,PeterVajda,andDeviParikh.2023.Emu:
EnhancingImageGenerationModelsUsingPhotogenicNeedlesinaHaystack.arXiv
(2023).
RinonGal,YuvalAlaluf,YuvalAtzmon,OrPatashnik,AmitHBermano,GalChechik,
andDanielCohen-Or.2022. Animageisworthoneword:Personalizingtext-to-
imagegenerationusingtextualinversion.arXiv(2022).
Fig.8. Ourapproachiscapableofoutpaintingscenesbyinvertedmasks.RefFusion:ReferenceAdaptedDiffusionModelsfor3DSceneInpainting • 9
IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,Sherjil RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer.
Ozair,AaronCourville,andYoshuaBengio.2014. Generativeadversarialnets. 2022b.High-ResolutionImageSynthesisWithLatentDiffusionModels.InCVPR.
NeurIPS(2014). NatanielRuiz,YuanzhenLi,VarunJampani,YaelPritch,MichaelRubinstein,andKfir
Ayaan Haque, Matthew Tancik, Alexei Efros, Aleksander Holynski, and Angjoo Aberman.2023. DreamBooth:FineTuningText-to-imageDiffusionModelsfor
Kanazawa.2023. Instruct-NeRF2NeRF:Editing3DSceneswithInstructions.In Subject-DrivenGeneration.(2023).
ICCV. ChitwanSaharia,WilliamChan,HuiwenChang,ChrisLee,JonathanHo,TimSalimans,
JonathanHo,AjayJain,andPieterAbbeel.2020. DenoisingDiffusionProbabilistic DavidFleet,andMohammadNorouzi.2022a. Palette:Image-to-imagediffusion
Models.InNeurIPS. models.InSIGGRAPH.
JonathanHoandTimSalimans.2021. Classifier-freediffusionguidance. NeurIPS ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyDenton,
Workshops(2021). SeyedKamyarSeyedGhasemipour,RaphaelGontijo-Lopes,BurcuKaragolAyan,
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang, TimSalimans,JonathanHo,DavidJ.Fleet,andMohammadNorouzi.2022b.Photo-
LuWang,andWeizhuChen.2022.LoRA:Low-rankadaptationoflargelanguage realisticText-to-ImageDiffusionModelswithDeepLanguageUnderstanding.In
models.ICLR(2022). NeurIPS.
NikolaiKalischek,TorbenPeters,JanD.Wegner,andKonradSchindler.2022.Tetrahe- VirajShah,NatanielRuiz,ForresterCole,ErikaLu,SvetlanaLazebnik,YuanzhenLi,and
dralDiffusionModelsfor3DShapeGeneration.arXiv(2022). VarunJampani.2023.ZipLoRA:AnySubjectinAnyStylebyEffectivelyMerging
TeroKarras,SamuliLaine,MiikaAittala,JanneHellsten,JaakkoLehtinen,andTimo LoRAs.arXiv(2023).
Aila.2020.AnalyzingandImprovingtheImageQualityofStyleGAN.InCVPR. JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan,andSuryaGanguli.2015.
BernhardKerbl,GeorgiosKopanas,ThomasLeimkühler,andGeorgeDrettakis.2023. DeepUnsupervisedLearningusingNonequilibriumThermodynamics.InICML.
3DGaussianSplattingforReal-TimeRadianceFieldRendering.ToG(2023). YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoEr-
SeungWookKim,BradleyBrown,KangxueYin,KarstenKreis,KatjaSchwarz,Daiqing mon,andBenPoole.2021.Score-BasedGenerativeModelingthroughStochastic
Li,RobinRombach,AntonioTorralba,andSanjaFidler.2023. NeuralField-LDM: DifferentialEquations.InICLR.
SceneGenerationwithHierarchicalLatentDiffusionModels.InCVPR. JingxiangSun,BoZhang,RuizhiShao,LizhenWang,WenLiu,ZhendaXie,andYebin
WenboLi,ZheLin,KunZhou,LuQi,YiWang,andJiayaJia.2022.Mat:Mask-aware Liu.2023.Dreamcraft3d:Hierarchical3dgenerationwithbootstrappeddiffusion
transformerforlargeholeimageinpainting.InCVPR. prior.arXiv(2023).
YixunLiang,XinYang,JiantaoLin,HaodongLi,XiaogangXu,andYingcongChen. RomanSuvorov,ElizavetaLogacheva,AntonMashikhin,AnastasiaRemizova,Arsenii
2023. LucidDreamer:TowardsHigh-FidelityText-to-3DGenerationviaInterval Ashukha,AlekseiSilvestrov,NaejinKong,HarshithGoka,KiwoongPark,andVictor
ScoreMatching.arXiv(2023). Lempitsky.2022.Resolution-robustlargemaskinpaintingwithfourierconvolutions.
Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,XiaohuiZeng,XunHuang, InWACV.
KarstenKreis,SanjaFidler,Ming-YuLiu,andTsung-YiLin.2023.Magic3D:High- LumingTang,NatanielRuiz,ChuQinghao,YuanzhenLi,AleksanderHolynski,DavidE
ResolutionText-to-3DContentCreation.InCVPR. Jacobs,BharathHariharan,YaelPritch,NealWadhwa,KfirAberman,andMichael
HuanLing,SeungWookKim,AntonioTorralba,SanjaFidler,andKarstenKreis.2023. Rubinstein.2023.RealFill:Reference-DrivenGenerationforAuthenticImageCom-
AlignYourGaussians:Text-to-4DwithDynamic3DGaussiansandComposed pletion.arXiv(2023).
DiffusionModels.arXiv(2023). DongqingWang,TongZhang,AlaaAbboud,andSabineSüsstrunk.2023b. Inpaint-
HongyuLiu,ZiyuWan,WeiHuang,YibingSong,XintongHan,andJingLiao.2021. NeRF360:Text-Guided3DInpaintingonUnboundedNeuralRadianceFields.arXiv
Pd-gan:Probabilisticdiverseganforimageinpainting.InCVPR. (2023).
Hao-KangLiu,IShen,Bing-YuChen,etal.2022.NeRF-In:Free-formNeRFinpainting ZhengyiWang,ChengLu,YikaiWang,FanBao,ChongxuanLi,HangSu,andJun
withRGB-Dpriors.CGA(2022). Zhu.2023a.ProlificDreamer:High-FidelityandDiverseText-to-3DGenerationwith
AndreasLugmayr,MartinDanelljan,AndresRomero,FisherYu,RaduTimofte,andLuc VariationalScoreDistillation.InNeurIPS.
VanGool.2022.Repaint:Inpaintingusingdenoisingdiffusionprobabilisticmodels. EthanWeber,AleksanderHołyński,VarunJampani,SaurabhSaxena,NoahSnavely,
InCVPR. AbhishekKar,andAngjooKanazawa.2023. NeRFiller:CompletingScenesvia
ChenlinMeng,YangSong,JiamingSong,JiajunWu,Jun-YanZhu,andStefanoErmon. Generative3DInpainting.arXiv(2023).
2022. Sdedit:Imagesynthesisandeditingwithstochasticdifferentialequations. SilvanWeder,GuillermoGarcia-Hernando,AronMonszpart,MarcPollefeys,GabrielJ
ICLR(2022). Brostow,MichaelFirman,andSaraVicente.2023.Removingobjectsfromneural
LarsMescheder,AndreasGeiger,andSebastianNowozin.2018.Whichtrainingmethods radiancefields.InCVPR.
forGANsdoactuallyconverge?.InICML. RundiWu,BenMildenhall,PhilippHenzler,KeunhongPark,RuiqiGao,DanielWatson,
BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRa- PratulP.Srinivasan,DorVerbin,JonathanT.Barron,BenPoole,andAleksander
mamoorthi,andRenNg.2020.NeRF:Representingscenesasneuralradiancefields Holynski.2023. ReconFusion:3DReconstructionwithDiffusionPriors. arXiv
forviewsynthesis.InECCV. (2023).
AshkanMirzaei,TristanAumentado-Armstrong,MarcusABrubaker,JonathanKelly, ShaoanXie,ZhifeiZhang,ZheLin,TobiasHinz,andKunZhang.2023.Smartbrush:
AlexLevinshtein,KonstantinosGDerpanis,andIgorGilitschenski.2023a.Reference- Textandshapeguidedobjectinpaintingwithdiffusionmodel.InCVPR.
guidedControllableInpaintingofNeuralRadianceFields.ICCV(2023). BangbangYang,YindaZhang,YinghaoXu,YijinLi,HanZhou,HujunBao,Guofeng
AshkanMirzaei,TristanAumentado-Armstrong,KonstantinosGDerpanis,Jonathan Zhang,andZhaopengCui.2021.LearningObject-CompositionalNeuralRadiance
Kelly,MarcusABrubaker,IgorGilitschenski,andAlexLevinshtein.2023b.SPIn- FieldforEditableSceneRendering.InICCV.
NeRF:Multiviewsegmentationandperceptualinpaintingwithneuralradiance JiahuiYu,ZheLin,JimeiYang,XiaohuiShen,XinLu,andThomasSHuang.2019.
fields.InCVPR. Free-formimageinpaintingwithgatedconvolution.InICCV.
AlexNichol,HeewooJun,PrafullaDhariwal,PamelaMishkin,andMarkChen.2022. XiaohuiZeng,ArashVahdat,FrancisWilliams,ZanGojcic,OrLitany,SanjaFidler,and
Point-E:ASystemforGenerating3DPointCloudsfromComplexPrompts. KarstenKreis.2022.LION:LatentPointDiffusionModelsfor3DShapeGeneration.
PakkaponPhongthawee,WoramethChinchuthakun,NontaphatSinsunthithet,Amit InNeurIPS.
Raj,VarunJampani,PramookKhungurn,andSupasornSuwajanakorn.2023.Diffu- RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.2018.
sionLight:LightProbesforFreebyPaintingaChromeBall.arXiv(2023). TheUnreasonableEffectivenessofDeepFeaturesasaPerceptualMetric.InCVPR.
DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,Jonas ShengyuZhao,JonathanCui,YilunSheng,YueDong,XiaoLiang,EricIChang,andYan
Müller,JoePenna,andRobinRombach.2023.SDXL:ImprovingLatentDiffusion Xu.2021.Largescaleimagecompletionviaco-modulatedgenerativeadversarial
ModelsforHigh-ResolutionImageSynthesis.arXiv(2023). networks.ICLR(2021).
BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall.2023. DreamFusion: HaitianZheng,ZheLin,JingwanLu,ScottCohen,EliShechtman,ConnellyBarnes,
Text-to-3Dusing2DDiffusion.ICLR(2023). JianmingZhang,NingXu,SohrabAmirghodsi,andJieboLuo.2022.Imageinpainting
KiraPrabhu,JaneWu,LynnTsai,PeterHedman,DanBGoldman,BenPoole,and withcascadedmodulationGANandobject-awaretraining.InECCV.
MichaelBroxton.2023.Inpaint3D:3DSceneContentGenerationusing2DInpainting YufengZheng,XuetingLi,KokiNagano,SifeiLiu,OtmarHilliges,andShaliniDeMello.
Diffusion.arXiv(2023). 2023. Aunifiedapproachfortext-andimage-guided4dscenegeneration. arXiv
JiaweiRen,LiangPan,JiaxiangTang,ChiZhang,AngCao,GangZeng,andZiweiLiu. (2023).
2023.DreamGaussian4D:Generative4DGaussianSplatting.arXiv(2023).
BarbaraRoessle,NormanMüller,LorenzoPorzi,SamuelRotaBulò,PeterKontschieder,
andMatthiasNießner.2023. GANeRF:LeveragingDiscriminatorstoOptimize
NeuralRadianceFields.ToG(2023).
RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer.
2022a.High-ResolutionImageSynthesiswithLatentDiffusionModels.InCVPR.10 • AshkanMirzaei,RiccardodeLutio,SeungWookKim,DavidAcuna,JonathanKelly,SanjaFidler,IgorGilitschenski,andZanGojcic
Masked View Reference View Novel View #1 Novel View #2 Novel View #3
Fig.9. QualitativeobjectremovalresultsontheSPIn-NeRFdataset.RefFusionsynthesizesplausiblecontentthatishighlymulti-viewconsistent.RefFusion:ReferenceAdaptedDiffusionModelsfor3DSceneInpainting • 11
Masked View Reference w/o personalization Ours
Masked View Reference w/o adversarial loss Ours
Masked View Reference w/o local SDS Ours
Masked View Reference w/o depth loss Ours
Masked View Reference w/o splitting the Gaussians Ours
Fig.10. QualitativeresultsoftheablationstudyonSPIn-NeRFdataset.Notehowdifferentcomponentsofourmethodhelpimprovedifferenttypesofartifacts.
Masked View 1 View 2 Views 4 Views
Sparse Views
Fig.11. QualitativeevaluationofsparseviewreconstructiononSPIn-NeRFdataset.BothRefFusionand3DGSusethereconstructionlossonsparseinput
imagesinthemaskedregion.Additionally,RefFusionusesgenerativepriorsofthereferenceadaptedLDMthroughSDSlossesaswellasthedepthand
adversarialregularizationterms.NotehowgenerativepriorscansuccessfullyguidethereconstructionevenintheextremecaseofasingleGTview.
SGD3
sruO