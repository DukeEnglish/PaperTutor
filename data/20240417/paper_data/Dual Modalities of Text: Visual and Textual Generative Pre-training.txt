Dual Modalities of Text: Visual and Textual Generative Pre-training
YekunChai♠ QingyiLiu∗♡ JingwuXiao*♢
ShuohuanWang♠ YuSun♠ HuaWu♠
♠BaiduInc. ♡SunYat-senUniversity ♢PeakingUniversity
{chaiyekun,wangshuohuan,sunyu02}@baidu.com
{liuqy95}@mail2.sysu.edu.cn
Abstract asPDFs,wheredirectparsingintotextincurssig-
nificant information loss. Traditional methodolo-
Harnessingvisualtextsrepresentsaburgeoning
giestypicallyemploypre-trainedopticalcharacter
frontierintheevolutionoflanguagemodeling.
recognition(OCR)toolsforextractinginformation
Inthispaper,weintroduceanovelpre-training
fromsuchvisualtexts,butthesemethodsareinher-
framework for a suite of pixel-based autore-
gressive language models, pre-training on a entlylimitedbythefidelityoftextextraction.
corpusofover400milliondocumentsrendered In response to these challenges, a novel
asRGBimages. Ourapproachischaracterized paradigm of pixel-based language modeling has
by a dual-modality training regimen, engag- emerged, offering a direct pathway to learning
ingbothvisualdatathroughnextpatchpredic-
fromtextasvisualdata(images),transcendingthe
tionwitharegressionheadandtextualdatavia
constraintsoftextualmodality(Rustetal.,2023;
nexttokenpredictionwithaclassificationhead.
Tschannen et al., 2023). This approach promises Thisstudyisparticularlyfocusedoninvestigat-
tosurmountthevocabularybottleneckissue(Rust
ingthesynergisticinterplaybetweenvisualand
textualmodalitiesoflanguage.Ourcomprehen- etal.,2023)—atrade-offinherentinbalancingin-
siveevaluationacrossadiversearrayofbench- putencodinggranularityagainstthecomputational
marksrevealsthattheconfluenceofvisualand feasibilityofvocabularyprobabilityestimationin
textualdatasubstantiallyaugmentstheefficacy
conventionallanguagemodels.
ofpixel-basedlanguagemodels. Notably,our
In the previous literature, the development of
findingsshowthataunidirectionalpixel-based
pixel-basedlanguagemodelshasbeenbifurcated
model,devoid oftextualdataduringtraining,
into encoder-based (Rust et al., 2023; Tschan-
canmatchtheperformancelevelsofadvanced
bidirectionalpixel-basedmodelsonvariouslan- nen et al., 2023) or encoder-decoder architec-
guageunderstandingbenchmarks. Thiswork tures(Saleskyetal.,2023),encompassingmodels
highlightstheconsiderableuntappedpotential thateitheremploybidirectionalmechanismsakin
of integrating visual and textual information to MAE (He et al., 2022) or utilize an encoder-
for language modeling purposes. We will re-
decoder framework, where a pixel-based model
leaseourcode,data,andcheckpointstoinspire
servesastheencoder,pairedwithaunidirectional
furtherresearchadvancement.
language decoder. Despite these advancements,
1 Introduction theexplorationofpixel-basedmodelsemployinga
decoder-centricapproachremainsinitsinfancy.
The landscape of large language models (LLMs)
Moreover, current research often processes vi-
is undergoing a significant transformation, with
sualtextas8-bitgrayscale(Rustetal.,2023)or2-
advancements that extend the boundaries of lan-
bitbinaryimages(Taietal.,2024). Thisapproach
guageassistant(Touvronetal.,2023a),codegener-
restrictstherepresentationofcolor,criticalforele-
ation(Lozhkovetal.,2024;Chaietal.,2023),and
mentslikeemojisandfonthighlights,anddiverges
multimodalcomprehension(OpenAI,2023;Anil
from the natural image format in RGB. Notably,
etal.,2023). Thesemodelstraditionallytokenize
thereappearstobealackofstudiespre-trainingon
inputdataintodiscreteelements,treatingthemas
RGBimages,whichcouldmoreaccuratelyreflect
sequencesofidentifiers,therebyenablingdiverse
thecomplexitiesofvisualtext.
applications. However,thisapproachoftenstrug-
This research aims to fill these gaps by offer-
gles with visually enriched textual content, such
ingacomprehensiveexaminationoftheeffectsof
*WorkdoneduringQLandJX’sinternshipatBaidu. pixel-based versus text-based pre-training within
4202
rpA
61
]LC.sc[
1v01701.4042:viXraanautoregressivelanguagemodelingcontext. Our graphicpropertiesoftextthroughvisualrepresen-
studyissteeredbythreecriticalresearchquestions: tations. PIXEL(Rustetal.,2023)utilizesmasked
RQ1: Feasibilityofpurepixel-basedautogressive auto-encoders to address the vocabulary bottle-
pre-trainingonRGBimagesofvisualtexts. Canan neck by reconstructing pixels in masked text im-
autoregressivelanguagemodeltrainedsolelyon24- ages. Moreover,CLIPPO(Tschannenetal.,2023)
bitRGBimagesofvisualtextsachievecompetitive demonstrates enhanced language comprehension
performance? using a unified encoder for both image and text
RQ2: Impactofautoregressivepixelpre-training modalities. FurtherresearchbyLotzetal.(2023)
onmultilingualtasks. Weexplorewhetherautore- evaluates the impact of rendering techniques on
gressivepixelpre-trainingcanovercomethevocab- theefficacyofpixel-basedencoders. Thesestudies
ularybottleneckinmultilingualcontexts,assessing primarilyutilizebidirectionalencodersandprocess
itseffectivenessingeneralizinglinguisticfeatures textasgrayscaleimages.
acrosslanguages. Incontrast,ourapproachleveragesRGBimag-
ingtorendertext,employinga24-bitcolordepthto
RQ3: Synergistic effects of multimodal pre-
enrichthevisualdatainterpretation. Thisenhance-
training. Howdopixel-basedandtext-basedpre-
mentallowsforhandlingofelementslikeemojis
training synergize, and in what ways does this
andcoloredtext, prevalentindigitalcommunica-
multimodal strategy enhance the model’s perfor-
tions. ConcurrentworkbyTaietal.(2024)explores
mance on language understanding tasks and its
binaryimagerenderingandbinarycross-entropy
cross-lingualapplicability?
loss in discrete space, whereas we implement a
Contributions (1) We introduce a novel autore-
mean square error loss in continuous pixel space
gressive pre-training approach that combines the
forfinerreconstructiongranularity. Moreover,re-
pre-trainingobjectiveofnexttokenandnextpatch
search such as OCR-free visually-rich document
prediction, bridging text and visual modalities
understanding (Kim et al., 2022), which focuses
throughtailoredclassificationandregressionheads,
on direct learning from visual document images,
respectively. Thismethodologymarksasignificant
sharessimilaritieswithourapproach. However,our
advancementinmultimodallanguagemodeltrain-
workdistinctivelyexploresrenderedtext,expand-
ing. (2)Weconstructacomprehensivevisualtext
ing the potential for comprehensive multimodal
datasetofover400milliondocumentsbytextren-
textpre-training.
dering for pixel-based pre-training (equivalent to
Autoregressive Pre-training on Pixels Exist-
roughly236billiontexttokens). Wewillreleasethe
ing methods in pixel-based autoregressive pre-
fine-tuningdatasetsusedforlanguageunderstand-
training divide into vector quantization tech-
ingandmultilingualevaluationinourexperiments,
niques—transformingcontinuousimagesintodis-
facilitatingfurtheradvancementsinthisemerging
crete tokens—and direct pixel prediction. These
field. (3) In this work, we demonstrate that pre-
approachesincludeVQ-VAE(VanDenOordetal.,
trainingdecoder-onlytransformersonRGBvisual
2017)andVQGAN(Esseretal.,2021)followedby
images can surpass or match the performance of
nexttokenprediction(Chenetal.,2020;Ramesh
encoder-basedmodelsinlanguageunderstanding
et al., 2021), and prefix language modeling that
andmultilingualtransfertasks. Ourfindingssug-
predicts future visual patches from bidirectional
gest that leveraging the rich information content
pixelcontexts(El-Noubyetal.,2024).
ofRGBimagesindecoder-onlyarchitecturesnot
Thesemodelsaretrainedonregularimages. Our
onlyenhancesmodelunderstandingofcomplexlin-
researchdivergesbyfocusingexclusivelyonvisual
guisticpatternsbutalsofacilitateseffectiveknowl-
andrenderedtexts,therebyextendingthecapability
edgetransferacrosslanguages. (4)Weempirically
ofautoregressivemodelstounderstandandgener-
demonstrate the substantial potential of integrat-
atelanguagefromitsvisualform.
ingvisualandtextualdataforenhancedlanguage
modeltraining.
3 Pre-trainingonPixelsandTexts
2 RelatedWork
3.1 RenderingTextasImages
PixelRepresentationsforTextAdvancesinpixel- Following Rust et al. (2023), we utilize text
based language modeling have increasingly fo- renderer adept at converting textual data into a
cused on exploiting the orthographic and typo- visually-richRGBformat. ThispivotalcomponentNext Patch Prediction Transformer Decoder
L x +
Transformer Decoder SwiGLU
Norm
+
Patchify
Multi-head
2 Image Encoding
Attention
Rendered
My cool cat ᓚᘏᗢ sits in a beautiful box lunch and dinner.
Image Norm
1 Render Text as Image
Input Text My cool cat ᓚᘏᗢ sits in a beautiful box lunch and Embedded
Patches/Tokens
dinner.
(a) Visualtextimagepre-training(PixelGPT). (b) Modelarchitecture.
Figure1: Illustrationofpixel-basedautoregressivepre-training.
takes input text and transforms it into a detailed decodershandlingvisualtextimagery,asshownin
RGBimage,x ∈ RH×W×C. Wedefinetheheight Figure1(a). Thisprocesscommencesbyrendering
(H)at16pixelsandthewidth(W)at16,384pix- the textual input as RGB images x ∈ RH×W×C
els, encapsulating the text within a 24-bit color asaforementionedin§3.1,subsequentlypartition-
depthacrossthreechannels(C = 3),thusforming ing these into uniform patches x ∈ RN×(P2·C)
p
avisualtextimagethatrepresentsagridof1024 illustratedasFigure8,where(H,W)definesthe
patches,each16x16pixelsinsize. originalimage’sresolution,(P,P)specifieseach
The text renderer supports rendering required patch’s resolution with P = H, and N = W/P
foradiversesetoftextualrepresentations,includ- denotesthetotalnumberofpatches. Thepatches
ingmulticoloredemojis,bidirectionaltextsystems, are then flattened, mapped to a D-dimensional
and scripts necessitating the use of ligatures. In space through a learnable linear projection, and
alignment with models like PIXEL, our text se- finally fed into the transformer’s sequential pro-
quences may be single paragraphs or pairs of re- cessing stream. Unlike ViT, which caters to two-
latedsegments. Weuse16x16blackpatchesasvi- dimensional inputs, our model processes these
sualcuesforend-of-sequence(EOS)marker. These patches in the sequence order in which the text
patchesaretreatedasnon-interactiveelementsby appears,emulatingthelinearprogressionofread-
our model, where no attention mechanism is en- ing. Thispatch-basedsegmentationalignswiththe
gagedorlosscalculated. sequentialnatureoflanguage,enablingourmodel
When confronted with sequences that surpass topredictivelylearnfromthevisualdata.
themaximumlengththreshold,ourmodelemploys Text Input We leverage the same tokenizer as
strategiesoftruncationorsegmentationintomulti- Llama2,segmentinginputtextintodiscretetokens
plesequences,ensuringefficientprocessingwhile withatotalvocabularysizeof32k. Thesetokens
preserving contextual integrity. We refer to Ap- arethentransformedintodensevectorrepresenta-
pendix§Afortherenderingdetails. tionsthroughanembeddinglookuptable.
3.2 InputRepresentation 3.3 Pre-trainingObjectives
Thetransformerdecoderingestsalinearsequence As illustrated in Figure 2, our training architec-
ofembeddings,eachderivedfromdiscretepatches turefeaturesseparateheadsfollowingtheterminal
ofimagedataortextualtokens, forvisualortext transformerlayersforvariousinputs.
inputs,respectively. NextPatchPrediction GivenasequenceofN
ImageInput InspiredbytheVisionTransformer visualpatchesx = (x1,x2,··· ,xN)whereeach
p p p p
(ViT,Dosovitskiyetal.,2020),ourmethodtailors visualpatchxt isaflattenedpatchembedding. We
p
theimagepatchprocessingparadigmtothesequen- decomposetheimagepatchsequenceintothepro-
tialprocessingneedsofautoregressivetransformer ductionofN conditionalprobabilities:Next Patch Prediction Next Token Prediction
Transformer Decoder
2 Image Encoding 3 Text Tokenization
My cool cat ᓚᘏᗢ sits in a beautiful box lunch and dinner. My cool cat ᓚᘏᗢ sits in a beautiful box lunch and dinner.
Rendered Image 1 Render Text as Image Input Text
Figure2: Illustrationofdual-modalitypre-trainingonpairedtext-image(DualGPT).Autoregressivepre-trainingon
puretextandvisualtextimages,applynextpatchpredictionandnexttokenprediction,respectively.
corporateRMSNormforpre-normalization(Zhang
and Sennrich, 2019), SwiGLU activation func-
N
(cid:89) tions(Shazeer,2020;Chaietal.,2020),rotarypo-
p(x1,x2,··· ,xN) = p(xt|x1,x2,··· ,xt−1)
p p p p p p p
sition embeddings (Su et al., 2024), and grouped
t=1
(1) queryattention(Ainslieetal.,2023). Comprehen-
Forvisualinputs,weemployanextpatchpredic- sivespecificationsandadditionalimplementation
tion strategy, where a normalized mean squared detailsofourarchitectureareinAppendix§B.
error(MSE)lossquantifiesthepixelreconstruction Data For visual image data, we use rendered
accuracybycomparingthenormalizedtargetimage the corpus of peS2o, English Wikipedia and C4
patcheswiththereconstructedoutputs,excluding datasets for pre-training; while for text data, we
theEOSpatches. adopt peS2o, English Wikipedia, C4, Commen
NextTokenPrediction Fortextinputs,weuti- Crawl, and The Stack v1. We refer the readers
lizeaconventionalnexttokenpredictionobjective, toAppendix§Cfordetails.
optimizingacross-entropylossthatevaluatesthe
fidelityofpredictedtokensequencesgeneratedvia 4 Experiments
teacher-forcingagainstthegroundtruthtokens.
4.1 ExperimentalSetup
3.4 ModelConfiguration
Fine-tuningProtocols Ourevaluationentailed
To explore previous research questions, our pre- fine-tuning an autoregressive pixel-based pre-
training regimen explores various configurations trainedmodelfordownstreamtaskstothoroughly
for ablation analysis: (1) TextGPT: Pre-training assessitsperformance. Weadaptedourpixel-based
solely on text data. (2) PixelGPT: This involves modeltovariousdownstreamtasksbysubstituting
trainingsolelyonrenderedimagedata,employing thelanguagemodelingheadwithalinearMLPfor
a mean squared error (MSE) loss, as visualized downstreamtasks. Specifically,PixelGPT,initially
in Figure 1(a). (3) MonoGPT: Trained on separate pre-trainedonpixeldata,undergoesfine-tuningon
streams of rendered image and text data without similarlyrenderedpixeldata. Conversely,MonoGPT
any intermodal pairing. (4) DualGPT: Trained on and DualGPT, which benefitted from a joint pre-
unpairedimageandtextinput,andonpairedimage- training regime incorporating both text and pixel
text data (dual-modality). When handling paired data,werefine-tunedacrossdifferentinputmodali-
data,weconcatenatetheimagedatasequencebe- ties: pixel,text,andacombinationofboth.
fore the text sequence and feed them simultane- EvaluationTasks Ourassessmentofthegenera-
ouslytothemodel,asdelineatedinFigure2. We tivepixelpre-trainingmodelsencompassestasksin
refertoAppendix§Dfordetails. naturallanguageunderstanding(NLU)andcross-
linguallanguageunderstanding. ForNLU,weuti-
3.5 Pre-trainingDetails
lizetheGLUEbenchmark,aligningthefine-tuning
ModelArchitecture Ourarchitecture,illustrated datarenderingapproachwiththepre-trainingpro-
inFigure1(b),isbuiltuponastackofN = 24stan- cessoutlinedinAppendixA.Sentencepairsfrom
dard transformer decoder (Vaswani et al., 2017), GLUE’snaturallanguageinferencetasksareindi-
followingLlama2(Touvronetal.,2023b). Wein- viduallyrenderedandsubsequentlyconcatenated,Input Modality MNLI-m/mm QQP QNLI SST-2 CoLA STS-B MRPC RTE WNLI
Model #Param Avg.
Text Pixel Acc F1 Acc Acc MCC Spear. F1 Acc Acc
BERT 110M ✓ ✗ 84.0/84.2 87.6 91.0 92.6 60.3 88.8 90.2 69.5 51.8 80.0
GPT-2 126M ✓ ✗ 81.0 89.4 87.7 92.5 77.0 74.9 71.5 52.0 54.9 75.6
DONUT 143M ✗ ✓ 64.0 77.8 69.7 82.1 13.9 14.4 81.7 54.9 57.7 57.2
CLIPPO 93M ✗ ✓ 77.7/77.2 85.3 83.1 90.9 28.2 83.4 84.5 59.2 - -
PIXEL 86M ✗ ✓ 78.1/78.9 84.5 87.8 89.6 38.4 81.1 88.2 60.5 53.8 74.1
PIXAR 85M ✗ ✓ 78.4/78.6 85.6 85.7 89.0 39.9 81.7 83.3 58.5 59.2 74.0
PixelGPT 317M ✗ ✓ 79.0/78.2 86.0 85.6 90.1 35.3 80.3 84.6 63.9 59.2 74.2
Table1: ComparativeevaluationontheGLUEbenchmark. Performancemetricsforeachmodelacrossvarious
GLUE tasks are presented, along with the aggregate average performance. #Param indicates the model scale.
PixelGPTstandsoutastheleadingmodel,surpassingotherpixel-basedcounterpartsintermsofoverallperformance.
withablackblockservingastheend-of-sentence mentscomparedtoGPT-2. Thisdemonstratesthe
token. Thecross-lingualunderstandingcapability viability of pixel-based pre-training in capturing
is evaluated on the XNLI dataset over fifteen dif- complexlinguisticconstructs.
ferentlanguages. FollowingConneauetal.(2020), Moreover,whenjuxtaposedwithPIXEL,which
our evaluation is performed in two distinct sce- leverages a bidirectional encoder architecture,
narios: (1) Translate-Train-All, where the model PixelGPTexhibitsenhancedperformanceinQQP
is fine-tuned on a blend of original English and (+1.5), RTE (+3.4), and WNLI (+5.4). These
machine-translateddatafromother14languages, results collectively affirm the hypothesis that au-
aiming to appraise the model’s multilingual un- toregressive pre-training on RGB visual data is
derstanding; (2) Cross-lingual Transfer settings, feasible and advantageous for language model-
wherein fine-tuning is conducted solely on En- ing. PixelGPTachievestheoptimalperformance
glishdata,withmulti-languagetestsetsemployed among pixel-based approaches on GLUE, under-
to evaluate the model’s transferability across lan- scoring the transformative impact of integrating
guages. Comprehensive experimental details are richvisualinformationintopre-training.
providedintheAppendix§E.
RQ2: Impact of Autoregressive Pixel Pre-
Baselines Forathoroughevaluation,webench-
trainingonMultilingualTasks. Traditionallan-
markagainstmodelsspecializedintextualandvi-
guagemodels,exemplifiedbyBERT,typicallyuti-
sualrepresentations. Inthetextualcategory,BERT
lizeasubwordtokenizationprocesssuchasWord-
andGPT-2(Radfordetal.,2019)arechosen. For
Piece(Devlinetal.,2019)orBPE(Sennrichetal.,
pixel-basedmodels,wecontrastourapproachwith
2015)thatdecomposessentencesintoapredefined
DONUT (Kim et al., 2022), CLIPPO (Tschan-
setoftexttokens. Whileeffectivewithinthescope
nen et al., 2023), PIXEL (Rust et al., 2023), and
ofasinglelanguageorsimilarlanguagefamilies,
PIXAR (Tai et al., 2024), which are trained on
thisapproachisconstrainedbyavocabularybottle-
pixel-based representation. Detailed discussions
neck (Rustetal.,2023)inmultilingualscenarios,
areprovidedinAppendix§F.
limiting its efficacy. Pixel-based representations,
however,transcendthislimitationbyrepresenting
4.2 Results
textinamodalitythatinherentlysupportsunified
RQ1: AutoregressivePixel-basedPre-training processing—thevisualdomainofimages.
RivalsPIXEL. Ourempiricalinvestigation,de- In our cross-lingual evaluation, conducted on
tailedinTable1,scrutinizesthefeasibilityofpure theXNLIdatasetinthetranslate-train-allconfig-
pixel-based autoregressive pre-training on RGB urationanddetailedinTable2,PixelGPTdemon-
images of visual texts. The proposed PixelGPT stratesarobustcapabilityformultilingualcompre-
model, training solely on rich 24-bit RGB vi- hension. It not only matches the performance of
sualinputs,demonstratesnotmerelyacompetitive BERT,butalsoconsistentlysurpassesthePIXEL
edge but, in several tasks, surpasses the perfor- model in average accuracy across evaluated lan-
manceofmodelspre-trainedontextalone. Specifi- guages. Remarkably, PixelGPT exhibits pro-
cally,PixelGPTexhibitsremarkablesuperiorityon nounced gains over BERT in languages that di-
GLUEbenchmarks—evidencedbyitsmarkedper- vergesignificantlyfromEnglish,suchasThaiand
formanceincreasesontheSTS-B(+5.4),MRPC Chinese, with improvements of +11.3 and +4.3,
(+13.1), RTE (+11.9), and WNLI (+4.3) assess- respectively. ThisenhancedperformancemaybeInput Modality
Model #lg #Param ENG ARA BUL DEU ELL FRA HIN RUS SPA SWA THA TUR URD VIE ZHO Avg.
Text Pixel
Fine-tune model on all training sets (Translate-train-all)
mBERT 104 179M ✓ ✗ 83.3 73.2 77.9 78.1 75.8 78.5 70.1 76.5 79.7 67.2 67.7 73.3 66.1 77.2 77.7 74.8
XLM-R base 100 270M ✓ ✗ 85.4 77.3 81.3 80.3 80.4 81.4 76.1 79.7 82.2 73.1 77.9 78.6 73.0 79.7 80.2 79.1
BERT 1 110M ✓ ✗ 83.7 64.8 69.1 70.4 67.7 72.4 59.2 66.4 72.4 62.2 35.7 66.3 54.5 67.6 46.2 63.9
PIXEL 1 86M ✗ ✓ 77.2 58.9 66.5 68.0 64.9 69.4 57.8 63.4 70.3 60.8 50.2 64.0 54.1 64.8 52.0 62.8
PixelGPT 1 317M ✗ ✓ 77.7 55.4 66.7 69.0 67.4 71.2 59.1 65.6 71.4 61.7 47.0 65.2 54.4 66.1 50.5 63.2
Table2: Cross-lingualperformanceevaluationontheXNLIdatasetintranslate-train-allsettings. Wereportthe
accuracyachievedbyeachmodelacrossthemultiplelanguagesfeaturedintheXNLIdataset, alongwiththeir
average accuracy scores. The number of languages (#lg) incorporated during pre-training and the model size
(#Param)areprovidedforreference. PixelGPTdemonstratessuperiorperformanceoverPIXEL,showcasingthe
efficacyofexclusivepixel-basedinputmodalityincross-lingualcontexts.
attributed to two primary factors: the absence of This result reinforces the proposition that pre-
PixelGPT’srelianceonlanguage-specifictokeniza- training modality conflicts can be effectively re-
tion,enablingmoreeffectivelearningfromthevi- solvedviatheintegrationofpaireddual-modality
sualforms of text, and thelimitationsofBERT’s data,fosteringmorerobustmultimodallearning.
English-centricpre-training,whichexhibitsshort-
4.3 Analysis
comingswhenfacedwithlinguisticallydistantfam-
ilies. Thus, PixelGPT’s proficiency in leverag-
ing the visual features of text contributes to its 80 BERT
advancedmultilingualunderstanding,signalinga
75
PIXELPixelGPT
significantstrideinovercomingthechallengesas-
sociatedwiththevocabularybottleneck. 70
65
RQ3: Synergistic Effects of Multimodal Pre-
Model training. In our investigation into the inter- 60 TextGPT (text only)
PixelGPT (pixel only)
playbetweendistinctpre-trainingdatamodalities, MonoGPT (text+pixel only)
55 DualGPT (text+pixel+pair)
we contrasted the performances of MonoGPT and Inference modality
text
DualGPT—models that integrate different input 50 pixel
10 20 40 100 200
modalities—with that of TextGPT under equiva- #tokens/patches(B)
lent conditions of aligned text token pre-training.
Figure3: Trainingtokens/patchesversusoverallperfor-
TextGPT and MonoGPT underwent pre-training on manceonGLUEbenchmark.
40 billion text tokens, with MonoGPT additionally
ScalingTrainingTokensvs. GLUEPerformance
exposedto40billionimagepatches. DualGPT,on
InFigure3,wedelineatethecorrelationbetween
theotherhand,waspre-trainedon38.4billiontext
the scale of training data and the ensuing per-
tokenscomplementedby48billionimagepatches
formance on the GLUE benchmark. Our analy-
and9.6billiontokensofimage-textpaireddata.
sis encompasses a spectrum of total training to-
Thiscomparativeanalysis,spanningbothGLUE kens/patches from 10 billion (B) to 240B, jux-
andXNLIdatasets(thelatterwithinthetranslate- taposing the trajectories of TextGPT, PixelGPT,
train-all settings), is shown in Tables 3 and 4. A MonoGPT, and DualGPT, with BERT and PIXEL
pivotal finding is that the incorporation of dual- servingasbenchmarks. TheMonoGPTandDualGPT
modality data during pre-training markedly en- models are evaluated under two different input
hances average performance across language un- modalities: textandpixel. Fromourfindings,two
derstandingtasks: DualGPT(76.9)surpassesboth primaryinsightsemerge: (1)Pixel-basedautore-
TextGPT(76.3)andMonoGPT(75.4). Thissug- gressivepretrainingmodelsexhibitanincreased
geststhatpotentialconflictsarisingfromunimodal datademand. Withminimaltraining(e.g.,at10B),
training can be significantly alleviated through a pixel-basedmodelsinitiateatalowerperformance
multimodalpre-trainingapproach. Thisinference thresholdinpixelmodality(allunder55%),com-
is corroborated by XNLI outcomes, wherein the pared to their text modality counterparts, which
addition of pixel-text paired data bolstered the approximate a performance level of 70%. Never-
model’smultilingualinterpretativeproficiency. theless,withtheincreaseoftrainingdata,acritical
Further,withpixelmodalityinput,DualGPTsur- volumethresholdcatalyzesasubstantialriseinper-
passesTextGPTacrossvariousdownstreamtasks. formanceforPixelGPT,MonoGPT,andDualGPTin
ecnamrofreP
).gvA(
EULGInput Modality MNLI-m/mm QQP QNLI SST-2 CoLA STS-B MRPC RTE WNLI
Model Avg.
Text Pixel Acc F1 Acc Acc MCC Spear. F1 Acc Acc
TextGPT (text only) ✓ ✗ 79.9/80.0 86.1 86.1 91.5 47.3 85.8 86.3 63.5 56.3 76.3
✓ ✗ 80.0/80.5 85.9 87.3 90.1 40.2 83.8 87.0 62.8 56.3 75.4
MonoGPT (text+pixel)
✗ ✓ 64.7/65.9 78.9 77.3 74.8 11.6 73.2 83.5 59.9 57.7 64.8
✓ ✗ 80.1/80.4 86.5 86.8 91.6 49.0 85.4 87.6 65.7 56.3 76.9
DualGPT (text+pixel+pair)
✗ ✓ 71.5/71.7 82.8 81.6 83.4 17.2 80.2 84.1 66.4 59.2 69.4
Table3: AblationresultsofmodelperformanceontheGLUEbenchmark.
Input Modality
Model ENG ARA BUL DEU ELL FRA HIN RUS SPA SWA THA TUR URD VIE ZHO Avg.
Text Pixel
Fine-tune model on all training sets (Translate-train-all)
TextGPT (text only) ✓ ✗ 72.4 60.4 62.8 64.8 63.3 65.0 58.5 61.5 65.2 57.7 59.9 61.2 54.9 63.6 63.1 62.3
✓ ✗ 72.9 60.8 63.2 63.5 63.5 63.6 57.9 60.7 64.4 58.8 59.4 60.6 55.2 63.2 60.7 61.9
MonoGPT (text+pixel) ✗ ✓ 66.8 47.1 61.2 61.8 63.4 64.5 56.7 59.2 64.9 56.8 48.7 61.8 52.1 61.0 50.7 58.4
✓ ✗ 72.7 61.6 63.8 64.7 63.9 65.1 58.8 61.6 65.4 59.0 59.8 62.2 55.8 63.4 62.1 62.7
DualGPT (text+pixel+pair) ✗ ✓ 71.7 55.0 67.6 66.5 66.8 68.4 59.0 64.4 68.9 61.3 48.7 64.3 54.7 65.8 54.4 62.5
Table4: AblationresultsofmodelperformanceonXNLIunderTranslate-Train-Allsettings.
pixel modality. This trajectory reveals a progres- ure4,focusedontheTranslate-Train-Allsettingof
sive convergence of PixelGPT towards the text- theXNLIbenchmark. (1)Pixel-basedautoregres-
based baseline, culminating in its overtaking of sive models display a heightened requirement
PIXEL at around 200B tokens/patches and near- for training data in multilingual tasks, corrob-
ingTextGPTwithalessthan5-pointperformance orating the trend observed on the GLUE bench-
differential, while still on an upward trend. (2) mark. Initially, there is a notable performance
The integration of paired dual-modality data disparity between pixel and text modalities, with
duringpretrainingappearstoconfersignificant pixel-basedmodelslaggingbehindwhentraining
benefitsonmultimodallearning,particularlyfor on a lesser volume of tokens/patches. However,
pixel-basedinput. Whenmatchedfortrainingdata thisgapdiminishessubstantiallywiththeincrease
volume, DualGPT consistently eclipses MonoGPT in training volume. Remarkably, upon reaching
across comparable benchmarks, with the former the200B,PixelGPTnotonlysurpassesPIXELbut
maintaining a pronounced lead in pixel modality. alsomatchestheperformanceofBERT,indicating
Thistrendunderscoresthevalueofincorporating a continued potential for further enhancement in
pairedtext-imagedatainpretrainingtoenhancethe itsmultilingualproficiencywithadditionaltraining
efficacyofmultimodallearning. data. (2)Theinjectionofdual-modalitydataat
theearlystagesoftrainingappearstobepartic-
ularlybeneficialformodelslearningfrompixel
65
BERTPixelGPT
data. WhencomparingDualGPTandMonoGPTun-
PIXEL
der the pixel modality, DualGPT demonstrates a
60
notable performance advantage at the outset of
training (55% vs. 45.8% at the 10B token/patch
55
Model mark). Although this edge tapers as the train-
TextGPT (text only)
PixelGPT (pixel only) ing volume expands, it suggests that early-stage
50 MonoGPT (text+pixel)
DualGPT (text+pixel+pair) multimodalalignmentaidsthepixel-basedmodels
Inference modality
text in leveraging the textual data for enhanced mul-
pixel
45 10 20 40 100 200 tilingualunderstanding. (3)Ourtext-basedpre-
#tokens/patches(B)
training approach, TextGPT, demonstrates su-
Figure4: Trainingtokens/patchesversusoverallperfor- periorresultsoverBERT.Thisisevidentwhen
manceonXNLIbenchmark.
trainingreachesapproximately100Btokens,where
Scaling Training Tokens vs. XNLI (Translate- TextGPT outperforms BERT. This improvement
Train-All)Performance Wefurtherexploredthe may be attributed, in part, to our byte-level BPE
progressionofmodelperformanceinmultilingual tokenization as utilized in Llama 2, which effec-
capability across varying volumes of pre-trained tivelydeconstructsunseenlanguagesintotheircon-
tokens/patches. Thiscomparison,delineatedinFig- stituent raw bytes—a capability not afforded by
)niart-etalsnart(
ecnamrofreP
)gva(ILNXQQP CoLA STS-B GoNotoCurrent JournalDingbats1
86 35
75 NotoSerif-Regular
90 85 30 50
25 70
84 25
0 50
64128256512 64128256512 64128256512
30
Figure5: Analysisofescalatingtheglobalbatchsize.
10
CoLA STS-B MRPC RTE WNLI
BERT.Additionally,theenrichmentofourtextpre-
training corpus from diverse sources contributes Figure6: Analysisoffine-tuningondifferentfonts.
to this. For a detailed breakdown of the text pre-
trainingdata,wereferreaderstoAppendix§C.2. Render Mode Font Acc ∆
A Large Batch Size Improves Stable Train-
Grayscale 58.7 -
Apple Emoji
ing We observe a distinct preference for larger RGB 61.4 +2.7
batch sizes when fine-tuning pixel-based modal-
ities across certain datasets. As in Figure 5, we Table 5: Comparison performance on HatemojiBuild
datasetwithgrayscaleandRGBrendering.
evaluatehowdifferentbatchsizes—64,128,256,
and 512—affect model performance on selected RGB Rendering Grayscale Rendering
GLUEbenchmarktasks,namelyQQP,CoLA,and
STS-B. A clear trend emerges from the data: in- Prediction: hate Prediciton: non-hate
creasing the batch size correlates with improved
Prediction: hate Prediciton: non-hate
model performance. Our analysis suggests that
pixelmodalityfine-tuningexhibitsgreatervariance Figure7: ExamplecasesofHatemojiBuildpredictions.
✓and✗indicatethecorrectandincorrectpredictions.
than text modality and benefits from the use of
largerbatchsizes. Thisappearstomitigatethevari-
richerinformationalcontent. Weevaluatedtheper-
abilityinherentindifferenttrainingbatches,thus
formanceoftheserenderingapproachesonHate-
enhancingtrainingstability. Itpreventspremature
mojiBuild dataset (Kirk et al., 2022), designed
convergencetosuboptimallocalminimaandfos-
fordetectingonlinehatespeechconveyedthrough
tershighermodelaccuracy.
emojis. Table 5 presents our findings, where the
Font Transfer Analysis We extend to ex-
RGB-rendereddatafine-tuningsignificantlyoutper-
amining the adaptability of PixelGPT to di-
formsitsgrayscalecounterpart. Thisperformance
verse font styles during fine-tuning. We em-
enhancement can be attributed to the model’s ca-
ployed three distinct fonts for rendering the data:
pacity to utilize color cues within emojis, which
GoNotoCurrent, which was utilized during pre-
are critical for inferring the emotional context of
training;NotoSerif-Regular,afontstylistically
sentences. Foramoredetailedillustration,Figure7
akin to GoNotoCurrent; and JournalDingbats1,
providesspecificexampleswherecolorretention
a font that renders text as distinct image-based
hasimprovedmodelinterpretability.
symbols,markedlydivergentfromtheothers. The
adaptabilitywastestedacrossfivedatasetsfromthe 5 ConclusionandFutureWork
GLUEbenchmark—CoLA,STS-B,MRPC,RTE,
In this paper, we have investigated the potential
and WNLI. As depicted in Figure 6, the perfor-
of pixel-based autoregressive pre-training using
manceofPixelGPTremainedstableacrossdiffer-
visual text images. Our results demonstrate that
ent fonts for all selected datasets barring CoLA.
incorporatingvisualorthographicfeaturessignifi-
Notably,evenwhenfine-tunedwithdatarendered
cantlyenhanceslanguageunderstandingandmul-
in JournalDingbats1, which bears little resem-
tilingualcapabilities. Additionally,ourempirical
blancetothepre-trainingfont,theresultsdemon-
findings suggest that using pixel-text paired data
stratedacommendabledegreeofresilience,indicat-
effectively reduces modality competition during
ingthatthepixelpre-trainingisrobusttogeneralize
training, thereby improving model performance.
acrosssignificantlyvariedvisualrepresentations.
Looking forward, scaling this approach to larger
Impact Analysis of Color Retention Unlike
modelsizesholdsconsiderablepromiseforadvanc-
PIXAR that renders text as binary images,
ingthefieldofmultimodallanguageprocessing.
PixelGPTemploysRGB-rendereddata, retaining
1F CCM .raepS
ecnamrofrePLimitations Environmental Impact The training of large-
scalemodelsisresource-intensiveandhasasignif-
ModelScale Thecurrentimplementationofour
icant environmental footprint. We must consider
model utilizes 24 layers of transformer decoders,
sustainablepracticesinmodeltraining,including
which has been effective for the scope of our ex-
optimizingcomputationalefficiencyandexploring
perimentalframework. However, theexploration
energy-efficienthardwaretoreducetheoverallcar-
ofscalingourmodeltomuchlargerconfigurations,
bonemissionsassociatedwithourresearch.
such as 7B, 13B, 70B, or over 100B parameters,
remainsuntested. Expandingthelanguagemodel’s
MisusePotential Whileourstudyfocusesonthe
capacitycouldsignificantlyimproveitsabilityof
positiveapplicationsofenhancingmultilingualca-
scaling, potentially enhancing both performance
pabilities and understanding, there is a potential
andgeneralizability.
formisuseinvariouscontexts. Weadvocateforre-
TrainingCompute Ourtrainingwasrestricted sponsibleuseguidelinesandtransparencyinmodel
by computational resources, limiting us to pre- deployment to prevent malicious applications of
training on only 100 to 200 billion tokens or thetechnology.
patches. This constraint curtails our capacity to
Continual Monitoring and Evaluation Post-
exploitthefullbenefitsofextensivedatascaletrain-
deployment monitoring and ongoing evaluation
ing. Future work can extend the pre-training to
of the model’s performance and societal impact
more than 1,000 billion tokens or patches could
are crucial. This process helps ensure the model
yieldpromisinginsightsintothescalability.
adaptstochangesovertimeandcontinuestooper-
PreliminaryNatureofStudy Itiscrucialtoac- atewithintheethicalboundariessetforthbyevolv-
knowledgethatthisresearchconstitutesaprelim- ingstandardsandexpectations.
inary foray into the realm of pixel-based autore- Byaddressingtheseethicalconsiderations,we
gressive models for multilingual and multimodal aimto promoteresponsible research andapplica-
languageprocessing. Assuch,whiletheresultsare tion of advanced machine learning techniques in
encouraging,theyshouldbeviewedasexploratory. languageprocessing,contributingpositivelytothe
We invite further research to build upon our ini- fieldandsocietyatlarge.
tialfindings,addressingtheselimitationsandfur-
thertestingtherobustnessandapplicabilityofthe
modelinawiderarrayofsettings. References
EthicalConsiderations JoshuaAinslie,JamesLee-Thorp,MichieldeJong,Yury
Zemlyanskiy,FedericoLebrón,andSumitSanghai.
Thisresearchintopixel-basedautoregressivepre- 2023. Gqa: Traininggeneralizedmulti-querytrans-
trainingforvisualtextimagesraisesseveralethical formermodelsfrommulti-headcheckpoints. arXiv
preprintarXiv:2305.13245.
considerationsthatwarrantcarefulattention:
Data Privacy and Security The utilization of Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-
visualtextimages,especiallyfromdiversesources Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan
Schalkwyk,AndrewM.Dai,AnjaHauth,KatieMil-
suchasmultilingualdatasets,necessitatesstringent
lican, David Silver, Slav Petrov, Melvin Johnson,
adherencetodataprivacyandsecurityguidelines.
Ioannis Antonoglou, Julian Schrittwieser, Amelia
It is vital to ensure that all data used for training Glaese, Jilin Chen, Emily Pitler, Timothy P. Lilli-
andtestingrespectstheprivacyrightsofindividuals crap,AngelikiLazaridou,OrhanFirat,JamesMolloy,
Michael Isard, Paul Ronald Barham, Tom Henni-
andcomplieswithapplicablelegalframeworks.
gan,BenjaminLee,FabioViola,MalcolmReynolds,
BiasandFairness Machinelearningmodels,par- YuanzhongXu,RyanDoherty,EliCollins,Clemens
Meyer, Eliza Rutherford, Erica Moreira, Kareem
ticularlythoseinvolvedinlanguageprocessing,are
Ayoub, Megha Goel, George Tucker, Enrique Pi-
susceptible to biases that may be present in the queras,MaximKrikun,IainBarr,NikolaySavinov,
trainingdata. Itisimperativetoconductthorough IvoDanihelka,BeccaRoelofs,AnaïsWhite,Anders
biasauditsandfairnessassessmentstoidentifyand Andreassen, Tamara von Glehn, Lakshman Yagati,
MehranKazemi,LucasGonzalez,MishaKhalman,
mitigateanydiscriminatorypatternsinmodelpre-
JakubSygnowski,andetal.2023. Gemini: Afam-
dictions,ensuringthatthetechnologyisequitable
ily of highly capable multimodal models. CoRR,
acrossdifferentlanguagesandculturalcontexts. abs/2312.11805.YekunChai,ShuoJin,andXinwenHou.2020. High- PatrickEsser,RobinRombach,andBjornOmmer.2021.
waytransformer: Self-gatingenhancedself-attentive Tamingtransformersforhigh-resolutionimagesyn-
networks. InProceedingsofthe58thAnnualMeet- thesis. InProceedingsoftheIEEE/CVFconference
ingoftheAssociationforComputationalLinguistics, oncomputervisionandpatternrecognition, pages
pages6887–6900,Online.AssociationforComputa- 12873–12883.
tionalLinguistics.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li,
YekunChai,ShuohuanWang,ChaoPang,YuSun,Hao PiotrDollár,andRossB.Girshick.2022. Maskedau-
Tian, and Hua Wu. 2023. ERNIE-code: Beyond toencodersarescalablevisionlearners. InIEEE/CVF
English-centriccross-lingualpretrainingforprogram- ConferenceonComputerVisionandPatternRecog-
minglanguages. InFindingsoftheAssociationfor nition, CVPR 2022, New Orleans, LA, USA, June
ComputationalLinguistics: ACL2023,pages10628– 18-24,2022,pages15979–15988.IEEE.
10650,Toronto,Canada.AssociationforComputa-
Geewook Kim, Teakgyu Hong, Moonbin Yim,
tionalLinguistics.
JeongYeonNam,JinyoungPark,JinyeongYim,Won-
seok Hwang, Sangdoo Yun, Dongyoon Han, and
MarkChen,AlecRadford,RewonChild,JeffreyWu,
Seunghyun Park. 2022. Ocr-free document under-
HeewooJun,DavidLuan,andIlyaSutskever.2020.
Generativepretrainingfrompixels. InProceedingsof standing transformer. In European Conference on
the37thInternationalConferenceonMachineLearn-
ComputerVision,pages498–517.Springer.
ing, ICML 2020, 13-18 July 2020, Virtual Event,
Hannah Kirk, Bertie Vidgen, Paul Rottger, Tristan
volume 119 of Proceedings of Machine Learning
Thrush,andScottHale.2022. Hatemoji: Atestsuite
Research,pages1691–1703.PMLR.
andadversarially-generateddatasetforbenchmark-
inganddetectingemoji-basedhate. InProceedings
AlexisConneau,KartikayKhandelwal,NamanGoyal,
ofthe2022ConferenceoftheNorthAmericanChap-
Vishrav Chaudhary, Guillaume Wenzek, Francisco
teroftheAssociationforComputationalLinguistics:
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
HumanLanguageTechnologies,pages1352–1368,
moyer,andVeselinStoyanov.2020. Unsupervised
Seattle,UnitedStates.AssociationforComputational
cross-lingualrepresentationlearningatscale. InPro-
Linguistics.
ceedingsofthe58thAnnualMeetingoftheAssocia-
tionforComputationalLinguistics,ACL2020,On-
DenisKocetkov,RaymondLi,LoubnaBenAllal,JiaLi,
line,July5-10,2020,pages8440–8451.Association
ChenghaoMou,CarlosMuñozFerrandis,YacineJer-
forComputationalLinguistics.
nite,MargaretMitchell,SeanHughes,ThomasWolf,
etal.2022. Thestack: 3tbofpermissivelylicensed
AlexisConneau,RutyRinott,GuillaumeLample,Ad-
sourcecode. arXivpreprintarXiv:2211.15533.
inaWilliams,SamuelR.Bowman,HolgerSchwenk,
andVeselinStoyanov.2018. XNLI:evaluatingcross- Jonas F. Lotz, Elizabeth Salesky, Phillip Rust, and
lingualsentencerepresentations. InProceedingsof Desmond Elliott. 2023. Text rendering strategies
the2018ConferenceonEmpiricalMethodsinNatu- for pixel language models. In Proceedings of the
ralLanguageProcessing,Brussels,Belgium,Octo- 2023ConferenceonEmpiricalMethodsinNatural
ber31-November4,2018,pages2475–2485.Asso- LanguageProcessing,EMNLP2023,Singapore,De-
ciationforComputationalLinguistics. cember6-10,2023,pages10155–10172.Association
forComputationalLinguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT: Pre-training of AntonLozhkov,RaymondLi,LoubnaBenAllal,Fed-
deepbidirectionaltransformersforlanguageunder- ericoCassano, JoelLamy-Poirier, NouamaneTazi,
standing. InProceedingsofthe2019Conferenceof AoTang,DmytroPykhtar,JiaweiLiu,YuxiangWei,
theNorthAmericanChapteroftheAssociationfor Tianyang Liu, Max Tian, Denis Kocetkov, Arthur
ComputationalLinguistics: HumanLanguageTech- Zucker, Younes Belkada, Zijian Wang, Qian Liu,
nologies,Volume1(LongandShortPapers),pages DmitryAbulkhanov,IndraneilPaul,ZhuangLi,Wen-
4171–4186,Minneapolis,Minnesota.Associationfor DingLi,MeganRisdal,JiaLi,JianZhu,TerryYue
ComputationalLinguistics. Zhuo,EvgeniiZheltonozhskii,NiiOsaeOsaeDade,
WenhaoYu,LucasKrauß,NamanJain,YixuanSu,
Alexey Dosovitskiy, Lucas Beyer, Alexander XuanliHe,MananDey,EdoardoAbati,YekunChai,
Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Niklas Muennighoff, Xiangru Tang, Muhtasham
Thomas Unterthiner, Mostafa Dehghani, Matthias Oblokulov,ChristopherAkiki,MarcMarone,Cheng-
Minderer,GeorgHeigold,SylvainGelly,etal.2020. haoMou, MayankMishra, AlexGu, BinyuanHui,
An image is worth 16x16 words: Transformers TriDao,ArmelZebaze,OlivierDehaene,NicolasPa-
for image recognition at scale. arXiv preprint try,CanwenXu,JulianJ.McAuley,HanHu,Torsten
arXiv:2010.11929. Scholak,SébastienPaquet,JenniferRobinson,Car-
olyn Jane Anderson, Nicolas Chapados, and et al.
Alaaeldin El-Nouby, Michal Klein, Shuangfei Zhai,
2024. Starcoder2andthestackv2: Thenextgenera-
MiguelAngelBautista,AlexanderToshev,Vaishaal tion. CoRR,abs/2402.19173.
Shankar, Joshua M Susskind, and Armand Joulin.
2024. Scalablepre-trainingoflargeautoregressive OpenAI. 2023. GPT-4 technical report. CoRR,
imagemodels. arXivpreprintarXiv:2401.08541. abs/2303.08774.Alec Radford, Jeff Wu, Rewon Child, David Luan, Yintao Tai, Xiyang Liao, Alessandro Suglia, and An-
DarioAmodei,andIlyaSutskever.2019. Language tonio Vergari. 2024. Pixar: Auto-regressive lan-
modelsareunsupervisedmultitasklearners. guage modeling in pixel space. arXiv preprint
arXiv:2401.03321.
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
Lee,SharanNarang,MichaelMatena,YanqiZhou, Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
Wei Li, and Peter J Liu. 2020. Exploring the lim- bert, Amjad Almahairi, Yasmine Babaei, Nikolay
its of transfer learning with a unified text-to-text Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
transformer. Journalofmachinelearningresearch, Bhosale,DanBikel,LukasBlecher,CristianCanton-
21(140):1–67. Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott CynthiaGao,VedanujGoswami,NamanGoyal,An-
Gray,ChelseaVoss,AlecRadford,MarkChen,and thonyHartshorn,SagharHosseini,RuiHou,Hakan
IlyaSutskever.2021. Zero-shottext-to-imagegener- Inan,MarcinKardas,ViktorKerkez,MadianKhabsa,
ation. InInternationalconferenceonmachinelearn- IsabelKloumann,ArtemKorenev,PunitSinghKoura,
ing,pages8821–8831.Pmlr. Marie-AnneLachaux,ThibautLavril,JenyaLee,Di-
anaLiskovich,YinghaiLu,YuningMao,XavierMar-
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
PhillipRust,JonasF.Lotz,EmanueleBugliarello,Eliz-
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
abethSalesky,MiryamdeLhoneux,andDesmond
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Elliott.2023. Languagemodellingwithpixels. In
Ruan Silva, Eric Michael Smith, Ranjan Subrama-
TheEleventhInternationalConferenceonLearning
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-
Representations,ICLR2023,Kigali,Rwanda,May
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
1-5,2023.OpenReview.net.
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
Melanie Kambadur, Sharan Narang, Aurélien Ro-
Elizabeth Salesky, Neha Verma, Philipp Koehn, and
driguez,RobertStojnic,SergeyEdunov,andThomas
MattPost.2023. Multilingualpixelrepresentations
Scialom.2023a. Llama2: Openfoundationandfine-
fortranslationandeffectivecross-lingualtransfer. In
tunedchatmodels. CoRR,abs/2307.09288.
Proceedings of the 2023 Conference on Empirical
MethodsinNaturalLanguageProcessing,EMNLP
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
2023,Singapore,December6-10,2023,pages13845–
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
13861.AssociationforComputationalLinguistics.
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
Rico Sennrich, Barry Haddow, and Alexandra Birch.
tion and fine-tuned chat models. arXiv preprint
2015. Neuralmachinetranslationofrarewordswith
arXiv:2307.09288.
subwordunits. arXivpreprintarXiv:1508.07909.
MichaelTschannen,BasilMustafa,andNeilHoulsby.
NoamShazeer.2020. Gluvariantsimprovetransformer.
2023. CLIPPO:image-and-languageunderstanding
arXivpreprintarXiv:2002.05202.
frompixelsonly. InIEEE/CVFConferenceonCom-
puterVisionandPatternRecognition, CVPR2023,
LucaSoldaini,RodneyKinney,AkshitaBhagia,Dustin Vancouver, BC, Canada, June 17-24, 2023, pages
Schwenk,DavidAtkinson,RussellAuthur,BenBo- 11006–11017.IEEE.
gin,KhyathiChandu,JenniferDumas,YanaiElazar,
ValentinHofmann,AnanyaHarshJha,SachinKumar, AaronVanDenOord,OriolVinyals,etal.2017. Neural
LiLucy,XinxiLyu,NathanLambert,IanMagnusson, discreterepresentationlearning. Advancesinneural
Jacob Morrison, Niklas Muennighoff, Aakanksha informationprocessingsystems,30.
Naik, Crystal Nam, Matthew E. Peters, Abhilasha
Ravichander,KyleRichardson,ZejiangShen,Emma Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Strubell,NishantSubramani,OyvindTafjord,Pete Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Walsh,LukeZettlemoyer,NoahA.Smith,Hannaneh Kaiser,andIlliaPolosukhin.2017. Attentionisall
Hajishirzi,IzBeltagy,DirkGroeneveld,JesseDodge, youneed. Advancesinneuralinformationprocessing
and Kyle Lo. 2024. Dolma: An Open Corpus of systems,30.
ThreeTrillionTokensforLanguageModelPretrain-
ingResearch. arXivpreprint. Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2018.
LucaSoldainiandKyleLo.2023. peS2o(Pretraining GLUE: A multi-task benchmark and analysis plat-
Efficiently on S2ORC) Dataset. Technical report, formfornaturallanguageunderstanding. InProceed-
AllenInstituteforAI. ODC-By, https://github. ings of the Workshop: Analyzing and Interpreting
com/allenai/pes2o. NeuralNetworksforNLP,BlackboxNLP@EMNLP
2018,Brussels,Belgium,November1,2018,pages
JianlinSu, MurtadhaAhmed, YuLu, ShengfengPan, 353–355.AssociationforComputationalLinguistics.
Wen Bo, and Yunfeng Liu. 2024. Roformer: En-
hancedtransformerwithrotarypositionembedding. Adina Williams, Nikita Nangia, and Samuel R. Bow-
Neurocomputing,568:127063. man.2018. Abroad-coveragechallengecorpusforsentence understanding through inference. In Pro-
ceedingsofthe2018ConferenceoftheNorthAmer-
icanChapteroftheAssociationforComputational
Linguistics:HumanLanguageTechnologies,NAACL-
HLT2018,NewOrleans,Louisiana,USA,June1-6,
2018, Volume 1 (Long Papers), pages 1112–1122.
AssociationforComputationalLinguistics.
BiaoZhangandRicoSennrich.2019. Rootmeansquare
layernormalization. AdvancesinNeuralInformation
ProcessingSystems,32.Contents
1 Introduction 1
2 RelatedWork 2
3 Pre-trainingonPixelsandTexts 2
3.1 RenderingTextasImages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
3.2 InputRepresentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.3 Pre-trainingObjectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3.4 ModelConfiguration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.5 Pre-trainingDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
4 Experiments 4
4.1 ExperimentalSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
4.2 Results. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
4.3 Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
5 ConclusionandFutureWork 8
A TextRendererDetails 14
B ModelArchitecture 14
C Pre-trainingData 14
C.1 Pre-trainingDataforVisualImages . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
C.2 Pre-trainingDataforText . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
D Pre-trainingDetails 16
E Fine-tuningDetails 16
E.1 Fine-tuningDataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
E.2 Fine-tuningSetting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
E.3 ImplementationforDifferentRenderModes . . . . . . . . . . . . . . . . . . . . . . . . 17
F Baselines 17
F.1 Text-basedBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
F.2 Image-basedBaselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
G DetailedResults&Analysis 19
G.1 PerformanceonCross-lingualTransfer . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
G.2 ProbingDual-ModalityFine-Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
G.3 RGBvs. Grayscalevs. BinaryRendering . . . . . . . . . . . . . . . . . . . . . . . . . 19
G.4 ComparisononXNLIunderTranslate-Train-AllSettings . . . . . . . . . . . . . . . . . 20A TextRendererDetails embeddings are both set at 1024, facilitating de-
tailedrepresentationofinputsandaccommodating
Therenderertransposesoneormoresegmentsof
sequencesupto1024tokens.
textontoavirginRGBcanvasstructuredinto1024
The model’s attention architecture utilizes
distinctpatches,eachdelineatedintoa16x16pixel
groupedqueryattention(Ainslieetal.,2023)with
matrix. ThisconfigurationisshowninTable6.
16attentionheadsand8key-valueheads. Weusea
A visual syntax is adopted to distinguish text
stackof24transformerlayers,endowingthemodel
boundaries: asolitaryblackpatchof16x16pixels
withsubstantialdepthforcomplexpatternrecog-
operatesasbothadelimiterandanindicatorofthe
nition. Also,weuseRMSNorm(ZhangandSen-
sequence’s conclusion (End of Sequence, EOS).
nrich,2019)withepsilonof1e-05androtaryem-
Subsequent white patches post-EOS are deemed
beddings(Suetal.,2024).
padding—theyremaininertintheattentionmech-
anism,thusexcludingthemfromthecomputation
C Pre-trainingData
ofattentionscores.
Fortherenditionoftextdocuments,therenderer For the text-based pre-training, we utilized the
tackles content on a line-by-line basis. It incor- expansive Dolma dataset (Soldaini et al., 2024),
porates a binary search algorithm to intelligently whichcomprisesanextensivecollectionof3tril-
gaugethemaximumquotaofwordsrenderablein lion tokens. This dataset is sourced from a het-
a single pass, ensuring the text’s width remains erogenouscompilationofmaterials,includingan
within the permissible pixel threshold. This dy- arrayofweb-basedcontent,scholarlyarticles,pro-
namicsegmentationcapabilitycircumventspoten- gramming code, literary works, and comprehen-
tial truncation issues inherent in rendering exten- sive encyclopedic entries. For the image-based
sivelinesoftext,allowingforaseamlessintegra- pre-training, we transformed the textual content
tionoflongerpassageswithoutcompromisetovi- fromthepeS2ocorpus,EnglishWikipedia,andthe
sualfidelityorcontextualintegrity. C4datasetintovisualrepresentations,amounting
toatotalofover400milliondocumentimages.
Parameter Value
Background Color White C.1 Pre-trainingDataforVisualImages
DPI 120
WepretrainedonarenderedversionofthepeS2o,
Font Color black
English Wikipedia and C4.The peS2o dataset, a
Font type GoNotoCurrent curatedcollectionofapproximately40millioncre-
Font size 8 ativeopen-accessacademicpapers,hasbeenmetic-
Max sequence length 1024 ulouslycleaned,filtered,andformattedtofacilitate
Padding size 3 the pretraining of language models. Meanwhile,
Pixels per patch 16x16 TheC4datasetrepresentsasubstantialrefinement
oftheCommonCrawlcorpus. Thisdataset,derived
Table6: Configurationoftextrendering.
from the extensive Common Crawl web scrape,
undergoesrigorouscleaningandpreprocessingto
ensure the quality and relevance of the text data.
B ModelArchitecture
TheC4datasetisexclusivelycomposedofEnglish
Table8specifiesthecomprehensiveconfiguration languagetexts,withastringentcriterionthateach
ofourmodel’sarchitecture,basedonsimilartrans- pagemusthaveatleasta99%probabilityofbeing
formerdecoderarchitecturetoLlama2(Touvron in English, as determined by the langdetect tool,
etal.,2023b)withspecificadaptations. Weemploy tobeincluded. Thisselectionprocessensuresthat
SwiGLUasthehiddenactivationfunction(Shazeer, thedatasetprimarilycontainsnaturallanguagetext,
2020;Chaietal.,2020),notedforitseffectivenon- freefromboilerplateornonsensicalcontent,andis
linearprocessingcapabilities. Theinitializerrange extensivelydeduplicatedtoavoidredundancy.
is set to 0.02 to promote optimal weight initial-
C.2 Pre-trainingDataforText
ization. Anintermediatesizeof2816isspecified,
offeringabalancebetweenthemodel’srepresenta- CommenCrawl CommonCrawlisacomprehen-
tional capacity and computational demands. The sivewebcorpusthatcollectsdatafromavarietyof
hiddensizeandthemaximumnumberofposition webpages. ThisdatasetusestheURLofeachwebFigure 8: Illustration of patchifying rendered visual images into a sequence of patches, with a black patch as
end-of-sequencemarker.
Source Type Gzip files (GB) Documents (M) Tokens (B)
CommonCrawl web 4,197 4,600 2,415
C4 web 302 364 175
peS2o academic 150 38.8 57
The Stack code 319 236 430
Project Gutenberg books 6.6 0.052 4.8
Wikipedia encyclopedic 5.8 6.1 3.6
Total 4980.4 5,245 3,084
Table7: Statisticsofpre-trainingcorpus.
Parameter Value specifically extracted from a shard dated April
2019. It includes URLs as metadata, which can
hidden activation SwiGLU
beusedtorestoretheoriginalHTMLfilesandun-
initializer_range 0.02
derstanddocumentlinkages. Thedatasetcontains
intermediate_size 2816
364milliondocuments,totaling175billiontokens,
hidden_size 1024
andisavailableontheHuggingFaceHubunderthe
max_position_embeddings 1024
num_attention_heads 16 ODC-By1.0license,allowingforbroadacademic
num_hidden_layers 24 andresearchusage.
num_key_value_heads 8
rms_norm_eps 1e-05
peS2o(SoldainiandLo,2023) Derivedfromthe
rope_scaling null
SemanticScholarOpenResearchCorpus(S2ORC),
rope_theta 10000
peS2o uses the Semantic Scholar Corpus ID to
tie_word_embeddings false
linkdocumentstotheircorrespondingmanuscripts,
vocab_size 32,000
enabling the recovery of original PDFs through
associatedmetadata. Thedatasetencompasses38.8
Table8: Modelconfigurationparameters.
million documents and 57 billion tokens, and is
accessible through the Semantic Scholar Public
pageasitsidentifier,facilitatingtheexplorationof APIundertheODC-By1.0license.
relationships between different documents. Cov-
ering data from May 2020 to June 2023 across TheStack(Kocetkovetal.,2022) Thisdataset
24 shards, Common Crawl includes about 4,600 comprisesavarietyofcomputercodesourcedfrom
milliondocumentsand2,415billiontokens. Itis different GitHub repositories, with metadata that
hostedonAmazonS3aspartoftheAmazonWeb includes filenames and repository names to facil-
Services’OpenDataSponsorshipprogramandcan itate the retrieval of original content. The Stack
beaccessedfreely,adheringtotheCommonCrawl contains 236 million documents and 430 billion
termsofuse. tokensandishostedontheHuggingFaceHub. It
featurescodereleasedundervariouspermissiveli-
C4 (Raffel et al., 2020) The C4 dataset is a censes, supportingdiversesoftwaredevelopment
cleanedand annotatedsubset ofCommon Crawl, andresearchprojects.Project Gutenberg Project Gutenberg offers a image data, and their conjunction in image-text
collectionofpublicdomainbooksintheU.S.,with pairs,underscoringthecomprehensivenatureofits
eachdocumentbeginningwiththebook’stitleto pre-trainingregimen.
ease identification. This dataset provides access
toabout52,000documentsand4.8billiontokens, Text data Image data Image-text pair
and is freely available at gutenberg.org without
TextGPT ✓ ✗ ✗
any copyright restrictions, making it a valuable PixelGPT ✗ ✓ ✗
resourceforliteraryandhistoricalresearch. MonoGPT ✓ ✓ ✗
DualGPT ✓ ✓ ✓
WikipediaandWikibooks Thesedatasetscon-
sist of encyclopedic content from Wikipedia and Table10: Breakdownsofpre-trainingtasksforvarious
educational materials from Wikibooks, featuring modelconfigurations.
metadatathatincludesURLsfromwhichcontentis
extracted. Thisallowsuserstoreconstructthestruc-
tureandconnectionsbetweendocuments. Together, E Fine-tuningDetails
theycontain6.1milliondocumentsand3.6billion
tokens. ThedataisfreelyavailableviaWikimedia In this section, we present the details of the fine-
data dumps and is released under the CC BY-SA tuning experiments, including (1) the dataset for
4.0license,promotingwidespreadeducationaland theexperiments,(2)thefine-tuningsettingofthe
informationaluse. differentpre-trainedmodels(includingPixelGPT,
MonoGPT,DualGPTandTextGPT),and(3)howthe
D Pre-trainingDetails
differentrenderingmodeswereimplemented.
We list the pre-training hyperparameters in Ta-
ble9. Pre-trainingwasexecutedacrossasuiteof32 E.1 Fine-tuningDataset
NVIDIAA100GPUs. ForTextGPTandPixelGPT,
The main experiments of our fine-tuning phase
weadoptedaglobalbatchsizeof4milliontokens
were conducted on GLUE and XNLI to evaluate
orpatches,respectively. InthecaseofMonoGPT,the
themodel’slanguageandmultilingualunderstand-
globalbatchsizewassetat8million,maintaining
ingability,respectively. HatemojiBuildwasused
anequaldistributionbetweentextandimagedata.
toanalyzetheeffectofcolorretention. Thedetails
ForDualGPT,theglobalbatchsizewasincreased
ofthedatasetaredescribedbelow:
to10million,witharatiooftext/image/pairdata
with4:4:2.
GLUE(Wangetal.,2018) Abenchmarkofnine
sentence- or sentence-pair language understand-
Hyper-parameter Value
ing tasks, including MNLI(392k), QQP(363k),
patch size P 16
QNLI(108k), SST-2(67k), CoLA(8.5k), STS-
maximum learning rate 5e-4
B(5.7k), MRPC(3.5k), RTE(2.5k), WNLI(635),
max seq length 1024
builtonestablishedexistingdatasetsandselectedto
learning rate scheduler linear
coverasetofthreetasks. Inthispaper,forMNLI,
warmup steps 200
QNLI,SST-2,RTE,andWNLItasks,wereportthe
mixed precision bfloat16
Accuracy (Acc); for QQP and MRPC, we report
optimizer AdamW
(β ,β ) (0.9, 0.999) the F1 score; for CoLA, we report the Matthews
1 2
correlationcoefficient(MCC);forSTS-Bwereport
Table9: Hyperparametersofpre-trainingsettings. Spearmancorrelation(Spear.). TheMNLIdataset
hasmatcheddevelopment/testsetswiththesame
For clarification, we summarize the training sourcesasthoseinthetrainingset,andunmatched
tasks in Table 10 for various training configura- setsthatdonotcloselyresembleanyofthesetswe
tions. TextGPT was trained exclusively on text sawduringtrainingaredenotedasMNLI-m/mm.
data. Incontrast,PixelGPTwaspre-trainedsolely Weconductexperimentsonbothsettings. Inaddi-
withimagedata. MonoGPTrepresentsahybridap- tion,somepreviousworksignoredWNLIbecause
proach,utilizingbothtextandimagedataindepen- of its different training and validation/testing set
dently but not in paired form. DualGPT stands as distribution. We still performed on it and found
themostintegrativemodel,incorporatingtextdata, thatPixelpre-trainingleadstoaboostatWNLI.XNLI (Conneau et al., 2018) The Cross- Fine-Tuning Hyperparameters Value
lingual Natural Language Inference (XNLI) cor- Optimizer AdamW
Adam’s betas (0.9, 0.999)
pus is an extension of the Multi-Genre NLI Adam’s epsilon 1e-8
Weight decay 0
(MultiNLI)(Williamsetal.,2018)corpus,designed Learning rate {1e-5, 3e-5, 5e-5, 1e-4}
forcross-lingualnaturallanguageinference,con- Learning rate schedule {Cosine Annealing, Linear Decay}
Warmup steps {10, 100}
tainingdatain15languages. Thedatasetwascre- Batch size {32, 64, 128, 256, 512}
Max sequence length {256, 768}
ated by manually translating the validation and Training steps {250, 500, 2000, 8000, 15000,
30000}
test sets of MultiNLI into each of these 15 lan-
Dropout Probability {0.1, 0}
guages. For all languages, the English training Early Stopping True
Seed 42
setwasmachine-translated. Thetaskistopredict
textual entailment, a classification task determin- Table11: Fine-tuninghyperparametersforgridsearch.
ingwhethersentenceAimplies,contradicts,oris
neutraltosentenceB,giventwosentences.
terresultsonsomedatasets. Foradetailedanalysis,
HatemojiBuild (Kirk et al., 2022) Hatemo- see§4.3.
jiBuild is a benchmark for online hate detection
E.3 ImplementationforDifferentRender
involvingemojis. Thedatasetincludes5,912chal-
Modes
lengingexamplesofadversarialperturbationsgen-
WeuseRGBrendermodeforfine-tuningdataren-
eratedthroughahuman-and-model-in-the-loopap-
deringbydefault,asdescribedinAppendixA.To
proach on Dynabench. This allows us to predict
obtainandadapttograyscaleandbinaryrendered
hatefulemotionsexpressedwithemojis.
data, we modify (1) the data preprocessing pro-
E.2 Fine-tuningSetting cess and (2) the model’s linear projection in the
patch embedding layer. Specifically, we firstren-
We fine-tune PixelGPT, MonoGPT, DualGPT and
der the data uniformly using RGB mode and get
TextGPT on downstream tasks. we use NVIDIA
three-channelRGBimages. Afterthat,inthepre-
Tesla V100 GPUs to fine-tune TextGPT and the
processing stage, to get the grayscale version of
NVIDIAA100GPUstofine-tunepixel-basedpre-
the rendered image, we converted the RGB im-
training models. The same rendering settings as
age to grayscale (with pixel values ranging from
in pre-training are used to render pixel data for
0to255)usingtheconvertfunctionoftheImage
fine-tuningPixelGPT,MonoGPT,andDualGPT,un-
class in the PIL library and setting the function
lessspecified. Weusethelastpatchtopredictthe
parametermodelto’L’togettherenderedbinary
labelwhenfine-tuningthegenerativepixel-based
image, we set the pixel threshold (set to 128 in
pre-trainingmodels. Inouranalysisexperiments,
ourexperiments)basedontheconvertedgrayscale
MonoGPTandDualGPTarealsofine-tunedondual-
image and set the pixels below the threshold in
modalitydataobtainedbyconcatenatingrendered
thegrayscaleimageto0andthepixelsabovethe
images with the original text. Specifically, we
threshold to 255. This way, we transformed the
right-filltheimagewithwhitepaddingblocksfor
three-channelRGB-renderedimageintoasingle-
alignment. Toavoidtheimpactofpaddingpatches
channel grayscale and binary image. Next, since
between the image and the text, we then set the
thepatchembeedinglayerofthepre-trainedmodel
attentionmasktomaskthepaddingblocksduring
takesthethree-channelimageasinputbydefault,
fine-tuning.
weneedtomodifythelinearprojectionlayerinit
We searched fine-tuning hyperparameters for
to adapt to the single-channel image. Therefore,
each dataset in GLUE and two XNLI settings
weaveragethelinearlayerweightsbychanneland
forPixelGPT,MonoGPT,DualGPTandTextGPT,re-
use them as initial weights before fine-tuning so
spectively. Table11showsthesearchedhyperpa-
that the model supports the processing of single-
rametersandvalues. Wepresentthebestsearched
channelimages.
resultsforGLUEinTable12andTable13andfor
translate-train-allandcross-lingualtransfersettings F Baselines
onXNLIinTable14. Duringthehyperparameter
F.1 Text-basedBaselines
searching,wefoundthatusingalargerbatchsize
tofine-tunethegenerativepixel-basedpre-training GPT-2 GPT-2 (Radford et al., 2019) is an ex-
modelimprovestrainingstabilityandachievesbet- tension of the original GPT model, substantiallyHyperparameters MNLI-m/mm QQP QNLI SST-2 CoLA STS-B MRPC RTE WNLI
Max Sequence Length 768
Batch Size 64 64 64 64 32 64 32 64 32
Learning Rate 3e-5 3e-5 5e-5 3e-5 1e-5 5e-5 5e-5 1e-5 3e-5
Learning Rate Schedule Linear Decay
Warmup steps 100 100 100 100 10 10 10 10 10
Dropout Probability 0.0
Table12: Settingsforfine-tuningTextGPTonGLUE.
Hyperparameters MNLI-m/mm QQP QNLI SST-2 CoLA STS-B MRPC RTE WNLI
Max Sequence Length 768
Batch Size 64 512 64 64 512 512 32 32 32
Learning Rate 5e-5 1e-4 5e-5 5e-5 5e-6 3e-5 5e-5 3e-5 3e-5
Learning Rate Schedule Linear Cosine Linear Cosine Cosine Cosine Linear Linear Linear
Decay Annealing Decay Annealing Annealing Annealing Decay Decay Decay
Warmup steps 100 100 100 100 10 10 10 10 10
Dropout Probability 0.0 0.1 0.0 0.1 0.1 0.1 0.0 0.0 0.0
Max Training Steps 15000 1500 8000 8000 2000 2000 2000 2000 250
Table13: Settingsforfine-tuningPixelGPTontheGLUEbenchmark.
Hyperpameters TextGPT PixelGPT MonoGPT(pixel) MonoGPT(text) MonoGPT(pair) DualGPT(pixel) DualGPT(text) DualGPT(pair)
Fine-tune model on all training sets (Translate-Train-All)
Max Sequence Length 768 256 256 256 256 256 256 256
Batch Size 64 512 512 64 256 512 64 512
Learning Rate 5e-5 1e-4 1e-4 5e-5 5e-5 1e-4 5e-5 5e-5
Max Training Steps 15000 30000 30000 15000 30000 30000 15000 30000
Learning Rate Schedule Linear Decay
Warmup steps 100
Dropout Probability 0
Fine-tune model on English training set (Cross-lingual Transfer)
Max Sequence Length 768 256 256 768 256 256 768 256
Batch Size 64 256 256 64 256 512 64 512
Learning Rate 5e-5 1e-4 5e-5 5e-5 5e-5 1e-4 5e-5 3e-5
Max Training Steps 15000 15000 30000 15000 30000 15000 15000 30000
Learning Rate Schedule Linear Decay
Warmup steps 100
Dropout Probability 0
Table14: Fine-tuningsettingsforXNLI.WereportthebesthyperparametersforallmodelsonTranslate-Train-All
andCross-lingualTransfer,respectively.
increasestheparametercountto1.5billion,which or right-to-left), BERT processes words simulta-
enhancesitsabilitytogeneratemorecoherentand neously in both directions. This bi-directionality
contextually relevant text across a wide array of allows the model to capture a richer understand-
domains without task-specific training. With a ing of context. Pre-trained on a large corpus of
transformer-basedarchitecture,GPT-2operateson unlabeledtext,BERTisfine-tunedwithadditional
unsupervised learning, using only a large corpus outputlayerstoperformawidearrayoflanguage
of text data scraped from the internet (WebText) processingtasks.
tolearnvariouslanguagepatternsandtasks. This
modelexemplifiesasignificantshifttowardsmore F.2 Image-basedBaselines
robust and generalized language models, thereby
DONUT ThisOCR-freevisualdocumentunder-
supportingthedevelopmentofAIsystemscapable
standingmodel(Kimetal.,2022)isfundamentally
of understanding and generating human-like text
designed to interpret and extract structured infor-
withminimaltask-specificdata.
mation directly from document images, bypass-
BERT BERT(BidirectionalEncoderRepresen- ingtraditionalopticalcharacterrecognition(OCR)
tations from Transformers) is a groundbreaking techniques. DONUT leverages a transformer ar-
model in natural language processing introduced chitecturetoencodedocumentimagesintoembed-
by Devlin et al. (2019) at Google AI Language. dingsanddecodetheseembeddingsintostructured
ItutilizesthebidirectionalTransformer, anatten- outputslikeJSONformatswithoutpreliminarytext
tionmechanismthatlearnscontextualrelationsbe- detection and recognition stages. Pre-trained us-
tweenwordsinatext. Unlikepreviousmodelsthat ingacombinationofrealandsyntheticallygener-
onlyconsidertextinasingledirection(left-to-right ateddocumentimages,DONUTachievesimpres-sivebenchmarksonseveralvisualdocumentunder- G DetailedResults&Analysis
standingtasks,outperformingstate-of-the-artOCR-
G.1 PerformanceonCross-lingualTransfer
dependent models in terms of both accuracy and
processingspeed. Asyntheticdatageneratorfur- Inthissection,Weanalyzethecross-lingualtrans-
therenhancesThemodel’spre-training,enabling ferabilityofpixel-basedautoregressivemodelson
ittoreadilyadapttodifferentlanguagesanddoc- XNLIundertheCross-lingualTransfersetting. As
umentformats,therebyextendingitsapplicability shown in Table 15, we compared three different
toglobalanddiverseapplicationscenarios. models: PixelGPT, MonoGPT, and DualGPT. Our
findingsindicatethatincorporatingadditionaltext
CLIPPO CLIPPO(Tschannenetal.,2023)inte-
modality data in the pre-training phase enhances
gratesasinglevisiontransformerthatprocessesall
thecross-lingualtransfercapabilitiesofthesemod-
inputtypes—imagesandtext—equally,usingthe
els. Nevertheless,anotableperformancedisparity
samemodelparameters. Byadoptingacontrastive
remains when benchmarked against the multilin-
learning framework, this unified model learns to
gual prowess of the XLM-R base, a model pre-
align the representations of text and images into
trainedextensivelyacross100languages.
acohesivelatentspace. Thisapproachsimplifies
thearchitecturebyremovingthenecessityforsepa-
G.2 ProbingDual-ModalityFine-Tuning
ratetextandimagetowersandenhancesefficiency
byhalvingtheparametercountcomparedtodual- We delved into the synergistic potential between
tower systems. The key innovation of CLIPPO text and pixel modalities during the fine-tuning
lies in its ability to perform complex multimodal phase. Acomparativeexperimentaldesignwasim-
tasks,includingzero-shotclassificationandnatural plementedtofine-tunepixelpre-trainedmodelsin
languageunderstanding,withcompetitiveperfor- twodistinctmanners: (1)exclusivelyontextdata,
mancewhilerelyingsolelyonpixeldata. and(2)onanamalgamationofrenderedimagedata
andoriginaltext. Weassessedtheperformanceim-
PIXEL The PIXEL (Rust et al., 2023) (Pixel-
pactofthesefine-tuningapproacheswithMonoGPT
based Encoder of Language) model reimagines
and DualGPT models on XNLI. As delineated in
language modeling by rendering text as images,
Table16,themodelsfine-tunedwithdual-modality
effectivelybypassingthevocabularybottleneckof
dataconsistentlyoutperformedthosefine-tunedon
languagemodels. Thispre-trainedmodelconverts
textdataalone,withcleargainsinmultilingualun-
textintofixed-sizedimagepatches,whicharethen
derstandingtasks. Thisevidencesuggeststhatthe
processedbyaVisionTransformer(ViT)encoder.
inherent strengths of pixel-based representations
Unlikeconventionalmodelsthatpredictadistribu-
in capturing multilingual nuances are amplified
tionoveravocabularyoftokens,PIXELfocuseson
when combined with textual information during
reconstructingthepixelsofmaskedimagepatches.
fine-tuning.
ThisapproachallowsPIXELtosupportmanylan-
guagesandscripts,leveragingorthographicsimilar- G.3 RGBvs. Grayscalevs. BinaryRendering
ities. Themodelperformsbetterinhandlingscripts
Renderingmodesoffertrade-offsbetweentherich-
notpresentinitstrainingdataandisrobustagainst
nessofinformationandprocessingefficiency,with
orthographicattacksandlinguisticcode-switching.
RGBprovidingathree-channelimagedensewith
PIXAR PIXAR(Taietal.,2024)isapixel-based information,whereasgrayscaleandbinarymodes
pre-trained autoregressive language model on bi- are optimized for speed. To assess the impact of
narytextimages. PIXARoperatesdirectlyonpixel theserenderingchoices,weexploredtherobustness
representationsoftext,enablingittoperformfree- of our model, pre-trained using RGB visual text,
form generative tasks without the constraints of acrossdifferentrenderingmodeswithinthedown-
a predefined vocabulary. The model architecture streamcontextoftheXNLItask. AsshowninFig-
employsadecoder-onlysetup,similartoGPT-like ure9,ourexperimentsrevealthattheperformance
modelsbutadaptedtohandlepixelinput. PIXAR when fine-tuning in grayscale and binary modes
representsasignificantadvancementinthefieldby closely parallels that of RGB. This equivalence
enablingtextgenerationinimagesandleveraging underscorestherobustnessofthepixel-basedpre-
a novel adversarial pre-training stage to enhance training,indicatingthatitscross-linguistictransfer
theclarityandaccuracyofgeneratedtext. capabilitytranscendsthespecificrenderingmodeInputModality
Model #lg #Param ENG ARA BUL DEU ELL FRA HIN RUS SPA SWA THA TUR URD VIE ZHO Avg.
Text Pixel
Fine-tunemodelonEnglishtrainingset(Cross-lingualTransfer)
XLM-Rbase 100 270M ✓ ✗ 85.8 73.8 79.6 78.7 77.5 79.7 72.4 78.1 80.7 66.5 74.6 74.2 68.3 76.2 76.7 76.2
PixelGPT(pixelonly) 1 ✗ ✓ 75.1 35.1 36.9 37.3 37.0 42.2 35.6 34.9 43.1 37.4 35.9 38.1 33.8 38.4 35.5 39.8
MonoGPT(text+pixel) 1 317M ✗ ✓ 67.1 34.6 40.6 41.7 44.2 47.5 36.4 40.8 51.4 41.7 37.0 41.1 34.4 38.8 34.1 42.1
DualGPT(text+pixel+pair) 1 ✗ ✓ 71.0 36.9 40.3 39.7 39.6 47.2 36.3 38.9 48.2 38.7 38.0 40.1 37.0 41.3 36.8 42.0
Table15: Comparisonofpixel-basedpre-trainingmodelsonXNLIdatasetinCross-lingualTransfersetting.
Input Modality
Model ENG ARA BUL DEU ELL FRA HIN RUS SPA SWA THA TUR URD VIE ZHO Avg.
Text Pixel
Fine-tune model on all training sets (Translate-train-all)
MonoGPT (text+pixel) ✓ ✗ 74.0 60.9 62.7 63.4 63.4 64.2 58.2 59.9 64.3 58.6 59.3 61.0 55.0 63.6 61.3 62.0
✓ ✓ 75.4 61.9 65.0 65.2 66.8 66.7 59.3 63.3 67.7 61.1 59.9 63.6 54.9 66.2 62.9 64.0
DualGPT (text+pixel+pair) ✓ ✗ 72.7 61.6 63.8 64.7 63.9 65.1 58.8 61.6 65.4 59.0 59.8 62.2 55.8 63.4 62.1 62.7
✓ ✓ 75.8 64.4 66.5 66.3 67.7 68.0 61.4 65.1 69.0 61.1 60.4 64.4 57.5 67.7 64.0 65.3
Fine-tune model on English training set (Cross-lingual Transfer)
MonoGPT (text+pixel) ✓ ✗ 79.9 34.4 35.3 37.6 34.3 38.9 34.4 35.4 44.4 39.3 34.2 39.2 33.3 35.0 37.4 39.5
✓ ✓ 77.5 35.6 37.7 40.4 37.0 43.7 34.9 38.1 46.6 41.0 35.0 41.0 33.8 37.1 37.4 41.1
DualGPT (text+pixel+pair) ✓ ✗ 79.1 35.5 36.0 40.8 35.1 41.3 35.4 36.6 44.6 38.2 35.2 38.2 34.6 36.4 37.4 40.3
✓ ✓ 75.2 38.5 36.0 42.3 36.9 40.3 34.9 36.9 45.4 39.2 34.8 42.8 36.3 37.8 35.8 40.9
Table16:Comparisonofusingdual-modalitiyandtext-onlymodalityforfine-tuningonXNLI.Addingpixeldatafor
fine-tuningbooststhemodel’smultilingualabilityinthesettingsofTranslate-Train-AllandCross-lingualTransfer.
Render Mode ENG ARA BUL DEU ELL FRA HIN RUS SPA SWA THA TUR URD VIE ZHO Avg.
Fine-tune model on all training sets (Translate-train-all)
RGB 77.7 55.4 66.7 69.0 67.4 71.2 59.1 65.6 71.4 61.7 47.0 65.2 54.4 66.1 50.5 63.2
Binary 78.2 55.8 67.0 68.4 66.8 70.6 58.1 63.9 70.7 61.7 47.5 64.1 53.3 65.9 52.9 63.0
Grayscale 77.0 55.0 65.2 67.6 66.3 69.8 57.1 62.4 70.8 61.2 46.3 63.9 52.1 63.7 51.9 62.0
Fine-tune model on English training set (Cross-lingual Transfer)
RGB 77.3 35.9 38.0 39.7 38.0 44.7 36.3 37.5 46.4 39.6 35.8 40.9 35.3 41.8 35.0 41.5
Binary 76.3 37.8 37.9 37.2 38.9 42.1 37.8 39.0 43.2 37.8 37.9 38.8 36.9 40.7 36.7 41.3
Grayscale 77.3 34.2 37.3 40.7 36.6 46.0 35.6 38.4 46.4 39.6 36.3 41.4 33.7 40.6 34.3 41.2
Table17: Comparisonofusingthreedifferentrendermodestofine-tunePixelGPTonXNLI.RGBrenderingyields
thebestresults.
employed in downstream tasks. Detailed experi- BERT PIXEL PixelGPT (pixel only)
XXNNLLII((aavvgg))
mentalresultsareintheTable17. ZHO ENG
90 VIE 70 ARA
RGB
60
Grayscale
70
Binary
URD BUL
50
30
TUR DEU
10
Translate-train-all Cross-lingual Transfer
THA ELL
Figure9: Performanceofusingthreerendermodesto
fine-tunePixelGPTonXNLI.PixelGPTshowsstrong
robustnesstofine-tuningrendermode SWA FRA
SPA HIN
RUS
G.4 ComparisononXNLIunder
Figure10: ComparisonofourPixelGPTtoPIXELand
Translate-Train-AllSettings
BERTbaselinesinthetranslate-train-allsettings.
We evaluate the efficacy of PixelGPT against the
PIXELandBERTbaselinesacrossfifteendiverse
languages within the XNLI dataset’s Translate-
).gvA(ecnamrofrePTrain-All configuration. The comparative per-
formance, visualized in Figure 10, demonstrates
that PixelGPT outstrips PIXEL in twelve of the
fifteen assessed languages. Notably, PixelGPT
achievesperformanceparitywithBERTinallbut
English and Arabic. Particularly, PixelGPT reg-
isters marked improvements over BERT in Thai
andChineselanguages. Theseresultssuggestthat
thetokenizer-independent,pixel-basedautoregres-
sive design of PixelGPT offers a potent solution
to the vocabulary bottleneck issue commonly en-
counteredinlanguagemodels,thusenhancingits
applicabilitytomultilingualtasks.