How Deep Networks Learn Sparse and Hierarchical Data:
the Sparse Random Hierarchy Model
UmbertoM.Tomasini1 MatthieuWyart1
Abstract fundamentalproblemsofmachinelearning,rangingfrom
theexistenceofscalinglaws(Cortesetal.,1993;Hestness
Understandingwhatmakeshigh-dimensionaldata
etal.,2017;Spigleretal.,2020;Kaplanetal.,2020;Bom-
learnable is a fundamental question in machine
masanietal.,2022)thatrelatethesizeofthetrainingset
learning. On the one hand, it is believed that
totheperformanceandtheemergenceofnewabilities,the
thesuccessofdeeplearningliesinitsabilityto
successoftransferlearning,orthatofunsupervisedmethods
buildahierarchyofrepresentationsthatbecome
suchasdiffusionmodels(Hoetal.,2020).
increasinglymoreabstractwithdepth,goingfrom
simplefeatureslikeedgestomorecomplexcon- Oneviewisthatlearnabledataoftenconsistsoflocalfea-
cepts. On the other hand, learning to be insen- turesthatareassembledhierarchically(Grenander,1996): a
sitivetoinvariancesofthetask,suchassmooth dogismadeofabody,limbs,andaface,itselfconsistingof
transformationsforimagedatasets,hasbeenar- ears,eyes,etc... Thisviewisconsistentwiththeobservation
gued to be important for deep networks and it thataftertraining,deepneuralnetworksformahierarchical
stronglycorrelateswiththeirperformance. Inthis representationoftheinput(Zeiler&Fergus,2014;Doimo
work,weaimtoexplainthiscorrelationandunify etal.,2020). Thesepropertiesalsomirrorthearchitecture
thesetwoviewpoints. Weshowthatbyintroduc- of Convolutional Neural Networks (CNNs) (Lecun et al.,
ingsparsitytogenerativehierarchicalmodelsof 1998;LeCunetal.,2015),whicharedeepanddisplaysmall
data,thetaskacquiresinsensitivitytospatialtrans- filtersizes. Fromatheoreticalviewpoint,thelocalityofthe
formations that are discrete versions of smooth featuresprovestobeadvantageoustoapproximateandlearn
transformations. Inparticular,weintroducethe high-dimensional tasks (Favero et al., 2021; Abbe et al.,
SparseRandomHierarchyModel(SRHM),where 2021;Biettietal.,2022;Xiao,2022;Xiao&Pennington,
weobserveandrationalizethatahierarchicalrep- 2022;Bietti,2022;Meietal.,2022;Cagnettaetal.,2023a).
resentation mirroring the hierarchical model is Moreover,varioushierarchicalmodelsofdata,organizing
learntpreciselywhensuchinsensitivityislearnt, local features within a hierarchical structure, have been
therebyexplainingthestrongcorrelationbetween proposed, (Mossel,2018;Poggioetal.,2017;Malach&
thelatterandperformance. Moreover,wequan- Shalev-Shwartz, 2018; Schmidt-Hieber, 2020; Malach &
tifyhowthesamplecomplexityofCNNslearning Shalev-Shwartz,2020;Cagnettaetal.,2023b;Allen-Zhu
theSRHMdependsonboththesparsityandhier- &Li,2023). Asdetailedinsubsection1.2,deepnetworks
archicalstructureofthetask. canrepresentandlearnsuchmodelsmoreefficientlythan
shallownetworks,bothintermsofnumberofparameters
andnumberoftrainingpoints.
1.Introduction
Intrinsically, hierarchical models have a discrete nature,
whichseemswell-suitedfortexts.Imageshoweverareoften
DeepLearninghasdemonstratedremarkableefficacyacross
approximatedasacontinuousfunctionofatwo-dimensional
diversetasks,fromimageclassification(Voulodimosetal.,
space(Castleman,1996),whoselabelisinvarianttosmooth
2018)tothedevelopmentofchatbots(Brownetal.,2020).
transformations. This stability of image labels to small
Thissuccessisnotwellunderstood: learninggenerictasks
smooth transformations has been proposed in (Bruna &
requiresanumberoftrainingpointsexponentialinthedata
Mallat,2013;Mallat,2016)asakeysimplificationenabling
dimension(Luxburg&Bousquet,2004;Bach,2017),which
imageclassificationinhighdimension. Enforcingsuchsta-
isunachievableinpracticeforimagesortextthatlieinhigh
bility in neural networks can improve their performance
dimension. Learnabledatamustthenbehighlystructured.
(Kayhan & Gemert, 2020). Moreover, the stability can
Understandingthenatureofthisstructureiskeytovarious
alsoimproveduringtrainingbylearningpoolingoperations,
1Institute of Physics, EPFL, Lausanne, Switzerland. Corre- (Dielemanetal.,2016;Azulay&Weiss,2018;Ruderman
spondencetoumberto.tomasini@epfl.ch
1
4202
rpA
61
]LM.tats[
1v72701.4042:viXraHowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
CIFAR10 FullConn FL u2 llConnL6 SRHM
FullConnL4
LeNet
AlexNet
EfficientNetB0 101 101
VGG11
VGG16bn VGG19bn VGG11bn L CC NN N
EfficientNetB0
ResNet18 ResNet34ResNet50 V V ReG G sG G N1 1 e1 6
t18
EfficientNetB2 R Efe fs icN iee nt3 tN4 etB0
(A) (C) 101 100 (D) 101 100
Sensitivity to diffeomorphisms (relative) Sensitivity to diffeo. Sensitivity to synonyms
CIFAR10 100 SRHM 100
101 101
ResNet18
ResNet34
E Leff Nic eie tntNetB0 102 E Vf Gfi Gci 1e 6ntNetB0 102
FullConnL4 LCN
train set sizeP train set sizeP
512 5e4 1e3 5e4
103 103
102 101 100 101 100
(B) (E) (F)
Sensitivity to diffeomorphisms (relative) Sensitivity to diffeo. Sensitivity to synonyms
Figure1.CIFAR10: (A)TesterrorvssensitivitytodiffeomorphismsofcommonarchitecturestrainedonallCIFAR10,showinga
remarkablecorrelationbetweenthetwoquantities. Agreyline,correspondingtoapower-law,guidestheeye. (B)Sameas(A),for
increasingsizeofthetrainingsetP,whosevalueisindicatedbythedegreeofopacity. Thesensitivitytosmoothtransformationsis
computedinrelativetermstothesensitivitytowhitenoise.(A,B)areadaptedfrom(Petrinietal.,2021). TheSRHMcapturesthese
observations:(C)TesterrorvssensitivitytodiffeomorphismsofaCNNtrainedwithP =7400ontheSHRMmodel,withparameters
L = s = s = 2andn = m = 10. Thesensitivitytodiffeomorphismsisdefinedasthechangeofnetworkoutputinducedbya
0 c
diffeomorphismappliedontheinput,seeEq.7.Fordetailsaboutthearchitecturesandtheirtrainingprocess,seeAppendixB.(D)Same
as(C)forsensitivitytoexchangeofsynonyms,definedasthechangeofthenetworkoutputinducedbyanexchangeofsynonyms(defined
inSection2)appliedontheinput,seeEq.6.(E)and(F):astoppanels(C)and(D),forincreasingP (increasingopacity).
et al., 2018; Zhang, 2019; Tomasini et al., 2023). One todiffeomorphismsandtesterror,asdepictedinFig.1
findsthat(i)thereexistsastrongcorrelationbetweenthe (C).
network’stesterroranditssensitivitytodiffeomorphisms,
which characterizes the change of output when a diffeo- • This correlation arises because a hierarchical repre-
morphismisappliedtotheinput (Petrinietal.,2021),as sentation,crucialforachievinggoodperformance,is
recalled in Figure 1 (A) and (ii) the sensitivity to diffeo- learntpreciselyatthesamenumberoftrainingpoints
morphisms decreases with the size of the training set, as atwhichinsensitivitytodiffeomorphismsisachieved.
showninFigure1(B).Currently,theseobservationsremain Weprovideargumentsjustifyingthisobservation.
unexplained,anditisnotclearhowtheyrelatewiththefact
• Wequantifyhowthenumberoftrainingpointsneeded
thatneuralnetworksbuildahierarchicalrepresentationsof
to learn the task, also called sample complexity, of
data.
deepnetworksisinfluencedbyboththesparsityand
hierarchicalnatureofthetask,andhowitdependson
1.1. Ourcontributions
thepresenceofweightsharinginthearchitecture.
• Wearguethatincorporatingsparsityintohierarchical
generativemodelsnaturallyleadstoclassificationtasks
insensitivetotheexactpositionofthelocalfeatures, 1.2.Priorwork
implying insensitivity to discrete versions of diffeo-
Hierarchicalrepresentations: Itisrecognizedthatdeepnet-
morphisms.
workscanrepresenthierarchicallycompositionalfunctions
withsignificantlyfewerparametersthanshallownetworks
• Toillustratethisprocess,weintroducetheSparseRan- (Poggio et al., 2017). Sufficient information exists in a
dom Hierarchy Model (SRHM), which captures the trainingsetwithsizepolynomialindtoreconstructsuch
empirically observed correlation between sensitivity tasks(Schmidt-Hieber,2020)—althoughthetrainingmay
2
rorre
tseT
rorre
tseTHowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
Low-level
High-level
features
(1)Classfeatures
(2)
Class Input
(4)
(3)
(5)
Class Input
Figure2.(1)Onthetopofthatpanel,aninstanceoftheproductionrulesofaRandomHierarchicalModel(RHM)withn =2classes,
c
L=2,m=3synonymsperfeature,vocabularysizev =3,ands=2. HeretheclassessetisC ={green,orange},thehigh-level
featuresvocabularyisV ={red,blue,purple}andthelow-levelfeaturesvocabularyisV ={turquoise,pink,green}.Onthebottom,a
2 1
coupleofexamplesgeneratedviatherulesaboveareshown.Thefirstexampleisgeneratedbytheproductionrulesintheblackboxes
(i.e.thegreenlabelgenerates(red,blue),whichthemselvesgeneratethecouples(turquoise,pink)and(pink,green). (2)Top:effectof
adiffeomorphismτ onadog. Thebluearrowsrepresentthedisplacementfieldinducedbyτ. Bottom: effectofadiffeomorphismτ
onaninstanceofasparsegenerativehierarchicaltask.(3)Differentdefinitionsofsparsity.(A)Eachoneofthesinformativefeatures
isembeddedinasub-patchofsizes +1withstrictlys uninformativeelements,yieldingapatchofs(s +1)elements. (B)Thes
0 0 0
informativefeaturescanoccupyanypositionwithinthepatchofs(s +1)elements. Inbothcases,allthepossiblerearrangements
0
areshownfors = 2ands = 1. Atthenextproductionrule,eachuninformativeelementgeneratesanemptypatchofs(s +1)
0 0
uninformativeelements,aspicturedin(4).(5)Fourdatasampledfromthegenerativehierarchicaltaskshowninpanel(1)withsparsity
(A).Thefirsttwoexamplesfollowtherulesinblackboxesinpanels(1)and(4),showcasingdifferentfeaturerearrangements.
takeanimmensetime. Generativehierarchicalmodelsof classificationofdifferentcombinatorialpropertiesoftasks
data(Mossel,2018;Malach&Shalev-Shwartz,2018;2020) is introduced. The SRHM displays the properties of sys-
canbelearntefficientlybyiterativeclusteringmethods,if tematicity,substitutivityandlocalisminthatclassification
correlationsexistbetweeninputfeaturesandoutput. Deep (Hupkesetal.,2021).
architecturestrainedwithgradientdescentdisplayahierar-
CNNswerecraftedtohaveinternalrepresentationsequiv-
chicalrepresentationofthedata,correspondingtothelatent
arianttocertaintransformationssuchasrotations(Cohen&
variables in these models (Cagnetta et al., 2023b; Allen-
Welling,2014;2016;Ensignetal.,2017;Kondor&Trivedi,
Zhu & Li, 2023). Deep networks, as opposed to shallow
2018;Finzietal.,2021;Batzneretal.,2022;Blum-Smith
ones,beatthecurseofdimensionality,withasamplecom-
&Villar,2023). Howsuchaprocedurecanimprovesample
plexitythatispolynomialinthedimension(Cagnettaetal.,
complexity has been addressed for linear models such as
2023b). However, none of these works consider sparsity
randomfeaturesandkernelmethods(Faveroetal.,2021;
infeaturespace,andthestabilityitconferstodiscretised
Biettietal.,2021;Elesedy&Zaidi,2021;Elesedy,2021;
smoothtransformationsoftheinput.
Mei et al., 2021). Instead, our work focuses on how in-
Taskstructureandinvariance: In(Hupkesetal.,2021)a varianceemergesduringtrainingandhowitaffectssample
3HowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
complexity,inaregimewherefeaturesarelearnt. nevergeneratethesamedata,implyingthatm≤vs−1. An
example of a hierarchical generation process is shown in
Finally,inthecontextofadversarialrobustnesstheresponse
Figure2,panel(1).
oftrainednetworkstosmall-normtransformationsofthe
inputdatathatchangetheinputlabelareinvestigated(Fawzi Note that (i) we represent each input level feature in V
1
&Frossard,2015;Kanbaketal.,2018;Alaifarietal.,2018; with a one-hot encoding of length v, yielding input data
Athalyeetal.,2018;Xiaoetal.,2018;Alcornetal.,2019; withdimensiond×v. Therepresentationofhigher-level
Engstrometal.,2019).Ourapproachdiffersfromthislitera- featuresneednotbespecified,astheyarelatentvariables.
turesinceweconsidertheresponsetogenericperturbations (ii) Although we focused on generating one-dimensional
belongingtospecificensembles,ratherthanworst-caseper- datapatches,thesamemodelcangeneratesquare‘images’
turbations. withoutmodificationsifsisthesquareofanaturalnumber.
TheRHMcanbeusedtogenerateP trainingpointsofinput-
2.Background: hierarchicalgenerativemodels labelpairs(x,y)wherey =1,..,n indicatestheclassand
c
x∈Rd×v istheinputcorrespondingtodsub-featuresµ(1),
We consider hierarchical classification tasks where im-
eachone-hotencodedinadimensionv. Usingsuchdatato
ages are generated from class labels through a hierar-
traindeepnetworkstoclassifyyfromx,onefinds(Cagnetta
chical composition of production rules (Mossel, 2018;
etal.,2023b)that:
Malach&Shalev-Shwartz,2018;DeGiuli,2019;Malach
& Shalev-Shwartz, 2020; Cagnetta et al., 2023b; Allen-
• ThesamplecomplexityP∗ofshallownetworkgrows
Zhu & Li, 2023). These tasks represent a specific case
exponentially in d, whereas for deep networks it is
of context-free grammars, a generative model in formal
polynomialindandreadsP∗ ∝n mL,withaprefac-
language theory (Rozenberg & Salomaa, 1997). We fo- c
toroforderunityforCNNs.
cusonL-levelcontext-freegrammar,consideringasetof
class labels C ≡ {1,...,n c} and L disjoint vocabularies • Thesamplecomplexitycorrespondsalsotothetraining
V ℓ≡ (cid:8) aℓ 1,...,aℓ vℓ(cid:9) of low- and high-level features, with setsizeatwhichahierarchicalneuralrepresentation
ℓ = 1,...,L. Henceforth, werefertoℓ > 1ashigh-level emerges. The latent variables µℓ are encoded closer
featuresorlatentvariables. Uponselectingaclasslabelα to the output as ℓ increases. This representation is
uniformlyatrandomfromC,thedataisgeneratediteratively insensitivetochangeofsynonymsintheinput.
fromasetofproductionrules:
• Synonyms produced by the same high-level feature
α(cid:55)→µ(L),...,µ(L)forα∈C andµ(L) ∈V , (1) µ(2)oftheimagesharethesamecorrelationwiththe
1 sL i L
µ(ℓ) (cid:55)→µ(ℓ−1),...,µ(ℓ−1)forµ(ℓ) ∈V ,µ(ℓ−1) ∈V , (2)
output,afacttrueforanygivenpatchofsizesinthe
1 sℓ ℓ i ℓ−1 input. When a sufficient number of data is present,
forℓ = 2,...,L. Ateachlevelℓ, weconsiderm distinct i.e. P ≥ P∗, these correlations are larger than the
ℓ
rulesstemmingfromeachhigher-levelfeaturea(ℓ). Inother samplingnoisegivenbythefinitessofthetrainingset.
i
words,therearem equivalentlower-levelrepresentations Thesecorrelationscanbeusedtogroupsynonymsto-
ℓ
ofa(ℓ)foralli=1,...,v. Theseequivalentrepresentations gether. Gradientdescentcancapturethesecorrelations,
i constructing a representation invariant to synonyms
aretermed‘synonyms’. InFigure2,panel1,weshowon
exchange. Thesecorrelationsbetweensynonymsand
thetopthegenerationrulesofagenerativehierarchicaltask
outputarenecessaryforthenetworktosolvethetask:
with n = 2 classes with two levels of production rules,
c
ifahierarchicalmodelisdesignedwithoutthem,learn-
eachcounting3synonyms.
ingisessentiallyimpossible(itrequiresanexponen-
To simplify notation, we opt for the case of the Random tialnumberofdatainthedimension)(Cagnettaetal.,
HierarchyModel(RHM)(Cagnettaetal.,2023b),forwhich: 2023b).
(i)∀ℓ,v =v,m =m,s =s,(ii)themproductionrules
ℓ ℓ ℓ
associatedwithanylatentvariableorclass,shownontop
intheexampleofpanel1inFigure2,arerandomlychosen 3.Sparsityandstabilitytodiffeomorphisms
amongthevspossiblesonesand(iii)themproductionrules
Ourkeyinsightisthatspatialsparsityoffeaturesimplies
ofanylatentvariableorclassaresampleduniformlyduring
stabilitytodiffeomorphisms. Thispointisillustratedusing
data generation, as exemplified on the bottom of panel 1
a sketched dog in Figure 2, panel (2): if a few lines can
in Figure 2. Consequently, the total number of possible
d−1 define what is on a drawing, then small changes in the
data produced per class is ms−1, where the dimension d
relativedistancesbetweentheselinesshouldnotalterthe
is defined as d = sL. (iv) Two distinct classes or latent
classlabel.
variables cannot yield the same low-level representation.
Thisconditionensures,forexample,thattwodistinctclasses Thisideacanbereadilyimplementedingenerativemodels
4HowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
that do not affect the task include the motion of the s in-
formativefeaturesµ1 withinpatchesofsize(s +1). For
0
(B),anymotionofthesetofinformativelow-levelfeatures
µ1 thatleavestheirorderunchanged(asdiffeomorphisms
woulddo)doesnotalterthelabel,asillustratedinFigure2,
panel(3).
4.Samplecomplexity
We empirically analyze the number of training points re-
quiredtolearntheSRHMforbothCNNsandforLocally
ConnectedNetworks(LCNs),aversionofCNNswithout
weight sharing (Fukushima, 1975; le Cun, 1989; Favero
et al., 2021). In (Cagnetta et al., 2023b), it is shown that
in the absence of sparsity, the sample complexity mildly
dependsonthearchitecture,aslongasitisdeepenough–
even for fully connected networks. Here, we start by re-
strictingourselvestoarchitecturesthatmatchthegenerative
process. Specifically, theLCNarchitecturehasLhidden
layers with filter size and stride both equal to s(s +1),
0
followed by a linear readout layer, as shown in Figure 3
Figure3.Networks:(a)LocallyConnectedNetwork(LCN).Each
(a). The CNNs we consider are structured as the LCNs,
neuron’sweightfocusesonasingleinputelement(inred). Net-
workshaveLhiddenlayers,withfiltersmatchingpatchesofsize buttheyimplementweightsharing,asdepictedinFigure3
s(s +1)fromthegenerativeprocessinFigure2(hereL = 2, (b). BycomparingthesamplecomplexitiesofLCNsand
0
s=2,s =1).Alastfullyconnectedlayerconnectstheoutput CNNswequantifytheimprovementachievedthroughthe
0
ofthelastlocallayerwiththeoutput. (b)ConvolutionalNeural implementation of weight sharing. For details about the
Network(CNN)withthestructureof(a),featuringweightsharing trainingscheme, werefertoAppendixD. Howcommon
suchthateachweightconsidersdifferentpixelsinallpatchesof architecturessuchasVGG,ResNet,andEfficientNetlearn
sizes(s +1)(inred).
0 theSRHMisinvestigatedinAppendixB.
WeutilizetheSRHMtogenerateP trainingpoints,corre-
spondingtoinput-labelpairs(x,y)wherey =1,..,n and
byaddingan‘uninformative’feature0toeachvocabulary c
x∈Rd×v. Anexampleofalearningcurverepresentingthe
V andimposing,forexample,theconstraintthateachpro-
ℓ
testerrorϵdependenceontrainingsetsizeP isshownin
duction rule in Eq. 1 and Eq. 2 contains exactly s×s
0
Figure4(A).ThetesterrorremainslargewithincreasingP
uninformativefeatures. Weimplementsparsityintwoways,
untilitdecaysrapidlytonear-zerovaluesatacertainscale
asshowninFigure2,panel(3). (A)Eachofthesinforma-
P∗,correspondingtothesamplecomplexity. Toestimate
tivefeaturesisembeddedinasub-patchofsize(s +1)with
0
it, we fix a threshold (e.g. 0.1) and measure the minimal
strictlys emptyelements.Thepositionofeachinformative
0 numberoftrainingpointsP∗atwhichthetesterrorachieves
feature is independent of the other feature positions. (B)
suchathreshold. Modifyingthethresholdvaluedoesnot
Thesinformativefeaturescanoccupyanypositionwithin
qualitativelyalterourresultsbelow. WefocusonsparsityA,
thepatchofs(s +1)elements,aslongastheirorderre-
0
depictedinFigure2panel3,asitiseasiertoanalyzethan
mainsthesame. Forboth(A)and(B),ateachlevelofthe
thesparsityB.WeshowinAppendixAthattheyleadtothe
hierarchy,eachuninformativefeaturewillproduceapatch
samesamplecomplexity.
of s(s +1) uninformative features at the next level, as
0
depictedinFigure2,panel(4). WedenotemodelAasthe Wesystematicallyapplythisproceduretoextractthesample
SparseRandomHierarchicalModel(SRHM). complexityP∗astheparameterss,s ,vandLarechanged
0
whilekeepingthenumberofinformativesynonymsm =
Notethat(i)theseprocessesgenerateverysparsedata,with
justsLinformativefeaturesrandomlyplacedininputsofdi-
vs−1toitsmaximalvalue.
mensiond=(s(s 0+1))L,asshownforafewexamplesin FortheLCNs, theresultsshowninFigure4(B)andFig-
Figure2,panel(5).Theinformativefeaturesarerepresented ure10inAppendixDindicatethat:
withone-hotencodingwithdimensionv,whileuninforma-
tivepixelsarerepresentedbyemptycolumns,yieldinginput P∗ ∼C (s,L)(s +1)Ln mL. (3)
LCN 0 0 c
withsized×v. (ii)Sparsityimpliesthatsometransforma-
tionleavesthetaskinvariant. For(A),thetransformations where our observations for C (s,L) are consistent with
0
5HowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
LCN CNN
v=10,s=2,s0=2
106
s0=0 s0=0
0.8 s0=1 s0=1
s0=2 105 s0=2
105 s0=4 s0=4
0.6 y x s0=6
y x
104
104
0.4
P*
103
0.2 103 v, L=2 v, L=2
4 10 3 12
v, L=3 v, L=3
0.0
102
4 10 102 3 12
(A) 102 10 P3 104 (B)102 (s10 L/3
2)(s
01 +04
1)Ln
c1 m05
L
106 (C) 102
(s
010 +3
1)2n
cm104
L
105
Figure4.(A)Testerrorε(P)versusnumberoftrainingpointsP. ToextractthesamplecomplexityP∗,wefixanarbitrarythreshold
ε∗ =ε(P∗).Hereε∗ =10%.(B)EmpiricalsamplecomplexityP∗foraLCNtoreacha10%testerrorεversusestimationofEq.3for
s=2,differentdepthsL(redforL=2,blueforL=3),differentvocabularysizesv(differentdarkness),numberofclassesn =v,
c
maximalm=vs−1anddifferents (differentmarkers).(C)Sameas(B)forCNNs,supportingEq.4.FurthersupportforEq.3andEq.
0
4isobtainedbyvaryings,asshowninAppendixD,Figure10andAppendixE,Figure13.
C (s,L) ∼ sL/2. We will motivate the dependence of takes its maximum value m = vs−1. This indicates that
0
P∗ withs insection6. neuralnetworkscanadapttothesparsityofthetask.
LCN 0
For CNNs, we observe in Figure 4 (C) and Figure 13 in
AppendixEthatthesamplecomplexityfollows:
m=vs 1
124
P C∗
NN
∼C 1(s 0+1)2n cmL. (4) 11 01 86 28
100 26
whereC isaconstant.
1 92
24
84
TherelationsEq.3andEq.4arecentraltothiswork,asthey
76 22
specify how sparsity and combinatorial properties of the 68
20
dataaffectthesamplecomplexity.Remarkably,bothsample 60
52 18
complexitiesareexponentialinLandthuspolynomialin
theinputdimensiond=(s(s 0+1))L,effectivelybeating 34 64 16
the curse of dimensionality. Weight sharing proves to be 28 14
20
beneficial for networks trained on the SRHM, since the 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
sample complexity is only quadratic in L in (s +1) for Relevant Fraction F
0
CNNs,whileitisexponentialinLforLCNs. Wewillargue
whythisisthecaseinsection6. Figure5.SamplecomplexityofLCNlearningtheSRHMforvary-
inginputdimensiondandinputrelevantfractionF atthemaximal
Benefit of Sparsity. We show that for locally connected casem=vs−1,withv=10ands=5,accordingtoEq.5.The
nets,forreasonableassumptionsandforafixeddimensiond,
colormapisinlogscale.Atfixeddimensiond,asmallerF (hence
ahighersparsityispreferableasitleadstoasmallersample highersparsity)makesthetaskeasier.
complexity. Introducingthefractionofinformative‘pixels’
in the image F = (s +1)−L as a measure of sparsity,
0
wecanreformulatethesamplecomplexityP∗ interms
LCN 5.Learninginvariantrepresentation
of the input dimension d and image relevant fraction F,
assumingthatmandsarefixedbutlettingLands 0change. In section 4, we have shown that deep networks trained
Oneobtains: on the SRHM beat the curse of dimensionality, learning
P∗ ∼Fl lo og gm s−1 2dl lo og gm s+ 21 , (5) thetaskwithasamplecomplexitypolynomialintheinput
LCN dimension. Here, we show that they manage to do so by
√
whichisindeedagrowingfunctionofF,aslongasm> s. learning representations that are insensitive to irrelevant
ThisresultisillustratedinFigure5forthecasewherem aspectsofthetask.
6
rorre
tseT
%01=
.t.s
*P
d
noisnemiD
%01=
.t.s
*P
cn/*PHowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
(A) (B)
(C) (D)
106
s0=0
s0=1
s0=2
105 s y0= x4
105
104
104
103
v, L=2
3 12
v, L=3
3 12 103
102
103 104 105 106 103 104 105
P* s.t.S =30% P* s.t.D =10%
S 2 D 2
Figure6. Top.ProceduretocomputethesensitivityS andD ,definedinEq.6andEq.7,illustratedforL=2.Wetakeatwo-layer
2 2
network,trainedwithP trainingpointsonthesparsehierarchicaldatasetwithL=2levels.Weapplyeithera(A)synonymsexchange
pora(B)diffeomorphismτ ontheinputdatax.Notethatin(A)pchangesthefeaturesbutnottheirpositionwhilein(B)τ changes
thepositionofthefeaturesbutnottheirvalue.Thenwetestthesecondnetlayerf sensitivitytothesetransformations. Bottom.(C)
2
EmpiricalsamplecomplexityP∗ toreacha10%testerrorεversusempiricalsamplecomplexityP∗ toreachS = 30%fors = 2,
S 2
differentdepthsL(redforL=2,blueforL=3),differentvocabularysizesv(differentdarkness),numberofclassesn =v,maximal
c
m=vs−1anddifferents (differentmarkers).(D)Sameas(C),forempiricalsamplecomplexityP∗toreacha10%testerrorεversus
0
empiricalsamplecomplexityP∗ toreachD =10%.ThesensitivitythresholdshavebeentunedbasedontheformofS andD versus
D 2 2 2
P,reportedinAppendixD,Figure12forL=2.BothP∗andP∗ arenearlyequaltoP∗.
S D
ThefirstinvarianceoftheSRHMtaskcorrespondstothe thequantityS :
k
fact that different combinations of s informative features
⟨||f (x)−f (p(x))||2⟩
aresynonyms: tosolvethetask,itisnotnecessarytocarry S = k k x,p , (6)
withinthenetworktheinformationofwhichactualsynonym k ⟨||f k(x 1)−f k(x 2))||2⟩ x1,x2
was present in the input. The second invariant considers
wherex,x ,andx belongtoatestset,andtheaverage
1 2
transformationsakintodiffeomorphisms, whichshiftthe
⟨.⟩isoverthetestsetandontherandomexchangeof
position of the informative features µ(1). Formally, we
synonymsp. AvisualizationofS fork =2foratwo-
k
measuresuchinvariantbydefiningtwooperatorsactingon
layernetworkisshowninFigure6(A).Iftheinternal
theinput:
representationf haslearntthesynonymsassociated
k
with production rule Eq. 2 at level ℓ = 2, then S
k
is small. The sensitivity of the network output to p,
showninFigure1(D,F),isdefinedsimilarlyto(6).
1. Synonymicexchangeoperatorp.Thisoperatortakesin
adatumxandsubstituteseachinformatives−patchof 2. Adiscretiseddiffeomorphismoperatorτ. Thisoper-
featuresµ(1),producedbyagivenµ(2),withoneofits atortakesasinputadatumxandrandomlyshiftsits
m−1synonyms,chosenuniformlyatrandom,keeping informativefeaturesaccordingtothepossiblepositions
thefeaturepositionsintact. Figure6(A)illustratesthe obtained when the features µ(2) generate patches of
actionofp. Thesensitivityoftherepresentationata featuresµ1,asillustratedinFigure2,panel(3). Inthis
givenhiddenlayerf totheactionofpismeasuredby process,thenatureofthefeatureremainsthesame. In
k
7
%01=
.t.s
*P
%01=
.t.s
*PHowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
Figure6(B)theactionofτ onadatumxisshown.The quantities evolve with the size of the training set P, as
sensitivityD off todiffeomorphismsτ isdefined documented in Figure 1 (B, E, F). The results shown in
k k
as: Figure 1 (C,D,E,F) for the network output hold also for
D = ⟨||f k(x)−f k(τ(x))||2⟩ x,τ . (7) thesensitivitiesofinternallayers,asshowninFigure17in
k ⟨||f k(x 1)−f k(x 2))||2⟩ x1,x2 AppendixG.
Iff haslearnttheinvariancetodiffeomorphisms,then
k
D kiszero.Thisdefinitionofsensitivityisakintowhat 6.Samplecomplexitiesarguments
isusedforimages(Petrinietal.,2021). Thesensitivity
ofthenetworkoutputf toτ,showninFigure1(C,E), Intheabsenceofsparsitys 0 = 0,thesamplecomplexity
isdefinedanalogouslyto(7). essentially corresponds to the training set size necessary
fordetectingsynonyms(Cagnettaetal.,2023b). Following
It is possible to generalize the sensitivities Eq. 6 and Eq. thisresult,wepresentasimpleheuristicargumentforthe
7 to measure how much the representation at any hidden sample complexity of the LCN architecture in the sparse
layerkofanetwork,oritsoutput,issensitivetoexchange case s 0 > 0. Crucially, this argument also explains why
ofsynonymsordiffeomorphismsrelatedtothefeaturesµ(ℓ) invariancetosynonymsandtosmoothtransformationsare
producedbythelatentµ(ℓ+1). WeshowinAppendixDand learnttogether.
AppendixEthattherepresentationatlayerk ≥2becomes
insensitivetotransformationsappliedtolevelsℓ≤k−1of • Any given 2-level latent variable µ(2) closest to the
thehierarchy,fortrainingsetsizeP ≫ P∗. Atthatpoint, inputcangeneratedifferentsynonyms,distortedwith
theoutputbecomesinsensitivetotransformationsappliedat different diffeomorphisms due to sparsity, as in Fig-
anylevelℓofthehierarchy. ure2,panel(3).
Here, we focus on S 2 and D 2 (corresponding to the case • All these different representations produced by that
k =2andℓ=1). Wemeasurehowthesequantitiesdepend latentvariableµ(2)havethesamecorrelationwiththe
on the size of the training set, as shown in Appendix D, classlabel.
Figure12,andinAppendixE,Figure15,forL=2. Subse-
quently,wedefinesamplecomplexitiesP∗orP∗ associated • Thiscorrelationcanbeusedtogrouptogethertherep-
S D
withS andD ,usingasimilarprocedureasforthetester- resentations produced by µ(2), if the training set is
2 2
ror: wemeasurethetrainingsetsizewherethesequantities largerthansomesamplecomplexity. Wecanestimate
reachsomethresholdvalue. thissamplecomplexityforLCNsasfollows. Asingle
weightconnectedtotheinputwillnotseeanyinforma-
Figure6(C)and(D)presentourkeyresultforLCNs:
tionformostdata,asitwillmostoftenseethe”pixel”
P∗ ≈P∗ , P∗ ≈P∗ , (8) or low-level feature µ(1) = 0, which is uninforma-
S LCN D LCN
tive. Withrespecttothecases = 0,thefractionof
0
furthersupportedbyFigure11inAppendixDforadifferent
datawithlocalinformationisdiminishedbyafactor
valueofs.Inessence,theinsensitivitiestodiffeomorphisms (s +1)−L. Todetectcorrelationsbeyondsampling
0
andsynonymsexchangeareacquiredforthesametraining
noise,thesamplecomplexityhasthustobeincreased
setsize,preciselywhenthenetworklearnsthetask. This byafactor(s +1)L,asempiricallyobservedinEq. 3.
0
observationappearstobeuniversal,asitholdsforcommon
WeshowinAppendixCthatatthissamplecomplex-
convolutionalarchitectureslikeVGG,ResNet,andEfficient-
ity a single step of GD can aggregate the equivalent
NetasdemonstratedinAppendixB,Figure9,forthesimple representationsproducedbyµ(2).
convolutionalarchitecturespicturedinFigure3(b)inAp-
pendixE,Figure14andevenforfully-connectednetworks • Thehiddenrepresentationinthefirsthiddenlayerthus
inAppendixF,Figure16. becomesinsensitivetodiffeomorphismsandsynonyms
atthesamesamplecomplexity,asshowninFigure6(C,
Therefore, deep networks learn to be insensitive to dif-
D).Oncetherepresentationsthatencodeforfeatures
feomorphismsastheylearnahierarchicalrepresentation,
µ(2)havebeendetected,theproblemismuchsimpler,
whichisthoughttobecrucialtoachievehighperformance.
correspondingtoagenerativemodelwithL−1levels
Thiscentralresultrationalizestheobservedcorrelationbe-
insteadofL. Thus,representationsofhigherproduc-
tweensensitivitytodiffeomorphismsofthenetworkoutput
tionrulesµℓ,ℓ>2arealsodetectedandrepresented
(thus indicative of building a hierarchical representation
togetherinthehigherlayersofthenetworkbeyondthat
of the data,) and test error displayed in Figure 1 (A) for
characteristictrainingsetsize. Indeed,synonymsand
CIFAR10and(C)intheSRHM.
insensitivitytodiffeomorphismsarelearntatalllevels
Therelationshipbetweeninsensitivitytosynonymsandto ofthehierarchyinonego,asillustratedinAppendixD,
diffeomorphismsalsoappearswhenconsideringhowthese Figure12.
8HowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
Qualitatively,thesamescenarioholdsforCNNs. Oneex- Impactstatement
pects a different sample complexity since each weight is
Thispaperpresentsworkwhosegoalistoadvancethefield
nowconnectedtoafractionoftheinputthatisindependent
of Machine Learning. There are many potential societal
ofL. Yet,thequadraticdependenceons +1remainsto
0
consequencesofourwork,noneofwhichwefeelmustbe
beunderstood.
specificallyhighlightedhere.
7.Limitations
Acknowledgements
We predicted that good performance, insensitivity to dif-
TheauthorsthankFrancescoCagnetta,AlessandroFavero,
feomorphism, and insensitivity to synonyms exchange
LeonardoPetriniandAntonioSclocchiforfruitfuldiscus-
should occur concomitantly as the training set size is in-
sionsandhelpfulfeedbackonthemanuscript. Thiswork
creased. However to test this prediction on benchmark
was supported by a grant from the Simons Foundation
imagedatasets,wearecurrentlymissinganempiricalpro-
(#454953MatthieuWyart).
ceduretomeasureinsensitivitytosynonyms. Toperform
suchameasurement,oneneedstobeabletochangewhat
composesagivenimageatvariousscales. Therecentresult References
of(Sclocchietal.,2024)suggestthatchangeoflow-level
Abbe, E., Boix-Adsera, E., Brennan, M. S., Bresler, G.,
featurescanbeobtainedusingdiffusion-basedgenerative
andNagaraj,D. Thestaircaseproperty: Howhierarchi-
models, and that the scale where the composition of the
cal structure can guide deep learning. In Ranzato, M.,
imagechangesiscontrolledbythemagnitudeofthenoise
Beygelzimer, A., Dauphin, Y., Liang, P., andVaughan,
thatentersinthesemethods. Itwouldbeinterestingtouse
J.W.(eds.),AdvancesinNeuralInformationProcessing
thisresulttotestourprediction.Anotherapproachwouldbe
Systems, volume 34, pp. 26989–27002. Curran Asso-
toteststabilitytoactualsynonymsintextdata,andquantify
ciates,Inc.,2021.
howthispropertycorrelateswiththeperformanceofnatural
languageprocessingsystems.
Alaifari, R., Alberti, G. S., and Gauksson, T. ADef: an
Iterative Algorithm to Construct Adversarial Deforma-
8.Conclusions tions.September2018.URLhttps://openreview.
net/forum?id=Hk4dFjR5K7.
Understanding the nature of real data is an elusive prob-
lemyetcentraltomanyquantitativequestionsofthefield. Alcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku,
Wehaveunifiedinacommonframeworktwodifferentap- W.-S., and Nguyen, A. Strike (With) a Pose: Neu-
proachestothisproblem: thefirstassumesthedatatobe ral Networks Are Easily Fooled by Strange Poses of
combinatorial and hierarchical, while the second empha- Familiar Objects. In 2019 IEEE/CVF Conference on
sizes the task insensitivity with respect to smooth trans- Computer Vision and Pattern Recognition (CVPR), pp.
formations,relevantforexampleforimagedatasets. This 4840–4849, Long Beach, CA, USA, June 2019. IEEE.
frameworkwasobtainedbyintroducingmodelsthatdisplay ISBN 978-1-72813-293-8. doi: 10.1109/CVPR.2019.
bothproperties,basedonthenotionthatsparsityofinforma- 00498. URL https://ieeexplore.ieee.org/
tivefeaturesinspacenaturallyendowsstabilitytosmooth document/8954212/.
transformations. Thesemodelsexplainthestrongempiri-
calcorrelationbetweenstabilitytosmoothtransformations Allen-Zhu,Z.andLi,Y. Howcandeeplearningperforms
and performance, and further predict that both quantities deep (hierarchical) learning, 2023. URL https://
shouldcorrelatewiththeinsensitivitytodiscretechangesof openreview.net/forum?id=j2ymLjCr-Sj.
low-levelfeaturesinthedata.
Athalye, A., Engstrom, L., Ilyas, A., andKwok, K. Syn-
Finally,althoughwefocusedonclassificationproblems,the thesizingRobustAdversarialExamples. InInternational
generativemodelsweintroduceddisplayarichstructurein ConferenceonMachineLearning,pp.284–293.PMLR,
thedatadistributionP(x)itself,whichisnotonlyhierar- July 2018. URL http://proceedings.mlr.
chical but is invariant to smooth transformations. These press/v80/athalye18b.html. ISSN: 2640-
modelsarethuspromisingtounderstandhowunsupervised 3498.
methodsbeatthecurseofdimensionality,bycomposingor
deformingfeaturestheyhavelearntfromexamples. Azulay,A.andWeiss,Y. Whydodeepconvolutionalnet-
works generalize so poorly to small image transforma-
tions? arXivpreprintarXiv:1805.12177,2018.
Bach, F. Breaking the curse of dimensionality with con-
9HowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
vexneuralnetworks. TheJournalofMachineLearning on Machine Learning, volume 48 of Proceedings of
Research,18(1):629–681,2017. Machine Learning Research, pp. 2990–2999, New
York, New York, USA, 20–22 Jun 2016. PMLR.
Batzner, S., Musaelian, A., Sun, L., Geiger, M., Mailoa,
URLhttps://proceedings.mlr.press/v48/
J. P., Kornbluth, M., Molinari, N., Smidt, T. E., and
cohenc16.html.
Kozinsky,B. E(3)-equivariantgraphneuralnetworksfor
data-efficientandaccurateinteratomicpotentials. Nature Cortes,C.,Jackel,L.D.,Solla,S.,Vapnik,V.,andDenker,
Communications, 13(1):2453, May 2022. ISSN 2041- J. Learningcurves: Asymptoticvaluesandrateofcon-
1723. doi: 10.1038/s41467-022-29939-5. URLhttps: vergence. Advances in neural information processing
//doi.org/10.1038/s41467-022-29939-5. systems,6,1993.
Bietti,A. Approximationandlearningwithdeepconvolu-
DeGiuli, E. Random language model. Physical Review
tionalmodels: akernelperspective,2022.
Letters,122(12):128301,2019.
Bietti,A.,Venturi,L.,andBruna,J.Onthesamplecomplex-
Dieleman,S.,DeFauw,J.,andKavukcuoglu,K. Exploiting
ityoflearningunderinvarianceandgeometricstability,
cyclicsymmetryinconvolutionalneuralnetworks. arXiv
2021.
preprintarXiv:1602.02660,2016.
Bietti,A.,Bruna,J.,Sanford,C.,andSong,M.J. Learning
Doimo,D.,Glielmo,A.,Ansuini,A.,andLaio,A. Hierar-
single-indexmodelswithshallowneuralnetworks,2022.
chicalnucleationindeepneuralnetworks. Advancesin
Blum-Smith,B.andVillar,S. Machinelearningandinvari- NeuralInformationProcessingSystems,33:7526–7536,
anttheory,2023. URLhttps://arxiv.org/abs/ 2020.
2209.14991.
Elesedy,B. Provablystrictgeneralisationbenefitforinvari-
Bommasani,R.,Hudson,D.A.,Adeli,E.,andetal. Onthe ance in kernel methods. In Ranzato, M., Beygelzimer,
opportunitiesandrisksoffoundationmodels,2022. A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.),
Advances in Neural Information Processing Systems,
Brown, T. B., Mann, B., Ryder, N., and et al. Language
volume 34, pp. 17273–17283. Curran Associates, Inc.,
modelsarefew-shotlearners,2020.
2021. URL https://proceedings.neurips.
Bruna, J. and Mallat, S. Invariant scattering convolution cc/paper_files/paper/2021/file/
networks. IEEE transactions on pattern analysis and 8fe04df45a22b63156ebabbb064fcd5e-Paper.
machineintelligence,35(8):1872–1886,2013. pdf.
Cagnetta, F., Favero, A., and Wyart, M. What can Elesedy,B.andZaidi,S. Provablystrictgeneralisationben-
be learnt with wide convolutional neural networks? efitforequivariantmodels. InMeila,M.andZhang,T.
In International Conference on Machine Learning, pp. (eds.),Proceedingsofthe38thInternationalConference
3347–3379.PMLR,2023a. on Machine Learning, volume 139 of Proceedings of
MachineLearningResearch,pp.2959–2969.PMLR,18–
Cagnetta,F.,Petrini,L.,Tomasini,U.M.,Favero,A.,and
24 Jul 2021. URL https://proceedings.mlr.
Wyart,M.Howdeepneuralnetworkslearncompositional
press/v139/elesedy21a.html.
data: Therandomhierarchymodel,2023b.
Engstrom, L., Tran, B., Tsipras, D., Schmidt, L.,
Castleman,K.R. Digitalimageprocessing. PrenticeHall
and Madry, A. Exploring the Landscape of Spa-
Press,1996.
tial Robustness. In International Conference on
Cohen, T. and Welling, M. Learning the irreducible Machine Learning, pp. 1802–1811. PMLR, May 2019.
representations of commutative lie groups. In Xing, URL http://proceedings.mlr.press/v97/
E. P. and Jebara, T. (eds.), Proceedings of the 31st engstrom19a.html. ISSN:2640-3498.
International Conference on Machine Learning, vol-
Ensign,D.,Neville,S.,Paul,A.,andVenkatasubramanian,
ume 32 of Proceedings of Machine Learning Research,
S. Thecomplexityofexplainingneuralnetworksthrough
pp.1755–1763,Bejing,China,22–24Jun2014.PMLR.
(group)invariants. InHanneke,S.andReyzin,L.(eds.),
URLhttps://proceedings.mlr.press/v32/
Proceedings of the 28th International Conference on
cohen14.html.
AlgorithmicLearningTheory,volume76ofProceedings
Cohen, T. and Welling, M. Group equivariant convolu- ofMachineLearningResearch,pp.341–359.PMLR,15–
tionalnetworks. InBalcan,M.F.andWeinberger,K.Q. 17 Oct 2017. URL https://proceedings.mlr.
(eds.),ProceedingsofThe33rdInternationalConference press/v76/ensign17a.html.
10HowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
Favero,A.,Cagnetta,F.,andWyart,M. Localitydefeatsthe LakeCity,UT,June2018.IEEE.ISBN978-1-5386-6420-
curseofdimensionalityinconvolutionalteacher-student 9. doi: 10.1109/CVPR.2018.00467. URL https://
scenarios. Advances in Neural Information Processing ieeexplore.ieee.org/document/8578565/.
Systems,34:9456–9467,2021.
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
Fawzi, A. and Frossard, P. Manitest: Are clas- Chess,B.,Child,R.,Gray,S.,Radford,A.,Wu,J.,and
sifiers really invariant? In Procedings of the Amodei, D. Scaling laws for neural language models.
British Machine Vision Conference 2015, pp. 106.1– arXivpreprintarXiv:2001.08361,2020.
106.13, Swansea, 2015. British Machine Vision Asso-
Kayhan,O.S.andGemert,J.C.v.Ontranslationinvariance
ciation. ISBN978-1-901725-53-7. doi: 10.5244/C.29.
incnns: Convolutionallayerscanexploitabsolutespatial
106. URL http://www.bmva.org/bmvc/2015/
location. In Proceedings of the IEEE/CVF Conference
papers/paper106/index.html.
onComputerVisionandPatternRecognition,pp.14274–
14285,2020.
Finzi, M., Welling, M., and Wilson, A. G. G.
A practical method for constructing equivariant
Kondor,R.andTrivedi,S. Onthegeneralizationofequiv-
multilayer perceptrons for arbitrary matrix groups.
ariance and convolution in neural networks to the ac-
In Meila, M. and Zhang, T. (eds.), Proceedings
tion of compact groups. In Dy, J. and Krause, A.
of the 38th International Conference on Machine
(eds.),Proceedingsofthe35thInternationalConference
Learning, volume 139 of Proceedings of Machine
on Machine Learning, volume 80 of Proceedings of
Learning Research, pp. 3318–3328. PMLR, 18–24 Jul
MachineLearningResearch,pp.2747–2755.PMLR,10–
2021.URLhttps://proceedings.mlr.press/
15 Jul 2018. URL https://proceedings.mlr.
v139/finzi21a.html.
press/v80/kondor18a.html.
Fukushima, K. Cognitron: A self-organizing multilay- leCun,Y. Generalizationandnetworkdesignstrategies. In
ered neural network. Biological Cybernetics, 20(3): Connectionisminperspective,volume19,pp.143–155,
121–136, September 1975. ISSN 1432-0770. doi: 1989. URL https://api.semanticscholar.
10.1007/BF00342633. URLhttps://doi.org/10. org/CorpusID:244797850.
1007/BF00342633.
Lecun, Y., Bottou, L., Bengio, Y., and Haffner, P.
Grenander,U.Elementsofpatterntheory.JHUPress,1996. Gradient-basedlearningappliedtodocumentrecognition.
ProceedingsoftheIEEE,86(11):2278–2324,November
He, K., Zhang, X., Ren, S., and Sun, J. Deep Residual
1998. ISSN1558-2256. doi: 10.1109/5.726791. Confer-
Learning for Image Recognition. IEEE Conference on
enceName: ProceedingsoftheIEEE.
Computer Vision and Pattern Recognition (CVPR), pp.
770–778,June2016. doi: 10.1109/CVPR.2016.90. LeCun, Y., Bengio, Y., and Hinton, G. Deep learning.
Nature,521(7553):436,2015.
Hestness, J., Narang, S., Ardalani, N., Diamos, G., Jun,
H.,Kianinejad,H.,Patwary,M.,Ali,M.,Yang,Y.,and Luxburg,U.v.andBousquet,O. Distance-basedclassifi-
Zhou,Y.Deeplearningscalingispredictable,empirically. cationwithlipschitzfunctions. TheJournalofMachine
arXivpreprintarXiv:1712.00409,2017. LearningResearch,5(Jun):669–695,2004.
Malach, E. and Shalev-Shwartz, S. A provably correct
Ho, J., Jain, A., and Abbeel, P. Denoising diffusion
algorithmfordeeplearningthatactuallyworks,2018.
probabilistic models. Advances in neural information
processingsystems,33:6840–6851,2020.
Malach,E.andShalev-Shwartz,S.Theimplicationsoflocal
correlationonlearningsomedeepfunctions.Advancesin
Hupkes, D., Dankers, V., Mul, M., and Bruni, E. Com-
NeuralInformationProcessingSystems,33:1322–1332,
positionality decomposed: how do neural networks
2020.
generalise? (extended abstract). In Proceedings
of the Twenty-Ninth International Joint Conference Mallat, S. Understanding deep convolutional networks.
on Artificial Intelligence, IJCAI’20, 2021. ISBN Philosophical Transactions of the Royal Society A:
9780999241165. Mathematical, Physical and Engineering Sciences, 374
(2065):20150203,2016.
Kanbak,C.,Moosavi-Dezfooli,S.-M.,andFrossard,P. Ge-
ometricRobustnessofDeepNetworks: AnalysisandIm- Mei, S., Misiakiewicz, T., and Montanari, A. Learn-
provement.In2018IEEE/CVFConferenceonComputer ing with invariances in random features and ker-
Vision and Pattern Recognition, pp. 4441–4449, Salt nel models. In Belkin, M. and Kpotufe, S. (eds.),
11HowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
Proceedings of Thirty Fourth Conference on Learning Conference on Machine Learning, pp. 6105–6114.
Theory, volume 134 of Proceedings of Machine PMLR, May 2019. URL http://proceedings.
LearningResearch,pp.3351–3418.PMLR,15–19Aug mlr.press/v97/tan19a.html. ISSN: 2640-
2021.URLhttps://proceedings.mlr.press/ 3498.
v134/mei21a.html.
Tomasini, U. M., Petrini, L., Cagnetta, F., and Wyart, M.
Mei,S.,Misiakiewicz,T.,andMontanari,A.Generalization How deep convolutional neural networks lose spatial
errorofrandomfeatureandkernelmethods: hypercon- information with training. Machine Learning: Science
tractivityandkernelmatrixconcentration. Appliedand andTechnology,4(4):045026,nov2023. doi: 10.1088/
ComputationalHarmonicAnalysis,59:3–84,2022. 2632-2153/ad092c. URL https://dx.doi.org/
10.1088/2632-2153/ad092c.
Mossel,E. Deeplearningandhierarchalgenerativemodels,
2018. Voulodimos,A.,Doulamis,N.,Doulamis,A.,andProtopa-
padakis,E. Deeplearningforcomputervision: Abrief
Petrini,L.,Favero,A.,Geiger,M.,andWyart,M. Relative
review. Computational Intelligence and Neuroscience,
stabilitytowarddiffeomorphismsindicatesperformance
pp.1–13,2018.
indeepnets.AdvancesinNeuralInformationProcessing
Systems,34:8727–8739,2021. Xiao,C.,Zhu,J.-Y.,Li,B.,He,W.,Liu,M.,andSong,D.
SpatiallyTransformedAdversarialExamples. February
Poggio,T.,Mhaskar,H.,Rosasco,L.,Miranda,B.,andLiao,
2018. URLhttps://openreview.net/forum?
Q. Why and when can deep-but not shallow-networks
id=HyydRMZC-.
avoidthecurseofdimensionality: areview. International
Journal of Automation and Computing, 14(5):503–519, Xiao,L. Eigenspacerestructuring: aprincipleofspaceand
2017. frequencyinneuralnetworks.InConferenceonLearning
Theory,pp.4888–4944.PMLR,2022.
Rozenberg, G. and Salomaa, A. Handbook of Formal
Languages. Springer, January 1997. doi: 10.1007/ Xiao, L. and Pennington, J. Synergy and symmetry in
978-3-642-59126-6. deep learning: Interactions between the data, model,
and inference algorithm. In Chaudhuri, K., Jegelka,
Ruderman,A.,Rabinowitz,N.C.,Morcos,A.S.,andZoran,
S., Song, L., Szepesvari, C., Niu, G., and Sabato, S.
D.Poolingisneithernecessarynorsufficientforappropri-
(eds.),Proceedingsofthe39thInternationalConference
atedeformationstabilityinCNNs.arXiv:1804.04438[cs,
on Machine Learning, volume 162 of Proceedings of
stat], May 2018. URL http://arxiv.org/abs/
MachineLearningResearch,pp.24347–24369.PMLR,
1804.04438. arXiv: 1804.04438.
17–23Jul2022.URLhttps://proceedings.mlr.
press/v162/xiao22a.html.
Schmidt-Hieber, J. Nonparametricregressionusingdeep
neuralnetworkswithreluactivationfunction.TheAnnals
Zeiler,M.D.andFergus,R. Visualizingandunderstanding
ofStatistics,48(4):1875–1897,2020.
convolutional networks. In Computer Vision – ECCV
2014,LectureNotesinComputerScience,pp.818–833,
Sclocchi,A.,Favero,A.,andWyart,M. Aphasetransition
2014.
indiffusionmodelsrevealsthehierarchicalnatureofdata,
2024.
Zhang,R. Makingconvolutionalnetworksshift-invariant
again. arXivpreprintarXiv:1904.11486,2019.
Simonyan,K.andZisserman,A. VeryDeepConvolutional
Networks for Large-Scale Image Recognition. ICLR,
2015.
Spigler, S., Geiger, M., and Wyart, M. Asymptotic
learning curves of kernel methods: empirical data ver-
sus teacher–student paradigm. Journal of Statistical
Mechanics: Theory and Experiment, 2020(12):124001,
December 2020. ISSN 1742-5468. doi: 10.1088/
1742-5468/abc61d. URL https://doi.org/10.
1088/1742-5468/abc61d. Publisher: IOPPublish-
ing.
Tan,M.andLe,Q. EfficientNet: RethinkingModelScal-
ingforConvolutionalNeuralNetworks. InInternational
12HowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
Appendix
A.SparsityB
InthisAppendix,weshowsupportfortherobustnessofthesamplecomplexitiesEq. 3andEq. 4tothechoiceofthesparsity
inFigure2,panel(3). Indeed,weobserveinFigure7andFigure8thatthelearningcurvesobtainedimplementingSparsity
BinthedataareconsistentwithEq. 3andEq. 4,obtainedfromdatageneratedwithSparsityA.
LCN, Sparsity B
0.8
s0=1 LL==22
0.8
s0=2 LL==33
s0=4
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
103 104 105 106 10 2 10 1 100 101
P P/[(s 0+1)Ln cmL]
Figure7.Left:testerrorversusnumberoftrainingpointsP ofLCNtrainedontheSRHMwithsparsityB(describedinFigure2,panel
(3)),withdifferentL(differentcolors),differents (differentmarkers),s=2andm=v=n =6.Right:sameasleftbutrescalingP
0 c
byEq.3.
CNN, Sparsity B
s0=1 LL==22
0.8 s0=2 LL==33 0.8
s0=4
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
103 104 105 106 10 1 100 101 102 103
P P/[(s 0+1)2n cmL]
Figure8.Left:testerrorversusnumberoftrainingpointsP ofCNNtrainedontheSRHMwithsparsityB(describedinFigure2,panel
(3)),withdifferentL(differentcolors),differents (differentmarkers),s=2andm=v=n =6.Right:sameasleftbutrescalingP
0 c
bypredictionEq.4.
B.CommonarchitectureslearningtheSRHM
Architectures. Allnetworksimplementationscanbefoundatgithub.com/leonardopetrini/diffeo-sota/tree/main/models. In
Table1,adaptedwithpermissionoftheauthorsfrom(Petrinietal.,2021),welistthestructuralchoicesofthosenetworks.
Trainingscheme. WeuseStochasticGradientDescent(SGD)asoptimizer,withbatchsize4andmomentum0.9. The
learningratelrhasbeenoptimizedbyextensivegridsearch,finding: lr =10−4. Weuseastrainlossthecrossentropyloss,
andwestopthetrainingwhenitreachesathresholdof10−2.
13
rorre
tseT
rorre
tseTHowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
Figures.InFigure9wereportthetesterrorandthesensitivitiestoinputtransformationsof5differentcommonconvolutional
networkswithrespecttothenumberoftrainingpointsP. Thesensitivitiesarecomputedfor3differentlayersatdifferent
relativedepths.
VGG11 VGG16 ResNet18 ResNet34 EfficientNetB0
0.8 0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4 0.4
0.2 Sk 0.2 0.2 0.2 0.2
Dk
0.0 0.0 0.0 0.0 0.0
102 103 104 105 102 103 104 105 102 103 104 105 102 103 104 105 102 103 104 105
0.8 0.8 0.8 0.8 0.8
Sk
0.6 Dk 0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0 0.0
102 103 104 105 102 103 104 105 102 103 104 105 102 103 104 105 102 103 104 105
0.8 0.8 0.8 0.8 1.0
Sk
0.6 Dk 0.6 0.6 0.6 0.8
0.6
0.4 0.4 0.4 0.4
0.4
0.2 0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0 0.0
102 103 104 105 102 103 104 105 102 103 104 105 102 103 104 105 102 103 104 105
P P P P P
Figure9.Testerror(inblue),sensitivitytosynonymicexchange(inorange),sensitivitytodiffeomorphisms(ingreen)forincreasing
number of training points P of different common architectures (different columns). The sensitivities compute how much internal
representations,atincreasingrelativedepthkforincreasingrow,aresensitivetotransformationsappliedattheinput.Notethatat80%
relativedepthallthenetworksreachsmallsensitivitiesandtesterrorforlargeenoughP.Wechoosethisrelativedepthfortheresults
showninFigure1(A,B).ThesensitivitiesS andD refertothecaseℓ=1inEq.13andEq.14.
k k
Table1.Networkarchitectures,maincharacteristics.Foreachcolumnwelistthesalientstructuresofadifferentarchitecturesusedin
Figure9.Tableadaptedwithpermissionoftheauthorsfrom(Petrinietal.,2021)
structures VGG ResNet EfficientNetB0-2
(Simonyan&Zisserman,2015) (Heetal.,2016) (Tan&Le,2019)
depth 11,16 18,34 18
num.parameters 9-15M 11-21M 5M
FClayers 1 1 1
activation ReLU ReLU swish
pooling max avg.(lastlayeronly) avg.(lastlayeronly)
dropout / / yes+dropconnect
batchnorm if’bn’inname yes yes
skipconnections / yes yes(inv.residuals)
C.LearningtheSRHMwithGradientDescent
Inhierarchicalgenerativemodels,itisknownthatthefirststepofgradientdescentinsomesimplifiedsetupcanalreadybe
sufficienttogrouptogetherlowest-levelsynonyms(Malach&Shalev-Shwartz,2020;Cagnettaetal.,2023b). Inparticular,
14
htpeD
1=k
htpeD
1=k
htpeD
4=k
5
2
5HowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
thecaseoftheSRHMwiths = 0hasbeenanalyzedin(Cagnettaetal.,2023b). HereweshowforLCNswithagiven
0
initializationthatthestatisticsofthefirststepofgradientdescentfors >0isequivalenttothecases =0,withatraining
0 0
setsizereducedbyafactor(s +1)L,thussupportingtheresultofEq. 3onsamplecomplexity.
0
WeconsideraninstanceoftheSRHMwithLlevels,fromwhichwegenerateP trainingpointsx withlabely(x ). We
k k
recallthattrainingpointshavedimensiond=(s(s +1))L,witheachoneoftheinformativesLfeaturesbeingrepresented
0
withone-hotencodingofvfeatures,whilethe(s +1)Luninformativefeaturesbyemptycolumnsofdimensionv. The
0
labelsarerepresentedwithaone-hotencodingofthen labels. TolearntheSRHM,weuseasanetworkaLCNwithL
c
hiddenlayers,followedbyalinearlayer. Eachoneofthehiddenlayersf fork ∈{1,...,L}isdefinedwithrespecttothe
k
previouslayerf . Weusefilterswithsizeandstrideequaltos(s +1),yieldingareductionofs(s +1)inthespatial
k−1 0 0
sizeofthehiddenlayerateachlayer. Eachentryoff isspecifiedbytwoindices,oneforthechannelc∈{1,...,H }and
k k
oneforthespatiallocationi∈{1,...,(s(s +1))L−k,anditisgivenby:
0
 
1 H (cid:88)k−1
[f k(x)]
c;i
=ϕ(cid:112)
H
w⃗ ck ,c′,i·p⃗ i([f k−1(x)] c′), (9)
k−1 c′=1
whereϕistheReLUnon-linearityfunctionandw⃗k arethefiltersofthec−thchannelofthek-thlayer. Eachfilterw⃗k
c,c′,i c,c′,i
connectschannelcoflayerkwiththepatchp⃗ channelc′oflayerk−1. Notethatthefiltersdependonthepatchlocationi,
i
differentlywithrespecttoCNNs(seeFigure3). Thevectorp⃗ ([f (x)] )denotesas(s +1)-dimensionalpatchof
i,j k−1 c′ 0
[f (x)] centeredinthespatiallocationiatchannelc′ ∈{1,..,H }. Thebottomlayerk =1isdirectlylookingatthe
k−1 c′ k−1
input,hencewedefinef (x)astheidentityandthenumberofchannelsH isequaltov. TheoutputoftheLCNisgivenby:
0 1
1
(cid:88)HL
f(x) = a [f (x)] , (10)
α H α,c L c
L
c=1
withf(x)beingavectorofdimensionequaltothenumberofclassesn and{a }theparametersofthelastlinearlayer.
c α,c
WetrainsuchaLCNwithGDonthecross-entropyloss
(cid:88)P  (cid:88)nc (cid:32) e(f(xk))
β
(cid:33)
L= − y(x l) βlog
(cid:80)nc e(f(xk)) β′
. (11)
k=1 β=1 β′=1
Forsimplicitywe(i)takeallthenumberofchannelsH equaltoH fork >1,(ii)sendH →∞,(iii)fixthelinearlayer
k
parametersa tobei.i.d. standardGaussianvariablesand(iv)initializealltheweightsω⃗k tobeequaltotheunitary
α,c √ c,c′,i
vector⃗1renormalizedby H. Asaconsequenceof(iii),thenetworkoutputf(x) iszeroidenticallyatinitialization.
α
We now derive the weight updates after one step of GD. Let’s focus on the weights at the bottom layer [ω1 ] ,
c,c,′,i i0
where i denotes a position within the filter of size s(s +1) and i ∈ {1,...,[s(s +1)]L−1}. Note that, since there
0 0 0
is no weight sharing, each position z within the input dimension d is seen just one weight at the bottom layer, with
z =z(i,i )=(i−1)(s(s +1))+i . Consequently,theweightupdateforaweight[ω1 ] justdependsonthecontent
0 0 0 c,c,′,i i0
ofsuchpixellocationz =z(i,i ):
0
∂∆[ω1 ]
c,c,′,i i0
=−∇ L
∂t [ω c1 ,c,′,i]i0
 
1
(cid:88)P (cid:88)nc (cid:18)
1
(cid:19)
1
(cid:88)H
1
(cid:88)H
1
(cid:88)H
=− P  n −(y(x k)) α H a α,hLH ... H [x k] c′,z(i,i0) (12)
c
k=1 α=1 hL=1 hL−1=1 h2=1
1
(cid:88)P (cid:34) (cid:88)nc (cid:18)
1
(cid:19)
1
(cid:88)H (cid:35)
=− −(y(x )) a [x ] .
P n k α H α,hL k c′,z(i,i0)
c
k=1 α=1 hL=1
Onaverageoverallthetrainingsets,thepixelatlocationz(i,i )oftheinputdatumx relatestoaninformativefeaturewith
0 k
probabilityp =(s +1)−L.Consequently,onaveragejustafractionP′ =p P oftrainingpointswillgiveanon-vanishing
L 0 L
contributetotheweightupdateEq. 12,whichwillbeidenticaltotheGDequationofaninstanceofthesparselessSRHM
withP′trainingpoints. Thus,thesamplecomplexityinthesparsecases >0isequaltothesparselesssamplecomplexity
0
n mL increasedbyafactor(s +1)L,asempiricallyobservedinEq. 3. Atthissamplecomplexity,asinglestepofGD
c 0
iscapabletogrouptogetherrepresentationswithequalcorrelationwiththelabel,asshownin(Cagnettaetal.,2023b).
15HowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
D.SamplecomplexitiesandlearntrepresentationsforLCNs
Architectures. ShowninFigure3(a). Alllayersconsistof512channels.
Trainingscheme. WeuseStochasticGradientDescent(SGD)asoptimizer,withbatchsize4andmomentum0.9. The
learningratelrhasbeenoptimizedbyextensivegridsearch,finding: lr =0.01fors=2ands <4,lr =0.01fors=2
0
ands ≥4andlr =0.003fors≥2. Weuseastrainlossthecrossentropyloss,andwestopthetrainingwhenitreachesa
0
thresholdof10−3.
Samplecomplexityfigures. InFigure10,realisedwiths = 3,weshowadditionalsupporttothescalingofthesample
complexity P∗ , defined in Eq. 3. In Figure 11 we show for s = 3 that the insensitivity to diffeomorphism and to
LCN
synonymsexchangeareachievedatthesamesamplecomplexityP∗ atwhichthetaskislearned,supportingEq. 8.
LCN
s=3 v=4, s 0=1
s0=0 106 L=2
106 s0=1 L=3
s0=2 y x
s0=4
y x
105
105
104
v=4, L=2
104 v=6, L=2 s=2
v=4, L=3 103 s=3
104 105 103 104 105
(sL/2)(s +1)Ln mL sL/2
0 c
Figure10.LCN:Leftpanel: empiricalsamplecomplexityP∗toreacha10%testerrorεversuspredictionEq. 3fors=3,different
vocabularysizesvanddifferentdepthsL(differentcolors),numberofclassesn =v,maximalm=vs−1anddifferents (different
c 0
markers).NoteanadditionalfactorsL/2intheprediction,furthersupportedbytherightpanel,realisedatfixeds =1,n =m=4,and
0 c
varyingsandL.
106 s s0 0= =0
1 106
s0=2
s0=4
105
105
vv==44,, LL==22
104 vv==66,, LL==22
vv==44,, LL==33
105 106 105 106
(sL/2)(s +1)LvL+1 P* s.t. D =10%
0 S 2
Figure11.LCN.Leftpanel: empiricalsamplecomplexityP∗ toreacha10%testerrorεversusempiricalsamplecomplexityP∗ to
S
reachS =50%(asdefinedinEq.6)fors=3,differentvocabularysizesvanddifferentdepthsL(differentcolors),numberofclasses
2
n =v,maximalm=vs−1anddifferents (differentmarkers). Rightpanel: sameasleftforthesamplecomplexityP∗ toreacha
c 0 D
D =10%(withD asdefinedinEq.7).ThethresholdshavebeentunedbasedontheformofS andD versusP.
2 2 2 2
Learnt representations figures. To assess whether the internal representation of the trained network have learnt the
productionrulesEq. 1andEq. 2,wedefineanoperatorp whichtakesinadatumxandsubstituteseachinformatives
l
16
%01=
.t.s
*P
%01=
.t.s
*PHowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
patchesatgenerationlevellwithoneofitsm−1synonyms,chosenuniformlyatrandom,keepingthefeaturepositions
intact. Thelevellgoesfroml=1,theinput,toL,thepatchclosesttothelabel. WedefinethesensitivityS ofthenetwork
k,l
representationf ataninternallayerk ∈{1,...,L}(withk =1beingthelayerclosesttotheinputwhilek =Ltheone
k
closesttotheoutput)tosynonymicexchangep asitsaveragechangebeforeandafterapplyingp onaninputdatumx:
l l
⟨||f (x)−f (p (x))||2⟩
S = k k l x,pl , (13)
k,l ⟨||f (x )−f (x ))||2⟩
k 1 k 2 x1,x2
wherex,x andx belongtoatestset,whiletheaverage⟨.⟩isoverthetestsetandontherandomexchangeofsynonyms
1 2
P . Thesamedefinitionapplyfortheoutputofthedeepnetwork. ThemeasureEq. 6isaparticularcaseofEq. 13fork =2
l
andl=1,whichcanbevisualizedinFigure6(A).
Toestimatethesensitivitytodiffeomorphismsoftrainednetworkswedefineanoperatorτ ,whichtakesasinputadatumx
l
andshiftsrandomlyitsfeaturesaccordingtothepossiblepositionsshowninFigure2,panel(3),notimpactingthefeature
values. SimilarlytoS ,wedefinethesensitivityoff todiffeomorphismsτ atlayerlas
k,l k l
⟨||f (x)−f (τ (x))||2⟩
D = k k l x,τl . (14)
k,l ⟨||f (x )−f (x ))||2⟩
k 1 k 2 x1,x2
ThemeasureEq. 7specializesEq. 13tok =2andl=1,picturedinFigure6(B).T
hebehavioursofS andD withrespecttoP areshownfordifferentvaluesofs=L=2inFigure12. Wevarykandl
k,l k,l
in{1,...,L},andweanalysethesensitivityoftheoutputtoo. Fork ≥l+1thesensitivitiesbecomesignificantlylowerfor
largeP. WecheckedtherobustnessofsuchobservationfordifferentvaluesofsandL(notshownhere).
E.SamplecomplexitiesandlearntrepresentationsforCNNs
Architectures. ShowninFigure3(b). Alllayersconsistof512channels.
Trainingscheme. SameasLCN,exceptforthelearningrate: lr = 0.01fors = 2ands < 4,lr = 0.01fors = 2and
0
s ≥4andlr =0.003fors≥2.
0
Samplecomplexityfigures. InFigure13,realisedwithpatchsizes=3,weshowadditionalsupporttothescalingofthe
samplecomplexityEq. 3. InFigure14weshowthat,asfortheLCNs,alsoforCNNstheinsensitivitytodiffeomorphism
andtosynonymsexchangeareachievedatthesamesamplecomplexityP∗ atwhichthetaskislearned,definedinEq. 4.
CNN
Learntrepresentationfigures. InFigure15wereportthetesterrorandthesensitivitiestoinputtransformationsS and
k,l
D definedinEq. 13andEq. 14,withrespecttoP,forCNNstrainedontheSRHMfordifferentvaluesofLands. We
k,l
varykandlin{1,...,L},andweanalysethesensitivityoftheoutputtoo. AsfortheLCNs,fork ≥l+1thesensitivities
becomesignificantlylowerforlargeP. WecheckedtherobustnessofsuchobservationfordifferentvaluesofsandL(not
shownhere).
F.SamplecomplexitiesforFCNs
Architectures. ThenetworksaremadebystackingLfull-connectedlayers,followedbyareadoutlayer. Alllayersconsist
of512channelsforL=2and256channelsforL=3.
Trainingscheme. SameasLCN,exceptforthelearningrate: lr =0.01forL=2andlr =0.003forL=3.
Samplecomplexityfigures. InFigure16,realisedwithpatchsizes=2,weshowthat,asfortheLCNsandCNNs,alsofor
full-connectednetworks(FCN)theinsensitivitytodiffeomorphismandtosynonymsexchangeareachievedatthesame
samplecomplexityatwhichthetaskislearned.
G.Correlationbetweentesterrorandinternalsensitivities
17HowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
LCN, L=2 and s=2
Net layer k:1 Net layer k:2 Output
1.0 s0=0 1.0 1.0
s0=1
s0=2
s0=4
0.8
0.8 0.8
0.6 0.6 0.6
Sem. permutation l=1
Sem. permutation l=2
Test error
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
101 102 103 101 102 103 101 102 103
1.0 Diffeo l=1
Diffeo l=2
0.8
0.8
0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
101 102 103 101 102 103 101 102 103
P/(s 0+1)L P/(s 0+1)L P/(s 0+1)L
Figure12.Top: sensitivity to synonyms exchange S Eq. 13 of a L−layer LCN, trained on the SRHM with L = 2, s = 2 and
k,l
m=v=n =8,versusnumberoftrainingpointsP rescaledbypredictionEq.3.Goingfromlefttorightcolumnthenetworklayerk
c
increasesfromthelayerclosesttotheinputtotheoutput.ForeachcolumnatfixedkthereareplottedincolorthesensitivitiesS to
k,l
synonymsexchangeatthedatalevell∈[1,...,L],andingreythetesterror.Differentmarkersstandfordifferents .Bottom:sameas
0
top,butwiththesensitivitytodiffeomorphismsD Eq.14.ThesensitivitiesS andD definedinEq.6andEq.7relatetothecase
k,l 2 2
k=2andl=1here.
18
l,kS
ytivitisnes
citnameS
l,kD
ytivitisnes
oeffiDHowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
s=3
s0=0
s0=1
s0=2
s0=4
105
s0=6
104
vv==44,, LL==22
vv==66,, LL==22
vv==44,, LL==33
103 104 105
(s +1)2n mL
0 c
Figure13.CNN:EmpiricalsamplecomplexityP∗toreacha10%testerrorεversuspredictionEq.4fors=3,forvocabularysizev
anddifferentdepthsL(differentcolors),maximalm=vs−1anddifferents (differentmarkers).
0
s=2 s=3
s0=0
s0=1
105 s0=2
s0=4
s0=6
y x 105
104
103
v, L=2
3 12
v, L=3
104
102 3 12
102 103 104 105 104 105
105
104
105
103
103 104 105 105
P S* s.t. S 2=T s,L% P D* s.t. D 2=20%
Figure14.CNN.Leftpanels:empiricalsamplecomplexityP∗toreachS =T %(withthethresholdT =40ifL=2ands=2,
S 2 s,L s,L
50ifL=2ands=3,20ifL=3)versusempiricalsamplecomplexityP∗toreacha10%testerrorεfor(Top)s=2and(Bottom)
s=3,fordifferentdepthsL(redforL=2,blueforL=3)vocabularysizesv(differentdarkness),maximalm=vs−1anddifferent
s (differentmarkers).Rightpanels:sameasleftpanels,butwiththesamplecomplexityP∗ toreachD =20%.BothP∗andP∗ align
0 D 2 S D
withP∗.ThethresholdshavebeentunedbasedontheformofS andD versusP.
2 2
19
%01=
.t.s
*P
%01=
.t.s
*P
%01=
.t.s
*PHowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
CNN, L=2 and s=2
Net layer k:1 Net layer k:2 Output
1.0 1.0
1.0 s0=0
s0=1
s0=2
0.8 s0=4 0.8 0.8
0.6 0.6 0.6
Sem. permutation l=1
Sem. permutation l=2
Test error
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
102 103 104 102 103 104 102 103 104
1.0 1.0 1.0
Diffeo l=1
Diffeo l=2
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
102 103 104 102 103 104 102 103 104
P/(s 0+1)2 P/(s 0+1)2 P/(s 0+1)2
Figure15.Top:sensitivitytosynonymsexchangeS ofaL−layerCNN,trainedontheSRHMwithL=2,s=2andm=v =8,
k,l
versusnumberoftrainingpointsP rescaledbypredictionEq.4.Goingfromlefttorightcolumnthenetworklayerkincreasesfromthe
layerclosesttotheinputtotheoutput.ForeachcolumnatfixedkthereareplottedincolorthesensitivitiesS tosynonymsexchange
k,l
atthedatalevell ∈ [0,...,L],andingreythetesterror. Differentmarkersstandfordifferents . Bottom: sameastop,butwiththe
0
sensitivitytodiffeomorphismsD .ThesensitivitiesS andD definedinEq.6andEq.7relatetothecasek=2andl=1here.
k,l 2 2
20
l,kS
ytivitisnes
citnameS
l,kD
ytivitisnes
oeffiDHowDeepNetworksLearnSparseandHierarchicalData:theSparseRandomHierarchyModel
106
s0=0
106 s0=1
s0=2
y x
105 105
104 vv==66,, LL==22
vv==88,, LL==22
vv==44,, LL==33 104
vv==66,, LL==33
vv==88,, LL==33
103
103 104 105 106 104 105
P S* s.t. S 2=30% P D* s.t. D 2=10%
Figure16.FCN.Leftpanel: empiricalsamplecomplexityP∗ toreacha10%testerrorεversusempiricalsamplecomplexityP∗ to
S
reachS =30%fors=2,differentvocabularysizesvanddepthsL(differentcolors),numberofclassesn =v,maximalm=vs−1
2 c
anddifferents (differentmarkers).Rightpanel:sameasleft,forempiricalsamplecomplexityP∗toreacha10%testerrorεversus
0
empiricalsamplecomplexityP∗ toreachD =10%.
D 2
SRHM
10 1 10 1
LCN
CNN
VGG11
VGG16
ResNet18
ResNet34
EfficientNetB0
(A()B) D1 D0 iL n1 D* t,L e1* r, n1
al
100 (B()C)10 1 SS SLi*n,it1ne tr en ra nl
al
100
100
SSRRHHMM
10 1 10 1
100
ResNet34
EfficientNetB0
10 2 VGG16 10 2
LCN
train set sizeP
1e3 5e4
10 3 10 3
10 2 10 1 100 10 1 100
(C) D internal (D) S internal
Figure17.(A)TesterrorvssensitivityD todiffeomorphismsofaninternaldeeplayerforaCNNtrainedwithP =7400onthe
internal
SHRMmodel,withparametersL = s = s = 2andn = m = 10. D isdefinedasthechangeofthehiddenrepresentation
0 c internal
inducedbyadiffeomorphismappliedontheinput,seeEq.7.Theinternallayerisat80%relativedepthofthearchitecture,exceptforthe
2hidden-layerLCN,whereitcorrespondstothesecondlayer.Thetesterrorshowsaremarkablecorrelationwiththesensitivities.Agrey
line,correspondingtoapower-law,guidestheeye.Fordetailsaboutthearchitecturesandtheirtrainingprocess,seeAppendixB.(B)
Sameas(A)forsensitivityS tosynonymicexchanges,definedasthechangeofthehiddenrepresentationinducedbyanexchangeof
internal
synonymsappliedontheinput,seeEq.6.(C)and(D):astoppanels(A)and(B),forincreasingP (increasingopacity).Thesensitivities
ofthenetworkoutputyieldthesameobservations,asshowninFigure1.
21
%01=
.t.s
*P