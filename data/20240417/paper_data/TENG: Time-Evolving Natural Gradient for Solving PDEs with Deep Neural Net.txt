TENG: Time-Evolving Natural Gradient for Solving PDEs with Deep Neural Net
ZhuoChen12 JacobMcCarran*1 EstebanVizcaino*1 MarinSoljacˇic´12 DiLuo123
Abstract intothestudyofPDEs(Hanetal.,2017;Yuetal.,2018;
Long et al., 2018; Carleo & Troyer, 2017; Raissi et al.,
Partial differential equations (PDEs) are instru-
2019;Lietal.,2020;Luetal.,2019;Hanetal.,2018;Sirig-
mentalformodelingdynamicalsystemsinscience
nano&Spiliopoulos,2018;Chenetal.,2022;2023b)has
andengineering. Theadventofneuralnetworks
marked a transformative shift in both fields, particularly
hasinitiatedasignificantshiftintacklingthese
highlightedintherealmsofcomputationalmathematicsand
complexitiesthoughchallengesinaccuracyper-
data-driven discovery. Machine learning offers new pos-
sist,especiallyforinitialvalueproblems. Inthis
sibilities for tackling the complexities inherent in PDEs,
paper, we introduce the Time-Evolving Natural
whichoftenposesignificantchallengesfortraditionalnu-
Gradient (TENG), generalizing time-dependent
mericalmethodsduetohighdimensionality,nonlinearity,
variationalprinciplesandoptimization-basedtime
orchaoticbehavior. Byleveragingneuralnetworks’ability
integration,leveragingnaturalgradientoptimiza-
to approximate complex functions, algorithms have been
tion to obtain high accuracy in neural-network-
developedtosolve,simulate,andevendiscoverPDEsfrom
based PDE solutions. Our comprehensive de-
data,circumventingtheneedforexplicitformulations.
velopmentincludesalgorithmslikeTENG-Euler
anditshigh-ordervariants,suchasTENG-Heun, Partialdifferentialequationswithinitialvalueproblems,cru-
tailored for enhanced precision and efficiency. cialindescribingtheevolutionofdynamicalsystems,repre-
TENG’seffectivenessisfurthervalidatedthrough sentafundamentalclasswithintherealmofPDEs. Despite
itsperformance,surpassingcurrentleadingmeth- the promising advancements made by machine learning
odsandachievingmachineprecisioninstep-by- techniquesinapproximatingthesolutionstothesecomplex
step optimizations across a spectrum of PDEs, PDEs,theyfrequentlyencounterdifficultiesinmaintaining
includingtheheatequation,Allen-Cahnequation, highlevelsofaccuracy,achallengethatbecomesparticu-
andBurgers’equation. larlypronouncedwhennavigatingtheintricateinitialcondi-
tions. Thischallengelargelyoriginatesfromthecumulative
and propagative of errors in PDE solvers over time, ne-
1.Introduction cessitatingprecisesolutionsateachtimestepforaccuracy.
Although various training strategies, both global-in-time
Partialdifferentialequations(PDEs)holdprofoundsignifi-
training(Mu¨ller&Zeinhofer,2023)andsequential-in-time
canceinboththetheoreticalandpracticalrealmsofmathe-
training(Chenetal.,2023a;Berman&Peherstorfer,2023),
matics,science,andengineering.Theyareessentialtoolsfor
have been proposed to address this issue, it continues to
describingandunderstandingamultitudeofphenomenathat
standasacriticalchallengeinthefield.
exhibitvariationsacrossdifferentdimensionsandpointsin
time. ThestudyandsolutionofPDEshavedrivenadvance- Contributions.Inthispaper,weintroduceahighlyaccurate
mentsinnumericalanalysisandcomputationalmethods,as andefficientapproachfortacklingtheabovechallengebyin-
manyreal-worldproblemsmodeledbyPDEsaretoocom- troducingTime-EvolvingNaturalGradient(TENG).Our
plex for analytical solutions. The long-pursued quest for keycontributionsarethree-foldandhighlightedasfollows:
anefficientandaccuratenumericalPDEsolvercontinues
tobeacentralendeavorpassionatelypursuedbyresearch
• ProposetheTENGmethodwhichgeneralizestwofun-
communities.
damentalapproachesinthefield,time-dependentvari-
Inrecentyears,theintroductionofmachinelearning(ML) ationalprinciple(TDVP)andoptimization-basedtime
integration (OBTI), and achieves highly accurate re-
*Equalcontribution 1DepartmentofPhysics,Massachusetts
sultsbyintegratingnaturalgradientwithsequential-in-
InstituteofTechnology2NSFAIInstituteforArtificialIntelligence
timeoptimization.
andFundamentalInteractions3DepartmentofPhysics,Harvard
University.Correspondingauthor:DiLuo.
• Developefficientalgorithmswithsparseupdateforthe
Preprint.Copyright2024bytheauthor(s). realizationofTENG,includingthebasicTENG-Euler
1
4202
rpA
61
]GL.sc[
1v17701.4042:viXraTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
andthehighlyaccuratehigher-orderversions,suchas 2020),andPINNtraining(Mu¨ller&Zeinhofer,2023).
TENG-Heun.
Time Dependent Variational Principle
• Demonstratethatourapproachobtainsordersofmagni-
tudebetterperformancesthanstate-of-the-artmethods
ℒ𝑢𝑢�𝜃𝜃𝑡𝑡
suchasOBTI,TDVPwithsparseupdates,andPINN 𝑇𝑇𝑢𝑢�𝜃𝜃𝑡𝑡ℳΘ
Generalize
with energy natural gradient, and achieves machine
Repeated tangent projection
𝑢𝑢�𝜃𝜃𝑡𝑡
precisionaccuracyduringper-stepoptimizationona 𝑢𝑢�𝜃𝜃𝑡𝑡+Δ𝑡𝑡
variety of PDEs, including the heat equation, Allen-
Cahnequation,andBurgers’equation. Time
ℳΘ
Evolving
2.Relatedwork Natural Optimization-Based Time Integration
Gradient
MachineLearninginPDEs. Machinelearninghasbeen 𝑢𝑢�𝜃𝜃𝑡𝑡+Δ𝑡𝑡ℒ𝑢𝑢�𝜃𝜃𝑡𝑡
usedtosolvePDEsbyusingneuralnetworksasfunctionap-
proximatorstothesolutions. Ingeneral,therearetwotypes
Optimize in -space 𝑢𝑢�𝜃𝜃𝑡𝑡
ofstrategies,global-in-timeoptimizationandsequential-in- Generalize 𝑢𝑢�𝜃𝜃𝑡𝑡+Δ𝑡𝑡
𝑢𝑢
time optimization. Global-in-time optimization includes
thephysics-informedneuralnetwork(PINN)(Raissietal., ℳΘ
2019;Wang&Perdikaris,2023;Wangetal.,2021b;Sirig-
Figure1.TENGgeneralizestheexistingTDVPandOBTImethods.
nano & Spiliopoulos, 2018; Wang et al., 2021a), which
Withinasingletimestep,TDVPprojectstheupdatedirectionLuˆ
optimizestheneuralnetworkrepresentationovertimeand θt
ontothetangentspaceoftheneuralnetworkmanifoldT M at
spacesimultaneously,ordeepRitzmethod(Weinanetal., uˆθt Θ
timet,andevolvestheparametersθaccordingtothistangentspace
2021; Yu et al., 2018) when the variational form of the
projection. OBTIoptimizesθtoobtainanapproximationtothe
PDE exists. In contrast, sequential-in-time optimization
targetfunctionuˆ +∆tLuˆ onthemanifoldM .Generalizing
(sometimesalsocalledneuralGalerkinmethod)onlyuses thesetwomethodθt s,TENGdθ et finesthelossfunctioΘ ndirectlyinthe
the neural network to represent the solution at a particu- u-spaceandoptimizesthelossfunctionviarepeatedprojections
lar time step and updates the neural network representa- tothetangentspaceT M .
uˆθt Θ
tion step-by-step in time. There are different approaches
toachievingsuchupdates,includingtime-dependentvari-
ational principle (TDVP) (Dirac, 1930; Koch & Lubich, 3.ProblemFormulationandChallenges
2007;Carleo&Troyer,2017;Du&Zaki,2021;Berman
3.1.Problemformulation
&Peherstorfer,2023)andoptimization-basedtimeintegra-
tion(OBTI)(Chenetal.,2023a;Kochkov&Clark,2018; Given a spatial domain X ⊆ Rd and temporal domain
Gutie´rrez&Mendl,2022;Luoetal.,2022;2023). Machine T ⊂ R,letubeafunctionX ×T → Rthatsatisfiesthe
LearninghasalsobeenappliedtomodelPDEsbasedondata.
followinginitialvalueproblemofaPDE
Such data-driven approaches include neural ODE (Chen
et al., 2018), graph neural network methods (Pfaff et al., ∂u(x,t)
=Lu(x,t) for (x,t)∈X ×T and
2020;Sanchez-Gonzalezetal.,2020),neuralFourieropera- ∂t (1)
tor(Lietal.,2020),andDeepONet(Luetal.,2019). u(x,0)=u (x),
0
NaturalGradient. Theconceptofnaturalgradients,first
with appropriate boundary conditions. The sequential-in-
introducedbyAmari(Amari,1998)hasbecomeacorner-
timeoptimizationapproachusesneuralnetworktoparame-
stoneintheevolutionofoptimizationtechniqueswithinma-
terizethesolutionofthePDEataparticulartimestept∈T
chinelearning. Thesemethodsmodifytheupdatedirection
as uˆ (x) : Θ×X → R, where the parameters have an
ingradient-basedoptimizationasasecond-ordermethod, θt
typicallyinvolvingusingtheFishermatrix. Distinctfrom
explicitly time dependence θ
t
: T → RNp (with N
p
the
number of parameters) and evolves over time. To solve
traditionalgradientmethodsduetoitsconsiderationofthe
thePDE,theneuralnetworkisfirstoptimizedtomatchthe
underlying data geometry, natural gradient descent leads
initialconditionuˆ (x) = u (x),andthenoptimizedina
tofasterandmoreeffectiveconvergenceinvariousscenar- θ0 0
time-step-by-time-stepfashiontoupdatetheparameters.
ios. Natural gradient descent and its variants have found
widespreadapplicationinareassuchasneuralnetworkop- We contrasted this with the global-in-time optimization
timization (Peters et al., 2003; Pascanu & Bengio, 2013; method,suchasPINN(Raissietal.,2019),wheretheneu-
Zhang et al., 2019), reinforcement learning (Peters et al., ralnetworkisusedtoparameterizethesolutionforalltime
2003;Kakade,2001),quantumoptimization(Stokesetal., uˆ (x,t)withasinglesetofparameters. Inthiscontext,a
θ
2TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
lossfunctionthatgivesrisetotheglobalsolutionofthePDE lossfunction
isusedtooptimizetheparameters.
θ =argminL(uˆ ,u ). (3)
t+∆t θ target
θ∈Θ
3.2.TimeDependentVariationalPrinciple
Dependingonthediscrete-timeintegrationschemesused,
Time-dependentvariationalprinciple(TDVP)isanexisting
u can be different. The most commonly used in-
target
sequential-in-time method. It aims to derive an ODE in
tegration scheme is the forward Euler’s method, where
theparameterθ-spacebasedonthefunctionu-spacePDE
u =uˆ +∆tLuˆ Sometypicallossfunctionsused
(Fig.1). Themostcommonlyusedprojectionmethodisthe inta Org Bet TImeθ tt hodsincluθ dt etheL2-distance,theL1-distance,
Dirac–Frenkelvariationalprinciple(Dirac,1930),whichde-
andtheKL-divergence.
finestheODEbysolvingthefollowingleastsquareproblem
ateachtimestep Limitations. AlthoughtheoptimalsolutiontoEq.(3)gives
thebestapproximationofu inM ,inpractice,theop-
(cid:13) (cid:13)2 target Θ
∂ θ =argmin(cid:13)Luˆ (·)−(cid:80) J ∂ θ (cid:13) , (2) timizationcanbeverydifficultwithanon-convexlandscape.
t (cid:13) θ j (·),j t j(cid:13)
∂tθ∈RNp L2(X) CommonoptimizerssuchasAdamandBFGS(L-BFGS)
whereJ :=∂uˆ (x)/∂θ istheJacobian. often require a significant number of iterations to obtain
(x),j θ j
anacceptablelossvalue. Sincethisoptimizationhastobe
DenotingthefunctionspaceofuwithU,themanifoldof
repeatedoveralltimesteps,theaccumulationoferrorand
neuralnetworkparameterizedfunctionsuˆ withM ,and
θ Θ costoftenresultsinpoorperformance. Inaddition,theinte-
the tangent space to the manifold at uˆ with T M ,
θt uˆθt Θ grationschemeusedincurrentimplementationsofOBTIis
Eq. (2) gives the orthogonal projection of the evolution
oftenlimitedtotheforwardEuler’smethod,whichrequires
direction∂ u=LuontothetangentspaceT M gener-
t uˆθt Θ small time steps and further amplifies the issue of error
atedbythepushfowardof∂ θ. TheresultingODEinthe
t accumulation and cost. We note that while higher-order
θ-space can then be evolved in discrete time steps using
integrationschemeshavebeenexploredinpriorworks,they
numerical integrators such as the 4th-order Runge–Kutta eitherinvolveapplyingLmultipletimesonuˆ (Donatella
θ
(RK4)method. et al., 2023) or require differentiating through Luˆ with
θ
Limitations. TheDirac–Frenkelvariationalprinciplepro-
respecttoθ(Luoetal.,2022;2023),bothofwhichrequires
ducestheorthogonalprojectionoftheevolutionontothe high-order differentiation, leading to stability issues and
tangentspaceT M atuˆ duringeachtimestep. For furtherincreaseofthecost.
uˆθt Θ θt
nonzero time step sizes ∆t, however, the result becomes
onlyanapproximationtotheoptimalprojectionofthetarget 4.Time-EvolvingNaturalGradient(TENG)
solutionontothemanifoldM . TheevolutiononM can
Θ Θ
alsodeviatefromtheprojecteddirectiononT M due 4.1.GeneralizationfromTDVPandOBTI
uˆθt Θ
tononzerotimestepsizes,whichgivesrisetothefollow-
Wefirstmakethefollowingobservation.
ing consequence: although Eq. (2) is reparameterization
invariant,itsnonzero∆tversionisnot(seeAppendixThe- Observation: TDVPcanbeviewedassolvingEq.(3)with
oremA.1fordetail). Inaddition,theleastsquareproblem
the(squared)L2-distanceasthelossfunctionusingasingle
in Eq. (2) is often ill-conditioned and the solution could tangentspaceapproximationateachtimestep.
be sensitive to the number of parameters, the number of
samples used, and the regularization method. Although Proof. Attimet,theneuralnetworkmanifoldM Θcanbe
Ref.(Berman&Peherstorfer,2023)proposedasparseup- approximatedatthepointuˆ θt byitstangentspaceas
datemethod,wherearandomsubsetofparametersareup-
datedateachtimestep,itisstillhardtoverifywhethersuch uˆ =uˆ
+(cid:88)∂uˆ
θδθ +O(δθ2), (4)
θ+δθ θ ∂θ j
choicegivesthebestprojectioninpractice. Meanwhile,the j j
solution of Eq. (2) after regularization could be different
fromoptimal. Let L(uˆ θ+δθ,u target) = ∥uˆ θ+δθ−u target∥2 L2(X). For
small δt, u = uˆ +δtLuˆ +O(δt2). Keeping ev-
target θ θ
3.3.Optimization-basedTimeIntegration erythingtofirstorder,thelossfunctiontakesitsminimum
when
Optimization-basedtimeintegration(OBTI)isanalterna-
tivesequential-in-timemethod. Itdirectlydiscretizesthe (cid:13) (cid:13) (cid:13) (cid:13)2
PDEintotimestepsintheoriginalfunctionu-space;ineach δθ =argmin(cid:13) (cid:13) (cid:13)δtLuˆ θ(·)−(cid:88)∂u ∂ˆ θ θ(·) δθ j(cid:13) (cid:13)
(cid:13)
. (5)
timestep,OBTIfirstfindsthenext-time-steptargetfunction δθ∈RNp (cid:13) j j (cid:13)
L2(X)
u based on the current-time-step uˆ , and then opti-
target θt
mizesthenext-time-stepparametersθ byminimizinga DividingbothsidesbyδtrecoverstheTDVP(Eq.(2)).
t+∆t
3TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Inspired by such observation, we introduce the time- Proof. TENGachievesitsoptimumwhenuˆ∈M isclos-
Θ
evolving natural gradient (TENG) method, which gener- esttou ateachtimestep. Sinceareparameterization
target
alizesTDVPandOBTImethodsinthefollowingway: doesnotchangethemanifoldM andthelossisdefined
Θ
inthefunctionspace,therefore,theoptimalsolutiondiffers
TENGsolvesEq.(3)viaarepeatedtangentspaceapproxi-
bymerelyarelabelingofparametersatthesamepointin
mationtothemanifoldforeachtimestep.
M .
Θ
TENGsubroutinewithineachtimestep. Thekeyideaof
TENGisshowninFig.1. Duringeachtimestep, TENG
Theglobalconvergenceofnaturalgradienthasbeenstudied
minimizesthelossfunctioninmultipleiterations(similarto
in certain non-linear neural network (Zhang et al., 2019).
OBTI),andwithineachiteration,itupdatestheparameters
Althoughachievingglobaloptimalisnottheoreticallyguar-
basedonthefunctionu-spacegradient(ofthelossfunction)
anteedingeneral, inpractice, thelossfunctionisusually
projectedtotheparameterθ-space(similartoTDVP).Here,
convexintheu-space,anduˆ isoftenclosetou be-
θ target
wereservethephrase“timestep”forphysicaltimestepsof
cause of small time step sizes. The result is likely to be
thePDEand“iteration”foroptimizationstepswithineach
close to global optimal. In practice, we observe the opti-
physicaltimestep.
mizationcanresultinlossvaluesclosetomachineprecision
The details of TENG iterations within a single time (O(10−14)).
step are shown in Subroutine TENG stepper, where
In addition, while TDVP may require solving an ill-
α is the learning rate at the nth iteration, and the
n conditioned least square equation and an inaccurate so-
least square(J ,∆u(x)) should be interpreted as
(x),j lution directly affects the θ-space ODE, solving the least
solvingtheleastsquareproblem
squareproblemisonlypartoftheoptimizationprocedure
forTENG,whichturnsouttohaveasmallersideeffect. An
(cid:13) (cid:13)2
∆θ =argmin(cid:13)∆u(·)−(cid:80) J ∆θ (cid:13) . (6) inaccurate least square solution does not lead to an inac-
(cid:13) j (·),j j(cid:13)
∆θ∈RNp L2(X) curatesolutiontoEq.(3),givensufficientiterations. The
resultinglossvalueofEq.(3)alsoprovidesaconcretemet-
ricforTENGontheaccuracyduringoptimization.
SubroutineTENG stepper
AsdiscussedinSec.3.3,themainchallengeforthecurrent
Input: θ init,u target OBTImethodliesinthedifficultyofoptimizingthetime
n←0,θ ←θ init integratinglossfunction(Eq.(3)). Whilethelossfunction
whilen<N itdo is a complicated non-convex function in the parameter θ-
∆u(x)←−α
∂L(uˆ θ,u target)
(x)
space, it is usually convex in the u-space; therefore, it is
n ∂uˆ θ advantageoustoperformgradientdescentinu-spaceand
∂uˆ (x)
J ← θ project the solution to θ-space. Furthermore, TENG can
(x),j ∂θ j alsobenefitfromthereparametrizationinvariantproperty
∆θ ←least square(J (x),j,∆u(x)) describedabove. Whileanefficienthigher-ordertimeinte-
θ ←θ+∆θ grationmethodisstilllackinginthecurrentOBTImethod,
n←n+1 inthisworkweshowhowtoincorporatehigher-ordermeth-
endwhile odsintoTENG.
Output: θ
TENGformulationovertimesteps. Themostsimpletime
integration scheme is the forward Euler’s method, which
onlykeepsthelowestorderTaylorexpansionofthePDE.
We note that when Subroutine TENG stepper is per-
When integrated in the TENG method, we set u =
formed under certain approximations, it can be reduced target
uˆ +∆tLuˆ anduseSubroutineTENG steppertosolve
toTDVPorOBTI(seeAppendixAfordetail). θt θt
for uˆ . The full algorithm is summarized in Algo-
θt+∆t
TENGresolvesthelimitationsofbothTDVPandOBTI. rithmTENG Euler.
As mentioned in Sec. 3.2, TDVP suffers from inaccurate
Beyond the first-order Euler’s method, Algorithm
tangentspaceapproximationfornonzero∆t. TENGdoes
TENG Heunprovidesanexampleofapplyingsecond-order
notsufferfromthisissuebecauseoftherepeateduseoftan-
integrationmethod. Inthismethod,anintermediatetarget
gentspaceprojections,whicheventuallyminimizesEq.(3)
solutionu isused,andasetofintermediateparameters
onthemanifold. Thisalsogivesthefollowingtheoremasa temp
θ istrained. Theintermediateparametersareusedto
directconsequence,whichdoesnotholdforTDVP. temp
constructu andθ . Ourmethodavoidstermslike
target t+∆t
Theorem4.1. TheoptimalsolutionofTENGisreparame- Lnuˆ or ∂Luˆ /∂θ that often appear in existing OBTI
θt θt t
terizationinvariantevenwithnonzero∆t. methods (Donatella et al., 2023; Luo et al., 2022; 2023),
4TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
AlgorithmTENG Euler: A1st-orderintegrationscheme AlthoughthecostofTENGincludesbothC andN ,
lstsq it
Input: θ ,∆t,T
bothofthetermscanbesignificantlysmallerthanthosein
t=0
t←0 TDVPandOBTIduetothefollowingreasons.
whilet<T do In TDVP, the quality of the least square solution directly
u (x)←uˆ (x)+∆tLuˆ (x)
target θt θt correspondstotheaccuracy;thereforetheleastsquareequa-
θ ←TENG stepper(θ ,u )
t+∆t t target tion must be solved with high accuracy and thus require
t←t+∆t
highcost. Evenwithsparseupdate(Berman&Peherstor-
endwhile fer,2023),onemaynotbeabletousetoofewparameters;
Output: θ t=T otherwise, theupdatemaybeinaccurate. Incontrast, the
leastsquareequationcanbesolvedapproximatelyinTENG
AlgorithmTENG Heun: A2nd-orderintegrationscheme withoutcompromisingaccuracy. Inthiswork, wedesign
a sparse update scheme for TENG. In each iteration, we
Input: θ ,∆t,T
t=0 randomlysub-sampleparametersandonlysolvetheleast
t←0
squareequationwithinthespaceoftheseparameters,which
whilet<T do
significantlyhelpsreducethecost.
u (x)←uˆ (x)+∆tLuˆ (x)
temp θt θt
θ ←TENG stepper(θ ,u ) OBTI,ontheotherhand,requiresalargenumberofitera-
temp t temp
∆t(cid:0) (cid:1) tionsineverytimesteptominimizethelossfunction,due
u (x)←uˆ (x)+ Luˆ (x)+Lu (x)
target θt 2 θt θtemp tothedifficultyofthenon-convexoptimizationlandscape.
θ ←TENG stepper(θ ,u )
t+∆t temp target Incontrast,inourTENGmethod,thelossvaluesdecrease
t←t+∆t
toclosetomachineprecisionwithonlyO(1)iterations(see
endwhile
Sec.5.2andAppendixBfordetail). Inpractice,weobserve
Output: θ
t=T that TENG is able to improve the accuracy by orders of
magnitude while keeping a similar computational cost to
TDVPandOBTI(alsoseeAppendixB).
reducingthecostandimprovingnumericalstability.
TheerrorofTENGisingeneraldeterminedby(i)theex-
Connection to natural gradient. We note that the algo- pressivity of the neural network (ii) the optimization al-
rithmoutlinedinSubroutineTENG steppercanberefor-
gorithm (iii) the time integration scheme. Based on the
mulatedusingtheconventionalHilbertnaturalgradientin universalapproximationtheorem,withaproperchoiceof
theform neural network, it is likely that the neural network is suf-
ficiently powerful to represent the underlying solution at
∆θ i =−α(cid:88) G−1(θ) i,j∂L(uˆ θ ∂, θu target) (x), (7) everytimestep;thustheerrorfrom(i)issmallingeneral.
j j OurTENGalgorithmisabletoachievelossvaluesclose
to machine precision at every time step; therefore the er-
with G(θ) the Hilbert gram matrix (see Appendix A for
ror from (ii) error is also small. Given sufficiently small
detail). However, solving least square equations is more
errorsin(i)and(ii),factor(iii)dominatestheconvergence
stable, withtheaddedflexibilityofchoosingleastsquare
property of TENG. At the same time, higher-order time-
solvers. Therefore, we use the formulation in Subrou-
integrationschemescanbeintegratedwithTENG,inwhich
tineTENG stepperforpracticalimplementation.
casetheerrorfrom(iii)followsthestandardnumericalanal-
Alternatively, Subroutine TENG stepper can also be ysis results for solving differential equations. From the
viewed as a generalized Gauss–Newton method. There- aboveperspectives,wefurthercontrastTENGwithother
fore,TENGcanalsobeinterpretedastheabbreviationof algorithms. While TDVP does not have an optimization
time-evolvingNewton–Gauss(alsoseeAppendixA). error, the projection step already introduces some errors,
whichcanbeseverefornonzerotimestepsizesandwhen
theleastsquareequationinEq.(2)islowrank(Berman&
4.2.ComplexityandErrorAnalysis
Peherstorfer,2023). ForOBTI,theerrorfrom(ii)canbe
ThecomputationalcomplexityofTENGfromt = 0toT large,resultinginpoorperformances,inadditiontothelack
isO(C lstsqN itT/∆t),whereC lstsq =O(N sN p2)(withN s ofefficienthigher-ordertime-integrationschemesinprior
thenumberofsamplesandN p thenumberofparameters) works.
isthecostofsolvingtheleastsquareequationineachitera-
TENGalsopermitserrorestimationbasedonthedecompo-
tion,N isthenumberofiterationsineachtimestep,and
it
sitionoferrorsabove. Below,weoutlinetheerrorestima-
T/∆tisthenumberofphysicaltimesteps. Incomparison,
thecomputationalcomplexityofTDVPisO(C′ T/∆t) tionforTENG-Euler. ErrorsforTENGwithhigher-order
andthecomputationalcomplexityofOBTIisO(l Nsts ′q
T/∆t).
integrationmethodscanbeestimatedanalogously.
it
5TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Let ε (t) be the Lp-error between the TENG-Euler solu- cangeneratesharpgradientsthatpropagateovertime,es-
p
tion and the exact solution at time t, εEE(t) between the pecially for small ν, which can be challenging to solve.
p
exactsolutionandthesolutionevolvedexactlyaccording SimilartotheAllen–Cahnequation,Burgers’equationdoes
to the Euler’s method, εTE(t) between the TENG-Euler not have a general analytical solution either. Therefore,
p
and the solution evolved exactly according to the Euler’s wealsousethespectralmethod(Canutoetal.,2007)solu-
method,r(·,t)theresidualfunctionaftertheTENG-Euler tionasaproxyoftheexactsolutionasthereference(see
optimizationofthetimestepatt,andG :=1+∆tL. AppendixDfordetail).
Theorem 4.2. The error ε p(t) is bounded by εE pE(t) + PDE domain, boundary, and initial condition. For all
εTE(t), where εEE(t) is an order O(∆t) quantity, and threeequations,wefirstbenchmarkontwospatialdimen-
p p
(cid:13) (cid:13)
εTE(t)=(cid:13)(cid:80)t/∆t−1Gnr(·,t−n∆t)(cid:13)
.
sionsinthedomainX = [0,2π)×[0,2π)andT = [0,4],
p (cid:13) n=0 (cid:13) Lp(X) withperiodicboundaryconditionandthefollowinginitial
condition
Proof. SeeAppendixTheoremA.2. 1 (cid:18) (cid:16) (cid:17)
u (x ,x )= exp 3sin(x )+sin(x )
0 1 2 100 1 2
5.Experiments
(cid:16) (cid:17)
+exp −3sin(x )+sin(x )
1 2
5.1.EquationsandSetup
(cid:16) (cid:17)
−exp 3sin(x )−sin(x )
Heat equation. The first example we choose is the two- 1 2
dimensionalisotropicheatequation (cid:16) (cid:17)(cid:19)
−exp −3sin(x )−sin(x )
1 2
∂u (cid:18) ∂2u ∂2u(cid:19) (11)
=ν + (8)
∂t ∂x2 ∂x2 This initial condition is anisotropic, contains peaks and
1 2
valleys at four different locations, and consists of many
with a diffusivity constant ν = 1/10. The heat equation frequenciesbesidesthelowestfrequency,whichcanresult
describesthephysicalprocessofheatfloworparticlediffu- inchallengingdynamicsforvariousPDEs.
sioninspace. Sinceitpermitsananalyticalsolutioninthe
FortheHeatequation,weinadditionconsiderachalleng-
frequencydomain,theheatequationisanidealtestbedfor
ingthree-dimensionalbenchmark,whereweagainchoose
benchmarking(seeAppendixDfordetails).
periodicboundaryconditionsinthedomainX =[0,2π)3
Allen–Cahnequation. WealsoconsiderAllen–Cahnequa- andT =[0,8]. Theinitialconditionischosentobeacom-
tion bination of sinusoidal terms in the following form so the
∂u (cid:18) ∂2u ∂2u(cid:19)
=ν + +u−u3 (9) exactsolutioncanbeanalyticallycalculated.
∂t ∂x2 ∂x2
1 2
u (x ,x ,x )=A
0 1 2 3 000
withadiffusivityconstantν =1/200,whichisareaction-
2 2 2 3
diffusion model that describes the process of phase sep- (cid:88) (cid:88) (cid:88) (cid:89)
+ A cos(k x )
aration. The Allen–Cahn equation is nonlinear and does k1k2k3 i i
notpermitanalyticalsolutionsingeneral. Inaddition,its
k1=1k2=1k3=1 i=1
2 2 2 3
solutionusuallyinvolvessharpboundariesandcanbechal- (cid:88) (cid:88) (cid:88) (cid:89)
+ B sin(k x ),
lenging to solve numerically. As a benchmark, we solve k1k2k3 i i
it using a spectral method (Canuto et al., 2007) (see Ap-
k1=1k2=1k3=1 i=1
(12)
pendixDfordetail)andconsideritssolutiontobeaproxy
wherethecoefficientsarerandomlychosen(seeAppendixC
oftheexactsolutionasthereference.
for coefficients used in this work). In Appendix C, we
Burgers’equation. Wefurtherbenchmarkourmethodon also explore the heat equation on a 2D disk X = D 2 =
theviscousBurgers’equation {(x 1,x 1):x2 1+x2 2 ≤1}.
∂u (cid:18) ∂2u ∂2u(cid:19) (cid:18) ∂u ∂u (cid:19) For the Burgers’ equation, cases with unequal domains
=ν + −u + (10) and additional initial conditions are also explored in Ap-
∂t ∂x2 ∂x2 ∂x ∂x
1 2 1 2 pendixC.
withadiffusivity(viscosity)constantν = 1/100. InAp- Baselines. WhileTENGissequential-in-time,ourbench-
pendixB,wealsoexplorecaseswithsmallerν. Burgers’ marksincludebothsequential-in-time(TDVPandOBTI)
equationisaconvection-diffusionequationthatdescribes andglobal-in-time(PINN)methods. ForTDVP,wechoose
phenomenainvariousareas,suchasfluidmechanics,nonlin- the recently proposed sparse update method (Berman &
earacoustics,gasdynamics,andtrafficflow. Thisequation Peherstorfer,2023),whichhasbeenshowntooutperform
6TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Heat Equation 2D Heat Equation 3D Allen Cahn Equation Burgers' Equation
TENG-Euler (Ours) TENG-Heun (Ours) TDVP-RK4 OBTI-Adam OBTI-LBFGS PINN-ENGD PINN-BFGS
100
10 2
10 4
10 6
10 8
0 1 2 3 4 0 2 4 6 8 0 1 2 3 4 0 1 2 3 4
Time Time Time Time
Figure2.Benchmark of TENG, in terms of relative L2-error as a function of time, against various algorithms on two- and three-
dimensionalheatequations,Allen–CahnequationandBurgers’equation.Allsequential-in-timemethodsusethesametimestepsize
∆t=0.005forheatandAllen–Cahnequationsand∆t=0.001forBurgers’equation.
previousfullupdatemethods. Inaddition,weusethesame 5.2.Results
fourth-orderRunge–Kuttaintegrationscheme.FortheOBTI
Benchmarkagainstothermethods. Westartthebench-
method,wechoosethestandardEuler’sintegrationscheme
withL(uˆ ,u ) = ∥uˆ −u ∥2 (thesameloss mark of our method against other methods described in
θ target θ target L2(X) Sec.5.1intermsofboththerelativeL2-error(Eq.(13))as
asTENG).BothAdamandL-BFGSoptimizersareusedas
afunctionoftime(Fig.2)andtheglobalrelativeL2-error
benchmarks. Forallsequential-in-timemethods,weusethe
(Eq.(14))integratedoveralltime(Table1).
sametimestep∆t = 5×10−3 fortheheatequation. For
Allen–CahnequationandBurgers’equation,wefirstcom-
0.4
pareallsequential-in-timemethodswith∆t = 5×10−3
and∆t=1×10−3respectively,beforeanalyzingtheeffect 0
ofvarious∆t. Inaddition,Allsequential-in-timemethods 0.4
sharethesameneuralnetworkarchitectureandinitialpa- 0.4
rametersatt = 0. ForPINN,wetestbothBFGSandthe 0
recentlyproposedENGDoptimizer. SinceRef.(Mu¨ller& 0.4
Zeinhofer, 2023) did not provide the implementation for 5×10 6
Allen–CahnequationandBurgers’equation,weomitthe 0
benchmarkofENGDoptimizerforthetwoequations. We 5×10 6
useanetworkarchitecturesimilartoRef.(Mu¨ller&Zein- T=0 T=1 T=2 T=3 T=4
hofer,2023)(seeAppendixEfordetail).
Figure3.Referencesolution,TENGsolution,andthedifference
Errormetric. Weconsiderthefollowingtwoerrormetrics:
betweenthemforBurgers’equation. Thereferencesolutionis
generatedusingthespectralmethod,andtheTENGsolutionshown
1. relativeL2-errorateachtimestep hereusestheTENG-Heunmethodwith∆t=0.001.
∥uˆ(·,t)−u (·,t)∥ Inallcases, ourTENG-Heunmethodachievesresultsor-
reference L2(X)
ε(t)= , (13) dersofmagnitudebettercomparedtoothermethods. Upon
∥u (·,t)∥
reference L2(X)
closerinspection,ourTENGmethodwithEuler’sintegra-
tion scheme is already comparable to or better than the
2. globalrelativeL2-errorintegratedoveralltimesteps
TDVPmethodwiththeRK4integrationscheme. Inaddi-
tion, TENG-Euler is significantly better than OBTI with
∥uˆ(·,·)−u (·,·)∥
ε = reference L2(X×T) , (14) bothAdamandL-BFGSoptimizers,bothofwhichusethe
g ∥u (·,·)∥
reference L2(X×T) sameintegrationscheme. InFig.3,Weshowthedifference
betweenTENG-Heunandthereferencesolutionbyplotting
whereu referstotheanalyticalsolutionfortheheat the function evolution over time. It can be seen that our
reference
equation,andthespectralmethodsolutionforAllen–Cahn method traces closely with the reference solution with a
andBurgers’equation(seeAppendixDfordetail). tinydeviationontheorderofO(10−6). InAppendixB,we
7
rorrE-2L
evitaleR
u
u
u
u
tcaxE
GNET
tcaxE
GNETTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
GlobalRelativeL2-Error↓
Method
Heat(2D) Heat(3D) Allen–Cahn Burgers’
TENG-Euler(Ours) 3.006×10−4 3.664 ×10−4 1.249 ×10−3 3.598 ×10−4
TENG-Heun(Ours) 1.588×10−6 1.139×10−5 6.187×10−6 2.643×10−6
TDVP-RK4 3.279×10−4 3.841×10−3 1.258×10−3 2.437×10−3
OBTI-Adam 1.391×10−2 – 4.966×10−2 1.696×10−1
OBTI-LBFGS 6.586×10−3 8.743×10−2 4.180×10−2 1.047×10−1
PINN-ENGD 1.403×10−5 2.846×10−3 – –
PINN-BFGS 1.150 ×10−5 1.389×10−2 5.540×10−3 6.538×10−2
Table1.BenchmarkofTENG,intermsofglobalrelativeL2-error,againstvariousalgorithmsontheheatequation,Allen–Cahnequation
andBurgers’equation.Thebestresultineachcolumnismarkedinboldfaceandthesecondbestresultismarkedinitalicfont.Here,the
same∆tasinFig.2isused.
showadditionaldetailsofruntime,andtherelationbetween InFig.5,weshowtheglobalrelativeL2-error(definedin
runtimeandperformance. Eq.(14))asafunctionoftimestepsize∆t. Itcanbeseen
fromthefigurethatwhileTENG-Euleralreadyachievesa
Convergencespeedandmachineprecisionaccuracy. We
globalrelativeL2-errorofO(10−4)forsmall∆t,usinga
further demonstrate the convergence of TENG-Euler in
higher order integration scheme significantly reduces the
Fig.4comparedtoOBTI-AdamandOBTI-LBFGS.Inasin-
errortoO(10−6). Inaddition,TENG-Heuncanmaintain
gletimestep,TENGachievesatraininglossvalue(withthe
thelowerrorevenatrelativelylarge∆t,signifyingthead-
squaredL2distanceasthelossfunction)closetomachine
vantageofourimplementationofhigher-orderintegration
precisionO(10−14)withonlyafewiterations,whileOBTI
schemes.Wenotethat,forsmalltimestepsizes,theaccumu-
canonlygettoO(10−7)lossafterafewhundrediterations.
lationofper-steperrordominates,whileforlargetimestep
Wealsoplotthefinallossofeachtimestepoptimization
sizes,thediscretizationerrorfromtheintegrationscheme
andshowthatTENGstablyreachesthemachineprecision
dominates,resultingintheTENG-Heunwiththesmallest
foralltime,whichissevenordersofmagnitudebetterthan
∆t not as good as larger ∆t. In addition, the curves for
OBTI.OurresultshaveshownthehighaccuracyofTENG
TENG-EulerandTENG-Heunhavedifferentslopes. Both
comparedtotheexistingapproaches(seeAppendixBfor
phenomenaareconsistentwithnumericalanalysisresults
additionalresults). Sincethefinallossvaluesarenearma-
fortraditionalPDEsolvers. Additionalexplorationswith
chineprecisionforalltimesteps,webelievethemainerror
TENG-RK4methodcanbefoundinAppendixB.
sourceofTENG-EulercomesfromtheEuler’sexpansion,
insteadoftheneuralnetworkapproximation. Thisisfurther
verifiedlaterduringtimeintegrationschemecomparisons. Allen Cahn Equation Burgers' Equation
TENG-Euler (Ours) TENG-Heun (Ours)
10 2
TENG-Euler (Ours) OBTI-Adam OBTI-LBFGS
104 104 10 3
106 106
10 4 108 108
1010 1010 10 5
1012 1012
1014 1014 10 6
10 2 10 3 10 2
1016 1016
1 7 50 150 300 0 1 2 3 4 Time Step Size Time Step Size
Iteration Time
Figure5.Comparison of different time integration schemes of
Figure4.TraininglossduringthetimestepatT = 1andfinal
TENGwithrespecttothetimestepsizesonAllen–Cahnequation
traininglossesforalltimestepsfortheTENG-Eulermethodand
andBurgers’equation,usingglobalrelativeL2-errorasametric.
thetwoOBTImethodsforAllen–Cahnequation.
Compare time integration schemes. We further exam- 6.DiscussionandConclusion
inetheeffectsoftimeintegrationschemesonTENGand
compareourTENG-EulerandTENG-Heunmethodswith WeintroduceTime-EvolvingNaturalGradient,anovelap-
differenttimestepsizes. proachthatgeneralizestime-dependentvariationalprinci-
8
ssoL
gniniarT
1=T
ssoL
laniF
petS
emiT
rorrE-2L
evitteleR
labolGTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
plesandoptimization-basedtimeintegration,resultingin Acknowledgements
a highly accurate and efficient PDE solver utilizing natu-
TheauthorsacknowledgesupportfromtheNationalScience
ralgradient. TENG,encompassingalgorithmslikeTENG-
Foundation under Cooperative Agreement PHY-2019786
EulerandadvancedvariantssuchasTENG-Heun,signifi-
(TheNSFAIInstituteforArtificialIntelligenceandFunda-
cantlyoutperformsexistingstate-of-the-artmethodsinac-
mentalInteractions,http://iaifi.org/). Thismate-
curacy,achievingmachineprecisioninsolvingarangeof
rialisbaseduponworksupportedbytheU.S.Departmentof
PDEs. Forfuturework,itwouldbeinterestingtoexplore
Energy,OfficeofScience,NationalQuantumInformation
theapplicationofTENGinmorediverseandcomplexreal-
ScienceResearchCenters,Co-designCenterforQuantum
worldscenarios,particularlyinareaswheretraditionalPDE
Advantage(C2QA)undercontractnumberDE-SC0012704.
solutions are currently unfeasible. While this work is fo-
Thismaterialisalsoinpartbaseduponworksupportedby
cusedontwo-andthree-dimensional(spatial)scalar-valued
theAirForceOfficeofScientificResearchundertheaward
PDEswithperiodicboundaryconditions,thesamemethod
numberFA9550-21-1-0317. Theresearchwassponsoredby
can be considered for generalizing to vector-valued PDE
theUnitedStatesAirForceResearchLaboratoryandtheDe-
inothernumbersofdimensions,andotherboundarycon-
partmentoftheAirForceArtificialIntelligenceAccelerator
ditions, such as the Dirichlet boundary condition or the
andwasaccomplishedunderCooperativeAgreementNum-
Neumannboundarycondition. Itwillalsobeimportantto
berFA8750-19-2-1000. TheauthorsacknowledgetheMIT
developTENGforbroaderclassesofPDEsbesidesinitial
SuperCloudandLincolnLaboratorySupercomputingCen-
value problems with applications to nonlinear and multi-
terforproviding(HPC,database,consultation)resources
scalephysicsPDEsinvariousdomains.AdvancingTENG’s
thathavecontributedtotheresearchresultsreportedwithin
integrationwithcutting-edgemachinelearningarchitectures
thispaper. Somecomputationsinthispaperwererunonthe
and optimizing its performance for large-scale computa-
FASRCclustersupportedbytheFASDivisionofScience
tionaltaskswillbeavitalareaofresearchforcomputational
ResearchComputingGroupatHarvardUniversity.
scienceandengineering.
BroaderImpact
Through the advancement of the Time-Evolving Natural
Gradient (TENG), which solves partial differential equa-
tions (PDEs) with enhanced accuracy and efficiency, our
workexhibitsbroaderimpactspansmultipledisciplines,in-
cludingbutnotlimitedto,climatemodeling,fluiddynamics,
materialsscience,andbiomedicalengineering. Whilethe
primarygoalofthisworkistopushforwardcomputational
techniques within the field of machine learning, it inher-
entlycarriesthepotentialforsignificantsocietalbenefits,
suchasimprovedenvironmentalforecastingmodels,more
efficientengineeringdesigns, andadvancementsinmedi-
caltechnology. Ethically,thedeploymentofTENGshould
beapproachedwithconsiderationtoensurethatenhanced
computationalcapabilitiestranslateintopositiveoutcomes
without unintended consequences given its accuracy and
reliabilityinreal-worldapplications. Therearenospecific
ethical concerns that we feel must be highlighted at this
stage;however,weacknowledgetheimportanceofongoing
evaluation of the societal and ethical implications as this
technology is applied. This acknowledgment aligns with
ourcommitmenttoresponsibleresearchandinnovation,un-
derstandingthatthetruevalueofadvancementsinmachine
learning is realized through their contribution to societal
progressandwell-being.
9TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
References Han,J.,Jentzen,A.,etal. Deeplearning-basednumerical
methodsforhigh-dimensionalparabolicpartialdifferen-
Amari,S.-I. Naturalgradientworksefficientlyinlearning.
tialequationsandbackwardstochasticdifferentialequa-
Neuralcomputation,10(2):251–276,1998.
tions. Communicationsinmathematicsandstatistics,5
Berman,J.andPeherstorfer,B. Randomizedsparseneural (4):349–380,2017.
galerkin schemes for solving evolution equations with
Han,J.,Jentzen,A.,andE,W. Solvinghigh-dimensional
deepnetworks. InThirty-seventhConferenceonNeural
partial differential equations using deep learning. Pro-
InformationProcessingSystems,2023.
ceedingsoftheNationalAcademyofSciences,115(34):
8505–8510,2018.
Canuto, C., Hussaini, M. Y., Quarteroni, A., and Zang,
T.A. Spectralmethods: evolutiontocomplexgeometries
Kakade,S.M.Anaturalpolicygradient.Advancesinneural
andapplicationstofluiddynamics. SpringerScience&
informationprocessingsystems,14,2001.
BusinessMedia,2007.
Koch,O.andLubich,C. Dynamicallow-rankapproxima-
Carleo,G.andTroyer,M. Solvingthequantummany-body tion. SIAMJournalonMatrixAnalysisandApplications,
problem with artificial neural networks. Science, 355 29(2):434–454,2007.
(6325):602–606,2017.
Kochkov,D.andClark,B.K.Variationaloptimizationinthe
Chen,H.,Wu,R.,Grinspun,E.,Zheng,C.,andChen,P.Y. aiera: Computationalgraphstatesandsupervisedwave-
Implicitneuralspatialrepresentationsfortime-dependent functionoptimization. arXivpreprintarXiv:1811.12423,
pdes. InInternationalConferenceonMachineLearning, 2018.
pp.5162–5177.PMLR,2023a.
Langley,P.Craftingpapersonmachinelearning.InLangley,
Chen,R.T.,Rubanova,Y.,Bettencourt,J.,andDuvenaud, P.(ed.),Proceedingsofthe17thInternationalConference
D.K. Neuralordinarydifferentialequations. Advances onMachineLearning(ICML2000),pp.1207–1216,Stan-
inneuralinformationprocessingsystems,31,2018. ford,CA,2000.MorganKaufmann.
Chen,Z.,Luo,D.,Hu,K.,andClark,B.K.Simulating2+1d Li, Z., Kovachki, N., Azizzadenesheli, K., Liu, B., Bhat-
latticequantumelectrodynamicsatfinitedensitywithneu- tacharya, K., Stuart, A., and Anandkumar, A. Fourier
ralflowwavefunctions.arXivpreprintarXiv:2212.06835, neuraloperatorforparametricpartialdifferentialequa-
2022. tions. arXivpreprintarXiv:2010.08895,2020.
Chen,Z.,Newhouse,L.,Chen,E.,Luo,D.,andSoljacic,M. Long,Z.,Lu,Y.,Ma,X.,andDong,B. Pde-net: Learning
Antn:Bridgingautoregressiveneuralnetworksandtensor pdesfromdata. InInternationalconferenceonmachine
networks for quantum many-body simulation. In Oh, learning,pp.3208–3216.PMLR,2018.
A.,Neumann,T.,Globerson,A.,Saenko,K.,Hardt,M.,
Lu,L.,Jin,P.,andKarniadakis,G.E. Deeponet: Learning
and Levine, S. (eds.), Advances in Neural Information
nonlinearoperatorsforidentifyingdifferentialequations
Processing Systems, volume 36, pp. 450–476. Curran
basedontheuniversalapproximationtheoremofopera-
Associates,Inc.,2023b.
tors. arXivpreprintarXiv:1910.03193,2019.
Dirac,P.A. Noteonexchangephenomenainthethomas
Luo, D., Chen, Z., Carrasquilla, J., andClark, B.K. Au-
atom. In Mathematical proceedings of the Cambridge
toregressiveneuralnetworkforsimulatingopenquantum
philosophical society, volume 26, pp. 376–385. Cam-
systemsviaaprobabilisticformulation. Physicalreview
bridgeUniversityPress,1930.
letters,128(9):090501,2022.
Donatella, K., Denis, Z., Le Boite´, A., and Ciuti, C. Dy- Luo,D.,Chen,Z.,Hu,K.,Zhao,Z.,Hur,V.M.,andClark,
namicswithautoregressiveneuralquantumstates: Ap- B.K.Gauge-invariantandanyonic-symmetricautoregres-
plication to critical quench dynamics. Physical Re- siveneuralnetworkforquantumlatticemodels. Physical
view A, 108(2), August 2023. ISSN 2469-9934. doi: ReviewResearch,5(1):013216,2023.
10.1103/physreva.108.022210.
Mu¨ller, J. and Zeinhofer, M. Achieving high accuracy
Du, Y. and Zaki, T. A. Evolutional deep neural network. with pinns via energy natural gradient descent. In In-
PhysicalReviewE,104(4),October2021. ISSN2470- ternationalConferenceonMachineLearning,pp.25471–
0053. doi: 10.1103/physreve.104.045303. 25485.PMLR,2023.
Gutie´rrez,I.L.andMendl,C.B. Realtimeevolutionwith Pascanu,R.andBengio,Y. Revisitingnaturalgradientfor
neural-networkquantumstates. Quantum,6:627,2022. deepnetworks. arXivpreprintarXiv:1301.3584,2013.
10TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Peters,J.,Vijayakumar,S.,andSchaal,S. Reinforcement
learning for humanoid robotics. In Proceedings of the
thirdIEEE-RASinternationalconferenceonhumanoid
robots,pp.1–20,2003.
Pfaff, T., Fortunato, M., Sanchez-Gonzalez, A., and
Battaglia, P. W. Learning mesh-based simulation with
graphnetworks. arXivpreprintarXiv:2010.03409,2020.
Raissi,M.,Perdikaris,P.,andKarniadakis,G.E. Physics-
informedneuralnetworks:Adeeplearningframeworkfor
solvingforwardandinverseproblemsinvolvingnonlinear
partialdifferentialequations. JournalofComputational
physics,378:686–707,2019.
Sanchez-Gonzalez, A., Godwin, J., Pfaff, T., Ying, R.,
Leskovec, J., and Battaglia, P. Learning to simulate
complexphysicswithgraphnetworks. InInternational
conferenceonmachinelearning,pp.8459–8468.PMLR,
2020.
Sirignano,J.andSpiliopoulos,K. Dgm: Adeeplearningal-
gorithmforsolvingpartialdifferentialequations. Journal
ofcomputationalphysics,375:1339–1364,2018.
Stokes,J.,Izaac,J.,Killoran,N.,andCarleo,G. Quantum
naturalgradient. Quantum,4:269,2020.
Wang,J.,Chen,Z.,Luo,D.,Zhao,Z.,Hur,V.M.,andClark,
B. K. Spacetime neural network for high dimensional
quantum dynamics. arXiv preprint arXiv:2108.02200,
2021a.
Wang,S.andPerdikaris,P. Long-timeintegrationofpara-
metricevolutionequationswithphysics-informeddeep-
onets. JournalofComputationalPhysics,475:111855,
2023.
Wang,S.,Wang,H.,andPerdikaris,P.Learningthesolution
operatorofparametricpartialdifferentialequationswith
physics-informed deeponets. Science advances, 7(40):
eabi8605,2021b.
Weinan,E.,Han,J.,andJentzen,A. Algorithmsforsolving
high dimensional pdes: from nonlinear monte carlo to
machinelearning. Nonlinearity,35(1):278,2021.
Yu,B.etal.Thedeepritzmethod:adeeplearning-basednu-
mericalalgorithmforsolvingvariationalproblems. Com-
munications in Mathematics and Statistics, 6(1):1–12,
2018.
Zhang,G.,Martens,J.,andGrosse,R.B. Fastconvergence
ofnaturalgradientdescentforover-parameterizedneural
networks. AdvancesinNeuralInformationProcessing
Systems,32,2019.
11TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
A.AdditionalTheoreticalResults
TheoremA.1. TDVPisnotreparameterizationinvariantwith∆t.
Proof. Wewillconstructanexplicitcounter-example. Forsimplicity,considerazero-dimensionalPDE(anODE)∂ u=u,
t
whosesolutionisu = u exp(t). Letuˆ = θ andvˆ = expϕ,whicharejustreparameterizationsofeachother. Inthe
0 θ ϕ
parameterspace,thetwoODEsread∂ θ =θand∂ ϕ=1. Letbothofthemevolveforadiscretetimestep∆tfromt=0,
t t
wehaveθ =θ +∆tθ andϕ =ϕ +∆t.Pluggingbackintothefunctions,uˆ =uˆ +∆tuˆ andvˆ=vˆ exp(∆t)
∆t 0 0 ∆t 0 θ∆t θ0 θ0 θ0
Itisevidentialthatthetwoparameterizationsgivedifferentsolutions.
TENGcanbereducedtoTDVPundercertainassumptions. Forsimplicity,wewillbefocusingonthefirst-orderEuler’s
method. ConsiderSubroutineTENG stepper. LetthelossfunctionL(uˆ ,u )=∥uˆ −u ∥2 andN =1.
θt target θt target L2(X) it
Forsimplicity,letu =uˆ +∆tLuˆ bethefirst-orderEulerexpansion. Then,
target θt θt
∂L
(x)=2(uˆ (x)−u (x))≡2∆tLuˆ . (A.1)
∂uˆ θt target θt
θt
Choosingα=1/2,wehave∆u=∆tLuˆ . Then,theleastsquareequationbecomes
θt
(cid:13) (cid:13)2
∆θ =argmin(cid:13)∆tLuˆ (·)−(cid:80) J ∆θ (cid:13) , (A.2)
∆θ∈RNp
(cid:13) θt j (·),j j(cid:13)
L2(X)
whichisthesameastheTDVPalgorithmwithnonzerotimestepsizes.
TENGcanbereducedtoOBTIundercertainassumptions. LetN >0. Asmentionedinthemainpaper,approximate
it
methodscanbeusedtosolvetheleastsquareequationinSubroutineTENG stepper.Here,letitssolutionbeapproximated
byasinglegradientdescent,whichgivesriseto
(cid:90) (cid:90) ∂uˆ (x) ∂L ∂L
∆θ = J ∆u(x)dx≡−α θ (x)dx=−α , (A.3)
j (x),j ∂θ ∂uˆ ∂θ
j θ j
whichreducestotheregulargradientdescentintheθ-spacewithmanyiterations.
HilbertnaturalgradientformulationofSubroutineTENG stepper. Considertheleastsquareequation
(cid:13) (cid:13)2
∆θ =argmin(cid:13)∆u(·)−(cid:80) J ∆θ (cid:13) . (A.4)
(cid:13) j (·),j j(cid:13)
∆θ∈RNp L2(X)
It’ssolutionisgivenbythenormalequationJTJ∆θ =JT∆uwhereweusethematrixnotationandomittheindices. The
solutiontothenormalequationisgivenby∆θ =(JTJ)−1JT∆u.Noticethat
(cid:90) (cid:90) ∂uˆ (x) ∂L ∂L
(JT∆u) = J ∆u(x)dx≡−α θ (x)dx=−α . (A.5)
j (x),j ∂θ ∂uˆ ∂θ
j θ j
Inaddition,
(cid:90) ∂uˆ (x)∂uˆ (x)
(JTJ) = θ θ dx≡G (θ), (A.6)
i,j ∂θ ∂θ i,j
i j
whereG(θ)istheHilbertgrammatrix(Mu¨ller&Zeinhofer,2023). Therefore, Eq.(A.4)canbewrittenastheHilbert
naturalgradientdescent
∆θ
=−α(cid:88)
G−1(θ)
∂L(uˆ θ,u target)
(x). (A.7)
i i,j ∂θ
j
j
Wenotethatwhilethesetwoformulationsaremathematicallyequivalent,theleastsquareformulationhasafewpractical
advantages. First,itallowsformorestablenumericalsolvers. Ingeneral,theHilbertgrammatrixhasaconditionnumber
twiceaslargeastheoriginalJacobianmatrix. Iftheoriginalleastsquareequationisill-conditioned,Eq.(A.7)isevenworse.
Inaddition,whentheleastsquareequationisunderdetermined,solvingtheoriginalleastsquareproblemgivestheminimum
normsolution,whereasEq.(A.7)hastobesolvedwithpseudo-inverse,whichcanbenumericallyunstableinpractice.
12TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Generalized Gauss–Newton formulation of Subroutine TENG stepper. Let the loss function be the squared L2-
distance. Definer(x):=uˆ (x)−u (x).Thederivativeoflossinfunctionspaceisgivenby
θ target
∂L
(x)=2(uˆ (x)−u (x))≡2r(x). (A.8)
∂uˆ θ target
θ
Inmatrixnotation,theiterationabovebecomes
∆θ =−α(JTJ)−1JTr. (A.9)
Whenα=1/2,thisreducestooneiterationoftheGauss–Newtonmethod. Therefore,SubroutineTENG steppercan
alsobeviewedasageneralizedGauss–Newtonmethod.
TheoremA.2. Theerrorε (t)isboundedbyεEE(t)+εTE(t),whereεEE(t)isanorderO(∆t)quantity,andεTE(t)=
p p p p p
(cid:13) (cid:13)
(cid:13)(cid:80)t/∆t−1Gnr(·,t−n∆t)(cid:13)
.
(cid:13) n=0 (cid:13)
Lp(X)
Proof. Denote D(·,t) = u(·,t) − uˆ (·,t) = (u(·,t) − uEu(·,t)) + (uEu(·,t) − uˆ (·,t)) ≡ DEE(·,t) + DTE(·,t),
θ θ
where u(·,t) is the exact solution, uEu(·,t) is the solution from Euler method, uˆ (·,t) is the TENG-Euler solu-
θ
(cid:13) (cid:13) (cid:13) (cid:13)
tion at time t. By definition, ε p(t) = ∥D(·,t)∥ Lp(X), (cid:13)u(·,t)−uEu(·,t)(cid:13)
Lp(X)
= (cid:13)DEE(·,t)(cid:13)
Lp(X)
= εEE(t),
(cid:13) (cid:13) (cid:13) (cid:13)
and (cid:13)uEu(·,t)−uˆ(·,t)(cid:13)
Lp(X)
= (cid:13)DTE(·,t)(cid:13)
Lp(X)
= εTE(t). It follows by the triangular inequality that ε p(t) ≤
εEE(t)+εTE(t). SincetheEulermethodisafirst-ordermethod,εEE(t)hasanerroroforderO(∆t).
p p p
Denotetheoptimizationerrorintimetasr(·,t),suchthatuˆ (·,t+∆t)−Guˆ (·,t)=r(·,t).ItfollowsthatuEu(·,t+∆t)−
θ θ
DTE(·,t+∆t)−G(uEu(·,t)−DTE(·,t))=r(·,t),whichimpliesthatDTE(·,t+∆t)=GDTE(·,t)−r(·,t)duetothecan-
(cid:13) (cid:13)
cellationofuEu(·,t)−GuEu(·,t)fromtheexactEulermethod.Byinduction,εTE(t)=(cid:13)(cid:80)t/∆t−1Gnr(·,t−n∆t)(cid:13)
.
p (cid:13) n=0 (cid:13)
Lp(X)
B.AdditionalExperimentalResults
Inthissection,weshowadditionalbenchmarkresults. InFig.B.1,weshowthetraininglossesofTENGandOBTImethods
duringthetimestepsatT =0.8,1.6,2.4,3.2,4.0forAllen–Cahnequation. ForEuler’sintegrationscheme,wecompare
ourTENG-EulermethodwithbothOBTI-AdamandOBTI-LBFGSalgorithmsandfindthatouralgorithmconsistently
achieveslossvaluesordersofordersofmagnitudesbetterthanOBTI,withonly7iterations. ForTENG-Heunmethod,each
timesteprequirestrainingasetofintermediateparameters. Therefore,eachfigureincludestwocurves. Asshowninthe
figure,allstagesconvergetomachineprecisionwithinasmallnumberofiterations.
InFig.B.2B.3andB.4,weshowthedensityplotsforthetwo-dimensionalheatequation,Burgers’equationandAllen–Cahn
equation. Ineachfigure,weplotthereferencesolution(seeAppendixDfordetailsonobtainingthereferencesolution),
theTENG-Heunsolution,theTDVP-RK4solution,theOBTI-LBFGSsolutionandthePINN-BFGSsolution,andtheir
differencetothereferencesolution. Inallcases,theTENG-Heunsolutioncloselytracksthereferencesolution,witha
maximumerroroforderO(10−6),whereassolutionsgeneratedbyothermethodscanhaverelativelylargersolutions.
InFig.B.5,weplottheglobalrelativeL2-errorofPINNduringtraining. WeshowbothPINN-ENGDandPINN-BFGSfor
theheatequation,andPINN-BFGSforAllen–CahnequationandBurgers’equation. WeobservethatwhilePINN-ENGD
converges very quickly on the heat equation, PINN-BFGS eventually surpases PINN-ENGD. In addition, Allen–Cahn
equationandBurgers’equationappeartobesignificantlymorechallengingforPINN,whereitfindsdifficultyoptimizing
theerrortobelowO(10−2).
InFig.B.6,wefurtherexploretheadvantageofhigher-orderintegrationschemes. Inparticular,weplottherelativeL2-error
asafunctionoftimeforTENG-Euler,TENG-Heun,andTENG-RK4.Fortheheatequation,TENG-RK4failstosignificantly
surpassTENG-Heun,whichcouldbeattributedtotheerroraccumulationundersmall∆tinthiscase. Fortheothertwo
equations,weexplorelarger∆tandfindthatTENG-RK4isabletoachievesmallerrors,whileTENG-Heun’sperformance
startstodeteriorate.
InTable.B.1,wereporttheruntimefortherunsinFig.2. OurTENGmethodssignificantlyimprovethesimulationaccuracy
withasimilarruntimetootheralgorithms. WenotethatTENG-HeunisroughlytwiceascostlyasTENG-Eulerduetothe
13TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
T=0.8 T=1.6 T=2.4 T=3.2 T=4.0
10 4
10 7
TENG-Euler (Ours) TENG-Euler (Ours) TENG-Euler (Ours) TENG-Euler (Ours) TENG-Euler (Ours)
10 10 O OB BT TI I- -A LBd Fa Gm S O OB BT TI I- -A LBd Fa Gm S O OB BT TI I- -A LBd Fa Gm S O OB BT TI I- -A LBd Fa Gm S O OB BT TI I- -A LBd Fa Gm S
10 13
10 16
1 7 50 150 300 1 7 50 150 300 1 7 50 150 300 1 7 50 150 300 1 7 50 150 300
Iteration Iteration Iteration Iteration Iteration
10 4 TENG-Heun (Ours) Stage 1 TENG-Heun (Ours) Stage 1 TENG-Heun (Ours) Stage 1 TENG-Heun (Ours) Stage 1 TENG-Heun (Ours) Stage 1
TENG-Heun (Ours) Stage 2 TENG-Heun (Ours) Stage 2 TENG-Heun (Ours) Stage 2 TENG-Heun (Ours) Stage 2 TENG-Heun (Ours) Stage 2
10 7
10 10
10 13
10 16
1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7 1 3 5 7
Iteration Iteration Iteration Iteration Iteration
FigureB.1.TraininglossduringmanytimestepsforTENG-Euler,TENG-Heun,andthetwoOBTImethodsforAllen–Cahnequation
withatimestepsize∆t=0.005.TENG-Heunmethodrequirestwotrainingstages,oneforθ ,andtheotherforθ .Therefore,
temp t+∆t
eachfigurecontainstwocurves.
Heat Equation
0.4
0.2
0.0
0.2
0.4
T=0 T=1 T=2 T=3 T=4
0.25 0.25
0.00 0.00
0.25 0.25
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.0000005 0.0001
0.0000000 0.0000
0.0000005 0.0001
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.25 0.25
0.00 0.00
0.25 0.25
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.0025 0.00001
0.0000 0.00000
0.0025 0.00001
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
FigureB.2.Exact,TENG,TDVP,OBTI,andPINNsolutionsandtheirdifferencesfromthereferencesolutionforthetwo-dimensional
heatequation.Thereferencesolutionisgeneratedusingtheanalyticalsolution,theTENGsolutionshownhereusestheTENG-Heun
method,theOBTIshownhereusestheOBTI-LBFGSmethod,andthePINNshownhereusesthePINN-BFGSmethod.Theerrorofour
TENGmethodisordersofmagnitudesmallerthanothermethods.
two-stagetrainingprocessineachtimestep. Inaddition,allsequential-in-timemethodsusesignificantlymoretimeon
14
ssoL
gniniarT
ssoL
gniniarT
petS
emiT
gnirud
petS
emiT
gnirud
tcaxEu
GNETu
tcaxEu
GNETu
ITBOu
tcaxEu
ITBOu
PVDTu
tcaxEu
PVDTu
NNIPu
tcaxEu
NNIPuTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Burgers' Equation
0.4
0.2
0.0
0.2
0.4
T=0 T=1 T=2 T=3 T=4
0.25 0.25
0.00 0.00
0.25 0.25
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.000005 0.0025
0.000000 0.0000
0.000005 0.0025
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.25 0.25
0.00 0.00
0.25 0.25
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.1 0.1
0.0 0.0
0.1 0.1
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
FigureB.3.Exact,TENG,TDVP,OBTIandPINNsolutionsandtheirdifferencesfromthereferencesolutionforBurgers’equation.The
referencesolutionisgeneratedusingthespectralmethod,theTENGsolutionshownhereusestheTENG-Heunmethod,theOBTIshown
hereusestheOBTI-LBFGSmethod,andthePINNshownhereusesthePINN-BFGSmethod.TheerrorofourTENGmethodisordersof
magnitudesmallerthanothermethods.
Allen Cahn Equation
0.4
0.2
0.0
0.2
0.4
T=0 T=1 T=2 T=3 T=4
0.25 0.25
0.00 0.00
0.25 0.25
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.000025 0.005
0.000000 0.000
0.000025 0.005
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.25 0.25
0.00 0.00
0.25 0.25
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
0.02
0.25
0.00 0.00
0.25
0.02
T=0 T=1 T=2 T=3 T=4 T=0 T=1 T=2 T=3 T=4
FigureB.4.Exact,TENG,TDVP,OBTIandPINNsolutionsandtheirdifferencesfromthereferencesolutionforAllen–Cahnequation.
Thereferencesolutionisgeneratedusingthespectralmethod,theTENGsolutionshownhereusestheTENG-Heunmethod,theOBTI
shownhereusestheOBTI-LBFGSmethod,andthePINNshownhereusesthePINN-BFGSmethod.TheerrorofourTENGmethodis
ordersofmagnitudesmallerthanothermethods.
15
tcaxEu
GNETu
tcaxEu
GNETu
ITBOu
tcaxEu
ITBOu
tcaxEu
GNETu
tcaxEu
GNETu
ITBOu
tcaxEu
ITBOu
PVDTu
tcaxEu
PVDTu
NNIPu
tcaxEu
NNIPu
PVDTu
tcaxEu
PVDTu
NNIPu
tcaxEu
NNIPuTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
Heat Equation Heat Equation Allen Cahn Equation Burgers' Equation
101
PINN-ENGD PINN-BFGS PINN-BFGS PINN-BFGS
100
10 1
10 2
10 3
10 4
0 2000 4000 0 50000 100000 0 50000 100000 0 50000 100000
Iteration Iteration Iteration Iteration
FigureB.5.GlobalrelativeL2-errorforPINNasafunctionoftrainingiterationsforthetwo-dimensionalheatequation,Allen–Cahn
equation,andBurgers’equation.
Heat Equation Allen Cahn Equation Burgers' Equation
100 100 100
TENG-Euler TENG-Euler TENG-Euler
TENG-Heun TENG-Heun TENG-Heun
10 2 TENG-RK4 10 2 TENG-RK4 10 2 TENG-RK4
10 4 10 4 10 4
10 6 10 6 10 6
10 8 10 8 10 8
0 1 2 3 4 0 1 2 3 4 0 1 2 3 4
Time Time Time
FigureB.6.RelativeL2-errorasafunctionoftimeforthetwo-dimensionalheatequation(∆t=0.005),Allen–Cahnequation(∆t=
0.01),andBurgers’equation(∆t=0.01)forvariousintegrationmethods.
Burgers’equation,duetothereducedtimestep∆t. WhiletheresultofPINNcouldbenefitfromalongertrainingprocess
fortheBurgers’equation,webelieveitisunlikelyasshowninthetrainingdynamicsinFig.B.5. InFig.B.7,weplotthe
globalrelativeL2-errorasafunctionofruntime,withvariouschoicesofhyperparameterslistedinAppendixE.Thefigure
showsthatTENGachievessignificantlylowererrorcomparedtoothermethods,evenforlowruntimes. (Thefivepoints
withthehighesterrorsforTENGallusetheEulerintegrationscheme,wherethedominanterroristheEulerdiscretization
error.) WenotethatallexperimentsareperformedonasingleNVIDIAV100GPUwith32GBmemory. Inallcases,the
32GBmemoryissufficientforourbenchmarks.
Runtime(Hours)
Method
Heat Allen–Cahn Burgers’
TENG-Euler(Ours) 2.5 2.5 12.7
TENG-Heun(Ours) 4.1 4.2 20.9
TDVP-RK4 4.6 4.4 21.1
OBTI-Adam 3.0 3.2 19.6
OBTI-LBFGS 4.4 4.1 22.1
PINN-ENGD 1.1 – –
PINN-BFGS 2.0 2.9 3.6
TableB.1.Runtimeforvariousalgorithmsforthetwo-dimensionalheatequation,Allen–Cahnequation,andBurgers’equation.
16
rorrE-2L
evitaleR
labolG
rorrE-2L
evitaleR
rorrE-2L
evitaleR
rorrE-2L
evitaleRTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
10 2
10 3
10 4
TENG (Ours)
10 5 TDVP
OBTI
PINN
103 104 105
Runtime (Second)
FigureB.7. GlobalrelativeL2-errorasafunctionofruntimeforvariousalgorithmsundervarioushyperparameters.
C.AdditionalInitialConditionsandBenchmarks
Asmentionedinthemainpaper,forthethree-dimensionalheatequation,weconsiderainitialconditionintheform
2 2 2 (cid:32) 3 3 (cid:33)
(cid:88) (cid:88) (cid:88) (cid:89) (cid:89)
u (x ,x ,x )=A + A cos(k x )+B sin(k x ) . (C.10)
0 1 2 3 000 k1k2k3 i i k1k2k3 i i
k1=1k2=1k3=1 i=1 i=1
Here,wechoosethefollowingcoefficients: A =0.043withtherestofA ’sandA ’sshowninTableC.2.
000 k1k2k3 k1k2k3
k =1 k =2
A 1 1
k1k2k3 k =1 k =2 k =1 k =2
2 2 2 2
k =1 0.047 -0.021 0.034 -0.02
3
k =2 -0.021 -0.041 0.024 0
3
k =1 k =2
B 1 1
k1k2k3 k =1 k =2 k =1 k =2
2 2 2 2
k =1 -0.075 -0.056 -0.027 -0.008
3
k =2 0.074 -0.007 0.032 0
3
TableC.2. A ’sandB ’sfortheinitialconditionofthree-dimensionalheatequation.
k1k2k3 k1k2k3
In addition, we consider an example of the heat equation defined on a two-dimensional disk with Dirichlet boundary
condition. Here,theboundaryconditionisenforcedviaanadditionallossterminEq.(3),andtheinitialconditionisshown
below.
(cid:18)
1 1 1 1
u (r,θ)= Z (r,θ)− Z (r,θ)+ Z (r,θ)− Z (r,θ)
0 4 01 4 02 16 03 64 04
(C.11)
(cid:19)
1 1 1
+Z (r,θ)− Z (r,θ)+ Z (r,θ)− Z (r,θ)+Z (r,θ)+Z (r,θ)+Z (r,θ) ,
11 2 12 4 13 8 14 21 31 41
whererandθisthepolarcoordinatevariablesandZ representthediskharmonicsdefinedas
mn
Z (r,θ)=J (λ r)cos(mθ) (C.12)
mn m nm
withJ themthBesselfunctionandλ thenthzeroofthemthBesselfunction. Wenotethatwhiletheanalyticalsolution
m nm
is solved in the polar coordinates, all neural network based methods solve the equation and benchmark in the original
Cartesiancoordinates.
ForBurgers’equation,we,inaddition,considerbenchmarksthatincludeacasewithsmallerν =3/1000withtheoriginal
domain,boundary,andinitialconditions,andacasewithν =1/100butwithnonequaldomainX =[0,2)×[0,2π)with
17
rorrE-2L
evitaleR
labolGTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
periodicboundarycondition,andT =[0,4],andthefollowinginitialcondition.
1
u (x ,x )= exp(cos(πx −2)+sin(x −1))2. (C.13)
0 1 2 50 1 2
Heat Equation on a Disk Burgers' Equation =0.003 Burgers' Equation New u 0
TENG (Ours) TDVP OBTI PINN
100 100 100
10 2 10 2 10 2
10 4 10 4 10 4
10 6 10 6 10 6
10 8 10 8 10 8
0.0 0.5 1.0 1.5 2.0 0 1 2 3 4 0 1 2 3 4
Time Time Time
FigureC.8.RelativeL2-errorasafunctionoftimeforadditionalbenchmarks.Forallsequential-in-timemethods,wechoosetimestep
size∆t=0.005fortheheatequationand∆t=0.001forBurgers’equation.
InFig.C.8,weshowtheadditionalbenchmarksfortheaforementionedexamples. Here,TENGreferstotheTENG-Heun
method,OBTIreferstotheOBTI-LBFGSmethod,andPINNreferstothePINN-BFGSmethod. Wenotethatfortheheat
equationonadiskwithDirichletboundarycondition,anadditionalboundarytermisincludedinthelossfunctiondefinedin
Eq.(3)forTENGandOBTImethod. (PINNcanalsoincorporatethisboundarytermanalogously.) However,itisunclear
howtoenforcetheboundaryconditioninTDVPwithoutredesigningtheneuralnetworkarchitecture;therefore,wechoose
tonotenforcetheboundaryconditionforTDVP,whichcouldbethereasonwhyTDVPperformsparticularlybadlyonthe
heatequationonadisk.
D.DetailsonObtainingReferenceSolutions
Heatequation. Asmentionedinthemainpaper,theheatequationpermitsananalyticalsolutionintermsofFourierseries.
Forexample,weshowthetwo-dimensionalcasebelow.
u(x ,x ,t)=
(cid:88) exp(cid:0) −ν(cid:0) k2+k2(cid:1) t(cid:1)
u˜ (k ,k )exp(ik x +ik x ), (D.14)
1 2 1 2 0 1 2 1 1 2 2
k1,k2
whereweomittheterms2π/P becauseinourcaseP =2π. Forthetwo-dimensionalcase,evaluatingtheanalyticalsolution
isnotpracticalsinceitisdifficulttoexpressourinitialconditioninFourierseriesanalytically,
(cid:88)
u˜ (k ,k )= u (x ,x )exp(−ik x −ik x ) (D.15)
0 1 2 0 1 2 1 1 2 2
x1,x2
not to mention calculating an infinite sum of frequencies. Therefore, we choose to evaluate the initial condition on a
2048×2048=4194304grid. Then,weusethediscreteFouriertransformtocalculatetheinitialconditionintheFourier
space,beforetruncatingthemaximumfrequency48. (Thesummationcontainsaround(2·48)2 ≈9000termsintotal). For
thethree-dimensionalcaseandthecasewherethedomainisadisk,sincetheinitialconditionisalreadydefinedintermsof
sinusoidalfunctions(orBesselfunctions),thesolutionisanalyticallycalculated.
Allen–Cahnequation.Differentfromtheheatequation,Allen–Cahnequationgenerallydoesnotpermitanalyticalsolutions.
Therefore,wesolveitusingthespectralmethodandconsiderthesolutionasaproxyfortheexactsolutionasthereference.
Here,thebasisfunctionsofthespectralmethodarechosentobethesameFourierplanewaves,sothesolutioninrealspace
canbewrittenas
(cid:88)
u(x ,x ,t)= u˜(k ,k ,t)exp(ik x +ik x ). (D.16)
1 2 1 2 1 1 2 2
k1,k2
18
rorrE-2L
evitaleR
rorrE-2L
evitaleR
rorrE-2L
evitaleRTENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
WhenswitchingfromrealspacetoFourierspace,wehave
∂u
→ik u˜ and uv →u˜◦v˜, (D.17)
∂x j
j
where◦meansconvolution. Therefore,thePDEcanberewrittenintheFourierspaceas
∂u˜
=−ν(k2+k2)u˜+u˜−u˜◦u˜◦u˜. (D.18)
∂t 1 2
Here,wechooseamaximumfrequencycut-offof128. (Noticethatthemaximumnumberoffrequenciesencounteredis
(3·2·128)2 ≈600000whencalculatingthedoubleconvolution.) Theinitialconditioniscalculatedanalogoustothecase
oftheheatequation,viaadiscreteFouriertransformonthe2048×2048=4194304grid. Then,Eq.(D.18)issolvedusing
thefourth-orderRunge–Kuttaintegrationschemewithatimestep∆t=2×10−4.
Burgers’equation. AnalogoustoAllen–Cahnequation,Burger’sequationdoesnothaveageneralanalyticalsolutioneither,
exceptinthecaseofν =0. Therefore,weusethesamespectralmethodusedtosolveAllen–Cahnequation. Noticethatthe
termu∂u/∂x =∂u2/∂x . Therefore,Burgers’equationintheFourierspacereads
j j
∂u˜ i
=−ν(k2+k2)u˜− (k +k )u˜◦u˜. (D.19)
∂t 1 2 2 1 2
Here,wechooseamaximumfrequencycut-offof192(withamaximumofaround(2·2·192)2 ≈ 600000termswhen
calculatingtheconvolution.) TheinitialconditioniscalculatedinthesamewayastheheatandAllen–Cahnequation,and
Eq.(D.19)issolvedusingthefourth-orderRunge–Kuttaintegrationschemewithatimestep∆t=1×10−4.
Accuracyofthesolutions. Ineachcase,wecarefullyverifythatthenumberofgridpoints,themaximumfrequency,and
the∆taresufficienttoobtainasolutionthatisaccuratetonearnumericalprecision,byvaryingthemovermultiplevalues
andobservingthatthesolutionconverges. WenotethatthecaseforBurgers’equationwithν =0.003ischallengingforthe
spectralmethodandthesolutionmaynotconvergeyet,whichmeanstheerrorswereportcouldbelargerthantheactual
values.
E.DetailsofNeuralNetworkArchitecturesandOptimization
AllthealgorithmsusedinthisworkareimplementedinJAXandusedoubleprecision. OurcodewillbepostedtoGitHub
subsequenttotheacceptanceofthiswork.
Neuralnetworkarchitectures. Wechoosethesamearchitectureforallsequential-in-timemethods,whichallowsafair
comparison. OurneuralnetworkarchitectureislooselybasedonRef.(Berman&Peherstorfer,2023)whichconsistsof
multiplefeedforwardlayerswithtanhactivationfunctionas
uˆ(x)=W tanh(···tanh(W periodic embed(x)+b )···)+b , (E.20)
nl 1 1 nl
wheretheperiodicembeddingfunctionisdefinedas
(cid:16)(cid:104) (cid:105)(cid:17)
(cid:80) (cid:80)
periodic embed(x)=concatenate a cos(x +ϕ )+c , a cos(x +ϕ )+c (E.21)
j j 1 j j j j 2 j j
toexplicitlyenforcetheperiodicboundaryconditionintheneuralnetwork. Here,allW,b,a,andcaretrainableparameters.
Here,wechoosen =7layersandd =40hiddendimensions(periodicembeddingvectorwithsize20foreachx ).
l h j
ForPINN,weadoptthesamearchitecturefromRef.(Mu¨ller&Zeinhofer,2023)withtheadditionofperiodicembedding.
Inaddition,weincreasethehiddendimensionto64comparedtoRef.(Mu¨ller&Zeinhofer,2023)forbetterexpressivity.
Inthecaseoftheheatequationona2Ddisk,wesimplyremovetheperiodicembeddinglayer.
Optimizationmethods. ForTENG,werandomlysub-sampletrainableparameterswhensolvingtheleastsquareproblems.
Thiscanbeviewedasaregularizationmethodwhentheoriginalleastsquareproblemisill-conditionedandcansignificantly
reducethecomputationalcost. Duringeachtimestep,werandomlysub-sample1536parametersinthefirstiterationand
sub-sample1024parametersinthesubsequentiterations. InTENG-Euler,theneuralnetworkisoptimizedfor7iterationsin
19TENG:Time-EvolvingNaturalGradientforSolvingPDEswithDeepNeuralNet
eachtimestep;inTENG-Heun,theneuralnetworkisoptimizedfor7iterationstoobtainθ ,followedby5iterationsfor
temp
θ . Wereducethenumberofiterationsinthesecondstagebecauseθ alreadygivesagoodinitializationforθ .
t+∆t temp t+∆t
ForTDVP,weusethesparseupdatemethodproposedbyRef.(Berman&Peherstorfer,2023),whichisalsoarandom
sub-sample of parameters for each TDVP step, and has been shown to significantly improve the result compared to a
full update of a smaller neural network. Here, we randomly sub-sample 2560 parameters at each time step so that the
computationalcostofTDVPateachtimesteproughlymatchesthatofTENG(overthetrainingiterationswithineachtime
step).
ForOBTI,wecompareourmethodwithboththeAdamoptimizerandtheL-BFGSoptimizer. Withineachtimestep,the
neuralnetworkisoptimizedfor300iterationswhenusingtheAdamoptimizer,and150iterationswhenusingtheL-BFGS
optimizer. The Adam optimizer uses an initial learning rate of 1×10−5 and an exponential scheduler that decays the
learningrateby1/2bytheendofthe300iterations.
Forallsequential-in-timemethods,weneedtotraintheinitialparameterstomatchtheinitialconditions. Hereweusethe
sameinitialparametersforafaircomparison. Theinitialparametersaretrainedbyfirstminimizingthelossfunction
L(uˆ θ,u 0)=∥uˆ θ−u 0∥2 L2(X)+(cid:13) (cid:13) (cid:13) (cid:13)∂ ∂u xˆ θ − ∂ ∂u x0(cid:13) (cid:13) (cid:13) (cid:13)2 +(cid:13) (cid:13) (cid:13) (cid:13)∂ ∂u xˆ θ − ∂ ∂u x0(cid:13) (cid:13) (cid:13) (cid:13)2 (E.22)
1 1 L2(X) 2 2 L2(X)
usingnaturalgradientdescent,whereweusetheleastsquareformulationasmentionedinthemainpaperand(approximately)
solvetheleastsquareproblemusingCGLSmethod,untilthelossvaluedecaysbelow1×10−7. Then,weswitchtheloss
functionto
L(uˆ ,u )=∥uˆ −u ∥2 (E.23)
θ 0 θ 0 L2(X)
andusetherandomsub-sampleversionofthenaturalgradientdescent,with1536parametersupdatedforeachiteration
untilthelossvaluedecaystonearmachineprecision(1×10−14). TheL2-norminbothstagesareintegratedona2Dgrid
of1024pointsineachdimension(around1000000pointsintotal).
ForPINN,boththeinitialconditionandthetimeevolutionareoptimizedsimultaneously;therefore,itdoesnotusethe
initialparametersmentionedabove. Inaddition,allthetimestepsofPINNareoptimizedsimultaneously,insteadofstepby
step. Fortheoptimization,wetesttheBFGSoptimizer,andtherecentlyproposedENGDoptimizer(Berman&Peherstorfer,
2023). We note that the ENGD optimizer requires custom implementation for individual PDEs. Since Ref. (Berman
&Peherstorfer,2023)didnotprovidetheimplementationforAllen–CahnequationandBurgers’equation,weomitthe
benchmarkofENGDoptimizerforthetwoequations. Wetraintheneuralnetworkfor100000iterationswhenusingthe
BFGSoptimizer,and4000iterationswhenusingtheENGDoptimizer.
ForFigB.7,theresultsincludevarioushyperperameters. Forallsequential-in-timemethods,weincludedifferenttimestep
sizes∆t=0.0016,0.0032,0.005,0.01and0.02. ForTENG,weincludeTENG-Euler,TENG-Heun,andTENG-RK4with
differentnumbersofiterations(withineachtimestep)rangingfrom2to20anddifferentnumbersofrandomlysubselected
parameters for solving least squares (within each iteration) ranging from 384 to 2048; for TDVP, we include different
numbersofrandomlysubselectedparametersforsolvingleastsquareprojectionsrangingfrom384to2560(wherewereach
thememorylimitofV100GPU);forOBTI,weincludebothOBTI-AdamandOBTI-LBFGSwithdifferentnumbersof
iterations(withineachtimestep)rangingfrom150to300;andforPINN,weusetheBFGSoptimizer,andincluderesultsof
differentnumberofiterations(globally)anddifferentneuralnetworksizes.
20