LaDiC: Are Diffusion Models Really Inferior to Autoregressive
Counterparts for Image-to-Text Generation?
YuchiWang1*, ShuhuaiRen1*, RundongGao1,LinliYao1 ,QingyanGuo2,
KaikaiAn1,JianhongBai3,XuSun1†
1 NationalKeyLaboratoryforMultimediaInformationProcessing,PekingUniversity
2TsinghuaUniversity 3ZhejiangUniversity
{wangyuchi,shuhuai_ren}@stu.pku.edu.cn xusun@pku.edu.cn
Abstract
Visual A man … frisbee <EOS>
Encoder
Diffusion models have exhibited remarkable
Text Decoder
capabilitiesintext-to-imagegeneration. How-
ever,theirperformanceinimage-to-textgener-
ation,specificallyimagecaptioning,haslagged <BOS> A man … frisbee
behindAuto-Regressive(AR)models,casting (a) Auto-Regressive model
doubtontheirapplicabilityforsuchtasks. In
this work, we revisit diffusion models, high-
lightingtheircapacityforholisticcontextmod- A man … frisbee [EOS]
…
elingandparalleldecoding. Withthesebene-
Non-AutoRegressiveText Decoder
fits,diffusionmodelscanalleviatetheinherent
limitationsofARmethods,includingtheirslow Visual …. Sentence vector/latent
inferencespeed,errorpropagation,andunidi- Encoder
rectional constraints. Furthermore, we iden- Add
Diffuser noise
tify the prior underperformance of diffusion
models stemming from the absence of an ef-
Noise …. Back&Refine
fectivelatentspaceforimage-textalignment,
and the discrepancy between continuous dif-
(b) Diffusion-based model (Ours)
fusionprocessesanddiscretetextualdata. In
response, we introduce a novel architecture, Figure 1: Inference process for image captioning.
LaDiC, which utilizes a split BERT to cre- (a) Token-by-token generation manner of AR-based
ate a dedicated latent space for captions and model. (b)Graduallydenoisinggenerationmannerof
integratesaregularizationmoduletomanage thediffusion-basedmodel(Ours).
varying text lengths. Our framework also in-
cludes a diffuser for semantic image-to-text
conversion and a Back&Refine technique to Dai et al., 2023). However, the inverse process
enhance token interactivity during inference. ofimage-to-textgenerationremainslessexplored.
LaDiC achieves state-of-the-art performance
Some pioneering efforts (Li et al., 2022b; Yuan
fordiffusion-basedmethodsontheMSCOCO
etal.,2022)haveattemptedtointegratediffusion
datasetwith38.2BLEU@4and126.2CIDEr,
modelsintotextgenerationtasks. Theymainlyfol-
demonstratingexceptionalperformancewith-
lowedthetraditionalEncoder-Decoderframework,
outpre-trainingorancillarymodules.Thisindi-
catesstrongcompetitivenesswithARmodels, utilizingthediffusionmodelasatextdecoder. Sub-
revealingthepreviouslyuntappedpotentialof sequentresearch(Heetal.,2023b;Liuetal.,2023a)
diffusionmodelsinimage-to-textgeneration.1
introducesvisualcapabilityintothisparadigmby
treatingvisualinputsasspecialtokensorencoded
1 Introduction
hiddenstates,therebyextendingtheresearchscope
totherealmofmulti-modaltasks,suchasimage-
Inrecentyears,therehasbeenasurgeofimpressive
to-textgeneration. However,theirperformancehas
applicationsofdiffusionmodelsintext-to-image
consistentlylaggedbehindthatofAuto-Regressive
generationtasks(OpenAI,2023;Podelletal.,2023;
(AR)models(Lietal.,2022a;Zhangetal.,2021;
* Equalcontribution.
Wangetal.,2022). Onlythroughintricatearchitec-
† Correspondingauthor.
ture(Luoetal.,2022)orexternaldata(Zhuetal.,
1Codereleasedathttps://github.com/wangyuchi369/
LaDiC 2022)cantheybarelyachievecomparableresults,
4202
rpA
61
]IA.sc[
1v36701.4042:viXra0.300 0.50
BLIP BLIP
0.275 Diffusion model (Ours) Diffusion model(Ours)
0.45
0.250
0.225
0.40
0.200
0.175
0.35
0.150
0.125 0.30
0.100
8 9 10 11 12 13 14 15 16 0.25 7 8 9 10 11 12 13 14 15
Generated caption length Generated caption length
(a)Inferencetimeassequencelengthincreases. (b)BLEUscoreassequencelengthincreases.
Input Image Custom Generation
• two giraffes standing on a ___ ___ ____ ____.
• two giraffes standing on a field near some trees.
• ___ __ giraffes are _______ on the ____ with ____ besides.
• two tall giraffes are standingon the grasswith trees besides.
• there are ___ ______ standing on a _____ field.
• there are two giraffes standing on a grassyfield.
• ___ ______ __ _______ ___ trees.
• two giraffes are standing near trees.
(c)LaDiC’sabilityofcustomgeneration.
Figure2: Threeadvantagesofourdiffusion-basedmodel(LaDiC)comparedtoauto-regressivemodels(BLIP).
raisingdoubtsaboutwhetherdiffusionmodelshave elscanconsidermoreholisticcontexts,mitigating
inherentlimitations,potentiallymakingthemless erroraccumulation(Heetal.,2023b). Asdepicted
suitablefortheimage-to-texttask. inFig.2b,theBLEUmetricofBLIP-generatedcap-
tionsdeclinesrapidlywithincreasingtextlength,
In this study, we aim to dispel this doubt by
whereasourdiffusion-basedmodelmaintainsper-
deeplyreexaminingthediffusion-basedimage-to-
formance. (3) Flexible Generation: AR models
text paradigm and unveiling its distinct benefits.
adheretoafixedunidirectionalgenerationmanner,
Unlike conventional AR approaches that sequen-
whereas our diffusion model demonstrates much
tially generate captions token by token (Fig. 1a),
greater flexibility. As shown in Fig. 2c, we can
diffusion-basedmodelstakeGaussiannoiseasin-
customgeneratecaptionsbasedontokensinnearly
put and iteratively denoise it under image guid-
anyposition,whichisacapabilitychallengingfor
ance to simultaneously produce the entire cap-
ARimagecaptioningmodels.
tion (Fig. 1b). This Non-AutoRegressive (NAR)
diffusion-based model exhibits three key advan- Despite the above benefits, the underperfor-
tages: (1) Parallel Decoding: Diffusion-based manceofpreviousdiffusionmodelsonimage-to-
modelsemitalltokensinparallel,significantlyre- text tasks hinders their popularity. Upon examin-
ducinginferencetimeforlengthytargetcaptions. ingthesediffusion-basedmodels,wededucethat
AsillustratedinFig.2a,theinferencetimeofAR their unsatisfactory performance primarily stems
modelslikeBLIP(Lietal.,2022a)proliferatesas fromtwofactors: (I)Semanticgapsintranslat-
text length increases, while our diffusion model ingfromimagestotexts,whichmanifestsintwo
canensurestableinferencetimeregardlessofthe dimensions: 1)thegapbetweenvisualinformation
length increase. For instance, when the caption andtextualrepresentation,and2)thegapbetween
lengthreaches16,ourmodelisapproximately3× high-leveltextsemanticsandspecificwords. The
fasterthanBLIP. (2)HolisticContextConsidera- previousparadigmattemptstosimultaneouslyad-
tion: Unliketheuni-directionalinformationflow dressthesetwogaps(illustratedbythegreydotted
ofARmodels(lefttoright),diffusion-basedmod- lineinFig.3),butthisprovesoverlychallenging,
)gmi/s(
emit
ecnerefnI
UELBSample Ours data flow Diffuser
(decoder)
Previous works’ data flow
Image
Diffuser Text decoder
encoder
Image Space Image Latent Space (cid:2336) Text Latent Space (cid:2338) Discrete Text Space (cid:2317)
Figure3:ComparisonofthepipelinebetweenourLaDiCandthatofpreviousdiffusion-basedmodels. Weintroduce
textlatentspacetoalleviatetheburdenonthediffuser.
resulting in poor performance. (II) Incompati- resultsunderscorethepotentgenerativeabilityand
bilitybetweencontinuousdiffusiontechnology immense potential of diffusion models in image-
(imagegeneration)anddiscreteinputs(textgen- to-textgeneration. Wehopethatourworkoffersa
eration). Specifically, classical continuous diffu- freshperspective,fosteringfutureresearchondif-
sionmodelsnaturallyalignwiththepixelspacebut fusionmodelsforimage-to-textgenerationoreven
struggle to transition directly to the discrete text othertext-centeredmultimodalgenerationtasks.
space. Additionally,generatedimageshaveafixed
size,whilecaptionlengthsvary,presentinganother 2 RelatedWorks
challengefordiffusionmodelsindeterminingthe
2.1 DiffusionModelsandtheirApplications
boundariesofgeneratedcaptions.
Diffusion models have recently emerged as pow-
Given these considerations, we meticulously
erfulgenerativemodels,withrepresentativefoun-
design a novel architecture LaDiC, a Latent
dational architectures such as DDPM (Ho et al.,
Diffusion-basedCaptioner,forfurtheramplifying
2020b) and DDIM (Song et al., 2020). These
thecapabilityofdiffusionmodelsinimage-to-text
methods gradually transform samples into Gaus-
generation. AsdepictedinFig.1bandFig.3(blue
siannoiseandtrainamodeltorecoverthem,pre-
line), ratherthandirectlygeneratingdiscretetext
sentingasimpleandstablelearningobjectivefor
fromimagerepresentation,wetreatthediffuseras
addressingissueslikeposteriorandmodecollapse
aninterfacetranslatingimageinformationtohigh-
thatchallengepriormodelslikeVAE(Kingmaand
level text representation (sentence latent). This
Welling,2013)andGAN(Goodfellowetal.,2014).
approachalleviatesthediffusionmodel’sburden,
Theimpressivegenerativecapabilitiesofdiffu-
enablingittoleverageitspowerfulgenerationca-
sionmodelshaveledtotheirapplicationacrossa
pabilities in high-level semantic spaces (Ramesh
spectrumoffields,includingimage (Rameshetal.,
et al., 2022). After that, we utilize a Non-Auto-
2022; Dai et al., 2023), audio (Liu et al., 2023b;
Regressive(NAR)textdecodertogeneratediscrete
Juetal.,2024),video(Blattmannetal.,2023;Bai
tokens from latent space. To address problems
et al., 2024), 3D (Poole et al., 2022; Lee et al.,
like variable length of text, we propose a post-
2024), and human avatar (He et al., 2023a; Hu
processing module including normalization and
etal.,2023),amongothers. Yet,theirapplication
reassignmentprocedures. Duringinference,wein-
totextisstillinitsinitialstate. Howtoadaptdis-
troduceaBack&Refinetechniquetoprovidemore
cretetokensintoadiffusionmodelisanongoing
interaction between tokens, thus yielding better
challenge. Existing approaches for tackling this
performance.
problemgenerallyfallintotwocategories: (1)Dis-
WeconductedexperimentsmainlyontheCOCO creteTextDiffusionModels(Austinetal.,2021;
dataset (Lin et al., 2014) to validate our model’s Reidetal.,2022;Heetal.,2022),whichmimicthe
capabilities. Remarkably,withoutpretrainingorex- diffusionprocessonthediscretespacebydirectly
ternalmodules,ourmodelachieves38.2BLEU@4 corruptingtextwith[MASK]tokens. (2)Continu-
and 126.2 CIDEr, significantly surpassing both ousTextDiffusionModels(Lietal.,2022b;Gong
diffusion-basedmethodsandtraditionalNARmod- et al., 2022; Dieleman et al., 2022; Yuan et al.,
els. Inadditiontotheuniqueadvantagesdiscussed 2022;Linetal.,2022),whichusecontinuousem-
earlier,ourmodelalsomatchestheperformanceof beddingstorepresenteachtokenandthenperform
well-establishedpretrainedARmodelsandoutper- the classical diffusion process. While these ap-
formsBLIPinimageparagraphcaptioning. These proaches demonstrate the feasibility of applyingdiffusionmodelstotextgenerationandshowcom- 3 Methodology
parabilitywithARmethods,theyarelimitedtouni-
In this section, we introduce our diffusion-based
modalrepresentationsandmayoverlookhigh-level
image captioning model, LaDiC. In § 3.1, we
overall semantics to some extent. Lovelace et al.
presenttheoverallarchitectureofLaDiC,including
(2023) explore the concept of a text latent space.
its training and inference pipeline. Subsequently,
However,itsdiffusionmodel,designedforpredict-
from§3.2to§3.4,weofferadetailedillustration.
ingBART’s(Lewisetal.,2019)hiddenstates,still
reliesonanARgenerationmechanism,whichsuf-
3.1 Overview
fersfromitsissueslikelowinferenceefficiency.
AsillustratedinFig.3,weutilizeatextencoderto
transformthediscretetextspaceC intoacontinu-
2.2 Image-to-textGeneration ous text latent space X. Subsequently, a diffuser
is trained to serve as a bridge between the image
Image-to-textgeneration,especiallytheimagecap- representationspaceV andthetextspaceX, and
tioning task, aims to describe the content of an finally, a text decoder maps the text latent codes
image in natural language. Other task variants backtothediscretetext.
include dense captioning, which illustrates each
TrainingProcedure. AsshowninFig.4,given
object in the picture (Johnson et al., 2016), and
animagev ∈ V anditscorrespondingcaptionc ∈
paragraphcaptioning,whichgeneratesadetailed,
C,weencodethecaptioncintothelatentcodex ∈
lengthyparagraph(Krauseetal.,2016)andsoon. 0
X. Subsequently,wetrainthediffusionmodelto
EarlyARapproachesforcaptioning(Karpathyand
fit the distribution of space X. Initially, various
Fei-Fei, 2017; Vinyals et al., 2014) employed an
levelsofnoise(introducedbydifferenttimestepst)
encoder-decoder architecture with a CNN (Con-
areaddedtox togenerateanoisyversionx (left
volutionalNeuralNetwork)toencodeimagesand 0 t
panel). The diffuser then functions as a denoiser,
anRNN(RecurrentNeuralNetwork)togenerate
aiming to recover x conditioned on the image v
captions. WiththeadventofTransformer(Vaswani 0
(rightpanel). Thisprocesscanberepresentedbya
et al., 2017) and large-scale pretraining methods,
v
functionf : x −→ x .
pretrainedvision-languagemodels(Lietal.,2022a; t 0
Zhangetal.,2021;Lietal.,2020;Renetal.,2023, InferenceProcedure. Duringinference,x t(t →
2021;Zhaoetal.,2023;Liuetal.,2023c)emerged ∞) is replaced with pure Gaussian noise x ∼
∞
andachievedhighperformance. N(0,I). Afterthat,x isiterativelydenoisedby
∞
v
In contrast to the unidirectional generation of the diffuser f, resulting in x ∞ −→ xˆ 0, where xˆ 0
ARmodels,NARmodelsgenerateentirecaptions represents the predicted text latent code. Finally,
in parallel. MNIC (Gao et al., 2019) introduced thedecoderconvertsthepredictedlatentcodeback
the mask token strategy, and NAIC (Guo et al., intodiscretetextcˆ∈ C.
2020) employed reinforcement learning in NAR
3.2 LatentSpaceTailoredforText
captiongeneration. AspecialclassofNARmeth-
ods,diffusion-basedmodelshasrecentlyemerged. Asdiscussedin§1,thetextlatentspaceX serves
Mostdiffusion-basedmodels(Xu,2022;Heetal., as a bridge between image space V and discrete
2023b;Liuetal.,2023a)followtheparadigmuti- text space C, significantly alleviating the burden
lized in continuous diffusion models mentioned ondiffusionmodels. Therefore,carefuldesignof
above. Additionally, Bit Diffusion (Chen et al., theX spaceisessential. Thisspaceshouldpossess
2022a)encodescaptionsintobinarybits,andDD- an appropriate semantic density to facilitate the
Cap(Zhuetal.,2022)appliesadiscretediffusion semanticconversionfromimagestotext.
modeltocaptioning. SCD-Net(Luoetal.,2022) Generally,thetextlatentspaceX isconstructed
isthestate-of-the-artdiffusion-basedmodelwith byatextencoder. Dependingontheselectionofthe
a semantic-conditional diffusion process. How- textencoder,thepreviousworkcanbedividedinto
ever,itscascadedarchitectureisrelativelycomplex two branches. The first branch (He et al., 2023b;
and requires an external retrieval module, limit- Liuetal.,2023a)usesaveryshallowencoder,e.g.,
ing its further extension. Our work reexamines a singleembedding layer ofBERT(Devlinet al.,
thediffusion-basedparadigmandproposesanovel, 2019) to convert the discrete text into a contin-
compactarchitecturewithimprovedperformance. uous form. However, this method lacks interac-Text latent space (cid:2338)
T mra oi dn ua lb el se Noisy latent (cid:2206)(cid:2202) … Sampling Initial (cid:2206)(cid:2998) Training
Inference
Frozen
modules Training &
Inference
(cid:2173) Q fou r e tr ey x tvector Back & Refine Padding
Key vector Image Timestep Tokens
(cid:2167) for image Encoder t
Value vector Add
(cid:2178) for image nA od isd e Image (cid:2204)(cid:1488)(cid:2336) (cid:2167) (cid:2178) (cid:2173)(cid:2202)(cid:2200)(cid:2183)(cid:2191)(cid:2196) (cid:2173)(cid:2202)(cid:2187)(cid:2201)(cid:2202) noise
Diffuser
Clean Latent (cid:2206)(cid:2777) …
(cid:2326)(cid:2194)(cid:2183)(cid:2202)(cid:2187)(cid:2196)(cid:2202)
Predicted latent (cid:3549)(cid:2206)(cid:2777) …
RegularizationModule
Text Decoder
Text Encoder
LM Head
Discrete text space (cid:2317)
Input(cid:2185): A man in a grassy area with a frisbee. Output(cid:2185)(cid:3548): A man in a grassy area with a frisbee.
(cid:2326)(cid:2185)(cid:2183)(cid:2198)(cid:2202)(cid:2191)(cid:2197)(cid:2196)
Figure 4: An overview of our LaDiC model. It mainly consists of the Image Encoder, Text Encoder, Diffuser,
andTextDecoder. Thediffusionprocessisdepictedontheleft,whilethedenoisingprocessisdepictedonthe
right. Initially,thecaptioncisencodedintoatextlatentx bythetextencoder. Subsequently,diffusionprocess
0
occurswithinthetextuallatentspaceX,whereadiffuseristrainedtorestorethenoisytextlatentx toitsclean
t
counterpartsxˆ ,guidedbytheassociatedimage. Finally,thedenoisedtextlatentxˆ ispassedthroughaNARtext
0 0
decodertogeneratethefinalcaptioncˆ.
tion between tokens and overall semantic model- acting on this space. We gather a subset of all
ing,whichposesachallengewhenaligningimages captions from the dataset and compute the mean
with these independent token embeddings. The and standard deviation of their corresponding la-
secondbranch(Tangetal.,2023;Xuetal.,2022) tentcodes,µˆ(x)andσˆ(x). Duringtraining,these
utilizes the entire BERT model as a text encoder, statisticsareutilizedtoregularizethefeaturespace
yieldingdenselypackedsemanticrepresentations ofBERTbyoperatingoneachsampleasfollows:
forsentences. However,comparedtosentencela- norm(x) = [x−µˆ(x)]/[σˆ(x)+ϵ].
tentwithhighinformationdensity,imagestypically
possessmuchlowerinformationdensity,character- We note that the lengths of target captions are
ized by substantial redundancy in pixel data (He variable,whichnecessitatesthediffusionmodelto
etal.,2021;Renetal.,2023). Thisdiscrepancybe- learntheendpositionofdifferentcaptionsduring
tweentheinformationdensitiesofimagesandtexts training. Consideringthecaptionswithinabatch
impedesthediffuser’sabilitytolearnimage-to-text arepaddedtomatchthesamesequencelength,we
translationeffectively.
canleveragethe[PAD]tokentodeterminethecon-
clusionofacaption. Thisentailsinitiallyrestoring
Incontrasttothesetwomethods,weopttosplit the caption with [PAD] tokens and then eliminat-
theBERTmodelinhalffromthemiddle,utilizing ingallthe[PAD]tokenstoobtainthefinalcaption.
the lower part as the text encoder and the upper However, our diffusion models operate in a text
part as the NAR text decoder. Setting the text latent space X, i.e., the feature space of the half
latent space based on the middle layer of BERT BERT model, wherein the [PAD] token’s feature
yieldsimprovedalignmentbetweenvisionandlan- isfusedwithcontextualinformation,impedingits
guage, thereby enhancing performance. In addi- restoration to a [PAD] token by the text decoder.
tion, to improve the decoder’s ability to recon- Toaddressthisissue,weextractallthespecialto-
struct the text space, we make the parameters in kenslike[CLS],[SEP],[PAD]inacaption,whose
the language model head trainable. To achieve latentwillbemessyinthespaceX,toformaset
a more standardized sentence feature space con- S. Wethenreassignanemptylatent0tothelatent
ducivetonoiseaddition,weemploynormalization ofthese specialtokens, asdemonstratedin Eq.1.
noisuffiD
DenoisingGuassian noise Difficult-to-restore tokens 2023b), this approach achieves a better modal-
Easy-to-restore tokens Restored tokens
ity fusion effect, as will be verified in the abla-
tion study. Additionally, we adapt classifier-free
(cid:2202)=(cid:2176) (cid:2202)=(cid:2176)/(cid:2779) (cid:2202)=(cid:2777)
guidance (Ho and Salimans, 2022) to this task
Denoise
to improve the alignment between text and im-
[] [] [] [] [] be boy a a shirt
age by randomly dropping some images during
Back
training. During inference, a linear combination
[] boy [] [] shirt
Refine(denoise)
a boy in blue shirt
of the conditional estimate f ϕ(x t,v,t) and un-
conditional one f (x ,∅,t) is performed: xˆ =
ϕ t 0
Figure5: IllustrationofBack&Refinetechnique. (1 + w)f ϕ(x t,v,t) − wf ϕ(x t,∅,t) where w is a
hyper-parameter.
Here, x˜(i) represents the i-th position of the final We use a two-fold loss to train the diffuser in
LaDiC. The first one is the loss L , operating
regularizedlatent. latent
withinthetextlatentspace. Thislosscalculatesthe
(cid:40)
[norm(x)](i) i ∈/ S MeanSquaredError(MSE)betweenthepredicted
x˜(i) = (1) text latent xˆ = f (x ,v,t) and the original text
0, i ∈ S 0 ϕ t
latentx . Thesecondoneisthecross-entropyloss
0
Through this technique, for short captions with L inthediscretecaptionspace. Specifically,
caption
pad tokens at the end, the diffuser can quickly letxˆi bethei-thpositionofthetextlatentandwi
0
identify this repeated pattern and easily recover bethei-thwordintheground-truthcaption. Given
theseunifiedzerovectors,implicitlylearningsen- xˆi, the probability of correctly predicting wi in
0
tence boundaries. This approach avoids the need the vocabulary is p (wi|xˆi), where θ represents
θ 0
for an additional module for predicting sentence theparametersofthediffuserandlanguagemodel
length (Zhu et al., 2022). During inference, the head in text decoder. This loss makes the output
token predicted as [PAD] can be easily erased by of the caption diffuser shrink faster, sharing the
post-processingtogeneratevariouslengthcaptions. same intuition with XE loss in (Luo et al., 2022)
andanchorlossin(Gaoetal.,2022). Meanwhile,
3.3 DiffuserMappingImagetoText
italsohelpstrainthelanguagemodelheadinthe
As shown in Fig. 3, the diffuser serves as an in- decoder. Insummary,thefinallossLis:
terface transforming the vision space V into the
text latent space X. To fit the distribution of L = L +λL
latent caption
space X by diffusion models, firstly we sample n
(cid:89) (2)
x t, the noisy ver √sion of √the latent code x
0
∈ X, = ∥f ϕ(x t,v,t)−x 0∥−λ p θ(wi|xˆi 0),
as x t|x 0 ∼ N( α¯ tx 0, 1−α¯ tI), where α¯ t = i=1
(cid:81)t
α =
(cid:81)t
(1 − β ) and β ∈ (0,1) is the
i=1 i i=1 i t
whereλisahyper-parameter.
varianceschedule. Anotablepropertyofthisset-
ting is that as t → ∞, x is equivalent to an
t
3.4 Back&RefineTechniqueduringInference
isotropicGaussiandistribution, aligningwiththe
starting state of inference. Then for the denois- Chen et al. (2022a) noted that the traditional in-
ing process, we use diffuser to predict the orig- ferenceprocessofdiffusionmodelsoftendiscards
inal x based on the image directly, denoted as the previously estimated value xˆ at each subse-
0 0
xˆ = f (x ,v,t),whereϕrepresentstheparame- quenttimestep,whichleadstosuboptimalresults.
0 ϕ t
ters of the diffuser. A rigorous mathematical ex- Wenoticethatthissituationoccursnotonlyinthe
planation of the diffusion model can be found in temporaldimension(timestep)butalsoinspatial
App.D. dimensions, i.e., the positions of words within a
Tointegratevisualinformationintothedenois- sentence. IncontrasttoARmodels,whichexplicit
ingprocess,weemploythecross-attentionmech- sequential dependencies across tokens, the diffu-
anism, where text serves as the query to extract sionmodelemitsalltokensinparallel. Considering
information from related image patches. In con- that sometokens, such asthe mainobjects in the
trast to previous approaches that typically inject image, can be easily restored. Conversely, some
imageinformationbyappendingthe[CLS]token tokens, representing visual details, are challeng-
ofthevisionencodertotext(Xu,2022;Heetal., ing to restore. Adding the same scale of noise toModel #Images BLEU@4 CIDEr METEOR SPICE ROUGE-L CLIP-Score BERT-Score
Autoregressive
ShowandTell(Vinyalsetal.,2014) - 31.4 97.2 25.0 18.1 53.1 69.7 93.4
CLIPCap(Mokady,2021) - 33.5 113.1 27.5 21.1 - - -
OSCAR (Lietal.,2020) 7M 36.5 123.7 30.3 23.1 - - -
†
ViTCap (Fangetal.,2021) 4M 36.3 125.2 29.3 22.6 58.1 - -
†
VinVL (Zhangetal.,2021) 6M 38.2 129.3 30.3 23.6 60.9 76.6 88.5
†
BLIP (Lietal.,2022a) 129M 39.7 133.3 - - - 77.4 94.4
†
GIT (Wangetal.,2022) 4M 40.4 131.4 30.0 23.0 - - -
†
TraditionalNon-autoregressive
NAICKD(Guoetal.,2020) 0.1M 28.5 98.2 23.6 18.5 52.3 - -
MNIC(Gaoetal.,2019) 0.1M 31.5 108.5 27.5 21.1 55.6 - -
FNIC(Fei,2019) 0.1M 36.2 115.7 27.1 20.2 55.3 - -
Diffusionmodelbased
DiffCap(Heetal.,2023b) 0.1M 31.6 104.3 26.5 19.6 55.1 73.6* 92.2*
BitDiffusion(Chenetal.,2022b) 0.1M 34.7 115.0 - - 58.0 - -
DDCap(Zhuetal.,2022) 0.1M 35.0 117.8 28.2 21.7 57.4 74.1* 93.4*
SCDNet(Luoetal.,2022) 0.1M 37.3 118.0 28.1 21.6 58.0 74.5* 93.4*
LaDiC(ours,5steps) 0.1M 35.1 115.2 27.4 21.3 56.7 77.1 93.8
LaDiC(ours,30steps) 0.1M 38.2 126.2 29.5 22.4 58.7 77.3 94.4
Table1: ModelperformanceonCOCOdataset. †indicatespretrainedmodels,andwegraythemoutsincethey
usemuchmoretrainingdata. *representstheresultsofmodelsreproducedbyourselves. Ourmodelachieves
state-of-the-art performance across various metrics for both diffusion-based and traditional NAR models, and
exhibitscomparableperformancewithsomewell-establishedpretrainingauto-regressiveframeworks,despitebeing
trainedonsignificantlylessdata. TheinferencetimemeasuredonanA100GPUfor5stepsis0.020s/imgand30
stepsis0.105s/img.
theeasy-to-restoredtokensastheothersisunrea- 2014; Karpathy and Fei-Fei, 2014), which com-
sonable and inefficient. Hence, it is intuitive to prises 113,287 training images, 5,000 validation
leveragetheseeasilyrestoredinformativetokensas images, and 5,000 test images. Each image is
conditionstoaidintherefinementprocessofthe associated with 5 reference captions. For evalu-
morechallengingones. ation,wefollowthecommonpracticeandusesev-
Accordingly, we propose a method named eral metrics including BLEU@4 (Papineni et al.,
Back&Refinetorollbackandrefinethosechalleng- 2002), CIDEr-D (Vedantam et al., 2014), ME-
ingtokenswithlowpredictionconfidence. Asillus- TEOR(BanerjeeandLavie,2005),ROUGE-L(Lin,
tratedinFig.5,givenasentencewithasequence 2004), and SPICE (Anderson et al., 2016). Ad-
lengthLandasamplingstepT toberestored. At ditionally, we employ two model-based metrics:
timeT/2,wecalculatethemodel’spredictioncon- CLIP-Score(Hesseletal.,2021)toassesssemantic
fidenceforalltokens. WekeepL/2tokenswiththe alignmentbetweengeneratedcaptionsandimages,
highestconfidence(depictedingreen),astheyare and BERT-Score (Zhang et al., 2019) to evaluate
consideredeasilyrestorableandrelativelyaccurate. textquality.
Conversely,fortheremainingL/2tokenswiththe
lowestconfidence(inred),wereproducethemby Implementation Details In our LaDiC model,
noising them with complete Gaussian noise (in theencoderanddecoderarefrozen,exceptforthe
grey). Then we set the current t = T and start a LM-head. Theweightsoftheencoderanddecoder
newdenoisingproceduretorestorethechallenging areinitializedfromthebottom6layersandtop6
tokensbasedontheeasierones. Applyingthistech- layers of BERT base, respectively. The rationale
nology,weobserveperformancegains,andwealso for selecting such a latent space is explained in
provideanotherexamplefordeeperunderstanding App. A.4. For the diffusion forward process, we
Back&RefineinApp.A.5. employ the widely used cosine β schedule and
adoptthenoisefactor(Gaoetal.,2022). Thedif-
4 Experiments fuser consists of 12 transformer encoder blocks
withadditionalcross-attentionlayersineachblock,
4.1 ExperimentalSettings
and the weights are randomly initialized. To ex-
Dataset and Metrics We conduct our experi- tractimagefeatures,weusethepretrainedimage
ments on MS COCO Karpathy split (Lin et al., encoder from BLIP (Li et al., 2022a), which
baseemploysViT-B/16,forafaircomparisonwithBLIP.
The model is trained on 8×V100 GPUs for 60
epochs with a peak learning rate of 5e-5 and a
warmupratioof0.1. Moredetailscanbefoundin
App.C.
4.2 QuantitativeAnalysis
WebenchmarkourLaDiCmodelagainstpriorbase- BLIP: a baseball player holding a bat next
lines, encompassing auto-regressive, traditional to home plate.
non-autoregressive, and diffusion-based models,
Ours 1: a batter getting ready to swing at a
leveragingtheCOCOdataset. AsshowninTab.1),
baseball game.
our model achieves state-of-the-art performance
Ours 2: a baseball player standing near an
across various metrics for both diffusion-based
umpire at home plate.
andtraditionalNARmodels. Specifically,LaDiC
Ours 3: a baseball player is swinging a bat at
achieves38.2BLEU@4and126.2CIDEr,marking
a game.
improvements of 0.9 and 8.2, respectively, com-
paredtothepreviousSOTAmethod,SCD-Net. Re- GT1: a baseball player is going to swing a bat.
markably,avariantofourmodel,utilizingonly5in-
GT2: a man is at bat at a baseball game with
ferencesteps,evenoutperformsallpriordiffusion-
a crowd watching.
basedmodelsinbothCLIP-ScoreandBERT-Score.
Moreover, it is noteworthy that LaDiC exhibits
Figure6: Anexamplegeneratedbyourmodel.
comparableperformancewithwell-establishedpre-
training auto-regressive frameworks such as ViT-
Cap and VinVL, despite being trained on signifi- Model SCD-Net BLIP Ours
cantlylessdata. Fluency 2.8 4.9 4.5
Toevaluateourmodel’scapacityforconsidering Accuracy 3.3 4.2 4.4
holisticcontext,wetacklethetaskofimagepara-
Conciseness 3.4 4.4 4.7
graphcaptioning(Krauseetal.,2016)togeneratea
multi-sentencedescriptionofanimage. Ourmodel Table2: Resultsofhumanevaluation.
seamlessly adapts to paragraph captioning by ex-
tending the predefined length without additional
jointed,withoccasionalrepetitionsandatendency
specialdesigns. Trainingourmodelonthedataset
tostartwith‘theman’. Conversely,byleveraging
from(Krauseetal.,2016)yieldsaBLEU@4score
a broader context, our model produces sentences
of7.3, surpassingfinetunedBLIP’s6.1andhigh-
withamorecohesivelogicalrelationship.
lightingourmodel’sadvantageinmitigatingerror
We conduct user studies to evaluate the gener-
accumulation(refertoApp.B.1formoredetails).
atedcaptionsofLaDiC,invitingvolunteerstorate
All these quantitative indicators above substanti-
captions on a five-point scale (1-5) for accuracy,
ate the accuracy and high quality of the captions
concisenessandfluency. Theresults,presentedin
generatedbyourmodel.
Tab. 2, demonstrate that our model surpasses the
4.3 CaseStudyandHumanEvaluation previousdiffusion-basedstate-of-the-artSCD-Net
in both aspects and achieves comparable results
We conduct a case study to illustrate the faithful-
withBLIP.DetailscanbefoundinApp.B.2.
ness and diversity of the captions generated by
LaDiC. As depicted in Fig. 6, the generated cap-
4.4 UnleashingtheSpeedofDiffusionModel
tionsarenotonlyreasonableandfluentbutalsoex-
hibitinherentdiversityduetothevariedsampling Despitetheirpowerfulgenerativecapabilities,dif-
noises introduced at the start of inference. Addi- fusion models are notorious for their slow infer-
tionalexamplescanbefoundinApp.A.1. Inthe ence speed. Most previous works require more
imageparagraphcaptioningtask,Fig.7highlights than50inferencesteps, significantlyslowerthan
a notable disparity between the two approaches. traditionalNARmethods,whichtypicallyinvolve
WhileBLIP-generatedcaptionsexhibitgoodqual- around 10 refinement procedures. However, as
ity, the sentences in the output often seem dis- shown in Tab. 1, our model achieves remarkableModel DiffCap DDCap Ours
Inferencelatency(s/img) 0.625 0.113 0.049
Table3: Inferencelatencyofdiffusion-basedmodels.
BLEU-4 CIDER
w/oBack&Refine 37.3 121.5
t=0.5T;l=0.5L(Ourfinalversion) 38.2 126.2
Finetuned BLIP: t=0.8T;l=0.5L 38.1 126.6
a man playing tennis. the man is wearing a
t=0.2T;l=0.5L 37.5 122.9
white shirt and black shorts. the man is
holding a tennis racket in his hand. the man t=0.5T;l=0.8L 37.6 123.5
is wearing a white shirt and black shorts. t=0.5T;l=0.2L 37.5 122.3
Ours:
Table4: ResultsofdifferentsettingsofBack&Refine
a man playing tennis is standing on a tennis
Technique. Thesecondbestresultisunderlined.
court. there is a green tennis ball above him.
he is wearing white shirt, and blank shorts.
there is a white line on the court.
thismethod,ourmodelfunctionsasacustomized
generatorbasedontheprovidedtokens. Additional
Figure 7: An example generated by fine-tuned BLIP
resultscanbefoundinApp.A.2.
modelandoursinimageparagraphcaptioning.
4.6 AnalysisfortheBack&RefineTechnique
performance even with just 5 steps. We attribute Regarding the Back&Refine technique, as dis-
thissurprisingconvergencespeedtospecifictech- cussed in § 3.4, we specify that when predict-
niquesemployedinourLaDiCmodel. Firstly,the ingasentencewithasequencelengthofLanda
directpredictionofx andthedefinitionofcaption sampling step of T, we opt to backtrack at time
0
lossenablethemodeltorapidlylearnthedistribu- t = 0.5T (T → 0) and discard l = 0.5L to-
tionofdiscretecaptiontext,akintotheconsistency kens. Wepresentanexperimenttoinvestigatethe
model(Songetal.,2023). Secondly,thecarefully influence of the two ratios in Back&Refine (ap-
selected noise schedule and noise factor signifi- plied once). As shown in Tab. 4, choosing an
cantly enhance the learning process of diffusion early backtracking time (t = 0.8T) leads to in-
models. Regarding observed latency, the results adequate recovery of easy-to-restore tokens, fail-
in Tab. 3 (measured on a single A40 GPU with a ingtoprovidesufficientinformationandresulting
batchsizeof256)andFig.2ademonstratethatour in performance just similar to scenarios without
modelshowcasesarapidinferencespeed,excelling Back&Refine. Conversely, backtracking at a late
notonlyinthedomainofdiffusion-basedmodels time(t = 0.2T)doesnotyieldsignificantimprove-
butalsowhencomparedtoauto-regressivemodels. ment,aseasy-to-restoretokensaretypicallyrecov-
ered quickly, and the additional steps introduce
4.5 CustomizingtheGenerationProcess
anundesirabledropininferencespeed. Similarly,
In contrast to the unidirectional generation man- droppingthemajorityoftokensintheBackproce-
nerofARmodels,ourLaDiCmodeladeptlyfills dure(0.8L)resultsinasituationakintoscenarios
in empty words at almost any position within a without Back&Refine. Dropping too few tokens
sentence,harnessingitscapabilitytocapturemore (0.2L) may introduce many mistaken tokens, ad-
holistic information, as demonstrated in Fig. 2c. verselyaffectingperformance. Therefore,wedelib-
Technically,whenprovidedwithacaptioncontain- eratelychoosethesettingt = 0.5T andl = 0.5L
ing blanks, we extract contextual embeddings of for our final version. Furthermore, an alternative
the given tokens and mask the blank tokens with optionistoestablishaconfidencescorethreshold
Gaussian noise. The standard denoising process insteadofdirectlydroppingthelasthalf. However,
is then applied, with the exception of reinserting inthemajorityofourearlyexperiments,thesetwo
theembeddingsofpredefinedtokensbacktotheir settingsexhibitanegligibleperformancegap. Con-
respectivepositionsaftereachinferencestep,ensur- sequently,weoptforthesimplersecondmethodin
ingthatthegiveninformationisretained. Through ourfinalversion.Cross Cap. Norm pure text generation with minimal modifications.
#Row PLM Split B&R B@4 C
attn. loss Reass
Weleavethesepotentialextensionsforfuturework,
a 15.4 46.3
b ✓ 20.3 59.1 and meanwhile, we hope this paper will inspire
c ✓ ✓ 22.8 76.3
d ✓ ✓ ✓ 26.9 91.8 confidence among researchers engaging in text-
e ✓ ✓ ✓ ✓ 31.6 103.5 centered multimodal generation tasks with diffu-
f ✓ ✓ ✓ ✓ ✓ 33.4 110.0
g ✓ ✓ ✓ ✓ ✓ ✓ 34.1 113.4 sion models and look forward to exciting future
works in this area. Furthermore, due to resource
Table5: AblationonCOCOdataset.
constraints,themodelparametersanddatasetsem-
ployedinourstudyarenotextensive. Considering
4.7 Ablationstudy theremarkableemergentabilitiesdemonstratedby
scaling up autoregressive models like GPT, it be-
To validate the effectiveness of our core designs,
comesanintriguingandworthwhileexplorationto
weconductablationstudiesontheCOCOdataset.
investigatewhetherourmodelorgeneraldiffusion
Owing to the extensive time required for the ab-
models,canexhibitsimilarscalability.
lation study, we opted for a subset of the dataset
and trained all models for 40 epochs, (at which
RiskConsideration: Asagenerativemodel,our
point the validation loss has already converged).
modelmayinadvertentlyproduceresultsthatare
Fortheinferencephase,weperformed5steps. We
challengingtodistinguishfromhuman-writtencon-
beginwithasimplebaselinethatappendsonlythe
tent,raisingconcernsaboutpotentialmisuse. Em-
[CLS]tokenoftheimagefeaturetotheendoftext
ployingtextwatermarktechniquescouldbebenefi-
embeddingsandthentrainsthediffusertorecover
cialinmitigatingthisissue. Additionally,diffusion
them. Subsequently,weprogressivelyincorporate
modelstypicallydemandsubstantialcomputational
our proposed techniques to evaluate their perfor-
resourcesfortraining,leadingtoincreasedcarbon
mance. AsdepictedinTab.5,allmodulesexhibit
dioxideemissionsandenvironmentalimpact.
performancegains. TheuseofPLM(BERT)and
regularizationinthisspacesignificantlyenhances
performance,emphasizingtheimportanceofare- References
finedlatentspace. Techniquesaimedatbettercap-
PeterAnderson,BasuraFernando,MarkJohnson,and
turingvisualinformation,suchascross-attention
StephenGould.2016. Spice: Semanticpropositional
and splitting the BERT, also play pivotal roles in
imagecaptionevaluation. ArXiv,abs/1607.08822.
improvingperformance.
JacobAustin,DanielD.Johnson,JonathanHo,Daniel
5 Conclusion Tarlow,andRiannevandenBerg.2021. Structured
denoisingdiffusionmodelsindiscretestate-spaces.
In this paper, we reexamine the diffusion-based InAdvancesinNeuralInformationProcessingSys-
image-to-text paradigm and introduce a novel tems.
architecture, denoted as LaDiC. Our model at-
JianhongBai,TianyuHe,YuchiWang,JunliangGuo,
tainsstate-of-the-artperformanceamongdiffusion-
HaojiHu,ZuozhuLiu,andJiangBian.2024. Uniedit:
basedmethodsanddemonstratescomparablecapa-
A unified tuning-free framework for video motion
bilities with some pre-trained AR models. More- andappearanceediting. ArXiv,abs/2402.13185.
over,ourextensiveexperimentsrevealtheexciting
advantagesofdiffusionmodelsoverARmodelsin SatanjeevBanerjeeandAlonLavie.2005. Meteor: An
automatic metric for mt evaluation with improved
consideringmoreholisticcontextsandemittingall
correlation with human judgments. In IEEvalua-
tokensinparallel. Consequently,wepositthatdif-
tion@ACL.
fusionmodelsholdsubstantialpotentialforimage-
to-textgenerationandwehopethatourworkwill A. Blattmann, Tim Dockhorn, Sumith Kulal, Daniel
Mendelevitch,MaciejKilian,andDominikLorenz.
opennewpossibilitiesinthisfield.
2023. Stable video diffusion: Scaling latent
video diffusion models to large datasets. ArXiv,
Limitations
abs/2311.15127.
For simplicity and focus, this paper concentrates
Ting Chen, Ruixiang Zhang, and Geoffrey Hinton.
on the main research topic of image-to-text gen-
2022a. Analog bits: Generating discrete data us-
eration. Nevertheless,weobservethatourmodel
ingdiffusionmodelswithself-conditioning. arXiv
canbereadilyadaptedtoothermodalitiesoreven preprintarXiv:2208.04202.TingChen, RuixiangZhang, andGeoffreyE.Hinton. KaimingHe,XinleiChen,SainingXie,YanghaoLi,Pi-
2022b. Analog bits: Generating discrete data us- otr Doll’ar, and Ross B. Girshick. 2021. Masked
ingdiffusionmodelswithself-conditioning. ArXiv, autoencoders are scalable vision learners. 2022
abs/2208.04202. IEEE/CVFConferenceonComputerVisionandPat-
ternRecognition(CVPR),pages15979–15988.
XiaoliangDai,JiHou,Chih-YaoMa,SamTsai,Jialiang
Tianyu He, Junliang Guo, Runyi Yu, Yuchi Wang,
Wang, Rui Wang, Peizhao Zhang, Simon Vanden-
JialiangZhu,KaikaiAn,LeyiLi,XuTan,Chunyu
hende,XiaofangWang,AbhimanyuDubey,Matthew
Wang, Han Hu, HsiangTao Wu, Sheng Zhao, and
Yu,AbhishekKadian,FilipRadenovic,DhruvMa-
Jiang Bian. 2023a. Gaia: Zero-shot talking avatar
hajan, Kunpeng Li, Yue Zhao, Vladan Petrovic,
generation.
MiteshKumarSingh,SimranMotwani,YiWen,Yi-
wenSong,RoshanSumbaly,VigneshRamanathan,
Yufeng He, Zefan Cai, Xu Gan, and Baobao Chang.
ZijianHe,PeterVajda,andDeviParikh.2023. Emu:
2023b. Diffcap: Exploringcontinuousdiffusionon
Enhancing image generation models using photo-
imagecaptioning. ArXiv,abs/2305.12144.
genicneedlesinahaystack.
Zhengfu He, Tianxiang Sun, Kuan Wang, Xuanjing
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Huang, and Xipeng Qiu. 2022. Diffusionbert: Im-
KristinaToutanova.2019. Bert: Pre-trainingofdeep provinggenerativemaskedlanguagemodelswithdif-
bidirectionaltransformersforlanguageunderstand- fusionmodels. InAnnualMeetingoftheAssociation
ing. ArXiv,abs/1810.04805. forComputationalLinguistics.
JackHessel,AriHoltzman,MaxwellForbes,RonanLe
Sander Dieleman, Laurent Sartran, Arman Roshan-
Bras,andYejinChoi.2021. Clipscore: Areference-
nai, Nikolay Savinov, Yaroslav Ganin, Pierre H.
freeevaluationmetricforimagecaptioning. ArXiv,
Richemond,A.Doucet,RobinStrudel,ChrisDyer,
abs/2104.08718.
Conor Durkan, Curtis Hawthorne, Rémi Leblond,
Will Grathwohl, and Jonas Adler. 2022. Con-
Jonathan Ho, Ajay Jain, and P. Abbeel. 2020a. De-
tinuous diffusion for categorical data. ArXiv,
noising diffusion probabilistic models. ArXiv,
abs/2211.15089.
abs/2006.11239.
ZhiyuanFang,JianfengWang,XiaoweiHu,LinLiang, JonathanHo,AjayJain,andPieterAbbeel.2020b. De-
ZheGan,LijuanWang,YezhouYang,andZicheng noising diffusion probabilistic models. Neural In-
Liu.2021. Injectingsemanticconceptsintoend-to- formation Processing Systems,Neural Information
endimagecaptioning. 2022IEEE/CVFConference ProcessingSystems.
onComputerVisionandPatternRecognition(CVPR),
pages17988–17998. JonathanHoandTimSalimans.2022. Classifier-free
diffusionguidance.
ZhengcongFei.2019. Fastimagecaptiongeneration
LiHu,XinGao,PengZhang,KeSun,BangZhang,and
withpositionalignment. ArXiv,abs/1912.06365.
LiefengBo.2023. Animateanyone: Consistentand
controllableimage-to-videosynthesisforcharacter
Junlong Gao, Xi Meng, Shiqi Wang, Xia Li, Shan-
animation. arXivpreprintarXiv:2311.17117.
she Wang, Siwei Ma, and Wen Gao. 2019.
Maskednon-autoregressiveimagecaptioning. ArXiv, JustinJohnson,AndrejKarpathy,andLiFei-Fei.2016.
abs/1906.00717. Densecap: Fullyconvolutionallocalizationnetworks
for dense captioning. In Proceedings of the IEEE
ZhujinGao,JunliangGuo,XuejiaoTan,YongxinZhu, ConferenceonComputerVisionandPatternRecog-
Fang Zhang, Jiang Bian, and Linli Xu. 2022. Dif- nition.
former: Empoweringdiffusionmodelonembedding
spacefortextgeneration. ArXiv,abs/2212.09412. ZeqianJu,YuanchengWang,KaiShen,XuTan,Detai
Xin,DongchaoYang,YanqingLiu,YichongLeng,
KaitaoSong,SiliangTang,ZhizhengWu,TaoQin,
ShansanGong,MukaiLi,JiangtaoFeng,ZhiyongWu,
Xiang-YangLi,WeiYe,ShikunZhang,JiangBian,
andLingPengKong.2022. Diffuseq: Sequenceto
Lei He, Jinyu Li, and Sheng Zhao. 2024. Natural-
sequencetextgenerationwithdiffusionmodels.
speech3: Zero-shotspeechsynthesiswithfactorized
codecanddiffusionmodels.
IanJ.Goodfellow,JeanPouget-Abadie,MehdiMirza,
BingXu,DavidWarde-Farley,SherjilOzair,AaronC.
Andrej Karpathy and Li Fei-Fei. 2014. Deep visual-
Courville, and Yoshua Bengio. 2014. Generative
semantic alignments for generating image descrip-
adversarialnets. InNIPS.
tions. IEEE Transactions on Pattern Analysis and
MachineIntelligence,39:664–676.
LongtengGuo,JingLiu,XinxinZhu,XingjianHe,Jie
Jiang, and Hanqing Lu. 2020. Non-autoregressive Andrej Karpathy and Li Fei-Fei. 2017. Deep visual-
imagecaptioningwithcounterfactuals-criticalmulti- semantic alignments for generating image descrip-
agentlearning. InInternationalJointConferenceon tions. IEEE Transactions on Pattern Analysis and
ArtificialIntelligence. MachineIntelligence,page664–676.Diederik P. Kingma and Max Welling. 2013. Auto- Haohe Liu, Zehua Chen, Yiitan Yuan, Xinhao Mei,
encodingvariationalbayes. CoRR,abs/1312.6114. Xubo Liu, Danilo P. Mandic, Wenwu Wang, and
MarkD . Plumbley. 2023b. Audioldm: Text-to-
JonathanKrause,JustinJohnson,RanjayKrishna,and audiogenerationwithlatentdiffusionmodels. ArXiv,
LiFei-Fei.2016. Ahierarchicalapproachforgen- abs/2301.12503.
erating descriptive image paragraphs. 2017 IEEE
ConferenceonComputerVisionandPatternRecog- HaotianLiu,ChunyuanLi,QingyangWu,andYongJae
nition(CVPR),pages3337–3345. Lee. 2023c. Visual instruction tuning. ArXiv,
abs/2304.08485.
RanjayKrishna,YukeZhu,OliverGroth,JustinJohn-
son, Kenji Hata, Joshua Kravitz, Stephanie Chen, Justin Lovelace, Varsha Kishore, Chao Wan, Eliot
Yannis Kalantidis, Li-Jia Li, David A. Shamma, Shekhtman, and Kilian Q. Weinberger. 2023. La-
Michael S. Bernstein, and Fei-Fei Li. 2016. Vi- tentdiffusionforlanguagegeneration.
sualgenome: Connectinglanguageandvisionusing
crowdsourceddenseimageannotations. Jianjie Luo, Yehao Li, Yingwei Pan, Ting Yao, Jian-
lin Feng, Hongyang Chao, and Tao Mei. 2022.
KyungminLee,KihyukSohn,andJinwooShin.2024. Semantic-conditionaldiffusionnetworksforimage
Dreamflow: High-quality text-to-3d generation by captioning.
approximatingprobabilityflow.
Ron Mokady. 2021. Clipcap: Clip prefix for image
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan captioning. ArXiv,abs/2111.09734.
Ghazvininejad,AbdelrahmanMohamed,OmerLevy,
VeselinStoyanov,andLukeZettlemoyer.2019. Bart: OpenAI.2023.
Denoisingsequence-to-sequencepre-trainingfornat-
ural language generation, translation, and compre- KishorePapineni,SalimRoukos,ToddWard,andWei-
hension. InAnnualMeetingoftheAssociationfor JingZhu.2002. Bleu: amethodforautomaticevalu-
ComputationalLinguistics. ationofmachinetranslation. InProceedingsofthe
40thAnnualMeetingoftheAssociationforCompu-
JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. tational Linguistics, pages 311–318, Philadelphia,
2022a. Blip: Bootstrapping language-image pre- Pennsylvania,USA.AssociationforComputational
training for unified vision-language understanding Linguistics.
andgeneration.
Dustin Podell, Zion English, Kyle Lacey, Andreas
XiangLisaLi,JohnThickstun,IshaanGulrajani,Percy Blattmann,TimDockhorn,JonasMüller,JoePenna,
Liang,andTatsunoriB.Hashimoto.2022b. Diffusion- andRobinRombach.2023. Sdxl: Improvinglatent
lmimprovescontrollabletextgeneration. diffusionmodelsforhigh-resolutionimagesynthesis.
Xiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu, Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben
Pengchuan Zhang, Lei Zhang, Lijuan Wang, Mildenhall.2022. Dreamfusion: Text-to-3dusing2d
Houdong Hu, Li Dong, Furu Wei, Yejin Choi, diffusion. arXiv.
and Jianfeng Gao. 2020. Oscar: Object-semantics
aligned pre-training for vision-language tasks. In AdityaRamesh,PrafullaDhariwal,AlexNichol,Casey
EuropeanConferenceonComputerVision. Chu, and Mark Chen. 2022. Hierarchical text-
conditionalimagegenerationwithcliplatents. ArXiv,
Chin-YewLin.2004. Rouge: Apackageforautomatic abs/2204.06125.
evaluationofsummaries. InAnnualMeetingofthe
AssociationforComputationalLinguistics. MachelReid,VincentJ.Hellendoorn,andGrahamNeu-
big.2022. Diffuser: Discretediffusionviaedit-based
Tsung-YiLin,MichaelMaire,SergeJ.Belongie,James reconstruction.
Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C. Lawrence Zitnick. 2014. Microsoft coco: ShuhuaiRen,SishuoChen,ShichengLi,XuSun,and
Commonobjectsincontext. InEuropeanConference LuHou.2023. TESTA:Temporal-spatialtokenag-
onComputerVision. gregationforlong-formvideo-languageunderstand-
ing. In Findings of the Association for Computa-
Zheng-WenLin,YeyunGong,YelongShen,TongWu, tional Linguistics: EMNLP 2023. Association for
ZhihaoFan,ChenLin,NanDuan,andWeizhuChen. ComputationalLinguistics.
2022. Textgenerationwithdiffusionlanguagemod-
els: Apre-trainingapproachwithcontinuouspara- ShuhuaiRen,JunyangLin,GuangxiangZhao,RuiMen,
graphdenoise. InInternationalConferenceonMa- AnYang,JingrenZhou,XuSun,andHongxiaYang.
chineLearning. 2021. Learning relation alignment for calibrated
cross-modalretrieval. InProceedingsofthe59thAn-
Guisheng Liu, Yi Li, Zhengcong Fei, Haiyan Fu, Xi- nualMeetingoftheAssociationforComputational
angyang Luo, and Yanqing Guo. 2023a. Prefix- Linguisticsandthe11thInternationalJointConfer-
diffusion: Alightweightdiffusionmodelfordiverse ence on Natural Language Processing (Volume 1:
imagecaptioning. ArXiv,abs/2309.04965. LongPapers).OlafRonneberger,PhilippFischer,andThomasBrox. ZixinZhu,YixuanWei,JianfengWang,ZheGan,Zheng
2015. U-net: Convolutionalnetworksforbiomedical Zhang,LeWang,GangHua,LijuanWang,Zicheng
imagesegmentation. ArXiv,abs/1505.04597. Liu,andHanHu.2022. Exploringdiscretediffusion
modelsforimagecaptioning.
JiamingSong,ChenlinMeng,andStefanoErmon.2020.
Denoisingdiffusionimplicitmodels. arXiv: Learn-
ing,arXiv: Learning.
Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya
Sutskever. 2023. Consistency models. ArXiv,
abs/2303.01469.
ZinengTang,ZiyiYang,ChenguangZhu,MichaelZeng,
andMohitBansal.2023. Any-to-anygenerationvia
composablediffusion. ArXiv,abs/2305.11846.
AshishVaswani,NoamM.Shazeer,NikiParmar,Jakob
Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser,andIlliaPolosukhin.2017. Attentionisall
youneed. InNIPS.
RamakrishnaVedantam,C.LawrenceZitnick,andDevi
Parikh.2014. Cider:Consensus-basedimagedescrip-
tionevaluation. 2015IEEEConferenceonComputer
VisionandPatternRecognition(CVPR),pages4566–
4575.
Oriol Vinyals, Alexander Toshev, Samy Bengio, and
D. Erhan. 2014. Show and tell: A neural image
captiongenerator. 2015IEEEConferenceonCom-
puterVisionandPatternRecognition(CVPR),pages
3156–3164.
Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Lin-
jie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu,
and Lijuan Wang. 2022. Git: A generative image-
to-texttransformerforvisionandlanguage. ArXiv,
abs/2205.14100.
ShitongXu.2022. Clip-diffusion-lm: Applydiffusion
modelonimagecaptioning.
XingqianXu,ZhangyangWang,EricZhang,KaiWang,
andHumphreyShi.2022. Versatilediffusion: Text,
imagesandvariationsallinonediffusionmodel.
Hongyi Yuan, Zheng Yuan, Chuanqi Tan, Fei Huang,
andSongfangHuang.2022. Seqdiffuseq: Textdif-
fusion with encoder-decoder transformers. ArXiv,
abs/2212.10325.
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei
Yang,LeiZhang,LijuanWang,YejinChoi,andJian-
fengGao.2021. Vinvl: Revisitingvisualrepresenta-
tionsinvision-languagemodels. In2021IEEE/CVF
ConferenceonComputerVisionandPatternRecog-
nition(CVPR).
TianyiZhang,VarshaKishore,FelixWu,KilianQ.Wein-
berger, and Yoav Artzi. 2019. Bertscore: Evaluat-
ing text generation with bert. Cornell University -
arXiv,Learning.
Haozhe Zhao, Zefan Cai, Shuzheng Si, Xiaojian Ma,
Kaikai An, Liang Chen, Zixuan Liu, Sheng Wang,
WenjuanHan,andBaobaoChang.2023. Mmicl:Em-
poweringvision-languagemodelwithmulti-modal
in-contextlearning. ArXiv,abs/2309.07915.A Hierarchical Approach for Generating Descriptive Image Paragraphs
JonathanKrause JustinJohnson RanjayKrishna LiFei-Fei
StanfordUniversity
jkrause,jcjohns,ranjaykrishna,feifeili @cs.stanford.edu
{ }
A AdditionalResults
Abstract
A.1 GeneratedSamplesfromCOCODataset
Recentprogressonimagecaptioninghasmadeitpossible
AddtoitigoennearlateexnoavmelpsleenstengceesnedreasctreidbinbgyimaoguersiLnanDatuiCral
language,butcompressinganimageintoasinglesentence
model are presented in Fig. 10. It is shown that
candescribevisualcontentinonlycoarsedetail.Whileone
our model adeptly captures the main objects and
newcaptioningapproach,densecaptioning,canpotentially
theidrersecrlaibteioimnsahgeispsinifinnethrelevdeelspoicftdeedtaiilmbyagcaepst.ioSniinmgumltaan-y
regionswithinanimage,itinturnisunabletoproducea
neously,thegeneratedcaptionsexhibitahighlevel
coherentstoryforanimage.Inthispaperweovercomethese
offluency.
limitationsbygeneratingentireparagraphsfordescribing
images,whichcantelldetailed,unifiedstories.Wedevelop
A.2amCoduelstthoamtdeGcoemnpeorsaestiboonthimagesandparagraphsinto
theirconstituentparts,detectingsemanticregionsinimages
Utilizing the partially adding noise technique de-
andusingahierarchicalrecurrentneuralnetworktoreason Sentences
scriabbeodutinlan§gu4a.g5e,.wLiengoubisstiecravneadlytshisact,onufinrlmiksethethceomupnliedxiit-y 1)A girl is eating donuts with a boy in a restaurant
rectoifotnhealpagreangrearpahtigoenneraaptiponrotaasckh,aondftAhoRroumghodexeplesr,imoeunrts 2)A boy and girl sitting at a table with doughnuts.
3)Two kids sitting a coffee shop eating some frosted donuts
onanewdatasetofimageandparagraphpairsdemonstrate
LaDiCmodelcaneffectivelyinsertwordsintoal- 4)Two children sitting at a table eating donuts.
theeffectivenessofourapproach.
mostanypositionwithinasentence. Fig.11offers 5)Two children eat doughnuts at a restaurant table.
Paragraph
additionalexamplestoillustratethegeneralization
Two children are sitting at a table in a restaurant. The
abil1 it. yIn ot fr to hd isuc mti eo tn hod. children are one little girl and one little boy. The little girl is
eating a pink frosted donut with white icing lines on top of it.
Visionistheprimarysensorymodalityforhumanpercep- The girl has blonde hair and is wearing a green jacket with a
A.3tionG,arnaddlaunagluaDgeenisooiusrinmgosPtproowceersfsuldtouorlifnorgcommuni- black long sleeve shirt underneath. The little boy is wearing a
black zip up jacket and is holding his finger to his lip but is
catiInngfweirthenthceeworld.Buildingsystemsthatcansimultane- not eating. A metal napkin dispenser is in between them at
ouslyunderstandvisualstimulianddescribetheminnatural the table. The wall next to them is white brick. Two adults are
Welasnhgouwagetihsethegrreafodrueaalcodreenporoisbilnemgipnrboocthescsomopfutoeruvri- on the other side of the short white brick wall. The room has
white circular lights on the ceiling and a large window in the
modsieoln.aAndsaartgifiecniaelrainttievleligmenocdeeals,athwehodlief.fWusiitohnthmeaoddveenlt front of the restaurant. It is daylight outside.
oflargedatasetspairingimageswithnaturallanguagede-
iscapableofmodelingthedistributionofanyspace Figure1. Paragraphsarelonger, moreinformative, andmore
scriptions[20,34,10,16]ithasrecentlybecomepossibleto Figure8: Anexamplefromimageparagraphcaptioning
bybgeeinnergatteranoinveeldsetnotepnrcoesgdreesscsriivbienlgyimtraagnessf[o4r,m6,1r2a,n2d2o,3m0]. linguisticallycomplexthansentence-levelcaptions.Hereweshow
daatnaismeatg(eciwteitdhiftrsosment(eHncee-eletvaell.c,a2p0tio2n1s)f)r.omMSCOCO[20]
noisWehiinletothaesgurcoceusnsdoftrthuetshesmaemthpodles.isInenocouurramgiongd,etlh,ewyaell
(top)andtheparagraphusedinthiswork(bottom).
shareonekeylimitation:detail.Byonlydescribingimages
opttoapplydiffusiontothelatentspaceoftext,i.e.,
with a single high-level sentence, there is a fundamental
the us pe pn et re -bn oc ue ndfe oa ntu thr ee qs up ana tc ite yo anf dB qE uaR liT ty. oA fs ini fl ol ru ms at tr ia ot ne ad
p-
asdeinnssepciarpattioioninngfodersocruibreBsiamcakg&esRinecfionnseidTereacbhlynmiqoureed,ea-s
in§p1ro,aochuerspcarnocperosdsubcee.ginswithGaussiannoise. At ditasicluthsasnesdtainnda§rd3i.m4a.gecaptioning.However,thiscomesat
each sOtenpe,rewceentsaultbertnraatcivteatocseenrtteanicne-alemveolucanpttioonfinngoiissteh.e acost:descriptionsgeneratedfordensecaptioningarenot
taskofdensecaptioning[11],whichovercomesthislimita- Ac .o 4here Ent x, pi.e lo.t rh aey tid oo nn oot nfo tr hm ea Cco hh oes iciv ee ow fho Lle ad tees nc tribing
Ast th ioe nn bu ym deb tee cr tio ngfs mt ae np ys ri en gc ior ne sa ose fs in, tt eh ree std ie nn ao ni is me ad gs ee an n- d theentireimage.
Space
tencdeesfceriabtiungreeaccohnwviethrgaeshsotrotwphararsdes.Bthyeexgtreonduinngd-thtreuttahsk Inthispaperweaddresstheshortcomingsofbothtra-
sentoefnocbejefcetadetuterceti.oMntaotihneclmudaetincaatullryal,loanugrudagifefudessecrriwptiiolln, Indi§tio3n.a2l,imwaegeacdadprteiosnsiendgatnhdetihnefroercmenatltyi-opnrodpoesnesditdyengsaep
modelsuchadistribution: P(T i+1|T i,I). Here,I between vision and language by diffusing on the
representsimagefeatures,T i denotesthesentence middle layer of the BERT model. Regarding the
featureatthelaststepi,andT i+1 signifiesthenew choiceofdifferentpossiblelatentspaces,wecon-
sentencefeaturewithreducednoise. ductedpreliminaryexperimentstoinvestigatethis
As an illustrative example, refer to a specific issue. We implemented various latent spaces ex-
caseinFig.9. Withanincreaseinsteps,theMean tractedfromdifferentlayers(specifically,the3rd,
Squared Error (MSE) distance between the cur- 6th,and9thlayersoftheBERT ),withthefind-
base
rentsentencevectorandtheground-truthsentence ingspresentedinTab.6. Itisimportanttonotethat
vectordiminishes,andthesentencesgeneratedby intheseinitialtests,themodelwastrainedfor40
thepredictedsentencevectorateachstepbecome epochswithoutincorporatingcaptionlossandthe
progressively more fluent. These findings collec- Back&Refinetechnique. Ourresultsindicatethat
tivelydemonstratethecapabilityofourdiffusion the6thlayeroutperformstheothers,whichisthe
model to gradually steer noised sentence vectors rationalebehinditsselectionasourfinalsettingin
towardsground-truthsentencevectorsandgener- thepaper. Althoughwedidnotexploreeverylayer,
atehigh-qualitysamples. Whenwecarefullycheck our preliminary experiments already provided us
the generated captions, notably, the main objects withadegreeofconfidenceandsuggestedthatlay-
initially emerge, and subsequently, more details ersproximaltothemidpointofBERT(whilenot
areincrementallyadded,resultinginincreasingly necessarily exactly the 6th layer due to different
fluent sentences. This characteristic also serves datasetsorhyperparameters)mayhavebetteralign-
7102
rpA
01
]VC.sc[
2v70660.1161:viXraTimestep MSE Generated Captions
T=1 0.2944 a girl is a her her.
T=2 0.2191 a young girl a acat a cat.
T=5 0.0648 young girl holding a cat holding a cat.
T = 10 0.0262 a young girl holding a small cat.
Figure9: Gradualdenoisingprocessofdiffusionmodels.
BLEU-4 CIDER etal.,2016),wheretheauthorsproposedadataset
Layer3ofBERT base 31.1 98.4 comprising 19,551 images from MS COCO (Lin
Layer6ofBERT base 33.7 112.3 et al., 2014) and Visual Genome (Krishna et al.,
2016),eachannotatedwithaparagraphdescription.
Layer9ofBERT base 32.3 106.8
AnillustrativeexampleispresentedinFig.8(cited
Table6: PerformancefordifferentlayersofBERT. from(Krauseetal.,2016)).
Toassessourmodel’sabilitytoconsiderholistic
context,wecomparetheperformanceofourmodel
mentwithimagespace.
andBLIPonthistask. Forourmodel,weextend
A.5 TheSelf-CorrectionAbilityof the predefined length to 60 and conduct training
Back&RefineTechnique over 120 epochs. For BLIP, we fine-tune from
InourBack&Refinetechnique,weutilizethepre- BLIP base using the same number of epochs and
aninitiallearningrateof1e-5. Subsequently, we
servedeasy-to-restoretokenstofacilitatethegen-
evaluatetheresultsusingBLEUonthetestset. In
eration of hard-to-restore tokens. However, it is
thecaseofBLIP,themaximumlengthissetto60,
importanttoemphasizethattheremainingtokens
andthenumberofbeamsis5duringinference.
intheBackprocedurestillhaveopportunitiesfor
revisionintheRefineprocedureratherthanbeing
B.2 HumanEvaluation
fixed. On the one hand, our initial experiments
find that well-restored tokens are inclined to be Asagenerativetask,inadditiontoautomaticmet-
preservedduringtheRefineprocedure. Thisobser- rics,itisimperativetoassessresultsthroughhuman
vationguidesourintuitiontoleveragethesewell- subjectiveevaluation. Tothisend,weutilizeMOS
restored tokens to enhance the denoising process (MeanOpinionScore)asourmetricandenlistthe
for challenging-to-restore tokens. On the other feedbackof20experiencedvolunteers,whowere
hand, it’s noteworthy that, as these well-restored taskedwithratingresultsonascaleof1-5. They
tokensarealsorequiredtopassthroughtheRefine evaluatedtheresultsfromthreeperspectives: flu-
procedure,therearestillchancestoaddresserrors ency, accuracy, andconciseness. Fluencygauges
inadvertentlyretained,suchasgrammarissues. For the quality of generated captions in terms of lan-
example, in a real-case scenario, when the Back guage,accuracyassesseswhetherthemainobjects
procedurefinishes,asentenceis“[]childrenis[] andactionsinthecaptionaccuratelyreflectthepic-
[][][]apizza.” wheretheungrammaticalword“is” ture,andconcisenessevaluatestheextenttowhich
ispreserved. However,throughtheRefineproce- generative captions are informative and succinct,
dure,thefinaloutputcaptioniscorrectedto“Two avoidingunnecessarydetails.
childrenaresittingatatableeatingpizza.” Toensureevaluationquality,werandomlysam-
pled10picturesfromtheCOCOdatasetandgener-
B AdditionalDetailsinExperiments
atedcorrespondingcaptionsforSCD-Net,BLIP2,
B.1 DetailsaboutExperimentsonImage andourLaDiCmodel. Subsequently,weshuffled
ParagraphCaptioning thethreecaptionsandrequiredvolunteerstorate
them. Toguaranteethereliabilityoftheevaluation,
Theobjectiveofimageparagraphcaptioningisto
werandomlyselected2evaluatorsandcalculated
generatecomprehensiveparagraphsthatdescribe
images,providingdetailedandcohesivenarratives.
2ForBLIP,weutilizedthefollowingpageforconvenient
This concept was initially introduced in (Krause inference:https://replicate.com/salesforce/blip.Hyperparameters Values D MathematicalDetailsforDiffusion
Models
Training
Batchsize 64*8(GPUs) The training flow of the diffusion models is di-
Epoch 60 videdintotwophases: theforwarddiffusionpro-
PeakLearningrate 5e-5 cess and the backward denoising process. Given
Learningrateschedule Linear
a data point sampled from a real data distribu-
Warmupratio 0.1
tion x ∼ q(x)3, we define a forward diffusion
0
Optimizer AdamW
processinwhichGaussiannoiseisincrementally
β 1 0.9
β 2 0.999 added to the sample, generating a sequence of
noisy samples x ,...,x . The noise scales are
1 T
Inference
controlled by a variance schedule β ∈ (0,1),
t
Method DDIM and the density is expressed as q(x |x ) =
√ t t−1
SamplingCriterion MinimumBayesRisk
N(x ; 1−β x ,β I). Basedonthereparame-
t t t−1 t
DiffusionProcess terizationtrick(Hoetal.,2020a),aniceproperty
Diffusionsteps 1000 oftheaboveprocessisthatwecansampleatany
βminimum 0.0001 arbitrarytimestepinaclosedform:
βmaximum 0.02 √ √
βschedule Cosine x t = α tx t−1+ 1−α tϵ t−1
Classifierfreeprobability 0.1 √ √ (cid:112)
= α ( α x + 1−α ϵ )
t t−1 t−2 t−1 t−2
Classifierfreeweight 1 √
Self-conditioningprobability 0.5 + 1−α tϵ t−1
√ (cid:112)
Loss = α tα t−1x t−2+( α t(1−α t−1)ϵ t−2
√
λ 0.2 + 1−α ϵ )
t t−1
Losstype l 2 √ (cid:112)
= α α x + 1−α α ϵ
t t−1 t−2 t t−1 t−2
ImageEncoder
=···
Imagesize 224 √ √
= α x + 1−α ϵ.
ImageEncoder BLIPbase t 0 t
DiffuserModule whereα = 1−β andα¯ = (cid:81)t α . Thus:
t t t i=1 i
Sequencelength 24
Hiddensize 768 √ √
Layers 12 q(x t|x 0) = N(x t; α¯ tx 0, 1−α¯ tI), (3)
FFNsize 3072
Attentionheads 16 Furthermore, from this equation, it becomes evi-
dentthatasT → ∞,x convergestoanisotropic
T
Table7: MorehyperparametersofourLaDiCmodel. Gaussiandistribution,aligningwiththeinitialcon-
ditionduringinference.
However, obtaining the closed form of the re-
their correlation on each metric. This procedure versedprocessq(x |x )ischallenging. Notably,
t−1 t
wasrepeated5times,andallresultswerefoundto if β is sufficiently small, the posterior will also
t
besatisfactory.
beGaussian. Inthiscontext,wecantrainamodel
AsdepictedinTab.2,ourmodelsurpassesthe p (x |x )toapproximatetheseconditionalprob-
θ t−1 t
previousdiffusion-basedstate-of-the-artSCD-Net abilities:
in all aspects, achieving comparable results with
BLIP.Aslightdecreaseintextqualitycomparedto
p (x |x ) = N(x ;µ (x ,t),Σ (x ,t)),
BLIPmaybeattributedtothesubstantialtraining θ t−1 t t−1 θ t θ t
datausedinBLIP’spretraining.
whereµ (x ,t)andΣ (x ,t)areparameterizedby
θ t θ t
adenoisingnetworkf likeU-Net (Ronneberger
θ
C MoreHyperparameters etal.,2015)orTransformer(Vaswanietal.,2017).
3We follow the notation and derivation process
WelistmorehyperparametersforLaDiCmodelin
of https://lilianweng.github.io/posts/2021-07-11-diffusion-
Tab.7. models.Ours: a teddy bear sitting on a book Ours: two children are sitting at a table
shelf. eating pizza.
GT: there is a stuffed bear sitting on a GT: two children sitting at a little table
book shelf. eating pizza.
Ours: a group of people riding skis Ours: a group of people riding waves in
down a snowy slope. the ocean.
GT: people are skiing on the snowy GT: a group of surfers in the ocean riding
slopes in a designated area. on the waves.
Ours: a group of people sitting
Ours: a plate of food next to a cup of
around a table with drinks.
coffee.
GT: a bunch of people are sitting
GT: a plate of food and a cup of coffee
around a table.
on table.
Ours: a red fire hydrant on a sidewalk Ours: a dog sitting in the passenger seat
near a street. of a car.
GT: a red fire hydrant on a city GT: an image of a dog sitting in the
sidewalk. passenger seat of a car.
Ours: two boys sitting on the floor playing Ours: a bicycle parked in the grass near
a video game. a lake.
GT: two young boys sit on the carpet GT: A bike is parked on the grass in
playing a video game. front of the lake.
Figure10: MoreexamplesgeneratedbyourmodelonCOCOdatasets.
SimilartoVAE(KingmaandWelling,2013),we 2020b)theyderive:
canderivethevariationallowerboundtooptimize
(cid:20) (cid:21)
1
t 2h 0e 20n beg ),a :tivelog-likelihood ofinput x 0 (Ho etal., L t =E x0,ϵ 2 ||Σ θ(x t,t) ||2 2||µ˜ t(x t,x 0) −µ θ(x t,t) ||2
(cid:20)
1 1 β
=E (x t ϵ )
x0,ϵ 2 ||Σ θ(x t,t) ||2 2||√a
t
t − √ (cid:21)1 −a
t
t
1 β
(x t ϵ (x ,t)) 2
t θ t
−√a − √1 a ||
t t
−
(cid:20) β2 (cid:21)
=E t ϵ ϵ (x ,t) 2
x0,ϵ 2α t(1 −α t) ||Σ θ ||2 2)|| t − θ t ||
(cid:34)
L vlb = E q[D KL(q(x t|x 0)||p θ(x T))]−logp θ(x 0|x 1) =E β t2
(cid:124) L(cid:123) T(cid:122) (cid:125) (cid:124) L(cid:123)(cid:122)
0
(cid:125) x0,ϵ 2α t(1 −α t) ||Σ θ ||2 2)×
(cid:35)
(cid:88)T ϵ
t
ϵ θ(√α tx 0+√1 α tϵ t,t) 2
+E [ D (q(x |x ,x )||p (x |x ))]. || − − ||
q KL t−1 t 0 θ t−1 t
(cid:124) (cid:123)(cid:122) (cid:125)
t=2
Lt−1 Removing the coefficients, a much more simple
DDPMlearningobjectivecanbeobtained:
T
L = (cid:88) E (cid:2) ||ϵ (x ,x )−ϵ (x ,t)||2(cid:3) ,
simple q t t 0 θ t
t=1
where ϵ is the noise added in original data x .
t 0
Appliedtotextualdata,(Lietal.,2022b)introduces
Withanadditionalconditiononx ,theposterior anevensimplerarchitecturetotrainanetworkto
0
of the forward process q(x |x ,x ) can be cal- predictx directly,withthelossfunctiondefined
t−1 t 0 0
culated using Bayes theorem. Then in (Ho et al., asL = ||x −f (x ,t)||.
0 θ tInput: there is a boy [UNK] Input: [UNK] [UNK] [UNK] Input: a [UNK] [UNK] is Input: [UNK] [UNK] [UNK] [UNK]
[UNK] [UNK] cows [UNK] [UNK] on the grass holding a [UNK] in her hand [UNK] in front of a computer.
Output: there is a boy standing Output: An old blue car parked Output: a young girl is Output: a cup of coffee sitting in
by several cows on the grass holding a cat in her hand. front of a computer.
Figure11: Moreexamplesofcustomgeneration.
During inference, the reverse process com-
mences by sampling noise from a Gaussian dis-
tribution p(x ) = N(x ;0,I) and iteratively de-
T T
noisingitusingp (x |x )untilreachingx . In
θ t−1 t 0
DDIM(Songetal.,2020),ageneralformisderived
fromEquation3.
(cid:112) (cid:112)
x = α x + 1−α ϵ
t−1 t−1 0 t−1 t−1
(cid:113)
(cid:112)
= α x + 1−α −σ2ϵ
t−1 0 t−1 t t
+σ ϵ
t
(cid:113)
(cid:112)
= α x + 1−α −σ2
t−1 0 t−1 t
√
x − α x
t t 0
( √ )+σ ϵ
t
1−α
t
(cid:112)
q (x |x ,x ) = N(x ; α x +
σ t−1 t 0 t−1 t−1 0
√
(cid:113) x − α x
1−α −σ2( t √ t 0 ),σ2I).
t−1 t 1−α t
t
whereσ2 = ηβ˜ = η1−αt−1β ,allowingustoad-
t t 1−αt t
justη asahyperparametertocontrolthesampling
stochasticity. The special case of η = 0 renders
the sampling process deterministic. This model
is referred to as the denoising diffusion implicit
model(DDIM).ItisnoteworthythatDDIMshares
thesamemarginaldistributionasDDPM.Conse-
quently,duringgeneration,wecansampleonlya
subsetofdiffusionstepsτ ,...,τ ,andtheinfer-
1 S
enceprocessbecomes:
√
q (x |x ,x ) = N(x ; α¯ x
σ,τ τi−1 τt 0 √τi−1 t−1 0
(cid:113) x − α¯ x
+ 1−α¯ −σ2 τ √i t 0 ,σ2I)
t−1 t 1−α¯ t
t
which,significantlyreducesinferencelatency.