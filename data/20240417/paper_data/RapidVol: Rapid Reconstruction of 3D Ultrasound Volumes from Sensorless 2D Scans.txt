RapidVol: Rapid Reconstruction of 3D
Ultrasound Volumes from Sensorless 2D Scans
Mark C. Eid1,2,* Pak-Hei Yeung2 Madeleine K. Wyburd2
Joa˜o F. Henriques1 Ana I.L. Namburete2
1Visual Geometry Group, University of Oxford
2Oxford Machine Learning in NeuroImaging Lab, University of Oxford
*markeid@robots.ox.ac.uk
Abstract
Two-dimensional (2D) freehand ultrasonography is one of the most com-
monly used medical imaging modalities, particularly in obstetrics and
gynaecology. However, it only captures 2D cross-sectional views of in-
herently 3D anatomies, losing valuable contextual information. As an
alternative to requiring costly and complex 3D ultrasound scanners, 3D
volumescanbeconstructedfrom2Dscansusingmachinelearning. How-
ever this usually requires long computational time. Here, we propose
RapidVol: aneuralrepresentationframeworktospeedupslice-to-volume
ultrasound reconstruction. We use tensor-rank decomposition, to decom-
posethetypical3Dvolumeintosetsoftri-planes,andstorethoseinstead,
as well as a small neural network. A set of 2D ultrasound scans, with
theirgroundtruth(orestimated)3Dpositionandorientation(pose)isall
that is required to form a complete 3D reconstruction. Reconstructions
are formed from real fetal brain scans, and then evaluated by requesting
novel cross-sectional views. When compared to prior approaches based
on fully implicit representation (e.g. neural radiance fields), our method
is over 3x quicker, 46% more accurate, and if given inaccurate poses is
more robust. Further speed-up is also possible by reconstructing from a
structural prior rather than from scratch.
Keywords: 3D Reconstruction • Ultrasound • Tensor Decomposition •
NeRF
4202
rpA
61
]VI.ssee[
1v66701.4042:viXraM. Eid et al.
1 Introduction
Two-dimensional (2D) freehand ultrasonography is routinely used in prenatal
checkups, as well as to image other organs. It is affordable, provides instant
visualisation, can be repeated numerously due to lack of ionising radiation, and
canevenbecarriedoutfromasmartphone. However,whilstmagneticresonance
imaging (MRI) and computerised tomography (CT) scans capture and store
inherently3Dstructureswithinthebodyas3Dvolumes,thesameisnottruefor
freehand ultrasound (US). Three-dimensional (3D) US scans do exist, and have
several clinical benefits over 2D methods, such as improved detection of cleft
lip[1,6,8],andprovidinggreaterdiagnosticaccuracyirrespectiveofsonographer
experience [10,17]. One study quantified this as a 60.8% improvement [11]. To
acquire these 3D US scans, native 3D scanners can be used, however they are
stillnotroutinelyusedastheyare∼10timesmoreexpensivethanstandardUS
probes and are bulkier [24]. Operators also require additional training.
Analternativetomovingto3Dscanners, istoreconstruct3Dvolumesfrom2D
freehandUSscans, acquiredfromstandardsensorlessprobes. Yeung. etal.[24]
proposed a framework, ImplicityVol, to implicitly construct a 3D volume of a
fetal brain using a series of freehand 2D US scans, which can also be applied to
scans of other fetal or adult organs. Currently, ImplicitVol [24] takes O(hours)
to reconstruct a 3D brain, however ideally this would be of O(minutes) so that
acquisition and reconstruction (as well as analysis of it by a clinician) can take
placetogetherwithinthesameappointment. This paper therefore presents
anewrepresentationmethod, termedRapidVol, whichissignificantly
more accurate (up to 46%), and forms the reconstructed 3D brain
from inputted 2D freehand ultrasound scans 3 times quicker.
2 Preliminary
Traditionally, 3D scenes are represented purely explicitly, where the entire 3D
volume/sceneisstoredasagridofvoxels[18,20]. Morelightweightversionscan
usesparseoroctreevoxelgrids,ormeshes[19]. Amoderntechniqueistoinstead
store the scene implicitly, by saving a fully trained neural network [7,9,12,26].
More recently, hybrid methods have been proposed, combining and drawing
upon the benefits of both pure explicit and implicit representation [3,4,21,25].
Explicit Representation: The 3D brain can be represented discretely as a
cost volume V ∈ RH×W×D×C, a tensor of height H, width W, depth D and
channels C. A 3D US volume will have C = 1 as US is a grayscale modality,
colour objects will have C = 3. If there are any voxels which were not seen in
any of the 2D images used for reconstruction, tri-linear interpolation, nearest
neighbour, or spline fitting can be used to predict these missing voxel values
[5,13].
2RapidVol
Implicit Representation: Based of NeRF [12], ImplicitVol [24] instead rep-
resents the 3D brain volume V ∈ RH×W×D×1 as a trainable deep neural net-
work, F : x → c, where Θ denotes the learnable parameters, x the (x,y,z)
Θ
co-ordinates of any point within the 3D brain, and c the intensity value. By
predictingintensitycatagiveninputvoxellocationx,thediscrepancybetween
c and the actual intensity at x can be computed as the loss to then update Θ
through back-propagation.
Hybrid Implicit-Explicit Representation: Implicit and explicit represen-
tation models have contrasting benefits and limitations. For example, explicit
models scale cubically with resolution, but are quicker to train. Recent works
such as TensoRF [4] and EG3D [3] attempt to bridge these two approaches and
harness the benefits of both. As shown in Fig. 1, they use tensor rank decom-
positiontoinsteadstoretheH×W×D×C costvolumemuchmorecompactly
as C sets of three 1D Vectors (CP Decomposition [2]), or C sets of three 2D
planes (“Tri-Planar Decomposition”) as in EG3D. Only a very small MLP is
thenneededtodecodefromC channelsto1/3colourchannel(s). Byusingamix
of explicit and implicit representation techniques, storage reduces from O(n3)
to O(n2) and O(n) for Tri-Planar and CP Decomposition respectively. More
importantly, the speed benefits associated with explicit representation are still
maintained,andinCPDecompositionitisimproved(asforeachtrainingimage,
(H +W) rather than (H ×W) parameters have to now be updated). Linear
and bilinear interpolation can also be quickly applied within each individual
1D vector (CP Decomposition) or 2D plane (Tri-Planar Decomposition). This
compensates for the lack of continuity when using pure explicit representation.
WethusadaptTensoRF[4]andEG3D[3]fromtheoriginalnaturalscenesetting
to the medical imaging setting, specifically on fetal brains. Challenges that
must be addressed include the relative lack of training data, the much higher
intricacies present in the brain as opposed to the Synthetic-NeRF dataset [12],
and the need for high accuracy and interpretability for clinical deployment.
Figure1: Diagramshowingthetwodifferenttypesoftensordecompositionemployed,
and how the value of a voxel in T can be retrieved.
3M. Eid et al.
3 Methodology
3.1 Problem Setup
Inamedicalsetting,wehaveastackΠofN 2Dultrasoundimages(Π={I }N ),
i i=1
allofwhicharedifferentcross-sectionsbutofthesameinherently3Dfetalbrain
(seeFig.2). Eachcross-sectionalimagehasaknownposeΛ relativetothecen-
i
tre of the 3D brain, which can be parameterised by 3 Euler angles (E) and 3
Translations (T). Our goal is to reconstruct the 3D brain such that a 2D cross-
section can be viewed at any specified pose Λ, and at any resolution. The final
output of RapidVol appears to be a high-resolution cost volume V, however it
is actually a set of tri-planes or tri-vectors, and a small MLP.
Figure 2: Pipeline of our proposed method RapidVol. During the reconstruction
process, a set of Π images and their corresponding poses Λ are required as input.
Oncethebrainisreconstructed,onlytheposeΛatwhichonewishestoseeisrequired
as input, and parameters are not updated.
Nb. Ultrasound probe image adapted from Flaticon.com.
3.2 Tensor Decompositions
Tri-Planar Decomposition: Given a 3D Tensor T ∈ RI×J×K, Tri-Planar
Decomposition factorises T such that for a given ijk index position within T:
R
(cid:88)
T = PXY ◦PYZ ◦PXZ (1)
ijk r,ij r,jk r,ik
r=1
where ◦ is either the sum (shown in Fig. 1) or product. In preliminary results,
we found that the product performs better, so exclusively used it throughout
this work. PXY ∈ RI×J, PYZ ∈ RJ×K, PXZ ∈ RI×K are the factorised
r r r
2D planes for the rth component. R is the rank of the decomposition, and is
a user-selected hyper-parameter. Increasing R improves the accuracy of the
decomposition, but at the expense of memory and computation time. Explicit
representation utilises a 4D Tensor V ∈ RH×W×D×C with C ≥ 1. However
since C << H,W,D, for simplicity the Channel dimension is not decomposed
4RapidVol
and instead Eq. (1) is repeated C times. Eq. (1) can also be evaluated at
non-integer ijk indices by bilinearly interpolating within each tri-plane.
3.2.1 CP Decomposition:
Given the same 3D Tensor T ∈ RI×J×K, CP Decomposition factorises T such
that for a given ijk index:
R
(cid:88)
T = vX vY vZ (2)
ijk r,i r,j r,k
r=1
where vX ∈ RI,vY ∈ RJ,vZ ∈ RK are the factorised 1D vectors for the
r r r
rth component. As before, R is the rank and is user-selected, and Eq. (2) is
repeatedforeachChannelinV. Non-integerindicescanalsobeusedbylinearly
interpolating. Both Eqs. (1) and (2) can be visualised in Fig. 1.
3.3 Pipeline of RapidVol
To form V from a set of 2D ultrasound freehand scans (Π) with known poses
Λ, RapidVol undergoes the following steps (as shown in Fig. 2):
1. The pose (parameterised by E and T) of each image is used to construct
a grid G = {x }n = {x ,y ,z }n of a nominated resolution, which contains
i i=1 i i i i=1
the3Dco-ordinatesofallnpixelswhichlieonthatcross-sectionalplane/image.
2. G is fed into our reconstruction model, which performs either Eq. (1)
or Eq. (2) on each x . This in turn produces a grid of the same shape as G,
i
but now with C channels rather than 1. Positional encoding (see Appendix A
or [12]) is then applied on this grid, before being fed into the MLP.
3. A lightweight, trainable MLP decodes the positionally encoded grid of C
channels to 1 channel, resulting in a grayscale image of the fetal brain at the
requested pose.
4. Theloss, chosentobethenegativeStructuralSimilarityIndexMeasure
(SSIM) [22] between the rendered image and the ground truth image, is com-
puted. Using standard back-propagation the tri-planes (or tri-vectors) are then
refined, as is the decoder MLP and optionally the poses of the training images.
Oncethebrainisreconstructed,cross-sectionalviewsatanyposecanbeviewed
simply by specifying the pose and performing steps 1-3 above.
4 Experimental Setup
4.1 Technical Details
Dataset: Throughout, we use a set of 3D ultrasound fetal brain scans at 20
gestational weeks of size 1603 voxels, with a resolution of 0.6 mm3 isotropic,
collected as part of the INTERGROWTH-21st study [16]. In practice, only
a set of N freehand 2D images are required for reconstruction. However to
5M. Eid et al.
evaluate the accuracy of our reconstruction method, we also require a 3D scan
of the fetus so that we can compare an arbitrary plane from the reconstructed
volume to the ground truth (extracted from the 3D scan). For each fetus we
only had a 3D scan, rather than a 3D scan and a series of freehand images, so
we had to mimic the latter. This was done by sampling N linearly spaced axial
images from the 3D scan, which simulates the ultrasound probe moving along
thestomachfromheadtotoeduringatypicalprenatalscanningsession. Πwas
then those N images, and Λ was their (ground truth) poses. When evaluating
the accuracy of our reconstruction, views at those poses were naturally not
requested.
Implementation Details: We implement our framework in PyTorch. Unless
otherwise stated, the tri-planes/tri-vectors are randomly initialised. A Stochas-
tic Gradient Descent (SGD) optimiser with a learning rate (lr) of 0.5 is used to
refine the decomposition/reconstruction model, and a SGD with a lr of 0.001
for the decoder MLP. Training is done for 5,000 epochs.
4.2 Experiments Performed
Ablation Study: We first performed an ablation study to find the optimum
typeofdecomposition(Tri-PlanarorCP),MLP(seeFig.2),aswellasthedegree
topositionallyencodeupto,andwhethertherawinputshouldbeconcatenated
with the encoded input or not. The MLPs ablated over all had input size
C, output size 1, and were two to four layers, with hidden layers being of
size {32,64,128}. ReLU activation functions were used, except on the final
layer which had a sigmoid activation function to constrain the output to be a
grasyscale value between 0.0 and 1.0.
Performance Comparison: We then compared the performance, both in
terms of reconstruction accuracy and speed, of our method to a state-of-the-art
reconstruction method, ImplicitVol [24], on 15 3D fetal brains. Two different
setsofinputscansΠwereused: ThefirstwasasetofN ={128,256}axialslices
as previously described (Π ), the second was a set of N = {128,256} coronal
1
slices uniformly rotated 360◦ about the Vertical Axis of the brain (Π ). This is
2
to simulate the ultrasound probe being rotated by hand as is often done in a
prenatalscan,andisthesamedatasetusedin[24]. Reconstructionaccuracywas
quantified by requesting views of N ={128,256} linearly spaced axial, coronal
and sagittal slices.
Training from an Atlas: Additionally, we investigated whether the recon-
struction of a 20 week old fetal brain could be sped up by initialising the tri-
planes/tri-vectors from an atlas of the same age, rather than from random
values. This atlas was a pre-computed reconstruction of multiple fetal brains.
All fetuses were of the same age, and the 3D scans of their brains were deemed
to be of high enough quality to be used in constructing a digital atlas for that
gestational week [15].
6RapidVol
Use of Inaccurate Poses: Ordinarily, we mimicked the set of input freehand
scans Π by sampling N images from a 3D scan, and so we knew their ground
truth poses which is what we set Λ to be. However in practice, the poses of
the freehand images are unknown, so we require methods like PlaneInVol [23]
to predict them. Unfortunately neither the dataset used to train PlaneInVol
nor a fully trained version of PlaneInVol is publicly available. In this experi-
ment, we instead set Λ to be the ground truth poses plus some random noise
(X∼U(−3,3)),tosimulatetheinevitablyinaccurateposeestimationofPlaneIn-
Vol. Theposeswerealsosettobelearnable,soastrainingprogressed,theposes
could tend towards their ground truth values and reconstruction became more
accurate. The poses were refined using an Adam optimiser with a learning rate
of 0.001. Optimisation of the poses was done jointly with optimisation of the
tri-planes and MLP, as in [24].
5 Results and Discussion
Ablation Study: CP decomposition, although quicker and more memory effi-
cient than Tri-Planar Decomposition, was found to be too simple to accurately
reconstruct the intricate 3D fetal brain (see Appendix B). A two-layer MLP
with a hidden layer size of 64 (as used in [14]), alongside positional encoding
to degree L=2 and concatenating the raw input with the encoded input, per-
formed the best (see Appendix B). For this MLP, it was found that the best
compromisebetweenaccuracy, speedandmemorystoragewaswhenTri-Planar
DecompositionwasperformedwithR=5andC =10. Thereforeallsubsequent
experiments used Tri-Planar Decomposition with these parameters.
Performance Comparison: Table 1 shows the reconstruction performance
of RapidVol compared to current state of the art, ImplicitVol. If Π is a series
of scans acquired by rotating an US probe by hand (Π ), then our method is
2
more accurate than ImplicitVol at generating novel cross-sectional views, but
only marginally so. Nevertheless it still remains significantly quicker. However
if it is instead given a series of axial scans (Π ), then our method is just as
1
quick as with Π , but also on average 41% more accurate. RapidVol therefore
2
prefers if US scans are acquired through longitudinal sweeps as opposed to
Table1: Evaluationof-SSIMaccuracyresultsattesttimeforourmethod(RapidVol)
vsthebaseline(ImplicitVol)on15,20weekfetalbrains. MorenegativeSSIMisbetter.
N=128TestingSlices N=256TestingSlices
Π Method Axial Coronal Sagittal Axial Coronal Sagittal
RapidVol N/A -0.955±0.008-0.952±0.009 N/A -0.973±0.005-0.969±0.006
Axial
ImplicitVol N/A -0.689±0.038-0.700±0.046 N/A -0.677±0.040-0.662±0.037
360◦ RapidVol -0.941±0.016-0.932±0.018-0.935±0.017-0.952±0.015-0.950±0.016-0.950±0.014
CoronalImplicitVol-0.909±0.021-0.901±0.023-0.905±0.022-0.912±0.020-0.904±0.023-0.908±0.022
7M. Eid et al.
rotational sweeps of the probe, both of which are readily available. Regardless
oftheacquisitiontechnique,ourmethodperformsbetterwhengivenmoreinput
scans. Since 2D US acquires images in real-time, collecting hundreds of images
is quick and effortless, so the quality of the 3D reconstruction can easily be
improved. Finally, our method generates novel views equally well in all three
orthogonal planes, regardless of the acquisition technique used. Their accuracy
is also much less variable from fetus to fetus than ImplicitVol (c.f standard
deviations in Table 1).
All runs were conducted on a shared high-performance cluster, however pre-
emption by other processes often occured. Therefore to ensure a fair timing
comparison between RapidVol and ImplicitVol, a selection of these runs were
made on a much slower, but isolated GPU (GTX TITAN X). The results of
the relative speed up, as well as the training profiles, can be seen in Fig. 3,
showing that RapidVol is 3 times quicker than ImplicitVol per epoch. This
significant speed-up, coupled with the accuracy improvements discussed above,
makes for a very powerful reconstruction model. This experiment was run on
a slow GPU, so upon deployment onto an isolated modern GPU, the absolute
reconstruction/training time will be in the order of minutes as opposed to over
1 hour as shown in Fig. 3.
Figure 3: Testing accuracy profile curves. Training was done to 5,000 epochs, accu-
racy reported every 250 epochs. For consistent timings, these runs were done on an
isolated but much slower GPU. Π = 160 axial slices, Testing dataset = 160 coronal
slices rotated 360◦ about the Vertical Axis.
8RapidVol
TrainingfromanAtlas: Fig.3showsthatinitialisingfromapre-trainedfetal
brainatlasasopposedtorandomlyhasnoeffectonreconstructionspeed. Itdoes
however result in a far quicker rate of convergence, which is especially useful if
the reconstruction process is required to be terminated after a certain duration
(e.g. a30minuteprenatalappointment). Tocreateareconstructedbrainwitha
respectableSSIMof0.9,initialisingRapidVolfromanatlasallowsyoutoachieve
this 2.7 times quicker than if RapidVol was initialised from random values. We
therefore recommend all fetal brains are reconstructed using RapidVol which
has been pre-initialised from a digital fetal atlas of the same age (these digital
atlases are publicly available for most gestational ages [15]).
Use of Inaccurate Poses: Table 2 shows that as expected, inaccurate poses
leadtopoorerreconstructionaccuracy,regardlessofthemethod. Howeverwhen
faced with inaccurate poses, which in practice will always be the case to some
extent, our method is up to 35% more accurate than the baseline. Even when
a series of parallel sagittal poses are requested, we still perform 28% better,
demonstrating the robustness of our proposed method.
Table2: -SSIMaccuracyresultsattesttime,whenourmethoddoesordoesnothave
accurate initial poses. Π = 256 coronal slices rotated 360◦ about the Vertical Axis.
Testing dataset = 256 {axial, coronal, sagittal} slices. More negative SSIM is better.
Poses Method Axial Coronal Sagittal
RapidVol -0.952 -0.950 -0.950
GroundTruth
ImplicitVol -0.912 -0.904 -0.908
RapidVol -0.812 -0.790 -0.790
Estimated
ImplicitVol -0.599 -0.596 -0.616
6 Conclusion
We propose RapidVol, a hybrid implicit-explicit representation method util-
ising Tri-Planar Decomposition to rapidly reconstruct 3D ultrasound volumes
from sensorless 2D scans. When compared to current state of the art, our
method is up to 3x quicker, up to 46% more accurate, and even if faced with
slightly inaccurate poses is still robust, performing on average 32% better that
other methods do in that scenario. Reconstructing from a fetal atlas can also
offer further speed-up. RapidVol helps to bring us one step closer to more effi-
cient and accurate medical diagnostics, especially in settings where only basic
2D US probes are available but 3D views would be clinically beneficial.
Acknowledgements. The authors acknowledge the generous support of the
EPSRC Centre for Doctoral Training in Autonomous Intelligent Machines &
Systems (EP/S024050/1), Amazon Web Services, EPSRC Impact Acceleration
AccountAward,EPSRCDoctoralPrizeScheme,RoyalAcademyofEngineering
(RF\201819\18\163), and the Bill & Melinda Gates Foundation.
9M. Eid et al.
References
[1] Campbell, S.: Prenatal ultrasound examination of the secondary
palate. Ultrasound in Obstetrics & Gynecology 29(2), 124–127 (2007).
https://doi.org/10.1002/uog.3954
[2] Carroll, J.D., Chang, J.J.: Analysis of individual differences in
multidimensional scaling via an n-way generalization of “Eckart-
Young” decomposition. Psychometrika 35(3), 283–319 (Sep 1970).
https://doi.org/10.1007/BF02310791
[3] Chan, E.R., Lin, C.Z., Chan, M.A., Nagano, K., Pan, B., De Mello, S.,
Gallo, O., Guibas, L., Tremblay, J., Khamis, S., Karras, T., Wetzstein,
G.: Efficient Geometry-aware 3D Generative Adversarial Networks. In:
2022 IEEE/CVF Conference on Computer Vision and Pattern Recogni-
tion(CVPR).pp.16102–16112.IEEE,NewOrleans, LA,USA(Jun2022).
https://doi.org/10.1109/CVPR52688.2022.01565
[4] Chen, A., Xu, Z., Geiger, A., Yu, J., Su, H.: TensoRF: Tensorial Radiance
Fields (Nov 2022), arXiv:2203.09517 [cs]
[5] Chen, A., Xu, Z., Zhao, F., Zhang, X., Xiang, F., Yu, J., Su, H.: MVS-
NeRF:FastGeneralizableRadianceFieldReconstructionfromMulti-View
Stereo (Aug 2021), arXiv:2103.15595 [cs]
[6] Chen, M.L., Chang, C.H., Yu, C.H., Cheng, Y.C., Chang, F.M.:
Prenatal diagnosis of cleft palate by three-dimensional ultrasound.
Ultrasound in Medicine and Biology 27(8), 1017–1023 (Aug 2001).
https://doi.org/10.1016/S0301-5629(01)00403-3, publisher: Elsevier
[7] Flynn, J., Broxton, M., Debevec, P., DuVall, M., Fyffe, G., Overbeck, R.,
Snavely,N.,Tucker,R.: DeepView: ViewSynthesiswithLearnedGradient
Descent (Jun 2019), arXiv:1906.07316 [cs, eess]
[8] Gon¸calves, L.F.: Three-dimensional ultrasound of the fetus: how
does it help? Pediatric Radiology 46(2), 177–189 (Feb 2016).
https://doi.org/10.1007/s00247-015-3441-6
[9] Henzler, P., Rasche, V., Ropinski, T., Ritschel, T.: Single-image Tomogra-
phy: 3D Volumes from 2D Cranial X-Rays (Nov 2018), arXiv:1710.04867
[cs]
[10] Huang, Q., Zeng, Z.: A Review on Real-Time 3D Ultrasound Imag-
ing Technology. BioMed Research International 2017, 1–20 (2017).
https://doi.org/10.1155/2017/6027029
10RapidVol
[11] Merz, E., Pashaj, S.: Advantages of 3D ultrasound in the assessment of
fetal abnormalities. Journal of Perinatal Medicine 45(6), 643–650 (Aug
2017). https://doi.org/10.1515/jpm-2016-0379, publisher: De Gruyter
[12] Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi,
R.,Ng,R.: NeRF:RepresentingScenesasNeuralRadianceFieldsforView
Synthesis (Aug 2020), arXiv:2003.08934 [cs]
[13] Mohamed, F., Vei Siang, C.: A Survey on 3D Ultrasound Reconstruc-
tion Techniques. In: Antonio Aceves-Fernandez, M. (ed.) Artificial Intel-
ligence - Applications in Medicine and Biology. IntechOpen (Jul 2019).
https://doi.org/10.5772/intechopen.81628
[14] Mu¨ller,T.,Evans,A.,Schied,C.,Keller,A.: Instantneuralgraphicsprimi-
tiveswithamultiresolutionhashencoding.ACMTransactionsonGraphics
41(4), 1–15 (Jul 2022). https://doi.org/10.1145/3528223.3530127
[15] Namburete, A.I.L., Papiez˙, B.W., Fernandes, M., Wyburd, M.K., Hesse,
L.S., Moser, F.A., Ismail, L.C., Gunier, R.B., Squier, W., Ohuma, E.O.,
Carvalho, M., Jaffer, Y., Gravett, M., Wu, Q., Lambert, A., Winsey, A.,
Restrepo-M´endez, M.C., Bertino, E., Purwar, M., Barros, F.C., Stein, A.,
Noble,J.A.,Moln´ar,Z.,Jenkinson,M.,Bhutta,Z.A.,Papageorghiou,A.T.,
Villar, J., Kennedy, S.H.: Normative spatiotemporal fetal brain matura-
tion with satisfactory development at 2 years. Nature pp. 1–9 (Oct 2023).
https://doi.org/10.1038/s41586-023-06630-3,publisher: NaturePublishing
Group
[16] Papageorghiou, A.T., Ohuma, E.O., Altman, D.G., Todros, T., Ismail,
L.C., Lambert, A., Jaffer, Y.A., Bertino, E., Gravett, M.G., Purwar, M.,
Noble, J.A., Pang, R., Victora, C.G., Barros, F.C., Carvalho, M., Sa-
lomon, L.J., Bhutta, Z.A., Kennedy, S.H., Villar, J.: International stan-
dards for fetal growth based on serial ultrasound measurements: the Fetal
Growth Longitudinal Study of the INTERGROWTH-21st Project. The
Lancet 384(9946), 869–879 (Sep 2014). https://doi.org/10.1016/S0140-
6736(14)61490-2, publisher: Elsevier
[17] Pistorius, L.R., Stoutenbeek, P., Groenendaal, F., de Vries, L., Manten,
G., Mulder, E., Visser, G.: Grade and symmetry of normal fetal cor-
tical development: a longitudinal two- and three-dimensional ultrasound
study. Ultrasound in Obstetrics & Gynecology 36(6), 700–708 (2010).
https://doi.org/10.1002/uog.7705
[18] Seitz, S., Dyer, C.: Photorealistic scene reconstruction by voxel coloring.
In: ProceedingsofIEEEComputerSocietyConferenceonComputerVision
and Pattern Recognition. pp. 1067–1073. IEEE Comput. Soc, San Juan,
Puerto Rico (1997). https://doi.org/10.1109/CVPR.1997.609462
11M. Eid et al.
[19] Shalma, H., Selvaraj, P.: A review on 3D image reconstruction on specific
and generic objects. Materials Today: Proceedings 80, 2400–2405 (Jan
2023). https://doi.org/10.1016/j.matpr.2021.06.371
[20] Szeliski, R., Golland, P.: Stereo matching with transparency and mat-
ting. In: Sixth International Conference on Computer Vision (IEEE Cat.
No.98CH36271). pp. 517–524. Narosa Publishing House, Bombay, India
(1998). https://doi.org/10.1109/ICCV.1998.710766
[21] Tang, J., Chen, X., Wang, J., Zeng, G.: Compressible-composable NeRF
via Rank-residual Decomposition (Oct 2022), arXiv:2205.14870 [cs]
[22] Wang, Z., Bovik, A., Sheikh, H., Simoncelli, E.: Image Qual-
ity Assessment: From Error Visibility to Structural Similarity.
IEEE Transactions on Image Processing 13(4), 600–612 (Apr 2004).
https://doi.org/10.1109/TIP.2003.819861
[23] Yeung, P.H., Aliasi, M., Papageorghiou, A.T., Haak, M., Xie, W., Nambu-
rete,A.I.L.: Learningtomap2Dultrasoundimagesinto3Dspacewithmin-
imal human annotation. Medical Image Analysis 70, 101998 (May 2021).
https://doi.org/10.1016/j.media.2021.101998
[24] Yeung, P.H., Hesse, L., Aliasi, M., Haak, M., Consortium, t.I.s., Xie, W.,
Namburete, A.I.L.: ImplicitVol: Sensorless 3D Ultrasound Reconstruction
with Deep Implicit Representation (Sep 2021), arXiv:2109.12108 [cs, eess]
[25] Yu, A., Fridovich-Keil, S., Tancik, M., Chen, Q., Recht, B., Kanazawa,
A.: Plenoxels: Radiance Fields without Neural Networks (Dec 2021),
arXiv:2112.05131 [cs]
[26] Zhou, T., Tucker, R., Flynn, J., Fyffe, G., Snavely, N.: Stereo Magni-
fication: Learning View Synthesis using Multiplane Images (May 2018),
arXiv:1805.09817 [cs]
12RapidVol
Appendix A - Positional Encoding
Each element in feature vector p is encoded from R to R2L+1 by the following
function:
γ(p)=(p, sin(20πp), cos(20πp), ..., sin(2L−1πp), cos(2L−1πp)) (A1)
The degree of encoding (ie value of L), and whether or not to include p itself in
γ(p), are application specific choices.
Appendix B - Ablation Study
Table B1: Comparison between possible RapidVol setups. L = positional encoding
degree. “+ input” = concatenate raw input with enocded input. Tri-Planar and CP
Decomposition are both with Rank R=5, Channels C =10. Reconstruction is from
160linearlyspacedaxialimages. Testingdatasetis160linearlyspacedcoronalslices.
Values shown are reconstruction accuracy as measured with Negative SSIM (more
negativeisbetter). Thebestresultforeachnetworkisshowninbold,withtheoverall
best performance across the networks underlined. Network names are of the form
“MLP n-w”. This stands for a Multilayer Perceptron (MLP) network with n fully
connected layers, and hidden layer(s) all of width w. ReLU activation functions are
used throughout, except for the last layer which has a sigmoid activation function.
L=2 L=5 L=10
Network L=0 L=2 L=5 L=10 +input +input +input
MLP2-128 -0.9568 -0.9717 -0.2573 -0.2630 -0.9733 -0.2608 -0.9303
MLP3-128 -0.9647 -0.9743 -0.2591 -0.3055 -0.9748 -0.2623 -0.9243
MLP4-128 -0.9709 -0.9745 -0.2617 -0.8803 -0.9750 -0.2635 -0.9403
MLP3-32 -0.9641 -0.9744 -0.2567 -0.9341 -0.9758 -0.2636 -0.9227
MLP3-64 -0.9654 -0.9738 -0.2583 -0.8738 -0.9743 -0.2632 -0.9307
MLP2-64 -0.9562 -0.9729 -0.2556 -0.8448 -0.9764 -0.2609 -0.9167
CP+MLP2-64 -0.8544 -0.8475 -0.8265 NaN -0.8512 -0.8392 -0.508
13
ranalP-irT