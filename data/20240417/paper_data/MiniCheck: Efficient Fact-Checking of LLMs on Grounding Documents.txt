MiniCheck: Efficient Fact-Checking of LLMs on Grounding Documents
LiyanTang♢ PhilippeLaban♠ GregDurrett♢
♢TheUniversityofTexasatAustin ♠SalesforceAIResearch
lytang@utexas.edu
Abstract Fixed-Doc Genera/on Retrieve-Then-Generate Post-Hoc Grounding
(e.g., TofuEval) (e.g., ClaimVerify) (e.g., Factcheck-GPT)
RecognizingifLLMoutputcanbegrounded Summarize the
What/Why/… ? What/Why/… ?
document(s):
in evidence is central to many tasks in NLP:
retrieval-augmented generation, summariza- retrieved The answer is…
web docs
tion,document-groundeddialogue,andmore.
Current approaches to this kind of “fact- The doc According to [1], retrieved docs
describes… the answer is … to verify answer
checking” are based on verifying each piece
of a model generation against potential evi-
Fact-Checking on Grounding Documents
dence using an LLM. However, this process
This work: generate
canbeverycomputationallyexpensive,requir- Fact-Check ( , ) synthe/c data to train a
ingmanycallstoLLMstocheckasinglere- sentences sentence-level fact-checker
sponse. In this work, we show how to build
small models that have GPT-4-level perfor- Figure 1: We unify the task of fact-checking across
mance but for 400x lower cost. We do this varioussettingsthatrelyongroundingdocuments. We
by constructing synthetic training data with trainasmallsentence-levelfact-checkerbyleveraging
GPT-4, which involves creating realistic yet newsyntheticallygenerateddata,whichdemonstrates
challenging instances of factual errors via a strongperformanceonanewunifiedbenchmarkLLM-
structuredgenerationprocedure. Trainingon AGGREFACT,comparabletoGPT-4but400xcheaper.
thisdatateachesmodelstocheckeachfactin
theclaimandrecognizesynthesisofinforma-
tionacrosssentences. Forevaluation,weunify
et al., 2023). Different but related errors occur
pre-existingdatasetsintoabenchmarkLLM-
ingroundedgenerationsettingswhereevidenceis
AGGREFACT, collected from recent work on
alreadyavailable,likesummarizationofinputdoc-
fact-checkingandgroundingLLMgenerations.
Our best system MiniCheck-FT5 (770M pa- umentsorretrieval-augmentedquestionanswering,
rameters) outperforms all systems of compa- whereanLLMcanblendinformationincorrectly
rable size and reaches GPT-4 accuracy. We (Liuetal.,2023;Tangetal.,2024).
releaseLLM-AGGREFACT,codefordatasyn-
Pastworkhaslargelydealtwiththeseproblems
thesis,andmodels.1
separately. When post-hoc verifying the content
1 Introduction of closed-book generated answers, a separate re-
trievalstageisneeded(Gaoetal.,2023;Malaviya
Freeformgenerationofresponsesisaflexibleway
et al., 2024; Jacovi et al., 2024), and many state-
toemploylargelanguagemodels(LLMs)forques-
mentscannotbesourcedormayhaveconflicting
tionanswering,summarization,andbeyond. How-
informationavailable(Wangetal.,2023;Glockner
ever, this kind of generation can lead to factual
et al., 2024). Grounding statements in these re-
errors,the“hallucination”probleminLLMs(Falke
trieveddocumentsaddsanotherlayerofdifficulty.
etal.,2019;Maynezetal.,2020;McKennaetal.,
In contrast, when the grounding is known in set-
2023; Zhang et al., 2023a). Such errors arise in
tingslikesummarization,theattributionproblemis
generation settings where an LLM is prompted
oftenframedasdocument-leveltextualentailment
closed-book,butitsparametricknowledgemaybe
(Nie et al., 2020; Yin et al., 2021) and has been
insufficient to produce the right facts (Min et al.,
studied extensively for smaller language models
2023;Mallenetal.,2023;Zhouetal.,2023;Chen
(Falkeetal.,2019;GoyalandDurrett,2020,2021;
1Availableathttps://github.com/Liyan06/MiniCheck. Labanetal.,2022;Tangetal.,2023a).
4202
rpA
61
]LC.sc[
1v47701.4042:viXraTheproblemsinthisspacehaveasharedprim- onclosed-bookandgroundedgenerationsettings.
itive operation: the need to check a statement (3)EvaluationshowsthatourMiniChecksystem
against grounding documents, either retrieval- can beat previous specialized systems by 4% to
augmentedcontentorpost-hocretrievedevidence. 10% in absolute values, despite using less fine-
Wecallthisprimitivefact-checkingongrounding tuningdata,andisonparwithGPT-4withamuch
documents,showninFigure1. Implementations smallermodelsize,fasterinferencespeed,and400
ofthisprimitiveneedtobeaccurate,spottingsubtle timeslesscost. Furthermore,wecandothiswith-
errorswhilemaintainingalowfalsepositiverate,as outaseparateclaimdecompositionstep.
moststatementsareusuallycorrect. Theyalsoneed
2 BackgroundandMotivation
tobeefficient: asingleLLMresponsemaycontain
dozensoffactstoverify,andself-verificationwith
ProblemSetup: ClaimVerification Weassume
anLLMmayincreasecostbyanorderofmagni-
acollectionofstatementstobecheckedconsisting
tude(Wengetal.,2023;Geroetal.,2023). Forin-
ofsentencesc = [c ,...,c ]. Typically,thiswill
1 |c|
stance,the110-150wordbiographiesinFActScore
beasequenceofsentencesproducedbyanLLM.
(Minetal.,2023)contain26-41atomicfactsthat
Eachsentencec hasanassociatedsetofground-
i
arecheckedagainst5documentseach,resultingin
ing documents = D ,...,D . These
130-205entailmentchecks.
Di
{
i,1 i,|Di|
}
different per sentence accommodate post-hoc
i
Inthiswork,webuildanefficientsystemforfact- D
retrievalsettingswhereeachsentencehasdifferent
checkingongroundingdocuments. Ourkeyinsight
retrieved evidence; however, some settings may
istodevelopanewsynthetictrainingdatasetwhich
use shared evidence across all sentences or even
istailoredtothecomplexitiesofthefact-checking
asinglegroundingdocumentfortaskslikesingle-
task. UnlikestandarddistillationfromLLMs(Taori
documentsummarization(i.e.,all onlycontain
i
et al., 2023; Hsieh et al., 2023), this setting dif- D
thedocumentbeingsummarized).
fers in that we do not necessary have access to
Weviewthesesentencesasclaims. Ourgoalin
taskinstancesthatwecanlabelwithstrongLLMs.
thisworkistobuildasystemthatcanvalidateeach
Forinstance,indatasetslikeExpertQA(Malaviya
claim against the documents. Following Laban
etal.,2024),eventheinputstotheLLMareexpert-
etal.(2022),wedefineadiscriminator
writtenquestions. Asaresult,wesynthesizechal-
lenging fact-checking instances from the ground M(D ,c ) 0,1 ,
i,j i
up,asascalablewaytoteachasmallmodelhowto ∈ { }
simultaneouslyverifymultiplefactsinasentence that classifies each claim c into unsupported, 0,
i
againstmultiplesentencesingroundingdocuments. or supported, 1, according to a provided docu-
Oursystem,MiniCheck,isaninstanceofFlan-T5 mentD .2
i,j
(Chung et al., 2022) fine-tuned on this data plus Thisprocessmakestwoassumptions. First,we
standardentailmentdata(Nieetal.,2020). assume that each supported claim can be vali-
For our experiments, we introduce a new uni- datedagainstasingledocument;thatis,claims
fiedbenchmark,LLM-AGGREFACT,whichaggre- are “atomic enough”. Our methodology can be
gates10existingdatasetsforbothclosed-bookand generalizedtohandleclaimssupportedbymultiple
groundedgenerationsettings. Ineachconstituent documents by simply appending multiple docu-
dataset,sentence-levelfactualerrorsarelabeledby ments in the context of M, but we did not find it
humanannotators. WeshowthatMiniCheckcan necessaryinanyofthedatasetswestudied.
perform as well as GPT-4 in aggregate and sub- Second, we assume that we can perform our
stantiallyoutperformpastfine-tunedsystemslike entailmentchecksoneachsentencec onitsown,
i
AlignScore(Zhaetal.,2023). Moreover,wefind without context c . In general, sentences do
<i
thatdecompositionofsentencesintoatomicfacts, needcontexttobeunderstood(e.g.,mostsentences
whichhasbeenexploredinpastwork(Kamoietal., starting with pronouns), but this can be resolved
2023;Gaoetal.,2023;Wangetal.,2023), isnot throughtheuseofadecontextualizationstep(Choi
necessarytoachievethishighperformance. et al., 2021). Section 7 examines whether such a
Our contributions are as follows: (1) Two syn- stepimprovesperformanceofoursystem.
theticdatagenerationmethodstoaddressthechal-
2Followingpastwork(Kamoietal.,2023;Sanyaletal.,
lengesoffact-checkingongroundingdocuments.
2024),wedisregardtheusual“contradiction”classfromtex-
(2)Anewbenchmarkunifyingfactualevaluation tualentailment,ascontradictionsarerareinourbenchmark.Ar#cle 3 Methodology: TrainingDataSynthesis
… LIN: Well, and some airlines are going to say
that so many factors are out of their control: like Toaddressthesechallenges,newdataisrequired.
weather and now labor problems. … ExistingdatasetslikeMNLI(Williamsetal.,2018)
TRIPPLER: … I believe those are in their andANLI(Nieetal.,2020)donotfeatureinstances
control. Weather, I understand. Labor: Come on, thatreflectthecomplexityofLLMfact-checking.
airlines, let’s get it together. …
Annotation of real errors is challenging to scale;
datasets of such errors (including those in LLM-
Some airlines argue that factors like
weather and labor problems are beyond AGGREFACT)arelargelytest-only.
their control, but experts disagree. Our goal is to construct a dataset
Atomic Facts: (D ,c ,y ) N of N instances of documents
{ i i i }i=1
1. Some airlines argue that factors like D paired with claims c with label y 0,1 ,
i i i
weather are beyond their control. ∈ { }
usingtwonovelsyntheticdatagenerationmethods
2. Some airlines argue that factors like labor
(Figure 3). Statistics about our final synthetic
problems are beyond their control.
3. Experts disagree that factors like weather trainingdatacanbefoundinTable1. Additional
are beyond airlines' control. details, including the sources of claims and
4. Experts disagree that factors like labor documentsandexamplesofgenerateddata,canbe
problems are beyond airlines' control.
foundinAppendixD.Weprovideallpromptsand
Figure2: AnexampledialoguesnippetwithanLLM- qualityassurancedetailsinAppendixH.
generatedsummarysentence,fromtheTofuEvaldataset.
3.1 ClaimtoDoc(C2D)Generation
IntheC2Dmethod,weassumethatwehaveaccess
We judge a sentence c by taking
i to a set of human-written claim statements. The
max M(D ,c ): it is supported if and only if
j i,j i goalistogeneratesyntheticdocumentsthatrequire
thereexistssomedocumentthatsupportsit.
modelsbeabletocheckmultiplefactsintheclaim
againstmultiplesentenceseach.
Challenges of verification Two aspects of the
task make this process challenging. First, there
Step1: Claimdecomposition Givenaclaimc,
maybeseveralindividualfactsinD whichare
i,j we first decompose it into a set of atomic facts a
necessarytovalidateaclaimc . Forexample,the
i withGPT-3.5: Decomp(c) = a ,...,a .
1 l
LLM-generatedsentenceinFigure2canbebroken { }
down into four atomic facts. Each fact must be Step2: Atomicfactexpansion Fortheclaimc,
checkedeveniftheyarenotexplicitlymaterialized. we ask GPT-4 to generate a pair of sentences for
Second,andrelatedly,theclaim,orafactinthe eachofitsatomicfactswitha4-shotprompt:
claim, may require making inferences that span
SentPair(a ) = (s ,s ), i 1,...,l .
multiple sentences within D . In Figure 2, Lin i i,1 i,2
i,j ∀ ∈ { }
initially argues that airlines have no control over
The generated sentence pairs are designed such
eitherweatherorlaborissues. However,Trippler’s
thattheatomicfactissupportedifandonlyifthe
later statement, “Weather, I understand. Labor:
informationfrombothsentencesiscombined.
Come on, airlines, let’s get it together,”, implies
agreementthatweatherisuncontrollablebutsug- Step3: Supportingdocumentgeneration Af-
gests that labor problems are within the airlines’ ter expanding atomic facts a into sentences s =
control. Thisindicatesthatthethirdatomicfactis s ,s ,...,s ,s , we ask GPT-4 to gen-
1,1 1,2 l,1 l,2
{ }
unsupportedbythedocument. erate a document D that mentions all sentences
Wearguethatexistingspecializedfact-checkers fromthegeneratedsentencepairsinitsownwords
fallshortineffectivelyconsideringallatomicfacts D = PassageGen(s)withazero-shotprompt.3
withinaclaimtobeverifiedandstrugglewithrea- By following these steps, we create a triplet
soning across multiple sentences. Results in Ap- (D,c,y = 1). This procedure increases the diffi-
pendix A.1 support this characterization. To ad- cultyofthetaskbyensuringthatmultiple-sentence
dress this issue, we come up with two synthetic reasoningisrequiredtocorrectlyclassifyaclaim.
data generation methods (Section 3) to enhance
3WeaskGPT-4tonotstatededucedfactsorconclusions
themodels’abilityintheseareas. Wediscussthe
basedontheprovidedsentences,andwefindthatGPT-4can
relationtopriorworkinSection8. followthisinstructionwell.Ste Dp e1 c: o C mla pim S Fate cp t E2 x: pA at no sm ioic n Step 3,4: Doc Genera#on S Dt oe cp u 5 m: eS nu tb Pcl aa ii rm ins g- generate tuples (D a′ i\j,Merge(a′),1) if a i ∈/ a′,
indicatingthatthedocumentstillsupportsthesub-
PassageGen( )
claim absent the atomic fact a . Conversely, we PassageGen( ) i
O Cri lg ai in mal PassageGen( ) have (D a′ ,Merge(a′),0) if a i a′, suggesting
…… i\j ∈
that the document does not support the subclaim
A Chr u# nc kl 1e Step 2: Claim Decomp Step 4: Cross-Doc-
Claim Augmenta#on duetotheabsenceofa i.
Chunk 2 Su Cm hm una kri z 2e Becausethesamesubclaimissupportedbycer-
Step 1: Chunk-Level Chunk 1 taindocumentsandunsupportedbyothersdepend-
Chunk 3 Summariza#on Chunk 2
ingonthepresenceorabsenceofspecificatomic
Chunk 2 Chunk 2 Chunk 3
Step 3: Doc- Abla#on 1 Abla#on 2 Chunk 2 Abl. 1 facts,weachievethesamebenefitsthattrainingon
Claim Abla#on Chunk 2 Abl. 2 contrast sets provides (Cao and Wang, 2021; Liu
Figure3: Oursyntheticdatagenerationpipeline: C2D et al., 2022; Tang et al., 2023b), namely making
(upper)andD2C(lower). Weillustratewithaclaimthat themodelmoresensitivetothespecificsofthede-
containstwoatomicfacts. Examplesofgenerateddata cisionboundaryandencouragingittoconsiderall
canbefoundinAppendixD.
atomicfactswithinaclaimduringprediction.
Step 4: Nonsupporting document generation 3.2 DoctoClaim(D2C)Generation
By construction, an atomic fact a in the claim c
i IntheD2Cmethod,ourobjectiveistoenhancethe
is supported by the sentence pair (s ,s ) men-
i,1 i,2 diversity of documents and ensure that the docu-
tioned in the generated document D. Therefore,
mentsaremorerealisticthanthoseinC2D,thereby
byomittingoneofthesentencesfromthepairina
reducing the distribution shift between synthetic
newlygenerateddocumentD′,itislikelythata ,
i documentsusedduringtrainingandrealdocuments
andconsequentlyc,isnolongersupportedbyD′
at test time. To achieve this, we assume that we
(exceptincasesofredundancyinthesentencess).
haveaccesstoasetofhuman-writtendocumentsto
Moreformally,wecanconstructadocumentD′
a startwith. Thegoalistogenerateclaimsandpair
i\j
thatprobablycannotsupportfacta inc(andhence
i them with portions of these human written docu-
c)byremovingsentences fromitssentencepair:
i,j ments,which,onceagain,requiremulti-sentence,
D′ = PassageGen(s s ), multi-factreasoningtochecktheclaims.
a i,j
i\j \
foralli 1,...,l andj 1,2 (Figure3;top Step 1: Chunk-level summarization We first
∈ { } ∈ { }
right). Tocollectdocumentsthatdonotsupportthe divideahumanwrittendocumentintothreechunks
claimc,weretainD′ ifa cannotbesupported D ,D ,D with approximately equal length.
a i\j i { 1 2 3 }
bytheinformationcombinedfromtheremaining We then use GPT-4 to generate a summary sen-
sentence from its sentence pair and other atomic tence for each chunk, resulting in a set of sum-
facts (s a a ) via an entailment check marysentencesc = c ,c ,c . Weassumethese
i,3−j i 1 2 3
∪{ \ } { }
byGPT-4. Notethatthisentailmentcheckisagain generatedsummarysentencesarefactuallyconsis-
more accurate than directly checking a against tent with respect to their corresponding chunks,
i
D′ duetotheshortercontext. i.e. (D ,c ,1) for all i, as each chunk is short
a i i
i\j
andLLMscanalmostalwaysgeneratefactualsum-
Step 5: Pairing subclaims and generated doc-
mariesinthissetting(Zhangetal.,2024).
uments We have collected tuples (D,c,1) and
(D′ ,c,0)forsomeiandj. Wecanfurtheraug-
a Step2: Claimdecompositionandsubclaimaug-
i\j
mentthisdatatoproducemoreexamples. Wefirst
mentation SimilartotheC2Dmethod,forasum-
generateapowersetPower(a),thatconsistsofall
marysentencec inc,wedecomposeitintoatomic
i
possiblesubsetsofatomicfactsainc,butexcludes
factsa = a ,...,a ,andcreateasetofaug-
i i,1 i,l
theemptyset. Wethencreateasetofaugmented mentedsub{ claimsAug(c} ) = Merge(a′) : a′
subclaims Aug(c) by merging atomic facts from i { i ∀ i ∈
Power(a ) .
i
eachsubset: }
Step 3: Document-claim augmentation This
Aug(c) = Merge(a′) : a′ Power(a) .
{ ∀ ∈ } step aims to do data augmentation on a (D ,c )
i i
It follows that we obtain tuples (D,c′,1) for ev- pair. GivenachunkD = Concat(s),whichisthe
i
ery c′ Aug(c). Similarly, for each D′ , we concatenation of n sentences s = s ,...,s ,
a i,1 i,n
∈ i\j { }
} <latexit sha1_base64="XDOhZgRVlgMKNwWKNanE7yzbils=">AAADEXicrVJNi9QwGE7r11q/ZvXoJTgMtihDuwp68LAoBY8rOLsD06G8SdPdsGlTklS2lPwFL/4VLx4U8erNm//GzIe4zg6efCHw5HmfJ3nfNyGN4NrE8U/Pv3T5ytVrO9eDGzdv3b4z2L17qGWrKJtQKaSaEtBM8JpNDDeCTRvFoCKCHZHTV4v80TumNJf1W9M1bF7Bcc1LTsE4Kt/1Ho4yKnWYVWBOSNkzm3f4ET63PXuM07DI+yWlqh6ItVGEX+BNH/wfH9nqC1ZlpuFZ5ORp2EX/PD4NYaUj23XBqPlDNUo21oYZAdVPrauFF3jqJOlvKgoymw+G8TheBr4IkjUYonUc5IMfWSFpW7HaUAFaz5K4MfMelOFUMBtkrWYN0FM4ZjMHa6iYnvfLF7V45JgCl1K5VRu8ZM87eqi07irilIsm9GZuQW7LzVpTPp/3vG5aw2q6uqhsBTYSL74HLrhi1IjOAaCKu1oxPQEF1LhPFLghJJstXwSHe+PkyXjvzdPh/sv1OHbQffQAhShBz9A+eo0O0ARR77330fvsffE/+J/8r/63ldT31p576K/wv/8COZDzZQ==</latexit>
}
}
<latexit sha1_base64="XDOhZgRVlgMKNwWKNanE7yzbils=">AAADEXicrVJNi9QwGE7r11q/ZvXoJTgMtihDuwp68LAoBY8rOLsD06G8SdPdsGlTklS2lPwFL/4VLx4U8erNm//GzIe4zg6efCHw5HmfJ3nfNyGN4NrE8U/Pv3T5ytVrO9eDGzdv3b4z2L17qGWrKJtQKaSaEtBM8JpNDDeCTRvFoCKCHZHTV4v80TumNJf1W9M1bF7Bcc1LTsE4Kt/1Ho4yKnWYVWBOSNkzm3f4ET63PXuM07DI+yWlqh6ItVGEX+BNH/wfH9nqC1ZlpuFZ5ORp2EX/PD4NYaUj23XBqPlDNUo21oYZAdVPrauFF3jqJOlvKgoymw+G8TheBr4IkjUYonUc5IMfWSFpW7HaUAFaz5K4MfMelOFUMBtkrWYN0FM4ZjMHa6iYnvfLF7V45JgCl1K5VRu8ZM87eqi07irilIsm9GZuQW7LzVpTPp/3vG5aw2q6uqhsBTYSL74HLrhi1IjOAaCKu1oxPQEF1LhPFLghJJstXwSHe+PkyXjvzdPh/sv1OHbQffQAhShBz9A+eo0O0ARR77330fvsffE/+J/8r/63ldT31p576K/wv/8COZDzZQ==</latexit>
<latexit sha1_base64="XDOhZgRVlgMKNwWKNanE7yzbils=">AAADEXicrVJNi9QwGE7r11q/ZvXoJTgMtihDuwp68LAoBY8rOLsD06G8SdPdsGlTklS2lPwFL/4VLx4U8erNm//GzIe4zg6efCHw5HmfJ3nfNyGN4NrE8U/Pv3T5ytVrO9eDGzdv3b4z2L17qGWrKJtQKaSaEtBM8JpNDDeCTRvFoCKCHZHTV4v80TumNJf1W9M1bF7Bcc1LTsE4Kt/1Ho4yKnWYVWBOSNkzm3f4ET63PXuM07DI+yWlqh6ItVGEX+BNH/wfH9nqC1ZlpuFZ5ORp2EX/PD4NYaUj23XBqPlDNUo21oYZAdVPrauFF3jqJOlvKgoymw+G8TheBr4IkjUYonUc5IMfWSFpW7HaUAFaz5K4MfMelOFUMBtkrWYN0FM4ZjMHa6iYnvfLF7V45JgCl1K5VRu8ZM87eqi07irilIsm9GZuQW7LzVpTPp/3vG5aw2q6uqhsBTYSL74HLrhi1IjOAaCKu1oxPQEF1LhPFLghJJstXwSHe+PkyXjvzdPh/sv1OHbQffQAhShBz9A+eo0O0ARR77330fvsffE/+J/8r/63ldT31p576K/wv/8COZDzZQ==</latexit>
}
<latexit sha1_base64="XDOhZgRVlgMKNwWKNanE7yzbils=">AAADEXicrVJNi9QwGE7r11q/ZvXoJTgMtihDuwp68LAoBY8rOLsD06G8SdPdsGlTklS2lPwFL/4VLx4U8erNm//GzIe4zg6efCHw5HmfJ3nfNyGN4NrE8U/Pv3T5ytVrO9eDGzdv3b4z2L17qGWrKJtQKaSaEtBM8JpNDDeCTRvFoCKCHZHTV4v80TumNJf1W9M1bF7Bcc1LTsE4Kt/1Ho4yKnWYVWBOSNkzm3f4ET63PXuM07DI+yWlqh6ItVGEX+BNH/wfH9nqC1ZlpuFZ5ORp2EX/PD4NYaUj23XBqPlDNUo21oYZAdVPrauFF3jqJOlvKgoymw+G8TheBr4IkjUYonUc5IMfWSFpW7HaUAFaz5K4MfMelOFUMBtkrWYN0FM4ZjMHa6iYnvfLF7V45JgCl1K5VRu8ZM87eqi07irilIsm9GZuQW7LzVpTPp/3vG5aw2q6uqhsBTYSL74HLrhi1IjOAaCKu1oxPQEF1LhPFLghJJstXwSHe+PkyXjvzdPh/sv1OHbQffQAhShBz9A+eo0O0ARR77330fvsffE/+J/8r/63ldT31p576K/wv/8COZDzZQ==</latexit>Uniq. Uniq. Doc Claim %of MiniCheck-DBTA and MiniCheck-FT5 As
Data Size
Claim Doc Len Len Neg models trained on the ANLI dataset (Nie et al.,
C2D 7076 2004 1188 189 19 42% 2020) have demonstrated strong performance
D2C 7319 1392 4967 164 12 65%
(Kamoi et al., 2023; Honovich et al., 2022), we
integrateourdatawiththeANLIdatasetforfine-
Table1: Statisticsofsynthetictrainingdata. Amount
tuning deberta-v3-large (He et al., 2021) and
of synthetic data for training, the number of unique
claims and documents, the average number of words flan-t5-large (Chung et al., 2022). We take a
indocumentsandclaims,andtheproportionofunsup- subset(21K)oftheANLItrainingdata,selecting
portedclaims. examples where their trained entailment models
madeincorrectpredictionsduringdatasetconstruc-
tion. TrainingonmoreofANLIwasnoteffective.
weconstructnewdocumentsbyiterativelyremov-
Combiningthese21Kdatapointswithour14K-
ingeachsentences froms:
i,j
sized dataset, we have 35K training datapoints
D i′ \j = Concat(s \{s i,j }). in total. We map the labels contradiction and
neutralfromANLItounsupported.
We then determine the entailment label for each
atomicfacta inc ,wherek 1,...,l :
i,k i
∈ { }
MiniCheck-RBTA Wealsoexplorewhetheritis
L−j(a ) = Ent(D′ ,a ) 0,1 . possibletoimproveuponthepreviousAlignScore
i,k i\j i,k ∈ { } (Zhaetal.,2023)system,theexistingSOTAspe-
Similartostep5inC2D,ifL−j(a i,k) = 1forall cialized fact-checking model. We fine-tune the
a i,k
∈
a′ i, we create tuples (D i′ \j,Merge(a′ i),1). tuned roberta-large (Liu et al., 2019) model
Conversely, if there exists any a a′ such fromAlignScorewithabinaryclassificationhead,
i,k ∈ i
that L−j(a ) = 0, we then create tuples onour14Ksyntheticdatapoints.
i,k
(D′ ,Merge(a′),0).
i\j i Producingclassificationdecisions Althoughour
Step 4: Cross-document-claim augmentation taskisframedasbinaryclassification,inrealitythe
The objective of this step is to perform data aug- modelswehaveareoftheformM(D ,c ) z
i i
→ ∈
mentation on a (D ,c ) pair, where j = i. The R,mappingeach(document,claim)pairtoascore
j i
̸
rationalebehindthisisthattheimportantinforma- in the range z [v ,v ]. Following Laban
min max
∈
tioninadocumentcanbeconveyedmultipletimes etal.(2022);Zhaetal.(2023);Tangetal.(2023a),
in various ways. Given that each chunk D has we convert each method into a binary classifier
i
an associated summary c , it is probable that the M(D ,c ) 0,1 bypickingathresholdtsuch
i i i
→ { }
summaryc conveyssomeinformationthatcanbe thatwepredict1ifM(D ,c ) > tand0otherwise.
i i i
indirectlysupportedbyotherchunksD withinthe Unlessotherwisespecified,wesett = 0.5.
j
document, even if D are not used to generate c .
j i
Therefore,weconsiderchunksD ,wherej = i,as
4 LLM-AGGREFACT Benchmark
j
̸
morechallengingchunkstoeithersupportorrefute
Weconstructafactverificationbenchmark,LLM-
theclaimc oritsatomicfactsa .
i i AGGREFACT, by aggregating 10 of the most up-
Moreformally,wedeterminetheentailmentla-
to-datepubliclyavailabledatasetsonfactualcon-
belforeachatomicfacta inc ,usingthedocu-
i,k i sistency evaluation across both closed-book and
mentchunkD ,wherek 1,...,l ,andj = i:
j groundedgenerationsettings.
∈ { } ̸
LDj(a ) = Ent(D ,a ) 0,1 .
i,k j i,k ∈ { } Characteristics In LLM-AGGREFACT, all
IfLDj(a i,k) = 1foralla
i,k
∈
a′ i,wecreatetuples datasets contain human-annotated (document,
(D ,Merge(a′),1). Conversely,ifthereexistsany claim, label)tuples. Thedocumentscomefrom
j i
a
i,k ∈
a′
i
suchthatLDj(a i,k) = 0,wethencreate diverse sources, including Wikipedia paragraphs,
tuples(D ,Merge(a′),0). interviews, web text, covering domains such as
j i
news, dialogue, science, and healthcare. The
3.3 MINICHECKModels
claims to be verified are mostly generated from
Wefine-tunethreemodelswithvariousmodelar- recent generative models (except for one dataset
chitectures by leveraging our synthetic data. We of human-written claims), without any human
usethestandardcross-entropylossforallmodels. intervention in any format, such as injecting
SeeAppendixGfortrainingdetails. certain error types into model-generated claims.(cid:16) (cid:17)
Type Dataset Feature 1 TP + TN ,whereTP,TN,FP,andFN
2 TP+FN TN+FP
AGGREFACT Summaries from SOTA representtrue/falsepositives/negatives.
Fixed-Doc (CNN/XSum) fine-tuned summarizers
Generation TOFUEVAL Topic-focused dialogue 5 ExperimentalSetup
(MediaS/MeetB) summaries from LLMs
CLAIMVERIFY Weincludethefollowingspecializedfact-checkers:
Retrieve-then
Generate
LFQA (Check-worthy) T5-NLI-Mixed (Honovich et al., 2022), DAE
sentences from LLMs/
EXPERTQA (Goyal and Durrett, 2021), QAFactEval (Fabbri
search engines'
EXPERTQA responses to search etal.,2022),SummaC-ZSandSummaC-CV(La-
Post-Hoc
Grounding REVEAL queries ban et al., 2022), AlignScore (Zha et al., 2023),
andFT5-ANLI-Lthatfine-tunesflan-t5-large
FACTCHECK-GPT
onthefullANLItrainingset. Ameta-comparison
Wikipedia claims with
Written Claims WICE
citations ofthosespecializedfact-checkersandourmodels
canbefoundinTable3. Moreinferencedetailscan
Figure4: 10datasetsinLLM-AGGREFACT. Detailsof
befoundinAppendixE.2.
thesedatasetsaswellasrelatedbutexcludeddatasets
We also include the following LLMs as fact-
canbefoundinAppendixC.
checkers: Gemini-Pro (Team et al., 2023),
PaLM2-Bison (Thoppilan et al., 2022), Mistral-
AnoverviewoftheLLM-AGGREFACTisshown 8x7B,Mistral-Large(Jiangetal.,2024),Claude
in Figure 4, with statistics and detailed dataset 2.1, Claude 3 Opus (Bai et al., 2022), GPT-3.5
descriptionsinAppendix C. and GPT-4 (OpenAI, 2023). More details about
themodelscanbefoundinAppendixE.1. Forthe
4.1 BenchmarkDetails LLM-basedfact-checkers,weadaptapromptfrom
Luo et al. (2023) for zero-shot prediction, which
Validation/Test set split For the datasets from
canbefoundinAppendixH.
AGGREFACTandTOFUEVAL,aswellasWICEand
CLAIMVERIFY,wedirectlyusetheexistingvalida-
6 Results
tionandtestsplitsfromtheoriginalwork. For RE-
VEAL,FACTCHECK-GPT,EXPERTQAandLFQA, 6.1 MainResults
we randomly divide each of them into validation
Our synthetic data improves performance
andtestsets(50%/50%),assuringthatresponsesto
across diverse model architectures. Table 2
uniquequeriesdonotappearinbothsets.
demonstrates that our synthetic data gives strong
One potential use of the validation data is to
performancewhenusedinthreedifferentbackbone
allowforper-datasetthresholdtuning. Thisset-
models: RoBERTa,DeBERTa,andFlan-T5. These
tingisusedinsubstantialpastwork(Labanetal.,
modelsoutperformpriormodelsofasimilarscale.
2022;Luoetal.,2023;Zhaetal.,2023;Tangetal.,
Notably,MiniCheck-FT5 achievesa4.3%overall
2023a, 2024). However, we do not follow this
improvementoverAlignScore,outperformingiton
trendinordertofocusonbuildingsystemsthatcan
6outof10datasetsandmatchingitsperformance
bedeployedzero-shotacrossmultipledownstream
ontheremaining4. Weattributeitsadditional2%
tasks,withoutanyadditionalhyperparametertun-
gainoverMiniCheck-RBTAand-DBTAtoitslarger
ing. Instead,forM(d,c) z [v ,v ],the
min max model size. However, model size alone does not
→ ∈
thresholdissetasthemidpointoftheoutputscore
guaranteesuperiorperformance,asevidencedby
ranget = (v +v )/2,whichis0.5formost
min max T5-NLI-MixedandFT5-ANLI-L,which,despite
fact-checkers. In practice, most fact-checkers re-
being trained on NLI datasets, underperform on
turnscoresattheextremesoftherange,sosmall
mostofthebenchmarksettings. Thisunderscores
tweaksonthisprocedurehavelittleeffect. SeeAp-
theimportanceoftrainingdataselectioninaddition
pendixB.1fortheresultsofthethresholdtuning
tomodelcapacity.
setting,whichyieldsqualitativelysimilarresults.
Our models achieve performance on par with
Evaluation Metric Following Laban et al. themostcapableLLM-basedfact-checkers. In
(2022); Fabbri et al. (2022); Tang et al. (2023a), the top rows of Table 2, we present the perfor-
we evaluate the performance of fact-checkers mance of strong LLM-based fact-checkers. We
using balanced accuracy (BAcc): BAcc = observe that existing specialized fact-checkersLLM-AGGREFACT(withoutthresholdtuning)
Model AGGREFACT TOFUEVAL CLAIM FACT EXPERT
WICE REVEAL LFQA Avg
Name VERIFY CHECK QA
CNN XSum MediaS MeetB
Gemini-Pro 49.4 60.6 63.8 65.8 65.8 85.5 61.8 76.8 56.8 75.9 66.2
PaLM2-Bison 52.4 59.0 68.3 73.6 63.4 84.2 60.5 76.4 56.6 71.4 66.6
Mistral-8x7B 55.0 65.5 68.5 73.3 63.8 80.8 64.3 75.1 56.3 70.8 67.3
GPT-3.5 63.2 72.4 66.8 73.4 68.5 84.7 65.2 70.8 57.2 73.8 69.6
Claude-2.1 59.9 66.4 69.2 72.3 64.3 88.2 69.7 79.3 59.8 78.2 70.7
Mistral-Large 58.4 76.3 67.3 78.9 76.6 88.4 67.6 79.0 60.0 81.7 73.4
Claude-3Opus 65.2 72.4 74.1 82.4 75.0 83.8 69.3 78.8 58.8 81.6 74.1
GPT-4 66.7 76.5 71.4 79.9 80.4 87.8 67.6 79.9 59.2 83.1 75.3
SummaC-CV 65.2 54.5 63.7 62.8 54.3 67.7 70.9 53.4 54.9 62.1 62.1
T5-NLI-Mixed 54.6 52.3 59.1 55.3 55.3 87.2 59.5 69.0 55.6 61.8 61.0
FT5-ANLI-L 51.2 60.0 57.4 60.1 67.0 77.5 58.3 67.7 52.2 63.0 61.4
DAE 50.8 59.1 65.1 69.5 58.5 81.3 64.0 72.5 56.2 72.2 64.9
QAFactEval 54.3 62.1 61.3 65.7 62.5 83.2 73.2 66.1 56.0 80.6 66.5
SummaC-ZS 51.1 61.5 69.5 71.0 62.8 85.3 69.7 75.2 55.2 77.6 67.9
AlignScore 52.4 71.4 69.2 72.6 66.0 85.3 69.6 74.3 58.3 84.5 70.4
MiniCheck-DBTA 64.2 71.0 69.3 72.7 69.4 87.3 75.6 73.0 58.9 83.9 72.6
MiniCheck-RBTA 63.7 70.8 71.9 75.9 67.6 88.8 77.4 73.3 57.4 84.4 72.7
MiniCheck-FT5 69.9 74.3 73.6 77.3 72.2 86.2 74.6 74.7 59.0 85.2 74.7
Table 2: Performance (BAcc) of models on the test set of LLM-AGGREFACT without per-dataset threshold
tuning. ModelsaresplitintoLLM-basedfact-checkers|specializedfact-checkers|Ours. Wehighlightthe best
performanceforeachdataset,wheremultiplegreenhighlightsindicatesystemsindistinguishablefromthebest
accordingtoapairedbootstraptestwithp-value<0.05. Detailsfor-Dectxand-DecmpareinSection7. Our
MiniCheckmodelsoutperformotherspecializedevaluatorsandMiniCheck-FT5reachestheperformanceofGPT-4.
Model Backbone Model #FT Cost 6.2 ComputationalCostComparison
Name Model Size Data ($)
T5-NLI-Mixed T5-XXL 11B 1,697K 7.39 Wecomparethecomputationalcostofspecialized
FT5-ANLI-L Flan-T5-L 770M 163K 0.24
fact-checkers and LLMs on the test set of LLM-
DAE ELECTRA-B 110M 95K 0.26
QAFactEval multiple∗ 1.4B - 1.87 AGGREFACT. For specialized fact-checkers, we
SummaC-ZS ALBERT-XL 60M 371K 0.85 useourownhardwareandconverttheprediction
SummaC-CV ALBERT-XL 60M 381K 0.85
timeonourGPUstotheequivalentcostofusing
AlignScore RoBERTa-L 355M 4,700K 0.20
cloudcomputingservices(seeAppendixE.2.1for
MiniCheck-RBTA AlignScore 355M 14K 0.20
details). For LLM-based fact-checkers, we com-
MiniCheck-DBTA DeBERTa-L 355M 35K 0.20
MiniCheck-FT5 Flan-T5-L 770M 35K 0.24 putethecostsofcorrespondingAPIcalls. Results
areshowninTable3and4. Weseethatspecialized
Table 3: Comparison of specialized fact-checkers on
modelsingeneralhavemuchlowerinferencecosts.
modelsizes,trainingdatasizes,andtheinferencecost
Inparticular,ourmostcapablemodelMiniCheck-
($0.8/GPU-hr) on the 13K LLM-AGGREFACT test
FT5 has almost the same performance as GPT-4
set. ∗QAFactEvalcontainsseveralmodelcomponents,
butismorethan400timescheaper.
whichsumupto1.4Binsize.
achievesimilarperformancetonon-frontierLLM- 7 RethinkingLLMFact-Checking
basedfact-checkerslikeMistral-8x7BandGPT-3.5.
MiniCheck-RBTA andMiniCheck-DBTA cansur-
We now revisit two other stages of the typical
passthesenon-frontierLLM-basedfact-checkers
LLMfact-checkingpipeline: claimdecomposition
by a large margin. MiniCheck-FT5 achieves the
anddecontextualization. Surprisingly,wefindthat
sameperformanceasClaude-3Opusandisclose
claimdecompositionisnotneededinoursettings,
toGPT-4,butwithamuchsmallermodelsize.
contradicting prior work (Yang and Zhu, 2021;
Extended Analysis See Appendix A for an in- Kamoietal.,2023). Furthermore,wefindthatde-
trinsic evaluation on our synthethic data and an contextualizationdoesn’thelponourbenchmark,
ablationstudyonourbestmodelMiniCheck-FT5. althoughwebelievethatitisneededingeneral.Model Cost Model Cost Model Decomposition Decontextualization
Name ($) Name ($)
GPT-4 75.6(↑0.3) 75.3(+0.0)
Gemini-Pro 5.24 Claude-2.1 89.9
SummaC-CV 58.8(↓3.3) 60.8(↓1.3)
PaLM2-Bison 10.9 Claude-3Opus 165
QAFactEval 64.6(↓1.9) 66.4(↓0.1)
Mistral-8x7B 7.78 GPT-3.5 4.75
SummaC-ZS 69.1(↑1.2) 67.7(↓0.2)
Mistral-Large 90.2 GPT-4 107
AlignScore 71.5(↑1.1) 70.4(+0.0)
GPT-4-Dectx 161 GPT-4-Decmp 212
MiniCheck-RBTA 73.2(↑0.5) 72.4(↓0.3)
Table4: InferencecostcomparisonforAPImodelson MiniCheck-DBTA 72.7(↑0.1) 71.2(↓1.4)
MiniCheck-FT5 73.3(↓1.4) 74.1(↓0.6)
the13KLLM-AGGREFACTtestset. Bothdecontextu-
alizationanddecompositionaddcosttoGPT-4. Overall,
Table5: AverageperformanceonthetestsetofLLM-
decodingourtestsetwiththemostcapablemodelsin-
AGGREFACT by aggregating predictions on decom-
curssignificantcost.
posed claims (left); doing claim decontextualization
whereitisapplicable(right). Weshowtheperformance
change compared to predicting using original claims
7.1 ClaimDecomposition
fromTable2. FullresultsinAppendixTables8and9.
Wealsoexperimentwithasettingusingclaimde-
composition. In thissetting, wedecompose each
claimc intoatomicfactsa withthepromptfrom
PERTQA and LFQA,whicharethedatasetsinour
i i
benchmarkwheresentencesneedtobeinterpreted
Kamoietal.(2023)anduseafact-checkertopre-
dict the factuality label for each (D ,a ) pair,
in context (FACTCHECK is already decontextual-
i i,k
izaed). WepromptGPT-4fordecontextualization
k 1,...,l . Ifallatomicfactsaresupportedby
∈ { } asshowninAppendixH,usingthepreviousclaims
thedocument,thentheclaimissupported,andun-
or response sentences as context to expand the
supportedotherwise. Wedothisforeverydataset
claim. Respectively,33%,33%,39%,11%,35%,
except FactCheck-GPT which is already atomic
47%, and 57% of the claims from those datasets
facts. Therearetypically2-4atomicfactsperclaim
arechangedafterdecontextualization.
acrossdatasets.
InTable5, weshowtheaveragefact-checking
We show the results from GPT-4 and a subset
performancewhenusingthisdecontextualization
of specialized fact-checkers in Table 5. We ob-
step (see the prompt in Table 23). These results
serve near-zero performance change for GPT-4
suggest that models may make decent guesses
and mixed changes for specialized fact-checkers.
aboutcontext-dependentcontent,particularly
Overall,thereisnoclearindicationthatdecom-
when the retrieval stage already implicitly en-
posingclaimsintoatomicfactscanconsistently
forcessharedcontextbetweentheclaimandthe
improvemodels’performance. Becausethisap-
groundingdocuments. However,fortaskssuchas
proachincreasestheinferencetimeandcostsbya
retrieval-augmentedgeneration,webelievedecon-
factor of 2-4 for different datasets, depending on
textualizationstillplaysacrucialroleinensuring
theaveragenumberofatomicfactsperclaim,we
meaningful document retrieval. Furthermore, as
believeitshouldnotbeuseduntilitprovidesaclear
LLMs scale further and their responses get more
accuracybenefit.4
complex,thelevelofcontextualizationtheyfeature
7.2 ClaimDecontextualization maybehigher,makingthisstepmorenecessary.
As mentioned in Section 2, our approach relies 8 RelatedWork
on being able to check each sentence in isola-
HallucinationsinLLMs LLMsarepronetohal-
tion. However, phenomena like coreference and
lucinations across various settings (Huang et al.,
ellipsis may make sentences difficult to ground
2023;Zhangetal.,2023b;Rawteetal.,2023),gen-
out of context. We can address this with an ex-
eratinginformationthatcannotbesupportedbyany
plicit decontextualization step (Choi et al., 2021;
source. For example, in the closed-book setting,
Wang et al., 2023; Jacovi et al., 2024). We ex-
whereLLMsrelysolelyontheirparametricknowl-
periment with TOFUEVAL-MediaS, TOFUEVAL-
edge, theymayfabricatedetailswhendescribing
MeetB, WICE, REVEAL, CLAIMVERIFY, EX-
biographiesorprovidingWikipediaentityinforma-
4NotethatforFactcheck-GPT,retrievaloperatesoverindi- tion (Min et al., 2023; Guan et al., 2023; Mallen
vidualatomicfacts.Decompositionmaystillbenecessaryto
etal.,2023). Inretrieval-augmentedsettings,where
retrievetherelevantinformation,butourresultsshowthatit
maynotbenecessaryforentailmentchecks. modelshaveaccesstoexternaldocumentstopro-videresponsestouserqueries,theymaygenerate Entailment Datasets Our work contributes a
supplementary information that is not faithful to newdatasetfortrainingtextualentailmentmodels
theprovideddocuments(Chiesurinetal.,2023;Ad- overdocumentsordocument-chunks. Mostprior
lakhaetal.,2023;Chenetal.,2024). Evenwhen entailment datasets have been human-authored
LLMsareprovidedwithgolddocuments,suchas (Bowmanetal.,2015;Williamsetal.,2018),which
intextsummarizationandsimplificationtasks,they isknowntointroduceartifacts(Gururanganetal.,
stillgeneratefactuallyinconsistentoutputswithdi- 2018),orcollectedinthewild(Kamoietal.,2023),
verseerrortypesacrossdifferentdomains(Joseph whichischallengingtoscale. Pastworkhasauto-
et al., 2023; Shaib et al., 2023; Tang et al., 2024, maticallygeneratedcontrastsetsforNLI(Lietal.,
2023c). In this work, we construct a new bench- 2020). DocNLI(Yinetal.,2021)isarestructureof
mark dataset, LLM-AGGREFACT, which unifies existingdatasetswherethelengthofmosttraining
human-annotated model responses across all set- examples cannot fit into the input limit of small
tings,andevaluatetheperformanceofexistingfact- models. Our work differs from these in its hand-
checkersandourproposedonesonthebenchmark built,syntheticnaturetoencouragemulti-sentence
indetectingsucherrors. andmulti-factreasoning,whichisimportanttothe
taskoffact-checkingongroundingdocuments.
Methods in Detecting Hallucinations When 9 Conclusion
documents are directly available for model-
generatedsentences,suchasintextsummarization Inthiswork,weintroducetwosyntheticdatagen-
(Falkeetal.,2019;Kryscinskietal.,2020;Maynez erationmethodsthataddresskeylimitationsofspe-
etal.,2020;Fabbrietal.,2021;Tangetal.,2023a) cialized fact-checkers by encouraging models to
orretrieval-augmentedgeneration(Liuetal.,2023; verifyeachatomicfactwithinaclaimandreason
Malaviyaetal.,2024),theentireclaimsaredirectly acrossmultiplesentences. WealsopresentLLM-
verifiedagainstthesourcedocuments. However,in AGGREFACT,anewfactualconsistencyevaluation
caseswheresuchdocumentsarenotreadilyavail- dataset covering both closed-book and grounded
able, such as in close-book generation, Gao et al. generationsettings. Amodelfine-tunedonoursyn-
(2023); Min et al. (2023); Wang et al. (2023) de- thetic data outperforms all prior specialized fact-
composeeachgeneratedsentenceintoatomicfacts checkersonLLM-AGGREFACT,whilebeingmuch
andthensearchforrelevantdocumentstosupport cheaperthanLLM-basedfact-checkers.
each atomic fact. Alternatively, Malaviya et al.
Limitations
(2024)directlysearchforrelevantdocumentsfor
eachsentenceasawhole.
Interpretation Likemanyotherspecializedfact-
Therearetwomainapproachestoverifyingsen- checking models, our models do not reveal their
tencesagainstdocuments. Thefirstinvolvestrain- internaldecision-makingprocesses,makingitchal-
ingspecializedfact-checkersspecificallydesigned lengingtolocalizeerrorstoparticularmismatched
for factual consistency evaluation, which are pri- spansofaclaimordocument. Therearetwoways
marilyevaluatedinthecontextofsummarization toalleviatethisissue. Thefirstistoperformclaim
(Kryscinskietal.,2020;Fabbrietal.,2022;Goyal decomposition and check the models’ prediction
andDurrett,2020;Labanetal.,2022). Thesecond labelsoneachatomicfact,therebylocalizingthe
approach leverages LLMs as fact-checkers, par- error from the original claim. Our models show
ticularlyforevaluatingLLM-generatedresponses betterperformancecomparedtootherspecialized
from retrieval-augmented generation and closed- fact-checkerswhenusingthisclaimdecomposition
book generation (Min et al., 2023; Wang et al., method(Table5),butthismethodcangivegreater
2023;Malaviyaetal.,2024;Gaoetal.,2023). In interpretability. Thesecondapproach,whichcan
this work, we bridge the gap between these two beafutureresearchdirection,istoenableourbest
approaches by evaluating both specialized fact- model, MiniCheck-FT5, orgenerativemodelsin
checkersandLLM-basedfact-checkersacrossall general,toprovidereliableexplanationsinaddition
these settings using our new benchmark, LLM- tothebinarypredictions. Webelieveamodelcan
AGGREFACT. We show that our best model can providereliableexplanationsonlyifitcanfirstcor-
matchGPT-4performanceandperformwellinall rectlyidentifyerrorsinourbinarysetting, which
settingswithoutdoingsentencedecomposition. wasthefocusofthiswork.Multi-DocumentReasoning Whileourbench- uating correctness and faithfulness of instruction-
markincludesinstancesthatevaluatemodels’abil- following models for question answering. arXiv
preprintarXiv:2307.16877.
ity to reason across multiple sentences, these
datasetsdonotnecessitatereasoningoverevidence Yuntao Bai, Saurav Kadavath, Sandipan Kundu,
thatissignificantlyseparatedorspreadacrossvar- AmandaAskell,JacksonKernion,AndyJones,Anna
ious documents. Future research could focus on Chen, Anna Goldie, Azalia Mirhoseini, Cameron
McKinnon,CarolChen,CatherineOlsson,Christo-
evaluatingtheperformanceofexistingmodelsin
pher Olah, Danny Hernandez, Dawn Drain, Deep
such scenarios, creating new labeled datasets of
Ganguli,DustinLi,EliTran-Johnson,EthanPerez,
errors to expand our benchmark, and developing Jamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua
better fact-checking models to handle these chal- Landau,KamalNdousse,KamileLukosuite,Liane
Lovitt, Michael Sellitto, Nelson Elhage, Nicholas
lenges.
Schiefer,NoemiMercado,NovaDasSarma,Robert
Lasenby, Robin Larson, Sam Ringer, Scott John-
Synthetic Data The effectiveness of our syn-
ston,ShaunaKravec,SheerElShowk,StanislavFort,
thetic data is demonstrated by the improved per-
TameraLanham,TimothyTelleen-Lawton,TomCon-
formancewhentrainingonitacrossvariousmodel erly,TomHenighan,TristanHume,SamuelR.Bow-
architectures. AsLLMscontinuetoadvance, the man,ZacHatfield-Dodds,BenMann,DarioAmodei,
NicholasJoseph,SamMcCandlish,TomBrown,and
quality of the synthetic data generated using our
JaredKaplan.2022. Constitutionalai: Harmlessness
approach is also expected to improve. However,
fromaifeedback.
wedidnotconductaformalhumanevaluationto
evaluatethequalityofoursyntheticallygenerated SamuelR.Bowman,GaborAngeli,ChristopherPotts,
and Christopher D. Manning. 2015. A large anno-
data(i.e.,howmanyinstancesaremislabeled).
tatedcorpusforlearningnaturallanguageinference.
In Proceedings of the 2015 Conference on Empiri-
Language Our models are trained exclusively
calMethodsinNaturalLanguageProcessing,pages
on English data. Although the backbone model,
632–642,Lisbon,Portugal.AssociationforCompu-
Flan-T5, istrainedonmultilingualdata, wehave tationalLinguistics.
notsystematicallyassessedhowwellourmodel’s
MengCao,YueDong,andJackieCheung.2022. Hal-
performanceextendstootherlanguagesduetothe
lucinated but factual! inspecting the factuality of
absenceofahuman-annotatedfactualconsistency hallucinationsinabstractivesummarization. InPro-
evaluation dataset for LLM-generated outputs in ceedingsofthe60thAnnualMeetingoftheAssocia-
non-Englishlanguages. Webelievethatdeveloping tionforComputationalLinguistics(Volume1: Long
Papers),pages3340–3354,Dublin,Ireland.Associa-
afact-checkerthatcanperformwellacrossmultiple
tionforComputationalLinguistics.
languagesisimportantforfuturework.
ShuyangCaoandLuWang.2021. CLIFF:Contrastive
Acknowledgments learningforimprovingfaithfulnessandfactualityin
abstractive summarization. In Proceedings of the
WewouldliketothankJessyLiforcommentson 2021ConferenceonEmpiricalMethodsinNatural
LanguageProcessing,pages6633–6649,Onlineand
a draft of this work. This work was principally
Punta Cana, Dominican Republic. Association for
supportedbyagiftfromAmazonaspartoftheUT-
ComputationalLinguistics.
Amazon Science Hub. It was partially supported
by NSF CAREER Award IIS-2145280, the NSF Hung-Ting Chen, Fangyuan Xu, Shane A Arora, and
Eunsol Choi. 2023. Understanding retrieval aug-
AIInstituteforFoundationsofMachineLearning
mentationforlong-formquestionanswering. arXiv
(IFML),a grantfromOpen Philanthropy, agrant
preprintarXiv:2310.12150.
from the UT Austin Office of the Vice President
for Research through the “Creating Connections Jiawei Chen, Hongyu Lin, Xianpei Han, and Le Sun.
2024. Benchmarking large language models in
for National Security Research Grants” program,
retrieval-augmented generation. In Thirty-Eighth
andGoodSystems,5 aUTAustinGrandChallenge
AAAI Conference on Artificial Intelligence, AAAI
todevelopresponsibleAItechnologies. 2024,Thirty-SixthConferenceonInnovativeApplica-
tionsofArtificialIntelligence,IAAI2024,Fourteenth
Symposium on Educational Advances in Artificial
References Intelligence,EAAI2014,February20-27,2024,Van-
couver,Canada,pages17754–17762.AAAIPress.
VaibhavAdlakha,ParishadBehnamGhader,XingHan
Lu, Nicholas Meade, and Siva Reddy. 2023. Eval- SabrinaChiesurin,DimitrisDimakopoulos,MarcoAn-
tonio Sobrevilla Cabezudo, Arash Eshghi, Ioannis
5https://goodsystems.utexas.edu/ Papaioannou, Verena Rieser, and Ioannis Konstas.2023. The dangers of trusting stochastic parrots: MaxGlockner,IevaStaliu¯naite˙,JamesThorne,Gisela
Faithfulnessandtrustinopen-domainconversational Vallejo,AndreasVlachos,andIrynaGurevych.2024.
questionanswering. InFindingsoftheAssociation AmbiFC: Fact-Checking Ambiguous Claims with
forComputationalLinguistics:ACL2023,pages947– Evidence. TransactionsoftheAssociationforCom-
959,Toronto,Canada.AssociationforComputational putationalLinguistics,12:1–18.
Linguistics.
TanyaGoyalandGregDurrett.2020. Evaluatingfactu-
Eunsol Choi, Jennimaria Palomaki, Matthew Lamm, alityingenerationwithdependency-levelentailment.
Tom Kwiatkowski, Dipanjan Das, and Michael InFindingsoftheAssociationforComputationalLin-
Collins. 2021. Decontextualization: Making sen- guistics: EMNLP 2020, pages 3592–3603, Online.
tencesstand-alone. TransactionsoftheAssociation AssociationforComputationalLinguistics.
forComputationalLinguistics,9:447–461.
TanyaGoyalandGregDurrett.2021. Annotatingand
HyungWonChung,LeHou,ShayneLongpre,Barret modeling fine-grained factuality in summarization.
Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi InProceedingsofthe2021ConferenceoftheNorth
Wang,MostafaDehghani,SiddharthaBrahma,etal. AmericanChapteroftheAssociationforComputa-
2022. Scalinginstruction-finetunedlanguagemodels. tionalLinguistics: HumanLanguageTechnologies,
arXivpreprintarXiv:2210.11416. pages1449–1462,Online.AssociationforComputa-
tionalLinguistics.
AlexanderFabbri,Chien-ShengWu,WenhaoLiu,and
CaimingXiong.2022. QAFactEval: ImprovedQA-
JianGuan,JesseDodge,DavidWadden,MinlieHuang,
basedfactualconsistencyevaluationforsummariza-
andHaoPeng.2023. Languagemodelshallucinate,
tion. InProceedingsofthe2022Conferenceofthe
but may excel at fact verification. arXiv preprint
NorthAmericanChapteroftheAssociationforCom-
arXiv:2310.14564.
putationalLinguistics: HumanLanguageTechnolo-
gies,pages2587–2601,Seattle,UnitedStates.Asso-
SuchinGururangan,SwabhaSwayamdipta,OmerLevy,
ciationforComputationalLinguistics. RoySchwartz,SamuelBowman,andNoahA.Smith.
2018. Annotationartifactsinnaturallanguageinfer-
AlexanderR.Fabbri,WojciechKrys´cin´ski,BryanMc-
encedata. InProceedingsofthe2018Conferenceof
Cann,CaimingXiong,RichardSocher,andDragomir
theNorthAmericanChapteroftheAssociationfor
Radev.2021. SummEval: Re-evaluatingSummariza-
ComputationalLinguistics: HumanLanguageTech-
tionEvaluation. TransactionsoftheAssociationfor
nologies,Volume2(ShortPapers),pages107–112,
ComputationalLinguistics,9:391–409.
NewOrleans,Louisiana.AssociationforComputa-
Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie tionalLinguistics.
Utama,IdoDagan,andIrynaGurevych.2019. Rank-
PengchengHe,JianfengGao,andWeizhuChen.2021.
inggeneratedsummariesbycorrectness: Aninterest-
DeBERTaV3: Improving DeBERTa using electra-
ingbutchallengingapplicationfornaturallanguage
stylepre-trainingwithgradient-disentangledembed-
inference. InProceedingsofthe57thAnnualMeet-
dingsharing. arXivpreprintarXiv:2111.09543.
ingoftheAssociationforComputationalLinguistics,
pages 2214–2220, Florence, Italy. Association for
OrHonovich, RoeeAharoni, JonathanHerzig, Hagai
ComputationalLinguistics.
Taitelbaum,DoronKukliansy,VeredCohen,Thomas
AngelaFan,YacineJernite,EthanPerez,DavidGrang- Scialom, Idan Szpektor, Avinatan Hassidim, and
ier, Jason Weston, and Michael Auli. 2019. ELI5: Yossi Matias. 2022. TRUE: Re-evaluating factual
Long form question answering. In Proceedings of consistencyevaluation. InProceedingsoftheSecond
the57thAnnualMeetingoftheAssociationforCom- DialDocWorkshoponDocument-groundedDialogue
putationalLinguistics,pages3558–3567,Florence, andConversationalQuestionAnswering,pages161–
Italy.AssociationforComputationalLinguistics. 175,Dublin,Ireland.AssociationforComputational
Linguistics.
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
Chen,ArunTejasviChaganty,YichengFan,Vincent Cheng-Yu Hsieh, Chun-Liang Li, Chih-kuan Yeh,
Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and HootanNakhost,YasuhisaFujii,AlexRatner,Ranjay
KelvinGuu.2023. RARR:Researchingandrevising Krishna,Chen-YuLee,andTomasPfister.2023. Dis-
whatlanguagemodelssay, usinglanguagemodels. tillingstep-by-step! outperforminglargerlanguage
In Proceedings of the 61st Annual Meeting of the models with less training data and smaller model
AssociationforComputationalLinguistics(Volume1: sizes. In Findings of the Association for Compu-
LongPapers),pages16477–16508,Toronto,Canada. tational Linguistics: ACL 2023, pages 8003–8017,
AssociationforComputationalLinguistics. Toronto,Canada.AssociationforComputationalLin-
guistics.
ZelalemGero,ChandanSingh,HaoCheng,TristanNau-
mann, Michel Galley, Jianfeng Gao, and Hoifung Yebowen Hu, Timothy Ganter, Hanieh Deilamsalehy,
Poon.2023. Self-verificationimprovesfew-shotclin- FranckDernoncourt,HassanForoosh, andFeiLiu.
icalinformationextraction. InICML3rdWorkshop 2023. MeetingBank: Abenchmarkdatasetformeet-
on Interpretable Machine Learning in Healthcare ingsummarization. InProceedingsofthe61stAn-
(IMLH). nualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),pages16409– Processing,pages9662–9676,Singapore.Associa-
16423,Toronto,Canada.AssociationforComputa- tionforComputationalLinguistics.
tionalLinguistics.
PhilippeLaban,TobiasSchnabel,PaulN.Bennett,and
Dandan Huang, Leyang Cui, Sen Yang, Guangsheng MartiA.Hearst.2022. SummaC:Re-visitingNLI-
Bao, Kun Wang, Jun Xie, and Yue Zhang. 2020. basedmodelsforinconsistencydetectioninsumma-
Whathaveweachievedontextsummarization? In rization. TransactionsoftheAssociationforCompu-
Proceedings of the 2020 Conference on Empirical tationalLinguistics,10:163–177.
MethodsinNaturalLanguageProcessing(EMNLP),
pages 446–469, Online. Association for Computa- Chuanrong Li, Lin Shengshuo, Zeyu Liu, Xinyi Wu,
tionalLinguistics. Xuhui Zhou, and Shane Steinert-Threlkeld. 2020.
Linguistically-informed transformations (LIT): A
LeiHuang,WeijiangYu,WeitaoMa,WeihongZhong,
method for automatically generating contrast sets.
Zhangyin Feng, Haotian Wang, Qianglong Chen, InProceedingsoftheThirdBlackboxNLPWorkshop
WeihuaPeng,XiaochengFeng,BingQin,etal.2023. onAnalyzingandInterpretingNeuralNetworksfor
Asurveyonhallucinationinlargelanguagemodels: NLP,pages126–135,Online.AssociationforCom-
Principles,taxonomy,challenges,andopenquestions.
putationalLinguistics.
arXivpreprintarXiv:2311.05232.
JunyiLi,XiaoxueCheng,XinZhao,Jian-YunNie,and
AlonJacovi,YonatanBitton,BerndBohnet,Jonathan
Ji-Rong Wen. 2023. HaluEval: A large-scale hal-
Herzig, Or Honovich, Michael Tseng, Michael
lucinationevaluationbenchmarkforlargelanguage
Collins, Roee Aharoni, and Mor Geva. 2024. A
models. InProceedingsofthe2023Conferenceon
chain-of-thoughtisasstrongasitsweakestlink: A
EmpiricalMethodsinNaturalLanguageProcessing,
benchmarkforverifiersofreasoningchains.
pages6449–6464,Singapore.AssociationforCom-
putationalLinguistics.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine
Roux, Arthur Mensch, Blanche Savary, Chris
NelsonLiu,TianyiZhang,andPercyLiang.2023. Eval-
Bamford, Devendra Singh Chaplot, Diego de las
uatingverifiabilityingenerativesearchengines. In
Casas, Emma Bou Hanna, Florian Bressand, Gi-
FindingsoftheAssociationforComputationalLin-
anna Lengyel, Guillaume Bour, Guillaume Lam-
guistics:EMNLP2023,pages7001–7025,Singapore.
ple, Lélio Renard Lavaud, Lucile Saulnier, Marie-
AssociationforComputationalLinguistics.
AnneLachaux,PierreStock,SandeepSubramanian,
Sophia Yang, Szymon Antoniak, Teven Le Scao,
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
Théophile Gervet, Thibaut Lavril, Thomas Wang,
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
TimothéeLacroix,andWilliamElSayed.2024. Mix-
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
tralofexperts.
RoBERTa: Arobustlyoptimizedbertpretrainingap-
proach. arXivpreprintarXiv:1907.11692.
SebastianJoseph,KathrynKazanas,KeziahReina,Vish-
nesh Ramanathan, Wei Xu, Byron Wallace, and
YixinLiu,PengfeiLiu,DragomirRadev,andGraham
Junyi Jessy Li. 2023. Multilingual simplification
Neubig.2022. BRIO:Bringingordertoabstractive
of medical texts. In Proceedings of the 2023 Con-
summarization. InProceedingsofthe60thAnnual
ferenceonEmpiricalMethodsinNaturalLanguage
Meeting of the Association for Computational Lin-
Processing,pages16662–16692,Singapore.Associ-
guistics(Volume1: LongPapers),pages2890–2903,
ationforComputationalLinguistics.
Dublin,Ireland.AssociationforComputationalLin-
RyoKamoi,TanyaGoyal,JuanDiegoRodriguez,and guistics.
GregDurrett.2023. WiCE:Real-worldentailment
forclaimsinWikipedia. InProceedingsofthe2023 Zheheng Luo, Qianqian Xie, and Sophia Ananiadou.
Conference on Empirical Methods in Natural Lan- 2023. ChatGPTasaFactualInconsistencyEvaluator
guageProcessing,pages7561–7583,Singapore.As- forTextSummarization.
sociationforComputationalLinguistics.
ChaitanyaMalaviya,SubinLee,SihaoChen,Elizabeth
WojciechKryscinski,BryanMcCann,CaimingXiong, Sieber, Mark Yatskar, and Dan Roth. 2024. Ex-
and Richard Socher. 2020. Evaluating the factual pertQA:Expert-curatedquestionsandattributedan-
consistency of abstractive text summarization. In swers. In2024AnnualConferenceoftheNorthAmer-
Proceedings of the 2020 Conference on Empirical icanChapteroftheAssociationforComputational
MethodsinNaturalLanguageProcessing(EMNLP), Linguistics.
pages9332–9346,Online.AssociationforComputa-
tionalLinguistics. AlexMallen,AkariAsai,VictorZhong,RajarshiDas,
Daniel Khashabi, and Hannaneh Hajishirzi. 2023.
PhilippeLaban,WojciechKryscinski,DivyanshAgar- When not to trust language models: Investigating
wal,AlexanderFabbri,CaimingXiong,ShafiqJoty, effectivenessofparametricandnon-parametricmem-
andChien-ShengWu.2023. SummEdits: Measuring ories. InProceedingsofthe61stAnnualMeetingof
LLM ability at factual reasoning through the lens theAssociationforComputationalLinguistics(Vol-
ofsummarization. InProceedingsofthe2023Con- ume 1: Long Papers), pages 9802–9822, Toronto,
ferenceonEmpiricalMethodsinNaturalLanguage Canada.AssociationforComputationalLinguistics.Joshua Maynez, Shashi Narayan, Bernd Bohnet, and 2023. Improving Wikipedia verifiability with AI.
Ryan McDonald. 2020. On faithfulness and factu- NatureMachineIntelligence,5(10):1142–1148.
alityinabstractivesummarization. InProceedings
of the 58th Annual Meeting of the Association for VipulaRawte,AmitSheth,andAmitavaDas.2023. A
Computational Linguistics, pages 1906–1919, On- surveyofhallucinationinlargefoundationmodels.
line.AssociationforComputationalLinguistics. arXivpreprintarXiv:2309.05922.
NickMcKenna,TianyiLi,LiangCheng,Mohammad Soumya Sanyal, Tianyi Xiao, Jiacheng Liu, Wenya
Hosseini,MarkJohnson,andMarkSteedman.2023. Wang, and Xiang Ren. 2024. Minds versus ma-
Sourcesofhallucinationbylargelanguagemodels chines: Rethinkingentailmentverificationwithlan-
on inference tasks. In Findings of the Association guagemodels. arXivpreprintarXiv:2402.03686.
forComputationalLinguistics: EMNLP2023,pages
2758–2774, Singapore. Association for Computa- Chantal Shaib, Millicent Li, Sebastian Joseph, Iain
tionalLinguistics. Marshall,JunyiJessyLi,andByronWallace.2023.
Summarizing,simplifying,andsynthesizingmedical
SewonMin,KalpeshKrishna,XinxiLyu,MikeLewis, evidence using GPT-3 (with varying success). In
Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettle- Proceedings of the 61st Annual Meeting of the As-
moyer,andHannanehHajishirzi.2023. FActScore: sociationforComputationalLinguistics(Volume2:
Fine-grainedatomicevaluationoffactualprecision ShortPapers),pages1387–1407,Toronto,Canada.
inlongformtextgeneration. InProceedingsofthe AssociationforComputationalLinguistics.
2023ConferenceonEmpiricalMethodsinNatural
Language Processing, pages 12076–12100, Singa- Liyan Tang, Tanya Goyal, Alex Fabbri, Philippe La-
pore.AssociationforComputationalLinguistics. ban,JiachengXu,SemihYavuz,WojciechKryscin-
ski,JustinRousseau,andGregDurrett.2023a. Un-
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos,
derstandingfactualerrorsinsummarization: Errors,
CaglarGulcehre,andBingXiang.2016. Abstractive
summarizers,datasets,errordetectors. InProceed-
textsummarizationusingsequence-to-sequencernns
ingsofthe61stAnnualMeetingoftheAssociationfor
and beyond. In Proceedings of The 20th SIGNLL
ComputationalLinguistics(Volume1: LongPapers),
Conference on Computational Natural Language
pages11626–11644,Toronto,Canada.Association
Learning. Association for Computational Linguis-
forComputationalLinguistics.
tics.
Liyan Tang, Yifan Peng, Yanshan Wang, Ying Ding,
Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
Greg Durrett, and Justin Rousseau. 2023b. Less
2018. Don’tgivemethedetails,justthesummary!
likelybrainstorming: Usinglanguagemodelstogen-
topic-aware convolutional neural networks for ex-
eratealternativehypotheses. InFindingsoftheAs-
treme summarization. In Proceedings of the 2018
sociationforComputationalLinguistics: ACL2023,
Conference on Empirical Methods in Natural Lan-
pages12532–12555,Toronto,Canada.Association
guageProcessing,pages1797–1807,Brussels,Bel-
forComputationalLinguistics.
gium.AssociationforComputationalLinguistics.
LiyanTang, IgorShalyminov, AmyWingmeiWong,
YixinNie,AdinaWilliams,EmilyDinan,MohitBansal,
Jon Burnsky, Jake W. Vincent, Yu’an Yang, Siffi
JasonWeston,andDouweKiela.2020. Adversarial
Singh, Song Feng, Hwanjun Song, Hang Su, Lijia
NLI:Anewbenchmarkfornaturallanguageunder-
Sun,YiZhang,SaabMansour,andKathleenMcKe-
standing. InProceedingsofthe58thAnnualMeet-
own.2024. TofuEval: EvaluatingHallucinationsof
ingoftheAssociationforComputationalLinguistics,
LLMsonTopic-FocusedDialogueSummarization.
pages4885–4901,Online.AssociationforComputa-
tionalLinguistics.
LiyanTang,ZhaoyiSun,BetinaIdnay,JordanG.Nestor,
OpenAI. 2023. GPT-4 technical report. ArXiv, AliSoroush,PierreA.Elias,ZiyangXu,YingDing,
abs/2303.08774. Greg Durrett, Justin F. Rousseau, Chunhua Weng,
andYifanPeng.2023c. Evaluatinglargelanguage
Artidoro Pagnoni, Vidhisha Balachandran, and Yulia models on medical evidence summarization. npj
Tsvetkov.2021. Understandingfactualityinabstrac- DigitalMedicine,6(1).
tivesummarizationwithFRANK:Abenchmarkfor
factualitymetrics. InProceedingsofthe2021Con- Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
ferenceoftheNorthAmericanChapteroftheAsso- Dubois,XuechenLi,CarlosGuestrin,PercyLiang,
ciationforComputationalLinguistics: HumanLan- andTatsunoriB.Hashimoto.2023. StanfordAlpaca:
guageTechnologies,pages4812–4829,Online.As- An Instruction-following LLaMA model. https:
sociationforComputationalLinguistics. //github.com/tatsu-lab/stanford_alpaca.
Fabio Petroni, Samuel Broscheit, Aleksandra Pik- Gemini Team, Rohan Anil, Sebastian Borgeaud,
tus, Patrick Lewis, Gautier Izacard, Lucas Hos- YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,Radu
seini,JaneDwivedi-Yu,MariaLomeli,TimoSchick, Soricut, Johan Schalkwyk, Andrew M. Dai, Anja
MicheleBevilacqua,Pierre-EmmanuelMazaré,Ar- Hauth,andetal.2023. Gemini: Afamilyofhighly
mandJoulin,EdouardGrave,andSebastianRiedel. capablemultimodalmodels.Romal Thoppilan, Daniel De Freitas, Jamie Hall, theAssociationforComputationalLinguistics: ACL-
Noam M. Shazeer, Apoorv Kulshreshtha, Heng- IJCNLP2021,pages4913–4922,Online.Association
Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, forComputationalLinguistics.
Yu Du, Yaguang Li, Hongrae Lee, Huaixiu Steven
Zheng,AminGhafouri,MarceloMenegali,Yanping YuhengZha,YichiYang,RuichenLi,andZhitingHu.
Huang, Maxim Krikun, Dmitry Lepikhin, James 2023. AlignScore: Evaluating factual consistency
Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, with a unified alignment function. In Proceedings
AdamRoberts,MaartenBosma,YanqiZhou,Chung- of the 61st Annual Meeting of the Association for
ChingChang,I.A.Krivokon,WillardJamesRusch, ComputationalLinguistics(Volume1: LongPapers),
MarcPickett, KathleenS.Meier-Hellstern, Mered- pages11328–11348,Toronto,Canada.Association
ithRingelMorris,TulseeDoshi,RenelitoDelosSan- forComputationalLinguistics.
tos, Toju Duke, Johnny Hartz Søraker, Ben Zeven-
bergen,VinodkumarPrabhakaran,MarkDíaz,Ben
Muru Zhang, Ofir Press, William Merrill, Alisa
Hutchinson,KristenOlson,AlejandraMolina,Erin
Liu, and Noah A Smith. 2023a. How language
Hoffman-John,JoshLee,LoraAroyo,RaviRajaku-
modelhallucinationscansnowball. arXivpreprint
mar,AlenaButryna,MatthewLamm,V.O.Kuzmina,
arXiv:2305.13534.
JosephFenton,AaronCohen,RachelBernstein,Ray
Kurzweil, Blaise Aguera-Arcas, Claire Cui, Mar-
TianyiZhang,FaisalLadhak,EsinDurmus,PercyLiang,
ianRogersCroak,EdHuaihsinChi,andQuocLe.
Kathleen McKeown, and Tatsunori B. Hashimoto.
2022. LaMDA:LanguageModelsforDialogAppli-
2024. Benchmarkinglargelanguagemodelsfornews
cations. ArXiv,abs/2201.08239.
summarization. TransactionsoftheAssociationfor
AlexWang, KyunghyunCho, andMikeLewis.2020. ComputationalLinguistics,12:39–57.
Askingandansweringquestionstoevaluatethefac-
tualconsistencyofsummaries. InProceedingsofthe YueZhang,YafuLi,LeyangCui,DengCai,LemaoLiu,
58thAnnualMeetingoftheAssociationforCompu- TingchenFu,XintingHuang,EnboZhao,YuZhang,
tationalLinguistics,pages5008–5020,Online.Asso- Yulong Chen, et al. 2023b. Siren’s song in the AI
ciationforComputationalLinguistics. ocean: asurveyonhallucinationinlargelanguage
models. arXivpreprintarXiv:2309.01219.
Yuxia Wang, Revanth Gangi Reddy, Zain Muham-
madMujahid,ArnavArora,AleksandrRubashevskii,
Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and
Jiahui Geng, Osama Mohammed Afzal, Liang-
Muhao Chen. 2023. Context-faithful prompting
mingPan,NadavBorenstein,AdityaPillai,Isabelle
for large language models. In Findings of the As-
Augenstein, Iryna Gurevych, and Preslav Nakov.
sociation for Computational Linguistics: EMNLP
2023. Factcheck-GPT: End-to-End Fine-Grained
2023,pages14544–14556,Singapore.Association
Document-Level Fact-Checking and Correction of
forComputationalLinguistics.
LLMOutput. ArXiv,abs/2311.09000.
YixuanWeng,MinjunZhu,FeiXia,BinLi,ShizhuHe, ChenguangZhu,YangLiu,JieMei,andMichaelZeng.
Shengping Liu, Bin Sun, Kang Liu, and Jun Zhao. 2021. MediaSum: A large-scale media interview
2023. Large language models are better reasoners datasetfordialoguesummarization. InProceedings
with self-verification. In Findings of the Associa- ofthe2021ConferenceoftheNorthAmericanChap-
tionforComputationalLinguistics: EMNLP2023, teroftheAssociationforComputationalLinguistics:
pages2550–2575,Singapore.AssociationforCom- HumanLanguageTechnologies,pages5927–5934,
putationalLinguistics. Online.AssociationforComputationalLinguistics.
AdinaWilliams,NikitaNangia,andSamuelBowman.
A AdditionalAnalysisandAblations
2018. A broad-coverage challenge corpus for sen-
tenceunderstandingthroughinference. InProceed-
ingsofthe2018ConferenceoftheNorthAmerican A.1 IntrinsicEvaluationonC2D/D2C
Chapter of the Association for Computational Lin-
Our model achieves strong overall performance,
guistics: Human Language Technologies, Volume
1 (Long Papers), pages 1112–1122, New Orleans, but we would like to have more insight as to
Louisiana.AssociationforComputationalLinguis- whether it actually does well at the types of in-
tics.
stancesinD2CandC2D.Weevaluatetheperfor-
Xiaoyu Yang and Xiaodan Zhu. 2021. Exploring de- manceofQAFactEval,SummaC-ZS,SummaC-CV,
compositionfortable-basedfactverification. InFind- AlignScore,FT5-ANLI-Lonourheld-outsynthetic
ingsoftheAssociationforComputationalLinguis-
dataofC2DandD2Casawaytounderstandtheir
tics: EMNLP2021,pages1045–1052,PuntaCana,
abilitytoreasonovermultiplesentencesandcon-
DominicanRepublic.AssociationforComputational
Linguistics. sidermultipleatomicfactswithinaclaim. There
are2Kheld-outinstancesfromC2DandD2C,re-
Wenpeng Yin, Dragomir Radev, and Caiming Xiong.
spectively, in the format of (document, claim,
2021. DocNLI:Alarge-scaledatasetfordocument-
level natural language inference. In Findings of label)tuples. PerformanceismeasuredbyBAcc.LLM-AGGREFACT(withoutthresholdtuning)
Model AGGREFACT TOFUEVAL CLAIM FACT EXPERT
WICE REVEAL LFQA Avg
Name VERIFY CHECK QA
CNN XSum MediaS MeetB
MiniCheck-FT5 69.9 74.3 73.6 77.3 72.2 86.2 74.6 74.7 59.0 85.2 74.7
-C2D 64.7 68.6 70.7 76.6 75.5 85.4 70.4 75.1 58.6 82.0 72.7(↓2.0)
-D2C 62.9 70.6 70.1 75.9 74.0 83.1 67.1 75.5 58.0 77.9 71.5(↓3.2)
-BOTH 54.7 59.4 54.1 55.6 61.5 77.1 57.4 65.0 51.9 63.0 59.9(↓14.8)
Table6: Ablationstudyonthetrainingdata. ModelsareevaluatedonthetestsetofLLM-AGGREFACTwithout
thresholdtuning. Weshowtheaverageperformancedowngradeinred. -BOTHdropsfrom69.1to59.9without
thresholdtuning.
Syntheticdataablations Besidethosespecial- QAFactEval SummaC-ZS SummaC-CV
izedmodels,wefine-tuneflan-t5-largeonthe AlignScore FT5-C2D-S FT5-D2C-S
FT5-ANLI-L FT5-D2C FT5-C2D
training set of C2D (FT5-C2D) and D2C (FT5-
Hold-Out C2D Hold-Out D2C
D2C),respectively. WeevaluateFT5-C2DonD2C
80 80
asanout-of-distribution(OOD)evaluationset,and
evaluateFT5-D2ConC2D. cc70 70
A
To demonstrate the necessity of steps in creat- B
60 60
ing our synthetic data, we also create simplified
50 50
versions of the synthetic data for both methods, Evaluation Models
denotedas C2D-SIMP and D2C-SIMP,eachcom-
Figure5: Performanceoffact-checkersontheheld-outs
prising7Ktrainingexamples,sameasinC2Dand
setsofC2D(left)andD2C(right). Theblackdashed
D2C. For the C2D-SIMP method, we ask GPT-4
lineshowsthein-distributionperformanceofFT5-C2D
to directly generate documents that support and
(left)andFT5-D2C(right).
not support a provided claim, with the require-
mentmentionedinthepromptthattheinferenceon
thatthemodeltrainedon163KANLIdatapoints
the claim should require reasoning over multiple
fail to reach the performance of models trained
sentences from a document. For the D2C-SIMP
solelyon7Ksyntheticdata. Thesefindingsdemon-
method,weaskGPT-4togeneratesummarysen-
strate that both of our synthetic data generation
tencesforGoogleNewsarticlesandcomeupwith
methodscaneffectivelyencouragemodelstopay
unsupported summaries by injecting errors into
more attention to multiple atomic facts and rea-
thosesummarysentencesfollowingthemethodin
son over multiple sentences, even with a limited
SummEdit (Laban et al., 2023). We denote mod-
amountoftrainingdata.
els trained on those simplified synthetic data as
FT5-C2D-SandFT5-D2C-S.Moredetailsforcre-
A.2 AblationofD2C/C2D
atingthesesyntheticdatasetscanbefoundinAp-
pendixF. Weobservethattherearestillgapsbetweentheop-
timalperformanceandthatachievedbyotherspe-
Syntheticdataneedstobecarefullyconstructed cializedfact-checkersinFigure5. Notably,Align-
for an fact-checker to work well. Figure 5 Score demonstrates the best performance among
showsthein-distributionperformanceofFT5-C2D the four specialized metrics, but its performance
and FT5-D2C as optimal performance in each can still be outperformed by FT5-C2D and FT5-
sub-figure,representedbytheblackdashedlines. D2C.AsshowninTable2,wecanimproveAlign-
Weobservethatmodelstrainedonsyntheticdata Score’s performance on the benchmark by fine-
withsimplifiedconstructionsteps(C2D-SIMPand tuningitonthissmallamountofdata,effectively
D2C-SIMP)failtodevelopthedesiredproperties equippingitwiththedesiredproperties.
weexpect. FT5-D2C-Sperformsevenworsethan Tofurtherinvestigate,weconductedanablation
randomchanceontheheld-outsetofC2D.Incon- study on our top-performing model, MiniCheck-
trast,modelstrainedonC2DandD2Coutperform FT5,byremovingoursyntheticdatafromthetrain-
allotherfact-checkersontheOODheld-outsetof ing set. The results, presented in Table 6, reveal
D2CandC2D,respectively. Additionally,wenote that the two types of synthetic data complementLLM-AGGREFACT(withthresholdtuning)
Model AGGREFACT TOFUEVAL CLAIM FACT EXPERT
WICE REVEAL LFQA Avg
Name VERIFY CHECK QA
CNN XSum MediaS MeetB
T5-NLI-Mixed 59.9 56.1 60.6 55.9 58.0 87.6 61.5 69.3 56.8 62.8 62.9
DAE 58.6 67.1 67.6 63.2 57.1 83.8 71.1 72.6 58.6 68.5 68.5
QAFactEval 63.9 63.7 64.0 66.8 65.8 85.3 73.3 72.1 57.6 81.5 69.4
SummaC-ZS 63.0 67.2 69.5 70.0 61.8 86.4 69.9 75.7 57.5 82.0 70.4
SummaC-CV 67.6 69.7 68.2 71.0 66.1 87.3 71.3 74.3 59.3 71.3 71.3
AlignScore 62.6 69.6 71.6 71.8 66.8 85.3 72.9 76.5 59.2 85.6 72.2
MiniCheck-RBTA 64.6 70.2 71.1 75.2 73.7 88.0 77.1 77.3 58.4 84.2 73.7
MiniCheck-DBTA 63.4 74.7 69.1 72.8 76.3 87.4 75.5 76.0 58.8 84.1 73.8
MiniCheck-FT5 71.5 74.8 73.7 76.7 75.0 86.4 73.8 76.4 58.6 84.4 75.1
Table7: PerformanceofmodelsonthetestsetofLLM-AGGREFACTwiththresholdtuningonthevalidation
set. Balancedaccuracyiscomputedforeachmodelonthe10datasetsinLLM-AGGREFACT,andtheaverageis
computed. Ineachdataset,afactualitymetricselectsathresholdbasedontheperformanceonthecorresponding
validationset.
each other. Notably, the model performs poorly SummaC-CV. These results suggest that our syn-
when trained solely on the ANLI subset, with a theticdatanotonlyimprovesoverallperformance
performance drop of nearly 10% in the absence butalsoenhancestherobustnessofmodelsacross
of threshold tuning. However, the addition of ei- the domains of our benchmark, enabling them to
ther 7K C2D or 7K D2C to the training data sig- maintainstrongperformanceevenwithoutthresh-
nificantlyenhancesthemodel’sperformanceand oldtuning.
robustness.
B.2 FullResultsUsingClaimDecomposition
B AdditionalResults
InTable8,weshowtheperformance(andperfor-
mancechanges)ofGPT-4andasubsetofspecial-
B.1 ResultsusingThresholdTuning
izedfact-checkersacrossalldatasetsfrom LLM-
As shown in Table 7, AlignScore achieves the
AGGREFACT,usingclaimdecompositiontodeter-
highest overall performance (72.2%) on LLM-
minethefactualitylabelforeachclaim.
AGGREFACT among fact-checkers from prior
work. However,byfine-tuningAlignScore’sback- B.3 FullResultsUsingClaim
boneRoBERTamodelonour14Ksyntheticdata Decontextualization
(MiniCheck-RBTA),wesurpassAlignScore’sper-
In Table 9, we show the performance (and per-
formance by 1.5%. This improvement is more
formance changes) of GPT-4 and a subset of
significantinthesettingwithoutthresholdtuning
specializedfact-checkersacrossalldatasetsfrom
(Section6). Remarkably,thisboostinperformance
LLM-AGGREFACT,usingclaimdecontextualiza-
isattainedusingadatasetthatconstituteslessthan
tion when applicable. In particular, we perform
0.3% of the total data on which AlignScore was
claimdecontextualizationon TOFUEVAL-MediaS,
initiallytrained(Table3). Thisfindinghighlights
TOFUEVAL-MeetB, WICE, REVEAL, CLAIMVER-
thepotentialofcuratedsyntheticdatainenhancing
IFY, EXPERTQA, and LFQA. Note that the
the performance of state-of-the-art fact-checkers.
claimdecontextualizationstepaddanon-negligible
Overall, our models achieve new state-of-the-art
amountofcost,asshowninTable4.
amongspecializedmodelsunderthethresholdtun-
ingsetting.
B.4 ResultsPredictingAllErrors
Oursyntheticdataenhancesmodelrobustness Whenagroundingdocumentisrelevanttomultiple
and performance in the absence of threshold sentencesinaresponse,itbecomesfeasibletoask
tuning. ComparingTable2withTable7,wesee anLLM-basedfact-checkertosimultaneouslypre-
thattheperformanceofspecializedfact-checkers dictthefactualitylabelsforallsentences,thereby
decreases without threshold tuning. However, reducing cost. We investigate this approach us-
MiniCheck-FT5onlydropsby0.4%comparedto ingGPT-4onthe TOFUEVAL datasets,wherewe
larger drops for other systems, such as 9.2% for provideGPT-4withadocumentandtheentiresum-LLM-AGGREFACT(decomposition-withoutthresholdtuning)
Model AGGREFACT TOFUEVAL CLAIM FACT EXPERT
WICE REVEAL LFQA Avg
Name VERIFY CHECK QA
CNN XSum MediaS MeetB
GPT-4 70.1 74.3 70.9 79.4 77.6 87.4 73.2 79.9 59.6 84.9 75.6(↑0.3)
SummaC-CV 65.2 51.2 58.0 55.2 50.9 66.2 69.6 53.4 53.5 65.0 58.8(↓3.3)
QAFactEval 61.7 60.4 65.0 60.2 59.1 81.7 68.6 66.1 54.6 73.0 64.6(↓1.9)
SummaC-ZS 62.4 64.5 64.5 70.0 64.8 85.6 70.0 75.2 56.3 77.2 69.1(↑1.2)
AlignScore 65.6 68.3 71.2 73.2 63.9 86.2 73.7 74.3 57.6 82.6 71.5(↑1.1)
MiniCheck-RBTA 65.0 72.1 72.6 75.1 66.9 88.5 76.1 73.3 58.0 84.3 73.2(↑0.5)
MiniCheck-DBTA 59.9 73.3 67.9 74.5 75.0 88.2 72.5 73.0 57.9 84.5 72.7(↑0.1)
MiniCheck-FT5 65.9 71.7 69.2 77.4 72.9 87.0 73.5 74.7 57.5 83.2 73.3(↓1.4)
Table8: PerformanceofmodelsonthetestsetofLLM-AGGREFACTbyaggregatingpredictionsondecomposed
claims. WeincludetheperformancechangecomparedtopredictingusingoriginalclaimsfromTable2(redfor
worseperformanceandgreenforbetterperformance).
LLM-AGGREFACT(decontextualization-withoutthresholdtuning)
Model AGGREFACT TOFUEVAL CLAIM FACT EXPERT
WICE REVEAL LFQA Avg
Name VERIFY CHECK QA
CNN XSum MediaS MeetB
GPT-4 66.7 76.5 71.5 79.2 80.3 87.0 66.2 79.9 60.1 86.4 75.3(+0.0)
SummaC-CV 65.2 54.5 62.6 62.1 52.5 67.8 69.0 53.4 54.7 66.5 60.8(↓1.3)
QAFactEval 54.3 62.1 62.8 64.5 62.7 82.8 71.7 66.9 56.5 80.3 66.4(↓0.1)
SummaC-ZS 51.1 61.5 67.5 71.4 63.0 85.8 69.0 75.2 56.4 76.1 67.7(↓0.2)
AlignScore 52.4 71.4 68.8 72.2 65.1 85.5 69.1 74.3 59.6 85.2 70.4(+0.0)
MiniCheck-RBTA 63.7 70.8 70.9 75.6 64.3 88.9 76.0 73.3 57.5 83.2 72.4(↓0.3)
MiniCheck-DBTA 64.2 71.0 69.6 69.1 63.3 87.5 73.6 73.0 57.8 83.2 71.2(↓1.4)
MiniCheck-FT5 69.9 74.3 74.0 75.6 69.7 86.2 73.0 74.7 58.4 85.2 74.1(↓0.6)
Table9: PerformanceofmodelsonthetestsetofLLM-AGGREFACTbydoingclaimdecontextualizationwhereit
isapplicable. WeincludetheperformancechangecomparedtopredictingusingoriginalclaimsfromTable2.
mary,askingthemodeltopredictthefactualityla- Model TOFUEVAL
belsforallsummarysentencesatonce,denotedas Name
MediaS MeetB
GPT4-Full. ThepromptisshowninTable27. Ta-
ble10showsthatGPT4-Fullachievesperformance GPT-4 71.4 79.9
similartopredictingthefactuallabelforeachsum- GPT-4-Full 72.3 79.7
marysentenceindividually. Theinferencecoston
theTOFUEVALtestsetcanbereducedfrom$16.7 Table 10: Comparison of GPT-4 and GPT-4-Full on
to$6.72withthismethod.
TOFUEVAL,adatasetwheresentencesfromanLLM-
generatedresponsesharethesamegroundingdocument.
However,inretrieve-then-generateandpost-hoc
groundingsettings,evidenceistypicallyretrieved
for each claim separately, meaning no document
tuned summarizers, since their analysis suggests
can be shared across claims in a single response,
thatsummariesaremorechallengingtoevaluatefor
andthustheinferencecostisbarelyreduced.
factual consistency compared to summaries gen-
C LLM-AGGREFACT Details erated by pre-SOTA summarizers. Data in AG-
GREFACTcomesfrom9factualconsistencyevalu-
C.1 DatasetDescriptions
ationdatasetsonCNNorXSum,includingwidely
AGGREFACT (Tang et al., 2023a) is a factual used ones such as SummaC (Laban et al., 2022),
consistency evaluation benchmark for new sum- FRANK (Pagnoni et al., 2021), and SummEval
marization,targetingCNN(/DM)(Nallapatietal., (Fabbrietal.,2021). CheckAppendixCforacom-
2016) and XSum (Narayan et al., 2018). Our plete set of evaluation datasets in AGGREFACT.
focus is on the SOTA sets within AGGREFACT, Because CNN/DM and XSum feature quite dif-
where summaries are generated from SOTA fine- ferent styles of summaries, we report these num-bersseparatelyinourbenchmark. However,wedo Doc Claim %of
Dataset Split Size
nototherwisereportresultsonthesmallerdatasets Len Len Neg
withinthe AGGREFACTSOTAsubset. dev 459 558 56 13%
CNN
test 558 563 58 10%
AGGREFACT
TOFUEVAL (Tangetal.,2024)isafactualcon-
dev 777 374 26 49%
XSum
sistency evaluation benchmark for dialogue sum- test 558 370 25 49%
marization,targetingMediaSum(interviews,Zhu dev 1800 990 21 20%
Media
etal.(2021))andMeetingBank(citycouncilmeet-
TOFUEVAL
test 726 922 21 24%
ings, Hu et al. (2023)). It includes topic-focused dev 1607 930 22 18%
MeetB
test 772 915 22 19%
dialogue summaries generated by 6 LLMs, with
sentence-levelfactualconsistencyannotationsby WICE dev 349 1622 27 67%
test 358 1683 28 69%
linguists.
dev 1656 104 11 77%
REVEAL
WICE (Kamoi et al., 2023) is a textual entail- test 1710 103 11 77%
ment dataset that consists of naturally occurring CLAIM dev 1093 1874 21 25%
claimsfromWikipediaandtheirciteddocuments.
VERIFY test 1088 1841 22 27%
Based on its cited documents, each claim is la- FACT dev 1537 100 14 83%
CHECK test 1566 100 13 76%
beled as supported, partially-supported, or
non-supported. EXPERT dev 3773 491 29 22%
QA test 3702 506 29 20%
REVEAL (Jacovi et al., 2024) is a benchmark dev 2029 383 25 43%
LFQA
test 1911 380 25 41%
datasetthatevaluatesthecorrectnessofreasoning
chainsgeneratedbyLLMsinthecontextofopen-
Table11:StatisticsofdatasetsinLLM-AGGREFACT.
domainquestion-answering. Thedatasetincludes
Weshowthesizeofdatasets,theaveragelengthofdoc-
annotationsatthesentencelevel,coveringvarious
umentsandclaims,andtheproportionofunsupported
aspects of response correctness. For our dataset, claims.
we focus on the subset of sentences that have at-
tribution annotations, which indicate whether a
sentenceinareasoningchaincanbeattributedto
informationretrievedfromWikipediaparagraphs
EXPERTQA (Malaviyaetal.,2024)containsre-
sponsesfrom6differentsystemstoqueriescurated
withthreelabelcategories: fully attributable,
by experts from 32 fields. These systems answer
partially attributable,orcontradictory.
querieseitherinaclose-bookfashionwith/without
CLAIMVERIFY (Liuetal.,2023)evaluatesthe in-linecitations,orbasedonretrieveddocument(s).
correctness of responses from four generative For each sentence in the response, the sentence
search engines in answering user queries. Simi- is verified against the concatenation of cited or
lar to WICE, the dataset contains annotations on retrieved document(s), if any. We include exam-
whethercheck-worthysentencesfromtheengines’ ples where documents are judged as complete,
responses can be fully supported by their associ- partial, or incomplete in supporting the corre-
atedciteddocuments. Thedatasetcontainsbinary- sponding sentences. We do not include human
levelfactualconsistencyannotationsforeachcite- editedclaimsandevidenceinourbenchmark.
worthysentence.
FACTCHECK-GPT (Wang et al., 2023) con-
tainsfactualconsistencyannotationsforLLMs’re- LFQA (Chen et al., 2023) contains LLM-
sponsestosearchqueries. Inthisdataset,eachsen- generated responses to questions from the ELI5
tence from LLMs’responses is first decomposed (“ExplainLikeI’mFive”)dataset(Fanetal.,2019).
intoatomicfactsandthoseatomicfactsarethenfur- LLMsgenerateresponsesbasedondocumentsthat
therdecontextualizedsothattheycanstandalone. are either retrieved by humans, models, or ran-
Finally,eachworth-checkinganddecontextualized domly selected. Human annotators then evalu-
atomic fact is labeled as completely support, ateeachsentenceintheLLM-generatedresponses
partially support, refute, or irrelevant. againstthecorrespondingdocumentset,classify-
Weincludethosedecontextualizedatomicfactsand ingthemintosupported,partially supported,
theircorrespondingdocumentsinthebenchmark. ornot supported.C.2 LabelUnification Model Checkpoint
For AGGREFACT, TOFUEVAL, and CLAIMVER- Gemini-Pro gemini-1.0-pro
IFY,wekeepusingthebinarylabelfromtheorig- PaLM2-Bison chat-bison@001
inal work. For the remaining datasets, we map Mistral-8x7B open-mixtral-8x7b
supported, fully attributable, completely
Mistral-Large mistral-large-2402
support, and complete to supported, and Claude-2.1 claude-2.1
unsupportedotherwise.
Claude-3Opus claude-3-opus-20240229
GPT-3.5 gpt-3.5-turbo-0125
C.3 ExcludedDatasets
GPT-4 gpt-4-0125-preview
We excluded HALUEVAL (Li et al., 2023) and
SUMMEDITS(Labanetal.,2023)fromourbench- Table12: LLMcheckpoints
marksincetheyaresynthetic,witherrorsinsum-
mariesgeneratedviainstructionpromptsthatguide
AppendixH.1forhowwemaintainthequalityof
the model to intentionally make errors in sum-
oursyntheticdata.
maries. These errors are unnatural and do not fit
with our goal of detecting true LLM generation Characteristics of Synthetic Data It is worth
errors. FACTSCORE (Min et al., 2023) contains noting that constructing our synthetic dataset in-
naturally generated biographies from LLMs and volvesusinghuman-writtenornaturallygenerated
has human-annotated labels of individual atomic claims,whichsetsitapartfrompriorsyntheticdata
facts. However, humans could potentially search generationmethodsusedtotrainfact-checkersfor
differentarticlestoverifythecorrectnessofthose textsummarization. Thesemethods,suchasentity
sentencesthantheonesthatmodelsretrieve. Asa swappingandsentencenegation(Kryscinskietal.,
result,anon-negligiblefractionoftheclaimsinthe 2020;GoyalandDurrett,2021),weredesignedto
dataset appear mislabeled from the standpoint of targetspecificerrortypesthatoccurredinclaims
thefact-checkingongroundeddocumentstask. fromearliersummarizationmodels. However,as
errorsfromgenerativemodelsprogress(Tangetal.,
C.4 Statistics
2023a) and new error types emerge from LLMs,
ThestatisticsofLLM-AGGREFACTcanbefound focusing on specific error types may not general-
in Table 11. Our use of these datasets is for re- izewelltounseendatasetswithpotentiallynovel
searchpurposesonly,whichisconsistentwiththeir errors.
intendeduse. Examples of synthetic data for C2D and D2C
AGGREFACT contains the following 9 factual canbefoundinTable14and15.
consistencyevaluationdatasetsonCNNorXSum:
E Fact-CheckingModelDetails
FactCC(Kryscinskietal.,2020),Wang’20(Wang
etal.,2020),SummEval(Fabbrietal.,2021),Poly-
E.1 LLM-BasedFact-Checkers
tope (Huang et al., 2020), Cao’22 (Cao et al.,
We use the official APIs for LLM-based fact-
2022),XSumFaith(Maynezetal.,2020),FRANK
checkers. Thecheckpointswe useforLLMs can
(Pagnonietal.,2021),Goyal’21(GoyalandDur-
befoundinTable12. Theinferencepromptisthe
rett,2021),andCLIFF(CaoandWang,2021).
same for all LLMs and can be found in Table 22.
Weuseatemperatureofzerotocollectdeterminis-
D SyntheticDataDetails
ticoutputs,whichistypicalfrompreviouswork.
SourceofData InourC2Dmethod,wechoose
E.2 SpecializedFact-Checkers
around400claimsfromWikipediathathavecited
web articles (Kamoi et al., 2023; Petroni et al., QAFactEval (Fabbrietal.,2022)isaQA-based
2023) to generate synthetic documents. In our fact-checker with optimized components for an-
D2Cmethod,wescrapedaround300GoogleNews swerselection,questionanswering,questiongen-
articles since November 2023 from diverse topic eration,andansweroverlapcalculation. Itselects
categories, including science, politics, world, en- spans as answers from a summary sentence, gen-
tertainment, business, and technology. Each doc- eratesquestionsbasedontheseanswers,andthen
ument is approximately 500 words. Statistics of answersthesequestionsusingthesourcedocument.
our generated data can be found in Table 1. See Finally, it computes an overall overlap score forthesummarysentencebycomparingtheselected scoresofindividualsentences. AswithSummaC-
spansfromthesummarysentencewiththeanswers ZS, we use paragraph-level segmentation in our
derivedfromthesourcedocument,giventhegen- experiments and keep other settings as default.
eratedquestions. QAFactEvalproducesscoreson SummaC-Convoutputsascorebetween0and1.
a continuous scale ranging from 0 to 5. In our
AlignScore (Zha et al., 2023) is an entailment-
experiments,weusethedefaultmodelandhyper-
basedmodelthathasbeentrainedondatafroma
parametersasprovidedbytheauthors.
wide range of tasks such as NLI, QA, fact verifi-
DAE (Goyal and Durrett, 2020, 2021) is an cation, and summarization. It works similarly to
entailment-based fact-checker that evaluates the SummaC-ZS,withtheonlydifferencebeing that
factual consistency of each dependency arc in it splits a document D i = d i,1,...,d i,|d| into
{ }
a summary sentence. It independently verifies sequential chunks at sentence boundaries. Each
whether the semantic relationship of each depen- chunk contains approximately 350 tokens, deter-
dencyarcisfactuallysupportedbythesourcedoc- minedbywhitespacesplitting. Inourexperiments,
ument. Finally,itaggregatesthescoresforallde- weusethedefaultmodelandhyperparametersas
pendencyarcstocomputeanoverallsentence-level provided by the authors. AlignScore outputs a
factuality score ranging from 0 to 1. In our ex- scorebetween0and1.
periments,weusethedefaultmodelandhyperpa-
T5-NLI-Mixed (Honovich et al., 2022) is an
rametersasprovidedbytheauthorsinGoyaland
entailment-based fact-checker built on T5-XXL.
Durrett(2021).
IthasbeentrainedonadiversesetofNLIdatasets
andpredictswhetheragivenclaimissupportedby
SummaC-ZS (Laban et al., 2022) is an
a document, outputting “1” for supported claims
entailment-basedfact-checker. Toevaluateasum-
and “0” for unsupported ones. The final entail-
mary sentence c , it divides the source document
i
ment score is calculated as the probability of the
D into a set of sentences or paragraphs D =
i i
model predicting the token “1”. To optimize its
d ,...,d ,andthescoreforc isdetermined
i,1 i,|d| i
{ } performanceon2GPUsfromourhardwaresetup,
by the highest score among all (d ,c ) pairs,
i,j i
weselectachunksizeof350tokensaccordingto
i.e., score(c ) = max M(di,j,c ). For a multi-
i j i
the T5 tokenizer. T5-NLI-Mixed outputs a score
sentencesummary,thefinalscoreiscalculatedas
between0and1.
the average of the individual sentence scores. In
our experiments, we do not use the authors’ de-
MINICHECK-RBTA, MINICHECK-DBTA also
faultsettingofsplittingthedocumentD intosen-
i split a document into chunks at sentence bound-
tencesandinsteadchooseparagraph-levelsegmen-
aries, with a chunk size of approximately 400 to-
tation,asmostdatapointsinLLM-AGGREFACTre-
kens according to RoBERTA and DeBERTa tok-
quirereasoningacrossmultiplesentences. Wefind
enizers. This results in approximately the same
this change not only improves the overall perfor-
chunk size as in AlignScore, which has a chunk
mancebutalsoacceleratesinferencespeed. Apart
sizeof350tokensusingwhitespacesplitting. The
from this adjustment, we adhere to the default
outputscoresfallwithintherangeof0to1.
model and hyperparameters provided by the au-
thors. SummaC-ZSreturnsascorebetween-1and FT5-ANLI-L,MiniCheck-FT5 workthesame
1. way as T5-NLI-Mixed, but using only one GPU
andsettingthechunksizeto500tokensusingwhite
SummaC-CV (Laban et al., 2022) extends space splitting. The output scores fall within the
SummaC-ZSbyconsideringallentailmentscores rangeof0to1.
foreachsummarysentencec . SimilartoSummaC-
i
E.2.1 MachineConfigurationforSpecialized
ZS, SummaC-Conv evaluates a summary sen-
Fact-Chekers
tencec bydividingthesourcedocumentD into
i i
D = d ,...,d . However,insteadofselect- WeusetwoNVIDIA RTX A6000GPUsforT5-NLI-
i i,1 i,|d|
{ }
ingthemaximumscoreamongall(d ,c )pairs, Mixed,givenitsmodelsize,andoneGPUforthe
i,j i
SummaC-Convusesalearnedconvolutionallayer remaining models, all on our own hardware. Ac-
totransformthedistributionofentailmentscores cordingtoLambda,6 asingleNVIDIA RTX A6000
M(d ,c ) : j into a single score. The fi-
i,j i 6Detailed price specifications are available at
{ ∀ }
nalsummaryscoreiscomputedbyaveragingthe https://lambdalabs.com/service/gpu-cloud##pricing.GPUcosts$0.8perhourandhas48GBVRAM. constructtheirSummEditfactualconsistencyeval-
uationbenchmark. Theeditingpromptisprovided
F BaselineSyntheticDataGeneration in Table 26. We sample 7K datapoints from the
Methods generateddatatoconstruct D2C-SIMP.
Wedescribethesimplifiedmethodsingenerating
G TrainingDetails
C2D and D2C datasets, denoted as C2D-SIMP
andD2C-Simp. Performanceonmodelstrainedon Weincludethetrainingdetailsandhyperparemater
thosesimplifiedsyntheticdatasetscanbefoundin detailsinthesection. Unlessotherwisespecified,
SectionA.1. weusethedefaulthyperparematersofthebackbone
Themotivationforthesemodelsistocapturethe models. Allmodelsaretrainedusingthestandard
performanceofamorebasicpromptingapproach, cross-entropylossfunction.
wherewesimplyaskGPT-4togenerateadatain- Forourbaselinemodels: FT5-C2D, FT5-D2C,
stanceinoneshot. Comparingtheperformanceof FT5-ANLI-L, FT5-C2D-S, and FT5-D2C-S,
thiswithC2D/D2Chelpsvalidateourmoresophis- wefine-tuneflan-t5-large7 for2epochsonpre-
ticatedpromptingstrategy. pared data described in Section 5 and A, using a
batchsizeof4andalearningrateof5e-5.
C2D-SIMP Togeneratethe C2D-SIMP dataset,
For MiniCheck-RBTA, MiniCheck-DBTA and
we begin by providing GPT-4 with a claim c and
MiniCheck-FT5, we begin by fine-tuning the
asking it to create a supporting document D that
tuned RoBERTa-Large model from AlignScore,
requiresmultiplesentencestogethertosupportthe
deberta-v3-large8, and flan-t5-large on
claim(seeTable24forthegenerationprompt). We
their respective training data (Section 3.3) for 2
thenaskGPT-4tominimallymodifyD tocreatea
epochs, while excluding 7K D2C synthetic data.
newdocumentD′,whichcansupportsomeatomic
Weuseabatchsizeof4withanaccumulationstep
factsmentionedincbutnotallofthem. Inspiredby
of2andalearningrateof1e-5forRoBERTaand
theerrortypedefinitionsfromTangetal.(2023a),
5e-5fortheothertwomodels. Wethenfine-tune
we provide four different revision types to help
thesemodelson7KD2Csyntheticdatafor1epoch,
GPT-4generatediversenon-supportingdocuments,
withabatchsizeof4andlearningrateof1e-5.
covering various reasons for not supporting the
Weobservethatfollowingthistrainingpipeline
claim(seeTable25fortheprompt). Asthegener-
consistentlyyieldshigherperformanceacrossall
atedsupportingdocumentsforagivenclaimtendto
three backbone models compared to training on
besimilardespiteadjustingthemodeltemperature,
alldatasimultaneously. Wehypothesizethatthis
wedonotgeneratemultiplesupportingdocuments
improvement stems from the fact that the source
for c. Instead, for each claim, we generate one
documentsintheD2Cdatasetarehuman-written
supportingandpairitwithonenon-supportingdoc-
documents, incontrasttothesyntheticallygener-
ument. Toenhancethediversityofthetrainingdata
ated source documents in the C2D dataset. Fine-
andmaintainacomparabledatasetsizetoourC2D
tuningontheserealisticdocumentsattheendhelps
method, we randomly select 3,500 claims from
the models adapt back to a realistic distribution,
Wikipediawithcitedwebarticles,resultinginthe
preventingthemfromoverfittingtosyntheticdocu-
C2D-SIMPdatasetcontaining7Kdatapoints.
mentsandallowingthemtoperformwellonreal
D2C-SIMP We start by directly using the documentsinthebenchmark.
summary sentences generated using the chunk-
H Prompts
level summarization step of our D2C method
(Section 3.2). That is, for each human writ-
InTable13,wepresentthefulllistoftheprompts
ten document, we have three document chunks
used throughout our work. We use GPT-3.5 for
D ,D ,D andcorrespondingsupportingsum-
1 2 3
{ } sentencedecompositionandmergingatomicfacts,
mary sentences c ,c ,c generated by GPT-4.
1 2 3
{ } and GPT-4 for the remaining prompts. We next
For each (D ,c ,1) tuple, we ask GPT-4 to mod-
i i
elaborateonhowweensurethelabelingqualityof
ify the summary sentence c such that the edited
i
oursyntheticallygenerateddata.
summary sentence c′ is no longer supported by
i
thedocumentchunkD i. Weleveragetheediting 7huggingface.co/google/flan-t5-large
methodfromLabanetal.(2023),whichisusedto 8huggingface.co/microsoft/deberta-v3-largePromptFunctionality Ref. anentailmentcheckonthe(D, c)pairwithGPT-
4mayintroducemanylabelingerrors,whichcan
Sentencedecomposition Table16
Atomicfactexpansion(C2D) Table17 negatively impact the performance of the trained
Documentgeneration(C2D) Table18
models.
Supportingdoc.generation(C2D-SIMP) Table24
Non-supportingdoc.generation(C2D-SIMP) Table25
Mergingatomicfacts(C2D,D2C) Table21
Chunk-levelsummarization(D2C,D2C-Simp) Table20
Non-supportingdoc.generation(D2C-SIMP) Table26
Entailmentcheckfordataconstruction Table19
Zero-shotfactualconsistencyevaluation Tabel22
Sentencedecontextualization Table23
Table13: Referencestoprompts. Upper: promptsfor
oursyntheticdatagenerationmethods,andsimplified
syntheticdatagenerationmethods. Lower: Promptsfor
evaluationonLLM-AGGREFACT.
H.1 QualityAssuranceforGenerations
Sentence decomposition We adapt a few-shot
sentencedecompositionpromptfrom(Kamoietal.,
2023), which can generate complete and correct
atomicfactsmostofthetimeaccordingtotheirhu-
manevaluation. Theprompt(Table16)isusedfor
bothofoursyntheticdatagenerationmethodsand
theclaimdecompositionexperimentinSection7.1.
C2D: Atomic fact expansion We use a 4-shot
prompt(Table17)forthisstep,whereweaskGPT-
4toproduceasentencepairwheretheatomicfact
is supported if and only if the information from
bothsentencesiscombined. Toensurethequality
ofthegeneration,weverifythecorrectnessofthis
conditionaftergenerationviaanentailmentcheck
byGPT-4(Table19). Ifthecorrectnessisnotmet,
we iterate the process and regenerate a new sen-
tence pair up to a specified number of attempts.
In cases where the correctness criterion remains
unmetafterthespecifiedattempts,weremovethe
datapointfromthedataset.
C2D:Supportingdocumentgeneration Ween-
sure that all sentences s from the generated sen-
tencepairsarementionedin(andhenceareentailed
by)thegenerateddocumentD byusingtheentail-
mentcheckbyGPT-4. Sameasabove,ifthedoc-
umentfailstomentionallsentencesfromthesen-
tencepairs,weiterativelygeneratenewdocuments
untilaspecifiednumberofattemptsisreached. Itis
importanttonotethatweonlyverifywhethersen-
tencessarementionedinD,aswebelieveGPT-4
canperformwellonthissimpletask. Byconstruc-
tion, if all sentences are mentioned in D, then c
issupportedbyD. However,directlyperformingDocument Claim Label
Morethan5,000individuals,partofacaravanthatcrossedinto Bythisdate, over5,000membersofthecaravan S
Mexicolastmonth,arenowseekingasylumandhaveestablished werestayingattheTijuanaStadium—astructure
atemporaryencampmentattheTijuanaStadiumasoftoday.The withacapacityof3,000.
TijuanaStadium,knownforhostingsportingevents,recently
Bythisdate, over5,000membersofthecaravan S
underwentrenovationsthatdoubleditsseatingcapacity. Prior
werestayingattheTijuanaStadium.
tothesechanges,thestadiumhadacapacitytoaccommodate
1,500spectators. TheTijuanaStadiumhasacapacityof3,000. S
More than 5,000 individuals who are part of a caravan that Bythisdate, over5,000membersofthecaravan U
crossedintoMexicolastmonthhavenowestablishedatemporary werestayingattheTijuanaStadium—astructure
encampmentattheTijuanaStadium,wheretheyarereportedly withacapacityof3,000.
seekingasylum.Thestadium,knownforhostingsportingevents,
Bythisdate, over5,000membersofthecaravan S
couldoriginallyaccommodate1,500spectatorsbeforeitbecame
werestayingattheTijuanaStadium.
thesiteofthemakeshiftsettlement.Asoftoday,thefacilityis
beingrepurposedtoprovidetheasylumseekerswithtemporary TheTijuanaStadiumhasacapacityof3,000. U
shelterastheyawaittheprocessingoftheirclaims.
Asoftoday,agroupofindividualshasestablishedatemporary Bythisdate, over5,000membersofthecaravan U
encampmentwithinthepremisesoftheTijuanaStadium, ac- werestayingattheTijuanaStadium—astructure
cordingtoofficials.Thestadium,whichhasrecentlyundergone withacapacityof3,000.
extensiverenovationsthatincludedanexpansiontodoubleits
Bythisdate, over5,000membersofthecaravan U
originalcapacity,cannowwelcomeasignificantlylargeraudi-
werestayingattheTijuanaStadium.
ence. Priortotheupgrade,theTijuanaStadiumwasknownto
haveaseatingcapacityfor1,500spectators;therecentimprove- TheTijuanaStadiumhasacapacityof3,000. S
mentsareexpectedtoenhanceitsutilityforvariouseventsand
gatherings.
Inasignificantmovementattheborder,acaravancomprising Bythisdate, over5,000membersofthecaravan U
over5,000asylumseekerspenetratedMexico’sboundarieslast werestayingattheTijuanaStadium—astructure
month,bringingtotheforefronttheongoingchallengesfacedby withacapacityof3,000.
migrantsfrommultipleorigins.Thegrouphastodayestablished
Bythisdate, over5,000membersofthecaravan S
amakeshiftcampwithintheconfinesoftheTijuanaStadium,a
werestayingattheTijuanaStadium.
venueknownforitsrecentrenovationthatdoubleditsseating
capacity. Thetemporalshiftmarksanewchapterfortheindi- TheTijuanaStadiumhasacapacityof3,000. U
vidualsontheirquestforsafetyandstability,withthestadium
offeringatransientsanctuaryastheynavigatetheirnextsteps.
Table14: ExamplesusingtheC2Dmethod. Documentsaregeneratedfromtheclaim(fromWikipedia): Bythis
date,over5,000membersofthecaravanwerestayingattheTijuanaStadium—astructurewithacapacityof
3,000. Thesameclaimcanbebothsupported(S)andunsupported(U)bydocuments,whichencouragemodelsto
payattentiontomultipleatomicfactsinasentence. Determiningthefactualitylabelsofclaimsrequiresmodelsto
reasonovermultiplesentences. Unsupportingfactsarehighlighted.Document Claim Label
(DocChunk1)WiththeSAG-AFTRAstrikesettled,thesix-month,multi-guildHol- 2024 box-office hopes S
lywoodlabordisruptionhasfinallyended,butthetheatricaldamagehasonlybegunto dashedbyproductionde-
surface.Reviewingthefilmsdelayeduntilnextyear,aroughestimatesuggeststhatthe laysandmajortitlepost-
stoppagecosttheatersaround$400million-$600millioningross–more,whenincluding ponements,costingpoten-
lostconcessionrevenue. "Barbie,""Oppenheimer,""SoundofFreedom,"and"Taylor tially$500million.
Swift: TheErasTours"keptthedamagefrombeingworse. Immediatelyfollowingthe
SAG-AFTRAsettlement,Disneyannouncedwholesaledelaysinitsupcomingrelease
schedule. TheirrevivedplansincludesonlyoneMarveltitlefor2024("Deadpool3,"
movedtoJuly26fromMay3), downfromthecustomarythreeperyearfromMCU.
Disneywasamongthefirststudiostoannouncedelays,withSonyalreadyoutWednesday
eveningwithwordthethird"Venom"filmwouldmovefromJulytoNovember.Related
StoriesThegoodnewsfortheatersisdespiteitall,2023shouldstillreachthe$9billionin
domesticgrosshopedforthisyear.
(DocChunk2;correspondingchunk)However,anyhopesthat2024mightreturnto 2024 box-office hopes S
2019box-officeparityaredashed.Grossesfromrescheduledtitleswillhelp,butproduction dashedbyproductionde-
delayswillleavesubstantialgaps. Evenso: Itcouldhavebeenworse. "Dune: Part2" laysandmajortitlepost-
(WarnerBros.)and"KraventheHunter"and"Ghostbusters:FrozenEmpire"(Sony)will ponements,costingpoten-
costthisyear’stotalthemost–perhaps$400million("Ghostbusters"wouldhavehadonly tially$500million.
12daysof2023play). Figureotherfilms,mostlylimited/specializedentrieslikeLuca
Guadagnino’s"Challengers"andJeffNichols"TheBikeriders"(Disney),EthanCoen’s
"DriveAwayDolls"(Focus),and"TheBookofClarence"(Sony)couldhavecontributed
$100millionormorewhileridingtheawardswave.Thebiggestunknownishowmuchthe
lackofpromotionhurtthefilmsreleasedduringthestrikes.BythetimetheSAG-AFTRA
strikebeganJuly11,mostofthesummer’stoptitleshadalreadybeenreleasedorwere
abouttobe,whichmeanttheirpromotionalpusheswereallbutcomplete.
(Doc Chunk 2; corresponding chunk) However, any hopes that 2024 might return 2024 box-office hopes U
to 2019 box-office parity are dashed. Grosses from rescheduled titles will help, but dashedbyproductionde-
productiondelayswillleavesubstantialgaps.Evenso:Itcouldhavebeenworse."Dune: laysandmajortitlepost-
Part2"(WarnerBros.) and"KraventheHunter"and"Ghostbusters: FrozenEmpire" ponements,costingpoten-
(Sony)willcostthisyear’stotalthemost–perhaps$400million("Ghostbusters"would tially$500million.
havehadonly12daysof2023play).Figureotherfilms,mostlylimited/specializedentries
likeLucaGuadagnino’s"Challengers"andJeffNichols"TheBikeriders"(Disney),Ethan
Coen’s "Drive Away Dolls" (Focus), and "The Book of Clarence" (Sony) could have
contributed$100millionormorewhileridingtheawardswave.Thebiggestunknownis
howmuchthelackofpromotionhurtthefilmsreleasedduringthestrikes. Bythetime
theSAG-AFTRAstrikebeganJuly11,mostofthesummer’stoptitleshadalreadybeen
releasedorwereabouttobe,whichmeanttheirpromotionalpusheswereallbutcomplete.
(DocChunk2;correspondingchunk)However,anyhopesthat2024mightreturnto 2024 box-office hopes U
2019box-officeparityaredashed.Grossesfromrescheduledtitleswillhelp,butproduction dashedbyproductionde-
delayswillleavesubstantialgaps. Evenso: Itcouldhavebeenworse. "Dune: Part2" laysandmajortitlepost-
(WarnerBros.)and"KraventheHunter"and"Ghostbusters:FrozenEmpire"(Sony)will ponements,costingpoten-
costthisyear’stotalthemost–perhaps$400million("Ghostbusters"wouldhavehadonly tially$500million.
12daysof2023play). Figureotherfilms,mostlylimited/specializedentrieslikeLuca
Guadagnino’s"Challengers"andJeffNichols"TheBikeriders"(Disney),EthanCoen’s
"DriveAwayDolls"(Focus),and"TheBookofClarence"(Sony)couldhavecontributed
$100millionormorewhileridingtheawardswave. Thebiggestunknownishowmuch
thelackofpromotionhurtthefilmsreleasedduringthestrikes. BythetimetheSAG-
AFTRAstrikebeganJuly11,mostofthesummer’stoptitleshadalreadybeenreleasedor
wereabouttobe,whichmeanttheirpromotionalpusheswereallbutcomplete.
(Doc Chunk 3) Some, like DC Comics’ "Blue Beetle" (WB) and "The Equalizer 3" 2024 box-office hopes U
(Sony),mayhavesufferedmore.Wouldpromotionforallstrike-periodreleaseshavetotal dashedbyproductionde-
$100million? Maybe. Theaterswerefortunatethatboth"Barbie"and"Oppenheimer" laysandmajortitlepost-
alreadyhadenormouspublicitybeforeactorsstruck,andbothhadmajor,Oscar-nominated ponements,costingpoten-
directorstocarrytheball.Their$950millioncombineddomesticgrossmorethandoubled tially$500million.
expectations.Addthemid-summersleepersuccessof"SoundofFreedom"andJulyand
Augustbothwerestrongmonths.Awildcard,unknownwhenthestrikebegan,wasTaylor
Swift’sconcertfilm.ItcertainlyfilledanOctobervoidthatexistedbeforethestrikeandits
SAG-AFTRAwavermeantshecouldpromoteit.Theoutside-studiosuccessof"Soundof
Freedom"and"TheErasTour"werenotonlywelcomefortheirgrosses,butalsobecause
theyshowit’spossibletofindreleasesoutsidethestudios.
Table15: ExamplesusingtheD2Cmethod. Thefulldocumentisfromthiswebsite. Thesameclaim(summary
sentence)canbesupportedbybothitsdirectlyassociateddocumentchunk(chunk2)andaseparatechunk(chunk
1)originatingfromthesamedocument,whichhasbeendividedintothreedistinctchunks. Somesentencesare
removedfromthechunktomaketheclaimunsupported. Unsupportingfactsarehighlighted.Segmentthefollowingsentenceintoindividualfacts:
Sentence:OthertitlechangesincludedLordStevenRegalandTheNastyBoyswinningtheWorldTelevisionChampionshipand
theWorldTagTeamChampionshiprespectively.
Facts:
-LordStevenRegalwantheWorldTelevisionChampionship.
-TheNastyBoyswanandtheWorldTagTeamChampionship.
Sentence: Theparkwaywasopenedin2001afterjustunderayearofconstructionandalmosttwodecadesofcommunity
requests.
Facts:
-Theparkwaywasopenedin2001.
-Theparkwaywasopenedafterjustunderayearofconstruction.
-Theparkwaywasopenedaftertwodecadesofcommunityrequests.
Sentence:TouringbeganinEuropeinApril-JunewithguitaristPaulGilbertastheopeningact,followedbyAustraliaandNew
ZealandinJuly,MexicoandSouthAmericainlateJuly-August,andconcludinginNorthAmericainOctober-November.
Facts:
-TouringbeganinEuropeinApril-June.
-TheopeningactofthetourwasguitaristPaulGilbert.
-ThetourwasinAustraliaandNewZealandinJuly.
-ThetourwasinMexicoandSouthAmericainlateJuly-August.
-ThetourwasconcludedinNorthAmericainOctober-November.
Sentence: InMarch 2018, the company partnered With Amazon WebServices (AWS) to offer Al-enabled conversational
solutionstocustomersinIndia.
Facts:
-ThecompanypartneredwithAmazonWebServices(AWS)inMarch2018.
-ThetwocompaniespartneredtoofferAl-enabledconversationalsolutionstocustomersinIndia.
Sentence:ThemostsignificantoftheseisinGermany,whichnowhasaYazidicommunityofmorethan200,000livingprimarily
inHannover,Bielefeld,Celle,Bremen,BadOeynhausen,PforzheimandOldenburg.
Facts:
-ThemostsignificantoftheseisinGermany.
-GermanynowhasaYazidicommunityofmorethan200,000.
- Yazidi community in Germany lives primarily in Hannover, Bielefeld, Celle, Bremen, Bad Oeynhausen, Pforzheim and
Oldenburg.
Sentence:Aprevioussix-timewinneroftheNations’Cup,SebastianVettelbecameChampionofChampionsforthefirsttime,
defeatingTomKristensen,whomadethefinalforthefourthtime,2-0.
Facts:
-SebastianVettelisaprevioussix-timewinneroftheNations’Cup.
-SebastianVettelbecameChampionofChampionsforthefirsttime,defeatingTomKristensen,2-0.
-TomKristensenmadethefinalforthefourthtime.
Sentence:[SENTENCE]
Facts:
Table16: Sentencedecompositionpromptadaptedfrom (Kamoietal.,2023).Yourtaskistogenerateapairofsentencessothattheprovidedclaimcanbeentailedbythesentencepair.Youmustmakesure
thattheclaimcanonlybedeductedbycombiningtheinformationfromthetwosentencesthatcontainuniqueinformation.
Examples:
ProvidedClaim:Theinvestigationisintoallegationsthathismayoralcampaignreceivedillegalforeignfunds.
Sentence1: Duringtheperiodleadinguptothemayoralelection,therewasanotableincreaseinhiscampaign’sfinancial
resources.
Sentence2:Investigationshowsthefundshavingoriginsbeyondnationalboundaries,adetailraisingquestionsundercurrent
campaignlaws.
ProvidedClaim:Approximately1,000fansfaintedattheconcert.
Sentence1:Emergencyservicesreportedanunusuallyhighnumberofcallsformedicalassistanceduringtheconcertwithan
attendanceof20,000.
Sentence2:Venueofficialsestimatedthatapproximately5%oftheaudiencerequiredmedicalattentionforfainting.
ProvidedClaim:Theinterestratehikeswereintendedtomanageinflationandmoderateeconomicgrowth.
Sentence1:Centralbankofficialsexpressedconcernovertherisingconsumerpriceindexandtheoverheatingoftheeconomy.
Sentence2:Themonetarypolicycommitteedecidedtoadjusttheinterestratesasaresponsetotheseeconomicindicators.
ProvidedClaim:SeveraladvertisersareconsideringhaltingtheiradsonsocialmediaplatformX.
Sentence1:Somecompaniesarere-evaluatingtheirmarketingstrategiestoavoidassociationwithplatformsthatfailtoaddress
misinformation.
Sentence2:RecentreportsshowthatplatformXhasreceivedcriticismforitshandlingoffalseinformationspreadingunchecked.
PleasemakesurethatNEITHERsentencealonesupportstheclaim.
Yourturn:
ProvidedClaim:[CLAIM]
Table17: PromptforStep2: AtomicfactexpansionfortheC2Dmethod(Section3.1).
Wearecreatinganewsarticle(oneparagraph)inthestyleofTheNewYorkTime.Wewillgiveyoualistoffactstousewhen
writingyourarticle.Youmustincludeallthefactsinthelist.Neverstatededucedfactsorconclusions.Thearticleshouldstick
tothefactlistprettyclosely.Includeasmanysentencesasneededtowriteeachfactfromthelistoffacts.
Factsyoumustinclude:
-{FACT1}
-{FACT2}
...
Table18: PromptforStep3: DocumentgenerationfortheC2Dmethod(Section3.1).
Source:[SOURCE]
Claim:[CLAIM]
Istheclaimfullyentailed,orimplied,bythesource?Pleaseanswerwith"yes"or"no".
Table19: Promptforallstepsinourmethodsthatrequireentailmentcheck.
Document:
[DOCUMENT]
Pleasegenerateasummaryforthedocumentwiththefollowingrequirements:
1.Thesummaryshouldbeafluentandgrammaticalsentence.
2.Thesummaryshouldbenomorethan15words.
3.Thesummaryshouldcoverinformationacrossthedocument.
Summary:
Table20: SummarizationpromptforStep1: Chunk-levelsummarizationfortheD2Cmethod(Section3.2)and
D2C-SIMPmethod.Mergethefollowingindividualfactsintoasinglesentence:
Facts:
-LordStevenRegalwantheWorldTelevisionChampionship.
-TheNastyBoyswanandtheWorldTagTeamChampionship.
Sentence:OthertitlechangesincludedLordStevenRegalandTheNastyBoyswinningtheWorldTelevisionChampionshipand
theWorldTagTeamChampionshiprespectively.
Facts:
-Theparkwaywasopenedin2001.
-Theparkwaywasopenedafterjustunderayearofconstruction.
-Theparkwaywasopenedaftertwodecadesofcommunityrequests.
Sentence: Theparkwaywasopenedin2001afterjustunderayearofconstructionandalmosttwodecadesofcommunity
requests.
Facts:
-TouringbeganinEuropeinApril-June.
-TheopeningactwasguitaristPaulGilbert.
-TherewasatourinAustraliainJuly.
-TherewasatourinNewZealandinJuly.
-TherewasatourinMexicoinlateJuly-August.
-TherewasatourinSouthAmericainlateJuly-August
-ThetourwasconcludedinNorthAmericainOctober-November.
Sentence:TouringbeganinEuropeinApril-JunewithguitaristPaulGilbertastheopeningact,followedbyAustraliaandNew
ZealandinJuly,MexicoandSouthAmericainlateJuly-August,andconcludinginNorthAmericainOctober-November.
Facts:
-ThecompanypartneredwithAmazonWebServices(AWS)inMarch2018.
-ThetwocompaniespartneredtoofferAl-enabledconversationalsolutionstocustomersinIndia.
Sentence: InMarch 2018, the company partnered With Amazon WebServices (AWS) to offer Al-enabled conversational
solutionstocustomersinIndia.
Facts:
-ThemostsignificantoftheseisinGermany.
-GermanynowhasaYazidicommunityofmorethan200,000.
-YazidicommunityinGermanylivesprimarilyinHannover.
-YazidicommunityinGermanylivesprimarilyinBielefeld.
-YazidicommunityinGermanylivesprimarilyinCelle.
-YazidicommunityinGermanylivesprimarilyinBremen.
-YazidicommunityinGermanylivesprimarilyinBadOeynhausen.
-YazidicommunityinGermanylivesprimarilyinPforzheim.
-YazidicommunityinGermanylivesprimarilyinOldenburg.
Sentence:ThemostsignificantoftheseisinGermany,whichnowhasaYazidicommunityofmorethan200,000livingprimarily
inHannover,Bielefeld,Celle,Bremen,BadOeynhausen,PforzheimandOldenburg.
Facts:
-SebastianVettelisaprevioussix-timewinneroftheNations’Cup.
-SebastianVettelbecameChampionofChampionsforthefirsttime.
-SebastianVetteldefeatedTomKristensen.
-TomKristensenmadethefinalforthefourthtime.
-Thescorewas2-0.
Sentence:Aprevioussix-timewinneroftheNations’Cup,SebastianVettelbecameChampionofChampionsforthefirsttime,
defeatingTomKristensen,whomadethefinalforthefourthtime,2-0.
Facts:
-{FACT1}
-{FACT2}
...
Sentence:
Table21:MergingpromptforStep5:PairingsubclaimsandgenerateddocumentsfortheC2Dmethod(Section3.1)
andStep2: ClaimdecompositionandsubclaimaugmentationfortheD2Cmethod(Section3.2).Determinewhethertheprovidedclaimisconsistentwiththecorrespondingdocument.Consistencyinthiscontextimpliesthat
allinformationpresentedintheclaimissubstantiatedbythedocument.Ifnot,itshouldbeconsideredinconsistent.
Document:[DOCUMENT]
Claim:[CLAIM]
Pleaseassesstheclaim’sconsistencywiththedocumentbyrespondingwitheither"yes"or"no".
Answer:
Table22: Zero-shotfactualconsistencyevaluationpromptforallLLMs.
Youareproviedwithacontextandaclaim.Pleasefirstdetermineiftheclaimcanstandalonewhitouttheconext.Ifnot,provide
adecontextualziedversionoftheclaimthatincorporatesnecessaryinformationfromthecontexttomakeitself-contained.
The revision should be as minimum as possible. Please respond with a JSON format: {"label": "yes"/"no", "decontext":
"NA"/decontextualizedclaim}.
Example1:
Context: Therearemanyreasonswhypoetryisimportantforchildren. Poetrycanhelpchildrenbuildconfidencethrough
memorizingandrecitingpoems.Itcanalsoprovideaneasywayforchildrentorememberalessonorvalue.
Claim:Itcanalsoprovideaneasywayforchildrentorememberalessonorvalue.
Answer:{"label":"no","decontext":"Poetrycanprovideaneasywayforchildrentorememberalessonorvalue."}
Example2:
Context:Yes,ancientsocietieshadconceptsofrights.Theconceptofrightsfirstappearedinthetheoryofnaturallawwhich
existedinthestateofnature.Inthisstate,peopleenjoyedcertainrightssanctionedbynaturallaw.
Claim:Inthisstate,peopleenjoyedcertainrightssanctionedbynaturallaw.
Answer:{"label":"no","decontext":"Inthestateofnature,peopleenjoyedcertainrightssanctionedbynaturallaw"}
Example3:
Context:TheancientGreekshadsomeconceptofhumanrights,althoughthereisnosinglewordinclassicalGreekthatcaptures
thesenseof"rights"asitisusedinmodernpoliticalthought.However,Greekcustomsandinstitutionsprovidedprotectionto
privatepropertyuniqueintheancientworld,instillingastrongsenseofequality.Theideaofhumanrightsspreadquicklyfrom
BabylontoGreeceandeventuallyRome,wheretheconceptof"naturallaw"arose.
Claim:TheideaofhumanrightsspreadquicklyfromBabylontoGreeceandeventuallyRome,wheretheconceptof"natural
law"arose.
Answer:{"label":"yes","decontext":"NA"}
YourTurn:
Context:[CONTEXT]
Claim:[CLAIM]
Answer:
Table23: DecontextualizationpromptforGPT-4.
Wearecreatinganewsarticle(oneparagraph)inthestyleofTheNewYorkTimes.Wewillgiveyouaclaimthatmustbecovered
whenwritingyourarticle.Allinformationintheclaimmustbesupportedbyweavingtogethervariouspiecesofevidencewithin
thetext.Thatis,theclaimshouldnotbedirectlysupportedbyusingonesentencefromthearticle.Thegeneratedarticleshould
bearound140words.
Claim:[CLAIM]
Article:
Table24: SupportingdocumentgenerationpromptforthesimplifieddatagenerationmethodC2D-SIMP.Youarepresentedwithaclaimandanarticlethatfullysupporttheclaim.Youtaskistominimallymodifythearticlewiththe
followingrequirements:
1.Themodifiedarticlenolongerfullysupportstheclaim.Some(butnotall)statementsintheclaimshouldbesupportedbythe
modifiedarticle.
2.Theeditedarticlelooksclosetotheoriginalclaim.
3.Theeditedclaimarticleshouldhavethesimilarlengthwiththeoriginalarticle.
Thefollowingsarethetypeofrevisionsyoucanusetorevisethearticle:
-Entityrevision:Anentity(likeaperson,place,organization,etc.)fromaclaimisbeingeditedornotmentionedintherevised
article.
-Numberrevision:Anumberfromaclaimisbeingeditedornotmentionedintherevisedarticle.
-Attributerevision:Asyntaxunit(eitheraword,phraseorclause)thatmodifiesanounisbeingeditedornotmentionedinthe
revisedarticle.
-Predicaterevision:Amaincontentverborcontentlikeadverbsthatcloselyrelatetotheverbisbeingeditedornotmentioned
intherevisedarticle.
Claim:[CLAIM]
Article:[ARTICLE]
PleaserespondinaJSONformat:{“revision_type”:...,“revised_article”:...}.
Table25: NonsupportingdocumentgenerationpromptforthesimplifieddatagenerationmethodC2D-SIMP.
Document:
[DOCUMENT]
ConsistentSummary:
[CONSISTENT_SUMMARY]
Given the document and consistent summary above, generate 10 slightly modified versions of the summary such that the
modificationsintroduceafactualinconsistency.Forexample,youcanmodifyanumber,date,orentity,andnegateormodifya
statement.Herearesomerulestofollow:
-Eachmodificationshouldchangeatmost3-4wordsfromtheoriginalsummary,andkeeptherestthesame.
-Eachmodificationshouldchangeadifferentpartoftheoriginalsummary.
-Yourmodificationsshouldbechallengingtodetect:modifyminimallywhilestillintroducingafactualinconsistency.
-Thefactualinconsistencyyouintroduceshouldbesubtle.Forexample,ifyoureplaceanentity,makesureyoureplaceitwith
anotherentityfromthedocument.
-Eachmodificationshouldstartwith“[FIRST_THREE_WORDS][...]”,andendwith“[LAST_THREE_WORDS]”
PleaserespondinaJSONformatwiththefollowingstructure:
{“inconsistent_summaries”:[“Firstinconsistentsummary”,“Secondinconsistentsummary”,...]}
Table26: NonsupportingdocumentgenerationpromptforthesimplifieddatagenerationmethodD2C-SIMP. The
promptisadaptedfromSummEdit(Labanetal.,2023).
Determinewhethereachoftheprovidedclaimsareconsistentwiththecorrespondingdocument.Consistencyinthiscontext
impliesthatallinformationpresentedinaclaimissubstantiatedbythedocument.Ifnot,itshouldbeconsideredinconsistent.
Document:[DOCUMENT]
Claims:[CLAIM]
Claimsaredisplayedwithsentenceindices.Pleaseevaluateeachclaim’sconsistencywiththedocumentbyrespondingwith
either“yes”or“no”intheJSONformat:{“[1]”:...,“[2]”:...,...}.
Answer:
Table27: Promptforpredictingthefactualitylabelsofallclaimsinaresponseforaprovideddocument. Thisis
usedmainlyfortextsummarizationwheremultiplesummarysentencessharethesamedocument.