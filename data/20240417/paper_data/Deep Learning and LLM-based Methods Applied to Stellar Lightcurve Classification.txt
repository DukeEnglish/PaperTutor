Deep Learning and LLM-based Methods Applied to Stellar
Lightcurve Classification
Yu-Yang Li1,2,3, Yu Bai1,2,3, Cunshi Wang1,2,3*, Mengwei Qu7,8, Ziteng Lu9, Roberto
Soria2,4,5, and Jifeng Liu1,2,3,6
1Key Laboratory of Optical Astronomy, National Astronomical Observatories,
Chinese Academy of Sciences, 20A Datun Road, Chaoyang District, Beijing 100101,
People’s Republic of China
2College of Astronomy and Space Sciences, University of Chinese Academy of
Sciences, Beijing 100049, China
3Institute for Frontiers in Astronomy and Astrophysics, Beijing Normal University,
Beijing 102206, China
4INAF—Osservatorio Astrofisico di Torino, Strada Osservatorio 20, I-10025 Pino
Torinese, Italy
5Sydney Institute for Astronomy, School of Physics A28, The University of Sydney,
Sydney, NSW 2006, Australia
6New Cornerstone Science Laboratory, National Astronomical Observatories,
Chinese Academy of Sciences, Beijing 100101, People’s Republic of China
7State Key Laboratory of Isotope Geochemistry, Guangzhou Institute of
Geochemistry, Chinese Academy of Sciences, Guangzhou 510640,China
8College of Earth and Planetary Sciences, University of Chinese Academy of
Sciences, Beijing 100049, China
9School of Foreign Studies, TonglingUniversity, Tongling, Anhui, 244061, People’s
Republic of China
*Address correspondence to: wangcunshi@nao.cas.cn
Abstract
Light curves serve as a valuable source of information on stellar formation and evolution.
Withtherapidadvancementofmachinelearningtechniques,itcanbeeffectivelyprocessedtoex-
tractastronomicalpatternsandinformation. Inthisstudy,wepresentacomprehensiveevalua-
tionofdeep-learningandlargelanguagemodel(LLM)basedmodelsfortheautomaticclassifica-
tionofvariablestarlightcurves,basedonlargedatasetsfromtheKeplerandK2missions. Spe-
1
4202
rpA
61
]MI.hp-ortsa[
1v75701.4042:viXracial emphasis is placed on Cepheids, RR Lyrae, and eclipsing binaries, examining the influence
of observational cadence and phase distribution on classification precision. Employing AutoDL
optimization, we achieve striking performance with the 1D-Convolution+BiLSTM architecture
and the Swin Transformer, hitting accuracies of 94% and 99% correspondingly, with the latter
demonstrating a notable 83% accuracy in discerning the elusive Type II Cepheids—comprising
merely 0.02% of the total dataset. We unveil StarWhisper LightCurve (LC), an innovative
Series comprising three LLM-based models: LLM, multimodal large language model (MLLM),
and Large Audio Language Model (LALM). Each model is fine-tuned with strategic prompt
engineering and customized training methods to explore the emergent abilities of these mod-
els for astronomical data. Remarkably, StarWhisper LC Series exhibit high accuracies around
90%,significantlyreducingtheneedforexplicitfeatureengineering,therebypavingthewayfor
streamlined parallel data processing and the progression of multifaceted multimodal models in
astronomicalapplications. Thestudyfurnishestwodetailedcatalogsillustratingtheimpactsof
phase and sampling intervals on deep learning classification accuracy, showing that a substan-
tial decrease of up to 14% in observation duration and 21% in sampling points can be realized
without compromising accuracy by more than 10%.
1 Introduction
The phenomenon of light variation is a crucial aspect of astrophysics and has long been studied
in the field of time-domain astronomy. Cepheid variables, a special kind of variable star, serve a
critical role as a universal standard candle, enabling us to measure the distance to clusters and
galaxies with their period-luminosity relation. Other types of variable stars, including RR Lyrae,
δ Sct, and γ Dor stars, are characterized by their unique absolute magnitudes, pulsation patterns,
each exhibiting diverse period-luminosity relations. The classification of these pulsating stars has
significantly enriched our understanding of their formation and evolution. Moreover, it has shed
light on the structure and dynamics of binary, n-body systems and our Galaxy. [1–8]
Long periods of observations are crucial in order to understand the nature of variable stars. To
carry out these observations, it is important to make wise use of telescope time that is a valuable
resource [9]. One such strategy involves the use of simulations to create plans that optimally allo-
cate available telescope time, considering overheads and other factors. Another strategy employs
autonomous agents to optimize the observation of time-varying phenomena [10]. In this study, we
aimtounderstandtheimportancetiedtodifferentphasesofvariablesandtheclassificationaccuracy
provided by varying cadences. This can help us to establish robust guidelines for future star gazing
observations [11].
Asubstantialamountoflabeleddataisrequired,inordertounderstandthesignificanceofphase
and cadence for different variable stars. In recent years, astronomy has entered the era of big data.
For example, the Zwicky Transient Factory (ZTF) [12] at Palomar Observatory scans the sky every
twodayswitha1.2mtelescope,generating 1TBofrawdataeachnight[13]. TheupcomingSiTian
survey [14], aims to monitor over 1,000 deg2 of sky every 30 minutes using fifty 1m class Schmidt
telescopes, and is expected to produce around 140TB of processed data each night.
As the volume of data grows, there’s a increasing requirement for efficient and automated in-
2terpretation and analysis methods. Deep learning, a component of machine learning, has risen as
a powerful tool for image and signal processing [15]. It can learn and extract features from data
[15, 16], making it an ideal solution for managing large, complex datasets. Specifically, recurrent
neuralnetworks(RNNs)haveshowngreateffectivenessinprocessingtimeseriesdata[17],whilecon-
volutional neural networks (CNNs) have demonstrated their superiority in image processing tasks
[16, 18]. Moreover, the transformer architecture, with its attention mechanism [19], has shown re-
markable performance across various applications [20–23]. RNNs originally developed by [24], led
to Simple RNNs, which were later enhanced by LSTMs [25] to address long-term dependencies.
These were further simplified by GRUs [26] for computational efficiency. CNNs initially introduced
for hand-written digit recognition [27], and have evolved with architectures like AlexNet, VGGNet,
and ResNet. Recent advancements include EfficientNet [28, 29], which optimizes depth, width, and
resolution for model accuracy and efficiency.
In recent years, Transformer architecture has shown remarkable performance across a diverse
range of applications.Initially introduced to address sequential data processing tasks [19]. Trans-
former use self-attention mechanisms to model relationships with sequences, enabling them to cap-
ture long-range dependencies and relationships between sequence elements. The success of the
Transformer in natural language processing has sparked interest in their potential application in
otherdomains,suchascomputervisionandspeechrecognition. AnexampleofavisionTransformer
is the Swin Transformer [30, 31], which has optimized the computation of attention mechanism and
shown promising results in various benchmark datasets.
Beyond the traditional reliance on extensive data for transfer learning, it is imperative to inves-
tigate the potential of LLM that incorporate both substantial datasets and large parameter counts.
StarWhisper 1 is a LLM for astronomy, which has strong astronomical ability and instruction fol-
lowing ability, and can complete a series of functions such as knowledge question answering, calling
multi-modal tools, and docking telescope control systems. The StarWhisper LC Series is initiated
with the purpose of leveraging the experience gained from the prior training of the StarWhisper
language model. It seeks to explore and discuss the potential harnessed from vast data to engen-
der emergent properties in the analysis of light curve data. LLMs, such as the Gemini 7B model,
have shown promise in adapting to new data types through fine-tuning with specific prompt tem-
plates [32]. MLLMs, like the deepseek-vl-7b-chat, are adept at handling tasks involving image
classification due to their extensive training on datasets containing chart data [33]. LALMs, such
as Qwen-audio,trained on audio datasets, exhibits exceptional performance in audio classification.
[34].
Inthisstudy,weperformacomprehensiveevaluationofdeep-learningandLLM-basedmodelsfor
the classification of variable star light curves, using the data from Kepler and K2 observations. The
typesofvariablestarsandthedatapre-processingarepresentedinSection2. InSection3,weutilize
various classification models, including the LSTM, the GRU, the Transformer, the LightGBM, the
EfficientNet, the Swin Transformer and StarWhisper LC serires. Training methods are introduced
inSection4. Wealsopresentmodelperformanceandcatalogsofthephaseimportanceandsampling
intervals in Section 5. A discussion is provided in Section 6.
1https://github.com/Yu-Yang-Li/StarWhisper
32 Data
2.1 Kepler and K2
The Kepler spacecraft was launched in 2009 aiming to discover for Earth-like planets [35]. It was
equippedwithanopticaltelescopea95cmapertureanda115.6◦fieldofview. Theadvancedtechnol-
ogyallowedKeplertopreciselytracklightcurvesfrom200,000differenttargets. Thishighprecision
resulted in the discovery of over 2,000 planets. For stars with V band magnitude between 13mag
to 14mag, the precision was 100 ppm (parts per million), while for stars with V band magnitude
between 9mag to 10mag, the precision was 10 ppm. Unfortunately, after four years, half of Kepler’s
fourreactionwheelsfailed,leadingtotheendofitsprimarymission. Yet,thismarkedthebeginning
of the K2 mission. The K2 mission used the transit method to detect changes in light along the
ecliptic plane and created catalogs with photometric precision closely matching that of the original
Kepler mission. The observations of K2 were controlled using the remaining reaction wheels and
thrusters, with each campaign limited to 80 days.
There are two types of light curve available, named Presearch Data Conditioning (PDC) and
Simple Aperture Photometry (SAP) light curves [36]. While SAP light curves retain long-term
trends, PDC light curves were generated by the Kepler Operation Science Center, and are free from
systematic errors. Therefore, we adopt PDC light curves for our analysis. Additionally, we consider
two types of data with different time resolutions: long-cadence data, with a 30-minute sampling
interval, and short-cadence data, with a one-minute sampling interval. Due to the limited number
of short-cadence data, we focus on the long-cadence light curves. For each light curve, we remove
thequarter-to-quarterdifferencesandconvertthefluxintorelativeflux,similartothemethodsused
in [37, 38].
2.2 Variable Stars
Ourtrainingsamplesaresimilarto[11],includingeclipsingbinaries,RRLyrae,δ Scuti,γ Dor,andδ
Scuti/γ Dorhybrids. ThetypeIICepheidsareincluded,inordertomakeamoreuniversalsample.
Table 1 lists our training sample and corresponding references. The training samples are seriously
biased among different variable types. This bias often leads to decreased performance, especially
in situations few-shot or small sample scenarios [39, 40]. However, [41] suggest that certain models
may help overcome these challenges, although such cases seem to be the exception rather than the
rule. These biased training samples offer a unique opportunity to study the algorithm’s dependence
onsuchbiasesandtounderstandhowthisdependenceaffectstheoverallperformanceandaccuracy
when applied to astronomical data.
2.3 Pre-processing
A pre-processing method, as described by [11], was adopted to manage and clean the light curves,
with the aim of enhancing their features and expanding the training data. The lightcurves were
segmented into 10-day intervals. Any segments with gaps exceeding a day were removed, while
those with gaps less than one day were interpolated with a time sequence of 30 minute intervals.
4Table 1: Training Sample
Label Input Catalog Final Sample References
δ Sct 1389 111528 (1), (2), (3), (4), (14), (15)
EB 2908 226937 (6), (7), (8), (9), (10), (11), (12), (13)
γ Dor 941 65786 (14),(15)
HYB 1552 33751 (1), (2), (3), (4), (14), (15)
RR 482 9306 (5)
T2CEP 3 94 (1), (2), (3), (4), (5)
Total 7275 447402 (1)-(15)
NoteTheinputcatalogcolumnindicatesthesourcesofdatathatweobtainedfromseveral
catalogs. Thefinalsamplecolumnrepresentsthefinalsamplesizeafterapplyingallthe
pre-processing procedures. References: (1)[42], (2)[43], (3)[44], (4)[45], (5)[46], (6)[1],
(7)[2], (8)[3], (9)[4], (10)[5], (11)[6], (12)[7], (13)[47], (14)[48], (15)[49].
Table 2: Period and Observation Time Saving
Star Period (d) ∆ (%) t (%) ∆ (%) t (%)
phase phase sampling sampling
RR 251457011 0.509 1.06 41 8.63 50
RR 251457012 0.529 3.48 71 0 0
RR 251457013 0.542 2.47 31 1.83 50
RR 251457014 0.593 9.99 29 9.32 50
RR 251457015 0.574 0.53 55 7.82 50
RR 251457016 0.510 1.30 83 1.38 50
RR 251457020 0.478 0 0 0.76 50
RR 251457021 0.546 0.70 74 4.48 80
RR 251457022 0.508 1.56 49 0 0
RR 251457024 0.553 0 0 0 0
NoteThetableshowstheperiodofthestars(intheunitofdays)alongwiththevariationofaccuracy
and time saved (percentages), for both phase importance and sampling research.
Analternativeapproachtotime-seriesdataclassificationinvolvescategorizingofimagesthatare
createdfromgraphicallyrepresentedlightcurves. Thisisachievedusingtransferlearningtechniques.
As highlighted in their result, the continuous wavelet transform (CWT) method has demonstrated
superior results in imaging light curves.
The Morlet wavelet was chosen as the core function for the CWT, owing to its proven effective-
ness in analyzing signals that display shifts in amplitude. This analysis resulted in a collection of
images that provide valuable insights into the time-frequency characteristics of the signals. Impor-
tantly, these images facilitate the identification of patterns, trends, and anomalies, thus offering a
comprehensive understanding of the signals. Figure 1 shows some variable stars from the training
sample.
2.4 Lomb-Scargle Periodogram
WeemploytheLomb-Scarglealgorithmtoextractthemostsignificantperiodscompletelightcurves,
which are used to study the phase importance of periodic variables. The Lomb-Scargle algorithm,
a variant of the Discrete Fourier Transform (DFT) developed by [50] and [51], has been specifically
5Figure 1: CWT images of different objects.
designedforunevenlysampledtime-seriesdata. Ittransformsatimeseriesintoalinearcombination
of sinusoidal waveforms, simplifying the conversion from time to frequency domain. We calculate
the period by applying the Lightkurve Collaboration’s LSP method [52]. Some examples are shown
in Figure 2.
More specifically, we use a uniform sampling strategy in our set frequency domain to select an
array of frequency points for periodicity analysis. The periodogram calculation is carried out by
assessing the Power Spectral Density (PSD) at each frequency. This periodogram reveals the in-
tensity of periodic signals across the spectrum of frequencies. Significant peaks in the periodogram
indicate strong periodic signals. We determine the exact periods through the reciprocals of these
frequencies. Ourimplementationalsoincludesconsiderationsfornormalizationmethodsandcompu-
tationalstrategies. Weuseaspecificapproachtofrequencysamplingtoensurethattheperiodogram
computation is both precise and efficient. The results are shown in Table 2.
63 Model Construction
In order to classify variable stars from their light curves, we have explored various deep learning
models. The basic architectures of these models are shown in Figure 3.
3.1 CNN and RNN
WeexploreseveraladvancedLSTMarchitectures. ForConv1D+BiLSTM,itcombines1-dimensional
convolutional layers with bidirectional LSTM layers. The convolutional layer is used to extract fea-
tures from the data [53], while the bidirectional LSTM layer capture contextual information from
both the past and future time steps [54]. For Conv1D + BiLSTM + Attention, the addition of an
attention mechanism enables the model to focus on specific parts of the input data, improving per-
formance by giving higher weights to more relevant parts [19]. This can help improve the accuracy
ofclassificationandfacilitatefurtherresearchontheimportanceofdifferentphasesforclassification
[55].
GRUmodelsareavariationofLSTMthatarecomputationallymoreefficientastheyhavefewer
parameters, making them easier to train and less prone to overfitting [26, 56]. Specifically, we use
the Conv1D + BiGRU architecture.
In addition to the deep learning approaches, it is also important to consider the performance
of classic machine learning methods for comparison. One such method is LightGBM, a gradient-
boosting decision tree (GBDT) framework that uses tree-based learning algorithms [57].
3.2 Transformer
Transformer are highly effective in managing data dependencies, a critical aspect of time series
analysis. Thisresultsinfavorableoutcomes[58,59]. Theirmultipleattentionmechanismsareskilled
inidentifyingthemostrelevantpartsofinputdata[19]. Bycombininga1-dimensionalconvolutional
layer with a transformer encoder layer, we can effectively capture both global dependencies and
local interactions in the data. This approach addresses the transformer architecture’s limitation in
detectinglocal nuances.
WehaveadoptedtheSwinTransformerforfew-shotclassificationtasks(i.e. T2CEP).Transform-
ers excel in modeling long-range dependencies in computer vision tasks [60, 61] by focusing directly
on an image’s essential parts [19]. The Swin Transformer takes this a step further; it streamlines
computation by limiting attention within small windows while maintaining effectiveness [30, 31].
3.3 Efficientnet
For a more thorough comparative analysis, we have adopted an advanced CNN known as the pre-
trainedEfficientNet. Thisnetworkexceedsitspredecessorsbyintegratingoptimizationelementsfor
depth,width,andresolutionwithinitsframework. Incomparison,previousnetworksonlyoptimized
one or two of these elements [28, 29]. EfficientNet uses a diverse range of architectural techniques
includingdepth-wiseseparableconvolutions,squeeze-and-excitationblocks,anddynamicimagescal-
7Table 3: Accuracy and Macro F1-Score
Input Format Model Accuracy(All/T2CEP) Macro F1-Score
Time-Series Conv1D+Transformer 85%/17% 0.7
LightGBM 87%/25% 0.71
BiLSTM+Attention 93% 0.76
Conv1D+BiGRU 93% 0.76
Conv1D+BiLSTM 94% 0.77
CWT Image Efficientnet 99% 0.82
Swin Transformer 99%/83% 0.96
Textual Time-Series LLM-based model 89% 0.80
Lightcurve Image MLLM-based model 95% 0.94
Transformed Audio LALM-based model 93% 0.71
NoteThetablepresentstheaccuracyandmacroF1-scoreforvariousmodels,includingtheaccuracy
of some models specifically on T2CEP.
ing. These techniques are validated to boost performance while reducing computational resource
requirements.
3.4 StarWhisper LC Series
3.4.1 LLM
LLMs,owingtotheiremergentcapabilities,canlearnnewlanguages(broadlydefined)throughfew-
shotlearningorfine-tuningprocesses, likelean3[62]. TheGemini7Bmodel2, pre-trainedonavast
corpusoftext,exhibitsemergentbehavior,enablingittoperformtasksnotexplicitlycoveredduring
its initial training. Therefore, we are contemplating the integration of specific prompt template as
shown in 4, to transform time series data into a form of language in a broader sense, in order to
leverage these advanced models more effectively.
3.4.2 MLLM
Next, we introduce a MLLM trained on the deepseek-vl-7b-chat [63]. MLLMs expand upon the
capabilitiesofLLMsbyincorporatingtheabilitytoprocessandinterpretvisualinformationalongside
text. The deepseek-vl-7b-chat model, with its extensive training on chart data, is particularly well-
suited for tasks involving time series image classification, making it a valuable addition to the
StarWhisper LC series.
3.4.3 LALM
Thefinalmodelintheseriesisbasedonaudioprocessing,utilizingtheQwen-audio[64]. Thismodel
has been specifically enhanced for audio classification tasks, offering a novel approach to time series
analysis by converting time series data into audio signals. A sampling frequency of 500Hz is used to
transform the normalized time-series data into audio signals, which possess distinct characteristics
that aid in classification.
2storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf
8Table 4: Prompt and Output Template
Instruction
Given the normalized flux data at 0.02 day intervals (delimited by the triple backticks), classify the
TYPE of light curve. Return the answer in an Assignment Statement, containing ONE variable:
TYPE. Only return the classification, not the Python code.
‘‘‘{flux}‘‘‘
Output
TYPE = ’Label’
For other LLM-based models, replace “Python code” with “description,” and “flux” with “image path or
audio path.”
4 Training
4.1 Auto deep learning methods
Auto deep learning methods are a set of techniques that aim to automate the process of designing,
training, and evaluating neural network models. These methods typically use techniques such as
Bayesian optimization [65], reinforcement learning [66], or evolutionary algorithms [67] to optimize
the model architecture and hyperparameters for the best possible performance.
Bayesian optimization is a powerful approach for hyperparameter tuning and exploration[68]. It
utilizesGaussianProcessesRegression(GPR)orTree-structuredParzenEstimators(TPE)tomodel
theprobabilitydistributionofhistoricaldataandemploystheSequentialModel-BasedOptimization
(SMBO) framework for iterative hyperparameter selection.
The SMBO process is an iterative method for hyperparameter optimization that consists of four
main steps [69]. First, a probability distribution model is built based on the existing tuning history.
Next, an acquisition function, such as Expected Improvement (EI), is employed to select the next
hyperparameter. Followingthis,thenewobservationisincorporatedintotheexistingtuninghistory.
The process is then repeated until the predefined maximum number of iterations is reached.
In this study, we used Auto deep learning to automate the process of training and evaluating
deep learning models. Specifically, 80% of the data was allocated for training and the remaining
20% was used as a validation set. Bayesian optimization allow us to efficiently find the best set of
hyperparameters[70].
4.2 LLM-based models training
For LLM, the light curve data was prepared to fit within the model’s context length limitations and
underwent a normalization process. The fine-tuning process involves all layers of the model using
the Qlora technique [71] a highly efficient fine-tuning method that achieves near-full fine-tuning
performance with significantly reduced memory requirements.
The MLLM’s training involved a comprehensive fine-tuning process that adjusted both the tex-
tualandvisualprocessingcomponentsofthemodel. Thisapproachisnecessarytoeffectivelyhandle
the multimodal nature of the task, leveraging the model’s pre-trained knowledge of visual patterns
9for time series image classification.
For the LALM, the success of the training phase hinge on the conversion of time series data into
audio signals. This innovative approach allow the model to classify light curves transformed into
audio signals, demonstrating the potential of audio-based analysis in scientific research.
Figure 2: The images of different variable types processed using the Lomb-Scargle method, with a
selected frequency range from 0 to Nyquist frequency. The final frequency is also indicated in each
figure.
5 Result
5.1 Performance
Theconfusionmatrices(Appendix)serveasthebasisforcomputingnumerousperformance. F −score,
1
abalanceindicatorderivedfromprecisionandrecall,iscalculatedastheirharmonicmean. Wehere
calculate the Macro−F −score [72] to evaluate the model’s overall performance, which averages the
1
F1 scores across all classes, making it a vital measure when working with imbalanced data.
Fortransferlearning,bothmodelsachievedanaccuracyof99%. TheSwinTransformerachieved
10Figure 3: Deep learning models structures for different input formats. It should be noted that not
all branch combination models are adopted due to varying degrees of effectiveness.
thehighestMacro−F −scoreof0.96,demonstratinghigheffectivenessevenwithasmallsamplesize.
1
Itachievedan83%accuracyrateonT2CEP,aclasscomprisingonly94outof447,402samples. For
the non-pretrained models, RNN-based models showed efficiency while maintaining high accuracy
levels. The Conv1D+BiLSTM model showed the best performance with an accuracy of 94%. It
achieved a Macro-F1 score of 0.77, comparable to that of the pre-trained Efficientnet. This proves
its ability to manage imbalanced data.
Transfer learning’s good performances are due to preprocessing via the CWT, a technique that
simplifies the process of initial feature extraction. Its efficiency is further supported by its prior
training on a numerous of images, enhancing its feature extraction capability. However, the HYB
categoryperformsshortrelativetoothers,whenevaluatingclassificationaccuracyforvariablestars.
This underperformance could be due to the intrinsic complexity of the HYB category, resulting in
an overlap of features and subsequently complicating the classification process [11, 73].
The StarWhisper LC series also yielded impressive results in the classification of light curves,
showcasing the robust capabilities of LLM-based models in scientific data analysis. The LLM,
despite the significant data reduction achieved by trimming the time-series data to input samples of
0.2d and normalizing the data’s precision to one part in a hundred thousand, attained an accuracy
rate of approximately 89%. This demonstrates the model’s ability to efficiently process and analyze
condensedtime-seriesdatawithoutsubstantiallossofinformation. TheMLLM,whichdidnotutilize
CWT remarkably achieved a 95% accuracy rate, underscoring its inherent strength in handling
multimodal data, including chart data akin to time series image classification. Furthermore, it
11exhibits excellent classification performance on small samples like T2CEP, with an F −score of
1
0.94, which further validates the sensitivity of image-based models to small sample sizes. The
LALM’sinnovativeapproachofconvertingtime-seriesdataintoaudiosignalsforclassificationledto
a commendable accuracy rate of 93%, highlighting the potential of audio-based analysis in scientific
research. These results collectively emphasize the effectiveness and versatility of the StarWhisper
LCseriesinleveragingtransferlearningandLLM-basedmodelforhigh-accuracyclassificationtasks
in the realm of astrophysics.
5.2 Catalogs
5.2.1 Phase Importance
Using the Conv1D + BiLSTM model, we assess the impact on accuracy by obscuring observation
points in a specific phase. We set the zero phase to correspond to the peak of the light curve.
The resulting decrease in the model’s confidence level for accurate classification helped us measure
the importance of each phase. Table 5 lists the importance with an interval of 0.1 phase. Figure
5, Figure 6 and Figure 7 reveal that the key features for GDOR, DSCT, and HYB, are primarily
located in the phase interval following the peak flux. For EB and RR shown in the Figure 8 and
Figure 9, the main concentration is detected in the phase interval during which the flux returns to
its peak. According to the heatmap distribution, significant or dark intervals predominantly occur
in the first half of the EB and RR. In contrast, DSCT and HYB show two distinct phase intervals
localized within them. We discuss the time saving associated with this and present a catalog in
Section 6.
5.2.2 Sampling
The observation schedule is significantly influenced by sampling, which itself is limited by the tele-
scope’s clear aperture, the CCD’s read-out speed, the scientific object of the survey. We calculated
the sampling-importance relations for the variables with a single prime frequency. We adjusted the
sampling rate by changing the number of points per period. The effect on classification accuracy is
shown in Figure 10.
We found that for EB, accuracy can remain above 75% even if sampling points are decreased to
one-tenth. ForRRandDSCTstars,halvingthesamplingratestillmaintainsanaccuracyofaround
80%. Thiscanbeexplainedbythephaseimportance: exceptforEB,thekeyfeaturesarelocatedin
onlyafewspecificphases. Itisprobablythatthereisasignificantdecreaseinthepredictionmodel’s
accuracy beyond a certain sampling rate. This is likely due to an increase in the sampling interval,
leading to the model’s failure to capture former phase features. The observed correlation between
sampling and phase importance further substantiates the reliability of the method we employed.
We also found that at a specific lower sampling interval, some variables exhibit a decline in
accuracy with increasing sampling. We suggest that within this sampling interval, the specific
instrumentalnoisemayhaveledtoadeclineinaccuracy,giventhataccuracygenerallyincreaseswith
higher sampling rates beyond this range. This suggests a critical sampling threshold where feature
12capture is optimized before noise becomes predominant. Table 6 list the accuracies corresponding
to different sampling rates.
6 Discussion
6.1 Observation time Saving
By correlating classification accuracy across variables with phase importance, we sequentially elim-
inate multiple phase intervals in ascending order of significance. This process help us estimate the
maximum observational time that can be saved for each star type. It is suggest that, on average, a
14% reduction in observation time can be achieved across different variables, provided the accuracy
variation remains within 10% over a given observation period. Specifically, RR and EB stars can
save an average of 44% and 29% of observation time, respectively.
Inaddition,welinkclassificationaccuracywiththenumberofsamplingpointswithineachperiod
to assess the potential decrease in observation time by reducing the sampling rate. Given that the
accuracy variation within a single observation period remains under 10%, our findings indicate an
averagepotentialreductionof21%inthenumberofsamplingpointsforobservations. Amongthese,
EB stars can benefit from an average reduction of 54% in the number of sampling points required
for observations. The results are presented in Table 2.
Our proposed RNN-based models show substantial performance without the need for image
preprocessing. Furthermore, using automated deep learning enables us to more efficiently identify
suitable hyperparameters. Additionally, we take into consideration the impact of masked points or
unobservedpointswithinthecompletephaseonclassificationaccuracy. ForEBstarswithsampling
pointsreducedtojust10%, andDSCTstarswithsamplinghalved, theaccuracyconsistentlyhovers
around 0.75. And RR stars maintain an accuracy of over 0.85 even when its sampling is reduced
by half. As a result, our method is apt for more efficient, and potentially real-time, astronomical
time-series recognition scenarios, such as exoplanets [55] and transients [74].
6.2 Learning form imbalanced data
Imbalancedsamplesareacommonoccurrenceinastronomicaldata. Inthisstudy,forthefirsttime,
we have applied transfer learning models that were previously trained on a significant volume of
image data to augment the recognition capabilities for these less prevalent variable stars. Notably,
the Swin Transformer and MLLM yield particularly fairly good results.
Additionally,wefindthattheself-attentionmechanismcansignificantlyenhancetherecognition
abilitytorecognizesmallsamples. Wewillfurtherconsiderapplyingdataaugmentation,resampling,
pre-training, and other methods in self-attention-based deep learning models, in order to improve
their performance on tasks involving imbalanced data, such as the TESS variable star.
136.3 Time serises as Language
Our research highlights the potential of leveraging the emergent capabilities of LLM-based models
fortheprocessingoflightcurves, ataskthatrequiresrapidconvergenceasshownin4, resistanceto
overfitting, and minimal susceptibility to data quality variations [14]. By integrating these models
with additional capability modules, such as visual and audio encoding, we aim to enhance their
performance and applicability in the domain of astronomy. This approach not only introduces
a novel analytical method for astronomical data but also explore the development of multimodal
models that can process a variety of astronomical inputs.
The optimization of LLMs for inferential tasks, coupled with their capacity for parallel and
rapid data processing [75], underscores their utility in handling the vast and complex datasets
encountered in astronomy field. There is a promising prospect of training specialized astronomical
encoding modules that build upon the robust foundation of LLMs. Such modules could be tailored
to interpret and analyze astronomical phenomena with a high degree of accuracy and efficiency.
Future, we will focus on refining the models by adjusting parameters such as data volume,
samplingpoints,precision,andtemporallength. Thiswillenableustodelvedeeperintothefeature
extraction thresholds of the models and further enhance their capabilities for astronomy-specific
tasks. Moreover,thepotentialforapplyingthismethodologytoothertimeseriestasksissignificant,
offering a versatile path for developing multi-task applications that operate on a multimodal basis.
Figure 4: Loss curves of LLM-based methods.
6.4 SiTian project
TheSiTianprototype,introducedin[14],isscheduledtoreleaseitsinternaldata. Thedatacontains
information from three bands, and we aim to construct its light curves using numerical simulations
and evaluate their classifications using deep learning techniques. Given the large volume of data,
striking a balance between prediction time and accuracy is essential.
Highly accurate models, particularly sensitive to smaller samples, can significantly enhance
SiTian’s capabilities in identifying variable stars. The catalogs we’ve developed related to phase
importance and sampling provide insights into optimal observation intervals and sampling rates
during monitoring. This allows for a calculated tradeoff between model accuracy and the time costs
associated with training and prediction, ensuring efficient and effective astronomical analyses.
14Acknowledgments
The research presented in this paper was generously funded by the National Programs on Key Re-
search and Development Project, with specific contributions from grant numbers 2019YFA0405504
and 2019YFA0405000. Additional support came from the National Natural Science Foundation of
China (NSFC) under grants NSFC-11988101, 11973054, and 11933004. We also received backing
fromtheStrategicPriorityProgramoftheChineseAcademyofSciences,grantedunderXDB41000000.
Special acknowledgment goes to the China Manned Space Project for their science research grant,
denoted by NO.CMS-CSST-2021-B07.
JFL extends gratitude for the support received from the New Cornerstone Science Foundation,
particularlyviatheNewCornerstoneInvestigatorProgram,andthehonoroftheXPLORERPRIZE.
This research incorporates data sourced from the Kepler mission, with its funding being at-
tributed to the NASA Science Mission Directorate. We sourced all data for this study from the
Mikulsk Archive for Space Telescopes (MAST). The operation of STScI is overseen by the Associ-
ation of Universities for Research in Astronomy, Inc., under the NASA contract NAS5-26555. The
MAST’ssupportfornon-HSTdatacomesthroughtheNASAOfficeofSpaceScience, notablygrant
NNX09AF08G, and various other grants and contracts.
Appendix
In Figures 11 through 17, we present the confusion matrices of different models. The main body of
each matrix, represented by varying shades of blue, illustrates the number of each crossed object in
the classification model.
References
1. PrˇsaA,BatalhaN,SlawsonRW,etal.KEPLERECLIPSINGBINARYSTARS.I.CATALOG
ANDPRINCIPALCHARACTERIZATIONOF1879ECLIPSINGBINARIESINTHEFIRST
DATA RELEASE. The Astronomical Journal 2011;141:83.
2. Slawson RW, Prˇsa A, Welsh WF, et al. KEPLER ECLIPSING BINARY STARS. II. 2165
ECLIPSING BINARIES IN THE SECOND DATA RELEASE. The Astronomical Journal
2011;142:160.
3. Matijeviˇc G, Prˇsa A, Orosz JA, Welsh WF, Bloemen S, and Barclay T. KEPLER ECLIPS-
ING BINARY STARS. III. CLASSIFICATION OF KEPLER ECLIPSING BINARY LIGHT
CURVESWITHLOCALLYLINEAREMBEDDING.TheAstronomicalJournal2012;143:123.
4. ConroyKE,PrˇsaA,StassunKG,OroszJA,FabryckyDC,andWelshWF.KEPLERECLIPS-
INGBINARYSTARS.IV.PRECISEECLIPSETIMESFORCLOSEBINARIESANDIDEN-
TIFICATIONOFCANDIDATETHREE-BODYSYSTEMS.TheAstronomicalJournal2014;147:45.
155. Conroy KE, Prˇsa A, Stassun KG, et al. Kepler Eclipsing Binary Stars. V. Identification of 31
CandidateEclipsingBinariesintheK2EngineeringDataset.PublicationsoftheAstronomical
Society of the Pacific 2014;126:914.
6. LaCourse DM, Jek KJ, Jacobs TL, et al. Kepler Eclipsing Binary Stars. VI. Identification of
Eclipsing Binaries in the K2 Campaign 0 Data-set. 2015. doi: 10.48550/arXiv.1503.01829.
arXiv: 1503.01829 [astro-ph].
7. Kirk B, Conroy K, Prˇsa A, et al. KEPLER ECLIPSING BINARY STARS. VII. THE CAT-
ALOG OF ECLIPSING BINARIES FOUND IN THE ENTIRE KEPLER DATA SET. The
Astronomical Journal 2016;151:68.
8. Abdul-Masih M, Prˇsa A, Conroy K, et al. KEPLER ECLIPSING BINARY STARS. VIII.
IDENTIFICATIONOFFALSEPOSITIVEECLIPSINGBINARIESANDRE-EXTRACTION
OF NEW LIGHT CURVES. The Astronomical Journal 2016;151:101.
9. Garcia-Piquer A, Morales JC, Ribas I, et al. Efficient Scheduling of Astronomical Observa-
tions - Application to the CARMENES Radial-Velocity Survey. Astronomy & Astrophysics
2017;604:A87.
10. SaundersES.OptimalObservingofAstronomicalTimeSeriesUsingAutonomousAgents.2007.
11. Wang C, Bai Y, Han H, Yang H, and Liu J. Transfer Learning Applied to Stellar Light Curve
Classification. 2023. arXiv: 2305.13745 [astro-ph]. (Visited on 06/12/2023).
12. Bellm EC. The Zwicky Transient Facility. 2014. doi: 10.48550/arXiv.1410.8185. arXiv:
1410.8185 [astro-ph].
13. Mahabal A, Rebbapragada U, Walters R, et al. Machine Learning for the Zwicky Transient
Facility. Publications of the Astronomical Society of the Pacific 2019;131:038002.
14. LiuJ,SoriaR,WuXF,WuH,andShangZ.TheSiTianProject.AnaisdaAcademiaBrasileira
de Ciˆencias 2021;93:e20200628.
15. LeCun Y, Bengio Y, and Hinton G. Deep Learning. Nature 2015;521:436–44.
16. Krizhevsky A, Sutskever I, and Hinton GE. ImageNet Classification with Deep Convolutional
Neural Networks. In: Advances in Neural Information Processing Systems. Vol. 25. Curran
Associates, Inc., 2012.
17. Lipton ZC, Berkowitz J, and Elkan C. A Critical Review of Recurrent Neural Networks for
Sequence Learning. 2015. doi: 10.48550/arXiv.1506.00019. arXiv: 1506.00019 [cs].
18. He K, Zhang X, Ren S, and Sun J. Deep Residual Learning for Image Recognition. 2015. doi:
10.48550/arXiv.1512.03385. arXiv: 1512.03385 [cs].
19. Vaswani A, Shazeer N, Parmar N, et al. Attention Is All You Need. 2017. doi: 10.48550/
arXiv.1706.03762. arXiv: 1706.03762 [cs].
1620. Devlin J, Chang MW, Lee K, and Toutanova K. BERT: Pre-training of Deep Bidirectional
TransformersforLanguageUnderstanding.In:Proceedingsofthe2019ConferenceoftheNorth
American Chapter of the Association for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers).Minneapolis,Minnesota:AssociationforCompu-
tational Linguistics, 2019:4171–86. doi: 10.18653/v1/N19-1423.
21. Radford A, Narasimhan K, Salimans T, and Sutskever I. Improving Language Understanding
by Generative Pre-Training. 2018.
22. Radford A, Wu J, Child R, Luan D, Amodei D, and Sutskever I. Language Models Are Unsu-
pervised Multitask Learners. 2019.
23. Brown TB, Mann B, Ryder N, et al. Language Models Are Few-Shot Learners. 2020. doi:
10.48550/arXiv.2005.14165. arXiv: 2005.14165 [cs].
24. Elman JL. Finding Structure in Time. Cognitive Science 1990;14:179–211.
25. HochreiterSandSchmidhuberJ.LongShort-TermMemory.NeuralComputation1997;9:1735–
80.
26. Cho K, van Merrienboer B, Bahdanau D, and Bengio Y. On the Properties of Neural Machine
Translation: Encoder-Decoder Approaches. 2014. doi: 10.48550/arXiv.1409.1259. arXiv:
1409.1259 [cs, stat].
27. LeCun Y, Boser B, Denker JS, et al. Backpropagation Applied to Handwritten Zip Code
Recognition. Neural Computation 1989;1:541–51.
28. TanMandLeQV.EfficientNet:RethinkingModelScalingforConvolutionalNeuralNetworks.
2020. arXiv: 1905.11946 [cs, stat].
29. TanMandLeQV.EfficientNetV2:SmallerModelsandFasterTraining.2021.doi:10.48550/
arXiv.2104.00298. arXiv: 2104.00298 [cs].
30. Liu Z, Lin Y, Cao Y, et al. Swin Transformer: Hierarchical Vision Transformer Using Shifted
Windows. 2021. arXiv: 2103.14030 [cs].
31. Liu Z, Hu H, Lin Y, et al. Swin Transformer V2: Scaling Up Capacity and Resolution. 2022.
doi: 10.48550/arXiv.2111.09883. arXiv: 2111.09883 [cs].
32. Zhou T, Niu P, Wang X, Sun L, and Jin R. One Fits All:Power General Time Series Analysis
by Pretrained LM. 2023. arXiv: 2302.11939 [cs.LG].
33. TsaiYHH,BaiS,LiangPP,KolterJZ,MorencyLP,andSalakhutdinovR.MultimodalTrans-
former for Unaligned Multimodal Language Sequences. 2019. arXiv: 1906.00295 [cs.CL].
34. Yang CHH, Tsai YY, and Chen PY. Voice2Series: Reprogramming Acoustic Models for Time
Series Classification. 2022. arXiv: 2106.09296 [cs.LG].
35. Borucki WJ, Koch D, Basri G, et al. Kepler Planet-Detection Mission: Introduction and First
Results. Science (New York, N.Y.) 2010;327:977–80.
36. Cleve JEV and Caldwell DA. Kepler Instrument Handbook. In: 2016.
1737. Yang H and Liu J. The Flare Catalog and the Flare Activity in the Kepler Mission. The
Astrophysical Journal Supplement Series 2019;241:29.
38. HanHG,CuiKM,LiuJF,YangHQ,FangX,andZhaoRN.StellarActivityCyclesasRevealed
by Long-Term Beat-like Patterns from Light Curves of Kepler. Research in Astronomy and
Astrophysics 2021;21:142.
39. Kokol P, Kokol M, and Zagoranski S. Machine Learning on Small Size Samples: A Synthetic
Knowledge Synthesis. Science Progress 2022;105:00368504211029777.
40. Cl´emen¸conSandLaforgueP.StatisticalLearningfromBiasedTrainingSamples.arXive-prints
2019:arXiv:1906.12304.
41. Taniguchi H, Sato H, and Shirakawa T. A Machine Learning Model with Human Cognitive
Biases Capable of Learning from Small and Biased Datasets. Scientific Reports 2018;8:7397.
42. Kholopov PN, Samus N, Frolov M, et al. Combined General Catalogue of Variable Stars. In:
1998.
43. DurlevichOV,FrolovMS,KazarovetsEV,andSamusNN.TheListofErrorsintheGCVS,4th
Edition. I. Volumes I-III. Bulletin d’Information du Centre de Donnees Stellaires 1994;45:19.
44. ArtyukhinaNM,DurlevichOV,FrolovMS,etal.VizieROnlineDataCatalog:GCVS,Vol.V.:
Extragalactic Variable Stars (Artyukhina+ 1996). VizieR Online Data Catalog 1996:II/205.
45. SamusNN,PastukhovaEN,DurlevichOV,KazarovetsEV,andKireevaN.The84thName-List
of Variable Stars. Globular Clusters (Third Part) and Novae. Peremennye Zvezdy 2021;41:7.
46. Moln´ar L, Plachy E, Juh´asz A´L, and Rimoldini L. Gaia Data Release 2 - Validating the Clas-
sification of RR Lyrae and Cepheid Variables with the Kepler and K2 Missions. Astronomy &
Astrophysics 2018;620:A127.
47. Abdul-Masih M, Prˇsa A, Conroy K, et al. KEPLER ECLIPSING BINARY STARS. VIII.
IDENTIFICATIONOFFALSEPOSITIVEECLIPSINGBINARIESANDRE-EXTRACTION
OF NEW LIGHT CURVES. The Astronomical Journal 2016;151:101.
48. BradleyPA,GuzikJA,MilesLF,UytterhoevenK,JackiewiczJ,andKinemuchiK.RESULTS
OF A SEARCH FOR γ DOR AND δ SCT STARS WITH THE KEPLER SPACECRAFT.
The Astronomical Journal 2015;149:68.
49. Balona LA. Gaia Luminosities of Pulsating A-F Stars in the Kepler Field. Monthly Notices of
the Royal Astronomical Society 2018;479:183–91.
50. Lomb NR. Least-Squares Frequency Analysis of Unequally Spaced Data. Astrophysics and
Space Science 1976;39:447–62.
51. Scargle JD. Studies in Astronomical Time Series Analysis. II - Statistical Aspects of Spectral
Analysis of Unevenly Spaced Data. The Astrophysical Journal 1982;263:835.
52. VanderPlasJT.UnderstandingtheLomb-ScarglePeriodogram.TheAstrophysicalJournalSup-
plement Series 2018;236:16.
1853. Kim Y. Convolutional Neural Networks for Sentence Classification. 2014. doi: 10.48550/
arXiv.1408.5882. arXiv: 1408.5882 [cs].
54. Schuster M and Paliwal K. Bidirectional Recurrent Neural Networks. Signal Processing, IEEE
Transactions on 1997;45:2673–81.
55. Salinas H, Pichara K, Brahm R, P´erez-Galarce F, and Mery D. Distinguishing a Planetary
TransitfromFalsePositives:ATransformer-basedClassificationforPlanetaryTransitSignals.
Monthly Notices of the Royal Astronomical Society 2023:stad1173.
56. Chung J, Gulcehre C, Cho K, and Bengio Y. Empirical Evaluation of Gated Recurrent Neural
Networks on Sequence Modeling. 2014. arXiv: 1412.3555 [cs].
57. Ke G, Meng Q, Finley T, et al. LightGBM: A Highly Efficient Gradient Boosting Decision
Tree. 2017.
58. Kitaev N, Kaiser L, and Levskaya A. Reformer: The Efficient Transformer. In: International
Conference on Learning Representations. 2019. (Visited on 09/25/2023).
59. Liu S, Yu H, Liao C, et al. PYRAFORMER: LOW-COMPLEXITY PYRAMIDAL AT- TEN-
TION FOR LONG-RANGE TIME SERIES MODELING AND FORECASTING. 2022.
60. Dosovitskiy A, Beyer L, Kolesnikov A, et al. An Image Is Worth 16x16 Words: Transformers
for Image Recognition at Scale. 2021. doi: 10.48550/arXiv.2010.11929. arXiv: 2010.11929
[cs].
61. Wang W, Xie E, Li X, et al. Pyramid Vision Transformer: A Versatile Backbone for Dense
Prediction without Convolutions. 2021. arXiv: 2102.12122 [cs].
62. Ying H, Zhang S, Li L, et al. InternLM-Math: Open Math Large Language Models Toward
Verifiable Reasoning. 2024. arXiv: 2402.06332 [cs.CL].
63. LuH,LiuW,ZhangB,etal.DeepSeek-VL:TowardsReal-WorldVision-LanguageUnderstand-
ing. 2024. arXiv: 2403.05525 [cs.AI].
64. Chu Y, Xu J, Zhou X, et al. Qwen-Audio: Advancing Universal Audio Understanding via
Unified Large-Scale Audio-Language Models. 2023. arXiv: 2311.07919 [eess.AS].
65. Snoek J, Larochelle H, and Adams RP. Practical Bayesian Optimization of Machine Learning
Algorithms. 2012. doi: 10.48550/arXiv.1206.2944. arXiv: 1206.2944 [cs, stat].
66. Mnih V, Kavukcuoglu K, Silver D, et al. Human-Level Control through Deep Reinforcement
Learning. Nature 2015;518:529–33.
67. Real E, Moore S, Selle A, et al. Large-Scale Evolution of Image Classifiers. 2017. doi: 10.
48550/arXiv.1703.01041. arXiv: 1703.01041 [cs].
68. BergstraJ,BardenetR,BengioY,andK´eglB.AlgorithmsforHyper-ParameterOptimization.
In:Advances in Neural Information Processing Systems.Vol.24.CurranAssociates,Inc.,2011.
(Visited on 05/08/2023).
69. Hutter F, Hoos HH, and Leyton-Brown K. Sequential Model-Based Optimization for General
AlgorithmConfiguration.In:LearningandIntelligentOptimization.Ed.byCoelloCAC.Berlin,
Heidelberg: Springer Berlin Heidelberg, 2011:507–23.
19Table 5: Phase Importance Catalog
Star/Focused phase 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
DSCT 001026294 0.9996 0.9934 1.0000 1.0000 0.9995 0.9986 0.9986 0.9692 0.6977 0.0000
DSCT 001162150 0.1585 0.1310 1.0000 0.7921 0.3214 0.1172 0.1245 0.0323 0.0184 0.0000
DSCT 001163943 0.3598 0.1016 1.0000 0.4261 0.0467 0.1701 0.0711 0.0031 0.0393 0.0188
DSCT 001294670 0.3018 0.3963 1.0000 0.6999 0.0631 0.0211 0.0327 0.0001 0.0034 0.0004
DSCT 001430590 0.1579 0.2669 1.0000 0.5738 0.4043 0.0964 0.0573 0.0266 0.0102 0.0041
DSCT 001434660 0.0923 0.1533 1.0000 0.6423 0.6384 0.1624 0.0591 0.0247 0.0037 0.0142
DSCT 001570023 0.0688 0.1608 1.0000 0.5841 0.4261 0.0822 0.0513 0.0040 0.0000 0.0027
DSCT 001571717 0.9996 0.9994 1.0000 0.9999 1.0000 0.9997 1.0000 0.9942 0.9730 0.0000
DSCT 001572768 0.0000 0.9147 0.9228 0.9773 0.9816 0.8840 0.8007 0.8885 0.4628 0.4773
DSCT 001575977 0.1454 0.6365 1.0000 0.8420 0.2347 0.3248 0.3394 0.0303 0.0057 0.0241
Phase-importancecatalogillustratestherelationshipbetweenimportanceandphase,withthefirsttenrows
shown.
Table 6: Sampling Catalog
Star/Sampling Rate 0.02d 0.04d 0.06d 0.08d 0.1d 0.12d 0.14d 0.16d 0.18d 0.2d
DSCT 001026294 0.9288 0.4412 0.0002 0.0003 0.0000 0.0000 0.0434 0.0847 0.0998 0.0141
DSCT 001162150 0.9817 0.9598 0.1599 0.6392 0.3082 0.1531 0.4663 0.2866 0.1713 0.1649
DSCT 001163943 0.9891 0.7806 0.6678 0.5106 0.5077 0.6081 0.6654 0.6412 0.5840 0.4549
DSCT 001294670 0.9985 0.9980 0.9521 0.7222 0.3226 0.6149 0.5517 0.5106 0.5157 0.2001
DSCT 001430590 0.9935 0.9918 0.2952 0.6864 0.6574 0.2160 0.1984 0.5479 0.2953 0.3322
DSCT 001434660 0.9746 0.9851 0.8613 0.3703 0.4243 0.3810 0.5342 0.2130 0.3090 0.2803
DSCT 001570023 0.9944 0.9897 0.8082 0.8866 0.4413 0.2945 0.5183 0.4653 0.2904 0.1887
DSCT 001571717 0.9656 0.0958 0.0018 0.0159 0.0000 0.0001 0.3438 0.0925 0.0448 0.0198
DSCT 001572768 0.9253 0.3443 0.1441 0.1154 0.1095 0.0479 0.0768 0.0454 0.1152 0.1194
DSCT 001575977 0.9839 0.7853 0.6452 0.3186 0.3033 0.3250 0.2376 0.3029 0.2770 0.2040
Sampling catalog illustrate the relationship between accuracy and sampling rate, with the first ten rows
shown here
70. AkibaT,SanoS,YanaseT,OhtaT,andKoyamaM.Optuna:ANext-generationHyperparam-
eter Optimization Framework. 2019. doi: 10.48550/arXiv.1907.10902. arXiv: 1907.10902
[cs, stat].
71. Dettmers T, Pagnoni A, Holtzman A, and Zettlemoyer L. QLoRA: Efficient Finetuning of
Quantized LLMs. 2023. arXiv: 2305.14314 [cs.LG].
72. Opitz J and Burst S. Macro F1 and Macro F1. CoRR 2019;abs/1911.03347.
73. Balona LA. Gaia luminosities of pulsating A-F stars in the Kepler field. Monthly Notices of
the Royal Astronomical Society 2018;479:183–91.
74. Muthukrishna D, Mandel KS, Lochner M, Webb S, and Narayan G. Real-Time Detection
of Anomalies in Large-Scale Transient Surveys. Monthly Notices of the Royal Astronomical
Society 2022;517:393–419.
75. Kwon W, Li Z, Zhuang S, et al. Efficient Memory Management for Large Language Model
Serving with PagedAttention. 2023. arXiv: 2309.06180 [cs.LG].
20Figure 5: The importance-phase diagram for GDOR is provided. The upper section displays the
flux variation of specific object, with heatmap illustrating phase importance. The lower part of the
diagram shows the importance-phase for GDOR. The dispersion observed in the folded light curve
might be associated with the normalization process of the flux.
21Figure 6: The importance-phase diagrams for DSCT are provided. The upper section displays the
flux variation of an example, with heatmaps illustrating phase importance. The lower part of the
diagrams shows the importance-phase for the variable.
22Figure 7: The importance-phase diagrams for HYB are provided. The upper section displays the
flux variation of an example, with heatmaps illustrating phase importance. The lower part of the
diagrams shows the importance-phase for the variable.
23Figure 8: The importance-phase diagrams for EB are provided. The upper section displays the flux
variation of specific objects, with heatmaps illustrating phase importance. The lower part of the
diagrams shows the importance-phase for each variable.
24Figure 9: The importance-phase diagrams for RR are provided. The upper section displays the flux
variation of specific objects, with heatmaps illustrating phase importance. The lower part of the
diagrams shows the importance-phase for each variable.
25   
   
   
     ' 6 & 7
 ( %
 * ' 2 5
   
 + < %
 5 5
   
                     
 3 R L Q W V  S H U  3 H U L R G
Figure 10: The influence of each period sampling point on model accuracy.
Figure 11: The confusion matrix of Conv1D+Transformer model are presented, including T2CEP
class. Thediagonalelementsrepresentthecorrectpredictions,alongwiththepercentagesofcorrectly
classified observations for each true class.
26
 \ F D U X F F $Figure 12: The confusion matrix of LIGHTGBM model are presented, including T2CEP class. The
diagonalelementsrepresentthecorrectpredictions,alongwiththepercentagesofcorrectlyclassified
observations for each true class.
27Figure 13: The confusion matrix of BiLSTM+Attention model. The diagonal elements represent
the correct predictions, along with the percentages of correctly classified observations for each true
class.
28Figure 14: The confusion matrix of Conv1D+GRU model. The diagonal elements represent the
correct predictions, along with the percentages of correctly classified observations for each true
class.
29Figure 15: The confusion matrix of Conv1D+BiLSTM model. The diagonal elements represent the
correctpredictions,alongwiththepercentagesofcorrectlyclassifiedobservationsforeachtrueclass.
30Figure 16: The confusion matrix of Efficientnet model. The diagonal elements represent the correct
predictions, along with the percentages of correctly classified observations for each true class.
31Figure 17: The confusion matrix of Swin Transformer model are presented, including T2CEP class.
The diagonal elements represent the correct predictions, along with the percentages of correctly
classified observations for each true class.
32Figure 18: The confusion matrix of LLM method in StarWhisper LC.
33Figure 19: The confusion matrix of MLLM method in StarWhisper LC.
34Figure 20: The confusion matrix of LALM in StarWhisper LC.
35