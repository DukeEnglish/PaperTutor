Aligning Individual and Collective Objectives in Multi-Agent Cooperation
YangLi1 WenhaoZhang2 JianhongWang1 ShaoZhang2 YaliDu3 YingWen2 WeiPan1
Abstract cent progress in cooperative Multi-Agent Reinforcement
Learning(MARL)hasprimarilyfocusedonaddressingpure-
In the field of multi-agent learning, the chal-
motivecooperation,whichalsoknownasacommon-payoff
lengeofmixed-motivecooperationispronounced,
game, represents situations where individual goals fully
given the inherent contradictions between indi-
alignwithcollectiveobjectives(Yuetal.,2022;Wangetal.,
vidualandcollectivegoals. Currentresearchin
2020; Zhong et al., 2023; Li et al., 2024). Despite pure-
this domain primarily focuses on incorporating
motive cooperation, mixed-motive cooperation, which is
domainknowledgeintorewardsorintroducingad-
typically defined by the imperfect alignment between in-
ditionalmechanismstofostercooperation. How-
dividual and collective rationalities, is more common in
ever,manyofthesemethodssufferfromthedraw-
real-worldscenarios(Rapoport,1974;McKeeetal.,2020a).
backs of manual design costs and the lack of a
theoreticalgroundingconvergenceprocedureto Recentmixed-motivecooperativestudiesinMARLhave
the solution. To address this gap, we approach largelyemployedhand-crafteddesignstopromotecollabo-
themixed-motivegamebymodelingitasadiffer- ration. Incorporatingadditionalmechanismstoalignobjec-
entiable game to study learning dynamics. We tivesappearstobeapopularavenue,includingaspectssuch
introduce a novel optimization method named asreputation(Anastassacosetal.,2021),norms(Vinitsky
Altruistic Gradient Adjustment (AgA) that em- etal.,2023),andcontracts(Hughesetal.,2020). Another
ploys gradient adjustments to novelly align in- prevalent method draws on intrinsic motivation to align
dividualandcollectiveobjectives. Furthermore, individualandcollectiveobjectives,augmentingaltruistic
we provide theoretical proof that the selection collaboration by integrating heuristic knowledge into the
ofanappropriatealignmentweightinAgAcan incentivefunction. Conventionally,someworksaccumulate
accelerate convergence towards the desired so- individualrewardswiththegrouptocultivatealtruisticcon-
lutionswhileeffectivelyavoidingtheundesired duct(Hostalleroetal.,2020;Peysakhovich&Lerer,2018;
ones. The visualization of learning dynamics Apt & Scha¨fer, 2014). Furthermore, some studies derive
effectively demonstrates that AgA successfully moresophisticatedpreferencesignalsfromtherewardsof
achievesalignmentbetweenindividualandcollec- other agents (Hughes et al., 2018; McKee et al., 2020a).
tiveobjectives. Additionally,throughevaluations Additionally,severalapproachesaimtolearnthepotential
conducted on established mixed-motive bench- influencesofanagent’sactionsonothers(Yangetal.,2020;
marks such as the public good game, Cleanup, Jaques et al., 2019; Lu et al., 2022). Most of these algo-
Harvest,andourmodifiedmixed-motiveSMAC rithms are highly dependent on carefully crafted designs,
environment,wevalidateAgA’scapabilitytofa- necessitatingsignificanthumanexpertiseanddetaileddo-
cilitatealtruisticandfaircollaboration. mainknowledge. However,theylackin-depththeoretical
analysisconcerningalignmentandconvergence.
Inthiswork,wefirstpresentthedifferentiablemixed-motive
1.Introduction
game (DMG), premised on the assumption that the play-
ers’ loss functions are at least twice differentiable. The
Multi-agentcooperationprimarilyfocusesonlearninghow
proposed formulation offers an efficient tool for analyz-
topromotecollaborativebehaviorinsharedenvironments.
ing the dynamics of gradient-based methods when opti-
Typically,researchdividesmulti-agentcooperationintotwo
mizingmixed-motiveobjectives. Furthermore,weexplore
prominentareas,pure-motivecooperationandmixed-motive
theoptimizationtrajectoriesofsimultaneousoptimization
cooperation (Du et al., 2023; McKee et al., 2020a). Re-
methodsandgradientadjustmentmethods(consensusopti-
1DepartmentofComputerScience,UniversityofManchester, mization(Meschederetal.,2017)andsymplecticgradient
Manchester,UK2ShanghaiJiaoTongUniversity,Shanghai,China adjustment(Letcheretal.,2019))inthecontextofatwo-
3DepartmentofInformatics,King’sCollegeLondon,London,UK.
player DMG, as shown in Fig. 1. First, we observe that
Correspondenceto:YingWen<ying.wen@sjtu.edu.cn>,WeiPan
gradientadjustmentmethodsareineffectiveinaddressing
<wei.pan@manchester.ac.uk>.
1
4202
beF
91
]AM.sc[
1v61421.2042:viXraAligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
mixed-motiveproblems. Thesemethodstendtoprioritize games. Prosocial(Peysakhovich&Lerer,2018)improves
stabilityattheexpenseofmitigatingindividualloss,which collectiveperformancebyblendingindividualrewardsto
conflictswiththeinterestsofindividualparticipants(Chen redrafttheagent’soverallutility. WhilePED-DQNenables
etal.,2023). Whilesimultaneousoptimizationwithcollec- agents to incrementally adjust their reward functions for
tivelosssuccessfullyenhancescollectiveinterests,itseems enhancedcollaborativeactionthroughinter-agentevaluative
toneglectindividualinterests. signalexchanges(Hostalleroetal.,2020),Giftingdirectly
rewardotheragentsaspartofactionspace(Lupu&Precup,
To address the issue, we propose an Altruistic Gradient
2020). Inequityaversionfurtherintegratestheconceptinto
Adjustment(AgA)optimizationalgorithm,designedtoalign
Markovgamesbyaddingenvy(disadvantageousinequality)
individual and collective objectives from a gradient per-
andguilt(advantageousinequality)rewardstotheoriginal
spective. We provide a theoretical proof to suggest that
individualrewards(Hughesetal.,2018). Simultaneously,
appropriately choosing the sign of the alignment weight
SocialValueOrientation(SVO)strategyintroducesaunique
of the gradient terms could speed up the convergence of
sharedrewards-basedcompensationapproach,encouraging
the AgA optimization strategy towards stable fixed point
behavior modifications in line with interdependence the-
ofcollectiveobjectivewhileeffectivelybypassingunstable
ory (McKee et al., 2020b). The LIO strategy bypasses
ones. ThetrajectoryvisualizationsinFig.1,alongwiththe
the need to modify extrinsic rewards by empowering an
signselectionablationstudytrajectoriespresentedinFig.3,
agenttodirectlyinfluenceitspartner’sactions(Yangetal.,
servetodemonstratesuccessfulalignmentofobjectives.
2020). Meanwhile,otherstudiesintroducenewcooperative
To further assess the effectiveness of our proposed AgA mechanismssuchasincorporatingareputationmodelasan
approach, weperformanassortmentofexperimentsona intrinsicreward(McKeeetal.,2023). TheCNMstrategy
two-player public goods game and two sequential social delineatesacceptableandunacceptablebehaviorsusingclas-
dilemmagames(CleanupandHarvest)(Leiboetal.,2017). sifiersassocialnorms,aidingagentstolearnadvantageous
Giventherelativelysimplenatureofthesestandardtestsce- normsforimprovedreturnsperagent(Vinitskyetal.,2023).
nariosusedinmixed-motivecooperationcontexts,wechose Despitetheseadvances,manycurrentstudieslackbothcost-
tomodifythewidelyusedSMACenvironment(Samvelyan efficiencyindesignandtheoreticalanalysisofalignment
etal.,2019)tocreateamorecomplexmixed-motivecoop- andconvergence. Toaddressthis,ourstudyappliesagra-
erativesetting. Evaluationbasedonvariousperformance dientperspectivetoaligngoalsandfurtherinvestigatesthe
metricsincludingsocialwelfare,equality,andwinrate,sug- learningdynamicsofourproposedmethod.
geststhattheAgAapproachsurpassestheperformanceof
Gradient-based Methods. Our proposed AgA is fun-
baselinemethods.
damentally a gradient adjustment methodology, making
The contribution of the paper can be summarized as fol- gradient-basedoptimizationmethodshighlyrelevanttothe
lows: (1) We introduce the differentiable mixed-motive contextofthispaper. Methodsbasedongradienthavebeen
game(DMG)andproposeAltruisticGradientAdjustment developedtofindstationarypoints,suchastheNashequi-
(AgA) algorithm to align individual and collective objec- libriumorstablefixedpoints. OptimisticMirrorDescent
tivesfromagradientperspective. (2)Wetheoreticallyprove leverageshistoricaldatatoextrapolatesubsequentgradients
that careful selection of the alignment weight’s sign can (Daskalakisetal.,2018),whileGideletal.(2020)extends
expediteconvergenceinAgAandeffectivelyavoidunsta- this concept by advocating averaging methodologies and
blefixedpoints. (3)Visualizationsbasedonatwo-player variants of extrapolation techniques. Consensus gradient
DMG show that our method efficiently aligns individual adjustment,orconsensusoptimization,isatechniquethat
andcollectiveobjectives. Furtherexperimentsconductedon embeds a consensus agreement term within the gradient
mixed-motivetestbedsandmodifiedSMACenvironments toensureitsconvergence(Meschederetal.,2017). Learn-
underscorethesuperiorperformanceofourAgAmethodin ing with Opponent-Learning Awareness (LOLA) utilizes
severalmeasurementmetricscomparedtobaselines. informationfromotherplayerstocomputeoneplayer’san-
ticipatedlearningsteps(Foersteretal.,2018).Subsequently,
StableOpponentShaping(SOS)(Letcheretal.,2021)and
2.RelatedWork
ConsistentOpponent-LearningAwareness(COLA)(Willi
Mixed-motive cooperation. Mixed-motive cooperation etal.,2022)methodshaveenhancedtheLOLAalgorithm,
referstoscenarioswherethegroup’sobjectivesaresome- targetingconvergenceassuranceandinconsistencyelimina-
timesalignedandatothertimesconflicted(PhilipS.Gallo tion,respectively. SymplecticGradientAdjustment(SGA)
& McClintock, 1965). Recently, there has been a surge alterstheupdatedirectiontowardsthestablefixedpoints
in academic interest in the Sequential Social Dilemma based on a novel decomposition of game dynamics (Bal-
(SSD)(Leiboetal.,2017),whichexpandstheconceptfrom duzzietal.,2018;Letcheretal.,2019). Recently,Learning
itsrootsinmatrixgames(Macy&Flache,2002)toMarkov toPlayGames(L2PG)ensuresconvergencetowardsasta-
2AligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
blefixedpointbypredictingupdatestoplayers’parameters ingtheHessianH(w)isinvertible,then (1 ξ(w) 2)=
∇ 2∥ ∥
derivedfromhistoricaltrajectories(Chenetal.,2023).How- HTξ = 0 holds true if and only if ξ = 0. However, it
ever,thesemethods,whilefocusingonzero-sumorgeneral couldfailtoconvergetostablefixedpoint(Meschederetal.,
sumgames,couldpotentiallyactcountertotheirindividual 2017). Hence,theconsensusoptimizationmethodhasbeen
interests,astheymayprioritizestabilityoverminimizing proposed,incorporatinggradientadjustment (Mescheder
personalloss. Ourresearchspecificallytargetsthemixed- etal.,2017),shownasfollows:
motivesettingwiththeaimofreconcilingindividualand
1
collectiveobjectives. ξ˜=ξ+λ ξ(w) 2 =ξ+λ HTξ. (1)
·∇2∥ ∥ ·
3.Preliminaries Forsimplicity,wewillrefertotheconsensusgradientad-
justment as CGA. While CGA proves effective in certain
DifferentialGame. Thetheoryofdifferentialgameswas
specific scenarios, such as two-player zero-sum games,
initiallyproposedbyIsaacs(1965),aimingtoexpandthe
it unfortunately falls short in general games (Balduzzi
scopeofsequentialgametheorytoencompasscontinuous-
et al., 2018). To address this shortage, symplectic gradi-
timescenarios. Throughthelensofmachinelearning,we
entadjustment(SGA)(Balduzzietal.,2018;Letcheretal.,
formalizethedifferentialgame,asshowninDefinition3.1.
2019) is introduced to find the stable fixed point in gen-
Definition 3.1 (Differential Game (Balduzzi et al., 2018; eral sum games, such that ξ˜ = ξ +λ ATξ. Herein, A
·
Letcheretal.,2019)). Adifferentialgamecouldbedefined representstheantisymmetriccomponentofthegeneralized
asatuple ,w,ℓ ,where = 1,...,n delineatesthe HelmholtzdecompositionofH,H(w)=S(w)+A(w),
assembly{ oN fplayers} . ThepaN rame{ tersetw} = [w ]n Rd where S(w) denotes the symmetric component. The hy-
i
i ℓs =defi ℓne :d R, e dach Rwit nh w ri ep∈ reR sed ni tsa tn hd ed co= rre(cid:80) spon i= nd1 id ni g. l∈ oH se sere s,
.
p λer =para sim gnet (cid:0)er 1λ ξ,i Hs d Te ξterm Ai ⊤ne ξd ,Hby Tt ξhe +fo ϵl (cid:1)lo ,w win hg erf eor ϵm iu sla a:
{ i → }i=1 d⟨ ⟩⟨ ⟩
Theselossesareassumedtobeatleasttwicedifferential. smallpositivenumber.
Eachplayeri isequippedwithapolicy,parameterized
∈N
byw i,aimingtominimizeitsindividuallossℓ i. 4.Method
Werewritethesimultaneousgradientξ(w)ofadifferential 4.1.DifferentiableMixed-motiveGame
game as ξ(w) = ( ℓ ,..., ℓ ) Rd, which is
∇w1 1 ∇wn n
∈ Wefirstformulatethemixed-motivegameasadifferentiable
the gradient of the losses with respect to the parameters
game. Specifically,thedifferentiablemixed-motivegame
oftherespectiveplayers. Furthermore,HessianmatrixH
(DMG)isdefinedasatuple( ,w,ℓ),whereℓ ℓisat
mentionedinadifferentialgameistheJacobianmatrixof i
N ∈
leasttwicethedifferentiablelossfunctionfortheagenti.
thesimultaneousgradient.
Differentiablelossesexhibitthemixedmotivationproperty:
Thelearningdynamicsofdifferentialgameoftenrefersto minimization of individual losses can result in a conflict
theprocessofsequentialupdatesoverw. Thelearningrule between individuals or between individual and collective
foreachplayerisdefinedastheoperator, w w γξ, objectives(e.g.,maximizingsocialwelfareorbeatingop-
← −
whereξisafeasibledirection,andγ isastepsize(learning ponentsinbasketballmatches). Theprimaryobjectiveof
rate)todeterminethedistancetomoveforeachupdate. DMG is to optimize the collective objective by minimiz-
ing collective loss, denoted as ℓ , while simultaneously
GradientAdjustmentOptimization. Thestablefixed c
taking into account the need to minimize individual ob-
point,acriterioninitiatedfromstabilitytheory,corresponds
jectives, represented by ℓ. Before delving into our inter-
tothenatureoflocalNashequilibria(Balduzzietal.,2018),
est alignment methods, let us first introduce some nota-
asdefinedwithinthefieldofgametheory.Morespecifically,
tions. Weuseξ (w)toreferthegradientofcollectiveloss:
astablefixedpoint,exhibitingitsstability(robustness)to c
ξ (w)=( ℓ ,..., ℓ ).
minorperturbationsfromenvironments,makingitapplica- c ∇w1 c ∇wn c
ble to many real-world scenarios. The concept of stable Nonetheless,directoptimizationofindividualorcollective
fixedpointprevailingingeneral-sumandzero-sumgames, lossescanleadtoconvergenceissuesandamismatchbe-
isdefinedasfollows. tweenindividualandcollectiveinterests. Itisimprobable
Definition3.2. Apointw⋆isafixedpointifξ(w⋆)=0. If thatbyminimizingonlytheindividuallossofeachagent,
H(w⋆) 0andH(w⋆)isinvertible,thefixedpointw⋆is acollectiveminimumwillbeachieved(Leiboetal.,2017;
calledsta⪰ blefixedpoint. IfH(w⋆) 0,thepointiscalled McKeeetal.,2020b). Ontheotherhand,optimizingcol-
unstable. ≺ lectivelossmightproduceimprovedcollectiveoutcomes,
butattheriskofoverlookingindividualinterests. Thisphe-
Anaiveideatosteerthedynamictowardsconvergenceat nomenonisalsodemonstratedinExample4.1.Furthermore,
stablefixedpointsinvolvesminimizing 1 ξ(w) 2. Assum- thelocalconvergenceofthegradientdescentonsingularcol-
2∥ ∥
3AligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
6 5
4
30.800.4.08 00 000 0. 08 .. ... .48 440 8 .0 80. 0004.8 ...0 448.08.A S4 0i0 g .00 40 m.0 .. . 00A 44 0 .. 08u ( l0O -.4 Cu 000 .o
0
40 ..r .48. 40 08s. .4 8)0 0.0 .8000 .. 80.0 0. 08 0 .4.80.0 08 .. 0 84 .0 00 .4.4
0.4
0.0 00 .. 8 04 .80 0.8 .0 00
0
.4. .0 80.0 4.8
0.0
6 5
4
30 0 0
0
A Sig mA u ( lO -Cu ors) 56 6 44
0
1 40 84 96 72 211 4220 88 0 38 21 8131 164 21452
2
0.8
2
16
Simul-Ind 0.4 0.8 8
Simul-Co 1 Start Point 1 Start Point
CGA 0
SGA 0 1 2 3 4 0 1 2 3 4
AgA (ours) Action of Player 1 Action of Player 1
(a)CollectiveRewardLandscape (b)RewardContourofPlayer1 (c)RewardContourofPlayer2
Figure1.Trajectoriesofoptimizationinatwo-playerDMG(asdelineatedinExample4.1). Fig.1adisplaysthetrajectoriesoverthe
collectiverewardlandscape-deeperorangeequatestohigherrewards.Remarkably,onlySimul-CoandAgAmakesuccessfulstrides
towardsthesocialoptimum.Fig.1bandFig.1cdelineatetrajectoriesontheindividualrewardcontour,underscoringSimul-Co’sneglect
forPlayer1’sinterestseventhoughitachieveshigherrewardswithfewersteps(65stepsversus100stepsofAgA).Conversely,ourAgA
optimizesalongthesummitofPlayer1’srewardwhilealsomaximizingthecollectivereward,demonstratingasuccessfulalignment.
lectivefunctionsisnotalwaysguaranteed(Balduzzietal., updatetrajectorynavigatesthroughthecrestsandtroughs
2018). Thisalsotypifiestheproblemofcreditassignment ofPlayer1’srewardcontour. Thetrajectorysuggestsadis-
inMARL. regardforPlayer1’spreferences,signifyingthattheupdates
arepredominantlydrivenbytheoverarchingcollectivegoal.
AlignmentDilemmainDMG. Here,weprovideanexam-
pleofatwo-playerdifferentiablemixed-motivegameand
4.2.AltruisticGradientAdjustment
theoptimizationresultsoftheaforementionedmethods.
Inthissection,weproposethealignmentofindividualand
Example 4.1. Consider a two-player DMG with
ℓ (a ,a )= sin(a a +a2)andℓ (a ,a )= [cos(1+ collective interests towards achieving stable fixed points
a1 1 (12
+a
)− 2)+a1 a22
].
Th2 erewar2 dsf1 ort2 hetw−
oplayers
ofcollectiveobjectiveswithinthecontextofmixed-motive
1 − 2 1 2 cooperation.Revisitthatthelearningdynamicsofsimultane-
arethenegationoftheirrespectivelosses.
ousoptimizationoncollectivelossisgivenbyw =w γξ ,
c
−
where ξ represents the gradients of collective loss ℓ re-
Weprovideavisualrepresentationofoptimizationtrajec- c c
spectively, with respect to the parameters of each player
toriesusingaseriesofmethodstoinvestigatethelearning
w w. AlthoughSimul-Coefficientlyenhancescollec-
dynamicsinvolvedinresolvingthetoytwo-playerdifferen- i ∈
tiveperformance,itsshortcomingslieinnotguaranteeing
tiablemixed-motivegame. Inparticular,wefirstimplement
stablefixedpoints(Balduzzietal.,2018)andoverlooking
simultaneousoptimizationofindividuallosses(Simul-Ind)
individualinterestsasdiscussedinExample4.1.
withrespecttoeachparameter,definedbythelearningrule
w i =w i −γξ i,fori ∈{1,2 },whereξ i = ∇wiℓ i. Simul- To tackle this challenge, we introduce the Altruistic Gra-
taneousoptimizationofcollectiveloss(Simul-Co)replaces dientAdjustment(AgA)methodwiththegoalofaligning
theindividualgradientwiththecollectivegradientξ cofcol- individual and collective objectives while seeking stable
lectivelossℓ c =ℓ 1+ℓ 2.Furthermore,inordertoinvestigate fixed points. Unlike existing gradient adjustment meth-
the learning dynamics of prevalent gradient modification ods(Meschederetal.,2017;Balduzzietal.,2018;Letcher
optimizationapproachessuchasSGAandCGAforthetwo- etal.,2019;Chenetal.,2023)thatprimarilyfocusoniden-
player differentiable mixed-motive game, we modify the tifyingthestablefixedpointsofindividuallosses,ourpro-
learningrulebyintegratingtherespectiveadjustedgradient posedAgAmethodintegratestheindividualgradientinto
asdelineatedinSection3. the collective gradient and ensures convergence to stable
fixed points of the collective loss. Specifically, AgA is
Fig.1ashowstheoptimizationpathsonthecollectivere-
definedasfollows.
ward landscapes, with every path starting from the front-
bottomofthelandscape. Thecollectiverewardisdefined Proposition4.2(AltruisticGradientAdjustment). Altruistic
as social welfare, i.e., the sum of individual rewards. As gradientadjustment(AgA)extendsthegradientterminthe
showninthefigure,Simul-Ind,CGA,andSGAconvergeto learningdynamicas
unstablepointsorlocalmaxima. Simul-Coeffectivelynav- ξ˜:=ξ +λξ =ξ +λ(cid:0) ξ+HTξ (cid:1) , (2)
igatestowardstheapexofthecollectiverewardlandscape c adj c c c
asdepictedinFig.1a. However,Simul-Coisineffectivein whereλ Risalignmentparameter,λξ iscalledadjust-
adj
∈
aligningindividualandcollectiveobjectives,leadingtothe mentterm. Inξ ,ξ andH isthegradientvectorand
adj c c
neglectofindividualinterests. AsdepictedinFig.1b,the Hessianmatrixofthegameaboutcollectiveloss.
4
2 reyalP
fo
noitcA
2 reyalP
fo
noitcAAligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
⇠<latexit sha1_base64="teIFfj7KlQS0a5XJNxiiKlI9GSU=">AAAB+XicbVDLSsNAFL2pr1pfUZduBovgqiQi6rLoxmUF+4AmhMl00g6dTMLMpFhC/8SNC0Xc+ifu/BsnbRbaemDgcM693DMnTDlT2nG+rcra+sbmVnW7trO7t39gHx51VJJJQtsk4YnshVhRzgRta6Y57aWS4jjktBuO7wq/O6FSsUQ86mlK/RgPBYsYwdpIgW17MdajMMq9JxbkZDYL7LrTcOZAq8QtSR1KtAL7yxskJIup0IRjpfquk2o/x1Izwums5mWKppiM8ZD2DRU4psrP58ln6MwoAxQl0jyh0Vz9vZHjWKlpHJrJIqda9grxP6+f6ejGz5lIM00FWRyKMo50gooa0IBJSjSfGoKJZCYrIiMsMdGmrJopwV3+8irpXDTcq8blw2W9eVvWUYUTOIVzcOEamnAPLWgDgQk8wyu8Wbn1Yr1bH4vRilXuHMMfWJ8/PYKUEQ==</latexit> c ⇠<latexit sha1_base64="Hqi9ieXIgIzwk5pwEv5hWYXd7l8=">AAAB/XicbVDLSsNAFL2pr1pf8bFzEyyCq5JIUZdFNy4r9AVNKJPppB06mYSZiVhD8FfcuFDErf/hzr9x0mahrQcGDufcyz1z/JhRqWz72yitrK6tb5Q3K1vbO7t75v5BR0aJwKSNIxaJno8kYZSTtqKKkV4sCAp9Rrr+5Cb3u/dESBrxlprGxAvRiNOAYqS0NDCP3BZlQ5K6IVJjP0jdB5plA7Nq1+wZrGXiFKQKBZoD88sdRjgJCVeYISn7jh0rL0VCUcxIVnETSWKEJ2hE+ppyFBLppbP0mXWqlaEVREI/rqyZ+nsjRaGU09DXk3lIuejl4n9eP1HBlZdSHieKcDw/FCTMUpGVV2ENqSBYsakmCAuqs1p4jATCShdW0SU4i19eJp3zmnNRq9/Vq43roo4yHMMJnIEDl9CAW2hCGzA8wjO8wpvxZLwY78bHfLRkFDuH8AfG5w9FQJXF</latexit>˜ ⇠<latexit sha1_base64="teIFfj7KlQS0a5XJNxiiKlI9GSU=">AAAB+XicbVDLSsNAFL2pr1pfUZduBovgqiQi6rLoxmUF+4AmhMl00g6dTMLMpFhC/8SNC0Xc+ifu/BsnbRbaemDgcM693DMnTDlT2nG+rcra+sbmVnW7trO7t39gHx51VJJJQtsk4YnshVhRzgRta6Y57aWS4jjktBuO7wq/O6FSsUQ86mlK/RgPBYsYwdpIgW17MdajMMq9JxbkZDYL7LrTcOZAq8QtSR1KtAL7yxskJIup0IRjpfquk2o/x1Izwums5mWKppiM8ZD2DRU4psrP58ln6MwoAxQl0jyh0Vz9vZHjWKlpHJrJIqda9grxP6+f6ejGz5lIM00FWRyKMo50gooa0IBJSjSfGoKJZCYrIiMsMdGmrJopwV3+8irpXDTcq8blw2W9eVvWUYUTOIVzcOEamnAPLWgDgQk8wyu8Wbn1Yr1bH4vRilXuHMMfWJ8/PYKUEQ==</latexit> c Algorithm1AltruisticGradientAdjustment(AgA)
1: Input: individual losses ℓ = [ℓ 1, ,ℓ n], collective
⇠<latexit sha1_base64="Hqi9ieXIgIzwk5pwEv5hWYXd7l8=">AAAB/XicbVDLSsNAFL2pr1pf8bFzEyyCq5JIUZdFNy4r9AVNKJPppB06mYSZiVhD8FfcuFDErf/hzr9x0mahrQcGDufcyz1z/JhRqWz72yitrK6tb5Q3K1vbO7t75v5BR0aJwKSNIxaJno8kYZSTtqKKkV4sCAp9Rrr+5Cb3u/dESBrxlprGxAvRiNOAYqS0NDCP3BZlQ5K6IVJjP0jdB5plA7Nq1+wZrGXiFKQKBZoD88sdRjgJCVeYISn7jh0rL0VCUcxIVnETSWKEJ2hE+ppyFBLppbP0mXWqlaEVREI/rqyZ+nsjRaGU09DXk3lIuejl4n9eP1HBlZdSHieKcDw/FCTMUpGVV2ENqSBYsakmCAuqs1p4jATCShdW0SU4i19eJp3zmnNRq9/Vq43roo4yHMMJnIEDl9CAW2hCGzA8wjO8wpvxZLwY78bHfLRkFDuH8AfG5w9FQJXF</latexit>˜ lossℓ ,parametersw = [w , ,· w·· ],magnitudeof
H<latexit sha1_base64="ZAGsdAyCAIwLFXb7jGnWelwtL5s=">AAAB/XicbVDLSsNAFL2pr1pf8bFzM1gEVyWRoi6LbrqsYGuhCWEynbRDJ5MwMxFqKP6KGxeKuPU/3Pk3TtsstPXAwOGce5h7T5hyprTjfFulldW19Y3yZmVre2d3z94/6Kgkk4S2ScIT2Q2xopwJ2tZMc9pNJcVxyOl9OLqZ+vcPVCqWiDs9Tqkf44FgESNYGymwj7wY62EY5c1JQJBnsgQ5gV11as4MaJm4BalCgVZgf3n9hGQxFZpwrFTPdVLt51hqRjidVLxM0RSTER7QnqECx1T5+Wz7CTo1Sh9FiTRPaDRTfydyHCs1jkMzOd1VLXpT8T+vl+noys+ZSDNNBZl/FGUc6QRNq0B9Zq7VfGwIJpKZXREZYomJNoVVTAnu4snLpHNecy9q9dt6tXFd1FGGYziBM3DhEhrQhBa0gcAjPMMrvFlP1ov1bn3MR0tWkTmEP7A+fwC61pTC</latexit> c 0 H<latexit sha1_base64="ujyRYHlkYoJnE8Qm5CAbZFVqH0s=">AAAB/3icbVDLSsNAFJ3UV62vqODGzWARXJVEirosuumygn1AE8LkdtIOnTycmQglduGvuHGhiFt/w51/46TNQlsPDBzOuZd75vgJZ1JZ1rdRWlldW98ob1a2tnd298z9g46MUwG0DTGPRc8nknIW0bZiitNeIigJfU67/vgm97sPVEgWR3dqklA3JMOIBQyI0pJnHjkhUSM/yJpTD7AjUwB6jy3PrFo1awa8TOyCVFGBlmd+OYMY0pBGCjiRsm9biXIzIhQDTqcVJ5U0ITAmQ9rXNCIhlW42yz/Fp1oZ4CAW+kUKz9TfGxkJpZyEvp7M08pFLxf/8/qpCq7cjEVJqmgE80NByrGKcV4GHjBBQfGJJgQE01kxjIggoHRlFV2CvfjlZdI5r9kXtfptvdq4Luooo2N0gs6QjS5RAzVRC7URoEf0jF7Rm/FkvBjvxsd8tGQUO4foD4zPH1/KlbA=</latexit> c⌫0 alignmc
entparameterλ
1 ··· n
Unstable Fixed Point Stable Fixed Point 2: ξ [grad(ℓ ,w )for(ℓ ,w ) (ℓ,w)]
i i i i
← ∈
3: ξ [grad(ℓ ,w )forw w]
Push out of UC na ss te a 1 b l: e Fixed Point Pull towardC Sa ts ae b 2 le: Fixed Point 54 :: λ∇c H← c λ←[g sir ga ndc ( (cid:0)21 1∥i ξ ξc
∥
,2,w ii )∈ (cid:0)fo ξr ,w i ∈w +]
2(cid:1)(cid:1)
Figure2.IllustrationofCorollary4.3.Incase1,withinanunstable ← × d⟨ c ∇Hc ⟩ ⟨ ∇Hc ⟩ ∥∇Hc ∥
6: Output: ξ˜=ξ +λ(ξ+ )
fixedpoint’sneighborhoods,anappropriateselectionoftheλsign c c
∇H
Plugintoanygradientdescentoptimizer
pushAgAtoevadetheunstablefixedpointandpulltowardsa
{ }
stablefixedpointinitsneighborhoodsasshownincase2.
Note that the Hessian matrix H is a symmetric matrix. trativevisualizationofCorollary4.3. Thefiguredepictstwo
c
Tosimplifythenotation, weuse todenotetheterm scenarios: theneighborhoodsofstableandunstablefixed
c
∇w(cid:0)1
2∥ξ
c
∥2(cid:1)
,i.e.,H cTξ c.
∇H
point(denotedbyastar),respectively. Intheneighborhood
ofanunstablefixedpointofthecollectiveobjective,asde-
Intuitively,AgAisdesignedtoinvolvebothcollectiveand
pictedinCase1,ourAaAgradientξ˜ispushedoutofthe
individual objectives, in order to harmonize the interests
region,resultinginafasterescapecomparedtotheoriginal
of individuals and the team. The alignment parameter λ
collectivegradientξ . Thisbehaviorisexemplifiedbythe
adjuststhebalancebetweentheindividualandthecollective c
greenarrow. Conversely,Case2illustratesthescenarioof
objective. Furthermore, we minimize = 1 ξ 2 to
Hc 2∥ c ∥ astablefixedpoint,whereinourAaAgradientξ˜ispulled
convergetoafixedpoint. Thisobjectiveisreflectedinthe
gradienttermHTξ inξ˜. AssumingthatH isinvertible, towardsthestableequilibrium,exemplifiedbytheredarrow.
c c c
wecandeducethat =HTξ =0,ifandonlyifξ = Implementation of MARL Algorithms with AgA. The
∇Hc c c c
0.Therefore,minimizing willguaranteetheconvergence concreteimplementationofaltruisticgradientadjustment
c
H
toafixedpoint(Letcheretal.,2019). iselaboratedinAlgorithm1. AgAcanbeeasilyincorpo-
ratedintoanycentralizedtrainingdecentralizedexecution
ImpactoftheSignofλ. WhilethemodifiedAgAgradient
(CTDE)frameworkofMARL.InatypicalCTDEframe-
introducesadditionalcomplexity,wetheoreticallydemon-
work,eachagent,denotedasi,strivestolearnapolicythat
stratethatanappropriatechoiceofthesignofλensuresthat
theAgAgradientξ˜isattractedtowardsastablefixedpoint. employs local observations to prompt a distribution over
personalactions. Theuniquecharacteristicduringthecen-
Incontrast,whendealingwithanunstablefixedpoint,AgA
tralized learning phase is that agents gain supplementary
ispushedawaythepoint.
informationfromtheirpeersthatisinaccessibleduringthe
Beforewedelveintothecorollaryofthesignselectionforλ, executionstage. Furthermore,acentralizedcriticoftenoper-
wefirstestablishsomefoundationaldefinitions. Theinner atesintandeminCTDEsettings,leveragingsharedinforma-
product of vectors a and b are denoted by a,b . If the tionandevaluatingindividualagentpolicies’potency. To
⟨ ⟩
HessianmatrixH isnon-negative-definite, ξ , 0 incorporateourproposedAgAalgorithmintoCTDEframe-
c
⟨ ∇H⟩≥
foranon-zeroξ . Analogously,ifH isnegative-definite, works,supplementaryrewardexchangesbetweenagentsare
c
ξ , <0foranon-zeroξ . Lastly,theanglebetween required,offeringtheessentialinputsforAlgorithm1. The
c c
⟨ ∇H⟩
thetwovectorsaandbisdenotedbyθ(a,b). individuallossesarecalculatedfromindividualrewardscon-
formingtoestablishedMARLalgorithmframework,such
ThefollowingcorollarystatesthatAgAwithasuitablesign
of λ will bring the adjusted gradient ξ˜point closer to a asIPPO(deWittetal.,2020)orMAPPO(Yuetal.,2022).
Here,thecollectivelossℓ iscalculatedaccordingtoindi-
stablefixedpointandescapefromunstablefixedpoints. c
vidualrewards;forexample,itcouldbeasumofindividual
Corollary 4.3. If we let the sign of λ satisfy λ rewardsoranelementarytransformationthereof. Further-
·
ξ c, c ( ξ, c + c 2) 0, the optimization more,theAgAalgorithmrequiresamagnitudevalueofthe
⟨ ∇H ⟩ ⟨ ∇H ⟩ ∥∇H ∥ ≥
processusingtheAgAmethodexhibitfollowingattributes.
alignmentparameterλ,whichshouldbeapositiveentity.
Primarily,inneighborhoodoffixedpointofcollectiveob-
Thesignofλisthencalculatedinline5ofAlgorithm1and
jective, if the point is stable (H
c
0), the AgA gradi-
isbasedoneachgradientcomponentcalculatedinlines2-4.
⪰
ent will be pulled toward this point, which means that
Subsequently,thealgorithmproducestheadjustedgradient
θ(ξ˜, c) θ(ξ c, c). Ontheotherhand,ifthepoint ξ˜.ξ˜isthendistributedacrosseachcorrespondingparameter,
∇H ≤ ∇H
represents unstable equilibria where H
c
0, the AgA
andoptimizationiscarriedoutusingasuitableoptimizer
≺
gradient will be pushed out of the point, indicating that
suchasAdamoptimizer(Kingma&Ba,2015).
θ(ξ˜, ) θ(ξ , ).
c c c
∇H ≥ ∇H
TheproofisprovidedinAppendixA.Fig.2presentsaillus-
5AligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
Algorithms
Metrics
Simul-Ind CGA SGA SVO Simul-Co AgA
r1 1.13 1.16 1.18 1.10 1.43 1.46
r2 1.18 1.15 1.14 1.06 1.38 1.45
pull pull SW 2.32 2.31 2.31 2.16 2.81 2.90
E 0.92 0.93 0.92 0.93 0.94 0.96
Table1.The comparison of the average individual rewards (de-
noted as r ,r ), social welfare (denoted as SW) and equality
1 2
(a)RewardContourofPlayer1 (b)RewardContourofPlayer2
metric(denotedasE).Eachvalueisthemeanof50randomruns.
Figure3.ThecomparisonbetweenAgA(showninred)andAgA
without sign alignment (AgA w/o Sign, in purple) trajectories Baselines and Evaluation Metrics. A comparison of
spans40steps,markedateverytenthstep. Normgradientsare
experiments involves specific baselines such as simulta-
representedwithbluearrows. Startingfromthe14thstep,sign
neous optimization with individual and collective losses
alignmentpulltheupdatedirection.Consequently,AgAisabout6
(abbreviatedasSimul-IndandSiml-Co),CGA(Mescheder
stepsaheadofAgAw/oSignatthetrajectory’send.
et al., 2017), SGA (Balduzzi et al., 2018) and SVO (Mc-
4.3.AlignmentEfficiencyofAgA:AToyExperiment Kee et al., 2020a). In the two-player public goods game
experiment,thesetechniquesaresimplyintegratedintothe
Toaddressthetwo-playerDMGasoutlinedinExample4.1,
basic gradient ascent algorithm, whereas, for more intri-
weincorporatetheAgAmethodintothefundamentalgra-
catesettingslikeCleanup,Harvest(Leiboetal.,2017),and
dientdescentalgorithm. ThefindingsrevealthattheAgA
SMAC(Samvelyanetal.,2019),thesebaselinesareinfused
method is effective in facilitating the alignment of objec-
withinvariousRLalgorithms. Detailedoperationsforim-
tives. The optimization trajectories in both the collective
plementing these RL algorithms are further elucidated in
rewardlandscapeandtheindividualplayerrewardcontour
theirrespectivesections.
arerepresentedinFig.1a,Fig.1b,andFig.1c,withbrighter
colors reflecting higher collective rewards. In contrast to We assess collaborative performance in mixed-motive
the objective misalignment exhibited by Simul-Co, AgA gamesusingathree-tierapproach. Primarily, weemploy
successfully aligns the agents’ interests, as evidenced in social welfare to gauge overall team performance. Next,
Fig.1bandFig1c. Fig.1bdepictsthetrajectoryofAgA an equality metric is introduced to scrutinize fairness in
meticulously carving its course along the summit of the environments such as Cleanup and Harvest. Lastly, the
individualrewardcontour. Furthermore,AgAisalsoshown probabilityofvictoryagainstoppositioninanSMACframe-
to improve the reward of Player 2 (as shown in Fig. 1c), workdeterminestheprobabilityoftheteam’ssuccess. So-
despite a convergence rate slower than that of Simul-Co. cialWelfareisacommonlyacceptedmetricforevaluating
Evidently, the trajectory of AgA (as depicted in Fig. 1b) the performance of mixed motivation cooperation, repre-
takes100steps,whereasSimul-Cotakes65steps. sentingthetotalepisodicrewardsearnedbyallagentsas
SW
=(cid:80)n
r . Socialwelfarelackstheabilitytoanalyze
Inaddition,Fig.3presentsacriticalcomparisonbetween i=1 i
teamworkinmixedcooperative-competitivecontexts. Here,
thetrajectoriesofAgA(inred)andAgAwithoutsignalign-
weusetheaverageWinRateagainstthebuilt-incomputer
ment (AgA w/o Sign, depicted in purple), as outlined in
AIbotinSMACtoassessalgorithmicperformance.Wealso
Corollary4.3. Thisside-by-sidecomparisoncovers40steps
involvetheEqualityduetothemeasureslikesocialwelfare
andfeatureshighlightedpointseverytenthstep. Itprovides
andwinratearelimitedtoageneralperformanceevaluation,
a visual illustration of the efficiency introduced by sign
ignoringindividualinterest. Therefore,theGinicoefficient,
alignmentstartingfromthe14thstepinthetrajectoriesof
originallyintendedasanincomeequalitymeasure(David,
AgAandAgAwithoutsignalignment. Remarkably,norm
1968), hasbeenmodifiedtoassessrewardequalityinco-
gradientsarerepresentedbybluearrows,indicatingthedi-
operativeAIsettings. ThisstudyutilizesanexpeditedGini
rectionofthefastestupdates.Asdepictedinthefigure,AgA
coefficientcalculationmethod,withanassociatedequality
withsignselectionisalignedtowardsthefastestupdatedi-
metric E, formulated as E := 1 G. Each individual’s
rection,resultinginAgAprogressingapproximately6steps −
rewards are arranged in ascending order to create a new
aheadofAgAwithoutSignattheendofthetrajectory.
payoff vector p, and the Gini coefficient is computed as
G= 2 (cid:80)n i(p p¯),followedbyE =1 G. Here,
5.Experiments n2p¯ i=1 i − −
p¯denotesthemeanoftherankedpayoffvectorpandnsig-
nifiesthetotalnumberofplayers. AlargerE valueimplies
Inthissection,weconductaseriesofexperimentstover-
greaterequality.
ify the AgA algorithm. First, we provide an overview of
theexperimentalsettingsandclarifytheevaluationmetrics
employed.
6AligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
150 150 800 800
700 700
100 100
600 600
50 50 500 500
400 400
0 AgA ( =0.1) 0 300 AgA ( =0.1) 300 S Si im mu ul l- -I Cn od
AgA ( =1) Simul-Ind CGA 200 AgA ( =1) 200 SVO
50 A Agg AA / S( ig= n100) 50 S Si Vm Oul-Co AgA ( =100) 100 A Agg AA / S( ig= n100) 100 C AG gAA ( =0.1)
100 100 0 0
0.000.250.500.751.001.251.501.752.00 0.000.250.500.751.001.251.501.752.00 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Total Steps (1e7) 1e7 Total Steps (1e7) 1e7 Total Steps (1e7) 1e7 Total Steps (1e7) 1e7
(a)Cleanup:AgAwithdifferent (b) Cleanup: comparison be- (c)Harvest:AgAwithdifferent (d) Cleanup: comparison be-
λandAgA/Sign tweenAgAandbaselines λandAgA/Sign tweenAgAandbaselines
Figure4.ComparisonofEpisodicSocialWelfare: ourproposedAgAmethodsalongwithbaselinesonCleanup(figureaandb)and
Harvest(figurecandd)tasks.Figure(a)and(c)provideanillustrationofvaryingalignmentparameterλvaluesinAgA,aswellasAgA
sanssignselection(AgA/Sign),asappliedtoCleanupandHarvesttasksrespectively.Subfigures(b)and(d)exhibitthesocialwelfare
obtainedbyAgAutilizingtheoptimalλvalue,incomparisontothosesecuredbySimul-Ind,Simal-Co,CGA,SGAandSVO.Boldlines
representtheaverageofthesocialwelfareacrossthreeseeds,withtheencompassingshadedareasdenotingthestandarddeviation.
5.1.Two-PlayerPublicGoodsGame Algorithms
Envs Metrics
Simul-Ind Simul-Co SVO CGA AgA
A two-player Public Goods matrix game is widely uti-
SW 24.86 72.02 27.30 44.23 83.90
G Cleanup
lizedtostudycooperationinsocialdilemmas. Thegame E 0.97 0.88 0.85 0.91 0.95
involvesplayers {1,2 }withparameters {w 1,w 2 }andpay- Harvest S EW 6 02 .3 9. 713 6 02 .5 9. 680 63 08 .9.9 75 6 06 .7 9. 837 68 03 .9.4 82
offs p ,p .Thesocialwelfare,denotedasSW =p +p .
1 2 1 2
{ }
Eachplayer,i.e.,i 1,2 ,contributesanamounta iwithin Table2.Thehighestsocialwelfareachievedthroughouttraining
∈{ }
abudgetedrange[0,b],andthehostevenlydistributesthese alongwiththecorrespondingequalitymetric.Theforemostrow
contributions as c(a + a ), where 1 < c 2. Con- foreachenvironmentreflectsthebestsocialwelfare, whilethe
2 1 2 ≤
sequently, each player’s payoff p (a ,a ) is estimated as secondrowindicatestheassociatedequalitymetric.
i 1 2
b a +c(a +a ).Inourexperiments,wesetthebudgetb
− i 2 1 2 negligencetowardstheescalatingpollutionlevelssurpasses
to1andweightcto1.5,withtheNashequilibriumofgame
a threshold, apple growth ceases, hence leading to a con-
at(0,0).
flictofinterestbetweenobtainingindividualrewardsand
Results. Table1comparesaverindividualrewards,social safeguardingjointwelfare. Harvest’spremise,ontheother
welfare,andtheequalitymetricsourcedfrom50random hand,involvesagentsreceivingrewardsbygatheringapples,
runs, eachcappedat100steps. Therowsr ,r andSW wherebyoptimalappleregrowthisdependentonthepres-
1 2
representtherewardsaccruedbyeachindividualplayerand enceofadjacentapples.Acollectivedilemmaappearswhen
thecollectivegrouprespectively. RowE denotestheequal- rampantharvestingreducestheappleregenerationrate,thus
itymetric. ThetablerevealsthatAgAsurpassesbenchmark decrementingoverallteamrewards. Simul-Ind,Simul-Co,
methodsinindividualrewardsandsocialwelfare,andequal- CGA,andSVOarebenchmarkedagainstourAgAmethod-
itymetric. Thesocialoptimumofthegameisatcoordinates ologyinourexperiments. TheimplementationoftheSimul-
(1,1),whichcorrespondstoamaximumachievablesocial IndalgorithmisbasedontheIPPOalgorithm(deWittetal.,
welfareof3. Amongallthealgorithms,AgAachievesthe 2020),withallotherstrategiesleveragingthesharedparam-
closestsocialwelfarescoretothisoptimum,withavalue etersoftheIPPOalgorithmaswell. Thejointrewardfor-
of 2.90. In addition, AgA demonstrates the highest level mulausedforthecalculationofcollectivelossforSimul-Co,
of equality, with a score of 0.96, when compared to the CGA,andAgAsisdesignedtopromotebothimprovedcol-
baseline algorithms. Additionally, Fig. 6 in Appendix B lectiveperformanceandequitablebehavioramongagents,
(cid:16) (cid:16) (cid:17)(cid:17)
visualizestheactiondistributionsofthesemethods,demon- i.e,(cid:80) r α(1 actan sumj,j̸=irj withconstantα.
i i − − ri
strating convergence points, further supporting that AgA
Moredetailedinformationontheseenvironmentsandalgo-
convergesmoreefficientlytowardsthesocialoptimum.
rithmexecutioncanbefoundinAppendixC.1.
Results. Fig.4aandFig.4cpresenttheaverageepisodic
5.2.SequentialSocialDilemma: CleanupandHarvest
social welfare comparison of the AgA algorithm and
Weconductexperimentsinsequentialsocialdilemmaen- AgA/Sign, excluding the sign selection (Corollary 4.3).
vironmentswithagroupsizeoffivenamely,Harvestand Thesefiguresalsohighlighttheeffectofvaryingthealign-
Cleanup(Hughesetal.,2018). InCleanup,theagentspri- mentparameterλ. TheprescribedλvaluesforAgA/Signis
marilyderiverewardsfromharvestingapplesinanearby 100forCleanupand0.1forHarvest,resultinginthebestper-
orchard,whichiscontingentuponriverpollutionlevels. If formanceforeach.FailingtoselectsignsinAgA/Signleads
7
erafleW
laicoS
erafleW
laicoS
erafleW
laicoS
erafleW
laicoSAligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
1.0 1.0 rithm(deWittetal.,2020)forSimul-Ind,whileemploying
AgA ( =0.1) Simul-Ind
0.8 AgA ( =1) 0.8 Simul-Co theMAPPOframework(Yuetal.,2022)forSimul-Co,SVO,
AgA ( =100) SVO
0.6 AgA/Sign 0.6 CGA CGA,andAgA.MoredetailscanbefoundinAppendixC.2.
AgA ( =100)
0.4 0.4 Results. Fig.5acomparesaveragewinratesforvariousλ
0.2 0.2 parametersinAgA,alsoencompassingAgAwithoutsign
0.0 0.0 selection, as stated in Corollary 4.3. The results suggest
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Total Steps (1e7) 1e7 Total Steps (1e7) 1e7 comparableperformanceforλ = 1and100,withalower
(a)MMM2: AgAwithvarious (b) MMM2: comparison be- valueof0.1trailingnoticeably. Notably,AgAwithλ=0.1
λandAgA/Sign tweenAgAandbaselines
underperforms,evenwhencontrastedwithAgA’sablation
study. Fig.5bbenchmarksthewinrateforAgA(λ=100)
Figure5.The comparison of win rates on the mixed-motive
againstSimul-Ind,Simul-Co,CGA,andSVO.Withinthis
MMM2 map of SMAC. The left figure shows results of vary-
context,Simul-Ind,implementedviaIPPO,struggleswith
ingalignmentparameterλvaluesinAgA,aswellasAgAsans
themixed-motiveMMM2task.Impressively,ourAgA(λ=
signselection(AgA/Sign),Therightfigurecomparethewinrate
100) method achieves a win rate peak of 0.85 (standard
ofAgA(λ = 100)withbaselines. Theboldlinesillustratethe
meanofthewinrateover3seeds,whiletheencompassingshaded
deviation of 0.09), significantly outperforming the SVO
regionsdepictthestandarddeviation. methodthatonlynets44.55.
topoorercollectiveresults. Fig.4bandFig.4dcomparethe
6.Conclusion
socialwelfareachievedbyAgA,withtheoptimalλvalue,
withbaselinemethods. Here,AgA(λ = 100)inCleanup
In this paper, we propose AgA, an gradient adjustment
andAgA(λ=0.1)inHarvestsurpassthebenchmarkmeth-
method,specificallydesignedtoalignindividualandcollec-
odsforthemajorityofthetrainingprocess. Table2presents
tiveobjectiveswithinmixed-motivecooperationscenarios.
thepeaksocialwelfareachievedduringtrainingalongside
Tomaterializethis,wefirstmodelthemixed-motivegame
theequalitymetric. Forsimplicity,theAgAcolumndenotes
asadifferentiablegame,therebyfacilitatingtheanalysisof
the respective optimal λ for each scenario. In Cleanup,
learningdynamics. Thereafter,weanalyzetheoptimization
AgAsurpassesthesecond-bestmethod,Simul-Co,byover
pathsinatoydifferentiablemixed-motivegame. Wereveal
10 points. However, although Simul-Ind shows superior
thatoptimizationtechniquesthatheavilyrelyoncollective
equalityperformance,itdeliversthelowestsocialwelfare.
losssufferfrominherentlimitations. Theseapproachesof-
Contrarily, AgA achieves a satisfactory balance between
tenoverlookindividualpreferences,andexistinggradient
equityandteamperformance. Analogousconclusionsapply
adjustmentmethodsareinadequateforaddressingmixed-
totheHarvest,whereAgAoutperformsbaselinesinsocial
motivecooperationchallenges.OurAgAstrategyefficiently
welfareandequality.
bridges the gap between individual and collective objec-
tives. Theoptimizationpathsfurthersupportourtheoretical
5.3.Mixed-motiveSMAC
claimsbydemonstratingthatcarefulselectionofthesign
ofthealignmentweightenablesconvergencetowardsstable
Despitetheirwidespreaduseinthestudyofmixed-motive
fixedpointswhileescapingunstableones. Comprehensive
games,sequentialsocialdilemmagamescanberelatively
experimentsconductedonmixed-motivetestbedsandmod-
simplisticcomparedtoothertestbedsinthefieldofMARL,
ifiedSMACenvironmentsprovidefurtherevidenceofthe
particularlywhenconsideringreal-worldscenarios. Coop-
superiorperformanceofAgA.Itconsistentlyoutperforms
erativeMulti-AgentReinforcementLearningoftenadopts
existingbaselinesacrossmultipleevaluationmetrics.
more intricate environments such as SMAC (Samvelyan
etal.,2019). Inthisgameenvironment,agentsmustcollab- LimitationsandFutureWork.Theexperimentsconducted
oratetodefeattheiropponents. WemodifytheMMM2map inthisstudywereconfinedtoasinglemapoftheSMAC
fromSMACasamixed-motivetestbed. Here,agentscom- environment. Ourfutureobjectivesincludeinvestigatingthe
mand various game entities confronting opposing forces. mixed-motivecooperativemechanisminreal-worldscenar-
Unlike the original StarCraft II game’s team-oriented ar- iosanddevelopingamorecomplexmixed-motivetestbed.
chitecture, we instill individual interests in each agent In this paper, while we strive to align the objectives of
to enhance mixed-motive characteristics. Reward mech- bothindividualsandthecollective,ourprimaryobjective
anismshavebeenmodifiedsuchthatanagent’sindividual is to converge towards a stable equilibrium of the collec-
rewardalignswiththedamageitinflictsontheenemy,re- tive objective. For future research, we intend to broaden
placingthestandardpracticeofdistributingtotaldamage ourcomprehensionofhowindividualobjectivesinteractin
acrossallagents. Penaltiesforagentremovalalsoreinforce mixed-motivegames,placingagreateremphasisonunder-
self-preservation, accentuating inherent agent selfishness. standingtheirdynamicsinmixed-motivecooperation.
The experimental implementation utilizes the IPPO algo-
8
etaR
niW
etaR
niWAligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
ImpactStatements David, H. A. Gini’s mean difference rediscovered.
Biometrika,55(3):573–575,1968. ISSN00063444. URL
Thispaperpresentsanovelapproachinthefieldofmulti-
http://www.jstor.org/stable/2334264.
agent learning, aimed at advancing collaborations with
mixedmotivations. Theproposedframeworkaddressesa deWitt,C.S.,Gupta,T.,Makoviichuk,D.,Makoviychuk,
fundamentalchallengeinmachinelearning,whereinagents V., Torr, P. H. S., Sun, M., and Whiteson, S. Is in-
withdiverseobjectivesarerequiredtocooperatetoachieve dependent learning all you need in the starcraft multi-
collective goals. The work described in this paper holds agent challenge? CoRR, abs/2011.09533, 2020. URL
significant potential for various societal applications. By https://arxiv.org/abs/2011.09533.
enablingeffectivecooperationamongagentswithdifferent
Du,Y.,Leibo,J.Z.,Islam,U.,Willis,R.,andSunehag,P.
motivations,ourapproachcanbeleveragedindomainssuch
Areviewofcooperationinmulti-agentlearning,2023.
asgames,autonomoussystems,smartcities,anddecentral-
izedresourceallocation. Suchadvancementshavenumer-
Foerster,J.N.,Chen,R.Y.,Al-Shedivat,M.,Whiteson,S.,
oussocietalconsequences,includingimprovedefficiency
Abbeel, P., and Mordatch, I. Learning with opponent-
intransportationsystems,optimizedresourceallocationin
learningawareness,2018.
energy grids, and enhanced decision-making in complex
multi-agentenvironments. Furthermore,ourfindingscon- Gidel,G.,Berard,H.,Vignoud,G.,Vincent,P.,andLacoste-
tributetothebroaderfieldofmachinelearningbyoffering Julien,S. Avariationalinequalityperspectiveongenera-
insightsintomanagingmixed-motivationscenarios. Con- tiveadversarialnetworks,2020.
sequently,ourworkpavesthewayforfutureresearchand
Hostallero,D.E.,Kim,D.,Moon,S.,Son,K.,Kang,W.J.,
innovation,drivingprogressinmulti-agentlearningandits
andYi,Y.Inducingcooperationthroughrewardreshaping
applicationsacrossvariousdomains.
basedonpeerevaluationsindeepmulti-agentreinforce-
mentlearning. InSeghrouchni,A.E.F.,Sukthankar,G.,
References
An,B.,andYorke-Smith,N.(eds.),Proceedingsofthe
Anastassacos, N., Garc´ıa, J., Hailes, S., and Musolesi, 19thInternationalConferenceonAutonomousAgentsand
M. Cooperationandreputationdynamicswithreinforce- MultiagentSystems,AAMAS’20,Auckland,NewZealand,
ment learning. In Dignum, F., Lomuscio, A., Endriss, May9-13,2020,pp.520–528.InternationalFoundation
U., and Nowe´, A. (eds.), AAMAS ’21: 20th Interna- forAutonomousAgentsandMultiagentSystems,2020.
tional Conference on Autonomous Agents and Multia- doi: 10.5555/3398761.3398825. URL https://dl.
gentSystems,VirtualEvent,UnitedKingdom,May3-7, acm.org/doi/10.5555/3398761.3398825.
2021,pp.115–123.ACM,2021. doi: 10.5555/3463952.
Hughes, E., Leibo,J.Z., Phillips, M., Tuyls, K., Due´n˜ez-
3463972. URL https://www.ifaamas.org/
Guzma´n, E. A., Castan˜eda, A. G., Dunning, I., Zhu,
Proceedings/aamas2021/pdfs/p115.pdf.
T., McKee, K.R., Koster, R., Roff, H., andGraepel, T.
Inequityaversionimprovescooperationinintertemporal
Apt, K. R. and Scha¨fer, G. Selfishness level of strategic
social dilemmas. In Bengio, S., Wallach, H. M.,
games. J.Artif.Int.Res.,49(1):207–240,jan2014. ISSN
Larochelle, H., Grauman, K., Cesa-Bianchi, N., and
1076-9757.
Garnett, R. (eds.), Advances in Neural Information
Processing Systems 31: Annual Conference on Neural
Balduzzi,D.,Racaniere,S.,Martens,J.,Foerster,J.,Tuyls,
Information Processing Systems 2018, NeurIPS 2018,
K.,andGraepel,T. Themechanicsofn-playerdifferen-
December 3-8, 2018, Montre´al, Canada, pp. 3330–
tiablegames. InInternationalConferenceonMachine
3340, 2018. URL https://proceedings.
Learning,pp.354–363.PMLR,2018.
neurips.cc/paper/2018/hash/
7fea637fd6d02b8f0adf6f7dc36aed93-Abstract.
Chen,X.,Vadori,N.,Chen,T.,andWang,Z. Learningto
html.
optimizedifferentiablegames. InKrause,A.,Brunskill,
E., Cho, K., Engelhardt, B., Sabato, S., andScarlett, J.
Hughes, E., Anthony, T.W., Eccles, T., Leibo, J.Z., Bal-
(eds.),Proceedingsofthe40thInternationalConference
duzzi, D., and Bachrach, Y. Learning to resolve al-
on Machine Learning, volume 202 of Proceedings of
liancedilemmasinmany-playerzero-sumgames. CoRR,
MachineLearningResearch,pp.5036–5051.PMLR,23–
abs/2003.00799,2020. URLhttps://arxiv.org/
29 Jul 2023. URL https://proceedings.mlr. abs/2003.00799.
press/v202/chen23ab.html.
Isaacs, R. Differential Games: A Mathematical Theory
Daskalakis,C.,Ilyas,A.,Syrgkanis,V.,andZeng,H. Train- withApplicationstoWarfareandPursuit, Controland
ingganswithoptimism,2018. Optimization. Dover books on mathematics. Wiley,
9AligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
1965. ISBN9780471428602. URLhttps://books. andYorke-Smith,N.(eds.),Proceedingsofthe19thInter-
google.co.uk/books?id=gtlQAAAAMAAJ. nationalConferenceonAutonomousAgentsandMultia-
gentSystems,AAMAS’20,Auckland,NewZealand,May
Jaques,N.,Lazaridou,A.,Hughes,E.,Gulcehre,C.,Ortega,
9-13, 2020, pp. 869–877. International Foundation for
P., Strouse, D., Leibo, J.Z., andDeFreitas, N. Social
AutonomousAgentsandMultiagentSystems,2020a. doi:
influence as intrinsic motivation for multi-agent deep
10.5555/3398761.3398863. URLhttps://dl.acm.
reinforcementlearning. InInternationalconferenceon
org/doi/10.5555/3398761.3398863.
machinelearning,pp.3040–3049.PMLR,2019.
McKee,K.R.,Gemp,I.,McWilliams,B.,Due´n˜ez-Guzma´n,
Kingma, D. P. and Ba, J. Adam: A method for stochas-
E. A., Hughes, E., and Leibo, J. Z. Social diversity
tic optimization. In Bengio, Y. and LeCun, Y. (eds.),
and social preferences in mixed-motive reinforcement
3rd International Conference on Learning Represen-
learning,2020b.
tations, ICLR 2015, San Diego, CA, USA, May 7-9,
2015,ConferenceTrackProceedings,2015. URLhttp: McKee, K. R., Hughes, E., Zhu, T. O., Chadwick, M. J.,
//arxiv.org/abs/1412.6980. Koster, R., Castaneda, A. G., Beattie, C., Graepel, T.,
Botvinick,M.,andLeibo,J.Z. Amulti-agentreinforce-
Leibo, J. Z., Zambaldi, V. F., Lanctot, M., Marecki, J.,
ment learning model of reputation and cooperation in
and Graepel, T. Multi-agent reinforcement learning in
humangroups,2023.
sequentialsocialdilemmas.CoRR,abs/1702.03037,2017.
URLhttp://arxiv.org/abs/1702.03037. Mescheder, L. M., Nowozin, S., and Geiger, A. The
numerics of gans. In Guyon, I., von Luxburg, U.,
Letcher, A., Balduzzi, D., Racanie`re, S., Martens, J., Fo-
Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan,
erster, J., Tuyls, K., and Graepel, T. Differentiable
S. V. N., and Garnett, R. (eds.), Advances in Neural
game mechanics. Journal of Machine Learning Re-
InformationProcessingSystems30: AnnualConference
search,20(84):1–40,2019.URLhttp://jmlr.org/
on Neural Information Processing Systems 2017,
papers/v20/19-008.html.
December4-9,2017,LongBeach,CA,USA,pp.1825–
1835, 2017. URL https://proceedings.
Letcher,A.,Foerster,J.,Balduzzi,D.,Rockta¨schel,T.,and
neurips.cc/paper/2017/hash/
Whiteson,S. Stableopponentshapingindifferentiable
4588e674d3f0faf985047d4c3f13ed0d-Abstract.
games,2021.
html.
Li,Y.,Zhang,S.,Sun,J.,Zhang,W.,Du,Y.,Wen,Y.,Wang,
Peysakhovich,A.andLerer,A. Prosociallearningagents
X.,andPan,W. Tacklingcooperativeincompatibilityfor
solvegeneralizedstaghuntsbetterthanselfishones. In
zero-shothuman-aicoordination,2024.
Andre´, E., Koenig, S., Dastani, M., and Sukthankar,
Lu,C.,Willi,T.,deWitt,C.A.S.,andFoerster,J.N.Model- G. (eds.), Proceedings of the 17th International Con-
free opponent shaping. In Chaudhuri, K., Jegelka, S., ference on Autonomous Agents and MultiAgent Sys-
Song,L.,Szepesva´ri,C.,Niu,G.,andSabato,S.(eds.), tems, AAMAS 2018, Stockholm, Sweden, July 10-15,
InternationalConferenceonMachineLearning,ICML 2018,pp.2043–2044.InternationalFoundationforAu-
2022,17-23July2022,Baltimore,Maryland,USA,vol- tonomousAgentsandMultiagentSystemsRichland,SC,
ume162ofProceedingsofMachineLearningResearch, USA / ACM, 2018. URL http://dl.acm.org/
pp. 14398–14411. PMLR, 2022. URL https:// citation.cfm?id=3238065.
proceedings.mlr.press/v162/lu22d.html.
PhilipS.Gallo,J.andMcClintock,C.G. Cooperativeand
Lupu,A.andPrecup,D. Giftinginmulti-agentreinforce- competitivebehaviorinmixed-motivegames. Journal
mentlearning. InProceedingsofthe19thInternational ofConflictResolution,9(1):68–78,1965. doi: 10.1177/
Conference on autonomous agents and multiagent sys- 002200276500900106.URLhttps://doi.org/10.
tems,pp.789–797,2020. 1177/002200276500900106.
Macy,D.andFlache,A. Learningdynamicsinsocialdilem- Raffin,A.,Hill,A.,Gleave,A.,Kanervisto,A.,Ernestus,
mas. ProceedingsoftheNationalAcademyofSciences M.,andDormann,N. Stable-baselines3: Reliablerein-
oftheUnitedStatesofAmerica,99Suppl3:7229–36,06 forcementlearningimplementations. JournalofMachine
2002. doi: 10.1073/pnas.092080099. Learning Research, 22(268):1–8, 2021. URL http:
//jmlr.org/papers/v22/20-1364.html.
McKee,K.R.,Gemp,I.,McWilliams,B.,Due´n˜ez-Guzma´n,
E.A.,Hughes,E.,andLeibo,J.Z. Socialdiversityand Rapoport, A. Prisoner’s dilemma—recollections and ob-
socialpreferencesinmixed-motivereinforcementlearn- servations. In Game Theory as a Theory of a Conflict
ing. In Seghrouchni, A. E. F., Sukthankar, G., An, B., Resolution,pp.17–34.Springer,1974.
10AligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
Samvelyan, M., Rashid, T., de Witt, C. S., Farquhar, Zhong, Y., Kuba, J. G., Hu, S., Ji, J., and Yang, Y.
G., Nardelli, N., Rudner, T. G. J., Hung, C., Torr, P. Heterogeneous-agentreinforcementlearning,2023.
H. S., Foerster, J. N., and Whiteson, S. The starcraft
multi-agent challenge. In Elkind, E., Veloso, M., Ag-
mon, N., and Taylor, M. E. (eds.), Proceedings of the
18th International Conference on Autonomous Agents
and MultiAgent Systems, AAMAS ’19, Montreal, QC,
Canada, May 13-17, 2019, pp. 2186–2188. Interna-
tional Foundation for Autonomous Agents and Multi-
agent Systems, 2019. URL http://dl.acm.org/
citation.cfm?id=3332052.
Vinitsky, E., Jaques, N., Leibo, J., Castenada, A., and
Hughes, E. An open source implementation of se-
quential social dilemma games. https://github.
com/eugenevinitsky/sequential_social_
dilemma_games/issues/182, 2019. GitHub
repository.
Vinitsky, E., Ko¨ster, R., Agapiou, J.P., Due´n˜ez-Guzma´n,
E. A., Vezhnevets, A. S., and Leibo, J. Z. Eu-
gene2023collectiveized multi-agent settings. Collec-
tiveIntelligence,2(2):26339137231162025,2023. doi:
10.1177/26339137231162025. URL https://doi.
org/10.1177/26339137231162025.
Wang,J.,Zhang,Y.,Kim,T.,andGu,Y. Shapleyq-value:
Alocalrewardapproachtosolveglobalrewardgames.
InTheThirty-FourthAAAIConferenceonArtificialIntel-
ligence,AAAI2020,TheThirty-SecondInnovativeAppli-
cationsofArtificialIntelligenceConference,IAAI2020,
The Tenth AAAI Symposium on Educational Advances
in Artificial Intelligence, EAAI 2020, New York, NY,
USA,February7-12,2020,pp.7285–7292.AAAIPress,
2020. doi: 10.1609/AAAI.V34I05.6220. URLhttps:
//doi.org/10.1609/aaai.v34i05.6220.
Willi, T., Letcher, A. H., Treutlein, J., and Foerster,
J. COLA: Consistent learning with opponent-learning
awareness. In Chaudhuri, K., Jegelka, S., Song, L.,
Szepesvari, C., Niu, G., and Sabato, S. (eds.), Pro-
ceedings of the 39th International Conference on Ma-
chineLearning,volume162ofProceedingsofMachine
LearningResearch,pp.23804–23831.PMLR,17–23Jul
2022.URLhttps://proceedings.mlr.press/
v162/willi22a.html.
Yang, J., Li, A., Farajtabar, M., Sunehag, P., Hughes, E.,
andZha,H. Learningtoincentivizeotherlearningagents.
AdvancesinNeuralInformationProcessingSystems,33:
15208–15219,2020.
Yu, C., Velu, A., Vinitsky, E., Gao, J., Wang, Y., Bayen,
A.M.,andWu,Y. ThesurprisingeffectivenessofPPO
incooperativemulti-agentgames. InNeurIPS,2022.
11AligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
A.ProofofCorollary4.3
Corollary4.3. Ifweletthesignofλsatisfyλ ξ , ( ξ, + 2) 0,theoptimizationprocessusingthe
c c c c
·⟨ ∇H ⟩ ⟨ ∇H ⟩ ∥∇H ∥ ≥
AgAmethodexhibitfollowingattributes. Primarily,inneighborhoodoffixedpointofcollectiveobjective,ifthepointis
stable(H 0),theAgAgradientwillbepulledtowardthispoint,whichmeansthatθ(ξ˜, ) θ(ξ , ). Onthe
c c c c
⪰ ∇H ≤ ∇H
otherhand,ifthepointrepresentsunstableequilibriawhereH 0,theAgAgradientwillbepushedoutofthepoint,
c
indicatingthatθ(ξ˜, ) θ(ξ , ). ≺
c c c
∇H ≥ ∇H
Proof. Priortoembarkingontheelucidationofthecorollary’sproof,itisessentialtofirstintroducerelevantnotationsand
thefoundationalconcepts. Takenoteofthenotationθ(a,b),whichsymbolizestheangularmeasurebetweentwovectors.
Furthermore,wewriteθ (ξ˜,w)torepresenttheangularmeasurebetweenAgAgradientξ˜andareferenceupdatedirection
λ
w. Forsimplicity,weuseξ˜=u+λvtodenotetheAgAgradientξ˜=ξ +λ(cid:0) ξ+HTξ (cid:1) ,asdefinedinEq.2. Specifically,
c c c
(cid:0) (cid:1)
udenotescollectivegradientξ andvdenotes ξ+HTξ .
c c c
Wethenextendthedefinitionofinfinitesimalalignment(Balduzzietal.,2018)toourproposedAgAgradient. Givenathird
referencevectorw,theinfinitesimalalignmentforAgAgradientξ˜withwisdefinedas
d
align(ξ˜,w):= cos2θ . (3)
dλ{
λ }|λ=0
Intuitively,wemaypresumethatvectorsuandwaredirectionallyaligned,giventhatuTw >0. Inthisscenario,align>0
impliesthatvectorvisdrawinguclosertowardsthereferencevectorw. Conversely,vectorvispropellinguawayfrom
w. Similarargumentsholdforalign<0: vpushesuawayfromthereferencevectorwwhenuandwsharethesame
directionalorientation. Conversely,vpullsutowardsthereferencevectorwwhenthevectorsuandwarenotorientedin
thesamedirection.
Theensuinglemmaprovidesasimplifiedmethodfordeterminingthesignofalign,circumventingtheneedforgenerating
anexactsolution.
LemmaA.1(Signofalign.). GivenAgAgradientξ˜andreferencevector ,thesignofinfinitesimalalignmentalign
c
∇H
couldbecalculatedby
sign(cid:16) align(ξ˜, )(cid:17) =sign(cid:0) ξ, ( ξ , + 2)(cid:1) .
c c c c c
∇H ⟨ ∇H ⟩ ⟨ ∇H ⟩ ∥∇H ∥
Proof. Bythedefinitionofθ ,wecoulddirectlyget:
λ
cos2θ (4a)
λ
(cid:32) (cid:33)2
ξ˜,
= ⟨ ∇Hc ⟩ (4b)
ξ˜ 2 2
c
∥ ∥ ·∥∇H ∥
(cid:18)
ξ +λ(ξ+ ),
(cid:19)2
= ⟨ c ∇Hc ∇Hc ⟩ (4c)
ξ˜ 2 2
c
∥ ∥ ·∥∇H ∥
ξ , 2+2λ ξ , ξ+ , + (λ2)
=⟨ c ∇Hc ⟩ ⟨ c ∇Hc ⟩⟨ ∇Hc ∇Hc ⟩ O (4d)
(cid:16) (cid:17)2
ξ˜ 2 2
c
∥ ∥ ·∥∇H ∥
ξ , 2+2λ ξ, ξ , +2λ 2 ξ , + (λ2)
=⟨ c ∇Hc ⟩ ⟨ ∇Hc ⟩⟨ c ∇Hc ⟩ ∥∇Hc ∥ ⟨ c ∇Hc ⟩ O (4e)
(cid:16) (cid:17)2
ξ˜ 2 2
c
∥ ∥ ·∥∇H ∥
ξ , 2+2λ ξ , ( 2+ ξ, )+ (λ2)
=⟨ c ∇Hc ⟩ ⟨ c ∇Hc ⟩ ∥∇Hc ∥ ⟨ ∇Hc ⟩ O . (4f)
(cid:16) (cid:17)2
ξ˜ 2 2
c
∥ ∥ ·∥∇H ∥
Inaccordancewiththeestablisheddefinitionofinfinitesimalalignment,itbecomesfeasibletofurthercomputethesignof
12AligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
alignbytakingthederivativeofλwithrespecttocos2θ :
λ
(cid:16) (cid:17)
sign align(ξ˜, ) (5a)
c
∇H
(cid:18) (cid:19)
d
=sign cos2θ (5b)
λ
dλ
=sign(cid:0)
ξ ,
(cid:0)
ξ, +
2(cid:1)(cid:1)
. (5c)
c c c c
⟨ ∇H ⟩ ⟨ ∇H ⟩ ∥∇H ∥
LemmaA.1empowersustocalculatethesignofinfinitesimalalignmentrelyingoncomponentsthatarereadilycomputable.
Letusobserve ξ , =ξTH ξ forsymmetricHessianmatrixH . Undertheassumptionofξ =0,wecouldderive:
⟨ c ∇Hc ⟩ c c c c c ̸
(cid:40)
if H 0,then ξ , 0;
c c c
⪰ ⟨ ∇H ⟩≥ (6)
if H 0,then ξ , <0.
c c c
≺ ⟨ ∇H ⟩
AsdemonstratedbyEq.6,whensituatedintheneighborhoodofastablefixedpoint(thatistosay,H 0),weobserve
c
⪰
ξ , 0. Inconversecircumstances, ξ , < 0occurs. Followingthis, weshallproceedtodelveintotwo
c c c c
⟨ ∇H ⟩ ≥ ⟨ ∇H ⟩
specificscenarios: stabilityandinstabilityofthefixedpoint.
1)Case1:Inaneighborhoodofastablefixedpoint. Ifweareinaneighborhoodofastablefixedpointthen ξ , 0.
c c
⟨ ∇H ⟩≥
Thisindicatesthatbothvectorsξ and pointinthesamedirection,i.e.,θ(ξ , ) π/2. ReferringtoLemmaA.1,
c c c c
the sign of align(ξ˜, ) is the same∇ aH s the sign of ξ, + 2 if ξ∇ ,H ≤ 0. Consequently, we have
c c c c c
∇H ⟨ ∇H ⟩ ∥∇H ∥ ⟨ ∇H ⟩ ≥
sign(λ)=sign( ξ, + 2)=sign(align)matchingtheclaimofourcorollary.
c c
⟨ ∇H ⟩ ∥∇H ∥
Letusnowconjecturethescenariosofsign(align),namely,sign(align) 0orsign(align)<0.Whensign(align) 0,
≥ ≥
itfollowsthatsign(λ) 0. Consequently,vectorv willdrawutowardsw. Subsequently,whensign(align) < 0,we
≥
observe that sign(λ) < 0. A negative λ reverses the mechanism such that v pushes u away from w, aligning with
the discussion following Eq. 3 that assumes a positive λ. Thus, regardless of the sign of align, if we set sign(λ) =
sign( ξ, + 2),vectorvensuresuisdrawntowardsw.
c c
⟨ ∇H ⟩ ∥∇H ∥
Fromnow,weprovethatintheneighborhoodofastablefixedpoint,ifweletsign(λ)satisfyλ ξ , ( ξ, +
c c c
2) 0,theAgAgradientξ˜willbepulledmorecloserto thanξ . ·⟨ ∇H ⟩ ⟨ ∇H ⟩
c c c
∥∇H ∥ ≥ ∇H
2)Case2: Inaneighborhoodofaunstablefixedpoint. Theproofingapproachfortheunstablecaseexhibitssimilarityto
Case1.
Therefore, we prove that if we let the sign of λ follows the condition λ ξ , ( ξ, + 2) 0, the
c c c c
·⟨ ∇H ⟩ ⟨ ∇H ⟩ ∥∇H ∥ ≥
optimization process using the AgA method exhibit following attributes. In areas close to fixed points, 1) if the point
isstable,theAgAgradientwillbepulledtowardthispoint,whichmeansthatθ(ξ˜, ) θ(ξ , );2)ifthepoint
c c c
representsunstableequilibria,theAgAgradientwillbepushedoutofthepoint,indic∇ atH ingt≤ hatθ(ξ˜,∇H
) θ(ξ , ).
c c c
∇H ≥ ∇H
AnillustrativeexampleofthecorollaryisprovidedinFig.2.
B.AdditionalExperimentResults
B.1.ResultVisualizationofTwo-playerPublicGoodsGame
Fig.6depictstheactionsproducedbyvariousmethodsinatwo-playerpublicgame. Theactiongeneratedbyeachmethod
isrepresentedbyacircle,whichiscolor-codedaccordingtothecorrespondingmethod. Toensurefairness,eachmethod’s
updates are constrained to a maximum of 100 steps. These methods include the individual loss-driven simultaneous
optimization(Simul-Ind),twovariantsofgradientadjustmentmethodstermedasCGAandSGA,andthesimultaneous
optimizationleveragingcollectiveloss(Simul-Co)alongsideourproposedAgA,whereλ=1. Eachofthesemethodsis
fundamentallyunderpinnedbythegradientascentalgorithm. Theillustrationconsolidatesdatafrom50randomizedruns
initializedfromdiversestartingpoints. The‘X’demarcatedcirclesindicatetheaverageactionsmappedtoeachmethod.
Fromthefigure,it’sapparentthatSimul-Ind,CGA,SGA,SVOtendtoconvergetowardsscenarioswhereatleastoneplayer
13AligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
(1,1) Social Optimum
Figure6.Thescatterofactionsinatwo-playerpublicgoodsgameachievedthroughdifferentoptimizationmethods.Eachcirclerepresents
thepositionattainedwithinamaximumof100steps,withthecolorindicatingthecorrespondingmethod.The’X’markrepresentsthe
meanactionsof50randomruns.WiththeexceptionofSimul-Co,thebaselinemethodsconvergetowardstheNashequilibrium(0,0).
Notably,whilebothAgAandSimul-Codisplayaltruisticbehavior,theactionsofAgAaremoretightlyclusteredaroundthe(1,1)point
comparedtoSimul-Co.
abstainsfromcontributing,anoccurrenceoftentaggedas‘free-riding’. Incontrast,Simul-Co(representedbyayellow
circle) and our AgA method (illustrated as a red circle) tend to converge towards scenarios characterized by enhanced
contributionsor’altruism’. TheobservedresultsunderlinethatplayersemployingtheAgAmethodexhibitheightened
altruistictendenciesrelativetothoseoperatingunderSimul-Co. Further,AgAexhibitsasuperioralignmenttowardssocially
optimaloutcomepoint(1,1).
C.ExperimentDetails
C.1.SocialDilemmaGames: CleanupandHarvest
SSD games (Vinitsky et al., 2019) implements the Harvest and Cleanup as grid world games.The agents use partially
observedgraphicsobservation,whichcontainsagridof15 15centeredonthemselves. ComparedtotheoriginalCleanup
×
andHarvestgames,SSDgamesaddafirebeammechanic,whereagentscanfireonthegridtohitotherpartners. Anagent
willgain1rewardifharvestinganappleand-1rewardiffiringabeam. Besides,beinghitbyabeamwillresultina50
individualrewardloss. Underthismechanic,selfishagentscaneasilyusefiretopreventothersfromharvestingapplesto
gainmoreindividualrewardsbutharmsocialwelfare.
WeutilizedPPOalgorithminstable-baselines3(Raffinetal.,2021)toimplementthebaselinesandourmethods,withallthe
agentsusingseparatedpolicyparametersforSimul-Indandsharingsamepolicyparametersforotherexperiments. ForSVO,
(cid:16) (cid:17)
wemodifytheindividualrewardtober α(1 actan sumj,j̸=irj . ToemployCGAandAgAstoPPOtrainingprocess,
i − − ri
wecomputebothindividualandcollectivePPO-Clippolicylossesandsubsequentlyutilizethemtocalculatetheadjusted
gradientthroughautomaticdifferentiation. Wedon’tmakechangestothecriticlossnorthecriticnetoptimizationprocess.
Thehyper-parametersforPPOtrainingareasfollows.
• Thelearningrateis1e-4
• ThePPOclippingfactoris0.2.
• Thevaluelosscoefficientis1.
• Theentropycoefficientis0.001.
• Theγ is0.99.
• Thetotalenvironmentstepis1e7forHarvestand2e7forCleanup.
• Theenvironmentepisodelengthis1000.
14AligningIndividualandCollectiveObjectivesinMulti-AgentCooperation
• Thegradclipis40.
C.2.Mixed-motiveStarCraftII
SMAC,oftenemployedasatestbedwithinthesphereofcooperativeMARL,ischaracterizedbythesharedrewardsystem
amongplayers. Withinthispaper,wepresenttheMMM2maptransformedasaversatiletestbeddesignedformixed-motive
cooperation. Specifically,MMM2mapencompasses10agentsand12adversariesasenumeratedbelow:
• ControlledAgents: Comprisedof1Medivac,2Marauders,and7Marines.
• Adversaries: Incorporates1Medivac,3Marauders,and8Marines.
Moreover,thissettingembodiesbothheterogeneityandasymmetry,whereagentswithinthesameteamexhibitdiverse
gamingskills. Additionally,theteamcompositions,forinstance,variationinagentnumberandtype,differinbothopposing
factions. Consequently, the intricate gaming mechanism, the presence of heterogeneous players, and the discernible
asymmetryconspiretoestablishanexemplarymilieuconducivefortheexplorationofmixed-motiveissues.
Inordertocreateamixed-motivetestbed,weadjustedtherewardsystemhailingfromtheoriginalSMACenvironment.
Withthis,weproposedarevampedrewardfunctionthatfeaturestwodistinctcomponents: arewardforimposingdamage
andapenaltyassociatedspecificallywithagentfatalities. Inamovetopotentiallyintensifyinternalconflictsamongst
agents,therewardforimposingdamageissolelyallocatedtotheagentresponsibleforexecutingtheattackontheenemy.
Additionally,thedeathofanagentresultsintheimpositionofanextrapenalty. Consequently,thismechanismfostersan
environmentwhereagentsarepredisposedtowardsindividualprotectionandseparaterewardacquisition,asopposedto
collectivecooperation.
Formally,theagenti’srewardatsteptisdefinedasfollows:
(cid:88)
delta-enemy = [(previous-health current-health)+(previous-shield current-shield)]
i − −
j∈Enemy
ihitjatt
The individual reward for Player i is determined as follows: If the player dies, then r = delta-enemy β penalty;
i i − ×
otherwise,iftheplayersurvives,thenr =delta-enemy .
i i
Experimentsettings. Simul-IndleveragesIPPOwithrecurrentpoliciesanddistinctpolicynetworks. Conversely,Simul-Co
employsMAPPOwithrecurrentpoliciesandconsolidatedpolicynetworks. TheimplementationoftheIPPOandMAPPO
algorithmspresentedinthispaperisfoundedonthemethodologydetailedin(Yuetal.,2022).Weutilizegradientadjustment
optimizationmethods,suchasCGAandAgAs,derivedfromMAPPO,implementinggradientadjustmentsasoutlinedin
Algorithm1.
Thehyper-parametersforPPO-basedtrainingareasfollows.
• Thelearningrateis5e-4
• ThePPOclippingfactoris0.2.
• Thevaluelosscoefficientis1.
• Theentropycoefficientis0.01.
• Theγ is0.99.
• Thetotalenvironmentstepis1e7.
• Thefactorβ inrewardfunctionis1.
• Theenvironmentepisodelengthis400.
15