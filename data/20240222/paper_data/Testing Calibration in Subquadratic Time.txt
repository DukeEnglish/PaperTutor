Testing Calibration in Subquadratic Time
Lunjia Hu∗ Kevin Tian† Chutong Yang‡
Abstract
Intherecentliteratureonmachinelearninganddecisionmaking,calibration hasemergedas
a desirable and widely-studied statistical property of the outputs of binary prediction models.
However, the algorithmic aspects of measuring model calibration have remained relatively less
well-explored. Motivated by [BGHN23a], which proposed a rigorous framework for measuring
distances to calibration, we initiate the algorithmic study of calibration through the lens of
propertytesting. Wedefinetheproblemofcalibrationtesting fromsampleswheregivenndraws
from a distribution D on (predictions,binary outcomes), our goal is to distinguish between the
case where D is perfectly calibrated, and the case where D is ε-far from calibration.
We design an algorithm based on approximate linear programming, which solves calibration
testinginformation-theoreticallyoptimally(uptoconstantfactors)intimeO(n1.5log(n)). This
improvesuponstate-of-the-artblack-boxlinearprogramsolversrequiringΩ(nω)time,whereω >
2istheexponentofmatrixmultiplication. Wealsodevelopalgorithmsfortolerantvariantsofour
testing problem, and give sample complexity lower bounds for alternative calibration distances
to the one considered in this work. Finally, we present preliminary experiments showing that
the testing problem we define faithfully captures standard notions of calibration, and that our
algorithms scale to accommodate moderate sample sizes.
∗Stanford University, lunjia@stanford.edu
†University of Texas at Austin, kjtian@cs.utexas.edu
‡University of Texas at Austin, cyang98@utexas.edu
1
4202
beF
02
]GL.sc[
1v78131.2042:viXraContents
1 Introduction 3
1.1 Our results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.2 Our techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
1.3 Related work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2 Preliminaries 8
2.1 Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
2.2 Rounding linear programs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.3 Box-simplex games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
3 Smooth calibration 13
3.1 Rounding for empirical smooth calibration . . . . . . . . . . . . . . . . . . . . . . . . 13
3.2 Sufficiency and insufficiency of S . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
base
3.3 Testing via smooth calibration. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
4 Lower distance to calibration 21
4.1 LDTC preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
4.2 Rounding for empirical U-LDTC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
4.3 Testing via LDTC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
5 Sample complexity lower bounds for calibration distances 27
5.1 Lower bound for convolved ECE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
5.2 Lower bound for (surrogate) interval CE . . . . . . . . . . . . . . . . . . . . . . . . . 31
6 Experiments 32
21 Introduction
Probabilistic predictions are at the heart of modern data science. In domains as wide-ranging as
forecasting (e.g. predicting the chance of rain from meteorological data [MW84, Mur98]), medicine
(e.g. assessing the likelihood of disease [Doi07]), computer vision (e.g. assigning confidence values
for categorizing images [VDDP17]), and more (e.g. speech recognition [AAA+16] and recommender
systems [RRSK11]), prediction models have by now become essential components of the decision-
making pipeline. Particularly in the context of critical, high-risk use cases, the interpretability of
prediction models is therefore paramount in downstream applications. That is, how do we assign
meaning to the predictions our model gives us, especially when the model is uncertain?
We focus on perhaps the most ubiquitous form of prediction modeling: binary predictions, repre-
sented as tuples (v,y) in [0,1]×{0,1} (where the v coordinate is our prediction of the likelihood
of an event, and the y coordinate is the observed outcome). We model prediction-outcome pairs
in the binary prediction setting by a joint distribution D over [0,1] × {0,1}, fixed in the follow-
ing discussion. In this context, calibration of a predictor has emerged as a basic desideratum. A
prediction-outcome distribution D is said to be calibrated if
E [y | v = t] = t for all t ∈ [0,1]. (1)
(v,y)∼D
That is, calibration asks that the outcome is 1 exactly 60% of the time, when the model returns a
prediction v = 0.6. While calibration (or approximate variants thereof) is a relatively weak require-
ment on a meaningful predictor, as it can be achieved by simple models,1 it can still be significantly
violated in practice. For example, interest in calibration in the machine learning community was
spurred by [GPSW17], which observed that many modern deep learning models are far from cali-
brated. Moreover, variants of calibration have been shown to have strong postprocessing properties
for fairness constraints and loss minimization [HKRR18, DKR+21, GKR+22], which has garnered
renewed interest in calibration by the theoretical computer science and statistics communities.
The question of measuring the calibration of a distribution is subtle; even a calibrated distribution
incurs measurement error due to sampling. For example, consider the expected calibration error,
usedine.g.[NCH15,GPSW17,MDR+21a,RT21b]asaground-truthnotionofcalibrationdistance:
ECE(D) := E (v,y)∼D(cid:2)(cid:12) (cid:12)E (v′,y)∼D(cid:2) y | v′ = v(cid:3) −v(cid:12) (cid:12)(cid:3) .
Unfortunately, theempiricalECEistypicallymeaningless; ifthemarginaldensityofv iscontinuous,
we will almost surely only observe a single sample with each v value. Further, [KF08] observed that
ECE is discontinuous in v. In practice, binned variants of ECE are often used as a proxy, where a
range of v is lumped together in the conditioning event. However, hyperparameter choices (e.g. the
number of bins) can significantly affect the quality of binned ECE variants as a distance measure
[KLM19,NDZ+19,MDR+21a].2 Moreover, asweexploreinthispaper, binnedcalibrationmeasures
inherentlysufferfromlargersamplecomplexity-to-accuracytradeoffs,andarelessfaithfultoground
truth calibration notions in experiments than the calibration distance measures we consider.
Recently,[BGHN23a]undertookasystematicstudyofvariousnotionsofdistancetocalibrationpro-
posed in the literature. They proposed information-theoretic tractability in the prediction-only ac-
1The predictor which ignores features and always return the population mean is calibrated, for example.
2Forexample,[NDZ+19]observedthat,intheirwords,“dramaticdifferencesinbinsensitivity” canoccur“depend-
ing on properties of the (distribution) at hand,” a sentiment echoed by Section 5 of [MDR+21a].
3cess(PA)model,wherethedistancetocalibrationdefinitioncanonlydependonthejointprediction-
outcome distribution (rather than the features of training examples),3 as a desirable criterion for
calibration distances. Correspondingly, [BGHN23a] introduced Definition 1 as a ground-truth no-
tion of distance to calibration in the PA model, which we also adopt in this work.4
Definition 1 (Lower distance to calibration). Let D be a distribution over [0,1]×{0,1}. The lower
distance to calibration (LDTC) of D, denoted dCE(D), is defined by
dCE(D) := inf E |u−v|,
(u,v,y)∼Π
Π∈ext(D)
where ext(D) is all joint distributions Π over (u,v,y) ∈ [0,1]×[0,1]×{0,1} satisfying the following.
• The marginal distribution of (v,y) is D.
• The marginal distribution (u,y) is perfectly calibrated, i.e. E [y|u] = u.
Π
Definition 1 has various beneficial aspects: it is convex in v, computable in the PA model, and (as
shown by [BGHN23a]) polynomially-related to various other calibration distance notions, including
some which require feature access. Roughly, the LDTC of a distribution is the tightest lower bound
on the ℓ distance between v and any calibrated function of the features which can be made, after
1
taking features into account. The LDTC is the analog of this feature-aware definition of calibration
distance when limited to the PA model. We focus on this definition in the remainder of the paper.
1.1 Our results
We initiate the algorithmic study of the calibration testing problem, defined as follows.
Definition 2 (Calibration testing). Let ε ∈ [0,1]. We say algorithm A solves the ε-calibration
testing problem with n samples, if given n i.i.d. draws from a distribution D over [0,1]×{0,1}, A
returns either “yes” or “no” and satisfies the following with probability ≥ 2.5
3
• A returns “no” if dCE(D) ≥ ε.
• A returns “yes” if dCE(D) = 0.
In this case, we also call A an ε-calibration tester.
To our knowledge, we are the first work to formalize the calibration testing problem. Defini-
tion 2 is natural from the perspective of property testing, an influential paradigm in statistical
learning [Ron08, Ron09, Gol17]. In particular, there is an ε = Θ(n−1/2) so that it is information-
n
theoretically impossible to solve the ε -calibration testing problem from n samples (see Lemma 2),
n
soavariantofDefinition2withanexactdistinguishingthresholdbetween“calibrated/uncalibrated”
3Thisaccessmodelispracticallydesirablebecauseitabstractsawaythefeaturespace,whichcanleadtosignificant
memorysavingswhenourgoalisonlytotestthecalibrationofmodelpredictions. Moreover,thismatchesconventions
in the machine learning literature: for example, loss functions are typically defined in the PA model.
4We note that [BGHN23a] introduced an upper distance to calibration, also defined in the PA model, which
they showed is quadratically-related to the dCE in Definition 1. However, the upper distance does not satisfy basic
properties such as continuity, making it less amenable to estimation and algorithm design.
5As is standard in property testing problems, the success probability of either a calibration tester or a tolerant
calibration tester can be boosted to 1−δ for any δ ∈(0,1) at a k =O(log(1)) overhead in the sample complexity.
δ
This is because we can independently call k copies of the tester and output the majority vote, which succeeds with
probability ≥1−δ by Chernoff bounds, so we focus on δ= 1.
3
4is not tractable. Hence, Definition 2 only requires distinguishing distributions D which are “clearly
uncalibrated” (parameterized by a threshold ε) from those which are perfectly calibrated.
Our first algorithmic contribution is a subquadratic-time algorithm for calibration testing, in the
regime where the threshold ε is information-theoretically optimal up to a constant.
Theorem 1 (Informal, see Theorem 3, Corollary 3). Let n ∈ N, and let ε = Ω(ε ), where ε =
n n
Θ(n−1/2) is minimal such that it is information-theoretically possible to solve the ε -calibration
n
testing problem with n samples. There is an algorithm which solves the ε-calibration testing problem
with n samples, running in time O(nlog(n) ) = O(n1.5log(n)).
ε
We prove Theorem 1 by designing an algorithm for estimating smCE(D(cid:98)n), the smooth calibration
error (Definition 3), an alternative calibration distance measure, of an empirical distribution D(cid:98)n.
Definition 3 (Smoothcalibrationerror). LetW bethesetofLipschitzfunctionsw : [0,1] → [−1,1].
The smooth calibration error of distribution D over [0,1]×{0,1}, denoted smCE(D), is defined by
(cid:12) (cid:12)
smCE(D) = sup (cid:12)E (v,y)∼D[(y−v)w(v)](cid:12).
w∈W
It was shown in [BGHN23a] that smCE(D) is a constant-factor approximation to dCE(D) for all D
on [0,1]×{0,1} (see Lemma 12). Additionally, the empirical smCE admits a representation as a
linear program with an O(n)×O(n)-sized constraint matrix encoding Lipschitz constraints.6 Thus,
[BGHN23a] proposed a simple procedure for estimating smCE(D): draw n samples from D, and
solve the associated linear program on the empirical distribution. While there have been significant
recentruntimeadvancesinthelinearprogrammingliterature[LS14,CLS21,vdBLSS20,vdBLL+21],
all state-of-the-art black-box linear programming algorithms solve linear systems involving the con-
straint matrix, which takes Ω(nω) time, where ω > 2.371 [WXXZ23] is the current exponent of
matrix multiplication. Even under the best-possible assumption that ω = 2, the strategy of exactly
solving a linear program represents an Ω(n2) quadratic runtime barrier for calibration testing.
Our work bypasses this barrier by noting that it suffices to solve the smCE linear program to low
accuracy, i.e. ε = Ω(n−1/2), making it amenable to approximate (first-order method based) solvers.
However,thesmCElinearprogram(see(11))ishard-constrained,sostandardfirst-ordermethodsdo
notreadilyapply. Wedevelopanovelapproximatesolverbasedonacustomcombinatorialrounding
procedure, used to argue about the error incurred by lifting the smooth calibration constraints into
a penalty term, making it amenable to existing minimax optimization procedures. Our algorithm
proving the runtime bound in Theorem 1 is given in Section 3.
Wenextdefineatolerant variantofDefinition2(seeDefinition4),whereweallowforerrorthresholds
in both the “yes” and “no” cases; “yes” is the required answer when dCE(D) ≤ ε , and “no” is
2
required when dCE(D) ≥ ε . Our algorithm in Theorem 1 continues to serve as an efficient tolerant
1
calibration tester when ε ≥ 4ε , with formal guarantees stated in Theorem 3. This constant-factor
1 2
loss comes from a similar loss in the relationship between smCE and dCE, see Lemma 12. We make
theobservationthataconstantfactorlossinthetoleranttestingparametersisinherentfollowingthis
strategy, via a lower bound in Lemma 13. Thus, even given infinite samples, computing the smooth
6Formally,thenumberofconstraintsinthesmCElinearprogramisO(n2),butweshowthatinthehard-constrained
setting, requiring that “adjacent” constraints are met suffices (see Lemma 7).
5calibration error cannot solve tolerant calibration testing all the way down to the information-
theoreticthresholdε ≥ ε . Todevelopanimprovedtolerantcalibrationtester,wedirectlyshowhow
1 2
to approximate the LDTC of an empirical distribution, our second main algorithmic contribution.
Theorem 2 (Informal, see Theorem 4, Corollary 4). Let n ∈ N, and let ε −ε = Ω(ε ), where
1 2 n
ε = Θ(n−1/2) is minimal such that it is information-theoretically possible to solve the ε -calibration
n n
testing problem with n samples. There is an algorithm which solves the (ε ,ε )-tolerant calibration
1 2
testing problem with n samples, running in time O( nlog(n) ) = O(n2log(n)).
(ε1−ε2)2
While our algorithm in Theorem 2 is slower than that in Theorem 1, it directly approximates the
LDTC, rather than a related quantity. We mention that state-of-the-art black-box linear program-
ming based solvers, while still applicable to (a discretized variant of) the empirical LDTC, require
Ω(n2.5) time [vdBLL+21]. This is because the constraint matrix for the ε-approximate empirical
√
LDTClinearprogramhasdimensionsO(n)×O(n),resultinginaruntimeoverheadof≈ 1 = O( n)
ε ε
in both our approach and more direct solvers. We prove Theorem 2 in Section 4.
In Section 5, we complement our algorithmic results with lower bounds (Theorems 5, 6) on the
sample complexity required to solve variants of the testing problem in Definition 2, when dCE
is replaced with different calibration distances. For several widely-used distances in the machine
learning literature, including binned and convolved variants of ECE [NCH15, BN23], we show that
Ω(cid:101)(ε−2.5) samples are required to the associated ε-calibration testing problem. This demonstrates a
statistical advantage of our focus on dCE as our ground-truth notion for calibration testing.
Wecorroborateourtheoreticalfindingswithpreliminaryexperimentalevidenceonrealandsynthetic
datainSection6. First,onasimpleBernoulliexample,weshowthatdCEandsmCEtestersaremore
reliable indicators of calibration distance than a recently-proposed binned ECE variant. We then
apply our smCE tester to postprocessed neural network predictions to test their calibration levels,
validating against the findings in [GPSW17]. Finally, we implement our iterative method from
Theorem 1 on our Bernoulli dataset, showing that it scales to moderate dimensions and achieves
the expected convergence rate. We note that a current limitation of our work is optimizing the
implementation to scale more effectively to high dimensions, an important open direction.7
1.2 Our techniques
Theorem 1 and Theorem 2 follow from designing custom first-order methods for approximat-
ing empirical linear programs associated with the smCE and dCE of a sampled dataset D(cid:98)n :=
{(v ,y )} ∼ D. In both cases, known generalization bounds from [BGHN23a] show it suf-
i i i∈[n] i.i.d.
fices to approximate the value of the empirical calibration distances to error ε = Ω(n−1/2).
We begin by explaining our strategy for estimating smCE(D(cid:98)n) (Definition 3). By definition, the
smooth calibration error of D(cid:98)n can be formulated as a linear program,
1 (cid:88)
min x (v −y ), where |x −x | ≤ |v −v | for all (i,j) ∈ [n]×[n]. (2)
i i i i j i j
x∈[−1,1]n n
i∈[n]
Here, x ∈ [−1,1] corresponds to the weight on v , and there are 2(cid:0)n(cid:1) constraints on the decision
i i 2
7Ourcodecanbefoundat: https://github.com/chutongyang98/Testing-Calibration-in-Subquadratic-Time.
git.
6variable x, each of which corresponds to a Lipschitz constraint. We can write (2) as an inequality-
constrained linear program max ⟨c,x⟩, for appropriate A,b,c. While first-order meth-
x∈[−1,1]n:Ax≤b
ods (e.g. Frank-Wolfe type algorithms) can sometimes apply to hard-constrained linear programs, it
is unclear how to implement projection steps onto the Lipschitz constraints Ax ≤ b in our setting.
We instead follow an “augmented Lagrangian” method where we lift the constraints directly into
the objective as a soft-constrained penalty term. To prove correctness of this lifting, we follow a line
of results in combinatorial optimization [She13, JST19]. These works develop a “proof-by-rounding
algorithm” framework to show that the hard-constrained and soft-constrained linear programs have
equal values. This rounding-based framework is summarized in Section 2.2 (see Lemma 3).
To use this rounding-based framework, our key technical innovation is showing that if x violates
each of a carefully-selected set S of O(n) Lipschitz constraints by at most ∆, we can design a
procedure which produces x′ with ∥x′−x∥ = O(∆), yet x′ is fully feasible for the constraints
∞
Ax′ ≤ b. Our set S is a multilayer construction, stated formally in (20). Informally, we begin
by sorting v into nonincreasing order, and defining a “zeroth-layer” of constraints by enforcing the
Lipschitz condition on all pairs (i,i+1) for i ∈ [n−1]. The triangle inequality shows that exactly
enforcing these constraints suffices for feasibility of the linear program (Lemma 7). Enforcing these
zeroth-layer constraints only approximately, however, degrades poorly under iterative rounding.
Accordingly, we build three dyadic trees of constraints to protect how much our rounding procedure
can iteratively degrade constraint violations. After proving correctness of our rounding method for
the smooth calibration error linear program, we use it to show that the soft-constrained objective
min ⟨c,x⟩+∥max(A x−b ,0 )∥ (3)
S: S S ∞
x∈[−1,1]n
has the same value as (2), for A,b,c from the smCE linear program, and where subscripting by S
restricts to only the relevant rows of the matrix A, and the relevant coordinates of b. Finally, we
apply a recent first-order minimax optimization algorithm of [JT23] to solve the problem (3).
Ourhigh-levelapproachtoprovingTheorem2isanalogous, goingthroughthesamerounding-based
augmented Lagrangian framework as described before. However, the empirical dCE linear program
(see Lemma 16) is more complicated than that for smCE error, and imposes two sets of constraints,
roughly corresponding to marginal satisfaction of (v,y) and calibration of (u,y) (using notation
from Definition 1). We design a two-step rounding procedure, which first fixes the marginals on the
(v,y) coordinates, and then calibrates the u coordinates without affecting any (v,y) marginal. One
interesting feature of our dCE rounding algorithm, compared to prior works [She13, JST19], is that
it is cost-sensitive: we take into account the metric structure of the linear costs c to argue rounding
does not harm the objective value, rather than naïvely applying Hölder’s inequality.
Finally, our sample complexity lower bounds in Section 5 for alternative calibration measures fol-
low from an information-theoretic analysis using LeCam’s two-point method [LeC73] and Ingster’s
method [IS03]. We construct a perfectly-calibrated distribution as well as a family of miscalibrated
distributions with large calibration error (≥ ε) in the alternative calibration measures (convolved
ECE and interval CE). A testing algorithm for these measures needs to distinguish these two cases.
We show that the distinguishing task has high sample complexity (≈ ε−2.5) by bounding the total
variation distance between the two joint distributions of the input examples from the two cases.
Our proof is inspired by a similar analysis showing a sample complexity lower bound for identity
testing in the distribution testing literature (see e.g. Section 3.1 of [Can22]).
71.3 Related work
The calibration performance of deep neural networks has been studied extensively in the literature
(e.g. [GPSW17, MDR+21b, Rt21a, BGHN23b]). Measuring the calibration error in a meaningful
way can be challenging, especially when the predictions are not naturally discretized (e.g. in neu-
ral networks). Recently, [BGHN23a] addresses this challenge using the distance to calibration as a
central notion. They consider a calibration measure to be consistent if it is polynomially-related
to the distance to calibration. Consistent calibration measures include the smooth calibration error
[KF04], Laplace kernel calibration error [KSJ18], interval calibration error [BGHN23a], and con-
volved ECE [BN23].8 On the algorithmic front, substantial observations were made by [BGHN23a]
on linear programming characterizations of calibration distances such as the LDTC and smooth cal-
ibration. While there have been significant advances on the runtime frontier of linear programming
solvers, current runtimes for handling an n×d linear program constraint matrix with n ≥ d remain
Ω(min(nd + d2.5,nω)) [CLS21, vdBLL+21]. Our constraint matrix is roughly-square and highly-
sparse, so it is plausible that e.g. the recent research on sparse linear system solvers [PV21, Nie22]
couldapplytotherelevantNewton’smethodsubproblemsandimproveupontheserates. Moreover,
while efficient estimation algorithms have been proposed by [BGHN23a] for (surrogate) interval
calibration error and by [BN23] for convolved ECE, these algorithms require suboptimal sample
complexity for solving our testing task in Definition 2 (see Section 5). To compute their respective
distances to error ε from samples, these algorithms require Ω(ε−5) and Ω(ε−3) time. As compari-
son, under this parameterization Theorems 1 and 2 require O(cid:101)(ε−3) and O(cid:101)(ε−4) time, but can solve
stronger testing problems with the same sample complexity, experimentally validated in Section 6.
2 Preliminaries
2.1 Notation
Throughout this paper, we use D to denote a distribution over [0,1] × {0,1}. When D is clear
from context, we let D(cid:98)n = {(v i,y i)}
i∈[n]
denote a dataset of n independent samples from D and,
in a slight abuse of notation, the distribution with probability 1 for each (v ,y ). We say d is a
n i i
calibration distance if it takes distributions on [0,1]×{0,1} to the nonnegative reals R , so dCE
≥0
(Definition 1) and smCE (Definition 3) are both calibration distances.
When applied to a vector, we let ∥·∥ denote the ℓ norm for p ≥ 1. We denote [n] := {i ∈
p p
N | i ≤ n}. We let 0 and 1 denote the all-zeroes and all-ones vectors in dimension d. We let
d d
∆d := {x ∈ Rd | ∥x∥ = 1} denote the probability simplex in dimension d. The ith coordinate
≥0 1
basis vector is denoted e . We say x˜ ∈ R is an ε-additive approximation of x ∈ R if |x˜−x| ≤ ε.
i
For a set S ⊂ R, we say another set T ⊂ R is an ε-cover of S if for all s ∈ S, there is t ∈ T with
|s−t| ≤ ε. We use O(cid:101) and Ω(cid:101) to hide polylogarithmic factors in the argument.
We denote matrices in boldface throughout. For any matrix A ∈ Rm×n, we refer to its ith row by
A and its jth column by A . Moreover, for a set S identified with rows of a matrix A, we let A
i: :j s:
denote the row indexed by s ∈ S, and use similar notation for columns.
8Thecalibrationdistancewecalltheconvolved ECE inourworkwasoriginallycalledthesmooth ECE in[BN23].
We change the name slightly to reduce overlap with the smooth calibration error (Definition 3), a central object
throughout the paper.
8For any p,q ≥ 1, we define
∥A∥ := max ∥Ax∥ .
p→q q
x∈Rn|∥x∥ ≤1
p
Notice that in particular, ∥A∥ is the largest ℓ norm of any column of A, and ∥A∥ is the
1→1 1 ∞→∞
largest ℓ norm of any row. We say that x ∈ X is an ε-approximate minimizer of f : X → R if
1
f(x)−min f(x′) ≤ ε. Wecall(x,y) ∈ X×Y anε-approximatesaddlepointtoaconvex-concave
x′∈X
function f : X ×Y → R if its duality gap is at most ε, i.e.
maxf(x,y′)− minf(x′,y) ≤ ε.
y′∈Y x′∈X
The following simple claim will often be useful.
Lemma 1. Let f : X ×Y → R be convex-concave for compact X,Y, and let g(x) := max f(x,y)
y∈Y
for x ∈ X. If (x,y) is an ε-approximate saddle point to f, x is an ε-approximate minimizer to g.
Proof. Let y′ := argmax f(x,y) and x′ := argmin f(x′,y). The conclusion follows from
y∈Y x′∈X
min g(x⋆) = min maxf(x⋆,y⋆) = max min f(x⋆,y⋆) ≥ minf(x′,y),
x⋆∈X x⋆∈Xy⋆∈Y y⋆∈Yx⋆∈X x′∈X
where we used strong duality (via Sion’s minimax theorem), so that
g(x)− min g(x⋆) ≤ g(x)−f(x′,y) = f(x,y′)−f(x′,y) ≤ ε.
x⋆∈X
We next define a tolerant variant of the calibration testing problem in Definition 2.
Definition 4 (Tolerant calibration testing). Let 0 ≤ ε ≤ ε ≤ 1. We say algorithm A solves the
2 1
(ε ,ε )-tolerantcalibrationtestingproblemwithnsamples, ifgivenni.i.d.drawsfromadistribution
1 2
D over [0,1]×{0,1}, A returns either “yes” or “no” and satisfies the following with probability ≥ 2.
3
• A returns “no” if dCE(D) ≥ ε .
1
• A returns “yes” if dCE(D) ≤ ε .
2
In this case, we also call A an (ε ,ε )-tolerant calibration tester.
1 2
Note that an algorithm which solves the (ε ,ε )-tolerant calibration testing problem with n samples
1 2
also solves the ε -calibration testing problem with the same sample complexity. Moreover, we give
1
a simple impossibility result on parameter ranges for calibration testing.
Lemma 2. Let 0 ≤ ε ≤ ε ≤ 1 satisfy ε − ε = ε. There is a universal constant C such
2 1 2 1 2 coin
that, given n ≤ Ccoin samples from a distribution on [0,1]×{0,1}, it is information-theoretically
ε2
impossible to solve the (ε ,ε )-tolerant calibration testing problem.
1 2
Proof. Suppose ε ≤ 1 , else we can choose C small enough such that n < 1. We consider two
10 coin
distributions over [0,1]×{0,1}, D and D′, with dCE(D) ≥ ε but dCE(D′) ≤ ε , so if A succeeds
1 2
at tolerant calibration testing for both D and D′, we must have d (D⊗n,(D′)⊗n) ≥ 1, where we
TV 3
denote the n-fold product of a distribution with ·⊗n. Else, A cannot return different answers from
n samples with probability ≥ 2, as required by Definition 4. Specifically, we define D,D′ as follows.
3
9• To draw (v,y) ∼ D, let v = 1 +ε and y ∼ Bern(1).
2 1 2
• To draw (v,y) ∼ D′, let v = 1 +ε and y ∼ Bern(1 +ε).
2 1 2
We claim that dCE(D) = ε . To see this, let Π ∈ ext(D) and (u,v,y) ∼ Π, so E [u] = 1
1 (u,v,y)∼Π 2
because u is calibrated. By Jensen’s inequality, we have:
(cid:12) (cid:18) (cid:19)(cid:12)
(cid:12) 1 (cid:12)
E (u,v,y)∼Π[|u−v|] ≥ (cid:12) (cid:12)E (u,v,y)∼Π[u]−
2
+ε 1 (cid:12)
(cid:12)
= ε 1.
Theequalitycaseisrealizedwhenu = 1 withprobability1,provingtheclaim. Similarly,dCE(D′) =
2
ε . Finally, let π := Bern(1), π′ := Bern(1 + ε), and π⊗n,(π′)⊗n denote their n-fold product
2 2 2
distributions. Pinsker’s inequality shows that it suffices to show that d (π⊗n∥(π′)⊗n) ≤ 1 to
KL 5
contradict our earlier claim d ((D)⊗n,(D′)⊗n) ≥ 1. To this end, we have
TV 3
(cid:32) (cid:32) (cid:33) (cid:32) (cid:33)(cid:33)
d (π⊗n∥(π′)⊗n) = n·d (cid:0) π∥π′(cid:1) = n log 1 2 +log 1 2
KL KL 2 1 +ε 1 −ε
2 2
(cid:18) (cid:19)
n 1 n 1
= log ≤ ·5ε2 ≤ ,
2 1−4ε2 2 5
where the first line used tensorization of d , and the last chose C small enough.
KL coin
We also generalize Definitions 2 and 4 to apply to an arbitrary calibration distance.
Definition 5 (dtesting). Let d be a calibration distance. For ε ∈ R , we say algorithm A solves the
≥0
ε-d-testing problem (or, A is an ε-d tester) with n samples, if given n i.i.d. draws from a distribution
D over [0,1]×{0,1}, A returns either “yes” or “no” and satisfies the following with probability ≥ 2.
3
1. A returns “no” if d(D) ≥ ε.
2. A returns “yes” if d(D) = 0.
For 0 ≤ ε ≤ ε , we say algorithm A solves the (ε ,ε )-tolerant d testing problem (or, A is an
2 1 1 2
(ε ,ε )-tolerant d tester) with n samples, if given n i.i.d. draws from a distribution D over [0,1]×
1 2
{0,1}, A returns either “yes” or “no” and satisfies the following with probability ≥ 2.
3
1. A returns “no” if d(D) ≥ ε .
1
2. A returns “yes” if d(D) ≤ ε .
2
2.2 Rounding linear programs
In this section, we give a general framework for approximately solving linear programs, following
similar developments in the recent combinatorial optimization literature [She13, JST19]. Roughly
speaking, this framework is a technique for losslessly converting a constrained convex program to
an unconstrained one, provided we can show existence of a rounding procedure compatible with the
constrainedprograminanappropriatesense. Webeginwithourdefinitionofaroundingprocedure.
Definition 6 (Rounding procedure). Consider a convex program defined on the intersection of
convex set X with linear equality constraints Ax = b:
min c⊤x. (4)
x∈X
Ax=b
10We say Round is a (A(cid:101),˜b,p)-equality rounding procedure for (A,b,c,X) if p ≥ 1, and for any x ∈ X,
there exists x′ := Round(x) ∈ X such that Ax′ = b, A(cid:101)x′ =˜b, and
(cid:13) (cid:13)
c⊤x′ ≤ c⊤x+(cid:13)A(cid:101)x−˜b(cid:13) . (5)
(cid:13) (cid:13)
p
Similarly, consider a program on the intersection of a convex set with linear inequality constraints:
min c⊤x. (6)
x∈X
Ax≤b
We say Round is a (A(cid:101),˜b,p)-inequality rounding procedure for (A,b,c,X) if p ≥ 1, and for any
x ∈ X, there exists x′ := Round(x) ∈ X such that Ax′ ≤ b, A(cid:101)x′ ≤˜b, and
(cid:13) (cid:16) (cid:17)(cid:13)
c⊤x′ ≤ c⊤x+(cid:13) (cid:13)max A(cid:101)x−˜b,0
m
(cid:13)
(cid:13)
, (7)
p
where m is the number of rows in A(cid:101), ˜b, and the max operation is entrywise.
Intuitively, rounding procedures replace the hard-constrained problems (4), (6) with their soft-
constrained variants, i.e. the soft equality-constrained
(cid:13) (cid:13)
minc⊤x+(cid:13)A(cid:101)x−˜b(cid:13) , (8)
(cid:13) (cid:13)
x∈X p
and the soft inequality-constrained
(cid:13) (cid:16) (cid:17)(cid:13)
minc⊤x+(cid:13) (cid:13)max A(cid:101)x−˜b,0
m
(cid:13)
(cid:13)
, (9)
x∈X p
respectively, for some (A(cid:101),˜b,p) constructed from the hard-constrained problem instance (parameter-
ized by A,b,c,X). Leveraging the assumptions on our rounding procedure in each case (5), (7), we
now show how to relate approximate solutions to these problems, generalizing Lemma 1 of [JST19].
Lemma 3. Let x be an ε-approximate minimizer to (8), and let Round be a (A(cid:101),˜b,p)-equality round-
ing procedure for (A,b,c,X). Then x′ := Round(x) is an ε-approximate minimizer to (4). Similarly,
if x is an ε-approximate minimizer to (9) and Round is a (A(cid:101),˜b,p)-inequality rounding procedure for
(A,b,c,X), x′ := Round(x) is an ε-approximate minimizer to (6).
Proof. We only discuss the equality-constrained setting (4), (8), as the proof of the inequality-
constrained setting is entirely analogous. We first claim that a minimizing solution to (8) satisfies
the constraints Ax = b. To see this, given any x ∈ X, we can produce x′ ∈ X with Ax′ = b and
such that x′ has smaller objective value in (8). Indeed, letting x′ := Round(x′), (5) guarantees
(cid:13) (cid:13) (cid:13) (cid:13)
c⊤x′ = c⊤x′+(cid:13)A(cid:101)x′−˜b(cid:13) ≤ c⊤x+(cid:13)A(cid:101)x−˜b(cid:13) ,
(cid:13) (cid:13) (cid:13) (cid:13)
p p
as claimed. Now let x⋆ ∈ X satisfying Ax⋆ = b minimize (8). Then, if x is an ε-approximate
minimizer to (8) and x′ = Round(x), we have the desired claim from Ax′ = b, and
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
c⊤x′ = c⊤x′+(cid:13)A(cid:101)x′−˜b(cid:13) ≤ c⊤x+(cid:13)A(cid:101)x−˜b(cid:13) ≤ c⊤x⋆+(cid:13)A(cid:101)x⋆−˜b(cid:13) = c⊤x⋆+ε.
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
p p p
112.3 Box-simplex games
We will apply the rounding framework in Section 2.2 to various hard-constrained linear programs,
in the geometries defined by p = 1,∞. To aid in approximately solving the soft-constrained linear
programs arising from our framework, we use the following procedure from [JT23], building upon
the recent literature for solving box-simplex games at accelerated rates [She17, JST19, CST21].
Proposition 1 (Theorem 1, [JT23]). Let A ∈ Rn×d, b ∈ Rd, c ∈ Rn, and ε > 0. There is an
algorithm which computes an ε-approximate saddle point to the box-simplex game
min maxx⊤Ay−b⊤y+c⊤x, (10)
x∈[−1,1]ny∈∆d
in time
(cid:18) (cid:19)
∥A∥ logd
O nnz(A)· 1→1 .
ε
The following two corollaries of Proposition 1 will be particularly useful in our development. The
first is immediate using Lemma 1, upon negating the box-simplex game (10), exchanging the names
of the variables (x,y), (b,c), and explicitly maximizing over y ∈ [−1,1]n, i.e.
min ⟨c,x⟩+∥Ax−b∥ = min max ⟨c,x⟩+y⊤(Ax−b).
1
x∈∆d x∈∆dy∈[−1,1]n
Corollary 1. Let A ∈ Rn×d, b ∈ Rn, c ∈ Rd, and ε > 0. There is an algorithm which computes an
ε-approximate minimizer to min c⊤x+∥Ax−b∥ , in time
x∈∆d 1
(cid:18) (cid:19)
∥A∥ logd
O nnz(A)· 1→1 .
ε
We also have the following analog for a variant of ℓ regression.
∞
Corollary 2. Let A ∈ Rn×d,b ∈ Rn,c ∈ Rd, and ε > 0. There is an algorithm which computes
an ε-approximate minimizer to min c⊤x+∥max(Ax−b,0 )∥ , where the max operation
x∈[−1,1]d n ∞
is entrywise, in time
(cid:18) (cid:19)
∥A∥ logn
O nnz(A)· ∞→∞ .
ε
Proof. First, observe that we can rewrite the given objective as
min max c⊤x+y⊤(A(cid:101)x−˜b),
x∈[−1,1]dy∈∆n+1
where A(cid:101) ∈ R(n+1)×d appends an extra all-zeroes row to A, and˜b ∈ Rn+1 appends an extra zero to b.
Namely, if Ax−b ≤ 0
n
entrywise, we have y⊤(A(cid:101)x−˜b) = 0 = ∥max(Ax−b,0 n)∥ ∞, and otherwise
y⊤(A(cid:101)x−˜b) chooses the largest entry of Ax−b. The conclusion follows from Proposition 1, using
∥A(cid:101)∥
∞→∞
= ∥A∥
∞→∞
= ∥A⊤∥ 1→1.
123 Smooth calibration
Inthissection,weprovideourmainresultonapproximatingthesmoothcalibrationofadistribution
on [0,1]×{0,1}. In Section 3.1, we first develop a rounding procedure compatible with the smooth
calibration linear program (in the sense of Definition 6), when applied to an empirical distribution.
We provide some discussion of a simpler variant of our rounding procedure in Section 3.2. Finally,
in Section 3.3, we show how to use the solver resulting from our rounding procedure to develop an
efficient algorithm for solving the calibration testing problems in Definitions 2 and 4.
3.1 Rounding for empirical smooth calibration
In this section, we develop an efficient algorithm for computing the smooth calibration error of an
empirical distribution. Specifically, throughout the section, we fix a dataset under consideration,
D(cid:98)n := {(v i,y i)}
i∈[n]
⊂ [0,1]×{0,1},
and the corresponding empirical distribution (which, in an abuse of notation, we also denote D(cid:98)n),
i.e. we use (v,y) ∼ D(cid:98)n to mean that (v,y) = (v i,y i) with probability n1 for each i ∈ [n]. We also
assume without loss of generality that the {v } are in sorted order, so 0 ≤ v ≤ ... ≤ v ≤ 1.
i i∈[n] 1 n
Recalling Definition 3, the associated empirical smooth calibration linear program is
smCE(D(cid:98)n) := maxc⊤x, where X := [−1,1]n,
x∈X
Ax≤b
1
c := (y −v ) for all i ∈ [n],
i i i
n (11)
b = b := v −v for all (i,j) ∈ [n]×[n] with i < j,
(i,j,+) (i,j,−) j i
and A := e −e ,
(i,j,+): j i
A := e −e , for all (i,j) ∈ [n]×[n] with i < j.
(i,j,−): i j
Here, x represents the value w(v ). Because x = 0 is always feasible, the maximum ⟨c,x⟩ is non-
i i n
negative, so we can drop the absolute value in Definition 3. Moreover, A and b have rows identified
with [2(cid:0)n(cid:1) ], i.e. doubled pairs of sorted indices in [n]×[n], enforcing the Lipschitz constraints
2
max(x −x ,x −x ) ≤ v −v for all (i,j) ∈ [n]×[n] with i < j.
j i i j j i
Our solver for (11) goes through the machinery of Definition 6; we design a (A(cid:101),˜b,∞)-inequality
rounding procedure for (A,b,c,X) in (11). We let A(cid:101),˜b be carefully chosen subsets of the rows of
A,b, with O(n) rows. Before formally stating our choices of A(cid:101),˜b, we give several helper lemmas.
Lemma 4. Let i,j,k ∈ [n] with i < j < k. Suppose that for ∆ ≥ 0 and v,x ∈ Rn with v ≤ v ≤ v ,
i j k
|x −x | ≤ v −v +∆, |x −x | ≤ v −v , |x −x | ≤ v −v +∆.
i j j i i k k i j k k j
There is y ∈ Rn with y = x for all ℓ ̸= j, y ∈ [min x ,max x ], |y −x | ≤ ∆, and
ℓ ℓ j ℓ∈[n] ℓ ℓ∈[n] ℓ j j
|y −y | ≤ v −v , |y −y | ≤ v −v , |y −y | ≤ v −v .
i j j i i k k i j k k j
13Proof. We split into four cases, depending on which subset of the constraints |x −x | ≤ v −v and
i j j i
|x −x | ≤ v −v is false. In the first case, neither is false and returning y ← x suffices.
j k k j
In the second case, both are false, i.e.
v −v < |x −x | ≤ v −v +∆, v −v < |x −x | ≤ v −v +∆. (12)
j i i j j i k j j k k j
We claim that in this case, x ≤ min(x ,x ) or x ≥ max(x ,x ). To see this, if x ≤ x ≤ x , then
j i k j i k i j k
|x −x |+|x −x | = (x −x )+(x −x ) = x −x ≤ v −v = (v −v )+(v −v ), (13)
j i k j j i k j k i k i j i k j
where we used |x −x | ≤ v −v , so we cannot have (12) hold. Similarly, if x ≤ x ≤ x , this again
i k k i k j i
contradicts (12). Now, we claim it suffices to choose y = x coordinatewise except for y , defined by
j
y ← x +sign(x −x )max((|x −x |−(v −v )),(|x −x |−(v −v ))). (14)
j j i j i j j i k j k j
(cid:124) (cid:123)(cid:122) (cid:125)
:=ζ
In other words, we move x towards the other two points by the larger of the two violation amounts
j
in (12). To prove correctness of (14), suppose x ≤ min(x ,x ) without loss of generality, so that
j i k
y = x +ζ (the other case is symmetric by negating x). We first observe that
j j
max(x ,x )−x ≥ ζ =⇒ y ≤ max(x ,x ). (15)
i k j j i k
Next, consider the subcase where y ≤ min(x ,x ). Then, y −y = x −y = x −x −ζ ≤ v −v
j i k i j i j i j j i
so the Lipschitz constraint on (y ,y ) is enforced. We symmetrically have y −y ≤ v −v .
i j k j k i
The other subcases are when x ≤ y ≤ x or x ≤ y ≤ x , i.e. y is in between x and x . By the
i j k k j i j i k
definition of ζ, either |y −x | = v −v or |x −y | = v −v is a tight Lipschitz constraint, and
j i j i k j k j
in either case (13) shows that the other constraint is also enforced, since |x −x | ≤ v −v .
k i k i
In the third case, we have
v −v < |x −x | ≤ v −v +∆, |x −x | ≤ v −v . (16)
j i i j j i j k k j
We claim it suffices to choose
y ← x +sign(x −x )(|x −x |−(v −v )). (17)
j j i j i j j i
(cid:124) (cid:123)(cid:122) (cid:125)
:=ζ
Again assume without loss of generality that x ≤ x and y = x +ζ. By construction, |y −x | =
j i j j j i
v −v , so we need to check that |y −x | ≤ v −v . If originally x ≤ x ≤ x , then (13) shows
j i j k k j k j i
x −x ≤ (v −v )−ζ, so |y −x | ≤ x −x +ζ ≤ v −v as claimed. Otherwise, if x ≤ x , then
j k k j j k k j k j j k
(15) shows y ≤ x . If y ∈ [x ,x ], then |y −x | = v −v and (13) show that |y −x | ≤ v −v
j i j k i j i j i j k k j
as well. Finally, if y ≤ x , then 0 ≤ x −y = x −x −ζ ≤ v −v , as desired.
j k k j k j k j
In the fourth case, we have
v −v < |x −x | ≤ v −v +δ, |x −x | ≤ v −v . (18)
k j k j k j j i j i
We claim it suffices to choose
y ← x +sign(x −x )(|x −x |−(v −v )). (19)
j j k j k j k j
(cid:124) (cid:123)(cid:122) (cid:125)
:=ζ
14The correctness analysis of (19) proceeds symmetrically to the correctness analysis of (17).
Itisstraightforwardtocheckthatinallcases,|y −x | ≤ ∆,andy staysintherangewithendpoints
j j j
min(x ,x ,x ) and max(x ,x ,x ), proving the other claimed conditions.
i j k i j k
Lemma4saysthatgiventhreeLipschitzconstraintsinducedbythreecoordinatesofv, ifthe“outer”
constraint is satisfied and the other two constraints are violated by ≤ ∆, we can move the middle
coordinate by ≤ ∆ to fix all constraints. Moreover, it guarantees that if x ∈ X = [−1,1]n, then
y ∈ X. We next observe that this procedure does not make adjacent constraints worse.
Lemma 5. Let i,j,h ∈ [n] with i < j < h. Suppose that for ∆ ≥ 0 and v,x ∈ Rn with v ≤ v ≤ v ,
i j h
v −v ≤ |x −x | ≤ v −v +∆, |x −x | ≤ v −v +∆, |x −x | ≤ v −v +∆.
j i i j j i h i h i h j h j
Let t = x −sign(x −x )·ζ for ζ ∈ [0,|x −x |−(v −v )]. Then, |x −t| ≤ v −v +∆.
j j i i j j i h h j
Proof. The argument is symmetric in x and −x, so assume without loss that x ≤ x and
i j
t ∈ [x +(v −v ),x ].
i j i j
If x ≤ x , then since t ≤ x , we have |x −t| ≤ max(|x −x |,|t−x |) ≤ max(v −v +∆,∆) =
h j j h h j j h j
v −v +∆. Otherwise, if x ≥ x , since |x −t| = |x −x |+|x −t| in this case, we have
h j h j h h j j
|x −t| = x −x +(x −x )−(v −v ) ≤ v −v +∆,
h h j j i j i h j
which follows from |x −x | = x −x ≤ v −v +∆.
h i h i h i
We remark that Lemma 5 applies exactly to moves of the form induced by Lemma 4, regardless of
the case of Lemma 4 we are in. We symmetrically have the following claim.
Lemma6. Leth,j,k ∈ [n]withh < j < k. Supposethatfor∆ ≥ 0andv,x ∈ Rn withv ≤ v ≤ v ,
h j k
v −v ≤ |x −x | ≤ v −v +∆, |x −x | ≤ v −v +∆, |x −x | ≤ v −v +∆.
k j k j k j h j j h h k k h
Let t = x −sign(x −x )·ζ for ζ ∈ [0,|x −x |−(v −v )]. Then, |x −t| ≤ v −v +∆.
j j k k j k j h j h
Our rounding procedure design (parameterized by A(cid:101),˜b) is motivated by the observations in Lem-
mas 4, 5, and 6. We fix three sets of index pairs in [n]×[n] as follows:
S := S ∪S ∪S , L := ⌊log (n−1)⌋,
base left right 2
L
(cid:91)(cid:110)(cid:16) (cid:17) (cid:111)
S := ([n]×[n])∩ (i−1)2ℓ+1,i2ℓ+1 | i ∈ [2L−ℓ] ,
base
ℓ=0
L ℓ−1 (20)
(cid:91) (cid:91) (cid:110)(cid:16) (cid:17) (cid:111)
S := ([n]×[n])∩ (i−1)2ℓ−2m+1,i2ℓ+1 | i ∈ [2L−ℓ] ,
left
ℓ=1m=0
L ℓ−1
(cid:91) (cid:91) (cid:110)(cid:16) (cid:17) (cid:111)
S := ([n]×[n])∩ (i−1)2ℓ+1,i2ℓ+2m+1 | i ∈ [2L−ℓ] .
right
ℓ=1m=0
15We also call the following set of indices in S the ℓth layer of constraints:
(cid:16) (cid:110)(cid:16) (cid:17) (cid:111)(cid:17)
S := ([n]×[n])∩ (i−1)2ℓ+1,i2ℓ+1 | i ∈ [2L−ℓ] . (21)
ℓ
In other words, for each 0 ≤ ℓ ≤ L, the ℓth layer of constraints is entirely due to S , and consists
base
of index pairs in [n]×[n] spaced 2ℓ coordinates apart. There is a corresponding set of constraints
due to S which extends the ℓth layer of constraints, and consists of index pairs in [n] × [n]
left
spaced 2ℓ + 2m coordinates apart for each 0 ≤ m ≤ ℓ − 1, offset by 2ℓ coordinates each, where
the left endpoint is extended 2m beyond the corresponding constraint in S . We define S
base right
symmetrically, extending right endpoints of intervals in S by 2m instead.
base
Our rounding procedure fixes Lipschitz constraints one layer at a time using Lemma 4. We use
Lemma 5 to make sure that fixing a higher layer does not worsen the violation of any lower layer, by
using adjacent constraints in S to protect the lower layer. Similarly, we use Lemma 6 to protect
right
lower layers using adjacent constraints in S . This also explains our inclusion of the constraints
left
S , S . For completeness, we give a discussion in Section 3.2 on why using only S would
left right base
resultinlosingalogarithmicfactorintheroundingguarantee, yieldingasimilarlossintheruntime.
For example, if n = 9, so L = 3, S defined in (20) consists of the pairs
S = {(i,i+1) | i ∈ [8]}∪{(2i−1,2i+1) | i ∈ [4]}∪{(4i−3,4i+1) | i ∈ [2]}∪{(1,9)},
base
S = {(2i−2,2i+1) | 2 ≤ i ≤ 4}∪{(4,9)}∪{(3,9)},
left
S = {(2i−1,2i+2) | i ∈ [3]}∪{(1,6)}∪{(1,7)}.
right
We let A(cid:101), ˜b be scaled row subsets of A, b corresponding to S, i.e. following notation in (11), (20),
A(cid:101) ∈ R2|S|×n has A(cid:101)(s,+): := A (s,+):, A(cid:101)(s,−): := A
(s,−):
for all s ∈ S,
(22)
˜b ∈ R2|S| has ˜b := b , ˜b := b for all s ∈ S.
(s,+) (s,+) (s,−) (s,−)
WenextshowhowtorecursivelyapplyLemma4toshowthereexistsanefficient(A(cid:101),˜b,∞)-inequality
rounding procedure for (A,b,c,X), recalling the definition of rounding procedures in Definition 6.
We begin by observing that it suffices to enforce the 0th layer of constraints in S to show feasibility.
Lemma 7. If x,v ∈ Rn, where v has monotonically nondecreasing coordinates, and |x −x | ≤
i i+1
v −v for all i ∈ [n−1], then |x −x | ≤ v −v for all (i,j) ∈ [n]×[n] with i < j.
i+1 i i j j i
Proof. This follows from the triangle inequality:
j−1 j−1
(cid:88) (cid:88)
|x −x | ≤ |x −x | ≤ v −v = v −v .
i j k k+1 k+1 k j i
k=i k=i
We can now state and analyze our rounding procedure.
Lemma 8. Let (A,b,c,X) be defined as in (11), and let (A(cid:101),˜b) be defined as in (20), (22). There
exists Round, a (A(cid:101),˜b,∞)-inequality rounding procedure for (A,b,c,X) running in O(n) time.
16Proof. For a fixed x ∈ X, let
(cid:13) (cid:16) (cid:17)(cid:13)
∆ := (cid:13) (cid:13)max A(cid:101)x−˜b,0
2|S|
(cid:13)
(cid:13)
,
∞
so that ∆ is the maximum violation of any Lipschitz constraint in S at the start of the algorithm.
We claim we can produce x′ = Round(x) ∈ X with Ax′ ≤ b (i.e. x′ satisfies all of the Lipschitz
constraints in [n]×[n]), such that ∥x−x′∥ ≤ ∆. This implies Round is a (A(cid:101),˜b,∞)-inequality
∞
rounding procedure as desired, because (9) then follows from c ∈ [−1, 1]n =⇒ ∥c∥ ≤ 1, and
n n 1
c⊤(x′−x) ≤ ∥c∥ 1(cid:13) (cid:13)x′−x(cid:13) (cid:13)
∞
≤ ∆ = (cid:13) (cid:13) (cid:13)max(cid:16) A(cid:101)x−˜b,0 2|S|(cid:17)(cid:13) (cid:13)
(cid:13)
.
∞
Next, we show how to produce x′ ∈ X satisfying Ax′ ≤ b and ∥x−x′∥ ≤ ∆. Without loss of
∞
generality, let n = 2L+1; otherwise, we create O(n) additional coordinates of x and v, all equal to
x and v . We inductively apply Lemma 4 to each layer of constraints 0 ≤ ℓ ≤ L, starting from
n n
ℓ ← L and x(L+1) ← x. Each round of the algorithm corresponds to a single layer ℓ, takes as input
a point x(ℓ+1) ∈ X, and modifies it to produce x(ℓ) ∈ X as output. At the end of round ℓ (and
therefore the start of ℓ−1), we maintain the following invariants on x(ℓ).
1. Every Lipschitz constraint s in the ℓth layer holds, i.e. max(A x,A x) ≤ b .
(s,+): (s,−): (s,+)
2. max(A x−b ,A x−b ) ≤ ∆ for all s ∈ S .
(s,+): (s,+) (s,−): (s,+) base
3. ∥x(ℓ)−x(ℓ+1)∥ ≤ ∆.
∞
4. Only coordinates of the form j = i2ℓ+1 for odd i ∈ Z have x(ℓ+1) ̸= x(ℓ).
≥0 j j
Suppose for induction that Items 1, 2, 3, and 4 all hold for all rounds in [ℓ,L], and consider the
(ℓ−1)th round. We apply Lemma 4 to each (ℓ−1)th layer constraint, where the maximum violation
of any constraint is ∆ due to Item 2 from the previous round. By construction, Lemma 4 preserves
Items 3 and 4, and enforces Item 1 after completion. We are only left with Item 2. To see that this
is true, note that for each mth level constraint for m < ℓ−1, it can share an endpoint with at most
one (ℓ−1)th level constraint; if it shares no endpoints, correctness of Item 2 clearly still holds. If it
shares one endpoint, depending on whether the endpoint is on the left or the right of the mth level
constraint, we use Lemma 5 or 6, as well as existence of the relevant constraint in S or S ,
left right
to ensure that Item 2 holds after applying Lemma 4. Finally, at the end of the algorithm, each
coordinate has moved at most once, and by at most ∆, via Items 3 and 4. Because all the 0th-layer
constraints hold by Item 1, we have feasibility of x(0) via Lemma 7, and can set x′ ← x(0).
To bound the runtime, note that applying Lemma 4 to each set of three coordinates corresponding
to a consecutive pair of constraints takes O(1) time, and |S| = O(n) because of the following:
k k ℓ−1 k
(cid:88) (cid:88) (cid:88) (cid:88)
|S| ≤ 2k−ℓ+2 2k−ℓ ≤ 2k+1+2 ℓ2k−ℓ
ℓ=0 ℓ=1m=0 ℓ=1
k k
(cid:88) (cid:88)
≤ 2k+1+2 k2k−ℓ−2 (k−ℓ)2k−ℓ
ℓ=1 ℓ=1
≤ 2k+1+2k2k −2(k−2)2k ≤ 2n+4n = O(n).
17By combining Lemma 8 with Corollary 2, we have our main result of this section.
Proposition 2. Let ε ≥ 0. We can compute x ∈ X, an ε-approximate minimizer to (11), in time
(cid:18) (cid:19)
nlog(n)
O .
ε
Further, the objective value of x in (11) is an ε-additive approximation of smCE(D(cid:98)n).
Proof. Observe that for A(cid:101) defined in (22), we have ∥A(cid:101)∥
∞→∞
= 2 and nnz(A(cid:101)) = 4|S| = O(n).
Therefore, Corollary 2 shows we can compute an ε-approximate minimizer to
(cid:13) (cid:16) (cid:17)(cid:13)
min c⊤x+(cid:13) (cid:13)max A(cid:101)x−˜b,0
2|S|
(cid:13)
(cid:13)
x∈[−1,1]n ∞
within the stated runtime. Finally, the definition of Round from Lemma 8 shows that given this
ε-approximate minimizer, we can then produce an ε-approximate minimizer to (11) in O(n) time.
The last claim in the lemma statement follows immediately from the definition of (11).
3.2 Sufficiency and insufficiency of S
base
Inthissection,wegiveabriefdiscussionofanalternativestrategytothatinSection3.1. Specifically,
suppose we let S in (20) only consist of the pairs in S base. We instead define (A(cid:101),˜b) as follows:
A(cid:101) ∈ R2|S base|×n has A(cid:101)(s,+): := (L+1)A (s,+):, A(cid:101)(s,−): := (L+1)A
(s,−):
for all s ∈ S base,
(23)
˜b ∈ R2|S base| has ˜b := (L+1)b , ˜b := (L+1)b for all s ∈ S .
(s,+) (s,+) (s,−) (s,−) base
Here, L := ⌊log (n−1)⌋ as in (20). In other words, A(cid:101),˜b take a subset of rows of A,b and scale
2
them up by L+1. We first show that enforcing this simpler set of constraints yields a rounding
procedure losing only a logarithmic factor in quality over the more complicated Lemma 8.
Lemma 9. Let (A,b,c,X) be defined as in (11), and let (A(cid:101),˜b) be defined as in (20), (23). There
exists Round, a (A(cid:101),˜b,∞)-inequality rounding procedure for (A,b,c,X) running in O(n) time.
Proof. For a fixed x ∈ X, let
1 (cid:13) (cid:16) (cid:17)(cid:13)
∆ :=
L+1
(cid:13) (cid:13)max A(cid:101)x−˜b,0
2|S|
(cid:13)
(cid:13)
∞,
so that ∆ is the maximum violation of any Lipschitz constraint in S at the start of the algorithm,
since we have undone the scaling by L+1. We claim we can produce x′ = Round(x) ∈ X with
Ax′ ≤ b (i.e. x′ satisfies all of the Lipschitz constraints in [n]×[n]), such that ∥x−x′∥ ≤ δ(L+1).
∞
The proof that Round is a (A(cid:101),˜b,∞)-inequality rounding procedure is then identical to Lemma 8.
To produce such x′, we again apply Lemma 4 to one layer of constraints at a time, producing a
sequence {x(ℓ)}L+1. Specifically, in place of the invariants in the proof of Lemma 8, we instead
ℓ=0
enforce the following invariants on x(ℓ), for all 0 ≤ ℓ ≤ L.
1. Every Lipschitz constraint s in the ℓth layer of constraints is satisfied, i.e. A x ≤ b .
s: s
182. ∥x(ℓ)−x(ℓ+1)∥ ≤ ∆(L+1−ℓ).
∞
3. Only coordinates of the form j = i2ℓ+1 for odd i ∈ Z have x(ℓ+1) ̸= x(ℓ).
≥0 j j
The proof that Item 2 is inductively maintained replaces the use of Lemmas 5, 6 with the triangle
inequality, since each constraint is originally violated by at most ∆, and Lemma 4 moves ℓth-layer
constraints by at most ∆(L+1−ℓ) via Item 2. The rest of the proof is identical to Lemma 8.
By plugging in Lemma 9 into Corollary 2 instead of Lemma 8, we lose a log(n) factor in runtime,
since ∥A(cid:101)∥
∞→∞
= 2(L+1) = O(log(n)). We next show this loss is inherent, if we only enforce S base.
Lemma 10. Let L ∈ N, let n = 2L +1, and define A,b as in (11), and S as in (20). There
base
exists x ∈ [−1,1]n and ∆ ∈ [0,1] such that, letting [Ax−b] denote the 2|S | coordinates of
S base base
the vector Ax−b corresponding to elements of S ,
base
(cid:13) (cid:0) (cid:1)(cid:13)
(cid:13)max [Ax−b] S base,0 2|S base| (cid:13) ∞ = ∆,
but ∥max([Ax−b] ,0 )∥ > 0 for any x′ ∈ [−1,1]n with ∥x′−x∥ ≤ L∆.
S base 2|S base| ∞ ∞ 2
Proof. Let L ∈ N, n = 2L+1, and let L∆ ≤ 1. We consider the following hard example:
x = (L−⌈log (2L−j +1)⌉)∆ for all j ∈ [n−1] and x = L∆,
j 2 n
v = 0 for all j ∈ [n−1] and v = L∆.
j n
For example, if L = 3 so n = 9, we have
x = ... = x = 0, x = x = ∆, x = 2∆, and x = x = 3∆.
1 4 5 6 7 8 9
We first show that none of the constraints is violated by more than ∆ in the set S for our
base
construction. The Lth layer is not violated because x −x = v −v . Now, for the ℓth layer where
n 1 n 1
0 ≤ ℓ < L, we want to show that for any i ∈ [2L−ℓ], the constraint ((i−1)2ℓ+1,i2ℓ+1) is violated
by at most ∆. If L = 2L−ℓ, this is obvious because
(cid:12) (cid:12)
(cid:12)x −x (cid:12) ≤ v −v = L∆.
(cid:12) n (i−1)2ℓ+1(cid:12) n (i−1)2ℓ+1
Otherwise, because |⌈log (2L−(i−1)2ℓ)⌉−⌈log (2L−i2ℓ)⌉| ≤ 1 for i < 2L−ℓ,
2 2
(cid:12) (cid:12)
(cid:12)x −x (cid:12) ≤ ∆ = ∆+v −v .
(cid:12) i2ℓ+1 (i−1)2ℓ+1(cid:12) i2ℓ+1 (i−1)2ℓ+1
Finally, if x′ satisfies all Lipschitz constraints, x′ = x′ , so one of x or x must move by L∆.
1 2L 1 2L 2
3.3 Testing via smooth calibration
In this section, we build upon Proposition 2 and give algorithms that solve the testing problems in
Definitions 2 and 4. We begin with a result from [BGHN23a] which bounds how well the smooth
calibration of an empirical distribution approximates the smooth calibration of the population.
19Lemma 11 (Corollary 9.9, [BGHN23a]). For any ε ∈ (0,1), there is an n = O( ε1 2) such that if D(cid:98)n
is the empirical distribution over n i.i.d. draws from D, with probability ≥ 2,
3
(cid:12) (cid:12)
(cid:12) (cid:12)smCE(D)−smCE(D(cid:98)n)(cid:12)
(cid:12)
≤ ε.
Further, we recall the smooth calibration error is constant-factor related to the LDTC.
Lemma 12 (Theorem 7.3, [BGHN23a]). For any distribution D over [0,1]×{0,1}, we have
1
dCE(D) ≤ smCE(D) ≤ 2dCE(D).
2
For completeness, we make the simple (but to our knowledge, new) observation that, while the
constants in Lemma 12 are not necessarily tight, there is a constant gap between dCE and smCE.
Lemma 13. Suppose for constants B ≥ A > 0, it is the case that A · dCE(D) ≤ smCE(D) ≤
B·dCE(D) for all distributions D over [0,1]×{0,1}. Then, B ≥ 3.
A 2
Proof. First, we claim that A ≤ 1. To see this, let (v,y) ∼ D be distributed where v = 1 with
2
probability 1, and y ∼ Bern(1 + ε) for some ε ∈ [0, 1]. Clearly, smCE(D) = |1 − (1 + ε)| = ε.
2 2 2 2
Moreover, dCE(D) = ε, which follows from the same Jensen’s inequality argument as in Lemma 2,
so this shows that A ≤ 1. Next, we claim that B ≥ 3, concluding the proof. Consider the joint
2
distribution over (u,v,y) in Table 3.3, and let D be the marginal of (v,y). It is straightforward to
Probability mass u v y w(v)
1 1 1 −ε 1 1
2 2 2
1 1 1 0 1−ε
2 2 2
check(u,y)iscalibratedandE|u−v| = ε, sodCE(D) ≤ ε. Moreover, smCE(v,y) ≥ 3ε, aswitnessed
2 2 4
by the Lipschitz weight function w(v) in Table 3.3, finishing our proof that B ≥ 3:
2
(cid:18)(cid:18) (cid:19) (cid:19) (cid:18)(cid:18) (cid:19) (cid:19)
1 1 1 1 3ε
smCE(v,y) ≥ E[(y−v)w(v)] = +ε ·1 + − ·(1−ε) = .
2 2 2 2 4
Using these claims, we now give our tolerant calibration tester in the regime ε > 4ε .
1 2
Theorem 3. Let 0 ≤ ε ≤ ε ≤ 1 satisfy ε > 4ε , and let n ≥ C · 1 for a universal
2 1 1 2 tct (ε1−4ε2)2
constant C . There is an algorithm A which solves the (ε ,ε )-tolerant calibration testing problem
tct 1 2
with n samples, which runs in time
(cid:18) (cid:19)
nlog(n)
O .
ε −4ε
1 2
Proof. Throughout the proof, let α := ε1 −2ε > 0. Consider the following algorithm.
2 2
1. Sample n ≥ C 4αtc 2t samples to form an empirical distribution D(cid:98)n, where C
tct
is chosen large
enough so that Lemma 11 guarantees |smCE(D)−smCE(D(cid:98)n)| ≤ α
4
with probability ≥ 2 3.
202. Call Proposition 1 with ε ← α
4
to obtain β, an α 4-additive approximation to |smCE(D(cid:98)n)|.
3. Return “yes” if β ≤ 2ε + α, and return “no” otherwise.
2 2
Conditioned on the event that |smCE(D)−smCE(D(cid:98)n)| ≤ α 4, we show that the algorithm succeeds
in tolerant calibration testing. First, if dCE(D) ≤ ε , then smCE(D) ≤ 2ε by Lemma 12, and
2 2
therefore by the guarantee of Proposition 1 and the assumed success of Lemma 11, the algorithm
will return “yes.” Second, if dCE(D) ≥ ε , then smCE(D) ≥ ε1 by Lemma 12, and similarly the
1 2
algorithm returns “no” in this case. Finally, the runtime is immediate from Proposition 2 and the
definition of α.
Theorem 3 has the following implication for (standard) calibration testing.
Corollary 3. Let n ∈ N and let ε ∈ (0,1) be minimal such that it is information-theoretically
n
possible to solve the ε -calibration testing problem with n samples. For some ε = Θ(ε ), there is an
n n
algorithm A which solves the ε-calibration testing problem with n samples, which runs in time
O(cid:0) n1.5log(n)(cid:1)
.
Proof. Recall from Lemma 2 that ε = Ω(n−1/2). The conclusion follows by applying Theorem 3
n
with ε ← Θ(ε ) and ε ← 0.
1 n 2
4 Lower distance to calibration
In this section, we provide our main result on approximating the lower distance to calibration of
a distribution on [0,1]×{0,1}. In Section 4.1, we state some preliminary definitions and results
from [BGHN23a] which are used in our algorithm. In Section 4.2, we then develop a rounding
procedurecompatiblewithalinearprogramwhichcloselyapproximatestheempiricallowerdistance
to calibration. Finally, in Section 4.3, we use our rounding procedure to design an algorithm for
calibration testing, which can solve the testing problem for a larger range of parameters than
Theorem 4 (i.e. the entire relevant parameter range), at the cost of a slight runtime overhead.
4.1 LDTC preliminaries
In this section, we collect preliminaries for our testing algorithm based on estimating the LDTC.
First,analogouslytoLemma11,werecallaboundfrom[BGHN23a]onthedeviationoftheempirical
estimate of dCE(D) from the population truth which holds with constant probability.
Lemma 14 (Theorem 9.10, [BGHN23a]). For any ε ∈ (0,1), there is an n = O( 1 ) such that, if
ε2
D(cid:98)n is the empirical distribution over n i.i.d. draws from D, with probability ≥ 32,
(cid:12) (cid:12)
(cid:12) (cid:12)dCE(D)−dCE(D(cid:98)n)(cid:12)
(cid:12)
≤ ε.
Next, given a set U ⊂ [0,1], we provide an analog of Definition 1 which is restricted to U.
Definition 7 (U-LDTC). Let U ⊂ [0,1], and let D be a distribution over [0,1]×{0,1}. Define
extU(D) to be all joint distributions Π over (u,v,y) ∈ U×[0,1]×{0,1}, with the following properties.
21• The marginal distribution of (v,y) is D.
• The marginal distribution (u,y) is perfectly calibrated, i.e. E [y|u] = u.
Π
The U-lower distance to calibration (U-LDTC) of D, denoted dCEU(D), is defined by
dCEU(D) := inf E |u−v|.
(u,v,y)∼Π
Π∈extU(D)
Note that if we require {0,1} ⊂ U, then extU(D) is always nonempty, because we can let u = y
with probability 1. We also state a helper claim from [BGHN23a], which relates dCEU to dCE.
Lemma 15 (Lemma 7.11, [BGHN23a]). Let D be a distribution over [0,1]×{0,1}, and let U be a
finite ε-covering of [0,1] satisfying {0,1} ⊆ U. Then, dCE(D) ≤ dCEU(D) ≤ dCE(D)+ε.
2
To this end, in the rest of the section we define, for any ε ∈ (0,1),
(cid:26) (cid:20)(cid:22) (cid:23)(cid:21)(cid:27)
iε 2
U := {0,1}∪ | i ∈ , (24)
ε
2 ε
which is an ε-cover of [0,1] satisfying |U | = O(1). Finally, we state a linear program whose value
2 ε ε
is equivalent to dCEU(D), when the first marginal of D is discretely supported.
Lemma 16 (Lemma 7.6, [BGHN23a]). Let U,V ⊂ [0,1] be discrete sets, where {0,1} ⊂ U, and
let D be a distribution over V × {0,1}, where for (v,y) ∈ V × {0,1} we denote the probability
of (v,y) ∼ D by D(v,y). The following linear program with 2|U||V| variables Π(u,v,y) for all
(u,v,y) ∈ U ×V ×{0,1}, is feasible, and its optimal value equals dCEU(D):
(cid:88)
min |u−v|Π(u,v,y)
Π∈R2|U||V|
≥0 (u,v,y)∈U×V×{0,1}
(cid:88)
such that Π(u,v,y) = D(v,y), for all (v,y) ∈ V ×{0,1},
u∈U
(cid:88) (cid:88)
and (1−u) Π(u,v,1) = u Π(u,v,0), for all u ∈ U.
v∈V v∈V
4.2 Rounding for empirical U-LDTC
Analogously to Section 3.1, in this section we fix a dataset under consideration,
D(cid:98)n := {(v i,y i)}
i∈[n]
⊂ [0,1]×{0,1},
and the corresponding empirical distribution, also denoted D(cid:98)n, where (v,y) ∼ D(cid:98)n means (v,y) =
(v ,y ) with probability 1 for each i ∈ [n]. We let V := {v } be identified with [n] in the natural
i i n i i∈[n]
way. Moreover, for a fixed parameter ε ∈ (0,1) throughout, we let U := U defined in (24). Finally,
ε
we denote m := |U| = O(1), and let U ∈ [0,1]m×m be the diagonal matrix whose diagonal entries
ε
correspond to U. We also identify elements of U with j ∈ [m] in an arbitrary but consistent way,
writing u ∈ [0,1] to mean the jth element of U according to this identification.
j
We next rewrite the linear program in Lemma 16 into a more convenient reformulation.
22Lemma 17. The linear program in Lemma 16 can equivalently be written as:
(cid:18)
x ∈
Rmn(cid:19)
dCEU(D(cid:98)n) := m x∈i Xn c⊤x, where X := ∆2mn and we denote x = x0
1
∈ Rmn
, (25)
Mx= n11 n
UB0x0=(Im−U)B1x1
where we define c ∈ R2mn, M ∈ Rn×2mn, and B ,B ∈ Rm×mn by
0 1
c := |u −v | for all (i,j,k) ∈ [n]×[m]×{0,1},
(i,j,k) j i
(cid:40)
1 y = k, i′ = i
M := i for all i′ ∈ [n],(i,j,k) ∈ [n]×[m]×{0,1},
i′,(i,j,k) 0 else
(cid:40)
1 j = j′, k = 0
and [B ] := for all j′ ∈ [m],(i,j,k) ∈ [n]×[m]×{0,1},
0 j′,(i,j,k) 0 else
(cid:40)
1 j = j′, k = 1
[B ] := for all j′ ∈ [m],(i,j,k) ∈ [n]×[m]×{0,1}.
1 j′,(i,j,k) 0 else
Proof. This is clear from observation, but we give a brief explanation of the notation. First, x ∈ X
represents the density function of our joint distribution Π over U × V × {0,1}, and has 2mn
coordinates identified with elements (i,j,k) ∈ V ×U×{0,1} ≡ [n]×[m]×{0,1}. We let the subset
of coordinates with k = 0 be denoted x ∈ Rmn, defining x similarly. Recalling the definition of
0 1
the linear program in Lemma 16, x is indeed reweighted by c = |u −v |.
(i,j,k) (i,j,k) j i
Next, M represents the marginal constraints in Lemma 16, and enforcing Mx = 11 is equivalent
n n
to the statement that, for each i′ ∈ [n], the sum of all entries (i,j,k) of x with i = i′ and k = y is
i
n1, since that is the probability density assigned to (v i′,y i′) by the distribution D(cid:98)n.
Lastly, the jth calibration constraint in Lemma 16 is enforced by the jth row of the equation
UB x = (I −U)B x , which reads u ⟨[B ] ,x ⟩ = (1−u )⟨[B ] ,x ⟩. We can check by the
0 0 m 1 1 j 0 j: 0 j 1 j: 1
definitions of [B ] , [B ] that this is consistent with our earlier calibration constraints.
0 j: 1 j:
We give a convenient way of visualizing the marginal and calibration constraints described in
Lemma 17. For convenience, we identify each x ∈ ∆2mn with an m×2n matrix
mat(x) = X = (cid:0) X ∈ Rm×n X ∈ Rm×n(cid:1) , (26)
0 1
whereX consistsofentriesofx arrangedinamatrixfashion(withrowscorrespondingto[m] ≡ U
0 0
and columns corresponding to [n] ≡ V), and similarly X is a rearrangement of x , recalling (25).
1 1
When explaining how we design our rounding procedures to modify X to satisfy constraints, it will
be helpful to view entries of X as denoting an amount of physical mass which we can move around.
Thereare2ncolumnsinX,correspondingtopairs(i,k) ∈ V×{0,1};amongthese,wesayncolumns
are “active,” where column (i,k) is active iff y = k, and we say the other n columns are “inactive.”
i
Following notation (26), the marginal constraints X = 11 simply ask that the total amount of
n n
mass in each active column is 1, so there is no mass in any inactive column since x ∈ ∆2mn.
n
23Moreover, there are m rows in X, each corresponding to some j ∈ U. If we let ℓ denote the amount
j
of mass on [X ] and r the amount of mass on [X ] , the jth calibration constraint simply asks
0 j: j 1 j:
that u ℓ = (1−u )r , i.e. it enforces balance on the amount of mass in each row’s two halves.
j j j j
Finally, for consistency with Definition 6, the linear program in (25) can be concisely written as
min c⊤x, where A :=
(cid:18) M(cid:19)
, B := (cid:0) UB −(I −U)B (cid:1) , b :=
(cid:18) n11 n(cid:19)
(27)
x∈X B 0 m 1 0 m
Ax=b
and c, X are as defined in (25). In the rest of the section, following Definition 6, we develop an
equality rounding procedure for the equality-constrained linear program in (27) in two steps.
1. In Lemma 18, we first show how to take x ∈ X with ∥Mx− 11 ∥ = ∆, and produce x′ ∈ X
n n 1
such that Mx′ = 11 (i.e. x′ now satisfies the marginal constraints) and ∥x−x′∥ = O(∆).
n n 1
2. In Lemma 20, we then consider x ∈ X such that, following the notation (25), ∥UB x −(I −
0 0 m
U)B x ∥ = ∆. We show how to produce x′ ∈ X such that Mx = Mx′ (i.e. the marginals of
1 1 1
x′ are unchanged), UB x′ = (I −U)B x′ (i.e. x′ is calibrated), and ⟨c,x′−x⟩ = O(∆).
0 0 m 1 1
OurroundingprocedureusesLemma18tosatisfythemarginalconstraintsin(25), andthenapplies
Lemma 20 to the result to satisfy the calibration constraints in (25) without affecting the marginal
constraints. By leveraging the stability guarantees on these steps, we can show this is indeed a valid
rounding procedure in the sense of (5). We now give our first step for marginal satisfication.
Lemma 18 (Marginal satisfaction). Following notation in (25), let x ∈ X satisfy ∥Mx− 11 ∥ =
n n 1
∆. There is an algorithm which runs in time O(mn), and returns x′ with
Mx′ = n1 1 n, (cid:13) (cid:13)x−x′(cid:13) (cid:13)
1
≤ 2∆.
Proof. Recall for i ∈ [n], we say column i of X is active if y = 0, and similarly column i of X is
0 i 1
active if y = 1. We call I the set of n inactive columns, and partition A, which we call the set of n
i
active columns, into three sets A>, A=, and A<, where A> are the columns whose sums are > 1,
n
A< are the columns whose sums are < 1, and A= are the remaining columns. Hence, every column
n
of X belongs to I, A>, A=, or A≤. Note that until |A=| = n, we can never have A< = ∅, since this
means all column sums in A are ≥ 1 (with at least 1 strict inequality), contradicting x ∈ X.
n
We first take columns i ∈ A> one at a time, and pair them with an arbitrary column in i′ ∈ A<,
moving mass from column i arbitrarily to column i′ until either column i or column i′ enters A=.
Wechargethismovementtothemarginalconstraintscorrespondingtoiandi′,sincetheconstraints
were violated by the same amount as the mass being moved. After this process is complete, A> is
empty, and we only moved mass from columns originally in A> to columns originally in A<.
Next, we take columns i ∈ I one at a time, and pair them with an arbitrary column i′ ∈ A<,
moving mass until either column i is 0 or column i′ enters A=. We can charge half this movement
m
to the marginal constraint corresponding to i′, since the sign of the marginal violation stays the
same throughout. Hence, the overall movement is ≤ 2∆. After this is complete, all columns in I are
0 and all columns in A are in A=, so we can return x′ ∈ ∆2mn corresponding to the new matrix.
m
It is clear both steps of this marginal satisfaction procedure take O(mn) time, since we can sequen-
tially process columns in A− until they enter A=, and will never be considered again.
24We next describe a procedure which takes x ∈ ∆2mn, and modifies it to satisfy the calibration
constraints UB x = (I −U)B x without changing the marginals Mx. We first provide a helper
0 m 1
lemma used in our rounding procedure, which describes how to fix the jth marginal constraint.
Lemma 19. Let x ∈ ∆2mn and X := mat(x) as defined in (26). Let j ∈ [m] correspond to an
element u ∈ U, let ℓ := ∥[X ] ∥ , r := ∥[X ] ∥ , and let ∆ := |u ℓ −(1−u )r |. There exists
j j 0 j: 1 j 1 j: 1 j j j j j
j′ ∈ [m] such that we can move mass from only X to X , resulting in Rm×2n ∋ X′ ≡ x′ ∈ ∆2mn
j: j′:
such that Mx′ = Mx, u ∥[X′] ∥ = (1−u )∥[X′] ∥ , and ⟨c,x′−x⟩ ≤ ∆ .
j 0 j: 1 j 1 j: 1 j
Proof. Without loss of generality, suppose that the row j = 1 corresponds to u = 0, and j = m
j
corresponds to u = 1. We split the proof into two cases, depending on the sign of u ℓ −(1−u )r .
j j j j j
Case 1: u ℓ > (1−u )r . We let j′ = 1, i.e. we only move mass from the jth row to the first row.
j j j j
Specifically, we leave [X ] unchanged, and move mass from [X ] to [X ] , making sure to only
1 j: 0 j: 0 1:
move mass in the same column. The total amount of mass we must delete from [X ] is
0 j:
1−u u ℓ −(1−u )r ∆
j j j j j j
ℓ − ·r = = .
j j
u u u
j j j
Our strategy is to arbitrarily move mass within columns until we have deleted ∆j total mass. If we
uj
denote the mass moved in column i ∈ [n] as δ , and let x′ be the result after the move,
ij
(cid:10) c,x′−x(cid:11)
≤
(cid:88)
|c −c |δ =
(cid:88)
||u −v |−|u −v ||δ ≤
(cid:88)
u δ = ∆ .
(i,1,0) (i,j,0) ij j i 1 i ij j ij j
i∈[n] i∈[n] i∈[n]
Here,thefirstinequalitywasthetriangleinequality,thefirstequalityusedthedefinitionofcin(25),
the second inequality used u = 0 and the triangle inequality, and the last used (cid:80) δ = ∆j.
1 i∈[n] ij uj
Case 2: u ℓ < (1−u )r . This case is entirely analogous; we move mass arbitrarily from row j to
j j j j
row m, i.e. the last row with u = 1. The amount of mass we must move is
m
u (1−u )r −u ℓ ∆
j j j j j j
r − ℓ = = .
j j
1−u 1−u 1−u
j j j
Again denoting the amount of mass moved from column i ∈ [n] as δ , the claim follows:
ij
(cid:10) c,x′−x(cid:11)
≤
(cid:88)
||u −v |−|u −v ||δ ≤
(cid:88)
(1−u )δ = ∆ .
j i m i ij j ij j
i∈[n] i∈[n]
By iteratively applying Lemma 19, we have our marginal-preserving calibration procedure.
Lemma 20 (Marginal-preserving calibration). Following the notation (27), given x ∈ ∆2mn with
∥Bx∥ = ∆, we can compute x′ with Mx′ = Mx, Bx′ = 0 , and ⟨c,x′−x⟩ ≤ ∆ in O(mn) time.
1 m
Proof. It suffices to apply Lemma 19 to each row i ∈ [m]. All of the movement in the rows
i ∈ [2,m−1] are independent of each other, and do not affect the imbalance in the rows i ∈ {1,m}
when we have finished applying Lemma 19, since e.g. u ℓ = 0 regardless of how much mass is
1 1
25moved to [X ] , and a similar property holds for the mth row. The total change in ⟨c,x′−x⟩ is
0 1:
thus boundable by (cid:80) ∆ ≤ ∆, and applying Lemma 19 to each row takes O(n) time. Finally,
j∈[m] j
Mx = Mx′ followsbecauseweonlymovemasswithinthesamecolumn,sonomarginalchanges.
By combining Lemma 18 with Lemma 20, we can complete our rounding procedure.
Lemma 21. Let (A,b,c,X) be defined as in (25), (27), and let (A(cid:101),˜b) := (4A,4b). There exists
Round, a (A(cid:101),˜b,1)-equality rounding procedure for (A,b,c,X), running in O(mn) time.
Proof. Throughout the proof, let ∆ M := (cid:13) (cid:13)Mx− n11 n(cid:13) (cid:13) 1 and ∆ B := ∥Bx∥ 1, following the notation
(27). We also denote the total violation by
(cid:13) (cid:13)
∆ := (cid:13) (cid:13)A(cid:101)x−˜b(cid:13)
(cid:13)
= 4∆ M+4∆ B.
1
We first apply Lemma 18 to x to produce x˜ satisfying ∥x−x˜∥ ≤ 2∆ and Mx˜ = 11 , in O(mn)
1 M n n
time. Note that, because ∥B∥ ≤ 1 since all columns of B are 1-sparse, we have
1→1
∥Bx˜∥ ≤ ∥Bx∥ +∥B∥ ∥x−x˜∥ ≤ ∆ +2∆ .
1 1 1→1 1 B M
Next, we apply Lemma 20 to x˜, resulting in x′ with Mx′ = 11 , Bx′ = 0 , and ⟨c,x′−x˜⟩ ≤
n n m
∆ +2∆ , in O(mn) time. Recalling the definition (8), we have the conclusion from ∥c∥ ≤ 1, so
B M ∞
c⊤(x′−x) ≤ c⊤(x˜−x)+c⊤(x′−x˜)
≤ ∥c∥ ∥x˜−x∥ +c⊤(x′−x˜) ≤ 2∆ +∆ +2∆ ≤ ∆.
∞ 1 M B M
We conclude by applying the solver from Corollary 1 to our resulting unconstrained linear program.
Proposition 3. Let ε ≥ 0. We can compute x ∈ X, an ε-approximate minimizer to (25), in time
(cid:18) (cid:19)
nlog(n)
O .
ε2
Further, the objective value of x in (25) is a 2ε-additive approximation of dCE(D(cid:98)n).
Proof. Observe that for A(cid:101) = 4A, we have ∥A(cid:101)∥
1→1
≤ 8 and nnz(A(cid:101)) = O(mn), since no column is
more than 2-sparse and all entries of A are in [−1,1]. Further, recalling the definition of U from
(24), we have m = O(1). So, Corollary 1 shows we can compute an ε-approximate minimizer to
ε
(cid:13) (cid:13)
min c⊤x+(cid:13)A(cid:101)x−˜b(cid:13)
(cid:13) (cid:13)
x∈∆2mn 1
within the stated runtime. The rest of the proof follows as in Proposition 2, using Round from
Lemma21,wherewerecall|dCE(D(cid:98)n)−dCEU(D(cid:98)n)| ≤ εduetoourdefinitionofU andLemma15.
264.3 Testing via LDTC
We now give analogs of Theorems 3 and Corollary 3, using our solver in Proposition 3.
Theorem 4. Let 0 ≤ ε ≤ ε ≤ 1 satisfy ε > ε , and let n ≥ C · 1 for a universal constant
2 1 1 2 tct (ε1−ε2)2
C . There is an algorithm A which solves the (ε ,ε )-tolerant calibration testing problem with n
tct 1 2
samples, which runs in time
(cid:18) (cid:19)
nlog(n)
O .
(ε −ε )2
1 2
Proof. Throughout the proof, let α := ε −ε > 0. Consider the following algorithm.
1 2
1. For |U| = m ≥ α6, sample n ≥ C αt 2ct samples to form an empirical distribution D(cid:98)n, where C
tct
is chosen so Lemma 11 guarantees |dCE(D)−dCE(D(cid:98)n)| ≤ α
6
with probability ≥ 2 3.
2. Call Proposition 1 with ε ← α
6
to obtain β, an α 3-additive approximation to |dCE(D(cid:98)n)|.
3. Return “yes” if β ≤ ε + α, and return “no” otherwise.
2 2
Conditioned on the event that |dCE(D) − dCE(D(cid:98)n)| ≤ α 6, we show that the algorithm succeeds.
First, if dCE(D) ≤ ε 2, then dCE(D(cid:98)n) ≤ ε 2+ α
6
by assumption, and so β ≤ ε 2+ α
2
by Proposition 1,
so the tester will return “yes.” Second, if dCE(D) ≥ ε 1, then dCE(D(cid:98)n) ≥ ε
1
− α
6
by assumption,
so β ≥ ε − α by Proposition 1 and similarly the tester will return “no” in this case. Finally, the
1 2
runtime is immediate from Proposition 3 and the definition of α.
Theorem 4 has the following implication for (standard) calibration testing.
Corollary 4. Let n ∈ N and let ε ∈ (0,1) be minimal such that it is information-theoretically
n
possible to solve the ε -calibration testing problem with n samples. For some ε = Θ(ε ), there is an
n n
algorithm A which solves the ε-calibration testing problem with n samples, which runs in time
O(cid:0) n2log(n)(cid:1)
.
Proof. This claim follows analogously to Corollary 3.
5 Sample complexity lower bounds for calibration distances
Recent works [BN23, BGHN23a] have introduced other calibration measures (e.g. the convolved
ECE and interval CE), given efficient estimation algorithms for them, and showed that they are
polynomiallyrelatedtothelowerdistancetocalibrationdCE. Therefore, analternativeapproachto
the (non-tolerant) testing problem for dCE is by reducing it to testing problems for these measures.
The main result of this section is that this approach leads to suboptimal sample complexity: the
testing problems for these measures cannot be solved given only O(ε−2) data points {(v ,y )} .
i i i∈[n]
To establish this sample complexity lower bound, we construct a perfectly calibrated distribution
D and a family of miscalibrated distributions D parameterized by θ belonging to a finite set. We
0 θ
use D⊗n (and D⊗n) to denote the joint distribution of n independent examples from D (and D ).
0 θ 0 θ
In Lemma 22, we show that the total variation distance between D⊗n and the mixture E[D⊗n]
0 θ
of D⊗n is small unless n is large, and thus distinguishing them requires large sample complexity.
θ
27Consequently, thetestingproblemforacalibrationmeasurehaslargesamplecomplexityifitassigns
every D a large calibration error. Finally, we show every D indeed has large convolved ECE and
θ θ
interval CE, establishing sample complexity lower bounds for these measures in Theorems 5 and 6.
To construct D and D , we consider t values {u } ∈ [1, 2] where u = 1 + i for i ∈ [t]. We will
0 θ i i∈[t] 3 3 i 3 3t
determine the value of t ∈ N later. We also define the following distribution, a perfectly calibrated
distribution which is related to the miscalibrated synthetic dataset used in Section 6.
Definition 8. The distribution D of (v,y) ∈ [0,1] × {0,1} is defined such that the marginal
0
distribution of v is uniform over {u } and E [y|v] = v.
i i∈[t] D0
Fix α ∈ (0, 1). For θ ∈ {−1,1}t, we define distribution D of (v,y) ∈ [0,1]×{0,1} such that the
3 θ
marginal distribution of v is uniform over {u } and E [y|v = u ] = u +θ α. In other words,
i i∈[t] D θ i i i
each conditional distribution given v is miscalibrated by α, but the bias takes a random direction.
Wenowfollowastandardapproachby[IS03]toboundthetotalvariationbetweenourdistributions.
Lemma 22. For any t ∈ N and α ∈ (0, 1),
3
(cid:115)
1
(cid:18) 11α4n2(cid:19)
d (D⊗n,E [D⊗n]) ≤ exp −1.
TV 0 θ θ 2 t
Here, to construct the mixture distribution E [D⊗n], we first draw θ ∼ {−1,1}t, and then draw
θ θ unif.
n independent examples from D . We denote the distribution of the n examples by E [D⊗n].
θ θ θ
Proof. By a standard inequality between the total variation distance and the χ2 distance, we have
1(cid:113)
d (D⊗n,E [D⊗n]) ≤ χ2(E [D⊗n]∥D⊗n). (28)
TV 0 θ θ 2 θ θ 0
By Ingster’s method [IS03] (see also Section 3.1 of [Can22]),
 n
t
χ2(E θ[D θ⊗n]∥D 0⊗n) = E θ,θ′(cid:88) (cid:88) D θ(u Di,j () uD ,θ j′( )u i,j)  −1, (29)
0 i
i=1j∈{0,1}
where the expectation is over θ,θ′ drawn i.i.d. ∼ {−1,1}t. For every i ∈ [t], we have
unif.
D θ(u i,1)D θ′(u i,1)
=
(cid:16)
ui+
tθiα(cid:17)(cid:16)
ui+ tθ
i′α(cid:17)
=
u
i
+
(θ i+θ i′)α
+
θ iθ i′α2
,
D (u ,1) ui t t u t
0 i t i
and similarly
D θ(u i,0)D θ′(u i,0)
=
(cid:16)
(1−ui
t−θiα)(cid:17)(cid:16)
(1−ui t−θ
i′α)(cid:17)
=
1−u
i
−
(θ i−θ i′)
+
θ iθ i′α2
.
D 0(u i,0) 1−ui t t (1−u i)t
t
Adding up the two equations, we get
(cid:88) D θ(u i,j)D θ′(u i,j)
=
1
+
θ iθ i′α2 (cid:18) 1
+
1 (cid:19)
≤
1
+
9θ iθ i′α2
,
D (u ,j) t t u 1−u t 2t
0 i i i
j∈{0,1}
28where the last inequality uses the fact that u ∈ [1, 2]. Plugging this into (29), we get
i 3 3
(cid:34)(cid:32) 9α2 (cid:88)t (cid:33)n(cid:35)
χ2(E [D⊗n]∥D⊗n) ≤ E 1+ θ θ′ −1
θ θ 0 θ,θ′ 2t i i
i=1
(cid:34) (cid:32) 9α2n (cid:88)t (cid:33)(cid:35)
≤ E exp θ θ′ −1
θ,θ′ 2t i i
i=1
(cid:89)t (cid:20) (cid:18) 9α2n (cid:19)(cid:21)
= E exp θ θ′ −1
θ,θ′ 2t i i
i=1
(cid:89)t (cid:20) (cid:18) 81α4n2 (cid:19)(cid:21)
≤ E exp θ2 −1 (by Hoeffding’s lemma)
θ 8t2 i
i=1
(cid:18) 81α4n2(cid:19)
= exp −1.
8t
Plugging this into (28) completes the proof.
5.1 Lower bound for convolved ECE
We now introduce the definition of convolved ECE from [BN23], and show that for every θ ∈
{−1,1}t,D hasalargeconvolvedECEinLemma24. Thisallowsustoproveoursamplecomplexity
θ
lower bound for convolved ECE in Theorem 5, by applying Lemma 22.
Definition 9 (Convolved ECE [BN23]). Let π : R → [0,1] be the periodic function with period 2
R
satisfying π (v) = v if v ∈ [0,1], and π (v) = 2−v if v ∈ [1,2]. Consider a distribution D over
R R
[0,1]×{0,1}. For (v,y) ∼ D, define random variable vˆ:= π (v+η), where η is drawn independently
R
from N(0,σ2) for a parameter σ ≥ 0. The σ-convolved ECE is defined as follows:
cECE (D) := E|E[(y−v)|vˆ]|,
σ
where the outer expectation is over the marginal distribution of vˆ, and the inner expectation is over
the conditional distribution of (y,v) given vˆ. It has been shown in [BN23] that cECE (D) ∈ [0,1]
σ
is a nonincreasing function of σ ≥ 0 and there exists a unique σ∗ ≥ 0 satisfying cECE (D) = σ∗.
σ∗
The convolved ECE cECE(D) is defined to be cECE (D).
σ∗
We also mention that the following relationship is known between cECE and dCE.
Lemma 23 (Theorem 7, [BN23]). For any distribution D over [0,1]×{0,1}, it holds that
1 (cid:112)
dCE(D) ≤ cECE(D) ≤ 2 dCE(D).
2
We have the following lower bound on cECE(D ):
θ
Lemma 24. For integer t ≥ 3, choose α = √1 . Then for every θ ∈ {−1,1}t,
t lnt
1
cECE(D ) ≥ √ .
θ
100t lnt
29Proof. It suffices to show that cECE σ(D θ) ≥ 1√ whenever σ ≤ 1√ .
100t lnt 100t lnt
Consider(v,y) ∼ D andvˆ= π (v+η), whereη isdrawnindependentlyfromN(0,σ2). Bystandard
R
Gaussian tail bounds, we have
(cid:20) (cid:21)
1 1
Pr |η| ≥ ≤ . (30)
6t t2
Next, consider a function ℓ : [0,1] → [t] such that ℓ(vˆ) ∈ argmin |u −vˆ|. Let E denote the event
i∈[t] i
that v = u . Let I and I be the indicators of E and its complement, respectively. We have
ℓ(vˆ) E ¬E
E[(y−v)I | vˆ,v] = I E[y−v | vˆ,v] (I is fully determined by v and vˆ)
E E E
= I E[y−v | v] (y is independent of vˆ given v)
E
= I E[y−v | v = u ] = I θ α.
E ℓ(vˆ) E ℓ(vˆ)
Taking expectation over v conditioned on vˆ, we have
|E[(y−v)I | vˆ]| = |Pr[E | vˆ]θ α| = Pr[E | vˆ]α.
E ℓ(vˆ)
We also have
(cid:12) (cid:104) (cid:105)(cid:12) (cid:104) (cid:105)
(cid:12)E (y−v)I | vˆ (cid:12) ≤ E |(y−v)I | | vˆ ≤ Pr[¬E | vˆ].
(cid:12) ¬E (cid:12) ¬E
Therefore,
|E[y−v | vˆ]| ≥ Pr[E|vˆ]α−Pr[¬E | vˆ].
Taking expectations over vˆ, we have
cECE (D ) = E[|E[y−v | vˆ]|] ≥ Pr[E]α−Pr[¬E]. (31)
σ θ
Whenever E does not occur, it must hold that |v −vˆ| ≥ 1, which can only hold when |η| ≥ 1.
6t 6t
Therefore, by plugging (30) into (31), we get
(cid:18) (cid:19)
1 1 1
cECE (D ) ≥ 1− α− ≥ √ .
σ θ t2 t2 100t lnt
Theorem 5. If A is an ε-cECE tester with n samples (Definition 5), for ε ∈ (0, 1), then
3
(cid:32) (cid:33)
1
n = Ω .
ε2.5ln0.25(1)
ε
Proof. Without loss of generality, assume that ε ≤ 10−3. Let t ≥ 3 be the largest integer satisfying
ε ≤ 1√ . We choose α = √1 .
100t lnt t lnt
By Lemma 24, we have cECE(D ) ≥ ε for every θ ∈ {−1,1}t. By the guarantee of the tester, we
θ
have
1
d (D⊗n,E [D⊗n]) ≥ .
TV 0 θ θ 3
√
Combining this with Lemma 22, we get n = Ω(α−2 t) = Ω(t2.5lnt), so the claim holds.
305.2 Lower bound for (surrogate) interval CE
The interval calibration error was introduced in [BGHN23a] as a modified version of the popular
binned ECE to obtain a polynomial relationship to the lower distance to calibration (dCE). To give
an efficient estimation algorithm, [BGHN23a] considered a slight variant of the interval calibration
error, called the surrogate interval calibration error, which preserves the polynomial relationship.
Below we include the definition of the surrogate interval calibration error, and its polynomial rela-
tionshipwithdCE. Wethenestablishoursamplecomplexitylowerbound(Theorem6)forsurrogate
interval CE by showing that every D has a large surrogate interval CE (Lemma 26).
θ
Definition 10 ([BGHN23a]). For a distribution D over [0,1]×{0,1} and an interval width param-
eter w > 0, the random interval calibration error is defined to be
 
(cid:88)
RintCE(D,w) := E r |E (v,y)∼D[(y−v)I(v ∈ I rw ,j)]|, (32)
j∈Z
where the outer expectation is over r drawn uniformly from [0,w) and Iw is the interval [r+jε,r+
r,j
(j + 1)ε). Note that although the summation is over j ∈ Z, there are only finitely many j that
can contribute to the sum (which are the j that satisfy Iw ∩ [0,1] ̸= ∅). The surrogate interval
r,j
calibration error is defined as follows:
(cid:16) (cid:17)
SintCE(D) := inf RintCE(D,2−k)+2−k .
k∈Z
≥0
Lemma 25 (Theorem 6.11, [BGHN23a]). For any distribution D over [0,1]×{0,1}, it holds that
(cid:112)
dCE(D) ≤ SintCE(D) ≤ 6 dCE(D).
Lemma 26. For t ∈ N, let α = 1. Then for every θ ∈ {−1,1}t, it holds that SintCE(D ) ≥ 1.
3t θ 3t
Proof. It suffices to prove that RintCE(D ,w) ≥ 1 whenever w < 1, where we recall the definition
θ 3t 3t
(32). Fixsomer ∈ [0,1]. Everyu belongstotheintervalIw forauniquej ∈ Z. Sincetheinterval
i r,ji i
width w is smaller than the gap between u and u for distinct i,i′, we have j ̸= j . Therefore,
i i′ i i′
(cid:88) (cid:88)
|E [(y−v)I(v ∈ Iw )]| ≥ |E [(y−v)I(v ∈ Iw )]|
(v,y)∼D r,j (v,y)∼D r,ji
j∈Z i∈[t]
(cid:88)
= |E [(y−v)I(v = u )]|
(v,y)∼D i
i∈[t]
(cid:88)
= Pr[v = u ]E[y−v|v = u ]
i i
i∈[t]
1
= .
3t
Plugging this into (32), we get RintCE(D ,w) ≥ 1.
θ 3t
Theorem 6. If A is an ε-SintCE tester with n samples (Definition 5), for ε ∈ (0, 1), then
3
(cid:18) (cid:19)
1
n = Ω .
ε2.5
31Proof. Choose t ≥ 1 to be the largest integer satisfying 1 ≥ ε, and choose α = 1. By Lemma 26,
3t 3t
SintCE(D ) ≥ ε for every θ ∈ {−1,1}t. By the guarantee of the tester, we have
θ
1
d (D⊗n,E [D⊗n]) ≥ .
TV 0 θ θ 3
√
Combining this with Lemma 22, we get n = Ω(α−2 t) = Ω(ε−2.5).
6 Experiments
In this section, we present experiments on synthetic data and CIFAR-100 supporting our results.
Synthetic dataset. In our first experiment, we considered the ability of ε-d-testers (Definition 5)
todetectthemiscalibrationofasyntheticdataset,forvariouslevelsofε ∈ {0.01,0.03,0.05,0.07,0.1},
and various choices of d ∈ {smCE,dCE,ConvECE}.9 The synthetic dataset we used is n independent
draws from D, where a draw (v,y) ∼ D first draws v ∼ [0,1−ε⋆], and y ∼ Bern(v +ε⋆), for
unif.
ε⋆ := 0.01.10 Note that dCE(D) = ε⋆ = 0.01, by the proof in Lemma 13. In Table 1, where the
columns index n (the number of samples), for each choice of d we report the smallest value of ε such
that a majority of 100 runs of an ε-d-tester report “yes.” For d = ConvECE, we implemented our
testerbyrunningcodein[BN23]tocomputeConvECEandthresholdingat ε. Ford ∈ {smCE,dCE},
2
we used the standard linear program solver from CVXPY [DB16, AVDB18] and again thresholded
at ε. We remark that the CVXPY solver, when run on the dCE linear program, fails to produce
2
stable results for n > 29 due to the size of the constraint matrix. As seen from Table 1, both smCE
and dCE testers are more reliable estimators of the ground truth calibration error ε⋆ than ConvECE.
n 26+1 27+1 28+1 29+1 210+1 211+1
smCE 0.07 0.05 0.03 0.03 0.01 0.01
dCE 0.03 0.01 0.01
cECE 0.1 0.1 0.07 0.07 0.05 0.03
Ground Truth 0.01 0.01 0.01 0.01 0.01 0.01
Table 1: Calibration testing thresholds (smallest passed on half of 100 runs).
In Figure 1, we plot the median error with error bars for each calibration distance, where the x axis
denotes log (n−1), and results are reported over 100 runs.
2
Postprocessed neural networks. In [GPSW17], which observed modern deep neural networks
may be very miscalibrated, various strategies were proposed for postprocessing network predictions
to calibrate them. We evaluate two of these strategies using our testing algorithms. We trained
a DenseNet40 model [HLvdMW17] on the CIFAR-100 dataset [Kri09], producing a distribution
D , where a draw (v,y) ∼ D selects a random example from the test dataset, sets y to be its
base base
label, and v to be the prediction of the neural network. We also learned calibrating postprocessing
functions f and f from the training dataset, the former via isotonic regression and the latter
iso temp
via temperature scaling. These induce (ideally, calibrated) distributions D , D , where a draw
iso temp
9We implemented ConvECE using code from [BN23], which automatically conducts a parameter search for σ.
10This is a slight variation on the synthetic dataset used in [BGHN23a].
32Figure 1: The 25% quantile, median, and 75% quantile (over 100 runs) for smCE, dCE and cECE
respectively. The x-axis is for dataset with size 2x+1.
D D D D
base iso temp
Empirical smCE 0.2269 0.2150 0.1542
Table 2: Empirical smCE on postprocessed DenseNet40 predictions (median over 20 runs)
from D samples (v,y) ∼ D and returns (f (v),y), and D is defined analogously. The
iso base iso temp
neural network and postprocessing functions were all trained by adapting code from [GPSW17].
We computed the median smooth calibration error of 20 runs of the following experiment. In each
run, for each D ∈ {D ,D ,D }, we drew 4864 random examples from D, and computed the
base iso temp
smooth calibration error smCE of the empirical dataset. We report our findings in Table 2. Qual-
itatively, our results (based on smCE) agree with findings in [GPSW17] (based on binned variants
of ECE), in that temperature scaling appears to be the most effective postprocessing technique.
smCE tester. Finally, we implemented our algorithm from Section 3, based on the minimax solver
from [JT23]. We considered two types of augmented Lagrangian relaxations of the hard constraints,
based on the sets of constraints S (defined in (20)) and S (defined in (21)). In each case, we
base 0
scaleduptheLagrangianpenaltytermbylog (n−1). AsdescribedinSection3.2, whenusingS ,
2 base
this provably yields a soft-constrained empirical smCE objective which has the same value as the
empirical smCE linear program. When using S , the objective guarantee is no longer provable, but
0
we conduct this experiment because of the sufficiency of S under hard constraints (see Lemma 7).
0
In Tables 3 and 4, we report the average suboptimality gap (across 5 runs) using soft-penalized
objectives based on S and S respectively, varying the sample size and the number of iterations.
0 base
The dataset is the same synthetic dataset based on Bernoulli samples as described earlier, with
√
ε⋆ = 0.1. We scaled the number of iterations T as multiples of n, because this is the ε−1 value
suggested by our lower bound in Lemma 2. We find that both S and S reliably converge to
0 base
the empirical smCE computed by CVXPY, at the 1 rate guaranteed by [JT23]. While S works
T base
33T 25+1 26+1 27+1 28+1 29+1
√
4.8 n 0.04789 0.03096 0.01829 0.01380 0.01006
√
24 n 0.01206 0.00719 0.00478 0.00314 0.00236
√
48 n 0.00657 0.00389 0.00267 0.00170 0.00139
Table 3: Average suboptimality gap compared to CVXPY in T iterations of [JT23] solver, using
penalty based on S constraints, across 5 runs. Columns indicate sample size n.
0
T 25+1 26+1 27+1 28+1 29+1
√
4.8 n 0.04203 0.02793 0.01797 0.01217 0.00959
√
24 n 0.00886 0.00583 0.00359 0.00192 0.00559
√
48 n 0.00443 0.00279 0.00180 0.00122 0.00096
Table 4: Average suboptimality gap compared to CVXPY in T iterations of [JT23] solver, using
penalty based on S constraints, across 5 runs. Columns indicate sample size n.
base
better than S , there is not a large gap in their performance, suggesting it may be better to use S
0 0
in practice for efficiency. The constant 48 was chosen based on parameters in [JT23]’s algorithm.
Finally, we note that a limitation of our work is that for these moderate values of n, our preliminary
unoptimized code is slower than CVXPY’s custom linear program solver for smCE.11 We find it
encouraging that Tables 3, 4 demonstrate high levels of accuracy for small iteration counts ≈ 100,
and leave more efficient practical implementations as an important open direction.
11CVXPY does fail to solve the dCE linear program for moderate n, however, as mentioned earlier.
34References
[AAA+16] Dario Amodei, Sundaram Ananthanarayanan, Rishita Anubhai, Jingliang Bai, Eric
Battenberg, Carl Case, Jared Casper, Bryan Catanzaro, Jingdong Chen, Mike
Chrzanowski, Adam Coates, Greg Diamos, Erich Elsen, Jesse H. Engel, Linxi Fan,
Christopher Fougner, Awni Y. Hannun, Billy Jun, Tony Han, Patrick LeGresley, Xi-
angang Li, Libby Lin, Sharan Narang, Andrew Y. Ng, Sherjil Ozair, Ryan Prenger,
Sheng Qian, Jonathan Raiman, Sanjeev Satheesh, David Seetapun, Shubho Sen-
gupta, Chong Wang, Yi Wang, Zhiqian Wang, Bo Xiao, Yan Xie, Dani Yogatama,
Jun Zhan, and Zhenyao Zhu. Deep speech 2 : End-to-end speech recognition in en-
glishandmandarin. InProceedings of the 33nd International Conference on Machine
Learning, ICML 2016, volume 48 of JMLR Workshop and Conference Proceedings,
pages 173–182. JMLR.org, 2016.
[AVDB18] Akshay Agrawal, Robin Verschueren, Steven Diamond, and Stephen Boyd. A rewrit-
ing system for convex optimization problems. Journal of Control and Decision,
5(1):42–60, 2018.
[BGHN23a] Jarosław Błasiok, Parikshit Gopalan, Lunjia Hu, and Preetum Nakkiran. A uni-
fying theory of distance from calibration. In Proceedings of the 55th Annual ACM
Symposium on Theory of Computing, pages 1727–1740, 2023.
[BGHN23b] JarosławBłasiok, ParikshitGopalan, LunjiaHu, andPreetumNakkiran. Whendoes
optimizing a proper loss yield calibration? arXiv preprint arXiv:2305.18764, 2023.
[BN23] Jarosław Błasiok and Preetum Nakkiran. Smooth ECE: Principled reliability dia-
grams via kernel smoothing. arXiv preprint arXiv:2309.12236, 2023.
[Can22] ClémentL.Canonne. Topicsandtechniquesindistributiontesting: Abiasedbutrep-
resentative sample. Foundations and Trends® in Communications and Information
Theory, 19(6):1032–1198, 2022.
[CLS21] Michael B. Cohen, Yin Tat Lee, and Zhao Song. Solving linear programs in the
current matrix multiplication time. J. ACM, 68(1):3:1–3:39, 2021.
[CST21] MichaelB.Cohen,AaronSidford,andKevinTian. Relativelipschitznessinextragra-
dientmethodsandadirectrecipeforacceleration. In12th Innovations in Theoretical
Computer Science Conference, ITCS 2021, volume 185 of LIPIcs, pages 62:1–62:18.
Schloss Dagstuhl - Leibniz-Zentrum für Informatik, 2021.
[DB16] Steven Diamond and Stephen Boyd. CVXPY: A Python-embedded modeling lan-
guage for convex optimization. Journal of Machine Learning Research, 17(83):1–5,
2016.
[DKR+21] Cynthia Dwork, Michael P. Kim, Omer Reingold, Guy N. Rothblum, and Gal Yona.
Outcomeindistinguishability. InSTOC ’21: 53rd Annual ACM SIGACT Symposium
on Theory of Computing, 2021, pages 1095–1108. ACM, 2021.
[Doi07] Kunio Doi. Computer-aided diagnosis in medical imaging: historical review, cur-
rent status and future potential. Computerized medical imaging and graphics, 31(4–
5):198–211, 2007.
35[GKR+22] Parikshit Gopalan, Adam Tauman Kalai, Omer Reingold, Vatsal Sharan, and Udi
Wieder. Omnipredictors. In 13th Innovations in Theoretical Computer Science Con-
ference, ITCS 2022, volume 215 of LIPIcs, pages 79:1–79:21. Schloss Dagstuhl -
Leibniz-Zentrum für Informatik, 2022.
[Gol17] OdedGoldreich. Introduction to Property Testing. CambridgeUniversityPress,2017.
[GPSW17] ChuanGuo,GeoffPleiss,YuSun,andKilianQWeinberger.Oncalibrationofmodern
neural networks. In International conference on machine learning, pages 1321–1330.
PMLR, 2017.
[HKRR18] Úrsula Hébert-Johnson, Michael P. Kim, Omer Reingold, and Guy N. Rothblum.
Multicalibration: Calibration for the (computationally-identifiable) masses. In Pro-
ceedings of the 35th International Conference on Machine Learning, ICML 2018,
volume 80 of Proceedings of Machine Learning Research, pages 1944–1953. PMLR,
2018.
[HLvdMW17] GaoHuang,ZhuangLiu,LaurensvanderMaaten,andKilianQ.Weinberger.Densely
connected convolutional networks. In 2017 IEEE Conference on Computer Vision
and Pattern Recognition, CVPR 2017, pages 2261–2269. IEEE Computer Society,
2017.
[IS03] Y. I. Ingster and I. A. Suslina. Nonparametric Goodness-of-Fit Testing under Gaus-
sian Models, volume 169 of Lecture Notes in Statistics. Springer-Verlag, New York,
2003.
[JST19] ArunJambulapati,AaronSidford,andKevinTian. AdirectO(cid:101)(1/ε)iterationparallel
algorithmforoptimaltransport. AdvancesinNeuralInformationProcessingSystems,
32, 2019.
[JT23] Arun Jambulapati and Kevin Tian. Revisiting area convexity: Faster box-simplex
games and spectrahedral generalizations. arXiv preprint arXiv:2303.15627, 2023.
[KF04] ShamM.KakadeandDeanP.Foster. Deterministiccalibrationandnashequilibrium.
In John Shawe-Taylor and Yoram Singer, editors, Learning Theory, pages 33–48,
Berlin, Heidelberg, 2004. Springer Berlin Heidelberg.
[KF08] ShamM.KakadeandDeanP.Foster. Deterministiccalibrationandnashequilibrium.
J. Comput. Syst. Sci., 74(1):115–130, 2008.
[KLM19] Ananya Kumar, Percy Liang, and Tengyu Ma. Verified uncertainty calibration. In
Advances in Neural Information Processing Systems 32: Annual Conference on Neu-
ral Information Processing Systems 2019, NeurIPS 2019, pages 3787–3798, 2019.
[Kri09] Alex Krizhevsky. Learning multiple layers of features from tiny images.
https://www.cs.toronto.edu/ kriz/learning-features-2009-TR.pdf, 2009. Accessed:
2024-01-31.
[KSJ18] Aviral Kumar, Sunita Sarawagi, and Ujjwal Jain. Trainable calibration measures for
neural networks from kernel mean embeddings. In Jennifer Dy and Andreas Krause,
36editors, Proceedings of the 35th International Conference on Machine Learning, vol-
ume80ofProceedingsofMachineLearningResearch,pages2805–2814.PMLR,10–15
Jul 2018.
[LeC73] L.LeCam. ConvergenceofEstimatesUnderDimensionalityRestrictions. The Annals
of Statistics, 1(1):38 – 53, 1973.
[LS14] Yin Tat Lee and Aaron Sidford. Path finding methods for linear programming:
Solving linear programs in õ(vrank) iterations and faster algorithms for maximum
flow. In 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS
2014, pages 424–433. IEEE Computer Society, 2014.
[MDR+21a] Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai,
Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern
neural networks. In Advances in Neural Information Processing Systems 34: An-
nualConferenceonNeuralInformationProcessingSystems2021,pages15682–15694,
2021.
[MDR+21b] Matthias Minderer, Josip Djolonga, Rob Romijnders, Frances Hubis, Xiaohua Zhai,
Neil Houlsby, Dustin Tran, and Mario Lucic. Revisiting the calibration of modern
neural networks. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and
J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems,
volume 34, pages 15682–15694. Curran Associates, Inc., 2021.
[Mur98] Allan H. Murphy. The early history of probability forecasts: Some extensions and
clarifications. Weather and forecasting, 13(1):5–15, 1998.
[MW84] Allan H. Murphy and Robert L. Winkler. Probability forecasting in meteorology.
Journal of the American Statistical Association, 79(387):489–500, 1984.
[NCH15] Mahdi Pakdaman Naeini, Gregory F. Cooper, and Milos Hauskrecht. Obtaining well
calibrated probabilities using bayesian binning. In Proceedings of the Twenty-Ninth
AAAI Conference on Artificial Intelligence, January 25-30, 2015, pages 2901–2907.
AAAI Press, 2015.
[NDZ+19] Jeremy Nixon, Michael W. Dusenberry, Linchuan Zhang, Ghassen Jerfel, and Dustin
Tran. Measuring calibration in deep learning. In IEEE Conference on Computer
Vision and Pattern Recognition Workshops, CVPR Workshops 2019, pages 38–41.
Computer Vision Foundation / IEEE, 2019.
[Nie22] Zipei Nie. Matrix anti-concentration inequalities with applications. In STOC ’22:
54th Annual ACM SIGACT Symposium on Theory of Computing, pages 568–581.
ACM, 2022.
[PV21] Richard Peng and Santosh S. Vempala. Solving sparse linear systems faster than
matrixmultiplication. InProceedingsofthe2021ACM-SIAMSymposiumonDiscrete
Algorithms, SODA 2021, pages 504–521. SIAM, 2021.
[Ron08] Dana Ron. Property testing: A learning theory perspective. Found. Trends Mach.
Learn., 1(3):307–402, 2008.
37[Ron09] Dana Ron. Algorithmic and analysis techniques in property testing. Found. Trends
Theor. Comput. Sci., 5(2):73–205, 2009.
[RRSK11] Francesco Ricci, Lior Rokach, Bracha Shapira, and Paul B. Kantor. Recommender
Systems Handbook. Springer New York, 2011.
[Rt21a] Rahul Rahaman and alexandre thiery. Uncertainty quantification and deep ensem-
bles. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman
Vaughan, editors, Advances in Neural Information Processing Systems, volume 34,
pages 20063–20075. Curran Associates, Inc., 2021.
[RT21b] Rahul Rahaman and Alexandre H. Thiéry. Uncertainty quantification and deep
ensembles. In Advances in Neural Information Processing Systems 34: Annual Con-
ference on Neural Information Processing Systems 2021, pages 20063–20075, 2021.
[She13] Jonah Sherman. Nearly maximum flows in nearly linear time. In 54th Annual IEEE
Symposium on Foundations of Computer Science, FOCS 2013, pages 263–269. IEEE
Computer Society, 2013.
[She17] Jonah Sherman. Area-convexity, l regularization, and undirected multicommodity
∞
flow. In Hamed Hatami, Pierre McKenzie, and Valerie King, editors, Proceedings of
the 49th Annual ACM SIGACT Symposium on Theory of Computing, STOC 2017,
pages 452–460. ACM, 2017.
[vdBLL+21] Jan van den Brand, Yin Tat Lee, Yang P. Liu, Thatchaphol Saranurak, Aaron Sid-
ford, Zhao Song, and Di Wang. Minimum cost flows, mdps, and ℓ -regression in
1
nearly linear time for dense instances. In STOC ’21: 53rd Annual ACM SIGACT
Symposium on Theory of Computing, 2021, pages 859–869. ACM, 2021.
[vdBLSS20] Jan van den Brand, Yin Tat Lee, Aaron Sidford, and Zhao Song. Solving tall dense
linear programs in nearly linear time. In Proceedings of the 52nd Annual ACM
SIGACT Symposium on Theory of Computing, STOC 2020, pages 775–788. ACM,
2020.
[VDDP17] AthanasiosVoulodimos,NikolaosDoulamis,AnastasiosDoulamis,andEftychiosPro-
topapadakis. Deep learning for computer vision: A brief review. Computational
Intelligence and Neuroscience, 2018, 2017.
[WXXZ23] Virginia Vassilevska Williams, Yinzhan Xu, Zixuan Xu, and Renfei Zhou. New
bounds for matrix multiplication: from alpha to omega. CoRR, abs/2307.07970,
2023.
38