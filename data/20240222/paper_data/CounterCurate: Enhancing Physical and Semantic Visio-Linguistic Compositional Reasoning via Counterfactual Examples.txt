CounterCurate: Enhancing Physical and Semantic Visio-Linguistic
Compositional Reasoning via Counterfactual Examples
JianruiZhang*1 MuCai∗1 TengyangXie1,2 YongJaeLee1
jzhang2427@wisc.edu, {mucai,tx,yongjaelee}@cs.wisc.edu
1UniversityofWisconsin–Madison 2MicrosoftResearch
https://countercurate.github.io
Abstract
We propose CounterCurate, a framework to
comprehensivelyimprovethevisio-linguistic
compositional reasoning capability for both
contrastiveandgenerativemultimodalmodels.
Inparticular,weidentifytwounder-explored
criticalproblems: theneglectofthephysically
groundedreasoning(countingandpositionun-
derstanding)andthepotentialofusinghighly
capabletextandimagegenerationmodelsfor :Which caption correspond to this :Which caption correspond
image? to this image?
semanticcounterfactualfine-tuning. Ourwork
0: the dress on the left is long and the 0: a pink scoop on top and a blue
pioneersanapproachthataddressesthesegaps. dress on the right is short scoop on bottom
1: the dress on the left is short and the 1: a pink scoop on bottom and a
Wefirstspotlightthenear-chanceperformance dress on the right is long blue scoop on top
ofmultimodalmodelslikeCLIPandLLaVAin :Caption 1 is correct because :The image shows an ice
physicallygroundedcompositionalreasoning. the image shows a person on the left cream cone with a blue scoop
side wearing a red dress that ends of ice cream on top and what
Wethenapplysimpledataaugmentationusing
above the knee, and a person on the appears to be a pink scoop on
agroundedimagegenerationmodel,GLIGEN, right side wearing a white wedding the bottom. Therefore,
togeneratefinetuningdata,resultinginsignif- dress with a long train. Caption 1 is correct.
icant performance improvements: +33% and
Figure1: RepresentativeexamplesofGPT-4Vfailure
+37% for CLIP and LLaVA, respectively, on
cases. Inbothquestions,GPT-4Vcorrectlyidentifiesall
ournewlycuratedFlickr30k-Positionsbench-
objectsinquestion,butchoosesthewronganswerbe-
mark. Moreover, we exploit the capabilities
causeitfailstodistinguishbetweeneitherleftandright
ofhigh-performingtextgenerationandimage
(theleftquestion)orupanddown(therightquestion).
generation models, specifically GPT-4V and
DALLE-3,tocuratechallengingsemanticcoun-
terfactuals,therebyfurtherenhancingcompo-
sitionalreasoningcapabilitiesonbenchmarks ing capabilities. However, these models exhibit
suchasSugarCrepe,whereCounterCurateout- subpar performance in compositional reasoning
performsGPT-4V. (Diwan et al., 2022; Hsieh et al., 2023), such as
To facilitate future research, we will release differentiating between semantic distractors like
ourcode,dataset,benchmark,andcheckpoints “whiteshirtsandblackpants”and“blackshirtsand
at https://github.com/HanSolo9682/ whitepants”. Currentresearcheitherconcentrates
CounterCurate.
oncuratingevaluationbenchmarks(Diwanetal.,
2022;Hsiehetal.,2023;Leetal.,2023)orenhanc-
1 Introduction
ing compositional reasoning abilities by creating
LargelanguagemodelssuchasChatGPT(OpenAI, rule-basedcounterfactualfine-tuningdata(Yuksek-
2023a),GPT4(OpenAI,2023b),andLLaMA(Tou- gonuletal.,2023;Leetal.,2023).
vron et al., 2023) have demonstrated remarkable
Atpresent,themajorityofresearchfocuseson
knowledgeandreasoningabilities. Recently,large
semanticcompositionalreasoning,leavinganother
multimodalmodels(LMMs)(Radfordetal.,2021;
fundamentalproblem,physicallygroundedcompo-
OpenAI,2023c)furtherleveragelarge-scaleimage-
sitionalreasoning,under-explored. Thisincludes
textpairsforimprovingvisio-linguisticunderstand-
taskssuchascountinganddistinguishingleft/right
*EqualContribution. andup/downpositionsbetweenobjects. Forexam-
1
4202
beF
02
]VC.sc[
1v45231.2042:viXraple,inFigure1,itisnotablethatGPT-4V(OpenAI, tationtechniquesandagroundedimagegen-
2023c)makesthewrongchoiceeventhoughitiden- erationmodel,GLIGEN,togeneratecounter-
tifiesallobjectsintheimage,demonstratingweak factualimagesandcaptions.
physicalcompositionalreasoning.
Withsuchanobservation,itisnotsurprisingto • Weemploythemostcapableimageandtext
findthatbothcontrastivemodelslikeCLIP(Rad- generation models to generate semantically
ford et al., 2021) and generative models like counterfactual image/text pairs, further en-
LLaVA (Liu et al., 2023b) deliver near random hancingperformanceoverSOTAmethods.
chanceperformanceonournewlycuratedbench-
Weplantoreleaseourcode,dataset,benchmark,
marks. We hypothesize that modern LMMs are
andcheckpointsuponthepaper’sacceptance.
largelyoblivioustopositionaldifferences. Toad-
dress this, we curate counterfactual image-text
2 RelatedWorks
pairsusingsimplemethodssuchashorizontalflip-
ping for left/right distinctions, and leveraging a
2.1 LargeMultimodalModels
grounded image generation model, GLIGEN (Li
Multimodalmodelsaimatconnectingmultipledata
etal.,2023),toaccuratelyreplace/removeobjects
modalities. Forvisio-linguistictasks,multimodal
of interest for up/down and counting tasks. Our
modelsareconstructedeitherascontrastive(Rad-
approach shows significant improvements such
fordetal.,2021;Jiaetal.,2021)orgenerative(Ope-
as 33% for CLIP and 37% for LLaVA on our
nAI,2023b;Alayracetal.,2022;Liuetal.,2023b).
Flickr30k-Positionsbenchmark.
Recentlargemultimodalmodelsaretrainedwith
Existing methods also do not fully utilize the
large-scalepretrainingdataandbillionsoftrainable
capabilities of powerful generative models when
parameters,whichsignificantlyimprovetheirgen-
creatingsemanticcounterfactualfine-tuningdata.
eralization capabilities. Contrastive models such
Wearguethatthekeytoenhancingcompositional
asCLIP(Radfordetal.,2021)connectimagesand
reasoning capabilities lies in the careful curation
textbyaligningthetwomodality-specificencoders
ofaccurateandsufficientlychallengingimageand
usingcontrastiveloss. Generativemodelssuchas
textcounterfactuals. Forexample,augmentedneg-
LLaVA(Liuetal.,2023b,a)embedimagesintoa
ativecaptionswithlinguisticrulesusedbyrecent
decoder-onlylanguagemodelasprefixtokensand
fine-tuning approaches (e.g., Yuksekgonul et al.,
utilizenext-tokenpredictionlosstoalignthetwo
2023;Leetal.,2023)couldunintentionallyfollow
modalities. Thoughsuchmodelsdemonstrateim-
unnaturallanguagedistributions,whichareeasily
pressivecapabilitiesacrossvariousvision-language
distinguishablebyatext-onlymodelwithoutany
taskssuchasretrievalandvisualquestionanswer-
imageevidence(Linetal.,2023).
ing,theyperformlessdesirablyovercompositional
To overcome this, we utilize the high-
reasoning(Diwanetal.,2022;Hsiehetal.,2023).
performancetextgenerationmodelGPT-4V(Ope-
nAI,2023c)andimagegenerationmodelDALLE-
2.2 CompositionalReasoning
3 (Betker et al., 2023) to curate reasonably and
sufficientlydifficultnegatives. Ourmethodempiri- Compositionalreasoning(Diwanetal.,2022)invi-
callydemonstratesasignificantperformanceboost sionandlanguagetasksinvolvestheabilityofmod-
by fine-tuning CLIP and LLaVA using our data elstounderstandandmanipulatecomplexideasby
generationpipelineonbenchmarkssuchasSugar- breakingthemdownintosimpler,constituentcom-
Crepe,wherewesurpassNegCLIPandGPT-4V. ponentsandthenrecombiningtheminnovelways.
Ourcontributionsaresummarizedasfollows: A typical example can be distinguishing “blank
pants and white shorts” versus “white pants and
• Wesystematicallystudyphysicallygrounded
blankshorts”. Winoground(Diwanetal.,2022)is
compositionalreasoning,includingpositional
the pioneering benchmark for compositional rea-
understandingandcounting,andhighlightthe
soning, composed of 400 items, each with two
near random performance of current multi-
imagesandtwocorrespondingcaptions. Givenan
modal models, including CLIP and LLaVA,
image, multimodal models are asked to find the
onournewlycuratedbenchmark.
matching caption from the provided two options,
• Wesignificantlyimprovephysicalreasoning and vice versa. SugarCrepe (Hsieh et al., 2023)
capabilitiesbyutilizingsimpledataaugmen- andSeeTrue(Yarometal.,2023)furtherscalesthe
2Positive Image Negative Image Negative Caption Negative Image
Negative Caption A woman with DALLE-3
KeP yo wsi ot rio dn Fa ll ip Face i os ft o to t yh se right Hor Fiz lio pntal Positive Image Rep Nla oc ue n and b ol ru ae n gh ea t vestT2I Generation
Fa Pce o sis it it vo e t h Ce a l pe tf it o nof toys (a) Flickr30k-Positions GPT-4V Assisted Replace A man with red hat DALLE-3
Caption Generation Adjective and orange vest
T2I Generation
Positive Image
Digit
TheN re eg aa rt eiv 2e cC aa rsp t ai no dn
3
Grounded
Negative Image A
P
oam sn ia d tn
i
vo w er ai Cnth ag peb
t
l ivu oee ns h tat AdjecS tiw vea sp
A man with orange DALLE-3
Manipulation persons Image Inpainting hat and blue vestT2I Generation
There are 3 cars and
4 persons
(b) Flickr30k-Counting (c) Flickr30k-Attributes
Positive Caption
Figure2: ThedatacurationpipelineofCounterCurate. Givenapositiveimage-captionpair,wefirstgeneratethe
negativecaptions,basedonwhichwegeneratethenegativeimagesusingthemostsuitableapproach. Specifically,
(a)forFlickr30k-Positions(left/right),weflipthepositionalkeyword,andthenconductthehorizontalflipforthe
image;(b)forFlickr30k-Counting,wefirstmanipulatethedigit,andthenapplygroundedimageinpainting(Lietal.,
2023)asthenegativeimage;(c)forFlickr30k-Attributes,wefirstleverageGPT-4V(OpenAI,2023c)togenerate
reasonablehardnegativecaptionsforreplacingthenoun,replacingtheadjective,andswappingtheadjectives. Then
weleverageDALLE-3(Betkeretal.,2023)togeneratecoherentimages.
distractivecaptionsbyleveraginglanguagemodels hardenoughexamples. COCO-Counterfactual(Le
toautomaticallygeneratethem. et al., 2023) explores simple linguistic rules to
NegCLIP(Yuksekgonuletal.,2023)attemptsto generatenegativecaptionsandusesanimageedit-
improve compositional reasoning via fine-tuning ingmodelPrompt2Prompt(Hertzetal.,2022)to
CLIPwithperturbedcaptionsasthenegativetext produce the negative images (Hertz et al., 2022).
samples in contrastive learning. However, such Prompt2Promptislessdesirableinunderstanding
permutedcaptionscanbeeasilydetectedbyatext- complexlanguageinstructions,makingthecoun-
only transformer (Lin et al., 2023), demonstrat- terfactualexampleslessreliableandchallenging.
ing that a more effective approach is needed to In this paper, we collect the most challeng-
furtherenhancecompositionalreasoning. COCO- ing negative image and text examples by lever-
Counterfactual(Leetal.,2023)furtherleverages aging the most capable language models GPT-
bothnegativeimagesandnegativecaptionstofine- 4 (OpenAI, 2023b) and text-to-image generation
tuneCLIPviaarule-baseddatacurationpipeline. model DALLE-3 (Betker et al., 2023) and GLI-
Moreover,previousworksmainlyconsidercom- GEN(Lietal.,2023),whichsignificantlyimprove
positionreasoningwithsemanticallydifferentcon- ourmodel’sreasoningperformance. Byutilizing
cepts,ratherthanphysicallygroundedrelationships these hard negatives, our model can better grasp
suchascounting,anddistinguishingleft/right,and thenuancesoflanguageandvision,enhancingits
above/below. Inourpaper,wesystematicallyim- performanceontasksthatrequireasophisticated
prove both semantic and physical compositional understandingoftheworld.
reasoningbyleveragingthemostcapablegenera-
3 Approach
tive models including GPT-4V (OpenAI, 2023c)
andDALLE-3(Betkeretal.,2023).
InSec.3.1,wewillfirstintroducethereasoningbe-
hindtheselectedFlickr30kEntitiesdatasetthatis
2.3 CounterfactualReasoning
usedforbothbenchmarkingandfine-tuning. After
Counterfactual reasoning (Morgan and Winship, that, in Sec. 3.2 and Sec. 3.3, we show the near-
2015) refers to the process of imagining “what chance performance of multimodal models like
if” scenarios by modifying the input data. In the CLIPandLLaVAonphysicallygroundedcompo-
contextofvisio-linguisticsscenarios,thisinvolves sitionalreasoningtasks,includingpositionalunder-
curating negative images and captions by manip- standingandcounting,andshowhowtoimprove
ulating the original data and observing how it af- theirperformance bygeneratingnegative images
fectstheoutcome. Thishelpsmodelsunderstand viadataaugmentationandgroundedimageinpaint-
cause and effect and predict outcomes in situa- ing. Finally, weintroduceusingcapabletextand
tionsthey’veneverseenbefore. Thekeytocoun- imagegenerationmodelstoimprovethesemantic
terfactual reasoning is to curate meaningful yet compositionalreasoninginSec.3.4.
3A man with blue A woman with :Given the image,
hat and orange blue hat and Choose the correct caption:
Positive vest orange vest Negative
Captions Captions
A. A man with blue hat and
PositiveImages orange vest;
B. A woman with blue hat
and orange vest.
Large Multimodal
Model
: B. The correct caption is “A woman
with blue hat and orange vest”…
Negative Images
(a) Fine-tuning using contrastive learning (CLIP) (b) Fine-tuning using generative models (LLaVA)
Figure3: Fine-tuningdifferenttypesoflargemultimodalmodelswithCounterCurate. Ourpipelinecanenhance
bothcontrastivelearningmodelsandgenerativemodelsbyaugmentingvanillaimage-captiondatawithcurated
negativeimagesandcaptions. Specifically,ourcounterfactualimage-captionpairs(a)provideauxiliarycontrastive
loss for models like CLIP, where positive contrastive units in the similarity matrix are painted as blue/red and
negativeonesarepaintedaswhite,and(b)canbenaturallyintegratedintotheoriginalnext-tokenpredictionlossin
textgenerationssuchasLLaVA.
3.1 NecessityofGroundedImageCaptions
Previous approaches (Yuksekgonul et al., 2023; 
Hsieh et al., 2023) adopt global image captioned   x a2 ≤ x b1 =⇒ aistotheleftofb

x ≥ x =⇒ aistotherightofb
datasets such as COCO-Captions (Chen et al., a1 b2
2015)andcraftnegativecaptionstoeitherbench-  y a2 ≤ y b1 =⇒ aisaboveb

mark or improve compositional reasoning. This  y ≥ y =⇒ aisbelowb
a1 b2
method, however, lacks regional information im-
portanttophysicallygroundedreasoning,suchas Basedontheformula,wecangeneratepositive
determining the relative position of two objects. andnegativepositionalcaptionssuchas“abikeis
Therefore, we leverage the grounded image cap- to the left of a woman” vs “a bike is to the right
tioningdatasetFlickr30kEntities(Plummeretal., ofawomen”and“atableisaboveatowel”vs“a
2016)tobothcuratenegativeimage-captionpairs tableisbelowatowel”.
andbenchmarkmodels. We also prompt the pure-text GPT4 (OpenAI,
2023b)modeltorewritethevanillanegativeprompt
3.2 ImprovingPositionalUnderstanding
“c is above/below c ” to improve the quality of
a b
Currently,thereisnobenchmarkthatdirectlyeval- images generated by GLIGEN. For example, the
uatesthepositionalunderstandingofmultimodal caption“astreetisaboveyoungchild"isveryun-
models. TobuildsuchabenchmarkasinFigure2 natural, while GPT-expanded caption “the street
(a),weleverageboundingboxesinFlickr30kEnti- stretches out above a young child, elevated by a
ties(Plummeretal.,2016)toidentifythepositional bridge”ismuchbetter. Fordetailedpromptengi-
objectpairswithineachimageI thatarestrictlyto neering,seeAppendixB.
theleft/rightorabove/beloweachother.
Generatingnegativeimages. Furthermore,we
Generatingnegativecaptions. Specifically,this include negative images in our dataset to curate
process relies on the bounding box coordinates Flickr30k-Positions. Togeneratenegativeimages
thatareformattedas(x ,y ,x ,y )forevery for left-right pairs, we apply a simple data aug-
j1 j1 j2 j2
objectj ∈ I whoseboundingbox’supper-leftand mentation method: flip the original images hori-
lower-rightcornerareat(x ,y )and(x ,y ), zontally. Forabove-belowpairs,however,flipping
j1 j1 j2 j2
respectively. Foreachpairofobjects(a,b) ∈ I,we verticallywithoutcontextwillleadtounreasonable
identifythepositionalobjectpairsviatheformula: images. Toaddressthisissue,weutilizeGLIGEN
4formatwherethenumbersinthepositivecaptions
arereversedinthenegatives.
In most cases, however, objects in an im-
age do overlap, and we cannot ensure the incre-
ment/decrementruletogeneratehardcounterfactu-
alsasdescribedabove. Forexample,ifadogover-
lapswithacat,andwere-usethemethodaboveto
removeonedogandaddonecat,wemightendup
onlyremovingonedogalongwiththeoverlapping
catunintendedly. Inthiscase,wecanonlyremove
one of either a or b, for example, the dog b, and
allobjectsthatitoverlapswith. Anexampleofthe
resulting negative caption is “there are two cats
andthreedogs.”
Generatingnegativeimages. Inthecasewhere
objects do not overlap, we simply leverage GLI-
GEN (Li et al., 2023) to inpaint the category C
Figure4: Togeneratethecorrectabove-belownegative k
where B was. Otherwise, we remove the target
imageviaGLIGENwithprompt"theballisbelowthe j
sportsoutfit",werecentertheboundingboxesof"ball" objects by using GLIGEN to replace it with an
and"sportsoutfit"andfeedthemintoGLIGENtogether object from a generic category. In particular, we
withanexpandedcaptionfromGPT4. simplyusetheplantcategoryforreplacement,as
plantscanappearinalmostanynaturalsetting,let
itbeoutdoororindoor,andwefindthatGLIGEN
(Lietal.,2023),anobject-leveltext-to-imagegen-
does well in generating it according to each im-
eration model that can ground text prompts with
age’s background during inpainting. Please refer
boundingboxes. Specifically,foreachabove-below
toAppendixCformoredetails.
object pair (a,b) ∈ I, we obtain their bounding
boxesandswaptheircenterswhilemaintainingthe
3.4 ImprovingSemanticCompositional
width and height, ensuring that the generated im-
Reasoning
agesaremorenatural,preservingtheobjects’sizes
andaspect-ratiosasdemonstratedinFigure4. Existingworks(Yuksekgonuletal.,2023;Leetal.,
2023) use a rule-based approach to generate neg-
3.3 ImprovingObjectCounting ativeimages/captionsforsemanticcompositional
We introduce the Flickr30k-Counting dataset, reasoning,whichresultsinlessdesirablenegatives
which can be used to further enhance the object suchasuncommonnegativecaptionsincluding“A
countingcapabilityforLMMs. Again,wecurate babywithbluehatandorangevest". Hereweintro-
counterfactualimagesandcaptionsasinFigure2 ducetheFlickr30k-AttributesdatasetasinFigure2
(b). We count the number of bounding boxes n (c),whichfirstutilizesGPT-4V(OpenAI,2023c)
j
for each object category j ∈ I in the Flickr30k- togeneratehardnegativecaptionsandthenlever-
Entities dataset (Plummer et al., 2016). For two agesDALLE-3(Betkeretal.,2023)forgenerating
distinctobjectcategories(a,b),wegenerateapos- corresponding high-quality negative images. We
itive caption “there are n a a and n b b”, such as extract the first and most detailed caption C I for
“therearethreecatsandfourdogs”whereobjecta each image I from the originalFlickr30k dataset
is“cat”andobjectbis“dog”. (Youngetal.,2014).
Generating negative captions. If the two ob- Generatingnegativecaptions. Togeneratehigh-
jectsdonotspatiallyoverlapwithanyotherobject qualitynegativecaptions,weleverageGPT-4Vand
in the image, we create our negative caption by feedbothpositiveimagesandcaptionstogenerate
decrementing the one with more objects, b, and negativecaptions. ToletGPT-4Vknowtheobjects’
incrementing the one with less objects, a; for ex- locations,weoverlayboundingboxesontotheorig-
ample, “therearefourcatsandthreedogs”. We inalpositiveimage(Caietal.,2023)andalsofeed
onlyincrement/decrementby1thenumberofob- thisannotatedimageintoGPT-4V.WepromptGPT-
jects because this enforces a hard counterfactual 4V to generate three kinds of captions, including
5(i)changinganoun,(ii)changinganadjective,and 4.1 ImplementationDetails
(iii) swapping any of the two phrases, nouns, ad-
WhengeneratingimagesforFlickr30k-Attributes,
jectives, etc. Together, we curate our fine-tuning
weprovideDALLE-3(Betkeretal.,2023)withhd
datasetFlickr30-Attributes. Forexample,giventhe
qualityandnaturalstyle. WetrainCLIP(Radford
originalcaption“amanwearingablackshirtand
etal.,2021;Ilharcoetal.,2021)andLLaVA(Liu
bluejeans",GPT-4Vcangeneratecorresponding
et al., 2023a) on 1 and 4 NVIDIA A100 80GB
negatives: (i)“amanwearingablackjacketand
GPUs, respectively. When fine-tuning CLIP, we
bluejeans",(ii)“amanwearingablackshirtand
setAdamoptimizer’sβ andβ tobe0.9and0.98,
1 2
redjeans", and(iii)“amanwearingablueshirt
andweightdecaytobe0.2.
andblackjeans". DetailedpromptstoGPT-4Vare
For Flickr30k-Attributes, we trained the CLIP
showninAppendixA.
model with a learning rate of 1e-5, batch size of
Generating negative images. We simply feed 256,andmixedprecisionfor5epochs. ForLLaVA-
thegeneratedcaptionsfromGPT-4Vdirectlyinto 1.5,weusedalearningrateof2e-6andbatchsize
thecurrentmostcapabletext-to-imagegeneration of16for1epoch. ForFlickr30k-Positions,weused
model,DALLE-3(Betkeretal.,2023),togenerate a learning rate of 2.56e-5 and batch size of 1024
high-qualityimages. forCLIP,andtrainedfor50epochswithoutgroup-
ing and 15 epochs with grouping. For Flickr30k-
3.5 Fine-tuningMethodology
Counting,CLIPwastrainedwithalearningrateof
AsdemonstratedinFigure3,CounterCuratecanbe 5e-5. TheLLaVAparametersforbothFlickr30k-
used to fine-tune both contrastive (Radford et al., PositionsandFlickr30k-Countingwereidentical,
2021)andgenerative(Liuetal.,2023b)multimodal withalearningrateof2.56e-5andbatchsizeof16.
models, which significantly improve their perfor-
mance as shown in experiments. To fine-tune a 4.2 EvaluatingPositionalUnderstanding
contrastivemodellikeCLIP,wesampleN positive
Again, to the best of our knowledge, we are the
image-text pairs and the corresponding negative
first to comprehensively evaluate a multimodal
image-textpairstoformabatch,asshowninFig-
model’sabilityinunderstandingtheleft-and-right
ure 3 (a). We refer to this method as Grouping.
and/or above-and-below positioning relations be-
Inthisway,ourmodelisforcedtodistinguishthe
tween objects. We conduct a 4:1 train-test split
positivepairsfromtheircorrespondingnegatives,
on Flickr30k-Positions and use the test subset as
whichisdemonstratedtobehelpfulinexperiments.
the benchmark. We further separate this dataset
Wealsoconductadetailedanalysisoftheeffective-
into 3 subsets: left-and-right only, above-and-
ness of this method in Section 4.5. The training
belowonly,andboth. Weevaluateourfine-tuned
lossisthesameastheoriginal,wherecross-entropy
CLIP models by comparing the CLIP similar-
lossisusedtomaximizethediagonalelementsand
ity scores between the four image-caption pairs,
minimizeallotherentriesinthesimilaritymatrix.
(C,I),(C,I′),(C′,I),(C′,I′)fromeachgroupof
ForthegenerativemodelslikeLLaVA(Liuetal.,
datainthedataset,where(C,I,C′,I′)denotesthe
2023b,a),wereformatourdataintoconversations
positiveandnegativeimage/caption,respectively.
and follow their exact training paradigm to fine-
Themodelreceivesascoreof0.5ifs > s
(C,I) (C,I′)
tune,asshowninFigure3(b). Thetraininglossis
andanotherscoreof0.5ifs < s ,where
(C′,I) (C′,I′)
theoriginalnext-tokenpredictionloss.
s istheCLIPcosine-similarityscorebetween
(C,I)
captionC andimageI. ForLLaVA-1.5,wesimply
4 Experiments
querythemodeltochoosebetweenthepositiveand
Weutilizetheproposedthreedatasets,Flickr30k- negativecaptionswhenpresentedwiththeground
Positions, Flickr30k-Counting, and Flickr30k- truthimage. TheresultsareshowninTable1.
Attributes,createdviaCounterCurate,tofine-tune Aswehypothesized,LMMsareindeedlargely
both contrastive and generative models. Specifi- oblivious to the objects’ positioning in the im-
cally,weselecttwocommonmultimodalmodels, age,whichisespeciallymanifestedinthevanilla
ViT-B/32from(Ilharcoetal.,2021)OpenCLIPpre- CLIP’sperformance,whichisonlymarginallybet-
trained on LAION-2B (Schuhmann et al., 2022), ter than random guessing. Vanilla LLaVA-1.5
and LLaVA-1.5 (Liu et al., 2023a) as the base showsslightlybetterperformance.
model. After fine-tuning with the training split of
6Models LR AB Both AppendixF.
Random 50.00 50.00 50.00
4.4 EvaluatingSemanticCompositional
CLIP 50.55 52.80 51.56
Reasoning
+CounterCurate 75.88 91.52 84.90
SimilartothesetupinSec4.3,wefine-tuneboth
LLaVA-1.5 61.84 56.69 59.17
+CounterCurate 95.72 96.21 96.00 CLIPandLLaVA-1.5underourcuratedFlickr30k-
Attributes. We use another common benchmark
Table1:PerformanceofCLIPandLLaVAonFlickr30k- SugarCrepe (Hsieh et al., 2023) to evaluate the
Positions’stestdataset. “LR”meansleft-and-right,and
model’ssemanticcompositionalcapabilities. The
“AB”meansabove-and-below.
benchmarkhasthreemajorcategoriesasdepicted
inTable3. Theevaluationprotocolisalsothesame
CLIP LLaVA-1.5
as Sec 4.3. We choose NegCLIP (Yuksekgonul
Vanilla 57.50 44.87 etal.,2023)andGPT-4V(OpenAI,2023c)asthe
+CounterCurate 68.51 50.74 representativestrongbaselinesforcontrastiveand
generativemodelsrespectively.
Table2: ComparisonbetweenCLIPandLLaVAonthe
We observe significant improvements for both
benchmarkcreatedoutofPointQA-LookTwice.
CLIPandLLaVA-1.5,bothonaverageandcategor-
ically. ThemainresultsareshowninTable3. For
Flickr30k-Positions,bothmodelsperformsignifi- detailedscoresforeachsubcategoryinSugarCrepe,
cantlybetteracrossallsubsets. Specifically,forthe pleaseseeAppendixG.
mixedcase,CLIPimprovesby33%,andLLaVA For example, CounterCurate fine-tuned CLIP
achieves a high accuracy of 96%. These results model surpasses NegCLIP on average as well as
demonstratethatCounterCurateishighlyeffective in two main categories. Note that the source of
acrossdifferentkindsofmultimodalmodels. fine-tuningdata,Flickr30k-Entities,containsmuch
fewer image-caption pairs than COCO-Captions
4.3 EvaluatingObjectCounting
(around100kimage-captionpairs)whereNegCLIP
Hereweshowthecountingcapabilityofourmod- istrained. Furthermore,whenpromptingGPT-4V
els fine-tuned on the Flickr30k-Counting dataset. togeneratethenegativecaptions,weintentionally
Weselectadifferentdataset,PointQA-LookTwice promptedGPT-4VtoproduceNonewhenthereis
dataset(Manietal.,2022),astheevaluationbench- nofeasibleconditiontoconduct“swapping". Such
mark. Specifically, PointQA-LookTwice is de- two factors lead to the fact that our data curation
signedtoaskmodelsthenumbern j ofoccurrences pipelineresultsmuchfewernegativesamplesfor
of an object j in an image I. We reformat this the “swap” category, which is around 2.5k. We
datasetsuchthatforeveryj ∈ I withobjectname arguethatusingourpipelineonotherdatasetsthat
c j, we generate a positive caption C j = “There cangeneratemoreofthe“swap”attributeshalllead
aren j c j”andanegativecaptionC j′ =“Thereare tolargerimprovementsinperformance.
n j+1c j”. Forexample,giventhepositivecaption LLaVA-1.5 also performed better in all three
“thereare3dogs”,thenegativecaptionis“thereare categoriesafterfine-tuning. Itisalsosurprisingthat
4 dogs”. We use a similar evaluation protocol as ourfine-tunedmodeloutperformstheSOTALMM
Section4.2. Wemarkthatamodelmadeacorrect GPT-4Vbothonaverageandintwocategories,the
predictionif(i)CLIPshowss (Cj,I) > s (C j′,I) and mostsignificantboostoverthe“add”category. We
(ii)LLaVA-1.5correctlychoosestheoptionforC . observe improvements on other datasets (Diwan
j
TheresultsareshowninTable2. etal.,2022)aswell,attachedinAppendixE.
As CLIP performs slightly better than random CounterCurateshowsbetterperformancecom-
guessing,itissurprisingthatLLaVA-1.5performs paredtothepriorrule-basedmethodsorlessdesir-
worsethanrandom. Nevertheless,fine-tuningwith ablemodelsthatgeneratesnegatives. Thesesignifi-
Flickr30k-Countingimprovesbothmodels’count- cantimprovementsshowthatfine-tuningwithaccu-
ingcapability. Thisshowstheeffectivenessofus- rateandhardnegativesamplesisimportant,again
ingGLIGEN-generatednegativeimagesinCoun- demonstrating the effectiveness of our data cura-
terCuratetotackletheproblemofcounting. Further tion and fine-tuning pipeline for both contrastive
ablationsonremovingthenegativeimage-caption modelsandtextgenerationmodelsforimproving
pairs or without using grouping can be found in semanticcounterfactualunderstanding.
7Categories CLIP NegCLIP +CounterCurate LLaVA-1.5 GPT-4V +CounterCurate
Add 84.82 87.28 89.44(+4.62) 86.02 91.63 97.13(+11.11)
Replace 82.40 85.36 87.10(+4.70) 92.38 93.53 92.82(+0.43)
Swap 65.42 75.31 72.22(+6.80) 85.95 88.21 90.88(+4.93)
Average 81.23 84.85 86.15(+4.92) 89.27 92.19 94.17(+4.90)
Table3: ComparisonbetweenperformancesofCLIPandLLaVA-1.5onSugarCrepebeforeandafterfine-tuning.
WealsoaddNegCLIP(Yuksekgonuletal.,2023)andGPT-4V(OpenAI,2023c)asstrongbaselinesforcontrastive
andgenerativemultimodalmodels. Thebestperformancesarebolded,andimprovementsagainstCLIPandLLaVA
aremeasuredintheparentheses. CounterCurateshowssignificantperformanceboostcomparedtobothvanilla
CLIP/LLaVA-1.5modelandadvancedmodelssuchasGPT-4V.
4.5 In-DepthAnalysis scoreof84.67%,whichdemonstratesthehighqual-
ityoftheDALLE-3generatedimages.
EffectivenessofNegativeImages,NegativeCap-
tions,andGrouping ThecoreofCounterCurate
isto(i)fine-tunemodelswithbothnegativeimages Performanceonzero-shotvision-languagetasks
andnegativecaptionsand(ii)usegroupingtohelp To evaluate whether fine-tuning on the counter-
themodelbetterdistinguishthepositivepairsfrom factual data hurts the original zero-shot vision-
thenegatives. Hereweconductrigorousablation languageperformance,wecomparetheimageand
studiestodemonstratethenecessityofeachcompo- textretrievalperformanceonMSCOCO(Linetal.,
nent. Weusethesametrainingparametersandtest 2015) of the original CLIP model and Counter-
with the same methods as we did for fine-tuning Curate fine-tuned model on Flickr30k-Attributes.
CLIPandevaluatingitonFlickr30k-Positions. The results in Table 5 show that on average, the
modelfine-tunedviaCounterCuratemaintainsper-
formancewithmarginaldifferencecomparedtothe
Negative Negative Group
LR AB Both
Images Captions -ing originalCLIPmodel.
× × × 50.55 52.80 51.56
× ✓ × 55.86 62.25 58.68 Scores CLIP +CounterCurate
✓ × × 54.35 56.79 54.95
Image@1 39.44 37.81
✓ ✓ × 69.99 91.24 76.88 Image@5 65.43 64.24
✓ ✓ ✓ 75.88 91.52 84.90
Text@1 56.48 56.96
Text@5 79.74 80.12
Table4: Ablationstudydemonstratingthenecessityof
using (i) negative images, (ii) negative captions, and Average@1 47.96 47.39
Average@5 72.59 72.18
(iii) grouping strategies. Models are fine-tuned and
evaluatedontheFlickr30k-Positionsdataset.
Table5: Comparisonofimageandtextretrievalpreci-
sionscoresontheMSCOCOdatasetbetweentheorig-
AsshowninTable4, eventhoughusingeither
inal CLIP model and CounterCurate fine-tuned CLIP.
the negative caption or the negative image can Thelatterisabletomaintainoverallperformancewith
improveperformance,thescoresaresignificantly minorimprovementsintextretrievalprecision.
worsethanwhenusingbothelementstofine-tune.
ThisdemonstratesthatCounterCurate,whichincor-
5 Conclusion
poratesbothnegativecaptionsandnegativeimages,
isnecessarytoachievedesirableimprovements. It
In conclusion, CounterCurate significantly en-
alsoshowsthatusinggroupingallowsfurtherim-
hancesthevisio-linguisticcompositionalreasoning
provementscomparedtonotusingthisstrategy.
capabilitiesofmultimodalcontrastiveandgenera-
Correctness of DALLE-3 generated images tivemodels. Thisisachievedbyaddressingthene-
We randomly sample 300 DALLE-3 images gen- glectofphysicallygroundedreasoningandexploit-
erated from the negative captions in Flickr30k- ingthepotentialofusingtextandimagegeneration
Attributesforhumanannotatorstocheckwhether modelsforsemanticcounterfactualfine-tuning. We
the generated image is consistent (matches) with believeourcontributionscanpavethewayforfur-
the negative captions. We obtain a consistency therresearchincompositionalreasoning.
8Acknowledgement CLawrenceZitnick.2015. Microsoftcococaptions:
Datacollectionandevaluationserver. arXivpreprint
This work was supported in part by NSF CA- arXiv:1504.00325.
REER IIS2150012, and Institute of Information
Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed
&communicationsTechnologyPlanning&Eval-
ElKholy, FaisalAhmed, ZheGan, YuCheng, and
uation(IITP) grants funded by the Korea govern- Jingjing Liu. 2020. Uniter: Universal image-text
ment(MSIT)(No. 2022-0-00871,Developmentof representationlearning. InEuropeanconferenceon
AIAutonomyandKnowledgeEnhancementforAI computervision,pages104–120.Springer.
AgentCollaboration)and(No. RS2022-00187238,
AnujDiwan,LayneBerry,EunsolChoi,DavidHarwath,
Development of Large Korean Language Model and Kyle Mahowald. 2022. Why is winoground
TechnologyforEfficientPre-training). hard? investigatingfailuresinvisuolinguisticcompo-
sitionality. arXivpreprintarXiv:2211.00768.
This research is funded in part by the Univer-
sityofWisconsin–MadisonL&SHonorsProgram AmirHertz,RonMokady,JayTenenbaum,KfirAber-
throughaTrewarthaSeniorThesisResearchGrant. man, Yael Pritch, and Daniel Cohen-Or. 2022.
Prompt-to-promptimageeditingwithcrossattention
Limitations control. arXivpreprintarXiv:2208.01626.
Cheng-YuHsieh,JieyuZhang,ZixianMa,Aniruddha
We acknowledge that DALLE-3, one of the gen-
Kembhavi,andRanjayKrishna.2023. Sugarcrepe:
erative models we use, is a closed-source model,
Fixing hackable benchmarks for vision-language
whichmakesitdifficulttosystematicallyanalyze compositionality.
itsbehavior.
GabrielIlharco,MitchellWortsman,RossWightman,
CadeGordon,NicholasCarlini,RohanTaori,Achal
Dave,VaishaalShankar,HongseokNamkoong,John
References
Miller,HannanehHajishirzi,AliFarhadi,andLud-
wigSchmidt.2021. Openclip.
Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,
Antoine Miech, Iain Barr, Yana Hasson, Karel
ChaoJia,YinfeiYang,YeXia,Yi-TingChen,Zarana
Lenc,ArthurMensch,KatherineMillican,Malcolm
Parekh,HieuPham,QuocLe,Yun-HsuanSung,Zhen
Reynolds,etal.2022. Flamingo: avisuallanguage
Li, and Tom Duerig. 2021. Scaling up visual and
model for few-shot learning. NeurIPS, 35:23716–
vision-language representation learning with noisy
23736.
textsupervision. InInternationalconferenceonma-
chinelearning,pages4904–4916.PMLR.
JamesBetker,GabrielGoh,LiJing,TimBrooks,Jian-
fengWang,LinjieLi,LongOuyang,JuntangZhuang,
TiepLe,VasudevLal,andPhillipHoward.2023. Coco-
Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla
counterfactuals: Automaticallyconstructedcounter-
Dhariwal, Casey Chu, Yunxin Jiao, and Aditya
factualexamplesforimage-textpairs.
Ramesh. 2023. Improving image generation with
bettercaptions.
Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou
Mu,JianweiYang,JianfengGao,ChunyuanLi,and
MuCai,HaotianLiu,SivaKarthikMustikovela,Gre-
Yong Jae Lee. 2023. Gligen: Open-set grounded
gory P. Meyer, Yuning Chai, Dennis Park, and
text-to-imagegeneration.
Yong Jae Lee. 2023. Making large multimodal
models understand arbitrary visual prompts. In Tsung-YiLin,MichaelMaire,SergeBelongie,Lubomir
arXiv:2312.00784.
Bourdev,RossGirshick,JamesHays,PietroPerona,
DevaRamanan,C.LawrenceZitnick,andPiotrDol-
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-
lár.2015. Microsoftcoco: Commonobjectsincon-
giovanni, Piotr Padlewski, Daniel Salz, Sebastian
text.
Goodman, Adam Grycner, Basil Mustafa, Lucas
Beyer,AlexanderKolesnikov,JoanPuigcerver,Nan ZhiqiuLin,XinyueChen,DeepakPathak,Pengchuan
Ding,KeranRong,HassanAkbari,GauravMishra, Zhang, and Deva Ramanan. 2023. Visual-
LintingXue,AshishVThapliyal,JamesBradbury, gptscore: Visio-linguistic reasoning with multi-
Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, modalgenerativepre-trainingscores. arXivpreprint
Burcu Karagol Ayan, Carlos Riquelme Ruiz, An- arXiv:2306.01879.
dreasPeterSteiner,AneliaAngelova,XiaohuaZhai,
Neil Houlsby, and Radu Soricut. 2023. PaLI: A Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae
jointly-scaled multilingual language-image model. Lee.2023a. Improvedbaselineswithvisualinstruc-
InTheEleventhInternationalConferenceonLearn- tiontuning.
ingRepresentations.
Haotian Liu, Chunyuan Li, Qingyang Wu, and
Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr- Yong Jae Lee. 2023b. Visual instruction tuning.
ishna Vedantam, Saurabh Gupta, Piotr Dollár, and arXiv:2304.08485.
9Arjun Mani, Nobline Yoo, Will Hinthorn, and Olga Appendix
Russakovsky. 2022. Point and ask: Incorporating
pointingintovisualquestionanswering. A PromptguidancetoGPT-4Vfor
Flickr30k-Attributes
Stephen L Morgan and Christopher Winship. 2015.
Counterfactuals and causal inference. Cambridge
UniversityPress. Asaminordetailspecifictothedatasetitself,we
extracted the objectified caption C′ from the an-
OpenAI. 2023a. Chatgpt. https://openai.com/ I
notatedFlickr30k-Entitiesdataset(Plummeretal.,
blog/chatgpt/.
2016). Theobjectifiedcaptionshavesquarebrack-
OpenAI.2023b. Gpt-4technicalreport.
etsaddedtotheoriginalcaptionthatallowusand
OpenAI.2023c. Gpt-4v(ision)systemcard. theLMMstorefertoeachphrasefortheobjectj
inthecaptionwithauniqueID# .
BryanA.Plummer,LiweiWang,ChrisM.Cervantes, j
JuanC. Caicedo, Julia Hockenmaier, andSvetlana TheexactpromptguidanceweprovidedtoGPT-
Lazebnik. 2016. Flickr30k entities: Collecting 4V(OpenAI,2023c)isasfollows:
region-to-phrasecorrespondencesforricherimage-
You are given an image, the same image but
to-sentencemodels.
with bounding boxes, its corresponding caption
AlecRadford,JongWookKim,ChrisHallacy,Aditya
and an enhanced form of the caption. Their
Ramesh,GabrielGoh,SandhiniAgarwal,GirishSas-
format is as follows: Original Caption: A
try, Amanda Askell, Pamela Mishkin, Jack Clark,
GretchenKrueger,andIlyaSutskever.2021. Learn- child in a pink dress is helping a baby in a
ingtransferablevisualmodelsfromnaturallanguage blue dress climb up a set of stairs in an en-
supervision. try way. Enhanced Caption: [/EN#1/people A
Christoph Schuhmann, Romain Beaumont, Richard child] in [/EN#2/clothing a pink dress] helping
Vencu,CadeGordon,RossWightman,MehdiCherti, [/EN#3/peopleababy]in[/EN#4/clothingablue
Theo Coombes, Aarush Katta, Clayton Mullis,
dress] climb up [/EN#5/other a set of stairs] in
MitchellWortsman,PatrickSchramowski,Srivatsa
[/EN#6/sceneanentryway]. Intheenhancedcap-
Kundurthy, Katherine Crowson, Ludwig Schmidt,
RobertKaczmarczyk,andJeniaJitsev.2022. Laion- tion,thereisnonewdata,butthateach“entity”is
5b: An open large-scale dataset for training next markedbyapairofsquarebrackets. Mostentities
generationimage-textmodels. each correspond to one or more bounding boxes,
HugoTouvron,ThibautLavril,GautierIzacard,Xavier which will be specified. For example, entity 1 in
Martinet,Marie-AnneLachaux,TimothéeLacroix, the sentence is “A child”, which is marked by a
Baptiste Rozière, Naman Goyal, Eric Hambro, tag [/EN#1/people ...]. “people” states the type
Faisal Azhar, et al. 2023. Llama: Open and effi-
oftheentity. Ifentityis“other”,thenthereareno
cient foundation language models. arXiv preprint
arXiv:2302.13971. restrictionsapplied.
Youaretaskedto:
Michal Yarom, Yonatan Bitton, Soravit Changpinyo,
Roee Aharoni, Jonathan Herzig, Oran Lang, Eran Generateacaptionthatchangestheobjectbeing
Ofek,andIdanSzpektor.2023. Whatyouseeiswhat discussedinexactlyoneoftheentities. YouMUST
youread? improvingtext-imagealignmentevalua- ensurethatthenewobjectisthesametypeofentity
tion. InThirty-seventhConferenceonNeuralInfor-
astheoriginalobjectasspecifiedinthetag. Forex-
mationProcessingSystems.
ample: [/EN#1/peopleAchild]=>[/EN#1/people
PeterYoung,AliceLai,MicahHodosh,andJuliaHock- Anadult]isallowed,but[/EN#1/peopleAchild]
enmaier. 2014. From image descriptions to visual
=>[/EN#1/peopleAcat]isnotallowedbecausea
denotations: Newsimilaritymetricsforsemanticin-
catisnot“people”;
ferenceovereventdescriptions.
Generate a caption that changes the qualifier
MertYuksekgonul,FedericoBianchi,PratyushaKalluri,
(such as an adjective of a quantifier) that de-
DanJurafsky,andJamesZou.2023. Whenandwhy
vision-languagemodelsbehavelikebags-of-words, scribes the object in exactly one of the entities.
andwhattodoaboutit? InInternationalConference For example: [/EN#2/clothing a pink dress] =>
onLearningRepresentations.
[/EN#2/clothingagreendress].
Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Generate,ifpossible,acaptionthatreversestwo
Yang,LeiZhang,LijuanWang,YejinChoi,andJian- oftheentitiesortheirqualifierssuchthattheorigi-
fengGao.2021. Vinvl: Revisitingvisualrepresen-
nalsentencestructureisnotchanged,butproduces
tationsinvision-languagemodels. InProceedings
anegativeprompt. Forexample,giventwoentities
oftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pages5579–5588. “agreendress”and“ablueblouse”,youcaneither
10Categories CLIP NegCLIP +CounterCurate LLaVA-1.5 GPT-4V +CounterCurate
AddObject 87.15 88.78 90.35(+3.20) 87.83 91.59 97.82(+9.99)
AddAttribute 77.89 82.80 86.71(+8.82) 80.64 91.76 95.09(+14.45)
ReplaceObject 93.77 92.68 95.94(+2.17) 96.73 96.31 98.37(+1.64)
ReplaceAttribute 82.61 85.91 87.94(+5.33) 92.64 93.53 93.53(+0.89)
ReplaceRelations 68.92 76.46 76.24(+7.32) 87.13 90.26 85.92(-1.21)
SwapObject 60.00 75.51 68.57(+8.57) 84.90 83.13 85.72(+0.82)
SwapAttribute 67.42 75.23 73.57(+6.15) 86.34 90.09 92.79(+6.45)
Average 81.23 84.85 86.15(+4.92) 89.27 92.19 94.17(+4.90)
Table6: DetailedversionofTable3: comparisonofperformanceovereachsub-category.
swapthetwoentities’orderorswaptheadjectives climbupasetofstairsinanentryway.”}}
andproduce “ablue dress”and“a greenblouse”.
Ifyoucannotgenerateone,reportNone. B PromptguidancetoGPT-4Vfor
Flickr30k-Positions(above-and-below)
Allinall,thenewdescriptionmustmeetallof
theserequirements:
Togeneratehigh-qualitycounterfactualimagesfor
1. The change of attribute must be sufficiently
theabove-and-belowsubsetofFlickr30k-Positions,
different to make the new description inaccurate,
weusethere-writingtechniquetogenerateconvinc-
butitshouldalsobesomewhatrelatedtobechal-
ing and detailed captions for GLIGEN (Li et al.,
lengingtoanAImodel.
2023). Specifically,weleverageGPT-4V(OpenAI,
2. Comparedtotheoriginaldescription,thenew
2023c)withthefollowingprompt:
description must differ in only one attribute. All
I will give you a caption in the format "A is
otherdetailsmustbekeptthesame.
above B." You need to expand the sentence such
3. Thenewdescriptionmustmimicthesentence
thatthemeaning"AisaboveB"ispreservedand
structureoftheoriginaldescription.
your answer is reasonable for a human to under-
4. Thenewdescriptionmustbefluent, logical, stand what you’re describing. Do not make the
andgrammaticallycorrect. answertoolong;onelongsentenceisenough. For
5. Carefullylookattheimage,andgivenegative example, if i give you "a man is under a dog", a
captionsthatarereasonablegiventheobjects’po- goodanswerwouldbe"thereisamanrestingon
sition,size,andrelationshiptotheoverallsetting. the ground, and there is a dog lying above him."
6. Pose challenging(difficult enough) negative One restriction: A and B do not overlap. This
captionssothatalargemultimodaltextgeneration means that if I ask you to expand "a hat is below
model should struggle to distinguish the original water",youmustnotassumethatthehatisbelow
captionv.s. negativecaption. water. RememberthatyouMUSTincludebothA
Here are some examples whose output for- andBinyouranswer,likemyexampledid.
mat you should follow: Original Caption: A
child in a pink dress is helping a baby in a C ObjectRemovalvsInpaintingPlants
blue dress climb up a set of stairs in an en-
We explain here as for why we do not use object
try way. Enhanced Caption: [/EN#1/people A
removal tools to curate our data for Flickr30k-
child] in [/EN#2/clothing a pink blouse] helping
Counting as we make one example in Figure
[/EN#3/peopleababy]in[/EN#4/clothingablue
5. We used a tool from a public Github
dress] climb up [/EN#5/other a set of stairs] in
Repository https://github.com/treeebooor/
[/EN#6/sceneanentryway]. BoundingBoxes: #1:
object-remove and compared it with GLIGEN
purple Your answer: {“noun”: {“action”: (1, “a
(Li et al., 2023)’s inpainting results. We found a
child”,“anadult”),“caption”: “Anadultinagreen
significantpatternthatwhenobjectremovalfailed,
dressishelpingababyinabluedressclimbupaset
mostofGLIGEN’sinpaintingsucceeded.
ofstairsinanentryway.”]},“adjective”: {“action”:
(2,“apinkdress”,“agreendress”),“caption”: “A
D OverfittinginAbove-BelowSubset?
child in a green dress is helping a baby in a blue
dress climb up a set of stairs in an entry way.”}, Insection4.2,weobservedthatbothmodelsper-
“reverse”: {“action”: (2, 4), “caption”: “A child formedbetterontheabove-and-belowsubset,with
inablueblouseishelpingababyinapinkdress CLIP (Radford et al., 2021; Ilharco et al., 2021)
11tuningonFlickr30k-Attributes,showedsignificant
improvementsonthetextscoreofWinoground.
Model TextScore
CLIP (Radfordetal.,2021) 25.25
(ViT-B/32)
UNITER (Chenetal.,2020) 32.25
base
UNITER (Chenetal.,2020) 38.00
large
VinVL(Zhangetal.,2021) 37.75
BLIP2(Zhangetal.,2021) 44.00
PALI(Chenetal.,2023) 46.50
LLaVA-1.5(Liuetal.,2023a) 65.85
LLaVA-1.5+CounterCurate 69.15(+3.30)
(a)OriginalImage
Table 7: Our fine-tuned model of LLaVA-1.5 with
Flickr30k-Attributes shows significant improvements
on the difficult visio-linguistic reasoning dataset
Winoground.
F MoreAblationsonFlickr30k-Counting
(b)ObjectRemoval (c)GLIGENInpaintedPlant Wealsotestnotusinganyofthegeneratednegative
image-captionpairsandnotusinggroupingwhile
Figure5: Wewanttoremove/coverthepersononthe
fine-tuningCLIPwithFlickr30k-Counting. Eval-
rightwiththeirskateboardinimage(a)completely. Ob-
uationresultsonPointQA-LookTwiceareshown
jectremovalonlyachievesthegoalpartiallyandmakes
in Table 8. Both ablations showed much less im-
theimagemuchlessnatural,whileGLIGENinpainted
aplantperfectlyintotheimageasitalsopreservedthe provement,showcasingthatusinggroupingandthe
restoftheinformationintheimage. negativeimage-captionpairsismorepowerfulin
improvingmodels’countingabilities.
performingsignificantlybetter. However,weargue
ModelSetting Accuracy
thatthisisnotcausedbyoverfittingbecausethere
Vanilla 57.50
isanevennumberofboth"AisaboveB"captions
vs. "AisbelowB"captions,andbothcaptionsare +CounterCurate(NoNeg) 60.85
+CounterCurate(NoGroup) 65.70
notalwaysmatchedtoageneratedimagebyGLI-
+CounterCurate 68.51
GEN (Li et al., 2023) or the original image from
Flickr30k(Youngetal.,2014). Sincethetraining
Table8:MoreablationsontheCLIPmodeltrainedwith
andtestingdatasetsareshuffledandseparatedfrom
Flickr30k-Counting. the score without any negatives
the same dataset, this ensures that neither model
andthescorewithoutgrouping.
canfindapatternin,forexample,recognizingthe
generatedimageapartfromtheoriginalimageand
choosinganoptiononthatbasis. Anotherproofis G MoreResultsonSugarCrepe
that since LLaVA-1.5 (Liu et al., 2023a) can per-
Hereweshowtheperformanceimprovementsfor
formequallywellonbothsubsetsafterfine-tuning,
everysub-categoryofSugarCrepeinTable6. Over-
the two subsets are shown to be at least as well-
all,CounterCurateshowsclearperformancegain
defined as the other. We also welcome others to
over the two base models, CLIP (Radford et al.,
useourdatasetasabenchmarktotesttheirmodel’s
2021) and LLaVA (Liu et al., 2023b). We also
abilitytounderstandobjectpositioninginimages.
point out that the “swap object” category in Sug-
E ResultsontheWinogroundDataset arCrepe(Hsiehetal.,2023)onlyhas246itemsas
comparedtoothercategories’500+,1000+items;
WeevaluatedourLLaVA-1.5(Liuetal.,2023a)on this means that the performance on this specific
Winoground(Diwanetal.,2022),amodelspecifi- categorycouldshowmorefluctuationscausedby
callymadetotestamodel’slimitofvisio-linguistic thetrainingprocess.
capabilities via human-annoted difficult image-
caption pairing questions. Our model, after fine-
12