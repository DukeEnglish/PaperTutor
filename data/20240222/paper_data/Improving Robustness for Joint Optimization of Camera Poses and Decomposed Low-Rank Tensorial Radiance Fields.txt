Improving Robustness for Joint Optimization of Camera Poses
and Decomposed Low-Rank Tensorial Radiance Fields
Bo-YuCheng,Wei-ChenChiu,Yu-LunLiu
NationalYangMingChiaoTungUniversity
tomas1999.ee06@nycu.edu.tw,walon@cs.nctu.edu.tw,yulunliu@cs.nycu.edu.tw
Abstract
Inthispaper,weproposeanalgorithmthatallowsjointrefine-
mentofcameraposeandscenegeometryrepresentedbyde-
composedlow-ranktensor,usingonly2Dimagesassupervi-
sion.First,weconductapilotstudybasedona1Dsignaland
relateourfindingsto3Dscenarios,wherethenaivejointpose
optimization on voxel-based NeRFs can easily lead to sub-
optimalsolutions.Moreover,basedontheanalysisofthefre-
quency spectrum, we propose to apply convolutional Gaus-
sianfilterson2Dand3Dradiancefieldsforacoarse-to-fine
trainingschedulethatenablesjointcameraposeoptimization.
Leveragingthedecompositionpropertyindecomposedlow- Figure 1: Robust joint pose refinement on decomposed
ranktensor,ourmethodachievesanequivalenteffecttobrute- tensor. Our method enables joint optimization of camera
force3Dconvolutionwithonlyincurringlittlecomputational poses and decomposed voxel representation by applying
overhead.Tofurtherimprovetherobustnessandstabilityof efficient separable component-wise convolution of Gaus-
jointoptimization,wealsoproposetechniquesofsmoothed sianfilterson3Dtensorvolumeand2Dsupervisionimages.
2D supervision, randomly scaled kernel parameters, and
edge-guided loss mask. Extensive quantitative and qualita-
tive evaluations demonstrate that our proposed framework
achievessuperiorperformanceinnovelviewsynthesisaswell voxelgridviareplacingthedense3Dgridwithdecomposed
as rapid convergence for optimization. The source code is low-ranktensor.TensoRFachievesahighdatacompression
availableathttps://github.com/Nemo1999/Joint-TensoRF. ratioandlowcomputationalcostatthesametimewhilealso
achievingstate-of-the-artperformance.Providingawin-win
situationonmemoryusageandcomputationalefficiency,the
1 Introduction
decomposed low-rank tensor architecture has been widely
Inrecentyears,neuralrenderinghasbecomeawidely-used adopted in many recent works (Xu et al. 2023; Fridovich-
methodforhigh-qualitynovelviewsynthesis.NeRFasapi- Keiletal.2023;Goeletal.2022;HanandXiang2023;Shao
oneerwork(Mildenhalletal.2020)representsa3Dradiance etal.2023;Tangetal.2022;Meulemanetal.2023).
fieldasanimplicitcontinuousfunctionbuiltuponmultilayer
Ontheotherhand,theeffectivenessofNeRF(andmostof
perceptrons(MLPs)whichistrainedwithdifferentiablevol-
the aforementioned works) hinges on precise camera poses
ume rendering. While achieving excellent synthesis qual-
ofinputimages,whichareoftencalculatedusingStructure-
ity,NeRFsuffersfromtraining/inferenceinefficiencydueto
from-Motion(SfM)algorithmslikeCOLMAP(Scho¨nberger
denseevaluationofthecomputationallyexpensiveMLPs.
andFrahm2016).Whilesomeworks(Wangetal.2021;Lin
To this end, voxel-based methods built upon the explicit
etal.2021;Chngetal.2022)aimtobypasstheslowandoc-
scenerepresentationof3Dvoxelgrid(Sun,Sun,andChen
casionallyinaccurateCOLMAPprocessbyoptimizingcam-
2022; Fridovich-Keil et al. 2022; Liu et al. 2020) are pro-
era pose and scene representation jointly on the original
posedtoachievefastertrainingandprovidebetterrendering
MLP-basedNeRF,theirsuccessisoftentiedtothespectral
qualitythantheoriginalMLP-basedNeRF,hencebecoming
bias (Yu¨ce et al. 2022) of the MLP architecture which en-
themorepreferredchoicesfordownstreamapplications.
suresthesmoothnessof3Dradiancefieldearlyintraining.
Nevertheless,maintainingadense3Dvoxelgridisinturn
Voxel-basedmethods,however,lacksuchpropertiesandcan
memory intensive, thus still restricting wider applications
overemphasizesharpedges,makingnaivejointoptimization
of voxel-based methods. Fortunately, TensoRF (Chen et al.
problematicasgettingtrappedinlocaloptima(Fig.2(a)).
2022)proposestotacklesuchmemory-intensiveissueofthe
In this work, we present simple yet effective methods
Copyright©2024,AssociationfortheAdvancementofArtificial for refining the camera pose and the 3D scene using de-
Intelligence(www.aaai.org).Allrightsreserved. composed low-rank tensors (cf. Fig. 1). We identify that
4202
beF
02
]VC.sc[
1v25231.2042:viXracontrolling the frequency spectrum is vital for pose align- d 10k iterations 10k iterations
ment, while directly realizing such control in a dense 3D o h te Forward-facingview
m
gridcouldbenontrivial/challengingaswellascomputation-
e
Forward-facing view
v
ally demanding. To this end, we introduce an efficient 3D ïa
N
filtering method using component-wise separable convolu- )a ( 50k iterations 50k iterations
tionforenablingthespectralcontrol,whichismoreefficient 10k iterations 10k iterations
d Forward-facingview
thanthetraditionallywell-knowntrickofseparableconvo- o
h
lutionkernelasweadditionallyutilizetheseparabilityofthe te
m
input signal. To further ensure stability in the optimization ru
O
process,weproposeseveraltechniques,includingsmoothed )b
( 50k iterations 50k iterations
2D supervision, randomly scaled kernel paramter, and the RGB Depth Final camera poses
edge-guidedlossmask.Thesetechniquesareexperimentally
Figure 2: Comparison of naive joint pose optimization
provencrucialforsuccessfulposerefinementinourablation
and our proposed method on voxel-based NeRFs. (a)
studies. In results, our proposed method requires only 50k
Naively applying joint optimization on voxel-based NeRFs
trainingiterations,whereallthepreviousmethodstypically
leads to dramatic failure as premature high-frequency sig-
needs 200k iterations (e.g. the overall training time is re-
nals in the voxel volume would curse the camera poses to
ducedto25%,comparedtopreviousMLP-basedmethods).
stuckinlocalminima.(b)Weproposeacomputationallyef-
The main reason behind this advantage is not only based
fective manner to directly control the spectrum of the ra-
on property of voxel-based architecture, but also relies on
diance field by performing separable component-wise con-
our carefully designed efficient spectral filtering algorithm
volution of Gaussian filters on the decomposed tensor. The
thatrequiresonlysinglereusablevoxelgrid(pleasereferto
proposed training scheme allows the joint optimization to
Sec.4.3).Moreover,ourmethodperformsfavorablyagainst
convergesuccessfullytoabettersolution.
state-of-the-art methods on novel view synthesis. Our con-
tributionsarethree-fold:
• With1Dpilotstudy,weprovideinsightsintotheimpact
while(Chenetal.2022;Fridovich-Keiletal.2023)suggest
of spectral property of 3D scene on the convergence of
adoptingtensordecompositionfor3Dfeaturecompression,
jointoptimizationbeyondthecoarse-to-fineheuristicdis-
inwhichourmethodismainlybasedon(Chenetal.2022)
cussedinpriorresearch,andproposealearningstrategy
but can be adaptable to other tensor decomposition-based
built upon specially designed efficient component-wise
voxelstructureslikeK-Planes(Fridovich-Keiletal.2023).
convolutionalgorithm.
JointPoseEstimationonMLP-basedNeRFs.(Wangetal.
• To enhance the robustness of our joint optimization, we
2021) is one of the first NeRF-based attempts to tackle the
introducetechniquesofsmoothed2Dsupervision,scaled
joint problem of estimating camera poses and learning 3D
kernelparameters,andtheedge-guidedlossmask.
scenerepresentationbydirectlyadjustingcameraposeusing
• Trainingtimedropsby25%versusexistingMLP-based gradient propagation on neural radiance fields. The robust-
methods,withrequiringonly50kiterationsagainst200k ness of such joint optimization is further enhanced by (Lin
of previous methods. Results show state-of-the-art per- et al. 2021; Chng et al. 2022), where they propose various
formanceinnovelviewsynthesiswithunknownpose. methods to smooth the pose gradient derived from the un-
derlyingMLP.(Chenetal.2023a)furtherincreasesthenoise
2 RelatedWork tolerance by a specially designed local-global joint align-
mentapproach.Ourmethodalsotacklesjointproblemsbut
Accelerating Neural Rendering. As the seminal work of
isspecificallydesignedforthevoxel-basedNeRFbuiltupon
neural rendering, NeRF adopts MLPs to construct the im-
thedecomposedlow-ranktensorarchitecture.
plicitrepresentationofthe3Dscene,providinghigh-quality
view synthesis but having a time-consuming training pro- Pose Estimation on Decomposed Low-rank Tensors.
cess due to the computational demands of MLPs. For ad- Theredoexistworksthatoptimizecameraposeondecom-
dressingsuchissue,differentvariantsofNeRFareproposed posed low-rank tensor (Liu et al. 2023; Meuleman et al.
tousecustomspatialdatastructureswherethesceneinfor- 2023)butrequirerichadditionalgeometryclues(e.g.,depth
mationisdistributedonlylocallythusaidingfastertraining map and optical flow). To our best knowledge, we are the
andrendering,inwhichthosespatialdatastructuresinclude firstattempttojointlyoptimizethecameraposeandthede-
point cloud (Xu et al. 2022; Hu et al. 2023), space parti- composedlow-ranktensorusingonly2Dimagesupervision.
tioning tree (Wang et al. 2022; Yu et al. 2021), triangular Pose Estimation on Multi-Resolution Hash Encoding.
mesh (Chen et al. 2023b; Kulhanek and Sattler 2023), and Aside from decomposed low-rank tensor, multi-resolution
voxel gird (Sun, Sun, and Chen 2022; Fridovich-Keil et al. hashencodingisanothercompressedvoxel-basedarchitec-
2022; Liu et al. 2020; Hedman et al. 2021). Among these ture proposed by (Mu¨ller et al. 2022). Along with such a
variants, the voxel grid has become more popular due to choice of architecture, recently (Heo et al. 2023) has pro-
its easy implementation and quality reconstruction. How- posed to address the joint optimization of camera pose and
ever, as scene dimensions grow, the memory usage of the multi-resolution hash encoding. They suggest a new inter-
voxelgridbecomesinefficient.Toaddressthis,(Mu¨lleretal. polation scheme that provides smooth gradients hence pre-
2022)recommendscompressingthegridviahashencoding, ventinggradientfluctuationinthehashvolume,alongwithacurriculumlearningschemethatcontrolsthelearningrate training image I , warping function W (P, ) : R3 → R3
i 3d
ofthehashtableateachresolutionlevel.Althoughachiev- performs rigid 3D transformation parameterized by P ∈
ing impressive results on joint optimization, the effective- se(3) Lie algerbra, and W (P,s(⃗0,d⃗ )) maps each sam-
3d u
ness of their method is limited to multi-resolution hash en- ple 3D coordinate in canonical ray (⃗c = ⃗0) into a 3D sam-
codingandisnotapplicabletodecomposedlow-ranktensor,
plecoordinateofcameraraywithposeP.Notethatthisis
Whileourproposedseparablecomponent-wise3Dconvolu-
anill-posedreconstructionproblemthatsuffersfromshape-
tion (and randomly scaled kernel) is specifically designed
radianceambiguity(Zhangetal.2020).
fordecomposedlow-ranktensorandnotdirectlyapplicable
3D Joint Optimization. When it comes to jointly estimat-
tomulti-resolutionhashencoding,inwhichthesetworepre-
ing camera poses (where the camera poses P are also un-
sentationshavetheirrespectiveprosandcons.
known) and learning scene representation (Lin et al. 2021;
Chng et al. 2022; Chen et al. 2023a; Heo et al. 2023), the
3 OurProposedMethod
problemisevenmoreill-definedwiththeobjectivenowbe-
3.1 JointRefinementof3DScenesandPoses ingextendedfromEq.3anddefinedas:
V Bo aslu edm oe nR the en sd ee ttr ii nn gg of fo nr euR raa ld via on luc me eF ri ee nld deR rine gco inns Ntr eu Rc Ft ,io thn e. L joint(Fσ,Fc,P)=(cid:88)L (cid:88) ∥V(Fσ,Fc,W 3d(Pi,s(⃗0,d⃗ u)))−Iiu∥.
i=1u∈U
radiancefieldsrespectiveforgeometryandappearancefora (4)
3Dscenearerepresentedviatwofunctions(implementedby
Suchjointoptimizationishighlyinfluencedbythestructural
MLPs): F : R3 → R1 and F : R6 → R3, where F re-
σ c σ biasoftheunderlyingrepresentationof{F ,F },whichwe
turns the volume density of an input 3D coordinate, while σ c
willconductapilotstudywithasimpler1DcaseinSec.3.2.
F outputs the color at an input 3D coordinate given a 3D
c
viewingdirection.Forrenderingapixelon2Dcoordinateu
3.2 GaussianFilteron1DSignalAlignment
with its homogeneous form u¯ = [u;1]⊤, we first sample a
sequenceofN 3D-coordinates{s } alongthecam- Here we aim to analyze the effect of the signal spectrum
n n=1···N
era ray defined by the camera center ⃗c ∈ R3 and the ray (spectrum of F , F , and I in Eq. 4) on the joint optimiza-
c σ
directiond⃗ =K−1u¯, tion process. We begin by reducing 3D joint optimization
u
of camera pose and scene reconstruction into a simpler 1D
{sn}
n=1···N
=s(⃗c,d⃗ u)={⃗c+tn·d⃗ u}n=1···N, (1)
counterpartofsignalalignment.
whereK istheintrinsiccameramatrixand{t n} n=1···N are 1D Signal Alignment. Let us consider a target ground
N samplesequidistantlydistributedalongthedepthaxisin truth 1D signal f (assuming the signal to be continu-
GT
betweenthenearandfarplanesoftheviewfrustum.There- ous, bounded, and have finite support), which we aim to
sultantcolorofthepixelisobtainedbyintegratingthrough reconstruct and align with. We are given randomly trans-
thedensityfieldF σ andcolorfieldF cusingthevolumeren- latedversionsf 1,f 2 ofthegroundtruthsignalf GT,where
dering equation (Kajiya and Von Herzen 1984; Mildenhall f = W (f ,p ),f = W (f ,p ) with having W
1 1d GT 1 2 1d GT 2 1d
etal.2020),wherewedenotethediscretizedvolumerender- a signal translation operation defined as W (f,p)(x) =
1d
ingintregralbyafunctionV: f(x−p),andp ,p arethetranslationvalues.
1 2
V(Fσ,Fc,s(⃗c,d⃗ u))= (cid:88) Tn·αn·Cn, (2) inA orl dth eo ru togh mth ime ir cec to hn es ctr au sc eti oo fn 3i Dstr ji ov ii na tl oin pts iu mc ih zaa t1 ioD n,se wtt ein ag t-,
sn∈s(⃗c,d⃗)
tempttoestimateasignalgaswellasthetranslationvalues
where T n =
exp(−(cid:80)n
j=1δ jF σ(s j)) represents accumu- q 1andq 2viaadoptingtheiterativegradient-basedoptimiza-
latedtransmittancepriortos n,α n =1−exp(−δ nF σ(s n)) tiononthereconstructionloss.
representstheopacityofsamples n,andC n =F c(s n, d⃗ u ) L 1d(g,q1,q2)= (cid:88) (cid:90) ∥W 1d(g,qi)(x)−fi(x)∥2dx
representsthecolorofsamples ,andδ = ∥s −s ∥is
n j j j−1 i∈[1,2]
theeuclideandistancebetweentwoadjacentsamples. (cid:90) (5)
InthetypicalsettingofNeRF,givenasetofL2D-images = (cid:88) ∥g(x)−fGT(x−pi+qi)∥2dx.
I = {I ,··· ,I } with their corresponding camera poses i∈[1,2]
1 L
P={P ,··· ,P }∈se(3)Liealgebra(parametrizingrigid Note that Eq. 5 and Eq. 4 are analogous in terms of their
1 L
3D transformation as se(3) is a very common technique in structure/formulation, where the difference only lies in the
robotics, here we follow the usage of (Lin et al. 2021)) as dimensionality. And L 1d achieves the optimum whenever
input, we aim to reconstruct the 3D scene represented by q 1−q 2 =p 1−p 2andg=W 1d(f GT,p 1−q 1).Pleasecheck
F∗ and F∗, via minimizing the loss L of 2D photomet- Figure3(a)forasimplevisualrepresentationofEquation5,
σ c rec
ric reconstruction with the gradient-based optimization al- wheref 1andf 2areconnectedtogbythereconstructionloss
gorithm,inwhich L 1d (i.ebluearrows),whosegradientsareusedtoupdateg
andthetranslationvalues{q ,q }.
L 1 1
Lrec(Fσ,Fc)=(cid:88) (cid:88) ∥V(Fσ,Fc,W 3d(Pi,s(⃗0,d⃗ u)))−Iiu∥, Connection between 1D Signal Alignment and 3D Joint
i=1u∈U Optimization. The formulation of 1D signal alignment ef-
(3) fectivelysimulatesthe“localphenomenon”ofjointcamera
whereUisthesetofallpossible2Dcoordinatesinthein- posealignmentand3Dscenereconstructionona2Dcross-
putimages,I ∈R3istheRGBcolorofpixellocationuon section: As shown in Figure 3(c), where we consider two
iusignals,thatis
L1d(g,q1,q2)=L1d(g∗,q1,q2)
(cid:90) (6)
=L1d(u)= ∥fGT(x)−fGT(x+u)∥2dx,
where u = (p −p )−(q −q ) is the shift between two
1 2 1 2
groundtruthsignals,whichhasaninitialvalueofp −p
1 2
Weaimforutoreach0withgradient-basedoptimization.
Next,byanalyzingtherelationshipbetweenf andthe
GT
optimizationgradient d L intermsoftheirspectralprop-
du 1d
erties, we get the following result (cf. our supplement for
detailedderivationofthetheorem):
Theorem2
d (cid:90)
L1d= ∥F[fGT]∥2·H(u,k)dk, (7)
du
whereH(u,k) = 4πksin(2πku),F[f ]isFouriertrans-
GT
Figure3:SpectrumanalysisandeffectofGaussianfilter- formoff ,andkisthewavenumberinfrequencydomain.
GT
ingon1Dsignalalignment.(a)1Dsignalalignmentcom- Particularly, we are interested in the sign of d L which
parison:noisysignalscangettrappedinlocaloptimawith- determines the direction of our iterative optimdu iza1 tid on. We
out Gaussian filtering. (b)(Top) Visualization of H(u,k) in plot the value of H(u,k) in Fig. 3(b)(Top), where we can
Eq. 7, which shows alternating signs as k departs from observethatthesignofH iswell-behavedwhenthemagni-
0, causing misdirection in gradient-based optimization if tude of k is small (here well-behaving means the direction
there has too much high-frequency energy in the signal. ofthegradientisabletoletudescendto0,i.e.,beingpos-
(b)(Bottom)VisualizationofH˜(u,k)inEq.8,whichisthe
itive when u > 0 and negative when u < 0). However,
modulatedversionofH(u,k)withthehelpofGaussianfil- whenkincreases,thesignofH quicklybeginstoalternate,
ter N. (c) 1D alignment relates to 3D joint optimization in andthemagnitudeincreases,whichcausesthegradienttobe
Eq. 4, where effective pose refinement stems from the 1D largeandnoisy.Hencehigh-frequencysignalswithaspread-
alignmentinspecificcross-sections,withtheredlinesin3D ingspectrumcaneasilyleadtheoptimizationprocesstoget
scenecorrelatingtohorizontalshifts(bluearrows)androta- stuckinthelocaloptima.
tions(greenarrows). Tothisend,wedemonstratethatapplyingaGaussianfilter
on the signal f effectively mitigates the sign-alternating
GT
issueoftheoriginalH function.Specifically,weshowthat
filteringtheinputsignalisequivalenttomodulatingH bya
neighboring camera poses as well as a cross-section in the Gaussianwindow(cf.oursupplementforderivation):
3Dspacepassingthroughbothcameraplanesandintersect- Theorem3 Let L˜ denotes L calculated with Gaussian
ingwitheachcameraplaneonaprojectedstraightline,the 1d 1d
convoluted signal N ∗f , and F[N] denotes the Fourier
RGBcolorvaluesonsuchprojectedlinescorrespondtothe GT
transformoftheGaussiankernelN,thenwehave
1Dshiftedgroundtruthsignalsf ,f inEquation5,andthe
1 2 d (cid:90)
valueoftheradiancefieldonthecross-sectioncorresponds L˜ 1d= ∥F[fGT]∥2·H˜(u,k)dk, (8)
du
to reconstructed signal g in Equation 5. Similar to the loss
L inEquation5,theprojectedlinesonthecameraplanes
whereH˜(u,k)=∥F[N]∥2·H(u,k).
1d
andthecorrespondingcross-sectioninthe3Dradiancefield In Fig. 3(b)(Bottom), we plot the modulated H˜(u,k), with
are connected by the volume rendering function V and re- observingthatthemisbehaveregionissuppressed(notethat
constructionlossL joint inEquation4.Asaresult,thecom- we set the variance of N to 4 here). The gradient descent
plete 3D joint optimization can be intuitively viewed as si- will likely converge to u = 0 once the initial magnitude
multaneously performing many 1D signal analyses on the of u is less than 6.0. The region where d L˜ does well-
superpositionofall possiblecombinationsofcamera poses du 1d
behave is quasi-convex and is guaranteed to converge to
andcross-sections.
global optima given suitable learning rate that prevents us
Spectrum Analysis and Effect of Gaussian Filtering on fromgettingstuckonsaddlepoints.Ouranalysisagreeswith
1DSignalAlignment.Firstwetransformtheprobleminto themotivationbehindthecoarse-to-finetrainingscheduleof
asimplerformwithaassumptionthatisreflectedbythefast (Linetal.2021)and(Heoetal.2023).Specifically,observ-
convergent property of voxel grids (cf. our supplement for ingthatthewell-behavedregioninH(u,k)growswideras
detailedderivationofthetheorem): u approaches 0 (cf. Fig. 3(b)(Top)), which means that we
canloosenthefilteringstrengthofGaussiankernelasuap-
proaches0,leadingtolargerandmoreaccurategradient.
Theorem1 If we assume rapid convergence of signal g
(whichmeansgachieveslocaloptimag∗w.r.tcurrentq ,q 3.3 2DPlanarImageAlignment
1 2
wheneverweupdateq ,q .),wefindthattheprobleminEq.5 In addition to the 3D joint optimization problem, previous
1 2
is equivalent to pure alignment between two ground-truth works (Lin et al. 2021; Chng et al. 2022) also consider a2D image patches alignment task as a simpler example of isgeneratedbyouterproductoftwo1Dkernels.
joint optimization, in which there are L overlapping im-
2d
age patches I = {I ,··· ,I } cropped from a single 3.4 DecomposedLow-RankTensor
2d 1 L2d
groundtruthimageI beforebeingtransformedby2Dho-
gt
Thissectiondescribesthedecomposedlow-ranktensorpro-
mography. The homography transforms are parameterized
posedbyTensoRF(Chenetal.2022)whichisthescenerep-
byP = {P ,··· ,P } ∈ sl(3)andinitializedas⃗0(here
2d 1 L2d resentation that our proposed method is built upon. While
wealsofollowfrom(Linetal.2021)theusageLiealgrebra
there are two different types of tensor decomposition con-
toparameterize2Dhomographytransform).Analogouslyto
sidered in (Chen et al. 2022): CP-decomposition and VM-
Equation4,ourobjectiveistojointlyoptimizethe2Dimage
decomposition, in our discussion we mainly focus on VM-
content F : R2 → R2 and per-patch homography warps
2d decomposition,althoughourmethodisalsonaturallyappli-
P bythereconstructionloss.Jointoptimizationcanbefor-
2d cabletoCP-decomposition.
mulatedas:
To represent the 3D density field F , we store the infor-
σ
L 2d(F 2d,P 2d)=(cid:88)L (cid:88) ∥F 2d(W 2d(Pi,u))−Iiu∥2, (9) dm ea fit nio en di sn ima p3 lyD at sen cs oo mr pT oσ ne∈ nt-R wI i× seJ× inK te, ri pn olw ath ioic nh on fo Tw .F σ is
i=1u∈U2d σ
w imh ae gre epU
a2 tcd
hi es s,th Ie se ist to hf ea cll olp oo rs osi fb ple ix2 elD atco loo cr ad ti in oa nte us oin nt ih ne
-
Tσ
=(cid:88)R
v σX ,r⊗MY σ,, rZ+v σY ,r⊗MX σ,, rZ+v σZ ,r⊗MX σ,, rY, (12)
iu
put image patch I , warp function W (P , ) : R2 → R2 r=1
i 2d i whereRisthenumberofcomponentsinthedecomposition,
performs2Dhomographytransformationparameterized by (vX,vY,vZ) ∈ (RI,RJ,RK) are 1D vector-components
P ∈sl(3)Liealgebra,andW (P ,u)maps2Dcoordinate r r r
i 2d i for axes (X,Y,Z) repectively, (MY,Z,MX,Z,MX,Y) ∈
uonI gt intoatransformed2DcoordinateonpatchI i.No- (RJ×K,RI×K,RI×J) are 2D matrir x-compr onents r for axes
tice the strong structural correspondence among Eq. 5 (1D
(Y-X,X-Z,X-Y)repectively,operator⊗denotestheouter
alignment),Eq.9(2Dalignment),andEq.4(3Dalignment),
productbetweenvectorandmatrix.
thethreeproblemssharesimilarcomputationalproperty.
To represent the 3D color field F , the information
c
WeparameterizeF 2d bya2Ddecomposedlow-rankten- T (x) ∈ RG queried from 3D feature tensor T ∈
sorT 2d ∈ Rh×w,wherew,harethedimensionsoftheim- Rc I×J×K×G is decoded by a small MLP S into RGB cc olor
age. Motivated by our analysis in Section 3.2, we filter T
2d value (G is the input feature dimension of S). The imple-
with2DGaussiankerneltoavoidoverfitting.
mentationcanbeformulatedas
F 2d(x)=(N 2d∗ 2dT 2d)(x)=(N 2d∗
2d((cid:88)R
v rX⊗v rY))(x), (10)
Fc(x,d⃗)=S(Tc(x),d⃗)
where x ∈ R2 is 2D pixel coordinr= at1 es, N
2d
is 2D gaus- Tc= r(cid:88)R =1vcX ,r⊗MY c,, rZ⊗bX r + (13)
sian kernel, ∗
2d
is the convolution operator, and ⊗ denotes vcY ,r⊗MX c,r,Z⊗bY
r
+vcZ ,r⊗MX c,r,Y ⊗bZ r.
outer product between the 1D vector components vX ∈
Rw,v rY ∈Rh.“(x)”attheendoftheexpressionsmeanr sbi- T tec n( sx o) rvd oen luo mte es Tthe onc 3o Dmp co on oe rdn it n-w ati ese x.l d⃗in ie sa tr h- ein vte ierp wo il na gti do in reo c-f
linearlyinterpolatingtheprecedingdiscrete2Dvolumewith c
tion of the current ray. v and M have the same shape
continuouscoordinatex.Ourmethodoutperformsthena¨ıve c,r c,r
astheirv andM counterparts,bX,bY,bX ∈RG are
tensormethodandpreviousmethods(Linetal.2021;Chng σ,r σ,r r r r
featurecomponentstoexpandthefeatureaxisofT .
etal.2022),experimentresultsareshownatSec.4.1. c
The width of Gaussian kernel N is controlled by an
2d 3.5 SeparableComponent-WiseConvolution
exponential coarse-to-fine training schedule that changes
continuously (cf. our supplement for details of such kernel AstheoreticallyanalyzedinSec.3.2andempiricallyshown
schedule).Inordertosupportcontinuouschangingwidthon in Fig. 2(a), na¨ıvely applying low-rank decomposed tensor
a discrete Gaussian kernel, the kernel is generated by the (whichlacksinternalbiasthatlimitsthespectrumoflearned
followingrule: signal, hence corresponds to the top raw of Fig. 3) to joint
  L (cid:77)N/2 min(1,√ 21 πσe− 2x σ2 2) ifσ>0.0001 c tia om ne qr ua ap lio tyse ao np dti im nai cz ca uti ro an tere ps ou sl et ss .i Tn hs eu rb eo fop rt eim ,wal er pe rc oo pn os str eu tc o-
N (σ)= x=−LN/2 limit the spectrum of the radiance field F σ and F c with a
1d  L (cid:77)N/2
δ[x] otherwise,
coa Ifrs we- eto n- afi ¨ın ve elt yra cin oi nn vg ols vc ehe td hu ele 3.
D Gaussian kernel with our
x=−LN/2 3D volume T , (as in the 2D planar case of Eq. 10), we
σ
N (σ)=N (σ)⊗N (σ),
2d 1d 1d would have to reconstruct the whole 3D tensor before ap-
(11)
plying convolution, destroying the space compression ad-
where L is the size of the discrete kernel, 1D kernel
N vantageofdecomposedlow-ranktensor,seeEq.14.
N 1d(σ)∈RLN isdiscretelysampledfromcontinuousGaus-
sian distribution and clamped to a max value of 1.0 before Fσ(x,y,z)=(N 3d∗ 3dTσ)(x,y,z), (14)
beingconcatenatedintoavectorby⊕operator.Toavoidnu- where∗ denotes3Dconvolution,N isthe3DGaussian
3d 3d
mericalinstability,whenσ <0.001,weassignN (σ)tobe filterdefinedbyN ⊗N .Underthissetting,thetimecom-
1d 1d 2d
discreteimplusefunctionδ.2DkernelN 2d(σ) ∈ RLN×LN plexityandthespacecomplexityareO(I ·J ·K·L3 N)andO(I·J·K)respectively,whereL isthesizeof3DGaus- erationsandremains0afterward(fordetailedsettingsofσ,
N
siankernelineachdimension. pleaserefertothesupplement).
To achieve computationally efficient convolution on the
3D decomposed low-rank tensor volume, we perform our
Smoothed 2D Supervision. Inspired by the analysis in
proposedseparablecomponent-wiseconvolution,bytaking
Sec. 3.2, we discovered that blurring the 2D training im-
advantageofthefollowingidentity(whosecorrectnesswill
age with a parallel set of scheduled 2D Gaussian kernels
beproveninthesupplementarymaterial).
alsohelpsthejointoptimization.Ontheonehand,smoothed
Theorem4 supervision images produce smoothed image gradients and
R
T˜ σ = (cid:88) v˜ σX ,r⊗M˜Y σ,, rZ+v˜ σY ,r⊗M˜X σ,, rZ+v˜ σZ ,r⊗M˜X σ,, rY, (15) s trt aa ib ni il niz get ih me ac ga em ae lr sa oal hi eg ln pm se tn ot. rO esn trt ih ce to tt hh eer sh pa en ctd r, us mmo oo fth te hd
e
r=1
learned3Dscene.TheGaussianscheduleforsmoothing2D
where T˜ = (N ∗ T ) denotes the 3D Gaussian con-
σ 3d 3d σ trainingimagesissimilartothatofthe3Dradiancefields.
voluted tensor volume, v˜ = (N ∗ v˜ ) denotes the
σ,r 1d 1d σ,r
1D Gaussian convoluted vector component, and M˜ =
σ,r
(N ∗ M˜ ) denotes the 2D Gaussian convoluted ma- Randomly Scaled Kernel Parameter and Edge Guided
2d 2d σ,r
Loss. From the previous spectral analysis in Sec. 3.2,
trix component. In other words, the 3D convoluted ten-
one may have the impression that a larger kernel leads to
sor can be expressed as the composition of individually
stronger modulation, and hence always results in more ro-
convoluted components, which allows us to distribute the
bustposeregistration.However,thisisnotalwaystrue,be-
3DGaussianconvolutionacrosstheindividualcomponents
causethemagnitudeofH(u,k)decreaseslinearlyask ap-
of the decomposed low-rank tensor. Similar to Sec. 3.4,
proaches0.NoticethatinFig.3(b)themagnitudeofmodu-
the value of the density field is component-wised linearly
sampled from the Gaussian convoluted components, i.e., latedH˜ isweakerthanthatofH,whichmeansthat dd uL˜ 1dis
F˜ σ(x) = T˜ σ(x).Similarly,thespectralrestrictedversionof weaker than dd uL 1d and therefore is more easily influenced
thecolorfieldF canbeobtainedas bynoise.Inthe3Dcase,thisweakandnoisygradientprob-
c
lemcausedbyoverlyaggressivefilteringcorrespondstothe
F˜ c(x,d⃗)=S(T˜ c(x),d⃗)
excessive blur effect that destroys important edge signals
T˜
c=(cid:88)R
v˜ cX ,r⊗M˜Y c,, rZ⊗bX
r
+ (16)
i Fn igt .h 4e (btr )a fin oi rn ag vi im sua ag le izs a, tc ioa nus oin fg thp eo imse aa gl eig bn lum re ren dt t bo yf aa nil o. vS ee re
-
r=1
strengthkernel,inwhichthethinedgeinformationiselimi-
v˜Y ⊗M˜X,Z⊗bY +v˜Z ⊗M˜X,Y ⊗bZ.
c,r c,r r c,r c,r r nated,causingthecameraposetorandomlydrift.
Withseparablecomponent-wiseconvolution,thetimecom-
Based on the effect of weak and noisy gradient prob-
plexityrequiredisO(I·J·L +J·K·L +K·I·L )for
N N N lem, when applying only coarse-to-fine 3D schedule and
computing convoluted components (assuming that we sep-
smoothed2Dsupervision,wefoundthatitisinsufficientto
arate 2D Gaussian convolution on matrix components into
useasingle-sizekernelondifferentreal-worldscenestruc-
1DGaussianconvolutions),andO(R)foreachquerysam-
tures(inwhichthesamekernelmaybeoverlyaggressivein
ple(sameastheoriginaldecomposedtensorin(Chenetal.
onescene,butoverlygentleinanotherscene).Therefore,we
2022)),drasticallyreducingthecomputationrequiredforfil-
introduce randomly scaled kernel, which randomly scales
tering3DradiancefieldsF andF .
σ c the kernel by a factor uniformly sampled from [0,1]. Ran-
We stress here that our proposed component-wise con-
domscalesaresampledindependentlyamong3DGaussian
volution is different from traditional technique of sepa-
kernels(fortheradiancefield)and2DGaussiankernels(for
rated kernel convolution in signal processing literature, in
training images), allowing combinations of different-sized
thesensethatthecommonseparatedkerneltechniqueonly
kernels to guide the joint optimization. See Fig. 4(c) for a
separatesthe3Dkernelwithoututilizingtheseparabilityof
visualization of the same input image filtered by a range
the input signal itself, and hence requires sequentially per-
of randomly sampled kernels. We observe that the training
formingthree1Dconvolutionoperationon3Dvolume,the
schedule becomes more robust when we alternate between
timecomplexityoftraditionaltechniquewouldbeO(I ·J ·
theserandomlysampledkernelscales.
K ·L ), and also requires a 3-dimensional memory with
N
spacecomplexityof(I·J ·K)tostoreconvolutionresult. Another way to mitigate the weak and noisy gradient
problem is the edge guided loss , in which we increase the
3.6 TechniquesforIncreasingPoseRobustness learningrateby1.5x(andhenceamplifythegradientsignal)
on the pixels in the edge region, from which the learning
Here we summarize our improvements on na¨ıve decom-
signal for pose alignment mainly comes. See visualization
posedlow-ranktensorsthatimprovejointcameraposeopti-
in Fig. 4 (d), where we color the edge area that is detected
mizationandradiancefieldreconstruction.
usingtheSobelfilter(Kanopoulos,Vasanthavada,andBaker
Coarse-to-Fine 3D schedule. Using efficient 3D convo- 1988)onthefiltered2Dimagesinyellow.Edge-guidedren-
lution algorithm in Sec. 3.5. During training, we apply a deringlosshelpsthejointoptimizationfocusesmoreonthe
coarse-to-fine schedule on the 3D radiance field F˜ ,F˜ by edge area of the training images, resulting in more robust
σ c
controllingthekernelparameter(σ ofEq.11)oftheGaus- pose optimization. Empirically we apply this edge-guided
sian kernel, which is exponentially reduced to 0 at 10k it- scalealternatelyoneveryothertrainingiteration.(a) No Kernel (b) Overly Aggresive Kernel (c) Randomly Scaled Kernel (d) Edge Region
Figure 4: Visualization of 2D Randomly Sampled Kernel and Edge Guided Loss. (a) Input supervision without kernel.
Joint optimization using unblurred images easily overfit to high-frequency noises (b) Input supervision blurred by an overly
aggressive kernel. Notice that the edge information is largely destroyed by the blurring process, resulting in weak and noisy
gradients, causing the poses to drift around easily. (c) Same input supervision blurred by four randomly scaled kernels. We
empiricallyfoundthatmixingdifferentfilteringstrengthsresultsinamorerobustjointoptimization.(d)Weselectedgeareaof
ablurredimagebySobelfilterwithathresholdsetto1.25xoftheaveragevalueofthefilterededge-strengthmap.
GT Tensor + 2D Gaussian Naive Tensor BaRF
Methods sl(3)error↓ patchPSNR↑
BARF 0.0105 35.19
Na¨ıve2DTensoRF 0.5912 20.80
2DTensoRF+2DGaussian 0.0023 40.70
Table1:Quantitativeresultsofplanarimagealignment.
4 Experiments
Figure5:Qualitativecomparisonsofthe2Dimagepatch
Although our method is applicable to various decomposed
alignment.2DTensoRF+2DGaussiansuccessfullyregis-
low-ranktensorimplementations,inthissection,wevalidate
ters accurate warping parameters, verifying the analysis of
ourproposedmethodusingTensoRF(Chenetal.2022)with
Gaussianfilteringonjointoptimization.
inaccurateorunknowncameraposes.
Weevaluateourproposedmethodagainstthreeprevious
works BARF (Lin et al. 2021), GARF (Chng et al. 2022),
and HASH (Heo et al. 2023). Since the implementation of
GARF and HASH are unavailable, we directly use the re- on par with previous methods and produces the best aver-
sults reported in their paper for comparison. We compare ageviewsynthesisquality.Ourmethodalsoscoresthebest
these methods on the planar image alignment task and LPIPSin7outof8scenes,indicatingthatourmethodpro-
novel view synthesis task on NeRF-Synthetic and LLFF ducesperceptuallymorenaturalnovel-viewsynthesis.
dataset.Weprovidedetailedimplementationdetailsandex- Note that we achieve state-of-the-art results within only
perimentalsetupinthesupplementarymaterial. 20%to25%oftrainingiterations,whileallothercompeting
methodstrainfor200kiterations.
4.1 Results
4.2 Ablation
PlanarImageAlignment(2D). InFig.5wecompareour
method (i.e., 2D TensoRF + 2D Gaussian) with na¨ıve 2D Component Analysis. In Tab. 4, we report the effect of
TensoRFimplementation(Chenetal.2022)andBARF(Lin each proposed component on the pose error and PSNR of
et al. 2021). Quantitative results are reported in Tab. 1, in- the optimization results. The results are average across all
cluding sl(3) warp error and patch PSNR. These results real-world scenes in the LLFF dataset. In (a) (b), we show
demonstrate the effectiveness of Gaussian filtering in joint the effect of randomly scaled kernel described in Sec. 3.6.
optimization,verifyingtheanalysisinSec.3.2. In (b)(c), we show the effectiveness of edge guided loss
(Sec. 3.6). Finally, in (c)(d)(e), we show the necessity of
NeRF (3D): Synthetic Object & Real World Objects.
Gaussianfilteringonboth2Dsupervisingimagesand3Dra-
Tab.2reportstheposeerrorandnovel-viewsynthesisqual-
diancefieldrepresentedbyadecomposedtensorgrid,which
ityoftheNeRF-Syntheticdataset.Ourmethodachievesthe
validatestheanalysisinSec.3.2.
smallestposeerrorin5outof8scenesandachievesthebest
reconstruction quality in all eight scenes, and the quantita- PotentialBaselineofTensoRFwithBARF/GARF. One
tiveresultsareshowninFig.6. may suspect that we can solve the joint optimization prob-
Tab. 3 reports the pose error and novel-view synthesis lemofdecomposedlow-ranktensorbysimplyapplyingthe
qualityoftheLLFFdataset.Ourmethodachievesposeerror method of (Lin et al. 2021) or (Chng et al. 2022), we clar-
hctaP
egamI
spraW
omoHCameraPoseRegistration ViewSynthesisQuality
Scene Rotation(◦)↓ Translation↓ PSNR↑ SSIM↑
GARF BARF HASH Ours GARF BARF HASH Ours GARF BARF HASH Ours GARF BARF HASH Ours
Chair 0.113 0.096 0.085 0.874 0.549 0.428 0.365 3.501 31.32 31.16 31.95 35.22 0.959 0.954 0.962 0.984
Drum 0.052 0.043 0.041 0.037 0.232 0.225 0.214 0.118 24.15 23.91 24.16 25.78 0.909 0.900 0.912 0.934
Ficus 0.081 0.085 0.079 0.050 0.461 0.474 0.479 0.173 26.29 26.26 28.31 31.37 0.935 0.934 0.943 0.978
Hotdog 0.235 0.248 0.229 0.105 1.123 1.308 1.123 0.499 34.69 34.54 35.41 37.18 0.972 0.970 0.981 0.982
Lego 0.101 0.082 0.071 0.049 0.299 0.291 0.272 0.100 29.29 28.33 31.65 34.23 0.925 0.927 0.973 0.981
Materials 0.842 0.844 0.852 0.854 2.688 2.692 2.743 2.690 27.91 27.84 27.14 29.04 0.941 0.936 0.911 0.951
Mic 0.070 0.071 0.068 1.177 0.293 0.301 0.287 5.000 31.39 31.18 32.33 32.50 0.971 0.969 0.975 0.976
Ship 0.073 0.075 0.079 0.058 0.310 0.326 0.287 0.167 27.64 27.50 27.92 31.98 0.862 0.849 0.879 0.903
Mean 0.195 0.193 0.189 0.400 0.744 0.756 0.722 1.533 28.96 28.84 29.86 32.07 0.935 0.930 0.943 0.961
Table 2: Quantitative results on the NeRF-Synthetic dataset. Our method achieves the best average novel-view synthesis
qualityandthebestposeerrorin5outof8scenes.Noticethatourmethodconvergeswithin40kiterations,whileallprevious
methodstrainfor200kiterations.
CameraPoseRegistration ViewSynthesisQuality
Scene Rotation(◦)↓ Translation↓ PSNR↑ SSIM↑
GARF BARF HASH Ours GARF BARF HASH Ours GARF BARF HASH Ours GARF BARF HASH Ours
Fern 0.470 0.191 0.110 0.472 0.250 0.102 0.102 0.199 24.51 23.79 24.62 26.17 0.740 0.710 0.743 0.842
Flower 0.460 0.251 0.301 1.375 0.220 0.224 0.211 0.389 26.40 23.37 25.19 25.62 0.790 0.698 0.744 0.810
Fortress 0.030 0.479 0.211 0.449 0.270 0.364 0.241 0.419 29.09 29.08 30.14 29.68 0.820 0.823 0.901 0.882
Horns 0.030 0.304 0.049 0.386 0.210 0.222 0.209 0.251 22.54 22.78 22.97 22.84 0.690 0.727 0.736 0.819
Leaves 0.130 1.272 0.840 1.990 0.230 0.249 0.228 0.397 19.72 18.78 19.45 21.24 0.610 0.537 0.607 0.753
Orchids 0.430 0.627 0.399 0.279 0.410 0.404 0.386 0.340 19.37 19.45 20.02 20.57 0.570 0.574 0.610 0.698
Room 0.270 0.320 0.271 0.188 0.200 0.270 0.213 0.191 31.90 31.95 32.73 31.87 0.940 0.949 0.968 0.936
T-Rex 0.420 1.138 0.894 0.523 0.360 0.720 0.474 0.416 22.86 22.55 23.19 24.19 0.800 0.767 0.866 0.878
Mean 0.280 0.573 0.384 0.709 0.269 0.331 0.258 0.325 24.55 23.97 24.79 25.27 0.745 0.723 0.772 0.827
Table3:QuantitativeresultsontheLLFFdataset.Ourmethodachievesthebestaveragenovel-viewsynthesisqualityand
bestLPIPSin7outof8scenes.Ourmethodconvergeswithin50kiterations,whileallpreviousmethodstrainfor200kiterations.
3D 2D Random Edge Rot. Trans.PSNR
Gauss. Gauss. Kernel Guided ↓ ↓ ↑
(a) ✓ ✓ ✓ ✓ 0.72 0.33 25.36
(b) ✓ ✓ ✓ 1.00 0.37 25.25
(c) ✓ ✓ 1.91 0.93 25.12
(d) ✓ 33.00 12.7 20.10
(e) ✓ 26.25 8.9 19.73
(d) 23.29 9.4 23.97
Table 4: Ablation study of the components of the pro-
posedmethodonthereal-worldLLFFdataset.
Rot.↓ Trans.↓ PSNR↑ SSIM↑ LPIPS↓
Figure6:Visualcomparisonsofnovelviewsynthesis. TensoRF+BARF 45.47 0.17 20.71 0.630 0.314
TensoRF+GARF 73.92 0.29 10.47 0.287 0.679
Ours 0.43 0.003 26.92 0.872 0.104
ifythatthereexistsnosimplewayofintegratingBARF(i.e.,
gradually activating higher-frequency components in posi- Table5:AblationonDirectlyApplyingBARFandGARF
tional encoding) into TensoRF since the MLP decoder of onTensoRF(PotentialBaseline)
TensoRFdoesnottakespatialcoordinatesasinput(i.e.,con-
trolling spatial property in TensoRF is hard to achieve by
manipulating positional encoding). Nevertheless, we makeFilter Rot.↓ Trans.↓ PSNR↑ SSIM↑ LPIPS↓ 32
Ours
Boxfilter 9.98 0.06 20.18 0.387 0.165
Gaussianfilter 0.46 0.004 29.49 0.874 0.063
31
Table6:AblationOnLow-PassFilters.
30
HASH
Setting Rot.↓ Trans.↓ PSNR↑ x4 less
w/randomkernels&edgeguidedloss 0.06 0.002 34.34 29 GARF
w/orandomkernels&edgeguidedloss 0.28 0.010 34.43 BARF
28
Table 7: Ablation on Applying Randomly Scaled Kernel
0k 50k 100k 150k 200k
ParameterandEdgeGuidedLossinSyntheticScenes
Training Itera�ons
σ 0.125 0.15 0.175 0.2 Figure7:PSNRandtrainingiterationscomparison.
Rotation↓ 0.094 0.068 0.100 0.108
BARF
Translation↓ 0.004 0.004 0.005 0.005
The figure shows two advantages of our method: (1) rapid
Rotation↓ 0.07 0.062 0.072 0.066
Ours convergenceand(2)high-qualitynovelviewsynthesis.
Translation↓ 0.003 0.003 0.003 0.002
The early-stage blurry supervision can hinder detailed
structure reconstruction later in the optimization, impact-
Table 8: Ablation: Sensitivity Analysis On Gaussian
ing the final result quality. Our method resolves this prob-
NoiseinBlenderChair.
lembyapplying3Dfilterswithdirectlycontrollablekernel
parameters, which enables smooth and rapid transition (by
continuous exponential kernel schedule) of the 3D content
rough attempts to add a positional encoding schedule into
acrossthespectrumdomains,asopposedtopreviousmeth-
the MLP decoder input to simulate the setting of BARF
ods that use indirect methods (e.g., learning rate in (Heo
or replace the decoder with a GARF network. We conduct
et al. 2023), encoding magnitude in (Lin et al. 2021)) to
experiments on four randomly chosen scenes in the LLFF
influence learned 3D scene spectral property. Furthermore,
dataset. The results are shown in Tab. 5, which demon-
ourmethodiscarefullydesignedtouseasinglevoxelgrid,
strate the efficacy and pertinency of our proposed method
which is trained only once in the coarse-to-fine schedule
toachievesuccessfultraining.
controlledbyourproposedefficientcomponent-wisedcon-
UsingOtherLow-PassFilters. Aswewouldliketohave volution algorithm, thus leading to faster convergence; in
identical filtering strength along all spatial directions, we comparison,(Heoetal.2023),whichalsousesvoxel-based
adopttheGaussianfilterinourmethodasitistheonlyker- representation,requiressequentialcurriculumlearningupon
nelthatisbothcircularlysymmetricandseparable(awell- multiplevoxelgridsofdifferentresolutions,resultinginfour
knownpropertyinsignalprocessing).Nevertheless,weex- timesmoretrainingiterationsthanours.
periment with other low-pass filters. We report in Tab. 6
the performance of using the box filter (i.e., a representa- 5 Conclusion
tive low-pass filter) on the LLFF Fortress scene, in which
Our contributions is three fold: 1) Theoretically, we pro-
weclearlyobservethebenefitsofusingtheGaussianfilter.
vide insights into the impact of 3D scene properties on the
Applying Randomly Scaled Kernel Parameter. and Edge convergenceofjointoptimizationbeyondthecoarse-to-fine
GuidedLossonSyntheticScenes. Althoughthetwotech- heuristic discussed in prior research (e.g., BARF, Heo et
niquesareoriginallyproposedtoimprovetherobustnessof al.2023),thusofferingafilteringstrategyforimprovingthe
complex real-world scenes, they do not harm the perfor- jointoptimizationofcameraposeand3Dradiancefield.2)
mance of synthetic ones and even slightly boost the pose Algorithmically, we introduce (and prove the equivalence
estimation,asshowninTab.7. of) an effective method for applying the pilot study’s fil-
tering strategy on the decomposed low-rank tensor, notice
Sensitivityw.r.t.PoseInitialization. WeadopttheChair thattheproposedseparablecomponent-wiseconvolutionis
scenesintheBlenderdatasettoconductsensitivityanalysis moreefficientthanthetraditionallywell-knowntrickofsep-
uponposeinitializationviavaryingvarianceσ ofGaussian arableconvolutionkernelasweadditionallyutilizethesep-
noise.TheresultisshowninTab.8,whichdemonstratesthat arability of the input signal. Furthermore, we also propose
both BARF and our proposed method show certain robust- othertechniquessuchasrandomly-scaledkernelparameter,
nessagainstthenoisyinitializationofcameraposes. blurred2Dsupervision,andedge-guidedlossmask tohelp
ourproposedmethodbetterperformincomplexreal-world
4.3 TimeComplexity
scenes.3)Comprehensiveevaluationsdemonstrateourpro-
In Fig. 7, we compare with previous methods on average posed framework’s state-of-the-art performance and rapid
PSNRandtrainingiterationsintheSyntheticNeRFdataset. convergencewithoutknownposes.
)↑(
RNSPAcknowledgments Hu, T.; Xu, X.; Chu, R.; and Jia, J. 2023. TriVol: Point
ThisworkissupportedbyNationalScienceandTechnology CloudRenderingviaTripleVolumes. InProceedingsofthe
Council (NSTC) 111-2628-E-A49-018-MY4, 112-2221- IEEEConferenceonComputerVisionandPatternRecogni-
E-A49-087-MY3, 112-2222-E-A49-004-MY2, and Higher tion(CVPR).
EducationSproutProjectoftheNationalYangMingChiao Kajiya,J.T.;andVonHerzen,B.P.1984. Raytracingvol-
Tung University, as well as the Ministry of Education umedensities. ACMSIGGRAPHcomputergraphics.
(MoE),Taiwan.Inparticular,Yu-LunLiuacknowledgesthe
Kanopoulos, N.; Vasanthavada, N.; and Baker, R. L. 1988.
YushanYoungFellowProgrambytheMoEinTaiwan.
DesignofanimageedgedetectionfilterusingtheSobelop-
erator. IEEEJournalofsolid-statecircuits.
References
Kulhanek,J.;andSattler,T.2023. Tetra-NeRF:Represent-
Chen,A.;Xu,Z.;Geiger,A.;Yu,J.;andSu,H.2022. Ten-
ingNeuralRadianceFieldsUsingTetrahedra.arXivpreprint
sorf:Tensorialradiancefields. InProceedingsoftheEuro-
arXiv:2304.09987.
peanConferenceonComputerVision(ECCV).
Lin, C.-H.; Ma, W.-C.; Torralba, A.; and Lucey, S. 2021.
Chen,Y.;Chen,X.;Wang,X.;Zhang,Q.;Guo,Y.;Shan,Y.;
BARF: Bundle-Adjusting Neural Radiance Fields. In Pro-
andWang,F.2023a.Local-to-globalregistrationforbundle-
ceedingsoftheIEEEInternationalConferenceonComputer
adjustingneuralradiancefields. InProceedingsoftheIEEE
Vision(ICCV).
Conference on Computer Vision and Pattern Recognition
(CVPR). Liu, L.; Gu, J.; Lin, K. Z.; Chua, T.-S.; and Theobalt, C.
2020. NeuralSparseVoxelFields. AdvancesinNeuralIn-
Chen,Z.;Funkhouser,T.;Hedman,P.;andTagliasacchi,A.
formationProcessingSystems(NeurIPS).
2023b. MobileNeRF:ExploitingthePolygonRasterization
PipelineforEfficientNeuralFieldRenderingonMobileAr- Liu,Y.-L.;Gao,C.;Meuleman,A.;Tseng,H.-Y.;Saraf,A.;
chitectures.InProceedingsoftheIEEEConferenceonCom- Kim, C.; Chuang, Y.-Y.; Kopf, J.; and Huang, J.-B. 2023.
puterVisionandPatternRecognition(CVPR). Robust Dynamic Radiance Fields. In Proceedings of the
Chng, S.-F.; Ramasinghe, S.; Sherrah, J.; and Lucey, S. IEEE Conference on Computer Vision and Pattern Recog-
2022. Gaussianactivatedneuralradiancefieldsforhighfi- nition(CVPR).
delityreconstructionandposeestimation. InProceedingsof Meuleman, A.; Liu, Y.-L.; Gao, C.; Huang, J.-B.; Kim, C.;
theEuropeanConferenceonComputerVision(ECCV). Kim, M. H.; and Kopf, J. 2023. Progressively Optimized
Fridovich-Keil, S.; Meanti, G.; Warburg, F. R.; Recht, B.; Local Radiance Fields for Robust View Synthesis. In Pro-
andKanazawa,A.2023.K-Planes:ExplicitRadianceFields ceedings of the IEEE Conference on Computer Vision and
in Space, Time, and Appearance. In Proceedings of the PatternRecognition(CVPR).
IEEE Conference on Computer Vision and Pattern Recog- Mildenhall, B.; Srinivasan, P. P.; Tancik, M.; Barron, J. T.;
nition(CVPR). Ramamoorthi, R.; and Ng, R. 2020. NeRF: Representing
Fridovich-Keil, S.; Meanti, G.; Warburg, F. R.; Recht, B.; Scenes as Neural Radiance Fields for View Synthesis. In
andKanazawa,A.2023.K-Planes:ExplicitRadianceFields Proceedings of the European Conference on Computer Vi-
in Space, Time, and Appearance. In Proceedings of the sion(ECCV).
IEEE Conference on Computer Vision and Pattern Recog-
Mu¨ller, T.; Evans, A.; Schied, C.; and Keller, A. 2022.
nition(CVPR).
Instant Neural Graphics Primitives with a Multiresolution
Fridovich-Keil,S.;Yu,A.;Tancik,M.;Chen,Q.;Recht,B.; HashEncoding. ACMTransactionsonGraphics(TOG).
andKanazawa,A.2022.Plenoxels:RadianceFieldswithout
Scho¨nberger,J.L.;andFrahm,J.-M.2016. Structure-from-
Neural Networks. In Proceedings of the IEEE Conference
Motion Revisited. In Proceedings of the IEEE Conference
onComputerVisionandPatternRecognition(CVPR).
onComputerVisionandPatternRecognition(CVPR).
Goel, R.; Dhawal, S.; Saini, S.; and Narayanan, P. J. 2022.
Shao, R.; Zheng, Z.; Tu, H.; Liu, B.; Zhang, H.; and Liu,
StyleTRF:StylizingTensorialRadianceFields. InProceed-
Y.2023. Tensor4D:EfficientNeural4DDecompositionfor
ings of the Thirteenth Indian Conference on Computer Vi-
High-Fidelity Dynamic Reconstruction and Rendering. In
sion,GraphicsandImageProcessing.
Proceedings of the IEEE Conference on Computer Vision
Han,K.;andXiang,W.2023. MultiscaleTensorDecompo-
andPatternRecognition(CVPR).
sitionandRenderingEquationEncodingforViewSynthesis.
InProceedingsoftheIEEEConferenceonComputerVision Sun, C.; Sun, M.; and Chen, H. 2022. Direct Voxel Grid
andPatternRecognition(CVPR). Optimization: Super-fast Convergence for Radiance Fields
Reconstruction. InProceedingsoftheIEEEConferenceon
Hedman,P.;Srinivasan,P.P.;Mildenhall,B.;Barron,J.T.;
ComputerVisionandPatternRecognition(CVPR).
and Debevec, P. 2021. Baking Neural Radiance Fields for
Real-TimeViewSynthesis. ProceedingsoftheIEEEInter- Tang, J.; Chen, X.; Wang, J.; and Zeng, G. 2022.
nationalConferenceonComputerVision(ICCV). Compressible-Composable NeRF via Rank-residual De-
composition. Advances in Neural Information Processing
Heo,H.;Kim,T.;Lee,J.;Lee,J.;Kim,S.;Kim,H.J.;and
Systems.
Kim,J.-H.2023.RobustCameraPoseRefinementforMulti-
Resolution Hash Encoding. In Proceedings of the Interna- Wang, L.; Zhang, J.; Liu, X.; Zhao, F.; Zhang, Y.; Zhang,
tionalConferenceonMachineLearning(ICML). Y.; Wu, M.; Yu, J.; and Xu, L. 2022. Fourier plenoctreesfor dynamic radiance field rendering in real-time. In Pro-
ceedings of the IEEE Conference on Computer Vision and
PatternRecognition(CVPR).
Wang,Z.;Wu,S.;Xie,W.;Chen,M.;andPrisacariu,V.A.
2021. NeRF−−: Neural Radiance Fields Without Known
CameraParameters. arXivpreprintarXiv:2102.07064.
Xu,Q.;Xu,Z.;Philip,J.;Bi,S.;Shu,Z.;Sunkavalli,K.;and
Neumann,U.2022. Point-nerf:Point-basedneuralradiance
fields. InProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition(CVPR).
Xu, Y.; Wang, L.; Zhao, X.; Zhang, H.; and Liu, Y. 2023.
AvatarMAV: Fast 3D Head Avatar Reconstruction Using
Motion-Aware Neural Voxels. In ACM SIGGRAPH 2023
ConferenceProceedings.
Yu,A.;Li,R.;Tancik,M.;Li,H.;Ng,R.;andKanazawa,A.
2021. PlenOctreesforReal-timeRenderingofNeuralRadi-
anceFields. ProceedingsoftheIEEEInternationalConfer-
enceonComputerVision(ICCV).
Yu¨ce,G.;Ortiz-Jime´nez,G.;Besbinar,B.;andFrossard,P.
2022. Astructureddictionaryperspectiveonimplicitneural
representations. InProceedingsoftheIEEEConferenceon
ComputerVisionandPatternRecognition(CVPR).
Zhang, K.; Riegler, G.; Snavely, N.; and Koltun, V. 2020.
Nerf++: Analyzing and improving neural radiance fields.
arXivpreprintarXiv:2010.07492.