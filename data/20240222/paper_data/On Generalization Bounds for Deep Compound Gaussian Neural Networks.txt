On Generalization Bounds for Deep Compound
Gaussian Neural Networks
Carter Lyons, Raghu G. Raj, and Margaret Cheney
Abstract—Algorithm unfolding or unrolling is the technique I. INTRODUCTION
of constructing a deep neural network (DNN) from an iterative
algorithm. Unrolled DNNs often provide better interpretability MACHINE learning success in image classification has
and superior empirical performance over standard DNNs in recently spurred its application, in particular with deep
signalestimationtasks.Animportanttheoreticalquestion,which neuralnetworks(DNN),insignalestimationtasks.Signalesti-
has only recently received attention, is the development of
mationisoneexampleofaninverseproblemwherewedesirea
generalization error bounds for unrolled DNNs. These bounds
reconstructed signal from some undersampled measurements.
deliver theoretical and practical insights into the performance
of a DNN on empirical datasets that are distinct from, but Particular applications of interest include X-Ray computed
sampled from, the probability density generating the DNN tomography (CT), magnetic resonance imaging (MRI), and
training data. In this paper, we develop novel generalization compressive sensing.
error bounds for a class of unrolled DNNs that are informed
While DNNs have been shown to outperform iterative
by a compound Gaussian prior. These compound Gaussian
approaches in estimated signal quality and computational
networks have been shown to outperform comparative standard
and unfolded deep neural networks in compressive sensing and time [1]–[6], this typically requires a significant amount of
tomographic imaging problems. The generalization error bound training data and training time. Certain problems of interest,
is formulated by bounding the Rademacher complexity of the such as MRI, do not have large training datasets readily
class of compound Gaussian network estimates with Dudley’s
availableandthusDNNperformancecanfalterontheseprob-
integral. Under realistic conditions, we show that, at worst, the
(cid:112) lems. Additionally, standard DNNs, e.g. convolutional neural
generalization error scales O(n ln(n)) in the signal dimension
and O((Network Size)3/2) in network size. networks,donotincorporateanyoutsidepriorinformationinto
the estimation model as is included in iterative approaches.
Index Terms—Deep neural networks, generalization error,
Algorithm unfolding or unrolling is a technique, introduced
inverse problems, non-convex optimization, algorithm unrolling
by Gregor and LeCun [1], which combines the iterative ap-
proaches and deep-learning methods by structuring the layers
NOTATIONANDNOMENCLATURE
of a DNN such that they correspond to an iteration from
R Set of real numbers.
the iterative algorithm. While a standard DNN acts as a
v =[v ]∈Rd. Boldface characters are vectors.
i black-box process, we have an understanding of the inner
(·)T Transpose of vector or matrix (·).
workingsofanunrolledDNNfromunderstandingtheoriginal
⊙ Hadamard product.
iterative algorithm. Furthermore, algorithm unrolling allows
f(v) =[f(v )] for componentwise function f :R→R.
i for the incorporation of prior information into the deep-
N[d] ={1,2,...,d} is set of first d natural numbers.
learning framework. Example iterative algorithms that have
P a,b(x) =a+ReLU(x−a)−ReLU(x−b), for a,b∈R, is
been unrolled into DNNs include: iterative shrinkage and
a modified ReLU (mReLU) activation function.
thresholding algorithm (ISTA) [1, 3, 4, 7]–[10] and proximal
ρ (v) = v/max{1,b−1∥v∥ } for b > 0, is the projection
b 2 gradient descent [11, 12]. These unrolled DNNs have shown
onto the Euclidean ball of radius b.
excellent performance in image estimation while offering
A ∈ Rm×n is a measurement, observation, or sensing
simple interpretability of the network layers [2]. However,
matrix.
these unrolled DNNs still require large training datasets.
A =ADiag(z)∈Rm×n for any vector z ∈Rn.
z Recently, two compound Gaussian (CG) informed DNNs
T (z) ≡T (z;P ):=(ATA +P−1)−1ATy.
y y u z z u z were developed through the use of algorithm unrolling. These
H(i) HypothesisclassforG-CG-Net,CG-Net,andDR-CG-
CG unrolled CG-based DNNs have empirically produced superior
Net when i=1, i=2, and i=3, respectively.
estimated signals over comparative iterative and DNN meth-
CarterLyonsiswiththeU.S.NavalResearchLaboratory,Washington,DC ods,especiallyinscenariosoflow trainingdata[5,6,13,14].
20375USAandalsowithColoradoStateUniversity,FortCollins,CO80523 While this success was shown empirically, no generalization
USA(e-mail:carter.lyons@colostate.edu)
guarantees have been provided, a gap that this paper fills.
RaghuG.RajiswiththeU.S.NavalResearchLaboratory,Washington,DC
20375USA(e-mail:raghu.g.raj@ieee.org) Specifically, we:
MargaretCheneyiswithColoradoStateUniversity,FortCollins,CO80523
1) Establish and prove a Lipschitz property of the out-
USA(e-mail:margaret.cheney@colostate.edu)
This work was sponsored in part by the Office of Naval Research via puts from unrolled, CG-informed DNNs with respect to
the NRL base program and under award numbers N00014-21-1-2145 and (w.r.t.) the DNN parameters.
N0001423WX01875. Furthermore, this work was sponsored in part by the
2) Develop an encompassing generalization error bound
Air Force Office of Scientific Research under award number FA9550-21-1-
0169. (GEB) for unrolled, CG-informed DNNs by bounding
ThisworkhasbeensubmittedtotheIEEEforpossiblepublication.Copyrightmaybetransferredwithoutnotice,afterwhichthisversionmaynolongerbe
accessible.
4202
beF
02
]LM.tats[
1v60131.2042:viXra2
Rademacher complexities with covering numbers. Mean-squared error or mean-absolute error are common loss
3) ApplythedevelopedGEBandprovideasymptoticforms functions [19]. The actual loss of a hypothesis c∈H is
(cid:98)
in terms of DNN size and signal dimension for two dis-
L(c)=E [L(c(y;Θ),c)].
tinct formulations of unrolled, CG-based DNNs named (cid:98) (y,c)∼D (cid:98)
compoundGaussiannetwork(CG-Net)[5]anddeepreg- Finally,thegeneralizationerror(GE S)ofahypothesisc (cid:98)∈H
ularizedcompoundGaussiannetwork(DR-CG-Net)[6]. is the difference in the empirical and actual loss. That is,
We theoretically demonstrate that DR-CG-Net exhibits
GE (c)=|L(c)−L (c)|.
S (cid:98) (cid:98) S (cid:98)
a tighter GEB than CG-Net, which was empirically
observed in [5, 6]. ADNNlearnsitsparametersΘbyminimizingtheempirical
loss, i.e. by choosing a hypothesis c ∈ H that minimizes
The GEB we develop is formulated using techniques in- (cid:98)
L (c). It is critical in applications, however, for a DNN to
formedbythosediscussedin[15],whichproducesaGEBfor S (cid:98)
similarly generate excellent results when provided any new
an ISTA unrolled DNN with learned sparsity transformation.
datasampledrawnfromD thatisnotcontainedinthetraining
Such techniques are similarly used in [16] for an unrolled
dataset. This is tantamount to minimizing the generalization
DNNemployingalearnedanalysissparsitytransformationand
error, and thus, obtaining an estimate or bound on GE for a
in [17] for a ℓ -ℓ unrolled recurrent neural network. S
1 1 DNN is of significant interest.
A. Generalization Error
II. GENERALIZEDCOMPOUNDGAUSSIANNETWORK
The generalized compound Gaussian network (G-CG-Net)
A DNN can be viewed as a collection of ordered layers,
shown in Fig. 1 is an unrolled, CG-based DNN for solving
denoted L ,L ,...,L for K > 1, where layers feed into
0 1 K linear inverse problems. Note, the use of “generalized” in the
one another from the input layer, L , to the output layer, L .
0 K naming of G-CG-Net denotes the fact that this network en-
IntermediatelayersL ,...,L areknownashiddenlayers.
1 K−1 compasses the compound Gaussian network (CG-Net) [5, 13]
EachlayerL containsd hiddenunits[18]thatareassigneda
k k and deep regularized compound Gaussian network (DR-CG-
computed value when transmitting a signal through the DNN.
Net) [6, 14] as special cases. In contrast, the use of “gener-
A function, f
k
: Rdi1(k) × ··· × Rdij(k) → Rdk, that is
alized” in terms of network error denotes a DNNs ability to
parameterized by some θ defines the computation, i.e. signal
k transfer from training data to testing data.
transmission, at layer L where I := {i (k),...,i (k)} ⊆
k k 1 j Throughthestudyofimagestatistics,ithasbeenshownthat
{0,1,...,K−1} are the indices of layers that feed into L .
k sparsity coefficients of natural images exhibit self-similarity,
That is, given an input signal, y ∈ Rd0, assigned to L 0, a
heavy-tailed marginal distributions, and self-reinforcement
DNN is the composition of parameterized vector input and
among local coefficients [20]. These properties are encom-
vector output functions where
passed by the class of CG densities [20]–[22]. Thus, the CG
(cid:0) (cid:1) prior better captures statistical properties of natural images as
L ≡f L ,...,L ;θ .
k k i1(k) ij(k) k
wellasimagesfromothermodalitiessuchasradar[23,24].A
Let C and Y be the set of possible signals of interest, usefulformulationof theCGpriorismodeling asignalasthe
e.g. images, and possible signal measurements, e.g. image Hadamard product c = z⊙u such that u ∼ N(0,Σ u), z ∼
measurements,respectively.Further,let(C,A c,D c)beaprob- p z, and u and z are independent random variables [20, 25].
ability space where A and D are a σ-algebra and unknown We call z the scale variable and u the Gaussian variable. The
c c
probability density on C, respectively. Similarly, define the linear measurement model we consider is
probability space (Y,A ,D ). Now, consider the probability
y y y =Ac+ν ≡A(z⊙u)+ν. (1)
space (Y × C,A ⊗ A ,D) where D is an unknown joint
y c
G-CG-Net is a method that recovers c, by estimating z and
probability density with marginal distributions D and D .
c y
u, when given y and A.
Let S = {(y i,c i)} i∈N[Ns] be a training dataset where each
pair (y ,c ) is drawn i.i.d. from D. We denote the network
i i A. Iterative Algorithm
parameters as Θ = (θ ,...,θ ), and let Ω be the space
1 K k
of θ that can be learned by the DNN. Further, we write the Algorithm1providespseudocodefortheiterativealgorithm
k
networkoutput,i.e.signalatL ,thatisdependentonnetwork generalized compound Gaussian least squares (G-CG-LS) to
K
parameters and input measurements y, as c(y;Θ). be unrolled into G-CG-Net. Consider the cost function
(cid:98)
The hypothesis space, H, of a DNN is given by 1 1
F(u,z)= ∥y−A(z⊙u)∥2+ uTP−1u+R(z). (2)
2 2 2 u
H:={c(·;Θ):Θ∈Ω ×···×Ω }.
(cid:98) 1 K Amaximum-a-posteriori(MAP)estimateofz andufrom(1)
is a special case of (2) when P ∝Σ and R∝log(p (z)).
For a loss function, L(x ,x ), that measures a discrepancy u u z
1 2 Usingblockcoordinatedescent[26],G-CG-LSalternatively
between x
1
∈ RdK and x
2
∈ RdK, the empirical loss of a
minimizes (2) over z and u. The minimum of (2) in u is a
hypothesis c∈H, over a training dataset S, is given as
(cid:98) Tikhonov solution, T (z)≡T (z;P ), given by
y y u
1 (cid:88)Ns T (z):=(ATA +P−1)−1ATy
L (c)= L(c(y ;Θ),c ). y z z u z
S (cid:98) N s
i=1
(cid:98) i i =P uAT z(I+A zP uAT z)−1y3
Algorithm 1 Generalized Compound Gaussian Least Squares layer, mapping from Rn →Rn, is implemented in each g(j),
k
Input: Measurement y. Initial estimate z 1(0). then every θ k(j ,1) could be the weight matrix from the layer
1: Choose a u 0 (e.g. u 0 =T y(z 1(0))) while θ k(j ,2) could be the additive bias of the layer. Hence, the
2: for k ∈{1,2,...,K} do G-CG-Net parameters are
3: z ESTIMATION:
Θ={P
}∪{θ(j)}j∈N[J]
(3)
4: for j ∈{1,2,...,J} do u k,d k∈N[K],d∈N[D].
5: z k(j) =g(z k(j−1),u k−1)≡g(z k(j−1),u k−1;y) WeremarkthatastructuredformcanbeimposedonP u.In
6: end for particular,toensurethatP issymmetricandpositivedefinite
7: z k(0 +) 1 =z k(J) (SPD), we consider, for ϵu >0 a small fixed real number,
8: u ESTIMATION:
19 0:
:
endu k fo=
r
T y(z k(J))  m diaa gx ({ [λ m, aϵ x} {I
λ ,ϵ}]n )
S Dc ia al ge od naId lentity
Output: c∗ =z K(J)⊙u K P u = LL
t
LriL TT
t
+ri+ ϵIϵI i i=1 T Furi ld l.iagonal
where the second equality results from using the Woodbury In the scaled identity case, only a constant λ is learned. In
matrix identity. the diagonal case, a vector λ = [λ ]n is learned. In the
i i=1
As the minimum of (2) in z does not have a closed tridiagonal case, two vectors λ ∈ Rn and λ ∈ Rn−1 are
1 2
form solution, for general regularization R(z), we iteratively learned such that the lower triangular matrix component L
tri
minimize by performing J descent steps on z. That is, for is formed by placing λ on the diagonal and λ on the first
1 2
g : Rn × Rn → Rn a scale-variable-descent update, the subdiagonal. Finally, in the case of a full covariance matrix,
estimateofz ondescentstepj ofiterationk,denotedasz(j), an entire lower triangular matrix L is learned.
k
is given by z(j) = g(z(j−1),u ;y) where z(0) = z(J).
k k k−1 k+1 k
For instance, [5, 13, 14] take g as a steepest descent step and
C. Realizations
[6] uses a ISTA step.
Two specific forms of G-CG-Net are detailed here.
B. Unrolled Deep Neural Network (G-CG-Net) 1) Compound Gaussian Network (CG-Net) [13]: For this
method, the scale variable is formulated as z = h(χ) for
Applying algorithm unrolling to Algorithm 1, we create
χ∼N(0,I)andhisacomponentwise,non-linear,twicecon-
G-CG-Net with end-to-end structure shown in Fig. 1. Each
tinuously differentiable, and invertible function. Accordingly,
layer k of G-CG-Net, shown in the dashed box of Fig. 1b,
the scale variable regularization is R(z) = µ∥h−1(z)∥2, for
corresponds to iteration k of Algorithm 1 and implements a 2
a scalar constant µ>0, to enforce normality of χ=h−1(z).
complete scale variable mapping, Z , shown in Fig. 1c, that
k
Thescale-variable-descentupdate, g,isa projectedsteepest
updates z and a Tikhonov update of u. Each Z consists
k
of J scale variable updates Z(1),...,Z(J) where every Z(j) descent step based on a learned quadratic norm. We consider
k k k a slightly adjusted update, to assist in the analysis, given as
updates z once and is the output of a scale-variable-descent
update, denoted g k(j), as given in line 5 of Algorithm 1. g k(j)(z,u)=P a,b(z−B k(j)ρ ξ(∇ zF(u,z;µ( kj)))) (4)
Mathematically detailing the G-CG-Net blocks we have:
L 0=y is the input measurements to the network where a,b,ξ > 0 are fixed real-valued scalars, B k(j) is a
Z =P (AˆTy), for Aˆ= A , is an initial estimate of z. learned n×n positive definite matrix, and
0 0,b ∥A∥2
U k=T y(Z k(J)) is the Tikhonov estimate of u ∇ zF(u,z;µ( kj))=AT u(A uz−y)+µ( kj)[h−1]′(z)⊙h−1(z)
corresponding to line 1 and 9 of Algorithm 1.
for µ(j) a learned scalar. Note, the application of ρ ensures
The kth complete scale variable mapping, Z , contains: k ξ
k
a sufficiently small step size is used in each gradient update
Z(j)=P (g(j)(Z(j−1),U )) is the scale variable
k 0,z∞ k k k−1 for numerical stability.
update corresponding to line 5 in Algorithm 1.
For parameters, P is structured as a scaled identity matrix
u
O =ρ cmax(Z K ⊙U K) is the estimated signal output. and D = 2 where θ k(j ,1) = B k(j) and θ k(j ,2) = µ( kj). A structural
similarity index measure (SSIM) loss function is used to train
Note that ρ and P , for c ≥ 0 and z ≥ 0,
cmax 0,z∞ max ∞
CG-Net. For two images or matrices I and I of equivalent
are applied for technical reasons discussed in Section III-A. 1 2
Furthermore, to simplify notation, we let Z(0) = Z(J) for size, the SSIM loss function is given by L(I 1,I 2) = 1 −
k+1 k SSIM(I ,I ).
k ∈{1,2,...,K−1} and Z(0) =Z . 1 2
1 0 2) Deep Regularized Compound Gaussian Network (DR-
Every U layer is parameterized by the same covariance
k CG-Net)[14]: Inthismethod,thescalevariableregularization
matrix P and each Z(j) we assume, generally, to be parame-
u k is left as an implicit function that is learned, through its
terized by D weights θ k(j ,1),...,θ k(j ,D) . We additionally assume gradient, in the unrolled deep neural network. In DR-CG-Net,
that θ(j) ∈ Ω for k ∈ N[K], j ∈ N[J], and some finite- aprojectedgradientdescent(PGD)orISTAstepareemployed
k,d d
dimensionalvectorspaceΩ .Forinstance,ifafully-connected as scale-variable-descent updates. Due to similar analyses of
d4
(a) End-to-end network structure of G-CG-Net.
(b) Layer k analogous to iteration k in Algorithm 1. (c) Complete scale variable mapping, Z , producing estimate z(J) in Algorithm 1.
k k
Fig. 1: End-to-end network structure for G-CG-Net, the unrolled deep neural network of Algorithm 1, is shown in (1a). G-
CG-Net consists of an input block, L , initialization block, Z , K+1 Tikhonov blocks, U , output block, O, and K complete
0 0 k
scale variable mappings, Z , with structure in (1c). Each Z consists of J scale variable updates Z(j).
k k k
both scale-variable-descent updates, we focus on the PGD for G-CG-Net outputs w.r.t. G-CG-Net parameters. Our key
scale update method given by contributionhereisshowingtheG-CG-Netoutputsareindeed
Lipschitzw.r.t.tothenetworkparametersandapplyingourde-
g(j)(z,u)=v(j)(z,u;δ(j))+V(j)(z) (5)
k k k k rived GEB to the specific CG-Net and DR-CG-Net structures.
where, for a step size δ(j) >0 and fixed real number ξ >0, The remainder of this section is structured as follows. Sec-
k
tion III-A explicates boundedness assumptions that underlie
v(j)(z,u;δ(j))=z−δ(j)ρ (AT(A z−y)) (6) our GEBs for deep compound Gaussian networks. We detail
k k k ξ u u
a GEB for G-CG-Net in section III-B, which is subsequently
is a gradient update of z over the data fidelity term of (2) and
refined for the CG-Net and DR-CG-Net structures in sections
V(j) :Rn →Rnanembeddedsubnetwork.Weremarkthatthe
k III-C and III-D, respectively.
update in (5) is a gradient descent step on (2) w.r.t. z when
V(j) = ∇R. Hence, in training V(j), the regularization, or
k k A. Boundedness Assumptions
equivalentlypriordistribution,forthescalevariableislearned.
Finally, ρ is applied for numerical stability as in CG-Net. A common assumption in machine learning literature and
ξ
Each subnetwork, V(j), consists of L convolutional layers implementation is that the input data to a DNN is bounded.
k c
using ReLU activation functions. That is, layer ℓ consists of Specificboundsareoftenguaranteedthroughpreprocessingof
f convolutions, i.e. filter channels, using kernel size k ×k the data, which has been shown to assist in the performance
ℓ ℓ ℓ
with unit stride. Note, zero padding is applied to each filter of DNN models [27]. For instance, in [13, 14], which use
channeloftheinputsuchthattheoutput,atanyfilterchannel, imagesasthesignalsofinterest,eachimageisscaleddownto
isthesamesizeastheinputtoanyfilterchannel.Furthermore, be bounded in the Euclidean unit ball. Furthermore, bounded
we take f = 1 so that given a single channel image input data implies that the possible parameters to be learned by a
Lc
toV(j) theoutputisalsoasinglechannelimageofequivalent DNN are similarly bounded as DNNs are trained only for a
k
finitenumberofepochsusingasmalllearningratetoestimate
dimension. For our analysis, we assume, without loss of
generality, that convolutional layer ℓ of V(j) is implemented bounded signals from bounded inputs.
k
as a matrix-vector product with weight matrix W(j). Assumption 1. The following bounds hold almost surely:
k,ℓ
For parameters, P u is structured as a tridiagonal matrix 1) Original signals, c∈Rn, satisfy ∥c∥ 2 ≤c max.
and D = L c + 1 where θ k(j ,ℓ) = W k( ,j ℓ) for 1 ≤ ℓ ≤ L c 2) Scale variables, z ∈Rn, satisfy ∥z∥ ∞ ≤z ∞.
and θ k(j ,L) c+1 = δ k(j). A mean-absolute error loss function, 3) Covariance matrices, P u, satisfy P u ∈ P where, for
L(x 1,x 2)= n1∥x 1−x 2∥ 1, is used to train DR-CG-Net. bounding scalars 0<p min ≤p max,
P ={n×n SPD matrices with bounded spectrum}
III. GENERALIZATIONERRORBOUNDS
={P ∈Rn×n : SPD, ∥P∥ ≤p ,∥P−1∥ ≤p−1 }.
In order to estimate the generalization error we, similarly 2 max 2 min
to [15]–[17], derive an upper bound on L(c (cid:98)) in terms of 4) Each scale variable update parameter, θ(j), satisfies
k,d
L (c) and a Dudley’s inequality bound of the Rademacher
S (cid:98) ∥θ(j)∥ ≤ω forscalarω ≥0andsomenorm∥·∥ .
complexity for the hypothesis space generated by G-CG- k,d (d) d d (d)
Net. The Dudley’s inequality bound is evaluated using a Note that we assume the scale variables are bounded since
covering number argument dependent on a Lipschitz property the original signals are bounded. This is enforced by the5
mReLU activation function, P , in each scale variable for dim(P) = 1,n,2n−1, or n(n+1)/2 when P = P ,
0,z∞ const
updatelayerZ k(j).Finally,astheoriginalsignalsarebounded, P =P diag, P =P tri, or P =P full, respectively. Additionally,
we force the G-CG-Net estimates to be equivalently bounded
κ=z (c +p y ∥A∥ )c(K,J)+z c (8)
by applying the projection operator, ρ , to Z ⊙U . ∞ 1 max max ∞ (cid:98)1 ∞ 2
cmax K K
κ(j) =z (c +p y ∥A∥ )c(K,J) (9)
k,d ∞ 1 max max ∞ (cid:98)k,j,d,2
B. G-CG-Net
where
To derive a GEB for G-CG-Net, we require the an assump-
tion on the loss function and scale-variable-descent update. c =p y ∥A∥ (cid:0) 1+2z2 p ∥A∥2(cid:1)
1 max max 2 ∞ max 2
Assumption 2. For a vector space V, the loss function c =z y ∥A∥ (p /p )2
2 ∞ max 2 max min
L(x 1,x 2):V →R satisfies: K K
c(K,J) =c (cid:88) r(J) (cid:89) (r(J)+r(J)c )
1. Bounded: |L(x ,x )|≤c (cid:98)1 2 (cid:98)k,2 (cid:98)ℓ,1 (cid:98)ℓ,2 1
1 2
k=1 ℓ=k+1
2. τ-Lipschitz: ∥L(x ,x)−L(x ,x)∥ ≤τ∥x −x ∥
1 2 2 1 2 2 K
for c≥0, τ ≥0 and all x 1,x 2, x∈V. (cid:98)c( kK ,j, ,J d,)
2
=r (cid:98)k(j ,, dJ ,3) (cid:89) (r (cid:98)ℓ( ,J 1)+r (cid:98)ℓ( ,J 2)c 1)
ℓ=k+1
Assumption 3. For every z ∈ Rn satisfying ∥z ∥ ≤ z
i i ∞ ∞
and u
i
= T y(z i;P i) where P
i
∈ P, each g k(j)(z,u),
for r(J) =
(cid:89)J
r(j−1), r(J)
=(cid:88)J r(j−1)J (cid:89)−1
r(ℓ), and
parameterized by ϑ(j) =(θ(j),...,θ(j) ), satisfies (cid:98)k,1 k,1 (cid:98)k,2 k,2 k,1
k k,1 k,D j=1 j=1 ℓ=j
J−1
∥g k(j)(z 1,u 1;ϑ( kj))−g k(j)(z 2,u 2;ϑ(cid:101)( kj))∥ 2 r (cid:98)k(j ,, dJ ,3) =r k(j ,d− ,31) (cid:89) r k(ℓ ,) 1.
≤r(j−1)∥z −z ∥ +r(j−1)∥u −u ∥ ℓ=j
k,1 1 2 2 k,2 1 2 2
D A proof of Theorem 1 is provided in Appendix VII, which
+(cid:88) r k(j ,d− ,31)∥θ k(j ,d) −θ(cid:101) k(j ,d)∥
(d)
broadlyemploysDudley’sinequalitytoboundtheRademacher
d=1 complexity of H(1). Ideally, through training G-CG-Net, a
CG
for non-negative scalar constants r(j−1),r(j−1), r(j−1), and hypothesis c (cid:98)∈H C(1 G) is chosen such that L S(c (cid:98)) is minimized,
k,1 k,2 k,d,3
norms {∥·∥ }D . but any c (cid:98) possibly generated by G-CG-Net could be used
(d) d=1 in the Theorem 1 bound. For instance, if early stopping is
While the Lipschitz property of Assumption 3 may seem implemented in training then Theorem 1 still applies to the
arbitrarily restrictive, we show it holds for CG-Net and DR- generated c despite c not optimizing L .
(cid:98) (cid:98) S
CG-Net in Appendix VIII and Appendix IX, respectively. Lastly, we remark for noiseless measurements, that is
Next, define P const ⊂ P diag ⊂ P tri ⊂ P full to be the options y i = Ac i, that y max ≤ c max∥A∥ 2 for any set of training
forP correspondingtothevectorspacesofconstant,diagonal, data. For white noise measurements, that is y = Ac +ν
i i
tridiagonal, and full covariance matrices with bounded spec- where ν ∼ N(0,σ2I), then y ≤ c ∥A∥ +z σ with
max max 2 p
trum, respectively. Additionally, let each scale variable update high probability for large z . For instance, using the quantile
p
parameter, θ k(j ,d), be of dimension α d ≥ 0 (i.e. θ k(j ,d) ∈ Rαd) function of a normal distribution, y max ≤c max∥A∥ 2+6.11σ
and define the sets with probability (1−2×10−9)mNs ≈1−2mN s×10−9.
Ω
d
={θ ∈Rαd :∥θ∥
(d)
≤ω d}. (7)
C. CG-Net
Then the hypothesis class for G-CG-Net is
(cid:26) (cid:18) j∈N[J](cid:19) (cid:27) ForCG-Net,thescalevariableupdateparameterspaces,Ω
d
H C(1 G) = c
(cid:98)
·;{P u,θ k(j ,d)}k∈N[K] :P
u
∈P,θ k(j ,d) ∈Ω
d
. from(7),areΩ
1
=P
full
andΩ
2
=[−µ,µ]forconstantµ>0.
d∈N[D]
Thus, the hypothesis class for CG-Net is
Theorem 1 (Generalization Error Bound for G-CG-Net). Let
S
is
= giv{ e( ny i b, yc i) (} 1N i )=s 1 anb de a det fir na eini yng dat =aset mw ah xere each ∥y(c ∥i, .y i I)
f
H C(2 G) =(cid:26) c (cid:98)(cid:18) ·;(cid:110) P u,B k(j),µ( kj)(cid:111)j k∈ ∈N N[ [J K] ](cid:19) :P u ∈P const,
max 1≤i≤Ns i 2 (cid:111)
Assumption 1, 2, and 3 hold then with probability at least B(j) ∈Ω ,µ(j) ∈Ω .
1−ε, for all c∈H(1), the generalization error of G-CG-Net k 1 k 2
(cid:98) CG
is bounded as Theorem 2 (Generalization Error Bound for CG-Net). Let
S = {(y ,c )}Ns be a training dataset where each (c ,y )
L(c)≤L (c)+ i i i=1 i i
(cid:98) S (cid:98) is given by (1) and define y = max ∥y ∥ . If
8 √τc Nm sax (cid:32) (cid:112) dim(P)(cid:115) ln(cid:18) e(cid:18) 1+ 4p max(K c mJ aD
x
+1)κ(cid:19)(cid:19) A als lsu c (cid:98)m ∈pt Hio C(n 2 G)1 , th ho eld gs enth ee ran liw zait th ionm pra eox rb ra ob ril oit fy Ca Gt1 -≤ l Nei a e≤ s tN t is s1 b− oi uε n2 , df eo dr
as in Theorem 1 for τ a Lipschitz constant of the SSIM loss
(cid:118) 
+
(cid:88)K (cid:88)J (cid:88)D (cid:117) (cid:117)
(cid:116)α
dln(cid:32) e(cid:32)
1+
4ω d(KJD+1)κ( kj ,d)(cid:33)(cid:33)

function, dim(P)=1, c (cid:40)=2, D =2,
k=1j=1d=1
c max
(α ,ω )=
(n(n 2+1),p max) d=1
d d
(cid:112) (1,µ) d=2,
+4c 2ln(4/ε)/N
s6
1−rJ
r(J) =rJ, r(J) =r 1 , and Theorem 4 (Generalization Error Bound for DR-CG-Net).
(cid:98)k,1 1 (cid:98)k,2 21−r 1 Let S = {(y i,c i)}N i=s
1
be a training dataset where each
(cid:40) (c ,y ) is given by (1) and define y =max ∥y ∥ .
r (cid:98)k(j ,, dJ ,3) =r 1J−j ξ
p h
d d= =1
2
(10) Ifi Assi umption 1 holds then with prm oa bx ability at1≤ lei≤ aN sts 1 −i 2 ε,
max max for all c ∈ H(3), the generalization error of DR-CG-Net is
(cid:98) CG √
where bounded as in Theorem 1 with τ =1/ n, dim(P)=2n−1,
√
r =1+p ((z p y ∥A∥ ∥A∥ )2+µτ ) c=c max/ n, D =L c+1,
1 max ∞ max max 2 ∞ h
r 2 =p maxy max∥A∥ 2(cid:0) 1+z ∞2 p max∥A∥ 2(∥A∥ 2+∥A∥ ∞)(cid:1) (α ,ω )=(cid:40) (f d−1f dk d2,w d) d=1,2,...,L c (12)
h max = max [h−1]′(z)h−1(z), d d (1,δ) d=L c+1,
z∈[a,b]
and τ =max [h−1]′′(z)h−1(z)+[h−1]′(z)2. 1−rJ
h z∈[a,b] r(J) =rJ, r(J) =r 1 , and
(cid:98)k,1 1 (cid:98)k,2 21−r
We remark that in Theorem 2 we only state a Lipschitz 1
constant for the SSIM loss function exists as deriving one is (cid:18)√ (cid:19)
space consuming and not illuminating. This is, in part, due r(j,J) =rJ−j nz
∞(cid:81)L
ℓ=c 1w ℓ d=1,2,...,L c (13)
to the fact that SSIM(I ,I ), from the SSIM loss function, is (cid:98)k,d,3 1 ℓ̸=d
1 2 ξ d=L +1
implemented as the average structural similarity over a set of c
patchesfromtheinputimageswhereineachpatchaGaussian where
weighting filter is used [19].
In all numerical experiments in [13, 14], h(z) = exp(z)
(cid:89)Lc
r =1+δ(z p y ∥A∥ ∥A∥ )2+ w
on [a,b] = [1,exp(3)] and ξ = 1. Thus, h = exp(−1), 1 ∞ max max 2 ∞ ℓ
max
τ = 1, and z = exp(3). Furthermore, preprocessing is ℓ=1
h ∞ r =δy ∥A∥ (cid:0) 1+z2 p ∥A∥ (∥A∥ +∥A∥ )(cid:1) .
used such that c = 1 and for ϵ > 0, a small stabilizing 2 max 2 ∞ max 2 2 ∞
max
parameter, p = 1/ϵ, p = ϵ, and |µ| ≤ 1/ϵ. The
max min
remainingconstants∥A∥ ,∥A∥ ,andy canbecalculated
2 ∞ max
given the measurement model and training dataset. Ignoringconstants,theGEBofDR-CG-Netscalesasinthe
Let a≲b imply a≤s b for some s >0 and define following Corollary.
c c
r =ln(y )+ln(∥A∥ )+ln(∥A∥ ). (11) Corollary 5. The GEB for DR-CG-Net scales as
max 2 ∞
Corollary 3. The generalization error for CG-Net scales as (cid:115)
ln(n)
(cid:115) |L(c)−L (c)|≲KJr (L ) +
(KJ)3r (cid:98) S (cid:98) f c nN
|L(c)−L (c)|≲n . s
(cid:98) S (cid:98) N (cid:115)
s √ ln(KJL )+KJ(r+ln(1+(cid:81)Lc w ))
Furthermore,as,y max ≲√ m,∥A∥ 2 ≲n√ m,and∥A∥ ∞ ≲n ( n+KJ(1+r f(L c))) c nN s ℓ=1 ℓ
then the CG-Net generalization error scales at most as
(cid:115)
(KJ)3(ln(m)+ln(n))
for r f(L c)=(cid:80)L ℓ=c 1(cid:112) f ℓ−1f ℓk ℓ2 and r given in (11).
|L(c)−L (c)|≲n . Furthermore, bounding each w ℓ, f ℓ, and k ℓ respec-
(cid:98) S (cid:98) N s tively by the maximums w max = max 1≤ℓ≤Lcw ℓ, f max =
Corollary 3 results by ignoring constants in the GEB from max 1≤ℓ≤√Lcf ℓ, and k max√= max 1≤ℓ≤Lcf ℓ and noting that
Theorem 2 to consider how this GEB scales in network size y max ≲ m, ∥A∥ 2 ≲ n m, and ∥A∥ ∞ ≲ n then the GEB
and signal dimension. From Corollary 3, once the amount of of DR-CG-Net scales at most as
trainingdatasatisfiesN s ∼n2(KJ)3(ln(m)+ln(n))thenthe (cid:115)
GEB of CG-Net will be small with high probability. |L(c)−L (c)|≲ (KJL c)3(ln(m)+ln(n)) .
(cid:98) S (cid:98) N
s
D. DR-CG-Net
Observation: The CG-Net network size is ≈KJ and DR-
For DR-CG-Net, the scale variable parameter spaces are
CG-Net network size is ≈KJL . Thus, the GEB of CG-Net
(cid:40) c (cid:113)
Ω
d
= {W ∈Rnfd×nfd−1 :∥W∥ 2 ≤w d} d∈N[L c] further simplifies to |L−L S|≲n (NetworkSize) N3( sln(m)+ln(n))
[−δ,δ] d=L +1. while the GEB of DR-CG-Net further simplifies to
c
(cid:113)
for real value constants w d,δ > 0. Note, each W ∈ Ω d, for |L−L S| ≲ (NetworkSize) N3( sln(m)+ln(n)). Hence, constraining
d∈N[L ],correspondstoaconvolutionallayermappingfrom CG-Net and DR-CG-Net to an equivalent network size, the
c
f to f filter channels using convolutional kernels of size discrepancy in the GEBs is only the signal dimension where
d−1 d (cid:112)
k ×k . Thus, the hypothesis class for DR-CG-Net is CG-Net scales as O(n ln(n)) and DR-CG-Net scales as
d d (cid:112)
(cid:26) (cid:18) j∈N[J](cid:19) O( ln(n)). Therefore, DR-CG-Net produces a tighter GEB,
H C(3 G) = c ·;{P u,δ k(j),W k( ,j ℓ)} ℓk ∈∈ NN [[ LK c]
]
:P u ∈P tri, w shh oi wch Dis Rs -u Cp Gpo -Nrte ed
t
b py rot dh ue cn eu sm tee sr tica rel ce ox np se tr ri um cte in ot ns so of f[5 s, u6 p] et rh ioa rt
(cid:111)
W(j) ∈Ω ,δ(j) ∈Ω . quality as compared to those produced by CG-Net.
k,ℓ ℓ k Lc+17
IV. LIPSCHITZPROPERTYOFG-CG-NET and similarly r k(J ,1)r (cid:98)k(j ,, dJ ,3) =r (cid:98)k(j ,, dJ ,3+1) and r k(J ,d)
,3
=r (cid:98)k(J ,d+ ,31,J+1).
Combining these two notes with (14) gives the desired
In this section, we show that G-CG-Net is Lipschitz w.r.t.
induction result.
itsparametersΘin(3).Thisresultisacornerstoneofproving
Theorem 1 and is dependent on a Lipschitz and bounded
property of the Tikhonov solution along with Assumption 3. B. Properties of the Tikhonov Solution
Recall, for square matrix, M, the spectral norm, ∥M∥ , is
2
A. Lipschitz of Complete Scale Variable Mappings thelargestabsoluteeigenvalueofthematrix.First,weprovide
For notation we write Z(J) as the complete scale variable afewlemmasnecessarytoderiveaLipschitzconditionforthe
k
mappingconsistingofJ scalevariableupdates.ThatisZ(J) = Tikhonov solution.
k
Z(J)◦···◦Z(1). Lemma 7. For any invertible symmetric matrix P and scale
k k
variable z it holds that ∥(ATA +P−1)−1∥ ≤∥P∥ .
Proposition 6. Let Assumption 3 hold. Then the complete z z 2 2
scale variable mapping Z(J)(z,u), which is parameterized Proof. Let 0<λ ≤···≤λ be the eigenvalues of ATA +
k 1 n z z
by some Θ( kJ) =(θ k(j ,1),...,θ k(j ,D) ) j∈N[J], satisfies P−1,0≤γ 1 ≤···≤γ nbetheeigenvaluesofAT zA z,and0<
κ ≤ ··· ≤ κ the eigenvalues of P. Note these eigenvalues
1 n
∥Z k(J)(z 1,u 1;Θ( kJ))−Z k(J)(z 2,u 2;Θ(cid:101)( kJ))∥
2
≤r (cid:98)k(J ,1)∥z 1−z 2∥
2
are all non-negative as AT zA z, P, and AT zA z+P−1 are real,
symmetric matrices. Then 0 < λ−1 ≤ ··· ≤ λ−1 are the
J D n 1
+r (cid:98)k(J ,2)∥u 1−u 2∥ 2+(cid:88)(cid:88) r (cid:98)k(j ,, dJ ,3)∥θ k(j ,d) −θ(cid:101) k(j ,d)∥ (d) eigenvalues of (AT zA z +P−1)−1 and 0 < κ− n1 ≤ ··· ≤ κ− 11
are the eigenvalues of P−1. From Weyl’s inequality
j=1d=1
J J J−1 γ +κ−1 ≤λ
for r(J) = (cid:89) r(j−1), r(J) =(cid:88) r(j−1) (cid:89) r(ℓ), and 1 n 1
(cid:98)k,1 k,1 (cid:98)k,2 k,2 k,1
and thus
j=1 j=1 ℓ=j
J−1 1
r (cid:98)k(j ,, dJ ,3) =r k(j ,d− ,31) (cid:89) r k(ℓ ,) 1. ∥(AT zA z+P−1)−1∥ 2 =λ− 11 ≤ γ +κ−1 ≤κ n =∥P∥ 2.
1 n
ℓ=j
Lemma 8. Let z and z satisfy ∥z ∥ ,∥z ∥ ≤z . Then
Proof. Using induction on J, the base case, J = 1, holds 1 2 1 ∞ 2 ∞ ∞
b iny duA cs tis ou nm hp yti po on th3 esiw sh he or le dw foe
r
fise xt ed(cid:81) JJ j= >J+ 11 .r Uk(j s,1) ing= A1 s. suL me pt tit oh ne ∥AT z2A z2 −AT z1A z1∥ 2 ≤2z ∞∥A∥2 2∥z 1−z 2∥ ∞.
3, the induction hypothesis, and that P is 1-Lipschitz Proof. Observe
0,z∞
∥Z k(J+1)(z 1,u 1;Θ( kJ+1))−Z k(J+1)(z 2,u 2;Θ(cid:101)( kJ+1))∥ 2 ∥AT z2A z2 −AT z1A z1∥ 2
=(cid:13) (cid:13) (cid:13)P −0 P,z 0∞ ,z( ∞g k ((J g+ k(J1 +)( 1Z )(k( ZJ k) (( Jz )(1 z, 2u ,1 u; 2Θ ;( k Θ(cid:101)J) ( k) J, )u ),1 u; 2ϑ ;( k ϑJ (cid:101)+ ( kJ1 +)) 1) )))(cid:13) (cid:13)
(cid:13)
2
= ≤≤ (∥ ∥ ∥A A zT z T z 12 2 ∥A ( ∞Az2 z +2− ∥− zA 2AT z ∥2 z ∞A 1) )z ∥1 ∥2 A+ + TA ∥ AT z (cid:0) ∥2 A 2A T z ∥z 2 z1 1−− −AA zT zT z 211 ∥(cid:1)A ∞Az z1 1∥ ∥2 2
≤r k(J ,1)∥Z k(J)(z 1,u 1;Θ( kJ))−Z k(J)(z 2,u 2;Θ(cid:101)( kJ))∥
2
≤2z ∞∥A∥2 2∥z 1−z 2∥ ∞.
D
+r k(J ,2)∥u 1−u 2∥ 2+(cid:88) r k(J ,d) ,3∥θ k(J ,d+1)−θ(cid:101) k(J ,d+1)∥
(d)
Lemma 9. For any invertible matrices P and P(cid:101)
(cid:16)
d=1
(cid:17)
∥P(cid:101)−1−P−1∥
2
≤∥P−1∥ 2∥P(cid:101)−1∥ 2∥P −P(cid:101)∥ 2.
≤r(J) r(J)∥z −z ∥ +r(J)∥u −u ∥
k,1 (cid:98)k,1 1 2 2 (cid:98)k,2 1 2 2 Proof. Observe
J D
+r k(J ,1)(cid:88)(cid:88) r (cid:98)k(j ,, dJ ,3)∥θ k(j ,d) −θ(cid:101) k(j ,d)∥ (d)+r k(J ,2)∥u 1−u 2∥
2
∥P(cid:101)−1−P−1∥
2
=∥P(cid:101)−1(cid:16) P −P(cid:101)(cid:17) P−1∥
2
j=1d=1
D
≤∥P−1∥ 2∥P(cid:101)−1∥ 2∥P −P(cid:101)∥ 2.
+(cid:88) r k(J ,d) ,3∥θ k(J ,d+1)−θ(cid:101) k(J ,d+1)∥ (d). (14)
Next, we bound the spectral norm on the difference of two
d=1 invertible portions of the Tikhonov solution. For an invertible
J J+1 matrix, M, recall that the condition number of M is
First note r(J)r(J) =r(J)(cid:89) r(j−1) = (cid:89) r(j−1) =r(J+1).
k,1 (cid:98)k,1 k,1 k,1 k,1 (cid:98)k,1 Cond(M)=∥M∥ ∥M−1∥ .
j=1 j=1 2 2
Second note
Corollary 10. For any invertible symmetric matrices P and
J J−1
r(J)r(J)+r(J) =r(J)(cid:88) r(j−1) (cid:89) r(ℓ) +r(J) P(cid:101) andanyz 1 andz 2 satisfying∥z 1∥ ∞,∥z 2∥ ∞ ≤z ∞ itholds
k,1 (cid:98)k,2 k,2 k,1 k,2 k,1 k,2
J+1
j=1
J
ℓ=j ∥(AT z1A
z1
+P−1)−1−(AT z2A
z2
+P(cid:101)−1)−1∥
2
= (cid:88) r k(j ,2−1)(cid:89) r k(ℓ ,)
1
=r (cid:98)k(J ,2+1) ≤2z ∞∥A∥2 2∥P∥ 2∥P(cid:101)∥ 2∥z 1−z 2∥ ∞
j=1 ℓ=j +Cond(P)Cond(P(cid:101))∥P −P(cid:101)∥ 2.8
Proof. Using Lemma 9 and then Lemma 7 note C. Lipschitz Property of Network Outputs
∥(AT z1A
z1
+P−1)−1−(AT z2A
z2
+P(cid:101)−1)−1∥
2
scaI ln
e
t vh ais rias be lc etio en st, imw ae tesde ofi nne iteζ
rk
atia on nd kζ(cid:101)k whas enth Ge -CG G-C -NG e- tNe ist
≤∥P∥ 2∥P(cid:101)∥ 2∥AT z2A z2 +P(cid:101)−1−AT z1A z1 −P−1∥ 2. parameterizedbyΘorΘ(cid:101),from(3),respectively.Thatis,ζ k is
recursively defined by ζ =Z(J)(ζ ,T (ζ ;P );Θ(J))
k k k−1 y k−1 u k
Next, using Lemma 8 and Lemma 9 observe
First, we show a Lipschitz property of the final scale
variable estimate in the following proposition.
∥AT z2A
z2
+P(cid:101)−1−AT z1A
z1
−P−1∥
2
Proposition 13. If Assumption 3 holds then
≤∥AT z2A
z2
−AT z1A z1∥ 2+∥P(cid:101)−1−P−1∥
2
≤2z ∞∥A∥2 2∥z 1−z 2∥ ∞+∥P−1∥ 2∥P(cid:101)−1∥ 2∥P −P(cid:101)∥ 2.
∥ζ
K
−ζ(cid:101)K∥
2
≤ (cid:98)c( 1K,J)(y)∥P u−P(cid:101)u∥
2
Now, we prove the Tikhonov solution is Lipschitz. +(cid:88)K (cid:88)J (cid:88)D
(cid:98)c( kK ,j, ,J d,) 2(y)∥θ k(j ,d) −θ(cid:101) k(j ,d)∥
(d)
Proposition 11. Let z 1 and z 2 satisfy ∥z 1∥ ∞,∥z 2∥ ∞ ≤z ∞. k=1j=1d=1
Then the Tikhonov solution T , which is parameterized by a
y
SPD matrix P, satisfies for
∥T y(z 1;P)−T y(z 2;P(cid:101))∥
2
≤c 1(y)∥z 1−z 2∥ ∞+c 2(y)∥P −P(cid:101)∥
2 c(K,J)(y)=c
(y)(cid:88)K
r(J)
(cid:89)K
(r(J)+r(J)c (y))
(cid:98)1 2 (cid:98)k,2 (cid:98)ℓ,1 (cid:98)ℓ,2 1
for k=1 ℓ=k+1
K
(cid:16) (cid:17) c(K,J)(y)=r(j,J) (cid:89) (r(J)+r(J)c (y))
c 1(y)=∥P∥ 2∥A∥ 2∥y∥
2
1+2z ∞2 ∥P(cid:101)∥ 2∥A∥2
2
(cid:98)k,j,d,2 (cid:98)k,d,3 (cid:98)ℓ,1 (cid:98)ℓ,2 1
ℓ=k+1
c 2(y)=z ∞∥A∥ 2∥y∥
2
Cond(P)Cond(P(cid:101)).
where r(J),r(J), and r(j,J) are given in Proposition 6 and
(cid:98)k,1 (cid:98)k,2 (cid:98)k,d,3
Proof. Observe c (y) and c (y) are given in Proposition 11.
1 2
Proof. Combining Proposition 6 and 11 it holds for any k
∥T y(z 1;P)−T y(z 2;P(cid:101))∥
2
== =
+ +(cid:13)
(cid:13)∥ (cid:13) (cid:13)( ((
(
(cid:104)A AA
A
(AT z
T
zT
z
T
z1
11
1
T
zA AA
A
1Az
zz
z1
11
1
z+ ++
1+
+P PP
P
P− −− −1 11
1
−) ))
)
1− −−
−
)1 11
1
−A A
A (cid:0)
1AT z T
z
T z
−1
1
2 T
zy y
y
1
(A− −−
−
T
z( 2A( (A A
A AT
zT z T
z
T z
z22
1
2
2(cid:1)A A
A y
+z
z
z2
1
2
P(cid:101)+ +
+
−1P P P(cid:101)
(cid:101)
)−− −
−
11 1
1
(cid:105)) ) )− −
−
A1 1
1
T
zA A
A
2yT z T
z
T
z2
2
2
(cid:13)
(cid:13)
(cid:13)y y
y
2∥
(cid:13)
(cid:13)
(cid:13)2
2
≤∥ =ζ k ((cid:13)
(cid:13) (cid:13)
r
(cid:98)−−
kZ
(J
,Z
1k(
)ζ(cid:101)
JJ
k(k +J)∥
(
)2
ζ
r
(cid:98)( Dkζ
(k
(cid:101)
J
,−
k
2)−1 c,
1
1T
,
(yTy
y
)( )ζ
(
∥ζk
(cid:101)
ζ−
k
k−1 −;
1
1P
;P
−u (cid:101)u);
ζ)
(cid:101)Θ
; kΘ
−(cid:101)( kJ
1( kJ
∥))
2)) +(cid:13) (cid:13) (cid:13) 2
r (cid:98)k(J ,2)c 2(y)∥P u−P(cid:101)u∥
2
≤∥(AT z1A z1 +P−1)−1∥ 2∥y∥ 2∥AT z1 −AT z2∥ 2+ +(cid:88)(cid:88) r (cid:98)k(j ,, dJ ,3)∥θ k(j ,d) −θ(cid:101) k(j ,d)∥ (d). (15)
z ∞∥A∥ 2∥y∥ 2∥(AT z1A
z1
+P−1)−1−(AT z2A
z2
+P(cid:101)−1)−1∥
2
j=1d=1
≤∥P∥ ∥A∥ ∥y∥ ∥z −z ∥ +
2 2 2 1 2 ∞ Now, we use induction on K. The base case K =1 holds by
z ∞∥A∥ 2∥y∥ 2∥(AT z1A
z1
+P−1)−1−(AT z2A
z2
+P(cid:101)−1)−1∥
2
(15) as ζ
0
= ζ(cid:101)0 ≡ Z
0
and (cid:81)K ℓ=K+1(r (cid:98)ℓ( ,J 1) +r (cid:98)ℓ( ,J 2)c 1(y)) = 1.
Assume the induction hypothesis holds for fixed K−1 where
where in the last inequality we used Lemma 7. Combining K >2. By (15) and the induction hypothesis observe
the above inequality with Corollary 10 produces the desired
result. ∥ζ
K
−ζ(cid:101)K∥
2
≤(r (cid:98)K(J ,) 1+r (cid:98)K(J ,) 2c 1(y))∥ζ K−1−ζ(cid:101)K−1∥
2
Lastly, we derive a bound on the Tikhonov solution
+r (cid:98)K(J ,) 2c 2(y)∥P u−P(cid:101)u∥
2
J D
Proposition 12. For any SPD matrix P, any z, y, and 2 ≤ +(cid:88)(cid:88) r (cid:98)K(j, ,J d,) 3∥θ K(j ,) d−θ(cid:101) K(j ,) d∥
(d)
p≤∞ it holds that ∥T y(z;P)∥ p ≤∥z∥ ∞∥P∥ 2∥A∥ p∥y∥ p. j=1d=1
(cid:104) (cid:105)
Proof. Using Lemma 7 observe ≤ (r (cid:98)K(J ,) 1+r (cid:98)K(J ,) 2c 1(y)) (cid:98)c( 1K−1,J)(y)+r (cid:98)K(J ,) 2c 2(y) ∥P u−P(cid:101)u∥ 2
K−1 J D
∥T y(z;P)∥
p
=∥(AT zA z+P−1)−1AT zy∥
p
+(r (cid:98)K(J ,) 1+r (cid:98)K(J ,) 2c 1(y)) (cid:88) (cid:88)(cid:88) (cid:98)c( kK ,j,− d1 ,2,J)(y)∥θ k(j ,d) −θ(cid:101) k(j ,d)∥
(d)
≤∥(ATA +P−1)−1∥ ∥Diag(z)∥ ∥A∥ ∥y∥ k=1 j=1d=1
z z p p p p
J D
≤∥(AT zA z+P−1)−1∥ 2∥Diag(z)∥ 2∥A∥ p∥y∥ p +(cid:88)(cid:88) r (cid:98)K(j, ,J d,) 3∥θ K(j ,) d−θ(cid:101) K(j ,) d∥ (d). (16)
≤∥z∥ ∥P∥ ∥A∥ ∥y∥ .
∞ 2 p p j=1d=19
First note, V. CONCLUSIONANDFUTUREWORK
(r (cid:98)K(J ,) 1+r (cid:98)K(J ,) 2c 1(y)) (cid:98)c( 1K−1,J)(y)+r (cid:98)K(J ,) 2c 2(y) For a class of compound Gaussian prior based DNNs that
K−1 K−1 solve linear inverse problems, a generalization error bound
=(r(J) +r(J)c (y))c (y) (cid:88) r(J) (cid:89) (r(J)+r(J)c (y)) wasderived.Subsequently,thisgeneralizationerrorboundwas
(cid:98)K,1 (cid:98)K,2 1 2 (cid:98)k,2 (cid:98)ℓ,1 (cid:98)ℓ,2 1
k=1 ℓ=k+1 applied to two realizations, namely, CG-Net and DR-CG-Net,
+r(J)c (y) showingboundsforthesecases.Thedevelopedgeneralization
(cid:98)K,2 2
error bound was produced by bounding the Rademacher com-
K−1 K
=c (y) (cid:88) r(J) (cid:89) (r(J)+r(J)c (y)) plexityofthenetworkhypothesisclassbyDudley’sinequality,
2 (cid:98)k,2 (cid:98)ℓ,1 (cid:98)ℓ,2 1
whichisfurtherboundedusingaLipschitzpropertytoestimate
k=1 ℓ=k+1
covering numbers of the network hypothesis class. A key
K
+r(J)c (y) (cid:89) (r(J)+r(J)c (y)) contribution was in showing the parameters of compound
(cid:98)K,2 2 (cid:98)ℓ,1 (cid:98)ℓ,2 1
GaussianDNNssatisfyaLipschitzconditionunderreasonable
ℓ=K+1
assumptions thereby allowing us to produce generalization
K K
=c (y)(cid:88) r(J) (cid:89) (r(J)+r(J)c (y)) bounds for CG-based DNNs.
2 (cid:98)k,2 (cid:98)ℓ,1 (cid:98)ℓ,2 1
Whilethederivedgeneralizationerrorshowswithsufficient
k=1 ℓ=k+1
=c(K,J)(y). trainingdata,roughlyscalingquadraticallyinsignaldimension
(cid:98)1
and cubically in network size, small generalization guarantees
Similarly, note can be met, it still remains to be shown that such a property
(r(J) +r(J)c (y))c(K−1,J)(y)=c(K,J)(y) is true for small training datasets. This is desirable as CG-Net
(cid:98)K,1 (cid:98)K,2 1 (cid:98)k,j,d,2 (cid:98)k,j,d,2 and DR-CG-Net significantly outperform comparative meth-
and odsinimageestimationproblemswhentrainedonlyonasmall
K dataset. Likely, aspects of the iterative algorithm, in particular
r (cid:98)K(j, ,J d,)
3
=r (cid:98)K(j, ,J d,)
3
(cid:89) (r (cid:98)ℓ( ,J 1)+r (cid:98)ℓ( ,J 2)c 1(y)) the Tikhonov solution, would need to be further leveraged
ℓ=K+1 to provide insight into a small training generalization error
=c(K,J) (y). bound. Furthermore, PAC-Bayes generalization bounds [28]
(cid:98)K,j,d,2
for CG-based DNNs, extending the derived bounds in this
Combining these two notes with (16) produces the desired
paper, is another open question that can provide greater
result.
insights into the generalization for low training scenarios.
Finally, we show that G-CG-Net estimates, c(y;Θ), are
(cid:98)
Lipschitz w.r.t. the G-CG-Net parameters.
VI. APPENDIX:PRELIMINARIES
Theorem 14. Let Assumption 3 hold. Then for any parame-
A. Rademacher Complexity
terizations Θ and Θ(cid:101) of G-CG-Net, the G-CG-Net estimates
satisfy the following Lipschitz property: Recall a Rademacher variable is a discrete random variable
γ taking values ±1 with equal probability.
∥c (cid:98)(y;Θ)−c (cid:98)(y;Θ(cid:101))∥
2
K J D Definition 15. Let G be a set of functions g : X → R and
≤κ(y)∥P u−P(cid:101)u∥ 2+(cid:88)(cid:88)(cid:88) κ( kj ,d)(y)∥θ k(j ,d) −θ(cid:101) k(j ,d)∥ (d) S ={x i}N i=s
1
⊆X. The empirical Rademacher complexity is
k=1j=1d=1
for (cid:34) 1 (cid:88)Ns (cid:35)
R (G)=E sup γ g(x )
κ(y)=z (c (y)+∥P ∥ ∥A∥ ∥y∥ )c(K,J)(y)+z c (y) S γ g∈G N s i i
∞ 1 u 2 ∞ ∞ (cid:98)1 ∞ 2 i=1
κ( kj ,d)(y)=z ∞(c 1(y)+∥P u∥ 2∥A∥ ∞∥y∥ ∞) (cid:98)c( kK ,j, ,J d,) 2(y) for γ =[γ 1,...,γ Ns] a vector of i.i.d. Rademacher variables.
where (cid:98)c( 1K,J)(y), (cid:98)c( kK ,j, ,J d,) 2(y)aregiveninProposition13and ForaDNNwithhypothesisspaceHandusinglossfunction
c 1(y),c 2(y) are given in Proposition 11. L,weareinterestedintheRademachercomplexityofthereal-
valued set of functions G =L◦H={L◦h:h∈H}.
Proof. As ρ is 1-Lipschitz observe
cmax
Theorem16([29],Theorem26.5). LetHbeasetoffunctions,
∥c (cid:98)(y;Θ)−c (cid:98)(y;Θ(cid:101))∥
2 S = {(y ,c )}Ns a training set drawn i.i.d. from D, and L
i i i=1
=∥ρ cmax(ζ
K
◦T y(ζ K;P u))−ρ cmax(ζ(cid:101)K ◦T y(ζ(cid:101)K;P(cid:101)u))∥
2 a real-valued loss function satisfying |L(h(y),c)|≤c for all
(cid:13)
≤(cid:13) (cid:13)ζ
K
◦T y(ζ K;P u)−ζ(cid:101)K ◦T y(ζ K;P u) h ∈ H and (y,c) ∼ D. Then, for ε ∈ (0,1) with probability
at least 1−ε we have for all h∈H
(cid:13)
+ζ(cid:101)K ◦T y(ζ K;P U)−ζ(cid:101)K ◦T y(ζ(cid:101)K;P(cid:101)u)(cid:13)
(cid:13) (cid:112)
2 L(h)≤L (h)+2R (L◦H)+4c 2ln(4/ε)/N .
S S s
≤∥T y(ζ K;P u)∥ ∞∥ζ
K
−ζ(cid:101)K∥
2
+z ∞∥T y(ζ K;P u)−T y(ζ(cid:101)K;P(cid:101)u)∥ 2. Next,weprovideacontractionlemmaallowingustoignore
the loss function and only consider the hypothesis class.
UsingProposition11,12,and13producesthefinalresult.10
Lemma 17 ([30], Corollary 4). Let H be a set of functions D. Covering Number Bounds
h:X →Rd andS ={x }Ns ⊆X.Thenforanyτ-Lipschitz
i i=1 First, a covering number bound on a subset of the unit ball.
functions g :Rd →R, where i∈N[N ],
i s
Lemma 19 ([31],PropositionC.3[32]). Foranynorm∥·∥on
(cid:34) (cid:88)Ns (cid:35) √ (cid:34) (cid:88)Ns (cid:88)d (cid:35) Rn and subset U ⊆{x∈Rn :∥x∥≤1} it holds
E sup γ g ◦h(x ) ≤ 2τE sup γ h (x )
γ i i i Γ ik k i
h∈H
i=1
h∈H
i=1k=1
N(U,∥·∥,ϵ)≤(1+2/ϵ)n.
where γ =[γ i] i∈N[Ns] and Γ=[γ ik]k i∈∈ NN [[ Nd] s] are collections of Second, a covering number bound on a space of parametric
i.i.d. Rademacher variables. functions satisfying a Lipschitz criterion.
Lemma 20. Let (Θ,∥·∥ ) be a non-empty, bounded, and
ϑ
B. Dudley’s Inequality normed vector space. Define F = {f θ : X → Y|θ ∈ Θ}
andlet∥·∥
F
beanynormonF.Assumethatforanyθ,θ(cid:101)∈Θ
To define Dudley’s inequality, which is employed to obtain
a bound on R S(L ◦ H) in Theorem 16, we require some ∥f θ(x)−f θ(cid:101)(x)∥
F
≤Γ(x)∥θ−θ(cid:101)∥
ϑ
terminology from [31, 32]. First, for a metric space (M,d),
and let Γ=sup Γ(x). Then
let C M(ϵ) ⊆ M denote an ϵ-covering of M. That is, for x∈X
every x∈M there exists a c∈C (ϵ) such that d(x,c)≤ϵ.
M N(F,∥·∥ ,ϵ)≤N(Θ,∥·∥ ,ϵ/Γ).
Second, let N(M,d,ϵ) ∈ R denote the ϵ-covering number F ϑ
of M. That is, N(M,d,ϵ) is the minimum cardinality of all Proof. LetC beanyϵ/Γ-coveringofΘandC ={f :θ ∈
Θ F θ
C M(ϵ)or,equivalently,theminimumnumberofϵradiusballs C Θ}. Note |C F| ≤ |C Θ| as, at most, every θ ∈ C
Θ
uniquely
asmeasuredbyd,tocontainM.Third,areal-valuedstochastic defines a function f ∈ C . Further, for any f ∈ F there
θ F θ
process(X t) t∈T iscalledasubgaussianprocessifE(X t)=0 exists a c ∈ C Θ such that ∥θ −c∥ ≤ ϵ/Γ. Thus f c ∈ C F
and for all s∈T, t∈T and θ >0 satisfies
(cid:16) (cid:17)
E[exp(θ(X s−X t))]≤exp θ2d(cid:101)(X s,X t)2/2 ∥f θ(x)−f c(x)∥ F ≤Γ(x)∥θ−c∥ ϑ ≤Γ∥θ−c∥ ϑ ≤ϵ.
Hence, every ϵ/Γ-covering of Θ generates an ϵ-covering of
Fw oh ue rr te h,d(cid:101) t( hX
es
r, aX diut)
s
o=
f
T(E is|X
∆s
(− T)X =t|2 s) u1 p/2 is (cid:112)a Ep |Xseu |d 2.o-metric.
F. Take C
Θ
to be the minimal cardinality ϵ/Γ-covering of Θ,
t∈T t then N(F,∥·∥ ,ϵ)≤|C |=|C |=N(Θ,∥·∥ ,ϵ/Γ).
F F Θ ϑ
Lemma 18 (Dudley’s Inequality, [31], Theorem 8.23 [32]).
Finally, we generalize Lemma 20 to a class of functions
For a subgaussian stochastic process (X ) with pseudo-
t t∈T
parameterized by a sequence of parameters.
metric d(cid:101)
(cid:18) (cid:19) √ (cid:90) ∆(T)/2(cid:114) (cid:16) (cid:17) Corollary 21. Let (Θ i,∥·∥ ϑi), for i∈N[n], be a sequence of
E supX
t
≤4 2 ln N(T,d(cid:101),ϵ) dϵ. non-empty, bounded, and normed vector spaces. Define F =
t∈T 0 {f θ1,...,θn :X →Y|θ i ∈Θ i} and let ∥·∥ F be a norm on F.
Assume that
C. Rademacher Process n
(cid:88)
Let V be a vector space and X =VT ={f :T →V}. A
∥f θ1,...,θn(x)−f θ(cid:101)1,...,θ(cid:101)n(x)∥
F
≤ Γ i(x)∥θ i−θ(cid:101)i∥
ϑi
i=1
Rademacher process, (Y ) , is a stochastic process of the
t t∈T
formY t
=(cid:80)n
k=1γ kx k(t)wherex k ∈X andγ =[γ 1,...,γ n]
and let Γ
i
=sup
x∈X
Γ i(x). Then
are i.i.d. Rademacher variables. As E(γ k) = 0, γ k2 = 1, (cid:89)n
and γ ,γ are independent for k ̸= j, then for real-valued N(F,∥·∥ ,ϵ)≤ N(Θ ,∥·∥ ,ϵ/(nΓ )).
k j F i ϑi i
Rademacher processes, i.e. V =R, i=1
E |Y |2
=(cid:88)n
E (cid:0) γ2x (t)2(cid:1)
+(cid:88)n (cid:88)n
E (γ γ x (t)x (t))
tP hr eoo pf r. odL ue ct t( mS k e, trd ick) s, pf ao cr ek (S∈ ,N d)[n w], hb ee rem Set :r =ic Sspa ×ce ·s ··a ×nd Sdefi ann de
γ t γ k k γ j k j k 1 n
k=1 k=1j=1 d(s,c) :=
(cid:80)n
k=1a kd k(s k,c k) for s,c ∈ S and fixed a k ∈
j̸=k (0,∞).LetC beanyϵ/(a n)-coveringofS anddefineC :=
n k k k
=(cid:88) x (t)2. (17) C 1×···×C n. Then for any s∈S there exists a c∈C such
k
that d(s,c) =
(cid:80)n
a d (s ,c ) ≤
(cid:80)n
a ϵ/(na ) = ϵ,
k=1 k=1 k k k k k=1 k k
which implies that C is an ϵ-covering of S. Take every C to
k
Similarly, be the minimal cardinality ϵ/(a n)-covering of S then
k k
n n n (cid:18) (cid:19)
d(cid:101)(Y s,Y t)2 =E γ|Y s−Y t|2 =(cid:88) (x k(s)−x k(t))2. (18) N(S,d,ϵ)≤|C|= (cid:89) |C k|= (cid:89) N S k,d k, aϵ
n
. (19)
k
k=1 k=1 k=1
Finally, we remark that real-valued Rademacher processes are Next, let (Θ,∥·∥ ) be the normed vector space where Θ=
Θ
subgaussian [32] and thus satisfy Lemma 18. Θ × ··· × Θ and for θ = (θ ,...,θ ) ∈ Θ we define
1 n 1 n11
∥θ∥
Θ
=
(cid:80)n
i=1Γ i∥θ i∥ ϑi. Then for any θ,θ(cid:101)∈ Θ where θ = whereweusedthatanyoutputfromG-CG-Netisbounded,in
(θ 1,...,θ n) and θ(cid:101)=(θ(cid:101)1,...,θ(cid:101)n) we have Euclidean norm, by c max. Hence, by equation (17)
∥f (x)−f (x)∥ ∆(M(1))=sup ∥M ∥ ≤(cid:112) N c .
θ1, n...,θn θ(cid:101)1,...,θn nF CG c(cid:98)∈H( C1 G) c(cid:98) F s max
(cid:88) (cid:88) Therefore, using Dudley’s Inequality in Lemma 18 and (21)
≤ Γ i(x)∥θ i−θ(cid:101)i∥
ϑi
≤ Γ i∥θ i−θ(cid:101)i∥
ϑi
=∥θ−θ(cid:101)∥ Θ.
√
Comi b= in1
ing this Lipschitz
proi= pe1
rty with Lemma 20 and then R S(L◦H C(1 G))≤
N8τ (cid:90) Ns 2cmax (cid:114) ln(cid:16)
N(M( C1 G),∥·∥
F,ϵ)(cid:17)
dϵ.
applying (19) produces the covering number bound. s 0
(22)
VII. APPENDIX:PROOFOFTHEOREM1 Step (4). By Theorem 14 for any i∈N[N s]
ProvingTheorem1consistsofthefollowingsteps:Step(1),
we establish a Rademacher process generated by H(1). Step
∥c (cid:98)(y i;Θ)−c (cid:98)(y i;Θ(cid:101))∥
2
CG K J D
( o2 f), Lus ◦in Hg (L 1e )m am sa th1 e7, ew xpe ee cx tep dres ss upth ree mR ua mdem ofac Sh te er pco (m 1)p .le Sx ti et py ≤κ∥P u−P(cid:101)u∥ 2+(cid:88)(cid:88)(cid:88) κ( kj ,d)∥θ k(j ,d) −θ(cid:101) k(j ,d)∥ (d).
CG k=1j=1d=1
(3), invoking Dudley’s inequality we bound Step (2) by an
integralofsomecoveringnumbers.Step (4),weuseTheorem As κ and κ(j), given respectively in (8) and (9), are indepen-
k,d
14 and Corollary 21 to bound the covering numbers from dent of y then
i
Dudley’s inequality by covering numbers of the G-CG-Net (cid:112)
parameter spaces. Step (5), we use Lemma 19 to bound the
∥c (cid:98)(Y;Θ)−c (cid:98)(Y;Θ(cid:101))∥
F
≤ N s∥c (cid:98)(y i;Θ)−c (cid:98)(y i;Θ(cid:101))∥
2
covering numbers of the G-CG-Net parameter spaces. Step for any i∈N[N ]. Thus
s
(6), with the Step (5) bounds, we bound Dudley’s inequality
(cid:112)
by evaluable integrals where evaluation, simplification, and ∥c (cid:98)(Y;Θ)−c (cid:98)(Y;Θ(cid:101))∥ F ≤ N sκ∥P u−P(cid:101)u∥ 2+
using Theorem 16 produces the desired GEB for G-CG-Net. K J D
Proof of Theorem 1. Step (1). Let Y := [y ,...,y ] ∈
(cid:112) N s(cid:88)(cid:88)(cid:88) κ( kj ,d)∥θ k(j ,d) −θ(cid:101) k(j ,d)∥ (d).
Rm×Ns and
1 Ns k=1j=1d=1
Hence, from Corollary 21
M(1) :={M =[c(y ),...,c(y )]:c∈H(1)}
CG =(cid:26)
c
(cid:98)(cid:18)c(cid:98) Y;(cid:110)(cid:98) P,θ1
k(j
,d)(cid:111)
k
dj
∈
∈∈(cid:98)
N
NN
[
[[
K
DJN ] ]](cid:19)s :P(cid:98)
∈P
fuC llG
,θ k(j ,d) ∈Ω
d(cid:27) ln(cid:16) N (cid:18)(cid:16) M (cid:18)( C1 G),∥·∥ F,ϵ(cid:17)(cid:17)
ϵ
(cid:19)(cid:19)
≤ln N P,∥·∥ ,√
Nw oh wer ,e dc e(cid:98) fi(cid:0) nY e; tΘ h (cid:88)Ne(cid:1) sR: (cid:88)= a nde[ mc (cid:98) a( cy h1 e; rΘ p) ro, c. e. (cid:88)s. Ns, sc (cid:98) ( (cid:88)X(cid:0) ny MN c(cid:98)s ); MΘ c(cid:98)(cid:1) ∈] M∈ ( C1 G)R an s×Ns.
+
k(cid:88)K =1(cid:88) j=J 1(cid:88) dD =1ln(cid:32)2
N
(cid:32)N Ωs dκ ,( ∥K ·∥J (dD ),+
√
N1)
sκ( kj
,d)(ϵ KJD+1)(cid:33)(cid:33)
.
X Mc(cid:98) := γ ik[M c(cid:98)] ki = γ ik(cid:98)c k(y i) (20) (23)
i=1k=1 i=1k=1
Step (5).AsP containssymmetricn×nmatrices,thenfor
for Rademacher variables Γ=[γ ik]k i∈∈ NN [[ Nn s] ]. P = P
full
only n(n+1)/2 entries are required to uniquely
Step(2).UsingLemma17withH=H C(1 G)andeachg
i
=L, define a matrix and dim(P full) = n(n + 1)/2. Similarly,
forLaτ-LipschitzlossfunctionofG-CG-Net,e.g.SSIMloss dim(P tri) = 2n−1, dim(P diag) = n, and dim(P const) = 1.
or mean-absolute error, satisfying Assumption 2, we have Furthermore, as P/p max = {P/p max : P ∈ P} is contained
  in the unit ∥·∥ 2 ball, then, using Lemma 19, observe
R S(L◦H C(1
G))≤√
2τE Γ sup
N1 (cid:88)Ns (cid:88)n
γ ik(cid:98)c k(y i) N (P,∥·∥ 2,ϵ)=N (P/p max,∥·∥ 2,ϵ/p max)
√ (cid:18)c(cid:98)∈H( C1 G) s i= (cid:19)1k=1 ≤(1+2p max/ϵ)dim(P).
2τ
= N
s
E Γ c(cid:98)∈su Hp
( C1
G)X Mc(cid:98) . (21) Similarly,asΩ Nd/
(cid:0)
Ωω d ,i ∥s ·c ∥onta ,i ϵn (cid:1)e ≤di (n 1th +e 2u ωnit /∥ ϵ· )∥ α( dd) .ball,observe
Step (3). By equation (18), note that d (d) d
(cid:88)Ns (cid:88)n Combining these two observations with (23) gives
d(cid:101)(X Mc(cid:98)1,X Mc(cid:98)2)2 = i=1k=1([M c(cid:98)1] ki−[M c(cid:98)2] ki)2 ln(cid:16)
N
(cid:16)
M( C1 G),∥·∥
F,ϵ(cid:17)(cid:17)
√
=∥M −M ∥2 (cid:18) 2p N κ(KJD+1)(cid:19)
c(cid:98)1 c(cid:98)2 F ≤dim(P)ln 1+ max s
for ∥·∥
F
the Frobenius norm
(cid:118)
(cid:117)
(cid:117). O (cid:88)Nb sserve for any M (cid:112)c(cid:98)∈M( C1 G)
+(cid:88)K (cid:88)J (cid:88)D
α
dln(cid:32)
1+
2ω
d√ϵ
N sκ( kj ,d) ϵ(KJD+1)(cid:33)
.
∥M c(cid:98)∥ F =∥c (cid:98)(Y;Θ)∥ F =(cid:116) ∥c (cid:98)(y i;Θ)∥2 2 ≤ N sc max k=1j=1d=1
(24)
i=112
Step (6). For any ν,β >0, note that from [15] a differentiable function and thus continuous. As the CG-Net
(cid:82)β(cid:112) (cid:112) outputs are bounded, in the ∥·∥ 2 ball of radius c max, then the
ln(1+ν/ϵ)dϵ≤β ln(e(1+ν/β)). (25)
0 gradient of the SSIM loss function is bounded. Hence, by the
Combining (24), (25), and using subadditivity of square roots mean value theorem [33], there exists a Lipschitz constant for
theSSIMlossfunction,onthe∥·∥ ballofradiusc ,which
(cid:90) β(cid:114) ln(cid:16) N (cid:16) M(1),∥·∥ ,ϵ(cid:17)(cid:17) dϵ we denote by τ. Therefore, Assum2 ption 2 holds. max
0 CG F Next, for i ∈ {1,2}, let z i ∈ Rn satisfy ∥z i∥ ∞ ≤ z ∞
≤(cid:112) dim(P)(cid:90) β(cid:115) ln(cid:18) 1+ 2p max√ N sκ(KJD+1)(cid:19) dϵ Aan sd Pu i = isT 1y -Lp i( pz si c; hP ii tz) ,f to hr enso tm hee CP Gi -N∈ eP t sco cn as lt ea -vn ad rip ab∈ le-N up[N das t] e.
ϵ a,b
+(cid:88)K (cid:88)J (cid:88)D √0
α
(cid:90) β(cid:118) (cid:117) (cid:117) (cid:116)ln(cid:32) 1+2ω d√ N sκ( kj ,d)(KJD+1)(cid:33)
dϵ
method, g k(j), in (4) satisfies
k=1j=1d=1
d
0
ϵ ∥g k(j)(z 1,u 1;B k(j),µ( kj))−g k(j)(z 2,u 2;B(cid:101) k(j),µ (cid:101)( kj))∥
2
≤(cid:112) dim(P)β(cid:115) ln(cid:18) e(cid:18)
1+
2p max√ N sκ(KJD+1)(cid:19)(cid:19) =(cid:13) (cid:13) (cid:13)P a,b(z 1−B k(j)ρ ξ(∇ zF(u 1,z 1;µ( kj))))
β (cid:13)
+(cid:88)K (cid:88)J (cid:88)D √ α dβ(cid:118) (cid:117) (cid:117) (cid:116)ln(cid:32) e(cid:32) 1+2ω d√ N sκ( kj , βd)(KJD+1)(cid:33)(cid:33) .
≤∥z− 1P
−a,
Bb( k(z
j)2
ρ− ξ(∇B(cid:101)
k
z(j F)ρ
(ξ
u( 1∇
,z
zF 1;( µu
( k2
j), )z )2;µ (cid:101)( kj))))(cid:13)
(cid:13) 2
k=1j=1d=1
(26) −z 2+B(cid:101) k(j)ρ ξ(∇ zF(u 2,z 2;µ (cid:101)( kj)))∥ 2
Combining (26) and (22), for β = √ N sc max/2, with The- ≤∥z 1−z 2∥ 2+(cid:13) (cid:13) (cid:13)B(cid:101) k(j)ρ ξ(∇ zF(u 2,z 2;µ (cid:101)( kj)))
orem 16 produces the desired generalization error bound. (cid:13)
−B(j)ρ (∇ F(u ,z ;µ(j)))(cid:13) (29)
k ξ z 1 1 k (cid:13)
2
VIII. APPENDIX:PROOFOFTHEOREM2
where in the final line we used the triangle inequality. First,
First, we require a Lipschitz condition on the gradient of as ρ is 1-Lipschitz and bounded by ξ, note that
ξ
the data fidelity term in (2) that depends on Proposition 12.
Lemma 22. For every z ∈ Rn satisfying ∥z ∥ ≤ z and
∥B(cid:101) k(j)ρ ξ(∇ zF(u 2,z 2;µ (cid:101)( kj)))−B k(j)ρ ξ(∇ zF(u 1,z 1;µ( kj)))∥
2
i i ∞ ∞ (cid:13)
u
i
=T y(z i;P i) where P
i
∈P, it holds that =(cid:13) (cid:13)B(cid:101) k(j)ρ ξ(∇ zF(u 2,z 2;µ (cid:101)( kj)))−B(cid:101) k(j)ρ ξ(∇ zF(u 1,z 1;µ( kj)))
(cid:13)
∥AT u1(A u1z 1−y)−AT u2(A u2z 2−y)∥ 2 +B(cid:101) k(j)ρ ξ(∇ zF(u 1,z 1;µ( kj)))−B k(j)ρ ξ(∇ zF(u 1,z 1;µ( kj)))(cid:13) (cid:13)
≤(z p ∥y∥ ∥A∥ ∥A∥ )2∥z −z ∥ 2
∞ max 2 2 ∞ 1 2 2 ≤p ∥ρ (∇ F(u ,z ;µ(j)))−ρ (∇ F(u ,z ;µ(j)))∥
+∥y∥ ∥A∥ (cid:0) 1+z2 p ∥A∥ (∥A∥ +∥A∥ )(cid:1) ∥u −u ∥ . max ξ z 2 2 (cid:101)k ξ z 1 1 k 2
2 2 ∞ max 2 2 ∞ 1 2 2 +∥ρ ξ(∇ zF(u 1,z 1;µ( kj)))∥ 2∥B(cid:101) k(j)−B k(j)∥
2
Proof. Observe
≤p ∥∇ F(u ,z ;µ(j))−∇ F(u ,z ;µ(j))∥
max z 2 2 (cid:101)k z 1 1 k 2
∥AT u1(A u1z 1−y)−AT u2(A u2z 2−y)∥ 2 +ξ∥B(cid:101) k(j)−B k(j)∥ 2. (30)
≤∥AT A z −AT A z ∥ +∥(AT −AT )y∥
u1 u1 1 u2 u2 2 2 u1 u2 2
Second, note that
≤∥AT A z −AT A z ∥ +∥A∥ ∥y∥ ∥u −u ∥ .
u1 u1 1 u2 u2 2 2 2 2 1 2 2
(27) ∥∇ F(u ,z ;µ(j))−∇ F(u ,z ;µ(j))∥
z 2 2 (cid:101)k z 1 1 k 2
By Proposition 12 and the triangle inequality note that ≤∥AT (A z −y )−AT (A z −y )∥
u1 u1 1 p u2 u2 2 p 2
∥AT A z −AT A z ∥ +∥µ(j)[h−1]′(z )⊙h−1(z )−µ(j)[h−1]′(z )⊙h−1(z )∥
u1 u1 1 u2 u2 2 2 k 1 1 (cid:101)k 2 2 2
=(cid:13) (cid:13)A +T
u
A1A
T u2u
A1z
u1
2z− 1A −T
u
A1A
T u2u
A2z
u1
2z+
2(cid:13)
(cid:13)A 2T u1A u2z 1−AT u2A u2z
1 (cid:13) (cid:13)
(cid:13)=
µ(
k∥ jA )[T
u h1
−( 1A
]′u
(z1z
1)1
⊙− hy −p) 1(−
z
1A )T
u
−2(
µ
(cid:101)A
( kju )2
[hz
−2
1− ]′(y
zp
1) )∥ ⊙2+
h−1(z 1)
≤z ∞2 p max∥A∥2 2∥y∥ 2(∥A∥ 2+∥A∥ ∞)∥u 1−u 2∥ 2 +µ(j)[h−1]′(z )⊙h−1(z )−µ(j)[h−1]′(z )⊙h−1(z )(cid:13) (cid:13)
+(z ∞p max∥A∥ 2∥A∥ ∞∥y∥ 2)2∥z 1−z 2∥ 2. (28) (cid:101)k 1 1 (cid:101)k 2 2 (cid:13) 2
≤∥AT (A z −y )−AT (A z −y )∥
Combining (27) and (28) produces the desired result. u1 u1 1 p u2 u2 2 p 2
+h |µ(j)−µ(j)|+µτ ∥z −z ∥ . (31)
As an overview, proving Theorem 2 consists of the follow- max k (cid:101)k h 1 2 2
ing steps: First, we show Assumption 2 holds for the SSIM Combining Lemma 22 with (31), (30), (29) and the fact
loss function used in CG-Net. Second, we invoke Lemma 22 that∥y ∥ ≤y ,thenAssumption3holdsspecificallywith
p 2 max
to show that the Lipschitz condition of Assumption 3 holds r(j−1) =r ,r(j−1) =r and
for each CG-Net scale-variable-descent update in (4). Finally, k,1 1 k,2 2
we apply Theorem 1 to produce Theorem 2. (cid:16) (cid:17)
tP hr eoo Sf So IMf T lh oe so sre fum nc2 t. ioA ns isSS boIM undre et dur bn ys 2a .v Fa rl ou me in [1[ 9− ],1 S,1 S] IMthe in
s
(cid:16) θ k(j ,d),r k(j ,d− ,31),∥·∥ (d)(cid:17) = (cid:16)B µ( kk( jj )) ,, pξ m,∥ ax·∥ h2 max,|·|(cid:17) dd == 21 .13
Hence, by Proposition 6, r(J) =(cid:81)J r(j−1) =rJ, As an overview, proving Theorem 4 consists of the follow-
(cid:98)k,1 j=1 k,1 1
ing steps: First, we show Assumption 2 holds for the mean-
r(J) =(cid:88)J r(j−1)J (cid:89)−1 r(ℓ) =r (cid:88)J rJ−j =r 1−r 1J , absolute error loss function used in DR-CG-Net. Second,
(cid:98)k,2 k,2 k,1 2 1 21−r we use invoke Lemma 22 and Lemma 24 to show that the
1
j=1 ℓ=j j=1
Lipschitz condition of Assumption 3 holds for each DR-CG-
and similarly r(j,J) = r(j−1)(cid:81)J−1r(ℓ) = r(j−1)rJ−j is as Net scale-variable-descent update in (5). Finally, we apply
(cid:98)k,d,3 k,d,3 ℓ=j k,1 k,d,3 1
given in (10). Therefore, applying Theorem 1 produces the Theorem 1 to produce Theorem 4.
GEB of CG-Net in Theorem 2.
Proof of Theorem 4. As DR-CG-Net employs the mean ab-
solute loss function, L(x ,x ) = 1∥x − x ∥ , for any
IX. APPENDIX:PROOFOFTHEOREM4 x ,x ,x∈Rn byHo¨lder’s1 an2 dtheren verse1 triang2 le1 inequality
1 2
WefirstrequireaboundedandLipschitzpropertyforfully-
connected networks. Let G(i)(x) := W(i)x, for W(i) ∈ |L(x 1,x)−L(x 2,x)|
Rdt+1×dt, denote a fully-cont nected layer. Ft or component twise ≤L(x ,x )= 1 ∥x −x ∥ ≤ √1 ∥x −x ∥ ≤ c √max.
activation function σ, define a fully-connected network G(i,T) 1 2 n 1 2 1 n 1 2 2 n
√ √
as Hence,Assumption2holdsforc=c / nandτ =1/ n.
max
G(i,T)(x)=G(i)◦σ◦G(i) ◦···◦σ◦G(i)(x). (32) Next, for i ∈ {1,2}, let z i ∈ Rn satisfy ∥z i∥ ∞ ≤ z ∞ and
T T−1 1 u = T (z ;P ) for some P ∈ P and p ∈ N[N ]. Let
i y i i i tri s
p
Lemma 23. Let σ be a componentwise activation function V(j) and V(cid:101)(j) be convolutional subnetworks of DR-CG-Net
∥sa Wtis (f iy )∥ing ≤∥σ ϖ(x) fo∥ r2 a≤ ll∥ tx ∈∥ 2 Na [Tnd ],G th( ei, nT) be given as in (32). If thk at are park ameterized by {W k( ,j ℓ)} ℓ∈N[Lc] and {W(cid:102) k( ,j ℓ)} ℓ∈N[Lc],
t 2 t respectively. The DR-CG-Net scale-variable-update method,
T g(j), in (5) satisfies
(cid:89) k
∥σ(G(i,T)(x))∥ ≤ ϖ ∥x∥ .
2 t 2 (cid:13)
t=1
(cid:13) (cid:13)g k(j)(z 1,u 1;{δ k(j),W k( ,j ℓ)} ℓ∈N[Lc])
tP rir vo io af l. lyW
.
Ae su ss ue mi end thu ecti io nn duo cn tioT n. hT yh pe otb ha es se isc ha os le dsT fo=
r
fi1 xh eo dld Ts −g k(j)(z 2,u 2;{δ(cid:101) k(j),W(cid:102) k( ,j ℓ)} ℓ∈N[Lc])(cid:13) (cid:13)
(cid:13) 2
(cid:13)
where T >1. Observe =(cid:13)v(j)(z ,u ;δ(j))+V(j)(z )
(cid:13) k 1 1 k k 1
∥σ(G(i,T+1)(x))∥ 2 =∥σ(W T(i +) 1σ(G(i,T)(x)))∥ 2 −v k(j)(z 2,u 2;δ(cid:101) k(j))−V(cid:101) k(j)(z 2)(cid:13) (cid:13)
(cid:13)
T+1 2
(cid:89)
≤ϖ T+1∥σ(G(i,T)(x))∥
2
≤ ϖ t∥x∥ 2. ≤∥v k(j)(z 1,u 1;δ k(j))−v k(j)(z 2,u 2;δ(cid:101) k(j))∥
2
t=1 +∥V k(j)(z 1)−V(cid:101) k(j)(z 2)∥ 2. (33)
Now, we show that fully-connected networks are Lipschitz.
Define d =AT (A z −y ) for i=1,2. Using (6) note
Lemma 24. In addition to the assumptions of Lemma 23, i ui ui i p
assume σ is τ-Lipschitz. Then ∥v k(j)(z 1,u 1;δ k(j))−v k(j)(z 2,u 2;δ(cid:101) k(j))∥
2
∥G(1,T)(x )−G(2,T)(x )∥
≤τT−1(cid:89)T
ϖ ∥x −x ∥
≤∥z 1−z 2∥ 2+∥δ k(j)ρ ξ(d 1)−δ(cid:101) k(j)ρ ξ(d 2)∥ 2. (34)
1 2 2 t 1 2 2
As ρ is 1-Lipschitz and bounded, in norm ∥·∥ , by ξ then
t=1 ξ 2
T (cid:18) (cid:19)
+(cid:88) τT−t(cid:81)T t′=1ϖ t′∥x 1∥ 2 ∥W t(1)−W t(2)∥ 2. ∥δ k(j)ρ ξ(d 1)−δ(cid:101) k(j)ρ ξ(d 2)∥ 2
t=1 t′̸=t =∥δ k(j)ρ ξ(d 1)−δ k(j)ρ ξ(d 2)+δ k(j)ρ ξ(d 2)−δ(cid:101) k(j)ρ ξ(d 2)∥
2
Proof. Observe for any t ≤|δ k(j)|∥ρ ξ(d 1)−ρ ξ(d 2)∥ 2+∥ρ ξ(d 2)∥
2
|δ k(j)−δ(cid:101) k(j)|
∥G t(1)(x 1)−G t(2)(x 2)∥ 2 ≤δ∥d 1−d 2∥ 2+ξ|δ k(j)−δ(cid:101) k(j)|. (35)
=∥W(1)x −W(2)x ∥
t 1 t 2 2 Now, combining Lemma 22 with (35), (34), and the fact that
=∥W(1)x −W(2)x +W(2)x −W(2)x ∥ ∥y ∥ ≤y produces
t 1 t 1 t 1 t 2 2 p 2 max
≤ϖ ∥x −x ∥ +∥x ∥ ∥W(1)−W(2)∥ .
t 1 2 2 1 2 t t 2 ∥v k(j)(z 1,u 1;δ k(j))−v k(j)(z 2,u 2;δ(cid:101) k(j))∥
2
Therefore ≤(cid:0) 1+δ(z p y ∥A∥ ∥A∥ )2(cid:1) ∥z −z ∥
∞ max max 2 ∞ 1 2 2
∥G(1,t)(x 1)−G(2,t)(x 2)∥
2
+r 2∥u 1−u 2∥ 2+ξ|δ k(j)−δ(cid:101) k(j)|. (36)
≤τϖ t∥G(1,t−1)(x 1)−G(2,t−1)(x 2)∥ 2 As W k( ,j d) ∈ Ω
d
and W(cid:102) k( ,j d) ∈ Ω
d
for d ∈ N[L c], then
+∥σ(G(1,t−1)(x 1))∥ 2∥W t(1)−W t(2)∥ 2. W k( ,j d) and W(cid:102) k( ,j d) are defined by f d−1f dk d2 parameters and
Using Lemma 23 and induction on T similar to that of satisfy ∥W k( ,j d)∥ 2 ≤ w d and ∥W(cid:102) k( ,j d)∥ 2 ≤ w d. Additionally,
Proposition 6 produces the desired result. δ(j) ∈ Ω is a positive real number satisfying |δ(j)| ≤ δ.
k Lc+1 k14
Hence,thedimensionoftheparameterspaces,α d,andbounds [13] C.Lyons,R.G.Raj,andM.Cheney,“CG-Net:ACompoundGaussian
on the parameter spaces, ω , are given by (12). Prior Based Unrolled Imaging Network,” in 2022 IEEE Asia-Pacific
d
Signal and Information Processing Association Annual Summit and
AstheReLUactivationfunctionis1-Lipschitzand||z || ≤
√ 1 2 Conference,2022,pp.623–629.
nz ∞ then using Lemma 24 [14] C.Lyons,R.G.Raj,andM.Cheney,“ADeepCompoundGaussianReg-
ularizedUnfoledImagingNetwork,”in202256thAsilomarConference
∥V k(j)(z 1)−V(cid:101) k(j)(z 2)∥
2
≤
(cid:89)Lc
w ℓ∥z 1−z 2∥
2
[15]
o NAn
e.
uS
B
ri aeg lhn ba Nols eo,
td
wS i,y os rHt ke
.
sm Rs fra, oua mhn ud
t
a,C ao Snm
td
ap tiEu st
.
te icr
S
as c, lh2
n
L0
o
e2
o
a2
r
r,, nip
“
nCp g. o9
m
P4 ep0 rr–
se
p9
s
es4
i
c7
v
t.
ie veS ,”en is ning Coa mnd
-
ℓ=1 pressedSensingin InformationProcessing. Springer, 2022,pp.247–
+(cid:88)Lc (cid:18)√
nz ∞(cid:81)L ℓ′c =1w
ℓ′(cid:19)
∥W k( ,j ℓ)−W(cid:102) k( ,j ℓ)∥ 2. (37) [16]
2
fV
o7
.
r7 K.
Ao nu an li ysa isn -d BaY se. dP Can oa mga pk reis s, se“ dD SE eC nO siN ngET W: iA thn GU en nf eo rl ad lii zn ag tioN net Ew ro rork
r
ℓ=1
ℓ′̸=ℓ
Bounds,” IEEE Transactions on Signal Processing, vol. 71, pp. 1938–
Combining (37) and (36) with (33) then Assumption 3 holds 1951,2023.
with r(j−1) =r , r(j−1) =r , and [17] B.Joukovsky,T.Mukherjee,H.VanLuong,andN.Deligiannis,“Gen-
k,1 1 k,2 2 eralizationErrorBoundsforDeepUnfoldingRNNs,”inUncertaintyin
ArtificialIntelligence. PMLR,2021,pp.1515–1524.
(cid:16) θ k(j ,d),r k(j ,d− ,31),∥·∥ (d)(cid:17) = (cid:18) W k( ,j d),√ nz ∞(cid:81)L ℓℓ ̸==c d1w ℓ,∥·∥ 2(cid:19) d=1,...,L c [18] 2I. 0G 16o .odfellow,Y.Bengio,andA.Courville,DeepLearning. MITpress,
(δ(j),ξ,|·|) d=L +1. [19] H.Zhao,O.Gallo,I.Frosio,andJ.Kautz,“LossFunctionsforImage
k c Restoration With Neural Networks,” IEEE Transactions on Computa-
J tionalImaging,vol.3,no.1,pp.47–57,Mar2017.
Hence, by Proposition 6, r(J) = (cid:89) r(j−1) =rJ, [20] M. J. Wainwright, E. P. Simoncelli, and A. S. Willsky, “Random
(cid:98)k,1 k,1 1 Cascades on Wavelet Trees and Their Use in Analyzing and Model-
j=1 ing Natural Images,” Applied and Computational Harmonic Analysis,
vol.11,no.1,pp.89–123,2001.
r (cid:98)k(J ,2)
=(cid:88)J
r k(j
,2−1)J (cid:89)−1
r k(ℓ ,)
1
=r
2(cid:88)J
r 1J−j =r
21 1− −r r1J
,
[21] aM n. dJ t. heW Sa ti an tw isr tii cg sht ofan Nd atE ur. aP l. ImSi am geo sn .”ce il nli, Ad“S vac nal ce esM inix Ntu er ue rs alof InG foa ru ms asi ta ion ns
j=1 ℓ=j j=1 1 ProcessingSystems,vol.12. MITPress,1999,pp.855–861.
[22] J. Wang, A. Dogandzˇic´, and A. Nehorai, “Maximum Likelihood Es-
and similarly r(j,J) = r(j−1)(cid:81)J−1r(ℓ) = r(j−1)rJ−j is timation of Compound-Gaussian Clutter and Target Parameters,” IEEE
(cid:98)k,d,3 k,d,3 ℓ=j k,1 k,d,3 1 TransactionsonSignalProcessing,vol.54,no.10,pp.3884–3898,2006.
given as in (13). Therefore, applying Theorem 1 produces the [23] Z.Chance,R.G.Raj,andD.J.Love,“Information-theoreticstructure
GEB of DR-CG-Net in Theorem 4. of multistatic radar imaging,” in IEEE RadarCon (RADAR), 2011, pp.
853–858.
[24] Z.Idriss,R.G.Raj,andR.M.Narayanan,“WaveformOptimizationfor
REFERENCES Multistatic Radar Imaging Using Mutual Information,” IEEE Transac-
tions on Aerospace and Electronics Systems, vol. 57, no. 4, pp. 2410–
[1] K.GregorandY.LeCun,“Learningfastapproximationsofsparsecod- 2425,Aug2021.
ing,” in Proceedings of the 27th International Conference on Machine [25] R.G.Raj,“AhierarchicalBayesian-MAPapproachtoinverseproblems
Learning,2010,pp.399–406. inimaging,”InverseProblems,vol.32,no.7,p.075003,Jul2016.
[2] V.Monga,Y.Li,andY.C.Eldar,“AlgorithmUnrolling:Interpretable, [26] S.J.Wright,“Coordinatedescentalgorithms,”MathematicalProgram-
efficient deep learning for signal and image processing,” IEEE Signal ming,vol.151,no.1,pp.3–34,Jun2015.
ProcessingMagazine,vol.38,no.2,pp.18–44,Mar2021. [27] J. Sola and J. Sevilla, “Importance of input data normalization for the
[3] J. Zhang and B. Ghanem, “ISTA-Net: Interpretable Optimization- application of neural networks to complex industrial problems,” IEEE
InspiredDeepNetworkforImageCompressiveSensing,”inProceedings TransactionsonNuclearScience,vol.44,no.3,pp.1464–1468,1997.
of the IEEE Conference on Computer Vision and Pattern Recognition, [28] P. Alquier, “User-friendly introduction to pac-bayes bounds,” arXiv
2018,pp.1828–1837. preprintarXiv:2110.11216,2021.
[4] J.Song,B.Chen,andJ.Zhang,“Memory-AugmentedDeepUnfolding [29] S.Shalev-ShwartzandS.Ben-David,UnderstandingMachineLearning:
Network for Compressive Sensing,” in Proceedings of the 29th ACM FromTheorytoAlgorithms. CambridgeUniveristyPress,2014.
InternationalConferenceonMultimedia,Oct2021,pp.4249–4258. [30] A. Maurer, “A vector-contraction inequality for Rademacher complex-
[5] C. Lyons, R. G. Raj, and M. Cheney, “A Compound Gaussian Least ities,” in Interantional Conference on Algorithmic Learning Theory.
SquaresAlgorithmandUnrolledNetworkforLinearInverseProblems,” Springer,2016,pp.3–17.
IEEETransactionsonSignalProcessing,vol.71,pp.4303–4316,2023. [31] L.Devroye,L.Gyo¨rfi,andG.Lugosi,Aprobabilistictheoryofpattern
[6] C. Lyons, R. G. Raj, and M. Cheney, “Deep Regularized Compound recognition. SpringerScience&BusinessMedia,2013,vol.31.
Gaussian Network for Solving Linear Inverse Problems,” IEEE Trans- [32] S.FoucartandH.Rauhut,AMathematicalIntroductiontoCompressive
actionsonComputationalImaging,2024,inpress. Sensing. SpringerNewYork,2013.
[7] J. Song, B. Chen, and J. Zhang, “Deep Memory-Augmented Proximal [33] W.Rudin,PrinciplesofMathematicalAnalysis. McGrawHillEduca-
UnrollingNetworkforCompressiveSensing,”InternationalJournalof tion,1953.
ComputerVision,vol.131,no.6,pp.1477–1496,Jun2023.
[8] K. H. Jin, M. T. McCann, E. Froustey, and M. Unser, “Deep Con-
volutional Neural Network for Inverse Problems in Imaging,” IEEE
TransactionsonImageProcessing,vol.26,no.9,pp.4509–4522,2017.
[9] D. You, J. Xie, and J. Zhang, “ISTA-Net++: Flexible Deep Unfolding
Network for Compressive Sensing,” in 2021 IEEE International Con-
ferenceonMultimediaandExpo(ICME). IEEE,2021,pp.1–6.
[10] J.Xiang,Y.Dong,andY.Yang,“FISTA-Net:LearningaFastIterative
Shrinkage Thresholding Network for Inverse Problems in Imaging,”
IEEETransactionsonMedicalImaging,vol.40,no.5,pp.1329–1339,
May2021.
[11] T. Meinhardt, M. Moller, C. Hazirbas, and D. Cremers, “Learning
ProximalOperators:UsingDenoisingNetworksforRegularizingInverse
Imaging Problems,” in Proceedings of the IEEE International Confer-
enceonComputerVision,2017,pp.1781–1790.
[12] S. Diamond, V. Sitzmann, F. Heide, and G. Wetzstein, “Unrolled
OptimizationwithDeepPriors,”arXivpreprintarXiv:1705.08041,2019.