Video ReCap: Recursive Captioning of Hour-Long Videos
MdMohaiminulIslam1 NaganHo1 XitongYang2 TusharNagarajan2
LorenzoTorresani2 GedasBertasius1
1UNCChapelHill 2MetaAI
https://sites.google.com/view/vidrecap
Abstract overlongperiods(e.g.,severalhours)[28,46,51,63].
Inthispaper,weinvestigateahierarchicalvideocaption-
Most video captioning models are designed to process ingtaskrequiringgeneratingcaptionsatmultiplehierarchy
shortvideoclipsoffewsecondsandoutputtextdescribing levelsgivenalongvideoinput(e.g.,severalminutestosev-
low-level visual concepts (e.g., objects, scenes, atomic ac- eral hours). Studies in psychology [9, 11, 16] and social
tions). However,mostreal-worldvideoslastforminutesor cognitivetheories[5]haveshowntheinherenthierarchical
hours and have a complex hierarchical structure spanning structures of human behavior, consisting of atomic actions
differenttemporalgranularities. WeproposeVideoReCap, at the lowest level, intermediate steps in the middle and
a recursive video captioning model that can process video overallgoals/intentsatthehighestlevelofthehierarchy.In-
inputsofdramaticallydifferentlengths(from1secondto2 spiredbythesepriorstudies,wealsoassumethreelevelsof
hours)andoutputvideocaptionsatmultiplehierarchylev- hierarchiesforourvideocaptioningtask. Atthemostgran-
els. Therecursivevideo-languagearchitectureexploitsthe ularlevel,videocaptionsdescribeindividualframesorshort
synergy between different video hierarchies and can pro- videoclipsofseveralseconds,focusingonlow-levelvisual
cess hour-long videos efficiently. We utilize a curriculum elementssuchasobjects,scenes,andatomicactions.Aswe
learning training scheme to learn the hierarchical struc- moveupthehierarchy,theshort-termcaptionscoalesceinto
ture of videos, starting from clip-level captions describing medium-lengthvideosegmentdescriptionsspanningactiv-
atomicactions,thenfocusingonsegment-leveldescriptions, itiesextendingbeyondbriefmoments,suchastheinterme-
and concluding with generating summaries for hour-long diate steps within broader activities (e.g., a single step in
videos. Furthermore, we introduce Ego4D-HCap dataset a cooking recipe) or short segments or sequences within a
byaugmentingEgo4Dwith8,267manuallycollectedlong- more extended storyline (e.g., a several minute-long scene
range video summaries. Our recursive model can flexibly within a movie). Lastly, the top level of the hierarchy en-
generate captions at different hierarchy levels while also capsulatesthelong-termhumangoalsinthevideo,intricate
being useful for other complex video understanding tasks, relationshipsbetweeneventsandcharacters,andtheoverar-
such as VideoQA on EgoSchema. Data, code, and models chingpurposebehindthevideo,whichcanbecapturedvia
arepubliclyavailable[1]. long-rangevideosummaries(SeeFigure1).
The task of hierarchical video captioning poses several
technicalchallenges. Firstly,itnecessitatesmodelscapable
1.Introduction
of handling vastly different input lengths, ranging from a
Many videos in the real world exhibit a hierarchical infor- fewsecondstoseveralhours. Thiscontrastswithmostex-
mation structure that spans human behaviors at different istingmethods,designedforfixedvideodurationsofuptoa
temporalgranularities(i.e.,atomicactions,intermediateac- fewminutes.Secondly,long-rangevideosarehighlyredun-
tivitysteps, long-termgoals, etc.). However, mostmodern dant,requiringthemodeltoaggregateonlyessentialinfor-
videocaptioningmodelsignorehierarchicalvideostructure mation while discarding unimportant visual cues. Thirdly,
andarespecificallytailoredforshortvideoinputs,typically anothercriticalchallengeiscomprehendingthehierarchical
limited to 5-15 seconds [4, 14, 22, 34, 38, 39, 43, 46, 50, structureinlongvideosandleveragingthesynergybetween
51,57,63]. Theseshort-rangecaptioningmethodscapture distincthierarchies.
atomicactionsandlow-levelvisualdetails,suchasobjects Toaddressthesetechnicalchallenges,weproposeVideo
andscenes. Moreover,thesemodelsareoftenprohibitively ReCap, a model capable of processing videos of dramati-
resource-intensive when applied to longer videos, making callydifferentlengthswhereinputtimespansmaydifferby
themill-suitedforunderstandinghumanactivitiesoccurring uptothreeordersofmagnitude(fromahandfulofseconds
1
4202
beF
02
]VC.sc[
1v05231.2042:viXraFigure1. HierarchicalVideoCaptioning. Weaimtogeneratehierarchicalcaptionsforalong-rangevideo(e.g.,severalhourslong)at
threetemporalgranularities. First,wegenerateshortclipcaptionsforeachfewsecondsofthevideofocusingonatomichumanactions.
Afterward,weproducemedium-lengthsegmentdescriptionsforeveryfewminutesofthevideo,capturingtheintermediatestepswithina
longeractivityoravideosegmentwithinanextendedstoryline.Finally,ourmethodgeneratesasummaryforalong-rangevideodepicting
theoverallintentandgoalsoftheactorsinthevideo.
to a few hours) and generating captions at multiple hier- lize Ego4D [20], the largest publicly available long-range
archy levels. Our model encompasses three key attributes egocentricvideodataset,whichprovidestime-stampedcap-
that empower its hierarchical video captioning capability. tionsandvideo-segmentsummariesofupto5minutes. We
Firstly,VideoReCapadoptsarecursivevideo-languagear- then augment the subset of Ego4D videos with manually
chitecture, allowing it to generate captions across distinct annotated 8,267 long-range video summaries, where each
hierarchical tiers. At the first level, the model generates video spans up to two hours. Consequently, the Ego4D-
captionsfromfeaturesextractedfromshortvideoclips,typ- HCap becomes a rich resource with three levels of hierar-
ically lasting a few seconds. As we move up the hierar- chical captions for long untrimmed egocentric videos, en-
chy, the model uses sparsely sampled video features and compassing captions for short clips, intermediate descrip-
captions generated at the previous hierarchy level as in- tionsforfew-minutevideosegments,andvideo-levelsum-
puts to produce video captions for the current hierarchy mariesforlongvideosequences.
level. Sucharecursivedesigneffectivelyleveragesthesyn- Our results show that Video ReCap outperforms strong
ergy between different video hierarchies and allows us to prior video captioning baselines [29, 67] across all three
handle very long video inputs (e.g., up to 2 hours) effi- temporal hierarchies by a large margin. We also demon-
ciently. Moreover, it facilitates our model to leverage the strate that Video ReCap can be effectively used for other
powerful reasoning abilities of modern LLMs. Secondly, complex video understanding tasks, such as long-form
weimplementacurriculumlearningscheme,commencing video question-answering on EgoSchema [35] where our
withtrainingonshortvideoclipcaptionsandprogressively approach outperforms the previous best method by a sub-
incorporating data from higher-level hierarchies, namely stantialmargin(+18.13%).
medium-lengthsegmentdescriptionsandlong-rangevideo
summaries. Such a hierarchical curriculum learning strat- 2.RelatedWorks
egy allows the model to gradually learn the hierarchical
structureofthevideo,startingfromshortlow-levelcaptions VideoCaptioningMethods.Earlyworksinvideocaption-
tolonghigh-levelvideosummaries.Thirdly,tomitigatethe ing used template-based approaches [25, 27, 43, 49, 61].
challenge of limited manually annotated hierarchical cap- Subsequently, these methods were replaced by deep learn-
tioning data, we use LLMs to generate pseudo-summary ingmethodsbuiltusingCNN-RNNencoder-decoderarchi-
dataspanningdifferenttemporallengthsandthenusethese tectures[8,17,37,38,47,55,56,64]. Therecentintroduc-
pseudo-annotationsasadditionaldatatotrainourmodel. tionofTransformer[18,53]ledtoaplethoraoftransformer-
To evaluate Video ReCap, we introduce Ego4D-HCap based video captioning methods [8, 22, 28, 38, 39, 46, 47,
dataset,anewhierarchicalvideocaptioningbenchmarkthat 51,56,63].Thoughtheseapproacheshaveshowngreatsuc-
containslong-rangeegocentricvideoslastinguptoseveral cessinshortclipcaptioning,mostarelimitedtoshortvideo
hourswithmanuallyannotatedcaptionsatmultiplehierar- clipsequencesofafewsecondsand,thus,cannotgenerate
chical levels. To build Ego4D-HCap benchmark, we uti- captions spanning multiple temporal hierarchies for hour-
2longvideos. 3.2.RecursiveVideo-LanguageModel
Video Captioning Datasets. Most existing video cap-
We now describe the Video ReCap model, which con-
tioning datasets contain short video clip inputs (5-30 sec-
tainsthreehigh-levelcomponents:aVideoEncoder,Video-
onds) [13, 42, 58, 60]. There exist several datasets with
Language Alignment, and a Recursive Text Decoder. We
longervideosof1-5minutes[23,26,68], butthecaptions
illustrateourapproachinFigure2anddescribeeachcom-
of these datasets still focus on short-term visual concepts
ponentbelow.
(e.g.,atomicactions,presenceofobjects,etc.). Instead,our
Video Encoder. First, we utilize an off-the-shelf video
work aims to develop models and datasets for hierarchical
encoder (e.g., TimeSformer [10]) to extract features from
video captioning that spans multiple temporal granularity
a long-range video. Given a short video clip, the video
levelsrangingfromshortclipcaptionstolong-rangevideo
encoder outputs dense spacetime features. We divide the
summaries. Todothis,weintroduceEgo4D-HCapdataset
entire video uniformly and extract a sequence of features
byaugmentingEgo4Dwithlong-rangevideosummariesof
X = [x ] , where |C| is the number of video
hour-longvideos.Thisleadstoahierarchicalvideocaption- i i,j j=1,...,|C|
clips,x ∈ RF×H×W×D isthespatiotemporalfeaturesofa
ingdatasetconsistingofshortclipcaptions,medium-range
particularclip,F isthenumberofframes,H istheheight,
segmentdescriptions,andlong-rangevideosummaries.
W is the width, and D is the feature dimension. We use
Hierarchical Video Understanding. Several recent densespacetimefeaturesforshort-clipcaptionssothatthe
datasetsincludehierarchicalactivityannotationsforproce- model can identify low-level visual cues (i.e., objects and
dural videos [7, 45, 48, 52, 69]. However, these datasets atomicactions);forhigher-levelcaptions(e.g.,segmentde-
define a fixed taxonomy for the activity labels of each hi- scriptions and video summaries), we use global features
erarchy and focus on procedural activity recognition. In (e.g., CLS features) to reduce the computational cost and
contrast, we assume free-form natural language descrip- capturetheglobalpropertiesoflongvideoinputs.
tions for multiple levels to capture inherent hierarchical
Video-Language Alignment. Next, we utilize a Video-
structure in real-world videos (not limited to only instruc-
Language (VL) Alignment module which takes the video
tional videos). Aside from the datasets, several meth-
features,X andthecaptionsgeneratedintheprevioushier-
i
ods [3, 30, 66] learn hierarchical feature embeddings for
archyY(ℓ−1)asinputandoutputsafixednumberofembed-
several-minute-long videos (e.g., 5 minutes). In contrast, i
ourworkfocusesongeneratingfree-formhierarchicalcap-
dingsZ
i
=[z i,j] j=1,...,|Z|,wherez ∈RDz,|Z|isthenum-
ber of embeddings, and D is the hidden dimension. The
tionsforhour-longvideosatmultipletemporalscales. z
objectiveofthealignmentmoduleistomapthevideoand
textfeaturestothejointfeaturespacesothatthesubsequent
3.TechnicalApproach text decoder can jointly process both features as in [29].
Moreover,thisschemeenablesustocompressalargenum-
3.1.ProblemOverview berofvideoandtextfeatures(e.g.,severalthousand)intoa
small set of embeddings (e.g., 256), dramatically reducing
Given a long, untrimmed video input, we aim to generate the computational cost. In particular, we use a frozen pre-
textual captions at multiple hierarchy levels of the video. trained language model (e.g., DistilBERT [44]) to learn a
Formally,asourinputs,weconsideralong-rangevideose- fixednumberofvideoembeddingsfromthevideofeatures
quence V i = [I i(t)] t=1,...,T comprised of T RGB frames, X i by injecting trainable cross-attention layer inside each
denoted by I(t). Our goal is then to generate captions at transformerblockoftheLM.Wealsolearnafixednumber
i oftextembeddingsfromthecaptionsgeneratedatthepre-
three distinct hierarchical levels: Y(ℓ) = [y(ℓ)]
i i,j j=1,...,|Y(ℓ)| vious hierarchy Y(ℓ−1) by using a similar frozen LM with
forℓ = 1,2,3,wherey(ℓ) depictsajth wordinacaptioi ni trainablecross-atti entionlayers. Finally,weconcatenatethe
i,j
forthehierarchylevell. Eachhierarchyofcaptionsisgen- videoandtextembeddingstogetthejointembeddingsZ ,
i
erated sequentially starting with the short-term video clip whichisusedbythesubsequenttextdecoderforgenerating
captions, Y(1), describing fine-grained actions and objects captions Y(ℓ). Note that the first hierarchy level (i.e., clip
i i
occurringwithinfewsecondsintervalsthroughoutthevideo caption) has no text features and uses only video embed-
(e.g.,apersonpicksupanappleinFigure1).Afterward,the dingsasZ .
i
model outputs medium-length segment descriptions Y(2), Recursive Text Decoder. We use a pretrained language
i
which capture intermediate steps or summaries unfolding model (e.g., GPT2 [41]) as our recursive text decoder for
over a few minutes of the video (e.g., a person driving a generating captions at multiple hierarchy levels. The de-
carandparkingitinFigure1). Finally, themodelfinishes codertakesthevideo-textembeddingsZ producedbythe
i
itsgenerationwithlong-rangevideosummariesY(3)repre- video-language alignment module (described above) and
i
sentingvideocontentfortheentirevideoinput. then generates captions Yℓ for the hierarchy ℓ. Note that
i
3Figure2.TheVideoReCapmodel.(Left)First,wegeneratecaptionsforeachshortclip(e.g.,afewsecondslong)ofthevideousingthe
densespatiotemporalfeaturesextractedbyapretrainedvideoencoder(notshowninthefigure). (Middle)ThenVideoReCapproduces
segmentdescriptionsforeveryfewminutesofthevideousingsparselysampledfeatures(e.g.,CLSfeatures)andthepreviouslygenerated
clip captions belonging to a particular segment. (Right) Finally, Video ReCap generates the full video summary by utilizing sparsely
sampledCLSfeaturesfromtheentirevideoandthepreviouslygeneratedsegmentdescriptions. TheVideo-Language(VL)Alignment
modulemapsthevideoandtextfeaturestoajointspacesothatthesubsequenttextdecodercanjointlyprocessthem.Note:theyellowbox
representsthefirstsegmentofthevideoineachofthethreepanels,zoominginfromrighttoleft.
we use captions generated at the previous hierarchy level imbalancewhereshort-termclipcaptionsvastlyoutnumber
Yℓ−1 as one of the inputs (along with video features X ), the number of video segment descriptions and long-range
i i
whichenablesarecursivecaptiongenerationpipeline.Note summaries. Finally, exploiting the synergy between dif-
thatforshort-termcaptiongeneration(i.e.,Y1),thetextual ferenthierarchylevelsiscrucialforgeneratingmeaningful
i
featuresetisinitializedasempty(i.e.,thebasecaseofour andcontextuallyrelevantcaptions.Toovercomethesechal-
model’s recursion). Following prior works [2, 67], we in- lenges,wedrawmotivationfromclassicstudiesofpsychol-
serttrainablecross-attentionblocksinsideeachtransformer ogy [5, 9, 11, 16], which show a hierarchical organization
layerofourtextualdecoderandfreezetheremaininglayers. of human perception of actions. Just as humans first per-
Thecross-attentionlayerattendstovideo-textembeddings ceiveatomicactionsbeforegraspingmid-levelactionsand
of the alignment module. Therefore, the proposed Video theninfergoalsfrommid-levelactivities,ourtrainingstrat-
ReCap models the likelihood of caption Y(ℓ) conditioned egy unfolds in a similar hierarchical fashion. Specifically,
on the video X and the captions generated at lower-level our training begins with samples from the lowest hierar-
hierarchyY(ℓ−1)usingthefollowingtrainingobjective: chylevel,namelyclipcaptions. Subsequently,wetrainour
modelwithhigher-levelcaptions,e.g.,medium-lengthseg-
p(Y(ℓ)|X)=
(cid:89)K
p(y(ℓ)|y(ℓ),X,Y(ℓ−1)) (1)
ment descriptions and long-range video summaries. This
k <k strategic progression allows the model to gradually under-
k=1 stand the intricate hierarchical structure inherent in videos
Here,y(ℓ)denotesthelanguagetokenofthecaption,y(ℓ) and maximize the synergy between all hierarchies. More-
k <k
isthesetofprecedingtokens,andY(0) =∅. over, this strategy effectively handles highly imbalanced
training data across different hierarchies. Figure 3 shows
3.3.HierarchicalCurriculumLearning
anoverviewoftheproposedcurriculumlearningstrategy.
Training a recursive video-language model is challenging
3.4.AdditionalSupervisionusingLanguageModels
for several reasons. First, the model must process videos
ofdramaticallydifferentinputlengths(i.e.,fromafewsec- Collecting captioning annotations for hour-long videos is
onds to several hours). Second, there is a significant data time-consuming and costly. Thus, another critical chal-
4Figure 4. Large Language Model Supervision. Given short-
termgroundtruthcaptions, weuseanLLMtogeneratepseudo-
groundtruthannotationsformedium-lengthsegmentdescriptions
andlong-rangevideosummariestoaugmentourtrainingdata.
Figure 3. Hierarchical Curriculum Learning. We gradually
learn the hierarchical structure of the video, starting from short
low-levelcaptionstolonghigh-levelvideosummaries. HierarchyLevel #Samples Avg. Duration
ClipCaption 5.27M 0.96sec
SegmentDescription 17.5K 2.87min
lenge associated with hierarchical video captioning is the
VideoSummary 8.3K 28.46min
scarcityofmanuallyannotatedhierarchicalcaptioningdata,
particularly for medium-length segment descriptions and
Table1.SummaryofEgo4D-HCapdataset.
long-rangevideosummaries. WeleverageLargeLanguage
Models (LLMs) to mitigate this issue. LLMs can effec-
Ego4D[20],thelargestpubliclyavailableegocentricvideo
tively incorporate information from text inputs of varying
dataset. Ego4D videos have several unique features, mak-
lengths, which aligns perfectly with our objective of guid-
ing them ideal for the hierarchical video captioning task.
ing the video model to generate captions across multiple
First,mostvideosinEgo4Dareordersofmagnitudelonger
hierarchies. Motivated by these insights, we use LLMs to
(e.g., several hours) than the traditional video captioning
generate a large number of pseudo-caption annotations for
datasets. Second,egocentricvideostypicallycontaingoal-
medium-lengthandlong-rangevideos(i.e.,ourlasttwohi-
driven and human activities at different hierarchy levels.
erarchies). The process involves two main steps. First,
Third, Ego4D videos capture human behaviors from vari-
given manually annotated hierarchical captions, we fine-
ousscenariossuchascooking,gardening,assembly,etc.
tune an LLM teacher to generate medium-length segment
WhileEgo4Dcomeswithtime-stampedatomiccaptions
descriptions and long-range video summaries from short-
and video-segment descriptions spanning up to 5 minutes,
termclipcaptionsconcatenatedacrossvaryingtemporaldu-
it lacks video-level summaries for longer video durations.
rations. Afterward, we use such LLM-generated pseudo
Toaddressthisissue,weannotateasubsetof8,267Ego4D
ground truth caption data as additional training samples to
videoswithlong-rangevideosummaries,eachspanningup
train Video ReCap (see Figure 4). Our experiments indi-
totwohours. Thisenhancementprovidesathree-levelhier-
catethatsuchpseudogroundtruthdatageneratedbyLLMs
archyofcaptions, makingitaperfect resource forvalidat-
effectively complements manually annotated data and sig-
ingtheeffectivenessofourmodelonthehierarchicalvideo
nificantlyimprovesourmodel’scaptioningability.
captioningtask. InTable1,weprovideadetailedsummary
3.5.ImplementationDetails ofourintroducedEgo4D-HCapsubset.
OurproposedEgo4D-HCapdatasetcontainsvideosthat
We use TimeSformer [10] as our video encoder to ex-
capturediversescenariosinvariouscontexts,suchashouse-
tract features that take an input clip of 4 RGB frames of
holdsettings,outdoorenvironments,workplaces,leisureac-
224×224. WeuseGPT2[41]asourdefaulttext-decoder,
tivities,andmore,totaling127distinctscenarios. Thedis-
withahiddendimensionof768and12transformerblocks.
tributionofthemostcommon50scenariosisillustratedin
WeuseAdamoptimizer[24]withalearningrateof3−5and
Figure 5. The distribution of caption lengths for three hi-
aweightdecayof0.01. Ourtrainingpipelinealsoutilized
erarchy levels in the Ego4D-HCap dataset is illustrated in
cosineschedulingstrategy[33]. Pleaserefertosupplemen-
Figure6. Notably,clipcaptionsaregenerallyshorter,aver-
tarymaterialsforadditionalimplementationdetails.
aging7.74wordspercaption. Incomparison, segmentde-
scriptionsdisplayamediumlength,averaging15.79words,
4.Ego4D-HCapDataset
whilevideo summariesare thelongest, with anaverage of
WenowdescribeourintroducedEgo4D-HCapdataset,ahi- 25.59 words. Additionally, we observe that the maximum
erarchicalvideocaptioningdatasetcomprisedofathree-tier lengthforaclipcaptionis43words,whilesegmentdescrip-
hierarchy of captions: short clip-level captions, medium- tionsandvideosummariescanextendto73and172words,
length video segment descriptions, and long-range video- respectively.Oursupplementarymaterialsincludemorede-
level summaries. To construct Ego4D-HCap, we leverage tailsonthedatasetandourannotationcollectionprocess.
5Figure5.Distributionofthemostcommon50scenariosinEgo4D-HCapdataset.
Figure6.DistributionofthelengthsofthreehierarchicalcaptionsoftheEgo4D-HCapdataset.
5.ExperimentalSetup based baseline that takes LaViLa-generated clip cap-
tions and finetunes a text-only GPT2 model for seg-
5.1.HierarchicalVideoCaptioningBaselines
mentdescriptionandvideosummarygenerationwhile
keepingtheunderlyingLaViLamodelfrozen.
Hierarchical video captioning is a relatively unexplored
2. LaViLa + FLAN-T5 [15, 67]: Similar to the above,
task, so there are no well-established baselines for com-
afully-finetunedtext-basedbaselinethatusesFLAN-
paring our work. Thus, we introduce the following video-
T5ratherthanGPT2forsegmentdescriptionandvideo
languagebaselines,whichweextendforthistask.
summarygeneration.
• Zero-ShotBaselines: 3. LaViLa [67]: A video-based baseline, finetuned
1. BLIP2 [29]: A zero-shot baseline for short-term clip end-to-end to generate short-term captions, medium-
captioning that utilizes a state-of-the-art image cap- length segment descriptions, and long-range video
tioningmodel. summariesdirectlyusingvideoinputs. Notethatthis
2. BLIP2 + GPT3.5 [12, 29]: A zero-shot text-based baseline uses the same video encoder, text decoder,
baseline for video segment descriptions and long- andotherexperimentalsettingsasourmodel.
rangevideosummaries. GivenBLIP2-generatedcap-
tions, it uses GPT3.5 to generate video segment de-
5.2.OurModelVarients
scriptionsandlong-rangevideosummaries.
3. LaViLa + GPT3.5 [12, 67]: Similar to the above, 1. VideoReCap. Thisvariantofourmodelusesashared
a zero-shot baseline for video segment and summary video encoder but separate text decoders and video-
generationusingLaViLacaptionsfedintoGPT3.5. languagealignmentmodulestogeneratecaptionsatdif-
• FinetunedBaselines: ferenthierarchylevels(i.e.,theweightsacrossdifferent
1. LaViLa + GPT2 [41, 67]: A fully-finetuned text- hierarchiesarenotshared). Duetotheincreasedmodel
6Visual Text Train ClipCaption
Model
Encoder Decoder Params CIDEr ROUGE-L METEOR
Zero-Shot
BLIP2[29] VIT-G FT5-XL 0 8.1 7.4 12.7
Finetuned
LaViLa[67] TSF-B GPT2 258M 88.56 47.64 28.03
HierVidCap TSF-B GPT2 339M 98.35 48.77 28.28
HierVidCap-U TSF-B GPT2 113M 92.67 47.90 28.08
(a)Resultsforshort-rangeclipcaptioning.
Video Text Train Pseudo SegmentDescription VideoSummary
Model
Encoder Decoder Params Ann. C R M C R M
Zero-Shot
BLIP2[29]+GPT3.5[12] VIT-G FT5-XL 0 ✗ 5.68 16.87 13.47 11.13 22.41 12.10
LaVila[67]+GPT3.5[12] TSF-B GPT2 0 ✗ 5.79 19.77 13.45 12.16 24.49 12.48
Finetuned
LaVila[67]+GPT2[41] TSF-B GPT2 336M ✗ 38.22 38.10 16.58 17.98 29.48 12.81
LaVila[67]+FLANT5[15] TSF-B FT5-XL 586M ✗ 39.13 38.77 16.88 20.12 30.06 13.17
LaViLa[67] TSF-B GPT2 258M ✗ 24.63 33.31 15.30 6.54 23.97 10.95
HierVidCap TSF-B GPT2 339M ✗ 41.74 39.04 18.21 28.06 32.27 14.26
HierVidCap TSF-B GPT2 339M ✓ 46.88 39.73 18.55 29.34 32.64 14.45
HierVidCap-U TSF-B GPT2 113M ✓ 45.60 39.33 18.17 31.06 33.32 14.16
(b)Resultsformedium-lengthsegmentdescriptionandlong-rangevideosummarygeneration.
Table2.MainResultsontheEgo4D-HCapdataset.AllresultsareevaluatedinstandardCIDEr(C),ROUGE-L(R)andMETEOR(M)
metrics. Weobserveseveralinterestingtrends. First,finetunedmethodsperformsignificantlybetterthanthezero-shotbaselines. Second,
theVideoReCapmodelachievesthebestresultsinvideocaptioningacrossallthreehierarchies,surpassingstrongpriorbaselinessuchas
LaViLa[67].Third,usingLLM-generatedpseudoannotationsleadstoasignificantboostinperformance.Lastly,theunifiedvariantofthe
modelproducescompetitiveresultswhilehavingasignificantlysmallernumberoftrainableparametersthanourstandardvariant.
capacity of having specialized modules for each hierar- CIDEronvideosummarygeneration,despiteusingsignif-
chy,thisvarianttypicallyproducesthebestperformance. icantly more trainable parameters (586M vs 339M). This
2. Video ReCap-U. The unified variant using shared pa- indicatesthebenefitsofusinghierarchicalvideoandtextin-
rameters across all hierarchies. Since it has a lot fewer putsratherthanjusttextforvideosegmentdescriptionand
trainableparametersthanthepreviousvariant,itismore long-range video summary generation. Third, we notice
efficientbutperformsslightlyworseincertainsettings. thatourbestperformingVideoReCapvariantsignificantly
improves upon the strong LaViLa baseline on clip cap-
6.ResultsandAnalysis tioningforEgo4D[20], outperformingitby9.79%CIDEr
whileemployingthesamevisualencoder,textdecoder,and
6.1.HierarchicalVideoCaptioningResults
trainingdataasourmodel. WenotethatwhileLaViLauses
In Table 2, we present our main results for hierarchical a transformer resampler [2, 67], our model utilizes a Lan-
video captioning. We use standard captioning metrics, in- guage Model-based alignment module (see Section 3.2),
cluding CIDEr [54], ROUGE-L [31], and METEOR [6] whichwefoundveryeffectiveforthisparticulartask.
to evaluate our model on the hierarchical video captioning WealsonotethattheperformanceofLaViLadropssig-
task. Based on these results, we observe several interest- nificantlyforsegmentdescriptionandvideosummarygen-
ing trends. First, we note that zero-shot baselines (e.g., eration,indicatingitsinabilitytohandlelong-rangevideos.
BLIP2 [29], BLIP2 + GPT3.5 [12], LaViLa + GPT3.5) In contrast, Video ReCap maintains strong performance
perform considerably worse than the fully finetuned ap- on these longer video inputs, outperforming LaViLa by
proaches (e.g., LaViLa [67], LaViLa + GPT2 [41], LaV- 17.11%CIDEronsegmentdescriptionand21.52%CIDEr
iLa+FLAN-T5[15]), underscoringthesignificanceofin- on video summary generation. We also note that while
domain learning on the Ego4D-HCap dataset. Second, we Video ReCap uses more training parameters than LaV-
observethatthebestperformingfully-finetunedtext-based iLa (258M vs. 339M), Video ReCap-U has significantly
baseline LaViLa + FLAN-T5 [15] falls short of our model fewer training parameters (113M) than LaViLa but still
by 2.61% CIDEr on video segment description and 9.94% outperforms LaViLa by substantial margins (+20.97% and
7Input Ego4D QA Recursive SegmentDescription VideoSummary
Model
Feature Pretrain Acc Input C R M C R M
Random - ✗ 20.0 ✗ 40.17 38.65 17.59 25.64 29.61 13.57
GPT3.5[12] Question ✗ 19.57 ✓ 41.74 39.04 18.21 28.06 32.27 14.26
FrozenBiLM[62] Video ✗ 26.9
VIOLET[19] Video ✗ 19.9 Table4. InportanceofRecursiveInputs. Anon-recursivevari-
antofourmodelperformsworseinsegmentdescriptionandvideo
mPLUG-Owl[65] Video ✗ 31.1
summarygeneration(-1.57%and-2.42%inCIDEr).
InternVideo[59] Video ✗ 32.1
EgoVLP[32] Video ✓ 34.86 comparedtothevariantofourmethodthatusesonlyshort-
EgoVLPv2[40] Video ✓ 34.12 termcaptionsasinputstoGPT3.5,thevariantthatuseshi-
LaViLa[67]+GPT3.5[12] Captions ✓ 44.27 erarchicalvideocaptionsachievesasignificant4.2%boost
VideoReCap+GPT3.5[12] Captions ✓ 46.03 in performance. We also compare our method with a sim-
VideoReCap+GPT3.5[12] Hier.Captions ✓ 50.23 ilar baseline that uses LaViLa-generated short-term cap-
tionsratherthanourhierarchicalvideocaptionsasinputsto
Table 3. Long-Range VideoQA on EgoSchema [35] Our ap- GPT3.5andshowthatourapproachoutperformsthisbase-
proach achieves state-of-the-art results, outperforming the previ- line by 5.96%. This highlights the benefits of hierarchical
ousbestmethod,InternVideo,byasubstantialmarginof18.13%. video cues for long-range videoQA. Our results also indi-
Furthermore,leveragingthehierarchicalcaptionsproducedbyour
catethatourmethodoutperformsthepreviousbestmodel,
modelleadsto4.2%and5.96%boostinperformancecompared
InternVideo[59]byalargemarginof18.13%,settinganew
tousingourmodel’sshort-clipcaptionsorcaptionsgeneratedby
state-of-the-artonthisbenchmark. Wenote, however, that
LaViLa[67]. Thisdemonstratestheefficacyofhierarchicalvideo
sinceInternVideowasneverpretrainedonEgo4D,thecom-
captionsforlong-rangevideoquestionanswering.
parisonwithourapproachmightbesomewhatunfair.Thus,
in our comparisons, we also include two recent methods,
+24.50%inCIDErforsegmentdescriptionandvideosum-
pretrained on Ego4D, EgoVLP [32] and EgoVLPv2 [40].
mary generation respectively). This indicates that the per-
Notethatforallevaluations,weremovedallEgo4Dvideos
formance gain of our model comes from the recursive and
usedbytheEgoSchemabenchmarkfromourtrainingsetto
hierarchical design and not from the larger capacity of the
avoid data leakage. Compared to EgoVLP and EgoVLP2,
model. Our results also indicate that our model’s perfor-
our approach still achieves the best results, outperforming
mance can be further improved (5.14% CIDEr in segment
these two baselines by a significant margin of 16%, indi-
descriptionand1.28%CIDErinvideosummary)byincor-
catingthesuperiorityofourmethod.
poratingLLM-basedsupervision(seeSection3.4). Lastly,
the last two rows of Table 2 highlight the trade-off be-
6.3.AblationStudies
tween the two variants of our model, i.e., Video ReCap
achieves the highest performance across two out of three ImportanceofRecursiveArchitecture.Inthissection,we
hierarchies, while the unified variant, Video ReCap-U, at- analyzethesignificanceoftherecursivearchitectureofour
tains the second-best performance with significantly fewer VideoReCapmodel. Forthisvariant,wediscardtherecur-
trainableparameters. sive inputs (i.e., captions generated in the previous hierar-
chylevel)andcomparetheperformancewithourrecursive
6.2.Long-RangeVideoQAonEgoSchema
model.Table4showstheresult.Weobservethattheperfor-
InTable3,wevalidatetheeffectivenessofourhierarchical manceofanon-recursivevariantdropsby1.57%CIDErfor
video model on the recently introduced long-range video videosegmentdescriptions. Moreover,therecursivemodel
question-answering (VideoQA) EgoSchma dataset [35]. structureisevenmoreimportantforlong-rangevideosum-
EgoSchema contains over 5K human-curated multiple- marygeneration,wheretheperformanceofanon-recursive
choice question-answer pairs spanning 250 hours of real- variant drops by 2.42% CIDEr without it. These experi-
world videos, requiring hierarchical reasoning over long mentsrevealthattherecursivedesignofVideoReCapleads
videos. We use a simple two-stage approach to perform to better performance on the hierarchical video captioning
VideoQA on EgoSchema. First, given long EgoSchema task,particularlyonlong-rangevideoinputs.
video inputs, we generate hierarchical video captions like SignificanceofHierarchicalCurriculumLearningNext,
before.Afterward,wefeedourgeneratedhierarchicalvideo we investigate the significance of our hierarchical curricu-
captions as inputs to a text-only GPT3.5 [12] and prompt lum learning scheme. Table 5 shows the importance of
it to answer a question about a given video in a zero-shot such a curriculum learning scheme. We observe that if
manner. Thesimpleframeworkperformsverywellonthis we directly train our model on the segment description
benchmark despite the simplicity. We first observe that fromGPT2pretrainedinitialization,performancedropsby
8SegmentDescrption VideoSummary SegmentDescription VideoSummary
TrainingScheme Input
C R M C R M C R M C R M
Init→Segment 36.81 38.70 17.17 - - -
Video 40.17 38.65 17.59 25.64 29.61 13.57
Caption→Segment 41.74 39.04 18.21 - - -
Text 40.10 38.02 17.41 23.23 29.17 13.31
Init→Video - - - 8.62 26.33 11.24
Video+Text 41.74 39.04 18.21 28.06 32.27 14.26
Caption→Video - - - 24.84 30.74 13.25
Caption→Segment→Video - - - 28.06 32.27 14.26
Table7.Video-LanguageInputAblation.Usingbothvideoand
Table 5. Hierarchical Curriculum Learning. Using the pro- textfeaturesleadstobetterperformanceforbothsegmentdescrip-
posedcurriculumlearningschemeyieldsaperformanceboostof tionandvideosummarygeneration.
+4.93%insegmentdescriptionand+19.44%inlong-rangevideo
summarygenerationcomparedtotrainingthemodelfromGPT2
servethatFLAN-T5-Largeachievesthebestperformancein
pretrainedweights(Init).
allmetrics. Hence,weuseFLAN-T5-LargeasourTeacher
to generate pseudo-ground truth data for segment descrip-
SegmentDescription VideoSummary tions and long-range video summaries. Specifically, we
LLM
C R M C R M produce100Kpseudo-annotationsforsegmentdescriptions
GPT2 96.47 46.96 23.13 40.06 33.06 14.76 and 15K for video summaries. We combine these pseudo-
GPT2-L 104.30 47.68 23.15 43.18 33.86 15.00 annotationswiththemanuallyannotateddataandtrainour
FLAN-T5-S 95.61 46.16 22.30 43.27 34.19 14.69 model. Table 6b shows that utilizing supervision from
FLAN-T5-L 125.67 50.61 26.06 52.08 36.99 19.93
LLMsprovidesasubstantialperformanceboostinbothseg-
(a)TraininganLLMTeacher. mentdescription(+5.14%CIDErgain)andvideosummary
Pseudo SegmentDescription VideoSummary (+1.28%CIDErimprovement)generationperformance.
Ann. C R M C R M AblationofInputModalities.AsdescribedinSection3.2,
✗ 41.74 39.04 18.21 28.06 32.27 14.26 ourmodelutilizesbothvideofeaturesandtextinputs(gen-
✓ 46.88 39.73 18.55 29.34 32.64 14.45 erated in the previous hierarchy) for the segment descrip-
tions and video summaries. Note that we do not use any
(b)SupervisionUsingthebestLLMTeacher(FLAN-T5-Large).
textinputsforclipcaptionsastheydefinethebasecaseof
Table6. ImportanceofLLMSupervision. Top: Givenground- ourrecursivevideomodel. Sinceweneedtosparselysam-
truth short-term captions concatenated across varying temporal plevideofeaturestofitlong-rangevideosintoGPUmem-
lengths, FLAN-T5-Large generates the highest quality pseudo- ory,wehypothesizethatusingtextasanintermediaterepre-
annotations for segment description and long-range video sum-
sentationshouldcomplementthesparsevideofeatures. Ta-
mary annotations. Using this LLM Oracle, we produce 100K
ble7provesourhypothesisandshowsthatusingbothvideo
pseudo-annotations for medium-length segment descriptions and
and text features as inputs yields the best performance for
15Kforlong-rangevideosummaries.Bottom:CombiningLLM-
ourmodel.Specifically,forsegmentdescriptiongeneration,
generated annotations with manual annotations during training
combining video and text inputs produces a +1.57% boost
leadstoaperformanceimprovementof5.14%CIDErforsegment
descriptionand1.28%CIDErforthevideosummary. relativetovideo-onlyand+1.64%boostrelativetotext-only
baselines in CIDEr. Moreover, for long-range video sum-
mary generation, video + text inputs provide +2.42% and
a significant margin of 4.93% CIDEr. Moreover, the per- +4.83% gains compared to video-only and text-only vari-
formance drop is even more catastrophic (-19.44%) for ants.
videosummarygenerationwithoutcurriculumlearning.Fi-
6.4.QualitativeResultsonEgo4D-HCap
nally, we show that it is useful to progressively incorpo-
ratehigher-levelcaptions,startingfromshort-termcaptions, InFigure7,wepresentthreeinstancesofhierarchicalcap-
then transitioning to medium-length segment descriptions, tions generated by our model. It is evident that clip cap-
andlastly,finishingwithlong-rangevideosummaries. The tions mostly describe atomic actions and objects, such as
variant that progresses from short-term caption to long- ‘Cclosesthetap’(Figure7(a))and‘Cpushesthetrolley’
range video summary learning directly exhibits a 3.22% (Figure 7 (b)). In contrast, segment descriptions focus on
dropinCIDErperformance. intermediateconceptswithinthevideospanninglongerdu-
Importance of LLM-Based Supervision Next, we study rations, i.e., ‘C was in the kitchen, washed utensils’ (Fig-
the importance of LLM-based supervision for medium- ure 7 (a)), and ‘C arranged the tent and interacted with a
length segment descriptions and long-range video sum- woman’(Figure7(c)). Moreover,videosummariesaimto
maries. InTable6a, weshowtheperformanceofdifferent encapsulatetheoverarchingcontentandeventsofthevideo.
LLM Teachers (e.g., GPT2 [41], and FLAN-T5 [15]) that Forexample,‘Cwenttothesupermarket.Cpickedupfruits
we use to generate the pseudo ground truth data. We ob- vegetables,andinteractedwithotherpeople. Cboughtgro-
9(a)
(b)
(c)
Figure7. QualitativeResultsonEgo4D-HCap. Generally,clipcaptionsdepictatomicactionsandobjects;segmentdescriptionsfocus
onintermediateconcepts,andvideosummariesencapsulatetheoverallcontentandgoalsofthevideos.Whilegeneratingclipcaptionsand
segmentdescriptionsareoftenrelativelyeasiertasks,developingagoodvideosummaryisoftenchallenging.Ourmodelsperformwellon
videosummaries(a)and(b),butthegeneratedvideosummary(c)couldbefurtherimproved.
ceriesandpaidatthecashier’(Figure7(b)). segment descriptions is relatively more straightforward,
We also notice that while generating clip captions and generating video summaries is more challenging. For in-
10stance,whilethegeneratedvideosummariesofFigure7(a) [6] SatanjeevBanerjeeandAlonLavie. Meteor: Anautomatic
andFigure7(b)areofgoodquality,thevideosummaryof metricformtevaluationwithimprovedcorrelationwithhu-
Figure7(c)couldbefurtherimproved.Thevideosummary manjudgments. InProceedingsoftheaclworkshoponin-
ofFigure7(c)failstocapturesomeimportanteventsofthe trinsicandextrinsicevaluationmeasuresformachinetrans-
lationand/orsummarization,pages65–72,2005. 7
videoandincludesrepeatedwordsandphrases.Thesechal-
[7] SiddhantBansal,ChetanArora,andCVJawahar.Myviewis
lengeshighlightthecomplexityofsummarizingcontentin
thebestview:Procedurelearningfromegocentricvideos.In
long-rangevideos. Weanticipatethatfutureadvancements
EuropeanConferenceonComputerVision,pages657–675.
and the use of our released data will contribute to the de-
Springer,2022. 3
velopment of more effective methods and models for this
[8] LorenzoBaraldi,CostantinoGrana,andRitaCucchiara. Hi-
demandingtask.
erarchicalboundary-awareneuralencoderforvideocaption-
ing. In Proceedings of the IEEE conference on computer
7.ConclusionsandFutureWork visionandpatternrecognition,pages1657–1666,2017. 2
[9] R.G.BarkerandH.F.Wright.MidwestandItsChildren:The
We introduce Video ReCap a recursive video captioning PsychologicalEcologyofanAmericanTown.Row,Peterson,
model adept at producing hierarchical captions for videos 1954. 1,4
spanning diverse temporal granularities—from brief clip [10] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is
captions to extensive hour-long summaries. The incorpo- space-time attention all you need for video understanding?
ration of a curriculum learning scheme inspired by hu- InICML,page4,2021. 3,5,14
man psychology and an LLM-based supervision strategy [11] Matthew Botvinick and David C Plaut. Doing without
schema hierarchies: a recurrent connectionist approach to
enhances the model’s efficacy in tackling the hierarchical
normalandimpairedroutinesequentialaction. Psycholog-
video captioning problem. Beyond its primary focus, our
icalreview,111(2):395,2004. 1,4
model’shierarchicalcaptionsalsoprovesadvantageousfor
[12] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub-
long-rangevideoquestionanswering. Additionally,thecu-
biah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakan-
ratedEgo4D-HCapdatasetwillbereleased,intendedtocat-
tan,PranavShyam,GirishSastry,AmandaAskell,etal.Lan-
alyze ongoing progress in video understanding research. guagemodelsarefew-shotlearners. Advancesinneuralin-
Some promising future directions include real-time cap- formationprocessingsystems,33:1877–1901,2020. 6,7,8
tiongeneration,interactivevideounderstanding,andvideo- [13] DavidChenandWilliamBDolan. Collectinghighlyparal-
baseddialoguing. leldataforparaphraseevaluation.InProceedingsofthe49th
Acknowledgements. We thank Feng Cheng, Yan-Bo Lin, annualmeetingoftheassociationforcomputationallinguis-
CeZhang,YueYang,andSoumitriChattopadhyayforhelp- tics: human language technologies, pages 190–200, 2011.
3
fuldiscussions. ThisworkwassupportedbytheSonyFac-
[14] SihanChen,XingjianHe,LongtengGuo,XinxinZhu,Wein-
ultyInnovationaward,LaboratoryforAnalyticSciencesvia
ingWang,JinhuiTang,andJingLiu. Valor: Vision-audio-
NCStateUniversity,ONRAwardN00014-23-1-2356.
language omni-perception pretraining model and dataset.
arXivpreprintarXiv:2304.08345,2023. 1
References
[15] Hyung Won Chung, Le Hou, Shayne Longpre, Barret
Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang,
[1] VideoReCapwebpage:https://sites.google.com/view/vidrecap.
Mostafa Dehghani, Siddhartha Brahma, et al. Scaling
1
instruction-finetuned language models. arXiv preprint
[2] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine
arXiv:2210.11416,2022. 6,7,9
Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch,
[16] RichardPCooperandTimShallice. Hierarchicalschemas
KatherineMillican, MalcolmReynolds, etal. Flamingo: a
andgoalsinthecontrolofsequentialbehavior. 2006. 1,4
visual language model for few-shot learning. Advances in
[17] JeffreyDonahue,LisaAnneHendricks,SergioGuadarrama,
Neural Information Processing Systems, 35:23716–23736,
Marcus Rohrbach, Subhashini Venugopalan, Kate Saenko,
2022. 4,7,16,17
andTrevorDarrell. Long-termrecurrentconvolutionalnet-
[3] Kumar Ashutosh, Rohit Girdhar, Lorenzo Torresani, and works for visual recognition and description. In Proceed-
Kristen Grauman. Hiervl: Learning hierarchical video- ingsoftheIEEEconferenceoncomputervisionandpattern
language embeddings. In Proceedings of the IEEE/CVF recognition,pages2625–2634,2015. 2
Conference on Computer Vision and Pattern Recognition,
[18] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
pages23066–23078,2023. 3
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
Neural machine translation by jointly learning to align and vain Gelly, et al. An image is worth 16x16 words: Trans-
translate. arXivpreprintarXiv:1409.0473,2014. 1 formers for image recognition at scale. arXiv preprint
[5] Albert Bandura. Social cognitive theory: An agentic per- arXiv:2010.11929,2020. 2
spective. Asian journal of social psychology, 2(1):21–41, [19] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang
1999. 1,4 Wang,LijuanWang,andZichengLiu.Anempiricalstudyof
11end-to-endvideo-languagetransformerswithmaskedvisual [33] Ilya Loshchilov and Frank Hutter. Decoupled weight de-
modeling. 2023IEEE/CVFConferenceonComputerVision cayregularization. InInternationalConferenceonLearning
andPatternRecognition(CVPR),pages22898–22909,2022. Representations,2017. 5
8 [34] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan
[20] Kristen Grauman, Andrew Westbury, Eugene Byrne, Duan,TianruiLi,JasonLi,TaroonBharti,andMingZhou.
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Univl: Aunifiedvideoandlanguagepre-trainingmodelfor
Hamburger,HaoJiang,MiaoLiu,XingyuLiu,etal. Ego4d: multimodal understanding and generation. arXiv preprint
Aroundtheworldin3,000hoursofegocentricvideo.InPro- arXiv:2002.06353,2020. 1
ceedingsoftheIEEE/CVFConferenceonComputerVision [35] Karttikeya Mangalam, Raiymbek Akshulakov, and Jiten-
andPatternRecognition,pages18995–19012,2022. 2,5,7, dra Malik. Egoschema: A diagnostic benchmark for very
16 long-form video language understanding. arXiv preprint
arXiv:2308.09126,2023. 2,8,17
[21] SeppHochreiterandJu¨rgenSchmidhuber. Longshort-term
memory. Neuralcomputation,9(8):1735–1780,1997. 14 [36] Ron Mokady, Amir Hertz, and Amit H Bermano. Clip-
cap: Clip prefix for image captioning. arXiv preprint
[22] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Ziming Zhang,
arXiv:2111.09734,2021. 17
BretHarsham,JohnRHershey,TimKMarks,andKazuhiko
[37] PingboPan, ZhongwenXu, YiYang, FeiWu, andYueting
Sumi. Attention-basedmultimodalfusionforvideodescrip-
Zhuang.Hierarchicalrecurrentneuralencoderforvideorep-
tion.InProceedingsoftheIEEEinternationalconferenceon
resentationwithapplicationtocaptioning.InProceedingsof
computervision,pages4193–4202,2017. 1,2
theIEEEconferenceoncomputervisionandpatternrecog-
[23] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and
nition,pages1029–1038,2016. 2
RaduSoricut. Multimodalpretrainingfordensevideocap-
[38] YingweiPan,TingYao,HouqiangLi,andTaoMei. Video
tioning. arXivpreprintarXiv:2011.11760,2020. 3
captioningwithtransferredsemanticattributes. InProceed-
[24] Diederik P. Kingma and Jimmy Ba. Adam: A method for ingsoftheIEEEconferenceoncomputervisionandpattern
stochasticoptimization. CoRR,abs/1412.6980,2014. 5,14 recognition,pages6504–6512,2017. 1,2
[25] Atsuhiro Kojima, Takeshi Tamura, and Kunio Fukunaga. [39] WenjiePei,JiyuanZhang,XiangrongWang,LeiKe,Xiaoy-
Naturallanguagedescriptionofhumanactivitiesfromvideo ongShen,andYu-WingTai.Memory-attendedrecurrentnet-
imagesbasedonconcepthierarchyofactions. International workforvideocaptioning. InProceedingsoftheIEEE/CVF
JournalofComputerVision,50:171–184,2002. 2 Conference on Computer Vision and Pattern Recognition,
[26] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and pages8347–8356,2019. 1,2
JuanCarlosNiebles. Dense-captioningeventsinvideos. In [40] Shraman Pramanick, Yale Song, Sayan Nag,
Proceedings of the IEEE international conference on com- Kevin Qinghong Lin, Hardik Shah, Mike Zheng Shou,
putervision,pages706–715,2017. 3 Rama Chellappa, and Pengchuan Zhang. Egovlpv2:
[27] WeiyuLan,XirongLi,andJianfengDong. Fluency-guided Egocentric video-language pre-training with fusion in the
cross-lingualimagecaptioning. InProceedingsofthe25th backbone. In Proceedings of the IEEE/CVF International
ACMinternationalconferenceonMultimedia,pages1549– ConferenceonComputerVision,pages5285–5297,2023. 8
1557,2017. 2 [41] AlecRadford,JeffreyWu,RewonChild,DavidLuan,Dario
Amodei, Ilya Sutskever, et al. Language models are unsu-
[28] JieLei,LiweiWang,YelongShen,DongYu,TamaraLBerg,
pervisedmultitasklearners.OpenAIblog,1(8):9,2019.3,5,
and Mohit Bansal. Mart: Memory-augmented recurrent
6,7,9,14
transformerforcoherentvideoparagraphcaptioning. arXiv
[42] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket
preprintarXiv:2005.05402,2020. 1,2
Tandon,ChristopherPal,HugoLarochelle,AaronCourville,
[29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
andBerntSchiele.Moviedescription.InternationalJournal
Blip-2: Bootstrapping language-image pre-training with
ofComputerVision,123:94–120,2017. 3
frozen image encoders and large language models. arXiv
[43] MarcusRohrbach,WeiQiu,IvanTitov,StefanThater,Man-
preprintarXiv:2301.12597,2023. 2,3,6,7,17
fredPinkal,andBerntSchiele. Translatingvideocontentto
[30] Linjie Li, Yen-Chun Chen, Yu Cheng, Zhe Gan, Licheng
naturallanguagedescriptions.InProceedingsoftheIEEEin-
Yu, and Jingjing Liu. Hero: Hierarchical encoder for
ternationalconferenceoncomputervision,pages433–440,
video+language omni-representation pre-training. In Con-
2013. 1,2
ferenceonEmpiricalMethodsinNaturalLanguageProcess-
[44] Victor Sanh, Lysandre Debut, Julien Chaumond, and
ing,2020. 3
ThomasWolf.Distilbert,adistilledversionofbert:Smaller,
[31] Chin-YewLin. Rouge: Apackageforautomaticevaluation faster, cheaper and lighter. arxiv 2019. arXiv preprint
of summaries. In Text summarization branches out, pages arXiv:1910.01108,2019. 3,14,16,17
74–81,2004. 7 [45] Fadime Sener, Dibyadip Chatterjee, Daniel Shelepov, Kun
[32] KevinQinghongLin,JinpengWang,MattiaSoldan,Michael He, DipikaSinghania, RobertWang, andAngelaYao. As-
Wray,RuiYan,EricZXU,DifeiGao,Rong-ChengTu,Wen- sembly101: A large-scale multi-view video dataset for un-
zhe Zhao, Weijie Kong, et al. Egocentric video-language derstanding procedural activities. In Proceedings of the
pretraining.AdvancesinNeuralInformationProcessingSys- IEEE/CVF Conference on Computer Vision and Pattern
tems,35:7575–7586,2022. 8 Recognition,pages21096–21106,2022. 3
12[46] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and qualitymultilingualdatasetforvideo-and-languageresearch.
CordeliaSchmid.End-to-endgenerativepretrainingformul- In Proceedings of the IEEE/CVF International Conference
timodalvideocaptioning. InProceedingsoftheIEEE/CVF onComputerVision,pages4581–4591,2019. 3
Conference on Computer Vision and Pattern Recognition, [59] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun
pages17959–17968,2022. 1,2 Huang,ZhiyuZhao,HongjieZhang,JilanXu,YiLiu,Zun
[47] Jingkuan Song, Yuyu Guo, Lianli Gao, Xuelong Li, Alan Wang,etal. Internvideo: Generalvideofoundationmodels
Hanjalic,andHengTaoShen.Fromdeterministictogenera- via generative and discriminative learning. arXiv preprint
tive:Multimodalstochasticrnnsforvideocaptioning. IEEE arXiv:2212.03191,2022. 8
transactions on neural networks and learning systems, 30 [60] JunXu,TaoMei,TingYao,andYongRui. Msr-vtt:Alarge
(10):3047–3058,2018. 2 videodescriptiondatasetforbridgingvideoandlanguage.In
[48] Yale Song, Gene Byrne, Tushar Nagarajan, Huiyu Wang, ProceedingsoftheIEEEconferenceoncomputervisionand
MiguelMartin,andLorenzoTorresani.Ego4dgoal-step:To- patternrecognition,pages5288–5296,2016. 3
wardhierarchicalunderstandingofproceduralactivities. In [61] RanXu,CaimingXiong,WeiChen,andJasonCorso.Jointly
Thirty-seventh Conference on Neural Information Process- modelingdeepvideoandcompositionaltexttobridgevision
ingSystemsDatasetsandBenchmarksTrack,2023. 3 andlanguageinaunifiedframework. InProceedingsofthe
[49] ChenSunandRamNevatia.Semanticawarevideotranscrip- AAAIconferenceonartificialintelligence,2015. 2
tion using random forest classifiers. In Computer Vision– [62] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev,
ECCV 2014: 13th European Conference, Zurich, Switzer-
and Cordelia Schmid. Zero-shot video question answer-
land, September6-12, 2014, Proceedings, Part I13, pages ing via frozen bidirectional language models. ArXiv,
772–786.Springer,2014. 2 abs/2206.08155,2022. 8
[50] Chen Sun, Austin Myers, Carl Vondrick, Kevin Murphy,
[63] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, An-
and Cordelia Schmid. Videobert: A joint model for video
toineMiech,JordiPont-Tuset,IvanLaptev,JosefSivic,and
and language representation learning. In Proceedings of
CordeliaSchmid. Vid2seq: Large-scalepretrainingofavi-
theIEEE/CVFinternationalconferenceoncomputervision,
suallanguagemodelfordensevideocaptioning.InProceed-
pages7464–7473,2019. 1
ingsoftheIEEE/CVFConferenceonComputerVisionand
[51] IlyaSutskever,OriolVinyals,andQuocVLe. Sequenceto PatternRecognition,pages10714–10726,2023. 1,2
sequencelearningwithneuralnetworks. Advancesinneural
[64] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas,
informationprocessingsystems,27,2014. 1,2
ChristopherPal,HugoLarochelle,andAaronCourville.De-
[52] Yansong Tang, Dajun Ding, Yongming Rao, Yu Zheng,
scribingvideosbyexploitingtemporalstructure.InProceed-
DanyangZhang,LiliZhao,JiwenLu,andJieZhou. Coin:
ings of the IEEE international conference on computer vi-
Alarge-scaledatasetforcomprehensiveinstructionalvideo
sion,pages4507–4515,2015. 2
analysis. In Proceedings of the IEEE/CVF Conference
[65] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,
on Computer Vision and Pattern Recognition, pages 1207–
Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng Shi, Yaya
1216,2019. 3
Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng
[53] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
Tian, Qiang Qi, Ji Zhang, and Feiyan Huang. mplug-owl:
reit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
Modularizationempowerslargelanguagemodelswithmul-
Polosukhin. Attentionisallyouneed. Advancesinneural
timodality. ArXiv,abs/2304.14178,2023. 8
informationprocessingsystems,30,2017. 2
[66] BowenZhang,HexiangHu,andFeiSha. Cross-modaland
[54] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi
hierarchicalmodelingofvideoandtext. InEuropeanCon-
Parikh. Cider: Consensus-basedimage descriptionevalua-
ferenceonComputerVision,2018. 3
tion. In Proceedings of the IEEE conference on computer
[67] Yue Zhao, Ishan Misra, Philipp Kra¨henbu¨hl, and Rohit
visionandpatternrecognition,pages4566–4575,2015. 7
Girdhar. Learning video representations from large lan-
[55] Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Don-
guagemodels. InProceedingsoftheIEEE/CVFConference
ahue, RaymondMooney, TrevorDarrell, andKateSaenko.
on Computer Vision and Pattern Recognition, pages 6586–
Sequence to sequence-video to text. In Proceedings of the
6597,2023. 2,4,6,7,8,14,16,17
IEEE international conference on computer vision, pages
[68] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards
4534–4542,2015. 2
automatic learning of procedures from web instructional
[56] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Recon-
videos. InProceedingsoftheAAAIConferenceonArtificial
struction network for video captioning. In Proceedings of
Intelligence,2018. 3
theIEEEconferenceoncomputervisionandpatternrecog-
[69] Dimitri Zhukov, Jean-Baptiste Alayrac, Ramazan Gokberk
nition,pages7622–7631,2018. 2
Cinbis,DavidFouhey,IvanLaptev,andJosefSivic. Cross-
[57] JunkeWang,DongdongChen,ZuxuanWu,ChongLuo,Lu-
task weakly supervised learning from instructional videos.
owei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang
In Proceedings of the IEEE/CVF Conference on Computer
Jiang, and Lu Yuan. Omnivl: One foundation model for
VisionandPatternRecognition,pages3537–3545,2019. 3
image-languageandvideo-languagetasks.Advancesinneu-
ralinformationprocessingsystems,35:5696–5710,2022. 1
[58] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang
Wang,andWilliamYangWang. Vatex: Alarge-scale,high-
13Video ReCap: Recursive Captioning of Hour-Long Videos
Supplementary Material
Our supplementary materials contain Section S1: Ad- captionswithinthevideosat4-secondintervals. Then,dur-
ditional Implementation Details, Section S2: Ego4D- ingthesecondstage,wetrainthemodelfor10epochsusing
HCap Data Collection Process, Section S3: Additional abatchsizeof32usingsegmentdescriptionsamples,which
QuantitativeResults,andSectionS4: QualitativeResults. takeasinputbothvideofeaturesandtextfeatures(clipcap-
tions). Finally, in the third stage, we extract segment de-
S1.AdditionalImplementationDetails scriptionseverythreeminutesofthevideousingthetrained
model of the second stage and further train the model for
Figure S1 Shows the schematic diagram of the proposed 10 epochs using a batch size of 32 using video summary
VideoReCapmodel. data. We use AdamW optimizer with optimizer [24] with
Video Encoder. We employ the TimeSformer model [10] (β ,β ) = (0.9,0.999) and weight decay 0.01. We use a
1 2
as our video encoder. This model, consisting of 12 trans- learningrateof3−5andacosineschedulingstrategy.
former layers, is pretrained using a contrastive objec- Training the Video ReCap-U Model. Training a unified
tive[67].Theinputtotheencodercomprises4RGBframes model that shares all parameters across three hierarchies
of size 224×224. To process the video, we divide it into is more challenging. We employ a similar three-stage ap-
4-second clips and extract features for each clip using the proach with some additional tricks. In particular, the first-
pretrained video encoder. For clip caption, we utilize the stagetrainingisidenticaltotheVideoReCapmodel. How-
dense spatiotemporal features. This allows our model to ever, during the second stage, we train the Video ReCap-
capturefine-graineddetails. However,weonlyusetheCLS U model using both clip captions and segment description
featuresforsegmentdescriptionandvideosummary,allow- samplestopreventcatastrophicforgettingofclipcaptions.
ingefficientcomputation. One particular challenge is that the clip captions and seg-
Video-Language Alignment. We utilize a pretrained lan- ment description data are quite different. While clip cap-
guagemodelDistilBERT[44]asourVideo-Language(VL) tions use dense spatiotemporal features, segment descrip-
Alignment module. It is a 6-layer transformer encoder tionsutilizeCLSfeatures. Moreover,segmentdescriptions
model, where we freeze the self-attention blocks and in- use video and text features as inputs, while clip captions
sertatrainablecross-attentionmoduleinsideeachlayer. It only use video features. To overcome this challenge, we
takes video features output by the video encoder and cap- employ an alternate batching pipeline, where we sample
tions generated at the previous hierarchy as inputs. Note a batch of clip captions and segment descriptions alterna-
thattherearenotextinputsforclipcaptions. Forsegment tively during the training. Since we have a lot more clip
description, we extract clip captions at each 4 seconds of caption data (∼ 4M) compared to segment descriptions
the segment, and for video summary, we extract segment (100K including manually annotated and LLM-generated
descriptionsateach3minutesofthevideoandpassthemto pseudo annotations), we randomly sample 100K clip cap-
the VL alignment module along with corresponding video tionsandonlyusedthoseduringthesecondstageoftrain-
features. ing. Finally,wetrainthemodelduringthethirdstageusing
TextDecoder. WeleverageapretrainedGPT2[41])asour samplesfromallthreehierarchiesusingasimilaralternate
text decoder. It is a 12-layer transformer model, and we batching approach. Since we have only ∼ 20K (includ-
insertagatedcross-attentionblockinsideeachtransformer ing manually annotated and LLM-generated pseudo anno-
layer. Wetrainonlythecross-attentionmodulesandfreeze tations)samplesforvideosummaries,werandomlysample
therestofthemodel. Eachcross-attentionblockcontainsa 20K clipcaptionsand20Ksegmentdescriptionsandused
cross-attentionlayerandafeed-forwardlayer,followedby those along with video summaries during the third stage
atanhgating[21].Thetanh-gatingisinitializedwithanini- of training. This strategy prevents catastrophic forgetting
tial value of zero so that the model’s output is the same as of the model. It allows the training of the Video ReCap-
thepre-trainedLLMatthebeginning. Asthetrainingpro- U model, which shares all parameters across hierarchies.
gresses, the model gradually learns to attend to the video- For Video ReCap-U, We use the same learning rate, batch
textembeddingoutputbytheVL-alignmentmodule. size,trainingepoch,optimizer,andschedulerfortheVideo
Training the Video ReCap Model. We follow a three- ReCap(Seethepreviousparagraph).
stagetrainingpipelinefortheVideoReCapmodel.First,we Inference. During inference, we uniformly sample 4
trainourmodel5epochusingabatchsizeof128usingclip frames from the corresponding clip and extract spatiotem-
captiondata,whichonlyusesvideofeatures.Afterward,we poral features using the video encoder to use as inputs to
employthetrainedmodelfromthefirststagetoextractclip generateclipcaptions. Forsegmentdescription,weextract
14FigureS1.ModelArchitecture.
CLSfeaturesandclipcaptionsevery4secondsoftheseg- ter. FigureS2showsourdatacollectioninterface.
ment and use them as inputs to generate segment descrip-
S2.1.GuidelinesforAnnotators
tions. Lastly, we extract segment descriptions at each 3
minutesofthevideoandusethemalongwithpre-extracted
Overview. Inthisproject, weaimtodevelopamodelthat
CLS features to generate video summaries. Note that clip
canautomaticallysummarizelongvideos. Ourmodelgen-
boundaries are not given during the inference of segment
eratestextcaptionsforeachvideodescribingwhathappens
descriptions, and segment boundaries are not given during
every 3 minutes. We need your help to summarize those
theinferenceofvideosummaries.
captions into a summary for the entire video. The total
Wewillreleaseourcode,data,andpretrainedmodels.
lengthofavideocanbebetween10and100minutes.
Captions.
S2.Ego4D-HCapDataCollectionProcess
1. Youaregivenalistofcaptionsforeachvideo.
2. Each caption describes what is happening every 3 min-
TheEgo4D-HCapdatasetwascollectedoverthespanof2
utes.
months,fromApril2023toMay2023andfromSeptember
3. Creferstoapersonintheprovidedcaptions.
2023toOctober2023. Werecruited91specializedannota-
4. The captions are generated using a machine learning
tors through CloudResearch1, a participant-sourcing com-
model,sosometimes,theycanbeoutoforderorinaccu-
pany. AllannotatorsarebasedintheUnitedStatesandare
rate. In that case, you can exclude the events or details
compensatedatarateof9dollarsperhour,whichisabove
that do not make sense in the summary or refer to the
thenationalminimumwage.
GIFprovidedunderthecaptions.
WeutilizedQualtricsandGoogleDrivetobuildourdata
5. Thecaptionsmayalsousedifferenttermstorefertothe
collectioninterface. Ourinterfacebeganwithanintroduc-
same thing. If only technical terms are used, then use
tion to our project, guidelines for summarizing the videos,
theminyoursummary. Otherwise,wepreferyoutouse
and examples of good summaries. It then asked the an-
genericterms.
notators for their ConnectID and provided them a link to
GIFs.
thedocumentsofvideosassignedtothem. Eachdocument
1. Sincethevideosareverylong,wedonotprovidethefull
would contain 10-25 videos for the annotators to summa-
video. Instead,youarealsogivenaGIFforeachvideo.
rize,alongwithapromptandaGIFsummarizingtheevents
2. GIFscreatedbysparselysampledframesfromthevideo,
of each video. The last interfaces contain text boxes for
whichisintendedtohelpyoubetterunderstandtheover-
theannotatorstoputthetextsummariesforeachvideoand
allcontentsofthevideoalongwiththecaptions.
theannotator’sexperiencewiththedatacollectioninterface.
Summaries.
Weusedthelattertoimproveupontheinterfacesothatthe
1. The summary should be one paragraph long. Try to
qualityoftheannotatedsummariesultimatelybecamebet-
maintain a compression factor of 5, i.e., for every five
1https://www.cloudresearch.com captions, youshouldsummarizeitin1sentence. How-
15FigureS2.DataCollectionInterface.
ever,eachsummaryshouldbeatleastonesentence. notators.
2. The summary should cover the setting, characters, and
S2.3.De-identificationProcess
eventsthattakeplaceintheorderofthevideo.
3. Avoid using X, Y or other letters to refer to characters
Due to the nature of the dataset and our task, our dataset
other than C. Instead, use woman and man. Refer to
has already been de-identified. Since all of our videos are
examplesofgoodsummariesonthenextpage.
sourced from Ego4D, they have undergone sensitive ob-
4. The summary should not have an interpretation of the
ject detection, false positive removal, fast negative correc-
characters’personalitiesorqualities.
tion, and image blurring [20]. They were not modified
5. The summary should be logically coherent, unambigu-
duringthedatasetcollectionprocess, sothevideosremain
ous,andunderstandable.
de-identified. Our annotators are also anonymized, as we
6. Thesummaryshouldbegrammaticallycorrect.
recruited, managed, and corresponded with annotators on
7. Repetition of actions should have an underlying pur-
CloudResearch. Aside from their ConnectID, which we
pose/pattern.
used to revise annotations, we did not collect any of the
annotators’personalinformation.
S2.2.QualityControl
S2.4.ExampleVideoSummaries.
To control the quality of the annotations, we pre-selected
annotators before moving them forward with the official
Figure S3 Shows examples of annotated video summaries
annotation task and manually reviewed the annotations.
of the Ego4D-HCap dataset. We observe that video sum-
Before the official annotation task, we paid 171 annota-
mariesareofvariouslengthsandcapturediversescenarios,
torstocompleteapreliminaryannotationtaskandselected
places, and activities. Typically, each video is annotated
from this pool annotators who provided desirable annota-
with multiple summaries. However, the figure shows only
tion quality. We minimized the chances of getting low-
onesummarypervideoforclarityandconciseness.
qualityannotationsbypre-selectinghigh-qualityannotators
and familiarizing them with an interface similar to the ac-
S3.AdditionalQuantitativeResults
tualannotationtask.
Another quality control method we utilized was to re- Backbone Design. In this section, we ablate various as-
viewtheannotationsourselvesmanually. Foreachannota- pects of our Video-Language Backbone design. First,
tor,werandomlysampledhalfoftheannotationstheypro- we validate the effectiveness of a Language Model-based
vided. Weassessedtheirqualitybasedonwhethertheyfol- (LM)[44]Video-LanguageAlignmentmoduleratherthana
lowedtheexpectationsoutlinedinSectionS2.1.Iflessthan standardTransformerresamplerusedinpriorworks[2,67].
halfofthesampledannotationsareoflowquality,wewould Table S1 shows that an LM-based Alignment module per-
provideannotatorfeedbackandaskthemtoredotheiranno- forms significantly better than the standard transformer-
tations. Iftheannotationswereofbetterquality,wewould based resampler in all three hierarchies. Second, we in-
replace them with the initial annotation. Otherwise, we ject trainable cross-attention layers [2, 67] in the text de-
would discard both versions and assign them to other an- coder to incorporate video features. In contrast, several
16LM Trainable ClipCaption SegmentDescription VideoSummary
Alignment CA C R M C R M C R M
✗ ✓ 92.56 47.64 28.03 39.41 38.62 17.71 23.04 28.33 13.72
✓ ✗ 73.88 43.17 21.67 32.16 31.67 13.33 12.16 21.06 8.22
✓ ✓ 98.35 48.77 28.28 41.74 39.04 18.21 28.06 32.27 14.26
TableS1. ArchitectureAblation. AnLM-based[44]VideoLanguageAlignmentmoduleprovidessignificantperformancegainscom-
pared to the transformer-based resampler used in prior works [2, 67]. Adding trainable cross-attention layers inside the text decoder
performsmuchbetterthanfreezingthedecoder.
prior works [29, 36] inject video features only in the in-
put layer while freezing the whole text decoder. Table S1
showsthatusingtrainablecross-attentionlayersinthetex-
tualdecoderperformssignificantlybetterthanusingvideo
featuresintheinputlayeraloneacrossallthreehierarchical
levels.
S4.QualitativeResultsonEgoSchema
Figure S4 illustrates the qualitative outcomes of our
long-range video question answering experiment on the
EgoSchema [35] dataset. The approach, detailed in Sec-
tion 6.2, involves the generation of hierarchical captions
utilizingtheVideoReCapmodelforvideos. Subsequently,
these captions are presented to ChatGPT along with ques-
tionsandanswerchoicesasprompts,enablingthemodelto
selectthecorrectanswer.InFigureS4(a)andFigureS4(b),
itisevidentthatChatGPTtendstochooseincorrectanswers
when provided solely with clip captions. However, the
model consistently makes correct choices in both scenar-
ios when supplemented with video summaries. This high-
lightstheefficacyofourgeneratedhierarchicalcaptionsin
enhancing the performance of long-range video question
answering tasks. Nevertheless, in certain instances, as de-
pictedinFigureS4(c),ourapproachencounterschallenges
andfailstoidentifythecorrectanswer.
17FigureS3. ExamplesofannotatedvideosummariesoftheEgo4D-HCapdataset. Duetospacelimitationandconciseness,weshow
oneframeforeach5minutesofthevideo..
18(a)
(b)
(c)
FigureS4. QualitativeResultsonEgoSchema. Thebaselinemethodthatusesonlyshort-rangeclipcaptionsasinputfailsinexamples
(a)and(b),whereourapproachsucceedsbyutilizinghierarchicalcaptions(i.e.,clipcaptionsandvideosummaries). Bothmodelsfailin
Example(c).
19