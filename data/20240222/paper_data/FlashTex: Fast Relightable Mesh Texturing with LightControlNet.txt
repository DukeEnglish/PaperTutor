FlashTex: Fast Relightable Mesh Texturing with LightControlNet
KangleDeng2* TimothyOmernick1 AlexanderWeiss1 DevaRamanan2
Jun-YanZhu2 TinghuiZhou1 ManeeshAgrawala1,3
1Roblox 2CarnegieMellonUniversity 3StanfordUniversity
Generated Relightable Texture Generated Relightable Texture
Input Mesh View 1 View 2 Input Mesh View 1 View 2
Lighting 1 Lighting 2 Lighting 1 Lighting 2 Lighting 1 Lighting 2 Lighting 1 Lighting 2
“Futuristic Helmet” “Horse Saddle”
“Fariy Lantern” “Hiking Boot”
Figure1.Weproposeanefficientapproachfortexturinganinput3Dmeshgivenauser-providedtextprompt.Notably,ourgeneratedtexture
canberelitproperlyindifferentlightingenvironments.Weshowtwoviewpointsrenderedwithtwodifferentlightingconditions.
Abstract andindustrialdesign.Traditionalmeshtexturingtoolsare
Manually creating textures for 3D meshes is time- labor-intensive,andrequireextensivetraininginvisualde-
consuming, even for expert visual content creators. We sign.Asthedemandforimmersive3Dcontentcontinuesto
proposeafastapproachforautomaticallytexturinganin- surge,thereisapressingneedtostreamlineandautomate
put3Dmeshbasedonauser-providedtextprompt.Impor- themeshtexturingprocess.
tantly,ourapproachdisentangleslightingfromsurfacema- Inthepastyear,significantprogressintext-to-imagedif-
terial/reflectance in the resulting texture so that the mesh fusionmodels[37,39,40]hascreatedaparadigmshiftin
canbeproperlyrelitandrenderedinanylightingenviron- howartistscreateimages.Thesemodelsallowanyonewho
ment. We introduce LightControlNet, a new text-to-image candescribeanimageinatextprompttogenerateacorre-
modelbasedontheControlNetarchitecture,whichallows spondingpicture.Morerecently,researchershaveproposed
the specification of the desired lighting as a conditioning techniquesforleveragingsuch2Ddiffusionmodelsforauto-
image to the model. Our text-to-texture pipeline then con- maticallygeneratingtexturesforaninput3Dmeshbasedon
structsthetextureintwostages.Thefirststageproducesa auser-specifiedtextprompt[7,8,25,38].Butthesemethods
sparsesetofvisuallyconsistentreferenceviewsofthemesh suffer from three significant limitations that restrict their
usingLightControlNet.Thesecondstageappliesatexture wide-spreadadoptionincommercialapplications:(1)slow
optimization based on Score Distillation Sampling (SDS) generation speed (taking tens of minutes per texture), (2)
thatworkswithLightControlNettoincreasethetexturequal- potentialvisualartifacts(e.g.,seams,blurriness,lackofde-
itywhiledisentanglingsurfacematerialfromlighting.Our tails),and(3)baked-inlightingcausingvisualinconsistency
pipelineissignificantlyfasterthanprevioustext-to-texture innewlightingenvironments(Figure2).Whilesomerecent
methods,whileproducinghigh-qualityandrelightabletex- methodsaddressoneortwooftheseissues,noneadequately
tures.Projectpage:https://flashtex.github.io/. addressallthree.
In this work, we propose an efficient approach for tex-
turing an input 3D mesh based on a user-provided text
1.Introduction
prompt that disentangles the lighting from surface mate-
rial/reflectancetoenablerelighting(Figure1).Ourmethod
Creating high-quality textures for 3D meshes is crucial
introducesLightControlNet,anillumination-awaretext-to-
acrossindustriessuchasgaming,film,animation,AR/VR,
imagediffusionmodelbasedontheControlNet[57]archi-
*WorkdonewheninterningatRoblox. tecture,whichallowsspecificationofthedesiredlightingas
4202
beF
02
]RG.sc[
1v15231.2042:viXra(a) Mesh (b) Reference Lighting Composite RGB Diffuse Specular Composite RGB Diffuse Specular
(c) Fantasia3D Texture (d) Our Texture
Figure2.Givena3Dmeshofahelmet(a)andalightingenvironmentL,thereferencerendering(b)depictsthe“correct”highlightson
themeshduetoL,bytreatingitssurfacereflectanceashalf-metalandhalf-smoothwithagraydiffusecolor.(c)Thetexturegeneratedby
Fantasia3D[8]isnotproperlyrelitforthelightingLbecauseFantasia3Dbakesmostofthelightingintothediffusetextureforthemeshand
doesnotcapturethebrighthighlightsinthespeculartexture.(d)Incontrast,ourtext-to-texturepipelinedisentangleslightingfrommaterial,
bettercapturingthediffuseandspecularcomponentsofthemetalhelmetinthisenvironment.
aconditioningimageforthediffusionmodel.Ourtext-to- thesis[23,42].Forinstance,DreamFusion[35]andScore
texturepipelineusesLightControlNettogeneraterelightable JacobianChaining[49]werethefirsttoproposeScoreDis-
texturesintwostages.Instage1,weusemulti-viewvisual tillation Sampling to optimize a 3D representation using
promptingincombinationwiththeLightControlNettopro- 2Ddiffusionmodelgradients.Zero-1-to-3[23]synthesizes
ducevisuallyconsistentreferenceviewsofthe3Dmeshfora novelviewsusingapose-conditioned2Ddiffusionmodel.
smallsetofviewpoints.Instage2,weperformanewtexture Yet, these methods often produce blurry, low-frequency
optimizationprocedurethatusesthereferenceviewsfrom textures that bake lighting into surface reflectance. Fanta-
stage1asguidance,andextendsScoreDistillationSampling sia3D[8]cangeneratemorerealistictexturesbyincorporat-
(SDS)[35]toworkwithLightControlNet.Thisallowsusto ingphysics-basedmaterials.However,theresultingmaterials
increasethetexturequalitywhiledisentanglingthelighting remainentangledwithlighting,makingitdifficulttorelight
fromsurfacematerial/reflectance.Weshowthattheguidance thetexturedobjectinanewlightingenvironment.Incon-
fromthereferenceviewsallowsouroptimizationtogenerate trast,ourmethodcaneffectivelydisentanglethelightingand
textureswithover10xspeed-upthanpreviousSDS-basedre- surfacereflectancetexture.Concurrenttoourwork,MAT-
lightabletexturegenerationmethodssuchasFantasia3D[8]. LABER[55]aimstorecovermaterialinformationintext-
Furthermore,ourexperimentsshowthatthequalityofour to-3Dgenerationusingamaterialautoencoder.Ourmethod,
texturesisgenerallybetterthanthoseofpreviousbaseline however,differsinapproachandimprovesefficiency.
methodsintermsofFID,KID,anduserevaluations. 3DTextureGeneration.Theareaof3Dtexturegeneration
hasevolvedovertime.Earliermodelseitherdirectlytook
2.RelatedWork 3Drepresentationsasinputtoneuralnetworks[4,44,56]or
usedthemastemplates[32,34].Whilesomemethodsalso
Text-to-ImageGeneration.Recentyearshaveseensignifi- usedifferentiablerenderingtolearnfrom2Dimages[4,12,
cantadvancementsintext-to-imagegenerationempowered 34,56],thelearnedmodelsoftenfailtogeneralizebeyond
bydiffusionmodels[37,39,40].StableDiffusion[39],for thelimitedtrainingcategories.
example,trainsalatentdiffusionmodel(LDM)onthelatent Closest to our work are the recent works that use pre-
spaceratherthanpixelspace,deliveringhighlyimpressive trained 2D diffusion models and treat texture generation
resultswithrelativelyaffordablecomputationalcosts.Fur- asabyproductoftext-to-3Dgeneration.Examplesinclude
ther extending the scope of text-based diffusion models, Latent-Paint[25],whichusesScoreDistillationSamplingin
works such as GLIGEN [19], PITI [50], T2IAdapter [27], latentspace,Text2tex[7],whichleveragesdepth-based2D
andControlNet[57]incorporatespatialconditioninginputs ControlNet,andTEXTure[38],whichexploitsbothprevious
(e.g.,depthmaps,normalmaps,edgemaps,etc.)toenable methods.Nonetheless,similartorecenttext-to-3Dmodels,
localizedcontroloverthecompositionoftheresult.Beyond suchmethodsproducetextureswithentangledlightingef-
theirpowerinimagegeneration,these2Ddiffusionmodels, fects and suffer from slow generation. On the other hand,
trainedonlarge-scaletext-imagepaireddatasets,alsocon- TANGO[9],generatesmaterialtexturesusingaSpherical-
tributevaluablepriorstovariousothertaskssuchasimage Gaussian-baseddifferentiablerenderer,butstruggleswith
editing[13,24],recognition[54],and3Dgeneration[35]. complextexturegeneration.
Text-to-3DGeneration.Thesuccessoftext-to-imagegener- MaterialGeneration.BidirectionalReflectionDistribution
ationhassparkedconsiderableinterestinits3Dcounterpart. Function(BRDF)[31]iswidelyusedformodelingsurface
Someapproaches[17,30,43,59]trainatext-conditioned materialsincomputervisionandgraphics.Techniquesfor
3Dgenerativemodelakinto2Dmodels,whileothersem- recoveringmaterialinformationfromimagesoftenleverage
ploy2Dpriorsfrompre-traineddiffusionmodelsforopti- neural networks to resolve the inherent ambiguities when
mization[8,18,21,25,35,45,49,52]andmulti-viewsyn- appliedtoalimitedrangeofviewanglesorunknownillu-Stage 1: Multi-view Visual Prompting Text Prompt: “Doc Martens Boot” Noise Text Prompt
LightControlNet
<latexit sha1_base64="f/Sa/qgRNHnShoVeTrzcFtybX0I=">AAACAXicbVDLSsNAFJ34rPUVdSO4CRbBVUnE17KoCxcuKrUPaEKYTCft0MmDmRuxhLjxV9y4UMStf+HOv3HSZqGtBy4czrmXe+/xYs4kmOa3Nje/sLi0XFopr66tb2zqW9stGSWC0CaJeCQ6HpaUs5A2gQGnnVhQHHictr3hZe6376mQLArvYBRTJ8D9kPmMYFCSq+/aAYYBwTy9ydzUBvoAaeOqkWWuXjGr5hjGLLEKUkEF6q7+ZfcikgQ0BMKxlF3LjMFJsQBGOM3KdiJpjMkQ92lX0RAHVDrp+IPMOFBKz/AjoSoEY6z+nkhxIOUo8FRnfq+c9nLxP6+bgH/upCyME6AhmSzyE25AZORxGD0mKAE+UgQTwdStBhlggQmo0MoqBGv65VnSOqpap9WT2+NK7aKIo4T20D46RBY6QzV0jeqoiQh6RM/oFb1pT9qL9q59TFrntGJmB/2B9vkDVbeXeQ==</latexit>
UNet LSDS
Rendered
LightControlNet
Image
<latexit sha1_base64="U/XjRx2zjZ+r5CtHv5zfCxRnhTs=">AAACA3icbVDLSsNAFJ3UV62vqDvdBIvgqiTia1l048JFBfuAJoTJdNoOnUzCzI1YQsCNv+LGhSJu/Ql3/o2TNgttPTBwOOde5p4TxJwpsO1vo7SwuLS8Ul6trK1vbG6Z2zstFSWS0CaJeCQ7AVaUM0GbwIDTTiwpDgNO28HoKvfb91QqFok7GMfUC/FAsD4jGLTkm3tuiGFIME9vMj91gT5AKimJRJb5ZtWu2RNY88QpSBUVaPjml9uLSBJSAYRjpbqOHYOXYgmMcJpV3ETRGJMRHtCupgKHVHnpJENmHWqlZ/UjqZ8Aa6L+3khxqNQ4DPRkfrGa9XLxP6+bQP/CS5mIE6CCTD/qJ9yCyMoLsXpM5wU+1gQTyfStFhliiQno2iq6BGc28jxpHdecs9rp7Um1flnUUUb76AAdIQedozq6Rg3URAQ9omf0it6MJ+PFeDc+pqMlo9jZRX9gfP4AfemYug==</latexit>
recon
L
Mesh
Rendered LightControlNet
I<latexit sha1_base64="dMYLWh/eBHIRW0Xg8ZH/8C55I4k=">AAAB+HicbVDLSsNAFJ3UV62PRl26CRbBVUnE17LoRncV7APaECaTaTt0MhNmbsQa8iVuXCji1k9x5984bbPQ6oELh3Pu5d57woQzDa77ZZWWlldW18rrlY3Nre2qvbPb1jJVhLaI5FJ1Q6wpZ4K2gAGn3URRHIecdsLx1dTv3FOlmRR3MEmoH+OhYANGMBgpsKs3QdYH+gAZkSLK88CuuXV3Bucv8QpSQwWagf3ZjyRJYyqAcKx1z3MT8DOsgBFO80o/1TTBZIyHtGeowDHVfjY7PHcOjRI5A6lMCXBm6s+JDMdaT+LQdMYYRnrRm4r/eb0UBhd+xkSSAhVkvmiQcgekM03BiZiiBPjEEEwUM7c6ZIQVJmCyqpgQvMWX/5L2cd07q5/entQal0UcZbSPDtAR8tA5aqBr1EQtRFCKntALerUerWfrzXqft5asYmYP/YL18Q2AypOo</latexit> cond Reference ImageI<latexit sha1_base64="lxXAEPPSDmh76PhVXLAcnYvpbxI=">AAAB9XicbVBNS8NAEN34WetX1aOXYBE8lUT8Oha96K2C/YA2ls120i7dbMLuRC0h/8OLB0W8+l+8+W/ctjlo64OBx3szzMzzY8E1Os63tbC4tLyyWlgrrm9sbm2XdnYbOkoUgzqLRKRaPtUguIQ6chTQihXQ0BfQ9IdXY7/5AErzSN7hKAYvpH3JA84oGun+ppt2EJ4wVRBkWbdUdirOBPY8cXNSJjlq3dJXpxexJASJTFCt264To5dShZwJyIqdRENM2ZD2oW2opCFoL51cndmHRunZQaRMSbQn6u+JlIZaj0LfdIYUB3rWG4v/ee0Egwsv5TJOECSbLgoSYWNkjyOwe1wBQzEyhDLFza02G1BFGZqgiiYEd/bledI4rrhnldPbk3L1Mo+jQPbJATkiLjknVXJNaqROGFHkmbySN+vRerHerY9p64KVz+yRP7A+fwBCM5MG</latexit> ref Control Image SDS Loss
Stage 2: Texture Optimization Canonical views and lighting
R<latexit sha1_base64="9PF/YzzEWCcMjKBIYGdmgltjouI=">AAACFHicbVDLSgMxFM34tr6qLt0Ei9CqlBnxtSx2oQsXVawWOrXcSVMNJjNDckcoQz/Cjb/ixoUibl24829MHwu1Hkg4nHMvyTlBLIVB1/1yxsYnJqemZ2Yzc/MLi0vZ5ZVLEyWa8SqLZKRrARguRcirKFDyWqw5qEDyq+Cu3POv7rk2IgovsBPzhoKbULQFA7RSM7vlK8BbBjI97+b9Y1AK8n7A0d6sFWGhsE1Prze3afl6s9DM5tyi2wcdJd6Q5MgQlWb2029FLFE8RCbBmLrnxthIQaNgknczfmJ4DOwObnjd0hAUN420H6pLN6zSou1I2xMi7as/N1JQxnRUYCd7Ecxfryf+59UTbB82UhHGCfKQDR5qJ5JiRHsN0ZbQnKHsWAJMC/tXym5BA0PbY8aW4P2NPEoud4refnHvbDdXOhrWMUPWyDrJE48ckBI5IRVSJYw8kCfyQl6dR+fZeXPeB6NjznBnlfyC8/ENHG6cSg==</latexit> ( ( ( ·)),L⇤,C⇤)
 <latexit sha1_base64="eIknaGTr3tkVyi0JpoucZ1NyNBY=">AAACEHicbVDLSgMxFM3UV62vqks3wSK2IGVGfG2EogtdVrAPaMtwJ03b0CQzJBmhlH6CG3/FjQtF3Lp059+YabvQ1gO5HM65l5t7gogzbVz320ktLC4tr6RXM2vrG5tb2e2dqg5jRWiFhDxU9QA05UzSimGG03qkKIiA01rQv0782gNVmoXy3gwi2hLQlazDCBgr+dnD5g0IAflmQA3ko0IBX+J83ydHuO+LpKikyIKfzblFdww8T7wpyaEpyn72q9kOSSyoNISD1g3PjUxrCMowwuko04w1jYD0oUsblkoQVLeG44NG+MAqbdwJlX3S4LH6e2IIQuuBCGynANPTs14i/uc1YtO5aA2ZjGJDJZks6sQcmxAn6eA2U5QYPrAEiGL2r5j0QAExNsOMDcGbPXmeVI+L3lnx9O4kV7qaxpFGe2gf5ZGHzlEJ3aIyqiCCHtEzekVvzpPz4rw7H5PWlDOd2UV/4Hz+AAIlmg4=</latexit> ( (p))=(k c,k m,k r,k n) <latexit sha1_base64="f/Sa/qgRNHnShoVeTrzcFtybX0I=">AAACAXicbVDLSsNAFJ34rPUVdSO4CRbBVUnE17KoCxcuKrUPaEKYTCft0MmDmRuxhLjxV9y4UMStf+HOv3HSZqGtBy4czrmXe+/xYs4kmOa3Nje/sLi0XFopr66tb2zqW9stGSWC0CaJeCQ6HpaUs5A2gQGnnVhQHHictr3hZe6376mQLArvYBRTJ8D9kPmMYFCSq+/aAYYBwTy9ydzUBvoAaeOqkWWuXjGr5hjGLLEKUkEF6q7+ZfcikgQ0BMKxlF3LjMFJsQBGOM3KdiJpjMkQ92lX0RAHVDrp+IPMOFBKz/AjoSoEY6z+nkhxIOUo8FRnfq+c9nLxP6+bgH/upCyME6AhmSzyE25AZORxGD0mKAE+UgQTwdStBhlggQmo0MoqBGv65VnSOqpap9WT2+NK7aKIo4T20D46RBY6QzV0jeqoiQh6RM/oFb1pT9qL9q59TFrntGJmB/2B9vkDVbeXeQ==</latexit>
SDS
L
<latexit sha1_base64="EKyDMDivAzzhvNxUw26QI8xg23o=">AAAB6HicbVDLSgNBEOz1GeMr6tHLYBA8hV3xdQx68ZiAeUCyhNlJbzJmdnaZmRXCki/w4kERr36SN//GSbIHTSxoKKq66e4KEsG1cd1vZ2V1bX1js7BV3N7Z3dsvHRw2dZwqhg0Wi1i1A6pRcIkNw43AdqKQRoHAVjC6m/qtJ1Sax/LBjBP0IzqQPOSMGivVk16p7FbcGcgy8XJShhy1Xumr249ZGqE0TFCtO56bGD+jynAmcFLsphoTykZ0gB1LJY1Q+9ns0Ak5tUqfhLGyJQ2Zqb8nMhppPY4C2xlRM9SL3lT8z+ukJrzxMy6T1KBk80VhKoiJyfRr0ucKmRFjSyhT3N5K2JAqyozNpmhD8BZfXibN84p3VbmsX5Srt3kcBTiGEzgDD66hCvdQgwYwQHiGV3hzHp0X5935mLeuOPnMEfyB8/kD3VmM/g==</latexit>p
<latexit sha1_base64="cQ2v2O+QMCeYdUIbHMX/3jjcbKg=">AAACEHicbVC7SgNBFJ31bXytWtoMBjEBCbviqxQttLBQMSaQDeHuZGKGzOwuM3eFsOQTbPwVGwtFbC3t/Bsnj0KjB2Y4nHMv994TJlIY9LwvZ2Jyanpmdm4+t7C4tLzirq7dmjjVjJdZLGNdDcFwKSJeRoGSVxPNQYWSV8LOad+v3HNtRBzdYDfhdQV3kWgJBmilhrsdKMA2A5ld9wrBGSgFhSDkaH/WjLFY3KEXO/S02HDzXskbgP4l/ojkyQiXDfczaMYsVTxCJsGYmu8lWM9Ao2CS93JBangCrAN3vGZpBIqbejY4qEe3rNKkrVjbFyEdqD87MlDGdFVoK/vrm3GvL/7n1VJsHdUzESUp8ogNB7VSSTGm/XRoU2jOUHYtAaaF3ZWyNmhgaDPM2RD88ZP/ktvdkn9Q2r/ayx+fjOKYIxtkkxSITw7JMTknl6RMGHkgT+SFvDqPzrPz5rwPSyecUc86+QXn4xu0q5sS</latexit> ( ( ()),L,C)
R ·
<latexit sha1_base64="f/Sa/qgRNHnShoVeTrzcFtybX0I=">AAACAXicbVDLSsNAFJ34rPUVdSO4CRbBVUnE17KoCxcuKrUPaEKYTCft0MmDmRuxhLjxV9y4UMStf+HOv3HSZqGtBy4czrmXe+/xYs4kmOa3Nje/sLi0XFopr66tb2zqW9stGSWC0CaJeCQ6HpaUs5A2gQGnnVhQHHictr3hZe6376mQLArvYBRTJ8D9kPmMYFCSq+/aAYYBwTy9ydzUBvoAaeOqkWWuXjGr5hjGLLEKUkEF6q7+ZfcikgQ0BMKxlF3LjMFJsQBGOM3KdiJpjMkQ92lX0RAHVDrp+IPMOFBKz/AjoSoEY6z+nkhxIOUo8FRnfq+c9nLxP6+bgH/upCyME6AhmSzyE25AZORxGD0mKAE+UgQTwdStBhlggQmo0MoqBGv65VnSOqpap9WT2+NK7aKIo4T20D46RBY6QzV0jeqoiQh6RM/oFb1pT9qL9q59TFrntGJmB/2B9vkDVbeXeQ==</latexit>
SDS
L
Texture Hash Grid  <latexit sha1_base64="7uiY3S4OUL9nvNpm2nc84L7mZwI=">AAAB/XicbVDLSsNAFJ3UV62v+Ni5CRah3ZREfC2LLnRZwT6gCWUymbRDZ5IwcyPUUPwVNy4Ucet/uPNvnLZZaOuBezmccy9z5/gJZwps+9soLC2vrK4V10sbm1vbO+buXkvFqSS0SWIey46PFeUsok1gwGknkRQLn9O2P7ye+O0HKhWLo3sYJdQTuB+xkBEMWuqZB+4NFgJXXJ+C7iSIoVrtmWW7Zk9hLRInJ2WUo9Ezv9wgJqmgERCOleo6dgJehiUwwum45KaKJpgMcZ92NY2woMrLptePrWOtBFYYS10RWFP190aGhVIj4etJgWGg5r2J+J/XTSG89DIWJSnQiMweClNuQWxNorACJikBPtIEE8n0rRYZYIkJ6MBKOgRn/suLpHVSc85rZ3en5fpVHkcRHaIjVEEOukB1dIsaqIkIekTP6BW9GU/Gi/FufMxGC0a+s4/+wPj8ARh2lFw=</latexit> ( ())
· Random views and lighting
Figure3.OurText-to-TexturePipeline.Ourmethodefficientlysynthesizesrelightabletexturesgivenaninput3Dmeshandtextprompt.In
stage1(topleft),weusemulti-viewvisualpromptingwithourLightControlNetmodeltogeneratefourvisuallyconsistentcanonicalviews
ofthemeshunderfixedlighting,concatenatedintoareferenceimageI .Instage2weapplyanewtextureoptimizationprocedureusing
ref
I asguidancealongwithamulti-resolutionhash-gridrepresentationofthetextureΓ(β(·)).Foreachoptimizationiteration,werendertwo
ref
batchesofimagesusingΓ(β(·))–oneusingthecanonicalviewsandlightingofI tocomputeareconstructionlossL andtheother
ref recon
usingrandomlysampledviewsandlightingtocomputeanSDSlossL basedonLightControlNet.
SDS
minations.However,thesemethodsoftenrequirecontrolled where the output image I is conditioned on y and I .
out cond
setups[20]orcurateddatasets[2,11,51],andstrugglewith ControlNetintroducesaparametersthatsetsthestrengthof
in-the-wildimages.Meanwhile,materialgenerationmodels theconditioningimage.Whens=0,theControlNetsimply
likeControlMat[46],Matfuse[47],andMatfusion[41]use produces an image using the underlying Stable Diffusion
diffusion models for generating Spatially-Varying BRDF model,andwhens=1,theconditioningisstronglyapplied.
(SVBRDF)mapsbutlimitthemselvesto2Dgeneration.In ScoreDistillationSampling(SDS).DreamFusion[35]opti-
contrast, our method creates relightable materials for 3D mizesa3Dscenerepresentationconditionedontextprompts
meshes. usingapre-trained2Dtext-to-imagediffusionmodel.The
sceneisrepresentedasaNeRF [1,26]parametrizationθ.A
3.Preliminaries differentiablerenderer appliedtoθwitharandomlysam-
R
pledcameraviewCthengeneratesa2Dimagex= (θ,C).
Ourtext-to-texturepipelinebuildsonseveraltechniquesthat A small amount of noise ϵ N(0,1) is then addR ed to x
have been recently introduced for text-to-image diffusion to obtain a noisy image x .∼ DreamFusion leverages a dif-
t
models.Here,webrieflydescribethesepriormethodsand fusionmodelϕ(Imagen[40])toprovideascorefunction
thenpresentourpipelineinSection4. ϵˆ (x ;y,t), which predicts the sampled noise ϵ given the
ϕ t
ControlNet.ControlNet[57]isanarchitecturedesignedto noisyimagex ,textprompty,andnoiselevelt.Thisscore
t
addspatiallylocalizedcompositionalcontrolstoatext-to- functionguidesthedirectionofthegradientforupdatingthe
image diffusion model, such as Stable Diffusion [39], in sceneparametersθ,andthegradientiscalculatedbyScore
the form of conditioning imagery (e.g., Canny edges [5], DistillationSampling(SDS):
OpenPosekeypoints[6],depthimages,etc.).Inourwork,
(cid:20) (cid:21)
∂x
wherewetakea3Dmeshasinput,theconditioningimage (ϕ,x)=E w(t)(ϵˆ (x ;y,t) ϵ) ,
θ SDS t,ϵ ϕ t
∇ L − ∂θ
I (C) is a rendering of the mesh from a given camera
cond
viewpointC.Then,giventextprompty,
wherew(t)isaweightingfunction.Duringeachiteration,to
calculatetheSDSloss,werandomlychooseacameraview
I =ControlNet(I (C),y),
out cond C,rendertheNeRFθtoformanimagex,addnoiseϵtoit,andpredictthenoiseusingthediffusionmodelϕ.Werun sired view and lighting for any input 3D mesh. We first
theoptimizationfor5,000to10,000iterations. render the conditioning image with the desired view and
In our work, we introduce an illumination-aware SDS lightingandthenpassitalongwithatextpromptintoLight-
losstooptimizesurfacetextureona3Dmeshtosuppress ControlNet, to obtain high-quality images. These images
inconsistencyartifactsandsimultaneouslyseparatelighting arespatiallyalignedtothedesiredview,litwiththedesired
fromthesurfacereflectance. lighting,andcontaindetailedsurfacetextures(Figure1).
Distillingtheencoder.WeimprovetheefficiencyofLight-
4.Method
ControlNetbydistillingtheimageencoderinStableDiffu-
sion[39],thebasediffusionmodelintheControlNetarchi-
Ourtext-to-texturepipelineoperatesintwomainstagesto
tecture. The original Stable Diffusion image encoder con-
generatearelightabletextureforaninput3Dmeshwithacor-
sumes almost 50% of the forward and backward time of
respondingtextprompt(Figure3).Instage1,weuseamulti-
SDScalculationusingthelatentdiffusionmodel,primarily
viewvisualpromptingapproachtoobtainvisuallyconsistent
indownsamplingtheinputimage.Metzeretal.[25]have
viewsoftheobjectfromasmallsetofviewpoints,usinga
foundtheimagedecoderfromlatentspacetoimagespace
2DControlNet.Simplybackprojectingthesesparseviews
canbecloselyapproximatedbyper-pixelmatrixmultiplica-
ontothe3Dmeshcouldproducepatchesofhigh-qualitytex-
tion.Inspiredbythis,wedistilltheencoderbyremovingits
ture,butwouldalsogeneratevisibleseamsandothervisual
attentionmodulesandtrainingitontheCOCOdataset[22]
artifactswheretheviewsdonotfullymatch.Theresulting
tomatchtheoriginaloutput.Thisdistilledencoderruns5x
texturewouldalsohavelightingbaked-in,makingitdifficult
fasterthantheoriginalone,resultinginanapproximately
torelightthetexturedmeshinanewlightingenvironment.
2xaccelerationofourtext-to-texturepipelinewithoutcom-
Therefore,instage2,weapplyatextureoptimizationthat
promisingoutputquality.Anablationstudyofourdistilled
uses a ContolNet in combination with Score Distillation
encoderisdetailedinTable2,withadditionalimplementa-
Sampling(SDS)[35]tomitigatesuchartifactsandseparate
tionspecificsinSectionAoftheappendix.
lightingfromthesurfacematerialproperties/reflectance.In
bothstages,weintroduceanewillumination-awareControl-
4.2.Stage1:Multi-viewVisualPrompting
Netthatallowsustospecifythedesiredlightingasacon-
ditioning image for an underlying text-to-image diffusion Instage1,weleverageLightControlNettosynthesizehigh-
model. We call this model LightControlNet and describe quality2Dimagesforasparsesetofviewsofthe3Dmesh.
how it works in Section Section 4.1. We then detail each Specifically,wecreateconditioningimagesforfourcanon-
stageinSectionsSection4.2andSection4.3,respectively. ical views C ∗ around the equator of the 3D mesh using a
fixedlightingenvironmentmapL .Oneapproachtogener-
4.1.LightControlNet ∗
atingthecompletetextureforthemeshwouldbetoapplythe
LightControlNetadaptstheControlNetarchitecturetoen- LightControlNetindependentlywitheachsuchconditioning
ablecontroloverthelightinginthegeneratedimage.More image, but using the same text prompt, and then backpro-
specifically,wecreateaconditioningimagefora3Dmesh jectingthefouroutputimagestothesurfaceofthe3Dmesh.
byrenderingitusingthreepre-definedmaterialsandunder Inpractice,however,applyingtheLightControlNettoeach
knownlightingconditions(Figure4).Theserenderingsen- viewindependentlyproducesinconsistentimagesofvarying
capsulateinformationaboutthedesiredshapeandlighting appearanceandstyle,evenwhenthetextpromptandrandom
fortheobject,andwestackthemintoathree-channelcon- seedremainfixed(Figure5).
ditioningimage.Wehavefoundthatsettingthepre-defined Tomitigatethismulti-viewinconsistencyissue,wetakea
materialsto(1)non-metal,non-smooth;(2)non-metal,half multi-viewvisualpromptingapproach.Weconcatenatethe
smooth;and(3)puremetal,extremelysmooth,respectively, conditioningimagesforthefourcanonicalviewsintoasin-
workswellinpractice. gle2 2gridandtreatitasasingleconditioningimage.We
×
TotrainourLightControlNet,weuse40Kobjectsfrom observethatapplyingLightControlNettoallfourviewssi-
theObjaversedataset[10].Eachobjectisrenderedfrom12 multaneously,usingthiscombinedmulti-viewconditioning
viewsusingarandomlysampledcameraC andlightingL image,resultsinafarmoreconsistentappearanceandstyle
sampledfrom6environmentmapssourcedfromtheInternet. acrosstheviews,comparedtoindependentprompting(Fig-
Foreachresulting(L,C)pair,werendertheconditioning ure5).Wesuspectthispropertyarisesfromthepresenceof
image using the pre-defined materials, as well as the full- similartrainingdatasamples–grid-organizedsetsdepicting
colorrenderingoftheobjectusingitsoriginalmaterialsand thesameobject–inStableDiffusion’strainingset,which
textures.Weusetheresulting480Kpairsof(conditioning isalsoobservedinconcurrentworks[53,58].Formally,we
images,full-colorrendering)totrainLightControlNetusing generatetheconditioningimageI (L ,C )underafixed
cond ∗ ∗
theapproachofZhangetal.[57]. canonical lighting condition L using the four canonical
∗
OnceLightControlNetistrained,wecanspecifythede- viewpointsC .WethenapplyourLightControlNetwithtext
∗Conditioning Image “Leather …” “Metal …” “Wooden …”
Non-Metal,
Not Smooth
stack
Mesh
Half-Metal,
Half Smooth
Conditioning Image
Pure Metal,
Full Color Image Smooth
(a) Rendering Conditioning Image (b) Inference with LightControlNet
Figure4.(a)LightControlNetrequiresaconditioningimagethatspecifiesdesiredlightingLforaviewC ofa3Dmesh.Toformthe
conditioningimage,wefirstrenderthemeshwiththedesiredLandC usingthreedifferentmaterials:(1)non-metal,notsmooth,(2)
half-metal,half-smooth,and(3)puremetal,smooth,andthencombinetherenderingsintoasinglethree-channelimage.(b)LightControlNet
isadiffusionmodelthattakessuchlightconditioningimagesasinputaswellasatextpromptandgeneratesimagesaccordingly.
Conditioning Images LightControlNet Outputs Conditioning Image LightControlNet Ouput
(a) Independent Inputs to LightControlNet (b) Concatenated Input to LightControlNet
produce visual inconsistencies produces more consistent output
Figure5.Multi-ViewVisualPrompting.(a)WhenweindependentlyinputfourcanonicalconditioningimagestoLightControlNet,it
generatesfourverydifferentappearancesandstylesevenwithafixedrandomseed.(b)Whenweconcatenatethefourimagesintoa2×2grid
andpassthemasasingleimageintoLightControlNet,itproducesafarmoreconsistentappearanceandstyle.Textprompt:“HikingBoot”.
prompt y, to generate the corresponding reference output To address both of these issues, we employ a texture
imageI as optimizationusingSDSloss.Specifically,weuseamulti-
ref
resolution hash-grid [28] as our 3D scene representation,
I =ControlNet(I (L ,C ),y).
ref cond ∗ ∗ instead of NeRF as in the original DreamFusion formu-
4.3.Stage2:TextureOptimization lation [35]. Given a 3D point p R3 on the mesh, our
∈
hash-gridproducesa32-dimensionalmulti-resolutionfea-
Instage2,wecoulddirectlybackprojectthefourreference ture.Thisfeatureisthenfedtoa2-layerMLPΓtoobtainthe
viewsoutputinstage1ontothe3Dmeshusingthecamera texturematerialparametersforthispoint.SimilartoFanta-
matrix C associated with each view. While the resulting sia3D[8],thesematerialparametersconsistofmetallicness
texturewouldcontainsomehigh-qualityregions,itwould k R,roughnessk R,abumpvectork R3andthe
m r n
alsosufferfromtwoproblems(1)Itwouldcontainseams base∈ colork R3.Fo∈ rmally, ∈
c
andvisualartifactsduetoremaininginconsistenciesbetween ∈
overlappingviews,occlusionsintheviewsthatleaveparts (k ,k ,k ,k )=Γ(β(p)),
c m r n
ofthemeshuntextured,andlossofdetailwhenapplyingthe
backprojectiontransformationandresamplingtheviews.(2) whereβ isthemulti-resolutionhashencodingfunction.No-
Inaddition,aslightingisbakedintotheLightControlNet’s tably, this 3D hash-grid representation can be easily con-
RGBimages,itwouldalsobebakedintothebackprojected verted to 2D uv texture maps, which are more friendly to
texture,makingitdifficulttorelightthemesh. downstream applications. Given the mesh M, the textureΓ(β()),acameraviewC andlightingLwecanusenvd- relativelysmallnoiselevels(t 0.1)intheSDSoptimiza-
· ≤
iffrast [16], a differentiable renderer to produce a 2D tion.Wesamplethenoisefollowingalinearlydecreasing
R
renderingofitx,as schedule [15] with t = 0.1 and t = 0.02. We also
max min
adjustthe conditioningstrength s ofour LightControlNet
x= (M,Γ(β()),L,C). in the SDS loss linearly from 1 to 0 over these iterations
R ·
sothatLightControlNetisonlylightlyappliedbytheend
More details about the rendering equation are in the ap-
oftheoptimization.Wehaveexperimentallyfoundthatwe
pendix.Sincethemeshgeometryisfixed,weomitM inthe
obtainhigh-qualitytexturesafter400totaliterationsofthis
remainderofthepaper.
optimizationandthisisfarfeweriterationsthanotherSDS-
Recall that the optimization approach of DreamFu-
basedtexturegenerationtechniquessuchasFantasia3D[8]
sion[35]randomlysamplescameraviewsC,generatesan
whichrequires5000iterations.Moreimplementationdetails
imageforC usingdiffusionmodelϕ,andsupervisestheop-
areinSectionAoftheappendix.
timizationusingtheSDSloss.Weextendthisoptimization
Fasterpipelinewithbaked-inlighting.Ourtext-to-texture
intwoways.First,weusefourfixedreferenceimagesI
ref pipelineprimarilyusesthelightingcontrolcapabilitiesof
withitscanonicalcameraviewsC andlightingL toguide
∗ ∗ LightControlNetinthetextureoptimizationprocesstosepa-
thetextureoptimization,throughareconstructionlossthat
ratelightingfromsurfacereflectance.Wehaveexperimented
weapplywheneverwesampleacanonicalview.
withreplacingtheLightControlNetinstage1ofourpipeline
withadepthControlNetthatusesadepthrenderingofthe
meshastheconditioningimageandreplacingitwithStable
=λ [I , (Γ(β()),L ,C )]
Lrecon MSE LMSE ref R · ∗ ∗ Diffusion[39]basedSDSinstage2.Whilethisapproach
+λ VGG VGG[I ref, (Γ(β()),L ∗,C ∗)]. leaves lighting baked into the resulting texture, it does in-
L R ·
creasethespeedofourpipeline,becauseiteliminatesthe
When we sample a non-canonical view C, we sample a
forwardpassofLightControlNetintheSDSoptimization.
random lighting L and use the SDS loss to supervise the
AsshowninTable1,wegain2 speedupwiththisversion.
optimization,butwithourLightControlNetasthediffusion ×
modelϕ LCN,so 5.Experiments
(ϕ ,x) In this section, we present comprehensive experiments to
Γ,β SDS LCN
∇ L
(cid:20) (cid:21) evaluatetheefficacyofourproposedmethodforrelightable,
∂x
=E t,ϵ w(t)(ϵˆ ϕLCN(x t;y,t,I cond(L,C)) −ϵ)
∂Γ(β())
, text-basedmeshtexturing.Weperformbothqualitativeand
· quantitativecomparisonswithexistingbaselines,alongwith
wherex= (Γ(β()),L,C)andw(t)istheweight. anablationstudyonthesignificanceofeachofourmajor
Finally,R we emp· loy a material smoothness regularizer components.
oneveryiterationtoenforcesmoothbasecolors,usingthe Dataset.AsillustratedinFigure3,weemployObjaverse
approachofnvdiffrec[29].Forasurfacepointpwithbase [10]torenderpaireddatatotrainourLightControlNet.Ob-
colork (p),thesmoothnessregularizerisdefinedas javerse consists of approximately 800k objects, of which
c
we use the names and tags as their text descriptions. We
(cid:88) filteroutobjectswithlowCLIPsimilarity[36]totheirtext
= k (p) k (p+ϵ),
reg c c
L | − | descriptionsandselectaround40kasourtrainingset.Each
p S
∈ objectisrenderedfrom12viewsusingrandomlysampled
whereS denotestheobjectsurfaceandϵisasmallrandom camerasandlightingfromaspecificsetofenvironmental
3Dperturbation. lighting maps. To evaluate baselines and our method, we
Scheduling the optimization. We warm up the optimiza- holdout70randommeshesfromObjaverse[10]asthetest
tionbyrenderingthe4canonicalviewsandapplying set.Weadditionallygatheracollectionoftestmeshesfrom
recon
L
for50iterations.Wethenaddiniterationsusingthe 3D game assets to assess our method, showing its broad
SDS
L
lossandoptimizeoverrandomlychosencameraviewsand applicability.Furtherdetailscanbefoundintheappendix.
randomlyselectedlightingfromapre-definedsetofenvi- Baselines. We compare our approach with existing mesh
ronmentallightingmaps.Specificallywealternateiterations texturingmethods.Specifically,Latent-Paint[25]employs
betweenusing and .Inaddition,foraquarterof SDSlossinlatentspacefortexturegeneration.Text2tex[7]
SDS recon
L L
theSDSiterations,weusethecanonicalviewsratherthan progressivelyproduces2Dviewsfromchosenviewpoints,
randomlyselectingtheviews.Thisensuresthattheresulting followedbyaninverseprojectiontoliftthemto3D.TEX-
texturedoesnotoverfittothereferenceimagescorrespond- Ture[38]utilizesasimilarliftingapproachbutsupplements
ingtothecanonicalviews.Thewarm-upiterationscapture itwithaswiftSDSoptimizationpost-lifting.Beyondthese
thelarge-scalestructureofourtextureandallowustouse texturegenerationmethods,text-to-3Dapproachesserveas“Stylish Boot” “Medieval Windmill”
“1978 Puch Moped, motorcycle” “A sculpture of horse without rider”
“A vintage space explorer jacket “Jacket made from the “Hylian goblin soldier from “Cave Dweller, …”
with a matching helmet, …” fabrics of a ghost ship, …” legend of zelda …”
“A stylish jacket, …” “Jacket that gives the impression “Deep sea diver, …” “Mermaid warrior, …”
of a swirling nebula, …”
Figure6.SampleresultsfromourmethodappliedtoObjaversetestmeshes(tophalf)and3Dgameassets(bottomhalf).Toillustratethe
efficacyofourrelightabletextures,foreachtexturedmesh,wefixtheenvironmentlightingandrenderthemeshunderdifferentrotations.As
shownabove,ourmethodisabletogeneratetexturesthatarenotonlyhighlydetailed,butalsorelightablewithrealisticlightingeffects.
additionalbaselines,giventhattextureisacomponentof3D QualitativeAnalysisandUserStudy.AsshowninFigure6,
generation.Notably,wechoosefantasia3D[8]asabaseline, ourmethodcangeneratehighly-detailedtexturesthatcan
thefirsttouseamaterial-basedrepresentationfortexturesin berenderedproperlywiththeenvironmentlightingacross
text-to-3Dprocessing. a wide variety of meshes. We also visually compare our
methodandthebaselinesinFigure7.Ourmethodproduces
QuantativeEvaluation.InTable1,wecompareourmethod textures with higher visual fidelity than the baselines for
withthebaselinesontheObjaverse[10]testset.Foreach boththerelightableandnon-relightablevariants.Inparticu-
method,wegenerate16viewsandevaluateFrechetIncep- lar,WhencomparedwithFantasia3D[8],arecentworkthat
tionDistance(FID)[14,33]andKernelInceptionDistance alsoaimstogeneratematerial-basedtexture,ourresultsnot
(KID)[3]comparedwithground-truthrenderedviews.Two onlyhavesuperiorvisualquality,butalsodisentanglethe
variationsofourmethodareassessed.Bothvariantsuseour lightingmoresuccessfully.Tofurtherevaluatethequalityof
proposed two-stage pipeline, and the first employs a stan- therelightedtexturequantitatively,weconductauserstudy
dard depth-guided ControlNet, while the second uses our askingparticipantstocompareourresultswithFantasia3D.
proposedLightControlNet.Ourmethodsignificantlyoutper- Bothresultsarerenderedunderidenticallightingenviron-
formsthebaselinesinbothqualityandruntime.Relightable Texture Non-relightable Texture
Input Ours Fantasia3D Ours (Non-relightable) TEXTure Latent-Paint Text2tex
Lighting 1 Lighting 2 Lighting 1 Lighting 2
“Horse
Saddle…”
Figure7.QualitativeAnalysis.Wevisuallycompareourmethodwithexistingbaselines.AlthoughFantasia3D[8]alsoattemptstogenerate
relightabletexture,unlikeours,theirresultstilltendstohavebaked-inlighting.Fornon-relightabletexturegeneration,wereplaceour
LightControlNetwithdepthControlNetandachievesuperiorresultsintermsofbothimagequalityandruntimecomparedtothebaselines.
MoredetailsareinTable1.
Objaverse FID KID Runtime Objaverse FID KID Runtime
↓ ↓ ↓ ↓ ↓ ↓
subset ( 10 3) (mins) subset ( 10 3) (mins)
− −
× ×
Latent-Paint[25] 73.65 7.26 10 Ours(w/odist.enc.) 60.34 2.84 8
Fantasia3D[8] 120.32 8.34 30 Ours(w/om.v.v.p) 74.23 3.54 19
TEXTure[38] 71.64 5.43 6
Ours 62.67 2.69 4
Text2tex[7] 95.59 4.71 15
Ours(w/depth) 60.49 3.96 2 Table2.AblationStudy.Weperformanablationanalysisonour
Ours 62.67 2.69 4 distilledencoder(1strow)andmulti-viewvisualprompting(2nd
row).ReplacingthedistilledencoderwiththeoriginalVQ-VAE
Table1.QuantitativeEvaluation.Wetestourmethodsandbase- encoderdoubledthetimeofperformancewithoutanoticeablequal-
lineson70objectsfromObjaverse[10].WithdepthControlNet, ityimprovement.Whenremovingthemulti-viewvisualprompting
ourmethodyieldssuperiorresultstoallbaselinesinarapidrun- forinitialgeneration,thesystemrequires2000iterations(5xslow-
timeof2minutes.UsingLightControlNet(Ours)withinourmodel downcomparedtoour400iterations)toproducereasonableresults,
improvesthelightingdisentanglementofourtexturegeneration whichleadstoslightlyworsetexturequality.
andmaintainscomparableimagequality.
illumination-aware2Ddiffusionmodel(LightControlNet)
and an improved optimization process based on the SDS
mentstoensureafairevaluation,withashadedmeshimage
loss.Ourapproachissubstantiallyfasterthanpreviousmeth-
indisplaysignifyingthelightingdirection(pleasereferto
odswhileyieldinghigh-fidelitytextureswithillumination
the appendix for more details). 86.7% of the participants
disentangled from surface reflectance/albedo. We demon-
preferourresults,indicatingthatourapproachconsistently
stratedtheefficacyofourmethodthroughquantitativeand
surpassesFantasia3Dinrelightingquality.
qualitativeevaluationontheObjaversedataset.
Ablation Study. We perform an ablation analysis on our
Limitations.Ourapproachstillposesafewlimitations:(1)
multi-viewvisualprompting(m.v.v.p.)anddistilledencoder
Baked-inlightingcanstillbefoundincertaincases,espe-
(dist.enc.)asseeninTable2.Whensubstitutingourdistilled
ciallyformeshesthatareoutsideofthetrainingdatadistri-
encoderwiththeoriginalVQ-VAEencoder,theperformance
bution of Objaverse; (2)The generated materialmaps are
istwiceasslow,butthequalityofresultsisnotnoticeably
sometimesnotfullydisentangledandinterpretableasmetal-
superior.Ontheotherhand,withoutthemulti-viewvisual
licness,roughness,etc.;(3)Duetotheinherentlimitationof
promptingfortheinitialgeneration,thesystemrequires2000
the2Ddiffusionmodelbackbones,thegeneratedtextures
iterations(5xslowdowncomparedtoour400iterations)to
canfailtofollowthetextpromptinsomecases.
produce reasonable results, while still leading to slightly
Acknowledgments.WethankBenjaminAkrish,VictorZor-
worsetexturequality.
dan, Dmitry Trifonov, Derek Liu, Sheng-Yu Wang, Gau-
ravParmer,RuihanGao,NupurKumari,andSeanLiufor
6.Discussion
theirdiscussionandhelp.Theprojectispartlysupportedby
We proposed an automated texturing technique based on Roblox.JYZispartlysupportedbythePackardFellowship.
user-providedtextualdescriptions.Ourmethodemploysan KDissupportedbytheMicrosoftResearchPhDFellowship.
1
weiV
2
weiVReferences scaleupdateruleconvergetoalocalnashequilibrium. InAd-
vancesinNeuralInformationProcessingSystems(NeurIPS),
[1] JonathanT.Barron,BenMildenhall,MatthewTancik,Peter
2017. 7
Hedman,RicardoMartin-Brualla,andPratulP.Srinivasan.
[15] YukunHuang,JiananWang,YukaiShi,XianbiaoQi,Zheng-
Mip-nerf:Amultiscalerepresentationforanti-aliasingneu-
JunZha,andLeiZhang. Dreamtime:Animprovedoptimiza-
ral radiance fields. In IEEE International Conference on
tionstrategyfortext-to-3dcontentcreation. arXivpreprint
ComputerVision(ICCV),2021. 3
arXiv:2306.12422,2023. 6
[2] Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall,
[16] SamuliLaine,JanneHellsten,TeroKarras,YeonghoSeol,
Kalyan Sunkavalli, Milos Hasan, Yannick Hold-Geoffroy,
Jaakko Lehtinen, and Timo Aila. Modular primitives for
David Kriegman, and Ravi Ramamoorthi. Neural re-
high-performancedifferentiablerendering. 2020. 6
flectancefieldsforappearanceacquisition. arXivpreprint
[17] MuhengLi,YueqiDuan,JieZhou,andJiwenLu. Diffusion-
arXiv:2008.03824,2020. 3
sdf:Text-to-shapeviavoxelizeddiffusion. InIEEEConfer-
[3] MikołajBin´kowski,DanicaJSutherland,MichaelArbel,and
enceonComputerVisionandPatternRecognition(CVPR),
ArthurGretton. Demystifyingmmdgans. InInternational
2023. 2
ConferenceonLearningRepresentations(ICLR),2018. 7
[18] Weiyu Li, Rui Chen, Xuelin Chen, and Ping Tan. Sweet-
[4] Alexey Bokhovkin, Shubham Tulsiani, and Angela Dai.
dreamer:Aligninggeometricpriorsin2ddiffusionforconsis-
Mesh2tex:Generatingmeshtexturesfromimagequeries. In
tenttext-to-3d. arxiv:2310.02596,2023. 2
IEEEInternationalConferenceonComputerVision(ICCV),
[19] YuhengLi,HaotianLiu,QingyangWu,FangzhouMu,Jian-
2023. 2
wei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.
[5] JohnCanny. Acomputationalapproachtoedgedetection.
Gligen: Open-set grounded text-to-image generation. In
IEEETransactionsonpatternanalysisandmachineintelli-
IEEE Conference on Computer Vision and Pattern Recog-
gence,(6):679–698,1986. 3 nition(CVPR),2023. 2
[6] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. [20] ZhengqinLi,KalyanSunkavalli,andManmohanChandraker.
Realtimemulti-person2dposeestimationusingpartaffinity Materialsformasses:Svbrdfacquisitionwithasinglemobile
fields. InProceedingsoftheIEEEconferenceoncomputer phoneimage. InEuropeanConferenceonComputerVision
visionandpatternrecognition,pages7291–7299,2017. 3 (ECCV),2018. 3
[7] DaveZhenyuChen,YawarSiddiqui,Hsin-YingLee,Sergey [21] Chen-HsuanLin,JunGao,LumingTang,TowakiTakikawa,
Tulyakov,andMatthiasNießner. Text2tex:Text-driventex- XiaohuiZeng,XunHuang,KarstenKreis,SanjaFidler,Ming-
turesynthesisviadiffusionmodels. InIEEEInternational YuLiu,andTsung-YiLin. Magic3d:High-resolutiontext-to-
ConferenceonComputerVision(ICCV),2023. 1,2,6,8,13 3dcontentcreation. InIEEEConferenceonComputerVision
[8] RuiChen,YongweiChen,NingxinJiao,andKuiJia. Fan- andPatternRecognition(CVPR),2023. 2
tasia3d: Disentangling geometry and appearance for high- [22] Tsung-YiLin,MichaelMaire,SergeBelongie,LubomirBour-
qualitytext-to-3dcontentcreation. InIEEEInternational dev, Ross Girshick, James Hays, Pietro Perona, Deva Ra-
ConferenceonComputerVision(ICCV),2023. 1,2,5,6,7, manan, C. Lawrence Zitnick, and Piotr Dolla´r. Microsoft
8,12 coco:Commonobjectsincontext,2015. 4
[9] YongweiChen,RuiChen,JiabaoLei,YabinZhang,andKui [23] RuoshiLiu,RundiWu,BasileVanHoorick,PavelTokmakov,
Jia. Tango: Text-driven photorealistic and robust 3d styl- SergeyZakharov,andCarlVondrick. Zero-1-to-3:Zero-shot
izationvialightingdecomposition. InAdvancesinNeural oneimageto3dobject. InIEEEInternationalConferenceon
InformationProcessingSystems(NeurIPS),2022. 2 ComputerVision(ICCV),2023. 2
[10] MattDeitke,DustinSchwenk,JordiSalvador,LucaWeihs, [24] ChenlinMeng,YutongHe,YangSong,JiamingSong,Jiajun
OscarMichel,EliVanderBilt,LudwigSchmidt,KianaEhsani, Wu,Jun-YanZhu,andStefanoErmon.SDEdit:Guidedimage
AniruddhaKembhavi,andAliFarhadi.Objaverse:Auniverse synthesisandeditingwithstochasticdifferentialequations.
ofannotated3dobjects. InIEEEConferenceonComputer In International Conference on Learning Representations
VisionandPatternRecognition(CVPR),2023. 4,6,7,8,12 (ICLR),2022. 2
[11] DuanGao,XiaoLi,YueDong,PieterPeers,KunXu,and [25] GalMetzer,EladRichardson,OrPatashnik,RajaGiryes,and
XinTong. Deepinverserenderingforhigh-resolutionsvbrdf DanielCohen-Or. Latent-nerfforshape-guidedgeneration
estimation from an arbitrary number of images. In ACM of3dshapesandtextures. InIEEEConferenceonComputer
SIGGRAPH,2019. 3 VisionandPatternRecognition(CVPR),2023. 1,2,4,6,8,
[12] PaulHenderson,VagiaTsiminaki,andChristophLampert. 13
Leveraging2Ddatatolearntextured3Dmeshgeneration. In [26] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
IEEEConferenceonComputerVisionandPatternRecogni- JonathanT.Barron,RaviRamamoorthi,andRenNg. Nerf:
tion(CVPR),2020. 2 Representingscenesasneuralradiancefieldsforviewsyn-
[13] AmirHertz,RonMokady,JayTenenbaum,KfirAberman, thesis. InEuropeanConferenceonComputerVision(ECCV),
Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt im- 2020. 3
age editing with cross attention control. arXiv preprint [27] ChongMou,XintaoWang,LiangbinXie,JianZhang,Zhon-
arXiv:2208.01626,2022. 2 gangQi,YingShan,andXiaohuQie. T2i-adapter:Learning
[14] MartinHeusel,HubertRamsauer,ThomasUnterthiner,Bern- adapterstodigoutmorecontrollableabilityfortext-to-image
hardNessler,andSeppHochreiter.Ganstrainedbyatwotime- diffusionmodels. arXivpreprintarXiv:2302.08453,2023. 2[28] ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexander [43] J Ryan Shue, Eric Ryan Chan, Ryan Po, Zachary Ankner,
Keller. Instantneuralgraphicsprimitiveswithamultiresolu- JiajunWu,andGordonWetzstein. 3dneuralfieldgeneration
tionhashencoding. InACMSIGGRAPH,2022. 5 usingtriplanediffusion. InIEEEConferenceonComputer
[29] JacobMunkberg,JonHasselgren,TianchangShen,JunGao, VisionandPatternRecognition(CVPR),2023. 2
Wenzheng Chen, Alex Evans, Thomas Mu¨ller, and Sanja [44] Yawar Siddiqui, Justus Thies, Fangchang Ma, Qi Shan,
Fidler. Extracting Triangular 3D Models, Materials, and Matthias Nießner, and Angela Dai. Texturify: Generating
LightingFromImages. InIEEEConferenceonComputer textureson3dshapesurfaces. InEuropeanConferenceon
VisionandPatternRecognition(CVPR),2022. 6,12 ComputerVision(ECCV),2022. 2
[30] Gimin Nam, Mariem Khlifi, Andrew Rodriguez, Alberto [45] JingxiangSun,BoZhang,RuizhiShao,LizhenWang,Wen
Tono,LinqiZhou,andPaulGuerrero. 3d-ldm:Neuralim- Liu,ZhendaXie,andYebinLiu. Dreamcraft3d:Hierarchical
plicit3dshapegenerationwithlatentdiffusionmodels. arXiv 3dgenerationwithbootstrappeddiffusionprior,2023. 2
preprintarXiv:2212.00842,2022. 2 [46] GiuseppeVecchio,RosalieMartin,ArthurRoullier,Adrien
[31] FredENicodemus. Directionalreflectanceandemissivityof Kaiser, Romain Rouffet, Valentin Deschaintre, and Tamy
anopaquesurface. Appliedoptics,4(7):767–775,1965. 2 Boubekeur. Controlmat:Controlledgenerativeapproachto
[32] Keunhong Park, Konstantinos Rematas, Ali Farhadi, and materialcapture. arXivpreprintarXiv:2309.01700,2023. 3
Steven M. Seitz. Photoshape: Photorealistic materials for [47] GiuseppeVecchio,RenatoSortino,SimonePalazzo,andCon-
large-scaleshapecollections. 2018. 2 cettoSpampinato. Matfuse:Controllablematerialgeneration
[33] GauravParmar,RichardZhang,andJun-YanZhu. Onaliased with diffusion models. arXiv preprint arXiv:2308.11408,
resizingandsurprisingsubtletiesinganevaluation. InIEEE 2023. 3
Conference on Computer Vision and Pattern Recognition [48] BruceWalter,StephenRMarschner,HongsongLi,andKen-
(CVPR),2022. 7 nethETorrance. Microfacetmodelsforrefractionthrough
[34] DarioPavllo,JonasKohler,ThomasHofmann,andAurelien rough surfaces. In Proceedings of the 18th Eurographics
Lucchi. Learninggenerativemodelsoftextured3dmeshes conferenceonRenderingTechniques,2007. 12
fromreal-worldimages. InIEEEInternationalConference [49] HaochenWang,XiaodanDu,JiahaoLi,RaymondA.Yeh,and
onComputerVision(ICCV),2021. 2 GregShakhnarovich. Scorejacobianchaining:Liftingpre-
[35] BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall. trained2ddiffusionmodelsfor3dgeneration. InIEEECon-
Dreamfusion:Text-to-3dusing2ddiffusion. InInternational ferenceonComputerVisionandPatternRecognition(CVPR),
ConferenceonLearningRepresentations(ICLR),2023. 2,3, 2023. 2
4,5,6 [50] TengfeiWang,TingZhang,BoZhang,HaoOuyang,Dong
[36] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Chen, Qifeng Chen, and Fang Wen. Pretraining is all
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, you need for image-to-image translation. arXiv preprint
AmandaAskell,PamelaMishkin,JackClark,etal. Learning arXiv:2205.12952,2022. 2
transferablevisualmodelsfromnaturallanguagesupervision. [51] ZianWang,JonahPhilion,SanjaFidler,andJanKautz.Learn-
InInternationalConferenceonMachineLearning(ICML), ingindoorinverserenderingwith3dspatially-varyinglight-
2021. 6 ing. InIEEEInternationalConferenceonComputerVision
[37] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu, (ICCV),2021. 3
andMarkChen. Hierarchicaltext-conditionalimagegenera- [52] ZhengyiWang,ChengLu,YikaiWang,FanBao,Chongxuan
tionwithcliplatents. arXivpreprintarXiv:2204.06125,2022. Li,HangSu,andJunZhu. Prolificdreamer:High-fidelityand
1,2 diversetext-to-3dgenerationwithvariationalscoredistilla-
[38] EladRichardson,GalMetzer,YuvalAlaluf,RajaGiryes,and tion. InAdvancesinNeuralInformationProcessingSystems
DanielCohen-Or.Texture:Text-guidedtexturingof3dshapes. (NeurIPS),2023. 2
InACMSIGGRAPH,2023. 1,2,6,8,13 [53] EthanWeber,AleksanderHolynski,VarunJampani,Saurabh
[39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Saxena,NoahSnavely,AbhishekKar,andAngjooKanazawa.
Patrick Esser, and Bjo¨rn Ommer. High-resolution image Nerfiller:Completingscenesviagenerative3dinpainting. In
synthesiswithlatentdiffusionmodels. InIEEEConference arXiv,2023. 4
onComputerVisionandPatternRecognition(CVPR),2022. [54] JiaruiXu,SifeiLiu,ArashVahdat,WonminByeon,Xiao-
1,2,3,4,6,12 longWang,andShaliniDeMello. Open-vocabularypanoptic
[40] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi, segmentationwithtext-to-imagediffusionmodels. InPro-
JayWhang,EmilyLDenton,KamyarGhasemipour,Raphael ceedingsoftheIEEE/CVFConferenceonComputerVision
Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. andPatternRecognition,pages2955–2966,2023. 2
Photorealistictext-to-imagediffusionmodelswithdeeplan- [55] XudongXu,ZhaoyangLyu,XingangPan,andBoDai. Mat-
guage understanding. In Advances in Neural Information laber:Material-awaretext-to-3dvialatentbrdfauto-encoder.
ProcessingSystems(NeurIPS),2022. 1,2,3 arXivpreprintarXiv:2308.09278,2023. 2
[41] SamSartorandPieterPeers.Matfusion:agenerativediffusion [56] RuiYu,YueDong,PieterPeers,andXinTong. Learning
modelforsvbrdfcapture. InACMSIGGRAPHAsia,2023. 3 texturegeneratorsfor3dshapecollectionsfrominternetphoto
[42] YichunShi,PengWang,JianglongYe,LongMai,KejieLi, sets. 2021. 2
andXiaoYang. Mvdream:Multi-viewdiffusionfor3dgener- [57] LvminZhang,AnyiRao,andManeeshAgrawala. Adding
ation. arXiv:2308.16512,2023. 2 conditional control to text-to-image diffusion models. InIEEEInternationalConferenceonComputerVision(ICCV),
2023. 1,2,3,4
[58] MindaZhao,ChaoyiZhao,XinyueLiang,LinchengLi,Zeng
Zhao, Zhipeng Hu, Changjie Fan, and Xin Yu. Efficient-
dreamer:High-fidelityandrobust3dcreationviaorthogonal-
viewdiffusionprior. 2023. 4
[59] LinqiZhou,YilunDu,andJiajunWu. 3dshapegeneration
andcompletionthroughpoint-voxeldiffusion. InIEEEIn-
ternationalConferenceonComputerVision(ICCV),2021.
2A.ImplementationDetails
DistilledEncoder.InSection4.1(mainpaper),weimprove
the efficiency of LightControlNet by distilling the image
encoder in Stable Diffusion [39]. We profile the running
timeofourdistilledandoriginalencoderinthetablebelow.
Time(ms) Forward Backward Total
Original 113 569 682
Distilled 42 81 123(5.5 )
×
Table3.Weprofiletheforwardandbackwardpassofourdistilled
andoriginalencoderinStableDiffusion[39]onanA100GPU.
Ourdistilledencoderrunsmorethan5×fasterthantheoriginal
oneforasingleforwardandbackwardpass.
Figure8.EnvironmentMaps.Wedownload6HDRImapsfrom
Hyper-parameters.Weprovidethehyper-parametersused
polyhaventorepresentdifferentdifferentlightingconditions.For
by our pipeline in Section 4.3 (main paper). λ =
MSE randomlightingsamples,weselectonemapfromthemandapply
1000,λ = 1000,λ = 10. We use a batch of 4, a
VGG Reg arandomrotation.
learningrateof0.01foroptimization,andaCFGscaleof
50inScoreDistillationSamplingloss.InSection4.1and
Figure4(mainpaper),wesetthreepre-definedmaterialsto inFigure12.Wethenask15participantstoidentifythetex-
generateaconditioningimage.Thespecificmaterialparam- turewithgreaterfidelityinadheringtothereferencelighting
etersare(1)non-metal,non-smooth:k = 0,k = 1;(2) condition.
m r
halfmetal,halfsmooth:k =0.5,k =0.5;(3)puremetal, BRDFModelandRenderingEquation.Asdescribedin
m r
extremelysmooth:k =1,k =0.Thecolork isalways Section4.3,ourmaterialmodel[48]consistsofmetallicness
m r c
setto(1,1,1). k m R,roughnessk r R,abumpvectork n R3which
∈ ∈ ∈
BaseModels.Weusestablediffusionv1.5(SD1.5)asour isaperturbationofsurfacenormalintangentspace,andthe
basemodelfortheexperimentsinTable1,Table2andFig- basecolork c R3.Thegeneralrenderingequationis:
∈
ure 7 (main paper). Our pipeline is also compatible with
(cid:90)
otherbasemodelsfine-tunedfromSD1.5.Forexample,we
L(p,ω)= L (p,ω )f(p,ω ,ω)(ω n )dω ,
havealsousedDreamshaper,acommunityfine-tunedcheck- i i i i · p i
Ω
pointofSD1.5,togenerateavarietyofcaptivatingtextures.
where L is the rendered pixel color at the surface point p
WeincludesomeoftheresultsinFigure6(mainpaper)and
fromthedirectionω,Ωdenotesahemispherewiththesur-
Figure 11. Specifically, the results of the jackets, goblins,
facenormaln atp.L istheincidentlightrepresentedbyan
fishmen,andwolvesareobtainedusingDreamshaper. p i
environmentmap,andf istheBRDFfunctiondetermined
EnvironmentMaps.InSection4,weuserandomlyrotated
bythematerialparameters(k ,k ,k ,k ).Lcanbecalcu-
m r n c
environmentmapstorepresentdifferentlightingconditions.
latedasthesummationofdiffuseintensityL andspecular
d
Specifically, we download 6 HDRI light maps from poly-
intensityL asfollows:
s
haven.TheseHDRImapsarecapturedinastudioenviron-
ment. We show these light maps in Figure 8. We use the
middleleftoneasthefixedlightingL ∗inSection4.2. L(p,ω)=L d(p)+L s(p,ω),
(cid:90)
EvaluationandUserStudy.Forquantitativeevaluationin
L (p)=k (1 k ) L (p,ω )(ω n )dω ,
Table 1 and Table 2 (main paper), each resulting textured d c − m i i i · p i
Ω
objectisrenderedfromafixedsetof16surroundingview- L (p,ω)=
s
points.FIDandKIDarecomputedagainsttheground-truth (cid:90)
D(n )F(ω ,ω,n )G(ω ,ω,n )
renderingsofthesameviewpointsusingObjaverse’s[10] p i p i p L (p,ω )(ω n )dω ,
i i i p i
4(ω n )(ω n ) ·
originaltexture.FortheuserstudymentionedinSection5 Ω · p i · p
(main paper), we compare our method with Fantasia3D’s whereF,G,andD arefunctionsrepresentingtheFresnel
relightabletexture[8].Usingbothmethods,wegeneratethe term,thegeometricattenuation,andtheGGXnormaldis-
texturefor10testobjectsandplacetheminidenticallighting tribution [48], respectively. Following Nvdiffrec [29] and
conditions.Wealsorenderanuntexturedmeshunderthis Fantasia3D[8],thehemisphereintegrationcanbecalculated
lighttoillustratethereferencelightingdirection,asdepicted usingthesplit-summethod.B.AdditionalExperiments
BackprojectionBaseline.Wementionanalternativebase-
linebydirectlybackprojectingthesparseviewsinSection4.
WecomparethisbaselinewithourmethodinFigure9.
Base Color Roughness Metallic Bump vector
Artifacts
Baked-in lighting
"purple scallop" "futuristic helmet" "Doc Martens Boot""A sculpture of horse"
Missing Bottom Figure10.Weshowanexampleofourgeneratedmaterialmaps
Grid Representation Backprojection Baseline Our method intoprowandGIFresultswithrotatinglighting.AdobeAcrobat
needstobeusedtoviewthebottomresultsasGIFs.
Figure9.BackprojectionBaseline.Directlybackprojectingthe
gridrepresentationonto3Dmeshleadstobaked-inlighting,stitch- C.SocietalImpact
ingartifacts,andmissingareasoftexture.
Ourmethodintext-basedmeshtexturingwillenablemany
applicationsin3Dcontentcreation.First,ourmethoddras-
Evaluation on Game-Asset Data. We collect 22 meshes
tically reduces the time and expertise required for texture
from3Dgameassetswith5promptseachandevaluateour
authoring, making 3D content generation accessible to a
methodandbaselinesonthisnewdataset.Table4showsour
broader audience. Additionally, our faster inference time
methodoutperformsthebaselinesonthisnewdataset.
with improved result quality also reduces computational
costsandenergyconsumption.However,ourmethodmight
3DGameAssets KID alsobemisusedtogeneratefakecontentformisinformation.
↓
( 10 3) Nevertheless,humanscancurrentlydistinguishoursynthe-
−
×
sizedobjectsfromphotocapturesofrealobjects.
Latent-Paint[25] 9.34
TEXTure[38] 5.64
Text2tex[7] 5.21
Ours 3.43
Table4.QuantitativeEvaluationonGame-AssetData.Wecom-
pareourmethodwiththebaselinesonournewlycollecteddataset
outsideofObjaverse.Theresultsshowourmethodachievesbetter
resultsthanthebaselines.
Ablation on material bases. In Section 4.1, we render a
conditioningimageusing3pre-definedmaterialstoencom-
passabroadrangeoffeasiblematerials:(1)non-metal,not
smooth(diffuseeffect);(2)half-metal,half-smooth(mixed
effect);(3)puremetal,smooth(speculareffect).Wepresent
anablationstudyofthesematerialbasesinTable5:
MaterialBasis Ours:(1),(2),(3) w/o(1) w/o(2) w/o(3) w/o(1)(3)
FID 62.67 66.34 64.32 67.43 72.13
KID↓ 2.69 3.11 3.42 4.12 4.53
↓
Table5.Ablationonmaterialbases.
AdditionalResults.WeshowadditionalresultsinFigures11
and12.Wealsoshowanexampleofourgeneratedmaterial
mapsandGIFresultswithrotatinglightinginFigure10.“Wooden Boat” “Peony Flower”
“Ballet costume” “LAV-25, tank”
“A dog head statue” “Doc Martens Boot”
“Camra 16 mm, Paillard Bolex, H16 REX-5” “Suede Women’s Heeled Boot”
“Vintage cash register” “Pinecone”
“Hylian wolf soldier from “An astronaut wolf” “Moon necklace” “Casual gothic outfit”
Legend of Zelda”
“Japanese samurai wolf” “Pirate tribal wolf” “Thorsberg tunic” “A sculpture of monkey”
Figure11.AdditionalResults.Werotatetheobjectstoshowdifferentviewswhilefixingthelightingcondition.Relightable Texture Non-relightable Texture
Untextured Mesh
Text Prompt (reference lighting) Ours Fantasia3D Ours (Non-relightable) TEXTure Latent-Paint Text2tex
“vintage cash
register”
“a sculpture
of monkey”
“hiking boot”
“wooden boat”
“Medieval
windmill”
Figure12.AdditionalQualitativeAnalysis.Wepresentadditionalcomparisonswiththebaselines.Toevaluatethequalityofrelightable
textures,weplaceourresultsandthosefromFantasia3Dunderidenticallightingconditions.Anuntexturedmeshisalsorenderedunder
thisconditiontodenotethereferencelightingdirection.Ourresultsadheretothelightingconditionsandgenerallyoutperformallexisting
baselinesinquality.
1
weiV
2
weiV
1
weiV
2
weiV
1
weiV
2
weiV
1
weiV
2
weiV
1
weiV
2
weiV