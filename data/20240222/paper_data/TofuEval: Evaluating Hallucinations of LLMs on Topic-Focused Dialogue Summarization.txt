TOFUEVAL: Evaluating Hallucinations of LLMs
on Topic-Focused Dialogue Summarization
LiyanTang♢†,IgorShalyminov♠,AmyWing-meiWong♠,JonBurnsky♠,JakeW.Vincent♠
Yu’anYang♠,SiffiSingh♠,SongFeng♠,HwanjunSong♡‡,HangSu♠,LijiaSun♠,
YiZhang♠,SaabMansour♠,KathleenMcKeown♠
♠AWSAILabs ♡KoreaAdvancedInstituteofScience&Technology
♢TheUniversityofTexasatAustin
shalymin@amazon.com
Abstract 50~MediaSum
5 LLMs
50~MeetingBank
Singledocumentnewssummarizationhasseen
Topics (3/dialogue)
substantial progress on faithfulness in recent
Topic-focused Summ
years, driven by research on the evaluation
of factual consistency, or hallucinations. We
askwhethertheseadvancescarryovertoother
text summarization domains. We propose a
new evaluation benchmark on topic-focused Error Type & 1.5K
Expert
dialoguesummarization,generatedbyLLMs Explanation Linguists Summary
ofvaryingsizes. Weprovidebinarysentence- (Sent level) (4k Sents)
levelhumanannotationsofthefactualconsis-
tencyofthesesummariesalongwithdetailed
explanationsoffactuallyinconsistentsentences. Consistency Relevance Completeness
OuranalysisshowsthatexistingLLMshallu-
cinatesignificantamountsoffactualerrorsin Figure1: TOFUEVALcontains1.5Ktopic-focusedsum-
thedialoguedomain,regardlessofthemodel’s mariesfromtwodialoguesummarizationdatasets. We
size. Ontheotherhand,whenLLMs,includ- ask expert linguistic annotators to evaluate complete-
ingGPT-4,serveasbinaryfactualevaluators, ness, relevance and factual consistency of each sum-
theyperformpoorlyandcanbeoutperformed mary,alongwithexplanationsanderrortypesforfactu-
byprevailingstate-of-the-artspecializedfactu- allyinconsistentsentences.
alityevaluationmetrics. Finally,weconducted
an analysis of hallucination types with a cu-
rated error taxonomy. We find that there are Wangetal.,2023),theymaynotperformaswell
diverseerrorsanderrordistributionsinmodel- inotherless-exploredsummarizationdomains.
generatedsummariesandthatnon-LLMbased Existingstudiesprimarilyinvestigatenewssum-
metricscancaptureallerrortypesbetterthan
marizationbenchmarks,suchasTangetal.(2023a);
LLM-basedevaluators.1
Labanetal.(2022);Pagnonietal.(2021);Fabbri
etal.(2021). AlongsidethefindingthatLLMsare
1 Introduction
capableofgeneratingsummariesofnewsarticles
Recently, the field of automated text summariza- that align with human preferences (Goyal et al.,
tion has been increasingly inclined toward using 2022;Zhangetal.,2023),weask: canLLMsgen-
LargeLanguageModels(LLMs)toevaluatemodel erate factually consistent summaries without
outputs(Fuetal.,2023;Gaoetal.,2023;Madaan hallucinations for non-news domains? Given
etal.,2023). Giventheconsequentialnatureofthis thepotentialbenefitsthatdialoguesummarization
trend,weask: areLLMsuptothetaskofevalu- couldbringtootherareas,suchasenhancingpro-
atingmodeloutputs? Whilerecentworkonnews ductivityinmeetingsorstreamliningcustomerser-
summarizationhasshownthatLLMs’performance viceinteractions,wefocusondialoguesummariza-
atevaluatingthefactualconsistencyofgenerated tionasacasestudyinthiswork.
news summaries is promising (Luo et al., 2023; Weaddressthetwoquestionsmentionedabove
byintroducinganewbenchmarkdatasetTOFUE-
1Wereleasethebenchmarkdatasetwithexpertannotations
VAL,whichtargetsTopic-focusedDialoguesum-
atgithub.com/amazon-science/tofueval.
†WorkdoneasaninternatAmazon. marizationEvaluationoffactualconsistency. The
‡WorkdonewhileatAmazon. benchmarkdatasetcontainssummariesgenerated
4202
beF
02
]LC.sc[
1v94231.2042:viXra
rehtruF etatonnAbyfiveLLMsofvarioussizes. Summariesinthe effortstocollecthuman-annotateddataforassess-
benchmark are focused on specific topics in the ing the effectiveness and correlation of different
dialogues due to the length of the dialogues and evaluationmetricswithhumanjudgmentsindetect-
the fact that topics can hold varying degrees of inghalluciantedcontentsingeneratedsummaries
importancetodifferentusers. (Fabbrietal.,2021;CaoandWang,2021;Maynez
In TOFUEVAL,weengageprofessionallinguis- etal.,2020).2
ticdataannotatorstoperformbinaryfactualityeval- Our proposed benchmark TOFUEVAL aligns
uationofthesentenceswithineachsummaryand with these efforts but differs from prior work as
writeexplanationsforthesentencestheydetermine follows (summarized in Table 1): (1) unlike ex-
tocontainhallucinatedcontents(Section3.4). Hu- isting evaluation benchmarks that contains non-
man annotations reveal that LLMs are prone to LLM-generated summaries, TOFUEVAL focuses
makingasubstantialnumberoffactualerrors,and on LLM-generated summaries. Contrasting with
incontrasttocommonbelief,largerLLMsdonot SUMMEDITS(Labanetal.,2023),whichproduces
necessarilygeneratemorefactuallyconsistentdia- factually inconsistent summaries by editing cor-
loguesummariesthansmallermodels(Section4). rectLLMoutputs,wedirectlyidentifyfactualer-
Moreover,allLLMswestudied(includingGPT- rors in LLM-generated summaries. (2) TOFUE-
4),whenusedasbinaryfactualconsistencyevalu- VAL focuses on dialogue summarization. Even
ators,performpoorlyatdetectingerrorsinLLM- though DIALSUMMEVAL (Gao and Wan, 2022)
generatedsummariesthatfocusonthemaintopic sharesthisfocus,sourcedocumentsinTOFUEVAL
ofadocumentaccordingtohumanjudgment(Sec- are considerably longer than those in DIALSUM-
tion5). Incontrast,non-LLM-basedfactualitymet- MEVAL, which are based on short conversations
ricsdemonstratesuperiorperformancecompared in the SAMSum corpus (Gliwa et al., 2019). (3)
tomostLLMswetested,andtheyhavetheadded Human evaluation from prior work comes from
advantage of smaller model size and lower infer- diverse sources, such as crowd-workers in SUM-
encecosts. Ourerroranalysisfurtherrevealsthat MEVAL(Fabbrietal.,2021)and FRANK(Pagnoni
thosenon-LLMmetricsarebetteratcapturingall etal.,2021),andtrainedcollegestudentsfromDI-
errortypeswhencomparedtoLLMs. ALSUMMEVAL (GaoandWan,2022). TOFUEVAL
Ourcontributionscanbesummarizedasfollows: consistsofannotationsfromprofessionallinguistic
(1)wearethefirsttointroduceatopic-focuseddi- dataannotators.
aloguesummarizationbenchmarkTOFUEVALfor
Detecting Hallucinations Common automatic
factual consistency evaluation, which consists of
metrics for text summarization such as ROUGE
LLM-generatedsummarieswithexpert-annotated
(Lin, 2004), BLEU (Papineni et al., 2002), and
factualitylabelsandexplanations;(2)wesystem-
BERTScore (Zhang* et al., 2020) have poor cor-
aticallyevaluateLLMsassummarizersacrossrele-
relationswithhumanjudgementonfactualconsis-
vance,completeness,andfactualconsistency,and
tency (Kryscinski et al., 2019; Falke et al., 2019;
weshowthatLLMsperformpoorlyonfactualcon-
GaoandWan,2022;Tangetal.,2023b). Therefore,
sistencyinthedialoguedomain;(3)onfactuality
a few non-LLM-based metrics have been devel-
prediction, our evaluation benchmark shows that
oped to detect factuality errors (Kryscinski et al.,
withtheexceptionofGPT-4,allotherLLM-based
2020;GoyalandDurrett,2021;Labanetal.,2022;
evaluatorperformanceswestudiedareinferiorto
Fabbrietal.,2022;Zhaetal.,2023). Morerecently,
non-LLMfactualitymetrics;(4)weconductaner-
LLMshavebeenshowntohavesuperiorzero-shot
roranalysisusingacuratederrortaxonomy,reveal-
performanceatfactualconsistencyevaluationun-
ingthatnon-LLMfactualitymetricscancaptureall
der certain evaluation settings, highlighting their
errortypesbetterthanLLM-basedevaluators;(5)
potentialasstate-of-the-artfactualconsistencyeval-
wereleaseTOFUEVALwithhuman-annotateddata
uators(Luoetal.,2023;Wangetal.,2023).
toenablefurtherresearchintoimprovedautomated
Ashallucinationsfrommorerecentmodelsare
evaluationofsummaryfactuality.
hardertodetect(Tangetal.,2023a),were-evaluate
non-LLM-basedandLLM-basedfactualitymetrics
2 RelatedWork
using LLM-generated summaries within the con-
Factual Consistency Evaluation Benchmarks
2Weusethetermsfactualinconsistency,factualerrorsand
Intextsummarization,therehavebeensignificant hallucinationsinterchangeablyinthiswork.SUMMEVAL FRANK SUMMAC AGGREFACT DIALEVAL SUMMEDITS TOFUEVAL
SummariesfromLLMs ✗ ✗ ✗ ✗ ✗ ✓ ✓
Non-EditedSummaries ✓ ✓ ✓ ✓ ✓ ✗ ✓
Fine-GrainedAnnotations ✗ ✓ ✓— ✓— ✗ ✗ ✓
NaturalLanguageExplanations ✗ ✗ ✗ ✗ ✗ ✗ ✓
DocumentDomain news news news news dialogue mixed dialogue
SummaryType generic generic generic generic generic generic topic-focused
Annotators crowd crowd mixed mixed students trainedann. linguists
AverageDocumentLength 408 595 583 496 130 705 950
Table1: ComparisonbetweenTOFUEVALandexistingfactualconsistencyevaluationbenchmarksontext
summarization. Oursisthefirstthatfocusesondifferenttopicswithinadocumentandprovidesexpert-annotated
factualconsistencylabelsforsummarysentenceswithwrittenexplanations. Weconsidersentence-levelandmore
granularannotationsasfine-grainedannotations. SomedatasetsinSUMMACandAGGREFACTincludethistypeof
annotationpartially(✓—). DIALEVALstandsforDIALSUMMEVAL.
textofourdialoguesummarizationbenchmarkTO- community welfare, including budget allocation,
FUEVAL. Wefindthatnon-LLM-basedmetricscan infrastructureplanning,andcrimeprevention.
surpassmostLLM-basedevaluators. Nevertheless, Inoursamplingprocess,weoptfordocuments
allautomatedfactualitymetricsstillperformquite withlengthsrangingfrom800to1,200words. This
poorly, underlining the challenging nature of the decisionwasmadetoensurethat(1)theselected
problemandthesubstantialroomforimprovement documentlengthsfitthemaximuminputsizeofall
inautomatedfactualinconsistencydetection. themodelsbeingevaluatedand(2)thedocuments
were sufficiently long to potentially elicit factual
3 TOFUEVAL Benchmark inconsistencyerrorsinLLM-generatedsummaries.
Optingforlongerdocumentsmightposechallenges
Ourtopic-focuseddialoguesummarizationbench-
on manual evaluation. The benchmark statistics
mark TOFUEVAL is constructed as follows: (1)
are shown in Table 5. We randomly sample 50
sampledocumentsfromtwopubliclyavailabledi-
documentsfromtheoriginaltestsplitsofeachof
alogue summarization datasets (Section 3.1); (2)
thesedatasetsforthebenchmarkconstruction.
create a variety of topics for sampled documents
(Section3.2);and(3)generatetopic-focusedsum- 3.2 TopicGeneration
maries with various LLMs (Section 3.3). The re-
TheimpressiveperformanceofLLMsenablesthe
sultingbenchmarkcontains100dialoguesand15
generation of a variety of summaries for a single
LLM-generatedsummariesperdialogue;(4)lastly,
longdialoguebasedondifferentpointsofinterest
weprovidefine-grainedhumanannotationsonthe
inthedialoguewithvaryingdegreesofimportance
topicsandthegeneratedsummariesfordimensions
toreaders. Insteadofaskingforgenericsummaries
includingfactualconsistency,relevance,andcom-
(i.e.,summarizingadocumentinafewsentences),
pleteness (Section 3.4). The dataset construction
theperformanceofwhichhasalreadybeenheav-
pipelineisillustratedinFigure1.
ilyevaluated(Table1),weevaluateLLMs’perfor-
manceingeneratingfactuallyconsistentsummaries
3.1 DocumentSelection
forspecifictopicswithinthesampleddocuments.
Weselectdocumentsfromtwopubliclyavailable Herewebroadlydefineatopicasasubjectofdis-
dialoguesummarizationdatasets:
cussioninadocumentrelatedtoanentity,anevent,
oranissuethatreadersofthedocumentwouldlike
MediaSum (Zhu et al., 2021) is a large-scale
toknowabout(Hallidayetal.,2014).
dialoguesummarizationdatasetwithpublicinter-
SinceMediaSumandMeetingBankdonotcome
viewtranscriptsfromNPRandCNN.Thedataset
withhuman-writtentopics,andidentifyingtopics
features multi-party conversations across various
manuallyistime-consuming,wechosetoidentify
subjects,suchaspolitics,economics,andUSnews.
maintopicsinadocuementwithanLLMusinga
MeetingBank (Huetal.,2023)isasummariza- zero-shotpromptinAppendixC.1. Wegenerated
tiondatasetwithcitycouncilmeetings. Thesemeet- threetopicsforeachdocument.3 Notethatalthough
ings involve discussion and decisions about a di-
3Given the length of the dialogues in TOFUEVAL, we
verserangesubjectscentraltolocalgovernanceand restrictthenumberoftopicstothreeforeachdocument.weprompttheLLMtogeneratemaintopics,ourhu- that is being discussed or presented in the docu-
manevaluation(moredetailsinSection3.4)shows ment. Marginaltopicsarethosethatareexplored
that while the majority of LLM-generated topics less thoroughly in the documents. More detailed
arecloselyrelevant,asmallproportionofourcol- definitions can be found in Appendix F.1. Main
lectedtopicsaremarginallywithinthecontextof topicsmakeupapproximately75%ofthetopicsin
thedocument. Wedecidedtoretainthesemarginal TOFUEVALaccordingtoourcategorizationresults
topicsbasedontheassumptionthatmarginaltopics (Table5).
canalsobeusefultosummaryreaders.
FactualConsistency Asummarysentenceisfac-
3.3 SummarizationModelSelection tually consistent with a document if the sentence
is stated or implied by the document; otherwise,
Weconstructthesummarizationfactualconsistency
it is inconsistent. For any sentences deemed in-
evaluationbenchmarkbasedonsummariesgener-
consistent,theannotatorwroteabriefexplanation
ated by LLMs. This enables the creation of mul-
about the inconsistency. We aggregate sentence-
tiple summaries per dialogue and thus allows for
level binary factuality labels to obtain labels for
easyscalingofthedatasetwithlesshumaneffort.
theentirecorrespondingsummary: asummaryis
We chose to evaluate the summarization per-
factuallyconsistentwiththedocumentifallsum-
formance of one proprietary LLM, OpenAI’s
marysentencesarelabeledasfactuallyconsistent;
GPT-3.5-Turbo, and four open-source models,
otherwise,thesummaryisfactuallyinconsistent.
Vicuna-7B(Chiangetal.,2023)andWizardLM-
7B/13B/30B(Xuetal.,2023). Moredetailsabout
Relevance Arelevantsummaryfocusesontopic-
themodelsandourmodelselectioncanbefound
relatedcontentfromasourcedocument. Eachsum-
inAppendixB.Weusedazero-shotpromptinAp-
marywasassignedarelevancescorerangingfrom
pendix C.2 for topic-focused text summarization.
0to1,with1indicatinganon-topicsummary.
Unlessotherwisestated,wesetthemodeltemper-
ature to 0.7 and left the values of the remaining Completeness Acompletesummarysummarizes
hyper-parametersunchangedforallmodelsacross allinformationinthedocumentthatisrelatedtothe
allexperimentsettingsinthiswork. topic. Eachsummarywasassignedacompleteness
score ranging from 0 to 1, with 1 indicating the
DatasetSplits Insummary, TOFUEVAL consists
highestlevelofcompleteness(AppendixF.2).
of50documentsperdataset,3generatedtopicsper
document,and5summariesfromdiverseLLMsper
3.5 DialogueSummarizationvsNews
topic,resultingin50×2×3×5=1,500summaries
Summarization
(refertoTable5formoredetails). Further,were-
moved23modeloutputsthatweredeemedasnon- Comparedtonewssummarization,dialoguesum-
summariesbyhumanannotators,resultingin1,479 marization involves unique challenges due to the
summaries (3,966 summary sentences). We then informalandcolloquialnatureofdialogues,which
randomly split the benchmark into development requiressummarizationmodelstohandlesubtleties
and test sets, with a 70%/30% development/test andnoises. Additionally,dialoguesareinherently
partitionsplitondistinctdocuments. interactive, which often involves a mix of ques-
tions,answers,andopinionsamongdifferentspeak-
3.4 AnnotationCollection ers. Thisinteractionrequiresasophisticatedunder-
Using generated summaries, we collected high- standingbythemodelsofthecontextualrelation-
quality annotations from professional linguistic shipsbetweenthepiecesofinformationdiscussed
dataannotatorsforeachdimensiondefinedbelow.4 inthedialogue. Thesecomplexitiesmakedialogue
Thefullannotationinstructionsanddetailsabout summarizationchallengingandsusceptibletofac-
ourqualitycontrolcanbefoundinAppendixF. tualinconsistencies(Section4). Thisfurthermakes
it difficult to identify hallucinations in generated
TopicCategorization Wemanuallycategorized
summariesin TOFUEVAL(Section5).
topicswithinadocumentintomainandmarginal
topics. Main topics refer to central information 4 Results: LLMsasSummarizers
4WedonotevaluatefluencyandcoherencesinceLLMs
Weshowtheerrorrateingeneratedsummariesin
generallyexcelinthesedimensions(Goyaletal.,2022;Zhang
etal.,2023). Table2onbothmainandmarginaltopics. Overall,Sentence-Level(%Error) Summary-Level(%Error)
Summ.
Model MediaSum Meetingbank MediaSum Meetingbank
Main Marginal Main Marginal Main Marginal Main Marginal
Vicuna-7B 19.6 35.8 17.6 36.8 42.7 55.4 33.0 58.0
WizardLM-7B 29.1 36.4 21.3 42.4 49.6 54.8 35.6 49.0
WizardLM-13B 17.4 27.2 15.8 25.4 35.9 44.4 41.3 46.8
WizardLM-30B 14.6 27.2 13.7 26.2 35.9 48.2 31.5 44.8
GPT-3.5-Turbo 8.8 13.6 4.4 9.4 22.2 27.2 10.9 19.8
Average 17.5 27.8 14.4 27.8 37.2 46.0 30.4 43.6
Table2: Percentageofsentence/summary-levelfactualinconsistenciesacrossthefivemodelsusedinTOFUE-
VAL. Weshowtheerrorratesformain-topicsummariesseparatelyfromthoseformarginal-topicsummaries. We
highlightthe lowest and secondlowest errorrates. SeeexamplesofannotatedsummariesinTable12and13.
Figure2: ErrordistributionoverfactuallyinconsistentsummarysentencesforTOFUEVAL(left)andforeachsum-
marizerovermain/marginaltopics(right). Seeerrordistributionsoverallsummarysentencesforeachsummarizer
overmain/marginaltopicsinAppendixFigure4.
LLMswestudiedmakeasignificantamountof ExtrinsicInformation. Wefindthatwhenthetopic
factualerrors,especiallyonthesummarylevel. is barely mentioned in the document, models try
We further investigate the distribution of dif- torelyontheirknowledgetomakeinformedinfer-
ferenthallucinationtypesinTOFUEVALwithour encesaboutthetopic,bringingunsupportedinfor-
curated error taxonomy. Note that our taxonomy mationintothesummary. AnexceptionisGPT-3.5-
closelyresemblesthatofTangetal.(2022),which Turbo,whichgeneratesfarfewerExtrinsicInforma-
isbasedontheSAMSumdialoguedataset(Gliwa tionformarginaltopics. Comparedtoothersum-
et al., 2019). Due to the complexity of the long marizersthatgenerateunsupportedsentences,we
dialoguesinTOFUEVAL,weextendthetaxonomy findthatGPT-3.5-Turbooftenhandlesrequestsfor
of Tang et al. (2022) with new error types, such marginal-topic summaries by including off-topic
asreasoningerror andstatingopinionasfact. A contentoroccasionallyexplicitlysaying,“thetopic
summary of our curated error taxonomy for the
isnotdiscussedinthedocument”.5
benchmarkisprovidedinAppendixFigure5. We MorefindingscanbefoundinAppendixD.
leverage the error taxonomy to enrich all binary
5 Results: LLMsasEvaluators
factualinconsistencyannotationsinthebenchmark.
Additionaldetailsaboutthetaxonomycurationpro- We now move on to consider the use of LLMs
cessanderror-typeannotationcanbefoundinAp- asevaluatorsoffactualconsistencyratherthanas
pendixG. summarizers. Wefirstpresentanevaluationoftheir
performanceatmakingbinaryfactualconsistency
LLMs tend to produce more factually incon- predictionsforbothsummariesandsummarysen-
sistent summaries when prompted to focus on tences(Section5.1). Wethenprovideanerror-type
a marginal topic, especially with extrinsic in- analysis, investigating which error types models
formation error. As shown in Figure 2, when fail to detect well (section 5.2). Finally, given
promptingmodelsforsummariesaboutmarginal
5Furtheroptimizationofpromptstoreducetheerrorrate
topics,allsummarizersgeneratesignificantlymore forspecificerrortypesisbeyondthescopeofthecurrentwork.that LLMs have the ability to generate critiques we decided on a threshold value for each metric
ofmodeloutputandprovideexplanationsfortheir usingthedevelopmentsetandreportthetestsetre-
factualconsistencyjudgments(Madaanetal.,2023; sultsassumingtheselectedthreshold. Wechosethe
Saundersetal.,2022),weconsidertheaccuracyof thresholds for sentence-level and summary-level
model-generatedexplanationsbycomparingthem evaluations separately. Text that receives a value
tohuman-writtenexplanations(AppendixH). abovethethresholdisconsideredfactuallyconsis-
tentwiththedocument;otherwise,itisconsidered
EvaluatorSelection Foracomprehensivecom-
inconsistent. Foroursentence-levelandsummary-
parison,weincludethefollowingnon-LLMbased
level evaluations, the input text was a summary
SOTAfactualitymetrics: SummaC-ZS,SummaC-
sentenceandawholesummary,respectively.
CV(Labanetal.,2022),QAFactEval(Fabbrietal.,
2022),andAlignScore(Zhaetal.,2023). Wealso Obtaining Predictions and Explanations from
includethefollowingproprietaryandopen-source LLMs Wetestedtwomethodsforobtainingfac-
LLMsasfactualconsistencyevaluators: (1)GPT-4 tual consistency labels. First, we directly asked
(OpenAI, 2023); (2) GPT-3.5-Turbo; (3) Vicuna- LLMs to provide binary labels (DIR). In Ta-
13B/33B (Chiang et al., 2023); (4) WizardLM- ble3,weshowtheresultsobtainedbyasentence-
13B/30B (Xu et al., 2023). For all LLMs in the level prompt for all LLM-based metrics for both
aforementioned list we used a zero-shot configu- sentence-levelandsummary-levelevaluation.6 We
ration, as certain LLMs in this list are unable to run all LLMs three times per completed prompt
accommodate few-shot evaluations due to input andreporttheaverageperformance.
lengthconstraints. Inanycase,ithasbeenobserved Next, to obtain explanations from LLMs, we
that few-shot examples do not consistently yield attemptedtoelicitChain-of-Thoughtreasoningfol-
superioroutcomesincomparisontozero-shotsce- lowing (Wei et al., 2022). We adjusted the pre-
narios(Labanetal.,2023;Luoetal.,2023). More vious prompt, asking the LLM to provide expla-
detailsaboutmodelselectionareinAppendixB. nations for its decisions in addition to providing
binaryjudgments(EXP). Weextractedbinarypre-
5.1 PredictingFactualConsistency
dictionsfromtheoutputsofthispromptformodel
We first measure the performance of the selected self-agreement evaluation (presented later in this
factualconsistencyevaluatormodelsviaabinary section), and we extracted explanations for ex-
prediction task. For any evaluation model M, a planationevaluation(AppendixH).7 Promptsfor
dialogued,andsomegeneratedcontentc,weask summary-level and sentence-level evaluation for
M topredictwhethercisfactuallyconsistentwith bothmethodscanbefoundinAppendixC.3.
thecorrespondingdialogued:
Non-LLMfactualconsistencyevaluationmod-
M(d,c) ∈ {consistent,inconsistent}. els perform well. As shown in Table 3, GPT-
4 achieves the best performance when it comes
FollowingLabanetal.(2022);Fabbrietal.(2022);
to factual consistency evaluation across datasets
Tang et al. (2023a); Luo et al. (2023), we mea-
and topic types most of the time. Further, most
suredmodels’performanceusingthebalancedac-
of the second-best evaluators are the non-LLM-
curacy (BAcc) method, which takes into account
basedfactualitymetricsacrossallconfigurations,
theimbalanceoffactuallyconsistentandfactually
outperforming LLMs, including GPT-3.5-Turbo,
inconsistent summaries. We analyzed the results
byalargemargin. Whenevaluatingthemain-topic
based on both sentence-level and summary-level
summaries of MediaSum data, AlignScore even
prediction performance. Unlessstated otherwise,
surpasses GPT-4 in performance at both the sen-
allevaluationresultsshownherearebasedonthe
tencelevelandthesummarylevel. Itisworthnot-
testset.
ingthatthenon-LLM-basedevaluatorshavefaster
Obtaining Predictions from non-LLM based inferencespeed,costmuchless(comparedtoAPI
FactualityMetrics Thenon-LLM-basedmodels
6SeeSection3.4forhowweobtainedsummary-levella-
weusedtakeasinputasourceandapieceofsum-
belsfromsentence-levellabels.Performanceforthesummary-
marytexttobeevaluated,andtheyreturnascore levelpromptcanbefoundinAppendixTable9.
fortheevaluatedtextwithinaparticularrangeof 7Binary prediction performance of this prompt can be
found in Appendix Table 10, where we show that prompt-
continuousvalues. Higherscoressuggestmorecon-
ing for explanations does not improve performance on the
sistentsummaries. FollowingLabanetal.(2022), binarypredictiontask.Sentence-Level(BAcc↑) Summary-Level(BAcc↑)
Model Evaluation
Type Model MediaSum MeetingBank MediaSum MeetingBank
Main Marginal Main Marginal Main Marginal Main Marginal
- Baseline 50.0 50.0 50.0 50.0 50.0 50.0 50.0 50.0
SummaC-ZS 66.1 73.9 63.9 80.6 62.7 64.1 58.1 72.4
Non- SummaC-CV 67.6 73.0 62.6 77.3 61.2 66.5 52.4 72.9
LLM QAFactEval 53.9 74.0 58.0 75.8 61.4 74.2 55.1 68.2
AlignScore 69.2 76.2 61.2 78.6 65.5 72.1 63.4 71.8
Vicuna-13B 54.0 54.8 49.6 61.9 55.6 59.1 51.2 59.2
Open
Vicuna-33B 51.0 51.1 53.6 48.4 52.5 53.4 53.2 51.0
Source
WizardLM-13B 59.8 53.5 58.8 56.6 57.0 54.5 54.6 58.0
LLM
WizardLM-30B 54.5 53.9 53.5 53.4 53.3 54.4 53.0 53.2
Prop. GPT-3.5-Turbo 61.6 68.9 56.0 65.0 59.6 65.8 63.2 65.7
LLM GPT-4 64.9 80.2 67.5 90.3 63.7 78.9 74.7 83.1
Table3: Sentence-levelandsummary-levelbalancedaccuracy(BAcc)forfactualconsistencyevaluatorson
thetestsetofTOFUEVAL. ForLLM-basedmethods,weshowsummary-levellabelsbyaggregatingsentence-level
labels,asitachievesbetterperformancethandirectlypredictingconsistencylabelsonwholesummaries. Allresults
forLLMsaretheaverageofthreeruns. Notethatabaselinemethodthatalwayspredictsinconsistentorconsistent
achieves 50% balanced accuracy. We highlight the best and secondbest results. Prediction results for both
Main-topicsummariesandMarginal-topicsummariesareshown.
calls), and only need a 16GB or smaller GPU to Evaluation MediaSum Meetingbank
completethepredictiontask. Model
DIR EXP DIR EXP
For the open-source LLMs we tested, the bal-
Vicuna-13B 0.35 0.11 0.38 0.00
anced accuracy scores are all between 50% and Vicuna-33B 0.37 0.18 0.29 0.13
WizardLM-13B 0.47 0.33 0.54 0.18
60%,whichisbarelybetterthanthebaseline. Al-
WizadLM-33B 0.50 0.39 0.35 0.34
though these models are shown to generate out-
GPT-3.5-Turbo 0.57 0.44 0.59 0.51
putspreferredbyhumanscomparedtothepropri-
GPT-4 0.96 0.95 0.90 0.91
etarymodelsoveravarietyofinstruction-following
tasks (Xu et al., 2023; Li et al., 2023), they are Table4: Sentence-levelmodelself-agreementinpre-
not equipped with the discrimination skills suffi- dictingfactualconsistencylabelsonthewholetest
cient to perform this task well. Further, larger set of TOFUEVAL. Models are run 3 times and self-
agreementsarecalculatedbyCohen’skappa.
open-source models do not outperform their
smallercounterpartsonmostsettings. Forexam-
ple,Vicuna-33B’sperformanceis13%worsethan
Vicuna-13Bonmarginal-topicsummariesofMeet- hasaperformancegapofapproximately10-20%on
ingBankdata,anditisevenworsethanthebaseline. thesentence-levelpredictiontaskforbothdatasets.
Somepossibleexplanationsforthismightinclude Wehypothesizethatthisisduetothefactthatmain-
thatthesemodelsarenotpre-trainedonthistype topicsummariesdonotcontainalargeproportion
ofdata,ortheyarenotlargeenoughtodevelopthe ofextrinsicinformation(Figure2)whichwefind
emergentdiscriminationcapabilitiesforthistype modelscandetectrelativelywell(Section5.2). As
oftaskcomparedtotheproprietarymodels. mentionedpreviously,theopen-sourceLLMswe
Overall,thesefindingsraisecautionagainstun- testedarenotequippedwiththeskillsnecessaryto
questioning admiration for using cutting-edge performthisdiscriminationtask,hencewedonot
LLMsasevaluators. notice any consistent performance improvement
on the marginal-topic summaries, which seems
Itismorechallengingforallmodelstestedtode- slightly easier for other model types. Overall,
tecterrorsinmain-topicsummaries. Asshown thereisstillalargeroomforimprovementwhen
inTable3,forbothnon-LLM-basedfactualitymet- it comes to efficient, cost-effective factual in-
rics and proprietary LLMs, they are on average consistency detection, especially for main-topic
about 10% worse at detecting errors from main- summaries,whereexistingmodels’performanceis
topicsummaries,whereasthebestmodel,GPT-4, quiteclosetobaselineperformancewhichalwaysFigure3: Recallofsummaryfactualinconsistencypredictionsbyerrortypes. Non-LLMbasedfactuality
metricsarebetteratcapturingerrorsthanLLM-basedevaluatorsacrossallerrortypes.
predicts inconsistent or consistent. We explore sentence level. The results revealed a substantial
differences in the error types that models fail to correlationonMediaSumdata(ρ=0.79,p=0.02)
identifyinSection5.2. and a highly significant correlation on Meeting-
Bank data (ρ = 0.99, p = 0.00). In other words,
Most LLMs, especially the smaller ones, lack
thereisastronglinearrelationshipbetweenmod-
consistency over multiple predictions gener-
els’performanceandtheirselfagreement. When
atedwiththesameprompt. Wecalculateeach
modelsexhibitgreaterself-consistencyinpredic-
model’s self-agreement by comparing its predic-
tions, they allocate a higher probability mass to
tionsonthefactualityofsummarysentences.8 The
these predictive labels. Given that this correlates
sentence-levelselfagreementacrossthethreeruns
well with the models’ balanced accuracy, when
foreachmodel(asCohen’skappa)isprovidedin
thinkingofthesemodelsaltogether,weconclude
Table4,basedonthebinarypredictionresultsfrom thattheyarewell-calibratedforthistask.9
directbinarylabelpredictions(DIR)andthelabel
MorefindingscanbefoundinAppendixE.
predictionswithexplanations(EXP)(Section5.1).
WeobservethatGPT-4hasnear-perfectagreement 5.2 ErrorAnalysis
acrossallsettings,suggestingthatthemodelmakes FollowingTangetal.(2023a),weanalyzedtheeval-
consistent predictions, while the remaining mod- uatormodels’error-typedetectionperformancein
els have fair to moderate agreement (κ between termsofrecall.10 Wedividedallfactuallyinconsis-
0.2and0.6)mostofthetimeforDIRpredictions. tentsummarysentencesintoseveralsubsets,each
Interestingly, we observe that asking the model ofwhichcontainsonlyoneerrortype. Giventhe
foranexplanationinadditiontomakingabinary evaluators’performanceasshowninTable3, we
predictiononfactualitylowersitsCohen’skappa selectedasubsetofrelativelystrongevaluatorsfor
score. This is more apparent for the smaller 13B thisanalysisandshowtheresultinFigure3.
models,wherewehaveamaximumdropof0.38
Non-LLMbasedevaluationmetricsarebetter
inagreement. Wehypothesizethatpromptingmod-
at capturing all error types. In Figure 3, we
elstogenerateexplanationsalongwiththebinary
show the performance of the LLM-based evalua-
predictionaddsanextralayerofcomplexitytothe
torsinblueandthenon-LLMbasedevaluatorsin
task,yieldinglessdeterministicresultsandcausing
orange. We observe that all evaluators perform
lowerself-agreementcomparedtothedirectbinary
fairlywellatidentifyingwhatwetermedExtrinsic
labelpredictiontask.
Information. Thismightbeduetothefactthatthis
There is a strong positive correlation between type of error primarily involves unfamiliar noun
a model’s self-agreement and its performance phrasesorevents(relativetothedocument),which
atfactualconsistencyprediction. Wecomputed wesupposefacilitatesmodels’detectionofthiser-
Pearsoncorrelationcoefficientsρtoassesstherela- rortype. Thatsaid,wedofindthatGPT-3.5-Turbo
tionshipbetweenmodels’performance(asBAcc)
9Ourinsighthereisbasedonconfigurationswithatemper-
andtheirselfagreement(asCohen’sKappa)atthe atureof0.7.Thisfindingmaynotholdforothertemperatures.
Forexample,atemperatureofzerowouldleadtodeterministic
8BecauseproprietaryLLMsdonotprovidetokenprobabil- outputsandhenceperfectagreement,butbalancedaccuracy
itieslikeopen-sourceLLMs,forafaircomparisonwechose maystillbelowinsuchcases.
todirectlycomparethreerunsfromeachmodelandcalculate 10Precisioncannottechnicallybedefinedforeacherrortype
Cohen’skappaonthethreepredictions. becauseevaluatormodelsdonotpredicterrortypes.only detect 50% of such errors—approximately engineering to identify better prompts for LLMs,
30%lowerthantheratefornon-LLMbasedmet- whichcouldleadtoimprovementsinfactualconsis-
rics. Fortheremainingerrortypes,thereisalarge tencyorimproveddetectionoffactualerrors. We
gapinthedetectionratebetweenLLM-basedand leavethisinvestigationtofuturework. Finally,we
non-LLM based metrics. It is possible that the donotevaluatelargersetofLLMsasfactualconsis-
testedLLMsmayperformbetteratidentifyingcer- tencyevaluatorssinceGPT-3.5-TurboandGPT-4
tainerrortypeswithbetterpromptdesign,butwe are shown to be representative about the recent
leavethisforfuturework. LLMs’performanceinfactualconsistencyevalua-
Note that recall and balanced accuracy are tion(Labanetal.,2023). Despitetheacknowledged
complementary metrics that lend insight into limitations,wehopethatourproposedbenchmark
evaluators’ behavior in our analysis. Having a andtheinsightswillinspirefutureworkfocusing
high recall does not necessarily suggest high bal- onenhancingautomatedevaluationmetricsandthe
anced accuracy, and vice versa. For example, al- development of summarizers with better factual
thoughGPT-4doesnotcaptureasmanyerrorsas consistencyperformance.
non-LLM based models, it achieves a higher bal-
anced accuracy (see Table 3). However, the non- Acknowledgements
LLMbasedmetricsdosurpassGPT-3.5-Turboin
Theauthorswishtoexpressourgratitudetoouran-
both recall and balanced accuracy with most set-
notationteam,whosevitalcontributionshavebeen
tings,suggestingthattheirperformanceissuperior.
instrumentalinthisproject: MarikaHall,Hoyeol
6 Conclusion Kim, Paul Goeden, Elvira Magomedova, Teddy
Mutiga,DanielNorth,GiuseppinaSilverstri,Helen
Wehaveproposedanewfactualconsistencyeval- Satchwell,AnnaStallman,AidanThies,Michael
uation benchmark TOFUEVAL for topic-focused Valentekovic,JenniferWon,CarolinaKocuba,Neil
dialoguesummarizationwithLLM-generatedsum- Morrissey, Andy Bridges, Derek Chao, Mei Le-
maries. WefindthatLLMsservingassummariz- ung,MatthewMahoney,AndrewMcNally,Francis
ers make numerous and diverse hallucinations in O’Brien,AlexMowen,andNicoleLaManna. All
theirsummaries. Furthermore,bymeasuringbal- themembersofourannotationteamarebasedin
ancedaccuracyandanalyzingmodels’errortypes, theU.S.andarepaidacompetitivehourlyratethat
we conclude that it is still challenging for both isbenchmarkedagainstsimilarrolesintheU.S.
LLM-basedevaluatorsandexistingstate-of-the-art
non-LLMbasedfactualitymetricstodetectawide
range of hallucinations in dialogue, although the References
latterexhibitslightlybetterperformance.
ShuyangCaoandLuWang.2021. CLIFF:Contrastive
learningforimprovingfaithfulnessandfactualityin
Limitations
abstractive summarization. In Proceedings of the
2021ConferenceonEmpiricalMethodsinNatural
Ourwork hasafewlimitations. First, inourpro-
LanguageProcessing,pages6633–6649,Onlineand
posed TOFUEVAL benchmark, we do not ask hu- Punta Cana, Dominican Republic. Association for
manevaluatorstoannotatefactualconsistencyer- ComputationalLinguistics.
rors that may span beyond a single sentence due
Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
to the already-high complexity of our annotation
ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
task. Forexample,onetypeofinter-sententialerror
Zhuang,YonghaoZhuang,JosephE.Gonzalez,Ion
isadiscourseerror,asdiscussedinPagnonietal. Stoica, and Eric P. Xing. 2023. Vicuna: An open-
(2021). Secondly,ourevaluationframeworktreats sourcechatbotimpressingGPT-4with90%*Chat-
GPTquality.
allfactualerrorsashavingequalseverity,without
distinguishingbetweenthepotentially-varyingde-
AlexanderFabbri,Chien-ShengWu,WenhaoLiu,and
grees of impact that different factual error types CaimingXiong.2022. QAFactEval: ImprovedQA-
have. Thirdly, our summarization evaluation is basedfactualconsistencyevaluationforsummariza-
specificallytailoredforEnglishdialogues. Models tion. InProceedingsofthe2022Conferenceofthe
NorthAmericanChapteroftheAssociationforCom-
evaluated in this work may exhibit different per-
putationalLinguistics: HumanLanguageTechnolo-
formance for other domains and other languages.
gies,pages2587–2601,Seattle,UnitedStates.Asso-
Additionally,wedonotconductextensiveprompt ciationforComputationalLinguistics.AlexanderR.Fabbri,WojciechKrys´cin´ski,BryanMc- Yebowen Hu, Timothy Ganter, Hanieh Deilamsalehy,
Cann,CaimingXiong,RichardSocher,andDragomir FranckDernoncourt,HassanForoosh, andFeiLiu.
Radev.2021. SummEval: Re-evaluatingsummariza- 2023. MeetingBank: Abenchmarkdatasetformeet-
tionevaluation. TransactionsoftheAssociationfor ingsummarization. InProceedingsofthe61stAn-
ComputationalLinguistics,9:391–409. nualMeetingoftheAssociationforComputational
Linguistics(Volume1: LongPapers),pages16409–
Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie 16423,Toronto,Canada.AssociationforComputa-
Utama,IdoDagan,andIrynaGurevych.2019. Rank- tionalLinguistics.
inggeneratedsummariesbycorrectness: Aninterest-
ingbutchallengingapplicationfornaturallanguage WojciechKryscinski,NitishShirishKeskar,BryanMc-
inference. InProceedingsofthe57thAnnualMeet- Cann, Caiming Xiong, and Richard Socher. 2019.
ingoftheAssociationforComputationalLinguistics, Neuraltextsummarization: Acriticalevaluation. In
pages 2214–2220, Florence, Italy. Association for Proceedings of the 2019 Conference on Empirical
ComputationalLinguistics. MethodsinNaturalLanguageProcessingandthe9th
InternationalJointConferenceonNaturalLanguage
JosephL.Fleiss.1971. Measuringnominalscaleagree- Processing(EMNLP-IJCNLP),pages540–551,Hong
ment among many raters. Psychological Bulletin, Kong,China.AssociationforComputationalLinguis-
76(5):378–382. tics.
WojciechKryscinski,BryanMcCann,CaimingXiong,
JinlanFu,See-KiongNg,ZhengbaoJiang,andPengfei
and Richard Socher. 2020. Evaluating the factual
Liu.2023. GPTScore: Evaluateasyoudesire. arXiv
consistency of abstractive text summarization. In
preprintarXiv:2302.04166.
Proceedings of the 2020 Conference on Empirical
MethodsinNaturalLanguageProcessing(EMNLP),
Luyu Gao, Zhuyun Dai, Panupong Pasupat, Anthony
pages9332–9346,Online.AssociationforComputa-
Chen,ArunTejasviChaganty,YichengFan,Vincent
tionalLinguistics.
Zhao, Ni Lao, Hongrae Lee, Da-Cheng Juan, and
KelvinGuu.2023. RARR:Researchingandrevising PhilippeLaban,WojciechKryscinski,DivyanshAgar-
whatlanguagemodelssay, usinglanguagemodels. wal,AlexanderFabbri,CaimingXiong,ShafiqJoty,
In Proceedings of the 61st Annual Meeting of the andChien-ShengWu.2023. SummEdits: Measuring
AssociationforComputationalLinguistics(Volume1:
LLM ability at factual reasoning through the lens
LongPapers),pages16477–16508,Toronto,Canada. ofsummarization. InProceedingsofthe2023Con-
AssociationforComputationalLinguistics. ferenceonEmpiricalMethodsinNaturalLanguage
Processing,pages9662–9676,Singapore.Associa-
MingqiGaoandXiaojunWan.2022. DialSummEval: tionforComputationalLinguistics.
Revisitingsummarizationevaluationfordialogues.
InProceedingsofthe2022ConferenceoftheNorth PhilippeLaban,TobiasSchnabel,PaulN.Bennett,and
AmericanChapteroftheAssociationforComputa- MartiA.Hearst.2022. SummaC:Re-visitingNLI-
tionalLinguistics: HumanLanguageTechnologies, basedmodelsforinconsistencydetectioninsumma-
pages5693–5709,Seattle,UnitedStates.Association rization. TransactionsoftheAssociationforCompu-
forComputationalLinguistics. tationalLinguistics,10:163–177.
BogdanGliwa,IwonaMochol,MaciejBiesek,andAlek- Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan
sanderWawer.2019. SAMSumcorpus: Ahuman- Taori, Ishaan Gulrajani, Carlos Guestrin, Percy
annotated dialogue dataset for abstractive summa- Liang, and Tatsunori B. Hashimoto. 2023. Al-
rization. In Proceedings of the 2nd Workshop on pacaeval: An automatic evaluator of instruction-
NewFrontiersinSummarization,pages70–79,Hong following models. https://github.com/
Kong,China.AssociationforComputationalLinguis-
tatsu-lab/alpaca_eval.
tics.
Chin-Yew Lin. 2004. ROUGE: A package for auto-
maticevaluationofsummaries. InTextSummariza-
TanyaGoyalandGregDurrett.2021. Annotatingand
tionBranchesOut,pages74–81,Barcelona,Spain.
modeling fine-grained factuality in summarization.
AssociationforComputationalLinguistics.
InProceedingsofthe2021ConferenceoftheNorth
AmericanChapteroftheAssociationforComputa- Zheheng Luo, Qianqian Xie, and Sophia Ananiadou.
tionalLinguistics: HumanLanguageTechnologies, 2023. ChatGPTasafactualinconsistencyevaluator
pages1449–1462,Online.AssociationforComputa- for abstractive text summarization. arXiv preprint
tionalLinguistics. arXiv:2303.15621.
TanyaGoyal, JunyiJessyLi, andGregDurrett.2022. AmanMadaan, NiketTandon,PrakharGupta,Skyler
News summarization and evaluation in the era of Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon,
GPT-3. arXivpreprintarXiv:2209.12356. Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
Sean Welleck, Bodhisattwa Prasad Majumder,
M.A.K. Halliday, Christian M.I.M. Matthiessen, Shashank Gupta, Amir Yazdanbakhsh, and Peter
MichaelHalliday,andChristianMatthiessen.2014. Clark. 2023. Self-refine: Iterative refinement with
AnIntroductiontoFunctionalGrammar. Routledge. self-feedback.Joshua Maynez, Shashi Narayan, Bernd Bohnet, and withlinguistically-informedcontrastivefine-tuning.
Ryan McDonald. 2020. On faithfulness and factu- InProceedingsofthe2022ConferenceoftheNorth
alityinabstractivesummarization. InProceedings AmericanChapteroftheAssociationforComputa-
of the 58th Annual Meeting of the Association for tionalLinguistics: HumanLanguageTechnologies,
Computational Linguistics, pages 1906–1919, On- pages5657–5668,Seattle,UnitedStates.Association
line.AssociationforComputationalLinguistics. forComputationalLinguistics.
Marry L. McHugh. 2012. Interrater reliability: The HugoTouvron,ThibautLavril,GautierIzacard,Xavier
kappastatistic. BiochemiaMedica,pages276–282. Martinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,Faisal
AniNenkovaandRebeccaPassonneau.2004. Evaluat-
Azhar,AurelienRodriguez,ArmandJoulin,Edouard
ingcontentselectioninsummarization: Thepyramid
Grave,andGuillaumeLample.2023. LLaMA:Open
method. In Proceedings of the Human Language
andefficientfoundationlanguagemodels.
TechnologyConferenceoftheNorthAmericanChap-
teroftheAssociationforComputationalLinguistics:
Jiaan Wang, Yunlong Liang, Fandong Meng, Haoxi-
HLT-NAACL 2004, pages 145–152, Boston, Mas-
ang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu, and Jie
sachusetts,USA.AssociationforComputationalLin-
Zhou.2023. IsChatGPTagoodNLGevaluator? a
guistics.
preliminarystudy. arXivpreprintarXiv:2303.04048.
OpenAI. 2023. GPT-4 technical report. ArXiv,
JasonWei,XuezhiWang,DaleSchuurmans,Maarten
abs/2303.08774.
Bosma,BrianIchter,FeiXia,EdH.Chi,QuocV.Le,
Artidoro Pagnoni, Vidhisha Balachandran, and Yulia andDennyZhou. 2022. Chain-of-thoughtprompt-
Tsvetkov.2021. Understandingfactualityinabstrac- ing elicits reasoning in large language models. In
tivesummarizationwithFRANK:Abenchmarkfor NeurIPS.
factualitymetrics. InProceedingsofthe2021Con-
ferenceoftheNorthAmericanChapteroftheAsso- Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
ciationforComputationalLinguistics: HumanLan- PuZhao,JiazhanFeng,ChongyangTao,andDaxin
guageTechnologies,pages4812–4829,Online.As- Jiang. 2023. WizardLM: Empowering large lan-
sociationforComputationalLinguistics. guagemodelstofollowcomplexinstructions. arXiv
preprintarXiv:2304.12244.
KishorePapineni,SalimRoukos,ToddWard,andWei-
JingZhu.2002. Bleu: amethodforautomaticevalu- YuhengZha,YichiYang,RuichenLi,andZhitingHu.
ationofmachinetranslation. InProceedingsofthe 2023. AlignScore: Evaluating factual consistency
40thAnnualMeetingoftheAssociationforCompu- with a unified alignment function. In Proceedings
tational Linguistics, pages 311–318, Philadelphia, of the 61st Annual Meeting of the Association for
Pennsylvania,USA.AssociationforComputational ComputationalLinguistics(Volume1: LongPapers),
Linguistics. pages11328–11348,Toronto,Canada.Association
forComputationalLinguistics.
WilliamSaunders,CatherineYeh,JeffWu,StevenBills,
LongOuyang,JonathanWard,andJanLeike.2022.
TianyiZhang*,VarshaKishore*,FelixWu*,KilianQ.
Self-critiquingmodelsforassistinghumanevaluators.
Weinberger, and Yoav Artzi. 2020. BERTScore:
CoRR,abs/2206.05802.
EvaluatingtextgenerationwithBERT. InInterna-
tionalConferenceonLearningRepresentations.
Liyan Tang, Tanya Goyal, Alex Fabbri, Philippe La-
ban,JiachengXu,SemihYavuz,WojciechKryscin-
TianyiZhang,FaisalLadhak,EsinDurmus,PercyLiang,
ski,JustinRousseau,andGregDurrett.2023a. Un-
KathleenMcKeown,andTatsunoriHashimoto.2023.
derstandingfactualerrorsinsummarization: Errors,
Benchmarkinglargelanguagemodelsfornewssum-
summarizers,datasets,errordetectors. InProceed-
marization. ArXiv,abs/2301.13848.
ingsofthe61stAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1: LongPapers),
ChenguangZhu,YangLiu,JieMei,andMichaelZeng.
pages11626–11644,Toronto,Canada.Association
2021. MediaSum: A large-scale media interview
forComputationalLinguistics.
datasetfordialoguesummarization. InProceedings
LiyanTang,ZhaoyiSun,BetinaIdnay,JordanG.Nestor, ofthe2021ConferenceoftheNorthAmericanChap-
AliSoroush,PierreA.Elias,ZiyangXu,YingDing, teroftheAssociationforComputationalLinguistics:
Greg Durrett, Justin F. Rousseau, Chunhua Weng, HumanLanguageTechnologies,pages5927–5934,
andYifanPeng.2023b. Evaluatinglargelanguage Online.AssociationforComputationalLinguistics.
models on medical evidence summarization. npj
DigitalMedicine,6(1). A TOFUEVAL DescriptiveStatistics
XiangruTang,ArjunNair,BoruiWang,BingyaoWang,
WeshowthedescriptivestatisticsofTOFUEVALin
Jai Desai, Aaron Wade, Haoran Li, Asli Celikyil-
Table5. WemeasurethewordcountbytheNLTK
maz, Yashar Mehdad, and Dragomir Radev. 2022.
CONFIT: Toward faithful dialogue summarization package. AlldialoguesarewritteninEnglish.# Avg #Asp. # Main # #
Dataset
Doc. Len /Doc. LLM Topic Turn Speaker
MediaSum 50 970 3 5 78% 16.6 5.7
MeetingBank 50 930 3 5 73% 19.9 4.9
Table5: DatasetstatisticsonTOFUEVAL. Wesample50testsetdocumentsfromeachdataset;generate3topics
foreachdocumentandthencollectsummariesfrom5LLMsforeachtopic. Weshowthepercentageofmaintopics
evaluatedbyhuman(moreinSection3.4).
B ModelDetails generated questions. We refer readers to original
worksformoredetailsofthesemodels. ForVicuna-
B.1 SummaryGeneration
33B18, we use the one based on Llama (Touvron
WechosetouseVicuna-7B11(Chiangetal.,2023) etal.,2023). Wedonotincludeinstruction-tuned
and WizardLM-7B/13B/30B12 (Xu et al., 2023), LLMs such as Falcon-40b and mpt-30b, as these
bothofwhicharebasedonLlama(Touvronetal., modelsperformedpoorlyinourinitialbenchmarks.
2023), for summary generation. We also exper-
imented with other open-source LLMs, includ- C Prompts
ingFalcon-7b/40b-instruct13andmpt-7b-instruct14.
WefindthatVicunaandWizardLMgenerallydoa C.1 PromptforTopicGeneration
betterjobatinstructionfollowingforourtaskand
Weusethefollowingprompttogeneratetopicsfor
aremorerobusttoprompts. Wealsocollectsum-
dialoguedocumentsin TOFUEVAL.
mariesfromGPT-3.5-TurboviaitsofficialAPI.
Document: {Document}
Our model selection process for LLM-based
Enumerate three main topics that people
summarizationwasfinalizedinearlyJuneforhu-
would like to know from the provided docu-
man evaluation, and as a result, we have not in-
ment. Eachtopicshouldbearound5words.
cludedsummariesgeneratedbymodelsdeveloped
sincethen.
C.2 PromptforTopic-Focused
B.2 SummaryEvaluation Summarization
Inourstudy,weincorporatethreeSOTAandspe- We add the following instruction to the model’s
cialized factuality metrics based on entailment: defaultprefix,ifany,toformtheprompt19fortopic-
SummaC-ZS,SummaC-CV15 (Labanetal.,2022), focusedtextsummarizationinazero-shotmanner:
and AlignScore16 (Zha et al., 2023). These met- Document: {Document}
rics are designed to determine whether summary
Summarizetheprovideddocumentfocusing
sentencescanbeinferredbysomeportionoftext
on “{topic}”. The summary should be less
extractedfromthesourcedocuments. Wealsoin-
than50wordsinlength.
cludeaSOTAquestion-answering(QA)basedfac-
tualitymetricQAFactEval17 (Fabbrietal.,2022), C.3 PromptsforSummaryEvaluation
whichevaluatesfactualconsistencybygenerating
Thissectioncontainsallpromptsthatweusedfor
questions and verifying the answerability of the
obtainingbinaryfactualconsistencylabelsandex-
11https://huggingface.co/lmsys/ planationsfromLLMs. Additionaldetailscanbe
vicuna-7b-delta-v0.
found in Section 5.1. For sentence-level prompt,
12https://huggingface.co/WizardLM/
WizardLM-7B-V1.0, https://huggingface. we find that providing the previous summary
co/WizardLMTeam/WizardLM-13B-V1.0, sentencesinthepromptascontextdoesnotaf-
https://huggingface.co/WizardLM/
fecttheperformanceonaninitialstudy,sofor
WizardLM-30B-V1.0.
13https://huggingface.co/tiiuae/ simplicity,weonlyprovidedtheisolatedsentence.
falcon-7b-instruct, https://huggingface.
co/tiiuae/falcon-40b-instruct. 18https://huggingface.co/lmsys/
14mosaicml/mpt-7b-instruct. vicuna-33b-v1.3
15https://github.com/tingofurro/summac/ 19Wealsotriedtocontrolthesummarylengthbyasking
16https://github.com/yuh-zha/AlignScore models to generate a fixed number of sentences, but most
17https://github.com/salesforce/ modelsweevaluatedherecannotfollowthelengthconstraint
QAFactEval wellineitherformat.(DIR) Binary-Label, Sentence-Level Prompt entailed(eitherstatedorimplied)bythedocu-
We asked LLMs to provide a binary factual con- ment. Pleasestartyouranswerwith“Yes.” or
sistency label for a summary sentence using the “No.” Pleasebrieflyexplainthereasonwithin
followingprompt: 50words.
Document: {Document}
Sentence: {Sentence} PromptingLLMstogenerateexplanationsbe-
Determineifthesentenceisfactuallycon- foreprovidingafinalanswerresultsinnoperfor-
sistentwiththedocumentprovidedabove. A mancedifferences. Inasmall-scaleexperiment,
sentenceisfactuallyconsistentwiththedoc- we observed that when we prompt the model to
ument if it can be entailed (either stated or generate an explanation before providing the fi-
implied)bythedocument. Pleaseanswerwith nal answer, the response generated by the model
“Yes”or“No”. tendstobeginwithasentencesuchas“Thissen-
tence/summaryisfactually(in)consistentwiththe
(DIR) Binary-Label, Summary-Level Prompt document”,andtheactualexplanationbeginsafter
We asked LLMs to provide a binary factual con- thesecondsentence. Sincethisissimilartostart-
sistencylabelforasummaryusingthefollowing ingtheresponsewith“Yes”or“No”,wechosethe
prompt: latterforsimplicity.
Document: {Document}
Summary: {Summary}
D ExtendedResults: LLMsas
Determineifthesummaryisfactuallycon-
Summarizers
sistentwiththedocumentprovidedabove. A
summaryisfactuallyconsistentwiththedocu-
D.1 RelevanceandCompletenessEvaluation
mentifallinformationinthesummarycanbe
entailed(eitherstatedorimplied)bythedoc- As shown in the Rel. column in Table 6, each
ument. Pleaseanswerwith“Yes”or“No”. LLM’saveragerelevancescoreisclosetothemax-
imumof1(seeSection 3.4formoredetails). We
(EXP)Binary-LabelwithExplanation,Sentence- concludethatallLLMsarequitecapableoffocus-
LevelPrompt WeaskedLLMstoprovideexpla- ingontherequestedtopics,withbiggermodels
nations for their decisions in addition to provid- achievingslightlybetterperformance.20
ing the binary factuality judgment. Below is the
Next, we compare the models’ performance at
promptweusedforsentence-levelevaluationwith
coveringtherequestedtopic. Itisworthnotingthat
correspondingexplanations:
althoughsmallerLLMscanachieveequivalentor
Document: {Document}
evensuperiorperformanceinsummarycomplete-
Sentence: {Sentence}
ness,thelengthofsummariesgeneratedbysmall
Determineifthesentenceisfactuallycon-
LLMs is much higher. For example, Vicuna-7B
sistentwiththedocumentprovidedabove. A
achieves 0.72 in completeness on MeetingBank
sentence is factually consistent if it can be
with an average summary length of 72 words. In
entailed(eitherstatedorimplied)bythedocu-
contrast, GPT-3.5-Turbo achieves a comparable
ment. Pleasestartyouranswerwith“Yes.” or
completenessscoreof0.74withmoreconcisesum-
“No.” Pleasebrieflyexplainthereasonwithin
maries (53 words). This trend also holds true for
50words.
thethreeWizardLMmodelsofvaryingsizes. The
larger the WizardLM model size, the more ca-
(EXP) Binary-Label with Explanation,
pable the model is of either maintaining or cov-
Summary-LevelPrompt Thefollowingprompt
ering more relevant information in the summary
wasusedforsummary-levelfactualityevaluation
while making the summary shorter (WizardLM-
withacorrespondingexplanation.
13Bvs. WizardLM-30B).Therefore,weconclude
Document: {Document}
thatlargerLLMsaremorecapableofgenerat-
Summary: {Summary}
inginformation-densesummariescomparedto
Determineifthesummaryisfactuallycon-
smallerLLMs.
sistentwiththedocumentprovidedabove. A
summaryisfactuallyconsistentwiththedocu-
20This is based on limited observation on open-source
mentifallinformationinthesummarycanbe LLMssincethemodelsizeofGPT-3.5-Turboisunknown.Summ. MediaSum MeetingBank Datasetaffectsmodelerrorrate. Asshownin
Model Table2,models,onaverage,makemoreerrorson
Rel Cmp Err Rel Cmp Err
Len Len
[0,1] [0,1] % [0,1] [0,1] % MediaSumthanonMeetingBank. Thedifference
Vicuna-7B 65 0.89 0.64 47.3 72 0.81 0.72 43.6 ismoresignificantforthemaintopicsandismagni-
Wizard-7B 44 0.84 0.53 51.4 51 0.76 0.61 41.0 fiedbythesummary-levelperformancecompared
Wizard-13B 70 0.87 0.69 38.9 73 0.88 0.75 43.6
to the sentence level. This shows that there is a
Wizard-30B 69 0.91 0.72 40.3 66 0.88 0.75 37.2
GPT-3.5-Tb 57 0.91 0.70 24.0 53 0.91 0.74 14.7 non-negligible impact of text distribution on the
models’summarizationperformance. Onehypoth-
Table6: SummarizationmodelstatisticsonTOFUE-
esis is that it is more challenging for models to
VAL. Foreachmodelunderevaluation,weincludethe
generatefactuallyconsistentsummariesrelatedto
human-evaluatedcompletenessscore(Cmp),relevance
aspecifictopicthatrequiresaggregatingandsyn-
score(Rel)andpercentageofsummarieswithatleast
thesizinginformationacrossconversationalturns,
onefactualinconsistency(Err%)foreachdataset. Wiz-
ardisanabbreviationforWizardLM. andwefindtopic-relatedinformationtendstobe
moreevenlydistributedacrossconversationalturns
inMediaSumthaninMeetingBank.
D.2 FactualConsistencyEvaluation
E ExtendedResults: LLMsasEvaluators
ExistingLLMsstillmakeaconsiderableamount In addition to assessing evaluators’ performance
of factual errors. In Table 6, we show the per- throughbalancedaccuracyandrecall,asdetailedin
centage of summaries with at least one factually Section5,wealsoexaminethereliabilityofevalu-
inconsistency for each model across datasets, ac- atorsinidentifyingfactuallyinconsistentsummary
cordingtoourannotations. Wefindthatoutofall orsummarysentencesbyanalyzingthefalseposi-
models we evaluated except GPT-3.5-Turbo, ap- tiverate(FPR)andfalsenegativerate(FNR):
proximately 40–50% of their summaries contain
FP FN
at least one factual inconsistency. Furthermore, FPR = , FNR = .
FP+TN FN+TP
thereisnodirectpositivecorrelationbetweena
Onsentence-levelevaluation,FPRindicatesthe
summary’slengthandthequantityoferrorsit
rateatwhichanevaluatorincorrectlypredictsthat
contains. Forexample,onMediaSumdata,38.9%
a summary sentence contains an error when it is
ofWizardLM-13B’ssummarieswerefactuallyin-
actuallycorrect,whereFPrepresentsthenumberof
consistentwithanaveragesummarylengthof70;
falsepositives(factuallyconsistentsentencesincor-
whereas24.0%ofGPT-3.5-Turbo’ssummariesare
rectly labeled as inconsistent) and TN represents
inconsistentwhilehavingahigheraveragelength
the true negatives (factually consistent sentences
of 57. The computed Pearson correlation coeffi-
cientρbetweenmodels’lengthandproportionof correctlyidentifiedassuch). AhighFPRsuggests
inconsistentsummariesis0.18,withap-valueof thattheevaluatorisfrequentlyflaggingsummary
sentencesaserroneouswhentheyarefactuallycon-
0.57,showingweakpositivecorrelation.
sistent.
FNR represents the rate at which an evaluator
LargerLLMsdonotnecessarilygeneratefewer incorrectlypredictsthatasummarysentenceiscor-
factuallyinconsistentsummaries. Asshownin rectwhenitactuallycontainsanerror,whereFN
Table 6, when comparing the same model fam- stands for false negatives (factually inconsistent
ily, WizardLM-30B generates a slightly higher summarysentencesincorrectlylabeledascorrect)
number of errors than WizardLM-13B on Medi- and TP stands for true positives (factually incon-
aSum,andWizardLM-13Bgeneratesmoreerrors sistencysummarysentencescorrectlyidentifiedas
than WizardLM-7B on MeetingBank. Further- such). AhighFNRmeanstheevaluatoroftenover-
more,whilealargermodelmayhavealowererror lookserrorsinthesummarysentences.
rate, the reduction may be minor. For example, It is worth mentioning that FPR and FNR pro-
WizardLM-30B’serrorrateisonly3.8%lowerthan videamoredetailedbreakdownofevaluators’per-
WizardLM-7B’sonMeetingBankdata. Comparing formancecapturedbybalancedaccuracy(BAcc)in
models from different families, the error rate of Table3:
WizardLM-13BisthesameasthatofVicuna-7B
1
onMeetingBank. BAcc = 1− (FPR+FNR).
2Sentence-Level(FPR↓) Summary-Level(FPR↓)
Model Evaluation
Type Model MediaSum MeetingBank MediaSum MeetingBank
Main Marginal Main Marginal Main Marginal Main Marginal
SummaC-ZS 26.1 26.0 26.9 20.2 51.9 51.8 35.6 38.1
Non- SummaC-CV 47.2 40.2 27.9 30.6 53.5 46.9 24.3 21.7
LLM QAFactEval 33.2 22.7 30.1 29.2 35.4 22.8 45.9 45.1
AlignScore 23.9 26.0 14.9 25.4 41.6 35.8 36.6 47.1
Prop. GPT-3.5-Turbo 3.7 14.6 13.4 48.4 11.2 26.2 25.3 43.7
LLM GPT-4 1.4 5.7 3.5 6.7 3.5 12.5 4.5 7.8
Table7: Sentence-levelandsummary-levelfalsenegativerate(FNR)forfactualconsistencyevaluatorsonthe
testsetofTOFUEVAL(amodelincorrectlypredictsthatasummaryorsummarysentencecontainsanerror).
Sentence-Level(FNR↓) Summary-Level(FNR↓)
Model Evaluation
Type Model MediaSum MeetingBank MediaSum MeetingBank
Main Marginal Main Marginal Main Marginal Main Marginal
SummaC-ZS 41.8 26.4 45.4 18.7 22.8 20.1 48.1 17.3
Non- SummaC-CV 17.8 14.0 47.1 14.9 24.2 20.1 71.0 32.6
LLM QAFactEval 59.0 29.5 54.1 19.4 41.9 28.8 43.9 18.7
AlignScore 37.9 21.6 62.7 17.5 27.4 20.1 36.6 17.5
Prop. GPT-3.5-Turbo 73.0 47.7 48.4 21.6 69.6 42.2 48.4 24.9
LLM GPT-4 68.8 33.9 46.1 12.6 69.0 29.7 46.1 26.1
Table8: Sentence-levelandsummary-levelfalsepositiverate(FPR)forfactualconsistencyevaluatorsonthe
testsetofTOFUEVAL(amodelincorrectlypredictsthatasummaryorsummarysentenceiscorrect).
SignificantTest The highlighted performance tencesfrommarginaltopics. Notably,thistrendis
issignificantlybetterthantherestwithp-value< morepronouncedforLLM-basedevaluators. For
0.05byapairedbootstraptestacrossalltablesin instance, LLMs show a substantial 20% to 40%
thiswork. decrease in sentence-level FNR, indicating their
improved error detection capabilities. However,
LLM-based evaluators often overlook errors,
it appears that LLMs achieve this by identifying
whilenon-LLM-basedfactualconsistencymet-
moresummarysentencesasfactuallyinconsistent,
ricstendtoproducefalsealarms. Inlinewith
leading to a higher FPR on marginal topics (Ta-
theapproachdetailedinSection5.2,weshowour
ble7). Thisisparticularlynoticeableinthecaseof
findingsforasubsetofrelativelystrongevaluators
GPT-3.5-Turbo.
in Tables 7 and 8. The non-LLM-based metrics
exhibitasignificantissuewithahighFPR.When
F TOFUEVAL AnnotationInstructions
these metrics signal a potential error, it necessi-
tatesamanualcomparisonbetweenthesummary We separated the human evaluation work for the
sentenceandthesourcedocumenttoverifyitsaccu- TOFUEVAL benchmark into two tasks due to the
racy. Thisprocessresultsinaconsiderableamount workload. Eachtaskwasannotatedbyadifferent
of unnecessary effort. On the other hand, LLM- groupofannotators. Thereare300annotationsfor
basedevaluatorsdisplayahigherFNR.Whilethis eachofthetwotasks(2datasets×50documents
mightreducetheimmediateworkload,itintroduces × 3 topics). The first task (Task 1) consisted of
the risk of missing inconsistent sentences, which topiccategorization,factualconsistencyevaluation,
canbedetrimentalinthelongrun. and relevance evaluation. The second annotation
task(Task2)involvedcompletenessevaluation.
All evaluation models miss fewer errors for
marginal-topic summary sentences, but LLM-
F.1 Task1
based evaluators bring more false alarms
when evaluating marginal-topic summaries. Topic Categorization is defined as T (dia-
topic
AsshowninTable8,allmodelshaveadecreased loguedocument,topic)→{main,marginal,irrele-
FNRwhenevaluatingsummaryorsummarysen- vant}. Wedefinedthecategoriesasfollows:Summary-Level(BAcc↑) Summary-Level(BAcc↑)
Model Evaluation Model Evaluation
Type Model MediaSum MeetingBank Type Model MediaSum MeetingBank
Main Margin Main Margin Main Margin Main Margin
- Baseline 50.0 50.0 50.0 50.0 - Baseline 50.0 50.0 50.0 50.0
SummaC-ZS 62.7 64.1 58.1 72.4 Vicuna-13B 51.7 49.2 49.9 49.6
Open
Non- SummaC-CV 61.2 66.5 52.4 72.9 Vicuna-33B 54.4 56.0 54.6 54.0
Source
LLM QAFactEval 61.4 74.2 55.1 68.2 Wizard-13B 56.8 57.1 56.2 60.1
LLM
AlignScore 65.5 72.1 63.4 71.8 Wizard-30B 56.0 57.7 54.9 56.1
Vicuna-13B 49.8 52.3 48.6 53.1 Prop. GPT-3.5-Turbo 60.1 64.3 61.9 61.5
Open
Vicuna-33B 50.0 50.5 50.8 47.6 LLM GPT-4 64.2 78.7 75.9 83.9
Source
Wizard-13B 52.0 52.3 47.3 51.9
LLM
Wizard-30B 50.0 51.8 50.0 51.2
Table10: Summary-levelBAcconthetestsetofTO-
Prop. GPT-3.5-Turbo 55.0 71.2 51.5 59.9 FUEVALfortheEXPsetting,whereweaskamodelto
LLM GPT-4 58.2 68.5 56.6 80.0 provideexplanationsforitsdecisionsinadditiontopro-
vidingbinaryjudgments(Section5.1). Summary-level
Table9: Summary-levelBAcconthetestsetofTO- labelsareobtainedbyaggregatingsentence-levellabels.
FUEVALbydirectlyevaluatingonwholesummaries. Non-LLMsdonotprovideexplanations.
Directlypredictingthefactualconsistencyofsummaries
isworsethanaggregatingsentence-levelfactualitypre-
dictionresultsforallmodels(Table3). summary. If any sentence is labeled as factually
inconsistent,annotatorsareaskedtoexplaintheir
reasoninginnaturaltext.
MainTopicreferstothecentralinformationina
documentthatisunderdiscussionorispresentedin Relevancy Evaluation is defined as T (dia-
rel
thedocument. Themaintopicsareoftenwhatthe loguedocument,topic,summary)→{1,2,3,4,5}.
document is primarily about, and understanding Wedefinedthe1-5Likertscaleasfollows.
themiscriticaltounderstandingtheoverallideaof 5-Excellent: Thesummarydoesnotcontainnon-
thedocument. topicrelatedcontent;4-VeryGood: Thesummary
Marginal Topic refers to information in a doc- containsasmallamountofnon-topicrelatedcon-
umentthatisnotthemainfocusofthedocument tent; 3-Good: Half of the summary is off-topic.
butisstillpartofthecontext. Thesetopicsaretyp- Thecontentissomewhatbalancedbetweentopic-
icallylessprominentorlessextensivelyexplored relatedandnon-topicrelatedcontent;2-Fair: More
thanthemaintopics. Theymaycontributetothe thanhalfofthesummaryisoff-topic,butthereis
overallcontext,provideadditionalinformation,or stilltopicrelatedcontent;1-Poor: Thesummaryis
enhanceunderstandingofthemaintopics,butthey composedofnon-topicrelatedcontent.
arenottheprimaryfocus. See Section F.4 for an explanation of how we
Irrelevant Topic refers toinformation ina doc- merged scores ({1,2} → 0, and {3,4,5} → 1)
umentthatisnotdirectlyrelatedtothesubjector toimproveannotationagreement. Theannotation
purpose of the document. Such topics might not guidelinesandinterfaceforTask1canbefoundin
contributetothemaintopic(s)orobjectiveofthe Figure6and 7.
documentandcanbeseenasadiversionordistrac-
TwoPassAnnotations Toensurethequalityof
tionfromthemaintopicsathand.
our collected annotations, a subset of the annota-
SeeSectionF.4forinformationaboutourpost-
tiontaskwascompletedbytwoseparateannotators.
processingoftopiccategories,inwhichwemerged
Weusedthefeedbackfromthesetwo-passannota-
marginal and irrelevant topics together after data
tionstoidentifyanyambiguitiesorissuesinthean-
collection.
notationguidelinesandmakenecessaryrevisions.
Factual Consistency Evaluation is defined as Formoredetailsonhowweenhancedconsensus
T (dialoguedocument,sentence)→{consistent, amongannotations,pleaserefertoSectionF.3.
fact
inconsistent},whereafactuallyconsistentsentence
F.2 Task2
is a sentence that is entailed (either stated or im-
plied)bythedocument. CompletenessEvaluation InspiredbythePyra-
Notethatweconductedthistaskatthesentence midmethod(NenkovaandPassonneau,2004),we
level, with annotators having access to the entire firstaskedanannotatortowritedownnkeypointsin grammatical sentences: T (dialogue docu- familiarizethemselveswiththetasksandcalibrate
keys
ment,topic)→K,whereK = {k ,...,k }. These theirevaluations.
1 n
key points were supposed to be what the anno-
ProvidingCustomizedFeedback Aftertheprac-
tator thought an ideal summary covering a given
tice session, we held multiple rounds of pilot an-
topic should include. For each key point k , we
i notations. In each round, every annotation task
then asked the annotator whether a given sum-
wasundertakenbytwoannotatorssothatwecould
mary contains k , i.e., 1(summary, k ). After the
i i calculateinter-annotatoragreementtoensurethat
completeness annotation was finished, we calcu-
wecouldmaintainorimprovetheagreementrate.
latedthecompletenessscoreinthefollowingway:
1 (cid:80)n 1(summary,k ). After each round, we compared the annotations,
n i=1 i providedindividualizedfeedbacktoeachannota-
Sinceitisimpracticaltoconsolidatekeypoints
tor,andrefinedtheannotationguidelines. Oncewe
frommultipleannotators,weonlyutilizedonean-
achievedaconvergingannotationagreementrate,
notatorforeachtaskandprovidedthewrittenkey
weproceededwiththeremainingannotationtasks.
points for reproducibility. The annotation guide-
linesandinterfaceforTask2canbefoundinFig- AnnotationEfforts Theaveragetimespentper
ure8and 9. annotationonthetwoannotationtaskswas36and
24minutes,respectively. Alloftheannotatorsin-
We would like to note that we found that an-
volved in the human evaluation tasks presented
notatingcompletenessusinga1-5Likertscale
in the current work have native-level proficiency
diminished the quality of the results. Without
in English, and they are compensated a competi-
askingannotatorstoexplicitlywritedowntheim-
tivehourlyratethatisbenchmarkedagainstsimilar
portant key points of a dialogue, we found that
rolesintheircountryofresidence.
the order in which summaries were presented to
annotators had a noticeable impact on what they
F.4 Inter-AnnotatorAgreement
considered important to include in an ideal sum-
WeuseCohen’sKappaκ(McHugh,2012)tomea-
mary. Thiseffectoccurredbecausetheannotator’s
surehumanannotationagreementontaskswhere
perception of what should be in a summary may
wereceivedtwoannotationpasses(Task1). Below,
change based on recently-read summaries. As a
weprovideκforeachevaluationdimensioninTask
result,annotatorsneededtorevisetheirannotations
1aswellasthepost-processingstepsweundertook
forsummariestheyhadalreadyassessed,toggling
toimproveannotationagreement.
backandforthtoalignevaluationstheyhadalready
finalizedevaluationswiththeirevolvingopinions. TopicCategorizationandMapping Agreement
This situation may result in an increase in errors wasmoderateforbothdatasets(forMediaSum,κ
and a reluctance or disinclination to amend prior =0.47;forMeetingBank,κ=0.53). Sinceonly2%
responses, ultimately leading to unreliable anno- oftopicsinthebenchmarkwereannotatedasirrele-
tations. Therefore, we encourage future work to vantandweobservedthatannotatorsfrequentlyla-
evaluatethecompletenessofsummariesbyadopt- bel“marginal”and“irrelevant”interchangeablyin
ingourapproach. thesetopics,wemerged{marginal,irrelevant}→
{marginal}aftertheannotationswerecompleted.
F.3 QualityControl
FactualConsistency Weachievedκ=0.42and
ArrangingthePracticeSession Wedeveloped 0.34 for evaluations on MediaSum and Meeting-
theannotationguidelinesandarrangedapractice Banksummaries,respectively,showingfairtomod-
sessionfortheannotationtaskswiththeassistance erate agreement. Based on human-written expla-
ofaselectgroupofprofessionallinguisticdataan- nations,wefoundthattheprimarydisagreements
notators. During our preparation of the practice occurredwhentheannotatorserroneouslylabeled
session,werefinedtheguidelinesandanswersto sentencesasfactuallyinconsistent,whenthesum-
theaforementioneddimensions,emphasizingpre- mary was incomplete, when summary sentences
cautionswewantedtheannotatorstoconsiderdur- did not directly address the topic, or when other
ingannotation. Weengagedindiscussionswiththe situationsarosethatshouldnotbeconsideredunder
grouptoreachaconsensusontheseanswers. Using thisevaluationdimension.
thefinalizedguidelines,allprofessionallinguistic Giventhesefindings,afterwecollectedallanno-
dataannotatorsparticipatedinapracticesessionto tations,theauthorsofthecurrentworkperformedFigure4: Errordistributionsoverallsummarysentencesforeachsummarizerformain/marginaltopics.
asecond-stageannotationreviewtoeliminateany the guidelines, ultimately improving annotation
false positive labels from the benchmark dataset agreement. Fortopiccategorizationandrelevance
based on written explanations. Summarization evaluation,onceapilotroundwascompleted,we
modelswerehiddenduringthisreviewprocessto asked annotators to share their thoughts and un-
baragainstanyunconsciousbias. Wewanttoem- certaintiesonexampleswhereweobservedsignifi-
phasizethereliabilityofoursecond-stagereview cantdisagreement. Wethenrefinedtheannotation
process. Itwasconductedusingourdefinitionsfor guidelinesbasedontheirfeedback.
factual consistency along with careful considera-
tionoftheexplanationsprovidedbytheannotators. G ErrorTypeCurationandAnnotation
The reviewers discussed challenging cases to en-
surealignment,andalldecisionswerereviewedby G.1 ErrorTypeCuration
anotherreviewer. Webelievethisprocesssubstan-
DuetothecomplexityofthedialoguesinTOFUE-
tiallyimprovedthequalityofthedataset.
VAL, we curated the following error types with
Relevance Unlikethefactualconsistencyevalua- professional linguistic data annotators (all coau-
tioninwhichexplanationswereelicited,wefound thors of the current work) based on the explana-
it challenging to achieve high agreement on rele- tionswrittenbytheannotatorsforfactuallyincon-
vanceevaluationevenafterafewroundsoftargeted sistentsentences. Notethatourerrortaxonomyis
feedback. Weultimatelydecidedtobinthescores stronglyinfluencedbythatfromTangetal.(2022),
as follows: {1,2} → 0, and {3,4,5} → 1. With which was initially proposed for short dialogues.
thisgrouping,κ=0.25forMediaSumsummaries Aconcisedescriptionofourerrortaxonomywith
and0.37forMeetingBanksummaries, whichwe illustratedexamplesisprovidedinFigure5.
considertobefairagreement. Weevaluatedrele-
ExtrinsicInformationError SimilartoMaynez
vance on main-topic summaries only since topic-
et al. (2020), we define the error as follows: the
relatedcontentmaynotexistformarginaltopics.
summarysentencecontainsnewinformationthat
Writing explanations is helpful for providing is not from the source document and cannot be
feedbackandimprovingannotationconsensus. verifiedfromthesourcedocument. Ifthenewin-
We observed that the value of κ for these three formationisnotfromthesourcedocumentbutis
dimensionsincreasedfrom0.1to0.3afterthean- everydaycommonsenseknowledge,wedonotla-
notatorsreceivedadditionaltraining. Notably,we belthesentenceascontaininganerror.
foundthatimprovingannotationagreementisrela-
tivelystraightforwardforthedimensionoffactual Misreferencing Error The summary sentence
consistency;basedontheexplanationswrittenby refers to an entity (e.g. as a noun phrase in sub-
theannotators,wecaneasilygrasptheannotator’s ject/objectposition)thatisgroundedinthesource
reasoning, allowing us to identify shortcomings, document,butthesentenceattributesapropertyor
whether in their reasoning or in our annotation eventtothisentity;thiswrongpropertyoreventis
guidelines. Thisfurtherenabledustoprovidetar- groundedinthesourcedocumentbutisattributed
geted feedback and incorporate clarifications to toadifferententityinthesourcedocument.Error Type Definition Example Explanation
Extrinsic The summary sentence contains new President Obama has called for reforms in The document does not explicitly mention
Information information not grounded in the source the procurement process. that President Obama has called for
document reforms in the procurement process.
Mis- A property or an event in the summary The current fleet is over budget and It is not the current fleet that is “over
Referencing sentence can be found in the source material, overdue, with additional requirements added budget and overdue”, but the one they are
but are associated with the wrong entity over time. intending to replace the current fleet with.
Stating Opinion The summary sentence entails a proposition Government intervention may be needed as It is the opinion of Trippler, and not a fact,
As Fact that's mentioned in the source material not as airlines have forfeited the right to self- that the airlines have “forfeited” the right to
a fact, but as someone’s opinion regulate. self-regulate.
Reasoning The summary sentence makes one or more General Motors, Ford, and Daimler Chrysler 30,000 (General Motors) + 30,000 (Ford) +
Error wrong inferences from information in the are planning to eliminate a combined total 40,000 (Daimler Chrysler) = 100,000 jobs,
source document of 300,000 jobs by 2008. not 300,000 jobs.
Tense/Modality The tense or modal (e.g. can, may, must) A judge has allowed thousands of students Per source, the ruling issued by the judge
Error used in the summary sentence does not match who did not pass to potentially graduate. could allow student to graduate, not has
the tense/modality of the source document allowed.
Contradiction The summary sentence contradicts the source Some airlines argue that factors like weather Experts don’t disagree that weather is
material and labor problems are beyond their beyond the airlines’ control.
control, but experts disagree.
Nuanced The summary sentence twists information The trial of Dr. Conrad is set to begin on The trial is set to resume, not begin on
Meaning Shift from the source material in a subtle way Monday. Monday.
Figure5: Errortaxonomyanddefinitions. Weincludeexamplesoffactuallyinconsistentsummarysentences
andcorrespondinghumanannotatedexplanationsfromTOFUEVAL. Errorspansarehighlighted(notincludedin
TOFUEVAL).
Stating Opinion-as-Fact Error The summary the source document by using paraphrasing with
sentence presents a proposition as fact (i.e. there wordsorphrasesthatareassociatedwithdifferent
are no uncertainty modals or adverbs like might senses (e.g. paraphrasing "make a recommenda-
orprobablyinthesentence)whentheproposition tion"to"makearequest").
is presented as someone’s opinion in the source
Others Inourannotationprocess,weintention-
document.
allyintroducedanothererrorcategory. However,
ReasoningError Thesummarysentencemakes asnosentencewasfoundtobefactuallyinconsis-
wrong inferences (e.g. it contains error in arith- tent within this category, we have chosen not to
metic calculation, it draws incorrect relations be- includeitinourerrortaxonomy.
tweenAandB,oritmakeswrongextrapolations)
G.2 ErrorTypeAnnotation
based on premises or pieces of evidence that are
grounded in the source document. (Note that if Withthecuratederrortaxonomy,theprofessional
the evidence for the inference is not grounded in linguisticdataannotatorsintheauthorlistassigned
thesourcedocument,thatwouldbeconsideredan one or more error types to all factually inconsis-
ExtrinsicInformationError). tent sentences in TOFUEVAL by referring to the
summaries and the explanations provided by the
Tense/Aspect/Modality Error The summary
annotators. The source documents were referred
sentenceusesthewrongtense(e.g. pasttenseinthe
toifwecouldnotidentifytheerrortypebasedon
sourcedocumentbutfuturetenseinthesummary
humanexplanations.
sentence), aspect (e.g. progressive in the source
Weconductedfourroundsofpilotstudiesforthe
documentbutperfectiveinthesummarysentence),
error type assignment task. The first two rounds
ormodality(e.g. epistemicpossibilitymodalmight
wereusedtofinalizetheerrortaxonomywemod-
in the source document but epistemic necessity
ifiedfromTangetal.(2022),whilethefollowing
modalmustinthesummarysentence).
two rounds were dedicated to calibrating our er-
ror type categorization and improving our agree-
Contradiction Error The summary sentence
ment rates (in cases of unresolved disagreement,
fullycontradictsthesourcedocument,eitherdue
wetookthemajorityvotetoarriveatthefinaler-
to erroneous presence or absence of negation or
ror category or categories). For each pilot round,
duetotheuseofanantonymofawordusedinthe
annotatorsassignederrortypestothesamesetof
sourcedocument.
sentences. We achieved a Fleiss’ Kappa score of
Nuanced Meaning Shift Error The summary (Fleiss, 1971) 0.78 after the final pilot round, in-
sentencealtersthemeaningofcertainsentencesin dicatingsubstantialagreement,andweproceededOpen-SourceLLM Prop.LLM human-written explanation. We optionally pro-
vided the source document to the annotators in
Model Acc(%) Model Acc(%)
case they needed more context. The models that
Vicuna-13B 45
Vicuna-33B 40 generatedtheseexplanationswerehiddenfromthe
WizardLM-13B 60 GPT-3.5-Turbo 50 annotators.
WizardLM-30B 55 GPT-4 80
It is possible that a factually inconsistent sum-
mary sentence could have semantically different
Table11: Percentageofcorrectexplanationsacross
modelsonTOFUEVAL. WeshowHuman’sevaluation butvalidexplanations. Thiscouldpotentiallyim-
results on a small sampled set of 20×6=120 explana- pactthequalityofourmanualanalysisifamodel
tionswherebothhumanannotationsandmodelspredict provides a reasonable explanation that is consid-
thatthesummariescontainerrorsfromthemain-topic eredincorrectsimplybecausetheexplanationdoes
set,wherewehavemorediverseerrortypes.
not resemble the one provided by an annotator.
Toquantifythepotentialimpactofthis, fortasks
wherewehadcompletedtworoundsofannotation
withtheremainingerrortypecategorizations.
(see Section 3.4 for more details), we compared
H PerformanceinGenerating theexplanationsgeneratedbytwoannotatorswhen
explanations theybothidentifiedasentenceasfactuallyincon-
sistent. Thisinvestigationrevealedthattheexplana-
It has been observed that LLMs can generate cri-
tionswrittenbybothannotatorsweresemantically
tiques of model outputs, in some cases leading
equivalent over 95% of the time, suggesting that
to enhanced output quality (Madaan et al., 2023;
the alternative model-generated explanations
Saundersetal.,2022). Wenowinvestigatewhether
should not require much concern in TOFUE-
LLMs are capable of generating correct explana-
VAL.
tionsforfactuallyinconsistentsentences.21 Inpar-
WeobtainedaFleissKappascore(Fleiss,1971)
ticular, we focused on examples where both hu-
of0.65,indicatingsubstantialagreement. Wetook
mansandLLMslabeledthesummarysentenceas
themajorityvotetoobtainthefinallabelforeach
factually inconsistent, and we examine whether
model-generatedexplanation(supportedornotsup-
LLMscangeneratecorrectexplanationsofthefac-
ported by the human explanation) and calculated
tualinconsistenciesinthesecases.
theexplanationaccuracyforeachLLM-basedeval-
uatorusingthefinalizedlabels. Theresultispro-
Human Evaluation We randomly sampled 20
vided in Table 11, where it can be observed that
summary sentences to conduct a small-scale hu-
GPT-4iscapableofprovidingcorrectexplanations
manevaluationtask. Sentenceswereonlyselected
80%ofthetimewhenitidentifiesthatasummary
if one of the eight LLM-based evaluators and a
sentence is factually inconsistent with the source
humanannotatorlabeledthesentenceasfactually
document. Other models provide accurate expla-
inconsistent. We only sampled from main-topic
nations about half of the time without significant
summary sentences since they contain a more di-
differencesbetweenthemodels.
verserangeoferrortypes(Figure2). Threeauthors
ofthisworkmanuallyevaluatedthequalityofthe
I ComputingInfrastructure
model-generatedexplanationsbycomparingthem
to human-written explanations. Specifically, hu- Forinferenceontheproprietarymodels,weused
manswereprovidedwith(1)awholesummary;(2) theofficialAPIs. ForLLMswith7Band13Bpa-
afactuallyinconsistentsentenceinthesummary; rameters,weutilizedaclusteroffourTeslaV100-
(3)ahuman-writtenexplanationforthesentence; SXM2 GPUs, each with 16GB memory. For the
and (4) a model’s explanation. We asked annota- larger30Band33BLLMs,weusedfourNVIDIA
torstoperformabinaryclassificationtaskinwhich A100-SXM4GPUs, eachwith40GBofmemory.
they determined whether the model-provided ex- Fornon-LLM-basedmodels,weusedasingleTesla
planation was supported by or equivalent to the V100-SXM2GPU.
For API calls, we use gpt-3.5-turbo for
21SinceweobservedthatalltestedLLMsperformworse
GPT-3.5-Turbo;gpt-4forGPT-4.
atbinaryfactualerrordetectionwhenpromptedtoconsidera
wholesummaryratherthananisolatedsummarysentence,we
onlyevaluatedthequalityoftheLLM-generatedexplanations
atthesentencelevel.MediaSum-DocumentID:CNN-25553
CAROLLIN,CNNANCHOR:Well,thegovernmentisalsoabouttotelluswhatairlinepassengers
alreadyknow: thattheserviceislessthanperfect,evenafterthenation’scarrierspromisedtoupgrade
service. AreportcomesfromtheTransportationDepartment’sinspectorgeneralinlessthanfourhours.
Andwearejoinedbyatravelexpert,TerryTrippler,inMinneapolis. Terry,Iwonderifyou’vebeenflying
latelybecauseyougotachanceatleasttopeakatthepreliminaryreports. Whatarewelikelytohear?
TERRYTRIPPLER,TRAVELEXPERT:Ithinkwhatwe’regoingtoseeissomethingsimilartothe
interimreportthatcameoutinJuneoflastyear: improvement,butalongwaytogo. AndIthinkthat’s
whatwe’regoingtohavehappenagain. We’regoingtoseethisnoon.
LIN:Well,let’stouchonatleastsomeofthepromisesthattheairlinessaidthattheywouldtrytowork
on. Forexample,whenIgoaheadandIbookmyticket,amIguaranteedthattheairlineisgoingtoquote
methecheapestfair?
TRIPPLER:That’satoughone,Carol. Theyhavepromisedtodothat. Someoftheairlinesaremaking
prettygoodonthatpromise. Otheronesaren’tdoingtoowell. Basically, wheretheproblemliesisin
theselast-minuteInternetfaresthattheyhavethat–therearesomepassengersclaimthey’renotbeing
toldabout. Sothereneedstobesomeimprovementonthatarea.
LIN:Allright. Well,whatare–aretheygoingtobeabletotellme–orwilltheytellmeiftheflightis
oversoldasIbookmyseat?
TRIPPLER:Ifyouask,fromwhatIgather,theyaretellingyouiftheflightisoversold. Whatwefind
happening on this one is, once the passenger is finding the flight is oversold, they book on that flight
becausetheywanttogetvoluntarilybumpedandgetthemilesandthemoneyandthemeals,etcetera. So
thatonesortofbackfiredonthem.
LIN:Allright,let’ssaytheylosemyluggage. Howlongisitgoingtotakeformetogetitbackthese
days?
TRIPPLER:Theyclaimthey’lldoitin24hours. Luggagecomplaintsareup. And,ofcourse,werecently
allhaveseenthefilmofwheretheluggagehandlerswereplayingbasketballwithpeople’spackages,his
luggage. Thatdidnothelp. Complaintsareup. They’regoingtohavetodoabetterjobonluggage.
LIN:Allright,well,also,Ifindmyselfmoreoftenthannotsittingonaplane,gettingreadytotaxithe
runway,andsuddenlyeverythingcomestoahalt. AndI’mtoldthatitisproblemswithairtrafficcontrol
orsomethingthatreallydoesn’tmeanmuchtomeasapassenger. AmIgoingtobetold,orshouldIbe
told–amIbeingtoldwhyI’mbeingdelayedspecifically?
TRIPPLER:Well,theysaytheyaretellingyou. Andhere’swherethebigproblemlies. Andtheseare
thecomplaintsthatIamreceivingbytheliterallyhundredsperdayine-mails. Peoplefeelthey’renot
beingtoldthetruth. They’renotbeingtoldbeforetheyboardtheaircraftthatthere’sapossibilitythat
they’llbedelayed. Imean,peopleareboardingaircraft–Idid,Iboardedone,wentoutandsatattheend
ofthetarmac. Iwastherelongenoughtoqualifytovoteinthatprecinct. They’vegottodoabetterjobon
this. Getoutofthegate. Getofftheground.
LIN:Soifthey’renotquitemeetingthepromisesthattheysaidthattheywouldkeeptoCongress,does
thismeanthatCongresswillorshouldgoaheadwithlegislationtoforcetheairlinestogivebetterservice?
TRIPPLER:Well,Carol,Ithinkthat,assoonasthisreportisissued,thatwe’regoingtohavepeople
lined up all the way around the Capitol to file a bill for passenger-rights legislation. Already we have
oneortwoofthe1999billsthathavebeenresurrected. Thosewerebillsthatwereputasidewhenthe
airlinespromisedtodoabetterjob. Yes,Ithinktheairlines,unfortunately–andI’mafree-enterprise
person–but,unfortunately,Ithinktheairlineshaveforfeitedtheirrighttooperatewithoutsometypeof
governmentintervention. Ithinkit’sinevitable. It’sgottohappen.
Table12: AdialogueexamplewithgeneratedsummariesandhumanannotationsinTOFUEVAL(part1). Weshow
allthreegeneratedtopicsforthedialogueinthetable. Foreachtopic,weshowoneofthefivegeneratedsummaries
withhuman-writtenexplanationsanderrortypesforfactuallyinconsistentsentences.MediaSum-DocumentID:CNN-25553
LIN:Well,andsomeairlinesaregoingtosaythatsomanyfactorsareoutoftheircontrol: likeweather
andnowlaborproblems. Delta’spilotsareexpectedtoannouncetodaywhattheyvotedonFriday: whether
they’reactuallygoingtogoonstrike.
TRIPPLER:That’sright. Andwe’retalkingNorthwestnow,Delta. Beforetheyear’sover,we’regoing
tobetalkingtoAmericanandUnited. AndIdisagreewiththeairlinesthere. Ibelievethoseareintheir
control. Weather,Iunderstand. Labor: Comeon,airlines,let’sgetittogether.
LIN:Allright,thankyouverymuchforthatpreview. Itreallymakesyouwanttohittheroad,doesn’tit?
LIN:Thanks,Mr. Trippler.
TRIPPLER:Thankyou.
Topic: Airlines’commitmenttoserviceimprovements(main)
Summary: [1]Theairlinespromisedserviceimprovementsbuthavenotfullydelivered. [2]Complaints
remainoninaccuratefarequotes,overbooking,delayedluggagereturnandlackofcommunicationduring
delays. [3]Governmentinterventionmaybeneededasairlineshaveforfeitedtherighttoself-regulate.
Explanationfor[2]: Re: farequotes,thecomplaintisaboutpricetransparencyratherthanpriceaccuracy.
NuancedMeaningShift
Explanationfor[3]: ItistheopinionofTrippler,andnotafact,thattheairlineshave“forfeited”theright
toself-regulate. Statingopinionasfact
Topic: Factorsaffectingairlines’controloverservicequality(main)
Summary: [1]Thegovernmentissettoreleaseareportontheservicequalityofairlines,withimprove-
ments needed in areas such as quoting the cheapest fare, informing passengers about oversold flights,
handlingluggagecomplaints,andprovidingaccurateinformationaboutdelays. [2]Thereportmayleadto
legislationforpassenger-rightsandgovernmentinterventionduetotheairlines’failuretomeetpromises.
[3]Someairlinesarguethatfactorslikeweatherandlaborproblemsarebeyondtheircontrol,butexperts
disagree.
Explanationfor[3]: Expertsdon’tdisagreethate.g. weatherisbeyondtheairlines’control. Contradiction
Topic: Passengers’rightsandlegislation(main)
Summary: [1]Thegovernmentreportwillshowairlineshaven’tfullydeliveredonservicepromises. [2]
Passengerscomplainofinaccuratefarequotes,lackofinfoondelays/oversoldflights,mishandledbaggage.
[3]Congressmaypasslegislationtomandatepassengerrightsasairlineshaveforfeitedself-regulation.
Explanationfor[1]: Itisnotstatedinthedocumentwhetherthereportwillactuallyshowthatairlines
haven’tfullydeliveredonservicepromises. Itisthetravelexpert’sopinion. Statingopinionasfact
Explanationfor[2]: Itisnotaboutinaccuratefarequotebutnotgettingthecheapestquote. Nuanced
MeaningShift
Explanationfor[3]: “Airlineshaveforfeitedself-regulation”ispresentedaspostulationinthetext-the
summarymakesitseemslikeafact. Statingopinionasfact
Table13: AdialogueexamplewithgeneratedsummariesandhumanannotationsinTOFUEVAL(part2). Weshow
allthreegeneratedtopicsforthedialogueinthetable. Foreachtopic,weshowoneofthefivegeneratedsummaries
withhuman-writtenexplanationsanderrortypesforfactuallyinconsistentsentences.Figure6: Screenshotofannotationinterfacefor(1)factualconsistencyevaluation;(2)relevanceevaluation;and(3)
topiccategorization.Figure 7: (Continue) Screenshot of annotation interface for (1) factual consistency evaluation; (2) relevance
evaluation;and(3)topiccategorization. Weprovidesomepossibleerrorcategoriesthatannotatorscouldlookfor
duringtheannotation. Notethatwedevisedtheseerrorcategoriesduringthepracticesessionandthisisnot
thefinalversionofouterrortaxonomymentionedinSection4.Figure8: Screenshotofannotationinterfaceforcompletenessevaluation. Thisisaseparateannotationtaskfromthe
previousoneduetotheworkload.Figure9: (Continue)Screenshotofannotationinterfaceforcompletenessevaluation.