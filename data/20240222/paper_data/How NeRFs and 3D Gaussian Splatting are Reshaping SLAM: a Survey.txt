1
How NeRFs and 3D Gaussian Splatting are
Reshaping SLAM: a Survey
Fabio Tosi1 Youmin Zhang1,2 Ziren Gong1 Erik Sandstro¨m3
Stefano Mattoccia1 Martin R. Oswald3,4 Matteo Poggi1
1UniversityofBologna,Italy 2RockUniverse,China 3ETHZurich,Switzerland 4UniversityofAmsterdam,Netherlands
Abstract—Overthepasttwodecades,researchinthefieldofSimultaneousLocalizationandMapping(SLAM)hasundergonea
significantevolution,highlightingitscriticalroleinenablingautonomousexplorationofunknownenvironments.Thisevolutionranges
fromhand-craftedmethods,throughtheeraofdeeplearning,tomorerecentdevelopmentsfocusedonNeuralRadianceFields
(NeRFs)and3DGaussianSplatting(3DGS)representations.Recognizingthegrowingbodyofresearchandtheabsenceofa
comprehensivesurveyonthetopic,thispaperaimstoprovidethefirstcomprehensiveoverviewofSLAMprogressthroughthelensof
thelatestadvancementsinradiancefields.Itshedslightonthebackground,evolutionarypath,inherentstrengthsandlimitations,and
servesasafundamentalreferencetohighlightthedynamicprogressandspecificchallenges.
IndexTerms—SimultaneousLocalizationandMapping,SLAM,DeepLearning,NeuralRadianceField,NeRF,3DGaussianSplatting
✦
NICER-SLAM[18]
ORB-SLAM[10] Code-SLAM[11]
iMAP[1]
DIM-SLAM[16]
iLabel[7]
NeRF-LOAM[15] SR LG AB M UncLe-SLAM[17] Our
Semantic
RN Ge BR -F D- s Sty Ll Ae M SLAM L Si LD AA MR U En sc tie mrt aa ti in ot ny Survey
2011 2017 2020
Hand-crafted DeepLearning 1 RadianceFields ≈4 ≈41 ≈12
2021 2022 2023 2024 (February)
2015 2018
Submaps-based
Surveys Surveys SLAM 3D Gaussian Event-based
Grisettiet al., 2010[19] Taketomiet al., 2017[22] Splatting SLAM
Yousifet al., 2015[20] Duanet al., 2019[23] MeSLAM[6] EnD viy rn oa nm mi ec nts
Cadenaet al., 2016[21] Mokssitet al., 2023[24] GS-SLAM[12] EN-SLAM[14] DDN-SLAM[9]
KinectFusion[2] … CNN-SLAM[3] … Structure-SLAM[4] NICE-SLAM[5] DN-SLAM[13] Hi-Map[8]
Fig. 1: Timeline SLAM Evolution. This timeline begins by illustrating the transition from hand-crafted to deep learning
techniques,featuringkeysurveysfrombotheras.In2021,apivotalshiftfocusesonradiance-field-basedSLAMsystems,marked
byiMap[1].Thecirclesontherightsideofthefigurerepresentkeypapersforeachyear,withsizeindicatingpublicationvolume.
Theoutercirclefor2024signalsaprojectedsurge,highlightingthegrowinginterestinNeRFand3DGS-inspiredSLAM.
1 INTRODUCTION were prevalent choices due to their high precision, de-
spitebeingcumbersomeandcostly.Subsequently,thefocus
Simultaneous Localization and Mapping (SLAM) is a fun-
shiftedtowardsvisualsensorssuchasmonocular/stereoor
damental concept in the fields of computer vision and
RGB-D cameras, which offer advantages in terms of porta-
robotics. It addresses the challenge of enabling machines
bility,cost-effectiveness,anddeploymentease.Thesevisual
to autonomously navigate and incrementally build a map
sensors enable Visual Simultaneous Localization and Map-
of unknown environments (mapping) while simultaneously
ping (VSLAM) systems to capture more detailed environ-
determiningtheirownpositionandorientation(tracking).
mentalinformation,improveprecisepositioningincomplex
Originally conceived for robotics and automated sys-
scenarios,anddeliverversatileandaccessiblesolutions.
tems,thedemandforSLAMhasexpandedintoavarietyof
As we outline the ideal SLAM criteria, several key
domains, including augmented reality (AR), visual surveil-
aspects emerge. These include global consistency, robust
lance, medical applications, and beyond. To meet these
camera tracking, accurate surface modeling, real-time per-
needs,researchershavefocusedondevelopingmethodsfor
formance,accuratepredictioninunobservedregions,scala-
machinestoautonomouslyconstructincreasinglyhighlyac-
bilitytolargescenes,androbustnesstonoisydata.
curatescenerepresentations,influencedbytheconvergence
of robotics, computer vision, sensor technology, and the Overtheyears,SLAMmethodologieshaveevolvedsig-
recentprogressinartificialintelligence(AI). nificantlytomeetthesespecificrequirements.Attheoutset,
Typically, SLAM techniques rely on the integration of hand-crafted algorithms [2], [10], [25], [26], [27] demon-
diversesensingtechnologies,includingcameras,laserrange strated remarkable real-time performance and scalability.
instruments,inertialdevices,andGPS,toeffectivelyaccom- However, they face challenges in strong illumination, ra-
plish the task at hand. Initially, sonar and LiDAR sensors diometric changes, and dynamic/poorly textured environ-
4202
beF
02
]VC.sc[
1v55231.2042:viXra2
ments, resulting in unsatisfactory performance. The incor- Section3isthecoreofourpaper,focusingonkeyNeRF
•
poration of advanced techniques, employing deep learning and 3DGS-inspired SLAM techniques and our struc-
methodologies [3], [4], [11], [28], became crucial in improv- turedtaxonomyfororganizingtheseadvancements.
ingtheprecisionandreliabilityoflocalizationandmapping. Section4presentsquantitativeresultsevaluatingSLAM
•
This integration takes advantage of the robust feature ex- frameworks in tracking, mapping, rendering, and per-
tractioncapabilitiesofdeepneuralnetworks,whicharepar- formanceanalysisacrossdiversescenarios.
ticularly effective in challenging conditions. Nonetheless, Sections 5 and 6 focus on limitations, future research
•
their dependence on extensive training data and accurate directions,andsummarizethesurveycomprehensively.
groundtruthannotationslimitstheirabilitytogeneralizeto
unseenscenarios.Furthermore,bothhand-craftedanddeep
2 BACKGROUND
learning-based methods encounter limitations related to
2.1 ExistingSLAMSurveys
using discrete surface representations (point/surfel clouds
[29], [30], voxel hashing [31], voxel-grids [2], octrees [32]), SLAM has seen significant growth, resulting in a variety
which lead to challenges such as sparse 3D modeling, lim- range of comprehensive survey papers. In the early stages,
itedspatialresolutionanddistortionduringthereconstruc- Durrant-Whyte and Bailey introduced the probabilistic na-
tionprocess.Additionally,accuratelyestimatinggeometries ture of the SLAM problem and highlighted key methods,
inunobservedareasremainsanongoinghurdle. alongside implementations [39], [40]. Grisetti et al. [19]
Driven by the need to overcome existing obstacles and furtherdelvedintothegraph-basedSLAMproblem,empha-
influenced by the success of recent Neural Radiance Fields sizing its role in navigating in unknown environments. In
(NeRF) [33] and 3D Gaussian Splatting (3DGS) [34] repre- thefieldofvisualSLAM,Yousif[20]providedanoverview
sentations in high-fidelity view synthesis, along with the oflocalizationandmappingtechniques,incorporatingbasic
introduction of learned representations for modeling geo- methodsandadvancesinvisualodometryandSLAM.The
metricfields[35],[36],[37]–extensivelydiscussedin[38]–a advent of multiple-robot systems led to Saeedi and Clark
revolutionisreshapingSLAMsystems.Leveraginginsights [41] reviewing state-of-the-art approaches, with a focus on
fromcontemporaryresearch,theseapproachesofferseveral multiple-robot SLAM challenges and solutions. Cadena et
advantages over previous methods, including continuous al. [21] presented a comprehensive reflection on the his-
surfacemodeling,reducedmemoryrequirements,improved tory,robustness,andnewfrontiersofSLAM,addressingits
noise/outlierhandling,andenhancedholefillingandscene evolving significance across real-world applications. Take-
inpainting capabilities for occluded or sparse observations. tomi et al. [22] categorized and summarized VSLAM algo-
In addition, they have the potential to produce denser and rithmsfrom2010to2016,classifyingthembasedonfeature-
morecompactmapsthatcanbereconstructedas3Dmeshes based, direct, and RGB-D camera approaches. Saputra et
atarbitraryresolutions.However,itisimportanttonotethat al. [42] addressed the challenge of dynamic environments
at this early stage, the strengths of each technique coexist in VSLAM and Structure from Motion (SfM), presenting a
with specific challenges and limitations. As such, the field taxonomy of techniques for reconstruction, segmentation
is constantly evolving, and continuous investigation and and tracking of dynamic objects. The integration of deep
innovationarerequiredtomakefurtherprogress. learning with SLAM was meticulously examined by Duan
InresponsetothelackofSLAMsurveysfocusingonthe et al. [23], highlighting the progression of deep learning
latest developments and the growing interest in research methodsinvisualSLAM.Insensor-specificcontexts,Zaffar
exploring this paradigm, this paper conducts a thorough etal.[43]discussedsensorsemployedinSLAM,whileYang
reviewofcontemporaryradiancefield-inspiredSLAMtech- et al. [44] and Zhao et al. [45] explored the applications
niques.Specifically,weundertakeanin-depthinvestigation of LiDAR and underwater SLAM, respectively. In recent
of 58 SLAM systems that have emerged in the past three years, deep learning-based VSLAM has gained consider-
years,reflectingtherapidpaceofprogressinthefield. able attention, extensively covered in [46], [47], [48], [49].
Thisevolution isillustrated inFigure 1,whichprovides Notably, [50] delves into recent advancements in RGB-D
a visual timeline of the current state of SLAM advance- scene reconstruction. Ongoing developments in SLAM are
ments. We aim to fill the existing gap in the survey lit- explored in surveys like [51], focusing on active SLAM
erature by closely examining and analyzing these cutting- strategiesforprecisemappingthroughmotionplanning.
edge techniques, and by highlighting the rapid emergence Within the existing literature, notably in influential
of innovative solutions aimed at improving their inherent works like [52], two principal SLAM strategies emerge as
weaknesses. Through a detailed exploration, we intend to the frame-to-frame and frame-to-model tracking approaches.
categorizethesemethods,tracetheirprogression,andoffer Typically, the former strategy is used in real-time systems,
insights that are tailored to the specific requirements of ofteninvolvingfurtheroptimizationoftheestimatedposes
SLAM.Byservingasavaluableresourceforboththenovice throughloop-closure(LC)orglobalBundleAdjustment(BA),
and the expert, we believe that this survey represents a whereas the latter estimates camera poses from the recon-
significantcornerstoneforthefutureofthisparadigm. structed3Dmodel,oftenavoidingfurtheroptimizations,yet
Theupcomingsectionswillbeorganizedasfollows: resultinglessscalabletolargescenes.Thesestrategiesform
Section 2 provides an overview of existing SLAM sur- thebasisforthemethodologiesweareabouttodelveinto.
•
veys(2.1),delvesintorecentradiance-fieldvolumeren- While existing surveys cover traditional and deep
dering theory (2.2), introduces prevalent datasets and learning-basedapproaches,therecentliteraturelacksacom-
benchmarks in the field (2.3), and presents the main prehensive exploration of the advancing frontiersin SLAM
qualityassessmentmetricsusedinthiscontext(2.4). techniquesrootedinthelatestprogressinradiancefields.3 3 3 3
MLP MLP MLP
MLP MLP MLP
ψ(x) ψ(x) ψ(x)
c Sperical c Sperical c Sperical
x c x x x c x x x x Harmocni xcs x x Harmonics x Harmonics
f Θ f Θ f Θ f Θ f Θ f Θ
Fig.2:Illustrationcomparingimplicit,explicitanFFdiiggh.y.2b2:r:IildlCusosctmernapetiaorrenipscrooenmseponaftariStnicogeninmmepelRitcheiotp,dersxe.psalei)cniIttmaaF tpnii lg odic. nih2 tsy: u:bIl trIl iiu mldis zpst ercla seit cnai ioetnn ,reec Eupo xrmrapelpsliena cnr eiitttnaw,gtioaoi rnmnkdpmlicHeittyh, beoxrdpisdli.c .ait F)a rIn omd mplhicy liebtfrutidttis olcize ren isge hare tn:per Iue msreapnllitnca iet titowunosm erkset aho nd es u. ra a) lImplicitutilizesaneuralnetwork
toapproximatearadiancefield,whileb)Explictnioteamtwpopodrroeklxsitmcooanatepdpuacrrotaxvdioimalnuacmteeefiareelrdna,ddweirahinnilcgeedbfi)ireEelcxdtp,llt yio ecxoiatpnpmlip lceor io atdx recnilm osendacdtoe sunpa cdatur tsa icadtvli ova fenolualcutme umfi reeeel srdree–, nnw ddh eei rrl iie nnbgg)ddEiixrrep ecl citc tliylt ymoono ndlee lal esranc reo ndn ed dsupc sat ptiv aaoltilfauelmatfeuearree tsund r–eer (in vg oxd eir le sc ,tlyonlearnedspatialfeatures–
voxels,hashgrids,neuralpoints,andmore–evxhocaxlsue hdlsig,nrhgia dns she,ug err tacidl.)sc,,oenmxe cpu lor uandleipnnotgsi.n nFts ei,nuaarn alld ly,cmco)o mhre pybo–rnv iedo exx nmcel tlu sos ,d, dahien na lgss dh innhgecyur obird rraps il d, ocn rioae nmu tcer opa lrol epnap oero nn ri an ets tdt e.s, sFai lnn ead al rlmy n, eoc dr)ehs–ypbe arx tic idl aum ld foi end age tulns re eiun srca ψolrcp woom ir tap htoen nle een uatr rsn a.e lFdi nn ea tl wly, oc r) kh sy .bridmodelsincorporatelearned
spatial features with neural networks. Both hybs Bproiadtt hiaanl hdyfe baerxt iu dprliaecnsitdwaeipt xhpprlnoiceaiucthr aaeplspnerenotaw abcolherk eassc. ecB neolaet bhralehts ey ap db ca r ct iti d era all a einnf re aida ntt egeu dxr ape tnls ridc aw i itrnit ea ih np ndpn gere o aru ia nr nca dghl e rn bese unt etw dno ear rbk il nes. gaB c bo c ut eh tle rhr eya qbter uidd irt ea rn a ad i dne dinx ipg tl ii oac nint ada lp r mep nr eo d ma ec r oh i rne ys g re b en u sa t ob ule rca ec sc .eleratedtrainingand renderingbut
demandadditionalmemoryresources.SWAPFIGURES.
demandadditionalmemoryresources.SWAPFdIGemUaRnEdSa.dditionalmemoryresources.SWAPFIGURES.
2.2 ProgressinRadianceFieldTheory
b we eir ge hp tr ses oe fn ate nd ei um rap lli nc eit tl wy, ob ry ke on rc eo xd pi ln icg iti lt y,en bytir melb w ayT se cph ew rpire ige bii nhtpt eh te gr ssiern lsm to ihe gt fn h ehart e te ba ndd ee hii ua lm aarn vtap ec l iHle doi nc re e tfii r rt atel e aw nyl, n,d doT sb r m( dy krte ii)e o tsf tne tr= arrc e niso bxe cd upx etiop l tn fi ic rg o(cid:16) a oi nt−i mlrt y oe(cid:82) ,e tp ftn bt 11r lyt ieσi tgr os mb w w(e here l tae iyn t( tisr p aht wge w) la p ihp o) nt iiid t nr ti nt soe hh as ggs no(cid:17) iie dnn tf ln hid ita g st at h eeh ce hnan rd te rt ehet ao ti u ryetm d eer .la se esp a pl- -l tti an ehc H tdeei it t ael w tay lr rc, e ao scb t, nru ry kT s um m ce ( ot tn ur i u) tc -e t ro = ax ed np li e cl in i kx eC cg ep i ft(i rl (cid:16)vt r oy o−,)e m xn b= eRyt t ltit 1r 1m (cid:90)ge σ tl rtoay i1(t dpr2 tw ( spTs .aii n)t l E(h o) gt xd ni )n ps l gσ i l(cid:17)gt i( th ch hrd ie t t( eetn r) ao) la ycte .t( esrH Tdt( hhe t t er r)ee a, ia, nd ncT s t)c m e( du gt imt) rtt a= au ln- ce c ox e mp fr p(cid:16)o u− m ta( Rt1 ttt 1 i1) oσ t no(r ut( ss a e) lo s)d n qs g u(cid:17)t ahd dee rn ara to uyte r. est bh yea dc ivcu idm inu g-
within a discrete spatial structure like voxel grw didiit msh .i eEn nxsa piold incis iatc lre st pe as cp eTa .ht Ii etal iens ntt cer au gpc rt asu lur cle aotl mi ek spe huv too awx tie oll nigrg er uhpi std rees isns. etE qnex utrap aatl cdii otc rsni at stwutyritephicbaT ylh lyde iovi fnHfidete reingrfaer gsa,tldercto adm cecp ensuostta bet usio ttn rheequ uds ieirfs efemqrueoanrd etiraalt tudhr eiesrtabayyncid netiv otri Ndavin eeg vleednlyb-ysptahceedbins:
representationstypicallyofferfasteraccessbutrr see uqp rur fe iars cee en mst ,a ot rmi eon ats ett rhy ip eali rc saa ,yll ay inno dtf ofe tNr hf ea es vst eue nr rlra yoc -uc se pns ads cib enu dgmt br eeie nmnq svu o:ii rr rye onm amno dr ee nhta.vt Ih ete rr ea sry oalyi un tat io otneN acce ohv nie snn trtl aey ig- nsrtpas,aticowend hisb lteienpis m:.Tphliceittermsσ(r(t))andc(r(t),d)
memory and have resolution constraints, while implicit
memory and have resolution constraints, whcialenbimeprelipcritesentedimplicitly,byencodingreitpreensteinretaltyiownsithpirnovidreeparecsoemnptatchtesvceonleumenecoddeinnsgitywiathnd color at point r N(t) along i 1
r p a
o
n
he p
f
e
aop
t
spt
e
wr e hxre n o
ip
os nt ae rli gica kn chl st
i
gla
t
,ey rlt s
y
iui dho t ssa sin ig tk ns
o
[h e
g
5re
e
1p ar
d
]vr d ,o r av
l
[ev
ro
5an ii
c
2ond d
a
]ute ,le a sr mg lia aen
s
ut
teg oc
lrn
tuo f ic
t
-m cbo
rtf
em o uep st
a
ra ohp etc luu
s
ut b rt ty ea
s
is
s
ot uc ui ne co asn hn ni dne a
d
eg al nse sn a shn e esacr p a o
n
h
[ct l E
q
i
we
p
5e o li mp oh f
ve
ao d
l
ug x 3p ad
o
itm spe t oes ]tp wh
ri
pr e hi
w
,xr
x
hr. se n n bt lw lo
ie
op
o
e
es i niHg t i pa c ccwe lrnlni ge mic i ta
k
in y ovc gea t ri thiw l sgt b i
eo
tgut tl
o
ra t ,e r ery h i eih r
x
rli
r
ro et es
d
nyt i ii
a u
ei et nd dph n ph no
s
tts
ll s
osa isrin
rmi
ag te dak eo ns o
[
lh s se eef
g
5
lr apd e
e
ye sme 1p naa lnr ni d
]v
[er sd
hd,o
5t to sr acn wav
a
il 4[rev r
r
go c5te t ta ]yn e ihi
i
oi ic h,2ou nd o otd ea
la]a
euaetre o,n nrle C xa
n
ra
nse
rr ms ss ig l dl (
,
si da rap een sr
-eδ
ut ptn
vt
aa teg y) nihoc
l
ir
r
le ant
t
+p
u
io do= f dai it lc gta
-
1m i uw
c vv
eebo
r
ncl .tf
e
an(cid:88) rii em o uae ep o Td=N is
s
tot a dl nra er ot rhp he1l ettrk deyc glu
e
tu esαu ut b
s
rasrt caio
o
poc tiy ea
ps
its T ott cr lf as ot uhc u luu f
o
mi i
sr
ane co eec ae r aats
m
nn hrn n iex i pi
mmi do
en,e na dp
p
uf es ng al 2petal anu tese si Dts lan cac
ecr
sp a o n
h
[
uhn ecT t
5
te
tv
ei
o
rp sfh e e ao nac gict 3i pe
spa
ot snpo rlt sl oeo rv]wd lye
h
o= la
a
(c n,
id xr rsom on , a ds σo
i idr
es
b
op o
t
ai w
n
nx. sct
r
sn a nb b ce
i
eerli lgv
e
eac t,ic tHa k rxg tiy [ec nrlenh ecn wio 5l isp i egnl y
v
ds et , ige e 5axy eem a(cid:16)w rl
e
)s tb os eu ]t ry ne
i
lu
s pe
,i
nd
ir xh
di
oi r−la ot ,b dcns
n
oot i osa a esi nsdph n oig u wg t
s
riddk lln .o cj(cid:88) ngi
[
dph r te− ei= g
5o
Hr thi ce ii se
1
td n1 1ra nn
a
sir hd
ap]
yve [ld ess tσ
,
gg
5e
enl- -er a te.v l
e
4[j se dro
c5
ra δ
ts
]n i
w u
a[c
,2
hon j
c
5d ta
a
y]t(cid:17) eut t t tht
i o
6,ile ,xh i r ha vs
e
]lo ar dma ig rl
o
.e erC si ea
n
en n een es Fr-uTt sc
d
ransa t( ,eg s io pelr aa
h
glar n ml
sδ
tu
i
vtf l ym
i
e)
m
uc gi( iet iy -c
t
a2 cbo +i
r
n
ry= d, tf it le
-
-)
e
em o iue
e1 n
ut eT nsr t aa d.r
2to
anXh ipa tet n
T
o=N ( tleu
o
tu s
vu
etr b c rh1 gtrt i) Na ditey ee sa
e
psα riy uss ot u a=
l
afu pii
an
aerco
t
lTw a ts
a
nh
loh vn nii lsrcde een yi mca ed aaot x eg ail 2nh
mmi
inm, p nss lDtn
l
la shv
pe
yt
u1(cid:16)e peesa
t
gci
l-
se p er− ee t ul soT
r
tvd vl o ra rw ipom toi
ps d
as ar(cid:82) aw axt. s to=
l
si (b t
t
e. cen t e1 σH
i
ibi
[
sel
nonn 5g e ieσy v
d t
tnge
,
5ax
t
hdb ( ou
cw
]t
r
ip ,i
b
er ir x iri uo
d
a(cid:16)
oe
si )r ( a ied n
s
elne
se
rs ll
i
o− enc n)
s
nst ) :di cj X gi ,
ap
to− i=
o q
id
cnl
vtn n1 1
u ae
hs
d
ess
tσ w ad e(cid:17)
lee
ycj
t
dc, h
r.i
oδ ti
u r
ahr Aej ls
a a
oe ytrC (cid:17)
en di
te ,rs t
v ud
dh( , dp reer eδ
r
ee ie vt)
te
si nisc
ia+
pa oa= d st
lb e1m
u
ni c i( e tv 2
.
cyc aanXi
y-
-) T=e u lto lel
h
d1 ym ty de ,eα i. s
v
αu apA i itT itl
a
dhad i
s
=rec it aad nei mmi (,i d n
g
1- pete
−t
ler eT v
r
ei psa xo= l
( pσ
ib (ne
i
−e t,x t
c
σw ip
i
ia(cid:16) e
)
δle
ii
o− )n
n
n
)dcj X
g
c− i= o
hc
tn1
a
h
as tσ reee aj c
r
cδ tu
ah
tj eyt(cid:17) ei r,v
id
r
ze
ee
esns spa
s
tem i( ht2
cy
e- -)
[ u5 n3 o], rdoc et rr ee de pn oo id ne ts s[ e5 t4 s] [, 5a 6x ]i .s F-a igli ugn reed 2t vr ii sp ula an lle y2 ilD lug u sbr tnr ri iod adr ts d ea[ se5 pr t5 e hp] d, ero o sp er aocihntetissveettalsyk.[e5A6ad].ddFvitiagionuntraaegll2ey,voαisf iub=aollt(yh1i−lblruyeespxturrpesa(sit−enensgσtat iahδtie ioc)sn)oesmc.hRbat oeii r pc-v ae ae cn cl ty te it. pr yA irzo rd e egsd sr uei tt hs li tso e inn in ga tll h fy re o, fiα meil aN= d lph( ha1 s a−s ci oge mx np i pfi( c o−a sσ n itti ilδ nyi g)) ac tohp sa aar Tmcai htc peyt le er er ie xi psz p1u oe eils cnti tt tn eh dg ie .f dro epm thal ap lh oa ngco am rp ao ys ,it inin sg tea at ds ,a cm anpl be epo cain lct ui -.
representations.Recentprogressinthefieldhasr isn nne ia ep flg ut nr uie ro i es fi anne lc cn ao ent n daf ett le ti ay wox dnp ovsl ra.io kc nRpi sceta ,elc Tyc se ui hnt iss ey nt it npor eSgerr xLoes pudg Averl
aM
cteli trosn eisc odg mai unlf
e
dsr ttlo ehha spm oet te tdrfi hna uoetl lcalp od tf lh guoeha ia nrea et gi tscus sn h,osrfl
r
am pse oiug aus
u
rpen
r
acgno tai yhhificsn ,ceic utdt id aa
h
ni ln sn
ae
ssa rgt th udl lsey yaa psv aelta dalnos
o
r,a scw
f
ce em
i
as
m
np Ti pnl h be lie eS cp iL eo
t
cA x ai mn p lM ct oe uc di Cm -t. ee lde ( srth s)d uo e= cd p ho tl h ao sg a Ni le αo es ni, RTgp Fia ca ar int ,ri dc au y m,la T oir nirly es=teaeldax,tpecdanu −sbine− gctaσhlecjuδaj-ccumula(t2e)dtransmittance:
i t r dn h e ecfl r so eu cnu re itgn pehc tx ie ot pd h nle i sa cud i otsv fmea thn eo etc f h se i eos m d ti p en s cl si hS c u niL ct iA h qm uM ao esd s3m e aDl re s eGth s gSuo i.cd vBh eo r nl a io es bg fN ei be lu oes, R wtt r dv g sh ep iF .e unro cra si ao e fax d cr on lu net r lsri itlg ydc m p[ehu h t5 im x a il la 5 ota p lth s o ui] nr l vh ,e irl sscy e eui tu in o rt ns afg me o ttl ehrga eo sd et trf he sei ti dd e ho rm es d etu dp e ss[s c el 5 sii hpc n3 u rnoi ]g ect i, ih pqnm t[ uh rt5 a eo ee4 s s dsd s] ea e3 (,e atc D nrl sm rcs )teGu a[s =u 5 gm tSu i6l i. otc v(cid:90)u ]i Bh n,- el rra nta sa ie 2t ,e ns es bf TwdodN eb (l l huu m toter dr )ieR wtt e cac oi ·siF hno .e n rc σn sn erfa h(m it o .n p rad re Fd (i tm vx t ie tiotp eg )nm a an )l usn ti rso ic ·e erc vir o te cete ev f de:m 2o t nth ,e x v tel t lea ih ys -lt eoe d td es cw tu s h ius hn ai cn ie nh qrg duea (t e,sh 3s tδ3 )ie ia +dD ra (d 1eGc r ,ec )gS nu wi. =vom B hetr Zu en(cid:88) iii l=e tsl e 1ba tf1 2tet b σhe l Tu od iewt ( atit i .nr )n na tf ·dn eo σs rr c(mm v r iaa (i lt t itt ni )ba v ) dn ee ·itc ctwe ad: e tt ee ,n t S(cid:16) h ic meon id lj (cid:88) as e= e rn lc1 ysu i tt t oi y(d v 3 E( )e ar q(cid:17) ) n .s d= a 2,m tcZ hp ot i1t ll s2 oe cT s r a( nt) b· eσ a( pr p(t r) o) x· imtd at t,
edas:
(3)
hadasignificantimpactonSLAMmte1thod2o.l2o.g1iesN,epurriamlRaraidlyianSciemev Fia liael ru ldla y(t Nte oed REa qFt .)s 2a ,m thp isle cap no bin et api pa rl oo xn ig mt ah tee dr aa sy :,respectively.Ad-
ditionally, α = (1 exp( σ δ )) characterizes the opacity N
2.2.1 NeuralRadianceField(NeRF)
2th.2r.o1ugNheuthraelRinacdoiarSpnimcoerilaaFtriieloylndto(oNfEeqRm.F2o),dtehlisscdaenr Inibve 2e 0da 2p 0fp ,rr Moomx ili dm eN nae hteR ad lF la es t: arle.s[u33lt]iinngtrofrdoui mcedalNpheR− aFc,oNamnipm− opsilii tcinii t,gatsamplepointi. Dˆ(r)= α it iT i. (4)
andmorerecentexplicitmethodssuchas3DGS.Below,we
I cn on2 t0 in20 u, oM usild ve on luh mal el te rt ical r. e[ p3 r3 e] si en nt tr ao td iou nc ,ed seN ttie nR gF,I c db
r
an oa
e
ar nn
c
rni2 de ot0 eiifl nn w2 m fy0 u
s
op,
t
ro srM d lu tui nace s ci onisl tt vd v -c ,
io
ere oi ln nlb uh ve –ma iel aN el wt ne re dt i sR c ya F 3l nr. De t[ h– p3
G
er3 e sf S] o is
,
si ern .ent
s
Ii tr nm sao etd i ca no ou g tD n nˆc ie a,e t( rld srr ae fe) sN otn tt= rie d tnR oue(cid:88) g ic d e=NF nro
x
ca, i 1a dn
p
onra dαt nlegn n ii rcn vief sitiu eoa twm i
t
nro Tn
av
tup nidn
o
is. osl dto li a
u
nc sv iv ni
mn
aueto - l, glrl eu fv ta trm i hice ce e ewt mr Ii s noc y l dn ar tee ht t lhp e
s
iT se dr ,e sh
t
cis u hs(e oe 4. isn ns) ie It tnna mx egt xpi c eo toe t
t
,n h hc n s, ot etD oe rˆ ds a made( s ct er t et c md)i utn me o= pg mp
l
ec ot Xa i tuoh y= hn l sn 1 oavae dtα aelw e so ni d stn t
p
pi isT og t at rra n oi ra.n sa a pn e- l osr sma ey di, uett pi san I itn nn hs gt cte e th oa e:i xsd pe, c
i
eo tc
h
cna te( t en r4 e dx) ib t m,e s poc oma sl eecu dm- ee pXi t t= h h1 od ss upp er ro vp io sis oe nu fs ri on mg e ex xp te ec rt ne ad
l
d e s s a t
w
vhe cx ia
e
eet erpr ia wnd l gmo di e
h
ic f i nf oi a to ft gsdi u nr nv e Θcn dpn lo ec iu ,o l lt ru ei fit edv o vm cee en ev tll
n
ie d, ri ooaev t .rr nw tgi ei eFe c ep dsw do srm re aat m =s o s so ny e ad fnn Mo l (e Θltt θp yl ih Ls ,n ,te , i ϕPeg ams t x )pi h (s pti M
p
az. hi rs
r
ne r eI u oen dsm xlea s t i- ic ee im-d no dt c Lh i -n ao m a so a tt n y cir d se nt eea i n ngrs n fe st eu Pm (i ao xt o e 3o p n 5u r, Dc Dl a dsc oe s s a t w
v
xeu 2 I c d
e
slo h)e c cx y pn
i
eo xf.vp ae een t e orsp u2s t tn
p
=ia w→n ro cc rv 2 ol g. nm do de ot li a leo 1 0e
r
oh ic iiu cf n n di
n
cnmn 2 oi
ft
(a ts m fft e ( i)
gi
is0 dt xui ou np
to
ncsii n i,Nv we ,eo rn o Θcna
n
n, adp
v
ylo Mt en v ur c σeg ti pr inu o, ,l l ts eot ria iu su )e hi fi ua zie oct ed
l
sS fl o ,l v urm )ce tvdL evan ev
.t
ml n ee ieA dl o, ri
vo
No laet d p e e
H
anR l. err nM iuw tgn m sre he ev ti oeF ea sc i e rh mp p wta ds io
d r
oo pd isI F aeaa et r clm rn r cleih
b
soe l wn a ,p at sa om i=to s lr amso nec yp nt Cr ye ttath itd [ i ,ef sc onr eno M5 ocoo gl (ie yo da Θl dets t7 tg θp p dyl i hl (na ,L r]s e o,n ,.t t erF sc , we ec , tii ϕPi peg arc[ lo )mt m hph i set 3xh )p[ ee
i
tn rh ( e t5e r ,p ip 3 dt nie iM epl
h
met az shi 8ds z] rr r pte ees r i nse. ] ar e he u o s nx rri ie(e g dt ssm ni z. eiN xt l oineoma es se t, io
s
et i I- tie nne r io te m er -d en ns np ad mt c Lo t nmRo sh a,i t-eta aoo m d a
t
hsim c eo dF a tp aens o ty ca oiu etd hs ce t te n)pt ene e
h
ir ni ocn ea
n
o
tgrln y ,f s oee rtyd ns sm nequ rP am d( dgi a ase tao ux io e iee r3n Lp r nep s nn 5 oN au r,t te
D
es s a t w v x
v
dd nt t ich Dthl a ud r ue c gs
m
nh i
i
eo ie se ee e ee lot e =r e tnu
n
o)
pgc
uy p =Ria w wn ofd n pv g og us dm els uo s rd te ea F→fo ics h i nlu r
o
ef sn mcoi rn o(cid:80)r o ,e o ca l s(a t da trp rf oiu cp r gsn oyd rxn dai u n z o aec ve tn n us m nrr e m(e , sn )a inΘ icn nr gir dpo o∈p o ncs v tyl sne tvc w te ee
h
uip ia an Ru i l, , ai l,l egt pr
s
,oi wt uymevr i σfi z lto it edsr n to thvn ts e∥ ao cd
es
mi )ii ce )s ie h oa tne pco
s
oe nCv pn.o -ft,le e in ˆs fid, rn
l
eoi l ct dt p o at No oae (i oe.
y
v
tu a nr nc rt rw tg
r
rc me o Rih ne sd p e e H
a
bf F is a) ie ehp r t bt eds n mi csd ao fre - ls w,o e an s en yr os−i thrp dm bte om r oa pa cgt im
r
iF r=a eto s eq nl hsr n pclih yoe n o ,C ns oca
t
gu S ie r,fn r
ah
oM oceo l y
e
oC(x ie gΘl tt t i[ t dm fx θ et dp ly e tp ts eno i h5 o ghL o e,n t (, id rit
d
Ie t7 g e re p ci pi ϕn sP r saeg na ([ e,m lc ]e tr s )t yx 5 e i)a rps,
wi
ct tn ri i( x∥ nrcp se 9 st )i ht r tm tM ep aaz e ehh i2 2i[ e s
ig
hld ]r p eosr ft5 nye pl ld., .nr ee iu oo
h
ir n8 ze ds srr te eexl rte ] ,ha ea s t n rso ig- ie t s
s
cei wmi e-d oi nom d e ye oc L inoE ti
n
h-nr a voo
n
nm a ten sp qa it atn om ty c s
l
t,i td eao s h. te en lt e ehe id upi ( ens t en2 oagr xn efr che t mp ss , ne tru oP() e ii a a
t
,l y t so zx
e
lo reyd 3sh o= q e in a5 sgu r, a D ne rni du Dc oD ia d rn Ls s ˆp r
n
ge so el a m(cid:90) ce) d c upt i(c nfr ugv nth tos u rreta
=h
e1 s=e→to u nc r go e)n in2 n ee ol s tdl rsu mce yan nf i=dT (u tb t mo r Pm r re m( a) i i tp r ei( e ,or n σcs r rz ow
a
(cid:88)e ice tt guun g, ra =Na hr gai)t r v ,∈σ lt ii t enr tv tp ent e 1o obhi ·i Ri t) ihga pc s
,
)oi -f
-
d, p ασs ihn ∥ ac mdi r s( io o nCooe ˆ tr an lp e e H a b it dt pp ix o n( (o gen m s T ar t rt ri re gc sm ei Rth h mf) oi)o r oph c /r .F W.a) fe phr o e cs ln o ro −an , ds o im t· em ohi rr at orc y C eq nt e
i
si ti[ fo Ce
n
lc ten u d5 d ed o g ehd roe d ot7 g ee gpt a Nx e(t , l t] a t urs, t yh o rs e, wt ( ii s er sc ne s) pm n s r s:e[[ Re iir s g)5 e i ft5 np d tnn i x∥ oh Fs9 h8 z r gse ea e2 2i rs]] a e
e
pas n rl l., .g t s s
c
eeoio e ye i ho ext nnr o n
i
den petm ts, stea vh ehdp e et e pca eht dp s ter( (e i ea tl ry s z3 4 sry s dsq ia u) )ga inu si crn Lr n g toa ce .d i er ug n t Fe sh=eu n g osn edl
,
lea f i lctmo r oPr a htri r wr rz o a ac u gra r gi
l
-∈t en
l
ut e ehRi tg p p,o nihn ∥ ac gmd mo nCo ee ˆ alt sdt p eo(o ge tlt r rc m he iRh ) kh . ofe esn o− ddm ti r sr seq li ,o C n oc tu h co o we g oet l tts h o me( tpn s r s r[ ps a)5 e i tx∥
i
rs9 h
n
ee2 2is] es
-
-l, . .
x v di ee p= w enc( dox en, ny s ti l, s yz te) on. fcN y vo iebt wa yb inl py gr, et ddh iie rc eti cr n te igp or nte ,hs e wen hvt ia o lt eli uo cmn oe loe rn dv dds s ae cui eec ne pper r=swe an i ees t dnne y (c dd irmo af ,seσn u ngu n os cn ,tl i ni l et n bcs yi )t t- - be i fio oon ef tnc hlb iy v d, na vi . gr etb ic e /w W eFy h p w roi ehr nop ri ne i nmgfr ls de e gr e d a ed a Nn d rlii y lr ic t ie y nes i rt n Ri ec gfn t cego Fig t sxor i pa opntst ch en,h ry h er ee wn ai de set ne s shv h v d- eio ee pd l del ds 3 eiu i m D rz c s am soi u se in ce sl cn oog tfcd c.os rd e at (ie Frse h co sxp d os tn Te ne i, i, l=s nnh ln a dcti ogaa et hd l ) wy tr (s a ceNg sr al -s →o c,σ e l ume .egent R pnn, ei ib Fn b gm re ( mo ) a- e cs wta s e, rhg av o tσl he i yv i hri k )n. ka osi e,e egfl dntw / hW so ssl ri r iw ,on d p voeh cwg n uerei ofl lid gp oe md yot hert ri rN h sr r pa s tne ui hi re nc on reR ett [g vv- -oi 5Fo se e7s cn l ypa ]e ee,vc e na i dth ien eh e [i dd 5 iwe te ns o8v r3 ]e p [s gD 6d yi e eo 0m n nrcs ]rso t e,u p hio rssc e ao ar etc s.d tps ee eie Fi ks psn sos lia ta, yd ln ot mlce oveh es pwop r n.a el lt vl - highl nuee au gpsn nlsg camu ee rph t sis e qrz pee tal uah ainr ik atn rv os ie sli id oi iev ns ts s ne ygil 3,ool / Dy ocnw tro rees gmu cn ft f rar hrr d ipa sv do nei tre smn r ee iy iq r- - [ne 6ugd 1e e ]x ui s .nt s He i[ [nr6 o5gn0 w9a] t], e el ,s c ve h ee n rk ,iq tt huo eee ss en sh mua c en h tc he a osq dhu saa ssl thi it i ly n lo g str r[5 ufa 1 gs ] gt oe lerr
themodelleveragesanMLP(Multi-LayerPerceptron)with enhancingscenegeometryandenforcingdepthsmoothness.
dependsonbothviewingdirectionand3DcoordinTahteesN. eRF w ho er nk sfl io vw elyfo sur rn vo ev ye el dv ii new [60s ]y ,n st ehpeeoksini ts otsi enpnvehoralvpneicxs eelq,utcraoalmiitnypinuogtri/nfrgaesnltoedcrearlincgoluorsinangdtedcehnnsiiqtuyeusssiungchatsohaacschuinragte[l5y1]reoprresentempty spaces and face imagequal-
The NeRF workflow for novel view synthecw saissetiig innhgvt ocsalvΘmee, srdaer tna roy at is netd ih nr gao /su rgf ehΘnd,th eae rp is npc greon uxe si imt no gagt tei enn cge htrhnaa eit qe5 NuDs eea sRmfu Fsp un Ml cic hnt LigPao s(n sh)so afpf osahrriseneagc3h[D5F s1o ag]r mroidpro lsp in[t 6gim 1p].i oz Hia not ti ,wo an env, dea re,mts hpq elu oseya ir mne gete hr or ito dyr slsip mtih lilto ast tto irom ungse gt dlr euic e tl oos ss truis ctured grids, which significantly
c pa os it nin tsg pc ea rm pe ir xa er l,ay cs omth pro uu tig nh gt lh oe casc le cn oe lot ro ag ne dne drap tev xht noei eei sns
=
iw Ntat ysmi en R(p upg xFe sl ,r iid nn Mypi gg ,r Li zex P)ce .(tl s tsi, opo )Nc an
f
ao oor cm s trd ce
a
up eb3 rau=
l
aD cyt thi
,
en( g lg tθ
s
yr ha, idl
me
rϕo es)c
p
pra [
el
ra 6l
ip
enn 1c srg]do ee. nl spHoi teonr o
n
ei- nwa
t
ms atnc ,e
t
ped iv an
o
te
n
yv td ne hr do,e
e
sl3n t pu eeh cDs nm amoei ct smsey
p
ec ue slo pr rouemo
eu
ayns str ni ie addn
n
dt te mhig gin oro fi
un
aand cltg os tet ietio fst -yso itt ma hi lcsl iel amcye Hu gsn cm it
o
er et trha alrp u
o
qte teel ig rs u,o oligy
C
az ny Cl le see
(
-r grdet td)hp (, ue rr ree er )2e ssD tuep don lr etite ism nns te orgeam utng efcpe rt stoe.t uy mdS trhp es dae eapcsa ci ggfic arrL e mc ios da eul sal r= ,y nan, wdd
h
(cid:80)f ti ia rm cucr hpe ∈theR sidm ig∥ ce na oC rˆ g ile fio( enr crdq a) e nfur o− tai lrnl y-gC thsgp ete( per d) ix∥s. e2 2 l.
the NeRF MLP(s) for each sampling point, andvvoieelmuwmplecooyrneinnsigdsetreinn itcg yyt lo ib mysy itnp at trh ie oed nsii sczte din uthg ee tt o2hDe sti rvm uo cal tug urem ra. eyeS dprd( getec r) iin dfi=sc sia ,toyl wl+y hσ,t id ci hniimn- svpigoel ndav iese fissrcoea acnn nida ti lnet yerteidnggrwasliptfhoeerrmdsui.nlattihoen:training im2a.2g.e2, aSnudrfaRcedRenecootensstrtuhcetionfromNeuralFields
volume rendering to synthesize the 2D image.tdh See ppececonim fidcpeaunllttyal,ytioon if mov pfi eeth dwe einc reog nlod dr eirC re in(crt gi)o srn pe, esu ew dlth sin .ilgefcroolmoracca=me(rra,g,b) batchofraysforsynthesizingthetaDrgeseptiitmetahgeep.otentialofNeRFanditsderivativestocapture
the computation of the color C(r) resulting frordmaeypare( cnt a)dm=seorona+botdthinvvieowlviensgandiirnetcetgiroanlfaonrmdu3lDaticoono:rdinates. 2.2.2t2 SuWrfahcileeRNeecRoFnsatcruhciteiovnedfrosumccNeesusr,aclhtFhaieellled3nDsgegseolimkeetsrylowoftarasince-ne, these models are implicitly
C(r)= T(t)σ(r(t))c(r(t),d)dt (1)
rayr(t)=o+tdinvolvesanintegralformulation:The NeRF2w.2o.r2kflSowurfafocrenRoevceolnsvtireuwctiosynnfrtohmesiNseiunrvaolFlvieeDlsdesspiitnegt/hreepnodteernitniaglospfeNeedRsFpaenrdsiistts.dFeorlilvoadwtei-fivunepsedtmoinecatthphoteudwrsee,icgohmtsporfet-heneuralnetwork.Obtainingan
castingcam Ce (r ra D)r =ea sy ps ittt eh2tr Tho (eu t)pg σoh (tret (h nte t)i)as clc (e orn f(te N),t eo dR)g
F
de tan ne dra it te sdsa em
(r 1iv
)p al ti int vhg eeZ st t31 ohDe cang psei tov ume releytrsyurovfeayesdceinne,[6t0h]e,sseeemkotdoeelsnhaarenciemqpuliaciltiltyyorfaster
C(r)=(cid:90) t2 T(t)σ(r(t))c(r(t),d)dtp tho ein Nts eRp Fe (r 1M)p Lix Pt deh (el se, fi)c nZ3 fo eoDtm
1
drpg ienaeuo ctt hhimn eeg swt arl my eo ic pgoa hlfl itnsac go osl fpoc toer hn ineae tn, n,d et ah unde rds aee ln ensm mi et toy pwd looue yrlss ki in n.d ag gOe refi bn ti ae mt sird pnap aiii nlnn ri sgcit einh t agl 3e ny/ Dwree gni rgd ih det srsi [no 6gf 1t ]uh .se Hinn oge wutre eac vlh enn re ,itq tw hu eoe srsk es. muOc ebh tt haai osn dihn sag ss tha ini ln lg str[5 u3 g] go ler
t1
volume rendering to synthesize the 2D image. Specifically, to accurately represent empty spaces and face image qual-
the computation of the color C(r) resulting from a camera ity limitations due to structured grids, which significantly
rayr(t)=o+tdinvolvesanintegralformulation: impederenderingspeeds.4
2.2.2 SurfaceReconstructionfromNeuralFields
DespitethepotentialofNeRFanditsvariantstocapturethe
3Dgeometryofascene,thesemodelsareimplicitlydefined
in the weights of the neural network. Obtaining an explicit
representation of the scene through 3D meshes is desirable
for 3D reconstruction applications. Starting with NeRF, a
NeRF[33] GaussianSplatting[34]
basic approach to achieving coarse scene geometry is to
thresholdthedensitypredictedbytheMLP.Moreadvanced
Fig. 3: NeRF and 3DGS differ conceptually. (left) NeRF
solutionsexplorethreemainrepresentations.
queries an MLP along the ray, while (right) 3DGS blends
Occupancy.Thisrepresentationmodelsfreeversusoccu-
Gaussiansforthegivenray.
pied space by replacing alpha values α along the ray with
i
a learned discrete function o(x) 0,1 . Specifically, an
occupancyprobability
[0,1]ises∈ tim{ ated}
andsurfacesare
theopacityo i ∈[0,1],andcolorc i representedbyspherical
∈ harmonics (SH) for view-dependent appearance, where all
obtainedbyrunningthemarchingcubesalgorithm[62].
the properties are learnable and optimized through back-
SignedDistanceFunction(SDF).Analternativemethod
propagation. This allows for the compact expression of the
forscenegeometryisthesigneddistancefromanypointto
spatialinfluenceofanindividualGaussianprimitiveas:
the nearest surface, yielding negative values inside objects
andpositivevaluesoutside.NeuS[63]wasthefirsttorevisit g i(x)=e −1 2(x −µi)⊤Σ− i1(x −µi) (8)
the NeRF volumetric rendering engine, predicting the SDF
with an MLP as f(r(t)) and replacing α with ρ(t), derived Here, the spatial covariance Σ defines an ellipsoid and
fromtheSDFasfollows: it is computed as Σ = RSS ⊤R ⊤, where S R3 is the
spatialscaleandR R3 ×3 representstherotati∈ on,parame-
dΦ(f(r(t))) ∈
ρ(t)=max −dt ,0 (5) terizedbyaquaternion.Forrendering,3DGSoperatesakin
Φ(f(r(t))) to NeRF but diverges significantly in the computation of
(cid:0) (cid:1) blending coefficients. Specifically, the process involves first
withΦbeingthesigmoidfunctionand dΦ itsderivative,i.e.,
dt projecting 3D Gaussian points onto a 2D image plane, a
thelogisticdensitydistribution.
process commonly referred to as “splatting”. This is done
Truncated Signed Distance Function (TSDF). Finally,
expressing the projected 2D covariance matrix and center
predicting a truncated SDF with the MLP allows for re-
as Σ ′ = JWΣWTJT and µ ′ = JWµ, where W repre-
moving the contribution by any SDF value too far from
sents the viewing transformation, and J is the Jacobian of
individual surfaces during rendering. In [64], pixel color is
the affine approximation of the projective transformation.
obtainedasaweightedsumofcolorssampledalongtheray:
Consequently, 3DGS computes the final pixel color C by
C(r)=
N i=1w ici
(6)
b sole rn ted din bg y3 thD eiG ra du es ps ti ha :n splats that overlap at a given pixel,
N w
(cid:80) i=1 i
i 1
withw
i
defined,accordingto(cid:80)truncationdistancet r,as
C = c α
−
(1 α ) (9)
i i j
−
w =Φ
f(r(t))
Φ
f(r(t))
(7)
i (cid:88)∈N j (cid:89)=1
i t r · − t r wherethefinalopacityα i isthemultiplicationresultofthe
(cid:0) (cid:1) (cid:0) (cid:1) learnedopacityo i andtheGaussian:
2.2.3 3DGaussianSplatting(3DGS)
1
I rn adtr io ad nu cece fid eb ldy tK ece hrb nl iqe ut eal f. o[ r34 e] ffii cn ie2 n0 t23 a, n3 dD hG igS hi -s quan alie tx yp rl eic ni -t α i =o iexp (cid:18)−2(x ′ −µ ′i) ⊤Σ′i−1(x ′ −µ ′i)
(cid:19)
(10)
deringof3Dscenes.Unlikeconventionalexplicitvolumetric where x ′ and µ ′i are coordinates in the projected space.
representations, such as voxel grids, it provides a contin- Similarly,thedepthDisrenderedas:
uous and flexible representation for modeling 3D scenes
in terms of differentiable 3D Gaussian-shaped primitives. i −1
D = d α (1 α ) (11)
Theseprimitivesareusedtoparameterizetheradiancefield i i − j
and can be rendered to produce novel views. In addition, i (cid:88)∈N j (cid:89)=1
in contrast to NeRF, which relies on computationally ex- Here, d refers to the depth of the center of the i-th 3D
i
pensive volumetric ray sampling, 3DGS achieves real-time Gaussian, obtained by projecting onto the z-axis in the
rendering through a tile-based rasterizer. This conceptual cameracoordinatesystem.
difference is highlighted in Figure 3. This approach offers For optimization, instead, the process begins with pa-
improvedvisualqualityandfastertrainingwithoutrelying rameterinitializationfromSfMpointcloudsorrandomval-
on neural components, while also avoiding computation ues, followed by Stochastic Gradient Descent (SGD) using
in empty space. More specifically, starting from multi- an L1 and D-SSIM loss function against ground truth and
view images with known camera poses, 3DGS learns a set render views. Additionally, periodic adaptive densification
= g ,g ,...,g of 3D Gaussians, where N denotes handlesunder-andover-reconstructionbyadjustingpoints
1 2 N
G { }
the number of Gaussians in the scene. Each primitive g , withsignificantgradientsandremovinglow-opacitypoints,
i
with 1 < i < N, is parameterized by a full 3D covariance refiningscenerepresentationandreducingrenderingerrors.
matrix Σ
i
R3 ×3, the mean or center position µ
i
R3, Formoredetailson3DGSandrelatedworks,referto[65].
∈ ∈5
data, and reflective surfaces. It spans different scene cat-
egories, includes 88 semantic classes, and incorporates 6
scansofasinglespacecapturingdifferentfurniturearrange-
ments and temporal snapshots. Reconstruction involved a
custom-built RGB-D capture rig with synchronized IMU,
RGB, IR, and wide-angle grayscale sensors, accurately fus-
ing raw depth data through 6 degrees of freedom (DoF)
(a)ETH3D-SLAM[66] (b)ScanNet[67] poses. Although the original data was captured in the real
world, the portion of the dataset used for SLAM evalua-
tion is synthetically generated from the accurate meshes
produced during reconstruction. Consequently, synthetic
sequenceslackreal-worldcharacteristicslikespecularhigh-
lights,autoexposure,blur,andmore.
The KITTI [69]4 dataset serves as a popular bench-
mark for evaluating stereo, optical flow, visual odome-
try/SLAM algorithms, among others. Acquired from a ve-
(c)TUMRGB-D[30] (d)Replica[68]
hicle equipped with stereo cameras, Velodyne LiDAR, GPS
Fig. 4: Qualitative Comparison of Key SLAM Datasets. andinertialsensors,thedatasetcontains42,000stereopairs
RGB-D images from: (a) ETH3D-SLAM [30], (b) ScanNet and LiDAR pointclouds from 61 scenes representing au-
[67],(c)TUMRGB-D[66],and(d)Replica[68]. tonomous driving scenarios. The KITTI odometry dataset,
with22LiDARscansequences,contributestotheevaluation
2.3 Datasets ofodometrymethodsusingLiDARdata.
The Newer College [70]5 dataset comprises sensor data
Thissectionsummarizesdatasetscommonlyusedinrecent
captured during a 2.2 km walk around New College, Ox-
SLAM methodologies, covering various attributes such as
ford. It includes information from a stereoscopic-inertial
sensors, ground truth accuracy, and other key factors, in
camera, a multi-beam 3D LiDAR with inertial measure-
both indoor and outdoor environments. Figure 4 presents
ments,andatripod-mountedsurvey-gradeLiDARscanner,
qualitative examples from diverse datasets, which will be
generating a detailed 3D map with around 290 million
introducedintheremainder.
points. The dataset provides a 6 DoF ground truth pose
The TUM RGB-D [66]1 dataset comprises RGB-D se-
for each LiDAR scan, accurate to approximately 3 cm. The
quenceswithannotatedcameratrajectories,recordedusing
dataset encompasses diverse environments, including built
two platforms: handheld and robot, providing a diverse
spaces,openareas,andvegetatedzones.
range of motions. The dataset features 39 sequences, some
with loop closures. Core elements include color and depth
images from a Microsoft Kinect sensor, captured at 30 Hz 2.3.1 OtherDatasets
and 640 480 resolution. Ground-truth trajectories are de- Moreover, we draw attention to less-utilized alternative
×
rived from a motion-capture system with eight high-speed datasetsinrecentSLAMresearch.
camerasoperatingat100Hz.Theversatilityofthedatasetis The ETH3D-SLAM [30]6 dataset includes videos from
demonstrated through various trajectories in typical office
a custom camera rig, suitable for assessing visual-inertial
environmentsandanindustrialhall,encompassingdiverse
mono, stereo, and RGB-D SLAM. It features 56 training
translationalandangularvelocities.
datasets, 35 test datasets, and 5 independently captured
The ScanNet [67]2 dataset provides a collection of real- trainingsequencesusingSfMtechniquesforgroundtruth.
worldindoorRGB-Dacquisitions,featuring2.5millionim- The EuRoC MAV [135]7 dataset offers synchronized
ages from 1513 scans in 707 unique spaces. In particular, it
stereo images, IMU, and accurate ground truth for a micro
includesestimatedcalibrationparameters,cameraposes,3D
aerial vehicle. It supports visual-inertial algorithm design
surfacereconstructions,texturedmeshes,detailedsemantic
andevaluationindiverseconditions,includinganindustrial
segmentationsattheobject-level,andalignedCADmodels.
setting with millimeter-accurate ground truth and a room
Thedevelopmentprocessinvolvedthecreationofauser-
for3Denvironmentreconstruction.
friendly capture pipeline using a custom RGB-D capture The 7-scenes [136]8 dataset, created for relocalization
setup with structure sensors attached to handheld devices
performance evaluation, was recorded using a Kinect at
such as iPads. The subsequent offline processing phase re-
640 480 resolution. Ground truth poses were obtained
sultedincomprehensive3Dscenereconstructions,complete ×
through KinectFusion [2]. Sequences from different users
with available 6-DoF camera poses and semantic labels.
were divided into two sets—one for simulating keyframe
Note that camera poses in ScanNet are derived from the
harvesting and the other for error calculation. The dataset
BundleFusionsystem[52],whichmaynotbeasaccurateas
alternativessuchasTUMRGB-D.
4.https://www.cvlibs.net/datasets/kitti/
The Replica [68]3 dataset features 18 photorealistic 3D
5.https://arxiv.org/pdf/ori.ox.ac.uk/datasets/
indoor scenes with dense meshes, HDR textures, semantic newer-college-dataset
6.https://www.eth3d.net/slam overview
1.https://cvg.cit.tum.de/data/datasets/RGB-D-dataset 7.https://projects.asl.ethz.ch/datasets/doku.php?id=
2.http://www.scan-net.org/ kmavvisualinertialdatasets
3.https://github.com/facebookresearch/Replica-Dataset 8.http://research.microsoft.com/7-scenes/6
(a) (b) (c) (d) (e) (f)
Section Method Venue RGB-D RGB D Stereo IMU CE av me en rt a EnS cc oe dn ie ng ReG pre eo sm ene tt ar ty ion O Seb gj/ mS ee nm t.. Uncert. Fr Mam ode e-t lo Fr Fa rm ame- eto E Tx rate cr kn ea rl Gl Bo Abal CL loo so up re MSu ab p- s D Eny vn .. ExtraPriors Link
RGB-D(Sec.3.1)
iMAP[1] ICCV2021 ✓ MLP Density ✓ WebPage
NICE-SLAM[5] CVPR2022 ✓ Hier.Grid+MLP Occupancy ✓ Code
Vox-Fusion[71] ISMAR2022 ✓ OctreeGrid+MLP SDF ✓ Code
ESLAM[72] CVPR2023 ✓ FeaturePlanes+MLP TSDF ✓ Code
Co-SLAM[73] CVPR2023 ✓ HashGrid+MLP SDF ✓ ✓ Code
GO-SLAM[74] ICCV2023 ✓ ✓ ✓ HashGrid+MLP SDF ✓ DROID[75] ✓ ✓ Code
Sec.3.1.1 P Too Fin -St- LS ALA MM [7[ 77 ]6] I IC CC CV V2 20 02 23 3 ✓ ✓ N Heu ir ea r.l GPo rii dnt +s+ MM LPLP Occ Su Dp Fancy ✓ ✓ WC ebo Pd ae ge
ADFP[78] NeurIPS2023 ✓ Hier.Grid+MLP Occupancy ✓ Code
MLM-SLAM[79] RAL2023 ✓ MLP Occupancy ✓
Plenoxel-SLAM[80] WACV2024 ✓ Plenoxels Density ✓ Code
Structerf-SLAM[81] C.&G.2024 ✓ Hier.Grid Occupancy ✓ ORB2[82] Super-pixelSegmentation[83]
iDF-SLAM[84] Arx.09/2022 ✓ MLP TSDF ✓ URR[85]
NeuV-SLAM[86] Arx.02/2024 ✓ MultiRes.Voxels SDF ✓ Code†
GS-SLAM[12] Arx.11/2023 ✓ 3DGaussians Density ✓
Photo-SLAM[87] Arx.11/2023 ✓ ✓ ✓ 3DGaussians Density ✓ ORB3[88] ✓
Sec.3.1.2 SplaTAM[89] Arx.12/2023 ✓ 3DGaussians Density ✓ Code
GSSLAM[90] Arx.12/2023 ✓ ✓ 3DGaussians Density ✓ WebPage
Gaussian-SLAM[91] Arx.12/2023 ✓ 3DGaussians Density ✓ ✓ WebPage
MeSLAM[6] SMC2022 ✓ MLP Density ✓ C-ICP[92] ✓
CP-SLAM[93] NeurIPS2023 ✓ NeuralPoints+MLP Occupancy ✓ ✓ ✓ ✓
NISB-Map[94] RAL2023 ✓ MLP Density ✓ Any ✓
Multiple-SLAM[95] TIV2023 ✓ OctreeGrid+MLP SDF ✓ ✓
Sec.3.1.3 MIPS-Fusion[96] TOG2023 ✓ MLP TSDF ✓ ✓ ✓ ✓
NEWTON[97] Arx.03/2023 ✓ ✓ HashGrid+MLP Density ✓ ORB2[82] ✓ ✓
NGEL-SLAM[98] Arx.11/2023 ✓ OctreeGrid+MLP Occupancy ✓ ✓ ORB3[88] ✓ ✓ ✓
PLGSLAM[99] Arx.12/2023 ✓ FeaturePlanes+MLP SDF ✓ ✓
Loopy-SLAM[100] Arx.02/2024 ✓ NeuralPoints+MLP Occupancy ✓ ✓ ✓ Code
iLabel[7] RAL2023 ✓ MLP Density ✓ ✓ ✓ User WebPage
FR-Fusion[101] ICRA2023 ✓ MLP Density ✓ ✓ User&EfficientNet[102]/DINO[103] WebPage
vMap[104] CVPR2023 ✓ MLP Occupancy ✓ ✓ Code
Sec.3.1.4 NIDS-SLAM[105] Arx.05/2023 ✓ HashGrid+MLP SDF ✓ ✓ ORB3[88] ✓ ✓ Mask2Former[106]
SNI-SLAM[107] Arx.11/2023 ✓ Hier.Grid+MLP TSDF ✓ ✓ Dinov2[108]
DNSSLAM[109] Arx.11/2023 ✓ HashGrid+MLP Occupancy ✓ ✓ ✓
SGS-SLAM[110] Arx.02/2024 ✓ 3DGaussians Density ✓ ✓ ✓
DN-SLAM[13] SensorsJ.2023 ✓ HashGrid+MLP Density ✓ ORB3[88] ✓ SAM[111]
Sec.3.1.5 D Dy Dn Na -M SLo AN M[11 [92 ]] A Ar rx x. .0 09 1/ /2 20 02 23 4 ✓ ✓ ✓ ✓ ✓ HH ae sx hP Gla rn ie d+ +M ML LP P De Sn Ds Fity ✓ ✓ D OR RO BI 3D [8[7 85 ]] ✓ ✓ ✓ ✓ YOLOD ve 5ep &La Zb oV eD3 e[1 p1 t3 h] [114] Code†
NID-SLAM[115] Arx.01/2024 ✓ Hier.Grid+MLP Occupancy ✓ ✓
Sec.3.1.6 O Unp ce Ln eW -So Lrl Ad M-SL [A 17M ][116] ICC CR VV W20 22 03 23 ✓ ✓ ✓ ✓ H Hi ie er r. .G Gr ri id d+ +M ML LP P O Oc cc cu up pa an nc cy y ✓ ✓ ✓ ✓ Code
Sec.3.1.7 EN-SLAM[14] Arx.11/2023 ✓ ✓ Hier.Grid+MLP TSDF ✓ ✓
RGB(Sec.3.2)
DIM-SLAM[16] ICLR2023 ✓ Hier.Grid+MLP Density ✓ Code
Orbeez-SLAM[117] ICRA2023 ✓ HashGrid+MLP Density ✓ ORB2[82] Code
Sec.3.2.1 FMapping[118] Arx.06/2023 ✓ GridFact.+MLP Density ✓ ✓ Code†
TT-HO-SLAM[119] Arx.12/2023 ✓ Hier.Grid+MLP Density ✓
Hi-Map[8] Arx.01/2024 ✓ GridFact.+MLP SDF ✓ WebPage
iMode[120] ICRA2023 ✓ MLP Density ✓ ORB[10] Sparse-to-dense[121]
Hi-SLAM[122] RAL2023 ✓ HashGrid+MLP TSDF ✓ DROID[75] ✓ ✓ Omnidata[123]
Sec.3.2.2 NICER-SLAM[18] 3DV2024 ✓ Hier.Grid+MLP SDF ✓ GMFlow[124]&Omnidata[123] WebPage
NeRF-VO[125] Arx.12/2023 ✓ HashGrid+MLP Density ✓ ✓ DPVO[126] Omnidata[123]
MoD-SLAM[127] Arx.02/2024 ✓ ✓ HashGrid+MLP SDF ✓ DROID[75] ✓ DPT[128]&ZoeDepth[114]
Sec.3.2.3 RO-MAP[129] RAL2023 ✓ HashGrid+MLP Occupancy ✓ ✓ ORB2[82] YOLOv8 Code
Sec.3.2.4 NeRF-SLAM[130] IROS2023 ✓ HashGrid+MLP Density ✓ ✓ DROID[75] Code
LiDAR(Sec.3.3)
NeRF-LOAM[15] ICCV2023 ✓ OctreeGrid+MLP SDF ✓ Code
Sec.3.3.1 LONER[131] RAL2023 ✓ Hier.Grid+MLP Density ✓ P2P-ICP[132] Code
PIN-SLAM[133] Arx.01/2024 ✓ ✓ NeuralPoints+MLP SDF ✓ ✓ ✓ ✓ Code
Sec.3.3.2 LIV-GaussMap[134] Arx.01/2024 ✓ ✓ 3DGaussians Density ✓ Code†
TABLE 1: SLAM Systems Overview. We categorize the different methods into main RGB-D, RGB, and LiDAR-based
frameworks.Intheleftmostcolumn,weidentifysub-categoriesofmethodssharingspecificproperties,detailedinSections
3.2.1to3.3.2.Then,foreachmethod,wereport,fromthesecondleftmostcolumntothesecondrightmost,themethodname
and publication venue, followed by (a) the input modalities they can process: RGB, RGB-D, D (e.g. LiDAR, ToF, Kinect,
etc.),stereo,IMU,orevents;(b)mappingproperties:sceneencodingandgeometryrepresentationslearnedbythemodel;
(c) additional outputs learned by the method, such as object/semantic segmentation, or uncertainty modeling (Uncert.);
(d)trackingpropertiesrelatedtotheadoptionofaframe-to-frameorframe-to-modelapproach,theutilizationofexternal
trackers,GlobalBundleAdjustment(BA),orLoopClosure;(e)advanceddesignstrategies,suchasmodelingsub-mapsor
dealing with dynamic environments (Dyn. Env.); (f) the use of additional priors. Finally, we report the link to the project
pageorsourcecodeintherightmostcolumn. indicatescodenotreleasedyet.
†
presentschallengessuchasspecularities,motionblur,light- A. Mapping. Metrics assessing the quality of 3D recon-
ingconditions,flatsurfaces,andsensornoise. structionand2Ddepthestimationinclude:
The ScanNet++ [137]9 dataset comprises 460 high- Accuracy (cm) : Computes the average distance be-
• ↓
resolution3Dindoorscenereconstructions,densesemantic tweensampledpointsfromthereconstructedmeshand
annotations, DSLR images, and iPhone RGB-D sequences. thenearestground-truthpoint.
Captured with a high-end laser scanner at sub-millimeter Completion (cm) : Measures the average distance be-
• ↓
resolution, each scene includes annotations for over 1,000 tweensampledpointsfromtheground-truthmeshand
semanticclasses,addressinglabelambiguitiesandintroduc- thenearestreconstructed.
ing new benchmarks for 3D semantic scene understanding Precision (%) : Indicates the proportion of points
• ↑
andnovelviewsynthesis. within the reconstructed mesh with Accuracy under a
Additional Datasets. For an exhaustive survey of spe- distancethresholdd.
cialized SLAM-related datasets beyond those mentioned, Recall (%) : Indicates the proportion of points within
• ↑
readerscanrefertotheworkbyLiuetal.[138].Thispaper the reconstructed mesh with Completion under a dis-
providesanin-depthexplorationofawiderangeofdatasets tanced.ItisoftenreferredtoasCompletionRatio.
designedtofacilitateresearchandbenchmarking. F-Score (%) : An aggregate score defined as the har-
• ↑
monicmeanbetweenPrecisionandRecall.
2.4 EvaluationMetrics L1-Depth(cm) :Following[5],itcomputestheabsolute
• ↓
difference between depth maps obtained from ran-
TheevaluationofSLAMsystemstypicallyemploysseveral
domlysampledviewpointsfromthereconstructedand
metricsacrossdomainslike3Dreconstruction,2Ddepthes-
thecorrespondinggroundtruthmeshesrespectively.
timation,trajectoryestimation,andviewsynthesistoassess
B.Tracking.Metricsforposeestimation,crucialfortrack-
theeffectivenessofmethodsagainstgroundtruthdata.
ingperformance,primarilyinclude:
9.https://cy94.github.io/scannetpp/ Absolute Trajectory Error (ATE)(cm) : Evaluates tra-
• ↓7
Fig. 5: Overview of iMap [1], the Pioneering Approach in Neural Implicit-based SLAM. (Left) The illustration depicts
twoconcurrentprocesses:tracking,optimizingthecurrentframe’sposewithinthelockednetwork;mapping,jointlyrefining
the network and camera poses of selected keyframes. (Right) Jointly optimizing scene network parameters and camera
posesforkeyframesusingdifferentiablerenderingfunctions.Figurefrom[1].
jectory estimation accuracy by measuring the average depthinformationoftheenvironment.Thesetechniquesfall
Euclidean translation distance between corresponding into distinct categories: NeRF-style SLAM solutions (3.1.1)
poses in estimated and ground truth trajectories, often and alternatives based on the 3D Gaussian Splatting rep-
reported in terms of Root Mean Square Error (RMSE). resentation (3.1.2). Specialized solutions derived from both
As both trajectories can be specified in arbitrary coor- approachesincludesubmap-basedSLAMmethodsforlarge
dinate frames, alignment is required. Importantly, this scenes (3.1.3), frameworks that address semantics (3.1.4),
metricfocusessolelyonthetranslationcomponent. and those tailored for dynamic scenarios (3.1.5). Within
C.ViewSynthesis.Theevaluationofviewsynthesisrelies thisclassification,sometechniquesassessreliabilitythrough
mainlyonthreevisualqualityassessmentmetrics: uncertainty (3.1.6), while others explore the integration of
additionalsensorslikeevent-basedcameras(3.1.7).
Peak Signal to Noise Ratio (PSNR) :Measuresimage
• ↑
quality by evaluating the ratio between the maximum
3.1.1 NeRF-styleRGB-DSLAM
pixel value and the root mean squared error, usually
Recent advances in implicit neural representations have
expressedintermsofthelogarithmicdecibelscale.
enabledaccurateanddense3Dsurfacereconstruction.This
Structural Similarity Index Measure (SSIM [139]) :
• ↑ has led to novel SLAM systems derived from or inspired
Assessesimagequalitybyexaminingthesimilaritiesin
by NeRF, initially designed for offline use with known
luminance,contrast,andstructuralinformationamong
cameraposes.Inthissection,wedescribethesedenseneural
patchesofpixels.
VSLAMmethods,analyzetheirmainfeatures,andprovide
Learned Perceptual Image Patch Similarity (LPIPS
• aclearoverviewoftheirstrengthsandweaknesses.
[140]) : Utilizes learned convolutional features to as-
↓ iMAP [1].Thisworkmarksthefirstattempttoleverage
sessimagequalitybasedonfeaturemapmeansquared
implicit neural representations for SLAM. This ground-
erroracrosslayers.
breaking achievement not only pushes the boundaries of
SLAM but also establishes a new direction for the field.
3 SIMULTANEOUS LOCALIZATION AND MAPPING In particular, iMAP demonstrates the potential of an MLP
This section introduces latest SLAM systems that leverage to dynamically create a scene-specific implicit 3D model.
recentprogressinradiancefieldrepresentations.Organized By doing so, this work provides an alternative to con-
in a method-based taxonomy, the papers are categorized ventional techniques, offering efficient geometry represen-
by their approaches, offering readers a clear and organized tation, automatic detail control, and seamless filling-in of
presentation. The section begins with a basic classification unobserved regions. Specifically, the framework (depicted
intoRGB-D(3.1),RGB(3.2),andLiDAR(3.3)methodologies, in Figure 5) uses an MLP to map 3D coordinates to color
setting the stage for the development of specific subcat- and volume density, without addressing specularities or
egories. Each category lists officially published papers in considering viewing directions. Differentiable rendering,
conferences/journalsbypublicationdates,followedbypre- guided by camera pose and pixel coordinates, generates
printsfromarXivarrangedbytheirinitialpreprintdates. depth and color images through network queries. Joint
For a comprehensive understanding, Table 1 offers a optimizationofphotometricandgeometriclossesforafixed
detailed overview of the surveyed methods. This table set of keyframes refines network parameters and camera
provides an in-depth summary, highlighting key features poses. A parallel process ensures close-to-frame-rate cam-
of each method, and includes references to project pages era tracking, with dynamic keyframe selection based on
or source code whenever available. For further details or informationgain.Activesamplingoptimizesasparsesetof
methodspecifics,pleaserefertotheoriginalpapers. pixels,guidedbylossstatistics.Integratingkeyframe-based
structure,multi-processingcomputation,anddynamicpixel
samplingenablesreal-timetrackingandglobalmapupdat-
3.1 RGB-DSLAMApproaches
ing. However, due to the limited capacity of the model, it
Here we focus on dense SLAM techniques using RGB- leads to less detailed reconstruction and faces challenges
D cameras that capture both color images and per-pixel withcatastrophicforgettinginlargerenvironments.8
NICE-SLAM [5]. In contrast to iMAP’s use of a single ray sampling, potentially leading to oversmoothing issues.
MLP as the scene representation, NICE-SLAM adopts a In contrast, methods such as NICE-SLAM use parametric
hierarchical strategy that integrates multi-level local data. embeddings through feature grids, which partially avoid
This approach effectively addresses issues like excessively oversmoothing but cannot fully address hole-filling. Co-
smoothed reconstructions and scalability limitations in SLAM fills the gap by combining the smoothness of coor-
larger scenes. By optimizing a hierarchical representation dinate encodings (using one-blob encoding [141]) with the
using pre-trained geometric priors, NICE-SLAM achieves fastconvergenceandlocaldetailadvantagesofsparsepara-
high-quality scene reconstruction. This procedure involves metricencodings(usingahashgrid[53]).Consequently,this
representinggeometrybyencodingitintothreevoxelgrids results in more robust camera tracking, high-fidelity maps,
ofvaryingresolutions,eachassociatedwithitscorrespond- and improved hole-filling. In addition, unlike previous
ingpre-trainedMLPdecoder—coarse,mid,andfinelevels. neural SLAM systems, Co-SLAM performs global bundle
Moreover, a dedicated feature grid and decoder are uti- adjustment (BA) by sampling few rays from all previous
lized for capturing scene appearance. Notably, compared keyframes(around5%ofpixelsforeachkeyframe).
to iMAP, it updates only the visible grid features at each GO-SLAM [74]. The absence of global optimization
step. This strategy significantly enhances the precision and techniques like LC and BA in previous works leads to
efficiency of optimization processes, overcoming the con- trackingerrorsovertime.Toaddresstheselimitations,GO-
straints of iMAP’s global updates and addressing its issues SLAMisdesignedforreal-timeglobaloptimizationofcam-
with catastrophic forgetting. However, the predictive capa- era poses and 3D reconstructions. At the core is a robust
bilities are limited to the scale of the coarse representation, poseestimationmodule,integratingefficientLCandonline
anditdoesnotincorporateloopclosures. fullBAthatutilizesthefullhistoryofinputframestoensure
Vox-Fusion [71]. This work combines traditional volu- accuratetrajectoryestimationandtomaintainacoherent3D
metricfusionmethodswithneuralimplicitrepresentations. map representation. Specifically, the architecture operates
Specifically, it leverages a voxel-based neural implicit sur- throughthreeparallelthreads:front-endtracking,responsible
facerepresentationtoencodeandoptimizethescenewithin for iterative pose and depth updates along with efficient
eachvoxel.WhilesharingsimilaritieswithNICE-SLAM[5], loop closing; back-end tracking, focused on generating glob-
its distinctiveness lies in its adoption of an octree-based ally consistent pose and depth predictions via full BA;
structure to enable a dynamic voxel allocation strategy. and instant mapping, which updates the 3D reconstruction
This dynamic approach empowers real-time tracking and basedonthelatestavailableposesanddepths.GO-SLAM’s
mapping for diverse scenes, eliminating the need for a instant mapping draws inspiration from the Instant-NGP
pre-allocated hierarchical voxel grid, as in NICE-SLAM. framework [53]. It employs a rendering strategy that maps
Moreover, a fundamental feature of Vox-Fusion involves 3D points to multi-resolution hash encodings and predicts
modelingthelocalscenegeometrywithinindividualvoxels both SDF and color using shallow networks. Notably, GO-
using a continuous SDF. This SDF is encoded via a neural SLAMsupportsmonocular,stereo,andRGB-Dcameras.
implicitdecoderalongwithsharedfeatureembeddings.The Point-SLAM [76]. Unlike grid-based or network-based
useofsharedembeddingvectorsenablesamorelightweight methods, Point-SLAM introduces a dynamic neural point
decoder, benefiting from its capacity to encapsulate local cloudrepresentation,adjustingpointdensitybasedoninput
geometry and appearance knowledge. Furthermore, Vox- datainformation,ensuringmorepointsinareaswithhigher
Fusion introduces an efficient keyframe selection strategy detail and fewer points in less informative regions. The
tailored for sparse voxels, further enhancing its capability methodusestheperpixelinputimagegradientmagnitude
forefficientmapmanagement. todeterminethedensityofthepoints.Depthandcolorim-
ESLAM [72].ThecoreofESLAMisitsimplementation agesarerenderedviavolumerendering,witheachpixelray
of multi-scale axis-aligned feature planes, diverging from extracting geometric and color features from point groups.
traditional voxel grids. This approach optimizes memory The features extracted are further processed by specialized
usage through quadratic scaling, in contrast to the cubic decoders,asin[5],tocomputeoccupancyandcolorvalues
growth exhibited by voxel-based models. These tri-plane and optimized via gradient descent using an RGB-D re-
architectures store and optimize features on perpendicular rendering loss. The mapping process runs alternatively to
axes, enhancing reconstruction quality and addressing the the tracking process to update the scene and to estimate
forgettingproblembymanaginggeometryandappearance the location of the camera. A key feature is that its neural
changes separately. The method employs three coarse and point cloud representation expands incrementally during
three fine feature planes for both geometry and scene ap- exploration, stabilizing as all relevant regions are incorpo-
pearance.Furthermore,ESLAMadoptsTSDFasthegeomet- rated. Unlike voxel-based methods, this strategy optimizes
ric representation. This improves convergence speed and memoryusagebyonlyaddingpointsinaregionaroundthe
enhances reconstruction quality compared to conventional surface, removing the need to model free space. Moreover,
rendering-basedmethodslikevolumedensityiniMAPand duetothedynamicresolutionofthepointcloud,areaswith
occupancy in NICE-SLAM. The combination of multi-scale fewdetailsarecompressed,furthersavingmemory.
feature planes and TSDF representation improves recon- ToF-SLAM [77]. This work presents the first SLAM
structionandlocalizationwhileprocessingframesuptoten system that leverages both a monocular camera and a
timesfasterthaniMAPandNICE-SLAM. lightweightToFsensor,whichislimitedtoprovidingcoarse
Co-SLAM [73]. Systems such as iMAP use coordi- measurements in the form of low-resolution depth dis-
nate networks for real-time SLAM. However, to ensure tributions. To achieve this, a multi-modal feature grid is
interactive operation, they adopt strategies like sparsified introduced, offering the ability to perform both zone-level9
renderingtailoredforToFsensorsandpixel-levelrendering MLP takes as input view directions to enhance per-frame
optimized for other high-resolution signals (e.g. RGB). The rendering outputs. Keyframe updates involve pose opti-
system optimizes camera poses and scene geometry by mization and MLP weight tuning, preventing catastrophic
comparingtheserenderedsignalstotherawsensorinputs. forgetting through replay-based keyframe buffering. The
Additionally, a predicted depth is employed for intermedi- selection of keyframes is strategically based on covisibility
atesupervision,enhancingposetrackingandreconstruction scores, enhancing map optimization robustness. The front-
accuracy. The authors also develop a coarse-to-fine opti- end features an unsupervised R&R (URR) model [85] for
mizationstrategytoefficientlylearntheimplicitrepresenta- camera tracking, utilizing deep features and point cloud
tion. Furthermore, temporal information is incorporated to registration. iDF-SLAM employs runtime fine-tuning and
handlenoisyToFsensorsignals,enhancingsystemaccuracy. point cloud registration for improved tracking accuracy,
ADFP [78]. This work incorporates an attentive depth withthetracker’sfeatureextractorpre-trainedonScanNet.
fusion prior derived from TSDF formed by fusing multiple NeuV-SLAM [86]. This framework exploits a voxel-like
depthimages.Thisallowsneuralnetworkstodirectlyutilize representation to encode the scene geometry. Specifically,
learned geometry and TSDFs during volume rendering, NeuV-SLAM handles multi-resolution voxels using a hash
overcoming issues such as incomplete depth at holes and table,allowingforincrementalexpansioninnewlyexplored
unawareness of occluded structures in the reconstruction areas,namedhashMV.Thisiscoupledwithanovelimplicit
process. Through a process involving ray tracing, feature scene representation, VDF, that combines the implementa-
interpolation,occupancypredictionpriors,andanattention tion of neural SDF voxels with the SDF activation strategy
mechanismtobalancethecontributionsoflearnedgeometry by directly optimizing color features and SDF values that
and the depth fusion prior, the methodology significantly areanchoredwithinthevoxels.
enhancesaccuracyin3Dreconstruction.
MLM-SLAM [79]. This work introduces a multi-MLP 3.1.2 3DGS-styleRGB-DSLAM
hierarchical scene representation that utilizes different lev- Here, we present an overview of pioneering frameworks
els of decoders to extract detailed features, enhancing the that use explicit volumetric representations based on 3D
reconstruction process without sacrificing scalability. The Gaussian Splatting for the development of SLAM solu-
systememploysneuralimplicitrepresentations,optimizing tions. These approaches typically exploit the advantages
depth and color estimation through geometric and photo- of 3DGS, such as faster and more photorealistic rendering
metric losses without fixed pre-trained decoders, ensuring comparedtootherexistingscenerepresentations.Theyalso
better generalization across various scenes. Additionally, offertheflexibilitytoincreasemapcapacitybyaddingmore
it implements a refined tracking strategy and keyframe Gaussianprimitives,completeutilizationofper-pixeldense
selection approach, enhancing system reliability, especially photometric losses, and direct parameter gradient flow to
inchallengingdynamicenvironments. facilitatefastoptimization.Todate,the3DGSrepresentation
Plenoxel-SLAM [80]. This work builds upon the has primarily been employed in offline systems dedicated
Plenoxel radiance field model [61], devoid of neural net- to novel view synthesis from known camera poses. In the
works. The paper describes a novel approach: the use of a following section, we introduce seminal SLAM method-
voxel grid representation and trilinear interpolation within ologies that enable the simultaneous optimization of scene
the Plenoxel framework for efficient dense mapping and geometryalongwithcameraposes.
tracking. The key highlight lies in the analytical derivation GS-SLAM [12]. GS-SLAM introduces a paradigm shift
of equations essential for both offline RGB-D mapping and by leveraging 3D Gaussians as the representation coupled
online camera pose optimization. Despite the novel ap- withsplattingrenderingtechniques.Incontrasttomethods
proach outlined by Plenoxel-SLAM, it is worth mentioning relying on neural implicit representations, GS-SLAM pro-
thatnoexplicit3Dmeshiscurrentlyreconstructedfromthe vides a substantial acceleration in map optimization and
learnedrepresentation. re-rendering, by employing a novel approach utilizing 3D
Structerf-SLAM [81].Thismethodologyusestwo-layer Gaussians together with opacity and spherical harmonics
feature grids and pre-trained decoders to decode inter- to encapsulate both scene geometry and appearance, as
polated features into RGB and depth values. During the depicted in Figure 6. A key innovation lies in its adaptive
trackingphase,theuseofthree-dimensionalplanarfeatures, expansion strategy, dynamically managing the addition or
based on the Manhattan assumption, improves stability removalof3DGaussianstoefficientlyreconstructobserved
and rapid data association, overcoming the limitations of scene geometry while enhancing mapping precision. Addi-
insufficient texture. Camera pose optimization involves the tionally,GS-SLAMintroducesarobustcoarse-to-finecamera
application of photometric, geometric, and planar feature tracking technique refining camera pose estimation itera-
matching loss terms. In the mapping stage, a planar con- tively through image refinement and reliable 3D Gaussian
sistency constraint ensures that the depth predicted by the selection.ThisisfollowedbyBA,aimingtooptimizecamera
dual-layer neural radiance field aligns with a plane, result- posesandthe3DGaussianscenerepresentationsimultane-
inginsmoothermapreconstruction. ously.Despiteitsstrengths,GS-SLAMfaceslimitationscon-
iDF-SLAM [84]. This work integrates a feature-based cerning its dependence on high-quality depth information
neural tracker at the front-end for robust camera tracking. andsubstantialmemoryusageinlargescalescenes.
The back-end, instead, includes a neural implicit mapper Photo-SLAM[87].Thisworkintegratesexplicitgeomet-
using a single MLP as the map representation and is re- ricfeaturesandimplicittexturerepresentationswithinahy-
sponsible for estimating TSDF values. Notably, in addition per primitives map. This methodology combines ORB fea-
to the 3D position and unlike previous approaches, the tures[142],rotation,scaling,density,andsphericalharmonic10
Fig.6:OverviewofGS-SLAM[12].Thisframeworkleveragesthe3DGaussianscenerepresentationandrenderedRGB-D
imagesforinversecameratracking.ThroughanovelGaussianexpansionstrategy,GS-SLAMachievesreal-timetracking,
mapping,andrenderingonGPUs,enhancingscenereconstructioncapabilities.Figurefrom[12].
coefficientstooptimizecameraposesandmappingaccuracy tion proposed to ensure geometric consistency. For map-
whileminimizingaphotometricloss.Notably,Photo-SLAM ping and keyframing, GSSLAM integrates techniques for
employs a multi-threaded architecture encompassing mod- efficient online optimization and keyframe management,
ulesforlocalization,mapping,photorealisticrendering,and which involves selecting and maintaining a small window
loop closure. This design facilitates efficient factor graph of keyframes based on inter-frame covisibility. Addition-
solving, sparse 3D point generation, and progressive op- ally, resource allocation and pruning methods are used
timization of hyper primitives. A key feature lies in the to eliminate unstable Gaussians and avoid artifacts in the
utilizationof3DGS[34]forimagerenderingfromthehyper model. A visualization showing the qualitative aspects of
primitivesmap.Byleveragingadvancedtechniques,includ- 3DGaussianprimitivesisprovidedinFigure7.
ing geometry-based densification and Gaussian-Pyramid- Gaussian-SLAM [91]. This framework employs a
based learning, the framework achieves high-quality ren- pipeline involving map construction and optimization, cre-
dering,increasedmappingaccuracy,andreal-timeoperabil- ating separate sub-maps represented by separate 3D Gaus-
ity. The paper evaluates Photo-SLAM with diverse camera sian point clouds to prevent catastrophic forgetting and
setups,includingstereo,monocular,andRGB-D. to maintain computational efficiency. This process includes
SplaTAM [89]. This method represents the scene as a seedingnewGaussiansprogressivelyborrowingideasfrom
collectionofsimplified3DGaussians,enablinghigh-quality Point-SLAM [76] to optimize the active sub-map with-
color and depth image rendering. The SLAM pipeline en- out distorting depth sensor-derived geometry. Optimiza-
compasses several key steps: Camera Tracking: Minimizing tion ensures accurate rendering of depth and color for all
re-rendering errors for precise camera pose estimation, fo- keyframes within the sub-map, employing losses for color
cusing on visible silhouette pixels and optimizing within and depth. As the tracking module, the authors initially
well-structured map regions. Gaussian Densification: Adds opted for a frame to model loss against the 3D Gaussian
newGaussiansbasedontherenderedsilhouetteanddepth map.However,limitationsintheextrapolationcapabilityof
information,enhancingthescenerepresentationonlywhere the3DGaussiansgiverisetoartifactsintherenderedcolor
needed for accuracy. Map Update: Refines the Gaussian pa- anddepthframesleadingtoworsetrackingresults.Instead
rameters across frames, minimizing RGB and depth errors theauthorsopttouseDROID-SLAM[28]asthestandalone
while optimizing over influential frames to update the ge- tracker.Theauthors,however,showthepotentialofaframe
ometry of the scene. By adopting this approach, SplaTAM to model based tracker by defining an oracle experiment
fundamentally redefines dense SLAM practices, offering which shows superior accuracy to DROID-SLAM, high-
advancements in rendering efficiency, optimization speed, lighting that tracking issues are rooted in the extrapolation
and spatial mapping capabilities. While the study shows abilityofthe3DGaussiansplats.
remarkable progress, it also acknowledges limitations such
assensitivitytomotionbluranddepthnoise. 3.1.3 Submaps-basedSLAM
GSSLAM [90].This system employs 3D Gaussian Splat- Inthiscategory,wefocusonmethodsthataddressthechal-
ting as its only representation for online 3D reconstruction lengesofcatastrophicforgettingandtheapplicabilityissues
using a single moving RGB or RGB-D camera. The frame- in large environments faced by the previously discussed
work includes several key components, such as tracking denseradiancefield-inspiredSLAMsystems.
and camera pose optimization, Gaussian shape verification MeSLAM [6].MeSLAMintroducesanovelSLAMalgo-
andregularization,mappingandkeyframing,andresource rithm for large-scale environment mapping with minimal
allocation and pruning. The tracking phase adopts a direct memory footprint. This is achieved by combining a neural
optimization scheme against the 3D Gaussians, providing implicit map representation with a novel network distri-
fast and robust tracking capability with a broad basin of bution strategy. Specifically, by using distributed MLP net-
convergence for the camera pose estimation. Meanwhile, works, a global mapping module facilitates the segmenta-
geometricverificationandregularizationtechniquesarein- tionoftheenvironmentintodistinctregionsandcoordinates
troduced to handle ambiguities in incremental 3D dense thestitchingoftheseregionsduringthereconstructionpro-
reconstruction, with a novel Gaussian shape regulariza- cess.Thisallowsforthecreationofacomprehensiveglobal11
laborative implicit SLAM framework to tackle catastrophic
forgetting. By employing multiple SLAM agents to process
scenesinblocks,itminimizestrajectoryandmappingerrors.
The system empowers agents in the frontend to operate
independently,whilealsofacilitatingthesharingandfusion
of map information in the backend server. Specifically, the
architecture enables complex scene reconstruction through
collaborative pose estimation and map fusion processes.
The pose estimation process efficiently determines relative
poses between agents using a two-stage approach: match-
ing keyframes through a NetVLAD-based [143] global de-
scriptor extraction model and fine-tuning inter-agent poses
through an implicit relocalization process. Conversely, the
map fusion stage integrates local maps using a floating-
point sparse voxel octree for precise alignment. Moreover,
Fig. 7: 3D Gaussian Visualization. (Left) Rasterized Gaus- formoreaccurateandefficientmapfusion,themethodad-
sians,(Right)Gaussiansshadedtohighlighttheunderlying dressesoverlappingregionsbyremovingredundantvoxels
geometry.Imagesadaptedfrom[90]. basedonobservationconfidenceandareconstructionloss.
MIPS-Fusion [96]. This work introduces a divide-and-
conquer mapping scheme for online dense RGB-D recon-
map that captures the entire environment or specific parts
struction, using a grid-free, purely neural approach with
thereof. Additionally, a key aspect relies in its integration
incremental allocation and on-the-fly learning of multiple
ofexternalodometry[92]withneuralfield-basedoptimiza-
neuralsubmaps,asdepictedinFigure8.Italsoincorporates
tion. This combination enhances the system’s ability to ro-
efficient on-the-fly learning through local bundle adjust-
bustlytrackposesinregionswheremapsintersect,leading
ment, distributed refinement with back-end optimization,
to improved accuracy and stability. The joint optimization
andglobaloptimizationthroughloopclosure.Moreover,the
mechanismoptimizesbothneuralnetworkparametersand
methodologyincludesahybridtrackingscheme,combining
posessimultaneously,refiningthemaprepresentationwhile
gradient-based and randomized optimizations via particle
ensuringpreciselocalization.
filtering to ensure robust performance, particularly under
CP-SLAM [93]. This work stands as a collaborative fastcameramotions. Keyfeaturesincludeadepth-to-TSDF
neural implicit SLAM approach, characterized by a unified loss for efficient fitness evaluation, a lightweight network
frameworkencompassingfront-endandback-endmodules. for classification-based TSDF prediction, and support for
At its core, it leverages a neural point-based 3D scene parallel submap fine-tuning. Notably, MIPS-Fusion detects
representation associated with keyframes. This allows for loop closures via covisibility thresholds, which does not
seamless adjustments during pose optimization and en- allowforthecorrectionoflargedrifts.
hances collaborative mapping capabilities. Leveraging a NEWTON [97]. Most of the neural SLAM systems use
unique learning strategy, CP-SLAM employs a distributed- a world-centric map representation with a single neural
to-centralizedapproachtoensureconsistencyandcoopera- field model. However, this approach faces challenges in
tion among multiple agents. The method’s front-end mod- capturing dynamic and real-time scenes, as it relies on
ulesuseneuralpointcloudsanddifferentiablevolumeren- accurate and fixed prior scene information. This can be
deringtoachieveefficientodometry,mapping,andtracking. particularlyproblematicinextensivemappingscenarios.In
Additionally,CP-SLAMimplementsloopdetectionandsub- response, NEWTON introduces a view-centric neural field-
map alignment techniques to mitigate pose drift. The ap- based mapping method designed to overcome these lim-
proachconcludeswithglobaloptimizationtechniquessuch itations. Unlike existing methods, NEWTON dynamically
asposegraphoptimizationandmaprefinement. constructsmultipleneuralfieldmodels,eachrepresentedas
NISB-Map [94]. NISB-Map uses multiple small MLP amulti-resolutionfeaturegrid[53]inasphericalcoordinate
networks,followingthedesignofiMAP[1],torepresentthe system,basedonreal-timeobservationsandallowingcam-
large-scale environment in compact spatial blocks. Along- eraposeupdatesthroughloopclosuresandsceneboundary
side sparse ray sampling with depth priors, this enables adjustments.Thisisfacilitatedbythecoordinationwiththe
scalable indoor mapping with low memory usage. Sparse cameratrackingcomponentofORB-SLAM2[82].
raysampling,however,canresultinvaryingdensitylevels NGEL-SLAM [98]. Utilizing two modules, namely
amongadjacentspatialblocks,leadingtoinconsistenciesin the tracking and mapping modules, this system integrates
density.Toremedythis,adistillationprocedureforoverlap- the robust tracking capabilities of ORB-SLAM3 [88] with
pingNeuralImplicitSpatialBlock(NISBs)isimplemented, the scene representation provided by multiple implicit
effectively minimizing density variations and ensuring ge- neural maps. Operating through three concurrent pro-
ometric consistency. In this process, knowledge from the cesses—tracking, dynamic local mapping, and loop clos-
last trained NISB serves as the teacher and is distilled only ing—thesystemensuresglobalconsistencyandlowlatency.
within overlapping regions with the current NISB. This Thetrackingmodule,basedonORB-SLAM3,estimatesreal-
ensurescontinuitywhilereducingcomputationandtraining time camera poses and identifies keyframes, which are
timecomparedtotraininganextraglobalNISB. then processed in the dynamic local mapping phase for
Multiple-SLAM[95].Thispaperintroducesanovelcol- local BA and efficient scene representation training. Loop12
3.1.4 SemanticRGB-DSLAM
Operating as SLAM systems, these methodologies inher-
ently include mapping and tracking processes while also
incorporating semantic information to enhance the under-
standing of the environment. Tailored for tasks such as ob-
jectrecognitionorsemanticsegmentation,theseframeworks
provide a holistic approach to scene analysis - identifying
andclassifyingobjectsand/orefficientlycategorizingimage
regionsintospecificsemanticclasses(e.g.tables,chairs,etc.).
iLabel [7]. This framework is a novel system for inter-
activelyunderstandingandsegmenting3Dscenes.Itusesa
neural field representation to map 3D coordinates to color,
volumetric density, and semantic values. Specifically, the
core of this work is established on the basis of iMAP [1].
User interaction within the framework, instead, involves
providing annotations through user-clicks on the scene.
The system then employs these annotations to optimize
its predictions of semantic labels, which essentially assigns
Fig. 8: Submaps Visualization. Neural submaps, allocated meaningfullabelstodifferentpartsofthescene.Theframe-
incrementally along the scanning trajectory, encode precise work supports two modes of interaction: a manual mode,
scene geometry and colors in their dedicated local coordi- where users provide semantic labels through clicks, and a
nateframes.Figurefrom[96]. hands-free mode, where the system automatically proposes
informative positions for labeling based on semantic un-
certainty, thereby reducing user effort. Moreover, iLabel is
capable of achieving efficient interactive labeling without
closing optimizes poses using global BA, and the system’s
relyingonpre-existingtrainingdata.
utilization of multiple local maps minimizes re-training
FR-Fusion [101]. This method seamlessly integrates a
time, ensuring a quick response to significant changes in
neural feature fusion system into the iMAP [1] frame-
trackingposes.Thesystemfurtherincorporatesuncertainty-
work.Byincorporatinga2Dimagefeatureextractor(either
based image rendering for optimal sub-map selection, and
EfficientNet [102] or DINO-based [103]) and augmenting
its scene representation is based on a sparse octree-based
iMAP with a latent volumetric rendering technique, the
gridwithimplicitneuralmaps,achievingmemoryefficiency
systemefficientlyfuseshigh-dimensionalfeaturemapswith
andaccuraterepresentationoftheenvironment.
low computational and memory requirements. Prioritiz-
PLGSLAM [99]. The progressive scene representation
ing a “feature-realistic” scene representation over “photo-
methodproposedinthisworkdividestheentiresceneinto
realistic” models, the scene network operates incremen-
multiplelocalscenerepresentations,allowingforscalability
tally and enables dynamic open-set semantic segmentation
to larger indoor scenes and improving robustness. As the
through sparse user interaction. The system’s effectiveness
local scene representation, the system utilizes axis-aligned
isdemonstratedonseveraltasks,includingobjectgrouping,
triplanesforhigh-frequencyfeaturesandanMLPforglobal
object part category specialization, and unreconstructed re-
low-frequencyfeatures.Thisallowsforaccurateandsmooth
gion exploration, demonstrating its potential for practical
surface reconstruction. Additionally, it reduces memory
applications. This approach is particularly promising in
growth from cubic to square with respect to the scene
complex and unconventional domains where pre-trained
size, enhancing scene representation efficiency. Moreover,
semanticsegmentationnetworkscurrentlyhavelimitations.
thesystemintegratestraditionalSLAMwithanend-to-end
vMap [104]. This framework introduces a novel ap-
pose estimation network, introducing a local-to-global BA
proach to object-level dense SLAM. In scenarios where 3D
algorithmtomitigatecumulativeerrorsinlarge-scaleindoor
priorsarenotavailable,vMAPefficientlyconstructsacom-
scenes. Efficiently managing keyframe databases during
prehensive scene model by representing each object with
operationenablesseamlessBAacrossallpastobservations.
a dedicated MLP. This strategy facilitates the creation of
Loopy-SLAM [100]. This system leverages neural point watertight and complete object models, evenwhen dealing
cloudsintheformofsubmapsforlocalmappingandtrack- with partially observed or occluded objects in real-time
ing. The method employs frame-to-model tracking with a RGB-D input streams. The methodology revolves around
data-driven point-based submap generation approach, dy- using object masks for segmentation, efficient object asso-
namically growing submaps based on camera motion dur- ciation, and tracking via the off-the-shelf ORB-SLAM3 [88]
ingsceneexploration.Globalplacerecognitiontriggersloop algorithm.Throughaseamlessintegrationofthesefeatures,
closures online, enabling robust pose graph optimization the system models the 3D scene in a flexible and efficient
for global alignment of submaps and trajectory. The point- manner.Thisnotonlyensuresprecisereconstructionsofob-
based representation facilitates efficient map corrections jects,butalsosupportstheprocessofre-composingscenes,
withoutstoringtheentirehistoryofinputframes,compared independentlytrackingdiverseobjects,andcontinuallyup-
to previous methods such as [74]. The system addresses datingobjectsofinterestinrealtime.
challenges of error accumulation in camera tracking and NIDS-SLAM [105]. This approach enhances scene per-
avoidsvisibleseamsinoverlappingregions. ception by dynamically learning 3D geometry and seman-13
superior texture capture and finer geometric details in re-
constructions, effectively addressing the over smoothed re-
constructionproblemsseeninothermethods.Furthermore,
DNS SLAM introduces a novel real-time tracking strategy
using a lightweight coarse scene representation, trained
throughself-supervision.Thisoptimizationenhancestrack-
ing efficiency while utilizing the multi-class representation
aspseudoground-truth.
SGS-SLAM[110].SGS-SLAMemploysthefirstsemantic
dense SLAM system using 3D Gaussians. To achieve this,
the framework uses multi-channel optimization during the
mappingprocesstointegratevisual,geometric,andseman-
ticconstraintsallatonce.Accordingly,semanticinformation
is embedded in the 3D Gaussians in the form of addi-
(a)Room1 (b)Office1
tional color channels that are optimized over 2D semantic
Fig.9:Semantic Visualization.3Dsemanticmesh(bottom) segmentation maps corresponding to the color keyframes.
anditsdecompositionwithRGBcolors(top)fortwoscenes During tracking, the camera pose for a new keyframe is
fromtheReplica[68]dataset.Imagesfrom[109]. estimated from the previous one by assuming a constant
velocity camera motion model, and iteratively refined by
minimizingthelossofcolor,depth,andsemanticsbetween
tic segmentation online. The algorithm combines classical therenderedviewandtheground-truthimage.
trackingandloopclosuremechanismsbasedonORB-SLAM
3 [88] with neural fields-based mapping, leveraging the 3.1.5 SLAMinDynamicEnvironments
strengthsofexistingmethodologies.Thecoreofthemethod MostoftheSLAMmethodsreviewedsofararebasedonthe
lies in its mapping strategy. A mapping network, built fundamental assumptionof a static environmentcharacter-
upontheInstant-NGP[53]backbonewithadaptationsfrom ized by rigid, non-moving objects. While these techniques
Neus[63],learnstheSDFoftheenvironment.Thenetwork perform promisingly in static scenes, their performance in
optimizes its structure through a combination of photo- dynamic environments faces significant challenges, limit-
metric, geometric, and semantic losses on selected pixels, ing their applicability in real-world scenarios. Therefore, in
withdynamickeyframeselectionoptimizingcomputational this section, we provide an overview of methods that are
efficiency. A notable contribution is the novel approach to specifically designed to address the challenges of accurate
dense 3D semantic segmentation using 2D semantic color mappingandlocalizationestimationindynamicsettings.
maps from keyframes, as outlined in [144]. The mapping DN-SLAM [13]. This work integrates various compo-
networkseamlesslyintegratesthisstrategy,enablingprecise nents to address challenges in accurate location estimation
labelinginscenarioswithinconsistentkeyframesemantics. and map consistency in dynamic environments. Leverag-
SNI-SLAM [107]. SNI-SLAM employs a neural im- ing ORB features for object tracking and employing se-
plicit representation and hierarchical semantic encoding mantic segmentation, optical flow, and the Segment Any-
for multi-level scene comprehension. Key features include thing Model (SAM) [111], DN-SLAM effectively identifies
cross-attention mechanisms for collaborative integration of and segregates dynamic objects within the scene while
appearance, geometry, and semantic features. Hierarchical preserving static regions, enhancing SLAM performance.
semantic mapping, an integral component, is adopted as Specifically, the methodology involves utilizing semantic
a coarse-to-fine optimization scheme, enabling detailed re- segmentationforobjectidentification,refiningdynamicob-
construction while conserving memory resources. A novel ject segmentation via SAM, extracting static features, and
decoder design ensures unidirectional interaction between employingNeRFfordensemapgeneration.
appearance, geometry, and semantic features, preventing DynaMoN [112].ThisframeworkbuildsuponDROID-
mutual interference and yielding improved results. Ad- SLAM[28],enhancingitwithmotionandsemanticsegmen-
ditionally, the method incorporates various loss functions tation. The methodology integrates these elements into a
(semantic, feature, color, depth) as guidance for the scene denseBAprocess,utilizingmotionandsegmentationmasks
representationandnetworkoptimization. to weight the optimization process and ignore potentially
DNS SLAM [109]. This work leverages the potential of dynamicpixels.Semanticsegmentation,facilitatedbyapre-
2D semantic priors, enabling stable camera tracking while trained DeepLabV3 [113] network, aids in refining masks
concurrently training class-wise scene representations. The for known object classes and incorporates motion-based
integration of semantic information with multi-view ge- filtering for handling unknown dynamic elements. The
ometry results in a comprehensive and semantically de- studyalsointroducesa4DscenerepresentationusingNeRF,
composed geometric representation, crucial for improving employing a combination of implicit and explicit represen-
accuracy and detailing in scene reconstructions. This al- tations for effective 3D reconstruction [145]. Optimization
lows for reconstructing a semantically annotated 3D mesh, of NeRF involves mean squared error and Total Variation
as shown in Figure 9. Additionally, by employing image- (TV)lossforregularization,enablingthegenerationofnovel
basedfeatureextractionandimposingmulti-viewgeometry viewsindynamicscenes.
constraints,thisframeworkensuresenhancedcolor,density, DDN-SLAM [9]. This framework identifies key chal-
and semantic class information. This approach leads to lenges within dynamic environments, such as dynamic ob-14
jects, low-texture areas, and significant changes in lighting
andviewpoints.Toaddresstheseproblems,theframework
comprises semantic perception, sparse flow constraints, a
background filling strategy, multi-resolution hash encod-
ing, and tracking. In semantic systems, it detects the static
and dynamic feature points using conditional probability
fields. Meanwhile, the constraints are created for potential
dynamic points and keyframes to improve the tracking
performance.Toalleviatetheproblemsbroughtbydynamic
objects in the scene, the framework adopts a skip voxel
strategy based on selectively updating voxels to perform
pixel control on selected keyframes and specific dynamic
pixels. Furthermore, it employs semantic mask joint multi- (a)RGB (b)EventInput (c)GroundTruth
resolutionhashencodingtoeliminatedynamicinterferences
Fig. 10: Overview of the DEV-Indoors Dataset [14]. (a)
and decrease the ghosting artifacts. When achieving the
RGBimagesdepictingnormal,motionblur,anddarkscenes
complete bundle adjustment and loop closure detection,
with corresponding (b) event streams and (c) ground truth
DDN-SLAM[115]filtersoutthefeaturepointsvalidatedby
meshes.Imagesfrom[14].
semanticandflowthreadstogainrobustperformance.
NID-SLAM [115]. This method addresses the dynamic
environment by employing a specialized dynamic process- work introduces novel improvements, including depth un-
ing procedure to remove dynamic objects, addressing inac- certaintyintegrationfromRGB-Dimagesforlocalaccuracy
curaciesindepthinformation.Depth-guidedsemanticmask refinement, motion information utilization from an inertial
enhancement is then introduced to reduce inconsistencies measurement unit (IMU), and a division of NeRF a finite
along edge regions and accurately detect dynamic objects, foreground grid and a background spherical grid for di-
with subsequent background inpainting for occluded ar- verse environment handling. These enhancements result in
eas using static information from prior viewpoints. The improved tracking precision and map representation while
keyframeselectionstrategyprioritizesframeswithminimal maintainingNeRF-basedSLAMadvantages.Thisworkem-
presence of dynamic objects and limited overlap with pre- phasizestheneedforspecializeddatasetssupportingNeRF-
vious keyframes for enhanced optimization efficiency. The based SLAM, especially those providing outdoor mesh
scene representation involves multi-resolution geometric models,motiondata,andwell-characterizedsensors.
and color feature grids. The optimization process includes UncLe-SLAM [17]. UncLe-SLAM jointly learns the
geometric and photometric losses, jointly optimizing scene scenegeometryandaleatoricdepthuncertaintiesonthefly.
representationfeaturesandcameraextrinsicparametersfor ThisisachievedbyemployingtheLaplacianerrordistribu-
selectedkeyframes.Inparallel,atrackingprocessoptimizes tionassociatedwiththeinputdepthsensor.Unlikeexisting
camera poses for the current frame. The paper suggests methods, which lack the integration of depth uncertainty
potential improvements to achieve real-time performance modeling, UncLe-SLAM employs a learning paradigm to
by addressing segmentation network speed and exploring adaptively assign weights to different image regions based
the predictive capabilities of neural networks for compre- on their estimated confidence levels, obtained without re-
hensivebackgroundinpainting. quiring ground truth depth or 3D. This strategic weighting
mechanism allows UncLe-SLAM to prioritize more reliable
3.1.6 UncertaintyEstimation sensorinformation,leadingtorefinedtrackingandmapping
Analyzing uncertainties in input data, especially depth results.Furthermore,UncLe-SLAM’sadaptabilityextendsto
sensor noise, is critical for robust system processing. This scenariosinvolvingeitherRGB-Dconfigurationsormultiple
includes tasks such as filtering unreliable sensor measure- depth sensors. This enables the system to handle diverse
mentsorincorporatingdepthuncertaintyintotheoptimiza- sensor setups, where each sensor might exhibit distinct
tion process. The overall goal is to prevent inaccuracies noisecharacteristics.
within the SLAM process that could significantly impact
system accuracy. At the same time, acknowledging the in- 3.1.7 Event-basedSLAM
trinsic uncertainty in the neural model reconstruction adds Whileradiancefield-inspiredVSLAMmethodsofferadvan-
a critical layer for assessing system reliability, especially tages in accurate dense reconstruction, practical scenarios
in challenging scenarios. This section marks the beginning involving motion blur and lighting variations pose signif-
of uncertainty exploration in neural SLAM, emphasizing icant challenges that affect the robustness of the mapping
the integration of both epistemic (knowledge-based) and andtrackingprocesses.Inthissection,weexploreacategory
aleatoric (environmental-noise-based) uncertainty informa- ofsystemsthatmakeuseofdatacapturedbyeventcameras,
tion as essential components to improve overall SLAM to exploit its dynamic range and temporal resolution. The
systemperformance. asynchronous event generation mechanism, triggered by a
OpenWorld-SLAM [116]. This work improves upon logarithmic change in luminance at a given pixel, shows
NICE-SLAM [5], addressing its non-real-time execution, potential advantages in terms of low latency and high
limited trajectory estimates, and challenges with adapting temporal resolution. This has the potential to improve the
to new scenes due to its dependency on a predefined robustness,efficiency,andaccuracyofneuralVSLAMinex-
grid.Toenhanceapplicabilityinopen-worldscenarios,this treme environments. Although event camera-based SLAM15
systems are still in the early stages of investigation, we Orbeez-SLAM [117]. This approach seamlessly com-
believethatongoingresearchholdsgreatpromiseforover- bines dense monocular SLAM strengths with neural ra-
comingthelimitationsoftraditionalRGB-basedapproaches. diance field modeling. In accordance with established
EN-SLAM [14]. This framework introduces a new methodologies,thesystemoperateswithinastructuredpro-
paradigm shift by seamlessly integrating event data along- cess that encompasses both tracking and mapping phases.
sideRGB-Dthroughtheimplicitneuralparadigm.Itaimsto Thetrackingprocessentailstheextractionofimagefeatures
overcome challenges encountered by existing SLAM meth- and the estimation of camera poses using visual odometry,
ods when operating in non-ideal environments character- specificallyderivedfromORB-SLAM2[82].Conversely,the
ized by issues such as motion blur and lighting variation. mapping stage, leveraging the capabilities of Instant-NGP
The methodology centers around a differentiable Camera [53], is dedicated to the generation of map points via tri-
ResponseFunction(CRF)renderingtechnique,enablingthe angulation and the execution of BA for the optimization of
mapping of a unified representation from event and RGB camera poses and 3D points alike. This synergy not only
cameras. This process entails decoding the scene encoding, ensuresreal-timeefficiencywithouttherequirementforpre-
establishingaunifiedrepresentationforgeometryandradi- trainingbutalsoguaranteespreciseposeestimations,robust
ance,anddecomposingthesharedradiancefieldintocolor cameratracking,andaccurate3Dreconstructions.
and luminance via differentiable CRF Mappers. Addition- FMapping [118]. FMapping is a neural field mapping
ally, optimization strategies are implemented for tracking framework for real-time RGB SLAM, aiming to boost effi-
andBA.Thepaperfurtherproposesthecreationoftwochal- ciency and reduce mapping uncertainty, especially in the
lenging datasets—DEV-Indoors and DEV-Reals—including absenceofdepthdata.Toaddressthis,theauthorsconduct
scenarios with practical motion blur and lighting changes, a thorough theoretical analysis, breaking down the SLAM
as shown in Fig. 10, to evaluate EN-SLAM’s effectiveness systemintotrackingandmappingcomponents,andexplic-
androbustnessindiverseenvironments. itly defining mapping uncertainty within neural represen-
tations. Building on this analysis, the paper presents an in-
3.2 RGB-basedSLAMMethodologies novative factorization scheme for the scene representation.
This section explores RGB dense SLAM methods, which Specifically,thisapproachemploysafactorizedneuralfield
rely solely on visual cues from color images, eliminating to effectively manage uncertainty by decomposing it into
the need for depth sensors, typically light-sensitive, noisy, a lower-dimensional space, thereby increasing robustness
and, in most cases, only applicable indoors. Hence, RGB- to noise and training efficiency. A complementary sliding
only SLAM using monocular or stereo cameras is gaining window strategy further reduces uncertainty during scene
attention for scenarios where RGB-D cameras are impracti- reconstruction by incorporating coherent geometric cues
cal or costly, making RGB cameras a more viable solution from observed frames. FMapping offers significant advan-
applicable to a broader range of indoor and outdoor envi- tages, including low memory usage, streamlined computa-
ronments. However, these methods often face challenges, tion,andrapidconvergenceduringmapinitialization.
particularly in monocular setups, as they lack geometric TT-HO-SLAM [119]. Motivated by the observation that
priors, leading to depth ambiguity issues. As a result, they existing methods fail to adhere to the binary-type opacity
tendtoexhibitsloweroptimizationconvergenceduetoless prior for rigid 3D scenes, the paper introduces a novel
constrainedoptimization. ternary-type (TT) opacity model for improved optimiza-
SimilartotheRGB-Dcase,wegroupthesemethodsinto tion during volumetric rendering. The authors empirically
categories. We begin by examining the main NeRF-style observe that frame-wise volumetric rendering and the ab-
(3.2.1) SLAM techniques. Subsequently, we cover related senceofabinary-typeopacitypriorcontributetoinstability
RGB SLAM systems that utilize external frameworks for in RGB-only NeRF-SLAM. To tackle this, they propose a
additional supervision signals during optimization (3.2.2), hybrid odometry scheme that combines volumetric and
those specifically designed for semantic estimation (3.2.3), warping-basedimagerenderingsduringtracking,leadingto
andfinally,thoseaddressingsystemuncertainty(3.2.4). a substantial speed-up. Fine adjustments to camera odom-
etry are made during the BA step, which occurs jointly
3.2.1 NeRF-styleRGBSLAM with the mapping process. The methodology involves soft
DIM-SLAM [16]. This paper introduces the first RGB binarizationofadecodernetworkduringmapinitialization
SLAM system using a neural implicit map representation. andutilizestheoreticalinsightstooptimizeopacity,demon-
Similar to NICE-SLAM, it combines a learnable multi- stratingsuperiorresultsintermsofbothspeedandaccuracy.
resolutionvolumeencodingandanMLPdecoderfordepth Hi-Map [8] (formerly: FMapping [118]). This work
and color prediction. The system dynamically learns scene presentsahierarchicalfactorizedrepresentationformonoc-
featuresanddecoderson-the-fly.Moreover,DIM-SLAMop- ular mapping. The key idea is to represent the scene as
timizesoccupanciesinasinglestepbyfusingfeaturesacross a hierarchical feature grid that encodes and factorizes the
scales,improvingoptimizationspeed.Notably,itintroduces radiance into feature planes and vectors. It simplifies the
a photometric warping loss inspired by multi-view stereo, scene’sdatastructurewithlower-dimensionalelementsand
enforcingalignmentbetweensynthesizedandobservedim- allows for fast convergence on changed views where the
ages to enhance accuracy by addressing view-dependent underlying geometry is unknown. To enhance photometric
intensitychanges.SimilartootherRGB-Dapproaches,DIM- cues for distant and texture-less regions, Hi-Map employs
SLAM leverages parallel tracking and mapping threads for a dual-path encoding strategy that incorporates absolute
the concurrent optimization of camera poses and implicit coordinates into the appearance encoding. It enables the
scenerepresentation. framework to learn variations in color and lighting caused16
by changes in viewpoints. As a result, Hi-Map enhances offering a multi-level framework for modeling SDF and
geometric reconstruction and textural details, resulting in color.Additionally,thesystem’sjointmappingandtracking
higher-qualitymappingwithoutexternaldepthpriors. capabilities are facilitated by a comprehensive suite of loss
functions. These include the standard RGB rendering loss,
3.2.2 AidedSupervision the RGB warping loss for enforcing geometric consistency,
In this section, we explore RGB-based SLAM methods that andtheopticalflowlossthataidsinaddressingambiguities
use external frameworks to integrate regularization infor- andimposingsmoothnesspriors.Complementingtheseare
mation into the optimization process, referred to as aided monocular depth and normal losses extracted from an off-
supervision.Theseframeworksincludevarioustechniques, the-shelf monocular depth predictor [123], along with the
suchassupervisionderivedfromdepthestimatesobtained Eikonalloss[147]forregularization.
from single or multi-view images, surface normal estima- NeRF-VO [125]. This work follows a two-stage ap-
tion, optical flow, and more. The incorporation of external proach: firstly, employing a learning-based sparse visual
signals is crucial for disambiguating the optimization pro- trackingmethod,DPVO[126],togenerateinitialposesand
cess and helps to significantly improve the performance of sparse depth information. Then, enhancing these sparse
SLAMsystemsusingonlyRGBimagesasinput. cues involves a dense geometry module predicting dense
iMODE [120]. The system operates through a multi- depthmapsusingthestate-of-the-artmonoculardepthnet-
threaded architecture consisting of three core processes. work,DPT[128],[148],andextractingsurfacenormalsusing
First, a localisation process utilizes the ORB-SLAM2 [82] Omnidata[123]frommonocularRGBinput.Thealignment
sparse SLAM system for real-time camera pose estimation of these cues with sparse data is achieved through a scale
on a CPU, selecting keyframes for subsequent mapping. alignment procedure. Secondly, the system employs Ner-
Second, inspired by iMAP [1], a semi-dense mapping pro- facto [149] for dense scene representation. It optimizes the
cess enhances reconstruction accuracy by supervising real- representationwithcameraposes,RGBimages,depthmaps
time training through depth-rendered geometry. Despite (with uncertainty-based loss), and surface normals. This
lackingadepthcamera,monocularmulti-viewstereometh- jointoptimizationrefinesscenegeometryandcameraposes
ods provide depth measurements [146]. Third, the dense by minimizing disparities between captured images and
reconstruction process, executed on a GPU, optimizes an renderedviewsfromtheneuralrepresentation.
MLP representing the neural field. This latter differs from MoD-SLAM[127].Thismonocularframeworkfollowsa
iMAP’s implementation by incorporating both a view de- two-step process: first estimating depth and then refining
pendency aspect, which addresses photometric consistency it for precise scene reconstruction. The depth estimation
byintegratingview-dependenteffectslikespecularities,and comprisesrelativeandmetricdepthmodulesutilizingDPT
frequency separation, where a lower frequency embedding and ZoeDepth architectures. To enhance accuracy, depth
is employed for initial input, while a higher frequency estimates undergo refinement through a depth distillation
embedding is reserved for the color head only. This op- module,addressinginaccuraciesfromtheinitialdepthesti-
timization aligns keyframe images and semi-dense depth mation. MoD-SLAM integrates a multivariate Gaussian en-
maps,minimizingphotometricandgeometricerrors. coding and a ray reparameterization technique, facilitating
Hi-SLAM [122]. Three challenges are put forward in efficient representation of unbounded scenes by capturing
thiswork:lowtextureandrapidmovement,scaleambiguity detailed 3D space information. The system also employs
inherentindepthpriors,andlackofglobalconsistency.Hi- loop closure to mitigate pose drift, ensuring more precise
SLAM [122] builds on DROID-SLAM [28] to obtain dense globalposeoptimization.
pixel correspondences between nearby frames. This allows
the methodology to track camera poses with optical flow 3.2.3 SemanticRGBSLAM
andcreateakeyframegraphandakeyframebuffer.Monoc-
RO-MAP [129]. RO-MAP is a real-time multi-object map-
ulardepthpriors [123]areincorporatedtofurtherimprove
ping system that operates without depth priors, utilizing
depth accuracy. Besides, the framework proposes a joint
neural radiance fields for object representation. This ap-
depthandscaleadjustment(JDSA)moduletoachievedepth
proach combines a lightweight object-centric SLAM with
estimation and avoid scale ambiguity of monocular depth
NeRFmodelsforsimultaneouslocalizationandreconstruc-
priors. In this module, the scales and offsets of the depth
tionofobjectsfrommonocularRGBinput.Thesystemeffi-
priors are estimated and then incorporated as variables in
cientlytrainsseparateNeRFmodelsforeachobject,demon-
the BA optimization. As a result, the module maintains
strating real-time performance in semantic object mapping
the scale consistency of depth priors. To maintain global
and shape reconstruction. Key contributions include the
consistency, Hi-SLAM [122] employs a Sim(3)-based pose
development of the first 3D prior-free monocular multi-
graphbundleadjustment(PGBA)approachforonlineloop
object mapping pipeline, an efficient loss function tailored
closure.Whendetectingtheloopclosures,thePGBAprocess
forobjects,andahigh-performanceCUDAimplementation.
constructstheposegraphusingSim(3)toupdatethescales.
Thisenablestheframeworktooptimizetheposegraphwith
3.2.4 UncertaintyEstimation
globalconsistencydespitepotentialposeandscaledrift.
NICER-SLAM [18]. Thisworkintroducesaunifiedend- NeRF-SLAM [130]. By employing real-time implemen-
to-endframeworkthatconcurrentlyoptimizestrackingand tations of DROID-SLAM [28] as the tracking module and
mapping, given an RGB stream of images as input. At the Instant-NGP [53] as the hierarchical volumetric neural ra-
core,hierarchicalfeaturegridsprovidethestructureforac- diance field map, this approach successfully achieves real-
curatelyrepresentingthescene’sgeometryandappearance, time operational efficiency given RGB images as input.17
orientation while constructing a comprehensive 3D repre-
sentation of large-scale environments using LiDAR data.
The framework comprises three interconnected modules:
neural odometry, neural mapping, and mesh reconstruc-
(a)LiDARScans (b)SDF (c)Mesh tion. The neural odometry module estimates a 6-DoF pose
for each incoming LiDAR scan by minimizing SDF errors
through a fixed implicit network. The poses are subse-
quently optimized via back-projection. In parallel, the neu-
ral mapping module employs dynamic voxel embeddings
withinanoctree-basedarchitecture,adeptlycapturinglocal
geometry. This dynamic allocation strategy ensures effi-
cient utilization of computational resources, avoiding the
complexitiesofpre-allocatedembeddingsortime-intensive
hash tablesearches. Themethod uses adynamic voxelem-
(d)Point-basedImplicitNeuralMap bedding look-up table, boosting efficiency and eliminating
computationalbottlenecks.Akey-scansrefinementstrategy
Fig. 11: Overview of PIN-SLAM [133]. Top: (a) LiDAR
enhances reconstruction quality and addresses catastrophic
scans, (b) Implicit SDF , (c) Reconstructed mesh from the
forgettingduringincrementalmapping,leadingtodetailed
SDF. Bottom: (d) Visualization of the Point-based Implicit
3Dmeshrepresentationsinthefinalstep.
Neural(PIN)Map.Imagesfrom[133].
LONER [131]. This system employs parallel tracking
and mapping threads, with the tracking thread processing
incoming scans using ICP for odometry estimation. The
Moreover, incorporating depth uncertainty estimation ad-
mapping thread utilizes selected keyframes to update the
dressesinherentnoiseindepthmaps,resultinginimproved
neural scene representation. LiDAR scans are transformed
outcomes through depth loss supervision for neural ra-
usingPoint-to-PlaneICP[132],andthesceneisrepresented
diance fields – with weights determined by the depth’s
by an MLP with a hierarchical feature grid encoding. The
marginal covariance. Specifically, the pipeline involves two
proposed dynamic margin loss function combines Jensen-
real-timesynchronizedthreads:trackingandmapping.The
Shannon Divergence [150], depth loss, and sky loss. The
tracking thread minimizes BA re-projection errors for a
dynamic margin adapts to diverse map regions during
sliding keyframe window. The mapping thread optimizes
online training, enabling the system to learn new areas
all keyframes from the tracking thread without a sliding
while preserving acquired geometry. The system incorpo-
window. Communication only occurs when the tracking
ratesmeshingforofflinevisualization,creatingameshfrom
thread creates a new keyframe, sharing keyframe data,
theimplicitgeometryusingestimatedkeyframeposes.
poses,depthestimates,andcovariances.
PIN-SLAM [133]. PIN-SLAM is a SLAM system de-
signed for LiDAR scans, featuring a point-based implicit
3.3 LiDAR-BasedSLAMStrategies
neural map representation for building globally consistent
WhileVSLAMsystemsdiscussedsofaroperatesuccessfully maps, depicted in Figure 11. The system leverages sparse,
in smaller indoor scenarios where both RGB and dense optimizable neural points that exhibit elasticity, allowing
depth data are available, their limitations become appar- forcontinuousdeformationduringglobalposeadjustments.
ent in large outdoor environments where RGB-D cameras Employinganalternatingapproach,itperformsincremental
are impractical. LiDAR sensors, which provide sparse yet learningoflocalimplicitsigneddistancefieldsandposees-
accurate depth information over long distances and in a timationusingcorrespondence-freepoint-to-implicitmodel
variety of outdoor conditions, play a critical role in en- registration. The methodology includes efficient odometry
suring robust mapping and localization in these settings. estimation,dynamicpointfiltering,andloopclosuredetec-
However, the sparsity of LiDAR data and the lack of tion based on local polar context descriptors. The system
RGB information pose challenges for the application of the corrects drift through optimized neural point maps after
previously outlined dense SLAM approaches in outdoor loopclosure,achievingglobalconsistency.Prominentclaims
environments. Our focus is now on novel methodologies includesupportinglarge-scalemapping,enablingreal-time
that exploit the precision of 3D incremental LiDAR data executionthankstovoxelhashingandefficientneuralpoint
to improve autonomous navigation in outdoor scenarios, indexing,andprovidingacompactmaprepresentationsuit-
while taking advantage of scene representations based on ableforaccuratemeshreconstruction.
radiance fields, offering the potential to achieve dense,
smoothmapreconstructionoftheenvironment,eveninar- 3.3.2 3DGS-styleLiDAR-basedSLAM
easwithsparsesensorcoverage.Giventhelimitednumber LIV-GaussMap [134].TheproposedLiDAR-Inertial-Visual
ofstudiesaddressingthisspecificsetting,wecategorizethe (LIV) fused radiance field mapping system integrates
methodologies into two simple groups: NeRF (3.3.1) and hardware-synchronizedLiDAR-inertialsensorswithacam-
3DGS-style(3.3.2)LiDAR-basedSLAMcategories. era for precise data alignment. The methodology begins
with LiDAR-inertial odometry, utilizing size-adaptive vox-
3.3.1 NeRF-styleLiDAR-basedSLAM els to represent planar surfaces. LiDAR point clouds are
NeRF-LOAM [15].NeRF-LOAMintroducesthefirstneural segmented into voxels, and covariance matrices are com-
implicit approach to jointly determine sensor position and putedforinitialellipticalsplattingestimates.Thesystemis18
1.0 1.0 0.25
0.8 0.8
0.20
0.6 0.6
0.15
0.4 0.4
0.10
0.2 0.2
500 1000 1500 2000 500 1000 1500 2000 500 1000 1500 2000 500 1000 1500 2000
Frames Frames Frames 0.0 Frames 0.0 0.05
0.0 0.2 0.4 0.6 0.8 1.00 1
NICE-SLAM[5] Co-SLAM[73] ESLAM[72] PLGSLAM[99]
Fig.12:SLAM Methods Comparison on the ScanNet [67] Dataset – Surface Reconstruction and Localization Accuracy.
Groundtruthtrajectoryinblue,estimatedtrajectoryinorange.ATEvisualizedwithacolorbar.
refined by optimizing spherical harmonic coefficients and Method Tracker Global Loop fr1/desk fr2/xyz fr3/office Avg(↓)
Basedon BA Closure
LiDARGaussianstructuresusingvisual-derivedphotomet- RGB-D
Kintinuous[2] 3.7 2.9 3.0 3.2
ricgradients,enhancingmappingprecisionandvisualreal- BAD-SLAM[30] ✓ ✓ 1.7 1.1 1.7 1.5
ORB-SLAM2[82] ✓ ✓ 1.6 0.4 1.0 1.0
ism. The initialization of Gaussians involves size-adaptive Vox-Fusion [71] 3.5 1.5 26.0 10.3
voxel partitioning, with further subdivision based on a MeSLAM [6] 6.0 6.5 7.8 6.8
iMAP[1] 4.9 2.0 5.8 4.2
specified parameter. Adaptive control of the 3D Gaussian GS-SLAM[151] 3.3 1.3 6.6 3.7
SplaTAM[89] 3.4 1.2 5.2 3.3
mapaddressesunder-reconstructionandover-densescenar- MIPS-Fusion [96] ✓ 3.0 1.4 4.6 3.0
Point-SLAM[76] 4.3 1.3 3.5 3.0
ios through structure refinement and photometric gradient Loopy-SLAM[100] ✓ 3.8 1.6 3.4 2.9
NICE-SLAM[5] 2.7 1.8 3.0 2.5
optimization.Thesystemachievesreal-timerenderingusing
vMAP[104] ORB3[88] 2.6 1.6 3.0 2.4
rasterizationandalphablending. Co-SLAM[73] ✓ 2.4 1.7 2.4 2.2
ESLAM[72] 2.5 1.1 2.4 2.0
GSSLAM[90] 1.5 1.6 1.7 1.6
GO-SLAM[74] DROID[75] ✓ ✓ 1.5 0.6 1.3 1.1
NGEL-SLAM[98] ORB3[88] ✓ ✓ 1.5 0.5 1.0 1.0
4 EXPERIMENTS AND ANALYSIS DDN-SLAM[9] ORB3[88] ✓ ✓ 1.5 0.4 0.9 0.9
RGB
Inthissection,wecomparemethodsacrossdatasets,focus- DROID-SLAM[75] - ✓ 1.8 0.5 2.8 1.7
ORB-SLAM2[82] - ✓ ✓ 1.9 0.6 2.4 1.6
ing on tracking (visual in 4.1.1, LiDAR in 4.2.1) and 3D GSSLAM[90] 4.2 4.8 4.4 4.4
reconstruction (visual in 4.1.2, LiDAR in 4.2.2). Addition- DDN-SLAM[9] ORB3[88] ✓ ✓ 1.9 2.4 2.9 2.4
DIM-SLAM[16] 2.0 0.6 2.3 1.6
ally, we explore novel view synthesis (4.1.3) and analyze Orbeez-SLAM[117] ORB2[82] 1.9 0.3 1.0 1.1
performance in terms of runtime and memory usage (4.3). TABLE2:TUMRGB-D[66]CameraTrackingResults.ATE
In each subsequent table, we emphasize the best results RMSE[cm]( )isusedastheevaluationmetric.
↓
withingasubcategoryusingboldandhighlighttheabsolute
best in purple. In our analysis, we organized quantitative
data from papers with a common evaluation protocol and Method BT ar sa ec dke or n Gl Bo Abal CL loo so up re 0000 0059 0106 0169 0181 0207 Avg(↓)
RGB-D
cross-verifiedtheresults.Ourprioritywastoincludepapers D DR RO OI ID D- -S SL LA AM M( [V 75O ])[75] ✓ 8 5. .0 30 6 1 71 .7.3 20 9 7. .9 07 6 8 8. .6 04 1 7 6. .3 98 7 - - - -
with consistent benchmarks, ensuring a reliable basis for i iM DFA -SP L[ A1] M[84] 5 55 7. .9 75 - 3 82 .. 20 -6 1 57 .. 85 -0 7 30 9. .5 91 - 3 22 9. .1 10 - 1 11 5. .9 41 - 23 66 .. 06 17 -
ADFP[78] - 10.50 7.48 9.31 - 5.67 -
comparison across multiple sources. Although not exhaus- Point-SLAM[76] 10.24 7.81 8.65 22.16 14.77 9.54 12.19
SplaTAM[89] 12.83 10.10 17.72 12.08 11.10 7.46 11.88
tive,thisapproachguaranteestheinclusionofmethodswith MIPS-Fusion[96] ✓ 7.9- 10.7- 9.7- 9.7- 14.2- 7.8- 10.0-
Vox-Fusion[71] 8.39 - 7.44 6.53 12.20 5.57 -
verifiableresultsandasharedevaluationframeworkinour NICE-SLAM[5] 8.64 12.25 8.09 10.28 12.93 5.59 9.63
NeuV-SLAM[86] 12.71 9.70 8.50 8.92 12.72 5.61 9.68
tables.Forperformanceanalysis,weutilizedmethodswith Co-SLAM[73] ✓ 7.18 12.29 9.57 6.62 13.43 7.13 9.37
Loopy-SLAM[100] ✓ 4.2- 7.5- 8.3- 7.5- 10.6- 7.9- 7.7-
availablecodetoreportruntimeandmemoryrequirements ESLAM[72] 7.3- 8.5- 7.5- 6.5- 9.0- 5.7- 7.4-
Structerf-SLAM[81] ORB2[82] 7.28 6.07 8.50 7.35 - 7.28
on a common hardware platform, a single NVIDIA 3090 N DNGE SL S- LS ALA MM [1[ 09 98 ]] ORB3[88] ✓ ✓ ✓ 7 5. .2 43 2 6 5. .9 28 0 7 9. .9 15 1 6 7. .1 72 0 1 10 0. .1 14 2 6 4. .2 97 1 7 7. .4 04 7
GPU. For specific implementation details of each method, S GN OI -- SS LL AA MM [[ 71 40 ]7] DROID[75] ✓ ✓ 6 5. .9 30 5 7 7. .3 58 2 7 7. .1 09 3 4 7. .7 70 4 6.- 84 4.- 78 6.- 54
readersareencouragedtorefertotheoriginalpapers. MoD-SLAM[127] DROID[75] ✓ 5.27 7.44 6.73 6.48 3.29 5.31 5.75
RGB
DROID-SLAM(VO)[75] - 11.05 67.26 11.20 16.21 9.94 - -
DROID-SLAM[75] - ✓ 5.48 9.00 6.76 7.86 7.41 - -
Orbeez-SLAM[117] ORB2[82] 7.22 7.15 8.05 6.58 15.77 7.16 8.66
4.1 VisualSLAMEvaluation Hi-SLAM[122] DROID[75] ✓ ✓ 6.40 7.20 6.50 8.50 7.60 8.40 7.40
GO-SLAM[74] DROID[75] ✓ ✓ 5.94 8.27 8.07 8.42 8.29 5.31 7.38
MoD-SLAM[127] DROID[75] ✓ 5.39 7.78 7.44 6.69 3.97 5.63 6.15
Inlinewithexistingprotocols,thissectioncomparesSLAM
TABLE 3: ScanNet [67] Camera Tracking Results. ATE
systems using RGB-D or RGB data. We evaluate tracking,
RMSE[cm]( )isusedastheevaluationmetric.
3D reconstruction, rendering, and consider runtime and ↓
memory usage. Specifically, results are presented on the
TUM-RGB-D[66],Replica[68],andScanNet[67]datasets. images. Key benchmarks include established methods like
Kintinuous, BAD-SLAM, and ORB-SLAM2, representing
4.1.1 Tracking traditionalhand-craftedbaselines.
TUM-RGB-D.Table2providesathoroughanalysisofcam- In the RGB-D setting, it is evident that methods based
era tracking results on three scenes of the TUM RGB-D on recent radiance field representations generally exhibit
dataset, marked by challenging conditions such as sparse lower performance compared to reference methods like
depth sensor information and high motion blur in RGB BAD-SLAM and ORB-SLAM2. However, a noteworthy ob-
ATE
Values
)ETA(
rorrE
yrotcejarT
etulosbA19
ESLAM[72] GO-SLAM[74] Point-SLAM[76] Loopy-SLAM[100] Ground-Truth
Fig.13:SLAMMethodsComparisonontheReplica[68]Dataset–Mapping.Imagessourcedfrom[100].
servation is that methods leveraging external trackers such Method BT ar sa ec dke or n Gl Bo Abal CL loo so up re R0 R1 R2 O0 O1 O2 O3 O4 Avg(↓)
RGB-D
asORB3andDROID,coupledwithadvancedstrategieslike iMAP[1] 3.12 2.54 2.31 1.69 1.03 3.99 4.05 1.93 2.58
NICE-SLAM[5] 1.69 2.04 1.55 0.99 0.90 1.39 3.97 3.08 1.95
Global BA and LC, emerge as top performers. Specifically, ADFP[78] 1.39 1.55 2.60 1.09 1.23 1.61 3.61 1.42 1.81
MIPS-Fusion[96] ✓ 1.10 1.20 1.10 0.70 0.80 1.30 2.20 1.10 1.19
NGEL-SLAM, DNN-SLAM, and GO-SLAM demonstrate Co-SLAM[73] ✓ 0.65 1.13 1.43 0.55 0.50 0.46 1.40 0.77 0.86
NIDS-SLAM[105] ORB3[88] ✓ 0.58 0.41 0.58 0.62 0.40 1.20 0.88 1.80 0.80
superior accuracy in RGB-D scenarios, with DDN-SLAM ESLAM[72] 0.71 0.70 0.52 0.57 0.55 0.58 0.72 0.63 0.63
GSSLAM[90] 0.76 0.37 0.23 0.66 0.72 0.30 0.19 1.46 0.58
achievingthebestaverageATERSMEresultsof0.9. Vox-Fusion[71] 0.27 1.33 0.47 0.70 1.11 0.46 0.26 0.58 0.65
Point-SLAM[76] 0.61 0.41 0.37 0.38 0.48 0.54 0.72 0.63 0.52
When shifting the focus to the RGB scenario, ORB- G DS N- SSL SA LM AM[1 [2 1] 09] ✓ 0 0. .4 48 9 0 0. .5 43 6 0 0. .3 33 8 0 0. .5 32 4 0 0. .4 31 5 0 0. .5 39 9 0 0. .4 66 2 0 0. .7 60 0 0 0. .5 40 5
SLAM2 and DROID-SLAM serve as baselines, with ORB- S Gp Ola -ST LA AM M[8 [9 7] 4] DROID[75] ✓ ✓ 0 0. .3 31 2 0 0. .4 30 0 0 0. .2 39 9 0 0. .4 37 9 0 0. .2 47 6 0 0. .2 39 4 0 0. .3 22 9 0 0. .5 25 9 0 0. .3 36 4
SLAM2 exhibiting superior tracking accuracy. Among re- M Loo oD p- yS -L SLA AM M[1 [2 17 0] 0] DROID[75] ✓ ✓ 0.- 24 0.- 24 0.- 28 0.- 26 0.- 40 0.- 29 0.- 22 0.- 35 0 0. .3 23 9
cent SLAM methods, GSSLAM and DDN-SLAM exhibit DROID-SLAM[75] - ✓ RG -B - - - - - - - 0.42
highATERMSEvaluesof4.4and2.4,respectively.Thisisin TT-HO-SLAM[119] 4.51 0.91 7.49 0.59 1.74 1.70 0.81 3.47 2.65
NICER-SLAM[18] 1.36 1.60 1.14 2.12 3.23 2.12 1.42 2.01 1.88
contrasttotheRGB-DcasewhereDDN-SLAMachievesthe D GOIM -S-S LL AA MM [7[ 41 ]6] DROID[75] ✓ ✓ 0. -48 0. -78 0. -35 0. -67 0. -37 0. -36 0. -33 0. -36 0 0. .4 36 9
MoD-SLAM[127] DROID[75] ✓ - - - - - - - - 0.35
bestresults,indicatingtheimportanceofdepthinformation
for this methodology to perform well and its greater sensi- TABLE4:Replica[68]CameraTrackingResults.ATERMSE
tivityintheRGB-onlyscenario.Despitethis,Orbeez-SLAM [cm]( )isusedastheevaluationmetric.
↓
jointlywithORB-SLAM2leadswithanATERMSEof1.1.
Method L1-Depth↓ Acc.[cm]↓ Comp.[cm]↓ Comp.Ratio[%]↑
These results emphasize the varied performance of
RGB-D
SLAM frameworks, with approaches based on the latest COLMAP[152] - 8.69 12.12 67.62
TSDF[153] 7.57 1.60 3.49 86.08
radiance field representations exhibiting effective results in iMAP[1] 7.64 6.95 5.33 66.60
RGB-Dscenariosthroughexternaltrackingapproachesand NICE-SLAM[5] 3.53 2.85 3.00 89.33
GO-SLAM*[74] 3.38 2.50 3.74 88.09
additional optimization strategies. However, in scenarios GO-SLAM[74] 4.68 2.50 3.74 88.09
MoD-SLAM[127] 3.21 2.29 - -
wheretheselatterarenotapplied,mostmethodsstillstrug- DNSSLAM[109] 3.16 2.76 2.74 91.73
ADFP[78] 3.01 2.77 2.45 92.79
gle with trajectory drift and sensitivity to noise. Further- NID-SLAM[115] 2.87 2.72 2.56 91.16
more, challenges persist for methods in RGB-only settings, Vox-Fusion[71] 2.67 4.55 3.00 86.59
Co-SLAM [73] 1.51 - - -
except for Orbeez-SLAM, which outperforms traditional NGEL-SLAM[98] 1.28 - - -
ESLAM[72] 1.18 - - -
methodslikeORB-SLAM2andDROID-SLAM. Point-SLAM[76] 0.44 1.41 3.10 88.89
Loopy-SLAM[100] 0.35 - - -
ScanNet. Table 3 presents the evaluation of camera RGB
tracking methods on six scenes of the ScanNet dataset. In NeRF-SLAM[130] 4.49 - - -
DIM-SLAM[16] - 4.03 4.20 79.60
the RGB-D domain, standout performers are the frame-to- GO-SLAM[74] 4.39 3.81 4.79 78.00
NICER-SLAM[18] - 3.65 4.16 79.37
frame models MoD-SLAM and GO-SLAM. Both leverage Hi-SLAM[122] 3.63 3.62 4.59 80.60
MoD-SLAM[127] 3.40 2.68 - -
externaltrackers(suchasDROID-SLAM)andLCstrategies,
withGO-SLAMincorporatingalsoGlobalBA.Significantly, TABLE5:Replica[68]MappingResults.L1-Depth( ),Acc.
MoD-SLAM achieves the best average ATE RMSE result of [cm]( ),Comp.[cm]( )andComp.Ratio[%]( )wit↓ h5cm
5.75.AsimilartrendcanbeobservedintheRGBcase,where thresh↓ oldareusedast↓ heevaluationmetrics.*e↑ valuateson
once again, the best results are achieved by GO-SLAM and groundtruthposes.
MoD-SLAM.Nonetheless,itisworthnotingthatalternative
solutionssuchasOrbeez-SLAMandHi-SLAM–whichalso scenesfromReplica,usinghigher-qualityimagescompared
rely on external trackers – manage to be comparable or tochallengingcounterpartslikeScanNetandTUMRGB-D.
even superior to many other SLAM methods that leverage TheevaluationincludesthereportingofATERMSEresults
depth information from RGB-D sensors. In Figure 12, we foreachindividualscene,alongsidetheaveragedoutcomes.
reportsomequalitativeresultsfromselectedRGB-DSLAM On top, we report the evaluation concerning RGB-D
systems on ScanNet, highlighting recent improvements in methods. In line with observations from TUM RGB-D and
trajectoryerrorcomparedtotheseminalsystems. ScanNetdatasets,thehighestaccuracyisachievedbylever-
Replica. Table 4 evaluates camera tracking across eight agingexternaltrackingandmethodologiesinvolvingGlobal
0mooR
3ecfifO20
NICE-SLAM[5] Co-SLAM[73] ESLAM[72] GS-SLAM[12] Ground-Truth
Fig.14:SLAMMethodsComparisonontheReplica[68]Dataset–ImageRendering.Imagessourcedfrom[12].
BAandLC.Inparticular,GO-SLAMandMoD-SLAMonce Method Metric R0 R1 R2 O0 O1 O2 O3 O4 Avg
RGB-D
again stand out on Replica, confirming their effectiveness PSNR↑ 22.39 22.36 23.92 27.79 29.83 20.33 23.47 25.21 24.41
Vox-Fusion[71] SSIM↑ 0.68 0.75 0.80 0.86 0.88 0.79 0.80 0.85 0.80
in optimizing camera tracking accuracy. It is worth not- LPIPS↓ 0.30 0.27 0.23 0.24 0.18 0.24 0.21 0.20 0.24
ing that, in this setting, Loopy-SLAM achieves the overall PSNR↑ 22.12 22.47 24.52 29.07 30.34 19.66 22.23 24.94 24.42
NICE-SLAM[5] SSIM↑ 0.69 0.76 0.81 0.87 0.89 0.80 0.80 0.86 0.81
best results. Additionally, promising results are evident for LPIPS↓ 0.33 0.27 0.21 0.23 0.18 0.23 0.21 0.20 0.23
methods utilizing Gaussian Splatting, such as SplatTAM PSNR↑ - - - - - - - - 27.38
GO-SLAM[74] SSIM↑ - - - - - - - - 0.851
andGS-SLAM.Thissuggeststhattheseapproachesstruggle LPIPS↓ - - - - - - - - -
PSNR↑ - - - - - - - - 27.80
with noise and work best in simpler situations, showing
ESLAM[72] SSIM↑ - - - - - - - - 0.921
less reliability in complex conditions—similar to what was LPIPS↓ - - - - - - - - 0.25
PSNR↑ - - - - - - - - 29.95
observedintheTUMRGB-DandScanNetdatasets. MoD-SLAM[127] SSIM↑ - - - - - - - - 0.862
At the bottom, we collect results achieved by RGB-only LPIPS↓ - - - - - - - - -
PSNR↑ 32.86 33.89 35.25 38.26 39.17 31.97 29.70 31.81 34.11
frameworks. Again, we can notice how global BA and LC SplaTAM[89] SSIM↑ 0.98 0.97 0.98 0.98 0.98 0.97 0.95 0.95 0.97
LPIPS↓ 0.07 0.10 0.08 0.09 0.09 0.10 0.12 0.15 0.10
playavitalrole inachievingthehighesttrackingaccuracy;
PSNR↑ 31.56 32.86 32.59 38.70 41.17 32.36 32.03 32.92 34.27
indeed, GO-SLAM and MoD-SLAM yield the best results, GS-SLAM[12] SSIM↑ 0.97 0.97 0.97 0.99 0.99 0.98 0.97 0.97 0.97
LPIPS↓ 0.09 0.07 0.09 0.05 0.03 0.09 0.11 0.11 0.08
outperforming even most RGB-D frameworks not making PSNR↑ 32.40 34.08 35.50 38.26 39.16 33.99 33.48 33.49 35.17
useofeitherglobalBAorLC. Point-SLAM[76] SSIM↑ 0.97 0.98 0.98 0.98 0.99 0.96 0.96 0.98 0.97
LPIPS↓ 0.11 0.12 0.11 0.10 0.12 0.16 0.13 0.14 0.12
PSNR↑ - - - - - - - - 35.47
Loopy-SLAM[100] SSIM↑ - - - - - - - - 0.981
4.1.2 Mapping LPIPS↓ - - - - - - - - 0.109
PSNR↑ 33.16 35.18 36.49 40.22 38.90 34.22 34.74 33.24 35.76
Replica. In Table 5, we provide mapping results according NIDS-SLAM [105] SSIM↑ 0.99 0.99 0.99 0.99 0.97 0.93 0.99 0.99 0.98
LPIPS↓ - - - - - - - - -
to the evaluation protocol proposed in [5], highlighting the PSNR↑ 34.83 36.43 37.49 39.95 42.09 36.24 36.70 36.07 37.50
performance in terms of both 3D reconstruction and 2D GSSLAM[90] SSIM↑ 0.95 0.96 0.96 0.97 0.98 0.96 0.96 0.96 0.96
LPIPS↓ 0.07 0.08 0.07 0.07 0.06 0.08 0.07 0.10 0.07
depth estimation on the Replica dataset. Examining the PSNR↑ 34.31 37.28 38.18 43.97 43.56 37.39 36.48 40.19 38.90
table, a noticeable progression in both 3D reconstruction Gaussian-SLAM[91] SSIM↑ 0.99 0.99 0.99 1.00 0.99 0.99 0.99 1.00 0.99
LPIPS↓ 0.08 0.07 0.07 0.04 0.07 0.08 0.08 0.07 0.07
and 2D depth estimation metrics is observed, showcasing RGB
PSNR↑ - - - - - - - - 22.13
an improvement from iMap to more recent methods such
GO-SLAM[74] SSIM↑ - - - - - - - - 0.73
as NID-SLAM and ADFP. Notably, Loopy-SLAM leads in LPIPS↓ - - - - - - - - -
PSNR↑ 25.33 23.92 26.12 28.54 25.86 21.95 26.13 25.47 25.41
theL1-Depthmetric,closelyfollowedbyPoint-SLAM.This NICER-SLAM[18] SSIM↑ 0.75 0.77 0.83 0.87 0.85 0.82 0.86 0.87 0.83
suggests that the neural point representation holds signif- LPIPS↓ 0.25 0.22 0.18 0.17 0.18 0.20 0.16 0.18 0.19
PSNR↑ - - - - - - - - 27.31
icant promise for generating highly accurate scene recon- MoD-SLAM[127] SSIM↑ - - - - - - - - 0.85
LPIPS↓ - - - - - - - - -
structions.Intermsof3Derrormetrics,DNSSLAM,ADFP
PSNR↑ - - - - - - - - 33.30
and NID-SLAM outperform other methods, even surpass- Photo-SLAM[87] SSIM↑ - - - - - - - - 0.93
LPIPS↓ - - - - - - - - -
inghand-craftedapproacheslikeCOLMAPandTSDF,with
Point-SLAMperformingcomparably,excellingintheAccu- TABLE 6: Replica [68] Train View Rendering Results. We
racymetricwithavalueof1.41,indicatingitssuperiorityin reportPSNR,SSIM,andLPIPSmetrics.
this aspect. Notably, despite GO-SLAM’s notable achieve-
ments in tracking, it holds a relatively low position in this
methodsrelyingsolelyonRGBperformlessfavorablythan
ranking, indicating challenges for the mapping process. In
those leveraging depth sensor information. The exception
Figure13,qualitativesfromasubsetofreviewedsystemson
to this trend is iMAP. This emphasizes the crucial role of
Replica are presented, emphasizing specific improvements
depthsensorsinSLAMandpointstowardsthepotentialfor
achievedbyrecentmethodsinthemappingprocess.
advancementsinRGB-onlymethodologies.
Shifting focus to RGB methods, NICER-SLAM and Hi-
SLAM exhibit a well-balanced performance, showcasing
competitive scores in both Accuracy and Completion met- 4.1.3 ImageRendering
rics. However, the distinction among different methods in Replica. In Table 6, we show the rendering quality on
the RGB context is less pronounced compared to RGB- the training input views of Replica, following the standard
D scenarios. Notably, it becomes evident that, expectedly, evaluationapproachofPoint-SLAMandNICE-SLAM.
2ecfifO
0mooR21
On top, we focus on RGB-D frameworks: recent solu- Method 00 01 02 03 04 05 06 07 08 09 10 Avg. 11-21
LiDAROdometryEvaluation
tions, particularly those based on Gaussian Splatting or MULLS[154] 0.56 0.64 0.55 0.71 0.41 0.30 0.30 0.38 0.78 0.48 0.59 0.52 0.65
CT-ICP[155] 0.49 0.76 0.52 0.72 0.39 0.25 0.27 0.31 0.81 0.49 0.48 0.50 0.59
neural points such as Point-SLAM, yield significantly bet-
SuMa-LO[157] 0.72 1.71 1.06 0.66 0.38 0.50 0.41 0.55 1.02 0.48 0.71 0.75 1.39
ter average metrics in terms of PSNR, SSIM, and LPIPS Litamin-LO[156] 0.78 2.10 0.95 0.96 1.05 0.55 0.55 0.48 1.01 0.69 0.80 0.88 -
Nerf-LOAM[15] 1.34 2.07 - 2.22 1.74 1.40 - 1.00 - 1.63 2.08 1.69 -
comparedtomethodologiesproposedintheearlystagesof PIN-LO[133] 0.55 0.54 0.52 0.74 0.28 0.29 0.32 0.36 0.83 0.56 0.47 0.50
the evolution of neural SLAM (showing an improvement 00† 01 02† 03 04 05† 06† 07† 08† 09† 10 Avg.† Avg.
LiDARSLAMEvaluation
of over 10dB in PSNR). These early methods are based MULLS[154] 1.1 1.9 5.4 0.7 0.9 1.0 0.3 0.4 2.9 2.1 1.1 1.9 1.6
on multi-resolution feature grids such as NICE-SLAM or SuMa[157] 1.0 13.8 7.1 0.9 0.4 0.6 0.6 1.0 3.4 1.1 1.3 2.1 3.2
Litamin2[156] 1.3 15.9 3.2 0.8 0.7 0.6 0.8 0.5 2.1 2.1 1.0 1.5 2.4
voxel-based neural implicit surface representations such as HLBA[159] 0.8 1.9 5.1 0.6 0.8 0.4 0.2 0.3 2.7 1.3 1.1 1.5 1.4
PIN-LO[133] 4.3 2.0 7.3 0.7 0.1 2.1 0.7 0.4 3.5 1.8 0.6 2.9 2.1
Vox-Fusion.Thissuggeststhatparadigmsbasedonexplicit
PIN-SLAM[133] 0.8 2.0 3.3 0.7 0.1 0.2 0.4 0.3 1.7 1.0 0.6 1.1 1.0
Gaussian primitives or neural points lead to significant
improvementsinimagerendering. TABLE 7: KITTI [69] LiDAR Odometry/SLAM Results.
Atthebottom,whenconsideringRGB-onlymethods,the indicates sequences with loops and Avg. denotes the
† †
useofthe3DGSframeworkallowsPhoto-SLAMtogenerate averagemetricforsuchsequences.
novelviewswithsuperiorqualitycomparedtootherNeRF-
style SLAM systems. In Figure 14, we present qualitative Method 01 02 quad math e ug e cloister e stairs Avg.
MULLS[154] 2.51 8.39 0.12 0.35 0.86 0.41 - 2.11
results for image rendering from selected RGB-D SLAM
SuMa[157] 2.03 3.65 0.28 0.16 0.09 0.20 1.85 1.18
systems on Replica. The latest frameworks demonstrate PIN-LO[133] 2.21 4.93 0.09 0.10 0.07 0.41 0.06 1.12
improvedrenderingoffinedetails,withGS-SLAMshowing PIN-SLAM[133] 0.43 0.30 0.09 0.09 0.07 0.18 0.06 0.19
superiorrenderingqualityduetoits3DGSrepresentation.
TABLE 8: Newer College [70] Camera Tracking Results.
Inouranalysis,weagreewiththefindingspresentedin
ATERMSE[cm]( )isusedastheevaluationmetric.
the SplaTAM paper. In particular, we share concerns about ↓
therelevanceoftherenderingresultsontheReplicadataset.
Quad MathInstitute
Method
Evaluating the same training views used as input raises Acc.[cm]↓ Comp.[cm]↓ Acc.[cm]↓ Comp.[cm]↓
valid concerns about potential biases introduced by high SLAMesh[158] 19.21 48.83 12.80 23.50
Nerf-LOAM[15] 12.89 22.21 - -
model capacity and the risk of overfitting to these specific
PIN-SLAM[133] 11.55 15.25 13.70 21.91
images.OuragreementwiththeSplaTAMperspectiveleads
us to support the exploration of alternative methods for TABLE9:Newer College [70] Mapping Results.Acc.[cm]
evaluating novel view rendering in this specific context. ( )andComp.[cm]( )areusedastheevaluationmetrics.
↓ ↓
It is crucial to emphasize that our agreement with these
observations is rooted in a collective understanding of the
NewerCollege.Table8reportsthetrackingaccuracyon
constraintsinherentinthecurrentSLAMbenchmarks.
theNewerCollegedataset,measuredintermsofATERMSE
[cm]. Again, we can observe how PIN-SLAM consistently
4.2 LiDARSLAM/OdometryEvaluation outperformsPIN-LO,withanaverageRMSEof0.19cmover
thewholesetofsequences,whichis5 lowercomparedto
4.2.1 Tracking ×
PIN-LO.ThisfurtherconfirmsthesuperiorityofPIN-SLAM
KITTI. Table 7 presents the evaluation of LiDAR SLAM atglobaltrajectorytracking.
strategiesontheKITTIdataset,detailingodometryaccuracy
at the top and SLAM performance metrics at the bottom. 4.2.2 Mapping
The odometry section reports the average relative transla- Newer College. Table 9 collects the results concerning the
tionaldrifterror(%)andhighlightstheperformanceofPIN- quality of 3D reconstruction on the New College dataset –
LO, a variant of PIN-SLAM that disables the loop closure specifically,onQuadandMathInstitutesequences.Accuracy
detection correction and pose graph optimization mod- andCompletenessscoresareusedtoassesstheeffectiveness
ules.PIN-LOoutperformsseveralLiDARodometrysystems of Nerf-LOAM and PIN-SLAM, with the latter confirming
using different map representations (feature points [154], again as the best LiDAR-based SLAM system among those
denser voxel downsampling points [155], normal distribu- evaluating on this dataset. In particular, on Quad we can
tion transformation [156], surfels [157] and triangle meshes appreciatealargemarginintermsofcompletenessbetween
[158]), achieving an impressive translation error of 0.5%, PIN-SLAMandNerf-LOAM–i.e.,about7cm.
competing with KISS-ICP and CT-ICP, and outperforming
the neural implicit approach Nerf-LOAM due to improved
4.3 PerformanceAnalysis
SDFtrainingandrobustpoint-to-SDFregistration.
IntheLiDARSLAMevaluationatthebottomofthetable We conclude the experimental studies by considering the
7, the ATE RMSE [m] is used as the evaluation metric. As efficiencyoftheSLAMsystemsreviewedsofar.Forthispur-
a representative of implicit LiDAR-based SLAM strategies, pose, we run methods with source code publicly available
PIN-SLAMconsistentlyoutperformsstate-of-the-artLiDAR andmeasure1)theGPUmemoryrequirements(asthepeak
SLAMsystems.Specifically,PIN-SLAMachievesanaverage memory use in GB) and 2) the average FPS (computed as
RMSE of 1.1 m on sequences with loop closure and 1.0 m thetotaltimerequiredtoprocessasinglesequence,divided
over all eleven sequences. The results of PIN-LO under- by the total amount of frames in it) achieved on a single
scorethesignificantimprovementofPIN-SLAMinensuring NVIDIA RTX 3090 board. Table 10 collects the outcome
globaltrajectoryconsistency. of our benchmark for RGB-D and RGB systems running22
on Replica, sorted in increasing order of average FPS. On Method SceneEncoding GPUMem.[G]↓ Avg.FPS↑
RGB-D
top, we consider RGB-D frameworks: we can notice how
iMAP[1] MLP 6.44 0.13
SplaTAM, despite its high efficiency at rendering images, SplaTAM[89] 3DGaussians 18.54 0.14
Point-SLAM[76] NeuralPoints+MLP 7.11 0.23
is however much slower at processing both tracking and
UncLe-SLAM[17] Hier.Grid+MLP 8.24 0.24
mapping simultaneously. This is also the case for hybrid NICE-SLAM[5] Hier.Grid+MLP 4.70 0.61
methodsusinghierarchicalfeaturegrids,ontheotherhand ADFP[78] Hier.Grid+MLP 3.76 0.74
Vox-Fusion [71] SparseVoxels+MLP 21.22 0.74
require much less GPU memory – 4 to 5 lower compared Plenoxel-SLAM[80] Plenoxels 13.04 1.25
×
to SplaTAM. Finally, the use of more advanced representa- ESLAM[72] FeaturePlanes+MLP 13.04 4.62
Co-SLAM[73] HashGrid+MLP 3.56 7.97
tions such as hash grids or point features allows for much
GO-SLAM[74] HashGrid+MLP 18.50 8.36
faster processing. This is confirmed also by the studies on RGB
the RGB-only methods, in the middle, with NeRF-SLAM DIM-SLAM[16] Hier.Grid+MLP 4.78 3.14
Orbeez-SLAM[117] Voxels+MLP 7.55 17.70
resulting 6 faster than DIM-SLAM. Finally, concerning NeRF-SLAM[130] HashGrid+MLP 9.38 20.00
×
LiDAR SLAM systems, we can observe how PIN-SLAM LiDAR
Nerf-LOAM[15] SparseVoxel+MLP 11.58 0.24
is much more efficient than Nerf-LOAM, requiring as few
PIN-SLAM[133] NeuralPoints+MLP 6.93 6.67
as 7 GB of GPU memory while running at nearly 7 FPS,
compared to the nearly 12 GB and 4 seconds per frame TABLE10:Performance Evaluation:GPUmemoryrequire-
requiredbyNerf-LOAM. ments (GB) and average FPS efficiency on Replica room0
This analysis highlights how, despite the great promise (RGB/RGB-D)andKITTI00sequence(LiDAR).
brought by this new generation of SLAM systems, most
of them are still unsatisfactory in terms of hardware and
runtime requirements, making them not yet ready for real- Despite significant progress over the past three years,
timeapplications. ongoing research is still actively engaged in overcoming
existing scene representation limitations and finding ever
more effective alternatives to improve accuracy and real-
5 DISCUSSION timeperformanceinSLAM.
Catastrophic Forgetting.Existingmethodsoftenexhibit
In this section we focus on highlighting the key findings a tendency to forget previously learned information, par-
of the survey. We will outline the main advances achieved ticularly in large scenarios or extended video sequences.
through the most recent methodologies examined, while In the case of network-based methods, this is attributed to
identifying ongoing challenges and potential avenues for their reliance on single neural networks or global models
futureresearchinthisarea. with fixed capacity, which are affected by global changes
Scene Representation. The choice of scene represen- during optimization. One common approach to alleviate
tation is critical in current SLAM solutions, significantly this problem is to train the network using sparse ray sam-
affectingmapping/trackingaccuracy,renderingquality,and pling with current observations while replaying keyframes
computation. Early approaches, such as iMAP [1], used from historical data. However, in large-scale incremental
network-based methods, implicitly modeling scenes with mapping,suchastrategyresultsinacumulativeincreasein
coordinate-based MLP(s). While these provide compact, data,requiringcomplexresamplingproceduresformemory
continuous modeling of the scene, they struggle with real- efficiency. The forgetting problem extends to grid-based
time reconstruction due to challenges in updating local approaches. Despite efforts to address this issue, obstacles
regionsandscalingforlargescenes.Inaddition,theytendto arise due to quadratic or cubic spatial complexity, which
produce over-smoothed scene reconstructions. Subsequent poses scalability challenges. Similarly, while explicit repre-
research has explored grid-based representations, such as sentations, such as 3DGS-style solutions, offer a practical
multi-resolutionhierarchical[5],[73]andsparseoctreegrids workaroundforcatastrophicforgetting,theyfacechallenges
[71], [95], which have gained popularity. Grids allow for due to increased memory requirements and slow process-
fastneighborlookups,butrequireapre-specifiedgridreso- ing,especiallyinlargescenes.Somemethodsattempttomit-
lution, resulting in inefficient memory use in empty space igatetheselimitationsbyemployingsparseframesampling,
and a limited ability to capture fine details constrained butthisleadstoinefficientinformationsamplingacross3D
by the resolution. Recent advances, such as Point-SLAM space, resulting in slower and less uniform model updates
[76], favor hybrid neural point-based representations. Un- comparedtoapproachesthatintegratesparseraysampling.
like grids, point densities vary naturally and need not Eventually,somestrategiesrecommenddividingtheen-
be pre-specified. Points concentrate efficiently around sur- vironment into submaps and assigning local SLAM tasks
faces while assigning higher density to details, facilitating to different agents. However, this introduces additional
scalability and local updates compared to network-based challenges in handling multiple distributed models and
methods.However,similartootherNeRF-styleapproaches, devising efficient strategies to manage overlapping regions
volumetricraysamplingsignificantlyrestrictsitsefficiency. whilepreventingtheoccurrenceofmapfusionartifacts.
Promisingtechniquesincludeexplicitrepresentationsbased Real-Time Constraints. Many of the techniques re-
onthe3DGaussianSplattingparadigm,whichexhibitfaster viewed face challenges in achieving real-time processing,
rendering/optimization compared to previous representa- oftenfailingtomatchthesensorframerate.Thislimitation
tions.However,amongvariouslimitations,theyrelyheavily ismainlyduetothechosenmapdatastructureorthecom-
on initialization and lack control over primitive growth in putationally intensive ray-wise rendering-based optimiza-
unobservedregions. tion, which is especially noticeable in NeRF-style SLAM23
methods.Inparticular,hybridapproachesusinghierarchical [67], where ground truth poses are derived from Bundle-
grids require less GPU memory but exhibit slower runtime Fusion [52], raising concerns about the reliability and gen-
performance. On the other hand, advanced representations eralizability of evaluation results. Furthermore, evaluating
such as hash grids or sparse voxels allow for faster com- renderingperformanceusingtrainingviewsasinputraises
putation, but with higher memory requirements. Finally, valid concerns about the risk of overfitting to specific im-
despite their advantages in fast image rendering, current ages.Weemphasizetheneedtoexplorealternativemethods
3DGS-style methods still struggle to efficiently handle si- for evaluating novel view rendering in the SLAM context,
multaneous tracking and mapping processing, preventing andhighlighttheimportanceofaddressingtheseissuesfor
theireffectiveuseinreal-timeapplications. morerobustresearchoutcomes.
Global Optimization. Implementing LC and global BA AdditionalChallenges.SLAMapproaches,whethertra-
requires significant computational resources, risking per- ditional, deep learning based, or influenced by radiance
formance bottlenecks, especially in real-time applications. fieldrepresentations,facecommonchallenges.Onenotable
Many reviewed frame-to-model methods (e.g., iMap [1], obstacle is the handling of dynamic scenes, which proves
NICE-SLAM[5],etc.)facechallengeswithloopclosureand difficult due to the underlying assumption of a static envi-
global bundle adjustment due to the prohibitive compu- ronment,leadingtoartifactsinthereconstructedsceneand
tational complexity of updating the entire 3D model. In errors in the tracking process. While some approaches at-
contrast, frame-to-frame techniques (e.g., GO-SLAM [74], tempttoaddressthisissue,thereisstillsignificantroomfor
etc.) facilitate global correction by executing the global BA improvement,especiallyinhighlydynamicenvironments.
inabackgroundthread,whichsignificantlyimprovestrack- Another challenge is sensitivity to sensor noise, which
ingaccuracy, asdemonstratedin thereportedexperiments, includes motion blur, depth noise, and aggressive rotation,
althoughataslowercomputationalspeedcomparedtoreal- all of which affect tracking and mapping accuracy. This
time rates. For both approaches, the computational cost is is further compounded by the presence of non-Lambertian
largelyduetothelackofflexibilityoflatentfeaturegridsto objects in the scene, such as glass or metal surfaces, which
accommodate pose corrections from loop closures. Indeed, introduce additional complexity due to their varying re-
this requires re-allocating feature grids and retraining the flective properties. In the context of these challenges, it is
entiremap oncealoop iscorrectedand posesareupdated. noteworthy that many approaches often overlook explicit
However, this challenge becomes more pronounced as the uncertainty estimation across input modalities, hindering a
number of frames processed increases, leading to the ac- comprehensiveunderstandingofsystemreliability.
cumulation of camera drift errors and eventually either an Additionally, the absence of external sensors, especially
inconsistent 3D reconstruction or a rapid collapse of the depth information, poses a fundamental problem to RGB-
reconstructionprocess. onlySLAM,leadingtodepthambiguityand3Dreconstruc-
NeRF vs. 3DGS in SLAM. NeRF-style SLAM, which tionoptimizationconvergenceissues.
relies mostly on MLP(s), is well suited for novel view Alesscriticalbutspecificissueisthequalityofrendered
synthesis, mapping and tracking but faces challenges such images of the scene. Reviewed techniques often struggle
as oversmoothing, susceptibility to catastrophic forgetting, with view-dependent appearance elements, such as specular
and computational inefficiency due to its reliance on per- reflections,duetothelackofmodelingofviewdirectionsin
pixel ray marching. 3DGS bypasses per-pixel ray marching themodel,whichaffectsrenderingquality.
and exploits sparsity through differentiable rasterization
over primitives. This benefits SLAM with an explicit vol-
umetric representation, fast rendering, rich optimization, 6 CONCLUSION
direct gradient flow, increased map capacity, and explicit
spatialextentcontrol.Thus,whileNeRFshowsaremarkable In summary, this overview pioneers the exploration of
abilitytosynthesizenovelviews,itsslowtrainingspeedand SLAM methods influenced by recent advances in radiance
difficulty in adapting to SLAM are significant drawbacks. field representations. Ranging from seminal works such as
3DGS, with its efficient rendering, explicit representation, iMap[1]tothelatestadvances,thereviewrevealsasubstan-
andrichoptimizationcapabilities,emergesasapowerfulal- tial body of literature that has emerged in just three years.
ternative.Despiteitsadvantages,current3DGS-styleSLAM Throughstructuredclassificationandanalysis,ithighlights
approacheshavelimitations.Theseincludescalabilityissues keylimitationsandinnovations,providingvaluableinsights
for large scenes, the lack of a direct mesh extraction algo- withcomparativeresultsacrosstracking,mapping,andren-
rithm (although recent methods such as [160] have been dering.Italsoidentifiescurrentopenchallenges,providing
proposed), the inability to accurately encode precise ge- interestingavenuesforfutureexploration.
ometry and, among others, the potential for uncontrollable As a result, this survey is intended to serve as an
Gaussiangrowthintounobservedareas,causingartifactsin essential guide for both novices and seasoned experts, es-
renderedviewsandtheunderlying3Dstructure. tablishingitselfasacomprehensivereferenceinthisrapidly
Evaluation Inconsistencies. The lack of standardized evolvingfield.
benchmarks or online servers with well-defined evalua-
tion protocols results in inconsistent evaluation methods,
making it difficult to conduct fair comparisons between
REFERENCES
approaches and introducing inconsistencies within the
[1] E.Sucar,S.Liu,J.Ortiz,andA.J.Davison,“imap:Implicitmap-
methodologies presented in different research papers. This
pingandpositioninginreal-time,”inProceedingsoftheIEEE/CVF
is exemplified by challenges in datasets such as ScanNet InternationalConferenceonComputerVision,2021,pp.6229–6238.24
[2] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux, D. Kim, [24] S. Mokssit, D. B. Licea, B. Guermah, and M. Ghogho, “Deep
A.J.Davison,P.Kohi,J.Shotton,S.Hodges,andA.Fitzgibbon, learning techniques for visual slam: A survey,” IEEE Access,
“Kinectfusion: Real-time dense surfacemapping and tracking,” vol.11,pp.20026–20050,2023.
inIEEEISMAR. IEEE,2011,pp.127–136. [25] R. A. Newcombe, S. J. Lovegrove, and A. J. Davison, “Dtam:
[3] K.Tateno,F.Tombari,I.Laina,andN.Navab,“Cnn-slam:Real- Densetrackingandmappinginreal-time,”in2011international
time dense monocular slam with learned depth prediction,” in conferenceoncomputervision. IEEE,2011,pp.2320–2327.
Proceedings of the IEEE conference on computer vision and pattern [26] R.F.Salas-Moreno,R.A.Newcombe,H.Strasdat,P.H.Kelly,and
recognition,2017,pp.6243–6252. A.J.Davison,“Slam++:Simultaneouslocalisationandmapping
[4] Y.Li,N.Brasch,Y.Wang,N.Navab,andF.Tombari,“Structure- at the level of objects,” in Proceedings of the IEEE conference on
slam:Low-driftmonocularslaminindoorenvironments,”IEEE computervisionandpatternrecognition,2013,pp.1352–1359.
RoboticsandAutomationLetters,vol.5,no.4,pp.6583–6590,2020. [27] T. Whelan, S. Leutenegger, R. Salas-Moreno, B. Glocker, and
[5] Z.Zhu,S.Peng,V.Larsson,W.Xu,H.Bao,Z.Cui,M.R.Oswald, A. Davison, “Elasticfusion: Dense slam without a pose graph.”
andM.Pollefeys,“Nice-slam:Neuralimplicitscalableencoding Robotics:ScienceandSystems,2015.
forslam,”inProceedingsoftheIEEE/CVFConferenceonComputer [28] Z.TeedandJ.Deng,“Droid-slam:Deepvisualslamformonoc-
VisionandPatternRecognition,2022,pp.12786–12796. ular, stereo, and rgb-d cameras,” Advances in neural information
[6] E.Kruzhkov,A.Savinykh,P.Karpyshev,M.Kurenkov,E.Yudin, processingsystems,vol.34,pp.16558–16569,2021.
A.Potapov,andD.Tsetserukou,“Meslam:Memoryefficientslam [29] Q.-Y.Zhou,S.Miller,andV.Koltun,“Elasticfragmentsfordense
basedonneuralfields,”in2022IEEEInternationalConferenceon scene reconstruction,” in Proceedings of the IEEE International
Systems,Man,andCybernetics(SMC). IEEE,2022,pp.430–435. ConferenceonComputerVision,2013,pp.473–480.
[7] S. Zhi, E. Sucar, A. Mouton, I. Haughton, T. Laidlow, and A. J. [30] T. Schops, T. Sattler, and M. Pollefeys, “Bad slam: Bundle ad-
Davison,“ilabel:Revealingobjectsinneuralfields,”IEEERobotics justeddirectrgb-dslam,”inProceedingsoftheIEEE/CVFConfer-
andAutomationLetters,vol.8,no.2,pp.832–839,2022. enceonComputerVisionandPatternRecognition,2019,pp.134–144.
[8] T.Hua,H.Bai,Z.Cao,M.Liu,D.Tao,andL.Wang,“Hi-map:
[31] M. Nießner, M. Zollho¨fer, S. Izadi, and M. Stamminger, “Real-
Hierarchicalfactorizedradiancefieldforhigh-fidelitymonocular
time3dreconstructionatscaleusingvoxelhashing,”ACMTrans-
densemapping,”arXivpreprintarXiv:2401.03203,2024.
actionsonGraphics(ToG),vol.32,no.6,pp.1–11,2013.
[9] M.Li,J.He,G.Jiang,andH.Wang,“Ddn-slam:Real-timedense
[32] F. Steinbrucker, C. Kerl, and D. Cremers, “Large-scale multi-
dynamic neural implicit slam with joint semantic encoding,”
resolutionsurfacereconstructionfromrgb-dsequences,”inPro-
arXivpreprintarXiv:2401.01545,2024.
ceedings of the IEEE International Conference on Computer Vision,
[10] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “Orb-slam: a
2013,pp.3264–3271.
versatileandaccuratemonocularslamsystem,”IEEEtransactions
[33] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ra-
onrobotics,vol.31,no.5,pp.1147–1163,2015.
mamoorthi, and R. Ng, “Nerf: Representing scenes as neural
[11] M. Bloesch, J. Czarnowski, R. Clark, S. Leutenegger, and A. J.
radiancefieldsforviewsynthesis,”CommunicationsoftheACM,
Davison,“Codeslam—learningacompact,optimisablerepresen-
vol.65,no.1,pp.99–106,2021.
tationfordensevisualslam,”inProceedingsoftheIEEEconference
[34] B. Kerbl, G. Kopanas, T. Leimku¨hler, and G. Drettakis, “3d
oncomputervisionandpatternrecognition,2018,pp.2560–2568.
gaussiansplattingforreal-timeradiancefieldrendering,”ACM
[12] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, and X. Li,
TransactionsonGraphics,vol.42,no.4,2023.
“Gs-slam:Densevisualslamwith3dgaussiansplatting,”arXiv
[35] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and
preprintarXiv:2311.11700,2023.
A. Geiger, “Occupancy networks: Learning 3d reconstruction
[13] C.Ruan,Q.Zang,K.Zhang,andK.Huang,“Dn-slam:Avisual
in function space,” in Proceedings of the IEEE/CVF conference on
slam with orb features and nerf mapping in dynamic environ-
computervisionandpatternrecognition,2019,pp.4460–4470.
ments,”IEEESensorsJournal,2023.
[36] Z.ChenandH.Zhang,“Learningimplicitfieldsforgenerative
[14] D.Qu,C.Yan,D.Wang,J.Yin,D.Xu,B.Zhao,andX.Li,“Implicit
shape modeling,” in Proceedings of the IEEE/CVF Conference on
event-rgbdneuralslam,”arXivpreprintarXiv:2311.11013,2023.
ComputerVisionandPatternRecognition(CVPR),June2019.
[15] J. Deng, Q. Wu, X. Chen, S. Xia, Z. Sun, G. Liu, W. Yu, and
[37] J.J.Park,P.Florence,J.Straub,R.Newcombe,andS.Lovegrove,
L.Pei,“Nerf-loam:Neuralimplicitrepresentationforlarge-scale
“Deepsdf: Learning continuous signed distance functions for
incrementallidarodometryandmapping,”inProceedingsofthe
shape representation,” in Proceedings of the IEEE/CVF conference
IEEE/CVF International Conference on Computer Vision, 2023, pp.
oncomputervisionandpatternrecognition,2019,pp.165–174.
8218–8227.
[38] Y. Xie, T. Takikawa, S. Saito, O. Litany, S. Yan, N. Khan,
[16] H.Li,X.Gu,W.Yuan,L.Yang,Z.Dong,andP.Tan,“Densergb
F. Tombari, J. Tompkin, V. Sitzmann, and S. Sridhar, “Neural
slamwithneuralimplicitmaps,”inProceedingsoftheInternational
fields in visual computing and beyond,” in Computer Graphics
Conference on Learning Representations, 2023. [Online]. Available:
Forum,vol.41,no.2. WileyOnlineLibrary,2022,pp.641–676.
https://openreview.net/forum?id=QUK1ExlbbA
[39] H.Durrant-WhyteandT.Bailey,“Simultaneouslocalizationand
[17] E.Sandstro¨m,K.Ta,L.V.Gool,andM.R.Oswald,“Uncle-SLAM:
Uncertainty learning for dense neural SLAM,” in International mapping: part i,” IEEE robotics & automation magazine, vol. 13,
no.2,pp.99–110,2006.
ConferenceonComputerVisionWorkshops(ICCVW),2023.
[18] Z.Zhu,S.Peng,V.Larsson,Z.Cui,M.R.Oswald,A.Geiger,and [40] T.BaileyandH.Durrant-Whyte,“Simultaneouslocalizationand
M.Pollefeys,“Nicer-slam:Neuralimplicitsceneencodingforrgb mapping (slam): Part ii,” IEEE robotics & automation magazine,
slam,”inInternationalConferenceon3DVision(3DV),March2024. vol.13,no.3,pp.108–117,2006.
[19] G.Grisetti,R.Ku¨mmerle,C.Stachniss,andW.Burgard,“Atuto- [41] S. Saeedi, M. Trentini, M. Seto, and H. Li, “Multiple-robot si-
rialongraph-basedslam,”IEEEIntelligentTransportationSystems multaneouslocalizationandmapping:Areview,”JournalofField
Magazine,vol.2,no.4,pp.31–43,2010. Robotics,vol.33,no.1,pp.3–46,2016.
[20] K. Yousif, A. Bab-Hadiashar, and R. Hoseinnezhad, “An [42] M. R. U. Saputra, A. Markham, and N. Trigoni, “Visual slam
overview to visual odometry and visual slam: Applications to andstructurefrommotionindynamicenvironments:Asurvey,”
mobile robotics,” Intelligent Industrial Systems, vol. 1, no. 4, pp. ACMComputingSurveys(CSUR),vol.51,no.2,pp.1–36,2018.
289–311,2015. [43] M. Zaffar, S. Ehsan, R. Stolkin, and K. M. Maier, “Sensors,
[21] C. Cadena, L. Carlone, H. Carrillo, Y. Latif, D. Scaramuzza, slam and long-term autonomy: A review,” in 2018 NASA/ESA
J. Neira, I. Reid, and J. J. Leonard, “Past, present, and future ConferenceonAdaptiveHardwareandSystems(AHS). IEEE,2018,
of simultaneous localization and mapping: Toward the robust- pp.285–290.
perceptionage,”IEEETransactionsonrobotics,vol.32,no.6,pp. [44] J. Yang, Y. Li, L. Cao, Y. Jiang, L. Sun, and Q. Xie, “A survey
1309–1332,2016. ofslamresearchbasedonlidarsensors,”InternationalJournalof
[22] T.Taketomi,H.Uchiyama,andS.Ikeda,“Visualslamalgorithms: Sensors,vol.1,no.1,p.1003,2019.
Asurveyfrom2010to2016,”IPSJTransactionsonComputerVision [45] W. Zhao, T. He, A. Y. M. Sani, and T. Yao, “Review of slam
andApplications,vol.9,no.1,pp.1–11,2017. techniquesforautonomousunderwatervehicles,”inProceedings
[23] C. Duan, S. Junginger, J. Huang, K. Jin, and K. Thurow, “Deep of the 2019 International Conference on Robotics, Intelligent Control
learning for visual slam in transportation robotics: A review,” andArtificialIntelligence,2019,pp.384–389.
Transportation Safety and Environment, vol. 1, no. 3, pp. 177–184, [46] C. Chen, B. Wang, C. X. Lu, N. Trigoni, and A. Markham,
2019. “A survey on deep learning for localization and mapping: To-25
wards the age of spatial machine intelligence,” arXiv preprint [67] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and
arXiv:2006.12567,2020. M. Nießner, “Scannet: Richly-annotated 3d reconstructions of
[47] W. Chen, G. Shang, A. Ji, C. Zhou, X. Wang, C. Xu, Z. Li, and indoor scenes,” in Proceedings of the IEEE conference on computer
K.Hu,“Anoverviewonvisualslam:Fromtraditiontosemantic,” visionandpatternrecognition,2017,pp.5828–5839.
RemoteSensing,vol.14,no.13,p.3010,2022. [68] J.Straub,T.Whelan,L.Ma,Y.Chen,E.Wijmans,S.Green,J.J.
[48] I.A.Kazerouni,L.Fitzgerald,G.Dooly,andD.Toal,“Asurveyof Engel, R. Mur-Artal, C. Ren, S. Verma, A. Clarkson, M. Yan,
state-of-the-artonvisualslam,”ExpertSystemswithApplications, B. Budge, Y. Yan, X. Pan, J. Yon, Y. Zou, K. Leon, N. Carter,
vol.205,p.117734,2022. J. Briales, T. Gillingham, E. Mueggler, L. Pesqueira, M. Savva,
[49] Y.Tang,C.Zhao,J.Wang,C.Zhang,Q.Sun,W.X.Zheng,W.Du, D.Batra,H.M.Strasdat,R.D.Nardi,M.Goesele,S.Lovegrove,
F.Qian,andJ.Kurths,“Perceptionandnavigationinautonomous and R. Newcombe, “The Replica dataset: A digital replica of
systems in the era of learning: A survey,” IEEE Transactions on indoorspaces,”arXivpreprintarXiv:1906.05797,2019.
NeuralNetworksandLearningSystems,2022. [69] A. Geiger, P. Lenz, and R. Urtasun, “Are we ready for au-
[50] M. Zollho¨fer, P. Stotko, A. Go¨rlitz, C. Theobalt, M. Nießner, tonomousdriving?thekittivisionbenchmarksuite,”inConfer-
R.Klein,andA.Kolb,“Stateofthearton3dreconstructionwith enceonComputerVisionandPatternRecognition(CVPR),2012.
rgb-dcameras,”inComputergraphicsforum,vol.37,no.2. Wiley [70] M.Ramezani,Y.Wang,M.Camurri,D.Wisth,M.Mattamala,and
OnlineLibrary,2018,pp.625–652. M. Fallon, “The newer college dataset: Handheld lidar, inertial
[51] J. A. Placed, J. Strader, H. Carrillo, N. Atanasov, V. Indelman, and vision with ground truth,” in 2020 IEEE/RSJ International
L.Carlone,andJ.A.Castellanos,“Asurveyonactivesimultane-
ConferenceonIntelligentRobotsandSystems(IROS). IEEE,2020,
pp.4353–4360.
ouslocalizationandmapping:Stateoftheartandnewfrontiers,”
IEEETransactionsonRobotics,2023. [71] X. Yang, H. Li, H. Zhai, Y. Ming, Y. Liu, and G. Zhang, “Vox-
fusion: Dense tracking and mapping with voxel-based neural
[52] A. Dai, M. Nießner, M. Zollho¨fer, S. Izadi, and C. Theobalt,
implicitrepresentation,”in2022IEEEInternationalSymposiumon
“Bundlefusion: Real-time globally consistent 3d reconstruction
MixedandAugmentedReality(ISMAR). IEEE,2022,pp.499–507.
usingon-the-flysurfacereintegration,”vol.36,no.4,p.1,2017.
[72] M. M. Johari, C. Carta, and F. Fleuret, “Eslam: Efficient dense
[53] T. Mu¨ller, A. Evans, C. Schied, and A. Keller, “Instant neural
slam system based on hybrid representation of signed distance
graphicsprimitiveswithamultiresolutionhashencoding,”ACM
fields,” in Proceedings of the IEEE/CVF Conference on Computer
Trans.Graph.,vol.41,no.4,pp.102:1–102:15,Jul.2022.[Online].
Vision and Pattern Recognition (CVPR), June 2023, pp. 17408–
Available:https://doi.org/10.1145/3528223.3530127
17419.
[54] Z.Li,T.Mu¨ller,A.Evans,R.H.Taylor,M.Unberath,M.-Y.Liu,
[73] H. Wang, J. Wang, and L. Agapito, “Co-slam: Joint coordinate
andC.-H.Lin,“Neuralangelo:High-fidelityneuralsurfacerecon-
and sparse parametric encodings for neural real-time slam,” in
struction,”inProceedingsoftheIEEE/CVFConferenceonComputer
Proceedings of the IEEE/CVF Conference on Computer Vision and
VisionandPatternRecognition,2023,pp.8456–8465.
PatternRecognition(CVPR),June2023,pp.13293–13302.
[55] S.Peng,M.Niemeyer,L.Mescheder,M.Pollefeys,andA.Geiger,
[74] Y.Zhang,F.Tosi,S.Mattoccia,andM.Poggi,“Go-slam:Global
“Convolutionaloccupancynetworks,”inComputerVision–ECCV
optimizationforconsistent3dinstantreconstruction,”inProceed-
2020:16thEuropeanConference,Glasgow,UK,August23–28,2020,
ingsoftheIEEE/CVFInternationalConferenceonComputerVision,
Proceedings,PartIII16. Springer,2020,pp.523–540.
2023,pp.3727–3737.
[56] Q.Xu,Z.Xu,J.Philip,S.Bi,Z.Shu,K.Sunkavalli,andU.Neu-
[75] Z.TeedandJ.Deng,“Droid-slam:Deepvisualslamformonoc-
mann, “Point-nerf: Point-based neural radiance fields,” in Pro-
ular, stereo, and rgb-d cameras,” NeurIPS, vol. 34, pp. 16558–
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
16569,2021.
Recognition,2022,pp.5438–5448.
[76] E. Sandstro¨m, Y. Li, L. Van Gool, and M. R. Oswald, “Point-
[57] K.Deng,A.Liu,J.-Y.Zhu,andD.Ramanan,“Depth-supervised
slam: Dense neural point cloud-based slam,” in Proceedings of
nerf:Fewerviewsandfastertrainingforfree,”inProceedingsofthe
theIEEE/CVFInternationalConferenceonComputerVision(ICCV),
IEEE/CVFConferenceonComputerVisionandPatternRecognition,
2023.
2022,pp.12882–12891.
[77] L.Xinyang,L.Yijin,T.Yanbin,B.Hujun,Z.Guofeng,Z.Yinda,
[58] B. Roessle, J. T. Barron, B. Mildenhall, P. P. Srinivasan, and andC.Zhaopeng,“Multi-modalneuralradiancefieldformonoc-
M.Nießner,“Densedepthpriorsforneuralradiancefieldsfrom ulardenseslamwithalight-weighttofsensor,”inInternational
sparseinputviews,”inProceedingsoftheIEEE/CVFConferenceon ConferenceonComputerVision(ICCV),2023.
ComputerVisionandPatternRecognition,2022,pp.12892–12901.
[78] P. Hu and Z. Han, “Learning neural implicit through volume
[59] M.Niemeyer,J.T.Barron,B.Mildenhall,M.S.Sajjadi,A.Geiger, rendering with attentive depth fusion priors,” in Advances in
and N. Radwan, “Regnerf: Regularizing neural radiance fields NeuralInformationProcessingSystems(NeurIPS),2023.
for view synthesis from sparse inputs,” in Proceedings of the [79] M.Li,J.He,Y.Wang,andH.Wang,“End-to-endrgb-dslamwith
IEEE/CVFConferenceonComputerVisionandPatternRecognition, multi-mlpsdenseneuralimplicitrepresentations,”IEEERobotics
2022,pp.5480–5490. andAutomationLetters,2023.
[60] K. Gao, Y. Gao, H. He, D. Lu, L. Xu, and J. Li, “Nerf: Neural [80] A.L.Teigen,Y.Park,A.Stahl,andR.Mester,“Rgb-dmapping
radiance field in 3d vision, a comprehensive review,” arXiv and tracking in a plenoxel radiance field,” in Proceedings of the
preprintarXiv:2210.00379,2022. IEEE/CVF Winter Conference on Applications of Computer Vision,
[61] S. Fridovich-Keil, A. Yu, M. Tancik, Q. Chen, B. Recht, and 2024,pp.3342–3351.
A. Kanazawa, “Plenoxels: Radiance fields without neural net- [81] H. Wang, Y. Cao, X. Wei, Y. Shou, L. Shen, Z. Xu, and K. Ren,
works,” in Proceedings of the IEEE/CVF Conference on Computer “Structerf-slam: Neural implicit representation slam for struc-
VisionandPatternRecognition,2022,pp.5501–5510. turalenvironments,”Computers&Graphics,p.103893,2024.
[62] W. E. Lorensen and H. E. Cline, “Marching cubes: A high res- [82] R. Mur-Artal and J. D. Tardo´s, “Orb-slam2: An open-source
olution 3d surface construction algorithm,” in Seminal graphics: slam system for monocular, stereo, and rgb-d cameras,” IEEE
pioneeringeffortsthatshapedthefield,1998,pp.347–353. transactionsonrobotics,vol.33,no.5,pp.1255–1262,2017.
[63] P. Wang, L. Liu, Y. Liu, C. Theobalt, T. Komura, and W. Wang, [83] P. F. Felzenszwalb and D. P. Huttenlocher, “Efficient graph-
“Neus: Learning neural implicit surfaces by volume rendering based image segmentation,” International journal of computer vi-
for multi-view reconstruction,” arXiv preprint arXiv:2106.10689, sion,vol.59,pp.167–181,2004.
2021. [84] Y.Ming,W.Ye,andA.Calway,“idf-slam:End-to-endrgb-dslam
[64] D.Azinovic´,R.Martin-Brualla,D.B.Goldman,M.Nießner,and withneuralimplicitmappinganddeepfeaturetracking,”arXiv
J.Thies,“Neuralrgb-dsurfacereconstruction,”inProceedingsof preprintarXiv:2209.07919,2022.
theIEEE/CVFConferenceonComputerVisionandPatternRecogni- [85] M.ElBanani,L.Gao,andJ.Johnson,“Unsupervisedr&r:Unsu-
tion,2022,pp.6290–6301. pervised point cloud registration via differentiable rendering,”
[65] G.ChenandW.Wang,“Asurveyon3dgaussiansplatting,”arXiv inProceedingsoftheIEEE/CVFConferenceonComputerVisionand
preprintarXiv:2401.03890,2024. PatternRecognition,2021,pp.7129–7139.
[66] J.Sturm,N.Engelhard,F.Endres,W.Burgard,andD.Cremers, [86] W. Guo, B. Wang, and L. Chen, “Neuv-slam: Fast neural mul-
“Abenchmarkfortheevaluationofrgb-dslamsystems,”in2012 tiresolutionvoxeloptimizationforrgbddenseslam,”2024.
IEEE/RSJ international conference on intelligent robots and systems. [87] H. Huang, L. Li, H. Cheng, and S.-K. Yeung, “Photo-slam:
IEEE,2012,pp.573–580. Real-time simultaneous localization and photorealistic map-26
ping for monocular, stereo, and rgb-d cameras,” arXiv preprint [111] A.Kirillov,E.Mintun,N.Ravi,H.Mao,C.Rolland,L.Gustafson,
arXiv:2311.16728,2023. T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo et al., “Segment
[88] C. Campos, R. Elvira, J. J. G. Rodr´ıguez, J. M. Montiel, and anything,”arXivpreprintarXiv:2304.02643,2023.
J. D. Tardo´s, “Orb-slam3: An accurate open-source library for [112] M. A. Karaoglu, H. Schieber, N. Schischka, M. Go¨rgu¨lu¨,
visual,visual–inertial,andmultimapslam,”IEEETransactionson F. Gro¨tzner, A. Ladikos, D. Roth, N. Navab, and B. Busam,
Robotics,vol.37,no.6,pp.1874–1890,2021. “Dynamon:Motion-awarefastandrobustcameralocalizationfor
[89] N.Keetha,J.Karhade,K.M.Jatavallabhula,G.Yang,S.Scherer, dynamicnerf,”arXivpreprintarXiv:2309.08927,2023.
D. Ramanan, and J. Luiten, “Splatam: Splat, track & map 3d [113] L.-C.Chen,G.Papandreou,F.Schroff,andH.Adam,“Rethink-
gaussiansfordensergb-dslam,”arXivpreprintarXiv:2312.02126, ingatrousconvolutionforsemanticimagesegmentation,”arXiv
2023. preprintarXiv:1706.05587,2017.
[90] H.Matsuki,R.Murai,P.H.Kelly,andA.J.Davison,“Gaussian [114] S.F.Bhat,R.Birkl,D.Wofk,P.Wonka,andM.Mu¨ller,“Zoedepth:
splattingslam,”arXivpreprintarXiv:2312.06741,2023. Zero-shottransferbycombiningrelativeandmetricdepth,”arXiv
[91] V. Yugay, Y. Li, T. Gevers, and M. R. Oswald, “Gaussian-slam: preprintarXiv:2302.12288,2023.
Photo-realisticdenseslamwithgaussiansplatting,”arXivpreprint [115] Z. Xu, J. Niu, Q. Li, T. Ren, and C. Chen, “Nid-slam: Neural
arXiv:2312.10070,2023. implicit representation-based rgb-d slam in dynamic environ-
[92] J.Park,Q.-Y.Zhou,andV.Koltun,“Coloredpointcloudregistra- ments,”arXivpreprintarXiv:2401.01189,2024.
tion revisited,” in Proceedings of the IEEE international conference [116] D. Lisus, C. Holmes, and S. Waslander, “Towards open world
oncomputervision,2017,pp.143–152. nerf-based slam,” in 2023 20th Conference on Robots and Vision
[93] J. Hu, M. Mao, H. Bao, G. Zhang, and Z. Cui, “CP-SLAM: (CRV),2023,pp.37–44.
Collaborative neural point-based SLAM system,” in Thirty- [117] C.-M. Chung, Y.-C. Tseng, Y.-C. Hsu, X.-Q. Shi, Y.-H. Hua,
seventh Conference on Neural Information Processing Systems, J.-F. Yeh, W.-C. Chen, Y.-T. Chen, and W. H. Hsu, “Orbeez-
2023. [Online]. Available: https://openreview.net/forum?id= slam:Areal-timemonocularvisualslamwithorbfeaturesand
dFSeZm6dTC nerf-realizedmapping,”in2023IEEEInternationalConferenceon
[94] B.Xiang,Y.Sun,Z.Xie,X.Yang,andY.Wang,“Nisb-map:Scal- RoboticsandAutomation(ICRA). IEEE,2023,pp.9400–9406.
ablemappingwithneuralimplicitspatialblock,”IEEERobotics [118] T. Hua, H. Bai, Z. Cao, and L. Wang, “Fmapping: Factorized
andAutomationLetters,2023. efficientneuralfieldmappingforreal-timedensergbslam,”arXiv
[95] S.LiuandJ.Zhu,“Efficientmapfusionformultipleimplicitslam preprintarXiv:2306.00579,2023.
agents,”IEEETransactionsonIntelligentVehicles,2023.
[119] J. Lin, A. Nachkov, S. Peng, L. Van Gool, and D. P. Paudel,
[96] Y. Tang, J. Zhang, Z. Yu, H. Wang, and K. Xu, “Mips-fusion: “Ternary-type opacity and hybrid odometry for rgb-only nerf-
Multi-implicit-submapsforscalableandrobustonlineneuralrgb- slam,”arXivpreprintarXiv:2312.13332,2023.
d reconstruction,” ACM Transactions on Graphics (TOG), vol. 42,
[120] H. Matsuki, E. Sucar, T. Laidow, K. Wada, R. Scona, and A. J.
no.6,pp.1–16,2023.
Davison,“imode:Real-timeincrementalmonoculardensemap-
[97] H.Matsuki,K.Tateno,M.Niemeyer,andF.Tombari,“Newton:
pingusingneuralfield,”in2023IEEEInternationalConferenceon
Neural view-centric mapping for on-the-fly large-scale slam,”
RoboticsandAutomation(ICRA). IEEE,2023,pp.4171–4177.
arXivpreprintarXiv:2303.13654,2023.
[121] F.MaandS.Karaman,“Sparse-to-dense:Depthpredictionfrom
[98] Y.Mao,X.Yu,K.Wang,Y.Wang,R.Xiong,andY.Liao,“Ngel-
sparsedepthsamplesandasingleimage,”2018.
slam:Neuralimplicitrepresentation-basedglobalconsistentlow-
[122] W.Zhang,T.Sun,S.Wang,Q.Cheng,andN.Haala,“Hi-slam:
latencyslamsystem,”arXivpreprintarXiv:2311.09525,2023.
Monocularreal-timedensemappingwithhybridimplicitfields,”
[99] T. Deng, G. Shen, T. Qin, J. Wang, W. Zhao, J. Wang, D. Wang,
IEEERoboticsandAutomationLetters,2023.
and W. Chen, “Plgslam: Progressive neural scene represena-
[123] A.Eftekhar,A.Sax,J.Malik,andA.Zamir,“Omnidata:Ascalable
tion with local to global bundle adjustment,” arXiv preprint
pipelineformakingmulti-taskmid-levelvisiondatasetsfrom3d
arXiv:2312.09866,2023.
scans,”inProceedingsoftheIEEE/CVFInternationalConferenceon
[100] L. Liso, E. Sandstro¨m, V. Yugay, L. V. Gool, and M. R. Os-
ComputerVision,2021,pp.10786–10796.
wald, “Loopy-slam: Loopy-slam: Dense neural slam with loop
[124] H. Xu, J. Zhang, J. Cai, H. Rezatofighi, and D. Tao, “Gmflow:
closures,”arXivpreprintarXiv:2402.09944,2024.
Learningopticalflowviaglobalmatching,”inProceedingsofthe
[101] K. Mazur, E. Sucar, and A. J. Davison, “Feature-realistic neural
IEEE/CVFConferenceonComputerVisionandPatternRecognition,
fusionforreal-time,opensetsceneunderstanding,”in2023IEEE
2022,pp.8121–8130.
InternationalConferenceonRoboticsandAutomation(ICRA). IEEE,
[125] J.Naumann,B.Xu,S.Leutenegger,andX.Zuo,“Nerf-vo:Real-
2023,pp.8201–8207.
timesparsevisualodometrywithneuralradiancefields,”arXiv
[102] M. Tan and Q. Le, “Efficientnet: Rethinking model scaling for
preprintarXiv:2312.13471,2023.
convolutional neural networks,” in International conference on
[126] Z.Teed,L.Lipson,andJ.Deng,“Deeppatchvisualodometry,”
machinelearning. PMLR,2019,pp.6105–6114.
arXivpreprintarXiv:2208.04726,2022.
[103] M.Caron,H.Touvron,I.Misra,H.Je´gou,J.Mairal,P.Bojanowski,
and A. Joulin, “Emerging properties in self-supervised vision [127] H.Zhou,Z.Guo,S.Liu,L.Zhang,Q.Wang,Y.Ren,andM.Li,
transformers,” in Proceedings of the IEEE/CVF international con- “Mod-slam:Monoculardensemappingforunbounded3dscene
ferenceoncomputervision,2021,pp.9650–9660. reconstruction,”2024.
[104] X.Kong,S.Liu,M.Taher,andA.J.Davison,“vmap:Vectorised [128] R. Ranftl, A. Bochkovskiy, and V. Koltun, “Vision transformers
object mapping for neural field slam,” in Proceedings of the fordenseprediction,”ICCV,2021.
IEEE/CVF Conference on Computer Vision and Pattern Recognition [129] X.Han,H.Liu,Y.Ding,andL.Yang,“Ro-map:Real-timemulti-
(CVPR),June2023,pp.952–961. object mapping with neural radiance fields,” IEEE Robotics and
[105] Y. Haghighi, S. Kumar, J. P. Thiran, and L. Van Gool, “Neural AutomationLetters,vol.8,no.9,pp.5950–5957,2023.
implicit dense semantic slam,” arXiv preprint arXiv:2304.14560, [130] A.Rosinol,J.J.Leonard,andL.Carlone,“Nerf-slam:Real-time
2023. dense monocular slam with neural radiance fields,” in 2023
[106] B.Cheng,A.G.Schwing,andA.Kirillov,“Per-pixelclassification IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems
isnotallyouneedforsemanticsegmentation,”2021. (IROS). IEEE,2023,pp.3437–3444.
[107] S. Zhu, G. Wang, H. Blum, J. Liu, L. Song, M. Pollefeys, [131] S. Isaacson, P.-C. Kung, M. Ramanagopal, R. Vasudevan, and
and H. Wang, “Sni-slam: Semantic neural implicit slam,” arXiv K.A.Skinner,“Loner:Lidaronlyneuralrepresentationsforreal-
preprintarXiv:2311.11016,2023. timeslam,”IEEERoboticsandAutomationLetters,2023.
[108] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, [132] S. Rusinkiewicz and M. Levoy, “Efficient variants of the icp
V.Khalidov,P.Fernandez,D.Haziza,F.Massa,A.El-Noubyetal., algorithm,” in Proceedings third international conference on 3-D
“Dinov2: Learningrobust visualfeatures withoutsupervision,” digitalimagingandmodeling. IEEE,2001,pp.145–152.
arXivpreprintarXiv:2304.07193,2023. [133] Y. Pan, X. Zhong, L. Wiesmann, T. Posewsky, J. Behley, and
[109] K.Li,M.Niemeyer,N.Navab,andF.Tombari,“Dnsslam:Dense C.Stachniss,“Pin-slam:Lidarslamusingapoint-basedimplicit
neuralsemantic-informedslam,”arXivpreprintarXiv:2312.00204, neural representation for achieving global map consistency,”
2023. arXivpreprintarXiv:2401.09101,2024.
[110] M.Li,S.Liu,andH.Zhou,“Sgs-slam:Semanticgaussiansplat- [134] S.Hong,J.He,X.Zheng,H.Wang,H.Fang,K.Liu,C.Zheng,and
tingforneuraldenseslam,”2024. S.Shen,“Liv-gaussmap:Lidar-inertial-visualfusionforreal-time27
3dradiancefieldmaprendering,”arXivpreprintarXiv:2401.14857, [156] M.Yokozuka,K.Koide,S.Oishi,andA.Banno,“Litamin2:Ultra
2024. light lidar-based slam using geometric approximation applied
[135] M. Burri, J. Nikolic, P. Gohl, T. Schneider, J. Rehder, S. Omari, with kl-divergence,” in 2021 IEEE International Conference on
M.W.Achtelik,andR.Siegwart,“Theeurocmicroaerialvehicle RoboticsandAutomation(ICRA). IEEE,2021,pp.11619–11625.
datasets,” The International Journal of Robotics Research, vol. 35, [157] J.BehleyandC.Stachniss,“Efficientsurfel-basedslamusing3d
no.10,pp.1157–1163,2016. laserrangedatainurbanenvironments.”inRobotics:Scienceand
[136] B.Glocker,S.Izadi,J.Shotton,andA.Criminisi,“Real-timergb- Systems,vol.2018,2018,p.59.
dcamerarelocalization,”in2013IEEEInternationalSymposiumon [158] J. Ruan, B. Li, Y. Wang, and Y. Sun, “Slamesh: Real-time
MixedandAugmentedReality(ISMAR). IEEE,2013,pp.173–179. lidar simultaneous localization and meshing,” arXiv preprint
[137] C. Yeshwanth, Y.-C. Liu, M. Nießner, and A. Dai, “Scannet++: arXiv:2303.05252,2023.
Ahigh-fidelitydatasetof3dindoorscenes,”inProceedingsofthe [159] X.Liu,Z.Liu,F.Kong,andF.Zhang,“Large-scalelidarconsis-
IEEE/CVF International Conference on Computer Vision, 2023, pp. tentmappingusinghierarchicallidarbundleadjustment,”IEEE
12–22. RoboticsandAutomationLetters,vol.8,no.3,pp.1523–1530,2023.
[160] A. Gue´don and V. Lepetit, “Sugar: Surface-aligned gaussian
[138] Y. Liu, Y. Fu, F. Chen, B. Goossens, W. Tao, and H. Zhao,
splatting for efficient 3d mesh reconstruction and high-quality
“Simultaneouslocalizationandmappingrelateddatasets:Acom-
meshrendering,”arXivpreprintarXiv:2311.12775,2023.
prehensivesurvey,”arXivpreprintarXiv:2102.04036,2021.
[139] Z.Wang,A.C.Bovik,H.R.Sheikh,andE.P.Simoncelli,“Image
qualityassessment:fromerrorvisibilitytostructuralsimilarity,”
IEEEtransactionsonimageprocessing,vol.13,no.4,pp.600–612,
2004.
[140] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang,
“Theunreasonableeffectivenessofdeepfeaturesasaperceptual
metric,”inProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,2018,pp.586–595.
[141] T.Mu¨ller,B.McWilliams,F.Rousselle,M.Gross,andJ.Nova´k,
“Neural importance sampling,” ACM Transactions on Graphics
(ToG),vol.38,no.5,pp.1–19,2019.
[142] E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “Orb: An
efficientalternativetosiftorsurf,”in2011Internationalconference
oncomputervision. Ieee,2011,pp.2564–2571.
[143] R. Arandjelovic, P. Gronat, A. Torii, T. Pajdla, and J. Sivic,
“Netvlad: Cnn architecture for weakly supervised place recog-
nition,”inProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,2016,pp.5297–5307.
[144] B. Cheng, I. Misra, A. G. Schwing, A. Kirillov, and R. Girdhar,
“Masked-attention mask transformer for universal image seg-
mentation,”inProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,2022,pp.1290–1299.
[145] A. Cao and J. Johnson, “Hexplane: A fast representation for
dynamic scenes,” in Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition,2023,pp.130–141.
[146] R.Mur-ArtalandJ.D.Tardo´s,“Probabilisticsemi-densemapping
fromhighlyaccuratefeature-basedmonocularslam.”inRobotics:
ScienceandSystems,vol.2015. Rome,2015.
[147] A.Gropp,L.Yariv,N.Haim,M.Atzmon,andY.Lipman,“Im-
plicitgeometricregularizationforlearningshapes,”arXivpreprint
arXiv:2002.10099,2020.
[148] R. Ranftl, K. Lasinger, D. Hafner, K. Schindler, and V. Koltun,
“Towards robust monocular depth estimation: Mixing datasets
forzero-shotcross-datasettransfer,”IEEETransactionsonPattern
AnalysisandMachineIntelligence,vol.44,no.3,2022.
[149] M.Tancik,E.Weber,E.Ng,R.Li,B.Yi,T.Wang,A.Kristoffersen,
J. Austin, K. Salahi, A. Ahuja et al., “Nerfstudio: A modular
framework for neural radiance field development,” in ACM
SIGGRAPH2023ConferenceProceedings,2023,pp.1–12.
[150] J. Lin, “Divergence measures based on the shannon entropy,”
IEEETransactionsonInformationtheory,vol.37,no.1,pp.145–151,
1991.
[151] C. Yan, D. Qu, D. Wang, D. Xu, Z. Wang, B. Zhao, and X. Li,
“Gs-slam:Densevisualslamwith3dgaussiansplatting,”arXiv
preprintarXiv:2311.11700,2023.
[152] J. L. Scho¨nberger, E. Zheng, M. Pollefeys, and J.-M. Frahm,
“Pixelwise view selection for unstructured multi-view stereo,”
inEuropeanConferenceonComputerVision(ECCV),2016.
[153] A. Zeng, S. Song, M. Nießner, M. Fisher, J. Xiao, and
T. Funkhouser, “3dmatch: Learning local geometric descriptors
fromrgb-dreconstructions,”inProceedingsoftheIEEEconference
oncomputervisionandpatternrecognition,2017,pp.1802–1811.
[154] Y. Pan, P. Xiao, Y. He, Z. Shao, and Z. Li, “Mulls: Versatile
lidar slam via multi-metric linear least square,” in 2021 IEEE
InternationalConferenceonRoboticsandAutomation(ICRA). IEEE,
2021,pp.11633–11640.
[155] P. Dellenbach, J.-E. Deschaud, B. Jacquet, and F. Goulette, “Ct-
icp:Real-timeelasticlidarodometrywithloopclosure,”in2022
InternationalConferenceonRoboticsandAutomation(ICRA). IEEE,
2022,pp.5580–5586.