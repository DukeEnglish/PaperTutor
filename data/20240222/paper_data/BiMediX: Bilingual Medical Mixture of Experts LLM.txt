BiMediX: Bilingual Medical Mixture of Experts LLM
SaraPieri*,SahalShajiMullappilly*,FahadShahbazKhan,
RaoMuhammadAnwer,SalmanKhan,TimothyBaldwin,HishamCholakkal
MohamedBinZayedUniversityofArtificialIntelligence(MBZUAI)
Abstract
Hello BiMediX, I've been experiencing recurring headaches for
the past few weeks, and they seem to be getting more frequent.
Inthispaper,weintroduceBiMediX,thefirst
I see. Headaches can have various causes. Can you please provide
bilingualmedicalmixtureofexpertsLLMde- more details about your symptoms? For example, where is the
signedforseamlessinteractioninbothEnglish pain located and how would you describe the pain?
andArabic. Ourmodelfacilitatesawiderange
ملأ هنإ .طقف دحاو بناج لىع اًنايحأ ،سأرلا يبناج لىع بلاغلا يف ثدحي مللأا
of medical interactions in English and Ara- أوسأ هنلاعجي امهنأ ودبي ءاضوضلاو ءوضلا .تاعاس ةدعل رمتسي نأ نكمي ضبان
bic,includingmulti-turnchatstoinquireabout
نم يناعت دق كنأ ودبي ،كضارعأ لىإ ادًانتسا .ةلصفملا تامولعملا لىع كل ارًكش
additional details such as patient symptoms لاك وأ دحاو بناج يف ضبان ملأب ةداع زيمتي ةقيقشلا عادص .ةقيقشلا عادص
نأ مهملا نم .اهتدش نم ءاضوضلاو ءوضلا ديزي نأ نكميو ،سأرلا نم نيبناجلا
andmedicalhistory,multiple-choicequestion
ءارجإ لىإ جاتحن دق اذل ،كضارعلأ ةلمتحم ىرخأ اًبابسأ كانه نأ رابتعلاا يف ذخأن
answering, and open-ended question answer- ؟عادصلا هذه ريثت ةددحم لماوع يأ تظحلا له .مييقتلا نم ديزم
ing. We propose a semi-automated English- Yes, I've been under a lot of stress at work lately, and my
sleep hasn't been great.
to-Arabictranslationpipelinewithhumanre-
Stress and inadequate sleep can definitely contribute to headaches.
finement to ensure high-quality translations.
It's important to manage your stress levels and prioritize getting
Wealsointroduceacomprehensiveevaluation enough restful sleep. In the meantime, I can recommend some over-
the-counter pain relievers such as ibuprofen or acetaminophen to
benchmark for Arabic medical LLMs. Fur- help alleviate your symptoms. However, I would strongly advise
scheduling an appointment with your primary care physician for a
thermore,weintroduceBiMed1.3M,anexten-
thorough evaluation and further guidance. They may recommend
sive Arabic-English bilingual instruction set additional tests or treatments based on your specific situation.
covering 1.3 Million diverse medical interac-
Figure1: BiMediXisadeptatconductingprecisemedi-
tions,resultinginover632millionhealthcare
calconversationsseamlesslyinbothEnglishandArabic.
specializedtokensforinstructiontuning. Our
Itdemonstratesadvancedinteractivecapabilities,such
BiMed1.3Mdatasetincludes250ksynthesized
assolicitingfurtherdetailsfromthepatientandseam-
multi-turndoctor-patientchatsandmaintains
lesslyswitchinglanguagesbasedonpatientinputwhile
a1:2Arabic-to-Englishratio. Ourmodelout-
maintainingthecontinuityoftheconversation. TheAra-
performsstate-of-the-artMed42andMeditron
bic text translates as follows: User: The pain occurs
byaverageabsolutegainsof2.5%and4.1%,
mostly on both sides of the head, sometimes on only
respectively,computedacrossmultiplemedical
oneside. Itisathrobbingpainthatcanlastforseveral
evaluation benchmarks in English, while op-
hours.Lightandnoiseseemtomakeitworse.BiMediX:
eratingat8-timesfasterinference. Moreover,
Thankyouforthedetailedinformation. Basedonyour
ourBiMediXoutperformsthegenericArabic-
symptoms,itsoundslikeyoumayhaveamigraine. Mi-
EnglishbilingualLLM,Jais-30B,byaverage
graine headaches are usually characterized by throb-
absolute gains of 10% on our Arabic medi-
bingpainononeorbothsidesoftheheadandcanbe
cal benchmark and 15% on bilingual evalua-
triggeredorincreasedinintensitybylightandnoise. It
tionsacrossmultipledatasets. Ourprojectpage
is important to consider that there are other possible
with source code and trained model is avail-
causesforyoursymptoms,sowemayneedtodofurther
able at https://github.com/mbzuai-oryx/
evaluation. Haveyounoticedanyspecificfactorsthat
BiMediX.
triggertheseheadaches?
1 Introduction
ofapplications. Specifically,LLMshavethepoten-
LargeLanguageModels(LLMs)demonstrateex-
tialtoimpactthemedicalfieldbyofferingvirtual
cellent ability to understand instructions, gener-
chat support across diverse medical departments
atehuman-liketext,andtackleunseenorcomplex
toenhancediagnosticaccuracy. However,despite
tasks, rendering them valuable for a broad range
the promise of LLMs in healthcare, the medical
**Equallycontributingfirstauthors. domainposesdistinctchallenges,includingthene-
1
4202
beF
02
]LC.sc[
1v35231.2042:viXraQuestion: Infections which are transmitted from man to
vertebrateanimals are called? ؟ىمست ةيراقفلا تاناويحلا لىإ ناسنلإا نم لقتنت يتلا ىودعلا :لاؤسلا
Options: A. Anthropozoonosis B. Zooanthroponosis سيزونوبورثنأوز .ب سيزونوزوبورثنأ .أ :تارايخلا
C. Amphixenosis D. Enzootics سكيتوزنإ .د سيسونيسكيفمأ .ج
Infections which are transmitted from man to vertebrate animals ب وه حيحصلا باوجلا
are called Zooanthroponosis. The correct answer is B. .ةيراقفلاتاناويحلا لىإ ناسنلإا نم لقتنت يتلا ىودعلا يه سيزونوبورثنأوز
How does hypertension contribute to the development of heart disease?
؟بلقلا ضارمأ ريوطت يف مدلا طغض عافترا مهاست فيك
Hypertension can lead to several changes in the heart, including left ventricular
hypertrophy (LVH), which is an enlargement of the heart muscle. LVH can occur as ،رسيلأا نيطبلا مخضت لثم ،بلقلا يف تاريغت لىإ مدلا طغض عافترا يدؤي نأ نكمي
a compensatory mechanism to handle the increased workload caused by high blood لماعتلل ةيضيوعت ةيلآك كلذ ثدحي نأ نكمي .بلقلا ةلضع مجح يف ةدايز يه يتلاو
pressure. Over time, this can lead to diastolic dysfunction, which is a decline in the نأ نكمي ،تقولا رورم عم .مدلا طغض عافترا نع مجانلا بلقلا لىع ديازتملا ءبعلا عم
heart's ability to relax and fill with blood during the resting phase of the cardiac بلقلا ةردق يف بارطضا وهو ،طاسبنلاا بارطضا روطت لىإ رسيلأا نيطبلا مخضت دؤي
cycle. Diastolic dysfunction can result in symptoms such as shortness of breath, بارطضا لىجتي نأ نكمي .طاسبنلاا ةلحرم ءانثأ مدلاب ةئبعتلاو ءاخرتسلاا لىع
fatigue, and exercise intolerance. It is important to manage hypertension through نم .ةيندبلا نيرامتلا لمحت مدعو ،بعتلاو ،سفنتلا قيض لثم ضارعأب طاسبنلاا
lifestyle modifications, medication, and regular monitoring to prevent the ةعباتمو ،ةيودلأاو ،ةايحلا طمن يف تلايدعت للاخ نم مدلا طغض عافترا ةرادإ مهملا
development of heart disease and related complications. ةطبترملا ىرخلأا تافعاضملاو بلقلا ضارمأ روطت عنمل ةمظتنم
Figure2: EnglishandArabiclanguagecapabilitiesofourBiMediXwithmultiple-choicequestionsandanswers
(MCQA)onthetoprow,andopen-endedquestionsandresponses(QA)onthebottomrow.
cessity for domain-specific data and expertise to andArabic.
ensureaccurateinterpretations,aswellasconcerns
1.1 Contributions
regardingtransparencyandbias.
Ourcontributionscanbesummarizedasfollows:
Recentmethodshaveemergedtobridgethegap (i)Weintroducethefirstbilingualmedicalmixture
inmedicaldomainknowledgebydevelopingopen- ofexpertsLLM,namedBiMediX,havingseamless
sourcemedicalLLMalternatives(Chenetal.,2023; interactioncapabilitiesinbothEnglishandArabic
Tomaetal.,2023;Wuetal.,2023)toproprietary languages(seeFig.1). Ourmodelfacilitatesvarious
modelslikeChatGPT(Achiametal.,2023). How- medicalinteractions,includingmulti-turnchatses-
ever,amongtheleadingmedicalLLMsinEnglish, sentialforfollow-upinquirieswithhumanpatients
Med42-70B(Christopheetal.,2023)hasnotmade (Fig.1),multiple-choicequestionanswering(Fig.2
itstrainingdataandresourcesavailable. Meditron- toprow)andopen-endedquestionanswering(Fig.2
70B (Chen et al., 2023), relies on separate fine- bottomrow).
tuningforeachevaluationdataset. Thisapproachis (ii)Wedevelopasemi-automatediterativetrans-
likelytocompromisethemodel’sabilitytoserveas lationpipeline,incorporatingahumanverification
aunifiedsolutionwithdiverseinteractioncapabil- stepto ensurehigh-quality translationof English
ities. Moreover,manyoftheleadingopen-source medicaltextintoresource-constrainedArabic. This
medicalLLMs(Christopheetal.,2023;Chenetal., pipelinefacilitatesthecompilationofinstruction-
2023)arelimitedtotheEnglishlanguage,posing tuningdatasetandacomprehensivebenchmarkfor
limitationsintheirabilitytounderstandandinteract evaluating Arabic healthcare LLMs and Arabic-
inresource-constrainedlanguagessuchasArabic. English Bilingual LLMs. The proposed Arabic
DespiteArabic’spotentialtocatertoapopulation evaluation benchmark can play a pivotal role in
ofmorethan400millionpeople,itremainsunder- futureresearchendeavors.
represented in the medical LLM literature. The (iii) We curate a comprehensive Arabic-English
unique features of Arabic, such as its distinctive bilingualinstructionset namedBiMed1.3M com-
scriptandlessconventionalright-to-leftwritingfor- prisingover1.3millioninstructions,resultingina
mat,theunavailabilityoflarge-scalemedicaltrain- totalofover632millionhealthcarespecializedto-
ing data in Arabic, and the lack of a comprehen- kens. Ourdatasetcomprisesopen-endedquestion-
sivebenchmarktoevaluateArabicmedicalLLMs, and-answer, multiple-choice question answering,
present challenges in the development of Arabic andover200ksynthesizedmulti-turnchatsrooted
orEnglish-ArabicbilingualmedicalLLMs. Inthis in authentic medical content. The integration of
work,weintroduceBiMediX,abilingualmedical multi-turnconversationsallowsforthecreationof
mixture of experts LLM with seamless conversa- achatbotcapableofconductingfollow-upinquiries
tionalcapabilitiesinbothEnglishandArabic. Our with human patients. Our BiMed1.3M dataset
BiMediXisachatagentwithstate-of-the-artper- maintainsa1:2Arabic-to-Englishratioacrossdi-
formancesonnon-chatbenchmarksinbothEnglish versemedicalinteractions.
2(iv)Weintroduceaparameter-efficientfine-tuning ties. ClinicalCamel(Tomaetal.,2023)introduced
ofroutingandexpertlayersinMixtral(Jiangetal., questionansweringdatawiththeDialogue-Based
2024) using BiMed1.3M, requiring fewer train- KnowledgeEncoding(DBKE)process,converting
ingresourcescomparedtoMed42andChenetal. PubMedarticlesandMedQAintoquestionsandde-
(Christophe et al., 2023; Chen et al., 2023). Our scriptiveanswers. PMC-LLaMA(Wuetal.,2023)
BiMediX achieves state-of-the-art performance and Meditron (Chen et al., 2023) perform pre-
on multiple medical exam question datasets in trainingonPubMedcontentandmedicaltexts,with
bothEnglishandArabic. Ourmodeloutperforms furtherrefinementsonindividualMCQAdatasets.
Med42 and Meditron by average absolute gains Med42(Christopheetal.,2023)instruction-tuned
of 2.5% and 4.1%, respectively, across multiple LLaMAmodelformedicaltasks,thoughthedetails
medical benchmarks in English, while operating ofitstrainingremainundisclosed.
8 times faster. Additionally, BiMediX surpasses
Recent years have witnessed progress in Ara-
the generic Arabic-English bilingual LLM, Jais-
biclanguageprocessingmodels,includingnotable
30B,byaverageabsolutegainsof10%onArabic
modelssuchasAraT5(Nagoudietal.,2021)and
medicalbenchmarksand15%onbilingualevalua-
AraBART (Eddine et al., 2022). Beyond these
tionsacrossmultiplemedicaldatasets. Moreover,
monolingualframeworks,Arabichasalsobeenin-
bilingualinstructiontuningwithourBiMed1.3M
corporated into multilingual systems (Scao et al.,
leadstoanaverageabsolute10%gainoverthebase
2022;Muennighoffetal.,2022). Therecentlaunch
mixtral(Jiangetal.,2024)onbilingualevaluations.
oftheJaismodel(Senguptaetal.,2023)markeda
substantialadvancementasageneral-purposebilin-
2 RelatedWorks gual LLM for English and Arabic. Additionally,
manyofthelatestfoundationalmodels(Touvron
Proprietarymodels,includingOpenAI’sGPT-3.5
etal.,2023a,b;Jiangetal.,2024)demonstratethe
and GPT-4 (Achiam et al., 2023), as well as
abilitytoreplyinArabic,despitetheirlimitedex-
Google’s Flan-PaLM (Chung et al., 2022), Med-
posuretothelanguageduringpre-training. How-
PaLM (Singhal et al., 2022), and Med-PaLM 2
ever,thesegenericmodels(Senguptaetal.,2023;
(Singhal et al., 2023), have showcased remark-
Touvronetal.,2023a,b;Jiangetal.,2024)lacksuf-
ableachievementsonEnglishmedicalbenchmarks.
ficientmedicaldomainknowledge, makingthem
Nonetheless, the scarcity of public information,
ill-equippedformedicalapplicationsinbothAra-
alongsideconcernsregardingaccessanddatapri-
bicandEnglishlanguages.
vacy,motivatethedevelopmentofopen-sourceal-
ternativestoproprietarymodels. Thesealternatives
aimforcustomization,transparentevaluation,and 3 Method
safe clinical integration. Early open medical lan-
guage models, enhanced BERT-style models for Overview: To develop a bilingual healthcare
medicalpurposes,leveragingthePubMedcorpus chat assistant, we first introduce a comprehen-
foreithercontinuouspre-training(Leeetal.,2020; sivebilingualdatasetnamedBiMed1.3Mthaten-
Huang et al., 2019) or specific domain training compasses diverse medical interactions such as
fromscratch(Beltagyetal.,2019;Guetal.,2021; multiple-choicequestionanswers(MCQAs),open-
Shinetal.,2020). Subsequentresearchenhanced endedquestions,andmulti-turnchatconversations,
bidirectionalsystemstoincorporatelinkstructures inbothEnglishandArabic(Sec.3.1). Thisbilin-
(Yasunagaetal.,2022b)andknowledgegraphs(Ya- gual dataset is created by first compiling English
sunagaetal.,2022a). BuildingontheGPTmodel instruction,asoutlinedinSec. 3.1.1. Subsequently,
series,laterstudieshavetailoredGPT-2basedmod- in Sec. 3.1.2, we describe our semi-automated
elstomedicalandscientificliterature(Boltonetal.; English-to-Arabic translation pipeline, which en-
Luo et al., 2022; Taylor et al., 2022). Recent ables us to obtain high-quality Arabic medical
workssuchasMedAlpaca(Hanetal.,2023)focus benchmarks and instruction sets, as detailed in
onfinetuninglargescaleopen-sourceLLMssuch Sec. 3.1.3. The generated Arabic instructions
as LLaMA (Touvron et al., 2023a,b) on a set of are then combined with our English instructions
medicalinstructions. ChatDoctor(Yunxiangetal., to obtain the BiMed1.3M bilingual dataset. Fi-
2023)enhancedaLLaMAmodelon100Kclinical nally,wepresentthemodelandinstruction-tuning
Q&As,incorporatingknowledgeretrievalcapabili- strategyinSec. 3.2.
3Dataset Description
Multiple-ChoiceQuestionAnswering(MCQA)
PubMedQA(Jinetal.,2019) Closed-answerquestionsfrommedicalabstractsinPubMedresearchpapers.
MedMCQA(Paletal.,2022) Multiple-choicequestionsfromtheIndianAIIMSandNEETPGmedicalentranceexams.
MedQA(Jinetal.,2021) Multiple-choicequestionsfromtheUSMLEforU.S.medicallicensing.
QuestionAnswering(QA)
HealthCareMagic(Yunxiangetal.,2023) Specialist-patientQ&AscrawledfromHealthCareMagic.
iCliniq(Yunxiangetal.,2023) Physician-patientQ&AsfromtheiCliniqconsultationsite.
MedicalMeadow(Hanetal.,2023) Q&ApairsfromAnkiflashcardsandWikidoc’smedicaltextbooksandPatientInformation.
UMLS(Wuetal.,2023) Q&Asonmedicaltermmeaningsandentityrelationshipsfromknowledgegraphs.
LiveQA(Abachaetal.,2017) AnnotatedconsumerhealthquestionsfromtheNationalLibraryofMedicine(NLM).
MedicationQA(Abachaetal.,2019) AnnotatedconsumermedicationsinquiriesfromMedlinePlus.
Table1: Summaryofcollecteddatasourcesformultiple-choice(MCQA)andopenquestionanswering(QA).
Data Samples Avg.Turns #Tokens
QA 423.8K 1.00 131.8K Pain
MCQA 638.1K 1.00 342.5M PubMedQA Levels Symptoms Associated
Chat 249.7K 4.72 158.0M Chat Patient Study
Expression
UMLS Cause
Total 1311.6K 1.71 632.3M Relation MCQA Medical
Answer
QA
Figure 3: Statistics of the BiMed1.3M across QA, UMLS MedMCQA Question Doctor Long
Treatment
MCQA, and Chat (with more than one turn of ex- Increased Blood Description Help
changes)totaling623Mtokensand1.3Msamples. This Cell Risk
bilingual dataset, derived from translating approxi- (a) English-ArabicTraining Data (a)FrequentWords
mately50%oftheEnglishdatasetintoArabic,is1.5
Figure4: BiMed1.3MDatasetOverview: (a)Datatype
times larger due to its 2:1 English-to-Arabic content
distributionand(b)Analysisoffrequentwords.
ratio.
3.1 BiMed1.3M:BilingualDatasetwith collection of question-answer pairs, as detailed
DiverseMedicalInteractions in Table 1. For the multi-turn chat component,
we generate realistic conversations grounded on
ThegrowingneedforAI-drivenmedicalassistants,
MCQAscenarios,asdescribedbelow.
proficient across a spectrum of NLP tasks, high-
lightsthenecessityforcomprehensivedatasets. In
particular,theabilitytodeliverconciseanswersis MCQA-groundedMulti-turnChatGeneration:
criticalforevaluatingmodelperformance,yetthe Togeneraterealisticandengagingmulti-turncon-
functionality for engaging in chat is essential for versationsthataremedicallyaccurate,weleverage
practicaldeployment. Thisholdsparticularsignif- ChatGPT’sconversationalflowtogetherwithpub-
icance in the healthcare domain, where multiple licly available medical MCQAs (multiple-choice
turns of interaction with further inquiries about question answers). Utilizing ChatGPT, we sim-
symptoms, examinations, andpre-existingcondi- ulate potential doctor-patient dialogues based on
tionsareoftennecessary. Tothisend,wecompile MCQAs. Thechatcreationprocessinvolvesusing
ourEnglishinstructionsetasdetailedbelow. ChatGPTtodraftmessagesforboththeuserand
theAIassistantinadialogueformat. Inputsfrom
3.1.1 CompilingEnglishInstructionSet thePubmedQA,MedQA,andMedMCQAdatasets
are provided with specific directives to generate
At first, we compile a dataset in English en-
multi-turndiscussionsrelevanttothemedicalques-
compassing three types of medical interactions:
tionanswerpairs(MCQA).Aprompttemplateis
multiple-choice question answering (MCQA),
employed to define the format and criteria along-
whichfocusesonspecializedmedicalknowledge
side the input question, generating conversation
andreasoningfordefinitiveanswers;openquestion
transcriptsforbothparties(doctorandpatient)un-
answering(QA),thatincludesreal-worldconsumer
tilalogicalconclusionisreached.
questions; and multi-turn chat conversations for
dynamic exchanges. For the first two categories, The MCQA-grounded multi-turn interactions
wecombinevariousexistingsourcesintoaunified producedbyourapproacharegenerallythorough
4andinsightful,coveringaspectslikesymptomin- highquality,weadditionallysamplearandomsub-
quiriesandfurtherinformationrequests. Basedon setoftranslationswithhighscoresforprofessional
thisapproach,weproducemorethan200,000high- review. Our iterative translation process ensures
qualitymulti-turnmedicaldialogues,eachlinked thattranslationsadheretorigorousacademicand
toaspecificMCQA,collectivelycomprisingover clinicalstandards.
74milliontokens.
3.1.3 BilingualBenchmark&InstructionSet
Followingthegenerationofmulti-turndialogues,
ourdatasetcomprehensivelyencompassesmultiple- Creation of Medical Benchmark: To evaluate
choicequestionanswering(MCQA),openquestion the accuracy and applicability of Arabic medical
answering(QA),anddynamicchatconversations AImodels,wetranslatetheEnglishmedicaleval-
(Chats)—totalingover860,000instruction-tuning uation benchmarks (detailed in Sec. 4.2.1) into
data in English. Moreover, we leverage various Arabicusingtheaforementionediterativetransla-
publiclyavailablemedicalbenchmarkdatasetsin tionprocedure. Byprovidingahigh-qualityArabic
Englishtoevaluatetheperformanceofourmodel, medicalbenchmarkalignedwithitsEnglishcoun-
as elaborated in Sec. 4.2.1. Next, we detail our terpart,weaimtoestablishafundamentalstepin
semi-automated iterative translation pipeline to bridgingthelinguisticdivideinmodelevaluation
generate medical evaluation benchmarks and in- andcomparison,therebyofferingavaluableasset
structionsetstailoredfortheresource-constrained
forfurtherresearch1.
Arabiclanguage. These Arabic benchmarks are combined with
theoriginalEnglishevaluationbenchmarkstocre-
3.1.2 Semi-AutomatedIterativeTranslation ateanEnglish-Arabicbilingualbenchmark. This
Oursemi-automatediterativetranslationpipeline allows us to assess the linguistic capabilities of
isshowninFig.5. Atfirst,weperformEnglishto ourbilingualmodelaswellasitsmedicaldomain
ArabictranslationusingChatGPT,whichdelivers knowledge.
high-qualitytranslationsbyconsideringtheentire BilingualInstructionSet: Wetranslate444,995
input English text as context. Subsequently, we English samples into Arabic, covering all three
task ChatGPT with evaluating the quality of the typesofmedicalinteractions. Following(Sengupta
translated Arabic text relative to its original En- etal.,2023),weadoptedabilingualapproach,mix-
glishversion. Weassignscoresrangingfrom0to ingArabicandEnglishina1:2ratio. Thisapproach
100, directly proportional to the translation qual- ledtothecreationofanextensivebilingualinstruc-
ity. Thisscoringaimstoquantitativelyassessthe tiontuningdatasetthatintegratesbothlanguages.
fidelityandclarityoftranslations,ensuringtheac- Consequently,wecompileanEnglish-Arabicbilin-
curate preservation of technical terminology and gualdatasetnamedBiMed1.3M,1.5timeslarger
specifics. Additionally,weverifythealignmentof thanourEnglishcounterpart,comprisingmorethan
these scores to human preferences with the assis- 1,311,000samples. Furtherdetailsareillustrated
tanceofnativeArabicspeakers. inTab. 3andFig. 4.
For translations that score below a predefined Next we introduce our model and its bilingual
threshold, we initiate a refinement process. This medicalinstructiontuning.
involvesprovidingChatGPTwiththeoriginalEn-
3.2 BilingualMedicalInstructionTuningof
glishtext,alongwiththecurrenttranslationsand
MixtureofExpertsLLM
theirscoresasfeedback. WepromptChatGPTto
updatethetranslationtoensureoptimalconsistency Recently,Mixtral(Jiangetal.,2024)introduceda
and alignment with the English originals. This MixtureofExperts(MoE)basedarchitecturethat
feedbackloopiterativelyimprovesthetranslation achievesimpressiveperformanceongenericNLP
qualitythroughsuccessiverevisions. benchmarksinEnglish. Itenablessubstantialscal-
However,weobservedthatcertainsampleswith ing of model size within the same computational
initiallylowscoresfacedlimitationsinenhancing budget as traditional dense models. Unlike tradi-
theirscoresandtranslationqualitythroughthisau- tionaldensefeed-forwardnetworklayers,Mixtral
tomatedprocedure. Toaddressthis,wesubjectall employsasetnumberof"experts"inasparseman-
translationswithscoresbelowathresholdtoathor-
1The Arabic benchmark is available for download at
oughmanualverificationprocessperformedbya
the Hugging Face repository https://huggingface.co/
medical professional fluent in Arabic. To ensure BiMediX.
5Prompt HiR ga hn Sd co om re s Doctor High-Quality
English Data ChatGPT Verification ArabicTranslation
Translation &
Q Lou we s it nio sun l: i n to glucagon Updated Low Scores RT era fin ns ela mti eo nn t ىلإ نيلوسنلأا ةبس: نل ظاؤ حس لال ُيا
r ta ht ei so e i s e xs ce ee pn t i :n all of Translation ي :ف ء اة نض ثتسفخ ابن م تلا لا ن احو لج ا ا هك ذو هل ج لل كا
Scoring
A.Glycogen synthesis نيجوكيلجلا قيلخت .أ
B. Glycogen breakdown 0 100 High Scores نيجوكيلجلا للحت .ب
C. Gluconeogenesis ديدجلا زوكولجلا نيوكت .ج
D.Ketogenesis Verifiedalignment نتيكت .د
with human scoring
Figure 5: Overview of the proposed semi-automated, iterative translation pipeline featuring human alignment.
TheprocessinvolvesiterativetranslationandscoreoptimizationutilizingLLMs(suchasChatGPT)andmanual
refinementbyprofessionalsforsampleswithlowscoresalongwitharandomselectionofhigh-scoringsamples.
ner, replacing the FFN layers in dense networks. etal.,2023),werefrainfromconductingadditional
Additionally,agatenetworkorrouterisemployed medicalpre-trainingbeforeinstruction-tuningiton
todirectinputtokenstoappropriateexpertsbased ourmedicaldataset.
onlearnedparameters. WhileMixtral(Jiangetal., Insummary,throughcarefulinstructiontuning
2024) offers advantages such as improved infer- oftheroutingnetworkandexpertsintheMixtralde-
encetimesoverdensemodels,itfaceschallengesin coderusingourBiMed1.3Mdataset,weachieveen-
Arabic-Englishbilingualmedicalapplicationsdue hancedmedicaldomaincapabilitiesinbothArabic
tolimitedmedicaldomainknowledgeandArabic andEnglishlanguages. Thisincludesproficiency
languagecapabilities. Toaddressthesechallenges, inmultiple-choicequestionanswering,open-ended
we perform Arabic-English bilingual medical in- questionanswering,andrealisticmulti-turnques-
structiontuningofMixtralMoEarchitectureusing tionansweringbetweenourBiMediXmodeland
ourBiMed1.3Mdataset. Notethatperformingcon- the user (e.g., patient), making it suitable for de-
ventionalsupervisedfine-tuningofMixtral(Jiang ploymentasabilingualmedicalchatbot.
etal.,2024)requiressubstantialcomputationalre-
4 Experiments
sources and often encounters challenges in care-
fullytuningtheexpertlayersandtheroutingnet-
4.1 ExperimentalSettings
work,leadingtogeneralizationissues. Tothisend,
Our BiMediX is built upon Mixtral-8x7B (Jiang
weemployParameter-EfficientFinetuning(PEFT)
et al., 2024) base network, a sparse mixture of
techniques (Hu et al., 2021; Li and Liang, 2021;
experts language model. This model extends the
Dettmersetal.,2023)toadaptthepre-trainedMix-
Mistral-7Barchitecture(Jiangetal.,2023),featur-
tralmodel(Jiangetal.,2024)withminimalcompu-
ingauniquedesignwitheachlayerhostingeight
tationalresourceswhileensuringhighperformance
"expert"feedforwardblocks. Arouternetworkde-
andadaptability. Inourapproach,weuseQLoRA-
termineswhichtwoexpertswillprocesseachtoken,
based PEFT training. Specifically, we attach the
mergingtheiroutputs. Consequently,ithandles47
QLoRA adapters to the decoder layers of (Jiang
billion parameters, with fewer than 13 billion ac-
et al., 2024), including the experts and the rout-
tiveduringinference. Thismodelfeaturesahidden
ingnetwork,torepurposetheMixtureofExperts
statedimensionof14,336,acontextwindowcapa-
architecture as a bilingual medical chat assistant
bleofaccommodating32,768tokens,32layers,32
byutilizingtheproposedBiMed1.3Mdatasetfor
attentionheads,andavocabularysizeof32,000.
instructiontuning.
For fine-tuning Mixtral, we utilize QLoRA, a
Moreover,toalignwiththeconversationalstyle low-rankadaptationtechnique,byincorporatinga
in our BiMed1.3M dataset, for both English and setoflearnablelow-rankadapterweightsintoboth
Arabicinstructions,weadopttheVicuna(Chiang theexpertsandtheroutingnetwork. Thisadapta-
etal.,2023)dataformat"conversations": ["from": tionappliestoq,k,andv,settingtherankto128
"human/AI,""value": "..."],calculatinglossexclu- andtheαparameterto64. Consequently,thisap-
sively for the AI’s responses within the dialogue proachresultsinthetrainingofapproximately4%
script. Unlikealternativeapproachessuchas(Wu of the original parameters. Our bilingual model
6underwenttrainingonroughly632milliontokens knowledge and language comprehension. The
sourced from the Arabic-English corpus, includ- datasetincludesatestsetof4,183questions,each
ing288milliontokensthatareexclusivelyforEn- withfouroptions.
glish. Thetraininghyper-parametersincludedan (iii) MedQA (Jin et al., 2021) is a dataset com-
effectivebatchsizeof16withtwogradientaccu- prising multiple-choice questions from medical
mulationstepsandutilizedtheAdamWoptimizer board examinations in the US, Mainland China,
withalearningrateof0.0002,cosinelearningrate andTaiwan. Itfeaturestwotypesofquestions: con-
schedule,andtenwarmupsteps. Themodelimple- cisesentencestoassessspecificknowledgeandex-
mentationleveragedPyTorch(Paszkeetal.,2019), tendedparagraphsthatdetailapatient’scondition.
alongwiththeDeepspeedandZeRO(Rasleyetal., Our analysis concentrates on the English portion
2020;Rajbhandarietal.,2020)packages,withgra- (USMLE), containing 1,273 samples for testing
dientcheckpointing(Chenetal.,2016). Ourbilin- purposes. Engagingwiththisbenchmarkinvolves
gualBiMedixistrainedfortwoepochswitheight multiplestagesofreasoningandevidenceretrieval.
A100 (80GB) GPUs, requiring only 35 hours for (iv)TheMedicalMMLU(MassiveMultitaskLan-
thetraining. guageUnderstanding)isagroupofsixdatasetsthat
compiles1,089testquestionswithfouroptionsre-
4.2 QuantitiveEvaluation
latedtodifferentmedicalsubjects. ThesixMMLU
In the literature, evaluating medical language datasetsareClinicalKnowledge(Cli-KG),College
models predominantly involves multiple-choice Biology(C-Bio),CollegeMedicine(C-Med),Med-
question-answeringtasks,withaccuracyastheper- ical Genetics (Med-Gen), Professional Medicine
formance metric. We employed the EleutherAI (Pro-Med)&Anatomy(Ana).
evaluationframework2. All 7045 questions from the above English
datasetsaretranslatedintoArabicusingoursemi-
4.2.1 MedicalBenchmarks
automatedpipelinetocreateourArabicandBilin-
Our analysis incorporates multiple prominent gualevaluationbenchmarks.
benchmarksinmedicalmultiple-choicequestion-
answering. Among these datasets is MedQA,
whichissourceddirectlyfromtheNationalBoard 4.2.2 Results
ofMedicalExaminers(NBME),theentitytasked Bilingual Evaluation: Here, we assess our
withoverseeingtheassessmentsnecessaryforU.S. BiMediXonArabic-Englishbilingualevaluation
medicallicensure. benchmarkderivedfromevaluatingresultsinboth
(i) PubMedQA (Jin et al., 2019) is a question- languages. Tab. 2 shows our BiMediX results
answering dataset derived from biomedical re- against the base model, Mixtral-8x7B (not fine-
search papers on PubMed. Given a question tuned) and Jais-30B. The latter is a larger model
sourced from the title of a research paper and a designedspecificallyfortheArabiclanguageand
context from the abstract, the task is to provide capable of functioning in both English and Ara-
an answer in the form of ’yes’, ’no’, or ’maybe’, bic. BiMediXdemonstratessuperiorperformance
mirroring the conclusion of the paper. Building acrossallbenchmarks,achievingaccuracythatis,
onpriorresearch,ourexaminationfocusesonthe onaverage,morethan10and15pointshigher,re-
PQA-LsubsetofPubMedQA,whichiscomprised spectively,whencomparedtothebaselinemodel
ofmanuallyannotatedquestion-answer(QA)pairs and Jais-30B. This achievement underscores the
sourcedfromPubMed. Answeringthesequestions substantial value of our proposed BiMed1.3M
withaccuracyrequiresthoroughreasoningoverthe datasetanditsunmatchedeffectivenessandadapt-
biomedicalcontextsandquantitativedataanalysis. abilityinaddressingmedicalquerieswithinabilin-
(ii) MedMCQA (Pal et al., 2022) is a multiple- gualframework.
choicedatasetconstructedfromquestionsfeatured ArabicBenchmark: WetestBiMediXusingour
in the Indian AIIMS and NEET PG medical en- Arabicbenchmark,comparingitsperformancewith
tranceexams. Itcoversabroadspectrumofmedi- thatofJais-30B.InTable3,wepresentthefindings
calsubjects,evaluatingbothprofessionaldomain for Jais and our BiMediX in two configurations:
onepre-trainedexclusivelyonArabiccontent(Ara-
2https://github.com/EleutherAI/
bic)andtheotherwithbilingualdata(Bilingual).
lm-evaluation-harness, is the backend for Hugging
FaceOpenLLMLeaderboard. Ourbilingualmodeloutperformsinallcategories
7MMLU MedMCQA MedQA PubmedQA AVG
Model
Cli-KG C-Bio C-Med Med-Gen Pro-Med Ana
Jais-30B(Senguptaetal.,2023) 57.4 55.2 46.2 55.0 46.0 48.9 40.2 31.0 75.5 50.6
Mixtral-8x7B(Jiangetal.,2024) 59.1 57.6 52.6 59.5 53.3 54.4 43.2 40.6 74.7 55.0
BiMediX(Bilingual) 70.6 72.2 59.3 74.0 64.2 59.6 55.8 54.0 78.6 65.4
Table2: PerformanceofBiMediXontheBilingualbenchmark.
MMLU MedMCQA MedQA PubmedQA AVG
Model
Cli-KG C-Bio C-Med Med-Gen Pro-Med Ana
Jais-30B(Senguptaetal.,2023) 52.1 50.7 40.5 49.0 39.3 43.0 37.0 28.8 74.6 46.1
BiMediX(Arabic) 60.0 54.9 55.5 58.0 58.1 49.6 46.0 40.2 76.6 55.4
BiMediX(Bilingual) 63.8 57.6 52.6 64.0 52.9 50.4 49.1 47.3 78.4 56.5
Table3: PerformanceofBiMediXontheArabicbenchmark.
MMLU MedMCQA MedQA PubmedQA AVG
Model
Cli-KG C-Bio C-Med Med-Gen Pro-Med Ana
PMC-LLaMA-13B(Wuetal.,2023) 63.0 59.7 52.6 70.0 64.3 61.5 50.5 47.2 75.6 60.5
Med42-70B(Christopheetal.,2023) 75.9 84.0 69.9 83.0 78.7 64.4 61.9 61.3 77.2 72.9
ClinicalCamel-70B(Tomaetal.,2023) 69.8 79.2 67.0 69.0 71.3 62.2 47.0 53.4 74.3 65.9
Meditron-70B(Chenetal.,2023) 72.3 82.5 62.8 77.8 77.9 62.7 65.1 60.7 80.0 71.3
BiMediX 78.9 86.1 68.2 85.0 80.5 74.1 62.7 62.8 80.2 75.4
Table4: PerformanceofBiMediXontheEnglishbenchmark.
Model ActiveParam Latency Tokens/sec achievefavorableoutcomes. Thisdemonstratesour
PMC-LLaMA-13B(Wuetal.,2023) 13B 4.1s 124.1 model’sversatilityinhandlingvariousmedicalin-
Med42-70B(Christopheetal.,2023) 70B 24.5s 20.9
teractionssimultaneously. Moreover,ourmethod
Meditron-70B(Chenetal.,2023) 70B 24.5s 20.9
Jais-30B(Senguptaetal.,2023) 30B 14.0s 36.5 outperforms Med42 by an average gain of 2.5%
BiMediX 13B 2.8s 180.6
Furthermore,ourmodelismoreefficientinterms
Table5: Modelstatisticscomparingmemorysizeand ofpredictionspeed(latencyandtokenspersecond)
inferencespeed,computedwith(LLm-analysis,2023) thanallothermodelsconsidered,asdetailedinTa-
onGPUA100-80GB. ble5. Additionalqualitativeexamplesareprovided
. intheAppendix.
5 Conclusion
within the Arabic context, underscoring that the
integration of both types of training data signifi- Our work introduces the first bilingual medical
cantlyenhancestheunderstandingandprocessing mixture of experts (LLM) BiMediX designed to
ofmedicalinformationinanArabicsetting. facilitatecomprehensivemedicalinteractions, in-
English Benchmark: We evaluate the quantita- cludingconversations,MCQA,andQAinbothEn-
tive performance of our English model against glishandArabiclanguages. Theimplementation
previousstate-of-the-artEnglishmedicalmodels. ofasemi-automatedtranslationpipeline,coupled
As illustrated in Table 4, BiMediX exhibits out- withhumanrefinement,ensuresthehighqualityof
standingperformanceacrossallsubsets,securing English-to-Arabictranslations. BiMediXdemon-
the highest average scores among all models as- strates superior performance over existing mod-
sessed. When compared to Clinical Camel-70B elssuchasMed42andMeditroninEnglish-based
model, our BiMediX exhibits around 10% aver- medicalevaluationbenchmarks,andsignificantly
age performance gain. Furthermore, it also out- outperformsthegenericbilingualLLM,Jais-30B,
performsMeditron-70Binalmosteverysubsetex- inArabicmedicalandbilingualevaluations. The
ceptforMedMCQA.Notably,ourmodel’ssuccess introductionofanovelevaluationbenchmarkand
comes without the need for separate fine-tuning theextensiveBiMed1.3Minstructionset,encom-
onthetrainingsetforeachevaluationbenchmark, passing 1.3 million diverse medical interactions,
in contrast to Meditron, which employs individ- includingchats,furtherhighlightsthesignificance
ual fine-tuning for each evaluation benchmark to ofourcontributions.
86 Limitations ai: Harmlessnessfromaifeedback. arXivpreprint
arXiv:2212.08073.
Despite its overall improvement, BiMediX, like
otherlanguagemodels,mayexperienceissuessuch IzBeltagy,KyleLo,andArmanCohan.2019. Scibert:
Apretrainedlanguagemodelforscientifictext. arXiv
ashallucinations,toxicity,andstereotypes. These
preprintarXiv:1903.10676.
issuesstemfromboththeinheritedlimitationsof
thebasemodelsandthenatureofthepre-training Elliot Bolton, David Hall, Michihiro Yasunaga, Tony
data. Whilewehaveconductedautomaticandqual- Lee,ChrisManning,andPercyLiang. BioMedLM:a
domain-specificlargelanguagemodelforbiomedical
itativeevaluationswithmedicalprofessionals,we
text. https://crfm.stanford.edu/2022/12/15/
acknowledgethatourmodel’smedicaldiagnoses
biomedlm.html.
andrecommendationsmaynotalwaysbeaccurate.
Extensive human evaluation is more reliable but Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos
costlier and time-consuming. The exploration of Guestrin. 2016. Training deep nets with sublinear
memorycost. arXivpreprintarXiv:1604.06174.
alternative solutions remains an important focus
forongoingresearch. Currently, ourmodelslack
Zeming Chen, Alejandro Hernández Cano, Angelika
explicitmechanismstocurbundesirablebehaviors. Romanou,AntoineBonnet,KyleMatoba,Francesco
Future research will focus on enhancing security Salvi,MatteoPagliardini,SiminFan,AndreasKöpf,
AmirkeivanMohtashami,etal.2023. Meditron-70b:
and alignment strategies (Christiano et al., 2017;
Scalingmedicalpretrainingforlargelanguagemod-
Baietal.,2022;Rafailovetal.,2023). Onabrighter
els. arXivpreprintarXiv:2311.16079.
note, we believe that releasing our weight could
contributetoinvestigatingandmitigatingriskstied Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,
ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
toapplicationuses.
Zhuang,YonghaoZhuang,JosephEGonzalez,etal.
2023. Vicuna: Anopen-sourcechatbotimpressing
7 SafetyandEthicalImplications
gpt-4with90%*chatgptquality. Seehttps://vicuna.
lmsys.org(accessed14April2023).
Werecognizethesignificantsocietalimpactofthis
technology,emphasizingethicalconsiderationsand PaulFChristiano,JanLeike,TomBrown,MiljanMar-
transparency. This release, intendedfor research, tic, Shane Legg, and Dario Amodei. 2017. Deep
isnotreadyforclinicalorcommercialuse. Further reinforcementlearningfromhumanpreferences. Ad-
vancesinneuralinformationprocessingsystems,30.
researchisneededtoensuresafetyandaccuracyin
clinicalsettingsandtopreventpatientharm. Col- ClémentChristophe,AvaniGupta,NasirHayat,Praveen
laborationwithpatients,medicalprofessionals,and Kanithi, Ahmed Al-Mahrooqi, Prateek Munjal,
ethicistsiscrucialforethicaloversight. MarcoPimentel,TathagataRaha,RonnieRajan,and
Shadab Khan. 2023. Med42 - a clinical large lan-
guagemodel.
References
HyungWonChung,LeHou,ShayneLongpre,Barret
Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi
Asma Ben Abacha, Eugene Agichtein, Yuval Pinter,
Wang,MostafaDehghani,SiddharthaBrahma,etal.
andDinaDemner-Fushman.2017. Overviewofthe
2022. Scalinginstruction-finetunedlanguagemodels.
medicalquestionansweringtaskattrec2017liveqa.
arXivpreprintarXiv:2210.11416.
InTREC,pages1–12.
Asma Ben Abacha, Yassine Mrabet, Mark Sharp, Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and
Travis R Goodwin, Sonya E Shooshan, and Dina LukeZettlemoyer.2023. Qlora: Efficientfinetuning
Demner-Fushman.2019. Bridgingthegapbetween ofquantizedllms. arXivpreprintarXiv:2305.14314.
consumers’ medication questions and trusted an-
swers. InMedInfo,pages25–29. Moussa Kamal Eddine, Nadi Tomeh, Nizar Habash,
Joseph Le Roux, and Michalis Vazirgiannis. 2022.
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama Arabart: a pretrained arabic sequence-to-sequence
Ahmad, Ilge Akkaya, Florencia Leoni Aleman, modelforabstractivesummarization. arXivpreprint
DiogoAlmeida,JankoAltenschmidt,SamAltman, arXiv:2203.10945.
ShyamalAnadkat,etal.2023. Gpt-4technicalreport.
arXivpreprintarXiv:2303.08774. YuGu,RobertTinn,HaoCheng,MichaelLucas,Naoto
Usuyama,XiaodongLiu,TristanNaumann,Jianfeng
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Gao,andHoifungPoon.2021. Domain-specificlan-
Amanda Askell, Jackson Kernion, Andy Jones, guagemodelpretrainingforbiomedicalnaturallan-
Anna Chen, Anna Goldie, Azalia Mirhoseini, guageprocessing. ACMTransactionsonComputing
Cameron McKinnon, et al. 2022. Constitutional forHealthcare(HEALTH),3(1):1–23.
9Tianyu Han, Lisa C Adams, Jens-Michalis Papaioan- Schoelkopf, et al. 2022. Crosslingual generaliza-
nou,PaulGrundmann,TomOberhauser,Alexander tion through multitask finetuning. arXiv preprint
Löser, Daniel Truhn, and Keno K Bressem. 2023. arXiv:2211.01786.
Medalpaca–an open-source collection of medical
conversational ai models and training data. arXiv El Moatez Billah Nagoudi, AbdelRahim Elmadany,
preprintarXiv:2304.08247. andMuhammadAbdul-Mageed.2021. Arat5: Text-
to-texttransformersforarabiclanguagegeneration.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan arXivpreprintarXiv:2109.12068.
Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,
Ankit Pal, Logesh Kumar Umapathi, and Malaikan-
and Weizhu Chen. 2021. Lora: Low-rank adap-
nanSankarasubbu.2022. Medmcqa: Alarge-scale
tation of large language models. arXiv preprint
multi-subject multi-choice dataset for medical do-
arXiv:2106.09685.
mainquestionanswering. InConferenceonHealth,
Kexin Huang, Jaan Altosaar, and Rajesh Ranganath. Inference,andLearning,pages248–260.PMLR.
2019. Clinicalbert: Modeling clinical notes and
Adam Paszke, Sam Gross, Francisco Massa, Adam
predicting hospital readmission. arXiv preprint
Lerer, James Bradbury, Gregory Chanan, Trevor
arXiv:1904.05342.
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, et al. 2019. Pytorch: An imperative style,
Albert Q Jiang, Alexandre Sablayrolles, Arthur Men-
high-performancedeeplearninglibrary. Advancesin
sch,ChrisBamford,DevendraSinghChaplot,Diego
neuralinformationprocessingsystems,32.
delasCasas,FlorianBressand,GiannaLengyel,Guil-
laumeLample,LucileSaulnier,etal.2023. Mistral
RafaelRafailov,ArchitSharma,EricMitchell,Stefano
7b. arXivpreprintarXiv:2310.06825.
Ermon,ChristopherDManning,andChelseaFinn.
2023. Directpreferenceoptimization:Yourlanguage
Albert Q Jiang, Alexandre Sablayrolles, Antoine
model is secretly a reward model. arXiv preprint
Roux,ArthurMensch,BlancheSavary,ChrisBam-
arXiv:2305.18290.
ford,DevendraSinghChaplot,DiegodelasCasas,
Emma Bou Hanna, Florian Bressand, et al. 2024.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase,
Mixtralofexperts. arXivpreprintarXiv:2401.04088.
andYuxiongHe.2020. Zero: Memoryoptimizations
towardtrainingtrillionparametermodels. InSC20:
DiJin,EileenPan,NassimOufattole,Wei-HungWeng,
InternationalConferenceforHighPerformanceCom-
HanyiFang,andPeterSzolovits.2021. Whatdisease
puting,Networking,StorageandAnalysis,pages1–
does this patient have? a large-scale open domain
16.IEEE.
questionansweringdatasetfrommedicalexams. Ap-
pliedSciences,11(14):6421.
JeffRasley,SamyamRajbhandari,OlatunjiRuwase,and
Yuxiong He. 2020. Deepspeed: System optimiza-
QiaoJin,BhuwanDhingra,ZhengpingLiu,WilliamW
tionsenabletrainingdeeplearningmodelswithover
Cohen,andXinghuaLu.2019. Pubmedqa: Adataset
100billionparameters. InProceedingsofthe26th
forbiomedicalresearchquestionanswering. arXiv
ACMSIGKDDInternationalConferenceonKnowl-
preprintarXiv:1909.06146.
edgeDiscovery&DataMining,pages3505–3506.
JinhyukLee,WonjinYoon,SungdongKim,Donghyeon Teven Le Scao, Angela Fan, Christopher Akiki, El-
Kim,SunkyuKim,ChanHoSo,andJaewooKang. lie Pavlick, Suzana Ilic´, Daniel Hesslow, Roman
2020. Biobert: a pre-trained biomedical language Castagné,AlexandraSashaLuccioni,FrançoisYvon,
representation model for biomedical text mining. et al. 2022. Bloom: A 176b-parameter open-
Bioinformatics,36(4):1234–1240. accessmultilinguallanguagemodel. arXivpreprint
arXiv:2211.05100.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning:
Optimizingcontinuouspromptsforgeneration. arXiv Neha Sengupta, Sunil Kumar Sahu, Bokang Jia,
preprintarXiv:2101.00190. Satheesh Katipomu, Haonan Li, Fajri Koto,
Osama Mohammed Afzal, Samta Kamboj, Onkar
LLm-analysis.2023. Llm-analysis-latencyandmem- Pandit, Rahul Pal, et al. 2023. Jais and jais-chat:
oryanalysisoftransformermodelsfortrainingandin- Arabic-centricfoundationandinstruction-tunedopen
ference. Availabeathttps://github.com/cli99/ generative large language models. arXiv preprint
llm-analysis. arXiv:2308.16149.
Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Hoo-Chang Shin, Yang Zhang, Evelina Bakhturina,
Zhang, Hoifung Poon, and Tie-Yan Liu. 2022. Raul Puri, Mostofa Patwary, Mohammad Shoeybi,
Biogpt: generative pre-trained transformer for and Raghav Mani. 2020. Biomegatron: Larger
biomedical text generation and mining. Briefings biomedicaldomainlanguagemodel. arXivpreprint
inBioinformatics,23(6):bbac409. arXiv:2010.06060.
NiklasMuennighoff,ThomasWang,LintangSutawika, KaranSinghal,ShekoofehAzizi,TaoTu,SSaraMah-
Adam Roberts, Stella Biderman, Teven Le Scao, davi,JasonWei,HyungWonChung,NathanScales,
MSaifulBari,ShengShen,Zheng-XinYong,Hailey AjayTanwani,HeatherCole-Lewis,StephenPfohl,
10etal.2022. Largelanguagemodelsencodeclinical
knowledge. arXivpreprintarXiv:2212.13138.
Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres,
Ellery Wulczyn, Le Hou, Kevin Clark, Stephen
Pfohl, Heather Cole-Lewis, Darlene Neal, et al.
2023. Towards expert-level medical question an-
sweringwithlargelanguagemodels. arXivpreprint
arXiv:2305.09617.
RossTaylor,MarcinKardas,GuillemCucurull,Thomas
Scialom,AnthonyHartshorn,ElvisSaravia,Andrew
Poulton, Viktor Kerkez, and Robert Stojnic. 2022.
Galactica:Alargelanguagemodelforscience. arXiv
preprintarXiv:2211.09085.
AugustinToma,PatrickRLawler,JimmyBa,RahulG
Krishnan,BarryBRubin,andBoWang.2023. Clini-
calcamel: Anopen-sourceexpert-levelmedicallan-
guagemodelwithdialogue-basedknowledgeencod-
ing. arXivpreprintarXiv:2305.12031.
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
Martinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,Faisal
Azhar, et al. 2023a. Llama: Open and effi-
cient foundation language models. arXiv preprint
arXiv:2302.13971.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Bhosale, et al. 2023b. Llama 2: Open founda-
tion and fine-tuned chat models. arXiv preprint
arXiv:2307.09288.
ChaoyiWu,XiaomanZhang,YaZhang,YanfengWang,
and Weidi Xie. 2023. Pmc-llama: Further fine-
tuning llama on medical papers. arXiv preprint
arXiv:2304.14454.
Michihiro Yasunaga, Antoine Bosselut, Hongyu Ren,
Xikun Zhang, Christopher D Manning, Percy S
Liang,andJureLeskovec.2022a. Deepbidirectional
language-knowledgegraphpretraining. Advancesin
NeuralInformationProcessingSystems,35:37309–
37323.
MichihiroYasunaga,JureLeskovec,andPercyLiang.
2022b. Linkbert: Pretraininglanguagemodelswith
documentlinks. arXivpreprintarXiv:2203.15827.
LiYunxiang,LiZihan,ZhangKai,DanRuilong,and
ZhangYou.2023. Chatdoctor:Amedicalchatmodel
fine-tuned on llama model using medical domain
knowledge. arXivpreprintarXiv:2303.14070.
11How would you differentiate between Autism Spectrum Disorder and Rett Syndrome in a young
child, considering the nuances in developmental milestones and neurobehavioral patterns?
Autism Spectrum Disorder (ASD) and Rett Syndrome are both neurodevelopmental disorders, but
they have distinct features. In general, ASD is characterized by impaired social interaction and
communication, as well as restricted and repetitive behaviors. Rett Syndrome, on the other hand, is
a rare genetic disorder that primarily affects females and is characterized by regression of
developmental milestones, loss of purposeful hand skills, and development of repetitive hand
movements. While both conditions can present with similar symptoms, the regression of
developmental milestones and loss of purposeful hand skills are more specific to Rett Syndrome.
Additionally, a genetic test known as the methylation-specific PCR for MECP2 can be used to
confirm the diagnosis of Rett Syndrome.
In a case of severe pancreatitis, how do you decide between conservative management and
surgical intervention, taking into account the patient's overall condition and comorbidities?
The decision between conservative management and surgical intervention in a case of severe
pancreatitis takes into account the patient's overall condition, comorbidities, and potential
complications. In cases of mild pancreatitis, conservative management is usually sufficient. However,
in cases of severe pancreatitis, surgical intervention may be necessary if there is a concern for
complications such as necrotizing pancreatitis, infected pancreatic necrosis, or abscess formation.
The patient's overall condition and comorbidities should be considered when making this decision.
How would you manage a case of refractory hyperthyroidism in a patient with Graves' disease
who is not a candidate for radioactive iodine therapy?
Refractory hyperthyroidism in a patient with Graves' disease who is not a candidate for radioactive
iodine therapy can be managed with pharmacological thyroid suppression. This involves using
medications to reduce the production and release of thyroid hormones. One commonly used
medication is propylthiouracil, which inhibits the synthesis of thyroid hormones. Another
medication is methimazole, which also inhibits the synthesis of thyroid hormones. These
medications can be used alone or in combination with other treatments, such as beta-blockers, to
manage symptoms of hyperthyroidism and control thyroid hormone levels. It is important to work
closely with a healthcare provider to determine the most appropriate treatment plan for the
individual patient, as the choice of medication and dosage will depend on factors such as the
patient's age, overall health, and other medical conditions they may have.
Is surgery a viable option in this case?
Surgery may be considered as a treatment option for refractory hyperthyroidism in a patient with
Graves' disease who is not a candidate for radioactive iodine therapy. However, it is important to
note that surgery carries a higher risk of complications than other treatment options, such as
pharmacological thyroid suppression. Surgical options include thyroidectomy, which involves
removing part or all of the thyroid gland, and thyroid lobectomy, which involves removing only one
lobe of the thyroid gland. These procedures may be recommended for patients who have failed
other treatment options or who have specific contraindications to other treatments. The decision to
undergo surgery should be made in consultation with a healthcare provider, taking into account the
patient's individual circumstances and the potential risks and benefits of the procedure.
Figure6: QualitativeExamplesofourBiMediX.
12Figure7: QualitativeExamplesofourBiMediX,whichistranslatedtoEnglishasfollows:
User:Ihavebeendiagnosedwithtype2diabetes. CanyouexplainhowIshouldmonitormybloodsugarlevels,and
whatdietarychangesmightbenecessary? BiMediX:Monitoringbloodsugarlevelsiscrucialtocontrollingtype2
diabetes. Itisrecommendedtomonitoryourbloodsugarlevelatleastonceaday. Monitoringyourbloodsugar
levelscanhelpyouunderstandhowyourlifestyleanddietaffectyourbloodsugarlevels. Itusuallyrequiresdietary
changes,suchasreducingconsumptionofaddedsugarandfast-absorbingcarbohydratesandincreasingfiber
intake,tocontroltype2diabetes. Itisalsoimportanttoeatregular,balancedmealstohelpregulatebloodsugar
levels. Irecommenddiscussingabloodsugarmonitoringplananddietchangeswithyourhealthcareprovider.
User: I’vebeenexperiencingoccasionalshortnessofbreathandfatigue. Couldthesebeearlysignsofaheart
orlungcondition? BiMediX:Shortnessofbreathandfatiguecanbeearlysymptomsofseveralheartandlung
conditions. Thiscanberelatedtoheartdiseasesuchasheartfailureorcoronaryarterydisease,orlungdisease
suchasasthmaorchronicobstructivepulmonarydisease. Itisimportanttoconsultwithyourhealthcareprovider
toevaluateyoursymptomsandperformappropriateteststodeterminetheunderlyingcause. Theywillbeableto
provideanaccuratediagnosisandrecommendanappropriatetreatmentplan.
User: IsthereanythingIcandowithmydiettohelpimprovemysituation? BiMediX:Dietarychangescanhavea
positiveimpactonheartandlungconditions. Generally,foodsrichinfiber,fruits,vegetablesandomega-3fatty
acidsshouldbeincludedinthediet. Foodstoreduceyourintakemayincludedairyproductshighinsaturatedfat,
processedmeats,softdrinks,andprocessedfoods. Inaddition,yourhealthcareprovidermayrecommendaspecific
dietsuchasamoderate-moderatedietoralow-sodiumdietdependingonyourspecificcondition. Itisimportantto
consultyourhealthcareproviderbeforemakinganymajordietarychangestoensureyourneedsaremet.
13