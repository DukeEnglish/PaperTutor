Preprint
MODE ESTIMATION WITH PARTIAL FEEDBACK
BY CHARLES ARNAL*1, VIVIEN CABANNES*2 AND VIANNEY PERCHET3
1Datashape,Inria,Saclay,France
2FundamentalArtificialIntelligenceResearch(FAIR),MetaAI,NewYork,USA
3CenterforResearchinEconomicsandStatistics(CREST),ENSAE,Palaiseau,France
The combination of lightly supervised pre-training and online fine-
tuninghasplayedakeyroleinrecentAIdevelopments.Thesenewlearning
pipelinescallfornewtheoreticalframeworks.Inthispaper,weformalizecore
aspectsofweaklysupervisedandactivelearningwithasimpleproblem:the
estimationofthemodeofadistributionusingpartialfeedback.Weshowhow
entropycodingallowsforoptimalinformationacquisitionfrompartialfeed-
back, develop coarse sufficient statistics for mode identification, and adapt
banditalgorithmstoournewsetting.Finally,wecombinethosecontributions
intoastatisticallyandcomputationallyefficientsolutiontoourproblem.
CONTENTS
1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
1.1 TheModeEstimationwithPartialFeedbackProblem . . . . . . . . . . . . . 2
1.2 RelatedWork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.3 PerformanceMetrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
1.4 SummaryofContributions . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 TheEmpiricalModeEstimator . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.1 ProofofTheorem1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
2.2 MinimaxOptimality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3 ExhaustiveDichotomicSearchProcedures . . . . . . . . . . . . . . . . . . . . . . 9
3.1 EntropyCoding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.2 ExhaustiveSearchwithFixedCoding . . . . . . . . . . . . . . . . . . . . . 12
3.3 ExhaustiveSearchwithAdaptiveCoding . . . . . . . . . . . . . . . . . . . 12
3.4 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
4 TruncatedSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.1 CoarseSufficientStatistics . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
4.2 AdaptiveTruncatedSearch . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
4.3 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
5 Bandit-InspiredElimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
5.1 Designoftheeliminationschedule . . . . . . . . . . . . . . . . . . . . . . . 20
5.2 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
6 SetElimination . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
6.1 Proofs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
Appendix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
A.1 InformationProjectionComputation . . . . . . . . . . . . . . . . . . . . . . 29
*Theseauthorscontributedequallytothiswork.
MSC2020subjectclassifications:Primary62L05,62B86;secondary62D10,62B10.
Keywordsandphrases:ActiveLearning,PartialFeedback,EntropyCoding,CoarseSearch,BestArmIdenti-
fication.
1
4202
beF
02
]LM.tats[
1v97031.2042:viXra2
A.2 AdditionalProofsforSection3 . . . . . . . . . . . . . . . . . . . . . . . . . 35
A.3 AdditionalProofsforSection4 . . . . . . . . . . . . . . . . . . . . . . . . . 37
A.4 AdditionalProofsforSection5 . . . . . . . . . . . . . . . . . . . . . . . . . 38
A.5 AdditionalProofsforSection6 . . . . . . . . . . . . . . . . . . . . . . . . . 44
Acknowledgments. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46
1. Introduction. Themodeofadistributionisafundamentalconceptinstatistics,serv-
ing as a key identifier for the most likely event to occur. For instance, identifying modes
of conditional distributions is the main task of classification algorithms –classification con-
sists in learning the mapping f (x)= argmax p(y x) for a joint distribution p over in-
∗ y
|
puts x and classes y. Traditionally, datasets were small enough to fully annotate samples
before learning the modes of the underlying distributions. However, with the increasing
scale of machine learning problems, data collection has become a significant part of ma-
chine learning pipelines. This is illustrated by the substantial efforts dedicated to data pre-
processing to train foundational AI models (see, e.g., OpenAI, 2023; Touvronetal., 2023).
Moreover, it is foreseeable that future models will incorporate annotation feedback loops.
Indeed, fine-tuning foundational models already relies on various active learning strategies,
suchasreinforcementlearningwith humanfeedback(Ziegleretal.,2020)andpartialanno-
tations (Zhu,JordanandJiao, 2023). This makes theories of weakly-supervised, and active
learning highly relevant to the machine learning community. This paper introduces one of
the simplest setups to combine active and weakly-supervised learning. It focuses on partial
annotations,andsearchesforthebestalgorithmstoidentifythemodeofadistributiongiven
abudgetofannotations.
1.1. TheModeEstimationwithPartialFeedbackProblem. Thetaskathandistheiden-
tificationofthemodeofadistribution p P( ), where isasetofm elementsorclasses,
∈ Y Y
through partial feedback.Let us denoteas = y ,...,y this set ofclasses,andassume
1 m
Y { }
that the m elements y ,...,y are indexed by decreasing probability, i.e. p(y ) p(y )
1 m i i+1
≥
for any i<m. The goal is to find the most probable value y of , which we assumeto be
1
Y
uniqueforsimplicity,i.e.
(1) y =argmaxP (Y =y).
1 Y p
y ∼
∈Y
To estimate the mode,we assumethe existenceof independentsamples (Y ) distributed ac-
j
cording to p. However, the practitioner does not directly observe the samples; instead, they
cansequentiallyacquireweakinformationonthem.Formally,ateachtimet N,thepracti-
tionerselectsanindexj NandasetofpotentiallabelsS ,andaskwhe∈ therthesample
t t
∈ ⊂Y
Y belongstoS ornot,leadingtotheobservationofthebinaryvariable
jt t
(2) 1 .
Yjt∈St
We callthe processofcheckingwhetherY S a query,leadingto the following problem
jt
∈
t
description:
p,wP hR eO rB eL eE aM ch1 q. uerD ye 1signas ce aq nue dn ec pe eno dfq ou ne tr hie es p( r1 eY vj it o∈uS st o) bto see rf vfi ac ti ie on nt sly oe fs (t 1imateth )em .odeof
Yjt∈St Yjs∈Ss s<t
Theoverarchingobjectiveistodesignandanalyzeefficientalgorithmsthatoutputagood
estimate of the mode using a minimal number of queries. Although real-world applications
often come with peculiarities, Problem 1 is generic enough that its resolution should shed
lightonalargevarietyofonlinelearningtaskswithpartialfeedback.MODEESTIMATIONWITHPARTIALFEEDBACK 3
Among possible variants of Problem 1, one could forbid the experimenter from asking
more than one query per sample, i.e. forcing j =t, add a contextual variable X that
t
conditions the distribution of 1 , assume that some queries are cheaper to m∈ akX e than
Y S
others,orconsidercaseswhereran∈ domnoiseaffectstheobservations1 .Ratherthanthe
Y S
∈
mode y of the distribution p, we may wantto identify insteadone or all the classes y such
1
that p(y) p(y ) ǫ for some ǫ>0. Finally, more structure could be added to the set ,
1
≥ − Y
rather than let it be a collection of unrelated classes without any meaningful interactions.
In particular, the queried sets S could be restricted to belong to some predefined collection
of sets –e.g., each class in represents an animal species, and you can ask whether the
S Y
animalY isafeline,butnotwhetheritbelongsto cat,wolf,commonsnappingturtle .More
generally,theobservationof1 couldbereplac{ edbyarandomvariableF(Y,S)fo} rsome
Y S
∈
functionF,sothatF(Y,S)neednotbediscreteorequalto F(Y,S).
y S
Problem 1 and its variations appear in many contexts. A na∈tural illustration would be a
P
content-providing service, such as a social network app with a scrolling-centric interface,
thattries toidentifywhichtypeofcontenttheuserismostlikely tolikefromacollection
Y
of possibletopics.The appshowsa batchof postsorvideosto the user(which corresponds
to the choice of a set S), and receives some measure of user satisfaction in return (such as
their scrolling time); from this information, the app must design future batches of content
andfindtheuser’sfavouritetopics.Anothernaturalexamplecomesfromadvertising:ahotel
can composeonline ads using variouscombinations of pictures from a set of photos of a
given room. Each combination corresponds to a set S, and the variable 1 Y is equal to 1
Y S
∈
when online visitor Y clicks on the ad. The hotel tries to identify which pictures maximize
thechancesoftheadbeingclicked.Similarsituationscanalsoariseinexperimentalsciences:
a biologisttries to understandwhichgenesy in acollection ofgenesconsideredhavethe
Y
strongesteffectonacertainproperty.Tothatend,theycantestwhetheractivatingasetS of
genesresultsintheexpressionoftheproperty.
1.2. Related Work. In terms of related problems, Problem 1 bears resemblances to
the active labeling framework introduced by Cabannesetal. (2022). This framework was
motivated by dynamic pricing (Cesa-Bianchi,CesariandPerchet, 2019), active ranking
(JamiesonandNowak, 2011; Braverman,MaoandPeres, 2019), andhierarchicalclassifica-
tion (Cesa-Bianchi,GentileandZaniboni, 2006; GangaputraandGeman, 2006). Addition-
ally,aclearconnectioncanbedrawnwith thewell-studiedtaskofbest-armidentificationin
multi-armed bandit settings (Bubeck,MunosandStoltz, 2010),1 particularly in the context
of combinatorial bandits (Chenetal., 2016) and even more precisely of transductive linear
bandits(Fiezetal.,2019).
In terms of techniques, our algorithms draw their inspirations from the entropic coding
schemesofHuffman(1952)andVitter(1987),theeliminationalgorithmofEven-Dar,MannorandMansour
(2006), together with the doubling trick of Aueretal. (1995). Finally, we notice that the
expectation-maximum (EM) algorithm of Dempster,LairdandRubin (1977) was notably
motivated by the estimation of probability from partial observations, although we did not
pursuetheirapproximateBayesianapproach.
Philosophically,ourapproachwasmotivatedbythefactthatKolmogorovcapacity,hence
entropy coding, lies at the heart of statistical learning theory (see, e.g., CuckerandSmale
(2002),aswellasGrünwald(2007)),suggestingthatacontextualversionofoursetupmight
provideinsightfulperspectivesonactivelearning.
1Theconnectionisapparentifweforceit=tandSttobeasingletonforeveryt N.
∈4
ExhaustiveSearchAlgorithm3
2 2
∆−2 +∆−2 log2(m)
↓
AdaptiveSearchAlgorithm4 EliminationAlgorithm8
∆−22 +∆−22 (cid:16) Pi ≥1p(yi) |log 2p(yi) |(cid:17) −→ ∆−22 + Pi ≥1p(yi)∆−i 2 |log 2p(yi) |
↓ ↓
TruncatedSearchAlgorithm7 SetEliminationAlgorithm9
∆−22 +∆−22 |log 2p(y1) | −→ ∆−22 + (cid:16) Pi ≥1p(yi)∆−i 2 (cid:17)|log 2p(y1) |
TABLE1
Summaryofthecoefficientαasper(4)foundfordifferentmethods,uptouniversalconstantswhenp(y1)is
boundedawayfrom1,andwith∆1=∆2.Thearrowsindicateimprovements.
1.3. Performance Metrics. Various performance metrics can be applied to algorithms
thatoutputaguessyˆofthemodeofpafteracertainnumberofqueries.Wewillfocusonthe
probabilityoferror,i.e.,ontheminimalδ suchthat
(3) E[1 ]=P(yˆ=y ) δ,
yˆ=y 1
6 1 6 ≤
as a function of the number of queries T. Here, the randomness is inherited from that of
the samples (Y ) and of the algorithm. Equation (3) can be seen as the “risk” associated
j
with the “zero-oneloss”,which is usedin classificationproblems.Other reasonablemetrics
includetheexpecteddifferenceE[p(y ) p(yˆ)],ortheprobabilityofǫ-approximatesuccess
1
P(p(yˆ) p(y ) ǫ). Despite those vari− ations, the principles behind our algorithms can be
1
≥ −
readilyadaptedtodifferentmetrics.
Among additionalnuances,in some contexts,a usermight fix in advancea “confidence”
levelδ,andbeinterestedinalgorithmsthatminimizethe(expected)numberofqueriesT to
achieve it. Reciprocally, one might fix a “budget” of queries T, and design algorithms that
minimize the probability δ of false prediction using those queries. These settings typically
leadtoequationsoftheshape
(4) E[T] ln(C /δ)α , orrespectively δ C exp( T/α ),
1 1 2 2
≤ ≤ −
wheretheconstantsα andC dependontheunknowndistributionp.Onceagain,algorithm
i i
design principlesare often agnosticto the settings differentiation, usuallyallowing the con-
version of one type of boundinto the other. To offer quantitative comparisons,we focus on
the joint asymptoticbehaviorwith respectto δ and T, whichis determinedby theconstants
α, removingC fromthepicture.Thisprovidesasimplemetric:thesmallerα,thebetterthe
algorithm.
Note that our proofs also yield guarantees on the sample complexity of our algorithms;
however,the expectednumberof queries requiredis a more relevantquantity, asit captures
both the sampleefficiencyofanalgorithm andtheefficiencywith which itextractsthe nec-
essaryinformationfromeachsample,whichisakeyaspectofoursetting.
1.4. Summary of Contributions. Our first contribution is to introduce a new problem,
Problem 1, which captures key aspects of most online learning tasks with partial feedback.
Itissimpleenoughtoallowforrigoroustheorytobedeveloped,yetnaturallygeneralizesto
match practical scenariosfrom the real world. We introduce severalimportant ideas to effi-
cientlysolveProblem1,whichleadtoincreasinglyrefinedalgorithms,whoseperformances
canbecomparedintermsofthecoefficientαintroducedin(4).AnoutlineisprovidedinTa-
ble 1, up to universalmultiplicative constants.Thougheach algorithm is discussedin detail
laterinthearticle,letusintroducethemhere.MODEESTIMATIONWITHPARTIALFEEDBACK 5
Themostnaiveone,theExhaustiveSearchAlgorithm3,consistsinfullyidentifyingeach
sample Y through binary search. At any time t, it outputs the most frequent class among
j
the identified samplesasan estimation ofthe true mode.Fora tolerated probability of error
δ (0,1] anda classy , a numberofsamplesproportionalto ∆ 2ln(1/δ) is neededto
∈
i
∈Y
−i
correctlyidentifyy asthemodeamong y ,y ,where
1 1 i
{ }
(5) ∆2:= ln 1 ( p(y ) p(y ))2 .
i − − 1 − i
Unsurprisingly, ∆ 2 is increasing w(cid:16) ith p(yp ) [0,p(yp )) and g(cid:17) rows infinitely large when
−i i
∈
1
p(y )getclosertop(y ).TheprobabilityoferrorofAlgorithm3isdominatedbytheproba-
i 1
bility of mistakenlypickingthe secondmostlikely candidatey as themode,leadingto the
2
asymptoticperformanceα=∆ 2 log (m) ,asdetailedinSection3.Itcanbeimprovedby
−2 ⌈ 2 ⌉
identifyingnewsampleswithanentropy-baseddichotomicsearchthatusesalearnedempir-
ical distribution pˆon , which yields Algorithm 5. Asymptotically, it replacesthe log (m)
Y 2
queries per sample of Algorithm 3 by an expected number of queries equal to the entropy
H(p):= p(y ) log (p(y )) log (m) of p, plus one query per sample due to some
i 1 i | 2 i |≤ 2
boundary eff≥ects, as reflected in Table 1 and as explained in Subsection 3.3. Showing how
P
onlinelearningwithpartialfeedbackbenefitsfromentropycodingisoursecondcontribution.
TherearetwodisjointwaystofurtherimproveAlgorithm5.Thefirstoneexploitsthemain
characteristicof ourproblem:the possibility ofaskingforpartial information on samplesat
a lower cost than for complete identification. In particular, when searching for the mode
byidentifyingsampleswithentropy-basedtechniquesusingHuffmantrees,onecanstopthe
identificationprocedureatroughlythedepthofthemodeinthetree,asclassesthataredeeper
in the tree are unlikely candidates. This idea leads to the design of the Truncated Search
Algorithm 7, detailed in Section 4. It reduces the asymptotic number of queries per sample
from a constantplus the average depth of leaves in a Huffman tree H(p) to a constantplus
the minimal depth log (p(y )) , as seen in Table 1. The second amelioration comes from
| 2 1 |
the adaptation of bandit algorithms to our setting. We develop in Section 5 the Elimination
Algorithm 8, that discards mode candidates as soon as they seem unlikely to be the true
mode.Theclassy canbeeliminated afterroughly ∆ 2ln(1/δ) samples,leadingto an
i
∈Y
−i
asymptoticperformanceα ∆ 2+ ∆ 2p(y ) log (p(y )) ,withtheabuseofnotation
≃ −2 i 1 −i i | 2 i |
∆ 2:=∆ 2. ≥
−1 −2 P
While Algorithms 7 and 8 are both improvements over the Adaptive Exhaustive Search
Algorithm5,neitherisstrictly betterthantheother.Whenp(y ) p(y )isverysmallcom-
1 2
−
pared to the other gaps p(y ) p(y ), the advantage goes to the Elimination Algorithm,2
1 i
−
whileitgoestotheTruncatedSearchAlgorithmwhentherearemanyy withsmallmass
∈Y
p(y) p(y ) and p(y ) p(y ) is not too small.3 We combine the ideas of each algorithm
1 1 2
≪ −
andgetthebestofbothworldswiththeSetEliminationAlgorithm9,presentedinSection6,
which can be understood either as grouping together low mass classes before applying an
elimination procedure to the resulting partitions, or as refining the truncated search proce-
dure by taking into account confidence intervals. This is reflected in the corresponding co-
efficientα=∆ 2+ ∆ 2p(y ) log (p(y )) , wherethenumberofsamples∆ 2 isas
−2 i 1 −i i | 2 1 | −i
fortheEliminationAlgo≥rithm,whiletheexpectednumberofqueriesneededforeachsample
P
1+ log (p(y )) comesfromthe TruncatedSearchAlgorithm, resulting in the bestasymp-
| 2 1 |
toticperformanceamongtheproposedalgorithms.ThissophisticatedsolutiontoProblem1,
togetherwithitsimplementationavailableonline,isourfinalcontribution.
2Considerthedistributionp(y1)=2/m,p(y2)=2/m 1/m2 andp(y)=(1 p(y1) p(y2))/(m 2)
for 3a Cll oo nth sie dr ey rt∈ heY d= istr{ iy b1 u, ti. o. n., py (m y1} ). =Th 1e /n 2α aE nd= p(P y)i ≥ =1 1∆ /(−i 2,− ∗2 (mp(y i) 1| )l )og fo2 r( ap l( ly oi t) h) e| r= y− O(α T =S/− ym 1) ,.a .s .,m ym→ .∞ T− .
hen
α
E
→∞whileα TS=
Pi
≥1∆−22 p(yi) |log 2(p(y1)) |=O(1− )asm →∞. ∈Y { }6
Inconclusion,ourmaincontributionsaresummarizedasfollows.
1. Introducingtheproblemofmodeestimationwithpartialfeedback,Problem1.
2. Unveilinglinksbetweenadaptiveentropycodingandactivelearning.
3. Combiningadaptiveentropycoding,coarsesearchproceduresandbandits-inspiredprin-
ciplesintoAlgorithm9,anintuitiveyetefficientsolutiontoProblem1.
Last but not least, we provide a code base to help researchers advance our knowledge of
weaklysupervisedonlinelearning,availableatwww.github.com/VivienCabannes/mepf.
2. TheEmpiricalModeEstimator. Inthefirstpartofthisarticle,wewilldesignmode
estimationalgorithmsthatfullyidentifyeachsampleY oneaftertheother.Giventheidenti-
j
ficationofnsamples,thosealgorithmsestimatethemodeofpastheempiricalmodeamong
thensamples,
(6) yˆ :=argmax 1 ,
n Yj=y
y
∈Y j X∈[n]
with ties broken arbitrarily. A tight characterization of the performance of the estimator (6)
isofferedbythefollowingtheorem.
THEOREM 1 (Empirical mode performance). Let (Y j)
j [n]
be n independentvariables
sampledaccordingtop P( ).Thentheprobabilityoferro∈rof (6)satisfies
∈ Y
(7) exp n∆2 mln(n+1) c P(yˆ =y ) exp n∆2 .
− 2− − p ≤ n 6 1 ≤ − 2
where∆ 2 isdefine(cid:0)dinEq.(5),andc p issome(cid:1)constantthatdepends(cid:0)onp. (cid:1)
Whenaccessingnsamples,theperformanceofempiricalmodeyˆ cannotbebestedwith-
n
outadditionalinformationonp,whichleadstothefollowingcorollary.
COROLLARY 2 (Minimax lower bound). For any distribution p
0
P( ), and any al-
∈ Y
gorithm thatpredictsyˆ:= ((Y ) ) basedon n observations(Y ), thereexistsa per-
j j [n] j
mutationA σ S such that,A when th∈e data are generated according to p P( ) defined
m
∈ ∈ Y
throughtheformulap(y)=p (σ(y)),thelowerbound(7)holdsforthisalgorithm.
0
Asoneneedsatleastasinglequerypersampletogainanymeaningfulinformation,Corol-
lary 2 statesthatthenumberofqueriesT needtobegreaterthan ∆ 2ln(1/δ), upto higher
−2
order terms, to reach precision δ as defined by Equation (3). The main challenge is thus to
getasclosetothislaxlowerboundaspossible.
2.1. ProofofTheorem1. In thissubsection,weproveTheorem1. Wefollowa proofof
Sanov’stheoremduetoCsiszárandKörner(2011),togetherwithanexplicitcomputationof
an information projection. Let us partition all possible sequences of observations (Y )
j j [n]
accordingtotheirempiricaldistributiondefinedas ∈
pˆ (y):=n 1 1 .
(Yj) − Yj=y
j [n]
X∈
Todoso,wedefineforanysuchempiricaldistributionq P( ) n 1 N thetypeclass
− Y
∈ Y ∩ ·
(q)= (y ) n y ; pˆ (y)=q(y) .
T
t
∈Y ∀ ∈Y
(yt)
(cid:8) (cid:12) (cid:9)
(cid:12)MODEESTIMATIONWITHPARTIALFEEDBACK 7
Theevent yˆ =y istheunionoverallthevaluesthatpˆcantakeoftheevents“pˆdoesnot
n 1
{ 6 }
havetherightmode”,whichwecanwrite asaunionofdisjointeventsaccordingtothetype
of(Y ),whichleadstothebound
j
P (Y ) (q) P (yˆ=y ) P (Y ) (q) ,
(Yj) ∼p j
∈T ≤
(Yj)j∈[n]
6
1
≤
(Yj) ∼p j
∈T
q ∈XQn,− (cid:0) (cid:1) q ∈XQn,+ (cid:0) (cid:1)
whereweaccountforthecaseswherepˆhasseveralmodesbydifferentiating
(8) = q P( ) n 1 N y /argmaxq(y) ,
n, − Y 1
Q − ∈ Y ∩ · ∈
and (cid:8) (cid:12) (cid:9)
(cid:12)
(9) = q P( ) n 1 N argmaxq(y)=argmaxp(y) .
n,+ − Y
Q ∈ Y ∩ · 6
We would like to e(cid:8)numerate the different(cid:12)classes in the previous sums,(cid:9)as well as their
(cid:12)
probability.Afewlinesofderivationsleadtotheequality,
P((Y j)=(z j))=2 −n(H(pˆ (zj))+D(pˆ (zj)kp)).
Here,D istheKullback-Leiblerdivergence,andH theentropy
q(Y)
D(q p)=E [log ( )], H(q)=E [ log (q(Y))].
k Y ∼q 2 p(Y) Y ∼q − 2
Asaconsequence,usingtheexchangeabilityofthe(Y ),
j
P((Y ) (q))=P(pˆ =q)= (q) 2 n(H(q)+D(q p)).
j
∈T
(Yj)
|T |
− k
Weareleftwiththecomputationofthecardinalityofeach (q).Thisisnothingbut
T
n n!
(q) = = .
|T | (nq(y)) (nq(y))!
(cid:18) y ∈Y(cid:19) y
∈Y
This cardinality can be bounded with probabilistic aQrguments, as shown in Theorem 11.1.3
(whichis12.1.3inthe2ndedition)ofCoverandThomas(1991),4
(n+1) m2nH(q) (q) 2nH(q).
−
≤|T |≤
Collectingthedifferentpiecessofar,wereachtheconclusion,
(n+1) m 2 nD(q p) P(yˆ=y ) 2 nD(q p).
− − k 1 − k
≤ 6 ≤
q ∈XQn,− q ∈XQn,+
Fromthere,usingthefactthatthecardinalityof isatmost(n+1)m,wegettherough
n,+
Q
bound
(10) (n+1) m max 2 nD(q p) P(yˆ=y ) (n+1)m max 2 nD(q p).
− − k 1 − k
q ∈Qn,− ≤ 6 ≤ q ∈Qn,+
It is useful to define the “limit” = q P( ) argmaxq(y)=argmaxp(y) of the sets
Q { ∈ Y | 6 }
and .Weareleftwiththecomputationoftheso-called“informationprojections”
n, n,+
Q − Q
min D(q p)andmin D(q p).
q ∈Qn,− k q ∈Qn,+ k
4Slightlytighterboundscanbederivedfromthefactthat,asprovenbyRobbins(1955),
k!=√2πkk+1/2 e−k+rk,
with 0
≤
12k1
+1
≤r
k ≤
121
k
≤1.8
LEMMA 1. For any distribution p P( ), there exists a constant c
p
such that for any
n Nwith and definedbyE∈ quatY ions(8)and(9),
n, n,+
∈ Q − Q
∆2 c ∆2
2 + p min D(q p) min D(q p) minD(q p)= 2 .
ln(2) nln(2) ≥q ∈Qn,− k ≥q ∈Qn,+ k ≥ q ∈Q k ln(2)
ThislemmaisprovedinAppendixA.1.CombiningitwithEquation(10),wefindthat
exp n∆2 c mln(n+1) P(yˆ=y ) exp n∆2+mln(n+1) .
− 2− p − ≤ 6 1 ≤ − 2
Finally,wenotethatanargumentofDinwoodie(1992)showsthatln(P(yˆ =y))/nisalways
(cid:0) (cid:1) (cid:0) n (cid:1)
6
belowitslimitprovedbySanov(1957),whichyieldsthestrongerbound
P(yˆ=y ) exp( n∆2)
6 1 ≤ − 2
whichwereportedinTheorem1.
2.2. Minimax Optimality. This subsection proves Corollary 2. To turn Theorem 1 into
theminimaxCorollary2,wecanuseaprioronpsuchthattheoptimalBayesianalgorithmis
themodeestimationalgorithm.Todoso,letusendowthesimplexP( ) withadistribution
Y
D andtrytominimize
( )=E E 1 p ,
E A
p ∼D (Yj) A((Yj)) 6=y p∗
where ((Y )) is the mode predicted byh an alhgorithm u(cid:12) poiniobserving the independent
j (cid:12)
sampleA s(Y )generatedaccordingto p P( ),andy isA the(cid:12)modeofp.Tofindtheoptimal
j
∈ Y
p∗
algorithm ,wecaninverttheexpectationas
∗
A
( )=E E 1 (Y ) .
E A
(Yj) p ∼D A((Yj)) 6=y p∗ j
Thisleadstotheoptimalalgorithm h h (cid:12) ii
(cid:12)
(cid:12)
((Y ))=argminE 1 (Y ) =argmaxP y =y (Y ) .
A∗ j
y
p y 6=y p∗ j
y
p ∼D p∗ j
Itfollowsfrom ( ) (∈Y ∗)thah ttheree(cid:12) (cid:12) (cid:12)xistsi atleast∈ oY nedistribu(cid:0) tionpin(cid:12) (cid:12)thes(cid:1) upportofD
E A ≥E A
suchthattheerrormadebyanyalgorithm onthisdistributioncannotbebetterthattheone
A
madefor ,i.e.,
∗
A
E 1 p E 1 p .
(Yj) A((Yj)) 6=y p∗
≥
(Yj) A∗((Yj)) 6=y p∗
Onecandefineapriorsuchhthat corr(cid:12) esipondstothheempiricalm(cid:12) oide,i.e., ((Y ))=yˆ .
∗ (cid:12) (cid:12) ∗ j n
In particular, Corollary 2 followsA from t(cid:12)aking D as the uniform d(cid:12)istributionA over the set of
permutations of the given distribution p , i.e., S p = p σ S ,p=σ p . Since
0 m 0 m # 0
· { |∃ ∈ }
any algorithm has to have its expected performance over the distribution p bounded by the
one of , for any algorithm, there exists at least one distribution p S p such that its
∗ m 0
A ∈ ·
performanceisnobetterthantheoneof .
∗
A
AsideonUser-KnowableBounds. Fora userthatdoesnotknow ∆ , theboundofThe-
y
orem 1 is not practically helpful, and the practitioner might be interested in stronger for-
mal guarantees. In particular, Valiant (1984) introduced the notion of (1 δ)-probably ǫ-
approximately correct estimator, for some ǫ,δ >0, which reads
P(p(yˆ)<−
p(y ) ǫ) δ.
1
− ≤
Proofs based on concentration inequalities can usually be reworked to derive such (ǫ,δ)-
PAC bounds by replacing some quantity ∆, related to differences p(y) p(y ), by ǫ in the
1
−
boundontheerror δ. Thisis notablythecaseforTheorem1.Onecanalsoestimate ∆ˆ and
y
use plug-in techniques. The second half of this paper will provide algorithms, namely the
Elimination andSetElimination Algorithms, suchthatthe userknows,andfurthermore can
choose, the probability δ with which the algorithm will fail to output the true mode –in the
banditliterature,suchalgorithmsarecalled(0,δ)-PAC.MODEESTIMATIONWITHPARTIALFEEDBACK 9
3. Exhaustive Dichotomic SearchProcedures. This section introducesbaselinealgo-
rithms based on exhaustive search procedures. To leverage the empirical mode estimator, a
naivebaselineconsistsinfullyidentifyingeachsampleY oneaftertheother.Usingabinary
j
search,thiscanbedonewith log (m) queries(2)persample.Wecanthusfullyidentifyn
⌈ 2 ⌉
sampleswith T =n log (m) queries.TogetherwithTheorem1,wededucethatthisnaive
⌈ 2 ⌉
baselinegetstoprecisionδ asdefinedbyEquation(3)withanumberofqueriesboundedby
T =∆ 2 log (m) ln(1/δ). Our first proposedimprovement overthis baseline is to refine
−2 ⌈ 2 ⌉
binary searches through entropy codes in order to minimize the average number of binary
questionsaskedto fully identifyeachsampleY ,reducingthenumberofqueriespersample
j
from log (m) toaquantityclosetotheentropyH(p)ofp(Shannon,1948).
2
⌈ ⌉
3.1. Entropy Coding. In order to identify each sample Y with the least amount of
j
queries, we will use binary search algorithms stemming from coding theory. This section
fixesterminologyandaddsself-containeddetailsonthematter.
DEFINITION 1 (Binary Treeetc.). Fora set ofelements , we defineleavesas abstract
Y
objectsV forally .Fromleaves,wedefinenodesV asabstractobjectsassociatedwith
y
∈Y
arightchildV =r(V)andaleftchildV =l(V)whichareeithernodesorleaves.Avertex
1 2
is an abstract object V which is either a node or a leaf. A vertex V is a descendant of V ,
1 2
denotedbyV ⊳V ,ifitcanbebuiltfromthecompositionV =s s s s (V )for
1 2 1 1 2 3 d 2
d N ands r,l ThedescendantsofanodeV areallthevertic◦ est◦ hatc◦ a· n· b·◦ ebuiltfrom
i
∈ ∈{ }
thecompositionV ⊳V.Abinarytree = (R)isdefinedfromarootnodeRwithafinite
′
T T
number of descendantsforming a collection of vertices, such that each V in this collection
of vertices is definedby a uniquepath V =s s s s (R), with d=D (V) N
1 2 3 d
beingknownasthedepthofV in . ◦ ◦ ◦···◦ T ∈
T
Abinarytreeisassociatedwithaprefixcode.
DEFINITION2(Vertexcode). Thecodec (V) 0,1 D T(V)ofvertexV in isdefined
T ∈{ } T
as the unique path to go from the root node of to V by reading c (V) and recursively
T T
advancingtotherightchildifreadinga1andtotheleftchildifreadinga0.
Theconceptofbinarytreeisgraphicallyintuitive.Letusprovideanexampleofatree,and
annotateeachnodeinthetreewithitscodeasperDefinition2.Inthefollowingillustration,
eachleft(resp.right)childofanodearerepresentedbelowthenodeontheleft(resp.onthe
right). We emphasizethe separationbetweenright andleft with a verticalbar. Forexample,
ifV =r l l r(R),thenc (V)=1001.
◦ ◦ ◦ T
Root Node
Leaf: 0 | Node: 1
| Node: 10 | Leaf: 11
| Node: 100 | Leaf: 101 |
| Leaf: 1000 | Leaf: 1001 | |
Prefix codes have been heavily studied in information theory, where the goal is to trans-
formaset inordertodescribeitselementsywithasequenceofbitsc(y).Inparticular,the
Y
minimal number of bits transmitted when encodingsequencesof tokens (Y ) n into the
j
∈Y
concatenationofthecodes(c(Y )) islinkedwiththeentropyoftheempiricaldistribution
j j [n]
ofthey in(Y ).Thereisawel∈lknownfundamentallimit, foranyp ,
j
∈Y ∈Y
minE [D (Y)] H(p):=E [ log p(Y)],
Y ∼p T ≥ Y ∼p − 2
T10
where D (y) is seen as the length of the code c(y), the expectation is seen as the average
T
code length, and the minimization as the search for the best coding algorithm to minimize
messagelength.
A simple algorithm to obtain optimal codes was found by Huffman (1952) –we repro-
duce it in Algorithm 1. It takes as input a set of elements such that some positive value
Y
v(y) is associated to each y . It outputs a tree whose leaves are the elements of and
∈Y Y
whose vertices also come associated to values, and which satisfies certain conditions with
respect to those values. When the value v(y) of element y is equal to p(y) for some
distribution p P( ), which is assumed in Algorithm 1, the∈ n Y the prefix code of the result-
∈ Y
ing tree on the elements of is optimal with respect to the distribution p, i.e. it minimizes
min E [D (Y)](seeHY uffman,1952).
Y p
T ∼ T
TodescribeAlgorithm1succinctly,weintroducetwonotionsofvertexorderings.Thefirst
comesfromthevaluev(y)associatedbyhypothesistoeachy ,whichwouldtypicallybe
∈Y
p(y),N(y)orN(y)/n,respectivelytheprobabilityofyforsomedistributionp,theempirical
occurrence counts of y for some set of samples, or its empirical probability. Given a tree
whoseleavesV areinbijectionwiththeelementsof ,letusassociatetoV thevalueofthe
y y
Y
corresponding y , i.e. v(V )=v(y). The value of a node is then defined recursively as
y
∈Y
the sumofits children valuev(V)=v(r(V))+v(l(V)). This leadsto the following notion
ofordering.
DEFINITION 3(Nodeordering). TwoverticesV
1
andV
2
satisfythepartialorderingV
1
⊢
V ifv(V )<v(V )orifv(V )=v(V )andV isanodewhileV isaleaf.Thetwovertices
2 1 2 1 2 2 1
areequivalent,whichwewriteV V ,ifV V andV V .
1 2 1 2 2 1
∼⊢ 6⊢ 6⊢
Thesecondorderingordersthenodesfrombottomtotop,andlefttoright.
DEFINITION 4 (Code ordering). Two vertices V
1
and V
2
in a tree satisfy the total
T
ordering V < V if D (V ) > D (V ) or if D (V ) = D (V ) and c (V ) < c (V )
1 2 1 2 1 2 1 2
T T T T T T T
withrespecttothelexicographicalorder.
Onceagain,thisisahighlyvisualconcept.Letusnumbertheverticesoftheprevioustree
accordingtothetotalorderingfromDefinition4.
Root Node: 9
Leaf: 7 | Node: 8
| Node: 5 | Leaf: 6
| Node: 3 | Leaf: 4 |
| Leaf: 1 | Leaf: 2 | |
Data:Setofelements endowedwithaprobabilitydistributionp P( ).
Y ∈ Y
CreateverticesVy foreachy withvaluev(Vy)=p(y);
∈Y
Sortallthenodeintoaheap accordingtothecomparison (Definition3);
S ⊢
while hasmorethanoneelementdo
PoSpV1,V2therespectivesmallestelementsin ;
S
MergethemintoaparentnodeV withV1astheleftchildandV2therightone;
InsertV intotheheap withitsvaluev(V)=v(V1)+v(V2);
S
end
SettheremainingnodeV intheheap astherootnodeof = (V);
S T T
Result:Huffmantree
T Algorithm1:HuffmanSchemeMODEESTIMATIONWITHPARTIALFEEDBACK 11
Let us illustrate Algorithm 1 with the count vector N =(69,14,8,6,3). At the first iter-
ation, we set all nodes in a heap. Let us represent this heap as a sorted list with comma-
separatedelements.
Leaf 3; Leaf 6; Leaf 8; Leaf 14; Leaf 69;
Then we merge the two smallest onesand add the result in the heap.Let us picture descen-
dantsbelowthenodes.
Leaf 8; Node 9; Leaf 14; Leaf 69;
Leaf 3 | Leaf 6
Weiteratetheprocess.
Leaf 14; Node 17; Leaf 69
Leaf 8 | Node 9
Leaf 3 | Leaf 6
Nowtheheapisonlymadeofanodewithvalue31andaleafwithvalue69.
Node 31; Leaf 69;
Leaf 14 | Node 17
Leaf 8 | Node 9
Leaf 3 | Leaf 6
Weendupwithabinarytree.
Node: 100
Node: 31 | Leaf: 69
Leaf: 14 | Node: 17 |
| Leaf: 8 | Node: 9 |
| | Leaf: 3 | Leaf: 6 |
In this paper,we introducealgorithms thatbuild andadaptHuffman treeswith respectto
a distribution p that is simultaneously being learnt online. This requires updating the trees
on the fly. We can do so with the rebalancing algorithm of Vitter (1987), which we present
in Algorithm2.Thisalgorithmusesintegervalues(whichcorrespondstoempiricalcounts),
and updatesthe tree in reaction to an increasein value of +1 for a single leaf. Of course,it
canberepeatedlyappliedtoaccountforgreaterchanges.
Data:AHuffmantree withcountsv(V) N.AnodeV.
T ∈
SwapV withthebiggestV1inthesenseof< suchthatV V1;
Updatev(V):=v(V)+1; T ∼⊢
SwapV withthesmallestV2inthesenseof< suchthatV V2;
ifv(V)=v(V2)then T ⊢
UpdatethenewparentofV2bycallingthisalgorithmon( ,V2);
T
else
UpdatethenewparentofV bycallingthisalgorithmon( ,V);
T
end
Result:UpdatedHuffmantree .
ATlgorithm2:VitterRebalancing
OnecanprovethefollowingpropertiesoftheHuffmantreeconstructionandrebalancing.
PROPOSITION 2 (Vitter (1987)). When is built with Algorithm 1 with some initial
value v(V )= 1 >0 (for someT set of samples (Y ) ) and is updated in-
crementally y at eacj h∈[ tt i0 m] eY stj e= py t N with respectto Algorithm 2j aj n⊂ d tY he observation of new
P ∈12
samples Y , the total ordering < (Definition 4) is always compatible with the partial
t
ordering (∈ DY efinition3). T
⊢
Note that some subtleties are to be taken into accountat initialization to dealwith leaves
that have not yet been observed. In coherence with the construction of Vitter (1987), we
consider a special “not yet observed” node which defines a balanced subtree containing all
the unobserved leaves as its descendants. When observing a new element y, we remove it
from this subtree,re-balancethe subtree,and createa new nodewhoseleft child is the “not
yetobserved”nodeandwhoserightoneistheleafcorrespondingtothisy.Thisnewnodeis
setattheformerplaceofthe“notyetobserved”node.
Finally, the following code property will be useful to derivestatistical guaranteesfor our
algorithms.
DEFINITION 5. A codingscheme thatassociatesabinarytree (p) to aprobability
A TA
vectorp P( )issaidtobeC-balancedif c (y) C log (p(y)) forally .
∈ Y | TA(p) |≤ ⌈ 2 ⌉ ∈Y
Shannoncodesarebuiltexplicitlytobe1-balanced(Shannon,1948).However,incontrast
withHuffmancoding,Shannoncodingdoesnotprovidethelowestexpected.Huffmancodes
arenotalways1-balanced,butthefollowinglemma,whichweproveinAppendixA.2,states
thattheyareatleast2-balanced–infact,onecanproveaslightlybetterconstantthan2.
LEMMA 3. Let be a Huffman tree with respect to a value function v on its vertices
T
suchthatv(R)=1,whereRistherootof .ThenforanyvertexV of ,wehave
T T
D (V) 2 log (1/v(V)) .
T ≤ ⌈ 2 ⌉
Inotherwords,Huffmancodesare2-balanced.
3.2. ExhaustiveSearchwithFixedCoding. Asmentionedatthestartofthesection,our
first,andrathernaive,proposedmethodofsolvingProblem1istofullyidentifyeachsample
Y usinga fixedsearchprocedure,i.e., a setof q predefinedquestions (1 ) suchthat
ani
y element y is fully identified by the values of the functions 1
y ∈S .i Si ∈u[ cq h]
a search
procedure an b∈ e mY apped to a prefix code c: 0,1 q that
associay
te∈
sSi
any y to the bi-
nary code c(y)=(1 ) . It can also be mY a→ ppe{ d to} a binary tree, the code describing
branching
propertiesy
t∈
oSi rei a∈c[q h]
the leaf y from the root node. Each question can be seen as
eliciting one bit in the code of Y , or equivalently going down one node in the correspond-
j
ingtree.Reciprocally,acode(c(y) ) isassociatedwiththesearchprocedureconsidering
j j [q]
S = y c(y) =1 .Insimplete∈rms,S enumeratesallelementswhosecodeccontains
j j j
{ ∈Y| }
a1inthej-thposition.ThisstrategyforsolvingProblem1isformalizedinAlgorithm3.The
averagenumberofquestionsaskedbysuchasearchprocedurereads p(y)c(y) where
y | |
c(y) is the length of the code of y, or equivalently the depth of y in th∈eYassociated binary
| | P
tree. Consequently, if the predefined search procedure is a simple binary search, the length
ofthecodeofanyelementisatmost log (m) ,andTheorem1guaranteesthatweneedat
mostT =ln(1/δ)α
querieswithα=∆⌈ 22 log⌉
(m) toboundtheprobabilityoferrorbyδ.
−2
⌈
2
⌉
3.3. Exhaustive Search with Adaptive Coding. Ideally, we would like to apply Algo-
rithm 3 with a code c that is optimal with respect to the distribution p. As p is not known
a priori, we need to learn the code on the fly. This leads to Algorithm 4, where a code is
adapted iteratively to bestreflect the current estimate pˆof p basedon past observations.As
shown by the following theorem, the need to learn a code online does not impact too much
thecomplexityofAlgorithm5incomparisontothatofAlgorithm3withanoptimalcode.MODEESTIMATIONWITHPARTIALFEEDBACK 13
Data:Setofclasses = y1,...,ym endowedwithp P( ).Acodec: 0,1 d(bydefault,we
Y { } ∈ Y Y→{ }
letd= log (m) andc(y )bethebinaryrepresentationofthenumberiwithzerosinfront)
2 i
⌈ ⌉
forj [n]do
∈
GetnewsampleYj pandqueryc k(Yj)fork 1,...,length(c(Yj)) untilYj isidentified;
∼ ∈{ }
end
Setyˆ=argmax N(y),whereN(y)= 1 ;
Result:Estimatedy ∈ mY
odeyˆ=argmax
P 1j ∈[n] Y oj f= py
.
Algorithm3:P
Fj i∈x[ en d] CY oj= diy
ngExhaustiveSearch
Data:Setofclasses = y1,...,ym endowedwithp P( ).
Y { } ∈ Y
InitializeN(y)=0forally ;Acodingscheme ,aninitialtree ;
∈Y A T
forj [n]do
G∈etnewsampleYj pandidentifyitbyqueryingtheentriesofc ;
UpdateN(Yj)=N∼ (Yj)+1,and = (pˆ)wherepˆ(y)=N(y)T/j;
T TA
end
Setyˆ=argmax N(y);
y
Result:Estimated∈mYodeyˆ=argmax 1 ofp.
Algorithm4:AP
dj a∈p[ tn i] veY Cj= oy
dingExhaustiveSearch
Data:Setofclasses = y1,...,ym endowedwithp P( ).
Y { } ∈ Y
InitializeN(y)=0forally ;Set aHuffmantreewithVy=N(y);
∈Y T
forj [n]do
∈
GetnewsampleY pandidentifyitbyqueryingtheentriesofc (Y );
j j
UpdateN(Yj)=N∼ (Yj)+1,andtheHuffmantree withAlgoriTthm2;
T
end
Setyˆ=argmax N(y);
y
Result:Estimated∈mYodeyˆ=argmax 1 ofp.
Algorithm5:AdaptiveCoP dij n∈g[n E] xY hj a= uy
stiveSearchwithHuffmanCoding
THEOREM 3 (Adaptive entropic search performance). Given a C-balanced coding
scheme(Definition5),inordertofullyidentifynsamples(Y )followingtheadaptivecoding
j
strategyofAlgorithm4,oneneedsonaverageE[T]queries,where
(11) nH(p) E[T] Cn(H(p)+1)+28Cmlog (n)+6Cm+m2,
≤ ≤ 2
andH(p):= p(y)log (p(y)) istheentropyofthedistributionp P( ). Inpartic-
− y 2 ∈ Y
ular,inthecaseofH∈Yuffmancoding,Algorithm5yields
P
nH(p) E[T] 2n(H(p)+1)+o(n).
≤ ≤
AsshownbyTheorem3,toreachaprobabilityoferrorsmallerthanδ,Algorithm4needs
T =ln(1/δ)α querieswith α=C∆ 2(H(p)+1) upto higher-orderterms, with C =1 for
−2
Shannoncodingand C =2 forHuffmancoding.Thisis animprovementinmostcasesover
the coefficient α=∆ 2 log (m) of Algorithm 3. Nonetheless, both Algorithms 3 and 4
−2 ⌈ 2 ⌉
preciselyestimate p(y) for eachclass y, which is moreinformation than neededif one only
wantsthemodeofp,leavingroomforimprovements.
3.4. Proofs. ThissubsectionisdevotedtotheproofofTheorem3.Itiswellknownfrom
informationtheorythatonehastoaskatleastH(p)questionsonaveragetobeabletoidentify
Y p(Shannon,1948),whichexplainsthelowerbound.Weneedtoprovetheupperbound.
∼14
RecallthatAlgorithm4 identifieseachY byfollowing a C-balancedtreewith respectto
i
the empirical distribution pˆ of the previouslyobservedsamples (Y ) as values.Let
i j j i
≤ ⊂Y
us denote by T the number of queries made to identify all the (Y ) for i n. At round i,
n i
≤
oursearchprocedureassociatedwiththeprobabilitydistributionpˆ identifieseachelementy
i
in such that pˆ(y)=0 with at most C log (pˆ(y)) queries as per Definition 5. When
Y i 6 ⌈− 2 i ⌉
y is suchthat pˆ(y)=0, we identify it with at most m queries. Hence,atround i+1 [n],
i
∈
Y pisidentifiedwithatmostthefollowingnumberofquestionsonaverage
i+1
∼
C+E 1 Clog (pˆ(y))+1 m ,
y ∼p
−
pˆi(y) 6=0
·
2 i pˆi(y)=0
·
where pˆ
i
is our estimate of p(cid:2)after the complete identification of Y i, he(cid:3)nce the one used for
the querieson Y . Notethatin this setting, foreachy , there is atmosta singleround
i+1
∈Y
whereweneedtoidentifyY =y whilepˆ (y)=0.Recursively,thisleadsto
i i 1
−
n 1
−
E [T ] Cn C E p(y)1 log (pˆ(y)) +m2.
(Yi)i≤n n ≤ − (Yj)  pˆi(y) 6=0 · 2 i 
i=1 y
X X∈Y
 
Letusassumewithoutlossofgeneralitythatp(y)isnever0,asthetermsforwhichp(y)=0
do not contribute to the sum. Now we split this equation into two parts: a first part where
one does not have a tight control of the empirical distribution, but that is really unlikely
and will not contribute much to the full picture; and a part where the empirical distribution
concentratestowardsitsrealmean,
n 1 n 1
− E p(y)1 log (pˆ(y)) = − p(y)E 1 log N y,i
− (Yj)  pˆi(y) 6=0 2 i  − (Yj) Ny,i6=0 2 i
i=1 y i=1y (cid:20) (cid:18) (cid:19)(cid:21)
X X∈Y XX∈Y
 
i i
= p(y)E 1 1 log +1 log ,
i,y
(Yj)
(cid:20)
{|Ny, ii p− (yip )(y) |≥21
}
Ny,i6=0 2 (cid:18)N
y,i(cid:19)
{|Ny, ii p− (yi )p(y) |< 21
}
2 (cid:18)N
y,i(cid:19)(cid:21)
X
whereN = 1 is thenumberoftimeswehaveseeny inthefirstisamples.The
y,i j i Yj=y
firsttermcorrespo≤ndstoahighlyunlikelyevent,whichweproveinAppendixA.2.
P
LEMMA 4. WithN
i,y
denotingtheempiricalcount
j
[i]1 Yj=y,
∈
n 1 P
− p(y)E 1 1 log N y,i 22mlog (n),
−
i=1y
(Yj)
(cid:20)
{|Ny, ii p− (yip )(y) |≥21
}
Ny,i≥1 2
(cid:18)
i (cid:19)(cid:21)≤ 2
XX∈Y
We now considerthe secondterm. Letus extractthe scalingin H(p) inherentto entropy
coding.Tothatend,werewriteitas
n 1
− p(y)E 1 log (p(y))+log N y,i .
− (Yj) Ny,i−ip(y) <1 2 2 ip(y)
i=1y (cid:20) {| ip(y) | 2}(cid:18) (cid:18) (cid:19)(cid:19)(cid:21)
XX∈Y
Thefirsttermpresentsthedesiredscaling
n 1 n 1
− −
p(y)E 1 log (p(y)) p(y)log (p(y)) nH(p).
− (Yj) Ny,i−ip(y) <1 2 ≤− 2 ≤
i=1y (cid:20) {| ip(y) | 2} (cid:21) i=1y
XX∈Y XX∈Y
Finally, we deal with the rightmost logarithm, using the Taylor series of the logarithm to
show concentration for the empirical mean of the logarithm. The details are provided in
AppendixA.2.MODEESTIMATIONWITHPARTIALFEEDBACK 15
LEMMA 5. WithN
i,y
denotingtheempiricalcount
j
[i]1 Yj=y,
∈
N P
ln(2)E 1 log y,i 4m(ln(n)+1).
− (Yj) Ny,i−ip(y) <1 2 ip(y) ≤
(cid:20) {| ip(y) | 2} (cid:18) (cid:19)(cid:21)
Collectingthedifferentpiecestogether,wefindtheupperbound,
n 1
−
E [T ] Cn C E p(y)1 log (pˆ(y)) +m2
(Yi)i≤n n ≤ − (Yj)  pˆi(y) 6=0 · 2 i 
i=1 y
X X∈Y
 
4Cm
Cn(1+H(p))+m2+22Cmlog (n)+ (ln(n)+1)+m2
≤ 2 ln(2)
Cn(1+H(p))+28Cmlog (n)+6Cm+m2.
2
≤
4. TruncatedSearch. Inthissection,weimproveupontheprevioussearchprocedures
usingthefollowingkeyobservation.Whenestimatingtheempiricalmodeofabatchbyiden-
tifying samples following a Huffman tree, one can stop the search procedure roughly when
reachingthedepthD= log (p(y )) ofthemode,resultinginabout log p(y ) queriesper
| 2 1 | | 2 1 |
sampleonaverage,ratherthanH(p)querieswhentryingtofullyidentifyeachsample.
4.1. Coarse Sufficient Statistics. We introduce the concept of admissible partitions,
which provide sufficient statistics for mode estimation that are weaker than the full empiri-
cal distribution pˆ, as well as the concept of η-admissible partition, which will be useful to
build statistically andcomputationallyefficientalgorithms. Recallthat a partition of is
P Y
a subset of 2 , such that for any S ,S , S S = , and S = . Here and
Y 1 2 1 2 S
throughoutthetext,forp P( )a{ ndS }⊂ ,wP edefi∩ nep(S)∅ := ∪ p∈ (P y). Y
∈ Y ⊂Y y S
∈
P
DEFINITION 6 (Admissible partitions). An admissible partition of with respect to
Y
p P( )referstoapartition 2 suchthat
Y
∈ Y P ⊂
(12) argmaxp(S)= y , where y =argmaxp(y).
∗ ∗
{ }
S y
∈P ∈Y
Forη>0,anη-admissiblepartitionof withrespecttopissimilarlydefinedasanadmissi-
Y
blepartition 2 forwhichallsetsS suchthatp(S) η aresingletons,andatmost
Y
P ⊂ ∈P ≥
asinglesetS verifiesp(S)<η/2.Formally,
(13) S p(S)<η/2 1 and S , p(S) η S =1.
|{ ∈P| }|≤ ∀ ∈P ≥ ⇒ | |
Theinterestofadmissiblepartitionscomesfromthefactthatif isanadmissiblepartition
P
withrespecttobothpandp P( ),thenpandp havethesamemodes.Inparticular,if
′ ′
∈ Y P
is an admissible partition for the empirical distribution pˆof some batch of samples, the set
S with thelargestmasspˆ(S) is asingletoncontainingtheempiricalmodeofthebatch.
∈P
Onthecomputationalside,whenpisknown,η-admissiblepartitionsareeasytobuild,which
contrasts with the NP-hardnessof finding an admissible partition of minimal cardinality, or
offindingasetS thatmaximizesp(S)undertheconstraintp(S) p(y ).5
∗
≤
5Thelatterisequivalenttotheknapsackproblem(Mathews,1896),whilethepartitionproblem(Korf,1998)
canbereducedtotheformerwiththefollowingconstruction.Consideralistp0ofpositiveintegersandincludean
elementp equaltohalfthesumofp0 plusaninfinitesimalquantity.Next,normalizeallelementstotransform
∗
the new list intoa probability vector p whose elements add up to one. Checking if p0 can be partitioned into
twolistsS1 andS2 thatsumtothesamevalueisequivalenttodeterminingwhetherthereexistsanadmissible
partitionofpofcardinalitythree.16
4.2. Adaptive Truncated Search. Building upon Definition 6, Algorithm 6 efficiently
constructs an η-admissible partition of with respect to the empirical frequencies pˆof
P Y
a batch of samples (Y ) . It uses a predefined binary tree , and takes two parameters
j j [n]
γ,ǫ [0,1]thatdefineη=∈γpˆ(yˆ) ǫ,whereyˆisthemodeofpˆ.T Thealgorithmstartswiththe
∈ −
trivial partition = , and recursively refines it by splitting the set S with the greatest
P {Y} ∗
empiricalmassuntilS isasingleton,whichmustthenbeequalto yˆ .Itthenkeepssplitting
∗ { }
non-singletonsetsofmassstrictlygreaterthanγpˆ(yˆ) ǫuntiltherearenosuchsetsleft.The
−
splittingisdoneusingthetree asfollows:weidentifyeachnodeV inthetreewiththeset
T
ofalltheelementsthatmaptoitsdescendentleavesS(V)= y V ⊳V .Ateachtime
y
{ ∈Y| }
step,thesetsS ofthecurrentpartitioncorrespondtonodes(V )of ,andthesetS =S(V )
S
that has to be split is replaced in the partition by its two childrenT S(l(V )),S(r(V∗ )) in ∗.
2
This consumesN(S )= 1 queriesto identify whichsample∗ belongsto S aT nd
whichtoS
.Atthee∗ nd,aHuj ∈ff[ mn] anYj s∈ cS h∗ emeisappliedtomergesetsintoanη-admissible1
par-
2
P
tition,andre-balancethetree sothatallsetsinthepartitionareatasimilardepth,roughly
T
equalto log (2/η), in the new tree. This re-balancingkeepsthe structure of the “sub-trees”
2
below the nodescorrespondingto the sets of the partition intact. In addition, the algorithm
identifieswhichsamplebelongstowhichsetof ,aswellastheempiricalmodeyˆ.
P
Data:Set = y1,...,ym ,binarytree ,nsamples(Yj),parametersγ,ǫ R.
Y { } T ∈
SetV therootofthetree ,S = ,N(S )=n;
∗ T ∗ Y ∗
Set = (V :n) builtasaheap, = anemptylist,andC= ;
ForS allS{ and∗ V,w} edenoteN(S)=L Pj{ ∈} [n]1
Yj ∈S
∈NandS(V− )=∞ {y ∈Y|Vy⊳V }⊂Y.
whiletheheap isnon-emptyandN(S ) Cdo
SetV
=aS
rgmax
N(S(V))b∗yp≥
oppingitoutoftheheap ;setS =S(V );
V
ifV ∗ isaleaf then ∈S S ∗ ∗
∗Ifitwasthefirstencounteredleaf,set yˆ :=S ,andrefineC:=γN( yˆ ) ǫn;
{ } ∗ { } −
AddV tothelist ;
else ∗ L
MakeN(S )queriestogetalltheinformationon(1 ) forV l(V ),r(V ) ;
∗ Yj ∈S(V) j ∈[n] ∈{ ∗ ∗ }
InserteachchildV l(V ),r(V ) intotheheap withvalueN(S(V));
∈{ ∗ ∗ } S
end
end
Addalltheremainingelementsoftheheaptothelist ;
L
ApplyHuffman’sscheme,Algorithm1,tothelist torebalancethetopofthetree ;
L T
Result:Atree containingan(γpˆ(yˆ) ǫ)-admissiblepartitionforpˆ,andtheempiricalmodeyˆ.
T Algorithm−6:BatchTreeRebalancing
This suggests a new way to tackle Problem 1: given a batch of samples, Algorithm 6
yields an admissible partition with respect to the empirical distribution pˆ, and in particular
identifies its empirical mode yˆ, which is the best possible mode estimate. It does not need
to fully identify each sample to do so, unlike Algorithms 3 and 5. The number of queries
consumed by Algorithm 6 depends on the tree that it takes as input: upon reaching the
T
final partition , the algorithm will have required D queries for each sample belonging to
P
a given set S , where D is the depth of the node associated with S in the tree . To
∈P T
minimize the expected number of queries needed, should be a partial Huffman tree with
T
respectto the distribution p; this would result in D C log (2/p(y )) for each S ,
2 1
≤ ⌈− ⌉ ∈P
with C =2asperLemma3.Wedonothaveaprioriaccesstothedistribution p,butwecan
learnthestructureofsuchatreeoverseveralrounds,withaslackparameterǫ toaccountfor
r
the gradually increasing precision of our estimates. This is Algorithm 7, whose guarantees
areprovidedbyTheorem4.MODEESTIMATIONWITHPARTIALFEEDBACK 17
Data:Set = y1,...,ym ,samples(Ys),scheduling(nr)
NN
and(ǫr)
RN
.
Y { } ∈ ∈
Set abinarysearchtree;
forrT Ndo
∈
Getnr freshsamples(Y j,r)
j
[nr]defininganempiricalprobabilitypˆr;
RunAlgorithm6withtree ,∈γ=1andǫ=ǫr toobtaintheempiricalmodeyˆr andupdate ;
T T
end
Result:Returnthelastyˆr astheestimatedmode.
Algorithm7:TruncatedSearch
THEOREM 4 (Truncated search performance). For any δ>0, the number of queries T
r
needed by Algorithm 7 run with the schedulings n r :=2r and ǫ r := 41 m 2 3 2 to correctly
identifythemodewithprobabilityatleast1 δ satisfies
− (cid:0) (cid:1)
4
(14) E[T] 8ln(1/δ)∆ 2 log +o(ln(1/δ)).
≤ −2 2 p(y )
(cid:24) (cid:18) 1 (cid:19)(cid:25)
Algorithm7isaclearameliorationovertheprevioussearchalgorithms:theprobabilityof
error remains the same as a function of the number of samples, while reducing the average
number of queries per sample from H(p) to roughly log p(y ) , going from average code
| 2 1 |
lengthtominimalcodelength.Nonetheless,thereisstillroomforimprovement.Algorithm7
identifiesthe empiricalmodeoftheentire batchwith absolutecertitude. Thiscontrastswith
the main takeaway from the bandit literature: one should leverage confidence intervals to
buildstatisticallyefficientalgorithmssoastoavoidspendingqueriessolelytoruleouthighly
unlikelyevents.Weexplorethisintuitioninthenextsection.NotethatreplacingtheHuffman
codefromAlgorithm6byanyC-balancedcodewouldresultinamodifiedbound
4
E[T] 4Cln(1/δ)∆ 2 log +o(ln(1/δ)).
≤ −2 2 p(y )
(cid:24) (cid:18) 1 (cid:19)(cid:25)
inTheorem4(inparticular,usingaShannoncodewouldyieldC =1).
4.3. Proofs. ThissubsectionprovidesthekeyelementsfortheproofofTheorem4.Our
strategyisthefollowing.
• Findalikelyevent wherethealgorithmbehavesasdesired.
A
• Show that its complement c is unlikely enough that the additional queries it elicits are
A
asymptoticallynegligible.
Letr 2.Welet betheupdatedtreeattheendofroundr,and bethecorresponding
r r
≥ T P
η -admissible partition where η =pˆ (yˆ ) ǫ . Here pˆ is the empirical distribution of the
r r r r r r
−
samplesatroundr,andyˆ isthecorrespondingempiricalmode.Letusdefinetheevent
r
ǫ ǫ
r 1 r
(15) = S , max pˆ r 1(S) p(S), pˆ r(S) p(S) − − .
A ∀ ⊂Y {| − − | | − |}≤ 4
(cid:26) (cid:27)
Aunionbound,detailedinAppendixA.3,showsthat islikelytohappenasthenumberof
A
roundsincreases,asstatedinthefollowinglemma.
LEMMA 6. Theevent definedby(15)satisfies
A
2
n ǫ ǫ
(16) P(c ) 2m+1exp r −1 r −1 − r .
A ≤ − 2 4
!
(cid:18) (cid:19)18
We claim that under the event , the η -admissible partition obtained at the end of
r 1
A −
roundr 1staysadmissibleatroundr,namely,wehavetheimplication
−
(17) AllsetsS suchthatpˆ (S)>pˆ (yˆ ) ǫ aresingletons .
r 1 r r r r
A ⊂ { ∈P − − }
PROOF OF EQUATION (17). Indeed, implies that pˆ r 1(S) pˆ r(S) (ǫ r 1 ǫ r)/2
A | − − |≤ − −
and as a consequence, for any set S of cardinality at least 2, the fact that is
r 1 r 1
∈P − P −
(pˆ (yˆ ) ǫ )-admissiblewithrespecttopˆ leadsto
r 1 r r 1 r 1
− − − −
ǫ ǫ ǫ ǫ
r 1 r r 1 r
pˆ r(S) pˆ r 1(S)+ − − <pˆ r 1(yˆ r 1) ǫ r 1+ − −
≤ − 2 − − − − 2
pˆ (yˆ ) ǫ pˆ (yˆ ) ǫ ,
r r 1 r r r r
≤ − − ≤ −
Hence,under ,allsetsS suchthatpˆ (S)>pˆ (Sˆ ) ǫ aresingletons.
r 1 r r r r
A ∈P − −
Letusassumethat isrealized–thenallsetsS areeithersingletonsorsuchthat
r 1
A ∈P −
pˆ (S) pˆ (yˆ ) ǫ . Hence, when Algorithm 6 is applied to the n samples of round r, a
r r r r r
≤ −
sample Y belonging to some set S only consumes a number of queries smaller6
i,r r 1
∈P −
or equal to the depth D(S) of S in the tree . Since was obtained by applying
r 1 r 1
T − T −
Huffman’sschemetothesetsof attheendofAlgorithm6tore-balancethetree ,
r 1 r 2
P − T −
wecanleverageLemma3toboundthedepthD(S).Since y ,when isrealized
1
{ }⊂Y A
ǫ ǫ
r 1 r
p(y 1) pˆ r 1(yˆ r 1) p(y 1) pˆ r 1(y 1)< − − <ǫ r.
− − − ≤ − − 4
As is(pˆ (yˆ ) ǫ )-admissible,necessarilyeachS (exceptatmostone)
r 1 r 1 r 1 r 1 r 1
P − − − − − ∈P −
issuchthat
pˆ (yˆ ) ǫ p(y ) ǫ ǫ p(y ) 2ǫ p(y )
r 1 r 1 r 1 1 r r 1 1 r 1 1
pˆ r 1(S) − − − − > − − − − − ,
− ≥ 2 2 ≥ 2 ≥ 4
whereweusethefactthatǫ <1/4m p(y )/4.ApplyingLemma3,wededucethat
r 1 1
− ≤
(18) D(S) 2 log (4/p(y )) .
2 1
≤ ⌈ ⌉
IfS issuchthatpˆ
r
1(S)< pˆr−1(yˆr− 21) −ǫr−1 (andtherecanbeonlyonesuchset),thenitmust
−
share a parentin with someother set S , from which we deducethat its depth follows
r 1 ′
T −
thesamebound.Hencethetotalnumberofqueriesconsumedatroundr,assumingthat is
A
realized,isatmostn 2 log (4/p(y )) .Iftheevent isnotrealized,wecanroughlyupper
r ⌈ 2 1 ⌉ A
bound the number of queries spent per sample by m, hence we can upper bound the total
numberofqueriesbyn m.Inthespecialcaser=1,wesimilarlyneedatmostn mqueries.
r r
AfewlinesofderivationsprovidedinAppendixA.3leadtothefollowinglemma.
LEMMA 7. When running Algorithm 7 with the schedule of Theorem 4, the expected
numberofqueriesneededforroundr satisfies
r
4 4 C
E[T ] 2r log +m2m+1exp
r ≤ 2 p(y ) − 3 m2
(cid:18)(cid:24) (cid:18) 1 (cid:19)(cid:25) (cid:18) (cid:18) (cid:19) (cid:19)(cid:19)
forsomeconstantC >0.
6ThenumberofqueriesneededcanbestrictlysmallerthanthedepthofS ifsomeparentsetS′ofS issuch
thatpˆr 1(S′)>pˆr 1(yˆr 1) ǫr 1butpˆr(S′) pˆr(yˆr) ǫr.
− − − − − ≤ −MODEESTIMATIONWITHPARTIALFEEDBACK 19
At the end of round r, the algorithm outputs the empirical mode yˆ of the n samples
r r
(Y ) definingtheempiricalprobabilitypˆ .WeknowfromTheorem1thatitsprobabil-
ityi, or fi e∈r[ rn or r]
δ isboundedby
r
r
δ exp n ∆2 =exp 2r∆2 .
r ≤ − r 2 − 2
Nowletδ>0,anddefinetheround (cid:0) (cid:1) (cid:0) (cid:1)
r =min r exp( n ∆2) δ =min r n ∆ 2ln(1/δ)
δ − r 2 ≤ r ≥ −2
By construction, n =(cid:8)2n(cid:12) 2ln(1/δ)∆(cid:9)2. At th(cid:8)e e(cid:12)nd of round r , the(cid:9)probability of
rδ (cid:12)rδ−1 ≤ −2 (cid:12) δ
outputtingthecorrectclassisatleast1 δ,andthetotalnumberofexpectedqueriesusedso
−
farbyAlgorithm7isatmost
rδ rδ
4 4
r
C
E[T ] n 2 log +m2m+1exp
r ≤ r 2 p(y ) − 3 m2
r=1 r=1 (cid:18) (cid:24) (cid:18) 1 (cid:19)(cid:25) (cid:18) (cid:18) (cid:19) (cid:19)(cid:19)
X X
4
rδ
4
r
C
2 log n + 2rm2m+1exp
≤ 2 p(y ) r − 3 m2
(cid:24) (cid:18) 1 (cid:19)(cid:25)r=1 r 1 (cid:18) (cid:18) (cid:19) (cid:19)
X X≥
4 4
4n log +C˜ 8ln(1/δ)∆ 2 log +o(ln(1/δ)).
≤ rδ 2 p(y ) ≤ −2 2 p(y )
(cid:24) (cid:18) 1 (cid:19)(cid:25) (cid:24) (cid:18) 1 (cid:19)(cid:25)
Thiscompletestheproof.
5. Bandit-InspiredElimination. Thecoreideaofthissectionis tosuccessivelyelimi-
natecandidateclassesy inordertolowertherequirednumberofqueriescomparedtothe
∈Y
ExhaustiveSearchAlgorithmfromSection3,whichcanbedonebyadaptingandimproving
theseminalSuccessiveEliminationAlgorithmfromEven-Dar,MannorandMansour(2006),
resultinginAlgorithm8.
Data:Set ,probabilityp P( ),samples Yr r N,scheduleσ:N R Y R Y.
SetSe= Y thesetofelimin∈ atedY guesses,r={ 0,N} (∈y)=0 Rm; × →
∅ ∈
while Seisnotasingletondo
SeYt\ r r+1andquery1 ;
ifYr
←
/Sethen
Yr ∈Se
∈
IdentifyYr withaHuffmantreeadaptedtoempiricalcountsN(y)on Se;
Y\
UpdateN(Yr)=N(Yr)+1,andsettheempiricaldistributionpˆr(y) N(y)accordingly;
∝
SetSe=Se ∪{y ∈Y\Se |pˆr(y)+σy(r,pˆr)<max
z
∈Y\Sepˆr(z) };
end
Result:Estimatemodeyˆastheonlyelementof Se.
AlgorithmY8\:Elimination
Algorithm 8 takes as input a generic parameter function σ and works as follows: at any
time,itmaintainsasetofeliminatedclassesS thataredeemedunlikelytobethemode.
e
⊂Y
At the start of round r, the algorithm tests whether the new sample Y belongs to S ; if
r e
not, it is identified with a Huffman tree adapted to the empirical distribution pˆ on S ,
r e
Y\
which is then updated.7 Any class y such that its empirical mass pˆ (y) satisfies p (y)+
r r
7There is a minor abuse of notations in the description of Algorithm 8: in the expression “σy(r,pˆr)", the
distributionpˆr on Se isimplicitlyseenasadistributionon bysettingpˆr(y)=0ify Se sothatitisa
Y\ Y ∈
validargumentforσ.20
σ (r,pˆ )<max pˆ (z) isfinallyaddedto thesetofeliminatedclassesS .Forawell-
chy osenr parametez r∈fuYn\S ce tior nσ,Algorithm8hasahighprobabilityofidentifyingthe
etruemode,
asstatedinTheorem5.
THEOREM 5. Letδ>0,anddefinetheeliminationschedulingσ:N R
Y
R
Y
as
× →
cmax pˆ (z)ln(π2mr2/δ)
z r
(19) σ (r,pˆ)= , with c=24.
y
r
r
With probability 1 δ, Algorithm 8 terminates,identifies the rightmode andhas consumed
−
lessthanT queries,where,conditionallytothissuccessfulevent,
E[T] p(y ) p(y ) 1
1 1
C +C p(y ) log +o(1),
ln(1/δ) ≤ 1 2 2 i 2 2 p(y )
∇2 i [m] ∇i (cid:24) (cid:18) i (cid:19)(cid:25)
X∈
fortwouniversalconstantsC ,C , :=p(y ) p(y )ifi=1and = .
1 2 i 1 1 1 2
∇ − 6 ∇ ∇
Asshownin Proposition8below,whoseproofcanbefoundin AppendixA.4, 2p(y )
∇−i 1
is equal to ∆ 2 up to a multiplicative factor as long as p(y ) is bounded away from 1 –
−i 1
this is no constraint in most interesting use cases, as Problem 1 gets easier the closer p(y )
1
gets to 1. Hencewe seethatTheorem5 is a clearimprovementuponthe ExhaustiveSearch
Algorithms 3 and 5. In essence, one requires roughly ∆ 2log(1/δ) samples to know with
−i
certainty 1 δ thatclassy isnotthemode.AstheExhaustiveSearchAlgorithmfromSec-
i
−
tion 3 nevereliminates anyclass,the numberof samplesit requires to reach accuracy1 δ
−
is conditioned by the second most likely class y , the one that is hardest to correctly dis-
2
miss.Conversely,Algorithm8eliminateswithhighprobabilitytheclassy afterhavingseen
i
about log(1/δ)∆ 2 samples, which translates to an improved asymptotic expected number
−i
of required queries as a function of δ. However, Algorithm 8 does not leverage the coarse
statistics and search truncation mechanismpresented in Section 4, leaving room for further
improvement. We combine the elimination mechanism of Algorithm 8 and the search trun-
cation technique of Algorithm 7 in Section 6, after the remainder of this section which is
dedicatedtothediscussionofAlgorithm8andtheproofofTheorem5.
PROPOSITION 8. Letp P( )withp(y 1)>p(y i)foralli [m].Wehavethefollowing
∈ Y ∈
inequality,with :=p(y ) p(y )and∆2= ln(1 ( p(y ) p(y ))2),
∇i 1 − i i − − 1 − i
p(y 1)
p(y ) 2 ∆ 2
p
4p(y )
p
2.
ln(1 p(y ))
1 ∇−i
≤
−i
≤
1 ∇−i
1
− −
In particular,if p(y ) is boundedawayfrom 1, then p(y )/ln(1 p(y ) is boundedaway
1 1 1
from0and∆ 2 p(y ) 2 uptoamultiplicativec− onstants. −
−i
≃
1 ∇−i
5.1. Design of the elimination schedule. Algorithm 8 was inspired by the seminal al-
gorithm of Even-Dar,MannorandMansour (2006). Their Successive Elimination Algo-
rithm uses Hoeffding’s inequality to define σ (r,p) = σ where σ2 ln(1/δ)/r. It is
y r r ≃
known that Hoeffding’s inequality can be refined for sub-gamma variables with Bern-
stein’s inequalities, which was the main motivation behind the UCB-V algorithm of
Audibert,MunosandSzepesvári (2009). Bernstein’s inequality can be seen as a conve-
nient weak formulation of Chernoff’s bounds, which in the case of Bernoulli variables
are expressed in terms of KL-divergences, and have motivated the KL-UCB algorithm of
Cappéetal.(2013).KL-basedconfidenceintervalsaredefinedas
σ (r,p)=inf q [0,1] exp( rD(p(y) p(y) q)) δ .
y
{ ∈ | − k ± ≤ }MODEESTIMATIONWITHPARTIALFEEDBACK 21
While theyarguablyprovidethebestasymptoticresults,theyintroducecomputationalover-
headthatwewerekeentoavoidinouralgorithms.
Rather than trying to find the tightest confidence intervals, our definition of σ (r,p) in
y
Theorem 5 comes from a different perspective: the optimization of elimination times. We
knowfromLemma24,avariantofTheorem1providedinAppendixA.1basedonamethod
of Cramér (1938), that we can not safely eliminate y as a mode candidate before having
i
made r ∆ 2ln(1/δ) observations of both 1 and 1 . The Proposition 8 below
i
≃
−i Yj=yi Yi=y
1
statesthatthisisroughlythesameasaskingfor
r p(y ) 2ln(1/δ)=p(y )(p(y ) p(y )) 2ln(1/δ).
i
≃
1 ∇−i 1 1
−
i −
Pluggingthis in the elimination criterion ofAlgorithm 8, we wouldlike to defineσ soasto
ensurethatwithhighprobability,
pˆ (y ) pˆ (y ) σ (r,pˆ) 0
r 1
−
r i
−
yi
≥
for any r r . Assuming that the empirical probability pˆ converges fast enough to p, a
i r
≤
confidenceparameterσ definedas
r p(y )ln(1/δ)
i 1
σ (r,p) (p(y ) p(y ))
yi
≃
1
−
i
r ≃ r
r r
satisfies this constraint. Of course, we do not have a priori access to p(y ). Instead, we
1
estimate it at round r with max pˆ (y). Up to a few constants needed to account for
y r
union bounds, this leads to σ as∈Ydefined in Theorem 5. In comparison to the results
of Even-Dar,MannorandMansour (2006), whose algorithm requires O( 2log(1/δ))
i∇−i
samples, we gain a factor p(y )<1, thanks to which we reach the ideal scaling in ∆ 2 as
1
P
−i
longasp(y )isboundedawayfrom1.
1
5.2. Proofs. Inthissection,weproveTheorem5.Letpˆ denotestheempiricalprobabil-
r
ity of the first r samples (Y ) , and S be the set S of rejected classes, updated at each
i i r r e
iterationrbyAlgorithm8.We≤ defineyˆ =argmax pˆ (y),andδ =δ/π2mr2,andwe
considertheeliminationcriterion
r y ∈Y\Sr r r
24pˆ (yˆ )ln(1/δ )
r r r
pˆ (y)+σ <pˆ (yˆ ), σ =
r r r r r
r
r
fromAlgorithm8withtheschedulefromTheorem5.
We have seen in the discussion of the previous subsection that the estimation of the un-
known quantity p(y ) by pˆ (yˆ ) is a key componentof the definition of σ . In fact, it turns
1 r r r
out that estimating p(y ) is much easierthan finding the mode y , yetcan help us with that
1 1
second,hardertask. Thenexttwo lemmas dealwith this subproblem,and are crucialto our
proof of Theorem5. Chernoff’s boundfor Bernoullivariablesstates that oneneedsroughly
ln(1/δ)/p(y) samplesto geta good estimate of p(y) up to multiplicative constants.It leads
tothefollowinglemma,provedinAppendixA.4.
LEMMA 9. Forany c>1, let pˆ
r
bethe empiricalprobabilityassociatedwith r random
samples(Y ) independentlydistributedaccordingtop P( ).Itholdsthat
i i r
≤ ∼ Y
c+1 1
r ln(1/δ), P(pˆ (y)>cp(y)) δ
∀ ≥ (c 1)2p(y) r ≤
−
and
c2 1
r ln(1/δ), P(pˆ (y)<c 1p(y)) δ.
∀ ≥ (c 1)2 p(y) r − ≤
−22
While Lemma 9showsthatoneneedsaboutr ln(1/δ)/p(y ) samplesin orderto geta
1
≃
goodestimateofp(y ),itisnotofmuchusebyitself:asp(y )isaprioriunknown,wecannot
1 1
compute this required number of samples. We could naively estimate it as the first round r
suchthatrmax pˆ (y) ln(1/δ).Thenextlemmashowsthatthisstrategyactuallyworks
y r
∈Y ≥
well.
theL eE vM enM tA 10. Foranyr ∈N ≥1,δ>0andc>1,letyˆ r=argmax
y
∈Ypˆ r(y)andconsider
= rpˆ (yˆ ) cln(1/δ) .
r r r
A { ≤ }
Then
2c2 c 1
r − ln(1/δ), P(c ) mδ,
r
∀ ≤ 2c+1+√1+8cp(y ) A ≤
1
and
c2 1
r ln(1/δ), P( ) δ.
r
∀ ≥ c+1 √1+2cp(y ) A ≤
1
−
NotethatbycombiningLemmas9and10,onecanderiveanefficient(0,δ)-PACalgorithm
togetagoodestimateofp(y )uptouser-definedmultiplicativeconstants:first,useLemma9
1
andthe aforementionedconstantsto expressthenumberr ofsamplesneededasa function
0
of p(y ) and δ. This numbercannotbe explicitly computedby the user, asthey do nothave
1
accesstop(y );however,onecanuseLemma10todefinesomeevent associatedtosome
1
A
constantcsuchthatonce doesnotholdanymore,risbiggerthanr willhighprobability.
0
A
For any such r, pˆ (yˆ ) will be a good estimate of p(y ). Though we do not explicitly use
r r 1
suchanalgorithm,itisimplicitlyatthecoreofTheorem4,aswillbecomeapparentinwhat
follows.
AsfortheproofofTheorem4,wedefinelikelyeventsthateasetheanalysis.Theirdefini-
tions,andlowerboundsontheirprobabilities,areprovidedbythefollowinglemma,proven
inAppendixA.4.
LEMMA 11. Letuswriteσ˜ r= 3p(y 1)ln(1/δ r)/r,anddefinetheevents
• A = pˆ (y ) p(y )/2forallpr 4ln(1/δ )/p(y ) ,
1 r 1 1 r 1
{ ≥ ≥ }
• A = pˆ (yˆ ) 2p(y )forallr 4ln(1/δ )/p(y ) ,
2 r r 1 r 1
{ ≤ ≥ }
• A = pˆ (y ) p(y ) σ˜ forallr 4ln(1/δ )/p(y )andi [m],
3 r i i r r 1
{| − |≤ } ≥ ∈
• A = σ pˆ (yˆ )forallr 4ln(1/δ )/p(y ) .
4 r r r r 1
{ ≥ ≤ }
ThenP(A ) 1 δ/6fori 1,2,4 andP(A ) 1 δ/3.
i 3
≥ − ∈{ } ≥ −
ThoseeventswillallowustoguaranteethevalidityofAlgorithm8withhighprobability.
Notethat,duetoacoincidenceinourchoiceofconstants,theeventA isinfactimpliedby
2
A . We nonetheless keep A as a separate event for increased readability. Intuitively, these
3 2
events should be interpreted as follows: A and A ensure that pˆ (yˆ) is roughly equal to
1 2 r
p(y )oncethethresholdr= 4 ln(1/δ ),whichisgivenbyLemmas9and10,isreached.
1 p(y ) r
1
Pastthis threshold,the quantity σ˜ = 3p(y 1)ln(1/δr) acts as a good deterministic proxy for
r r
σ /2= 6pˆr(yˆ)ln(1/δr), and A ensurq es that the empirical probability mass pˆ (y ) of each
r r 3 r i
class y iqis within an interval of width σ˜
r
centered around the true mass p(y i). Before this
threshold, A ensures that σ is too large for the elimination criterion to be satisfied, hence
4 rMODEESTIMATIONWITHPARTIALFEEDBACK 23
thatnoclassesareeliminatedbefore r 4 ln(1/δ ). Allofthiscombinedkeepsy from
≤ p(y 1) r 1
being wrongly eliminated, and yields upper bounds on the elimination times of the classes
y with i>1. Thenextparagraphswillformalizetheseclaims.Westartbyshowingthatthe
i
modeisneverwrongfullyeliminated.
LEMMA 12. Whentheevents(A i)
i [4]
hold,themodey
1
isnevereliminated.
∈
PROOF. If A
4
holds, then for any r 4ln(1/δ r)/p(y 1) we have pˆ r(y)+σ
r
pˆ r(yˆ r),
≤ ≥
hence,bydefinitionoftheeliminationcriterion,noclassescanbeeliminatedatroundr.
Let us now consider any round r 4ln(1/δ )/p(y ). Assume that y has not yet been
r 1 1
≥
eliminatedatthestartofroundr.TheeventA implies
3
pˆ (y ) p(y ) σ˜ p(y ) σ˜ pˆ (y ) 2σ˜ ,
r 1 1 r i r r i r
≥ − ≥ − ≥ −
whiletheeventA leadstopˆ (yˆ ) pˆ (y ) p(y )/2,hence
1 r r r 1 1
≥ ≥
σ = 24pˆ (yˆ )ln(1/δ )/r 12p(y )ln(1/δ )/r=2σ˜ .
r r r r 1 r r
≥
Thismeansthatthecrpiterionpˆ (y )+σ <pˆ (pyˆ )cannotbesatisfied,andthusy cannotbe
r 1 r r r 1
eliminated at round r if it was not at round r 1. Recursively, we deduce that y is never
1
−
eliminated.
Letusnowfocusontheeliminationoftheotherclassesy=y .
1
6
LEMMA 13. Whentheevent(A j)
j [4]
hold,theclassy
i
y
1
iseliminatednolater
thanwhen ∈ ∈Y\{ }
p(y )
1
r>108 ln(1/δ ).
2 r
∇i
Sinceln(δ )=ln(1/δ)+2ln(r)+c,thisdefinesaneliminationtime
r
p(y )
1
(20) r(y )=108 ln(1/δ)+o(ln(1/δ)).
i 2
∇i
PROOF. Let i>1, andassumethat the events(A j)
j [4]
hold. We knowfrom Lemma 12
that our hypothesis implies that y is never eliminated,∈hence that pˆ (yˆ ) pˆ (y ). Due to
1 r r r 1
≥
A ,italsoimpliesthat
2
3p(y )ln(1/δ ) 3pˆ (yˆ )ln(1/δ ) σ
1 r r r r r
σ˜ = = .
r
r ≥ 2r 4
r r
Furthermore,A impliesthat
3
pˆ (yˆ ) pˆ (y ) p(y ) p(y ) 2σ˜ .
r r r i 1 i r
− ≥ − −
Ifthefollowinginequalityholds
p(y ) p(y )>6σ˜ .
1 i r
−
then
pˆ (yˆ ) pˆ (y ) p(y ) p(y ) 2σ˜ >4σ˜ σ .
r r r i 1 i r r r
− ≥ − − ≥
Thelemmareducestothecharacterizationofr suchthatp(y ) p(y )>6σ˜ .
1 i r
−24
Thetwopreviouslemmashaveshownthatwhen(A ) hold,y isnevereliminatedand
i i [4] 1
y is eliminated at the latest when r=108(p(y ) p(y∈ )) 2p(y )ln(1/δ)+o(ln(1/δ)). In
i 1 i − 1
−
particular,thealgorithmendsandoutputsthecorrectmodebeforer .UsingLemma11,this
2
happenswithprobabilityatleast
4
(21) P( ) 1 P(cA cA cA cA ) 1 P(cA ) 1 δ,
i [4] i 1 2 3 4 i
∩∈ A ≥ − ∪ ∪ ∪ ≥ − ≥ −
i=1
X
Regardingthenumberofexpectedqueries,weuseonequeryforeachsampletocheckifit
belongstotheeliminatedset.Ifitisnot,i.e.Y /S ,weexpect,basedonTheorem3,tohave
i r
∈
toaskabout log (p˜(y)) log (p(y)) queriestoidentifyY =y,wherep˜istherestriction
| 2 |≤| 2 | i
ofptotheset S .
r
Y \
ThisexplainsthenumberofqueriesinTheorem5,
E T (A ) . p(y )(1+1 log (p(y )))
j j ∈[4] i yi∈/Sr| 2 i |
r r(y )i [m]
(cid:2) (cid:12) (cid:3) ≤X2 X∈
(cid:12)
.r(y )+ r(y )p(y ) log (p(y ))
2 i i | 2 i |
i [m]
X∈
p(y ) p(y )
1 1
+ p(y ) log (p(y )) ln(1/δ).
≃ 2 i 2 | 2 i |
(cid:16) ∇2 i X≤m ∇i (cid:17)
TheprooftechnicalitiesareprovidedinAppendixA.4,yieldingthefollowinglemma.
LEMMA 14. InthesettingofTheorem5,withtheevent(A i)
i [n]
definedinLemma11,
∈
E T
l|
n(A (1j /) δj ∈[4] ≤324p(y 21)
+216
m
p(y i) |log 2(p(y i))
|p(y 21)
+o(1)
(cid:2) (cid:3) ∇2 i=1 ∇i
X
Lemma14endstheproofofTheorem5andexplicitstheconstantsC andC .
1 2
6. SetElimination. TheintuitionbehindAlgorithm7isthatgivenaHuffmantreewith
respect to p, one does not need to go far below the depth log p(y )+1 when searching
− 2 1
for the mode (or equivalently, that classes with low probability can be grouped into sets of
classes with probability roughly equal to p(y )/2). Meanwhile, the intuition behind Algo-
1
rithm 8 is that one can quickly eliminate low-probability classes to focus on likely candi-
dates. The combination of these ideas yields Algorithm 9, which outperforms both. Classes
are partitioned into sets of mass roughly equal to p(y )/2, which are eliminated as soon as
1
they appear unlikely to be the mode. Those partitions must sometimes be redefined (as the
sets mightbe of greatermassthan initially estimated), butthese reorderingsare rare and do
notimpactasymptoticperformance.Hence,underthelightofProposition8,Algorithm9re-
quiresroughly∆ 2log(1/δ)samplestodiscardclassy asamodecandidatewithconfidence
−i i
level1 δ,andasampleconsumesroughly log p(y ) queries.ThisexplainsTheorem6.
− | 2 1 |
r
THEOREM 6. IfAlgorithm9is runwithschedulen r :=2r andǫ r := 41 m 32 2 andcon-
fidenceintervalparameterσ definedfromaconfidencelevelparameterδ>0as8
(cid:0) (cid:1)
cmax pˆ(z)ln(π2mn2/δ)
(22) σ(n,pˆ)=
z ∈Y\Se
, with c=24,
n
r
8Notethat although wedonot haveaccesstoallthe {pˆr(z)
}z ∈Y\Se
whenrunning Algorithm9,wehave
accesstotheirmaximizer(asanoutputofAlgorithm6),whichensuresthatσ(nr,pˆr)iscomputablebytheuser.MODEESTIMATIONWITHPARTIALFEEDBACK 25
Data: ,(Ys) p,scheduling(nr)
NN
and(ǫr)
RN
,confidenceintervalparameterσ.
Y ∼ ∈ ∈
SettheeliminatedsetSe= ,r=0,and abinarysearchtree;
∅ T
while Seisnotasingletondo
SeYt\ r r+1;
←
Getnr freshsamples(Y j,r)
j
[nr];
Ask(1
(Yj,r)
Se)
j
[nr]tore∈ movesamplesthatbelongtotheeliminatedsetSe;
∈ ∈
Wecallpˆr theempiricaldistributionon(Yj,r)
j
∈[nr]andyˆr themodeofitsrestrictionto Y\Se;
RunAlgorithm6withtree andparametersγ=1/2,ǫ=ǫr/pˆr( Se)onthesamplesYj,r Se;
T Y\ 6∈
Thisupdates andyieldsapˆr(yˆr)/2 ǫr-admissiblepartition r of Sewithrespecttopˆr;
T − P Y\
Setpˆ+(S)=pˆr(S)+σ(nr,pˆr)forS r;
∈P
SetSe=Se y S S ,pˆ+(S)<pˆr(yˆr) toeliminateunlikelymodecandidates;
∪{ ∈ | ∈P }
end
Result:Returnthemodeestimateyˆr ofthelastroundrastheestimatedmode.
Algorithm9:SetElimination
then with probability 1 δ Algorithm 9 terminates, identifies the right mode and consumes
−
lessthanT queries,where,conditionallytothissuccessfulevent,
E[T] p(y ) p(y ) 10
1 1
C +C p(y ) log +o(1),
log(1/δ) ≤ 1 2 2 i 2 2 p(y )
∇2 i m ∇i (cid:24) (cid:18) 1 (cid:19)(cid:25)
X≤
fortwouniversalconstantsC ,C , :=p(y ) p(y )ifi=1and = .
1 2 i 1 1 1 2
∇ − 6 ∇ ∇
Notethattheasymptoticexpectednumberofqueriesrequiredonlydependsontheclasses
ythatarecloseinprobabilitymasstoy ,asthecontributiontothesumofalltheclassesythat
1
aresuchthatp(y) p(y )/2issmallerthan4C log (10/p(y )) log(1/δ)/p(y ).Inpartic-
≤ 1 2 ⌈ 2 1 ⌉ 1
ular,wehavefreedourselvesfromanydirectdependenceinthenumbermofclasses.Onthe
otherhand,weexpecttohavetopreciselyestimatetheprobability massp(y ) oftheclasses
i
for which p(y ) is closeto p(y ). In line with this intuition, the dependencein thoseclasses
i 1
oftheexpectednumberofqueriesisroughlyC ∆ 2 log (10/p(y )) log(1/δ), whichcor-
2 −i
⌈
2 1
⌉
responds, up to a multiplicative factor, to the number of samples needed to disqualify y as
i
a modecandidatemultiplied by theroughnumberofqueriesneededto preciselyidentify y
i
usingaHuffmantreeadaptedtop.Thesesemi-heuristicargumentssuggestthatthereshould
be no easy way to improve upon Algorithm 9, besides tightening the various constants in
Theorem6throughmorecarefulcomputations.
6.1. Proofs. This section provides the proof for Theorem 6, which is a combination of
sortsoftheproofsofTheorems4and5.Letr 2,n =2r,andpˆ betheempiricalprobabil-
r r
≥
ity distribution associatedto the samples (Y ) used in the r-th round in Algorithm 9.
We denote by S the random set of all
clai s, sr ei s∈[ tn hr a]
t have been eliminated in the previous
r
rounds, and by yˆ the mode of pˆ restricted to S , i.e. yˆ =argmax pˆ (y). To sim-
plifynotation,wer
writeσ forσ
r
(n ,pˆ
),andseY t\
δ
r =δ/π2r
mn2.
y 6∈Sr r
r y r r r r
We start by introducing the same events as in the proof of Theorem 5, with the small
nuancethatthedistributions pˆ for r Nareindependentfromeachother,andthatweonly
r
considerthe subsetofindices n
∈N.
Theproofofthe associatedlemma is thesameas
r r
{ } ⊂
thatofLemma11.
LEMMA 15. Letuswriteσ˜ r= 3p(y 1)ln(1/δ r)/n r,anddefinetheevents
• A 1= pˆ r(y 1) p(y 1)/2forallpr suchthatn
r
4ln(1/δ r/p(y 1) ,
{ ≥ ≥ }
• A = pˆ (yˆ ) 2p(y )forallr suchthatn 4ln(1/δ )/p(y ) ,
2 r r 1 r r 1
{ ≤ ≥ }
• A = pˆ (y ) p(y ) σ˜ forallr suchthatn 4ln(1/δ )/p(y )andi [m] ,
3 r i i r r r 1
{| − |≤ ≥ ∈ }26
• A = σ pˆ (yˆ )forallr suchthatn 4ln(1/δ )/p(y ) .
4 r r r r r 1
{ ≥ ≤ }
ThenP( ) 1 δ.
i [4] i
∩∈ A ≥ −
Onceagainy cannotbeeliminatedwhentheevents(A ) hold.
1 i i [4]
∈
LEMMA 16. Whentheevents(A i)
i [4]
definedinLemma15holds,Algorithm9doesnot
eliminatethemode. ∈
PROOF. Assume that y
1
was not eliminated at the start of round r. The round iteration
definesa(pˆ (yˆ )/2 ǫ )-admissiblepartition on S withrespecttopˆ ,andallsamples
r r r r r r
− P Y\
Y areidentifiedalong S . Necessarily yˆ . Attheendofthis round,a class
i,r r r r r
P ∪{ } { }∈P
y S can only be added to the set S of eliminated classes if it belongs to some set
r r
∈Y \
S S suchthatpˆ (S)+σ <pˆ (yˆ ).Inparticular,thisimpliesthatpˆ (y)+σ <p (yˆ ),
r r r r r r r r r
⊂Y\
whichisthecriterion thatwasappliedintheElimination Algorithm8fromSection5.Asin
the proof of Theorem5, the events(A ) ensurethat pˆ (y )+σ <p (yˆ ) is nevertrue,
i i [4] r 1 r r r
whichmeansthaty cannotbeeliminated∈.
1
Let us now estimate the expectednumber of queries required for round r, still assuming
thatevents(A ) arerealized.WeintroducethesameeventasintheproofofTheorem4.
i i [4]
∈
ǫ ǫ
r 1 r
(23) r = S , max pˆ r 1(S) p(S), pˆ r(S) p(S) − − .
B ∀ ⊂Y {| − − | | − |}≤ 4
(cid:26) (cid:27)
Wehavealreadyshownthat
n ǫ ǫ 2 4 r C
(24) P(c Br) ≤2m+1exp
−
r 2−1 r −1 4− r ≤2m+1exp
− 3 m2
,
(cid:16) (cid:16) (cid:17) (cid:17) (cid:16) (cid:16) (cid:17) (cid:17)
for someconstantC >0. When holds,we canbound the numberof queriessimilarly to
r
B
whatwehavedoneforTheorem4.
LEMMA 17. Let
r 1
bethetreeasupdatedbyAlgorithm6attheendofroundr 1of
Algorithm 9. A samplT e Y− S belongingto some set S only consumesat ro− und r
i,r r r 1
anumberofqueriessmaller6∈ thanorequaltothedepthD(∈ S)P of− S inthetree .When
r 1 r
(23)holds, T − B
10
D(S) 2 log .
≤ 2 p(y )
(cid:24) 1 (cid:25)
(cid:16) (cid:17)
PROOF. Forr 1,
r
isbuiltasaHuffmantreeonthenodescorrespondingtothesetsof
≥ T
withrespecttopˆ .SimilarlytothecaseoftheTruncatedSearchAlgorithm7andtheproof
r r
P
of the associatedTheorem4, when B is realized,all sets S are eithersingletonsor
r r 1
∈P −
such that pˆ (S) pˆ (yˆ )/2 ǫ . Hence, when Algorithm 6 is applied to the n samples of
r r r r r
≤ −
round r, a sample Y S belonging to some set S only consumes a number of
i,r r r 1
6∈ ∈P −
queriessmallerthanorequalto the depth D(S) of S in thetree . Using asin theproof
r 1
T −
ofTheorem4thefactsthatǫ <p(y )/4andthatallS (exceptatmostone)satisfy
r 1 1 r 1
− ∈P −
pˆ
r
1(S)
pˆ
r
−1(yˆ
r
−1)/2 −ǫ
r −1
>
p(y 1)
−
ǫr−1 4−ǫr −2ǫ
r −1
>
p(y 1)
−
9ǫr 4−1
>
7p(y 1)
.
− ≥ 2 4 4 64
WeconcludethatD(S) 2 log (64/7p(y )) thankstoLemma3.
≤ ⌈ 2 1 ⌉MODEESTIMATIONWITHPARTIALFEEDBACK 27
If the event is not realized, we can roughly upper bound the number of queries spent
r
B
per sample by m, hence we can upper bound the total number of queries by n m. In the
r
specialcaser=1,wesimilarlyneedatmostn mqueries.
r
Let T be the total number of queries needed for round r (it is a random variable). We
r
spend n queries at the start of the round to check whether Y S for each sample Y ,
r i,r r i,r
∈
i [n ].Wehaveshownthatifr 2,then
r
∈ ≥
nr
10
(25) T 1+1 2 log +1 m ,
r ≤
i=1
{Yi,r6∈Sr}
(cid:18) (cid:24)
2 (cid:18)p(y 1)
(cid:19)(cid:25)
cBr
(cid:19)
X
whileifr=1,T n m.
r 1
≤
As previously, we will bound (the expectation of) 1 by a deterministic quantity
undertheevents(A ) .
{Yi,r6∈Sr}
j j [4]
∈
LEMMA 18. Whentheevents(A j)
j [4]
holds,then y i=y
1
is addedto S
r
nolater than
whenr=r(y),wheretheeliminationtim∈er(y)isdefineda6
s
r(y)=min r n >108max (p(y ) p(y)) 2,(p(y )/2) 2 p(y )ln(1/δ )
r 1 − 1 − 1 r
−
Inparticular (cid:8) (cid:12) (cid:0) (cid:1) (cid:9)
(cid:12)
n 216max (p(y ) p(y)) 2,(p(y )/2) 2 p(y )ln(1/δ ).
r(y) 1 − 1 − 1 r
≤ −
(cid:0) (cid:1)
PROOF. We write σ˜
r
= 3p(y 1)ln(1/δ r)/n r; we have seen in the proof of Theorem 5
thatA implies4σ˜ σ .Letusnowconsidery ,andletS (y)bethesetofthepartition
2 r r r
≥ p ∈Y
to which y belongs, assuming that it has not yet been eliminated at the start of round r.
r
P
Notethatbydefinitionof ,eitherpˆ (S (y)) pˆ (yˆ )/2orS (y)= y .
r r r r r r
P ≤ { }
Wefirstexaminethecasep(y) p(y )/2.TheeventA impliesthatpˆ (y) p(y )/2+σ˜
1 3 r 1 r
≤ ≤
and that pˆ (yˆ )/2 p(yˆ )/2+σ˜ /2 p(y )/2+σ˜ /2. Thus pˆ (S (y)) p(y )/2+σ˜ .
r r r r 1 r r r 1 r
≤ ≤ ≤
ThankstoA again,weknowthatpˆ (yˆ ) p(y ) σ˜ .Hence
3 r r 1 r
≥ −
pˆ (yˆ ) pˆ (S (y)) p(y )/2 2σ˜ .
r r r r 1 r
− ≥ −
Ifp(y )/2 6σ˜ ,then
1 r
≥
pˆ (yˆ ) pˆ (S (y)) 4σ˜ σ
r r r r r r
− ≥ ≥
andS (y)getseliminatedattheendofroundr.
r
Nowletusconsiderthecasep(y) p(y )/2.Similarly,theeventA ensuresthatpˆ (y)
1 3 r
≥ ≤
p(y)+σ˜ andthat
r
pˆ (S (y)) pˆ (yˆ )/2 p(y )/2+σ˜ /2 p(y)+σ˜ /2
r r r r 1 r r
≤ ≤ ≤
inthecasewhere S (y) 2,hencethat
r
| |≥
pˆ (S (y)) p(y)+σ˜ .
r r r
≤
Asabove,pˆ (yˆ ) p(y ) σ˜ ,hence
r r 1 r
≥ −
pˆ (yˆ ) pˆ (S (y)) p(y ) p(y) 2σ˜ .
r r r r 1 r
− ≥ − −
Ifp(y ) p(y) 6σ˜ ,then
1 r
− ≥
pˆ (yˆ ) pˆ (S (y)) 4σ˜ σ ,
r r r r r r
− ≥ ≥
andS (y)mustbeeliminatedattheendofroundr.
r
We have shown that if p(y) p(y )/2, y is eliminated at the latest at the end of the first
1
≤
round r for which p(y )/2 6σ˜ , and if p(y) p(y )/2, it is eliminated at the latest at the
1 r 1
≥ ≥28
endofthefirstroundforwhichp(y ) p(y) 6σ˜ .NotethatasintheproofofTheorem5,
1 r
− ≥
theconditionρ>6σ˜ (forρ p(y ) p(y),p(y )/2 )isequivalentto
r 1 1
∈{ − }
n >108ρ 2p(y )ln(1/δ ),
r − 1 r
whichdefinestheeliminationtimer(y).
The previous lemma allows us to bound 1 by 1 . We are now left with the
computation of the upper bound in Equation
(Y 2i 5,r )∈/ .S Wr
e
pror v≤idr e(y)
the derivations in Appendix
A.5,whichyieldthefollowinglemma.
LEMMA 19. Withtheevents(A i)
i [4]
asdefinedinLemma15,
∈
E T (A i)
i [4]
p(y 1)
ln(1/δ)∈ ≤432
(p(y ) p(y
))2p(y 1)
(cid:2) (cid:12) (cid:3) 1 − 2
(cid:12)
4 10
+864 p(y) p(y ) log
p(y )2 1 2 p(y )
y s.t.p(y)<p(y )/2 1 (cid:24) (cid:18) 1 (cid:19)(cid:25)
∈Y X 1
1 10
+864 p(y) p(y ) log +o(1).
(p(y ) p(y))2 1 2 p(y )
y s.t.p(y) p(y )/2 1 − (cid:24) (cid:18) 1 (cid:19)(cid:25)
∈Y X≥ 1
Notethatsince
max (p(y ) p(y)) 2,(p(y )/2) 2 4(p(y ) p(y)) 2
1 − 1 − 1 −
− ≤ −
foranyy ,wecan(cid:0)weakenandsimplifythisupper(cid:1)boundas
∈Y
E T (A i)
i [4]
p(y 1)2 p(y i)p(y 1) 10
∈ 432 +3456 log +o(1)).
ln(1/δ) ≤ (p(y ) p(y ))2 2 2 p(y )
(cid:2) (cid:12) (cid:12) (cid:3) 1 − 2 y X∈Y ∇i (cid:24) (cid:18) 1 (cid:19)(cid:25)
Thiscompletestheproof.
Aside on “ForeverRunning” Algorithms. It is easy to reuse parts of the proofs of The-
orems 5 and 6 to turn algorithms that output the empirical mode, namely the Exhaustive,
AdaptiveandTruncatedSearchAlgorithms, into (ǫ,δ)-PAC algorithms for ǫ 0 and δ>0.
≥
On canalsomodify the Elimination andSetElimination Algorithms to turn theminto algo-
rithmsthatrunforeverforeverincreasingaccuracybyadaptingthedoublingtrickcreditedto
Aueretal.(1995)toourpartialfeedbacksetting.Theasymptoticexpectednumberofqueries
asafunctionoftheprobabilityoferrorδ wouldscaleinthesamewayasfortheinitialalgo-
rithms,thoughwithworseconstants.
Conclusion. This article introducesProblem1, a newframeworkin whichto formalize
the problem of active learning with weak supervision. It presents three important ideas on
how to solve it, namely the use of adaptive entropy coding, coarse sufficient statistics and
confidence intervals, and illustrates these ideas through increasingly complex algorithms.
Finally, it combines those ideas into Algorithm 9, which provably only needs an expected
number
10
E[T] C ∆ 2+C ∆ 2 log +o(1) log(1/δ),
≤ 1 −2 2 −i 2 p(y )
i X≤m (cid:24) (cid:16) 1 (cid:17)(cid:25) !
ofqueriestoidentifythemodewithprobabilityatleast1 δ(assumingthatp(y )isbounded
1
−
awayfrom1).MODEESTIMATIONWITHPARTIALFEEDBACK 29
APPENDIX
A.1. InformationProjectionComputation. Recallthedefinitionsoftheset
= q P( ) argmaxq(y)=argmaxp(y) ,
Q { ∈ Y | 6 }
(8) = q P( ) n 1 N y /argmaxq(y) ,
n, − Y 1
Q − ∈ Y ∩ · ∈
and (cid:8) (cid:12) (cid:9)
(cid:12)
(9) = q P( ) n 1 N argmaxq(y)=argmaxp(y) .
n,+ − Y
Q ∈ Y ∩ · 6
In this section, we pro(cid:8)ve Lemma 1 from Sec(cid:12)tion 2, which we restate here(cid:9)for the reader’s
(cid:12)
convenience,aswellassomerelatedresults.
LEMMA 1. For any distribution p P( ), there exists a constant c
p
such that for any
n Nwith and definedbyE∈ quatY ions(8)and(9),
n, n,+
∈ Q − Q
∆2 c ∆2
2 + p min D(q p) min D(q p) minD(q p)= 2 .
ln(2) nln(2) ≥q ∈Qn,− k ≥q ∈Qn,+ k ≥ q ∈Q k ln(2)
Lemma 1 is a substatement of Lemmas 21 and 23 below. We first prove the following
intermediateresult.
LEMMA 20. Let = y 1,...,y
m
.Letq,p P( )andy ′,y
′′
besuchthat
Y { } ∈ Y ∈Y
q(y ) q(y )
′ ′′
> .
p(y ) p(y )
′ ′′
Letusdefineforǫ (0,min q(y ),1 q(y ) ],thedistributionq P( )
′ ′′ ǫ
∈ { − } ∈ Y
q(y) ǫify=y
′
−
q (y)= q(y)+ǫify=y
ǫ ′′

q(y) otherwise,

then

q(y )p(y ) q(y )p(y )
′ ′′ ′′ ′
ǫ 0, − D(q p)<D(q p).
ǫ
∈ p(y )+q(y ) ⇒ k k
(cid:18) ′ ′ (cid:21)
PROOF. Indeed,
q(y ) ǫ q(y )+ǫ
′ ′′
D(q
ǫ
p)=(q(y ′) ǫ)log − +(q(y ′′)+ǫ)log +C(q,p),
k − p(y ) p(y )
(cid:18) ′ (cid:19) (cid:18) ′′ (cid:19)
whereC(q,p)aresometermsthatdonotdependonǫ,and
d p(y )(q(y )+ǫ)
′ ′′
D(q p)=log
ǫ
dǫ k (q(y ) ǫ)p(y )
(cid:18) ′ − ′′ (cid:19)
is strictly negative for ǫ > 0 such that
p(y′)(q(y′′)+ǫ)
< 1, which leads to the condition on
(q(y′) ǫ)p(y′′)
ǫ. −
Wecannowcharacterizetheinformationprojectionmin D(q p).
q Q
∈ k30
LEMMA 21. Let = y 1,...,y
m
and p P( ) be such that p(y 1)>p(y 2)>0 and
Y { } ∈ Y
p(y ) p(y )foralli=2,...,m.Define
i 1 i
− ≥
:= q P( ) y y s.t.q(y) q(y ) .
1 1
Q { ∈ Y |∃ ∈Y \{ } ≥ }
Thereexistsq suchthatD(q p)=min D(q p),and9
∗ ∗ q Q
∈Q k ∈ k
1 λ(1 p(y ) p(y ))
1 2
q ∗(y 1)=q ∗(y 2)= − − −
2
and
q (y )=λp(y ) i 3,...,m
∗ i i
∀ ∈{ }
for
1
λ= .
1 ( p(y ) p(y ))2
1 2
− −
Itsatisfies p p
D(q p)= log(1 ( p(y ) p(y ))2).
∗ 1 2
k − − −
p p
PROOF. The statement is trivial when m=2 or m i=3p(y i)=0 (with λ=1), which we
assumehenceforthnottobethecase.
P
Observe first that if y is such that p(y) = 0, then necessarily q (y) = 0. Indeed,
∗
∈ Y
if p(y) = 0 and q (y) > 0, then D(q p) = , while min D(q p) < 0 (consider for
∗ ∗ q Q
example q P( ) defined as q(y )=k q(y )=∞ (p(y )+p(y∈ ))/2 ank d q(y)= p(y) for all
1 2 1 2
∈ Y
y y ,y ).
1
∈Y\{ ∗}
Lety y beamodeofq ,whichexistsbyhypothesis.Wemusthave
1 ∗
∗∈Y \{ }
q(y ) q(y )
′ ′′
y ,y y ,y s.t.p(y )=0,p(y )=0, ,
′ ′′ 1 ′ ′′
∀ ∈Y\{ ∗ } 6 6 p(y ′) ≤ p(y ′′)
as otherwise one could apply Lemma 20 to y and y to find q such that q (y ) =
′ ′′ ǫ ǫ 1
∈ Q
q(y ) q(y )=q ,henceq ,andD(q p)<D(q p),whichwouldcontradictthedefi-
1 ǫ ǫ ǫ
≤ ∗ ∈Q k k
nitionofq .
∗
Inparticular,thisimpliesbysymmetrythat
λ 0 s.t. y y ,y q(y)=λp(y).
1
∃ ≥ ∀ ∈Y \{ ∗}
Weobservethat
q (y )=q (y ).
∗ 1 ∗
∗
Indeed, we always have q (y )/p(y )>q (y )/p(y ), since q (y ) q (y ) and p(y )>
∗ ∗ 1 1 ∗ ∗ 1 1
∗ ∗ ∗ ≥
p(y ). If q (y )<q (y ), wecouldapplyLemma20to y ,y witha smallenoughǫ to find
∗ 1 ∗ 1
∗ ∗ ∗
q QwithD(q p)<D(q p).
ǫ ǫ
∈ k k
Nowletusshowthat
y y y q (y)=q (y ) s.t. p(y)=p(y ).
1 ∗ ∗ 2
∃ ∈{ ∈Y \{ }| ∗ }
Indeed,assumethatitisnotthecase:theny doesnotsatisfythoseconditions,henceitmust
∗
be such that p(y )<p(y ), and y does not satisfy those conditions, hence it must be such
2 2
∗
9Thischaracterizationofq∗isexactuptosomerenamingofclassesywithequalprobabilitymasses.MODEESTIMATIONWITHPARTIALFEEDBACK 31
that q (y )>q (y ). Consider the distribution q defined as q(y )=q (y ), q(y )=q (y )
∗ ∗ 2 2 ∗ ∗ 2
andq(y)∗ =q (y)forally y ,y .Thenq Q,and ∗ ∗
∗ 2
∈Y \{ ∗} ∈
D(q p) D(q p)=q (y )log(q (y )/p(y ))+q (y )log(q (y )/p(y ))
∗ ∗ ∗ ∗ 2 ∗ 2 2
k − k ∗ ∗ ∗
q (y )log(q (y )/p(y )) q (y )log(q (y )/p(y ))
∗ 2 ∗ 2 ∗ ∗ 2
− ∗ − ∗ ∗
=q (y )log(p(y )/p(y ))+q (y )log(p(y )/p(y ))
∗ 2 ∗ 2 2
∗ ∗ ∗
=(q (y ) q (y ))log(p(y )/p(y ))>0,
∗ ∗ 2 2
∗ − ∗
leading to a contradiction. Hence there exists y˜ y y q (y)=q (y ) such that
1 ∗ ∗
p(y˜)=p(y ). Consider now the distribution q de∈ fi{ ned∈ aY s q\ (y{ ˜)=}| q (y ), q(y )∗ =} q (y˜) and
2 ∗ 2 2 ∗
q(y)=q (y) forall y y ,y˜ ;thenq andD(q p)=D(q p),andwithoutlossof
∗ ∗ ∗
∈Y\{ } ∈Q k k
generalitywecanrenameq intoq fortheremainderoftheproof.
∗
Letuswritex=q (y )=q (y ).As
∗ 1 ∗ 2
m m
1= q (y )=2x+ λp(y )=2x+λ(1 p(y ) p(y )),
∗ i i 1 2
− −
i=1 i=3
X X
wefindthatx=(1 λ(1 p(y ) p(y )))/2.
1 2
− − −
Weshouldhave
q (y ) q (y) q (y )
∗ 1 ∗ ∗ 2
λ=
p(y ) ≤ p(y) ≤ p(y )
1 2
for y y ,y , as we could otherwise apply Lemma 20 to reach a contradiction with
1 2
∈Y \{ }
thedefinitionofq .Thisleadstotheconstraint
∗
x 1 λ(1 p(y ) p(y )) 1
1 2
= − − − λ, i.e.λ ,
p(y ) 2p(y ) ≤ ≥ (1+p(y ) p(y ))
1 1 1 2
−
aswellas
x 1 λ(1 p(y ) p(y )) 1
1 2
λ = − − − , i.e.λ .
≤ p(y ) 2p(y ) ≤ (1 p(y )+p(y ))
2 2 1 2
−
Hencewehaveshownthatq mustbesomedistributionq definedasq (y )=λp(y )for
∗ λ λ i i
alli 3and
≥
1 λ(1 p(y ) p(y ))
1 2
q (y )=q (y )= − − − .
λ 1 λ 2
2
forsome
1 1
λ , .
∈ 1+p(y ) p(y ) 1 p(y )+p(y )
(cid:20) 1 − 2 − 1 2 (cid:21)
If we show that D(q p) is minimized for λ = 1 and that D(q p) =
λ k 1 (√p(y ) √p(y ))2 ∗ k
− 1 − 2
log(1 ( p(y ) p(y ))2), the proof is complete; this is the statement of Lemma
1 2
− − −
22below.
p p
LEMMA 22. Consider the family of distributions q
λ
defined for λ [0,(1 p(y 1)
∈ − −
p(y )) 1]asq (y )=λp(y )foralli 3and
2 − λ i i
≥
1 λ(1 p(y ) p(y ))
1 2
q (y )=q (y )= − − − .
λ 1 λ 2
2
Then
1
argminD(q p)=
λ
λ k 1 ( p(y 1) p(y 2))2
− −
p p32
and
minD(q p)=log(1 ( p(y ) p(y ))2).
λ 1 2
λ k − −
p p
PROOF. Wecompute
1 λ(1 p(y ) p(y )) (1 λ(1 p(y ) p(y )))2
1 2 1 2
D(q p)= − − − log − − −
λ
k 2 4p(y )p(y )
(cid:18) 1 2 (cid:19)
m
+ λp(y )log(λ)
i
i=3
X
1 aλ (1 aλ)2
= − log − +aλlog(λ),
2 b
(cid:18) (cid:19)
wherea=1 p(y ) p(y )andb=4p(y )p(y ),whichimpliesthatthederivativereads
1 2 1 2
− −
d a (1 aλ)2 a
D(q p)= log − +(1 aλ) − +alog(λ)+a
λ
dλ k −2 b − · 1 aλ
(cid:18) (cid:19) −
a bλ2
= log
2 (1 aλ)2
(cid:18) − (cid:19)
1 p(y ) p(y ) 4p(y )p(y )λ2
1 2 1 2
= − − log .
2 (1 λ(1 p(y ) p(y )))2
(cid:18) − − 1 − 2 (cid:19)
Wecanstudythesignofthisderivative,
d bλ2
sign D(q p) =sign 1 =sign bλ2 (1 aλ)2
dλ λ k (1 aλ)2 − − −
(cid:18) (cid:19) (cid:18) − (cid:19)
(cid:0) (cid:1)
=sign (b a2)λ2+2aλ 1 .
− −
Therootsofthispolynomialare (cid:0) (cid:1)
a √a2+b a2 a √b 1
λ = − ± − = − ± =
± b a2 b a2 a √b
− − ±
1 1
= = .
1 p(y ) p(y ) 2 p(y )p(y ) 1 ( p(y ) p(y ))2
1 2 1 2 1 2
− − ± − ∓
Thesignoftheleadingcoefficientofthpepolynomialis p p
sign(b a2)=sign(√b a)=sign(2 p(y )p(y )+p(y )+p(y ) 1)
1 2 1 2
− − −
=sign(( p(y )+ p(yp))2 1)=sign( p(y )+ p(y ) 1).
1 2 1 2
− −
Asaconsequence,thereareptwopossibpilities. p p
If p(y )+ p(y ) 1, then b a2, as well as √b a, is positive, and dD/dλ is
1 2
≥ − −
negativefrom λ =(a √b) 1 0 to λ =(a+√b) 1 andpositiveafterwards,in which
caseλp isthem−ip nimize− rofD− (q≤
p).
+ −
+ λ
k
If p(y )+ p(y ) 1, then b a2, as well as √b a is negative, and dD/dλ is
1 2
≤ − −
negativefromλ=0toλ =(a+√b) 1,thenpositiveuntilλ =(a √b) 1 andnegative
afterwp ards,inwhp ichcase+
eitherλ
ort− herightextremityofthe−domain− ofλi−
stheminimizer
+
of D(q p) onthedomainofλ.Yetthis rightextremity is (1 p(y ) p(y )) 1 issmaller
λ 1 2 −
k − −
thanλ ,since
−
p(y )+p(y ) p(y )+2 p(y )p(y )+p(y )=( p(y )+ p(y ))2,
1 2 1 1 2 2 1 2
≤
p p pMODEESTIMATIONWITHPARTIALFEEDBACK 33
implies
(1 p(y ) p(y )) 1 (1 ( p(y )+ p(y ))2) 1=λ ,
1 2 − 1 2 −
− − ≤ − −
hencetheminimizerhastobeλ +. p p
We concludethat λ is always the minimizer of D(q p). Hence,using that λ solves
+ λ +
+k
bλ2=(1 aλ)2,
−
1 aλ (1 aλ )2
+ +
D(q p)= − log − +aλ log(λ )
λ + +
+k 2 b
(cid:18) (cid:19)
1 aλ
= − + log λ2 +aλ log(λ )=log(λ ).
2 + + + +
Thisendsthecomputationoftheinformati(cid:0)onp(cid:1)rojection.
Toclosethis subsection,letusfinally provetheupperboundon min D(q p) from
Lemma1(infact,weprovealittle more).
q ∈Qn,− k
LEMMA 23. Let = y 1,...,y
m
,n 1,
Y { } ≥
= q P( ) n 1 N y /argmaxq(y)
n, − Y 1
Q − ∈ Y ∩ · ∈
and
(cid:8) (cid:12) (cid:9)
(cid:12)
= q P( ) argmaxq(y)=argmaxp(y) .
Q { ∈ Y | 6 }
Then
min D(q p) minD(q p)
q ∈Qn,− k ≤ q ∈Q k
m 1 λ(1 p(y ) p(y ))
1 2
+ max log(λ),log − − − +1
n − 2p(y )
(cid:26) (cid:18) 2 (cid:19)(cid:27) !
1 1
+ log 1+
n nq (y)
y ∈XY>0 (cid:18) ∗ (cid:19)
whereλ=(1 ( p(y ) p(y ))2) 1 and = y p(y)=0 .
1 2 − >0
− − Y { ∈Y | 6 }
p p
PROOF. Let q
∗
∈argmin
q
∈QD(q kp) be such that q ∗(y 1)=q ∗(y 2)= 1 −λ(1 −p( 2y 1) −p(y 2))
andq (y)=p(y)forλ=(1 ( p(y ) p(y ))2) 1 (asinLemma21).Thenforn>m,
∗ 1 2 −
− −
one can construct q such that q (y) q (y)+1/n for all y and such that
n n,+ n ∗
∈ Q p p ≤ ∈ Y
q (y)=0 if y .Thegeneralideaisasfollows:let q (y)=0forall y . Thenlet
n >0 n >0
6∈Y 6∈Y
q (y )= q (y )n /n andq (y )=( q (y )n +1)/n,andq (y)=( q (y)n +ǫ )/n
n 1 ∗ 1 n 2 ∗ 1 n ∗ y,n
⌊ ⌋ ⌊ ⌋ ⌊ ⌋
for y y ,y such that q (y)=0, with ǫ 0,1 chosen so that q (y)=1
∈Y\{ 1 2 } ∗ 6 y,n ∈{ } y n
(smalladjustmentscanbenecessarydependingonwhetherq (y )n Netc.). ∈Y
∗ 1
∈ P
Then
q (y) q (y) q (y)
n ∗ n
D(q p)= q (y)log = q (y)log + q (y)log .
n n n n
k p(y) p(y) q (y)
y (cid:18) (cid:19) y (cid:18) (cid:19) y (cid:18) ∗ (cid:19)
X∈Y X∈Y X∈Y
Thefirstsumcanbeboundedas
q (y) q (y) q (y)
∗ ∗ ∗
q (y)log = q (y)log + (q (y) q (y))log
n ∗ n ∗
p(y) p(y) − p(y)
y (cid:18) (cid:19) y (cid:18) (cid:19) y (cid:18) (cid:19)
X∈Y X∈Y X∈Y
m q (y )
∗ 2
D(q p)+ max log(λ),log ,
∗
≤ k n p(y )
(cid:26) (cid:18) 2 (cid:19)(cid:27)34
andthesecondsumcanbeboundedasfollows
q (y) 1
n
q (y)log (q (y)+1/n)log 1+
n ∗
q (y) ≤ nq (y)
y X∈Y (cid:18) ∗ (cid:19) y ∈XY>0 (cid:18) ∗ (cid:19)
1 1 1
q (y)log 1+ + log 1+
∗
≤ nq (y) n nq (y)
y ∈XY>0 (cid:18) ∗ (cid:19) y ∈XY>0 (cid:18) ∗ (cid:19)
1 1 1
q (y) + log 1+
∗
≤ nq (y) n nq (y)
y ∈XY>0 ∗ y ∈XY>0 (cid:18) ∗ (cid:19)
m 1 1
+ log 1+ .
≤ n n nq (y)
y ∈XY>0 (cid:18) ∗ (cid:19)
Thus
min D(q p) D(q p)
n
q ∈Qn,− k ≤ k
m q (y ) 1 1
∗ 2
D(q p)+ max log(λ),log +1 + log 1+ .
∗
≤ k n p(y ) n nq (y)
(cid:18) (cid:26) (cid:18) 2 (cid:19)(cid:27) (cid:19) y ∈XY>0 (cid:18) ∗ (cid:19)
ThisconcludestheproofoftheLemma.
Analternativeconstructionyields
min D(q p) D(q p)+C(m,p(y ),p(y ))/√n
∗ 1 2
q ∈Qn,− k ≤ k
for some constant C(m,p(y ),p(y )) that only depends on m,p(y ),p(y ) (rather than on
1 2 1 2
allp(y)),thoughatthecostofalessfavorableasymptoticbehaviourinn.
Sanity Check with Cramér-Chernoffmethod. We remark that the upperbound of Theo-
rem1canbeprovedmoredirectly.Considerfirstthesimpleunionbound
P(yˆ =y )=P min 1 1 0 =P 1 1 0
n
6
1
i=1
Yj=y
1 −
Yj=yi
≤
∪i 6=1 Yj=y
1 −
Yj=yi
≤
6 j [n] j [n]
(cid:0) X∈ (cid:1) (cid:0) (cid:8)X∈ (cid:9)(cid:1)
P 1 1 0 (m 1)P 1 1 0 .
≤
Yj=y
1 −
Yj=yi
≤ ≤ −
Yj=y
1 −
Yj=y
2 ≤
i=1 j [n] j [n]
X6 (cid:0)X∈ (cid:1) (cid:0)X∈ (cid:1)
ThetermP 1 1 0 canbeboundedwithChernoff’smethod:
j [n] Yj=y 1 − Yj=y 2 ≤
∈
(cid:0)P (cid:1)
LEMMA 24 (Chernoff’s bound). Let p P( ) be a distribution over , and (Y j) be n
∈ Y Y
independentsamplesdistributedaccordingtopwithp(y )>p(y ).Then
1 i
P( 1 1 0) exp n∆2 ,
Yj=y 1 − Yj=yi ≤ ≤ − i
j [n]
X∈ (cid:0) (cid:1)
where∆ isdefinedinEquation(5).
i
PROOF. Thiscanbeprovenusingthefactthat,forarandomvariableX,
P(X 0)=infP(exp(tX) 1) infE[etX].
≥ t>0 ≥ ≤t>0MODEESTIMATIONWITHPARTIALFEEDBACK 35
Asaconsequence,withX = X andX =1 1 ,
j j Yj=yi
−
Yj=y
1
P(X 0) infPE[etX]=infE[etX 1]n=infexp(nln(E[etX 1])).
≥ ≤t>0 t>0 t>0
Weareleftwithasimplecomputation,
infE[etX 1]=inf etp(y i)+(1 p(y i) p(y 1))+e −tp(y 1)
t>0 t>0 − −
(cid:0) (cid:1)
= 2 p(y )p(y )+(1 p(y ) p(y )) = 1 ( p(y ) p(y ))2
1 i 1 i 1 i
− − − −
(cid:16) p (cid:17) (cid:16) p p (cid:17)
whereweusedthattheinfimumisfoundforet= p(y )/p(y )>1.
1 i
p
In fact, we know from Cramér (1938) that Chernoff’s bound is asymptotically exponen-
tiallytight.Aswealsohave
P(yˆ =y ) P 1 1 0 ,
n
6
1
≥
Yj=y
1 −
Yj=y
2 ≤
j [n]
(cid:0)X∈ (cid:1)
wegetthat
limln(P(yˆ n 6=y 1)) =limln(P j ∈[n]1 Yj=y 1 −1 Yj=y 2 ≤0) =∆2.
n n 2
(cid:0)P (cid:1)
As mentioned in Section 2, Dinwoodie (1992) states that ln(P(yˆ = y ))/n is always
n 1
below min D(q p), while Sanov (1957) states that ln(P(yˆ = y6 ))/n converges to
q n 1
− ∈Q k 6
min D(q p)asngrowslargewhen isdiscrete.Thisshowswithoutanyfurthercom-
q
− putation∈ sQ thatmk
in D(q
p)=∆2,andY
impliestheupperboundofTheorem1.
q ∈Q k 2
A.2. AdditionalProofsforSection3.
LEMMA 3. Let be a Huffman tree with respect to a value function v on its vertices
T
suchthatv(R)=1,whereRistherootof .ThenforanyvertexV of ,wehave
T T
D (V) 2 log (1/v(V)) .
2
T ≤ ⌈ ⌉
Inotherwords,Huffmancodesare2-balanced.
PROOF. Letusproceedbyinductionondepth (V).Ifdepth (V)=0,i.e.V =R,orif
depth (V)=1, the statementis trivial. Letus asTsumethat deptTh (V) 2. Atsomepoint
duringTtheconstructionof ,theelementV wasmergedwithanothTerele≥ mentV tocreatea
′
T
newparentnode P. Atthatpoint, V andV werethetwo elementswith smallestvaluev in
′
theheap .Ifv(V ) v(V),thenv(P) 2v(V).Byinduction,
′
S ≥ ≥
depth (V)=depth (P)+1 2 log (1/v(P)) +1 2 log (1/2v(V)) +1
T T ≤ ⌈ 2 ⌉ ≤ ⌈ 2 ⌉
2 log (1/v(V)) 1 +1 2 log (1/v(V)) .
≤ ⌈ 2 − ⌉ ≤ ⌈ 2 ⌉
If v(V ) v(V), then any v(P) v(V), and as any other element V must be such that
′ ′′
v(V ) ≤ v(V), the parent P˜ (wh≥ ich results from the merging of P and some V ) of P
′′ ′′
satisfies≥ v(P˜) 2v(V).Byinduction,
≥
depth (V)=depth (P˜)+2 2 log (1/v(P˜)) +2 2 log (1/2v(V)) +2
T T ≤ 2 ≤ ⌈ 2 ⌉
l m
2 log (1/v(V)) 1 +2 2 log (1/v(V)) .
2 2
≤ ⌈ − ⌉ ≤ ⌈ ⌉
Thisconcludestheproof.36
LEMMA 4. WithN
i,y
denotingtheempiricalcount
j
[i]1 Yj=y,
∈
n 1 P
− p(y)E 1 1 log N y,i 22mlog (n),
−
i=1y
(Yj)
(cid:20)
{|Ny, ii p− (yip )(y) |≥21
}
Ny,i≥1 2
(cid:18)
i (cid:19)(cid:21)≤ 2
XX∈Y
PROOF. Lookingatit,weseethat
n 1
− p(y)E 1 1 log N y,i
−
i=1y
(Yj)
(cid:20)
{|Ny, ii p− (yip )(y) |≥21
}
Ny,i≥1 2
(cid:18)
i
(cid:19)(cid:21)
XX∈Y
n 1
− p(y)P N y,i −ip(y) 1 log (i)
≤ ip(y) ≥ 2 2
i=1y (cid:18)(cid:12) (cid:12) (cid:19)
XX∈Y (cid:12) (cid:12)
(cid:12) (cid:12)
UsingasimplificationofChernoff’s(cid:12) boundforBe(cid:12)rnoullivariables(Hoeffding,1963),weget
thefollowingbound
N ip(y) 1 ip(y)
P y,i − 2exp( ).
ip(y) ≥ 2 ≤ − 10
(cid:18)(cid:12) (cid:12) (cid:19)
(cid:12) (cid:12)
Hence (cid:12) (cid:12)
(cid:12) (cid:12)
n 1 n 1
− p(y)P N y,i −ip(y) 1 log (i) p(y)log (n) − 2√2exp( ip(y) )
ip(y) ≥ 2 2 ≤ 2 − 10
i=1y (cid:18)(cid:12) (cid:12) (cid:19) y i=1
XX∈Y (cid:12) (cid:12) X∈Y X
(cid:12) (cid:12)
(cid:12) (cid:12) p(y)
2log (n) 22mlog (n),
≤ 2 1 e p(y) ≤ 2
y X∈Y − − 10
p
where the lastinequality comesfrom the factthat p p/(1 e ) is upperboundedby 1
−10
7→ −
forp [0,1].Notethattheconstantcouldbeimproved,butnotdramatically.
∈
LEMMA 5. WithN
i,y
denotingtheempiricalcount
j
[i]1 Yj=y,
∈
N P
ln(2)E 1 log y,i 4m(ln(n)+1).
− (Yj) Ny,i−ip(y) <1 2 ip(y) ≤
(cid:20) {| ip(y) | 2} (cid:18) (cid:19)(cid:21)
PROOF. Wewilldevelopthelogarithmuptothesecondorder–higherordersdonoteasily
yieldbetterbounds.
N
ln(2)E 1 log y,i
− (Yj) Ny,i−ip(y) <1 2 ip(y)
(cid:20) {| ip(y) | 2} (cid:18) (cid:19)(cid:21)
N ip(y)
= ln(2)E 1 log 1+ y,i −
− (Yj) Ny,i−ip(y) <1 2 ip(y)
(cid:20) {| ip(y) | 2} (cid:18) (cid:19)(cid:21)
2
N ip(y) 1 N ip(y)
= E 1 y,i − y,i −
− (Yj) Ny,i−ip(y) <1 ip(y) − 2(1+ξ)2 ip(y)
" {| ip(y) | 2} (cid:18) (cid:19) !#
where ξ is a random variable (inheriting its randomness from the (Y )) that belongs to the
j
interval 0,Ny,i−ip(y) if Ny,i−ip(y) 0 (respectively Ny,i−ip(y) ,0 if Ny,i−ip(y) 0) and
ip(y) ip(y) ≥ ip(y) ip(y) ≤
dependshon the valueiof N y,i. Note that the Taylor ehxpansion of ix ln(1 + x) is valid
7→MODEESTIMATIONWITHPARTIALFEEDBACK 37
because Ny,i−ip(y) < 1.Now
ip(y) 2
(cid:12) (cid:12)
(cid:12) (cid:12) 2
N ip(y) 1 N ip(y)
(cid:12) (cid:12) y,i y,i
1 − + −
Ny,i−ip(y) <1 − ip(y) 2(1+ξ)2 ip(y)
{| ip(y) | 2} (cid:18) (cid:19) !
2
N ip(y) N ip(y)
y,i y,i
<1 − +2 −
Ny,i−ip(y) <1 − ip(y) ip(y)
{| ip(y) | 2} (cid:18) (cid:19) !
2
N ip(y) N ip(y) N ip(y)
y,i y,i y,i
2 − − +1 −
≤ ip(y)+1 − ip(y) Ny,i−ip(y) 1 ip(y)
(cid:18) (cid:19) {| ip(y) |≥2}(cid:18) (cid:19)
2 2
N ip(y) N ip(y) N ip(y)
y,i y,i y,i
2 − − +1 2 −
≤ ip(y) − ip(y) Ny,i−ip(y) 1 ip(y)
(cid:18) (cid:19) {| ip(y) |≥2} (cid:18) (cid:19)
2
N ip(y) N ip(y)
y,i y,i
4 − −
≤ ip(y) − ip(y)
(cid:18) (cid:19)
where the first inequality is valid because 1+ξ > 1. The last two inequalities are used to
2
avoid computing means of truncated binomials, together with the observations that directly
bounding y,ip(y)E[1 Ny,i−ip(y) 1 Ny, ii p− (yip )(y) ] with a bound on P( |Ny, ii p− (yip )(y) | ≥ 1 2)
{| ip(y) |≥2}
does not y Pield a good dependence in(cid:16)p(y), henc(cid:17)e the rather crude upper bound with the
square.Usingthestandardformulasforthemomentsofbinomialvariables,wegetthat
n 1 2
− p(y)E 4 N y,i −ip(y) N y,i −ip(y)
(Yj)
ip(y) − ip(y)
" #
i=1y (cid:18) (cid:19)
XX∈Y
n −1 4ip(y)2(1 p(y)) n −1 4
= − 0 4m(ln(n)+1).
(ip(y))2 − ≤ i ≤
i=1y i=1y
XX∈Y XX∈Y
Thisprovesthelemma.
A.3. AdditionalProofsforSection4.
LEMMA 6. Theevent definedby(15)satisfies
A
2
n ǫ ǫ
(16) P(c ) 2m+1exp r −1 r −1 − r .
A ≤ − 2 4
!
(cid:18) (cid:19)
PROOF. Considerthecrudeunionbound
ǫ ǫ
P(c ) P S 2 Y s.t. pˆ r 1(S) p(S) > r −1 − r
A ≤ ∃ ∈ | − − | 4
(cid:18) (cid:19)
ǫ ǫ
+P S 2 Y s.t. pˆ r(S) p(S) > r −1 − r .
∃ ∈ | − | 4
(cid:18) (cid:19)
Summing over all possible sets S and using Hoeffding’s inequality, we find the following
(ratherrough)bounds:
2
ǫ ǫ n ǫ ǫ
P S 2 Y s.t. pˆ r 1(S) p(S) > r −1 − r 2mexp r r −1 − r
∃ ∈ | − − | 4 ≤ − 2 4 !
(cid:18) (cid:19) (cid:18) (cid:19)38
and
2
ǫ ǫ n ǫ ǫ
P S 2 Y s.t. pˆ r(S) p(S) > r −1 − r 2mexp r −1 r −1 − r ,
∃ ∈ | − | 4 ≤ − 2 4
!
(cid:18) (cid:19) (cid:18) (cid:19)
Hencethelemma.
LEMMA 7. When running Algorithm 7 with the schedule of Theorem 4, the expected
numberofqueriesneededforroundr satisfies
r
4 4 C
E[T ] 2r log +m2m+1exp
r ≤ 2 p(y ) − 3 m2
(cid:18)(cid:24) (cid:18) 1 (cid:19)(cid:25) (cid:18) (cid:18) (cid:19) (cid:19)(cid:19)
forsomeconstantC >0.
PROOF. Dissociatingthenumberofquerieswhen holdsandwhenitdoesnot,together
A
withEquation(16),theexpectednumberofqueriesneededforroundr 2satisfies
≥
4
E[T ] n 2 log +mP(c )
r ≤ r 2 p(y ) A
(cid:18) (cid:24) (cid:18) 1 (cid:19)(cid:25) (cid:19)
2
4 n ǫ ǫ
≤n
r
2 log
2 p(y )
+m2m+1exp
−
r 2−1 r −1 4− r
(cid:24) (cid:18) 1 (cid:19)(cid:25) (cid:18) (cid:19) !!
r 1
2r 2 log 4 +m2m+1exp 2r 6 2 − (ǫ ǫ )2
≤ 2 p(y ) − − 3 0 − 1
(cid:24) (cid:18) 1 (cid:19)(cid:25) (cid:18) (cid:19) !!
r
4 4 C
2r 2 log +m2m+1exp
≤ 2 p(y ) − 3 m2
(cid:18) (cid:24) (cid:18) 1 (cid:19)(cid:25) (cid:18) (cid:18) (cid:19) (cid:19)(cid:19)
forsomeconstantC >0.Inthespecialcaser=1,weneedatmost
E[T ] n m=2m
1 1
≤
queriestocompletetheround.
A.4. AdditionalProofsforSection5. LetusfirstproveProposition8,whichwerestate
here.
PROPOSITION 8. Letp P( )withp(y 1)>p(y i)foralli [m].Wehavethefollowing
∈ Y ∈
inequality,with :=p(y ) p(y )and∆2= ln(1 ( p(y ) p(y ))2),
∇i 1 − i i − − 1 − i
p(y 1)
p(y ) 2 ∆ 2
p
4p(y )
p
2.
ln(1 p(y ))
1 ∇−i
≤
−i
≤
1 ∇−i
1
− −
In particular,if p(y ) is boundedawayfrom 1, then p(y )/ln(1 p(y ) is boundedaway
1 1 1
from0and∆ 2 p(y ) 2 uptoamultiplicativec− onstants. −
−i
≃
1 ∇−i
PROOF. ( p(y 1)+ p(y i))2 2(p(y i)+p(y 1)),togetherwithln(1+x) x,imply
≤ ≤
p p(p(y ) p(y ))2 (p(y ) p(y ))2 (p(y ) p(y ))2
∆2= ln 1 1 − i 1 − i 1 − i .
i − − ( p(y 1)+ p(y i))2 ≥ 2(p(y 1)+p(y i)) ≥ 4p(y 1)
(cid:16) (cid:17)
Fortheotherside,notepthatlnisaplwaysaboveitscords,henceforx [a,b],
∈
(x a)
ln(1 x) − (ln(1 b) ln(1 a))+ln(1 a).
− ≥ b a − − − −
−MODEESTIMATIONWITHPARTIALFEEDBACK 39
Appliedtoa=0andb=p(y ),wegetthat
1
xln(1 p(y ))
1
x [0,p(y )], ln(1 x) − .
1
∀ ∈ − ≥ p(y )
1
Asaconsequence,
(p(y ) p(y ))2 ln(1 p(y )) (p(y ) p(y ))2ln(1 p(y ))
∆2 − 1 − i − 1 − 1 − i − 1 .
i ≤ ( p(y 1)+ p(y i))2 p(y 1) ≤ p(y 1) p(y 1)
Thiscompletepstheproofp.
Wenowrecallthefollowingclassicalconcentrationresult.
LEMMA 25 (Chernoff’s bound). Let (X i)n
i=1
be independent Bernoulli variables with
meanp.Foranyλ 0,
≥
1 nλ2p
P X >(1+λ)p exp .
i
n ≤ −2+λ
i [n] (cid:18) (cid:19)
X∈
 
Similarly,forλ (0,1)
∈
1 nλ2p
P X <(1 λ)p exp .
i
n − ≤ − 2
i [n] (cid:18) (cid:19)
X∈
 
PROOF. Those are classical relaxations of some results that can be found in Hoeffding
(1963).
LEMMA 9. Forany c>1, let pˆ
r
bethe empiricalprobabilityassociatedwith r random
samples(Y ) independentlydistributedaccordingtop P( ).Itholdsthat
i i r
≤ ∼ Y
c+1 1
r ln(1/δ), P(pˆ (y)>cp(y)) δ
∀ ≥ (c 1)2p(y) r ≤
−
and
c2 1
r ln(1/δ), P(pˆ (y)<c 1p(y)) δ.
∀ ≥ (c 1)2 p(y) r − ≤
−
PROOF. ThisfollowsfromLemma25since
r(c 1)2p(y)
P(pˆ (y)>cp(y))=P(pˆ (y)>(1+(c 1))p(y)) exp − −
r r
− ≤ 2+(c 1)
(cid:18) − (cid:19)
r(c 1)2p(y)
=exp − − exp(ln(1/δ)=δ,
c+1 ≤
(cid:18) (cid:19)
and
P(pˆ (y)>c 1p(y))=P(pˆ (y)>(1 (1 c 1))p(y))
r − r −
− −
r(1 c 1)2p(y)
−
exp − − exp(ln(1/δ))=δ.
≤ 2 ≤
(cid:18) (cid:19)
ThisexplainstheresultoftheLemma.40
theL eE vM enM tA 10. Foranyr ∈N ≥1,δ>0andc>1,letyˆ r=argmax
y
∈Ypˆ r(y)andconsider
= rpˆ (yˆ ) cln(1/δ) .
r r r
A { ≤ }
Then
2c2 c 1
r − ln(1/δ), P(c ) mδ,
r
∀ ≤ 2c+1+√1+8cp(y ) A ≤
1
and
c2 1
r ln(1/δ), P( ) δ.
r
∀ ≥ c+1 √1+2cp(y ) A ≤
1
−
PROOF. Wecanupperboundtheprobabilityof
r
nothappeningwith
A
P(c )=P(rpˆ (yˆ )>cln(1/δ))=P(rmaxpˆ (y)>cln(1/δ))
r r r r
A y
∈Y
P(rpˆ (y)>cln(1/δ)) mP(rpˆ (y )>cln(1/δ)).
r r 1
≤ ≤
y
X∈Y
Letusset
cln(1/δ) cln(1/δ)
α = 1, i.e., r= .
r
rp(y ) − p(y )(1+α )
1 1 r
Notethatα 0whenr 2c2 2c 1 ln(1/δ). UsingLemma25leadsto
r ≥ ≤ 2c+1+−√1+8cp(y 1)
P(c ) mP pˆ (y ) p(y )>cln(1/δ)/r p(y ) =mP pˆ (y ) p(y )>α p(y )
r r 1 1 1 r 1 1 r 1
A ≤ − − −
(cid:16) (cid:17) (cid:16) (cid:17)
rα2p(y ) cln(1/δ)α2
mexp − r 1 =mexp − r .
≤ 2+α (1+α )(2+α )
(cid:18) r (cid:19) (cid:18) r r (cid:19)
Wecheckthat
2c2 2c 1 c 2c2 2c
r − ln(1/δ) −
≤ 2c+1+√1+8cp(y ) ⇔ 1+α ≤ 2c+1+√1+8c
1 r
3+√1+8c cα2
α (c 1)α2 3α 2 0 r 1,
⇔ r ≥ 2(c 1) ⇒ − r − r − ≥ ⇔ (1+α )(2+α ) ≥
r r
−
whichallowsustoconcludethatP(c ) mδ.Thisexplainstheconditiononrstatedinthe
r
A ≤
Lemma.
Wecansimilarlylowerboundtheprobabilityof happeningwith
r
A
P( )=P(rpˆ (yˆ )<cln(1/δ))=P(rmaxpˆ (y)<cln(1/δ)) P(rpˆ (y )<cln(1/δ)).
r r r r r 1
A y ≤
∈Y
Thistimeweset
cln(1/δ) cln(1/δ)
α =1 , i.e., r= .
r
− rp(y ) p(y )(1 α )
1 1 r
−
Onecancheckthatifr c2 1 ln(1/δ),thenα (0,1).UsingLemma25,
≥ c+1 √1+2cp(y 1) r ∈
−
P( ) P pˆ (y ) p(y )<cln(1/δ)/r p(y ) =P pˆ (y ) p(y )< α p(y )
r r 1 1 1 r 1 1 r 1
A ≤ − − − −
(cid:16) (cid:17) (cid:16) (cid:17)
rα2p(y ) cln(1/δ)α2
exp − r 1 exp − r .
≤ 2 ≤ 2(1 α )
(cid:18) (cid:19) (cid:18) − r (cid:19)MODEESTIMATIONWITHPARTIALFEEDBACK 41
Wecheckthat
c2 1 c c2
r ln(1/δ)
≥ c+1 √1+2cp(y ) ⇔ 1 α ≥ c+1 √1+2c
1 r
− − −
√1+2c 1 cα2
α − cα2+2α 2 0 r 1.
⇔ r ≥ c ⇒ r r − ≥ ⇔ 2(1 α ) ≥
r
−
WeconcludethatP( ) δ forther definedintheLemma.
r
A ≤
LEMMA 11. Letuswriteσ˜ r= 3p(y 1)ln(1/δ r)/r,anddefinetheevents
• A 1= pˆ r(y 1) p(y 1)/2forallpr 4ln(1/δ r)/p(y 1) ,
{ ≥ ≥ }
• A = pˆ (yˆ ) 2p(y )forallr 4ln(1/δ )/p(y ) ,
2 r r 1 r 1
{ ≤ ≥ }
• A = pˆ (y ) p(y ) σ˜ forallr 4ln(1/δ )/p(y )andi [m],
3 r i i r r 1
{| − |≤ } ≥ ∈
• A = σ pˆ (yˆ )forallr 4ln(1/δ )/p(y ) .
4 r r r r 1
{ ≥ ≤ }
ThenP(A ) 1 δ/6fori 1,2,4 andP(A ) 1 δ/3.
i 3
≥ − ∈{ } ≥ −
PROOF. We can apply the secondstatementof Lemma 9 with c=2 to y
1
to see that for
anyr 4 ln(1/δ )= 22 1 ln(1/δ ),wehave
≥ p(y 1) r (2 −1)2 p(y 1) r
P(pˆ (y )<p(y )/2) δ .
r 1 1 r
≤
Hence
P(cA ) δ δ δ/6.
1 r r
≤ ≤ ≤
r ≥p(y4 1X)ln(1/δr) Xr ≥1
Similarly,wecanapplythefirststatementofLemma9withc=2toy toseethatforany
1
r 4 ln(1/δ ) 2+1 1 ln(1/δ )andanyy ,wehave
≥ p(y 1) r ≥ (2 −1)2 p(y 1) r ∈Y
P(pˆ (y)>2p(y )) P(pˆ (y )>2p(y )) δ .
r 1 r 1 1 r
≤ ≤
Hence,usingaunionboundoverally ,wefindthat
∈Y
4
(26) P(pˆ (yˆ)<2p(y )) mδ r ln(1/δ ),
r 1 r r
≤ ∀ ≥ p(y )
1
thus
P(cA ) mδ mδ δ/6.
2 r r
≤ ≤ ≤
r ≥p(y4 1X)ln(1/δr) Xr ≥1
We can now consider A . Let i 1,...,m . Applying10 Lemma 25, we get that if r
3
∈{ } ≥
4 ln(1/δ )(hence 4ln(1/δr) (0,1)),then
p(y 1) r p(y 1)r ∈
q
3p(y )ln(1/δ ) 3p(y )ln(1/δ )
P pˆ (y )<p(y ) 1 r =P pˆ (y )<p(y ) 1 1 r
r i i −
r
r
!
r i i −s p(y i)2r
!!
rp(y )3p(y )ln(1/δ )
i 1 r
exp
≤ − 2p(y )2r
(cid:18) i (cid:19)
3
=exp ln(1/δ ) δ .
r r
− 2 ≤
(cid:18) (cid:19)
10NotethatwecanalsouseHoeffdinginequality,whichleadsto
P(pˆr(yi) p(yi)+σ˜r) exp( rσ˜r2 /2)
≥ ≤ −
whichistighterwhenp(yi)>1 σ˜r/2.
−42
UsingthistimethesecondinequalityofLemma25,andwritingσ˜ = 3p(y 1)ln(1/δr),we
r r
seethatforanyr 1wehave q
≥
3p(y )ln(1/δ ) σ˜
P pˆ (y )>p(y )+ 1 r =P pˆ (y )>p(y ) 1+ r
r i i r i i
r p(y )
r ! (cid:18) (cid:18) i (cid:19)(cid:19)
σ˜2
exp
rp(y i) p(yr
i)2
=exp
rσ˜ r2
≤ − 2+ p(σ˜ yr
i)
 (cid:18)−2p(y i)+σ˜ r(cid:19)
 
rσ˜2
exp r .
≤ −2p(y )+σ˜
(cid:18) 1 r(cid:19)
Furthermore,ifr 4 ln(1/δ ),thenσ˜ = 3p(y 1)ln(1/δr) 3p(y )andconsequently
≥ p(y 1) r r r ≤ 4 1
q q
rσ˜2 rσ˜2 3
exp r exp r exp ln(1/δ ) δ .
r r
(cid:18)−2p(y 1)+σ˜ r(cid:19)≤ − p(y 1)(2+ 3 4)≤ − 2+ 3 4≤
 q   q 
Combiningthosetwobounds,wefindthat
m
3p(y )ln(1/δ )
P(cA ) P pˆ (y ) p(y ) > 1 r 2mδ δ/3.
3 r i i r
≤ | − | r ≤ ≤
r !
r ≥p(y4 1X)ln(1/δr)Xi=1 Xr ≥1
LetusnowconsiderA .WecanapplyLemma10(with c=24)toseethatif
4
4 2 242 24 1
r ln(1/δ ) · − ln(1/δ ),
r r
≤ p(y ) ≤ 2 24+1+√1+8 24p(y )
1 1
· ·
then
P(σ <pˆ (yˆ))=P(rpˆ (yˆ)>24ln(1/δ )) P(rpˆ (yˆ )>24ln(1/δ )) mδ .
r r r r r all r r
≤ ≤
Hence,usinganunionboundoverallr 4 ln(1/δ ),wegetthat
≤ p(y 1) r
P(cA ) mδ mδ δ/6.
4 r r
≤ ≤ ≤
r ≤p(y4 1X)ln(1/δr) Xr ≥1
Thisendstheproof
LEMMA 14. InthesettingofTheorem5,withtheevent(A i)
i [n]
definedinLemma11,
∈
E T
l|
n(A (1j /) δj ∈[4] ≤324p(y 21)
+216
m
p(y i) |log 2(p(y i))
|p(y 21)
+o(1)
(cid:2) (cid:3) ∇2 i=1 ∇i
X
PROOF. Forr 1suchthatAlgorithm8hasnotyetterminatedatthestartofroundr,let
≥
S be the set of all classesthat have been eliminated in the previous rounds (it is a random
r
set), andlet Q be the numberof queriesnecessaryto identify Y accordingto the partition
r r
S y y S using first the query 1 , then a Huffman tree adapted to the
{ (renr } or∪ m{ al{ ize} d| )e∈ mY pi\ ricr a}
ldistribution pˆr
onYr∈S Sr
ifY S .HenceQ =1ifY S ,
andasinSubsection3.4,
1 −pˆr(Sr) Y\ r r 6∈ r r r ∈ r
1 pˆ (S )
Q 1+2+2 1 log − r r +1 m
r ≤ · {pˆr(y) 6=0 } 2
(cid:18)
pˆ r(y)
(cid:19)
{pˆr(y)=0 }MODEESTIMATIONWITHPARTIALFEEDBACK 43
ifY =y S .
r r
Fory ∈Y\ y ,letr(y)beasabovethesmallestr Nsuchthat
1 0
∈Y\{ } ∈
1
r>108 p(y )ln(1/δ )
(p(y ) p(y))2 1 r
1
−
for all r r , with the additional convention that r(y ):=r(y ). As seen in Lemma 13, if
0 1 2
≥
A ,A ,A andA hold,thentheclassy necessarilybelongsto S atthe startofround r as
1 2 3 4 r
soon as r >r(y), hence 1 1 . Thus the conditional expectation of Q with
respecttotheevents A
4{y 6∈ sS ar t} is≤ fies{r ≤r(y) } r
{ i }i=1
E Q A 4 3+E 1 m A 4
r |{ i }i=1 ≤ {pˆr(y)=0 } { i }i=1
(cid:2) (cid:3)
+2E
1(cid:2)
1
(cid:12)
(cid:12) log
(cid:3)1 −pˆ r(S r)
A 4
(cid:20)
{pˆr(y) 6=0 } {y 6∈Sr} 2
(cid:18)
pˆ r(y) (cid:19)(cid:12){ i }i=1
(cid:21)
(cid:12)
3+E 1 m A 4 (cid:12)
≤ {pˆr(y)=0 } { i }i=1 (cid:12)
(cid:2) (cid:12) (cid:3) 1
+2E 1 1 (cid:12) log A 4 .
(cid:20)
{pˆr(y) 6=0 } {r ≤r(y) } 2 (cid:18)pˆ r(y) (cid:19)(cid:12){ i }i=1
(cid:21)
(cid:12)
NowletΩbetheprobabilityspaceofallpossibleoutcomes,andletS(cid:12)(ω)denotethe(positive)
(cid:12)
randomvariable1 1 log 1 .Then
{pˆr(y) 6=0
}
{r ≤r(y)
}
2 pˆr(y)
(cid:16) (cid:17)
1
E S(ω) A 4 = S(ω)d
{ i }i=1 P( 4 A ) ω
(cid:2) (cid:12) (cid:3)
∩i=1 i Z ∩4 i=1Ai
(cid:12) 1 1
S(ω)d = E[S(ω)].
≤ P( 4 A ) ω P( 4 A )
∩i=1 i ZΩ ∩i=1 i
AsP( 4 A ) 1 δ,thismeansthat
∩i=1 i ≥ −
1
E 1 1 log A 4
(cid:20)
{pˆr(y) 6=0 } {r ≤r(y) } 2 (cid:18)pˆ r(y) (cid:19)(cid:12){ i }i=1
(cid:21)
(cid:12)
1 (cid:12) 1
E 1 1 log (cid:12)
≤ (1 −δ)
(cid:20)
{pˆr(y) 6=0 } {r ≤r(y) } 2 (cid:18)pˆ r(y)
(cid:19)(cid:21)
1 1
= p(y)1 E 1 log
(1 −δ)
y
{r ≤r(y) }
(cid:20)
{pˆr(y) 6=0 } 2 (cid:18)pˆ r(y)
(cid:19)(cid:21)
X∈Y
WehavealreadyshowninSubsection3.4thatforanyy suchthatp(y)=0,
6
1 4 rp(y)
E 1 log log (p(y))+ +2exp log (r).
(cid:20)
{pˆr(y) 6=0 } 2 (cid:18)pˆ r(y) (cid:19)(cid:21)≤ 2 p(y)rln(2) (cid:18)− 10
(cid:19)
2
As we have assumed that A ,A ,A and A hold, we know that Algorithm 8 terminates
1 2 3 4
at the end of some round R r(y ). Let T = R Q be the number of queries used to
≤ 2 r=1 r
identifythoseRsamples.Combiningtheresultsabove,wefindthat
P
R
E T A 4 3R+ E 1 m A 4
R |{ i }i=1 ≤ {pˆr(y)=0 } { i }i=1
r=1
(cid:2) (cid:3) X (cid:2) (cid:12) (cid:3)
(cid:12)
r(y)
p(y) 4 rp(y)
+2 log (p(y)) + +2exp log (r) ,
(1 δ) | 2 | p(y)rln(2) − 10 2
y r=1 − (cid:18) (cid:18) (cid:19) (cid:19)
X∈YX44
wherethetermsinthesumareunderstoodtobe0forthoseclassesy suchthatp(y)=0.As
1 canonlybenon-zeroforasingleindexr foreachclassy,
{pˆr(y)=0
}
R
E 1 m A 4 m2.
{pˆr(y)=0 } { i }i=1 ≤
r=1
X (cid:2) (cid:12) (cid:3)
Furthermore, (cid:12)
r(y)
4 4m(ln(R)+1)
rln(2) ≤ ln(2)
y r=1
X∈YX
and
r(y)
rp(y) p(y)
p(y)2exp log (r) 2 log (R) 22mlog (R)
− 10 2 ≤ 1 ep(y) 2 ≤ 2
y X∈YXr=1 (cid:18) (cid:19) y X∈Y − 10
p
duetop p/(1 e )beingupperboundedby11forp [0,1].Finally,usingthefactthat
−10
r(y )=17→ 08∆˜ 2p− (y )ln(1/δ)+o(ln(1/δ)) (asseeninEq∈ uation20),weseethat
i −i 1
r(y) m
p(y) log (p(y)) = p(y ) log (p(y )) r(y )
| 2 | i | 2 i | i
y r=1 i=1
X∈YX X
m
=108 p(y ) log (p(y )) ∆˜ 2p(y )ln(1/δ)+o(ln(1/δ)).
i | 2 i | −i 1
i=1
X
Thus
m
216
E T A 4 3r(y )+ p(y ) log (p(y )) ∆˜ 2p(y )ln(1/δ)+o(ln(1/δ))
R |{ i }i=1 ≤ 2 1 δ i | 2 i | −i 1
− i=1
(cid:2) (cid:3) X
2 4m(ln(r )+1)
+m2+ 2 +22mlog (r )
1 δ ln(2) 2 2
− (cid:18) (cid:19)
m
=324∆˜ 2p(y )ln(1/δ)+216 p(y ) log (p(y )) ∆˜ 2p(y )ln(1/δ)
−2 1 i | 2 i | −i 1
i=1
X
+o(ln(1/δ))
Thiscompletestheproof.
A.5. AdditionalProofsforSection6.
LEMMA 19. Withtheevents(A i)
i [4]
asdefinedinLemma15,
∈
E T (A i)
i [4]
p(y 1)
ln(1/δ)∈ ≤432
(p(y ) p(y
))2p(y 1)
(cid:2) (cid:12) (cid:3) 1 − 2
(cid:12)
4 10
+864 p(y) p(y ) log
p(y )2 1 2 p(y )
y s.t.p(y)<p(y )/2 1 (cid:24) (cid:18) 1 (cid:19)(cid:25)
∈Y X 1
1 10
+864 p(y) p(y ) log +o(1).
(p(y ) p(y))2 1 2 p(y )
y s.t.p(y) p(y )/2 1 − (cid:24) (cid:18) 1 (cid:19)(cid:25)
∈Y X≥ 1MODEESTIMATIONWITHPARTIALFEEDBACK 45
venP tiR oO nO tF h. atU rn (d yer )( :A =j r) (j ∈y[4 )] .,w We eh sa ev ee tt hh ae tb tho eun ad lg1 o{rY ii t, hr m6∈Sr n}e≤ ces1 s{ar r≤ilr y(Y ti e,r r) m}. inL ae tt eu ss aa td td heth ee nc do on f-
1 2
someroundR r(y ). Goingbackto Equation(25), wededucethatthe conditionalexpec-
1
≤
tationofT (for2 r R)withrespecttotheevents A 4 satisfies
r ≤ ≤ { i }i=1
nr
10
E T (A ) E 1+1 2 log +1 m (A )
(cid:2)
r
(cid:12)
i i ∈[4] (cid:3)≤
hXi=1
{r ≤r(Yi,r) }
(cid:18) (cid:24)
2 (cid:18)p(y 1)
(cid:19)(cid:25)
cBr
(cid:19)(cid:12) (cid:12)
i i ∈[4]
i
Usingthe(cid:12)samesimpleintegralargumentasintheproofofTheorem5,wefurth(cid:12)erseethatas
P( 4 A ) 1 δ,then
∩i=1 i ≥ −
nr
10
E 1+1 2 log +1 m A 4
"
Xi=1
{r ≤r(Yi,r) }
(cid:18) (cid:24)
2 (cid:18)p(y 1)
(cid:19)(cid:25)
cBr (cid:19)(cid:12) (cid:12){ i }i=1 #
(cid:12)
1 nr 10 (cid:12)
E 1+1 2 log +1(cid:12) m .
≤ 1 −δ "
i=1
{r ≤r(Yi,r) }
(cid:18) (cid:24)
2 (cid:18)p(y 1)
(cid:19)(cid:25)
cBr (cid:19)#
X
Wecanfurtherwrite
nr
10
E 1+1 2 log +1 m
"
i=1
{r ≤r(Yi,r) }
(cid:18) (cid:24)
2 (cid:18)p(y 1)
(cid:19)(cid:25)
cBr (cid:19)#
X
10
n +n P(cB )m+2n p(y)1 log
≤ r r r r
y
{r ≤r(y) }
(cid:24)
2 (cid:18)p(y 1)
(cid:19)(cid:25)
X∈Y
We can now bound the expectation of the total number T of queries required before the
algorithmterminatesattheendofroundR:
R R
(1 δ)E[T] n m+ n + n P(cB )m
1 r r r
− ≤
r=2 r=2
X X
R
10
+ 2n p(y)1 log .
r=2
r
y
{r ≤r(y) }
(cid:24)
2 (cid:18)p(y 1)
(cid:19)(cid:25)
X X∈Y
Observethat R n 2n 2n andthat
r=2 r ≤ R ≤ r(y 1)
P R
n m+ n P(cB )m m(n + n P(cB )) C˜
1 r r 1 r r
≤ ≤
r=2 r N
X X∈
forsomeconstantC˜>0independentfromδ (usingEquation(24)).Furthermore,
R r(y)
10 10
2n p(y)1 log log p(y) 2n
r=2
r
y
{r ≤r(y) }
(cid:24)
2 (cid:18)p(y 1) (cid:19)(cid:25)≤
(cid:24)
2 (cid:18)p(y 1)
(cid:19)(cid:25)y r=2
r
X X∈Y X∈Y X
10
log p(y)4n .
≤ 2 p(y ) r(y)
(cid:24) (cid:18) 1 (cid:19)(cid:25)y
X∈Y
CombiningthoseresultsandusingLemma(18),wefinallyseethat
10
(1 δ)E[T] C˜+2n + log p(y)4n
− ≤ r(y 1) 2 p(y ) r(y 1)
(cid:24) (cid:18) 1 (cid:19)(cid:25)y
X∈Y46
1
C˜+432 p(y )ln(1/δ )
≤ (p(y ) p(y))2 1 r
1
−
4 10
+864 p(y) p(y ) log ln(1/δ )
p(y )2 1 2 p(y ) r
y s.t.p(y)<p(y )/2 1 (cid:24) (cid:18) 1 (cid:19)(cid:25)
∈Y X 1
1 10
+864 p(y) p(y ) log ln(1/δ ),
(p(y ) p(y))2 1 2 p(y ) r
y s.t.p(y) p(y )/2 1 − (cid:24) (cid:18) 1 (cid:19)(cid:25)
∈Y X≥ 1
hencethat
1
E[T] 432 p(y )ln(1/δ)
≤ (p(y ) p(y ))2 1
1 2
−
4 10
+864 p(y) p(y ) log ln(1/δ)
p(y )2 1 2 p(y )
y s.t.p(y)<p(y )/2 1 (cid:24) (cid:18) 1 (cid:19)(cid:25)
∈Y X 1
1 10
+864 p(y) p(y ) log ln(1/δ)+o(ln(1/δ)).
(p(y ) p(y))2 1 2 p(y )
y s.t.p(y) p(y )/2 1 − (cid:24) (cid:18) 1 (cid:19)(cid:25)
∈Y X≥ 1
Thisendstheproofofthelemma.
Acknowledgments. Theauthorswould like to thankGilles Blanchard,RemiJezequiel,
MarcJourdan,NadavMerlis,andKarenUllrichforfruitfuldiscussions.
REFERENCES
AUDIBERT, J.-Y., MUNOS, R. and SZEPESVÁRI, C. (2009). Exploration–exploitation tradeoff using variance
estimatesinmulti-armedbandits.TheoreticalComputerScience.
AUER, P.,CESA-BIANCHI,N.,FREUND,Y.andSCHAPIRE,R.(1995).Gamblinginariggedcasino:Thead-
versarialmulti-armedbanditproblem.InAnnualSymposiumonFoundationsofComputerScience.
BRAVERMAN,M.,MAO,J.andPERES,Y.(2019).SortedTop-kinRounds.InCOLT.
BUBECK, S., MUNOS, R. and STOLTZ, G. (2010). Pure Exploration for Multi-Armed Bandit Problems. In
COLT.
CABANNES,V.,BACH,F.,PERCHET,V.andRUDI,A.(2022).ActiveLabeling:StreamingStochasticGradients.
InNeurIPS.
CAPPÉ,O.,GARIVIER,A.,MAILLARD,O.-A.,MUNOS, R.andSTOLTZ,G.(2013).Kullback–Leiblerupper
confidenceboundsforoptimalsequentialallocation.TheAnnalsofStatistics.
CESA-BIANCHI,N.,CESARI,T.andPERCHET,V.(2019).DynamicPricingwithFinitelyManyUnknownVal-
uations.InALT.
CESA-BIANCHI,N.,GENTILE,C.andZANIBONI,L.(2006).IncrementalAlgorithmsforHierarchicalClassifi-
cation.JournalofMachineLearningResearch.
CHEN,W.,WANG,Y.,YUAN,Y.andWANG,Q.(2016).CombinatorialMulti-ArmedBanditandItsExtension
toProbabilisticallyTriggeredArms.JournalofMachineLearningResearch.
COVER,T.andTHOMAS,J.(1991).ElementsofInformationTheory.Wiley.
CRAMÉR, H. (1938). Surun nouveau théorème-limite delathéorie des probabilités. Actual Sci Ind. Colloque
consacréàlathéoriedesprobabilités.
CSISZÁR,I.andKÖRNER,J.(2011).InformationTheory:CodingTheoremsforDiscreteMemorylessSystems,
2ed.CambridgeUniversityPress.
CUCKER, F. and SMALE, S. (2002). On theMathematical Foundations of Learning. Bulletinof the American
MathematicalSociety.
DEMPSTER, A., LAIRD, N. and RUBIN, D. (1977). Maximum Likelihood from Incomplete Data via the EM
Algorithm.JournaloftheRoyalStatisticalSociety.SeriesB(Methodological)391-38.
DINWOODIE,I.(1992).MesuresdominantesetthéorèmedeSanov.Annalesdel’InstitutHenriPoincare.
EVEN-DAR,E.,MANNOR, S.andMANSOUR, Y.(2006).ActionEliminationandStoppingConditionsforthe
Multi-ArmedBanditandReinforcementLearningProblems.JournalofMachineLearningResearch.MODEESTIMATIONWITHPARTIALFEEDBACK 47
FIEZ, T., JAIN, L.,JAMIESON, K. and RATLIFF, L. (2019). SequentialExperimental Designfor Transductive
LinearBandits.InNeurIPS.
GANGAPUTRA,S.andGEMAN,D.(2006).ADesignPrincipleforCoarse-to-FineClassification.InConference
onComputerVisionandPatternRecognition.
GRÜNWALD,P.(2007).Theminimumdescriptionlengthprinciple.MITpress.
HOEFFDING,W.(1963).ProbabilityInequalitiesforSumsofBoundedRandomVariables.JournaloftheAmeri-
canStatisticalAssociation5813-30.
HUFFMAN, D.(1952).AMethodfortheConstructionofMinimum-RedundancyCodes.ProceedingsoftheIn-
stituteofRadioEngineers401098–1101.
JAMIESON,K.andNOWAK,R.(2011).ActiveRankingusingPairwiseComparisons.InNeurIPS.
KORF,R.(1998).Acompleteanytimealgorithmfornumberpartitioning.ArtificialIntelligence106181–203.
MATHEWS, G.(1896).OnthePartitionofNumbers.ProceedingsoftheLondonMathematicalSociety28486-
490.
OPENAI(2023).GPT-4TechnicalReportTechnicalReport,OpenAI.
ROBBINS,H.(1955).ARemarkonStirling’sFormula.TheAmericanMathematicalMonthly.
SANOV, I. N. (1957). Ontheprobability of largedeviations of random variables. Matematicheskii Sbornik 42
11-44.
SHANNON,C.(1948).AMathematicalTheoryofCommunication.BellSystemTechnicalJournal27379–423.
TOUVRON, H., LAVRIL, T., IZACARD, G., MARTINET, X., LACHAUX, M.-A., LACROIX, T., ROZIÈRE, B.,
GOYAL,N.,HAMBRO,E.,AZHAR,F.,RODRIGUEZ,A.,JOULIN,A.,GRAVE,E.andLAMPLE,G.(2023).
LLaMA:OpenandEfficientFoundationLanguageModelsTechnicalReport,Meta.
VALIANT,L.(1984).Atheoryofthelearnable.CommunicationsoftheACM.
VITTER,J.(1987).DesignandAnalysisofDynamicHuffmanCodes.JournaloftheACM34825–845.
ZHU, B., JORDAN, M. and JIAO, J. (2023). Principled Reinforcement Learning with Human Feedback from
PairwiseorK-wiseComparisons.InICML.
ZIEGLER,D., STIENNON, N., WU, J., BROWN, T., RADFORD, A., AMODEI, D., CHRISTIANO, P. and IRV-
ING,G.(2020).Fine-TuningLanguageModelsfromHumanPreferencesTechnicalReport,OpenAI.