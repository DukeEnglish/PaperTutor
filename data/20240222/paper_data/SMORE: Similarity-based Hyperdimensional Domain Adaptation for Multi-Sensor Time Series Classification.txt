SMORE: Similarity-based Hyperdimensional Domain Adaptation
for Multi-Sensor Time Series Classification
JunyaoWang,MohammadAbdullahAlFaruque
{junyaow4,alfaruqu}@uci.edu
DepartmentofComputerScience,UniversityofCalifornia,Irvine,USA
ABSTRACT Distribution Shift
Manyreal-worldapplicationsoftheInternetofThings(IoT)employ ECG/EMG Multi-Sensor OOD Samples
machinelearning(ML)algorithmstoanalyzetimeseriesinforma- Sensor Time Series
Sweat Modeling
tioncollectedbyinterconnectedsensors.However,distributionshift, Sensor Pressure
afundamentalchallengeindata-drivenML,ariseswhenamodel Sensor Prediction
Training Inference
isdeployedonadatadistributiondifferentfromthetrainingdata
andcansubstantiallydegrademodelperformance.Additionally,in- Domain Adaptation Needed!
creasinglysophisticateddeepneuralnetworks(DNNs)arerequired (a) An Example of Distribution Shift in Multi-Sensor Time Series Data
tocaptureintricatespatialandtemporaldependenciesinmulti-
90 90
sensortimeseriesdata,oftenexceedingthecapabilitiesoftoday’s 80 80
edgedevices.Inthispaper,weproposeSMORE,anovelresource- 6 60 0 6 60 0
efficientdomainadaptation(DA)algorithmformulti-sensortime
3 40 0 3 40 0 Standard 𝒌-fold
seriesclassification,leveragingtheefficientandparalleloperations
LODO
ofhyperdimensionalcomputing.SMOREdynamicallycustomizes 200
10 20 30 40 50
200
0.5k 1k 2k 4k 6k
test-timemodelswithexplicitconsiderationofthedomaincontext Iterations Dimensions
ofeachsampletomitigatethenegativeimpactsofdomainshifts.
(b) Comparing LODO CV and Standard k-fold CV of SOTA HDC
Ourevaluationonavarietyofmulti-sensortimeseriesclassifica-
tiontasksshowsthatSMOREachievesonaverage1.98%higher Figure1:MotivationofOurProposedSMORE
accuracythanstate-of-the-art(SOTA)DNN-basedDAalgorithms
amountofinformationnowadays,andthepotentialinstabilitiesof
with18.81×fastertrainingand4.63×fasterinference.
IoTsystems,alightweightandefficientdomain-adaptivelearning
approachformulti-sensortimeseriesdataiscriticallyneeded.
1 INTRODUCTION Brain-inspiredhyperdimensionalcomputing(HDC)hasbeen
With the emergence of the Internet of Things (IoT), many real- introducedasapromisingparadigmforedgeMLforitshighcompu-
worldapplicationsutilizeheterogeneouslyconnectedsensorsto tationalefficiencyandultra-robustnessagainstnoises[8,11,12].Ad-
collect information over the course of time, constituting multi- ditionally,HDCincorporateslearningcapabilityalongwithtypical
sensor time series data [1]. Machine learning (ML) algorithms, memoryfunctionsofstoring/loadinginformation,bringingunique
includingdeepneuralnetworks(DNNs),areoftenemployedto advantagesintacklingnoisytimeseries[8].Unfortunately,existing
analyzethecollecteddataandperformvariouslearningtasks.How- HDCsarevulnerabletoDS.Forinstance,asshowninFigure1(b),on
ever,distributionshift(DS),afundamentalchallengeindata-driven thepopularmulti-sensortimeseriesdatasetUSC-HAD[13],SOTA
ML,cansubstantiallydegrademodelperformance.Inparticular,the HDCsconvergeatnotablyloweraccuracyinleave-one-domain-out
excellentperformanceoftheseMLalgorithmsheavilyreliesonthe (LODO)cross-validation(CV)thaninstandard𝑘-foldCVregardless
criticalassumptionthatthetrainingandinferencedatacomefrom oftrainingiterationsandmodelcomplexity.LODOCVdevelops
thesamedistribution,whilethisassumptioncanbeeasilyviolated amodelwithalltheavailabledataexceptforonedomainleftout
asout-of-distribution(OOD)scenariosareinevitableinreal-world forinference,whilestandard𝑘-foldCVrandomlydividesdatainto
applications[2,3].Forinstance,asshowninFigure1(a),ahuman 𝑘 subsetswith𝑘 −1subsetsfortrainingandtheremainingone
activityrecognitionmodelcansystematicallyfailwhentestedon forevaluation.Suchperformancedegradationindicatesaverylim-
individualsfromdifferentagegroupsordiversedemographics. iteddomainadaptation(DA)capabilityofexistingHDCs.However,
Avarietyofinnovativedomainadaptationtechniqueshavebeen standard𝑘-foldCVdoesnotreflectreal-worldDSissuessincethe
proposedfordeeplearning(DL)[4–7].However,duetotheirlimited randomsamplingprocessintroducesdataleakage,enablingthe
capacityformemorization,DLapproachesoftenfailtoperformwell trainingphasetoincludeinformationfromallthedomainsand
onmulti-sensortimeseriesdatawithintricatespatialandtemporal thusinflatingmodelperformance.Toaddressthisproblem,wepro-
dependencies[1,8].Althoughrecurrentneuralnetworks(RNNs), poseSMORE,anovelHDC-basedDAalgorithmformulti-sensor
includinglongshort-termmemory(LSTM),haverecentlybeenpro- timeseriesclassification.Ourmaincontributionsarelistedbelow:
posedtoaddressthisissue,thesemodelsarenotablycomplicated • Tothebestofourknowledge,SMOREisthefirstHDC-based
andinefficienttotrain.Specifically,theircomplexarchitectures DAalgorithm.Byexplicitlyconsideringthedomaincontextof
requireiterativelyrefiningmillionsofparametersovermultiple eachsampleduringinference,SMOREprovidesonaverage1.98%
timeperiodsinpowerfulcomputingenvironments[9,10].Consid- higheraccuracythanSOTADL-basedDAalgorithmsonawide
eringtheresourceconstraintsofembeddeddevices,themassive rangeofmulti-sensortimeseriesclassificationtasks.
4202
beF
02
]GL.sc[
1v33231.2042:viXra
)%(
ycaruccA
)%(
ycaruccADAC’24,June23–27,2024,SanFrancisco,CA JunyaoWang,MohammadAbdullahAlFaruque
• Leveragingtheefficientandparallelhigh-dimensionaloperations, uniquepropertyofthehyperdimensionalspaceistheexistenceof
SMOREprovidesonaverage18.81×fastertrainingand4.63× largeamountsofnearlyorthogonalhypervectors,enablinghighly
fasterinferencethanSOTADL-basedDAalgorithms,providing parallel and efficient operations such as similarity calculations,
amoreefficientsolutiontotackletheDSchallenge. bundling,andbinding.Similarity(𝛿):calculationofthedistance
• WeevaluateSMOREacrossmultipleresource-constrainedhard- between two hypervectors. A common measure is cosine simi-
waredevicesincludingRaspberryPiandNVIDIAJetsonNano. larity.Bundling(+):element-wiseadditionofhypervectors,e.g.,
SMOREdemonstratesconsiderablylowerinferencelatencyand H𝑏𝑢𝑛𝑑𝑙𝑒 =H1+H2,generatingahypervectorwiththesamedi-
energyconsumptionthanSOTADL-basedDAalgorithms. mensionasinputs.Bundlingprovidesanefficientwaytocheckthe
existenceofahypervectorinabundledset.Inthepreviousexam-
2 RELATEDWORKS ple,𝛿(H𝑏𝑢𝑛𝑑𝑙𝑒,H1) ≫0while𝛿(H𝑏𝑢𝑛𝑑𝑙𝑒,H3)≈0(H3≠H1,H2).
Bundlingmodelshowhumanbrainsmemorize inputs.Binding
2.1 DomainAdaptation
(∗):element-wisemultiplicationoftwohypervectorstocreatean-
Distributionshift(DS)ariseswhenamodelisdeployedonadata othernear-orthogonalhypervector,i.e.H𝑏𝑖𝑛𝑑 = H1∗H2 where
distributiondifferentfromwhatitwastrainedon,posingserious 𝛿(H𝑏𝑖𝑛𝑑,H1) ≈0and𝛿(H𝑏𝑖𝑛𝑑,H2) ≈0.Duetoreversibility,i.e.,
robustnesschallengesforreal-worldMLapplications[4,5].Var- H𝑏𝑖𝑛𝑑 ∗H1 = H2, information from both hypervectors can be
ious innovative methodologies have been proposed to mitigate preserved.Bindingmodelshowhumanbrainsconnectinputs.Per-
DS,andcanbeprimarilycategorizedasdomaingeneralizations mutation(𝜌):asinglecircularshiftofahypervectorbymoving
(DG)anddomainadaptations(DA).DGtypicallyseekstoconstruct thevalueofthefinaldimensiontothefirstpositionandshiftingall
modelsbyidentifyingdomain-invariantfeaturessharedacrossmul- othervaluestotheirnextpositions.Thepermutedhypervectoris
tiplesourcedomains[2].However,itcanbechallengingtoextract nearlyorthogonaltoitsoriginalhypervector,i.e.,𝛿(𝜌H,H) ≈0.
commonfeatureswhenthereexistmultiplesourcedomainswith Permutationmodelshowhumanbrainshandlesequentialinputs.
distinctcharacteristics[6,8,14];thus,existingDGapproachesoften
failtoprovidecomparablehigh-qualityresultsasDA[7,15].In
contrast,DAgenerallyutilizesunlabeleddataintargetdomains 3.2 ProblemFormulation
toquicklyadaptmodelstrainedindifferentsourcedomains[4]. WeassumethatthereareK (K > 1)sourcedomains,i.e.,D𝑆 =
Unfortunately,mostexistingDAtechniquesrelyonmultipleconvo- {D1,D2,...,DK},intheinputspaceI,andwedenotetheout-
S S S
lutionalandfully-connectedlayerstotraindomaindiscriminators, putspaceasY.I consistsoftime-seriesdatafrom𝑚 (𝑚 ≥ 1)
requiringintensivecomputationsanditerativerefinement.Wepro- interconnectedsensors,i.e.,I={I1,I2,...,I𝑚}.Ourobjectiveis
poseSMORE,thefirstHDC-basedDAalgorithm,toprovideamore toutilizetrainingsamplesinIandtheircorrespondinglabelsinY
resource-efficientDAsolutionforedgeplatforms. totrainaclassificationmodel𝑓 :I→Ytocapturelatentfeatures
sothatwecanmakeaccuratepredictionswhengivensamplesfrom
2.2 HyperdimensionalComputing anunseentargetdomainD .Thekeychallengeisthatthejoint
T
distributionbetweensourcedomainsandtargetdomainscanbedif-
SeveralrecentworkshaveutilizedHDCasalightweightlearning
paradigmfortimeseriesclassification[16–18],achievingcompa- ferent,i.e,P (S I,Y) ≠P (T I,Y),andthusthemodel𝑓 canpotentially
rableaccuracytoSOTADNNswithnotablylowercomputational failtoadaptmodelstrainedonD tosamplesfromD .
S T
costs.However,existingHDCsdonotconsidertheDSchallenge. AsshowninFigure2,ourproposedSMOREstartswithmapping
Thiscanbeadetrimentaldrawbackasout-of-distribution(OOD) trainingsamplesfromtheinputspaceI toahyperdimensional
instancesareinevitableduringreal-worlddeploymentofMLap- spaceX withanencoderΩ (A),i.e.,X = Ω(I),thatpreserves
plications.HyperdimensionalFeatureFusion[3]identifiesOOD thespatialandtemporaldependenciesinI.Wethenseparatethe
samplesbyfusingoutputsofseverallayersofaDNNmodeltoa encodeddataintoKsubsets(X1,Y1),(X2,Y2),...,(X K,Y K)(B)
commonhyperdimensionalspace,whileitsbackboneremainsrely- basedontheirdomains.Inthetrainingphase,wetrainKdomain-
ingonresource-intensivemulti-layerDNNs,andasystematicway specificmodelsM = {M1,M2,...,M K}suchthatM𝑘 : X𝑘 →
totackleOODsampleshasyettobeproposed.DOMINO[8],anovel Y𝑘 (1 ≤ 𝑘 ≤ K) (C),andconcurrentlydevelopK expressive
HDC-basedDGmethod,constantlydiscardsandregeneratesbiased domaindescriptorsU = {U1,U2,...,U K}encodingthepattern
dimensionsrepresentingdomain-variantinformation;nevertheless, ofeachdomain(D).WhengivenainferencesampleI fromthe
T
itrequiressignificantlymoretrainingtimetoprovidereasonable targetdomainD ,wemapI tohyperdimensionalspacewiththe
T T
accuracy.OurproposedSMOREleveragesefficientoperationsof sameencoderΩusedfortraining,andidentifyOODsamples(E)
HDCtocustomizetest-timemodelsforOODsamples,providing withabinaryclassifierΦutilizingthedomaindescriptorsU,i.e.,
accuratepredictionswithoutcausingsubstantialcomputational Φ(Ω(I T),U).Finally,weconstructatest-timemodelM
T
based
overheadforbothtrainingandinference. ondomain-specificmodelsM(F)andwhetherthesampleisOOD
tomakeinferences(G)withexplicitconsiderationofthedomain
3 METHODOLOGY contextofI T,i.e.,Yˆ
I
=M T(Φ(Ω(I T),U),M).
3.1 HDCPrelimnaries
Inspiredbyhigh-dimensionalinformationrepresentationinhu- 3.3 Multi-SensorTimeSeriesDataEncoding
man brains, HDC maps inputs onto hyperdimensional space as WeemploytheencodingtechniquesinFigure3tocaptureand
hypervectors, each of which contains thousands of elements. A preservespatialandtemporaldependenciesinmulti-sensortimeSMORE:Similarity-basedHyperdimensionalDomainAdaptationforMulti-SensorTimeSeriesClassification DAC’24,June23–27,2024,SanFrancisco,CA
E OOD Detection F Test-Time Modeling G Reasoning
Inference Data cosine 𝓜 cosine
A 𝜹 𝟏 𝓜 𝟏 𝓜 𝟐 𝓒 𝟏𝝉𝓣 𝜹 𝟏
⋯𝜹 𝟐 ⋯ 𝓜 𝓣 ⋯ ⋯𝓒 𝟐𝝉 ⋯𝜹 𝟐
𝜹 𝓷 𝓜 𝟑 ⋯ 𝓜 𝓚 𝓒 𝒏𝝉 𝜹 𝓷
B Domains C Domain-Specific Modeling D Domain Descriptors
Similarity Model Update Models
Training Data 𝓧 ,𝓨 ∑𝒏𝟏 𝓗𝟏 𝓤
𝟏 𝟏 Modeling cosine 𝓜 𝒊(cid:2880)𝟏 𝒊 𝟏 sensor I sensor II 𝓧 ⋯𝟐,𝓨 𝟐 ⋯𝓒𝓒 𝟏𝒌𝟏𝒌 ⋯𝜹𝜹 𝟐𝟏 𝓜 ⋯𝟐𝟏 ∑ 𝒊𝒏 (cid:2880) ⋯𝟐 𝟏𝓗 𝒊𝟐 ⋯𝓤 𝟐
sensor m 𝓧 𝓚,𝓨 𝓚 𝓒 𝟏𝒌 𝜹 𝓷 𝓜 𝓚 ∑ 𝒊𝒏 (cid:2880)𝓚 𝟏𝓗 𝒊𝓚 𝓤 𝓚
Figure2:TheWorkflowofOurProposedSMORE
Sampling Window Vector Quantization Temporally Sorted Spatially Integrated
overallinformationfromsensor𝑖,and𝑚denotesthetotalnumber
𝓨
𝓨𝓨
𝒕
𝒕𝒕
𝟑
𝟐𝟏
𝒕𝟏
S 𝒕𝟐ens 𝒕o 𝟑r I
T
m ma inx 𝓗
𝓗
𝓗𝒕
𝒕
𝒕𝟏
𝟐
𝟑
𝝆 𝓗𝝆𝝆
𝓗
𝒕𝓗 𝟑𝒕𝟐𝒕𝟏
𝓢 +∗ 𝓗
o
f
hrf
yo
ps me en
rS
vs eo
en
crs
ts
o.
o
rF
r
so
I
Gr ao anu ndr de Sx Gea
n
′m
s
aop nrl de
I
cIin
ab
lcF
y
uig
lr
au
a
tnr ine
d
go3 m, (w Glye ∗c
g
Ho em
n
)eb +ri an (te Gini ′gn ∗f so Hir gm
n
′)aa .tt uio rn
e
𝓨 𝓨𝓨 ′ ′′ 𝒕 𝒕𝒕 𝟏 𝟑𝟐
𝒕𝟏
S 𝒕e 𝟐ns 𝒕o 𝟑r II
T
m ma inx 𝓗𝓗 𝓗 𝒕(cid:4593)𝒕(cid:4593) 𝒕(cid:4593) 𝟏𝟐
𝟑
𝝆 𝝆 𝓗𝝆 𝓗 𝒕(cid:4593)𝓗 𝟑𝒕(cid:4593) 𝟐𝒕(cid:4593) 𝟏 𝓢′ ∗ 𝓗′
A
s3 p. s4
acsh eo
(D
w
Ano )m
,in
Sa MFi in
g
Ou- RrS Eep
2
se e,c pai aftfi
re
arc temM
sa
to
p
rapd
ii
nne igl ni gn dag
st aa mt po leh syp ine tr odim Ken susi bo sn ea tsl
basedontheirdomains(B),whereKrepresentsthetotalnumber
Figure3:HDCEncodingforMulti-SensorTimeSeriesData
ofdomains..WethenemployahighlyefficientHDCalgorithm
seriesdatawhenmappinglow-dimensioninputstohyperdimen- tocalculateadomain-specific modelforeverydomain(C),i.e.,
sionalspace.Wesampletimeseriesdatain𝑛-gramwindows;in M = {M1,M2,...,M K}. Our approach aims to provide high-
eachsamplewindow,thesignalvalues(𝑦-axis)storetheinforma- qualityresultswithfastconvergencebyidentifyingcommonpat-
tionandthetime(𝑥-axis)representsthetemporalsequence.We ternsduringtrainingandeliminatingmodelsaturations.Webundle
firstassignrandomhypervectorsH𝑚𝑎𝑥 andH𝑚𝑖𝑛torepresentthe datapointsbyscalingaproperweighttoeachofthemdepending
maximumandminimumsignalvalues.Wethenperformvector onhowmuchnewinformationisaddedtoclasshypervectors.In
quantizationtovaluesbetweenthemaximumandminimumvalues particular,eachdomain-specificmodelM𝑘(1≤𝑘 ≤K)consists
to generate vectors with a spectrum of similarity to H𝑚𝑎𝑥 and of𝑛classhypervectorsC 1𝑘,C 2𝑘,...,C𝑛𝑘 (𝑛=thenumberofclasses),
H𝑚𝑖𝑛. Forinstance,inFigure3,SensorIandSensorIIfollowa eachofwhichencodesthepatternofaclass.AnewsampleH in
timeseriesintrigram.SensorIhasthemaximumvalueat𝑡1and domain D𝑘 updatesmodelM𝑘 basedonitscosinesimilarities,
theminimumvalueat𝑡2,andthusweassignrandomlygenerated denotedasS 𝛿(H,·),withalltheclasshypervectorsinM𝑘,i.e.,
𝑦h a 𝑡s ′y 3sp i ige nr nv Se r eac nt n so d or ros m IH I.l𝑡 y W1 ga een tnd he eH r na𝑡 t a2 e st d so igh𝑦 ny𝑡1 p ha e yn r pvd ee𝑦 rc v𝑡 t2 eo, crr tse os H rp se 𝑡′ t2c ot ai 𝑦v n 𝑡e d 3ly iH n.S 𝑡 S′ 3i em t noi sl oa 𝑦r r𝑡′l 2y I, a aw n nd de 𝛿(H,C𝑡𝑘 )= ∥HH ∥· ·C ∥C𝑡𝑘
𝑡𝑘∥
= ∥HH
∥
· ∥C C𝑡 𝑡𝑘
𝑘∥
∝H ·Norm(C𝑡𝑘 ) (1)
valueat𝑦′ inSensorIIwithvectorquantization;mathematically,
𝑡1 where1 ≤𝑡 ≤𝑛.IfH hasthehighestcosinesimilaritywiththe
H𝑡3 =H𝑡2+𝑦 𝑦𝑡 𝑡3 1− −𝑦 𝑦𝑡 𝑡2
2
·(H𝑡1−H𝑡2) c dl oa mss aih ny -p spe erv cie fic cto mr oC d𝑖𝑘 elw Mhi 𝑘le ui pt ds at tr eu se al sabel C 𝑗𝑘 (1 ≤ 𝑖,𝑗 ≤ 𝑛),the
𝑦′ −𝑦′
H𝑡′ 1 =H𝑡′ 3+ 𝑦𝑡 𝑡′1 2−𝑦𝑡 𝑡′3
3
·(H𝑡′ 2−H𝑡′ 3). C 𝑗𝑘 ←C 𝑗𝑘 +𝜂·(cid:16) 1−𝛿(cid:0)H,C 𝑗𝑘(cid:1)(cid:17) ×H
(2)
Werepresentthetemporalsequenceofdatawiththepermutation C𝑘 ←C𝑘 −𝜂·(cid:16) 1−𝛿(cid:0)H,C𝑘(cid:1)(cid:17) ×H,
𝑖 𝑖 𝑖
operationinsection3.1.ForSensorIandSensorIIinFigure3,we
performrotationshift(𝜌)twicetoH𝑡1 andH 𝑡′ 1,oncetoH𝑡2 and where𝜂denotesalearningrate.Alarge𝛿(H,·)indicatestheinput
H
sa𝑡
m′ 2, pa lin nd gk we ie dp owH𝑡
b3
ya cn ad lcH
ul𝑡
a′
3
tit nh ge Hsam =e 𝜌.W 𝜌He 𝑡b 1in ∗d 𝜌Hda 𝑡t 2a ∗s Ham 𝑡3p ale ns din Ho ′n =e d anat da tp ho ein mt oi ds em la isrg ui pn da all ty edm bis ym aa dt dc ih ne gd ao vr ea rl yre sa mdy ale lx pi ost rs tii on nth oe ftm heod ee nl -,
𝜌𝜌H′ ∗𝜌H′ ∗H′.Finally,tospatiallyintegratedatafrommultiple codedvector(1−𝛿(H,·)≈0).Incontrast,asmall𝛿(H,·)indicates
senso𝑡1
rs,
we𝑡2 gene𝑡3
rate a random signature hypervector for each anoticeablynewpatternandupdatesthemodelwithalargefactor
sensorandbindinformationas(cid:205)𝑚 𝑖=1[G𝑖∗H𝑖],whereG𝑖denotesthe (1−𝛿(H,·)≈1).Ourlearningalgorithmprovidesahigherchance
signaturehypervectorforsensor𝑖,H𝑖isthehypervectorcontaining fornon-commonpatternstobeproperlyincludedinthemodel.
emiT
rosneS-itluM
gnidocnEseireS
rotceV
yreuQ
niamoD srotpircseD
ataD
hctaB
xam ?DOO )∗𝜸≤
𝜹(
𝐱𝐚𝐦
ledoM elbmesnE
gnildnuB
xam noitciderPDAC’24,June23–27,2024,SanFrancisco,CA JunyaoWang,MohammadAbdullahAlFaruque
3.5 Out-of-DistributionDetection Algorithm1DomainAdaptiveHDCInference
3.5.1 DomainDescriptors. Inparalleltodomain-specificmodeling, Input: AnencodedtestingsamplesQ,adomainclassifierwithKclass
asshowninFigure2,weconcurrentlyconstructdomaindescriptors hypervectorsU1,U2,...,U K,athresholdforOODdetection𝛿∗,K
(D) U = {U1,U2,...,U K} to encode the distinct pattern of domain-specificmodelsM1,M2,...,M K
eachdomain.Specifically,foreachdomainD𝑘 (1≤𝑘 ≤K),we Output: ApredictedlabelPforQ.
utilizethebundlingoperationtocombinethehS
ypervectorwithin
1: 𝛿𝑚𝑎𝑥 =max{𝛿(Q,U1),𝛿(Q,U2)...,𝛿(Q,U K)} ⊲OODdetection
t dh ee scd ro ipm toa rin U, 𝑘i.e .. M,{ aH th1𝑘 em,H at2𝑘 ic, a. l. l. y, ,H U𝑛𝑘
𝑘𝑘
=}, (cid:205)an
𝑛
𝑖d 𝑘c Ho 𝑖n 𝑘s .t Gru ivc et nan the ex pp rr oe pss ei rv tye 432 ::: eif ls𝛿 e𝑚 M𝑎 T𝑥 ←<𝛿 (cid:205)∗ 𝑖Kt =h 1e 𝛿n (Q,U𝑖)·M𝑖 ⊲Q ⊲i msc oo ⊲dn e Qs li ed ine ssr ne e od m tO b OlO i OnD g
D
ofthebundlingoperation(explainedinsection3.1),U𝑘 iscosine- 5: forall𝛿(Q,U𝑖) ≥𝛿∗do ⊲1≤𝑖 ≤𝑘
similartoallthesamplesH 𝑖𝑘 (1≤𝑖 ≤𝑛 𝑘)withindomainD𝑘
S
as 6: MT ←(cid:205) 𝑖K =1𝛿(Q,U𝑖)·M𝑖 ⊲partialmodelensembling
t sh amey pc leo sn tt hri ab tu at re et no ot th pe ab rtun ofdl ti hn eg bp ur no dc le es ,s i, .ea .n ,nd od ti is nsi dm oi mla ar it no Dal 𝑘lt .he 87 :: rP et← urnar Pgmax C𝑖T{𝛿(Q,C 1T),𝛿(Q,C 2T)...,𝛿(𝑄,C𝑛T)}
S
3.5.2 Out-of-DistributionDetection. AsshowninFigure2,akey
Table1:DetailedBreakdownsofDatasets
componentofourinferencephaseisthedetectionofOODsamples
(N:numberofdatasamples)
(E).Westartwithmappingthetestingsampletohyperdimensional
DSADS[19] USC-HAD[13] PAMAP2[20]
spacewiththesameencodingtechniqueasthetrainingphaseto
Domains N Domains N Domains N
obtainaqueryvectorQ(A).Then,asdetailedinAlgorithm1,we
Domain1 2,280 Domain1 8,945 Domain1 5,636
calculatethecosinesimilarityscoreofthequeryvectorQtoeachdo-
Domain2 2,280 Domain2 8,754 Domain2 5,591
maindescriptorU1,U2,...,U
K
(line1).Atestingsampleisidenti- Domain3 2,280 Domain3 8,534 Domain3 5,806
fiedasOODifitspatternissubstantiallydifferentfromallthesource Domain4 2,280 Domain4 8,867 Domain4 5,660
Domain5 8,274
domains.Therefore,whenthecosinesimilaritybetweenQandits
mostsimilardomain,i.e.,max(cid:8)𝛿(Q,U1),𝛿(Q,U2),...,𝛿(Q,U K)(cid:9), Total 9,120 Total 43,374 Total 22,693
issmallerthanathreshold𝛿∗,weconsiderQasanOODsample
(line2).Here𝛿∗isatunableparameterandweanalyzetheimpactof (line7).Notethat,unliketheinferenceforOODsampleswherewe
𝛿∗insection4.2.1.Wethendynamicallyconstructtest-timemodels ensembleallthedomain-specificmodelstoenhanceperformance,
(F)basedonwhetherthesampleisOOD(section3.6). thetest-timemodelM T foranin-distributionsampledoesnot
considerdomain-specificmodelsofthedomainswhereQ show
aminorsimilarityscorelowerthan𝛿∗.Inparticular,ifatesting
3.6 AdaptiveTest-TimeModeling
sampleQdoesnotshowhighsimilaritytoanyofthesourcedo-
3.6.1 InferenceforOODSamples. Foreachtestingsampleidenti-
mains,weincludeinformationfromallthedomainstoconstructa
fiedasOOD,wedynamicallyadaptdomain-specificmodelsM =
sufficientlycomprehensivetest-timemodeltomitigatethenegative
{M1,M2,...,M𝑘}tocustomizeatest-timemodelM
T
thatbest
impactsofdistributionshift.Incontrast,whenQexhibitsconsider-
fitsitsdomaincontextandtherebyprovidesanaccurateprediction.
ablesimilaritytocertaindomains,addinginformationfromother
AsdetailedinAlgorithm1,foranOODsampleQ,weensemble
domainsiscomparabletointroducingnoisesandcanpotentially
eachdomain-specificmodelbasedonhowsimilarQistothedo-
misleadtheclassificationanddegrademodelperformance.
main(line3).Specifically,let𝛿(Q,U1),𝛿(Q,U2),...,𝛿(Q,U K)
denotethecosinesimilaritybetweenQandeachdomaindescriptor
4 EXPERIMENTALEVALUATIONS
U1,U2,...,U K,weconstructthetest-timemodelM
T
forQas
4.1 ExperimentalSetup
M
T
=𝛿(Q,U1)·M1+𝛿(Q,U2)·M2+...+𝛿(Q,U K)·M K. (3)
WeevaluateSMOREonwidely-usedmulti-sensortimeseriesdatasets
Specifically,M T isofthesameshapeasM1,M2,...,M𝑘;itcon- DSADS[19],USC-HAD[13],PAMAP2[20].Domainsaredefined
sistsof𝑛classhypervectors(𝑛 =numberofclasses),denotedas bysubjectgroupingchosenbasedonsubjectIDfromlowtohigh.
C 1T,C 2T,...,C𝑛T,formulatedwithexplicitconsiderationofthedo- Thedatasizeofeachdomainineachdatasetisdemonstratedin
maincontextofQ.Wethencomputethecosinesimilaritybetween TABLE1.WecompareSMOREwith(i)twoSOTACNN-basedDA
Qandeachoftheseclasshypervectors,andassignQtotheclass algorithms:TENT[4]andmultisourcedomainadversarialnetworks
towhichitachievesthehighestsimilarityscore(line7). (MDANs)[5],and(ii)twoHDCalgorithms:theSOTAHDCnot
3.6.2 InferenceforIn-DistributionSamples. Wepredictthelabelof consideringdistributionshifts[21](BaselineHD)andDOMINO[8],
anin-distributiontestingsample,i.e.,anon-OODsample,leverag- arecentlyproposedHDC-baseddomaingeneralizationframework.
ingdomain-specificmodelsofthedomainstowhichthesampleis TheCNN-basedDAalgorithmsaretrainedwithTensorFlow,and
highlysimilar.AsdemonstratedinAlgorithm1,forallthedomains weapplythecommonpracticeofgridsearchtoidentifythebest
D𝑖 (1 ≤ 𝑖 ≤ K)wheretheencodedqueryvectorQ achievesa hyper-parametersforeachmodel.SinceDOMINOinvolvesdimen-
S
similarityscorehigherthan𝛿∗,weensembletheircorresponding sionregenerationineverytrainingiteration,forfairness,weinitiate
domain-specificmodelsincorporatingaweightoftheircosinesimi- itwithdimension𝑑∗=1𝑘andmakeitstotaldimensionalities,i.e.,
larityscorewithQtoformulatethetest-timemodelM (line5-6). the sum of its initial dimension and all the regenerated dimen-
T
Similartosection3.6.1,M Tconsistsof𝑛classhypervectors,andQ sionsthroughoutretraining,thesameasSMOREandBaselineHD
isassignedtotheclasswhereitobtainedthehighestsimilarityscore (𝑑 =8𝑘).Ourevaluationsincludeleave-one-domain-out(LODO)SMORE:Similarity-basedHyperdimensionalDomainAdaptationforMulti-SensorTimeSeriesClassification DAC’24,June23–27,2024,SanFrancisco,CA
TENT MDANs BaselineHD DOMINO SMORE (Our Work) 85
90
USC-HAD
80
75
75
60
45 70
Domain 1 Domain 2 Domain 3 Domain 4 Domain 5 Average 00.4.5 00..56 00..67 00..78 00..89
𝜹∗
95
DSADS Figure5:Imapctof𝛿∗onModelPerformance
80
4.2 Accuracy
TheaccuracyoftheLODOclassificationisdemonstratedinFigure
65
4.TheaccuracyofDomain𝑘indicatesthatthemodelistrainedwith
datafromalltheotherdomainsandevaluatedwithdatafromDo-
50
Domain 1 Domain 2 Domain 3 Domain 4 Average main𝑘.Thisaccuracyscoreservesasanindicatorofthemodel’sdo-
95 mainadaptationcapabilitywhenconfrontedwithunseendatafrom
PAMAP2
unseendistributions.SMOREachievescomparableperformanceto
80 TENTandonaverage1.98%higheraccuracythanMDANs.Addi-
tionally,SMOREprovides20.25%higheraccuracythanBaselineHD,
65 demonstratingthatSMOREsuccessfullyadaptstrainedmodelsto
fitsamplesfromdiversedomains.Additionally,SMOREprovides
50 onaverage4.56%higheraccuracythanDOMINO,indicatingDA
Domain 1 Domain 2 Domain 3 Domain 4 Domain 5 Average
canmoreeffectivelyaddressthedomaindistribution(DS)challenge
Figure4:ComparingLODOAccuracyofSMOREandCNN- innoisymulti-sensortimeseriesdatacomparedtoDG.
basedDomainAdaptationAlgorithms
4.2.1 ImpactofHyperparameter𝛿∗. Theimpactof𝛿∗ onclassi-
performance,learningefficiencyonbothserverCPUandresource-
fication results is demonstrated in Figure 5, where we evaluate
constraineddevices,andscalabilityusingdifferentsizesofdata.
SMOREonthedatasetUSC-HADasanexample.SMOREachieved
itsbestperformancewhen𝛿∗isaround0.65.Smallvaluesof𝛿∗are
4.1.1 Platforms. ToevaluatetheperformanceofSMOREonboth
morelikelytodetectin-distributionsamplesasOODsothatthe
high-performancecomputingenvironmentsandresource-limited
test-timemodelswouldincludenoisesfromthedomainswhere
edgedevices,weincluderesultsfromthefollowingplatforms:
thesampleonlyhasaminimalsimilarityscore,therebycausing
• ServerCPU:IntelXeonSilver4310CPU(12-core,24-thread,
notableperformancedegradation.Ontheotherhand,largeval-
2.10GHz),96GBDDR4memory,Ubuntu20.04,Python3.8.10,
uesof𝛿∗aremorelikelytoconsiderOODsamplesin-distribution.
PyTorch1.12.1,TDP120W.
Consequently,thetest-timemodelonlyincludesdomain-specific
• Embedded CPU: Raspberry Pi 3 Model 3+ (quad-core ARM modelsofthedomainsthatexhibitlimitedsimilaritytothegiven
A53@1.4GHz),1GBLPDDR2memory,Debian11,Python3.9.2,
sample;therefore,theensembledtest-timemodelisunlikelytobe
PyTorch1.13.1,TDP5W.
sufficientlycomprehensivetoprovideaccuratepredictions.
• EmbeddedGPU:JetsonNano(quad-coreARMA57@1.43GHz,
128-coreMaxwellGPU),4GBLPDDR4memory,Python3.8.10,
4.3 Efficiency
PyTorch1.13.0,CUDA10.2,TDP10W.
4.3.1 EfficiencyonServerCPU. Foreachdataset,eachdomaincon-
4.1.2 DataPreprocessing. Wedescribethedataprocessingforeach sistsofroughlysimilaramountsofdataasdetailedinTABLE1;
datasetprimarilyondatasegmentationanddomainlabeling. thus,weshowtheaverageruntimeoftrainingandinferenceforall
• DSADS[19]:TheDailyandSportsActivitiesDataset(DSADS) thedomains.AsdemonstratedinFigure6(a),SMOREexhibitson
includes19activitiesperformedby8subjects.Eachdatasegment average11.64×fastertrainingthanTENTand18.81×fastertraining
isanon-overlappingfive-secondwindowsampledat25Hz.Four thanMDANs.Additionally,SMOREdelivers4.07×fasterinference
domainsareformedwithtwosubjectseach. thanTENTand4.63×fasterinferencethanMDANs.Suchnotably
• USC-HAD[13]:TheUSChumanactivitydataset(USC-HAD) higherlearningefficiencyisthankstothehighlyparallelmatrix
includes12activitiesperformedby14subjects.Eachdatasegment operationsonhyperdimensionalspace.Additionally,SMOREpro-
isa1.26-secondwindowsampledat100Hzwith50%overlap.Five videsonaverage5.84×fastertrainingcomparedtoDOMINO.In
domainsareformedwiththreesubjectseach. particular,DOMINOachievesdomaingeneralizationbyiteratively
• PAMAP2 [20]: The Physical Activity Monitoring (PAMAP2) identifyingandregeneratingdomain-variantdimensionsandthus
datasetincludes18activitiesperformedby9subjects.Eachdata requiresnotablymoreretrainingiterationstoprovidereasonable
segmentisa1.27-secondwindowsampledat100Hzwith50% performance.Ontheotherhand,duringeachretrainingiteration,
overlap.Fourdomains,excludingsubjectnine,areformedwith DOMINOonlykeepsdimensionsplayingthemostpositiverolein
twosubjectseach. theclassificationtask,andthusiteventuallyarrivesatamodelwith
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccA
)%(
ycaruccADAC’24,June23–27,2024,SanFrancisco,CA JunyaoWang,MohammadAbdullahAlFaruque
TENT MDANs BaselineHD DOMINO SMORE (Our Work) increasingthetrainingdatasize,SMOREmaintainshighefficiency
100𝟏00𝟎𝟒 12 inbothtrainingandinferencewithasub-lineargrowthinexecution
time.Incontrast,thetrainingandinferencetimeofCNN-based
8 DAalgorithmsincreasesconsiderablyfasterthanSMORE.This
1𝟏00𝟎𝟐 positionsSMOREasanefficientandscalableDAsolutionforboth
4 high-performanceandresource-constrainedcomputingplatforms.
𝟏1𝟎𝟎 0
5 CONCLUSIONS
DASAD USCHADPAMAP2 DASAD USCHADPAMAP2
(a)EfficiencyofSMOREandCNN-based Algorithms on Server CPU WeproposeSMORE,anoveldomainadaptiveHDClearningframe-
workthatdynamicallycustomizestest-timemodelswithexplicit
10𝟏0𝟎0𝟑 𝟏𝟎𝟒
considerationofthedomaincontextofeachsampleandtherebypro-
1𝟏00𝟎𝟐 videsaccuratepredictions.OurSMOREoutperformsSOTACNN-
basedDAalgorithmsintermsofbothaccuracyandtrainingand
𝟏𝟎𝟐
𝟏1𝟎0𝟏 inferenceefficiency.SMOREalsoexhibitsnotablylowerinference
latencyandpowerconsumptiononedgeplatforms,makingita
scalableandefficientsolutiontoaddresstheDSchallenge.
𝟏1𝟎𝟎
𝟏𝟎𝟎
Raspberry Pi Jetson Nano Raspberry Pi Jetson Nano
(b)Efficiency ofSMOREandCNN-based Algorithms on Edge Platforms 6 ACKNOWLEDGEMENT
ThisworkwaspartiallysupportedbytheNationalScienceFounda-
Figure6:EfficiencyofSMOREandCNN-basedDAAlgorithms
tion(NSF)underawardCCF-2140154.
TENT MDANs SMORE (Our Work)
100𝟏0𝟎0𝟒 9
REFERENCES
[1] HuihuiQiaoetal.Atime-distributedspatiotemporalfeaturelearningmethod
6 formachinehealthmonitoringwithmulti-sensortimeseries.Sensors,2018.
1𝟏0𝟎0𝟐 [2] IshaanGulrajanietal.Insearchoflostdomaingeneralization.InInternational
ConferenceonLearningRepresentations,2021.
3 [3] SamuelWilsonetal.Hyperdimensionalfeaturefusionforout-of-distribution
detection.InProceedingsoftheIEEE/CVFWinterConferenceonApplicationsof
𝟏𝟎1𝟎 0
[4]
C Do em qup au nte WrV ai nsi gon et,2 a0 l.23 T.
ent:Fullytest-timeadaptationbyentropyminimization.
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
InInternationalConferenceonLearningRepresentations,2020.
Percentage of Training Data Percentage of Inference Data [5] HanZhaoetal.Multiplesourcedomainadaptationwithadversariallearning.In
Figure7:ComparingScalabilityUsingDifferentSizeofData
6thInternationalConferenceonLearningRepresentations,ICLR2018,2018.
[6] XinQinetal.Generalizablelow-resourceactivityrecognitionwithdiverseand
discriminativerepresentationlearning.InProceedingsofthe29thACMSIGKDD
compresseddimensionalityandprovidesaslightlyhigherinference ConferenceonKnowledgeDiscoveryandDataMining,page1943–1953,2023.
efficiency.Furthermore,comparedtoBaselineHD,SMOREprovides [7] ShioriSagawaetal.Distributionallyrobustneuralnetworks.InInternational
ConferenceonLearningRepresentations,2019.
significantlyhigheraccuracy(demonstratedinFig.4)withoutsub- [8] JunyaoWangetal.Domino:Domain-invarianthyperdimensionalclassification
stantiallyincreasingboththetrainingandinferenceruntimes. formulti-sensortimeseriesdata.In2023IEEE/ACMInternationalConferenceon
ComputerAidedDesign(ICCAD),pages1–9.IEEE,2023.
4.3.2 EfficiencyonEmbeddedPlatforms. Tofurtherunderstand [9] YaoQinetal.Adual-stageattention-basedrecurrentneuralnetworkfortime
theperformanceofSMOREonresource-constrainedcomputing s Ae rr ti ie fis cip ar le Id ni tc et lli io gn e. ncI en ,2P 0r 1o 7ce .edingsofthe26thInternationalJointConferenceon
platforms,weevaluatetheefficiencyofSMORE,TENT,MDANs, [10] SeppHochreiteretal.Longshort-termmemory.Neuralcomputation,1997.
andBaselineHDusingaRaspberryPi3ModelB+boardandan [11] JunyaoWangetal. Disthd:Alearner-awaredynamicencodingmethodfor
hyperdimensionalclassification.arXivpreprintarXiv:2304.05503,2023.
NVIDIAJetsonNanoboard.Bothplatformshaveverylimitedmem- [12] JunyaoWangetal. Hyperdetect:Areal-timehyperdimensionalsolutionfor
oryandCPUcores(andGPUcoresforJetsonNano).Figure6(b) intrusiondetectioniniotnetworks.IEEEInternetofThingsJournal,2023.
[13] MiZhangetal.Usc-had:Adailyactivitydatasetforubiquitousactivityrecog-
demonstratestheaverageinferencelatencyforeachalgorithmpro-
nitionusingwearablesensors. InProceedingsofthe2012ACMconferenceon
cessing each domain in the PAMAP2 dataset. On Raspberry Pi, ubiquitouscomputing,2012.
SMOREprovidesonaverage14.82×speedupscomparedtoTENT [14] QiDouetal.Domaingeneralizationviamodel-agnosticlearningofsemantic
features.AdvancesinNeuralInformationProcessingSystems,2019.
and19.29×speedupscomparedtoMDANsininference.OnJet- [15] YaroslavGaninetal.Domain-adversarialtrainingofneuralnetworks.Thejournal
son Nano, SMORE delivers 13.22× faster inference than TENT ofmachinelearningresearch,2016.
[16] AbbasRahimietal.Hyperdimensionalbiosignalprocessing:Acasestudyfor
and17.59×fasterinferencethanMDANs. Additionally,SMORE
emg-basedhandgesturerecognition.InICRC.IEEE,2016.
exhibitssignificantlylessenergyconsumption,providingamore [17] AliMoinetal.Awearablebiosensingsystemwithin-sensoradaptivemachine
resource-efficientdomainadaptationsolutionforthedistribution learningforhandgesturerecognition.NatureElectronics,2021.
[18] JunyaoWangetal. Robustandscalablehyperdimensionalcomputingwith
shiftchallengeonedgedevices.
brain-likeneuraladaptations.arXivpreprintarXiv:2311.07705,2023.
[19] BillurBarshanetal.Recognizingdailyandsportsactivitiesintwoopensource
4.4 Scalability machinelearningenvironmentsusingbody-wornsensorunits.TheComputer
Journal,2014.
WeevaluatethescalabilityofSMOREandSOTACNN-basedDA [20] AttilaReissetal.Introducinganewbenchmarkeddatasetforactivitymonitoring.
algorithmsusingvarioustrainingdatasizes(percentagesofthefull In16thinternationalsymposiumonwearablecomputers.IEEE,2012.
[21] AlejandroHernández-Canoetal. Onlinehd:Robust,efficient,andsingle-pass
dataset)ofthePAMAP2dataset.AsdemonstratedinFigure7,with onlinelearningusinghyperdimensionalsystem.InDATE.IEEE,2021.
)s(
emiT
gniniarT
)s(
ycnetaL
ecnerefnI
)s(
emiT
gniniarT
ygrenE
)s(
ycnetaL
ecnerefnI
)J(
noitpmusnoC
)s(
emiT
ecnerefnI