Order-Optimal Regret in Distributed Kernel Bandits using
Uniform Sampling with Shared Randomness
Nikola Pavlovic*, Sudeep Salgia‡, and Qing Zhao*
*School of Electrical & Computer Engineering, Cornell University, Ithaca, NY,
{np358, qz16}@cornell.edu
‡Carnegie Mellon University, Pittsburgh, PA, ssalgia@andrew.cmu.edu
Feb 2024
Abstract
We consider distributed kernel bandits where N agents aim to collaboratively
maximize an unknown reward function that lies in a reproducing kernel Hilbert space.
Each agent sequentially queries the function to obtain noisy observations at the query
points. Agents can share information through a central server, with the objective
of minimizing regret that is accumulating over time T and aggregating over agents.
We develop the first algorithm that achieves the optimal regret order (as defined by
centralizedlearning)withacommunicationcostthatissublinearinbothN andT. The
key features of the proposed algorithm are the uniform exploration at the local agents
and shared randomness with the central server. Working together with the sparse
approximation of the GP model, these two key components make it possible to preserve
the learning rate of the centralized setting at a diminishing rate of communication.
1 Introduction
1.1 Distributed Kernel Bandits
We study the problem of zeroth-order online stochastic optimization in a distributed setting,
where N agents aim to collaboratively maximize a reward function with communications
facilitated by a central server. The reward function f : X → R is unknown; it is only known
thatitlivesinaReproducingKernelHilbertSpace(RKHS)associatedwithaknownkernelk.
Each agent sequentially chooses points in the function domain X to query and subsequently
receives noisy feedback on the function values (i.e., random rewards) at the query points.
The goal is for each distributed agent to converge quickly to x∗ ∈ argmax f(x), a global
x∈X
1
4202
beF
02
]GL.sc[
1v28131.2042:viXramaximizer of f. We quantify this goal as minimizing the cumulative regret summed over a
learning horizon of length T and over all N agents:
N T
R =
(cid:88)(cid:88)(cid:16)
f(x∗)−f(x(n)
)(cid:17)
, (1)
t
n=1t=1
(n)
where x denotes the point queried by agent n at time t.
t
The above zeroth-order stochastic optimization problem can be viewed as a continuum-
armed kernelized-bandit problem (Srinivas et al., 2010). The expressive power of the RKHS
model represents a broad family of objective functions. In particular, it is known that the
RKHS of typical kernels, such as the Matérn family of kernels, can approximate almost all
continuous functions on compact subsets of Rd (Srinivas et al., 2010). The problem has been
studied extensively under a centralized setting with a single decision maker (i.e., N = 1),
for which several algorithms have been proposed, including UCB-based algorithms (Srinivas
et al., 2010; Chowdhury and Gopalan, 2017; Abbasi-Yadkori et al., 2011), batched pure
exploration (Li and Scarlett, 2022), tree-based domain shrinking (Salgia et al., 2021) and
RIPS (Camilleri et al., 2021). Optimal learning efficiency in terms of regret order in T has
been obtained in both the stochastic (Li and Scarlett, 2022; Salgia et al., 2021) and the
contextual setting (Valko et al., 2013).
In addition to learning efficiency, distributed kernel bandits face a new challenge of com-
munication efficiency. Without constraints on the communication overhead, all agents can
share their local observations and coordinate their individual query actions at no cost. The
distributedproblemcanbetriviallyreducedtoacentralizedone. Attheotherendofthespec-
trum is a complete decoupling of the agents, resulting in N independent single-user problems
without the benefit of data sharing for accelerated learning. The tension between learning ef-
ficiency (which demands data sharing and action coordination) and communication efficiency
is evident. A central question to this trade-off is how to achieve the optimal learning rate en-
joyedbythecentralizedsettingusingaminimumamountofmessageexchangeamongagents.
In contrast to the extensive literature on centralized kernel bandits, distributed kernel
bandits are much less explored despite their broad applications (e.g., federated learning for
hyperparameter tuning (Dai et al., 2020) and collaborative training of neural nets using the
recent theory of Neural Tangent Kernel (Jacot et al., 2018)). There exist only a handful
of studies under drastically different settings and constraints (see Sec.1.3). For the setting
considered in this work, no distributed learning algorithms exist that achieve the optimal
regret order with a sublinear (in both T and N) message exchange among agents.
21.2 Main Results
In this paper, we develop the first algorithm for distributed kernel bandits that achieves the
optimal order of regret enjoyed by centralized learning with a sublinear message exchange
in both T and N.
To tackle the essential tradeoff between learning rate and communication efficiency, a
distributed learning algorithm needs a communication strategy that governs what to com-
municate and how to integrate the shared information into local query actions. To minimize
the total regret that is accumulating over time and aggregating over the agents, the commu-
nication strategy needs to work in tandem with the query actions to ensure a continual flow
of information available at all agents for decision-making.
A natural answer to what to communicate in a distributed learning problem is certain
sufficient local statistics of the underlying unknown parameters. For example, for multi-
armed (i.e., discrete arms) and linear bandits, this corresponds to the local estimates of the
arm mean values and the mean reward vector respectively. However, for kernel bandits,
the corresponding quantity would be an estimate of the function, which is potentially
infinite-dimensional and hence an impractical choice for communication. Existing studies
resolve this issue by exchanging local query actions and observations across all agents and
throughout the learning horizon (Li et al., 2022; Dubey and Pentland, 2020), resulting in a
communication cost growing linearly in both N and T.
Even with a communication cost growing linearly in both N and T, preserving the full
learning power of a centralized decision maker with NT query points is not immediate. The
prevailing approaches to centralized kernel bandits that achieve order optimal regret build
on the maximum posterior variance (MPV) sampling strategy (Li and Scarlett, 2022) which
queries, at each time, the point with the highest posterior variance conditioned on all past
observations. Ensuring such a maximal uncertainty reduction at each query point is believed
to be crucial in utilizing the full statistical power of all query points. Unfortunately, such a
fully adaptive query strategy is incompatible with the parallel learning among distributed
agents. To emulate the MPV sampling at each of the NT query points would require the
agents to take turns in their queries and share the local observations immediately with all
other agents, an infeasible strategy for most distributed learning problems. Implementing
MPV-based sampling in parallel across agents, however, loses the full adaptivity. This is
arguably the main obstacle in realizing the optimal learning rate of a centralized kernel
bandit in a distributed setting.
To tackle the above challenges, our proposed algorithm represents major departures from
the prevailing approaches. Referred to as DUETS (Distributed Uniform Exploration of
Trimmed Sets), this algorithm has two key features: uniform exploration at the local agents
3and shared randomness with the central server.
In DUETS , each agent employs uniform (at random) sampling as the query strategy. Uni-
form sampling is fully compatible with parallel learning. In particular, note that the union
of the local sets of size t query points obtained at the agents through uniform sampling is
identical (in distribution) to the set of size Nt query points obtained at a centralized decision
maker using the same uniform sampling strategy. This superposition property of uniform
sampling allows us to leverage the recent results on random exploration in centralized kernel
bandits (Salgia et al., 2023a), and is crucial in achieving the optimal learning rate defined by
the centralized setting. In addition to preserving the learning rate of the centralized setting,
uniform sampling enjoys advantages in computation as well as communication aspects.
Comparing with the MPV strategy that requires an expensive maximization of a non-convex
acquisition function for finding each query point, uniform sampling is extremely simple to
implement. This computational efficiency can be particularly attractive to distributed local
devices. In terms of communication efficiency, uniform sampling makes it possible to bypass
the exchange of query points altogether and reduce the exchange of reward observations
through the shared randomness strategy detailed below.
In DUETS, each agent has access to an independent coin, i.e., a source of randomness,
which is unknown to the other agents but is known to the server. The shared randomness
enables the server to reproduce the points queried by the agents, thereby resulting in
effective transmission of the local set of queried points at each agent to the server at no
communication cost. To reduce the communication overhead associated with the reward
observations, we employ sparse approximation of GP models (Wild et al., 2021). The
availability of all the queried points at the server provides the perfect platform for leveraging
the power of sparse approximation to reduce the communication to a diminishing fraction of
the total number of observations. Specifically, the server, with access to all the query points,
selects a small subset of points that can approximate, to sufficient accuracy, the posterior
statistics corresponding to all the points queried by the agents. This allows a diminishing
rate of communication to share local reward observations. It is this integration of uniform
sampling, shared randomness, and sparse approximation in DUETS that makes it possible
to achieve the optimal learning rate of the centralized setting at a communication cost that
is sublinear in both N and T.
We analyze the performance of DUETS and establish that it incurs a cumulative regret of
√
O(cid:101)( NTγ
NT
log(T/δ))1 with probability 1−δ, where γ
NT
denotes the maximal information
gain of the kernel and represents the effective dimension of the kernel. Note that this
matches the lower bound (up to logarithmic factors) for any centralized algorithm with
a total of NT queries as established in Scarlett et al. (2017), thereby establishing the
1The notation O˜(·) hides poly-logarithmic factors.
4order-optimality of the proposed algorithm. To the best knowledge of the authors, this is
the first algorithm to achieve the optimal order of regret for the problem of distributed
kernel bandits. We also establish a bound of O˜(γ ) on the communication cost incurred by
NT
DUETS ,wherecommunicationcostismeasuredbythenumberofrealnumberstransmitted
during the algorithm (See Section 2 for more details). This significantly improves over the
state-of-the-art of O(Nγ3 ) achieved by ApproxDisKernelUCB algorithm proposed by Li
NT
et al. (2022) and is always guaranteed to sublinear in the total number of queries, NT.
1.3 Related Work
The existing literature on distributed kernel bandits is relatively slim. The most relevant to
our work is that by Li et al. (2022), where the authors consider the problem of distributed
contextual kernel bandits and propose a UCB based policy with sparse approximation of
GP models and intermittent communication. Their proposed policy was shown to incur
√
a cumulative regret of O(cid:101)( NTγ NT) and communication cost of O(Nγ N3 T). The DUETS
algorithm proposed in this work, offers an improvement over the algorithm in Li et al. (2022)
both in terms of regret and communication cost. While the contextual setting with varying
arm action sets considered in their work is more general that the setting with a fixed arm
set considered in this work, their proposed algorithm does not offer non-trivial reduction
in regret or communication cost in the fixed arm setting. Moreover, both the regret and
communication cost incurred by the algorithm in Li et al. (2022) are not guaranteed to be
sublinear in the total number of queries, NT, for all kernels. Consequently, their algorithm
does not guarantee convergence to x∗ or a non-trivial communication cost for all kernels.
On the other hand, both regret and communication cost of DUETS is guaranteed to be
sub-linear implying both convergence and communication efficiency.
Among other studies, Du et al. (2023) consider the problem of distributed pure exploration
in kernel bandits over finite action set, where they focus on designing learning strategies with
low simple regret. In this work, we consider the more challenging continuum-armed setup
with a focus on minimizing cumulative regret as opposed to simple regret. Another line of
work explores impact of heterogeneity among clients and design algorithms to minimize
this impact. Salgia et al. (2023b) consider personalized kernel bandits in which agents
have heterogeneous models and aim to optimize the weighted sum of their own reward
function and the average reward function over all the agents. Dubey and Pentland (2020)
consider heterogeneous distributed kernel bandits over a graph in which they use additional
kernel-based modeling to measure task similarity across different agents.
In contrast to the distributed kernel bandit, the problems of distributed multi-armed
bandits and linear bandits have been extensively studied. For distributed multi-armed
bandits (MAB), a variety of algorithms have been proposed for distributed learning under
different network topologies (Landgren et al., 2017; Shahrampour et al., 2017; Sankararaman
5et al., 2019; Chawla et al., 2020; Zhu et al., 2021). Shi et al. (2021) and Shi and Shen
(2021) have analyzed the impact of heterogeneity among agents in the distributed MAB
problem. Similarly, the problem of distributed linear bandits is also well-understood in
variety of settings with different network topologies (Korda et al., 2016), heterogeneity
among agents (Mitra et al., 2021; Ghosh et al., 2021; Hanna et al., 2022) and communication
constraints (Mitra et al., 2022; Wang et al., 2019; Huang et al., 2021; Amani et al., 2022;
Salgia and Zhao, 2023).
2 Problem Formulation
WeconsideradistributedlearningframeworkconsistingofN agentsindexedby{1,2,...,N}.
Under this framework, we study the problem of collaboratively maximizing an unknown
function f : X → R, where X ⊂ Rd is a compact, convex set. The function f belongs to the
Reproducing Kernel Hilbert Space (RKHS), H , associated with a known positive definite
k
kernel k : X ×X → R. The RKHS, H , is a Hilbert space that is endowed by with an
k
inner product ⟨·,·⟩ that obeys the reproducing property, i.e., ⟨g,k(x,·)⟩ = g(x) for all
H H
k k
g ∈ H , and induces the norm ∥g∥ = ⟨g,g⟩ .
k H k H k
The agents can access the unknown function by querying the function at different points
in the domain X. Upon querying a point x ∈ X, the agent receives a reward y = f(x)+ϵ,
where ϵ is a noise term. We make the following assumptions on the unknown function f
and noise.
Assumption 2.1. The RKHS norm of the function f is bounded by a known constant B,
i.e., ∥f∥ ≤ B.
H
k
Assumption 2.2. The noise term ϵ is assumed to be independent across all agents and all
queries and is a zero-mean, R sub-Gaussian random variable i.e., it satisfies the relation
E[exp(λϵ)] ≤ exp λ2R2 for all λ ∈ R.
2
Assumption 2.3. For each r ∈ N, there exists a discretization U of X with |U | = poly(r)2
r r
suchthat,foranyf ∈ H ,wehave|f(x)−f([x] )| ≤
∥f∥Hk,where[x]
= argmin ∥x−
k Ur r Ur x′∈Ur
x′∥ .
2
Assumption 2.4. Let L = {x ∈ X|f(x) ≥ η} denote the level set of f for η ∈ [−B,B].
η
We assume that for all η ∈ [−B,B], L is a disjoint union of at most M < ∞ components,
η f
each of which is closed and connected. Moreover, for each such component, there exists
a bi-Lipschitzian map between each such component and X with normalized Lipschitz
constant pair L ,L′ < ∞.
f f
2The notation g(x)=poly(x) is equivalent to g(x)=O(xk) for some k∈N.
6Assumptions 2.1-2.3 are standard, mild assumptions that are commonly adopted in the
literature (Srinivas et al., 2010; Chowdhury and Gopalan, 2017; Li and Scarlett, 2022; Vakili
et al., 2022, 2021a). The existence of the discretization U in Assumption 2.3 has been
r
justified and adopted in previous studies (Srinivas et al., 2010; Vakili et al., 2021a). In
particular, the popular class of kernels like Squared Exponential and Matérn kernels are
known to be Lipschitz continuous, in which case a ε-cover of the domain with ε = O(1/r) is
sufficient to show the existence of such a discretization. At a high level, Assumption 2.4
ensures that the structure of the levels sets of f satisfy a mild regularity condition. This is
a mild assumption on f that we require to adopt a result from Salgia et al. (2023a) for our
analysis.
The agents collaborate with each other by communicating through a central server. At
each time instant, each agent can send a message to the server through the uplink channel.
Based on the messages from different agents received by the server, it can then broadcast a
message back to all the agents through the downlink channel.
Our objective is to design a distributed learning policy π that specifies for each agent n,
(n)
the point x to be queried at each time instant t, based on the information available at
t
that agent upto time instant t. The performance of a collaborative learning policy π is
measured through its performance in terms of both learning and communication efficiency
over a learning horizon of T steps. The learning efficiency is measured using the notion of
cumulative regret, as defined in (1).
The communication efficiency is measured using the sum of the uplink and downlink
(n)
communication costs. In particular, let C (T) denote the number of real numbers sent by
up
the agent n to the server over the time horizon. The uplink cost of π, Cπ (T) is then given
up
as the average communication cost over all agents:
N
1 (cid:88)
Cπ (T) = C(n)(T). (2)
up N up
n=1
Similarly, the downlink cost of π, Cπ (T) is given as the number of real numbers broadcast
down
by the server over the entire time horizon averaged over all agents . The overall communica-
tion cost of π, Cπ(T), is given as Cπ(T) = Cπ (T)+Cπ (T).
up down
The objective is to design a distributed learning policy that achieves the order-optimal
cumulative regret and incurs a low communication cost. We aim to provide high probability
bounds on both the cumulative regret and communication cost that hold with probability
1−δ for any given δ ∈ (0,1).
7We overview the basis of Gaussian Process models and their sparse approximation, both of
which are central to our proposed policy.
2.1 GP Models
In this section we present a brief overview of Gaussian Process models and their application
on establishing confidence interval for RKHS elements.
A Gaussian Process (GP) is a random process G indexed by X and is associated with a
mean function µ : X → R and a positive definite kernel k : X ×X → R. The random
process G is defined such that for all finite subsets of X, {x ,x ,...,x } ⊂ X, m ∈ N, the
1 2 m
random vector [G(x ),G(x ),...,G(x )]⊤ follows a multivariate Gaussian distribution with
1 2 m
mean vector [µ(x ),...,µ(x )]]⊤ and covariance matrix Σ = [k(x ,x )]m . Throughout
1 n i j i,j=1
the work, we consider GPs with µ ≡ 0. When used as a prior for a data generating process
under Gaussian noise, the conjugate property provides closed form expressions for the
posterior mean and covariance of the GP model. Specifically, given a set of observations
{X ,Y } = {(x ,y )}m from the underlying process, the expression for posterior mean
m m i i i=1
and variance of GP model is given as follows:
µ (x) = k (x)⊤(λI +K )−1Y , (3)
m Xm m Xm,Xm m
σ2 (x) = (k(x,x)−k⊤ (x)(λI +K )−1k (x)). (4)
m Xm m Xm,Xm Xm
Intheaboveexpressions,k (x) = [k(x ,x),k(x ,x)...k(x ,x)]⊤,K = {k(x ,x )}m ,
Xm 1 2 n Xm,Xm i j i,j=1
I is the m×m identity matrix and λ corresponds to the variance of the Gaussian noise.
m
Following a standard approach in the literature (Srinivas et al., 2010), we model the data
corresponding to observations from the unknown f, which belongs to the RKHS of a positive
definite kernel k, using a GP with the same covariance kernel k. In particular, we assume
a fictitious GP prior over the fixed, unknown function f along with fictitious Gaussian
distribution for the noise. The benefit of this approach is that the posterior mean and
variance of this GP model serve as tools to both predict the values of the function f and
quantify the uncertainty of the prediction at unseen points in the domain, as shown by the
following lemma .
Lemma 2.5. Vakili et al. (2021a, Thm. 1) Assume that 2.1 and 2.2 hold. Given a set
of observations {X ,Y } as described above, such that the query points X are chosen
m m m
independent of the noise sequence, then for a fixed x ∈ X, the following relation holds with
probability at least 1−δ:
|h(x)−µ (x)| ≤ β(δ)·σ (x),
m m
(cid:113)
where β(δ) = B+R 2 log(cid:0)2(cid:1) .
λ δ
8We would like to emphasize that these assumptions are modeling techniques used as a part
of algorithm and not a part of the problem setup. In particular, the function f is fixed,
deterministic function in H and the noise is R-sub-Gaussian.
k
Lastly, given a set of points X = {x ,x ,...,x } ∈ X, the information gain of the set X
m 1 2 m m
is defined as γ := 1 log(det(I +λ−1K )). Using this, we can define the maximal
Xm 2 m Xm,Xm
information gain of a kernel as γ := sup γ . Maximal information gain is closely
m Xm Xm
relatedtotheeffectivedimensionofakernel(Calandrielloetal.,2019)andhelpscharacterize
the regret performance of kernel bandit algorithms (Srinivas et al., 2010; Chowdhury and
Gopalan, 2017). γ depends only the kernel and λ and has been shown to be an increasing
m
sublinear function of m (Srinivas et al., 2010; Vakili et al., 2021b).
2.2 Sparse approximation of GP models
The sparsification of GP models refers to the idea of approximating the posterior mean and
variance of a GP model, corresponding to a set of observations {X ,Y }, using a subset
m m
of query points X . In particular, let S be a subset of X consisting of r < m points.
m m
The approximate posterior mean and variance based on the points in S, referred to as the
inducing set, is given as follows(Wild et al., 2021).
(cid:16) (cid:17)−1
µ˜ (x) = z (x)⊤ λI +Z⊤ Z Z⊤ Y (5)
m S |S| Xm,S Xm,S Xm,S m
λσ˜2 (x) = (cid:2) k(x,x)−z⊤(x)Z⊤ Z
(cid:16)
λI +Z⊤ Z
(cid:17)−1
z (x)(cid:3) , (6)
m S Xm,S Xm,S |S| Xm,S Xm,S S
where z (x) =
K−1
2k (x) and Z = [z (x ),z (x ),...,z (x )]⊤.
S S,S S Xm,S S 1 S 2 S m
Note that it is sufficient to know the matrix Z⊤ Z ∈ Rr×r, vector Z⊤ Y ∈ Rr
Xm,S Xm,S Xm,S m
and the set S in order for µ˜ and σ˜ to be calculated.
3 The DUETS Algorithm
In this section, we present the proposed algorithm DUETS.
We first describe the randomization at each agent and the shared randomness with the
server. Each agent n has a private coin C for generating random bits that are independent
n
of those generated by other agents. Each agent’s coin is private to other agents, but known
to the central server. As a result, the server can reproduce the random bits generated at all
agents.
DUETS employs an epoch-based elimination structure where the domain X is successively
trimmed across epochs to maintain an active region that contains a global maximizer x∗
9with high probability for future exploration. Specifically, in each epoch j, the server and
the agents maintain a common active subset of the domain X ⊆ X with X initialized to
j 1
X. The operations in each epoch are as follows.
During the jth epoch, each agent n, using its private coin C , generates D(n) , a set of T
n j j
pointsthatareuniformlydistributedinthesetX 3. T issetto⌊(cid:112) TT ⌋, withT beingan
j j j−1 1
input to the algorithm. Each agent n queries all the points in D(n) and obtains Y(n) ∈ RTj,
j j
the corresponding vector of reward observations.
Since the server has access to the coins of all the agents, it can faithfully reproduce the set
D =
(cid:83)N D(n)
without any communication between the server and the agents. In order to
j n=1 j
efficientlycommunicatetheobservedrewardvaluesfromtheagentstotheserver, weleverage
sparse approximation of GP models along with the knowledge of the set D at the server.
j
The server constructs a global inducing set S by including each point in D with probability
j j
p := p σ2 , independent of other points where σ2 = sup σ2(x) and σ2(·) is the
j 0 j,max j,max x∈Xj j j
posterior variance corresponding to points collected in D . Here, p =
72log(cid:0)4NT(cid:1)
is
j 0 δ′
an appropriately chosen universal constant which ensures that the approximate posterior
statistics constructed using S are a faithful representation of the true posterior statistics
j
corresponding to the set D with probability 1−δ. The server broadcasts the inducing set
j
S to all the agents.
j
Upon receiving the inducing set, each agent n computes the projection v(n) ∈ R|Sj| of its
j
reward vector onto the inducing set as follows:
v(n) := Z⊤ Y(n) . (7)
j D j(n),Sj j
(n)
Each agent then sends back the lower-dimensional projected observations v to the server,
j
which subsequently aggregates them to obtain the vector v given as
j
(cid:32) N (cid:33)
v := (cid:16) λI +Z⊤ Z (cid:17)−1 (cid:88) v(n) . (8)
j |Sj| Dj,Sj Dj,Sj j
n=1
Note that the summation (cid:80)N v(n) equals to Z⊤ Y , i.e., projection of the rewards of
n=1 j Dj,Sj j
all agents onto the inducing set. The server then broadcasts the vector v and σ to all
j j,max
the agents. The benefit of sending v as opposed to the sum of rewards is that it allows the
j
agents to compute the posterior mean at the agents using their knowledge of the inducing
set S (See. Eqn (5)).
j
3If the active region consists of multiple disjoint regions, then we carry out this step for each region
separately. Forsimplicityofdescription,weassumetheactiveregionconsistsofasingleconnectedcomponent.
10As the last step of the epoch, all the agents and the server trim the current set X to X
j j+1
using the following update rule:
(cid:40) (cid:41)
X = x ∈ X : µ˜ (x) ≥ sup µ˜
(x′)−2β(δ′
)σ , (9)
j+1 j j j j,max
x′∈Xj
where δ′ = δ and µ˜ (x) = z⊤(x)v is the approximate posterior mean
2|UT|·(log(logNlogT))+4) j Sj j
computed based on the inducing set S (See Eqn. (5)). Since the posterior mean provides
j
an estimate for the function values, the update condition is designed to eliminate all points
at which the (estimated) function value is smaller than the current best estimate of the
maximum value, upto an estimation error. Note that trimming is a deterministic procedure
which ensures that all the agents and the server share a common value of X .
j+1
A detailed pseudocode of both the agent and the server side of the DUETS is provided in
Algorithms 1 and 2 respectively.
Algorithm 1 DUETS : Agent n ∈ {1,2,...,N}
1: Input: Size of the first epoch T 1, error probability δ
2: t ← 0,j ← 1, X 1 ← X
3: while t < T do
(n)
4: D = ∅
j
5: for i ∈ {1,2,...,T j} do
6: Query a point x( tn) uniformly at random from X j−1 using the coin C n and observe
(n)
y
t
(n) (n) (n)
7: D ← D ∪{x }
j j t
8: t ← t+1
9: if t > T then
10: Terminate
11: end if
12: end for
13: Receive the global inducing set S j
14: Set v j(n) ← Z⊤
D
j(n),SjY j(n) , where Y j(n) = [y t−Tj,y t−Tj+1,...,y t]⊤
15: Receive v j and σ j,max from the server
16: Use v j to compute µ˜ j(·) = z S⊤ j(·)v j
17: Update X j to X j+1 using Eqn. (9)
(cid:112)
18: T j+1 ← ⌊ TT j⌋
19: j ← j +1
20: end while
11Algorithm 2 DUETS : Server
1: input: Size of the first epoch T 1, error probability δ
2: t ← 0,j ← 1, X 1 ← X
3: while t < T do
4: Use the coins C 1,C 2,...,C N to reproduce the sets D j(1) ,D j(2) ,...,D j(N)
5: D j ←
(cid:83)N
n=1D
j(n)
6: Set σ j,max ← sup x∈Xj σ j(x)
7: Construct the set S j by including each point from D j with probability p j, independent
of other points
8: Broadcast S j to all the agents
(n)
9: Receive v from all agents n ∈ {1,2,...,N}
j
(cid:18) (cid:19)−1
10: Set v j ← λI |Sj|+Z⊤ D j(n),SjZ D j(n),Sj ((cid:80)N n=1v j(n) ).
11: Broadcast v j and σ j,max to all the agents
12: Update X j to X j+1 using Eqn. (9)
13: t ← t+T j
(cid:112)
14: T j+1 ← ⌊ TT j⌋
15: j ← j +1
16: end while
4 Performance Analysis
The following theorem characterizes the regret performance and communication cost of
DUETS.
Theorem 4.1. Consider the distributed kernel bandit problem described in Section 2. For
a given δ ∈ (0,1), let the policy parameters of DUETS be such that T ≥ M/N and
1
p = 72log 4N. Then with probability at least 1−δ, the regret and communication cost
0 δ
incurred by DUETS satisfy the following relations:
R =
O˜((cid:112)
NTγ log(T/δ))
DUETS NT
C = O˜(γ ).
DUETS NT
Here, M is a constant that depends only upon the kernel k and the domain X and it is
independent of N and T.4
As shown in above theorem, DUETS achieves order-optimal regret as it matches the
lower bound established in Scarlett et al. (2017) upto logarithmic factors. DUETS is the
4The constant M is the same as one in Lemma 4.3, which has been adopted from Salgia et al. (2023a).
We refer the reader to Salgia et al. (2023a) for an exact expression of the constant and additional related
discussion.
12first algorithm to close this gap to the lower bound in the distributed setup and achieve
order-optimal regret performance. Moreover, DUETS incurs a communication cost that is
sublinear in both T and N for all kernels. Furthermore, it can be much smaller that NT,
depending upon the smoothness of the kernel. For example, using the bounds on information
gain (Vakili et al., 2021b), we can show that the communication cost incurred by DUETS
is O(logd(NT)).
Proof. WeprovideasketchoftheproofofTheorem4.1here. Theregretboundisobtainedby
first bounding the regret incurred by DUETS in each epoch j and then summing the regret
across different epochs. In any epoch j, the agents take purely exploratory by uniformly
sampling the region X . Thus, to bound the regret incurred at any step during an epoch,
j
we use the crude bound ∆ := sup (f(x∗)−f(x)). Consequently, the regret during the
j x∈Xj
jth epoch, denoted by R(j), is upper bounded by N ·∆ T . Note that the update criterion
j j
(Eqn. (9)) is designed to obtain a refined localization of x∗ by eliminating the points with
low function values consequently leading to smaller values of ∆ as the algorithm proceeds.
j
The epoch lengths are carefully chosen to balance the increase in epoch length with the
decrease in ∆ to obtain the tightest bound. These ideas are captured in the following
j
lemmas from the regret bound follows.
Lemma 4.2. Let ∆ := sup f(x∗)−f(x). Then, the following bound holds all epochs
j x∈Xj
j ≥ 1 with probability 1− δ.
2
(cid:32) (cid:33)
4B
∆ ≤ 8β(δ′)· sup σ (x) + ,
j j
T
x∈Xj−1
where δ′ = δ and U denotes the discretization defined in Assump-
2(log(logN+logT)+4)|UT| T
tion 2.3.
Lemma 4.3. Let σ2(·) denote the posterior variance corresponding to the set D obtained
j j
by sampling NT points uniformly at random from the domain X . Then, for T ≥ M(δ)/N
j j 1
and for any f satisfying Assumption 2.4, the following bound holds with probability 1−δ for
all epochs j ≥ 1:
γ
sup σ2(x) ≤ C · NTj.
j f,X NT
x∈Xj j
Here C denotes a constant that depends only on f and the domain X and is independent
f
of both N and T.
Lemma 4.4. The total number of epochs in DUETS over a time horizon of T is less than
log(log(max{N,T}))+4.
13Lemma 4.3 is result adopted from the recent work by Salgia et al. (2023a) that establishes
bounds on worst-case posterior variance corresponding to a set of randomly sampled points.
For the bound on communication cost, note that each epoch j, the server broadcasts the
inducing set S j, which consists of |S j| vectors in Rd, the vector v
j
∈ R|Sj| and the scalar
σ , resulting in a downlink cost of O(|S |) in epoch j. Similarly, since each agent just up-
j,max j
loads v j(n) ∈ R|Sj|, the uplink cost of DUETS in epoch j also satisfies O(|S j|). Consequently,
the communication cost of DUETS in epoch j is bounded by O(|S |). The following lemma
j
gives a high probability bound on the |S |.
j
Lemma 4.5. Let S denote the inducing set construct in jth epoch, as outlined in Section 3.
j
Then, with probability at least 1−δ,
(cid:18) (cid:18) (cid:19)(cid:19)
log(logN logT)
|S | ≤ C · 3+log ·γ ,
j f,X NT
δ
holds for all epochs j. In the above expression, C is same as the constant in Lemma 4.3.
f,X
The bound on the communication cost follows directly from Lemmas 4.5 and 4.4. Please
refer to Appendix A for a detailed proof.
5 Empirical Studies
We perform several empirical studies to corroborate our theoretical findings. We compare
the regret performance and communication cost of our proposed algorithm, DUETS, against
three baseline algorithms — DisKernelUCB, ApproxDisKernelUCB and N-KernelUCB. The
firsttwoaredistributedkernelbanditsalgorithmsproposedinLietal.(2022). N-KernelUCB
is a baseline algorithm considered in Li et al. (2022) where each agent locally runs the
GP-UCB algorithm (Chowdhury and Gopalan, 2017) with no communication among the
agents.
We compare the performance of all the four algorithm across four benchmark functions.
The first two are synthetic functions h ,h : B → R considered in Li et al. (2022), where B
1 2
denotes the unit ball centered at origin in R10. The functions are given by:
h (x) := cos(3x⊤θ⋆)
1
h (x) := (x⊤θ⋆)3−3(x⊤θ⋆)2+3(x⊤θ⋆)+3.
2
For both the functions θ⋆ is randomly chosen from the surface of the unit ball B. The other
twofunctionsareBranin(Azimietal.,2012;Pichenyetal.,2013)andHartmann-4D(Picheny
14(a) h (x) (b) h (x) (c) Branin (d) Hartmann-4D
1 2
(e) h (x) (f) h (x) (g) Branin (h) Hartmann-4D
1 2
Figure 1: Cumulative regret (Fig. (1a-1d) and communication cost (1e-1h) for all algorithms
across different benchmark functions averaged over 5 Monte Carlo runs. The shaded region
represents error bars corresponding to one standard deviation. As seen from the above plots,
DUETS obtains a superior performance, both in terms of regret and communication cost,
over other algorithm across all functions.
et al., 2013), which are commonly used benchmark functions for Bayesian Optimization.
The Branin function is defined over X = [0,1]2 while the Hartmann-4D function is defined
over X = [0,1]4.
We consider a distributed kernel bandit described in Section 2 with N = 10 agents. For all
the experiments, we use the Squared Exponential kernel. The length scale was set to 0.2 for
Branin and to 1 for all other functions. The observations were corrupted with zero mean
Gaussiannoisewithastandarddeviationof0.2. TheparameterD forApproxDisKernelUCB
and DisKernelUCB was set to 20 and 10 respectively. For DUETS , we set T = 2 and
1
p = 10. The parameter β was selected using a grid search over {0.2,0.5,1,2,5} for all the
0
algorithms. All the algorithms were run for T = 50 time steps. We averaged the cumulative
regret and the communication cost incurred by different algorithms over 5 Monte Carlo
runs.
15The cumulative regret incurred by different algorithms across the different benchmark
function are shown in the top row of Figure 1. The bottom row consists of the corresponding
plots for the communication cost incurred by the different algorithm. The shaded regions
denotes error bars upto standard deviation on either side of the mean value. As evident from
the plots, DUETS achieves a significantly lower regret as compared to all other algorithms
consistently across benchmark functions. DUETS also incurs a smaller communication
overhead as compared to other algorithms, corroborating our theoretical results.
References
Y. Abbasi-Yadkori, D. Pál, and C. Szepesvári. Improved algorithms for linear stochastic
bandits. In Proceedings of the 25th Annual Conference on Neural Information Processing
Systems, 2011. ISBN 9781618395993. (Cited on 2)
S. Amani, T. Lattimore, A. György, and L. F. Yang. Distributed Contextual Linear Bandits
with Minimax Optimal Communication Cost, 2022. URL http://arxiv.org/abs/2205.
13170. (Cited on 6)
J.Azimi, A.Jalali, andX.Z.Fern. Hybridbatchbayesianoptimization. InProceedings of the
29th International Conference on Machine Learning, ICML, volume 2, pages 1215–1222,
2012. ISBN 9781450312851. (Cited on 14)
D. Calandriello, L. Carratino, A. Lazaric, M. Valko, and L. Rosasco. Gaussian Process
Optimization with Adaptive Sketching: Scalable and No Regret. Proceedings of Machine
Learning Research, 99:1–25, 2019. (Cited on 9, 21)
R. Camilleri, J. Katz-Samuels, and K. Jamieson. High-Dimensional Experimental Design
and Kernel Bandits. In Proceedings of the 38th International Conference on Machine
Learning, 2021. (Cited on 2)
R.Chawla, A.Sankararaman, A.Ganesh, andS.Shakkottai. TheGossipingInsert-Eliminate
Algorithm for Multi-Agent Bandits, 2020. (Cited on 6)
S. R. Chowdhury and A. Gopalan. On kernelized multi-armed bandits. In Proceedings of the
34th International Conference on Machine Learning, ICML, volume 2, pages 1397–1422,
2017. (Cited on 2, 7, 9, 14)
Z. Dai, B. K. H. Low, and P. Jaillet. Federated Bayesian optimization via Thompson
sampling. In Proceedings of the 34th Annual Conference on Neural Information Processing
Systems, volume 2020-Decem, 2020. (Cited on 2)
Y. Du, W. Chen, Y. Kuroki, and L. Huang. Collaborative Pure Exploration in Kernel
Bandit. In Proceedings of the 11th International Conference on Learning Representations,
ICLR, 2023. (Cited on 5)
16A. Dubey and A. Pentland. Kernel methods for cooperative multi-agent contextual bandits.
In Proceedings of the 37th International Conference on Machine Learning, ICML 2020,
pages 2720–2730, 2020. ISBN 9781713821120. (Cited on 3, 5)
A.Ghosh, A.Sankararaman, andK.Ramchandran. AdaptiveClusteringandPersonalization
in Multi-Agent Stochastic Linear Bandits, 2021. URL http://arxiv.org/abs/2106.
08902. (Cited on 6)
O. Hanna, L. Yang, and C. Fragouli. Learning from distributed users in contextual linear
bandits without sharing the context. In Proceedings of the 36th Annual Conference on
Neural Information Processing Systems, volume 35, pages 11049–11062, 2022. (Cited on
6)
R. Huang, W. Wu, J. Yang, and C. Shen. Federated Linear Contextual Bandits. In Advances
in Neural Information Processing Systems, volume 32, pages 27057–27068, 2021. ISBN
9781713845393. (Cited on 6)
A. Jacot, F. Gabriel, and C. Hongler. Neural tangent kernel: Convergence and generalization
in neural networks. In Proceedings of the 32nd Annual Conference on Neural Information
Processing Systems, pages 8571–8580, 2018. (Cited on 2)
N. Korda, B. Szorenyi, and S. Li. Distributed clustering of linear bandits in peer to peer
networks. In 33rd International Conference on Machine Learning, ICML 2016, volume 3,
pages 1966–1980, 2016. ISBN 9781510829008. (Cited on 6)
P. Landgren, V. Srivastava, and N. E. Leonard. On distributed cooperative decision-making
in multiarmed bandits. In Proceedings of the European Control Conference, ECC, pages
243–248, 2017. ISBN 9781509025916. (Cited on 5)
C. Li, H. Wang, M. Wang, and H. Wang. Communication Efficient Distributed Learning for
Kernelized Contextual Bandits. In Proceedings of the 36th Annual Conference on Neural
Information Processing Systems, 2022. (Cited on 3, 5, 14)
Z. Li and J. Scarlett. Gaussian process bandit optimization with few batches. In Proceedings
of the 25th International Conference on Artificial Intelligence and Statistics, AISTATS,
2022. (Cited on 2, 3, 7)
A. Mitra, H. Hassani, and G. Pappas. Exploiting Heterogeneity in Robust Federated
Best-Arm Identification, 2021. URL http://arxiv.org/abs/2109.05700. (Cited on 6)
A. Mitra, H. Hassani, and G. J. Pappas. Linear Stochastic Bandits over a Bit-Constrained
Channel, 2022. (Cited on 6)
V. Picheny, T. Wagner, and D. Ginsbourger. A benchmark of kriging-based infill criteria for
noisy optimization. Structural and Multidisciplinary Optimization, 48(3):607–626, 2013.
ISSN 1615147X. (Cited on 14)
17S. Salgia and Q. Zhao. Distributed linear bandits under communication constraints. In
Proceedings of the 40th International Conference on Machine Learning, ICML, pages
29845–29875. PMLR, 2023. (Cited on 6)
S.Salgia, S.Vakili, andQ.Zhao. Adomain-shrinkingbasedBayesianoptimizationalgorithm
with order-optimal regret performance. In Proceedings of the 35th Annual Conference on
Neural Information Processing Systems, volume 34, 2021. (Cited on 2)
S. Salgia, S. Vakili, and Q. Zhao. Random exploration in bayesian optimization: Order-
optimal regret and computational efficiency, 2023a. (Cited on 4, 7, 12, 14)
S. Salgia, S. Vakili, and Q. Zhao. Collaborative learning in kernel-based bandits for
distributed users. IEEE Transactions on Signal Processing, 71:3956–3967, 2023b. (Cited
on 5, 21)
A. Sankararaman, A. Ganesh, and S. Shakkottai. Social learning in multi agent multi armed
bandits. Proc. ACM Meas. Anal. Comput. Syst., 3(3), dec 2019. (Cited on 5)
J. Scarlett, I. Bogunovic, and V. Cehver. Lower Bounds on Regret for Noisy Gaussian
Process Bandit Optimization. In Conference on Learning Theory, volume 65, pages 1–20,
2017. (Cited on 4, 12)
S. Shahrampour, A. Rakhlin, and A. Jadbabaie. Multi-armed bandits in multi-agent
networks. In 2017 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pages 2786–2790, 2017. (Cited on 5)
C. Shi and C. Shen. Federated Multi-Armed Bandits. In Proceedings of the 35th AAAI
Conference on Artificial Intelligence, pages 9603–9611, 2021. (Cited on 6)
C. Shi, C. Shen, and J. Yang. Federated Multi-armed Bandits with Personalization, 2021.
URL http://arxiv.org/abs/2102.13101. (Cited on 6)
N. Srinivas, A. Krause, S. Kakade, and M. Seeger. Gaussian process optimization in the
bandit setting: no regret and experimental design. In Proceedings of the 27th International
Conference on Machine Learning, ICML, pages 1015–1022, 2010. (Cited on 2, 7, 8, 9)
S. Vakili, N. Bouziani, S. Jalali, A. Bernacchia, and D.-s. Shiu. Optimal order simple regret
for Gaussian process bandits. In Proceedings of the 35th Annual Conference on Neural
Information Processing Systems, 2021a. (Cited on 7, 8)
S. Vakili, K. Khezeli, and V. Picheny. On information gain and regret bounds in Gaus-
sian process bandits. In Proceedings of the 24th International Conference on Artificial
Intelligence and Statistics, AISTATS, 2021b. (Cited on 9, 13)
18S. Vakili, J. Scarlett, D.-s. Shiu, and A. Bernacchia. Improved convergence rates for sparse
approximation methods in kernel-based learning. In Proceedings of the 39th International
Conference on Machine Learning, ICML, pages 21960–21983. PMLR, 2022. (Cited on 7)
M. Valko, N. Korda, R. Munos, I. Flaounas, and N. Cristianini. Finite-time analysis of
kernelised contextual bandits. In Proceedings of the 29th Conference on Uncertainty in
Artificial Intelligence, UAI, pages 654–663, 2013. (Cited on 2)
Y. Wang, J. Hu, X. Chen, and L. Wang. Distributed Bandit Learning: Near-Optimal Regret
with Efficient Communication. In Proceedings of the 7th International Conference on
Learning Representations (ICLR), 2019. (Cited on 6)
V. Wild, M. Kanagawa, and D. Sejdinovic. Connections and equivalences between the
nyström method and sparse variational gaussian processes, 2021. (Cited on 4, 9)
Z. Zhu, J. Zhu, J. Liu, and Y. Liu. Federated Bandit: A Gossiping Approach. Proceedings
of the ACM on Measurement and Analysis of Computing Systems, 5(1):1–29, 2021. (Cited
on 6)
A Appendix A.
A.1 Proof of Theorem 4.1
Proof. In this section, we provide a detailed proof of Theorem 4.1. For the regret bound,
we first bound the regret incurred by DUETS in each epoch j and then sum it across
different epochs to obtain a bound on the overall cumulative regret. We first prove the theo-
remassumingtheresultsfromLemmas4.2,4.3and4.4andthenseparatelyprovethelemmas.
Consider any epoch j ≥ 1 and let R(j) denote the regret incurred by DUETS in this
epoch. Since the agents take purely exploratory actions by uniform sampling points
from the current set, we have the following crude bound R(j) ≤ ∆ · NT · M , where
j j f
∆ := sup (f(x∗)−f(x)). The term NT ·M corresponds to number of points sampled
j x∈Xj j f
during the epoch as we sample each connected component of X , of which there are at most
j
M , NT times. For j = 1, we use the trivial bound,
f j
∆ = sup(f(x∗)−f(x)) ≤ 2supf(x) ≤ 2B,
1
x∈X x∈X
which gives us R(1) ≤ 2B·NT ·M . On invoking Lemma 4.2 for j > 1 we obtain,
1 f
R(j) ≤ ∆ ·NT ·M
j j f
(cid:32) (cid:32) (cid:33) (cid:33)
4B
≤ NT ·M · 8β(δ′)· sup σ (x) + ,
j f j−1
T
x∈Xj−1
19δ
where δ′ = . Using Lemma 4.3, we can further bound this expression
2(loglogNT +4)|U |
T
as
R(j) ≤ ∆ ·NT ·M
j j f
(cid:18) (cid:114)γ 4B(cid:19)
≤ NT ·M · 8β(δ′)·C · NTj−1 +
j f f,X
NT T
j−1
(cid:18) (cid:19)
(cid:113) 4BNT
≤ M · 8C1/2 ·β(δ′)· NTγ + j
f f,X NTj−1 T
(cid:18) (cid:19)
≤ M ·
8C1/2 ·β(δ′)·(cid:112)
NTγ +
4BNT j
.
f f,X NT T
√
In the third line, we used the inequality
√Tj
≤ T, which follows from the definition of
Tj−1
T . In the last line, we used the fact that γ is an increasing function of m. Thus, if J
j m
denotes an upper bound on the number of epochs, we can write:
J J (cid:18) (cid:19)
(cid:88) R(j) ≤ 2BM ·NT +(cid:88) M · 8C1/2 ·β(δ′)·(cid:112) NTγ + 4BNT j
f 1 f f,X NT T
j=1 j=2
J
≤ 2BM ·NT +J ·(cid:16) 8C′ ·β(δ′)·(cid:112) NTγ (cid:17) + 4BNM f (cid:88) T
f 1 f,X NT T j
j=1
(cid:16) (cid:112) (cid:17)
≤ 2BM ·NT +J · 8C′ ·β(δ′)· NTγ +4BNM . (10)
f 1 f,X NT f
We next optimize the length of the first epoch T in order to achieve order optimal regret.
1
DUETS achieves order optimal regret for N ≤ max(T,γ ).
NT
(cid:113)
If N < T we can choose T = T +M(δ′) where δ′ = δ . Left hand side of
1 N 2(loglogNT+4)
√ √
equation (10) can now be writt √en as O(cid:101)( NTγ NTβ(δ′)) ≡ O(cid:101)(cid:0) NT √γ
NT
(cid:0) log T δ(cid:1)(cid:1) .
If N ≤ γ
NT
we can fix T 1√= T +M(δ′). We have NT
1
≤ O(cid:101)( NTγ NT) and the left
hand-side is once again O(cid:101)(cid:0) NTγ
NT
(cid:0) log T δ(cid:1)(cid:1) .
Note that by Lemma 4.4, J is upper bounded by log(logN logT)+4 and is thus O(cid:101)(1).
Before moving onto the proofs of Lemmas 4.2 and 4.4, we state two auxiliary lemmas that
will be useful for our analysis.
Definition A.1. Let D = {x ,x ,...,x } ⊂ X be a collection m points and S be any
1 2 m
subset of D. Let σ2(·) denote the posterior variance corresponding to the points in D and
D
σ˜2(·) denote the approximate posterior computed based on the points in S. We call S to be
S
20an ε-accurate inducing set if the following relations are true for all x ∈ X.
1−ε 1+ε
·σ˜2(x) ≤ σ2(x) ≤ ·σ˜2(x).
1+ε S D 1−ε S
Lemma A.2 (Adapted from Calandriello et al. (2019)). Let D = {x ,x ,...,x } ⊂ X be
1 2 m
a collection m points and S be a random subset of D constructed by including each point
with probability p, independent of other points. Then S is an ε-accurate inducing set with
(cid:18) 3pε2 (cid:19)
probability 1−4mexp − , where σ2 = sup σ2(x).
8σ2 max x∈X D
max
Lemma A.3. Let DUETS be run with a choice of p = 72log(4NT/δ′). Then, for all
0
epochs j ≥ 1, the global inducing set S is 0.5-accurate with probability 1−δ.
j
Proof. The statement is an immediate consequence of Lemma A.2 with the given choice of
parameter p .
0
We are now ready to prove Lemmas 4.2 and 4.4.
A.2 Proof of Lemma 4.2
Proof. Consider any epoch j ≥ 2 and let x ∈ X . Let ∆(x) := f(x∗) − f(x). We will
j
obtain a bound on ∆(x) for any general x in order establish the bound on ∆ . Using the
j
discretization from Assumption 2.3 for X , we obtain,
j
∆(x) = f(x∗)−f(x)
≤ f(x∗)−f([x∗] )+f([x∗] )−(f(x)−f([x] ))−f([x] )
UT UT UT UT
2B
≤ f([x∗] )−f([x] )+ .
UT UT
T
Using the result from Salgia et al. (2023b), we obtain the following high probability bound
that holds with probability 1−δ:
2B
∆(x) ≤ f([x∗] )−f([x] )+
UT UT
T
2B
≤ µ˜ ([x∗] )+β(δ′)σ˜ ([x∗] )−µ˜ ([x] )+β(δ′)σ˜ ([x] )+
j UT j UT j UT j UT
T
4B
≤ µ˜ (x∗)−µ˜ (x)+β(δ′)σ˜ ([x∗] )+β(δ′)σ˜ ([x] )+ ,
j j j UT j UT
T
where we again used Assumption 2.3 in the last step. We claim that x∗ ∈ X for all j ≥ 2.
j−1
Assuming this claim this true, we can bound the above expression as
4B
∆(x) ≤ sup µ˜ (x′)−µ˜ (x)+β(δ′)σ˜ ([x∗] )+β(δ′)σ˜ ([x] )+
j j j UT j UT
T
x∈Xj−1
4B
≤ 2β(δ′)σ +β(δ′)σ˜ ([x∗] )+β(δ′)σ˜ ([x] )+ ,
j,max j UT j UT
T
21where we used the update condition (Eqn. (9)) in the second step. Since S is 0.5-accurate
j
(Lemma A.3), we have σ˜2(x) ≤ 3σ2(x) ≤ 3σ2 . On plugging this back into the above
j j j,max
equation, we obtain,
4B
∆(x) ≤ 8β(δ′)σ + .
j,max
T
The statement of the lemma follows by ∆ = sup ∆(x).
j x∈Xj
Weproveourclaimx∗ ∈ X forallj ≥ 1usinginduction. Clearly,x∗ ∈ X = X,bydefinition.
j 1
Assume x∗ ∈ X for some j ≥ 2. Fix an arbitrary x ∈ X , from the confidence bound
j−1 j−1
lemma we have:
µ (x)−µ (x∗) ≤(f(x)−f(x∗))+β(δ′ )(σ (x)+σ (x∗)) ≤ 2σ (x),
j−1 j−1 j j j−1.max
where the second inequality follows as f(x) ≤ f(x∗). As the inequality holds ∀x ∈ X we
j−1
must have:
sup µ (x)−µ (x∗) ≤ 2σ (x)
j−1 j−1 j−1.max
x∈Xj−1
and thus indeed x ∈ X .
j
A.3 Proof of Lemma 4.4
We define E(s) := min{j : T ≥ T/4 | T = s}. Note that T is an increasing function of
j 1 j
j. Since T ≥ T/4, we can conclude that E(s)+4 is an upper bound on the number of
E(s)
epochs. Thus, we focus on bounding E(s). We first show that E(s) is a non-decreasing
function of s.
To that effect, we claim that for j ≥ 2 the epoch lengths satisfy the relation T ≥ T1−2−j+1 ·
j
T2−j+1. This relation follows immediately using induction. For the base case, note that
1
T ≥
T1/2·T1/2
, by definition. Assume that the relation holds for j −1. Thus,
2 1
T ≥
T1/2·T1/2
≥
T1/2·T1−2−(j−1)+1−1 ·T2−(j−1)+1−1
≥
T1−2−j+1 ·T2−j+1
. (11)
j j−1 1 1
Since T ’s are lower bounded by an increasing function of T , the number of epochs E(s) is
j 1
a non-increasing function of s. Since T ≥ T , E(cid:0)T (cid:1) is an upper bound on the number of
1 N N
epochs for all choices of T .
1
Let j∗ = max{log(log(T)),log(log(N))}. Using the above relation for T from Eqn. (11)
j
and the lower bound on T , we have,
1
T
j∗
≥ T ·N−21−j = T ·(cid:16) 2−lo 2g jN(cid:17)2 ≥ T ·2−2
22We can hence conclude that T ≥ T/4, which implies that E(T ) ≤ j∗ for all permissible
j∗ 1
choices of T . Consequently, the number of epochs are bounded as log(log(max{N,T}))+4.
1
A.4 Proof of Lemma 4.5
For all epochs j ≥ 1, recall that the inducing set is constructed by including each point
from D with probability p , independent of other points. Thus, |S | is a binomial random
j j j
variablewithparameters|D | = NT andp . UsingtheChernoffboundforBinomialrandom
j j j
variables, we can conclude that
(cid:18) ε2NT p (cid:19)
j j
Pr(|S | > (1+ε)NT p ) ≤ exp − .
j j j
2+ε
Invoking the bound with ε = 2+log(1/δ′), with δ′ = δ/(loglog(NT)+4) yields that the
following relation holds with probability 1−δ′:
|S | ≤ (3+log(1/δ′))·NT p
j j j
≤ (3+log(1/δ′))·NT ·p σ2
j 0 j,max
γ
≤ (3+log(1/δ′))·NT p ·C · NTj
j 0 f,X
NT
j
≤ (3+log(1/δ′))p γ ,
0 NT
where we used Lemma 4.3 in the third step and monotonicity of γ in the last step. On
m
taking a union bound over all epochs and using the bound on the number of epochs from
Lemma 4.4, we conclude that for all epochs j, |S | = O˜(γ ) with probability 1−δ.
j NT
23