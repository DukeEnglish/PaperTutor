[
    {
        "title": "How NeRFs and 3D Gaussian Splatting are Reshaping SLAM: a Survey",
        "authors": "Fabio TosiYoumin ZhangZiren GongErik SandströmStefano MattocciaMartin R. OswaldMatteo Poggi",
        "links": "http://arxiv.org/abs/2402.13255v1",
        "entry_id": "http://arxiv.org/abs/2402.13255v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13255v1",
        "summary": "Over the past two decades, research in the field of Simultaneous Localization\nand Mapping (SLAM) has undergone a significant evolution, highlighting its\ncritical role in enabling autonomous exploration of unknown environments. This\nevolution ranges from hand-crafted methods, through the era of deep learning,\nto more recent developments focused on Neural Radiance Fields (NeRFs) and 3D\nGaussian Splatting (3DGS) representations. Recognizing the growing body of\nresearch and the absence of a comprehensive survey on the topic, this paper\naims to provide the first comprehensive overview of SLAM progress through the\nlens of the latest advancements in radiance fields. It sheds light on the\nbackground, evolutionary path, inherent strengths and limitations, and serves\nas a fundamental reference to highlight the dynamic progress and specific\nchallenges.",
        "updated": "2024-02-20 18:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13255v1"
    },
    {
        "title": "CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples",
        "authors": "Jianrui ZhangMu CaiTengyang XieYong Jae Lee",
        "links": "http://arxiv.org/abs/2402.13254v1",
        "entry_id": "http://arxiv.org/abs/2402.13254v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13254v1",
        "summary": "We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two under-explored\ncritical problems: the neglect of the physically grounded reasoning (counting\nand position understanding) and the potential of using highly capable text and\nimage generation models for semantic counterfactual fine-tuning. Our work\npioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\na grounded image generation model, GLIGEN, to generate finetuning data,\nresulting in significant performance improvements: +33% and +37% for CLIP and\nLLaVA, respectively, on our newly curated Flickr30k-Positions benchmark.\nMoreover, we exploit the capabilities of high-performing text generation and\nimage generation models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V.",
        "updated": "2024-02-20 18:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13254v1"
    },
    {
        "title": "Improving Robustness for Joint Optimization of Camera Poses and Decomposed Low-Rank Tensorial Radiance Fields",
        "authors": "Bo-Yu ChengWei-Chen ChiuYu-Lun Liu",
        "links": "http://arxiv.org/abs/2402.13252v1",
        "entry_id": "http://arxiv.org/abs/2402.13252v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13252v1",
        "summary": "In this paper, we propose an algorithm that allows joint refinement of camera\npose and scene geometry represented by decomposed low-rank tensor, using only\n2D images as supervision. First, we conduct a pilot study based on a 1D signal\nand relate our findings to 3D scenarios, where the naive joint pose\noptimization on voxel-based NeRFs can easily lead to sub-optimal solutions.\nMoreover, based on the analysis of the frequency spectrum, we propose to apply\nconvolutional Gaussian filters on 2D and 3D radiance fields for a\ncoarse-to-fine training schedule that enables joint camera pose optimization.\nLeveraging the decomposition property in decomposed low-rank tensor, our method\nachieves an equivalent effect to brute-force 3D convolution with only incurring\nlittle computational overhead. To further improve the robustness and stability\nof joint optimization, we also propose techniques of smoothed 2D supervision,\nrandomly scaled kernel parameters, and edge-guided loss mask. Extensive\nquantitative and qualitative evaluations demonstrate that our proposed\nframework achieves superior performance in novel view synthesis as well as\nrapid convergence for optimization.",
        "updated": "2024-02-20 18:59:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13252v1"
    },
    {
        "title": "FlashTex: Fast Relightable Mesh Texturing with LightControlNet",
        "authors": "Kangle DengTimothy OmernickAlexander WeissDeva RamananJun-Yan ZhuTinghui ZhouManeesh Agrawala",
        "links": "http://arxiv.org/abs/2402.13251v1",
        "entry_id": "http://arxiv.org/abs/2402.13251v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13251v1",
        "summary": "Manually creating textures for 3D meshes is time-consuming, even for expert\nvisual content creators. We propose a fast approach for automatically texturing\nan input 3D mesh based on a user-provided text prompt. Importantly, our\napproach disentangles lighting from surface material/reflectance in the\nresulting texture so that the mesh can be properly relit and rendered in any\nlighting environment. We introduce LightControlNet, a new text-to-image model\nbased on the ControlNet architecture, which allows the specification of the\ndesired lighting as a conditioning image to the model. Our text-to-texture\npipeline then constructs the texture in two stages. The first stage produces a\nsparse set of visually consistent reference views of the mesh using\nLightControlNet. The second stage applies a texture optimization based on Score\nDistillation Sampling (SDS) that works with LightControlNet to increase the\ntexture quality while disentangling surface material from lighting. Our\npipeline is significantly faster than previous text-to-texture methods, while\nproducing high-quality and relightable textures.",
        "updated": "2024-02-20 18:59:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13251v1"
    },
    {
        "title": "Video ReCap: Recursive Captioning of Hour-Long Videos",
        "authors": "Md Mohaiminul IslamNgan HoXitong YangTushar NagarajanLorenzo TorresaniGedas Bertasius",
        "links": "http://arxiv.org/abs/2402.13250v1",
        "entry_id": "http://arxiv.org/abs/2402.13250v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13250v1",
        "summary": "Most video captioning models are designed to process short video clips of few\nseconds and output text describing low-level visual concepts (e.g., objects,\nscenes, atomic actions). However, most real-world videos last for minutes or\nhours and have a complex hierarchical structure spanning different temporal\ngranularities. We propose Video ReCap, a recursive video captioning model that\ncan process video inputs of dramatically different lengths (from 1 second to 2\nhours) and output video captions at multiple hierarchy levels. The recursive\nvideo-language architecture exploits the synergy between different video\nhierarchies and can process hour-long videos efficiently. We utilize a\ncurriculum learning training scheme to learn the hierarchical structure of\nvideos, starting from clip-level captions describing atomic actions, then\nfocusing on segment-level descriptions, and concluding with generating\nsummaries for hour-long videos. Furthermore, we introduce Ego4D-HCap dataset by\naugmenting Ego4D with 8,267 manually collected long-range video summaries. Our\nrecursive model can flexibly generate captions at different hierarchy levels\nwhile also being useful for other complex video understanding tasks, such as\nVideoQA on EgoSchema. Data, code, and models are available at:\nhttps://sites.google.com/view/vidrecap",
        "updated": "2024-02-20 18:58:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13250v1"
    }
]