[
    {
        "title": "CHILI: Chemically-Informed Large-scale Inorganic Nanomaterials Dataset for Advancing Graph Machine Learning",
        "authors": "Ulrik Friis-JensenFrederik L. JohansenAndy S. AnkerErik B. DamKirsten M. Ø. JensenRaghavendra Selvan",
        "links": "http://arxiv.org/abs/2402.13221v2",
        "entry_id": "http://arxiv.org/abs/2402.13221v2",
        "pdf_url": "http://arxiv.org/pdf/2402.13221v2",
        "summary": "Advances in graph machine learning (ML) have been driven by applications in\nchemistry as graphs have remained the most expressive representations of\nmolecules. While early graph ML methods focused primarily on small organic\nmolecules, recently, the scope of graph ML has expanded to include inorganic\nmaterials. Modelling the periodicity and symmetry of inorganic crystalline\nmaterials poses unique challenges, which existing graph ML methods are unable\nto address. Moving to inorganic nanomaterials increases complexity as the scale\nof number of nodes within each graph can be broad ($10$ to $10^5$). The bulk of\nexisting graph ML focuses on characterising molecules and materials by\npredicting target properties with graphs as input. However, the most exciting\napplications of graph ML will be in their generative capabilities, which is\ncurrently not at par with other domains such as images or text.\n  We invite the graph ML community to address these open challenges by\npresenting two new chemically-informed large-scale inorganic (CHILI)\nnanomaterials datasets: A medium-scale dataset (with overall >6M nodes, >49M\nedges) of mono-metallic oxide nanomaterials generated from 12 selected crystal\ntypes (CHILI-3K) and a large-scale dataset (with overall >183M nodes, >1.2B\nedges) of nanomaterials generated from experimentally determined crystal\nstructures (CHILI-100K). We define 11 property prediction tasks and 6 structure\nprediction tasks, which are of special interest for nanomaterial research. We\nbenchmark the performance of a wide array of baseline methods and use these\nbenchmarking results to highlight areas which need future work. To the best of\nour knowledge, CHILI-3K and CHILI-100K are the first open-source nanomaterial\ndatasets of this scale -- both on the individual graph level and of the dataset\nas a whole -- and the only nanomaterials datasets with high structural and\nelemental diversity.",
        "updated": "2024-02-21 08:07:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13221v2"
    },
    {
        "title": "Testing Calibration in Subquadratic Time",
        "authors": "Lunjia HuKevin TianChutong Yang",
        "links": "http://arxiv.org/abs/2402.13187v1",
        "entry_id": "http://arxiv.org/abs/2402.13187v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13187v1",
        "summary": "In the recent literature on machine learning and decision making, calibration\nhas emerged as a desirable and widely-studied statistical property of the\noutputs of binary prediction models. However, the algorithmic aspects of\nmeasuring model calibration have remained relatively less well-explored.\nMotivated by [BGHN23], which proposed a rigorous framework for measuring\ndistances to calibration, we initiate the algorithmic study of calibration\nthrough the lens of property testing. We define the problem of calibration\ntesting from samples where given $n$ draws from a distribution $\\mathcal{D}$ on\n(predictions, binary outcomes), our goal is to distinguish between the case\nwhere $\\mathcal{D}$ is perfectly calibrated, and the case where $\\mathcal{D}$\nis $\\varepsilon$-far from calibration.\n  We design an algorithm based on approximate linear programming, which solves\ncalibration testing information-theoretically optimally (up to constant\nfactors) in time $O(n^{1.5} \\log(n))$. This improves upon state-of-the-art\nblack-box linear program solvers requiring $\\Omega(n^\\omega)$ time, where\n$\\omega > 2$ is the exponent of matrix multiplication. We also develop\nalgorithms for tolerant variants of our testing problem, and give sample\ncomplexity lower bounds for alternative calibration distances to the one\nconsidered in this work. Finally, we present preliminary experiments showing\nthat the testing problem we define faithfully captures standard notions of\ncalibration, and that our algorithms scale to accommodate moderate sample\nsizes.",
        "updated": "2024-02-20 17:53:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13187v1"
    },
    {
        "title": "Order-Optimal Regret in Distributed Kernel Bandits using Uniform Sampling with Shared Randomness",
        "authors": "Nikola PavlovicSudeep SalgiaQing Zhao",
        "links": "http://arxiv.org/abs/2402.13182v1",
        "entry_id": "http://arxiv.org/abs/2402.13182v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13182v1",
        "summary": "We consider distributed kernel bandits where $N$ agents aim to\ncollaboratively maximize an unknown reward function that lies in a reproducing\nkernel Hilbert space. Each agent sequentially queries the function to obtain\nnoisy observations at the query points. Agents can share information through a\ncentral server, with the objective of minimizing regret that is accumulating\nover time $T$ and aggregating over agents. We develop the first algorithm that\nachieves the optimal regret order (as defined by centralized learning) with a\ncommunication cost that is sublinear in both $N$ and $T$. The key features of\nthe proposed algorithm are the uniform exploration at the local agents and\nshared randomness with the central server. Working together with the sparse\napproximation of the GP model, these two key components make it possible to\npreserve the learning rate of the centralized setting at a diminishing rate of\ncommunication.",
        "updated": "2024-02-20 17:49:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13182v1"
    },
    {
        "title": "On Generalization Bounds for Deep Compound Gaussian Neural Networks",
        "authors": "Carter LyonsRaghu G. RajMargaret Cheney",
        "links": "http://arxiv.org/abs/2402.13106v1",
        "entry_id": "http://arxiv.org/abs/2402.13106v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13106v1",
        "summary": "Algorithm unfolding or unrolling is the technique of constructing a deep\nneural network (DNN) from an iterative algorithm. Unrolled DNNs often provide\nbetter interpretability and superior empirical performance over standard DNNs\nin signal estimation tasks. An important theoretical question, which has only\nrecently received attention, is the development of generalization error bounds\nfor unrolled DNNs. These bounds deliver theoretical and practical insights into\nthe performance of a DNN on empirical datasets that are distinct from, but\nsampled from, the probability density generating the DNN training data. In this\npaper, we develop novel generalization error bounds for a class of unrolled\nDNNs that are informed by a compound Gaussian prior. These compound Gaussian\nnetworks have been shown to outperform comparative standard and unfolded deep\nneural networks in compressive sensing and tomographic imaging problems. The\ngeneralization error bound is formulated by bounding the Rademacher complexity\nof the class of compound Gaussian network estimates with Dudley's integral.\nUnder realistic conditions, we show that, at worst, the generalization error\nscales $\\mathcal{O}(n\\sqrt{\\ln(n)})$ in the signal dimension and\n$\\mathcal{O}(($Network Size$)^{3/2})$ in network size.",
        "updated": "2024-02-20 16:01:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13106v1"
    },
    {
        "title": "Mode Estimation with Partial Feedback",
        "authors": "Charles ArnalVivien CabannesVianney Perchet",
        "links": "http://arxiv.org/abs/2402.13079v1",
        "entry_id": "http://arxiv.org/abs/2402.13079v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13079v1",
        "summary": "The combination of lightly supervised pre-training and online fine-tuning has\nplayed a key role in recent AI developments. These new learning pipelines call\nfor new theoretical frameworks. In this paper, we formalize core aspects of\nweakly supervised and active learning with a simple problem: the estimation of\nthe mode of a distribution using partial feedback. We show how entropy coding\nallows for optimal information acquisition from partial feedback, develop\ncoarse sufficient statistics for mode identification, and adapt bandit\nalgorithms to our new setting. Finally, we combine those contributions into a\nstatistically and computationally efficient solution to our problem.",
        "updated": "2024-02-20 15:24:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13079v1"
    }
]