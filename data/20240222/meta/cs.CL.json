[
    {
        "title": "CounterCurate: Enhancing Physical and Semantic Visio-Linguistic Compositional Reasoning via Counterfactual Examples",
        "authors": "Jianrui ZhangMu CaiTengyang XieYong Jae Lee",
        "links": "http://arxiv.org/abs/2402.13254v1",
        "entry_id": "http://arxiv.org/abs/2402.13254v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13254v1",
        "summary": "We propose CounterCurate, a framework to comprehensively improve the\nvisio-linguistic compositional reasoning capability for both contrastive and\ngenerative multimodal models. In particular, we identify two under-explored\ncritical problems: the neglect of the physically grounded reasoning (counting\nand position understanding) and the potential of using highly capable text and\nimage generation models for semantic counterfactual fine-tuning. Our work\npioneers an approach that addresses these gaps. We first spotlight the\nnear-chance performance of multimodal models like CLIP and LLaVA in physically\ngrounded compositional reasoning. We then apply simple data augmentation using\na grounded image generation model, GLIGEN, to generate finetuning data,\nresulting in significant performance improvements: +33% and +37% for CLIP and\nLLaVA, respectively, on our newly curated Flickr30k-Positions benchmark.\nMoreover, we exploit the capabilities of high-performing text generation and\nimage generation models, specifically GPT-4V and DALLE-3, to curate challenging\nsemantic counterfactuals, thereby further enhancing compositional reasoning\ncapabilities on benchmarks such as SugarCrepe, where CounterCurate outperforms\nGPT-4V.",
        "updated": "2024-02-20 18:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13254v1"
    },
    {
        "title": "BiMediX: Bilingual Medical Mixture of Experts LLM",
        "authors": "Sara PieriSahal Shaji MullappillyFahad Shahbaz KhanRao Muhammad AnwerSalman KhanTimothy BaldwinHisham Cholakkal",
        "links": "http://arxiv.org/abs/2402.13253v1",
        "entry_id": "http://arxiv.org/abs/2402.13253v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13253v1",
        "summary": "In this paper, we introduce BiMediX, the first bilingual medical mixture of\nexperts LLM designed for seamless interaction in both English and Arabic. Our\nmodel facilitates a wide range of medical interactions in English and Arabic,\nincluding multi-turn chats to inquire about additional details such as patient\nsymptoms and medical history, multiple-choice question answering, and\nopen-ended question answering. We propose a semi-automated English-to-Arabic\ntranslation pipeline with human refinement to ensure high-quality translations.\nWe also introduce a comprehensive evaluation benchmark for Arabic medical LLMs.\nFurthermore, we introduce BiMed1.3M, an extensive Arabic-English bilingual\ninstruction set covering 1.3 Million diverse medical interactions, resulting in\nover 632 million healthcare specialized tokens for instruction tuning. Our\nBiMed1.3M dataset includes 250k synthesized multi-turn doctor-patient chats and\nmaintains a 1:2 Arabic-to-English ratio. Our model outperforms state-of-the-art\nMed42 and Meditron by average absolute gains of 2.5% and 4.1%, respectively,\ncomputed across multiple medical evaluation benchmarks in English, while\noperating at 8-times faster inference. Moreover, our BiMediX outperforms the\ngeneric Arabic-English bilingual LLM, Jais-30B, by average absolute gains of\n10% on our Arabic medical benchmark and 15% on bilingual evaluations across\nmultiple datasets. Our project page with source code and trained model is\navailable at https://github.com/mbzuai-oryx/BiMediX .",
        "updated": "2024-02-20 18:59:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13253v1"
    },
    {
        "title": "TofuEval: Evaluating Hallucinations of LLMs on Topic-Focused Dialogue Summarization",
        "authors": "Liyan TangIgor ShalyminovAmy Wing-mei WongJon BurnskyJake W. VincentYu'an YangSiffi SinghSong FengHwanjun SongHang SuLijia SunYi ZhangSaab MansourKathleen McKeown",
        "links": "http://arxiv.org/abs/2402.13249v1",
        "entry_id": "http://arxiv.org/abs/2402.13249v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13249v1",
        "summary": "Single document news summarization has seen substantial progress on\nfaithfulness in recent years, driven by research on the evaluation of factual\nconsistency, or hallucinations. We ask whether these advances carry over to\nother text summarization domains. We propose a new evaluation benchmark on\ntopic-focused dialogue summarization, generated by LLMs of varying sizes. We\nprovide binary sentence-level human annotations of the factual consistency of\nthese summaries along with detailed explanations of factually inconsistent\nsentences. Our analysis shows that existing LLMs hallucinate significant\namounts of factual errors in the dialogue domain, regardless of the model's\nsize. On the other hand, when LLMs, including GPT-4, serve as binary factual\nevaluators, they perform poorly and can be outperformed by prevailing\nstate-of-the-art specialized factuality evaluation metrics. Finally, we\nconducted an analysis of hallucination types with a curated error taxonomy. We\nfind that there are diverse errors and error distributions in model-generated\nsummaries and that non-LLM based metrics can capture all error types better\nthan LLM-based evaluators.",
        "updated": "2024-02-20 18:58:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13249v1"
    },
    {
        "title": "Unlocking Insights: Semantic Search in Jupyter Notebooks",
        "authors": "Lan LiJinpeng Lv",
        "links": "http://arxiv.org/abs/2402.13234v1",
        "entry_id": "http://arxiv.org/abs/2402.13234v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13234v1",
        "summary": "Semantic search, a process aimed at delivering highly relevant search results\nby comprehending the searcher's intent and the contextual meaning of terms\nwithin a searchable dataspace, plays a pivotal role in information retrieval.\nIn this paper, we investigate the application of large language models to\nenhance semantic search capabilities, specifically tailored for the domain of\nJupyter Notebooks. Our objective is to retrieve generated outputs, such as\nfigures or tables, associated functions and methods, and other pertinent\ninformation.\n  We demonstrate a semantic search framework that achieves a comprehensive\nsemantic understanding of the entire notebook's contents, enabling it to\neffectively handle various types of user queries. Key components of this\nframework include:\n  1). A data preprocessor is designed to handle diverse types of cells within\nJupyter Notebooks, encompassing both markdown and code cells. 2). An innovative\nmethodology is devised to address token size limitations that arise with\ncode-type cells. We implement a finer-grained approach to data input,\ntransitioning from the cell level to the function level, effectively resolving\nthese issues.",
        "updated": "2024-02-20 18:49:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13234v1"
    },
    {
        "title": "Investigating Cultural Alignment of Large Language Models",
        "authors": "Badr AlKhamissiMuhammad ElNokrashyMai AlKhamissiMona Diab",
        "links": "http://arxiv.org/abs/2402.13231v1",
        "entry_id": "http://arxiv.org/abs/2402.13231v1",
        "pdf_url": "http://arxiv.org/pdf/2402.13231v1",
        "summary": "The intricate relationship between language and culture has long been a\nsubject of exploration within the realm of linguistic anthropology. Large\nLanguage Models (LLMs), promoted as repositories of collective human knowledge,\nraise a pivotal question: do these models genuinely encapsulate the diverse\nknowledge adopted by different cultures? Our study reveals that these models\ndemonstrate greater cultural alignment along two dimensions -- firstly, when\nprompted with the dominant language of a specific culture, and secondly, when\npretrained with a refined mixture of languages employed by that culture. We\nquantify cultural alignment by simulating sociological surveys, comparing model\nresponses to those of actual survey participants as references. Specifically,\nwe replicate a survey conducted in various regions of Egypt and the United\nStates through prompting LLMs with different pretraining data mixtures in both\nArabic and English with the personas of the real respondents and the survey\nquestions. Further analysis reveals that misalignment becomes more pronounced\nfor underrepresented personas and for culturally sensitive topics, such as\nthose probing social values. Finally, we introduce Anthropological Prompting, a\nnovel method leveraging anthropological reasoning to enhance cultural\nalignment. Our study emphasizes the necessity for a more balanced multilingual\npretraining dataset to better represent the diversity of human experience and\nthe plurality of different cultures with many implications on the topic of\ncross-lingual transfer.",
        "updated": "2024-02-20 18:47:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.13231v1"
    }
]