[
    {
        "title": "CPS-TaskForge: Generating Collaborative Problem Solving Environments for Diverse Communication Tasks",
        "authors": "Nikita HaduongIrene WangBo-Ru LuPrithviraj AmmanabroluNoah A. Smith",
        "links": "http://arxiv.org/abs/2408.08853v1",
        "entry_id": "http://arxiv.org/abs/2408.08853v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08853v1",
        "summary": "Teams can outperform individuals; could adding AI teammates further bolster\nperformance of teams solving problems collaboratively? Collaborative problem\nsolving (CPS) research commonly studies teams with two agents (human-human or\nhuman-AI), but team research literature finds that, for complex tasks, larger\nteams are more effective. Progress in studying collaboration with more than two\nagents, through textual records of team interactions, is hindered by a major\ndata challenge: available CPS corpora are predominantly dyadic, and adapting\npre-existing CPS tasks to more agents is non-trivial. We address this data\nchallenge by developing a CPS task generator, CPS-TaskForge, that can produce\nenvironments for studying CPS under a wide array of conditions, and releasing a\nCPS task design checklist grounded in the theoretical PISA 2015 CPS framework\nto help facilitate the development of CPS corpora with more agents.\nCPS-TaskForge takes the form of a resource management (tower defense) game, and\ndifferent CPS tasks can be studied by manipulating game design parameters. We\nconduct a case study with groups of 3-4 humans to validate production of\ndiverse natural language CPS communication in a game instance produced by\nCPS-TaskForge. We discuss opportunities for advancing research in CPS (both\nwith human-only and human-AI teams) using different task configurations. We\nwill release data and code.",
        "updated": "2024-08-16 17:28:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08853v1"
    },
    {
        "title": "A Transparency Paradox? Investigating the Impact of Explanation Specificity and Autonomous Vehicle Perceptual Inaccuracies on Passengers",
        "authors": "Daniel OmeizaRaunak BhattacharyyaMarina JirotkaNick HawesLars Kunze",
        "links": "http://arxiv.org/abs/2408.08785v1",
        "entry_id": "http://arxiv.org/abs/2408.08785v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08785v1",
        "summary": "Transparency in automated systems could be afforded through the provision of\nintelligible explanations. While transparency is desirable, might it lead to\ncatastrophic outcomes (such as anxiety), that could outweigh its benefits? It's\nquite unclear how the specificity of explanations (level of transparency)\ninfluences recipients, especially in autonomous driving (AD). In this work, we\nexamined the effects of transparency mediated through varying levels of\nexplanation specificity in AD. We first extended a data-driven explainer model\nby adding a rule-based option for explanation generation in AD, and then\nconducted a within-subject lab study with 39 participants in an immersive\ndriving simulator to study the effect of the resulting explanations.\nSpecifically, our investigation focused on: (1) how different types of\nexplanations (specific vs. abstract) affect passengers' perceived safety,\nanxiety, and willingness to take control of the vehicle when the vehicle\nperception system makes erroneous predictions; and (2) the relationship between\npassengers' behavioural cues and their feelings during the autonomous drives.\nOur findings showed that passengers felt safer with specific explanations when\nthe vehicle's perception system had minimal errors, while abstract explanations\nthat hid perception errors led to lower feelings of safety. Anxiety levels\nincreased when specific explanations revealed perception system errors (high\ntransparency). We found no significant link between passengers' visual patterns\nand their anxiety levels. Our study suggests that passengers prefer clear and\nspecific explanations (high transparency) when they originate from autonomous\nvehicles (AVs) with optimal perceptual accuracy.",
        "updated": "2024-08-16 14:59:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08785v1"
    },
    {
        "title": "TextCAVs: Debugging vision models using text",
        "authors": "Angus NicolsonYarin GalJ. Alison Noble",
        "links": "http://arxiv.org/abs/2408.08652v1",
        "entry_id": "http://arxiv.org/abs/2408.08652v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08652v1",
        "summary": "Concept-based interpretability methods are a popular form of explanation for\ndeep learning models which provide explanations in the form of high-level human\ninterpretable concepts. These methods typically find concept activation vectors\n(CAVs) using a probe dataset of concept examples. This requires labelled data\nfor these concepts -- an expensive task in the medical domain. We introduce\nTextCAVs: a novel method which creates CAVs using vision-language models such\nas CLIP, allowing for explanations to be created solely using text descriptions\nof the concept, as opposed to image exemplars. This reduced cost in testing\nconcepts allows for many concepts to be tested and for users to interact with\nthe model, testing new ideas as they are thought of, rather than a delay caused\nby image collection and annotation. In early experimental results, we\ndemonstrate that TextCAVs produces reasonable explanations for a chest x-ray\ndataset (MIMIC-CXR) and natural images (ImageNet), and that these explanations\ncan be used to debug deep learning-based models.",
        "updated": "2024-08-16 10:36:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08652v1"
    },
    {
        "title": "Models Matter: Setting Accurate Privacy Expectations for Local and Central Differential Privacy",
        "authors": "Mary Anne SmartPriyanka NanayakkaraRachel CummingsGabriel KaptchukElissa Redmiles",
        "links": "http://arxiv.org/abs/2408.08475v1",
        "entry_id": "http://arxiv.org/abs/2408.08475v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08475v1",
        "summary": "Differential privacy is a popular privacy-enhancing technology that has been\ndeployed both in industry and government agencies. Unfortunately, existing\nexplanations of differential privacy fail to set accurate privacy expectations\nfor data subjects, which depend on the choice of deployment model. We design\nand evaluate new explanations of differential privacy for the local and central\nmodels, drawing inspiration from prior work explaining other privacy-enhancing\ntechnologies. We find that consequences-focused explanations in the style of\nprivacy nutrition labels that lay out the implications of differential privacy\nare a promising approach for setting accurate privacy expectations. Further, we\nfind that while process-focused explanations are not enough to set accurate\nprivacy expectations, combining consequences-focused explanations with a brief\ndescription of how differential privacy works leads to greater trust.",
        "updated": "2024-08-16 01:21:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08475v1"
    },
    {
        "title": "Voicing Uncertainty: How Speech, Text, and Visualizations Influence Decisions with Data Uncertainty",
        "authors": "Chase StokesChelsea SankerBridget CogleyVidya Setlur",
        "links": "http://arxiv.org/abs/2408.08438v1",
        "entry_id": "http://arxiv.org/abs/2408.08438v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08438v1",
        "summary": "Understanding and communicating data uncertainty is crucial for informed\ndecision-making across various domains, including finance, healthcare, and\npublic policy. This study investigates the impact of gender and acoustic\nvariables on decision-making, confidence, and trust through a crowdsourced\nexperiment. We compared visualization-only representations of uncertainty to\ntext-forward and speech-forward bimodal representations, including multiple\nsynthetic voices across gender. Speech-forward representations led to an\nincrease in risky decisions, and text-forward representations led to lower\nconfidence. Contrary to prior work, speech-forward forecasts did not receive\nhigher ratings of trust. Higher normalized pitch led to a slight increase in\ndecision confidence, but other voice characteristics had minimal impact on\ndecisions and trust. An exploratory analysis of accented speech showed\nconsistent results with the main experiment and additionally indicated lower\ntrust ratings for information presented in Indian and Kenyan accents. The\nresults underscore the importance of considering acoustic and contextual\nfactors in presentation of data uncertainty.",
        "updated": "2024-08-15 22:11:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08438v1"
    }
]