[
    {
        "title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models",
        "authors": "Le XueManli ShuAnas AwadallaJun WangAn YanSenthil PurushwalkamHonglu ZhouViraj PrabhuYutong DaiMichael S RyooShrikant KendreJieyu ZhangCan QinShu ZhangChia-Chih ChenNing YuJuntao TanTulika Manoj AwalgaonkarShelby HeineckeHuan WangYejin ChoiLudwig SchmidtZeyuan ChenSilvio SavareseJuan Carlos NieblesCaiming XiongRan Xu",
        "links": "http://arxiv.org/abs/2408.08872v1",
        "entry_id": "http://arxiv.org/abs/2408.08872v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08872v1",
        "summary": "This report introduces xGen-MM (also known as BLIP-3), a framework for\ndeveloping Large Multimodal Models (LMMs). The framework comprises meticulously\ncurated datasets, a training recipe, model architectures, and a resulting suite\nof LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen\ninitiative on foundation AI models. Our models undergo rigorous evaluation\nacross a range of tasks, including both single and multi-image benchmarks. Our\npre-trained base model exhibits strong in-context learning capabilities and the\ninstruction-tuned model demonstrates competitive performance among open-source\nLMMs with similar model sizes. In addition, we introduce a safety-tuned model\nwith DPO, aiming to mitigate harmful behaviors such as hallucinations and\nimprove safety. We open-source our models, curated large-scale datasets, and\nour fine-tuning codebase to facilitate further advancements in LMM research.\nAssociated resources will be available on our project page above.",
        "updated": "2024-08-16 17:57:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08872v1"
    },
    {
        "title": "SAM2-UNet: Segment Anything 2 Makes Strong Encoder for Natural and Medical Image Segmentation",
        "authors": "Xinyu XiongZihuang WuShuangyi TanWenxue LiFeilong TangYing ChenSiying LiJie MaGuanbin Li",
        "links": "http://arxiv.org/abs/2408.08870v1",
        "entry_id": "http://arxiv.org/abs/2408.08870v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08870v1",
        "summary": "Image segmentation plays an important role in vision understanding. Recently,\nthe emerging vision foundation models continuously achieved superior\nperformance on various tasks. Following such success, in this paper, we prove\nthat the Segment Anything Model 2 (SAM2) can be a strong encoder for U-shaped\nsegmentation models. We propose a simple but effective framework, termed\nSAM2-UNet, for versatile image segmentation. Specifically, SAM2-UNet adopts the\nHiera backbone of SAM2 as the encoder, while the decoder uses the classic\nU-shaped design. Additionally, adapters are inserted into the encoder to allow\nparameter-efficient fine-tuning. Preliminary experiments on various downstream\ntasks, such as camouflaged object detection, salient object detection, marine\nanimal segmentation, mirror detection, and polyp segmentation, demonstrate that\nour SAM2-UNet can simply beat existing specialized state-of-the-art methods\nwithout bells and whistles. Project page:\n\\url{https://github.com/WZH0120/SAM2-UNet}.",
        "updated": "2024-08-16 17:55:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08870v1"
    },
    {
        "title": "DPA: Dual Prototypes Alignment for Unsupervised Adaptation of Vision-Language Models",
        "authors": "Eman AliSathira SilvaMuhammad Haris Khan",
        "links": "http://arxiv.org/abs/2408.08855v1",
        "entry_id": "http://arxiv.org/abs/2408.08855v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08855v1",
        "summary": "Vision-language models (VLMs), e.g., CLIP, have shown remarkable potential in\nzero-shot image classification. However, adapting these models to new domains\nremains challenging, especially in unsupervised settings where labelled data is\nunavailable. Recent research has proposed pseudo-labelling approaches to adapt\nCLIP in an unsupervised manner using unlabelled target data. Nonetheless, these\nmethods struggle due to noisy pseudo-labels resulting from the misalignment\nbetween CLIP's visual and textual representations. This study introduces DPA,\nan unsupervised domain adaptation method for VLMs. DPA introduces the concept\nof dual prototypes, acting as distinct classifiers, along with the convex\ncombination of their outputs, thereby leading to accurate pseudo-label\nconstruction. Next, it ranks pseudo-labels to facilitate robust self-training,\nparticularly during early training. Finally, it addresses visual-textual\nmisalignment by aligning textual prototypes with image prototypes to further\nimprove the adaptation performance. Experiments on 13 downstream vision tasks\ndemonstrate that DPA significantly outperforms zero-shot CLIP and the\nstate-of-the-art unsupervised adaptation baselines.",
        "updated": "2024-08-16 17:30:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08855v1"
    },
    {
        "title": "HistoGym: A Reinforcement Learning Environment for Histopathological Image Analysis",
        "authors": "Zhi-Bo LiuXiaobo PangJizhao WangShuai LiuChen Li",
        "links": "http://arxiv.org/abs/2408.08847v1",
        "entry_id": "http://arxiv.org/abs/2408.08847v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08847v1",
        "summary": "In pathological research, education, and clinical practice, the\ndecision-making process based on pathological images is critically important.\nThis significance extends to digital pathology image analysis: its adequacy is\ndemonstrated by the extensive information contained within tissue structures,\nwhich is essential for accurate cancer classification and grading.\nAdditionally, its necessity is highlighted by the inherent requirement for\ninterpretability in the conclusions generated by algorithms. For humans,\ndetermining tumor type and grade typically involves multi-scale analysis, which\npresents a significant challenge for AI algorithms. Traditional patch-based\nmethods are inadequate for modeling such complex structures, as they fail to\ncapture the intricate, multi-scale information inherent in whole slide images.\nConsequently, there is a pressing need for advanced AI techniques capable of\nefficiently and accurately replicating this complex analytical process. To\naddress this issue, we introduce HistoGym, an open-source reinforcement\nlearning environment for histopathological image analysis. Following OpenAI Gym\nAPIs, HistoGym aims to foster whole slide image diagnosis by mimicking the\nreal-life processes of doctors. Leveraging the pyramid feature of WSIs and the\nOpenSlide API, HistoGym provides a unified framework for various clinical\ntasks, including tumor detection and classification. We detail the observation,\naction, and reward specifications tailored for the histopathological image\nanalysis domain and provide an open-source Python-based interface for both\nclinicians and researchers. To accommodate different clinical demands, we offer\nvarious scenarios for different organs and cancers, including both WSI-based\nand selected region-based scenarios, showcasing several noteworthy results.",
        "updated": "2024-08-16 17:19:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08847v1"
    },
    {
        "title": "RGBT Tracking via All-layer Multimodal Interactions with Progressive Fusion Mamba",
        "authors": "Andong LuWanyu WangChenglong LiJin TangBin Luo",
        "links": "http://arxiv.org/abs/2408.08827v1",
        "entry_id": "http://arxiv.org/abs/2408.08827v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08827v1",
        "summary": "Existing RGBT tracking methods often design various interaction models to\nperform cross-modal fusion of each layer, but can not execute the feature\ninteractions among all layers, which plays a critical role in robust multimodal\nrepresentation, due to large computational burden. To address this issue, this\npaper presents a novel All-layer multimodal Interaction Network, named AINet,\nwhich performs efficient and effective feature interactions of all modalities\nand layers in a progressive fusion Mamba, for robust RGBT tracking. Even though\nmodality features in different layers are known to contain different cues, it\nis always challenging to build multimodal interactions in each layer due to\nstruggling in balancing interaction capabilities and efficiency. Meanwhile,\nconsidering that the feature discrepancy between RGB and thermal modalities\nreflects their complementary information to some extent, we design a\nDifference-based Fusion Mamba (DFM) to achieve enhanced fusion of different\nmodalities with linear complexity. When interacting with features from all\nlayers, a huge number of token sequences (3840 tokens in this work) are\ninvolved and the computational burden is thus large. To handle this problem, we\ndesign an Order-dynamic Fusion Mamba (OFM) to execute efficient and effective\nfeature interactions of all layers by dynamically adjusting the scan order of\ndifferent layers in Mamba. Extensive experiments on four public RGBT tracking\ndatasets show that AINet achieves leading performance against existing\nstate-of-the-art methods.",
        "updated": "2024-08-16 16:22:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08827v1"
    }
]