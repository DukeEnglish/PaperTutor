[
    {
        "title": "Accelerating Giant Impact Simulations with Machine Learning",
        "authors": "Caleb LammersMiles CranmerSam HaddenShirley HoNorman MurrayDaniel Tamayo",
        "links": "http://arxiv.org/abs/2408.08873v1",
        "entry_id": "http://arxiv.org/abs/2408.08873v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08873v1",
        "summary": "Constraining planet formation models based on the observed exoplanet\npopulation requires generating large samples of synthetic planetary systems,\nwhich can be computationally prohibitive. A significant bottleneck is\nsimulating the giant impact phase, during which planetary embryos evolve\ngravitationally and combine to form planets, which may themselves experience\nlater collisions. To accelerate giant impact simulations, we present a machine\nlearning (ML) approach to predicting collisional outcomes in multiplanet\nsystems. Trained on more than 500,000 $N$-body simulations of three-planet\nsystems, we develop an ML model that can accurately predict which two planets\nwill experience a collision, along with the state of the post-collision\nplanets, from a short integration of the system's initial conditions. Our model\ngreatly improves on non-ML baselines that rely on metrics from dynamics theory,\nwhich struggle to accurately predict which pair of planets will experience a\ncollision. By combining with a model for predicting long-term stability, we\ncreate an efficient ML-based giant impact emulator, which can predict the\noutcomes of giant impact simulations with a speedup of up to four orders of\nmagnitude. We expect our model to enable analyses that would not otherwise be\ncomputationally feasible. As such, we release our full training code, along\nwith an easy-to-use API for our collision outcome model and giant impact\nemulator.",
        "updated": "2024-08-16 17:59:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08873v1"
    },
    {
        "title": "PEDAL: Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars",
        "authors": "Sumanth Prabhu",
        "links": "http://arxiv.org/abs/2408.08869v1",
        "entry_id": "http://arxiv.org/abs/2408.08869v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08869v1",
        "summary": "Self-ensembling techniques with diverse reasoning paths such as\nSelf-Consistency have demonstrated remarkable gains in accuracy for Large\nLanguage Models (LLMs). However, such techniques depend on the availability of\nan accurate answer extraction process to aggregate across multiple outputs.\nMoreover, they acquire higher inference cost, in comparison to Greedy Decoding,\ndue to generation of relatively higher number of output tokens. Research has\nshown that the free form text outputs from Self-Consistency can be aggregated\nreliably using LLMs to produce the final output. Additionally, recent\nadvancements in LLM inference have demonstrated that usage of diverse exemplars\nin prompts have the ability to induce diversity in the LLM outputs. Such proven\ntechniques can be easily extended to self-ensembling based approaches to\nachieve enhanced results in text generation. In this paper, we introduce PEDAL\n(Prompts based on Exemplar Diversity Aggregated using LLMs), a hybrid\nself-ensembling approach, that combines the strengths of diverse exemplar based\nprompts and LLM based aggregation to achieve improvement in overall\nperformance. On the publicly available SVAMP and ARC datasets, our experiments\nreveal that PEDAL can achieve better accuracy than Greedy Decoding based\nstrategies with lower inference cost compared to Self Consistency based\napproaches.",
        "updated": "2024-08-16 17:54:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08869v1"
    },
    {
        "title": "A Hassle-free Algorithm for Private Learning in Practice: Don't Use Tree Aggregation, Use BLTs",
        "authors": "H. Brendan McMahanZheng XuYanxiang Zhang",
        "links": "http://arxiv.org/abs/2408.08868v1",
        "entry_id": "http://arxiv.org/abs/2408.08868v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08868v1",
        "summary": "The state-of-the-art for training on-device language models for mobile\nkeyboard applications combines federated learning (FL) with differential\nprivacy (DP) via the DP-Follow-the-Regularized-Leader (DP-FTRL) algorithm. Two\nvariants of DP-FTRL are used in practice, tree aggregation and matrix\nfactorization. However, tree aggregation suffers from significantly suboptimal\nprivacy/utility tradeoffs, while matrix mechanisms require expensive\noptimization parameterized by hard-to-estimate-in-advance constants, and high\nruntime memory costs.This paper extends the recently introduced Buffered Linear\nToeplitz (BLT) mechanism to multi-participation scenarios. Our BLT-DP-FTRL\nmaintains the ease-of-use advantages of tree aggregation, while essentially\nmatching matrix factorization in terms of utility and privacy. We evaluate\nBLT-DP-FTRL on the StackOverflow dataset, serving as a re-producible simulation\nbenchmark, and across four on-device language model tasks in a production FL\nsystem. Our empirical results highlight the advantages of the BLT mechanism and\nelevate the practicality and effectiveness of DP in real-world scenarios.",
        "updated": "2024-08-16 17:52:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08868v1"
    },
    {
        "title": "Visual Agents as Fast and Slow Thinkers",
        "authors": "Guangyan SunMingyu JinZhenting WangCheng-Long WangSiqi MaQifan WangYing Nian WuYongfeng ZhangDongfang Liu",
        "links": "http://arxiv.org/abs/2408.08862v1",
        "entry_id": "http://arxiv.org/abs/2408.08862v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08862v1",
        "summary": "Achieving human-level intelligence requires refining cognitive distinctions\nbetween System 1 and System 2 thinking. While contemporary AI, driven by large\nlanguage models, demonstrates human-like traits, it falls short of genuine\ncognition. Transitioning from structured benchmarks to real-world scenarios\npresents challenges for visual agents, often leading to inaccurate and overly\nconfident responses. To address the challenge, we introduce FaST, which\nincorporates the Fast and Slow Thinking mechanism into visual agents. FaST\nemploys a switch adapter to dynamically select between System 1/2 modes,\ntailoring the problem-solving approach to different task complexity. It tackles\nuncertain and unseen objects by adjusting model confidence and integrating new\ncontextual data. With this novel design, we advocate a flexible system,\nhierarchical reasoning capabilities, and a transparent decision-making\npipeline, all of which contribute to its ability to emulate human-like\ncognitive processes in visual intelligence. Empirical results demonstrate that\nFaST outperforms various well-known baselines, achieving 80.8% accuracy over\nVQA^{v2} for visual question answering and 48.7% GIoU score over ReasonSeg for\nreasoning segmentation, demonstrate FaST's superior performance. Extensive\ntesting validates the efficacy and robustness of FaST's core components,\nshowcasing its potential to advance the development of cognitive visual agents\nin AI systems.",
        "updated": "2024-08-16 17:44:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08862v1"
    },
    {
        "title": "Stochastic Bandits Robust to Adversarial Attacks",
        "authors": "Xuchuang WangJinhang ZuoXutong LiuJohn C. S. LuiMohammad Hajiesmaili",
        "links": "http://arxiv.org/abs/2408.08859v1",
        "entry_id": "http://arxiv.org/abs/2408.08859v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08859v1",
        "summary": "This paper investigates stochastic multi-armed bandit algorithms that are\nrobust to adversarial attacks, where an attacker can first observe the\nlearner's action and {then} alter their reward observation. We study two cases\nof this model, with or without the knowledge of an attack budget $C$, defined\nas an upper bound of the summation of the difference between the actual and\naltered rewards. For both cases, we devise two types of algorithms with regret\nbounds having additive or multiplicative $C$ dependence terms. For the known\nattack budget case, we prove our algorithms achieve the regret bound of\n${O}((K/\\Delta)\\log T + KC)$ and $\\tilde{O}(\\sqrt{KTC})$ for the additive and\nmultiplicative $C$ terms, respectively, where $K$ is the number of arms, $T$ is\nthe time horizon, $\\Delta$ is the gap between the expected rewards of the\noptimal arm and the second-best arm, and $\\tilde{O}$ hides the logarithmic\nfactors. For the unknown case, we prove our algorithms achieve the regret bound\nof $\\tilde{O}(\\sqrt{KT} + KC^2)$ and $\\tilde{O}(KC\\sqrt{T})$ for the additive\nand multiplicative $C$ terms, respectively. In addition to these upper bound\nresults, we provide several lower bounds showing the tightness of our bounds\nand the optimality of our algorithms. These results delineate an intrinsic\nseparation between the bandits with attacks and corruption models [Lykouris et\nal., 2018].",
        "updated": "2024-08-16 17:41:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08859v1"
    }
]