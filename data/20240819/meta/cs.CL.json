[
    {
        "title": "xGen-MM (BLIP-3): A Family of Open Large Multimodal Models",
        "authors": "Le XueManli ShuAnas AwadallaJun WangAn YanSenthil PurushwalkamHonglu ZhouViraj PrabhuYutong DaiMichael S RyooShrikant KendreJieyu ZhangCan QinShu ZhangChia-Chih ChenNing YuJuntao TanTulika Manoj AwalgaonkarShelby HeineckeHuan WangYejin ChoiLudwig SchmidtZeyuan ChenSilvio SavareseJuan Carlos NieblesCaiming XiongRan Xu",
        "links": "http://arxiv.org/abs/2408.08872v1",
        "entry_id": "http://arxiv.org/abs/2408.08872v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08872v1",
        "summary": "This report introduces xGen-MM (also known as BLIP-3), a framework for\ndeveloping Large Multimodal Models (LMMs). The framework comprises meticulously\ncurated datasets, a training recipe, model architectures, and a resulting suite\nof LMMs. xGen-MM, short for xGen-MultiModal, expands the Salesforce xGen\ninitiative on foundation AI models. Our models undergo rigorous evaluation\nacross a range of tasks, including both single and multi-image benchmarks. Our\npre-trained base model exhibits strong in-context learning capabilities and the\ninstruction-tuned model demonstrates competitive performance among open-source\nLMMs with similar model sizes. In addition, we introduce a safety-tuned model\nwith DPO, aiming to mitigate harmful behaviors such as hallucinations and\nimprove safety. We open-source our models, curated large-scale datasets, and\nour fine-tuning codebase to facilitate further advancements in LMM research.\nAssociated resources will be available on our project page above.",
        "updated": "2024-08-16 17:57:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08872v1"
    },
    {
        "title": "PEDAL: Enhancing Greedy Decoding with Large Language Models using Diverse Exemplars",
        "authors": "Sumanth Prabhu",
        "links": "http://arxiv.org/abs/2408.08869v1",
        "entry_id": "http://arxiv.org/abs/2408.08869v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08869v1",
        "summary": "Self-ensembling techniques with diverse reasoning paths such as\nSelf-Consistency have demonstrated remarkable gains in accuracy for Large\nLanguage Models (LLMs). However, such techniques depend on the availability of\nan accurate answer extraction process to aggregate across multiple outputs.\nMoreover, they acquire higher inference cost, in comparison to Greedy Decoding,\ndue to generation of relatively higher number of output tokens. Research has\nshown that the free form text outputs from Self-Consistency can be aggregated\nreliably using LLMs to produce the final output. Additionally, recent\nadvancements in LLM inference have demonstrated that usage of diverse exemplars\nin prompts have the ability to induce diversity in the LLM outputs. Such proven\ntechniques can be easily extended to self-ensembling based approaches to\nachieve enhanced results in text generation. In this paper, we introduce PEDAL\n(Prompts based on Exemplar Diversity Aggregated using LLMs), a hybrid\nself-ensembling approach, that combines the strengths of diverse exemplar based\nprompts and LLM based aggregation to achieve improvement in overall\nperformance. On the publicly available SVAMP and ARC datasets, our experiments\nreveal that PEDAL can achieve better accuracy than Greedy Decoding based\nstrategies with lower inference cost compared to Self Consistency based\napproaches.",
        "updated": "2024-08-16 17:54:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08869v1"
    },
    {
        "title": "PsychoLex: Unveiling the Psychological Mind of Large Language Models",
        "authors": "Mohammad Amin AbbasiFarnaz Sadat MirnezamiHassan Naderi",
        "links": "http://arxiv.org/abs/2408.08848v1",
        "entry_id": "http://arxiv.org/abs/2408.08848v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08848v1",
        "summary": "This paper explores the intersection of psychology and artificial\nintelligence through the development and evaluation of specialized Large\nLanguage Models (LLMs). We introduce PsychoLex, a suite of resources designed\nto enhance LLMs' proficiency in psychological tasks in both Persian and\nEnglish. Key contributions include the PsychoLexQA dataset for instructional\ncontent and the PsychoLexEval dataset for rigorous evaluation of LLMs in\ncomplex psychological scenarios. Additionally, we present the PsychoLexLLaMA\nmodel, optimized specifically for psychological applications, demonstrating\nsuperior performance compared to general-purpose models. The findings\nunderscore the potential of tailored LLMs for advancing psychological research\nand applications, while also highlighting areas for further refinement. This\nresearch offers a foundational step towards integrating LLMs into specialized\npsychological domains, with implications for future advancements in AI-driven\npsychological practice.",
        "updated": "2024-08-16 17:19:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08848v1"
    },
    {
        "title": "FLEXTAF: Enhancing Table Reasoning with Flexible Tabular Formats",
        "authors": "Xuanliang ZhangDingzirui WangLongxu DouBaoxin WangDayong WuQingfu ZhuWanxiang Che",
        "links": "http://arxiv.org/abs/2408.08841v1",
        "entry_id": "http://arxiv.org/abs/2408.08841v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08841v1",
        "summary": "The table reasoning task aims to answer the question according to the given\ntable. Currently, using Large Language Models (LLMs) is the predominant method\nfor table reasoning. Most existing methods employ a fixed tabular format to\nrepresent the table, which could limit the performance. Given that each\ninstance requires different capabilities and models possess varying abilities,\nwe assert that different instances and models suit different tabular formats.\nWe prove the aforementioned claim through quantitative analysis of experimental\nresults, where different instances and models achieve different performances\nusing various tabular formats. Building on this discussion, we propose\nFLEXTAF-Single and FLEXTAF-Vote to enhance table reasoning performance by\nemploying flexible tabular formats. Specifically, (i) FLEXTAF-Single trains a\nclassifier to predict the most suitable tabular format based on the instance\nand the LLM. (ii) FLEXTAF-Vote integrates the results across different formats.\nOur experiments on WikiTableQuestions and TabFact reveal significant\nimprovements, with average gains of 2.3% and 4.8% compared to the best\nperformance achieved using a fixed tabular format with greedy decoding and\nself-consistency decoding, thereby validating the effectiveness of our methods.",
        "updated": "2024-08-16 17:00:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08841v1"
    },
    {
        "title": "CIKMar: A Dual-Encoder Approach to Prompt-Based Reranking in Educational Dialogue Systems",
        "authors": "Joanito Agili LopoMarina Indah PrasastiAlma Permatasari",
        "links": "http://arxiv.org/abs/2408.08805v1",
        "entry_id": "http://arxiv.org/abs/2408.08805v1",
        "pdf_url": "http://arxiv.org/pdf/2408.08805v1",
        "summary": "In this study, we introduce CIKMar, an efficient approach to educational\ndialogue systems powered by the Gemma Language model. By leveraging a\nDual-Encoder ranking system that incorporates both BERT and SBERT model, we\nhave designed CIKMar to deliver highly relevant and accurate responses, even\nwith the constraints of a smaller language model size. Our evaluation reveals\nthat CIKMar achieves a robust recall and F1-score of 0.70 using BERTScore\nmetrics. However, we have identified a significant challenge: the Dual-Encoder\ntends to prioritize theoretical responses over practical ones. These findings\nunderscore the potential of compact and efficient models like Gemma in\ndemocratizing access to advanced educational AI systems, ensuring effective and\ncontextually appropriate responses.",
        "updated": "2024-08-16 15:29:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.08805v1"
    }
]