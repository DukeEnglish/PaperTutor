DPA: Dual Prototypes Alignment for Unsupervised
Adaptation of Vision-Language Models
EmanAli
MohamedBinZayedUniversityofArtificialIntelligence
AbuDhabi,UAE
eman.ali@mbzuai.ac.ae
SathiraSilva
MohamedBinZayedUniversityofArtificialIntelligence
AbuDhabi,UAE
sathira.silva@mbzuai.ac.ae
MuhammadHarisKhan
MohamedBinZayedUniversityofArtificialIntelligence
AbuDhabi,UAE
muhammad.haris@mbzuai.ac.ae
Abstract
Vision-languagemodels(VLMs),e.g.,CLIP,haveshownremarkablepotentialin
zero-shotimageclassification. However,adaptingthesemodelstonewdomains
remains challenging, especially in unsupervised settings where labelled data is
unavailable. Recentresearchhasproposedpseudo-labellingapproachestoadapt
CLIPinanunsupervisedmannerusingunlabelledtargetdata. Nonetheless,these
methods struggle due to noisy pseudo-labels resulting from the misalignment
betweenCLIP’svisualandtextualrepresentations. ThisstudyintroducesDPA,an
unsuperviseddomainadaptationmethodforVLMs. DPAintroducestheconceptof
dualprototypes,actingasdistinctclassifiers,alongwiththeconvexcombinationof
theiroutputs,therebyleadingtoaccuratepseudo-labelconstruction. Next,itranks
pseudo-labelstofacilitaterobustself-training,particularlyduringearlytraining.
Finally, it addresses visual-textual misalignment by aligning textual prototypes
withimageprototypestofurtherimprovetheadaptationperformance. Experiments
on13downstreamvisiontasksdemonstratethatDPAsignificantlyoutperforms
zero-shotCLIPandthestate-of-the-artunsupervisedadaptationbaselines.
1 Introduction
Contextandbackground: Vision-languagemodels(VLMs)[1–4]haveshownpromisingpotential
inzero-shotimageclassification. OnenotableexampleisCLIP[1]. Despitetheimpressivezero-shot
capabilitiesofCLIP,itsperformancecanbeimpactedbythediscrepancybetweenthepretraining
image-textpairsandthedownstreamtaskimages[1,5]. Toaddressthislimitation,severalstudies
haveattemptedtoenhanceCLIPtransferperformanceondownstreamtasksbyleveraginglimited
labelledsamplesfromthetargetdomains[6–10]. However,inmanypracticalapplications,suchas
securityandmedicaldiagnostics,collectinglabelledsamplescanbeparticularlychallengingdueto
highlabellingcostsanddataprivacyconcerns. Insuchcontexts,unsupervisedlearningpresentsa
promisingalternative. SeveralcontributionsforadaptingCLIPtoatargetdomainusinganunlabelled
datasethavebeenintroducedrecently[11–16]. AcommonapproachinvolvesleveragingCLIPto
Preprint.Underreview.
4202
guA
61
]VC.sc[
1v55880.8042:viXrageneratepseudo-labels,whicharethenusedtofine-tuneCLIPinanunsupervisedmanner. However,
this method encounters significant challenges due to the noisy pseudo-labels generated by CLIP.
DespiteeffortsbyrecentmethodsfortheunsupervisedadaptationofCLIP,asignificantmodality
gappersistsbetweenthetextandvisionrepresentations[16,17],leadingtoinaccuratepseudo-labels
causingconfirmationbiasduringadaptation.
Motivation: RecentstudieshighlightasignificantfactorbehindtheperformanceissuesofCLIP
in unsupervised adaptation scenarios: the visual domain gap between the source images used to
trainCLIPandtargetimagestypicallyoccurswhenthetargetsamplesoriginatefromanuncommon
domain [16]. As shown in Figure 1-(a), the t-SNE projection of visual and textual embeddings
on the EuroSAT dataset [18] reveals a significant misalignment, resulting in misclassifications.
Contemporarymethods(e.g.,[16])attempttoaddressthismisalignmentbylearningaprojection
spacetomitigatetheissueandusinglabelpropagation[19]toimprovepseudo-labels. However,they
couldbelimitedastheyareeithercomputationallyexpensiveand/orshowsuboptimalperformance
inaninductivesetting. Further,ondatasetswithalargenumberofclasses,theystruggletoperform
well,necessitatingturningofflabelpropagationandusingmodelpredictions. Effectivelyaddressing
thechallengesofunsupervisedadaptationofCLIPinaninductivesettingiscrucialforimproving
CLIP’sperformanceinunsupervisedadaptationscenarios.
Ourproposal: Toaddressthesechallenges,wepro-
poseDPA,anovelunsupervisedadaptationmethod
forVLMs(Figure2). DPAaimsataddressingthe
domaingapbetweenvisualandtextualrepresenta-
tionsindownstreamtasks. DPAintroducestheidea
of dual prototypes, namely image and textual pro-
totypes, which act as distinct classifiers and their
outputsarefusedviaconvexcombinationtowards
generating accurate pseudo-labels. There are two
mainreasonsforintroducingdualprototypes. Firstly,
themisalignmentbetweentheimagerepresentation
anditstextualrepresentationinzero-shotCLIP,due
to domain shift, often leads to inaccurate pseudo-
labels(seeFigure1-(a)).Secondly,imageprototypes,
Figure 1: Comparison of t-SNE projections
whichtendtobelessaffectedbynoise,asillustrated
for zero-shot CLIP [1], ReCLIP [16], and
inFigure1,aregenerallyclosertothetrueimagerep-
DPAvisualembeddings,alongwiththeircor-
resentationthantextualprototypes. Additionally,we
respondingvisual(circle )andtextual(star
tacklethechallengeofmisalignmentbetweenvisual
⋆) prototypes on EuroSAT dataset. The vi-
and textual embeddings by aligning textual proto- (cid:32)
sualprototypesarecomputedasthemeanof
typeswithimageprototypes. Thisalignmentprocess
eachcluster. Forthepurposeofenhancingthe
further enhances the performance of unsupervised
clarityofvisualizations,priortoapplyingthet-
CLIPadaptation.
SNEprojection,class-agnosticandredundant
Contributions: 1)WeproposeDPA,anovelunsu- featuresareremovedfromalltheembeddings
pervised domain adaptation framework for VLMs usingafixedprojectionfollowing[16]. The
whichallowsadaptingthesepowerfulmodelstonew cosinesimilaritiesbetweenvisualandtextual
domainswithoutrequiringlabelleddatafromthetar- prototypesandthewell-separatedvisualclus-
getdistribution.2)Weintroduceanovelapproachfor ters,asillustratedin(c),demonstratethesupe-
generatingaccuratepseudo-labelsbyleveragingtwo riorperformanceofourmethodincapturing
distinctprototypesandfusingtheiroutputsviaacon- bothinter-modalandintra-modalalignment
vexcombination. Wealsoproposetorankpseudo- comparedtoexistingapproachesdepictedin
labelsintheclassificationlosstomitigatetheirnoise, (a)and(b).
especiallyduringearlytraining. Moreover,wepro-
pose to tackle the visual-textual misalignment by
aligningtextualprototypeswithimageprototypes. 3)Experimentson13downstreamvisiontasks
demonstrate consistent and significant performance enhancements over zero-shot CLIP and the
state-of-the-artbaselines.
2Figure2: TheoverallframeworkofDPA.(a)Givenatargetdataset,DPAutilizesasetofcarefully
designedpromptstoinitializethetextualprototypesusingtheCLIP’szero-shottextualencoderE .
t
(b)Toachieveeffectiveself-training,DPAintroducesdualprototypes,namelyimageandtextual
prototypes,thatbehaveliketwodistinctclassifiers,anditfusestheiroutputsviaconvexcombination
toformaccuratepseudo-labels(PLs). Moreover,itranksPLsforclassificationlosstoalleviatenoisy
PLsimpactduringearlyself-training. Finally,DPAalignstextualandvisualprototypestoadeptly
adjusttothetargetfeaturesemanticrelationslearnedbythevisualencoder. (c)Duringinference,
DPAdiscardsthevisualprototypesandreliessolelyonthetextualprototypesforprediction.
2 RelatedWork
Large-scaleVision-Language(VL)Models: Amongothervision-languagemodels[1–3,20–23],
CLIP[1]standsoutasapioneeringexample,aligningvisualandtextualfeaturesviaacontrastive
objective on a vast web-crawled collection of image-text pairs. This alignment empowers CLIP
to generalize to diverse downstream classification tasks effectively. Building on CLIP’s success,
subsequentresearchhasexploredmethodsforefficientlytransferringthepre-trainedmodeltohandle
diversedownstreamtaskswithlimitedlabelledtargetdata. Theseapproachesoftenleveragevision-
specific adapters [6,24–26], language-specific adapters [7,8,10], or both [9]. However, these
techniquesrequireaminimumnumberoflabelledsamples,posingchallengesduetodatascarcity,
annotationexpenses,privacyissues,andpracticallimitations. Recentadvancementshaveseenthe
emergenceofunsupervisedadaptationtechniques,focusingontailoringCLIPtotargettasksusing
unlabelleddatasets[11–16]. However,thesemethodsstilldependonadditionalsupervisionsignals,
suchasfine-tuningclassifierswithalargelanguagemodellikeGPT-3,asseeninLaFTer[12]. They
alsooftenrequiresignificantcomputationalresources,exemplifiedbyMUST[11]. ReCLIP[16]
tackle misaligned embeddings through source-free domain learning, utilizing a projection space
toaligntheseembeddingsandemployingpseudo-labelsforself-training. Despiteprogress,there
remainssignificantpotentialforimprovingtheperformanceofunsupervisedadaptationtechniques
acrossvariousimageclassificationbenchmarks. Incontrast,ourworkintroducesafullylabel-free
approachforadaptingVLMstoatargettaskbygeneratingmoreaccuratepseudo-labelsgroundedon
imageandtextprototypes.
Prototype-based Learning: Prototype-based learning uses a fixed set of distinctive prototypes
representingthedata,andthenthelearningisachievedbycomparingthetestsamplesdirectlywith
theseprototypes. Prototype-basedlearninghasbeenextensivelystudiedinvariouscontexts,including
few-shot classification [27,28], unsupervised learning [29,30] and supervised classification [31,
32]. Ourmethodinnovativelyincorporatestwotypesofprototypes,imageprototypesandtextual
3prototypes,toharnessbothprototypesforultimatelygeneratingaccuratepseudo-labelsandeffectively
adaptingCLIPtothetargetdataset.
Pseudo-labelling: The success of pseudo-labelling has been demonstrated in various domains,
includingvision[33,34]andvision-languagetasks[11,12,14,16]. Thepseudo-labellingpipelineof
DPAleveragesconsistencyregularization[35]toproduceconsistentpredictionsacrossdifferentdata
augmentations,therebyenhancingtheaccuracyofpseudo-labels.
3 Methodology
Thisstudyaddressesthetaskofunsupervisedadaptationofvision-languagemodels(i.e.,CLIP)for
imageclassification. Followingarethesettingsforthistask:
• Pre-trainedCLIPconsistsofvisualencoderE andtextualencoderE .
v t
• TargetdatasetD ={X }consistsofunlabelledimages{x }N ,wherex ∈X .
t t i i=1 i t
• UniqueclassnamesC ={c }C fortheunlabelledtargetdataset.
j j=1
Weuseapre-trainedCLIPtoprocesstheunlabelledtargetdataD . Initially,thesourcemodelassigns
t
a pseudo-label to each unlabelled target image x . During the adaptation phase, we employ two
i
prototypestogeneratepseudo-labelsfortheadaptationofCLIPtothetargetdomain.
3.1 Zero-shotVisualClassificationinCLIP
CLIPachievesimpressivezero-shotperformanceonimageclassificationtasksbylearningajoint
embeddingspacethatalignsimageandtextrepresentations. Duringpre-training,CLIPminimizesa
symmetriccontrastivelossbetweensemanticallysimilarimage-textpairs(xs,ts)n sampledfroma
i i i=1
large-scalesourcedatasetD :
s
(cid:88)
−log
exp(f isT ·z i/τ)
−log
exp(z iT ·f is/τ)
, (1)
(cid:80)mexp(fsT ·z /τ) (cid:80)mexp(zT ·fs/τ)
i j i j j j i
where fs = E (xs) and z = E (ts), with both fs ∈ R1×d and z ∈ R1×d. The parameter τ
i v i i t i i i
represents a learned temperature, while m indicates the mini-batch size. For inference, given an
unlabelledtargetimagexandalistofclassnamesC,weleveragethepre-trainedtextualencoderE
t
togeneratetextembeddingsforeachclass. Specifically,aseriesofwell-craftedpromptsarefedinto
E ,producingasetoftextembeddingsforeachclass,denotedasZwhereZ∈RC×d. Thezero-shot
t
predictionyˆisthenobtainedasfollows:
yˆ=argmax(f ·ZT) (2)
c
3.2 ProposedFramework(DPA)
Motivation: Whilezero-shotCLIPshowsimpressiveperformanceacrossdiversedomains,itstill
exhibits notable limitations compared to models adapted under supervised conditions due to the
misalignmentbetweenimageandcorrespondingtextualrepresentations[1]. AsdepictedinFigure1-
(a),thisdiscrepancyisevidentinCLIPwhenappliedtouncommondomains.Suchdisparitiescanlead
toinaccuratepseudo-labels,especiallywhenutilizingzero-shotCLIPtogeneratepseudo-labelsforthe
unsupervisedadaptation[16]. OurobjectiveistoadaptCLIPtothetargetdomainusingitsunlabelled
samples by bridging the gap between image representations and their textual representations, all
withoutdependenceonground-truthlabels. Simultaneously,weseektoachieveefficientparameter-
basedunsupervisedadaptationforCLIP.Ourobjectivereliesontwokeycomponents: (1)ensuring
the accuracy of the pseudo-labels generated for the unlabelled samples, and (2) aligning textual
representations with their corresponding visual representations. To generate the pseudo-labels
foradapting CLIPto thetargetdomain, we introducedual prototypeswhich actas twodifferent
classifiersandfusetheircomplementaryoutputsviaconvexcombination. Furthermore,wepropose
visual-textualprototypealignmenttowardsfurtherimprovingtheadaptationperformance.
TextualPrototypesConstruction: Tocreatetextualprototypes,weobtainthetextualrepresentation
for each class in the target domain. This is achieved by incorporating each class name c into
j
4a predefined prompt template. It is then processed by CLIP’s text encoder to produce a textual
representationz . Weutilizekdistinctpromptsforeachclass,resultinginkcorrespondingtextual
j
representations z ∈ Rk×d, where d is the dimensionality of the embedding space. The textual
j
prototypeZ forclassc isconstructedbyaveragingthesekindividualtextualrepresentations:
j j
k
1 (cid:88)
Z = z (3)
j k ji
i=1
whereZ ∈R1×d. Finally,wedefinethetextualprototypesforthesetofC classesasZ∈RC×d.
j
Thisapproachenablesustocaptureacomprehensiveandrobustrepresentationofclasssemanticsby
consolidatingtextualrepresentationsderivedfrommultipleprompts. Finally,thesetextualprototypes
act as the initialization for a parametric classifier that is updated during training alongside CLIP
parameters.
ImagePrototypesConstruction:Toconstructtheimageprototypes,CLIP’simageencoderprocesses
aweakly-augmentedunlabelledimageα(x)andgeneratesafeaturerepresentationf =E (α(x)),
v
where f ∈ R1×d. Without class labels, we utilize Equation 2 to generate pseudo-labels yˆfrom
zero-shotCLIP.ToobtaintheimageprototypeP foraclassc ,weaveragethefeaturerepresentation
j j
f oftheunlabelledimagesassignedtoclassc asfollows:
j
N
1 (cid:88)
P = I f (4)
j N yˆi=j i
j
i=1
whereN = (cid:80)N I . ForthesetofC classes,wedefinetheimageprototypesasP ∈ RC×d.
j i=1 yˆi=j
Theimageprototypesfunctionasanon-parametricclassifier. Giventhatthepseudo-labelsgenerated
byzero-shotCLIPareinitiallynoisy,ourapproach,inspiredbypreviousworks[36–38],employsa
memorybankMB.Thismemorybankfacilitatestheincrementalupdateofimageprototypesinanon-
parametricwaythroughoutthetrainingprocess. Specifically,MBstoresthefeaturerepresentations
ofallunlabelledtargetsamplesalongwiththeircorrespondingpseudo-labels: MB={(f ,yˆ)}N .
i i i=1
Astheimageprototypesarenon-parametric,weiterativelyrefinethemusingthememorybankas
follows: (1)WeinitiallycalculatetheimageprototypesusingEquation4,utilizingtheimagefeatures
andthepseudo-labelsgeneratedfromzero-shotCLIPacrosstheentiretargetdataset. (2)During
training,wecontinuouslyupdatethememorybankbystoringtheimagerepresentationsalongside
theircorrespondingpseudo-labels. (3)Attheendofeachepoch,weupdatetheprototypesP using
j
Equation4,incorporatingtheimagerepresentationsandtheircorrespondingpseudo-labelsfromMB.
(4)Werepeatsteps(2)and(3)untilthecompletionofthetrainingprocess. DPAmaximizesthe
utilizationofallavailabletrainingsamplestoenhancetheimageprototypes. Thisiterativeprocess
ensures thateach prototype continuouslyevolves, progressivelyassimilating relevant knowledge
fromindividualtrainingsamplesthroughoutthemodeltrainingphase, whilemitigatingtheerror
propagationcommonlyassociatedwithpseudo-labels.
Pseudo-labelsGeneration: Bothimageandtextualprototypesserveasdistinctclassifiersforthe
unlabelledtargetdataset. Wegenerateapseudo-label(PL)fortheweakly-augmentedunlabelled
targetimageα(x)fromthesetwoprototypes.Initially,wedeterminethesimilaritybetweentheimage
featuresandthetextualprototypesasfollows:
p =f ·ZT (5)
t
Then,wefindthesimilaritybetweentheimagefeaturesandtheimageprototypesasfollows:
p =f ·PT (6)
v
Finally,wefusep andp toformpˆasfollows:
t v
pˆ=β·DA(p )+(1−β)·p (7)
t v
Following [39], we perform distribution alignment (DA) to prevent the model’s prediction from
collapsingontospecificclasses. Here,DA(p )=p /p¯,wherep¯ representsarunningaverageof
t t t t
p duringtraining,andβ isahyperparameter. Finally,thePLisobtainedasyˆ=argmax pˆ. This
t c
process of generating pseudo-labels improves the accuracy of the PL during training, indicating
thatthesePLsmayinitiallybenoisy(seeFigure3). Therefore, assigninguniformweightstoall
pseudo-labelscouldhindertheadaptationprocess. Toaddressthischallenge,weproposeadjusting
5theclassificationlossweightforthepseudo-labelyˆbasedonitssimilaritytothetextualandimage
prototypes,asfollows:
w =⟨f,Z(yˆ)⟩⟨f,P(yˆ)⟩ (8)
x
⟨·,·⟩representsthecosinesimilaritybetweentheimagefeaturesandthecorrespondingprototypes
derivedfromthePLs. Finally,weutilizethepseudo-labelyˆ,obtainedfromaweakly-augmented
imageα(x),asself-supervisionforthestrongly-augmentedcounterpartA(x)asfollows:
 
C
L
st
=−E
x∈Xt
w x·(cid:88) I yˆ=jlog (cid:0) p A(x)(cid:1) , (9)
j=1
wherew representstheweightcalculatedfromEquation8,andp =E (A(x))·ZT represents
x A(x) v
theprobabilisticoutputforthestrongly-augmentedimageA(x). Toalleviatetheconfirmationbias
inducedbyCLIP[40],weadopt“fairness”regularizationassuggestedin[11]asfollows:
C
1 (cid:88) (cid:0) (cid:1)
L =− log p¯ , (10)
reg C A(x),j
j=1
wherep¯ representsthemodel’saveragepredictionfromthestronglyaugmentedimagesacross
A(x)
thebatch. Byincorporatingthisfairnessregularization, weaimtoencouragethemodeltomake
moreuniformpredictionsacrossclasses,reducingthetendencytooverfitthePLsandpromotinga
morebalancedadaptationtothetargetdomain. Equations9and10areusedtoupdateboththeimage
encoderandthetextualprototypes.
PrototypesAlignment: Duringtraining,thetextualprototypesundergocontinualupdatestoalign
withtheassociatedimagerepresentation,leveragingpseudo-labelsusingEquation9and10. Con-
versely,therefinementoftheimageprototypesoccursthroughaveragingimagefeatures,asdetailed
in Section 3.2. We exploit the image prototypes to provide additional supervision for updating
textualprototypesbeyondpseudo-labelalone. Theseimageprototypes,lessinfluencedbyerroneous
pseudo-labelsandclosertoimagefeatures(asillustratedinFigure1),offerextraguidanceforrefining
textualprototypes. Ourprimaryobjectiveistooptimizetherelationshipbetweenimageprototypes
PandtextualprototypesZbymaximizingthe(cosine)similaritybetweenthesefeaturesets. To
accomplish this optimization, we employ the InfoNCE loss [41] to directly assess the alignment
betweenthecorrespondingfeaturevectorsinPandZ,asfollows:
L
=−(cid:88)C
log
exp(P
j
·ZT j/τ)
(11)
align (cid:88)
exp(P ·ZT/τ)
j=1 j r
r
Withthisalignment,wemaximizethe(cosine)similaritybetweentheimageprototypesPandthe
textual prototypes Z, encouraging the model to learn more closely aligned representations. The
overalllossfunctionusedfortrainingonthetargetdatais,L=λ L +λ L +λ L ,where
1 st 2 reg 3 align
λ ,λ ,λ arenon-learnableparameters.
1 2 3
4 Experiments
Datasetsandbaselines:Weextensivelyevaluateourapproachon13diversedatasets:ImageNet[42],
Caltech101[43], DTD[44],EuroSAT[18],FGVCAircraft[45],Food101[46], Flowers102[47],
OxfordPets [48], SUN397 [49], StandfordCars [50], CIFAR10\100 [51], and UCF101 [52]. We
performcomparativeanalysiswithfourstate-of-the-art(SOTA)unsupervisedadaptationmethods:
CLIP[1],UPL[14],POUF[13],andLaFTer[12].
ImplementationDetails: Forallexperiments,unlessotherwisespecified,weutilizeaViT/B-32
CLIPpre-trainedbyOpenAI[1]. Inthecontextofunsupervisedfine-tuning,wespecificallytarget
the layer-normalization weights of the image encoder and the textual prototypes. This strategy
is effective and stable for adapting models with noisy supervision [53,54]. The text encoder of
CLIPwillbeexcludedafterconstructingthetextualprototypes. Inputimagesarestandardizedto
asizeof224×224. Duringtraining,weapplyaseriesofdataaugmentationtechniques,including
RandomResizedCrop,Flip,RandAugment[55]asastrongaugmentationandResize+RandomCrop
forgeneratingpseudo-labels. Fortesting,weextractaCentercropafterresizingtheimageto224.
6WeutilizeAdamWoptimizer[56]withacosinelearningrateschedule. Forfaircomparisons,we
reproduce the results of UPL, POUF, and LaFTer using their publicly available code. For more
implementationdetails,pleaserefertothesupplementarymaterials.
Results: Table1reportsthetop-1accuracyofourproposedmethodDPAandfourSOTAbaselines:
CLIP,UPL,POUF,andLaFTeron13datasets. Ourapproachconsistentlyoutperformszero-shot
CLIP,improvingtop-1accuracyby+7.83%onaverage.ComparedtoUPL,whichoptimizesprompts,
DPAstillachievesconsistentimprovementswithanaveragegainof+8.24%withminimaladditional
computationaloverheaddespiteslightlylargerparameters. DPAalsosurpassesPOUFandLaFTer,
achievingaverageincreasesof+7.47%and+3.93%,respectively,acrossthe13datasets. Notably,
DPAoutperformsLaFTerwithoutrequiringadditionaltextcorporaorfurtherpre-training.
Table1: ComparisonwithSOTAunsupervisedadaptionmethods.
Method ImgNet Caltech DTD ESAT FGVCA Food Flower OxPets SUN StCars CIFAR10 CIFAR100 UCF Avg
Zero-shotCLIP[1] 63.30 90.69 44.42 43.84 19.50 82.40 66.46 87.50 61.99 58.74 89.80 65.10 64.20 64.46
UPL[14] 58.22 92.36 45.37 51.88 17.07 84.25 67.40 83.84 62.12 49.41 91.26 67.41 62.04 64.05
POUF[13] 52.20 94.10 46.10 62.90 18.20 82.10 67.80 87.80 60.00 57.70 90.50 62.00 61.20 64.82
LaFTer[12] 61.63 94.39 50.32 69.96 19.86 82.45 72.43 84.93 65.87 57.44 94.57 69.79 65.08 68.36
DPA 64.64 96.06 55.69 80.04 20.67 84.76 75.56 90.71 68.13 62.62 95.97 76.47 68.49 72.29
Forablationstudies,weuse11outof13datasets,excludingImageNetandSUN397duetotheir
largesize. Excludingtheselarge-scaledatasetsallowsustoperformmoreextensiveexperimentsand
analyseswhilemaintainingcomputationalfeasibility.
AnalysisofModelComponents: Weinvestigatetheimpactofdifferentcomponentsthroughthe
trainingoffourdistinctmodels: Base: Thismodeliterativelygeneratespseudo-labelsusingtextual
prototypes and incorporates regularization loss through self-training. Center: CLIP undergoes
trainingwithPLsgenerationusingimageandtextprototypes,asdetailedinSection3.2,inaddition
tobuildinguponthefoundationestablishedintheBasemodel. Center+w: introducestheweighting
strategyforCentermodel.Center+w+Align:introducesthealignmentloss,asdetailedinSection3.2.
As shown in Table 2, Base model, which relies solely on self-training, demonstrates a notable
improvement compared to the zero-shot CLIP. However, as training progresses, the impact of
confirmationbiasbecomesevident,leadingtoadecreaseintheaccuracyofthepseudo-labels,as
illustratedinFigure3-(a). Tomitigatetheeffectsofconfirmationbias,weintroducethepseudo-label
generationstrategyin Centermodel.Thisapproachenhancesthequalityofpseudo-labelsthroughout
thetrainingprocess,leadingtoasignificantgainof+7.28%overzero-shotCLIPinaverageaccuracy.
IncorporatingtheweightingstrategyinCenter+wmodelresultsinanadditionalincreaseof+8.01%
overzero-shotCLIPinaccuracy. Byassigningappropriateweightstotheself-trainingloss,themodel
canbetterbalancethecontributionsofhigh-confidenceandlow-confidencepseudo-labels,leading
to more stable and practical training. Finally, including the alignment loss in Center+w+Align
modelleadstoafurtherincreaseof+8.58%overzero-shotCLIPinaverageaccuracy. Byexplicitly
encouragingthealignmentbetweenimageandtextprototypes,themodellearnsmorerobustand
transferablerepresentations, whicharecrucialforeffectiveunsupervisedadaptation. Finally, we
conductadetailedanalysisoftheevolutionofpseudo-labelaccuracyduringtrainingepochsforour
method(DPA)comparedtoBasemodel(Figure3-(a,b,c)).
Table2: Ablationstudy. Seetextfordetails.
Method Caltech DTD ESAT FGVCA Food Flower OxPets StCars CIFAR10 CIFAR100 UCF Avg
Zero-shotCLIP 90.69 44.42 43.84 19.50 82.40 66.46 87.50 58.74 89.80 65.10 64.20 64.79
Base 93.57 48.99 61.94 19.20 84.10 68.45 90.24 59.25 95.95 73.55 65.45 69.15
Center 95.44 55.53 70.56 19.80 84.65 75.27 90.71 61.53 95.96 76.01 67.30 72.07
Center+w 95.46 54.54 80.06 19.56 84.63 75.44 90.49 61.19 95.97 75.92 67.51 72.80
Center+w+Align(DPA) 96.06 55.69 80.04 20.67 84.76 75.56 90.71 62.62 95.97 76.47 68.49 73.37
IncorporatingImagePrototypesasPseudo-LabelSource: Weanalysetheimpactofincorporating
imageprototypesasanadditionalclassifier. WeexcludeEquation7andgeneratePLssolelybasedon
textprototypes,whilepreservingthealignmentofimage-textprototypesandtheweightingstrategy.
WeupdateMBsolelyusingthepseudo-labelsderivedfromthetextualprototypes.Table3showsthat
thealignmentandweightingstrategiesgenerallyperformwellacrossdatasets,withminordifferences
comparedtotheentiremodel,exceptforEuroSATandFlowers-102.
ImpactofImagePrototypeInitializationonPerformance: Weevaluatethreeinitializationsfor
imageprototypemethods: mean,weightedmean,andsimilarityweightedmean[28](Table4). The
resultsdemonstratesignificantimprovementsoverthebaselinesforallinitializationstrategies.
7Figure3: Basevs. DPAin(a)PLs,(b)trainingand(c)testingaccuracyonEuroSATdataset.
Table3: Imageprototypesvs. textprototypesforpseudo-labelgeneration.
Method Caltech DTD ESAT FGVCA Food Flower OxPets StCars CIFAR10 CIFAR100 UCF Avg
Zero-shotCLIP 90.69 44.42 43.84 19.50 82.40 66.46 87.50 58.74 89.8 65.10 64.20 64.79
DPA(w/oPLsGeneration) 95.41 54.20 61.94 19.44 84.49 69.22 91.17 61.95 95.93 76.12 67.96 70.71
DPA 96.06 55.69 80.04 20.67 84.76 75.56 90.71 62.62 95.97 76.47 68.49 73.37
RobustnessAnalysisunderDistributionShifts: Toevaluaterobustnessagainstnaturaldistribu-
tionshifts,weshowresultsonImageNet-Sketch[57],ImageNet-Adversarial[58],andImageNet-
Rendition[59]. Initially,weevaluateafine-tunedViT-B/32modelonImageNetusingourmethod,
achievinganaccuracyof64.64%ontheImageNetvalidationset. Weexploretransductivetransfer
learning[60,61]withourapproach. Transductivelearninginvolvesassumingaccesstounlabelled
testimagesfromthenewdistribution(Table5). Weobserveamodestenhancementinperformance
underdistributionshiftin2outof3datasets. Thisslightimprovementisattributedtoutilizingonly
25%ofImageNetdataset,adeliberatechoicemadetomanagecomputationalresourceseffectively
andaddressthesubstantialsizeofImageNettrainingdataset.
FullFine-tuning: Wecomparefullfine-tuningofthemodelsinsteadofsolelyfine-tuningthelayer-
normalizationweights(Table6)undernoisyPLs. ThelastrowshowsthePLsofzero-shotCLIPused
atthestartofthetrainingforconstructingtheinitialimageprototypes. IndatasetswithnoisyPLs,
Table6highlightstheimportanceofsolelyfine-tuninglayer-normalizationweightsduringadaptation
topreventoverfittingandmaximizeperformance.
With other VLMs: We evaluate the efficacy of DPA with other VLM models in Table 7. To
assessthis,weconductedexperimentsonSLIP[62],aversionofCLIPincorporatingself-supervised
learningobjectivesduringpre-training. Additionally,weevaluatedDPAusingadifferentCLIP-based
architecture,ratherthanViT-B/32. AsdepictedinTable7,DPAconsistentlydemonstratessubstantial
andconsistentimprovementsacrossdiverseVLMsandarchitectures.
Table4: PerformanceofDPAwiththreedifferentinitializationsforimageprototypes.
Method Caltech DTD ESAT FGVCA Food Flower OxPets StCars CIFAR10 CIFAR100 UCF Avg
Zero-shotCLIP 90.69 44.42 43.84 19.50 82.40 66.46 87.50 58.74 89.80 65.10 64.20 64.79
DPA(WeightedMean) 96.00 54.52 68.34 20.31 84.70 73.81 90.49 63.14 95.96 75.81 68.04 71.92
DPA(SimilarityWeightedMean) 94.52 55.16 79.76 21.12 84.70 77.06 90.84 62.98 95.93 76.33 68.28 73.33
DPA(Mean) 96.06 55.69 80.04 20.67 84.76 75.56 90.71 62.62 95.97 76.47 68.49 73.37
8Table5: Transductiveadaptationtodistributionshift
Method ImageNet ImageNet-S ImageNet-A ImageNet-R
Zero-shotCLIP 63.30 42.10 68.60 32.10
DPA(Transductive) 64.64 42.60 68.40 32.20
Table6: Comparisonoffullfine-tuningvs. layer-normalizationweightsfine-tuningforDPA
Method Caltech DTD ESAT FGVCA Food Flower OxPets StCars CIFAR10 CIFAR100 UCF Avg
Zero-shotCLIP 90.69 44.42 43.84 19.50 82.40 66.46 87.50 58.74 89.80 65.10 64.20 64.46
DPA(LayerNormTuning) 96.06 55.69 80.04 20.67 84.76 75.56 90.71 62.62 95.97 76.47 68.49 73.31
DPA(FullTuning) 94.57 53.03 62.92 17.37 82.62 75.23 91.14 54.06 97.44 78.91 68.57 70.53
DPA(PLsaccuracyofZero-shotCLIP) 88.83 43.54 44.90 17.79 77.91 66.53 83.80 57.81 89.64 65.60 64.10 63.68
Table7: EffectivenessofDPAondifferentmodelarchitecturesandpre-trainingstrategies.
Method Caltech DTD ESAT FGVCA Food Flower OxPets StCars CIFAR10 CIFAR100 UCF
Init→Adapt Init→Adapt Init→Adapt Init→Adapt Init→Adapt Init→Adapt Init→Adapt Init→Adapt Init→Adapt Init→Adapt Init→Adapt
SLIP(ViT-L/16) 83.90→90.20 25.53→36.28 20.90→31.50 8.40→9.18 60.45→68.08 64.23→75.07 33.22→44.02 8.00→9.03 81.67→88.10 48.70→59.88 38.09→52.76
DPA(ViT-B/16) 92.60→96.09 44.70→50.69 49.00→81.22 23.97→25.14 87.80→89.83 70.89→78.68 89.00→93.19 64.70→68.45 90.80→96.43 68.22→78.10 69.10→74.62
ComparisonwithReCLIP:Weprovideacomparisonwithaveryrecentmethodforunsupervised
adaptationofVLMs,namelyReCLIP[16](Table8). DPAconsistentlyoutperformsReCLIP[16]
(Table8)inbothinductiveandtransductivesettings. Also,DPAhassuperiortrainingefficiencyand
performancewithfewerparametersandreducedcomputationalcomplexity(Figure4).
Table 8: Comparison of top-1 classification accuracy
(%)ofDPAwithReCLIP[16],asource-freeunsuper- 80
viseddomainadaptationmethodforCLIP.† indicates 75
thatthemodelwasoriginallytrainedinatransductive 70
setting,butwetraineditinaninductivesettingforfair
65
comparison.
60 CLIP
Method Caltech DTD ESAT FGVCA Flower OxPets StCars UCF Avg UPL
Zero-shotCLIP 90.69 44.42 43.84 19.50 66.46 87.50 58.74 64.20 59.42 55 LaFTer
Inductive 50 POUF
ReCLIP†[16] 95.94 53.88 70.80 18.87 72.63 87.49 59.22 67.01 65.73 ReCLIP
DPA 96.06 55.69 80.04 20.67 75.56 90.71 62.62 68.49 68.73 45 Zero-Shot DPA (ours)
Transductive
ReCLIP[16] 92.43 52.50 59.30 20.34 70.65 88.42 59.06 69.13 63.98 0 10 20 30 40
Training Time (s)
DPA 91.62 57.13 71.84 20.76 74.14 91.11 62.58 68.44 67.20
Figure4:EfficiencycomparisonofDPAwith
baselines. The radius of each circle repre-
sentstrainableparametersineachmethod.
Comparison with Few-Shot methods: We compare with the few-shot methods (CoOp [7] and
MaPLe[9])inTable9. DPAmatchorsurpassfew-shotmethodsin6of11datasets.
Table9: Comparisonwithfew-shotmethodsacrossmultipledatasets.
Method Caltech DTD ESAT FGVCA Food Flower OxPets StCars CIFAR10 CIFAR100 UCF Avg
Zero-shotCLIP 90.69 44.42 43.84 19.50 82.40 66.46 87.50 58.74 89.80 65.10 64.20 64.79
DPA 96.06 55.69 80.04 20.67 84.76 75.56 90.71 62.62 95.97 76.47 68.49 73.37
8-shot
CoOp 93.96 61.82 77.17 26.73 78.84 88.71 85.61 66.20 94.63 75.91 77.66 75.20
MaPLe 93.23 34.99 65.25 21.96 80.51 68.78 81.85 58.51 88.23 71.14 69.18 66.69
16-shot
CoOp 95.09 67.55 76.78 30.93 79.12 93.34 86.70 70.73 94.50 75.78 79.51 77.28
MaPLe 93.35 53.43 71.49 21.48 81.01 72.47 86.54 60.28 91.94 71.74 70.26 70.36
Limitationsandbroaderimpact: Whileourmethodshowspromisingresults,thereisstillroomfor
improvementinmitigatingzero-shotCLIP’sstrongconfirmationbiases,asillustratedinFigure5,
whichwasnottheprimaryfocusofthiswork. Thisfigureunderscorestheremarkableefficacyofour
approachinsource-freeadaptationusingdualclassifierstogeneratepseudo-labelstoreducefalse
positiveratesacrossallcategories. However,zero-shotCLIPhasstrongconfirmationbiases,resulting
inourmethodconsistentlyachievingfalsepositivepredictionsforcertainclasses. Theconfusion
9
)%(
ycaruccA
1-poTbetweenclassesisalsoevidentinReCLIP,indicatingthataddressingthesestrongbiasesrequires
morerobustde-biasingstrategies.
Annual Crop Land 0.94 0.00 0.01 0.01 0.00 0.02 0.01 0.00 0.01 0.00
Annual Crop F oLa ren sd t 0 0. .6 011 0 0. .0 54 5 0 0. .0 00 0 0 0. .0 38 1 00 .. 021 1 0 0. .0 00 0 00 .. 00 02 00 .. 00 03 0 0. .0 00 0 0 0. .0 111 Annual Crop F oLa ren sd t 0 0. .8 04 1 00 .. 50 01 0 0. .0 00 0 0 0. .0 02 2 0 0. .0 09 0 0 0. .0 00 0 0 0. .0 00 0 00 .. 00 01 0 0. .0 00 0 0 0. .0 44 7 Herbaceous VegetationF o Lare ns dt 0 0. .0 00 0 0 0. .8 188 0 0. .0 740 00 .. 00 21 0 0. .0 00 0 0 0. .0 06 3 0 0. .0 04 0 0 0. .0 00 1 0 0. .0 00 2 0 0. .0 00 0 0.8
Herbaceous V he igg he wta at yio on r L ra oand d 0 0. .0 111 0 0. .6 06 1 0 0. .0 00 0 00 .. 621 1 0 0. .0 252 00 .. 00 01 0 0. .0 00 1 0 0. .0 00 1 00 .. 00 03 0 0. .0 05 0 Herbaceous V he igg he wta at yio on r L ra oand d 00 .. 00 92 0 0. .6 00 1 0 0. .0 00 0 00 .. 715 6 0 0. .0 111 0 0. .0 01 1 0 0. .0 00 0 0 0. .0 00 1 0 0. .0 05 1 00 .. 017 1 Indh uig sth rw iaa l By uo ilr d r io na gd s 0 0. .0 04 0 0 0. .0 00 0 00 .. 00 01 0 0. .8 08 0 0 0. .0 93 7 00 .. 00 02 0 0. .0 00 0 00 .. 00 21 00 .. 00 02 0 0. .0 00 0 0.6
Industrial Buildings 0.00 0.00 0.00 0.00 0.91 0.00 0.00 0.09 0.00 0.00 Industrial Buildings 0.00 0.00 0.00 0.00 0.88 0.00 0.00 0.12 0.00 0.00
Pasture Land 0.01 0.01 0.02 0.01 0.00 0.95 0.00 0.00 0.00 0.00
Pasture Land 0.62 0.09 0.00 0.03 0.05 0.14 0.06 0.00 0.00 0.01 Pasture Land 0.22 0.08 0.00 0.02 0.01 0.63 0.00 0.00 0.01 0.03 0.4
permanent crop land 0.50 0.04 0.00 0.07 0.35 0.00 0.02 0.02 0.01 0.00 permanent crop land 0.57 0.06 0.00 0.09 0.21 0.02 0.00 0.03 0.01 0.01 permanent crop land 0.56 0.00 0.29 0.06 0.02 0.06 0.00 0.00 0.01 0.00
Residential Buildings 0.00 0.01 0.00 0.00 0.26 0.00 0.00 0.73 0.00 0.00 Residential Buildings 0.00 0.01 0.00 0.00 0.13 0.00 0.00 0.86 0.00 0.00 Residential Buildings 0.00 0.00 0.00 0.00 0.01 0.00 0.00 0.98 0.00 0.00 0.2
River 0.14 0.02 0.00 0.22 0.33 0.01 0.01 0.02 0.22 0.04 River 0.09 0.01 0.00 0.16 0.11 0.02 0.00 0.01 0.49 0.11 River 0.03 0.00 0.00 0.04 0.01 0.05 0.00 0.00 0.87 0.00
Sea or Lake 0.01 0.04 0.00 0.13 0.02 0.00 0.04 0.00 0.07 0.69 Sea or Lake 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.99 Sea or Lake 0.00 0.03 0.00 0.00 0.00 0.01 0.18 0.00 0.15 0.63 0.0
(a)Zero-shotCLIP (b)ReCLIP(inductivetraining) (c)DPA(Ours)
Figure5: Confusionmatricesofzero-shotCLIP,ReCLIP[16],andourmethod(DPA).
5 Conclusion
We present DPA, an unsupervised domain adaptation method for VLMs that aims at bridging
thedomaingapbetweenvisualandtextualrepresentations. DPAintroducesanovelideaofdual
prototypes,whichworkastwodistinctclassifiers,andtheiroutputsarefusedviaaconvexcombination.
It also ranks pseudo-labels for robust self-training during early training. DPA also enhances the
alignment of visual-textual prototypes to improve adaptation performance further. DPA shows
improvedpseudo-labellingaccuracy,whichleadstonotableperformanceimprovementsonvarious
downstreamtasks. DPAsignificantlyoutperformszero-shotCLIPandthestate-of-the-artbaselines
across13downstreamvisiontasks.
References
[1] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin,J.Clarketal.,“Learningtransferablevisualmodelsfromnaturallanguagesupervi-
sion,”inInternationalconferenceonmachinelearning. PMLR,2021,pp.8748–8763.
[2] C.Jia,Y.Yang,Y.Xia,Y.-T.Chen,Z.Parekh,H.Pham,Q.Le,Y.-H.Sung,Z.Li,andT.Duerig,
“Scalingupvisualandvision-languagerepresentationlearningwithnoisytextsupervision,”in
InternationalConferenceonMachineLearning. PMLR,2021,pp.4904–4916.
[3] J. Yang, J. Duan, S. Tran, Y. Xu, S. Chanda, L. Chen, B. Zeng, T. Chilimbi, and J. Huang,
“Vision-languagepre-trainingwithtriplecontrastivelearning,”inProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,2022,pp.15671–15680.
[4] Y. Li, F. Liang, L. Zhao, Y. Cui, W. Ouyang, J. Shao, F. Yu, and J. Yan, “Supervision
exists everywhere: A data efficient contrastive language-image pre-training paradigm,”
in International Conference on Learning Representations, 2022. [Online]. Available:
https://openreview.net/forum?id=zq1iJkNk3uN
[5] B. An, S. Zhu, M.-A. Panaitescu-Liess, C. K. Mummadi, and F. Huang, “PerceptionCLIP:
Visualclassificationbyinferringandconditioningoncontexts,”inTheTwelfthInternational
ConferenceonLearningRepresentations,2024.
[6] R.Zhang,W.Zhang,R.Fang,P.Gao,K.Li,J.Dai,Y.Qiao,andH.Li,“Tip-adapter: Training-
freeadaptionofclipforfew-shotclassification,”inEuropeanConferenceonComputerVision.
Springer,2022,pp.493–510.
[7] K.Zhou, J.Yang, C.C.Loy, andZ.Liu, “Learningtopromptforvision-languagemodels,”
InternationalJournalofComputerVision,vol.130,no.9,pp.2337–2348,2022.
[8] ——, “Conditional prompt learning for vision-language models,” in Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,2022,pp.16816–16825.
[9] M.U.khattak,H.Rasheed,M.Maaz,S.Khan,andF.S.Khan,“Maple: Multi-modalprompt
learning,”inTheIEEE/CVFConferenceonComputerVisionandPatternRecognition,2023.
10
dnaL
porC launnA
tseroF dnaL
noitategeV
suoecabreH
daor
ro yawhgih
sgnidliuB
lairtsudnI
dnaL
erutsaP
dnal
porc tnenamrep
sgnidliuB
laitnediseR
reviR ekaL
ro aeS
dnaL
porC launnA
tseroF dnaL
noitategeV
suoecabreH
daor
ro yawhgih
sgnidliuB
lairtsudnI
dnaL
erutsaP
dnal
porc tnenamrep
sgnidliuB
laitnediseR
reviR ekaL
ro aeS
dnaL
porC launnA
tseroF dnaL
noitategeV
suoecabreH
daor
ro yawhgih
sgnidliuB
lairtsudnI
dnaL
erutsaP
dnal
porc tnenamrep
sgnidliuB
laitnediseR
reviR ekaL
ro aeS[10] M.U.Khattak,S.T.Wasim,M.Naseer,S.Khan,M.-H.Yang,andF.S.Khan,“Self-regulating
prompts: Foundationalmodeladaptationwithoutforgetting,”inProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision(ICCV),October2023,pp.15190–15200.
[11] J.Li,S.Savarese,andS.C.H.Hoi,“Maskedunsupervisedself-trainingforlabel-freeimage
classification,”inICLR,2023.
[12] M.J.Mirza,L.Karlinsky,W.Lin,M.Kozinski,H.Possegger,R.Feris,andH.Bischof,“Lafter:
Label-freetuningofzero-shotclassifierusinglanguageandunlabeledimagecollections,”in
ConferenceonNeuralInformationProcessingSystems(NeurIPS),2023.
[13] K.Tanwisuth,S.Zhang,H.Zheng,P.He,andM.Zhou,“Pouf: Prompt-orientedunsupervised
fine-tuningforlargepre-trainedmodels,”inInternationalConferenceonMachineLearning.
PMLR,2023,pp.33816–33832.
[14] T.Huang, J.Chu, andF.Wei, “Unsupervisedpromptlearningforvision-languagemodels,”
arXivpreprintarXiv:2204.03649,2022.
[15] Q.Qian,Y.Xu,andJ.Hu,“Intra-modalproxylearningforzero-shotvisualcategorizationwith
clip,”AdvancesinNeuralInformationProcessingSystems,vol.36,2024.
[16] X.Hu,K.Zhang,L.Xia,A.Chen,J.Luo,Y.Sun,K.Wang,N.Qiao,X.Zeng,M.Sunetal.,
“Reclip: Refinecontrastivelanguageimagepre-trainingwithsourcefreedomainadaptation,”in
ProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision,2024,pp.
2994–3003.
[17] V. W. Liang, Y. Zhang, Y. Kwon, S. Yeung, and J. Y. Zou, “Mind the gap: Understanding
the modality gap in multi-modal contrastive representation learning,” Advances in Neural
InformationProcessingSystems,vol.35,pp.17612–17625,2022.
[18] P.Helber,B.Bischke,A.Dengel,andD.Borth,“Eurosat: Anoveldatasetanddeeplearning
benchmark for land use and land cover classification,” IEEE Journal of Selected Topics in
AppliedEarthObservationsandRemoteSensing,vol.12,no.7,pp.2217–2226,2019.
[19] A.Iscen,G.Tolias,Y.Avrithis,andO.Chum,“Labelpropagationfordeepsemi-supervised
learning,”inProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecog-
nition(CVPR),June2019.
[20] K.DesaiandJ.Johnson,“Virtex: Learningvisualrepresentationsfromtextualannotations,”in
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,2021,pp.
11162–11173.
[21] M.B.Sariyildiz,J.Perez,andD.Larlus,“Learningvisualrepresentationswithcaptionannota-
tions,”inEuropeanConferenceonComputerVision. Springer,2020,pp.153–170.
[22] Q.Cui,B.Zhou,Y.Guo,W.Yin,H.Wu,O.Yoshie,andY.Chen,“Contrastivevision-language
pre-trainingwithlimitedresources,”inEuropeanConferenceonComputerVision. Springer,
2022,pp.236–253.
[23] X.Hu,Z.Gan,J.Wang,Z.Yang,Z.Liu,Y.Lu,andL.Wang,“Scalingupvision-language
pre-trainingforimagecaptioning,”inProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,2022,pp.17980–17989.
[24] P.Gao,S.Geng,R.Zhang,T.Ma,R.Fang,Y.Zhang,H.Li,andY.Qiao,“Clip-adapter: Better
vision-languagemodelswithfeatureadapters,”InternationalJournalofComputerVision,vol.
132,no.2,pp.581–595,2024.
[25] R.Zhang,X.Hu,B.Li,S.Huang,H.Deng,Y.Qiao,P.Gao,andH.Li,“Prompt,generate,then
cache: Cascadeoffoundationmodelsmakesstrongfew-shotlearners,”inProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,2023,pp.15211–15222.
[26] X.Zhu,R.Zhang,B.He,A.Zhou,D.Wang,B.Zhao,andP.Gao,“Notallfeaturesmatter:
Enhancing few-shot clip with adaptive prior refinement,” in Proceedings of the IEEE/CVF
InternationalConferenceonComputerVision,2023,pp.2605–2615.
[27] J.Snell,K.Swersky,andR.Zemel,“Prototypicalnetworksforfew-shotlearning,”Advancesin
neuralinformationprocessingsystems,vol.30,2017.
[28] K.J.Liang,S.B.Rangrej,V.Petrovic,andT.Hassner,“Few-shotlearningwithnoisylabels,”in
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,2022,
pp.9089–9098.
11[29] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, “Unsupervised feature learning via non-parametric
instancediscrimination,”inProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,2018,pp.3733–3742.
[30] W.Xu,Y.Xian,J.Wang,B.Schiele,andZ.Akata,“Attributeprototypenetworkforzero-shot
learning,” Advances in Neural Information Processing Systems, vol. 33, pp. 21969–21980,
2020.
[31] H.-M. Yang, X.-Y. Zhang, F. Yin, and C.-L. Liu, “Robust classification with convolutional
prototypelearning,”inProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,2018,pp.3474–3482.
[32] P.Mettes,E.VanderPol,andC.Snoek,“Hypersphericalprototypenetworks,”Advancesin
neuralinformationprocessingsystems,vol.32,2019.
[33] X. Zhai, X. Wang, B. Mustafa, A. Steiner, D. Keysers, A. Kolesnikov, and L. Beyer, “Lit:
Zero-shottransferwithlocked-imagetexttuning,”inProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,2022,pp.18123–18133.
[34] A.Sahito,E.Frank,andB.Pfahringer,“Betterself-trainingforimageclassificationthrough
self-supervision,”inAustralasianJointConferenceonArtificialIntelligence. Springer,2022,
pp.645–657.
[35] K.Sohn,D.Berthelot,N.Carlini,Z.Zhang,H.Zhang,C.A.Raffel,E.D.Cubuk,A.Kurakin,
andC.-L.Li,“Fixmatch:Simplifyingsemi-supervisedlearningwithconsistencyandconfidence,”
Advancesinneuralinformationprocessingsystems,vol.33,pp.596–608,2020.
[36] L.Zhou,N.Li,M.Ye,X.Zhu,andS.Tang,“Source-freedomainadaptationwithclassprototype
discovery,”Patternrecognition,vol.145,p.109974,2024.
[37] Y.Wen,K.Zhang,Z.Li,andY.Qiao,“Adiscriminativefeaturelearningapproachfordeep
facerecognition,”inComputervision–ECCV2016: 14thEuropeanconference,amsterdam,the
netherlands,October11–14,2016,proceedings,partVII14. Springer,2016,pp.499–515.
[38] S.Xie,Z.Zheng,L.Chen,andC.Chen,“Learningsemanticrepresentationsforunsupervised
domain adaptation,” in International conference on machine learning. PMLR, 2018, pp.
5423–5432.
[39] J.Li,C.Xiong,andS.C.Hoi,“Comatch: Semi-supervisedlearningwithcontrastivegraph
regularization,”inProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,
2021,pp.9475–9484.
[40] X.Wang,Z.Wu,L.Lian,andS.X.Yu,“Debiasedlearningfromnaturallyimbalancedpseudo-
labels,”inProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion,2022,pp.14647–14657.
[41] A.v.d.Oord,Y.Li,andO.Vinyals,“Representationlearningwithcontrastivepredictivecoding,”
arXivpreprintarXiv:1807.03748,2018.
[42] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei,“Imagenet: Alarge-scalehierar-
chicalimagedatabase,”in2009IEEEconferenceoncomputervisionandpatternrecognition.
Ieee,2009,pp.248–255.
[43] L.Fei-Fei,R.Fergus,andP.Perona,“Learninggenerativevisualmodelsfromfewtrainingex-
amples: Anincrementalbayesianapproachtestedon101objectcategories,”in2004conference
oncomputervisionandpatternrecognitionworkshop. IEEE,2004,pp.178–178.
[44] M.Cimpoi,S.Maji,I.Kokkinos,S.Mohamed,andA.Vedaldi,“Describingtexturesinthe
wild,”inProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,2014,
pp.3606–3613.
[45] S.Maji,E.Rahtu,J.Kannala,M.Blaschko,andA.Vedaldi,“Fine-grainedvisualclassification
ofaircraft,”arXivpreprintarXiv:1306.5151,2013.
[46] L.Bossard,M.Guillaumin,andL.V.Gool,“Food-101–miningdiscriminativecomponentswith
randomforests,”inEuropeanconferenceoncomputervision. Springer,2014,pp.446–461.
[47] M.-E.NilsbackandA.Zisserman, “Automatedflowerclassificationoveralargenumberof
classes,”in2008SixthIndianConferenceonComputerVision,Graphics&ImageProcessing.
IEEE,2008,pp.722–729.
12[48] O. M. Parkhi, A. Vedaldi, A. Zisserman, and C. Jawahar, “Cats and dogs,” in 2012 IEEE
conferenceoncomputervisionandpatternrecognition. IEEE,2012,pp.3498–3505.
[49] J.Xiao,J.Hays,K.A.Ehinger,A.Oliva,andA.Torralba,“Sundatabase: Large-scalescene
recognitionfromabbeytozoo,”in2010IEEEComputerSocietyConferenceoncomputervision
andPatternRecognition. IEEE,2010,pp.3485–3492.
[50] J. Krause, M. Stark, J. Deng, and L. Fei-Fei, “3d object representations for fine-grained
categorization,” in Proceedings of the IEEE International Conference on computer vision
workshops,2013,pp.554–561.
[51] A.Krizhevsky,G.Hintonetal.,“Learningmultiplelayersoffeaturesfromtinyimages,”2009.
[52] K.Soomro,A.R.Zamir,andM.Shah,“Adatasetof101humanactionclassesfromvideosin
thewild,”CenterforResearchinComputerVision,vol.2,no.11,pp.1–7,2012.
[53] J.L.Ba,J.R.Kiros,andG.E.Hinton,“Layernormalization,”arXivpreprintarXiv:1607.06450,
2016.
[54] D.Wang,E.Shelhamer,S.Liu,B.Olshausen,andT.Darrell,“Tent: Fullytest-timeadaptation
byentropyminimization,”arXivpreprintarXiv:2006.10726,2020.
[55] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V. Le, “Randaugment: Practical automated data
augmentationwithareducedsearchspace,”inProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognitionworkshops,2020,pp.702–703.
[56] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,” arXiv preprint
arXiv:1711.05101,2017.
[57] H.Wang,S.Ge,Z.Lipton,andE.P.Xing,“Learningrobustglobalrepresentationsbypenalizing
localpredictivepower,”AdvancesinNeuralInformationProcessingSystems,vol.32,2019.
[58] D.Hendrycks,K.Zhao,S.Basart,J.Steinhardt,andD.Song,“Naturaladversarialexamples,”
inProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,2021,
pp.15262–15271.
[59] D.Hendrycks,S.Basart,N.Mu,S.Kadavath,F.Wang,E.Dorundo,R.Desai,T.Zhu,S.Para-
juli,M.Guoetal.,“Themanyfacesofrobustness: Acriticalanalysisofout-of-distribution
generalization,”inProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision,
2021,pp.8340–8349.
[60] Y. Xian, B. Schiele, and Z. Akata, “Zero-shot learning-the good, the bad and the ugly,” in
Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp.
4582–4591.
[61] M.Rohrbach,S.Ebert,andB.Schiele,“Transferlearninginatransductivesetting,”Advances
inneuralinformationprocessingsystems,vol.26,2013.
[62] N. Mu, A. Kirillov, D. Wagner, and S. Xie, “Slip: Self-supervision meets language-image
pre-training,”inEuropeanconferenceoncomputervision. Springer,2022,pp.529–544.
13