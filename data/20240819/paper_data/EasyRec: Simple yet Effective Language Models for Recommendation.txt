EasyRec: Simple yet Effective Language Models
for Recommendation
XubinRen ChaoHuang‚àó
UniversityofHongKong UniversityofHongKong
xubinrencs@gmail.com chaohuang75@gmail.com
ABSTRACT Inrecentyears,therehasbeenanotablesurgeofadvancements
Deepneuralnetworkshavebecomeapowerfultechniqueforlearn- inenhancingrecommendersystemsthroughtheincorporationof
ingrepresentationsfromuser-iteminteractiondataincollaborative neuralnetwork-poweredcollaborativefilteringframeworks,partic-
filtering(CF)forrecommendersystems.However,manyexisting ularlyinthedomainofgraphneuralnetworks(GNNs)[10,32,37].
methodsheavilyrelyonuniqueuseranditemIDs,whichlimits Byeffectivelyleveragingtheinherentgraphstructurepresentin
theirabilitytoperformwellinpracticalzero-shotlearningscenar- thedata,GNNshavedemonstratedexceptionalcapabilitiesincap-
ioswheresufficienttrainingdatamaybeunavailable.Inspiredby turinghigh-orderrelationshipsandcomplexdependenciesamong
thesuccessoflanguagemodels(LMs)andtheirstronggeneraliza- usersanditems.NotableexamplesofsuchGNN-basedapproaches
tioncapabilities,acrucialquestionarises:Howcanweharnessthe includeNGCF[32]andLightGCN[10].Thesemethodshaveshow-
potentialoflanguagemodelstoempowerrecommendersystems casedimpressiveperformanceinrecommendationtasksbyvirtue
andelevateitsgeneralizationcapabilitiestonewheights?Inthis oftheirabilitytomodelthecollaborativesignalspresentinthedata
study,weproposeEasyRec-aneffectiveandeasy-to-useapproach throughrecursivemessagepassingmechanisms.
thatseamlesslyintegratestext-basedsemanticunderstandingwith Theissueofdatascarcityinrecommendersystemsposesasig-
collaborativesignals.EasyRecemploysatext-behavioralignment nificantchallengeforexistingdeepcollaborativefilteringmodels,
framework,whichcombinescontrastivelearningwithcollabora- hinderingtheirabilitytolearnpreciseuser/itemrepresentations,
tivelanguagemodeltuning,toensureastrongalignmentbetween particularlywhendealingwithsparseinteractiondata[9,18,26,33].
thetext-enhancedsemanticspaceandthecollaborativebehavior Thischallengeprimarilyarisesfromthelimitedavailabilityoftrain-
information.Extensiveempiricalevaluationsacrossdiversereal- inglabels,whichinturnimpedesthemodels‚Äôcapacitytocapture
worlddatasetsdemonstratethesuperiorperformanceofEasyRec theintricaterelationshipsanddependenciesthatexistbetween
comparedtostate-of-the-artalternativemodels,particularlyinthe usersanditems.Toalleviatedatascarcityeffects,recentstudies
challengingtext-basedzero-shotrecommendationscenarios.Fur- exploredthepotentialofself-supervisedlearningtoprovideef-
thermore,thestudyhighlightsthepotentialofseamlesslyintegrat- fectivedataaugmentation.Forexample,intermsofcontrastive
ingEasyRecasaplug-and-playcomponentintotext-enhancedcol- augmentation,methodslikeSGL[34]andNCL[19]leveragegraph
laborativefilteringframeworks,therebyempoweringexistingrec- contrastivelearningtosupplementthesupervisedrecommendation
ommendersystemstoelevatetheirrecommendationperformance taskwiththecross-viewmutualinformationmaximization.For
andadapttotheevolvinguserpreferencesindynamicenviron- generativeaugmentation,approachessuchasAutoCF[36]arebuilt
ments.ForbetterresultreproducibilityofourEasyRecframework, uponthemaskedautoencodingmechanism,enablingthemodels
themodelimplementationdetails,sourcecode,anddatasetsare toreconstructtheinteractionstructuresforself-supervision.
availableatthelink:https://github.com/HKUDS/EasyRec. Whiletherecentadvancementsinself-supervisedlearningtech-
niqueshaveofferedpromisingavenuesformitigatingtheimpact
ACMReferenceFormat:
ofdatascarcityincurrentcollaborativefilteringmodels,theyalso
XubinRenandChaoHuang.2024.EasyRec:SimpleyetEffectiveLanguage
comewithasignificantlimitation[45].Thislimitationstemsfrom
ModelsforRecommendation.InProceedingsofACMConference.ACM,
theinherentdesignofthemodels,whichheavilyreliesonusing
Barcelona,Spain,13pages.https://doi.org/10.1145/nnnnnnn.
uniqueidentities(IDs)torepresentusersanditemsthroughout
theentireprocessoflearningrepresentations.Inpracticalrecom-
1 INTRODUCTION
menders,however,weoftencomeacrossnewrecommendationdata
Deep learning has established itself as a highly promising and collectedfromdifferentdomainsortimeperiods,involvingdiverse
powerfulsolutionforcapturinguserpreferencesinthecontext sets ofusers and items. This creates achallenge as existingID-
ofonlinerecommendersystems[42,50].Thisapproachharnesses basedrecommendationmodelsstruggletoeffectivelyincorporate
thepowerofdeepneuralnetworkstolearnrichandmeaningful andadapttosuchnewdata,especiallywhenthereisachangeinthe
useranditemrepresentationsbyanalyzingthecomplexpatterns setofidentitytokensofusersanditemsforzero-shotrecommen-
ofuser-iteminteractions.This,inturn,enablesrecommendation dation.TherigiddependenceonuseranditemIDsinthesemodels
algorithmstoaccuratelyinferuserpreferencesandprovidehighly hinderstheirabilitytogeneralizeandperformwellinscenarios
relevantandpersonalizedrecommendations[30,41]. wheretheuseranditemspacesarenotstaticorfullyoverlapping.
Forinstance,theconstantgenerationofnewitemslikevideos
‚àóChaoHuangistheCorrespondingAuthor.
andsocialmediacontentnecessitatesaccuraterecommendations
in real-life recommender systems, even when there are limited
ACMConference,XXX,XXX
interaction observations. While cross-domain recommendation
2024.ACMISBN978-x-xxxx-xxxx-x/YY/MM.
https://doi.org/10.1145/nnnnnnn.
4202
guA
61
]RI.sc[
1v12880.8042:viXraACMConference,XXX,XXX XubinRenandChaoHuang
82M 125M 356M Insummary,thisworkmakesthefollowingkeycontributions:
0.06 EasyRec EasyRec
0.05 BGE 0.020 ‚Ä¢ Motivation.Theprimaryobjectiveofthisstudyistointroducea
0.04 GTR 0.015 BG GTR E novelrecommendersystemthatisbuiltuponlanguagemodelsto SimCSE functionasazero-shotlearner.Thisinnovativeapproachaimsto
0.03
0.02 BART 0.010 Sim BC AS RE T demonstrateexceptionaladaptabilitytodiverserecommendation
data,whilealsoexhibitingrobustgeneralizationcapabilities.
75 100 125~ 350 400 75 100 125~ 350 400 ‚Ä¢ Methodology.Toaligntext-basedsemanticencodingwithcol-
Model Parameters (Miliion) Model Parameters (Miliion)
laborativesignalsfromuserbehavior,weproposeanovelcon-
Figure1:EasyRecdemonstratessuperiorperformancecom- trastivelearning-poweredcollaborativelanguagemodelingap-
paredtostate-of-the-artlanguagemodelsintext-basedzero- proach,whichallowsthesystemtocaptureboththesemantic
shotrecommendationontheAmazonandSteamdatasets. representationsofusers/items,aswellastheunderlyingbehav-
ioralpatternsandinteractionswithintherecommendationdata.
methods[1,39]drawinspirationfromleveraginginformationand ‚Ä¢ Zero-ShotRecommendationCapacity.TheEasyRecisexten-
knowledgeacrossmultipledomainstoenhancerecommendation sivelyevaluatedthroughrigorousexperimentsasatext-based
performance,theyoftenassumethatusersfromdifferentdomains zero-shotrecommender.Performancecomparisonsrevealthatit
belongtothesameset[3,39].Unfortunately,thisassumptionsig- consistentlyachievessignificantadvantagesoverbaselinemeth-
nificantlyrestrictstheflexibilityandgeneralizationcapabilitiesof odsintermsofrecommendationaccuracyandgeneralization
recommenders.Asaresult,existingmethodsmayencounterdiffi- capabilities.Furthermore,thestudydemonstratesthemodel‚Äôs
cultiesinadaptinganddeliveringaccuraterecommendationswhen remarkablepotentialingeneratingdynamicuserprofilesthat
confrontedwithdiverseuserpopulationsacrossdifferentdomains. arehighlyadaptivetotime-evolvinguserpreferences.
LanguageModelsasZero-ShotRecommenders.Considering ‚Ä¢ ExistingCFModelEnhancement.OurproposedEasyRechas
thechallengesandmotivationsdiscussedearlier,theobjectiveof beenseamlesslyintegratedasalightweight,plug-and-playcom-
thisstudyistointroducearecommendersystemthatfunctionsasa ponentwithstate-of-the-artcollaborativefiltering(CF)models.
zero-shotlearner,possessingrobustgeneralizationcapabilitiesand Thelightweightandmodulardesignofourlanguagemodelis
theflexibilitytoadapttonewrecommendationdataseamlessly.To akeystrength,asitfacilitatestheadoptionofournovelrecom-
accomplishthisobjective,weproposeintegratinglanguagemod- mendationparadigmacrossawiderangeofusecases.
elswithcollaborativerelationmodeling,forminganeffectivetext
embedder-EasyRecthatisbothlightweightandeffective.Thisin-
2 PRELIMINARIES
tegrationseamlesslycombinestext-basedsemanticencodingwith
high-ordercollaborativesignals,resultinginarecommendersystem Inrecommendersystems,wehaveasetofusersU andasetof
thatoffersastrongmodelgeneralizationability. items I, along with the interactions between them (e.g., clicks,
Recentresearchhasexploredleveraginglargelanguagemod- purchases).Foreachuserùë¢ ‚ààU,wedefineNùë¢ asthesetofitems
els(LLMs)toenhancerecommendersystems.Existingapproaches
thatuserùë¢hasinteractedwith.Likewise,foreachitemùëñ ‚ààI,we
broadlyfallintotwocategories.ThefirstusesLLMsfordataaug- defineNùëñ asthesetofuserswhohaveinteractedwiththatitem.To
mentation(e.g.,RLMRec[25],AlterRec[15]),encodingtextualinfor- representtheseuser-iteminteractions,wecanuseaninteraction
mationtocomplementcollaborativefiltering.Whilethiscombines
matrixA|U|√ó|I|,wheretheentryAùë¢,ùëñ is1ifuserùë¢hasinteracted
LLMandcollaborativestrengths,thesemethodsremainID-based
withitemùëñ,and0otherwise.Theprimarygoalofarecommender
andstruggletogeneralize.ThesecondapproachutilizesLLMstodi- model is to estimate the probabilityùëù ùë¢,ùëñ of a future interaction
rectlygenerateuser-iteminteractionpredictionsbasedonlanguage
betweenauserùë¢andanitemùëñ.Thispredictedprobabilitycanthen
(e.g., LLaRA [17], CoLLM [49]). However, such LLM-based rec- beusedtogeneratepersonalizeditemrecommendationsforeach
ommendersfacesignificantreal-worldlimitations,sufferingfrom user,tailoredtotheirindividualpreferencesandpastbehavior.
poorefficiency(e.g.,‚àº1secondperprediction),renderingthem Text-basedZero-ShotRecommendationisessentialinrecom-
impracticalforlarge-scalerecommendationtasks.Thesechallenges mendersystems,asitcanaddressthecommoncold-startproblem.
highlighttheneedformoreefficient,scalablesolutionsthatseam- Inreal-worldscenarios,newusersandnewitemsoftenlacksuffi-
lesslyintegratethesemanticunderstandingoftextualinformation cientinteractiondata,makingitchallengingtoprovideeffective
withthecollaborativestrengthsofzero-shotrecommenders. personalizedrecommendations.Byleveragingthetextualdescrip-
Ourproposedmodelhasdemonstratedsuperiorperformance tionsofusersanditems,suchasproducttitles,details,anduserpro-
incomparisontostate-of-the-artlanguagemodels,asillustrated files,thelanguagemodelscanconstructsemanticrepresentationsto
inFigure1.Thisperformanceadvantageisachievedwithinacost- enabletext-basedrecommendationsforthesecold-startsituations.
efficientparameterspace,rangingfrom100to400millionparame- Thisovercomesthelimitationsoftraditionalcollaborativefilter-
ters,withthecomputationalcostof‚àº0.01secondperprediction. ingmethods,byofferingadistinctadvantageoverthetraditional
Notably,ourmodelexhibitsthescalinglawphenomenon,where ID-basedparadigm.Byleveragingrobustlanguagemodels,this
itsperformancecontinuallyimprovesastheparametersizeisin- approachexhibitsremarkablepotentialin‚Äúzero-shot‚Äùscenarios,
creased.Furthermore,incontrasttoexistingapproachesthatsuffer wherethetestingdatahavenotbeenpreviouslyencountered.
frompoorefficiency,ourmodelisdesignedtobehighlyscalable Formally,wedefinePùë¢ andPùëñ asthegeneratedtext-basedpro-
andpracticalforlarge-scalerecommendationtasks. files of userùë¢ and itemùëñ, respectively, which are encoded into
)nozamA(
02@llaceR
)maetS(
02@llaceREasyRec:SimpleyetEffectiveLanguageModelsforRecommendation ACMConference,XXX,XXX
Item MetaInformation
[ R e c o m m e n d e d I t e m L is t ] IU ns te er r- aI ct te im on atT th r laai bs c e t lb s =a N s pk oB se A itt i b vfa ea nll s . Th swis li ai m bte em lm =i n nis g e g n l aoi tc iv ve e e rfo sr Th bis a ss lkh ae bo ete lb =i as l p lf o pa siv l ta io vyr eee rd s .by It Ie t Rem m a wT Fi t Dele a e: t s uB cra re is psk t e ( iT otb a nga )ll CoC nl ce aa tn en& ate M Tee b xt a ta s Dd ea d et sa c- .
Ranking ‚ãØ EasyRec
This basketball Generated
Other LLM attractsNBA fans. Item Profile
ùëùùëùùë¢ùë¢,ùëñùëñ1 > ùëùùëùùë¢ùë¢,ùëñùëñ2 Items
EasyRec ‚ãØ EasyRec This basketball This ball This basketball
Text embedding draws in NBA captivates fans captivates
It Be am ùëñùëñs1k P er tbo afi ll le U Ss pe ùë¢ùë¢or r P tsr o fafi nle Item C oP ùëñùëñar 2o tfile PP uu sll h c alo ws ae y baskeT th bi as l u l-s re er la l to ev de is t ems. Th swis iu ms mer i nlik ge .s enthusiasts. of the NBA. ‚ãØ sports lovers.
LLM-diversified profiles
(a) Profiles for Rec. (b) Collaborative Language Models with Contrastive Learning (c) Profile Diversification
ùë°ùë°
Figure2:Theoverallframeworkofourproposedcollaborativeinformation-guidedlanguagemodelEasyRec.
representationseùë¢ andeùëñ usingalanguagemodel,asshownas: andcollaborativeaspectsoftheitemsinaunifiedtextualprofile.
eùë¢ =LM(Pùë¢), eùëñ =LM(Pùëñ). (1) 3.1.1 ItemProfiling. Giventherawiteminformation,suchas
Theinteractionlikelihoodùëù
ùë¢,ùëñ
betweenauserùë¢andanitemùëñ title‚Ñé ùëñ,categoriesùëê ùëñ,anddescriptionùëë ùëñ (e.g.,booksummary),we
aim to generate a comprehensive item profile Pùëñ that captures
canbecalculatedasthecosinesimilaritybetweentheirrespective
boththesemanticandcollaborativeaspects.Toreflectuser-item
textembeddingseùë¢ andeùëñ,i.e.,ùëù ùë¢,ùëñ =ùëêùëúùë†(eùë¢,eùëñ).Wecanthenrec-
interactions,weincorporatethetextualinformation(e.g.,posted
ommendtotheuserthetop-ùëòuninteracteditemswiththehighest
reviewsùëü ùë¢,ùëñ)providedbytheitem‚Äôscorrespondingusers.Formally,
similarityscores,yieldingapersonalizedrecommendationset.
theitemprofilegenerationprocessis:
Rùë¢ =top-kùëñ‚ààI\Nùë¢ùëêùëúùë†(eùë¢,eùëñ). (2) Pùëñ =LLM(Mùëñ,‚Ñé ùëñ,ùëê ùëñ,{ùëü ùë¢,ùëñ}) orLLM(Mùëñ,‚Ñé ùëñ,ùëë ùëñ), (4)
Here,Mùëñ representsthegenerationinstruction,while‚Ñé ùëñ andùëë ùëñ
Text-enhancedCollaborativeFiltering.Collaborativefiltering
serveasinput.Byleveraginglargelanguagemodels(LLMs),we
(CF)isawidelyusedrecommendersystemapproachthatleverages
cangenerateaconciseyetinformativeitemprofilePùëñ.
thecollaborativerelationshipsamongusersanditems.Thisexisting
CFparadigmcanbeenhancedbyintegratingencodedsemanticrep- 3.1.2 User Profiling. In practical scenarios, privacy concerns
resentations.Typically,thelikelihoodvalueùëù ùë¢,ùëñ iscalculatedbased oftenlimitthefeasibilityofgeneratinguserprofilesbasedondemo-
ontheinteractiondata,i.e.,ùëù
ùë¢,ùëñ
=ùëì(ùë¢,ùëñ,A),whereArepresents
graphicinformation.Instead,wecanprofileusersbyconsidering
theinteractiondata.Text-enhancedcollaborativefilteringbuilds theircollaborativerelationships,usingthegeneratedprofileinfor-
uponthisfoundationbyincorporatingtextualfeatureseencoded mationfromtheirinteracteditems.Thisapproachallowstheuser
bylanguagemodelsassupplementaryrepresentations.Thisinte- profilestoeffectivelycapturethecollaborativesignalsthatreflect
grationaimstofurtherimprovetherecommendationperformance theirpreferences.Formally,theuserprofilegenerationprocessis:
ofthetraditionalID-basedcollaborativefilteringframeworks.
Pùë¢ =LLM(Mùë¢,{‚Ñé ùëñ,Pùëñ,ùëü ùë¢,ùëñ|ùëñ ‚ààNùë¢}). (5)
ùëù ùë¢,ùëñ =ùëì(ùë¢,ùëñ,A,eùë¢,eùëñ). (3)
Here,Mùë¢ representstheinstructionforgeneratingtheuserprofile
usingalargelanguagemodel(LLM).Wesampleasetofinteracted
3 METHODOLOGY
itemsNùë¢ fromtheuser‚Äôspurchasehistory.Wethencombinethe
In this section, we first discuss how we gather textual profiles user‚Äôsfeedbackùëü ùë¢,ùëñwiththepre-generateditemprofilesPùëñtocreate
forusersanditemsinrecommendersystems,whichareessential theuser‚ÄôstextdescriptionPùë¢,whichcapturestheirpreferences.
forpre-trainingandevaluatingourmodel.Next,we‚Äôlldiveinto
the specifics of EasyRec and its training approach. Lastly, we‚Äôll 3.1.3 AdvantagesofCollaborativeProfiling. Ourcollaborative
introduceourmethodfordiversifyinguserprofiles,whichimproves useranditemprofilingframeworkofferstwokeyadvantagesfor
themodel‚Äôsabilitytoadapttovarioussituations. real-worldrecommendationscenarioswhichareelaboratedas:
‚Ä¢ CollaborativeInformationPreserved.Ourcollaborativepro-
3.1 CollaborativeUserandItemProfiling filingapproachgoesbeyondjusttheoriginaltextualcontent,
Inreal-worldrecommenders,theonlyavailableinformationmaybe also capturing the semantics of user/item characteristics and
rawtextdata,suchasproducttitlesandcategories,associatedwith theirinteractionpatterns.Byencodingtheserichprofilesintoa
theitems.Privacyconcernsoftenmakeitdifficulttocollectcom- sharedfeaturespaceusingarecommendation-orientedlanguage
prehensiveuser-sideinformation.Furthermore,directlyleveraging model,theresultingembeddingsofinteractedusersanditems
suchtextualinformationmayoverlookthecrucialcollaborative arebroughtclosertogether.Thisenablestherecommendersto
relationshipsneededforaccurateuserbehaviormodelingandpref- betteridentifyrelevantmatches,evenfor"zero-shot"usersand
erenceunderstanding.Toaddresstheselimitations,weproposeto items(thosewithoutpriorinteractions)whichareubiquitousin
generatetextualprofilesbyleveraginglargelanguagemodels(e.g., real-worldscenarios.Thesystemcanleveragethecollaborative
GPT,LLaMAseries)[25]toinjectcollaborativeinformationinto signalsencodedwithinthetext-basedprofilestomakebetter
thetextualprofiles.Thisallowsustocaptureboththesemantic recommendations,bridgingthegapforthesecold-startcases.
SuperviseACMConference,XXX,XXX XubinRenandChaoHuang
‚Ä¢ FastAdaptationtoDynamicScenarios.Ouruseranditem / Pos. item for / Neg. items for both users Nice optimization
Bad optimization
profilingapproach,poweredbyrobustlanguagemodels,enables ùëñùëñ1 ùëñùëñ2 User in ùë¢ùë¢ t1he ùë¢ùë¢ f2eature space Recommendation rage
therecommendersystemtoeffectivelyhandlethetime-evolving
nature of user preferences and interaction patterns. The key
advantageisthatsimpleupdatestothetextualuserprofilescan ùëñùëñ1 ùëñùëñ2 ùëñùëñ1 ùëñùëñ2
seamlessly reflect shifts in user interests and behaviors. This
ùëñùëñ3 ùëñùëñ3
flexibilityandresponsivenessmakesourapproachwell-suited ùëñùëñ4 ùëñùëñ4
fordeployingrecommendersystemsindynamicenvironments
whereuserinterestscanevolveovertime.
In-baùë¢ùë¢tc1
h
ùë¢ùë¢2 In-batchùë¢ùë¢1 ùë¢ùë¢2
Trn. Data Trn. Data
ùë¢ùë¢1,pùëñùëño1 s.,ùëñùëñ ne4 g. ùë¢ùë¢1,ùëñùëñ p1 os, .[ùëñùëñ3 n, egùëñùëñ .4]
3.2 ProfileEmbedderwithCollaborativeLM (a) One-Neg. It{eùë¢ùë¢m2 ,pùëñùëñe2r, Sùëñùëña3m}ple (b) Multi-Ne{gùë¢ùë¢. 2It,eùëñùëñm2s, [pùëñùëñe3r, Sùëñùëñ4a]m}ple
Figure3:Contrastivetuningofcollaborativelanguagemodel
Sofar,wehavegeneratedrichtextualprofilesforusersanditems,
empowers the model to learn rich representations that
movingbeyondconventionalID-basedembeddings.However,di-
closelyalignthetext-basedsemanticspacewiththeglob-
rectlyencodingthesetextualprofilesintolatentembeddingsfor
allycollaborativesignalsfromuserbehaviorpatterns.
makingrecommendationsmayhavetwokeylimitations:
fortheTransformerlayers:
‚Ä¢ CapturingRecommendation-SpecificSemantics.Thetext-
basedembeddings,whileexpressive,maynotbeoptimizedfor {x [(0 C) LS],...xùëõ(0) }=Tokenization({ùë§ [CLS],...,ùë§ ùëõ}). (6)
thespecificsemanticsandrelationshipsmostrelevanttotherec-
ommendationtask.Forexample,considertwoitemprofiles:(1) Here,ùë•(0) ‚àà Rùëë istheembeddingretrievedfromtheembedding
‚ÄúThisuserispassionateaboutadvancedAItechniques,focusing
tablecorrespondingtothetokens,andthe(0)superscriptindicates
ondeeplearningandAIresearchworks‚Äù.(2)‚ÄúWithapassion
thatitistheinputtothe(0)-thlayerofthelanguagemodel.The
foradvancedAIdevelopment,thisuserdelvesintosciencefic- tokenizationprocessalsoaddspositionalembeddingstotheinitial
tionandAI-themednovels‚Äù.Thoughtheprofilessharetextual embeddings.Thelanguagemodelthenencodesasequenceoffinal
similarityaboutAI,theirtargetaudiencesdiffer-thefirstcaters embeddings(oneforeachtoken):
t tho eA seIs pc ri oe fin lt ei sst ms, at yhe ovse erc lo on od kt no us ac ni c-fi edr ,e ra ed ce or ms. mD ei nre dc at tl iy one -n sc po ed ci in fig
c
{e[ùê∂ùêøùëÜ],...,eùëõ}=Encoder({x [(0 C) LS],...xùëõ(0) }), (7)
semantics.Refinementisneededtobetteralignembeddingswith whereEncoder(¬∑)referstotheTransformer-basedencoder-only
thespecificcontextandrequirementsoftherecommendation languagemodel.Thekeyoperationintheencodingprocessisthe
system,beyondjusttextualsimilarity. self-attentionmechanism:
‚àö
‚Ä¢ OverlookingHigh-OrderCollaborativeSignals.Whiletex- Attention(ùëÑ,ùêæ,ùëâ)=softmax(ùëÑùêæùëá / ùëë)ùëâ (8)
tualprofilesofferrichsemanticinformation,relyingsolelyon
w.r.t.ùëÑ =ùëãùëäùëÑ,ùêæ =ùëãùëäùêæ,ùëâ =ùëãùëäùëâ. (9)
them may cause us to overlook valuable high-order collabo-
rative patterns that emerge from complex user-item interac- Here,ùëã ‚àà Rùëõ√óùëë representsthestackoftokenembeddings,and
tions[32,36].Thesehigher-ordersignals,suchastransitiveasso- ùëäùëÑ/ùêæ/ùëâ
aretheparametermatricesthatmaptheseembeddings
ciationsandcommunity-levelpreferences,canprovideadditional
intoqueries,keys,andvalues.Thisself-attentionmechanismallows
insightsthatcomplementthetextualdataforuserpreference
eachtokentoaggregateinformationfromallothertokens,ensuring
learninginrecommendersystems.
that each token is informed about the entire sequence. Finally,
Toaddresstheselimitations,weproposeacollaborativelanguage
we select the first embedding e[CLS], which corresponds to the
[CLS]token,astherepresentativeembeddingfortheentireprofile.
modelingparadigmthatseamlesslyintegratesthestrengthsofthe
Thisembeddingisthenpassedthroughamulti-layerperceptronto
semanticrichnessoftheprofilesandthevaluablecollaborative
obtainthefinalencodedrepresentationùëí,asmentionedinEq.(1):
signalsencodedfromcomplexuser-iteminteractionbehaviors.
e=MLP(e[CLS])=LM(P). (10)
3.2.1 BidirectionalTransformerEncoderasEmbedder. We
Withtheseencodedtextembeddingseforeachuseranditem,we
leverageamulti-layerbidirectionalTransformerencoderasthe
canpredictthelikelihoodofinteractionusingcosinesimilarityand
embedder backbone, considering two key benefits: 1) Efficient
makerecommendationsasdescribedinEq.(2).
Encoding:Theencoder-onlyarchitecturefocusessolelyongen-
eratingeffectivetextrepresentations,enablingfasterinferencein 3.2.2 CollaborativeLMwithContrastiveLearning. Themo-
recommendationsystems.2)FlexibleAdaptation:Bybuildingon tivationbehindfine-tuningthecollaborativelanguagemodelus-
pre-trainedTransformermodels,wecanleveragetransferlearning ingcontrastivelearningistoeffectivelycaptureandincorporate
tooptimizetheembedderforspecificrecommendationtasks. high-ordercollaborativesignalsintotherecommendationmodel.
Let‚Äôs consider a user‚Äôs profile as a passage ofùëõ words: P = TraditionalrecommendersusingBayesianPersonalizedRanking
ùë§ 1,...,ùë§ ùëõ.Westartbyaddingaspecialtoken[CLS]atthebegin- (BPR)[27]optimizeencodedembeddingswithonlyonenegative
ningofthewordsequence.Thetokenizationlayerthenencodesthe itempertrainingsample.Thisapproachlimitsthemodel‚Äôsability
inputsequenceintoinitialembeddings,whichserveastheinput tocapturecomplexglobaluser-itemrelationships.EasyRec:SimpleyetEffectiveLanguageModelsforRecommendation ACMConference,XXX,XXX
Thesupervisedcontrastivelossprovidesastrongalternativeto Table1:Statisticsofthedatasets,with"Avg.n"representing
traditionalmethods.Bytreatinginteracteduser-itempairsasposi- theaveragenumberofinteractionsperuser.Datasetsmarked
tiveviewsandnon-interactedpairsasnegatives,itbringsrelated withunderlinearefromdifferentplatforms.
itemembeddingsclosertogetherinthefeaturespace.Asshown
Datasets #Users #Items #Inters. Avg.n Density
inFigure3,contrastivelearningincorporatesabatchofnegatives,
allowingforacomprehensiveadjustmentoftheencodedfeature TrainData 124,732 67,455 802,869 6.44 -
space,whichenablesthemodeltocapturehigh-ordercollabora- -Arts 14,470 8,537 96,328 6.66 7.8ùëí-4
-Movies 17,397 8,330 120,255 6.91 8.3ùëí-4
tive relationships. To evaluate this, we conduct experiments in
-Games 16,994 9,370 134,649 7.92 8.5ùëí-4
Section4.2.3toassesstheimpactofdifferentlearningobjectives.
-Home 22,893 13,070 131,556 5.75 4.4ùëí-4
Specifically,thesupervisedcontrastivelossfortuningthelanguage -Electronics 26,837 14,033 165,628 6.17 4.4ùëí-4
modelcanbeexpressedasfollows: -Tools 26,141 14,155 154,453 5.91 4.2ùëí-4
‚àëÔ∏Å
exp(cos(eùë¢,eùëñ )/ùúè)
TestData 55,877 28,988 615,210 11.01 -
L con=‚àí
(ùë¢,ùëñ pos,ùëñ
neg)log ‚àí(cid:205)
ùëö‚ààN
negexp(cosp (o es ùë¢,eùëö)/ùúè), (11) -- SS tp eo ar mts 2 21 3, ,4 37 16
0
12 5, ,7 24 31
7
1 33 12 6, ,4 10 90
0
16 3. .1 57
6
24 .. 68ùëí ùëí- -4
3
-Yelp 11,091 11,010 166,620 15.02 1.4ùëí-3
Here,ùúèisatemperaturehyperparameterthatcontrolsthedegreeof
contrastivelearning,andN negrepresentsthesetofin-batchnega-
Table2:ThespecificsofEasyRec‚Äôsmodelvariantsdifferbased
tiveitems,includingthenegativeitemùëñ negaswellasotheritemsin
ontheirmodelarchitectureandparametersizes.
thebatch,excludingthepositiveitemùëñ pos.Additionally,webuildon
priorwork[12,16]andincorporateanauxiliarymaskedlanguage Model Layers Hiddensize Heads Params
modeling(MLM)lossL .Thistechniquerandomlymasksinput
mlm EasyRec-Small 6 768 12 82M
tokensandtrainsthemodeltopredictthem,whichhelpsstabilize
EasyRec-Base 12 768 12 125M
thetrainingprocessandenhancethemodel‚Äôsgeneralizationability.
EasyRec-Large 24 1024 16 355M
Thefinaltrainingobjectiveisthecombinationofthesupervised
contrastivelossandtheMLMloss:
L=L con+ùúÜL mlm. (12)
H LLe Mre -, rP epùë¢/ hùëñ rare sp edre pse rn ot fis leth s,e wo ir ti hgi ùë°na inl dp ir co afi til ne, gw thhi ele nP umùë¢1 /‚àí bùëñùë° erde on fo dt ie vs et rh sie
-
whereùúÜisahyperparameterthatcontrolsthebalancebetweenthe ficationsteps.Duringtraining,werandomlyselectaprofilefrom
theprofilesetfortheuseroritemineachbatchofuser-itempairs.
contrastivelearningandthemaskedlanguagemodelingloss.
4 EVALUATION
3.3 AugmentationwithProfileDiversification
ThissectionevaluatestheperformanceoftheproposedEasyRec
Thegoalofourprofilediversificationapproachistoenhancethe
frameworkinaddressingthefollowingresearchquestions(RQs):
model‚Äôs ability to generalize to unseen users and items. Repre-
sentingeachuseroritemwithasingleprofileinherentlylimits ‚Ä¢ RQ1:HoweffectivelydoestheproposedEasyRecperformin
thediversityoftherepresentations,whichcannegativelyimpact matchingunseenusersanditems(zero-shot)withintext-based
themodel‚Äôsperformanceandgeneralization.Toaddressthis,we recommendationscenarios?
proposeaugmentingtheexistinguser/itemprofilestoallowfor ‚Ä¢ RQ2:HoweffectivelydoesEasyRecintegratewithandenhance
multipleprofilesperentity.Theseaugmentedprofilescapturethe variousrecommenderswithintext-basedcollaborativefiltering
samesemanticmeaning,suchasthepersonalizedinteractionpref- scenarios,leveragingitscapabilitiesasalanguageembedder?
erencesofusersorthevariedcharacteristicsofitems.Ourtwo ‚Ä¢ RQ3:Howeffectiveisourproposedprofilediversificationmech-
specificaugmentationmethodsintroducecontrolledvariationsin anismfordataaugmentationinimprovingtheperformanceof
theprofileswhilepreservingthecoresemanticmeaning. therecommendationlanguagemodel?
Inspiredbyself-instructionmechanisms[40,40],largelanguage
‚Ä¢ RQ4:Howwellcanourproposedtext-basedEasyRecparadigm
models(LLMs)canbeleveragedtorephraseuseroritemprofiles
adapttoaccommodatechangesinusers‚Äôdynamicpreferences?
whilepreservingtheirunderlyingmeaning.Thisallowsgenerating
multiplesemanticallysimilaryetdistinctlywordedprofilesfroma
4.1 ExperimentalSettings
singleinput.Applyingthisiterativerephrasingprocesscancreatea
diversesetofaugmentedprofiles,substantiallyexpandingtheavail- 4.1.1 Datasets. Toassessourproposedmodel‚Äôscapabilityinen-
abletrainingdata.Thisdataaugmentationtechniqueisparticularly codinguser/itemtextualprofilesintoembeddingsforrecommen-
valuablewhentheoriginaldatasetislimited,astheLLM-generated dation,wecurateddiversedatasetsacrossvariousdomainsand
profilescanimprovemodelgeneralizationandrobustness. platforms.Aportionwasusedfortraining,whiletheremainder
Byleveraginglargelanguagemodelsforprofilediversification, servedastestsetsforzero-shotevaluation.Thedatasetstatistics
wecancreateasetofdiverseprofilesforeachuserùë¢anditemùëñ. areshowninTable1.Duetothepagelimit,weplacethedetailof
dataresourcesaredescribedinAppendixA.2.1
{P}ùë¢ ={Pùë¢; Pùë¢1, Pùë¢2, ..., Pùë¢ùë° }, (13)
4.1.2 EvaluationProtocols. Weemploytwocommonlyused
{P}ùëñ ={Pùëñ; P ùëñ1, P ùëñ2, ..., P ùëñùë° }. (14) ranking-basedevaluationmetrics,Recall@ùëÅ andNDCG@ùëÅ,toACMConference,XXX,XXX XubinRenandChaoHuang
Table3:Text-basedrecommendationperformanceofvariouslanguagemodelsacrossdifferentdatasets.Thebestperformance
isindicatedinbold,whilethesecond-bestishighlightedwithunderline.Thescript‚àódenotessignificance(p<0.05).
Data Sports Steam Yelp
Methods Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20 Recall@10 Recall@20 NDCG@10 NDCG@20
ProprietaryModels
OpenAIv3-Small 0.0324 0.0444 0.0198 0.0230 0.0066 0.0119 0.0049 0.0068 0.0028 0.0056 0.0021 0.0031
OpenAIv3-Large 0.0300 0.0436 0.0180 0.0217 0.0070 0.0137 0.0049 0.0073 0.0029 0.0055 0.0023 0.0032
BaseSizeModels
BERT-Base 0.0015 0.0032 0.0008 0.0013 0.0015 0.0031 0.0011 0.0017 0.0009 0.0018 0.0006 0.0010
RoBERTa-Base 0.0121 0.0206 0.0065 0.0087 0.0041 0.0078 0.0031 0.0043 0.0018 0.0034 0.0014 0.0020
BART-Base 0.0088 0.0143 0.0048 0.0063 0.0033 0.0062 0.0024 0.0035 0.0014 0.0027 0.0012 0.0016
SimCSE-Base 0.0240 0.0341 0.0143 0.0170 0.0048 0.0091 0.0035 0.0050 0.0020 0.0039 0.0015 0.0022
BLaIR-Base 0.0251 0.0352 0.0145 0.0173 0.0041 0.0081 0.0030 0.0044 0.0025 0.0046 0.0019 0.0026
GTR-Base 0.0290 0.0417 0.0172 0.0206 0.0075 0.0136 0.0056 0.0077 0.0025 0.0052 0.0019 0.0029
BGE-Base 0.0315 0.0437 0.0194 0.0227 0.0094 0.0170 0.0069 0.0095 0.0029 0.0053 0.0022 0.0031
LargeSizeModels
BERT-Large 0.0011 0.0021 0.0006 0.0008 0.0019 0.0040 0.0014 0.0022 0.0008 0.0019 0.0007 0.0010
RoBERTa-Large 0.0039 0.0065 0.0021 0.0028 0.0027 0.0052 0.0021 0.0029 0.0012 0.0023 0.0010 0.0014
BART-Large 0.0114 0.0170 0.0064 0.0080 0.0036 0.0065 0.0026 0.0037 0.0017 0.0033 0.0014 0.0019
SimCSE-Large 0.0232 0.0328 0.0134 0.0160 0.0051 0.0095 0.0037 0.0052 0.0023 0.0044 0.0017 0.0025
BLaIR-Large 0.0227 0.0322 0.0133 0.0159 0.0057 0.0108 0.0041 0.0059 0.0027 0.0047 0.0019 0.0027
GTR-Large 0.0329 0.0446 0.0198 0.0229 0.0095 0.0168 0.0069 0.0094 0.0021 0.0042 0.0018 0.0025
BGE-Large 0.0324 0.0449 0.0196 0.0230 0.0089 0.0153 0.0066 0.0088 0.0025 0.0052 0.0020 0.0029
EasyRecSeries
EasyRec-Small 0.0186 0.0286 0.0108 0.0135 0.0097 0.0174 0.0070 0.0097 0.0022 0.0046 0.0017 0.0026
EasyRec-Base 0.0360 0.0518 0.0210 0.0253 0.0114 0.0203 0.0081 0.0112 0.0034 0.0063 0.0026 0.0037
EasyRec-Large 0.0396* 0.0557* 0.0236* 0.0279* 0.0129* 0.0225* 0.0093* 0.0127* 0.0034* 0.0065* 0.0026* 0.0037*
Improve ‚Üë20.36% ‚Üë24.05% ‚Üë19.19% ‚Üë21.30% ‚Üë35.79% ‚Üë32.35% ‚Üë39.13% ‚Üë33.68% ‚Üë17.24% ‚Üë16.07% ‚Üë13.04% ‚Üë15.63%
Table4:ComparisonofEasyRectrainingwithdifferentopti- BART[14];(ii)LanguageModelsforDenseRetrieval:SimCSE[7],
mizationobjectives(where"Contrast"standsforcontrastive). GTR[23],andBGE[38];(iii)Pre-trainedLanguageModelsforRec-
ommendation:BLaIR[12].Additionally,wealsocomparedagainst
Sports Yelp
Objective thestate-of-the-arttextembeddingmodelsprovidedbyOpenAI.
Recall@10 NDCG@10 Recall@10 NDCG@10
Thedetaileddescriptionsofthesebaselinemodelscanbefoundin
BPRLoss 0.0381 0.0226 0.0028 0.0021 AppendixA.3.Thisselectionoflanguagemodelscoversabroad
ContrastLoss 0.0395 0.0236 0.0034 0.0026
spectrum,fromgeneral-purposecontextualencoderstospecialized
modelstailoredfortaskslikedenseretrievalandrecommendation.
assessperformanceinbothtext-basedrecommendationandcollab-
orativefilteringscenarios.Specifically,wecomputethesemetrics
4.2.2 ResultAnalysis. Theoverallcomparisonofdifferentmod-
forùëÅvaluesof10and20[10,11].Theevaluationisconductedusing
elsispresentedinTable3.Thisevaluationrevealsseveralnotewor-
theall-rankprotocol[10]withthepredictedpreferencescores.In
thyobservations,whichareoutlinedbelow:
thecontextoftext-basedrecommendation,particularlyfordatasets
containingmultipleLLM-diversifiedprofiles(asdiscussedinSec- ‚Ä¢ SuperiorityacrossDiverseDatasets.Ourevaluationconsis-
tion3.3),wecalculatethemetricsseparatelyùë° timesbasedondif- tently shows that the EasyRec outperforms all other models
ferentprofilepairs(Pùë¢1,...,ùë°,P ùëñ1,...,ùë° ).Subsequently,wecomputethe acrossthethreedatasetsspanningdifferentplatforms.Thispro-
videsstrongevidencefortheeffectivenessoftheEasyRec.We
meanvalueforeachmetrictoobtainacomprehensiveassessment.
attributetheseimprovementstotwokeyfactors:i)Byinjecting
Forthetrainingdatasets,weutilizethevalidationsplitforevalua-
collaborativesignalsintothelanguagemodels,weeffectively
tion,whileforthetestdatasets,weemploythetestsplit.
optimizedourEasyRecusingsupervisedcontrastivelearning
4.2 PerformanceComparisionforText-based withintherecommendationcontext.Thisapproachallowsthe
modeltoinherentlyencodeuseranditemtextembeddingsthat
Recommendation(RQ1)
arewell-suitedforrecommendationtasks.ii)Byintegratinga
Weevaluatetheperformanceofvariouslanguagemodels(LMs)for
diversearrayofdatasetsacrossmultiplecategoriesandutilizing
zero-shottext-basedrecommendationontheunseenSports,Steam,
dataaugmentationtechniquestoenrichthetextdescriptionsfor
andYelpdatasets.Thisapproachdirectlyleveragestheencodedem-
training,ourEasyRecexhibitsimpressivegeneralizationcapabil-
beddingsderivedfromuser/itemprofilestomakerecommendations,
ities,enablingittoeffectivelyhandleunseendata.
withoutanyadditionaltrainingonthetargetdatasets.
‚Ä¢ ScalingLawInvestigationofEasyRecModel.Ourexperi-
4.2.1 BaselineMethodsandSettings. Forourcomparativeeval- mentsrevealedthatasthesizeoftheEasyRecmodelincreases
uation,weincludedadiversesetoflanguagemodelsastextembed- (from small to large), its performance consistently improves
ders:(i)GeneralLanguageModels:BERT[4],RoBERTa[21],and acrossallthreedatasets.Thisobservationreflectsascalinglaw,EasyRec:SimpleyetEffectiveLanguageModelsforRecommendation ACMConference,XXX,XXX
Table 5: Recommendation performance in text-enhanced
collaborativefiltering.Theexperimentwasconductedonthe 2.25
1.75 2.00
Steamdatasetwith5-runstoobtainthemeanresults. 2.20
1.95
1.70
Metric Recall NDCG small 1.90 base 2.15 large
@5 @10 @20 @5 @10 @20 2.10
0 1 2 3 0 1 2 3 0 1 2 3
Augmentation Count Augmentation Count Augmentation Count
ID-basedMethods
GCCF 0.0497 0.0826 0.1314 0.0555 0.0665 0.0830 (a)PerformanceonSteamdata
LightGCN 0.0518 0.0851 0.1349 0.0575 0.0686 0.0854
4.65
Text-enhancedGCCF 4.60 small 6.40 6.60 large
BERT 0.0500 0.0822 0.1313 0.0556 0.0663 0.0829 4.55 6.20
RoBERTa 0.0517 0.0848 0.1351 0.0573 0.0684 0.0854 6.40
4.50 6.00 base
BART 0.0529 0.0874 0.1383 0.0585 0.0701 0.0874
SimCSE 0.0529 0.0877 0.1395 0.0588 0.0706 0.0881 0 1 2 3 0 1 2 3 0 1 2 3
Augmentation Count Augmentation Count Augmentation Count
BLaIR 0.0535 0.0880 0.1392 0.0593 0.0708 0.0882
(b)PerformanceonYelpdata
GTR 0.0532 0.0873 0.1387 0.0592 0.0706 0.0880
BGE 0.0535 0.0875 0.1393 0.0591 0.0705 0.0881 Figure4:Performancewithrespecttodatasize."Augmenta-
EasyRec 0.0540 0.0881 0.1402 0.0597 0.0712 0.0888 tionCount"indicatesthenumberùë° ofdiversifiedprofiles.
Text-enhancedLightGCN ‚Ä¢ Comparedtothebackbonemethodsalone,theintegrationofthe
BERT 0.0518 0.0849 0.1347 0.0575 0.0684 0.0852 text-enhancedframeworkgenerallyimprovestheperformance
RoBERTa 0.0531 0.0867 0.1374 0.0587 0.0699 0.0870 forbothGCCFandLightGCN.Thisobservationhighlightsthe
BART 0.0540 0.0887 0.1407 0.0599 0.0715 0.0891
significanceofincorporatingtextmodalityintotherecommen-
SimCSE 0.0541 0.0891 0.1417 0.0602 0.0719 0.0898
dationparadigm,asitcanenhanceuseranditemprofiles.
BLaIR 0.0551 0.0897 0.1418 0.0609 0.0724 0.0901
‚Ä¢ AmongthevariousLMstested,EasyRecconsistentlyachieves
GTR 0.0542 0.0894 0.1417 0.0601 0.0719 0.0896
thehighestperformanceinthetext-enhancedrecommenders.
BGE 0.0547 0.0891 0.1407 0.0603 0.0718 0.0893
EasyRec 0.0554 0.0908 0.1430 0.0614 0.0732 0.0908 This outcome not only illustrates the efficacy of EasyRec for
recommendationtasks,butalsoemphasizestheadvantagesof
incorporatingcollaborativeinformationintolanguagemodels.
where the model‚Äôs performance growth is directly correlated
withitssize.Furthermore,thisfindingeffectivelyreinforcesthe 4.4 EffectivenessofProfileDiversification(RQ3)
validityoftext-basedrecommendationsystems.Italsovalidates
Inthissection,weexaminetheimpactofdiversifyinguseranditem
ourapproachtotrainingthelanguagemodel,whichenablesit
profileswithlargelanguagemodels(LLMs)onmodelperformance.
tolearncollaborativesignalsfromanewperspective.
AsmentionedinSection3.3,weperformLLM-baseddiversifica-
4.2.3 ImpactofTrainingObjectives. Furthermore,weevaluate tionthreetimesontheoriginalgeneratedprofiles.Thisprocess
theimpactofdifferenttrainingobjectivesonthelanguagemodel‚Äôs continuouslyincreasesthenumberofprofilesinthetrainingset.
learningprocess.Tothisend,weimplementedEasyRec-Largetrain- Toinvestigatewhetherdataaugmentationpositivelyaffectsmodel
ingusingBPRloss(i.e.,onenegativeitempertrainingsample)for performance,weconductexperimentswiththreevariantsofthe
comparisonwiththecontrastivelearningresults.Thisapproach EasyRecunderdifferentnumbersofdiversifiedprofiles.Theresults
allowsustodirectlyassesshowthechoiceoftrainingobjectiveinflu- areshowninFigure4,leadingtothefollowingkeyobservations:
encesmodelperformance.AsshowninTable4,theperformanceof ‚Ä¢ EffectivenessofProfileDiversification.Theincreaseinthe
themodeltrainedwithcontrastivelearninggenerallyoutperforms numberofdiversifiedprofiles(from0to3)enhancesmodelperfor-
thatofthemodeltrainedwithBPRloss.Thisoutcomehighlights mance,particularlyforlargermodels.Thisfindingunderscores
theeffectivenessofemployingcontrastivelearningtobetterincor- theeffectivenessofouraugmentationapproachusingLLMsfor
poratecollaborativeinformationintothelanguagemodels,thereby profilediversification,andemphasizesthesignificanceofincreas-
enhancingtheiroverallperformanceinrecommendationtasks. ingtrainingdataforimprovedoutcomes.
‚Ä¢ ScalingRelationship:Thescalingexperimentsonbothmodel
4.3 PerformanceofText-enhancedCF(RQ2) sizeanddatasizerevealacrucialrelationshipthatinfluences
Inadditiontoourinvestigationofzero-shotrecommendationsce- model performance. This demonstrates that our approach of
narios,weexplorethepotentialofEasyRecasanenhancement trainingthelanguagemodelwithcollaborativesignalsfollows
whenintegratedwithCFmodels.Toassesstheeffectivenessof ascalinglaw,indicatingthatmodelperformancebenefitsfrom
variousLMsinCF,weemploytwowidelyusedID-basedmethods bothincreasedcapacityanddatavolume.Suchscalinglawsare
asbackbonemodels:GCCF[2]andLightGCN[10],whichwere vitalastheyprovideinsightsintohowmodelcapacityanddata
chosenfortheirproveneffectivenessandefficiency.Furthermore, availabilityinteract,guidingfutureresearchanddevelopment.
weutilizetheadvancedmodel-agnostictext-enhancedframework
4.5 ModelFastAdaptationCaseStudy(RQ4)
RLMRec[25]withcontrastivealignmenttoconductourinvesti-
gation.WecomparethelargeversionsofbothEasyRecandother AsmentionedinSection3.1.3,akeyadvantageofEasyRecisitsabil-
open-sourceLMs.ThekeyfindingsfromtheresultsinTable5are: itytoempowerrecommendersystemstoefficientlyadapttoshifts
)2e1√ó(
02@llaceR
)3e1√ó(
02@llaceR
)2e1√ó(
02@llaceR
)3e1√ó(
02@llaceR
)2e1√ó(
02@llaceR
)3e1√ó(
02@llaceRACMConference,XXX,XXX XubinRenandChaoHuang
EasyRec Feature Space personalization[46].Forinstance,thegraphcollaborativefiltering
This user used to be a network[20]hasbeenintroducedtoaggregatebothcommonand
basketball fan. But now EasyRec
he likes swimming. domain-specificuserfeaturesusingGraphNeuralNetworks(GNNs)
Only thatcapturehigh-orderuser-itemconnections.Recentstudieshave
PreferenceShift Edit
furtherenrichedcross-domainrecommendersystemsthroughthe
Profile
integrationofself-supervisedlearningtechniques.Forexample,
This user is a basketball fan
and likes to play basketball EasyRec C2DSR[1]utilizescontrastivelearningwithbothsingle-domainand
and watch NBA games.
cross-domainrepresentations.CCDR[39]proposesintra-domain
andinter-domaincontrastivelearningtoenhancerecommendation
Matched Top Items (Before) Matched Top Items (After)
performance.Moreover,SITN[29]employsself-attentionmodules
1. Spalding NBA Zi/O Excel Basketball 1. IspeedMen‚Äôs Competition Jammer Swimsuit
2. Spalding NBA Street Basketball 2. Speedo Swedish Two-Pack Swim Goggles assequenceencoderstorepresenttwouser-specificsequencesfrom
3. Spalding NBA Zi/O Indoor/Outdoor Basketball‚Äì 3. FINIS Foam Pull Buoy
Official Size 7 4. Speedo Men‚Äôs Endurance+ Polyester Solid thesourceandtargetdomains,followedbycontrastivelearning
4. Spalding NBA Tack Soft Basketball Jammer Swimsuit
betweenthem.However,asignificantlimitationincurrentcross-
Figure5:CasÔøΩeÔøΩÔøΩ studyonhandlinguserpreÔøΩfÔøΩÔøΩerenceshift. domainresearchisitsrelianceoncorrelations(e.g.,overlapping
users) between source and target data, which constrains its ap-
inuserpreferencesandbehaviordynamicsovertime.Toevaluate plicabilityandgeneralizability.Incontrast,text-basedzero-shot
thiscapability,wecreatetwouserprofilesreflectingshiftedprefer- learningdoesnothassuchconstraint,allowingforeffectivetrans-
encesontheAmazon-Sportdatasetandexaminetherecommended ferevenacrossdifferentdatasets.OurproposedEasyRectakesa
itemsfromtheEasyRec.AsshowninFigure5,theoriginaluser fundamentalstepforwardinthislineofresearch.
profileindicatesthattheuserenjoysplayingbasketball.However, GraphCollaborativeFilteringforRecommendation.Graph
theuser‚Äôspreferencelatertransitionstoapreferenceforswimming. NeuralNetworkshaverecentlyemergedasapromisingapproach
Wevisualizealltheencodedembeddingsusingt-SNE[31],which forrecommendationsystems,enablingthemodelingofcomplex
revealsasignificantshiftintheuserembeddingswithinthefea- user-iteminteractionsandcapturinghigh-orderdependenciesin
turespace.Correspondingly,recommendeditemstransitionfrom recommendation[6].Thesemodelsarecapableoflearningrep-
basketball-relatedproductstoswimminggear,reflectingtheuser‚Äôs resentationsofusersanditemsbyaggregatingtheirinteractions
changingpreferences.Notably,thisadjustmentisaccomplished inagraphstructure,suchasPinSage[43],NGCF[32],andLight-
solelybymodifyingtheuser‚Äôsprofile,withouttheneedforfurther GCN[10].ToenhancetherepresentationcapacityofGNNsagainst
trainingofthemodel.Thisunderscorestheefficiencyandflexibility datasparsityinrecommendersystems,furtherstudiesrealizethe
ofourapproachinadaptingtoevolvinguserpreferences. marriagebetweenself-supervisedlearningandcollaborativefilter-
ingwithdataaugmentation.ExamplesincludeSGL[34],SimGCL[44],
5 RELATEDWORK
andHCCF[37].Thesedevelopmentsshowcasethepotentialofself-
LMs-PoweredRecommenderSystems.Recentadvancements supervised graph learning in powering recommenders through
inrecommendersystemsincreasinglyincorporatetextualmodal- graphaugmentationbasedonnodeself-discrimination.
ities [45], thus enhancing traditional approaches. The semantic
representationsencodedbypre-trainedlanguagemodelsareessen- 6 CONCLUSION
tialfeaturesforimprovingrecommendermodels,particularlyin
TheEasyRecframeworkeffectivelyintegratesLMstoenhancerec-
click-throughrateprediction[8,35]andtransferablesequencerec-
ommendationtasks.Ournewparadigm,whichisbothstraightfor-
ommendations[13].Theseembeddingscaptureinformativecontent
wardandeffective,hasconsistentlyproventoexcelacrossvari-
relevanttorecommendationtasks[28].Someworksalsoleverage
ousscenarios,includingtext-basedzero-shotrecommendationand
text-basedagentstoenhanceperformance[47,48].Anotablere-
text-enhancedcollaborativefiltering.AttheheartofEasyRec‚Äôssuc-
centcontributionisRLMRec[25],atext-enhancedframeworkthat
cessliesaninnovativemethodologythatcombinescollaborative
improvesID-basedrecommendersusingprinciplesfrominforma-
languagemodeltuningwiththetransformativecapabilitiesofcon-
tiontheory.However,manypriorstudieshavereliedongeneral
trastivelearning.ThisuniqueapproachhasempoweredEasyRec
textembeddings,suchasBERT-basedmodels[5,45]orproprietary
to capture nuanced semantics and high-order collaborative sig-
OpenAIembeddings[25],ratherthanthosespecificallytailored
nals‚Äîcriticalelementsthathavebeeninstrumentalindrivingre-
for recommendation purposes. Arecent work BLaIR [12] lever-
markableimprovementsinrecommendationperformance.Ourex-
agestheitemmetadataandinteraction-leveluserfeedbackonthis
tensiveexperiments,whichspanadiversearrayofdatasets,have
itemforLMstraining,yieldingpromisingresultsinquery-based
consistently validated the superiority of EasyRec over existing
itemretrieval.Incontrast,ourapproachassignseachuseranditem
languagemodels.Thisrobustandgeneralizedperformanceunder-
acollaborativelygeneratedprofilethatreflectstheirpreferences.
scorestheframework‚Äôsremarkablecapacitytoadapttodynamic
EasyRecoptimizesthelearnedcorrelationsbetweenuseranditem
userpreferences,makingitwell-suitedforreal-worldindustrysce-
entitiesusingCFsignals,whichnotonlydemonstratesimpressive
narios.Furthermore,theconsistentimprovementsobservedacross
zero-shotperformancebutalsoenhancestext-augmentedresults.
differentsettingsindicatethatEasyRecisnotonlyeffectivebut
Cross-DomainRecommendation.Thefundamentalconceptbe-
alsoversatileinitsapplication.Lookingahead,thepotentialfor
hindcross-domainrecommendationistoenhancerecommenda-
EasyRectobeseamlesslyintegratedwithmulti-modalinformation
tionsinonedomainbyleveragingdatafromanotherdomain,which
presentsanenticingfrontierforourfutureinvestigations.
istypicallymoreabundant,toaddressdatasparsityandimproveEasyRec:SimpleyetEffectiveLanguageModelsforRecommendation ACMConference,XXX,XXX
REFERENCES
forrecommendation.InWWW.3464‚Äì3475.
[1] JiangxiaCao,XinCong,JiaweiSheng,TingwenLiu,andBinWang.2022.Con- [26] XubinRen,LianghaoXia,YuhaoYang,WeiWei,TianleWang,XuhengCai,and
trastiveCross-DomainSequentialRecommendation.InCIKM.138‚Äì147. ChaoHuang.2024.Sslrec:Aself-supervisedlearningframeworkforrecommen-
[2] LeiChen,LeWu,RichangHong,KunZhang,andMengWang.2020.Revisiting dation.InWSDM.567‚Äì575.
graphbasedcollaborativefiltering:Alinearresidualgraphconvolutionalnetwork [27] SteffenRendle,ChristophFreudenthaler,ZenoGantner,andLarsSchmidt-Thieme.
approach.InAAAI,Vol.34.27‚Äì34. 2012.BPR:Bayesianpersonalizedrankingfromimplicitfeedback.arXivpreprint
[3] MaurizioFerrariDacrema,Iv√°nCantador,IgnacioFern√°ndez-Tob√≠as,Shlomo arXiv:1205.2618(2012).
Berkovsky,andPaoloCremonesi.2012.Designandevaluationofcross-domain [28] LehengSheng,AnZhang,YiZhang,YuxinChen,XiangWang,andTat-Seng
recommendersystems.InRecommenderSystemsHandbook.Springer,485‚Äì516. Chua.2024.LanguageModelsEncodeCollaborativeSignalsinRecommendation.
[4] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert: arXivpreprintarXiv:2407.05441(2024).
Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.arXiv [29] GuoqiangSun,YibinShen,SijinZhou,XiangChen,HongyanLiu,ChunmingWu,
preprintarXiv:1810.04805(2018). ChenyiLei,XianhuiWei,andFeiFang.2023.Self-SupervisedInterestTransfer
[5] HaoDing,YifeiMa,AnoopDeoras,YuyangWang,andHaoWang.2021.Zero- NetworkviaPrototypicalContrastiveLearningforRecommendation. arXiv
shotrecommendersystems.arXivpreprintarXiv:2105.08318(2021). preprintarXiv:2302.14438(2023).
[6] ChenGao,YuZheng,NianLi,YinfengLi,YingrongQin,JinghuaPiao,Yuhan [30] JianingSun,ZhaoyueCheng,SabaZuberi,FelipeP√©rez,andMaksimsVolkovs.
Quan,JianxinChang,DepengJin,XiangnanHe,etal.2023.Asurveyofgraph 2021.Hgcf:Hyperbolicgraphconvolutionnetworksforcollaborativefiltering.
neuralnetworksforrecommendersystems:Challenges,methods,anddirections. InWWW.593‚Äì601.
ACMTORS1,1(2023),1‚Äì51. [31] LaurensVanderMaatenandGeoffreyHinton.2008.Visualizingdatausingt-SNE.
[7] TianyuGao,XingchengYao,andDanqiChen.2021.Simcse:Simplecontrastive JMLR9,11(2008).
learningofsentenceembeddings.arXivpreprintarXiv:2104.08821(2021). [32] XiangWang,XiangnanHe,MengWang,FuliFeng,andTat-SengChua.2019.
[8] BinzongGeng,ZhaoxinHuan,XiaoluZhang,YongHe,LiangZhang,FajieYuan, Neuralgraphcollaborativefiltering.InSIGIR.165‚Äì174.
JunZhou,andLinjianMo.2024.Breakingthelengthbarrier:Llm-enhancedCTR [33] YinweiWei,XiangWang,QiLi,LiqiangNie,YanLi,XuanpingLi,andTat-Seng
predictioninlongtextualuserbehaviors.InSIGIR.2311‚Äì2315. Chua.2021.Contrastivelearningforcold-startrecommendation.InMM.5382‚Äì
[9] BowenHao,JingZhang,HongzhiYin,CuipingLi,andHongChen.2021.Pre- 5390.
traininggraphneuralnetworksforcold-startusersanditemsrepresentation.In [34] JiancanWu,XiangWang,FuliFeng,XiangnanHe,LiangChen,JianxunLian,and
WSDM.265‚Äì273. XingXie.2021.Self-supervisedgraphlearningforrecommendation.InSIGIR.
[10] XiangnanHe,KuanDeng,XiangWang,YanLi,YongdongZhang,andMeng 726‚Äì735.
Wang.2020.Lightgcn:Simplifyingandpoweringgraphconvolutionnetworkfor [35] YunjiaXi,WeiwenLiu,JianghaoLin,XiaolingCai,HongZhu,JiemingZhu,Bo
recommendation.InSIGIR.639‚Äì648. Chen,RuimingTang,WeinanZhang,RuiZhang,etal.2023.Towardsopen-world
[11] XiangnanHe,LiziLiao,HanwangZhang,LiqiangNie,XiaHu,andTat-Seng recommendationwithknowledgeaugmentationfromlargelanguagemodels.
Chua.2017.Neuralcollaborativefiltering.InWWW.173‚Äì182. arXivpreprintarXiv:2306.10933(2023).
[12] YupengHou,JiachengLi,ZhankuiHe,AnYan,XiusiChen,andJulianMcAuley. [36] LianghaoXia,ChaoHuang,ChunzhenHuang,KangyiLin,TaoYu,andBen
2024. Bridginglanguageanditemsforretrievalandrecommendation. arXiv Kao.2023.Automatedself-supervisedlearningforrecommendation.InWWW.
preprintarXiv:2403.03952(2024). 992‚Äì1002.
[13] YupengHou,ShanleiMu,WayneXinZhao,YaliangLi,BolinDing,andJi-Rong [37] LianghaoXia,ChaoHuang,YongXu,JiashuZhao,DaweiYin,andJimmyHuang.
Wen.2022.Towardsuniversalsequencerepresentationlearningforrecommender 2022.Hypergraphcontrastivecollaborativefiltering.InSIGIR.70‚Äì79.
systems.InKDD.585‚Äì593. [38] ShitaoXiao,ZhengLiu,PeitianZhang,andNiklasMuennighof.2023.C-pack:
[14] MikeLewis,YinhanLiu,NamanGoyal,MarjanGhazvininejad,Abdelrahman Packagedresourcestoadvancegeneralchineseembedding. arXivpreprint
Mohamed,OmerLevy,VesStoyanov,andLukeZettlemoyer.2019.Bart:Denoising arXiv:2309.07597(2023).
sequence-to-sequencepre-trainingfornaturallanguagegeneration,translation, [39] RuobingXie,QiLiu,LiangdongWang,ShukaiLiu,BoZhang,andLeyuLin.2022.
andcomprehension.arXivpreprintarXiv:1910.13461(2019). Contrastivecross-domainrecommendationinmatching.InKDD.4226‚Äì4236.
[15] JuanhuiLi,HaoyuHan,ZhikaiChen,HarryShomer,WeiJin,AminJavari,and [40] Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng,
JiliangTang.2024. EnhancingIDandTextFusionviaAlternativeTrainingin ChongyangTao,andDaxinJiang.2024.Wizardlm:Empoweringlargelanguage
Session-basedRecommendation.arXivpreprintarXiv:2402.08921(2024). modelstofollowcomplexinstructions.InICLR.
[16] JiachengLi,MingWang,JinLi,JinmiaoFu,XinShen,JingboShang,andJulian [41] ShuyuanXu,YingqiangGe,YunqiLi,ZuohuiFu,XuChen,andYongfengZhang.
McAuley.2023. Textisallyouneed:Learninglanguagerepresentationsfor 2023.Causalcollaborativefiltering.InSIGIR.235‚Äì245.
sequentialrecommendation.InKDD.1258‚Äì1267. [42] MenglinYang,MinZhou,JiahongLiu,DefuLian,andIrwinKing.2022.HRCF:
[17] JiayiLiao,SihangLi,ZhengyiYang,JiancanWu,YanchengYuan,XiangWang, Enhancingcollaborativefilteringviahyperbolicgeometricregularization.In
andXiangnanHe.2024. Llara:Largelanguage-recommendationassistant.In WWW.2462‚Äì2471.
SIGIR.1785‚Äì1795. [43] RexYing,RuiningHe,KaifengChen,PongEksombatchai,WilliamLHamilton,
[18] XixunLin,JiaWu,ChuanZhou,ShiruiPan,YananCao,andBinWang.2021. andJureLeskovec.2018. Graphconvolutionalneuralnetworksforweb-scale
Task-adaptiveneuralprocessforusercold-startrecommendation.InWWW. recommendersystems.InKDD.974‚Äì983.
1306‚Äì1316. [44] JunliangYu,HongzhiYin,XinXia,TongChen,LizhenCui,andQuocVietHung
[19] ZihanLin,ChangxinTian,YupengHou,andWayneXinZhao.2022.Improving Nguyen.2022.Aregraphaugmentationsnecessary?simplegraphcontrastive
graphcollaborativefilteringwithneighborhood-enrichedcontrastivelearning. learningforrecommendation.InSIGIR.1294‚Äì1303.
InWWW.2320‚Äì2329. [45] ZhengYuan,FajieYuan,YuSong,YouhuaLi,JunchenFu,FeiYang,Yunzhu
[20] MengLiu,JianjunLi,GuohuiLi,andPengPan.2020.Crossdomainrecommen- Pan,andYongxinNi.2023.Wheretogonextforrecommendersystems?id-vs.
dationviabi-directionaltransfergraphcollaborativefilteringnetworks.InCIKM. modality-basedrecommendermodelsrevisited.InSIGIR.2639‚Äì2649.
885‚Äì894. [46] TianziZang,YanminZhu,HaobingLiu,RuohanZhang,andJiadiYu.2022.A
[21] YinhanLiuetal.2019.Roberta:Arobustlyoptimizedbertpretrainingapproach. surveyoncross-domainrecommendation:taxonomies,methods,andfuture
arXivpreprintarXiv:1907.11692(2019). directions.ACMTOIS41,2(2022),1‚Äì39.
[22] JianmoNi,JiachengLi,andJulianMcAuley.2019.Justifyingrecommendations [47] AnZhang,YuxinChen,LehengSheng,XiangWang,andTat-SengChua.2024.
usingdistantly-labeledreviewsandfine-grainedaspects.InEMNLP-IJCNLP. Ongenerativeagentsinrecommendation.InSIGIR.1807‚Äì1817.
188‚Äì197. [48] JunjieZhang,YupengHou,RuobingXie,WenqiSun,JulianMcAuley,WayneXin
[23] JianmoNi,ChenQu,JingLu,ZhuyunDai,GustavoHern√°ndez√Åbrego,JiMa, Zhao,LeyuLin,andJi-RongWen.2024.Agentcf:Collaborativelearningwith
VincentYZhao,YiLuan,KeithBHall,Ming-WeiChang,etal.2021.Largedual autonomouslanguageagentsforrecommendersystems.InWWW.3679‚Äì3689.
encodersaregeneralizableretrievers.arXivpreprintarXiv:2112.07899(2021). [49] YangZhang,FuliFeng,JizhiZhang,KeqinBao,QifanWang,andXiangnanHe.
[24] AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,Gregory 2023.Collm:Integratingcollaborativeembeddingsintolargelanguagemodels
Chanan,TrevorKilleen,ZemingLin,NataliaGimelshein,LucaAntiga,etal.2019. forrecommendation.arXivpreprintarXiv:2310.19488(2023).
Pytorch:Animperativestyle,high-performancedeeplearninglibrary.NeurIPS [50] YidingZhang,ChaozhuoLi,XingXie,XiaoWang,ChuanShi,YumingLiu,Hao
32(2019). Sun,LiangjieZhang,WeiweiDeng,andQiZhang.2022.Geometricdisentangled
[25] XubinRen,WeiWei,LianghaoXia,LixinSu,SuqiCheng,JunfengWang,Dawei collaborativefiltering.InSIGIR.80‚Äì90.
Yin,andChaoHuang.2024.RepresentationlearningwithlargelanguagemodelsACMConference,XXX,XXX XubinRenandChaoHuang
Table6:Costsforuseranditemprofilegenerationanddiver- Theinstructionpromptsforgenerationthatweutilizedreference
sification,includingthreeiterationsofdiversification. previousresearch[25]toconductefficientuseranditemprofiling.
Theprofilingprocessadoptsanitem-to-userparadigm,wherewe
Operation #Data #In.Tokens #Out.Tokens #Cost($)
firstgenerateitemprofilesinparallelusingmulti-threadprocessing,
Generation 7 169M 20M ‚àº114 followedbytheparallelgenerationofuserprofilesthatincorporate
Diversification 9 102M 30M ‚àº97 collaborativeinformation.Thelargelanguagemodelemployedis
GPT-3.5-TurbofromOpenAI.
Specifically,weprocessdatasetsfromAmazonreviewdata[22],
A APPENDIX
whichincludethecategoriesArts,Movies,Games,Home,Electron-
A.1 ImplementationandTrainingDetails ics,Tools,andSports.Foritemprofilegeneration,eachitemùëñ
WeimplementedourEasyRecandconductedallexperimentsus- includesatitle‚Ñé ùëñ andanoriginaldescriptionùëë ùëñ.Weleveragethese
ing PyTorch [24]. For the transformer-based encoder backbone, twopiecesofinformationtogeneratetheprofile,asdescribedon
weadoptedthearchitectureofRoBERTa[21]andutilizeditspre- theleft-handsideofEq.4.Next,foruserprofilegeneration,we
trainedparametersasinitialization.Wetrainedthreeversionsof uniformlysampleamaximumoffiveinteracteditemsfromeach
EasyRecwithvaryingparametersizes(small,base,andlarge),as user‚Äôsbehaviorhistoryascollaborativeinformation.Wethenar-
detailedinTable2.Forthelossfunction,wesetthehyperparam- rangetheinputpromptforthelargelanguagemodelaccording
etersùúè to0.05andùúÜto0.1.Thetokenmaskingratioformasked toEq.5.Allpromptsandinstructionsforuseranditemprofile
languagemodelingis0.15,andthelearningrateissetto5√ó10‚àí5. generationareprovidedinthecodeforreference.
Wetrainthemodelfor25epochs.Forprofileaugmentation,weset
A.2.3 DetailsofProfileDiversification. AsdescribedinSec-
thediversificationtimetforLLM-basedmethodsto3,resulting
tion3.3,wealsoconductprofilediversificationusinglargelanguage
in4userprofilesand5itemprofilesperdataset.Duringtraining,
models(LLMs)toenhancethediversityofthetrainingandtest
weevaluatethemodelevery1000stepsandusethevalidationin-
datasets,therebyimprovingandbetterevaluatingthemodel‚Äôsgen-
teractionsfromeachtrainingdatasettoselecttheoptimalmodel
eralizationabilityacrossdifferentuseranditemprofiles.Foreach
parameters,employingtheRecall@20metric.Detailedimplemen- useroritem,weperformùë°iterationsofdiversificationstartingfrom
tationofourmodelisprovidedinouranonymousreleasedcode.
theinitiallygeneratedprofile.Thismeansthatweobtainthefirst
diversifiedprofilebasedontheoriginalprofileandthenusethis
A.2 DatasetsandUser/ItemProfiles
diversifiedprofileforfurtherdiversificationwiththeLLMs.
Inthissection,weprovidedetailedinformationonthedatasets, Forreference,examplesofuseranditemprofilediversification
aswellastheprocessesforprofilegenerationanddiversification, areprovidedinFigureAandFigureA,respectively.Asillustratedin
includinginstructions,examples,andassociatedcosts. thecaseofuserprofilediversification,theprofilesforthesameuser
differatthewordlevelwhilestillrepresentingthesamepreferences.
A.2.1 DetailsofDataset. WeutilizedatasetsfromAmazonre-
Suchdiversificationsignificantlyenhancesthediversityandquality
viewdata[22]acrosssixcategoriestoformthetrainingdata:Arts,
ofthetextualdatawhilealsoincreasingtheoveralldatasetsize.
CraftsandSewing(Arts),MoviesandTV (Movies),VideoGames
(Games),HomeandKitchen(Home),Electronics(Electronics),and A.2.4 CostofGenerationandDiversification. Wesummarize
ToolsandHomeImprovement(Tools).Forthetestdatasets,weuse thetotalnumberoftokensandtheassociatedcostsforutilizing
onedomain,SportsandOutdoors(Sports),fromtheAmazonre- theproprietarymodelforprofilegenerationanddiversification
viewdata,alongwithtwocross-platformdatasets:SteamandYelp, inTable6.ItisworthnotingthattheprofilesfortheSteamand
forcomprehensiveevaluation.ForthedatasetsfromtheAmazon Yelpdatasetshavealreadybeengenerated;therefore,thenumber
platform,wefirstfilterthedatatoincludeonlythosewitharating ofprofileddatasetsanddiversifieddatasetsdiffers.Asshownin
scoregreaterthan3andapplya10-corefilteringtodensifythe theresults,thetotalnumberoftokensrequiredtoprocessboth
dataset.Subsequently,foreachcategory,wesplittheinteractions profilegenerationanddiversificationisapproximately322million,
intotraining,validation,andtestsplitsinaratioof8:1:1.Incontrast, includingbothinputandoutputtokens.Thisgenerallyincursa
fortheSteamandYelpdatasets,wedirectlyusethedataprocessed costofaround200dollarswithGPT-3.5-TurboAPItoprocessthe
inpreviouswork[25],whichmaintainsasplitratioof3:1:1. entiredataset,makingitanaffordableoption.
A.2.2 DetailsofProfileGeneration. Afterdataprocessing,each
A.3 DetailsofText-basedRecommendation
datasetcontainsasplitoftraininginteractions.Weusetheseinter-
actionstogenerateuseranditemprofilesfollowingtheparadigm A.3.1 BaselineModels. Inthissection,weprovideadetailed
describedinSection3.1,asthisrequiresuser-iteminteractionin- descriptionofthelanguagemodelscomparedinthiswork.
formation.FortheSteamandYelpdatasets,wedirectlyusethe (i)GeneralLanguageModels.
providedprofiles,whichadheretothesamegenerationprotocol.It ‚Ä¢ BERT[4]:Alandmarktransformer-basedmodelrenownedfor
isimportanttonotethat,foreachdataset,theprofilesaregener- stronglanguageunderstandingthroughbidirectionaltraining.
atedexclusivelybasedonthetraininginteractions.Thisapproach WeusethepooledBERToutputasthetextembedding.
ensuresthatthevalidationandtestinteractionsarereservedfor ‚Ä¢ RoBERTa[21]:AnoptimizedBERTthatemploysdynamicmask-
evaluationpurposes,preventingdataleakageandallowingfora ingandlargerdatasets.Weusethefinal[CLS]tokenembedding.
more accurate assessment of the model‚Äôs generalization perfor-
manceonunseenrecommendationdata.EasyRec:SimpleyetEffectiveLanguageModelsforRecommendation ACMConference,XXX,XXX
User Profile Diversification
Instruction
You will assist me in revising a user‚Äôs profile while maintaining its original meaning. I will present you
with the user‚Äôs initial profile.
Instructions:
USER PROFILE: The original user profile.
Requirements:
1. Please provide the revised profile directly begin with "REVISED PROFILE: ".
2. The rephrased profile should minimize duplication with the original text while preserving its intended
meaning.
3. The revised profile should exhibit varied sentence structures while faithfully conveying the original
profile‚Äôs essence.
InputPrompt
USER PROFILE: This user is likely to enjoy items related to baking, entertaining, and colorful table settings.
They appreciate convenience, efficiency, and practicality in kitchen appliances.
Response
REVISED PROFILE: An individual who finds pleasure in baking, hosting gatherings, and vibrant table
arrangements. They value kitchen devices that offer convenience, efficiency, and practicality.
Figure6:Anexampleoflargelanguagemodels-baseduserprofilediversification.
Table7:Detailsofcomparedlanguagemodels. ‚Ä¢ BGE[38]:BGEisastate-of-the-artfamilyofwell-trainedmodels
Model Pre-trainedWeights(FromHuggingFace) forgeneraltextembeddingmodels.WeutilizetheEnglishversion
ofthismodelforevaluation.
BERT-Base google-bert/bert-base-uncased
BERT-Large google-bert/bert-large-uncased (iii)LangaugeModelsforRecommendation.
BART-Base facebook/bart-base ‚Ä¢ BLaIR[12]:Aseriesofsentenceembeddingmodelsforrecom-
BART-Base facebook/bart-large mendation.BLaIRlearnsinteraction-levelcorrelationsbetween
RoBERTa-Base FacebookAI/roberta-base itemmetadataanduserfeedbackonthisitem,improvingquery-
RoBERTa-Large FacebookAI/roberta-large
baseditemretrievalandalsorecommendation.
SimCSE-Base princeton-nlp/sup-simcse-roberta-base
The pre-trained weights utilized for each baseline language
SimCSE-Large princeton-nlp/sup-simcse-roberta-large
BLaIR-Base hyp1231/blair-roberta-base model are listed in Table 7. For proprietary embedding models
BLaIR-Large hyp1231/blair-roberta-large namedOpenAIv3,weusethelatesttext-embedding-3-small
GTR-Base sentence-transformers/gtr-t5-base andtext-embedding-3-largefromOpenAIasbaselines.
GTR-Large sentence-transformers/gtr-t5-large
A.3.2 DetailofBaselineSettings. Giventhattheexperiment
BGE-Base BAAI/bge-base-en-v1.5
BGE-Large BAAI/bge-large-en-v1.5 wasconductedinazero-shotsetting,wherethetestdataremained
unseenduringtrainingforbothourEasyRecandtheotherlan-
guagemodelbaselines,wedirectlyutilizedtheoriginalreleased
‚Ä¢ BART[14]:Adenoisingautoencodertransformertrainedon
parametersofthesebaselinesfromHuggingFaceforinitialization
corruptedtextreconstruction.Weapplymeanpoolingonthe
andcomparison.Besides,forBGE,weaddedtherecommended
lasthiddenstateforthetextembedding.
retrievalinstructioninfrontoftheuserprofiles,followingthepro-
(ii)LanguageModelsforDenseRetrieval.
videdguidelinesfromtheopen-sourcecode,aswefounditoffered
‚Ä¢ SimCSE[7]:Aframeworkthatleveragescontrastivelearning betterperformanceinzero-shotrecommendations.Additionally,
togeneratehigh-qualitysentenceembeddings,enhancingthe weappliednormalizationtotheoutputsfromthelanguagemodels
model‚Äôsabilitytodiscernsemanticsimilaritybetweensentences. toderivetextembeddingsandcomputedcosinesimilarityusing
‚Ä¢ GTR[23]:GTRisageneralizableT5-baseddenseretrieverthat thedotproductoftheuseranditemembeddings.
improvesretrievaltasksacrossvariousdomainsbyovercoming
limitationsoftraditionaldualencoders.ACMConference,XXX,XXX XubinRenandChaoHuang
Item Profile Diversification
Instruction
You will assist me in revising a item‚Äôs profile while maintaining its original meaning. I will present you
with the item‚Äôs initial profile.
Instructions:
ITEM PROFILE: The original item profile.
Requirements:
1. Please provide the revised profile directly begin with "REVISED PROFILE: ".
2. The rephrased profile should minimize duplication with the original text while preserving its intended
meaning.
3. The revised profile should exhibit varied sentence structures while faithfully conveying the original
profile‚Äôs essence.
InputPrompt
ITEM PROFILE: The Innovee Lemon Squeezer is a high-quality stainless steel manual citrus press that comes
with a lemon recipes ebook. Ideal for those who enjoy fresh lemon juice and recipes.
Response
REVISED PROFILE: The Innovee Lemon Squeezer is a stainless steel manual citrus press that includes a lemon
recipes ebook, perfect for individuals who appreciate the taste of freshly squeezed lemon juice and love
trying out new recipes.
Figure7:Anexampleoflargelanguagemodels-baseditemprofilediversification.