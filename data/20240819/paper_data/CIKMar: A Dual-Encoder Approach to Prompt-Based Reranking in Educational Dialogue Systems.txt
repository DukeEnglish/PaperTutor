CIKMar: A Dual-Encoder Approach to Prompt-Based Reranking in
Educational Dialogue Systems
JoanitoAgiliLopo and MarinaIndahPrasasti and AlmaPermatasari
DepartmentofComputerScienceandEngineering
UniversitasGadjahMada
name@mail.ugm.ac.id
Abstract
Teacher : “ keywords they focus on in that page i mean ”
In this study, we introduce CIKMar1, an effi- Student : “ OK, I'll do it now ”
cientapproachtoeducationaldialoguesystems Teacher : “ ok ”
poweredbytheGemmaLanguagemodel. By S t u d e n t : “ _ m__ ep sr so av go ec sa t aiv be o, u a tn pd s s yu cop lp ol gie yd _ _+ _s ”tartling+
leveragingaDual-Encoderrankingsystemthat
Student : “ Is it enough this piece of text to work on it? the word I
incorporates both BERT and SBERT model, don't konw is startling”
we have designed CIKMar to deliver highly Teacher : “ ok thanks....'startling' does it mean: obvoius or
surprising do you think? is there a context in the
relevantandaccurateresponses,evenwiththe text to help you? ”
constraints of a smaller language model size.
Teacher : “ obvious ”
OurevaluationrevealsthatCIKMarachieves
a robust recall and F1-score of 0.70 using REFERENCE
BERTScoremetrics. However,wehaveidenti- RESPONSE
fiedasignificantchallenge: theDual-Encoder Figure1: TeacherContinuationDataVisualization
tends to prioritize theoretical responses over
practicalones. Thesefindingsunderscorethe
potentialofcompactandefficientmodelslike tasks presents several crucial challenges such as
Gemmaindemocratizingaccesstoadvanced
inconsistentlydeliveringaccurateandcontextually
educationalAIsystems,ensuringeffectiveand
appropriateresponses(Tacketal.,2023). Further-
contextuallyappropriateresponses.
more,Languagemodelsincurrentscenariosmostly
1 Introduction use extremely large language models in terms of
their parameter size, such as proprietary 175 and
TheemergenceofpowerfulLargeLanguageMod-
137billion-parameterGPT-3model(Brownetal.,
els(LLMs)suchasChatGPThasbeenprovenef-
2020), or open source LLMs such as 70 billion-
fectiveinvarioustasks,includinggeneratingtext
parameter LLaMA2 (Touvron et al., 2023), 14
thatisnearlyindistinguishablefromhuman-written
billion-parameter Qwen (Bai et al., 2023), and 6
text(Kasnecietal.,2023;OmidvarandAn,2023).
billion-parameterChatGLM3models(Zengetal.,
Buildingonthesuccessintextgeneration,LLMs
2023). Languagemodelsatthisscalearenotpracti-
haveshownsignificantpotentialinvariousapplica-
calandinaccessibleformanyresearchersandeven
tions,especiallyintheeducationaldomain.
practitioners,duetotheirlargememoryconsump-
Inrecentyears,therehavebeenvariousefforts
tionandslowgenerationtimes(Dingetal.,2024;
to utilize these powerful large language models
JimenezGutierrezetal.,2022),dataprivacy,and
(LLMs) in education. They have been deployed
inflexibilityofcustomization(Sinhaetal.,2024).
inteacher-studentcollaborationsasvirtualtutors,
Therefore, it is essential to determine how solid
guidingstudentsthroughexercises,offeringperson-
thatfoundationisandhowitcanbeaccessiblefor
alizedlearningexperiences,andprovidingintelli-
furtherdevelopment,especiallyintheeducational
genttutoring(Kamalovetal.,2023). Additionally,
domain.
theyareusedforadaptiveassessmentsandserveas
Accordingtothechallengesabove,wedesigned
conversationalpartnersinlearningscenarios(Tan
asimplebuteffectiveapproachbyleveragingLarge
etal.,2023;Lietal.,2024).
LanguageModelsandprompt-and-rerankapproach
Despitethesepromisingopportunities,theuseof
(Suzgunetal.,2022)tobuildthedialogueAIsys-
generativemodelsasafoundationfordownstream
tem especially in educational domain. We cho-
1https://github.com/joanitolopo/cikmar-system sen to work with a smaller, pre-trained language
4202
guA
61
]LC.sc[
1v50880.8042:viXra
REHCAET
NOITAUNITNOCmodelcalledGemma1.12B(IT),whichcanrun Ourresearchaimstodevelopaneducationaldi-
efficientlyonlessthan12GBofRAMandasin- aloguesystemusingGemma1.1IT2B.Thissys-
gleGPUT4. Thismakesitsuitableforreal-world tem uses prompts to guide LLMs in generating
applications by maintaining a reasonable model outputs based on contextual understanding, rele-
sizewithoutcompromisingperformance. Addition- vance,engagement,clarity,andfeedback. Toopti-
ally, a Dual-Encoder approach strategy has been mizeresults,itemploysdualencoders(BERTand
adoptedtorerankthecandidateoutputsgenerated SBERT)toreranktopcandidates. Ourobjectiveis
by the model using hand-written prompts. This todemocratizeopenmodelLLMinreal-worldsce-
approachaimstoincreasetherelevanceandeffec- narios,ensuringaccurate,relevantresponseswhile
tivenessoftheresponsesgeneratedbyoursystem enhancingstudentengagementandunderstanding
ineducationaldialogues. ineducationaldialogues.
2 RelatedWork 3 Methods
3.1 Data
Researchershaveextensivelyinvestigatedtheeffec-
tivenessofvariousapproachesutilizinglanguage We used data from the BEA 2023 shared task,
models. Sridharetal.(2023)enhancedLLMper- sourcedfromtheTeacher-StudentChatroomCor-
formance on web navigation tasks using Actor- pus(TSCC)(Cainesetal.,2020,2022). Thiscor-
SummarizerHierarchical(ASH)prompting,while pusconsistsofseveralconversationswhereanEn-
Kongetal.(2024)improvedreasoningbenchmarks glish teacher interacts with a student to work on
with role-play prompting. Kojima et al. (2023) languageexercisesandassessthestudent’sEnglish
showed that modifying prompt structure enables proficiency(Tacketal.,2023). Eachconversation
LLMstoperformmulti-stepreasoninginzero-shot containsmultipleresponsesandstartswitheither
settings. teacher: orstudent: prefixed. Thereferencetext
In the educational context, Adigwe and Yuan istheteacher’sresponsethatfollowstheprevious
(2023) and Hicke et al. (2023) used GPT-3 and inputdialogue. Thecorpusincludesatrainingset
GPT-4togenerateeducationaldialogueresponses, of2,747conversations,adevelopmentsetof305
achievinghighDialogRPTandBERTScoreresults conversations,andatestsetof273conversations,
with hand-written zero-shot prompts. Similarly, totaling3,325conversations.
Vassellietal.(2023)usedGPT-3.5Turbowithman- Since the data was collected from real-time
ual few-shot prompts based on DialogRPT selec- teacher-student conversations, turn-taking is not
tion,whichcontributedmosttothefinaloutputs. asconsistentasinmostdialoguesystems(Vasselli
Fine-tuninghasalsoproveneffectivebyutilizing et al., 2023). There are two patterns mostly oc-
largelanguagemodels(LLMs)ineducationaldo- cur: conversationsendingwiththestudent(teacher
main. Baladónetal.(2023)usedtheLoRamethod reply) and conversations ending with the teacher
to fine-tune models like BLOOM-3B, Llama 7B (teacher continuation). This condition occurs in
(Touvronetal.,2023),andOPT2.7B(Zhangetal., 38%ofthetrainingdataand40%ofthedevelop-
2022). TheyfoundthateventhesmallerOPT2.7B mentdata. Figure1showsanexampleofaconver-
model,withcarefulfine-tuning,couldachievebet- sationintheteachercontinuationcondition.
ter performance. Similarly, Huber et al. (2022)
3.2 PromptEnsemble
demonstratedthatGPT-2,enhancedwithreinforce-
ment learning via the NLPO algorithm (Rama- We utilized hand-written prompts from Vasselli
murthyetal.,2023),achievedhighBERTScores. etal.(2023)tobuildoursystem. Thepromptsin-
Duetothehighcomputationalpowerneededfor cludeZero-shotandFew-shottypes,targetingboth
fine-tuning and domain adaptation, Omidvar and general and specific scenarios. We used only the
An (2023) introduced semantic in-context learn- fivemainpromptsavailableastheyarealreadytai-
ing, using private knowledge sources for accu- loredforteacherresponsesandcontinuations. This
rateanswers. Guetal.(2024)proposedreducing selectionalsoensuresgeneralapplicabilitytoother
LLM sizes through knowledge distillation, train- datasetsorconversations. Forfulldetailsexplana-
ingsmallermodelstoreplicatelargerones. Their tionofeachprompttakealookatAppendixA.
experimentswithdistilledGPT-3versionsshowed In the creation of the few-shot prompts, it re-
competitiveperformanceonvariousbenchmarks. quirespositiveandnegativeexamplestohelptheRe-score and re-rank the outputs
Output 1 average
Given conversation and instruction Pick the highest scoring output
Output 2 average
Prompt Ensemble Final Output
Output 3 average
Output 4 average
Conversation Instruction
Generate multiple outputs
BERT SBERT
Contextual Relevance Semantic Similarity
Figure2: AnillustrationoftheCIKMarsystem. Givenaninputconversationandinstruction,wecreatetheprompt
ensembleandfeedittoGemmatogeneratemultipleoutputs. Wethenre-scoreeachoutputbyaveragingBERTand
SBERTscoresandselectthecandidatewiththehighestre-rankedscoreasthefinaloutput
model avoid irrelevant responses. We adopt the User: <start_of_turn>user
method of Vasselli et al. (2023) who applied the conversation
instruction<end_of_turn>
handcrafted,generative,anditerativepromptmeth-
<start_of_turn>model
ods. However, we modified the iterative method
Model: responses<end_of_turn>
from the original paper. Instead of using Dialo-
gRPT,weemployedtheBM25rankingfunctionto Table1: Exampledialoguewithuserandmodelcontrol
selectthehighestandlowestscoringresponsesas tokens.
positiveandnegativeexamples.
BM25 (Robertson and Walker, 1994; Robert-
token user represents the role, and its content
son and Zaragoza, 2009) was chosen over Dialo-
includestheconversationhistoryfollowedbythe
gRPTbecauseitreducesthecomputationalpower
prompt. Meanwhile, the model turn responds to
requiredforthepromptingandre-rankingprocess,
theuserdialogue.
asDialogRPTneedsadditionalmemorycapacity
Inourexperimentswiththetraininganddevelop-
tocalculateandchoosethebestcandidate. Addi-
mentsets,theGemmamodelsometimesgenerated
tionally,BM25isknownasthefirst-stagerankerin
hallucinations on the first attempt, such as factu-
lexicalretrievalsystems(Askarietal.,2023)which
allyincorrectresponse,nonsensicalcontent,overly
ensurespositiveandnegativeexamplesareselected
longresponse,andcontentdisconnectedfromthe
basedontheirlexicalmatchwiththeconversation
inputprompt. However,performanceimprovedon
history.
thesecondandthirdattempts. Therefore,toensure
3.3 GemmaInstruct-tunedModel the best response, we generated each candidate
threetimesbeforeselectingthefinaloutput.
Ourmainsystemleveragesapretrainedlanguage
We configured several parameters to con-
modelwithapromptingapproachratherthantrain-
trol the model’s output such as set the
ing one from scratch or fine-tuning it on a new
max_length of the generated output to 512
dataset. We used the Gemma 1.1 IT 2B model
tokens, no_repeat_ngram_size to 2 to avoid
(Teametal.,2024),2-billionparameteropenmodel
repetition, and used top_k=50 and top_p=0.95
developed by Google for efficient CPU and on-
to balance randomness and coherence. The
deviceapplications. Themodelhasshownstrong
temperaturewassetto0.7formoreconservative
performanceacrossacademicbenchmarksforlan-
choices. Finally,weenabledprobabilisticsampling
guageunderstanding,reasoning,andsafety,such
overgreedydecoding.
as MMLU (Hendrycks et al., 2021), SIQA (Sap
etal.,2019),HumanEval(Chenetal.,2021),and
3.4 Dual-EncoderReranking
Winogrande(Sakaguchietal.,2019). Theseresults
indicateitspromisingperformanceineducational Inspiredbypreviousresearch(Vassellietal.,2023;
contexts. Suzgun et al., 2022; Haroutunian et al., 2023),
We followed the instruction-formatted control our system generates multiple candidate outputs
tokens suggested in the Gemma technical report fromdifferentmanuallydesignedpromptsandthen
to avoid out-of-distribution and poor generation. rerankstheseoutputsusingaheuristicallydefined
Table1showsanexampledialoguewithuserand scoringfunction. Specifically,forthescoringfunc-
model control tokens. Specifically, the relevant tion, we utilize SBERT (Reimers and Gurevych,
{ {2019)andBERT(Devlinetal.,2019), averaging # Precision Recall F1-Score
cosinesimilarityscoresfromtheirembeddingsto CIKMar(ours) 0.69 0.70 0.70
assessfine-grainedsemanticrelevanceandcontext- NAISTeacherVassellietal.(2023) 0.71 0.71 0.71
AdaioAdigweandYuan(2023) 0.72 0.69 0.71
response matching in the embedding space be-
GPT-4Hickeetal.(2023) 0.71 0.69 0.70
tween the conversationhistory and the generated S-ICLOmidvarandAn(2023) 0.72 0.69 0.70
OPT-2.7BBaladónetal.(2023) 0.74 0.68 0.71
response.
NLP-HSGHuberetal.(2022) 0.72 0.63 0.67
In the given setup, we start with a dialog as AlpacaBaladónetal.(2023) 0.72 0.68 0.70
DTTacketal.(2023) 0.67 0.62 0.64
a context ctx and a list of candidate responses
{cand ,cand ,...,cand }. Initially,wecompute
1 2 m Table2:Comparisonofourproposedsystemwithprevi-
SBERT and BERT embeddings for both the con- ousresearchbasedonBERTScore(Zhangetal.,2020)
text and the candidate responses. For BERT em-
beddingswecalculatedbyaveragingtokenembed-
4 Result&Analysis
dingsacrossthesequencedimension.
The cosine similarity between the context and 4.1 MainResult
each candidate response embedding, for both
Our main result are presented in Table 2, show-
SBERTandBERT,iscalculatedusing:
casingcomparisonsamongsystemsfromtheBEA
Shared Task 2023 (Tack et al., 2023), ranked pri-
eemb·eemb
S (i) = cos(cid:0) eemb,eemb (cid:1) = ctx candi marilybyBERTScore. However,thiscomparison
emb ctx candi ∥eemb∥∥eemb ∥
ctx candi isn’tfullycomprehensiveastheBEASharedTask
alsoconsidershumanevaluationsandDialogRPT
whereemb ∈ {sbert,bert}.
(Gao et al., 2020) score. The human evaluation
Tocombinethesesimilarityscoresforeachcan- metricisrestrictedandnotpubliclyavailable,and
didateresponse,weaveragetheSBERTandBERT weencounteredchallengeswithDialogRPT,which
similarityscores. mighthaveissueswiththemodel,asitisreturnthe
Finally,thecandidatesarerankedbasedonthese samescoreforeachcontext.
combined similarity scores in descending scores. CIKMardemonstratescompetitiveperformance
Theindicesofthecandidatesaresortedaccording against baseline systems like NAISTeacher and
totheircombinedscores,anditreturnsthelistof Adaio based on BERTScore2. Specifically, we
candidates responses ordered from most to least achieve a robust recall score of 0.70, slightly be-
relevanttothegivencontext. low NAISTeacher’s 0.71. This indicates that our
Dual-Encoder ranking effectively retrieves many
3.5 Post-processing
contextually relevant responses compared to the
Therawoutputsfrommodeloftenincludedincon- reference answer. Furthermore, our F1-Score of
sistentformatting,suchasphrasesprefixedby"**" 0.70 is comparable to models such as S-ICL and
or starting with unwanted text like Teacher: or Alpaca,whichutilizefine-tuningandlargermodel
Student:. Additionally,themodelsometimesap- sizes,demonstratingourmodel’scapabilitytocap-
pended lengthy explanations to its responses be- turesimilarityandproducecoherent,contextually
ginningwithExplanation:, addingunnecessary appropriateresponsesevenusingsimpleandsmall
length. However, we observed a consistent pat- modelsize.
ternwheretheactualresponsealwaysbeganwitha
4.2 EvaluationMetrics
quotationmark".
To ensure the reliability of our approach, we
To standardize these outputs, we implemented
employed word overlap-based metric ROUGE
a post-processing step. First, we defined a reg-
ular expression pattern, \*\*.\*?:\*\*\n\n, to (Lin, 2004) and the neural network-based met-
ric Dialog perplexity (Li et al., 2016)3 to further
identifyandremoveanyunwantedinitialphrases.
assesoursystem. WecomputedROUGEmetrics:
Thispatterneffectivelyremovedprefixeslike"**",
Teacher:,orStudent:. Next,eachresponsewas
2BERTScorewascalculatedusingHuggingFace’sevalu-
processed to retain only the text following the atepackagewiththedistilbert-base-uncasedmodel,av-
first occurrence of a quotation mark, discarding eragingprecision,recall,andF1scoresacrosstheentiretest
set.
anypreambleorunnecessarycontent. Finally,we
3We used DialoGPT and its reverse model to compute
trimmedanyleadingortrailingwhitespace. perplexityid responses analysis
test_0006 Iunderstandyou’refeelingabitchilly.Wouldyouliketocontinuewiththe feeling,relevant,andem-
grammarpracticeordoyouhaveanyquestionsaboutthevocabularyword phatic
’cold’?
test_0004 Great point! Speed is an important aspect of your time management, so appreciation, discussion
maintainingasenseofefficiencyiscrucial.Howdidyouachieveyourfaster focus, and skill develop-
timeframe? ment
test_0007 Greatpoint!Now,let’sfocusonusingthisnewvocabularyinameaningful appreciation, practice of
way.Canyoutellmesomeexamplesofhowyou’veusedthiswordinyour thenewmaterial,andre-
ownwritingorconversations? flection
test_0010 Followingyourinsightfulobservation,couldyouprovideaninstancewhere appreciation,andcritical
speaking truthfully might be particularly effective in achieving a desired thinking
outcome?
test_0011 Iappreciateyourperspectiveonthis,student. Tofurtherexplorethisidea, relevant, emotion, moti-
howdoesplayinggameshelpchildrencopewithdifficultemotions? vation,criticalskillsand
practicalsolutions
Table3: Qualitativeexamplesofmodelgeneratedresponses. Wemanuallyhighlightincolor,phrasesthatwefindto
beassociatedwiththecontextandthecriticalaspectsasmentionedinTackandPiech(2022).
Distribution of Mutual Information Scores cational contexts, it also reveals a drawback: the
responseslackdepthandexhibitmonotony,signif-
60
icantly reduce text engagement and the nuanced
50
understandingrequiredfordeeperlearning.
40
30 4.3 In-depthOutputAnalysis
20 We manually inspected the model’s outputs and
10 evaluatedeachprompt’scontributionbyexamining
10outputsindetail. Table3presentsthetopcan-
0
0 2000 4000 6000 8000 10000 12000 14000
Mutual Information Score
didate responses selected through Dual-Encoder
Figure3: Thedistributionofmutualinformationscores rankingforfiveexamples.
derivedfromcombinedperplexityvalues
To examine the impact of prompts on the best
responses,weusethedialoguecontexttest_0006,
rouge1, rouge2, rougeL, and rougeLsum result- as shown in Table 4, as an example. Here, the
ing in scores of 0.12, 0.0047, 0.084, and 0.087, teacher is explaining a grammar lesson when the
respectively. studentmentionsneeding10moreminutesandfeel-
BasedontheROUGEscores,thegeneratedtext ing very cold in the room. The model’s response
demonstratessignificantoverlapwiththereference is inconsistent, as it incorrectly associates "cold"
textattheunigramlevel(ROUGE-1)andinlonger withthegrammarlessonratherthanthestudent’s
common sequences (ROUGE-L and ROUGE-Lsum). condition. Thissuggeststhatthemodelmayfocus
Thissuggeststhesystemstayson-topicandusesrel- on one situation in the conversation and struggle
evantvocabulary,beneficialforeducationalcontent. to adapt when new contexts arise. Consequently,
However,itshowsnoticeableshortcomingswithex- thecontextof"cold"isincorrectlyforcedtofitthe
actwordsequences(ROUGE-2),anddiscrepancies contextitself.
in longer sequences (ROUGE-L and ROUGE-Lsum)
indicatechallengesinmaintainingcoherenceand Teacher: Whichiseasy,becauseyoucanusemy/his/youretc.andnot
thinkaboutarticles!
well-structuredresponses. Student: Only10minutesleft!
Teacher: Iknow,wecanfinishearlyifyouaregettingcold?
Additionally, Figure 3 depicts the distribution
Student: I’mreallycold
of mutual information scores derived from com-
bined perplexity values. The histogram’s right- Table 4: Example dialogue context of test_006 be-
skewed shape, with scores predominantly in the tweenstudentandteacher
lower range, suggests that the generated teacher
responses are often predictable. While this indi- We also found that the model struggles with
cates clarity, conciseness, and consistency in the teachercontinuationproblems. Whenthedialogue
generated text, which are advantageous for edu- ends with the teacher, the model often seems un-
ycneuqerF(cid:13)(cid:12)(cid:11)(cid:25)(cid:28)(cid:28)(cid:27)(cid:29)(cid:19)(cid:24)(cid:10)(cid:27)(cid:9)(cid:18)(cid:30)(cid:21)(cid:27)(cid:8)(cid:30)(cid:26)(cid:27)(cid:20)(cid:29)(cid:24)(cid:20)(cid:7)(cid:24)(cid:22)(cid:27)(cid:30)(cid:21)(cid:20)(cid:19)(cid:18)(cid:25)(cid:9)(cid:24)(cid:30)(cid:29)(cid:28)(cid:24)(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:9)
(cid:16)(cid:27)(cid:30)(cid:15)(cid:14)(cid:13)(cid:12)(cid:25)(cid:11)
Candidate 3
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:11)(cid:24)(cid:10)(cid:14)(cid:9)(cid:24)(cid:16)(cid:27)(cid:30)(cid:15)(cid:14)(cid:13)(cid:12)(cid:25)(cid:24)(cid:20)
(cid:17)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:11)(cid:24)(cid:10)(cid:14)(cid:9)(cid:24)(cid:16)(cid:27)(cid:30)(cid:15)(cid:14)(cid:13)(cid:12)(cid:25)(cid:24)(cid:23)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:11)(cid:24)(cid:10)(cid:14)(cid:9)(cid:24)(cid:16)(cid:27)(cid:30)(cid:15)(cid:14)(cid:13)(cid:12)(cid:25)(cid:24)(cid:22)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:11)(cid:24)(cid:10)(cid:14)(cid:9)(cid:24)(cid:16)(cid:27)(cid:30)(cid:15)(cid:14)(cid:13)(cid:12)(cid:25)(cid:24)(cid:21)
(cid:21) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:11)(cid:24)(cid:10)(cid:14)(cid:9)(cid:24)(cid:16)(cid:27)(cid:30)(cid:15)(cid:14)(cid:13)(cid:12)(cid:25)(cid:24)(cid:17)
Candidate 2
Candidate 4
(cid:22)
Candidate 3
(cid:23) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:23)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:22)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)C(cid:25)a(cid:24)n(cid:21)didate 1
(cid:20) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:24)(cid:20)
(cid:22)(cid:27)(cid:30)(cid:21)(cid:20)(cid:19)(cid:18)(cid:25)(cid:24)(cid:17)
(cid:18)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:22)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:21) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:20) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:24)(cid:21) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:24)(cid:23) (cid:22)(cid:27)(cid:30)(cid:21)(cid:20)(cid:19)(cid:18)(cid:25)(cid:24)(cid:16)
(cid:19)(cid:20)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:23)
(cid:22)(cid:27)(cid:30)(cid:21)(cid:20)(cid:19)(cid:18)(cid:25)(cid:24)(cid:15) (cid:22)(cid:27)(cid:30)(cid:21)(cid:20)(cid:19)(cid:18)(cid:25)(cid:24)(cid:23)
(cid:19)(cid:23) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:23)
(cid:22)(cid:27)(cid:30)(cid:21)(cid:20)(cid:19)(cid:18)(cid:25)(cid:24)(cid:14)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:22)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:14)(cid:24)(cid:24)
(cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:21) (cid:31)(cid:30)(cid:29)(cid:28)(cid:27)(cid:28)(cid:30)(cid:26)(cid:25)(cid:24)(cid:20)
(cid:19)(cid:22)
(cid:19)(cid:21) (cid:19)(cid:23) (cid:18) (cid:23) (cid:21)
(cid:6)(cid:31)(cid:5)(cid:24)(cid:31)(cid:20)(cid:12)(cid:4)(cid:20)(cid:29)(cid:25)(cid:29)(cid:26)(cid:24)(cid:23)
Figure4: EmbeddingSpaceVisualization
sure about the next response, which happens fre- sation where Prompt 1 was not chosen because
quentlyinthegeneratedoutputs. Thisalignswith theteacherneededtoexplainthelearningcontext
research by Vasselli et al. (2023), indicating that in greater depth. As conversation complexity in-
instruct-tunedmodelstrainedinuser-assistantset- creases, the dual encoder selects Prompts 2 and
tings find it difficult to adapt when the setting 4,whicharebettersuitedtohandlemoreintricate
changesabruptly. Forexample, intest_004, the dialogues.
modelrepeatstheword"great"fromthedialogue
butfailstounderstandthecontextdespitemanag- 4.4 DualEncoderEffect
ingtointroduceafollow-upconversationbyasking
Weconductedamanualinvestigationtoassessthe
thestudentexamples.
dual encoder’s impact on selecting the best can-
Furthermore, we analyzed several dialogues didates. Analyzing five dialogue-response pairs’
with minimal context, some having only two ex- embedding spaces, as shown in Figure 4, we dis-
changes. This limited context makes it difficult covered that the Dual-Encoder can avoid the pit-
forthemodeltograsptheoverallconversationand falls of distance-measurement-only. Notably, in
provides fewer reference words. A prime exam- dialogue2,candidate2appearedclosertoitscon-
pleistest_0011,whichhasonlyoneturnwithat
textthancandidate1intheembeddingspace,yet
least5wordsperturn. Thislackofcontextmakes thedualencoderrankedcandidate1asthebestcan-
it challenging for the model to generate the best didate(denotedby∗). Thisphenomenonoccurred
response,asthecontextisinsufficientlyclear. acrossmultipledialogues,highlightingSBERTand
Lastly, we analyzed the contributions of each BERT’srole inenhancingthemodel’sconsidera-
prompttothefinaloutputselectedbyDual-Encoder tionofcontextualrelevanceandsemanticsimilarity
rankingfor10datapoints. Prompt1significantly betweendialoguesandresponses,asdiscussedear-
influenced the final output, being chosen in 5 ex- lier.
amples. This is likely due to the model’s strong To evaluate the dual encoder’s ranking quality,
performanceinacademictasksandthestraightfor- we investigated the phenomenon of closely clus-
wardnatureoftheseconversations,whichaligned teredembedding. Specifically,candidatesfordia-
well with Prompt 1’s instructions. In contrast, logue3exhibiteddenseclustering,whereincreas-
test_010involvedacomplex,multi-turnconver- ing embedding proximity indicated greater simi-
(cid:14)(cid:24)(cid:26)(cid:29)(cid:25)(cid:29)(cid:20)(cid:4)(cid:12)(cid:20)(cid:31)(cid:24)(cid:5)(cid:31)(cid:6)larity,complicatingcandidateselection. Afterana- (BEA2023),pages796–804,Toronto,Canada.Asso-
lyzingallcandidates,candidates1and4emerged ciationforComputationalLinguistics.
asoptimalchoicesforthisdialogue,supportedby
ArianAskari,AminAbolghasemi,GabriellaPasi,Wes-
theirrelatednessintheembeddingspace. However, selKraaij,andSuzanVerberne.2023. Injectingthe
theDual-Encoderprioritizedcandidate4,suggest- bm25scoreastextimprovesbert-basedre-rankers.
ingapreferencefortheoreticaldiscussionandex-
JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,
plorationratherthanpracticalcontextinitsranking
XiaodongDeng,YangFan,WenbinGe,YuHan,Fei
criteria. Huang,BinyuanHui,LuoJi,MeiLi,JunyangLin,
Furthermore, we noted a tendency for candi- RunjiLin,DayihengLiu,GaoLiu,ChengqiangLu,
KemingLu,JianxinMa,RuiMen,XingzhangRen,
dates within each dialogue to cluster together.
XuanchengRen,ChuanqiTan,SinanTan,Jianhong
ThisindicatesthattheGemmamodelconsistently Tu, Peng Wang, Shijie Wang, Wei Wang, Sheng-
produces similar embeddings for each candidate guangWu,BenfengXu,JinXu,AnYang,HaoYang,
per dialogue, demonstrating stable performance Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu,
HongyiYuan,ZhengYuan,JianweiZhang,Xingx-
across various dialogues. However, certain can-
uanZhang,YichangZhang,ZhenruZhang,Chang
didateswerepositionedfartherfromtheircluster
Zhou,JingrenZhou,XiaohuanZhou,andTianhang
and nearer to candidates in another cluster. This Zhu.2023. Qwentechnicalreport.
suggeststhatthemodelsometimesencountersdif-
Alexis Baladón, Ignacio Sastre, Luis Chiruzzo, and
ficulties accurately interpreting the dialogue con-
Aiala Rosá. 2023. RETUYT-InCo at BEA 2023
text. Wesuspectthatthisissuemayarisebecause sharedtask: Tuningopen-sourceLLMsforgenerat-
SBERT’sdominanceoverBERTleadstoalossof ing teacher responses. In Proceedings of the 18th
full context. Further investigation is required to WorkshoponInnovativeUseofNLPforBuildingEd-
ucationalApplications(BEA2023),pages756–765,
delvedeeperintothismatter.
Toronto,Canada.AssociationforComputationalLin-
guistics.
5 Conclusion&FutureWork
TomB.Brown,BenjaminMann,NickRyder,Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
We have shown that CIKMar, an educational di-
Neelakantan,PranavShyam,GirishSastry,Amanda
alogue generation approach using prompts and a
Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Dual-EncoderrankingwiththeGemmalanguage Gretchen Krueger, Tom Henighan, Rewon Child,
model,yieldspromisingresultsineducationalset- Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,
ClemensWinter,ChristopherHesse,MarkChen,Eric
tings. ByutilizingtheGemma2Bmodel,wemain-
Sigler,MateuszLitwin,ScottGray,BenjaminChess,
tain high performance in response relevance and
Jack Clark, Christopher Berner, Sam McCandlish,
accuracywithasmaller,moreaccessiblemodel. Alec Radford, Ilya Sutskever, and Dario Amodei.
Despitethesestrongperformances,wehaveiden- 2020. Languagemodelsarefew-shotlearners.
tifiedlimitationshinderingoptimalresults. Specifi-
Andrew Caines, Helen Yannakoudakis, Helen Allen,
cally,theDual-Encoderoftenprioritizestheoreti- PascualPérez-Paredes,BillByrne,andPaulaButtery.
caldiscussionoverpracticalcontextualresponses, 2022. Theteacher-studentchatroomcorpusversion
potentially leading to irrelevant rankings. Future 2: more lessons, new annotation, automatic detec-
tionofsequenceshifts. InProceedingsofthe11th
research should explore scenarios where either
WorkshoponNLPforComputerAssistedLanguage
SBERTorBERTdominatesrankingscores. Learning,pages23–35,Louvain-la-Neuve,Belgium.
Additionally,craftingmorespecificpromptsis LiUElectronicPress.
crucialfordeepercontextualunderstandingined-
AndrewCaines,HelenYannakoudakis,HelenaEdmond-
ucational dialogues. Lastly, refining the Gemma
son,HelenAllen,PascualPérez-Paredes,BillByrne,
modeltofocusoneducationalcontextsandadapt andPaulaButtery.2020. Theteacher-studentchat-
toshiftingconversationdynamicsisrecommended. room corpus. In Proceedings of the 9th Workshop
onNLPforComputerAssistedLanguageLearning,
pages10–20,Gothenburg,Sweden.LiUElectronic
Press.
References
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming
AdaezeAdigweandZhengYuan.2023. TheADAIO Yuan,HenriquePondedeOliveiraPinto,JaredKa-
systemattheBEA-2023sharedtask:Sharedtaskgen- plan, HarriEdwards, YuriBurda, NicholasJoseph,
eratingAIteacherresponsesineducationaldialogues. Greg Brockman, Alex Ray, Raul Puri, Gretchen
InProceedingsofthe18thWorkshoponInnovative Krueger,MichaelPetrov,HeidyKhlaaf,GirishSas-
UseofNLPforBuildingEducationalApplications try, Pamela Mishkin, Brooke Chan, Scott Gray,NickRyder,MikhailPavlov,AletheaPower,Lukasz Toronto,Canada.AssociationforComputationalLin-
Kaiser, Mohammad Bavarian, Clemens Winter, guistics.
Philippe Tillet, Felipe Petroski Such, Dave Cum-
mings, Matthias Plappert, Fotios Chantzis, Eliza- Patrick Huber, Armen Aghajanyan, Barlas Oguz,
beth Barnes, Ariel Herbert-Voss, William Hebgen DmytroOkhonko,ScottYih,SonalGupta,andXilun
Guss,AlexNichol,AlexPaino,NikolasTezak,Jie Chen. 2022. CCQA: A new web-scale question
Tang,IgorBabuschkin,SuchirBalaji,ShantanuJain, answering dataset for model pre-training. In Find-
William Saunders, Christopher Hesse, Andrew N. ingsoftheAssociationforComputationalLinguis-
Carr,JanLeike,JoshAchiam,VedantMisra,Evan tics:NAACL2022,pages2402–2420,Seattle,United
Morikawa, Alec Radford, Matthew Knight, Miles States.AssociationforComputationalLinguistics.
Brundage,MiraMurati,KatieMayer,PeterWelinder,
BobMcGrew,DarioAmodei,SamMcCandlish,Ilya Bernal Jimenez Gutierrez, Nikolas McNeal, Clayton
Sutskever,andWojciechZaremba.2021. Evaluating Washington, You Chen, Lang Li, Huan Sun, and
largelanguagemodelstrainedoncode. YuSu.2022. ThinkingaboutGPT-3in-contextlearn-
ingforbiomedicalIE?thinkagain. InFindingsofthe
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and AssociationforComputationalLinguistics: EMNLP
Kristina Toutanova. 2019. BERT: Pre-training of 2022, pages 4497–4512, Abu Dhabi, United Arab
deepbidirectionaltransformersforlanguageunder- Emirates.AssociationforComputationalLinguistics.
standing. InProceedingsofthe2019Conferenceof
theNorthAmericanChapteroftheAssociationfor FiruzKamalov,DavidSantandreuCalong,andIkhlaas
ComputationalLinguistics: HumanLanguageTech- Gurrib. 2023. New era of artificial intelligence in
nologies,Volume1(LongandShortPapers),pages education: Towardsasustainablemultifacetedrevo-
4171–4186,Minneapolis,Minnesota.Associationfor lution.
ComputationalLinguistics.
Enkelejda Kasneci, Kathrin Sessler, Stefan Küche-
TianyuDing,TianyiChen,HaidongZhu,JiachenJiang, mann, Maria Bannert, Daryna Dementieva, Frank
Yiqi Zhong, Jinxin Zhou, Guangzhi Wang, Zhihui Fischer, Urs Gasser, Georg Groh, Stephan Günne-
Zhu, Ilya Zharkov, and Luming Liang. 2024. The mann, Eyke Hüllermeier, Stephan Krusche, Gitta
efficiency spectrum of large language models: An Kutyniok, Tilman Michaeli, Claudia Nerdel, Jür-
algorithmicsurvey. genPfeffer,OleksandraPoquet,MichaelSailer,Al-
brechtSchmidt,TinaSeidel,MatthiasStadler,Jochen
XiangGao,YizheZhang,MichelGalley,ChrisBrock-
Weller, Jochen Kuhn, and Gjergji Kasneci. 2023.
ett,andBillDolan.2020. Dialogueresponseranking
Chatgptforgood? onopportunitiesandchallengesof
training with large-scale human feedback data. In
largelanguagemodelsforeducation. Learningand
Proceedings of the 2020 Conference on Empirical
IndividualDifferences,103:102274.
MethodsinNaturalLanguageProcessing(EMNLP),
pages 386–395, Online. Association for Computa-
TakeshiKojima,ShixiangShaneGu,MachelReid,Yu-
tionalLinguistics.
takaMatsuo,andYusukeIwasawa.2023. Largelan-
guagemodelsarezero-shotreasoners.
YuxianGu,LiDong,FuruWei,andMinlieHuang.2024.
Minillm: Knowledgedistillationoflargelanguage
AoboKong,ShiwanZhao,HaoChen,QichengLi,Yong
models.
Qin,RuiqiSun,XinZhou,EnzhiWang,andXiao-
hangDong.2024. Betterzero-shotreasoningwith
LevonHaroutunian,ZhuangLi,LucianGalescu,Philip
role-playprompting.
Cohen,RajTumuluri,andGholamrezaHaffari.2023.
Rerankingfornaturallanguagegenerationfromlogi-
JiweiLi,MichelGalley,ChrisBrockett,JianfengGao,
calforms: Astudybasedonlargelanguagemodels.
InProceedingsofthe13thInternationalJointCon- and Bill Dolan. 2016. A diversity-promoting ob-
ferenceonNaturalLanguageProcessingandthe3rd jectivefunctionforneuralconversationmodels. In
Proceedings of the 2016 Conference of the North
ConferenceoftheAsia-PacificChapteroftheAssoci-
AmericanChapteroftheAssociationforComputa-
ationforComputationalLinguistics(Volume1:Long
Papers),pages1067–1082,NusaDua,Bali.Associa-
tionalLinguistics: HumanLanguageTechnologies,
pages110–119,SanDiego,California.Association
tionforComputationalLinguistics.
forComputationalLinguistics.
DanHendrycks,CollinBurns,StevenBasart,AndyZou,
MantasMazeika,DawnSong,andJacobSteinhardt. QingyaoLi,LingyueFu,WeimingZhang,XianyuChen,
2021. Measuringmassivemultitasklanguageunder- JingweiYu,WeiXia,WeinanZhang,RuimingTang,
standing. andYongYu.2024. Adaptinglargelanguagemodels
foreducation: Foundationalcapabilities,potentials,
Yann Hicke, Abhishek Masand, Wentao Guo, and andchallenges.
Tushaar Gangavarapu. 2023. Assessing the effi-
cacy of large language models in generating accu- Chin-Yew Lin. 2004. ROUGE: A package for auto-
rate teacher responses. In Proceedings of the 18th maticevaluationofsummaries. InTextSummariza-
WorkshoponInnovativeUseofNLPforBuildingEd- tionBranchesOut,pages74–81,Barcelona,Spain.
ucationalApplications(BEA2023),pages745–755, AssociationforComputationalLinguistics.AminOmidvarandAijunAn.2023. Empoweringcon- Anaïs Tack, Ekaterina Kochmar, Zheng Yuan, Serge
versationalagentsusingsemanticin-contextlearning. Bibauw, and Chris Piech. 2023. The BEA 2023
InProceedingsofthe18thWorkshoponInnovative shared task on generating AI teacher responses in
UseofNLPforBuildingEducationalApplications educational dialogues. In Proceedings of the 18th
(BEA2023),pages766–771,Toronto,Canada.Asso- WorkshoponInnovativeUseofNLPforBuildingEd-
ciationforComputationalLinguistics. ucationalApplications(BEA2023),pages785–795,
Toronto,Canada.AssociationforComputationalLin-
Rajkumar Ramamurthy, Prithviraj Ammanabrolu, guistics.
KiantéBrantley, JackHessel, RafetSifa, Christian
Bauckhage, Hannaneh Hajishirzi, and Yejin Choi. AnaïsTackandChrisPiech.2022. Theaiteachertest:
2023. Is reinforcement learning (not) for natural Measuring the pedagogical ability of blender and
language processing: Benchmarks, baselines, and gpt-3ineducationaldialogues.
buildingblocksfornaturallanguagepolicyoptimiza-
tion. Kehui Tan, Tianqi Pang, Chenyou Fan, and Song Yu.
2023. Towardsapplyingpowerfullargeaimodelsin
Nils Reimers and Iryna Gurevych. 2019. Sentence- classroomteaching: Opportunities, challengesand
BERT:SentenceembeddingsusingSiameseBERT- prospects.
networks. InProceedingsofthe2019Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing Gemma Team, Thomas Mesnard, Cassidy Hardin,
andthe9thInternationalJointConferenceonNatu- RobertDadashi,SuryaBhupatiraju,ShreyaPathak,
ralLanguageProcessing(EMNLP-IJCNLP),pages Laurent Sifre, Morgane Rivière, Mihir Sanjay
3982–3992,HongKong,China.AssociationforCom- Kale,JulietteLove,PouyaTafti,LéonardHussenot,
putationalLinguistics. PierGiuseppeSessa,AakankshaChowdhery,Adam
Roberts, Aditya Barua, Alex Botev, Alex Castro-
S. E. Robertson and S. Walker. 1994. Some simple Ros, Ambrose Slone, Amélie Héliou, Andrea Tac-
effectiveapproximationstothe2-poissonmodelfor chetti, Anna Bulanova, Antonia Paterson, Beth
probabilisticweightedretrieval. InSIGIR’94,pages Tsai, Bobak Shahriari, Charline Le Lan, Christo-
232–241,London.SpringerLondon. pherA.Choquette-Choo,ClémentCrepy,DanielCer,
Daphne Ippolito, David Reid, Elena Buchatskaya,
Stephen Robertson and Hugo Zaragoza. 2009. The Eric Ni, Eric Noland, Geng Yan, George Tucker,
probabilistic relevance framework: Bm25 and be- George-ChristianMuraru,GrigoryRozhdestvenskiy,
yond. Found.TrendsInf.Retr.,3(4):333–389. HenrykMichalewski,IanTenney,IvanGrishchenko,
Jacob Austin, James Keeling, Jane Labanowski,
KeisukeSakaguchi,RonanLeBras,ChandraBhagavat- Jean-Baptiste Lespiau, Jeff Stanway, Jenny Bren-
ula,andYejinChoi.2019. Winogrande: Anadver- nan,JeremyChen,JohanFerret,JustinChiu,Justin
sarialwinogradschemachallengeatscale. Mao-Jones, Katherine Lee, Kathy Yu, Katie Milli-
can, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon,
Maarten Sap, Hannah Rashkin, Derek Chen, Ronan MachelReid,MaciejMikuła,MateoWirth,Michael
Le Bras, and Yejin Choi. 2019. Social IQa: Com- Sharman, Nikolai Chinaev, Nithum Thain, Olivier
monsense reasoning about social interactions. In Bachem,OscarChang,OscarWahltinez,PaigeBai-
Proceedings of the 2019 Conference on Empirical ley, Paul Michel, Petko Yotov, Rahma Chaabouni,
Methods in Natural Language Processing and the RamonaComanescu,ReenaJana,RohanAnil,Ross
9thInternationalJointConferenceonNaturalLan- McIlroy,RuiboLiu,RyanMullins,SamuelLSmith,
guageProcessing(EMNLP-IJCNLP),pages4463– SebastianBorgeaud,SertanGirgin,SholtoDouglas,
4473,HongKong,China.AssociationforComputa- ShreePandya,SiamakShakeri,SohamDe,TedKli-
tionalLinguistics. menko, Tom Hennigan, Vlad Feinberg, Wojciech
Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao
NeelabhSinha, VinijaJain, andAmanChadha.2024. Gong,TrisWarkentin,LudovicPeran,MinhGiang,
Evaluatingopenlanguagemodelsacrosstasktypes, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray
application domains, and reasoning types: An in- Kavukcuoglu,DemisHassabis,ZoubinGhahramani,
depthexperimentalanalysis. Douglas Eck, Joelle Barral, Fernando Pereira, Eli
Collins,ArmandJoulin,NoahFiedel,EvanSenter,
AbishekSridhar,RobertLo,FrankF.Xu,HaoZhu,and AlekAndreev,andKathleenKenealy.2024. Gemma:
ShuyanZhou.2023. Hierarchicalpromptingassists Openmodelsbasedongeminiresearchandtechnol-
largelanguagemodelonwebnavigation. ogy.
Mirac Suzgun, Luke Melas-Kyriazi, and Dan Juraf- Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
sky.2022. Prompt-and-rerank: Amethodforzero- bert, Amjad Almahairi, Yasmine Babaei, Nikolay
shotandfew-shotarbitrarytextualstyletransferwith Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
smalllanguagemodels. InProceedingsofthe2022 Bhosale,DanBikel,LukasBlecher,CristianCanton
Conference on Empirical Methods in Natural Lan- Ferrer,MoyaChen,GuillemCucurull,DavidEsiobu,
guage Processing, pages 2195–2222, Abu Dhabi, JudeFernandes,JeremyFu,WenyinFu,BrianFuller,
United Arab Emirates. Association for Computa- CynthiaGao,VedanujGoswami,NamanGoyal,An-
tionalLinguistics. thonyHartshorn,SagharHosseini,RuiHou,HakanInan,MarcinKardas,ViktorKerkez,MadianKhabsa, A EnsemblePromptsExplanation
IsabelKloumann,ArtemKorenev,PunitSinghKoura,
Marie-AnneLachaux,ThibautLavril,JenyaLee,Di- Belowarethepromptsweareusinginthisresearch.
anaLiskovich,YinghaiLu,YuningMao,XavierMar- Thedetailsexplanationofeachpromptcanreferto
tinet,TodorMihaylov,PushkarMishra,IgorMoly-
Vassellietal.(2023).
bog, Yixin Nie, Andrew Poulton, Jeremy Reizen-
Zero-shotpromptsconsistofinstructionswith-
stein,RashiRungta,KalyanSaladi,AlanSchelten,
Ruan Silva, Eric Michael Smith, Ranjan Subrama- outexamples,whilefew-shotpromptsincludeex-
nian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay- amples to guide the model towards relevant re-
lor, Adina Williams, Jian Xiang Kuan, Puxin Xu,
sponses. Prompt(1)iscategorizedasazero-shot
ZhengYan,IliyanZarov,YuchenZhang,AngelaFan,
prompt,refinedtoaddressissueslikeoverlydirect
Melanie Kambadur, Sharan Narang, Aurelien Ro-
driguez,RobertStojnic,SergeyEdunov,andThomas answersandsoundingtoomuchlikeanassistant.
Scialom.2023. Llama2: Openfoundationandfine- Therestoftheprompts—(2), (3), (4), (5)—are
tunedchatmodels. few-shot prompts that require positive and nega-
tive examples to help the model avoid irrelevant
JustinVasselli,ChristopherVasselli,AdamNohejl,and
TaroWatanabe.2023. NAISTeacher: Apromptand responses.
rerankapproachtogeneratingteacherutterancesin Eachpromptservesaspecificpurpose: Prompt
educational dialogues. In Proceedings of the 18th
(1)focusesonContextualUnderstanding,Prompt
Workshop on Innovative Use of NLP for Building
(2)ensuresRelevance,Prompt (3)aimstoenhance
Educational Applications (BEA 2023), pages 772–
784,Toronto,Canada.AssociationforComputational Engagement,Prompt (4)emphasizesClarity,and
Linguistics. Prompt (5)isdedicatedtoprovidingFeedback. To-
gether,thesepromptstailorthemodel’sresponses
Aohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,
Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, to match the student’s current learning stage and
Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan needs. By grasping the context (Contextual Un-
Ma,YufeiXue,JidongZhai,WenguangChen,Peng derstanding),thepromptsdirectthemodeltopro-
Zhang,YuxiaoDong,andJieTang.2023. Glm-130b:
duce responses that are relevant to the student’s
Anopenbilingualpre-trainedmodel.
queries,therebymaintainingfocusandrelevance
Susan Zhang, Stephen Roller, Naman Goyal, Mikel (Relevance). Thisrelevanceboostsstudentengage-
Artetxe,MoyaChen,ShuohuiChen,ChristopherDe-
ment(Engagement),encouragingsustainedinter-
wan,MonaDiab,XianLi,XiVictoriaLin,TodorMi-
estandparticipation,whichisfurthersupportedby
haylov,MyleOtt,SamShleifer,KurtShuster,Daniel
Simig, Punit Singh Koura, Anjali Sridhar, Tianlu clearcommunication(Clarity)thatmakescomplex
Wang,andLukeZettlemoyer.2022. Opt: Openpre- concepts easier to understand and reduces confu-
trainedtransformerlanguagemodels.
sion. Collectively, these prompts help the model
generate optimal responses for educational con-
Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.
Weinberger,andYoavArtzi.2020. Bertscore: Evalu- texts.
atingtextgenerationwithbert.
(1) The following is a partial conversation be-
tween an English language learner and their
teacher:
(conversation)
Canyougiveanexampleteacherfollow-upto
theirpreviousmessagethatwouldbehelpful
forthelanguagelearner? Themessageshould
beconcise,andwordedsimply. Itshouldei-
therencouragethecontinuationofthecurrent
topicorgracefullytransitiontoanewteacher-
providedtopic. Questionsshouldbespecific
andnotopen-ended. Trytonotsoundlikean
assistant,butateacher,inchargeoftheflow
ofthelesson.
(2) Concatenationofprompt(1)andthefollow-
ing:Good example: ’Can you make a sentence insteadof"build"sincetheconstructionhasal-
using’within’?’ Badexample: ’Doyouhave readybeencompleted. Sothecorrectsentence
anyquestionsaboutprepositions?’ is: "The International Space Station is built
by NASA." Keep up the good work! Now,
(3) Concatenationofprompt(1)andthefollow-
let’smoveontoanewtopic-let’stalkabout
ing:
yourfavoritehobbies. Canyoutellmewhat
Howdoesateachersoundwhenresponding activitiesyouenjoydoinginyourfreetime?"
to a student? What kinds of things would
Hereisanexampleofapoorteacherfollowup:
teachers say that chatbots would not? What
"That’saninterestingobservationaboutposh-
do they not say? In your response provide
ness. CanyouthinkofanyexamplesofBritish
an example of a response that sounds like a
accents that might be associated with posh-
teacher and one that sounds like a chatbot?
ness?"
Respondsuccinctly
(4) The following is a partial conversation be-
tween an English language learner and their
teacher:
(conversation)
Theyareinthemiddleofalesson. Canyou
giveapossiblewaytheteachercouldrespond?
Remember: A teacher typically sounds
knowledgeable,authoritative,andfocused
onguidingandinstructingstudents. They
may use formal language and provide de-
tailed explanations. Teachers often offer
constructive feedback, encourage critical
thinking,andaskprobingquestionstostim-
ulatelearning.
Example of a teacher-like response:
"That’sagreatobservation,butlet’sdelve
deeper into the topic. Can you provide
someevidencetosupportyourclaim?"
A chatbot, on the other hand, may sound
moreinformalandconversational. Ittends
toprovidegeneralinformationorbriefre-
sponseswithoutmuchelaboration.
Exampleofachatbot-likeresponse: "Inter-
esting! Tell me more." Teachers typically
avoid expressing personal opinions or bi-
ases. They also refrain from engaging in
casual banter or unrelated conversations
tomaintainaprofessionalandeducational
atmosphere.
(5) Concatenationofprompt(1)andthefollow-
ing:
Hereisanexampleofanexceptionalteacher
follow-up:
"Great job, student! Just a small correction,
we should use the present tense verb "built"