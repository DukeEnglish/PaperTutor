Constructing Domain-Specific Evaluation Sets for LLM-as-a-judge
RaviRaju1 SwayambhooJain1 BoLi1 JonathanLi1 UrmishThakkar1
Abstract
LargeLanguageModels(LLMs)haverevolution-
izedthelandscapeofmachinelearning,yetcur-
rentbenchmarksoftenfallshortincapturingthe
diverse behavior of these models in real-world
applications. Abenchmark’susefulnessisdeter-
mined by its ability to clearly differentiate be-
tweenmodelsofvaryingcapabilities(separabil-
ity) and closely align with human preferences. Figure1:Comparedtootherbenchmarkframeworksourap-
Existing frameworks like Alpaca-Eval 2.0 LC proachintroducesadatapipelinethatcuratesunlabeleddata
(Duboisetal.,2024a)andArena-Hardv0.1(Li into categories that contain domains/capabilities that the
etal.,2024a)arelimitedbytheirfocusongeneral- practitionercaresabout. Ithasthecapabilitytoberefreshed
purposequeriesandlackofdiversityacrossdo- withnewdataandisdiversecomparedtoalternatives.
mains such as law, medicine, and multilingual
contexts. In this paper, we address these limi-
tationsbyintroducinganoveldatapipelinethat
1.Introduction
curatesdiverse, domain-specificevaluationsets
tailored for LLM-as-a-Judge frameworks. Our LargeLanguageModels(LLMs)havedramaticallychanged
approachleveragesacombinationofmanualcu- thelandscapeofmachinelearningresearchandhavebeen
ration,semi-supervisedlearningtogenerateclus- incorporated in products for the past few years. Along
ters, and stratified sampling to ensure balanced withtheirrise,amultitudeofbenchmarksandframeworks
representationacrossawiderangeofdomainsand (Liang et al., 2023) have been proposed to assess the ca-
languages. Theresultingevaluationset,whichin- pabilities of LLMs which include knowledge tasks such
cludes1573samplesacross14categories,demon- asMMLU(Hendrycksetal.,2021a),reasoningtaskslike
strates high separability (84%) across ten top- GSM8k(Cobbeetal.,2021)andmorestandardNLPtasks
rankedmodels,andagreement(84%)withChat- (Zellersetal.,2019;Narayanetal.,2018). However,these
botArenaand(0.915)Spearmancorrelation. The benchmarksfailtocapturethebehaviorthatauserexperi-
agreementvaluesare9%betterthanArenaHard encesinachat/generativeapplications. Typically,human
and 20% better than AlpacaEval 2.0 LC, while evaluationsareseenasagoldstandardtodeterminingwhich
the Spearman coefficient is 0.7 more than the LLMresponsesarepreferableoverothersinachatsetting
next best benchmark, showcasing a significant butistime-consumingandexpensivetoconduct(Chiang
improvementintheusefulnessofthebenchmark. etal.,2024).
Wefurtherprovideanopen-sourceevaluationtool
Toaddressthisshortcoming, Zhengetal. introducedthe
that enables fine-grained analysis of model per-
concept of LLM as a judge as an automatic evaluator al-
formance across user-defined categories, offer-
ternative, which uses another LLM the judging of model
ingvaluableinsightsforpractitioners. Thiswork
completions to another LLM such as GPT-4 or GPT-4o
contributestotheongoingefforttoenhancethe
(Zhengetal.,2023b;OpenAIetal.,2024). Alpaca-Evalis
transparency,diversity,andeffectivenessofLLM
anotherbenchmarkdesignedundertheparadigmofLLM
evaluationmethodologies.
asanevaluatorwhereatargetLLM’scompletionsarecom-
paredagainstareferenceLLM’soutput(thedefaultbeing
GPT-4Turbo)andassignedawinrateagainstthereference
1SambaNova Systems. Correspondence to: Ravi Raju
(Lietal.,2023). Ithasseenwidespreadadoptionsinceitis
<ravi.raju@sambanovasystems.com>.
cheap,fast,andmitigateslengthbias(Chiangetal.,2024).
Similarly,Arena-Hardv0.1isrecentbenchmarkwhichfo-
1
4202
guA
61
]GL.sc[
1v80880.8042:viXraConstructingDomain-SpecificEvaluationSetsforLLM-as-a-judge
Figure2: Alpaca-Evalcategorybreakdown Figure3: Arena-Hardv0.1categorybreakdown
cusesondistillingtheHardcategoryofChatbotArenainto to human-defined specific categories, generate those em-
asmallerevaluationset(Lietal.,2024a). Theyuseatopic beddingsandtrainak-NNclassifierwhichwecanuseto
clustering pipeline to cluster prompts with OpenAI’s em- classify the unlabeled data that we sampled. In order to
beddingmodel(text-embedding-3-small)(OpenAI,2024b) makesurethatnocluster/categorydominates,weemploy
and score each cluster based on difficulty, creativity, and stratifiedsamplingtoensurebalancedrepresentationacross
reasoningabilitywithGPT-3.5Turbo.Theyalsointroducea alldomainsandlanguagesintheevaluationset. Wefurther
notionofseparability(howwellcanabenchmarkdifferenti- refine the quality of the prompts by manual curation and
atebetweenmodels)andagreementwithhumanpreferences ensurethateachcategoryhasasufficientnumberofprompts
(i.e. ChatBotArena)asameasureofbenchmarkquality. tomitigatetheinherentvariabilityinLLM-as-a-judgeand
ultimatelyendupwith1573samplesintheevaluationset.
Unfortunately,therearestillsomelimitationswiththecur-
rentopen-sourceLLM-as-a-judgeframework. Alpaca-Eval Thereareseveraladvantagestoourapproachasshownin
2.0 LC is dominated by general chat queries/instructions Figure1.SimilartoArena-Hardv0.1,ourapproachisrobust
and has few prompts in domains such as coding, medi- tocontaminationaswecanperiodicallyrunourdatapipeline
cal, finance, law and mathematics as shown in Figure 2. onthesamedatatogetnewsamplesorpotentiallyevena
Arena-Hardv0.1addressessomeofthesedeficienciesby newdatamixture. Asmentionedearlier,ourmethodology
upweightingcodingandmathematicspromptsandrestrict- allowsintroductionofnewdatasetswhichenablesdiversity
ing the general chat queries to 30% if the evaluation set. ratherthanofferedbyArena-Hardv0.1andAlpaca-Eval. In
However,bothevaluationsetsarestrictlyinEnglishthere- addition,ourevaluationsetmorecloselymirrorsChatbot
forenotaccessingthemodel’smultilingualcapabilityand Arena rankings; Figure 4 shows a visual comparison of
haveasmallernumberofpromptsinmorenichecategories model rankings. In particular, our evaluation set places
likelawandmedicine. Asmodelsareacquiringmoreca- Gemini-1.5-Flash (DeepMind, 2024) over Gemma2 27B
pabilities across various data types such as charts/tables, Instruct (Team, 2024) which aligns with ChatBot Arena
domains and languages, it becomes crucial to determine rankingswhereastheothersrankGemma227BoverGemini-
howtoevaluateeachmodel’sabilityinascalablemanner. 1.5-Flash. Moreover,sinceweuseopensourcemodelsfor
theentirepipeline,practitionerscanmoldthepipelineand
Inthispaper,weattempttoaddresschallengesfromAlpaca-
generateevaluationsetstotestdomainsandcapabilitiesthey
Eval2.0LCandArena-Hardv0.1byintroducingmoredi-
careabout.
versityacrossdomainknowledgeandlanguages. Toaccom-
plishthis,weintroduceasimpledatapipelinemethodology Afterwehaveobtainedtheevaluationset,weexecutethe
to create a new evaluation set designed for these specific sameprocedureasLLM-as-a-Judgebygeneratingtheout-
contexts.First,wesourcepromptsfromvariousopensource puts completions from GPT-4o and using them as refer-
datasets(showninTable4)toensureourevaluationsethas encetoconstructaleaderboardfromtenvariousopenand
highdatadiversity. Forthenextstep,wegenerateembed- closed-sourcedmodels. Withthislabelingapproach,weare
dingsfromasubsampleofeachofthesedatasetsusingan abletobreakdownthecompositionofpromptsintovarious
embeddingmodel. Tolabelthecorrespondingembeddings, categories and report category win rates. We release an
wemanuallycurateaseedsetofpromptsandlabelthem evaluationtoolwhichdisplaysthecategorywinrateforall
2ConstructingDomain-SpecificEvaluationSetsforLLM-as-a-judge
open-sourcemodelscanbeusedtoseparatebetween
modelrankings,agreementwithChatbotArenamodel
rankingsisroughly10%(405B)and20%(70B)than
GPT-4o.
2.RelatedWork
Attheircore,benchmarksaretooltoestimateLLMcapa-
bilities. There are many different flavors of benchmarks,
spanningeitheracrossdomainsorvarioustasks.Somepopu-
larbenchmarksinclude: Boolq(Clarketal.,2019),MMLU
(Hendrycks et al., 2021a), GSM8k (Cobbe et al., 2021),
MATH(Hendrycksetal.,2021b),XSUM(Narayanetal.,
2018), Hellaswag (Zellers et al., 2019), and MGSM (Shi
etal.,2022).Anexpandedframeworkofstaticbenchmarkis
AutoBencherwhichautomaticallycreatesnewbenchmarks
Figure4: Visualcomparisonbetweenourmethod,Arena- whichfindsholesinknowledgeofcurrentSOTALLMs(Li
Hardv0.1,andAlpaca-Eval2.0LCon10modelsonsep- etal.,2024b).
arability of winrates. Our method has fewer overlaps of
These types of benchmarks have ground-truth references
confidenceintervalsthantheotherbaselines.
andcomparehowcloselytheLLM’scompletionalignswith
thosereferences. Aninherentlimitationwithstaticbench-
marks is that they are hosted on the internet and thus are
modelsontheleaderboardandanexplorerwhichdisplays
susceptibletestleakagecontamination(Sainzetal.,2023;
boththetargetmodelaswellasthereferencemodel’scom-
Yangetal.,2023).Theotherstyleofbenchmarkingrelieson
pletionsforapromptandthereasoninggivenbytheLLM
constructingahumanevaluationtrialsonasetofevaluation
judge. Thisanalysistoolallowsuserstoobtainfine-grained
prompts.Duetotheexpensivenatureofhumanevaluation,a
insightsonwheredifferentmodelssucceedandfailfortheir
recent,cheaperalternativeistouseSOTALLMstoevaluate
particularuse-case.
modelcompletionseitherthroughsinglescoreorpairwise
Ourmaincontributionscanbesummarizedbelow: comparisonwithareferenceanswer,popularlyreferredto
asLLM-as-a-Judge(Lietal.,2023;Zhengetal.,2023b;Li
etal.,2024a;Duboisetal.,2024b;Vergaetal.,2024).
• Weintroduceanewmethodologythatenablescreation
ofabenchmarkthattestsfordiverseskillsetsofmod- Thismotivatestheneedfor”live,refreshable”benchmarks
els. Weopen-sourceourevaluationinfrastructureso so that the integrity of the benchmark can be maintained.
practitionerscanviewhowdifferentmodelsperform LiveBenchisaframeworkwhichsourcesdatafromarXiv
on separate tasks according to how they define their papers,newsarticles,anddatasetstoperiodicallyreplace
categories. This fine-grained breakdown allows the the stale prompts (White et al., 2024). Chatbot arena is
practitionertoselectmodelsthatworkwellfortheir anopenplatformthatallowsonlineuserstosendprompts
particularusecase. totwodifferentmodelsandcompare/contrastthemodels’
response(Chiangetal.,2024).Userscanthenvoteonwhich
• Our benchmark creation methodology encourages
completion was superior. Other live benchmarks include
morediversityandtransparencytothepractitionercom-
DynaBench(Kielaetal.,2021),LiveCodeBench(Jainetal.,
paredtootheralternatives.Incomparisontootherbase-
2024),andR2E(Jainetal.).Ourworkliesintheintersection
lineslikeAlpaca-EvalandArena-Hardv0.1,ourbench-
betweenLLM-as-a-Judgeandlivebenchmarksasourdata
markhas84%separability,84%agreementwithcon-
pipeline enables periodic refreshing of the evaluation set
fidenceinterval(95%)withrespecttoChatbotArena
from existing clusters. Furthermore, our data pipeline is
rankings,0.915Spearman’scorrelationcoefficientwith
fairlygeneralasitcanconsumeavarietyofdiversedatasets
respecttoChatbotArenarankingsand0.04BrierLoss
(relativetoArena-Hardv0.1andAlpaca-Eval),consistsof
Score.
usingopen-sourcemodels,andisflexibleenoughtowork
ontheuser’sdesireddata.
• We also analyze the aforementioned metrics on our
evaluationsetwith4LLMjudges: GPT-4o(OpenAI
et al., 2024), GPT-4o-mini (OpenAI, 2024a), Llama
3.1405BInstructandLlama3.170BInstruct(Dubey
etal.,2024). Ouroverallfindingssuggestthatwhile
3ConstructingDomain-SpecificEvaluationSetsforLLM-as-a-judge
Figure5: Datapipeline: Afteraggregatingthepromptsfromdatasets,wegenerateembeddingsusingatextembedding
model. Wesetasideasetofpromptstouseasaseedsetfortrainingthek-NN,labelthemintoeachcategorywecareabout,
andgeneratetheircorrespondingembeddingstotrainthek-NNwiththeembeddingmodel. Subsequently,weclassifythe
unlabeleddatawithourtrainedk-NNtocreateclustersofcategories. Webalancetheclusterswithstratifiedsamplingand
thenmanuallycuratetheremainingpromptsbyremovingoverlylongprompts(greaterthan5000words)andcheckingfor
low-qualitycontent(nonsenseprompts,NSFWetc.) toobtainthefinalevaluationset.
3.Methodology them distinct labels) and embed those prompts with the
aforementionedembeddingmodel. Wetrainak-NNmodel
Inthissection,wedescribeourapproachtocreatingnovel
(Mucherinoetal.,2009)ontopofthoseembeddingsand
evaluation set using LLM-as-a judge. We enumerate the
usethek-NNtolabelthelargerunlabeledcorpus.
datasetsthatwesourcefromtocreateourunlabeledcorpus
andsubsequentlydescribeourdatapipelineforgenerating The final step in our pipeline involves applying stratified
theevaluationset. sampling (Parsons, 2017) to each cluster. The reason for
thislaststepisthatwewantourevaluationsettoretaindi-
3.1.DataSources versityofourlargerdatacorpusratherthanuniformrandom
sampling. Foreachcategory,wesub-sample100prompts
We use data sources from a variety of source to ensure from the aggregate clusters and disregard clusters which
wecoveravarietyofdomainsaswellaslanguages. The have a lower count than the number of prompts we sam-
domains we target can be broadly classified as the fol- pled. Toobtainourfinalevaluationset,wemanuallycurate
lowing: medical, law, finance, mathematics and coding. theremainingpromptstoensurehighquality,variedtask
The languages we cover are standard but also more eso- capabilityanddatadiversity.
teric: Japanese(ja),Arabic(ar),Thai(th),Hungarian(hu),
Russian(ru),Serbian(sr),Slovenian(sl),andTurkish(tr).
4.ExperimentalSetup
Prompts that don’t neatly fit into these groups fall into a
catch-allgeneralcategory. Acompletelistofallthedatawe In this section, we discuss finer details about the data
usecanbefoundinTable4intheAppendix. pipelinewementionedinthepriorsection. experimental
setuponasetoftenhighlyratedmodels1aswellasdefining
3.2.Datapipeline themetricswhichdeterminethequalityofthebenchmark.
Our data pipeline can be divided into 3 distinct steps, as
4.1.Datapipelinedetails
showninFigure5. Wefirsttakethedatacorpusandusean
embeddingmodeltogeneratetheircorrespondingembed- Forthedatapipeline,weusesemi-supervisedlearningvia
ding. Eachembeddingencapsulatessomelevelofsemantic a k-NN classifier. We consider 13 categories comprising
understandingofitsassociatedprompt,andnearbyembed- ofdomains: finance,law,medical,maths,codingandlan-
dingstypicallyencodesimilarsemanticinformation. guages: Arabic, Russian, Serbian, Hungarian, Japanese,
Togeneratethelabelsfortheunlabeleddata,wetakeinspi-
1gpt-4o-2024-05-13, claude-3-5-sonnet-20240620, claude-
rationfromsemi-supervisedlearning(Hady&Schwenker, 3-opus-20240229, gemini-1.5-flash-latest, google/gemma-2-
2013). Wemanuallydefineasetofcategories,curateaseed 27b-it, Meta-Llama-3-70B-Instruct, claude-3-sonnet-20240229,
setofpromptswhichfallintothosecategories(assigning Qwen/Qwen2-72B-Instruct,Meta-Llama-3-8B-Instruct,Mixtral-
8x7B-Instruct-v0.1
4ConstructingDomain-SpecificEvaluationSetsforLLM-as-a-judge
ThaiandSlovenian.Wefollowusualsupervisedtrainingand garian. Thejudgemodelwillratethemodelansweringin
viahyperparametersweepovervalidationsetyieldk =40 theincorrectlanguagehigher,whichisclearlynotamea-
asthebestvalueofk. sureofthemodel’smultilingualcapability(Marchisioetal.,
2024). Inordertoreducetheseincorrectdecisions,wemod-
Togeneratetheembeddingsoftheunlabeleddatacollected,
ifiedthejudgeprompttospecificallypenalizeresponsesthat
weusethee5-mistral-7b-instructembeddingmodel(Wang
respondtothepromptintheincorrectlanguage.
et al., 2024) for its strong performance on the Massive
TextEmbeddingBenchmark(MTEB)Leaderboard(Muen- In addition to issues with multilingual queries, we also
nighoff et al., 2022) and multilingual capability. If the note specifically for coding that GPT-4o seems to prefer
k-NN encounters a sample which it is not familiar with models which provide detailed explanations to the code
oruncertaintolabel, wewantthosesamplestobeclassi- evenifthecodeprovidedisoflowerqualitycomparedtoa
fiedasgeneralprompts. Weuseentropyofk-NNclassifier modelwhichhasbettercodequalitybutisnotasverbose.
probabilitiesofvariouscategoriesforagivenpromptasthe This leads to scenarios where models that have chat but
measureofuncertainty. Ifentropyiftoohighentropyofthe lower benchmark performance (e.g. HumanEval (Chen
output of the classifier is too high, we bucket the sample etal.,2021))obtainhigherwinratethanmodelswhichare
intothedefault/generalcategory(Settles,2010). Wesetthe objectivelybetteroncodingprompts. Tocircumventthis
entropythresholdtobe1.5basedoncarefulerroranalysis issue,weexplicitlypromptGPT-4othatitshouldfocuson
onthevalidationset. thecorrectnessoftheresponseasopposedtothestyleofthe
response. OurjudgetemplatecanbefoundintheAppendix.
Afterlabelingwithk-NN,weconductedstratifiedsampling
withineachcluster,selecting100samplesforcuration. We
4.3.ObtainingConfidenceIntervals
thenfilteredoutexcessivelylongprompts(longerthan5000
words)thatcouldoverwhelmthejudge’scontextwindow. WefollowthesetupoutlinedinLiet. al(Lietal.,2024a;
Additionally,wereviewedtheremainingpromptstoelimi- Chiang et al., 2024). We use the Bradley-Terry model in
natethosethatwerenonsensicaloroflowquality. During ordertomodelthepreferencedistributionbetweenmodels
the evaluation, we observed that categories with a small on the leaderboard and the reference model (GPT-4o in
numberofexampleshadasignificantimpactonthecate- ourcase). Weaggregatepreferencepairsbetweenmodels
gory’swinrate. TheinherentvariabilityoftheLLM-as-a- and perform 100 rounds of bootstrapping to obtain 95%
Judgeevaluation,evenwithafixedrandomseedandtem- confidenceintervalsforeachmodelranking.
perature set to 0.0, made it challenging to discern which
Weconductthesameanalysiswithannotations,denoting
modelperformedbetterinthosecategories. Tomitigatethis
foreachpromptwhichmodelresponsewaspreferred,from
uncertainty,weensuredthatanycategorywithfewerthan
theAlpaca-EvalrepotoobtainmeanELOrankingsand95%
90-100exampleswassupplementedwithadditionaldata,
confidenceintervalsaccordingtotheirleaderboard. Since
enablingustoobtainmeaningfulandinterpretableresults.
similar artifacts (model preference comparisons) are not
Ourfinalevaluationsetcomprises1573examples.
updatedonArena-Hardv0.1,wetakethemodelwinrates
(ELOscoresnotlisted)and95%confidenceintervalsfrom
4.2.LLM-as-a-JudgeDetails
theirrepo2. ForChatbotArena,wedothesamethingand
WefollowasimilarscoringsetupasArena-Hard(Lietal., tookmodelwinrates/ELOscoresaswellastheconfidence
2024a)andAlpaca-Eval(Duboisetal.,2024a)wherewe intervalsfromthewebsite3asasourceofgroundtruth.
use GPT-4o as a judge model and GPT-4o as a reference
modelaswell. Foreachmodelwewanttotest,weobtain 4.4.Metrics
the completions and ask GPT-4o to record which model
Therearefourdifferentmetricsweusetojudgetheefficacy
responsesisbetterfortheinputprompt. Inordertomitigate
ofabenchmark. ThefirstoftheseisSpearman’scorrelation
positionalbias,weswapthecompletionsbetweenthemodel
coefficient,whichmeasurestherankingsorderbetweenthe
weareevaluatingandthereferenceonacoinflip.
twobenchmarks. Theothermetricsare: separability,agree-
For the judge prompt, we used the default prompt from mentwithConfidenceInterval(CI),andBrierScore.Separa-
theMT-Benchworkwithonenotablechange(Zhengetal., bilityreferstohowwellthebenchmarkcanseparatevarious
2023b). When we evaluated multilingual prompts with modelswithhighconfidence. Inparticular,ifonbenchmark
LLM-as-a-judge, the judge at times incorrectly awards AmodelM1hasahigherELO/winratethanmodelM2and
winstomodelswhichdon’tnecessarilyfollowinstructions. C referstotheconfidenceintervalsofmodelM,S isa
M
Giventhesentence”Pleaserespond’Howdoestheeconomy binaryvariableindicatingifbenchmarkAisabletoseparate
work?’ in Hungarian,” two models might respond differ-
ently: 1) one provides a detailed English response with
27/26/2024
37/25/2024
bulletedlists,while2)theotherrespondsconciselyinHun-
5ConstructingDomain-SpecificEvaluationSetsforLLM-as-a-judge
ChatbotArena ArenaHardv0.1 Alpaca-Eval2.0LC Ours
Separability 100% 80% 73.33% 84.44%
AgreementwithCI(95%) N/A 75.50% 64.44% 84.44%
Spearman’sCorrelation N/A 0.187 0.2969 0.915
BrierScore N/A N/A 0.0937 0.0417
Table1: Mainresultscomparingthevariousbenchmarks.
betweenmodelM1andM2,S =1 . Thesepa- arethelatestSOTAmodelsthathavebeenreleasedsoasto
CM1∩CM2=∅
rabilityisthencalculatedasaratiooverallpossiblemodel havethemaximumamountofoverlappossible. Finally,our
pairs.AgreementwithCImeasureshowwellbenchmarksA benchmarkscoredaBrierscoreof0.0417,whichislower
andBconfidentlydistinguishbetweentwomodelswiththe than Alpaca-Eval 2.0 LC’s 0.0937, demonstrating better
sameordering. TheBrierScoreevaluatesanLLMbench- confidenceinaccuratepredictions.
mark’sabilitytopredicttherankingofapairofcompeting
models,rewardingconfidenceinaccuratepredictionsand 5.2.Diversity
penalizingconfidenceinincorrectones.Moredetailsbehind
Duetoourdatasourcesbeingquitediverseratherthansim-
thesemetricscanbefoundin(Lietal.,2024a). Ultimately,
plyjustChatBotArena(Chiangetal.,2024),weareable
wewantourbenchmarktoalignwithChatbotArenaasthat
to have more diversity in our evaluation set. To demon-
isseenasanoracleformodelinghumanpreferences.
stratethis,welabelArena-Hardv0.1withourkNNmodel
using the entropy threshold to get a distribution of cate-
5.Results
goriesinthatevaluationset. AsshowninFigure3,there
isanover-representationofcodingprompts,whichcomes
5.1.Separability,AgreementwithCI(95%),PairBrier
fromabyproductoftheirdatapipelinefilteringforthehard-
Score
est,highestqualitywhichskewstowardscoding. Similarly,
Our main results can be found in Table 1. With the ex- Alpaca-Eval’spromptdistributionshowninFigure2demon-
ceptionofChatbotArena,ourbenchmark’sseparabilityis stratesthatthereisalargeemphasisongeneralchatqueries,
84.4%comparedtootherbaselineslikeArena-Hardv0.1 alongwithsomecodingandmathpromptswhilemedical
(80%)andAlpaca-Eval2.0LC(73.33%),whichshowsthat andlawpromptsarerelativelyunderrepresented.
our benchmark can better differentiate amongst different
Our evaluation set breakdown in Figure 6(a) which cov-
models.
ers more domains than the baseline, such languages like
OneinterestingdatapointregardingseparabilityisChatbot Arabic,Japanese,Hungarianandmore. Theclosetoequal
Arena’sscoreof100%whichmaybeattributedtoacom- distributionamongstthecategoriesislikelyduetotheef-
binationoftwofactors: 1)ChatbotArenahasmorebattles fectstratifiedsampling. Wecomparehowourevaluation
thananyofthebenchmarkslistedinTable1and2)Chatbot setcategorybreakdowncomparedwithLM-SYSConver-
Arenaincludesbattlesbetweenmanydifferentmodelsrather sations(usingourk-NNlabelingapproach)(Zhengetal.,
thanfixingareferencemodelliketheotherbenchmarks. By 2023a)inFigure6(b),whichisasnapshotofcleanedChat-
providingtheBradley-Terrymodelbootstrappingprocess botArenaconversationsfromApriltoJune2023. InFigure
withmorevariedbattles,ChatbotArenaisabletoproduce 6(b), ”Other” refers to the languages our k-NN classifier
tighterconfidenceintervals,suggestingafutureavenuefor recognizesbutgroupsthemtogethercollectively. Wenote
investigation is whether confidence estimation should in- thatthisdistributionlookssimilartoAlpaca-Evalandthe
clude multiple reference answers during judging to more generalcategorymaycontainadditionallanguagesnotrec-
closelysimulateChatbotArena. ognizedbytheclassifiersoitmayhaveexceededtheentropy
threshold.
Ourbenchmarkshowedan84.44%agreementwithCIwith
respecttoChatbotArena,whichishigherthanArena-Hard
5.3.CategorySeparability
v0.1’s 75.50% and Alpaca-Eval 2.0 LC’s 64.44%. This
demonstratesthatourbenchmarkhashigheralignmentwith
Duetoouruniqueabilitytocategorizetheprompts,wecan
respecttoChatbotArenawhichissupposedtobeapproxi-
computecategoryseparabilityforallthevariouscategories
mationofhumanpreferences. Inaddition,ourbenchmark
inourevaluationset. Across14differentcategories,wedo
hasaSpearman’scorrelationcoefficientof0.915,indicating
thesamebootstrappingprocedureonthecategorydatato
astrongcorrelationinrankingsordercomparedtoAlpaca-
obtainthemeanwinrate/ELOand95%CI,showninTable2.
Eval2.0LC’s0.2969. Whileourleaderboardrankingcon-
Ingeneral,thereisadropinseparabilitywhenwelookboth
sists of 10 models, the pool of models we have included
6ConstructingDomain-SpecificEvaluationSetsforLLM-as-a-judge
(a) Ourevaluationsetcategorybreakdown (b) LMSysConversationscategorybreakdown
Figure6: Welookatthecategorybreakdownonourevaluationset(Figure6(a))comparedtoLM-SYSConversations
(Figure6(b)). Wecanclearlyseethatourevaluationsetcoversmorelanguagesandnichedomainssuchaslawandmedical
categoriesareahigherpercentageofourevaluationset.
Category Rankingwinrate RankingELO 5.4.Usingdifferentjudges
ar 73.33% 57.78%
Weconductanablationofjudgemodelsonourevaluation,
ru 71.11% 55.56%
as we want to understand the effect of judge models on
finance 75.56% 57.78%
separability,AgreementwithCI(95%)andBrierScore. We
sr 71.11% 53.33%
considerGPT-4ominiasoneofthejudgestobeasmall-
tr 73.33% 55.56%
closed source foil to GPT-4o. The other judges that we
general 77.78% 55.78%
considerareopensourcemodelssuchas: Llama3.1405B
hu 75.56% 66.67%
instruct(usingSambaNova’sdeveloperAPI)4andLlama3.1
ja 71.11% 57.78%
70BInstruct-Turbo5. Wefollowthesamesetupasgpt-4o
medical 68.89% 55.56%
withtheseotherjudgemodels.
law 73.33% 51.11%
th 71.11% 57.78% Our results are shown in Table 3. In terms of separabil-
coding 73.33% 55.56% ity, GPT-4o-mini and 405B get 82.2% and 70B get 84%
sl 77.78% 53.33% separability, comparable to GPT-4o’s separability. 405B
math 73.33% 55.56% andGPT-4o-miniattainsimilarAgreementwithCI(95%)
closeto76%while70Bisalmost10pointslower;GPT-4o
Table 2: Winrate and ELO separability for different cate-
is the clearwinner having the highest agreement with CI
gories
(95%). Withtheexceptionof70B,allmodelsgetsimilar
BrierScoresindicatingthattheBradley-Terrymodelsused
to generate the rankings on confidence intervals for each
judgearesimilarlyconfident. 70B’shighBrierscore(rel-
ative to other judges), in addition to Agreement with CI,
atELOratingsandwinrateduetoeachcategoryhavinga
indicatesthatitpoorjudgethantheotherlistedinTable3.
lowernumberofsamplesandthuslargerCIsasaresult.
TheSpearman’scorrelationcoefficient(withrespecttoChat-
Thecategory-wiseseparabilitycanactasanindicatorwhich
BotArenarankings)seemstoindicatethatGPT-4o-mini,
categories are superior at testing out the performance of
Llama3.1405B,and70Barepoorjudgesgettingacorrela-
models. Interestingly, across ELO and winrate rankings,
tionofonly0.0787vs. GPT-4o’s0.915. LookingatFigure
Hungarianhasthebestseparabilityofallcategories,achiev-
7, itseemsthisaberrationcomesfrombothjudgesrating
ing66.67%and75.56%respectively. Themedicalcategory
seemstobelowestseparabilityaround55.56%and68.89% 4https://sambanova.ai/fast-api
respectively. Theseparabilityalsoindicatestousewhich 5https://api.together.ai/models/meta-llama/Meta-Llama-3.1-
categorieswemayneedtoaddmoresamplestoimprovethe 70B-Instruct-Turbo
confidenceintervals.
7ConstructingDomain-SpecificEvaluationSetsforLLM-as-a-judge
GPT-4o GPT-4o-mini Llama3.1405B Llama3.170B
Separability 84.44% 82.22% 82.22% 84.44%
AgreementwithCI(95%) 84.44% 76.77% 75.55% 66.66%
Spearman’sCorrelation 0.915 0.0787 0.0787 0.0787
BrierScore 0.0417 0.062 0.0603 0.0955
Table3: Comparingvariousjudgesonourevaluationset.
ClaudeSonnet3.5overGPT-4o,Llama370boverClaude ity,welooktocreatingamethodologyonfiguringoutthe
OpusandGemma227BoverGemini1.5Flash. Ofcourse, minimumnumberofsamplesrequiredtoimproveseparabil-
Spearman’scorrelationonlymeasurescorrelationthefinal ity.
rank order of models with respect to ChatBot Arena and
Theotheraspectoffutureworkreliestodetailsregarding
isastrictlyweakermetricthanAgreementwithCI(95%).
LLM-as-a-judge evaluation. Typically, the judge models
Thisfindingseemstosuggestwhileweakerclosed-source
areablatedbutlessexploredisthequalityofthereference
models(likeGPT-4o-mini)andopen-sourcejudgemodels
answer and whether one can use a weaker model instead
seemtobeabletoseparateothermodelsbasedoncapability,
ofastrongeronetoseeifmetricsaremaintained. Current
they stilllack the precisenessthat GPT-4ooffers toalign
metricsdefinehowseparableabenchmarkisandhowmuch
withrankingsfromChatbotArena.
italignswithhumanpreferencesbutfailstoaccountforthe
compositionanddiversityoftheunderlyingdata. Forfuture
work,weseektoquantifythediversityofeachbenchmark
tounderstandhowmanycapabilities/domainsitspans.
7.Conclusion
We introduce a data pipeline that leverages via semi-
supervisedlearningwithak-NNtoenablepractitionersto
createbenchmarksontheirowndatafortargeteddomains.
Throughevaluationsoftenvariousclosedandopen-sourced
models, we demonstrated that our benchmark achieves
higherseparabilityandagreementwithCIwithrespectto
Chatbot Arena, nearly 5 and 10 percentage points higher
than the next best baseline, respectively. Our benchmark
coversawidevarietyoftopicssuchasfinance,medicine,
legalanddifferentlanguagesabsentinotherLLMasajudge
benchmarks. We hope that LLM developers can use our
data pipeline to create their own benchmarks to evaluate
theirmodelsfortheirparticularuse-case.
Figure7:Visualcomparisonofdifferentjudge’sseparability
onourbenchmark.
References
Amini,A.,Gabriel,S.,Lin,S.,Koncel-Kedziorski,R.,Choi,
6.Limitations/FutureWork
Y., and Hajishirzi, H. MathQA: Towards interpretable
math word problem solving with operation-based for-
There are certain limitations to our work. Currently, the
malisms. InProceedingsofthe2019Conferenceofthe
categoriesweenumerateinourdatapipelineismanually
North American Chapter of the Association for Com-
specified by humans and significant curation is done to
putationalLinguistics: HumanLanguageTechnologies,
ensurehighqualityprompts; forfuturework,wewantto
Volume1(LongandShortPapers),pp.2357–2367,Min-
expand to using LLMs as category generators as well as
neapolis,Minnesota,June2019.AssociationforCompu-
qualitycheckerstoautomatethehumaneffortoutofthis
tationalLinguistics. doi: 10.18653/v1/N19-1245. URL
pipeline. Forimprovingourleaderboard, wewishtoadd
https://aclanthology.org/N19-1245.
moremodelstobemorerepresentativeoftheentirespec-
trumofotherleaderboardsandfuthurincreasingthequality
oftheBradley-Terrymodelsweusetoobtainthemodel’s Chen,M.,Tworek,J.,Jun,H.,Yuan,Q.,deOliveiraPinto,
confidenceintervals. Inordertoimprovecategoryseparabil- H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N.,
8ConstructingDomain-SpecificEvaluationSetsforLLM-as-a-judge
Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, K.,Stone,K.,El-Arini,K.,Iyer,K.,Malik,K.,Chiu,K.,
M.,Khlaaf,H.,Sastry,G.,Mishkin,P.,Chan,B.,Gray, Bhalla,K.,Rantala-Yeary,L.,vanderMaaten,L.,Chen,
S.,Ryder,N.,Pavlov,M.,Power,A.,Kaiser,L.,Bavar- L.,Tan,L.,Jenkins,L.,Martin,L.,Madaan,L.,Malo,L.,
ian,M.,Winter,C.,Tillet,P.,Such,F.P.,Cummings,D., Blecher,L.,Landzaat,L.,deOliveira,L.,Muzzi,M.,Pa-
Plappert,M.,Chantzis,F.,Barnes,E.,Herbert-Voss,A., supuleti,M.,Singh,M.,Paluri,M.,Kardas,M.,Oldham,
Guss, W. H., Nichol, A., Paino, A., Tezak, N., Tang, M., Rita, M., Pavlova, M., Kambadur, M., Lewis, M.,
J., Babuschkin, I., Balaji, S., Jain, S., Saunders, W., Si,M.,Singh,M.K.,Hassan,M.,Goyal,N.,Torabi,N.,
Hesse, C., Carr, A. N., Leike, J., Achiam, J., Misra, Bashlykov,N.,Bogoychev,N.,Chatterji,N.,Duchenne,
V., Morikawa, E., Radford, A., Knight, M., Brundage, O., C¸elebi, O., Alrassy, P., Zhang, P., Li, P., Vasic, P.,
M., Murati, M., Mayer, K., Welinder, P., McGrew, B., Weng, P., Bhargava, P., Dubal, P., Krishnan, P., Koura,
Amodei,D.,McCandlish,S.,Sutskever,I.,andZaremba, P.S.,Xu,P.,He,Q.,Dong,Q.,Srinivasan,R.,Ganapa-
W. Evaluating large language models trained on code, thy,R.,Calderer,R.,Cabral,R.S.,Stojnic,R.,Raileanu,
2021. R., Girdhar, R., Patel, R., Sauvestre, R., Polidoro, R.,
Sumbaly, R., Taylor, R., Silva, R., Hou, R., Wang, R.,
Chiang,W.-L.,Zheng,L.,Sheng,Y.,Angelopoulos,A.N.,
Hosseini,S.,Chennabasappa,S.,Singh,S.,Bell,S.,Kim,
Li,T.,Li,D.,Zhang,H.,Zhu,B.,Jordan,M.,Gonzalez,
S.S.,Edunov,S.,Nie,S.,Narang,S.,Raparthy,S.,Shen,
J. E., and Stoica, I. Chatbot arena: An open platform
S., Wan, S., Bhosale, S., Zhang, S., Vandenhende, S.,
for evaluating llms by human preference, 2024. URL
Batra,S.,Whitman,S.,Sootla,S.,Collot,S.,Gururangan,
https://arxiv.org/abs/2403.04132.
S.,Borodinsky,S.,Herman,T.,Fowler,T.,Sheasha,T.,
Georgiou,T.,Scialom,T.,Speckbacher,T.,Mihaylov,T.,
Clark,C.,Lee,K.,Chang,M.-W.,Kwiatkowski,T.,Collins,
Xiao,T.,Karn,U.,Goswami,V.,Gupta,V.,Ramanathan,
M.,andToutanova,K. Boolq: Exploringthesurprising
V.,Kerkez,V.,Gonguet,V.,Do,V.,Vogeti,V.,Petrovic,
difficultyofnaturalyes/noquestions,2019.URLhttps:
V.,Chu,W.,Xiong,W.,Fu,W.,Meers,W.,Martinet,X.,
//arxiv.org/abs/1905.10044.
Wang,X.,Tan,X.E.,Xie,X.,Jia,X.,Wang,X.,Gold-
Cobbe,K.,Kosaraju,V.,Bavarian,M.,Chen,M.,Jun,H., schlag,Y.,Gaur,Y.,Babaei,Y.,Wen,Y.,Song,Y.,Zhang,
Kaiser,L.,Plappert,M.,Tworek,J.,Hilton,J.,Nakano, Y., Li, Y., Mao, Y., Coudert, Z. D., Yan, Z., Chen, Z.,
R.,Hesse,C.,andSchulman,J.Trainingverifierstosolve Papakipos,Z.,Singh,A.,Grattafiori,A.,Jain,A.,Kelsey,
math word problems, 2021. URL https://arxiv. A.,Shajnfeld,A.,Gangidi,A.,Victoria,A.,Goldstand,
org/abs/2110.14168. A., Menon, A., Sharma, A., Boesenberg, A., Vaughan,
A., Baevski, A., Feinstein, A., Kallet, A., Sangani, A.,
DeepMind. Gemini flash, 2024. URL https:
Yunus,A.,Lupu,A.,Alvarado,A.,Caples,A.,Gu,A.,
//deepmind.google/technologies/
Ho,A.,Poulton,A.,Ryan,A.,Ramchandani,A.,Franco,
gemini/flash/. Accessed: 2024-08-14.
A.,Saraf,A.,Chowdhury,A.,Gabriel,A.,Bharambe,A.,
Eisenman,A.,Yazdan,A.,James,B.,Maurer,B.,Leon-
Dubey,A.,Jauhri,A.,Pandey,A.,Kadian,A.,Al-Dahle,A.,
hardi,B.,Huang,B.,Loyd,B.,Paola,B.D.,Paranjape,B.,
Letman,A.,Mathur,A.,Schelten,A.,Yang,A.,Fan,A.,
Liu,B.,Wu,B.,Ni,B.,Hancock,B.,Wasti,B.,Spence,
Goyal,A.,Hartshorn,A.,Yang,A.,Mitra,A.,Sravanku-
B.,Stojkovic,B.,Gamido,B.,Montalvo,B.,Parker,C.,
mar,A.,Korenev,A.,Hinsvark,A.,Rao,A.,Zhang,A.,
Burton,C.,Mejia,C.,Wang,C.,Kim,C.,Zhou,C.,Hu,
Rodriguez,A.,Gregerson,A.,Spataru,A.,Roziere,B.,
C., Chu, C.-H., Cai, C., Tindal, C., Feichtenhofer, C.,
Biron,B.,Tang,B.,Chern,B.,Caucheteux,C.,Nayak,
Civin, D., Beaty, D., Kreymer, D., Li, D., Wyatt, D.,
C.,Bi,C.,Marra,C.,McConnell,C.,Keller,C.,Touret,
Adkins, D., Xu, D., Testuggine, D., David, D., Parikh,
C.,Wu,C.,Wong,C.,Ferrer,C.C.,Nikolaidis,C.,Al-
D.,Liskovich,D.,Foss,D.,Wang,D.,Le,D.,Holland,
lonsius,D.,Song,D.,Pintz,D.,Livshits,D.,Esiobu,D.,
D.,Dowling,E.,Jamil,E.,Montgomery,E.,Presani,E.,
Choudhary,D.,Mahajan,D.,Garcia-Olano,D.,Perino,
Hahn,E.,Wood,E.,Brinkman,E.,Arcaute,E.,Dunbar,
D.,Hupkes,D.,Lakomkin,E.,AlBadawy,E.,Lobanova,
E.,Smothers,E.,Sun,F.,Kreuk,F.,Tian,F.,Ozgenel,F.,
E., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F.,
Caggioni,F.,Guzma´n,F.,Kanayet,F.,Seide,F.,Florez,
Synnaeve,G.,Lee,G.,Anderson,G.L.,Nail,G.,Mialon,
G.M.,Schwarz,G.,Badeer,G.,Swee,G.,Halpern,G.,
G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H.,
Thattai,G.,Herman,G.,Sizov,G.,Guangyi,Zhang,Lak-
Xu,H.,Touvron,H.,Zarov,I.,Ibarra,I.A.,Kloumann,
shminarayanan,G.,Shojanazeri,H.,Zou,H.,Wang,H.,
I., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J.,
Zha,H.,Habeeb,H.,Rudolph,H.,Suk,H.,Aspegren,H.,
Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der
Goldman,H.,Molybog,I.,Tufanov,I.,Veliche,I.-E.,Gat,
Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi, J.,
I.,Weissman,J.,Geboski,J.,Kohli,J.,Asher,J.,Gaya,
Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak,
J.-B.,Marcus,J.,Tang,J.,Chan,J.,Zhen,J.,Reizenstein,
J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Al-
J.,Teboul,J.,Zhong,J.,Jin,J.,Yang,J.,Cummings,J.,
wala,K.V.,Upasani,K.,Plawiak,K.,Li,K.,Heafield,
9ConstructingDomain-SpecificEvaluationSetsforLLM-as-a-judge
Carvill,J.,Shepard,J.,McPhie,J.,Torres,J.,Ginsburg, Hady,M.F.A.andSchwenker,F. Semi-supervisedLearn-
J., Wang, J., Wu, K., U,K.H., Saxena, K., Prasad, K., ing, pp. 215–239. Springer Berlin Heidelberg, Berlin,
Khandelwal,K.,Zand,K.,Matosich,K.,Veeraraghavan, Heidelberg, 2013. ISBN 978-3-642-36657-4. doi:
K.,Michelena,K.,Li,K.,Huang,K.,Chawla,K.,Lakho- 10.1007/978-3-642-36657-4 7. URL https://doi.
tia, K., Huang, K., Chen, L., Garg, L., A,L., Silva, L., org/10.1007/978-3-642-36657-4_7.
Bell, L., Zhang, L., Guo, L., Yu, L., Moshkovich, L.,
Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika,
Wehrstedt,L.,Khabsa,M.,Avalani,M.,Bhatt,M.,Tsim-
M., Song, D., and Steinhardt, J. Measuring massive
poukelli,M.,Mankus,M.,Hasson,M.,Lennie,M.,Reso,
multitasklanguageunderstanding,2021a. URLhttps:
M.,Groshev,M.,Naumov,M.,Lathi,M.,Keneally,M.,
//arxiv.org/abs/2009.03300.
Seltzer,M.L.,Valko,M.,Restrepo,M.,Patel,M.,Vy-
atskov,M.,Samvelyan,M.,Clark,M.,Macey,M.,Wang,
Hendrycks,D.,Burns,C.,Kadavath,S.,Arora,A.,Basart,
M.,Hermoso,M.J.,Metanat,M.,Rastegari,M.,Bansal,
S.,Tang,E.,Song,D.,andSteinhardt,J.Measuringmath-
M.,Santhanam,N.,Parks,N.,White,N.,Bawa,N.,Sing-
ematicalproblemsolvingwiththemathdataset,2021b.
hal,N.,Egebo,N.,Usunier,N.,Laptev,N.P.,Dong,N.,
URLhttps://arxiv.org/abs/2103.03874.
Zhang,N.,Cheng,N.,Chernoguz,O.,Hart,O.,Salpekar,
O.,Kalinli,O.,Kent,P.,Parekh,P.,Saab,P.,Balaji,P., Jain,N.,Shetty,M.,Zhang,T.,Han,K.,Sen,K.,andStoica,
Rittner,P.,Bontrager,P.,Roux,P.,Dollar,P.,Zvyagina, I.R2e:Turninganygithubrepositoryintoaprogramming
P., Ratanchandani, P., Yuvraj, P., Liang, Q., Alao, R., agentenvironment. InICML2024.
Rodriguez,R.,Ayub,R.,Murthy,R.,Nayani,R.,Mitra,
R.,Li,R.,Hogan,R.,Battey,R.,Wang,R.,Maheswari, Jain, N., Han, K., Gu, A., Li, W.-D., Yan, F., Zhang, T.,
R.,Howes,R.,Rinott,R.,Bondu,S.J.,Datta,S.,Chugh, Wang, S., Solar-Lezama, A., Sen, K., and Stoica, I.
S., Hunt, S., Dhillon, S., Sidorov, S., Pan, S., Verma, Livecodebench: Holistic and contamination free eval-
S.,Yamamoto,S.,Ramaswamy,S.,Lindsay,S.,Lindsay, uation of large language models for code, 2024. URL
S.,Feng,S.,Lin,S.,Zha,S.C.,Shankar,S.,Zhang,S., https://arxiv.org/abs/2403.07974.
Zhang, S., Wang, S., Agarwal, S., Sajuyigbe, S., Chin-
Jin,Q.,Dhingra,B.,Liu,Z.,Cohen,W.,andLu,X. Pub-
tala, S., Max, S., Chen, S., Kehoe, S., Satterfield, S.,
medqa: Adatasetforbiomedicalresearchquestionan-
Govindaprasad,S.,Gupta,S.,Cho,S.,Virk,S.,Subrama-
swering. InProceedingsofthe2019ConferenceonEm-
nian,S.,Choudhury,S.,Goldman,S.,Remez,T.,Glaser,
piricalMethodsinNaturalLanguageProcessingandthe
T., Best, T., Kohler, T., Robinson, T., Li, T., Zhang, T.,
9thInternationalJointConferenceonNaturalLanguage
Matthews,T.,Chou,T.,Shaked,T.,Vontimitta,V.,Ajayi,
Processing(EMNLP-IJCNLP),pp.2567–2577,2019.
V.,Montanez,V.,Mohan,V.,Kumar,V.S.,Mangla,V.,
Ionescu, V., Poenaru, V., Mihailescu, V. T., Ivanov, V., Jon Durbin. airoboros-gpt4-1.2, 2024. URL
Li,W.,Wang,W.,Jiang,W.,Bouaziz,W.,Constable,W., https://huggingface.co/datasets/
Tang,X.,Wang,X.,Wu,X.,Wang,X.,Xia,X.,Wu,X., gbharti/finance-alpaca.
Gao,X.,Chen,Y.,Hu,Y.,Jia,Y.,Qi,Y.,Li,Y.,Zhang,
Y.,Zhang,Y.,Adi,Y.,Nam,Y.,Yu,Wang,Hao,Y.,Qian, Kiela,D.,Bartolo,M.,Nie,Y.,Kaushik,D.,Geiger,A.,Wu,
Y.,He,Y.,Rait,Z.,DeVito,Z.,Rosnbrick,Z.,Wen,Z., Z.,Vidgen,B.,Prasad,G.,Singh,A.,Ringshia,P.,Ma,
Yang,Z.,andZhao,Z. Thellama3herdofmodels,2024. Z.,Thrush,T.,Riedel,S.,Waseem,Z.,Stenetorp,P.,Jia,
URLhttps://arxiv.org/abs/2407.21783. R.,Bansal,M.,Potts,C.,andWilliams,A. Dynabench:
Rethinking benchmarking in nlp, 2021. URL https:
Dubois,Y.,Galambosi,B.,Liang,P.,andHashimoto,T.B. //arxiv.org/abs/2104.14337.
Length-controlled alpacaeval: A simple way to debias
Kornilova, A. and Eidelman, V. BillSum: A corpus
automaticevaluators,2024a. URLhttps://arxiv.
for automatic summarization of US legislation. In
org/abs/2404.04475.
Wang, L., Cheung, J. C. K., Carenini, G., and Liu, F.
(eds.), Proceedingsofthe2ndWorkshoponNewFron-
Dubois, Y., Li, X., Taori, R., Zhang, T., Gulrajani, I., Ba,
tiersinSummarization,pp.48–56,HongKong,China,
J., Guestrin, C., Liang, P., and Hashimoto, T. B. Al-
November 2019. Association for Computational Lin-
pacafarm: A simulation framework for methods that
guistics. doi: 10.18653/v1/D19-5406. URL https:
learn from human feedback, 2024b. URL https:
//aclanthology.org/D19-5406.
//arxiv.org/abs/2305.14387.
Li,J.,Bhambhoria,R.,andZhu,X.Parameter-efficientlegal
GaurangBharti. finance-alpaca(revision51d16b6),2024. domainadaptation. InProceedingsoftheNaturalLegal
URL https://huggingface.co/datasets/ LanguageProcessingWorkshop2022,pp.119–129,Abu
gbharti/finance-alpaca. Dhabi,UnitedArabEmirates(Hybrid),December2022.
10ConstructingDomain-SpecificEvaluationSetsforLLM-as-a-judge
AssociationforComputationalLinguistics.URLhttps: OpenAI.Gpt-4omini:Advancingcost-efficientintelligence,
//aclanthology.org/2022.nllp-1.10. 2024a. URL https://openai.com/index/
gpt-4o-mini-advancing-cost-efficient-intelligence/.
Li,T.,Chiang,W.-L.,Frick,E.,Dunlap,L.,Wu,T.,Zhu,B.,
Accessed: 2024-08-14.
Gonzalez,J.E.,andStoica,I. Fromcrowdsourceddatato
high-qualitybenchmarks: Arena-hardandbenchbuilder OpenAI. New embedding models and api updates,
pipeline,2024a. 2024b. URL https://openai.com/index/
new-embedding-models-and-api-updates/.
Li, X., Zhang, T., Dubois, Y., Taori, R., Gulrajani, I.,
Accessed: 2024-08-14.
Guestrin,C.,Liang,P.,andHashimoto,T.B. Alpacae-
val: An automatic evaluator of instruction-following OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L.,
models. https://github.com/tatsu-lab/ Akkaya,I.,Aleman,F.L.,Almeida,D.,Altenschmidt,J.,
alpaca_eval,2023. Altman,S.,Anadkat,S.,Avila,R.,Babuschkin,I.,Bal-
aji,S.,Balcom,V.,Baltescu,P.,Bao,H.,Bavarian,M.,
Li, X. L., Liu, E. Z., Liang, P., and Hashimoto, T. Auto-
Belgum,J.,Bello,I.,Berdine,J.,Bernadett-Shapiro,G.,
bencher: Creatingsalient,novel,difficultdatasetsforlan-
Berner,C.,Bogdonoff,L.,Boiko,O.,Boyd,M.,Brakman,
guagemodels,2024b. URLhttps://arxiv.org/
A.-L.,Brockman,G.,Brooks,T.,Brundage,M.,Button,
abs/2407.08351.
K.,Cai,T.,Campbell,R.,Cann,A.,Carey,B.,Carlson,
Liang,P.,Bommasani,R.,Lee,T.,Tsipras,D.,Soylu,D., C., Carmichael, R., Chan, B., Chang, C., Chantzis, F.,
Yasunaga,M.,Zhang,Y.,Narayanan,D.,Wu,Y.,Kumar, Chen,D.,Chen,S.,Chen,R.,Chen,J.,Chen,M.,Chess,
A.,Newman,B.,Yuan,B.,Yan,B.,Zhang,C.,Cosgrove, B.,Cho,C.,Chu,C.,Chung,H.W.,Cummings,D.,Cur-
C.,Manning,C.D.,Re´,C.,Acosta-Navas,D.,Hudson, rier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N.,
D.A.,Zelikman,E.,Durmus,E.,Ladhak,F.,Rong,F., Deville,D.,Dhar,A.,Dohan,D.,Dowling,S.,Dunning,
Ren,H.,Yao,H.,Wang,J.,Santhanam,K.,Orr,L.,Zheng, S.,Ecoffet,A.,Eleti,A.,Eloundou,T.,Farhi,D.,Fedus,
L., Yuksekgonul, M., Suzgun, M., Kim, N., Guha, N., L.,Felix,N.,Fishman,S.P.,Forte,J.,Fulford,I.,Gao,L.,
Chatterji,N.,Khattab,O.,Henderson,P.,Huang,Q.,Chi, Georges,E.,Gibson,C.,Goel,V.,Gogineni,T.,Goh,G.,
R., Xie, S. M., Santurkar, S., Ganguli, S., Hashimoto, Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S.,
T.,Icard,T.,Zhang,T.,Chaudhary,V.,Wang,W.,Li,X., Greene,R.,Gross,J.,Gu,S.S.,Guo,Y.,Hallacy,C.,Han,
Mai,Y.,Zhang,Y.,andKoreeda,Y.Holisticevaluationof J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse,
languagemodels,2023. URLhttps://arxiv.org/ C.,Hickey,A.,Hickey,W.,Hoeschele,P.,Houghton,B.,
abs/2211.09110. Hsu,K.,Hu,S.,Hu,X.,Huizinga,J.,Jain,S.,Jain,S.,
Jang,J.,Jiang,A.,Jiang,R.,Jin,H.,Jin,D.,Jomoto,S.,
Lin,S.,Hilton,J.,andEvans,O.Truthfulqa:Measuringhow
Jonn,B.,Jun,H.,Kaftan,T.,ŁukaszKaiser,Kamali,A.,
modelsmimichumanfalsehoods,2022. URLhttps:
Kanitscheider,I.,Keskar,N.S.,Khan,T.,Kilpatrick,L.,
//arxiv.org/abs/2109.07958.
Kim,J.W.,Kim,C.,Kim,Y.,Kirchner,J.H.,Kiros,J.,
Marchisio, K., Ko, W.-Y., Be´rard, A., Dehaze, T., and Knight,M.,Kokotajlo,D.,ŁukaszKondraciuk,Kondrich,
Ruder, S. Understandingandmitigatinglanguagecon- A.,Konstantinidis,A.,Kosic,K.,Krueger,G.,Kuo,V.,
fusion in llms, 2024. URL https://arxiv.org/ Lampe,M.,Lan,I.,Lee,T.,Leike,J.,Leung,J.,Levy,D.,
abs/2406.20052. Li,C.M.,Lim,R.,Lin,M.,Lin,S.,Litwin,M.,Lopez,T.,
Lowe,R.,Lue,P.,Makanju,A.,Malfacini,K.,Manning,
Mucherino, A., Papajorgji, P. J., and Pardalos, P. M. k- S., Markov, T., Markovski, Y., Martin, B., Mayer, K.,
NearestNeighborClassification,pp.83–106. Springer Mayne,A.,McGrew,B.,McKinney,S.M.,McLeavey,C.,
NewYork,NewYork,NY,2009.ISBN978-0-387-88615- McMillan,P.,McNeil,J.,Medina,D.,Mehta,A.,Menick,
2. doi:10.1007/978-0-387-88615-2 4. URLhttps:// J.,Metz,L.,Mishchenko,A.,Mishkin,P.,Monaco,V.,
doi.org/10.1007/978-0-387-88615-2_4. Morikawa,E.,Mossing,D.,Mu,T.,Murati,M.,Murk,O.,
Me´ly,D.,Nair,A.,Nakano,R.,Nayak,R.,Neelakantan,
Muennighoff, N., Tazi, N., Magne, L., and Reimers, N.
A.,Ngo,R.,Noh,H.,Ouyang,L.,O’Keefe,C.,Pachocki,
Mteb: Massive text embedding benchmark. arXiv
J.,Paino,A.,Palermo,J.,Pantuliano,A.,Parascandolo,
preprint arXiv:2210.07316, 2022. doi: 10.48550/
G.,Parish,J.,Parparita,E.,Passos,A.,Pavlov,M.,Peng,
ARXIV.2210.07316. URL https://arxiv.org/
A.,Perelman,A.,deAvilaBelbutePeres,F.,Petrov,M.,
abs/2210.07316.
deOliveiraPinto,H.P.,Michael,Pokorny,Pokrass,M.,
Narayan,S.,Cohen,S.B.,andLapata,M. Don’tgiveme Pong,V.H.,Powell,T.,Power,A.,Power,B.,Proehl,E.,
thedetails,justthesummary! topic-awareconvolutional Puri,R.,Radford,A.,Rae,J.,Ramesh,A.,Raymond,C.,
neuralnetworksforextremesummarization,2018. URL Real,F.,Rimbach,K.,Ross,C.,Rotsted,B.,Roussez,H.,
https://arxiv.org/abs/1808.08745. Ryder,N.,Saltarelli,M.,Sanders,T.,Santurkar,S.,Sastry,
11ConstructingDomain-SpecificEvaluationSetsforLLM-as-a-judge
G.,Schmidt,H.,Schnurr,D.,Schulman,J.,Selsam,D., Team,G. Gemma. 2024. doi: 10.34740/KAGGLE/M/3301.
Sheppard,K.,Sherbakov,T.,Shieh,J.,Shoker,S.,Shyam, URLhttps://www.kaggle.com/m/3301.
P.,Sidor,S.,Sigler,E.,Simens,M.,Sitkin,J.,Slama,K.,
Thakur, N., Reimers, N., Ru¨ckle´, A., Srivastava, A., and
Sohl,I.,Sokolowsky,B.,Song,Y.,Staudacher,N.,Such,
Gurevych, I. BEIR: A heterogeneous benchmark for
F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N.,
zero-shotevaluationofinformationretrievalmodels. In
Thompson,M.B.,Tillet,P.,Tootoonchian,A.,Tseng,E.,
Thirty-fifthConferenceonNeuralInformationProcess-
Tuggle,P.,Turley,N.,Tworek,J.,Uribe,J.F.C.,Vallone,
ingSystemsDatasetsandBenchmarksTrack(Round2),
A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang,
2021. URLhttps://openreview.net/forum?
J.J.,Wang,A.,Wang,B.,Ward,J.,Wei,J.,Weinmann,
id=wCu6T5xFjeJ.
C.,Welihinda,A.,Welinder,P.,Weng,J.,Weng,L.,Wi-
ethoff, M., Willner, D., Winter, C., Wolrich, S., Wong,
Verga,P.,Hofstatter,S.,Althammer,S.,Su,Y.,Piktus,A.,
H.,Workman,L.,Wu,S.,Wu,J.,Wu,M.,Xiao,K.,Xu,
Arkhangorodsky, A., Xu, M., White, N., and Lewis, P.
T.,Yoo,S.,Yu,K.,Yuan,Q.,Zaremba,W.,Zellers,R.,
Replacingjudgeswithjuries: Evaluatingllmgenerations
Zhang,C.,Zhang,M.,Zhao,S.,Zheng,T.,Zhuang,J.,
with a panel of diverse models, 2024. URL https:
Zhuk, W., and Zoph, B. Gpt-4 technical report, 2024.
//arxiv.org/abs/2404.18796.
URLhttps://arxiv.org/abs/2303.08774.
Wang, L., Yang, N., Huang, X., Yang, L., Majumder, R.,
Parsons, V. Stratified Sampling. 02 2017. ISBN
andWei,F. Improvingtextembeddingswithlargelan-
9781118445112. doi: 10.1002/9781118445112. guage models, 2024. URL https://arxiv.org/
stat05999.pub2. abs/2401.00368.
Rajani, N., Tunstall, L., Beeching, E., Lambert, White,C.,Dooley,S.,Roberts,M.,Pal,A.,Feuer,B.,Jain,
N., Rush, A. M., and Wolf, T. No robots. S., Shwartz-Ziv, R., Jain, N., Saifullah, K., Naidu, S.,
https://huggingface.co/datasets/
Hegde,C.,LeCun,Y.,Goldstein,T.,Neiswanger,W.,and
HuggingFaceH4/no_robots,2023.
Goldblum,M. Livebench: Achallenging,contamination-
free llm benchmark, 2024. URL https://arxiv.
Sainz,O.,Campos,J.,Garc´ıa-Ferrero,I.,Etxaniz,J.,deLa-
org/abs/2406.19314.
calle, O.L., andAgirre, E. NLPevaluationintrouble:
On the need to measure LLM data contamination for Yang, S., Chiang, W.-L., Zheng, L., Gonzalez, J. E., and
eachbenchmark. InBouamor,H.,Pino,J.,andBali,K. Stoica,I. Rethinkingbenchmarkandcontaminationfor
(eds.), Findings of the Association for Computational language models with rephrased samples, 2023. URL
Linguistics: EMNLP 2023, pp. 10776–10787, Singa- https://arxiv.org/abs/2311.04850.
pore, December 2023. Association for Computational
Linguistics. doi: 10.18653/v1/2023.findings-emnlp. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and
722. URL https://aclanthology.org/2023. Choi, Y. Hellaswag: Can a machine really finish your
findings-emnlp.722. sentence?,2019. URLhttps://arxiv.org/abs/
1905.07830.
Settles,B. Activelearningliteraturesurvey. 072010.
Zheng,L.,Chiang,W.-L.,Sheng,Y.,Zhuang,S.,Wu,Z.,
Shi, F., Suzgun, M., Freitag, M., Wang, X., Srivats, S., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang,
Vosoughi, S., Chung, H. W., Tay, Y., Ruder, S., Zhou, H.,Gonzalez,J.E.,andStoica,I. Judgingllm-as-a-judge
D., Das, D., and Wei, J. Language models are multi- withmt-benchandchatbotarena,2023a.
lingualchain-of-thoughtreasoners,2022. URLhttps:
//arxiv.org/abs/2210.03057. Zheng,L.,Chiang,W.-L.,Sheng,Y.,Zhuang,S.,Wu,Z.,
Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E. P., Zhang,
Singh, S., Vargus, F., Dsouza, D., Karlsson, B. F., Ma- H., Gonzalez, J. E., and Stoica, I. Judging llm-as-a-
hendiran, A., Ko, W.-Y., Shandilya, H., Patel, J., Mat- judge with mt-bench and chatbot arena, 2023b. URL
aciunas, D., OMahony, L., Zhang, M., Hettiarachchi, https://arxiv.org/abs/2306.05685.
R.,Wilson,J.,Machado,M.,Moura,L.S.,Krzemin´ski,
D., Fadaei, H., Ergu¨n, I., Okoh, I., Alaagib, A., Mu-
dannayake, O., Alyafeai, Z., Chien, V. M., Ruder, S.,
Guthikonda,S.,Alghamdi,E.A.,Gehrmann,S.,Muen-
nighoff,N.,Bartolo,M.,Kreutzer,J.,U¨stu¨n,A.,Fadaee,
M., and Hooker, S. Aya dataset: An open-access col-
lectionformultilingualinstructiontuning, 2024. URL
https://arxiv.org/abs/2402.06619.
12ConstructingDomain-SpecificEvaluationSetsforLLM-as-a-judge
8.Appendix
8.1.DataSources
Datasets
LMSysChatbotArena(Chiangetal.,2024)
PubMedQA(Jinetal.,2019)
MathQA(Aminietal.,2019)
NoRobots(Rajanietal.,2023)
Aya(Singhetal.,2024)
Legalreddit(Lietal.,2022)
LegalSumm. BillSum(Kornilova&Eidelman,2019)
Airoboros-gpt4(JonDurbin,2024)
FinanceAdvisor(GaurangBharti,2024)
FinanceBierQA(Thakuretal.,2021)
MMLU(Hendrycksetal.,2021a)
TruthfulQA(Linetal.,2022)
GSM8K(Cobbeetal.,2021)
Table4: DatasetSourcesusedasinputtothedatapipelineinFigure5.
Table4includesvariousdatasetsacrossmultipledomainssuchasmedical,legal,financial,andmultilingualcategories.
Thesesourceswereselectedtoensureawiderangeofcoverage,contributingtothediversityoftheevaluationset. The
datasetslistedherewerecrucialforconstructingthedomain-specificevaluationsets,allowingforthethoroughtestingof
modelsacrossdifferentcontextsandlanguages.
8.2.JudgeTemplate
BelowisourjudgetemplatethatweusedforourLLM-as-a-judgeevaluation:
PleaseactasanimpartialjudgeandevaluatethequalityoftheresponsesprovidedbytwoAIassistantstotheuserquestion
displayedbelow. Youshouldchoosetheassistantthatfollowstheuser’sinstructionsandanswerstheuser’squestionbetter,
aswellasansweringinthedesiredlanguageoftheuser. Yourevaluationshouldconsiderfactorssuchasthehelpfulness,
relevance,accuracy,depth,creativity,andlevelofdetailoftheirresponses. Beginyourevaluationbycomparingthetwo
responsesandprovideashortexplanation. Avoidanypositionbiasesandensurethattheorderinwhichtheresponseswere
presenteddoesnotinfluenceyourdecision. Donotallowthelengthoftheresponsestoinfluenceyourevaluation. Donot
favorcertainnamesoftheassistants. Beasobjectiveaspossible. Yourevaluationshouldonlyfocusonthecorrectnessof
theresponse. Afterprovidingyourexplanation,outputyourfinalverdictbystrictlyfollowingthisformat: [[A]]ifassistant
Aisbetter,[[B]]ifassistantBisbetter,and[[C]]foratie.
8.3.EvaluationTool
Withthenotionofself-definedcategoriesandusingtheLLM-as-a-judgeframework,wecreateanevaluationtoolwhich
loadsaninternalleaderboardfromacsvfileandbreaksdownthewinrateintoseveralcategoriestheuserdefined. TheUI
showstheleaderboardinadataframeandshowsthewinratesinsetofbarplotsacrossdifferentcategories. Ascreenshotof
thetoolcanbeseeninFigure8.
There is also a feature which enables the user to view completions on the evaluation from both the model the user is
interestedin,thereferencemodel,andthejudgemodeltoexamineitsreasoning. Thistoolenablestheusertoexamine
wherethemodeltheyaredevelopingisperformingbetterthanothercompetitorsandareaswhereimprovementisrequired.
13ConstructingDomain-SpecificEvaluationSetsforLLM-as-a-judge
Figure8: Ascreenshotofourevaluationtool.
14