Shapley Marginal Surplus for Strong Models
Daniel de Marchi Michael Kosorok Scott de Marchi
University of North Carolina University of North Carolina Duke University
Abstract trained model using the Shapley values of the covari-
ates with respect to each individual prediction (Lund-
berg et al., 2020; Lundberg & Lee, 2017). Researchers
Shapley values have seen widespread use in
can gain a precise understanding of how each variable
machine learning as a way to explain model
influences the predictions of their model. This is es-
predictions and estimate the importance of
peciallycriticalfordeployedsystemsthatwillinteract
covariates. Accurately explaining models is
with sensitive data, or cases where models are not the
criticalinreal-worldmodelstobothaidinde-
primary decision makers and their outputs need to be
cision making and to infer the properties of
explainedtotheprimarydecisionmaker. Forinstance,
the true data-generating process (DGP). In
if the model is predicting the success or failure of a
thispaper,wedemonstratethatwhilemodel-
drugtreatmentforaspecificpatient,aclinicianmight
based Shapley values might be accurate ex-
want to review the model’s decision and make sure
plainersofmodelpredictions, machinelearn-
that the factors leading to the model’s output aligns
ingmodelsthemselvesareoftenpoorexplain-
with their medical knowledge. However, there are sig-
ers of the DGP even if the model is highly
nificant limitations to variable importance algorithms
accurate. Particularly in the presence of in-
basedonasinglesupervisedlearningmodel. Theseal-
terrelated or noisy variables, the output of a
gorithmsassumethateitherthismodelisareasonable
highly predictive model may fail to account
characterization of the DGP, or that the DGP is not
for these relationships. This implies expla-
important in the current application (Kumar et al.,
nations of a trained model’s behavior may
2020).
fail to provide meaningful insight into the
DGP.Inthispaperweintroduceanovelvari- Theexamplefrequentlyusedintheliteraturetojustify
ableimportancealgorithm,ShapleyMarginal focusing on model-based explanations is a bank with
Surplus for Strong Models, that samples the ahighlyaccurateloangrantingmodel. Thebankmay
space of possible models to come up with want explanations of this model so that they can rec-
aninferentialmeasureoffeatureimportance. ommend strategies to their customers to be approved
We compare this method to other popular for a loan. However, the actual validity of the model
feature importance methods, both Shapley- isn’t relevant to the problem. If the true variable re-
based and non-Shapley based, and demon- sponsibleforacustomer’suntrustworthinessforaloan
strate significant outperformance in inferen- is their income, but the bank’s model denied them
tial capabilities relative to other methods. based on their credit score, this is not an issue to the
bank since their primary goal is model accuracy, not
inference. The two variables are likely tightly corre-
1 Problem Introduction
lated and the model is highly accurate, so whether or
not the credit score is actually part of the DGP isn’t
With the rise in popularity of difficult-to-interpret
an issue to the bank (Chen et al., 2020).
”black box” models, Shapley values have become a
common tool in the growing field of explainable AI In many cases however, researchers need models that
(Burkart & Huber, 2021). Originally developed for are accurate with respect to the true DGP. If a re-
trees, and later generalized to any model, these al- searcher is testing a model for racial bias, it is critical
gorithms provide explanations for the outputs of a that the model is an accurate representation. If other
variables (such as where a person lives, their medical
history, etc.) are associated with an individual’s race,
themodelmaynotuseraceasafeatureeventhoughif
itispartofthetrueDGP,andracialbiasexistsinthe
4202
guA
61
]LM.tats[
1v54880.8042:viXraShapley Marginal Surplus for Strong Models
data. A feature importance tool that focuses on a sin- esis tests about the parameters of the realized model
gle model may give the illusion of no racial bias even f. Coefficients that are not statistically significantly
though the underlying data is actually highly racially different from zero are assumed to be not present in
biased (Chen et al., 2020; Kumar et al., 2020). Some the true functional form F.
feature importance algorithms attempt to solve this
These hypothesis tests are conducted under the as-
problem by fitting multiple models to consider a va-
sumption that the true function F(X) may only use
riety of different possible explanations, depending on
a subset of the features. Out of our total sub-
the class of models chosen.
set of features X = [X ...X ], statistical hypothesis
1 p
The Model Class Reliance (MCR) algorithm ap- tests attempt to identify some subset T = [t ...t ] ⊆
1 k
proaches this by conducting an analysis on the [1,2,...p],X = [X ...X ] ⊆ [X ...X ] such that the
T t1 tk 1 p
”Rashomon set” of relatively strong models. It fits a elementsinX aretheelementsinthetruefunctional
T
large number of these high-quality models, then aver- form of F(X). This gives us the following property:
agesthepermutationimportanceofeachfeaturetothe
Rashomon set of strong models. While this method is
F(X )=F(X) ∀ x ∈ X
not directly Shapley based, it works off similar princi- T
ples,buthasadditionalusefulpropertiesduetoassess-
Note that T is the smallest such set that the above
ingfeatureimportanceacrossabroadclassofpossible
holds. In fact, for any set of covariates W such that
models (Fisher et al., 2019).
F(X )=F(X)∀x∈X, we know that T ⊆W.
W
Leave-One-Covariate-Out (LOCO) uses the loss of a
Definition Feature Importance: A feature impor-
full model relative to submodels that have been refit
tance is an allocation of weights w = [w ...w ], where
1 p
without each covariate to evaluate the marginal im-
theweightindicatesthechangeineitheraqualitymet-
portance of each covariate (Lei et al., 2018). Typi-
ric or the model’s predictions when the corresponding
cally, this requires generating a number of train/test
feature X is removed relative to the original predic-
i
setsfromanoriginaldataset,andassessingtheLOCO
tions. The sum is sometimes normalized such that
importance across these different replicates. (cid:80)p
w =1. Some possible examples of quality func-
i=1 i
Whiletheseapproachesprovidebetterexplanationsof tions are mean squared error (MSE) for regression
the true DGP than single-model approaches, signifi- or the area under the receiver operating characteris-
cant weaknesses exist. These include failing to con- tic (ROC) curve for a binary classification problem
sider the full class of models, failing to correctly an- (Covert et al., 2021).
alyze the full class of models, and failing to properly
Feature importance can be analyzed locally on a per-
makeuseofShapleyvalues. Tothisend, weintroduce
prediction level for each x ∈ X or globally across the
ShapleyMarginalSurplusforStrongModels(SMSSM)
entire dataset X (Lundberg & Lee, 2017). Addition-
to generate explanations that respect the true under-
ally, we can distinguish between selective feature im-
lying DGP.
portance and attributive feature importance.
Definition Attributive and Selective Importance: we
2 Theoretical Preliminaries
define a feature importance function to be selective if
the weight vector w can be used to identify the sub-
Consider a supervised model f ∈ F for some class
set of true covariates X by analyzing the weights
of possible models F, where we are training on data T
where w > 0, or by conducting hypothesis tests on
X and target variable y, such that our predicted i
the realized distribution of the components of sam-
yˆ = f(X),X = [X ...X ]. Denote any individual ob-
1 p pledweightvectorsw1...wk forsomesamplingmethod.
servation of X to be x∈X.
This makes a selective feature importance function
Definition Data-Generating Process (DGP): The more similar to a statistical hypothesis test.
data-generating process of a dataset is the true func-
The alternative is attributive feature importance. At-
tional relationship between y and X, and can be ex-
tributive feature importance is concerned with ex-
pressed as y ∼F(X)+ϵ(X), where ϵ(X) is some irre-
plainingthepropertiesofthemodelf,andnotthetrue
ducibleerrordistributionthatmayormaynotdepend
data-generating process F(X), so a positive weight
on the input data. In general, outside of simulated
only implies that the feature is useful to that particu-
data this true relationship is unknown.
lar model. The assumption for an attributive variable
Classical statistical inference is concerned with identi- importance analysis is that either f(X) ∼ F(X) to
fying this true relationship: a common approach is to a sufficient degree or that we are not concerned with
assume that F is a linear function of the features, ϵ the true relationship and are only concerned with the
does not depend on X, and to then conduct hypoth- trained model.Daniel de Marchi, Michael Kosorok, Scott de Marchi
We are making this distinction so we can later dis- replaced using an imputation model such that
cusswhetheramethodisspecifictoaparticularmodel X ∼ g (X × s) for some arbitrary imputation
i i
or if it is an inferential technique about the underly- model g .
i
ing mechanism. We also note that selective explana-
• Refit-basedreplacement: thisrefitsthemodelen-
tionmethodscanstillvaryintheexplanationreturned
tirelytonotuseanycolumnsX suchthats =0,
andmaydependonthefunctionalclasschosen: forin- i i
andanalyzesperformancerelativetotheoriginal.
stance,theexplanationofarandomforestandagradi-
ent boosted ensemble might be dramatically different.
Thefirstthreemethodscanbeunderstoodasfunction-
However, both explanations will still only be based on
based replacements, where each value in the column
covariates in the true-data generating process.
X is replaced by a value h (x,s) for some function of
i i
the non-removed covariates. These methods constrain
3 Subset Functions
that ∃X s.t. h (X,s)̸=X , because otherwise no val-
i i i
ues have been ”removed” in any meaningful sense.
To analyze how a model performs when a particular
Allfouroftheseapproachescansatisfythedefinitions
feature is removed, we need to create a way to gener-
ofasubsetextension,andhavevaryingbenefits,draw-
ate functions that will behave similarly to the trained
backs, and computational requirements. Now that we
modelf,butonlyoperateonanarbitrarysubsetofthe
have defined some of the terminology for how to sim-
featuresusedbyf. Thisisknownasasubsetfunction
ulate the removal of features from a model, we turn
(Covert et al., 2021).
to a brief discussion of game theory, and how game-
Consider a set P(p) of every possible binary vector of theoreticconceptsareusedtocalculatefeatureimpor-
length p. Because X is p dimensional, any subset of tance weights from the changes in model performance
the covariates in X can be expressed as X × s for when features are removed using a subset extension
some s∈P(p), where X =[X 1...X p] and s is a vector method (Covert et al., 2021).
of zeros and ones.
Definition Subset Function: A subset function is a 4 Game Theory and Shapley Values
functionthatmodelsg(X,s)∼y, suchthatcovariates
excluded by the indicator vector s are not present in Definition Cooperative Game: a cooperative game
the functional form of g. That is, we have g(x,s) = is mapping between a set of actors a ∈ A, where
g(x′,s)foralltriplets(x,x′,s)suchthatx×s=x′×s. A = [1...n], and a set function ν, where ν(a) is the
reward an individual set of actors would receive from
Definition Subset Extension: a subset extension of
playing the game in question. Often, the reward is
f is a subset function f such that f(x) = f (x,s)
E E a step function, where a certain quota needs to be
wheneversisavectorofonlyones,i.e. inthepresence
met for a group of actors to receive a positive re-
of all features. Therefore the model f and f satisfy:
E ward. For this section, assume that any natural in-
teger 1 ≤ i ≤ n is an actor i ∈ A, and an arbitrary
f(x)=f (x,[1,1,....1]) ∀ x ∈ X group of actors [a ,a ,...a ] are the members of our
E 1 2 j
sets a∈A (Branzei et al., 2008).
There are many approaches to creating subset func-
ν(∅) is typically 0 for any cooperative game, and the
tions, which we will not cover exhaustively. The four
nonzero winnings are typically normalized such that
broad groups of methods are as follows:
the maximum equals 1. If we wanted to divide these
winnings between players based on their pivotality to
• Constant value replacement: for any s equal to
i receivingrewards,wewouldturntotheShapleyvalue.
0, all values in the corresponding column X are
i TheShapleyvalueisawaytoallocatethesurplus(re-
replaced with a constant value c, typically either
ward) between the team such that each actor is com-
the expected value or zero.
pensated for their pivotality in the game. It is calcu-
• Distribution-based replacement: for any s equal lated from the set of possible marginal utilities.
i
to 0, all values in the corresponding column X i Definition Marginal Utility: in a cooperative game,
are replaced using a probability distribution p i, the marginal utility of player i to a set a ∈ A, where
such that X i ∼ p i(X ×s). Typically, this is the i ∈ a is ν(a)−ν(a/{i}), i.e. the reward of the set a
conditionaldistributionofX i onthenon-removed with player i minus the reward without player i.
values of X.
Definition Shapley Value: Label the payouts to each
• Model-based replacement: for any s equal to 0, player for this coalition ϕ ...ϕ , where each value is
i 1 j
all values in the corresponding column X are calculated as follows:
iShapley Marginal Surplus for Strong Models
Eachindividualobservationinadatasetcanbetreated
asacooperativegame,wherethelossbetweenthepre-
(cid:88) |a|!(n−|a|−1)!
ϕ = (ν(a∪{i})−ν(a)) diction and the ground truth is the reward function.
i n!
The need for the Shapley properties is now clear: the
a⊆A/{i}]
overall dataset’s Shapley value is the sum of the in-
dividual predictions, features that never change loss
Thiscanbeinterpretedasthemarginalcontributionof
are given a zero value, identical players are handled
playeritoallpossiblesubsetsofAthatdonotalready
correctly, and the Shapley values sum to the model’s
includeplayeri,normalizedbytheplayercount. These
improvement in the loss function.
ϕ valuesaretheShapleyvaluesforeachplayer. Shap-
j
leyvalueshaveseveralusefulpropertiesthatareappli- Shapley values are highly conditional upon the choice
cabletovariableimportance(Shapley,1953;Lundberg of value function ν. This choice of value function
et al., 2020). is critical to the analysis. For instance, it has been
pointed out that the Shapley value function in SHAP
• Efficiency: thesumofShapleyvaluesofindividual can attribute positive value to covariates X i not used
in the model due to the way it has been defined,
playersadduptotherewardofthegrandcoalition
whereas alternative choices of value function assign
minus the reward of the empty coalition.
Shapley values of zero to covariates not used in the
model (Chen et al., 2020).
n
(cid:88)
ϕ i =ν([1...n])−ν(∅) Subset extensions and Shapley values form the basis
i=1 of variable importance analysis. Most variable impor-
tancemethodsbeginbydefiningsomemethodforcal-
• Symmetry: two players with identical marginal
culating subset extensions f , and evaluate the effect
contributions to all coalitions that do not include E
ofeachfeature’sremovalbycalculatingShapleyvalues
either of them have the same Shapley value.
ofthechangeintheperformanceoff underdifferent
E
feature subsets s ∈ P(p). The choice of how to gener-
ate the subset extension is also very important since
If ∀a:i,j ∈/ a, ν(a∪{i})=ν(a∪{j}) →ϕ i =ϕ j there are many possible choices. Input-based meth-
ods that modify the data matrix X have been known
• Monotonicity: if one player has greater contri- to generate impossible points that are not possible or
bution than another player to all coalitions that vanishinglyunlikelyunderthedata-generatingprocess
contain neither of them, then that player has the y ∼F(X)+ϵ(X),whichconfusestheanalysisbyeval-
greater Shapley value. uating the model’s performance on these impossible
observations. Distribution and model-based methods
need to be properly calibrated such that the distribu-
ν(a∪{i})≥ν(a∪{j}) ∀ a→ϕ ≥ϕ
i j tion or model being used is accurate. And refit-based
methodstendtobeextremelycomputationallyexpen-
• Null Player: a player with zero marginal contri-
sive (Covert et al., 2021).
bution for all coalitions has a Shapley value of
zero.
5 Selective Importance for Universal
Approximators
ν(a∪{i})=ν(a) ∀ a→ϕ =0
i
• Additivity: a player’s Shapley value for two inde- Definition Universal Approximator: a universal ap-
pendentgamesG ,G isthesumoftheirShapley proximator is a class of functions F such that for any
1 2
value for each individual game. compact data X and any arbitrary function g(X),
there exists some f ∈ F such that sup ||f(x) −
x∈X
g(x)||<ϵ for any arbitrarily small ϵ.
ϕ =ϕ +ϕ
i∈G1,G2 i∈G1 i∈G2
If the model class F is a universal approximator, and
wehaveadata-generatingprocessy ∼F(X)+ϵ(x), it
The Shapley value is the only formulation that satis-
is always possible for some f ∈ F to achieve the min-
fies all these properties (Shapley, 1953). Shapley val-
imum possible loss such that L(f(X),F(X)) = L =
uescanaccountforafeature’scontributioneveninthe 0
E(L(ϵ(X))).
face of complex interrelationships like multicollinear-
ity, feature redundancy, mediating behavior, and syn- Theorem If a feature importance method produces
ergistic behavior (Covert et al., 2021). a vector of weights w that is selective with respectDaniel de Marchi, Michael Kosorok, Scott de Marchi
to the true data-generating process with respect to a functionalformoff mustuseallthecovariatesinX ,
T
class of functions F that is a universal approximator, but may also use some unnecessary covariates not in
the weights will be selective even if the model class is X . Thentheincreasedlossassociatedwithreplacing
T
misspecified with respect to the true functional form. any given X with h (X,s) in our subset extension
i i
f (X) is proportional to L(f(X),y) − L(f(X|X =
For instance, if F is a class of tree models, which are E i
h (X,s)),y),assumingthatX ispartofthefunctional
universal approximators, but the true data-generating i i
form of f.
process can be expressed as a polynomial, any selec-
tive variable importance function on the class F will We can prove that this quantity will be positive for
still identify the true set of covariates X such that all X used in the functional form of f under light as-
T i
F(X )=F(X)∀X. sumptions. Assume that no column in X is a perfect
T
function of the other columns. This then implies that
Wecanprovethisbycontradiction. Assumethatthere
we cannot have h (X,s) = X when s = 0, since no
is some w ∈ w such that w > 0, yet X ∈/ X . This i i i
i i i T method (constant value, function based replacement,
would then imply that L(f(X),y) < L(f(X/X ),y),
i or distribution based replacement) can perfectly im-
assuming that f is an optimal model that achieves
putethevaluesofX . WealsoknowL(f(X),y)=L ,
the minimal possible loss. However, since f ∈ F is i 0
and by definition f(X) ̸= f (X,s) = f(X|X =
a model in a class of universal approximators, this E i
h (X,s))). Therefore, predictions on the replaced val-
cannot be true because then that would imply that i
ues of X cannot give the optimal loss L . Therefore,
sup ||f(x)−g(x)|| > ϵ for some cutoff ϵ value, in- i 0
x∈X we will always see increased loss for any X used in
creasing the loss. i
the functional form of f when using feature impor-
tance methods that adjust the inputs to a model f,
6 Refit-Based Importance for
but do not refit the model entirely.
Selective Feature Importance
If any X is not part of the true covariate set X , but
i T
ispartofthemodelf,thenwehaveintroducedapos-
Due to the aforementioned computational burden,
itive increase in the associated loss between f (X,s)
most variable importance methods have focused on E
and y, without any assumptions as to the functional
function-based replacements, rather than repeatedly
form of h . Therefore, unless f happens to only use
refitting the entire model f. Refit-based importance i
covariates in the true feature set X , we will see a de-
methods tend to be especially costly when f is a com- T
crease in model performance associated with the im-
plex model.
portance metric and therefore positive importance to
However, we contend that a full refit is the only ap- the associated feature. Since these sorts of methods
proachthatisguaranteedtoleadtoaselectivefeature arenotintendedtobeinferentialaboutthetrueDGP,
importance function. To see why, assume we have a this is intended behavior, but it makes them less use-
subset extension function f such that when s = 0, ful as inferential tools. Therefore, this class of feature
E i
the values of X are replaced or imputed in some way importance methods cannot be guaranteed to give a
i
andpassedbackthroughthetrainedmodelf,whether selective feature importance method.
by replacing values in X with a constant value, dis-
i On the other hand, if f (X,s) is the function f refit
tribution, or model output. We further know that by E
without covariate X , then it is possible for f(X) =
construction, ∃X s.t. h (X,s)̸=X for any given f . i
i i i E f (X,s) if the only covariates removed are not in the
Infact, somefeatureimportancemethodsenforce”re- E
true set X . Because functional replacement meth-
placed” covariate columns to not be equal to the orig- T
ods require the values of X to be different from the
inal column for all covariates. Then the change in the i
true values post-removal while keeping the predic-
output between f and f can be represented as:
E tion model the same, the loss of the function post-
replacement is necessarily higher. Refit based meth-
odsdonothavethisconstraintifX isnotanelement
i
f(X)−f (X,s)=f(X)−f(X|{X :s =0→h (X,s)})
E i i i ofX ,becausetheminimallossisstillachievableeven
T
if X is excluded by definition.
i
Recall that we are concerned with the true DGP,
Note also that it is possible in an asymptotically large
F(X) ∼ y +ϵ(X). Here, ϵ(X) constrains the mini-
dataset to have X ∈ X ,X ∈/ X , even when
mal achievable loss on the given dataset. Denote this i f i T
minimal loss as E(L(F(X),y))=E(L(ϵ(X)))=L . L(f(X),y) = L 0. To see why, assume that f is a
0
gradient boosted decision tree model. Assume that
Assume that the functional form of our trained model X = [X ,X ,X ], and y = X + X + ϵ. Let
1 2 3 1 2
f(X) ̸= F(X), but for our desired loss function L, X =X +X +ϵ+γ,whereγ isansmalladdedmean-
3 1 2
L(f(X),y)=L(F(X),y)=L . This implies that the
0Shapley Marginal Surplus for Strong Models
zero noise term. This means X is a noisy collinear following hypothesis test can be performed, using any
3
combination of X ,X . one of a wide variety of nonparametric tests:
1 2
The correlation ρ(y,X ) > max(ρ(y,X ),ρ(y,X )).
3 2 1
This means that the gradient boosting algorithm will H :θ >0 versus H :θ ≤0
0 j 1 j
selectthefirstsplittobeonX ,asX maximallysep-
3 3
arates different values of y despite being a collinear
TheLOCOpaperalsosuggestsseveralalternativetest-
combination and containing no marginal utility. Since
ing methodologies, such as using the median of the
thesetoftruevariablesisX =[X ,X ],wehavethat
T 1 2 trials to estimate θ as a way to smooth out the dis-
j
X ∈ X ,X ∈/ X as desired. However, X is not
3 f 3 T 3 tribution and allow for more standard tests, such as a
needed to achieve the optimal loss. A model fit only
Wilcoxon signed-rank test (Lei et al., 2018).
to X ,X will achieve L . This implies a refit-based
1 2 0
method could be guaranteed to correctly identify that However, the LOCO methodology is missing one cru-
X is not an element of X . cial element common to most variable importance
3 T
functions. LOCOdoesnotexamineallpossiblesetsof
s∈P(p). Instead, it only examines the sets [0,1,...1],
7 Conformal Inference and LOCO
[1,0,...1], ... [1,1,...0], which only constitute p out of
the2p possiblesets. ThismeansthatLOCOisnotes-
We now consider a specific approach to variable im-
timating Shapley values of the variables with respect
portance, where we assume that the model f may be
to any possible value function ν. Instead, LOCO as-
significantly mis-specified relative to the true model
sumes that the set of p feature subsets is a sufficient
F. Since we make no assumptions as to the true func-
sample of the space of covariate subsets. Therefore,
tionalformofF,wecannotresorttothecommontools
the Shapley value properties do not apply to LOCO
used for linear models. Instead, we turn to the tools
importance values.
of conformal inference.
Specifically, LOCO values are not efficient when the
DefinitionLeave-one-covariate-out(LOCO)Variable
variables X ...X are not fully independent. In this
Importance: LOCOoptimizesafullmodelf andthen 1 p
case, for any given variable X that is correlated with
estimatespsub-modelsthatarethemodelf refitwith- i
other covariates, there is some marginal change in ν
out each individual covariate X ...X . This process
1 p attributable to X that is independent of all other
can be repeated many times on a dataset using cross i
X ,j ̸= i which can be denoted ξ and some depen-
validationorbootstrapresamplingsofX togetamore j i
dent effect that is partially attributable to a set of
consistentestimationofthemarginalutilityofeachX
i covariates including X , denoted δ . LOCO will only
(Lei et al., 2018). i i
measuretheindependentξ effectassociatedwitheach
i
LOCO is a flexible approach. As defined above it variable, without measuring utility due to the δ be-
i
makes no assumptions about the class of model F be- tween covariates. This can be a significant issue when
ing used, or the distribution of the data X. Assum- a set of variables is highly collinear, and the values of
ingthatLOCOisrunforseveralresamplediterations, δ are relatively large compared to the ξ . LOCO will
i i
conformalpredictionbandsforthechangeinthequal- always focus on each variable’s marginal contribution,
ity metric for each covariate ν(f(X)) − ν(f(X/X )) and miss the high utility of the overall group.
i
can be calculated. LOCO is also a selective variable
LOCO values also violate symmetry when the vari-
importance method, as it fully refits the function f at
ables are not independent. Consider a pair of vari-
each step rather than using a single trained f. In an
ables,X andX ,suchthatξ =ξ ,butδ >δ . These
asymptoticallylargesample,thisshouldbeguaranteed i j i j i i
variables will have identical LOCO scores, despite X
to tell whether any given X has a positive marginal i
i being more useful to smaller coalitions.
importance to the overall feature set.
WefocusontheglobalimportancemeasuresofLOCO.
8 Model Class Reliance
Assume that θ is the true value of ν(f(X)) −
j
ν(f(X/X )), and θˆ is the estimated value of θ from
j j j DefinitionModelClassReliance(MCR):MCRtrains
running k iterations of LOCO. Then an asymptotic
kmodelsf ...f andthenevaluatesthesubsetf =
confidence interval for θˆ is: 1 k MCR
j [f : L(f (X),y) ≤ L +δ] for some positive constant
i i 0
δ. MCR importance then calculates the exhaustive
θˆ −
𭟋 α/s2
j,θˆ +
𭟋 α/s2
j permutation importance within the set of δ-optimal
j (cid:112) k/2 j (cid:112) k/2 models f MCR to assess the feature importance. They
refer to the set of δ-optimal models as the Rashomon
Where s2 is the sample standard deviation. Then the set,
jDaniel de Marchi, Michael Kosorok, Scott de Marchi
MCRexploitsthevaryingfunctionalformsofthemod- of the full models, and average the marginal contribu-
els within this set of high-performing models, and as- tion of each covariate to this top b percent of models,
sesses the importance of covariates to each model by where the cutoff loss can be denoted L . This is the
b
testing the permutation importance with respect to Shapleyvaluewithrespecttothefollowingvaluefunc-
every possible permutation of the columns. The ad- tion:
vantage of this approach is that it does not rely on a
singlemodellikeSHAPdoes,butthedownsideisthat
ν(c)=L(f(c),y)I(L(f(c),y)≤L )
bynotevaluatingthevalidityofthediversefunctional b
forms, only their overall loss, MCR is guaranteed to
ThisissimilartoLOCOinapproach. However,rather
never capture the true DGP. There is only one truly
than only use a single model, we sample a space of
correct DGP, therefore any average of multiple differ-
manymodelstomorepreciselyattributemarginalsur-
ent explanations is necessarily wrong unless it could
plus to features using their Shapley values. The full
be guaranteed that the different explanations were all
algorithm can be written as follows:
noisyyetunbiasedestimatorsofthetrueDGP.Toour
knowledge, no such proof exists.
Algorithm 1 Shapley Marginal Surplus for Strong
More rigorously, we can show this using the proper- Models
ties of permutation importance and the construction Input: covariates X =[X 1...X P], target y
of f . The permutation importance is defined as Initialize k random subsets of the covariates,
MCR
thedecreaseinalossmetricL(f(X),y)whenafeature S ∗,1...S ∗,k of size i%p for i=1...k
X is randomly permuted. Define this permutation to for i=1 to k do
i
be Perm(X,i) for the data matrix X with X i per- Cross-validate a model on X×S ∗,i
muted. Since MCR uses exhaustive permutation (all Record the cross-validation performance L S∗,i
possiblepermutationsofX ,denotedPerm¯ (X,i)),the for l=1 to p do
i
only way for a permutation to result in a decrease of if S l,i =1: then
L(f(Perm(X,i)),y) is if the model is poorly fit, and Create S i,Xl=0 equal to S ∗,i except index l=
would benefit from dropping X . By definition of the 0
i
Rashomon Set, such a model could not be included Refit the model on X×s i,Xl=0
unless the chosen δ was extremely large. Therefore, Record the cross-validation performance
the only way for MCR to output a selective variable L S∗,i/Xl
importancefunctionisifeverymodelinf MCR isusing Record the difference ∆ i,l =L S∗,i/Xl −L S∗,i
the correct feature set. Since MCR specifically seeks end if
diversefunctionalforms,thisseemsincrediblyunlikely end for
in any real-world scenario. end for
Calculate the top b% of the L
MCRisalsoobviouslynotanattributivemethod,since
S∗,i
Subset to the evaluations L =L ≤L
it does not deal with any particular realized model. It
opt S∗,i B
Calculate the Shapley values ϕ ...ϕ of the covari-
1 p
isinauniquespace,whereitattributestheimportance
ates X ...X using the ∆ values corresponding to
1 p i,l
of different covariates to different possible models in
L
the model class F. opt
return ϕ ...ϕ
1 p
9 Shapley Marginal Surplus for
TheoremWithanasymptoticsamplesize,SMSSMis
Strong Models
a selective feature importance method with respect to
the data-generating process when L =L , assuming
B 0
We now introduce our method, Shapley Surplus for
k is also allowed to grow asymptotically.
Strong Models (SMSSM). Fix a model class F and
a maximal number of iterations k. We then sample Thisisstraightforwardfromthedefinitions. First,ob-
k elements of the power set of possible feature sets, servethatthereisafixedprobabilityateachiteration
P(p), evenly distributed across the possible number of that we select all of the elements of X T. Assume that
nonzero covariates with at least two elements, 2...p. the number of members of the set is T out of a total p
Label these k feature vector indicators as S ...S . members, and at the current iteration we are going to
∗,1 ∗,k
For each S ∗,i, we evaluate the cross-validation perfor- have sum(S ∗,i)=j. Then the probability of selecting
mance of models fit to that subset, and the marginal the full set, plus some added covariates, is:
surplus of each feature within the subset as estimated
bythecross-validationperformanceofthemodelwith- j! p!
P =I(j ≥T)( )/( )
out that feature. Finally, we restrict the top b percent T,j T! j!(p−j!)Shapley Marginal Surplus for Strong Models
ofadataset,thenoneorbothofthecalculatedimpor-
tances is necessarily wrong.
However,thisisnotanidealtest,becauseahighalign-
ment between the two subsets is also not indicative of
anaccuratemethod;itisentirelypossibleforamethod
tobeconsistentlywrong,meaningthattheresultscan
only be interpreted in the context of our results on
simulated data with a known ground truth.
Figure 1: Most feature importance methods take the Datasets were pulled from the UCI machine learn-
model as truth, going straight from creating a model ing repository. These include an Auto MPG dataset
to calculating importance (red arrow). Our procedure (Auto), a real estate valuation dataset (RE), a
addstheintermediatestepofcalculatingthevalidityof liver disease dataset (Liver), and a cancer prediction
theclassofmodels(blackarrows),whichdramatically dataset (Cancer) (Quinlan, 1993; Zwitter & Soklic,
improves feature importance allocation. 1988;mis,1990;Yeh,2018). Weevaluatedourtwose-
lective methods, LOCO and SMSSM, on evenly split
subsetsofeachdatasetacrossfivetrials, andaveraged
With an infinite sample size, any model that con- performance.
tains all members of X will achieve L . Similarly,
T 0
in asymptotic sample the loss change associated with
11 Results
droppinga covariate X ∈/ X will be zero. Therefore,
i T
only elements in X will have nonzero ∆ values, and
T
11.1 Simulated Data Test
thereforenonzeroShapleyvalues,intheseoptimalloss
samples.
We ran the full set of gradient boosted ensemble fea-
ture importance metrics for the datasets described.
10 Methods and Data The ”XGB” metric is the gain score from the XG-
Boost model used for SHAP. For our method, we as-
In this work, we primarily focused on simulated data, sessed the feature importance using the top 25% of
thoughwedidusetworeal-worlddatasets. Thisisbe- models.For simulated datasets 1, 5 and 6, we use the
cause only in simulated data can we have an explicit anglebetweenthereturnedweightsofthemethodand
formforthedata-generatingprocess,andexactlyeval- a known accurate ground truth weight. For simulated
uate to what extent the returned weights are selective datasets2,3,and4,weusedtherelativeweightonthe
with regard to this true data-generating process. covariates not in X T as the score, where X T is known
since the data was simulated according to a known
DGP. For the first metric, a low score is better; for
10.1 Simulated Data
the second, a high score is better. We denote these
with the superscipt of 1 and 2 respectively on the ta-
We simulate five simple feature relationships where
ble. These two metrics reference different important
the ground truth is known. These are simple strong
aspects of feature importance. In the first, we are try-
multicollinearity (DS 1), weak multivariate collinear-
ingtoexactlymatchagroundtruth. Inthesecond,we
ity (DS 2), mediation (DS 3), a noisy polynomial (DS
aretryingtoseetowhatextentmethodsareselective,
4), multivariate strong collinearity (DS 5), and higher
and can correctly allocate weight to true covariates.
dimensional strong multicollinearity (DS 6). For fur-
therdiscussionofthesedatasetsandtheexplicitdata-
generating processes used, please see the supplemen-
Table 1: Feature Importance Method Performance for
tary material.
Simulated Data
10.2 Real-World Data
- DS11 DS22 DS32 DS42 DS51 DS61
We cannot directly test the underlying accuracy with
XGB 1.49 0.99 0.70 0.92 0.54 1.56
respect to a DGP of real data, since the real DGP is SHAP 1.03 0.99 0.70 0.89 0.45 1.33
always unknown. However, similar to our argument MCR 0.97 0.99 0.74 0.92 0.46 1.28
for why MCR explanations are not selective, what we LOCO 0.12 1.00 1.00 1.00 0.62 0.64
can assess is the consistency of calculated feature im- SMSSM 0.06 0.99 1.00 0.99 0.28 0.44
portances on a randomly split master dataset. If a
method is not consistent on two evenly sized samples For the simulated data, SMSSM emerges as highestDaniel de Marchi, Michael Kosorok, Scott de Marchi
average performer, and receives either the best or sec- 12 Discussion
ond best score on all six simulated datasets, though
we must somewhat discount dataset 2 as all meth-
ods performed extremely well. Every other method
Table 3: Average Rankings of Feature Importance
dramatically underperforms on at least one dataset.
Methods
The XGBoost feature importance and SHAP are con-
sistent low performers, which is to be expected since Method Average Rank Best Worst
theseareattributiveexplanationtools,ratherthanse-
XGB 4.33 3 5
lective. MCR is the worst performer of the methods SHAP 3.67 2 5
that operate on multiple models, and does not stand MCR 3.17 3 4
out relative to LOCO and SMSSM. Despite excellent LOCO 2 1 5
performance on datasets 3, and 4, LOCO is the worst SMSSM 1.17 1 2
performer among all methods on dataset 5, and also
doesnotperformaswellasSMSSMondatasets1and
We calculate the average performance of each method
6. This is because the variables in these datasets have
on the simulated data. From the relative perfor-
an extremely high degree of collinearity, and LOCO
mances,aclearwinneremerges. SMSSMhasboththe
does not handle interdependent importance correctly,
bestoverallperformanceaswellasthemostconsistent
insteadonlyevaluatingeachfeature’smarginalutility.
performance,rankingaseitherthebestorsecondbest
This leads to under-performance on highly collinear
method on every dataset.
tasks,asevidencedbyLOCOreceivingtheworstscore
on dataset 5. Three methods - MCR, LOCO, and SMSSM have ex-
tremely high compute budgets required to make them
work. If a faster method is required, the vanilla XG-
11.2 Real Data Results Boost importance metric is the best. SHAP does a
good job of explaining models, but explaining nonlin-
We also present the results of alignment between two ear models is frequently harmful for actually explain-
randomly selected subsets of real datasets. As with ing the true DGP as shown above.
datasets 1 and 5 in the simulated data, a lower score
The most interesting approach is LOCO, which did
is better for this metric, as we also used the vector
well on four simulated datasets but was the lowest
angle method to evaluate performance.
performer on the fifth simulated dataset, and signif-
icantly underperformed SMSSM on dataset 1 despite
being the second best method. The common fac-
tor is high multicollinearity between covariates. Since
Table2: AverageScoreOverFiveTrialsforRealData
LOCO drops covariates one at a time, rather than in
groups like SMSSM, it appears to struggle when mul-
Dataset LOCO SMSSM
ticollinearity is high. This provides a potentially in-
Auto 0.249 0.264
teresting extension to LOCO where a grouped LOCO
RE 0.721 0.504
approachmightworkwellinfinitesamples,wheremul-
Liver 0.786 0.705
Cancer 1.111 0.596 ticollinearity between predictors can throw off esti-
mates.
Mean 0.717 0.517
Overall, we believe this is a valuable contribution to
the model-based feature importance literature. We
have demonstrated the limits of model-based expla-
From the above, we can see that SMSSM is also the
nations, and demonstrated that sampling the space of
mostconsistentofourtwoselectivefeatureimportance
possible models is the more effective approach.
methods, achieving a lower angle between trials on
paired datasets in three out of four trials. This rein-
13 Future Work
forcesourhypothesisthatSMSSMislikelytobegiving
moreaccuratefeatureimportancescores,aslowercon-
sistency implies a less trustworthy feature importance The most obvious future direction for SMSSM is to
function. We believe this is due to the added noisi- incorporateameta-modeltoestimatetheoptimalfea-
ness LOCO incurs by only focusing on each feature’s tureset. Theperformanceoftheoverallmodelateach
marginal utility, at the cost of the added robustness time step could be treated as a multi-armed bandit
of analyzing the mutual utility between covariates as problem, with the inclusion or exclusion effect of each
well. covariate as the target variable.Shapley Marginal Surplus for Strong Models
This approach would provide both theoretical and Lundberg, S. M. and Lee, S.-I. A unified approach to
practical benefits. In a theoretical sense, sampling interpreting model predictions. Advances in neural
more of the space of highly promising models and less information processing systems, 30, 2017.
of the space of unpromising models should decrease
Lundberg, S. M., Erion, G., Chen, H., DeGrave, A.,
the noise of the procedure and improve the conver-
Prutkin, J. M., Nair, B., Katz, R., Himmelfarb, J.,
gence rate. In a practical sense, this would also pro-
Bansal,N.,andLee,S.-I.Fromlocalexplanationsto
vide a large computational benefit, as more efficient
global understanding with explainable ai for trees.
samplingwouldrequirefeweroverallsampleswhilede-
Nature machine intelligence, 2(1):56–67, 2020.
livering the same or higher-quality results.
Quinlan, R. Auto MPG. UCI Ma-
Thisisimportantbecausecurrently,thebiggestbottle- chine Learning Repository, 1993. DOI:
neck in using higher-quality importance methods like https://doi.org/10.24432/C5859H.
LOCO or SMSSM is the compute required, making
Shapley, L. A value for n-person games. 1953.
these methods infeasible in large sample.
Yeh, I.-C. Real Estate Valuation. UCI
Otherwise, we feel that this work is a promising
Machine Learning Repository, 2018. DOI:
first step towards inferential explanations for machine
https://doi.org/10.24432/C5J30W.
learning methods, that both respect the model used
Zwitter, M. and Soklic, M. Breast Cancer.
aswellastheunderlyingDGP.Ifthesemethodscould
UCI Machine Learning Repository, 1988. DOI:
be made more efficient, it might someday be possible
https://doi.org/10.24432/C51P4M.
to use powerful and flexible models in the same way
linear models are used today.
References
Liver Disorders. UCI Machine Learning Repository,
1990. DOI: https://doi.org/10.24432/C54G67.
Branzei, R., Dimitrov, D., and Tijs, S. Models in co-
operativegametheory,volume556. SpringerScience
& Business Media, 2008.
Burkart, N. and Huber, M. F. A survey on the ex-
plainabilityofsupervisedmachinelearning. Journal
ofArtificialIntelligenceResearch,70:245–317,2021.
Chen, H., Janizek, J. D., Lundberg, S., and Lee, S.-
I. True to the model or true to the data? arXiv
preprint arXiv:2006.16234, 2020.
Covert, I., Lundberg, S., and Lee, S.-I. Explaining
by removing: A unified framework for model expla-
nation. Journal of Machine Learning Research, 22
(209):1–90, 2021.
Fisher,A.,Rudin,C.,andDominici,F. Allmodelsare
wrong, but many are useful: Learning a variable’s
importancebystudyinganentireclassofprediction
models simultaneously. J. Mach. Learn. Res., 20
(177):1–81, 2019.
Kumar, I. E., Venkatasubramanian, S., Scheidegger,
C., and Friedler, S. Problems with shapley-value-
basedexplanationsasfeatureimportancemeasures.
In International Conference on Machine Learning,
pp. 5491–5500. PMLR, 2020.
Lei, J., G’Sell, M., Rinaldo, A., Tibshirani, R. J., and
Wasserman,L.Distribution-freepredictiveinference
for regression. Journal of the American Statistical
Association, 113(523):1094–1111, 2018.