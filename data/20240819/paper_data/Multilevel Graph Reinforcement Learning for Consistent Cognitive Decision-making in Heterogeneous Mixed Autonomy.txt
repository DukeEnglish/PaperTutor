JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 1
Multilevel Graph Reinforcement Learning for
Consistent Cognitive Decision-making in
Heterogeneous Mixed Autonomy
Xin Gao1, Graduate Student Member, IEEE, Zhaoyang Ma1, Xueyuan Li*, Xiaoqiang Meng, Zirui Li, Graduate
Student Member, IEEE
Abstract—In the realm of heterogeneous mixed autonomy, expected to remain in a state of transition for the foreseeable
vehicles experience dynamic spatial correlations and nonlin- future,withthesimultaneouspresenceofvehicleswithdiverse
ear temporal interactions in a complex, non-Euclidean space.
levelsofintelligence,includingHuman-drivenVehicles(HVs),
These complexities pose significant challenges to traditional
ConnectedHuman-drivenVehicles(CHVs),andconnectedand
decision-making frameworks. Addressing this, we propose a
hierarchical reinforcement learning framework integrated with autonomous vehicles (CAVs) [2]. Such diversity introduces
multilevel graph representations, which effectively comprehends significant heterogeneity in terms of intelligence, communica-
and models the spatiotemporal interactions among vehicles tioncapabilities,decision-makingprocesses,anddrivingtasks
navigating through uncertain traffic conditions with varying
[3], posing challenges to collaborative micro-level decision-
decision-making systems. Rooted in multilevel graph represen-
making.
tation theory, our approach encapsulates spatiotemporal rela-
tionships inherent in non-Euclidean spaces. A weighted graph Efficient navigation in diverse and intricate traffic scenar-
represents spatiotemporal features between nodes, addressing ios is essential for the advancement of autonomous driving
the degree imbalance inherent in dynamic graphs. We integrate technologies. These scenarios are defined by heavy traffic,
asynchronousparallelhierarchicalreinforcementlearningwitha
the convergence of vehicles and pedestrians, unpredictable
multilevelgraphrepresentationandamulti-headattentionmech-
behaviors, and numerous operational restrictions. We term the
anism, which enables connected autonomous vehicles (CAVs)
to exhibit capabilities akin to human cognition, facilitating operationofautonomousvehiclesinsuchsettingsasheteroge-
consistent decision-making across various critical dimensions. neous mixed autonomy. Present decision-making frameworks
Theproposeddecision-makingstrategyisvalidatedinchallenging based on expert rules exhibit a lack of flexibility, and struggle
environments characterized by high density, randomness, and
to adjust to varying scenarios. In the sphere of heterogeneous
dynamism on highway roads. We assess the performance of
mixedautonomy,theintelligenceofautonomousvehiclesdoes
our framework through ablation studies, comparative analyses,
and spatiotemporal trajectory evaluations. This study presents a notyetmatchthatofhumandrivers,whichcompromisestraffic
quantitative analysis of decision-making mechanisms mirroring flow and jeopardizes road safety. This area is distinguished
human cognitive functions in the realm of heterogeneous mixed by dynamic spatial correlations and nonlinear temporal cor-
autonomy, promoting the development of multi-dimensional
relations. Dynamic spatial correlation highlights the changing
decision-making strategies and a sophisticated distribution of
relationship of traffic patterns across locations over time, and
attentional resources.
nonlineartemporalcorrelationreflectsanonlinearrelationship
Index Terms—Heterogeneous mixed autonomy, connected au-
between traffic states and their timing. These nonlinear spa-
tonomous vehicles, Multi-level Graph theory, parallel asyn-
tiotemporalinteractionsamongentitiesoccurinnon-Euclidean
chronous hierarchical reinforcement learning.
spaces.Hence,conventionaldeeplearningapproaches,suchas
ConvolutionalNeuralNetworks(CNNs)andLongShort-Term
Memory (LSTM) networks, fail to adequately capture these
I. INTRODUCTION
intricate spatiotemporal relationships.
AUTOMOBILEautomationisemergingasapivotaltrend
CAVs embody advanced systems characterized by substan-
[1]. In the United States, 62 companies specializing in tial self-learning and iterative improvement capabilities. In a
autonomousdrivinghavebeenauthorizedtoperformroadtests nascentstageofintelligence,theyareprimarilysuitedtostruc-
in California. It is anticipated that by 2040, 40% of China’s tured road scenarios with sparse traffic flows. In urban traffic
vehicles will achieve automation. Road traffic systems are conditions, CAVs still must effectively integrate spatiotempo-
ral dynamics with nearby vehicles and develop self-learning
1 XinGaoandZhaoyangMacontributeequallytothiswork.
abilities to navigate unknown scenarios. Within the frame-
*Correspondingauthor:XueyuanLi
Xin Gao,Xueyuan Li,Xiaoqiang Meng andZirui Liare withthe School work of heterogeneous mixed autonomy, CAVs demonstrate
of Mechanical Engineering, Beijing Institute of Technology, Beijing, China. elementary cognitive functions comparable to humans, and
(E-mails: x.gao@bit.edu.cn; lixueyuan@bit.edu.cn; 3120220377@bit.edu.cn;
significant dependence on human intervention. Insights from
z.li@bit.edu.cn.
ZhaoyangMaiswiththeSchoolofComputerandInformationTechnology, cognitivepsychologyandalliedfieldssuggestthattheskillful-
BeijingJiaotongUniversity,Beijing,China.(E-mails:zhy.ma@bjtu.edu.cn) nessofbiologicalintelligenceatsolvingproblemsstemsfrom
ZiruiLiisalsowiththeChairofTrafficProcessAutomation,”Friedrich-
hierarchical cognitive processes [4], and it becomes evident
List”FacultyofTransportandTrafficSciences,TUDresden,Germany.
that a similar approach could be beneficial in the domain of
4202
guA
61
]AM.sc[
1v61580.8042:viXraJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 2
autonomous vehicles [5]. Hierarchical reinforcement learning simulation results. Conclusions are drawn in section VII.
is a promising computational approach, potentially equipping
CAVs with problem-solving skills mirroring those of biolog-
ical intelligence [6], effectively enabling the segmentation of
II. RELATEDWORK
decision-making tasks into separate dimensions. Yet, in com-
A. Graph Representation for Connected and Autonomous Ve-
plex and dense traffic scenarios, human drivers still markedly
hicles
outperform artificial systems. The key factors contributing to
this gap include: (a) inadequately capturing the full spectrum Scholars have identified limitations in traditional decision-
ofvehicleinteractionswithindiversedimensions;and(b)ade- makingmethodologiesbasedonMultilayerPerceptron(MLP)
ficiencyinconsistentcognitiveprocessingfordecision-making and LSTM that primarily focus on individual vehicle charac-
across these dimensions. This leads to the question of how teristics, neglecting interactive dynamics among neighboring
CAVs can be equipped with a hierarchical learning capability vehicles [7]. To address this, several researchers have adopted
that parallels the consistent cognitive abilities observed in theGraphNeuralNetwork(GNN)torepresentdynamictraffic
humans. scenarios [8], [9], where vehicles are depicted as nodes in a
graph, with inter-vehicular relationships portrayed as edges
We propose a hierarchical reinforcement learning frame-
between nodes. Hart et al. [10] employed a GNN to display
work based on multilevel graph representation theory, to
thesevehicularinteractions,andfoundthataGNN,incontrast
accurately capture the spatiotemporal dynamics in the realm
toconventionaldeepneuralnetworks,canwellhandlevarying
of heterogeneous mixed autonomy. This facilitates decision-
numbers of vehicles across diverse scenarios, significantly
making processes reflecting human cognitive strategies by
enhancing the versatility of decision-making models. Chen
managing the complexities of spatiotemporal relationships,
et al. [11] conceptualized the interaction of CAVs in traffic
particularly in non-Euclidean spaces. A parallel asynchronous
scenarios as a dual-layer network, where the first layer is
hierarchical graph reinforcement learning strategy is designed
a star-shaped local network, encompassing CAVs and their
toenhancetheself-learningcapabilitiesanditerativeimprove-
adjacent HVs, and the second layer is a global network,
ment processes of CAVs. We employ specialized node feature
consistingofnodesthatcorrespondtoCAVsonroadways.By
matrices,dynamicadjacencymatrices,andrewardfunctionsto
integrating a Graph Convolutional Network (GCN) and Deep
analyzevariouscriticaldimensions.Thisensuresanoptimized
Q-Network(DQN)inacombinedGCQmodel,theyfacilitated
distribution of attentional resources, mirroring the human
collaborative lane-changing decisions among CAVs.
capacity for divided attention and adept multitasking. We
assesstheviabilityandeffectivenessofourproposeddecision- Several studies have further explored graph representations
makingframeworkinasimulatedenvironmentofhigh-density, ofinteractionsamongCAVs.Huetal.[12]introducedaGCN
unpredictable, and dynamic highway traffic. The principal featuring a trainable adjacency matrix, addressing the inef-
contributions of this paper are outlined as follows: fective representation of dynamically changing relationships
between nodes. Hu et al. [13] used dynamic coordination
(a) A multilevel graph representation theory is proposed to
graphs to model the evolving topological structures inherent
model the spatiotemporal dependencies present in non-
invehicleinteractions,andtwofundamentallearningmethods
Euclidean spaces;
to coordinate the driving maneuvers of a group of vehicles,
(b) Weighted graphs based on spatiotemporal features be-
thusenhancingthecoordinationofdecision-making.Gaoetal.
tween nodes address the degree imbalance problem in
[14] included additional dimensions of vehicular interactions,
dynamicgraphs.Thecouplingofmultidimensionalgraphs
representing interactions between vehicles, and those between
withmulti-headattentionmechanismsenableshuman-like
traffic signals and CAVs, as edges and nodes, respectively.
divided attention and consistent cognitive capabilities;
Experimental results indicated that the method enables CAVs
(c) A parallel asynchronous hierarchical graph reinforcement
to rapidly and accurately grasp global traffic states and local
learningapproachisproposedtoenhancetheself-learning
interaction data.
iteration capabilities of CAVs. This includes an asyn-
IntherealmofCAVs,whilepreviousresearchongraphrep-
chronous multidimensional Markov decision process and
resentationshasadeptlymodeledtheinteractionsandrelation-
improved GRL algorithms based on multilevel graphs;
ships among multiple vehicles, these studies have not delved
(d) The comprehensive performance of the proposed frame-
deeply into the spatiotemporal interactive behaviors of CAVs.
workistestedthroughablationstudies,comparativeexper-
Researchhasprimarilyprovidedasuperficialrepresentationof
iments,andspatiotemporaltrajectoryanalysis,whichshow
spatialinteractions,focusingonlyontheimmediateneighbors
that CAVs are capable of inter-dimensional collaboration,
of a target vehicle and neglecting the dynamic spatial cor-
and hence to make cognitive decisions.
relations inherent in heterogeneous mixed autonomy. Recent
Theremainderofthisarticleisorganizedasfollows.Section workhasintroduceddynamicgraphstorepresentthetemporal
II introduces related work. The proposed multilevel graph correlationsofCAVs.However,vehiclesthatsuddenlyexceed
representation theory is illustrated in section III. Section IV a predefined distance threshold can cause abrupt topological
introduces the methodology, including an asynchronous mul- changes, resulting in the disappearance of nodes. This type of
tidimensional graphical Markov decision process and parallel step-likeinteractionessentiallyembodiesanonlineartemporal
asynchronous hierarchical reinforcement learning. Section V correlation, which can affect robustness in decision-making
explains the setting of experiments. Section VI analyzes the processes.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 3
B. Decision-making based on Hierarchical Reinforcement III. MULTILEVELGRAPHREPRESENTATIONTHEORY
learning
We introduce the proposed multi-level graph representation
theory. The foundational theory of traffic graphs includes the
mathematical representation of the node feature matrix and
Deep Reinforcement Learning (DRL) offers a paradigm
adjacency matrix. Expanding upon the principles of divided
shiftinlearningcomplexdecision-makingstrategies,asitdoes
attention and cognitive consistence, we explore traffic graph
not rely on a large amount of labeled driving data [15]–[17],
theory, broadening its applicability to dynamic, weighted, and
learning instead through direct interaction with the environ-
multidimensional graph theories [31].
ment.Thisapproachhasbeenextensivelyappliedintrafficsce-
narios to address the autonomous decision-making challenges
faced by CAVs. Various methods employing DRL have been
A. Foundational Theory of Traffic Graphs
developed to train CAVs for highway merging tasks, aiming
toenhancetrafficthroughputwhilemaintainingzerocollisions
Assume a scenario where, at each discrete time-step t, the
[18], [19]. Other techniques focus on training CAVs for safe
traffic environment is comprised of N vehicles, where M
andefficientdecision-makingaturbanintersections[20]–[22].
(M ≤ N) of those are autonomous vehicles that interact
Notableamongthesemethodsareriskassessmentmodels[23],
with others. The traffic scenario is conceptualized as a graph
[24],combinatorialdecisionstrategies[22],[25],andmultitask
G (V,E) with N nodes, where V(G) denotes the collection
t
reward models [26], [27]. Much research concentrates on the
of nodes within the graph, each node u (u∈V) representing
performance of autonomous vehicles in singular, controlled
a vehicle; E(G) represents the set of edges, illustrating the
scenarios,butreal-worldtrafficenvironmentspresentfarmore
exchangeofinformationbetweenvehiclesasedges(u,v)∈E
dynamic and complex scenarios.
[14], [32]. The graph at time-step t is characterized by the
Hierarchical Reinforcement Learning (HRL) can well man- node feature matrix N t ∈ RN×F and adjacency matrix A t ∈
age complex, dynamic traffic scenarios by introducing a RN×N, where F is the total number of features attributed to
hierarchical structure in the decision-making process [28]. each node.
HRL decomposes the decision-making of CAVs into distinct 1) Node Feature Matrix: The node feature matrix N t
hierarchical layers, simplifying the learning of complex tasks, indicates the states of all vehicles, including CAVs and HVs.
and enabling CAVs to more efficiently solve large-scale and For the i th vehicle, the state matrix S t indicates parameters
intricate problems. A hierarchical DRL framework was con- such as vehicle speed V i, lateral position X i, longitudinal
structed by dividing driving tasks into sub-tasks, allocated position Y i, road segment occupancy R i, lane occupancy L i,
to different sub-models for training, which are subsequently and vehicle category I i. The mathematical representation is
integrated. Experiments demonstrated a 100% success rate
(cid:26)
S =[V ,X ,Y ,R ,L ,··· ,I ]
in task completion [27]. Duan et al. [29] proposed an HRL Nt =(cid:2) Si 1,Si 2,·i
··
,i Si,i
···
,SNi (cid:3) , (1)
method that does not depend on a large volume of labeled t t t t t
driving data. The driving task is dissected into strategies of
whereSi (i=1,2,··· ,N)isthefeaturesetofthei vehicle
driving in lane, right lane change, and left lane change. A t th
at the t time-step.
master strategy is learned, so as to select the appropriate th
maneuvering strategy in the current state. Peng et al. [30] 2) Adjacency Matrix: The interactions between vehicles at
introducedupper-andlower-leveldeepreinforcementlearning thet th time-steparerepresentedbytheadjacencymatrixA t ∈
algorithmstoaddresscomplexstatespaces,employingD3QN
RN×N,
and DDPG to train lane-changing and following decisions,  
a a ··· ··· a
respectively. The model was found to increase the driving 11 12 1N
speed by 23.99%.  


a 2 .
.
.1 a 2 .
.
.2 · .· ..· ··· a 2 .
.
.N  


achA ielt vh eo dug th he p dr ee cv oio mu ps osiH tioR nL ofm de eth co isd is on-mha av ke ingsu tc ac se ks ssfu inll ty o A t =   a ij   , (2)
distinct dimensions, deeper exploration is needed across var-   . . . . . . ... . . .  
ious dimensions, and a re-examination and study of vehicle a a ··· ··· a
N1 N2 NN
interaction relationships within these decision dimensions are
imperative. For instance, in the lane-changing and tracking a binary matrix, where a ij has a value of 1 or 0 to indicate
dimensions, emphasis should be placed on interactions with thepresenceorabsence,respectively,ofaconnectionbetween
vehicles in adjacent lanes and in the same lane, respectively. the ith and jth nodes.
HRLdecision-makingmethodologieshavebeguntoadoptpar- Basedontheprinciplesofdirectedgraphs,thecomputation
allel learning strategies. But the synchronous parallel learning oftheadjacencymatrixisbasedonfourassumptions:a)CAVs
approach disregards the cognitive integration across diverse withinadefineddistancethresholdonadjacentlanescanshare
decision-making dimensions. For instance, within environ- information; b) Information cannot be shared between HVs;
mentscharacterizedbyheterogeneousmixedautonomy,CAVs c) All CAVs can share information about HVs within their
must adeptly modulate their acceleration to identify a secure threshold area; d) A vehicle can share information with itself,
spatiotemporal region before changing lanes. i.e., a =1.
iiJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 4
(a) Localschematicrepresentationofadynamictrafficgraph. (b) Localschematicrepresentationofamultidimensionaltrafficgraph.
Figure 1. Schematic diagrams of dynamic and multidimensional traffic graphs. Colored and gray circles denote CAVs and HVs, respectively. (a) Spatial
dynamicsamongvehiclesaltergraph’stopologicalstructurefromttot+1,affectingnumberofnodesandedges,andobjectivesofedgeconnections,while
categorizationofedgesremainsunchanged;(b)Interactionsaredistinguishedbasedonvehicletypeandspatialdynamics.LC:interactionbetweenCAVand
HVinadjacentlane;FL:interactionbetweenCAVandHVinsamelane;CC:interactionbetweentwoCAVsinadjacentlanesorsamelane.
B. Multilevel Graph Representation Theory Theorem 1 addresses this issue, and is proved in Appendix
B.
In the context of heterogeneous mixed autonomy, dynamic
Theorem 1 (Equilibrium Theorem for Degree Distribution in
spatialcorrelationsandnonlineartemporalinteractionsamong
WeightedDynamicGraphs). Upontransitioningfromtimetto
diversevehicular entitiesareobserved within anon-Euclidean
t+1,withinadynamicgraphG characterizedbyaweighted
domain,inwhichthemultilevelstructureofthegraphcanwell t
adjacency matrix A –with weights at stemming from both
capture dependencies. Graphs constituting the traffic scenario t ij
spatial distances and attributes of nodes–the disruption in
canbecategorizedasdirectedorundirected,staticordynamic,
degreedistributionpromptedbytheeliminationofanodeuis
unidimensionalormultidimensional,anduniformorweighted.
alleviated by the graph’s weighted mechanism. In particular,
We discussed directed graphs above, and below we address
the alteration in the weighted degree of any node v resultant
dynamic, multidimensional, and weighted graphs in traffic
from node u’s elimination undergoes a seamless adjustment,
scenarios.
conforming to the formula
1) Dynamic Traffic Graph: In traffic scenarios, the mo-
bility of vehicles results in changes to both the individual dw (v)=dw(v)−at , (3)
t+1 t uv
vehicle characteristics and the topological structure of the
which facilitates a steadier degree distribution transition than
graph, rendering it dynamic. In the dynamic traffic graph,
that typically encountered in unweighted (binary) graphs.
based on changes in its spatiotemporal properties, we identify
twodynamiccharacteristics:a)Newvehiclenodescanappear 2) Multidimensional Traffic Graph: In dynamic traffic
or disappear, and new interactions between vehicles can be scenarios, the interactions between vehicles exhibit dynamic
establishedorterminatedwithindifferenttimeperiods;b)The spatial correlations. As vehicles move through space, their
movement of vehicles leads to changes in their individual modes of interaction change, and can have the following
characteristics and the nature of their interactions. three characteristics: a) Interactions between CAVs and HVs
on adjacent lanes are considered lane-changing (LC) edges;
Definition 1 (Dynamic Traffic Graph). A dynamic traffic b) Interactions between CAVs and HVs on the same lane
graph is defined as G = (G 1,G 2,··· ,G t,··· ,G T), and at are considered as following (FL) edges; and c) Interactions
time-step t is defined as G t = G t(V t,E t), where t ∈ T, and between CAVs on adjacent lanes are considered connected
T is the total number of time-steps. Each edge et ij = E t at CAV-CAV (CC) edges. The CC edges form the basis of a
time-step t connects the i-th and j-th vehicles in interaction. multidimensional traffic graph through expansion to LC and
Eachnode isrepresented bythestate informationof avehicle FL edges.
at the current moment t. Figure 1(a) shows a local schematic
Definition 2 (Multidimensional Traffic Graph). Define the
representation of a dynamic traffic graph.
traffic environment at each time t as a multidimensional
When a node disappears in a dynamic graph, its edges graph G (T,V,E), where T ∈ (L,F) represents the di-
t
with other nodes are severed, directly impacting the graph’s mensions of the graph, V(G) is the set of nodes, and E(G)
structural characteristics, especially its degree distribution. In is the set of edges. Depending on the graph dimension T,
uniformgraphs,whoseadjacencymatrixvaluesareonly0or1, graph G(T,V,E) is decoupled into a lane-changing dimen-
with the disappearance of connections between nodes, related sion graph G (V ,E ) and a following dimension graph
L L L
edges transition from 1 to 0, which is referred to as a step- G (V ,E ).Subsequently,fortemporalrepresentationofthe
F F F
wise imbalance in the graph’s degree distribution. Appendix graph, at time-step t, it is characterized by the node feature
A shows the mathematical analysis. matrix NM×FT and adjacency matrix AM×M, where F is
T−t T−tJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 5
the total number of features per vehicle, and M is the total relationships between vehicles are often simplified as binary.
numberofvehicles.Thesub-nodefeaturematricesfordifferent Inweightedgraphs,theedgeweightschangewiththestatesof
dimensions {NM×FL,NM×FF} ∈ NM×F are extracted, vehicles, and hence can represent the strength and timeliness
L−t F−t T−t
where F ≤ F. Based on vehicle interactions in different of vehicle interactions. Based on the proposed dynamic and
T
dimensions (u ,v )∈E , adjacency matrices are similarly multidimensional traffic graph, weight representations are de-
T T T
decoupledintotwodimensions{AM×M,AM×M}.Figure1(b) termined separately for the two-dimensional dynamic graphs.
L−t F−t
shows a local schematic representation of a multidimensional Theadjacency matricesof thelane-changing dimensiongraph
traffic graph. and following dimension graph {AM×M,AM×M} are indi-
WL−t WF−t
vidually designed.
The proposed multidimensional traffic graph accounts for
The sub-adjacency matrix AM×M for the lane-changing
lane-changingandfollowing,whichenablesprecisecaptureof WL−t
dimensiongraphisconstructedbasedonthedistancebetween
spatiotemporal interactions between vehicles across different
vehicles and their lanes. Considering the difficulty human
dimensions. Coupled with the multilevel multi-head attention
drivers face in estimating the speed of vehicles in adjacent
mechanism(sectionIV-C),itispossibletoemulatehuman-like
lanes, the weight aL is defined as a function of the distance
divided attention and consistent cognitive capabilities, such ij
andlane,decreasingwithdistance,andrangingbetween0and
as observing the surrounding environment and preparing for
1. For the j vehicle located on an adjacent lane to the i
lane changes while maintaining lane discipline.This nuanced th th
CAV, with a lane-direction distance of ∆d and a distance
approachtomodelingvehiculardynamicsservesasaprecursor ij
threshold X, the weight function is defined as
totheTheorem2,whichdemonstratesthesystematicreduction
in entropy within sub-dimensional graphs, underscoring the (cid:26) max(0,1− ∆dij), if d <X,
aL = X ij (8)
efficacy of targeted attention in complex traffic scenarios. ij 0, if d ≥X.
ij
Theorem 2 (Targeted Attention-Driven Entropy Reduction As the distance ∆d approaches the threshold X, the
ij
in Sub-Dimensional Graphs). Consider a complex dynamic weight approaches 0. When ∆d is small, indicating close
ij
graph G that encompasses multi-dimensional representation, proximity, the weight approaches 1.
along with its corresponding sub-dimensional graphs G , For the sub-adjacency matrix AM×M of the following
T WF−t
wherein T ∈ {G ,G } signifies distinct dimensions of dimension graph, which is built based on the distance and
L F
interaction. For any nodes i that is congruent with decision- speed difference between two vehicles on the same lane,
making objectives in G , it is posited that the probability of the weight aF is a function of both the distance and speed
T ij
attentionp (i)surpassestheanalogousprobabilityacrossthe difference, decreasing with an increase in distance, and a
T
global graph G, represented by p(i), Formally, decreased speed difference, and ranging between 0 and 1. For
the j-th vehicle on the same lane as the i-th CAV, with a
p (i)>p(i), ∀i∈V (4)
T T lane-direction distance of ∆d and distance threshold Y, the
ij
Basedonthispremise,theentropyrelatedtoedgeattention weight function is defined as
in each dimension-specific subgraph, H , is demonstrably (cid:40) (cid:16) (cid:17)
lower than the entropy found in the globT al graph, H G. This aF ij = exp −∆ λd dij − ∆λ vv ij , if d ij <Y or ∆v ij ≥∆V,
relationship is defined mathematically as
0, else.
(9)
(cid:88)
H T =− p T(i)logp T(i), (5) As ∆d ij approaches the distance threshold X, and ∆v ij
i∈VT approaches the speed threshold ∆V, the weight approaches
(cid:88) 0, and conversely, the weight approaches 1 as ∆d decreases
H =− p(i)logp(i), (6) ij
G
and ∆v increases.
i∈V ij
H <H . (7)
T G
IV. METHODOLOGY
This conclusion substantiates that the focused allocation
We discuss the methodology employed in this study. The
of attention towards nodes that are directly relevant to the
problem formulation includes an asynchronous multidimen-
decision-making objectives in each sub-dimension results in
sional graphical Markov decision process (AMG-MDP), with
a significant reduction in entropy, H , evidencing a more
T statespaces,actionspaces,andrewardfunctionsacrossdiffer-
concentrated and efficient distribution of attention within sub-
ent dimensions. The proposed Parallel Asynchronous Hierar-
dimensional graphs compared to the comprehensive graph.
chical Reinforcement Learning (PAHRL) strategy and multi-
3) Weighted Traffic Graph: In traffic scenarios, a weighted
levelmulti-headgraphattentionnetworkmodulearedescribed.
graph can represent spatiotemporal interactions between vehi-
MultilevelgraphreinforcementlearningisillustratedinFigure
cles with greater precision than a uniform graph. It can more
2.
finely characterize the dynamic spatial interactions between
vehicles, as the edge weights can represent not only the exis-
A. Problem Formulation
tence of a connection between vehicles, but can include mul-
tidimensional information such as distance, relative speed, or 1) AMG-MDP: Heterogeneous mixed autonomy presents
trafficdensity.Also,weightedgraphscanmoreeffectivelysim- novel challenges for the coordination of cross-dimensional
ulate nonlinear temporal correlations. In uniform graphs, the decisions by CAVs. AMG-MDP modifies the conventionalJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 6
Multi-level Graph Representation L-dimension
Backpropagation
(cid:71)
L-Graph Representation a TD-Algorithm L
(cid:122) L-Node Feature Matrix NM(cid:117)FH
L(cid:16)t q(cid:4) t(cid:14)1(cid:32)Q i,t(cid:14)1
(cid:122) aL L- (cid:16)A ijd (cid:32)j
(cid:173) (cid:176)
(cid:174)a mce an xc (0y
,
1M (cid:16)a (cid:39)t Xdri ijx
),if dij(cid:31)X
CAV
G L Update (cid:146)(cid:82)Q(g tL,a tL,(cid:82))
(cid:175)(cid:176) 0, if dij(cid:116)X
HV softmax
(cid:71) Lane-changing Action
F-Graph Representation a L
(cid:122) F-Node Feature Matrix NM(cid:117)FL
(cid:122) F-Adjacency Matrix F(cid:16)t F-dimension
aF(cid:16)ij(cid:32)(cid:174)
(cid:176)
(cid:175)(cid:173) (cid:176)exp((cid:16)(cid:39) (cid:77)d dij 0(cid:16) ,(cid:39)(cid:77) vv ij), ifdij(cid:31)Y elo sr e(cid:39)vij(cid:116)(cid:39)V
G F
a tF Q(cid:11)gtF,atF|(cid:88)Q(cid:12) a(cid:71) F
Update Update BP AlgoT rD it- hm
(cid:28595)G((cid:44),V,E) N (cid:44)M (cid:16)(cid:117) tF(cid:44) Hierarchical
Rewards a tF (cid:14)1
Local Environment
(cid:139) Energy
Q(cid:11)gtF (cid:14)1,atF (cid:14)1|(cid:88)Qtarg(cid:12)
(cid:139) Safety (cid:139) L Acceleration Action
(cid:139) Comfort (cid:139) F (R ,R ) (cid:71) (cid:71)
(cid:139) Tasks F L a F,a L
Centralized Control System
(cid:28595)G((cid:44),V,E) N (cid:44)M (cid:16)(cid:117) tF(cid:44) ALaNteSrIa lA AMI ECL5o7n:g1it9u9di8nal E Ex xp po lon re an tt ioia nl
Controller Controller mechanism
Figure2. Proposedmultilevelgraphreinforcementlearningframework.Trafficscenarioisrepresentedasmultilevelgraph.Basedoncharacterizationofits
multidimensionalgraphs,parallelasynchronoushierarchicalgraphreinforcementlearningalgorithmisproposedfortrainingactionsinbothlane-changingand
followingdimensions.Actionsareenhancedinexplorationefficiencythroughexponentialexplorationmechanism,andsubjecttocentralizedcontrolsystem.
Hierarchicalrewardfunctionscomputesrewardfeedbackforcorrespondingdimensions,facilitatingpolicyiterationupdates.
where (sT,sT,...,sT,...,sT) ∈ S ; G is the graph
1 2 i n T T
g tL (cid:81) tL r tL g TL (cid:14)(cid:39) (cid:81) TL (cid:14)(cid:39) r tL (cid:14)(cid:39) representation information of all vehicles under dimension T,
constructed from the original states S according to graph
aL aL
t t(cid:14)(cid:39) representation theory, where (gT,gT,...,gT,...,gT)∈G ;
s P s P s P 1 2 i n T
t t(cid:14)1 t(cid:14)(cid:39) A representsallpossiblejointactions(a ,a )∈A ,where
T L F T
a tH a tH (cid:14)1 a tH (cid:14)(cid:39) (AT 1,AT 2,...,AT m)∈A T;P ((g L′ ,g H′ )|(g L,g H),(a L,a H))∈
g tF (cid:81) tF r tF g tF (cid:14)1 (cid:81) tF (cid:14)1 r tF (cid:14)1 g TF (cid:14)(cid:39) (cid:81) tF (cid:14)(cid:39) r tF (cid:14)(cid:39) P is the probability of transitioning from joint graph ob-
servation state g to g′ when joint action a occurs; and
T T T
R (g ,a ) is the reward calculated when joint action a
T T T T
Figure3. AsynchronousMultidimensionalGraphicalMarkovDecisionPro- occurs in joint graph observation state g under decision
T
cess.
dimension T, and can be computed as R (g ,a ) =
T T T
(cid:80)m rT(gT,aT), where gT and aT are the local graph
i=1 i i i i i
MDP framework to model the spatiotemporal correlations in observation state and action pertaining to the i-th CAV,
CAV decisions across various dimensions, allowing the MDP respectively. The objective is to learn a joint policy π T
f mra am kie nw g.ork to accommodate multi-agent coupled decision- Eth πat (cid:2)(cid:80)ma
∞
t=xi 0m γi tz Res T(th ge tT,g al
T
tob )a |gl 0Tv =alu ge T(cid:3)f .unction Qπ T(g T,a T) =
Definition 3 (AMG-MDP). The Asynchronous Multidimen- TheAMG-MDPchainisillustratedinFigure3.Attime-step
sional Graphical Markov Decision Process can be de- t, the current state of the environment s is observed through
t
fined by the tuple N = (n,m,T,∆,S,G ,A ,P,R ), graph representation theory, yielding the lane-changing and
T T T
where n is the number of vehicles present in the cur- following dimension graph observation states g and g ,
L H
rent scenario {V ,V ,...,V }; m is the number of CAVs respectively. Using these as inputs, the corresponding policy
1 2 n
{CAV ,CAV ,...,CAV }; T ∈ (L,F) gives the decision networks πL and πH output lane-changing action aL and
1 2 m t t t
dimensionsofAMG-MDP,whereLandF arerespectivelythe acceleration action aH, which are input to the state transition
t
lane-changingandfollowingdimensions;∆isthedifferencein probability P to update the state. Here, the lane-changing
time-steps between decisions in the two dimensions; S is the actionaL ispartofthefollowingdimensiongraphobservation
t
state information of all vehicles in the current environment, state g . Subsequently, rewards rL and rH are calculated
H t t
krowteN
eulaV
krowteN
tegraT
rotcA
niaM
rotcA
tegraT
citirC
niaM
citirC
tegraTJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 7
througheachdimension’srewardfunction,andusedtoupdate
the corresponding dimension’s policy networks πL and πH.
t t
In subsequent Markov decision processes at each time-step,
the following dimension is updated at every step, while the
lane-changing dimension is updated every ∆ time-steps.
110000mm 880000mm 110000mm
2) State Space: The state space S includes the state in-
T
formationofallvehiclesinthescenario.Thestateinformation
for a single vehicle sT includes data about the self-state,
i
environmentalinformationunderdimensionT,andadditional OOrriiggiinnaattiinngg ppooiinntt
EExxppeeccttaattiioonn ppaatthh
information. We can write DDeessttiinnaattiioonn ppooiinntt
(cid:26) sT =[sself,senvsadd]
i T T , (10)
gT =G (sT)
i OBS i Figure4. Three-lanehighwayentry/exitscenario.CAVsentermainlanefrom
originatingpoint,changelanesandspeedbyjudginggapintrafficflow,and
wheresself isthecurrentstateinformationofthevehicle,senv exitfromdestinationpoint.
T
is its relative information in the environment under dimension
T, sadd indicates additional information about the vehicle
T
under dimension T, and G (·) is the graph observation considers vehicles directly ahead and behind in the current
OBS
function for state information. lane. Specifically, the senv is defined as
T
Specifically,
sself =[v,y,l], (11) (cid:26) se Lnv =[κ ij(j =1,2,...,6)]
, (14)
senv =[κ (j =1,4)]
F ij
wherev isthevehicle’sspeedinthedirectionoftravel,y isits
longitudinal position along the lane, and l is the lane number wherej =1,2,...,6representthefront,leftfront,rightfront,
occupied by the vehicle. rear, left rear and right rear of the ego vehicle, respectively.
senv is the current vehicle’s safety state with respect to
T Additional information can be represented as
nearby vehicles, helping CAVs to quickly master decisions
in the safety domain. For this purpose, we adopt a vehicle (cid:26) sadd =[I]
risk assessment model based on the safety distance metric sL add =[I,A ] , (15)
between vehicles. Initially, the braking process is divided into F L
four stages to calculate the safety distance, including com-
where I indicates the type of vehicle, where I =1 represents
munication delay, braking gap, linear braking, and constant
a CAV, and otherwise, it is an HV; and A represents the
L
braking, and the braking safety distance is
output of the lane-changing dimension decision.
t v2−v2 3) Action Space: The action spaces for the lane-changing
D =(t +t )v + 3(v −v )+ e f +d, (12)
sf 1 2 e 2 e f 2a dimensionA L,aswellasforthefollowingdimensionA F,en-
max
compasslane-changingandaccelerationcommands,providing
where t is the sensor delay time during the communication
1 complete control for CAVs, where
delay,typicallyrangingfrom0.1sto0.3s,withthemaximum
value of 0.3 s taken in this paper; t is the time to eliminate (cid:26)
2 A ={a ∈[−1,0,1]},
the braking gap, with a value of 0.1 s; t is the braking time L L , (16)
3 A ={a ∈[a ,a ]}
F F dec acc
during linear braking, with a value of 0.24 s; v is the initial
e
speedofthevehicle;v isitsfinalspeed;a isthemaximum
f max where a signifies lane-changing commands, with –1, 0, 1
L
deceleration during braking; and d is the safety gap between
representing a change to the left lane, maintaining the current
two vehicles after braking to a stop, with a value of 0.6 m.
lane, and a change to the right lane, respectively; and a
F
The actual distance between two vehicles is D , and the
es signifiesaccelerationcommands,wherea denotesmaximum
acc
standard safety distance coefficient between vehicles i and j
acceleration, and a is maximum deceleration.
dec
is defined as
(cid:18) (cid:19) 4) Reward Function: After decomposing the decision sys-
D
κ ij =min Dsf −1,1 . (13) tem into its respective dimensions, the objectives of the sub-
es
decision modules for each dimension must be examined. The
This formula shows that the standard safety distance coef- reward functions for both the lane-changing and following
ficient is a dimensionless parameter ranging between –1 and dimensionsrequiredistinctexploration.Weadoptathree-lane
1. A positive risk value indicates a potential collision if both highway traffic scenario, where CAVs are set to enter from
vehicles continue under their current states. an on-ramp and exit from the next off-ramp on the same side,
Based on the proposed standard safety coefficient, taking asshowninFigure4.Forthelane-changingdimension,CAVs
a three-lane traffic scenario as an example, the lane-changing mustmakeefficientdecisionsthatalignwiththeirdestinations.
dimensiondecisionmustconsiderthestandardsafetydistance For the following dimension, CAVs are expected to make
coefficients of up to six vehicles ahead and behind in the adaptive decisions that comply with traffic rules and prioritize
currentandadjacentlanes,whilethefollowingdimensiononly comfort and energy efficiency.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 8
Initially, the reward function for the lane-changing dimen- employed constant or linear noise for exploration. However,
sion R is divided into respective sub-reward functions for for the commands issued to CAVs, substantial exploration
H
safety, task, and comfort, noiseisrequiredintheinitialstagestothoroughlyexplorethe
 Rsafe =−Cκij+1, j =2,3,5,6, environment, while reduced exploration noise in later training
 L (cid:26) f1
(L,∆y ),
stages enhances the robustness of decision-making. Linear
Rtask = 1 task (17) noise, as demonstrated in prior research, may not adapt well
L f (L,∆y ),

Rcomfort
=f2
(∆L),
speed to dynamic environments. Hence, we adopt an Exponential
H 3 Exploration mechanism for both decision dimensions.
where C is a lane-changing safety constant. κ considers Q-learningisemployedforthediscretedecisioncommands
1 ij
only the four vehicles ahead and behind in adjacent lanes. L in the lane-changing dimension, with exploration based on
denotesthelaneoccupiedbyCAVs,correspondingtothefast, a greedy algorithm. Within this framework, the multidimen-
middle, and slow lanes. ∆y is the longitudinal distance to sional DRL actions during training can be represented as an
task
the exit when in the slow lane. ∆y is the longitudinal energy sub-reward function Renergy, where
speed L
distancefromtheentrancewheninthefastlane.Thefunction
(cid:40) argmaxQ(cid:0) gL,a ;θ (cid:1) , p=(1−ε )
f T1 h( e·) fure nw cta iord ns fC 2(A ·V )s reo wn art dh se Cfa As Vt sla on ne ts hh eor stl ly owaf lt ae nr een at ser ti hn eg y. aL
t
= aL∈AL
U(A
Lt
),
L i
p=ε
t
t , (19)
approach the exit. ∆L signifies the total number of lane
changes by CAVs within a constant time period. f 3(·) is a where U(·) signifies the uniform sampling function, and ε t
linearly increasing function of ∆L. is the exploration rate at time-step t, decreasing over time
The reward function for the following dimension R
L
is and calculated based on the initial exploration rate ε 0, final
divided into respective sub-reward functions for safety, task, exploration rate ε T, and maximum exploration step T as
comfort, and energy,
 Rsafe =−Cκij+1, j =1,4, ε =ε exp(cid:34) −ln(ε εT 0) t(cid:35) . (20)
 RF
task =f
(L2
,v),
t 0 T
F 4 (18)
Rcomfort =f (a ),
 RF
energy =f
5 (E)L
,
For continuous decision commands in the following di-
F 6 mension, the Deterministic Policy Gradient (DPG) method is
where C is a following safety constant. κ considers only applied, with exploration achieved by adding noise to actions,
2 ij
thetwovehiclesdirectlyaheadandbehindinthecurrentlane,
and v is vehicle speed. The function f 4(·) rewards CAVs in aF t =CLIP(cid:0) µ ω(g tL)+N t,aF min,aF max(cid:1) , (21)
different lanes if their speed is within a specified range, and
where CLIP(·) bounds the final action within the minimum
penalizes those outside the range, and f (·) penalizes CAVs
5 aF and maximum aF values. N ∼N(0,σ(t)) represents
whose absolute value of acceleration exceeds a comfortable min max t
the noise added to the action at time-step t, and σ(t) is
range.E istheenergyconsumedbyCAVswithinatime-step.
calculated in a similar manner to Equation 20.
f (·) is a linearly increasing function of E.
6
Through the adoption of the above behavioral strategies,
interactionsbetweenCAVsandtheenvironmentgeneratetwo-
B. Parallel Asynchronous Hierarchical Graph Reinforcement dimensional tuples,
Learning
(cid:26) ζL =(s ,gL,aL,rL,s ,gL )
DRL serves as a pivotal tool to address complex problems. t t t t t t+1 t+1 , (22)
ζF =(s ,gF,aF,rF,s ,gF )
The graphical representations of spatiotemporal interactions, t t t t t t+1 t+1
hierarchical learning, and efficient exploration capabilities are which are then stored in the corresponding experience replay
methodologies suited for addressing dynamic complexities. buffers containing training data.
Decision-making for CAVs is segmented into lane-changing 2) Learning objective: For discrete decisions for the lane-
and following dimensions, which respectively issue high- changing dimension, Q-learning may overestimate the true
level commands over intervals of ∆ time-steps, and low-level value, and this overestimation is non-uniform. There are two
commands at each time-step. Considering the continuity of main reasons for this: bootstrapping leads to the propagation
specific commands, high-level commands utilize an Exponen- of biases, and maximization causes the Temporal Difference
tial Greedy Dueling Double Graph Q-Learning Network (EG- (TD)targettooverestimatethetruevalue.Therefore,weadopt
GDDQN) to output discrete lane-changing instructions [15], theDuelingnetworkandDoublealgorithm.TheDuelingNet-
[33]–[35], while low-level commands employ an Exponential work architecture, θ , includes an optimal state-value function
i
Noisy Graph Deterministic Policy Gradient (EN-GDPG) to θV and optimal advantage function θD. Hence, the gradient
generate continuous acceleration instructions. computation formula for EG-GDDQN based on AMG-MDP
1) Exploration Mechanism: DRL algorithms enhance en- is
vironmental exploration by introducing randomness into the

Q(gL,a ;θ )=V(gL;θV)+D(gL,a ;θD)
actions produced by their policies. Integrating noise into  t L i t t L
actions helps to increase exploration efficiency and enhances − mean D(g tL,a L;θD), (23)
the robustness of the algorithm. DRL studies have typically  δ =Q(gL,a ;θa )L −∈A rL L−γQ(gL ,aL ;θ−),
i t L i t t+1 t+1 iJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 9
where mean addresses the issue of non-uniqueness; in prac- Algorithm 1: PA-HGRL
tice, this has been found to perform slightly better than the input : initial F-policy parameters ω,
max operation. F-Q-function parameters φ,F-empty replay
Forthefollowingdimension’scontinuousdecisions,random buffer D , F-training batch size
F
policies, due to their action variability, can degrade CAV U ,L-Q-function parameters θ,L-empty
F
performance because the update direction of random policy replay buffer D , L-training batch size U
L L
parameters may not align with the optimal direction of policy 1 Set target parameters equal to main parameters
gradients. Unlike random policies that integrate action and ω ←ω,φ ←φ,θ ←θ;
targ targ targ
state spaces, deterministic policies only integrate the state
space,meaningthatforagivenstateandparameters,onlyone 2 for episode∈{0,1,...,M} do
specific action is output. The Actor gradient ∇ J and loss 3 Based on Equation (10), receive initial
ωµ
observation state s , graph observation states
function for value L(φ,D ) for EN-GDPG based on AMG- 0
F gL and gF
MDP are: 0 0
∇ J ≈E (cid:2) ∇ Q(cid:0) g ,a|ωQ(cid:1) |g =gL, 4 for t ∈{0,1,...,T} do
ωµ g tL∼ρβ ωµ L L t 5 for t ∈{0,5,...,⌊T/∆⌋} do
=a
E
g= tL∼µ ρ(cid:0) βg (cid:2)tL ∇| aω Qµ (cid:0)(cid:1) g(cid:3)
L,a|ωQ(cid:1) |g
L
=g tL,
(24)
6 B
O
ga Fs be td ainon reE wq au ra dti ro tFn ,(2 n1 ew), s se tale tect sa t+ct 1io an na dF t
a=µ(cid:0)
g
tL(cid:1)
·∇ ωµµ(g L
|ωµ)(cid:3)
7
Adt d+1
tuple (s t,g tF,aF t ,r tF,s t+1,g tF +1)
into D
L(φ,D )= E [(Q (g ,a )− F
F
(cid:32)
(st, gF t,aF t,r tF,st+1, gF t+1)∼DF φ (cid:33)L (cid:33)2F

8
9
S Ua pm dp al te
e
U FF -vatu lup ele ns ef tr wo om rkD iF
n Eqn. (25)
r tF +1+γ(1−d)maxQ φ(cid:0) g tF +1,aF t+1(cid:1) . 1 10
1
U Up pd da at te
e
F F- -a tac rt go er tn ne et tw wo or rk ksin Eqn. (24)
aL
t+1 ω ←ω,φ ←φ
targ targ
(25)
3) Integration: PAH-GRL, through the integration of the
12 Update graph observation states g 0L in
Eqn. (15)
above mechanisms, leverages multilevel graph representation
Theory, and is described as Algorithm 1. Given the extensive 13 end
body of research, this paper omits discussion of DRL algo- 14 Based on Eqn. (19), select action aL t
rithms. 15 Obtain reward r tL, new state s t+1 and g tL +1
16 Add tuple (s t,g tL,aL t,r tL,s t+1,g tL +1) into
D
C. Multilevel Multi-head Graph Attention Network Module L
17 Sample U L tuples from D L
Theconstructionofthegraphnetworkmodelmustbecom-
18 Update L-value network in Eqn. (23)
patible with the matrix of the previously proposed multilevel
19 Update L-target value network θ targ ←θ
graph representation. Thus, a Multilevel Multi-head Graph
AttentionNetwork(ML-MGAT)isintroducedtobetterhandle 20 end
feature information across different dimensions. 21 end
Specifically,foratargetnodev,N representsallneighbor-
u
ing nodes of u. For each pair of nodes (u,v), the normalized
attentioncoefficientek iscomputedonlywhena isnonzero
ij uv
(i.e., there exists an edge between nodes u and v), ensuring
thatattentioncoefficientsarecalculatedonlybetweenactually
where σ is a nonlinear activation function, and K is the
connected nodes [36]. For dimension T, the revised formula
number of heads.
for the normalized attention coefficients is
ML-MGAT allows the model to more finely consider the
exp(cid:0) LeakyReLU(cid:0) aT,T ·(cid:2) WThTWThT(cid:3)(cid:1) ·aT (cid:1)
αT = u v uv ,relationships between nodes, such as by adjusting the alloca-
u,v (cid:80) exp(LeakyReLU(aT,T ·[WThTWThT])·aT ) tion of attention based on factors like relative distance and
u v uv
u∈N uT speed between vehicles, so the multi-head graph attention
(26)
network can more precisely capture the dynamic changes and
whereWT isalearnableprojectionmatrix,aT,T isatrainable
multidimensionalinteractionsinthetrafficnetwork,providing
vector indicating the direction of attention, and [·||·] denotes
deeperandmoredetailedinformationsupportforautonomous
vector concatenation.
driving decisions. The policy network in the Dueling network
A multi-head attention mechanism is introduced, where
should be replaced with state value and action advantage sub-
heads can learn different information,
networks. The value network in EN-GDPG has a structure
  similar to the policy network, except that the action variable
K
(cid:88) (cid:88)
hT
u
= σ α uT ,vWThT v, (27) is added as an input, and the output is replaced by Q-values.
Wesetthevaluenetworktobewiderthanthepolicynetwork.
k=1 v∈NT
uJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 10
TableI TableII
EXPERIMENTALPLATFORMPARAMETERSETTINGS. COMPARISONOFALGORITHMS.
Parameter Value Algorithm L-dimension H-dimension Network Graph
Operatingsystem Ubuntu20.04 D3QN D3QN DNN None
Language Python3.7.3 PAH-DNN EG-D3QN EN-DDPG DNN None
CUDAversion CUDA11.3 PAH-GCN EG-GDDQN EN-GDPG GCN Basic
PyTorch v1.14.0 PAH-GAT EG-GDDQN EN-GDPG MGAT Basic
torch-geometric v2.0.4 PAH-MLGCN EG-GDDQN EN-GDPG GCN Multi-level
Optimizer Adam PAH-MLGAT EG-GDDQN EN-GDPG MGAT Multi-level
Nonlinearity ReLU
Numberofvehicles,n 50
NumberofCAVs,m 5 TableIII
lane0:360vehicles/h HYPERPARAMETERSETTINGS.
TrafficdensityofHVs lane1:720vehicles/h
lane2:720vehicles/h Parameter L-dimension F-dimension
lane0:12m/s
InitialvelocityofHVs lane1:18m/s Learningrate 0.00075 Critic:0.0001
lane2:22m/s Actor:0.000025
InitialvelocityofCAVs 15m/s Time-step 0.5s 0.1s
Longitudinalspeedlimitation 35m/s Startingexplorationvalue 0.6 0.5
Lengthofvehicle 5m Terminatingexplorationvalue 0.02 0.05
Continuousexplorationtime-step 140000 700000
Minibatchsize 64 64
Discountrate 0.99
V. EXPERIMENTS
Time-stepofcurrentnetworkupdate 100 200
We evaluated the performance of the proposed spatiotem- Time-stepoftargetnetworkupdate 400 800
Softupdaterate 0.01 0.005
poral interaction-enhanced decision-making framework. We
Numberofheads,K 3 3
introducetheexperimentalplatform,explaintheparametersof
the designed heterogeneous mixed autonomy traffic scenario,
and describe the baseline methods and testing metrics. choosing the appropriate opportunity to exit. We established
the following rules according to actual traffic scenarios and
A. Experimental Platform computational costs:
Simulations were conducted on a computer equipped with (a) Each episode was limited to 150 seconds, with five CAVs
a standard keyboard. The hardware supporting real-time com- randomly appearing within 50 seconds, ensuring their
putations included an Intel Core i7-11800H CPU @ 2.30 interaction;
GHz, with 16 GB RAM, and an NVIDIA GeForce RTX 3090 (b) HVs (grey vehicles) appeared randomly on roads, over-
GPU.Flow[37]wasutilized,withalgorithmsandscriptspro- lapping CAVs’ routes with a certain traffic density;
grammed in Python. Neural network models were constructed (c) CAVs could continue to drive on the right main road, but
using the PyTorch framework. The hyperparameters related to this was considered a mission failure;
this study are shown in Table I. (d) CAVs could always drive on the slow lane and exit, but
this strategy was considered as low-scoring;
B. Simulation Scenario Setup (e) Lanes had different speed ranges, and speeding or driving
too slowly were considered to be violations;
High-density, high-randomness, and high-dynamic highway
(f) All vehicles had to strictly adhere to traffic rules.
trafficscenariosaretypicalofheterogeneousmixedautonomy.
TheprimarychallengesCAVsfaceinoperatingefficientlyand The above rules were established to closely adapt to real
safely include: 1) minimizing disruption to overall traffic flow traffic environments. HVs on different lanes pose traffic flow
when entering and exiting the highway; 2) making complex challenges to CAVs, which must choose areas with low
multi-objective decisions while sharing the road with HVs, traffic flow density for lane-changing and speed adjustment,
considering the surrounding vehicles’ speed and position, minimizingtheimpactonHVs.Scenarioparametersaregiven
trafficflowdensity,potentialconflicts,passengercomfort,and in Table I.
energyconsumptionrequirements;3)dynamicandcontinuous
spatiotemporal propagation of decisions made by CAVs; e.g.,
C. Baseline Methods
current driving behaviors can be transmitted from HVs to
subsequent CAVs. We employed EN-GDPG, EG-GDDQN, GCN, MGAT, and
We designed a 1-kilometer, three-lane highway section, other methods, embedded in the proposed framework for
incorporating practical tasks and requirements, as shown in comparison through ablation and contrast experiments. Six
Figure 4. Colored vehicles represent autonomous cars, which algorithms are shown in Table II, whose hyperparameters are
entered the scenario from the left junction and exited from given in Table III. We emphasize several points:
the right junction. In this interactive scenario, CAVs had to (a) The basic graph representation theory (section III-A) is
execute four logical tasks: choosing the right time to merge considered a baseline comparison, and multilevel graph
onto the main road, lane-changing to the fast lane and accel- representation theory (section III-B2) is used for further
erating, lane-changing to the slow lane and decelerating, and comparison;JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 11
2 0 0 0
1 0 0 0
0
P A H -M L G A T P A H -M L G C N
P A H -G A T P A H -G C N
0 2 0 0 4 0 0 6 0 0 8 0 0 1 0 0 0
E p iso d e
(a) Lane-changedimension.
-1 6 0 0
-2 0 0 0
Figure 6. Task success rate of CAVs of different GRL methods: (a) PAH-
MLGAT; (b) PAH-MLGCN; (c) PAH-GAT; (d) PAH-GCN. Error bar refers
tostandarddeviation.
-2 4 0 0
160
P A H -M L G A T (a) PA H -M LG A T (b) PA H -M LG C N
P A H -M L G C N
-2 8 0 0 P A H -G A T 140
P A H -G C N
120
0 2 0 0 4 0 0 6 0 0 8 0 0 1 0 0 0
E p iso d e
100
(b) Followingdimension.
80
Figure5. Variationsinrewardsoftwodecision-makingdimensionsover1000
episodesinablationtrainingexperiments.
160
(c) PA H -G A T (d) PA H -G C N
(b) An ablation experiment training results graph is provided 140
for the last four algorithms in Table II. All algorithms
120
were trained with five different random seeds, and results
compared using mean and variance;
100
(c) All algorithms were compared in tests. Each model was
tested 10 times, and all the test indicators are average 80
values.
0 500 1000 0 500 1000
E pisode E pisode
Reward values, exit rates, emergency braking incidents,
task time, and energy consumption were used to assess the Figure 7. Traveling time of CAVs of different GRL methods: (a) PAH-
performance, driving efficiency, driving comfort, and energy MLGAT; (b) PAH-MLGCN; (c) PAH-GAT; (d) PAH-GCN. Error bar refers
tostandarddeviation.
efficiency of the all algorithms in Table II. A trajectory
analysis was conducted based on the medians of the test
rewards.
A. Ablation Training Experiments
Figure 5 displays the reward values during each train-
VI. RESULTS ing iteration of the four algorithms in ablation experiments.
WithintheproposedPAH-GRLframework,boththeproposed
Weanalyzethetrainingprocessoftheablationexperiments. multilevel graph representation and the corresponding MGAT
Also, we conducted a comparative test experiment involving exhibited superior performance compared to the traditional
six algorithms, followed by a comprehensive comparison. Fi- GCN algorithm. PAH-GCN showed continually expanding
nally,wepresentananalysisofthespatiotemporaltrajectories error during training, indicating that the basic graph represen-
of the optimal model. tation struggles to precisely represent the interactive behavior
s
dr
a
w
e
R-
L
s
dr
a
w
e
R-
F
)s(
e
mi
T
gnilevar
T
)s(
e
mi
T
gnilevar
TJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 12
and interaction topologies, enhancing traffic efficiency and
decision robustness while reducing energy consumption.
B. Comparative Test Experiment
Table IV shows that PAH-MLGAT achieved the best over-
all performance and robustness across six testing metrics
in heterogeneous mixed autonomy. This demonstrates that
PAH-MLGAT effectively couples multilevel graph representa-
tion, PAH-GRL, and multi-head graph attention mechanisms,
significantly enhancing the comprehensive performance and
robustnessofCAVs’decisions.Incontrast,D3QN,whichout-
putsbothlane-changingandaccelerationcommands,struggled
tofacilitatenormaldrivinginheterogeneousmixedautonomy,
andexhibitedhighvarianceinperformancemetrics.Theintro-
ductionofParallelAsynchronousHierarchicalandbasicgraph
Figure 8. Energy consumption of ablation study. Quantities are energy representation theory gradually enabled CAVs to achieve set
expendituresofCAVsduringtrainingphaseofproposedalgorithm.
driving goals. However, although PAH-GCN and PAH-GAT
reduced the time consumed per round, they failed to master
the decision strategy for exiting the highway and conserving
of heterogeneous mixed autonomy. The introduction of the
energy. Furthermore, multilevel graph representation aided
multilevel graph representation module resulted in a smoother
CAVsineffectivelycapturingthespatiotemporaldependencies
increase in the reward curve during training, suggesting ef-
of scenarios, leading to higher driving efficiency, comfort,
fective extraction of scenario features, enhancing the learning
energy conservation, and robustness.
capabilitiesofCAVs.MGATbasedonmultilevelgraphrepre-
sentation also displayed higher rewards, indicating improved
explorationabilitiesforCAVs.PAH-MLGATdemonstratedthe C. Analysis of spatiotemporal trajectories
optimal convergence speed and highest exploration rewards Utilizing the median of the test rewards as a basis, a set of
in both decision dimensions. Importantly, even with lane- spatiotemporal trajectory diagrams were selected to evaluate
changingdimensionoutputsservingasinputsforthefollowing the decision-making of PAH-MLGAT, PAH-GAT, and PAH-
dimension, rewards in both dimensions continued to rise DNN.InFigure9,thez-axisshowstime;thex-axisrepresents
steadily. This validates the robustness and stability of the lanes0to2,correspondingtotheslow,middle,andfastlanes;
proposed PAH-GRL framework. andthebaseplanemapsoutthespatialmovementtrajectories.
Figures 6-8 respectively illustrate the task success rate, A preliminary examination focuses on task completion rates.
total driving time, and energy consumption for each training As depicted in Figure 9(a), CAVs trained via PAH-MLGAT
iteration of the four algorithms in ablation experiments. As predominantly succeeded in their tasks, despite the third
showninFigure6,thetasksuccessratesofthefouralgorithms CAV’s failure to merge into the fast lane through traffic. This
align with the trend of reward values in the lane-changing indicates that PAH-MLGAT substantially explored all logical
dimension, indicating that PAH-MLGAT better assesses the objectives, adeptly capturing the spatiotemporal attributes of
motivation for lane changes. However, the final success rate the traffic flow. In Figure 9(b), only three CAVs of PAH-
did not meet expectations due to the residual randomness in GAT managed to complete their tasks, with only the second
the greedy algorithm used in the lane-changing dimension CAV successfully navigating the fast lane. This suggests that
toward the end of training. A more precise analysis of the PAH-GAT identified objectives associated with utilizing the
task success rate can be obtained in the test experiments fast lane and ramp exit, but fell short in pinpointing suitable
if greedy actions are omitted. As shown in Figure 7, there spatiotemporal openings in traffic. Figure 9(c) demonstrates
was no significant difference in the total time per episode thatPAH-DNN’sCAVsfailedtofulfilltheirtasks,highlighting
among the four algorithms, as only the exit of the last CAV its inclination toward immediate fast-lane rewards without
concluded the episode, making it difficult to reflect overall recognizing the rewards associated with distant destinations,
driving efficiency. Therefore, we analyzed the variance in thereby exhibiting a myopic aspect.
driving time, and supplemented it with a CAV trajectory We conducted a trajectory analysis of CAVs before and
analysis. In the figure, PAH-GCN exhibited large fluctuations after a lane change. In the 2D diagrams, a diminished slope
in driving time, while PAH-GAT had shorter times. How- signifies an ability to traverse longer distances in shorter
ever, trajectory analysis revealed frequent speed changes by time spans. As illustrated in Figure 9(d), upon entry from
CAVs in the slow lane in both cases, reducing mission time. the ramp into the slow lane, the slope for PAH-MLGAT’s
From the energy analysis in Figure 8, PAH-GCN and PAH- CAVs lessens, signaling acceleration initiation. Notably, the
GAT experienced frequent acceleration changes, increasing snugtrajectoryalignmentofthethirdCAVwiththefollowing
energy consumption. A comprehensive analysis of these three vehicle indicates its mastery over the lane’s optimal speed.
trainingindicatorsshowsthatPAH-MLGAT-trainedCAVscan TheabbreviatedtrajectoriesoftheremainingfourCAVsinthe
effectively aggregate other vehicles’ driving characteristics middle lane imply precise acquisition of adjacent lane trafficJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 13
TableIV
TESTINGEVALUATIONRESULTS.
Algorithms psuccess N braking T travel Energy L-Reward F-Reward
µ(s) σ µ σ µ σ µ(J) σ µ σ µ σ
D3QN 10.40% 0.098 7.560 1.215 1282.51 152.03 1.577 0.211 - - - -
PAH-DNN 13.60% 0.135 5.920 1.383 1179.60 134.60 1.301 0.116 0.3842 0.1591 0.3155 0.1356
PAH-GCN 20.80% 0.132 5.160 0.967 1004.28 131.84 1.198 0.1629 0.4689 0.1338 0.3558 0.1406
Imp.↑ 52.94% 2.22% 12.84% 30.08% 14.96% 2.05% 7.92% -40.43% 22.04% 15.90% 12.77% -3.69%
PAH-GAT 54.40% 0.223 3.360 0.889 951.96 129.46 1.219 0.152 0.7294 0.1043 0.5011 0.1272
Imp.↑ 300.00% -65.18% 43.24% 35.72% 19.30% 3.82% 6.30% -31.03% 89.85% 34.44% 58.82% 6.20%
PAH-MLGCN 74.40% 0.174 2.320 0.733 971.72 114.99 1.137 0.1241 0.8178 0.0656 0.6630 0.1323
Imp.↑ 447.06% -28.89% 60.81% 47.00% 17.62% 14.57% 12.61% -6.98% 112.86% 58.77% 110.14% 2.43%
PAH-MLGAT 95.20% 0.085 1.320 0.835 960.52 103.24 1.0751 0.0821 0.9063 0.0376 0.7963 0.0910
Imp.↑ 600.00% 37.03% 77.70% 39.62% 18.57% 23.30% 17.36% 29.22% 135.89% 76.37% 152.39% 32.89%
ImprovementsforeachalgorithmcomparedwithPAH-DNNareshownasImp.Bestresultsareinboldface;second-bestareunderlined.
100 100 120
80 80 100
80
T im60 T im60 T
im
e
(s)
40
e
(s)
40
e (s)60
40
20 0
Lane1 2 0
200400600 P8 os0 itio0 n1 (m0 )00 20 0
Lane1 2 0
200400600 P8 osi0 tio0 n1 (m0 )00 20 0
Lane1 2 0
200400600 P8 os0 iti0 o1 n (0 m)00
(a) 3D spatiotemporal trajectory test diagram of (b) 3D spatiotemporal trajectory test diagram of (c) 3D spatiotemporal trajectory test diagram of
PAH-MLGAT. PAH-GAT. PAH-DNN.
100 100 120
75 75 90
50 50 60
25 25 30
0 0 0
0 500 1000 0 500 1000 0 500 1000 0 500 1000 0 500 1000 0 500 1000 0 500 1000 0 500 1000 0 500 1000
Position (m ) Position (m ) Position (m ) Position (m ) Position (m ) Position (m ) Position (m ) Position (m ) Position (m )
(d) Lane-based spatiotemporal trajectory test dia- (e) Lane-based spatiotemporal trajectory test dia- (f) Lane-based spatiotemporal trajectory test dia-
gramofPAH-MLGAT. gramofPAH-GAT. gramofPAH-DNN.
Figure 9. Spatiotemporal trajectory test diagram of five CAVs trained by three algorithms. (a)-(c) 3D spatiotemporal trajectories; (d)-(f) planar trajectories
acrosslanes0to2.TrajectoriesofHuman-drivenVehicles(HVs)areingray,whilethoseofCAVstrainedbyPAH-MLGAT,PAH-GAT,andPAH-DNNare
respectivelyshowninblue,orange,andgreen.
flowinformation.AsdepictedinFigure9(e),thefirstandfifth mimic human abilities for divided attention and cognitive
CAVs of PAH-GAT are ensnared between the traffic flows of consistence. Central to this approach is the multilevel graph
the middle and fast lanes, and are prevented from changing representation theory, which addresses dynamic spatial cor-
lanes. This shows their challenges in balancing lane-changing relations and nonlinear temporal interactions peculiar to non-
andfollowingdecisions.DisplayedinFigure9(f),PAH-DNN’s Euclideanenvironments.Weproposedaparallelasynchronous
CAVs exhibit continuously increasing trajectory slopes, with hierarchical graph reinforcement learning framework to en-
a corresponding decrease in speed, signifying the model’s hance the autonomous learning and iterative refinement of
inabilitytomakeviablefollowingdecisionspost-stratification. CAVs, facilitating simultaneous strategy development across
PAH-MLGAT demonstrates an ability to undertake col- multiple dimensions despite limited attention capacities. This
laborative decisions for lane changing and speed adjustment promotescross-dimensionalcooperationamongCAVs,leading
in dense traffic, adeptly choosing unoccupied spatiotemporal to decision-making processes that resemble human cognitive
regions. This corroborates the proposed framework’s capac- functions. To validate our approach, we conducted ablation
ity for synchronized, human-like cognitive decision-making studiesandcomparativeanalysesinasimulatedheterogeneous
across both lane-changing and following dimensions. mixedautonomyenvironment,emphasizingspatiotemporalin-
teractions.SimulationresultsindicatedthatourPAH-MLGAT,
VII. CONCLUSION integratingmultilevelgraphrepresentationwithPAH-GRLand
multi-headgraphattentionmechanisms,substantiallyenhances
To enable CAVs to better capture spatiotemporal depen-
the decision-making performance and robustness of CAVs.
dencies in non-Euclidean spaces within heterogeneous mixed
Compared to PAH-DNN, our approach resulted in a sixfold
autonomy, we introduced a decision-making strategy with
increase in task success rate, a 77.70% reduction in braking
spatiotemporal interaction capabilities, which is designed to
)s(
e
miT
)s(
e
miT
)s(
e
miTJOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 14
instances, an 18.57% decrease in driving time, a 17.36% where ∆d is the spatial distance between nodes i and j, x
ij i
reduction in energy consumption, and a 39.74% reduction in andx aretheirrespectivefeaturevectors,andf isamapping
j
average variance across six metrics. function that translates these inputs into a weight value in the
For future research, it is suggested to refine the study interval [0,1].
of the hierarchical decision-making framework. Moreover, In a weighted graph, the weighted degree of node i can be
it is imperative to further explore graph network models, defined as
particularlythosebasedontrafficgraphrepresentations.From dw(i)= (cid:88) at , (33)
t ij
an experimental perspective, the development of a micro-
j∈Vt
trafficsandboxenvironmentisadvocated,whichwouldenable
hardware-in-the-loop experiments. reflecting the total interaction strength of node i with other
nodes.
When node u disappears, for any remaining node v, the
APPENDIXA
changeinitsweighteddegreeisnolongerasimplesubtraction
PROOFOFDEGREEDISTRIBUTIONIMBALANCE
of 1, but depends on the value of at ,
Agraphattime-steptisconsideredasG =(V ,E ),where uv
t t t
the disappearance of node u leads to the removal of all its dw (v)=dw(v)−at . (34)
t+1 t uv
connected edges. The adjacency matrix A represents graph
t
G , where at = 1 if nodes i and j are connected at time t, The weighted degrees of all nodes not directly connected to
t ij
and is 0 otherwise. u remain unchanged.
The degree of node u, d (u), is defined as the number of Sincetheweightat reflectsthecombinedimpactofspatial
t ij
elementswithavalueof1intherow(orcolumn,asthegraph distance and node features, as shown in equations 8 and 9,
isundirected)correspondingtonodeuintheadjacencymatrix, when the distance threshold (X,Y)→+∞,
(cid:88)
d (u)= at . (28) lim f(∆d ,x ,x )=0, (35)
t uj ij i j
j∈Vt
∆dij→+∞
At time t+1, node u and its associated edges are removed, which implies that even with node disappearance, the degree
and hence at+1 = 0 and at+1 = 0 for all j. The degree of distribution of the graph can adjust more smoothly, reducing
uj ju
any node v ̸=u that was a neighbor of u becomes abrupt changes.
Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
d (v)=d (v)−1. (29)
t+1 t
Thedegreesofallnodesdirectlyconnectedtoudecreaseby REFERENCES
1, leading to a change in the degree distribution. In particular,
the total number of edges |E |=|E |−d (u). [1] J. Baruch, J (Baruch, “Steer driverless cars towards full automation,”
t+1 t t Nature,pp.127–127,2016.
Thedegreedistributioncanberepresentedthroughadegree
[2] S.S.Avedisov,G.Bansal,andG.Orosz,“Impactsofconnectedauto-
sequence or degree distribution function. Let P(k;t) be the matedvehiclesonfreewaytrafficpatternsatdifferentpenetrationlevels,”
proportion of nodes with degree k in the graph at time t. IEEETransactionsonIntelligentTransportationSystems,vol.23,no.5,
pp.4305–4318,2022.
After the disappearance of node u, the proportions of nodes
[3] S.Liu,L.Liu,J.Tang,B.Yu,Y.Wang,andW.Shi,“Edgecomputing
with degrees k and k−1 will change. Specifically, for each forautonomousdriving:Opportunitiesandchallenges,”Proceedingsof
node connected to u, theIEEE,vol.107,no.8,pp.1697–1716,2019.
[4] M. M. Botvinick, Y. Niv, and A. G. Barto, “Hierarchically
P(k;t+1)=P(k;t)−∆P(k;t), (30) organized behavior and its neural foundations: A reinforcement
learning perspective,” Cognition, vol. 113, no. 3, pp. 262–280, 2009,
reinforcement learning and higher cognition. [Online]. Available:
where ∆P(k;t) is the decrease in the proportion of nodes
https://www.sciencedirect.com/science/article/pii/S0010027708002059
with degree k due to node disappearance, and the proportion [5] J. Wu, Y. Zhou, H. Yang, Z. Huang, and C. Lv, “Human-guided
of nodes with degree k−1 will increase accordingly: reinforcementlearningwithsim-to-realtransferforautonomousnaviga-
tion,”IEEETransactionsonPatternAnalysisandMachineIntelligence,
P(k−1;t+1)=P(k−1;t)+∆P(k;t). (31) vol.45,no.12,pp.14745–14759,2023.
[6] M. Eppe, C. Gumbsch, M. Kerzel, P. D. Nguyen, M. V. Butz, and
Since this change is concentrated on nodes directly connected S. Wermter, “Intelligent problem-solving as integrated hierarchical re-
inforcement learning,” Nature Machine Intelligence, vol. 4, no. 1, pp.
to u, it causes a sudden and significant shift in the graph’s
11–20,2022.
degree distribution, causing an imbalance. [7] X.Li,X.Ying,andM.C.Chuah,“Grip:Graph-basedinteraction-aware
trajectoryprediction,”in2019IEEEIntelligentTransportationSystems
Conference(ITSC),2019,pp.3960–3966.
APPENDIXB [8] Y. Peng, G. Tan, H. Si, and J. Li, “Drl-gat-sa: Deep reinforcement
PROOFOFTHEOREM1. learning for autonomous driving planning based on graph attention
networks and simplex architecture,” Journal of Systems Architecture,
Define the weighted adjacency matrix A t ∈ [0,1]N×N, vol.126,p.102505,2022.
where each element at represents the interaction strength [9] J.Dong,S.Chen,P.Y.J.Ha,Y.Li,andS.Labi,“Adrl-basedmultiagent
ij
cooperativecontrolframeworkforcavnetworks:agraphicconvolution
betweennodesiandj attimet,whichcanbebasedonspatial
qnetwork,”arXivpreprintarXiv:2010.05437,2020.
distance and node features, as [10] P. Hart and A. Knoll, “Graph neural networks and reinforcement
learning for behavior generation in semantic environments,” in 2020
at
ij
=f(∆d ij,x i,x j), (32) IEEEIntelligentVehiclesSymposium(IV),2020,pp.1589–1594.JOURNALOFLATEXCLASSFILES,VOL.14,NO.8,AUGUST2021 15
[11] S. Chen, J. Dong, P. Ha, Y. Li, and S. Labi, “Graph neural [28] P.-L.Bacon,J.Harb,andD.Precup,“Theoption-criticarchitecture,”in
network and reinforcement learning for multi-agent cooperative Proceedings of the AAAI conference on artificial intelligence, vol. 31,
controlofconnectedautonomousvehicles,”Computer-AidedCiviland no.1,2017.
Infrastructure Engineering, vol. 36, no. 7, p. 838 – 857, 2021, cited [29] J. Duan, S. Eben Li, Y. Guan, Q. Sun, and B. Cheng, “Hierarchical
by:93.[Online].Available:https://www.scopus.com/inward/record.uri? reinforcementlearningforself-drivingdecision-makingwithoutreliance
eid=2-s2.0-85107485926&doi=10.1111%2fmice.12702&partnerID= on labelled driving data,” IET Intelligent Transport Systems, vol. 14,
40&md5=0f692dc7cac34bea77f09ba056b29b9c no. 5, pp. 297–305, 2020. [Online]. Available: https://ietresearch.
[12] X.Hu,Y.Liu,B.Tang,J.Yan,andL.Chen,“Learningdynamicgraph onlinelibrary.wiley.com/doi/abs/10.1049/iet-its.2019.0317
for overtaking strategy in autonomous driving,” IEEE Transactions on [30] J. Peng, S. Zhang, Y. Zhou, and Z. Li, “An integrated model for
Intelligent Transportation Systems, vol. 24, no. 11, pp. 11921–11933, autonomousspeedandlanechangedecision-makingbasedondeepre-
2023. inforcementlearning,”IEEETransactionsonIntelligentTransportation
[13] C.Yu,X.Wang,X.Xu,M.Zhang,H.Ge,J.Ren,L.Sun,B.Chen,and Systems,vol.23,no.11,pp.21848–21860,2022.
G. Tan, “Distributed multiagent coordinated learning for autonomous [31] S. Rahmani, A. Baghbani, N. Bouguila, and Z. Patterson, “Graph
driving in highways based on dynamic coordination graphs,” IEEE neuralnetworksforintelligenttransportationsystems:Asurvey,”IEEE
Transactions on Intelligent Transportation Systems, vol. 21, no. 2, pp. Transactions on Intelligent Transportation Systems, vol. 24, no. 8, pp.
735–748,2020. 8846–8885,2023.
[14] X. Gao, X. Li, Q. Liu, Z. Ma, T. Luan, F. Yang, and Z. Li, [32] Q.Liu,Z.Li,X.Li,J.Wu,andS.Yuan,“Graphconvolution-baseddeep
“Rate gqn: A deviations-reduced decision-making strategy for reinforcement learning for multi-agent decision-making in interactive
connected and automated vehicles in mixed autonomy,” IEEE traffic scenarios,” in 2022 IEEE 25th International Conference on
Transactions on Intelligent Transportation Systems, p. 1–13, 2023, IntelligentTransportationSystems(ITSC). IEEE,2022,pp.4074–4081.
citedby:0.[Online].Available:https://www.scopus.com/inward/record. [33] Z.Wang,T.Schaul,M.Hessel,H.Hasselt,M.Lanctot,andN.Freitas,
uri?eid=2-s2.0-85173064758&doi=10.1109%2fTITS.2023.3312951& “Dueling network architectures for deep reinforcement learning,” in
partnerID=40&md5=068d721b43f7a18c32a62f3e8a62ba83 Internationalconferenceonmachinelearning. PMLR,2016,pp.1995–
[15] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. 2003.
Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski [34] H.VanHasselt,A.Guez,andD.Silver,“Deepreinforcementlearning
et al., “Human-level control through deep reinforcement learning,” with double q-learning,” in Proceedings of the AAAI conference on
nature,vol.518,no.7540,pp.529–533,2015. artificialintelligence,vol.30,no.1,2016,pp.2094–2100.
[16] J. Wu, Y. Zhou, H. Yang, Z. Huang, and C. Lv, “Human-guided rein- [35] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
forcementlearningwithsim-to-realtransferforautonomousnavigation,” D.Silver,andD.Wierstra,“Continuouscontrolwithdeepreinforcement
IEEETransactionsonPatternAnalysisandMachineIntelligence,2023. learning,”arXivpreprintarXiv:1509.02971,2015.
[17] X.He,J.Wu,Z.Huang,Z.Hu,J.Wang,A.Sangiovanni-Vincentelli,and [36] P. Velicˇkovic´, G. Cucurull, A. Casanova, A. Romero, P. Lio`, and
C.Lv,“Fear-neuro-inspiredreinforcementlearningforsafeautonomous Y.Bengio,“Graphattentionnetworks,”2018.
driving,” IEEE transactions on pattern analysis and machine intelli- [37] C. Wu, A. R. Kreidieh, K. Parvate, E. Vinitsky, and A. M. Bayen,
gence,2023. “Flow: A modular learning framework for mixed autonomy traffic,”
[18] L.SchesterandL.E.Ortiz,“Automateddrivinghighwaytrafficmerging IEEE Transactions on Robotics, vol. 38, no. 2, p. 1270–1286, Apr.
usingdeepmulti-agentreinforcementlearningincontinuousstate-action 2022.[Online].Available:http://dx.doi.org/10.1109/TRO.2021.3087314
spaces,” in 2021 IEEE Intelligent Vehicles Symposium (IV), 2021, pp.
280–287.
[19] D. Chen, M. R. Hajidavalloo, Z. Li, K. Chen, Y. Wang, L. Jiang,
and Y. Wang, “Deep multi-agent reinforcement learning for highway
on-ramp merging in mixed traffic,” IEEE Transactions on Intelligent
TransportationSystems,vol.24,no.11,pp.11623–11638,2023.
[20] D. Kamran, C. F. Lopez, M. Lauer, and C. Stiller, “Risk-aware high-
level decisions for automated driving at occluded intersections with
reinforcement learning,” in 2020 IEEE Intelligent Vehicles Symposium
(IV),2020,pp.1205–1212.
[21] Z.Qiao,K.Muelling,J.Dolan,P.Palanisamy,andP.Mudalige,“Pomdp
and hierarchical options mdp with continuous actions for autonomous
driving at intersections,” in 2018 21st International Conference on
IntelligentTransportationSystems(ITSC),2018,pp.2377–2382.
[22] Z. Bai, P. Hao, W. ShangGuan, B. Cai, and M. J. Barth, “Hybrid
reinforcement learning-based eco-driving strategy for connected and
automated vehicles at signalized intersections,” IEEE Transactions on
Intelligent Transportation Systems, vol. 23, no. 9, pp. 15850–15863,
2022.
[23] J.Bernhard,S.Pollok,andA.Knoll,“Addressinginherentuncertainty:
Risk-sensitive behavior generation for automated driving using dis-
tributional reinforcement learning,” in 2019 IEEE Intelligent Vehicles
Symposium(IV),2019,pp.2148–2155.
[24] G. Li, Y. Yang, S. Li, X. Qu, N. Lyu, and S. E. Li, “Decision
making of autonomous vehicles in lane change scenarios: Deep
reinforcementlearningapproacheswithriskawareness,”Transportation
Research Part C: Emerging Technologies, vol. 134, p. 103452, 2022.
[Online]. Available: https://www.sciencedirect.com/science/article/pii/
S0968090X21004411
[25] C.Xu,W.Zhao,J.Liu,C.Wang,andC.Lv,“Anintegrateddecision-
making framework for highway autonomous driving using combined
learning and rule-based algorithm,” IEEE Transactions on Vehicular
Technology,vol.71,no.4,pp.3621–3632,2022.
[26] S. Kai, B. Wang, D. Chen, J. Hao, H. Zhang, and W. Liu, “A
multi-taskreinforcementlearningapproachfornavigatingunsignalized
intersections,”in2020IEEEIntelligentVehiclesSymposium(IV),2020,
pp.1583–1588.
[27] B. Gangopadhyay, H. Soora, and P. Dasgupta, “Hierarchical program-
triggered reinforcement learning agents for automated driving,” IEEE
Transactions on Intelligent Transportation Systems, vol. 23, no. 8, pp.
10902–10911,2022.