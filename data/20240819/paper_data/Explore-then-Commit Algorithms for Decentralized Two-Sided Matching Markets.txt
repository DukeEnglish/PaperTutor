Explore-then-Commit Algorithms for
Decentralized Two-Sided Matching Markets
Tejas Pagare tejaspagare2002@gmail.com
Indian Institute of Technology Bombay
Avishek Ghosh avishek_ghosh@iitb.ac.in
Indian Institute of Technology Bombay
Abstract
Onlinelearninginadecentralizedtwo-sidedmatchingmarkets,wherethedemand-side(play-
ers)competetomatchwiththesupply-side(arms),hasreceivedsubstantialinterestbecause
itabstractsoutthecomplexinteractionsinmatchingplatforms(e.g. UpWork,TaskRabbit).
However,pastworksLiuetal.(2020;2021);Sankararamanetal.(2021);Basuetal.(2021);
Kong & Li (2023) assume that each arm knows their preference ranking over the players
(one-sided learning), and each player aim to learn the preference over arms through succes-
sive interactions. Moreover, several (impractical) assumptions on the problem are usually
madefortheoreticaltractabilitysuchasbroadcastplayer-armmatchLiuetal.(2020;2021);
Kong & Li (2023) or serial dictatorship Sankararaman et al. (2021); Basu et al. (2021);
Ghosh et al. (2022). In this paper, we study a decentralized two-sided matching market,
where we do not assume that the preference ranking over players are known to the arms
apriori. Furthermore, we do not have any structural assumptions on the problem. We
propose a multi-phase explore-then-commit type algorithm namely epoch-based CA-ETC
(collision avoidance explore then commit) (CA-ETC in short) for this problem that does not
require any communication across agents (players and arms) and hence decentralized. We
show that for the initial epoch length of T and subsequent epoch-lengths of 2l/γT (for the
◦ ◦
l−th epoch with γ ∈(0,1) as an input parameter to the algorithm), CA-ETC yields a player
(cid:16) (cid:17)
optimal expected regret of O T (KlogT)1/γ +T (T )γ for the i-th player, where T is the
◦ T◦∆2 ◦ T◦
learning horizon, K is the number of arms and ∆ is an appropriately defined problem gap.
Furthermore, weproposeablackboardcommunicationbasedbaselineachievinglogarithmic
regret in T.
1 Introduction
Online matching markets (e.g. Mechanical Turk, Upwork, Uber, Labour markets, Restaurant) are economic
platforms that connect demand side, (e.g. businesses in Mechanical Turk or Upwork, customers wanting
Uber ride or restaurant reservations), to the supply side (e.g. freelancers in Upwork, or crowdworkers in
Mechanical Turk, drivers in Uber, restaurant availability) (Johari et al. (2021); Das & Kamenica (2005)).
In these platforms, the demand side (also known as player side) makes repeated decisions to obtain (match)
the resources in the supply side (also known as arms side) according to their preference. The supply side
is usually resource constrained, and hence it is possible that more than one player compete for a particular
resource. Given multiple offers, the supply side arm chooses the agent of its choice. This agent is given the
non-zerorandomreward,whileallotherplayersparticipatedinthecollisiongetsadeterministiczeroreward.
Hence, in this framework, the players need to simultaneously compete as well as estimate the uncertainty in
the quality of resource.
Usually the interaction between the supply and demand side is modelled as a bipartite graph, with players
and arms in both sides having a preference (or ranking) over the other side, which is unknown apriori. Each
©2024TejasPagareandAvishekGhosh.
AcceptedatInternationalSymposiumofInformationTheory2024. ExtendedversionofPagare&Ghosh(2023).
4202
guA
61
]GL.sc[
1v09680.8042:viXraPagare and Ghosh
agents’ task is to learn this preference through successive but minimal interaction between the sides and
thereafter obtain an optimal stable matching between the demand and the supply side.
Multi-Armed Bandits (MAB) is a popular framework that balances exploration and exploitation while nav-
igating uncertainty in the system Lattimore & Szepesvári (2020); Auer et al. (2002). Learning in matching
markets has received considerable interest in the recent past, especially from the lens of a multi-agent MAB
framework Liu et al. (2021); Sankararaman et al. (2021); Kong & Li (2023); Basu et al. (2021). In this
formulation, the demand side corresponds to multiple players and the supply side resources correspond to
multiple arms. The additional complexity here is the presence of competition among players. The objective
of this problem is to learn the preference ranking for both the players and arms simultaneously through
successive interactions. Once the preferences are learnt, matching algorithm like Gale-Shapley (Deferred
Acceptance Gale & Shapley (1962)) may be used to obtain the optimal stable matching.
Although the MAB framework in markets and bandits have been popular in the recent past, starting with a
centralizedmatchingmarkets(i.e.,thereexistsacentralizedauthoritywhohelpsinthematchingprocess,Liu
et al. (2020); Jagadeesan et al. (2021)) to a more recent decentralized framework Lattimore & Szepesvári
(2020); Sankararaman et al. (2021); Basu et al. (2021); Kong & Li (2023); Maheshwari et al. (2022), all
the previous works have several strong (and often impractical) assumptions on the market for theoretical
tractability. We now discuss them in detail.
One-sided learning: One of the main assumptions made in all the previous works Liu et al. (2020; 2021);
Sankararaman et al. (2021); Basu et al. (2021); Maheshwari et al. (2022); Ghosh et al. (2022), including
the state-of-the-art Kong & Li (2023) is that of one-sided learning. It is assumed that the arms know their
preferencesoverplayersaprioribeforethestartofthelearning,andhencethecruxoftheproblemistolearn
the preferences for the players only. So, the problem of two sided learning reduces to one sided learning.
Here, algorithms based on Upper-Confidence-Bound Sankararaman et al. (2021); Liu et al. (2021) as well
as Explore-Then-Commit Kong & Li (2023); Basu et al. (2021) are both analyzed and sublinear regret
guarantees are obtained. With the knowledge of the arms’ ranking apriori, the system can correctly resolve
the conflicts every time a collision occurs, which is used crucially in the analysis of such algorithms.
Additional structural market assumptions: Apart from the one-sided learning assumption, several
additional structural assumptions on markets are usually being made to obtain sub-linear regret. In Liu
et al. (2021); Kong & Li (2023); Ghosh et al. (2022), it is assumed that when a player successfully receives
a reward (or in other words, a player and an arm are successfully matched), the matched pair becomes a
commonknowledgetoalltheplayers,wereferthisas(player,arm)broadcast. Thisassumptionallowsagents
tocommunicatetheirlearningprogress. Moreover,Sankararamanetal.(2021);Ghoshetal.(2022)assumed
a serial dictatorship model where the preference ranking of players are assumed to be same for all the arms.
In Liu et al. (2021), the same assumption is termed as global ranking. In Maheshwari et al. (2022), this
assumption is weakened and termed as α-reducible condition. Moreover, in Basu et al. (2021), the authors
propose a uniqueness consistency assumption on the market, implying that the leaving participants do not
alter the original stable matching of the problem.
1.1 Summary of Contributions
We study two learning algorithms, (i) Explore-then-Gale-Shapley (ETGS) in the presence of a blackboard,
a communication medium, which we define shortly and (ii) epoch-based collision avoidance explore-then-
commit CA-ETC a decentralized algorithm, both of which work for any two-sided market with no restrictive
assumptions. Bydecentralizedalgorithmwemeanthateachagentperformsactionsbasedonlyontheirpast
interaction in the market without any communication with other agents. Following we briefly explain our
contributions
Decentralized two-sided learning algorithm: We take a step towards obtaining a provable two-sided
learningalgorithmfordecentralizedmatchingmarkets. Tobeconcrete,wedonotassumethatthepreference
of the players are known to the arms apriori. Our proposed schemes, ETGS (with blackboard) and CA-ETC
are based on Explore Then Commit (ETC) algorithm and are also collision free, similar to Kong & Li
(2023). However, the algorithms are able to learn the preferences for both the agent and the arm side
2Pagare and Ghosh
Two- Assumption Regret Type Regret
sided
Liu et al. (2020) No centralized player-pessimal O(NK3logT/∆2)
(cid:16) (cid:17)
Liu et al. (2021) No (player, arm) broadcast player-pessimal O
N5K2log2T
ϵN4∆2
(cid:0) (cid:1)
Sankararaman No serial dictatorship player-optimal O NKlogT/∆2
et al. (2021)
(cid:0) (cid:1)
Basu et al. (2021) No uniqueness consistency player-optimal O NKlogT/∆2
(cid:0) (cid:1)
Maheshwari et al. No α-reducible player-optimal O CNKlogT/∆2
(2022)
(cid:0) (cid:1)
Kong & Li (2023) No (player, arm) broadcast player-optimal O KlogT/∆2
Pokharel & Das Yes (player, arm) broadcast – No Guarantee
(2023)
(cid:0) (cid:1)
This paper Yes blackboard (eqv. to (play, arm) player-optimal O KlogT/∆2
(Blackboard) broadcast)
This paper Yes no assumptions player-optimal
O(cid:16)
T
(cid:0)KlogT(cid:1)1/γ
+T
(cid:0)
T
(cid:1)γ(cid:17)
◦ T◦∆2 ◦ T◦
(CA-ETC)
Table 1: Table comparing the regret bound of CA-ETC with existing works. Here, N is the number of players, K is
the number of arms, T is the learning horizon, ∆ is the minimum gap (to be defined later). Also, ϵ (in Liu et al.
(2021))andC (inMaheshwarietal.(2022)areproblemdependenthyper-parameters. Forouralgorithm,CA-ETC,T
◦
is the initial epoch length and γ ∈(0,1) is an input parameter related to the subsequent epoch lengths.
simultaneously through obtained samples in the exploration phase. Further, CA-ETC is decentralized. Note
that in real world applications, like two-sided labor markets (Upwork, Uber, Restaurant), crowd-sourcing
platforms (Mechanical Turk), scheduling jobs to servers (AWS, Azure) Dickerson et al. (2019); Even et al.
(2009), the preference of players side are apriori unknown and a two-sided learning algorithm is necessary.
We would like to point out a few very recent works on two-sided learning. Jagadeesan et al. (2021) pro-
poses a centralized algorithm for a market framework that allows utility transfer (monetary transfers), with
convergence to a weaker notion of stability. Pokharel & Das (2023) introduces algorithms with (player,
arm) broadcasting and does not provide any regret guarantees. Anonymous (2024) assumes the existence of
arm-sample efficient strategy which allows them to reduce their problem to one-sided learning after finitely
many round-robin exploration rounds.
Nostructuralassumptionsonmarkets: Weemphasizethatourcollision-freealgorithmsdonotrequire
any additional assumption on the economic market, like serial dictatorship, global knowledge of matching
playerarmpair,uniqueconsistencyetc,whichmakesthemmorepractical. Inapplicationslikelabormarkets
(Upwork, Taskrabbit) Massoulié & Xu (2016), crowd-sourcing platforms (Mechanical Turk), scheduling jobs
to servers in an online marketplace (AWS, Azure) Dickerson et al. (2019); Even et al. (2009), question
answering platform (Quora, Stack Overflow) Shah et al. (2020) the structural assumptions mentioned above
are naturally not satisfied. So, there is a gap between theory and practice, and our proposed algorithms, is
a natural first step towards closing this gap.
2 Problem Setting
We now explain the problem formulation. Consider a market with N players and K arms with N ≤ K
w.l.o.g. (we comment on N > K case in the Appendix). Denote N = {p ,p ,...,p } as the set of players
1 2 N
and K = {a ,a ,...,a } as the set of arms. In general, not all players and arms will participate in the
1 2 K
market, but we consider that the participating agents include all the players and arms. At time step t, each
player p attempts to pull an arm A (t)∈K. When multiple players pull the same arm, only one player will
i i
successfully pull the arm based on arm’s preferences which are also learned over time.
Two-sided reward model: Since our goal is to learn the preferences of the players and arms simulta-
neously, we propose a two-sided reward model in the following way. If player p successfully pulls an arm
i
3Pagare and Ghosh
A (t) = a then p receives a stochastic reward X(i)(t) ∼ SubGaussian(µ(i)) (subGaussian with mean µ(i)
i j i j j j
and unity subGaussian parameter), and arm a receives a stochastic reward Y(j)(t) ∼ SubGaussian(η(j)).
j i i
We remark that for two sided learning, reward information for both the player and the arm side are nec-
essary, and hence we propose this two sided reward model. If µ(i) > µ(i), we say that player p truly
j j′ i
prefers arm a over a . Similarly, if η(j) > η(j), we say that arm a truly prefers player p over p . We
j j′ i i′ j i i′
denote P (t) := {p : A (t) = a } as the set of players proposing arm a , A¯ (t) as the successfully matched
j i i j j i
arm of player p , P¯ (t) as the successfully matched player of arm a i.e. P¯ (t) ∈ argmax η(j). Then
i j j j pi∈Pj(t) i
A¯ (t) = A (t) when p is successfully accepted by arm A (t) else if rejected A¯ (t) = ∅. When two or more
i i i i i
players propose an arm a then only the most preferred player P¯ (t) among P (t) gets an reward X(P¯ j(t))(t)
j j j j
and other get a zero reward 1. Denote m as the final matching at round t as a function from N to K s.t
t
playerp =m−1(A¯ (t))ismatchedwitharmm (p ). Weassumethatalltheparticipatingagentshavestrict
i t i t i
preference ranking i.e. µ(i) ̸=µ(i) and η(j) ̸=η(j) for all arms a ̸=a and players p ̸=p .
j j′ i i′ j j′ i i′
Stable matching: We seek to find stable matching Gale & Shapley (1962), which is shown to be closely
related to the notion of Nash equilibrium in game theory. A market matching is stable if no pair of players
andarmswouldprefertobematchedwitheachotherovertheirrespectivematches. Formally,m isstableif
t
there exists no player-arm pair (p ,a ) such that µ(i) >µ(i) and η(j) >η(j) , where we simply define
i j j mt(pi) i m− t1(aj)
µ(i) = −∞ and η(j) = −∞ for each i ∈ [N],j ∈ [K]. Let M := {m : m is a stable matching} be the set of
∅ ∅
all stable matching. Define player-optimal stable matching m¯ ∈ M as the players’ most preferred match
p
i.e. µ(i) ≥µ(i) for any match m∈M and for all i∈[N]. One can similarly define arm-optimal stable
m¯p(pi) m(pi)
matching m¯ ∈ M as the arms’ most preferred match i.e. η(j) ≥ η(j) for any m ∈ M and for all
a m¯− a1(aj) m− a1(aj)
j ∈ [K]. Previous works Kong & Li (2023) has also defined the notion of player-pessimal stable matching
defined as the players’ least preferred match m ∈ M i.e. µ(i) ≤ µ(i) for any m ∈ M and for all
p m p(pi) m(pi)
i ∈ [N]. Similarly we define arm-pessimal stable matching m ∈ M as the arms’ least preferred match i.e.
a
η(j) ≤η(j) for any m∈M and for all j ∈[K].
m− a1(aj) m− a1(aj)
Regret: Based on the different notions of stable matching, we define player-optimal regret for each player
p , i∈[N] over T rounds as
i
T T
RP (t)=X µ(i) −E[X X(i)(t)]
i m¯p(pi) j
t=1 t=1
and arm-pessimal regret for each arm a , j ∈[K] as follows
j
T T
RA (t)=X η(j) −E[X Y(j)(t)].
j m a(aj) i
t=1 t=1
One can similarly define player-pessimal and arm-optimal regret. Note that the player-pessimal (arm-
pessimal) regret is upper bounded by player-optimal (arm-optimal) regret, and hence any upper bound on
player-optimal regret automatically serves as an upper bound on player pessimal regret. When there are
morethanonestablematchingi.e. |M|>1,thedifferencebetweenplayer-pessimalandplayer-optimalregret
can be O(T) due to a constant difference between µ(i)(p ) and µ(i) , similarly for arms. Throughout we
m p i m¯p(pi)
give guarantees for player-optimal and arm-pessimal regret which are equal since a player-optimal match
is same as the arm-pessimal match and since both players and arms are using same algorithm. We will
henceforth refer to agents as players and arms i.e. all the participating agents in the market.
1Inoursetting, anarmlearnsonlywhenitisproposed. Thus, weexplicitlydonotdefinerewardfeedbacktothearmifit
isunproposed.
4Pagare and Ghosh
Algorithm 1: Index Estimation (view of player p )
i
Input : arbitrary preference ranking of arm a over players
1
1 for round t=1,2,..., N do
2 A i(t)=a 1
3 if A¯ i(t)=A i(t)=a 1 then
4 Index = t and break for loop
5 end
6 end
3 Algorithms for Two-sided matching markets
Gap: We define the gap of player p as ∆(i) = min |µ(i) − µ(i)| and gap of arm a as ∆′(j) =
i j̸=j′ j′ j j
min |η(j) − η(j)|. The universal gap is defined as the minimum of all the player and arm gap i.e.
i̸=i′ i′ i
∆=min {∆(i),∆′(j)}.
i∈[N],j∈[K]
We present learning algorithms in this section. Before this we first note that, when the gap ∆ is made
common knowledge to every agent, then a logarithmic regret can be obtained, as the gap knowledge suffices
toestimatethehigh-probabilityupperboundonthesufficientnumberofexplorationround. Wehypothesize
that such direct estimation without any communication is not possible. Thus we start with a simple setup,
where the participating agents are allowed to communicate using a blackboard then we present our main
algorithm, CA-ETC.
3.1 Warmup: Learning with Blackboard
The blackboard model of communication is standard in centralized multi-agent systems, with applications
in distributed optimization, game theory, and auctions Awerbuch & Kleinberg (2008); Buccapatnam et al.
(2015); Agarwal et al. (2009). We show that this is equivalent to broadcasting in the centralized framework.
Definition 3.1. A blackboard B is a Boolean array of size N +K where for i ∈ {1,...,N}, ith entry is
changedonlybyplayerp andforj ∈{1,...,K},(N+j)th entryischangedonlybyarma . Theblackboard
i j
can be viewed by all the players and arms.
The Algorithm 2 ETGS, is Explore-then-Commit type where each agent performs exploration in a round-
robin manner (thus avoiding collision) till everyone finds correct preference ranking which is communicated
to other agents by blackboard and then everyone commits to the stable-matching Gale-Shapley algorithm.
We now explain the stages in detail.
◦ Exploration Phase: We first emphasize that when arms are unaware of the preferences over players, they
are unable to resolve conflicts between colliding players. This necessitates a round-robin exploration, where
ateveryround,eachplayerproposestoadistinctarmandhenceavoidsthecollision. Toperformround-robin
exploration, an index estimation scheme is proposed which assigns each player in a decentralized manner a
distinct index.
⋄ Index estimation: In the proposed Algorithm 1, each player proposes to arm a till it gets accepted and
1
assignsitselftheindexastheroundofacceptance. Thus,thisphaselastsforN numberofrounds. Intuitively,
the index of each player will be the player’s preference rank for arm a . In the beginning arm a will have
1 1
arbitrary but distinct preference ranking over players, hence leading to a distinct index for each player.
Based on the index, each player now performs round-robin exploration and each arm accepts the proposal.
After every exploration round, each player p computes UCB(i) and LCB(i) for every arm a ,k ∈ [K] and
i k k k
similarly arms that got matched (i.e. those who got proposed) computes the confidence bounds for all
players. From the view of player p , after observing the reward from the matched arm A (t), it updates an
i i
estimate of mean reward µˆ(i) and the observed time T(i) which is defined as the number of times player
Ai(t) Ai(t)
p is matched with arm A (t) initialized as T(i) =0. At time t this is updated using
i i Ai(0)
5Pagare and Ghosh
Algorithm 2: ETGS (player p ) (Blackboard)
i
1 Perform Index estimation (Algorithm 1), t=1
2 while PN l=+ 1KB[l]̸=N +K do
3 A i(t)=a (Index+t−1)%K+1 // exploration
4 Observe X A(i i) (t)(t) and update µˆ( Ai) i(t),T A(i i)
(t)
if A¯ i(t)=A i(t)
5 Compute UCB(i)(t) and LCB(i)(t) for each k ∈[K]
k k
6 if ∃σ such that LCB( σi k)(t)>UCB( σi k) +1(t) for any k ∈[K−1] then
7 Preferences = σ and B[i]=1
8 end
9 t←t+1
10 end
// Gale-Shapley with σ =(σ ,...,σ )
1 K
11 Propose using σ till acceptance
12 Initialize s=1
13 while t̸=T do
14 A i(t)=a σs
15 s=s+1 if A¯ i(t)==∅
16 t←t+1
17 end
µˆ(i) T(i) +X(i)
µˆ(i) = Ai(t) Ai(t) Ai(t) , T(i) =T(i) +1
Ai(t) T(i) +1 Ai(t) Ai(t)
Ai(t)
r
Now every player p updates the UCB and LCB estimates of µ(i) for each arm a as UCB(i)(t)=µ(i)+ 2logt
i k k k k T(i)
k
r
and LCB(i)(t) = µ(i) − 2logt, where UCB(i)(t) and LCB(i)(t) are initialized as ∞ and −∞ respectively,
k k T(i) k k
k
similarly each arm a updates the confidence bounds for each player.
j
⋄ Preference ranking check: After computing the confidence bounds, each player and arm performs a check
whether they have found the true preference ranking over the other side. From the view of player p , it
i
requiresiteratingoverallpossiblepermutationsofarmpreferencesuntilapreferencerankingisfoundwhich
hasdisjointconfidenceintervalsi.e. arankingσ suchthatLCB(i) >UCB(i) for anyk ∈[K−1]whichimplies
σk σk+1
player p truly prefers a over a over a and so on.
i σ1 σ2 σ3
⋄Communication using Blackboard: Ifaplayerorarmfindssuchdisjointconfidenceintervals, theysettheir
corresponding bit on the blackboard to 1. Agents then enter the exploitation phase when all the bits are set
to 1. Note that this happens synchronously, i.e. every player and arm enters the exploitation phase in the
same round.
◦ Exploitation Phase: Now when every agent learns the true preferences over the other side, they perform a
decentralized Deferred Acceptance Algorithm Gale & Shapley (1962) which is a polynomial time algorithm
lastingatmostK2 rounds. Itfindsoptimalmatchfortheproposingsideandhenceapessimalmatchforthe
other side. In our algorithm, players are the proposing side, thus in each round, every player proposes to an
armbasedonthelearnedpreferencerankingtillacceptancei.e. ifrejectedinthelastrounditproposestothe
armwiththenextpreferencerank. Similarly,eacharm,basedonthelearnedpreferencerankingresolvesthe
conflict and chooses the best player. Players (arms) can thus incur non-zero player-optimal (arm-pessimal)
expected regret for at most K2 rounds, after which a zero expected regret is guaranteed.
Regret Guarantee We define ∆(i) as the maximum stable regret suffered by p in all rounds which is
max i
∆(i) =µ .
max i,m¯p(pi)
6Pagare and Ghosh
Algorithm 3: Epoch-based CA-ETC (view of player p )
i
Input : T , parameter γ ∈(0,1) with b=21/γ
◦
1 Run Algorithm 1 for Index Estimation
2 for l=1,2,... do
3 Base Algorithm (exploration = 2lT ◦, horizon = blT ◦)
4 end
Theorem 3.2 (Regret of Algorithm 2). Suppose every player plays Algorithm 2 and every arm plays Algo-
rithm 5 for T iterations. Then the player-optimal regret of player p satisfies
i
(cid:18)
64KlogT
2NKπ2(cid:19)
RP (T)≤ N + +K2+ ∆(i) .
i ∆2 3 max
A similar upper bound holds for arm-pessimal regret.
Remark 3.3 (Different terms). First term in the regret is from index estimation phase, second from a high-
probability number of exploration rounds, third from Gale Shapley algrorithm and fourth from the Sub-
Gaussian concentration bound.
Remark 3.4. Theregretorder-wisematcheswiththeregretintheone-sidedlearningcaseKong&Li(2023);
Basu et al. (2021).
Remark 3.5. The blackboard setting can be changed by the assumption of global knowledge of matched
(player,arm)pairbroadcastLiuetal.(2021);Kong&Li(2023). Usingthisonecandetectinadecentralized
manner whether all the agents have estimated correct preference ranking similar to the monitoring round of
Kong & Li (2023) for one sided learning. We comment more on this in the Appendix.
3.2 Epoch-based CA-ETC
Wenowpresentthemainalgorithm,namelyCA-ETCwhichdoesn’trequireablackboardandisdecentralized
and communication-free. Here we describe the player’s learning procedure in detail (the arm’s learning is in
the Appendix). See Fig. 2 for pictorial representation of CA-ETC.
◦ Main Algorithm: The proposed Algorithm 3, CA-ETC is epoch based, with each epoch consisting of in-
creasing interaction rounds, in which each individual performs round-robin exploration and commits using
learned preferences to the optimal matching Gale-Shapley algorithm. Here we choose exponentially in-
creasing rounds, however, a polynomial algorithm leads to similar regret bounds. As before, to perform
round-robinexplorationtheIndexestimationsubroutineAlgorithm1isusedbyeachplayertogetadistinct
index.
⋄ Inside an Epoch: In every epoch l, Algorithm 4 performs 2lT exploration rounds and blT (with b=21/γ
◦ ◦
throughout)exploitationrounds. Theparameterγcontrolstheexploration-exploitationtrade-offinanepoch.
The exploration phase is identical to the blackboard based algorithm with reduced computation. Here, each
agentcomputestheconfidenceboundsandperformspreferencerankingcheckfordisjointconfidenceintervals
only once after the defined number of exploration rounds for the epoch. If the check for disjoint interval
succeeds each agent uses this preference ranking and if it fails the agent uses some arbitrary preference
ranking and enter the exploitation phase which is identical to the blackboard based algorithm.
Theideaofthealgorithmisthatwhenthetotalsamplesfromexplorationroundsexceeds⌈log(16KlogT +1)⌉
T◦∆2
(see Kong & Li (2023)), with high probability, each agent can estimate the correct preference. In CA-ETC,
this happens after a finitely many epochs. Thus after these many epochs, each agent only incurs a non-zero
regret in the exploration phase and Gale-Shapley rounds.
Theoretical Guarantees Here, we present the regret bounds for CA-ETC for player p . Similar regret can
i
be obtained for arm a (we defer this to the appendix).
j
Theorem3.6. SupposeeveryagentrunsCA-ETCwithinitialepochlengthT andinputparameterγ ∈(0,1).
◦
(cid:16) (cid:17) 1
Then,providedtheinitialepochsatisfiesT ≥ 32KlogT 1−γ,theplayer-optimalregretforplayerp isgiven
◦ ∆2(T−N)γ i
7Pagare and Ghosh
Algorithm 4: Base Algorithm: ETGS (player p )
i
Input : Exploration rounds 2lT , Horizon blT
◦ ◦
1 for t=Pl l− ′=1 12l′T ◦+1,...,Pl l− ′=1 12l′T ◦+2lT ◦ do
2 A i(t)=a (Index+t−1)%K+1 // Exploration
3 Observe X A(i i) (t)(t) and update µˆ( Ai) i(t),T A(i i)
(t)
if A¯ i(t)=A i(t)
4 end
5 Compute UCB(i) and LCB(i) for each k ∈[K]
k k
6 if ∃σ such that LCB( σi k) >UCB( σi k) +1for any k ∈[K−1] then
7 Preferences = σ
8 else
9 Preferences = arbitrary
10 end
// Gale-Shapley with σ =(σ ,...,σ )
1 K
11 Propose using σ till acceptance
12 Initialize s=1
13 for t=1,2,...,blT ◦−2lT ◦ do
14 A i(t)=a σs
15 s=s+1 if A¯ i(t)==∅
16 end
by
(cid:18) 32KlogT(cid:19)1/γ (cid:18) 64KlogT(cid:19)
RP (T)≤N∆(i) +T log ∆(i)
i max ◦ T ∆2 T ∆2 max
◦ ◦
(cid:18) T (cid:19)γ (cid:18) T (cid:19) 2NKπ2
+2T ∆(i) +K2γlog ∆(i) + ∆(i) .
◦ T max T max 3 max
◦ ◦
Remark 3.7. CA-ETC takes γ and T as input parameters to be chosen by the learner. Typical values of
◦
γ would be {1/3,1/4,1/5}, which would imply a polynomial dependence on logT and a weakly increasing
function of T. In particular, for γ =1/4 we have
(cid:18) KlogT(cid:19)4 (cid:18) KlogT(cid:19) (cid:18)
T
(cid:19)1/4!
RP (T)≤O log +T
i T ∆2 T ∆2 ◦ T
◦ ◦ ◦
Incomparisontoone-sidedlearningpapersSankararamanetal.(2021);Liuetal.(2021);Kong&Li(2023),
the regret incurred by CA-ETC is high. This can be attributed to the cost of two-sided assumption-free
learning. Thedependenceonγ comesfromthemulti-phasenatureof CA-ETC.Ontopofaworsedependence
onlogT and∆,CA-ETCalsohasa(weaklygrowing)polynomialdependenceonT. Webelievewithalgorithms
that allow structured collision and some weak form of communication, these high regret term can be cut
down.
Remark 3.8 (Type of CA-ETC). We emphasize that here we use exponential increment, however polynomial
incrementi.e. l2T explorationandlbT totalroundscanbeused. Weobserveasimilarregretupperbound,
◦ ◦
explicit expression can be found in the appendix.
Remark 3.9 (Different terms). First term in the regret comes from the index estimation subroutine. The
second term results from the round-robin exploration before every agent estimates the true preference rank-
ing. The third and fourth term comes from the round-robin exploration and Gale-Shapley after the rank
estimation. The last term results from the SubGaussian concentration bound.
Remark 3.10 (Choice of T ). We have a condition that needs T to be not too small. However, note that,
◦ ◦
unless the gap ∆ is too small, the condition is rather-mild and is trivial when T is very large. Of course, the
optimal choice of T depends on the gap, ∆ and hence not known to the learner apriori.
◦
8Pagare and Ghosh
player1 player3 player4
70000
250000
60000
60000
200000 50000
40000 150000 40000
30000
100000
20000 20000
CA-ETCexp CA-ETCexp CA-ETCexp
50000
CA-ETCpoly CA-ETCpoly 10000 CA-ETCpoly
blackboard blackboard blackboard
0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
106 106 106
× × ×
arm1 arm4 arm5
0
250000
60000
−50000 200000
40000 100000 150000
−
100000
20000 CA-ETCexp −150000 CA-ETCexp
50000
CA-ETCexp
CA-ETCpoly CA-ETCpoly CA-ETCpoly
0 blackboard −200000 blackboard 0 blackboard
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
106 106 106
× × ×
Figure 1: 5x5 market cumulative regret plot
4 Simulations
Weconsideramarketsetupwith5playersand5arms. Themeanissampleduniformlybetween(0,1)without
replacement. Weplotcumulativeregretv/shorizoninFig. 1forasinglerunfor106 interactionrounds. For
CA-ETCexpandpolyweconsider2lT (blT )andl2T (lbT )exploration(totalrounds)respectively. weplot
◦ ◦ ◦ ◦
player-optimal and arm-pessimal regret for some players and arms. Arm-pessimal regret can be negative
as arm can match with better players during exploration round, we observe this effect for arm 4. We keep
γ = 0.4 and T = 500. Our results indicate that CA-ETC learns the true preferences after finitely many
◦
epochs which is l =3 for both CA-ETC exp and poly. We observe that the type of CA-ETC has an impact
max
on the performance due to different sensitivity to parameter γ. In practice, we propose to use sufficiently
larger T for a good exploration warm-up and values of γ stated in Remark 4.
◦
5 Discussion and Future Work
Inthispaper,weproposepracticalalgorithmsfortwo-sidedlearninginmatchingmarketswithoutrestrictive
assumptions, one with communication (blackboard) and other completely decentralized CA-ETC. Without
anycommunication(implicitorexplicit), andpriorknowledgeof∆, logarithmicregretintwo-sidedlearning
seems unachievable. Prior work Sankararaman et al. (2021) uses implicit communication using structured
collisionsinone-sidedlearning, however, itisnotbeneficialwhenthearmscannotresolveconflictscorrectly.
This is due to the inherent asymmetry in the learning of two-sided matching, the non-proposing side (arm)
need to learn the preferences before proposing side (players) for this communication to be useful. Our
algorithm CA-ETC is symmetrical and synchronous i.e. both sides learn in similar fashion and enter commit
phase at the same time. One future direction is to make the algorithm asynchronous, which can be done
when a subset of the market preserves the stable match. Further, Algorithm 4 in Line 9 uses arbitrary
preferences, which an agent based on prior knowledge and estimated confidence bounds can use a ranking
close to true preference ranking and can thus lead to sharper regret bounds. In future, we would also
like to study the market setup, with transferable utilities (i.e., monetary transfer) in a two-sided setting.
Furthermore, markets are seldom static, and the preference ranking changes over time and capturing the
dynamic behavior of markets in an assumption free two-sided setting is certainly challenging.
9Pagare and Ghosh
References
Alekh Agarwal, Martin J Wainwright, Peter Bartlett, and Pradeep Ravikumar. Information-theoretic lower
bounds on the oracle complexity of convex optimization. Advances in Neural Information Processing
Systems, 22, 2009.
Anonymous. Bandit learning in matching: Unknown preferences on both sides, 2024. URL https://
openreview.net/forum?id=nFJVpeYcnv.
Peter Auer, Nicolo Cesa-Bianchi, and Paul Fischer. Finite-time analysis of the multiarmed bandit problem.
Machine learning, 47(2):235–256, 2002.
Baruch Awerbuch and Robert Kleinberg. Competitive collaborative learning. Journal of Computer and
System Sciences, 74(8):1271–1288, 2008.
Soumya Basu, Karthik Abinav Sankararaman, and Abishek Sankararaman. Beyond log squared regret for
decentralizedbanditsinmatchingmarkets.InInternationalConferenceonMachineLearning,pp.705–715.
PMLR, 2021.
Swapna Buccapatnam, Jian Tan, and Li Zhang. Information sharing in distributed stochastic bandits. In
2015 IEEE Conference on Computer Communications (INFOCOM), pp. 2605–2613, 2015. doi: 10.1109/
INFOCOM.2015.7218651.
Sanmay Das and Emir Kamenica. Two-sided bandits and the dating market. In Proceedings of the 19th
International Joint Conference on Artificial Intelligence,IJCAI’05,pp.947–952,SanFrancisco,CA,USA,
2005. Morgan Kaufmann Publishers Inc.
John Dickerson, Karthik Sankararaman, Kanthi Sarpatwar, Aravind Srinivasan, Kung-Lu Wu, and Pan Xu.
Onlineresourceallocationwithmatchingconstraints. InInternational Conference on Autonomous Agents
and Multiagent Systems (AAMAS), 2019.
Guy Even, Magnús M Halldórsson, Lotem Kaplan, and Dana Ron. Scheduling with conflicts: online and
offline algorithms. Journal of scheduling, 12(2):199–224, 2009.
David Gale and Lloyd S Shapley. College admissions and the stability of marriage. The American Mathe-
matical Monthly, 69(1):9–15, 1962.
Avishek Ghosh, Abishek Sankararaman, Kannan Ramchandran, Tara Javidi, and Arya Mazumdar. Decen-
tralized competing bandits in non-stationary matching markets. arXiv preprint arXiv:2206.00120, 2022.
MeenaJagadeesan, AlexanderWei, YixinWang, MichaelJordan, andJacob Steinhardt. Learningequilibria
in matching markets from bandit feedback. Advances in Neural Information Processing Systems, 34:
3323–3335, 2021.
Ramesh Johari, Vijay Kamble, and Yash Kanoria. Matching while learning. Operations Research, 69(2):
655–681, 2021.
FangKongandShuaiLi.Player-optimalstableregretforbanditlearninginmatchingmarkets.InProceedings
of the 2023 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA),pp.1512–1522.SIAM,2023.
Tor Lattimore and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.
Lydia T Liu, Horia Mania, and Michael Jordan. Competing bandits in matching markets. In International
Conference on Artificial Intelligence and Statistics, pp. 1618–1628. PMLR, 2020.
Lydia T Liu, Feng Ruan, Horia Mania, and Michael I Jordan. Bandit learning in decentralized matching
markets. The Journal of Machine Learning Research, 22(1):9612–9645, 2021.
Chinmay Maheshwari, Shankar Sastry, and Eric Mazumdar. Decentralized, communication-and
coordination-free learning in structured matching markets. Advances in Neural Information Processing
Systems, 35:15081–15092, 2022.
10Pagare and Ghosh
Laurent Massoulié and Kuang Xu. On the capacity of information processing systems. In Conference on
Learning Theory, pp. 1292–1297. PMLR, 2016.
Tejas Pagare and Avishek Ghosh. Two-sided bandit learning in fully-decentralized matching markets. In
ICML 2023 Workshop The Many Facets of Preference-Based Learning,2023. URLhttps://openreview.
net/forum?id=1oFkZbODgD.
Gaurab Pokharel and Sanmay Das. Converging to stability in two-sided bandits: The case of unknown
preferences on both sides of a matching market, 2023.
Abishek Sankararaman, Soumya Basu, and Karthik Abinav Sankararaman. Dominate or delete: Decentral-
ized competing bandits in serial dictatorship. In International Conference on Artificial Intelligence and
Statistics, pp. 1252–1260. PMLR, 2021.
ViragShah,LennartGulikers,LaurentMassoulié,andMilanVojnović.Adaptivematchingforexpertsystems
with uncertain task types. Operations Research, 68(5):1403–1424, 2020.
A Appendix
A.1 Theoretical Guarantees
Firstwepresentlemmas,whichwillsupportourtheorems. Wenotethattheanalysisgoesmoreorlessalong
the same lines of Kong & Li (2023).
 
s
 2logt
Let us define the event F (t) = ∃i∈[N], j ∈[K]:|µˆ(i)−µ(i)|> , a player’s bad event that
p

j j T(i)

j
some preference of the arms are not estimated by the players correctly at time t. We present the following
Lemma 5.1, Lemma 5.4 ,and Lemma 5.5 of Kong & Li (2023)
Lemma A.1.
" XT # NKπ2
E 1{F (t)} ≤ (1)
p 3
t=1
Lemma A.2. Conditional on F (t)∁, UCB(i)(t)<LCB(i)(t) implies µ(i) <µ(i).
p j j′ j j′
Lemma A.3. In round t, let T(i)(t) = min T(i)(t) and T¯(i) = 32logT/(∆(i))2 where ∆(i) =
j∈[K] j
min ∆(i) = min |µ(i) −µ(i)|. Conditional on F (t)∁, if T(i) > T¯(i) we have UCB(i)(t) < LCB(i)(t)
j̸=j′ j,j′ j̸=j′ j j′ p j j′
for any j,j′ ∈[K] with µ(i) <µ(i).
j j′
Lemma A.3 can similarly be written for arms. Following we extend the Lemma 5.3 from Kong & Li (2023)
to two-sided learning
( s )
2logt
Define the event F (t) = ∃i∈[N], j ∈[K]:|ηˆ(j)(t)−η(j)|> , an arm’s bad event that some
a i i T(j)
i
preference of the players are not estimated by the arms correctly at time t.
Lemma A.4. Conditional on ∩T (F (t)∪F (t))∁, after epoch l , the player-optimal regret and arm-
t=1 p a max
pessimal regret for the period blT −2lT −K2 in epoch l>l is zero where
◦ ◦ max
( l )
l =min l: X 2l′ T ≥32KlogT/∆2 . (2)
max ◦
l′=1
Proof. Since in each exploration period all players propose to distinct arms using a round-robin fashion, no
collision occurs and all players are accepted at each round of the exploration period in each epoch. Thus at
11Pagare and Ghosh
the end of epoch l it holds that T(i) ≥32logT/(∆(i))2 and T(j) ≥32logT/(∆′(j))2 for any i∈[N] and
max j i
j ∈[K] since ∆=min {∆(i),∆′(j)}.
i∈[N],j∈[K]
Now according to Lemma A.3, when T(i) ≥ 32logT/(∆(i))2 for any arm a , player p finds a permutation
j j i
σ(i) over arms such that LCB(i) < UCB(i) for any k ∈ [K −1]. Similarly, for any player p , arm a finds a
σ(i) σ(i) i j
k k+1
permutation σ′(j) over players such that LCB(j) <UCB(j) for any k ∈[N −1].
σ(j) σ(j)
k k+1
This implies that after l > l , all players and arms finds a permutation of arms and players resp. with
max
disjoint confidence intervals. Since, Gale-Shapley algorithm will last at most K2 steps, we will have zero
regret in the period blT −2lT −K2 conditioned on the event ∩T (F (t)∪F (t))∁.
◦ ◦ t=1 p a
A.1.1 Proof of Theorem 1
Proof of Theorem 1. From Lemma A.3 after t = 32KlogT/∆2, conditioned on the event ∩T (F (t))∁
exp t=1 p
every player p finds true preference ranking over arms and similarly, conditioned on the event ∩T (F (t))∁
i t=1 p
every arm a finds correct preference ranking over players, thus the ith and (N +j)th entry of blackboard
j
is 1. Thus everyone enters Gale-Shapley rounds after t number of exploration rounds. Thus the optimal
exp
stable regret for player p is
i
" T #
RP (T)=E X µ(i) −X(i)(t)
i m¯i
t=1
" T #
≤E X 1{A¯(t)̸=m¯ }∆(i)
i max
t=1
" T # " T #
≤N∆(i) +E X 1{A¯(t)̸=m¯ ,F (t)∁ } ∆(i) +E X 1{F (t)}+1{F (t)} ∆(i)
max i p max p a max
N+1 N+1
≤N∆(i)
+E" XT
1{A¯(t)̸=m¯ ,F (t)∁
}#
∆(i) +
2NKπ2
∆(i)
max i p max 3 max
N+1
32KlogT 2NKπ2
≤N∆(i) + ∆(i) + ∆(i)
max ∆2 max 3 max
A.1.2 Proof of Theorem 2
We will find an equation of l , from the definition we have and using the fact that b>2 we get
max
(cid:24) (cid:18) (cid:19) (cid:25) (cid:24) (cid:18) (cid:19)(cid:25)
32KlogT 16KlogT
l = log +2 −1 = log +1
max T (∆(i))2 T (∆(i))2
◦ ◦
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
16KlogT 16KlogT 32KlogT
=⇒ log +1 ≤l <log +1 +1=log +2
T (∆(i))2 max T (∆(i))2 T (∆(i))2
◦ ◦ ◦
Define˜l,theepochnumberrequiredtocompleteT numberofinteractionrounds. Wehavefromthedefinition
˜l
X
blT =T −N
◦
l=1
b˜l+1−b
T =T −N
b−1 ◦
(T −N)
b˜l+1
= (b−1)+b
T
◦
(cid:18) b−1(T −N) (cid:19) (cid:18) T −N (cid:19)log b2 (cid:18) T −N (cid:19)γ
˜l=log +1 <log +1 =log +1
b b T T T
◦ ◦ ◦
12Pagare and Ghosh
Proof of Theorem 2. The optimal stable regret for player p is the sum of regret before and after l .
j max
" T #
RP (T)=E X µ(i) −X(i)(t)
i m¯i
t=1
" T #
≤E X 1{A¯(t)̸=m¯ }∆(i)
i max
t=1
" T # " T #
≤N∆(i) +E X 1{A¯(t)̸=m¯ ,F (t)∁ } ∆(i) +E X 1{F (t)}+1{F (t)} ∆(i)
max i p max p a max
N+1 N+1
≤N∆(i)
+E" XT
1{A¯(t)̸=m¯ ,F (t)∁
}#
∆(i) +
2NKπ2
∆(i)
max i p max 3 max
N+1
Let T = PlmaxblT and T = P˜l blT be the time-period before and after epoch ˜l with T −N =
1 l=1 ◦ 2 l=lmax+1 ◦
T +T . Thus we have
1 2
R(T)=E" XT
1{A¯(t)̸=m¯ i,F p(t)∁
}#
∆( mi) ax
=E"l Xmax
blT
◦#
∆ m(i)
ax+E

X˜l
2lT
◦+K2
∆( mi) ax
N+1 l=1 l=lmax+1
since the regret before epoch l i.e. for l ≤l is blT and after l we have non-zero regret only from
max max ◦ max
exploration and Gale-Shapley rounds due to Lemma A.4. Thus the total regret is
"l
Xmax
#  X˜l 
R(T)=E blT
◦
∆( mi) ax+E

2lT ◦+K2 ∆( mi)
ax
l=1 l=lmax+1
≤T blmax+1∆(i) +T (2˜l+1−2lmax+1)∆(i) +K2˜l∆(i)
◦ max ◦ max max
=2˜l+1T ∆(i) +(blmax+1−2lmax+1)T ∆(i) +K2˜l∆(i)
◦ max ◦ max max
≤2˜l+1T ◦∆( mi) ax+(l max+1)blmax(b−2)T ◦∆( mi) ax+K2˜l∆( mi)
ax
(convexity of xlmax+1 for l
max
≥1)
(cid:18)
T
(cid:19)γ (cid:18)
64KlogT
(cid:19)(cid:18)
32KlogT
(cid:19)1/γ (cid:18)
T
(cid:19)
≤2 T ∆(i) +log +4 +2 (21/γ −2)T ∆(i) +K2γlog ∆(i)
T ◦ max T (∆(i))2 T (∆(i))2 ◦ max T max
◦ ◦ ◦ ◦
We used the following inequality of convex functions
f(x)−f(y)≤∇f(x)(x−y).
Thus we have in total
(cid:18)
T
(cid:19)γ (cid:18)
64KlogT
(cid:19)(cid:18)
32KlogT
(cid:19)1/γ
RP (T)≤N∆(i) +2 T ∆(i) +log +4 +2 (21/γ −2)T ∆(i)
i max T ◦ max T (∆(i))2 T (∆(i))2 ◦ max
◦ ◦ ◦
(cid:18) T (cid:19) 2NKπ2
+ γK2log ∆(i) + ∆(i)
T max 3 max
◦
For T to be defined correctly we need that
2
(cid:18)
T −N
(cid:19)γ (cid:18)
32KlogT
(cid:19)
log +1 >˜l≥l +1≥log +2
T max T (∆(i))2
◦ ◦
(cid:18)
T
−N(cid:19)γ
32KlogT
=⇒ >
T T (∆(i))2
◦ ◦
(cid:18) (cid:19) 1
32KlogT 1−γ
T > ∀i
◦ (∆(i))2(T −N)γ
(cid:18) (cid:19) 1
32KlogT 1−γ
=⇒ T >
◦ ∆2(T −N)γ
13Pagare and Ghosh
A.1.3 Analysis for CA-ETC Poly
We experiment with different types exploration and horizon increment. Below we present analysis when the
number of exploration rounds and length of horizon in epoch l is l2T and lbT where b=2/γ respectively.
◦ ◦
&(cid:18) 96KlogT(cid:19)1/3’ &(cid:18)
T
(cid:19)1/(b+1)’
l = , ˜l= (b+1)
max T ∆2 T
◦ ◦
"l
Xmax
#  X˜l 
R(T)=E lbT
◦
∆( mi) ax+E

l2T ◦+K2 ∆( mi)
ax
l=1 l=lmax+1
1 1
≤ lb+1T + [˜l3−l3 ]T +K2˜l∆(i)
b+1 max ◦ 3 max ◦ max
1 1
≤ ˜l3T + lb+1T +K2˜l∆(i)
3 ◦ b+1 max ◦ max
1(cid:18)
T
(cid:19)3/(b+1) (cid:18) 96KlogT(cid:19)(b+1)/3
1
(cid:18)
T
(cid:19)1/(b+1)
≤ (b+1) T + T +K2 (b+1) ∆(i)
3 T ◦ T ∆2 (b+1) ◦ T max
◦ ◦ ◦
We similarly need
(cid:18)
96KlogT
(cid:19)(b+1)/(b−2)
T >
◦ ∆2((b+1)T)2/(b+1)
In total the player-optimal regret for player p is given as follows
i
1(cid:18)
T
(cid:19)3/(b+1) (cid:18) 96KlogT(cid:19)(b+1)/3
1
RP (T)≤N∆(i) + (b+1) T + T
i max 3 T ◦ T ∆2 (b+1) ◦
◦ ◦
(cid:18) T (cid:19)1/(b+1) 2NKπ2
+K2 (b+1) ∆(i) + ∆(i)
T max 3 max
◦
where recall b=21/γ. Similar expression can be obtained for arm-pessimal regret for each arms.
A.2 Pictorial representation of CA-ETC
Epoch1 Epoch2 Epoch3
Gale-ShapleyCommit
Ifcheckisfalse: useanypreferenceranking
Ifcheckistrue: usethelearnedpreferenceranking
round-robinexplorationrounds
preferencecheck
Figure 2: Pictorial representation of CA-ETC algorithm
The distinction between player’s and arm’s learning is only in the index estimation, which is required to be
performed by each player at the start of the learning process.
14Pagare and Ghosh
A.3 Arm’s Learning
Following we denote T(j)(t) by the number of times arm a has been matched with player p till round t.
i j i
A.3.1 Blackboard
Algorithm 5: ETGS (view of arm a ) (Blackboard)
j
1 while PN l=+ 1KB[l]̸=N +K do
2 Accept player with estimated preferences
3 Observe Y i(j)(t) and update ηˆ i(j)(t),T i(j)(t) if P¯ j(t)=p i
4 Compute UCB( ij) and LCB( ij) if P¯ j(t)=p i
5 if ∃β such that LCB(j) >UCB(j) for any n∈[N −1] then
βn βn+1
6 Preferences = β and B[N +j]=1
7 end
8 t←t+1
9 end
// Gale-Shapley with β =(β ,...,β )
1 N
10 while t̸=T do
11 Accept player using the estimated preference β(j)
12 t←t+1
13 end
A.3.2 CA-ETC
Algorithm 6: Base Algorithm: Expore-then-Gale-Shapley (view of arm a )
j
Input : Exploration rounds 2lT , Horizon blT
◦ ◦
// Learning Preferences
1 for t=Pl l− ′=1 12l′T ◦+1,...,Pl l− ′=1 12l′T ◦+2lT ◦ do
2 Accept player with estimated preferences
3 Observe Y i(j)(t) and update ηˆ i(j)(t),T i(j)(t) if P¯ j(t)=p i
4 end
5 Compute UCB( nj) and LCB( nj) for each n∈[N]
6 if ∃β such that LCB(j) >UCB(j) for any n∈[N −1] then
βn βn+1
7 Preferences = β
8 else
9 Preferences = arbitrary but fixed
10 end
// Perform Gale-Shapley using the Preferences β(j) =β =(β(j),β(j),...,β(j)) found
1 2 N
11 for t=1,2,...,blT ◦−2lT ◦ do
12 Accept player using the estimated preference β(j)
13 end
Algorithm 7: Main Algorithm: Epoch-based CA-ETC (view of player a )
j
Input: Epoch length T , Parameter γ ∈(0,1) with b=21/γ
◦
1 for l=1,2,... do
2 Base Algorithm (Exploration rounds = 2lT ◦, Horizon length = blT ◦)
3 end
15Pagare and Ghosh
A.3.3 Analysis
Theorem A.5 (Regret of Algorithm 7). Suppose CA-ETC is run with initial epoch length T and input
◦
parameter γ ∈(0,1). Then, provided the initial epoch satisfies
(cid:20) (cid:21) 1
32KlogT 1−γ
T > ,
◦ ∆2(T −N)γ
the arm-pessimal regret for arm a is given by
j
(cid:18)
64KlogT
(cid:19)1/γ
RA (t)≤T +4 ∆(j)
j ◦ T (∆(j))2 max
◦
(cid:18) T −N (cid:19)γ (cid:18) T −N (cid:19) 2NKπ2
+2T +1 ∆(j) +K2γlog +1 ∆(j) + ∆(j) .
◦ T max T max 3 max
◦ ◦
The proof of these follow the same steps as the proof for the players, and hence skipped.
16Pagare and Ghosh
A.4 Two-sided learning with (player, arm) broadcast
Assumption A.6. All players and arms observe the successfully accepted player for each arm at the end
of the round.
We consider the above assumption, which allow for each match being broadcasted to every agent. Below
we present a simple extension of Kong & Li (2023) for two-sided learning with these two assumption. We
show that this incurs the same regret bound as the blackboard algorithm presented in the paper. Thus,
our assumption of blackboard can be relaxed with these two assumptions. Note that here we have assumed
that the number of players is less than or equal to number of arms i.e. N ≤K. We comment on the case of
N >K below.
A.4.1 Player’s Learning
Algorithm 8: Expore-then-Gale-Shapley (view of player p )
j
// Perform index estimation
// Learning Preferences
1 for l=1,2,... do
2 Fp =False
l
3 for t=Pl l− ′=1 12l′ +1,...,Pl l− ′=1 12l′ +2l do
4 A j(t)=a (Index+t−1)%K+1 // Round-robin exploration
5 Observe X A(j j) (t)(t) and update µˆ( Aj j) (t),T A(j j)
(t)
if A¯ j(t)=A j(t)
6 end
7 Compute UCB(j) and LCB(j) for each k ∈[K]
k k
8
9
if ∃ Fσ psu =ch Trt uh eat anL dCB p( σj rk) ef> ereU nC cB e( σ sj k) + =1f σor any k ∈[K−1] then
l
10 end
11 Initialize O l =∅
12 A j(t)=a Index if F lp ==True and A j(t)=∅ otherwise
13 Update O l =∪ j′∈[N]{A¯ j′(t)} // set of successfully accepted arms for all players
14 if |O l|==N then
15 Perform Gale-Shapley using the Preferences σ(j) =σ =(σ 1(j),σ 2(j),...,σ K(j)) found; t 2 =t
16 end
17 end
// Gale-Shapley
18 Propose using σ till acceptance
19 Initialize s=1
20 for t=t 2,t 2+1,...,T ◦ do
21 A j(t)=a σs(j)
22 s=s+1 if A¯ j(t)==∅
23 end
17Pagare and Ghosh
A.4.2 Arm’s Learning
Algorithm 9: Base Algorithm: Expore-then-Gale-Shapley (view of arm a )
j
// Learning Preferences
1 for l=1,2,... do
2 F la =False
3 for t=Pl l− ′=1 12l′ +1,...,Pl l− ′=1 12l′ +2l do
4 Accept player with estimated preference ranking
5 Observe Y i(j)(t) and update µˆ( ij),T i(j)(t) if P¯ j(t)=p i
6 end
7 Compute UCB( nj) and LCB( nj) for each n∈[N]
8 if ∃β such that LCB( σj n) >UCB( σj n) +1for any n∈[N −1] then
9 F la =True and preferences = σ
10 end
11 Initialize O l =∅
12 Accept player using estimated preference ranking if F la ==True and reject otherwise
13 Update O l =∪ j′∈[N]{A¯ j′(t)} // set of successfully accepted arms for all players
14 if |O l|==N then
15 Perform Gale-Shapley using the Preferences β(j) =β =(β 1(j),β 2(j),...,β N(j)) found; t=t 2
16 end
17 end
// Gale-Shapley
18 for t=t 2,t 2+1,...,T do
19 Accept player using estimated preference ranking β(j)
20 end
The algorithm proceeds in three phases
– Phase 1:
Player p ’s POV: Propose arm a till it gets matched and denote Index as the time at which it gets
i 1
matched.
Result: Since at the start of the algorithm arm a will have arbitrary preferences over player’s, for
1
each player Index will be the preference of arm a over the players. Note that Index will be distinct
1
for each player.
– Phase 2:
This phase contains sub-phases of increasing duration 2l+1 for sub-phase l.
Exploration: Ineachsub-phaselofduration2l,leteachplayerp toproposeadistinctarminround-
i
robinfashionusingIndex. Basedontheinteraction, playerp andarma updatetheirUCB(i),LCB(i)
i j j j
and UCB(j),LCB(j) resp.
i i
Monitoring: Here,weletplayerp playarma ,ifp hasestimatedanaccuratepreferenceranking
i Index i
over arms. This happens when, for p the confidence interval for each arm a [LCB(i),UCB(i)] does
i j j j
not overlap. 2
After this, player p (arm a ) has to check whether all players and arms has also estimated correct
i j
preference ranking. If yes, we enter Phase 3 else we continue repeating sub-phases in Phase 2.
– Phase 3:
Here the scenario is that every player and arm has estimated correct preference ranking. Thus, at
this phase, each player and arm will run a Gale-Shapley algorithm to find an optimal stable match.
2Similarly,armaj hasestimatedcorrectpreferencerankingifforeachplayerpi [LCB i(j),UCB( ij)]doesnotoverlap.
18Pagare and Ghosh
Now we will see how exactly the monitoring phase proceeds in Phase 2.
Q. How does player p know that all players and arms have estimated correct preference ranking?
i
Ans. In each sub-phase l, players (arms) will maintain their own flag Fp (Fa), which is set to true if they
l l
have estimated corrected preference ranking. Note, since the algorithm is decentralized player p (arm a )
i j
does not know the flag of other players (arms).
Once Fp is true, i.e. player p has found correct preference ranking, it proposes arm a . If arm a
l i Index Index
has found the correct preference ranking as well i.e. Fa is true, then we let a to accept player p , if not,
l Index i
arm a should not accept player p .
Index i
Due to the assumption A.6, each player forms a set A = ∪
{A¯′}
which is set of successfully matched
l i′∈[N] i
arms for all players. If |A | is N then it implies that all players and arms have estimated correct preference
l
ranking.
Q. How does arm a know that all players and arms have estimated correct preference ranking?
j
Ans. Once player p estimates correct preference ranking, it proposes arm a , and if arm a has also
i Index Index
estimated correct preference ranking, it accepts the proposal. Due to the assumption A.6, each arm will
thus form a set A =∪
{A¯′}
which is set of successfully matched arms for all players. If |A | is N then
l i′∈[N] i l
it implies that all players and arms have estimated correct preference ranking.
A.4.3 Comments on the case with N >K
Recall that, N ≤ K was assumed hence we check the cardinality of O set equal to N. When N > K, one
l
may think of checking the cardinality of O with K, however, this extension of the above algorithm becomes
l
non-trivial, as the following may happen.
SupposeK playersandallarmshavefoundcorrectpreferenceranking,butplayerp ,whichisastablematch
i
of some arm, have not found the correct preference ranking. The above algorithm however, will continue
to proceed to Gale-Shapley as the set of successfully accepted arms is K. The extension thus should make
sure that every player has also found the correct preference ranking. To do this, the following scheme can
be used, which will require N rounds.
In each round 1 to N, player with index equal to round number, proposes if for him Fp is true, it gets
l
accepted if the proposed arm has Fa set to true. In this setup, every player and arm should proceed to
l
Gale-Shapley, only when every player in these N rounds got accepted to an arm.
The algorithms based on Blackboard and CA-ETC can be directly used without any modification for N >K.
A.4.4 Analysis
Theorem A.7 (Regret of Algorithm 8). Suppose each player plays Algorithm 8 and each arm plays Algo-
rithm 9 for T iterations. Then, player p incurs the player-optimal regret of
i
(cid:18)
64KlogT
2NKπ2(cid:19)
RP (t)≤ N + +K2+ ∆(i) .
i ∆2 3 max
Theorem A.8 (Regret of Algorithm 9). Suppose each player plays Algorithm 8 and each arm plays Algo-
rithm 9 for T iterations. Then, arm a incurs the arm-pessimal regret of
j
(cid:18)
64KlogT
2NKπ2(cid:19)
RA (t)≤ +K2+ ∆¯(j) .
j ∆2 3 max
The proof of both the theorems are omitted as they exactly follow Kong & Li (2023).
19