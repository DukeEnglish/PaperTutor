SAM2-UNet: Segment Anything 2 Makes Strong
Encoder for Natural and Medical Image
Segmentation
Xinyu Xiong1‚àó, Zihuang Wu2‚àó, Shuangyi Tan3, Wenxue Li4,
Feilong Tang5, Ying Chen6, Siying Li7, Jie Ma1, and Guanbin Li1‚Ä†
1School of Computer Science and Engineering, Sun Yat-sen University
2School of Computer and Information Engineering, Jiangxi Normal University
3The Chinese University of Hong Kong (Shenzhen)
4Tianjin University 5Monash University 6Pazhou Lab
7Smart Hospital Research Institute, Peking University Shenzhen Hospital
Abstract. Image segmentation plays an important role in vision un-
derstanding. Recently, the emerging vision foundation models continu-
ously achieved superior performance on various tasks. Following such
success, in this paper, we prove that the Segment Anything Model 2
(SAM2) can be a strong encoder for U-shaped segmentation models.
We propose a simple but effective framework, termed SAM2-UNet, for
versatileimagesegmentation.Specifically,SAM2-UNetadoptstheHiera
backbone of SAM2 as the encoder, while the decoder uses the classic
U-shapeddesign.Additionally,adaptersareinsertedintotheencoderto
allowparameter-efficientfine-tuning.Preliminaryexperimentsonvarious
downstream tasks, such as camouflaged object detection, salient object
detection,marineanimalsegmentation,mirrordetection,andpolypseg-
mentation, demonstrate that our SAM2-UNet can simply beat existing
specialized state-of-the-art methods without bells and whistles. Project
page: https://github.com/WZH0120/SAM2-UNet.
1 Introduction
Imagesegmentationisacrucialtaskinthefieldofcomputervision,servingasthe
foundation for various visual understanding applications. By dividing an image
into meaningful regions based on specific semantic criteria, image segmentation
enables a wide array of downstream tasks in both natural and medical domains,
such as camouflaged object detection [41,33], salient object detection [48,13],
marine animal segmentation [10,21], mirror detection [14,12], and polyp seg-
mentation [9,57]. Many specialized architectures have been proposed to achieve
superiorperformanceonthesedifferenttasks,whileitremainsanopenchallenge
to design a unified architecture to address the diverse segmentation tasks.
‚àó Authors contributed equally to this work.
‚Ä† Corresponding author.
4202
guA
61
]VC.sc[
1v07880.8042:viXra2 X. Xiong et al.
The emergence of vision foundation models (VFMs) [18,36,45,23] has in-
troduced significant potential in the field of image segmentation. Among these
VFMs, a notable example is the Segment Anything Model (SAM1) [18] and its
successor, Segment Anything 2 (SAM2) [36]. SAM2 builds upon the foundation
laid by SAM1, utilizing a larger dataset for training and incorporating improve-
ments in architectural design. However, despite these advancements, SAM2 still
produces class-agnostic segmentation results when no manual prompt is pro-
vided.Thislimitationhighlightstheongoingchallengeofeffectivelytransferring
SAM2to downstreamtasks, wheretask-specificor class-specificsegmentation is
oftenrequired.ExploringstrategiestoenhanceSAM2‚Äôsadaptabilityandperfor-
mance in these scenarios remains an important area of research.
ToadaptSAMtodownstreamtasks,severalapproacheshavebeenproposed,
including the use of adapters [4,54] for parameter-efficient fine-tuning and the
integration of additional conditional inputs such as text prompts [16,56,22] or
in-contextsamples[55,28].InspiredbythestrongsegmentationcapabilitiesofU-
Net [37] and its variants [58,3,2], some researchers have explored the possibility
oftransformingSAMintoaU-shapedarchitecture[11,50].However,theseefforts
have often been limited by the plain structure of the vanilla ViT encoder [5],
which lacks the hierarchy needed for more sophisticated segmentation tasks.
Fortunately, the introduction of SAM2, which features a hierarchical backbone,
opensnewavenuesfordesigningaU-shapednetworkwithimprovedeffectiveness.
In this paper, we propose SAM2-UNet, the benefit of which is summarized
as follows:
‚Äì Simplicity. SAM2-UNet adopts a classic U-shaped encoder-decoder archi-
tecture, known for its ease of use and high extensibility.
‚Äì Efficiency. Adapters are integrated into the encoder to enable parameter-
efficient fine-tuning, allowing the model to be trained even on memory-
limited devices.
‚Äì Effectiveness. Extensive experiments on eighteen public datasets demon-
stratethatSAM2-UNetdeliverspowerfulperformanceacrossfivechallenging
benchmarks.
2 Method
The overall architecture of SAM2-UNet is illustrated in Fig. 1, comprising four
maincomponents:encoder,decoder,receptivefieldblocks(RFBs),andadapters.
Note that we discard components that are not essential for constructing a basic
U-Net [37], such as memory attention, prompt encoder, memory encoder, and
memory bank.
Encoder.SAM2-UNetapplystheHiera[38]backbonepretrainedbySAM2.
Compared with the plain ViT [5] encoder used in SAM1 [18], Hiera uses a
hierarchical structure that allows multiscale feature capturing, which is more
suitable for designing a U-shaped network. Specifically, given an input imageSAM2-UNet 3
Double Conv C
Decoder
Adapterüî• Up Block üî•
Adapterüî•
Encoder Decoder
Block RFB Block
‚ùÑ
Norm
Encoder Decoder
Attention Block RFB Block
Norm Encoder Decoder
Block RFB Block
MLP
Encoder
Hiera Block Block RFB
Fig.1.OverviewoftheproposedSAM2-UNet.Notethattherearesomevariantsofthe
Hierablock,andweonlydemonstrateasimplifiedstructureforeaseofunderstanding.
I ‚àà R3√óH√óW, where H denotes height and W denotes width, Hiera will out-
put four hierarchical features X
i
‚àà RCi√ó 2iH +1√ó 2iW +1(i ‚àà {1,2,3,4}). For Hiera-L,
C ‚àà{144,288,576,1152}.
i
RFBs. After extracting the encoder features, we pass them through four
receptivefieldblocks[27,9]toreducethechannelnumberto64aswellasenhance
these lightweight features.
Adapters. As the parameters of Hiera may be huge (214M for Hiera-L),
performing full fine-tuning would not always be memory feasible. Therefore,
we freeze the parameters of Hiera and insert adapters before each multi-scale
block of Hiera to achieve parameter-efficient fine-tuning. Similar to the adapter
design in [15,35], each adapter in our framework consists of a linear layer for
downsampling, a GeLU activation function, followed by another linear layer for
upsampling, and a final GeLU activation.
Decoder. The original mask decoder in SAM2 uses a two-way transformer
approachtofacilitatefeatureinteractionbetweenthepromptembeddinganden-
coder features. In contrast, inspired by the highly customizable U-shaped struc-
turethathasproveneffectiveinmanytasks[58,3,2],ourdecoderalsoadheresto
theclassicU-Netdesign.Itconsistsofthreedecoderblocks,eachcontainingtwo
‚ÄòConv-BN-ReLU‚Äô combinations, where ‚ÄòConv‚Äô denotes a 3√ó3 convolution layer
and‚ÄòBN‚Äôrepresentsbatchnormalization. Theoutputfeaturefromeachdecoder
blockpassesthrougha1√ó1Convsegmentationheadtoproduceasegmentation
result S (i ‚àà 1,2,3), which is then upsampled and supervised by the ground
i
truth mask G.
Loss Function.Followingtheapproachesin[9,48],weusetheweightedIoU
lossandbinarycross-entropy(BCE)lossasourtrainingobjectives: L=Lw +
IoU
Lw . Additionally, we apply deep supervision to all segmentation outputs S .
BCE i
The total loss for SAM2-UNet is formulated as: L
=(cid:80)3
L(G,S ).
total i=1 i
nwoD ULeG pU ULeG4 X. Xiong et al.
Table 1. Detailed information of datasets for different tasks.
Tasks Dataset Train Set Test Set
CAMO [19] 1,000 250
COD10K [8] 3,040 2,026
Camouflaged Object Detection
CHAMELEON [40] - 76
NC4K [30] - 4,121
DUTS [44] 10,553 5,019
DUT-OMRON [52] - 5,168
Salient Object Detection HKU-IS [20] - 4,447
PASCAL-S [24] - 850
ECSSD [51] - 1,000
MAS3K [21] 1,769 1,141
Marine Animal Segmentation
RMAS [10] 2,514 500
MSD [53] 3,063 955
Mirror Detection
PMD [25] 5,096 571
Kvasir-SEG [17] 900 100
CVC-ClinicDB [1] 550 62
Polyp Segmentation CVC-ColonDB [42] - 380
CVC-300 [43] - 60
ETIS [39] - 196
3 Experiments
3.1 Datasets and Benchmarks
Ourexperimentsareconductedonfivedifferentbenchmarkswitheighteendatasets
in total, as shown in Table 1:
Camouflaged Object Detectionaimstodetectobjectswellhiddeninthe
environment. We adopt four datasets for benchmarking, including CAMO [19],
COD10K [8], CHAMELEON [40], and NC4K [30]. Four metrics are used for
comparison, including S-measure (S ) [6], adaptive F-measure (F ) [31], mean
Œ± Œ≤
E-measure (E ) [7], and mean absolute error (MAE).
œï
Salient Object Detection aims to mimic human cognition mechanisms
to identify salient objects. We adopt five datasets for benchmarking, including
DUTS [44], DUT-O [52], HKU-IS [20], PASCAL-S [24], and ECSSD [51]. Three
metrics are used for comparison, including S-measure (S ) [6], mean E-measure
Œ±
(E ) [7], and mean absolute error (MAE).
œï
Marine Animal Segmentation focuses on exploring underwater environ-
ments to find marine animals. We adopt two datasets for benchmarking, includ-
ing MAS3K [21] and RMAS [10]. Five metrics are used for comparison, includ-
ing mIoU, S-measure (S ) [6], weighted F-measure (Fw) [31], mean E-measure
Œ± Œ≤
(E ) [7], and mean absolute error (MAE).
œïSAM2-UNet 5
Image GT SINet PFNet ZoomNet FEDER Ours
Fig.2. Visualization results on camouflaged object detection.
Image GT PraNet SANet CaraNet CFA-Net Ours
Fig.3. Visualization results on polyp segmentation.
Mirror Detectioncanidentifythemirrorregionsinthegiveninputimage.
We adopt two datasets for benchmarking, including MSD [53] and PMD [25].
Threemetricsareusedforcomparison,includingIoU,F-measure[31],andmean
absolute error (MAE).
Polyp Segmentation helps in the diagnosis of colorectal cancer. We adopt
five datasets for benchmarking, including Kvasir-SEG [17], CVC-ClincDB [1],
CVC-ColonDB [42], CVC-300 [43], and ETIS [39]. Two metrics are used for
comparison, including mean Dice (mDice) and mean IoU (mIoU).
3.2 Implementation Details
Our method is implemented using PyTorch and trained on a single NVIDIA
RTX 4090 GPU with 24GB of memory. We use the AdamW optimizer with
an initial learning rate of 0.001, applying cosine decay to stabilize training. Two
dataaugmentationstrategiesareemployed:randomverticalandhorizontalflips.
Unless otherwise specified, we use the Hiera-L version of SAM2. All input im-6 X. Xiong et al.
Table 2. Camouflaged object detection performance on CHAMELEON [40] and
CAMO [19] datasets.
CHAMELEON CAMO
Methods
S F E MAE S F E MAE
Œ± Œ≤ œï Œ± Œ≤ œï
SINet [8] 0.872 0.823 0.936 0.034 0.745 0.712 0.804 0.092
PFNet [32] 0.882 0.820 0.931 0.033 0.782 0.751 0.841 0.085
ZoomNet [33] 0.902 0.858 0.943 0.024 0.820 0.792 0.877 0.066
FEDER [13] 0.903 0.856 0.947 0.026 0.836 0.807 0.897 0.066
SAM2-UNet 0.914 0.863 0.961 0.022 0.884 0.861 0.932 0.042
Table 3. Camouflaged object detection performance on COD10K [8] and NC4K [30]
datasets.
COD10K NC4K
Methods
S F E MAE S F E MAE
Œ± Œ≤ œï Œ± Œ≤ œï
SINet [8] 0.776 0.667 0.864 0.043 0.808 0.768 0.871 0.058
PFNet [32] 0.800 0.676 0.877 0.040 0.829 0.779 0.887 0.053
ZoomNet [33] 0.838 0.740 0.888 0.029 0.853 0.814 0.896 0.043
FEDER [13] 0.844 0.748 0.911 0.029 0.862 0.824 0.913 0.042
SAM2-UNet 0.880 0.789 0.936 0.021 0.901 0.863 0.941 0.029
Table 4.SalientobjectdetectionperformanceonDUTS-TE[44],DUT-OMRON[52],
and HKU-IS [20] datasets.
DUTS-TE DUT-OMRON HKU-IS
Methods
S E MAE S E MAE S E MAE
Œ± œï Œ± œï Œ± œï
U2Net [34] 0.874 0.884 0.044 0.847 0.872 0.054 0.916 0.948 0.031
ICON [59] 0.889 0.914 0.037 0.845 0.879 0.057 0.920 0.959 0.029
EDN [49] 0.892 0.925 0.035 0.850 0.877 0.049 0.924 0.955 0.026
MENet [46] 0.905 0.937 0.028 0.850 0.891 0.045 0.927 0.966 0.023
SAM2-UNet 0.934 0.959 0.020 0.884 0.912 0.039 0.941 0.971 0.019
Table 5. Salient object detection performance on PASCAL-S [24] and ECSSD [51]
datasets.
PASCAL-S ECSSD
Methods
S E MAE S E MAE
Œ± œï Œ± œï
U2Net [34] 0.844 0.850 0.074 0.928 0.925 0.033
ICON [59] 0.861 0.893 0.064 0.929 0.954 0.032
EDN [49] 0.865 0.902 0.062 0.927 0.951 0.032
MENet [46] 0.872 0.913 0.054 0.928 0.954 0.031
SAM2-UNet 0.894 0.931 0.043 0.950 0.970 0.020SAM2-UNet 7
Table 6. Marine animal segmentation performance on MAS3K [21] and RMAS [10]
datasets.
MAS3K RMAS
Methods
mIoU S Fw E MAE mIoU S Fw E MAE
Œ± Œ≤ œï Œ± Œ≤ œï
C2FNet [41] 0.717 0.851 0.761 0.894 0.038 0.721 0.858 0.788 0.923 0.026
OCENet [26] 0.667 0.824 0.703 0.868 0.052 0.680 0.836 0.752 0.900 0.030
ZoomNet [33] 0.736 0.862 0.780 0.898 0.032 0.728 0.855 0.795 0.915 0.022
MASNet [10] 0.742 0.864 0.788 0.906 0.032 0.731 0.862 0.801 0.920 0.024
SAM2-UNet 0.799 0.903 0.848 0.943 0.021 0.738 0.874 0.810 0.944 0.022
Table 7. Mirror detection performance on MSD [53] and PMD [25] datasets.
MSD PMD
Methods
IoU F MAE IoU F MAE
MirrorNet [53] 0.790 0.857 0.065 0.585 0.741 0.043
PMD [25] 0.815 0.892 0.047 0.660 0.794 0.032
SANet [12] 0.798 0.877 0.054 0.668 0.795 0.032
HetNet [14] 0.828 0.906 0.043 0.690 0.814 0.029
SAM2-UNet 0.918 0.957 0.022 0.728 0.826 0.027
Table 8. Polyp segmentation performance on Kvasir-SEG [17], CVC-ClinicDB [1],
CVC-ColonDB [42], CVC-300 [43], and ETIS [39] datasets.
Kvasir ClinicDB ColonDB CVC-300 ETIS
Methods
mDice mIoU mDice mIoU mDice mIoU mDice mIoU mDice mIoU
PraNet [9] 0.898 0.840 0.899 0.849 0.709 0.640 0.871 0.797 0.628 0.567
SANet [47] 0.904 0.847 0.916 0.859 0.752 0.669 0.888 0.815 0.750 0.654
CaraNet [29] 0.913 0.859 0.921 0.876 0.775 0.700 0.902 0.836 0.740 0.660
CFA-Net [57] 0.915 0.861 0.933 0.883 0.743 0.665 0.893 0.827 0.732 0.655
SAM2-UNet 0.928 0.879 0.907 0.856 0.808 0.730 0.894 0.827 0.796 0.723
ages are resized to 352√ó352, with a batch size of 12. The training epoch is
set to 50 for camouflaged object detection and salient object detection, and to
20 for marine animal segmentation, mirror detection, and polyp segmentation.
For polyp segmentation, we also adopt a multi-scale training strategy {1,1.25}
similar to [9].
3.3 Comparison with State-of-the-Art Methods
Inthissubsection,wefirstanalyzethequantitativeresultsacrossdifferentbench-
marks,followedbyvisualcomparisonsincamouflagedobjectdetectionandpolyp
segmentation.8 X. Xiong et al.
Results on Camouflaged Object Detection are presented in Tables 2
and 3. SAM2-UNet outperforms all other methods across all four benchmark
datasets,achievingthehighestscoresineverymetric.Specifically,intermsofS-
measure,SAM2-UNetsurpassesFEDERby1.1%ontheCHAMELEONdataset
and by 4.8% on the CAMO dataset. On the more challenging COD10K and
NC4K datasets, which have larger image counts and higher segmentation diffi-
culty, SAM2-UNet still exceeds the performance of FEDER by 3.6% and 3.9%
in S-measure, respectively.
Results on Salient Object Detection are reported in Tables 4 and 5.
SAM2-UNetconsistentlyachievesthetopresultsacrossallmetrics.ForS-measure,
SAM2-UNet outperforms MENet by 2.9%, 3.4%, 1.4%, 2.2%, and 2.2% on the
DUTS-TE, DUT-OMRON, HKU-IS, PASCAL-S, and ECSSD datasets, respec-
tively.
Results on Marine Animal SegmentationaredetailedinTable6.Once
again, SAM2-UNet achieves the best performance across all metrics on the
two benchmark datasets. Specifically, for mIoU, SAM2-UNet outperforms the
second-bestMASNetby5.7%ontheMAS3Kdatasetandby0.7%ontheRMAS
dataset.
Results on Mirror Detection are summarized in Table 7. SAM2-UNet
outshines all other comparison methods in every metric. For instance, SAM2-
UNetsignificantlyoutperformsHetNetintermsofIoUontheMSDdataset,with
a substantial improvement of 9%. Moreover, on the PMD dataset, SAM2-UNet
surpasses HetNet by 3.8% in IoU.
Results on Polyp Segmentation are shown in Table 8. SAM2-UNet
demonstrates state-of-the-art performance on three out of five datasets. For ex-
ample, on the Kvasir dataset, SAM2-UNet achieves a mDice score of 92.8%,
surpassingCFA-Netby1.3%.Additionally,SAM2-UNetdeliversthebestperfor-
mance on ColonDB and ETIS, exceeding CFA-Net by 6.5% and 6.4% in mDice.
Although our performance is weaker on the ClinicDB and CVC-300 datasets,
SAM2-UNet still outperforms CFA-Net by an average of 2.34% in mDice across
all five datasets.
Visual Comparison results are presented in Fig. 2 and 3. In camouflaged
object detection, our method demonstrates superior accuracy across various
scenes, such as detecting a hidden face (row 1), chameleon (row 2), caterpillar
(row 3), and seahorse (row 4). For polyp segmentation, our method effectively
reduces false-positive rates (row 1) and false-negative rates (row 2).
3.4 Ablation Study
To assess the impact of the Hiera backbone size, we conduct ablation experi-
ments, with the results presented in Table 9. Generally, a larger backbone typ-
ically results in better performance. With the smaller Hiera-Base+ backbone,
SAM2-UNet still surpasses FEDER and delivers satisfactory results. As the
backbone size decreases further, SAM2-UNet also produces results comparable
to PFNet and ZoomNet, even with parameter-efficient fine-tuning, demonstrat-SAM2-UNet 9
Table 9. Ablation study about different backbones on COD10K [8] and NC4K [30]
datasets.
COD10K NC4K
Backbones
S F E MAE S F E MAE
Œ± Œ≤ œï Œ± Œ≤ œï
Hiera-Tiny 0.822 0.706 0.883 0.035 0.857 0.804 0.902 0.045
Hiera-Small 0.839 0.729 0.900 0.031 0.869 0.822 0.913 0.040
Hiera-Base+ 0.853 0.749 0.910 0.027 0.879 0.833 0.920 0.037
Hiera-Large 0.880 0.789 0.936 0.021 0.901 0.863 0.941 0.029
ing the high-quality representations provided by the SAM2 pre-trained Hiera
backbone.
4 Conclusion
In this paper, we propose SAM2-UNet, a simple yet effective U-shaped frame-
workforversatilesegmentationacrossbothnaturalandmedicaldomains.SAM2-
UNetisdesignedforeaseofunderstandinganduse,featuringaSAM2pre-trained
Hiera encoder coupled with a classic U-Net decoder. Extensive experiments
across eighteen datasets on five benchmarks demonstrate the effectiveness of
SAM2-UNet.OurSAM2-UNetcanserveasanewbaselinefordevelopingfuture
SAM2 variants.
References
1. Bernal, J., S√°nchez, F.J., Fern√°ndez-Esparrach, G., Gil, D., Rodr√≠guez, C., Vilar-
i√±o,F.:Wm-dovamapsforaccuratepolyphighlightingincolonoscopy:Validation
vs. saliency maps from physicians. Computerized Medical Imaging and Graphics
43, 99‚Äì111 (2015)
2. Cao, H., Wang, Y., Chen, J., Jiang, D., Zhang, X., Tian, Q., Wang, M.: Swin-
unet: Unet-like pure transformer for medical image segmentation. In: ECCVW.
pp. 205‚Äì218. Springer (2022)
3. Chen,J.,Mei,J.,Li,X.,Lu,Y.,Yu,Q.,Wei,Q.,Luo,X.,Xie,Y.,Adeli,E.,Wang,
Y., et al.: Transunet: Rethinking the u-net architecture design for medical image
segmentation through the lens of transformers. Medical Image Analysis p. 103280
(2024)
4. Chen, T., Zhu, L., Deng, C., Cao, R., Wang, Y., Zhang, S., Li, Z., Sun, L., Zang,
Y., Mao, P.: Sam-adapter: Adapting segment anything in underperformed scenes.
In: ICCVW. pp. 3367‚Äì3375 (2023)
5. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T.,Dehghani,M.,Minderer,M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,Houlsby,N.:
An image is worth 16x16 words: Transformers for image recognition at scale. In:
ICLR (2021)
6. Fan,D.P.,Cheng,M.M.,Liu,Y.,Li,T.,Borji,A.:Structure-measure:Anewway
to evaluate foreground maps. In: ICCV. pp. 4548‚Äì4557 (2017)10 X. Xiong et al.
7. Fan, D.P., Ji, G.P., Qin, X., Cheng, M.M.: Cognitive vision inspired object seg-
mentation metric and loss function. Scientia Sinica Informationis 6(6), 5 (2021)
8. Fan,D.P.,Ji,G.P.,Sun,G.,Cheng,M.M.,Shen,J.,Shao,L.:Camouflagedobject
detection. In: CVPR. pp. 2777‚Äì2787 (2020)
9. Fan, D.P., Ji, G.P., Zhou, T., Chen, G., Fu, H., Shen, J., Shao, L.: Pranet: Par-
allel reverse attention network for polyp segmentation. In: MICCAI. pp. 263‚Äì273.
Springer (2020)
10. Fu, Z., Chen, R., Huang, Y., Cheng, E., Ding, X., Ma, K.K.: Masnet: A robust
deep marine animal segmentation network. IEEE Journal of Oceanic Engineering
(2023)
11. Gao,Y.,Xia,W.,Hu,D.,Gao,X.:Desam:Decouplingsegmentanythingmodelfor
generalizablemedicalimagesegmentation.arXivpreprintarXiv:2306.00499(2023)
12. Guan,H.,Lin,J.,Lau,R.W.:Learningsemanticassociationsformirrordetection.
In: CVPR. pp. 5941‚Äì5950 (2022)
13. He, C., Li, K., Zhang, Y., Tang, L., Zhang, Y., Guo, Z., Li, X.: Camouflaged
object detection with feature decomposition and edge reconstruction. In: CVPR.
pp. 22046‚Äì22055 (2023)
14. He,R.,Lin,J.,Lau,R.W.:Efficientmirrordetectionviamulti-levelheterogeneous
learning. In: AAAI. vol. 37, pp. 790‚Äì798 (2023)
15. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Ges-
mundo, A., Attariyan, M., Gelly, S.: Parameter-efficient transfer learning for nlp.
In: ICML. pp. 2790‚Äì2799. PMLR (2019)
16. Huang, D., Xiong, X., Ma, J., Li, J., Jie, Z., Ma, L., Li, G.: Alignsam: Aligning
segment anything model to open context via reinforcement learning. In: CVPR.
pp. 3205‚Äì3215 (2024)
17. Jha,D.,Smedsrud,P.H.,Riegler,M.A.,Halvorsen,P.,DeLange,T.,Johansen,D.,
Johansen, H.D.: Kvasir-seg: A segmented polyp dataset. In: MMM. pp. 451‚Äì462.
Springer (2020)
18. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. In: ICCV. pp.
4015‚Äì4026 (2023)
19. Le, T.N., Nguyen, T.V., Nie, Z., Tran, M.T., Sugimoto, A.: Anabranch network
forcamouflagedobjectsegmentation.ComputerVisionandImageUnderstanding
184, 45‚Äì56 (2019)
20. Li, G., Yu, Y.: Visual saliency based on multiscale deep features. In: CVPR. pp.
5455‚Äì5463 (2015)
21. Li,L.,Dong,B.,Rigall,E.,Zhou,T.,Dong,J.,Chen,G.:Marineanimalsegmen-
tation. IEEE Transactions on Circuits and Systems for Video Technology 32(4),
2303‚Äì2314 (2021)
22. Li, W., Xiong, X., Xia, P., Ju, L., Ge, Z.: Tp-drseg: Improving diabetic retinopa-
thy lesion segmentation with explicit text-prompts assisted sam. arXiv preprint
arXiv:2406.15764 (2024)
23. Li, X., Yuan, H., Li, W., Ding, H., Wu, S., Zhang, W., Li, Y., Chen, K., Loy,
C.C.: Omg-seg: Is one model good enough for all segmentation? In: CVPR. pp.
27948‚Äì27959 (2024)
24. Li, Y., Hou, X., Koch, C., Rehg, J.M., Yuille, A.L.: The secrets of salient object
segmentation. In: CVPR. pp. 280‚Äì287 (2014)
25. Lin, J., Wang, G., Lau, R.W.: Progressive mirror detection. In: CVPR. pp. 3697‚Äì
3705 (2020)
26. Liu, J., Zhang, J., Barnes, N.: Modeling aleatoric uncertainty for camouflaged
object detection. In: WACV. pp. 1445‚Äì1454 (2022)SAM2-UNet 11
27. Liu, S., Huang, D., et al.: Receptive field block net for accurate and fast object
detection. In: ECCV. pp. 385‚Äì400 (2018)
28. Liu,Y.,Zhu,M.,Li,H.,Chen,H.,Wang,X.,Shen,C.:Matcher:Segmentanything
with one shot using all-purpose feature matching. In: ICLR (2024)
29. Lou, A., Guan, S., Ko, H., Loew, M.H.: Caranet: context axial reverse attention
network for segmentation of small medical objects. In: SPIE MI. vol. 12032, pp.
81‚Äì92. SPIE (2022)
30. Lv, Y., Zhang, J., Dai, Y., Li, A., Liu, B., Barnes, N., Fan, D.P.: Simultaneously
localize, segment and rank the camouflaged objects. In: CVPR. pp. 11591‚Äì11601
(2021)
31. Margolin, R., Zelnik-Manor, L., Tal, A.: How to evaluate foreground maps? In:
CVPR. pp. 248‚Äì255 (2014)
32. Mei, H., Ji, G.P., Wei, Z., Yang, X., Wei, X., Fan, D.P.: Camouflaged object seg-
mentation with distraction mining. In: CVPR. pp. 8772‚Äì8781 (2021)
33. Pang,Y.,Zhao,X.,Xiang,T.Z.,Zhang,L.,Lu,H.:Zoominandout:Amixed-scale
tripletnetworkforcamouflagedobjectdetection.In:CVPR.pp.2160‚Äì2170(2022)
34. Qin, X., Zhang, Z., Huang, C., Dehghan, M., Zaiane, O.R., Jagersand, M.: U2-
net: Going deeper with nested u-structure for salient object detection. Pattern
Recognition 106, 107404 (2020)
35. Qiu, Z., Hu, Y., Li, H., Liu, J.: Learnable ophthalmology sam. arXiv preprint
arXiv:2304.13425 (2023)
36. Ravi, N., Gabeur, V., Hu, Y.T., Hu, R., Ryali, C., Ma, T., Khedr, H., R√§dle, R.,
Rolland,C.,Gustafson,L.,etal.:Sam2:Segmentanythinginimagesandvideos.
arXiv preprint arXiv:2408.00714 (2024)
37. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomed-
ical image segmentation. In: MICCAI. pp. 234‚Äì241. Springer (2015)
38. Ryali, C., Hu, Y.T., Bolya, D., Wei, C., Fan, H., Huang, P.Y., Aggarwal, V.,
Chowdhury, A., Poursaeed, O., Hoffman, J., et al.: Hiera: A hierarchical vision
transformer without the bells-and-whistles. In: ICML. pp. 29441‚Äì29454. PMLR
(2023)
39. Silva,J.,Histace,A.,Romain,O.,Dray,X.,Granado,B.:Towardembeddeddetec-
tion of polyps in wce images for early diagnosis of colorectal cancer. International
Journal of Computer Assisted Radiology and Surgery 9, 283‚Äì293 (2014)
40. Skurowski,P.,Abdulameer,H.,B≈Çaszczyk,J.,Depta,T.,Kornacki,A.,Kozie≈Ç,P.:
Animal camouflage analysis: Chameleon database. Unpublished Manuscript 2(6),
7 (2018)
41. Sun, Y., Chen, G., Zhou, T., Zhang, Y., Liu, N.: Context-aware cross-level fusion
network for camouflaged object detection. In: IJCAI. pp. 1025‚Äì1031 (2021)
42. Tajbakhsh,N.,Gurudu,S.R.,Liang,J.:Automatedpolypdetectionincolonoscopy
videosusingshapeandcontextinformation.IEEETransactionsonMedicalImag-
ing 35(2), 630‚Äì644 (2015)
43. V√°zquez, D., Bernal, J., S√°nchez, F.J., Fern√°ndez-Esparrach, G., L√≥pez, A.M.,
Romero, A., Drozdzal, M., Courville, A.: A benchmark for endoluminal scene
segmentation of colonoscopy images. Journal of Healthcare Engineering 2017(1),
4037190 (2017)
44. Wang, L., Lu, H., Wang, Y., Feng, M., Wang, D., Yin, B., Ruan, X.: Learning to
detectsalientobjectswithimage-levelsupervision.In:CVPR.pp.136‚Äì145(2017)
45. Wang, X., Zhang, X., Cao, Y., Wang, W., Shen, C., Huang, T.: Seggpt: Towards
segmenting everything in context. In: ICCV. pp. 1130‚Äì1140 (2023)12 X. Xiong et al.
46. Wang, Y., Wang, R., Fan, X., Wang, T., He, X.: Pixels, regions, and objects:
Multiple enhancement for salient object detection. In: CVPR. pp. 10031‚Äì10040
(2023)
47. Wei, J., Hu, Y., Zhang, R., Li, Z., Zhou, S.K., Cui, S.: Shallow attention network
for polyp segmentation. In: MICCAI. pp. 699‚Äì708. Springer (2021)
48. Wei, J., Wang, S., Huang, Q.: F3net: fusion, feedback and focus for salient object
detection. In: AAAI. pp. 12321‚Äì12328 (2020)
49. Wu,Y.H.,Liu,Y.,Zhang,L.,Cheng,M.M.,Ren,B.:Edn:Salientobjectdetection
viaextremely-downsamplednetwork.IEEETransactionsonImageProcessing31,
3125‚Äì3136 (2022)
50. Xiong, X., Wang, C., Li, W., Li, G.: Mammo-sam: Adapting foundation segment
anything model for automatic breast mass segmentation in whole mammograms.
In: MLMI. pp. 176‚Äì185. Springer (2023)
51. Yan, Q., Xu, L., Shi, J., Jia, J.: Hierarchical saliency detection. In: CVPR. pp.
1155‚Äì1162 (2013)
52. Yang, C., Zhang, L., Lu, H., Ruan, X., Yang, M.H.: Saliency detection via graph-
based manifold ranking. In: CVPR. pp. 3166‚Äì3173 (2013)
53. Yang, X., Mei, H., Xu, K., Wei, X., Yin, B., Lau, R.W.: Where is my mirror? In:
ICCV. pp. 8809‚Äì8818 (2019)
54. Zhang, K., Liu, D.: Customized segment anything model for medical image seg-
mentation. arXiv preprint arXiv:2304.13785 (2023)
55. Zhang, R., Jiang, Z., Guo, Z., Yan, S., Pan, J., Dong, H., Qiao, Y., Gao, P., Li,
H.: Personalize segment anything model with one shot. In: ICLR (2024)
56. Zhang,Y.,Cheng,T.,Hu,R.,Liu,H.,Ran,L.,Chen,X.,Liu,W.,Wang,X.,etal.:
Evf-sam:Earlyvision-languagefusionfortext-promptedsegmentanythingmodel.
arXiv preprint arXiv:2406.20076 (2024)
57. Zhou, T., Zhou, Y., He, K., Gong, C., Yang, J., Fu, H., Shen, D.: Cross-level
feature aggregation network for polyp segmentation. Pattern Recognition 140,
109555 (2023)
58. Zhou,Z.,Siddiquee,M.M.R.,Tajbakhsh,N.,Liang,J.:Unet++:Redesigningskip
connections to exploit multiscale features in image segmentation. IEEE Transac-
tions on Medical Imaging 39(6), 1856‚Äì1867 (2019)
59. Zhuge, M., Fan, D.P., Liu, N., Zhang, D., Xu, D., Shao, L.: Salient object detec-
tion via integrity learning. IEEE Transactions on Pattern Analysis and Machine
Intelligence 45(3), 3738‚Äì3752 (2022)