Misclassification excess risk bounds for PAC-Bayesian clas-
sification via convexified loss
The Tien Mai the.t.mai@ntnu.no
Department of Mathematical Sciences,
Norwegian University of Science and Technology,
Trondheim 7034, Norway.
Abstract
PAC-Bayesian bounds have proven to be a valuable tool for deriving generalization
bounds and for designing new learning algorithms in machine learning. However,
it typically focus on providing generalization bounds with respect to a chosen loss
function. In classification tasks, due to the non-convex nature of the 0-1 loss, a
convex surrogate loss is often used, and thus current PAC-Bayesian bounds are
primarily specified for this convex surrogate. This work shifts its focus to providing
misclassification excess risk bounds for PAC-Bayesian classification when using a
convex surrogate loss. Our key ingredient here is to leverage PAC-Bayesian relative
bounds in expectation rather than relying on PAC-Bayesian bounds in probability.
We demonstrate our approach in several important applications.
Keyword: binaryclassification, PAC-Bayes bounds,predictionbounds,misclassification excessrisk,
convex surrogate loss
1 Introduction and motivation
Building on the foundational works initiated by Shawe-Taylor and Williamson (1997); McAllester
(1998; 1999), PAC-Bayesian theory has become a crucial framework not only for deriving general-
ization boundsbutalso for developing novel learning algorithms in machine learning (Catoni, 2007;
Guedj, 2019; Alquier, 2024; Rivasplata, 2022). While PAC-Bayesian bounds traditionally address
riskboundswithrespecttoaspecificlossfunction,inclassification tasks, theinherentlynon-convex
and non-smooth nature of the 0-1 loss necessitates the use of a convex surrogate loss to facilitate
computation (Zhang, 2004; Bartlett et al., 2006). Several PAC-Bayes studies have addressed this
byincorporatingconvex surrogatelosses: Dalalyan and Tsybakov (2012a)andAlquier et al.(2016)
both focusing on risk boundsfor the convexified loss. Although recent research has made strides in
using PAC-Bayesian techniques to establish prediction bounds in classification, these efforts have
not succeeded in providing misclassification risk bounds (Cottet and Alquier, 2018; Mai, 2023;
2024). This paper aims to address this gap by focusing on misclassification excess risk bounds in
PAC-Bayesian classification using a convex surrogate loss.
We formally consider the following general binary classification. Given a covariate/feature X ,
∈ X
one has that the class label Y = 1 with probability p(X), and Y = 1 with probability 1 p(X).
− −
The accuracy of a classifier η is defined by the prediction or misclassification error, given as
R (η) = P(Y = η(X)).
0/1
6
1
4202
guA
61
]LM.tats[
1v57680.8042:viXraTheBayesclassifier,η (X) = sign(p(X) 1/2), iswidelyrecognizedforminimizingR (η)(Vapnik,
∗ 0/1
−
1998; Devroye et al., 1996), i.e.
R := R (η ) = infR (η).
0∗/1 0/1 ∗ 0/1
With p(X) being unknown, a classifier ηˆ(X) needs to be designed using the available data: a
random sample of n independent observations D = (x ,y ), ..., (x ,y ) . The design points
n 1 1 n n
{ }
x may be considered as fixed or random. The corresponding (conditional) prediction error of ηˆ is
i
now as
R (ηˆ) = P(Y = ηˆ(X) D )
0/1 n
6 |
and the goodness of ηˆ with respect to η is measured by the misclassification excess risk, defined
∗
as
ER (ηˆ) R = ER (ηˆ) R (η ).
0/1
−
0∗/1 0/1
−
0/1 ∗
The empirical risk minimization method is a general nonparametric approach to determine a clas-
sifier ηˆ from data, where the true prediction error R (η) minimization is replaced by the mini-
0/1
0/1
mization of the empirical risk r over a specified class of classifiers, η : 1,1 , θ Θ ,
n θ
{ X → {− } ∈ }
0/1
where r is given by:
n
n
1
r0/1(θ)= y = η (x ) .
n n { i 6 θ i }
i=1
X
PAC-Bayesian classification using the 0-1 loss was thoroughly examined in a series of works by
Olivier Catoni over 20 years ago, in Catoni (2003; 2004; 2007). However, due to the computational
challenges posed by the non-convexity of the zero-one loss function, particularly when dealing
with huge and/or high-dimensional data, a convex surrogate loss is often preferred to simplify the
computational problem. The convex surrogate loss in PAC-Bayesian approach for classification
has been considered in various studies. For example, Alquier et al. (2016) explored a variational
inference approach for PAC-Bayesian methods, emphasizing the importance of convexified loss,
while Dalalyan and Tsybakov (2012a) and Mai (2024) investigated PAC-Bayesian classification
using convex surrogate loss and gradient-based sampling methods such as Langevin Monte Carlo.
PAC-Bayesian bounds as in Alquier (2024), when using a convexified loss, often leads to prediction
bounds or excess risk with respect to the convexified loss. To the best of our knowledge, misclassi-
fication excess risk boundsfor PAC-Bayesian classification whenusing convexified loss have not yet
been established. In this work, we provide a unified procedure to obtain such results. Our work is
carried out underthe so-called low-noise condition. The low-noise condition described is a common
assumption in the classification literature, as seen in works such as Mammen and Tsybakov (1999);
Tsybakov (2004); Bartlett et al. (2006). Themain challenge for any classifier typically lies near the
decision boundary x :p(x) = 1/2 . In this region, accurately predicting the class label is particu-
{ }
larly difficult because the label information is predominantly noisy. Given this, it is reasonable to
assume that p(x) is unlikely to be very close to 1/2.
In the subsequent section, Section 2, we introduce our primary notations and present our main re-
sults. InSection3,weapplyourgeneralproceduretotwosignificantapplications: high-dimensional
sparseclassification and1-bitmatrixcompletion. Tothebestofourknowledge, theresultsobtained
for these two problems are novel. We conclude our work in Section 4, while all technical proofs are
provided in Appendix A.
2
12 Main result
2.1 PAC-Bayesian framework
We observe an i.i.d sample (X ,Y ),...,(X ,Y ), of a random pair (X,Y) taking values in
1 1 n n
X ×
1,1 , from the same distribution P. A set of classifiers is chosen by the user: η :
θ
{− } { X →
1,1 , θ Θ . For example, one may have η (x) = sign( θ,x ) 1,1 . In this paper, the
θ
{− } ∈ } h i ∈ {− }
symbol E will always denote the expectation with respect to the (unknown) law P of the (X ,Y )’s.
i i
Consider a convex loss surrogate function φ: R2 R+, the empirical convex risk is defined as
→
n n
1 1
rφ(θ):= φ (θ):= φ(Y ,η (X )),
n n i n i θ i
i=1 i=1
X X
and its expected risk is given as Rφ(θ)= E[φ(Y,η (X))].
θ
Convex loss functions commonly used in classification include logistic loss and hinge loss. More
examples can be found for example in Bartlett et al. (2006).
Let (Θ) denote the set of all probability measures on Θ. We define a prior probability measure
P
π() on the setΘ. For any λ > 0, as in the PAC-Bayesian framework Catoni (2007); Alquier (2024),
·
φ
the Gibbs posterior ρˆ , with respect to the convex loss φ, is defined by
λ
φ
exp[ λr (θ)]
φ n
ρˆ (dθ)= − π(dθ), (1)
λ φ
exp[ λr ]dπ
n
−
R
and our mean estimator is defined by θˆ= θρˆφ (dθ). From now, we will let θ denote a minimizer
λ ∗
of Rφ when it exists: Rφ(θ ) = min Rφ(θ).
∗ θ Θ
∈ R
In PAC-Bayes theory, when utilizing a φ-loss function, it is customary to regulate the excess φ-risk,
Rφ(θ) Rφ(θ )
∗
−
see e.g. Alquier (2024). However, in classification tasks, it is equally crucial to control the misclas-
sification excess risk, ER (θ) R , which is the primary focus of this paper.
0/1
−
0∗/1
2.2 Main result
2.2.1 Assumptions
Certain conditions are essential for deriving our main result.
Assumption 1 (Bounded loss). The convex surrogate loss function φ is assumed to be bounded,
with its values lying in the range [0,B].
Assumption 2 (Lipschitz loss). We assume that the loss function φ(y, ) is L-Lipschitz in the
·
sense that there exist some constant L > 0 such that φ(y,η θ(x)) φ(y,η θ′(x)) L θ θ
′
.
| − | ≤ k − k
Assumption 3 (Bernstein condition). Assuming that there is a constant K > 0 such that, for any
θ Θ, θ θ 2 K[Rφ(θ) Rφ(θ )].
∈ k − ∗ k2 ≤ − ∗
Assumption 4 (Margin condition). We assume that there exist a constant c> 0 such that
P 0 < p(X) 1/2 < 1/(2c) = 0.
{ | − | }
3The boundedness condition in Assumption 1 is not central to our analysis; rather, it serves to
simplify the presentation and enhance the clarity of the paper. It is important to note that PAC-
Bayesian bounds can also be derived for unbounded loss functions, as discussed in Alquier (2024).
Assumption 2 and 3 have been extensively studied in various forms in the learning theory litera-
ture, such as (Mendelson, 2008; Zhang, 2004; Alquier et al., 2019; Elsener and van de Geer, 2018;
Alaya and Klopp, 2019). Some examples of the loss functions that are 1-Lipschitz are: hinge loss
φ(y,y ) = max(0,1 yy ) and logistic loss φ(y,y ) = log(1+exp( yy )). Assumption 3 implicitly
′ ′ ′ ′
− −
means that our predictors are identifiability.
Remark 1. It is worth noting that our Bernstein condition in Assumption 3 is slightly stronger
than the one considered in Alquier (2024). Specifically, Definition 4.1 in Alquier (2024) defines a
Bernstein condition where there exists a constant K > 0 such that for any θ Θ,
∈
E [φ (θ) φ (θ )]2 K[Rφ(θ) Rφ(θ )].
i i ∗ ∗
− ≤ −
n o
Therefore, if we additionally assume that the loss function φ in our context is further L-Lipschitz,
then E [φ (θ) φ (θ )]2 L2E θ θ 2 L2K[Rφ(θ) Rφ(θ )], which satisfies Definition 4.1
i − i ∗ ≤ k − ∗ k2 ≤ − ∗
in Alqunier (2024). o
The low-noise condition described in Assumption 4 is a common assumption in the classifica-
tion literature, as seen in works such as (Abramovich and Grinshtein, 2018; Tsybakov, 2004;
Mammen and Tsybakov, 1999; Bartlett et al., 2006). The main challenge for any classifier typ-
ically lies near the decision boundary x : p(x) = 1/2 , which in logistic regression corresponds to
the hyperplaneθ x = 0, wherep(x)
={
(1+e
θ⊤x) 1.}
In this region, accurately predicting the class
⊤ − −
label is particularly difficult because the label information is predominantly noisy. Given this, it is
reasonable to assume that p(x) is unlikely to be very close to 1/2.
2.2.2 Main results
While high probability PAC-Bayes bounds for the excess φ-risk, Rφ(θ) Rφ(θ ), are frequently
∗
−
discussed in the literature (see e.g. Alquier (2024)), PAC-Bayes bounds in expectation have re-
ceived comparatively less attention. Utilizing high probability PAC-Bayes bounds for deriving
prediction bounds has also been explored to some extent, as evidenced by several works such as
Cottet and Alquier (2018); Mai (2023; 2024). However, these approaches often do not provide
bounds for misclassification excess risk unless under strictly noiseless conditions.
In this study, we illustrate the utility of PAC-Bayes bounds in expectation for deriving misclas-
sification excess risk bounds. Specifically, we first introduce a PAC-Bayesian relative bound in
expectation, which is a slight extension of Theorem 4.3 in Alquier (2024). For two probability
distributions µ and ν in (Θ), let (ν µ) denote the Kullback-Leibler divergence from ν to µ.
P K k
Put C := max(2L2K,B).
Theorem 1. Assuming that Assumptions 1, 2 and 3 are satisfied, let’s take λ = n/C. Then we
have:
C (ρ π)
E[E
θ ∼ρˆφ
λ[Rφ(θ)]] −Rφ(θ ∗)
≤
2
ρ
∈i Pn (f
Θ)
(cid:26)E
θ
∼ρ[Rφ(θ)] −Rφ(θ ∗)+ K nk (cid:27).
The proof is given in Appendix A. As discussed in Catoni (2007); Alquier (2024), the bound in
Theorem1 can beemployed to deriveerror rates fortheexcess φ-risk inageneral setting as follows:
4one needs to find a ρ such that E [Rφ(θ)] Rφ(θ )+ ǫ and ensure that (ρ π) ǫ to obtain:
ǫ θ ∼ρǫ ≃ ∗ n K ǫ k ≃
E[E [Rφ(θ)]] . Rφ(θ )+ ǫ + 2Cǫ. Hence the rate is of order 1/n.
θ ∼ρˆλ ∗ n n
Remark 2. One can derive a PAC-Bayesian relative bound without invoking the Bernstein con-
dition from Assumption 3, see e.g Alquier (2024). Nevertheless, this results in a slower conver-
gence rate of order n 1/2. In contrast, under the low-noise condition specified in Assumption 4,
−
which is our primary assumption, it is well-known that a faster rate of order 1/n can be obtained
Abramovich and Grinshtein (2018); Tsybakov (2004). Hence, the need for imposing the Bernstein
condition in Assumption 3 becomes crucial.
The following theorem presents our main results on misclassification excess risk bounds for PAC-
Bayesian classification approaches using convexified loss. The strategy involves utilizing a broad
result from Bartlett et al. (2006). To establish our main result presented in Theorem 2 below, we
furtherassumethattheφ-lossfunctionisclassification-calibrated. Specifically,forζ [0,1],ζ = 1/2,
∈ 6
the following condition must hold:
inf C (α) < inf C (α),
ζ ζ
α R α:α(2ζ 1) 0
∈ − ≤
where C (α) = ζφ(α)+(1 ζ)φ( α). This is a minimal requirement, indicating that the φ-loss
ζ
− −
function possesses the same capacity for classification as the Bayes classifier. For a more detailed
discussion, refer to Bartlett et al. (2006).
Theorem 2. Assuming both Theorem 1 and Assumption 4 hold, and by selecting λ = n/C, there
exists a constant Ψ > 0 such that
C (ρ π)
E[E θ ∼ρˆφ λ[R 0/1(θ)]] −R 0∗/1 ≤ Ψ ρ ∈i Pn (f Θ) (cid:26)E θ ∼ρ[Rφ(θ)] −Rφ(θ ∗)+ K nk (cid:27), (2)
and
C (ρ π)
E[R 0/1(θˆ)] −R 0∗/1
≤
Ψ
ρ
in (f
Θ)
E θ ∼ρ[Rφ(θ)] −Rφ(θ ∗)+ K nk . (3)
∈P (cid:26) (cid:27)
Remark 3. Similar to Theorem 1, the bound in Theorem 2 can be utilized to derive general mis-
classification error rates. For instance, since the bound in (2) holds for any ρ (Θ), one can
specify a distribution ρ such that E [Rφ(θ)] Rφ(θ ) . δ/n and that (ρ π∈ ) .P δ and conse-
δ θ ∼ρδ
−
∗
K
δ
k
quently: E[E [R (θ)]] R . δ + 2Cδ, hence the misclassification excess rate can be of the
θ ∼ρˆφ λ 0/1 − 0∗/1 n n
order 1/n. Some classical examples are given below.
From Theorem 2, we immediately obtain the following corollary regarding the ℓ error for the
2
predictor.
Corollary 1. Assuming that Theorem 2 is satisfied and let’s take λ = n/C. Then, with some
universal constant C > 0, we have that
(ρ π)+log 2
EE θ θ 2 C inf E [Rφ(θ) Rφ(θ )]+ K k ε .
θ ∼ρˆφ λ k − ∗ k2 ≤ ρ (Θ)( θ ∼ρ − ∗ λ )
∈P
(cid:2) (cid:3)
With the same rationale as provided in Remark 3, some error rates can be obtained from Corollary
1.
Remark 4. It is crucial to recognize that, in the absence of Assumption 4, one may not achieve
a result analogous to Theorem 2. For instance, as demonstrated by Zhang (2004), for the logistic
loss, E[R (θ)] R . (E[Rφ(θ)] Rφ(θ ))1/2. Consequently, it is generally unlikely to derive a
0/1
−
0∗/1
−
∗
comparable result for PAC-Bayesian methods without employing Assumption 4.
5Examples
We now demonstrate that using Theorem 2 can yield bounds on the misclassification excess risk in
various scenarios. Further non-trivial applications are discussed in Section 3.
Example 1 (Finite case). Let us begin with the special case where Θ is a finite set, specifically,
φ
card(Θ) = M < + . In this scenario, the Gibbs posterior ρˆ of (1) is a probability distribution
∞ λ
over the finite set Θ defined by
e
λrnφ(θ)π(θ)
φ −
ρˆ (θ)= .
λ
e
λrnφ(ϑ)π(ϑ)
ϑ Θ −
∈
As the bounds in (2) and (3) hold for all ρP (Θ), it holds in particular for all ρ in the set of
∈ P
Dirac masses δ ,θ Θ . That
θ
{ ∈ }
C (ρ π)
E[E θ ∼ρˆφ λ[R 0/1(θ)]] −R 0∗/1 ≤ Ψ θi ∈n Θf (cid:26)Rφ(θ) −Rφ(θ ∗)+ K nk (cid:27),
and in particular, for θ = θ , this becomes
∗
C (δ π)
E[E θ ∼ρˆφ λ[R 0/1(θ)]] −R 0∗/1 ≤ Ψ K nθ k ,
And, (δ π) = log
δθ(θ′)
δ (θ ) = log 1 . This gives us an insight into the role of the
K θ k θ′ Θ π(θ′) θ ′ π(θ)
∈
measure π: the bound will b(cid:16)e tight(cid:17)er for θ values where π(θ) is large. However, π cannot be large
P
everywhere because it is a probability distribution and that π(θ) = 1. The larger the set Θ,
θ Θ
the more this total sum of 1 will be spread out, resulting in larg∈e values of log(1/π(θ)). If π is the
P
uniform probability distribution, then log(1/π(θ)) = log(M), and the previous bound becomes
log(M)
E[E [R (θ)]] R ΨC .
θ ∼ρˆφ λ 0/1 − 0∗/1 ≤ n
Thus, in this case, the misclassification excess risk is of order log(M)/n.
Example 2. Now, we consider the continuous case where Θ = Rd, the loss function is Lipschitz,
and the prior π is a centered Gaussian: (0,σ2I ), where I denotes the d d identity matrix.
d d
N ×
When applying Theorem 2, the right-hand side in (2) involves an infimum over all ρ (Θ).
∈ P
However, for simplicity and practicality, it is advantageous to consider Gaussian distributions as
ρ= ρ = (m,s2I ) with m Rd,s > 0.
m,s d
N ∈
First, it is well known that, K(ρ m,s kπ) = k 2m σk22 + d 2 σs2 2 +log(σ s22 ) −1 . Moreover, the risk Rφ
inherits the Lipschitz property of the loss, that is, for ahny (θ,ϑ) Θ2, Riφ(θ) Rφ(ϑ) L ϑ θ .
∈ − ≤ k − k
And, by Jensen’s inequality, that E ϑ θ E [ θ m 2] s√d. Consequently,
θ ∼ρm,sk
− k ≤
θ ∼ρm,s
k − k ≤
putting all thing together, with m =θ ∗ q
E[E [R (θ)]] R Ψinf Ls√d+C k 2θ σ∗ k22 + d 2 σs2 2 +log(σ s22 ) −1 .
θ ∼ρˆφ λ 0/1 − 0∗/1 ≤ s>0 h n i
 
Taking s = 1/(n√d),  
E[E [R (θ)]] R Ψ L +C k 2θ σ∗ k22 + d 2 n2d1 σ2 +log(n2dσ2) −1 . dlog(n) .
θ ∼ρˆφ λ 0/1 − 0∗/1 ≤ (n (cid:2) n (cid:3)) n
Thus, in this case, the misclassification excess risk is of order dlog(n)/n.
63 Application
We note that our procedure is applicable to different classification contexts. Here, we will demon-
strate it with the following two important examples.
3.1 High dimensional sparse classifcation
In this context, we have that = Rd and that d > n. Consider the class of linear classifiers,
X
the empirical risk is now given by: r0/1 (θ) = 1 n Y (θ X ) < 0 , and the prediction risk
n n i=1 { i ⊤ i }
R (θ) = E r0/1 (θ) . For the sake of simplicity, we put R := R(θ ), where θ is the ideal Bayes
0/1 n P ∗ ∗ ∗
classifier. h i
Our analysis is centered on a sparse setting, where we assume s < n, with s = θ , denoting
∗ ∗ ∗ 0
k k
the number of nonzero elements in the parameter vector. Here, we primarily focus on the hinge
loss, which results in the following hinge empirical risk:
n
1
rh(θ)= (1 Y (θ x )) ,
n n − i ⊤ i +
i=1
X
where (a) := max(a,0), a R. We consider the following Gibbs-posterior distribution: ρˆh(θ)
+ ∀ ∈ λ ∝
exp[ λrh(θ)]π(θ) where λ > 0 is a tuning parameter and π(θ) is a prior distribution, given in (4),
− n
that promotes (approximately) sparsity on the parameter vector θ. Given a positive number C ,
1
for all θ B (C ):= θ Rd : θ C , we consider the following prior,
1 1 1 1
∈ { ∈ k k ≤ }
d
π(θ) (τ2+θ2) 2, (4)
∝ i −
i=1
Y
where τ > 0 is a tuning parameter. For technical reason, we assume that C > 2dτ. This prior
1
is known as a scaled Student distribution with 3 degree of freedom. This type of prior has been
previouslyexaminedinthedifferentsparseproblems(Dalalyan and Tsybakov,2012a;b;Mai,2024).
Theorem 3. Given that E X C < , Theorem 1 and Assumption 4 are satisfied, and by
x
k k ≤ ∞
setting λ = n/C, it follows that
s log(d/s )
E[E [R (θ)]] R C ∗ ∗ ,
θ ∼ρˆh λ 0/1 − 0∗/1 ≤ n
and
s log(d/s )
E[R (θˆ)] R C ∗ ∗ ,
0/1
−
0∗/1
≤ n
for some universal constant C > 0 depending only on K,B,C ,C .
1 x
Remark 5. According to Theorem 3, the misclassification excess rate is of order s log(d/s )/n
∗ ∗
which is established as minimax-optimal in high-dimensional sparse classification, according to
Abramovich and Grinshtein (2018). This result is novel and extends the work of Mai (2024), which
addresses only the misclassification excess rate in the noiseless scenario.
3.2 1-bit matrix completion
For sake of simplicity, for any positive integer m, let [m] denote 1,...,m .
{ }
7
1Formally, the 1-bit matrix completion problem can be defined as a classification problem as follow:
we observe (X ,Y ) that are n i.i.d pairs from a distribution P. The X ’s take values in
k k k [n] k
∈
= [d ] [d ] and the Y ’s take values in 1,+1 . Hence, the k-th observation of an entry of
1 2 k
X × {− }
the matrix is Y and the corresponding position in the matrix is provided by X = (i ,j ).
k k k k
Here, a predictor is a function [d ] [d ] R, and it can therefore be represented by a matrix M.
1 2
× →
A natural approach is to employ M such that when (X,Y) P, the predictor M predicts Y using
∼
sign(M ). The performance of this predictor in predicting a new matrix entry is subsequently
X
measured by the risk
R(M) = E P[ (YM
X
<0)],
and its empirical counterpart is: r (M) = 1 n (Y M < 0) = 1 n (Y M < 0).
n n k=1 k Xk n k=1 k ik,jk
From the classification theory (Vapnik, 1998), the best possible classifier is the Bayes classifier
P P
η(x) = E(Y X = x) or equivalently η(i,j) = E[Y X = (i,j)],
| |
and equivalently we have a corresponding optimal matrix M = sign[η(i,j)]. We define r =
i∗j n
r (M ). Note that, clearly, if two matrices M1 and M2 are such as, for every (i,j), sign(M1) =
n ∗ ij
sign(M2) then R(M1) = R(M2), and obviously, M, (i,j) [d ] [d ], sign(M ) = M
ij ∀ ∀ ∈ 1 × 2 ij i∗j ⇒
r (M) = r .
n n
In the paper (Cottet and Alquier, 2018), the authors deal with the hinge loss, which leads to the
following so-called hinge risk and hinge empirical risk:
n
1
Rh(M) = E P[(1 −YM X) +], r nh(M) =
n
(1 −Y kM Xk) +.
k=1
X
Specifically,withM = LR ,Cottet and Alquier(2018)definethepriordistributionasthefollowing
⊤
hierarchical model:
k [K], γ iid πγ,
k
∀ ∈ ∼
iid
L ,R γ (0,diag(γ)), (i,j) [m ] [m ],
i, j, 1 2
· ·| ∼ N ∀ ∈ ×
where the prior distribution on the variances πγ is either the Gamma or the inverse-Gamma dis-
tribution: πγ = Γ(α,β), or πγ = Γ 1(α,β).
−
Let θ denote the parameter θ = (L,R,γ). As in PAC-Bayes theory Catoni (2007), the Gibbs-
posterior is as follows:
exp[ λrh(LR )]
ρh(dθ) = − n ⊤ π(dθ)
λ exp[ λrh]dπ
− n
where λ > 0 is a parameter to bebfixed by thRe user.
The paper (Cottet and Alquier, 2018) explores a Variational Bayes (VB) approximation, which
facilitates the replacement of MCMC methods with more efficient optimization algorithms. They
define a VB approximation as ρ = argmin (ρ ρh).
λ ρ ∈F K k λ
We define (r,B) for r 1 and B > 0 as the set of pairs of matrices (U¯,V¯), with dimensions
d K andM d K respe≥ ctivee ly, that meet the condb itions U¯ B, V¯ B, U¯ = 0 for
1 2 i,ℓ
i >× r, and V¯ × = 0 for j > r. Consistent with Cottet andk Alqk u∞ ie≤ r (201k 8);k A∞ lq≤ uier and Ridgway
j,ℓ
(2020), we assume that M = U¯V¯t for some (U¯,V¯) in (r,B).
∗
M
8
1
1 1Theorem 4. Assuming that Theorem 1 and Assumption 4 holds and taking λ = n/C, then we
have that
r(d +d )log(nd d )
E[E θ ∼ρe λ[R 0/1(θ)]] −R 0∗/1
≤
C 1 2
n
1 2 ,
and
r(d +d )log(nd d )
E[R (θˆ)] R C 1 2 1 2 ,
0/1
−
0∗/1
≤ n
for some universal constant C > 0 depending only on K,B.
Remark 6. The misclassification excess error rate presented in Theorem 4, which is on the order
of r(d +d )/n (up to a logarithmic factor), is established as minimax-optimal, as demonstrated in
1 2
Alquier et al. (2019).
4 Concluding discussions
This paper presents misclassification excess risk bounds for PAC-Bayesian classification, achieved
through the application of a convex surrogate loss function. The methodology primarily relies
on the PAC-Bayesian relative bound in expectation, coupled with the assumption of low noise
condition. While our analysis assumes a bounded loss, it is worth mentioning that the findings
can be extended to unbounded loss scenarios, given additional conditions as elaborated in Alquier
(2024). Once the PAC-Bayesian relative bound in expectation for the chosen loss function is
established, our theoretical results are applicable.
In our work, the Bernstein condition is assumed; however, it may not always be necessary. Indeed,
as evidencedby several studiesCottet and Alquier (2018);Mai (2024), inthenoiseless scenario, the
margin condition alone is adequate for deriving a misclassification excess risk bound. Additionally,
Section 6 of Alquier et al. (2019) highlights that, under the hinge loss, the low-noise condition
aligns with the Bernstein condition. This suggests that investigating the relationship between the
Bernstein condition on convex loss and the margin condition within PAC-Bayes bounds could be a
valuable area for future research.
Acknowledgments
This work was supported by the Norwegian Research Council, grant number 309960, through the
CentreforGeophysicalForecastingatNTNU.TheauthorthanksPierreAlquierforusefuldiscussion
on the Bernstein’s condition.
Conflicts of interest/Competing interests
The author declares no potential conflict of interests.
A Proofs
A.1 Proof of Section 2
Proof os Theorem 1. From Assumption 2, the loss is Lipschitz,
E [φ (θ) φ (θ )]2 L2E θ θ 2 .
i − i ∗ ≤ k − ∗ k2
n o
and from Assumption 3, (cid:2) (cid:3)
E [φ (θ) φ (θ )]2 L2E θ θ 2 L2K[Rφ(θ) Rφ(θ )].
i − i ∗ ≤ k − ∗ k2 ≤ − ∗
n o
(cid:2) (cid:3)
9Therefore, the assumption (Definition 4.1) of Theorem 4.3 in Alquier (2024) is satisfied with L2K.
Thus, the result is obtained by using Theorem 4.3 in Alquier (2024).
Proof of Theorem 2. As Assumption 4 is satisfied, according to Theorem 3 in Bartlett et al.
(2006) (taking α = 1), there exists a constant C > 0 such that
E[R (θ)] R C E[Rφ(θ)] Rφ(θ ) ,
0/1
−
0∗/1
≤ −
∗
h i
φ
integrating with respect to ρˆ , and then using Fubini’s theorem,
λ
E[E [R (θ)]] R C E[E [Rφ(θ)]] Rφ(θ ) ,
θ ∼ρˆφ λ 0/1 − 0∗/1 ≤ θ ∼ρˆφ λ − ∗
(cid:16) (cid:17)
we obtain the result in (2) by utilizing the result from Theorem 1.
To obtain (3), as φ is convex, an application of Jensen’s inequality to Theorem 1 yields
E[Rφ(θˆ)] Rφ(θ ) EE [Rφ(θ)] Rφ(θ )
− ∗ ≤ θ ∼ρˆφ λ − ∗
thus we can now apply Theorem 3 in Bartlett et al. (2006) to get that
E[R (θˆ)] R C E[Rφ(θˆ)] Rφ(θ ) ,
0/1
−
0∗/1
≤ −
∗
(cid:16) (cid:17)
and the result is followed. This completes the proof.
Proof of Corollary 1. As Assumptions 2 and 3 are satisfied, we obtain Theorem 1,
C (ρ π)
E[E
θ ∼ρˆφ
λ[Rφ(θ)]] −Rφ(θ ∗)
≤
2
ρ
∈i Pn (f
Θ)
(cid:26)E
θ
∼ρ[Rφ(θ)] −Rφ(θ ∗)+ K nk (cid:27).
Moreover, from Assumption 3, θ θ 2 K[Rφ(θ) Rφ(θ )]. Therefore, the result is obtained by
k − ∗ k2 ≤ − ∗
combining these bounds.
A.2 Proof of Section 3
Proof of Theorem 3. As the hinge loss is 1-Lipschitz, one has that
Rφ(θ) Rφ(θ ) E X θ θ
∗ ∗
− ≤ k kk − k
We define the following distribution as a translation of the prior π,
p (β) π(β β ) (β β ). (5)
0
∝ −
∗ B1(2dτ)
−
∗
From Lemma 1, we have, for ρ:= p , that
0
1/2
[Rφ(θ) Rφ(θ )]p (dθ) C β β p (dβ) C β β 2p (dβ) C √4dτ2
∗ 0 x ∗ 0 x ∗ 0 x
− ≤ k − k ≤ k − k ≤
Z Z (cid:18)Z (cid:19)
and
C
1
(p π) 4s log +log(2).
0 ∗
K k ≤ τs
(cid:18) ∗(cid:19)
10
1Plug-in these bounds into inequality (2), one gets that
C 4s log C1 +log(2)
E[E [R (θ)]] R Ψ inf C 2τ√d+ 1 ∗ τs∗ ,
θ ∼ρˆφ λ 0/1 − 0∗/1 ≤ τ ∈(0,C1/2d)( x (cid:0)n (cid:1) )
and the choice τ = (C n√d) 1 leads to
x −
C 4s log
CxC1n√d
+log(2)
E[E [R (θ)]] R Ψ
2
+
1 ∗ s∗ cs ∗log(d/s ∗)
,
θ ∼ρˆφ λ 0/1 − 0∗/1 ≤ n (cid:16) n (cid:17)  ≤ n
 
for some positive constant c depending only on L,K,B,C ,C . A similarargument application to
1 x
inequality (3), one gets that
s log(d/s )
E[R (θˆ)] R . ∗ ∗ .
0/1
−
0∗/1
n
The proof is completed.
Proof of Theorem 4. Using similar argument as in the proof of Theorem 4.3 in Alquier (2024)
(see also the proof of Theorem 4.3 in Alquier et al. (2016)), one obtains that
C (ρ π)
E[E θ ∼ρe λ[Rφ(θ)]] −Rφ(θ ∗)
≤
2 ρinf E θ ∼ρ[Rφ(θ)] −Rφ(θ ∗)+ 1 K
n
k .
∈F(cid:26) (cid:27)
A similar argument as in Theorem 2,
C (ρ π)
E[E θ ∼ρe λ[R 0/1(θ)]] −R 0/1(θ ∗)
≤
Ψ ρinf E θ ∼ρ[Rφ(θ)] −Rφ(θ ∗)+ 1 K
n
k .
∈F(cid:26) (cid:27)
As the hinge loss is 1-Lipschitz, and noting that θ = M ,θ = LR ,one has that
∗ ∗ ⊤
Rφ(θ) Rφ(θ ) θ θ = LR M
∗ ∗ ⊤ ∗
− ≤ k − k k − k
Given B > 0 and r 1, for any pair (U¯,V¯) (r,B), we define
≥ ∈ M
ρ n(dU,dV,dγ) ∝ 1 ( kU −U¯ k∞ ≤δ, kU −U¯ k∞ ≤δ)π(dU,dV,dγ), (6)
where δ (0,B) to be selected later. For any (U,V) in the supportof ρ , given in (6), one has that
n
∈
M UVt = U¯V¯t U¯Vt+U¯Vt UVt
∗ F F
k − k k − − k
U¯(V¯t Vt) + (U¯ U)Vt
F F
≤ k − k k − k
U¯ V¯ V + U¯ U Vt
F F F F
≤ k k k − k k − k k k
d d U¯ 1/2 V¯ V 1/2+d d V 1/2 U¯ U 1/2
1 2 1 2
≤ k k∞ k − k∞ k k∞ k − k∞
d d δ1/2[B1/2+(B +δ)1/2]
1 2
≤
2d d δ1/2(B +δ)1/2 23/2d d δ1/2B1/2.
1 2 1 2
≤ ≤
Thus, with δ = B/[8(nd d )2], one gets that
1 2
E [Rφ(θ)] Rφ(θ ) B/n.
θ ∼ρn
−
∗
≤
11Now, from Lemma 2 with δ = B/[8(nd d )2], we have that
1 2
1 2(1+2a)r(d +d )[log(nd d )+C ]
1 2 1 2 a
(ρ π) .
n
nK k ≤ n
Putting all together,
B 2(1+2a)r(d +d )[log(nd d )+C ]
E[E θ ∼ρe λ[R 0/1(θ)]] −R 0/1(θ ∗)
≤
C
n
+ 1 2
n
1 2 a
(cid:26) (cid:27)
r(d +d )log(nd d )
1 2 1 2
. ,
n
for some numerical constant C > 0 depending only on a,C . The proof is completed.
1
Lemma 1. Let p be the probability measure defined by (5). If d 2 then β β 2p (dβ)
0 ≥ Λk − ∗ k 0 ≤
4dτ2, and (p π) 4s log C1 +log(2).
K 0 k ≤ ∗ τs∗ R
(cid:0) (cid:1)
Proof. The proof can be found in Mai (2024), which utilizes results from Dalalyan and Tsybakov
(2012a).
Lemma 2. Put C := log(8√πΓ(a)210a+1)+3 and with δ = B/[8(nd d )2] that satisfies 0 < δ < B,
a 1 2
we have for ρ in (6) that (ρ π) 2(1+2a)r(d +d )[log(nd d )+C ].
n n 1 2 1 2 a
K k ≤
Proof. This result can found in the proof of Theorem 4.1 in Alquier and Ridgway (2020).
References
Abramovich, F. and Grinshtein, V. (2018). High-dimensional classification by sparse logistic re-
gression. IEEE Transactions on Information Theory, 65(5):3068–3079.
Alaya, M. Z. and Klopp, O. (2019). Collective matrix completion. Journal of Machine Learning
Research, 20(148):1–43.
Alquier, P. (2024). User-friendly introduction to PAC-Bayes bounds. Foundations and Trends®
in Machine Learning, 17(2):174–303.
Alquier, P., Cottet, V., and Lecu´e, G. (2019). Estimation bounds and sharp oracle inequalities of
regularized procedures with Lipschitz loss functions. The Annals of Statistics, 47(4):2117 – 2144.
Alquier, P. and Ridgway, J. (2020). Concentration of tempered posteriors and of their variational
approximations. The Annals of Statistics, 48(3):1475–1497.
Alquier, P., Ridgway, J., and Chopin, N. (2016). On the properties of variational approximations
of gibbs posteriors. Journal of Machine Learning Research, 17(236):1–41.
Bartlett, P. L., Jordan, M. I., and McAuliffe, J. D. (2006). Convexity, classification, and risk
bounds. Journal of the American Statistical Association, 101(473):138–156.
Catoni, O. (2003). A PAC-Bayesian approach to adaptive classification. Preprint Laboratoire de
Probabilit´es et Mod`eles Al´eatoires PMA-840.
Catoni, O. (2004). Statistical learning theory and stochastic optimization, volume 1851 of Saint-
Flour Summer School on Probability Theory 2001 (Jean Picard ed.), Lecture Notes in Mathemat-
ics. Springer-Verlag, Berlin.
12Catoni, O.(2007). PAC-Bayesian supervised classification: the thermodynamics of statistical learn-
ing. IMSLectureNotes—MonographSeries,56.InstituteofMathematical Statistics, Beachwood,
OH.
Cottet, V. and Alquier, P. (2018). 1-Bit matrix completion: PAC-Bayesian analysis of a variational
approximation. Machine Learning, 107(3):579–603.
Dalalyan, A. S. and Tsybakov, A. (2012a). Mirror averaging with sparsity priors. Bernoulli,
18(3):914–944.
Dalalyan, A. S. and Tsybakov, A. B. (2012b). Sparse regression learning by aggregation and
langevin monte-carlo. Journal of Computer and System Sciences, 78(5):1423–1443.
Devroye, L., Gy¨orfi, L., and Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition,
volume 31. Springer Science & Business Media.
Elsener, A. and van de Geer, S. (2018). Robust low-rank matrix estimation. The Annals of
Statistics, 46(6B):3481–3509.
Guedj, B. (2019). A primer on PAC-Bayesian learning. In SMF 2018: Congr`es de la Soci´et´e
Math´ematique de France, volume 33 of S´emin. Congr., pages 391–413. Soc. Math. France.
Mai, T. T. (2023). A reduced-rank approach to predicting multiple binary responses through
machine learning. Statistics and Computing, 33(6):136.
Mai,T.T.(2024). High-dimensionalsparseclassificationusingexponentialweightingwithempirical
hinge loss. Statistica Neerlandica (in press), pages 1–28.
Mammen,E.andTsybakov,A.B.(1999). Smoothdiscriminationanalysis. The Annals of Statistics,
27(6):1808–1829.
McAllester, D. A. (1998). Some PAC-Bayesian theorems. In Proceedings of the 11th annual con-
ference on Computational learning theory, pages 230–234.
McAllester, D. A. (1999). Pac-bayesian model averaging. In Proceedings of the twelfth annual
conference on Computational learning theory, pages 164–170.
Mendelson, S. (2008). Obtaining fast error rates in nonconvex situations. Journal of Complexity,
24(3):380–397.
Rivasplata, O. (2022). PAC-Bayesian computation. PhD thesis, University College London.
Shawe-Taylor, J. and Williamson, R. C. (1997). A pac analysis of a bayesian estimator. In Pro-
ceedings of the tenth annual conference on Computational learning theory, pages 2–9.
Tsybakov, A. B. (2004). Optimal aggregation of classifiers in statistical learning. The Annals of
Statistics, 32(1):135–166.
Vapnik, V. N. (1998). Statistical Learning Theory. Wiley.
Zhang, T. (2004). Statistical behavior and consistency of classification methods based on convex
risk minimization. The Annals of Statistics, 32(1):56–85.
13