TextCAVs: Debugging vision models using text
Angus Nicolson1,2, Yarin Gal2, and J. Alison Noble1
1 Institute of Biomedical Engineering, University of Oxford
2 OATML, Department of Computer Science, University of Oxford
angus.nicolson@eng.ox.ac.uk
Abstract. Concept-based interpretability methods are a popular form
of explanation for deep learning models which provide explanations in
theformofhigh-levelhumaninterpretableconcepts.Thesemethodstyp-
ically find concept activation vectors (CAVs) using a probe dataset of
concept examples. This requires labelled data for these concepts – an
expensive task in the medical domain. We introduce TextCAVs: a novel
methodwhichcreatesCAVsusingvision-languagemodelssuchasCLIP,
allowing for explanations to be created solely using text descriptions of
theconcept,asopposedtoimageexemplars.Thisreducedcostintesting
concepts allows for many concepts to be tested and for users to inter-
act with the model, testing new ideas as they are thought of, rather
than a delay caused by image collection and annotation. In early ex-
perimental results, we demonstrate that TextCAVs produces reasonable
explanations for a chest x-ray dataset (MIMIC-CXR) and natural im-
ages(ImageNet),andthattheseexplanationscanbeusedtodebugdeep
learning-based models. Code: github.com/AngusNicolson/textcavs
Keywords: Interpretability · Concepts · Text Explanations · Chest X-
rays.
1 Introduction
Deep learning-based models are increasingly utilised in healthcare scenarios
where mistakes can have severe consequences. One approach for creating safer,
more reliable models is to use interpretability: the ability to explain or present
a model in terms understandable to a human [4].
Manydifferentinterpretabiltymethodshaveemerged,withexplanationstak-
ingavarietyofdifferentformssuchasindividualpixels,prototypesorconcepts.
Wefocusonconcept-basedmethodswhichprovideexplanationsusinghigh-level
terms that humans are familiar with. Concept activation vectors (CAVs) are a
common approach used to represent concepts within the activation space of a
model and are found using a probe dataset of concept exemplars [13].
The labels required for this can be expensive to obtain in medical domains
where expert clinical input is necessary. We introduce TextCAVs, a concept-
based interpretability method that uses solely the text label of the concept, or
descriptions of it, rather than image examples.
WedemonstratethatTextCAVsgivemeaningfulexplanationsforbothnatu-
ral image (ImageNet [3]) and chest X-ray (MIMIC-CXR [11,12]) tasks. Further,
4202
guA
61
]GL.sc[
1v25680.8042:viXra2 A. Nicolson et al.
as interpretability itself is difficult to measure, we demonstrate its usefulness in
debugging deep learning-based models through finding implanted dataset bias
in MIMIC-CXR.
Training and Reconstruction Loss
Target
Model
Cycle Loss
CLIP
The patient has
undergone prior aortic
valve replacement.
Creating TextCAVs Generating Explanations
Target
H coe na tr it n s ui eze s to be CLIP Model
mildly enlarged.
Fig.1. Explaining models with TextCAVs. In order to move between the activa-
tions of a CLIP model and our target model, we train linear transformations, h and
g,usingatextdataset,D ,andimagedataset,D .Thelosstermsaredetailedonthe
T I
right with I , I and T representing the image features of the target model, the im-
Φ Ψ Ψ
agefeaturesoftheCLIPmodel,andthetextfeaturesoftheCLIPmodel,respectively.
Oncehistrained,TextCAVscanbecreatedbypassingtextrepresentingsomeconcept,
c, through the CLIP model and h. The model’s sensitvity to c, for some logit output,
k, can then be measured using the directional derivative, S : the similarity between
c,k
the model gradient, ∇Φ , and a TextCAV, v .
b,k c
2 Related Work
Kim et al. [13] introduce Testing with Concept Activation Vectors (TCAVs)
where they use probe datasets of concept examples to create CAVs and then
compare the CAVs with model gradients to measure a model’s sensitivity to a
concept for a specific class. We also use the directional derivative (dot product
between CAV and gradient) to measure model sensitivity, but our CAVs are
created using a multi-modal model and so do not require a probe dataset for
each concept.
In order to reduce the cost of creating concept-based explanations, a vari-
ety of different methods automate the process of finding concepts [6,22,18,7,5].
However, the meaning of each concept is not always readily apparent and the
concept must be visually present in the dataset used to discover the concepts.
Our method reduces cost using a different approach as we also do not need to
collect labelled data for each concept, but our resulting CAVs have inherent
meaning from their text descriptions.TextCAVs: Debugging vision models using text 3
CLIP models [17] have demonstrated strong performance in vision-language
tasks. Their joint embedding space for text and images allows for built-in com-
parisons between the modalities and therefore for zero-shot classification. A va-
rietyofadaptationshavebeensuggestedforthebiomedicalspace[24]withsome
models being trained for specific modalities like chest X-rays (e.g. BioViL [1])
andothersmoregenerally(e.g.BiomedCLIP[23]).Weusethesevision-language
models in our method but, importantly, inference is performed by the target
model, without placing restrictions on its architecture or method of training.
Yuksekgonul et al. [21] use multimodal models to create CAVs and then
usethesimilaritybetweenmodelactivationsandtheseCAVstocreateaconcept
bottleneckmodel.Moayerietal.[15]extendthisapproachtotargetvisionmodels
more generally by, as in our work, training a simple linear layer to transfer the
features of the target model to a CLIP model. Also as in our work, Shipard
et al. [19] improve the transfer of features by training a linear layer in both
directionsandusingmultimodallosses.However,theseapproachesfocusonzero-
shotclassificationandonchanginghowthemodelinferenceisperformed,rather
than explaining the model in its current state using gradients.
3 TextCAVs
For some target model, Φ, and a CLIP-like vision-language model, Ψ, let I ∈
Φ
Rm and I ∈ Rn be the extracted features for some image dataset D . As Ψ
Ψ I
contains a joint embedding space between text and images we can also extract
textfeatures:T ∈Rn fromsometextdatasetD .Wetraintwolinearlayersh:
Ψ T
Rn →Rm and g :Rm →Rn which can be used to convert between the features
ofthetwomodels.TocreateTextCAVs,weonlyneedhbuttoimproveh’sability
to convert text features we use a cycle loss term which requires g. The loss is
composed of two parts: reconstruction loss and cycle loss. The reconstruction
loss is simply the mean squared error (MSE) between the image features and
converted features.
L =||h(I )−I ||2+||g(I )−I ||2 (1)
mse Ψ Φ Φ Ψ
The reconstruction loss can only be calculated for image features as we need
features from both models (Φ and Ψ). To include information from the text
featuresinthelossfunctionweusecyclelosswhichensuresthatthefeaturesare
consistent with their original form when converted back to their original space:
L =||h(g(I ))−I || (2)
cyc Φ Φ
+||g(h(I ))−I || (3)
Ψ Ψ
+||g(h(T ))−T ||. (4)
Ψ Ψ
Once trained, we use h, Ψ and a concept label, c, to obtain a concept vector
in the activation space of the target model:
v =h(Ψ(c)). (5)
c4 A. Nicolson et al.
Φ can be decomposed into two functions: Φ (x) = I ∈ Rm which maps
a Φ
the input x ∈ RN to its features I , and Φ (I ) which maps I to the output.
Φ b Φ Φ
To obtain the model’s sensitivity to a concept for a specific class, as in [13], we
calculate the directional derivative:
Φ (Φ (x)+ϵv )−Φ (Φ (x))
S (x)= lim b,k a c b,k a
c,k ϵ→0 ϵ (6)
=∇Φ (Φ (x))·v .
b,k a c
IfΦ ischosentobetheoutputofthepenultimatelayerinamodelthenthe
a
directional derivative can be calculated without image exemplars:
S =∇Φ ·v . (7)
c,k b,k c
This is due to the lack of non-linearities between the penultimate layer and
thelogitoutput.Havingsolelyalinearlayerbetweenthefeaturesandtheoutput
means the gradient of the activations with respect to the logit does not depend
on the activations. This means we can extract gradients, and therefore model
explanations,usingsolelythemodelweights.Therefore,oncehhasbeentrained,
TextCAVs requires only the text you wish to test to be able to generate an
explanation. In practice, to calculate the gradient, we input an array of zeros of
the same shape as the images, but this is an arbitrary choice. In this work, we
usethepenultimatelayerinallexperimentsandleaveexplorationofusingother
layers for future work.
By ranking concepts based on their directional derivative, we obtain a list of
sentences/words ordered by the model’s sensitivity for a specific class. If we can
filter this list for concepts which we expect to be there, we can discover bugs in
the model. Ideally, this would be done by a human expert who could use their
domain knowledge to explore different hypotheses. The minimal overhead for
testing new concepts allows the user to test words related to new hypotheses
quickly and provide an interactive process to model debugging.
4 Experiments
Inthissectionweprovideadescriptionofourtrainingsetup,ourmodelchoices,
evaluation and then a discussion and analysis of our results experiments with
both the ImageNet and MIMIC-CXR datasets.
4.1 ImageNet
TextCAVs achieved 3rd place at the Secure and Trustworthy Machine Learning
Conference (SaTML) interpretabilty competition to detect trojans (implanted
bugs) in vision models trained on ImageNet [2]. Additionally, as part of the
competition,TextCAVswasusedtoidentifyallfoursecrettrojansdemonstrating
its potential for interactive debugging.
In this section, however, we simply demonstrate that TextCAVs produces
reasonable explanations for a standard ResNet-50 [8] trained on ImageNet.TextCAVs: Debugging vision models using text 5
Training Details We use 20% of the ImageNet training dataset to train h and
g andtrainfor20epochs.Forthetargetmodel,Φweusethedefaultweightsfor
a ResNet-50 [8] in the TorchVision package in PyTorch. For the vision-language
model, Ψ, we use a pretrained ViT-B/16 CLIP model [17].
Concepts InasimilarmannertoOikarinenetal.[16],inordertoautomatethe
process, we use a large language model (LLM) to obtain a list of concepts. We
use three prompts asking for the “things most commonly seen around” “visual
elementsorparts” and“superclasses” ofeachclassinImageNet.Wethenextract
and perform basic filtering of the concepts, removing: plurals of the same word;
the words “an”, “a” and “the”; and concepts containing more than 2 words. To
obtainthefinallistofconceptsweremovesimilarconceptsusingtextembeddings
from Ψ. If a set of concepts have a cosine similarity greater than 0.9, only the
shortest concept is retained. This reduces the number of near synonyms in the
concept list. For the LLM, we use a 4-bit quantized version of the Tulu-v2-7b
model [10].
Results In Table 1, we show the top-10 concepts for a selection of ImageNet
classes.Alltheconceptsrelatetotheirrespectiveclass,indicatingthatTextCAVs
can produce reliable explanations.
Table 1.Top-10conceptsorderedbydirectionalderivativeforaselectionofclassesin
the ImageNet model.
bullfrog albatross orangutan bucket cellphone
americanbullfroggannet orangutan crabbuckets mp3player
greenfrog seagull howlermonkey diaperpail phone
borealtoad seaeagle macaque bucket phonecase
westerntoad shearwater tarsier laundrybasketmemorycard
frog gull greatape wateringcan walkman
muskturtle white-tailedeaglelong-nosedmonkeyflowerpot cordlessphone
snappingturtle petrel gibbon cookingpot bluetooth
toad merganser gorilla dustbin smartwatch
terrapinturtle wadingbird langur fishingbasket cardreader
4.2 MIMIC-CXR
In this section we demonstrate TextCAVs ability to produce meaningful expla-
nations for a model trained on the chest X-ray dataset MIMIC-CXR and how
we can use TextCAVs to discover bias in a model trained on a biased version of
the dataset.
Training Details We train both the linear transformations, h and g, and the
target model, Φ, using the MIMIC-CXR training set. The target model is a
ResNet-50 [8] pretrained on ImageNet and then fine-tuned for the 5-way multi-
labelclassificationofchestX-rayswiththeclasses:NoFinding,Atelectasis(lung
collapse),Cardiomegaly(enlargedheart),Edema(fluidinthelungs)andPleural6 A. Nicolson et al.
Effusion(fluidbetweenthelungsandthechestwall).WeusetheAdamoptimiser
[14] with weight decay of 1e−4 and initial learning rate of 1e−4. The learning
rate is halved or the training is stopped if the validation loss does not decrease
within 3 or 5 epochs, respectively. Images are resized to 256 × 256. We use
random rotation of up to 15 degrees, random horizontal flipping, random crop
and resize with a minimum size of 40%, and distortion to augment the images.
We use the published data splits and, after removing images with no positive
class labels, there are 368,945 training, 2,991 validation and 1,012 test images.
We use labels from CheXpert [9] for the training and validation labels, which
have been generated by a model using the text reports. Whereas, for the test
dataset, we use the provided labels annotated by a single radiologist.
We train both h and g on the training set of MIMIC-CXR for 20 epochs.
We use the output of the average pool operation as the features from the target
model as it simplifies the extraction of model gradients (Eqn. 7).
ForΨ,weuseBiomedCLIP[23]–thecurrentstateoftheartvision-language
model for chest X-ray tasks.
Concepts The MIMIC-CXR dataset has a clinical report associated with each
image. We use these reports as a source of concepts. We extract the sentences
from the “FINDINGS” and “IMPRESSION” sections of the reports and use a
random subset of 5000 sentences to obtain a wide variety of concepts to test.
Biased Data To evaluate TextCAVs as an interpretability tool we explore its
usefulness in model debugging. We induced a dataset bias in the MIMIC-CXR
training set by removing all participants with a positive label for Atelactesis
and a negative label for Support Devices. This means that all participants with
AtelactesisinthetrainingsetalsohadaSupportDevice(e.g.tubeorpacemaker)
as can be seen in Figure 2.
Metrics To provide a quantitative metric, we labelled the top-50 sentences for
eachclass,orderedbydirectionalderivative,onwhethertheyrelatetotheclass.
We report this information as a concept relevance score (CRS), which is simply
the proportion of concepts that were related to the class. Using Edema as an
example, a sentence was labelled as related if it directly diagnosed the class,
e.g., “Worsening cardiogenic pulmonary edema”, or if the class was implied, e.g.,
“bilateral parenchymal opacities” or “there is alveolar opacity throughout much
of the right lung”.
Results We are comparing two models: one trained on the standard MIMIC-
CXR dataset and the other trained on the biased version. We will refer to the
models as “standard” and “biased”, respectively. The standard model achieved a
mean area under the receiver operator characteristic curve (AUC) of 0.83 and
the biased model a mean AUC of 0.81. The individual class AUCs can be found
inTable2.Weexpect,andseethatthebiasedversionhashigherperformanceon
a biased version of the test set since Support Devices tend to be easy to detect.
As evidence for this, we trained a reference model separately and achieved an
AUC of 0.92 for Support Devices.TextCAVs: Debugging vision models using text 7
1.0
Dataset Dataset
125000 Standard Standard
0.8
100000 Biased Biased
0.6
75000
0.4
50000
25000 0.2
0 0.0
No
Finding Atelectasis Cardiomegaly Edem Pa
leural
Effusion
No
Finding Atelectasis Cardiomegaly Edema
Pleural
Effusion
Class Class
Fig.2. MIMIC-CXR dataset characteristics. Left: The number of images per
classinthetrainingsetofthetargetmodels.Right:Theproportionoftrainingimages
that contain a support device for each class.
Table 2. Area under the receiver operator characteristic curve (AUC) and concept
relevance score (CRS) for the standard and biased MIMIC-CXR models. AUC* was
calculatedonthebiasedversionoftheMIMIC-CXRtestset.ThelowCRSforAtelec-
tasis in the biased model means almost none of the top TextCAVs are relevant to the
class,demonstratingthattheycanbeusedtodetectifamodelisusingbiasedfeatures.
Model Standard Biased
Metric AUCCRSAUCAUC*CRS
NoFinding 0.87 0.74 0.85 0.94 0.76
Atelectasis 0.73 0.56 0.68 0.81 0.04
Cardiomegaly 0.81 0.94 0.81 0.82 0.90
Edema 0.85 0.90 0.84 0.81 0.80
PleuralEffusion 0.89 1.00 0.88 0.88 1.00
Mean 0.83 0.83 0.81 0.85 0.70
In Table 3, we show the five sentences whose CAVs have the highest direc-
tional derivatives for the classes of No Finding, Atelectasis (lung collapse) and
Cardiomegaly (enlarged heart). Some of these are clearly linked to the class in
question (e.g. “The lungs are clear” for No Finding and “Heart size continues
to be mildly enlarged” for Cardiomegaly) but there also sentences which do not
relate to the classes (e.g. “Nasogastric tube extends below the hemidiaphragm”
for Atelectasis or “There is a fracture of the upper most sternal wire” for No
Finding).Thenoisepresentintheexplanationscouldbeduetoseveraldifferent
causes: (1) the target model is using unexpected features in its classification;
(2) the feature conversion between Φ and Ψ is not perfect (i.e., h); or (3) the
inherent noise present in gradient vectors [20]. It is difficult to ascertain which
oftheseisthecausebutatoolcanstillbeusefulevenwithnoisepresent.Hence,
wedemonstrateitsabilitytodetectdatasetbiasthatweinduceinMIMIC-CXR.
Table 4 shows the top-5 sentences for a model trained on the biased version
ofMIMIC-CXR.Thebiasisapparentintheexplanations,asthetop-5sentences
for Atelectasis all refer to Support Devices, rather than to any concepts relating
ssalc
rep
segami
.oN
segami
fo
noitroporP
secived
troppus
htiw8 A. Nicolson et al.
to the class itself. The CRS values in Table 2 also indicate the presence of bias:
aCRSof0.04forAtelectasisforthebiasedmodelshowsthatalmostnoneofthe
top-50conceptscontainreferencetotheclass.Tofurtherquantifythedifference
between the two sets of explanations we also labelled whether they referred to
SupportDevices.FortheclassofAtelectasis,wefoundthat13/50conceptswere
related to Support Devices for the standard model compared to 44/50 for the
biased model, demonstrating that TextCAVs are sensitive to the difference in
behaviour between the two models.
Table 3. Top-5 concepts ordered by directional derivative for the standard MIMIC-
CXR model.
No Finding Atelectasis Cardiomegaly
The lungs are clear and theNasogastrictubeextendsbe-Markedcardiacenlargement
cardiac,mediastinal,andhi-low the hemidiaphragm andas before and unchanged
larcontoursarenormal. outofview. position of previously de-
scribedmetallicprosthesisof
porcinetype.
Normal chest radiographIntervalplacementofabasi-Heart size continues to be
with unremarkable appear-lar right sided pleural spacemildlyenlarged.
anceofthelungparenchymapigtail catheter with im-
and normal appearance ofprovedsmallrightpleuralef-
the heart and the mediasti-fusionandrightmediallung
nalandhilarcontours. baseatelectasis.
The trachea is slightly devi-Worsening of the left retro-The patient has undergone
atedtotherightbytheaor-cardiac opacity likely sec-prior aortic valve replace-
ticknob,whichisill-defined.ondary to increasing atelec-ment.
tasisand/oreffusion.
Thiscouldrepresentagran-ThereispersistentelevationDense retrocardiac opacity
ulomaorpossiblyaboneis-of the left hemidiaphragmwhich could represent ef-
landintheribitself. with evidence of Bochdalekfusion, atelectasis, consol-
hernia seen at the left loweridation or a combination
hemithorax. thereof.
Thereisafractureoftheup-Stable opacification of theThe heart continues to be
per most sternal wire, un-mid and lower right lungenlarged with mild to mod-
changed. consistent with large locu-erateCHF.
lated pleural effusions and
adjacentatelectasis.
5 Conclusion
In this work we introduce TextCAVs, an interpretability method that, once two
linear layers have been trained, can measure the sensitivity of a model to a
concept with only a text description of the concept. We show that TextCAVs
producereasonableexplanationsformodelstrainedonbothnaturalimages(Im-
ageNet [3]) a chest X-ray dataset (MIMIC-CXR [11]). As first demonstrated in
the SaTML CNN interpretability competition [2], we show that TextCAVs can
be used to debug models. We generated explanations for a model trained on a
biasedversionoftheMIMIC-CXRdatasetandshowedthatexplanationsfortheTextCAVs: Debugging vision models using text 9
biased class substantially changed with most (44/50) concepts referring to the
bias compared to just 13/50 for the unbiased model.
Once the linear transformations, h and g, have been trained, TextCAVs en-
ablesfastfeedbackwhentestingthesensitivityofdifferentconcepts.Thismakes
itideallysuitedforinteractivedebuggingwhichweaimtostudyinfuturework.
Some of the concepts with a high directional derivative did not appear to be
related to the class. In section 4.2 we state three possible sources of this: (1)
Φ, (2) h or (3) ∇Φ . In future work we will explore which of these have the
b,k
greatest effect.
Table 4.Top-5conceptsorderedbydirectionalderivativeforthebiasedMIMIC-CXR
model.
No Finding Atelectasis Cardiomegaly
Bronchial wall thickening isETandNGtubespositionedIfcardiomegalypersists,the
minimal. appropriately. presence of a pericardial ef-
fusion could be excluded
withechocardiography.
Hilar and mediastinal con-ET tube, nasogastric tube,Worsening heart failure in
toursareotherwisenormal. Swan-Ganz catheter, andthecontextofchronicatelec-
midline drains are all intasis.
standardplacements.
Thiscouldrepresentagran-Nasogastrictubeextendsbe-The patient has undergone
ulomaorpossiblyaboneis-low the hemidiaphragm andprior aortic valve replace-
landintheribitself. outofview. ment.
No discrete solid pulmonaryImpella LVAD and transve-Moderate-to-severe car-
noduleareconcerningmass. nous atrioventricular pacerdiomegaly and stigmata of
leads unchanged in their re-previous mitral valve repair
spectivepositions. noted.
Thereisafractureoftheup-Nasogastric tube has beenThe heart remains moder-
per most sternal wire, un-placedthatextendswellintoatelyenlargedandtheaorta
changed. thestomach. remainsunfoldedandtortu-
ous.
Acknowledgments. WeappreciatethemembersofOATMLandtheNoblegroupfor
your support and discussions during the project, in particular Lisa Schut. We thank
Shreshth Malik for organising the hackathon where we began developing TextCAVs.
A. Nicolson is supported by the EPSRC Centre for Doctoral Training in Health Data
Science (EP/S02428X/1). Y. Gal is supported by a Turing AI Fellowship financed
by the UK government’s Office for Artificial Intelligence, through UK Research and
Innovation(grantreferenceEP/V030302/1)anddeliveredbytheAlanTuringInstitute.
J.A. Noble acknowledges EPSRC grants EP/X040186/1 and EP/T028572/1.
Disclosure of Interests. The authors have no competing interests to declare that
are relevant to the content of this article.10 A. Nicolson et al.
References
1. Boecking,B.,Usuyama,N.,Bannur,S.,Castro,D.C.,Schwaighofer,A.,Hyland,S.,
Wetscherek,M.,Naumann,T.,Nori,A.,Alvarez-Valle,J.,etal.:Makingthemost
of text semantics to improve biomedical vision–language processing. In: ECCV
(2022)
2. Casper, S., Yun, J., Baek, J., Jung, Y., Kim, M., Kwon, K., Park, S., Moore,
H., Shriver, D., Connor, M., Grimes, K., Nicolson, A., Tagade, A., Rumbelow, J.,
Nguyen, H.M., Hadfield-Menell, D.: The SaTML ’24 CNN Interpretability Com-
petition: New Innovations for Concept-Level Interpretability. arXiv:2404.02949
(2024)
3. Deng,J.,Dong,W.,Socher,R.,Li,L.J.,Li,K.,Fei-Fei,L.:ImageNet:Alarge-scale
hierarchical image database. In: CVPR. pp. 248–255 (2009)
4. Doshi-Velez,F.,Kim,B.:Towardsarigorousscienceofinterpretablemachinelearn-
ing. arxiv:1702.08608 (2017)
5. Fel,T.,Picard,A.,Bethune,L.,Boissin,T.,Vigouroux,D.,Colin,J.,Cadène,R.,
Serre,T.:CRAFT:ConceptRecursiveActivationFacTorizationforExplainability.
In: CVPR (2023)
6. Ghorbani, A., Wexler, J., Zou, J., Kim, B.: Towards Automatic Concept-based
Explanations. In: Advances in Neural Information Processing Systems (2019)
7. Graziani, M., O’Mahony, L., phi Nguyen, A., Müller, H., Andrearczyk, V.: Un-
covering Unique Concept Vectors through Latent Space Decomposition. TMLR
(2023)
8. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.
In: CVPR (2016)
9. Irvin,J.,Rajpurkar,P.,Ko,M.,Yu,Y.,Ciurea-Ilcus,S.,Chute,C.,Marklund,H.,
Haghgoo, B., Ball, R., Shpanskaya, K., et al.: Chexpert: A large chest radiograph
dataset with uncertainty labels and expert comparison. In: AAAI conference on
artificial intelligence. vol. 33, pp. 590–597 (2019)
10. Ivison, H., Wang, Y., Pyatkin, V., Lambert, N., Peters, M., Dasigi, P., Jang, J.,
Wadden,D.,Smith,N.A.,Beltagy,I.,Hajishirzi,H.:CamelsinaChangingClimate:
Enhancing LM Adaptation with Tulu 2. arXiv:2311.10702 (2023)
11. Johnson,A.E.W.,Pollard,T.J.,Berkowitz,S.J.,Greenbaum,N.R.,Lungren,M.P.,
Deng,C.,Mark,R.G.,Horng,S.:MIMIC-CXR:Alargepubliclyavailabledatabase
of labeled chest radiographs (2019)
12. Johnson,A.E.W.,Pollard,T.J.,Berkowitz,S.J.,Greenbaum,N.R.,Lungren,M.P.,
Deng,C.,Mark,R.G.,Horng,S.:MIMIC-CXR-JPG-chestradiographswithstruc-
tured labels (2024)
13. Kim, B., Wattenberg, M., Gilmer, J., Cai, C., Wexler, J., Viegas, F., et al.: Inter-
pretabilitybeyondfeatureattribution:Quantitativetestingwithconceptactivation
vectors (TCAV). In: ICML (2018)
14. Kingma,D.,Ba,J.:Adam:Amethodforstochasticoptimization.In:ICLR(2015)
15. Moayeri, M., Rezaei, K., Sanjabi, M., Feizi, S.: Text-To-Concept (and Back) via
Cross-Model Alignment. In: ICML (2023)
16. Oikarinen,T.,Das,S.,Nguyen,L.M.,Weng,T.W.:Label-freeConceptBottleneck
Models. In: ICLR (2023)
17. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell,A.,Mishkin,P.,Clark,J.,Krueger,G.,Sutskever,I.:LearningTransferable
Visual Models From Natural Language Supervision. In: ICLR (2021)TextCAVs: Debugging vision models using text 11
18. Ramaswamy,V.V.,Kim,S.S.Y.,Meister,N.,Fong,R.,Russakovsky,O.:ELUDE:
Generatinginterpretableexplanationsviaadecompositionintolabelledandunla-
belled features. arXiv:2206.07690 (2022)
19. Shipard, J., Wiliem, A., Thanh, K.N., Xiang, W., Fookes, C.: Zoom-shot: Fast
and Efficient Unsupervised Zero-Shot Transfer of CLIP to Vision Encoders with
Multimodal Loss. arXiv:2401.11633 (2024)
20. Smilkov, D., Thorat, N., Kim, B., Viégas, F., Wattenberg, M.: Smoothgrad: re-
moving noise by adding noise. arXiv:1706.03825 (2017)
21. Yuksekgonul,M.,Wang,M.,Zou,J.:Post-hocconceptbottleneckmodels.In:ICLR
(2023)
22. Zhang, R., Madumal, P., Miller, T., Ehinger, K.A., Rubinstein, B.I.P.: Invertible
Concept-based Explanations for CNN Models with Non-negative Concept Activa-
tion Vectors. In: AAAI Conference on Artificial Intelligence (2020)
23. Zhang, S., Xu, Y., Usuyama, N., Xu, H., Bagga, J., Tinn, R., Preston, S., Rao,
R.,Wei,M.,Valluri,N.,Wong,C.,Tupini,A.,Wang,Y.,Mazzola,M.,Shukla,S.,
Liden,L.,Gao,J.,Lungren,M.P.,Naumann,T.,Wang,S.,Poon,H.:BiomedCLIP:
amultimodalbiomedicalfoundationmodelpretrainedfromfifteenmillionscientific
image-text pairs. arXiv:2303.00915 (2023)
24. Zhao, Z., Liu, Y., Wu, H., Li, Y., Wang, S., Teng, L., Liu, D., Cui, Z., Wang, Q.,
Shen, D.: CLIP in Medical Imaging: A Comprehensive Survey. arXiv:2312.07353
(2023)