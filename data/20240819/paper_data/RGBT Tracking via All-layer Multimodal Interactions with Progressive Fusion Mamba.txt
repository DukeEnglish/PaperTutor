RGBT Tracking via All-layer Multimodal
Interactions with Progressive Fusion Mamba
Andong Lu∗, Wanyu Wang†, Chenglong Li†, Jin Tang∗ and Bin Luo∗
∗School of Computer Science and Technology, Anhui University
†School of Artificial Intelligence, Anhui University
adlu ah@foxmail.com
Abstract—Existing RGBT tracking methods often design var- Tuned paramters Frozen paramters
ious interaction models to perform cross-modal fusion of each
layer, but can not execute the feature interactions among all RGB Backbone RGB Backbone RGB Backbone
layers, which plays a critical role in robust multimodal repre- all tokens
sentation, due to large computational burden. To address this (a) (b) (c)
issue,thispaperpresentsanovelAll-layermultimodalInteraction
Network, named AINet, which performs efficient and effective
TIR Backbone TIR Backbone TIR Backbone
feature interactions of all modalities and layers in a progressive
fusionMamba,forrobustRGBTtracking.Eventhoughmodality
(d)
features in different layers are known to contain different cues,
it is always challenging to build multimodal interactions in each
layer due to struggling in balancing interaction capabilities and
efficiency. Meanwhile, considering that the feature discrepancy
between RGB and thermal modalities reflects their complemen-
tary information to some extent, we design a Difference-based
Fusion Mamba (DFM) to achieve enhanced fusion of different
modalitieswithlinearcomplexity.Wheninteractingwithfeatures
from all layers, a huge number of token sequences (3840 tokens
in this work) are involved and the computational burden is thus
large. To handle this problem, we design an Order-dynamic
Fusion Mamba (OFM) to execute efficient and effective feature
interactionsofalllayersbydynamicallyadjustingthescanorder
of different layers in Mamba. Extensive experiments on four
publicRGBTtrackingdatasetsshowthatAINetachievesleading
performance against existing state-of-the-art methods.
Fig. 1. Comparison with existing RGBT tracking methods. (a) Interactions
I. INTRODUCTION
between specific layers, with joint fine-tuning of the entire backbone. (b)
RGBT tracking aims to leverage the complementary in- Interactions between all corresponding layers, with the pre-trained back-
bone being frozen. (c) Interactions between all corresponding layers, and
formation of visible light (RGB) and thermal infrared (TIR)
interactions among all layers, with joint fine-tuning with the backbone. (d)
images to predict the location and size of an object. By PerformancecomparisononLasHeR,andcomparisonofadditionalparameters
combining the penetration capability and nighttime sensitivity andGFLOPs.
of TIR images with the rich color and texture of RGB im-
ages under favorable lighting conditions, RGBT tracking has
attracted significant research attention. Numerous innovative lightweight interaction modules and adopting the strategy of
works [1]–[5] have been proposed to improve the robustness interacting between all corresponding layers to achieve inter-
and accuracy of RGBT tracking. modal interactions, as illustrated in Fig. 1 (b). For instance,
As a multimodal vision task, most RGBT tracking studies [4] deploy lightweight adapters in all layers to perform bi-
focus on modality interaction modules, and these approaches directional inter-modal interactions. Similar interaction strate-
canbebroadlyclassifiedintotwocategories.Onecategoryin- gies are also demonstrated in [9], [10]. However, these meth-
volves building complex interaction modules [1], [6], [7] with ods have restricted interaction capabilities and representation
powerful representation to achieve inter-modal interactions. capacity due to their extremely low parameter count. As a
For example, [1] employ six cross-attention blocks and extra result,existingmethodsstruggletobalanceinteractioncapabil-
fused template features for inter-modal feature interactions. ity and efficiency when constructing multimodal interactions
Similarly, [6] stack multiple Swin-Transformer [8] blocks to between all corresponding layers.
perform self-attention between two modality features. How- In addition, features from different layers show significant
ever, due to the significant computational burden, these meth- complementarity: low-level features provide detailed texture
ods can only interact at a limited number of specific layers, information, while high-level features capture abstract and
as shown in Fig. 1(a). Another category involves designing semantic content. Nevertheless, existing methods adopt CNNs
4202
guA
61
]VC.sc[
1v72880.8042:viXraor Transformer networks, where the small receptive field of basedontheirfeaturefusionstrategies.Onecategoryinvolves
CNNs hinders the modeling of global information between latefusion,whichtakesplaceafterthemainfeatureextraction
modalities, and the Transformer is hard to interact with in- backbone. For instance, [11] embed the multimodal feature
formation from multi-layer features due to the O(N2) com- concatenation process into the framework of a strong tracker
putational complexity. Hence, these architectural limitations DiMP[12]forRGBTtracking.[13]utilizetwo-streamconvo-
prevent previous RGBT tracking methods from achieving lutionalnetworkwithincreasingcouplingfilterstoextractboth
comprehensive interaction of all layer information, which is common and individual features, and ultimately achieves fu-
essential for robust RGBT tracking. sion through a simple channel concatenation. The second cat-
To address these issues, we propose a novel All-layer egory involves modality interaction during the feature extrac-
multimodal Interaction Network named AINet, as shown in tion phase. For example, [1] build a template-bridged cross-
Fig. 1 (c), for robust RGBT tracking. In particular, modality attention module between the RGB and TIR search areas to
feature interaction aims to leverage the mutual enhancement promotethoroughmodalityfusion.[4]proposeabi-directional
andcomplementarityoftwomodalities,whilecomplementary adaptertoperceivethedominantmodalitychangesindynamic
information reflected in their differences. Thus, we design a scenes and adaptively fuse multimodal information. However,
difference-basedfusionMamba(DFM)withlinearcomplexity, existing methods are limited by computational costs and
which not only can model modality differences to enhance cannot utilize information from all layers. Our method can
each modality feature, but also can be efficiently applied to fully leverage all-layer multimodal information while keeping
each layer. When performing feature interactions across all asmallnumberofparametersandlowcomputationalresource
layers, handling a large number of token sequences (3840 to- consumption.
kens)resultsinasignificantcomputationalburdenforexisting
networks. Although the standard Mamba network is efficient, B. Vision State Space Model
it is prone to forgetting early token information due to the
State space models (SSMs) [14]–[16] derived from classi-
inherent properties of causal models. To mitigate this issue,
cal control theory connects the input and output sequences
we design an Order-dynamic Fusion Mamba (OFM) module,
through hidden states. Recently, Mamba [17] has been widely
which dynamically adjusts the scan order of different layers
applied in various fields due to its selective mechanism and
based on the input to alleviate layer information forgetting.
efficient hardware acceleration design. [18] expand to visual
Extensive experiments on four public RGBT tracking bench-
tasks for the first time by bidirectional sequence modeling.
marks show that AINet significantly outperforms existing
[19] propose a residual state space block, using convolution
state-of-the-artmethodsinbothperformanceandefficiency,as
and channel attention to enhance the performance of Vanilla
depictedinFig.1(d).Ourmaincontributionsaresummarized
Mamba,andachievedsuccessinthefieldofimagerestoration.
as follows:
[20] integrate a selective scanning mechanism into the motion
• We propose a novel all-layer multimodal interaction generation task, constructing HTM and BSM modules to
network for RGBT tracking. It conducts multimodal handle temporal motion data and bidirectionally capture the
interaction of each layer and all layer interaction in a channel-wise flow of hidden information within the latent
progressivefusionMamba.Tothebestofourknowledge, pose. However, the application of Mamba in RGBT tracking
wearethefirsttointroducetheMambanetworkinRGBT has not yet been sufficiently explored. In this work, we lever-
tracking. age Mamba for modality enhancement and all-layer fusion,
• We design a difference-based fusion Mamba, which exploring the potential of Mamba in RGBT tracking.
achievesinter-modalenhancedfusionbymodellinginter-
modaldifferencestocapturecomplementaryinformation, III. METHODOLOGY
and efficiently applies it to each layer.
In this paper, we propose a novel All-layer multimodal
• We design an order-dynamic fusion Mamba, which im-
InteractionNetwork,namedAINet,forRGBTtracking,which
plementsall-layerfeatureinteractionwithaninput-aware
performs efficient and effective feature interactions of all
dynamicscanningschemetomitigateinformationforget-
modalities and layers in a progressive fusion Mamba. In
ting of early input tokens.
particular, AINet achieves multimodal interactions at each
• Extensive experiments on four RGBT benchmarks
layer by the designing difference-based fusion Mamba, and it
demonstrate that AINet achieves new state-of-the-art re-
employsanorder-dynamicfusionMambatoestablishall-layer
sults while maintaining a controllable number of param-
interactions. Next, we first review the mamba, then introduce
eters and computational load.
the overall architecture of AINet, and finally, we describe in
II. RELATEDWORK detail the two fusion Mamba architectures.
To visually demonstrate the necessity of applying all layer
A. RGBT Tracking
features, we perform a visual analysis of the final fusion
In recent years, the field of RGBT tracking has made features that incorporate different numbers of layer features,
significant progress, with many impressive works emerging. as shown in Fig. 3. It can be observed that as the number of
Existing methods can be roughly divided into two categories layers increases, the model’s response to the target becomesT
em
p la te S E m b e d d in B lo c
kT
r a n s fo r m B lo c
kT
r a n s fo r m ... B lo c
kT
r a n s fo r m EL E li l en e mmea eer nn P tt --r wwoj iie ssc eet i AMo dn u dltiply C R E FC elo e an m tc uea rn ete t Rn -wa et i oi so re dn eS ru inb g
ea rch g e r e r e r 0 1 ... 11 Layer index SSM State Space Model
RGB
tokens DFM DFM DFM
3840 tokens ... ... ... ...
OFM Head
TIR
T tokens
em
T T T
p la te E m Br a n Br a n Br a n
S ea
rch
b e d d in
g
lo c ks fo r m
e
r
lo c ks fo r m
e
r
... lo c ks fo r m
e
r
Fall Downsampling Order
Predictor Dynamic
MLP R Conv1d SSM
A
Forward d
Conv1d SSM Tanh C 0 FC 1 Conv1d SSM d itio
1 5 Backward n
σ Send to Layer Indices Conv1d SSM
Mamba OFM 2 0
3 10
z
... ... Fall SiLU
11 2
DFM (Difference-based Fusion Mamba) Original Reorderd OFM (Order-dynamic Fusion Mamba))
Fig. 2. The overall architecture of our proposed AINet. Firstly, RGB and TIR images are embedded as tokens and fed into Transformer blocks for joint
feature extraction and relationship modeling between search and template images. Following each block, the tokens from both modalities are processed by
theDFMfordifferenceinformationenhancementandthenreturnedtothebackbone.Meanwhile,thefusionfeaturesateachlayerarecascadedandfedinto
theOFMforall-layerinteraction.Finally,theoutputfeaturesfromtheOFMaresenttothetrackingheadfortargetlocalization.
morecomprehensiveandfocused,validatingthenecessityand h(t)∈RN. The models can be represented by linear ordinary
importance of applying all layer features. differential equations (ODEs) as follows:
′
h (t)=Ah(t)+Bx(t), y(t)=Ch(t), (1)
RGB TIR AINet-1 AINet-3 AINet-6 AINet-12
where N is the state size, A ∈ RN×N, B ∈ RN×1,
C ∈ R1×N. Specifically, ∆ denotes the timescale parameter
to transform the continuous parameters A, B to discrete pa-
RGB TIR AINet-1 AINet-3 AINet-6 AINet-12
rameters A, B. The commonly used method for discretization
isthezero-orderhold(ZOH)rule,whichisdefinedasfollows:
A=exp(∆A), B =(∆A)−1(exp(∆A)−I)·∆B. (2)
RGB TIR AINet-1 AINet-3 AINet-6 AINet-12
Then, the discretized version of Eq. 1 with step size ∆ can
be rewritten as:
h =Ah +Bx , y =Ch . (3)
t t−1 t t t
Fig.3. Illustrationoffusionfeaturevisualizationwithdifferentlayersapplied.
Here,“n”inAINet-“n”representsthenumberoflayersapplied.
Finally, the models compute output y through a global
convolution within a structured convolutional kernel K.
M−1
K = (CB,CAB,...,CA B), y=x∗K, (4)
A. Preliminaries
where M denotes the sequence length of x. In contrast to
The state-space sequence model (SSM) [14] and traditional SSMs, Mamba introduces the Selective Scanning
Mamba [17] are inspired by continuous linear systems, Mechanism (S6), which allows selective focus on what is
where a one-dimensional function or sequence, denoted as importantinlongcontexts.Inthiswork,weextendtheMamba
x(t) ∈ R, is mapped to x(t) ∈ R through a hidden states module to support all-layer multimodal interactions.B. Overall Architecture the activation function, is multiplied element-wise with xrgb
i
and xtir to obtain the difference compensation features for
Inspired by the success of ViT [21] in tracking tasks, we i
each modality. Finally, these compensated features are added
follow the OSTrack [22] framework and extend its backbone
backtoxrgb andxtir toobtaintheenhancedfeaturesxˆrgb and
to adapt to multimodal input. As shown in Fig. 2, our AINet i i i
xˆtir. The following equation summarizes the process:
incorporates a two-stream encoder structure, sharing the same i
parametersforboththeRGBandTIRmodalities.Additionally, xˆrgb =xrgb+xtir×τ(Mamba(xd)),
itincludesasetofdifference-basedfusionMamba(DFM),an i i i i (7)
xˆtir =xtir+xrgb×τ(Mamba(xd)),
order-dynamic fusion Mamba (OFM), and a prediction head. i i i i
Specifically, AINet first processes the search and template
where τ denotes the tanh function. Subsequently, the en-
frames of the given RGB and TIR modalities through the
hanced modality features are processed to obtain the fused
patch and position embedding layers, obtaining the initial
feature of the current layer:
tokenpairsforeachmodality.Thesearchandtemplatetokens
from each modality are then concatenated along the token xdfm =τ(LN([xrgb,xtir]·W )), (8)
i i i i
dimension to form RGB tokens xrgb and TIR tokens xtir,
0 0 where W ∈ R2C×C is a linear layer with reduced channel
which are fed into the ViT blocks for feature extraction and i
dimension, and LN represents layer normalization.
joint relationship modeling. Since the DFM is embedded in
each layer to facilitate modality interactions, for the i-th layer
D. Order-dynamic Fusion Mamba (OFM)
block, it learns to integrate enhanced modality features from
the output of the previous DFM, as described below. Thestrongcomplementaritybetweendifferentfeaturelayers
in deep networks has been proven in many visual tasks [23]–
(xˆrgb,xˆtir,xdfm)=FDFM(xrgb,xtir),i∈[1,N], (5)
i i i i i i [25]. However, no current method applies all feature layers
where N refers to the total number of blocks. xˆrgb and to RGBT tracking, primarily because existing methods use
i Transformers for feature interactions. To this end, we design
xˆtir represent the enhanced RGB and TIR modality features,
rei spectively. FDFM and xdfm denote the DFM and the fused anOrder-dynamicFusionMamba(OFM)toefficientlyandef-
i i fectively interact with features from all layers by dynamically
features, respectively, at the i-th layer. When all blocks are
executed, xdfm from all layers are concatenated along the adjusting the scan order of different layers in Mamba.
i In particular, We first concatenate the output features xdfm
tokendimensionandfedintotheOFM,representedasFOFM, i
i of each DFM layer along the token dimension to form a long
for all layers to interact. Then, the prediction head H predicts
token sequence F containing features from all layers. We
the tracking result B based on the output of the OFM. all
theninputF totheOFMandperformthefollowingforward
all
B =H(FOFM[xdfm,xdfm,...,xF]), (6) and backward scanning modeling process:
1 2 N
where [·] refers to concatenation along the token dimension. Fforward =SSM([xdfm,xdfm,...,xdfm],W ),
all 1 2 N c (9)
Fbackward =SSM([xdfm,xdfm ,...,xdfm],W ),
C. Difference-based Fusion Mamba (DFM) all N N−1 1 c
Visible and thermal infrared modalities capture comple- where W c represents a 1D convolution layer and SSM de-
mentary object properties due to different imaging principles, notestheselectivescanningmodel.However,onlyperforming
which implies that the differences between modalities often forward and backward scans can potentially ignore the first
contain complementary information. Nevertheless, current ad- andlastlayertokens.Therefore,weproposeanorder-dynamic
vanced approaches mainly adopt Transformer networks with scanning scheme that allows the scanning process to start and
long-range modeling capabilities to directly interact the fea- end at any layer. This innovative design enables OFM to rank
tures of the modalities, which ignores the explicit utilization the importance of different layer features based on the input
of modality differences and limits multimodal interactions at data.
each layer due to high secondary computational overhead. Intheorder-dynamicscanningmodeling,F all isfirstdown-
To this end, we design a difference-based fusion Mamba sampled (D) to the specified dimension and then fed into a
(DFM) with linear complexity, as depicted in Fig. 2 (b), multi-layer perceptron (MLP) and a fully connected layer
whichcanbeemployedtomodelmodalitydifferencesateach (FC) to predict an index covering the scanning order of all
layer toenhance modal representation. Tocapture inter-modal layers. The process is expressed as follows:
differences and enrich feature learning, DFM employs the
index=FC(MLP(D(F))). (10)
principles of differential amplifier circuits, which suppresses
common-mode signals and amplifies differential ones.
Then, the OFM reorders the long token sequence by the
In particular, we obtain the modality difference feature xd
i index. Thus, the dynamic ordering scan modeling can be
by subtracting between modal features in the same layer.
formulated as follows:
Since the difference feature contains both useful information
and noise, xd
i
is then fed to the Mamba to suppress noise xo ar llder ={[xd 1fm,xd 2fm,...,xd Nfm],index},
(11)
while enhancing useful information. Next, xd i, processed by F ad ly lnamic =SSM(xo ar llder,W c),where {·,index} represents the input sequence ordered ac- alignedRGBTvideosequencestotalingapproximately734.8K
cording to the given index, and xorder denotes the ordered frames.Italsoprovidesannotationsfor19challengeattributes,
all
result. Next, a simple gating strategy in Mamba fuses the such as HI (High Illumination) and AIV (Abrupt Illumination
results of three-scan modeling. Finally, all layer features are Variation), significantly raising the dataset’s challenge.
aggregated by element-wise addition and fed into the tracking 1) Overall Comparison. As shown in Table I, we compare
head. our method with 17 state-of-the-art trackers using PR, NPR,
and SR metrics on the LasHeR dataset. The results demon-
IV. EXPERIMENT
stratethatourmethodsignificantlyoutperformsothertrackers.
A. Implementation Details Specifically, compared to the most powerful tracker, GMMT,
We implement our AINet based on the PyTorch and train it our method achieves improvements of 3.5%/3.1%/2.5% in
on single NVIDIA RTX 4090 GPU. We follow the hyper- PR/NPR/SR. Compared to the unified frameworks One-
parameter settings of the baseline model [22] for the loss Tracker, Un-Track, and SDSTrack, we achieve performance
function.Forparameterinitialization,weutilizethepretrained improvements of 7.0%/5.3%, 7.5%/5.5%, and 7.7%/6.0% in
model provided by DropTrack [39]. For each sequence in the PR/SR,respectively.Theseresultsfullydemonstratethesupe-
training set, we collect training samples and apply standard riority of our method.
data augmentation operations, including rotation, translation, 2)Challenge-basedComparison.Wealsopresenttheresults
and gray-scaling, following the data processing scheme of the ofAINetagainstthemostadvancedRGBTtrackers,including
base tracker [22]. During training, we use the AdamW [40] SDSTrack [9], GMMT [7], QAT [2], and TBSI [1], on
optimizer with a weight decay of 10−4, and set the batch different challenge subsets. The evaluation results are shown
size and learning rate to 16 and 10−4, respectively. The entire in Fig 4, where each corner represents the attributes of the
networkistrainedend-to-endover15epochs,witheachepoch challengesubsetandthehighestandlowestperformanceunder
providing 6 × 104 pairs of samples. We use the LasHeR that attribute. The challenge subsets include no occlusion
training set to train our network, which is used to evaluate (NO), partial occlusion (PO), total occlusion (TO), hyaline
RGBT210 [41], RGB234 [42] and LasHeR [43]. For the occlusion (HO), motion blur (MB), low illumination (LI),
evaluation of VTUAV [32], we utilize the training set from high illumination (HI), abrupt illumination variation (AIV),
VTUAV as the training data. low resolution (LR), deformation (DEF), background clutter
(BC),similarappearance(SA),cameramoving(CM),thermal
B. Quantitative Comparison
crossover(TC),framelost(FL),out-of-view(OV),fastmotion
We evaluate our proposed AINet on four popular RGBT (FM), scale variation (SV), and aspect ratio change (ARC).
tracking benchmarks: RGBT210, RGBT234, LasHeR and The results show that AINet achieves the best performance in
VTUAV, and compare the performance with 20 state-of-the- almostallsubsets,especiallywithsignificantimprovementsin
artRGBTtrackers.WeadoptthePrecisionRate(PR),Success scenarioslikeMB,DEF,andFL,provingthatAINethasgreat
Rate (SR), and Normalized Precision Rate (NPR) from One- potential in various complex tracking scenarios.
PassEvaluation(OPE)asmetricsforquantitativeperformance Evaluation on VTUAV dataset.VTUAVcollectsRGBTdata
measurement, which are commonly used in current RGBT from UAV scenarios, expanding the application of RGBT
tracking tasks. The effectiveness of our proposed AINet is tracking. It contains 500 aligned RGBT video sequences
demonstrated in Table I, which summarizes the comparison with up to 1.7 million frames. We focus our experiments on
results. its short-term tracking subset. As shown in Table I, AINet
EvaluationonRGBT210dataset.RGBT210isachallenging achieves a PR/SR performance of 88.0%/75.3%, outperform-
dataset that contains 210 pairs of RGBT video sequences, ing four other trackers. Compared to QAT [2], the second-
totalingapproximately210Kframes,andprovidesannotations best algorithm on this dataset, AINet shows an advantage of
for 12 different challenge attributes. As shown in Table I, 7.9%/8.6% in PR/SR. These results demonstrate the advan-
AINet achieves the best performance, outperforming the cur- tages of our method under the UAV perspective.
rent state-of-the-art QAT and TATrack by 0.7%/2.9% and
C. Ablation Study
2.2%/3.0% in PR/SR, respectively.
Evaluation on RGBT234 dataset. RGBT234 is one of Componentanalysis.InTableII,weconductseveralablation
the most influential and extensively evaluated RGBT track- studies on the LasHeR dataset to verify the effectiveness of
ing dataset, containing 234 pairs of aligned RGBT videos, key components in our AINet.
amounting to approximately 233.4K frames. As shown in Baseline denotes the removal of DFM and OFM modules
Table I, we evaluate AINet against 20 state-of-the-art trackers from our method, while maintaining consistent training data
on RGBT234 dataset. Compared with QAT and GMMT, the and losses.
secondbest-performingalgorithmsinPRandSRrespectively, w/ DFM indicates that each layer in the Baseline back-
our method shows a performance advantage of 0.8%/2.9% in bone network is equipped with a DFM module, achieving
PR/SR and 1.3%/2.6% in PR/SR respectively. improvements of 1%/0.8%/0.5% in PR/NPR/SR, respectively.
Evaluation on LasHeR dataset. LasHeR is the largest and Thisexperimentshowsthatdifference-basedfusionMambais
most challenging RGBT tracking dataset, containing 1,224 effective.TABLEI
THEPR,NPR,ANDSRSCORES(%)OFVARIOUSTRACKERSONFIVEDATASETS.THEBESTANDSECONDRESULTSAREINREDANDBLUECOLORS,
RESPECTIVELY.
RGBT210 RGBT234 LasHeR VTUAV FPS
Methods Pub. Info. Backbone
PR↑ SR↑ PR↑ SR↑ PR↑ NPR↑ SR↑ PR↑ SR↑ ↑
mfDiMP [26] ICCVW 2019 ResNet−50 78.6 55.5 − − 44.7 39.5 34.3 67.3 55.4 10.3
CAT [27] ECCV 2020 VGG−M 79.2 53.3 80.4 56.1 45.0 39.5 31.4 − − 20
ADRNet [28] IJCV 2021 VGG-M − − 80.7 57.0 − − − 62.2 46.6 25
APFNet [29] AAAI 2022 VGG−M − − 82.7 57.9 50.0 43.9 36.2 − − 1.3
DMCNet [30] TNNLS 2022 VGG−M 79.7 55.5 83.9 59.3 49.0 43.1 35.5 − − 2.3
ProTrack [31] ACM MM 2022 ViT−B − − 78.6 58.7 50.9 − 42.1 − − 30
HMFT [32] CVPR 2022 ResNet−50 78.6 53.5 78.8 56.8 − − − 75.8 62.7 30.2
MFG [33] TMM 2022 ResNet−18 74.9 46.7 75.8 51.5 − − − − − −
DFNet [34] TITS 2022 VGG−M − − 77.2 51.3 − − − − − −
DRGCNet [35] IEEE SENS J 2023 VGG−M − − 82.5 58.1 48.3 42.3 33.8 − − 4.9
CMD [36] CVPR 2023 ResNet−50 − − 82.4 58.4 59.0 54.6 46.4 − − 30
ViPT [10] CVPR 2023 ViT−B − − 83.5 61.7 65.1 − 52.5 − − −
TBSI [1] CVPR 2023 ViT−B 85.3 62.5 87.1 63.7 69.2 65.7 55.6 − − 36.2
QAT [2] ACM MM 2023 ResNet−50 86.8 61.9 88.4 64.4 64.2 59.6 50.1 80.1 66.7 22
TATrack [3] AAAI 2024 ViT−B 85.3 61.8 87.2 64.4 70.2 66.7 56.1 − − 26.1
BAT [4] AAAI 2024 ViT−B − − 86.8 64.1 70.2 − 56.3 − − −
GMMT [7] AAAI 2024 ViT−B − − 87.9 64.7 70.7 67.0 56.6 − − −
OneTracker [37] CVPR 2024 ViT−B − − 85.7 64.2 67.2 − 53.8 − − −
Un-Track [38] CVPR 2024 ViT−B − − 84.2 62.5 66.7 − 53.6 − − −
SDSTrack [9] CVPR 2024 ViT−B − − 84.8 62.5 66.5 − 53.1 − − 20.9
AINet (256×256) − ViT−B 86.8 64.1 89.1 66.8 73.0 69.0 58.2 87.1 74.5 38.1
AINet (384×384) − ViT−B 87.5 64.8 89.2 67.3 74.2 70.1 59.1 88.0 75.3 37.5
TABLEII
QUANTITATIVECOMPARISONOFDIFFERENTVARIANTSOFOURMETHOD
ONTHELASHERDATASET.OFM∗INDICATESTHEREMOVALOFTHE
ORDER-DYNAMICSCANNINGSCHEMEWITHINTHEOFM.
Methods Resolution PR↑ NPR↑ SR↑
Baseline 256×256 71.1 67.5 56.9
w/ DFM 256×256 72.1 68.3 57.4
w/ OFM 256×256 72.2 68.0 57.3
w/ DFM OFM∗ 256×256 72.5 68.6 57.8
w/ DFM OFM 256×256 73.0 69.1 58.2
w/ DFM OFM 384×384 74.2 70.1 59.1
Fig.4. PrecisionRate(PR)ofchallengeattributesonLasHeR.Theaxesof addition, we remove our designed order-dynamic scanning
eachattributehavebeennormalized. scheme in w/ DFM OFM and denote it as w/ DFM OFM∗.
The results show a certain decrease, proving the effectiveness
w/ OFM denotes that each layer in the Baseline back- and necessity of order-dynamic scanning.
bone network uses simple feature addition to obtain fused Impact of different resolutions. Increasing the resolution of
features and is equipped with our proposed OFM module, input images for performance improvement has been proven
achieving improvements of 1.1%/0.5%/0.4% in PR/NPR/SR, effective in many unimodal vision tasks [44]–[46]. However,
respectively.Thisexperimentshowsthatorder-dynamicfusion existing RGBT tracking algorithms are constrained by the
Mamba is effective. highcomputationalcomplexityoftheinteractionmodule,pre-
w/ DFM OFM represents applying both our proposed venting them from utilizing this strategy. Benefiting from the
DFM and OFM modules in the Baseline, achieving a clear computational efficiency of Mamba, AINet introduce a larger
performanceimprovementof1.9%/1.6%/1.3%inPR/NPR/SR input resolution of 384×384 to further enhance performance.
metrics, respectively. This experiment demonstrates the ef- As shown in Table II, AINet achieves new state-of-the-art
fectiveness of our proposed progressive fusion Mamba. In performance with this resolution increase, demonstrating itsfully exploit information from all layers while balancing
performance and efficiency.
Out of Memory
TABLEIV
COMPARISONOFPERFORMANCE,ADDITIONALPARAMETERS,FLOPS,
ANDFPSWITHADVANCEDTRACKERSONLASHER.ALLALGORITHMS’
FPSAREEVALUATEDONASINGLERTX4090GPU.
Methods Pub.Info. PR↑ SR↑ Params↓ FLOPs↓ FPS↑
TBSI CVPR2023 69.2 55.6 99.3M 38.5G 35.7
GMMT AAAI2024 70.7 56.6 962.2M 146.5G 22.4
SDSTrack CVPR2024 66.5 53.1 10.1M 13.1G 18.4
Ours − 73.0 58.2 15.9M 5.2G 38.1
D. Efficiency Analysis
Fig.5. ComparisonofGPUmemoryusagebetweenourproposedframework To verify the efficiency of our method, we perform a
andatransformer-basedapproachundervariationsinlayercountandresolu- quantitativecomparisonwithexistingstate-of-the-artmethods.
tion.Thebluelineindicatesresolutionvariation,andthegreenlineindicates
As shown in Table IV, we present the number of parameters,
variationinlayercount.
computational burden compared to their respective baselines,
and the inference speed of each model. Compared to the fully
TABLEIII
finetunedhigh-performancealgorithmsTBSIandGMMT,our
ABLATIONSTUDYOFAPPLYINGDIFFERENTNUMBEROFLAYERSONTHE
LASHERDATASET. approachissuperiorinallmetrics.Whencomparedtothepar-
tially finetuned advanced method SDSTrack, despite a slight
Variants Layer index PR↑ NPR↑ SR↑ difference in the number of parameters, our approach shows
AINet-1 11 71.4 67.4 56.9
a significant advantage in performance and other efficiency
AINet-3 0,6,11 71.9 68.1 57.4
metrics.Notably,whileourapproachexcelsinFLOPsmetrics,
AINet-6 0,2,4,6,8,11 72.1 68.4 57.8
it underperforms in inference speed. This is attributed to the
AINet-12 all 73.0 69.1 58.2
factthatMambaisnotyetadaptedtotheavailableacceleration
hardware.
effectiveness across multiple datasets. In addition, we com-
pare the GPU memory usage of AINet and its Transformer V. CONCLUSION
variant (AINet-Transformer) with increased resolution Fig. 5.
In particular, we use cross-attention instead of DFM and self- In this paper, we explore for the first time the potential
attentioninsteadofOFM.AINet-Transformer’smemoryusage of Mamba in RGBT tracking by designing a novel All-
grows quadratically with resolution, becoming impractical, layerInteractiveNetwork(AINet),whicheffectivelyintegrates
while AINet scales linearly, maintaining efficiency. Note that information from all layers to achieve robust tracking perfor-
the batch size is 1. These experiments fully demonstrate the mance.ThecoreconceptofAINetistointroduceaprogressive
efficiency advantage of AINet’s interaction module. fusion of Mamba to enable efficient and effective all-layer
Impact of different layers. To verify the effectiveness of modalityinteractions.Specifically,AINetdesignsadifference-
utilizing all layer features for RGBT tracking, we explore based fusion Mamba that enhances modality interactions at
the impact of employing different numbers of layers. In each layer of the backbone network by modeling modality
Table III, we present three variants denoted as AINet-“n”, differences. Additionally, an order-dynamic fusion Mamba is
where“n”representsthenumberoflayersapplied.Theresults designed to perform interactions across all layer features,
show that overall performance improves as the number of mitigating the risk of early token information loss. Extensive
layers increases. Compared to using only the last layer for experiments demonstrate the superior performance of the
fusion, involving all layers leads to a performance increase of proposedmethodinfourpopularRGBTtrackingbenchmarks.
1.6%/1.7%/1.3%inPR/NPR/SRonLasHeR.Furthermore,we Currently, we still adopt ViT as the backbone network for
analyze the resource constraints faced by existing interaction modal feature extraction, thus there is room for improvement
strategies when applying different numbers of layer features. in the overall model efficiency. To enhance efficiency, we
As shown in Fig. 5, the GPU memory usage of AINet- plan to adopt Mamba as the backbone network in the future.
Transformer surges with additional layers, resulting in OOM Nevertheless,thereisnoavailableMamba-basedtrackingpre-
(Out of Memory) at five layers. In contrast, our approach trainingmodel,thusweintendtoestablishamodeldistillation
shows a linear correlation between computational resource method to create a pure Mamba-based multimodal tracking
requirements and the number of layers, allowing AINet to network.REFERENCES [21] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
[1] T. Hui, Z. Xun, F. Peng, J. Huang, X. Wei, X. Wei, J. Dai, J. Han, J.Uszkoreit,andN.Houlsby,“Animageisworth16x16words:Trans-
andS.Liu,“Bridgingsearchregioninteractionwithtemplateforrgb-t formersforimagerecognitionatscale,”inInternationalConferenceon
tracking,” in Proceedings of the IEEE conference on computer vision LearningRepresentations,2021.
andpatternrecognition,2023,pp.13630–13639. [22] B. Ye, H. Chang, B. Ma, S. Shan, and X. Chen, “Joint feature
[2] L. Liu, C. Li, Y. Xiao, and J. Tang, “Quality-aware rgbt tracking learningandrelationmodelingfortracking:Aone-streamframework,”
via supervised reliability learning and weighted residual guidance,” in inProceedingsoftheIEEEEuropeanConferenceonComputerVision,
ProceddingsofACMInternationalConferenceonMultimedia,2023,pp. 2022,pp.341–357.
3129–3137. [23] T.-Y.Lin,P.Dolla´r,R.Girshick,K.He,B.Hariharan,andS.Belongie,
[3] H. Wang, X. Liu, Y. Li, M. Sun, D. Yuan, and J. Liu, “Temporal “Featurepyramidnetworksforobjectdetection,”inProceedingsofthe
adaptive rgbt tracking with modality prompt,” in Proceedings of the IEEEconferenceoncomputervisionandpatternrecognition,2017,pp.
AAAI Conference on Artificial Intelligence, vol. 38, no. 6, 2024, pp. 2117–2125.
5436–5444. [24] Y.Pang,X.Zhao,L.Zhang,andH.Lu,“Multi-scaleinteractivenetwork
[4] B. Cao, J. Guo, P. Zhu, and Q. Hu, “Bi-directional adapter for multi- forsalientobjectdetection,”inProceedingsoftheIEEEconferenceon
modal tracking,” in Proceedings of the AAAI Conference on Artificial computervisionandpatternrecognition,2020,pp.9413–9422.
Intelligence,vol.38,no.2,2024,pp.927–935. [25] Y.Liu,S.Zhang,J.Chen,Z.Yu,K.Chen,andD.Lin,“Improvingpixel-
[5] A. Lu, J. Zhao, C. Li, Y. Xiao, and B. Luo, “Breaking modality gap basedmimbyreducingwastedmodelingcapability,”inProceedingsof
inRGBTtracking:Coupledknowledgedistillation,”inProceddingsof theIEEEinternationalconferenceoncomputervision,2023,pp.5361–
ACMInternationalConferenceonMultimedia,2024. 5372.
[6] H. Fan, Z. Yu, Q. Wang, B. Fan, and Y. Tang, “Querytrack: Joint- [26] L. Zhang, M. Danelljan, A. Gonzalez-Garcia, J. van de Weijer, and
modality query fusion network for rgbt tracking,” IEEE Transactions F.ShahbazKhan,“Multi-modalfusionforend-to-endrgb-ttracking,”in
onImageProcessing,2024. ProceedingsoftheIEEEInternationalConferenceonComputerVision
[7] Z. Tang, T. Xu, X. Wu, X.-F. Zhu, and J. Kittler, “Generative-based Workshops,2019.
fusionmechanismformulti-modaltracking,”inProceedingsoftheAAAI [27] C.Li,L.Liu,A.Lu,Q.Ji,andJ.Tang,“Challenge-awarergbttracking,”
Conference on Artificial Intelligence, vol. 38, no. 6, 2024, pp. 5189– inProceedingsoftheIEEEEuropeanConferenceonComputerVision,
5197. 2020,pp.222–237.
[8] Z.Liu,Y.Lin,Y.Cao,H.Hu,Y.Wei,Z.Zhang,S.Lin,andB.Guo, [28] P.Zhang,D.Wang,H.Lu,andX.Yang,“Learningadaptiveattribute-
“Swin transformer: Hierarchical vision transformer using shifted win- drivenrepresentationforreal-timergb-ttracking,”InternationalJournal
dows,”inProceedingsoftheIEEEinternationalconferenceoncomputer ofComputerVision,vol.129,pp.2714–2729,2021.
vision,2021,pp.10012–10022. [29] Y. Xiao, M. Yang, C. Li, L. Liu, and J. Tang, “Attribute-based pro-
[9] X.Hou,J.Xing,Y.Qian,Y.Guo,S.Xin,J.Chen,K.Tang,M.Wang, gressivefusionnetworkforrgbttracking,”inProceedingsoftheAAAI
Z. Jiang, L. Liu et al., “Sdstrack: Self-distillation symmetric adapter Conference on Artificial Intelligence, vol. 36, no. 3, 2022, pp. 2831–
learningformulti-modalvisualobjecttracking,”inProceedingsofthe 2838.
IEEEconferenceoncomputervisionandpatternrecognition,2024,pp. [30] A. Lu, C. Qian, C. Li, J. Tang, and L. Wang, “Duality-gated mutual
26551–26561. condition network for rgbt tracking,” IEEE Transactions on Neural
[10] J. Zhu, S. Lai, X. Chen, D. Wang, and H. Lu, “Visual prompt multi- NetworksandLearningSystems,2022.
modal tracking,” in Proceedings of the IEEE conference on computer [31] J. Yang, Z. Li, F. Zheng, A. Leonardis, and J. Song, “Prompting for
visionandpatternrecognition,2023,pp.9516–9526. multi-modaltracking,”inProceddingsofACMInternationalConference
[11] L. Zhang, M. Danelljan, A. Gonzalez-Garcia, J. Van De Weijer, and onMultimedia,2022,pp.3492–3500.
F. Shahbaz Khan, “Multi-modal fusion for end-to-end rgb-t tracking,” [32] P.Zhang,J.Zhao,D.Wang,H.Lu,andX.Ruan,“Visible-thermaluav
inProceedingsoftheIEEEinternationalconferenceoncomputervision tracking:Alarge-scalebenchmarkandnewbaseline,”inProceedingsof
workshops,2019,pp.0–0. theIEEEconferenceoncomputervisionandpatternrecognition,2022,
[12] G. Bhat, M. Danelljan, L. V. Gool, and R. Timofte, “Learning dis- pp.8886–8895.
criminativemodelpredictionfortracking,”inProceedingsoftheIEEE [33] X. Wang, X. Shu, S. Zhang, B. Jiang, Y. Wang, Y. Tian, and F. Wu,
conferenceoncomputervisionandpatternrecognition,2019,pp.6182– “Mfgnet:Dynamicmodality-awarefiltergenerationforrgb-ttracking,”
6191. IEEETransactionsonMultimedia,vol.25,pp.4335–4348,2022.
[13] J.Peng,H.Zhao,Z.Hu,Y.Zhuang,andB.Wang,“Siameseinfraredand [34] J. Peng, H. Zhao, and Z. Hu, “Dynamic fusion network for rgbt
visiblelightfusionnetworkforrgb-ttracking,”InternationalJournalof tracking,” IEEE Transactions on Intelligent Transportation Systems,
MachineLearningandCybernetics,vol.14,no.9,pp.3281–3293,2023. vol.24,no.4,pp.3822–3832,2022.
[14] A. Gu, I. Johnson, K. Goel, K. Saab, T. Dao, A. Rudra, and C. Re´, [35] J.Mei,D.Zhou,J.Cao,R.Nie,andK.He,“Differentialreinforcement
“Combiningrecurrent,convolutional,andcontinuous-timemodelswith and global collaboration network for rgbt tracking,” IEEE Sensors
linear state space layers,” Advances in neural information processing Journal,vol.23,no.7,pp.7301–7311,2023.
systems,vol.34,pp.572–585,2021. [36] T.Zhang,H.Guo,Q.Jiao,Q.Zhang,andJ.Han,“Efficientrgb-ttracking
[15] E.Nguyen,K.Goel,A.Gu,G.Downs,P.Shah,T.Dao,S.Baccus,and viacross-modalitydistillation,”inProceedingsoftheIEEEconference
C.Re´,“S4nd:Modelingimagesandvideosasmultidimensionalsignals oncomputervisionandpatternrecognition,2023,pp.5404–5413.
withstatespaces,”Advancesinneuralinformationprocessingsystems, [37] L.Hong,S.Yan,R.Zhang,W.Li,X.Zhou,P.Guo,K.Jiang,Y.Chen,
vol.35,pp.2846–2861,2022. J. Li, Z. Chen et al., “Onetracker: Unifying visual object tracking
[16] M.Zhang,K.K.Saab,M.Poli,T.Dao,K.Goel,andC.Re,“Effectively with foundation models and efficient tuning,” in Proceedings of the
modelingtimeserieswithsimplediscretestatespaces,”inTheEleventh IEEEconferenceoncomputervisionandpatternrecognition,2024,pp.
InternationalConferenceonLearningRepresentations,2023. 19079–19091.
[17] A. Gu and T. Dao, “Mamba: Linear-time sequence modeling with [38] Z. Wu, J. Zheng, X. Ren, F.-A. Vasluianu, C. Ma, D. P. Paudel,
selectivestatespaces,”arXivpreprintarXiv:2312.00752,2023. L.VanGool,andR.Timofte,“Single-modelandany-modalityforvideo
[18] L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang, “Vision object tracking,” in Proceedings of the IEEE conference on computer
mamba:Efficientvisualrepresentationlearningwithbidirectionalstate visionandpatternrecognition,2024,pp.19156–19166.
space model,” in Proceedings of the 41st International Conference on [39] Q.Wu,T.Yang,Z.Liu,B.Wu,Y.Shan,andA.B.Chan,“Dropmae:
MachineLearning,2024,pp.62429–62442. Maskedautoencoderswithspatial-attentiondropoutfortrackingtasks,”
[19] H. Guo, J. Li, T. Dai, Z. Ouyang, X. Ren, and S.-T. Xia, “Mambair: inProceedingsoftheIEEEconferenceoncomputervisionandpattern
A simple baseline for image restoration with state-space model,” in recognition,2023,pp.14561–14571.
Proceedings of the IEEE European Conference on Computer Vision, [40] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”
2024. arXivpreprintarXiv:1711.05101,2017.
[20] Z.Zhang,A.Liu,I.Reid,R.Hartley,B.Zhuang,andH.Tang,“Motion [41] C.Li,N.Zhao,Y.Lu,C.Zhu,andJ.Tang,“Weightedsparserepresenta-
mamba:Efficientandlongsequencemotiongeneration,”inProceedings tionregularizedgraphlearningforrgb-tobjecttracking,”inProceedings
oftheIEEEEuropeanConferenceonComputerVision,2024. ofACMInternationalConferenceonMultimedia,2017,pp.1856–1864.[42] C. Li, X. Liang, Y. Lu, N. Zhao, and J. Tang, “Rgb-t object tracking:
benchmarkandbaseline,”PatternRecognition,vol.96,p.106977,2019.
[43] C.Li,W.Xue,Y.Jia,Z.Qu,B.Luo,J.Tang,andD.Sun,“Lasher:A
large-scalehigh-diversitybenchmarkforrgbttracking,”IEEETransac-
tionsonImageProcessing,vol.31,pp.392–404,2021.
[44] S.Ren,K.He,R.Girshick,andJ.Sun,“Fasterr-cnn:Towardsreal-time
object detection with region proposal networks,” Advances in Neural
InformationProcessingSystems,vol.28,2015.
[45] C.-Y. Wang, A. Bochkovskiy, and H.-Y. M. Liao, “Yolov7: Trainable
bag-of-freebiessetsnewstate-of-the-artforreal-timeobjectdetectors,”
inProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,2023,pp.7464–7475.
[46] T. Cheng, L. Song, Y. Ge, W. Liu, X. Wang, and Y. Shan, “Yolo-
world: Real-time open-vocabulary object detection,” in Proceedings of
theIEEEconferenceoncomputervisionandpatternrecognition,2024,
pp.16901–16911.