A new perspective on Bayesian Operational Modal Analysis
Brandon J. O’Connell a,∗, Max D. Champneys a, Timothy J. Rogers a
aDynamics Research Group, Department of Mechanical Engineering, The University of Sheffield, Mappin Street,
Sheffield, S1 3JD, UK
Abstract
The quantification of uncertainty is of particular interest to the dynamics community, which in-
creasingly desires a measure of uncertainty for greater insight, allowing for more informed and
confident decision-making. In the field of operational modal analysis (OMA), obtained modal in-
formation is frequently used to assess the current state of aerospace, mechanical, offshore and civil
structures. However, the stochasticity of operational systems and the lack of forcing information
can lead to inconsistent results. Quantifying the uncertainty of the recovered modal parameters
through OMA is therefore of significant value. In this article, a new perspective on Bayesian OMA
is proposed — a Bayesian stochastic subspace identification (SSI) algorithm. Distinct from existing
approaches to Bayesian OMA, a hierarchical probabilistic model is embedded at the core of canon-
ical variate-weighted, covariance-driven SSI. Through substitution of canonical correlation analysis
withaBayesianequivalent, posteriordistributionsoverthemodalcharacteristicsareobtained. Two
inference schemes are presented for the proposed Bayesian formulation: Markov Chain Monte Carlo
and variational Bayes. Two case studies are then explored. The first is benchmark study using
data from a simulated, multi degree-of-freedom, linear system. Following application of Bayesian
SSI using both forms of inference, it is shown that the same posterior is targeted and recovered by
both schemes, with good agreement between the mean of the posterior and the conventional SSI
result. The second study applies the variational form of Bayesian SSI to data obtained from an
in-service structure — the Z24 bridge. The Z24 is chosen given its familiarity in the fields of OMA
and structural health monitoring. The results of this study are first presented at a single model
order, and then at multiple model orders using a stabilisation diagram. In both cases, the recovered
posterior uncertainty is included and compared to the conventional SSI result. It is observed that
the posterior distributions with mean values coinciding with the natural frequencies exhibit much
lower variance than posteriors situated away from the natural frequencies.
Keywords: Bayesian, Operational Modal Analysis, System Identification, Stochastic Subspace,
Uncertainty Quantification
∗Corresponding Author
Email address: b.j.oconnell@sheffield.ac.uk (Brandon J. O’Connell )
Preprint submitted to Mechanical Systems and Signal processing (MSSP) August 19, 2024
4202
guA
61
]LM.tats[
1v46680.8042:viXra1. Introduction
Thecharacterisationofdynamicalsystemsintheabsenceofmeasuredinputinformationcontinuesto
beofsignificantimportanceinmodernengineeringpractice. Thisproblemisofparticularinterestto
thestructuraldynamicscommunity,whoroutinelyencountersuchscenarioswhenconductingmodal
analysis. Operational modal analysis (OMA) is the subset of modal analysis methods concerned
with the recovery of modal characteristics in the absence of measured input information; often the
case when testing a structure in-situ (operationally) [1]. By the very nature of OMA, response data
istypicallyobtainedatverylowamplitudeslendingitselftolowsignal-to-noiseratios. Thistendsto
make the task of system identification more challenging. Nevertheless, various approaches to OMA
exist in the literature and work well in practice [1]. These include methodologies such as frequency-
domain decomposition (FDD) [2] and stochastic subspace identification (SSI) [3, 4], now industry
standards for performing frequency-domain and time-domain OMA. OMA as a methodology has
gained increased popularity in recent years. This is predominantly due to its high economic value
and convenience in many engineering applications. This is especially the case for high-value large-
scale assets [1].
With increasing amounts of data being measured and growing demand for improved data-based
models, there is now a desire in the engineering community to obtain some measure of the uncer-
tainty. Access to uncertainty can allow the practitioner to better assess the precision of a chosen
methodology and evaluate the risk of different outcomes. The inclusion and application of uncer-
taintythroughprobabilisticmachinelearningandBayesianmethodologieshasalreadybeenobserved
in the closely related fields of structural health monitoring (SHM) [5–9], digital-twins [10–13] and
risk-based decision-making [14]. From the perspective of OMA, understanding and assessing the
uncertainty of the recovered modal characteristics, i.e. the natural frequencies, damping ratios and
mode shapes, is of particular value. Multiple sources of uncertainty can result in variability in
the recovered modal parameters. Understanding this variability is important in SHM for example,
where modal information is often used to assess the health state of a structure. The impacts of
aleatory and epistemic uncertainties on operationally obtained modal identification has been stud-
ied in [15], whilst a comprehensive list of the associated uncertainties for SSI are described in [16].
Fundamentally, being able to quantify the uncertainty over the modal properties can provide the
practitioner with greater insight and additional information, allowing for more-informed and confi-
dentdecision-making. Furthermore,theauthorsbelieveprobabilisticsystemidentificationtoolsand
the inclusion of uncertainty may provide a framework for addressing many of the current research
challenges in OMA.
Acrossdifferentmethodologies, thedefinitionof‘uncertainty’ canvary, takingmanyforms: bounds,
confidence intervals, fiducial intervals, variance estimates, and distributions. There is no explicit
definitionforthecorrecttypeofuncertaintyinanycase,howevertheoverallobjectiveofuncertainty
quantification (UQ) is the same: To obtain a measure of the uncertainty to better understand the
variability of a given result. The authors’ preference is to operate within a Bayesian framework
where the recovery of posterior distributions is desired, hence the focus of this paper.
1.1. Uncertainty quantification for OMA
In the literature, UQ for OMA is approached and handled in a variety of ways. Several methodolo-
gies for quantifying uncertainty in output-only testing have been presented over the last 30 years,
2taking a variety of approaches including autoregressive techniques [17, 18], Bayesian methods [19–
23], sensitivity analysis and perturbation theory [16, 24–29].
One of the earliest works on this topic, by Pintelon et.al. [30], saw the derivation of expressions
for the uncertainty bounds on the estimated modal parameters using a combination of first-order
sensitivity and perturbation techniques. This method was shown specifically for reference-based
SSI by Reynders et.al in [16] where covariances on the identified system matrices were recovered.
Later this method was efficiently optimised through rederivation by D¨ohler [26] and also extended
for multiple-setup measurements in [27]. Other notable works include that of El-Kafafy et.al. [31],
who presented a fast maximum-likelihood identification method for obtaining modal parameters
with uncertainty intervals, and that of Mellinger et.al. [32] who employ sensitivity analysis of the
auto- and cross-covariance matrices to obtain variances over the modal parameters.
Many recent developments in UQ for OMA have centred around subspace methods, like SSI. This
is predominantly because of their praised performance in OMA scenarios; demonstrating high esti-
mation accuracy coupled with high computational robustness and overall efficiency. Several notable
advancements in SSI-based UQ methods have been presented by a collection of authors including
Reynders [16, 24, 25, 33], D¨ohler [26, 27, 32] and Gr´es [28, 29].
GiventhedevelopmentofnewUQtechniquesforOMA,researchersarealsobeginningtoexplorethe
useofuncertaintytoaddresskeyresearchproblemsinOMA,suchasautomatedmodalidentification
[34, 35] and model order selection [36]. Despite many advancements in UQ for SSI, the authors
are currently unaware of a Bayesian formulation of SSI in the literature, where Bayesian posterior
distributionsoverthemodalpropertiesarerecovered. Thisarticleseekstoaddressthisshortcoming.
1.2. Bayesian uncertainty quantification for OMA
This section introduces the current landscape of Bayesian approaches to OMA, highlighting asso-
ciated merits and limitations of existing methods. The major waypoints here include the popular
frequency-domain method BAYOMA [37] and more recent alternative approaches. These include
Gibbs sampling and variational schemes in the time domain using subspace methods, and a Gibbs
sampling approach based in the frequency domain.
Bayesian approaches to output-only system identification have been developed since the late 1990s
but the topic continues to be of significant interest to the engineering community. Bayesian ap-
proaches to OMA are more recent, with the earliest works appearing in late 2000s. The most cited
BayesianapproachtoOMAfollowsfromtheworkofAu, whosedevelopmentofafastBayesianFast
Fourier transform algorithm (fast-BFFTA) for modal identification [20, 21] removed the computa-
tionallimitationsoftheoriginalBayesianfastFouriertransformalgorithm[38,39]andsubsequently
led to a body of work known as Bayesian OMA or ‘BAYOMA’ [37, 40–43]. BAYOMA can be per-
ceived as the coupling of the fast-BFFTA with a frequency-domain modal analysis technique, such
as FDD [2]. This coupling can be used to recover modal estimates in the form of a most-probable
value and a representation of the uncertainty known as the coefficient of variation. These estimates
originate from Laplacian approximations to the uncertainty which are used to obtain Gaussian dis-
tributions. SincetheinitialdefinitionofBAYOMA,severalextensionshaveemergedintheliterature
providing a more general framework to use this methodology, including work on the identification
of close-modes [44–47].
3BAYOMA appears to divide some of the research community in its Bayesian definition. In the
construction of the algorithm, a flat improper prior is used. Under this assumption, some argue
(see [24]) the problem fundamentally reduces to a maximum likelihood approach, removing the
benefits of a Bayesian formulation. Nevertheless, others adopt BAYOMA as their preferred method
of choice [48–51]. Undoubtedly, BAYOMA provides a computationally effective way to perform
one form of UQ on structures of interest but is limited in its approximation of the posterior and
improper priors.
In search of Bayesian approaches that empower the user to use proper (and informative) prior
information, a few alternative Bayesian OMA approaches have been published in recent years. Li
and Der Kiureghian [52] proposed a variational methodology for OMA and stochastic state space
models, where the joint distributions over the state-transition and observation matrices, and the
process noise and measurement error, are calculated analytically. These analytical solutions are
then coupled with a first-order Taylor series expansion to recover Gaussian approximations to the
distributions over the modal properties. This is necessary given the intractability of the posteriors
over the modal properties because of the eigenvalue decomposition involved in their recovery [52].
Another Bayesian approach was also presented by Li et.al [53]. The contribution focused on a new
form of Bayesian OMA for civil structures under small or moderate seismic excitation. A proba-
bilistic model is first defined, taking advantage of the state space representation of the equations of
motion, with some unmeasured base motion included in the model as a stochastic process. Proper
and broadly uninformative priors are then introduced into the model, with the Bayesian inference
problem solved using a Gibbs sampling procedure to obtain approximate distributions over the
modal properties. The two aforementioned recent methodologies differ from those described later
in this article, in that they are constructed directly using stochastic state space models and not the
SSI-Cov algorithm.
Lastly, a fast-collapsed Gibbs sampling approach to OMA was introduced by Dollon et.al. [54],
as a new frequency domain approach. This alternative proposal performs inference on the modal
properties using established sampling techniques and the FFT of a system with well-separated
modes. The analysis results in sampled posterior distributions that are characterised by a mean
and covariance, rather than a most-probable value and coefficient of variation like BAYOMA.
It is clear that there is considerable interest in developing Bayesian methodologies for performing
UQinOMA.Specifically,methodsthatcanincorporateproperandinformativepriorstructures,and
that recover posteriors over the modal properties as distributional estimates. As such, the authors
of this paper seek to provide a new approach to Bayesian OMA that meets these requirements.
1.3. Contribution
This article presents a new perspective on Bayesian OMA, viewed through the lens of the time-
domainmodalanalysismethodcovariance-drivenSSI(SSI-Cov). Thisnewapproachisimplemented
using latent projections in a Bayesian framework, where canonical correlation analysis (CCA) [55]
— a fundamental statistical tool at the core of the canonical-variate weighted SSI-Cov algorithm —
is replaced with a hierarchical Bayesian formulation. This now Bayesian SSI algorithm is capable
of recovering posterior distributions over the observability and controllability matrices and, by
extension, posteriors over the modal properties.
4Two inference schemes for the proposed Bayesian formulation of SSI are provided. The first ap-
proach is a Gibbs sampling scheme that recovers an empirical representation of the posterior, which
asymptotically trends towards the true posterior with an increasing number of samples. The sec-
ond approach is a variational scheme that provides a computationally efficient alternative to Gibbs
sampling but instead recovers surrogate posteriors that approximate the true posterior.
To assess the identification and UQ performance of the proposed Bayesian formulation, two case
studies are presented. The first is a benchmark study, using data simulated from a linear multi
degree-of-freedom system. This initial study is used to compare the posteriors recovered using the
variational and Gibbs sampling schemes. It is shown that both schemes target and recover the
same posterior distributions over the modal properties, with means closely aligned to the conven-
tional SSI result. The second study analyses data from an in-service structure, specifically the Z24
bridge. The Z24 is chosen as a suitable dataset given its familiarity and frequent use in the fields
of OMA and SHM. This study is designed to demonstrate the identification and UQ performance
on experimentally obtained data. The results of this study are presented at single model orders,
and multiple model orders using a stabilisation diagram. In both cases, it is observed that posterior
distributions with mean values coinciding with the apparent natural frequencies exhibit much lower
variance than posteriors situated further away from the natural frequencies.
The remainder of this article is structured as follows. In Section 2 the underlying theory of CCA
is explored (Section 2.1) and relevant prerequisite topics are outlined, including probabilistic CCA
(Section 2.2) and probabilistic SSI (Section 2.3). Section 3 introduces the proposed Bayesian SSI
algorithm. The theory of Bayesian CCA is first described, before the Bayesian interpretation of SSI
is established. Methods for the recovery of the posterior distributions over the modal properties
are then discussed, considering a Gibbs sampling scheme and a variational Bayes approach. A
concise stepwise summary of the proposed Bayesian SSI methodology is provided in Section 3.2.
Section 4 presents the results of a numerical case study. Simulated data were generated and then
analysed using the Bayesian SSI algorithm. Following a description of the priors, results from both
inference schemes are shown and compared, with an exploration of the influence of data length on
the recovered variance. In Section 5, results from the application of the Bayesian SSI algorithm
to data from the Z24 bridge are presented. This includes results at single model orders, and in the
form of a stabilisation diagram. Finally, conclusions and future work are discussed in Section 6.
2. Theory
2.1. Canonical correlation analysis
It is beneficial to briefly review the theory of canonical correlations. CCA is a well established
statistical tool developed by Hotelling [55] which forms the mathematical basis of canonical variate-
weighted SSI-Cov [4]. The theory of canonical variate-weighted SSI-Cov is shown in Appendix A.
For the purpose of this paper, the acronym SSI-Cov will refer to the canonical variate-weighted
form. The task of CCA is to analyse the mutual dependency between two multivariate sets of data,
which can be evaluated by finding an appropriate set of orthogonal basis vectors, a and b, such
that the correlations of the projected variables aTx and bTy, are maximally correlated. One then
seeks several pairs of vectors that meet the above condition, subject to the constraint that the pairs
of transformed variables are uncorrelated from one another. This can be achieved by the following
maximisation
5aTΣ b
(a′,b′) = argmax corr(aTx,bTy) = argmax xy (1)
(cid:113)
a,b a,b aTΣ abTΣ bT
xx yy
where Σ is defined as the cross covariance between x and y, with Σ = ΣT , and Σ and Σ
xy xy yx xx yy
are the auto covariances. This maximisation can be computed by solving the following generalised
eigenvalue problem
(cid:18) (cid:19)(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19)
0 Σ a Σ 0 a
xy = λ xx (2)
Σ 0 b 0 Σ b
yx yy
whereaandbaretheeigenvectorsandλistheeigenvalueor‘canonicalcorrelation’. Inpractice,the
set of eigenvalues and eigenvectors are computed using the following singular value decomposition
(SVD)
Σ−1
2Σ
Σ−T
2 = V ΛVT. (3)
xx xy yy 1 2
where V and V are left and right singular vectors respectively and Λ is the matrix of singular
1 2
values.
2.2. Probabilistic CCA
In 2005, Bach and Jordan [56] presented an alternative probabilistic formulation of CCA (PCCA),
constructed using latent projections. At the heart of PCCA lies two observed variables x(1) ∈ RD1
n
and x(2) ∈ RD2 which are believed to be conditioned on a lower-dimensional latent space, described
n
by the variable z ∈ Rd. It was proposed that the two sets of observed variables can be modelled
n
by an independent linear mapping W(m) ∈ RDm×d of the shared latent variable to the relevant data
spaces, plus some mean offset µ(m) ∈ RDm and with covariance Σ(m) ∈ RDm×Dm, for m = 1,2.
This model is illustrated as the directed probabilistic graphical model [57] in Figure 1 and described
mathematically by Equations (4) - (6).
x(1) x(2)
n n
z
n
N
Figure 1: Graphical model for the probabilistic, latent variable interpretation of CCA (PCCA)
6z ∼ N(0,I) (4)
n
x(m)|z ∼ N(W(m)z +µ(m),Σ(m)) (5)
n n n
x |z ∼ N(Wz +µ,Σ) (6)
n n n
where N (·,·) corresponds to a Gaussian distribution, W = [W(1);W(2)], µ = [µ(1);µ(2)] and Σ
is a block-diagonal covariance matrix with Σ(1) and Σ(2) along the diagonal. The isotropic noise
model of the latent space enforces the necessary independence of the variables whilst imposing a
maximum correlation condition.
Using this model, the maximum likelihood estimates (MLE) of the model parameters can be ob-
tained. In the case of the linear transformations W, the MLE solution conveniently relates to the
components of the SVD (see Equation (3)), such that
W(1) = Σ1/2 V Λ1/2 (7)
11 1
W(2) = Σ1/2 V Λ1/2 (8)
22 2
2.3. Probabilistic SSI
In previous work by the authors [58], it was shown how SSI-Cov could be reimagined as problem in
probabilistic inference, using the theory of probabilistic projections (i.e. PCCA) to replace classical
CCA in the traditional SSI-Cov algorithm. When the two datasets, X(1) and X(2), are the Hankel
matrices of the future Y and past Y respectively, the MLE of the weights W(m) are equivalent
f p
to the extended observability matrix and controllability matrix transposed, with some arbitrary
rotational ambiguity R, such that
Wˆ (1) = Σ1/2 V Λ1/2R = O (9)
ff 1
Wˆ (2) = Σ1/2V Λ1/2R = CT (10)
pp 2
TheassimilationofPCCAandSSIinthiswayisanimportantrevelation. ThisnowprobabilisticSSI
algorithm allows for familiar hierarchical modelling techniques and permits the use of an arbitrary
priorstructure, similartootherprobabilisticmodels. Thepotentialofthismodelwasdemonstrated
bytheauthorsin[58], whereastatisticallyrobustnoisemodelwasemployedtoaddresstheproblem
of misidentification in OMA, brought on by atypical observations (outliers) in measured responses.
3. Bayesian SSI
Given the potential to admit an arbitrary prior structure, naturally a Bayesian formulation of SSI
is possible. It is perhaps worth considering what a Bayesian SSI algorithm might look like. The
7most desirable output from a Bayesian OMA algorithm would constitute posterior distributions
over the modal properties, given suitably chosen priors that originate from reasonable assumptions
given our knowledge of the classical SSI algorithm. Conveniently, such a model can be achieved
by incorporating a Bayesian approach to CCA. This section presents the details of a Bayesian
formulation of SSI.
Following the introduction of PCCA, Wang [59] and Klami and Kaski [60] presented the natural
hierarchical extension to the PCCA model and introduced Bayesian CCA. This new model incor-
porated priors over the model parameters θ = {W,Σ,µ} and introduced a sparsity inducing prior
over the columns of the transformation matrix. The sparsity inducing prior is not included in the
BayesianCCAmodeldescribedinthispaper, norisitincludedinlaterderivations. Theinclusionof
such a prior for model order selection is of interest however, but will be discussed further in future
work. Figure 2 presents the Bayesian CCA graphical model, whilst Equations (11) - (14) detail the
chosen priors over the model parameters.
µ(1) µ(2)
Ψ(1) Ψ(2)
x(1) x(2)
n n
W(1) W(2)
z
n
N
Figure 2: Graphical model for the Bayesian interpretation of CCA
w(m)
∼ N
(cid:0)
µ ,Σ
(cid:1)
(11)
i wi wi
d
(cid:89)
W(m) ∼ p(w ) (12)
i
i=1
µ(m) ∼ N (cid:0) µ ,Σ (cid:1) (13)
µ µ
Σ(m) ∼ IW(K ,ν ) (14)
0 0
(15)
where IW(K,ν) corresponds to an inverse Wishart distribution with scale K and ν degrees of
freedom. The precision Ψ can also be defined as the Wishart distribution Σ−1 = Ψ ∼ W(cid:0) K−1,ν(cid:1) .
Considering the model above, the full joint likelihood can be expressed as
p(x,z,W,Σ,µ) = p(x|z,W,Σ,µ)p(z)p(Σ)p(µ)p(W) (16)
Accounting for the independent columns of W, such that p(w ) describes each column, Equation
i
816 becomes
d
(cid:89)
p(x,z,W,Σ,µ) = p(x|z,W,Σ,µ)p(z)p(Σ)p(µ) p(w ) (17)
i
i=1
Given the commonality between the desired weights in PCCA and the desired weights in Bayesian
CCA, based on their ability to transform data to the relevant subspace, the posterior distributions
over those weights also corresponds to posterior distributions over the observability and controlla-
bility matrices in the context of SSI,
W(1) = O (18)
W(2) = CT (19)
Unlike the previous definition in Equations (9) - (10), here the observability and controllability
matrices are now distributional quantities, giving rise to posterior distributions over the modal
parameters of interest.
3.1. Inference schemes
The key aspect of a Bayesian methodology is the recovery of posterior distributions over the quanti-
ties of interest, in this case the modal properties of the structural system. However, as is commonly
the case, these distributions are not available in closed form given the form of the model presented
above. This intractability of the model arises from inability to compute the integral which defines
the normalising constant of the posterior — the marginal likelihood. Therefore, to solve Bayesian
inference problems one must turn to approximate or sampling inferential schemes.
Two potential solutions commonly arise. The first is to approximate the posterior distribution
in a Monte Carlo sense with a number of samples which form an empirical representation of the
posterior. These samples can be generated sequentially by means of a Markov Chain, leading to the
family of methods known as Markov Chain Monte Carlo (MCMC). Alternatively, the posterior is
approximated by a parametric distribution which is defined by the user and described in terms of a
setoffreeparameters. Theseparametersarethenoptimisedsuchthattheparametricapproximation
is as close as possible to the true posterior. When a Kullback-Liebler (KL) divergence from the
approximate distribution to the true posterior is used, this forms the family of variational inference
(VI) methods and the intractable inference problem is transformed into a simpler optimisation
problem.
Within each of these families, a range of approaches exist. In this work, one example from each will
be shown and compared. For the MCMC problem, a Gibbs sampling approach is adopted given the
direct access to the full conditionals. Gibbs sampling also has a relatively high efficiency in terms
of its formulation, whereby every sample proposed is a valid sample from the posterior. For the
VI scheme, a coordinate-ascent approach is adopted since the optimisation problem can be solved
efficiently in a manner similar to the expectation maximisation (EM) algorithm [61].
Both inferential schemes have their own set of advantages and disadvantages. Gibbs sampling
has the very useful property of converging to the true posterior in the limit of increasing number
of samples. However, the computational expense of sampling techniques can be significant when
9consideringlargeandcomplexjointdistributions,makingitunsuitableforsometasks. Alternatively,
variational methods only provide an approximation to the posterior but demonstrate considerably
better computational performance. Hence, both inference approaches are shown in this paper with
the results of the inference compared.
3.1.1. Gibbs sampling
Gibbs sampling is a common method in MCMC that can be used to obtain posterior samples of
multiple parameters of interest. The resulting distribution of these samples can then be used to
approximate the posterior. The premise of Gibbs sampling is that, given a multivariate target
distribution, it is much simpler to sample exactly from a conditional distributions rather than
marginalising (by integration) over the full joint distribution. For more on Gibbs sampling see e.g.
[61] or [62].
Adopting a Gibbs sampling approach, the parameter sampling equations are derived and are pro-
vided in Equations (20) - (23), with the overall algorithm summarised in Algorithm 1. For the
interested reader, the full derivation of these updates is provided in Appendix B, with the omission
of the sparsity inducing prior present in [60] and with the addition of generic mean and covariance
priors. The required Gibbs sampling updates for the parameters of the model described in Figure
2, are given below.
Σ update
Σ(τ+1) ∼ IW(K +K,ν +N) (20)
0 0
where
N
(cid:88)
K = (x −µ−Wz )(x −µ−Wz )T
n n n n
n=1
µ update
(cid:16) (cid:17)
µ(τ+1) ∼ N µˆ ,Σˆ (21)
µ µ
where
Σˆ = (NΣ−1+Σ−1)−1
µ µ
and
(cid:32) (cid:33)
N
(cid:88)
µˆ = Σˆ Σ−1 (x −Wz )+Σ−1µ
µ µ n n µ µ
n=1
w update
i
(cid:16) (cid:17)
w(τ+1) ∼ N µˆ ,Σˆ (22)
i wi wi
10where
(cid:32) (cid:33)−1
N
(cid:88)
Σˆ = z2 Σ−1+Σ−1
wi i,n wi
n=1
and
N
(cid:88)
µˆ = Σˆ (Σ−1 z x˜ +Σ−1µ )
wi wi i,n n wi wi
n=1
given
x˜ = x −µ−w z
n n i i,n
such that w corresponds to all columns except the ith column of interest and z corresponds to
i i,n
all rows of z except the ith row.
n
z update
(cid:16) (cid:17)
z(τ+1) ∼ N µˆ ,Σˆ (23)
n z z
where
Σˆ = (WTΣ−1W+I)−1
z
and
µˆ = Σˆ WTΣ−1x
z z n
Algorithm 1 Bayesian CCA - Gibbs Sampling (MCMC)
initialise: Σ,µ,W,z using the priors given appropriately chosen hyperparameters
for τ = 1...,T do
Sample Σ(τ+1) using Equation (20)
Sample µ(τ+1) using Equation (21)
for i = 1...,d do
(τ+1)
Sample w using Equation (22)
i
end for
(τ+1)
Sample z using Equation (23)
n
end for
return Samples of Σ,µ,W,z
Following application of Algorithm 1 for a desired number of samples, the resulting samples of
W(1) (i.e. the observability matrix) can be propagated to obtain samples of the state transition
matrix and subsequently propagated further onto the modal properties in the usual way for a linear
dynamic system [4], obtaining sampled posterior distributions.
Despite the useful property of MCMC techniques – guaranteed convergence to the true posterior in
the limit of the number of samples – such techniques can be inherently slow and computationally
inefficient, especially if the posteriors are highly correlated in Gibbs samplers.
113.1.2. Variational Bayes
VI is a well-known form of Bayesian inference and common alternative to MCMC, used to approx-
imate intractable posterior distributions with analytical approximations (see e.g. [61], [62]). The
generalpremiseofVIistoselectasuitableapproximation(surrogate)fromatractablefamilyofdis-
tributionsandtrytomaketheapproximationasclosetothetrue(intractable)posterioraspossible.
This is achieved by minimising the KL divergence KL(q||p) from the surrogate distribution q to the
true posterior distribution p. The minimisation of the KL divergence is equivalent to maximising
the evidence lower bound (ELBO), which is equal to the negative KL divergence up to an additive
constant. The ELBO can also be defined as the sum of the expected log of the joint distribution,
and the entropy of the variational distribution.
Assuming the surrogate posterior is determined by some free parameters, then the problem of VI
reduces to a more familiar optimisation problem [61]. Which, for certain cases, can be solved using
coordinate-ascent and conducted efficiently in a manner similar to the expectation maximisation
(EM) algorithm [61]. When latent variables z and the parameters of the model θ are desired, as
n
is the case in Bayesian CCA, then the method is known as variational Bayes (VB).
AnimportantstepinVBisdefiningtheformofthesurrogateposterior. Herethemean-fieldapprox-
imation[63]ischosen,wheretheposteriorcanbefullyfactorisedintheformq(z) =
q(θ)(cid:81)J
q (z ).
j=1 j j
Adopting a mean field approximation, the surrogate posterior for Bayesian CCA takes the following
factorised form
d
(cid:89)
q(z,µ,Ψ,W) = q(z) q(w )q(µ)q(Ψ) (24)
i
i=1
where
(cid:16) (cid:17)
q(z ) ∼ N z |µ˘ ,Σ˘ (25)
n n z z
q(Ψ) ∼
W(cid:16) Ψ|K˘−1 ,ν˘(cid:17)
(26)
(cid:16) (cid:17)
q(w ) ∼ N w |µ˘ ,Σ˘ (27)
i i wi wi
(cid:16) (cid:17)
q(µ) ∼ N µ|µ˘ ,Σ˘ (28)
µ µ
Following the VB methodology, the update equations for the model parameters were derived and
are given in Equations (29)-(32). The complete algorithm is summarised in Algorithm 2, whilst the
full derivation of these updates is also provided in Appendix C, with the omission of the sparsity
inducing prior present in [59] and with generic mean and covariance priors.
The variational update equations for the parameters of the model described in Figure 2, are given
below. In all equations, ⟨·⟩ represents the expected value E[·] of that variable or combination of
variables with respect to all the other parameters.
12z update
n
(cid:16) (cid:17)
q⋆(z ) ∼ N µ˘ ,Σ˘ (29)
n z z
where
(cid:16) (cid:17)−1
Σ˘ = ⟨WTΨW⟩+I
z
and
µ˘ = Σ˘ ⟨W⟩T⟨Ψ⟩(x −⟨µ⟩)
z z n
w update
i
(cid:16) (cid:17)
q⋆(w ) ∼ N µ˘ ,Σ˘ (30)
i wi wi
where
(cid:32) (cid:33)−1
N
(cid:88)
Σ˘ = ⟨z z ⟩⟨Ψ⟩+Σ−1
wi i,n i,n wi
n=1
and
(cid:32) (cid:33)
N
(cid:88)
µ˘ = Σ˘ ⟨Ψ⟩ x˜ ⟨zT ⟩+Σ−1µ
wi wi n n,i wi wi
n=1
given
x˜ = x −⟨µ⟩−⟨w ⟩⟨z ⟩
n n i i,n
such that w corresponds to all columns except the ith column of interest and z corresponds to
i i,n
all rows of z except the ith row.
n
Ψ update
q⋆(Ψ) ∼
W(cid:16) K˘−1 ,ν˘(cid:17)
(31)
where
N
(cid:88)(cid:68) (cid:69)
K˘ = K + (x −µ−Wz )(x −µ−Wz )T
0 n n n n
n=1
and
ν˘ = ν +N
0
µ update
(cid:16) (cid:17)
q⋆(µ) ∼ N µ˘ ,Σ˘ (32)
µ µ
where
Σ˘ = (N⟨Ψ⟩+Σ−1)−1
µ µ
13and
(cid:32) (cid:33)
N
(cid:88)
µ˘ = Σ˘ ⟨Ψ⟩ (x −⟨W⟩⟨z ⟩)+Σ−1µ
µ µ n n µ µ
n=1
Algorithm 2 Bayesian CCA - Coordinate Ascent Variational Inference (CAVI)
initialise: Variational factors q(z,µ,Ψ,W)
while the ELBO has not converged do
Update local variational parameters:
Update q⋆(z ) using Equation (29)
n
Update global variational parameters:
for i = 1,...,d do
Update q⋆(w ) using Equation (30)
i
end for
Update q⋆(Ψ) using Equation (31)
Update q⋆(µ) using Equation (32)
Compute ELBO(q) = E[lnp(x,z,θ)]+E[lnq(z,θ)]
end while
return Surrogate posterior distributions of Σ,µ,W,z
3.2. Overall methodology
Having explored two different forms of inference for the recovery of the posteriors in Bayesian CCA,
itisperhapsusefultosummarisethegeneralprocedureinthecontextofBayesianSSI.Thefollowing
summary gives an overview of the steps needed to conduct Bayesian SSI to a desired dataset.
Given response data, y ∈ Rds×N, with d sensors and N datapoints:
s
Step 1. Construct Block Hankel matrices of the future, Y ∈ RDm×N, and the past, Y ∈ RDm×N,
f p
where D = d ×d given that d is the chosen number of lags in the Hankel matrix.
m s Hankel Hankel
Step 2. Apply Algorithm 1orAlgorithm 2withtheinputsx = Y andx = Y , givenappro-
1 f 2 p
priately chosen priors and convergence criteria1 to obtain the posteriors over the model parameters.
Step 3. If Algorithm 1 has been applied, remove a proportion of the initial samples (as burn in)
and continue to Step 4. If Algorithm 2 has been applied, draw Monte Carlo samples from the
posterior distributions over the columns of the weight matrix and reconstruct the full weight matrix
to provide a sample of the observability, i.e W .
1
1The convergence criteria, or desired number of samples, in both algorithms must be selected. The choice and
specification of these in optimisation tasks constitutes its own research field and are therefore not explored in depth.
14Step 4. Using the samples of the observability matrix, calculate the state transition matrix2 A
and the modal properties in the usual way for state space models (e.g. Section 3.3 in [16]), for each
sample. Considering all the samples, this provides an approximate posterior distribution over each
modal property.
4. Simulated case study
4.1. Benchmarking the proposed algorithm
The performance of the proposed Bayesian SSI algorithm, outlined in Section 3.2, will be bench-
marked here by application to simulated data obtained using a numerical model of the four degree-
of-freedom shear frame illustrated in Figure 3. This structure is identical to that described by
Reynders in [25], although a new simulation is used. Using a mass-spring-damper model to repre-
sent the dynamics of the system, the mass m assigned to each floor is 2 kg and the stiffness k
j j
applied to the individual columns (springs) on each floor is 2500 N/m, where j = 1,2,3,4. The
dampingineachcolumnisproportionaltotheassociatedstiffness,suchthatc = k /1000. Horizon-
j j
tal forces are assumed to apply to each floor as the inputs u (t), whilst the horizontal acceleration
j
of each floor are the corresponding outputs y (t).
j
The equations of motion of the shear frame in continuous time can be defined as the ordinary
differential equation
[M]x¨ +[C]x˙ +[K]x = f (33)
InOMA,theinputisunknownandoftenassumedtobeGaussianwhitenoise. Therefore,theforcing
f can be defined as a white noise process in continuous time acting on each floor. This results in a
linearstochasticdifferentialequationwhichisthendiscretisedexactlyusingVanLoandiscretisation
[64]. The forcing white noise process is chosen with a spectral density of 5×10−5 (m s−2)2Hz−1
whilst some Gaussian measurement noise is added to each channel output y (t), sampling from a
j
zero-mean normal distribution with a standard deviation of 0.05 m s−2. Data are then simulated
using a sampling frequency of 50 Hz with length N = 216.
2There are multiple approaches used to do this step. The authors adopt the balanced truncation algorithm, see
Chapter 8 in Katayama [4]
15u4(t) y4(t)
m4
k4 k4
u3(t)
c4 c4
y3(t)
m3
k3 k3
u2(t)
c3 c3
y2(t)
m2
k2 k2
u1(t)
c2 c2
y1(t)
m1
k1 k1
c1 c1
Figure 3: Four-story shear building model used to simulate responses given a white noise input on each floor
Based on the model described in Figure 3, the matrix coefficients of Equation 33 are
   
m 0 0 0 k +k −k 0 0
1 1 2 2
 0 m 2 0 0   −k 2 k 2+k 3 −k 3 0 
M =   , K = 2  , C = K/1000
 0 0 m 3 0   0 −k 3 k 3+k 4 −k 4
0 0 0 m 0 0 −k k
4 4 4
4.1.1. Priors
Before carrying out any inference, attention must first be paid towards establishing sensible priors
for the model. Given the general model presented in Section 4.1, the following prior structure was
chosen
w(m) ∼ N (0,σ I) (34)
i wi
µ(m) ∼ N (0,σ I) (35)
µ
Σ(m) ∼ IW(K ,ν ) (36)
0 0
wheretheindependentcolumnsw sharethesamepriordefinition. Theremaininghyperparameters,
i
σ = σ = 1, K = 1×102, ν = D +2, were chosen to be weakly informative and proper, to
µ wi 0 0
provide sufficient flexibility to the model. Samples from the chosen prior over the weight matrices
werethenusedtogeneratesamplesoftheobservabilitymatrixandsubsequentlypropagatedthrough
an eigenvalue decomposition to obtain estimates for the posterior distributions over the modal
properties. The prior distributions over the modal properties are presented in Figure 4.
16(a)
(b)
(c)
Figure4: Priordistributionsoverthenaturalfrequencies(a),dampingratios(b)andmodeshapes(c),obtainedusing
propagated Monte Carlo samples of the priors as defined by Equations 11 - 14.
It is useful to briefly discuss the general shape of the priors on the modal parameters. Despite
Gaussian assumptions in the model, specifically on the weight prior, the propagated uncertainty
17results in distributions that are a mix of what appear to be non-Gaussian and even multimodal in
nature. The authors believe this is likely caused by the size of the prior variance coupled with the
non-linear transformation in the eigenvalue decomposition, which is most evident when considering
larger variances.
4.1.2. Posteriors
Assuming the correct number of modes (4 modes – corresponding to a state space dimension of 8),
the Gibbs sampling implementation of Bayesian SSI (Algorithm 1) was used to recover posterior
distributions over the modal properties of the simulated structure. 5000 samples were drawn, with
the first 20% removed as ‘burn in’ to remove transients in the Markov chain [61]. The posteriors
over the modal properties are shown in Figure 5. Similar to the Gibbs sampling case, assuming
the correct number of modes, data were analysed using the VI implementation of Bayesian SSI
(Algorithm 2). After the recovery of the closed form posterior over the observability, 4000 samples
were drawn and propagated. The resulting posteriors over the modal properties are also presented
in Figure 5.
On interpreting Figure 5, one can conclude that the surrogate posteriors over the modal param-
eters obtained using VB closely align with the true posteriors obtained through Gibbs sampling.
This closeness is evident from the aligned expected values and the shape of the posteriors, which
correspond well. Furthermore, the posterior estimates of both schemes converge toward the SSI-
Cov result. This convergence is largely expected as, given the use of weakly informative priors
and enough data, the maximum-a-posteriori (MAP) will be close to the MLE, i.e. SSI-Cov. Some
minor differences can be seen. The first is in the mean estimates of first and second damping ratios
compared to the SSI-Cov result. This misalignment is a consequence of the priors, which cause
slight bias in the posterior, more so than the damping ratio estimates for the third and fourth
modes. The second difference can be seen in the variance of the third mode shape in Figure 5c. A
non-GaussianposteriorrecoveredfromtheGibbsschemeexplainsthisdifference. Thisphenomenon
originates from application of the eigenvalue decomposition to obtain the mode shapes. However,
this difference is more likely a result of an underprediction of the variance using VI. Under or over-
prediction of the variance is a common characteristic of VI schemes which can occur when some
conditional distributions are factorised out of the model during its construction [61]. The omission
of these conditionals is a result of the independence assumptions in the surrogate posterior defini-
tion. Possible misalignment of the variance is one of several trade-offs that needs to be considered
when prioritising the computational efficiency of variational methods over convergence to the true
posterior guaranteed by MCMC sampling.
18(a)
mode1 mode2 mode3 mode4
800
1000
1000
800
600
750 750
600
500 500 400 400
250 250 200 200
0 0 0 0
0.009 0.010 0.011 0.025 0.026 0.027 0.037 0.038 0.039 0.046 0.048 0.050
Dampingratio[-] Dampingratio[-] Dampingratio[-] Dampingratio[-]
CAVI MCMC Truth SSI-Cov
(b)
(c)
Figure 5: Identified posterior distributions over the recovered natural frequencies (a), damping ratios (b) and mode
shapes (c), obtained using the Gibbs sampling and VB implementations of Bayesian SSI to response data simulated
from a 4DOF linear dynamic system (see Figure 3).
194.2. Influence of data length on variance
The influence of data length on the variance of the modal posteriors is also investigated. Using
the same system parameters and priors described in the Section 4.1.1, the data length was varied
from 217−212 in decreasing powers of 2, with the range selected solely for demonstrative purposes.
The results for the natural frequencies and the damping ratios are displayed in Figures 6a and 6b,
respectively. As is perhaps expected, the variance of the posterior estimates of both frequency and
damping ratio reduce given increasing amounts of data, meanwhile the means of the distributions
also converge towards the true values from the model. The influence of data length and perhaps
other factors (lags in the Hankel matrix) on the recovered uncertainty has interesting implications
and could be used to reduce the amount of data collection and storage required in OMA, for a
desired level of confidence. This poses an interesting question on choosing the right amount of data
for a given decision-making task, which will be considered in future work.
mode1 mode2 mode3 mode4
1000 300
150
125
800
600 200 100 100
75
400
100 50 50
200 25
0 0 0 0
2.75 2.76 2.77 2.78 7.94 7.96 7.98 12.17512.20012.22512.25012.275 14.9 15.0 15.1
Frequency[Hz] Frequency[Hz] Frequency[Hz] Frequency[Hz]
SSI-Cov(N=217) SSI-Cov(N=216) SSI-Cov(N=215) SSI-Cov(N=214) SSI-Cov(N=213) SSI-Cov(N=212)
CAVI(N=217) CAVI(N=216) CAVI(N=215) CAVI(N=214) CAVI(N=213) CAVI(N=212)
(a)
mode1 mode2 mode3 mode4
2000
2000 2000
1500
1500
1500 1500
1000
1000
1000 1000
500 500 500 500
0 0 0 0
0.010 0.015 0.020 0.025 0.025 0.030 0.040 0.045 0.05 0.06
DampingRatio[-] DampingRatio[-] DampingRatio[-] DampingRatio[-]
SSI-Cov(N=217) SSI-Cov(N=216) SSI-Cov(N=215) SSI-Cov(N=214) SSI-Cov(N=213) SSI-Cov(N=212)
CAVI(N=217) CAVI(N=216) CAVI(N=215) CAVI(N=214) CAVI(N=213) CAVI(N=212)
(b)
Figure 6: Identified posterior distributions over the natural frequencies (a) and damping ratio (b) estimates for each
mode, obtained using Bayesian SSI with varying data length N. The true values are represented by - -
205. Case Study: Z24 bridge
To demonstrate applicability to real world systems, the proposed Bayesian SSI algorithm is used to
analyse vibration data obtained from the Z24, a now decommissioned bridge once located between
Bern and Zurich, Switzerland. The Z24-Bridge has become a standard benchmark when demon-
stratingvibration-baseddamagediagnosismethods,newsystemidentificationandOMAtechniques,
and UQ tasks in structural dynamics, making it a sensible choice for this study. For more informa-
tion on the testing of the Z24, a full description, initial analysis and subsequent testing, the reader
is directed towards (bwk.kuleuven.be/bwm/z24) [65, 66].
This purpose of this study was to evaluate the behaviour of the new Bayesian SSI algorithm when
confronted with measured data. Analysis is only conducted using the variational approach, since
thehighcomputationtimeofGibbssamplerwasdeemedimpractical. Resultsattwodifferentsingle
model orders are shown, followed by a stabilisation diagram to assess the convergence of the modal
properties at different model orders.
Data, corresponding to acceleration measurements obtained on the 24th December 1997, were used
for processing. This dataset comes from the long-term continuous monitoring test set, and in this
case comprises data from 7 accelerometers corresponding to one side of the Z24 bridge. The first
segment (8192 data points), with f = 100Hz, was used in the analysis. The priors of the Bayesian
s
SSI model were left unchanged from those used in the previous numerical simulation (see Section
4.1.1) except for the precision hyperparameter K , which was changed to the identity providing
0
some additional flexibility to the model.
5.1. Single model order
Conducting analysis at model orders of 10 and 30, chosen solely for demonstrative purposes, the
posterior distributions of the modal characteristics were obtained using the variational implemen-
tation of Bayesian SSI. The posteriors over the natural frequencies, represented by histograms of
the samples and overlaid on the sum of the Welch spectra in each channel, are shown in Figures 7
and 8.
It is evident from both Figure 7a and Figure 8a that histograms with means centred around the
apparentnaturalfrequencies(peaksintheWelchspectrum)demonstratesignificantlylowervariance
than histograms corresponding to frequencies likely more spurious in nature. This lower variance
can be seen more clearly in the enhanced Figures 7b, 7c, 8b and 8c and is most noticeable around
4 Hz, 5 Hz, 10-11 Hz. In contrast, larger variances are observed in regions where there is typically
less evidence of a natural frequency. Focusing specifically on the case where the model order is 30,
largervariances canbe seenat4.5Hz, 9.75Hzand11.75Hz. Thisobviousvariabilityinthevariance
for each identified frequency suggests that there is a higher probability or belief in some frequencies
to describe the data over others.
2110−6
SumofWelchSpectra
PosteriorSamples 140
SSI-Cov
10−7
120
10−8 100
80
10−9
60
10−10
40
10−11
20
10−12 0
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
Frequency[Hz]
(a)
10−6 200 10−6 200
10−7 10−7
150 150
10−8 10−8 10−6
140
10−9 100 10−9 100
10−7
120
10−8 100
10−10
50
10−10
50 10−9 80
10−11 10−11 60
10−10
40
10−12
3.5 4.0 4.5 5.0 5.5
6.00 10−12
10 11 12
0
10−11
20
Frequency[Hz] Frequency[Hz] 10−12 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0 0
Frequency[Hz]
SumofWelchSpectra PosteriorSamples SSI-Cov
(b) (c)
Figure 7: The sum of the Welch spectra of the Z24 data overlaid with the posteriors of the natural frequencies,
represented by histograms of the natural frequency samples recovered using Bayesian SSI at a model order of 10 is
given in (a). Regions of interest are shown in (b) and (c), with axes limits chosen to highlight the differences in the
mean and variance of the histograms in different regions.
5.2. Stabilisation diagram
Asthetruenumberofmodesisunknown,itisstandardpracticeinOMAtoconstructastabilisation
diagram. Stabilisationdiagramsareatoolusedbythepractitionertodeterminethe“physical”poles
of a system, helping to decide the nature of the identified complex eigenvalues i.e. spurious or real
in a physical context. The decision of which is often governed by some heuristic criteria on the
stability or consistency of the modal parameters across a range of model orders.
After conducting Bayesian SSI at multiple model orders, the resulting stabilisation diagram for the
Z24 is shown in Figure 9. The original 2-dimensional histogram representation of the posterior
22
]-[edutilpmA ]-[ytilibaborP
]-[edutilpmA
]-[edutilpmA ]-[ytilibaborP
]-[edutilpmA
]-[ytilibaborP
]-[ytilibaborP10−6 50
SumofWelchSpectra
PosteriorSamples
SSI-Cov
10−7
40
10−8
30
10−9
20
10−10
10
10−11
10−12 0
0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 20.0
Frequency[Hz]
(a)
10−6 50 10−6 50
10−7
40
10−7
40
10−8 10−8 30
30 30 10−7
10−9 10−9
10−8
25
20 20
10−10 10−10 10−9 20
10−11 10 10−11 10 10−10 15
10−11
10−12
3.5 4.0 4.5 5.0 5.5
6.00 10−12
10 11 12
0
10−12
10
Frequency[Hz] Frequency[Hz] 10−13 5
SumofWelchSpectra
10−14 0.0 2.5 5.0 7.5 10.0 12.5 15.0 17.5 NaturalFre 2q 0u .e 0ncySamples 0
Frequency[Hz]
(b) (c)
Figure 8: The average Welch spectrum of the Z24 data overlaid with the posteriors of the natural frequencies,
representedbyhistogramsofthenaturalfrequencysamplesrecoveredbyBayesianSSIatamodelorderof30,isgiven
in (a). Regions of interest are shown in (b) and (c), with axes limits chosen to highlight the differences in the mean
and variance of the histograms in different regions.
at single model orders (see Figures 7 and 8) is ill-suited for the multi-order stabilisation diagram.
Instead, samplesareplottedwithtransparencysuchthatdarkerregionsindicateareaswithahigher
density of samples. The inclusion of uncertainty in stabilisation diagrams is not an entirely new
concept, having already been represented, and proven beneficial in assisting model order selection
[35], although no agreed form has yet been defined. In this plot, non-conjugate poles have been
removed, however no stability metrics have been defined, purely to observe the raw form of the
stabilisation diagram.
Considering the distributions of the posteriors in Figure 9, one can clearly see that peaks in the
Welch spectrum, typically indicative of a natural frequency, coincide with the posterior means and
23
]-[edutilpmA ]-[ytilibaborP
]-[edutilpmA
]-[edutilpmA ]-[ytilibaborP
]-[edutilpmA
]-[ytilibaborP
]-[redrOledoM30
10−7
25
10−8
20
10−9
10−10
15
10−11
10
10−12
5
10−13
SumofWelchSpectra
NaturalFrequencySamples
10−14 0
0 5 10 15 20 25
Frequency[Hz]
Figure 9: Stabilisation diagram of the Z24 bridge, constructed using the natural frequency samples obtained using
Bayesian SSI at a range of model orders.
are consistent across multiple model orders. This is true for multiple peaks, most distinctly around
4 Hz, 5 Hz and 10-11 Hz. The variance of the distributions in these areas is also low across a broad
range of model orders, especially when compared to more spurious frequency estimates away from
natural frequencies. Thus indicating a higher level of confidence in the ability of certain frequencies
to describe the dynamics. Another conspicuous observation is the general increase in variance at
higher model orders, particularly in the more spurious estimates, as moving to higher model orders
grant greater flexibility with numerous ways for the model to explain the data.
6. Conclusion
In this work, a new Bayesian perspective on OMA has been presented. Using the probabilistic
interpretation of SSI-Cov, it has been shown that prior knowledge can be incorporated over the
model parameters to form a Bayesian SSI algorithm capable of recovering posterior distributions
over modal characteristics of interest. This new approach was presented with two possible meth-
ods of inference: Markov chain Monte Carlo and variational Bayes. These two forms of Bayesian
SSI were then benchmarked using a simulated case study. The prior distributions over the modal
parameters were shown, followed by the recovered posteriors; which demonstrated good agreement
with one another and convergence of the posterior mean towards the conventional SSI result. The
effects of data length on the recovered posteriors was also briefly explored. It was shown that, with
increasing data length, the posterior mean trends toward the truth whilst the variance decreases as
expected. Finally, the practicality and applicability of the algorithm was then illustrated using data
obtained from an in-service structure, the Z24 bridge. Recovery of the modal parameters and asso-
ciated posterior uncertainty was shown at single model orders and in a stabilisation diagram using
the natural frequencies estimates. The results showed that posteriors with means centred around
the apparent natural frequencies display much lower variance when compared to more spurious
24
]-[edutilpmA
]-[redrOledoMfrequency estimates, further away from the natural frequencies.
6.1. Future Work
As is typical with Bayesian UQ, the choice and appropriate specification of the priors is an impor-
tant task. Priors should reflect ones initial belief, yet they can also be chosen tactically to ensure
fasterconvergencetotheposteriorunderlimiteddataorforadesiredconjugacywiththelikelihood.
In the case of Bayesian SSI, prior distributions are not placed over the modal properties but instead
over the columns of the weight matrices (the observability and controllability). This choice of prior
setting remains unchanged from the initial definition of Bayesian CCA [59, 60] but may not be the
most suitable for this chosen application. At present, the chosen prior can result in posterior den-
sities that include negative estimates for the damping ratio. Negative values of damping would be
consistent with non-physical attributes to a supposedly stable dynamic system. Nevertheless, neg-
ative estimates for the damping ratio can numerically occur in conventional SSI, meaning posterior
densities containing negative damping are plausible given the current model choice. It is interesting
to imagine if priors over the model parameters could be found, such that the modal properties are
constrained to be physically meaningful. This possible prior specification is to be explored in future
work.
Inotheraspects,itishopedthatthemethodologiesintroducedinthisworkwillenableresearchersto
tackle existing OMA research challenges and explore the many capabilities of this now Bayesian SSI
algorithm; including alternative noise models, the effect of alternative prior specification, decision-
making tasks, computationally efficient implementations, and inclusion in existing frameworks.
In summary, this work has shown that posterior uncertainty over the modal parameters has signifi-
cant benefits. The availability of this uncertainty information means it can be propagated to assist
in the solution of other important research challenges in OMA, such as model order selection, au-
tomatic OMA and sensor placement. In SHM frameworks, one could foresee the inclusion of modal
posterior uncertainties as a valuable addition to damage assessment and decision-making; providing
new information one could envisage altering decision boundaries and classification models. Going
forward,theauthorswouldpromotetheBayesianSSItechniquepresentedinthisworkasapowerful
building block for use in modal analysis and beyond.
Acknowledgements
The authors gratefully acknowledge the support of the Engineering and Physical Sciences Research
Council (EPSRC), UK through grant numbers EP/W002140/1 and EP/W005816/1. For the pur-
pose of open access, the authors have applied a Creative Commons Attribution (CC-BY) license to
any Author Accepted Manuscript version arising.
CRediT authorship contribution statement
Brandon J. O’Connell: Conceptualization, Writing - original draft, Visualization, Validation,
Methodology, Software. Max D. Champneys: Writing - review & editing, Methodology. Timo-
25thyJ.Rogers: Writing-review&editing,Supervision,Conceptualization,Methodology,Software,
Funding acquisition.
Appendix A. Canonical-variate weighted, covariance-driven stochastic subspace iden-
tification
For the reader’s benefit, a concise description of canonical-variate weighted SSI-Cov algorithm for
an output-only case is given here. This derivation follows descriptions for the balanced (canonical-
variate weighted) algorithm given in Katayama [4] and Van Overschee and De Moor [3]. The reader
is directed towards the aforementioned texts for a complete exposition.
Consider an rth order discrete state space model of a linear dynamic system, equivalent to a me-
chanical system with n degrees of freedom, such that r = 2n , in the form
dof dof
(cid:20)(cid:20) (cid:21) (cid:21) (cid:20) (cid:21)
x k+1 = A dx k +w k , E w q (cid:2) wT vT(cid:3) = Q S δ (A.1)
y k = Cx k +v k v q s s ST R qs
where y ∈ Rl is the output vector at discrete time step k, x ∈ Rp is the internal state vector,
k k
A ∈ Rp×p is the discrete state matrix such that A = expm(A ∆t) where A is the continuous
d d c c
state matrix, ∆t is the sampling time and C is the output matrix. The terms w ∈ Rp and v ∈ Rl
k k
are samples of the process noise and measurement noise respectively, E[·] denotes the expectation
and δ is the Kronecker delta for any two samples in time q and s. The process and measurement
qs
noise are both assumed to be stationary, white noise Gaussian with zero mean and some covariance
given by the second part of Eq.(A.1).
Given output measurements from this stationary process, based on l measurement channels, the
data can be arranged into a block Hankel matrix H ∈ R2lj×N to form
(cid:20) (cid:21) (cid:20) (cid:21)
Y Y
H = Y = 0|j−1 = p (A.2)
0|2j−1 Y Y
j|2j−1 f
with 2j block rows and N columns, with a block consisting of l rows and where j > 0 and N is
sufficiently large (i.e much larger than 2lj) and where j > r and the number of columns of block
matrices is N. The resultant cross-covariance matrix of the future Y with the past Y is therefore
f p
given by
(cid:20) (cid:21) (cid:20) (cid:21)
Σ˜ = 1 Y p (cid:2) YT YT(cid:3) = Σ pp Σ pf (A.3)
N Y p f Σ Σ
f fp ff
where Σ and Σ are block cross-covariance matrices, and Σ , Σ are block auto-covariance
pf fp ff pp
matrices respectively. The canonical correlations Λ = diag(λ ,··· ,λ ) between the future and past
1 r
are the singular values [4], obtained through the SVD of the following matrix
26Σ−1/2 Σ Σ−T/2 = V ΛVT ⋍ V˘ Λ˘V˘T (A.4)
ff fp pp 1 2 1 2
1/2 T/2
where Σ Σ = Σ , such that,
ff ff ff
Σ ⋍ Σ1/2 V˘ Λ˘V˘T ΣT/2 (A.5)
fp ff 1 2 pp
whereV andV aretheleftandrightsingularvectors,respectivelyandΛ˘ neglectssufficientlysmall
1 2
singularvalues(orcanonicalcorrelations)inΛsuchthattheresultantstatevectorhasthedimension
d = dim(Λ˘). Thecross-covariancematrix,Σ ,canbedecomposedintothecorrespondingextended
pf
observability (O) and controllability (C) matrices using Σ = OC such that
fp
O = Σ1/2 V˘ Λ˘1/2 , C = Λ˘1/2 V˘T ΣT/2 (A.6)
ff 1 2 pp
with rank(O) = rank(C) = d. Note in the context of output only SSI, the controllability has little
meaning. The extended observability and controllability matrices can then be used to recover the
state A and output C matrices, and consequently the modal properties, in the usual manner for
operational modal analysis (see Section 3.3 in [16] and also [67]). The method described here for
recovering the system matrices is one of many. See stochastic balanced realisation algorithm B,
Chapter 8 in Katayama [4].
27Appendix B. Bayesian CCA - Gibbs Sampler Derivation
This appendix provides a full derivation for the update equations provided in Algorithm 1, aligning
with the Bayesian CCA formulation presented in Klami and Kaski [60] but without the sparsity
prior and with the inclusion of generic priors.
Given the model defined in Figure 2, the joint likelihood of Bayesian CCA can be defined as
L = p(x,z,W,Σ,µ) = p(x|z,W,Σ,µ)p(z)p(Σ)p(µ)p(W) (B.1)
Assuming independent columns of W, the joint can also be defined as
d
(cid:89)
L = p(x|z,W,Σ,µ)p(z)p(Σ)p(µ) p(w ) (B.2)
i
i=1
Taking the natural log of the joint likelihood, the individual components of the log joint are given
by
N
N 1 (cid:88)
lnL ∝ − ln|Σ|− (x −(Wz +µ))TΣ−1(x −(Wz +µ))+const (B.3)
p(x|z,W,Σ,µ) n n n n
2 2
n=1
N N
N 1 (cid:88) 1 (cid:88)
lnL ∝ − ln|I|− (z −0)TI−1(z −0) = − zTz +const (B.4)
p(z) 2 2 n n 2 n n
n=1 n=1
lnL ∝
ν 0
ln|K |−
ν 0D ln(2)−ln(cid:16) Γ(cid:16)ν 0(cid:17)(cid:17)
−
ν 0+D+1
ln|Σ|−
1 Tr(cid:0)
K
Σ−1(cid:1)
+const (B.5)
Σ 0 0
2 2 2 2 2
N
1 1 (cid:88)
lnL ∝ − ln|Σ |− (µ−µ )TΣ−1(µ−µ )+const (B.6)
p(µ) 2 µ 2 µ µ µ
n=1
N
1 1 (cid:88)
lnL ∝ − ln|Σ |− (w −µ )TΣ−1(w −µ )+const (B.7)
p(wi)
2
wi
2
i wi wi i wi
n=1
where
(cid:40) (cid:41)
d d
(cid:89) (cid:88)
p(w ) = exp lnp(w ) (B.8)
i i
i=1 i=1
UsingthestandardtheoryofGibbsMCMCsamplingtheupdateequationsforthemodelparameters
can be constructed. The premise of Gibbs sampling is to sample each variable in turn, conditioned
28on the values of the other variables in the joint distribution to obtain a sample from the conditional
distribution that is sought. This can be achieved by considering the log of the joint likelihood with
respect to the parameter of interest, establishing the new conditional distribution, and evaluating
that function given the current estimates for all the necessary model parameters.
The following subsections summarise the derivation of the individual updates. Even though a
specified order is given in Algorithm 1, the nature of Gibbs sampling means that sampling from
the conditionals can be conducted in any order. As such, the equations here are written without
specifying which parameters are current or previous estimates, to simplify notation for clarity.
Appendix B.1. Sample Σ
Collecting the terms of the log joint pertaining to Σ
N
N 1 (cid:88)
lnp(Σ(τ+1)) ∝ − ln|Σ|− (x −(Wz +µ))TΣ−1(x −(Wz +µ))
n n n n
2 2
n=1
−
ν 0+D+1
ln|Σ|−
1 Tr(cid:0)
K
Σ−1(cid:1)
(B.9)
0
2 2
Using xTAx = trace(AxxT),
(cid:32) (cid:33)
N
N 1 (cid:88)
lnp(Σ(τ+1)) ∝ − ln|Σ|− Tr Σ−1 (x −µ−Wz )(x −µ−Wz )T
n n n n
2 2
n=1
−
ν 0+D+1
ln|Σ|−
1 Tr(cid:0)
K
Σ−1(cid:1)
(B.10)
0
2 2
Combining terms, the familiar inverse Wishart form can be obtained
(cid:32)(cid:32) (cid:33) (cid:33)
N
lnp(Σ(τ+1)) ∝
−ν 0+N +D+1 ln|Σ|−1
Tr K
+(cid:88)
(x −µ−Wz )(x −µ−Wz )T Σ−1
0 n n n n
2 2
n=1
(B.11)
Therefore, a new sample of Σ can be drawn from the found conditional distribution
Σ(τ+1) ∼ IW(K +K,ν +N) (B.12)
0 0
where
N
(cid:88)
K = (x −µ−Wz )(x −µ−Wz )T (B.13)
n n n n
n=1
29Appendix B.2. Sample µ
Collecting the terms of the log joint pertaining to µ
N
1 (cid:88)(cid:110) (cid:111) 1
lnp(µ(τ+1)) ∝ − (x −(Wz +µ))TΣ−1(x −(Wz +µ)) − (µ−µ )TΣ−1(µ−µ )
2 n n n n 2 µ µ µ
n=1
(B.14)
Expanding and ignoring terms not containing µ,
N
1 (cid:88)(cid:110) (cid:111)
lnp(µ(τ+1)) ∝ − µTΣ−1µ−µTΣ−1(x −Wz )−(x −Wz )TΣ−1µ
n n n n
2
n=1
1
− (µTΣ−1µ−µTΣ−1µ−µTΣ−1µ ) (B.15)
2 µ µ µ µ µ
setting Σˆ−1 = NΣ−1 +Σ−1, and in the knowledge that I = Σˆ−1 Σˆ , the log form of a Gaussian
µ µ µ µ
can be reached
(cid:40)
N
lnp(µ(τ+1)) ∝ −1 µTΣˆ−1 µ−µTΣˆ−1 Σˆ (Σ−1(cid:88) (x −Wz )+Σ−1µ )
2 µ µ µ n n µ µ
n=1
(cid:32) (cid:33) (cid:41)
N
(cid:88)
− (x −Wz )TΣ−1+µ Σ−1 µT (B.16)
n n µ µ
n=1
Therefore, a new sample of µ can be drawn from the conditional distribution defined by
(cid:16) (cid:17)
µ(τ+1) ∼ N µˆ ,Σˆ (B.17)
µ µ
where
Σˆ = (NΣ−1+Σ−1)−1 (B.18)
µ µ
and
N
(cid:88)
µˆ = Σˆ (Σ−1 (x −Wz )+Σ−1µ ) (B.19)
µ µ n n µ µ
n=1
Appendix B.3. Sample w
i
Collecting the terms of the log joint pertaining to W
30N
lnp(w(τ+1) ) ∝
−1 (cid:88)(cid:110)
(x −(Wz +µ))TΣ−1(x −(Wz
+µ))(cid:111)
i 2 n n n n
n=1
d
1 (cid:88)(cid:110) (cid:111)
− (w −µ )TΣ−1(w −µ ) (B.20)
2
i wi wi i wi
i=1
As the intention is to update only one column of W at a time, i.e the independent column w , the
i
above equation can be separated as follows
N
lnp(w(τ+1) ) ∝
−1 (cid:88)(cid:110)
(x −µ−w z −w z )TΣ−1(x −µ−w z −w z
)(cid:111)
i 2 n i i,n i i,n n i i,n i i,n
n=1
1
− (w −µ )TΣ−1(w −µ ) (B.21)
2
i wi wi i wi
where the negative indices notation, · , refers to every column/row of except the one specified.
i
Letting x˜ = x −µ−w z , this equation can be simplified to
n n i i,n
N
lnp(w(τ+1) ) ∝
−1 (cid:88)(cid:110)
(x˜ −w z )TΣ−1(x˜ −w z
)(cid:111)
−
1
(w −µ )TΣ−1(w −µ ) (B.22)
i 2 n i i,n n i i,n 2 i wi wi i wi
n=1
and expanded to give
N
lnp(w(τ+1) ) ∝
−1 (cid:88)(cid:110)
x˜TΣ−1x˜ −(w z )TΣ−1x˜ −x˜TΣ−1w z +(w z )TΣ−1w z
(cid:111)
i 2 n n i i,n n n i i,n i i,n i i,n
n=1
1
− (wTΣ−1w −wTΣ−1µ −µT Σ−1w +µT Σ−1µ ) (B.23)
2 i wi i i wi wi wi wi i wi wi wi
As z is a scalar, it can be manipulated as such, rearranging to give
i,n
(cid:40)
N N N
lnp(w(τ+1) ) ∝ −1 (cid:88) {z z }wTΣ−1w −wTΣ−1w −(cid:88) z wTΣ−1x˜ −(cid:88) z x˜TΣ−1w
i 2 i,n i,n i i i wi i i,n i n i,n n i
n=1 n=1 n=1
(cid:111)
−wTΣ−1µ −µT Σ−1w (B.24)
i wi wi wi wi i
setting Σˆ−1 = (cid:80)N {z z }Σ−1 −Σ−1, and in the knowledge that I = Σˆ−1 Σˆ , the following
wi n=1 i,n i,n wi wi wi
log normal form can be reached
31(cid:40) (cid:32) (cid:33)
N
lnp(w(τ+1) ) ∝ −1 wTΣˆ−1 −wTΣˆ−1 Σˆ (cid:88) z Σ−1x˜ +Σ−1µ
i 2 i wi i wi wi i,n n wi wi
n=1
(cid:32) (cid:33) (cid:41)
N
− (cid:88) z x˜TΣ−1+µT Σ−1 Σˆ Σˆ−1 w (B.25)
i,n n wi wi wi wi i
n=1
Therefore, a new sample of w can be drawn from the conditional distribution defined by
i
(cid:16) (cid:17)
w(τ+1) ∝ N µˆ ,Σˆ (B.26)
i wi wi
where
(cid:32) (cid:33)−1
N
(cid:88)
Σˆ = {z z }Σ−1+Σ−1 (B.27)
wi i,n i,n wi
n=1
and
N
(cid:88)
µˆ = Σˆ (Σ−1 z x˜ +Σ−1µ ) (B.28)
wi wi i,n n wi wi
n=1
Since each column of W is sampled independently, two methods of sampling can be used. Either:
new columns are used in the following column updates, or the columns of the weight matrix are all
sampled using the previous value for W before finally updating the full matrix.
Appendix B.4. Sample z
Collecting the terms of the log joint pertaining to z
N N
1 (cid:88)(cid:110) (cid:111) 1 (cid:88)
lnp(z(τ+1)) ∝ − (x −(Wz +µ))TΣ−1(x −(Wz +µ)) − zTz (B.29)
n 2 n n n n 2 n n
n=1 n=1
which can be written as
N
1 (cid:88)(cid:110) (cid:111)
lnp(z(τ+1)) ∝ − (x −µ−Wz )TΣ−1(x −µ−Wz )+zTIz (B.30)
n 2 n n n n n n
n=1
Expanding and ignoring any terms not containing z ,
n
N
1 (cid:88)(cid:110)
lnp(z(τ+1)) ∝ − −xTΣ−1Wz +µTΣ−1Wz −zTWTΣ−1x +zTWTΣ−1µ
n 2 n n n n n n
n=1
(cid:111)
+zTWTΣ−1Wz +zTIz (B.31)
n n n n
32letting Σˆ−1 = (WTΣ−1W+I), and in the knowledge that I = Σˆ−1 Σˆ , the log form of a Gaussian
z z z
can be reached
N
lnp(z(τ+1)) ∝ −1 (cid:88)(cid:110) zTΣˆ z −(x −µ)TΣ−1WΣˆ Σˆ−1 z −zTΣˆ−1 Σˆ WTΣ−1(x −µ)(cid:111)
n 2 n z n n z z n n z z n
n=1
(B.32)
Therefore, a new sample of z can be drawn from the conditional distribution defined by
(cid:16) (cid:17)
z(τ+1) ∼ N µˆ ,Σˆ (B.33)
n z z
where
Σˆ = (WTΣ−1W+I)−1 (B.34)
z
and
µˆ = Σˆ WTΣ−1x (B.35)
z z n
33Appendix C. Bayesian CCA - Variational Bayes Derivation
This appendix provides a full derivation for the update equations provided in Algorithm 2. These
update equations differ with those presented by Wang [59] in that these equations assume indepen-
dent columns of W, matching the definition in [60], rather than independent rows. Either definition
(row or column) is suitable but one common approach was chosen here. Unlike [59], this derivation
also includes generic priors on the model parameters.
Given the model defined in Figure 2, the joint likelihood of Bayesian CCA can be defined as
L = p(x,z,W,Σ,µ) = p(x|z,W,Σ,µ)p(z)p(Σ)p(µ)p(W) (C.1)
Assuming independent columns of W, as stated in the model, and using precision Ψ, the joint
likelihood can be written as:
d
(cid:89)
L = p(x|z,W,Ψ,µ)p(z)p(Ψ)p(µ) p(w ) (C.2)
i
i=1
with the same components of the log likelihood as shown in the Appendix B derivation.
The general premise of VI is to select a suitable approximation from a tractable family of distri-
butions and try to make the approximation as close to the true (intractable) posterior as possible,
usually by minimising the KL divergence. Assuming the surrogate posterior is determined by some
free parameters, this problem reduces the inference to an optimisation problem.
The following subsections summarise the derivation of the individual parameter updates needed to
perform this optimisation. Even though a specified order is given in Algorithm 2, the VI scheme
means updating the parameters can be conducted in any order, as long as the order remains fixed.
Usingameanfieldapproximation,thesurrogateposteriorisassumedtotakethefollowingfactorised
form
d
(cid:89)
q(z,W,Ψ,µ) = q(z)q(Ψ)q(µ) q(w ) (C.3)
i
i=1
where
(cid:16) (cid:17)
q(z ) ∼ N z |µ˘ ,Σ˘ (C.4)
n n z z
q(Ψ) ∼
W(cid:16) Ψ|K˘−1 ,ν˘(cid:17)
(C.5)
(cid:16) (cid:17)
q(w ) ∼ N w |µ˘ ,Σ˘ (C.6)
i i wi wi
(cid:16) (cid:17)
q(µ) ∼ N µ|µ˘ ,Σ˘ (C.7)
µ µ
Given this factorised form, coordinate ascent VI (CAVI) [61] updates for the parameters of the
model can be found using
34q⋆(ϕ ) = E [L] = E [p(x,z,θ)] (C.8)
k ϕ ϕ
where ϕ = {z,θ}, θ = {Ψ,µ,W}, ϕ denotes all elements of ϕ except the kth element being
updated and q⋆(ϕ ) refers to the updated surrogate posterior.
k
Working with the log likelihood, this can also be expressed as
lnq⋆(ϕ ) = E [lnp(x,z,θ)]+const (C.9)
k ϕ
Appendix C.1. Update z
Collecting the terms of the log joint pertaining to z and substituting into Equation C.9,
(cid:34) (cid:35)
N N
1 (cid:88)(cid:110) (cid:111) 1 (cid:88)
lnq⋆(z ) ∝ E − (x −(Wz +µ))TΨ(x −(Wz +µ)) − zTz (C.10)
n ϕ 2 n n n n 2 n n
n=1 n=1
The expectation E [•] is rewritten using ⟨•⟩ , such that
ϕ ϕ
(cid:42) (cid:43)
N
1 (cid:88)(cid:110) (cid:111)
lnq⋆(z ) ∝ − (x −(Wz +µ))TΨ(x −(Wz +µ))+zTz (C.11)
n 2 n n n n n n
n=1 W, µ, Ψ
The subscript notation for the expectation is written once at the start of each update derivation
with the relevant parameters and then omitted for clarity.
Expanding and ignoring terms not containing z , similar to the Gibbs sampling derivation, this
n
reduces to
(cid:42) (cid:43)
N
1 (cid:88)(cid:110) (cid:16) (cid:17) (cid:111)
lnq⋆(z ) ∝ − zT WTΨW+I z −(x −µ)TΨWz −zTWTΨ(x −µ) (C.12)
n 2 n n n n n n
n=1
Setting Σ˘−1 = ⟨WTΨW⟩+I, and given I = Σ˘−1 Σ˘ , the log form of a Gaussian can be reached
z z z
N
lnq⋆(z ) ∝ −1 (cid:88)(cid:110) zTΣ˘ z −(x −⟨µ⟩)T⟨Ψ⟩⟨W⟩Σ˘ Σ˘−1 z −zTΣ˘−1 Σ˘ ⟨W⟩T⟨Ψ⟩(x −⟨µ⟩)(cid:111)
n 2 n z n n z z n n z z n
n=1
(C.13)
Exponentiating, the surrogate posterior of z therefore has the following Gaussian form
(cid:16) (cid:17)
q⋆(z ) ∼ N µ˘ ,Σ˘ (C.14)
n z z
35where
(cid:16) (cid:17)−1
Σ˘ = ⟨WTΨW⟩+I (C.15)
z
and
µ˘ = Σ˘ ⟨W⟩T⟨Ψ⟩(x −⟨µ⟩) (C.16)
z z n
Update Ψ
Collecting the terms of the log joint pertaining to Ψ and substituting into Equation C.9,
(cid:42)
N
1 (cid:88)(cid:110) (cid:111)
lnq⋆(Ψ) ∝ − ln(|Ψ|−1)+(x−(Wz +µ))TΨ(x−(Wz +µ))
n n
2
n=1
(cid:29)
ν −D+1 1
0
+ ln|Ψ|− tr(ΨK ) (C.17)
0
2 2
W, µ, zn
Simplifying and rearranging terms, a log Wishart form can be reached such that
(cid:32) (cid:32) (cid:33)(cid:33)
N
1 (cid:88)(cid:68) (cid:69)
lnq⋆(Ψ) ∝ − tr Ψ K + (x−(Wz +µ))(x−(Wz +µ))T
0 n n
2
n=1
ν −D+1+N
0
+ ln|Ψ| (C.18)
2
Exponentiating, the surrogate posterior of Ψ is then defined by,
q⋆(Ψ) ∼
W(cid:16) K˘−1 ,ν˘(cid:17)
(C.19)
where
N
(cid:88)(cid:68) (cid:69)
K˘ = K + (x −µ−Wz )(x −µ−Wz )T (C.20)
0 n n n n
n=1
and
ν˘ = ν +N (C.21)
0
Update µ
Collecting the terms of the log joint pertaining to µ and substituting into Equation C.9,
36(cid:42)
N
1 (cid:88)(cid:110) (cid:111) 1
lnq⋆(µ) ∝ − (x−(Wz +µ))TΨ(x−(Wz +µ)) − µTΣ−1µ
n n µ
2 2
n=1
(cid:29)
1 1
− µTΣ−1µ− µTΣ−1µ (C.22)
µ µ µ µ
2 2
W, zn, Ψ
Simplifying,
(cid:40)
N
1 (cid:88)
lnq⋆(µ) ∝ − µT(N ⟨Ψ⟩+Σ−1)µ−µT ⟨Ψ⟩(x −⟨W⟩⟨z ⟩)
µ n n
2
n=1
(cid:41)
N
(cid:88)
− (x −⟨W⟩⟨z ⟩)T⟨Ψ⟩µ+µTΣ−1µ+µTΣ−1µ (C.23)
n n µ µ µ µ
n=1
letting Σ˘−1 = N ⟨Ψ⟩+Σ−1, and given I = Σ˘−1 Σ˘ ,
µ µ µ µ
(cid:40) (cid:32) (cid:33)
N
lnq⋆(µ) ∝ −1 µTΣ˘−1 µ−µTΣ˘−1 Σ˘ (cid:88) ⟨Ψ⟩(x −⟨W⟩⟨z ⟩)+Σ−1µ
2 µ µ µ n n µ µ
n=1
(cid:32) (cid:33) (cid:41)
N
− (cid:88) (x −⟨W⟩⟨z ⟩)T⟨Ψ⟩+µTΣ−1 Σ˘ Σ˘−1 µ (C.24)
n n µ µ µ µ
n=1
Exponentiating, the surrogate posterior of µ thus has the following form
(cid:16) (cid:17)
q⋆(µ) ∼ N µ˘ ,Σ˘ (C.25)
µ µ
where
Σ˘ = (N⟨Ψ⟩+Σ−1)−1 (C.26)
µ µ
and
(cid:32) (cid:33)
N
(cid:88)
µ˘ = Σ˘ ⟨Ψ⟩ (x −⟨W⟩⟨z ⟩)+Σ−1µ (C.27)
µ µ n n µ µ
n=1
Update w
i
Collecting the terms of the log joint pertaining to w and substituting into Equation C.9,
i
(cid:42)
N
1 (cid:88)(cid:110) (cid:111) 1
lnq⋆(w ) ∝ − (x−(Wz +µ))TΨ(x−(Wz +µ)) − wTΣ−1w
i 2 n n 2 i wi i
n=1
(cid:29)
1 1
− µT Σ−1w − wTΣ−1µ (C.28)
2 wi wi i 2 i wi wi
µ, zn, Ψ
37To construct the update for a single column, the full weight matrix must be separated into the
column of interest and the remaining columns. This is achieved by defining
x˜ = x −µ−w z (C.29)
n n i i,n
where w corresponds to all columns except the ith column of interest and z corresponds to all
i i,n
rows of z except the ith row. Using this definition, Equation C.28 becomes
n
(cid:42)
N
1 (cid:88)(cid:110) (cid:111) 1
lnq⋆(w ) ∝ − (x˜ −w z )TΨ(x˜ −w z ) − wTΣ−1w
i 2 n i i,n n i i,n 2 i wi i
n=1
(cid:29)
1 1
− µT Σ−1w − wTΣ−1µ (C.30)
2 wi wi i 2 i wi wi
As z is scalar, it can be manipulated as one. Rearranging and combining terms, this gives
i,n
(cid:42) (cid:32) (cid:33) (cid:32) (cid:33)
N N
1 (cid:88) (cid:88)
lnq⋆(w ) ∝ − wT Σ−1+ z z Ψ w −wT z Ψx˜ +Σ−1µ
i 2 i wi i,n i,n i i i,n n wi wi
n=1 n=1
(cid:32) (cid:33) (cid:43)
N
(cid:88)
− x˜TΨz +µT Σ−1 w (C.31)
n i,n wi wi i
n=1
Considering the expectations,
(cid:40) (cid:32) (cid:33) (cid:32) (cid:33)
N N
1 (cid:88) (cid:88)
lnq⋆(w ) ∝ − wT Σ−1+ ⟨z z ⟩⟨Ψ⟩ w −wT ⟨z ⟩⟨Ψ⟩x˜ +Σ−1µ
i 2 i wi i,n i,n i i i,n n wi wi
n=1 n=1
(cid:32) (cid:33) (cid:41)
N
(cid:88)
− x˜T⟨Ψ⟩⟨z ⟩+µT Σ−1 w (C.32)
n i,n wi wi i
n=1
letting Σ˘−1 = (cid:16) Σ−1+(cid:80)N ⟨z z ⟩⟨Ψ⟩(cid:17) , and as I = Σ˘−1 Σ˘ ,
wi wi n=1 i,n i,n wi wi
(cid:16) (cid:17)
q⋆(w ) ∼ N µ˘ ,Σ˘ (C.33)
i wi wi
where
N
(cid:88)
Σ˘ = ( ⟨z z ⟩⟨Ψ⟩+Σ−1)−1 (C.34)
wi i,n i,n wi
n=1
and
(cid:32) (cid:33)
N
(cid:88)
µ˘ = Σ˘ ⟨Ψ⟩ x˜ ⟨zT ⟩)+Σ−1µ (C.35)
wi wi n n,i wi wi
n=1
38References
[1] RBrinckerandC.EVentura. Introduction to Operational Modal Analysis. JohnWiley&Sons,
Ltd, Chichester, UK, 2015.
[2] R Brincker, L Zhang, and P Andersen. Modal identification of output-only systems using
frequency domain decomposition. Smart Materials and Structures, 10(3):441–445, 2001.
[3] P Van Overschee and B De Moor. Subspace Identification for Linear Systems. Springer US,
Boston, MA, 1996.
[4] T Katayama. Subspace Methods for System Identification: A Realisation Approach. Commu-
nications and Control Engineering. Springer, London, 2005.
[5] C. R Farrar and K Worden. Structural Health Monitoring: A Machine Learning Perspective.
Wiley, Chichester, 2013.
[6] J. H Mclean, M. R Jones, B. J O’Connell, E Maguire, and T. J Rogers. Physically meaning-
ful uncertainty quantification in probabilistic wind turbine power curve models as a damage-
sensitive feature. Structural Health Monitoring, 22(6):3623–3636, 2023.
[7] B Li. Uncertainty Quantification in Vibration-based Structural Health Monitoring Using
Bayesian Statistics. PhD thesis, University of California, Berkeley, 2016.
[8] L. A Bull, P Gardner, T. J Rogers, E. J Cross, N Dervilis, and K Worden. Probabilistic
Inference for Structural Health Monitoring: New Modes of Learning from Data. ASCE-ASME
Journal of Risk and Uncertainty in Engineering Systems, Part A: Civil Engineering, 7(1):
03120003, 2021.
[9] M Miah and W Lienhart. Uncertainties of Parameters Quantification in SHM. In 8th European
Congress on Computational Methods in Applied Sciences and Engineering. CIMNE, 2022.
[10] D. J Wagg, K Worden, R. J Barthorpe, and P Gardner. Digital Twins: State-of-the-Art
and Future Directions for Modeling and Simulation in Engineering Dynamics Applications.
ASCE-ASME Journal of Risk and Uncertainty in Engineering Systems, Part B: Mechanical
Engineering, 6(3):030901, 2020.
[11] A Thelen, X Zhang, O Fink, Y Lu, S Ghosh, B. D Youn, M. D Todd, S Mahadevan, C Hu,
and Z Hu. A comprehensive review of digital twin—part 2: Roles of uncertainty quantification
and optimization, a battery digital twin, and perspectives. Structural and Multidisciplinary
Optimization, 66(1):1, 2023.
[12] J R´ıos, G Staudter, M Weber, R Anderl, and A Bernard. Uncertainty of data and the digital
twin: A review. International Journal of Product Lifecycle Management, 12(4):329–358, 2020.
[13] B Kochunas and X Huan. Digital Twin Concepts with Uncertainty for Nuclear Power Appli-
cations. Energies, 14(14):4235, 2021.
[14] A. J Hughes. On Risk-Based Decision-Making for Structural Health Monitoring. PhD thesis,
University of Sheffield, Sheffield, UK, 2022.
[15] KCiloglu, YZhou, FMoon, andA.EAktan. ImpactsofEpistemicUncertaintyinOperational
Modal Analysis. Journal of Engineering Mechanics, 138(9):1059–1070, 2012.
39[16] E Reynders, R Pintelon, and G De Roeck. Uncertainty bounds on modal parameters obtained
from stochastic subspace identification. Mechanical Systems and Signal Processing, 22(4):948–
969, 2008.
[17] J.-H Yang and H.-F Lam. An innovative Bayesian system identification method using autore-
gressive model. Mechanical Systems and Signal Processing, 133:106289, 2019.
[18] J Kang and S Zeng. Uncertainty quantification in operational modal analysis of time-varying
structures based on time-dependent autoregressive moving average model. Journal of Sound
and Vibration, 548:117549, 2023.
[19] T. J Rogers, K Worden, and E. J Cross. On the application of Gaussian process latent force
models for joint input-state-parameter estimation: With a view to Bayesian operational iden-
tification. Mechanical Systems and Signal Processing, 140:106580, 2020.
[20] S.-KAu. FastBayesianambientmodalidentificationinthefrequencydomain, PartI:Posterior
most probable value. Mechanical Systems and Signal Processing, 26:60–75, 2012.
[21] S.-KAu. FastBayesianambientmodalidentificationinthefrequencydomain,PartII:Posterior
uncertainty. Mechanical Systems and Signal Processing, 26:76–90, 2012.
[22] J. L Beck. Bayesian system identification based on probability logic. Structural Control and
Health Monitoring, 17(7):825–847, 2010.
[23] Y Huang, J. L Beck, and H Li. Bayesian system identification based on hierarchical sparse
BayesianlearningandGibbssamplingwithapplicationtostructuraldamageassessment. Com-
puter Methods in Applied Mechanics and Engineering, 318:382–411, 2017.
[24] E Reynders, K Maes, G Lombaert, and G De Roeck. Uncertainty quantification in operational
modalanalysiswithstochasticsubspaceidentification: Validationandapplications. Mechanical
Systems and Signal Processing, 66–67:13–30, 2016.
[25] E. P Reynders. Uncertainty quantification in data-driven stochastic subspace identification.
Mechanical Systems and Signal Processing, 151:107338, 2021.
[26] M D¨ohler and L Mevel. Efficient multi-order uncertainty computation for stochastic subspace
identification. Mechanical Systems and Signal Processing, 38(2):346–366, 2013.
[27] M D¨ohler, X.-B Lam, and L Mevel. Uncertainty quantification for modal parameters from
stochasticsubspaceidentificationonmulti-setupmeasurements.MechanicalSystemsandSignal
Processing, 36(2):562–581, 2013.
[28] S Gres and M D¨ohler. Uncertainty propagation in subspace methods for operational modal
analysis under misspecified model orders. In ISMA-USD 2022 - International Conference on
Noise and Vibration Engineering, and Uncertainty Structural Dynamics, Leuven, Belgium,
2022.
[29] S Gre´s, R Riva, C. Y Su¨leyman, P Andersen, and M. M L(cid:32) uczak. Uncertainty quantification of
modal parameter estimates obtained from subspace identification: An experimental validation
on a laboratory test of a large-scale wind turbine blade. Engineering Structures, 256:114001,
2022.
40[30] R Pintelon, P Guillaume, and J Schoukens. Uncertainty calculation in (operational) modal
analysis. Mechanical Systems and Signal Processing, 21(6):2359–2373, 2007.
[31] MEl-kafafy, TDeTroyer, BPeeters, andPGuillaume. Fastmaximum-likelihoodidentification
ofmodalparameterswithuncertaintyintervals: Amodalmodel-basedformulation. Mechanical
Systems and Signal Processing, 37(1-2):422–439, 2013.
[32] P Mellinger, M D¨ohler, and L Mevel. Variance estimation of modal parameters from output-
only and input/output subspace-based system identification. Journal of Sound and Vibration,
379:1–27, 2016.
[33] E Reynders. System Identification Methods for (Operational) Modal Analysis: Review and
Comparison. Archives of Computational Methods in Engineering, 19(1):51–124, 2012.
[34] S Pereira, E Reynders, F Magalh˜aes, A´ Cunha, and J. P Gomes. The role of modal pa-
rameters uncertainty estimation in automated modal identification, modal tracking and data
normalization. Engineering Structures, 224:111208, 2020.
[35] J Priou, S Gres, M Perrault, L Guerineau, and M D¨ohler. Automated uncertainty-based
extraction of modal parameters from stabilization diagrams. In 9th International Operational
Modal Analysis Conference, Vancouver, BC, Canada, 2022.
[36] S Gre´s and M D¨ohler. Model Order Selection for Uncertainty Quantification in Subspace-
Based OMA of Vestas V27 Blade. In M. P Limongelli, P. F Giordano, S Quqa, C Gentile, and
A Cigada, editors, Experimental Vibration Analysis for Civil Engineering Structures, volume
433, pages 43–52. Springer Nature Switzerland, Cham, 2023.
[37] S.-KAu,F.-LZhang,andY.-CNi. Bayesianoperationalmodalanalysis: Theory,computation,
practice. Computers & Structures, 126:3–14, 2013.
[38] K.-V Yuen, L. S Katafygiotis, and J. L Beck. Spectral density estimation of stochastic vector
processes. Probabilistic Engineering Mechanics, 17(3):265–272, 2002.
[39] K.-V Yuen and L. S Katafygiotis. Bayesian Fast Fourier Transform Approach for Modal Up-
dating Using Ambient Data. Advances in Structural Engineering, 6(2):81–95, 2003.
[40] S.-K Au. Connecting Bayesian and frequentist quantification of parameter uncertainty in
system identification. Mechanical Systems and Signal Processing, 29:328–342, 2012.
[41] S.-K Au. Insights on the Bayesian spectral density method for operational modal analysis.
Mechanical Systems and Signal Processing, 66–67:1–12, 2016.
[42] S.-K Au. Operational Modal Analysis. Springer Singapore, Singapore, 2017.
[43] S.-K Au, J. M Brownjohn, and J. E Mottershead. Quantifying and managing uncertainty in
operational modal analysis. Mechanical Systems and Signal Processing, 102:139–157, 2018.
[44] Y.-C Zhu and S.-K Au. Bayesian operational modal analysis with asynchronous data, part I:
Most probable value. Mechanical Systems and Signal Processing, 98:652–666, 2018.
[45] Y.-C Zhu and S.-K Au. Bayesian operational modal analysis with asynchronous data, Part II:
Posterior uncertainty. Mechanical Systems and Signal Processing, 98:920–935, 2018.
41[46] Y.-C Zhu and S.-K Au. Bayesian data driven model for uncertain modal properties identified
from operational modal analysis. Mechanical Systems and Signal Processing, 136:106511, 2020.
[47] Z Zhu, S.-K Au, B Li, and Y.-L Xie. Bayesian operational modal analysis with multiple setups
and multiple (possibly close) modes. Mechanical Systems and Signal Processing, 150:107261,
2021.
[48] J Mao, X Su, H Wang, and J Li. Automated Bayesian operational modal analysis of the long-
span bridge using machine-learning algorithms. Engineering Structures, 289:116336, 2023.
[49] J. M. W Brownjohn, S.-K Au, Y Zhu, Z Sun, B Li, J Bassitt, E Hudson, and H Sun. Bayesian
operational modal analysis of Jiangyin Yangtze River Bridge. Mechanical Systems and Signal
Processing, 110:210–230, 2018.
[50] J.M.WBrownjohn, ARaby, S.-KAu, ZZhu, XWang, AAntonini, APappas, andDD’Ayala.
Bayesian operational modal analysis of offshore rock lighthouses: Close modes, alignment,
symmetry and uncertainty. Mechanical Systems and Signal Processing, 133:106306, 2019.
[51] W.-J Yan and L. S Katafygiotis. A two-stage fast Bayesian spectral density approach for
ambient modal analysis. Part II: Mode shape assembly and case studies. Mechanical Systems
and Signal Processing, 54–55:156–171, 2015.
[52] B Li and A Der Kiureghian. Operational modal identification using variational Bayes. Me-
chanical Systems and Signal Processing, 88:377–398, 2017.
[53] B Li, A Der Kiureghian, and S.-K Au. A Gibbs sampling algorithm for structural modal
identification under seismic excitation. Earthquake Engineering & Structural Dynamics, 47
(14):2735–2755, 2018.
[54] Q Dollon, J Antoni, A Tahan, M Gagnon, and C Monette. A fast collapsed Gibbs sampler for
frequency domain operational modal analysis. Mechanical Systems and Signal Processing, 173:
108985, 2022.
[55] H Hotelling. Relations between two sets of variates. Biometrika, 28(3/4):321–375, 1936.
[56] F. R Bach and M. I Jordan. A Probabilistic Interpretation of Canonical Correlation Analysis.
Technical Report 688, University of California, Berkeley, 2005.
[57] DKollerandNFriedman. Probabilistic Graphical Models: Principles and Techniques. Adaptive
Computation and Machine Learning. MIT Press, Cambridge, MA, 2009.
[58] B. J O’Connell and T. J Rogers. A robust probabilistic approach to stochastic subspace
identification. Journal of Sound and Vibration, 581:118381, 2024.
[59] CWang. VariationalBayesianApproachtoCanonicalCorrelationAnalysis. IEEETransactions
on Neural Networks, 18(3):905–910, 2007.
[60] A Klami and S Kaski. Local dependent components. In Proceedings of the 24th International
Conference on Machine Learning, pages 425–432, Corvalis Oregon USA, 2007. ACM.
[61] K. P Murphy. Machine Learning: A Probabilistic Perspective. The MIT Press, 2012.
[62] C. M Bishop. Pattern Recognition and Machine Learning. Information Science and Statistics.
Springer, New York, 2006.
42[63] M Opper and D Saad, editors. Advanced mean field methods: theory and practice. Neural
Information Processing. MIT, Cambridge, Massachusetts (US), 2001.
[64] S S¨arkk¨a and A Solin. Applied Stochastic Differential Equations. Cambridge University Press,
2019.
[65] J Maeck and G De Roeck. Description of Z24 benchmark. Mechanical Systems and Signal
Processing, 17(1):127–131, 2003.
[66] L Mevel, M Goursat, and M Basseville. Stochastic supspace-based structureal identification
and damage detection and localisation - application to the Z24 bridge benchmark. Mechanical
Systems and Signal Processing, 17(1):143–151, 2003.
[67] D. J Ewins. Modal Testing: Theory, Practice and Application. Number 10 in Mechanical
Engineering Research Studies Engineering Dynamics Series. Research Studies Press, Baldock,
2000.
43