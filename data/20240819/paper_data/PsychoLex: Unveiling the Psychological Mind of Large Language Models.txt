PsychoLex: Unveiling the Psychological Mind of Large Language Models
Mohammad Amin Abbasi1, Farnaz Sadat Mirnezami2, Hassan Naderi1
1Department of Computer Engineering, Iran University of Science and Technology, Tehran, Iran
2Department of Computer Engineering, University of Guilan, Rasht, Iran
m_abbasi1378@comp.iust.ac.ir
farnaz.mirnezami@gmail.com
naderi@iust.ac.ir
Abstract in sophisticated dialogues (Agrawal, 2023). How-
ever, as users increasingly rely on LLMs for psy-
This paper explores the intersection of chological and therapeutic questions (Lai et al.,
psychology and artificial intelligence
2023), the limitations of these models in special-
through the development and evaluation
ized domains have become apparent. Notably,
of specialized Large Language Models
there is a critical absence of datasets designed to
(LLMs). We introduce PsychoLex1, a
evaluate and enhance LLMs' performance in the
suite of resources designed to enhance
field of psychology.
LLMs' proficiency in psychological tasks
in both Persian and English. Key contribu- Despite considerable progress in general AI re-
tions include the PsychoLexQA dataset search, the integration of psychological expertise
for instructional content and the Psycho- into LLMs remains underdeveloped. Existing
LexEval dataset for rigorous evaluation of methodologies often lack the depth required to un-
LLMs in complex psychological scenar- derstand and respond accurately to complex psy-
ios. Additionally, we present the Psycho-
chological inquiries. Moreover, the field is hin-
LexLLaMA model, optimized specifically
dered by the lack of comprehensive datasets that
for psychological applications, demon-
include not only questions and answers but also
strating superior performance compared to
instructional content tailored to psychological
general-purpose models. The findings un-
contexts. This gap is significant because it re-
derscore the potential of tailored LLMs for
advancing psychological research and ap- stricts the practical applications of LLMs in psy-
plications, while also highlighting areas chological research, therapy, and education,
for further refinement. This research offers where nuanced and precise information is essen-
a foundational step towards integrating tial.
LLMs into specialized psychological do- Our research seeks to address this gap by intro-
mains, with implications for future ad-
ducing PsychoLex, a suite of resources and mod-
vancements in AI-driven psychological
els specifically designed for psychological appli-
practice.
cations in both Persian and English. The primary
objectives of this study are to develop and evalu-
1 Introduction
ate specialized datasets, namely PsychoLexQA
The rise of Large Language Models (LLMs) has and PsychoLexEval, and to introduce Psycho-
significantly advanced artificial intelligence (AI), LexLLaMA, an LLM developed for psycholo-
providing remarkable capabilities in natural lan- gyical tasks. These contributions include: (i) Psy-
guage processing and understanding (Guo et al., choLexQA, which provides comprehensive in-
2023; Minaee et al., 2024; Wu et al., 2023). These structional content and detailed questions and an-
models have shown proficiency in generating hu- swers to enhance LLM training; (ii) Psycho-
man-like text, translating languages, and engaging LexEval, a multiple-choice question and answer
(MCQA) dataset designed for rigorous evaluation
1 https://huggingface.co/collections/aminabbasi/psy-
cholex-66b64e3768da519596e49de9
1of LLMs in psychological contexts, ensuring they 2023a), GPT-4 (OpenAI, 2023b), and OpenChat-
can handle complex psychological queries accu- 3.5. This study, which is the first extensive bench-
rately and contextually;(iii) PsychoLexLLaMA, marking effort for Persian, aims to address the
which improves the performance of LLMs in psy- challenges posed by Persian as a low-resource
chological tasks through continual pre-training language with unique linguistic features. The
and fine-tuning of LLaMA 3.1.(Dubey et al., evaluation covers a broad range of natural lan-
2024) Together, these contributions aim to pro- guage processing (NLP) tasks, including senti-
vide robust solutions to existing challenges, en- ment analysis, question answering, natural lan-
hancing the accuracy and relevance of AI-driven guage inference, and translation. the study high-
psychological tools and paving the way for future lights the model's superior performance in multi-
advancements in integrating AI with psychologi- ple-choice questions(MCQs) related to math and
cal practice. general knowledge from the ParsiNLU dataset
The structure of this paper is organized as fol- (Khashabi et al., 2020). These benchmarks are
lows: Section 2 reviews related work in LLMs and particularly important for assessing the models'
their applications in psychology. Section 3 details reasoning capabilities in Persian. While
the datasets developed for this study, including ChatGPT-4 excels across several benchmarks, its
their creation and validation processes. Section 4 application in psychology has not been tested, un-
discusses the development and fine-tuning of the derscoring a critical area for future research.
PsychoLexLLaMA model. Section 5 presents the
2.2 Khayyam Challenge (PersianMMLU)
evaluation methodology and results, comparing
PsychoLexLLaMA with other state-of-the-art Recent advancements have focused on optimizing
models. Section 6 provides a comprehensive dis- the performance of Large Language Models
cussion of the findings, and Section 7 concludes (LLMs). The PersianMMLU (Ghahroodi et al.,
the paper with insights into future research direc- 2024) is particularly significant as it concentrates
tions and potential applications. on the Persian language capabilities of these mod-
By exploring the intersection of AI and psy- els. It evaluates their proficiency in answering
chology, this paper aims to unveil the psycholog- multiple-choice questions across diverse fields
ical capabilities of LLMs and demonstrate their such as mathematics, science, logic, and intelli-
potential to advance both fields significantly. gence testing. This comprehensive evaluation in-
volved advanced models like GPT-3.5, GPT-
2 Related Works 4(OpenAI, 2023b), Aya (Ustun et al., 2024), Per-
sianMind(Rostami et al., 2024), mT0
In this section, we review existing research that
(Muennighoff et al., 2023), mGPT (Shliazhko et
benchmarks the capabilities of large language
al., 2022), and Claude3-haiku (Anthropic, 2024).
models (LLMs) in Persian, followed by studies
The study utilized a robust dataset derived from
that explore the integration of LLMs into psycho-
Iran’s national university entrance exams and ed-
logical research and applications. This dual-focus
ucational assessments. While GPT-4 emerged as
review establishes the context for our work, em-
the superior model, its efficacy in psychological
phasizing both the linguistic challenges specific to
applications remains untested. This gap highlights
Persian and the broader implications of applying
the necessity of our current research, which aims
LLMs in the field of psychology.
to specifically evaluate the performance of LLMs
2.1 Benchmarking Large Language Models in psychology-related scenarios.
for Persian
2.3 Using large language models in psychol-
Recent advancements in large language models ogy
(LLMs), particularly ChatGPT, have generated
Dubey et al. (2024) explores the integration of
significant interest in their evaluation across vari-
LLMs, particularly GPT-3 and GPT-4, into psy-
ous languages and tasks. ChatGPT’s performance
chological research practices. These models'
on various Persian natural language processing
adeptness at text generation, dialogue engage-
tasks is evaluated by Abaskohi et al. (2024). they
ment, persona simulation, and information syn-
present a comprehensive evaluation of large lan-
thesis provides innovative approaches to studying
guage models (LLMs) for the Persian language,
various psychological subfields. The primary aim
focusing on models like GPT-3.5-turbo (OpenAI,
2is to evaluate the extent to which LLMs can enrich Persian and English versions to establish a bilin-
psychological research methodologies. Despite gual foundation for our models. The dataset com-
their potential, LLMs often fall short in delivering prised approximately 1.3 million tokens, offering
contextually accurate advice consistently. This a rich and diverse corpus that spans a broad spec-
study highlights the importance of refining LLMs trum of psychological topics. This extensive pre-
through fine-tuning and reinforcement learning training data enabled our models to develop a
from human feedback to ensure their practical ef- deep understanding of essential psychological
ficacy in real-world psychological settings. The concepts and terminology, facilitating their appli-
extensive datasets used to train these models, en- cation in both Persian and English contexts.
compassing diverse sources of human language
3.2 PsychoLexQA
data, are aimed at tailoring LLMs to better serve
both theoretical and applied psychology. For the instructional dataset, we adopted two dis-
tinct methodologies to generate detailed and com-
2.4 Exploring the Frontiers of LLMs in Psy-
prehensive instructional content in both Persian
chological Applications
and English. Appendix A demonstrates two exam-
The application of Artificial Intelligence (AI), es- ples of the PsychoLexQA dataset.
pecially large language models (LLMs), is revo-
lutionizing psychological research. A study by Ke
3.2.1 Document-Based Instructions
et al. (2024) underscores significant advances in
language models and their profound impact on the The first method involved extracting instructional
field of psychology. LLMs like OpenAI's content from "Introduction to Psychology" in both
ChatGPT facilitate various research activities, in- languages. This process was automated using the
cluding literature reviews, hypothesis formula- GPT-4o model, where paragraphs from the text-
tion, experiment design, data analysis, and schol- book were analyzed to grasp key concepts. For
arly writing across several psychological domains each paragraph, the model generated a series of
such as cognitive, behavioral, clinical, educa- questions and answers aimed at testing material
tional, developmental, and social psychology. comprehension. Each question was crafted to be
While these models offer substantial benefits, the clear and precise, with detailed answers provided
review also delineates key technical and ethical to ensure a thorough understanding of the dis-
challenges, including data privacy concerns and cussed psychological concepts. Paragraphs lack-
inherent limitations of LLMs. The authors advo- ing sufficient content for question generation
cate for the careful integration of these technolo- were identified and noted. This method resulted in
gies in psychological research to enhance our un- a dataset containing 7,055 entries.
derstanding of the human mind and improve the
methodologies employed in psychological stud- 3.2.2 Self-Instruct
ies. The second method focused on creating structured
instructional tasks for various psychological sub-
3 Dataset
categories in both Persian and English. This in-
volved defining tasks such as "Case Study Analy-
This section outlines the datasets developed to in-
sis", "Experiment Design", and "Data Interpreta-
vestigate the application of large language models
tion" across different psychological subfields like
(LLMs) in psychology. We detail the creation and
Clinical Psychology and Cognitive Psychology.
utilization of three pivotal datasets: the founda-
For each task and subcategory combination, de-
tional pretraining data, the PsychoLexQA dataset
tailed instructions were generated, including task
for instructional content, and the PsychoLexEval
descriptions, optional inputs, and expected out-
dataset for evaluating model comprehension and
puts. These tasks were presented in a bilingual
performance.
format, accommodating both Persian and English
3.1 Pretraining Data speakers. The dataset created using GPT-4o com-
prises a total of 3,001 rows, ensuring extensive
For the pretraining phase, we employed "Intro-
coverage of psychological topics.
duction to Psychology" by Hilgard (1953), a sem-
inal textbook noted for its comprehensive insights
into psychology. This text was used in both its
33.3 PsychoLexEval field of psychology, ensuring comprehensive cov-
erage across a wide spectrum of psychological
The PsychoLexEval dataset, a multiple-choice
fields. It includes general psychology, focusing on
question and answer (MCQA) format in both Per-
basic concepts; developmental psychology, which
sian and English, is designed to assess the com-
examines human growth and cognitive develop-
prehension and performance of LLMs in psychol-
ment; and clinical psychology, addressing the di-
ogy. This section will describe the data collection
agnosis and treatment of mental disorders. Addi-
and review process, the methods employed to en-
tionally, the dataset encompasses psychometrics,
sure quality and compliance, and the broad scope
highlighting methods of measuring psychological
and coverage of this MCQA dataset.
attributes; cognitive tests for assessing intelli-
3.3.1 Data Collection gence and aptitude; and industrial and organiza-
tional psychology, which looks at behavior and
To construct this dataset, we compiled questions
productivity in the workplace.
from multiple significant sources: (1) Graduate
Further expanding its breadth, the dataset co-
Entrance Exams: questions from psychology en-
vers social psychology, which explores social be-
trance exams (2014-2024) that cover advanced
haviors and group dynamics; educational psychol-
topics; (2) Employment Exams: questions from
ogy, focusing on learning processes and teaching
various job tests, including both specialized and
methods; and the needs of exceptional children
general psychology; (3) Online Sources: Ques-
with special requirements. It also integrates key
tions from trusted psychology test websites; (4)
concepts from "Introduction to Psychology," cov-
GPT-4 Generated Content: questions from Psy-
ering a range of fundamental topics including bi-
chology books, covering a wide range of topics.
ological foundations, sensory processes, learning,
3.3.2 Filtering and Review memory, motivation, emotion, intelligence, per-
sonality, and psychological disorders.
To ensure high quality and legal compliance, we
By providing such diverse and extensive con-
implemented rigorous filtering and review pro-
tent, the PsychoLexEval dataset serves as an in-
cesses for the dataset. Initially, a human review
valuable resource for researchers in psychology
was conducted where a sample of questions was
and artificial intelligence. It equips them with a
meticulously scrutinized by experts. This step was
robust tool to deepen insights into psychological
crucial to ensure that each question was relevant,
phenomena and advance the field by effectively
complete, and clearly articulated. During this
evaluating the capabilities of LLMs across vari-
phase, we specifically retained only those ques-
ous psychology domains. The dataset comprises
tions that had exactly four answer options, ensur-
3,430 rows. Appendix A shows an example of the
ing consistency and clarity in the evaluation pro-
PsychoLexEval dataset.
cess. Additionally, to avoid any legal complica-
tions, we carefully removed any content that po-
4 PsychoLexLLaMA
tentially violated copyright laws. This step was
essential to maintain the integrity of the dataset In this section, we detail the development of Psy-
and ensure that all included materials were legally choLexLLaMA, a specialized large language
compliant for use in our research and broader ac- model (LLM) designed explicitly for psychology.
ademic dissemination. These measures collec- Our goal was to surpass the performance of gen-
tively reinforced the dataset's reliability and ad- eral-purpose models by optimizing our model to
herence to legal standards, providing a robust require minimal data and hardware resources. We
foundation for evaluating large language models utilized the Transformers2 library for model de-
within psychological contexts. velopment The process of constructing our model
is illustrated in Appendix A.
3.3.3 Scope and Coverage
4.1 Continuous Pre-Training
The PsychoLexEval dataset is meticulously de-
signed to evaluate the comprehension and perfor- For continuous pre-training (Zhou et al., 2024),
mance of large language models (LLMs) in the we employed the LoRA technique (Hu et al.,
2021) on the bilingual texts of "Introduction to
2 https://github.com/huggingface/transformers
4Psychology" by Hilgard. This foundational work newly developed psychological expertise, produc-
was processed in both Persian and English, lever- ing a balanced and potent tool adept at handling
aging the established pretraining data. We utilized sophisticated psychological inquiries.
LLaMA 3.1(Dubey et al., 2024) as our base mod- Through these meticulous steps, Psycho-
els in two configurations: 8B and 70B. This stage LexLLaMA has been meticulously tailored to
was critical for aligning the base models with psy- meet the unique needs of psychological applica-
chological content, thereby enhancing their un- tions. It stands as a robust resource for researchers
derstanding and application of complex psycho- and practitioners in both psychology and artificial
logical concepts efficiently. The pre-training for intelligence, providing a reliable platform for fur-
the 8B model took 8 minutes using a single A100 ther explorations and advancements in these
80GB GPU, while the 70B model required 41 fields. The next sections will evaluate Psycho-
minutes on two A100 80GB GPUs. Table 1 pro- LexLLaMA’s performance in detail, comparing it
vides a detailed overview of the LoRA training with other models to underscore its enhanced ca-
configurations used during this phase. pabilities in the realm of psychological research
and practice.
Lr Rank Alpha Dropout 5 Evaluation
1e-5 8 16 0.0
In this study, we conducted a comprehensive eval-
Table 1: LoRA training configurations uation of various language models that operate in
both Persian and English, focusing on their ability
to understand and accurately respond to psycho-
4.2 Supervised Fine-Tuning
logical questions. The models assessed include in-
The supervised fine-tuning phase was essential clude Qwen2 (Yang et al., 2024), Aya-23
for tailoring our models to meet the specific de- (Aryabumi et al., 2024), Phi-3 (Abdin et al.,
mands of psychological analysis. Utilizing the 2024), Llama-3, Llama-3.1(Dubey et al., 2024),
PsychoLexQA dataset, which includes both in- Gemma 1.1 (Team et al., 2024), command-r, Per-
structional content and a comprehensive set of sianLLaMA (Abbasi et al., 2023), PersianMind
questions and answers, we applied the LoRA (Rostami et al., 2024b), and PsychoLexLLaMA.
technique to further train the pre-trained models. Our focus on open-source models was intended to
This phase was pivotal in refining the models' enhance the accessibility and reproducibility of
abilities to interpret and respond accurately to in- our findings. The generation configuration for all
tricate psychological queries and scenarios within the LLMs evaluated is consistent across the exper-
the dataset. The supervised fine-tuning for the 8B iments and is detailed in Table 2.
model took 22 minutes using a single A100 GPU,
while the 70B model required 32 minutes on two Max new Do sam-
Temp tokens top p ple
A100 GPUs. The LoRA training configurations
0.01 16 0.9 True
used during this phase were the same as those in
the continuous pre-training.
Table 2: Generation configurations for all evaluated
LLMs.
4.3 Linear Weight Combination
To bolster the final model’s robustness and pre-
5.1 Zero-shot Setting
serve the integrity of previous training advances,
In the zero-shot setting, m odels were tested with-
we implemented a linear weight combination
out any prior contextual examples, relying solely
strategy. This involved merging the weights of the
on their pre-existing knowledge. This setting eval-
LLaMA 3.1 Instruct model with our continuously
uated the models' intrinsic ability to generate ac-
pre-trained and finely-tuned models. Each model
curate responses based solely on their training.
contributed 50% of its weight to the final compo-
site. This method synergistically combined the
5.2 One-shot Setting
foundational capabilities of LLaMA with our
The one-shot setting involved presenting each
model with a single relevant example before it an-
swered a question. This setting was used to assess
5the impact of a minimal context on the accuracy the influence of model architecture and parameter
of the models, providing insights into their ability size on handling specialized tasks, such as inter-
to leverage new information quickly preting and responding to psychology-related
questions.
5.3 Five-shot Setting
6.1.1 Performance Trends Across Models
In the five-shot setting, models were given five re-
lated examples before responding to questions. The data reveal substantial variability in perfor-
This scenario tested the models' capacity to utilize mance across models and settings. For instance, the
more extensive contextual information to enhance Llama-3.1 Instruct with 70B parameters exhibits
their accuracy, offering a deeper understanding of superior performance in all scenarios, suggesting a
their learning capabilities. positive correlation between larger parameter sizes
and enhanced comprehension and response accu-
5.4 Evaluation Metric
racy. This trend is consistent in the English data,
The effectiveness of each model across the zero- where models with larger parameters, such as
shot, one-shot, and five-shot settings was meas- Llama-3.1 Instruct 70B, also demonstrate robust
ured using accuracy as the primary metric. Accu- performance, especially in zero-shot and five-shot
racy was defined as the proportion of correct an- settings.
swers provided by the models relative to the total Conversely, models with fewer parameters
number of questions posed. This rigorous evalua- sometimes perform well in lower-shot settings but
tion approach allowed us to discern the strengths typically exhibit decreased performance as the
and weaknesses of each model in processing and complexity of tasks increases. For example, the
understanding psychological content comprehen- Qwen2 Instruct with 7B parameters faces greater
sively. challenges in the Persian context than in English,
Through these methodical evaluations, we potentially indicating linguistic or dataset-specific
aimed to illustrate the varying capabilities of each hurdles that are more effectively managed by larger
model under different contextual conditions. This models.
analysis not only sheds light on how models adapt
6.1.2 Language-Specific Observations
to incremental information but also highlights
their potential applicability in psychological set- Our evaluation underscores distinct language-spe-
tings, where understanding nuanced human be- cific differences. In Persian, the increase in model
havior is crucial. accuracy from zero to five shots is more marked,
indicating that Persian language models signifi-
6 Results cantly benefit from added context. Conversely,
English language models tend to have higher
This section outlines the outcomes of our evalua-
baseline performances, likely reflecting the ad-
tion of selected large language models (LLMs) us-
vantages of more extensive pre-training datasets
ing the PsychoLexEval dataset in both Persian and
available in English.
English. The primary focus was on assessing the
models' proficiency in understanding and re- 6.1.3 Impact of Training and Fine-Tuning
sponding to psychological questions.
The results particularly underscore the critical im-
Tables 3 and 4 illustrate the accuracy results of
portance of targeted training and fine-tuning, as
the models on the PsychoLexEval dataset for Per-
seen with the PsychoLexLLaMA models. De-
sian and English, respectively. These tables quan-
signed to surpass its predecessor, Llama 3.1, the
tify how effectively each model comprehends and
70B PsychoLexLLaMA occasionally does not
addresses psychology-related questions across
reach its ambitious targets but consistently
languages.
matches or exceeds the performance of the origi-
6.1 Discussion nal Llama 3.1 model. This consistency indicates
that while specific enhancements did not univer-
The results from Tables 3 and 4 provide signifi-
sally lead to improvements, they significantly bol-
cant insights into the performance of various
stered the model's capabilities. The 70B version,
LLMs, showcasing their competencies in both
Persian English. Notably, these findings highlight
6Accuracy
Model # Param 0-shot 1-shot 5-shot Avg
Qwen2 Instruct 7B 03.55 06.18 08.63 6.12
Gemma 1.1 it 7B 43.07 40.68 27.57 37.11
PersianMind 7B 35.78 35.96 24.63 32.12
Aya-23 8B 39.64 41.42 27.02 36.03
Llama-3 Instruct 8B 33.88 10.66 34.49 26.34
Llama-3.1 Instruct 8B 45.89 41.36 35.78 41.01
PsychoLexLLaMA-pretrain-sft 8B 47.30 43.13 46.61 45.68
PsychoLexLLaMA-average 8B 48.52 41.97 47.05 45.85
PersianLLaMA 13B 20.13 18.52 19.89 19.51
Aya-23 35B 21.07 10.47 22.69 18.08
c4ai-command-r-v01 35B 35.96 21.75 46.20 34.64
Llama-3 Instruct 70B 19.54 09.31 0.5 9.78
Llama-3.1 Instruct 70B 70.34 67.83 70.40 69.52
PsychoLexLLaMA-pretrain-sft 70B 67.79 45.34 68.07 60.4
PsychoLexLLaMA-average 70B 65.84 53.06 69.66 62.85
Qwen2 Instruct 72B 31.37 05.82 50.3 29.16
Table 3 : Accuracy of LLMs on the PsychoLexEval dataset in Persian.
Accuracy
Model # Param 0-shot 1-shot 5-shot Avg
Qwen2 Instruct 7B 89.31 42.74 83.76 71.94
Gemma 1.1 it 7B 84.75 55.06 65.86 68.56
Aya-23 8B 73.62 33.80 77.05 61.49
Llama-3 Instruct 8B 85.77 78.57 68.22 77.52
Llama-3.1 Instruct 8B 88.97 89.25 87 88.41
PsychoLexLLaMA-pretrain-sft 8B 88.97 81.21 62.03 77.4
PsychoLexLLaMA-average 8B 90.10 89.03 90.04 89.72
Aya-23 35B 81.32 79.02 82 80.78
c4ai-command-r-v01 35B 87 78.06 75.08 80.05
Llama-3 Instruct 70B 90.55 88.58 76.77 85.3
Llama-3.1 Instruct 70B 93. 02 92. 63 92.1 92.58
PsychoLexLLaMA-pretrain-sft 70B 91.45 90.24 90.85 90.85
PsychoLexLLaMA-average 70B 92.13 91.85 91.87 91.95
Qwen2 Instruct 72B 91.11 73.79 92.29 85.73
Table 4 : Accuracy of LLMs on the PsychoLexEval dataset in English.
with its vast parameter count, possesses the capac- suggesting that precise, domain-specific fine-tun-
ity to acquire a broader knowledge base, making ing can yield remarkable effectiveness, even with
it challenging to add new knowledge without for- fewer parameters. This success highlights the po-
getting previously learned information. Conse- tential of smaller models, particularly when
qu ently, fine-tuning such a large model demands equipped with tailored enhancements for specific
considerably more data to achieve better out- applications like psychological evaluations.
comes due to its complexity. The varying impacts of scaling between the 8B
In contrast, the 8B version of Psycho- and 70B versions suggest that while larger models
LexLLaMA often outperforms larger models, possess a broad knowledge base enhancing their
7general performance, strategic fine-tuning is cru- due to the selection of textual materials and ques-
cial for maximizing efficacy in specialized do- tion design. These biases may restrict the general-
mains. This observation encourages further re- izability of our results to broader psychological
search into training strategies that optimize both contexts and populations. Additionally, the reli-
large and small models for specific tasks, ensuring ance on freely licensed sources limits the diversity
that they not only retain previous knowledge but and depth of psychological topics explored, omit-
also effectively integrate new information. ting valuable content protected by copyright laws.
Another significant constraint is the depend-
7 Conclusion ence on sophisticated hardware for model train-
ing. The need for high-performance GPUs poses
This study has significantly advanced our under-
a considerable barrier, particularly for researchers
standing of how large language models (LLMs)
with limited access to such resources, affecting
can be effectively tailored for applications within
both the replicability of our results and the broader
psychology. Through the integration of special-
research community’s ability to engage with and
ized psychological content, the development of
expand upon our work. Moreover, while this
the PsychoLexQA and PsychoLexEval datasets,
study aims to enhance model performance with
and the creation of the PsychoLexLLaMA model,
minimal data and hardware resources, achieving
we have demonstrated the substantial benefits of
optimal efficiency under these constraints remains
targeted model training and fine-tuning.
a challenge. Balancing resource conservation with
Our findings indicate that specific pretraining
model capability often requires compromises that
and fine-tuning strategies substantially enhance
may detract from the models' utility in practical
the performance of LLMs in psychological set-
psychological applications.
tings, underscoring the critical role of thoughtful
By addressing these limitations, future research
model architecture and training approaches. Nota-
can focus on broadening the diversity of training
bly, while larger models typically show strong
data and developing more resource-efficient mod-
performance, our results reveal that even smaller
eling techniques, thereby enhancing the practical
models can achieve exceptional outcomes when
deployment of LLMs in psychology and related
subjected to precise, domain-specific adjust-
fields.
ments. This suggests a scalable potential for
LLMs in psychological applications that can be References
adapted to different contexts and constraints.
Abaskohi, A., Baruni, S., Masoudi, M., Abbasi,
In conclusion, this research not only sheds light
N., Babalou, M. H., Edalat, A., Kamahi,
on the current capabilities and challenges of using
S., Sani, S. M., Naghavian, N.,
LLMs in psychology but also sets a foundation for
Namazifard, D., Sadeghi, P., &
future work. It encourages ongoing refinement of
Yaghoobzadeh, Y. (2024).
these models to improve their relevance and accu-
Benchmarking Large Language Models
racy, thereby enhancing their utility in real-world
for Persian: A Preliminary Study
psychological applications. Moving forward, we Focusing on ChatGPT. ArXiv,
anticipate that continued advancements in model abs/2404.02403.
training methodologies and evaluation strategies Abbasi, M. A., Ghafouri, A., Firouzmandi, M.,
will drive significant progress in the field, making Naderi, H., & Minaei-Bidgoli, B. (2023).
LLMs an indispensable tool in the arsenal of psy- PersianLLaMA: Towards Building First
chological research and practice. Persian Large Language Model. ArXiv,
abs/2312.15713.
Limitations Abdin, M., Jacobs, S. A., Awan, A. A., Aneja, J.,
Awadallah, A., Awadalla, H., Bach, N.,
This research has demonstrated the potential of Bahree, A., Bakhtiari, A., & Behl, H.
customizing large language models (LLMs) for (2024). Phi-3 technical report: A highly
psychological applications. However, it is crucial capable language model locally on your
to recognize several limitations that could impact phone. arXiv preprint arXiv:2404.14219.
the scope and applicability of our findings. The Agrawal, S. (2023). Are LLMs the Master of All
PsychoLexQA and PsychoLexEval datasets, fun- Trades? : Exploring Domain-Agnostic
damental to our study, inherently contain biases
8Reasoning Skills of LLMs. ArXiv, Services with AI-based Large Language
abs/2303.12810. Models. ArXiv, abs/2307.11991.
Anthropic. (2024, Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu,
M. A., Socher, R., Amatriain, X., & Gao,
). The Claude 3 Model Family: Opus, Sonnet,
J. (2024). Large Language Models: A
Haiku.
Survey. ArXiv, abs/2402.06196.
Aryabumi, V., Dang, J., Talupuru, D., Dash, S.,
Muennighoff, N., Wang, T., Sutawika, L.,
Cairuz, D., Lin, H., Venkitesh, B., Smith,
Roberts, A., Biderman, S., Scao, T. L.,
M., Marchisio, K., & Ruder, S. (2024).
Bari, M. S., Shen, S., Yong, Z.-X.,
Aya 23: Open weight releases to further
Schoelkopf, H., Tang, X., Radev, D. R.,
multilingual progress. arXiv preprint
Aji, A. F., Almubarak, K., Albanie, S.,
arXiv:2405.15032.
Alyafeai, Z., Webson, A., Raff, E., &
Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-
Raffel, C. (2023). Crosslingual
Dahle, A., Letman, A., Mathur, A.,
Generalization through Multitask
Schelten, A., Yang, A., Fan, A., Goyal,
Finetuning. Annual Meeting of the
A., Hartshorn, A., Yang, A., Mitra, A.,
Association for Computational
Sravankumar, A., Korenev, A., Hinsvark,
Linguistics,
A., Rao, A., Zhang, A., . . . Zhao, Z.
OpenAI. (2023a). Gpt-3.5.
(2024). The Llama 3 Herd of Models.
https://www.openai.com/. Accessed:
Ghahroodi, O., Nouri, M., Sanian, M. V., Sahebi,
2023-06-13.
A., Dastgheib, D., Asgari, E., Baghshah,
OpenAI. (2023b). Gpt-4 technical report.
M. S., & Rohban, M. H. (2024).
https://arxiv.org/abs/2303.08774
Khayyam Challenge (PersianMMLU): Is
Rostami, P., Salemi, A., & Dousti, M. J. (2024).
Your LLM Truly Wise to The Persian
PersianMind: A Cross-Lingual Persian-
Language? ArXiv, abs/2404.06644.
English Large Language Model. ArXiv,
Guo, Z., Jin, R., Liu, C., Huang, Y., Shi, D.,
abs/2401.06466.
Supryadi, Yu, L., Liu, Y., Li, J., Xiong,
Shliazhko, O., Fenogenova, A., Tikhonova, M.,
B., & Xiong, D. (2023). Evaluating Large
Mikhailov, V., Kozlova, A., & Shavrina,
Language Models: A Comprehensive
T. (2022). mGPT: Few-Shot Learners Go
Survey. ArXiv, abs/2310.19736.
Multilingual. Transactions of the
Hu, J. E., Shen, Y., Wallis, P., Allen-Zhu, Z., Li,
Association for Computational
Y., Wang, S., & Chen, W. (2021). LoRA:
Linguistics, 12, 58-79.
Low-Rank Adaptation of Large
Team, G., Mesnard, T., Hardin, C., Dadashi, R.,
Language Models. ArXiv,
Bhupatiraju, S., Pathak, S., Sifre, L.,
abs/2106.09685.
Rivière, M., Kale, M. S., & Love, J.
Ke, L., Tong, S., Cheng, P., & Peng, K. (2024).
(2024). Gemma: Open models based on
Exploring the Frontiers of LLMs in
gemini research and technology. arXiv
Psychological Applications: A
preprint arXiv:2403.08295.
Comprehensive Review. ArXiv,
Ustun, A., Aryabumi, V., Yong, Z.-X., Ko, W.-
abs/2401.01519.
Y., D'souza, D., Onilude, G., Bhandari,
Khashabi, D., Cohan, A., Shakeri, S., Hosseini, P.,
N., Singh, S., Ooi, H.-L., Kayid, A.,
Pezeshkpour, P., Alikhani, M.,
Vargus, F., Blunsom, P., Longpre, S.,
Aminnaseri, M., Bitaab, M., Brahman, F.,
Muennighoff, N., Fadaee, M., Kreutzer,
Ghazarian, S., Gheini, M., Kabiri, A.,
J., & Hooker, S. (2024). Aya Model: An
Mahabadi, R. K., Memarrast, O.,
Instruction Finetuned Open-Access
Mosallanezhad, A., Noury, E., Raji, S.,
Multilingual Language Model. ArXiv,
Rasooli, M. S., Sadeghi, S., . . .
abs/2402.07827.
Yaghoobzadeh, Y. (2020). ParsiNLU: A
Wu, Q., Bansal, G., Zhang, J., Wu, Y., Li, B., Zhu,
Suite of Language Understanding
E., Jiang, L., Zhang, X., Zhang, S., Liu,
Challenges for Persian. Transactions of
J., Awadallah, A. H., White, R. W.,
the Association for Computational
Burger, D., & Wang, C. (2023).
Linguistics, 9, 1147-1162.
AutoGen: Enabling Next-Gen LLM
Lai, T., Shi, Y., Du, Z., Wu, J., Fu, K., Dou, Y., &
Applications via Multi-Agent
Wang, Z. (2023). Psy-LLM: Scaling up
Conversation.
Global Mental Health Psychological
9Yang, A., Yang, B., Hui, B., Zheng, B., Yu, B.,
Zhou, C., Li, C., Li, C., Liu, D., & Huang,
F. (2024). Qwen2 technical report. arXiv
preprint arXiv:2407.10671.
Zhou, D.-W., Sun, H.-L., Ning, J., Ye, H.-J., &
Zhan, D.-c. (2024). Continual Learning
with Pre-Trained Models: A Survey.
ArXiv, abs/2401.16386.
10A Dataset Samples
Which strategy is NOT considered a form of
Figures 1 shows an example of PsychoLexQA da-
problem-focused coping?
taset based on psychology document. Figure 2 il-
lustrates an example of PsychoLexQA dataset
1) Defining the problem
which has been created using a self-instruct
2) Seeking emotional support
method. Figure 3 shows an example of the Psycho-
3) Generating alternative solutions
LexEval dataset that includes a multiple-choice
4) Changing personal goals
question and answer. Figure 4 depicts the process
involved in constructing the PsychoLexLLaMA.
Correct Answer is 2.
Question: How do researchers measure brain
Figure 3: Example of PsychoLexEval dataset.
activity during sleep and meditation?
Answer: Researchers measure brain activity
during sleep and meditation using neuroimag-
ing techniques such as electroencephalograms
(EEGs), magnetic resonance imagery (MRI),
and positron emission tomography (PET).
These techniques record electrical changes in Fi gure 4: Process of constructing PsychoLexLLaMA
the brain, brain wave patterns, and activation model.
of specific brain areas, allowing researchers to
observe and analyze different states of con-
sciousness.
Figure 1: Example of PsychoLexQA dataset (Docu-
ment-based).
Instruct: Design an experiment to investigate
the effects of different types of feedback (posi-
tive and negative) on learning outcomes. In-
c lude methods for measuring student perfor-
mance.
Output: The experiment will involve three
groups of students: one receiving only positive
feedback, another receiving only negative feed-
back, and a third receiving no feedback at all.
Each group will engage in a learning task, such
as a math problem-solving activity, with their
performance assessed through test scores and
self-reported motivation levels. The hypothesis
posits that students receiving positive feedback
will outperform their peers in both task perfor-
mance and motivation. This experiment will of-
fer insights into the impact of feedback on
learning and educational practices.
Figure 2: Example of PsychoLexQA dataset (Self-
instruct).
11