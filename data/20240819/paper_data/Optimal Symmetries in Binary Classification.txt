Optimal Symmetries in Binary Classification
VishalS.Ngairangbam, MichaelSpannowsky
DepartmentofPhysics,DurhamUniversity
DurhamDH13LE,UnitedKingdom
{vishal.s.ngairangbam,michael.spannowsky}@durham.ac.uk
Abstract
Weexploretheroleofgroupsymmetriesinbinaryclassificationtasks,presenting
a novel framework that leverages the principles of Neyman-Pearson optimality.
Contrarytothecommonintuitionthatlargersymmetrygroupsleadtoimproved
classificationperformance,ourfindingsshowthatselectingtheappropriategroup
symmetries is crucial for optimising generalisation and sample efficiency. We
developatheoreticalfoundationfordesigninggroupequivariantneuralnetworks
thatalignthechoiceofsymmetrieswiththeunderlyingprobabilitydistributionsof
thedata. Ourapproachprovidesaunifiedmethodologyforimprovingclassification
accuracyacrossabroadrangeofapplicationsbycarefullytailoringthesymmetry
group to the specific characteristics of the problem. Theoretical analysis and
experimental results demonstrate that optimal classification performance is not
alwaysassociatedwiththelargestequivariantgroupspossibleinthedomain,even
whenthelikelihoodratioisinvariantunderoneofitspropersubgroups,butrather
withthosesubgroupsthemselves.Thisworkoffersinsightsandpracticalguidelines
forconstructingmoreeffectivegroupequivariantarchitecturesindiversemachine-
learningcontexts.
1 Introduction
Equivariantneuralnetworks,whichleveragesymmetriesinherentindata,haveshownsignificant
promise in enhancing classification accuracy, data efficiency, and convergence speed. However,
selecting an appropriate group and defining its actions across the various layers of the network
remainsacomplextask, particularlyforapplicationsrequiringadherencetospecificsymmetries.
Thisresearchaimstoestablishafoundationalframeworkfordesigningequivariantneuralnetwork
architecturesbyutilisingstabilisergroups.
In the context of equivariant function approximation, a critical insight is that the preimage of a
targetelementintheoutputspacecanbecharacterisedbythesubsetoftheorbitofaninputelement,
restrictedtothoseconnectedbyelementsofthestabilisergroupofthetarget. Thisunderstanding
simplifiesthechoiceofgroupG anditsactions,aligningthemwiththeequivalenceclassofthetarget
function’sfibresintheinputspace.
Thispaperprovidesatheoreticalbasisfordesigningequivariantarchitecturesinscientificapplica-
tions where known symmetries play a crucial role. By grounding the architecture design in first
principles,weaimtoexplainempiricalsuccessesobservedindata-drivenapplicationsandenhance
theeffectivenessofthesearchitecturesinpracticalscenarios.
Byquantifyingtheimpactofequivariantarchitecturesonsampleefficiencyandgeneralisationerror,
weofferacomprehensiveframeworkthatnotonlyaidsintheoreticalunderstandingbutalsoprovides
practicalguidelinesforimplementingthesearchitecturesinreal-worldscientificproblems. Through
simplifiedexamplesandexperimentalresults,wedemonstratetheapplicabilityandeffectivenessof
ourproposedmethods.
Preprint.Underreview.
4202
guA
61
]GL.sc[
1v32880.8042:viXraOurcontribution: Inthiswork,weidentifythatgroupinvariancesoftheunderlyingprobability
distributiondrivethegeneralisationpowerofinvariantandequivariantclassification. Althoughthe
resultisasimpleextensionofthewell-knownNeymanPearsonlemma[1]whereanoptimalbinary
classifiershouldhavethesamefibredecompositioninthedomainofthetwoprobabilitiesasthe
likelihoodratio,itprovidesastrongbasisformaximallyutilisingdomain-knowledgeinsimulation-
basedinference,whichisquicklygainingpopularityinvariousdisciplines. Utilisingtheobservation
thataG-equivariantfunctionH=f(X)naturallystructuresthefibresf−1(H),tobeatleastaslarge
astheelementsoftheorbitcontainingXwhichareconnectedbygroupelementsofthestabiliser
groupofHinthefunction’simage,weidentifythefollowing:
• largergroupequivariancedoesnotalwaysleadtobettersampleefficiencyorgeneralisation
power,evenwhenthelikelihoodratioisinvariantundersomesubgroupG′. Thisisbecause
aG-equivariantfunctionisG′-invariantonlywhenthestabilisergroupofeveryelementin
theimageofthefunctionisG′whichistrueonlywhentheactionA istrivialforallgroup
H
elementsinG′.
• ifthelikelihoodratioisnotinvariantunderanypropersubgroupactionofagivengroupG,
aG-equivariantfunctioncanmaintainoptimalitywhenitsactiononIm(f)isfree. However,
suchanactionisessentiallyequaltonotassuminganygroupequivariantarchitecturesince
the stabiliser groups of a free action is always trivial (only the unit element), i.e. group
equivariancedoesnotputadditionalconstraintsontheminimalfibresofthefunction
• ifthelikelihoodratioisnotinvariantunderaG-actionbutundersomeofitssubgroups,the
spaceofcontinuousG-invariantfunctionsdonotcontainanarbitrarilycloseapproximation
toamonotonicfunctionofthelikelihoodratio. ThisisbecausealargerG-actionmixes
the orbits of any of its proper subgroup actions in the same domain, while the fibres of
G-invariantfunctionsareatleastaslargeasthecorrespondingorbits.
2 Relatedworks
GroupEquivariantNetworks: Groupequivariantarchitecturedesignisubiquitousinmodern
deeplearningalgorithmswithConvolutionalNeuralNetworks[2]beingequivarianttotranslations.
Moderngeneralisationsincludeequivariancetodiscretegroups[3],andgaugesymmetries[4]. Itwas
shownin[5]thatequivariancewithrespecttoanycompactgrouprequirestheoperationofalayerto
beequivalenttoageneralisedconvolutiononthegroupelements.
Forset-basedapproaches,theDeepSetsframework[6]formulatedtheuniversalstructureofpermuta-
tionequivariantandinvariantfunctionsonelementsbelongingtoacountableuniverse. Equivariant
feature extraction on point clouds for the classifical groups has also been explored in [7, 8, 9].
GeometricAlgebraTransformer[10,11]havebeendesignedfor3Dequivariantextractionofvari-
ousnon-trivialtransformationgroups,includingtheconformalandprojectivegroups[11]utilising
Cliffordgroupequivariantnetworks[12].
Theoreticalanalysisofequivariantmodels: Ontopoftheempiricalresults,severaltheoretical
resultsexistontheadvantagesofequivariantmodels.Undertheassumptionthattheclassificationtask
isinvarianttosymmetrytransformation,boundsonthegeneralisationerrorwerederivedin[13]. The
samplecomplexityforinvariantkernelmethodsisstudiedin[14]. IntheframeworkofPAClearning,
itwasshown[15]thatlearningwithinvariantorequivariantmethodsreducestolearningoverorbit
representatives. At the same time, the incorporation of appropriate group equivariances [16, 17]
resultsinastrictincreaseingeneralisation. Thegeneralintuitionfromtheseworksisthatlarger
groupsresultinbettersampleefficiencyandimprovedgeneralisationwhenthetargetfunctionfollows
theassumedsymmetry. Ourworkcontributesinthisdirectionbydefiningthenotionofacorrect
symmetryforabinaryclassificationtaskwithaprioriknownsymmetriesoftheprobabilities.
Equivarianceinfundamentalphysics: Theimportanceofequivarianceinfundamentalphysics
cannotbeoverstated,startingfromtheGalileantransformationsinnon-relativisticregimetotheexotic
structureofgaugetheories,whichformthemathematicalbackboneofourcurrentunderstandingofthe
universe. Lorentzgroupequivariancehasbeenexploredin[18,19,20,21]. Othergroupequivariance
studiesincludearchitecturesforsimulatingQCDonthelattice[22],andtheclassificationofradio
galaxies[23].
23 Preliminaries
Fibresofafunction: Ourworkreliesonstudyingtheminimalfibrestructureinducedbysymmetries
onthelikelihoodratioanditsrelationtodifferentgroupequivariantfunctions. Afibreisformally
definedas:
Definition1. Givenafunctionf :X →Y,thefibreofanelementyintheimageoff denotedas
f−1(y),isthesetofallelementsofX whichgetsmappedtoyviaf:
f−1(y)={x∈X : f(x)=y} .
ThefibresofallelementinIm(f)divideX intoequivalenceclasses. Thismeansthatthesetoffibres
ofanyfunctionpartitionsthedomainD intomutuallyexclusivesubsets. Thisholdstrueforany
equivalenceclassinanygivenset,i.e. anyelementx∈X canonlybelongtoasingleequivalence
class. Weutilisethisbasicfactimplicitlyindifferentpartsofthework.
Groupactions: Themainideaunderlyingsymmetriesistheactionofgroupsonanunderlyingset.
Groupactionsandsomerelatedterminologiesaredefinedasfollows.
Definition2. GivenagroupG andasetX,theleft-actionofG onX isamapA:G×X →X with
thefollowingproperties:
1. A(e,X)=Xfortheidentityelemente∈G andanyX∈X,and
2. A(g ,A(g ,X))=A(g g ,X)forallg ,g ∈G andX∈X.
1 2 1 2 1 2
A G-action will be denoted as (G,A(g,X)). Any set always admits the trivial action where
A(g,X) = X for any group element g and all elements X of the set X. Consequently, even
forthesamegroupG,onecanhavetwoactionsA (g,X)andA (g,X)actingonthesamesetX,
1 2
whicharenotnecessarilythesamefunctions. ThismotivatesdefiningtheequalityoftwoG-actions
asfollows.
Definition3. TwoG-actions(G ,A (g,X))and(G ,A (g,X))actingonthesamesetX ∋Xare
1 1 2 2
equal if and only if G is isomorphic to G , via an invertible map i : G → G and A (g,X) =
1 2 1 2 1
A (i(g),X)forallg ∈G andX∈X.
2 1
Definition4. TheorbitofanelementX∈X underthegroupactionA(g,X)ofthegroupG,isthe
setofallelementsX′ ∈X forwhichthereisagroupelementg ∈G suchthatX′ =A(g,X).
WewillwritetheorbitofaG-actioncharacterisedbyavectorofgroupinvariantsIas
ΩX(I)={X′ =A(g,X):{X,X′}⊂X andg ∈G} . (1)
ThesetofalldistinctorbitsdividesX intoequivalenceclasses.
Definition5. ThestabilisergroupofanelementX∈X withrespecttotheG-actionA,isthesetof
allgroupelementsgwhichleaveXinvariant. Mathematically,wewilldenoteitas
LA(X)={g ∈G :X=A(g,X)}⊆G
Agrouprepresentationρ:G →GL(m,R),naturallyinducesalinearactionA :G×Rm →Rm
ρ
onRmoftheformA (g,x)=ρ(g)x. Intherestofthepaper,wewillbeprimarilyconcernedwith
ρ
suchactionsofagiventransformationgrouponRm. Wewillworkundertherelaxationthatthese
actionsneednotbeclosedinthedomainofthetargetfunctionD ⊂ Rm,i.e. theremaybegroup
elementsg ∈ G whichtakesapointx ∈ D toρ(g)x = x′ ∈/ D. Forapedagogicaloverviewof
commontransformationgroupssee[24].
Groupequivariantfunctions: Symmetriesarebuiltintoneuralnetworksusinggroupequivariant
maps,definedasfollows.
Definition6. Afunctionf :X →HbetweentwospacesX andHwhichbothadmitgroupactions,
sayA andA respectively,foragroupG issaidtobeG-equivariantiff commuteswiththegroup
X H
actions,i.e.,
f(A (g,X))=A (g,f(X)) , (2)
X H
foreveryg ∈GandX∈X.Themapf isG-invariantiftheactionA istrivial,i.e.,A (g,H)=H
H H
forallg ∈G andanyH∈H.
3ItisstraightforwardtoseethataG-equivariantmapH = f(X),isLAH(H)-invariantwhichmay
howeverbedifferentgroupsfordifferentvaluesofH. Aswewillbeusingthenotionofuniversal
approximationinthespaceofG-equivariantfunctions,weoutlinethefollowinggeneraldefinitionof
theuniversalapproximationpropertyforafunctionspaceinthedomainDtoR.
Definition7. AclassofparametrisedmodelsΣ (D)issaidtobeauniversalapproximatorinthe
θ
functionspaceC(D)iftheclosureofthesetΣ (D)inC(D)isC(D).
θ
While the above definition subsumes definitions based on metrics via the corresponding metric
topology,wewillconcentrateonthosecaseswherethedefinedmetricsinvolvetakingasupremum
inthedomainD. Thismeansthatweareprimarilyinterestedinsubsetsofthespaceofcontinuous
functions rather than any strictly larger space like the space of L integrable functions. Some
1
instancesoftheuniversalapproximationproperty(UAP)inthesetofG-equivariantfunctionscanbe
foundin[25,9,26]. ForgeneralUAPsseeforinstance [27,28]and[29,30]foramoreaccessible
introduction.
NotationofG-equivariantfunctionspaces: Welayoutsomenotationoffunctionspaces,whichwill
beusedinthefollowingsections. ThesetofallcontinuousG-equivariantfunctionsforparticular
actions A and A on the domain X and the codomain H, respectively, will be denoted as
X H
E (A ,A ,X,H). When the action A is trivial, the set of all G-invariant functions will be
G X H H
denotedbyI (A ,X,H). Wherethereisnoambiguityoftheactions,domainandcodomainofthe
G X
componentfunctions,wewillimplicitlywritethesesetsasE andI .
G G
Likelihoodratioandhypothesistesting: Wefirstoutlineanon-technicalandintuitivepictureof
theubiquitousNeyman-Pearsonlemma[1]whichwewillthenconnecttooptimalbinaryclassifica-
tion. ConsideranobserveddataXandwehypothesisethatitoriginatesfromeitherP (X,Θ )or
0 0
P (X,Θ ),whereΘ areparametersoftheprobabilitydistribution. Thosehypotheseswherethe
1 1 α
probabilitiesarecompletelyspecifiedaresaidtobesimple,i.e.,eachoftheparametersΘ arefixed
α
tospecificvaluesforα∈{0,1}. TheNeyman-Pearsonlemmaappliestoscenarioswherethetwo
hypothesesaresimpleandexhaustive,i.e.,thedatacanoriginatefromonlythesetwocompletely
specifiedprobabilities. ThenullhypothesisH ,whereXfollowsP ,isusuallychosentorepresent
0 0
thecurrentlyunderstoodphenomena,whilethealternatehypothesisH ,whereXfollowsP ,are
1 1
takentodescribepossiblephenomenainthenewregimeprobedbytheexperiment.
The power of a statistical test is the probability of correctly rejecting the null hypothesis when
the alternate hypothesis is true. At the same time, the significance is the probability of rejecting
the null hypothesis when it is true. The Neyman-Pearson lemma states that the likelihood ratio
λ(X) = P (X)/P (X)isuniformlythemostpowerfulteststatisticviawhichwecanacceptthat
1 0
X originated from P for a given significance. Notably, a binary classification problem with an
1
appropriatelossfunction(includingbinarycross-entropy)reducestoapproximatingamonotonic
functionofthelikelihoodratio[31].
Optimal binary classification: As we aim to connect binary classification with the Neyman-
Pearsonoptimalityofhypothesistests,wewillutiliseananalogousandstraightforwarddefinition
of optimality in binary classification following the standard formulation of the receiver operator
characteristicscurve. LetustakethebinaryclassificationofasampleXsampledfromeitherP (X)
0
orP (X)withthesamesupportD ⊂ Rm, whereweareinterestedinmaximallyselectingthose
1
samplesoriginatingfromP . Often,thesupportofthealternatehypothesis’probabilitydistribution
1
function(pdf)P iscontainedwithinthatofthenullhypothesisP . Ontheotherhand,ifthereis
1 0
somepartofP ’ssupportoutsideP ,anyeventobtainedinthisregiontriviallycanonlyfollowthe
1 0
alternatehypothesis. Weare,therefore,primarilyinterestedinthosenon-trivialcaseswherethedata
fallsintheintersectionofthesupportofP andP . Foragivenclassifierfˆ:D →Rwhichlearns
0 1
theunderlyingclassassignmentmapbysegregatingsamplesfromP tonegativevalues,dependent
1
onathresholdt∈Rthecumulativedistributionofyˆ=fˆ(X)underP are
α
(cid:90)
ϵ =C (t)= dV P (X)Θ(yˆ<t) ,
α α α
D
wheredV isthevolumeelementinD.Therefore,ϵ isthetruepositiverateandϵ isthefalsepositive
1 0
rateandtheformercanbecastasafunctionofthelaterusingthethresholdt,ϵ =C ◦C−1(ϵ )=
1 1 0 0
ROC(ϵ ). Wenowhavethefollowingdefinitionofanoptimalbinaryclassifier.
0
4Definition8. AbinaryclassifierfordatasampledfromtwodistributionsP (x)andP (x)isoptimal
0 1
inacceptingsamplesfromP atagiventoleranceϵ ofacceptingfalsesamplesfromP ,ifithasthe
1 0 0
highestpossibleacceptanceofsamplesϵ fromP .
1 1
TheconnectiontoNeyman-Pearson’soptimalityofthelikelihoodratiocanbeseenasfollows. For
eachX,ifthenullhypothesisH isthattheunderlyingdistributionisP andthealternatehypothesis
0 0
H isthatitisP ,itisstraightforwardtoseethatϵ isthesignificanceofthetestandϵ isitspower.
1 1 0 1
Therefore,wehavethefollowingobservation.
Observation1. Abinaryclassifierfˆ:D →RofdataX∈DsampledfromeitherP (X)orP (X)
0 1
isanoptimalclassifierforallvaluesofϵ ,ifandonlyifitisamonotonicfunctionofthelikelihood
0
ratioP (X)/P (X).
1 0
Therelaxationtoamonotonicfunctionofthelikelihoodratiofollowsfromtheintegraldefinitionof
ϵ dependingimplicitlyonthethreshold,i.e.,weareinterestedinchoosingregionsinDdividedby
α
hypersurfaceswhichhaveconstantlikelihoodratios. Forad-dimensionaldomain,theseareatleast
d−1dimensional,andtherefore,theyarecomputationallynon-trivialtoevaluateinhighdimensions
evenwhenonecanwritedownclosed-formexpressionsfortheprobabilities.Additionally,anecessary
conditionforanyfunctiontobetheoptimalclassifieristohaveanidenticalfibredecompositionin
thedomainDwiththelikelihoodratio.
4 Minimalfibresofgroupequivariantfunctions
Sinceanecessaryoptimalityconditioninbinaryclassificationistohavethesamefibredecomposition
inthedomainD,wenowdescribetheminimalfibresofgroupofequivariantfunctions.
4.1 Invariantfunctions
Leth:X →HbeaG-invariantfunction,i.e. foranyX∈X andallg ∈G,
h(A (g,X))=h(X) . (3)
X
ThismeansthatifX ∈ ΩX(I)thenforallX′ = A (g,X) ∈ ΩX(I),h(X′) = h(X). Therefore,
X
wehavethefollowingobservation:
Observation2. Thefibreofanelement H = h(X) ∈ H intheimageofaG-invariantfunction
h:X →HisatleastaslargeastheorbitΩX(I)ofX∈X oftheactionA . Ifthemapisequal
X
forelementsbelongingtodistinctorbits,thefibrebecomesenlargedtotheunionoftheseorbits.
Leth(X)becomeequalforXinallorbitsparametrizedbyIinthesetF oforbitinvariantsI. The
fibreh(X)foranyXintheseorbitsis
(cid:91)
h−1(X)= ΩX(I) . (4)
I∈F
4.2 Equivariantfunctions
Letf : X → HbeaG-equivariantfunctionwithrespecttotheactionsA andA onX andH,
X H
respectively. IfX′ =A (g,X)isintheorbitofX,fromEq2,wehavef(X′)=A (g,f(X)). It
X H
isstraightforwardtoseethat
g ∈LAH(f(X)) =⇒ f(X′)=f(X) , (5)
i.e.,ifXandX′ areconnectedbyagroupelementg whichbelongstothestabilisergroupofthe
actionA atf(X),thenX′iscontainedinthefibref−1(H)ofH=f(X). Therefore,similartothe
H
caseforinvariantmaps,wehavethefollowingobservation:
Observation3. ThefibreofanelementH=f(X)∈HintheimageofaG-equivariantfunction
f : X → H, isatleastaslargeasthesubsetoftheorbitofXconnectedbyanelementg inthe
stabilisergroupofA atH. IfthemapisequalforelementsbelongingtodistinctorbitsinX,the
H
fibrebecomesenlargedtotheunionofallsuchsets.
5DefinethesubsetoftheorbitΩX(I)dependentonH=f(X)as
O(I,H)={X′ =A (g,X):g ∈LAH(H)} . (6)
X
Iff(X)becomeequalforXinallsuchsubsetsofdistinctorbitsparametrizedbyIinthesubset
F ⊆I. Thefibrescanbewrittenas
(cid:91)
f−1(H)= O(I,H) . (7)
I∈F
Asexpected,theinvariantresultisaspecialcasewhentheactionA istrivial,i.e. thestabiliser
H
groupofanyelementH∈HisthegroupG andO(I,H)=ΩX(I)foranyH.
AswewillbeprimarilyconcernedwithprobabilitieswithsupportonasubsetD ⊂X whereX =Rm,
thefibresbecometheintersectionoftherespectivesetswiththedomainD. Atthispoint,wepoint
outthatforgeneralequivariantfunctionapproximation,acorrectsymmetry(whetherinvariantor
equivariant) is one which does not mix the fibres of a target function through the minimal fibres
inducedviagroupequivariance.
5 Optimalsymmetriesinbinaryclassification
Infundamentalapplications,moreoftenthannot,theprobabilitiesareinvariantundersometransfor-
mationgroupactiononthedomainD. Evenwhenclosed-formexpressionsarenotknown,various
first-principleargumentsrequiretheprobabilitiestobeinvariantunderatransformationgroup. In
thissection,weanswerthequestionofwhichsymmetriesretaintheoptimalityofaclassifierwhen
the symmetry of the two probabilities is known a priori. We ignore the effects of noise in our
mathematicaldescriptionwhileweobservethatthefindingspersistinthepresenceofnoiseinour
numericalexperiments. Havingobservedthestructureoffibresofgroupequivariantfunctionsand
theneedforanoptimalclassifiertohavethesamefibredecompositionasthelikelihoodratio,we
defineanoptimalsymmetryasfollows.
Definition 9. Two G-actions A and A are an optimal pair of symmetries for a given binary
X H
classification task if the space E (A ,A ,X,H) contains functions which have the same fibre
G X H
decompositionasthelikelihoodratio.
Whilethedefinitioncoverstheinvariantcase,i.e. whenA istrivial,wewillnotexplicitlymention
H
thetrivialactionwhendiscussingtheinvariantcase. Noticethatthedefinitiondoesnotimposethat
allfunctionsfollowthesamefibredecompositionasthelikelihoodratio. Thisisbecauseagroup
invariant likelihood ratio only constrains the fibres to be at least as large as the group orbits and
distinctorbitscanhavethesamevalueofprobabilitieswhereinthefibresbecomeenlargedtothe
unionofallsuchorbits. Ontheotherhand, ifthefunctionsinE bydefinitionmixesanyofthe
G
possiblefibreofthelikelihoodratioimposedbyitsgroupinvariancei.e. allfunctionsf ∈E havea
G
differentfibredecompositiontothelikelihood,itfollowsthatanyuniversalapproximatoronE will
G
besuboptimalfortheparticularbinaryclassification.
5.1 Groupinvariantclassifiers
Usingthelikelihoodratio,itisnowstraightforwardtoobtainthenecessaryandsufficientconditions
foroptimalactionsonthefeaturesXviaasimpleapplicationoftheNeyman-Pearsonoptimalityin
observation1.
Observation4. AG-actiononRmisanoptimalsymmetryforG-invariantbinaryclassificationofdata
sampledviaP (X)andP (X)forX∈Rm,ifandonlyifthelikelihoodratioλ(X)=P (X)/P (X)
0 1 1 0
isG-invariant.
Therefore,foranoptimallyinvariantG-actionA ,anyuniversalapproximatoronI (A ,X,H)
X G X
willinprinciplecontainanarbitrarilycloseapproximation. However,ourprimaryconcernisthe
effectivenessoffindingsuchanapproximation.
LetS ={(G ,A ),(G ,A ),...,(G ,A )}denotethesetofknownG-actionsunderwhichthe
α 1 1 2 2 kα kα
probabilityP isinvariant. Therefore,aG-invariantuniversalapproximatorΣG(D)for(G,A) ∈
α θ
S ∩S ,willbeabletoapproximatetheoptimalclassifier. Consequently,itisnotnecessarythatany
0 1
6transformationgroupinthedomainDwillbeabletoapproximatetheoptimalclassifier. Whilethis
isintuitivelyknowninthecommunity,withacommonexamplebeingtheinadequacyofreflection
invarianceinclassifyingsixandnineintheMNISTdataset,numericalexperimentsandtheoretical
results point to a better generalisation and sample efficiency of larger groups. Our discussions
regardingequivariantclassificationandnumericalresultsindicatethisisnotalwaystrueinbinary
classification.
Lettheactioninducedbytherepresentationρ(g)ofG beanoptimalgroupactionforsomebinary
classification problem. The restricted action of a proper subgroup G′, i.e., for group elements
g ∈ G′ ⊂ G, willalsobeanoptimalsymmetryforthesamebinaryclassificationscenario. Note
thatsinceweareworkinginarepresentationρ(g),therearepossiblyaninfinitenumberofwaysin
whichwecanrealiseasubgrouprestriction,andweareconcernedwithonlyoneoftheseatatime.
Forexample,forthepermutationgroupS
’srepresentationofnelements,thereare(cid:0)n(cid:1)
waysof
n m
choosingarestrictiontothesubgroupactionofS foragivenm,andweselectonlyoneoutofthese
m
possiblechoices. TheparentG-invariantfeatureextractionwillgenerallyhavethemaximumsample
efficiencycomparedtoitsdifferentsubgroupssinceitisevidentthattheirorbitswillbelargerin
general.
Letusnowconsiderthecasewheresomeparticularsubgrouprestrictionisanoptimalsymmetrywhile
theparentactionisnot.Thismeansthatwecanfindnon-emptysetsV ⊂Dwhereλ(ρ(g)X)̸=λ(X)
foratleastoneg ∈ G\G′. Ontheotherhand,foraG-invariantfunctionsayf : D → R,wehave
h = f(X) = f(ρ(g)X)forallg ∈ G. Therefore,thefibref−1(h)whichisatleastaslargeasthe
orbitoftheelementX,willnotcoincidewiththatofthelikelihoodratio,resultinginsuboptimal
classificationperformance. Whiletheamountofthisimplicitdistortionofthefibrestructurebythe
largergroupinvariancewilldependonthesizeofthesetV,itisstraightforwardtoseethatcontinuous
universalapproximationonI doesnotguaranteeanarbitrarilyclosefunctionapproximationto
G
monotonicfunctionsofthelikelihoodratiosincebydesignthefibredecompositionofanymember
functionofthedomainDwhichareatleastaslargeastheintersectionoforbitsofthegroupaction
withD,willnotbeequivalenttothatofthelikelihoodratio.
5.2 Groupequivariantclassifiers
Let us now consider the case of equivariant feature extraction for classifying data sampled from
invariantprobabilitiessincethereislimitedutilityindefiningequivarianceintheone-dimensional
spaceofprobabilityvalues. Forsuchacase,thefollowingconjecturecapturesthenecessaryand
sufficientconditionsonthegroupactionsinthedomainandtherangeofaG-equivariantfunction.
Conjecture 1. The G-actions A and A acting on Rm and a hidden representation space H,
X H
respectivelyisanoptimalsymmetricpairofG-actionsforG-equivariantbinaryfeatureextraction
fromdatasampledviaP (X)andP (X)forX ∈ Rm,ifandonlyifthelikelihoodratioλ(X) =
0 1
P (X)/P (X)isinvariantundertheactionA restrictedtoasubgroupG′andtheactionA acts
1 0 X H
triviallyforallg ∈G′.
Thesketchofapossibleproofisasfollows. Forsufficiency,weneedtoproofthattheactionA
H
beingtrivialandthelikelihoodratiobeinginvariantundertheactionA forallg ∈G′guarantees
X
thatthesmallestfibresdemandedbyG-equivariancenevermixesdifferentfibresofthelikelihood
ratio. AswehavealreadyseenthatthesmallestfibreofagroupequivariantfunctionatH=f(X)is
determinedbythesubsetoftheorbitofXinthedomainX whichisconnectedbyanelementgin
thestabilisergroupLAH(H). Therefore,itsufficestoshowthatthestabilisergroupofeveryelement
H∈Im(f)isG′. Clearly,thisissatisfiedsince,bydefinition,wehavechosenatrivialactionofthe
groupG′.
Fornecessity,weneedtoshowthatoptimalityofthepairofG-actionsimpliesthatthebehaviourof
theactionA isconfinedtobetrivialforanyg ∈G′andallH∈Im(f)forf :D →Handthatthe
H
likelihoodratioshouldbeG′-invariant. Clearly,ifthelikelihoodratioisnotG′-invariantunderall
possiblesubgrouprestrictionsinG exceptforthetrivialgroup,theonlypossibilitytohaveafunction
spaceE whichcontainsthefibredecompositionstructureofthelikelihoodratioisfortheactionA
G H
tobefreeinIm(f). However,thisisequivalenttoanon-equivariantfeatureextractionsinceforafree
actionallstabilisersaretrivialtherebyputtingnoconstraintsonthefunction’sfibresinD. Therefore,
forthenon-trivialcasethelikelihoodratioshouldatleastbeinvariantundersomepropersubgroup
G′.
7NowwhenthelikelihoodratioisG′-invariantunderA ,eveniftheunrestrictedactionofG may
X
takeitoutsidethesupportDofthetwodistributions,therestrictedactionisclosedinD. Therefore,
foranyelementg ∈G′andX∈D =⇒ X′ =A (g,X)∈D.
X
One can now prove the triviality of the action A for g ∈ G′ by contradiction. Suppose let
H
us assume that the action A (g,H) is non-trivial for at least one non-identity element g ∈ G′
H v
whenthelikelihoodratioisinvariantundertheG′ restrictedactionA . Thismeansthatforany
X
H∈Im(f),H′ =A (g ,H)̸=H. Inotherwords,whileanytwoelementsX′ andX∈Dsuch
H v
thatX′ =A (g ,X),willbelongtothefibreofthelikelihoodratio,thesamedoesnotholdtruefor
X v
fibresofthefunctionH=f(X)andH′ =f(X′). Therefore,anon-trivialactionA inducesafibre
H
decompositionwhichdiffersfromthefibrestructureofthelikelihoodratioinducedbyG′-invariance.
Evidently,forgroupequivariantfeatureextractionviaalargergroup,oneneedstomanuallystructure
the group actions in the hidden representation to be trivial for some subgroups under which the
likelihoodratioisinvariant. Withoutsuchamanualtweak,oneexpectslargergroupequivarianceto
notbeabletoapproximatetheoptimalclassifier. Whileourexperimentsfollowthisbehaviour,we
deferarigorousstatementtofutureworkastheproofcanbemorenuanced.
Twoimportantdifferencesfromtheinvariantcasearetherestrictiontoafeatureextractioninahidden
representationratherthanaclassificationandthedependenceofoptimalityontheactionA . The
H
firstisduetothepracticalutilityofequivariantfeatureextractionwithapossiblynon-equivariant
classificationnetwork,whilethelatterisclearlyduetotheimplicitdependenceofthedefinitionof
equivarianceontheactionA . Nevertheless,theinvariantobservationisaspecialcasepointing
H
towardsthemoregeneralconjecture.
6 Experiments
Duetotheprevalenceofrichersymmetriesin3Dthaninimages,wechoosepointcloudclassification
ofsimpleshapesusingrandomnumbergenerators. Forallexperiments,weadd3Dnormallydis-
tributednoiseofdiagonalcovariance0.3tothecartesiancoordinaterepresentation. Ourarchitecture
isbasedonthegroupequivariantstructureasprescribedin[9]fortheclassicalgroups. Weconsider
groupsthegroupsE(3),O(3)andO(2)withinputvectoractionsonR3,wheretheO(2)actionacts
alongthez-axiswiththemodifiedmetricsignature(1,1,0),intheevaluationofthenormandthe
innerproduct. Fromtheprobabilitiesspecifiedbelow,wesample30pointstoconstructadatasample
for input to the neural network. We consider two training datasets of 10k and 100k samples per
classtocomparethesampleefficiencyofthesemodelsandusebinarycrossentropylossforthe
optimisation. Additionaldetailsofthenetworkcanbefoundinthesupplementarymaterial. Forall
reportedresults,wetrainthesamenetworkfromrandominitialisationtentimesandcomputethe
relevantmeanandstandarddeviation.
Uniform: Wetakeasimpleexampleofclassifyingapointcloudofahollowcylinderandasphere
wherethepointsareuniformlysampledfromtheirembeddingin3D.Thesphereandthecylinder
haveaunitradius,andwealignthecylindertothez-axisintherange(−1,1). Duetotheadded
noise, this is a simple yet non-trivial point cloud binary classification scenario where the largest
symmetryoftheprobabilitydistributionforthesphereisO(3),andthatofthecylinderisO(2)with
theaxisfixedtothez-axis. Therefore,sinceO(2)isasubgroupofO(3),thelargestgroupaction
underwhichthelikelihoodisinvariantundertheO(2)actionalongthez-axis.
TruncatedNormal: Tounderstandthegeneralisationcapabilitiesinscenarioswherethesymmetry
liesintheprobabilityitselfandnottheirsupport,weconsideratruncatedballandacylinder. The
radiusfortheballandthecylinderfollowatruncatednormaldistributionintherange(4,6)centredat
fiveandunitstandarddeviation. Theazimuthalangleforbothcasesfollowsauniformdistributionin
therange(−π/4,π/4). Forthesphere,weuniformlysamplethepolarangleintherange(π/4,3π/4)
while for the cylinder, we sample z-coordinates uniformly in the range (5cos3π/4,5cosπ/4).
Therefore,withtheaddednoiseof0.3standarddeviationsincartesiancoordinaterepresentation,the
probabilitieshavethesamesupportonR3withthesameunderlying(approximate)symmetriesasthe
previouscase.
Results: The mean and standard deviation of the minimum validation loss over each training
instance for both scenarios is plotted in figure 1. For each of the best models per training, the
8Arch. Invariant Equivariant
10k 100k 10k 100k
E(3) 0.853±0.005 0.868±0.007 0.904±0.069 0.971±0.049
O(3) 0.864±0.013 0.905±0.002 0.994±0.005 0.999±0.000
O(2) 1.000±0.000 1.000±0.000 1.000±0.000 1.000±0.000
Table1:Themeanandstandarddeviationof1−ϵ (ϵ =0.95)overtentrainingrunsfortheuniformcylinder
0 1
vsuniformsphereclassification.
Arch. Invariant Equivariant
10k 100k 10k 100k
E(3) 0.331±0.042 0.652±0.010 0.913±0.163 0.982±0.036
O(3) 0.988±0.004 0.994±0.000 0.988±0.003 0.995±0.002
O(2) 0.999±0.000 0.999±0.000 0.998±0.002 0.999±0.000
Table2:Themeanandstandarddeviationof1−ϵ (ϵ =0.95)overtentrainingrunsforthetruncatednormal
0 1
distributionofacylindricallysymmetricpdfvsasphericalsymmetricone.
background rejection: 1−ϵ at 95% signal acceptance is evaluated on the test dataset, and we
0
showitsmeanandstandarddeviationintables1and2. Comparingthedifferentarchitectures,we
find that the largest group E(3), leads to the poorest validation error for either data size, with a
slightimprovementforO(3)butstilllowerthanthebest-performingO(2). Whilethedifferenceis
significantlyreducedfortheequivariantcase,thelargergroupsE(3)andO(3)havenotbeenableto
matchtheperformanceofO(2)equivariance,whichsuggeststhatlargergroupequivariancedoesnot
alwaysleadtobettergeneralisation.
Comparing the sample efficiency, we also see that the smallest group O(2), which is the correct
symmetry,haspracticallythesamemeanvalueoftheminimumlossat10kand100ksamplesper
class. Incontrast,thelargergroupsO(3)andE(3)havemuchbettervaluesat100k. Whileforthe
invariantcase,thisisnotsurprising,theunderlyingdifferenceintheequivariantcasecoupledwith
conjecture1, pointstowardthesuboptimalityoflargegroupequivarianceswhentheunderlying
likelihoodratioisinvariantonlyunderoneofitspropersubgroups.
7 Conclusions
In this work, we have presented a novel framework for optimising group symmetries in binary
classificationtasks,challengingtheprevailingassumptionthatlargersymmetrygroupsuniversally
leadtobetterperformance. Ourtheoreticalanalysisandexperimentalresultsdemonstratethatthe
optimalselectionofgroupsymmetries,alignedwiththeintrinsicpropertiesoftheunderlyingdata
distribution,iscriticalforenhancingbothgeneralisationandsampleefficiency.
Wedevelopedasystematicapproachtodesigninggroupequivariantneuralnetworks,whichcarefully
tailorsthechoiceofsymmetriestothespecificcharacteristicsoftheproblemathand. Experimentally,
weshowedthatwhilelargergroups,suchasE(3)andO(3),mayappeartooffermorecomprehensive
symmetryhandling,theydonotnecessarilyresultinbetterclassificationperformance. Infact,our
Uniform Truncated Normal
0.25 (10k , Inv.) (100k , Inv.) (10k , Inv.) (100k , Inv.)
(10k , Equi.) (100k , Equi.) (10k , Equi.) (100k , Equi.)
0.5
0.20
0.4
0.15
0.3
0.10
0.2
0.05 0.1
0.00 0.0
E(3) O(3) O(2) E(3) O(3) O(2)
Figure1: Thevalueofthemeanoftheminimumvalidationlossovertentraininginstancesforthedifferent
scenarios.
9
ssol
.laV
ssol
.laVfindingsrevealthatthecorrectsymmetrygroup,evenifsmaller,canleadtosignificantlyimproved
performanceintermsofbothvalidationlossandclassificationaccuracy.
Thepracticalimplicationsofourworksuggestthataone-size-fits-allapproachtosymmetrygroup
selectionissuboptimalingeneral. Instead,itmaybeimportanttocarefullyselectsymmetrygroups
basedon thedata’s intrinsicsymmetries, which canleadto moreefficient andeffectivemachine
learningmodels.
Futureworkwillfocusonextendingthisframeworktomorecomplexclassificationproblemsand
exploringtheutilityofourapproachinotherdomainswheresymmetriesplayacrucialrole,suchas
inphysics-basedsimulationsandhigh-dimensionaldataanalysis. Additionally,weaimtoinvestigate
furthertheimpactofnoiseandotherreal-worldfactorsontheperformanceofgroupequivariant
architecturestoenhancetheirrobustnessandapplicabilityinpracticalscenarios.
References
[1] J.Neyman,E.S.PearsonandK.Pearson,Ix.ontheproblemofthemostefficienttestsof
statisticalhypotheses,PhilosophicalTransactionsoftheRoyalSocietyofLondon.SeriesA,
ContainingPapersofaMathematicalorPhysicalCharacter231(1933)289
[https://royalsocietypublishing.org/doi/pdf/10.1098/rsta.1933.0009].
[2] Y.LeCun,B.Boser,J.S.Denker,D.Henderson,R.E.Howard,W.Hubbardetal.,
BackpropagationAppliedtoHandwrittenZipCodeRecognition,NeuralComputation1(1989)
541
[https://direct.mit.edu/neco/article-pdf/1/4/541/811941/neco.1989.1.4.541.pdf].
[3] T.CohenandM.Welling,Groupequivariantconvolutionalnetworks,inProceedingsofThe
33rdInternationalConferenceonMachineLearning,M.F.BalcanandK.Q.Weinberger,eds.,
vol.48ofProceedingsofMachineLearningResearch,(NewYork,NewYork,USA),
pp.2990–2999,PMLR,20–22Jun,2016,https://proceedings.mlr.press/v48/cohenc16.html.
[4] T.Cohen,M.Weiler,B.KicanaogluandM.Welling,Gaugeequivariantconvolutional
networksandtheicosahedralCNN,inProceedingsofthe36thInternationalConferenceon
MachineLearning,K.ChaudhuriandR.Salakhutdinov,eds.,vol.97ofProceedingsof
MachineLearningResearch,pp.1321–1330,PMLR,09–15Jun,2019,
https://proceedings.mlr.press/v97/cohen19d.html.
[5] R.KondorandS.Trivedi,Onthegeneralizationofequivarianceandconvolutioninneural
networkstotheactionofcompactgroups,inProceedingsofthe35thInternationalConference
onMachineLearning,J.DyandA.Krause,eds.,vol.80ofProceedingsofMachineLearning
Research,pp.2747–2755,PMLR,10–15Jul,2018,
https://proceedings.mlr.press/v80/kondor18a.html.
[6] M.Zaheer,S.Kottur,S.Ravanbakhsh,B.Poczos,R.R.SalakhutdinovandA.J.Smola,Deep
sets,inAdvancesinNeuralInformationProcessingSystems,I.Guyon,U.V.Luxburg,
S.Bengio,H.Wallach,R.Fergus,S.Vishwanathanetal.,eds.,vol.30,CurranAssociates,Inc.,
2017,
https://proceedings.neurips.cc/paper_files/paper/2017/file/f22e4747da1aa27e363d86d40ff442fe-
Paper.pdf.
[7] H.Chen,S.Liu,W.Chen,H.LiandR.Hill,Equivariantpointnetworkfor3dpointcloud
analysis,inProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),pp.14514–14523,June,2021.
[8] V.G.Satorras,E.HoogeboomandM.Welling,E(n)equivariantgraphneuralnetworks,in
Proceedingsofthe38thInternationalConferenceonMachineLearning,M.Meilaand
T.Zhang,eds.,vol.139ofProceedingsofMachineLearningResearch,pp.9323–9332,PMLR,
18–24Jul,2021,https://proceedings.mlr.press/v139/satorras21a.html.
[9] S.Villar,D.W.Hogg,K.Storey-Fisher,W.YaoandB.Blum-Smith,Scalarsareuniversal:
Equivariantmachinelearning,structuredlikeclassicalphysics,inAdvancesinNeural
InformationProcessingSystems,A.Beygelzimer,Y.Dauphin,P.LiangandJ.W.Vaughan,eds.,
2021,https://openreview.net/forum?id=ba27-RzNaIv.
[10] J.Brehmer,P.deHaan,S.BehrendsandT.S.Cohen,Geometricalgebratransformer,in
AdvancesinNeuralInformationProcessingSystems,A.Oh,T.Naumann,A.Globerson,
10K.Saenko,M.HardtandS.Levine,eds.,vol.36,pp.35472–35496,CurranAssociates,Inc.,
2023,
https://proceedings.neurips.cc/paper_files/paper/2023/file/6f6dd92b03ff9be7468a6104611c9187-
Paper-Conference.pdf.
[11] P.deHaan,T.CohenandJ.Brehmer,Euclidean,projective,conformal: Choosingageometric
algebraforequivarianttransformers,inProceedingsofThe27thInternationalConferenceon
ArtificialIntelligenceandStatistics,S.Dasgupta,S.MandtandY.Li,eds.,vol.238of
ProceedingsofMachineLearningResearch,pp.3088–3096,PMLR,02–04May,2024,
https://proceedings.mlr.press/v238/haan24a.html.
[12] D.Ruhe,J.BrandstetterandP.Forré,Cliffordgroupequivariantneuralnetworks,in
Thirty-seventhConferenceonNeuralInformationProcessingSystems,2023,
https://openreview.net/forum?id=n84bzMrGUD.
[13] J.Sokolic,R.Giryes,G.SapiroandM.Rodrigues,GeneralizationErrorofInvariant
Classifiers,inProceedingsofthe20thInternationalConferenceonArtificialIntelligenceand
Statistics,A.SinghandJ.Zhu,eds.,vol.54ofProceedingsofMachineLearningResearch,
pp.1094–1103,PMLR,20–22Apr,2017,https://proceedings.mlr.press/v54/sokolic17a.html.
[14] S.Mei,T.MisiakiewiczandA.Montanari,Learningwithinvariancesinrandomfeaturesand
kernelmodels,inProceedingsofThirtyFourthConferenceonLearningTheory,M.Belkinand
S.Kpotufe,eds.,vol.134ofProceedingsofMachineLearningResearch,pp.3351–3418,
PMLR,15–19Aug,2021,https://proceedings.mlr.press/v134/mei21a.html.
[15] B.Elesedy,GroupsymmetryinPAClearning,inICLR2022WorkshoponGeometricaland
TopologicalRepresentationLearning,2022,https://openreview.net/forum?id=HxeTEZJaxq.
[16] B.ElesedyandS.Zaidi,Provablystrictgeneralisationbenefitforequivariantmodels,in
Proceedingsofthe38thInternationalConferenceonMachineLearning,M.Meilaand
T.Zhang,eds.,vol.139ofProceedingsofMachineLearningResearch,pp.2959–2969,PMLR,
18–24Jul,2021,https://proceedings.mlr.press/v139/elesedy21a.html.
[17] B.Elesedy,Provablystrictgeneralisationbenefitforinvarianceinkernelmethods,inAdvances
inNeuralInformationProcessingSystems,M.Ranzato,A.Beygelzimer,Y.Dauphin,P.Liang
andJ.W.Vaughan,eds.,vol.34,pp.17273–17283,CurranAssociates,Inc.,2021,
https://proceedings.neurips.cc/paper_files/paper/2021/file/8fe04df45a22b63156ebabbb064fcd5e-
Paper.pdf.
[18] A.Bogatskiy,B.Anderson,J.Offermann,M.Roussi,D.MillerandR.Kondor,Lorentzgroup
equivariantneuralnetworkforparticlephysics,inProceedingsofthe37thInternational
ConferenceonMachineLearning,H.D.IIIandA.Singh,eds.,vol.119ofProceedingsof
MachineLearningResearch,pp.992–1002,PMLR,13–18Jul,2020,
https://proceedings.mlr.press/v119/bogatskiy20a.html.
[19] S.Gong,Q.Meng,J.Zhang,H.Qu,C.Li,S.Qianetal.,AnefficientLorentzequivariantgraph
neuralnetworkforjettagging,JHEP07(2022)030[2201.08187].
[20] A.Bogatskiy,T.Hoffman,D.W.Miller,J.T.OffermannandX.Liu,Explainableequivariant
neuralnetworksforparticlephysics: PELICAN,JHEP03(2024)113[2307.16506].
[21] J.Spinner,V.Bresó,P.deHaan,T.Plehn,J.ThalerandJ.Brehmer,Lorentz-Equivariant
GeometricAlgebraTransformersforHigh-EnergyPhysics,2405.14806.
[22] M.Favoni,A.Ipp,D.I.MüllerandD.Schuh,Latticegaugeequivariantconvolutionalneural
networks,Phys.Rev.Lett.128(2022)032003.
[23] A.M.M.ScaifeandF.Porter,Fanaroff–Rileyclassificationofradiogalaxiesusing
group-equivariantconvolutionalneuralnetworks,MonthlyNoticesoftheRoyalAstronomical
Society503(2021)2369
[https://academic.oup.com/mnras/article-pdf/503/2/2369/36733567/stab530.pdf].
[24] H.Weyl,TheClassicalGroups.PrincetonUniversityPress,2024/08/15/,1966,
10.2307/j.ctv3hh48t.
[25] S.Ravanbakhsh,Universalequivariantmultilayerperceptrons,inProceedingsofthe37th
InternationalConferenceonMachineLearning,H.D.IIIandA.Singh,eds.,vol.119of
ProceedingsofMachineLearningResearch,pp.7996–8006,PMLR,13–18Jul,2020,
https://proceedings.mlr.press/v119/ravanbakhsh20a.html.
11[26] N.DymandH.Maron,Ontheuniversalityofrotationequivariantpointcloudnetworks,in
InternationalConferenceonLearningRepresentations,2021,
https://openreview.net/forum?id=6NFBvWlRXaG.
[27] G.Cybenko,Approximationbysuperpositionsofasigmoidalfunction,MathematicsofControl,
SignalsandSystems2(1989)303.
[28] M.Leshno,V.Y.Lin,A.PinkusandS.Schocken,Multilayerfeedforwardnetworkswitha
nonpolynomialactivationfunctioncanapproximateanyfunction,NeuralNetworks6(1993)
861.
[29] L.F.Guilhoto,Anoverviewofartificialneuralnetworksformathematicians,2018,
https://api.semanticscholar.org/CorpusID:85504929.
[30] V.Hutson,J.S.J.S.PymandM.J.Cloud,Applicationsoffunctionalanalysisandoperator
theory.,Mathematicsinscienceandengineering,v.200.Elsevier,Amsterdam;,2nded./vivian
hutson,johns.pym,michaelj.cloud.ed.,2005.
[31] K.Cranmer,J.PavezandG.Louppe,ApproximatingLikelihoodRatioswithCalibrated
DiscriminativeClassifiers,1506.02169.
[32] D.P.KingmaandJ.Ba,Adam: Amethodforstochasticoptimization,in3rdInternational
ConferenceonLearningRepresentations,ICLR2015,SanDiego,CA,USA,May7-9,2015,
ConferenceTrackProceedings,Y.BengioandY.LeCun,eds.,2015,
http://arxiv.org/abs/1412.6980.
A Additionaldetailsofexperiments
Architecture: Atthel-thstage,theequivariantoperationfortheinhomogeneouscaseupdatesthe
scalarnoderepresentationsh(l)andthevectornoderepresentationx(l)viathefollowingequations:
i i
m(l+1) =Φ(l+1)(h(l),h(l),|x(l)−x(l)|2) , (8a)
ij e i j i j
x(l+1) =x(l)+ (cid:88) (x(l)−x(l))Φ(l+1)(m(l+1)) , (8b)
i i i j x ij
j∈N(i)
m(l+1) = (cid:88) m(l+1) , (8c)
i ij
j∈N(i)
h(l+1) =Φ(l+1)(h(l),m(l+1)) . (8d)
i h i i
Thenorm|.|isevaluatedusingtheparticularmetricconservedbythegroup. ThefunctionsΦ(l+1),
e
Φ(l+1)andΦ(l+1)areMultiLayerPerceptrons(MLPs),withΦ(l+1)’soutputasinglevaluemadeto
x h x
fallintheopenunitintervalviathesigmoidactivationfunction. Wegettheequivariantvectorupdate,
whichdoesnotrespecttranslationequivariancebyreplacingEqs.8aand 8bto
m(l+1) =Φ(l+1)(h(l),h(l),|x(l)−x(l)|2,⟨x ,x ⟩) , (9a)
ij e i j i j i j
x(l+1) =x(l)+ (cid:88) x(l) Φ(l+1)(m(l+1)) , (9b)
i i j x ij
j∈N(i)
where⟨x ,x ⟩istheinnerproductunderthemetricpreservedbythehomogeneousgroup.
i j
Weconsiderbothinvariantandequivariantfeatureextractionwiththesamebasearchitecturewith
atotalofthreeequivariantmessagepassingoperations. Wedonotconsideranyinputscalarinthe
datainput,whilethesubsequentscalarrepresentationsaresixtyfourdimensional. Theoutputofall
Φ(l+1)aresixtyfourdimensionalwhichunambiguouslyfixestheinputdimensionsoftheothertwo
e
functions. Themodeltakesthe3DCartesiancoordinatesasthevectorrepresentationfromwhichit
evaluatesthescalarsoftheparticulargroupsasinputsforthescalarupdates. Intheequivariantblock,
allMLPshavetwohiddenlayersofsixtynodesandReLUactivation. WhileΦ(l+1)hasasigmoid
x
outputactivation,wedonotapplyanynon-linearactivationtotheotheroutputs. Forinvariantfeature
extraction,weforegothevectorupdateatthefinallayeranduseonlytheconcatenatedmeanglobal
representationofeachscalarnoderepresentationasinputtoaclassifier. Fortheequivariantcase,we
12alsotakethemeanglobalreadoutoftheconcatenatedvectornoderepresentationsandfeedthistothe
classifiernetwork. TheclassifiernetworkalsohastwohiddenlayersofsixtyfournodesandReLU
activation,andasingleoutputnodewithsigmoidactivation. Withthissetofhyperparametervalues,
theinvariantmodelshave120kparameterswhiletheequivariantoneshave130kparameters.
Training: Weconsidertwotrainingdatasetsof10kand100ksamplesperclasstocomparethe
sampleefficiencyofthesemodels. Thesearegeneratedwitharandomseedof1029209usingthe
random.default_rngimplementedinNUMPY(v1.26.4forthetruncatednormalcaseandv1.23.5
fortheuniformcase). Similarly,thevalidationdatasetisfixedtoanindependent40ksamplesper
classgeneratedwithainitialseed9278298,andthetestdataistakentobeanotherindependentset
of100ksamplesperclasswithinitialseed827470. Allnetworktrainingusesbinarycrossentropy
loss and is trained ten times from random initialisation for hundred epochs. We use the ADAM
optimiser[32]withinitiallearningrateof0.001,whichdecaysbyafactorof0.5ifthevalidation
losshasnotimprovedoverthreeepochs. Weuse PYTORCH forallnumericalexperiments. For
thetruncatednormalexperiments,weuseversion2.2.1withCUDATOOLKIT 11.8.0whileforthe
uniformscenarioweuseversion1.12.1withCUDATOOLKIT11.3.1. Allexperimentswerecarried
outonNVIDIAA100orV100GPUs. TrainingiseithercarriedoutontwohomogeneousGPUsor
onasingleGPUwithatotaleffectivebatchsizeof300samples.
13