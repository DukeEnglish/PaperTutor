xGen-MM (BLIP-3): A Family of Open Large Multimodal
Models
LeXue1◦ ManliShu1◦ AnasAwadalla1,2∗ JunWang1∗ AnYan1∗
SenthilPurushwalkam1∗ HongluZhou1∗ VirajPrabhu1∗ YutongDai1∗
MichaelSRyoo1∗ ShrikantKendre1∗ JieyuZhang1,2∗
CanQin1 ShuZhang1 Chia-ChihChen1 NingYu1
JuntaoTan1 TulikaManojAwalgaonkar1 ShelbyHeinecke1† HuanWang1†
YejinChoi2† LudwigSchmidt2† ZeyuanChen1†
SilvioSavarese1† JuanCarlosNiebles1† CaimingXiong1† RanXu1†
1SalesforceAIResearch 2UniversityofWashington
{lxue, ssavarese, jniebles, cxiong, ran.xu}@salesforce.com
◦Firstauthors;∗Coreauthors;†Seniorauthors
ProjectPage
Abstract. ThisreportintroducesxGen-MM(alsoknownasBLIP-3),aframeworkfordevelopingLarge
MultimodalModels(LMMs). Theframeworkcomprisesmeticulouslycurateddatasets,atrainingrecipe,
modelarchitectures,andaresultingsuiteofLMMs. xGen-MM,shortforxGen-MultiModal,expandsthe
SalesforcexGeninitiativeonfoundationAImodels. Ourmodelsundergorigorousevaluationacrossarange
oftasks,includingbothsingleandmulti-imagebenchmarks. Ourpre-trainedbasemodelexhibitsstrong
in-context learning capabilities and the instruction-tuned model demonstrates competitive performance
amongopen-sourceLMMswithsimilarmodelsizes. Inaddition,weintroduceasafety-tunedmodelwith
DPO,aimingtomitigateharmfulbehaviorssuchashallucinationsandimprovesafety. Weopen-sourceour
models,curatedlarge-scaledatasets,andourfine-tuningcodebasetofacilitatefurtheradvancementsinLMM
research. Associatedresourceswillbeavailableonourprojectpageabove.
Simple and small-scale Multiple training objectives Large-scale comprehensive free-form Unified training objective
COCO-style image caption data (ITM, ITC, and ITG losses) multimodal interleaved data (next-token prediction loss)
Leptis Magna, Around a
in modern day kilometre or
Libya, once two up the
A su c na gt l aw se sa er sing Complex BLIP-2 LMM A prf er mic ia e's r cr iro ca ud s a ar ne d t h the e Scalable Vision xGen-MM
Q-Former Roman city. It amphitheatre Token Sampler
… in the…
(a)BLIP-2framework (b)xGen-MM(BLIP-3)framework
Figure1: WeintroducexGen-MM(BLIP-3),aframework(b)fordevelopingLargeMultimodalModels(LMMs).
OurframeworkimprovesuponBLIP-2(a)[1]by(1)increasingtherichness,scale,anddiversityoftrainingdata,(2)
replacingtheQ-Formerlayerswithamorescalablevisiontokensampler,and(3)simplifyingthetrainingprocessviathe
unificationofthetrainingobjectivestoasinglelossateverytrainingstage.TheresultingsuiteofLMMscanperform
variousvisuallanguagetasksandachievecompetitiveperformanceacrossbenchmarks.
1 Introduction
LargeMultimodalModels(LMMs)haveattractedsignificantattentionwiththeirpotentialapplicationsand
emergentcapabilities.Recentadvancementsinbothproprietarymodels[2–5]andopen-sourceLMMs[6,1,7–
11]highlighttherapidprogressandgrowinginterestinthisfield. However,despitetheseadvancements,
1
4202
guA
61
]VC.sc[
1v27880.8042:viXrathereisstillagapbetweenopen-sourcemodelsandproprietaryonesintermsofaccesstoopenweights,
trainingrecipes,andcurateddatasets. Suchlimitationshindertheopen-sourcecommunitiesfromreplicating,
understanding,andimprovingLMMs.
Recentworkshavedemonstratedthatlarge-scaleandhigh-qualitydataareessentialfortrainingrobust
LMMs[8–12]. BLIP-2[1]wasoneofthepioneeringeffortsinexploringLMMs,whichleveragedsynthetic
datatoachieveimpressiveresultsatthetime(Figure1(a)). However,thedatausedinBLIP-2lacksthe
scale,quality,anddiversityrequiredtoreachcompetitiveperformancecomparedtomoremodernLMMs
nowadays. In addition, BLIP-2 employs an intricate Q-Former [1] architecture to bridge the vision and
languagemodalities,coupledwithasuiteofcomplextrainingobjectives(ITM,ITC,andITGlosses),bothof
whichposeobstaclesforlarger-scaletraining. Moreover,BLIP-2supportsonlysingle-imageinput,whereas
interleavedmultimodaldataformatsarethemostnaturalformofmultimodaldata[13].
In response to these challenges, we introduce xGen-MM (BLIP-3) (Figure 1 (b)), a new framework
designedtoscaleupLMMtrainingbyutilizinganensembleofmultimodalinterleaveddatasets,curated
captiondatasets,andotherpubliclyavailabledatasets[14–17]. xGen-MM,shortforxGen-MultiModal,fur-
therexpandsourpreviousgenerativeAIinitiativesandcorrespondingfoundationmodelsfortextxGen[18],
codegenerationcodeGen[19,20],functioncallingAPIGen[21],amongothers. InxGen-MM(BLIP-3),
asillustratedinFigure2,westreamlinethemodelarchitecturebyreplacingtheQ-Former[1]withamore
scalablevisiontokensampler(specifically,aperceiverresampler[22])andsimplifyingthetrainingobjectives
tofocussolelyontheauto-regressivelossoftexttokensinamultimodalcontext. Ourprimaryfocusis
on dataset curation and scaling up the training data. Recently, our BLIP-3 team introduced two large-
scale,high-qualitydatasets: MINT-1T[12],atrillion-tokenscaleinterleaveddataset;andBLIP3-KALE,
a knowledge-augmented high-quality dense captions dataset. In this technical report, we introduce two
additionalspecializeddatasets: BLIP3-OCR-200M,alarge-scaledatasetwithdenseOCRannotations;and
BLIP3-GROUNDING-50M,alarge-scalevisualgroundingdataset.
In addition to these datasets, we are committed to open-sourcing the series of models developed in
thiswork,includingthebase,instruction-tuned,andDPOmodels. Alongwiththemodelrelease,wealso
providecodeforeasyfine-tuningofourbasemodeloncustomdatasets. Bymakingtheseresourcespublicly
available, we aim to make LMM research and development more accessible to the community, and we
encourageresearchersandpractitionerstouseourmodelsanddatasetstounderstandandfurtherexplorethe
potentialandemergentabilitiesofLMMs.
2 Related Work
RecentadvancementsinLargeMultimodalModels(LMMs)haveexploredtwomainarchitecturalapproaches:
the cross-attention style [22, 23] and the decoder-only style. The cross-attention approach, exemplified
bymodelslikeFlamingo[22,23]andLlama3.1[5],integratesvisionandlanguagemodalitiesthrougha
complexattentionmechanismtoenabledeepmultimodalunderstanding. Anotherapproachisthedecoder-
only architecture [1, 7, 8, 24–36], which we adopt in xGen-MM (BLIP-3), offers a more streamlined
solution. Thisapproachconnectspre-trainedlanguagemodelstovisualinputsusinglightweightconnectors,
simplifyingtheintegrationprocesswhilemaintainingrobustmultimodalcapabilities. Theeffectiveness
ofthisarchitectureisevidentinmodelssuchasMM1[9],VILA[10],LLaVA[8],phi3-vision[37],and
Otter[38].
Training methodologies for LMMs typically follow one of the two strategies. The first one uses a
light pre-training procedure and heavily relies on visual instruction tuning, as seen in the LLaVA se-
ries[8,29]. Extensiveresearchhasbeenconductedoncreatingeffectiveinstruction-tuningdataforavariety
oftasks[32,39–43]. Thesecondstrategyinvolvesextensivepre-trainingonlarge-scale,diversedatasets,
followedbyvisualinstructionfine-tuning. Thisapproach,employedbymodelslikeMM1andIdefics2[11],
infusesbroadknowledgeintothemodel,whichisthenfine-tunedtoalignwithhuman-likeinteractionsand
safetystandards. WhileMM1[9]providesextensiveablationsandstudiesontherecipesaimedatimproving
LMMs,itreleaseslimitedresourcesforpractitionerstoadoptthemodel(e.g.,MM1modelsanddatasets
2Loss Loss
Vision Tokens Text Tokens Vision Tokens Text Tokens …
Pre-trained LLM
…
Vision Tokens Text Tokens Vision Tokens Text Tokens
Token Token
Sampler Sampler
…
Text Tokenizer Text Tokenizer
Vision Vision
Transformer Transformer
Leptis Magna, in modern day Around a kilometre or two up the
Libya, once Africa's premier road are the circus and the
Roman city. It is one of the amphitheatre in the second part of …
greatest archeological sites in the Leptis Magna complex. The
the whole Mediterranean. If amphitheatre was built to seat up to
Leptis Magna were in Tunisia 16,000 spectators who would come
or Morocco or Egypt… to be entertained…
Figure 2: Overview of the xGen-MM (BLIP-3) framework. Free-form interleaved images and texts from the
ensembledinterleavedandcaptiondatasetsareinputintotheframework,witheachmodalityundergoingaseparate
tokenizationprocesstobefedintothepre-trainedLLMinnaturalorder.Astandardauto-regressivelossisthenapplied
tothetexttokens.TheVisionTransformeriskeptfrozenduringtraining,whileallotherparameters,includingthetoken
samplerandthepre-trainedLLM,aretrained.
areclose-sourced). Idefics2[11]isamorerecentopen-sourceworkthatopen-sourcesasuiteofmodels
alongwithdetailedtrainingstrategiesanddatarecipes,butIdefics2mostlyresuesexistingdatasetsintheir
experimentswithoutcontributingnewones.
In this work, we present xGen-MM (BLIP-3). Unlike previous works, xGen-MM (BLIP-3) is an
open-sourcefamilyofaseriesofmodels,datarecipes,fine-tuningcode,andtwolarge-scalefoundational
multimodaldatasets,whichwehopewillenableandadvancefutureresearchinthisarea.
3 Model Architecture
ArchitectureOverview. AsillustratedinFigure2,thexGen-MM(BLIP-3)frameworkadoptsanarchitec-
tureconsistingofaViT[44,45],avisiontokensampler(perceiverresampler[22])todownsampletheimage
embeddings,andapre-trainedLargeLanguageModel(phi3-mini[37]). Theinputtothemodelcanbefree-
formmultimodalinterleavedtextsandvisiontokensfromthediversemultimodaldatasourcesweensemble.
Any-Resolution Vision Token Sampling. As proved effective in recent LMMs [46–48], we adopt a
dynamichigh-resolution(i.e.,"any-resolution")imageencodingstrategyatthefine-tuningandpost-training
stages. Weenablehigher-resolutionimageunderstandingwithimagepatch-wiseencoding. Thepatch-wise
3encodingpreservestheresolutionoftheoriginalimageasmuchaspossiblebysplittingasingleimageinto
multiplepatchesandencodingthemseparately. Followingthepreviousconvention, weconcatenatethe
encodedimagepatcheswithadownsizedoriginalimagethatprovidesglobalinformation.
IntheVLconnector,weuseaperceiverresamplertodownsamplethevisiontokens. Withany-resolution
imageencoding,weperformthedownsamplingforeachimagepatch(includingthedownsizedoriginal
image)independently. ThedownsamplervisiontokensarethenconcatenatedtogetherandsenttotheLLM.
WiththedownsamplinginourVLconnector,wecanreducethesequencelengthofvisiontokensbyafactor
offiveormoredependingonthenumberofquerytokensintheperceiverresampler. Weprovideablation
studiesondifferenttokensamplingstrategiesinSection7.2.
4 Training
Pre-training. Thepre-trainingobjectiveistopredictthenexttexttokenacrossthedatasetmixturewe
pre-trainon. Overall,theresultingbasemodelxGen-MM-Phi3-mini-base-rispre-trainedforabout100
billionmultimodaltokensfromtheensembleddataset,andourpre-trainingresolutionis384x384pixels,
whichalignswithSigLIP[45].
Supervised Fine-tuning (SFT). We further fine-tune our pre-trained models on instruction-following
examples to make them better understand and follow user queries. At the fine-tuning stage, we use a
collectionofpublicallyavailableinstruction-followingdatasets[11,29,49]. Weadopttheany-resolution
vision token sampling strategy to allow a better understanding of images of higher resolutions such as
text-richdocumentdata. Weintroducethetechnicaldetailsforthefine-tuningstageinthefollowingsections.
InterleavedMulti-ImageSupervisedFine-tuning. Weconductasecond-stagefine-tuningontheinstruc-
tionfine-tunedmodelonamixtureofmulti-imageandsingle-imageinstructions-followingsamples. The
goalforthissecond-stagefine-tuningistoenhancethemodel’sabilitytocomprehendinterleavedimage-text
input,whichishelpfulformultimodalin-contextlearning,multi-imagequestionanswering,andmanymore
practicalusecases. Forthemulti-imagefine-tuning,wealsoadopttheany-resolutionvisiontokensampling
strategysameasinthepreviousSFTstage.
Post-training. Finally,weperformtwostagesofpost-trainingtoimprovethemodel’shelpfulnesswhile
mitigatingharmfulqualitiessuchashallucinationandtoxicity.Wefirstperformdirectpreferenceoptimization
(DPO[50])toimprovethemodel’shelpfulnessandvisualfaithfulness. Wethenperformsafetyfine-tuning
toimprovethemodel’sharmlessness. WequantitativelydemonstrateParetogainsinmodelharmlessness
andhelpfulnessafterourpost-training.
5 Data
5.1 Pre-trainingDataRecipe
AsindicatedinFigure3,inxGen-MM(BLIP-3),wepre-trainonanensembleofdiversemultimodaldatasets
withtheindicatedsamplingratios.
InterleavedDatasetMixture. WecombineMINT-1T(includingitsHTML,PDF,andArXivsubsets)with
OBELICS(HTMLonly)tocreateamorediverseandcomprehensivedatasetmixturethatcoversabroader
rangeofdomains.
1. MINT-1T[12]isatrilliontokenscalemultimodalinterleaveddataset,containingdatasourcesfrom
HTML,PDF,andArXiv. AsevidencedbyMM1[9]andIdefics2[11],suchmultimodalinterleaved
4OBELICS MINT-1T
BLIP3-KALE BLIP3-GROUNDING
BLIP3-OCR Datacomp-1B
CC12m CC3m
SBU VG
SBU
CC3m 1.25% VG Please note the balance of euro states trade:
1.25% 1.25% Italy in surplus, Germany deficit. Another world
CC12m indeed. Consider that Italian public debt was not
“NY”, “BURGER”, “PORK&GRILL”, 1.25% different than today, but it was not a matter of
concern, the shock did reflect on the exchange
“www.bigstock.com 206857405” rate, no one was thinking about selling government
bonds under par…Please check,
Datacomp-1B OBELICS
10%
17.5%
BLIP3-OCR
5%
BLIP3-GROUNDING
5%
the following graph: red is gdp% shift in
private debt and blue public debt from 1999 to
2007, so much for another of the myths of this
MINT-1T crisis, the one that says that “the fault is of
BLIP3-KALE the public debt"…
32.5%
25%
The image contains garment (at
the bottom right corner), t
shirt polo shirt (in the
center), neckband collar (at The condition ( appl_cond ) for applicability of
the top left corner), neckband the effective action ( a0 ) requires that ….
(to the right of the center),
neckband collar (above the
center)
"A shabby chic country kitchen design is showcased in this W ve a ra ii am t ia ot n sc a ol fc u tl ha et i sn tg r it nh ge ap ra ot uh n di n tt he eg r ba ol u no cv ee r the
image, featuring a decorative canister set with a vibrant configuration, which involves in particular…
rooster illustration. The container, which may be made of
ceramic or metal, boasts a rustic, weathered appearance
with a metal handle and a lid adorned with a curved metal
loop. The rooster is depicted in rich colors, including
red, blue, and yellow, against a background of faded,
handwritten-style text and designs. The container rests on In terms of the introduced variables the action (
a wooden surface, and the image bears a watermark from a0 ) can be written in the quadratic
Farmhouse Temptations at its bottom right corner." approximation in the deviations from the bounce …
Figure3: OverviewofxGen-MM(BLIP-3)Pre-trainingDatasets.
datasets are essential for scaling up large multimodal model training and enabling fundamental
capabilitieslikemultimodalin-contextlearning. Notably,differentfromtheOBELICS[11],MINT-1T
hasthreesubsetsfromdifferentsources: theHTMLsubset,thePDFsubset,andtheArXivsubset. In
xGen-MM(BLIP-3),thesethreesubsetsaremixedina7:5:1ratio.
2. OBELICS[11]isanotherlarge-scalemultimodalinterleaveddatasetconstructedfromHTMLdocu-
mentssolely. ItdiffersslightlyindomaincoveragefromMINT-1Tduetothespecificpreprocessing
stepsadopted.
CaptionDatasetMixture. Weintegrateadiverserangeofcaptiondatasets,withthespecificsoutlinedin
thefollowingdetails.
1. BLIP3-KALE is a large-scale curated high-quality caption dataset. Details will be discussed in
anotherpaper,andthedatasetwillbemadepublicverysoon.
2. BLIP3-OCR-200Misacuratedlarge-scaleOCRdatasettoaddressthelimitationsofcurrentlarge
multimodalmodelsinhandlingtext-richimageslikedocumentsandcharts,astraditionalimage-text
datasetsoftenlackadequateOCRannotations. Toenhancetextcomprehensionabilities,weusethe
OCRenginePaddleOCR[51]toannotateimageswithOCR-specificannotations. Overall,wecurate
adatasetof200millionhigh-resolutionimagesfromDatacomp-1B[17]. Foreachimage,wecreate
captionswithOCRdatabyidentifyingandextractingtextualelementsusingtheoff-the-shelfOCR
engine[51]. Textsegmentsinacaptionlike"... text..."aremodifiedtoincludeOCRinformationas
"... text(ocr_info)...",whereocr_infocontainsboundingboxcoordinatesfortheextracted
5text,specifyingitsexactpositionwithintheimageintheformat"<bbox>x ,y ,x ,y </bbox>".
1 1 2 2
WehavemultiplegranularitiesofOCRinformation,includingwithandwithoutboundingboxdata.
Inourwork,weonlyutilizetextualinformationwithoutboundingboxdata,whichhasproventobe
effective. Notethat,inxGen-MM(BLIP-3),wepreprocessthecaptionstoremovefillertextssuch
as“thetext”,whichwefindimprovesOCR-relatedbenchmarkperformance. Wehypothesizethat
thisisbecausesuchfillertextisrelativelyeasytopredict,potentiallydilutingthelossassociatedwith
OCR-relevanttokens. Nonetheless,incorporatingboundingboxinformationcouldfurtherenhance
performance,andweencourageresearchersinthecommunitytoexplorethispotential.
3. BLIP3-GROUNDING-50M is a curated large-scale grounding dataset to enhance the ability to
groundsemanticconceptsinvisualfeatures,whichiscrucialfortaskslikeobjectdetection,semantic
segmentation,andunderstandingreferringexpressions[52](e.g.,"theobjecttotheleftofthedog").
Wecurateadatasetof50millionimagesfromDatacomp-1B[17]. Foreachimage,weidentifyobjects
andtheirlocationinformationusingoneofthestate-of-the-artopen-worldimagetagging[53]and
object detection models [54]. Objects mentioned in a caption like "... object ..." are modified to
includegroundinginformationas"... object(grounding_info)...",wheregrounding_info
containsboundingboxinformationinoneofthreeformats,eachcapturingadifferentgranularityof
localization: (1)<bbox>x ,y ,x ,y </bbox>,(2)"startsat(x ,y )andextendsupto(x ,y )",
1 1 2 2 1 1 2 2
or(3)"top-leftcorneroftheimage".
4. OtherPublicDatasetsMixture: Wealsoincludeotherpubliclyavailabledatasetssuchasuncurated
Datacomp-1B[17]image-textpairs,CC12M[14],CC3M[14],VG[15],andSBU[16].
5.2 SupervisedFine-tuningDataRecipe
Thedatasetsusedinthefine-tuningstagearefrompublicdatasetsofdifferentdomains. Wesampledata
withvariousdomainfocusesincludingmulti-modalconversation[29],imagecaptioning[55,56],visual
questionanswering[57–60],chart/documentunderstanding[61–64],scienceandmath[65,66]. Inaddition
tothemulti-modalimage-textdata, wealsomixinpuretextinstructionfollowingdata [67,68]during
visualinstructiontuning. Ultimately,wecollectamixtureof1millionpublicallyavailableinstruction-tuning
samples,onwhichwefine-tuneourmodelforoneepoch.
Themulti-imageinstructiontuningstagestartswithamodelfine-tunedonsingle-imagesamples. Weuse
amixtureofpublicmulti-image/interleavedimage-textinstructiondata[69,70]. Topreventthemodelfrom
deterioratingonsingle-imagecapabilities,wereuseasubsetofsingle-imagedatasetsusedintheprevious
fine-tuningstageandmixthemintothemulti-imagetrainingdata.
5.3 Post-trainingDataRecipe
ImprovingTruthfulnessbyDirectPreferenceOptimization. WeemployVLFeedback[71],asyntheti-
callyannotatedmultimodalpreferencedatasetthatusesoff-the-shelfVLMstogenerateresponsestoadiverse
mixofmultimodalinstructionsthatarethenscoredbyGPT4-V[2]alongthreeaxes–helpfulness,visual
faithfulness,andethics. Thedatasetcontains80ksuchinstructionsfromwhichweconstructpreferencedata
bymarkingaspreferred(anddispreferred)theresponsewiththehighest(andlowest)averagescoreacross
modelsandfilteringoutexampleswithlow-scoringpreferredresponses. Wethusgenerate62.6kpreference
examples.
Weperform1epochofDPOonthecombinedpreferencedatasetwhileupdatingasubset(2.5%)ofLLM
backboneweightsusinglow-rankadaptation(LoRA[72]). Also,followingrecentwork[50],wegeneratean
additionalsetofresponsesthatcapturethemodel’sintrinsichallucinations,byperformingasecondstep
ofDPOper-iterationagainstthemodels’outputtoanoisedversionoftheinputimageandoriginalquery,
whichwetreatasanadditionaldispreferredresponse.
6Our BLIP3-OCR-200M caption:
Level 1: The image contains the text
Our BLIP3-OCR-200M caption: "Emergency", the text "Mobile", the text
Level 1: The image contains the text "DO NOT", the text "teuscher "Charger"
chocolates of Switzerland"
Our BLIP3-OCR-200M caption:
Level 1: The image contains the text "Rakuten", the text "THE
THIRD SHIRT", the text "AVAILABLE NOW!” Our BLIP3-OCR-200M caption:
Level 1: The image contains the text "THE", the text "PATRiCK", the
text "STAR", the text "SHOW”
Level 2: The image contains the text "THE" located above the center,
the text "PATRiCK" located above the center, the text "STAR" located
above the center, the text "SHOW" located above the center
Level 3: The image contains the text "THE" located at 1/14 of the Our BLIP3-OCR-200M caption:
image from top to bottom and 1/2 of the image from left to right, the Level 1: The image contains the text
text "PATRiCK" located at 1/7 of the image from top to bottom and "GOD'S", the text "WAY OUT", the text
10/19 of the image from left to right, the text "STAR" located at 1/4 of "OF-THE", the text "DARKNESS", the text
the image from top to bottom and 10/19 of the image from left to right, "DARROLDG PARKER”
the text "SHOW" located at 6/19 of the image from top to bottom and
1/2 of the image from left to right
Level 4: The image contains the text "THE" approximately starts at
[0.4, 0.0] and extends up to [0.6, 0.1], the text "PATRiCK"
approximately starts at [0.2, 0.1] and extends up to [0.8, 0.2], the text
"STAR" approximately starts at [0.3, 0.2] and extends up to [0.7, 0.3],
the text "SHOW" approximately starts at [0.4, 0.3] and extends up to
[0.6, 0.4]
Level 5: The image contains the text "THE" starts at [0.42, 0.045]
and extends up to [0.585, 0.101], the text "PATRiCK" starts at [0.237,
0.089] and extends up to [0.799, 0.202], the text "STAR" starts at
[0.308, 0.182] and extends up to [0.728, 0.31], the text "SHOW" starts
at [0.371, 0.289] and extends up to [0.647, 0.357]
Level 6: "The image contains the <ocr>"THE</ocr><bbox>[0.42,
0.045][0.585, 0.101]</bbox>, the Our BLIP3-OCR-200M caption:
Our BLIP3-OCR-200M caption: <ocr>"PATRiCK</ocr><bbox>[0.237, 0.089][0.799, 0.202]</bbox>, Level 1: The image contains the text "IF
Level 1: The image contains the text "Rucka Rucka Ali", the text the <ocr>"STAR</ocr><bbox>[0.308, 0.182][0.728, 0.31]</bbox>, the YOU WANT", the text "TO BUILD A", the
"Ura", the text "Cartoonist” <ocr>"SHOW</ocr><bbox>[0.371, 0.289][0.647, 0.357]</bbox>" text "HOUSE"
Figure 4: SamplesfromBLIP3-OCR-200M.SixlevelsofOCRinformationgranularityareextracted, withand
withoutboundingboxdata.NotethatinxGen-MM(BLIP-3),OCR-relatedcaptionsarepreprocessedtoremovefiller
phraseslike’thetext,’resultinginimprovedOCRbenchmarkperformance.
ImprovingHarmlessnessbySafetyFine-tuning. Next,weperform3epochsofsafetyfine-tuningon
thetrainsplitoftheVLGuard[73]dataset,whichcontains2kexamplesofunsafeimagesandinstructions.
VLGuardcomprisestwotypesofunsafeexamples:(1)objectionableimagespairedwithsafeinstructionsand
adesirableabstentionresponse,and(2)safeimagespairedwithtwotypesofinstruction-responsepairs,one
safeandanotherunsafe.Thedatasetconsistsofunsafeexamplesbelongingtovarioussubcategoriesincluding
privacy-violating,risky/sensitivetopics(suchaspolitics,sex,andviolence),deception,anddiscrimination.
Followingtheoriginalwork,werandomlysample5kadditionalexamplesfromtheinstructionfine-tuning
datasettoretainthemodel’shelpfulnesswithoutexaggeratingitssafetybehavior. Asbefore,weupdatea
subset(2.5%)ofLLMbackboneweightsusinglow-rankadaptation.
7Coarse Grounding Caption
This image showcases a football player (in the center) in action, wearing a red jersey (in the center)
with the logo number (in the center) '11' and the logo of the 'Falcons'. The player is holding an NFL
football (at the bottom left corner) in one hand (at the bottom left corner) and is gesturing with the
other. The helmet (above the center) has a face mask obscuring the player's face, and the
background is blurred, emphasizing the player in the foreground.
Fine-grained Grounding Caption
This image showcases a <object>football player</object><bbox>[0.015, 0.005][0.845, 0.993]</
bbox> in action, wearing a <object>red jersey</object><bbox>[0.205, 0.207][0.638, 0.994]</
bbox> with the logo …… The <object>helmet</object><bbox>[0.287, 0.002][0.523, 0.449]</bbox>
has a face mask obscuring the player's face, and the background is blurred, emphasizing the player
in the foreground.
Figure5: SamplesfromBLIP3-GROUNDING-50M.Weintroducealarge-scaledatasetofimagesandcorresponding
captionscontaininglocalizationinformationaboutobjects.Furthermore,wereleasetheassociatedobjectboundingbox
datatofacilitatethecreationofcaptionswithcustomtemplates.
6 Experiments
6.1 Pre-training
Few-shotEvaluation. Afterthepre-trainingstage,weevaluateourpre-trainedmodelonclassiccaptioning
andVQAtasks,incomparisonwithpreviousmodelsthatsupportfew-shotlearningmulti-modalevaluation.
Wepresentzero-shotandfew-shot(4-and8-shots)results,asshowninTable1. Overall,ourmodelachieves
competitivemultimodalin-contextlearningperformancewithcomparable-sizedLMMs1. FortheOCRtasks
(TextCapsandTextVQA)andVQA-v2,itsignificantlyoutperformsMM1-3Bandevenlargermodelssuch
asIdefics-9B[13]andMM1-7B[9]. Onallbenchmarks,increasingthenumberofshotscanimprovethe
performance,demonstratingthemodel’sabilitytoadapttoin-contextdistributions.
6.2 SupervisedFine-tuning
Weevaluateourmodelonacomprehensivesuiteofmulti-modal(image-text)benchmarks,assessingthe
model’sabilityfrommultipleperspectives. OurevaluationcoversgeneralVQAbenchmarks[74–78],visual
perception[49],domainknowledge[79,80],OCRability[57,81],andhallucination[82,83]. Formodels
fine-tunedoninterleavedmulti-imagedatasets,wealsoevaluatetheirperformanceoncommonmulti-image
benchmarks[69,84–86].
Single-ImageEvaluation. InTable2,wecomparewithmodelsincomparablesizes(<5B),including
both closed-source [9] and SoTA open-source models [10, 37]. We report individual benchmark scores
alongwithtwoaveragescores: “Avg.(all)"issimplytheaverageoverallbenchmarks,and“Avg.(perc.)"is
theaveragescoreoverbenchmarksthatfocusongeneralVQAandvisualperceptions. xGen-MM-instruct
outperformspreviousbaselinesonbothgeneralVQAandvisualperceptionbenchmarks. Inaddition,we
1Forfew-shotevaluation,thezero-shotresultsareusedmainlyasareferencefortheircorrespondingfew-shotnumbers,andthey
canbesensitivetoprompts.Asalsomentionedin[9],theyaremostlyindicativeofhowwellthepre-trainingdistributionmatches
theassociatedevaluationtaskformat.Inpre-trainingevaluation,wemainlycareforfew-shotperformance,whichisrobusttoprompt
templates.
8VisualQuestionAnswering Captioning
Model Shot
VQAv2 TextVQA OKVQA COCO NoCaps TextCaps
<5BModelComparisons
0 49.2 30.1 41.2 73.0 – –
Flamingo-3B[22]
4 53.2 32.7 43.3 85.0 – –
8 55.4 32.4 44.6 90.6 – –
0 46.2 29.4 26.1 73.5 55.6 63.3
MM1-3B[9]
4 57.9 45.3 44.6 112.3 99.7 84.1
8 63.6 44.6 48.4 114.6 104.7 88.8
0 43.1 34.0 28.0 67.2 82.6 69.5
xGen-MM-base(4B)
4 66.3 54.2 48.9 107.6 100.8 89.9
8 66.9 55.3 50.1 109.8 104.6 94.0
LargerModelsforReference
Flamingo-9B[22] 8 58.0 33.6 50.0 99.0 - -
Idefics-9B[13] 8 56.4 27.5 47.7 97.0 86.8 63.2
MM1-7B[9] 8 63.6 46.3 51.4 116.3 106.6 88.2
Idefics2-8B[11] 8 70.3 57.9 54.6 116.0 - -
Table1: Few-shotPretrainingEvaluation. Following[9],werandomlysampledemonstrationsfromthe
trainingsetasfew-shotexamples. WereportCIDErscoreforcaptioningandaccuracyforVQA.
findthatxGen-MM-instruct-interleave,althoughfurtherfine-tunedonmulti-imagedata,maintainsgood
performanceonsingle-imagebenchmarksandhasthehighestoverallscores.
SEED SEED MMB MM MME CVB CVB RealW MMMU Math Sci Text Avg. Avg.
Model(Size) POPE
-IMG v2 (dev) Star (norm) -2D -3D QA (val) Vista QA VQA (all) (perc.)
Closed-sourcemodels
GPT-4V 72.0 - 80.8 49.7 63.3 64.3 73.8 56.5 53.8 48.2 82.1 75.4 - - -
MM1-3B-Chat(3B) 68.8 - 67.8 - 62.9 - - - 33.9 - - 87.4 - - -
Open-sourcemodels
HPT-1.5-edge(4B) 72.3 - 74.6 45.8 - - - - 42.6 45.1 85.4 91.0 - - -
VILA-1.5-3B(3B) 67.9 - 63.4 - - - - - 33.3 - 69.0 85.9 - - -
VILA-1.5-3B∗(3B) 67.9 51.9 62.4 40.3 58.5 50.1 60.3 53.3 34.1 30.6 68.9 86.9 58.1 55.6 59.1
phi-3-vision(4B) - - 80.5 - - - - - - 44.5 90.8 85.8 70.9 - -
phi-3-vision∗(4B) 71.0 52.7 74.2 47.9 55.3 60.7 68.2 59.1 46.1 45.1 90.2 83.5 73.3 63.6 63.6
xGen-MM-inst.(4B) 71.8 53.9 76 46.7 63.8 66.2 75.4 61.6 42.8 39.2 85.6 87.0 72.0 64.8 66.9
xGen-MM-inst.
72.2 55.5 76.8 48.1 64.4 69.3 72.3 60.5 41.1 39.6 88.3 87.0 71.0 65.1 67.3
-interleave(4B)
Table2: Evaluationonsingle-imagebenchmarks. phi-3-vision∗andVILA-1.5-3B∗aretestedwithour
evaluationcode2forafaircomparison. WealsoincludetheGPT-4V(gpt-4-1106-preview)performance
(providedbytheevaluationcodebase)asareferenceinthefirstrow.
Multi-ImageEvaluation. InTable3,wecomparexGen-MM-instructwithxGen-MM-instruct-interleave
onmulti-imagebenchmarks. Althoughtheformerisfine-tunedfromxGen-MM-basewhichcancomprehend
interleavedimage-textdata,itperformspoorlyonmulti-imagebenchmarks. Wesuspectitisbecausesolely
fine-tuningonsingle-imagedatahurtssuchability. Withmulti-imageSFT,weseesignificantlyimproved
scores. Inaddition,wealsoevaluateXgen-MM-interleaveonsingle-imagebenchmarks(SeeTable2)and
findthatitmaintainsgoodperformanceonallbenchmarkswiththehighestoverallscores.
2https://github.com/open-compass/VLMEvalKit
9Model BLINK QBench-2 Mantis-eval
GPT-4V 51.1 73.4 62.7
VILA-1.5-3B∗(3B) 39.8 51.7 41.9
xGen-MM-inst.(4B) 46.6 52.4 42.4
xGen-MM-inst.-interleave(4B) 49.7 75.1 56.7
Table3: Evaluationonmulti-imagebenchmarks. VILA-1.5-3B∗ resultsareobtainedusingthesame
evaluationcodeasourmodels. WeincludetheGPT-4Vperformanceasareferenceinthefirstrow.
* Zoom in for
illustration.
User: Which one of the following User: Where does the object in
can be found in the first image? image 1 appear in image 2?
A. B.
User: How much of BLIP-3's pre-training
xGen-MM: The object in image 1,
data is from MINT-1T?
a screwdriver, appears in image 2
xGen-MM: B on the left side, near the top left
xGen-MM: MINT-1T contributes 32.5% of
corner.
BLIP-3's pre-training data.
Figure 6: Example model outputs of xGen-MM-instruct-interleave. The model is capable of understanding
interleavedimage-textinputanduserqueriesaboutmultipleimageswhilemaintainingtheperformanceonsingle-image
QAs.
6.3 Post-training
Table4summarizestheresultsoftwopost-trainingstrategiesforxGen-MM-instruct. Wemeasuresafety
performancebyASR%(attacksuccessrate)ontheVLGuardtestsplitandhallucinationperformanceusing
HallusionBench[82](accuracyonimage-contextreasoning)andPOPE[83](averageF1scoreonbinary
entitypresencequestions). Toensurepost-trainingdoesn’tcompromisehelpfulness,wereportperformance
onafewcomprehensionbenchmarksasacontrol.
DPOenhancestruthfulnessbyimprovinghallucinationbenchmarks(Row2),whilesafetyfinetuning
significantlyreducesASR(Row3). Helpfulnessisalsoimprovedslightly,asshownbycontrolbenchmarks.
Thefinalmodel,xGen-MM-dpo,includesbothimprovements.
Safety Hallucination Helpfulness(Control)
Method
VLGuard(↓) HalBench(↑) POPE(↑) SEED-IMG(↑) MMB-dev(↑) MME(↑) MMStar(↑)
xGen-MM-inst.(4B) 56.6 56.3 87.0 71.8 76.0 63.8 46.7
+DPO 54.9 57.1 87.0 71.9 76.4 63.0 47.1
+SafetyFT 5.2 56.6 86.8 72.1 76.4 64.4 47.1
Table4: Post-trainingresults. Wereportresultsonsafetyandhallucinationbenchmarksafterpost-training,
as well as on four helpfulness benchmarks as a control. Post-training improves harmlessness without
compromisinghelpfulness.
107 Ablation Studies
7.1 Pre-trainingAblation
ScalingPre-trainingData. Weperformanablationstudytoexploretherelationbetweentheamount
of pre-training data and the pre-train evaluation metrics, by varying the data scale from 2B multimodal
tokensto100Bmultimodaltokens. Thedatarecipeweusedhereisamixtureofimagecaptiondatasetsand
multimodalinterleaveddata. AsshowninFigure7,wefindthatscalingupthenumberofmultimodaltokens
from2Bto60Bleadstosubstantialgainforimage-text(COCO-Caps)andOCR(Text-Caps,TextVQA)
tasks, and further increasing the data size to 100B has moderate additional benefit in terms of few-shot
evaluationmetrics.
50
110
90
105 48 100 C CO OC CO O C Ca ap ps s 4 8- -s sh ho ot t 80 T Te ex xt t C Ca ap ps s 4 8- -s sh ho ot t 46 O OK K- -V VQ QA A 4 8- -s sh ho ot t
95 70 44
410 20 40 60 100 410 20 40 60 100 410 20 40 60 100
Number of training tokens(B) Number of training tokens(B) Number of training tokens(B)
(a)COCO-Caps (b)Text-Caps (c)OK-VQA
Figure7: Few-shotperformancegivendifferentsizesofpretrainingdata.
Pre-trainingDataRecipe. Wediscusstheimpactofdifferentdatarecipesforpre-training. Specifically,
weperformablationstudiesontopofabasedatarecipe: useObelics[13]asthemultimodalinterleaveddata
sourcewhilekeepingthecaptiondatasetsmixturethesame. Wealsoconsidertwootherrecipes(1)using
MINT-1T[12]asinterleaveddatareplacement,and(2)mixingadditionalpuretext-onlyinstruction-tuning
dataasapre-traindataset. AsshowninTable5,weseeaperformanceimprovementusingMINT-1Tfor
image-textalignment(COCO-Caps)andOCR(Text-Caps,TextVQA),withaslightperformancedropon
OK-VQA, which is a knowledge-intensive task. We also find that adding text data can help attain the
performanceonOK-VQAthatreliesmoreonLLMcapacity.
Data Text-VQA OK-VQA COCO-Caps Text-Caps
Obelics 41.1/41.9 48.4/49.5 107.2/109.4 78.2/79.9
MINT-1T 41.0/42.0 46.5/48.2 109.3/111.4 80.3/82.0
MINT-1T+textdata 42.1/42.6 48.3/49.7 108.0/110.2 77.0/79.9
Table5: Few-shot(4-shot/8-shot)performancegivendifferentdatarecipes.
VisualBackbones. Wealsoexploreifdifferentvisualbackboneshaveanimpactontheperformanceof
vision-languagetasks. Wecomparetwotypesofvisualencoders,DFNandSigLIP.Empirically,wefind
SigLIPprovidesbettervisualrepresentationsthatboostperformanceonOCRtasks.
VisualBackbone Text-VQA OK-VQA COCO-Caps Text-Caps
DFN 41.1/41.8 48.4/49.5 107.2/109.4 78.2/79.9
SigLIP 49.1/50.5 48.4/48.9 108.7/110.2 84.7/88.6
Table6: Few-shot(4-shot/8-shot)performancegivendifferentvisualbackbones.
11
rediC rediC rediCNumberofVisualTokens. Anotherablationistostudytheimpactofdifferentnumbersofvisualtokens,
i.e.,inputimagetokensfedintothelanguagemodel. Wefindthatreducingthenumberofvisualtokensfrom
128to64canstillattainsimilarperformance,asshowninTable7. Thismakesitpossibleformodelstotake
inmorevisualimagesgivenafixedcontextwindow.
VisualToken Text-VQA OK-VQA COCO-Caps Text-Caps
128 41.1/41.8 48.4/49.5 107.2/109.4 78.2/79.9
64 41.2/42.6 47.6/48.3 108.0/109.3 79.5/81.6
Table7: Few-shot(4-shot/8-shot)performancegiventhedifferentnumberofvisualtokens.
7.2 SFTAblation
Weconductablationstudiesattheinstructionfine-tuningstage,focusingonseveralmodeldesignchoices
anddatarecipes. TheSFTablationstudiesareconductedonasimplifiedSFTdatamixture,sotheresultsin
thissectionarenotdirectlycomparabletothemainresultsinsection 6.2.
Any-ResolutionVisionTokenSampling. Ourany-resolutionstrategydiffersfrompreviouswork[46]in
thateverygroupofimageembeddings(ofthesameimagepatch)isdownsampledwithaperceiverresampler,
whichensuresthatthenumberofvisiontokensinputtotheLLMremainsrelativelysmall. Inthissection,
weablatetheeffectivenessofourany-resolutionstrategybycomparingitwitha“fixed-resolution"baseline
andotherdownsamplingdesigns.
The“fixed-resolution"baselineresizesallimagestothedefaultinputsizeofthevisionencoderwhile
keepingtheoriginalaspectratios. Wealsotriedanotherdownsamplingstrategywiththeperceiverresampler:
Insteadofdoingdownsamplingforeachpatchindependently,weconsidera"fixedsampling"(denotedas
anyres-fixed-samplinginFigure8a). Inthefixedsampling,weconcatenatetheimageembeddings
fromallimagepatchesandtheninputthemasasinglesequencetotheperceiverresamplertoobtainthe
fixednumberofvisiontokensforthewholeimage.
70 75
base resolution XGen-MM
65
anyres-fixed-sampling (ntok=128) 70 XGen-MM (instruction-aware)
60 anyres-fixed-sampling (ntok=256)
anyres-patch-sampling 65
55
50 60
45
55
40
50
35
30 DocVQA ChartQA OCRBench 45 Perception OCR Sci. & Math Hallucination
OCR Benchmarks Evaluation Domains
(a) (b)
Figure8: SFTablationstudies.(a).ComparisonofdifferentvisiontokensamplingstrategiesonOCRbenchmarks.
(b).Comparisonbetweenourmodelandits“instruction-aware"alternative.ForeachevaluationdomaininFigure(b),
wereporttheaveragescoreonmultiplerelevantbenchmarks.
Ourevaluationofthisdesignfocusesontext-richtasks(e.g.,documentunderstanding)thatwouldbenefit
fromhigh-resolutionencodingwithvisualdetails. FromFigure8a,wecanseesignificantimprovements
12
serocS
serocS
egarevAwithourresolutionimageencodingstrategyevenwithdownsampledvisiontokens. Thefixedsampling
strategy,althoughitshowsimprovementsoverthebaseresolutionbaseline,isnotasgoodasthepatch-wise
sampling. Wesuspectthatthismaybeduetotworeasons: (a)Withfixedsampling,avisiontokensequence
thatcanhaveaslongasover3,000embeddingtokenswillbecompressedto128tokens,whichmaybe
toofewtoretaintheinformation. (b)Theperceiverresamplermaynotworkwellwithaconcatenationof
differentimageembeddings.
Instruction-AwareVisionTokenSampling. InstructBLIP[7]proposesaninstruction-awareQ-Former[1]
forvisiontokensamplingandshowsthatitcanimprovethemodelperformanceonsomebenchmarks. With
the perceiver resampler as the VL connector, we can adopt a similar modification to make this process
instruction-aware. Tomakeourperceiverresampler“instruction-aware",weappendthetextinstructions
tokenstothequerytokensoftheperceiverresampler. UnlikeQ-Former,thereareonlycross-attentionlayers
insidetheperceiverresampler,sotheinstruction(texttokens)wouldinteractwithbothquerytokensand
imageembeddingsviacross-attention.
FromthecomparisoninFigure8b,wedonotobserveasignificantdifferencebetweenourmodelandits
instruction-awareversiononvariousbenchmarks. Itcouldbethatourmodificationtotheperceiverresampler
cannotbeidenticaltotheinstruction-awaremodificationmadetotheQ-Formerin Daietal.[7],andthus
the effectiveness differs. Because of the little difference we observe in this ablation study, we keep the
originalperceiverresamplerarchitectureinourmodelforsimplicity. Weleavethefurtherexplorationofthe
“instruction-aware"VLconnectortofutureworks.
MMMU MathVista Science MME
Text-onlySFTdata MMStar
(val) (mini) QA (norm)
Conversation 39.1 37.1 84.8 64.9 46.1
Conversation+Math+Coding 40.9 38.9 81.4 64.8 45.3
Table8: Theimpactoftext-onlySFTdata.Wecomparetwochoicesoftext-onlySFTdatausedfortheimage-text
SFTdatamixture.
QualityoftheText-onlyInstructionData. Itisacommonstrategytotrainorfine-tuneamulti-modal
LLMonbothmulti-modalandpuretextdata[8,11]. Itismainlyformaintainingthelanguageabilityof
thefine-tunedmodel. Inthisexperiment,westudyhowthispuretextsubsetwouldaffecttheperformance
onmulti-modalbenchmarks. Fortheinstructiontuningstage,wecomparewhetherthediversityofthepure
textdatawouldaffectthemulti-modalperformance. Forexample,howpure-textmathdataaffectsamodel’s
performanceonmultimodalmathbenchmarks.Inourmainexperiments,thedefaultcollectionofpuretextin-
structiondatacoversdiversedomainsincludingconversation[67],math[68,87],andcode[88].Forthisabla-
tionstudy,wesubstitutethesedatasetswiththesameamountofsamplesthatonlycovergeneralconversation.
InTable8,weobservethataddingmathandcodingdata,althoughinpure-textformat,canhelpimprove
amodelonrelevantbenchmarkslikeMathVista[80],whilehaslesseffectsongeneralVQAbenchmarks.
8 Conclusion
WeintroducexGen-MM(BLIP-3),acomprehensiveframeworkfortrainingaseriesofopen-sourcelarge
multimodalmodelsonacuratedmixtureoflarge-scaledatasets. xGen-MM(BLIP-3)demonstratesemergent
abilitiessuchasmultimodalin-contextlearningandachievesimpressiveresultsonmultimodalbenchmarks.
Byopen-sourcingxGen-MM(BLIP-3),ourcurateddatasets,andourSFTfine-tuningcodebase,wehope
toempowertheresearchcommunitywithaccessiblemultimodalfoundationmodelsanddatasets,allowing
practitionerstoexplorefurtherandadvancethepotentialandemergentabilitiesofLMMs.
139 Broader Impact
ThexGen-MM(BLIP-3)frameworkanditssuiteofLargeMultimodalModels(LMMs)havethepotential
to significantly advance multimodal AI research by providing accessible, open-source resources for the
broadercommunity. Byfacilitatingthedevelopmentandfine-tuningofstate-of-the-artLMMs,xGen-MM
(BLIP-3)empowersresearchersandpractitionersacrossvariousdomainstoinnovateandapplythesemodels
todiversereal-worldchallenges. Moreover,theintegrationofsafety-tuningprotocolswithinthexGen-MM
(BLIP-3)frameworkhelpsmitigateethicalriskssuchasbiasandmisinformation,promotingtheresponsible
deploymentofAItechnologies.
10 Acknowledgement
WewouldliketothankSrinathMeadusani,LavanyaKaranam,DhavalDilipMetrani,andEricHufortheir
workonthescientificcomputationinfrastructure,aswellasJasonLeeandJohnEmmonsfortheireffortsin
collectingthelarge-scaletext-onlySFTdatasetsusedinoneofourpre-trainingablationstudies.
References
[1] Junnan Li, Dongxu Li, Silvio Savarese, and Steven C. H. Hoi. BLIP-2: bootstrapping language-
imagepre-trainingwithfrozenimageencodersandlargelanguagemodels. InICML,volume202of
ProceedingsofMachineLearningResearch,pages19730–19742.PMLR,2023.
[2] OpenAI. Gpt-4v(ision)systemcard,2023. URLhttps://cdn.openai.com/papers/GPTV_
System_Card.pdf.
[3] Google. Gemini: Afamilyofhighlycapablemultimodalmodels,2023.
[4] OpenAI. Hellogpt-4o,2024. URLhttps://openai.com/index/hello-gpt-4o/.
[5] AI @ Meta Llama Team. The llama 3 herd of models, 2024. URL https://ai.meta.com/
research/publications/the-llama-3-herd-of-models/.
[6] JunnanLi,DongxuLi,CaimingXiong,andStevenC.H.Hoi. BLIP:bootstrappinglanguage-image
pre-training for unified vision-language understanding and generation. In ICML, volume 162 of
ProceedingsofMachineLearningResearch,pages12888–12900.PMLR,2022.
[7] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleFung,andStevenC.H.Hoi. Instructblip: Towardsgeneral-purposevision-languagemodels
withinstructiontuning. InNeurIPS,2023.
[8] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning,2023.
[9] BrandonMcKinzie,ZheGan,Jean-PhilippeFauconnier,SamDodge,BowenZhang,PhilippDufter,
DhrutiShah,XianzhiDu,FutangPeng,FlorisWeers,AntonBelyi,HaotianZhang,KaranjeetSingh,
Doug Kang, Hongyu Hè, Max Schwarzer, Tom Gunter, Xiang Kong, Aonan Zhang, Jianyu Wang,
ChongWang,NanDu,TaoLei,SamWiseman,MarkLee,ZiruiWang,RuomingPang,PeterGrasch,
AlexanderToshev, andYinfeiYang. MM1: Methods, Analysis&InsightsfromMultimodalLLM
Pre-training, March 2024. URL http://arxiv.org/abs/2403.09611. arXiv:2403.09611
[cs].
[10] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz,
MohammadShoeybi,andSongHan. Vila: Onpre-trainingforvisuallanguagemodels,2024. URL
https://arxiv.org/abs/2312.07533.
14[11] Hugo Laurençon, Léo Tronchon, Matthieu Cord, and Victor Sanh. What matters when building
vision-languagemodels?,2024. URLhttps://arxiv.org/abs/2405.02246.
[12] AnasAwadalla,LeXue,OscarLo,ManliShu,HannahLee,EtashKumarGuha,MattJordan,Sheng
Shen,MohamedAwadalla,SilvioSavarese,etal. Mint-1t: Scalingopen-sourcemultimodaldataby
10x: Amultimodaldatasetwithonetrilliontokens. arXivpreprintarXiv:2406.11271,2024.
[13] HugoLaurençon,LucileSaulnier,LéoTronchon,StasBekman,AmanpreetSingh,AntonLozhkov,
ThomasWang,SiddharthKaramcheti,AlexanderRush,DouweKiela,etal. Obelics: Anopenweb-
scalefiltereddatasetofinterleavedimage-textdocuments. AdvancesinNeuralInformationProcessing
Systems,36,2024.
[14] SoravitChangpinyo,PiyushSharma,NanDing,andRaduSoricut. Conceptual12m: Pushingweb-
scaleimage-textpre-trainingtorecognizelong-tailvisualconcepts. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages3558–3568,2021.
[15] RanjayKrishna,YukeZhu,OliverGroth,JustinJohnson,KenjiHata,JoshuaKravitz,StephanieChen,
YannisKalantidis,Li-JiaLi,DavidAShamma,etal. Visualgenome: Connectinglanguageandvision
usingcrowdsourceddenseimageannotations. Internationaljournalofcomputervision,123:32–73,
2017.
[16] Vicente Ordonez, Girish Kulkarni, andTamaraBerg. Im2text: Describing imagesusing1 million
captionedphotographs. Advancesinneuralinformationprocessingsystems,24,2011.
[17] SamirYitzhakGadre,GabrielIlharco,AlexFang,JonathanHayase,GeorgiosSmyrnis,ThaoNguyen,
RyanMarten,MitchellWortsman,DhrubaGhosh,JieyuZhang,etal. Datacomp: Insearchofthenext
generationofmultimodaldatasets. AdvancesinNeuralInformationProcessingSystems,36,2024.
[18] Erik Nijkamp, Hiroaki Hayashi Tian Xie, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih
Yavuz,PhilippeLaban,BenKrause,SenthilPurushwalkam,TongNiu,WojciechKryscinski,Lidiya
Murakhovs’ka,PrafullaKumarChoubey,AlexFabbri,YeLiu,RuiMeng,LifuTu,MeghanaBhat,
Chien-Sheng Wu, Silvio Savarese, Yingbo Zhou, Shafiq Rayhan Joty, and Caiming Xiong. Long
sequence modeling with xgen: A 7b llm trained on 8k input sequence length. ArXiv, 2023. URL
https://arxiv.org/abs/2309.03450.
[19] ErikNijkamp,BoPang,HiroakiHayashi,LifuTu,HuanWang,YingboZhou,SilvioSavarese,and
CaimingXiong. Codegen: Anopenlargelanguagemodelforcodewithmulti-turnprogramsynthesis.
arXivpreprintarXiv:2203.13474,2022.
[20] Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. Codegen2:
Lessonsfortrainingllmsonprogrammingandnaturallanguages. arXivpreprintarXiv:2305.02309,
2023.
[21] ZuxinLiu,ThaiHoang,JianguoZhang,MingZhu,TianLan,ShirleyKokane,JuntaoTan,Weiran
Yao,ZhiweiLiu,YihaoFeng,etal. Apigen: Automatedpipelineforgeneratingverifiableanddiverse
function-callingdatasets. arXivpreprintarXiv:2406.18518,2024.
[22] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel
Lenc,ArthurMensch,KatherineMillican,MalcolmReynolds,RomanRing,ElizaRutherford,Serkan
Cabi,TengdaHan,ZhitaoGong,SinaSamangooei,MarianneMonteiro,JacobL.Menick,Sebastian
Borgeaud,AndyBrock,AidaNematzadeh,SahandSharifzadeh,MikolajBinkowski,RicardoBarreira,
Oriol Vinyals, Andrew Zisserman, and Karén Simonyan. Flamingo: a visual language model for
few-shotlearning. InNeurIPS,2022.
15[23] AnasAwadalla,IrenaGao,JoshGardner,JackHessel,YusufHanafy,WanrongZhu,KalyaniMarathe,
YonatanBitton,SamirYitzhakGadre,ShioriSagawa,JeniaJitsev,SimonKornblith,PangWeiKoh,
GabrielIlharco,MitchellWortsman,andLudwigSchmidt. Openflamingo: Anopen-sourceframework
fortraininglargeautoregressivevision-languagemodels. CoRR,abs/2308.01390,2023.
[24] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,ChangZhou,
andJingrenZhou. Qwen-vl: Afrontierlargevision-languagemodelwithversatileabilities. arXiv
preprintarXiv:2308.12966,2023.
[25] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa, Soravit Changpinyo, Jialin Wu, Car-
losRiquelmeRuiz,SebastianGoodman,XiaoWang,YiTay,etal. Pali-x: Onscalingupamultilingual
visionandlanguagemodel. arXivpreprintarXiv:2305.18565,2023.
[26] DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,Ayzaan
Wahid,JonathanTompson,QuanVuong,TianheYu,etal. Palm-e: Anembodiedmultimodallanguage
model. arXivpreprintarXiv:2303.03378,2023.
[27] ShaohanHuang,LiDong,WenhuiWang,YaruHao,SakshamSinghal,ShumingMa,TengchaoLv,Lei
Cui,OwaisKhanMohammed,BarunPatra,etal. Languageisnotallyouneed: Aligningperception
withlanguagemodels. AdvancesinNeuralInformationProcessingSystems,36,2024.
[28] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu
Wei. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint
arXiv:2306.14824,2023.
[29] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning,2023.
[30] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,AnwenHu,
PengchengShi,YayaShi,etal. mplug-owl: Modularizationempowerslargelanguagemodelswith
multimodality. arXivpreprintarXiv:2304.14178,2023.
[31] QinghaoYe,HaiyangXu,JiaboYe,MingYan,AnwenHu,HaoweiLiu,QiQian,JiZhang,andFei
Huang. mplug-owl2: Revolutionizingmulti-modallargelanguagemodelwithmodalitycollaboration.
In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages
13040–13051,2024.
[32] TaoGong,ChengqiLyu,ShilongZhang,YudongWang,MiaoZheng,QianZhao,KuikunLiu,Wenwei
Zhang,PingLuo,andKaiChen. Multimodal-gpt: Avisionandlanguagemodelfordialoguewith
humans. arXivpreprintarXiv:2305.04790,2023.
[33] JunbumCha,WooyoungKang,JonghwanMun,andByungseokRoh. Honeybee: Locality-enhanced
projectorformultimodalllm. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages13817–13827,2024.
[34] PengGao,RenruiZhang,ChrisLiu,LongtianQiu,SiyuanHuang,WeifengLin,ShitianZhao,Shijie
Geng,ZiyiLin,PengJin,etal. Sphinx-x: Scalingdataandparametersforafamilyofmulti-modal
largelanguagemodels. arXivpreprintarXiv:2402.05935,2024.
[35] QuanSun,YufengCui,XiaosongZhang,FanZhang,QiyingYu,YuezeWang,YongmingRao,Jingjing
Liu, TiejunHuang, andXinlongWang. Generativemultimodalmodelsarein-contextlearners. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages14398–
14409,2024.
16[36] ShengdingHu, YugeTu, XuHan, ChaoqunHe, GanquCui, XiangLong, ZhiZheng, YeweiFang,
YuxiangHuang,WeilinZhao,etal. Minicpm: Unveilingthepotentialofsmalllanguagemodelswith
scalabletrainingstrategies. arXivpreprintarXiv:2404.06395,2024.
[37] MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,JyotiAneja,AhmedAwadallah,HanyAwadalla,
Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha
Bilenko,JohanBjorck,SébastienBubeck,QinCai,MartinCai,CaioCésarTeodoroMendes,Weizhu
Chen,VishravChaudhary,DongChen,DongdongChen,Yen-ChunChen,Yi-LingChen,ParulChopra,
XiyangDai,AllieDelGiorno,GustavodeRosa,MatthewDixon,RonenEldan,VictorFragoso,Dan
Iter,MeiGao,MinGao,JianfengGao,AmitGarg,AbhishekGoswami,SuriyaGunasekar,Emman
Haider,JunhengHao,RussellJ.Hewett,JamieHuynh,MojanJavaheripi,XinJin,PieroKauffmann,
NikosKarampatziakis,DongwooKim,MahoudKhademi,LevKurilenko,JamesR.Lee,YinTatLee,
YuanzhiLi,YunshengLi,ChenLiang,LarsLiden,CeLiu,MengchenLiu,WeishungLiu,EricLin,
Zeqi Lin, Chong Luo, Piyush Madan, Matt Mazzola, Arindam Mitra, Hardik Modi, Anh Nguyen,
BrandonNorick,BarunPatra,DanielPerez-Becker,ThomasPortet,ReidPryzant,HeyangQin,Marko
Radmilac,CorbyRosset,SambudhaRoy,OlatunjiRuwase,OlliSaarikivi,AminSaied,AdilSalim,
MichaelSantacroce,ShitalShah,NingShang,HiteshiSharma,SwadheenShukla,XiaSong,Masahiro
Tanaka,AndreaTupini,XinWang,LijuanWang,ChunyuWang,YuWang,RachelWard,Guanhua
Wang,PhilippWitte,HaipingWu,MichaelWyatt,BinXiao,CanXu,JiahangXu,WeijianXu,Sonali
Yadav,FanYang,JianweiYang,ZiyiYang,YifanYang,DonghanYu,LuYuan,ChengruidongZhang,
CyrilZhang,JianwenZhang,LiLynaZhang,YiZhang,YueZhang,YunanZhang,andXirenZhou.
Phi-3TechnicalReport: AHighlyCapableLanguageModelLocallyonYourPhone,May2024. URL
http://arxiv.org/abs/2404.14219. arXiv:2404.14219[cs].
[38] Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A
multi-modalmodelwithin-contextinstructiontuning.corrabs/2305.03726(2023),2023.
[39] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua
Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint
arXiv:2311.12793,2023.
[40] LeiLi,YuweiYin,ShichengLi,LiangChen,PeiyiWang,ShuhuaiRen,MukaiLi,YazhengYang,
JingjingXu,XuSun,etal. Alarge-scaledatasettowardsmulti-modalmultilingualinstructiontuning.
arXivpreprintarXiv:2306.04387,3,2023.
[41] BoZhao,BoyaWu,MuyangHe,andTiejunHuang. Svit: Scalingupvisualinstructiontuning. arXiv
preprintarXiv:2307.04087,2023.
[42] JunkeWang,LingchenMeng,ZejiaWeng,BoHe,ZuxuanWu,andYu-GangJiang. Toseeistobelieve:
Promptinggpt-4vforbettervisualinstructiontuning. arXivpreprintarXiv:2311.07574,2023.
[43] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,FanyiPu,JingkangYang,ChunyuanLi,and
ZiweiLiu. Mimic-it: Multi-modalin-contextinstructiontuning. arXivpreprintarXiv:2306.05425,
2023.
[44] Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal
Shankar. Datafilteringnetworks. arXivpreprintarXiv:2309.17425,2023.
[45] XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer. Sigmoidlossforlanguage
imagepre-training. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages11975–11986,2023.
[46] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee. Llava-
next: Improvedreasoning,ocr,andworldknowledge,January2024. URLhttps://llava-vl.
github.io/blog/2024-01-30-llava-next/.
17[47] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-
JuiFu, WilliamYangWang, Shih-FuChang, ZheGan, andYinfeiYang. Ferret-v2: AnImproved
Baseline for Referring and Grounding with Large Language Models, April 2024. URL http:
//arxiv.org/abs/2404.07973.
[48] XiaoyiDong,PanZhang,YuhangZang,YuhangCao,BinWang,LinkeOuyang,SongyangZhang,
HaodongDuan,WenweiZhang,YiningLi,HangYan,YangGao,ZheChen,XinyueZhang,WeiLi,
JingwenLi,WenhaiWang,KaiChen,ConghuiHe,XingchengZhang,JifengDai,YuQiao,Dahua
Lin,andJiaqiWang. InternLM-XComposer2-4KHD:APioneeringLargeVision-LanguageModel
HandlingResolutionsfrom336Pixelsto4KHD,April2024. URLhttp://arxiv.org/abs/
2404.06512.
[49] ShengbangTong,EllisBrown,PenghaoWu,SanghyunWoo,ManojMiddepogu,SaiCharithaAkula,
JihanYang,ShushengYang,AdithyaIyer,XichenPan,AustinWang,RobFergus,YannLeCun,and
SainingXie. Cambrian-1: AFullyOpen,Vision-CentricExplorationofMultimodalLLMs,June2024.
URLhttp://arxiv.org/abs/2406.16860. arXiv:2406.16860[cs].
[50] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,andChelsea
Finn. Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel. Advancesin
NeuralInformationProcessingSystems,36,2024.
[51] https://github.com/PFCCLab/PPOCRLabel. Awesomemultilingualocrtoolkitsbasedonpaddlepaddle.
[52] LichengYu,PatrickPoirson,ShanYang,AlexanderCBerg,andTamaraLBerg. Modelingcontextin
referringexpressions. InComputerVision–ECCV2016: 14thEuropeanConference,Amsterdam,The
Netherlands,October11-14,2016,Proceedings,PartII14,pages69–85.Springer,2016.
[53] YoucaiZhang,XinyuHuang,JinyuMa,ZhaoyangLi,ZhaochuanLuo,YanchunXie,YuzhuoQin,Tong
Luo,YaqianLi,ShilongLiu,etal. Recognizeanything: Astrongimagetaggingmodel. InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages1724–1732,2024.
[54] ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,HaoZhang,JieYang,ChunyuanLi,JianweiYang,
Hang Su, JunZhu, et al. Groundingdino: Marryingdino withgrounded pre-trainingfor open-set
objectdetection. arXivpreprintarXiv:2303.05499,2023.
[55] AnYan,ZhengyuanYang,JundaWu,WanrongZhu,JianweiYang,LinjieLi,KevinLin,Jianfeng
Wang,JulianMcAuley,JianfengGao,etal. Listitemsonebyone: Anewdatasourceandlearning
paradigmformultimodalllms. arXivpreprintarXiv:2404.16375,2024.
[56] Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua
Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint
arXiv:2311.12793,2023.
[57] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus
Rohrbach. Towardsvqamodelsthatcanread. InProceedingsoftheIEEEConferenceonComputer
VisionandPatternRecognition,pages8317–8326,2019.
[58] AnandMishra,ShashankShekhar,AjeetKumarSingh,andAnirbanChakraborty. Ocr-vqa: Visual
questionansweringbyreadingtextinimages. In2019InternationalConferenceonDocumentAnalysis
andRecognition(ICDAR),pages947–952,2019. doi: 10.1109/ICDAR.2019.00156.
[59] DustinSchwenk,ApoorvKhandelwal,ChristopherClark,KennethMarino,andRoozbehMottaghi.
A-okvqa: Abenchmarkforvisualquestionansweringusingworldknowledge. InComputerVision–
ECCV2022: 17thEuropeanConference,TelAviv,Israel,October23–27,2022,Proceedings,PartVIII,
page146–162,Berlin,Heidelberg,2022.Springer-Verlag. ISBN978-3-031-20073-1. doi: 10.1007/
978-3-031-20074-8_9. URLhttps://doi.org/10.1007/978-3-031-20074-8_9.
18[60] DrewA.HudsonandChristopherD.Manning. Gqa: Anewdatasetforreal-worldvisualreasoningand
compositionalquestionanswering. In2019IEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),pages6693–6702,2019. doi: 10.1109/CVPR.2019.00686.
[61] MineshMathew,DimosthenisKaratzas,andC.V.Jawahar. Docvqa: Adatasetforvqaondocument
images. In2021IEEEWinterConferenceonApplicationsofComputerVision(WACV),pages2199–
2208,2021. doi: 10.1109/WACV48630.2021.00225.
[62] Ahmed Masry, Do Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: A bench-
mark for question answering about charts with visual and logical reasoning. In Findings of the
Association for Computational Linguistics: ACL 2022, pages 2263–2279, Dublin, Ireland, May
2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.findings-acl.177. URL
https://aclanthology.org/2022.findings-acl.177.
[63] KushalKafle,ScottCohen,BrianPrice,andChristopherKanan. Dvqa: Understandingdatavisualiza-
tionsviaquestionanswering. InCVPR,2018.
[64] AniruddhaKembhavi,MikeSalvato,EricKolve,MinjoonSeo,HannanehHajishirzi,andAliFarhadi.
Adiagramisworthadozenimages. InBastianLeibe,JiriMatas,NicuSebe,andMaxWelling,editors,
ComputerVision–ECCV2016.SpringerInternationalPublishing,2016. ISBN978-3-319-46493-0.
[65] JustinJohnson,BharathHariharan,LaurensvanderMaaten,LiFei-Fei,C.LawrenceZitnick,andRoss
Girshick. Clevr: Adiagnosticdatasetforcompositionallanguageandelementaryvisualreasoning. In
2017IEEEConferenceonComputerVisionandPatternRecognition(CVPR),pages1988–1997,2017.
doi: 10.1109/CVPR.2017.215.
[66] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,OyvindTafjord,
Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for
sciencequestionanswering. InS.Koyejo,S.Mohamed,A.Agarwal,D.Belgrave,K.Cho,andA.Oh,
editors,AdvancesinNeuralInformationProcessingSystems,volume35,pages2507–2521.Curran
Associates,Inc.,2022.URLhttps://proceedings.neurips.cc/paper_files/paper/
2022/file/11332b6b6cf4485b84afadb1352d3a9a-Paper-Conference.pdf.
[67] Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and "Teknium".
Openorca: An open dataset of gpt augmented flan reasoning traces. https://https://
huggingface.co/Open-Orca/OpenOrca,2023.
[68] ArindamMitra,HamedKhanpour,CorbyRosset,andAhmedAwadallah. Orca-math: Unlockingthe
potentialofslmsingradeschoolmath,2024.
[69] DongfuJiang,XuanHe,HuayeZeng,CongWei,MaxKu,QianLiu,andWenhuChen. MANTIS:
InterleavedMulti-ImageInstructionTuning,May2024. URLhttp://arxiv.org/abs/2405.
01483. arXiv:2405.01483[cs].
[70] ZiyuLiu,TaoChu,YuhangZang,XilinWei,XiaoyiDong,PanZhang,ZijianLiang,YuanjunXiong,
YuQiao,DahuaLin,andJiaqiWang.Mmdu:Amulti-turnmulti-imagedialogunderstandingbenchmark
andinstruction-tuningdatasetforlvlms,2024. URLhttps://arxiv.org/abs/2406.11833.
[71] LeiLi,ZhihuiXie,MukaiLi,ShunianChen,PeiyiWang,LiangChen,YazhengYang,BenyouWang,
andLingpengKong. Silkie: Preferencedistillationforlargevisuallanguagemodels. arXivpreprint
arXiv:2312.10665,2023.
[72] EdwardJHu,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,WeizhuChen,
etal. Lora: Low-rankadaptationoflargelanguagemodels. InInternationalConferenceonLearning
Representations.
19[73] Yongshuo Zong, Ondrej Bohdal, Tingyang Yu, Yongxin Yang, and Timothy Hospedales. Safety
fine-tuningat(almost)nocost: Abaselineforvisionlargelanguagemodels. InThe41stInternational
ConferenceonMachineLearning,2024.
[74] BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-bench: Bench-
markingmultimodalllmswithgenerativecomprehension. arXivpreprintarXiv:2307.16125,2023.
[75] YuanLiu,HaodongDuan,YuanhanZhang,BoLi,SongyangZhang,WangboZhao,YikeYuan,Jiaqi
Wang, ConghuiHe, ZiweiLiu, etal. Mmbench: Isyourmulti-modalmodelanall-aroundplayer?
arXivpreprintarXiv:2307.06281,2023.
[76] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,ZhenyuQiu,WeiLin,
JinruiYang,XiawuZheng,KeLi,XingSun,andRongrongJi. MME:Acomprehensiveevaluation
benchmarkformultimodallargelanguagemodels. CoRR,abs/2306.13394,2023.
[77] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Zehui Chen, Haodong Duan, Jiaqi
Wang,YuQiao,DahuaLin,etal. Areweontherightwayforevaluatinglargevision-languagemodels?
arXivpreprintarXiv:2403.20330,2024.
[78] ai. Grok-1.5visionpreview,2024. URLhttps://x.ai/blog/grok-1.5v.
[79] XiangYue,YuanshengNi,KaiZhang,TianyuZheng,RuoqiLiu,GeZhang,SamuelStevens,Dongfu
Jiang,WeimingRen,YuxuanSun,CongWei,BotaoYu,RuibinYuan,RenliangSun,MingYin,Boyuan
Zheng,ZhenzhuYang,YiboLiu,WenhaoHuang,HuanSun,YuSu,andWenhuChen. MMMU:A
massivemulti-disciplinemultimodalunderstandingandreasoningbenchmarkforexpertAGI. CoRR,
abs/2311.16502,2023.
[80] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng,
Kai-WeiChang, MichelGalley, andJianfengGao. Mathvista: Evaluatingmathematicalreasoning
offoundationmodelsinvisualcontexts. InInternationalConferenceonLearningRepresentations
(ICLR),2024.
[81] YuliangLiu,ZhangLi,BiaoYang,ChunyuanLi,XuchengYin,ChenglinLiu,LianwenJin,andXiang
Bai. Onthehiddenmysteryofocrinlargemultimodalmodels,2024. URLhttps://arxiv.org/
abs/2305.07895.
[82] TianruiGuan,FuxiaoLiu,XiyangWu,RuiqiXian,ZongxiaLi,XiaoyuLiu,XijunWang,Lichang
Chen,FurongHuang,YaserYacoob,etal. Hallusionbench: anadvanceddiagnosticsuiteforentangled
language hallucination and visual illusion in large vision-language models. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pages14375–14385,2024.
[83] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object
hallucination in large vision-language models. In The 2023 Conference on Empirical Methods in
NaturalLanguageProcessing.
[84] Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li,
WenxiuSun,QiongYan,GuangtaoZhai,andWeisiLin. Q-bench: Abenchmarkforgeneral-purpose
foundationmodelsonlow-levelvision. InICLR.OpenReview.net,2024.
[85] FeiWang,XingyuFu,JamesY.Huang,ZekunLi,QinLiu,XiaogengLiu,MingyuDerekMa,Nan
Xu, Wenxuan Zhou, Kai Zhang, Tianyi Lorena Yan, Wenjie Jacky Mo, Hsiang-Hui Liu, Pan Lu,
ChunyuanLi,ChaoweiXiao,Kai-WeiChang,DanRoth,ShengZhang,HoifungPoon,andMuhao
Chen. Muirbench: A comprehensive benchmark for robust multi-image understanding. CoRR,
abs/2406.09411,2024.
20[86] XingyuFu,YushiHu,BangzhengLi,YuFeng,HaoyuWang,XudongLin,DanRoth,NoahA.Smith,
Wei-ChiuMa,andRanjayKrishna.BLINK:multimodallargelanguagemodelscanseebutnotperceive.
CoRR,abs/2404.12390,2024.
[87] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,Matthias
Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman.
Trainingverifierstosolvemathwordproblems. CoRR,abs/2110.14168,2021.
[88] TianyuZheng,GeZhang,TianhaoShen,XuelingLiu,BillYuchenLin,JieFu,WenhuChen,andXiang
Yue. Opencodeinterpreter: Integratingcodegenerationwithexecutionandrefinement,2024. URL
https://arxiv.org/abs/2402.14658.
21