Flexible Bayesian Last Layer Models Using Implicit Priors and Diffusion Posterior
Sampling
JianXu1, Zhiqi Lin1, Shigui Li1, MinChen1, Junmei Yang1,
DeluZeng1, JohnPaisley2
1SouthChinaUniversityofTechnoledge
2ColumbiaUniversity
Abstract theseobservationsinmindcanimproveperformanceonim-
age classification tasks. (Fortuin 2022) underscore the im-
Bayesian Last Layer (BLL) models focus solely on uncer-
portance of prior selection in Bayesian deep learning, ex-
taintyintheoutput layer ofneural networks, demonstrating
ploring various priors for deep Gaussian processes (Dami-
comparableperformancetomorecomplexBayesianmodels.
anou and Lawrence 2013), variational autoencoders(Doer-
However, the use of Gaussian priors for last layer weights
inBayesianLastLayer(BLL)modelslimitstheirexpressive sch2016),andBayesianneuralnetworks(Kononenko1989),
capacitywhenfacedwithnon-Gaussian,outlier-rich,orhigh- whilealsodiscussingmethodsforlearningpriorsfromdata.
dimensionaldatasets.Toaddressthisshortfall,weintroduce Their work encourages practitioners to carefully consider
anovelapproachthatcombinesdiffusiontechniquesandim- priorspecificationandprovidesinspirationforthisaspect.
plicit priors for variational learning of Bayesian last layer Driven by the need for enhanced model flexibility and
weights.Thismethodleveragesimplicitdistributionsformod- performance,weproposeaninnovativeapproachthatlever-
elingweightpriorsinBLL,coupledwithdiffusionsamplers
agesimplicitpriorsforvariationallearningofBayesianlast
for approximating true posterior predictions, thereby estab-
layer weights. Implicit priors are parameterized through a
lishing a comprehensive Bayesian prior and posterior esti-
neuralnetwork,replacingtraditionalGaussianweightdistri-
mation strategy. By delivering an explicit and computation-
butionsto achievegreaterflexibility.Thismethodconnects
ally efficient variational lower bound, our method aims to
augment the expressive abilitiesof BLL models, enhancing to the migration of variational implicit processes (Ma, Li,
modelaccuracy,calibration,andout-of-distributiondetection andHernández-Lobato2019)intotheBLLmodel,offering
proficiency. Through detailed exploration and experimental novelinsightsandopportunities.Byemployingimplicitdis-
validation, We showcase the method’s potential for improv- tributionsfor weightpriors, ourapproachaims to establish
ingpredictiveaccuracyanduncertaintyquantificationwhile arobuststrategyforBayesianpriorestimation.However,as
ensuringcomputationalefficiency. modelcomplexityincreases, inferencebecomesmore chal-
lenging,posingnewobstacles.
Introduction To address this, we shift to directly utilizing diffusion
models (Ho, Jain, and Abbeel 2020; Rombach et al. 2022;
BayesianLastLayer(BLL)models(Watsonetal.2021;Har-
Vargas,Grathwohl,andDoucet2023)forposteriorsampling.
rison, Sharma, and Pavone 2020;Kristiadi, Hein, and Hen-
Thisapproachenablesustoeffectivelycapturecomplexde-
nig 2020; Harrison, Willes, and Snoek 2023; Fiedler and
pendenciesand correlationsamonglatentvariables.By uti-
Lucia 2023) have emerged as a robust framework for un-
lizingdiffusionstochasticdifferentialequations(SDEs)and
certaintyquantificationinneuralnetworks,concentratingon
incorporatingelementssimilartoscorematching(Songetal.
theuncertaintyinherentinthefinallayerweights.However,
2020),weformulateanovelvariationallowerboundforthe
thesemodelsoftenutilizeGaussianpriorsfortheweightdis-
marginal likelihood function through KL divergence mini-
tributions,whichmaybeinsufficientforcapturingthecom-
mization.
plexityofnon-Gaussian,outlier-prone,orhigh-dimensional
Additional, we delve into the details of our proposed
data. This constraint can limit the expressiveness of BLL
methodand demonstrate its potentialthroughextensiveex-
modelsandadverselyaffecttheirperformanceinmorechal-
perimentalvalidation.By introducingacomputationallyef-
lengingscenarios.
ficient variationallower bound and showcasing its efficacy
Prior research highlights the critical need to enhance
in scenarios with non-Gaussian distributions, outliers, and
model flexibility through more adaptable priors. For in-
high-dimensionaldata,we highlightthesignificanceofour
stance, (Fortuin et al. 2021) demonstrated that isotropic
approach in advancing uncertainty quantification in neural
Gaussian priors may inadequately represent the true dis-
networks.Overall,ourcontributionsareasfollows:
tribution of weights in Bayesian neural networks, poten-
tially compromising performance. They observed signifi- • We proposed an innovative approach that utilizes im-
cant spatial correlations in convolutional neural networks plicit priors for variational learning of Bayesian last
and ResNets, as well as heavy-tailed distributions in fully layerweights.ThismethodreplacestraditionalGaussian
connected networks, suggesting that priors designed with weightparameterswithpriordistributionsparameterized
4202
guA
7
]GL.sc[
1v64730.8042:viXrathroughaneuralnetworktoachievemoreflexiblepriors. k(x,x′;θ) = φ(x;θ)⊤Λ−1φ(x′;θ) (Williams and Ras-
0
mussen 2006).For a more rigorousBayesian treatment, an
• Wedirectlyemployeddiffusionmodelsforposteriorsam-
inversegammapriorcanbeplacedonσ2,elicitingaStudent-
pling and then constructed a new objective that accu-
tweightposteriorandpredictivedensity.
rately captures the complex dependencies and correla-
tionsamonglatentvariables.Thisapproachexplicitlyde-
VariationalBayesian LastLayer
rives a variational lower bound for the marginal likeli-
hoodfunctionthroughKLdivergenceminimization. To leverage exact marginalization while avoiding the com-
• Weconductextensiveexperimentstodemonstratetheef- putational burden of full marginal likelihood computation,
fectiveness of the proposed method in handling regres- VariationalBayesianLastLayer(VBLL)(Harrison,Willes,
sion and image classification datasets. These tests high- and Snoek 2023) employs stochastic variational inference
lightthemethod’simpactonimprovinguncertaintyquan- (Hoffman et al. 2013). The objective is to jointly compute
tificationinneuralnetworks,demonstratingitseffective- an approximate posterior for the last layer parameters and
nessandrobustness. optimizenetworkweightsbymaximizinglower boundson
themarginallikelihood.Specifically,VBLLaimstofindan
Background
approximateposteriorq(β|η)parameterizedbyη.
Given a mini-batch D with |X | = B, where I ⊂
I I
BLLmodels {1,2,...,N} is the set of indices for any mini-batch, and
InthecontextofBayesianLastLayer(BLL)networks,these thelogmarginallikelihoodlogp(y|x,θ) withmarginalized
models can be understood as Bayesian linear regression parameters q(β|η) = N(w,S), they derive bounds of the
frameworksappliedwithinafeaturespacethatisadaptively form:
learned through neural network architectures. Another per-
B
N N
spectiveistoviewthemasneuralnetworkswheretheparam- logp(y |X ,θ)≥ [logN(y |w⊤φ ,σ2I)
I I i i
etersofthefinallayerundergorigorousBayesianinference. B B
i=1 (4)
While BLL networks are capable of handling multivariate X
1 N
regressiontasks,thisdiscussionwillbeconfinedtothecase − σ−2φ⊤Sφ ]− KL(q(β|η)kp(β)),
2 t t B
ofunivariatetargetsforthesakeofsimplicity.
Let the dataset D = {(x ,y )}N consist of observed Thisformulationresultsinamini-batchalgorithmforvari-
i i i=1
data,wherex ∈ RD andy ∈ R. We denoteX ∈ RN×D ationalBayesianinferenceinneuralnetworks.
i i
and y ∈ RN. We define a parameterizedfunctionφ(·;θ) :
RD → RM forprojectingfeatures, with θ representingits FlexiblePriorsinBayesianNeuralNetworks
parameters, and φ = φ(x ;θ) and Φ = φ⊤...φ⊤ ∈ Inpriorresearch,therehasbeenastrongemphasisontheim-
i i 1 N
RN×M, where the latter denotes a matrix of stacked row portanceofenhancingmodelflexibilitythroughmoreflexi-
vectors. (cid:2) (cid:3) blepriors,particularlyinthecontextofBayesianneuralnet-
A latent function f is modeled utilizing Bayesian linear works. Previous studies (Fortuin et al. 2021; Fortuin 2022)
regression (Bishop 2006; Hill 1974; Murphy 2023), incor- have highlightedthat applying isotropic Gaussian priorsto
poratingweightsβ ∈ RM andzero-meanGaussiannoiseǫ weightsmaynotfullycapturetruebeliefsaboutweightdis-
withvarianceσ2,where tributions,thushinderingoptimalperformance,asdescribed
inSection .
y i =f(x i;θ)=φ⊤ i β+ǫ i. (1) Thesestudiesprovidevaluableinsightssuggestingthatin-
troducingmoreflexibleweightpriorsinBayesianlinearre-
ByimposingaconjugateGaussianpriorN µ ,Λ−1 over
0 0 gressionmodelscouldenhancemodelflexibilityandperfor-
β, a Gaussian posterior N µ n,Λ− n1 is ob (cid:0)tained (W (cid:1)atson mance.Thisflexibilitymayinvolvedesigningpriordistribu-
etal.2021;Harrison,Willes,andSnoek2023), tionsbased onspecific task or data characteristicsto better
(cid:0) (cid:1)
capturethetruedatadistributionfeatures,therebyimproving
µ =(Λ +σ−2ΦTΦ)−1(Λ µ +σ−2ΦTy)
n 0 0 0 modelgeneralizationandperformance.
(2)
Λ =Λ +σ−2ΦTΦ,
n 0
Method
yielding an explicit Gaussian predictive distribution for a
givenqueryx, ImplictPrior
In continuation of techniques for generating implicit pro-
y |x,D,θ ∼N ·|φ⊤µ ,σ2+φ⊤Λ−1φ , (3)
x n x n x cesses from prior research on stochastic processes (Ma,
Li, and Hernández-Lobato 2019; Santana, Zaldivar, and
whereµ nandΛ ndeno(cid:0)tethemeanvectorandpreci(cid:1)sionma-
Hernandez-Lobato2021;MaandHernández-Lobato2021),
trixoftheposteriorweightdistribution,respectively.
The parameters encompassing the observation noise σ2, weintroducearegressionmodelwithanimplicitpriorover
theweightsβ:
thepriorweightparametersµ andΛ ,andθ,canbeopti-
0 0
mizedjointlythroughthemaximizationofthelog-marginal
y =f(x ;θ)=φ⊤β+ǫ ,
likelihood. In the scenario where µ = 0, this model i i i i (5)
0
aligns equivalently with a Gaussian process, with a kernel
β =G ψ(ω),ω ∼p(ω),where ω ∈ RK serves as an auxiliary variable to gener- Score Matching and Reference Process Trick If we
ate weightsβ, with p(ω)representingits priordistribution, could approximately simulate the diffusion process de-
which can be a simple distribution such as a Gaussian dis- scribed in (7), we would obtain approximate samples
tribution.AndG (·) : RK → RM beinganeuralnetwork from the target distribution q. However, implementing
ψ
parameterizedweightparametergenerator.Weusethishier- this idea requires approximating the intractable scores
archicalmodeltoreframetheBLLmodel.GiventhatG is (∇lnp (·)) . To achieve this, DDPM (Ho, Jain, and
ψ t t∈[0,T]
typicallyanon-linearfunction,weanticipatethattheweight Abbeel 2020; Song et al. 2020) relies on score matching
priorp(β)generatedbytheauxiliaryvariableβthroughG ψ techniques. Specifically, to approximate P, we consider a
will exhibitgreater expressiveness than the original model, pathmeasurePγ whosetime-reversalisdefinedby
capableofyieldingnon-Gaussiandistributions.
However, dealing with the intractable implicit model d← ω−γ = −λ(T −t)← ω−γ +g(T −t)2s T −t,← ω−γ dt
t t γ t
p ch(y alle| ngx es,θ as,ψ m, oω d) eld ce ofi mn ped lexb ity yE inq c. re( a5 s) esp ,o rs ee ns des ri ig nn gifi trc aa dn it
-
+g(T −(cid:0)t)dW t, ← ω−γ
0
∼N(0,σ 02I),
(cid:0) (cid:1)(cid:1)
(8)
tionalmean-fieldinferencemethodslikethosesuggestedin so that the backward process ← ω−γ ∼ Qγ. To obtain
VBLL(Hoffman et al. 2013; Harrison, Willes, and Snoek t t
s (t,·) ≈ ∇lnp (·), we parameterizes (t,·) usinga neu-
2023)inadequatefor our modified model. As the complex- γ t γ
ral network, with the parameters obtained by minimizing
ityofthemodelincreases,theposteriordistributionbecomes
KL(P||Pγ). Unlike traditional score matching techniques,
moreintricate.Consequently,itisnaturaltodesignmoreso- given that we can only obtain samples from Qγ, we alter-
phisticated posteriors to model Bayesian networks and im- t
nativelyminimizeKL(Pγ||P),byGirsanov’stheorem(Ok-
proveestimationaccuracy.Toaddressthisissue,weexplore
sendal2013)
anapproachcenteredonposteriorsamplingusingdiffusion
models(Songetal.2020). KL(Pγ||P)=KL(Qγ||Q)
DiffusionPosterior Sampling =KL(N(0,σ 02I)||p T)+KL(Qγ(·|← ω−γ 0)||Q(·|← ω− 0))
TPa imra em -Re et ver ei rz si an lg RA epu rx ei sli ea nr ty atiV onari oa fbl De iffP uo ss iote nrio Sr Ds EUs Oin ug r ≈1 2 T E Qγ t[g(T −t)2k∇lnp T−t(← ω−γ t)−s γ(T −t,← ω−γ t)k2 2]dt
goalistosamplefromtheposteriordistributionofauxiliary
Z0
(9)
variableq(ω),definedbyBayes’ruleasq(ω)= p(y| pω (y)p )(ω), However,althoughwecanobtainsamplesfromQγ t bysimu-
latingtheSDE(8),dealingwiththenonlineardriftfunction
where p(ω) represents the prior and p(y) represents the
ofSDE(8)makesitdifficulttoobtain∇lnp
(← ω−γ)inEq.
modelmarginallikelihood.Followingsimilarsetupsinprior T−t t
(9).
works(TzenandRaginsky2019;ZhangandChen2021;Var-
We use an alternative approach by constructing a refer-
gas, Grathwohl, and Doucet 2023), we begin by sampling
from a Gaussian distribution N(0,σ2I), where σ ∈ R+ ence process (Zhang and Chen 2021; Vargas, Grathwohl,
0 0 and Doucet 2023), denoted as Pref, to assist in measuring
isthecovarianceparameter.We thenfollowatime-reversal
KL(Pγ||P).Firstly,observethefollowingfact:
processoftheforwarddiffusionstochasticdifferentialequa-
tion(SDE): dPγ
d−→
ω
=−λ(t)−→
ω dt+g(t)dB ,
−→
ω ∼q, t∈[0,T],
KL(Pγ||P)=E Pγlog
dP
t t t 0 (10)
(6) dPγ dPref
where−λ(t)∈Risthedriftcoefficient,g(t)∈Risthedif- =E Pγlog
dPref
+E Pγ log
dP
,
fusioncoefficient,and(B ) isaK-dimensionalBrow-
t t∈[0,T]
nianmotion.ThisdiffusioninducesthepathmeasureP on wherethestochasticprocessKLisrepresentedastheRadon-
the time interval [0,T], and the marginal density of −→ ω is Nikodymderivative.GiventhespecificforminEq.(10),we
t
denotedp .Notethatbydefinition,wealwayshavep = q definethereferenceprocessPref tofollowthediffusionfor-
whenusint ganSDEtoperturbthisdistribution.Accord0 ingto mulaasinEq.(6),butinitializedatpr 0ef(−→ ωr 0ef)=N ←−(0,σ 02I)
(Anderson 1982;Haussmann and Pardoux 1986),the time- insteadofq,whichalignswiththedistributionof ω 0 inEq.
reversalrepresentationof Eq. (6)is givenby ← ω− = −→ ω (8),
t T−t
(whereequalityisindistribution).Thissatisfies: d−→ ωref =λ(t)−→ ωrefdt+g(t)dB , −→ ωref ∼N(0,σ2I).
d← ω− = −λ(T −t)← ω− +g(T −t)2∇lnp ← ω− dt t t t 0 0 (11)
t t T−t t −→ −→
+g(T −t)dW ,
← ω−
∼p ,
The transition kernel p t(ωr tef|ωr 0ef) is always a Gaussian
(cid:0) t 0 T (cid:0) (cid:1)(cid:1) distribution N(l ,Σ ), where the mean l and variance Σ
(7) t t t t
areoftenavailableinclosedform(SärkkäandSolin2019):
where(W ) isanotherK-dimensionalBrownianmo-
t t∈[0,T]
tion. In DDPM (Ho, Jain, and Abbeel 2020; Song et al. dl
2 an0 d20 e), nt sh ui rs et sim the a-r tev ← ω−ersal ∼star qts .f Tro hm en← ω− w0 e∼ cp anT ≈ parN am(0 e, teσ r02 izI e) dtt =−λ(t)l t, l 0 =0, (12)
T dΣ
the transition probability T (ω
ts+1
|ω ts) in the Euler dis- dtt =−2λ(t)Σ t+g(t)2I, Σ
0
=σ 02I.
cretized form (Särkkä and Solin 2019) of Eq. (7) for steps
t ∈{0,...,T −1}. By solving these ordinary differential equations (Hale and
sLunel2013),weobtainthegeneralsolutionsasfollows: Evidence Lower Bound Let l (γ) = E log dPγ .
1 Pγ dPref
l
t
=l 0e−R 0tλ(s)ds, C loo wm erbi bn oin ug ndE lq (. γ( ,5 θ, ,1 ψ0, )1 f7 o, r1 th8 e), mw ae rgo ib nt aa lin lika en lie hw oov dar li oa gti po (n ya )l
t (13) inourmethod,
Σ
t
= g(r)2eR 0rλ(s)dsdrI +Σ
0
e−R 0tλ(s)ds.
(cid:18)Z0 (cid:19) logp(y)
AccordingtoEq.(13),wecanderivethatforanyt,thedis- N(0,σ2I)
tributionpr tef of−→ ωr tef isazero-meanGaussiandistribution: =KL(Pγ||P)−l 1(γ)−E Qγ
T
log p(y|·)p0
(·)
pr tef(−→ ωr tef)= p t(−→ ωr tef|−→ ωr 0ef)p t(−→ ωr 0ef)d−→ ωr 0ef
(14)
=KL(Pγ||P)−l 1(γ)−E Qγ
T
logN(0,σ 02I)+E Qγ
T
logp(·)
=NZ (0,κ tI), +E Qγ T logp(y|x,θ,ψ,·)
wherethevarianceκ tisgivenby ≥−l 1(γ)−E Qγ
T
logN(0,σ 02I)+E Qγ
T
logp(·)
κ
t
= t g(r)2eR 0rλ(s)dsdr+σ 02 e−R 0tλ(s)ds. + =E l(Q γγ T ,θlo ,g ψp )(y|x,θ,ψ,·)
(cid:18)Z0 (cid:19)
(19)
Meanwhile, the SDE equation for the reverse process Qref
Inourderivation,p(·)representsthepriorfunctionofω.We
ofPref is
introduce a new variational lower bound for logp(y). Un-
d← ω−ref = −λ(T −t)← ω−ref +g(T −t)2∇lnpref ← ω−ref dt likethemean-fieldvariationalinferencemodelthatapproxi-
t t T−t t
+g(cid:0)(T −t)dW t, ← ω−r 0ef ∼pr Tef. (cid:0) (cid:1)(cid:1) m sioa ntes prq ow cei sth sa toG aa pu ps rs oia xn imd ais tetri tb hu eti po on s, to eu rir om
r
do id se trl iu bs ue tis oa n.di Tff hu e-
(15)
flexibility of the denoising neural network γ provides our
According to Eq. (14), we can derive an analytical expres-
modelwithasignificantadvantageinaccuratelyapproximat-
sion for the derivative of the log-likelihood function with
←− ingtheposteriordistribution.
respectto ωref:
t
←−
∇lnpref ← ω−ref =− ωr tef . (16) StochasticGradientDescent
T−t t κ T−t For ease of sampling, we consider a reparameterization
To compute KL(Pγ||P),(cid:0)we ca(cid:1)lculate the first term of Eq. version of Eq. (19) based on the approaximate transition
probabilityT (ω |ω )givenby
(10).UsingthechainruleforKLdivergenceandGirsanov’s γ ts+1 ts
theorem(Oksendal2013),andincorporatingEqs.(8,15,16),
T (ω )=ω −λ(T −t )ω +g(T −t )2s (T −t ,ω )
weobtain: γ ts+1 ts s ts s γ s ts
dPγ
+g(T −t)ǫ ts.
E Pγ log
dPref
=KL PγkPref =KL QγkQref (20)
=KL N(0,σ 02I)kpr Tef (cid:0)+KL Qγ(cid:1)(·|← ω−γ 0)k(cid:0)Q(·|← ω−r 0ef)(cid:1) where ǫ ts ∼ N(0,I). In order to accelerate training
and sampling in our inference scheme, we propose a scal-
=KL((cid:0)N(0,σ 02I)kpr Tef)(cid:1) (cid:0) (cid:1) able variational bounds that are tractable in the large data
+1
2
T E Qγ
t
g(T −t)2 κ← ω−γ t +s γ(T −t,← ω−γ t) 2 dt. r ae ng dim We elb lia ns ged 20o 1n 3;st Hoc oh ffa msti ac nv aa nr dia Btio len ia 2l 0i 1n 5fe ;r Sen alc ie m( bK enin ig am nda
Z0 " (cid:13) T−t (cid:13)2# Deisenroth 2017; Naesseth, Lindsten, and Blei 2020) and
(cid:13) (cid:13) (17)
(cid:13) (cid:13) stochastic gradient descent (Welling and Teh 2011; Chen,
At this point, we can sim(cid:13)ulate the SDE (8) to c(cid:13)ompute
Fox, and Guestrin 2014; Zou, Xu, and Gu 2019; Alexos,
the first term in Eq. (10). the integral term can be com-
Boyd,andMandt2022).OurmodelisshowninAlgorithm
putedusingeitherODEsolvers(Chenetal.2018)orbyem-
1,referredtoasDVI-IBLL.
ployingRiemannsummationmethods.Forthesecondterm,
E logdPref,wecanseefromEq.(6)andEq.(11)thatP PredictionDistribution
Pγ dP
andPref havethesamedynamicsystemτ,exceptfordiffer- For making predictions in our model, the prediction under
entinitialvalues.Therefore,wehave the variational posterior distribution is approximated for a
testinput/label(x⋆,y⋆)as:
dPref Pref(τ|·)pref(·)
E log =E log 0
Pγ
dP
Pγ
P(τ|·)p 0(·) p(y⋆ |x⋆,X,y)≈E Qγ
T
[p(y⋆ |x⋆,θ,ψ,ω)] (21)
pref(·) N(0,σ2I)
=E Qγ
T
log p0
(·)
=E Qγ
T
log
q
0 Here, Qγ
T
denotes the output of the diffusion process at
0 timeT.Theexpressionp(y⋆ |x⋆,θ,ψ,ω)canbeobtained
=E Qγ
T
logN p(( y0 |, ·)σ p02 (I ·)) +logp(y) b fry omsu Ebs qt .it (u 5t )i .n Fg ot rh ce lai sn sp iu fit c/ ao tu iotp nu tt aw ski sth orth oe tht ee rst lis ke et lii hn op ou dt/ fla ub ne cl
-
(18) tions,thesubstitutioncanbemadeaccordinglyduringtrain-
ing.Table1:ResultsforUCIregressiontasks.
BOSTON CONCRETE ENERGY
NLL(↓) RMSE(↓) NLL(↓) RMSE(↓) NLL(↓) RMSE(↓)
VBLL 2.55±0.06 2.92±0.12 3.22±0.07 5.09±0.13 1.37±0.08 0.87±0.04
GBLL 2.90±0.05 4.19±0.17 3.09±0.03 5.01±0.18 0.69±0.03 0.46±0.02
LDGBLL 2.60±0.04 3.38±0.18 2.97±0.03 4.80±0.18 4.80±0.18 0.50±0.02
MAP 2.60±0.07 3.02±0.17 3.04±0.04 4.75±0.12 1.44±0.09 0.53±0.01
RBFGP 2.41±0.06 2.83±0.16 3.08±0.02 5.62±0.13 0.66±0.04 0.47±0.01
Dropout 2.36±0.04 2.78±0.16 2.90±0.02 4.45±0.11 1.33±0.00 0.53±0.01
Ensemble 2.48±0.09 2.79±0.17 3.04±0.08 4.55±0.12 0.58±0.07 0.41±0.02
SWAG 2.64±0.16 3.08±0.35 3.19±0.05 5.50±0.16 1.23±0.08 0.93±0.09
BBB 2.39±0.04 2.74±0.16 2.97±0.03 4.80±0.13 0.63±0.05 0.43±0.01
DVI-IBLL(ours) 2.12±0.05 2.49±0.10 2.66±0.04 4.08±0.11 1.19±0.11 0.71±0.03
Table2:FurtherresultsforUCIregressiontasks.
POWER WINE YACHT
NLL(↓) RMSE(↓) NLL(↓) RMSE(↓) NLL(↓) RMSE(↓)
VBLL 2.73±0.01 3.68±0.03 1.02±0.03 0.65±0.01 1.29±0.17 0.86±0.17
GBLL 2.77±0.01 3.85±0.03 1.02±0.01 0.64±0.01 1.67±0.11 1.09±0.09
LDGBLL 2.77±0.01 3.85±0.04 1.02±0.01 0.64±0.01 1.13±0.06 0.75±0.10
MAP 2.77±0.01 3.81±0.04 0.96±0.01 0.63±0.01 5.14±1.62 0.94±0.09
RBFGP 2.76±0.01 3.72±0.04 0.45±0.01 0.56±0.05 0.17±0.03 0.40±0.03
Dropout 2.80±0.01 3.90±0.04 0.93±0.01 0.61±0.01 1.82±0.01 1.21±0.13
Ensemble 2.70±0.01 3.59±0.04 0.95±0.01 0.63±0.01 0.35±0.07 0.83±0.08
SWAG 2.77±0.02 3.85±0.05 0.96±0.03 0.63±0.01 1.11±0.05 1.13±0.20
BBB 2.77±0.01 3.86±0.04 0.95±0.01 0.63±0.01 1.43±0.17 1.10±0.11
DVI-IBLL(ours) 2.67±0.01 3.58±0.03 0.90±0.03 0.60±0.01 0.92±0.15 0.76±0.09
RelatedWork inference refer to models where the prior distribution is
not explicitly specified but instead learned through neural
BayesianLastLayers(BLL)Models
networks. These models are gaining traction due to their
Bayesian Last Layers (BLL) models are a class of meth- flexibilityand ability to capturecomplexdata distributions.
odsthatenhanceneuralnetworkperformancebyincorporat- Notably, (Ma, Li, and Hernández-Lobato 2019) proposed
ingBayesianprinciplesintothe finallayersof thenetwork. highlyflexibleimplicitpriorsoverfunctions,exemplifiedby
The primary advantage of BLL models lies in their abil- data simulators, Bayesian neural networks, and non-linear
itytoefficientlybalanceexplorationandexploitation.Early transformations of stochastic processes. (Takahashi et al.
work by (Box and Tiao 2011) integrated Bayesian layers 2019) introduced the VAE with implicit optimal priors to
with deep neural networks to improve robustness and gen- addressthechallengesofhyperparametertuningfortheag-
eralization. Recent advances have further refined these ap- gregatedposteriormodel.Recentadvancements(Kumarand
proaches. For instance, (Weber et al. 2018) explored train- Poole 2020) have focused on enhancing the efficiency and
ing neural networks online in a bandit setting to optimize accuracyofthesemethodsbyintegratingregularizationtech-
thebalancebetweenexplorationandexploitation.Addition- niques.
ally, (Watson et al. 2021) introduced a functional prior on
themodel’sderivativeswithrespecttotheinputs,enhancing DiffusionModels
predictive uncertainty. (Harrison, Willes, and Snoek 2023)
Diffusion models (Ho, Jain, and Abbeel 2020; Song et al.
appliedvariationalinferencetotrainBayesianlastlayerneu-
2020)haveemergedaspowerfultoolsformodelingcomplex
ralnetworks,improvingtheestimationofposteriordistribu-
distributions and generating high-quality samples. These
tions.Moreover,(FiedlerandLucia2023)addressedcompu-
models simulate a diffusion process in which a simple dis-
tationalchallengesinthelogmarginallikelihoodbyreintro-
tributionis graduallytransformedinto a more complextar-
ducing the weights of the last layer, avoiding the need for
get distribution. The incorporation of stochastic differen-
matrixinversion.
tial equations (SDEs) (Song et al. 2020) has further en-
hanced their capacity to model continuous dynamical sys-
ImplicitPriorModels
tems and capture intricate data patterns. Recent research
Implicit prior models (Hoffman, Riquelme, and Johnson (Vargas, Grathwohl, and Doucet 2023; Richter and Berner
2017; Ma, Li, and Hernández-Lobato 2019) in Bayesian 2023;Piriyakulkij,Wang,andKuleshov2024)hasexploredAlgorithm1:DiffuisonVariationalInferencealgorithmforimplictpriorBLLs(DVI-IBLL)
Input:trainingdatax,ymini-batchsizeB
Initializediffusioncoefficienth,g,allBLLhyperparametersθ,ψ,denoisingdiffusionnetworkparametersγ
Setl =0
0
repeat
fort =0toT −1do
s
Drawǫ fromstandardGaussiandistribution.
ts
Setω =ω −λ(T −t )ω +g(T −t )2s (T −t ,ω )+g(T −t)ǫ
ts+1 ts s ts s γ s ts ts
Computeκ byEq.(14)
T−(ts+1)
Setl =l +g(T −(t +1))2k ωts+1 +s (T −(t +1),ω )k2
ts+1 ts s κT−(ts+1) γ s ts+1 2
endfor
Samplemini-batchindicesI ⊂{1,...,N}with|I|=B
Setl(γ,θ,ψ)= 1 ω2 +Blogσ +logp(ω )+ N logp(y |x ,θ,ψ,ω )−KL N(0,σ2I)kN(0,κ ) − 1l
2 T 0 T B I I T 0 T 2 T
Dogradientdescentonl(γ,θ,ψ)
(cid:0) (cid:1) (cid:0) (cid:1)
untilγ,θ,ψconverge
integrating diffusion models with Bayesian inference, re- ison with our model. Additionally, we compare our model
sulting in the generation of unnormalized probability den- with SNGP (Liu et al. 2023) and last layer Laplace-based
sity function (PDF) samples. Our approach bears signifi- methods(Daxbergeretal.2021),whichareakintolastlayer
cantresemblanceto(Vargas,Grathwohl,andDoucet2023), modelsaimingtoapproximatedeepkernelGPs(Wilsonetal.
but it is distinct in that it derives a different form of SDE 2016) and computing an approximate posterior after train-
aimed at posterior sampling. Unlike (Piriyakulkij, Wang, ing,respectively.NotethatLaplacemethodsarenotassessed
andKuleshov2024),ourmethoddoesnotrequiretheintro- inregressionduetotheirsimilaritytotheMAPmodel.
ductionofadditionalauxiliaryvariablesorwake-sleepalgo- Furthermore, we examine various variational methods
rithms,enablingend-to-endoptimization. such as Bayes-by-Backprop (BBB) (Blundell et al. 2015),
Ensembles(Lakshminarayanan,Pritzel,andBlundell2017),
Experiments BayesianDropout(GalandGhahramani2016),andStochas-
tic Weight Averaging-Gaussian (SWAG) (Maddox et al.
MetricsandBaselines
2019) for a comprehensive evaluation of performance. All
In our regression experiments, we present the predictive ourexperimentswereconductedonanRTX4090GPU.
negative log likelihood (NLL) for test data, which can be
straightforwardlycalculatedforpointfeatureestimates.Ad-
Regression
ditionally,weincludetherootmeansquarederror(RMSE),
a widely used metric in regression analysis. For classifica- Our UCI experiments closely align with the methods out-
tion tasks, apart from the negative log likelihood, we also lined in (Watson et al. 2021; Harrison, Willes, and Snoek
showcase the predictive accuracy (based on the standard 2023),enablingadirectcomparisonwiththeirbaselinemod-
argmaxofthepredictivedistribution)andtheexpectedcali- els. Consistently, we employedthe same MLP architecture
brationerror(ECE),whichassesses thealignmentbetween as described in (Watson et al. 2021), comprising two lay-
themodel’ssubjectivepredictiveuncertaintyandactualpre- ers of 50 hidden units each. Maintaining consistency with
dictive error. Furthermore,we delve into evaluating out-of- (Watson etal. 2021),a batchsize of 32was utilized forall
distributiondetectionperformance,arecognizedassessment datasets,exceptforthePowerdatasetwhereabatchsizeof
approachforrobustandprobabilisticmachinelearning(Liu 256 was chosen to expedite training. Standard preprocess-
et al. 2023). Specifically, we measure the area under the ingtechniqueswere applied,includingnormalizationof in-
ROC curve (AUC) for datasets near and far from the dis- putsandsubtractionoftrainingsetmeansfromtheoutputs.
tribution,whichwillbeelaboratedonfurtherinthissection. Thereportedresultsinourmanuscriptexhibittheoutcomes
Intherealmofregression,wecontrastourmodelwithap- underleakyReLUactivations,withoptimizationperformed
proachesthatleverageexactconjugacy,includingBayesian using the AdamW optimizer (Loshchilovand Hutter 2017)
lastlayermodelssuchasGBLLandLDGBLL(Watsonetal. acrossallexperiments.
2021),aswellasVBLL(Harrison,Willes,andSnoek2023) In deterministic feature experiments, we conducted 20
andRBFkernelGaussianprocesses.We alsojuxtaposeour runswithvaryingseeds.Eachruninvolvedsplittingthedata
model with MAP learning (Snoek et al. 2015), which in- intotraining,validation,andtestingsetswithratiosof0.72,
volves training a complete network using MAP estimation 0.18, and 0.1, respectively. Training was executed on the
andsubsequentlyfittingaBayesianlastlayertothesefixed training set, while performance monitoring on the valida-
features. tion set was carried out to determine the optimal number
Inthedomainofclassification,ourprimarypointofcom- ofepochsformodelconvergence.Ourstudydelvesintothe
parisonisstandarddeepneuralnetworks(DNN),giventheir performanceanalysis of regression VBLL modelsacross 6
abilitytooutputadistributionoverlabelsfordirectcompar- UCI regressiondatasets, asdepictedinTables1 and2. No-Table3:ResultsforWideResNet-28-10onCIFAR-10.
Method Accuracy(↑) ECE(↓) NLL(↓) SVHNAUC(↑) CIFAR-100AUC(↑)
DNN 95.8±0.19 0.028±0.028 0.183±0.007 0.946±0.005 0.893±0.001
SNGP 95.7±0.14 0.017±0.003 0.149±0.005 0.960±0.004 0.902±0.003
D-VBLL 96.4±0.12 0.022±0.001 0.160±0.001 0.969±0.004 0.900±0.004
G-VBLL 96.3±0.06 0.021±0.001 0.174±0.002 0.925±0.015 0.804±0.006
DNN+LLLaplace 96.3±0.03 0.010±0.001 0.133±0.003 0.965±0.010 0.898±0.001
DNN+D-VBLL 96.4±0.01 0.024±0.000 0.176±0.000 0.943±0.002 0.895±0.000
DNN+G-VBLL 96.4±0.01 0.035±0.000 0.533±0.003 0.729±0.004 0.661±0.004
G-VBLL+MAP − − − 0.950±0.006 0.893±0.003
Dropout 95.7±0.13 0.013±0.002 0.145±0.004 0.934±0.004 0.903±0.001
Ensemble 96.4±0.09 0.011±0.092 0.124±0.001 0.947±0.002 0.914±0.000
BBB 96.0±0.08 0.033±0.001 0.333±0.014 0.957±0.004 0.844±0.013
DVI-IBLL(ours) 96.9±0.12 0.018±0.001 0.144±0.002 0.972±0.006 0.906±0.005
Table4:ResultsforWideResNet-28-10onCIFAR-100.
Method Accuracy(↑) ECE(↓) NLL(↓) SVHNAUC(↑) CIFAR-10AUC(↑)
DNN 80.4±0.29 0.107±0.004 0.941±0.016 0.799±0.020 0.795±0.001
SNGP 80.3±0.23 0.030±0.004 0.761±0.007 0.846±0.019 0.798±0.001
D-VBLL 80.7±0.03 0.040±0.002 0.913±0.011 0.849±0.006 0.791±0.003
G-VBLL 80.4±0.10 0.051±0.003 0.945±0.009 0.767±0.055 0.752±0.015
DNN+LLLaplace 80.4±0.29 0.210±0.018 1.048±0.014 0.834±0.014 0.811±0.002
DNN+D-VBLL 80.7±0.02 0.063±0.000 0.831±0.005 0.843±0.001 0.804±0.001
DNN+G-VBLL 80.6±0.02 0.186±0.003 3.026±0.155 0.638±0.021 0.652±0.025
G-VBLL+MAP − − − 0.793±0.032 0.765±0.008
Dropout 80.2±0.22 0.031±0.002 0.762±0.008 0.800±0.014 0.797±0.002
Ensemble 82.5±0.19 0.041±0.002 0.674±0.004 0.812±0.007 0.814±0.001
BBB 79.6±0.04 0.127±0.002 1.611±0.006 0.809±0.060 0.777±0.008
DVI-IBLL(ours) 81.6±0.05 0.035±0.002 0.732±0.014 0.854±0.006 0.804±0.004
tably,ourexperimentsunveilpromisingoutcomesforDVI- ical runtime, with results presented in Table 5. The results
IBLL models across diverse datasets, with the Gaussian indicatethatourmethodintroducedonlyasmallincreasein
process (GP) model showcasing competitive performance complexitycomparedtothebaseline.
acrosstheboard.The resultsobtainedfromourUCI exper-
iments underscore the substantial advancements achieved Conclusion
by our methodology compared to the baselines presented
In summary, our novel approach combining diffusion
in (Watson et al. 2021;Harrison, Willes, and Snoek 2023),
techniques and implicit priors for variational learning of
thereby highlighting the enhanced flexibility of our BLL
Bayesian last layer weights enhancesthe expressive capac-
modelsinacompellingmanner.
ityofBayesianLastLayer(BLL)models.Thisadvancement
addresseslimitationswithGaussianpriorsandboostsmodel
ImageClassification
accuracy, calibration, and out-of-distributiondetection pro-
To evaluate the performance of our model in image classi- ficiency in scenarios with non-Gaussian distributions, out-
fication on the CIFAR-10 and CIFAR-100 datasets using a liers, or high-dimensional data. Our method demonstrates
Wide ResNet-28-10backbonearchitecture,we investigated potentialforimprovingpredictiveaccuracyanduncertainty
differenttrainingmethods,post-trainingstrategies,andfea- quantification in challenging settings, promising enhanced
tureuncertaintyconsiderations.Ourevaluationincludedas- performanceofBLLmodelsinmachinelearningtasks.
sessingout-of-distribution(OOD)detectionperformanceus-
ing the Street View House Numbers (SVHN) dataset as a
Table5:TimeperbatchonCIFAR-10training.
far-OODdatasetandCIFAR-100asanear-OODdatasetfor
CIFAR-10.
Model Runtime(s) %aboveDNN
OurmodelsexhibitedstrongperformanceintermsofAc-
DNN 0.321 0%
curacy, SVHN AUC, and competitive metrics in terms of
D-VBLL 0.338 5.2%
ECE, NLL and SVHN metrics. These resultsshowcase the
G-VBLL 0.364 13.4%
excellent performance of our approach in image classifica-
DVI-IBLL(ours) 0.343 6.9%
tiontasks.Additionally,weconductedananalysisofempir-References Hill,B.M.1974. Bayesianinferenceinstatisticalanalysis.
Alexos, A.; Boyd, A. J.; and Mandt, S. 2022. Structured Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion
stochasticgradientMCMC. InInternationalConferenceon probabilistic models. Advances in neural information pro-
MachineLearning,414–434.PMLR. cessingsystems,33:6840–6851.
Anderson,B.D.1982.Reverse-timediffusionequationmod- Hoffman,M.D.;andBlei,D.M.2015.Structuredstochastic
els.StochasticProcessesandtheirApplications,12(3):313– variationalinference.InArtificialIntelligenceandStatistics,
326. 361–369.
Bishop,C.M.2006. Patternrecognitionandmachinelearn- Hoffman,M.D.;Blei,D.M.;Wang,C.;andPaisley,J.2013.
ing. Springergoogleschola,2:645–678. Stochasticvariationalinference. JournalofMachineLearn-
ingResearch,14(1):1303–1347.
Blundell,C.;Cornebise,J.;Kavukcuoglu,K.;andWierstra,
D.2015. Weightuncertaintyinneuralnetwork. InInterna- Hoffman, M. D.; Riquelme, C.; and Johnson, M. J. 2017.
tionalconferenceonmachinelearning,1613–1622.PMLR. Theβ-vae’simplicitprior. InWorkshoponBayesianDeep
Learning,NIPS,1–5.
Box, G. E.; and Tiao, G. C. 2011. Bayesian inference in
statisticalanalysis. JohnWiley&Sons. Kingma,D.P.;andWelling,M.2013. Auto-encodingvaria-
tionalBayes. InInternationalConferenceonLearningRep-
Chen, R. T.; Rubanova, Y.; Bettencourt, J.; and Duvenaud,
resentations.
D. K. 2018. Neural ordinary differential equations. Ad-
Kononenko,I.1989. Bayesianneuralnetworks. Biological
vancesinneuralinformationprocessingsystems,31.
Cybernetics,61(5):361–370.
Chen,T.;Fox,E.;andGuestrin,C. 2014. Stochasticgradi-
Kristiadi, A.; Hein, M.; and Hennig, P. 2020. Being
enthamiltonianmontecarlo. InInternationalconferenceon
bayesian, even just a bit, fixes overconfidence in relu net-
machinelearning,1683–1691.PMLR.
works. In International conference on machine learning,
Damianou,A.;andLawrence,N.2013. DeepGaussianpro-
5436–5446.PMLR.
cesses. In Conference on Artificial Intelligence and Statis-
Kumar,A.;andPoole,B.2020. OnImplicitRegularization
tics,207–215.
inβ-VAEs. InInternationalConferenceonMachineLearn-
Daxberger, E.; Kristiadi, A.; Immer, A.; Eschenhagen, R.;
ing,5480–5490.PMLR.
Bauer, M.; and Hennig, P. 2021. Laplace redux-effortless
Lakshminarayanan, B.; Pritzel, A.; and Blundell, C. 2017.
bayesian deep learning. Advances in Neural Information
Simpleandscalablepredictiveuncertaintyestimationusing
ProcessingSystems,34:20089–20103.
deepensembles.Advancesinneuralinformationprocessing
Doersch, C. 2016. Tutorial on variational autoencoders.
systems,30.
arXivpreprintarXiv:1606.05908.
Liu, J. Z.; Padhy, S.; Ren, J.; Lin, Z.; Wen, Y.; Jerfel, G.;
Fiedler,F.;andLucia,S.2023. Improveduncertaintyquan-
Nado, Z.; Snoek, J.; Tran, D.; and Lakshminarayanan, B.
tificationforneuralnetworkswithBayesianlastlayer. IEEE
2023. Asimpleapproachtoimprovesingle-modeldeepun-
Access.
certaintyviadistance-awareness.JournalofMachineLearn-
Fortuin,V.2022.Priorsinbayesiandeeplearning:Areview. ingResearch,24(42):1–63.
InternationalStatisticalReview,90(3):563–591. Loshchilov,I.;andHutter,F.2017.Decoupledweightdecay
Fortuin, V.; Garriga-Alonso, A.; Ober, S. W.; Wenzel, F.; regularization. arXivpreprintarXiv:1711.05101.
Ratsch, G.;Turner,R. E.;vanderWilk,M.;andAitchison, Ma, C.; and Hernández-Lobato, J. M. 2021. Functional
L.2021. BayesianNeuralNetworkPriorsRevisited. InIn- variationalinferencebasedonstochasticprocessgenerators.
ternationalConferenceonLearningRepresentations. Advances in Neural Information Processing Systems, 34:
Gal,Y.;andGhahramani,Z.2016.Dropoutasabayesianap- 21795–21807.
proximation:Representingmodeluncertaintyindeeplearn- Ma, C.; Li, Y.; and Hernández-Lobato, J. M. 2019. Vari-
ing.Ininternationalconferenceonmachinelearning,1050– ational implicit processes. In InternationalConference on
1059.PMLR. MachineLearning,4222–4233.PMLR.
Hale,J.K.;andLunel,S.M.V.2013. Introductiontofunc- Maddox,W. J.;Izmailov,P.;Garipov,T.;Vetrov,D.P.;and
tionaldifferentialequations,volume99.SpringerScience& Wilson,A. G. 2019. A simple baselineforbayesianuncer-
BusinessMedia. taintyindeeplearning. Advancesinneuralinformationpro-
Harrison, J.; Sharma, A.; and Pavone, M. 2020. Meta- cessingsystems,32.
learningpriorsforefficientonlinebayesianregression.InAl- Murphy, K. P. 2023. Probabilistic machine learning: Ad-
gorithmicFoundationsofRoboticsXIII:Proceedingsofthe vancedtopics. MITpress.
13thWorkshopontheAlgorithmicFoundationsofRobotics
Naesseth, C.; Lindsten, F.; and Blei, D. 2020. Marko-
13,318–337.Springer.
vian score climbing: Variational inference with KL (p|| q).
Harrison, J.; Willes, J.; and Snoek, J. 2023. Variational Advances in Neural Information Processing Systems, 33:
BayesianLastLayers. InTheTwelfthInternationalConfer- 15499–15510.
enceonLearningRepresentations. Oksendal,B.2013. Stochasticdifferentialequations:anin-
Haussmann,U.G.;andPardoux,E.1986. Timereversalof troductionwithapplications. SpringerScience& Business
diffusions. TheAnnalsofProbability,1188–1205. Media.Piriyakulkij, T.; Wang, Y.; and Kuleshov, V. 2024. Diffu- Zhang, Q.; and Chen, Y. 2021. Path integral sampler: a
sion VariationalInference:DiffusionModelsas Expressive stochastic control approach for sampling. arXiv preprint
VariationalPosteriors. arXivpreprintarXiv:2401.02739. arXiv:2111.15141.
Richter, L.; and Berner, J. 2023. Improved sampling via Zou,D.;Xu,P.;andGu,Q.2019.StochasticgradientHamil-
learneddiffusions. arXivpreprintarXiv:2307.01198. tonianMonteCarlomethodswithrecursivevariancereduc-
tion. Advances in Neural Information Processing Systems,
Rombach,R.;Blattmann,A.;Lorenz,D.;Esser,P.;andOm-
32.
mer, B. 2022. High-resolutionimage synthesis with latent
diffusionmodels. In Proceedingsofthe IEEE/CVFconfer-
ence on computer vision and pattern recognition, 10684–
10695.
Salimbeni,H.;andDeisenroth,M.2017. Doublystochastic
variationalinferencefor deep Gaussian processes. In Con-
ference on Neural Information Processing Systems, 4588–
4599.
Santana, S. R.; Zaldivar, B.; and Hernandez-Lobato, D.
2021. Function-space inference with sparse implicit pro-
cesses. arXivpreprintarXiv:2110.07618.
Särkkä,S.;andSolin,A.2019. Appliedstochasticdifferen-
tialequations,volume10. CambridgeUniversityPress.
Snoek, J.; Rippel, O.; Swersky, K.; Kiros, R.; Satish, N.;
Sundaram, N.; Patwary, M.; Prabhat, M.; and Adams, R.
2015. Scalablebayesianoptimizationusingdeepneuralnet-
works. In International conference on machine learning,
2171–2180.PMLR.
Song,Y.;Sohl-Dickstein,J.;Kingma,D.P.;Kumar,A.;Er-
mon,S.;andPoole,B.2020. Score-basedgenerativemodel-
ingthroughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456.
Takahashi, H.; Iwata, T.; Yamanaka, Y.; Yamada, M.; and
Yagi,S.2019.Variationalautoencoderwithimplicitoptimal
priors. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume33,5066–5073.
Tzen, B.; and Raginsky, M. 2019. Theoretical guarantees
forsamplingandinferenceingenerativemodelswithlatent
diffusions. InConferenceonLearningTheory,3084–3114.
PMLR.
Vargas,F.;Grathwohl,W.;andDoucet,A.2023. Denoising
diffusionsamplers. arXivpreprintarXiv:2302.13834.
Watson,J.;Lin,J. A.;Klink,P.;Pajarinen,J.;andPeters,J.
2021. LatentderivativeBayesianlastlayernetworks. InIn-
ternationalConferenceonArtificialIntelligenceandStatis-
tics,1198–1206.PMLR.
Weber,N.;Starc,J.;Mittal,A.;Blanco,R.;andMàrquez,L.
2018. Optimizing over a bayesian last layer. In NeurIPS
workshoponBayesianDeepLearning.
Welling, M.; and Teh, Y. W. 2011. Bayesian learning via
stochastic gradient Langevin dynamics. In Proceedings
of the 28th international conference on machine learning
(ICML-11),681–688.
Williams,C.K.;andRasmussen,C.E.2006. Gaussianpro-
cesses for machine learning, volume 2. MIT press Cam-
bridge,MA.
Wilson, A. G.; Hu, Z.; Salakhutdinov, R.; and Xing, E. P.
2016. Deep kernel learning. In Artificial intelligence and
statistics,370–378.PMLR.Reproducibility Checklist This paper specifies the computing infrastructure used
forrunningexperiments(hardwareandsoftware),including
Thispaper:
GPU/CPU models; amount of memory; operating system;
Includesaconceptualoutlineand/orpseudocodedescrip-
namesandversionsofrelevantsoftwarelibrariesandframe-
tionofAImethodsintroduced(yes)
works.(yes)
Clearly delineates statements that are opinions, hypothe-
Thispaperformallydescribesevaluationmetricsusedand
sis,andspeculationfromobjectivefactsandresults(yes)
explainsthemotivationforchoosingthesemetrics.(yes)
Provides well marked pedagogical references for less-
This paper states the number of algorithm runs used to
familiarereadersto gainbackgroundnecessaryto replicate
computeeachreportedresult.(yes)
thepaper(yes)
Analysisofexperimentsgoesbeyondsingle-dimensional
Doesthispapermaketheoreticalcontributions?(yes)
summariesofperformance(e.g.,average;median)toinclude
Ifyes,pleasecompletethelistbelow.
measuresofvariation,confidence,orotherdistributionalin-
Allassumptionsandrestrictionsarestatedclearlyandfor-
formation.(yes)
mally.(yes)
The significance of any improvementor decrease in per-
Allnovelclaimsarestatedformally(e.g.,intheoremstate-
formance is judged using appropriate statistical tests (e.g.,
ments).(yes)
Wilcoxonsigned-rank).(yes)
Proofsofallnovelclaimsareincluded.(yes)
Thispaperlistsallfinal(hyper-)parametersusedforeach
Proofsketchesorintuitionsaregivenforcomplexand/or
model/algorithminthepaper’sexperiments.(yes)
novelresults.(yes)
Thispaperstatesthenumberandrangeofvaluestriedper
Appropriate citations to theoretical tools used are given.
(hyper-)parameter duringdevelopmentof the paper, along
(yes)
with thecriterionused forselecting thefinal parameterset-
All theoretical claims are demonstrated empirically to
ting.(yes)
hold.(yes)
All experimental code used to eliminate or disprove
claimsisincluded.(yes)
Doesthispaperrelyononeormoredatasets?(yes)
Ifyes,pleasecompletethelistbelow.
A motivation is given for why the experiments are con-
ductedontheselecteddatasets(yes)
Allnoveldatasetsintroducedinthispaperareincludedin
adataappendix.(NA)
All noveldatasets introducedin this paper will be made
publicly available upon publication of the paper with a li-
censethatallowsfreeusageforresearchpurposes.(NA)
Alldatasetsdrawnfromtheexistingliterature(potentially
including authors’ own previously published work) are ac-
companiedbyappropriatecitations.(yes)
Alldatasetsdrawnfromtheexistingliterature(potentially
includingauthors’ownpreviouslypublishedwork)arepub-
liclyavailable.(yes)
Alldatasetsthatarenotpubliclyavailablearedescribedin
detail, with explanationwhy publicly available alternatives
arenotscientificallysatisficing.(NA)
Doesthispaperincludecomputationalexperiments?(yes)
Ifyes,pleasecompletethelistbelow.
Anycoderequiredforpre-processingdata isincludedin
theappendix.(yes).
Allsourcecoderequiredforconductingandanalyzingthe
experimentsisincludedinacodeappendix.(yes)
Allsourcecoderequiredforconductingandanalyzingthe
experiments will be made publicly available upon publica-
tion of the paper with a license that allows free usage for
researchpurposes.(yes)
All source code implementing new methods have com-
ments detailing the implementation, with references to the
paperwhereeachstepcomesfrom(yes)
Ifanalgorithmdependsonrandomness,thenthemethod
usedforsettingseedsisdescribedinawaysufficienttoallow
replicationofresults.(yes)