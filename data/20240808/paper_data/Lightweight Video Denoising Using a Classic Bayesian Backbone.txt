Lightweight Video Denoising Using a Classic Bayesian
Backbone
1st Cle´ment Bled 2nd Franc¸ois Pitie´
dept. of Electrical and Electronic Engineering dept. of Electrical and Electronic Engineering
Sigmedia Sigmedia
Trinity College, Dublin, Ireland Trinity College, Dublin, Ireland
bledc@tcd.ie pitief@tcd.ie
Abstract—In recent years, state-of-the-art image and video denoising parameters.Unlikeimagedenoisers,themostpopularvideodenoising
networkshavebecomeincreasinglylarge,requiringmillionsoftrainable algorithms (VRT, DVDNet, FastDVDNet, VNLB) are non-blind,
parameters to achieve best-in-class performance. Improved denoising
meaning the user is required to supply the denoiser with a measure
quality has come at the cost of denoising speed, where modern trans-
of the noise variance.
formernetworksarefarslowertorunthansmallerdenoisingnetworks
such as FastDVDnet and classic Bayesian denoisers such as the Wiener While transformer networks achieve greater PSNR quality scores,
filter. theyareslowertorunthansmallernetworks(SeeTableIV),andtheir
In this paper, we implement a hybrid Wiener filter which leverages increasedparametercountresultsinhighvideomemoryconsumption
small ancillary networks to increase the original denoiser performance,
when running inference on high-resolution images, limiting the
whileretainingfastdenoisingspeeds.Thesenetworksareusedtorefine
the Wiener coring estimate, optimise windowing functions and estimate hardware on which they may be deployed.
the unknown noise profile. Using these methods, we outperform several In recent work from Bled and Pitie´ [22], it was demonstrated
populardenoisersandremainwithin0.2dB,onaverage,ofthepopular that this trend of increasingly larger networks is not a fatality
VRTtransformer.Ourmethodwasfoundtobeoverx10fasterthanthe
and that the original Wiener filter can actually be optimised to
transformermethod,withafarlowerparametercost.
achieve performances close to popular image denoising DNNs such
Index Terms—Video Denoising, Image Sequence Denoising, Wiener
Filter as DnCNN [23].
In this paper, we adopt a similar approach for video denoising
I. INTRODUCTION and explore how the Wiener filter could be used as the backbone
of a state-of-the-art video denoiser architecture. We reconsider all
Denoising remains a crucial step in many applications of image
tuneable parameters of Bled’s Wiener filter, taking special care to
and video processing, from the smartphone camera ISP pipeline to
optimise for denoising speed as temporal data is introduced. We
denoising tools of the post-production industry. More recently, the
introduce trainable window functions, 4D FFTs and 3D CNNs, as
massadoptionofover-the-topstreamingservicessuchasNetflixand
well as an ablation study on the use of motion compensation in
Disney+, as well as social media driven by user-generated content
video denoisers. We also modify Bled’s blind denoiser to generalise
such as YouTube, Twitch.tv, Instagram and Facebook have placed
to Video denoising.
greater importance on efficient video encoding, where denoising is
Our key contribution is the implementation of a denoiser which
essential in reducing frame entropy and reducing the bandwidth
demands far fewer parameters (0.29 M) than current denoising net-
necessary to distribute and receive content.
works,outperformingDVDNet,FastDVDNet,andVNLB,onaverage
ClassicdenoiserswhichrelyonBayesianmodellingandfrequency
in terms of PSNR. We outperform all tested networks in SSIM and
filtering such as Wiener filters [1]–[4] and Wavelet filters [5]–
achievegreaterperformancethantheVisiontransformer[17]athigh
[7], or those which use patch similarity, as in BM3D [8], V-
noise levels.
BM4D [9] and VNLB [10], have recently been outperformed by
deep learning approaches [11]–[19]. In 2019, Maggioni et al. put
II. BACKGROUND
forward DVDNet [12], which outperformed VNLB [10] using a
two-step CNN architecture: a spatial denoising network applied to A. Baseline Video Wiener Filter
motion-compensated frames, followed by a temporal denoising step Givenanoisysignaly,composedoftheoriginal,unknownsignal
which consolidates the output of three spatially denoised adjacent x, and additive noise n, y=x+n; the Wiener filter [24] defines a
frames into a single frame. Originally 1.3M parameters in total, linear,minimummeansquareerror(MMSE)optimalfilter.Assuming
FastDVDNet[13]increasedthenetworksizeto2.5Mintotal,opting thattheimagesequenceandnoisesignalaresecond-orderstationary
foraU-Netarchitectureinitsdenoisingblocksandreplacingmotion and decorrelated, the optimal IIR Wiener filter is given by the
compensation with overlapping, multi-frame input blocks. following transfer function H(ω ,ω ,ω ):
1 2 t
InspiredbyDVDNet,similarnetworkssuchasVidenn[14](3.5M P (ω ,ω ,ω )
H(ω ,ω ,ω )= xx 1 2 t , (1)
parameters)andPaCNet[16](2.9Mparameters)havebeenproposed. 1 2 t P (ω ,ω ,ω )
yy 1 2 t
More recently, following the success of image vision transformers
where P and P are the power spectrum densities at spatial and
such as SwinIR [20] (Liang et al.) and Restormer [21] (Zamir et yy xx
temporalfrequenciesω ,ω ,ω atframetfortheinputsignaly and
al.), Liang et al. put forward the Video Restoration Transformer 1 2 t
original signal x. In practice, the PSD of the unknown, clean signal
[17] (VRT), achieving best-in-class results with a network of 35.6M
is estimated with the following coring function at each frequency
This research is supported by Science Foundation Ireland in the ADAPT (ω 1,ω 2,ω t):
Centre(Grant13/RC/2106)atTrinityCollegeDublin. Pˆ xx =max(P yy−P nn,0). (2)
4202
guA
7
]VI.ssee[
1v40930.8042:viXraAlgorithm 1 Kokaram’s Video 3D Wiener Filter [25]
3x3x3, 40 Channel Conv3D + LeakyReLU 3x3x3, 5 Channel Conv3D
Require: Noisyimageseq,noiseSTDσ,BlockSize
1: w(t,h,k)←RaisedCosine(t,h,k) ▷windowingdefinition Intra-Block CNN Inter-Block CNN
2: for allframesinseqdo
3: y ← 3D framebuffer made of current grayscale frame and 4
nearbymotioncompensatedneighbouringframes H Reshape Reshape
4: for allblocksy iny,forstride=BlockSize/2do
5: y¯←mean(y) ▷predictsblockmean
6: y w ←(y−y¯)⊙w ▷windowing
7: Y←FFT3D(y w)
8: P yy ←Y⊙Y∗ Tensor Shape
9: P nn←σˆ2∥w∥2 B, 5, 3, , , , [Bx x ], 40, 3, , [Bx x ], 40, 3, , B, 5, 3, , , ,
10: P xx←max(P yy−P nn,0) ▷coring Fig.1:Thetwo-stagecoringrefinementnetworkarchitectureusedto
11: xˆ w ←iFFT3D(Y⊙P xx⊘P yy)+y¯w optimise the initial prediction of the coring function H(ω ,ω ).
12: xˆ←overlap add(w⊙xˆ w) ▷combineblocks 1 2
Algorithm 2 Our Video 4D Wiener Filter
The noise PSD P can be measured offline, but if it is Additive Require: Noisyimageseq,noiseSTDσ,BlockSize
nn
White Gaussian (AWG), the PSD is a constant P
nn
∝σ2, where σ 1: w a(h,k,t)←exp(−α a(h2+k2)) ▷analysiswindow
is the noise standard deviation (STD). 2: w s(h,k,t)←exp(−α s(h2+k2)) ▷synthesiswindow
The use of Wiener filter filter for video denoising was first 3: for allframesinseqdo
4: y← 4D framebuffer made of current RGB frame and 4 motion
popularised by Kokaram [25], which made use of motion compen-
compensatedneighbouringRGBframes
sation algorithms as a preprocessing measure. We summarise this
5: for allblocksy iny,forstride=BlockSize/4do
implementation in Alg. 1 (the symbols ⊙ and ⊘ denote element-
6: y¯←median(y) ▷predictsblock’DCoffset
wisemultiplicationanddivisionsintheblocks).Asimagesequences 7: y w ←(y−y¯)⊙w a ▷analysiswindow
arenotstationaryprocesses,thesequencemustbebrokenintoblocks 8: Y←FFTn(y w)
(eg. 32×32) to approximate a stationary signal. An analysis window 9: P yy ←Y⊙Y∗
isusedforthefrequencyanalysisoftheblock.Allprocessedblocks 10: P nn←σˆ2∥w∥2
are overlapped and added, using a spatial interpolation windowing 11: P xx←max(P yy−P nn,0) ▷coring
function called the synthesis window. Kokaram used the same half- 12: xˆ w ←iFFTn(Y⊙P xx⊘P yy)+y¯
cosine for the synthesis and frequency analysis window, as it allows 13: x all←overlap add(w s⊙xˆ w) ▷combineblocks
for some simplification in the overlap-add step as the weights sum
14: w all←overlap add(w s⊙w a) ▷combinewindows
up to 1. 15: xˆ← wxa al ll
l
▷denoisedframe
B. Improving the Wiener Baseline
RecentlyBledandPitie´[22]demonstratedthatthisbaselineWiener
temporal window of 5 frames as the fourth dimension. While the
filterforimage-denoisingcouldbeimprovedbyabout+2.8dBPSNR
filter returns five filtered frames, only the target frame is saved.
bymakinganumberofsmalladjustments.Theseincludedirectlypro-
Asummaryofour4DWienerFilterisoutlinedinAlg.2.Anotable
cessingR,G,andBchannelsinaseparatedimension,takingdenser
difference with Kokaram’s Wiener filter baseline is that our window
blockoverlapswithaquarterblockstrideinsteadofthetypicalhalf-
functionsneedtobeexplicitlynormalisedtooneinthesynthesisstep.
block stride, using a Gaussian analysis and interpolation windows
Thisisbecausewealsoexplorethechoiceofanalysisandsynthesis
in place of the half-cosine windows, using median estimation over
windowing in terms of window overlap stride, window size, and,
pixelaveragingforDC-offsetremovalbeforetheFFTtransformand,
window shape. In section IV-A, we introduce trainable 3D windows
lastly, apply the filter at different scales.
andevaluatetheirperformancecomparedtoRaised-Cosinewindows
Theyalsooutlinethatafurther+0.5dBcouldbeobtainedbyrefin-
and Gaussian windows.
ingtheestimatedWienercoringkernelH(ω ,ω )withaverysmall
1 2 In section IV-A, we also show that the method of DC-offset
convolutional network, thus bringing the overall performance of the
removal, a necessary preprocessing step of the FFT, can have some
image denoiser on par with popular networks such as DnCNN [26],
significant impact. Because of range clipping, noise is biased in
but with fewer network parameters.
the black regions and white regions. This bias is rarely addressed
III. ENHANCEDWIENERDENOISINGFORVIDEO in the literature but it usually means that denoised images blacks
A. A Video Wiener 4D Backbone Network are not dark enough. In this paper, we show that using the median
forDC-offset estimationis surprisinglyeffective invideo denoising,
Inthiswork,weproposetoextendtheideafromBledetal.toform
suppressing any visible bias, and leading to similar performance as
avideodenoisingnetworkbasedonaWienerFilterbackbone.Aswe
when using the Ground-Truth DC values.
introduce the temporal dimension, we must revisit the optimisation
made by Bled, as previous optimal values no longer apply. We also
B. Video Wiener Coring Refinement Network
take extra care to optimise for denoising speed.
We start from the baseline 3D Wiener video denoising filter AsanalternativetothedefaultWienercoringfunctionofEq.(2),
implementationbyKokaramandincludesomeoftheideasproposed we propose, as in [22] a lightweight coring post-processing network
in [22] to form a new method that we will call Wiener 4D. thatoperatesonthe4Dspectraltensor.Thisnetworkaimstoreduce
As the name suggests, we first expand the Wiener filter to handle potential ringing artefacts caused by the default coring estimation
colourasanadditionaldimension,thusKokaram’s3DFFTbecomes errors. The network takes in the MSE-optimal Wiener filter transfer
a 4D FFT, using the RGB channels as the third dimension and a function H(ω ,ω ,ω ,ω ) as computed by Eq. (2) as a 4D tensor
1 2 t cfor the spatial frequencies ω ,ω , temporal frequency ω and RGB Stride PSNR(dB) SSIM([0-1]) Time(s)
1 2 t
channel frequency ω , and predicts a new estimate, Hˆ. 1/2 31.56 0.83756 5.39
c 1/3 31.73 0.84316 13.26
A simplified block diagram of our two-stage network is shown 1/4 31.74 0.84360 19.94
1/5 31.75 0.84378 35.19
in Figure 1. The network architecture significantly differs from [22]
1/6 31.75 0.84383 16.83
becausewehavenowtodealwiththetemporaldimension.Thenet- 1/7 31.75 0.84384 49.80
1/8 31.75 0.84384 78.46
workacceptsaWienertensorH,ofshape[B×T×C×M ×M ×H×W],
x y
which is rearranged to consolidate the M ×M overlapping analysis TABLEI:StudyofWienerwindowstrideasafractionofblocksize
x y
windows, to the batch dimension, B, to create a tensor of shape versusoutputquality(PSNR/SSIM).Ablocksizeof32×32isused.
[(B×M ×M )×T×C×H×W]. This allows us to refine the filter via Quality measurements are taken as an average of the 10-sequence
x y
3D trainable convolutions. In the second stage of refinement, the dataset. Time measurements are taken as the sum of denoising time
tensor is rearranged to have shape [B ×T×C×M ×M ] such that for the 10 sequences.
HW x y
we may refine the network via inter block pixel relationships. We
name two parts of the network the intra-block and the inter-block
stages respectively.
IV. EXPERIMENTS/RESULTS
The network is composed of 11, 3D-Convolution layers, each In this section, we iterate through the optimisations made to the
paired with a LeakyReLU activation, with the exception of the last 4D Wiener filter, measuring quality improvements at each step. We
layerineachblock,fromwhichtheyareomitted.40filters/channels evaluate our denoiser using ten, 64-frame sequences, taken from a
are used throughout and each convolution is bias-free to improve combination of Derf’s Collection [28] (HD, gaming) and the BVI
generalisation on unseen data [27]. We train the network using the (SynTex [29], DVC [30]) datasets. Each clip is centre-cropped to
weightedsumoftwoL1losses:firstlythatofthetargetcentreframe, 500×500forevaluation.Fortraining,wechoose173uncropped,full-
andsecondly,thatoftheentire5-framesequence.Forbothdenoising length videos from the corpus, omitting the test sequences. Additive
stages,thefinalnetworksumsto(139,320+139,995)279,315param- Gaussian noise is applied to the datasets at standard deviations of
eters. σ = [10, 20, 30, 40, 50]. At training time, five frames are randomly
C. Blind denoising selectedfromeachsequenceandcroppedintobatchesof5×128×128.
As is still typical in denoisers today, the classic Wiener filter A. Optimising Block Overlaps and Block Size
is a ‘nonblind’ filter, requiring an estimate of the degraded image
We first optimised the 4D filter for the overlap between denoised
noisestandarddeviation.Asdenoisingnetworksmovetowardsblind
temporalblocks.Wemeasurethestrideoftheslidinganalysiswindow
denoising,wealsoimplementablindWienerfilter,requiringnoextra
asafractionoftheblocksize,32×32,from1/2(Kokaram’sstandard)
inputs from the user.
ablockwidthto1/8ofablock.Ourqualitymeasurementsaretaken
Asmallancillary2DCNN,isimplementedforthispurpose,which
as the average across the 10 sequence dataset at a noise STD=20.
takesinthecentretargetframeofthe5-framesequenceandreturnsa
In Table I we observe a +0.17 dB PSNR gain by reducing the
noise standard deviation map of the same size. The map is repeated
strideto1/3ofablockwidth.Furtherdecreasingthestrideprovides
for the outer four frames and fed to the Wiener filter. Unlike the
littleimprovementinqualitywhilegreatlyincreasingdenoisingtime;
user-input noise STD, the predicted noise map is not constrained to
aquarterstrideincreasesperformancebyonly0.01dBandincreases
a single value across the 3-channel image. The network is trained
the denoising time by 6.7 seconds, as more analysis blocks must be
alongsidethecoringrefinement,withtheadditionalL1lossbetween
denoised.
the predicted noise STD map and the ground truth uniform map
Next, we optimise for window size using the same noise profile,
added. To allow the STD network to be trained unconstrained by
σ = 20. In Fig.2 we plot denoising quality w.r.t block size for both
the uniform standard deviation loss and to maximise output quality,
PSNR and SSIM. We observe that PSNR peaks at a window size of
a second training stage is run with only the target frame loss. All
18 (31.88 dB), and decreases as the analysis block size increases to
five noise level datasets are combined to train this network.
126 (31.05 dB). This optimal is + 0.14 dB greater than the previous
Thisancillarynetworkconsistsofonlyfour2Dconvolutionlayers
implementation.SSIMpeaksatawindowofsize22(0.8445),andthe
and three LeakyReLU activations. The network contributes only
lowest quality (0.8336) is also recorded at the largest window size,
8,280 extra parameters to the coring refinement network, increasing
126. Increasing window size was not found to significantly increase
its size to 287,595 parameters.
or decrease denoising time. While larger windows result in more
D. Motion Compensation and Multi-Scale Averaging expensive FFT transforms, fewer windows are required to cover the
Many video denoising networks still implement forms of motion frames.
estimationtomappixelsinnon-targetframestotheirpositioninthe
B. Optimising DC Offset Removal and Window Shape
target frames. This spatial alignment is often necessary to increase
performance in classic Bayesian filters which rely on temporal As mentioned in Section IV-A, it is necessary to zero-mean
consistencybutitseffectondenoisingnetworksisunclear.Toassess the temporal block before applying the FFT. For this purpose, we
theimpactofmotioncompensationintrainednetworks,wecompare compareclassicmeansubtractiontomediansubtraction.Tomeasure
the denoising performance of our optimised Wiener filter with, and, theperformancelostbytakingthenoisymean,werecordtheoutput
without motion compensation, for both our trained Wiener filter and quality when the denoiser is provided with the unseen ground truth
our untrained filter. mean.IntableII,weshowthatforallnoiselevels,usingthemedian
Lastly, we examine the performance benefits of a multi-scale of the block increases denoising performance. This effect is most
Wiener filter, whereby the image is denoised at multiple block noticeable at higher noise standard deviations, where pixels close to
sizes, and the outputs are averaged. Capturing multi-scale frequency brightnessboundariesdeviatefurtherfromtheirgroundtruthvalues.
information in this manner increases the amount of information At a noise level of STD=50, we note a + 0.27 dB increase in
available to the denoiser and creates a smoother final image. performance over using the noisy mean.Cosine Gaussian Trainable Isotropic
32 Scene PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM
0.845 India 31.45/0.9205 31.72/0.9246 31.82/0.9257 31.62/0.9234
Market 32.36/0.8524 32.33/0.8489 32.42/0.8508 32.33/0.8508
31.8 DOTA2 35.28/0.9055 35.46/0.9073 35.54/0.9076 35.31/0.9068
Hamster 30.69/0.5360 30.75/0.5358 30.79/0.5359 30.62/0.5342
)B31.6 0.84 )1 S Foh oo tp bp ai ln lg 3 21 9. .6 33 7/ /0 0. .9 82 80 97 0 3 21 9. .9 44 0/ /0 0. .9 82 87 91 1 3 22 9. .0 47 7/ /0 0. .9 82 96 09 7 3 21 9. .8 43 0/ /0 0. .9 82 85 90 7
d
( R N
-0
( M T Mr ie ne ecraft 3 22 8. .8 35 4/ /0 0. .8 75 45 10 6 3 23 8. .0 33 0/ /0 0. .8 75 36 78 9 3 23 8. .0 35 1/ /0 0. .8 75 37 80 4 3 22 8. .9 27 8/ /0 0. .8 75 37 81 7
S P31.4 IS
S
Bridge 32.25/0.9238 32.48/0.9261 32.63/0.9268 32.41/0.9258
0.835 Christmas 33.49/0.8955 33.63/0.8961 33.69/0.8968 33.59/0.8967
31.2 Mean 31.77/0.8440 31.90/0.8450 31.98/0.8457 31.84/0.8448
TABLEIII:Windowingfunctionvs.denoisedquality(PSNR/SSIM).
31 0.83 Trainable+ denotes a trained window constrained to positive values
0 20 40 60 80 100 120 140
Block Size only.WeevaluateallresultsonourtestsetofSTDσ=20,atawindow
size of 32×32 using a quarter overlap.
Fig. 2: Graph Measuring Wiener window block size versus output
qualityintermsofPSNR(dB)andSSIM(0-1).Qualitymeasurements
aretakenasanaverageofour10-sequencetestset.1/4overlapstride
C. Coring Refinement Network and Blind Denoising
used.
We now evaluate the performance of the coring refinement net-
work,asdescribedinSectionIII-B.Thenetworkistrainedusingthe
Mean Median GTMean same scheme as outlined in the previous section (IV-B), with a 1/3
STD PSNR/SSIM PSNR/SSIM PSNR/SSIM block stride and the learned Gaussian windows. The window sizes
10 35.61/0.9099 35.63/0.9105 35.63/0.9105 are set to 16×16 to maximise performance.
20 31.87/0.8445 31.90/0.8391 31.90/0.8449
Forournon-blinddenoiser,wetrainfivenetworksseparately,each
30 29.55/0.8527 29.65/0.8535 29.65/0.7908
40 27.71/0.7424 27.90/0.7445 27.89/0.7760 on a separate Gaussian noise profile: σ = [10, 20, 30, 40, 50].
50 26.09/0.7007 26.36/0.7040 26.35/0.7039 These denoisers are non-blind and require the user to provide the
denoiser with a noise STD. In Table IV we show that WienerNet
TABLEII:StudyofDCoffsetremovalstrategiesasapreprocessing
performance is on average + 3.5 dB (+ 0.1311 SSIM) greater than
steptotheWienerFilterusinga32×32window.Forcomparison,the
the optimised baseline Wiener (non-coring refinement network). We
groundtruthmeaninthefinalcolumnusestheunknowncleanimage
also show that WienerNet remains within 0.2 dB, on average, of the
to generate the DC offset. Each result is the 10-sequence average
VRT transformer, outperforming it for noise STDs of σ = 40 and σ
quality.
= 50, while being over ten times faster on the same hardware.
As described in Section III-C, we also train and evaluate a single
blind denoiser, WienerNet Blind, which requires no noise input,
Next, we study the impact of window shape on denoising perfor- instead generating its own noise map. We show that with no user
mance.Intheiroriginalpaper,Kokaramusedaraisedcosinewindow noiseinput,thisnetworkoutperformsouroptimisedWienerfilterby
whichactedasboththeanalysisandspatialinterpolationwindowfor + 3.1 dB (+ 0.1231 SSIM) on average, remaining within 0.4 dB of
theoverlappingblocks.AsmentionedinSectionIV-A,changingthe our non-blind denoiser, with very few extra parameters and almost
window stride means that overlapping blocks no longer sum to one. identical run times. Like the non-blind version, this denoiser also
Instead,weuseseparateanalysisandinterpolationwindowpairs,and, outperforms the VRT transformer at high noise levels.
toensuretheoverlappingwindowssumtoone,anormalisingweight Lastly,weevaluatetheblinddenoiserusingamulti-scaledenoising
map is applied to the reconstructed frame. approach,bydenoisingeachsequenceatblocksizesof16,32and64,
andaveragingtheoutput,asdescribedinSectionIII-D.Thismethod
In addition to the half cosine and Gaussian windows used by
improves the performance of the blind denoiser at σ = 10 and σ =
Kokaram and Bled respectively, we introduce two new analysis-
20.Thissuggestsanoptimalweightedaverageexists,pernoiselevel,
interpolation window pairs: trainable Gaussian (non-isotropic)
which outperforms the single-scale approach.
windows and trainable isotropic windows. The trainable Gaussian
windows are initialised as normal Gaussian windows and their
D. Motion Compensation
weights are set to be trainable. The trainable isotropic window is
Lastly, we evaluate motion compensation as a preprocessing step
initialised as a 1D Gaussian window, with the final window being
to denoising for both trained and untrained (WienerNetBlind) filters
interpolated onto 2D space. In both cases, the windows are saved
usingDeepflow[32]andRAFT[33]opticalflowalgorithms.Thisis
post-training and added to the filter as fixed weights.
implemented in the same manner as DVDNet [12].
In the untrained case, DeepFlow and RAFT do not improve
Training: The weights are trained using the same loss function
denoising results in terms of PSNR but improve SSIM at all noise
described in section III-B, with the AdamW optimiser [31] and a
levels except for σ=10. This result may be attributed to occlusions
cosineAnnealinglearning rateschedulerwhich reduces thelearning
created in the motion-compensated frames.
rate from 1e−3 to 1e−5 every 300 epochs, over 1200 epochs.
For the trained case, Deepflow matches the non-motion compen-
In table III we show that the original Raised Cosine window is sated network at σ = 10 and outperforms it at σ = 20 and σ = 30.
outperformed by our trainable Gaussian window by + 0.21 dB, However, SSIM results do not improve when motion compensation
a small improvement over the non-trained Gaussian window. The is applied to the trained network and no PSNR improvements are
isotropic window also outperforms the half-cosine window but we made at σ = 40 and σ = 50. These results may indicate some
were unable to exactly match Gaussian windowing in out training. denoisers have been trained to handle the occlusions generated byNoisy DVDNet[12] FastDVDNet[13] VRT[17] VNLB[10] WienerOpt. WienerNet WienerNetBlind WienerNetBlind+MS
STD PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM
10 28.37/0.6862 35.41/0.9174 35.48/0.9180 37.59/0.9307 37.58/0.9257 34.54/0.8941 37.14/0.9538 36.38/0.9480 36.41/0.9469
20 22.52/0.4431 32.75/0.8724 32.72/0.8700 34.55/0.8957 33.82/0.8720 30.82/0.8129 33.78/0.9148 33.14/0.9071 33.54/0.9112
30 19.22/0.3119 30.58/0.8292 30.67/0.8277 32.28/0.8661 31.13/0.8178 28.65/0.7480 31.96/0.8817 31.92/0.8823 31.80/0.8794
40 16.97/0.2325 28.66/0.7818 28.84/0.7863 30.22/0.8354 28.90/0.7665 26.99/0.6950 30.71/0.8539 30.69/0.8534 30.56/0.8503
50 15.27/0.1804 26.87/0.7345 27.14/0.7461 28.29/0.8029 26.96/0.7195 25.56/0.6515 30.35/0.8528 29.71/0.8261 29.59/0.8232
Time(s) - 4.2k 70.15 1.9k 26.6k 23.30 149.26 149.92 2.0k
Params(M) - 1.33 2.50 35.60 - - 0.29 0.29 0.86
TABLE IV: Quality benchmark of popular denoisers compared to WienerNet in PSNR (dB) and SSIM ([0-1]). Time is the total time taken
todenoisethe10testsequences,inseconds.Paramsisthenumberoftrainableparametersinthedeepdenoisers.WienerOpt.isourdenoiser
withoutthecoringrefinementnetwork,WienerNetisournon-blinddenoiserwiththecoringrefinementnetwork,WienerNetBlindisourblind
denoiser and WienerNetBlind+MS is our multiscale blind denoiser.
(a)Clean (b)Noisy (c)DVDNet (d)FastDVDNet
(e)VRT (f)VNLB (g)WienerNetNon-Blind (h)WienerNetBlind
Fig. 3: Sample output frame at σ = 20, taken from benchmark scenes. For complete sequences, please visit our .Supplementary Material
Repository.
None Deepflow Raft be optimised in future work, along with further improvements in
Sigma PSNR/SSIM PSNR/SSIM PSNR/SSIM
denoising speed and weighted averaging for multi-scale denoising.
10 35.67/0.9106 34.87/0.9071 34.94/0.9074
20 31.84/0.8441 31.51/0.8480 31.68/0.8502
30 29.74/0.7925 29.49/0.8016 29.53/0.8022
40 27.97/0.7466 27.81/0.7591 27.75/0.7567
50 26.42/0.7063 26.30/0.7196 26.17/0.7137
10 36.38/0.9480 36.38/0.9337 36.35/0.9338
20 33.14/0.9071 33.74/0.8914 33.62/0.8903
30 31.92/0.8823 31.99/0.8593 31.73/0.8556
40 30.69/0.8534 30.65/0.8310 30.24/0.8232
50 29.71/0.8261 29.51/0.8044 28.96/0.7915
TABLEV:Motioncompensationefficacybeforeandaftertrainingthe
Wiener Refinement network. Evaluation carried out on all datasets,
σ=[10-50] where WienerNetB represents our blind denoiser.
motioncompensationalgorithms.Inourcase,moreperformancemay
beextractedifwediscardorignoreframeswhichexceedathreshold
value for occluded pixels.
V. CONCLUSIONS
In our work, we have demonstrated the efficiency of using small
ancillary CNNs to improve the performance of a classic, optimised
Bayesian filter, moving away from the black-box approach of CNN
andtransformer-baseddenoisers.Ourdenoiserissmallerintermsof
parametersthanalltestednetworks,andfasterthanthemostcompeti-
tivemethods.Wehavealsoshownthatcurrentmotioncompensation
methods do not always improve denoising performance. This may
.tpOreneiW
.BteNreneiWREFERENCES [24] N. Wiener, N. Wiener, C. Mathematician, N. Wiener, N. Wiener,
and C. Mathe´maticien, Extrapolation, interpolation, and smoothing of
[1] W. K. Pratt, “Generalized wiener filtering computation techniques,”
stationary time series: with engineering applications. MIT press
IEEETransactionsonComputers,vol.100,no.7,pp.636–641,1972.
Cambridge,MA,1949,vol.113,no.21.
[2] M. A. King, P. W. Doherty, R. B. Schwinger, and B. C. Penney, “A
[25] A.Kokaram,“3dwienerfilteringfornoisesuppressioninmotionpicture
wiener filter for nuclear medicine images,” Medical physics, vol. 10,
sequencesusingoverlappedprocessing,”SignalProcessingVII,Vol3,pp.
no.6,pp.876–880,1983.
1780–1783,1994.
[3] M. L. Giger, K. Doi, and C. E. Metz, “Investigation of basic imaging
[26] K.Zhang,W.Zuo,Y.Chen,D.Meng,andL.Zhang,“Beyondagaussian
properties in digital radiography. 2. noise wiener spectrum,” Medical
denoiser: Residual learning of deep cnn for image denoising,” IEEE
physics,vol.11,no.6,pp.797–805,1984.
transactionsonimageprocessing,vol.26,no.7,pp.3142–3155,2017.
[4] J.Benesty,J.Chen,andY.Huang,“Studyofthewidelylinearwiener
[27] S.Mohan,Z.Kadkhodaie,E.P.Simoncelli,andC.Fernandez-Granda,
filter for noise reduction,” in 2010 IEEE International Conference on
“Robust and interpretable blind image denoising via bias-free con-
Acoustics,SpeechandSignalProcessing. IEEE,2010,pp.205–208.
volutional neural networks,” in International Conference on Learning
[5] S. G. Mallat, “A theory for multiresolution signal decomposition: the
Representations,2020.
wavelet representation,” IEEE transactions on pattern analysis and
[28] C. Montgomery and H. Lars, “Xiph. org video test media (derf’s
machineintelligence,vol.11,no.7,pp.674–693,1989.
collection),”Online,https://media.xiph.org/video/derf,vol.6,1994.
[6] P.L.CombettesandJ.-C.Pesquet,“Wavelet-constrainedimagerestora-
[29] D. Ma, F. Zhang, and D. R. Bull, “Bvi-dvc: A training database for
tion,”InternationalJournalofWavelets,MultiresolutionandInformation
deep video compression,” IEEE Transactions on Multimedia, vol. 24,
Processing,vol.2,no.04,pp.371–389,2004.
pp.3847–3858,2021.
[7] M. Malfait and D. Roose, “Wavelet-based image denoising using a
[30] A. V. Katsenou, G. Dimitrov, D. Ma, and D. R. Bull, “Bvi-syntex:
markov random field a priori model,” IEEE Transactions on image
A synthetic video texture dataset for video compression and quality
processing,vol.6,no.4,pp.549–565,1997.
assessment,” IEEE Transactions on Multimedia, vol. 23, pp. 26–38,
[8] K.Dabov,A.Foi,V.Katkovnik,andK.Egiazarian,“Imagedenoisingby
2020.
sparse3-dtransform-domaincollaborativefiltering,”IEEETransactions
[31] I. Loshchilov and F. Hutter, “Fixing weight decay regularization in
onimageprocessing,vol.16,no.8,pp.2080–2095,2007.
adam,”2018.
[9] M.Maggioni,G.Boracchi,A.Foi,andK.Egiazarian,“Videodenoising,
[32] P. Weinzaepfel, J. Revaud, Z. Harchaoui, and C. Schmid, “Deepflow:
deblocking,andenhancementthroughseparable4-dnonlocalspatiotem-
Largedisplacementopticalflowwithdeepmatching,”inProceedingsof
poral transforms,” IEEE Transactions on image processing, vol. 21,
theIEEEinternationalconferenceoncomputervision,2013,pp.1385–
no.9,pp.3952–3966,2012.
1392.
[10] P. Arias and J.-M. Morel, “Video denoising via empirical bayesian
[33] Z.TeedandJ.Deng,“Raft:Recurrentall-pairsfieldtransformsforopti-
estimationofspace-timepatches,”JournalofMathematicalImagingand
calflow,”inComputerVision–ECCV2020:16thEuropeanConference,
Vision,vol.60,no.1,pp.70–93,2018.
Glasgow,UK,August23–28,2020,Proceedings,PartII16. Springer,
[11] A.Davy,T.Ehret,J.-M.Morel,P.Arias,andG.Facciolo,“Anon-local
2020,pp.402–419.
cnn for video denoising,” in 2019 IEEE International Conference on
ImageProcessing(ICIP),2019,pp.2409–2413.
[12] M. Tassano, J. Delon, and T. Veit, “Dvdnet: A fast network for deep
video denoising,” in 2019 IEEE International Conference on Image
Processing(ICIP). IEEE,2019,pp.1805–1809.
[13] ——,“Fastdvdnet:Towardsreal-timedeepvideodenoisingwithoutflow
estimation,” in Proceedings of the IEEE/CVF conference on computer
visionandpatternrecognition,2020,pp.1354–1363.
[14] M. Claus and J. Van Gemert, “Videnn: Deep blind video denoising,”
in Proceedings of the IEEE/CVF conference on computer vision and
patternrecognitionworkshops,2019,pp.0–0.
[15] H. Yue, C. Cao, L. Liao, R. Chu, and J. Yang, “Supervised raw video
denoisingwithabenchmarkdatasetondynamicscenes,”inProceedings
oftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
2020,pp.2301–2310.
[16] G.Vaksman,M.Elad,andP.Milanfar,“Patchcraft:Videodenoisingby
deep modeling and patch matching,” in Proceedings of the IEEE/CVF
InternationalConferenceonComputerVision,2021,pp.2157–2166.
[17] J. Liang, J. Cao, Y. Fan, K. Zhang, R. Ranjan, Y. Li, R. Timofte, and
L. Van Gool, “Vrt: A video restoration transformer,” arXiv preprint
arXiv:2201.12288,2022.
[18] H.Yue,C.Cao,L.Liao,andJ.Yang,“Rvideformer:Efficientrawvideo
denoisingtransformerwithalargerbenchmarkdataset,”arXive-prints,
pp.arXiv–2305,2023.
[19] X. Wang, K. C. Chan, K. Yu, C. Dong, and C. Change Loy, “Edvr:
Video restoration with enhanced deformable convolutional networks,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognitionWorkshops,2019,pp.0–0.
[20] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte,
“Swinir: Image restoration using swin transformer,” in Proceedings of
theIEEE/CVFInternationalConferenceonComputerVision,2021,pp.
1833–1844.
[21] S.W.Zamir,A.Arora,S.Khan,M.Hayat,F.S.Khan,andM.-H.Yang,
“Restormer:Efficienttransformerforhigh-resolutionimagerestoration,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
PatternRecognition,2022,pp.5728–5739.
[22] C. Bled and F. Pitie´, “Pushing the limits of the wiener filter in image
denoising,”in2023IEEEInternationalConferenceonImageProcessing
(ICIP),2023,pp.2590–2594.
[23] K. Zhang, L. V. Gool, and R. Timofte, “Deep unfolding network for
image super-resolution,” in Proceedings of the IEEE/CVF conference
oncomputervisionandpatternrecognition,2020,pp.3217–3226.