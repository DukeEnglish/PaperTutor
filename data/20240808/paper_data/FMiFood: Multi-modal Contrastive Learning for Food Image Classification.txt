FMiFood: Multi-modal Contrastive Learning for
Food Image Classification
Xinyue Pan, Jiangpeng He, Fengqing Zhu
Elmore Family School of Electrical and Computer Engineering,
Purdue University, West Lafayette, IN, U.S.A.
{pan161, he416, zhu0}@purdue.edu
Abstract—Food image classification is the fundamental step
in image-based dietary assessment, which aims to estimate
participants’ nutrient intake from eating occasion images. A
commonchallengeoffoodimagesistheintra-classdiversityand
inter-classsimilarity,whichcansignificantlyhinderclassification
performance. To address this issue, we introduce a novel multi-
modal contrastive learning framework called FMiFood, which
learns more discriminative features by integrating additional
contextual information, such as food category text descriptions,
to enhance classification accuracy. Specifically, we propose a
flexible matching technique that improves the similarity match-
ing between text and image embeddings to focus on multiple
key information. Furthermore, we incorporate the classification
objectives into the framework and explore the use of GPT-4 to
enrich the text descriptions and provide more detailed context.
Our method demonstrates improved performance on both the
UPMC-101 and VFN datasets compared to existing methods.
Fig.1. Examplesofinter-classsimilarityandintra-classdiversity.
Index Terms—multi-modal contrastive learning, food image
classification
effectiveforcapturingtherichanddiverseinformationpresent
I. INTRODUCTION in multi-modal data because it simultaneously exploits the
Image-baseddietaryassessmentinvolvesmeasuringnutrient complementary strengths of visual and textual modalities. Im-
intakefromfoodimagescapturedduringeatingoccasionsand agesprovidedetailedvisualcontext,capturingtheappearance,
has been deployed in mobile and wearable technologies [1]– texture,andspatialrelationshipsofobjects.Ontheotherhand,
[3]. It also brings practical values to the nutrition science and text offers semantic and descriptive information, conveying
healthcarefields,whereassessmentofaperson’sdietaryintake the meaning, attributes, and relationships between concepts.
plays an important role in the overall health of the individual By leveraging both modalities, MMCL enables the model to
[4]. learn a more comprehensive and robust representation of the
data.
The first step in image-based dietary assessment is food
image classification, which has been extensively studied [5]– However, most MMCL models [12]–[15] primarily focus
[11]. However, existing methods often encounter challenges onlearningglobalrelationshipsbetweenimage-textpairs,thus
of inter-class similarity and intra-class diversity, as illustrated often struggle to handle noisy or irrelevant information in the
in Fig. 1. To address these issues, contextual information can text that is not directly related to the image. This limitation
be leveraged to provide additional information that enhance is particularly prevalent in the domain of food image classifi-
the model’s ability to learn more discriminative features from cation, where images often contain complex components with
food images. In this paper, contextual information refers to intricaterelationships.Evenminorvariationsintheingredients
food category text description, which is easy to obtain. can lead to a change in the food category. For example, as
Multi-modal contrastive learning (MMCL) aims to enhance illustrated in fig 1, a small alteration in the ingredients of
classification by leveraging intrinsic relationships between a lobster sandwich can result in its reclassification as a hot
different types of data, such as images and text. The core dog. Consequently, there is a need for a model capable of
idea behind MMCL is to learn a joint embedding space exploring the fine-grained relationships between image-text
where features from matched pairs (e.g., an image and its pairs to address these challenges.
corresponding text description) are aligned, while the feature To address these challenges, we propose the Flexible
representations of unmatched pairs are pushed apart. This Matching for image Classification on Food images(FMiFood)
contrastive learning process encourages the model to capture model, which introduces a flexible matching mechanism that
the shared semantic information between the modalities and allowsanimagepatchtomatchmultipletexttokensornone,as
learn more discriminative features. MMCL is particularly appropriate.Thisflexibilityenablesourmodeltobettercapture
4202
guA
7
]VC.sc[
1v22930.8042:viXrathe complexity andfine-grained details of foodimages, where between image patches and text tokens. However, FILIP is
ingredients may have intricate relationships. Inspired by [15], not specifically designed for image classification and can be
[16], we also combine the contrastive learning objective with generalized to other downstream tasks, such as image-text
a separate branch for the image classification objective, uti- retrieval. FILIP assumes that each image matches only one
lizing both the soft cross-entropy loss and the hard cross- text and vice versa within a single batch, which does not
entropy loss, respectively. To further enhance the richness account for scenarios where multiple texts can match a single
and informativeness of the textual descriptions, we leverage image and multiple images can belong to the same category.
the GPT-4 model [17], which possesses extensive knowledge Additionally, FILIP assumes each image patch matches only
about various food categories. By generating detailed and one text token, which is also not always the case. FILIP’s
semantically meaningful descriptions for each label using classification performance is further limited by a lack of
GPT-4, our model can better understand and disambiguate the detailed text descriptions for each text label. Therefore, while
subtle differences between food images, leading to improved FILIP partially addresses issues of intra-class diversity and
classification performance. inter-class similarity, there is still space for improvement.
The main contributions of our work can be summarized as: iCLIP [16] and UniCL [15] are MMCL models specifically
• We propose the FMiFood model, specifically designed designedforimageclassification.iCLIPenhanceslabelexpla-
for the food image classification task, which utilizes soft nations using the WordNet dictionary, a large lexical database
cross-entropy loss in contrastive learning and includes a of English that groups words into sets of synonyms called
separate branch for the image classification objective. synsets.ItalsoadaptstheCLIPmodelforimageclassification
• We introduce flexible matching in the context of con- by comparing images in a batch with all label descriptions to
trastive learning to allow multiple or no text tokens compute the cross-entropy loss. However, this approach is not
matched to an image patch and vice versa as a new way suitable for food categories since most food categories lack
to compute similarity scores between image-text pairs. corresponding explanations in the WordNet [25] dictionary.
• We leverage the GPT-4 model to generate detailed text Additionally, iCLIP cannot handle multiple positive pairs
descriptions for each food category, enriching the textual associated with one image or text label in the contrastive
information available for training. learning component. Although UniCL addresses this issue by
applying soft cross-entropy loss during the training phase, it
II. RELATEDWORK also has limitations as it cannot consider all possible negative
A. Food Image Classification samples (labels) in a single batch. These limitations highlight
the need for improved methods to address these challenges in
Many contributions have been made to food image classi-
MMCL for image classification.
fication. R. Mao et al. proposed visual hierarchy and multi-
task learning for food classification [5], [7]. M. Wei et al. III. METHOD
introduced ISIA-500 [18] and Food-2K [10] datasets, and In this section, we introduce our FMiFood framework as
enhanced local feature extraction [10]. S. Jiang et al. ag- shown in Fig. 2. The model computes similarity scores be-
gregated multi-level features for improved classification [11]. tweenimage-textpairsusingaflexiblematchingtechniqueand
These approaches address intra-class diversity and inter-class incorporates an image classification learning objective, which
similarity issues. J. He et al. focused on food continual are illustrated in Section III-A and Section III-B, respectively.
learning [19], [20] and imbalanced classification [21], [22] to We also augment text descriptions using the GPT-4 model to
simulate real world food data distribution. explain food categories as detailed in Section III-C.
B. Multi-Modal Contrastive Learning A. FlexibleMatchingBetweenImagePatchesandTextTokens
Multi-modal contrastive learning has gained attention in To enhance the alignment between image patches and text
image classification due to its significant improvements in tokens, we use a flexible matching strategy. This approach
tasks such as image-text retrieval and image classification allows an image patch to match multiple text tokens or none
compared to using image data alone. One of the widely atall,andviceversa.Insteadofconsideringonlythemaximum
recognized models in this field is CLIP [12], which aligns similarity score between an image patch and all text tokens,
features ofmatched image-textpairs and separatesunmatched we average all similarity scores above a certain threshold
pairs. Other models have introduced various improvements while ignoring those below a lower threshold. This ensures
based on CLIP, such as ALIGN, UNiCL, SLIP and DeCLIP that all relevant matches are included, capturing the complex
[13]–[15], [23]. However, these models typically focus on relationships between image patches and text tokens more
global relationships between image-text pairs and do not accurately.
explore fine-grained relationships, which are crucial for food Let I be the set of image patches and T be the set of
image classification. text tokens. Let s(i,t) be the similarity score between image
FILIP [24] is designed to capture fine-grained relationships patch i ∈ I and text token t ∈ T. The effective number of
between images and text by focusing on alignments between imagepatchesandtexttokenstakenintoaccountforsimilarity
image patches and text tokens. FILIP achieves this by bas- score computation are denoted as |I| and |T|, respectively.
ing similarity scores on the mean max token-wise similarity The similarity score between image patches and text tokens isFig.2. OverviewofourFMiFoodmodel:|B|denotesthebatchsizeand|C|representsthenumberoftextlabelsinthedataset.Imagesandtextdescriptions
arefedintotheimageencodersandtextencodersoftheFMiFoodmodeltoextractimagepatchandtexttokenorlabeltokenfeatures.Thesimilarityscore
betweenimage-textpairsiscomputedbasedontheflexiblematchingtechniquetolearnwithboththecontrastivelossandthecategoricalloss.
where
 1 (cid:80) s(t,i) if max(s(t,i))>d,
|I dt| i∈I dt
s = max s(t,i) if c≤max(s(t,i))≤d, (6)
t i∈It
0 [c,d]
if max(s(t,i))<c,
and
It ={i∈I |s(t,i)>d}, (7)
d
It ={i∈I |c≤s(t,i)≤d}. (8)
[c,d]
where s(i,t) is the cosine similarity score between image
patch i and text token t and −1 ≤ s(i,t) ≤ 1. max(s(i,t))
is the maximum similarity score between image patch i and
all text tokens t∈T, max(s(t,i)) is the maximum similarity
scorebetweentexttokentandallimagepatchesi∈I.Ti and
d
Ti are the sets of text tokens with similarity scores above
[c,d]
Fig. 3. Issue with contrastive learning on current multi-modal contrastive d and between c and d with image patch i, respectively. It
learning model: In a single batch, we cannot assume one image is only d
matchedtoonetextincontrastivelearningunderimageclassificationtask. and It are the sets of image patches with similarity scores
[c,d]
above d and between c and d with text token t, respectively.
computed as follows. The similarity score from image to text, B. Image Classification Learning Objective
sim(I,T), is given as
In the image classification task, text information typically
1 (cid:88) comes from category labels. To effectively manage the sce-
sim(I,T)= s (1)
|I| i nario where multiple images can match the same text label
i∈I within a single batch, as shown in fig. 3, we apply a soft
where cross-entropy loss in FMiFood’s contrastive learning, inspired
 |T1 di|(cid:80)
t∈T
dis(i,t) if max(s(i,t))>d, b Ly
I
kt fh oe rU
xI
kniC isL gim veo nde bl y[ :15]. The image-to-text contrastive loss
s = max s(i,t) if c≤max(s(i,t))≤d, (2)
i 0 t∈T [i c,d]
if max(s(i,t))<c, LI (cid:0) xI,{xT}b (cid:1)
=−1(cid:88)b
qI
log(cid:32) exp(sI k,j) (cid:33)
k k j j=1 b k,j (cid:80)b exp(sI )
and j=1 l=1 k,l
(9)
Ti ={t∈T |s(i,t)>d}, (3) where sI denotes the similarity of the k-th image to the j-th
d k,j
text,andqI representsthegroundtruthindicatorforwhether
Ti ={t∈T |c≤s(i,t)≤d}. (4) k,j
[c,d] the k-th image and the j-th text are a positive pair.
Similarly,thesimilarityscorefromtexttoimage,sim(T,I), The text-to-image contrastive loss for xT is:
k
can be defined anal so ig mo (u Tsl ,y I: )= 1 (cid:88) s (5) LT k (cid:0) xT k,{xI j}b j=1(cid:1) =−1 b (cid:88)b q kT ,jlog(cid:32) (cid:80)bexp e( xs pT j, (k s) T )(cid:33)
|T| t j=1 l=1 l,k
t∈T (10)where b is the batch size ,and sT denotes the similarity of A. Datasets
j,k
the j-th text to the k-th image, and q kT ,j represents the ground UPMC-Food101 [26]: This dataset contains 101 food cat-
truth indicator for whether the k-th text and the j-th image egories and 790 - 956 images per category with food recipe
are a positive pair. descriptionsextractedfromtheInternetforeachimage,which
The total contrastive loss is then: include noisy information associated with each image. How-
ever,consideringthegeneralizationofourmethodandthefact
b
L = 1(cid:88)(cid:0) LI +LT(cid:1) (11) that most food datasets do not have such descriptions, we do
con 2 k k notusethetextdescriptionsinthisdatasetforourexperiments.
k=1
The training and testing sets are preset in this dataset.
However, contrastive learning alone may not see all neg-
VFN [5]: This dataset contains 74 food categories with
ative classes in a batch. To address this, we incorporate an
around 14K images. Each food category represents a fre-
additional image classification objective. By computing the
quentlyconsumedfoodtermselectedfromtheWhatWeEatIn
cosine similarity between image and text labels and applying
America (WWEIA) database [27]. All food images are online
cross-entropy loss, we directly optimize the model for image
images uploaded by real users. We randomly split the dataset
classification. The predicted probability for the k-th image
into 80:20 as training and testing sets.
belonging to the j-th category is given by:
B. Experiment Setup
exp(sI ) We utilize the Vision Transformer (ViT) [28] with a patch
p(y =j |xI)= k,j (12)
k k (cid:80)C exp(sI ) size of 32 as the baseline model for uni-modal learning. For
l=1 k,l
multi-modal contrastive learning, we use CLIP [12], FILIP
where C is the total number of categories. The categorical [24], UNiCL [15] and iCLIP [16] as baseline models.
cross-entropy loss for a batch of b images is:
• ViT [28]: A Vision Transformer model with a patch size
L =
1(cid:88)b
L
=−1(cid:88)b log(cid:32) exp(sI k,yk) (cid:33)
(13) •
o Cf L3 I2 Pt [h 1a 2t ]u :t Cili oz ne ds uo cn tsly coim nta rg ase tii vn efo lr em ara nt ii no gn ofo nr it mra ai gn ein -tg e.
xt
cat b cat,k b (cid:80)C exp(sI ) pairs to make matched pairs as close as possible while
k=1 k=1 l=1 k,l
separatingunmatchedpairsbasedonglobalfeatures.The
wheresI denotesthesimilarityofthek-thimagetoitstrue
k,yk textinputusedis"Aphotoof[category],atypeoffood."
category label y .
k • FILIP [24]: Conducts contrastive learning on image-text
Finally, the total loss combines both losses:
pairs like CLIP but based on fine-grained relationships
using image patch features and text token features. The
L =m∗L +n∗L (14) textinputusedis"Aphotoof[category],atypeoffood."
total con cat
• UniCL [15]: A model that considers multiple texts
wheremandnarehyperparametersassociatedwitheachloss
matchedtoanimagewithinabatchduringthecontrastive
and can be tuned.
learning process. The text input used is "A photo of
[category], a type of food."
C. Generation of Text Descriptions Using GPT-4
• iCLIP [16]: A CLIP model applied to image classifi-
Typical multi-modal contrastive learning models use stan- cation tasks with augmented text descriptions from the
dardtexttemplateslike"Aphotoof[category],atypeoffood." WordNet dictionary. The text input consists of the text
These text templates lack detailed information, which can label along with its corresponding explanation from the
limit the model’s ability to learn nuanced features of different WordNet dictionary.
categories. To address this, we enhance text descriptions by
We also conduct ablation studies on related work and our
leveraging the GPT-4 model [17], which is pretrained on a
proposed methods. We use two types of text descriptions: the
vast amount of real-world data. By prompting GPT-4 with
standard one and the GPT-4 augmented one. The standard
"Describetheappearanceof[category]inlessthan50words,"
text description is “A photo of [category], a type of food.”
we generate detailed descriptions that provide richer context
The GPT-4 augmented description is “A photo of [category],
for the model to learn. GPT-4 can generate more specific de-
atypeoffood.[GPT-4generateddescriptions].”Ourproposed
scriptionsthatcanoffermoreinformativevisualcues,helping
methodincludesflexiblematching(FM)asdetailedinSection
the model to better distinguish between similar categories.
III-A and the image classification learning objective(IC) de-
Incorporating these enriched descriptions into the training
tailed in Section III-B. For flexible matching, we emperically
data improves the alignment between image and text features,
set the lower threshold c = 0 and upper threshold d = 0.85.
ultimately leading to enhanced classification performance.
For image classification learning objective(IC), we set hyper-
parameter m=0.75 and n=0.25.
IV. EXPERIMENTS
The image encoder used in the multi-modal contrastive
We evaluate our FMiFood model using the average classi- learning models is a Vision Transformer with a patch size of
fication accuracy on two datasets to conduct ablation studies 32,andthetextencoderisamaskedself-attentionTransformer
and compare the performance to existing works. from CLIP. We train the baseline models and the proposedTABLEI TABLEIII
AVERAGECLASSIFICATIONACCURACYINPERCENTAGEONTWO AVERAGECLASSIFICATIONACCURACYINPERCENTAGEONVFNDATASET
DATASETSFORDIFFERENTMETHODS FORABLATIONSTUDIESONOURPROPOSEDMETHOD
Method UPMC-Food101 VFN Average Accuracyfor Accuracyfor
ViT[28] 69.17 75.36 72.27 Method FM IC Standard GPT-4augmented
CLIP[12] 74.87 79.78 77.32 textdescription textdescription
FILIP[24] 75.10 79.51 77.31 CLIP[12] - - 79.78 80.80
UniCL[15] 75.53 80.24 77.89 FILIP[24] - - 79.51 80.60
iCLIP[16] 75.72 80.70 78.21 FMiFood Yes No 80.34 80.90
FMiFood 76.22 81.69 78.96 FMiFood No Yes 80.86 81.32
FMiFood Yes Yes 81.26 81.69
TABLEII
AVERAGECLASSIFICATIONACCURACYINPERCENTAGEON
UPMC-FOOD101DATASETFORABLATIONSTUDIESONOURPROPOSED
METHOD
Accuracyfor Accuracyfor
Method FM IC Standard GPT-4augmented
textdescription textdescription
CLIP[12] - - 74.87 73.70
FILIP[24] - - 75.10 75.59
FMiFood Yes No 75.61 75.76
FMiFood No Yes 75.75 75.92
FMiFood Yes Yes 76.06 76.22
method on the UPMC-Food101 [26] and VFN [5] food
datasets. The batch size is set to 128, and we use the AdamW
optimizer with an initial learning rate of 1e−5 with a cosine
annealing scheduler [29]. The model is trained for 40 epochs
across different methods. Fig. 4. Partial confusion matrix for selected categories from the UPMC-
Food101andVFNdatasetsfordifferentmethods
C. Quantitative Results and Ablation Studies
when matching image patches and text tokens, but FMiFood
Tables I, II, and III show the quantitative results in terms
can mitigate this issue.
of average classification accuracy for different methods and
Fig 4 presents the partial confusion matrices for selected
ablation studies on the UPMC-Food101 and VFN datasets.
categories from the UPMC-Food101 and VFN datasets. On
Our proposed FMiFood model outperforms all related works
theUPMC-Food101dataset,itcanbeseenthatthe‘ice-cream’
in terms of average classification accuracy.
category is often misclassified as ‘frozen yogurt’ by the CLIP
FromtheresultsontheUPMC-Food101dataset,weobserve
and FILIP models due to the visual similarity between these
that augmenting text descriptions with the GPT-4 model does
two categories. However, the FMiFood model demonstrates
notimproveCLIP’sclassificationperformance.Thisisbecause
betterperformanceindistinguishingbetweenthesecategories.
the text descriptions generated by the GPT-4 model on the
A similar phenomenon is observed for the ‘Macaroni and
UPMC-Food101 dataset are not directly related to the image,
noodles with cheese’ category, which is often confused with
making CLIP vulnerable to noisy information in the text.
‘pasta mixed dishes’ in the CLIP and FILIP models. The
However, for FILIP and FMiFood models, there is a small
improved performance of FMiFood can be attributed to its
improvementafteraugmentingtextdescriptionswiththeGPT-
ability to leverage richer text descriptions for each food
4 model. This is because these models can filter out irrelevant
category, thereby enhancing its discriminatory power between
information through the matching process between image
visually similar food items.
patches and text tokens. Specifically, FMiFood can effectively
filteroutimagepatchesthatdonotcorrespondtoanyrelevant
D. Qualitative Results for Flexible Matching in FMiFood
text tokens and vice versa. In all scenarios, incorporating the
image classification learning objective significantly improves Fig 5 shows qualitative results of matching between one
classification accuracy. This is because the model specifically image patch and all text tokens on one example food image
focuses on optimizing for image classification, ensuring that with or without the flexible matching method. From FILIP’s
the features learned are highly relevant to the task. results, we see that one image patch can only be matched to
For the VFN dataset, we observe that CLIP achieves much one text token, losing much information in the text. However,
better classification performance after augmenting text de- for FMiFood, one image patch can be matched to multiple
scriptions with the GPT-4 model. This is because the gener- text tokens, which includes rich information and captures key
ated text descriptions are related to the image content. This informationfromthetext.Therearesomematchedtexttokens
improvement is also seen in FILIP and FMiFood models, notdirectlyrelatedtotheimage,introducingnoisyinformation
but the improvement is smaller. This is because FILIP and during the learning process. This is also why FMiFood does
FMiFood models do not consider all important information notcontributemuchperformancegaininadditiontotheFILIP[10] W. Min, Z. Wang, Y. Liu, M. Luo, L. Kang, X. Wei, X. Wei, and
S. Jiang, “Large scale visual food recognition,” IEEE Transactions on
Pattern Analysis and Machine Intelligence, vol. 45, no. 8, pp. 9932–
9949,2023.
[11] S. Jiang, W. Min, L. Liu, and Z. Luo, “Multi-scale multi-view deep
featureaggregationforfoodrecognition,”IEEETransactionsonImage
Processing,vol.29,pp.265–276,2020.
[12] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
Fig. 5. Qualitative result of comparison between FILIP and FMiFood. The G.Sastry,A.Askell,P.Mishkin,J.Clark,G.Krueger,andI.Sutskever,
wordsinredarethetexttokensthatarematchedtotheimagepatch “Learning transferable visual models from natural language supervi-
sion,” Proceedings of the 38th International Conference on Machine
Learning,vol.139,pp.8748–8763,18–24Jul2021.
model.Therefore,futureworkwillfocusoninvestigatinghow [13] C.Jia,Y.Yang,Y.Xia,Y.-T.Chen,Z.Parekh,H.Pham,Q.V.Le,Y.-
to filter out unrelated information in the matched tokens. H.Sung,Z.Li,andT.Duerig,“Scalingupvisualandvision-language
representation learning with noisy text supervision,” Proceedings of
V. CONCLUSIONANDFUTUREWORK MachineLearningResearch,2021.
[14] N. Mu, A. Kirillov, D. Wagner, and S. Xie, “Slip: Self-supervision
In this paper, we apply a multi-modal contrastive learning
meetslanguage-imagepre-training,”EuropeanConferenceonComputer
model to the task of food image classification. To address Vision,2021.
the issues of intra-class diversity and inter-class similarity, we [15] J. Yang, C. Li, P. Zhang, B. Xiao, C. Liu, L. Yuan, and J. Gao,
“Unified contrastive learning in image-text-label space,” Proceedings
propose the FMiFood model, which incorporates fine-grained
ofIEEE/CVFConferenceonComputerVisionandPatternRecognition
learning. To learn richer text information and filter out irrel- (CVPR),pp.19141–19151,2022.
evant tokens, we introduce flexible matching between image [16] Y.Wei,Y.Cao,Z.Zhang,H.Peng,Z.Yao,Z.Xie,H.Hu,andB.Guo,
“iclip:Bridgingimageclassificationandcontrastivelanguage-imagepre-
patches and text tokens, extending beyond the capabilities of
training for visual recognition,” Proceedings of IEEE/CVF Conference
theFILIPmodel.Additionally,tospecificallytargettheimage onComputerVisionandPatternRecognition(CVPR),June2023.
classification learning objective, we apply soft cross-entropy [17] OpenAI, “Gpt-4 technical report,” OpenAI Technical Report, 2023.
[Online].Available:https://www.openai.com/research/gpt-4
loss in the contrastive learning component of the FMiFood
[18] W.Min,L.Liu,Z.Wang,Z.Luo,X.Wei,X.Wei,andS.Jiang,“Isia
model.Thisallowsthemodeltoconsidercaseswheremultiple food-500:Adatasetforlarge-scalefoodrecognitionviastackedglobal-
imagesinabatchmatchasingletextlabel.Wealsoincorporate local attention network,” Proceedings of the 28th ACM International
ConferenceonMultimedia,2020.
categorical loss to facilitate direct classification during the
[19] J. He, R. Mao, Z. Shao, and F. Zhu, “Incremental learning in online
learning process. Our experimental results demonstrate good scenario,”ProceedingsoftheIEEEConferenceonComputerVisionand
improvements in classification accuracy on two food datasets. PatternRecognition,pp.13926–13935,2020.
[20] J.HeandF.Zhu,“Onlinecontinuallearningforvisualfoodclassifica-
Inthefuture,weaimtoenhancetheflexiblematchingmethod
tion,”ProceedingsofIEEE/CVFInternationalConferenceonComputer
in the FMiFood model to ensure that the matched tokens do VisionWorkshops,pp.2337–2346,2021.
not include noisy information, further improving the model’s [21] ——,“Single-stageheavy-tailedfoodclassification,”IEEEInternational
ConferenceonImageProcessing,pp.1115–1119,2023.
robustness and performance.
[22] J. He, L. Lin, H. A. Eicher-Miller, and F. Zhu, “Long-tailed food
classification,”Nutrients,2023.
REFERENCES
[23] Y.Li,F.Liang,L.Zhao,Y.Cui,W.Ouyang,J.Shao,F.Yu,andJ.Yan,
[1] Z. Shao, Y. Han, J. He, R. Mao, J. Wright, D. Kerr, C. J. Boushey, “Supervision exists everywhere: A data efficient contrastive language-
and F. Zhu, “An integrated system for mobile image-based dietary image pre-training paradigm,” International Conference on Learning
assessment,” Proceedings of the 3rd Workshop on AIxFood, p. 19–23, Representations,2022.
2021. [24] L. Yao, R. Huang, L. Hou, G. Lu, M. Niu, H. Xu, X. Liang, Z. Li,
[2] H.Hassannejad,G.Matrella,P.Ciampolini,I.D.Munari,M.Mordonini, X. Jiang, and C. Xu, “FILIP: Fine-grained interactive language-image
andS.Cagnoni,“Automaticdietmonitoring:areviewofcomputervision pre-training,” International Conference on Learning Representations,
andwearablesensor-basedmethods,”IntJFoodSciNutr.,2017. 2022.
[3] L. Gemming, J. Utter, and C. N. Mhurchu, “Image-assisted dietary [25] G. A. Miller, “WordNet: A lexical database for English,” Human
assessment:Asystematicreviewoftheevidence,”J.Acad.Nutr.Diet., LanguageTechnology:ProceedingsofaWorkshopheldatPlainsboro,
2015. NewJersey,March8-11,1994,1994.
[4] J. Reedy, S. M. Krebs-Smith, P. E. Miller, A. D. Liese, L. L. Kahle, [26] I.Gallo,G.Ria,N.Landro,andR.L.Grassa,“Imageandtextfusion
Y. Park, and A. F. Subar, “Higher diet quality is associated with for upmc food-101 using bert and cnns,” Proceedings of International
decreasedriskofall-cause,cardiovasculardisease,andcancermortality ConferenceonImageandVisionComputingNewZealand(IVCNZ),pp.
amongolderadults,”TheJournalofNutrition,2014. 1–6,2020.
[5] R. Mao, J. He, Z. Shao, S. K. Yarlagadda, and F. Zhu, “Visual [27] H. A. Eicher-Miller and C. J. Boushey, “How often and how much?
aware hierarchy based food recognition,” Proceedings of International differencesindietaryintakebyfrequencyandenergycontributionvary
ConferenceonPatternRecognitionWorkshops,2020. amongu.s.adultsinnhanes2007‚Äì2012,”Nutrients,vol.9,2017.
[6] X. Pan, J. He, and F. Zhu, “Personalized food image classification: [28] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
Benchmark datasets and new baseline,” Proceedings of Asilomar Con- T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,
ferenceonSignals,Systems,andComputers,pp.1095–1099,2024. J.Uszkoreit,andN.Houlsby,“Animageisworth16x16words:Trans-
[7] R. Mao, J. He, L. Lin, Z. Shao, H. A. Eicher-Miller, and F. M. Zhu, formers for image recognition at scale,” Proceedings of International
“Improvingdietaryassessmentviaintegratedhierarchyfoodclassifica-
ConferenceonLearningRepresentations,2021.
tion,” Proceedings of the IEEE International Workshop on Multimedia [29] I. Loshchilov and F. Hutter, “SGDR: Stochastic gradient descent with
SignalProcessing,pp.1–6,2021. warm restarts,” Proceedings of International Conference on Learning
[8] X.Pan,J.He,andF.Zhu,“Muti-stagehierarchicalfoodclassification,”
Representations,2017.
Proceedingsofthe8thInternationalWorkshoponMultimediaAssisted
DietaryManagement,pp.79–87,2023.
[9] G.Liu,Y.Jiao,J.Chen,B.Zhu,andY.-G.Jiang,“Fromcanteenfoodto
dailymeals:Generalizingfoodrecognitiontomorepracticalscenarios,”
IEEETransactionsonMultimedia,pp.1–10,2024.