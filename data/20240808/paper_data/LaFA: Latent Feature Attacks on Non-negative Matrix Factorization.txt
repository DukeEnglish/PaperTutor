LaFA: Latent Feature Attacks on Non-negative Matrix Factorization
MinhVu BenNebgen ErikSkau
TheoreticalDivision,LANL TheoreticalDivision,LANL ComputationalSciences
LosAlamos,U.S LosAlamos,U.S ewskau@lanl.gov
mvu@lanl.gov bnebgen@lanl.gov
GeighZollicoffer JuanCastorena KimRasmussen
TheoreticalDivision,LANL ComputationalSciences,LANL TheoreticalDivision,LANL
LosAlamos,U.S LosAlamos,U.S LosAlamos,U.S
gzollicoffer@lanl.gov jcastorena@lanl.gov kor@lanl.gov
BoianS.Alexandrov ManishBhattarai
TheoreticalDivision,LANL TheoreticalDivision,LANL
LosAlamos,U.S LosAlamos,U.S
boian@lanl.gov ceodspspectrum@lanl.gov
Abstract experimentsonsyntheticandreal-worlddata.
As Machine Learning (ML) applications rapidly grow, 1.Introduction
concernsaboutadversarialattackscompromisingtheirreli-
Non-negativematrixfactorization(NMF)[1]isaversa-
abilityhavegainedsignificantattention. Oneunsupervised
tiletoolformulti-waydatareconstructionthroughfactoriz-
MLmethodknownforitsresiliencetosuchattacksisNon-
ingamatrixoramultidimensionalarrayinaleast-squares
negativeMatrixFactorization(NMF),analgorithmthatde-
approach. As real-world data often exhibit multiple ways,
composesinputdataintolower-dimensionallatentfeatures.
e.g., conditions, channels, spaces, times, and frequencies,
However, theintroductionofpowerfulcomputationaltools
theNMFcanbeaneffectivetooltoextractsalientfeatures
such as Pytorch enables the computation of gradients of
of those data. As such, NMF has become widely adopted
the latent features with respect to the original data, rais-
across scientific fields such as psychology, chemistry, sig-
ingconcernsaboutNMF’sreliability. Interestingly,naively
nalprocessing,computervision,andbioinformatics[2].
derivingtheadversariallossforNMFasinthecaseofML
However, as we will show, NMF’s decomposed factors
wouldresultinthereconstructionloss,whichcanbeshown
aresusceptibletoinput’sperturbations,knownasadversar-
theoreticallytobeanineffectiveattackingobjective. Inthis
ialnoise,whichareintentionallydesignedtodisruptfeature
work,weintroduceanovelclassofattacksinNMFtermed
extraction. These disruptions, termed adversarial attacks,
LatentFeatureAttacks(LaFA),whichaimtomanipulatethe
cancompromisethereliabilityofthesalientfeatureextrac-
latentfeaturesproducedbytheNMFprocess. Ourmethod
tion process. Standard NMF algorithms typically assume
utilizes the Feature Error (FE) loss directly on the latent
that data are sampled from a distribution with a low-rank
features. ByemployingFEloss,wegenerateperturbations
model and zero-mean i.i.d. Gaussian noise. In many real-
intheoriginaldatathatsignificantlyaffecttheextractedla-
world scenarios, this assumption may not hold due to the
tentfeatures,revealingvulnerabilitiesakintothosefoundin
presence of malicious attacks or anomalies, rendering the
other ML techniques. To handle large peak-memory over-
featureextractionprocessvulnerabletosuchnoise.
headfromgradientback-propagationinFEattacks,wede-
Themaincontributionsofthispaperare:
velopamethodbasedonimplicitdifferentiationwhichen-
ablestheirscalingtolargerdatasets.WevalidateNMFvul- • WeexaminetherobustnessofNMFagainstadversar-
nerabilities and FE attacks effectiveness through extensive ial attacks and find it vulnerable despite its resilience
4202
guA
7
]GL.sc[
1v90930.8042:viXrain data reconstruction. Specifically, we introduce mitigateevenextremeadversarialattacks,suggestingpath-
the Feature Error loss to directly assess latent fea- waystomoreresilientcommunitydetectionmethods. Ad-
tures generated by NMF. Through back-propagation ditionally, [12] merges deep learning with NMF to tackle
attacksusingtheFEloss,weshowthatinjectingsmall matrix completion tasks, incorporating elastic adversarial
amounts of adversarial noise into the data can lead to strategies to assess and improve the robustness of learned
significant distortions in the resulting latent features. patternsagainstdeliberatenoise.
The finding established a class of new attacks, called Whiletheaforementionedstudiesprimarilyfocusonde-
LatentFeatureAttacks(LaFA)onNMF. fensiveschemeswithinthecontextofNMF,especiallydur-
ingtraining, ourworkdivergesbycriticallyexaminingthe
• Back-propagating FE attacks require backtracking robustnessofNMFunderadversarialattacks. Weintroduce
NMF’siterativeupdates,demandingsubstantialpeak- novel techniques specifically designed to compromise fea-
memory. Toaddressthis,weutilizeimplicitdifferenti- tureintegrity, markingapioneeringeffortinexecutingtar-
ationtodetermineadirectexpressionforthegradients getedattackswithintherealmofunsupervisedlearning.
required for FE attacks, bypassing the need to back-
propagate throughNMF iterations. Thisapproach re- 3.Preliminaries
duced the peak-memory requirements, and removes
thehistorydependenceoftheFEattackgradients. NMF is particularly notable for its application in data
with inherent non-negativity, where it decomposes a non-
• We confirm the susceptibility of NMF to feature at- negative matrix X ∈ RM×N into two low-rank non-
tacks and illustrate the efficacy of our approaches negativematrices,W ∈ RM×k andH ∈ Rk×N,suchthat
throughcomprehensiveexperimentsconductedonone X≈WH,wherekismuchsmallerthanM andN.
syntheticdatasetandfourdifferentreal-worlddatasets: NMFprocedure. OneeffectiveapproachtofindWand
WTSI[3],Face[4],Swimmer[5],andMNIST[6]. HisbyutilizingKullback-Leibler(KL)divergenceasadis-
crepancymeasure,whichoffersasoundstatisticalinterpre-
This manuscript is organized as follows. Sect. 2 and tationinapplicationsinvolvingcountsorprobabilities. The
3 provide the related work and preliminaries. Sect. 4 de- optimization aims to minimize the divergence between X
scribes our proposed FE loss and the corresponding LaFA anditsapproximationWH,whichisgivenby[1]:
targeting extracted features. Our methods and technical
claims are illustrated via synthetic experiments in Sect. 5. (X⊘(WH))H⊤
Sect. 6 provides our experimental results on real-world W ij ←W ij (1H⊤) ij
datasets,andSect.7concludesthispaper. ij
W⊤(X⊘(WH))
H ←H ij
2.RelatedWork ij ij (W⊤1)
ij
Adversarial attacks in ML have predominantly targeted where ⊘ denotes element-wise division and 1 denotes a
supervisedlearningmodelswithnumerousstudiesdemon- one-matrix of X’s size. These updates are applied itera-
strating the susceptibility of these models to subtle, mali- tively, whereeachiterationimprovestheapproximationof
ciously crafted perturbations [7,8]. However, the explo- X by reducing the KL divergence. The process is gener-
rationofadversarialattacksinunsupervisedsettings,partic- allygovernedbyapre-determinednumberofiterations,or
ularlyinvolvingtechniqueslikeNMF,hasstartedtogarner until a convergence criterion is reached. We use W,H =
attentiononlyinrecentyears. NMF(X,W ,H ,T) to denote this iterative update
init init
Recentresearchhasincreasinglyfocusedonintegrating procedure,whereT representsthenumberofupdates.
adversarial learning with NMF, revealing both vulnerabili- RobustnessofNMF.NMFisappreciatedforitsrobust-
tiesandopportunitiesforenhancingrobustness. In[9],the nessindatareconstruction,particularlyagainstnoise. This
authors introduce adversarial perturbations during the fac- robustnesscanbereflectedviathetriangleinequality[13]:
torization process, uncovering potential manipulations and
inherentweaknessesintraditionalNMFalgorithms. Toad- ∥X+δ−WH∥≤∥X−WH∥+∥δ ∥ (1)
2
dressthis,[10]proposesatrainingregimethatincorporates
adversarial examples to foster NMF models that maintain TheinequalityimpliesthatanyperturbationδtothedataX
precisefactorizationsunderadversarialconditions. Extend- cannotinduceareconstructionerrorthatexceedstheorigi-
ing beyond the direct applications to NMF, [11] explores nalreconstructionerrorbyamarginof∥δ∥.
the effects of adversarial attacks on community detection Itismoreinterestingtoexaminetherobustnessofthere-
algorithms, often rooted in matrix factorization principles. sultingW′andH′fromaperturbedX+δ. Regardingthat,
Thefindingsillustratethatrobustalgorithmicstrategiescan Laurberg’sTheorem[14]providesacompellingmathemat-that error as a loss L taking two matrices WH and
NMF
WH asarguments:
true
FE=L(WH ,WH ) (2)
NMF true
Here, WH ∈ R(M+N)×k is the concatenation of W and
H⊤combinedwithamagnitudebalancingoperation:
Figure1. Adversarialgradientcomputation w.r.tFEloss(2)via WH i =concat(cid:0) W¯ i,H¯⊤ i (cid:1) where
back-propagate and implicit methods. As the implicit does not (cid:112) (cid:112)
∥W ∥∥H ∥ ∥W ∥∥H ∥
needtobackwardtheNMF,itresultsinthepeak-memoryadvan- W¯ =W i i ,H¯⊤ =H⊤ i i
i i ∥W ∥ i i ∥H ∥
tagecomparedtotheback-propagate. i i
with i refers to the column of the matrices and ∥.∥ is L
2
norm. For brevity, we denote the above construction of
icalfoundation. Particularly,bydenoting:
WH from W and H by WH = c¯at(W,H). The mag-
nitude balancing operation does not change the result of
J (W′,H′):=min∥W−W′(DP)∥
(W,H) D,P W ×H; however, it removes the ambiguity arising from
+∥H−(DP)−1H′∥ thescalingofWandH.Additionally,byincludingbothW
andHinthesamenormoperation,ratherthanintwosepa-
whereDisadiagonalmatrixandPisapermutationmatrix, ratetermsasLaurberg’sformulation[14],theFEsimplifies
wecanrestatetheLaurberg’sresultas: thefeatures’alignmentbetweenWH trueandWH NMF.
Assuming X = WH be a unique NMF. For any As identical Xs are recovered if the rows and columns
given ϵ > 0, there exists a δ > 0 such that for of W and H are permuted in combination, a meaning-
any nonnegative matrix Y = X + N with ∥N∥ < ful FE loss must minimize over all column-permutations
δ, we have J (W,H)(W′,H′) < ϵ, where [W′,H′] = of WH NMF. To address this, a feature-wise error matrix
argmin ∥Y−W′H′∥. FEMareconstructedasfollow:
W′≥0,H′≥0
In other words, the Theorem shows that the perturba- (cid:13) (cid:13)
tion’smagnitudeboundsthedistortioninthefactoredmatri-
FEM(i,j)=(cid:13)WH NMFi−WH truej(cid:13)
F
cesresultingfromperturbeddata,emphasizingthestability Then, the element-wise square of FEM can be fed into
androbustnessofNMFundernear-optimalconditions. the Hungarian Algorithm [15] to align WH so that
NMF
∥WH −WH ∥isminimized.
NMF true
4.LatentFeatureAttacksonNMF Consequently,wecanexpresstheFElossLas:
This section describes our proposed LaFA on NMF. FE=L(WH ,WH ) (3)
NMF true
Specifically, we introduce our proposed FE loss in Sub-
∥c¯at(PW ,PH )−WH ∥
sect. 4.1, and the gradient-ascent-based attacks in Sub- =min NMF NMF true F (4)
sect. 4.2. Fig. 1 provides an illustration of two proposed
P ∥WH true∥
F
LaFA, called Back-propagation and Implicit. Both at- wherePisapermutationmatrix.
tacks compute the gradients of the FE loss w.r.t the input Noting that squaring FEM linearizes the contribution
data X and utilize gradient-based methods to iteratively of the difference of each element of WH, thus allowing
find the adversarial perturbation maximizing the FE. The thelinearsumassignmentalgorithmtocorrectlychoosethe
Back-propagation method directly computes the gradient minimum permutation without sampling all possible per-
∇ XLbyreversingtheNMFprocedure,whichdemandsex- mutations. It would not be possible to utilize the Hungar-
tremely high peak-memory to store the gradient computa- ianAlgorithmtosimplifythepermutationproblemifLau-
tionalgraph. Incontrast, theImplicitmethodonlyneedto rberg’stwo-termdefinitionofFEwereused.
backward to the feature matrices W and H, significantly
reducingtheamountofmemoryrequiredandenablingthe 4.2.LatentFeatureAttacks
scalingofattackstoscenarioswithlargerdatasets.
With the FE loss (2), the optimal direction for an ad-
versarial feature attack can be computed. Given a ground-
4.1.FeatureErrorLoss
truthWH ,theFEcanbeconsideredasafunctionofX,
true
We now elaborate on how to formulate a loss func- andtheoptimaladversarialdirectionissimplythegradient
tion capturing the feature errors between the NMF- ∇ L. That gradient can be obtained by back-propagating
X
extracted matrices (W ,H ) and the true matri- the Multiplicative Updates of NMF (presented in subsec-
NMF NMF
ces (W ,H ) generating X. We begin by denoting tion 3) and the FE loss. Then, the optimal distortion ε
true trueAlgorithm 1 Adversarial gradients’ computation on NMF Algorithm 2 Adversarial gradients’ computation on NMF
viaback-propagation viaimplicitfunction
Input: X,W H ,andbudgetε Input: X,W H ,andbudgetε
ref ref ref ref
Parameters: NMFiterationsT Parameters: NMFiterationsT
Output:AdversarialgradientG Output:AdversarialgradientG
1: WH ref =c¯at(W ref,H ref) 1: WH ref =c¯at(W ref,H ref)
2: RandomlyinitializeW initandH init 2: RandomlyinitializeW initandH init
3: W,H=NMF(X,W init,H init,T) 3: W,H=NMF(X,W init,H init,T)
4: WH=c¯at(W,H) #Implicitgradients
5: L=L(WH,WH ref) 4: J WW,J WH,J WX,J HW,J HHandJ HX ←
6: return G=∇ XL#ComputeviabackwardtoX JacobiansofNMF(X,W,H,1)
(cid:20) (cid:21) (cid:20) (cid:21)
J J J
5: J y = JWW JWH ,J x = JWX
HW HH HX
of a given magnitude producing the largest FE can be ob- 6: G yx =−(J y−I)−1J x
tained via gradient-ascent algorithms. This leads to the 7: L=L(WH,WH ref)
Back-propagationmethod,whichwillbedescribedinSub- 8: G Ly =∇ W,HL#ComputeviabackwardtoW,H
sect.4.2.1. However,thismethoddemandsapeak-memory 9: return G=G Ly×G yx
usage proportional to the number of NMF iterations. This
memoryrequirementisofteninfeasibleforcurrentcompu-
y=0,where˜f istheNMFupdatewithargumentyinstead
tational capabilities, even for medium-sized datasets. As
ofWandH. Bydenotingf(x,y)=˜f(x,y)−y,wehave
such, we propose another method, called Implicit Method
utilizing the fixed-point condition of the NMF at conver- df i = ∂f i +(cid:88) ∂f i dy k =0
gence to implicitly compute the gradient (Subsect. 4.2.2). dx ∂x ∂y dx
j j k j
Since the memory requirement for the Implicit method is k
independent of the number of NMF iterations, it can scale ⇒∂f i +(cid:88) ∂f˜ i dy k −(cid:88) ∂y i dy k =0
the FE attack to larger datasets, effectively demonstrating ∂x ∂y dx ∂y dx
j k j k j
k k
thesignificantthreatposedbyNMFfeatureattacks.
Wecanrewritetheabovewiththematrix’snotations:
∂f dy dy ∂f dy
4.2.1 Back-propagateFeatureAttack ∂x +J ydx −I dx =0⇒ ∂x +(J y−I) dx =0
j j j j j
Algo. 1 shows how to compute the adversarial direction dy ∂f
⇒ =−(J −I)−1 =−(J −I)−1J (6)
for a feature attack using the Back-propagating method. dx y ∂x y x
That gradient then can be leveraged by Fast Gradient whereJ andJ aretheJacobianmatriceswithJ [i,k] =
y x y
SignedMethod(FGSM)[8]orProjectedGradientDescent ∂f˜ i andJ [i,k]= ∂f˜ i. For(6),weuse ∂fi = ∂f˜ i.
(PGD) [7] to generate the adversarial X˜. However, the ∂yk x ∂xj ∂xj ∂xj
Wecanseethat(6)offersanalternativetocomputethe
highmemoryrequirementtobackwardtheNMFiterations
gradient. First,weusetheJacobiansofoneNMFupdateto
W,H = NMF(X,W ,H ,T)(Line3)hindersthe
init init compute the partial derivatives of W and H w.r.t. X, i.e.,
practicality of the method. In fact, the gradients’ com-
J ,thenmultiplyitwiththegradientoftheFEloss(2)w.r.t.
putational graph for that step requires a peak-memory of y
WandHwouldgiveusthefeatureattack’sgradient:
T × O(MN). Since the number of NMF’s updates T is
typically≈ 104,itcreatesheavyburdensoncomputational (cid:18)(cid:20) J J (cid:21) (cid:19)−1 (cid:20) J (cid:21)
G=−∇ L× WW WH −I × WX
resourcesandpreventsthefeasibilityoftheattack. W,H J J J
HW HH HX
This implicit computation scheme is summarized in
4.2.2 ImplicitMethodforFeatureAttack Algo.2. Itcanbeseenthatthepeak-memoryofthecompu-
tationisforstoringJ ,whichisO((M +N)DMN),and
x
We now demonstrate our Implicit method to efficiently
itisindependentoftheNMF’siterationsT.
compute the gradient for feature attacks. The attack relies
onthefixed-pointconditionoftheNMFatconvergence: 5.IllustrativeSyntheticExperiments
(W,H)=NMF(X,W,H,1) (5) This section demonstrates the vulnerability of NMF
againstLaFAviaasyntheticexample. Weconsiderasyn-
We denote x and y as the flattened vectors of X, and thetic data X ∈ R100×200 (Fig. 2a) with known ground-
(W,H),respectively. Wethencanrewrite(5)as˜f(x,y)− truthW ∈R100×3 (Fig.2b)andH ∈R3×200,and
true true(a)SyntheticdataX. (b)OriginalW. (c)ReconstructedWofRemoving-spikeadversarial.
(d)DistortionscausedbyRemoving-spikeandRec.lossadversarial. (e)FEandReconstructionerrorsofproposedadversarialattacks.
Figure2.Featureattacksonsyntheticdataofrank3.Whilethereconstructionerrorsremainsmall,featureerrorscanbesignificantlylarge
(Fig.2e). Notably,theImplicitmethodachievesasignificantpeak-memoryadvantagecomparedtoBack-propagating(BP)method,i.e.,
186.4Mbscomparedto278.2Mbs,whilemaintainingcompetitiveattackingperformance.
jectedontheinputX.Thissupportsthetheoreticalanalysis
claimingthatNMFisrobusttoreconstructionerror.
Q2: Isthereanoisedirectionthatcausesfeaturematri-
ces to change the most and become unstable? If the NMF
decompositionofXisunique,thenthereisnobadnoisefor
arbitrarilysmallϵ[14]. However,whenwearenotdealing
withanarbitrarilysmallepsilon,abadsolutionmightexist.
Todemonstratethat,weconsideraperturbationX˜ ofX
ε=0 ε=0.01 ε=0.04
such that its NMF’s solutions would have high (or seman-
Figure3. ThereconstructionofthefirstcomponentofWinsyn- tical) feature errors to those of X. In particular, from W
theticunderPGD-L Implicitattacks.
∞ (Fig. 2b) generating X, we remove the spikes in the com-
ponentsofWandobtainarank2W˜ ∈ R100×3. Then,X˜
issettoW˜ ×H. X˜ isrefereedastheRemoving-spikead-
applythreeattack/perturbationschemesonNMFwhenap-
versarial. Fig. 2c shows the resulting feature matrix when
plied toX. Thefirst twocolumns of W arecombinations
NMFisappliedonX˜. ItissignificantlydifferentfromW
of a Gaussian signal with a spike signal. The last column
notonlyintheabsenceofthespikesbutalsointherank.
containsalinearcombinationofthefirsttwoGaussianand
anindependentspike.Thus,Whasrank3.ThematrixH∈ To further study the Removing-spike, we generate a set
generatingXhasitsentriesuniformlysampledfrom[0,1). of perturbations along the direction from X to X˜, i.e.,
WenowexaminethreefollowingquestionsregardingNMF: {αX˜ + (1 − α)X} 0≤α≤1, and compute the correspond-
Q1:FindthedirectionofnoiseaddingtoXthatinduces ing W and H. The resulting reconstruction, FE, and the
high reconstruction error. As stated in Sect. 3, it is infea- L 2 normerrorxofreconstructingWareplottedinFig.2d.
sibletoinjectasmallperturbationtoXthatcausesalarge Interestingly, whiletheerrorsonfeaturesmaintainpropor-
reconstructionerrorinNMF.Specifically,findingthedirec- tionaltothereconstructionerrorswhentheinputdistortion
tionmaximizingreconstructionerrorcorrespondstosolve:
issmall,asignificantspikeinWerrorsandFEerrorsoccur
around18%. Thisnotonlyshowsthatthefeatures’robust-
maximize min ||X+ϵ−W×H|| F. ness proved by Laurberg for small noise does not hold for
||ϵ||≤δ W≥0,H≥0
generalnoises, butalsoindicatesourproposedFEloss(2)
Fig. 2d shows our attempt to solve this with a gradient- hasstrongcorrelationtoerrorsonreconstructedmatrixW.
ascent. The Rec. error (BP) line shows the reconstruction Q3:Findthenoisethatcausesthehighestfeatureerrors.
errorbyusingPGDdirectlybackwardonthereconstruction ThesharpincreaseofFEinFig.2dsuggeststhatsmallper-
loss||X+ϵ−WH||. Theresultsshowthatthereconstruc- turbationinadifferentdirectionmayinduceamuchlarger
tionerrorisjustslightlylargerthantheamountofnoisein- FE.Specifically,thenoisedirectionfromXtoX˜ wouldre-quireustoperturbabout18%oftheinputtocauseasharp
increase of 40% in FE. The goal of our attacks is to find
smaller noises that can induce larger feature errors, and,
consequently,revealthethreatoffeatureattacks.
Fig. 2e shows the performance of PGD-L attacks [7]
∞
leveragingourback-propagating(Algo.1)andimplicitgra-
dients(Algo.2)basedontheFEloss. Theresultsshowthat
our attacks only require a perturbation of about 3% of the
input (at ϵ = 0.02) to cause 40% distortion in features.
∞
This importantly validates that NMF is vulnerable to fea-
(a)FeatureandReconstructionerrorsunderL2attacks.
ture attacks. Furthermore, this large distortion in features
cannotbedetectedsolelyfromthereconstructionerrorsas
thereconstructionerrorsremainapproximatelyequaltothe
magnitudeoftheinjectednoise(asdiscussedinQ1).
Fig. 3 shows a more detailed look on the attacked fea-
tures.Whileasmalladversarialperturbation(ϵ=0.01)can
distortthereconstructedWsignificantly,itdoesnotchange
its rank (the spikes is preserved). The rank collapses to 2
andthespikedisappearsatϵ = 0.04. Thus,featureattacks
(b)FeatureandReconstructionerrorsunderL∞attacks
notonlycreatelargedistortionsintermsofmetricdistances,
Figure 5. Performance of FE attack on Face dataset. Peak-
butalsoalterthefeatures’semantic.
memory:BP:8387.9Mbs/Imp:6049.4Mbs.
(a)FeatureandReconstructionerrorsunderL2attacks.
Figure6.Face’sreconstructedWunderL FEattacks.
∞
(b)FeatureandReconstructionerrorsunderL∞attacks
Figure 4. Performance of FE attacks on WTSI dataset. Peak-
memoryBP:109.5Mbs/Imp:29.9Mbs.
6.ExperimentalResults
(a)L2attack. (b)L∞attack
Thissectiondemonstratesourfindingsonthevulnerabil-
Figure 7. Performance of FE attack on Swim dataset. Peak-
itiesofNMFtoLaFAin4real-worlddataset: WTSI,Face,
memoryapproximately3Gbs.
Swimmer and MNIST. The experiments consider the at-
tackerhasaccesstothedatamatrixXanditsgoalistogen-
erate an adversarial noise resulting in high feature errors. ing on the gradients computed by either Back-propagating
AllattacksutilizethePGDattack[7]with40stepsleverag- (Algo. 1) or Implicit (Algo. 2) methods. The entries of XFigure8.Swim’sreconstructedWunderL FEattacks.
2
byincreasingε. TheBack-propagationconsistentlyshows
a slightly lower error trajectory compared to the Implicit
method,suggestingitmaybelesseffectiveatdamagingthe
features. Notably, the Back-propagation requires a signifi-
cantlyhigherpeak-memoryusagecomparedtotheImplicit
method,i.e.,109.5MBsversus29.9MBs,highlightingthe
memoryadvantageoftheImplicitmethod. Thesefindings
arecrucialforassessingthevulnerabilityanddesigningpro-
tectionstrategiesofgenomicdatatoadversarialattacks.
(a)L2attack. (b)L∞attack
6.2.ResultsonFaceDataset
Figure 9. Performance of FE attack on MNIST dataset. Peak-
memoryapproximately10Gbs. The Face dataset [4] comprises a set of 2,429 face and
4,548non-faceimages.Duetothehighcomputationalcom-
plexity associated with back-propagating the NMF, we fo-
cuson47119×19-grayscalefaceimagesfromthetestset,
forming X ∈ R471×361. We factorize X into 5 features.
Thischoiceisdrivenbythepreliminaryanalysisindicating
thatthisnumberoffeaturescapturestheessentialvariability
inthefacialdatawhileavoidingoverfitting.
NMF feature extraction vulnerability is evident in the
Face dataset, as depicted in Fig.5a and 5b. Notably, the
Implicit method displays a much more significant advan-
tageinattackingperformancecomparedtoBack-propagate
Figure10.MNIST’sreconstructedWunderL ∞attacks. comparedtotheresultsofWTSI.ThevisualizationinFig.6
showcases the effects of L norm-based FE attacks on
∞
the reconstructed facial features matrix W from a specific
arenormalizedbetween0and1.
dataset comprising face images. Each row represents dif-
The experiments are conducted on HPC clusters,
ferentlevelsofperturbation’smagnitudeεinL , ranging
∞
equippedwithAMDEPYC7713processorswith64cores
from 0.002 to 0.008. The clean row serves as a baseline,
and256GBofRAM,and4NVIDIAAmpereA100GPUs,
showingunperturbedlatentfeatures.Asεincreases,notice-
eachwith40GBofVRAM.
ablevisualdistortionsappearinthereconstructedfeatures,
particularly highlighted in the blue box. These distortions
6.1.ResultsonWTSIdataset
indicateadegradationinfeatureintegrity,affectingtheclar-
The WTSI [3] is a genomic dataset featuring sequenc- ity and structure of facial features. Moreover, the red box
ingdatafromover30species,includingasignificantnum- highlightstheintroductionofnewfeaturecomponents,un-
berofhumangenomicandcancergenomesequences. Ex- derscoringasignificantadversarialimpact.
perimental results on WTSI (Fig. 4) reveal several key in-
6.3.ResultsonSwimmerDataset
sights into the vulnerabilities of NMF to feature attacks.
ForbothL andL attacks,theFEfromBack-propagation TheSwimmerdataset[5]comprises256imagesdepict-
2 ∞
and Implicitmethods exhibita linear increasein L errors ing top-down representations of an individual swimming.
2Thisdatasetisspecificallydesignedtofacilitatetheexplo- with both Back-propagation and Implicit methods. Our
rationofsparserepresentationsandtheeffectivenessofvar- findings demonstrated that adversarial perturbations could
ioussignal-processingalgorithms.WeperformedNMFfac- significantly impair the feature extraction capabilities of
torizationofthedatasetinto16featuresassuggestedby[5]. NMF,asevidencedbybothnorm-basedmetricsanddirect
The results on Swimmer is reported in Fig. 7. Both L visualizations of the corrupted features. The novel attack
2
andL attacksshowasignificantdistortionsinL errorsas strategiesintroducedinthisstudyalsoprovideasignificant
∞ 2
ε increases. We cannot conduct Back-propagation attacks stepforwardinunderstandingandenhancingtherobustness
on the Swimmer due to memory constraint. On the other ofunsupervisedlearningframeworks.
hand,theImplicitmethoddemonstratesanescalationiner- Whileourmethodhasshownpromisingresultsinterms
roratalargerepsilonvaluescomparedtopreviousdatasets. of performance and memory efficiency, the current imple-
ThereasonisthelatentfeaturesW ofSwimmeraremuch mentationexhibitsarunningtimecomplexitythatmaynot
cleanerandmoredistinctive. Fig.8exemplifiesthedegra- besuitableforlarge-scaleapplicationsorreal-timeprocess-
dation of reconstructed Swimmer’s latent feature at higher ing(Table1). Toenhancethepracticalityandscalabilityof
noise. Thesequenceofimagesdemonstratestheimpacton ourapproach,weaimtoaddresstherunningtimecomplex-
thevisualintegrityontherecoveredfeatures: withlargeε, ityoftheproposedmethodinourfuturework.
theoutlinesandorientationsoftheswimmersbecomedis-
tortedandprogressivelylessrecognizablecomparedtothe References
cleanandgroundtruthimages.Thisvisualdistortionispar-
ticularlysignificantatε = 0.45,atwhichsomeadversarial [1] D. D. Lee and H. S. Seung, “Algorithms for non-
pixels begin to appear at the bottom corners of some fea- negative matrix factorization,” in Advances in neural
tures(orangebox). Thisshowcasestheeffectivenessofour informationprocessingsystems,2001.
adversarialattacksindisruptingtheNMF’sabilitytorecon-
[2] Y.-X.WangandY.-J.Zhang,“Nonnegativematrixfac-
structtheoriginallatentfeatures.
torization: A comprehensive review,” IEEE Transac-
6.4.ResultsonMNISTDataset tionsonknowledgeanddataengineering,2012.
The MNIST dataset [6] is a collection of handwritten
[3] P.-J. Huang, L.-Y. Chiu, C.-C. Lee, Y.-M. Yeh, K.-
digits commonly used for training and testing image pro-
Y.Huang,C.-H.Chiu,andP.Tang,“msignaturedb: a
cessingsystems. Itcontains70,00028×28-grayscaleim-
databasefordecipheringmutationalsignaturesinhu-
ages of digits. As the MNIST dataset comprises images
mancancers,”Nucleicacidsresearch,2018.
across10classes,wefactorizeditinto10features.
TheresultsofImplicitPGDattackonMNISTinFig.9 [4] D. D. Lee and H. S. Seung, “Learning the parts of
displays a sharp increase in feature errors under both L 2 objectsbynon-negativematrixfactorization,”Nature,
and L ∞ perturbations at a relatively low ε. The MNIST’s 1999.
extracted features (Fig. 10) under different ε clearly illus-
trates the degradation of NMF. Starting from a baseline of [5] D.DonohoandV.Stodden,“Whendoesnon-negative
clean,clearimages,theintroductionofevenasmallpertur- matrixfactorizationgiveacorrectdecompositioninto
bation(ε = 0.002)canaffecttheedgesandfinerdetailsof parts?” Advances in neural information processing
thedigits. Astheperturbationgrows,morepronouncedvi- systems,vol.16,2003.
sualartifactsappear,particularlydistortingdigitswithcom-
plexstructuressuchas’9’,’3’,and’5’. Thesedigitsstartto [6] Y. LeCun and C. Cortes, “MNIST handwritten digit
mergewiththebackgroundordeformsignificantly. database,”2010.
7.ConclusionandFutureWork [7] A. Madry, A. Makelov, L. Schmidt, D. Tsipras,
and A. Vladu, “Towards deep learning models
resistant to adversarial attacks,” arXiv preprint
Table1.RunningtimeofFEattackingmethods.
arXiv:1706.06083,2017.
Dataset Synthetic WTSI Face Swimmer MNIST [8] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Ex-
Back-propagate 11.8s 35.9s 76.4s N/A N/A plaining and harnessing adversarial examples,” arXiv
Implicit 19.9s 17.7s 297.5s 396.0s 5214.3s preprintarXiv:1412.6572,2014.
Throughout our investigation, we systematically ex- [9] L.Luo,Y.Zhang,andH.Huang,“Adversarialnonneg-
plored the susceptibility of NMF to adversarial attacks ativematrixfactorization,”in37thInternationalCon-
acrossaspectrumofbothsyntheticandreal-worlddatasets, ferenceonMachineLearning(ICML),2020.[10] T. Cai, V. Y. Tan, and C. Fe´votte, “Adversarially-
trained nonnegative matrix factorization,” IEEE Sig-
nalProcessingLetters,vol.28,pp.1415–1419,2021.
[11] C.Chen,W.Zhu,B.Peng,andH.Lu,“Towardsrobust
communitydetectionviaextremeadversarialattacks,”
in26thInternationalConferenceonPatternRecogni-
tion(ICPR). IEEE,2022.
[12] S. A. Seyedi, F. A. Tab, A. Lotfi, N. Salahian, and
J. Chavoshinejad, “Elastic adversarial deep nonnega-
tivematrixfactorizationformatrixcompletion,”Infor-
mationSciences,2023.
[13] A. Tversky and I. Gati, “Similarity, separability, and
thetriangleinequality.”Psychologicalreview,1982.
[14] H. Laurberg, M. G. Christensen, M. D. Plumbley,
L. K. Hansen, and S. H. Jensen, “Theorems on pos-
itivedata: Ontheuniquenessofnmf,”Computational
IntelligenceandNeuroscience,2008.
[15] H. W. Kuhn, “The hungarian method for the assign-
ment problem,” Naval research logistics quarterly,
1955.