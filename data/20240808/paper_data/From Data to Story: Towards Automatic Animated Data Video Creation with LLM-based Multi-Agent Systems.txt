© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE
Visualizationconference. Thefinalversionofthisrecordisavailableat: xx.xxxx/TVCG.201x.xxxxxxx/
From Data to Story: Towards Automatic Animated Data Video Creation
with LLM-based Multi-Agent Systems
LeixianShen * HaotianLi † YunWang ‡ HuaminQu §
The Hong Kong University The Hong Kong University Microsoft, The Hong Kong University
ofScienceandTechnology, ofScienceandTechnology, ofScienceandTechnology,
Beijing,China
HongKongSAR,China HongKongSAR,China HongKongSAR,China
ABSTRACT
Creatingdatastoriesfromrawdataischallengingduetohumans’
limitedattentionspansandtheneedforspecializedskills. Recent
advancementsinlargelanguagemodels(LLMs)offergreatoppor-
tunitiestodevelopsystemswithautonomousagentstostreamline
the data storytelling workflow. Though multi-agent systems have
benefits such as fully realizing LLM potentials with decomposed
tasksforindividualagents,designingsuchsystemsalsofaceschal-
lenges in task decomposition, performance optimization for sub-
tasks, and workflow design. To better understand these issues,
we develop Data Director, an LLM-based multi-agent system de-
signed to automate the creation of animated data videos, a repre-
sentative genre of data stories. Data Director interprets raw data,
breaks down tasks, designs agent roles to make informed deci-
sionsautomatically,andseamlesslyintegratesdiversecomponents
of data videos. A case study demonstrates Data Director’s effec-
tiveness in generating data videos. Throughout development, we
have derived lessons learned from addressing challenges, guiding
further advancements in autonomous agents for data storytelling. Figure1:ArchitectureofDataDirector.
We also shed light on future directions for global optimization,
human-in-the-loopdesign, andtheapplicationofadvancedmulti-
modalLLMs. fromrawdata,whichisanewprobleminthevisualizationandsto-
rytellingcommunity. Inthispaper,wespecificallyfocusonarep-
IndexTerms: DataStorytelling,LLM,Multi-Agent,DataVideo
resentativegenreofdatastories[19],animateddatavideos,which
encompassdiversecomponentsandnecessitatethecoordinationof
1 INTRODUCTION
thesediverseelements[6].Existingautomaticmethodsforcreating
Therapidgrowthofdataassetshasdrivenadvancementsinvarious
data videos either require users to prepare various materials from
domains,butithasalsopresentedchallengesforhuman-datainter-
rawdata[25,32,26]orstillinvolvecomplexandtime-consuming
action.Humanshavelimitedattentionspansandmaylackthespe-
manualauthoringprocesses[5,30,8,27,3]. Weenvisionthatau-
cializedskillstoextractvaluableinsightsandcraftengagingdata
tonomousagentscanfacilitatetheautomatictransformationofraw
storiesacrossmultiplemodalities[11]. Automatingthegeneration
data into animated data videos. However, achieving this goal in-
ofstoriesfromrawdatacangreatlyenhancetheefficiencyofdata
volvesovercomingseveralchallenges:
analysisandinformationcommunication.
Recently,advancementsinlargelanguagemodels(LLMs)have • Task Decomposition: Data storytelling involves the generation
showcasedrobustnaturallanguageunderstandingandreasoningca- andcoordinationofdiverseelementssuchasvisualizations,text
pabilities, proving effective across various tasks like data analy- narrations,audio,andanimations.Thesystemshouldaccurately
sis[34,4],documentgeneration[14],andvisualizationcreation[7]. interpretrawdata,breakdownthestorytellingtaskintomanage-
These capabilities open up new avenues to streamline the entire ablesub-tasks,andassignappropriaterolestoagentsspecialized
datastorytellingworkflowbydevelopingsystemsfeaturingLLM- inhandlingspecificaspectsofthetask.
poweredautonomousagents. Inthisparadigm,LLMsserveasthe • Performance Optimization: In each sub-task, the agent is re-
cognitivecoreoftheseagents,enablingthemtoperceiveenviron- quired to make informed decisions based on perception inputs
ments (Perception), make decisions (Brain), and take responsive and determine the appropriate tools and methods to use. Each
actions (Action), thereby assisting humans in automating a wide sub-task often relies on the outputs of preceding stages, high-
rangeoftasks[35]. lightingtheinterdependenceamongthesesub-tasks.Soensuring
Therefore, we aim to explore the potential of LLM-based au- optimalperformanceforeachoneiscrucial.
tonomous agents in facilitating end-to-end storytelling directly • Workflow Design: Storytelling involves numerous intercon-
nectedsub-taskswithdiversesequenceschemes. Taskssuchas
*e-mail:lshenaj@connect.ust.hk data visualization, crafting narration, recording audio, design-
†e-mail:haotian.li@connect.ust.hk ing animations, and aligning diverse components are typically
‡e-mail:wangyun@microsoft.com non-linearandtheirordermayvary,presentingchallengesinde-
§e-mail:huamin@cse.ust.hk terminingtheoptimalapproach.Thesystemneedstofacilitatea
seamlessworkflowthatautomatesandeffectivelyintegratesall
thesesub-tasks.
Tobetterunderstandtheseissues,wedevelopDataDirector,an
1
4202
guA
7
]CH.sc[
1v67830.8042:viXra© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization
conference.Thefinalversionofthisrecordisavailableat:xx.xxxx/TVCG.201x.xxxxxxx/
LLM-basedmulti-agentsystemthatautomatestheentireprocessof Perception. Theperceptionmoduleacceptsandprocessesdiverse
transformingrawdataintoengaginganimateddatavideos.Thesys- information from external environments, transforming it into un-
tem’sarchitectureisshowninFig.1. Specifically,wedecompose derstandablerepresentationsforLLMs[35]. InDataDirector,data
datavideocreationintodistinctsub-tasksbasedondatavideocom- tablesareinputteddirectlyinthepromptasformattedtext. During
ponentsandtheirrelationships. Wedesigntwoagentroles—data theperceptionphase(A),adatapreprocessingmoduleutilizesthe
analystanddesigner—tomanageandconductthesesub-tasks,and dataset’stitleandcontentwithinanLLMsessiontogeneratenat-
optimizetheperformanceofeachsub-taskthroughpromptdesign. urallanguagedescriptions. AsshowninFig.2-A,thedescription
Wealsoexploreeffectivewaystointerconnectthesesub-tasksand involvesahigh-leveloverviewofthedatatopic, alongwithade-
iterativelyrefinetheworkflow. Todemonstratetheeffectivenessof tailedelaborationofthesemanticsofeachdatacolumn,enhancing
DataDirector, weconductacasestudywhereDataDirectorgen- contextualinformationforsubsequentmodelprocesses.Thegener-
eratesadatavideoaboutreal-worldstockpricedata. Finally, we atedNLdatadescriptionandtherawdataarethenfedintothedata
summarizethelessonslearnedfromoursystemdesignfortaskde- analysisbrain.
composition,performanceoptimization,andworkflowdesign,and Role as Data Analyst. Inspired by insight-based visualization
provide insights to guide future developments in multi-agent sys- andunderstandingtechniques[39,33],whenanalyzingdata,Data
temsfortheautomatictransformationofrawdataintodatastories. Director first prompts the LLM to extract top-k interesting data
insights (B) from raw data, based on data analysis task model-
2 DATADIRECTOR ing[23,21]. Theseinsightsguidethegenerationofvisualizations
ThissectionwillfirstgiveanoverviewofDataDirector’sarchitec- (sub-task1)andtextnarration(sub-task2). FollowingtheChain-
tureandthenintroduceourdesignpractices. of-Thoughtstrategy[9]andcarefullydesignedLLMprompts,the
brainincrementallyderivesalistofinsights(B),declarativeVega-
2.1 Overview Litevisualizations[18](C),andtextnarrations(D)stepbystep.
Role as Designer. Once the “data analyst” has prepared visual-
Data Director is an LLM-based multi-agent system powered by
izationsandcorrespondingtextnarrations,the“designer”agentfo-
GPT-4 [2]. We follow existing conceptual framework of LLM-
cusesoncreatingdynamicanimations(sub-task3)andsynchroniz-
basedagenttodesignthreecomponents:perception,brain,andac-
ingcomponents(sub-task456). Animationsarecategorizedinto
tion[35]. ThearchitectureofDataDirectorisillustratedinFig.1,
twotypes: visualeffects(e.g.,fade,grow,fly,zoom,etc.) applied
with a central controller (a) scheduling all components. User-
tovisualizationelements,andannotationsthatintroduceadditional
generateddata(b)isdirectlyinputintoDataDirector. Thepercep-
visual elements at the right moments. Animation effects are de-
tionmodule(c)preprocessesthedata,whichisthenfedintothefirst
terminedbydistillingchoicesfromananimationlibraryintonat-
agentactingasadataanalyst(d).Thisagent’stasksincludeextract-
urallanguageprompts(E).Theagentistaskedwithselectingthe
inginsights,visualizingdata,andcraftingnarrationtext. Thegen-
optimal timing for animation applications, identifying the precise
eratedvisualizationandnarrationtextarepassedtothenextagent,
visualelementsthatwillbeanimated, andchoosingtheappropri-
whichactsasadesigner(e).Thisagentisresponsibleforanimating
ateanimationeffects. Furthermore,giventhecomplexityofVega-
andannotatingthecontent,aswellascoordinatingthedatavideo
Lite-specifiedannotatedvisualizations,weadoptahierarchicalap-
components. Finally,thecontroller(a)utilizesthedecisionsmade
proach, initially generating base visualizations with the data ana-
by the multi-agent system to generate a data video with relevant
lystagentandsubsequentlyenhancingthemwithannotations(with
tools(f).
Vega-Litespecifications)forimprovedoutcomes(F)usingthede-
2.2 TaskDecompositionandWorkflowDesign signeragent.Textnarrationservesasatimeline,transformingtem-
poralsynchronizationintosemanticlinksbetweenstaticnarration
Todesignmulti-agentsystemsforcomplextasks,suchasdatasto-
segmentsandvisualelements[32,25].
rytelling,itiscrucialtodecomposetheprocessintopatternedsub-
Controller. As shown in the middle of Fig. 2, based on the NL
tasks,facilitatingmodellearningandinterpretation[35]. Taskde-
outputsofthemodels,thecontrollerfirstutilizestext-to-speechser-
composition is a balance art between accuracy and efficiency.
vicestoconverttextnarrationsintoaudiowithprecisetimestamps,
Coarse tasks may exceed the model’s capabilities, leading to hal-
ensuring alignment with the data video timeline. Then, the con-
lucinations or non-computational results, while excessively fine-
trollerinvokesavisualizationrenderertoconvertVega-Litespec-
grainedtasksmayoverwhelmthemodelwithexcessivetasks, af-
ificationsintoSVGfiles. EachSVGelementisautomaticallyas-
fectingefficiencyandincreasingcosts.
sociated with blackened data and visualization structure informa-
To decompose the tasks in the context of data video creation,
tion[25,28]. NarrationsegmentsarefurtherlinkedtotheseSVG
we first break down data videos into basic components and the
visual elements based on the text-visual links established by the
relationship between these components, following previous re-
designeragent, akintoDataPlayer[25]. Thenthecorresponding
search[32,25]. Thebasiccomponentsofdatavideosincludedata
animationeffectsareappliedtotheSVGelementsbasedonthede-
visualizations,textnarrations,andvisualanimations.Tomakesure
signeragent’sdecisions. Next,annotatedvisualizationsareparsed
thesecomponentsappearcoherentlyindatavideos,threerelation-
to detect SVG annotation elements, integrating “fade in” anima-
shipsneedtobetakencareof: 1)Animatedvisualizationelements
tions at corresponding timestamps. Finally, the controller calls a
mustsemanticallyconnectwithcorrespondingtextnarrationseg-
videosynthesizertomergevisualizations,audionarration,andani-
ments;2)Animationeffectsmustbetailoredtothevisualizationel-
mationsequencesintothefinaldatavideo.
ementstheyaccompany;3)Textnarrationshouldaligntemporally
withtheanimations,servingasthetimeline.
With these identified components and relationships, we assign
3 CASESTUDY
thethreecomponentcreationtasksandthethreerelationshipman- Fig.2illustratesareal-worldcasestudyusingstockpricedata.Data
agementtasks(sixsub-tasksintotal)totwoLLM-poweredagents, Directorgeneratesdiverseinsights(B)andvisualizesstockprices
assumingtherolesofadataanalystandadesigner. Additionally, overtimeusinglinecharts(C).Thenarrationfollowedastructured
we develop a controller to manage the input and output of each approach(C):startingwithanoverview,detailingeachcompany’s
module, parse outputs, and invoke appropriate tools for tasks be- performance, andconcludingwithasummary. Basedonthenar-
yondLLMcapabilities. ration, pointsandtextannotationswereaddedtoeachcompany’s
Fig. 2 illustrates a case study based on real-world stock price linetohighlightitsnotablecharacteristic(F).ThebottomofFig.2
dataoffourITcompanies. displayskeyanimationframes(E)fromthevideo,witheachframe
2© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization
conference.Thefinalversionofthisrecordisavailableat:xx.xxxx/TVCG.201x.xxxxxxx/
Figure2:AnexamplewalkthroughofDataDirector.
numberedtocorrespondwithspecifictimestampsinthenarration 4.1 TaskDecomposition
(D).Thisdemonstrateswhenspecificanimationsaretriggered.The
final data video starts with an entrance animation showcasing all BalancingAccuracyandEfficiency.Taskdecompositionfordata
elements, and when discussing each company, the respective line storytellingrequiresbalancingaccuracyandefficiency. Whende-
is highlighted individually. Overall, the entire narrative is well- composing tasks, first, it is essential to identify which tasks the
crafted,withsmootharticulationandappropriatelydesignedvisu- modelexcelsin(e.g.,naturallanguagegeneration,reasoning,and
alizationsandanimations,resultinginanengagingdatavideo. text-based decision-making) and distinguish these from tasks that
necessitateexternaltools(e.g.,visualizationrendering,audiogen-
eration, and video synthesizing). Second, the sub-tasks result-
4 LESSONSLEARNED
ing from decomposition should be well-defined and manageable.
Thissectionwilldiscussthelessonslearnedthroughoutthedevel- These sub-tasks can then be grouped to shape agent roles that
opmentofDataDirector,focusingontaskdecomposition,perfor- align with the inherent characteristics of the tasks. For example,
manceoptimization,andworkflowdesign. DataDirectorbreaksdowntasksbasedonthedatavideocompo-
3© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization
conference.Thefinalversionofthisrecordisavailableat:xx.xxxx/TVCG.201x.xxxxxxx/
nents,andorganizessub-tasksintoanalysis-focusedtasksthatgen- LLMs to select insight types (B), animation types (E), and anno-
erate static content from raw data, and design-focused tasks that tationtypes(F)fromasetofpredefinedcandidates. Additionally,
require creative input and the derivation of dynamic effects from incorporatingconditionallogic(e.g.,if-elsestatements),employing
thestaticcontent. Third,thesuitablecombinationofsub-taskscan few-shotorone-shotpromptingtechniqueswithcuratedexamples,
enhanceboththemodel’saccuracyandefficiency,asdemonstrated and referencing URLs for concrete examples can further enhance
inSec.2.2,whereanimationdesignandtemporalsynchronization themodel’sunderstandingandtaskexecutionaccuracy. Morespe-
weremerged. Finally,atop-downapproachcanbeadoptedtodis- cificexamplescanbefoundinAppendixA.
secttasksprogressively,designingsuitableandefficientmodelsfor
eachsub-task. 4.3 WorkflowDesign
DataFeedingwithContextualInformation.Providingthemodel SharedRepresentation. ThispaperprimarilyutilizestheGPT-4
withamplecontextualinformationhasbeenfoundtoenhancethe model[2], withnaturallanguageservingasthemediumforinput
accuracyandqualityofitsgeneration[34].Asthemodelprogresses andoutput,settingthestageforthisdiscussion. Effectivecommu-
throughsub-tasksstepbystep,thecontextisenrichedandupdated, nicationbetweenagentsandbetweenagentsandexternaltoolsre-
offeringmoreinformationforsubsequentoperations.Forexample, quiresanappropriatesharedNLrepresentation.Forinstance,Vega-
inthedataanalystagent,themodelgraduallygathersinformation LiteisemployedinDataDirectortorepresentallvisualizationsand
ondatadescriptions,insights,visualizations,andnarrations. How- annotations,whileinsightsandanimatedvisualinformationareen-
ever,whenanagentperceivesdatafromtheenvironment,thedata capsulatedinaJSONformat,incorporatingspecificfeatureinfor-
itselfismerelynumericalwithlimitedcontext.Wehavefoundthat mation(seeFig.2). Suchsharedrepresentationsmustbecompre-
semanticallyenrichingthedataandsupplyingthemodelwithcon- hensibleandeasilygeneratedbythemodelandreadilyinterpreted
textualinsightssignificantlyenhanceitseffectiveness. Forexam- byexternaltoolsformappingtointernaloperations. Futurework
ple,DataDirectorusestheLLMtogenerateanNLdescriptionof couldinvolvedesigningaglobalsharedrepresentationforspecific
adatatablewithatitle. Futureresearchcouldintegrateinnovative applicationscenariostoassistmodelsinbetterpreservingandgen-
techniquesforenhancingdatacomprehensionandexploringnovel eratingcontextualinformation.
methodsforinputtingdataintoLLMs.
Iterative Development. Various sub-tasks within the workflow
presentdiversesequencingstrategies.Forinstance,annotationscan
4.2 PerformanceOptimization begeneratedsimultaneouslywithvisualizations,duringanimation
Effective prompt design is crucial for enhancing the performance generation, or using a hierarchical approach as described in Data
and output quality of LLM-based agent systems. The complete Director. Similarly, in data analysis, one may opt to produce vi-
prompt of Data Director can be found in Appendix A. The key sualizationseitherbeforeorafternarration. Designingtheoptimal
strategies for optimizing prompts within this context are outlined sequencingstrategyischallengingduetotheabsenceofaquantita-
asfollows: tiveglobaloptimizationobjective.Hence,forapplicationsofLLM-
basedagents, weadoptaniterativedesignmethodologybasedon
Assignment of Appropriate Tasks for LLMs. The foundation
task decomposition and local performance optimization [1]. This
of effective prompt design lies in the careful design of tasks for
involves a cycle of ideation, implementation, experimental evalu-
LLMs. It is essential to identify the tasks where LLMs are good
ation, and error analysis. Striving for consistency in the model’s
at. Furthermore,assigningthemodelaspecificrole,suchasadata
outputs also necessitates adherence to established guidelines and
analyst or designer in Data Director, can guide the model to pro-
meticulousparameteradjustments.WenotethatDataDirectorpre-
duce domain-specific and contextually relevant outputs. In addi-
sentedhereistheresultofouriterativeoptimizationsandmaynot
tion,supplyingthemodelwithpreciseandcomprehensivecontext
necessarily represent the optimal configuration. Our intention in
canenhanceitsunderstandingoftasks,therebyimprovingtheac-
developing this prototype tool is to uncover valuable lessons and
curacyandrelevanceofitsresponses. Thisinvolvescraftingwell-
insightsthatcanguidefutureresearch.
structured prompts (outlined in Appendix A) and designing com-
plementarymoduleslikedatapreprocessinginDataDirector.
Cognitive Processing Time and Task Decomposition. Allow-
5 FUTUREWORK
ing the model adequate cognitive processing time is essential for GlobalOptimizationandBenchmarking. Theiterativedevelop-
achievinghigh-qualityoutputsandalleviatinghallucinations. Be- mentofmulti-agentsystems,asmentionedinSec.2.2,suffersfrom
yondthetaskdecompositiondiscussedabove, intermsofprompt thelackofaglobaloptimizationandvalidationframeworkforend-
design, thesub-taskscanbeclearlydefinedwithsequentialsteps, to-end data video generation [16]. Additionally, the community
facilitatingtheapplicationoftheChain-of-Thoughtstrategy.More- lacksawidelyrecognizedbenchmark.Thecomplexityofthischal-
over,thenumberoftaskswithinonepromptshouldbebalancedto lenge is compounded by the inherently subjective nature of data
deducetaskdifficulty. Inaddition,promptingthemodeltoexplain storytelling quality, which is subject to individual interpretation
itsdecisionsoroutlineitssolutionmethodologybeforeconcluding andthemultifaceteddecision-makinginvolvedinvariousnarrative
canpromoteamorethoughtfulandpreciseresponsegenerationpro- forms.Futureworkcouldincludesummarizingrelevantrubricsand
cess.Forinstance,inthedesigneragent,DataDirectorpromptsthe conductingempiricalstudiestoderivequantitativeguidelines.With
LLMtoitschoicesregardingtheanimationandannotationdesign, thesewell-definedmetrics,anevaluationagentcanalsobeaddedto
whichenhancesthedecisionaccuracyandsimplifiesthedebugging enhanceexistingworkflow.Additionally,thereisaneedtodevelop
ofLLMapplicationsduringdevelopment. a universally shared representation for optimization and incorpo-
CraftingPreciseandUnambiguousInstructions.Theclarityand rate domain-specific languages and objectives tailored to diverse
specificityofinstructionsareparamountineffectivepromptdesign. scenarios[15,17].
Utilizingdelimiters(e.g.,“‘“‘,“”or<>)tosegmentpromptsec- Human-in-the-Loop.Data-drivenend-to-endgenerationsolutions
tionscanreduceambiguityandaidcomprehension.Providingfine- can result in one-size-fits-all outputs. To address the issues, in-
grained requirements and employing the correct use of keywords corporating human-in-the-loop is an essential approach to com-
(e.g., “summarize” vs. “extract”) ensures that the model adheres pensate for model limitations and generate more personalized re-
closelytothetaskparameters. Furthermore,requestingstructured sults [11, 10]. In data storytelling, three paradigms of human-in-
outputs(e.g.,JSONandHTML)andofferingarangeofresponse the-loop can be further explored: firstly, allowing users to input
optionscanguidethemodeltowardproducingorganizedandprac- moreinformationintheperceptionmodulewhilemaintainingthe
ticaloutputs.Forexample,asshowninFig.2,DataDirectorallows currentarchitecture,articulatingtheirgoalsandrequirementsinthe
4© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization
conference.Thefinalversionofthisrecordisavailableat:xx.xxxx/TVCG.201x.xxxxxxx/
formslikenaturallanguage[20],example[36,22],andsketch[14]; REFERENCES
secondly, integrating humans intosub-tasks to achieve local opti-
[1] Chatgpt prompt engineering for developers. https://learn.
mization before proceeding to the next stage, such as generating deeplearning.ai/chatgpt-prompt-eng/.4
multiplecandidatesforvisualizationandannotationaftergenerat- [2] Openaigpt-4.https://openai.com/gpt-4.2,4
ingdatainsights; thirdly,usersprovidingconversationalfeedback [3] F.Amini,N.H.Riche,B.Lee,A.Monroy-Hernandez,andP.Irani.
basedontheoutput[20,31],withtheagentgeneratingnewend-to- Authoringdata-drivenvideoswithdataclips. IEEETransactionson
endresultsbasedonthisfeedback.Additionally,thesemethodscan VisualizationandComputerGraphics,23(1):501–510,2017.1
alsobeflexiblycombined. [4] C.BeasleyandA.Abouzied. Pipe(line)Dreams: FullyAutomated
Keeping Up with Cutting-Edge Models. This paper primarily End-to-EndAnalysisandVisualization. InProceedingsofthe2024
usestheGPT-4model. However,withtherapidevolutionoflarge Workshop on Human-In-the-Loop Data Analytics, pp. 1–7. ACM,
language models (LLMs), GPT-4 is swiftly being augmented by 2024.1
theemergenceofmultimodalLLMs[38]. Theseadvancedmodels [5] Y. Cao, J. L. E, Z. Chen, and H. Xia. DataParticles: Block-based
offerexpandedfunctionalitiesforhandlingmultimodalinputsand andLanguage-orientedAuthoringofAnimatedUnitVisualizations.
outputs, significantly impacting task decomposition, performance In Proceedings of the 2023 CHI Conference on Human Factors in
optimization, and workflow design within our established frame-
ComputingSystems,CHI’23,pp.1–15.ACM,2023.1
[6] H.Cheng,J.Wang,Y.Wang,B.Lee,H.Zhang,andD.Zhang. In-
work(Fig.1). Forinstance,initialgenerationofvisualizationfiles
vestigatingtheroleandinterplayofnarrationsandanimationsindata
couldbefollowedbyrefinementinasubsequentmultimodalmod-
videos.ComputerGraphicsForum,41(3):527–539,2022.1
ule,potentiallyleadingtodirectgenerationofvideocontent. The
[7] V. Dibia. LIDA: A Tool for Automatic Generation of Grammar-
enhancement of model capabilities presents numerous opportuni-
AgnosticVisualizationsandInfographicsusingLargeLanguageMod-
ties.Futureworkshouldnotonlytrackthelatestmodelstodevelop
els. InProceedingsofthe61stAnnualMeetingoftheAssociationfor
morepowerfulagentsbutalsoleveragediversemodelswithdiffer-
ComputationalLinguistics,ACL’23,pp.113–126.ACL,2023.1
entcapabilitiestoenrichdatastorytelling.Thisincludesexpanding
[8] T.Ge, B.Lee, andY.Wang. CAST:AuthoringData-DrivenChart
beyond individual static charts to incorporate visual and musical Animations. InProceedingsofthe2021CHIConferenceonHuman
content[29],supportingmorecomplexinsightsandmulti-viewvi- FactorsinComputingSystems,CHI’21,pp.1–15.ACM,2021.1
sualizations[13],integratingexistingcomputationaldesignspaces [9] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large
(e.g.,camera[12]andnarrativestructure[37]),andaccommodating languagemodelsarezero-shotreasoners. InS.Koyejo,S.Mohamed,
moredatatypes(e.g.,unstructuredgraphs[24]). Achievingthese A.Agarwal,D.Belgrave,K.Cho,andA.Oh,eds.,ProceedingsofAd-
features requires enhancing shared representations and designing vancesinNeuralInformationProcessingSystems,NIPS’22,vol.35,
correspondingprompts(similartohowDataDirectorintegratesan- pp.22199–22213,2022.2
imation). [10] H.Li,Y.Wang,Q.V.Liao,andH.Qu. WhyisAInotaPanaceafor
Inherent Limitations of Large Language Models. LLMs are DataWorkers? AnInterviewStudyonHuman-AICollaborationin
powerfulbutexhibitseveralinherentlimitations,suchaserroraccu- DataStorytelling.arXivpreprintarXiv:2304.08366,2023.4
mulation,inconsistentresults,hallucinations,andhightimecosts. [11] H.Li, Y.Wang, andH.Qu. WhereAreWeSoFar? Understand-
Most importantly, we need to acknowledge that the content from ingDataStorytellingToolsfromthePerspectiveofHuman-AICol-
laboration. InProceedingsofCHIConferenceonHumanFactorsin
LLMsisgenerativebutnottruthful.Toaddresserroraccumulation,
ComputingSystems,CHI’24,pp.1–28,2024.1,4
incorporatingahuman-in-the-loopapproachandprovidingtimely
[12] W.Li,Z.Wang,Y.Wang,D.Weng,L.Xie,S.Chen,H.Zhang,and
tipscanimproveaccuracy. Consistencyinresultscanbeimproved
H.Qu.GeoCamera:TellingStoriesinGeographicVisualizationswith
by strictly following established guidelines and creating supple-
CameraMovements. InProceedingsofCHIConferenceonHuman
mentaryrulestohandlethemodel’soutput. Hallucinations,where
FactorsinComputingSystems,CHI’23,pp.1–15.ACM,2023.5
themodelgeneratesplausiblebutincorrectinformation,canbeim-
[13] Y.Lin,H.Li,A.Wu,Y.Wang,andH.Qu.DMiner:DashboardDesign
provedbyimplementingsomepromptoptimizationstrategies,such MiningandRecommendation. IEEETransactionsonVisualization
asself-repairmechanisms,theChain-of-Thought(CoT)approach, andComputerGraphics,30(7):4108–4121,2024.5
and code-interpreter functionalities [34]. Lastly, high-time costs [14] Y. Lin, H. Li, L. Yang, A. Wu, and H. Qu. InkSight: Leverag-
canbemanagedbybreakingdowntasks,findingsuitablesolutions ingSketchInteractionforDocumentingChartFindingsinComputa-
foreach(e.g.,heuristics,basicmodels,andLLMs).It’simportantto tionalNotebooks. IEEETransactionsonVisualizationandComputer
recognizethatLLMsarenotaone-size-fits-allsolution;sometimes, Graphics,30(1):944–954,2024.1,5
basicmodelsorheuristicrulescanbehighlyeffectivewithoutthe [15] Y. Ouyang, L. Shen, Y. Wang, and Q. Li. NotePlayer: Engaging
needforLLMs. JupyterNotebooksforDynamicPresentationofAnalyticalProcesses.
arXivpreprintarXiv:2408.01101,pp.1–15,2024.4
6 CONCLUSION [16] J.S.Park,J.O’Brien,C.J.Cai,M.R.Morris,P.Liang,andM.S.
Bernstein. GenerativeAgents: InteractiveSimulacraofHumanBe-
TherapidevolutionofLLMspresentsnewopportunitiesforcreat-
havior.InProceedingsoftheAnnualACMSymposiumonUserInter-
ingend-to-endmulti-agentsystemsfordatastorytelling. Through
faceSoftwareandTechnology,UIST’23,pp.1–22.ACM,2023.4
the development of Data Director, we have derived valuable in-
[17] S.Sallam,Y.Sakamoto,J.Leboe-McGowan,C.Latulipe,andP.Irani.
sights into task decomposition, local performance optimization
TowardsDesignGuidelinesforEffectiveHealth-RelatedDataVideos:
through prompt design, and workflow design. In addition, we
AnEmpiricalInvestigationofAffect,Personality,andVideoContent.
also shed light on future directions in the development of glob-
InProceedingsofCHIConferenceonHumanFactorsinComputing
ally optimized multi-agents, human-in-the-loop systems, integra- Systems,CHI’22,pp.1–22.ACM,2022.4
tion of cutting-edge multimodal models, and addressing inherent [18] A.Satyanarayan,D.Moritz,K.Wongsuphasawat,andJ.Heer. Vega-
LLMlimitations. Lite: AGrammarofInteractiveGraphics. IEEETransactionsonVi-
sualizationandComputerGraphics,23(1):341–350,2017.2
ACKNOWLEDGMENTS
[19] E. Segel and J. Heer. Narrative visualization: Telling stories with
The authors wish to thank all reviewers for their valuable feed- data. IEEETransactionsonVisualizationandComputerGraphics,
back. ThisworkhasbeenpartiallysupportedbyRGCGRFGrant 16(6):1139–1148,2010.1
16210321. [20] L. Shen, E. Shen, Y. Luo, X. Yang, X. Hu, X. Zhang, Z. Tai, and
J.Wang. TowardsNaturalLanguageInterfacesforDataVisualiza-
tion: ASurvey. IEEETransactionsonVisualizationandComputer
Graphics,29(6):3121–3144,2023.5
5© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization
conference.Thefinalversionofthisrecordisavailableat:xx.xxxx/TVCG.201x.xxxxxxx/
[21] L.Shen,E.Shen,Z.Tai,Y.Song,andJ.Wang.TaskVis:Task-oriented Conference on Human Factors in Computing Systems, CHI’21, pp.
Visualization Recommendation. In Proceedings of the 23th Euro- 1–18.ACM,2021.1
graphicsConferenceonVisualization(ShortPapers),EuroVis’21,pp. [31] Y. Wang, Z. Hou, L. Shen, T. Wu, J. Wang, H. Huang, H. Zhang,
91–95.Eurographics,2021.2 andD.Zhang. TowardsNaturalLanguage-BasedVisualizationAu-
[22] L.Shen,E.Shen,Z.Tai,Y.Wang,Y.Luo,andJ.Wang.GALVIS:Vi- thoring.IEEETransactionsonVisualizationandComputerGraphics,
sualizationConstructionthroughExample-PoweredDeclarativePro- 29(1):1222–1232,2023.5
gramming. In Proceedings of the 31st ACM International Confer- [32] Y.Wang,L.Shen,Z.You,X.Shu,B.Lee,J.Thompson,H.Zhang,and
enceonInformation&KnowledgeManagement,CIKM’22,pp.4975– D.Zhang. WonderFlow:Narration-CentricDesignofAnimatedData
4979.ACM,2022.5 Videos.IEEETransactionsonVisualizationandComputerGraphics,
[23] L.Shen,E.Shen,Z.Tai,Y.Xu,J.Dong,andJ.Wang. VisualData pp.1–15,2024.1,2
AnalysiswithTask-BasedRecommendations. DataScienceandEn- [33] Y.Wang,Z.Sun,H.Zhang,W.Cui,K.Xu,X.Ma,andD.Zhang.
gineering,7(4):354–369,2022.2 DataShot: Automatic Generation of Fact Sheets from Tabular
[24] L. Shen, Z. Tai, E. Shen, and J. Wang. Graph Exploration With Data. IEEETransactionsonVisualizationandComputerGraphics,
Embedding-GuidedLayouts.IEEETransactionsonVisualizationand 26(1):895–905,2020.2
ComputerGraphics,30(7):3693–3708,2024.5 [34] Y.Wu,Y.Wan,H.Zhang,Y.Sui,W.Wei,W.Zhao,G.Xu,andH.Jin.
[25] L.Shen,Y.Zhang,H.Zhang,andY.Wang. DataPlayer: Automatic AutomatedDataVisualizationfromNaturalLanguageviaLargeLan-
GenerationofDataVideoswithNarration-AnimationInterplay.IEEE guageModels:AnExploratoryStudy.InProceedingsoftheACMon
Transactions on Visualization and Computer Graphics, 30(1):109– ManagementofData,SIGMOD’24,pp.1–28.ACM,2024.1,4,5
119,2024.1,2 [35] Z.Xi,W.Chen,X.Guo,W.He,andetal. TheRiseandPotentialof
[26] D.Shi,F.Sun,X.Xu,X.Lan,D.Gotz,andN.Cao. AutoClips: An LargeLanguageModelBasedAgents:ASurvey.arXiv:2309.07864,
AutomaticApproachtoVideoGenerationfromDataFacts.Computer pp.1–86,2023.1,2
GraphicsForum,40(3):495–505,2021.1 [36] L.Xie,Z.Zhou,K.Yu,Y.Wang,H.Qu,andS.Chen.Wakey-Wakey:
[27] M.Shin,J.Kim,Y.Han,L.Xie,M.Whitelaw,B.C.Kwon,S.Ko, Animate Text by Mimicking Characters in a GIF. In Proceedings
andN.Elmqvist. Roslingifier:Semi-AutomatedStorytellingforAni- ofthe36thAnnualACMSymposiumonUserInterfaceSoftwareand
matedScatterplots.IEEETransactionsonVisualizationandComputer Technology,UIST’23,pp.1–14.ACM,2023.5
Graphics,29(6):2980–2995,2023.1 [37] L.Yang,X.Xu,X.Y.Lan,Z.Liu,S.Guo,Y.Shi,H.Qu,andN.Cao.
[28] L.S.SnyderandJ.Heer. DIVI:DynamicallyInteractiveVisualiza- ADesignSpaceforApplyingtheFreytag’sPyramidStructuretoData
tion. IEEE Transactions on Visualization and Computer Graphics, Stories.IEEETransactionsonVisualizationandComputerGraphics,
30(1):403–413,2024.2 28(1):922–932,2022.5
[29] T.Tang,J.Tang,J.Lai,L.Ying,Y.Wu,L.Yu,andP.Ren.SmartShots: [38] S.Yin,C.Fu,S.Zhao,K.Li,X.Sun,T.Xu,andE.Chen. Asurvey
AnOptimizationApproachforGeneratingVideoswithDataVisual- onmultimodallargelanguagemodels.arXiv:2306.13549,2023.5
izationsEmbedded.ACMTransactionsonInteractiveIntelligentSys- [39] L.Ying, Y.Wang, H.Li, S.Dou, H.Zhang, X.Jiang, H.Qu, and
tems,12(1):1–21,2022.5 Y.Wu. RevivingStaticChartsintoLiveCharts. IEEETransactions
[30] J.R.Thompson, Z.Liu, andJ.Stasko. DataAnimator: Authoring onVisualizationandComputerGraphics,pp.1–15,2024.2
ExpressiveAnimatedDataGraphics.InProceedingsofthe2021CHI
6© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization
conference.Thefinalversionofthisrecordisavailableat:xx.xxxx/TVCG.201x.xxxxxxx/
A FULLPROMPT
Listing1:Thepromptofgeneratingtextdescriptionfordatatables
Give a short and consistent description of the following data table and columns:
{{table}}
The title of the data table is: {{title}}
The output JSON format is:
{
"Description": [A]
}
where [A] is the generated description.
Listing2:Thepromptoftheagentactingasadataanalyst
You are a data analyst. You have a data table at hand.
{{description}}
The full data table is:
{{table}}
You need to complete several tasks, please think step by step.
Task 1: Please list the top insights you can gather from the following data table.
Notes for insight extraction:
- The output JSON format is:
[
{
"insight": insight content,
"type": a list of corresponding insight types
}
]
- The insight type belongs to the following list: [Change Over Time, Characterize Distribution, Cluster, Comparison,
Correlate, Determine Range, Deviation, Find Anomalies, Find Extremum, Magnitude, Part to Whole, Sort, Trend]. One
insight can correspond to multiple types.
- The selected insights should be obvious, valuable, and diverse.
- Double-check that the comparison of numerical magnitudes is accurate. Ensure that the insight is right.
- Ignore the "index" column.
Task 2: Please draw a Vega-Lite visualization to tell the insights based on the data table.
Notes for visualization generation:
- Your title should be less than 10 words.
- If you use the color channel, refer to the following color scheme: https://vega.github.io/vega/docs/schemes/
- Your visualization should have the right height and width.
- If the visualization is a line chart, it should include data points.
- If the focus of the data table presentation is on percentage information of one single column, then use a pie chart to
present it. The percentage of each sector should be displayed on the corresponding sector via text annotation like
https://vega.github.io/vega-lite/examples/layer_arc_label.html.
- Use a single chart to visualize the data, and the index column should not be visualized.
Task 3: Please write a streamlined narration for the insights.
Notes for narration generation:
- Writing narration in the tone of describing the visualization instead of describing the data table.
- Your logic should be compelling, linking insights into a story, and avoiding enumerating insights.
- Avoid using additional explanations. Avoid speculating on conclusions and redundant explanations beyond the data.
- Content should be streamlined.
- Ignore the "index" column.
The final output JSON format is:
{
"Insights": [A],
"Visualization": [B],
"Visualization_Type":[C]
"Narration": [D]
}
where [A] is the listed insights in Task 1, [B] is the visualization specification generated in Task 2, [C] is the
visualization type, which is one of the values of [bar, scatter, pie, line], and [D] is the narration text for the
insights in Task 3.
7© 2024 IEEE. This is the author’s version of the article that has been published in the proceedings of IEEE Visualization
conference.Thefinalversionofthisrecordisavailableat:xx.xxxx/TVCG.201x.xxxxxxx/
Listing3:Thepromptoftheagentactingasadesigner
You are a data video designer. You have a static visualization and corresponding insightful narration text at hand, as
well as the original data table. If necessary, embellish the visualization with corresponding annotations (e.g.,
arrow, text, circle, etc.) to tell the story more vividly in the narration text. Please think step by step.
The visualization is: {{visualization}}
The insightful narration is: {{narration}}
The data table is: {{table}}
Task 1: You want some animation to appear on the visual elements during the audio narration of the video to aid in
telling the data story. Consider the narration as a timeline for the video. Insert animations inside the narration
text where you feel they are needed. The corresponding animation will be triggered when the video reaches the
corresponding narration segment and ends at the end of the narration segment.
Notes for animation generation:
- The output JSON format is:
[
{
"animation": animation type,
"narration": narration segment,
"target": the visual elements that the animation applies to,
"index": a list of related data table rows index. If empty, output [],
"explanation": the explanation of why the animation needs to be added
}
]
- One sentence in the narration can correspond to multiple animations.
- Output only the part marked with animation.
- Narration text cannot be modified.
- There are three types of animations: entrance, emphasis, and exit.
- Entrance animations include [Axes-fade-in, Bar-grow-in, Line-wipe-in, Pie-wheel-in, Pie-wheel-in-and-legend-fly-in,
Scatter-fade-in, Bar-grow-and-legend-fade-in, Line-wipe-and-legend-fade-in, Fade-in, Float-in, Fly-in, Zoom-in].
- Emphasis animations include [Bar-bounce, Zoom-in-then-zoom-out, Shine-in-a-short-duration, Highlight-one-and-fade-
others].
- Exit animations include [Fade-out].
- [Axes fade in] animations can only be used at the beginning (first sentence) of a whole narration.
- Visual elements that have an entrance animation effect applied will not appear on the canvas until the animation is
triggered. Elements that have an exit animation effect applied will disappear from the canvas after the animation.
Elements that do not have any animation effect applied will appear on the canvas by default.
- Visual elements can only be emphasized or disappear after they appear on the canvas, and elements cannot be emphasized
after they disappear.
Task 2: Visualization embellishment generation
Notes for generation:
- Add annotation only when you think you need to, or export the original visualization if you do not feel the need to
add it.
- Tag the narration text in the format of:
[
{
"type": a list of annotation types, which is one or a set from [mark label, circle, text, rule, trend line, arrow],
"description": annotation description and explanation,
"index": a list of related data table rows index, If empty, output [],
"nar": narration segment
}
]
- The tagged annotations must correspond to the annotations in the visualization.
- Only output the narration segments marked with annotation. If the value of key "type" is [], do not output the item.
- If the annotated vega-lite specification has a key "layer", then all "mark" and "encoding" keys should be inside the
list value of the key "layer".
- The annotations should not be complex.
- The annotation must correspond to the narration segment. The annotation will appear when the video reaches the
corresponding narration segment.
- The text annotation should be short (e.g., fewer than 6 words).
- Please output the complete bug-free vega-lite specification.
The final output JSON format is:
{
"Annotated_Visualization": [A],
"Annotated_Narration_for_Animation": [B],
"Annotated_Narration_for_Annotation": [C]
}
where [A] is the generated annotated Vega-Lite specification, [B] is the tagged narration text for animation, and [C] is
the tagged narration text for annotation.
8