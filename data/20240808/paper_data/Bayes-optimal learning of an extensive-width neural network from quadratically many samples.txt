Bayes-optimal learning of an extensive-width neural network from
quadratically many samples
Antoine Maillard∗1, Emanuele Troiani2, Simon Martin3,4, Florent Krzakala5, and Lenka
Zdeborová2
1Department of Mathematics, ETH Zürich, Switzerland.
2Statistical Physics of Computation Laboratory, EPFL, Switzerland.
3INRIA - École Normale Supérieure, PSL Research University, Paris, France.
4Laboratoire de Physique de l’École Normale Supérieure, ENS, Université PSL, CNRS,
Sorbonne Université, Université de Paris, F-75005 Paris, France.
5Information, Learning, and Physics Laboratory, EPFL, Switzerland.
Abstract
We consider the problem of learning a target function corresponding to a single hidden
layer neural network, with a quadratic activation function after the first layer, and random
weights. Weconsidertheasymptoticlimitwheretheinputdimensionandthenetworkwidth
areproportionallylarge. Recentwork[Cuietal.,2023]establishedthatlinearregressionpro-
videsBayes-optimaltesterrortolearnsuchafunctionwhenthenumberofavailablesamples
is only linear in the dimension. That work stressed the open challenge of theoretically an-
alyzing the optimal test error in the more interesting regime where the number of samples
is quadratic in the dimension. In this paper, we solve this challenge for quadratic activa-
tions and derive a closed-form expression for the Bayes-optimal test error. We also provide
an algorithm, that we call GAMP-RIE, which combines approximate message passing with
rotationallyinvariantmatrixdenoising,andthatasymptoticallyachievestheoptimalperfor-
mance. Technically,ourresultisenabledbyestablishingalinkwithrecentworksonoptimal
denoising of extensive-rank matrices and on the ellipsoid fitting problem. We further show
empiricallythat,intheabsenceofnoise,randomly-initializedgradientdescentseemstosam-
ple the space of weights, leading to zero training loss, and averaging over initialization leads
to a test error equal to the Bayes-optimal one.
1 Introduction
Learningwithmulti-layerneuralnetworksbroughtimpressiveprogressandapplicationsinmany
areas. It is well established that a large enough non-linear neural network can represent a large
class of functions [Cybenko, 1989]. Yet the conditions under which the values of the weights can
befoundefficiently, andfromhowmanysamplesofthedata, remaintheoreticallyelusive. While
one may hope that a detailed understanding of these fundamental limitations will eventually
allow for a more efficient training, answering such questions for general data and target function
remains, however, beyond the reach of current theoretical methods.
In an early attempt to overcome the difficulty of the above generic question, a long line of work
originating in Gardner and Derrida [1989], Sompolinsky et al. [1990] proposed to study the
optimal sample-complexity in the so-called teacher-student setting, where the target function
corresponds to a “teacher” neural network. The architecture of this teacher neural network
∗To whom correspondence should be sent: antoine.maillard@math.ethz.ch.
1
4202
guA
7
]LM.tats[
1v33730.8042:viXrais chosen to be fully connected feed-forward with a given number of layers, their widths, and
activations. The values of each of the weights are generated independently, from a Gaussian
distribution. This teacher neural network is then used to generate an output label y ∈ R for
i
each input data sample x ∈ Rd. Given the architecture of the teacher networks (but not the
i
values of the teacher-weights W∗) and the training set of input-output pairs {y ,x }n , the
i i i=1
smallest achievable test error can then be obtained by averaging the output of a student-neural
network (with the same architecture as the teacher) over the values of weights drawn from the
posterior distribution. We will refer to the accuracy reached this way as the Bayes-optimal one.
It yields the fundamental limitations in learning such tasks, by any possible means, and can
therefore serve as a benchmark.
In the so-called high-dimensional limit [Donoho, 2000], when the input training data are d-
dimensional Gaussian vectors, in the limit d → ∞, the above research program has been carried
out in detail over the last decades for small neural networks having only m = O (1) hidden
d
units, and learning from n = αd data samples, where α = O (1) (see, e.g. Györgyi [1990], Opper
d
and Haussler [1991], Seung et al. [1992], Watkin et al. [1993], Schwarze [1993], Barbier et al.
[2019], Aubin et al. [2019b]). In the more recent literature, this setting is sometimes referred
to as learning single-index and multi-index functions [Bietti et al., 2023, Damian et al., 2024,
Collins-Woodfin et al., 2023]. While early works in this line originated in statistical physics and
used the heuristic replica method [Mézard et al., 1987] to derive the closed-form expressions for
quantities of interest in the high-dimensional limit (with d → ∞, m = O (1) and n = O (d)), a
d d
mathematical establishment followed using rigorous probabilistic methods [Barbier et al., 2019,
Aubin et al., 2019b].
Reaching a closed-form expression for the Bayes-optimal sample complexity for target functions
corresponding to multi-layer teacher neural networks is the next open and very challenging task.
Among the recent work is Cui et al. [2023], that established (non-rigorously, using the replica
method) the Bayes-optimal error for a target function corresponding to a multi-layer neural
network of extensive width (i.e. linearly proportional to the dimension) from a number of sam-
ples also linear in the dimension. Interestingly, in this limit, the Bayes-optimal error resulted
in a quite poor approximation of the function, which can be achieved as well by a simple linear
regression on the input-output pairs. No method, be it a multi-layer neural network (or even
refinementslikeatransformer), willbeabletoachievebetterperformance. [Cuietal.,2023]fur-
ther argue, based on numerical evidence, that quadratically many samples in the dimension are
necessary in order to be able to learn the target function with non-linear activations1 to an
infinitesimally small test error. This is perhaps intuitive as, with an extensive width, the num-
ber of parameters/weights in the teacher network is quadratic in dimension. However, such a
regime is challenging for current theoretical tools. Reaching an analytical explicit expression for
the Bayes-optimal performance in this regime, for the target function in the form of a neural
network of extensive width, is an open, challenging, theoretical problem that has not yet been
solved even for a single hidden layer architecture.
Our contributions – In this paper, we step up to this challenge and derive a closed-form
expression for the Bayes-optimal test error for a target/teacher function corresponding to a
one-hidden layer neural network of extensive width, from quadratically many samples, for a
particular case where the activation function (after the hidden layer) is quadratic. In particular,
our main contributions are:
• We provide a closed-form expression for the Bayes-optimal error of learning an extensive-
width neural network from quadratically many samples, which is the first type of such result
to the best of our knowledge. Such a form is enabled by the high-dimensional limit and
1Notethatforlinearactivations,thetargetfunctionsreducestolinearregressionandcanbelearnedfromlinearly
many samples.
2corresponding concentration of quantities of interest. It notably follows from our formula
that, in the absence of noise in the target function, zero test error is achievable for a sample
complexity α = n/d2 larger than a perfect-recovery threshold α > α where
PR
κ2 1
α = κ− if κ ≤ 1; α = if κ ≥ 1, (1)
PR PR
2 2
with κ = m/d the ratio between the width m and the dimension d. We further notice that
this matches a naive counting of the number of degrees of freedom in the target function.
• We introduce the GAMP-RIE algorithm that combines the generalized approximate message
passing (GAMP) [Donoho et al., 2009, Rangan, 2011, Zdeborová and Krzakala, 2016] with a
matrix denoiser that is based on so-called rotationally-invariant estimators (RIE) [Bun et al.,
2016], and show that in the large size limit, this algorithm reaches the Bayes-optimal error
for all α,κ = Θ(1), where α = n/d2 and κ = m/d.
• On the technical level, our result is enabled by combining results from the analysis of single-
layer neural networks [Barbier et al., 2019] and extensive-rank matrix denoising [Maillard
et al., 2022b]. The derived formula involves the asymptotics of the Harish-Chandra-Itzykson-
Zuber integral of random matrix theory [Harish-Chandra, 1957, Itzykson and Zuber, 1980].
Our approach is notably inspired by recent results on the ellipsoid fitting problem [Maillard
and Kunisky, 2024, Maillard and Bandeira, 2023]. These tools are of independent interest to
the machine learning community, and we anticipate they will have other applications in the
theory of learning.
• We empirically compare the Bayes-optimal performance to the one obtained by gradient de-
scent. In the noiseless case we observe a rather unusual and surprising scenario, as randomly-
initialized gradient descent seems to be sampling the space of interpolants, and leads to twice
the Bayes-optimal error. When averaged over initialization the gradient descent reaches an
error that is very close to the Bayes-optimal. The rigorous establishment of these properties
of gradient descent is left open.
All our numerical experiments are reproducible, and accessible freely in a public GitHub repos-
itory [Maillard et al., 2024].
Further related works – The problem studied in this work is known as phase retrieval in
the case of a single hidden unit (m = 1). Many works considered this problem in the high-
dimensional limit d → ∞, in the regime of n = O(dlogd) samples; see e.g. Candes et al. [2013],
Chen et al. [2019], Demanet and Hand [2014]. A subsequent line of work established that the
problem can be solved with only O(d) samples [Candès and Li, 2014, Chen and Candes, 2015,
Cai et al., 2022].
Eventually, for Gaussian i.i.d. input data and i.i.d. teacher weights, the optimal sample com-
plexity for learning phase retrieval in the high-dimensional limit has been established down to
the constant in α = n/d. Authors of Mondelli and Montanari [2019] derived the weak recov-
ery threshold for the noiseless case to be α = 1/2 for phase retrieval, and optimal spectral
WR
methods were shown to match this threshold in Luo et al. [2019], Maillard et al. [2022a]. The
information-theoretically optimal accuracy and the one achieved by an approximate message
passing algorithm were then derived in Barbier et al. [2019] for a general i.i.d. prior for the
teacher weights. In the absence of noise, these results imply sample complexities α = 1 and
IT
α ≈ 1.13 needed to achieve perfect learning for a Gaussian prior. Authors of Song et al.
AMP
[2021] proposed a non-robust polynomial algorithm capable of solving noiseless phase retrieval
for α ≥ α . Algorithms based on gradient descent were argued not to achieve the optimal
IT
sample complexity in Sarao Mannelli et al. [2020a], Mignacco et al. [2021]. Maillard et al. [2020]
derivedtheMMSEformoregeneralinputdatadistributions, includingthecomplex-valuedcase.
Phase retrieval with generative priors was studied in Hand et al. [2018], Aubin et al. [2020]. We
3refer to a recent review [Dong et al., 2023] for an overview of the relations between these recent
theoretical studies and practical applications of phase retrieval in imaging.
The case with different numbers of hidden units m⋆ in the teacher and m in the student model,
was also discussed in the literature. For m∗ = O (1), the problem is a special case of a multi-
d
index model that has been recently actively considered, e.g. in Aubin et al. [2019b], Bietti et al.
[2023], Damian et al. [2024], Collins-Woodfin et al. [2023]. This line of work has not focused on
the quadratic activations, as it does not bring particular simplification in this case.
The geometry of loss landscapes of one hidden-layer networks with quadratic activations was
studied, and the absence of spurious local minima was established for m ≥ d (when the read-
out layer is fixed as in our setting) in Du and Lee [2018]. Similar results were established
in Soltanolkotabi et al. [2018], Venturi et al. [2019] for a slightly more general setting where the
readout layer is learned.
Establishing results about sample complexity required for generalization in cases where m (or
both m and m∗) are Θ(d) is technically challenging, and so far, only a handful of works made
progress in that direction. In particular, Gamarnik et al. [2019] considered m∗ ≥ d and m ≥ d,
and have shown that a sample complexity n ≥ d(d+1)/2 is sufficient for perfect recovery of
the target function. Sarao Mannelli et al. [2020b] considered the overparametrized case with
m∗ = O (1) and m > d, and showed that gradient descent reaches exact recovery for a sample
d
complexity n > d(m∗+1)−(m∗+1)m∗/2, again considering the high-dimensional limit. Gra-
dient descent of the population risk has been studied for general values of (m∗,m) in Martin
et al. [2024], along with a discussion of the role of overparametrization.
2 Setting
As discussed above, we are studying the Bayes-optimal accuracy in the teacher-student setting.
More concretely, we consider a dataset of n samples D = {y ,x }n where the input data
i i i=1
is normal Gaussian of dimension d: (x )n i. ∼i.d. N(0,I ). We then draw i.i.d. d-dimensional
i i=1 d
teacher-weight vectors (w∗)m i. ∼i.d. N(0,I ), and noise (z )n i. ∼i.d. N(0,I ). Finally, the output
k k=1 d i i=1 m
labels (y )n are obtained by a one-hidden layer teacher network with m hidden units and
i i=1
quadratic activation:
y = f (x ) :=
1 Xm (cid:20) √1
(w∗)⊤x
+√
∆z
(cid:21)2
. (2)
i W∗ i m d k i i,k
k=1
Crucially, we assume we know the form of the (stochastic) target function f (·) (i.e. the value
W∗
of m, ∆, and the form of eq. (2), including the fact that the activation function is quadratic)
but we do not know the realization of neither the teacher weights W∗ = (w⋆,··· ,w⋆ ) nor the
1 m
noise z .
i
Remark: learning the second layer weights – We assume in eq. (2) that the second layer
weights are fixed and equal to 1. One can consider a more general problem in which the second
layer weights (a⋆)m are drawn i.i.d. from a probability distribution P , and the student must
k k=1 a
learn (w⋆,a⋆)m from the observation of {x }n and of
k k k=1 i i=1
y =
1 Xm a⋆(cid:20) √1
(w∗)⊤x
+√
∆z
(cid:21)2
. (3)
i m k d k i i,k
k=1
Equivalently, we consider in what follows the case P = δ . However, all our techniques and
a 1
results can be generalized to more generic choices of P . We sketch how to perform this gener-
a
alization, and the results it yields, in Appendix E: in particular, Claim 3 is the generalization
of our main result to this setting.
4Universality over the noise and weights distribution – While we consider Gaussian
distributions for the sake of our theoretical analysis, we expect our results to hold under more
general i.i.d. models with non-Gaussian distributions on both the noise and the teacher weights,
undermildconditionsofexistenceofmoments. ThisisrelatedtoarecentconjectureofSemerjian
[2024], see Sections 3 and 4.
Bayes-optimal test error – Since we know the law of the dataset D, we can study the Bayes-
optimal (BO) estimator, which minimizes the test error over all possible estimators. To do this,
we use Bayes’ theorem to obtain the posterior distribution P(W|D) of the weights W given the
dataset:
1
P(W|D) = P (W)P(y|W,{x }n )
Z(D) prior i i=1
whereP (W)isapriordistributionontheteacherweightsW∗,andthelikelihoodP(y|W,X)
prior
can be seen as a probabilistic channel that generates the labels given the input data (x )n
i i=1
and the teacher weights W∗, and Z(D) is a normalization constant. The Bayes-optimal (BO)
estimator of the labels for a test sample x not seen in the training set D then involves
test
the average over the posterior distribution as follows (where E denotes the expectation over
z
z ,··· ,z )
1 k
Z
yˆBO(x ) := E[y |x ,D] = E [f (x )]P(W|D)dW. (4)
D test test test z W test
We will evaluate the BO estimator in terms of its average generalization error, i.e. the mean
squared error (MSE) achieved on a new sample. We define it in the following way:
m (cid:20)(cid:16) (cid:17)2(cid:21)
MMSE := E E y −yˆBO(x ) −∆(2+∆). (5)
d 2 W∗,D ytest,xtest test D test
We denote it MMSE , standing for minimum-MSE, as it is the minimum MSE achievable given
d
the setting of the model, and we call MMSE := lim MMSE .
d→∞ d
Conventions for the MMSE – Notice the peculiar multiplicative factor (m/2) and the ad-
ditive term −∆(2 + ∆) in eq. (5). As we detail in Appendix D.1, these factors ensure that
MMSE → 1 for α → 0 (i.e. in the absence of data), and MMSE → 0 if the posterior concentrates
around the true W⋆ (i.e. if yˆBO(x) = E [f (x)]). Moreover, as we also detail in Appendix D.1,
D z W⋆
eq. (5) matches the MMSE of a matrix estimation task to which we will reduce the original
problem, see Section 3.1.
As motivated above, our goal is to analyze the MMSE in the high-dimensional limit, with an
extensive-width architecture and quadratically many data samples:
n m
d → ∞, α := = Θ(1), κ := = Θ(1), (6)
d2 d
In all that follows, we only consider the limit of eq. (6) (except when explicitly mentioned), so
that n,d,m all go to infinity together when we write e.g. lim . As we will see, in this limit,
d→∞
the value of the MMSE for a given realization of the randomness concentrates on the averaged
value defined in eq. (5).
Empirical risk minimization estimator – A more standard way of learning the target
function (2) is to minimize the empirical loss L corresponding to a “student” neural network
L(W) =
1 Xn (cid:16)
y −f˜ (x
)(cid:17)2
, where f˜ (x) :=
1 Xm (cid:20) √1
(w
)⊤x(cid:21)2
. (7)
i W i W k
n m d
i=1 k=1
Note that this does not account for the noise, but activations in neural networks are commonly
considered deterministic, so we consider this the most natural choice.
52.0
1.00
κ=0.05
κ=0.1
1.5
0.75 κ=0.2
κ=0.3
0.50 κ=0.5 1.0
κ=1.0
κ=5.0
0.25 0.5
0.00 0.0
0.0 0.2 0.4 0.6 0.0 0.2 0.4 0.6
Sample complexity α = n/d2 Sample complexity α = n/d2
Figure 1: Left: The asymptotic MMSE of eq. (8) for the noiseless (∆ = 0) case, as a function
of the sample complexity α, for various width ratios κ. Right: Phase diagram representing the
MMSE, brighter color indicates a higher value. The red curve is the perfect recovery transition
line α , see eq. (1), and its origin is discussed in Section 5.
PR
Minimization of the loss over the weights W = (w )m is commonly done using gradient
k k=1
descent (GD): one initializes the weights as W(0) ∼ P and then updates them to minimize
prior
the empirical loss, for an appropriately choice of learning rate, until convergence. Denoting
the weights at convergence as Wˆ (W(0),D) the estimator for test labels reads yˆGD (x ) :=
W(0),D test
f˜ (x ). As we will see, it will be interesting to consider also an estimator yˆAGD
Wˆ(W(0),D) test
obtainedbyaveragingtheGDestimatoronthelabelsovertheinitializationsW(0) oftheweights.
Towards more generic models – There are several extensions of our setting that one can
consider. Importantly, our analysis focuses on quadratic activations, which allows for significant
technical simplifications in our theoretical analysis as we will detail. We sketch in the conclusion
(Section 6) the challenges that arise when tackling more general activation functions such as the
ReLU or sigmoid function.
3 Main results
Notations – We use tr(·) := (1/d)Tr[·] for the normalized trace. We denote GOE(d) the
distribution of symmetric matrices ξ ∈ Rd×d such that ξ i. ∼i.d. N(0,(1+δ )/d), for i ≤ j. For
ij ij
m = κd with κ > 0, we denote W the Wishart distribution, and µ the Marchenko-Pastur
m,d MP,κ
distribution with ratio κ. More details on classical definitions and notational conventions are
given in Appendix A.
3.1 Information-theoretic optimal estimation
We start by stating the main result of our analysis, applied to the problem of eq. (2).
Result 1. The MMSE of eq. (5) is given in the high-dimensional limit of eq. (6) by:
˜
2ακ κ∆
MMSE = − , (8)
qˆ 2
where ∆˜ := 2∆(2+∆)/κ, and where qˆis a solution of the following equation:
∆eqˆ 4π2 Z
(1−2α)+ = µ (y)3dy. (9)
2 3qˆ 1/qˆ
6
ESMM
d/m
=
κ
htdiw
evisnetnIHere, µ := µ ⊞σ √ (for t ≥ 0) is the free convolution of the Marchenko-Pastur law and
t MP,κ s.c., t
a scaled semicircular density, see Appendix A for its precise definition.
Eq.(9)canbeefficientlysolvedusinganumericalscheme,whichisdetailedinAppendixF.1. We
present the results in Fig. 1. In what follows, we detail our approach towards deriving Result 1,
which is a consequence of our main theoretical result stated in Claim 2.
Reduction to a matrix estimation problem – We first notice that by expanding the square
in eq. (2), we can effectively reduce our learning task to an estimation problem in terms of
S⋆ := (1/m)Pm w⋆(w⋆)⊤. We give an analytical argument backing this observation in Ap-
k=1 k k
pendix D.5. Its conclusion is that, at leading order, the distribution of y = f (x) can be
√ W⋆
reduced to the following form, with y := d(y−1−∆):
e
q
y = Tr[ZS⋆]+ ∆eξ, (10)
e
√
with ξ ∼ N(0,1), ∆e := 2∆(2+∆)/κ, and where we defined Z := (xx⊤−I d)/ d.
Generalization error and MMSE on S –Thisequivalentproblemgivesusawaytointerpret
the convention we chose for eq. (5). Indeed, if we denote Sˆopt = E[S|y,Z] the Bayes-optimal
e
estimatorrelatedtotheproblemofeq.(10),thenwehaveMMSE = κEtr[(S⋆−Sˆopt)2],asproven
in detail in Lemma D.1.
The limit of the MMSE – We now describe the general form of estimation problems covered
by our theoretical analysis, which encompasses the one described in eq. (10) (and thus the
original eq. (2)). The goal is to recover the symmetric matrix S⋆ ∈ Rd×d, which was generated
from the Wishart distribution W , from observations (y )n , generated as
m,d i i=1
y ∼ P (·|Tr[Z S⋆]), (11)
i out i
√
with Z := (x x⊤−I )/ d. The “channel” P accounts for possible non-linearities and noise,
i i i d out
encompassing the case of additive Gaussian noise in eq. (10). We define the partition function
as:
n
Z({y ,x }n ) := E Y P (y |Tr[SZ ]). (12)
i i i=1 S∼W m,d out i i
i=1
Notice that the averaged logarithm of Z is (up to an additive constant) equal to the mutual
information between the observations and the hidden variables: I(W⋆;{y }|{x }) = ElogZ +
i i
nElogP (y |Tr[Z S⋆]). This links Z to the optimal estimation of W, an important idea
out 1 1
behind our study. We are now ready to state our main theoretical result. It gives a sharp
characterization of the Bayes-optimal error in any estimation problem of the type of eq. (11).
By the reduction described above, it can be directly applied to the original model of eq. (2), and
will imply Result 1.
Claim 2. Assume that m = κd with κ > 0, and n = αd2 with α > 0. Let Q := 1+κ−1. Then:
0
• The limit of the averaged log-partition function (sometimes called the free entropy) is given
by
1 (cid:20) Z (cid:21)
lim E logZ = sup I(q)+α dyDξJ (y,ξ)logJ (y,ξ) , (13)
d→∞ d2 {yi,xi} q∈[1,Q0] R×R q q
where
 (cid:20)(Q −q)qˆ 1 1 1(cid:21)
I(q) := qˆin ≥f
0
0
4
− 2Σ(µ 1/qˆ)−
4
logqˆ−
8
,
( √ ) (14)
Z dz (z− 2qξ)2
J
q(y,ξ) :=
p
4π(Q −q)
exp −
4(Q −q)
P out(y|z).
0 0
7Here, Σ(µ) := E log|X−Y|, and, for t ≥ 0, µ := µ ⊞σ √ is the free convolution
X,Y∼µ t MP,κ s.c., t
of the Marchenko-Pastur distribution and a (scaled) semicircle law, see Appendix A for its
definition.
• For any α > 0, except possibly in a countable set, the supremum in eq. (13) is reached in a
unique q⋆ ∈ [1,Q ]. Moreover, the asymptotic minimum mean-squared error on the estimation
0
of S⋆, achieved by the Bayes-optimal estimator SˆBO := E[S|{y ,x }], is equal to Q −q⋆:
i i 0
lim Etr[(S⋆−SˆBO)2] = Q −q⋆. (15)
0
d→∞
It is related to the MMSE of eq. (5) by MMSE = κ(Q −q⋆).
0
The condition q ≥ 1 – Notice that q⋆ = lim E[tr(S⋆SˆBO)] according to Claim 2. As the
d→∞
MMSEdecreaseswithα,itisclearthatq⋆ ≥ q⋆(α = 0). Whenα = 0,wehaveSˆBO = E[S⋆] = I ,
d
and thus q⋆(α = 0) = 1. We check in Appendix D.8 that the value q⋆(α = 0) = 1 is recovered
by eq. (13).
Specifying Claim 2 to the problem of eq. (10), we derive (details are given in Appendix D.7)
Result 1, more precisely eqs. (8) and (9).
3.2 Polynomial-time optimal estimation with the GAMP-RIE algorithm
LetusrecallacrucialobservationofSection3.1: thelearningproblemofeq.(2)canbeeffectively
reduced to a generalized linear model (GLM) on the matrix S⋆ (cf. eq. (11)):
y ∼ P (·|Tr[Z S⋆]), (16)
i out i
√
with Z := (x x⊤ −I )/ d, S⋆ ∼ W , and P a noise channel (which would be Gaussian
i i i d m,d out
in eq. (10)). An important difficulty in analyzing eq. (16) is the rather complex structure of
the matrices Z (which can be viewed as “sensing vectors” applied to S⋆). Determining the
i
optimal algorithm in GLMs when the sensing vectors have arbitrary structure is in general
open. Anticipating on a universality argument for the MMSE (cf. Section 4), we “forget”
momentarily about the structure of {Z }, and assume that the optimal algorithm takes the
i
form it would have if the {Z } were instead Gaussian matrices (i.e. GOE(d)). For generalized
i
linear models with Gaussian sensing vectors, a class of generalized approximate message-passing
(GAMP) algorithms have been extensively studied, and argued to reach optimal performance in
the absence of a computational-to-statistical gap [Donoho et al., 2009, Rangan, 2011, Zdeborová
and Krzakala, 2016]. The GAMP algorithm includes a denoiser that is adjusted to the prior
informationaboutthesignalS⋆, thatisinourcaseaWishartdistribution. Combiningthesetwo
facts, we propose the GAMP-RIE algorithm in Algorithm 1. An implementation of GAMP-RIE
is accessible in the public GitHub repository associated to this work [Maillard et al., 2024].
The functions g , f and F appearing in Algorithm 1 are defined as follows. First, we let
out RIE RIE
g (y,ω,V) :=
1
R dz(z−ω)e−(z− 2Vω)2
P out(y|z)
. (17)
out V R dze−(z− 2Vω)2
P out(y|z)
In particular, for the problem of eq. (10), we have
y−ω
g (y,ω,V) = .
out
∆e +V
Thetwofunctions(f ,F )arerelatedtotheproblemofmatrix denoising, inwhichoneaims
RIE RIE √
at recovering a matrix S ∼ W from the observation of R = S + ∆ξ, with ξ ∼ GOE(d).
0 m,d 0
We recall some important results on this problem, and how they relate to the definition of the
functions (f ,F ).
RIE RIE
8Algorithm 1: GAMP-RIE
Result: The estimator Sˆ
√
Input: Observations y ∈ Rn and “sensing vectors” Z := (x x⊤−I )/ d ∈ Rd×d;
i i i d
Initialize Sˆ0 ∼ W and cˆ,ω,V randomly;
m,d
while not converging do
• Estimation of the variance and mean of Tr[Z Sˆ];
i
Vt = cˆt and ωt = Tr[Z Sˆt]−g (y ,ωt−1,Vt−1)Vt ;
i i out i i
• Variance and mean of S estimated from the “channel” observations;
At =
2α Xn
g (y ,ωt,Vt)2 and Rt = Sˆt+
1 Xn
g (y ,ωt,Vt)Z ;
n out i i dAt out i i i
i=1 i=1
• Update of the estimation of S⋆ with the “prior” information;
(cid:18) 1 (cid:19) (cid:18) 1 (cid:19)
Sˆt+1 = f Rt, and cˆt+1 = 2F ;
RIE 2At RIE 2At
t = t+1;
end
(i) The optimal estimator (in the sense of mean squared error) of S has been worked
0
out in Bun et al. [2016], and belongs to the class of rotationally-invariant estimators
(RIE). f (R,∆) is this optimal estimator, and it admits the following explicit form.
RIE
If R = UΛU⊤ is the spectral decomposition of R, and letting ρ := µ ⊞ σ √
∆ MP,κ s.c., ∆
be its asymptotic eigenvalue distribution (see Appendix A for the definition of the free
convolution µ ⊞ ν and its relation to the sum of asymptotically free matrices), then
f (R,∆) = Uf (Λ)U⊤, where f (λ) = λ−2∆h (λ), with h the Hilbert transform
RIE ∆ ∆ ∆ ∆
of ρ . More precisely:
∆
Z 1
h (λ) := P.V. ρ (t)dt.
∆ ∆
λ−t
ρ and h can be evaluated numerically very efficiently, see Appendix A for details.
∆ ∆
(ii) F (∆) is defined as the asymptotic MMSE of the same matrix denoising problem. It
RIE
can be written in the two equivalent forms (see Maillard et al. [2022b], Pourkamali et al.
[2024], Semerjian [2024]):
4π2 Z Z
F (∆) = ∆− ∆2 dλρ (λ)3 = ∆−4∆2 dλρ (λ)h (λ)2. (18)
RIE ∆ ∆ ∆
3
In Appendix D.9 we sketch the derivation of the state evolution of Algorithm 1, assuming a
universality result discussed in Section 4 holds as well for GAMP-RIE. Concretely, we show
that one can analytically track the performance of its iterates in the high-dimensional limit, and
we draw a formal connection with the information-theoretic predictions of Claim 2. Notably,
we obtain a so-called state-evolution of the GAMP-RIE algorithm (which turns out to follow
from rigorous work on non-separable estimation with GAMP [Berthier et al., 2020, Gerbelot
and Berthier, 2023]), and show that its fixed points agree with the fixed point equations that
provide the Bayes-optimal error. In all regions of parameters that we investigated below we
observed a unique fixed point, meaning that the GAMP-RIE algorithm asymptotically reaches
the Bayes-optimal performance.
4 Derivation of the main results
Wederiveourmainresult(Claim2)intwoways. First,weshowhowonecanshowClaim2using
the replica method, a heuristic but exact method (hence the word “claim”) which originated in
9statistical physics [Mézard et al., 1987], and has been used extensively in theoretical physics, as
well as in a growing body of work in high-dimensional statistics, theoretical computer science,
and theoretical machine learning [Mezard and Montanari, 2009, Zdeborová and Krzakala, 2016,
Gabrié, 2020, Charbonneau et al., 2023]. The derivation, that has an interest on its own, is
performed in detail in Appendix B and leverages recent progress on the problems of ellipsoid
fitting [Maillard and Kunisky, 2024, Maillard and Bandeira, 2023] and extensive-rank matrix
denoising [Maillard et al., 2022b, Pourkamali et al., 2024, Semerjian, 2024].
Despite the replica method being conjectured to yield exact results in a large class of high-
dimensional models, a rigorous treatment of it remains elusive. It is important, we feel, to
present as well a more mathematically sound derivation of our claims, and we thus give an
alternative derivation of the Claim 2 using probabilistic techniques amenable to rigorous treat-
ment. In what follows, we present a three-step sketch of a mathematical proof of Claim 2 that
combines recent progress performed on the study of a problem known as the ellipsoid fitting
conjecture [Maillard and Kunisky, 2024, Maillard and Bandeira, 2023] with the analysis of the
fundamental limits of so-called generalized linear models [Barbier et al., 2019], as well as matrix
denoising problems [Bun et al., 2016, Maillard et al., 2022b, Pourkamali et al., 2024, Semer-
jian, 2024]. While a complete mathematical treatment requires more work, we detail the main
challenges arising in each of these steps, outlining a fully rigorous establishment of Claim 2.
We denote the free entropy Φ := (1/d2)ElogZ({y ,x }), cf. eq. (12). We detail three pre-
d i i
cise results (two conjectures and a theorem), motivated by recent mathematical works, whose
combination would rigorously establish the results of Claim 2. Recall that we consider the
high-dimensional limit of eq. (6).
Step 1: Universality with a “Gaussian equivalent” problem – The first step of our
approach is inspired by recent literature on the ellipsoid fitting problem [Maillard and Kunisky,
√
2024, Maillard and Bandeira, 2023]. It amounts to notice that, if Z := (x xT − I )/ d, by
i i i d
the central limit theorem, for any symmetric matrix S, Tr[Z S] is (under mild boundedness
i
conditions on the spectrum of S) approximately distributed as N(0,2tr[S2]) as d → ∞. A
large body of recent literature has established that the free entropy is universal for all data
distributions sharing the same asymptotic distribution of their “one-dimensional projections”,
seee.g.HuandLu[2022],MontanariandSaeed[2022],Dandietal.[2024],MaillardandBandeira
[2023]. This motivates the conjecture that the free entropy should remain identical (to leading
order) if one replaces the matrices Z with G ∼ GOE(d).
i i
Conjecture 4.1 (Universality). We define
Φ( dG) :=
d1
2E
({y
i′,Gi})logE
S∼W m,d
Yn
P out(cid:0) y i′(cid:12) (cid:12)Tr[G iS](cid:1) , (19)
i=1
where y′ ∼ P (·|Tr[G S⋆]), with S⋆ ∼ W and G i. ∼i.d. GOE(d). Then
i out i m,d i
(G)
lim |Φ −Φ | = 0.
d d
d→∞
Conjecture 4.1 can be seen as an extension of Corollary 4.10 of Maillard and Bandeira [2023], in
the context of a teacher-student model. In particular, we expect it to hold under mild regularity
conditions on the channel density P (which are satisfied by the Gaussian additive noise we
out
consider).
Step 2: A matrix generalized linear model with a Wishart prior – By the first step
(G)
above, we can focus on Φ , and the corresponding estimation problem. A key observation is
d
thatonecanviewthisproblemasaninstanceofageneralizedlinearmodel onS⋆,withaGaussian
10data matrix whose i-th row is the flattening of the matrix G . The limiting free entropy of such
i
models has been worked out in Barbier et al. [2019], when the “ground-truth vector” (here S⋆)
has i.i.d. elements. However, here the prior is far from being i.i.d. since S⋆ ∼ W . The results
m,d
of Barbier et al. [2019] generalize naturally to other priors, but such extensions have only been
rigorously analyzed in specific settings, e.g. for generative priors rather than i.i.d. [Aubin et al.,
2019a,2020]. Inoursetting,thestructureoftheWishartpriorraisesseveraltechnicaldifficulties
preventing to directly transpose the proof approaches of Barbier et al. [2019], so we state the
following result as a conjecture.
Conjecture 4.2 (The free entropy of a matrix generalized linear model). We have
(cid:20)(Q −q)qˆ Z (cid:21)
(G) 0
lim Φ = sup inf +Ψ(qˆ)+α dyDξJ (y,ξ)logJ (y,ξ) ,
d→∞ d q∈[1,Q0]qˆ≥0 4 R×R q q
where
Ψ(qˆ) := 1 + lim 1 E logE exp(cid:18) −d Tr[(Y−p qˆS)2](cid:19) (20)
4 d→∞ d2 Y S∼W m,d 4
√
is the asymptotic free entropy of the matrixdenoising problem Y = qˆS⋆+ξ, with ξ ∼ GOE(d),
and S⋆ ∼ W , and we assume that the d → ∞ limit in eq. (20) is well-defined.
m,d
Step 3: Extensive-rank matrix denoising – Asalaststep, westudythefunctionΨ(qˆ)de-
finedineq.(20). Theoptimalestimatorsandlimitingfreeentropyinmatrixdenoisinghavebeen
workedoutinBunetal.[2016],Maillardetal.[2022b],andformallyproven(undersomeassump-
tions)inPourkamalietal.[2024],Semerjian[2024]. Weprovideaveryshortandassumption-free
proof of the following result in Appendix D.2.
Theorem 4.3 (Free entropy of matrix denoising). For any qˆ ≥ 0, the limit in eq. (20) is
well-defined, and moreover (recall the definition of Σ(µ) and µ in Claim 2)
t
1 1 1
Ψ(qˆ) = − Σ(µ )− logqˆ− . (21)
2 1/qˆ 4 8
OursimpleproofcombinesarelationbetweenΨ(qˆ)andHCIZintegralsofrandommatrixtheory,
proven in Pourkamali et al. [2024] (without any assumptions), and fundamental results on the
largedeviationsoftheDysonBrownianmotion[GuionnetandZeitouni,2002]: wegivedetailsin
Appendix D.2. As a final remark, we notice that a recent conjecture2 of Semerjian [2024] states
that the free entropy of matrix denoising of S⋆ = (1/m)Pm w⋆(w⋆)⊤ remains the same if one
k=1 k k
considers any i.i.d. prior for w⋆, under the matching of its first two moments with the Gaussian
k
and the existence of all other moments. While the validity of this conjecture is subject to debate
(see Section VII of Semerjian [2024], and the findings of Camilli and Mézard [2023, 2024]), in
the present model it would imply universality of the generalization error given by Claim 2 for
any such teacher weight distribution.
The second part of Claim 2 – WebrieflydiscussthesecondpartofClaim2, concerningthe
large d limit of Etr[(S⋆−SˆBO)2]. The fact that the maximizer of eq. (13) is unique for almost all
values of α can be seen by simple convexity arguments, see Appendix D.6. The relationship of
q⋆ with the asymptotic MMSE on the estimation of S⋆ is a classical consequence of the I-MMSE
theorem in generalized linear models of which eq. (10) is an instance, see e.g. Barbier et al.
[2019] and Section D.5 of Maillard et al. [2020].
2Wementionherethe“strong”conjectureofSemerjian[2024]. Aweakerformofthisconjectureistheuniversality
of the best low-degree polynomial estimator for any i.i.d. prior.
115 Discussion of the main results
5.1 Analysis of the Bayes-optimal estimator
The noiseless case and the perfect recovery transition – We start by discussing the
noiseless case (∆ = 0), which is described by the phase diagram in Fig. 1. Since there is no
noise in the target function, we expect a sharp transition to zero MMSE at a critical sample
complexity α . We analytically show in Appendix D.3 from eq. (9) that α is given by the
PR PR
expression of eq. (1), and discuss how it is related to a naive counting argument of the “degrees
of freedom” of the target function. This transition was known for κ ≥ 1 where the problem is
convex, where Gamarnik et al. [2019] shows that there is perfect recovery as soon as α > 1/2.
For all values of κ we see the MMSE is a smooth curve going continuously from 1 at α = 0 to 0
at α . We derived the slope of the curve at α to be (see Appendix D.4)
PR PR
 4 12
∂MMSE(cid:12)
−2−
κ
+
1+κ
if κ ≤ 1,
(cid:12) =
∂α (cid:12) αPR −2+ 2 if κ ≥ 1.
κ
It is interesting to observe that the convexity of the curve changes. While we are observing
concave dependence on α for small κ it becomes convex when κ increases and α is close to α .
PR
We also note that the smooth limit MMSE → 1 as α → 0 supports the result of Cui et al. [2023]
about a quadratic number of samples being needed to learn better than linear regression.
Noisy setting – We also evaluated the MMSE in the presence of noise, where we observed it
to decrease smoothly as α increases with no particular phase transition. We show an example
of the theoretical prediction for the MMSE in this case in Fig. 3 right. As expected, in the
presence of noise, it decreases monotonically and smoothly, and goes to zero as α→∞.
The small κ limit – We consider here the limit κ → 0, i.e. the limit of small (but still
extensively large) hidden layer, and compute the limit of the MMSE curves shown in Fig. 1.
Since in the noiseless setting we have α = κ+O(κ2) (cf. eq. (1)), we will work in the rescaled
PR
regime α = ακ, with α remaining finite as κ ↓ 0. By analyzing eq. (9) in this regime (details are
e e
given in Appendix C.1), we reach that the MMSE satisfies, as κ → 0:
 1+∆(2+∆)
1 if α
e
≤
2
,
MMSE = (22)
(cid:20) q (cid:21) 1+∆(2+∆)
−∆(2+∆)+2α
e
1−α e+ (1−α e)2+∆(2+∆) if α
e
≥ .
2
In particular, in the noiseless case (∆ = 0), we have:
 1
1 if α
e
≤ 2,
MMSE = (23)
1
4α e(1−α e) if α
e
≥ ,
2
and we reach perfect recovery for α = 1. This limit is illustrated in Fig. 2 (left).
e
The small κ limit from a large but finite hidden layer – Remarkably, eq. (23) can be
computed as well by taking the limit m → ∞ when assuming that m = O(1) as d → ∞, a
setting which was studied extensively in the literature (see Aubin et al. [2019b] and references
therein). We detail this computation in Appendix C.2.
The large κ limit – Conversely, in the limit κ → ∞, we can expand eq. (9) as well, and we
detail this derivation in Appendix C.3. In the noiseless case, we reach that, for any fixed α > 0,
MMSE → max(1−2α,0), coherently with the behavior shown in Fig. 1.
κ→∞
121.00
κ 0 Theory (Noiseless)
→
κ=0.01
Theory (∆=0.252)
κ=0.05
0.75
κ=0.1 AMP (d=100)
κ=0.2 AMP (d=200)
κ=0.5
0.50
κ=1.0
κ=5.0
0.25
0.00
0.0 0.5 1.0 0.0 0.1 0.2 0.3 0.4 0.5
α/κ = n/dm α = n/d2
Figure 2: Left: Behavior of the asymptotic MMSE in the noiseless (∆ = 0) case as κ gets
increasingly small. The continuous lines are given by eq. (8), which we compare with the
asymptotic κ → 0 curve obtained by eq. (23). We emphasize that the horizontal axis is α/κ,
which remains of order Θ(1) as κ → 0: it corresponds to a number of samples n of the same
order as the number of parameters dm. Right: Comparison of the performance of GAMP-RIE
√
with the asymptotic MMSE (8) both in the noiseless (∆ = 0) and in a noisy ( ∆ = 0.25) case,
with κ = 0.5. Each dot is the average over 8 runs of GAMP-RIE at a moderate size of either
d = 100 (circle dots) or d = 200 (crosses). The error bars are the standard deviations of the
MSE.
5.2 GAMP-RIE algorithm reaching the optimal error
In Fig. 2 (right) we compare the asymptotic theoretical result for the Bayes-optimal error with
the performance of the GAMP-RIE algorithm for d = 100 and d = 200, in both the noiseless
(blue) and noisy (red) cases. We observe that even for such moderate sizes the agreement
between the algorithmic performance and the theory is excellent.
We also stress here that in all the cases we evaluated, the state evolution of the GAMP-RIE
convergestothefixedpointthatcorrespondstotheBayes-optimalperformance. Thismeansthat
the Bayes-optimal error discussed above is reachable efficiently with the GAMP-RIE algorithm.
In particular, unlike in the canonical phase retrieval problems (i.e. when m = 1) [Barbier et al.,
2019], we did not identify a computational-to-statistical gap when learning this extensive-width
quadratic-activation neural network.
5.3 Comparison to the ERM estimator obtained by gradient descent
The results discussed so far concern the Bayes-optimal MMSE, which requires evaluating the
marginals of the posterior distribution. We now investigate numerically the performance of
empirical risk minimization via gradient descent, which is the standard method of machine
learning. It would be typical to expect a gradient based approach to be suboptimal, as the
problem is non-convex for κ < 1. In Fig. 3, we compare (a) the MSE κtr[(S⋆−Sˆ )2] reached
GD
by gradient descent (GD) minimizing the loss (7) from random initialization, (b) the MSE
reached by GD averaged over initializations, and (c) the MMSE derived from the theory. All
these experiments are accessible in
In the noiseless case, ∆ = 0, we very remarkably observe that the MSE reached by gradient
descent is very close to exactly twice larger than the asymptotic MMSE. Such a relation is
known in high-dimensional generalized linear regression to hold between the Gibbs estimator,
where test error is evaluated for weights that are sampled uniformly from the posterior, and the
13
ESMMTheory Theory(∆=0)
2.0 Theory(∆=0.252)
GD (d=200) GD(∆=0)
AveragedGD(∆=0)
AGD (d=200)
1.5
GD(∆=0.252)
GD (d=100) AveragedGD(∆=0.252)
AGD (d=100)
1.0
0.5
0.0
0.0 0.1 0.2 0.3 0.4 0.0 0.1 0.2 0.3 0.4
Sample complexity α = n/d2 Sample complexity α = n/d2
Figure 3: Mean squared error (MSE) as a function of the sample complexity α for κ=1/2. Dots
are simulations using GD with a single initialization averaged over 32 realizations of the dataset,
crosses are averages over 64 initializations with 2 realizations of the dataset. The continuous
lines are the asymptotic MMSE given by (8). Left: noiseless ∆ = 0 case. The colors indicate
the size d. We can see how AGD appears to be well described by the theoretical MMSE. We
used the learning rates 0.2 for d=200 and 0.07 for d=100. Right: Comparison of GD between
√
the noisy ∆=0.25 case (red) and noiseless ∆=0 case (blue). Adding noise makes AGD worse
than the MMSE, and for sample complexity α≳0.3, all the initializations of GD converge to
the same point, making the GD and AGD curves collapse.
Bayes-optimalestimatorthataveragesovertheweightssampledfromtheposterior[Engel,2001,
Barbieretal.,2019]. Ingeneral, thereisnoreasonwhytherandomlyinitializedgradientdescent
should be able to sample the posterior measure. We nevertheless evaluate the average over the
initializationofgradientdescentandobservethat,indeed,theMSEreachedthiswayisconsistent
withtheMMSE.Thisleadsustoconjecturethatinthenoiselessone-hiddenlayerneuralnetwork
with quadratic activation and a target function matching this architecture, randomly-initialized
gradient descent samples the posterior despite the problem being non-convex, and hence its
average achieves the MMSE.
Let us offer a heuristic argument for this perhaps intriguing phenomenon. It starts with the
equivalent of the representer theorem: one can write S in the span of {x xT}n , plus a matrix
i i i=1
in the orthogonal space, that is S = Pn β x xT +Z. This means that gradient descent reaches
i=1 i i i
one solution of the minimization with one additional spurious component. The Bayes optimal
procedure would be to set this spurious reminder to zero since the data are not informative in
this direction. It is reasonable (although non-trivial) to assume that this is what is achieved by
averaging over initialization.
When comparing the MMSE to the performance of GD in the noisy setting, we observe a gap
between the MMSE and the performance of gradient descent, even averaged over initialization
or regularized (as shown in Appendix F.3, Figure 4 left). In particular, for the noisy case, we
see that for small sample complexity, the averaged GD is close to matching the MMSE, but as
the number of available samples increases, the error of the averaged and non-averaged versions
of GD coincide. This is a sign of the trivialization of the landscape, in the sense that GD
converges to the same function independently of the initialization: it can be quantified using the
variances of the function reached by GD. This is investigated further in Appendix F.3, together
with the effect of ℓ regularization. We can characterize empirically another phase transition:
2
for a sample complexity larger than α (∆), GD converges to the same function independently
T
of the initialization. In the noiseless ∆ = 0 case, this is simply the perfect recovery transition,
14
ESMand α = α (∆ = 0), while increasing the noise intensity makes the threshold lower until it
PR T
reaches a plateau, which for κ = 0.5 is at α (∆ → ∞) ≈ 0.2. We display this numerical finding
T
in Figure 4 (right) in Appendix F.3. A tight analytical study of the landscape-trivialization
threshold α (∆) as a function of the noise variance ∆ is left for future work.
T
6 Conclusion and limitations
Inthiswork, weprovideanexplicitformulaforthegeneralizationMMSEwhenlearningatarget
functionintheformofaone-hiddenlayerneuralnetworkwithquadraticactivationinthelimitof
large dimensions, extensive width and a quadratic number of samples. The techniques deployed
to obtain this result are novel and, we believe, of independent interest. There are many natural
extensions of the present works. While we presented, additionally to the replica derivation, a
mathematically sound derivation, a fully rigorous treatment, a technical and lengthy task, is
left for an extended version of this work. We analyzed the Bayes-optimal MMSE, presented
the GAMP-RIE algorithm that is able to reach it in polynomial time, and compared it to the
performance of gradient descent numerically. We leave for future work the theoretical analysis
of the properties of gradient descent that we discovered numerically. Of particular interest is the
role played by the implicit nuclear norm regularization when starting from small initialization,
as discussed for the matrix sensing problem e.g. in Gunasekar et al. [2017], Li et al. [2020],
Stöger and Soltanolkotabi [2021]. Finally, we also presented the natural extension of our results
and techniques to the case of a learnable second layer.
The main limitations of our setting are its restriction to Gaussian input data, random i.i.d.
weights of the target/teacher neural network, quadratic activation, and a single hidden layer.
Going beyond any of these limitations would be a compelling direction of research, in particular
for more generic activation and multiple layers, and we hope our work will spark interest in
these directions.
Generic activations – We end our work by a brief discussion on such an extension, namely
the case of a more generic activation function. While our derivation (cf. Section 4) heavily relies
on the non-linearity being quadratic, a first natural extension would be to consider polynomial
activations, with an output generated as (assuming a noiseless setting):
y =
1 Xm (w √k⋆)⊤x i!p
,
i
m d
k=1
for some integer p ≥ 3. One could also “linearize” this model, by writing it as y = ⟨T⋆,X ⟩, in
i i
which T⋆,X are now p-tensors, defined as
i

T⋆ :=
m1 Xm
(w k⋆)⊗p,
k=1
X
i
:=
dp1
/2x i⊗p.
However, two main challenges arise when carrying out the program of Section 4 in this “tensor”
model:
(i) First, determining whether the universality Conjecture 4.1 holds for these models (and if
yes, in which scaling of the number of samples n with d) is a challenging open question
that falls outside the scope of our results as well as of previous works on free entropy
universality [Hu and Lu, 2022, Montanari and Saeed, 2022, Dandi et al., 2024, Maillard
and Bandeira, 2023].
15(ii) Secondly, the generalized form of Conjecture 4.2 would involve the free entropy of a tensor
denoising problem. While a rich literature has studied the fundamental limits of denoising
low-rank tensors (see Lesieur et al. [2017], Ben Arous et al. [2019], Ros et al. [2019], Perry
et al. [2020], Gamarnik et al. [2022] and references therein), here T⋆ has rank m = O(d),
and the optimal denoising of a large-rank tensor is, as far as we know, a completely open
question.
These two challenges form the basis of an exciting but very challenging research program, which
we leave for future work. Provided such a program could be carried out for any polynomial
activation, one might then hope to analyze generic activation functions, such as the ReLU or
sigmoid, e.g. by decomposition over a basis of orthogonal polynomials (such as the Hermite
basis), see Ben Arous et al. [2021], Abbe et al. [2023] for examples of such analyses in the case
m = O(1).
Acknowledgements
WewanttothankGiulioBiroli,FrancisBach,GuilhemSemerjian,PierfrancescoUrbani,Vittorio
Erba, Jason Lee and Afonso Bandeira for insightful discussions about this work. This work
was supported by the Swiss National Science Foundation under grants SNSF SMArtNet (grant
number 212049) and SNSF OperaGOST (grant number 200390).
References
Emmanuel Abbe, Enric Boix Adsera, and Theodor Misiakiewicz. Sgd learning on neural net-
works: leapcomplexityandsaddle-to-saddledynamics.InTheThirtySixthAnnualConference
on Learning Theory, pages 2552–2623. PMLR, 2023.
Greg W Anderson, Alice Guionnet, and Ofer Zeitouni. An introduction to random matrices.
Cambridge university press, 2010.
Benjamin Aubin, Bruno Loureiro, Antoine Maillard, Florent Krzakala, and Lenka Zdeborová.
The spiked matrix model with generative priors. Advances in Neural Information Processing
Systems, 32, 2019a.
Benjamin Aubin, Antoine Maillard, Jean Barbier, Florent Krzakala, Nicolas Macris, and Lenka
Zdeborová. Thecommitteemachine: computationaltostatisticalgapsinlearningatwo-layers
neural network. Journal of Statistical Mechanics: Theory and Experiment, 2019(12):124023,
jan 2019b.
Benjamin Aubin, Bruno Loureiro, Antoine Baker, Florent Krzakala, and Lenka Zdeborová.
Exact asymptotics for phase retrieval and compressed sensing with random generative priors.
In Mathematical and Scientific Machine Learning, pages 55–73. PMLR, 2020.
Jean Barbier, Mohamad Dia, Nicolas Macris, and Florent Krzakala. The mutual information
in random linear estimation. In 2016 54th Annual Allerton Conference on Communication,
Control, and Computing (Allerton), pages 625–632. IEEE, 2016.
Jean Barbier, Florent Krzakala, Nicolas Macris, Léo Miolane, and Lenka Zdeborová. Optimal
errors and phase transitions in high-dimensional generalized linear models. Proceedings of the
National Academy of Sciences, 116(12):5451–5460, 2019.
Gérard Ben Arous, Song Mei, Andrea Montanari, and Mihai Nica. The landscape of the spiked
tensor model. Communications on Pure and Applied Mathematics, 72(11):2282–2330, 2019.
16Gérard Ben Arous, Reza Gheissari, and Aukosh Jagannath. Online stochastic gradient descent
onnon-convexlossesfromhigh-dimensionalinference. Journal of Machine Learning Research,
22(106):1–51, 2021.
Florent Benaych-Georges and Raj Rao Nadakuditi. The eigenvalues and eigenvectors of finite,
low rank perturbations of large random matrices. Advances in Mathematics, 227(1):494–521,
2011.
Raphael Berthier, Andrea Montanari, and Phan-Minh Nguyen. State evolution for approximate
message passing with non-separable functions. Information and Inference: A Journal of the
IMA, 9(1):33–79, 2020.
AlbertoBietti,JoanBruna,andLoucasPillaud-Vivien. Onlearninggaussianmulti-indexmodels
with gradient flow. arXiv preprint arXiv:2310.19793, 2023.
Joël Bun, Romain Allez, Jean-Philippe Bouchaud, and Marc Potters. Rotational invariant
estimator for general noisy matrices. IEEE Transactions on Information Theory, 62(12):
7475–7490, 2016.
Jian-FengCai, MengHuang, DongLi, andYangWang. Solvingphaseretrievalwithrandomini-
tial guess is nearly as good as by spectral initialization. Applied and Computational Harmonic
Analysis, 58:60–84, 2022.
FrancescoCamilliandMarcMézard. Matrixfactorizationwithneuralnetworks. Physical Review
E, 107(6):064308, 2023.
FrancescoCamilliandMarcMézard. Thedecimationschemeforsymmetricmatrixfactorization.
Journal of Physics A: Mathematical and Theoretical, 57(8):085002, 2024.
Emmanuel J Candès and Xiaodong Li. Solving quadratic equations via phaselift when there
are about as many equations as unknowns. Foundations of Computational Mathematics, 14:
1017–1026, 2014.
Emmanuel J Candes, Thomas Strohmer, and Vladislav Voroninski. Phaselift: Exact and stable
signal recovery from magnitude measurements via convex programming. Communications on
Pure and Applied Mathematics, 66(8):1241–1274, 2013.
Patrick Charbonneau, Enzo Marinari, Giorgio Parisi, Federico Ricci-tersenghi, Gabriele Sicuro,
FrancescoZamponi,andMarcMezard. SpinGlassTheoryandFarBeyond: ReplicaSymmetry
Breaking after 40 Years. World Scientific, 2023.
Yuxin Chen and Emmanuel Candes. Solving random quadratic systems of equations is nearly as
easy as solving linear systems. Advances in Neural Information Processing Systems, 28, 2015.
Yuxin Chen, Yuejie Chi, Jianqing Fan, and Cong Ma. Gradient descent with random initial-
ization: Fast global convergence for nonconvex phase retrieval. Mathematical Programming,
176:5–37, 2019.
Elizabeth Collins-Woodfin, Courtney Paquette, Elliot Paquette, and Inbar Seroussi. Hitting the
high-dimensional notes: An ode for sgd learning dynamics on glms and multi-index models.
arXiv preprint arXiv:2308.08977, 2023.
Hugo Cui, Florent Krzakala, and Lenka Zdeborova. Bayes-optimal learning of deep random
networks of extensive-width. In Proceedings of the 40th International Conference on Machine
Learning. PMLR, 2023. URL https://proceedings.mlr.press/v202/cui23b.html.
17George Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of
control, signals and systems, 2(4):303–314, 1989.
Alex Damian, Loucas Pillaud-Vivien, Jason D Lee, and Joan Bruna. The computational com-
plexity of learning gaussian single-index models. arXiv preprint arXiv:2403.05529, 2024.
YatinDandi,LudovicStephan,FlorentKrzakala,BrunoLoureiro,andLenkaZdeborová.Univer-
salitylawsforgaussianmixturesingeneralizedlinearmodels. Advances in Neural Information
Processing Systems, 36, 2024.
Laurent Demanet and Paul Hand. Stable optimizationless recovery from phaseless linear mea-
surements. Journal of Fourier Analysis and Applications, 20:199–221, 2014.
Jonathan Dong, Lorenzo Valzania, Antoine Maillard, Thanh-an Pham, Sylvain Gigan, and
Michael Unser. Phase retrieval: From computational imaging to machine learning: A tu-
torial. IEEE Signal Processing Magazine, 40(1):45–57, 2023.
David L Donoho. High-dimensional data analysis: The curses and blessings of dimensionality.
AMS math challenges lecture, 1(2000):32, 2000.
David L Donoho, Arian Maleki, and Andrea Montanari. Message-passing algorithms for com-
pressedsensing. Proceedings of the National Academy of Sciences, 106(45):18914–18919, 2009.
Simon Du and Jason Lee. On the power of over-parametrization in neural networks with
quadratic activation. In International conference on machine learning, pages 1329–1338.
PMLR, 2018.
Andreas Engel. Statistical mechanics of learning. Cambridge University Press, 2001.
Marylou Gabrié. Mean-field inference methods for neural networks. Journal of Physics A:
Mathematical and Theoretical, 53(22):223002, 2020.
DavidGamarnik, ErenCKızıldağ, andIliasZadik. Stationarypointsofshallowneuralnetworks
with quadratic activation function. arXiv preprint arXiv:1912.01599, 2019.
David Gamarnik, Cristopher Moore, and Lenka Zdeborová. Disordered systems insights on
computational hardness. Journal of Statistical Mechanics: Theory and Experiment, 2022(11):
114015, 2022.
ElizabethGardnerandBernardDerrida. Threeunfinishedworksontheoptimalstoragecapacity
of networks. Journal of Physics A: Mathematical and General, 22(12):1983, 1989.
Cédric Gerbelot and Raphaël Berthier. Graph-based approximate message passing iterations.
Information and Inference: A Journal of the IMA, 12(4):2562–2628, 2023.
Alice Guionnet and Ofer Zeitouni. Large deviations asymptotics for spherical integrals. Journal
of functional analysis, 188(2):461–515, 2002.
Suriya Gunasekar, Blake E Woodworth, Srinadh Bhojanapalli, Behnam Neyshabur, and Nati
Srebro. Implicit regularization in matrix factorization. Advances in neural information pro-
cessing systems, 30, 2017.
Dongning Guo, Shlomo Shamai, and Sergio Verdú. Mutual information and minimum mean-
square error in gaussian channels. IEEE transactions on information theory, 51(4):1261–1282,
2005.
Géza Györgyi. First-order transition to perfect generalization in a neural network with binary
synapses. Physical Review A, 41(12):7097, 1990.
18Paul Hand, Oscar Leong, and Vlad Voroninski. Phase retrieval under a generative prior. Ad-
vances in Neural Information Processing Systems, 31, 2018.
Harish-Chandra. Differential operators on a semisimple lie algebra. American Journal of Math-
ematics, pages 87–120, 1957.
Uwe Helmke and John B Moore. Optimization and dynamical systems. Springer Science &
Business Media, 2012.
Hong Hu and Yue M Lu. Universality laws for high-dimensional learning with random features.
IEEE Transactions on Information Theory, 69(3):1932–1964, 2022.
ClaudeItzyksonandJ-BZuber. Theplanarapproximation.ii. Journal of Mathematical Physics,
21(3):411–421, 1980.
Adel Javanmard and Andrea Montanari. State evolution for general approximate message pass-
ing algorithms, with applications to spatial coupling. Information and Inference: A Journal
of the IMA, 2(2):115–144, 2013.
Dmitriy Kunisky, Alexander S Wein, and Afonso S Bandeira. Notes on computational hardness
of hypothesis testing: Predictions using the low-degree likelihood ratio. In ISAAC Congress
(International Society for Analysis, its Applications and Computation), pages 1–50. Springer,
2019.
Lucien Le Cam. Locally asymptotically normal families of distributions. certain approximations
to families of distributions and their use in the theory of estimation and testing hypotheses.
Univ. California Publ. Statist., 3:37, 1960.
Ji Oon Lee and Kevin Schnelli. Tracy-widom distribution for the largest eigenvalue of real
sample covariance matrices with general population. The Annals of Applied Probability, pages
3786–3839, 2016.
Thibault Lesieur, Léo Miolane, Marc Lelarge, Florent Krzakala, and Lenka Zdeborová. Statis-
tical and computational phase transitions in spiked tensor estimation. In 2017 IEEE Inter-
national Symposium on Information Theory (ISIT), pages 511–515. IEEE, 2017.
Zhiyuan Li, Yuping Luo, and Kaifeng Lyu. Towards resolving the implicit bias of gradient
descent for matrix factorization: Greedy low-rank learning. In International Conference on
Learning Representations, 2020.
Wangyu Luo, Wael Alghamdi, and Yue M Lu. Optimal spectral initialization for signal recovery
with applications to phase retrieval. IEEE Transactions on Signal Processing, 67(9):2347–
2356, 2019.
Antoine Maillard and Afonso S Bandeira. Exact threshold for approximate ellipsoid fitting of
random points. arXiv preprint arXiv:2310.05787, 2023.
Antoine Maillard and Dmitriy Kunisky. Fitting an ellipsoid to random points: predictions using
the replica method. IEEE Transactions on Information Theory, 2024.
Antoine Maillard, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborová. Phase retrieval
in high dimensions: Statistical and computational phase transitions. Advances in Neural
Information Processing Systems, 33:11071–11082, 2020.
Antoine Maillard, Florent Krzakala, Yue M Lu, and Lenka Zdeborová. Construction of optimal
spectral methods in phase retrieval. In Mathematical and Scientific Machine Learning, pages
693–720. PMLR, 2022a.
19AntoineMaillard,FlorentKrzakala,MarcMézard,andLenkaZdeborová. Perturbativeconstruc-
tion of mean-field equations in extensive-rank matrix factorization and denoising. Journal of
Statistical Mechanics: Theory and Experiment, 2022(8):083301, 2022b.
AntoineMaillard,AfonsoSBandeira,DavidBelius,IvanDokmanić,andShutaNakajima. Injec-
tivityofrelunetworks: perspectivesfromstatisticalphysics. arXiv preprint arXiv:2302.14112,
2023.
Antoine Maillard, Emanuele Troiani, Simon Martin, Florent Krzakala, and Zdeborová
Lenka. Numerical code used for experimental results. https://github.com/SPOC-group/
ExtensiveWidthQuadraticSamples, 2024.
Vladimir Alexandrovich Marchenko and Leonid Andreevich Pastur. Distribution of eigenvalues
for some sets of random matrices. Matematicheskii Sbornik, 114(4):507–536, 1967.
Simon Martin, Francis Bach, and Giulio Biroli. On the impact of overparameterization on the
training of a shallow neural network in high dimensions. In International Conference on
Artificial Intelligence and Statistics, pages 3655–3663. PMLR, 2024.
MarcMezardandAndreaMontanari. Information, physics, and computation. OxfordUniversity
Press, 2009.
Marc Mézard, Giorgio Parisi, and Miguel Angel Virasoro. Spin glass theory and beyond: An
Introduction to the Replica Method and Its Applications,volume9. WorldScientificPublishing
Company, 1987.
Francesca Mignacco, Pierfrancesco Urbani, and Lenka Zdeborová. Stochasticity helps to nav-
igate rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval
problem. Machine Learning: Science and Technology, 2(3):035029, 2021.
Marco Mondelli and Andrea Montanari. Fundamental limits of weak recovery with applications
to phase retrieval. Foundations of Computational Mathematics, 19(3):703–773, Jun 2019.
AndreaMontanariandBasilNSaeed. Universalityofempiricalriskminimization. InConference
on Learning Theory, pages 4310–4312. PMLR, 2022.
Andrea Montanari and Subhabrata Sen. A friendly tutorial on mean-field spin glass techniques
for non-physicists. Foundations and Trends® in Machine Learning, 17(1):1–173, 2024.
Opper and Haussler. Generalization performance of bayes optimal classification algorithm for
learning a perceptron. Physical review letters, 66 20:2677–2680, 1991.
Amelia Perry, Alexander S Wein, and Afonso S Bandeira. Statistical limits of spiked tensor
models. In Annales de l’Institut Henri Poincaré-Probabilités et Statistiques, volume 56, pages
230–264, 2020.
FarzadPourkamali,JeanBarbier,andNicolasMacris. Matrixinferenceingrowingrankregimes.
IEEE Transactions on Information Theory, 2024.
Sundeep Rangan. Generalized approximate message passing for estimation with random linear
mixing. In 2011 IEEE International Symposium on Information Theory Proceedings, pages
2168–2172. IEEE, 2011.
Valentina Ros, Gérard Ben Arous, Giulio Biroli, and Chiara Cammarota. Complex energy
landscapes in spiked-tensor and simple glassy models: Ruggedness, arrangements of local
minima, and phase transitions. Physical Review X, 9(1):011003, 2019.
20Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent Krzakala, Pierfrancesco Ur-
bani, and Lenka Zdeborová. Complex dynamics in simple neural networks: Understanding
gradient flow in phase retrieval. Advances in Neural Information Processing Systems, 33:
3265–3274, 2020a.
Stefano Sarao Mannelli, Eric Vanden-Eijnden, and Lenka Zdeborová. Optimization and gener-
alization of shallow neural networks with quadratic activation functions. Advances in Neural
Information Processing Systems, 33:13445–13455, 2020b.
Henry Schwarze. Learning a rule in a multilayer neural network. Journal of Physics A: Mathe-
matical and General, 26(21):5781, 1993.
Guilhem Semerjian. Matrix denoising: Bayes-optimal estimators via low-degree polynomials.
arXiv preprint arXiv:2402.16719, 2024.
Hyunjune Sebastian Seung, Haim Sompolinsky, and Naftali Tishby. Statistical mechanics of
learning from examples. Physical review A, 45(8):6056, 1992.
Jack W Silverstein and Sang-Il Choi. Analysis of the limiting spectral distribution of large
dimensional random matrices. Journal of Multivariate Analysis, 54(2):295–309, 1995.
Mahdi Soltanolkotabi, Adel Javanmard, and Jason D Lee. Theoretical insights into the opti-
mization landscape of over-parameterized shallow neural networks. IEEE Transactions on
Information Theory, 65(2):742–769, 2018.
Haim Sompolinsky, Naftali Tishby, and H Sebastian Seung. Learning from examples in large
neural networks. Physical Review Letters, 65(13):1683, 1990.
Min Jae Song, Ilias Zadik, and Joan Bruna. On the cryptographic hardness of learning single
periodic neurons. Advances in neural information processing systems, 34:29602–29615, 2021.
RolandSpeicher. Freeconvolutionandtherandomsumofmatrices. Publications of the Research
Institute for Mathematical Sciences, 29(5):731–744, 1993.
Dominik Stöger and Mahdi Soltanolkotabi. Small random initialization is akin to spectral learn-
ing: Optimization and generalization guarantees for overparametrized low-rank matrix recon-
struction. Advances in Neural Information Processing Systems, 34:23831–23843, 2021.
Antonia M Tulino and Sergio Verdú. Random matrix theory and wireless communications.
Foundations and Trends® in Communications and Information Theory, 1(1):1–182, 2004.
Luca Venturi, Afonso S Bandeira, and Joan Bruna. Spurious valleys in one-hidden-layer neural
network optimization landscapes. Journal of Machine Learning Research, 20(133):1–34, 2019.
Roman Vershynin. High-dimensional probability: An introduction with applications in data sci-
ence, volume 47. Cambridge university press, 2018.
Timothy LH Watkin, Albrecht Rau, and Michael Biehl. The statistical mechanics of learning a
rule. Reviews of Modern Physics, 65(2):499, 1993.
Eugene P Wigner. Characteristic vectors of bordered matrices with infinite dimensions. Annals
of Mathematics, pages 548–564, 1955.
Lenka Zdeborová and Florent Krzakala. Statistical physics of inference: Thresholds and algo-
rithms. Advances in Physics, 65(5):453–552, 2016.
21A Additional definitions and conventions
Convention – Throughout this manuscript, we use E to denote the expectation solely over
X
the random variable X. We denote M+(R) the set of real probability distributions.
1
Random matrix ensembles – For any d ≥ 1, we define two standard random matrix distri-
butions over the space of symmetric d×d real matrices:
• A matrix ξ is distributed according to the GOE(d) distribution (standing for Gaussian Or-
thogonal Ensemble) if ξ i. ∼i.d. N(0,[1+δ ]/d) for any 1 ≤ i ≤ j ≤ d.
ij ij
• For any m ≥ 1, a matrix S is distributed according to the Wishart distribution W if
m,d
S = W⊤W/m, where W ∈ Rm×d with W i. ∼i.d. N(0,1) for k ∈ [m],i ∈ [d].
ki
ForasymmetricmatrixMwitheigenvalues(λ )d ,wedenoteµ := (1/d)Pd δ itsempirical
i i=1 M i=1 λi
eigenvalue distribution (ESD). It is well known that for d → ∞ the ESD of GOE(d) and W
m,d
matrices converge to (respectively) the Wigner semicircle and the Marchenko-Pastur density.
Theorem A.1. [Wigner [1955], Marchenko and Pastur [1967]] Let m = κd for κ > 0, and let
ξ ∼ GOE(d) and S ∼ W . Then, as d → ∞, the ESDs of ξ and S almost surely converge (in
m,d
the sense of weak convergence) to the following probability distributions (respectively).
• The semicircle law, with density
√
4−x2
σ (x) = √ 1{|x| ≤ 2}. (24)
s.c.
2π
√
We denote σ √ (x) := t−1/2σ (x/ t) the scaled semicircle law with variance t.
s.c., t s.c.
• The Marchenko-Pastur law, with density
 p
κ (λ −x)(x−λ )
(1−κ)δ(x)+ +
2πx
− if κ ≤ 1,
µ (x) = (25)
MP,κ p
κ (λ +−x)(x−λ −)
if κ ≥ 1.
2πx
Here λ := (1±κ−1/2)2.
±
Transforms of probability distributions – For any real probability measure µ, we define
its Stieltjes transform g (z) := E [1/(X −z)] for z ∈ C. If C := {z ∈ C : Im(z) > 0}, then
µ µ +
g (z) ∈ C for all z ∈ C . Moreover, we have the Stieltjes-Perron inversion formula:
µ + +
Theorem A.2 (Stieltjes-Perron inversion formula). For all a < b, we have
1 Z b−δ
µ((a,b)) = limlim [g (x+iϵ)−g (x−iϵ)]dx.
µ µ
δ↓0 ϵ↓0 2iπ a+δ
In particular, if µ has a continuous density with respect to the Lebesgue measure then:
dµ 1
∀x ∈ R, = lim Img (x+iϵ).
µ
dx ϵ↓0 π
R
We often use the logarithmic potential function Σ(µ) := µ(dx)µ(dy)log|x−y|. We further
define the R-transform of µ as:
1
R (s) := g−1(−s)− . (26)
µ µ s
22We refer to Tulino and Verdú [2004] for more details on the definitions of this transform, e.g.
concerning its complete domain of definition. Informally, the R transform is well-defined in a
neighborhood of 0 for all measures which have bounded support. In particular, we have for the
semicircle and the Marchenko-Pastur distributions:
R (s) = ts,
 σ s.c.,√ t
κ (27)
R µMP,κ(s) = κ−s.
Free additive convolution – The main interest of the R-transform lies in its connection to
the (additive) free convolution of measures. Informally, we can interpret the free convolution
µ ⊞ ν of two measures µ and ν as the limiting spectral measure of A + B, where A and B
are symmetric d×d random matrices, with limiting spectral distributions µ and ν, and which
are asymptotically free. While we refer to Anderson et al. [2010], Tulino and Verdú [2004] for
mathematical discussions of asymptotic freeness, we recall that in particular if B is a GOE(d)
matrix independent of A, then A and B are asymptotically free. Crucially, the R transform is
additive under free convolution (see Theorem 2.64 in Tulino and Verdú [2004] e.g.):
R µ⊞ν(s) = R µ(s)+R ν(s). (28)
Eq. (28) allows to efficiently compute the density of µ⊞ν given the ones of µ and ν, by relating
theRtransformtotheStieltjestransform,andthenusingtheStieltjes-Perroninversiontheorem
(Theorem A.2).
B Derivation of Claim 2 from the replica method
In this section, we give a non-rigorous derivation of eq. (13) using classical methods of statistical
physics. We start from the definition of the partition function in eq. (12). We denote D the
standard Gaussian measure, and S(W) = W⊤W/m.
Z n
Z(S⋆,{x }n ) = DWY P (y |Tr[Z S(W)]).
i i=1 out i i
Rm×d
i=1
The replica method –Wemakeuseoftheheuristicreplica trick [Mézardetal.,1987]. Letting
Φ := (1/d2)ElogZ, it consists in writing that lim Φ = lim (∂/∂r)lim Φ (r), with
d d→∞ d r→0 d→∞ d
Φ (r) := (1/d2)logE[Zr]. Onethencomputesthed → ∞limitofΦ (r)forinteger r ∈ N, before
d d
extendinganalyticallytheresulttoanyr ≥ 0. Whilebeingnon-rigorous, thereplicamethodhas
achieved a great success in the study of both spin glasses and statistical learning models, and is
widely conjectured to yield exact predictions. We refer the reader to Mézard et al. [1987] for an
introductiontothereplicamethodinthecontextofthestatisticalphysicsofdisorderedsystems,
Maillard et al. [2023], Montanari and Sen [2024] for mathematically-friendly descriptions of the
method, and to Mezard and Montanari [2009], Zdeborová and Krzakala [2016], Gabrié [2020]
for some of its applications in the context of theoretical computer science, high-dimensional
statistics, and machine learning.
The replicated free entropy – We now compute the “replicated free entropy” Φ (r), for
d
r ∈ N. Thanks to Bayes-optimality, we can write it as an average over r + 1 replicas of the
system, writing S⋆ as the replica of index 0. We write Sa := S(Wa) to simplify notations. We
reach:
Φ (r) =
1 logZ Yr DWa" Z
dyE
Yr
P
(y|Tr[SaZ])#n
. (29)
d d2 Z out
a=0 a=0
23For a fixed set of matrices {Sa}r , by the central limit theorem the law of the variables za :=
a=0
Tr[SaZ] approach, as d → ∞, a correlated Gaussian distribution, with mean E[za] = 0, and
covariance E[zazb] = E [Tr[SaZ]Tr[SbZ]] = 2tr(SaSb), as is easily checked from the fact that
Z
√
Z =d (xx⊤ − I )/ d, with x ∼ N(0,I ). Since n = Θ(d2), the leading order of the term
d d
R dyE Qr P (y|Tr[SaZ]) will be the only one entering the leading order of Φ (r). This
Z a=0 out d
means that we have, denoting the overlap matrix
Q := tr(SaSb), (30)
ab
that
Φ (r) =
1 logZ Yr DWa" Z dydze−1 4 √z⊤Q−1z Yr
P
(y|za)#n
+o (1),
d d2 R×Rr+1 (4π)r+1/2 detQ out d
a=0 a=0
=
1 logZ dQZ Yr DWa" Z
dydz
e− 41z⊤Q √−1z Yr
P
(y|za)#n
d2 R×Rr+1 (4π)r+1/2 detQ out
a=0 a=0
× Y δ(d2Q −dtr(SaSb))+o (1). (31)
ab d
a≤b
Notice that the CLT-based argument above is made formal in Conjecture 4.1, and implies the
universality of Φ under the replacement of Z by Gaussian GOE matrices G . Since Q ∈
d i i
R(r+1)×(r+1) is of finite size as d → ∞, we can perform the Laplace method over Q in eq. (31),
and we reach (omitting o (1) terms as d → ∞, and recall n/d2 → α):
d
Φ (r) = sup [J(Q)+αJ (Q)], (32)
d out
Q∈S+
r+1
where S+ is the set of positive semi-definite symmetric matrices of size r+1, and:
r+1
 J(Q) := d1
2
logZ aY =r 0DWa aY ≤bδ(d2Q ab−dTr[SaSb]),
(33)
J out(Q) := logZ R×Rr+1dydz (4πe )− r+1 4 1z /⊤ 2Q √− d1 ez
tQ
Yr P out(y|za).
a=0
Notice that we can rewrite J(Q) using Lagrange multipliers Qˆ ∈ S (or equivalently using
r+1
the Fourier transform of the delta distribution, and the saddle point method on the Fourier
parameters) as:
J(Q) = inf " 1 Tr[QQˆ]+ 1 logZ Yr DWae−d 4P a,bQˆ abTr[SaSb]# . (34)
Qˆ∈Sr+1 4 d2
a=0
The replica-symmetric ansatz – An important assumption we make now is that there is
a permutation symmetry between the different replicas in eq. (32), and we assume that this
symmetryisnotbrokenbythemaximizerQ. Thisassumptionisusuallycalledreplicasymmetry,
and is known to hold in generic statistical learning problems when they are in the Bayes-optimal
setting [Zdeborová and Krzakala, 2016, Barbier et al., 2019]. Formally, we assume that the
supremum over Q in eq. (32) (and the infimum over Qˆ in eq. (34)) are reached in matrices such
that, for all a,b ∈ {0,··· ,r} with a ̸= b:
( Q = Q, Qˆ = Qˆ,
aa ab
(35)
Q = q, Qˆ = −qˆ,
ab ab
24with 0 ≤ q ≤ Q, and Qˆ,qˆ≥ 0.
The term J (Q) – Under the ansatz of eq. (35), it is a classical computation [Zdeborová and
out
Krzakala, 2016] to reach:
( " √ # )r+1
Z Z dz (z− 2qξ)2
J (Q) = log dyDξ exp − P (y|z) . (36)
out p out
R2 4π(Q−q) 4(Q−q)
The term J(Q) – Using the replica-symmetric ansatz of eq. (35) in eq. (34), we get:
J(Q) =
inf" (r+1)(QQˆ −rqqˆ)
+
1 logZ Yr DWae−d(Qˆ 4+qˆ)P aTr[(Sa)2]+d 4qˆTrh (P aSa)2i#
.
Qˆ,qˆ 4 d2
a=0
We now use the following Gaussian integration identity, for any symmetric matrix M:
E ξ∼GOE(d)h ed 2Tr[Mξ]i = ed 4Tr[M2].
This allows to reach the following expression, which is analytic in r:
(cid:20)(r+1) r(r+1)
J(Q) = inf QQˆ − qqˆ
Qˆ,qˆ 4 4
+
d1
2
logE
ξ((cid:18)Z
DWe−d(Qˆ
4+qˆ)Tr[S2]+d√
2qˆ
Tr[Sξ](cid:19)r+1)#
. (37)
Recall that here S = S(W) = W⊤W/m.
The limit r → 0 – From eqs. (32), (36) and (37), we have:
Φ d(r = 0) = Qsu ≥p 0Qˆin ∈f R(cid:20) 41 QQˆ + d1
2
logZ DWe−d 4Qˆ Tr[S2](cid:21) . (38)
This implies that Qˆ = 0 and Q = Q = lim E tr[S2] = 1+κ−1 (recall m/d → κ), and
0 d→∞ S∼W m,d
we correctly recover that Φ (r = 0) = 0. Taking now the derivative with respect to r, followed
d
by the r → 0 limit, yields:
(cid:20) qqˆ Z
lim Φ = sup inf − +α dyDξJ (y,ξ)logJ (y,ξ) (39)
d q q
d→∞ 0≤q≤Q0qˆ≥0 4 R2
1 (cid:21)
+ lim E [H (ξ)logH (ξ)] ,
d→∞ d2 ξ∼GOE(d) qˆ qˆ
√
Z
H qˆ(ξ) := DWe−d 4qˆTr[S2]+d 2qˆ Tr[Sξ], (40)
Rm×d
" √ #
Z dz (z− 2qξ)2
J (y,ξ) := exp − P (y|z). (41)
q p out
4π(Q −q) 4(Q −q)
0 0
In order to obtain from eq. (39) the prediction of eq. (13), it therefore suffices to show that, for
any qˆ≥ 0:
1 Q qˆ 1 1 1
lim E [H (ξ)logH (ξ)] = 0 − Σ(µ )− logqˆ− . (42)
d→∞ d2 ξ∼GOE(d) qˆ qˆ 4 2 1/qˆ 4 8
We focus on deriving eq. (42) in the remaining of this section. We note that we can rewrite the
left-hand side as the free entropy of the following denoising problem:
Y =
S⋆+ξ/p
qˆ, (43)
25with ξ ∼ GOE(d), S⋆ ∼ W , and which we consider in the Bayes-optimal setting. Indeed, we
m,d
can define the free entropy of this problem as
1 Z (cid:18) dqˆ (cid:19)
E log DW exp − Tr[(Y−S)2]
d2
Y,S⋆
4
qˆEtr[Y2] 1 Z (cid:18) dqˆ dqˆ (cid:19)
= − + E log DW exp − Tr[S2]+ Tr[YS] ,
4 d2 Y 4 2
1+qˆQ 1
= − 0 + E [H (ξ)logH (ξ)]. (44)
4 d2 ξ∼GOE(d) qˆ qˆ
Crucially, this auxiliary problem is again Bayes-optimal, which we will use in what follows.
Remark – Eq. (43) defines a problem known as extensive-rank matrix denoising. The limit
free entropy of this problem, as well as the analytical form of the Bayes-optimal estimator, for a
rotationally-invariant prior on S⋆ and a rotationally invariant noise ξ (which is here Gaussian)
have both been understood and worked out completely [Bun et al., 2016, Maillard et al., 2022b,
Pourkamali et al., 2024, Semerjian, 2024]. We will leverage these results (and partially re-derive
them) in what follows.
We now use a change of variable to the singular values of W, see e.g. Proposition 4.1.3 of An-
derson et al. [2010]. We reach:
Z DW exp(cid:18) −d 4qˆ Tr[S2]+ d 2qˆ Tr[YS](cid:19) = C d,mZ
Rm
Ym dλ ke−m 2 Pm k=1λ k Ym λ kd− 2m
+ k=1 k=1
×
Y
|λ
k
−λ
k′|e−d 4qˆPm k=1λ2 kZ DOexp(cid:26)dqˆ Tr[OΛO⊤Y](cid:27)
, (45)
2
k<k′ O(d)
in which S = W⊤W/m = OΛO⊤, with Λ = Diag((λ ,··· ,λ ,0,··· ,0)), and C > 0 is a
1 m d,m
constant depending only on m and d. Notice that we (slightly abusively) used the notation DO
to denote here the Haar measure over the orthogonal group O(d). The large-d limit of the last
term is given by the HCIZ integral [Harish-Chandra, 1957, Itzykson and Zuber, 1980]:
2 Z nθd ⊺ o
I (θ,R,Y) = I (θ,µ ,µ ) := lim log DOexp Tr[OSO Y] , (46)
HCIZ HCIZ S Y d→∞ d2 O(d) 2
where S and Y are d × d matrices with asymptotic eigenvalue distributions µ and µ . We
S Y
can now apply the Laplace method in eq. (45) on the eigenvalue distribution of S. As the
problem of eq. (43) is Bayes-optimal, it is known that the typical eigenvalue distribution of
S under the distribution of eq. (45) is µ = µ = µ , as a consequence of the so-called
S S⋆ MP,κ
Nishimori identity, so that µ is the maximizer of the variational problem obtained by the
MP,κ
useofLaplace’smethod, seeMaillardetal.[2022b]fordetails. Sincetheasymptoticdistribution
of Y is (by eq. (43)) µ = µ ⊞σ √ , we reach by eq. (44):
Y MP,κ s.c.,1/ qˆ
1 qˆQ 1
lim E [H (ξ)logH (ξ)] = C(κ)− 0 + I (qˆ,µ ,µ ), (47)
d→∞ d2 ξ∼GOE(d) qˆ qˆ 4 2 HCIZ MP,κ Y
where C(κ) is a function of κ = m/d. It can be easily seen that C(κ) = 0 by considering qˆ= 0.
Fortunately, extensive-rank matrix denoising with Gaussian noise is one of the very few cases
for which an easily tractable analytical form is known for the HCIZ integral. More specifically,
we know that for any t > 0 and any ν, we have with µ := ν ⊞σ √ [Maillard et al., 2022b]:
t s.c., t
1 1 1 3 1 1
− Σ(µ )+ E [X2]− I (t−1,µ ,ν)− + logt+ E [X2] = 0,
2
t
4t
µt
2
HCIZ t
8 4 4t
ν
26R
with Σ(µ) := µ(dx)µ(dy)log|x−y|. Applying this formula with t = 1/qˆwe reach:
1 1 qˆ 3 1 qˆ
I (qˆ,µ ,µ ) = − Σ(µ )+ E[tr(Y2)]− − logqˆ+ Etr[(S⋆)2],
HCIZ MP,κ Y Y
2 2 4 8 4 4
1 1 3 1
= − Σ(µ )+ (2Q qˆ+1)− − logqˆ.
Y 0
2 4 8 4
Combining it with eq. (47), we reach eq. (42) (recall that µ = µ with the notations of
Y 1/qˆ
eq. (42)).
C Large and small κ limits
C.1 Details of the small-κ limit
Recall that by Claim 2, we have MMSE = κ(Q −q⋆), with Q = 1+κ−1. Since the MMSE
0 0
remains finite as κ → 0, we consider the scaling q = q/κ, with 0 ≤ q ≤ 1. We start again from
e e
eqs. (8) and (9). We denote Λ := ∆(2+∆). Eq. (8), combined with the scaling of α, implies
that qˆ∼ κ2/t for some finite t > 0, and since MMSE = 1−q as κ → 0, we have
e
κ2 1−q+Λ
t = = e .
qˆ 2α
e
Moreover, eq. (9) at order O(κ) yields:
Λ
−2α+ = ∂ [F(t,κ)] , (48)
e κ κ=0
t
where
4π2t Z
F(t,κ) := µ (y)3dy.
3κ2 t/κ2
Notice that F(t,0) = 1 since µ ≃ σ √ for ξ → ∞, and R σ √ (y)3dy = 3/[4π2ξ]. Thus,
ξ s.c., ξ s.c., ξ
the leading order of eq. (9) as κ → 0 is consistent but not informative.
In what follows, we work out the small κ limit of F(t,κ), at first order in κ. We denote
R
ν (y) := (1/κ)µ (y/κ), so that the Stieltjes transform g (z) := ν(y)/(y−z)dy of ν satisfies
κ t/κ2 κ
the self-consistent equation (see Appendix A):
κ 1
z = − −tg. (49)
1+g g
Moreover, we notice that ν = (κ#µ )⊞σ √ , so that the support of ν remains bounded
κ MP,κ s.c., t
as κ → 0. We then proceed to expand in κ eq. (49). For any finite z ∈ C, the leading order of
the expansion is easily given by z = −1/h−th+o (1), which gives that ν → σ √ . However,
κ κ s.c., t
as mentioned above, we need to go to the next order in this expansion to compute eq. (48).
A BBP-type transition – We notice that κ#µ (x) ≃ (1−κ)δ(x)+κδ(x−1) when κ → 0.
MP,κ
More precisely, it is composed of a mass (1−κ) in 0, and the rest of the mass κ is made up
√ √ √ √
of a continuous part supported between (1− κ)2 ≃ 1−2 κ and (1+ κ)2 ≃ 1+2 κ. ν
κ
can thus be seen as the spectral density of the sum of a GOE matrix (with variance t) and
a small-rank perturbation matrix of rank m = κd, with all non-zero eigenvalues located close
to 1. We therefore expect by the so-called BBP transition phenomenon [Benaych-Georges and
Nadakuditi,2011]thatν willpossessasetofmeigenvaluesoutsidethesemicirclebulkwhenever
κ
the condition
1
1 ≥ − √ (50)
g √ (2 t)
s.c., t
27is satisfied, with g √ (z) := E [1/(X − z)] the Stieltjes transform of the semicircle.
s.c., t X∼σ s.c.,√ √t
Since one can easily show that g √ (2 t) = −t−1/2, eq. (50) is equivalent to t ≤ 1. In this
s.c., t
case, these “spiked” eigenvalues are located around the value [Benaych-Georges and Nadakuditi,
2011]
g−1 √ (−1) = R √ (1)+1 = 1+t.
s.c., t s.c., t
√
Moreover, as the width of the continuous part of κ#µ is of size O( κ), we also expect this
√ MP,κ
“spiked” part of the spectrum to have a width O( κ).
Expansion of ν – Based on the remarks of the previous paragraph, we assume the following
behavior for ν , as κ → 0. For any y ∈ R with y ̸= 1+t, we have
κ
ν (y) = σ √ (y)+κν(1)(y)+o(κ). (51)
κ s.c., t
Furthermore, we also have, for all y ∈ R, when t ≤ 1:
√ (cid:18)y−(1+t)(cid:19)
κν √ → ρ(1)(y), (52)
κ κ→0
κ
for a finite density ρ(1), with R ρ(1)(y)dy = 1. Eqs. (51) and (52) can be used to expand the
Stieltjes transform of ν as a function of ν(1),ρ(1), and then eq. (49) used to find the values of
κ
these two functions. These computations are straightforward, and yield:
 (y−2) √
ν(1)(y) =
p
1{|y| ≤ 2 t},
2π(1+t−y) 4t−y2 (53)
ρ(1)
= ρ √ .
s.c., 1−t
Noticethatthesecondequationofeq.(53)isonlyvalidfort ≤ 1,whilethefirstoneisvalidforall
t ≥ 0. OnechecksforinstancethatR ν(1)(dy) = −1{t ≤ 1},whichimpliesthatthenormalization
R
condition ν (y)dy = 1 is well satisfied for all values of t ≥ 0. Using the expansion of eq. (53),
κ
we obtain that
4π2t Z
F(t,κ) = ν (y)3dy,
κ
3
(
2−t if t ≤ 1,
= 1−κ +o(κ).
1/t if t ≥ 1
So finally eq. (48) becomes
(
Λ 2−t if t ≤ 1,
2α− =
e
t 1/t if t ≥ 1,
And recall that MMSE = 2αt−Λ, so that
e
(
t(2−t) if t ≤ 1,
MMSE =
1 if t ≥ 1,
Since t = (MMSE + Λ)/(2α), we reach that t = (1 + Λ)/(2α) if α ≤ (1 + Λ)/2, and t =
e e e
p
1−α+ (1−tα)2+Λ otherwise. This yields eq. (22).
e
28C.2 The small-κ limit from a large but finite hidden layer
We consider here the noiseless case:
y =
1 Xm " (w √k⋆)⊤x i#2
,
i
m d
k=1
with m = O(1) as n,d → ∞. We denote α = n/d = αm, and we assume that α = Θ(1) as
e e
m → ∞ (after n,d → ∞). We can write the partition function (cf. eq. (12)) as:
Z =
Z DWYn
P y
(cid:12) (cid:12) (cid:12)w √k⊤x i!
, (54)
out i(cid:12)
Rd×m (cid:12) d
i=1
with P (y|z) = δ(y−∥z∥2/m). We can make a direct use of the results of Aubin et al. [2019b]
out
to write:
1 (cid:26) 1 (cid:27)
lim ElogZ = extr − Tr[qqˆ]+I +mαI , (55)
q,qˆ P e C
d→∞ d 2
 Z Z (cid:20) 1 (cid:21)
I
P
:= RmDξ RmDw ×0e lox gp (cid:20)Z−
R2
m( Dw w0) 0⊤ eqˆ xw p0
(cid:20)
−+ 21ξ w⊤q ⊤ˆ1 qˆ/ w2w +0
ξ⊤qˆ1/2w(cid:21)(cid:21)
,
Z ∞ Z Z n o
I
C
:=
0
dy RmDξ
×Rm
loD g(cid:20)Z Z0P
ou Dt
Zy P| o( uI
tm
n− y|(q I) m1/ −2Z q0 )+ 1/2q Z1/ +2ξ
q1/2ξo(cid:21)
.
Rm
Here, q,qˆ are symmetric m × m matrices, which satisfy moreover I ⪰ q ⪰ 0 and qˆ ⪰ 0.
m
The informal notation “extrf” in eq. (55) means that one should zero-out the gradient of the
function f to compute the values of q,qˆ.
The matrix q – Importantly, the matrix q can be interpreted as the “overlap matrix” of the
model: if we denote ⟨·⟩ the average under the posterior measure in eq. (54), then we have
* +
w⊤w′
q = E k l , (56)
kl
d
where w,w′ are two independent samples under ⟨·⟩. Moreover, thanks to the Bayes-optimality
of the problem, it is known that the overlap concentrates [Zdeborová and Krzakala, 2016], in
the sense that the random variable (w⊤w′)/d concentrates on its average under E⟨·⟩ as d → ∞.
k l
The “prior integral” I can be very easily computed with Gaussian integrals, and yields:
P
1 1
I = Tr[qˆ]− logdet(I +qˆ). (57)
P m
2 2
We now focus on computing the leading order of I in the large-m limit. We can write
C
 Z
I
C
= dyDξI q(y,ξ)logI q(y,ξ),
(58)
Z (cid:18) 1 (cid:13) (cid:13)2(cid:19)
I q(y,ξ) = RmDZδ y−
m
(cid:13) (cid:13)(I m−q)1/2Z+q1/2ξ(cid:13)
(cid:13) 2
.
√
Let y := m[y−tr(I −q)−(ξ⊤qξ)/m]. We can change variables in eq. (58), and obtain:
e m
 Z 1
I
C
= dy eDξJ q(y e,ξ)logJ q(y e,ξ)+
2
logm,
" #! (59)
Z √ 1 (cid:13) (cid:13)2 ξ⊤qξ
J q(y e,ξ) = RmDZδ y e− m
m
(cid:13) (cid:13)(I m−q)1/2Z+q1/2ξ(cid:13)
(cid:13)
2−tr(I m−q)−
m
.
29Noticethattheadditiveterm(1/2)logminI justamountstoarenormalizationofthepartition
C
function Z, so we remove this additional constant in what follows. We proceed to simplify
J (y,ξ) in the large-m limit. We have
q e
J (y,ξ)
q e
" #!
Z √ Z⊤(I −q)Z Z⊤(I −q)1/2q1/2ξ
m m
= DZδ y− m −tr(I −q)+2 ,
e m
Rm m m
Z du √ Z −iu√ mh Z⊤(Im−q)Z+2Z⊤(Im−q)1/2q1/2ξi
= eiuey+iu mtr(Im−q) DZe m m ,
2π
=
Z du eiuey+iu√ mtr(Im−q)−1 2logdeth Im+2iu(I √m m−q)i −2 mu2 ξ⊤q1/2(Im−q)1/2h Im+2iu( √Im m−q)i−1 (Im−q)1/2q1/2ξ
,
2π
= Z du eiuey−u2tr[(Im−q)2]−2 mu2 ξ⊤q1/2(Im−q)q1/2ξ+O(1/√ m),
2π
1
−(ey)2
√
=
q
e 2σ ξ2 +O(1/ m),
2πσ2
ξ
where
4
σ2 := 2tr[(I −q)2]+ ξ⊤q1/2(I −q)q1/2ξ.
ξ m m m
Plugging it back into eq. (59) yields:
Z 1
−(ey)2 "
1 y2
#
√
I
C
= dyDξ
q 2πσ
ξ2e 2σ ξ2 −
2
log2πσ ξ2−
2σ ξ2
+O(1/ m),
1 Z 1 √
= − Dξlog[2πσ2]− +O(1/ m).
2 ξ 2
Since ξ ∼ N(0,I ), it follows from elementary concentration of measure that σ2 concentrates
m ξ
on its average value σ2 given by:
σ2 := 2tr[(I −q)2]+4tr[(I −q)q] = 2tr[(I −q)(I +q)] = 2tr[I −q2].
m m m m m
All in all we reach that (up to additive constants):
1 √
I = − logtr[I −q2]+O(1/ m). (60)
C m
2
Combiningeqs.(57)and(60)ineq.(55),wegetatleadingorderinm,withΦ := lim(1/d)ElogZ:
1 (cid:26) 1 1 1 α (cid:27)
Φ = extr − tr[qqˆ]+ tr[qˆ]− trlog(I +qˆ)− e logtr[I −q2] . (61)
m m
m q,qˆ 2 2 2 2
Eq. (61) can be easily solved, and yields:
 qˆ = q(I −q)−1,
 m
2α
qˆ =
tr[I
−e q2]q.
m
This implies that (recall 0 ⪯ q ⪯ I ):
m
 1
 0 if α
e
≤ 2,
q = 1 (62)
(2α−1)I if ≤ α ≤ 1,

I
e
if α
≥m
1.
2 e
m e
30Now that we have obtained q in eq. (62), we can compute the MMSE, or generalization error.
Defining it as in eq. (5):
m
MMSE := E E [(y −yˆBO(x ))2],
d
2
W⋆,D ytest,xtest test test
the same arguments used in the proof of Lemma D.1 show that in the large m limit (but taken
after d → ∞), we have at leading order
m m
MMSE = Etr[(S⋆−SBO)2] = 1− Etr[(SBO)2],
d
d d
with S := (1/m)Pm w w⊤. Notice that SBO = ⟨S⟩, so that
k=1 k k
* !2+
MMSE = 1− 1 E X w k⊤w l′ ,
d
m d
1≤k,l≤m
where w,w′ are two independent samples under the posterior measure ⟨·⟩. We know that
the overlap concentrates (cf. the discussion around eq. (56)), so that at leading order, with
MMSE := lim MMSE :
d→∞ d
MMSE = 1− 1 E X q2 = 1−tr[q2].
m
kk′
1≤k,l≤m
Combining it with eq. (62), we reach:
 1
 1 if α
e
≤ 2,
MMSE = 1
4α(1−α) if ≤ α ≤ 1,
 0e
if α
≥e
1.
2 e
e
We have recovered eq. (23) from the limit m → ∞ taken after d → ∞!
C.3 The large κ limit
We consider here κ → ∞, with α remaining of order Θ(1) as κ → ∞. Since the MMSE remains
finite as well, we see from eq. (8) that we must have the scaling qˆ= κt, with t remaining finite
as κ → ∞. A very similar derivation to the one of Appendix C.1 yields that eq. (9) in this limit
becomes (with Λ := ∆(2+∆)):
4π2 Z
1−2α+Λt = lim µ (y)3dy,
κ→∞ 3κt 1/[κt]
1
= .
1+t
Combining it with eq. (8) yields that
p
1−2α−Λ+ (1−2α+Λ)2+8αΛ
MMSE = , (63)
2
where we recall Λ = ∆(2+∆). In particular, for ∆ = 0, we reach MMSE = max(1−2α,0).
31D Other technicalities
D.1 Properties of the MMSE of eq. (5)
Let S⋆ := (1/m)Pm w⋆(w⋆)⊤, and SˆBO := E[S|D] the Bayes-optimal estimator of S⋆. We
k=1 k k
show here the following lemma on the MMSE of eq. (5), under the high-dimensional limit of
eq. (6):
Lemma D.1. For a constant C = C(κ) > 0:
(cid:12)
(cid:12) (cid:12)MMSE d−κE
S⋆,Dtr(cid:20)(cid:16) S⋆−SˆBO(cid:17)2(cid:21)(cid:12)
(cid:12) (cid:12) ≤
C(κ)
.
(cid:12) (cid:12) n
Lemma D.1 shows that we can consider the MMSE on S equivalently to the generalization
MMSE of eq. (5).
Limits – Notice that if the posterior concentrates around the true W⋆, then SˆBO = E[S|D]
concentratesonS⋆, whichimpliesthatMMSE → 0. Conversely, forα = 0(i.e.intheabsenceof
d
data), the Bayes-optimal estimator becomes SˆBO = E[S⋆] = I , so that Etr[(S⋆−SˆBO)2] = κ−1.
d
Thus, we have MMSE → 1 for α = 0.
d
Proof of Lemma D.1. – Notice that (cf. eq. (2)):
x⊤Sx
E [f (x)] = ∆+ ,
z W
d
with S := (1/m)Pm w w⊤. Using this in eq. (4), and plugging it in eq. (5), we get (with
k=1 k k
z ∼ N(0,I ) and x ∼ N(0,I )):
m d
MMSE d =
m
E
S⋆,D,z,x
 ∆ 1−
∥z∥2!
+
x⊤(SˆBO−S⋆)x
−
2√ ∆ Xm
z k
x √⊤w k⋆!!2

2 m d m d
k=1
−∆(2+∆),
m  ∥z∥2!2 [x⊤(SˆBO−S⋆)x]2 4∆ 
=
2
E S⋆,D,z,x∆2 1−
m
+
d2
+
m
tr(S⋆)−∆(2+∆),
( =a) m E
" [x⊤(SˆBO−S⋆)x]2#
,
2
S⋆,D,x
d2
( =b) m E (cid:20)(cid:16) tr(S⋆−SˆBO)(cid:17)2(cid:21) +κE tr(cid:20)(cid:16) S⋆−SˆBO(cid:17)2(cid:21) , (64)
S⋆,D S⋆,D
2
where we used E[∥z∥4] = m2+2m and Etr(S⋆) = 1 in (a), and E [(x⊤Mx)2] = Tr[M]2+
x∼N(0,I )
d
2Tr[M2]in(b). Itremainstoboundthefirsttermofeq.(64)toconcludetheproofofLemmaD.1.
We notice that, by linearity of the trace, tr(SˆBO) is the Bayes-optimal estimator for tr(S⋆), i.e.
(cid:20)(cid:16) (cid:17)2(cid:21) h i
E tr(S⋆−SˆBO) = minE (tr(S⋆)−r(D))2 . (65)
S⋆,D S⋆,D
r(D)
In particular, considering the estimator
1 Xn
r(D) := (y −∆),
i
n
i=1
√
=
1 Xn ( x iS⋆x
i
+∆
∥z i∥2 −1!
+
2 ∆ Xm
z
x √i⊤w k⋆!)
,
i,k
n d m m d
i=1 k=1
32we have using eq. (65):
E
(cid:20)(cid:16) tr(S⋆−SˆBO)(cid:17)2(cid:21)
≤ E
"( tr"
S⋆
1 Xn
x x⊤−I
!#
+∆
Pn i=1∥z i∥2 −1!
S⋆,D S⋆,{xi},{zi} n i i d nm
i=1
+2√
∆ Xn Xm
z
i,k
x √i⊤w
k⋆!)2
,
nm d
i=1k=1
(a)
≤ 3[I +I +I ], (66)
1 2 3
using the Cauchy-Schwarz inequality in (a), with
 I 1 := E  tr " PS⋆
n
n1 ∥zX i=n
∥1
2x ix i⊤ !− 2I d!#!2 ,
I 2 := ∆2E  i=1 i −1 ,
nm
I
3 :=
4∆E

n1
m
Xn Xm
z i,k
x √i⊤w
dk⋆!!2
.
i=1k=1
It is a tedious but straightforward computation to compute {I }3 , as it only involves the first
a a=1
moments of Gaussian random variables. We get (recall m = κd):
 2
I
1
=
n
2∆d( 21+κ−1),
I = , (67)
2
I
3
=
κ 4n ∆d
.
κnd
Combining eqs. (66) and (67), and plugging it back in eq. (64), we get
(cid:12)
(cid:12) (cid:12)MMSE d−κE
S⋆,Dtr(cid:20)(cid:16) S⋆−SˆBO(cid:17)2(cid:21)(cid:12)
(cid:12) (cid:12) ≤
C(κ)
,
(cid:12) (cid:12) n
which ends the proof of Lemma D.1.
D.2 Proof of Theorem 4.3
First, we note that Theorem 1 of Pourkamali et al. [2024] implies that:
1 Q qˆ
0
Ψ(qˆ) = I (qˆ,µ ,µ )− , (68)
2 HCIZ MP,κ 1/qˆ 2
and we recall the definition of I in eq. (46). We recall then a fundamental result proven in
HCIZ
Guionnet and Zeitouni [2002]:
Theorem D.2 (Theorem 1.1 of Guionnet and Zeitouni [2002]). For any compactly supported
probability measures ν and µ, and any t > 0:
1 1 1 3 1 1
I (t−1,ν,µ) = −J(ν;µ)− Σ(ν)+ E [X2]− + logt+ E [X2]. (69)
HCIZ ν µ
2 2 4t 8 4 4t
Moreover, the function J(ν;µ) satisfies the following property. Let d be a distance on the space
√
of probability measures on R that is compatible with the weak topology. Let X := R + tW,
33where W ∼ GOE(d), and R is a fixed (deterministic) matrix, with uniformly bounded spectral
norm, and a compactly supported limiting eigenvalue distribution µ. Let µ denote the empirical
X
eigenvalue distribution of X. Then, for any ν ∈ M+(R):
1
1 1
limlimsup logP[d(ν,µ ) < δ] = limliminf logP[d(ν,µ ) < δ],
δ↓0 d→∞ d2 X δ↓0 d→∞ d2 X
= −J(ν;µ). (70)
In other words, the function J(ν;µ) is the large deviations rate function (in the scale d2) for
√
the empirical spectral measure of R + tW, where R is a fixed (deterministic) matrix with
asymptotic spectral distribution µ, and W ∼ GOE(d). It is a well-known property of the
free convolution [Speicher, 1993] that µ → µ⊞σ √ as d → ∞, where the convergence is
X s.c., t
meant in the weak sense (and almost surely). Combining this result with eq. (70), we have
J(µ⊞σ √ ;µ) = 0. This yields by eq. (69):
s.c., t
1 1 1
I (t−1,µ,µ⊞σ √ ) = −Σ(µ⊞σ √ )+ logt− + E [X2]. (71)
HCIZ s.c., t s.c., t 2 4 t µ
Combining eqs. (68) and (71) yields eq. (21).
Remark – The proof above can be straightforwardly extended to the free entropy of denoising
any matrix S with a rotationally-invariant distribution and a compactly-supported limiting
eigenvalue distribution (beyond the Wishart ensemble), as the results of Guionnet and Zeitouni
[2002], Pourkamali et al. [2024] hold under these more general assumptions.
D.3 Perfect recovery threshold in the noiseless case
Inthissection, wegiveananalyticargumenttoderivethevalueoftheperfectrecoverythreshold
α (see eq. (1)) in the noiseless setting. In the limit of perfect recovery the MMSE goes to 0,
PR
thus by eq. (8) (with ∆ = 0) this implies qˆ→ ∞. Using eq. (9), we can then write the equation
satisfied by the perfect recovery threshold as
3(1−2α ) Z
PR = limt dyµ (y)3, (72)
4π2 t↓0 t
in which µ = µ ⊞σ √ , see Appendix A.
t MP,κ s.c., t
D.3.1 The case κ < 1
Informal argument –Recallthatinthiscasewecanwriteµ (x) = (1−κ)δ(x)+κν (x),
MP,κ MP,κ
inwhichν iscompactlysupportedawayfromzero,seeAppendixA.Ast → 0,wethusexpect
MP,κ
µ to have a discontinuous support, made of two parts:
t
√
(a) A small semicircular density centered around 0, of radius O( t), with mass (1−κ).
(b) A smooth density, compactly supported away from zero, which has a well-defined limit as
t → 0, and a mass κ.
Because of the factor t in the right-hand side of eq. (72), only the part (a) will matter in the
limit.
Formal derivation – We first rewrite by a change of variable
Z Z √ √
t dyµ (y)3 = dz[ tµ ( tz)]3.
t t
34It is clear that for all x ̸= 0, we have µ (x) → κν (x) as t → 0, and R ν (y)3dy < ∞, so
t MP,κ√ MP,κ
that we can truncate the integral above to all |z| ≤ ε/ t, for any ε > 0 finite as t → 0. We will
now show the following, for any x ∈ R:
√ √
lim tµ (x t) = (1−κ)σ √ (x). (73)
t s.c., 1−κ
t→0
√
We fix z ∈ C (where C := {z ∈ C : Im(z) > 0}). Letting y = tz, we know from the
+ +
Marchenko-Pastur theorem [Marchenko and Pastur, 1967] that g (y) := E [1/(X −y)] is the
t µt
unique solution in C to the equation
+
1 1
y = − −tg.
1+g/κ g
√ √ √
Since y = tz, it is clear that g = O(1/ t), and letting h := tg, we easily get the expansion
1−κ √
z = − −h+O( t),
h
which can be inverted to
−z±p z2−4(1−κ) √
h = +O( t). (74)
2
Notice that if we denote S (z) the Stieltjes transform of σ √ , eq. (74) can be written as
κ √ s.c., 1−κ
(see e.g. Anderson et al. [2010]) h = (1−κ)S (z)+O( t). By considering z = x+iε with x ∈ R
κ
and the limit ε → 0, we reach using the Stieltjes-Perron inversion theorem (Theorem A.2) that
for any x ∈ R:
√ √
lim tµ (x t) = (1−κ)σ √ (x). (75)
t s.c., 1−κ
t→0
Coming back to eq. (72) this implies:
3(1−2α ) Z
PR = limt dyµ (y)3,
4π2 t→0 t
Z
= (1−κ)3 dyσ √ (y)3,
s.c., 1−κ
Z
= (1−κ)2 dyσ (y)3,
s.c.
3
= (1−κ)2.
4π2
Equivalently:
(1−κ)2−1 κ2
α = = κ− . (76)
PR
2 2
We notice that this critical value of n/d2 coincides with a naive counting argument of degrees of
freedom of S⋆. Indeed, as can be seen by the spectral decomposition, the set of d×d symmetric
matrices of rank m has, to leading order in d, p(κ)d2 degrees of freedom, where p(κ)d2 is
the dimension of the Stiefel manifold of orthonormal m-frames in Rd. It is well-known that
p(κ) = κ−κ2/2 for d → ∞ [Helmke and Moore, 2012].
D.3.2 The case κ ≥ 1
The case κ > 1 is simpler to carry out. In this case, µ does not have a singular part at
MP,κ
x = 0, and µ has a smooth density as t → 0, and
t
Z 3 κ2
dyµ (y)3 = ,
MP,κ 4π2κ−1
35so that
3(1−2α ) Z
PR = limt dyµ (y)3 = 0,
4π2 t↓0 t
and we reach α = 1/2, so that α d2 (asymptotically) coincides with the number d2/2 of
PR PR
degrees of freedom of symmetric matrices. Since α is increasing with κ, and has limit 1/2
PR
both for κ ↑ 1 and κ ↓ 1, we deduce that α = 1/2 for κ = 1 as well.
PR
D.4 The derivative of the error at the perfect recovery threshold
Here, we extend the derivation of Section D.3 to compute the derivative of the MMSE with
respect to α at the perfect recovery threshold. We start again from eqs. (8) and (9). Letting
t := 1/qˆ, we get, with α = α :
PR
(cid:18)∂MMSE(cid:19) (cid:18)∂t(cid:19)
= 2ακ ,
∂α ∂α
PR PR
3ακ (cid:20) (cid:18) Z (cid:19)(cid:21)−1
= − lim∂ t µ (y)3dy . (77)
π2 t→0 t t
We thus compute the next order of the expansion of tR µ (y)3dy as t → 0.
t
D.4.1 The case κ < 1
WeextendtheargumentmadeinSectionD.3.1. Noticethatherethesmoothpartofthedensity,
compactly supported away from zero, contributes at this order. Formally, for any small enough
ε > 0:
Z Z
t dyµ (y)3 = tκ3 dyν (y)3+o (t),
t MP,κ t
|y|≥ε
3tκ4
= +o (t), (78)
4π2(1−κ) t
On the other hand, we have around the singularity at y = 0:
Z Z √ √
t dyµ (y)3 = dz[ tµ ( tz)]3. (79)
t √ t
|y|≤ε |z|≤ε/ t
We evaluate the next order of the right-hand side of eq. (79) using the same approach as in
Section D.3.1, going to next orders in the expansion as t → 0 of eq. (74). Using then again the
Stieltjes-Perron inversion theorem, we reach with tedious but straightforward computations the
generalization of eq. (75):
√ √ √ zκ2
tµ ( tz) = (1−κ)σ √ (z)− t
t s.c., 1−κ 2π(1−κ)p 4(1−κ)−z2
κ3(cid:2) z4−6z2(1−κ)+2(4−κ)(1−κ)2(cid:3)
+t +O(t3/2), (80)
2π(1−κ)3[4(1−κ)−z2]3/2
√ √ √ √
for any |z| ≤ 2 1−κ, while tµ ( tz) = O(t3/2) if |z| > 2 1−κ. This then yields:
t
Z 3(1−κ)2 3tκ3
t dyµ (y)3 = + +O(t3/2). (81)
t 4π2 4π2(1−κ)
|y|≤ε
Combining eqs. (78) and (81) in eq. (77), we obtain (recall α = α = κ−κ2/2):
PR
(cid:18)∂MMSE(cid:19) 4 12
= −2− + .
∂α κ 1+κ
PR
36D.4.2 The case κ ≥ 1
Again, we consider κ > 1. The argument of Section D.4.1 generalizes immediately, removing
the analysis of the singular part around y = 0. We get directly
Z Z
t dyµ (y)3 = t dyµ (y)3+o (t),
t MP,κ t
3tκ2
= +o (t). (82)
4π2(κ−1) t
Plugging it in eq. (77), we get in this case:
(cid:18)∂MMSE(cid:19) 2
= −2+ .
∂α κ
PR
Again, the specific case κ = 1 can be tackled by continuity, as the derivative tends to 0 both as
κ ↑ 1 and κ ↓ 1.
D.5 Details on the reduction to matrix estimation
We describe here how to effectively reduce the problem of eq. (2) to an estimation problem in
terms of S⋆ := (1/m)Pm w⋆(w⋆)⊤.
k=1 k k
Remark – While our argument is backed by precise probabilistic concentration arguments, we
notice that it is not a proof of the equivalence of the problems of eq. (2) and eq. (10) under
all statistical tests, as would be implied e.g. by the contiguity of distributions [Le Cam, 1960,
Kunisky et al., 2019]. Rather, we analyze the leading order of eq. (2) and argue that (with high
probability over the distribution of the data and the teacher weights), the first non-trivial order
of the observations is characterized by the equivalent model of eq. (10). Notably, we do not
claim the statistical equivalence of the problems of eq. (2) and eq. (10), but rather only that
their asymptotic MMSEs coincide. While even this weaker statement is not formally implied
by the arguments sketched below, we expect that they form the backbone of a formal proof of
this claim, which we leave for future work and would be carried e.g. by Gaussian interpolation
techniques.
√
Let us define Z := (x x⊤ −I )/ d, and recall that x ∼ N(0,I ). Expanding the square, we
i i i d i d
can rewrite the law of the output y = f (x ) as
i W⋆ i
√
y = ∆+tr[S⋆]+
√1
Tr[Z S⋆]+∆
∥z i∥2 −1!
+
2 √∆ Xm
z x⊤w⋆, (83)
i d i m m d i,k i k
k=1
where (z )n i. ∼i.d. N(0,I ). In what follows, we analyze the leading order of eq. (83). More
i i=1 m √
specifically, we denote y := d(y −1−∆), and we decompose
ei i
√
y = Tr[Z
S⋆]+√ d(tr[S⋆]−1)+∆√
d
∥z i∥2 −1!
+
2 ∆ Xm
z x⊤w⋆. (84)
ei i m m i,k i k
| {z } k=1
=:I1
| {z }
=:I2
Let us consider the leading order of the different terms of eq. (84). Since S⋆ ∼ W , Tr[S⋆] =
m,d
Pm ∥w⋆∥2/m strongly concentrates on its average. More precisely, by Bernstein’s inequality
k=1 k
(see Corollary 2.8.3 of Vershynin [2018]) we have, for all t ≥ 0:
(cid:16) (cid:17)
P[|tr(S⋆)−1| ≥ t] ≤ 2exp −Cd2min(t,t2) ,
where C > 0 depends only on κ > 0. In particular,
√
P[|I | ≥ d−1/4] ≤ 2exp(−C d),
1
37so that we can replace I by 0 at leading order in eq. (84).
1
We now tackle I , first for fixed (x ,W⋆). Using that ∥z ∥2 strongly concentrates around its
2 i i
average, and the central limit theorem applied to the fluctuations of ∥z ∥2, one can see that
i
d
for all i ∈ [n], we have (with g ∼ N(0,I ) independently of z , and = denoting equality in
i m i
distribution):
√
√ d"
∆
∥z i∥2 −1!
+
2 √∆ Xm
z
x⊤w⋆#
m m d i,k i k
k=1
√
=d
√ d"
∆
∥z i∥2 −1!
+
2 √∆ ∥z i∥ Xm
g
x⊤w⋆#
,
m m d∥g i∥ i,k i k
k=1
s
2∆2x⊤S⋆x 4∆
∼ ξ i i + , (85)
d→∞ i
κ d κ
with ξ i. ∼i.d. N(0,1), independently of (x ,w⋆). The equivalence as d → ∞ is given for a fixed
i i k
i ∈ [n]: coherently with the remark above, we notice that a formal mathematical proof of
equivalence of the two problems of eq. (2) and eq. (10) would rather need to tackle the joint
law of all the observations, and to quantitatively control the deviation between the left and
right-hand sides of eq. (85) as d → ∞. We leave such a proof for future work.
We finally note that the variance term on the right-hand side of eq. (85) strongly concentrates,
uniformly in i ∈ [n], as by the Hanson-Wright inequality and the union bound, we have (see
Theorem 6.2.1 of Vershynin [2018]) for all t ≥ 0:
"(cid:12) (cid:12) # " !#
(cid:12)1 (cid:12) dt2 dt
P (cid:12) max|x⊤S⋆x −tr(S⋆)(cid:12) ≥ t ≤ 2nexp −Cmin , , (86)
{xi} (cid:12) (cid:12)d i∈[n] i i (cid:12) (cid:12) ∥S⋆∥2 op ∥S⋆∥ op
for some constant C > 0. Since the spectral norm of a Wishart matrix ∥S⋆∥ strongly concen-
op
trates on its average under the Wishart distribution (see Theorem 4.4.5 of Vershynin [2018]),
we see that, uniformly over i ∈ [n], the leading order of the variance in the right-hand side of
eq. (85) is equal to ∆e := 2∆(2+∆)/κ. This ends our justification of eq. (10).
D.6 Unique maximizer q⋆ in eq. (13)
R
Notice that if J (q) := dyDξJ (y,ξ)logJ (y,ξ), then one can check that J is a strictly
out R×R q q out
increasing function of q under mild regularity conditions on P (namely assuming the presence
out
of an additive Gaussian noise with arbitrarily small variance), see Proposition 21 of Barbier
et al. [2019]. The fact that q⋆ is uniquely defined for all values of α > 0 except possibly in a
countable set follows then from Proposition 1 of Barbier et al. [2019], see also Appendix A.2
there.
D.7 Derivation of Result 1 from Claim 2
In this section, we derive eqs. (8) and eq. (9) from Claim 2, in the case of Gaussian noise. More
p
precisely, we assume P out(y|z) = exp[−(y−z)2/(2∆e)]/ 2π∆e, in accordance with eq. (10). It is
then an easy computation to check (recall the definition of J in eq. (14)):
q
Z 1
dyDξJ q(y,ξ)logJ q(y,ξ) = − log[∆e +2(Q 0−q)].
R×R 2
We then reach that q = q⋆ is characterized as the maximum of the following function:
 α
F(q) = I(q)−
2
log[∆e +2(Q 0−q)],
(cid:20)(Q −q)qˆ 1 1 1(cid:21) (87)
I(q) := qˆin ≥f
0
0
4
− 2Σ(µ 1/qˆ)−
4
logqˆ−
8
.
38Recall that here µ := µ ⊞σ √ . It is known (see eqs. (77-78) of Semerjian [2024] e.g.)
t MP,κ s.c., t
that
∂Σ(µ ) 2π2 Z
t = µ (y)3dy.
t
∂t 3
Thus, qˆ= qˆ(q) can be characterized as the solution3 to
(Q −q) π2 Z 1
0 + µ (y)3dy− = 0. (88)
4 3qˆ2 1/qˆ 4qˆ
By eq. (87), q is a solution in [1,Q ] to:
0
4α
qˆ(q) = . (89)
∆e +2(Q 0−q)
Recalling that MMSE = κ(Q −q) by Claim 2, eq. (89) implies eq. (8). Combining eq. (89) with
0
eq. (88), we reach eq. (9).
D.8 The limit α → 0
In this section, we check that the state evolution equations derived in Section D.7 yield indeed
that q → 1 as α → 0. Indeed, in this limit, SBO = E[S⋆] = I , so that we must have q =
d
Etr[SBOS⋆] = 1.
Recall that qˆ = 4α/[∆e + 2(Q
0
− q)], and that qˆ is given by eq. (9). In particular, qˆ → 0 as
α → 0. Assuming the scaling qˆ∼ qˆ α as α → 0, we get
0

2 ∆e
q = Q 0−
qˆ
+
2
,
0 (90)
−2+ ∆eqˆ
0 = qˆ 0F′(0),
2
where F(p) := (4π2/3)R [p−1/2µ (z·p−1/2)]3dz. Letting ν (z) := p−1/2µ (z·p−1/2), we know
1/p p 1/p
by a similar reasoning as the one of Section D.3 that the Stieltjes transform h = h (z) of ν
p p
satisfies the equation:
√
κ p 1
z = √ − −h.
κ+h p h
As p → 0, we can thus compute the expansion of h (z) in powers of p. Applying then the
p
Stieltjes-Perron inversion theorem (Theorem A.2), we get the expansion of ν (z) in powers of p
p
as:
√ √
4−z2 √ 3z 4−z2 3p(2−z2)(4+κ−z2)
ν (z) = + p − √ +O(p3/2),
p 2π 8π3 8π3κ 4−z2
for |z| ≤ 2, and ν (z) = O(p3/2) for |z| ≥ 2. Plugging this expansion into F(p), we get:
p
p
F(p) = 1− +o(p).
κ
Coming back to eq. (90), this gives qˆ
0
= 4κ/[2+∆eκ], and (recall Q
0
= 1+κ−1) then q = 1, so
that our equations are indeed consistent in the limit α → 0.
3Notice that one can show that qˆis the minimizer of a convex function in eq. (87). This can be shown e.g. by
recalling the relationship of this function to the free entropy of a matrix denoising problem (Theorem 4.3) and
using the I-MMSE theorem. We refer to Barbier et al. [2019], Maillard et al. [2020] for more details.
39D.9 State evolution: a connection between Algorithm 1 and Result 1
We briefly sketch here the statistical-physics style derivation of the so-called state evolution
of Algorithm 1: this will draw a connection between the performance of the Bayes-optimal
estimator,characterizedbyResult1,andtheestimatorofAlgorithm1. Wedefineqt := tr[(Sˆt)2],
and mt := tr[SˆtS⋆]. Thanks to Bayes-optimality, one can show that, along the GAMP-RIE
trajectory, the so-called Nishimori identities are preserved (see Zdeborová and Krzakala [2016]
for more details), so that we have, at leading order as d → ∞, that qt = mt.
Uptosomecriticaldifferences,wecantransposethederivationofZdeborováandKrzakala[2016]
ofthestateevolutionofGAMPforgeneralizedlinearmodelswithGaussiansensingvectors, and
i.i.d. priors, to our GAMP-RIE algorithm. The differences with our setting are twofold:
(i) The “sensing vectors” Z are not Gaussian. We conjecture that the universality arguments
i
discussed in Section 4 extend to the analysis of the GAMP-RIE algorithm. This allows us
to replace Z by G i. ∼i.d. GOE(d) when evaluating (qt,mt) (i.e. when studying the high-
i i
dimensional performance of Algorithm 1). We are then able to make a direct use of some
results of Zdeborová and Krzakala [2016].
(ii) The prior over S⋆ is not i.i.d.: as we saw, this led to a non-trivial “denoising” part in Al-
gorithm 1. The performance of this denoising procedure in the high-dimensional limit can
however be characterized precisely, as the function F admits a closed-form expression.
RIE
We now briefly expose the derivation, transposed to our setting under the universality assump-
tionabove. Bydefinitionofqt, wehavecˆt = 2(Q −qt). Ifω,z arecenteredandjointlyGaussian
0
variables with E[ω2] = 2qt, E[z2] = 2Q , and E[ωz] = 2mt = 2qt, and y ∼ P (·|z), we define
0 out
qˆt := 4αE [g (y,ω,Vt)2], (91)
y,w out
sothatAt = qˆt/2inthen,d → ∞limit. Forthe“channel”partoftheGAMP-RIEalgorithm,the
standard analysis for generalized linear model, alongside the universality phenomenon discussed
above (which allows replacing Tr[Z Sˆt] by Tr[G Sˆt] in the update of ωt) shows that qˆt satisfies
i i i
the equation:
∂ (cid:20)Z (cid:21)
qˆt = 4α dyDξJ (y,ξ)logJ (y,ξ) , (92)
q q
∂q
q=qt
where J (y,ξ) is defined in eq. (14). Eq. (92) is the very same as for the standard GAMP for
q
generalized linear models [Zdeborová and Krzakala, 2016].
We have, however, a more structured prior. After replacing Z by Gaussian matrices G in
i i
Algorithm 1, the argument is that at leading order as d → ∞ one has:
1
Rt =d S⋆+ ξ, (93)
p
qˆt
with ξ ∼ GOE(d). Heuristic details on how to derive eq. (93) can be found again in Zde-
borová and Krzakala [2016], see Section 6.4.14 there. By definition of F , this implies
RIE
qt+1 := tr[(Sˆt+1)2] = Q −F ((qˆt)−1), so that by eq. (18):
0 RIE
1 4π2 Z
Q −qt+1 = − dλµ (λ)3, (94)
0 qˆt 3(qˆt)2 1/qˆt
4Section VI.D.1 in the arXiv version of Zdeborová and Krzakala [2016].
40with µ := µ ⊞ σ √ . Notice that remarkably, eqs. (92) and (94) precisely match the
t MP,κ s.c., t
extremization equations of the asymptotic free entropy, as given in Claim 2 and Result 1, ex-
actly as for “usual” generalized linear models with i.i.d. priors [Rangan, 2011, Javanmard and
Montanari, 2013, Zdeborová and Krzakala, 2016].
Mathematical consequences – The fact that, assuming the universality property above, our
GAMP-RIE algorithm can be seen as the usual GAMP algorithm in a generalized linear model
with a non-separable prior has a very interesting consequence. Indeed, the latter model admits
a rigorous state evolution thanks to the analysis of Berthier et al. [2020], Gerbelot and Berthier
[2023]. To make this point clearer, we notice that (after replacing Z by GOE(d) matrices G )
i i
Algorithm 1 can be written in the following form:
 ωt = Gvˆ(ut,Σ )−Vtg (y,ωt−1,Vt−1),
 t out
1 (95)
ut = dG⊤g out(y,ωt,Vt)+Σ−
t
1vˆ(ut,Σ t).
Let us clarify some notations used in eq. (95):
• ut ∈ Rp, with p := (cid:0)d+1(cid:1) , can be seen as the flattening of the symmetric matrix AtRt
2
of Algorithm 1 via the following canonical mapping. For S ∈ S , we define vec(S) ∈
√ d
Rp by vec(S) = S and vec(S) = 2S for i < j. This flattening is an isometry:
ii ii ij ij
⟨vec(S),vec(R)⟩ = Tr[SR]. We have ut = vec(AtRt).
• G ∈ Rn×p is a Gaussian i.i.d. matrix, whose elements have variance 2/d.
• Σ−1 := −2αEdiv[g (t,ωt,Vt)] is related to At by At = Σ−1.
t out t
• Vt := 2F (Σ /2).
RIE t
• vˆt(ut,Σ ) ∈ Rp istheflatteningoftheRIEdenoiserofAlgorithm1, i.e.ifwedenoteRt/Σ
t t
the matrix such that ut = vec(Rt/Σ ):
t
vˆt(ut,Σt) := vec[f (Rt,Σ /2)].
RIE t
Eq. (95) is the canonical form of the GAMP algorithm, as written e.g. in Berthier et al. [2020],
Gerbelot and Berthier [2023]. In particular, we can leverage their results to write:
Theorem D.3 (StateEvolution(informal)Berthieretal.[2020],GerbelotandBerthier[2023]).
Denote qt := tr[Sˆ S⋆] and qˆt := 4α Pn g (y ,ωt,Vt)2 (recall the definition of these
AMP t AMP n i=1 out i i
quantities in Algorithm 1). Assume that the “sensing matrices” (Z )n in Algorithm 1 are
i i=1
replaced by (G )n , which are i.i.d. GOE(d) matrices. Then for any t ≥ 0, qt and qˆt
i i=1 AMP AMP
follow the state evolution equations (92) and (94) asymptotically as d,n → ∞.
BeyondtherigorouscontroloftheGAMP-RIEalgorithm, TheoremD.3hasanadditionalmath-
ematical consequence: it allows to leverage a set of mathematical techniques that use AMP
algorithms to prove results on the asymptotic MMSE and on the mutual information, as Theo-
rem D.3 implies that they can be used verbatim in our setting. More precisely, the fact that the
GAMP-RIEalgorithmachievesanMSEwithvaluegivenbyClaim2immediatelyyieldsthatthe
latter is, at least, an upper bound on the asymptotic MMSE (when assuming Gaussian GOE(d)
“sensing vectors” G ). Additionally, the application of the I-MMSE theorem [Guo et al., 2005]
i
shows that our claimed free entropy (i.e. the limit of (1/d2)ElogZ in Claim 2) is a lower bound
on the real one (see e.g. section 2.C in Barbier et al. [2016]).
41E Learning the second layer weights
We sketch here in a mathematically informal way the generalization of our results to the setting
where the second layer weights are also learned, cf. eq. (3). Throughout this section, we will
assume for simplicity that P has bounded support, although we expect our results to hold also
a
for more general choices of P . We show how to extend Claim 2 to this case, by detailing the
a
differences in the steps outlined in Section 4. We eventually show that Algorithm 1 can also be
straightforwardly extended to this setting.
E.1 Generalizing the derivation
E.1.1 Reduction to matrix estimation
We first discuss the reduction to a matrix estimation problem, generalizing Section D.5 to this
setting. We define
S⋆ :=
1 Xm
a⋆w⋆(w⋆)⊤, (96)
m k k k
k=1
and we denote m := E [a] and c := E [a2]. We define the MMSE as (notice the additional
a Pa a Pa
factor c with respect to eq. (5)):
a
m (cid:20)(cid:16) (cid:17)2(cid:21)
MMSE := E E y −yˆBO(x ) −∆(2+c ∆). (97)
d 2 W∗,D ytest,xtest test D test a
By repeating the (mathematically informal) arguments of Section D.5 to this setting, we find
that, at leading order as m,d → ∞:
√ q
d(y i−∆−tr[S⋆]) = Tr[Z iS⋆]+ ∆eξ i, (98)
i.i.d.
with ξ
i
∼ N(0,1), and ∆e := 2∆(2+∆c a)/κ. We let
  
y
ei
:=
√
dy i−
n1 Xn
y i,
j=1
Y
:=
n1 Xn
y i.
i=1
The observation of (y )n is equivalent to the one of (y )n and Y. Notice that by eq. (98), we
i i=1 ei i=1
have
|Y −∆−tr(S⋆)| =
√1 (cid:12) (cid:12)
(cid:12)
(cid:12)Xn
{Tr[Z
iS⋆]+q
∆eξ
i}(cid:12) (cid:12)
(cid:12) (cid:12). (99)
n d (cid:12) (cid:12)
i=1
ConditionallyonS⋆, theright-hand-sideofeq.(99)isasumofnindependentzero-meanrandom
variables, which thus typically fluctuates in the scale5 O[(nd)−1/2] = O(d−3/2). Since y =
√ ei
d[y −Y], this implies that at leading order we have
i
q
y
ei
= Tr[Z iS⋆]+ ∆eξ i. (100)
The observer also has access to Y, alongside {y }n . Notice that by the argument above, Y
ei i=1
is (up to order d−3/2) a deterministic observation of tr[S⋆]. By eq. (97), and repeating the
5Recall that tr[(S⋆)2]=O(1) with high probability.
42arguments of the proof of Lemma D.1, we reach that again we have MMSE = κEtr[(S⋆−SˆBO)2]
as d → ∞. Moreover:
MMSE = κE tr[(S⋆−SˆBO)2],
S⋆,Y,{eyi}
= κE [E (tr[(S⋆−SˆBO)2]|Y)].
Y S⋆,{eyi}
Conditioning on Y amounts to condition on the value of tr(S⋆), as detailed above. Let us make
two important remarks:
(i) As d → ∞, Y concentrates around its typical value E[Y] = m . Since the MMSE is
a
bounded, we therefore have as d → ∞ that MMSE = κE (tr[(S⋆−SˆBO)2]|Y = m ).
S⋆,{eyi} a
(ii) As we will see in what follows (and exactly like in the case of fixed second layer), the
leading order of the MMSE of the inference problem of eq. (100) only depends on the
asymptotic spectral distribution of S⋆. In particular, at leading order:
MMSE = κE (tr[(S⋆−SˆBO)2]|Y = m ),
S⋆,{eyi} a
= κE (tr[(S⋆−SˆBO)2]|tr(S⋆) = m ),
S⋆,{eyi} a
( =a) κE (tr[(S⋆−SˆBO)2]), (101)
S⋆,{eyi}
where in (a) we used that conditioning on tr(S⋆) = m does not change the asymptotic
a
spectral distribution of S⋆.
All in all, we focus on characterizing the MMSE given in eq. (101), for the inference problem of
recovering S⋆ from the knowledge of {Z ,y } generated by eq. (100).
i i
E.1.2 Further steps of the derivation
Here, we notice that the arguments detailed in Section 4 on how to obtain an asymptotic
expression of eq. (101) do not depend on the specific asymptotic spectral distribution of S⋆.
More precisely:
A. Conjecture 4.1 can be directly extended to more general distributions of S⋆ than the
Wishart distribution. Indeed, the heuristic argument explaining this universality phe-
nomenon does not depend on the distribution of S⋆, and on a technical level, as mentioned
inthemaintext, Conjecture4.1isanextensionofCorollary4.10ofMaillardandBandeira
[2023], which holds for generic choices of distributions of matrices.
B. Conjecture 4.2 is also straightforwardly extended here, simply replacing the Wishart prior
by the more generic prior of eq. (96). More generally, we expect it to hold for any prior
such that the function Ψ(qˆ) of eq. (20) is well-defined [Aubin et al., 2019a, 2020].
C. Finally, the proof of Theorem 4.3 (see Appendix D.2) relies solely on the rotation invari-
ance of the distribution of S⋆, as well as the fact that S⋆ admits a compactly supported
asymptotic eigenvalue distribution. These two facts hold for the distribution of eq. (96)
for compactly supported P , see e.g. Silverstein and Choi [1995], Lee and Schnelli [2016].
a
E.2 Conclusion: Claim 2 when learning the second layer
WearenowreadytostatethegeneralizationofClaim2toalearnablesecondlayer. Theeffective
problem we consider is the recovery of a symmetric matrix S⋆ ∈ Rd×d, which was generated as
S⋆ = (1/m)Pm a⋆w⋆(w⋆)⊤, from observations (y )n , generated as
k=1 k k k i i=1
y ∼ P (·|Tr[Z S⋆]), (102)
i out i
43√
with Z := (x x⊤−I )/ d and x i. ∼i.d. N(0,I ).
i i i d i d
The asymptotic spectral distribution µ⋆ of S⋆ is called a generalized Marchenko-Pastur distri-
bution (or a free compound Poisson distribution: it is also the free multiplicative convolution
of the Marchenko-Pastur law and P , see Anderson et al. [2010]). µ⋆ is compactly supported,
a
and can be characterized by its R transform [Marchenko and Pastur, 1967, Silverstein and Choi,
1995, Tulino and Verdú, 2004]:
Z κa
R (s) = P (a)da. (103)
µ⋆ a
κ−sa
Eq. (103) allows for an efficient numerical evaluation of µ⋆ given P . Notice that E [X] = m ,
a µ⋆ a
and E [X2] = m2 +c /κ.
µ⋆ a a
The partition function for the learning problem of eq. (102) is again defined as:
n
Z({y ,x }n ) := E Y P (y |Tr[SZ ]). (104)
i i i=1 S out i i
i=1
We then obtain the following generalization of Claim 2.
Claim 3. Assume that m = κd with κ > 0, and n = αd2 with α > 0. Recall that m := E [a]
a Pa
and c := E [a2]. Let Q := E [X2] = m2 +c /κ. Then:
a Pa 0 µ⋆ a a
• The limit of the averaged log-partition function of eq. (104) is given by
1 (cid:20) Z (cid:21)
lim E logZ = sup I(q)+α dyDξJ (y,ξ)logJ (y,ξ) , (105)
d→∞ d2 {yi,xi} q∈[m2 a,Q0] R×R q q
where
 (cid:20)(Q −q)qˆ 1 1 1(cid:21)
I(q) := qˆin ≥f
0
0
4
− 2Σ(µ 1/qˆ)−
4
logqˆ−
8
,
( √ ) (106)
Z dz (z− 2qξ)2
J
q(y,ξ) :=
p
4π(Q −q)
exp −
4(Q −q)
P out(y|z).
0 0
Here, Σ(µ) := E log|X−Y|, and, for t ≥ 0, µ := µ⋆⊞σ √ is the free convolution of
X,Y∼µ t s.c., t
µ⋆ and a (scaled) semicircle law (see Appendix A).
• For any α > 0, except possibly in a countable set, the supremum in eq. (105) is reached
in a unique q⋆ ∈ [m2,Q ]. Moreover, the asymptotic minimum mean-squared error on the
a 0
estimation of S⋆, achieved by the Bayes-optimal estimator SˆBO := E[S|{y ,x }], is equal to
i i
Q −q⋆:
0
lim Etr[(S⋆−SˆBO)2] = Q −q⋆. (107)
0
d→∞
It is related to the MMSE of eq. (97) by MMSE = κ(Q −q⋆).
0
Therefore, generalizingSectionD.7, Result1holdsaswellinthiscase, with∆e = 2∆(2+c a∆)/κ,
and µ := µ⋆⊞σ √ , where µ⋆ is characterized by eq. (103).
t s.c., t
E.3 The GAMP-RIE algorithm
Finally, one can also generalize Algorithm 1 to this setting: the only change to perform is
to adapt the functions F and f . Indeed, instead of denoising a Wishart matrix (with
RIE RIE
an asymptotic spectrum given by the Marchenko-Pastur distribution), here one must denoise
a matrix S with asymptotic spectral distribution given by µ⋆ defined in Appendix E.2. As
0
mentioned, eq. (103) allows for an efficient numerical evaluation of µ⋆ given P . From there, one
a
can adapt Algorithm 1 to this case simply by replacing in the definitions of F and f the
RIE RIE
distribution ρ by ρ = µ⋆⊞σ √ .
∆ ∆ s.c., ∆
44F Details on the numerics
3.0 MMSE
λ=0
0.35
λ=0.001
2.5
λ=0.005
λ=0.01
2.0 λ=0.05 0.30
λ=0.1
1.5 λ=0.25
λ=0.5
0.25
1.0
0.5 0.20
0.0
0.0 0.1 0.2 0.3 0 1 2
Sample complexity α=n/d2 Noise level ∆
Figure 4: Left: Mean squared error as a function of the sample complexity α, for κ = 1/2
and ∆ = 0.252. Dots are simulations using GD with a single initialization averaged over 32
realizations of the dataset, crosses are averages over 64 initializations. The continuous line is
the asymptotic MMSE given by (8). The colors indicate the strength of the regularization.
Right: Trivialization threshold in the sample complexity α as a function of the noise level ∆
T
in the teacher without regularization, λ = 0. The measurement has a resolution of 0.1 on the
noise level and of 0.007 on the sample complexity
F.1 Solutions to the “state evolution” equations
We describe here how to solve eqs. (8),(9). The first step to solve is to obtain an analytical
expression for µ . We refer to Appendix A for the definition of quantities used in this section.
t
We recall that µ := µ ⊞σ √ is the free convolution of the Marchenko-Pastur law and a
t MP,κ s.c., t
scaled semicircular density. The R-transform of the scaled semicircle distribution is [Tulino and
Verdú, 2004]:
R (z) = zt,
σ √
s.c., t
while for the Marchenko-Pastur law we have
κ
R (z) = .
µMP,κ
κ−z
We can now use (cf. Appendix A):
κ
R µt(z) = R :=µMP,κ⊞σ s.c.,√
t
= R σ s.c.,√ t(z)+R µMP,κ(z) = zt+ κ−z.
The Stieltjes transform g(z) = E [1/(X −z)] of µ is the solution of the equation
µt t
1
z+ = R (−g(z)),
g(z)
µt
or equivalently
κ 1
z = −tg(z)+ − . (108)
κ+g(z) g(z)
45
ESM
α
dlohserht
noitasilaivirT
TAmong all the solutions to this equation, g(z) must be such that Im[g(z)] > 0 if Im(z) > 0,
and also satisfies g(z) ∼ 1/z for z → ∞. Eq. (108) is a third degree polynomial in g(z), and
can easily be solved by algebraic solvers, and has a single solution satisfying the constraints we
described. Finally, µ (x) is given by the Stieltjes-Perron inversion theorem (see Appendix A):
t
Im[g(x+iε)]
µ (x) = lim , (109)
t
ε→0 π
and we numerically choose ε = 10−8. We now discuss the computation of the integral of µ (x)3
t
in (9). Notice that the integrand is only non-zero over at most two finite intervals. Exact values
of the edges are given by setting the discriminant of equation (108) to zero. The last step is
findingasolutioninqˆtoequation (9). Wefindthefunction“root”inScipy, whichusesavariant
of the Powell hybrid method, to be performing quite well when initialized in the value 2α/Q .
0
This whole procedure is quite efficient and can be reproduced easily on any machine.
F.2 Gradient descent
In our experiments with gradient descent we are minimizing the objective R(W):
R(W) :=
1 Xn
(y −f (x ))2+
λ Xm Xd
w2 . (110)
4 i W i 2 kl
i=1 k=1l=1
All the simulations are done in PyTorch with the student weights initialized in the prior. For
“vanilla” gradient descent we iterate until convergence, and average over several repetitions. For
averaged gradient descent (AGD) we first generate the dataset, then train the student several
times with starting weights independently sampled in the prior, and "average the weights" at
the end of training. By this we mean that for each run we train until convergence, then obtain
the matrix S and average it. Finally, we average this procedure over several repetitions.
In Figure 3 the gradient descent is run for zero regularization, λ = 0. In Figure 4 (left) we
then study the effect of regularization to check whether regularization helps to achieve the
Bayes-optimal error, but conclude that it does not and in fact it hurts the performance. In
Figure 4 (right) we study the effect of the noise on the landscape of GD. We will expand on
this in Appendix F.3. All the error bars reported in Figure 3 and Figure 4 (left) are standard
deviationsoftheMSEmeasuredonthesamples. Figure4(right)hasafiniteresolutionindicated
in the caption. A single run of vanilla GD for the models we display can be completed in at
most 30 minutes on an average machine without using GPUs. For producing our figures we used
around 30000 hours of computing time.
F.3 Additional experiments with GD
Here we study in more detail the phenomenology observed in Figure 3 (right) where in the
presence of noise and at a large sample complexity all the runs of GD seem to converge to the
sameprediction. Inthefigurewenoticedthatabovecertainsamplecomplexitytheaveragedand
non-averaged GD errors are identical. This suggests that GD will eventually lead the weights
of the network to the same configuration up to the symmetries of the problem independently of
the initial state. We call this a trivialization of the landscape.
In Figure 4 (right) we study the trivialization threshold as a function of the noise level ∆. One
needs to take care of the symmetries on Wˆ , so we first define Sˆ:
1 (cid:16) (cid:17)⊤
Sˆ(W(0),D) := Wˆ (W(0),D) Wˆ (W(0),D),
m
46where we mean that for a fixed dataset we run GD, then take a matrix product to obtain S.
This procedure allows us to define the dispersion
(cid:20) (cid:16) h i (cid:17)2(cid:21)
δ := E tr E Sˆ(W(0),D) −Sˆ(W(0),D) .
GD D W(0)
If the dispersion becomes zero it means that all the runs will converge to the same value. As
we increase the sample complexity α the dispersion decreases, until it becomes zero. For each
value of the noise level ∆ we indicate the minimum sample complexity for which the dispersion
is either less than 10−2, or less than 10−3 of the maximum dispersion at fixed ∆.
In Figure 4 (left), where we studied the effect of ℓ regularization on the weights, we can also
2
see how even a relatively small λ > 0 regularization leads to a trivialization of the landscape
again in the sense that different initializations of GD provide the same prediction and averaging
does not lead to a better error.
47