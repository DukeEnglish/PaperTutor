Knowledge Probing for Graph Representation Learning
MingyuZhao∗ XingyuHuang∗ ZiyuLyu†
my.zhao1@siat.ac.cn xingyuhuang1998@outlook.com luziyucrystal@163.com
SchoolofCyberScienceand SchoolofCyberScienceand SchoolofCyberScienceand
Technology,ShenzhenCampusofSun Technology,ShenzhenCampusofSun Technology,SunYat-senUniversity
Yat-senUniversity.ShenzhenInstitute Yat-senUniversity.ShenzhenInstitute Shenzhen,Guangdong,China
ofAdvancedTechnology,Chinese ofAdvancedTechnology,Chinese
AcademyofSciences,Universityof AcademyofSciences.
ChineseAcademyofSciences. Shenzhen,Guangdong,China
Shenzhen,Guangdong,China
YanlinWang LixinCui LuBai
yanlin-wang@outlook.com cuilixin@cufe.edu.cn bailu@bnu.edu.cn
SunYat-senUniversity CentralUniversityofFinanceand BeijingNormalUniversity
Guangzhou,Guangdong,China Economics Beijing,China
Beijing,China
ABSTRACT KEYWORDS
Graphlearningmethodshavebeenextensivelyappliedindiverse GraphRepresentationLearning,KnowledgeProbing,Evaluation
applicationareas.However,whatkindofinherentgraphproper-
tiese.g.graphproximity,graphstructuralinformationhasbeen ACMReferenceFormat:
MingyuZhao,XingyuHuang,ZiyuLyu,YanlinWang,LixinCui,andLuBai.
encodedintographrepresentationlearningfordownstreamtasks
2018.KnowledgeProbingforGraphRepresentationLearning.InProceedings
isstillunder-explored.Inthispaper,weproposeanovelgraphprob-
ofMakesuretoenterthecorrectconferencetitlefromyourrightsconfirmation
ingframework(GraphProbe)toinvestigateandinterpretwhether
emai(Conferenceacronym’XX).ACM,NewYork,NY,USA,11pages.https:
thefamilyofgraphlearningmethodshasencodeddifferentlevels
//doi.org/XXXXXXX.XXXXXXX
ofknowledgeingraphrepresentationlearning.Basedontheintrin-
sicpropertiesofgraphs,wedesignthreeprobestosystematically
investigatethegraphrepresentationlearningprocessfromdiffer- 1 INTRODUCTION
entperspectives,respectivelythenode-wiselevel,thepath-wise Graphsareaprevalentdatastructureandhavebeenbroadlyin
level,andthestructurallevel.Weconstructathoroughevaluation multiplefields[16].Forexample,socialnetworks[13,23],molecular
benchmarkwithninerepresentativegraphlearningmethodsfrom graphstructures,andbiologicalproteinnetworksareuniversally
randomwalkbasedapproaches,basicgraphneuralnetworksand modeledasgraphs[5].Inrecentdecades,alotofgraphrepresen-
self-supervisedgraphmethods,andprobethemonsixbenchmark tationslearningmethodshavebeendevised,rangingfrommatrix
datasetsfornodeclassification,linkpredictionandgraphclassifi- factorizationmethods[1,6],random-walkbasedalgorithms[14,24],
cation.TheexperimentalevaluationverifythatGraphProbecan tothepopularfamilyofgraphneuralnetworks(GNN)[10,15,18,
estimatethecapabilityofgraphrepresentationlearning.Remaking 22,33,35,39].Thegraphrepresentationlearningmethodshave
resultshavebeenconcluded:GCNandWeightedGCNmethodsare demonstrateddifferentperformanceontheclassicaldownstream
relativelyversatilemethodsachievingbetterresultswithrespect tasks,e.g.nodeclassification,linkpredictionandgraphclassifica-
todifferenttasks. tion.Andthediversegraphrepresentationlearningmethodshave
beenextensivelyappliedinmultipleapplicationareas,e.g.social
CCSCONCEPTS
networkanalysis[13,23],recommendersystem[17,18,29],and
•Informationsystems;•Computingmethodologies→Knowl- proteinclassification[5,9].
edgerepresentationandreasoning; Thosegraphrepresentationlearningmethodstendtolearna
mappingwhichembednodesor(sub)graphsintoalow-dimensional
∗Bothauthorscontributedequallytothisresearch.
†Correspondingauthor. vectorsbyencodingrelationalinformationandstructuralinforma-
tion,andthelearnedembeddingsareusedforfurtherdownstream
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor
tasks[27].However,thereisnostudytoinvestigateandexplain
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation whatkindsofgraphpropertieshavebeenactuallycodedinthe
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe learnedembeddingthroughdifferentgraphrepresentationlearning
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
methods.Itlacksasystematicalevaluationtoprobewhetherthe
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. graphinherentproperties(e.g.graphproximity,graphstructural
Conferenceacronym’XX,June03–05,2018,Woodstock,NY information)areencodedintothelearnednodeandgraphrepre-
©2018Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM.
sentationswiththepopularbutblack-boxgraphrepresentation
ACMISBN978-x-xxxx-xxxx-x/YY/MM
https://doi.org/XXXXXXX.XXXXXXX learningmethods.
4202
guA
7
]GL.sc[
1v77830.8042:viXraConferenceacronym’XX,June03–05,2018,Woodstock,NY MingyuZhao,XingyuHuang,ZiyuLyu,YanlinWang,LixinCui,andLuBai
Inthispaper,wedeviseasystematicgraphprobingbenchmark approximatewithnegativesampling.However,matrixfactorization
(GraphProbe)toinvestigatewhattypesofknowledgeareencoded andrandomwalkmethodsareshallowembeddingapproaches[16]
intographrepresentationlearningfor9representativegraphlearn- andtheycannotcapturestructuralsimilarity.Inrecentdecades,
ingmethodsfromdiversecategoriesincludingrandomwalkbased graphneuralnetworkshavebeenextensivelyproposedforgraph
methods,classicalgraphneuralnetworkslikegraphconvolution representationlearning[16].Thefamilyofgraphneuralnetworks
networks(GCN)andgraphattentionnetworks(GAT),unsupervised relyontheneighborhoodaggregationstrategy,andsolvethemain
graphlearningmethodsandweightedgraphlearningmethods.We limitationsofthedirectencodingmethodslikematrixfactorization
devisethreetypesofprobesatthreedifferentlevels,respectively andrandomwalkapproaches.Forexample,Kipfetal.[22]pro-
thenode-wise,path-wiseandstructure-wiselevels.First,the posedascalableapproach(GraphconvolutionNetwork,GCN)for
node-wiseprobeisproposedtoinvestigatewhetherthenode-wise semi-supervisedlearningongraph-structureddata,byutilizingan
influencesareencodedingraphrepresentationlearning,andtwo efficientlayer-wisepropagationrulethatisbasedonafirst-order
intrinsicnodecentralitymetrics(e.g.eigenvectorcentralityand approximationofspectralconvolutionsongraphs.GraphSAGE[15]
betweennesscentrality)areadoptedtomeasurenode-wiseinflu- wasageneralinductivegraphrepresentationlearningframework,
ences.Second,adistanceprobeisdesignedtoexplorewhetherthe andlearnedafunctionthatgeneratesembeddingsbysamplingand
path-wiseinformation(distancebetweentwonodes)canbeem- leveragednodefeatureinformationtoefficientlygeneratenode
beddedintorepresentationlearningofnodes,basedontheshortest embeddingsforpreviouslyunseendata.Inaddition,someunsuper-
pathsofapairofnodes.Third,weleverageWeisfeiler-Lehman visedgraphlearningframeworkshavebeendevised.Forexample,
kernelalgorithm[31]asstructuralevaluationmetricsanddevisea VariationalGraphAuto-Encoders(VGAE)proposedanunsuper-
parameter-freestructuralprobetointerpretwhetherthestructural visedlearningframeworkbasedonthevariationalauto-encoder.
informationisenoverviecodedintothelearnedgraphrepresenta- Basedontherecentcontrastivelearningtechniques,Youetal.[36]
tions.Weconductextensiveexperimentstoprobetheperformance proposedagraphcontrastivelearningframeworkforunsupervised
ofgraphrepresentationlearningon6benchmarkdatasetsfrom representationlearningofgraphdataandexploredfourtypesof
diversedomainsforthreetraditionaldownstreamtasks.Oursys- graphaugmentationstoincorporatevariouspriors.Althoughex-
tematicevaluationhasconcludedsomeremarkedresults.Themain tensivegraphrepresentationlearningmethodshavebeenproposed
contributionsofthispaperaresummarizedasfollows: andworkedonthetraditionaldownstreamtasks,nostudieshave
• Toourbestknowledge,ourproposedmethodisthefirsttime investigatedwhethertheinherentgraphstructuralandtopological
informationhasbeenencodedintographrepresentationlearning.
toexploreknowledge probingon varioustypesof graph
Inthispaper,weproposeagraphknowledgeprobingframeworkto
learning models across all classical downstream tasks. A
probethelatentknowledgewithingraphrepresentationlearning,
systematicgraphprobingframeworkisnovellyintroduced,
andanswerwhatkindofgraphinformationhavebeenencoded
in which three types of probes based on graph intrinsic
whenperformingdownstreamtasks.
propertiesaredevisedrespectivelyfromthenode-wise,path-
wiseandstructure-wiselevels.
• Anevaluationbenchmarkforgraphrepresentationlearning
2.2 KnowledgeProbe
onnodeclassification,linkpredictionandgraphclassifica-
Recently,knowledgeprobeshavebeenproposedtoprobeknowl-
tion,broadlyincludingninerepresentativegraphlearning
edgeinpre-trainedlanguagemodels(PLMs)suchasELMo[25]
modelsfromfourdifferentcategoriesandsixbenchmarks
andBERT[11].Probingmethodsaredesignedtounderstandand
datasetsfromthreedomainsincludingcitationnetworks,
interpretwhatknowledgehavebeenlearnedinthepre-trainedlan-
socialnetworksandBio-chemicalnetworks.
• Ourexperimentalresultsconcluderemarkedfindings.Espe- guagemodels,andtheyprobespecificknowledgeincludinglinguis-
ticknowledgeConneauetal.[7],HewittandManning[19],Hou
cially,ourdevisedknowledgeprobesareverifiedtoreflect
andSachan[20],Shietal.[32],andfactualknowledgePetronietal.
thecapabilityofgraphrepresentationlearning,andhave
[26].Forexample,Hewittetal.[19]proposedastructuralprobeto
competitiveandconsistentresultswiththetraditionaleval-
evaluatewhethersyntaxtreeshavebeenencodedinalineartrans-
uationmetricssuchasaccuracy,AUCandF1scores.
formationofaneuralnetwork’swordrepresentationspace.The
2 RELATEDWORK probingresultsdemonstratedthatthetransformsexistforthetwo
PLMsELMoandBERT.Petronietal.proposedaLAMAbenchmark
2.1 GraphRepresentationLearning
toprobefactualknowledgeinPLMsusingpromt-basedretrieval.
GraphrepresentationLearningmethodsmapstructuralgraphdata Themostsimilarworkis[2]inwhichaprobingframeworkhas
intolow-dimensionaldensevectors,bycapturingthegraphtopol- beenproposedforquantifythechemicalknowledgeandmolec-
ogystructure,node-to-noderelationshipsandotherrelevantinfor- ularpropertiesingraphrepresentationsforgraphbasedneural
mation.Earlymethodsforlearningrepresentationsfornodeson networks.Differentfrompreviousstudies,weproposeaholistic
graph-structureddataweremainlymatrixfactorizationmethods graphprobingbenchmarktounderstandandinterpretwhether
basedondimensionreduction[1,6],andrandomwalkapproaches differenttypesofinherentgraphpropertieshavebeenencodedinto
basedonrandomwalkstatistics(e.g.DeepWalk[24]andNode2Vec graphrepresentationlearning,fromthenode-wise,path-wiseand
[14]).Forexample,DeepWalk[24]wasthefirsttoinputrandom structural-wiselevels.Thestudiedgraphmodelscoveringabroad
walkpathsintoaskip-grammodeltolearngraphnodeembeddings. rangeofgraphlearningmethods,rangingfromrandomwalkbased
Node2vec[14]combinedbothbreadth-firstanddepth-firstwalksto methodstographneuralnetworks.Inaddition,webenchmarkourKnowledgeProbingforGraphRepresentationLearning Conferenceacronym’XX,June03–05,2018,Woodstock,NY
knowledgeprobesonthreeclassicaldownstreamtaskswithnine Wedeviseasupervisedprobetocomparethenodecentrality
representativegraphlearningmethods. oftwodifferentnodes𝑣 𝑖 and𝑣 𝑗.Wecalculatethenodecentrality
values𝐶(𝑣 𝑖)and𝐶(𝑣 𝑗)basedonsomegraphnodecentralitymetrics
3 THEGRAPHEMBEDDINGPROBES (e.g.eigenvectorcentralityandbetweenesscentrality),andmap
thepair-wisecentralitycomparisonintoabinaryvalue𝑙 𝑖𝑗 asin
Inordertoinvestigatewhethertheinherentgraphpropertieshave
Equation2.Thebinaryvalue𝑙isusedasthecentralitylabeltotrain
beenencodedintographrepresentationlearningandrevealwhy
thenodecentralityprobe𝑃𝑐(h𝑘 ,𝑙,𝑇).
differentgraphlearningmethodshavedifferentperformanceon 1:|𝑉|
downstream tasks, we we propose a knowledge probing frame- (cid:26)1 𝐶(𝑣 𝑖) ≥𝐶(𝑣 𝑗)
work(GraphProbe)toprobegraphrepresentationlearning,and 𝑙 𝑖𝑗 = 0 𝐶(𝑣 𝑖) <𝐶(𝑣 𝑗). (2)
devisethreeknowledgeprobesfromdifferentlevels,respectively
node-wise,path-wise,andstructure-wiselevels.Figure1shows FollowingtheprobearchitectureinPimenteletal.[27]whichhas
the overall architecture of our proposed GraphProbe. From the beendesignedtoreducetheinformationloss,weadoptasimple
node-wiselevel,wedevisethenodecentralityprobestoinvestigate
learningnetworktwo-layerperception(MLP)1tolearnthesuper-
whethertheinfluencesofnodepropertiesandthelocalneighbour- visedprobe:𝑃 𝑐𝑘(h𝑘 𝑖,h𝑘 𝑗)=𝑀𝐿𝑃(h𝑘 𝑖 ∥h𝑘 𝑗),andoutputtheprobability
hoodinformationcanbeencodedintorepresentationlearningof 𝑝 thatthepreviousnode𝑣 𝑖 hasalargercentralitythannode𝑣 𝑗.
nodesfordownstreamtaskssuchasnodeclassificationandlink Cross-entropyisusedasthelossfunctiontotrainthesupervised
prediction.Fromthepath-wiselevel,weleveragethedistancemet- probe.Finally,theprobescoresofthenodecentralityprobe𝑆 𝑐𝑘
ricsofpathsbetweentwonodestoexplorewhetherthepath-wise formodel𝑀𝑘 aretheevaluationmeasurescorestomeasurethe
topologicalinformationcanbeencodedintographrepresentation predictionfromtheprobebasedonthelearnedrepresentationsof
learning.Fromthestructure-wiselevel,agraphstructuralprobe themodel𝑀𝑘 asfollows:
i ss ubd -e gv ri as pe hd it no foin rmve as tt ii og nat ie sew nh coet dh ee dr inth toe gst rr au pc htu rera pl rein sef nor tam tia ot nio ln eae r. ng -. 𝑆 𝑐𝑘 =𝐸𝑣𝑎𝑙(𝑃 𝑐𝑘 (h𝑘 𝑖,h𝑘 𝑗),𝑙 𝑖𝑗). (3)
ingwhenperformingtheclassicalgraphclassificationtask.Inthe Classicalevaluationmetricscanbeusedfor𝐸𝑣𝑎𝑙(e.g.F1-score,AUC,
followingparts,wefirstlyhaveaformalproblemdefinitionfor Accuracy).WereportresultswithAccuracyandF1-score[28]in
theknowledgeprobingongraphrepresentationleaningandthen experimentsection.Ahigherprobescore𝑆 𝑐𝑘 meansthegraph-based
illustratedifferentprobesindetails.
model𝑀𝑘
hasgreaterabilitytoencodecentralityinformationinto
thenoderepresentations.Weexploretwonodecentralitymetrics
ProblemDefinition. Wegiveaformaldefinitionfortheknowl- for𝐶(·),respectivelyeigenvectorcentrality[4]andbetweeness
edgeprobingproblemongraphrepresentationlearning.Giventhe
centrality[30].
constructedgraphdata𝐺 ={𝑉,𝐸},𝑉 isthesetofnodesin𝐺,and
𝐸isthesetoftheedges.h𝑖 representsthefeaturerepresentation EigenvectorCentrality. Eigenvectorcentrality[4]measuresthe
ofeachnode𝑣 𝑖,andthedimensionofthefeaturerepresentation importance of nodes in a network by exploiting adjacency and
is𝑑.ThenoderepresentationXcanberandomlyinitializedorini- eigenvectormatrices.Eigenvectorcentralityisauniquemeasure
tializedwithmetafeatures.Withagraph-basedmodel𝑀𝑘 ,wecan thatsatisfiescertainnaturalprinciplesforarankingalgorithm[3].
obtainthelearnednodefeaturerepresentationh𝑘 =𝑀(𝐺,X,𝜃𝑘). AndWangetal.[34]showthatseveralrecommendationalgorithms
Thegraphprobe𝑃 isafunctiontoestimatew1 h:| e𝑉 th| erthelearned b tia os ned ofon eign eo nd ve ei cm top ror ct ea nn tc re alh ita yv .e Abe ∈en R𝑛e ×n 𝑛ha in sc te hd ew adit jh act eh ne ci ynt mro ad tru ic x-
representations encode the specified properties I, as defined in
suchthat𝑎 𝑖𝑗 = 1ifnode𝑖 isconnectedtonode 𝑗 and𝑎 𝑖𝑗 = 0if
Equation1.
not.Theformaldefinitionoftheeigenvalue𝜆andtheeigenvector
𝑆𝑘 =𝑃(h𝑘 1:|𝑉|,𝐼,𝑇). (1) xis𝐴x =𝜆xAndtheprincipaleigenvectorx𝑝 = (𝑥 1𝑝 ,···,𝑥 𝑛𝑝 )is
theeigenvectorcorrespondingtotheeigenvaluewiththelargest
inwhich𝐼 denotestheinvestigatedmetricsusedinthedevised
modulus.Theeigenvectorcentralityofnode𝑖canbecomputedas
probes,and𝑇 denotestheapplieddownstreamtask.Theprobe
score𝑆𝑘
estimateshowwellthelearnedrepresentationsfromthe
inEquation4:
graph-basedmodel𝑀𝑘 encodeinformationforthedownstream 𝐸𝐶(𝑖)=𝑥 𝑖𝑝 ,𝐸𝐶(𝑛 𝑖)= 𝜆1 ∑︁ 𝐸𝐶(𝑛 𝑗),𝑥 𝑖 = 𝜆1 𝐴𝑇 𝑖,𝑗𝑥 𝑗. (4)
task𝑇 withrespectto𝐼.Inthefollowingsections,wewilldescribe 𝑛𝑗∈𝑁(𝑛𝑖)
thedifferentprobesindetails. where𝐸𝐶(𝑛 𝑖),𝐸𝐶(𝑛 𝑗)istheamountofinfluencethatnode𝑛 𝑖,𝑛
𝑗
carries,𝑁(𝑛 𝑖)isthesetofdirectneighborsofnode𝑛 𝑖,and𝜆isa
3.1 TheNodeCentralityProbe
constant.
Fromthenode-wiseperspective,weleveragethenodecentrality
propertiesofgraphdataastheestimatedmetrics𝐼 becausethe BetweennessCentrality. Ingraphtheory,betweennesscentrality[12]
isameasureofcentralityinagraph.Foreverypairofverticesina
nodecentralityreflectstheinfluenceandimportanceofanode
connectedgraph,thereexistsatleastoneshortestpathbetweenthe
andtotheextentcapturestheneighborhoodinformationofanode
verticessuchthateitherthenumberofedgesthatthepathpasses
[8].Wedeviseanodecentralityprobetoinvestigatewhetherthe
through(forunweightedgraphs).Thebetweennesscentralityfor
learnedrepresentationshaveencodedthenodecentralitywhen
eachvertexisthenumberoftheseshortestpathsthatpassthrough
performingclassicaldownstreamtasks,e.g,nodeclassificationand
linkprediction. 1Weuseatwo-layerMLPasthelearningnetwork,andtheactivationfunctionisReLU.Conferenceacronym’XX,June03–05,2018,Woodstock,NY MingyuZhao,XingyuHuang,ZiyuLyu,YanlinWang,LixinCui,andLuBai
Figure1:TheoverallarchitectureofourproposedGraphProbe.
thevertex.Itappliestoawiderangeofproblemsinnetworktheory, pairsinthetraininggraph.Specifically,weapproximatethrough
includingproblemsrelatedtosocialnetworks,biology,transport gradientdescent:
andscientificcooperation[12].Thebetweenesscentralityisdefined ∑︁ (cid:12) (cid:12)
asinEquation5:
𝑚 𝐵𝑖𝑛 (cid:12) (cid:12)𝑑 𝐺(𝑛 𝑖,𝑛 𝑗)−𝑑 𝐵(h𝑖,h𝑗)2(cid:12) (cid:12). (7)
𝑖,𝑗∈𝐺
∑︁ 𝜎 𝑗𝑡(𝑣 𝑖)
𝐵𝐶(𝑣 𝑖)= 𝜎 . (5) where𝑑 𝐺(𝑛 𝑖,𝑛 𝑗)denotesthedistanceoftheshortestpathoftwo
𝑗𝑡
𝑣𝑖≠𝑣𝑗≠𝑣𝑡∈𝑉 nodes𝑣 𝑖,𝑣
𝑗
in𝐺2.
𝜎 𝑗𝑡 isnumberoftheshortestpathsbetweennode𝑣 𝑗 and𝑣 𝑡,and
𝜎 𝑗𝑡(𝑣 𝑖)isthenumberofshortestpathpassingthroughnode𝑣 𝑖. 𝑆 𝑑𝑘 = (cid:12) 1 (cid:12). (8)
(cid:205) 𝑖,𝑗∈𝐺(cid:12) (cid:12)𝑑 𝐺(𝑛 𝑖,𝑛 𝑗)−𝑑 𝐵(h𝑘 𝑖,h𝑘 𝑗)2(cid:12)
(cid:12)
3.2 TheDistanceProbe
Theprobescore𝑆𝑘
representstheperformanceoftheprobeto
Ingraphdata,thedistancebetweentwonodescanbeestimated 𝑑
bytheshortestpath.Fromthepath-wiseperspective,wedevise recreatethegraphdistance.Therefore,Bigger𝑆 𝑑𝑘 indicatesbetter
thedistanceprobetoinvestigatewhetherthenoderepresentations performance.
encodethepath-leveldistanceinformationofgraphstructure.Fol-
lowingHewittandManning[19],wedevisethedistanceprobe𝑃
𝑑
3.3 TheGraphStructuralProbe
astoestimatethedifferencesbetweenthegroundedshortestpaths Weproposeastructuralprobetoestimatewhetherthestructural
oftwonodesandthevectordistanceoftwonodes’representations. informationhasbeenencodedintotheembeddingoftheentire
Firstly,wedefineafamilyofinnerproducts,h𝑇𝑊hparameterized graph.ThegraphrepresentationH𝑘
isconstructedbyaggregating
byanypositivesemi-definite,thesymmetricmatrix𝑊 ∈ 𝑆𝑚 +×𝑚 . thenoderepresentationsthroughthereadoutoperation:
E suq cu hiv ta hle an tt 𝑊ly, =we 𝐵c 𝑇a 𝐵n .v Tie hw eit nh nis ea rs pa roli dn uea cr tt hr 𝑇an 𝑊sf hor im sta hti eo nn e𝐵 qu∈ ivR a𝑘 l× e𝑚 nt, H𝑘 =𝑟𝑒𝑎𝑑𝑜𝑢𝑡(𝐺,h𝑘 1:|𝑉|). (9)
to(𝐵h)𝑇(𝐵h),thenormofhoncetransformedby𝐵.Everyinner
Thereadoutoperationcanobtaingraph-levelrepresentation,e.g.
productcorrespondstoadistancemetric.Therefore,thedefinition sum, mean and max pooling3. We report the results with sum
ofthedistancebetweentwonodes’embeddingh𝑘 𝑖,h𝑘
𝑗 is: operation.
Inordertoextracttheinherentstructuralinformationofgraphs,
𝑑 𝐵(h𝑘 𝑖,h𝑘 𝑗)2=(𝐵(h𝑘 𝑖 −h𝑘 𝑗))𝑇 (𝐵(h𝑘 𝑖 −h𝑘 𝑗)). (6) weusetheWeisfeiler-Lehman(WL)isomorphismtest[31]asevalu-
Thedistanceprobe𝑃 𝑑 istrainedtorecreatethegraphdistanceof ationmetricstomeasurethesimilaritybetweengraphstructures.
nodepairsinthetraininggraph,andoptimizedthroughgradient
2Consideringthecomputationalcomplexity,weonlykeepthenodepairswithdistance
descentinEquation7.Theparametersofourprobeareexactlythe
lessthanorequalto3.
matrix𝐵,whichwetraintorecreatethegraphdistanceofnode 3ThethreeoperationshavebeenimplementedinthebenchmarkKnowledgeProbingforGraphRepresentationLearning Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Table1:Benchmarkdatasets.TheClassesindicatethenode 4 EXPERIMENT
classnumberfortheCoraandFlickrdataset,andgraphclass
4.1 RepresentativeGraphLearningMethods
numberforENZYMESandMUTAG.
Inordertoevaluateourprobingmethodsondifferentcategories,we
Dataset Graphs Classes Nodes Edges NodeFeatures usesomerepresentativegraphlearningmethodsforexperimental
Citation evaluationandreporttheirresultsinSection55.Ingeneral,we
Cora 1 7 2,708 5,429 1,433
networks selectgraphlearningmethodsfrom4categories,includingrandom
Flickr 1 7 89,250 899,756 500
Social walk based graph embedding methods (e.g. Node2Vec [14] and
Yelp 1 - 69,716 1,561,406 -
networks MovieLens 1 - 10,352 100,836 404 DeepWalk[24]),basicgraphneuralnetworks(e.g.GCN[22]and
Bio- ENZYMES 600 6 33 124 21 GAT[33]),self-supervisedgraphlearningmethods(e.g.GCL[36]
networks MUTAG 188 2 18 20 7
andVGAE[21])andweightedgraphlearningmethods(WGCN[38]).
Inaddition,weaddthecontroltask,i.e.anaivetwo-layerMLP
methodtoavoidpotentialperformancebiasduetothelearning
performanceofourprobes6.
TheWLsubtreekernelalgorithmcollectstheinformationofneigh-
• DeepWalk[24]:isoneofrandomwalkbasedgraphembed-
bornodes,andusehashtoaggregatethemtogeneratethelabelof
dingmethod.Itadoptsthelocalinformationobtainedfrom
thenodeinoneiteration.
truncatedrandomwalkstolearnthelatentrepresentation
Wedeviseastructuralprobetoexplorewhetherthesegraph
ofnodesviaskip-Gramwithhierarchicalsoftmax.
structuralinformationarepreservedinthegraphembeddingrep-
𝑘 • Node2Vec[14]:isalsoaclassicgraphembeddingmethod.
resentationH .Thebottom-rightfigure(c)inFigure1showsthe
• Chebyshev[10]:generalizesconvolutionalneuralnetworks
workflowofthestructuralprobe.Thestructuralprobeistoestimate
(CNNs)inthecontextofspectralgraphtheoryanddesign
whetherthegraphembeddinghasencodedthegraphstructural
fastlocalizedconvolutionalfiltersongraphsforgraphlearn-
informatione.g.theWeisfeiler-Lehman(WL)isomorphismtestin-
ing.
formation.Fortheinputgraphs{𝐺 1,𝐺 2,·,𝐺 𝑛},weobtainthegraph
• GCN [22]: performs semi-supervised learning on graph-
representation𝐷 ={H𝑘 1,H𝑘 2,···,H𝑘 𝑛}fromthegraph-basedmodel
structureddataandintroducesasimpleandwell-behaved
4,andcomputethepair-wisesimilarityofgraphembeddings.Co-
layer-wisepropagationruleforneuralnetworkmodelsvia
sinesimilarityisused,andobtainthepairwisesimilarityofgraph the localized first-order approximation of spectral graph
representations𝑆 𝑐𝑘 𝑜𝑠(𝐺 𝑚,𝐺 𝑛)forapairofgraphs𝐺 𝑚 and𝐺 𝑛: convolutions.
• GAT[22]:incorporatesmaskedself-attentionlayersontop
H𝑘 ·H𝑘
ofGCN-stylemethods.
𝑆 𝑐𝑘 𝑜𝑠(𝐺 𝑚,𝐺 𝑛)= ∥H𝑘𝐺𝑚 ∥∥H𝐺 𝑘𝑛 ∥. • GraphSAGE[15]:isaninductiveframeworkforrepresen-
𝐺𝑚 𝐺𝑛 tationlearningonlargegraphswhichleveragesnodefeature
informationtoefficientlygeneratenodeembeddingsforun-
Andwealsocomputethepair-wisesimilarityoftheWLoutputs
seendata.
foreachpairofgraphs.AstheoutputoftheWLalgorithmisa • VGAE[21]:isunsupervisedlearningframeworkbasedon
setofeachnodelabels,weuseJaccardsimilaritytocomputethe
thevariationalauto-encoder.
structural-levelgraphsimilarity𝑆 𝑠𝑘 𝑡𝑟(𝐺 𝑚,𝐺 𝑛)forapairofgraphs • GCL[36]:isagraphcontrastivelearningframeworkforun-
𝐺 𝑚 and𝐺 𝑛: supervisedrepresentationlearningofgraphdataanddevises
fourtypesofgraphaugmentationstoincorporatevarious
𝑆 𝑠𝑘 𝑡𝑟(𝐺 𝑚,𝐺 𝑛)=𝐽𝑎𝑐𝑐𝑎𝑟𝑑(𝑊𝐿(𝐺 𝑚),𝑊𝐿(𝐺 𝑛)). priors
• WGCN[38]:considersthedirectionalstructuralinformation
𝑊𝐿(𝐺 𝑚) isthegraphlabeloutputfromtheWLsub-treekernel fordifferentnodesandproposesaGCNmodelwithweighted
algorithm.Basedonthepair-wisesimilarityscores,wecanhave structuralfeatures.
thepair-wisesimilaritymatrixfromthegraphembeddinglevel • Controlmethod(MLP):weuseasimpletwolayerMLP
S𝐺𝐸 andthestructurallevelS𝑠𝑡𝑟 .Spearmancorrelationcoefficient modelwithReLuafterthefirstlayerasthecontrolmethod.
isusedtoestimatethestructuralscores𝑆 𝑠𝑘 ,bycomparingthetwo We perform probing evaluation on the representative graph
similaritymatrixes,computedasinEquation10. learningmethodsforthreeclassicaldownstreamtasksofgraph
learningmethods.Inadditiontolearningwithrandominitialization,
𝑆 𝑠𝑘 = 𝑛1 ∑︁ 1− 6(cid:205) 𝑛𝐺 (𝑛𝑛 2∈ −𝐷 1𝜏 𝑚 )2 𝑛. (10) w pee rff ou rr mthe tr host ru od uy ghgr aa np ah lyle sa isrn inin Sg ew cti it oh nm 5e .ta-featuresinitialized,and
(𝐺𝑚∈𝐷
4Ifthegraphlearningalgorithmcanlearntherepresentationoftheentiregraph,we
𝜏 r𝑚 an𝑛 ki es dth 𝑅e (So 𝑚𝑠r 𝑡d 𝑟e )r .i 𝑅ng (·)di is sta rn ac ne kib ne gtw oe pe en rat th ioe nra on nke ad n𝑅 i( nS p𝑚𝐺 u𝐸 t) aa rn rad yt .h Ae d
g
5Orir
a
ue pc rht pl ry
re
opu brs iee
ns
geit n. mtO
a
ett tih hoe onr d.w ii sse fl, ew xie blu ys ue st eh de fr oe ra ad no yut gro ap pe hra lt ei ao rn nio nf gn mod ee ths oto d.r Dep ur ee ts oen st path ce
e
higher structure probe score𝑆 𝑠𝑘 indicates that the graph repre- limitation,wechosedifferentrepresentativegraphlearningmethodsfromdiverse
categories.
sentationshavethegreaterabilitytocapturethegraphstructural 6OurdevisedprobesarebasedonMLPandweuseMLPasthecontrolexperiment
information. variableforcomparison.Conferenceacronym’XX,June03–05,2018,Woodstock,NY MingyuZhao,XingyuHuang,ZiyuLyu,YanlinWang,LixinCui,andLuBai
4.2 DownstreamTasksandDatasets neighborfeaturesforeachnode.Also,weaggregatetheweighted
Weconductperformanceevaluationontheclassicdownstream neighborfeaturesforeachnode.Finally,weuseReLUactivation
tasksofgraphlearningmethods,includingnodeclassification[22], function.Inordertokeepperformanceofit,weusethesameen-
linkprediction[29]andgraphclassification[35]. coderGCNasVGAE[21].ForGCL,weusethesameaugmentation
astheGCL,werandomlydropedgesandswapsomenodeinorder
• Nodeclassification:Thistaskisoneofthemostpopular
tomakedifferentgraphs.Weusethecosinesimilaritiestocalculate
andwidelyusedapplicationsofgraphlearningmodels.The
thetwoaugmentinggraphsandcalculatethecontrastiveloss.The
graphlearningmethodslearnthenoderepresentationsand
activationfunctionELUisusedforGATandtheactivationfunction
classifynodesintodifferentgroups.
ReLUisusedfortheothermodels.Thedropoutratioforallmodels
• Linkprediction:Itistopredicatewhetherthereexista
is0.5.Theoutputdimensionsizeforallmodelsis64.Thelearning
linkbetweentwonodes.Forexample,therecommendation
rateissetas0.001.
probleminrecommendersystemscenarioscanbeformu-
latedasonelinkpredictiontaskandconstructauser-item
5 EXPERIMENTALRESULTS
interactiongraphtopredicttheprobabilityoflinkingauser
toaitem. 5.1 PerformanceofGraphLearningMethodson
• Graphclassification:Itsgoalistoclassifyawholegraph NodeClassification
intodifferentcategories.Themainapplicationsofgraphclas-
Inordertovalidatetheknowledgeprobingperformanceofour
sificationareproteinclassificationandchemicalcompound
methodswithrespecttothenodeclassificationtask,wecompare
classification.
theprobingscoresoftherepresentativegraphlearningmethods
Weadoptsomebenchmarkdatasetsfromdifferentdomainsin-
withreferencetocommonlyusedmetricsinnodeclassification
cludingcitationnetworks,socialnetworks,andBio-chemicalNet-
includingAccuracy(ACC)andF1scores.Table2showstheper-
works.Table1showsthestatisticsandproprietressoftheused
formancecomparisonfornodeclassification.Inadditionaltothe
benchmarks.
absoluteperformancenumbers,weaddtheoverallranknumbers
• CitationNetworks:Cora[23]isadatasetcontainingscien- amongthecomparedmethodswithbracketsastherelativeperfor-
tificpaperscategorizedintosevenclasses.Itcommonlybe mance.Forexample,thehighlightednumber79.9(1)indicatesGAT
usedfornodeclassification(transductive).Thenumberof obtains0.799accuracyonCoradataset,andrankfirstamongall
nodeclassesis7,andthenumberofnodefeaturesis1,433. comparedmethods.Inalltables,weuseboldfonttohighlightthe
• SocialNetworks:Flickr[37]isaimagehostingandvideo bestperformanceandunderlinestoindicatetheworstperformance
hostingplatform.Flickrdatasetisusedfornodeclassification 7.
(inductive).Thenumberofnodeclassesis7andthenumber Fornodeclassification,wereporttransductiveperformancefrom
of node features is 500. In addition, we adopt two social CoraandinductiveperformancefromFlickr.Wecanseethatthe
networksdatasetsYelp[29]andMovieLens[17]forLink bestmethodonCora(transductive)isGAT,andthebestmethodis
prediction.Yelpincludes69,716usersanditemsasnodes,and GCLonFlick(inductive)withrespecttobothACCandF1scores.
1,561,406interactionrecords.MovieLensdatasetincludes Ourcentralityprobes(ECandBC)haveconsistentevaluationre-
9,742moviesasnodesinthegraph.Italsoincludesover sultswiththecommonlyusedgoldenmetrics,namelyGATranks
100,836ratingsprovidedbyaround610users.Thenumber first.Fortheworstcases,ourcentralityproberhasthesameresults
ofmeta-featuresforitemsis404. (DeepWalkorMLP)withACCandF1onFlickdataset.OnCora,
• Bio-ChemicalNetworks:Twodatsetsareusedforgraph wefindaninterestingphenomenonthatthesimpleMLPmethods
classification,respectivelyEnzyme[5]andMutag[9].En- isbetterthanGCL,Node2VecandDeepWalkwithrespecttoAcc
zymedatasetcontainstheproteinsinformationwhichcon- andF1whileourcentralityprobingmethoddemonstratetheworst
tributetocatalyzingchemicalreactionsinthebody.Mutag methodsarerespectivelyMLPandNode2Vec.Itmightraisethe
datasetisawidelyusedtoxicitydatasetwhichhelpsassess questionwhetherthefinalstatisticalmetricslikeaccuracyandF1
the potential risks of exposure to various chemicals and actuallyreflectthecapabilityofthegraphrepresentationlearning
compounds. Incomparisonwiththecentralityprobes,ourdistanceprobeare
notveryconsistentwithourcentralityprobesandthetraditional
4.3 ExperimentalSetting
metricsalthoughtheycanfindtheworsecases.Itisbecausethe
AlldatasetsexpectYelpdatasetfollowusethesplittingrulesas thedistanceprobeisapath-wiseprobewhilethenodeclassifica-
previousstudies.FortheYelpdataset,thedatasplittingratiofor tionmightemphasizetoencodemoretopologicalinformationinto
thetrainingset,validationsetandtestsetissetas7:2:1. graphrepresentationlearning.Thepath-wiseprobemethode.g.the
ForChebyshev,weuseonelayerstructure.ForGCN,GAT,Graph- distanceprobemightbeinappropriateforknowledgeprobinginthe
SAGEandMLP,weadoptthetwolayerstructure.Thelengthof nodeclassificationtaskasonlyencodingthepath-wiseinformation
walkforNode2Vecissetas20.Thesizeofthecontextwindow ingraphrepresentationlearningcannotworkswell,e.g.random
fortheskip-gramandthewalkspernodeareboth10.Inorder walkbasedmethodslikeDeepWalkandNode2Vec.
tomakethewalkunbiased,wealsosetthepwhichcontrolshow Wheninitializingwithmeta-features,wehavethesimilarresults
likelythewalkistogobacktothepreviousnode,itissetas1.0.For onFlickdataset,andthebestmethodisGCLwithrespecttoboth
WGCNmethod,weusethesamesetas[38],threelayersstructure,
computetheneighborsnumberforeachnodeandusetheweighted 7Statisticalsignificancetestshavebeenconducted.AllreportedresultsaresignificantKnowledgeProbingforGraphRepresentationLearning Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Table2:ResultsofthegraphlearningmodelsonNodeClassification.Highlightthebestperformancewithboldfontandthe
worstoneswithunderlines.
Dataset-rand Metrics Chebyshev GCN GAT GraphSAGE VGAE GCL WGCN Node2Vec DeepWalk MLP
ACC 52.9(6) 78.6(3) 79.9(1) 77.3(4) 79.7(2) 45(8) 76.13(5) 14.1(10) 21.24(9) 52.4(7)
F1 56.94(6) 77.96(3) 79.18(1) 76.64(4) 78.84(2) 45.7(8) 74.32(5) 12.88(10) 21.19(9) 51.93(7)
Cora EC 60.45(7) 73.6(2) 75.12(1) 72.42(4) 72.83(3) 62.38(6) 67.17(5) 57.5(9) 58.13(8) 57.35(10)
BC 59.24(6) 70.24(3) 76.12(1) 58.01(7) 71.24(2) 61.13(5) 64.13(4) 54.1(9) 54.12(8) 53.8(10)
Distance 8.05(10) 11.24(7) 12.98(3) 11.93(6) 24.16(1) 10.08(9) 12.91(4) 14.68(2) 11.98(5) 11.24(7)
ACC 77.5(5) 81.14(4) 72.05(6) 57.05(7) 91.41(2) 92.8(1) 85.12(3) 50.49(8) 50.12(9) 47.94(10)
F1 77.5(5) 81.14(4) 72.05(6) 57.05(7) 91.41(2) 92.8(1) 85.12(3) 50.49(8) 50.12(9) 47.94(10)
Flickr EC 49.7(7) 50(6) 50.05(4) 50.05(4) 71.28(2) 76.12(1) 69.12(3) 32.18(8) 31.24(9) 23.13(10)
BC 68.17(4) 65.72(7) 66.12(6) 67.12(5) 70.13(2) 75.13(1) 70.1(3) 30.13(8) 28.17(10) 29.38(9)
Distance 13.45(6) 13.82(2) 13.82(2) 12.54(7) 13.62(5) 13.82(2) 19.52(1) 11.98(8) 10.44(9) 9.68(10)
Dataset-meta Initializationwithmeta-features
ACC 62.3(7) 79.4(2) 79.9(1) 77.3(5) 78.5(3) 55.14(9) 78.12(4) 55.18(8) 68.91(6) 53.4(10)
F1 63.9(6) 78.57(2) 79.02(1) 76.52(5) 77.83(4) 55.18(9) 78.01(3) 58.13(8) 61.13(7) 52.8(10)
Cora EC 65.79(6) 72.37(1) 72.14(2) 66.18(4) 70.24(3) 41.23(8) 66.01(5) 52.37(7) 32.44(9) 30.12(10)
BC 63.71(6) 70.12(1) 69.27(2) 66.04(4) 66.13(3) 51.28(8) 64.78(5) 57.89(7) 41.28(9) 39.38(10)
Distance 8.03(10) 11.18(8) 13.2(4) 12.03(5) 11.47(6) 11.04(9) 30.91(1) 14.59(2) 13.82(3) 11.19(7)
ACC 77.9(5) 82.12(4) 74.13(6) 59.38(7) 92.42(2) 93.12(1) 86.13(3) 52.48(8) 52.34(9) 50.13(10)
F1 77.9(5) 82.12(4) 74.13(6) 59.38(7) 92.42(2) 93.12(1) 86.13(3) 52.48(8) 52.34(9) 50.13(10)
Flickr EC 48.37(7) 81.38(5) 68.73(6) 86.81(3) 90.38(2) 91.24(1) 85.12(4) 48.37(7) 46.37(9) 42.41(10)
BC 38.14(6) 39.14(4) 37.89(7) 38.84(5) 67.4(2) 68.38(1) 59.24(3) 32.32(8) 29.38(9) 29.04(10)
Distance 24.25(1) 24.25(1) 21.48(5) 19.09(7) 23.01(4) 18.33(8) 23.62(3) 18.02(9) 19.1(6) 16.04(10)
Table3:ResultsofthegraphlearningmodelsonLinkPrediction.Highlightthebestperformancewithboldfontandtheworst
oneswithunderlines.
Dataset-rand Metrics Chebyshev GCN GAT GraphSAGE VGAE GCL WGCN Node2Vec DeepWalk MLP
AUC 61.24(7) 78.14(1) 63.12(6) 63.71(5) 73.14(3) 67.12(4) 76.12(2) 55.12(8) 52.37(9) 42.32(10)
F1 52.24(8) 71.50(1) 66.15(5) 65.23(6) 71.09(2) 70.01(3) 68.57(4) 53.17(7) 51.95(9) 51.05(10)
Yelp Distance 31.90(3) 32.02(1) 24.25(5) 22.05(8) 24.25(5) 28.32(4) 32.01(2) 24.18(7) 16.33(9) 14.04(10)
EC 46.28(9) 71.21(2) 60.71(5) 64.28(4) 72.16(1) 56.73(6) 70.12(3) 49.82(7) 48.12(8) 45.39(10)
BC 45.13(7) 72.12(1) 56.78(6) 63.47(4) 71.97(2) 58.12(5) 71.12(3) 39.02(9) 40.18(8) 36.85(10)
AUC 68.21(5) 73.75(2) 47.91(10) 71.21(3) 74.29(1) 68.14(6) 70.12(4) 56.43(7) 51.24(8) 50.12(9)
F1 50.75(9) 67.94(3) 61.25(4) 58.64(5) 70.48(2) 58.60(6) 70.87(1) 51.84(7) 51.06(8) 48.82(10)
MovieLens Distance 19.50(8) 19.52(7) 29.78(2) 24.25(4) 24.17(6) 28.77(3) 31.82(1) 24.20(5) 15.75(9) 12.31(10)
EC 50.52(7) 70.85(2) 61.23(5) 63.41(4) 72.16(1) 59.13(6) 70.12(3) 49.82(9) 50.12(8) 41.26(10)
BC 40.97(8) 72.30(1) 57.55(6) 63.77(4) 71.97(2) 62.12(5) 69.13(3) 39.02(9) 42.12(7) 37.24(10)
Dataset-meta Initializationwithmeta-features
AUC 63.85(7) 81.14(2) 78.03(4) 70.37(5) 78.24(3) 70.13(6) 82.34(1) 59.12(8) 55.37(9) 45.32(10)
F1 53.44(9) 75.35(1) 67.45(5) 66.83(6) 73.19(4) 74.60(2) 73.89(3) 57.12(7) 54.82(8) 52.44(10)
Yelp Distance 28.96(4) 33.20(2) 24.18(8) 24.47(7) 24.92(6) 25.70(5) 49.06(1) 29.87(3) 19.47(9) 13.82(10)
EC 47.21(8) 72.43(2) 61.24(5) 65.12(4) 73.13(1) 57.14(6) 71.21(3) 50.12(7) 47.12(9) 42.31(10)
BC 46.14(7) 73.13(3) 57.38(6) 64.18(4) 73.59(1) 59.12(5) 73.24(2) 41.31(9) 45.89(8) 38.13(10)
AUC 70.57(4) 73.56(3) 48.37(10) 74.92(2) 74.96(1) 52.38(8) 70.18(5) 56.66(7) 57.17(6) 50.12(9)
F1 62.45(5) 66.16(3) 45.63(8) 71.86(1) 67.97(2) 44.22(9) 64.51(4) 51.65(6) 50.13(7) 43.59(10)
MovieLens Distance 27.15(5) 29.77(2) 22.43(7) 28.02(4) 22.84(6) 28.68(3) 32.28(1) 20.51(8) 19.52(9) 16.33(10)
EC 60.73(5) 72.63(1) 71.88(2) 60.36(6) 71.61(3) 31.35(10) 65.35(4) 55.14(8) 60.12(7) 42.31(9)
BC 53.29(6) 71.10(2) 75.79(1) 50.47(8) 71.04(3) 43.13(9) 67.12(4) 53.17(7) 58.38(5) 40.13(10)
ofourcentralityprobesandtheclassicalmetrics.OnCora,the Inordertofurtherdiscusstherelationsbetweenthenode-wise
top-2methodwithrespecttoACCandF1areGATandGCNwhile propertiesandthegraphrepresentationlearning,wecalculatethe
top-2methodswithrespecttoourcentralityprobesareGATand homophily[40]ofdifferentgraphsfromdifferentdatasets.The
CCN.OnCoraandFlickdatasets,theworstcasesareallMLPwith homophilyratioofisabout0.8forCoranad0.32forFlickr.Itmight
respecttobothofourcentralityprobesandtheclassicalmetrics. explainwhydifferentgraphmethodsshowdifferentperformance,
Ingeneral,ithasslightperturbationintherelativeperformance duetotheinherentgraphproperties.GNNmethodslikeGATcan
wheninitializedwithmeta-features. havebetterperformanceonhighlyhomophilousgraphs.Ourprobe
alsohaveconsistentresultswiththehomophilyanalysis.Conferenceacronym’XX,June03–05,2018,Woodstock,NY MingyuZhao,XingyuHuang,ZiyuLyu,YanlinWang,LixinCui,andLuBai
Table4:ResultsofthegraphlearningmodelsonGraphClassification.Highlightthebestperformancewithboldfontandthe
worstoneswithunderlines.
Dataset-rand Metrics Chebyshev GCN GAT GraphSAGE VGAE GCL WGCN Node2Vec DeepWalk MLP
ACC 82.05(5) 92.31(1) 87.18(2) 84.62(4) 74.36(6) 74.36(6) 87.13(3) 72.71(9) 73.13(8) 71.49(10)
MUTAG F1 82.05(5) 92.31(1) 87.18(2) 84.62(4) 74.36(6) 74.36(6) 87.13(3) 72.71(9) 73.13(8) 71.49(10)
Structure 11.96(4) 15.69(1) 13.39(2) 8.29(5) 3.89(7) 3.79(8) 12.23(3) 5.12(6) 3.13(9) 2.62(10)
ACC 45(4) 44.17(5) 45.83(3) 54.17(2) 43.33(6) 43.33(6) 59.38(1) 42.5(8) 41.5(9) 37.5(10)
ENZYMES F1 15(10) 22.5(3) 20.83(4) 18.33(5) 26.67(1) 16.67(8) 25.81(2) 18.27(6) 17.93(7) 16.67(8)
Structure 3.44(7) 11.82(3) 8.37(4) 12.83(2) 7.26(6) 8.36(5) 15.48(1) 2.19(8) 2.04(9) 1.84(10)
Dataset-meta Initializationwithmeta-features
ACC 94.87(1) 82.05(6) 84.62(4) 92.31(2) 79.49(7) 84.62(4) 90.24(3) 77.31(8) 75.13(9) 74.62(10)
MUTAG F1 94.87(1) 82.05(6) 84.62(4) 92.31(2) 79.49(7) 84.62(4) 90.24(3) 77.31(8) 75.13(9) 74.62(10)
Structure 18.19(1) 4.78(7) 6.02(5) 16.48(3) 5.98(6) 15.8(4) 17.29(2) 2.38(9) 2.41(8) 1.73(10)
ACC 43.33(5) 47.5(3) 39.17(10) 57.5(2) 43.33(5) 46.67(4) 60.35(1) 41.32(8) 40.13(9) 43.33(5)
ENZYMES F1 22.5(3) 21.67(4) 15(10) 20(5) 19.17(6) 25.83(1) 25.23(2) 17.99(8) 18.12(7) 17.5(9)
Structure 7.49(3) 5.12(4) 3.48(6) 8.89(2) 3.24(7) 4.56(5) 10.23(1) 1.31(9) 1.78(8) 0.39(10)
Figure2:Radarchartcomparisonofgraphembeddingmodelsfordifferentinformationembeddingcapabilitie
• Ourcentralityprobesiseffectiveforknowledgeprob- 5.2 PerformanceofGraphLearningMethodson
ingofgraphrepresentationlearninginthenodeclas- LinkPrediction
sificationtask.Theyhasconsistentresultswiththetradi-
Toevaluatetheknowledgeprobingperformanceofourmethods
tionalevaluationmetricsaccuracyandF1,validatingtheir
withrespecttothelinkpredictiontask,wecomparetheprobing
effectivenessofourcentralityprobes.
scoresoftherepresentativegraphlearningmethodswithreference
• Thepath-wiseprobemethode.g.thedistanceprobemight
tocommonlyusedmetricsinthelinkpredictiontaskonYelpand
beinappropriateforknowledgeprobinginthenodeclassifi-
Movielensdataset,includingAUCandF1scores.Weevaluateboth
cationtask.
thecentralityprobesandthedistanceprobes,andtheperformance
• GATissuperiortootherrepresentativemethods.The
comparisonresultsonYelpandMovieLensforlinkpredictionare
top-2methodsareGATandGCN.Initializingwithmeta-
reportedinTable3.OnYelpdataset,thebestmethodsisGCNand
measuresresultsintosmallperturbationamongthetopper-
theworstmethodisDeepWalkexceptMLP,withrespecttoboth
centileinperformanceranking.
AUCandF1scores.ThedistanceprobehasconsistentresultswithKnowledgeProbingforGraphRepresentationLearning Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Skyline Plot on different Downstream Tasks Skyline Plot on different Downstream Tasks Skyline Plot on different Downstream Tasks
5 Data MLP 5 Data MLP 5 Data MLP
Dominate DeepWalk Dominate Node2Vec Dominate Node2Vec
4 Line Chebyshev 4 Line DeepWalk 4 Line DeepWalk
Node2Vec
3 GraphSAGE 3 VGAE GCL 3 VGAEGCL
GAT Chebyshev Chebyshev
2 WGCN 2 GraphSAGE 2 GraphSAGE
GCL WGCN WGCN
1 VGAE 1 GAT 1 GAT
GCN GCN GCN
1 2 3 4 5 1 2 3 4 5 1 2 3 4 5
Performance of Node Classification Performance of Node Classification Performance of Link Prediction
Figure3:SkylinePlotondifferentDownstreamTasksbasedonrandominitialization
bothAUCandF1scores.Althoughthecentralityprobescannotbe bestmethodisGCNwithwithbothACCandF1andourstructural
totallythesamewiththetraditionalscores,wecanseethatthey( probingresultisconsistentwiththetraditionalgoldenmetrics.On
especiallythecentralityprobewithbetweeness)havesimilarresults ENZYMESdataset,ourstructuralprobingresultsareconsistent
byrankingGCN,VGAEattop-3positionsinthecasethatthetwo withACCscores(WGCNisbest).Forthecaseswithmeta-features,
traditionalmetricshavesimilarresultsratherthantheconsistent ourstructuralprobingresultsareconsistentwithACCandF1scores
results.OnMoivelensdataset,thetraditionalmetricshaveshown onMUTAGdatasetandwithACCscores(Chebyshevisbest)on
considerabledifferentresultsintheperformanceranking,inwhich ENZYMESdataset(WGCNisbest).
thebestmethodisVGAEwithAUCandthatisWGCNwithF1. • Ourstructureprobeiseffectiveforknowledgeprobing
Thisphenomenontosomeextentindicatesthatdifferentevaluation
ofgraphrepresentationlearninginthegraphclassifica-
metricsmighthavesomedifferentrelativeresultsduetodifferent
tiontask.Theyhaveconsistentresultswiththetraditional
measuremechanisms.OurdistanceprobeisconsistentwithF1for
evaluationmetricsACCandF1fordifferentinitialization
thebestmethod,andourcentralityprobesaremoresimilarwith
setting.
AUCscores. • Ingeneral,nosinglemethodcandominateothermeth-
Wheninitializingwithmeta-features,theperformanceoflink
odsonthetwodatasets.WGCNhasrelativelyrobustpe-
predictiononYelpandMovielenshavesomedifferentresults.The
formance,rankingattop-3positionsonthetwodatasets.In
bestmethodonYelpisWGCN(AUC)andGCN(F1),incontrast
somecases,some"out-of-date"methodse.g.Chebyshevcan
withrandominitialization(GCN).OnMoivelensdataset,thebest
havebetterperformance.
ones are VGAE (AUC) and GraphSAGE (F1) while the random
initializationresultsareVGAE(AUC)andWGCN(F1).Ourdistance 5.4 VisualizationAnalysis
probealsocapturethedifferencesduetodifferentinitialization
Inordertofurthercomparetheoverallperformanceofdifferent
settingandisstillconsistentwiththetraditionalmetrics,ranking
methodsfordifferentinformationembeddingcapabilities,wecom-
WGCNandGCNatthetop-2positions.
putetherankingofthe9representativemethodsandtheMLPbase-
• Thedistanceprobeiseffectiveforknowledgeprobing
lineforeachprobeanduseradarchartstovisualizetheircapacities
ofgraphrepresentationlearninginthelinkprediction
inFigure2(IindicatestheinductiveandTindicatestransductive,
task.Theyhaveconsistentresultswiththetraditionaleval-
andMtheMeta).GCNandWGCNhavebetterperformance
uationmetricsAUCandF1fordifferentinitializationsetting.
inmostprobingaspectsincomparisonwithothermethods.
Thereasonmightbethatthepath-wiseprobeisdevisedto
Althoughhavingthesamecapacitiesofaggregatingtheneighbor
probethepath-wiseinformationwithingraphrepresenta-
information,VGAEhasthesub-optimalperformance.Furthermore,
tionlearningwhichactsthecorerolesinlinkprediction.
theyallhardlyrelyonmetainformations.GCLhasbetterperfor-
• Ingeneral,nosinglemethodcanbetotallysuperiortoother
manceonNodeClassification(Inductive),GAThasbetterperfor-
methodswithrespecttoallevaluationmetricsonthetwo
manceonNodeClassification(transductive).Chebyshevmightbe
datasets.GCN,VGAEandWGCNcanberankedatthetop
betterusedwithmetainformation.Node2Vechastheworstperfor-
positions.
manceonallaspects.
Wealsodrawtheskylineplotforfindingthemethodsondif-
5.3 PerformanceofGraphLearningMethodson
ferentdownstreamtaskswhichcannotbedominatedbyother
GraphClassification
methodsinFigure3.Wecaninvestigatethejoint-abilitiesofgraph
We compare the structure probing scores of the representative learningmethodsondifferentdownstreamtasks.InLinkPrediction-
graphlearningmethodswithreferencetocommonlyusedmetrics NodeClassificationtasks,GAT,VGAEandGCNhasbetterperfor-
inthegraphclassificationtaskonMUTAGandENZYMESdataset, mancethatcannotbedominated.InGraphClassification-Node
includingaccuracy(ACC)andF1scores.Table3demonstratesthe Classification,GATandGCNperformswell.GAThasbettergraph
performanceresultsongraphclassification.OnMUTAGdataset,the classificationcapabilitiesandGCNhasbetternodeclassification
noitciderP
kniL
fo
ecnamrofreP
noitacifissalC
hparG
fo
ecnamrofreP
noitacifissalC
hparG
fo
ecnamrofrePConferenceacronym’XX,June03–05,2018,Woodstock,NY MingyuZhao,XingyuHuang,ZiyuLyu,YanlinWang,LixinCui,andLuBai
abilities.InGraphClassification-LinkPredictions,onlyGCNcannot forComputationalLinguistics:HumanLanguageTechnologies,Volume1(Long
bedominatedbyothers.Fromtheskylineresults,wecanseethat andShortPapers),pages4171–4186,Minneapolis,Minnesota.Associationfor
ComputationalLinguistics.
GCNandGAThasbetterjoint-abillitesfordownstreamtasks.
[12] LintonFreeman.1977. Asetofmeasuresofcentralitybasedonbetweenness.
Sociometry,40:35–41.
5.5 EffectsofParameters [13] CLeeGiles,KurtDBollacker,andSteveLawrence.1998.Citeseer:Anautomatic
citationindexingsystem.InProceedingsofthethirdACMconferenceonDigital
Theonlyhyperparametersusedinourprobesisthepathparameter. libraries,pages89–98.
[14] AdityaGroverandJureLeskovec.2016.node2vec:Scalablefeaturelearningfor
Itcontrolstheshortestpaththatourprobecandetectinthegraph.
networks.InProceedingsofthe22ndACMSIGKDDinternationalconferenceon
WecalculatetheCorrelationswiththeF1scoresindifferentpath, Knowledgediscoveryanddatamining,pages855–864.
itshowsthatmostofourbestpatharebetween3and4.Weuse [15] WillHamilton,ZhitaoYing,andJureLeskovec.2017.Inductiverepresentation
learningonlargegraphs.Advancesinneuralinformationprocessingsystems,30.
themaxscoreofthepathforeachdatasets [16] WilliamL.Hamilton,RexYing,andJureLeskovec.2017.Representationlearning
ongraphs:Methodsandapplications.IEEEDataEng.Bull.,40:52–74.
[17] F.MaxwellHarperandJosephA.Konstan.2015.Themovielensdatasets:History
6 CONCLUSION
andcontext.ACMTrans.Interact.Intell.Syst.,5(4).
In this paper, we proposed a graph probing benchmark for the [18] XiangnanHe,KuanDeng,XiangWang,YanLi,YongDongZhang,andMeng
Wang.2020.Lightgcn:Simplifyingandpoweringgraphconvolutionnetworkfor
representativegraphlearningmethods.Diverseprobesatthree recommendation.InProceedingsofthe43rdInternationalACMSIGIRConference
differentlevels(node-wise,path-wiseandstructure-wise)arede- onResearchandDevelopmentinInformationRetrieval,SIGIR’20,page639–648,
NewYork,NY,USA.AssociationforComputingMachinery.
vised to investigate and interpret weather the graph properties
[19] JohnHewittandChristopherD.Manning.2019.Astructuralprobeforfinding
fromdifferentlevelsareencodedintorepresentationlearningof syntaxinwordrepresentations. InProceedingsofthe2019Conferenceofthe
thesevenrepresentativegraphneuralnetworksbasedmethods.We NorthAmericanChapteroftheAssociationforComputationalLinguistics:Hu-
manLanguageTechnologies,Volume1(LongandShortPapers),pages4129–4138,
conductsystemevaluationandthoroughanalysistoinvestigate
Minneapolis,Minnesota.AssociationforComputationalLinguistics.
whatkindofinformationhavebeenencodedandwhichmethods [20] YifanHouandMrinmayaSachan.2021.Bird’seye:Probingforlinguisticgraph
havecompetitiveperformancewithdifferenttargeteddownstream structureswithasimpleinformation-theoreticapproach.InProceedingsofthe
59thAnnualMeetingoftheAssociationforComputationalLinguisticsandthe11th
tasks.Theexperimentalevaluationvalidatetheeffectivenessof InternationalJointConferenceonNaturalLanguageProcessing(Volume1:Long
GraphProbe.Furthermore,Weconcludesomeremarkingfindings: Papers),pages1844–1859,Online.AssociationforComputationalLinguistics.
[21] ThomasNKipfandMaxWelling.2016.Variationalgraphauto-encoders.arXiv
GATissuperiorinthenodeclassification;GCNandWGCNare
preprintarXiv:1611.07308.
relativelyversatilemethodsachievingbetterresultswithrespectto [22] ThomasN.KipfandMaxWelling.2017.Semi-supervisedclassificationwithgraph
differenttasks.Thebenchmarkcodesandresourceswillbepublic convolutionalnetworks.InInternationalConferenceonLearningRepresentations.
[23] AndrewKachitesMcCallum,KamalNigam,JasonRennie,andKristieSeymore.
afteracceptance.
2000.Automatingtheconstructionofinternetportalswithmachinelearning.
InformationRetrieval,3:127–163.
REFERENCES [24] BryanPerozzi,RamiAl-Rfou,andStevenSkiena.2014.Deepwalk:Onlinelearning
ofsocialrepresentations.InProceedingsofthe20thACMSIGKDDinternational
[1] AmrAhmed,NinoShervashidze,ShravanNarayanamurthy,VanjaJosifovski, conferenceonKnowledgediscoveryanddatamining,pages701–710.
andAlexanderJ.Smola.2013.Distributedlarge-scalenaturalgraphfactorization. [25] MatthewE.Peters,MarkNeumann,MohitIyyer,MattGardner,Christopher
InProceedingsofthe22ndInternationalConferenceonWorldWideWeb,WWW Clark,KentonLee,andLukeZettlemoyer.2018.Deepcontextualizedwordrepre-
’13,page37–48,NewYork,NY,USA.AssociationforComputingMachinery. sentations.InProceedingsofthe2018ConferenceoftheNorthAmericanChapter
[2] MohammadSadeghAkhondzadeh,VijayLingam,andAleksandarBojchevski. oftheAssociationforComputationalLinguistics:HumanLanguageTechnologies,
2023. Probinggraphrepresentations. InInternationalConferenceonArtificial Volume1(LongPapers),pages2227–2237,NewOrleans,Louisiana.Association
IntelligenceandStatistics,pages11630–11649.PMLR. forComputationalLinguistics.
[3] AlonAltmanandMosheTennenholtz.2005. Rankingsystems:Thepagerank [26] FabioPetroni,TimRocktäschel,SebastianRiedel,PatrickLewis,AntonBakhtin,
axioms.InProceedingsofthe6thACMConferenceonElectronicCommerce,EC YuxiangWu,andAlexanderMiller.2019.Languagemodelsasknowledgebases?
’05,page1–8,NewYork,NY,USA.AssociationforComputingMachinery. InProceedingsofthe2019ConferenceonEmpiricalMethodsinNaturalLanguage
[4] PhillipBonacich.1987.Powerandcentrality:Afamilyofmeasures.American Processingandthe9thInternationalJointConferenceonNaturalLanguagePro-
journalofsociology,92(5):1170–1182. cessing(EMNLP-IJCNLP),pages2463–2473,HongKong,China.Associationfor
[5] KarstenM.Borgwardt,ChengSoonOng,StefanSchönauer,S.V.N.Vishwanathan, ComputationalLinguistics.
AlexJ.Smola,andHans-PeterKriegel.2005. Proteinfunctionpredictionvia [27] TiagoPimentel,JosefValvoda,RowanHallMaudslay,RanZmigrod,Adina
graphkernels.Bioinformatics,21(1):47–56. Williams,andRyanCotterell.2020.Information-theoreticprobingforlinguistic
[6] ShaoshengCao,WeiLu,andQiongkaiXu.2015.Grarep:Learninggraphrepre- structure.InProceedingsofthe58thAnnualMeetingoftheAssociationforCom-
sentationswithglobalstructuralinformation.InProceedingsofthe24thACM putationalLinguistics,pages4609–4622,Online.AssociationforComputational
InternationalonConferenceonInformationandKnowledgeManagement,CIKM Linguistics.
’15,page891–900,NewYork,NY,USA.AssociationforComputingMachinery. [28] C.J.VanRijsbergen.1979. InformationRetrieval,2ndedition. Butterworth-
[7] AlexisConneau,GermanKruszewski,GuillaumeLample,LoïcBarrault,and Heinemann,USA.
MarcoBaroni.2018. Whatyoucancramintoasingle$&!#*vector:Probing [29] PrithvirajSen,GalileoNamata,MustafaBilgic,LiseGetoor,BrianGalligher,and
sentenceembeddingsforlinguisticproperties. InProceedingsofthe56thAn- TinaEliassi-Rad.2008.Collectiveclassificationinnetworkdata.AImagazine,
nualMeetingoftheAssociationforComputationalLinguistics(Volume1:Long 29(3):93–93.
Papers),pages2126–2136,Melbourne,Australia.AssociationforComputational [30] MarvinE.Shaw.1954.Groupstructureandthebehaviorofindividualsinsmall
Linguistics. groups.TheJournalofPsychology,38(1):139–149.
[8] KousikDas,SovanSamanta,andMadhumangalPal.2018.Studyoncentrality [31] NinoShervashidze,PascalSchweitzer,ErikJanvanLeeuwen,KurtMehlhorn,
measuresinsocialnetworks:asurvey.Socialnetworkanalysisandmining,8:1–11. andKarstenM.Borgwardt.2011. Weisfeiler-lehmangraphkernels. J.Mach.
[9] AsimKumarDebnath,RosaL.LopezdeCompadre,GargiDebnath,AlanJ.Shus- Learn.Res.,12(null):2539–2561.
terman,andCorwinHansch.1991.Structure-activityrelationshipofmutagenic [32] XingShi,InkitPadhi,andKevinKnight.2016.Doesstring-basedneuralMTlearn
aromaticandheteroaromaticnitrocompounds.correlationwithmolecularorbital sourcesyntax? InProceedingsofthe2016ConferenceonEmpiricalMethodsin
energiesandhydrophobicity.JournalofMedicinalChemistry,34(2):786–797. NaturalLanguageProcessing,pages1526–1534,Austin,Texas.Associationfor
[10] MichaëlDefferrard,XavierBresson,andPierreVandergheynst.2016.Convolu- ComputationalLinguistics.
tionalneuralnetworksongraphswithfastlocalizedspectralfiltering.Advances [33] PetarVelickovic,GuillemCucurull,ArantxaCasanova,AdrianaRomero,Pietro
inneuralinformationprocessingsystems,29. Lio,YoshuaBengio,etal.2017.Graphattentionnetworks.stat,1050(20):10–48550.
[11] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2019.BERT: [34] L.Wang,C.Chen,andH.Li.2022.Linkpredictionofcomplexnetworkbased
Pre-trainingofdeepbidirectionaltransformersforlanguageunderstanding.In oneigenvectorcentrality.InJournalofPhysics:ConferenceSeries,volume2337,
Proceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociation page012018(8pp.).2022WorkshoponPatternRecognitionandDataMiningKnowledgeProbingforGraphRepresentationLearning Conferenceacronym’XX,June03–05,2018,Woodstock,NY
(WPRDM2022),24-26June2022,Wuhan,China. [38] YunxiangZhao,JianzhongQi,QingweiLiu,andRuiZhang.2021.Wgcn:graph
[35] KeyuluXu,WeihuaHu,JureLeskovec,andStefanieJegelka.2019.Howpowerful convolutionalnetworkswithweightedstructuralfeatures. InProceedingsof
aregraphneuralnetworks?InInternationalConferenceonLearningRepresenta- the44thInternationalACMSIGIRConferenceonResearchandDevelopmentin
tions. InformationRetrieval,pages624–633.
[36] YuningYou,TianlongChen,YongduoSui,TingChen,ZhangyangWang,and [39] HaoZhuandPiotrKoniusz.2021.Simplespectralgraphconvolution.InInterna-
YangShen.2020.Graphcontrastivelearningwithaugmentations.Advancesin tionalConferenceonLearningRepresentations.
neuralinformationprocessingsystems,33:5812–5823. [40] JiongZhu,YujunYan,LingxiaoZhao,MarkHeimann,LemanAkoglu,andDanai
[37] HanqingZeng,HongkuanZhou,AjiteshSrivastava,RajgopalKannan,andViktor Koutra.2020.Beyondhomophilyingraphneuralnetworks:Currentlimitations
Prasanna.2020.GraphSAINT:Graphsamplingbasedinductivelearningmethod. andeffectivedesigns.Advancesinneuralinformationprocessingsystems,33:7793–
InInternationalConferenceonLearningRepresentations. 7804.