How Well Can Vision Language Models See
Image Details?
Chenhui Gou1*, Abdulwahab Felemban2, Faizan Farooq Khan2,
Deyao Zhu,2 Jianfei Cai1, Hamid Rezatofighi1, and Mohamed Elhoseiny2
1 Monash University
2 King Abdullah University of Science and Technology
Abstract. LargeLanguageModel-basedVision-LanguageModels(LLM-
based VLMs) have demonstrated impressive results in various vision-
language understanding tasks. However, how well these VLMs can see
image detail beyond the semantic level remains unclear. In our study,
we introduce a pixel value prediction task (PVP) to explore "How Well
Can Vision Language Models See Image Details?" and to assist VLMs in
perceiving more details. Typically, these models comprise a frozen CLIP
visual encoder, a large language model, and a connecting module. After
fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle
to predict precise pixel values by only fine-tuning the connection module
and LLM; and 2) prediction precision is significantly improved when the
vision encoder is also adapted. Additionally, our research reveals that
incorporatingpixelvaluepredictionasoneoftheVLMpre-trainingtasks
and vision encoder adaptation markedly boosts VLM performance on
downstreamimage-languageunderstandingtasksrequiringdetailedimage
perception,suchasreferringimagesegmentation(withanaverage+10.19
cIoU improvement) and video game decision making (with average score
improvements of +80.34 and +70.54 on two games, respectively).
1 Introduction
Large Language Models (LLMs) have revolutionized the field of artificial intel-
ligence, enabling machines to perceive and generate human-like text with re-
markable performance. Following this advancement, LLM-based Vision-Language
Models (VLMs) are rapidly evolving within the cross-domain of vision and lan-
guage.RecentVLMs,suchas [8,31,32,61]haveshownpromisingperformanceon
multiple vision-language tasks, including visual question answering (VQA) and
referring expression comprehension (REC). Typically, these LLM-based VLMs
adopt a similar modeling design: a pre-trained visual encoder to extract visual
features, a projection module to align these features to the language space, and
an LLM to perform reasoning.
These VLMs primarily utilize CLIP-based (Contrastive Language-Image Pre-
training) visual encoders (e.g., CLIP [41], OpenCLIP [10], EVA-CLIP [13] and
⋆ MostworkdoneduringinternshipatKingAbdullahUniversityofScienceandTech-
nology.
4202
guA
7
]VC.sc[
1v04930.8042:viXra2
Reconstructed
image by VLM
Predicted RGB value for location (x,y)
[31, 6,108]
CLIP
ViT LLM VLM
with ViT
adaption
aC dL w aIP pit th iV o i nT LLM Ground< l to r/ ucI tm : h [g R{x G> } B ,[ { r cye o}c l] o o rr gn fobs rt : rr eu cc ot n] s truction:
[ 3 8 , 8 , 98] C Loro ss ss-Entropy
a) VLM use original CLIP ViT can only reconstruct a blurry image b) Our method: Pixel Reconstruction Pretraining for VLM
Rerferring lmage Segmentation Video Game playing given game instruction.
c) Pixel Reconstruction Pretraining improves visual detail understanding of VLM
Fig.1: Method.a)showsourfindings:UsingtheoriginalCLIPvisionfeatures,VLMs
can only reconstruct a blurry contour without many visual details. The reconstruction
result can be improved by adapting the vision encoder. The reconstructed image is
generated by querying pixel values with pixel locations, as shown in (b). For better
illustration, the connection module between ViT and LLM is ignored. b) shows that
we incorporate pixel prediction as a pretraining task for VLM. c) illustrates some
downstream tasks performed by VLM, which require both vision detail understanding
and language information. Our pretraining improves VLM performance on these tasks.
CLIP image features are advantageous for interpretation by LLMs [36] because
thesefeaturesarealignedwiththelanguagespacethroughtrainingonlarge-scale
image-text paired datasets. However, as CLIP image features are aligned with
short and brief language captions, it remains uncertain whether these LLMs can
truly "see" the original image content.
To investigate this, we propose a method to show how well current VLMs
can perceive visual details from original CLIP vision features by examining
their ability to predict pixel values of the perceived images. Inspired by Implicit
Neural Representation (INR) in image generation [3,17,48–50], we design a pixel
value prediction (PVP) task in a visual question-answering format, which can be
directly integrated into existing VLM pipelines. Given image CLIP features and
an (x,y) coordinate, we promptthe Large LanguageModel (LLM)to predict the
RGB pixel value at that coordinate. The question format can be seen in Fig. 1b).
We first fine-tune the VLM following common protocol, training the connection
module and LLM while freezing the vision encoder. As shown in Fig. 1a), for
better visualization, we visualize the reconstructed image by querying all pixel
locations in batch inference, while during training we only randomly sample a
locationfromarandomimageinthetrainingset.WefindthatVLMswithafrozen
CLIP encoder can only reconstruct a blurry contour without many visual details.
Furthermore, we notice a significant improvement in pixel prediction results if weHow Well Can Vision Language Models See Image Details? 3
also adapt the CLIP encoder for the PVP task. More visualization results can be
viewed in Fig. 3. Also, Pixel reconstruction [14,18,21] is a classical and effective
vision pre-training task for downstream tasks that require an understanding of
visiondetails,suchassegmentationordepthestimation[4,18,24].Inspiredbyour
findings and the previous successes of the pixel reconstruction task transferred
to detailed vision tasks, we adapt pixel value prediction as a pre-training task
for Vision-Language Models (VLMs), as demonstrated in Fig. 1b. and expect
the enhanced perception ability to be helpful for downstream tasks that require
detailed vision and language understanding. Due to the special properties of our
pre-trained model, we refer to it as the Pixel Autoencoded Large MultiModal
Model (PAE-LaMM).
Tovalidatewhethertheimprovedpixelpredictionabilitycantrulyhelpbetter
vision detail understanding ability in VLMs, we selected two downstream vision-
language tasks that require visual details to compare the performance of the base
VLM and PAE-LaMM: the Referring Image Segmentation task [23,38] and the
video game playing task, as shown in Fig. 1c. In the segmentation task, VLMs
need to accurately perceive the shape of an object referenced in a given phrase
within an image and generate its segmentation mask. For video game playing
tasks such as Car Racing and Space Invaders, VLMs need to correctly interpret
visual elements like the road or enemy bullets, generating appropriate actions
based on stacked video frames and the game description. We collected datasets
comprising over 53K observation-action pairs played by expert reinforcement
learning models and trained our model to imitate the actions of these experts. In
the experiments section, we first present the performance gap on the PVP task
betweenfine-tuningVLMwithafrozenvisionencoderandViTadaptation.Next,
we illustrate how our pre-training task and vision encoder adaptation strategy
benefit downstream tasks like referring segmentation and video gaming through
improvedperceptionofvisualdetails.Finally,weshowourmethod’sperformance
on mainstream Visual Question Answering (VQA) tasks, demonstrating that
our pre-trained model achieves results comparable to state-of-the-art approaches
whilealsoofferingadditionalcapabilitiesinpixelreconstruction.Ourcontribution
can be summarized as follows:
– We propose a pixel value prediction (PVP) task to examine the ability of
current LLM-based Vision-Language Models in perceiving original image
details. This task is designed as a vision question-answer type and can be
easily integrated into existing VLM pipelines without additional design.
– By fine-tuning VLMs on the PVP task, our research shows that these mod-
els face challenges in accurately discerning pixel-level details. Performance
significantly improves when adapting their typically frozen vision encoder,
revealingthatthefrozenCLIPvisionencoderlimitstheseVLMsinperceiving
visual details.
– We incorporate PVP into the existing VLM pre-training pipeline and adapt
the vision encoder during training. Results show our pretraining helps VLMs
perform better in downstream vision-language tasks requiring the perception
ofvisualdetails,suchasimagesegmentationandvideogamedecisionmaking.4
2 Related Work
LLM-based Vision-Language Models. Recent advancements in Large Lan-
guage Model-based Vision-Language Models (LLM-VL models) have demon-
strated remarkable achievements in tasks requiring both visual comprehension
and language understanding [8,31,61]. The effectiveness of LLM-based VLMs
largely stems from the reasoning and generalization capabilities of Large Lan-
guage Models [11,39,51,52] trained on large-scale datasets. Recent studies have
explored the abilities of Large VLMs in visual grounding tasks [8,9,55] and
referring image segmentation [27,44]. However, these works mainly focus on
aligning different levels of semantic vision information and language. How these
Vision-Language Models interpret the original image and whether they can see
the original image details beyond semantic information is less investigated. To
address this gap, we propose a method to investigate the original vision detail
perception ability and design a self-supervised pre-training method to augment
theiroriginalimageperceptualcapabilities.Wevalidatethattheenhancedability
can boost performance in many downstream tasks requiring detailed vision and
language understanding.
Pixel Reconstruction as Pretraining. Image pixel reconstruction has been
exploredasaneffectivemethodforpre-trainingcomputervisionmodels[14,18,21].
Pre-training vision models with a reconstruction task also aid in vision-specific
tasks that require pixel-level understanding such as semantic segmentation [18],
class-agnostic segmentation [24] and depth estimation [4]. However, current
LLM-based VLM models primarily utilize vision encoders pre-trained through
vision-language contrastive learning, as these features can be well understood
by the Large Language Model [36]. Thus, it is less effective to simply plug a
vision encoder into VLM that is pre-trained separately on a reconstruction task.
Additionally, it remains unclear how to incorporate the reconstruction task into
thetrainingofVLMandwhetheritwillenhancetheentireVLM’sunderstanding
of visual details. Considering the general vision-language task paradigm, we
design the pixel reconstruction as the VQA task and update the entire VLM on
this task to improve visual detail understanding, rather than focusing solely on
the vision model.
VLMs on Referring Image segmentation. Referring Image segmentation
task [23,38] aims to segment a specific object based on a given sentence de-
scription. This task requires pixel-level vision detail and language understanding.
VisionLLM [55] considers segmentation masks as polygons sequence prediction
while needing expanded vocabulary for LLM decoder and extra vision decoder
for image tokenizer. Lisa [27] combines LLM and strong segmentation expert
SAM [24] to do complex instruction reasoning segmentation. In our method,
we do not involve any extra vision component besides the CLIP encoder, and
referring segmentation results are predicted directly from LLM. We show the
VLM can provide a precise pixel-level mask if it can see image detail better.
Video Games Playing by Large Language Models. Games play a crucial
role in AI research, requiring multiple abilities from AI models, such as high-level
planning and reasoning [58]. Recently, LLMs have been investigated for theirHow Well Can Vision Language Models See Image Details? 5
potential as player agents in many game applications due to their excellent rea-
soning ability [1,53,54,56]. Similarly, humans can play video games by watching
videos and understanding the game’s instructions. A recent work [56] utilized
human-writteninstructionsandLLMstoaccelerateReinforcementLearning(RL)
algorithms for Atari games. Video games, in particular, require both vision per-
ceptionfromvideosandlanguageunderstandingofgameinstructions.Specifically,
some video games necessitate detailed image analysis, such as Carracing [25]
and the Atari game Space Invaders. For instance, a model may need to control
steering as the car approaches a corner on the driving road. Thus, we use VLM
to play video games and show that VLM can achieve a higher score if the vision
perception ability is improved.
3 Method
We first introduce our method for investigating the image perception abilities
of current Vision-Language Models (VLMs). We then present the design of our
pre-training task, pixel reconstruction for Large Language Model (LLM)-based
VLMs. Following this, we outline the designs of our downstream tasks, including
referring image segmentation and video game playing.
3.1 Method for investigating image perception ability of VLMs.
We begin by examining the ability of Vision-Language Models (VLMs) to un-
derstand image details by engaging them in pixel reconstruction tasks, which
require the model to perceive images at the pixel level. To adapt this task for
VLMs, we conceptualize pixel reconstruction as a Visual Question Answering
(VQA) task. We prompt the VLM to give the pixel value at a specific location
(x,y) on the image, as illustrated in part b) of Fig. 1. In line with the task design
for Large Language Model (LLM)-based VLMs, we introduce a task identifier
[reconstruct] followed by the question:
<Img> < ImageFeature> </Img> [reconstruct] loc: [{x},{y}] rgb: }
The answer format is [r,g,b], where r, g, and b represent the RGB values,
respectively. Fine-tuning the current VLM for this task reveals that the LLM
can reconstruct only a blurry image when using vision embedding from original
CLIP, while only training the Large Language Model and the connection module
according to previous training paradigms. We find that the quality of pixel
reconstruction greatly improves when we also adapt the vision encoder during
the training process. The comparison results are displayed in Fig. 3.
3.2 Pixel Reconstruction Pre-training for VLMs
We incorporate PVP into Vision-Language Model (VLM) pretraining pipeline.
Our method aims to enhance the ability of current VLMs to understand detailed
visualinformationwithoutlosinggeneralvision-languageknowledge.Weintroduce6
pixel reconstruction as a new task for Visual Question Answering (VQA) and
include it, along with other vision-language tasks, in our model’s training. For
the additional tasks, we follow the complete set from the instructional training of
MiniGPTv2 [8], and we use the same datasets as those in [8]. We have created a
three-stage training approach to help our model better align with visual details.
The first stage aims to familiarize the VLMs with new pixel reconstruction
tasks. Following the previous training settings used in [8], we train only the
Large Language Model (LLM) and the connection modules in this stage, as we
find that directly unfreezing the vision encoder leads to catastrophic forgetting
(experiments are provided in the supplementary materials). In the second stage,
in addition to the LLM and connection modules, we adapt the vision encoder to
improvetheVLM’sabilitytounderstandvisualdetails.Inthelaststage,wefreeze
the vision encoder and reduce the sampling ratio for the pixel reconstruction
task to achieve a balance between lower-level details and high-level semantics in
the vision-language space. We utilize LoRA to efficiently train the LLM. After
three stages of pretraining, we evaluate our approach on downstream tasks such
as Referring Image Segmentation and Video Game Playing. Necessary details of
the pretraining settings are provided in the supplementary materials.
3.3 Referring Image Segmentation
Following a similar paradigm as pixel reconstruction, we consider Referring to
Image Segmentation as a VQA task. We ask VLM to provide the answer to the
question "Does this pixel location (x,y) contain a specific object described in
the referring sentence?" We use a task identifier [segmentation] and follow the
question:
<Img> < ImageFeature> </Img> [segmentation] {referring sentence} loc:
[{x},{y}] mask: }
The answer would be 0 or 1, and 0,1 represent the binary mask of this object
at location (x,y). We don’t use an extra decoder or special codebook for segmen-
tation. The prediction mask is directly generated by the Large Language Model
in VLM by examining vision features and language guidance. The performance
of Referring Image Segmentation can reflect the pixel-level vision understanding
ability of the VLM.
3.4 Video Games Playing
We pick two video games sourced from OpenAI Gym environment [6]: Carracing
GameandSpaceInvaders.WeconsiderplayingaVideoGameasaVideoQuestion
Answering task, where each short video contains N stacked images. Given one
observation (one short video), the model needs to predict actions based on the
action space of each game. We first design a general template for video game
playing in the following format:How Well Can Vision Language Models See Image Details? 7
Game Instruction: Observation
You are playing a Car Racing game.
<Objective> Drive this red car as fast as possible and avoid
leaving the road.
<Tips> Control steering and avoid pressing the accelerator and
turning at the same time.
Action Space: [Steering, Gas, Brake]: [(0,20), (0,20), (0,20)].
[CarRacing] Please select a value for each action within their
respective ranges, providing three values in total:
prediction: '[8,10,10]'
( means turn left with no gas and no brake)
You are playing a Space Invaders game.
<Objective> Destroy the space invaders by shooting your laser
cannon at them before they reach the Earth. Avoid getting hit
by space invaders.
<Tips> The invaders in the back rows are worth more points.
<Action Space> [0: NOOP, 1: FIRE, 2: RIGHT, 3: LEFT, 4:
RIGHTFIRE, 5: LEFTFIRE]
[Space Invaders] Choose an action from the Action Space:
prediction: '2’
(means right)
Fig.2: Examples of Game Playing by VLM.TheinputtotheVLMisthestacked
imagesandthegameinstructions.ThefirstrowshowsanexampleofplayingCarracing.
The second row shows the SpaceInvaders game. The number of stacked frames depends
ontheexpertmodelweused.Forexample,CarracingusestwoframesandSpaceInvaders
uses four.
<Img> <Image1Feature> </Img><Img> < Image2Feature> </Img> ...
<ImageNFeature> </Img> {game instruction} [game identify] choose an action from
Action Space: }
ImageNFeature is the vision features of N image. Game instruction is the
th
necessary information for playing this game, which contains the objective of
the player, game tips, and the action space of the game. Fig. 2 shows the
illustration of gaming playing using LLM-based VLM. We find one pre-trained
expert Reinforcement Learning (RL) model for each game provided by stable-
baselines3 [43] according to game scores. We collect the dataset consisting of
observationsandcorrespondingactionstakenbytheexpertmodel.Duringtesting,
we set the game seed to be different from the training environment. We consider
the fine-tuning process of VLM on game playing as imitation learning, while we
only use the same loss used in other VQA tasks. The output is directly generated
by the VLM without any interpretation or extra decoders.
Carracing Game. We choose CarRacing-v0, as it is a widely used version. We
chooseRecurrentPPOastheexpertmodelprovidedbystable-baselines3[43].This
model uses two stacked frames as single observation input, and takes an action
which is a vector containing three continuous values (C ,C ,C ) representing
1 2 3
(steering, gas, brake) respectively. We first map these three values to the discrete
values, detail is provided in supplementary material. The data we collected is the8
Table 1: The training datasets used for our Pre-training.
Datatypes Dataset
Reconstruction COCOcaption [29]
Caption COCOcaption [29],TextCaptions [47]
REC RefCOCO[23],RefCOCO+[60],RefCOCOg[34],VisualGenome [26]
REG RefCOCO[23],RefCOCO+[60],RefCOCOg[34]
VQA GQA[22],VQAv2[15],OCR-VQA[37],OK-VQA[35],AOK-VQA[45]
MultimodalinstructionLLaVAdataset[32],Flickr30k[40],Multi-taskconversation[8]
Languagedataset UnnaturalInstructions[19]
observation and the action value taken by this expert model of each step. We
collect a dataset containing 30 games (with different game seeds), in total 28585
observations (stacked images), and corresponding actions.
SpaceInvaders. We use the SpaceInvadersV4 version and choose a pre-trained
DQN agent to play this game. This model uses four stacked frames as a single
observation input and chooses one action from the following action space ([0:
NOOP, 1: FIRE, 2: RIGHT, 3: LEFT, 4: RIGHTFIRE, 5: LEFTFIRE]). We
directly document each observation and the action taken by the expert model.
30 games are collected, containing 24618 observations and corresponding actions.
4 Experiments
In this section, we present experimental settings and results. We first show the
reconstruction results of baseline VLM and our method. Then we report our
results on two types of downstream tasks to demonstrate how much benefit VLM
can get from pixel reconstruction pretraining. The first one is referring image
segmentation and the second one is video game playing. We demonstrate both
quantitative and qualitative results. In the last section, we show our pre-trained
model can also achieve comparable results on other vision Language tasks and
ownsanextrapixelreconstructionability.Theablationstudyforthepre-training
strategy is provided.
Implementation details. We use MiniGPT-v2 [8] as our VLM base and utilize
their pre-trained weights to initialize our model. To investigate how adapting
the vision encoder affects the performance of VLM on the pixel prediction task,
we first obtain a VLM trained with the first stage of pretraining introduced in
our method. Then, we continue to train the model using two strategies: freezing
the ViT and adapting the vision encoder. We compare the qualitative and
quantitative image reconstruction performance of these two models after the
second stage to measure the pixel prediction quality. For pretraining details, we
employ the three-stage training strategy. Following a similar setting as described
in [8], we utilize LoRA [20] to accelerate our training. The entire pretraining
stage involves training the Large Language Model and the connection module
via low-rank adaptation, with the LoRA rank set to 64. The vision encoder
is only adaptable in the second stage, without using LoRA. The input imageHow Well Can Vision Language Models See Image Details? 9
(a) GroundTruth (b) Our method (c) Baseline (d) GroundTruth (e) Our method (f) Baseline
Fig.3: Qualitative results of Reconstruction (a) and (d) are the GroundTruth
for reconstruction. (b) and (e) is the reconstructed image of our method. (c) and (f)
are the baseline result without CLIP-Vit adaptation. Compared with the baseline, our
method reconstructs images with more details. The averaged Reconstruction error of
our method and baseline on these 10 images are 6.67, and 24.56, respectively.
resolution is 448×448. The reconstruction target is the downsampled image at a
resolutionof64×64.ThecompletedatasetusedinpretrainingisshowninTab.1.
The entire pretraining stage comprises approximately 3.6M pixel reconstruction
questions by randomly sampling locations from images in the COCO caption
dataset [29]. For downstream tasks, we directly fine-tune our baseline model and
our model after three-stage pretraining, the PAE-LaMM model, and report their
performance. In the referring segmentation task, we fine-tune both models on
Referring Expression Comprehension (REC) and referring segmentation data.
Consequently, our fine-tuned model acquires both localization and pixel-level
understandingcapabilities.Asforvideogameplaying,weutilizetheofficialgame
environment sourced from the OpenAI Gym library [7] and employ RL-Zoo3 [42]
for data collection and as the game interface for inference.
Training and Hyperparameters. We utilize a cosine learning rate and the
AdamW optimizer to train our model. All models are trained on 4xA100 GPUs.
The batch size of the pixel reconstruction task is 64, 16, and 64 in each stage,
respectively. For Referring Image Segmentation, we set the batch size to 64 for
segmentation and 24 for localization. For CarRacing and Space Invaders, we set
the batch sizes to 8 and 3, respectively. More detailed hyperparameters will be
provided in the supplementary material.10
Table 2: Abliation study for adapting Vit. We report the Reconstruction Error,
average referring expression comprehension (REC) on refcoco, refcoco+ and refcocog.
Also,wereporttheVQAtaskperformance.Thebestperformanceforeachbenchmarkis
indicated in bold. We report top-1 accuracy for other VQA tasks: GQA [22], VSR [30],
IconVQA [33] and VizWiz [16]
adapt ViT RE ↓ Average REC ↑ GQA↑ VSR ↑ IconVQA ↑ VizWiz ↑ HM ↑
✗ 20.38 77.2 55.5 56.7 49.7 53.7 57.6
✓ 6.65 82.3 56.2 56.7 49.6 53.22 57.6
4.1 Evaluation on pixel reconstruction
Fig. 3 shows the qualitative comparison results between using our method and
the baseline. The baseline model can only reconstruct a blurry contour without
many visual details, indicating that the VLM cannot see enough original image
detail from the original CLIP vision feature. In contrast, our method helps the
VLM reconstruct a better result with more detail. Additionally, we use mean
reconstructionerror(RE)toreportthequantitativeresultofpixelreconstruction
as shown in the following formula:
RE(x,y)=|pt_rgb(x,y)−gt_rgb(x,y)|; (1)
(cid:32)(cid:80)W (cid:80)H RE(x,y)(cid:33)
RE(I)= x=0 y=0 (2)
H ×W ×255
where RE(x,y) is the reconstruction error for a single pixel, and RE(I) is the
summed error across all locations of the entire image I. Here, pt_rgb(x,y) rep-
resents the reconstruction prediction at location (x,y), and gt_rgb(x,y) is the
ground truth pixel value. H,W are the height and width of reconstruction image,
and 255 is the normalization factor. The evaluation set comprises 409,600 pixel
reconstruction questions sampled from images in the test set of Conceptual
Captions [46] and requires the VLM to provide the RGB value for each ques-
tion. Tab. 2 shows the quantitative results on this evaluation set; our method
achieves a lower average RE and significantly outperforms the VLM without
adaptation, 6.65 vs 20.38. This demonstrates that our method enhances the
VLM’s perception of visual details, enabling it to reconstruct better images.
Additionally, we observed that during adaptation, the model does not lose its
general vision-language knowledge, as reflected in other VQA tasks. Furthermore,
the performance in referring expression comprehension is improved, attributed
again to the enhanced awareness of visual details.
4.2 Results on Downstream tasks
After pretraining, we validate the effectiveness of our method on downstream
tasks that require visual detail and language understanding.
Referring Image Segmentation For referring image segmentation, we re-
port our results on a subset of RefCOCO [23], RefCOCO+ [60], and Ref-
COCOg [34]. There are eight datasets in total: three for RefCOCO [23], threeHow Well Can Vision Language Models See Image Details? 11
Table 3: Comparison result on referring segmentation. For each dataset, we
test 100 referring sentences. We report the official evaluation metrics mask cIoU.
RefCOCO RefCOCO+ RefCOCOg
Models avg val testA testB val testA testB val test
MiniGPT-v2 62.42 72.63 65.30 65.07 66.22 60.68 58.37 54.94 56.18
PAE-LaMM 72.61 83.93 81.63 74.37 72.18 72.14 65.00 68.05 63.55
third from left
guy all the
way right in front
right screen
a brown couch
in a living room
closest
light with person
(a) Refer sentence (b) Input Image (c) GroundTruth (d) Baseline (e) Our Method
Fig.4: Qualitative results of Referring Image Segmentation. We first use the
referring localization ability of the fine-tuned model to generate a bounding box (bbox)
for the referring object, and then predict the segmentation mask inside the bbox.
for RefCOCO+ [60], and two for RefCOCOg [34]. From each dataset, we select
100 data points as our subset. Following previous work [27] on referring image
segmentation, we use cIoU as our evaluation metric, which is defined as the
cumulative intersection over the cumulative union. Tab. 3 shows our method
outperforming the baseline model by a large margin. For example, our method
improves baseline performance by 7.3, 16.5, and 9.5 on val, testA, testB of Ref-
COCO, respectively. Because our fine-tuning uses both referring localization and
segmentationdata,weenablethemodeltofirstpredictthebboxaccordingtothe
referent sentence and then output the segmentation mask inside the bbox during
inference.Fig.4showsthequalitativeresultsofthebaselineandourmethod;our
method achieves better localization (the second row of Fig. 4) and pixel-level
segmentation results (the bottom row of Fig. 4). The significant improvement
in referring image segmentation demonstrates the advantages brought by pixel
reconstruction pretraining.12
Expert
Model
Baseline
Our
Method
Fig.5: Qualitative results of Carracing. We show the game observation from
different models, including the expert Reinforcement Learning (RL) Model, Baseline
Model, and Our Method, all playing under the same game seed. These images depict
how each model behaves when controlling the car and approaching the same corner.
Table 4: Results on Carracing and SpaceInvaders.Wereporttheaveragereward.
The results for each game are collected in 15 rounds.
Carracing SpaceInvaders
Method
mean reward mean reward
Expert Model 853.91 476.33
MiniGPT-v2 465.36 152.33
PAE-LaMM 535.90 232.67
Results on Video Game Playing. We first report the game scores shown in
Tab. 4. The score is obtained from the RL-Zoo3 library [42]. Here, we briefly
introduce how the score is computed; details can be found in OpenAI Gym [7].
For CarRacing, the score is computed as -0.1 for every frame and +1000/N
for every track tile visited [7]. For Space Invaders, players can gain points by
destroying space invaders [7]. For both games, a higher score indicates better
performance. As shown in Tab. 4, for the CarRacing game, our method achieves
a better mean reward over the baseline model with an 70.54 score gap. For the
Space Invaders game, our method also outperforms the baseline with a score
over 80.34. Additionally, we present the qualitative results of the CarRacing
game in Fig. 5, demonstrating how each model behaves when controlling the
car and approaching the same corner. The expert model performs left steering
and braking, and our method also predicts the action that makes the correct
turn when approaching a sharp curve on the track. However, the baseline model
fails to perform the correct action. Both the quantitative and qualitative results
support our method in enhancing vision detail understanding.How Well Can Vision Language Models See Image Details? 13
Table 5: Results on multiple VQA tasks. We report top-1 accuracy for each task.
Extra Ability indicates whether the model incorporates pixel-level reconstruction and
visual localization capability. The best performance for each benchmark is indicated in
bold blue, and the second best in blue.
ExtraAbility VQAtasks
Method
Reconstruct Grounding GQA VSR IconVQA VizWiz HM
Flamingo-9B [2] ✗ ✗ - 31.8 - 28.8 57.0
BLIP-2(13B) [28] ✗ ✗ 41.0 50.9 40.6 19.6 53.7
InstructBLIP(13B) [12] ✗ ✗ 49.5 52.1 44.8 33.4 57.5
MiniGPT-4(13B) [61] ✗ ✗ 30.8 41.6 37.6 - -
LLaVA(13B) [32] ✗ ✗ 41.3 51.2 43.0 - -
Shikra(13B) [9] ✗ ✓ - - - - -
Qwen-VL(7B) [5] ✗ ✓ 59.3 - - 35.2 -
MiniGPT-v2(7B) [8] ✗ ✓ 60.1 62.9 51.5 53.6 58.8
PAE-LaMM(7B) ✓ ✓ 57.7 59.2 49.7 56.4 57.2
Table 6: State-of-the-art comparison of LLM-based Methods on referring
expressioncomprehensiontasks.Wereporttheofficialevaluationmetrics:precision
at IoU threshold 0.5 for referring (box) localization. Numbers for other methods are
taken from the original publications. The best performance for each benchmark is
indicated in bold blue, and the second best in blue.
Extra RefCOCO RefCOCO+ RefCOCOg Average
Models Decoder val testA testB val testA testB val test
VisionLLM[55] ✗ 86.7 - - - - - - - -
Shikra-7B [9] ✗ 87.0 90.6 80.2 81.6 87.4 71.1 82.3 82.2 82.8
Ferret-7B [59] ✗ 87.5 91.4 82.5 80.8 87.4 73.1 83.9 84.8 83.93
Qwen-VL-7B [5] ✗ 89.36 92.26 85.34 83.12 88.25 77.21 85.58 85.48 85.83
MiniGPT-v2 [8] ✗ 88.06 91.29 84.30 79.58 85.52 73.32 84.19 84.31 83.82
PixelLLM [57] ✓ 89.8 92.2 86.4 83.2 87.0 78.9 84.6 86.0 86.01
PAE-LaMM ✗ 88.55 91.1 86.2 83.3 87.30 78.7 85.2 86.6 85.87
4.3 Pixel Reconstruction Pre-training for VLM
In Tab. 5, we show that our pre-trained model, PAE-LaMM, achieves results
comparabletostate-of-the-artmodelsonmultipleVQAtasks.Additionally,ithas
the ability to reconstruct pixels, which helps the VLM see image details. After
fine-tuning for downstream referring sentence tasks, PAE-LaMM also achieves
competitive results compared to other large VLM models on referring expression
comprehension tasks without any extra model design, as shown in Tab. 6. We
then present an ablation study in Tab. 7 to analyze the impact of vision encoder
adaptation and the pixel reconstruction task on vision detail awareness. We
first train three models using three-stage pretraining under different settings.
These models, along with a baseline model, are then fine-tuned on the referring
segmentation task. The second row of Tab. 7 uses the same pretraining as PAE-
LaMMwhileexcludingthePVPtask,andthethirdrowfreezesthevisionencoder
during the entire pretraining and includes the PVP task. The results show that
both adapting ViT without the PVP task and fine-tuning the model on the PVP
taskwithoutadaptingViTcanimprovethebaselineresult,withtheimprovement
gap being almost the same. However, combining both adaptations leads to the
best results, significantly outperforming each approach individually.14
Table 7: Ablation on referring segmentation. For each dataset, we test 100
referring sentences. We report the official evaluation metrics mask cIoU.
RefCOCO RefCOCO+ RefCOCOg
Models avg val testA testB val testA testB val test
PAE-LaMM 72.61 83.93 81.63 74.37 72.18 72.14 65.00 68.05 63.55
(-)PVPtask 67.31 77.67 73.41 69.92 67.87 62.42 60.25 63.45 63.45
(-)adaptViT 67.28 80.28 70.87 62.51 72.80 64.91 62.51 62.77 61.62
Baseline 62.42 72.63 65.30 65.07 66.22 60.68 58.37 54.94 56.18
Table 8: Results of Multi-stage pretraining .WereporttheReconstructionError,
average referring expression comprehension (REC) on refcoco, refcoco+ and refcocog.
Also, we report the VQA task performance. The best performance for each benchmark
is indicated in bold.
adaptViT RE↓ AverageREC↑ GQA↑ VSR↑ IconVQA↑ VizWiz↑ HM↑
Stage1 ✗ 20.27 76.8 55.5 56.3 49.5 40.1 55.2
Stage2 ✓ 6.65 82.3 56.2 56.7 49.6 53.22 57.6
Stage3 ✗ 6.59 83.2 57.7 59.2 49.7 56.4 57.2
Intheend, Tab.8showseachstageperformanceofourthreestagepretraining.
Stage 3 achieves the best result in pixel reconstruction, REC, and most VQA
tasks.Thereconstructionerrorissignificantlyreducedafterstage2(ViTadapting
stage), which supports the notion that adapting ViT helps the VLM see more
visual details. Additionally, the referring localization ability can be significantly
improved by stage 2. Stage 3 mainly improves VQA tasks and REC performance
while maintaining reconstruction ability. We provide more ablation studies in
Supplementary material to demonstrate the training strategies of each stage.
5 Conclusion
Inthispaper,wemainlyinvestigatethequestion,"HowWellCanVisionLanguage
Models See Image Details?". We propose a method to examine vision detail
perception ability by querying VLMs to predict the pixels of input images given
pixel locations. We find that VLMs struggle to reconstruct the original image
using original CLIP vision features, and this issue can be significantly improved
by adapting the vision encoder. We design a pixel reconstruction pretraining to
enhance VLM vision detail perception ability. Then, we show the strong benefits
brought by the ability to see image details in referring segmentation and playing
video games. Additionally, we demonstrate that our pre-trained model, PAE-
LaMM, does not lose general vision-language knowledge while possessing vision
detail perception ability, which suggests our method may potentially be used for
many vision-language applications, especially those requiring vision details.How Well Can Vision Language Models See Image Details? 15
References
1. Akata, E., Schulz, L., Coda-Forno, J., Oh, S.J., Bethge, M., Schulz, E.: Playing
repeatedgameswithlargelanguagemodels.arXivpreprintarXiv:2305.16867(2023)
2. Alayrac, J.B., Donahue, J., Luc, P., Miech, A., Barr, I., Hasson, Y., Lenc, K.,
Mensch, A., Millican, K., Reynolds, M., Ring, R., Rutherford, E., Cabi, S., Han,
T., Gong, Z., Samangooei, S., Monteiro, M., Menick, J., Borgeaud, S., Brock,
A., Nematzadeh, A., Sharifzadeh, S., Binkowski, M., Barreira, R., Vinyals, O.,
Zisserman, A., Simonyan, K.: Flamingo: a visual language model for few-shot
learning. In: Oh, A.H., Agarwal, A., Belgrave, D., Cho, K. (eds.) Advances in
Neural Information Processing Systems (2022)
3. Anokhin,I.,Demochkin,K.,Khakhulin,T.,Sterkin,G.,Lempitsky,V.,Korzhenkov,
D.:Imagegeneratorswithconditionally-independentpixelsynthesis.In:Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp.
14278–14287 (2021)
4. Assran,M.,Duval,Q.,Misra,I.,Bojanowski,P.,Vincent,P.,Rabbat,M.,LeCun,Y.,
Ballas, N.: Self-supervised learning from images with a joint-embedding predictive
architecture (2023)
5. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou, J.:
Qwen-vl: A versatile vision-language model for understanding, localization, text
reading, and beyond. arXiv preprint arXiv:2308.12966 (2023)
6. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J.,
Zaremba, W.: Openai gym. arXiv preprint arXiv:1606.01540 (2016)
7. Brockman, G., Cheung, V., Pettersson, L., Schneider, J., Schulman, J., Tang, J.,
Zaremba, W.: Openai gym. arXiv preprint arXiv:1606.01540 (2016)
8. Chen,J.,Zhu,D.,Shen,X.,Li,X.,Liu,Z.,Zhang,P.,Krishnamoorthi,R.,Chandra,
V.,Xiong,Y.,Elhoseiny,M.:Minigpt-v2:largelanguagemodelasaunifiedinterface
for vision-language multi-task learning. arXiv preprint arXiv:2310.09478 (2023)
9. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleashing
multimodalllm’sreferentialdialoguemagic.arXivpreprintarXiv:2306.15195(2023)
10. Cherti, M., Beaumont, R., Wightman, R., Wortsman, M., Ilharco, G., Gordon, C.,
Schuhmann, C., Schmidt, L., Jitsev, J.: Reproducible scaling laws for contrastive
language-imagelearning.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition. pp. 2818–2829 (2023)
11. Chiang, W.L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S.,
Zhuang, Y., Gonzalez, J.E., Stoica, I., Xing, E.P.: Vicuna: An open-source chatbot
impressinggpt-4with90%*chatgptquality(March2023),https://vicuna.lmsys.
org
12. Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.:
InstructBLIP: Towards general-purpose vision-language models with instruction
tuning. In: Thirty-seventh Conference on Neural Information Processing Systems
(2023)
13. Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X.,
Cao,Y.:Eva:Exploringthelimitsofmaskedvisualrepresentationlearningatscale.
In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition. pp. 19358–19369 (2023)
14. Gao, P., Ma, T., Li, H., Lin, Z., Dai, J., Qiao, Y.: Convmae: Masked convolution
meets masked autoencoders. arXiv preprint arXiv:2205.03892 (2022)
15. Goyal,Y.,Khot,T.,Summers-Stay,D.,Batra,D.,Parikh,D.:Makingthevinvqa
matter: Elevating the role of image understanding in visual question answering. In:16
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 6904–6913 (2017)
16. Gurari, D., Li, Q., Stangl, A.J., Guo, A., Lin, C., Grauman, K., Luo, J., Bigham,
J.P.: Vizwiz grand challenge: Answering visual questions from blind people. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 3608–3617 (2018)
17. Haydarov, K., Muhamed, A., Lazarevic, J., Skorokhodov, I., Elhoseiny, M.: Hyper-
CGAN: Text-to-image synthesis with hypernet-modulated conditional generative
adversarial networks (2022)
18. He, K., Chen, X., Xie, S., Li, Y., Dollár, P., Girshick, R.: Masked autoencoders are
scalable vision learners. In: Proceedings of the IEEE/CVF conference on computer
vision and pattern recognition. pp. 16000–16009 (2022)
19. Honovich, O., Scialom, T., Levy, O., Schick, T.: Unnatural instructions: Tuning
language models with (almost) no human labor. arXiv preprint arXiv:2212.09689
(2022)
20. Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L.,
Chen, W.: Lora: Low-rank adaptation of large language models. arXiv preprint
arXiv:2106.09685 (2021)
21. Hu, R., Debnath, S., Xie, S., Chen, X.: Exploring long-sequence masked autoen-
coders. arXiv preprint arXiv:2210.07224 (2022)
22. Hudson, D.A., Manning, C.D.: Gqa: A new dataset for real-world visual reasoning
andcompositionalquestionanswering.In:ProceedingsoftheIEEE/CVFconference
on computer vision and pattern recognition. pp. 6700–6709 (2019)
23. Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.: Referitgame: Referring to
objects in photographs of natural scenes. In: Proceedings of the 2014 conference on
empirical methods in natural language processing (EMNLP). pp. 787–798 (2014)
24. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment anything. arXiv preprint
arXiv:2304.02643 (2023)
25. Klimov, O.: Carracing-v0. URL https://gym. openai. com/envs/CarRacing-v0
(2016)
26. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,
Kalantidis,Y.,Li,L.J.,Shamma,D.A.,etal.:Visualgenome:Connectinglanguage
and vision using crowdsourced dense image annotations. International journal of
computer vision 123, 32–73 (2017)
27. Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., Jia, J.: Lisa: Reasoning
segmentation via large language model. arXiv preprint arXiv:2308.00692 (2023)
28. Li,J.,Li,D.,Savarese,S.,Hoi,S.:Blip-2:bootstrappinglanguage-imagepre-training
with frozen image encoders and large language models. In: Proceedings of the 40th
International Conference on Machine Learning. ICML’23, JMLR.org (2023)
29. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: Computer Vision–
ECCV2014:13thEuropeanConference,Zurich,Switzerland,September6-12,2014,
Proceedings, Part V 13. pp. 740–755. Springer (2014)
30. Liu, F., Emerson, G., Collier, N.: Visual spatial reasoning. Transactions of the
Association for Computational Linguistics 11, 635–651 (2023)
31. Liu, H., Li, C., Li, Y., Lee, Y.J.: Improved baselines with visual instruction tuning.
arXiv preprint arXiv:2310.03744 (2023)
32. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. Advances in neural
information processing systems 36 (2024)How Well Can Vision Language Models See Image Details? 17
33. Lu, P., Qiu, L., Chen, J., Xia, T., Zhao, Y., Zhang, W., Yu, Z., Liang, X., Zhu,
S.C.: Iconqa: A new benchmark for abstract diagram understanding and visual
language reasoning. arXiv preprint arXiv:2110.13214 (2021)
34. Mao,J.,Huang,J.,Toshev,A.,Camburu,O.,Yuille,A.L.,Murphy,K.:Generation
and comprehension of unambiguous object descriptions. In: Proceedings of the
IEEE conference on computer vision and pattern recognition. pp. 11–20 (2016)
35. Marino, K., Rastegari, M., Farhadi, A., Mottaghi, R.: Ok-vqa: A visual question
answeringbenchmarkrequiringexternalknowledge.In:ProceedingsoftheIEEE/cvf
conference on computer vision and pattern recognition. pp. 3195–3204 (2019)
36. Merullo, J., Castricato, L., Eickhoff, C., Pavlick, E.: Linearly mapping from image
to text space. arXiv preprint arXiv:2209.15162 (2022)
37. Mishra, A., Shekhar, S., Singh, A.K., Chakraborty, A.: Ocr-vqa: Visual question
answeringbyreadingtextinimages.In:2019internationalconferenceondocument
analysis and recognition (ICDAR). pp. 947–952. IEEE (2019)
38. Nagaraja, V.K., Morariu, V.I., Davis, L.S.: Modeling context between objects for
referring expression understanding. In: Computer Vision–ECCV 2016: 14th Euro-
pean Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings,
Part IV 14. pp. 792–807. Springer (2016)
39. OpenAI: Introducing chatgpt. https://openai.com/blog/chatgpt (2022)
40. Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J., Lazeb-
nik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for richer
image-to-sentence models. In: Proceedings of the IEEE international conference on
computer vision. pp. 2641–2649 (2015)
41. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G.,
Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models from
natural language supervision. arXiv preprint arXiv:2103.00020 (2021)
42. Raffin, A.: Rl baselines3 zoo. https://github.com/DLR-RM/rl-baselines3-zoo
(2020)
43. Raffin, A., Hill, A., Gleave, A., Kanervisto, A., Ernestus, M., Dormann, N.: Stable-
baselines3: Reliable reinforcement learning implementations. Journal of Machine
Learning Research 22(268), 1–8 (2021), http://jmlr.org/papers/v22/20-1364.
html
44. Ren, Z., Huang, Z., Wei, Y., Zhao, Y., Fu, D., Feng, J., Jin, X.: Pixellm: Pixel
reasoning with large multimodal model. arXiv preprint arXiv:2312.02228 (2023)
45. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., Mottaghi, R.: A-okvqa: A
benchmark for visual question answering using world knowledge. In: European
Conference on Computer Vision. pp. 146–162. Springer (2022)
46. Sharma, P., Ding, N., Goodman, S., Soricut, R.: Conceptual captions: A cleaned,
hypernymed,imagealt-textdatasetforautomaticimagecaptioning.In:Proceedings
of the 56th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers). pp. 2556–2565 (2018)
47. Sidorov, O., Hu, R., Rohrbach, M., Singh, A.: Textcaps: a dataset for image
captioning with reading comprehension. In: European Conference on Computer
Vision. pp. 742–758. Springer (2020)
48. Sitzmann, V., Martel, J., Bergman, A., Lindell, D., Wetzstein, G.: Implicit neural
representations with periodic activation functions. Advances in neural information
processing systems 33, 7462–7473 (2020)
49. Skorokhodov, I., Ignatyev, S., Elhoseiny, M.: Adversarial generation of continuous
images. In: Proceedings of the IEEE/CVF conference on computer vision and
pattern recognition. pp. 10753–10764 (2021)18
50. Tancik,M.,Srinivasan,P.,Mildenhall,B.,Fridovich-Keil,S.,Raghavan,N.,Singhal,
U., Ramamoorthi, R., Barron, J., Ng, R.: Fourier features let networks learn high
frequency functions in low dimensional domains. Advances in Neural Information
Processing Systems 33, 7537–7547 (2020)
51. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient
foundation language models. arXiv preprint arXiv:2302.13971 (2023)
52. Touvron,H.,Martin,L.,Stone,K.,Albert,P.,Almahairi,A.,Babaei,Y.,Bashlykov,
N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation and
fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)
53. Tsai, C.F., Zhou, X., Liu, S.S., Li, J., Yu, M., Mei, H.: Can large language models
play text games well? current state-of-the-art and open questions. arXiv preprint
arXiv:2304.02868 (2023)
54. Wang,G.,Xie,Y.,Jiang,Y.,Mandlekar,A.,Xiao,C.,Zhu,Y.,Fan,L.,Anandkumar,
A.: Voyager: An open-ended embodied agent with large language models. arXiv
preprint arXiv:2305.16291 (2023)
55. Wang,W.,Chen,Z.,Chen,X.,Wu,J.,Zhu,X.,Zeng,G.,Luo,P.,Lu,T.,Zhou,J.,
Qiao, Y., et al.: Visionllm: Large language model is also an open-ended decoder for
vision-centric tasks. Advances in Neural Information Processing Systems 36 (2024)
56. Wu, Y., Fan, Y., Liang, P.P., Azaria, A., Li, Y., Mitchell, T.M.: Read and reap the
rewards: Learning to play atari with the help of instruction manuals. Advances in
Neural Information Processing Systems 36 (2024)
57. Xu, J., Zhou, X., Yan, S., Gu, X., Arnab, A., Sun, C., Wang, X., Schmid, C.: Pixel
aligned language models. arXiv preprint arXiv:2312.09237 (2023)
58. Yannakakis, G.N., Togelius, J.: Artificial intelligence and games, vol. 2. Springer
(2018)
59. You, H., Zhang, H., Gan, Z., Du, X., Zhang, B., Wang, Z., Cao, L., Chang, S.F.,
Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity. arXiv
preprint arXiv:2310.07704 (2023)
60. Yu, L., Poirson, P., Yang, S., Berg, A.C., Berg, T.L.: Modeling context in referring
expressions. In: Computer Vision–ECCV 2016: 14th European Conference, Amster-
dam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14. pp. 69–85.
Springer (2016)
61. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-
language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 (2023)