SLIM-RAFT: A Novel Fine-Tuning Approach to Improve
Cross-Linguistic Performance for Mercosur Common Nomenclature
Vin´ıciusDiOliveira1,2 a,YuriFac¸anhaBezerra1 b,LiWeigang1 c PedroCarvalhoBrom1,3 d and
VictorRafaelR.Celestino4 e
1TransLab,UniversityofBrasilia,Brasilia,FederalDistrict,Brazil
2SecretaryofEconomy,Brasilia,FederalDistrict,Brazil
3InstitutoFederaldeBrasilia,Brasilia,FederalDistrict,Brazil
4LAMFO,DepartmentofAdministration,UniversityofBrasilia,Brasilia,FederalDistrict,Brazil
vinidiol@gmail.com,weigang@unb.br
Keywords: Fine-tuning,HS,LargeLanguageModel,NCM,PortugueseLanguage,RetrievalAugmentedGeneration
Abstract: Natural language processing (NLP) has seen significant advancements with the advent of large language
models (LLMs). However, substantial improvements are still needed for languages other than English, es-
peciallyforspecificdomainsliketheapplicationsofMercosurCommonNomenclature(NCM),aBrazilian
HarmonizedSystem(HS).Toaddressthisgap,thisstudyusesTeenyTineLLaMA,afoundationalPortuguese
LLM,asanLLMsourcetoimplementtheNCMapplicationprocessing. Additionally,asimplifiedRetrieval-
AugmentedFine-Tuning(RAFT)technique,termedSLIM-RAFT,isproposedfortask-specificfine-tuningof
LLMs. This approach retains the chain-of-thought (CoT) methodology for prompt development in a more
concise and streamlined manner, utilizing brief and focused documents for training. The proposed model
demonstratesanefficientandcost-effectivealternativeforfine-tuningsmallerLLMs,significantlyoutperform-
ingTeenyTineLLaMAandChatGPT-4inthesametask.AlthoughtheresearchfocusesonNCMapplications,
themethodologycanbeeasilyadaptedforHSapplicationsworldwide.
1 INTRODUCTION ingcorpus,predominantlyEnglish-based(90%),with
only a small portion (10%) dedicated to other lan-
The widespread application of Generative Artifi- guages,includingFrench,Spanish,andPortuguese.
cial Intelligence (GenAI) systems has significantly While LLMs offer cross-language processing ca-
advanced the development of artificial intelligence pabilities, enabling some understanding of related
(Schulhoffetal.,2024;Radosavovicetal.,2024). On languages, particularly within the Latin language
one hand, more sophisticated large language models group, these capabilities are often insufficient for
(LLMs),suchasChatGPT,possessmultilingualcapa- morenuancedlanguageprocessingtasks(Souzaetal.,
bilities to process multimodal information and have 2020).Thisisespeciallytruefortasksinvolvingtech-
become key drivers for AI applications (Weigang nicalterminologyandenterprise-specificprivacydata
etal.,2022).However,formostusers,effectiveuseof requirements. The limited proportion of pre-trained
thesemodelsrequiresenhancingpromptengineering PortuguesecorpusinmodelslikeLLaMAhighlights
skills. Ontheotherhand,open-sourcelargelanguage these constraints, revealing significant limitations in
models,suchasLLaMA3,canlocallyfine-tunemod- theirlanguageprocessingcapabilitiesfornon-English
els,cateringtospecificsecurityandflexibilityneeds. languages.
Nonetheless, non-English users may encounter lim- Another way to tackle these downstream tasks
itations due to the composition of LLaMA’s train- could be to use significantly smaller LLMs and per-
form fine-tuning by retraining the model parame-
a https://orcid.org/0000-0002-1295-5221 ters,asdemonstratedintheTeenyTineLLaMAmodel
b https://orcid.org/0000-0000-0000-0000 (Correˆaetal.,2024),whichistheapproachaddressed
c https://orcid.org/0000-0003-1826-1850 inthiswork. Thisapproachconsidersthatsubsequent
d https://orcid.org/0000-0002-1288-7695 taskshaveawell-definedandreducedscopebutpre-
e https://orcid.org/0000-0002-1288-7695
4202
guA
7
]LC.sc[
1v63930.8042:viXraFigure1:TheSLIM-RAFTdiagram
servestheessenceofusingLLMs. response mechanism refines the model’s overall per-
Retrieval-Augmented Generation (RAG) ad- formance. Despite this technique’s positive results,
dresses several critical challenges inherent in LLMs producingthetrainingbaseaccordingtothechainof
(Lewis et al., 2020; Gao et al., 2023). These thought model is difficult and expensive, as another
challenges encompass hallucinations, outdated powerfulLLMisrequired.
knowledge, and non-transparent, untraceable rea- ThisworkfocusesontheMERCOSURCommon
soning processes. By incorporating information Nomenclature (NCM) code system, which is used
fromexternaldatabases,RAGsignificantlyenhances byallmembercountriesofMERCOSUR.TheNCM
the accuracy and credibility of generated content, code system is derived from the Harmonized Com-
particularly for knowledge-intensive tasks. This modity Description and Coding System, commonly
approachfacilitatescontinuousupdatesofknowledge known as the ”Harmonized System” (HS), which is
and the integration of domain-specific information, a multipurpose international product nomenclature
effectivelymergingtheintrinsiccapabilitiesofLLMs developed by the World Customs Organization and
with the extensive, dynamic repositories of external used by more than 200 countries. The NCM code is
databases. Consequently, RAG provides a robust widelyusedinBrazilandismandatoryforalldomes-
solution for improving the performance and relia- tic sales operations and international import and ex-
bility of language models, making it an invaluable porttransactions. DuetothecomplexityoftheNCM
approachacrossadiversearrayofapplications. system, which involves product classification, spec-
In the pursuit of enhancing the performance ifications, and tax ranges, advanced language pro-
of LLMs in downstream applications, Retrieval- cessing capabilities are required. Ordinary English-
Augmented Fine-Tuning (RAFT) presents numerous Portuguese translation is inadequate for this process,
significant benefits (Zhang et al., 2024; Warnakula- as it requires a sophisticated understanding of Por-
suriya and Hapuarachchi, 2024). While pre-training tuguese and precise handling of proper nouns. Pre-
LLMs on extensive corpora of textual data are now liminary experiments indicate that using only Chat-
standard practice, integrating new, time-sensitive, or GPTorTeenyTineLLaMA,aPortugueseLLM,isin-
domain-specific knowledge remains a complex chal- sufficientforeffectivelyprocessingthesetasks.
lenge. TraditionalmethodologiessuchasRAG-based Thesimpletaskofclassifyingcommoditydescrip-
prompting or fine-tuning are frequently employed; tionscanbehandledwithsimplertechniqueslikeneu-
however, RAFT offers a more optimal approach for ralnetworksbyahierarchicalsequencelearning(Du
embedding such knowledge. RAFT distinguishes it- et al., 2021). This study aims not merely to clas-
selfbytrainingthemodeltodisregarddistractordoc- sify product descriptions but to extract the inherent
uments—those that do not contribute to answering knowledgewithintheNCMcodesystem. Themodel
the query. By concentrating on pertinent documents can be enriched with specialised knowledge by elu-
andaccuratelycitingthesequencesnecessaryforre- cidating the semantic relationships between product
sponding to questions, RAFT substantially enhances descriptionsandtheirrespectiveclassifications. Such
themodel’sreasoningabilities.Thischain-of-thought enhancements hold considerable potential for com-merce,compliance,andtaxationapplications. demonstrating its remarkable performance despite a
Thisstudyintroducesthe“SimplifiedLogicalIn- comparatively smaller parameter count. This initial
telligent Model” - SLIM Retrieval-Augmented Fine- successpavedthewayforsubsequentiterations,with
Tuning (SLIM-RAFT) model to effectively address LLaMA2and3(Touvronetal.,2023b;Meta,2024)
thechallengesassociatedwiththeNCMcode. Itcon- furtherrefiningthemodels. Theselaterversions,par-
siderstheconstraintsofdevelopingacomprehensive ticularlytailoredfordialogueapplications,introduced
training base akin to the original RAFT model. The fine-tuned languagemodels optimizedfor chat inter-
SLIM-RAFTmodelcanbeappliedtoanyelementde- faces. Consequently,theseearlyadvancementslaida
scriptionandmulti-classificationproblems. robust foundation for future progress in natural lan-
A distinguishing feature of the SLIM-RAFT guageprocessing.
modelisitsuseofasignificantlysmallerLLMsource Although ample data exists for training
thantraditionalmodels.Specifically,theTeenyTineL- transformer-based language models in Portuguese,
LaMA model, comprising only 160 million param- native speakers can readily discern limitations in
eters, was utilised in constructing SLIM-RAFT as the text generation and performance of pre-trained
the fine-tuned LLM. The results achieved by our models primarily derived from English language
model substantially outperform those of ChatGPT 4 data. In recent years, there has been a rising interest
intheproposedchallenge: ChatGPT4scored4.5/10, in the development and enhancement of large-scale
whereas SLIM-RAFT achieved an impressive score languagemodelstailoredtothePortugueselanguage.
of8.67/10. TheSLIM-RAFTschemecanbeseenin This trend has been propelled by pioneering and in-
Figure1. novative research efforts, as evidenced by numerous
Thisworkisorganizedasfollows: recentcontributionstothefield.
In European Portuguese (PT-PT), the initiative
• Section2presentstheworksdirectlyrelatedtothe
knownasGlo´rIA(Lopesetal.,2024)meritsparticular
ideasproposedinconstructingSLIM-RAFT;
attention. Thisprojectinvolvesatraineddecoderlan-
• Section 3 shows the structure and functioning of guagemodelmeticulouslyconstructedfromacorpus
theHSandNCMcodes; comprising35billiontokensfromvarioussources.
• Section 4 properly demonstrates the construction For Brazilian Portuguese (PT-BR), the first rele-
oftheSLIM-RAFTmodel; vantLLMencounteredwasSabia´ (Piresetal.,2023).
Thisinitiativeunderscoresthedevelopmentofrobust
• Section5presentstheresultsobtainedinthecom-
andscalablelanguagemodelsforthePortugueselan-
parative evaluations of the models and discusses
guage. Leveragingadvancedmachinelearningarchi-
whatwasfounded;
tectures, these models have been instrumental in ad-
• Section6concludestheworkandsuggestsfuture vancing natural language processing applications in
workrelatedtothepresentedmodel. BrazilianPortuguese.
The Cabrita model (Larcher et al., 2023) was
launchedasalow-costalternativefortrainingLLMs.
2 RELATED WORKS The authors posited that their methodology could be
extendedtoanytransformer-likearchitecture.Tosub-
This section presents related work in two areas: stantiatetheirhypothesis, theyundertookcontinuous
1) research on the implementation of LLMs with pre-training exclusively on Portuguese text using a
Portuguese as the primary language, and 2) stud- 3-billion-parameter model known as OpenLLaMA.
ies on Retrieval-Augmented Generation (RAG) and ThiseffortculminatedinthecreationofopenCabrita
Retrieval-AugmentedFine-Tuning(RAFT). 3B.Remarkably,openCabrita3Bincorporatesanovel
tokenizer, significantly reducing the number of to-
kens necessary to represent the text. Subsequently,
2.1 PortugueseLLM’s
in a comparable approach, a new study introduced
a model predicated on LLaMA 2, designed specifi-
The introduction of LLaMA (Touvron et al., 2023a)
callyforhandlingpromptsinPortuguese.Thismodel,
asanopenandefficientfoundationallanguagemodel
namedBode(Garciaetal.,2024),isavailableintwo
markedasignificantmilestoneintheevolutionoflan-
versions: one with 7B and 13B parameters. Both
guageprocessing. Withmodelsspanning7billionto
models used the LoRa (Hu et al., 2021) fine-tuning
65 billion parameters, LLaMA underscored the via-
method over an open-source LLM. This technique
bility of training cutting-edge models exclusively on
preserves the original parameters intact while intro-
publicly available datasets. Notably, the LLaMA-
ducing a new terminal layer atop the model, which
13B model surpassed GPT-3 in various benchmarks,is subsequently trained to achieve the desired fine- RAG and RAFT were designed to confront the
tuningoutcome. complexityoftailoringLLMstospecializeddomains.
A noteworthy recent publication entitled Withintheserealms,theemphasispivotsfromgeneral
“TeenyTinyLlama: Open Source Tiny Language knowledgereasoningtooptimizingaccuracyvis-a`-vis
Models Trained in Brazilian Portuguese” (Correˆa ameticulouslydefinedarrayofdomain-specificdoc-
etal.,2024)offersavaluableperspectiveondevelop- uments.
ing compact, open source language models tailored
to Brazilian Portuguese. Despite their reduced 2.3 NCMDataSet
scale, these models hold significant potential for
democratizing access to natural language processing The ELEVEN data set, ELEctronic inVoicEs in the
technology, particularly within resource-limited Portuguese language (Di Oliveira et al., 2022), was
communities. meticulously curated to furnish researchers and en-
Collectively, these works signify substantial ad- trepreneurs with a repository of product descriptions
vancementsinimplementinglanguagemodelsforthe categorizedundertheMercosurCommonNomencla-
Portuguese language. They underscore the diversity ture (NCM). This extensive database comprises over
ofmethodologiesandtheabundanceofresourcesthat a million meticulously labelled records, each scru-
bolster research and applications in natural language tinized by taxation experts. These descriptions are
processing and related fields. These ongoing initia- short texts, limited to 120 characters, and extracted
tives are poised to continue influencing the future of from authentic electronic invoices documenting pur-
language technology for Portuguese speakers glob- chaseandsalestransactions.
ally. Labelled datasets are a rare commodity, yet they
provide indispensable resources for applications re-
2.2 Retrieval-AugmentedApproach liantonsupervisedlearning(VanEngelenandHoos,
2020). TheELEVENdatasethasservedasacorner-
RetrievalAugmentedGeneration-RAG(Lewisetal., stoneforseveralnoteworthyacademicendeavours:1)
2020)representsatransformativeapproachtoenrich- thedevelopmentofaCNN-basedsystemforclassify-
ing the quality and pertinence of generated content inggoods(Kieckbuschetal.,2021);2)thecreationof
by integrating external insights derived from exten- data visualization tools aimed at identifying outliers
sive datasets or repositories of knowledge. By em- anddetectingfraud(Marinhoetal.,2022);and3)the
bedding pertinent external knowledge sources into establishmentofaframeworkutilizingautomaticen-
the generation process, RAG is designed to elevate coders to cluster short-text data extracted from elec-
the coherence, factual precision, and overall utility tronicinvoices,therebyenhancinganomalydetection
of generated text. This methodology proves advan- withinnumericfields(Schulteetal.,2022).
tageousindomainsnecessitatingpreciseandcontex-
tually nuanced content generation, such as question-
answering,summarizing,andadvanceddialoguesys- 3 HS AND NCM CODES
tems. Bycontrollingretrievalmechanisms,RAGen-
suresthattheresultantoutputsareprovidedwithhigh
In the dynamic realm of international trade, customs
accuracyandcontextualrelevance,therebyadvancing
brokers,exporters,andimportersconfrontthecritical
the frontiers of natural language processing applica-
taskofaccuratelyclassifyinggoodsundertheHarmo-
tions.
nized System (HS) Code, which underpins the Mer-
In pursuit of enhancing the precision of model
cosurCommonNomenclature(NCM)code(Valenc¸a
responses and mitigating the phenomenon of LLM
etal.,2023).
hallucinations, a novel approach has emerged: Re-
The Harmonized System (HS) is the founda-
trievalAugmentedFine-Tuning(RAFT)(Zhangetal.,
tion for customs tariffs and the compilation of in-
2024). ThismethodologyintegratestheRAGframe-
ternational trade statistics in over 200 countries and
workwithfine-tuningtechniques, empoweringmod-
economies. Beyond these primary functions, the HS
elsnotonlytoacquiredomain-specificknowledgebut
isemployedforvariousotherpurposes,includingthe
alsotoadeptlyretrieveandcomprehendexternalcon-
monitoringofcontrolledgoods, establishingrulesof
textscrucialfortaskexecution. RAFTintroducesthe
origin,andfacilitatingtradenegotiations. Itisalsoa
idea of chain-of-thought prompting for building the
crucial component of fundamental customs controls
fine-tuning data set. This prompting technique en-
andprocedures(WCO,2024).
ablesthemodel’sanswerstoshowitsreasonlinewith
The MERCOSUR Common Nomenclature -
asequenceofarguments,enhancingitsexplicability.
NCM code system (Nomenclatura Comun do Mer-cosul in Portuguese) is utilized by all member coun- tradeagreements.Misclassificationcanleadtopenal-
tries of MERCOSUR: Argentina, Brazil, Paraguay, ties, delays in customs clearance, and substantial fi-
Uruguay, and Venezuela (MERCOSUR, 2024b). In nanciallosses(Yadav,2023).
Brazil,includingtheNCMcodeisalegalrequirement The MERCOSUR Common Nomenclature code
oneveryelectronicinvoiceissued. Consequently, all system adds two digits to the HS structure, making
commercial transactions, whether domestic or inter- it possible to address new items and their respective
national,mustincorporatethiscode(Brazil,2016). subgroups,asseeninTable3. Thissystemallowsfor
The Harmonized Commodity Description and amorepreciseandspecializedclassificationofprod-
Coding System, generally called the “Harmonized ucts,enablingmoreitemstobecatalogueddistinctly
System” or simply “HS,” is a multipurpose interna- byaccountingfortheirspecificcharacteristics.
tionalproductnomenclaturedevelopedbytheWorld
Table 3: Structure of the NCM Codes (MERCOSUR,
Customs Organization (WCO, 2018). The HS struc-
2024a).
ture is shown in Table 1. The NCM, structured hi-
2digit(01-97) Chapter
erarchically in the same way as HS, consists of nu-
4digit(01.01-97.06) Heading
merical codes assigned to various products, thereby
6digit(0101.21-9706.00) Subheading
facilitating their identification and categorization in
trade agreements and customs processes (MERCO- 7digit(0101.21.1-9706.00.9) Item
SUR,2024a). 8digit(0101.21.10-9706.00.90) Sub-item
Table1:StructureoftheHSCodes(WCO,2018).
Thetaskofclassifyingproductdescriptionswithin
2digit(01-97) Chapter the HS or NCM system has been explored in aca-
4digit(01.01-97.06) Heading demic literature (Du et al., 2021; Kieckbusch et al.,
6digit(0101.21-9706.00) Subheading 2021; Schulteetal., 2022). Techniquessuchasneu-
ral networks with hierarchical learning and convolu-
Table2showsanHSlistcutout,showingthedis- tionalneuralnetworks(CNNs)havebeeneffectively
tinctioninclassificationcodesforfreshanddriedap- employed to address this task. Nevertheless, certain
ples. Whilethisdifferentiationmayappeartrivial,in challengesassociatedwiththisdomainremaininsuf-
animportoperationwhereeachtypeofappleissub- ficientlyaddressed.
jecttodifferenttaxtreatments,anerrorincodedesig- Onenotablechallengeisthevariabilityinproduct
nationcanresultinsignificantrepercussions. Onone descriptions: the same product can be described in
hand, the seller faces the risk of tax penalties, while multipleways,andcontext-dependentsynonymsand
ontheother,customsauthoritiesmayencounteraloss abbreviations further complicate classification. This
ofrevenue. complexity makes using LLMs a compelling alter-
native for interpreting product descriptions. Beyond
Table2:Headings08.08and08.13intheHS(WCO,2018).
simpleclassification,LLMsofferthepotentialtoex-
08.08 Apples,pearsandquinces,fresh.
tract deeper knowledge by identifying relationships
0808.10 -Apples
betweenproducts. Table4showssomeabbreviations
0808.30 -Pears
thatcanbeeasilyfound.
0808.40 -Quinces
Table4:AbbreviationExamples.
... ...
-Fruit,dried,otherthanthatof English
08.13 headings08.01to08.06;mixtures Coc. 2L =Coca-Cola2Liters
ofnutsordriedfruitsofthisChapter. P.W.Rice =ParboiledWhiteRice
0813.10 -Apricots Portuguese
0813.20 -Prunes =Fraldadescarta´vel
Fr. Desc.
0813.30 -Apples (Disposablediaper)
0808.40 -OtherFruit =ToalhadePapelFolhaDupla
T.Pap. FDupla
-Mixturesofnutsordriedfruitsof (DoublePlyPaperTowel)
0808.50
thisChapter French
EDT =EaudeToilette
EDP =EaudeParfum
Accurate classification of goods is paramount, as
itdirectlyimpactstaxation,compliancewithspecific
regulations such as sanitary and phytosanitary stan- Context is crucial in language processing, as it
dards, and eligibility for benefits under international can help resolve ambiguities that often arise whenconsideringabbreviationsinisolation. Thisiswhere knowledgebase.Forinstance:1)element“a”belongs
TRANSFORMER-based algorithms shine, as their tosetA;2)setAiscontainedwithinsetB;3)conse-
ability to understand context is key (Vaswani et al., quently,“a”belongstosetB.Thenextsubsectionwill
2017). For example, the abbreviation “fr.” in Por- explainhowitwasdone.
tuguese could refer to “fralda” (diaper), as shown in
Table 4, or it could mean “fruta” (fruit). However, 4.1 FTDatabaseandPrompting
whentheterm“desc”(meaning“descarta´vel”ordis-
posable) follows, the context effectively resolves the Atheoreticalexampleofalistofargumentsforcon-
ambiguitybetween“fralda”and“fruit”. structingthesimplifiedchainofthought:
Therefore, developing LLMs capable of dealing
• Doc. 1:
withNCMandHScodesisusefulinmanyfields,es-
peciallycomplianceandtaxinspection. a∈A (1)
Import and export companies, as well as trading
• Doc. 2:
companies in general, are interested in issuing their
invoicescorrectlyundercurrentregulations. Errorsin A⊆B (2)
product descriptions or their classification according
toHSandNCMcodesmayresultinfinancialpenal- • Doc. 3: Consequently,
tiesorevenlegalprohibitionsoftrading. ∴a∈B (3)
Customs authorities carefully observe the correct
indicationofHSandNCMcodes,asthetaxtreatment This was an application of the training base built
of those products will be defined through this clas- forfine-tuningwithintheideaofthesimplifiedchain-
sification. Misguided indications in these codes can of-thought. See below for a generic example of a
beinterpretedasanintentiontocircumventtherules prompt:
to pay less taxes. Of course, fines or other sanctions
[
maybeappliedtothoseresponsibleforthecommer-
{
cialtransactioninvolved.
"content":
Therefore, usingAImodelscanimprovecontrol-
[context 1 ]....\n
ling and correcting the issuance of invoices and re-
[context 2 ]...\n
lated documents (Kieckbusch et al., 2021; Marinho
[context 3 ]...\n
etal.,2022).
[...] ... \n
[context n]... \n
\n
4 SLIM-RAFT MODEL Answer the following question
using information from the
The SLIM-RAFT model simplifies RAFT logically previous context: question",
and intelligently. Just as RAFT maintains the RAG "role": "user"
initsdesignedform,SLIM-RAFTalsomaintainsthe },
RAGmechanisminitsstructure,seeFigure1. {
The preceding sections have elucidated that con- "content": "response + reasoning
structingthetrainingbaseintheoriginalRAFTmodel based on context",
is an expensive endeavour, frequently necessitating "role": "assistant"
the deployment of another powerful LLM. This sub- }
stantialcostispredominantlyattributabletotwofea- ]
tures of RAFT: the chain-of-thought reasoning and
AnexpertintheNCMcodedevelopedaseriesof
theinclusionofirrelevantdocuments. Whilethecon-
question-and-answersets,completewiththeirrespec-
cept of learning to disregard irrelevant documents is
tive arguments. Utilising the open version of Chat-
both valid and logical within the context of RAFT’s
GPT3.5,numerousvariationsofthesequestionswere
objectives, it is not a requisite for all applications.
generated. Subsequently, a Python script was em-
Thisinsightpromptedtheexclusionofthisfeaturein
ployedtocreatethousandsofpairs[question+argu-
thedevelopmentofSLIM-RAFT.
ment, answer + argument], wherein information de-
SLIM-RAFTretainsthechain-of-thoughtconcept
rived from the NCM database populated the generic
initsfine-tuningprocess,albeitsimplified. Insteadof
questions.
using lengthy texts or entire documents as input, the
An example of a generic question used in con-
approachemployslogicalargumentsderivedfromthe
structingtheNCMbaseisasfollows:[ 4.2 SourceLLMandFine-tuning
{
"content": The LLM source chosen for this work was Teeny
[product {{product}} has NCM code TinyLLaMA - TTL, available in two sizes: 460 mil-
{{NCM}}], \n lionand160millionparameters.Twoprimarycharac-
[which refers to the category teristicsofTTLguidedthisselection:itscompactsize
{{category}}] \n andthetrainingonacorpusinBrazilianPortuguese.
\n While other source models trained in Brazilian
Answer the following question using Portugueseexist,asdiscussedinSection2,theirsub-
information from the previous stantial size can make fine-tuning costly, even when
context: What is the ’category’ of employing optimised techniques such as LoRa (Hu
the ’product’ {{product}}?" et al., 2021). In contrast, the compact size of TTL
"role": "user" made our fine-tuning process more cost-effective,
}, demonstratingitspracticalityandpotentialforwider
{ application.
"content": "the product {{product}} TheFine-tuningprocessadjustsallmodelparam-
has the NCM code {{NCM}}, which eters. The reduced size of TTL facilitated this task.
refers to the category The codes employed were adapted from those pro-
{{category}}, videdbytheauthorsoftheoriginalTTLpaper(avail-
"role": "assistant" ableonGitHub1)withminormodifications.
} AllcodesdevelopedforSLIM-RAFTareaccessi-
] ble on SLIM-RAFT’s GitHub repository. Both TTL
models,160millionand460millionparameters,were
AsamplerecordinPortugueseasitwasrecorded
fine-tunedtocreateSLIM-RAFT.The160millionpa-
isprovidedintheAppendixofthiswork.
rameter version was used in SLIM-RAFT, while the
Then, for building a data training base in SLIM-
460millionparameterversionwasusedforcompara-
RAFTmode,therearethreesteps:
tiveanalysisduringthefinalmodelevaluation.
1. A domain expert creates a small question-and- The SLIM-RAFT GitHub repository 2 is a valu-
answerset,e.g.“Whatisthecategoryoftheprod- able resource that provides the codes used in this
uct’product’?”; study. Thisopenaccessnotonlyallowsthecommu-
2. Construct question-and-answer set variations (an nitytoreproduceandassessthisexperimentbutalso
LLM could be used), e.g. “Could you specify encouragesfurthercollaborationandpotentialcontri-
the category to which the product ’product’ be- butionstonaturallanguageprocessing.
longs?”;
3. Populate de question-and-answer set mask. e.g.
5 RESULTS AND DISCUSSION
“What is the category of the product ’fresh ap-
plepackage’?”, “Couldyouspecifythecategory
to which the product ’fresh apple package’ be- The results were evaluated through a comparative
longs?” analysisoftheresponsesdeliveredbythetestedmod-
els. Threeothermodelswerechosenforthiscompar-
The total number of records in the data training
ison: TeenyTinyLLaMA 460m, TeenyTinyLLaMA
basewillbe:
460m + NCM fine-tuning, and ChatGPT 4. In the
end, fourmodelsweretestedandevaluatedbyChat-
N=q×v×n (4)
GPT4.0:
Where q is the number of question-and-answer
• Model 1: TeenyTinyLlama with tiny460M with-
createdbythedomainexpert,visthenumberofvari-
outfine-tuningonthedataset,definedasTTL.
ations from each question-and-answer unit, and n is
thetotalnumberofsamplesfromtheNCMdatabase. • Model2: ChatGPT4.0,definedasGPT.
As delineated earlier, a notable distinction be- • Model3:TeenyTinyLlamawithfine-tuningonthe
tweenSLIM-RAFTandtheoriginalRAFTliesinthe NCMdataset,definedasNCM-TTL.
simplified approach to constructing the fine-tuning
base while preserving the chain-of-thought method-
ology.
1https://github.com/Nkluge-correa/TeenyTinyLlama
2https://github.com/yurifacanha/ncmrag• Model 4: TinyLlama with fine-tuning on the Another pertinent scenario involves cases of at-
NCM dataset and using SLIM RAFT, defined as temptedtaxevasion. Forinstance,iftheproductfresh
SLIM-RAFT. apple package is exempt from taxation while apple
juice package is subject to tax, it could be mislead-
The model’s responses were then submitted to
ingly described in a tax document as apple j. pack.
ChatGPT-4, which compared the outputs produced
butassignedtheNCMcodeforfreshapplespackage,
withthedesiredoutputs. Toensureimpartialityinthe
which is tax-exempt. If this discrepancy goes unde-
evaluation,itisimportanttonotethatChatGPT-4was
tectedbycustomsauthorities,itcouldresultinaloss
not informed of which model each response referred
ofrevenueduetotheuncollectedtax.
to.
The SLIM-RAFT model can effectively help a
systemforcontrollingandinspectdocumentsregard-
5.1 ResultsPresentation
ing the NCM Code misuse. But, because of its re-
ducedsize,thecapabilityofextractingtherightques-
The evaluation used 100 questions and answers not
tion embedded in a bigger context is limited. Let us
included in the fine-tuning training base. These 100
considerthefollowingexample:
questionswerepresentedtovariousmodels,andtheir
Portuguese - Fui na padaria e comprei um suco
responses were recorded and compared. ChatGPT-4
de laranja, percebi que na nota fiscal aparecia um
assessedthequalityofeachresponse,scoringitona
co´digo chamado NCM, mas estava com a impressa˜o
scale from 0 to 10. The final score for each model
borrada. Qualseriaoco´digoimpresso?
representstheaverageofthescoresassignedtoeach
English - at the bakery and bought some orange
response. Table 5 presents the results of this evalua-
juice, I noticed that a code called NCM appeared on
tion. ItisclearthatModel4ofSLIMRAFTachieved
theinvoice, buttheprintwasblurry. Whatwouldbe
thebestscoreof8.63withastandarddeviationof2.30
theprintedcode?
acrossthe100Q/As.
When presented with this query, our model may
Table5:Scoreresultsbetweenthefourmodels. not discern the central issue, namely, “What is the
NCM category for orange juice?” Integrating an ad-
Model Aver. St. Dev. Min. Max.
ditionalLLMintothesystemcouldmitigatethislim-
TTL 0.2 0.98 0 5
itation.
NCM-TTL 4.71 3.53 0 10
The Few-shot prompting technique (Ma et al.,
GPT 4.5 1.39 0 5
2024; Gu et al., 2021) can enable other large lan-
SLIM-RAFT 8.63 2.30 0 10
guagemodels(LLMs)toreformulatequeries,thereby
adapting the context to enhance comprehension by
Table 6 provides a clear comparison of the re-
the smaller LLM integrated within the SLIM-RAFT
sponsesgeneratedbyallmodels,withthedesiredre-
model. Thisapproachisshownabove.
sponseservingasthebenchmarkforevaluation. This Portuguese:
benchmarkresponsesetsastandardagainstwhichthe
Possuo um modelo capaz de responder quest˜oes
models’performancecanbemeasured. diretas, mas n˜ao consegue responder quando
ela n˜ao vem nos padr˜oes em que foi treinado.
5.2 LLMJustification O padr˜ao de pergunta correto ´e "Qual a
categoria NCM correta para o produto: A?",
onde A ´e a descric¸˜ao do produto.
Itisessentialtounderscorethepotentialutilityofan
Sua miss˜ao ´e reformular os prompts para
LLMspecializedinthistypeoftask,asitextendsbe-
que meu modelo entenda:
yond mere classification. Whereas a straightforward Exemplo 1:
input-output classification system is confined to spe- Pergunta original
cificsubjectsandinputformats, anLLMsystemcan na padaria e comprei um suco de laranja,
extract semantic knowledge from the training base percebi que na nota fiscal aparecia um
c´odigo chamado NCM, mas estava com a
anddemonstratesflexibilityinhandlingvariousinput
impress˜ao borrada. Qual seria o c´odigo
formats.
impresso?
Answeringasimpledirectquestionlike“Whatis pergunta reforumulada: Qual a categoria
theNCMcodefortheproductfreshapplepackage”is NCM correta para o produto: suco de laranja?
notenough. Thequestioncancomeinseveralforms Exemplo 2:
or be embedded in a larger context, for example: “I Pergunta original
Sou um importador de produtos cosm´eticos e
don’t know the NCM code for fresh apple package,
vou preencher um documento fiscal para a
canyouhelpme?”.
importac¸˜ao do produto "TINT MAXTON 10.0 UN".Table6:ComparisonofQ/AresultsoffourmodelsinbothEnglishandPortuguese
Q/AandModels Samplequestion-Portuguese Samplequestion-English
QualacategoriaNCMcorretaparaoproduto:
WhatisthecorrectNCMcategoryforthe
LIMPADORVEJAPERF.NATPERF.SUAVE
product:CLEANERVEJAPERF.NATPERF.
1LT?Respondaapenasadescric¸a˜oda
SOFT1LT?Respondonlytothecategory
Prompting categoria.Casona˜osaibarespondaa
description.Ifyoudon’tknow,answerthe
categoriaquemaisseaproxima.Na˜oadicione
categorythatcomesclosest.Donotaddany
qualquerqualqueroutrocomenta´rioa`sua
othercommentstoyouranswer.
resposta.
Outrosagentesorgaˆnicosdesuperf´ıcie Otherorganicsurfaceagents(exceptsoaps);
(excetosabo˜es);preparac¸o˜estensoativas, surfactantpreparations,washingpreparations
preparac¸o˜esparalavagem(inclu´ıdasas (includingauxiliarywashingpreparations)and
preparac¸o˜esauxiliaresparalavagem)e cleaningpreparations(includingmultipurpose
preparac¸o˜esparalimpeza(inclusivemultiuso preparationsandcleaners),whetherornot
ExpectedAnswer
elimpadores),mesmocontendosaba˜o, containingsoap,excepttheproducts
excetoosprodutosdescritosnosCEST describedinCEST11.001.00,11.004.00,
11.001.00,11.004.00,11.005.00e11.006.00; 11.005.00and11.006.00;inpackagingwith
emembalagemdeconteu´doinferiorouiguala acontentoflessthanorequalto50litersor
50litrosou50kg. 50kg.
Na˜oconsigorespondercombasenas
Iamunabletorespondbasedonthe
informac¸o˜esfornecidas.Noentanto,posso
informationprovided.However,Icanprovide
TeenyTinyLLa- fornecerumaexplicac¸a˜ogeraldecomo
ageneralexplanationofhowtocategorize
MA categorizarprodutosNCM,incluindo
NCMproducts,includingspecificcategories
categoriasespec´ıficasesuasrespectivas
andtheirrespectivecategories.
categorias.
ChatGPT4.0 Preparac¸o˜esparalimpeza Cleaningpreparations
AcategoriaNCMcorretae´:Agentes
ThecorrectNCMcategoryis:Organicsurface
orgaˆnicosdesuperf´ıcie(excetosabo˜es);
agents(exceptsoaps);surfactantpreparations,
preparac¸o˜estensoativas,preparac¸o˜espara
washingpreparations(includingauxiliary
TeenyTinyLLa- lavagem(incluindoaspreparac¸o˜esauxiliares
washingpreparations)andcleaning
MAFTPTNCM paralavagem)epreparac¸o˜esparalimpeza,
preparations,whetherornotcontainingsoap,
mesmoquecontenhamsaba˜o,excetoasda
otherthanthoseofheading34.01.-
posic¸a˜o34.01.-Preparac¸o˜esacondicionadas
Preparationspackagedforretailsale
paravendaaretalho
oprodutoLIMPADORVEJAPERF.NATPERF.
SUAVE1LTpossuicategoria:Sabo˜es,agentes TheCLEANERVEJAPERF.NATPERF.SOFT
orgaˆnicosdesuperf´ıcie,preparac¸o˜espara 1LTproducthasacategory:Soaps,organic
lavagem,preparac¸o˜eslubrificantes,ceras surfaceagents,washingpreparations,
artificiais,ceraspreparadas,produtosde lubricatingpreparations,artificialwaxes,
conservac¸a˜oelimpeza,velaseartigos preparedwaxes,conservationandcleaning
semelhantes,massasoupastasparamodelar, products,candlesandsimilararticles,
”cerasparaodontologia”ecomposic¸o˜espara modelingmassesorpastes,”dentalwaxes”
SLIM-RAFT
odontologiaa`basedegesso.-Agentes andplaster-baseddentalcompositions.-
orgaˆnicosdesuperf´ıcie(excetosabo˜es); Organicsurfaceagents(exceptsoaps);
preparac¸o˜estensoativas,preparac¸o˜espara surfactantpreparations,washingpreparations
lavagem(incluindoaspreparac¸o˜esauxiliares (includingauxiliarywashingpreparations)and
paralavagem)epreparac¸o˜esparalimpeza, cleaningpreparations,whetherornot
mesmoquecontenhamsaba˜o,excetoasda containingsoap,otherthanthoseofheading
posic¸a˜o34.01.---Preparac¸o˜es 34.01.---Preparationsputupforretailsale
acondicionadasparavendaaretalhoPerdi minha planilha de c´odigos NCM, ent˜ao
pergunto qual NCM usar neste caso? 1) I am a merchant who has just started a
Pergunta reforumulada: Qual a categoria NCM business. I will sell beer and do not know
correta para o produto: TINT MAXTON 10.0 UN? Which NCM code should be used on the invoice?
Can you help me?
Agora ´e sua vez, fac¸a o mesmo para as 2) My son went to the market and bought a
perguntas abaixo: drink, leaving the invoice on the living room
table, which read "V. ITAL. CORBELLI PRIMITIVO
1) Eu sou um comerciante que acabou de TTO 750 ML". What drink did my son buy? And
iniciar as atividades. Vou vender cerveja e what is the NCM code?
n˜ao sei qual c´odigo NCM lanc¸ar na nota fiscal. 3) Knowing that fresh fruits are exempt from
Pode me ajudar? taxes and that industrialised fruit juices are
2) Meu filho foi no mercado e comprou uma taxed, I ask if the product Dell Vale Orange
bebida deixando a nota fiscal em cima da is exempt or taxed? And what is its NCM code?
mesa da sala, onde estava escrito "V. ITAL.
ChatGPToutputEnglishtranslation:
CORBELLI PRIMITIVO TTO 750 ML". Qual bebida
meu filho comprou? E qual o c´odigo NCM? Here are the questions rephrased to the
3) Sabendo que frutas frescas s˜ao isentas the appropriate format for your model:
de impostos e que sucos de frutas 1. Rephrased question: What is the correct NCM
industrializado s˜ao tributados, pergunto se category for the product: beer?
o produto Dell Vale Laranja ´e isento ou 2. Rephrased question: What is the correct NCM
tributado? E qual o seu c´odigo NCM? category for the product: V. ITAL. CORBELLI
PRIMITIVO TTO 750 ML?
ChatGPToutput:
3. Rephrased question: What is the correct NCM
Aqui est˜ao as perguntas reformuladas para o category for the product: Dell Vale Orange?
formato adequado ao seu modelo:
1. Pergunta reformulada: Qual a categoria NCM Nonetheless, it is important to acknowledge that
correta para o produto: cerveja? ourmodel,givenitscurrentlimitedsize,maynotfully
2. Pergunta reformulada: Qual a categoria NCM comprehend all contexts. The objective, however, is
correta para o produto: V. ITAL. CORBELLI
toexpandthemodelasmoreresourcesbecomeavail-
PRIMITIVO TTO 750 ML?
able,therebyenhancingitscapacityandperformance;
3. Pergunta reformulada: Qual a categoria NCM
asthemodelbecomeslarger,theneedforintegration
correta para o produto: Dell Vale Laranja?
withanothermodelwillbesuppressed.
Englishtranslation:
I possess a model capable of answering direct 5.3 Limitations
questions, but it struggles to respond when
they do not conform to the patterns in which
The SLIM-RAFT model is a prototype developed to
it was trained. The correct question format is
"What is the correct NCM category for the illustrate the original RAFT methodology’s simplifi-
product: A?", where A is the product cation and propose its application within the NCM
description. Your task is to rephrase the domain. Consequently, this model is a highly spe-
prompts so that my model can understand them: cialisedtooltailoredforitsdesignatedtask.
Example 1:
ItisnotrecommendedforuseinChatBotapplica-
Original question:
tions, as TeenyTinyLLaMA (TTL) creators have ad-
I was at the bakery and bought orange juice,
but I noticed that the invoice had a smudged vised against employing the TTL 160m model for
code called NCM. What would be the printed such purposes. The TTL 460m model is recom-
code? mendedforchatbotfunctionalities.
Rephrased question: What is the correct NCM A simplified chain-of-thought approach is em-
category for the product: orange juice?
ployed when constructing the training base for fine-
Example 2:
tuning. However,it’scrucialtorememberthatthein-
Original question:
volvementofadomainexpertisbeneficialandneces-
I am a cosmetic product importer and need to
fill out a tax document for the import of the sary for developing the reference lines of reasoning,
product "TINT MAXTON 10.0 UN." I lost my NCM highlightingtheirreplaceableroleofhumanexpertise
codes spreadsheet, so I’d like to know which inthisprocess.
NCM to use in this case.
Rephrased question: What is the correct NCM
category for the product: TINT MAXTON 10.0 UN?
Now it’s your turn; do the same for the
questions below:6 CONCLUSIONS Di Oliveira, V., Weigang, L., and Rocha Filho, G. P.
(2022).Elevendata-set:Alabeledsetofdescrip-
The SLIM-RAFT model demonstrated significantly tionsofgoodscapturedfrombrazilianelectronic
superior performance to ChatGPT 4 in interpreting invoices. InWEBIST,pages257–264.
andclassifyingproductdescriptionsaccordingtothe
Du, S., Wu, Z., Wan, H., and Lin, Y. (2021). Hsco-
NCMcode.
denet: Combining hierarchical sequential and
This outcome indicates that a smaller-scale LLM
globalspatialinformationoftextforcommodity
with specific domain knowledge can surpass a more
hs code classification. In Pacific-Asia Confer-
powerful LLM in specialized tasks, provided it is
enceonKnowledgeDiscoveryandDataMining,
appropriatelyadjustedandtrainedwhilemaintaining
pages676–689.Springer.
lowexecutioncosts.
Gao, Y., Xiong, Y., Gao, X., Jia, K., Pan, J., Bi, Y.,
Thetechniqueforsimplifyingtheconstructionof
Dai,Y.,Sun,J.,andWang,H.(2023). Retrieval-
the chain-of-thought, as proposed in SLIM-RAFT,
augmented generation for large language mod-
not only reduces costs but also proves to be a vi-
els: Asurvey. arXivpreprintarXiv:2312.10997.
ablealternativefordevelopingspecializedLLMswith
highaccuracy. Garcia, G. L., Paiola, P. H., Morelli, L. H., Can-
Since NCM coding is used not only for manag- dido, G., Ju´nior, A. C., Jodas, D. S., Afonso,
ing, transporting, paying, and taxing various goods L., Guilherme, I.R., Penteado, B.E., andPapa,
in the import and export trade between MERCO- J. P. (2024). Introducing bode: A fine-tuned
SUR countries but also for most tax bills for goods, large language model for portuguese prompt-
commodities, and restaurants in the Brazilian mar- basedtask. arXivpreprintarXiv:2401.02909.
ket, the practical value of this research is substan-
Gu,Y.,Han,X.,Liu,Z.,andHuang,M.(2021). Ppt:
tial. The findings provide convenience for govern-
Pre-trainedprompttuningforfew-shotlearning.
mentdepartmentsinvolvedinimportandexport,tax-
arXivpreprintarXiv:2109.04332.
ation, banks, transportation, and manufacturers. Ad-
ditionally,sinceover200countriesusetheHSsystem Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y.,
forimportandexporttrade, theLLM-NCMsolution Wang,S.,Wang,L.,andChen,W.(2021). Lora:
proposedinthisarticlecanalsofacilitatetheeffective Low-rank adaptation of large language models.
promotionofLLM-HSapplicationsworldwide. arXivpreprintarXiv:2106.09685.
Future research will focus on applying SLIM- Kieckbusch, D. S., Geraldo Filho, P., Di Oliveira,
RAFT to larger LLMs, such as LLaMA 3, empha- V., and Weigang, L. (2021). Scan-nf: A cnn-
sising multilingual tasks. Additionally, comparisons based system for the classification of electronic
withotherfine-tuningtechniques,suchasLoRa,will invoices through short-text product description.
beexplored. InWEBIST,pages501–508.
Larcher, C., Piau, M., Finardi, P., Gengo, P., Espos-
ito, P., and Carida´, V. (2023). Cabrita: closing
ACKNOWLEDGEMENTS
the gap for foreign languages. arXiv preprint
arXiv:2308.11878.
ChatGPT 4 was used in all sections of this work to
Lewis, P., Perez, E., Piktus, A., Petroni, F.,
standardize and improve the writing in British En-
Karpukhin, V., Goyal, N., Ku¨ttler, H., Lewis,
glish. ThisresearchispartiallyfundedbytheBrazil-
M., Yih, W.-t., Rockta¨schel, T., et al. (2020).
ianNationalCouncilforScientificandTechnological
Retrieval-augmented generation for knowledge-
Development(CNPq).
intensivenlptasks.AdvancesinNeuralInforma-
tionProcessingSystems,33:9459–9474.
REFERENCES Lopes, R., Magalha˜es, J., and Semedo, D.
(2024). Glo´ria-a generative and open large lan-
guage model for portuguese. arXiv preprint
Brazil, C. (2016). Ajuste sinief 17.
arXiv:2402.12969.
https://www.confaz.fazenda.gov.br/legislacao/ajustes
/2016/AJ 017 16AccessedonJun6th,2024. Ma, H., Zhang, C., Bian, Y., Liu, L., Zhang, Z.,
Zhao,P., Zhang,S., Fu,H.,Hu, Q.,andWu, B.
Correˆa, N. K., Falk, S., Fatimah, S., Sen, A., and
(2024). Fairness-guidedfew-shotpromptingfor
de Oliveira, N. (2024). Teenytinyllama: open-
large language models. Advances in Neural In-
sourcetinylanguagemodelstrainedinbrazilian
formationProcessingSystems,36.
portuguese. arXivpreprintarXiv:2401.16640.Marinho, M. C., Di Oliveira, V., Neto, S. A., 2: Openfoundationandfine-tunedchatmodels.
Weigang, L., and Borges, V. R. (2022). Visual arXivpreprintarXiv:2307.09288.
analysisofelectronicinvoicestoidentifysuspi-
Valenc¸a, P. R. M. et al. (2023). Essays on foreign
ciouscasesoftaxfrauds. InInternationalCon- trade,labor,innovationandenvironment. UCB.
ference on Information Technology & Systems,
VanEngelen,J.E.andHoos,H.H.(2020). Asurvey
pages185–195.Springer.
onsemi-supervisedlearning. Machinelearning,
MERCOSUR (2024a). Mercosur - consultas 109(2):373–440.
a` nomenclatura comum e a` tarifa externa.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
https://www.mercosur.int/pt-br/politica-
Jones, L., Gomez, A. N., Kaiser, Ł., and Polo-
comercial/ncm/AccessedonJun4th,2024.
sukhin, I. (2017). Attention is all you need.
MERCOSUR (2024b). Mercosur con- Advances in neural information processing sys-
tries. https://www.mercosur.int/en/about- tems,30.
mercosur/mercosur-countries/ Accessed on Jun
Warnakulasuriya, S. and Hapuarachchi, K. (2024).
6th,2024.
From knowledge to action: Leveraging
Meta, A. (2024). Introducing meta llama 3: retrieval augmented fine tuning (raft) to
The most capable openly available llm empower quick and confident first-aid deci-
to date. Last accessed June 3th, 2024 sions in emergencies. Researchgate (Preprint)
https://ai.meta.com/blog/meta-llama-3/. http://dx.doi.org/10.13140/RG.2.2.35911.30888.
Pires,R.,Abonizio,H.,Almeida,T.S.,andNogueira, WCO (2018). THE HARMONIZED SYS-
R. (2023). Sabia´: Portuguese large language TEM A universal language for inter-
models. In Brazilian Conference on Intelligent national trade. World Customs Or-
Systems,pages226–240.Springer. ganization. https://www.wcoomd.org/-
/media/wco/public/global/pdf/topics/nomenclature/
Radosavovic, I., Zhang, B., Shi, B., Rajasegaran, J.,
activities-and-programmes/30-years-hs/hs-
Kamat, S., Darrell, T., Sreenath, K., and Malik,
compendium.pdf(Visited2024-06-04).
J. (2024). Humanoid locomotion as next token
prediction. arXivpreprintarXiv:2402.19469. WCO (2024). List of contracting parties
to the hs convention and countries us-
Schulhoff,S.,Ilie,M.,Balepur,N.,Kahadze,K.,Liu,
ing the hs - world customs organization.
A.,Si,C.,Li,Y.,Gupta,A.,Han,H.,Schulhoff,
https://www.wcoomd.org/en/topics/nomenclature/
S., et al. (2024). The prompt report: A sys-
overview/list-of-contracting-parties-to-the-hs-
tematic survey of prompting techniques. arXiv
convention-and-countries-using-the-hs.aspx
preprintarXiv:2406.06608.
AccessedonJun6th,2024.
Schulte, J. P., Giuntini, F. T., Nobre, R. A., Nasci-
Weigang, L., Enamoto, L. M., Li, D. L., and
mento, K. C. d., Meneguette, R. I., Li, W.,
Rocha Filho, G. P. (2022). New directions for
Gonc¸alves,V.P.,andRochaFilho,G.P.(2022).
artificialintelligence: human, machine, biologi-
Elinac: autoencoder approach for electronic
cal,andquantumintelligence.FrontiersofInfor-
invoices data clustering. Applied Sciences,
mation Technology & Electronic Engineering,
12(6):3008.
23(6):984–990.
Souza, F., Nogueira, R., and Lotufo, R. (2020).
Yadav,B.K.(2023).Impactofregulationandconfor-
Bertimbau: pretrained bert models for brazilian
mity assessment procedures on global trade. In
portuguese. InIntelligentSystems:9thBrazilian
HandbookofQualitySystem,Accreditationand
Conference, BRACIS 2020, Rio Grande, Brazil,
ConformityAssessment,pages1–21.Springer.
October 20–23, 2020, Proceedings, Part I 9,
Zhang, T., Patil, S. G., Jain, N., Shen, S., Zaharia,
pages403–417.Springer.
M.,Stoica,I.,andGonzalez,J.E.(2024). Raft:
Touvron, H., Lavril, T., Izacard, G., Martinet, X.,
Adaptinglanguagemodeltodomainspecificrag.
Lachaux,M.-A.,Lacroix,T.,Rozie`re,B.,Goyal,
arXivpreprintarXiv:2403.10131.
N.,Hambro,E.,Azhar,F.,etal.(2023a). Llama:
Openandefficientfoundationlanguagemodels.
arXivpreprintarXiv:2302.13971.
APPENDIX
Touvron,H.,Martin,L.,Stone,K.,Albert,P.,Alma-
hairi, A., Babaei, Y., Bashlykov, N., Batra, S.,
Below, by illustration, a record used in the training
Bhargava,P.,Bhosale,S.,etal.(2023b). Llama
base is presented. It is in Portuguese, as it was in-sertedintothedatabase.
[
{ "content": " context:
[o codigo da categoria do VIN. PORT.
QUINTA DE VALE VEADOS RESERVA TT 2014
750ML ´e: 22041010],
[a categoria 22041010 possui a
seguinte descric¸˜ao: Bebidas, l´ıquidos
alco´olicos e vinagres. - Vinhos de uvas
frescas, incluindo os vinhos
enriquecidos com ´alcool; mostos de uvas,
excluindo os da posic¸˜ao 20.09. - -
Vinhos espumantes e vinhos espumosos -
Tipo champanha (champagne) tem c´odigo:
22041010],
[a categoria Bebidas, l´ıquidos
alco´olicos e vinagres. - Vinhos de uvas
frescas, incluindo os vinhos enriquecidos
com ´alcool; mostos de uvas, excluindo os
da posic¸˜ao 20.09. - - Vinhos espumantes
e vinhos espumosos - Tipo champanha
(champagne) tem posic¸˜ao: Vinhos de uvas
frescas, incluindo os vinhos enriquecidos
com ´alcool; mostos de uvas, excluindo os
da posic¸˜ao 20.09.] \n
responda a seguinte pergunta usando
informac¸˜oes do contexto anterior: O
produto VIN. PORT. QUINTA DE VALE VEADOS
RESERVA TT 2014 750ML est´a classificado
em qual categoria NCM?", "role": "user"
},
{ "content": "o produto VIN. PORT. QUINTA
DE VALE VEADOS RESERVA TT 2014 750ML
possui categoria: Bebidas, l´ıquidos
alco´olicos e vinagres. - Vinhos de uvas
frescas, incluindo os vinhos enriquecidos
com ´alcool; mostos de uvas, excluindo os
da posic¸˜ao 20.09. - - Vinhos espumantes
e vinhos espumosos - Tipo champanha
(champagne)", "role": "assistant"
}
]