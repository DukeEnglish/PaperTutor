Nadaraya–Watson kernel smoothing as a random energy model
Jacob A. Zavatone-Veth1,2,∗ and Cengiz Pehlevan2,3,4,†
1Society of Fellows, Harvard University, Cambridge, MA, USA
2Center for Brain Science, Harvard University, Cambridge, MA, USA
3John A. Paulson School of Engineering and Applied Sciences, Harvard University, Cambridge, MA, USA
4Kempner Institute for the Study of Natural and Artificial Intelligence, Harvard University, Cambridge, MA, USA
(Dated: August 8, 2024)
WeinvestigatethebehavioroftheNadaraya–Watsonkernelsmoothingestimatorinhighdimensions
using its relationship to the random energy model and to dense associative memories.
I. INTRODUCTION
Assumeonehassomescalarfunctionf(x)ofad-dimensionalvectorx, whichonewantstoestimategivenaccesston
examples D ={(x ,f(x )}n . A classic approach to this problem is Nadaraya–Watson kernel smoothing (henceforth
µ µ µ=1
the NW estimator), which given a choice of kernel k(x,x′) produces the estimate
(cid:80)n k(x,x )f(x )
fˆ (x)= µ=1 µ µ . (1)
D (cid:80)n
k(x,x )
µ=1 µ
The key question is then how close fˆ (x) is to the true function f(x). One might measure this in terms of the
D
mean-squared error, averaged over realizations of the training set, E E [(f(x)−fˆ (x))2], or perhaps in terms of a
D x D
worst-case error sup |f(x)−fˆ (x)|.
x D
Classical results show that the NW estimator suffers from the curse of dimensionality in the sense that n must
be exponentially large in the dimension d to achieve error below some fixed tolerance [1–3]. This contrasts with the
closely-related kernel ridge regression (KRR) estimator,
n
fˆKRR(x)= (cid:88) k(x,x )(K−1) f(x ), K =k(x ,x ), (2)
D µ µν ν µν µ ν
µ,ν=1
whichdependingonthedetailsofthetargetfunction f mayrequireonlypolynomiallymanysamplestoachieveagiven
accuracy [2, 4–7]. This separation is striking given that both of these methods linearly smooth the training labels
f(x ); as highlighted by Belkin et al. [2], the key difference is that the NW estimator is a ‘direct’ smoother, while KRR
µ
estimates an inverse model for the training data. Recent years have seen substantial advances in understanding the
high-dimensional asymptotics of KRR [2, 4–6], but a similarly detailed understanding of the NW estimator is lacking
[2, 8]. In particular, what are the physical reasons for the poor performance of direct smoothing in high dimensions
relative to building an inverse model?
In this note, we explore an approach to characterizing the NW estimator based on the observation that it can be
interpreted in terms of a Random Energy Model (REM) [9–11] 1. Concretely, we can view (1) as the average of the
observablef(x )withrespecttoadistributionoverµwithBoltzmannweightsk(x,x )thathavequenchedrandomness
µ µ
due to the randomness of the datapoints x . For suitable choices of the data distribution, kernel, and target function,
µ
it should thus be possible to apply standard approaches to the REM to study the high-dimensional behavior of the
NW estimator. The slightly peculiar feature of the problem is that both the energies and the observable depend on
the quenched disorder, i.e., on the random data. From the REM perspective, the relevant regime is when n=eαd
as d→∞ for some fixed α>0, which would match the sample complexity required by the curse of dimensionality.
Moreover, one expects that under suitable conditions the REM associated to the estimator will display a condensation
transition: there will be a phase in which all datapoints contribute relatively equally to the estimate, and another in
which the distribution is sharply condensed, and only a few points contribute.
∗ jzavatoneveth@fas.harvard.edu
† cpehlevan@seas.harvard.edu
1 SeeM´ezardandMontanari[9]forapedagogicalintroductiontotheREM.
4202
guA
7
]nn-sid.tam-dnoc[
1v96730.8042:viXra2
II. LARGE DEVIATIONS ANALYSIS FOR SPHERICAL DATA
We now leverage the REM analogy to analyze the asymptotic behavior of fˆ (x). For simplicity, we fix a concrete
D
model for the data, kernel, and target funct√ion, though many parts of this analysi√s could be generalized. We assume
that the inputs lie on the sphere of radius d, with a fixed test point x∈Sd−1( d) and uniformly-drawn training
√
examples x ∼U[Sd−1( d)], and take the kernel to be a radial basis function on the sphere, i.e.,
µ
k(x,x )=eβ⟨x,xµ⟩ (3)
µ
for some inverse bandwidth β > 0. This makes the mapping to the REM quite direct, and is particularly closely
related to Lucibello and M´ezard [10]’s recent work on dense associative memories (DAMs) [12], where it is shown that
the memorization capacity for spherical patterns is determined by the free energy of an auxiliary REM. With these
choices, we focus on the simplest class of functions on the sphere with underlying low-dimensional structure: single
index models. Such functions take the form
f(x)=g(⟨w,x⟩/d) (4)
√
for a vector w ∈ Sd−1( d) and a scalar link function g : R → R. We will often consider positive-homogeneous link
functions satisfying g(γt)=γkg(t) for all γ >0 and some degree k ≥0; among these is of course the linear rectifier
g(t)=max{0,t}.
√
We begin by considering the predictions on some fixed test point x ∈ Sd−1( d). We will argue that there is an
explicitly computable r =r (α,β), bounded from below by zero and fromabove by 1, such that we have the pointwise
∗ ∗
asymptotic
fˆ (x)∼g(r ⟨w,x⟩/d). (5)
D ∗
To obtain this result, we start with the fact that, under the stated assumptions, (1) becomes
fˆ (x)=
(cid:80)n µ=1eβ⟨x,xµ⟩g(⟨w,x µ⟩/d)
. (6)
D (cid:80)n eβ⟨x,xµ⟩
µ=1
By symmetry, the distribution of fˆ (x) induced by the randomness in the training data can depend on x and w only
D
through their overlap
ρ=⟨w,x⟩/d. (7)
Based on standard REM results, we expect the empirical joint distribution of the overlaps
t=⟨x,x ⟩/d and q =⟨w,x ⟩/d, (8)
µ µ
viewed as random variables with randomness induced by the empirical distribution of x over µ, to satisfy a large
µ
deviation principle. Concretely, what we expect is that for n=eαd and d→∞, there is a potential ϕ(t,q) such that
(cid:82) dtdqedϕ(t,q)g(q)
fˆ (x)∼ . (9)
D (cid:82) dtdqedϕ(t,q)
This potential will consist of an energetic contribution βt from the kernel, plus an entropic contribution resulting from
re-writing the sum (6) in terms of the density of states with a given (t,q). We will follow the standard random energy
model analysis to obtain an asymptotic of this form, closely following the related work of Lucibello and M´ezard [10].
Pedagogical accounts of this analysis can also be found in the textbook of M´ezard and Montanari [9], or in the original
paper of Derrida [11].
For the purpose of the computation, it is more convenient to instead work in coordinates
√ √
u=(t+q)/ 2 and v =(t−q)/ 2, (10)
as by the Cauchy-Schwarz inequality these variables lie within the ellipse u2/(1+ρ)+v2/(1−ρ)≤1. The first step is
to determine the density of states with a given u and v, leveraging the Ga¨rtner-Ellis theorem. Following Lucibello and
M´ezard [10], it is easy to show that the joint moment generating function of u and v for a given sample is, to leading
exponential order,
1 √ √
logE euˆ⟨x+w,xµ⟩/ 2+vˆ⟨x−w,xµ⟩/ 2 =ζ(a2)+o (1), (11)
d xµ d3
where
(cid:34) (cid:32) √ (cid:33)(cid:35)
1 (cid:112) 1+ 1+4a2
ζ(a2)= 1+4a2−1−log (12)
2 2
for
a2 = 1(cid:13) (cid:13) (cid:13)uˆx √+w +vˆx √−w(cid:13) (cid:13) (cid:13)2 (13)
d(cid:13) 2 2 (cid:13)
=(1+ρ)uˆ2+(1−ρ)vˆ2. (14)
Then, we must compute the convex conjugate to obtain the rate function as
s(u,v)=sup{uuˆ+vvˆ−ζ(a2)}. (15)
uˆ,vˆ
The density of states with a given u and v will then be, to leading exponential order, given by ed[α−s(u,v)] when u,v
are such that the exponent is positive; otherwise it is exponentially suppressed.
Because u and v lie within an ellipse, it is convenient to work in polar coordinates
(cid:112) (cid:112)
u= 1+ρrcosθ and v = 1−ρrsinθ (16)
for r ∈[0,1] and θ ∈[0,2π), in terms of which the rate function reduces to
1
s(r,θ)=− log(1−r2). (17)
2
Then, the threshold s(r,θ)≤α beyond which the density of states becomes suppressed is easily determined, and gives
√
r = 1−e−2α. This gives us the asymptotic
(cid:82) rdrdθedϕ(r,θ)g(q(r,θ))
fˆ (x)∼ (18)
D (cid:82) rdrdθ edϕ(r,θ)
for a potential
ϕ(r,θ)=βt(r,θ)+α−s(r,θ) (19)
√ √
1+ρcosθ+ 1−ρsinθ 1
=α+βr √ + log(1−r2), (20)
2 2
√
where the integral over r is restricted to r ≤ 1−e−2α. For d→∞ the integrals over r and θ can be evaluated using
Laplace’s method, which yields the asymptotic
fˆ (x)∼f(q(r ,θ )), (21)
D ∗ ∗
where r and θ maximize ϕ(r,θ) over the allowed region. Maximizing with respect to θ gives
∗ ∗
(cid:112)
θ =arccos (1+ρ)/2 (22)
∗
for any r >0, whereupon the potential for r reduces to
1
ϕ(r ,θ )=α+βr + log(1−r2) (23)
∗ ∗ ∗ 2 ∗
and q reduces to
√ √
1+ρcosθ − 1−ρsinθ
q(r ,θ )=r ∗√ ∗ =ρr . (24)
∗ ∗ ∗ ∗
2
The maximization in r now mirrors the standard REM analysis, hence we have that there exists a phase transition at
the ‘condensation threshold’
(cid:112)
β =e2α 1−e−2α. (25)
c4
In the un-condensed phase β <β , exponentially many states contribute to the average, and we have
c
(cid:112)
1+4β2−1
r = . (26)
∗ 2β
In the condensed phase β >β , the upper limit dominates, and
c
(cid:112)
r = 1−e−2α. (27)
∗
Importantly, r is a non-decreasing continuous function of β, and is bounded from above by 1. The condensation
∗
transition in the REM also has a clear interpretation: if β is sufficiently small—i.e., if the kernel bandwidth is
sufficiently large—the asymptotic estimate of the function depends only on the bandwidth, not on the total load.
However, for very small bandwidths the load becomes important.
Thus, we at last obtained the claimed asymptotic
fˆ (x)∼g(ρr ). (28)
D ∗
Therefore, the randomness in the data results in a multiplicative renormalization of ρ. In general, generalization
improvesasr ↑1. Asaconsequenceofthisrenormalizationeffect,onecanobtainanasymptoticallyunbiasedestimator
∗
for positive-homogeneous link functions g satisfying g(ρr )=rkg(ρ) for some k by dividing the NW estimator by rk.2
∗ ∗ ∗
III. MEAN-SQUARED GENERALIZATION ERROR
The asymptotic (28) gives us a prediction for the absolute deviation |f(x)−fˆ (x)| for a (typical) test point, but one
D
would also want to compute the asymptotics of the mean-squared generalization error. For the single-index models we
consider, this reduces to a scalar average over the distribution of ρ induced by the randomness in x. Asymptotically, ρ
should be approximately Gaussian with variance 1/d. Given that this distribution is manifestly dimension-dependent,
we are confronted with the question of the scale at which we should measure the generalization error, and the question
of whether the finite-size corrections to the simple asymptotic for a fixed x must be taken into account in order to
accurately predict the mean-squared generalization error.
Considering a positive-homogeneous activation of degree k, we have the asymptotic E [f(x)2] = E [g(ρ)2] ∼
x ρ
d−kE [g(t)2], which specifies a scale at which to measure the mean-squared error. As this scale is dimension-
t∼N(0,1)
dependent, contributions from corrections to the pointwise asymptotic may be non-negligible at this scale. Quite
generally, we may write
E E [(f(x)−fˆ (x))2]
D x D ∼(1−rk)2+δ (29)
E [f(x)2] ∗
x
where δ = δ(α,β) is an error term accounting for potential contributions from corrections, which are conceptually
separable from the asymptotic bias term (1−rk)2. Accurately determining δ requires a more careful analysis than
∗
the rather na¨ıve approach followed here, and thus lies outside the scope of this note. We thus leave this result as a
conjecture.
IV. TRAINING ERROR
Considering a typical training point x , we have, letting y =g(⟨w,x ⟩/d) for brevity,
ν µ µ
fˆ (x )=
eβdy
ν
+(cid:80)n µ̸=νeβ⟨xν,xµ⟩y
µ , (30)
D ν eβd+(cid:80)n eβ⟨xν,xµ⟩
µ̸=ν
where we have isolated the contribution of the ν-th sample. Now, for large d, we can see that there should be two
phases: one in which the dominant contribution to fˆ (x ) comes from the ν-th sample itself, and one in which the
D ν
contributions of the remaining n−1 samples dominate. The problem of determining when the transition between
2WethankSabarishSainathanforthisobservation.5
1
,c
0.9 ,r generalization
0.8
0.7
0.6
,0.5 retrieval
0.4
0.3
0.2
0.1 condensed
0
0 0.5 1 1.5 2
-
FIG. 1. Phase diagram of the NW estimator in inverse bandwidth β—load α space. The phase diagram coincides with that of
the exponential DAM studied by Lucibello and M´ezard [10], and the interpretation of the phases is similar.
these two phases occurs is analogous to the problem of determining the capacity of a dense associative memory, as
studied by Lucibello and M´ezard [10]. In analogy to the associative memory setting, we refer to the phase in which the
ν-th sample dominates the prediction as the retrieval phase.
In the retrieval phase, we simply have
fˆ (x )∼g(⟨w,x ⟩/d), (31)
D ν ν
and the training error vanishes. For retrieval to occur, we should have
1 log(cid:80)n eβ⟨xν,xµ⟩ <1, (32)
βd µ̸=ν
which is the condition defining the memorization capacity of a DAM found by Lucibello and M´ezard [10]. The
asymptotic behavior of the left-hand-side of this inequality can be obtained using a large deviations computation
identical to that above with q integrated out. The resulting condition is that ϕ(r ,θ )<β, where ϕ is the limiting
∗ ∗
potential given in (23). We then define the retrieval threshold
α =sup{α:ϕ(r ,θ )<β}. (33)
r ∗ ∗
α≥0
From the work of Lucibello and M´ezard [10], we have that in this phase the probability that all spherical patterns may
be simultaneously retrieved also tends to one, and that for any fixed β the threshold value in α where the retrieval
phase terminates appears to be in the non-condensed regime. This leads to a phase diagram which coincides with that
for the exponential DAM; we reproduce this in Figure 1.
Outside of the retrieval phase, the contributions of the n−1 other training points to fˆ (x ) dominate, and we have
D ν
fˆ (x )∼ (cid:80)n µ̸=νeβ⟨xν,xµ⟩y µ ∼g(cid:18) r ⟨w,x ν⟩(cid:19) (34)
D ν (cid:80)n eβ⟨xν,xµ⟩ ∗ d
µ̸=ν
conditioned on x , by analogy with our study of the prediction on a test point. Then, the computation of the training
ν
error matches the computation of the generalization error, so by combining this with the result in the retrieval phase
we obtain a complete description of the training performance. In particular, if we again consider positive-homogeneous
link functions of degree k, we expect that we should have
1 (cid:80)n [f(x )−fˆ (x )]2
n ν=1 ν D ν ∼(1−rk)2+δ (35)
1 (cid:80)n f(x )2 ∗
n ν=1 ν
if α > α , subject to the same caveats noted above. The curious feature of this result is then that it predicts a
r
discontinuity in the training error at the retrieval threshold provided that δ does not smooth the transition too much:
either the model can perfectly interpolate all training points, or it incurs a nontrivial relative error matching that on a
novel test point. In the phase diagram in Figure 1, we refer to this as the “generalization” phase as the model does not
distinguish between training and test data.6
(a) (b) (c)
, = 0.10, - = 1.00 , = 0.10, - = 1.00 , = 0.10, - = 1.00
0.5
d = 10
100
d-1 1
100
d-1 1
0.45 d = 25 d-1/2 d-1/2
(1
g)
'<
;
000 ...
0000
123
....
555
1234
d d
d
d d d d
t tr
h
u= =
=
= = = =
ee
o
5 7
1
1 1 1 2
r0 5
0
2 5 7 0
y0
5 0 5 0
yroeht
m orf
noitaived
etulosba
naem11 00 -- 21
p p- -1
1/2
0
;
yroeht
m orf noitaived
etulosba
fo
.ved
.dts11 00 -- 21
p p- -1
1/2
0
;
0.05
0
-1 -0.5 0 0.5 1
10-3
10 25 50 75 100 150 200
-1 10-3
10 25 50 75 100 150 200
-1
; d d
(d) (e) (f)
, = 0.10, - = 0.10 , = 0.10, - = 0.10 , = 0.10, - = 0.10
0.5
d = 10
100
d-1 1
100
d-1 1
0.45 d = 25 d-1/2 d-1/2
(1
g)
'<
;
000 ...
0000
123
....
555
1234 d d d
d d d d
t tr
h
u= = =
= = = =
ee
o
5 7 1
1 1 1 2
r0 5 0
2 5 7 0
y0
5 0 5 0
yroeht
m orf
noitaived
etulosba
naem11 00 -- 21
p p- -1 1/2
0
;
yroeht
m orf noitaived
etulosba
fo
.ved
.dts11111 00000 ----- 54321 p p- -1 1/2
0
;
0.05
0
-1 -0.5 0 0.5 1
10-3
10 25 50 75 100 150 200
-1 10-6
10 25 50 75 100 150 200
-1
; d d
FIG. 2. Simulations of the NW estimator with link function g(x)=|x|. (a). Comparison of the asymptotic (28) to numerical
√
evaluationsoftheestimator(1)inthecondensedphase(α=0.1,β =1;thecondensationthresholdisβ =e0.2 1−e−0.2 ≃0.52).
c
The red line shows (28), while the black line shows the true link function. Estimates for different dimensions d ranging from 10
to 200 are shown by shades of blue, with shaded patches showing ±1 standard deviation over 1000 realizations. (b). Mean
absolute deviation between numerics and (28) as a function of dimension d for different test point overlaps ρ indicated by color.
Shaded patches show 95% confidence intervals computed using the bias-corrected and accelerated percentile bootstrap method.
Dashed lines show scalings with d and p to guide the eye. (c). As in (b), but showing the standard deviation of the absolute
deviation from (28). (d-f). As in (a-c), but for an example in the un-condensed phase (α=0.1, β =0.1).
V. NUMERICAL EXPERIMENTS
We now would like to compare our theoretical predictions to numerical experiments. The key bottleneck is the
exponential dependence of the number of samples on dimension. This makes a na¨ıve approach to evaluating the NW
estimator impractical due to memory constraints. Instead, we use a slow but memory-efficient iterative algorithm. To
do so stably, we multiply and divide (1) by e−βm/n, where m=max ⟨x,x ⟩ is the maximum dot product, such that
µ µ
both the numerator and denominator of (1) are bounded. Then, we run the iteration
m =max{m ,⟨x,x ⟩}
µ µ−1 µ
(cid:20) (cid:21)
1
Z =eβ∆µZ + eβ(⟨x,xµ⟩−mµ)−eβ∆µZ
µ µ−1 µ µ−1 (36)
fˆ (x)=
eβ∆µZ
µ−1fˆ (x)+
1 (cid:20) eβ(⟨x,xµ⟩−mµ)
y −
eβ∆µZ
µ−1fˆ
(x)(cid:21)
µ Z µ−1 µ Z µ Z µ−1
µ µ µ
with ∆ =m −m , starting from
µ µ−1 µ
fˆ =y
1 1
m =⟨x,x ⟩ (37)
1 1
Z =1.
17
(a) (b) (c)
, = 0.10, - = 1.00 , = 0.10, - = 1.00 , = 0.10, - = 1.00
1
100
d-1 1
100
d-1 1
d-1/2 d-1/2
(1 g)
'<
;
----0000 0000.... ....2468
86420 d
d
d
d
d
d
d
d
d
=
=
=
=
=
=
=
=
=
1
2
5
7
1
1
1
1
20
5
0
5
0
2
5
7
00
5
0
5
0
yroeht
m orf
noitaived
etulosba
naem11 00 -- 21
p p- -1
1/2
0 ;
yroeht
m
orf noitaived
etulosba
fo
.ved
.dts11 00 -- 21
p p- -1
1/2
0 ;
true
-1 theory
-1 -0.5 0 0.5 1
10-3
10 25 50 75 100 150 200
-1 10-3
10 25 50 75 100 150 200
-1
; d d
(d) (e) (f)
, = 0.10, - = 0.10 , = 0.10, - = 0.10 , = 0.10, - = 0.10
1
100
d-1 1
100
d-1 1
d-1/2 d-1/2
(1 g)
'<
;
----0000 0000.... ....2468
86420 d
d
d
d
d
d
d
d
d
=
=
=
=
=
=
=
=
=
1
2
5
7
1
1
1
1
20
5
0
5
0
2
5
7
00
5
0
5
0
yroeht
m orf
noitaived
etulosba
naem11 00 -- 21
p p- -1
1/2
0 ;
yroeht
m
orf noitaived
etulosba
fo
.ved
.dts111 000 --- 321
p p- -1
1/2
0 ;
true
-1 theory
-1 -0.5 0 0.5 1
10-3
10 25 50 75 100 150 200
-1 10-4
10 25 50 75 100 150 200
-1
; d d
FIG. 3. As in Figure 2, but for link function g(x)=erf(4x).
It is easy to check that the endpoint of this iteration gives fˆ (x)=fˆ (x). As we assume the training examples are
n D
independentandidenticallydistributed, wecandrawanewdatumx ateachstep, andthuswecanavoidinstantiating
µ
an array containing all n examples. This memory savings comes at the cost of a time complexity linear in n—and thus
exponential in d.
We can now use this algorithm to numerically evaluate the NW estimator for example link functions g, and compare
the results against the asymptotic predictions obtained in the prelude. Unfortunately, the exponential cost in time is
still prohibitive, and thus far we have obtained satisfactory numerical results only for the pointwise prediction of the
estimator, not for the training or generalization error. In Figures 2 and 3, we show examples of the predictions for the
absolute value function g(x)=|x| and a scaled error function g(x)=erf(4x), respectively. By examining the size of
deviations from the theory, in the condensed phase we appear to have decay of the mean deviation and of the standard
deviation at a rate at least d−1/2 for d up to 200. In the un-condensed phase, the mean absolute deviation decays at a
similar d−1/2 rate, while the standard deviation of the absolute deviation increases far more rapidly, seemingly like
n−1/2. Intuitively, this is to be expected given the number of points that contribute to the estimator in each phase.
We note that already at this scale we must deal with e20 ≃485×106 datapoints, for which each simulation (i.e., a
sweep across dimensions for a single link function at a single load and bandwidth) requires around 90 hours of compute
time on one 32-core node of Harvard’s FASRC Cannon compute cluster. If nothing else, this illustrates the fact that a
careful understanding of finite-size effects is required to make predictions about generalization at practically-relevant
scales.
VI. CONCLUSIONS
We have reported the results of a very preliminary investigation of the NW estimator through a random energy
model lens. The primary result of our note is the asymptotic (28) for the prediction on a fixed test point, which shows
that the randomness from the training samples multiplicatively renormalizes the true overlap between the latent vector8
w and the test point x. Our numerical simulations support the accuracy of this prediction, though computational
constraints mean that we can access only relatively small dimensions, where finite-size effects are prominent.
There are many problems which we leave open for future inquiry. First, though we have contented ourselves with
simple asymptotics and comparisons to limited numerics, this analysis could in principle be made entirely quantitative
in the sense of explicit error bounds as a function of d. Some large deviations results for the NW estimator in fixed
dimension d as n→∞ are known [3, 13], but to our knowledge rigorous results when d and n tend to infinity together
are lacking. Given the close relationship of the settings, rigorous bounds on the finite-size effects for the setting
considered here would immediately imply similar bounds for memorization of spherical patterns in DAMs [10]. Second,
wehave specialized tothe settingof sphericaldata, aradial basisfunctionkernel, anda single-indextarget; in principle
all of these assumptions could be relaxed. Extending our results to anisotropic data would be of particular interest, as
introducing a preferred direction at some specified angle with the latent vector w would likely alter the condensation
threshold due to the local enhancement of data density. A natural starting point is thus to consider data drawn from a
von Mises-Fisher distribution, which would require one to track three overlaps between the latent vector w, the test
point x, and the mean direction η.
Finally, we comment on the broader context of our work. In the last few years, substantial effort has been devoted
to seeking precise characterizations of how various learning algorithms behave in high dimensions. As mentioned
in the introduction to this note, a key achievement of this program has been a sharp understanding of kernel ridge
regression [2, 4–7]. This essay represents a small step towards a similarly detailed understanding of qualitatively
different regression algorithms. Thus, in closing, we echo Jamie Simon’s call to arms [14]: let’s solve more learning
rules!
ACKNOWLEDGEMENTS
We thank Jamie Simon for posing the question that inspired this note [8], and thank Alexander Atanasov, Blake
Bordelon, Benjamin Ruben, and especially Sabarish Sainathan for useful discussions. JAZV further thanks Dmitry
Krotov for useful discussions regarding [10]. JAZV is supported by a Junior Fellowship from the Harvard Society of
Fellows. JAZV and CP were supported by NSF Award DMS-2134157 and NSF CAREER Award IIS-2239780. CP
is further supported by a Sloan Research Fellowship. This work has been made possible in part by a gift from the
Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial
Intelligence. The computations in this paper were run on the FASRC Cannon cluster supported by the FAS Division
of Science Research Computing Group at Harvard University.
[1] D. Conn and G. Li, An oracle property of the Nadaraya–Watson kernel estimator for high-dimensional nonparametric
regression, Scandinavian Journal of Statistics 46, 735 (2019).
[2] M.Belkin,A.Rakhlin,andA.B.Tsybakov,Doesdatainterpolationcontradictstatisticaloptimality?,inProceedings of the
Twenty-Second International Conference on Artificial Intelligence and Statistics,ProceedingsofMachineLearningResearch,
Vol. 89, edited by K. Chaudhuri and M. Sugiyama (PMLR, 2019) pp. 1611–1619.
[3] A. B. Tsybakov, Introduction to Nonparametric Estimation (Springer, New York, NY, 2009).
[4] A.Canatar,B.Bordelon,andC.Pehlevan,Spectralbiasandtask-modelalignmentexplaingeneralizationinkernelregression
and infinitely wide neural networks, Nature Communications 12, 2914 (2021).
[5] L.Xiao,H.Hu,T.Misiakiewicz,Y.Lu,andJ.Pennington,Preciselearningcurvesandhigher-orderscalingsfordot-product
kernel regression, in Advances in Neural Information Processing Systems, Vol. 35, edited by S. Koyejo, S. Mohamed,
A. Agarwal, D. Belgrave, K. Cho, and A. Oh (Curran Associates, Inc., 2022) pp. 4558–4570.
[6] A. B. Atanasov, J. A. Zavatone-Veth, and C. Pehlevan, Scaling and renormalization in high-dimensional regression, arXiv
(2024), arXiv:2405.00592 [stat.ML].
[7] S. Spigler, M. Geiger, and M. Wyart, Asymptotic learning curves of kernel methods: empirical data versus teacher–student
paradigm, Journal of Statistical Mechanics: Theory and Experiment 2020, 124001 (2020).
[8] J.B.Simon,Aeigenframeworkforthegeneralizationof1NN,onlineblogpost(2024),URL:https://james-simon.github.
io/blog/1nn-eigenframework/.
[9] M. M´ezard and A. Montanari, Information, Physics, and Computation (Oxford University Press, 2009).
[10] C. Lucibello and M. M´ezard, Exponential capacity of dense associative memories, Phys. Rev. Lett. 132, 077301 (2024).
[11] B. Derrida, Random-energy model: An exactly solvable model of disordered systems, Phys. Rev. B 24, 2613 (1981).
[12] D. Krotov and J. J. Hopfield, Dense associative memory for pattern recognition, in Advances in Neural Information
Processing Systems, Vol. 29, edited by D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (Curran Associates,
Inc., 2016).9
[13] A.Mokkadem,M.Pelletier,andB.Thiam,Largeandmoderatedeviationsprinciplesforkernelestimatorsofthemultivariate
regression, Mathematical Methods of Statistics 17, 146 (2008).
[14] J. B. Simon, Let’s solve more learning rules, online blog post (2024), URL: https://james-simon.github.io/blog/
lets-solve-learning-rules/.