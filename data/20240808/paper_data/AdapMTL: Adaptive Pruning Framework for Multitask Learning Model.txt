AdapMTL: Adaptive Pruning Framework for Multitask Learning
Model
MingcanXiang JiaxunTang QizhengYang
mingcanxiang@umass.edu jtang@umass.edu qizhengyang@umass.edu
UniversityofMassachusettsAmherst UniversityofMassachusettsAmherst UniversityofMassachusettsAmherst
Amherst,MA,USA Amherst,MA,USA Amherst,MA,USA
HuiGuan TongpingLiu
huiguan@umass.edu tongping@umass.edu
UniversityofMassachusettsAmherst UniversityofMassachusettsAmherst
Amherst,MA,USA Amherst,MA,USA
Abstract Model.InProceedingsofthe32ndACMInternationalConferenceonMultime-
Inthedomainofmultimediaandmultimodalprocessing,theeffi- dia(MM‚Äô24),October28-November1,2024,Melbourne,VIC,Australia.ACM,
NewYork,NY,USA,13pages.https://doi.org/10.1145/3664647.3681426
cienthandlingofdiversedatastreamssuchasimages,video,and
sensordataisparamount.Modelcompressionandmultitasklearn-
1 Introduction
ing(MTL)arecrucialinthisfield,offeringthepotentialtoaddress
theresource-intensivedemandsofprocessingandinterpretingmul- Inthelandscapeofmultimediaandmultimodalprocessing[2,40],
tiple forms of media simultaneously. However, effectively com- DeepNeuralNetworks(DNNs)[46]haveemergedasapivotaltech-
pressing a multitask model presents significant challenges due nology,poweringadvancementsacrossaspectrumofapplications
tothecomplexitiesofbalancingsparsityallocationandaccuracy fromimageandvideoanalysistonaturallanguageunderstanding
performanceacrossmultipletasks.Totacklethechallenges,we andbeyond.Theirprofoundabilitytolearnandabstractcomplex
proposeAdapMTL,anadaptivepruningframeworkforMTLmod- featuresfromarangeofmediaformsunderpinstheirutilityindi-
els.AdapMTLleveragesmultiplelearnablesoftthresholdsinde- versedomains,includingcontentcategorization,recommendation
pendentlyassignedtothesharedbackboneandthetask-specific systems,andinteractiveinterfaces.However,asthecomplexity
headstocapturethenuancesindifferentcomponents‚Äôsensitivity oftasksgrows,sodoesthedemandforlargerandmorepowerful
to pruning. During training, it co-optimizes the soft thresholds models,whichinturnrequiresubstantialcomputationalresources,
andMTLmodelweightstoautomaticallydeterminethesuitable memoryusage,andlongertrainingtimes.Thistrade-offbetween
sparsitylevelateachcomponenttoachievebothhightaskaccu- performanceandmodelcomplexityhasledtoacontinuouspursuit
racyandhighoverallsparsity.Itfurtherincorporatesanadaptive ofmoreefficientandcompactCNN[24]architectures,aswellas
weightingmechanismthatdynamicallyadjuststheimportanceof innovationsinpruningtechniquesthatcanmaintainhighperfor-
task-specificlossesbasedoneachtask‚Äôsrobustnesstopruning.We mancewithoutcompromisingthebenefitsofthemodel‚Äôsscale.
demonstratetheeffectivenessofAdapMTLthroughcomprehensive Pruningtechniques[13,19,23,25‚Äì27,36,48]haveemergedasa
experimentsonpopularmultitaskdatasets,namelyNYU-v2and promisingapproachtocompresslargemodelswithoutsignificant
Tiny-Taskonomy,withdifferentarchitectures,showcasingsuperior lossofperformance.Thesetechniquesaimtoreducethesizeof
performancecomparedtostate-of-the-artpruningmethods. amodelbyeliminatingredundantorlessimportantparameters,
suchasneurons,connections,orevenentirelayers,dependingon
CCSConcepts themethodemployed[9,28,62].Parameter-efficientprunedmod-
elscanprovidesignificantinferencetimespeedupsbyexploiting
‚Ä¢Computingmethodologies‚ÜíMulti-tasklearning.
the sparsity pattern [14, 31, 57, 61]. These models are designed
tohavefewerparameters,whichtranslatesintoreducedmemory
Keywords
footprintandlowercomputationalcomplexity(FLOPs)[31].By
Pruning,MultitaskLearning leveragingspecializedhardwareandsoftwaresolutionsthatcan
efficientlyhandlesparsematrixoperations,suchassparsematrix-
ACMReferenceFormat: vectormultiplication(SpMV),thesemodelscanachievefasterinfer-
Mingcan Xiang, Jiaxun Tang, Qizheng Yang, Hui Guan, and Tongping
encetimes[14,39,56].Additionally,sparsemodelscanbenefitfrom
Liu.2024.AdapMTL:AdaptivePruningFrameworkforMultitaskLearning
bettercacheutilization,astheyrequirelessmemorybandwidth,
therebyreducingtheoveralllatencyofthecomputation[41,61].
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor Althoughmanytechniqueshavebeenproposedinthepastfor
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed pruningasingle-taskmodel,thereismuchlessworkinpruninga
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
multitaskmodel.Multitaskmodels,whicharedesignedtosimul-
onthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.
Forallotheruses,contacttheowner/author(s). taneouslyhandlemultipletasks,havebecomeincreasinglypop-
MM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia ularduetotheirabilitytosharerepresentationsandlearnmore
¬©2024Copyrightheldbytheowner/author(s).
effectivelyfromdiversedatasources[16,65,68].Thesemodels
ACMISBN979-8-4007-0686-8/24/10
https://doi.org/10.1145/3664647.3681426 havefoundwide-rangingapplicationswheretasksareoftenrelated
4202
guA
7
]VC.sc[
1v31930.8042:viXraMM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia MingcanXiang,JiaxunTang,QizhengYang,HuiGuan&TongpingLiu
whilepreservingaccuracyforeachtask.Thisisachievedthrough
Input Input
asetoflearnablesoftthresholds[10,23]thatareindependently
Shared Shared
Backbone Backbone assignedtodifferentcomponentsandco-optimizedwithmodel
weightstoautomaticallydeterminethesuitablesparsitylevelfor
Enforce eachcomponentduringtraining.Specifically,wemaintainasetof
sparsity
softthresholdsùõº ={ùõº ùêµ,ùõº 1,ùõº 2,...,ùõº ùëá}ineachcomponent,where
ùõº ùêµ representsthethresholdforthesharedbackboneandùõº ùë° repre-
sentsthethresholdfortheùë°-thtask-specifichead.Intheforward
pass,onlytheweightslargerthanthethresholdùõº willbecounted
Task-specific Task-specific
Head Head inthemodel,whileothersaresettozero.Inthebackwardpass,we
automaticallyupdateallthecomponent-wisethresholdsùõº,which
Task1 Task2 Task3 Task1 Task2 Task3 willsmoothlyintroducesparsity.Additionally,AdapMTLemploys
(a) Dense (b) Sparse anadaptiveweightingmechanismthatdynamicallyadjuststheim-
portanceoftask-specificlossesbasedoneachtask‚Äôsrobustnessto
Figure1:Overviewofpruningadensemultitaskmodel.The pruning.AdapMTLdoesnotrequireanypre-trainingorpre-pruned
redpartsrepresentthesharedbackbone,andtheleafboxes modelsandcanbetrainedfromscratch.
representthetask-specificheads.Inthesparsemodel,the Weconductextensiveexperimentsontwopopularmultitask
blankspacesindicatetheprunedparameters. datasets:NYU-v2[47]andTiny-Taskonomy[63],usingdifferent
architecturessuchasDeeplab-ResNet34andMobileNetV2.When
andcanbenefitfromsharedknowledge[66].Acompactmultitask comparedwithstate-of-the-artpruningandMTLpruningmethods,
model,which isshowninFigure1,has thepotentialtodeliver AdapMTLdemonstratessuperiorperformanceinboththetraining
highperformanceacrossvarioustaskswhileminimizingresource andtestingphases.Itachieveslowertraininglossandbetternor-
requirements,makingitwell-suitedfordeploymentonresource- malizedevaluationmetricsonthetestsetacrossdifferentsparsity
constraineddevicesorinreal-timescenarios. levels.Thecontributionsofthispaperaresummarizedasfollows:
Traditional pruning techniques, which are primarily focused
(1) Weconductextensiveexperimentsthatrevealvaluablein-
onsingle-taskmodels,maynotbedirectlyapplicableorsufficient
sightsindesigningeffectiveMTLmodelpruningstrategies.
formultitaskingsettings.Recentworkhasstartedtoexplorethe
Thesefindingsmotivatethedevelopmentofnovelpruning
intersectionofmultitasklearningandpruning.Disparse[52]con-
strategiesspecificallytailoredformultitaskingscenarios.
sideredeachtaskindependentlybydisentanglingtheimportance
(2) WeproposeAdapMTL,anadaptivepruningframeworkfor
measurementandtakingtheunanimousdecisionsamongalltasks
MTLmodelsthatdynamicallyadjustssparsitylevelsacross
whenperformingparameterpruningandselection.Aparameter
differentcomponentstoachievehighsparsityandtaskac-
isremovedifandonlyifit‚Äôsshowntobenotcriticalforanytask.
curacy.AdapMTLfeaturescomponent-wiselearnablesoft
However,asthenumberoftasksincreases,itbecomeschallenging
thresholdsthatautomaticallydeterminethesuitablesparsity
toachieveunanimousselectionagreementamongalltasks,which
foreachcomponentduringtrainingandanadaptiveweight-
couldnegativelyaffecttheaverageperformanceacrosstasks.Thus,
ingmechanismthatdynamicallyadjuststaskimportance
thereisaneedfornovelcompressionapproachesthatcatertothe
basedontheirsensitivitytopruning.
complexitiesofmultitaskmodels,takingintoaccounttheinter-
(3) WedemonstratetheeffectivenessofAdapMTLthroughex-
dependenciesbetweentasks,thesharingofrepresentations,and
tensive experiments on multitask datasets with different
thedifferentsensitivityoftaskheads.
architectures,showcasingsuperiorperformancecompared
Totacklethechallenges,weconductextensiveexperimentsthat
toSOTApruningandMTLpruningmethods.Ourmethod
revealtwovaluableinsightsondesigninganeffectivemultitask
doesnotrequireanypre-trainingorpre-prunedmodels.
modelpruningstrategy.First,thesharedbackboneandthetask-
specificheadshavedifferentsensitivitytopruningandthusshouldbe
2 RelatedWork
treateddifferently.However,currentstate-of-the-artapproachesdo
notadequatelyrecognizethisaspect,leadingtoequaltreatmentof MultitaskLearning.Multitasklearning(MTL)[1,4,12,65]aims
eachcomponentduringpruning,ratherthanaccountingfortheir tolearnasinglemodeltosolvemultipletaskssimultaneouslyby
varyingsensitivities.Second,thechangeintraininglosscouldserve sharinginformationandcomputationamongthem,whichises-
asausefulguideforallocatingsparsityamongdifferentcomponents. sentialforpracticaldeployment.Overtheyears,variousMTLap-
Ifthetraininglossofaspecifictasktendstobestable,wecanprune proacheshavebeenproposed,includinghardparametersharing[3],
moreaggressivelyonthatcomponent,asthetaskheadisrobust softparametersharing[59],andtaskclustering[22].Inhardpa-
topruning.Onthecontrary,ifthelossofaspecifictaskfluctuates rametersharing,asetofparametersinthebackbonemodelare
significantly,weshouldconsiderpruninglessonthatcomponent shared among tasks while in soft parameter sharing, each task
sincethetrainingislesslikelytoconvergeathighersparsitylevels. hasitsownsetofparameters,butthedifferencebetweenthepa-
Motivatedbytheseobservations,weproposeAdapMTL,anadap- rameters of different tasks is regularized to encourage them to
tivepruningframeworkforMTLmodels.AdapMTLdynamicallyad- besimilar.MTLhasbeensuccessfullyappliedtoawiderangeof
justssparsityacrossdifferentcomponents,suchasthesharedback- applications,suchasnaturallanguageprocessing[8,18,29],com-
boneandtask-specificheadsbasedontheirsensitivitytopruning, putervision[17,30,44,58],andreinforcementlearning[42,54].AdapMTL:AdaptivePruningFrameworkforMultitaskLearningModel MM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia
Subsequently,theintegrationofneuralarchitecturesearch(NAS) methodsoftenneglecttheimportanceofthesharedbackbone,lead-
withMTLhasemergedasapromisingdirection.NASforMTL, ingtoequaltreatmentofeachcomponentduringpruning,rather
exemplifiedbyworkslikeMTL-NAS[15],LearningSparseSharing thanaccountingfortheirvaryingimportance.Ourworkaimsto
ArchitecturesforMultipleTasks[51],andControllableDynamic addressthislimitationbyadaptivelyallocatingsparsityacrossthe
Multi-TaskArchitectures[43],focusesondiscoveringoptimalarchi- sharedbackboneandtask-specificheadsbasedontheirimportance
tecturesthatcanefficientlylearnsharedandtask-specificfeatures. andsensitivity.
These approaches, including Adashare [53] and AutoMTL [64],
demonstratethepotentialofdynamicallyadjustingarchitectures 3 Methodology
totherequirementsofmultipletasks,optimizingbothperformance
3.1 Preliminary
andcomputationalefficiency.
Pruning.Pruningtechniqueshavebeenwidelystudiedtore- Weformulatemultitaskmodelpruningasanoptimizationprob-
ducethecomputationalcomplexityofdeepneuralnetworkswhile lem.GivenadatasetD={(ùë•ùëñ ; ùë¶ùëñ 1,ùë¶ùëñ 2,...,ùë¶ ùë°ùëñ), ùëñ ‚àà [1,ùëÅ]},asetofT
maintainingtheirperformance.Earlyworksonpruningfocusedon tasksT = {ùë° 1ùë° 2...,ùë° ùëá},andadesiredsparsitylevelùë† (i.e.theper-
unstructuredweightpruning[20,25],whereunimportantweights centageofzeroweights),themultitaskmodelpruningaimstofind
wereremovedbasedonagivencriterion,andtheremainingweights asparseweightùëä thatminimizesthesumoftask-specificlosses.
were fine-tuned. There are different kinds of criterion metrics, Mathematically,itisformulatedas:
suchasmagnitude-based[20,27],gradient-based[36,37],Hessian-
based [21], connection sensitivity-based [26, 33, 48], and so on.
ùëÅ ùëá
O ent th ire er fiw lo ter rk ss oe rx cp hlo anre nd els st ,r lu ec at du ir ne gd tp or mun oi rn eg effi[5 c7 i, e6 n7 t] i, mw ph leic mh er ne tm ato iov nes
s
m ùëäinL(ùëä;D)=m ùëäin ùëÅ1 ‚àëÔ∏Å ùëñ=1‚àëÔ∏Å ùë°=1Lùë°(ùëì(ùëä,ùë•ùëñ );ùë¶ ùë°ùëñ )
(1)
o hn ash aa tr td rw aca tr ee dp cla ot nfo sir dm es r. aR ble ece an ttt ely n, tt ih oe n,lo st ute gr gy esti tc ink get th hy ap to dt eh ne ss eis ,[ r1 a3 n] - s.t. ùëä ‚ààRùëë, ‚à•ùëä‚à• 0 ‚â§ (1‚àíùë†)¬∑ùëÉ,
domlyinitializedneuralnetworkscontainsubnetworks(winning
wheretheL(¬∑)isthetotallossfunction,Lùë°(¬∑)isthetask-specific
tickets)thatcanbetrainedtoachievecomparableaccuracywith loss for each individual taskùë°,ùëä are the parameters of neural
fewerparameters.Thishasledtofollow-upworks[13,32,38]that networktobelearned,ùëÉ isthetotalnumberofparametersand
provideabetterunderstandingofthepropertiesandinitialization ‚à• ¬∑ ‚à• 0 denotesthe‚Ñì 0-norm,i.e.thenumberofnon-zeroweights.
of winning tickets. Single-Shot Network Pruning (SNIP) [26] is Thekeychallengehereishowtoenforcesparsityonweightùëä
adata-drivenmethodforpruningneuralnetworksinaone-shot
whileminimizingtheloss.Thisinvolvesfindinganoptimalbalance
manner.Byidentifyinganinitialmasktoguideparameterselection,
betweenmaintainingtheperformanceofeachtaskandpruningthe
itmaintainsastaticnetworkarchitectureduringtraining.Some
modeltoachievethedesiredsparsitylevel.Wenextdescribeour
otherwork,likethelayer-wisepruningmethod[23],inspiringly
proposedadaptivepruningalgorithmthatcaneffectivelyhandlethe
attemptstolearnalayer-wisesparsityforindividuallayersrather
uniquecharacteristicsofmultitaskmodelsandefficientlyallocate
thanconsideringthenetworkasawhole.Thisapproachallowsfor
sparsityacrossdifferentcomponentstopreservetheoverallmodel
fine-grainedsparsityallocationacrosslayers.Toreducethetotal
performance.
timeinvolvedinpruningandtraining,pruningduringtraining
techniques[11,35,39]havebeenproposedtodirectlylearnsparse
3.2 AdaptiveMultitaskModelPruning
networkswithouttheneedforaniterativepruningandfinetuning
process.Thesemethodsinvolvetrainingnetworkswithsparsecon- Multitaskmodelstypicallyhaveabackbonesharedacrosstasks
nectivityfromscratch,updatingboththeweightsandthesparsity andtask-specificheads.Weobservethatthesedifferentmodelcom-
structureduringthetrainingprocess. ponents have different sensitivities to pruning and thus should
PruningforMultitaskLearning.Recently,attentionhasshifted betreateddifferently.Thechallengeliesinhowtoautomatically
totheintersectionofMTLandpruningtechniques.Acompactmul- capturethesensitivityofeachmodelcomponenttopruningand
titaskmodelhasthepotentialtodeliverhighperformanceacross leveragethesignaltoautomaticallyallocatesparsityacrosscom-
various tasks while minimizing resource requirements, making ponents.Toaddressthechallenge,weproposeacomponent-wise
itwell-suitedfordeploymentonresource-constraineddevicesor pruning framework that assigns different learnable soft thresh-
inreal-timescenarios.Forexample,MTP[6]focusesonefficient oldstoeachcomponenttocaptureitssensitivitytopruning.The
semanticsegmentationnetworks,demonstratingthepotentialof frameworkthenco-optimizesthethresholdswithmodelweights
multitaskpruningtoenhanceperformanceinspecializeddomains. toautomaticallydeterminethesuitablesparsitylevelforeachcom-
Similarly,theworkbyChengetal.[7]introducesanovelapproach ponentduringtraining.
to multi-task pruning through filter index sharing, optimizing Specifically,weintroduceasetoflearnablesoftthresholdsùõº =
model efficiency through a many-objective optimization frame- {ùõº ùêµ,ùõº 1,ùõº 2,...,ùõº ùëá}foreachcomponent,whereùõº ùêµ representsthe
work.Additionally,Yeetal.[60]proposeaglobalchannelpruning thresholdforthesharedbackboneandùõº ùë° representsthethreshold
methodtailoredformultitaskCNNs,highlightingtheimportance fortheùë°-thtask-specifichead.Thethresholdsùõº aredetermined
ofperformance-awareapproachesinmaintainingaccuracywhile basedonthesignificanceandsensitivityoftherespectivecompo-
reducingmodelsize.Disparse[52]proposesjointlearningandprun- nentsandareadaptivelyupdatedusinggradientdescentduringthe
ingmethodstoachieveefficientmultitaskmodels.However,these backpropagationprocess.Thesoftthresholdùõº ùë° andsparseweight
ùëä ùë° foreachcomponentcanbecomputedasfollows:MM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia MingcanXiang,JiaxunTang,QizhengYang,HuiGuan&TongpingLiu
Figure 2: Difference between hard and soft thresholding.
Hardthresholdingcausesabruptweightdiscontinuitiesdur-
ingtraining,whilesoftthresholdingensuresasmoothrela-
tionshipforconsistentlearning.
ùëÜ(ùëä ùë°,ùõº ùë°)=ùë†ùëñùëîùëõ(ùëä ùë°)¬∑ùëÖùëíùêøùëà(|ùëä ùë°|‚àíùõº ùë°)
(2)
ùõº
ùë°
=ùë†ùëñùëîùëöùëúùëñùëë(ùúÉ init),
Figure3:Breakdownofcomponent-wisesparsityallocation
whereùúÉ initisalearnableparameterthatcontrolstheinitialpruning
duringtraining.WeusetheResNet34backboneandachieve
thresholdùõº ùë°.WewilldiscussthechoiceofùúÉ initinthesupplemen-
90%overallsparsityintheend.
tarymaterial.TheùëÖùëíùêøùëà(¬∑)functionhereisusedtosetzeroweights.
Inotherwords,ifsomeweights|ùëä ùë°|arelessthanthethresholdùõº ùë°,
thenthesparseversionofthisweightùëÜ(ùë§ ùë°,ùõº ùë°)issetto0.Other- process,ourcomponent-wisepruningframeworkcaneffectively
accountforthesedifferencesinsensitivityandadaptivelyadjust
wise,weobtainthesoft-thresholdingversionofthisweight.
thesparsityallocationforeachcomponent.
harT dh te hrr ee sa hs oo ln diw ngh iy siw llue sc th rao to es de inso Ff it guth rere 2s .h So ol fd ti pn ag ra[ m55 e] ter rat sh he ar rit nh ga in
s Although
ùúïùëÜ( ùúïùëä ùëäùë°ùëõ ùëõ,ùõº ùë°ùëõ)
isnon-differentiable,wecanapproximate
ùë°
thebestfitforourapproachasitallowsustocalculatethegradient thegradientsusingthesub-gradientmethod.Inthiscase,wein-
andperformthebackpropagationprocessmoreeffectively. troduce
Bùë°ùëõ
,anindicatorfunctionthatactslikeabinarymask.
AdapMTLreformulatesthepruningprobleminEquation1tofind ThevalueofB ùë°ùëõ shouldbe0ifthesparseversionoftheweight
asetofoptimalthresholdsùõº ={ùõº ùêµ,ùõº 1,ùõº 2,...,ùõº ùëá}acrossdifferent ùëÜ(ùëä ùë°ùëõ,ùõº ùë°ùëõ)isequalto0.Thisindicatorfunctionfacilitatestheap-
componentsasfollows: proximationofgradientsandtheupdateofthesparseweightsand
softthresholdsduringthebackpropagationprocess.Mathemati-
ùëÅ ùëá cally,theindicatorfunctionis:
ùëämi ,ùõºnL(ùëä,ùõº;D)= ùëäm ùë°,i ùõºn
ùë°
ùëÅ1 ‚àëÔ∏Å ùëñ=1‚àëÔ∏Å ùë°=1ùõΩ ùë° ¬∑Lùë°(ùëì(ùëÜ(ùëä ùë°,ùõº ùë°),ùë•ùëñ );ùë¶ ùë°ùëñ )
ùëõ
(cid:40) 0, ifùëÜ(ùëä ùë°ùëõ,ùõº ùë°ùëõ)=0,
s.t. ùõº = ùë†ùëñùëîùëöùëúùëñùëë(ùúÉ init), ùëä ‚ààRùëë, ‚à•ùëä‚à• 0 ‚â§ (1‚àíùë†)¬∑ùëÉ, Bùë° = 1, otherwise. (5)
(3) Byupdatingthesparseweightsùëä ùë°,andsimilarlythesoftthresh-
wheretheùõΩ
ùë°
representstheadaptiveweightingfactorforùë°-thtask,
oldsùõº ùë°,foreachcomponentinthismanner(thederivationprocess
whichwillbeelaboratedinSection3.3.
isprovidedinthesupplementarymaterial),theframeworkcanef-
WenextdescribehowAdapMTLoptimizestheprobleminEquan-
fectivelyanddiscriminativelyallocatesparsityacrossthemultitask
tion3.ConsideringamultitaskmodelwithTtasks,wedividethe
model.Bytakingintoaccountthesignificanceandsensitivityof
weightparametersintoùëä ={ùëä ùêµ,ùëä 1,ùëä 2,...,ùëä ùëá},whereùëä ùêµ rep-
eachcomponent,thisapproachultimatelyleadstomoreefficient
resents the weight parameters for the shared backbone andùëä ùë°
andaccuratemultitasklearning.
representstheweightparametersfortheùë°-thtask-specifichead.
Wederivethegradientdescentupdateequationattheùëõ-thepoch
3.3 AdaptiveWeightingMechanism
forùëä ùë° asfollows:
Thissubsectionintroducestheadaptiveweightingmechanismthat
ùëä ùë°ùëõ+1=ùëä ùë°ùëõ ‚àíùúÇùëõùúïL(ùëä ùúïùëä,ùõº ùëõ;D) d tay sn ka ‚Äôsm ri oc ba ull sy tna ed sj sus tots pt rh ue niw ne gi .g Th ht eo af de aa pc th ivt eas wk eil go hss tinba gs med eco hn ane ia sc mh
ùë°
=ùëä ùë°ùëõ ‚àíùúÇùëõùúï ùúïL ùëÜ(( ùëäùëä ùëõ,ùõº ,ùõº;D ùëõ)) ‚äô ùúïùëÜ(ùëä ùúïùëäùë°ùëõ ùëõ,ùõº ùë°ùëõ) (4) det Ter hm ei rn ae ts iot nh ae leùõΩ sùë° bfo ehr it nh de tùë° h-t eh pt ra os pk oi sn eE dq au da at pio tin ve3 wdu er ii gn hg tit nr gain min ecg h. -
ùë° ùë° ùë° anismaretwofolds.First,ifthetraininglossofaspecifictaskùë°
=ùëä ùë°ùëõ ‚àíùúÇùëõùúï ùúïL ùëÜ(( ùëäùëä ùë°ùëõ,ùõº ,ùõº;D ùë°ùëõ)) ‚äôBùë°ùëõ, t ae nn dd ss ut bo sb ee qus eta nb tl le y, pth ruen new me oc ra en aa gs gsi rg en ssa ivh ei lg yh oe nrw the ai tg ch oti mng pofa nc et no tr ,ùõΩ aùë°
s
whereùúÇùëõ isthelearningrateattheùëõ-thepoch.Weusethepartial thetaskheadisrobusttopruning.Onthecontrary,ifthelossofa
derivativetocalculatethegradients.Asmentionedearlier,different specifictaskfluctuatessignificantly,weshouldconsiderpruning
taskheadsmayhavevaryingsensitivitiestopruningand,conse- less on that component by lowering the weighting factor since
quently,mayrequiredifferentlevelsofsparsitytoachievethebest thetrainingislesslikelytoconvergeathighersparsitylevels.The
accuracy.Bysettingasetoflearnableparametersforeachcom- weightingfactorislearnedinanadaptiveway,eliminatingtheneed
ponentandtreatingthemseparatelyduringthebackpropagation formanualefforttofine-tunethehyper-parameterselaborately.AdapMTL:AdaptivePruningFrameworkforMultitaskLearningModel MM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia
Second,theadaptiveweightingmechanismshouldautomatically NYUv2dataset,therearetotallythreetasks.ForSemanticSegmen-
considerdifferentmultitaskmodelarchitecturesaswell.Theratio tation,weemploythemeanIntersectionoverUnion(mIoU)and
ofbackbonetotaskheadweights,ùëäùëè ùëäùëé ‚Ñéùëêùëò ùëíùëéùëè ùëëùëúùëõùëí,mattersbecauseitmay PixelAccuracy(PixelAcc)asourprimaryevaluationmetricsand
bebeneficialtofocusmoreonpruningthetaskheadsinsteadifthe usecross-entropytocalculatetheloss.Surfacenormalprediction
backboneisalreadyhighlycompact.Forexample,inMobileNet-V2, usestheinverseofcosinesimilaritybetweenthenormalizedpre-
thebackbonehasonly2.2Mparameters,whichis25timesfewer dictionandgroundtruth,andisperformedusingmeanandmedian
thanthetaskhead. angledistancesbetweenthepredictionandthegroundtruth.We
WedefineasetofadaptiveweightsùõΩ ={ùõΩ ùêµ,ùõΩ 1,ùõΩ 2,...,ùõΩ ùëá},where alsoreportthepercentageofpixelswhosepredictioniswithinthe
ùõΩ ùêµ represents the weighting factor for the shared backbone, ùõΩ ùë° anglesof11.25¬∞,22.5¬∞,and30¬∞tothegroundtruth.Depthestimation
representstheweightingfactorfortheùë°-thtask-specifichead.The utilizestheL1loss,withtheabsoluteandrelativeerrorsbetween
weightingfactorcanbeformulatedasfollows: thepredictionandgroundtruthbeingcalculated.Again,Wealso
presenttherelativedifferencebetweenthepredictionandground
ùõΩ ùë° = (cid:18) T1 (cid:205) ùë°Tùúé =L 1(ùë°w ùúéi Lnd ùë°wow ind(cid:14) oL wùë° (cid:14)Lùë°)(cid:19)‚àí1 ¬∑ùúÜ (cid:205)|ùëä ùë°T =ùêµ 1|ùëè |ùëäùëéùëê ùë°ùëò |ùëè ‚Ñéùëú ùëíùëõ ùëéùëí ùëë. (6) t t tr h hu e et rh t ehb ary e rs ec ha to wlc ld ou sl ma ot f oin r1g e.2t t5h a,e s1 kp . s2e .5r I2c n,en a thnta edg c1e o.o 2 nf 5 t3ùõø e. x= O tùëö on fùëé t bhùë• oe( tùë¶ T hùë¶ùëù aùëü ùëî tsùëí ùë° hkùëë eo, n Kùë¶ oùë¶ eùëùùëüùëî m yùëíùë° pùëë y o) d iw na tti at ah s nein dt,
Here,ùúéL ùë°windowistheaveragedeviationofthelosswithinthe EdgeDetectiontasks,themeanabsoluteerrorcomparedtothe
slidingwindowfortheùë°-thtask,whichisthendividedbyLùë° to providedground-truthmapservesasthemainevaluationmetric.
normalizethescale.Wedivideitbythesumofalltaskstonormalize Inmultitasklearningscenarios,tasksinvolvemultipleevaluation
betweendifferenttasks.The(¬∑)‚àí1isamultiplicativeinverse.ùúÜisa metricswithvaluespotentiallyatdifferentscales.Toaddressthis,
scalingfactor,andwewilldiscussthechoiceofùúÜfordifferentar- we compute a single relative performance metric following the
chitecturesinthesupplementarymaterial.|ùëä ùêµ|ùëèùëéùëêùëòùëèùëúùëõùëí,|ùëä ùë°|‚Ñéùëíùëéùëë commonpractice[34][50].
representtheweightparametersofsharedbackboneandùë°-thtask-
specifichead,separately.Therightratiointheequationreveals |ùëÄ|
theimportanceofeachcomponentbyconsideringtheirrelative ‚ñ≥ùëáùëñ = |ùëÄ1
|
‚àëÔ∏Å (‚àí1)ùëôùëó ¬∑(ùëÄ ùëáùëñ,ùëó ‚àíùëÄ ùê∑ùëÄ,ùëó)/ùëÄ ùê∑ùëÄ,ùëó ‚àó100% (7)
parameterizingcontributionstotheoverallmodelstructure.The ùëó=1
weightingfactorùõΩ ùë°isusedtoguidethepruningforthetask-specific
head,dependingonthestabilityofitslossanditscontributionto whereùëô ùëó =1ifalowervalueshowsbetterperformanceforthe
themodel. metricùëÄ ùëó and0otherwise.ùëÄ ùëáùëñ,ùëó,ùëÄ ùê∑ùëÄ,ùëó arethesparseanddense
To make the multitask pruning more robust, we incorporate
modelvalueofmetricùëó,respectively.The‚ñ≥ùëáùëñ isdefinedtocompare
results with their equivalent dense task values and the overall
aslidingwindowmechanismthattracksthepastlossvaluesto
calculatetheaverageùúéL inEquation6insteadofrelying performanceisobtainedbyaveragingtherelativeperformance
solelyonthevariancebetww ein ed no tw
woadjacentepochs.Thisapproach
acrossalltasks,denotedas‚ñ≥ùëá = ùëá1 (cid:205)ùëá ùëñ=1‚ñ≥ùëáùëñ,Thismetricprovides
aunifiedmeasureofrelativeperformanceacrosstasks.Eventually,
providesamorestableandreliableestimationofthefluctuationsin
byemployingthesediverseevaluationmetrics,wecaneffectively
thetasklosses,asitaccountsforalargernumberofsamplesand
assesstheperformanceofourmethodaswellasthecounterparts
reducestheimpactofpotentialoutliersorshort-termvariations.
acrossvarioustasksanddatasets.
4 Experiments
4.1.3 BaselinesforComparison. WecompareourworkwithLTH[13],
4.1 ExperimentSettings IMP[19],SNIP[26],andDiSparse[52].ForLTH,wefirsttraina
densemodelandsubsequentlypruneituntilthedesiredsparsity
4.1.1 Datasetsandtasks. Weconducttheexperimentsontwopop-
levelisreached,yieldingthewinningtickets(sparsenetworkstruc-
ularmulti-taskdatasets:NYU-v2[47],andTiny-Taskonomy[63].
ture).Wethenresetthemodeltoitsinitialweightstostartthe
TheNYU-v2datasetiscomposedofRGB-Dindoorsceneimages
sparsetrainingprocess.ForIMP,weiterativelyremovetheleast
andcoversthreetasks:13-classsemanticsegmentation,depthes-
importantweights,determinedbytheirmagnitudes.ForSNIPand
timation,andsurfacenormalprediction.Thetrainingsetconsists
IMP,wedirectlyusetheofficialimplementationprovidedbytheau-
of795images,whilethetestingsetincludes654images.Forthe
thorsfromGitHub.ForDiSparse,thelatestmultitaskpruningwork
Tiny-Taskonomydataset,theexperimentsinvolvejointtraining
andfirst-of-its-kind,weutilizetheofficialPyTorchimplementation
onfivetasks:SemanticSegmentation,SurfaceNormalPrediction,
andconfigurethemethodtousetheDiSparsedynamicmechanism,
DepthPrediction,KeypointDetection,andEdgeDetection.The
whichisclaimedasthebest-performingapproachinthepaper.We
trainingsetincludes1.6millionimages,whilethetestsetcomprises
alsotrainafullydensemultitaskmodelasourbaseline,whichwill
0.3millionimages.Thetrainingsetincludes1.6millionimagesfrom
beusedtocalculateasinglerelativeperformancemetricNorm.
25differentclasses,whilethetestsetcomprises0.3millionimages
Score.
across5classes.
Weusethesamebackbonemodelatthesamesparsitylevelacross
4.1.2 EvaluationMetricsandLossFunctions. Weadoptarangeof allmethodsforafaircomparison.Inourwork,wedefineoverall
evaluationmetricsfordifferenttasks,evaluatingthemodelperfor- sparsityasthepercentageofweightsprunedfromtheentireMTL
manceatdifferentsparsitylevelstoprovideacomprehensiveview model,whichincludesboththesharedbackboneandtask-specific
ofthemodel‚Äôseffectivenessandrobustnessacrosstasks.Onthe heads.WeutilizeDeeplab-ResNet34[5]andMobileNetV2[45]astheMM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia MingcanXiang,JiaxunTang,QizhengYang,HuiGuan&TongpingLiu
Table1:Comparisonwithstate-of-the-artpruningmethodsontheNYU-V2datasetusingtheDeeplab-ResNet34backbone.
Eachpruningmethodenforcesaconsistentoverallsparsityof90%,withthe‚ñ≥ùëá indicatingthenormalizedperformanceofall
threetaskstothebaselinedensemodel‚Äôsperformance.Wealsoreporttheevaluationmetricsforeachtaskandthesparsity
allocationforeachcomponent.
ùëá 1:SemanticSeg. ùëá 2:SurfaceNormalPrediction ùëá 3:DepthEstimation Sparsity%
Model mIoU‚Üë Api cx ce ‚Üël ‚ñ≥ùëá1‚Üë MeaE nrr Mor e‚Üì dian1A 1.n 2g 5¬∞le 2ùúÉ 2,w .5¬∞ithi 3n 0¬∞‚Üë ‚ñ≥ùëá2‚Üë AE br sr .o Rr e‚Üì
l.
1.25‚ñ≥, 1w .2i 5t ^h 2in 1.25^‚Üë
3
‚ñ≥ùëá3‚Üë bB oa nck
e
hS e. aS d. S h.N ea.P d. hD e. aE d. ‚ñ≥ùëá‚Üë
DenseModel(baseline) 25.54 57.91 0.00 17.11 14.95 36.35 72.25 85.44 0.00 0.55 0.22 65.21 89.87 97.52 0.00 - 0.00
SNIP[26] 24.09 55.32 -10.15 16.94 14.93 36.17 72.39 86.98 2.63 0.61 0.23 60.61 87.88 96.77 -25.4985.4690.24 92.28 91.17-11.00
LTH[13] 25.42 57.98 -0.35 16.73 15.08 35.20 72.35 87.22 0.41 0.57 0.22 60.93 88.64 96.20 -12.9278.3290.54 95.21 95.49 -4.29
IMP[19] 25.68 57.86 0.46 16.86 15.18 35.53 71.96 86.26 -1.77 0.56 0.22 65.23 89.29 97.53 -3.82 74.9892.34 97.23 95.15 -1.71
DiSparse[52] 25.71 58.08 0.96 17.03 15.23 35.10 71.85 86.22 -4.48 0.57 0.22 64.93 88.64 97.20 -5.76 75.0790.41 98.51 94.86 -3.10
AdapMTLw/oadaptivethresholds 25.59 57.53 -0.46 17.26 15.75 36.21 71.53 85.91 -7.06 0.58 0.22 62.52 87.12 96.50 -13.6879.1289.37 96.85 95.74 -7.07
AdapMTL(ours) 26.28 58.29 3.55 16.92 14.91 36.3672.97 86.29 3.41 0.550.2265.39 89.93 97.58 0.38 71.7493.18 99.26 96.22 2.45
1.00 1.00
0.98
0.95
0.96
0.90 0.94
0.92
0.85 Dense Model 0.90 Dense Model
Ours Ours
0.80
LTH LTH
Disparse 0.85 Disparse
0.75
SNIP SNIP
IMP IMP
0.70
0.80
50 60 70 80 90 95 99 50 60 70 80 90 95 99
Overall sparsity (%) Overall sparsity (%)
(a)ResNet34 (b)MobileNetV2
Figure4:Comparisonofstate-of-the-artmethods,includingDiSparse[52],LTH[13],SNIP[26],andIMP[19],ontheNYUv2
dataset,evaluatedwithdifferentMTLbackbonesandundervarioussparsitysettings.
backbonemodels,andtheAtrousSpatialPyramidPooling(ASPP) SNIP[26]exhibitsthelowestperformanceinthemulti-tasksce-
architecture[5]asthetask-specifichead.Bothofthemarepopular nariobecauseitspruningmaskisdeterminedfromasinglebatchof
architecturesforpixel-wisepredictiontasks.Weshareacommon data‚Äôsgradient,whichtreatsallcomponents,includingtheshared
backboneforalltaskswhileeachtaskhasanindependenttask- backbone,equally.Sinceallinputinformationpassesthroughthe
specificheadbranchingoutfromthefinallayerofthebackbone, sharedbackbone,accuracylossintheshallowlayersisinevitable,
whichiswidelyusedinmultitaskingscenarios. regardlessofhowwellthetaskheadsperformwithrelativelyhigh
density.LTH‚Äôs[13]winningticketsdonotsufficientlyfocuson
thebackbone,astheyintentionallycreateadensesurfacenormal
predictiontaskhead.Althoughthisapproachperformswellonthis
4.2 ExperimentResults
specifictask,thebiasstillcausesanimbalanceinthemetricsacross
4.2.1 ResultsonNYU-V2. Wefirstpresentthecomparisonresults alltasks,resultinginalower‚ñ≥ùëá score.IMP[19]achievesagood
withstate-of-the-artmethodsontheNYU-V2datasetintable1. normalizedscoreacrossalltasks.However,thismethodistrainedin
Overall,AdapMTLoutperformsallothermethodsbyasignificant aniterativemannerandprunesthemodelstep-by-step,resultingin
marginacrossmostmetricsandachievesthehighest‚ñ≥ùëá.Recall asignificantlylongertrainingtime.DiSparse[52]learnsaneffective
thatthemajordifferencebetweenourmethodandthebaselines densebackbonebyadoptingaunanimousdecisionacrossalltasks.
liesinourabilitytoadaptivelylearnthesparsityallocationacross However,itfallsshortofdifferentiatingtherelativesensitivities
thecomponentsadaptively,maintainingadensesharedbackbone betweenspecifictaskheads,leadingtoanimbalancednormalized
(71.74%)whilekeepingthetask-specificheadsrelativelysparse. scoreamongalltasks.Here,weaddanadditionalrow,AdapMTL
Withinthescopeofourresearch,wecharacterizeoverallsparsity withoutadaptivethresholds,todemonstratetheeffectivenessof
asthepercentageofweightsprunedfromtheentireMTLmodel, ourapproach.Ratherthanusingmultipleadaptivethresholds,this
whichincludesboththesharedbackboneandtask-specificheads. versionutilizesasinglesharedthresholdforallcomponents.As
erocs
tset
dezilamroN
erocs
tset
dezilamroNAdapMTL:AdaptivePruningFrameworkforMultitaskLearningModel MM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia
Table2:Comparisonwithstate-of-the-artpruningmethodsontheNYU-V2datasetusingtheMobileNetV2backbone.Each
pruningmethodenforcesaconsistentoverallsparsityof90%,withthe‚ñ≥ùëá indicatingthenormalizedperformanceofallthree
taskstothebaselinedensemodel‚Äôsperformance.Wealsoreporttheevaluationmetricsforeachtaskandthesparsityallocation
foreachcomponent.
ùëá 1:SemanticSeg. ùëá 2:SurfaceNormalPrediction ùëá 3:DepthEstimation Sparsity%
Model mIoU‚Üë Api cx ce ‚Üël ‚ñ≥ùëá1‚Üë MeaE nrr Mor e‚Üì dian1A 1n .2g 5l ¬∞eùúÉ 22, .w 5¬∞ithi 3n 0¬∞‚Üë ‚ñ≥ùëá2‚Üë AE br sr .o Rr e‚Üì
l.
1.25‚ñ≥, 1w .2i 5t ^h 2in 1.25^‚Üë 3‚ñ≥ùëá3‚Üë bB oa nck
e
hS e. aS d. S h.N ea.P d. hD e. aE d. ‚ñ≥ùëá‚Üë
DenseModel[5](baseline) 19.94 48.71 0.00 17.85 16.21 29.77 72.19 86.19 0.00 0.64 0.24 58.93 86.27 96.16 0.00 - 0.00
SNIP[26] 18.96 46.93 -8.57 18.33 16.97 28.93 71.21 85.78 -12.03 0.64 0.25 56.75 85.71 95.33 -9.38 78.4688.19 92.08 90.25-9.99
LTH[13] 19.14 47.25 -7.01 17.67 16.32 29.67 72.15 86.22 -0.03 0.65 0.25 57.68 85.89 96.13 -8.32 71.3288.34 92.19 90.52-5.12
IMP[13] 18.76 48.12 -7.13 18.71 16.68 29.63 71.76 85.91 -9.11 0.64 0.2359.75 86.52 96.31 6.00 68.4988.07 95.13 87.74-3.41
DiSparse[52] 19.87 48.83 -0.10 17.92 16.79 29.87 71.76 85.64 -4.87 0.65 0.24 58.42 85.72 96.28 -2.94 65.2287.21 93.55 90.53-2.64
AdapMTLw/oadaptivethresholds 18.93 47.51 -7.53 18.16 16.87 28.37 71.53 86.63 -10.91 0.65 0.24 58.26 85.82 95.92 -3.47 73.6188.64 92.37 89.82-7.30
AdapMTL(ours) 20.16 49.14 1.99 17.53 15.96 30.1672.3686.51 5.25 0.64 0.24 59.03 86.57 96.38 0.75 52.7486.18 94.72 90.76 2.66
expected,performancesignificantlydeterioratesbecauseauniform Table3:ResultsonTiny-Taskonomydataset.T1:Semantic
thresholdmakesithardtocapturethenuancesindifferentcompo- Segmentation,T2:SurfaceNormalPrediction,T3:DepthPre-
nents‚Äôsensitivity. diction,T4:KeypointEstimation,T5:EdgeEstimation.
Moreover,weextendedourexperimentstodifferentmodelarchi-
tecturestoassessthemodel-agnosticnatureofourmethod,using Model ‚ñ≥ùëá‚Üë ‚ñ≥ùëá‚Üë ‚ñ≥ùëá‚Üë ‚ñ≥ùëá‚Üë ‚ñ≥ùëá‚Üë ‚ñ≥ùëá‚Üë
1 2 3 4 5
SNIP -11.2 -15.7 -9.4 +1.2 -2.8 -7.58
MobileNetV2asanalternativearchitecture.Theresults,detailed
LTH -9.9 -1.3 -10.7 +0.5 +3.1 -3.66
inTable2,showhowAdapMTLadeptlymanagesthedenserepre-
IMP -6.3 -9.7 +3.1 -1.1 +2.4 -2.32
sentationofMobileNetV2‚Äôscompactbackbone,ensuringitremains DiSparse -1.6 +1.2 -3.9 -1.5 +4.2 -0.32
sufficientlydense(52.74%)whileenforcinghighersparsityinthe AdapMTLw/oadaptivethresholds -8.7 -12.6 -4.7 +0.2 -1.4 -5.44
AdapMTL(ours) +2.8 +4.7 +1.5 +0.5 +4.9 +2.88
task-specificheads.Thisisveryimportant,especiallywithsuch
backbonecompactarchitectureswhereover-pruningthebackbone
caneasilyleadtosignificantdegradationinaccuracy.Ourapproach high sparsity with minimal performance degradation for multi-
ensuresthatthebackboneremainsdenseenough,therebypreserv- taskmodels.Moreresultsontheotherdatasets,usingthedifferent
ingoverallperformance. architectures,canbefoundinthesupplementarymaterial.
4.2.2 Resultsundervarioussparsitysettings. Weshowacompari-
sonofresultsunderdifferentsparsitysettingsusingdifferentback-
bones,namelyResNet34andMobileNetV2,asillustratedinFigure4,
whereAdapMTLconsistentlydemonstratessuperiorityoverother
1.00
e
m tice eth [o 34d ]s. [T 50h ]e ,n iso orm bta al ii nz ee dd bte yst as vc eo rare g, info gll to hw ein reg lat th ie veco pm erm foo rn mp ar na cc e- 00 .. 99 05
ed
scor
across all tasks with respect to the dense model. We observe a 0.85
aliz
m
slightlybetterperformanceformediumsparsitylevels(from50%to 0.80 or
N
80%),whichevensurpassesdedicateddensemultitasklearningap- 0.75 z:
proachesdespitethehighsparsityenforced.Thisobservationaligns 0.70
withourassumptionsandmotivatestheresearchcommunityto
furtherexploreanddevelopsparsemodels.ThescoreofSNIPdrops 0.99
0.97
s
b
ei ffeg
c
en
a
ci
u
tfi
is
vc
e
ea ln
i
ytt .l fy aia ls sh toig mhe ar insp taa ir nsit ty hele dv ee nls si( t> y90 o% f) thar ee se hn af ro er dce bd a. cT kh bi os ni es
0.50
0 x.6
:
0
Back0 b.7 o0
ne
s0 p. a8 r0
sity0.90
0.905.99
0.900 y:. 9
H0
3
e. a9
d
5 sparsity
4.2.3 ResultsonTiny-Taskonomy. OntheTiny-Taskonomydataset,
Figure5:Visualizationcomparingthesensitivityoftheback-
whichencompassesfivedistincttasks,AdapMTLexhibitsamore
boneandtaskheadinaMobileNetV2backboneMTLmodel.
consistentperformanceacrossalltasks,asdetailedinTable3.Here,
They-axisrepresentsthetotalsparsityofalltaskheads.
weusetheResNetbackboneatsparsity90%.Ourmethodconsis-
tentlyachievedthehighestscoresineachtask,unlikeothermethods
whichexhibitednoticeablebiases.TheDiSparsemethodstruggles
4.3 Analysis
toachieveunanimousdecisions,particularlyasthenumberoftasks
increases,highlightingakeylimitationinitsapproach. 4.3.1 Pruningsensitivity. AdapMTLresultsindifferentsparsityfor
TheconsistentsuperiorityofAdapMTLacrossbothNYUv2and backboneparametersandtask-specificparameters,indicatingthat
Tiny-Taskonomydatasets,andwithdifferentbackbonearchitec- itcapturestheirdifferentsensitivitytopruning.Tocomparethe
tures,highlightstheeffectivenessofourapproachinachieving sensitivitytopruningbetweenthesharedbackboneandtaskheads,MM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia MingcanXiang,JiaxunTang,QizhengYang,HuiGuan&TongpingLiu
Table4:ComputationalcostofAdapMTL Table5:AblationStudyonNYU-V2.T1:SemanticSegmenta-
tion,T2:SurfaceNormalPrediction,T3:DepthPrediction.
Method Sparsity(%) Params ‚ñ≥ùëá ‚Üë FLOPs‚Üì
Deeplab-ResNet34 0 197.6M - 56.32G Model ‚ñ≥ùëá 1‚Üë ‚ñ≥ùëá 2‚Üë ‚ñ≥ùëá 3‚Üë ‚ñ≥ùëá‚Üë
AdapMTL 79.83 39.52M 6.7 9.04G w/oùúÜ(=5) 1.26 1.74 -1.83 0.39
AdapMTL 85.01 29.64M 4.3 7.84G w/oslidingwindow 3.07 2.84 -0.49 1.81
AdapMTL 90.03 19.77M 2.45 5.32G w/oadaptivethresholds -0.46 -7.06 -13.68 -7.07
MobileNetV2 0 155.2M - 37.32G only2adaptivethresholds -0.32 -3.28 -9.74 -4.45
AdapMTL 80.12 31.04M 7.8 5.79G AdapMTL 3.55 3.41 0.38 2.45
AdapMTL 85.03 23.28M 5.2 4.21G
AdapMTL 89.93 15.51M 2.66 2.98G
Datasets
2.8 NYU-V2
wecreatea3Dplot,asshowninFigure5.Thex-axisrepresents Tiny-Taskonomy
2.6
thesharedbackbonesparsityfrom50%to99%,whilethey-axis
T 2.4
representsthetotalheadsparsityforallthreetasksfrom90%to
2.2
99%.Thez-axisrepresentsthenormalizedscore.
2.0
Fromthexz-plane,wecanobservethatthenormalizedscore
dropssignificantlywhenweprunethebackboneatsparsitylev- 1.8
0 100 200 300 400 500 600 700 800
elsof90%andhigher.Incontrast,fromtheyz-plane,wecansee Sliding Window Size
thatthetaskheadsarehighlyrobusttopruning,astheymaintain
Figure6:Choiceofslidingwindowsize
agoodnormalizedscoreevenwhenextremesparsitylevelsare
reached.Thisobservationhighlightstheimportanceofpreserving
settings,underscoringtheindispensablenatureofeachproposed
thesharedbackbone‚Äôsdensityandsuggeststhatpruningstrategies
component.
shouldprioritizemaintainingthebackbone‚Äôsperformancewhile
Wehaveimplementedaslidingwindowmechanismtoenhance
aggressivelypruningthetask-specificheads.
therobustnessandaccuracyofourpruningstrategy.Thismecha-
4.3.2 Computationalcost. ThecomputationalcostoftheAdapMTL nismispivotalintrackingthelossvaluesoverasequenceofepochs
undervaryingsparsitylevelsisdetailedinTable4,whichillustrates tocomputetheaveragechangeinloss,ùúéL ,asformalizedin
window
asignificantreductioninbothparametersandFLOPsassparsity Equation6.Byintegratingthisapproach,wesignificantlymitigate
increases.Thesereductionshighlightnotonlytheadaptabilityof theinfluenceofabruptvariationsandpotentialoutliersthatmay
AdapMTLacrossdifferentarchitecturesbutalsoitscapabilityto occurintask-specificlosscalculations.Theslidingwindow,setat
maintainabalancebetweenperformance,measuredby‚ñ≥ùëá,and asizeof400asdemonstratedinFigure6,representsanoptimal
efficiency,evidencedbythesubstantialdecreaseinFLOPs.Thisbal- balancebetweencomputationalmemorydemandsandtheneed
anceiscrucialfordeployinghigh-performancemodelsinresource- foracomprehensivedatascope.Thissizeensuresthatthemodel
constrainedenvironments.Byleveragingspecializedhardwareand capturessufficienttemporallossinformationwithoutexcessive
softwaresolutionsthatcanefficientlyhandlesparsematrixoper- memoryconsumption,therebymaintainingefficiency.
ations,suchassparsematrix-vectormultiplication(SpMV),these
modelscanachievefasterinferencetimes[14,39,56]. 5 Conclusion
Inthispaper,weproposeanoveladaptivepruningmethoddesigned
4.4 AblationStudies
specificallyformultitasklearning(MTL)scenarios.Ourapproach
Weconductedablationstudiestovalidatetheeffectivenessofthe effectivelyaddressesthechallengesofbalancingoverallsparsity
proposedadaptivemultitaskmodelpruning(Section3.2),andthe andaccuracyforalltasksinmultitaskmodels.AdapMTLintroduces
adaptiveweightingmechanism(Equation6).Wetestedvariations multiplelearnablesoftthresholds,eachindependentlyassignedto
includingmodelswithoutadaptivethresholds,whereallcompo- thesharedbackboneandtask-specificheadstocapturethenuances
nentsshareasinglethreshold,andmodelswithonlytwoadaptive indifferentcomponents‚Äôsensitivitytopruning.Ourmethodco-
thresholds,wherethebackbonehasauniquethresholdwhileother optimizesthesoftthresholdsandmodelweightsduringtraining,
taskheadsshareanother.Theresults,presentedinTable5,highlight enablingautomaticdeterminationoftheidealsparsitylevelfor
thecriticalroleofadaptivethresholding.Modelswithoutadaptive eachcomponenttoachievehightaskaccuracyandoverallsparsity.
thresholdsshowedsignificantlypoorerperformance,withadrastic Furthermore,AdapMTLincorporatesanadaptiveweightingmecha-
decreasein‚ñ≥ùëá,especiallyaffectingtaskswithhighersensitivityto nismthatdynamicallyadjuststheimportanceoftask-specificlosses
pruning,suchasDepthPrediction.Conversely,thefullAdapMTL basedoneachtask‚Äôsrobustnesstopruning.Theeffectivenessof
configuration,employingindependentthresholdsforeachcompo- AdapMTLhasbeenextensivelyvalidatedthroughcomprehensive
nent,achievedthebest‚ñ≥ùëá score.Thesevariationshelpillustrate experimentsontheNYU-v2andTiny-Taskonomydatasetswith
theimpactandnecessityofdifferentiatedthresholdinginmultitask differentarchitectures.Theresultsdemonstratethatourmethod
environments.TheresultsconfirmthatourfullAdapMTLsetup, outperformsstate-of-the-artpruningmethods,therebyestablishing
withallcomponentsactive,performssuperiorlyacrossdifferent itssuitabilityforefficientandeffectivemultitasklearning.AdapMTL:AdaptivePruningFrameworkforMultitaskLearningModel MM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia
Acknowledgments
[23] AdityaKusupati,VivekRamanujan,RaghavSomani,MitchellWortsman,Prateek
Jain,ShamKakade,andAliFarhadi.2020.Softthresholdweightreparameter-
ThismaterialisbaseduponworksupportedbytheNationalScience
izationforlearnablesparsity.InInternationalConferenceonMachineLearning.
Foundation under Grant No. CNS-2312396, CNS-2338512, CNS- PMLR,5544‚Äì5555.
2224054,andDMS-2220211,DUE-2215193,CCF-2024253,andCNS- [24] YannLeCun,L√©onBottou,YoshuaBengio,andPatrickHaffner.1998.Gradient-
basedlearningappliedtodocumentrecognition.Proc.IEEE86,11(1998),2278‚Äì
1750760.Anyopinions,findings,conclusions,orrecommendations 2324.
expressedinthismaterialarethoseoftheauthor(s)anddonot [25] YannLeCun,JohnSDenker,andSaraASolla.1990. Optimalbraindamage.
Advancesinneuralinformationprocessingsystems(1990),598‚Äì605.
necessarilyreflecttheviewsoftheNationalScienceFoundation.
[26] NamhoonLee,ThalaiyasingamAjanthan,andPhilipHSTorr.2018. Snip:
PartoftheworkisalsosupportedbyAdobegiftfunding. Single-shotnetworkpruningbasedonconnectionsensitivity. arXivpreprint
arXiv:1810.02340(2018).
[27] HaoLi,AsimKadav,IgorDurdanovic,HananSamet,andHansPeterGraf.2016.
References Pruningfiltersforefficientconvnets.arXivpreprintarXiv:1608.08710(2016).
[28] ShaohuiLin,RongrongJi,YuchaoLi,YongjianWu,FeiyueHuang,andBaochang
[1] AndreasArgyriou,TheodorosEvgeniou,andMassimilianoPontil.2006.Multi- Zhang.2020.HRank:FilterPruningusingHigh-RankFeatureMap.InProceedings
taskfeaturelearning. Advancesinneuralinformationprocessingsystems19 oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.1529‚Äì
(2006). 1538.
[2] TadasBaltru≈°aitis,ChaitanyaAhuja,andLouis-PhilippeMorency.2018.Multi- [29] XiaodongLiu,JianfengGao,XiaodongHe,LiDeng,KevinDuh,andYe-YiWang.
modalmachinelearning:Asurveyandtaxonomy.IEEEtransactionsonpattern 2015.Representationlearningusingmulti-taskdeepneuralnetworksforsemantic
analysisandmachineintelligence41,2(2018),423‚Äì443. classificationandinformationretrieval.InProceedingsofthe53rdAnnualMeeting
[3] RichCaruana.1993.Multitasklearning:Aknowledge-basedsourceofinductive oftheAssociationforComputationalLinguisticsandthe7thInternationalJoint
bias.InMachineLearningProceedings1993.Elsevier,41‚Äì48. ConferenceonNaturalLanguageProcessing.912‚Äì921.
[4] RichCaruana.1997.Multitasklearning.Machinelearning28(1997),41‚Äì75. [30] YingluLiu,MingcanXiang,HailinShi,andTaoMei.2021.One-stageContext
[5] Liang-ChiehChen,GeorgePapandreou,IasonasKokkinos,KevinMurphy,and andIdentityHallucinationNetwork.InProceedingsofthe29thACMInternational
AlanLYuille.2017.Deeplab:Semanticimagesegmentationwithdeepconvolu- ConferenceonMultimedia.835‚Äì843.
tionalnets,atrousconvolution,andfullyconnectedcrfs.IEEEtransactionson [31] ZehaoLiu,HaoliangLi,ShuichengShen,JunjieYan,XiaolinZhang,andNenghai
patternanalysisandmachineintelligence40,4(2017),834‚Äì848. Wang.2019.MetaPruning:MetaLearningforAutomaticNeuralNetworkChannel
[6] XinghaoChen,YimanZhang,andYunheWang.2022. MTP:multi-taskprun- Pruning.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
ingforefficientsemanticsegmentationnetworks.In2022IEEEInternational Vision.3296‚Äì3305.
ConferenceonMultimediaandExpo(ICME).IEEE,1‚Äì6. [32] ZhuangLiu,MingjieSun,TinghuiZhou,GaoHuang,andTrevorDarrell.2018.
[7] HanjingCheng,ZidongWang,LifengMa,XiaohuiLiu,andZhihuiWei.2021. Rethinkingthevalueofnetworkpruning.InProceedingsoftheIEEEInternational
Multi-taskpruningviafilterindexsharing:Amany-objectiveoptimizationap- ConferenceonComputerVision.7002‚Äì7012.
proach.CognitiveComputation13(2021),1070‚Äì1084. [33] Jian-HaoLuoandJianxinWu.2020. Neuralnetworkpruningwithresidual-
[8] RonanCollobertandJasonWeston.2008.Aunifiedarchitecturefornaturallan- connectionsandlimited-data.InProceedingsoftheIEEE/CVFConferenceonCom-
guageprocessing:Deepneuralnetworkswithmultitasklearning.InProceedings puterVisionandPatternRecognition.1458‚Äì1467.
ofthe25thinternationalconferenceonMachinelearning.ACM,160‚Äì167. [34] Kevis-KokitsiManinis,IlijaRadosavovic,andIasonasKokkinos.2019.Attentive
[9] XiaohanDong,HuiziMao,TianchenLiu,YimingYang,JiHuang,SenChen, single-taskingofmultipletasks.InProceedingsoftheIEEE/CVFConferenceon
ZhangYang,GengYuanTong,ZhenLin,SongTang,etal.2021.HAWQ:Hessian ComputerVisionandPatternRecognition.1851‚Äì1860.
AWareQuantizationofNeuralNetworkswithMixed-Precision.InProceedingsof [35] DecebalConstantinMocanu,ElenaMocanu,PeterStone,PhuongHNguyen,
theIEEE/CVFInternationalConferenceonComputerVision.9234‚Äì9243. MadeleineGibescu,andAntonioLiotta.2018.Scalabletrainingofartificialneural
[10] DavidLDonoho.1995.De-noisingbysoft-thresholding.IEEEtransactionson networkswithadaptivesparseconnectivityinspiredbynetworkscience.In
informationtheory41,3(1995),613‚Äì627. Proceedingsofthe35thInternationalConferenceonMachineLearning.1125‚Äì1143.
[11] UtkuEvci,TrevorGale,JacobMenick,PabloSamuelCastro,ErichElsen,Jakob [36] DmitryMolchanov,ArseniiAshukha,andDmitryVetrov.2017. Variational
Uszkoreit,andAvitalDubey.2020.Riggingthelottery:Makingallticketswinners. dropoutsparsifiesdeepneuralnetworks.InInternationalConferenceonMachine
InInternationalConferenceonLearningRepresentations. Learning.PMLR,2498‚Äì2507.
[12] TheodorosEvgeniouandMassimilianoPontil.2004. Regularizedmulti‚Äìtask [37] PavloMolchanov,ArunMallya,StephenTyree,IuriFrosio,andJanKautz.2019.
learning.InProceedingsofthetenthACMSIGKDDinternationalconferenceon Importanceestimationforneuralnetworkpruning.InProceedingsoftheIEEE
Knowledgediscoveryanddatamining.109‚Äì117. ConferenceonComputerVisionandPatternRecognition.11264‚Äì11272.
[13] JonathanFrankle,GintareKarolinaDziugaite,DanielMRoy,andMichaelCarbin. [38] AriSMorcos,HaonanYu,MichelaPaganini,andYuandongTian.2019.Oneticket
2020.Thelotterytickethypothesisatscale.InternationalConferenceonLearning towinthemall:generalizinglotteryticketinitializationsacrossdatasetsand
Representations(2020). optimizers.InAdvancesinNeuralInformationProcessingSystems.11644‚Äì11655.
[14] TrevorGale,ErichElsen,andSaraHooker.2019.TheStateofSparsityinDeep [39] HeshamMostafa,XiaoxiaoWang,andDecebalConstantinMocanu.2019.Pa-
NeuralNetworks.InarXivpreprintarXiv:1902.09574. rameterefficienttrainingofdeepconvolutionalneuralnetworksbydynamic
[15] YuanGao,HaopingBai,ZequnJie,JiayiMa,KuiJia,andWeiLiu.2020.Mtl-nas: sparsereparameterization.InProceedingsofthe36thInternationalConferenceon
Task-agnosticneuralarchitecturesearchtowardsgeneral-purposemulti-task MachineLearning.4648‚Äì4657.
learning.InProceedingsoftheIEEE/CVFConferenceoncomputervisionandpattern [40] JiquanNgiam,AdityaKhosla,MingyuKim,JuhanNam,HonglakLee,andAn-
recognition.11543‚Äì11552. drewYNg.2011.Multimodaldeeplearning.InProceedingsofthe28thinternational
[16] SiddhantGarg,LijunZhang,andHuiGuan.2023.StructuredPruningforMulti- conferenceonmachinelearning(ICML-11).689‚Äì696.
TaskDeepNeuralNetworks.arXivpreprintarXiv:2304.06840(2023). [41] WeiNiu,XiaolongMa,ShengLin,ShihaoWang,XuehaiQian,XueLin,Yanzhi
[17] RossGirshick.2015.FastR-CNN.InProceedingsoftheIEEEinternationalconfer- Wang,andBinRen.2020.Patdnn:Achievingreal-timednnexecutiononmobile
enceoncomputervision.1440‚Äì1448. deviceswithpattern-basedweightpruning.InProceedingsoftheTwenty-Fifth
[18] HuiGuan,XipengShen,andHamidKrim.2017.Egeria:Aframeworkforauto- InternationalConferenceonArchitecturalSupportforProgrammingLanguagesand
maticsynthesisofHPCadvisingtoolsthroughmulti-layerednaturallanguage OperatingSystems.907‚Äì922.
processing.InProceedingsoftheInternationalConferenceforHighPerformance [42] EmilioParisotto,JimmyLeiBa,andAntoineBordes.2016.Actor-mimic:Deep
Computing,Networking,StorageandAnalysis.1‚Äì14. multitaskandtransferreinforcementlearning.InInternationalConferenceon
[19] SongHan,HuiziMao,andWilliamJDally.2015.Deepcompression:Compressing LearningRepresentations.
deepneuralnetworkswithpruning,trainedquantizationandhuffmancoding. [43] DriptaSRaychaudhuri,YuminSuh,SamuelSchulter,XiangYu,MasoudFaraki,
arXivpreprintarXiv:1510.00149(2015). AmitKRoy-Chowdhury,andManmohanChandraker.2022. Controllabledy-
[20] SongHan,JeffPool,JohnTran,andWilliamDally.2015.Learningbothweights namicmulti-taskarchitectures.InProceedingsoftheIEEE/CVFConferenceon
andconnectionsforefficientneuralnetwork.InAdvancesinneuralinformation ComputerVisionandPatternRecognition.10955‚Äì10964.
processingsystems.1135‚Äì1143. [44] ShaoqingRen,KaimingHe,RossGirshick,andJianSun.2015. FasterR-CNN:
[21] BabakHassibiandDavidStork.1992. Secondorderderivativesfornetwork Towardsreal-timeobjectdetectionwithregionproposalnetworks.InAdvances
pruning:Optimalbrainsurgeon. Advancesinneuralinformationprocessing inneuralinformationprocessingsystems.91‚Äì99.
systems5(1992). [45] MarkSandler,AndrewHoward,MenglongZhu,AndreyZhmoginov,andLiang-
[22] LaurentJacob,FrancisRBach,andJean-PhilippeVert.2009.Clusteredmulti-task ChiehChen.2018. Mobilenetv2:Invertedresidualsandlinearbottlenecks.In
learning:Aconvexformulation.InAdvancesinneuralinformationprocessing ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.
systems.745‚Äì752. 4510‚Äì4520.MM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia MingcanXiang,JiaxunTang,QizhengYang,HuiGuan&TongpingLiu
[46] J√ºrgenSchmidhuber.2015. Deeplearninginneuralnetworks:Anoverview.
Neuralnetworks61(2015),85‚Äì117.
[47] NathanSilberman,DerekHoiem,PushmeetKohli,andRobFergus.2012.Indoor
segmentationandsupportinferencefromrgbdimages. ECCV(5)7576(2012),
746‚Äì760.
[48] JingtongSu,YihangChen,TianleCai,TianhaoWu,RuiqiGao,LiweiWang,
andJasonDLee.2020.Sanity-checkingpruningmethods:Randomticketscan
winthejackpot. AdvancesinNeuralInformationProcessingSystems33(2020),
20390‚Äì20401.
[49] ChenSun,AustinMyers,CarlVondrick,KevinMurphy,andCordeliaSchmid.
2019.Videobert:Ajointmodelforvideoandlanguagerepresentationlearning.
InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision.7464‚Äì
7473.
[50] RuoyuSun,DaweiLi,ShiyuLiang,TianDing,andRayadurgamSrikant.2020.
Thegloballandscapeofneuralnetworks:Anoverview.IEEESignalProcessing
Magazine37,5(2020),95‚Äì108.
[51] TianxiangSun,YunfanShao,XiaonanLi,PengfeiLiu,HangYan,XipengQiu,and
XuanjingHuang.2020.Learningsparsesharingarchitecturesformultipletasks.
InProceedingsoftheAAAIconferenceonartificialintelligence,Vol.34.8936‚Äì8943.
[52] XinglongSun,AliHassani,ZhangyangWang,GaoHuang,andHumphreyShi.
2022.DiSparse:DisentangledSparsificationforMultitaskModelCompression.In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
12382‚Äì12392.
[53] XimengSun,RameswarPanda,RogerioFeris,andKateSaenko.2020.Adashare:
Learningwhattoshareforefficientdeepmulti-tasklearning.AdvancesinNeural
InformationProcessingSystems33(2020),8728‚Äì8740.
[54] YeeWhyeTeh,VictorBapst,WojciechMarianCzarnecki,JohnQuan,James
Kirkpatrick,RaiaHadsell,NicolasHeess,andRazvanPascanu.2017. Distral:
Robustmultitaskreinforcementlearning.InAdvancesinNeuralInformation
ProcessingSystems.4496‚Äì4506.
[55] AntoineVanderschuerenandChristopheDeVleeschouwer.2023.AreStraight-
ThroughgradientsandSoft-ThresholdingallyouneedforSparseTraining?.In
ProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputerVision.
3808‚Äì3817.
[56] YulongWang,YifanXu,SiyuanQiao,HanxiaoLiu,ZhijianYang,ChaoXu,Daiyi
Lin,TongWang,XinyuDai,YichenHuang,etal.2020. EagleEye:FastSub-
netEvaluationforEfficientNeuralNetworkPruning.InProceedingsofthe37th
InternationalConferenceonMachineLearning.10016‚Äì10026.
[57] WeiWen,ChunpengWu,YandanWang,YiranChen,andHaiLi.2016.Learning
structuredsparsityindeepneuralnetworks.InAdvancesinneuralinformation
processingsystems.2074‚Äì2082.
[58] MingcanXiang,YingluLiu,TingtingLiao,XiangyuZhu,CanYang,WuLiu,
andHailinShi.2021. The3rdgrandchallengeoflightweight106-pointfacial
landmarklocalizationonmaskedfaces.In2021IEEEInternationalConferenceon
Multimedia&ExpoWorkshops(ICMEW).IEEE,1‚Äì6.
[59] YuYangandTimothyMHospedales.2016.Tracenormregularizeddeepmulti-
tasklearning.InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition.4333‚Äì4341.
[60] Hancheng Ye, Bo Zhang, Tao Chen, Jiayuan Fan, and Bin Wang. 2023.
Performance-awareApproximationofGlobalChannelPruningforMultitask
CNNs.IEEETransactionsonPatternAnalysisandMachineIntelligence(2023).
[61] RuichiYu,AngLi,Chun-FuChen,JiwenLai,VladIMorariu,XintongHan,
MingfeiGao,Ching-YungLin,andLarrySDavis.2018.NISP:PruningNetworks
usingNeuronImportanceScorePropagation.InProceedingsoftheIEEEConference
onComputerVisionandPatternRecognition.9194‚Äì9203.
[62] YujiaYu,ShuaishuaiLiu,AnfengZhang,andChunhuaShen.2019. Playing
LotteryTicketswithVisionandLanguage.InarXivpreprintarXiv:1912.04488.
[63] AmirRZamir,AlexanderSax,WilliamShen,LeonidasJGuibas,JitendraMalik,
andSilvioSavarese.2018.Taskonomy:Disentanglingtasktransferlearning.In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.
3712‚Äì3722.
[64] LijunZhang,XiaoLiu,andHuiGuan.2022.Automtl:Aprogrammingframework
forautomatingefficientmulti-tasklearning. AdvancesinNeuralInformation
ProcessingSystems35(2022),34216‚Äì34228.
[65] LijunZhang,XiaoLiu,andHuiGuan.2022.ATree-StructuredMulti-TaskModel
Recommender.InInternationalConferenceonAutomatedMachineLearning.PMLR,
10‚Äì1.
[66] LijunZhang,QizhengYang,XiaoLiu,andHuiGuan.2023.AnAlternativeHard-
ParameterSharingParadigmforMulti-DomainLearning.IEEEAccess11(2023),
10440‚Äì10452.
[67] ShaokaiZhang,ShanheDu,WentaiWang,YiranChen,andHaiLi.2018. A
systematicDNNweightpruningframeworkusingalternatingdirectionmethod
ofmultipliers.InProceedingsoftheEuropeanConferenceonComputerVision
(ECCV).184‚Äì199.
[68] YuZhangandQiangYang.2018.ASurveyonMulti-TaskLearning.arXivpreprint
arXiv:1707.08114(2018).AdapMTL:AdaptivePruningFrameworkforMultitaskLearningModel MM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia
A Thresholdsupdating Table6:Hyper-parametersfortrainingonNYUv2andTiny-
taskonomydatasets
WedetailtheprocessofupdatingthresholdswithintheAdapMTL
frameworkinthissection.ThethresholdsaredeterminedbyùúÉ init,
suchthatùõº ùë° = sigmoid(ùúÉ init).Consequently,thechallengeofup- Dataset lr lrdecay epoch
datingthethresholdsistransformedintothetaskofupdatingtheùúÉ
ùë°
NYUv2 0.001 0.5/4,000ters 20,000
Tiny-Taskonomy 0.0001 0.3/10,000iters 50,000
foreachspecifictask.ConsideringamultitaskmodelwithTtasks,
wedividetheweightparametersùëä intoùëä ={ùëä ùêµ,ùëä 1,ùëä 2,...,ùëä ùëá},
whereùëä ùêµ representstheweightparametersforthesharedback-
boneandùëä ùë° representstheweightparametersfortheùë°-thtask- WeusetheùúÉ initparameter,setto-20,toregulatethedurationof
specifichead.Wederivethegradientdescentupdateequationat densetrainingphases.AlowerùúÉ initvalueextendstheperioddedi-
theùëõ-thepochforùúÉ ùë° asfollows: catedtodenserepresentation,allowingformorecomprehensive
learningbeforepruningbegins.
ùúÉ ùë°ùëõ+1=ùúÉ ùë°ùëõ‚àíùúÇùëõùúïL(ùëä ùúïùúÉ, ùëõùõº;D) C AdditionalResults
ùë° WeprovideadditionalresultsontheTiny-Taskonomydatasetusing
=ùúÉ
ùë°ùëõ‚àíùúÇùëõùúïL( ùúïùúÉ ùëä,ùõº ùëõ;D)
‚äô
ùúï ùúïùëä ùúÉùëõùë°ùëõ
Resnet34andMobileNetV2architecture,separately.OntheTiny-
ùë° ùë° Taskonomydataset,whichcomprisesatotalof5tasks,AdapMTL
=ùúÉ ùë°ùëõ‚àíùúÇùëõ¬∑(‚àíùë†ùëñùëîùëöùëúùëñùëë(ùúÉ ùë°ùëõ))‚Ä≤¬∑ ùúïL( ùúïùúÉ ùëä,ùõº ùëõ;D) demonstratesamorebalancedperformanceacrosstasks.Asshown
ùë° inTable7,8,ourmethodachievedthehighest‚ñ≥ùëá scoreandthe
=ùúÉ ùë°ùëõ‚àíùúÇùëõ¬∑(‚àíùë†ùëñùëîùëöùëúùëñùëë(ùúÉ ùë°ùëõ))‚Ä≤¬∑ ùúï ùúïL ùëÜ(( ùëäùëä ùëõ,ùõº ,ùõº;D ùëõ)) ‚äô ùúïùëÜ(ùëä ùúïùëäùë°ùëõ ùëõ,ùõº ùë°ùëõ) lowestabsoluteerrorformosttasks.
ùë° ùë° ùë°
=ùëä ùë°ùëõ+ùúÇùëõ¬∑(ùë†ùëñùëîùëöùëúùëñùëë(ùúÉ ùë°ùëõ))‚Ä≤¬∑ ùúï ùúïL ùëÜ(( ùëäùëä ùëõ,ùõº ,ùõº;D ùëõ)) ‚äôBùë°ùëõ, C.1 PruningSensitivityAnalysis
ùë° ùë° Differenttaskheadsmayhavesimilaramountsofmodelweights,
(8)
whereùúÇùëõ isthelearningrateattheùëõ-thepoch.Weusethepar- buttheirsensitivitiestopruningcanvary.Thisobservationsuggests
tialderivativetocalculatethegradients.Although
ùúïùëÜ( ùúïùëä ùëäùë°ùëõ ùë°ùëõ,ùõº ùë°ùëõ)
is
t foh rat ea acm ho tr ae sd ki ,s tc ar kim ini gna it niv toep acru con uin ng ta thpp eir roa uc nh iqs uh eou sl ed nb sie tie vm itip el so .y Te od
non-differentiable, we can approximate the gradients using the verifythisobservation,wefixedaResNet34backboneatasparsity
sub-gradientmethod.Inthiscase,weintroduceBùë°ùëõ
,anindicator levelof95%forbettervisualizationandprunedthreetaskheads
functionthatactslikeabinarymask.ThevalueofBùë°ùëõ
shouldbe independentlytoexaminetheirsensitivitytopruning.
0ifthesparseversionoftheweightùëÜ(ùëä ùë°ùëõ,ùõº ùë°ùëõ)isequalto0.This
AsshowninFigure7,theheadofthesurfacenormalprediction
indicatorfunctionfacilitatestheapproximationofgradientsand taskistheleastsensitive,asitmaintainsgoodaccuracyevenwhen
theupdateofthesparseweightsandsoftthresholdsduringthe extremesparsityisenforced.Therefore,AdapMTLlearnstokeep
backpropagationprocess.Mathematically,theindicatorfunctionis: thistaskheadatahighlevelofsparsityduringpruning,whichis
alignedwiththecomponent-wisesparsityallocationinthetableof
Bùë°ùëõ =(cid:40) 0 1,
,
i of tùëÜ he(ùëä rwùë°ùëõ i, sùõº e.ùë°ùëõ)=0,
(9)
t th ase km isan reu ls ac tr ivip et l. yIn mc oo rn et sr ea nst s, itt ih ve eh te oa pd ro uf nt ih ne g,s se om wan et sic trs ie vg em toen kt ea et pio in
t
asdenseaspossiblethroughoutthetrainingprocess.Thistailored
approachtopruninghelpsAdapMTLachievebetteroverallperfor-
B TrainingDetails manceacrossdifferenttasksbyconsideringthespecificpruning
sensitivityofeachtaskhead.
WeadoptthesametrainingconfigurationsasthoseusedinDiS-
parse [52], which is the latest multitask pruning work, for fair
C.2 Adaptiveweightingfactor
comparisons.WeconductallourexperimentsusingPyTorchand
RTX8000GPUs,andweemploytheAdamoptimizerwithabatch The adaptive weighting mechanism is used to decide the head
sizeof16.FortheNYUV2dataset,werun20Kiterationswithan sparsityallocationamongdifferenttasksbasedontheirvarying
initiallearningrateof1e-3,decayingby0.5every4,000iterations. sensitivitytopruning.Byadaptivelylearningaweightingfactor,we
FortheTiny-Taskonomydataset,wetrainfor100Kiterationswith canassigndifferentimportancetoeachtask,subsequentlypruning
aninitiallearningrateof1e-4,decayingby0.3every12Kiterations. discriminativelyondifferenttaskheads.
Thesizeoftheslidingwindowinourexperimentsissetto400to AsshowninFigure8,weinitiallysettheweightingfactorofeach
smoothlossdeviations.Weutilizedcross-entropylossforSemantic taskequal,suchas1,toensuresufficienttrainingofeachtaskfor
Segmentation,negativecosinesimilaritybetweenthenormalized 5,000epochs.Overtime,thesurfacenormalpredictiontasktends
predictionandgroundtruthforSurfaceNormalPrediction,andL1 tostabilizeandconverge,leadingtosmalllossfluctuations.This
lossfortheremainingtasks.Toavoidbiasanddiversityindifferent impliesthatwecanprunemoreaggressivelyonthatcomponent,
pre-trainedmodels,wetrainedallmodelsfromscratch,ensuring asthetaskheadisrobusttopruning.Consequently,theweighting
afaircomparisonamongvariousmethods.It‚Äôsnoteworthythat, factorofthistaskwillbelargerthantheothers.Thisobservationis
unlike many previous works, our method does not require any alignedwiththeresultsfromTableinthemaintext,wherethesur-
pre-trainingorpre-prunedmodels. facenormalpredictionachieveshighersparsitycomparedtootherMM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia MingcanXiang,JiaxunTang,QizhengYang,HuiGuan&TongpingLiu
Table7:ComparisonwithMTLpruningmethodsontheTiny-TaskonomydatasetusingtheDeeplab-ResNet34backbone.
Model ùëá A1 b: s.S ‚Üìeman ‚ñ≥t ùëáic ‚ÜëSeg.ùëá A2 b: s.N ‚Üëorm ‚ñ≥al ùëáP ‚Üëred.ùëá A3 b: sD .‚ÜìepthEs ‚ñ≥t ùëáim ‚Üëationùëá A4 b: s.K ‚Üìeypo ‚ñ≥in ùëát ‚ÜëDet.ùëá A5 b: s.E ‚Üìdge ‚ñ≥ùëáDe ‚Üët.spa %rsity ‚ñ≥ùëá‚Üë
1 2 2 2 2
DenseModel 0.5053 0 0.8436 0.00 0.0222 0.00 0.1961 0.00 0.2131 0.00 95 0.00
SNIP[26] 0.5659 -11.99 0.7301 -13.45 0.0246 -10.81 0.1972 -0.56 0.2221 -4.22 95 -8.21
LTH[13] 0.5345 -5.78 0.8189 -3.38 0.0234 -5.41 0.2004 -2.19 0.2187 -2.63 95 -3.88
IMP[19] 0.5163 -2.18 0.8371 -0.79 0.0221 0.45 0.1962 -0.05 0.2184 -2.49 95 -1.01
DiSparse[52] 0.5287 -4.63 0.8423 -0.16 0.0217 2.25 0.1987 -1.33 0.2089 1.97 95 -0.38
AdapMTLw/oadaptivethresholds0.5468 -8.21 0.8059 -4.48 0.02296 -3.42 0.1937 1.22 0.2153 -1.03 95 -3.18
AdapMTL 0.5038 0.30 0.8513 0.96 0.0221 0.45 0.1923 1.94 0.2074 2.67 95 1.26
Table8:ComparisonwithMTLpruningmethodsontheTiny-TaskonomydatasetusingtheMobileNetV2backbone.
Model ùëá A1 b: s.S ‚Üìeman ‚ñ≥t ùëáic ‚ÜëSeg.ùëá A2 b: s.N ‚Üëorm ‚ñ≥al ùëáP ‚Üëred.ùëá A3 b: s.D ‚ÜìepthE ‚ñ≥st ùëáim ‚Üëationùëá A4 b: s.K ‚Üìeypo ‚ñ≥in ùëát ‚ÜëDet.ùëá A5 b: s.E ‚Üìdge ‚ñ≥ùëáDe ‚Üët.spa %rsity ‚ñ≥ùëá‚Üë
1 2 2 2 2
DenseModel 1.0783 0.00 0.7429 0.00 0.0318 0.00 0.203 0.00 0.2242 0.00 95 0.00
SNIP[26] 1.0901 -1.09 0.7243 -2.50 0.0321 -0.94 0.2157 -6.26 0.2364 -5.44 95 -3.25
LTH[13] 1.0869 -0.80 0.7407 -0.30 0.0325 -2.20 0.2118 -4.33 0.2328 -3.84 95 -2.29
IMP[19] 1.0795 -0.11 0.7415 -0.19 0.0327 -2.83 0.2012 0.89 0.2351 -4.86 95 -1.42
DiSparse[52] 1.0781 0.02 0.7423 -0.08 0.0322 -1.26 0.208 -2.46 0.2287 -2.01 95 -1.16
AdapMTLw/oadaptivethresholds1.0868 -0.79 0.7329 -1.35 0.0344 -8.18 0.2043 -0.64 0.2233 0.40 95 -2.11
AdapMTL 1.0751 0.30 0.7421 -0.11 0.0305 4.09 0.2021 0.44 0.2225 0.76 95 1.10
1.00 1.4
0.98 1.2
1.0
0.96
Depth Estimation
SN Prediction 0.8
0.94
Semantic Seg.
Dense Model 0.6 Semantic Seg.
0.92 SN Prediction
Depth Estimation
60 70 80 90 93959799 0.4
Task head sparsity (%) 0 2500 5000 7500 10000 12500 15000 17500 20000
Epoch
Figure7:Comparisonoftaskheadsensitivitiestopruningfor Figure8:Theevolutionofadaptiveweightingfactorsduring
differentvisiontasks.Weusea95%sparseResnet34backbone training.Equalweightsareinitiallyassignedtoeachtaskfor
forbetterdepiction.Thedashedlinedensemodelindicates thefirst5000epochstoensuresufficienttraining.
thetaskheadisdense.
taskheads.Incontrast,thelossofsemanticsegmentationfluctuates whilesimultaneouslymakingthetaskheadssparser.Notably,even
significantly,indicatingthatweshouldconsiderpruninglesson thoughthetask-specificheadofsemanticsegmentationisassigned
thatcomponentbyloweringtheweightingfactor,asthetraining alowervalue,itachieveshighersparsitythanwithequalweight.
islesslikelytoconvergeathighersparsitylevels.Itisworthmen- Thisphenomenonarisesbecausethesharedbackboneisalready
tioningthattheweightingfactorislearnedadaptively,eliminating dense,promptingasparsertaskheadthanbefore.Theright-most
theneedformanualefforttofine-tunethehyper-parameters. sub-figurepresentstheoverallperformanceunderthissparsity
Tovalidatetheeffectivenessofouradaptiveweightingmech- allocation.Itisevidentthatourmethod,withtheincorporationof
anism,wealsocarryoutanexperimentwhereweassignequal theadaptiveweightingmechanism,outperformsthevariantofour
weights(ùõΩùë°
=1)toeachtaskandthenvisualizethesparsityallocation methodwithoutit.
acrosseachcomponentunderthisconfiguration.Forcomparison, Through this experiment, we demonstrate that the adaptive
wealsovisualizethesparsityallocationwithadaptiveweighting, weightingmechanismplaysacrucialroleinmaintaininghighden-
wheretheweightingfactorforsemanticsegmentation,surfacenor- sity for the shared backbone and efficiently allocating sparsity
malprediction,anddepthestimationissetto1.35,1.15,and0.5 amongthetask-specificheads.Bytakingintoaccountthesensi-
respectively.ThisconfigurationisfromtheoneshowninFigure8. tivityandimportanceofdifferentcomponentsintheMTLmodel,
AsillustratedinFigure9,theadaptiveweightingfactorresultsin theadaptiveweightingmechanismallowsforbetteroverallperfor-
adensersharedbackbonecomparedtotheequalweightingfactor, manceevenwhenhighsparsityisenforced.
ycarucca
tset
dezilimroN
thgiew
evitpadAAdapMTL:AdaptivePruningFrameworkforMultitaskLearningModel MM‚Äô24,October28-November1,2024,Melbourne,VIC,Australia
balancebetweenthetargetedsparsityandthenecessitytopreserve
adequateperformancelevels.Ourcurrentstrategytomaximally
approximatethedesiredsparsitylevelinvolvesfixingthepruning
maskoncethedesiredsparsityhasbeenreached.Anotherpoten-
tialapproachcouldinvolvearecoverymechanismthatregrows
somecrucialparametersthatwereprunedinearlierepochs.Future
researchcouldexploremoreprecisecontrolmechanismsoverthe
finalsparsitywhileensuringthatthemodel‚Äôsperformanceremains
robust.
Figure9:Comparisonofsparsityallocationbetweenequal
weight (1) and adaptive weight configurations, where we
assignweightingfactorsof1.35,1.15,and0.5toeachtask-
specificheadrespectively.Theoverallsparsityis90%
Table9:VideocaptioningperformanceonYouCookIIusing
VideoBERT.Eachpruningmethodenforcesasparsityof80%.
Model BLEU-3‚Üë BLEU-4‚Üë METEOR‚Üë ROUGE-L‚Üë CIDEr‚Üë ‚ñ≥ùëá‚Üë
DenseModel 6.74 4.03 10.69 27.35 0.49 0.00
SNIP 6.38 3.72 10.42 26.85 0.47 -4.35
Disparse 6.70 4.04 10.65 27.17 0.49 -0.37
AdapMTL 6.77 4.12 10.64 27.24 0.49 0.45
C.3 Multimodalandmultimediascenarios
Whileourworkprimarilyfocusesonunimodaltasksincomputer
visionforafaircomparisonwiththeSOTAmethods,theAdapMTL
frameworkisversatileandcanbeappliedtomultimodalandmulti-
mediascenariosaswell.
Todemonstratethis,wehaveconductednewexperimentson
thevideocaptioningtaskusingVideoBERTmodel[49].VideoBERT
isavariantoftheBERTmodel(transformerarchitecture)thattar-
getstext-to-videogeneration.Wetreatthemulti-headattention
layersandfeed-forwardlayersasindependentcomponentsand
assigncorrespondingadaptivesoftthresholdsùõº tothem.Thesoft
thresholdsarelearnedadaptivelyandeventuallystopatthedesired
sparsitylevel.Table9showstheaccuracycomparisonofthepruned
modelonsparsity80%forallbaselines.AdapMTLoutperformsits
counterpartswhilemaintainingperformancemetricscomparableto
thedenseVideoBERTmodel.Weexpectthattheproposedmethods
canscaletolargermodels,includingLLMs.SinceLLMsarecom-
posedofmultipletransformerblocks,ourcomponent-wisepruning
frameworkisnaturallywell-suitedforthisarchitecture.
D Discussion
OneofthenoticeableaspectsofAdapMTListhatthefinalsparsity
ofourmodelmaynotexactlymatchtherequestedsparsity.Thisdis-
crepancyarisesduetotheintrinsicbehaviorofthesoftthresholds.
Duringpruning,thesesoftthresholdsdeterminewhetheraspecific
parametershouldbesettozero,therebyintroducingsparsityinto
themodel.However,thesoftthresholdsdonotstrictlyenforcethe
exactlevelofsparsitybutratherguidethemodeltoapproachthe
desiredsparsitylevel.Thislevelofflexibilityisadesignchoicemade
topreventanyunduenegativeimpactonmodelperformancedue
tooverlyrigidsparsityconstraints.Itallowsthemodeltostrikea