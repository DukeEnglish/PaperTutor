Sensitivity analysis using the
Metamodel of Optimal Prognosis
Thomas Most, Johannes Will, Dynardo GmbH, Weimar, Germany
8th Optimization and Stochastic Days, Weimar, Germany, 24-25 November, 2011
1 Introduction
Optimization and robustness analysis have become important tools for the vir-
tual development of industrial products. In parametric optimization, the opti-
mization variables are systematically modified by mathematical algorithms in
order to get an improvement of an existing design or to find a global optimum.
The design variables are defined by their lower and upper bounds or by sev-
eralpossiblediscretevalues. Inrealworldindustrialoptimizationproblems,the
numberofdesignvariablescanoftenbeverylarge. Unfortunately,theefficiency
of mathematical optimization algorithms decreases with increasing number of
design variables. For this reason, several methods are limited to a moderate
number of variables, such as gradient based and Adaptive Response Surface
Methods. With the help of sensitivity analysis the designer identifies the vari-
ableswhichcontributemosttoapossibleimprovementoftheoptimizationgoal.
Basedonthisidentification,thenumberofdesignvariablesmaybedramatically
reducedandanefficientoptimizationcanbeperformed. Additionaltotheinfor-
mationregardingimportantvariables,sensitivityanalysismayhelptodecide,if
theoptimizationproblemisformulatedappropriatelyandifthenumericalCAE
solver behaves as expected.
By definition, sensitivity analysis is the study of how the uncertainty in
the output of a model can be apportioned, qualitatively or quantitatively, to
different sources of variation in the input of a model [1]. Since robustness
analysis investigates the influence of the input variation on the variation of the
model outputs, sensitivity analysis can directly be applied as a post-processing
tool to analyze the contribution of each input variable to the scatter of each
model response. In order to quantify this contribution, variance based methods
are very suitable. With these methods, discussed in this paper, the proportion
of the output variance, which is caused by an random input variable, is directly
quantified.
ForoptiSLangrobustnessanalysis,thescatteringinputvariablesaredefined
as random variables. This means that for each scattering input a distribution
typeincludingmeanvalueandvarianceisspecified. Additionally, dependencies
betweentheinputscanbeformulatedintermsoflinearcorrelations. Themodel
1
4202
guA
7
]EM.tats[
1v09530.8042:viXraoutputvariationisestimatedbyrandomsampling. Theestimatedvariationand
the sensitivity measures are strongly influenced by the chosen variation of the
input variables.
Variance based sensitivity analysis is also very suitable as an optimization
pre-processing tool. By representing continuous optimization variables by uni-
formdistributionswithoutvariableinteractions,variancebasedsensitivityanal-
ysis quantifies the contribution of the optimization variables to a possible im-
provement of the model responses. In contrast to local derivative based sen-
sitivity methods, the variance based approach quantifies the contribution with
respect to the defined variable ranges.
Unfortunately, sufficiently accurate variance based methods require huge
numerical effort due to the large number of simulation runs. Therefore, often
meta-models are used to represent the model responses surrogate functions in
terms of the model inputs. However, many meta-model approaches exist and
it is often not clear which one is most suitable for which problem [2]. Another
disadvantageofmeta-modelingisitslimitationtoasmallnumberofinputvari-
ables. Due to the curse of dimensionality the approximation quality decreases
forallmeta-modeltypesdramaticallywithincreasingdimension. Asaresult,an
enormous number of samples is necessary to represent high-dimensional prob-
lems with sufficient accuracy. In order to overcome these problems, Dynardo
developed the Metamodel of Optimal Prognosis [3]. In this approach the op-
timal input variable subspace together with the optimal meta-model approach
are determined with help of an objective and model independent quality mea-
sure, theCoefficientofPrognosis. Inthefollowingpaperthenecessityofsucha
procedureisexplainedbydiscussingotherexistingmethodsforsensitivityanal-
ysis. After presenting the MOP concept in detail, the strength of this approach
is clarified by a comparison with very common meta-model approaches such as
Kriging and neural networks. Finally, an industrial application is given, where
the benefit of the MOP is illustrated.
2 Scanning the space of input variables
Inordertoperformaglobalsensitivityanalysis,thespaceoftheinputvariables,
which is either the design or the random space, has to be scanned by discrete
realizations. Each realization is one set of values belonging to the specified
inputs. For each set of values the CAE model is a black box solver and the
model responses are evaluated. In the case of random variables, only random
sampling can be used to generate discrete realizations. The sampling scheme
needs to represent the specified variable distributions and their dependencies
withasufficientaccuracy. AverycommonapproachisMonteCarloSimulation
(MCS). However, if only a small number of samples is used, often clusters and
holescanberemarkedintheMCSsamplingset. Morecriticalistheappearance
of undesired correlations between the input variables. These correlations may
have a significant influence on the estimated sensitivity measures. In order to
overcome such problems, optiSLang provides optimized Latin Hypercube Sam-
2pling (LHS), where the input distributions and the specified input correlations
are represented very accurately even for a small number of samples. For the
minimization of the undesired correlation the method according to [4] is used.
Figure 1: Dimension reduction for a nonlinear function with five inputs based
on Latin Hypercube Sampling (left) with 100 samples and full factorial design
(right) with 35 =243 samples
As a design exploration for optimization problems deterministic Designs of
Experiments(DoE)areoftenapplied[5]. Thesedesignschemesaremainlybased
on a regular arrangement of the samples, as in the full factorial design. Gener-
ally the number of samples increases exponentially with increasing dimension.
Fractionalfactorialdesignsuseonlyapartofthefullfactorialsamples,however
thenumberoflevelsineachdirectionislimitedtothree. Fromourpointofview,
deterministicdesignschemeshavetwomaindisadvantagescomparedtorandom
sampling: They are limited to a small number of variables due to the rapidly
increasing number of required samples when increasing the model dimension.
Further a reduction of the number of inputs does not improve the information
gained from the samples, since only two or three levels are used in each dimen-
sion. This is illustrated in Figure 1. In this figure, a nonlinear function having
one major and four almost unimportant variables is evaluated. Using the LHS,
the nonlinearity can be represented very well in the reduced space. In the case
of the full factorial design, which contains three levels in each directions, again
only three positions are left in the reduced space and the dimension reduction
does not allow a better representation of the model response.
3 Variance based sensitivity analysis
3.1 First order and total effect sensitivity indices
Assuming a model with a scalar output Y as a function of a given set of m
random input parameters X
i
Y =f(X ,X ,...,X ), (1)
1 2 m
3the first order sensitivity measure was introduced as [6]
V (E (Y|X ))
S = Xi X∼i i , (2)
i V(Y)
whereV(Y)istheunconditionalvarianceofthemodeloutputandV (E (Y|X ))
Xi X∼i i
is named the variance of conditional expectation with X denoting the matrix
∼i
of all factors but X . V (E (Y|X )) measures the first order effect of X on
i Xi X∼i i i
the model output.
Since first order sensitivity indices measure only the decoupled influence of
each variable an extension for higher order coupling terms is necessary. There-
fore total effect sensitivity indices have been introduced [7]
V (E (Y|X ))
S =1− X∼i Xi ∼i , (3)
Ti V(Y)
where V (E (Y|X )) measures the first order effect of X on the model
X∼i Xi ∼i ∼i
output which does not contain any effect corresponding to X .
i
In order to estimate the first order and total sensitivity indices, a matrix
combination approach is very common [8]. This approach calculates the condi-
tional variance for each variable with a new sampling set. In order to obtain
a certain accuracy, this procedure requires often more than 1000 samples for
each estimated conditional variance. Thus, for models with a large number of
variables and time consuming solver calls, this approach can not be applied
efficiently.
3.2 Coefficient of Correlation
Thecoefficientofcorrelationisthestandardizedcovariancebetweentworandom
variables X and Y
COV(X,Y)
ρ(X,Y)= , (4)
σ σ
X Y
where COV(X,Y) is the covariance and σ is the standard deviation. This
quantity, known as the linear correlation coefficient, measures the strength and
thedirectionofalinearrelationshipbetweentwovariables. Itcanbeestimated
from a given sampling set as follows
1
(cid:80)N
(x −µˆ )(y −µˆ )
ρ(X,Y)≈ i=1 i X i Y , (5)
N −1 σˆ σˆ
X Y
whereN isthenumberofsamples,x andy arethesamplevalues,andµˆ and
i i X
σˆ aretheestimatesofthemeanvalueandthestandarddeviation,respectively.
X
The estimated correlation coefficient becomes more inaccurate, as its value is
closer to zero, which may cause a wrong deselection of apparently unimportant
variables.
Ifbothvariableshaveastrongpositivecorrelation,thecorrelationcoefficient
is close to one. For a strong negative correlation ρ is close to minus one. The
4squared correlation coefficient can be interpreted as the first order sensitivity
index by assuming a linear dependence. The drawback of the linear correlation
coefficient is its assumption of linear dependence. Based on the estimated coef-
ficients only, it is not possible to decide on the validity of this assumption. In
manyindustrialapplicationsalineardependenceisnotthecase. Correlationco-
efficients, which assume a higher order dependence or use rank transformations
[9] solve this problem only partially. Additionally, often interactions between
theinputvariablesareimportant. Theseinteractionscannotbequantifiedwith
the linear and higher order correlation coefficients.
We can summarize that although the correlation coefficient can be simply
estimatedfromasinglesamplingset,itcanonlyquantifyfirstordereffectswith
an assumed dependence without any quality control of this assumption.
4 Polynomial based sensitivity analysis
4.1 Polynomial regression
A commonly used approximation method is polynomial regression, where the
model response is generally approximated by a polynomial basis function
pT(x)=(cid:2) 1 x x x ... x2 x2 x2 ... x x x x ... x x ...(cid:3) (6)
1 2 3 1 2 3 1 2 1 3 2 3
of linear or quadratic order with or without coupling terms. The model output
y for a given set x of the input parameters X can be formulated as the sum
i i
of the approximated value yˆ and an error term ϵ
i i
y(x )=yˆ(x )+ϵ =pT(x )β+ϵ , (7)
i i i i i i
where β is a vector containing the unknown regression coefficients. These co-
efficients are generally estimated from a given set of sampled support points
by assuming independent errors with equal variance at each point. By using a
matrix notation the resulting least squares solution reads
βˆ=(PTP)−1PTy, (8)
where P is a matrix containing the basis polynomials of the support point
samples and y is the vector of support point values.
4.2 Coefficient of Determination
The Coefficient of Determination (CoD) can be used to assess the approxima-
tion quality of a polynomial regression. This measure is defined as the relative
amount of variation explained by the approximation [10]
SS SS
R2 = R =1− E, 0≤R2 ≤1, (9)
SS SS
T T
5whereSS isequivalenttothetotalvariation,SS representsthevariationdue
T R
to the regression, and SS quantifies the unexplained variation,
E
N N N
(cid:88) (cid:88) (cid:88)
SS = (y −µ )2, SS = (yˆ −µ )2, SS = (y −yˆ)2. (10)
T i Y R i Yˆ E i i
i=1 i=1 i=1
IftheCoDisclosetoone,thepolynomialapproximationrepresentsthesupport
pointvalueswithsmallerrors. However,thepolynomialmodelwouldfitexactly
through the support points, if their number is equivalent to the number of
coefficients p. In this case, the CoD would be equal to one, independent of the
true approximation quality. In order to penalize this over-fitting, the adjusted
Coefficient of Determination was introduced [10]
N −1
R2 =1− (1−R2). (11)
adj N −p
However, the over-estimation of the approximation quality can not be avoided
completely.
1.00
CoD lin
CoD lin
0.90 adj
CoD quad
CoD quad
adj
0.80
0.70
0.60
0.50
25 50 100 200 500
Number of samples
Figure 2: Subspace plot of the investigated nonlinear function and convergence
of the CoD measures with increasing number of support points
In order to demonstrate this statement, an investigation of a nonlinear an-
alytical function is performed. The function reads in terms of five independent
and uniformly distributed input variables as follows
Y =0.5X +X +0.5X X +5.0sin(X )+0.2X +0.1X , −π ≤X ≤π, (12)
1 2 1 2 3 4 5 i
where the contributions of the five inputs to the total variance are X : 18.0%,
1
X : 30.6%, X : 64.3%, X : 0.7%, X : 0.2%. This means, that the three
2 3 4 5
variables, X , X and X , are the most important.
1 2 3
In Figure 2, the convergence of the standard CoD of linear and quadratic
responsesurfacesisshown,whereastrongover-estimationoftheapproximation
quality can be noticed, when the number of samples is relatively small. Even
the adjusted CoD shows a similar behavior. This fact limits the CoD for cases
6
noitairav
denialpxEwhere a large number of support points compared to the number of polynomial
coefficientsisavailable. However, inindustrialapplications, weareinourinter-
est, this is often not the case. Another disadvantage of the CoD measure is its
limitation to polynomial regression. For other local approximation models, like
interpolating Kriging, this measure may be equal or close to one, however the
approximation quality is still poor.
4.3 Coefficient of Importance
The Coefficient of Importance (CoI) was developed by Dynardo to quantify the
input variable importance by using the CoD measure. Based on a polynomial
model, including all investigated variables, the CoI of a single variable X with
i
respect to the response Y is defined as follows
CoI(X ,Y)=CoI =R2 −R2 , (13)
i Y,Xi Y,X Y,X∼i
where R2 is the CoD of the full model including all terms of the variables
Y,X
in X and R2 is the CoD of the reduced model, where all linear, quadratic
Y,X∼i
and interactions terms belonging to X are removed from the polynomial basis.
i
For both cases the same set of sampling points is used. If a variable has low
importance, its CoI is close to zero, since the full and the reduced polynomial
regression model have a similar quality. The CoI is equivalent to the explained
variation with respect to a single input variable, since the CoD quantifies the
explained variation of the polynomial approximation. Thus it is an estimate
of the total effect sensitivity measure given in Equation 3. If the polynomial
model contains important interaction terms, the sum of the CoI values should
be larger than the CoD of the full model.
SinceitisbasedontheCoD,theCoIisalsolimitedtopolynomialmodels. If
thetotalexplainedvariationisover-estimatedbytheCoD,theCoImayalsogive
a wrong estimate of the variance contribution of the single variables. However,
in contrast to the Coefficient of Correlation, the CoI can handle linear and
quadratic dependencies including input variable interactions. Furthermore, an
assessment of the suitability of the polynomial basis is possible. Nevertheless,
an estimate of the CoI values using a full quadratic polynomial is often not
possible because of the required large number of samples for high dimensional
problems.
5 Metamodel of Optimal Prognosis
5.1 Moving Least Squares approximation
In the Moving Least Squares (MLS) approximation [11] a local character of the
regression is obtained by introducing position dependent radial weighting func-
tions. MLSapproximationcanbeunderstoodasanextensionofthepolynomial
regression. Similarly the basis function can contain every type of function, but
7generallyonlylinearandquadratictermsareused. Theapproximationfunction
is defined as
yˆ(x)=pT(x)a(x), (14)
with changing (“moving”) coefficients a(x) in contrast to the constant global
coefficientsofthepolynomialregression. Thefinalapproximationfunctionreads
yˆ(x)=pT(x)(PTW(x)P)−1PTW(x)y, (15)
where the diagonal matrix W(x) contains the weighting function values cor-
responding to each support point. Distance depending weighting functions
w =w(∥x−x ∥)havebeenintroduced. MostlythewellknownGaussianweight-
i
ing function is used
(cid:18)
∥x−x
∥2(cid:19)
w (∥x−x ∥)=exp − i , (16)
exp i α2D2
where the influence radius D directly influences the approximation error. A
suitable choice of this quantity enables an efficient smoothing of noisy data. In
Figure 3 the local weighting principle and the smoothing effect is shown.
10.0 Supports
D=0.3
D=2.0
D=5.0
5.0
0.0
-5.0
1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0
Figure 3: Local weighting of support point values (left) and influence of the
influenceradiusD onthesmoothingoftheMLSapproximationfunction(right)
The MLS approach has the advantage that no training is necessary before
an approximation point can be evaluated. At each point only the weighting
factors and the local polynomial coefficients have to be calculated. This makes
this method very fast compared to other approximation techniques.
5.2 Coefficient of Prognosis
In [3] a model independent measure to assess the model quality was proposed.
This measure is the Coefficient of Prognosis (CoP), which is defined as follows
SSPrediction
CoP =1− E , (17)
SS
T
8where SSPrediction is the sum of squared prediction errors. These errors are
E
estimated based on cross validation. In the cross validation procedure, the
set of support points is mapped to q subsets. Then the approximation model
is built by removing subset i from the support points and approximating the
subset model output y˜ using the remaining point set. This means that the
j
model quality is estimated only at these points, which are not used to build the
approximation model. Since the prediction error is used instead of the fit, this
approach applies to regression and even interpolation models.
The evaluation of the cross validation subsets, which are usually between 5
and10sets,causesadditionalnumericaleffortinordertocalculatetheCoP.Nev-
ertheless, for polynomial regression and Moving Least Squares, this additional
effort is still quite small since no complex training algorithm is required. For
other meta-modeling approaches as neural networks, Kriging and even Support
Vector Regression, the time consuming training algorithm has to be performed
for every subset combination.
In Figure 4, the convergence of the CoP values of an MLS approximation
of the nonlinear coupled function given in Equation 12 is shown in comparison
to the polynomial CoD. The figure indicates that the CoP values are not over-
estimating the approximation quality as the CoD does for a small number of
samples. TheinfluenceradiusoftheMLSapproximationisfoundbymaximizing
1.00
0.90
0.80
0.70
0.60 CoD quad
CoD quad
adj
0.50 CoP MLS 5Var
CoP MLS 3Var
0.40
25 50 100 200 500
Number of samples
Figure 4: Convergence of the CoP measure by using MLS approximation com-
pared to the polynomial CoD measure
the CoP measure. As shown in Figure 4, the convergence of the approximation
quality is much better if only the three important variables are used in the
approximation model.
5.3 Metamodel of Optimal Prognosis
As shown in the previous section, the prediction quality of an approximation
model may be improved if unimportant variables are removed from the model.
ThisideaisadoptedintheMetamodelofOptimalPrognosis(MOP)proposedin
[3] which is based on the search for the optimal input variable set and the most
9
noitairav
denialpxEappropriate approximation model (polynomial or MLS with linear or quadratic
basis). Due to the model independence and objectivity of the CoP measure,
it is well suited to compare the different models in the different subspaces. In
1.00
0.80
0.60
0.40
MLS
0.20 Polynomial
Optimal
0.00
1 2 3 4 5
Number of variables
Figure5: CoPvaluesofdifferentinputvariablecombinationsandapproximation
methods obtained with the analytical nonlinear function
Figure5,theCoPvaluesofallpossiblesubspacesandallpossibleapproximation
models are shown for the analytical nonlinear function. The figure indicates
that there exists an optimal compromise between the available information,
the support points and the model complexity, the number of input variables.
TheMLSapproximationbyusingonlythethreemajorimportantvariableshas
a significantly higher CoP value than other combinations. However for more
complex applications with many input variables ,it is necessary to test a huge
numberofapproximationmodels. Inordertodecreasethiseffort,in[3]advanced
filter technologies are proposed, which reduce the number of necessary model
tests. Nevertheless, a large number of inputs requires a very fast and reliable
constructionoftheapproximationmodel. ForthisreasonpolynomialsandMLS
are preferred due to their fast evaluation.
As a result of the MOP, we obtain an approximation model, which includes
the important variables. Based on this meta-model, the total effect sensitivity
indices, proposed in section 3.1, are used to quantify the variable importance.
Thevariancecontributionofasingleinputvariableisquantifiedbytheproduct
of the CoP and the total effect sensitivity index estimated from the approxima-
tion model
CoP(X )=CoP ·SMOP(X ). (18)
i T i
Since interactions between the input variables can be represented by the MOP
approach, they are considered automatically in the sensitivity indices. If the
sum of the single indices is significantly larger as the total CoP value, such
interaction terms have significant importance.
Additionally to the quantification of the variable importance, the MOP can
be used to visualize the dependencies in 2D and 3D subspaces. This helps the
designertounderstandandtoverifythesolvermodel. InFigure6twosubspace
plots are shown for the MOP of the analytical test function. In the X -X and
2 3
10
sisongorP
fo
tneiciffeoCX -X subspace plots the sinusoidal function behavior and the coupling term
1 2
canbeobserved. Additionalparametricstudies,suchasglobaloptimizationcan
Figure 6: X -X and X -X subspace plots of the MOP of the nonlinear ana-
2 3 1 2
lytical function
alsobedirectlyperformedontheMOP.Nevertheless,asinglesolverrunshould
be used to verify the final result of such a parametric study or optimization.
Ifthesolveroutputcontainsunexplainableeffectsduetonumericalaccuracy
problems, the MOP approximation will smooth these noise effects as shown in
Figure 7. If this is the case, the CoP value of the MOP can be used to estimate
thenoisevariationastheshortcomingoftheCoPto100%explainability. How-
ever, the unexplained variation may not be caused only by solver noise but also
by a poor approximation quality. This problem should be analyzed by increas-
ingthenumberofsamplesinthecaseoflowexplainability. IftheCoPdoesnot
increase then it is an indicator for unexplainable solver behavior.
1.00
0.90
0.80
0.70
0.60
0.50 Non-robust model
Robust model
0.40
Input parameters 25 50 100 200 500
Number of samples
Figure7: Representationofanoisymodeloutputbyasmoothingapproximation
(left)andconvergencebehavioroftheexplainedvariationinthecaseofarobust
and non-robust model (right)
11
esnopseR noitairav
denialpxE6 Comparison with other approximation and se-
lection methods
In this section we compare the MOP approach with other approximation and
variable selection methods. Before using advanced meta-models, we investigate
the test case with polynomials and Moving Least Squares. The analytical non-
linear function introduced in section 4.2 is investigated by different numbers of
input variables: only the three main important variables, all five variables and
additional variables without any contribution. In Figure 8, the explained varia-
tion of the polynomial and MLS approximation obtained with 100 support and
100 test points is shown with respect to the total number of variables. Due to
theso-calledcurseofdimensionalitytheapproximationqualitydecreasesrapidly
with increasing dimension. If the MOP is applied, only the important variables
are filtered out and the approximation is build in the optimal subspace. This
leads to a high approximation quality even for larger input dimensions.
1.00
Polynomial
0.90 MLS
MOP
0.80
0.70
0.60
0.50
3 5 10 15 20
Number of variables
Figure 8: Approximation quality for polynomial and MLS approximation com-
pared to the MOP approach for the analytical function with increasing number
of input variables
In the next step, we investigate the Kriging approximation, that is also
known as Gaussian process model, which assumes a linear regression model
similar to polynomial regression
y(x)=pT(x)β+ϵ(x). (19)
Instead of independent errors, correlations between the error values are intro-
duced by a spatial correlation function similar to these in random fields
C =σ2Ψ, Ψ =exp(−θ∥x −x ∥2), (20)
ϵϵ ij i j
where C is the covariance matrix of the support points. The exponential
ϵϵ
correlation function Ψ uses often the quadratic norm of the spatial distance.
AspecialcaseiscalledordinaryKrigingwhereonlyconstantregressionterms
are used
y(x)=µ+ϵ(x). (21)
12
atad
tset
ni
noitairav
denialpxEFor this case the approximation function reads
yˆ(x)=µˆ+ψ(x)TΨ−1(y−1µˆ)=µˆ+ψ(x)Tw (22)
The optimal correlation parameters are obtained generally by the maximum
likelihood approach or by cross validation. In our study we use the additional
test data set, however, the cross validation approach is more robust. The de-
terminationoftheKrigingweightsw requirestheinversionoftheGrammatrix
Ψ, which is very time consuming for a large number of support points. For this
reason, the cross validation procedure requires a significantly higher numerical
effort as for Kriging when compared to the MLS approach. In Figure 9, the
approximation quality of the ordinary Kriging approach is shown for the test
function. The significant decrease of the explained variation is similar to that
of the MLS approach.
1.00
Kriging
0.90 SVR
ANN
0.80 MOP
0.70
0.60
0.50
3 5 10 15 20
Number of variables
Figure9: ApproximationqualityforKriging,SupportVectorRegression(SVR)
and Artificial Neural Networks (ANN) compared to the MOP approach for the
analytical function
Furthermore, Support Vector Regression (SVR) and Artificial Neural Net-
works (ANN) are investigated. A detailed presentation of these methods can
be found in [2]. The obtained results are shown additionally in Figure 9, which
show a similar behavior as in Kriging and MLS. All the presented results show
that the utilization of complex meta-model approaches will not overcome the
curse of dimensionality. However, the MOP enables the determination of the
optimalvariablesubspacebyusingfastandreliableapproximationmethods. In
themostcasesthisvariablereductionleadstoasignificantlybetterapproxima-
tion quality.
FinallytheMOPapproachiscomparedtothepolynomialstepwiseselection
method. In this approach polynomial coefficients are selected by different im-
portancecriteriainordertodetecttheimportantvariables. Forthecomparison,
the state of the art implementation in [12] is used. In this implementation im-
portantpolynomialcoefficientsareselectedbyF-teststatisticsbasedonagiven
polynomial degree. The results given in Figure 10 indicate that the selection
procedure works appropriately only for a linear polynomial basis. By using a
13
atad
tset
ni
noitairav
denialpxEfull quadratic basis, the number of selected coefficients increases dramatically
and the approximation quality decreases. This example clarifies the power of
the prediction based variable selection applied inside the MOP approach.
1.00 20
0.90 MOP
stepwise linear 15
0.80 stepwise quadratic
10
0.70
5
0.60
stepwise linear
stepwise quadratic
0.50 0
3 5 10 15 20 3 5 10 15 20
Number of variables Number of variables
Figure10: ApproximationqualityandvariableselectionofMATLAB’sstepwise
regression approach compared to the MOP results
7 Application in Noise Vibration Harshness anal-
ysis
In this example we apply the MOP approach in the framework of a robustness
analysis in automotive industry. Here we investigate an example presented in
[13]wheretheNoiseVibrationHarshness(NVH)isanalyzed. Inputparameters
in this analysis are 46 sheet thicknesses of a car body which are varied within
a +/- 20%. The sound pressure levels at certain frequencies are the outputs
which are investigated. In Figure 11, the car body including the sheets with
varying thicknesses are shown.
In order to find the important parameters with a small number of samples,
which are obtained from very time consuming finite element simulations, the
application of MOP is very attractive. Based on the MOP, the total effect sen-
sitivity indices are calculated. In table 1, the resulting indices of one sound
pressure value including the approximation quality are given for different num-
bers of samples obtained from Latin Hypercube Sampling. The table indicates
that even for a very small number of samples compared to the number of input
variables the most important variables can be detected. When increasing the
numberofsamples, additionalminorimportantinputsaredetectedandconsid-
eredintheoptimalmeta-model. Theinfluenceofcouplingtermsincreasesfrom
approximately 5% to 11% due to the better approximation quality. In figure 12
the approximation functions are shown in the subspace of the two most impor-
tant variables X and X . The figure indicates that with only 100 samples
20 23
the general functional behavior can be represented.
Due to the efficiency of the proposed sensitivity analysis even for nonlinear
coherencesbetweeninputsandoutputs,theMOPapproachisappliedinseveral
14
atad
tset
ni
noitairav
denialpxE
stneiciffeoc
fo
rebmuNFigure 11: Car body with varied 46 sheets thicknesses and investigated sound
pressure level depending on the frequency
Figure 12: Approximation function in the subspace of the two most important
inputs using 100 samples as supports (left) and 800 samples (right)
15No. samples 100 200 400 600 800
CoP 90.9% 91.7% 95.7% 96.3% 96.9%
CoP(X5) - - 2.4% 2.3% 2.7%
CoP(X6) 6.0% 5.3% 8.2% 8.3% 8.7%
CoP(X20) 41.3% 42.7% 42.3% 43.4% 42.2%
CoP(X23) 49.1% 48.0% 50.7% 51.0% 53.8%
Table 1: Convergence of approximation quality and total sensitivity indices for
the most important sheet thickness
industrial projects in cooperation with the German automotive industry.
8 Summary
In this paper the Metamodel of Optimal Prognosis was presented. It was
shown, that in contrast to one-dimensional correlation coefficients and multi-
dimensionalpolynomialbasedsensitivityanalysis,theMOPenablesanefficient
and reliable estimation of the input variable importance. With the help of the
objective quality measure CoP, the MOP approach detects the subset of most
important variables necessary for the optimal approximation. The application
of variable selection proves to be successful even for higher dimensional prob-
lems where the full space approximations using advanced meta-models show
weak performance.
The determined optimal approximation function of one or more model re-
sponses can be used further to get a first estimate of a global optimum. In the
frameworkofrobustnessanalysis,theCoPvalueoftheMOPisausefulestimate
ofpossiblesolvernoiseandanindicatorfortheusabilityoftheinvestigatedCAE
model.
16References
[1] A. Saltelli, K. Chan, and M. Scott, editors. Sensitivity Analysis. John
Wiley and Sons publishers, Probability and Statistics series, 2000.
[2] D. Roos, T. Most, J. F. Unger, and J. Will. Advanced surrogate models
within the robustness evaluation. In Proc. Weimarer Optimierungs- und
Stochastiktage 4.0, Weimar, Germany, November 29-30, 2007. 2007.
[3] T. Most and J. Will. Metamodel of Optimal Prognosis - an automatic
approach for variable reduction and optimal metamodel selection. In
Proc.WeimarerOptimierungs-undStochastiktage5.0, Weimar, Germany,
November 20-21, 2008. 2008.
[4] R. L. Iman and W. J. Conover. A distribution-free approach to inducing
rank correlation among input variables. Communications in Statistics -
Simulation and Computation, 11:311–334, 1982.
[5] R. Myers and D. C. Montgomery. Response Surface Methodology. John
Wiley & Sons, Inc., 2 edition, 2002.
[6] I. M. Sobol’. Sensitivity estimates for nonlinear mathematical models.
Mathematical Modelling and Computational Experiment, 1:407–414, 1993.
[7] T.HommaandA.Saltelli. Importancemeasuresinglobalsensitivityanaly-
sisofnonlinearmodels. ReliabilityEngineeringandSystemSafety,52:1–17,
1996.
[8] A. Saltelli et al. Global Sensitivity Analysis. The Primer. John Wiley &
Sons, Ltd, Chichester, England, 2008.
[9] optiSLang. The optimizing Structural Language, An Users’ Manual. Dy-
nardo GmbH, Weimar, Germany, version 3.2.1 edition, 2011.
[10] D. C. Montgomery and G. C. Runger. Applied Statistics and Probability
for Engineers. John Wiley & Sons, third edition, 2003.
[11] P.LancasterandK.Salkauskas. Surfacegeneratedbymovingleastsquares
methods. Mathematics of Computation, 37:141–158, 1981.
[12] MATLAB. User’s guide. MathWorks Inc., Natick, Massachusetts, version
7.11.0 (r2010b) edition, 2010.
[13] J. Will, J.-S. M¨oller, and E. Bauer. Robustness evaluations of the NVH
comfort using full vehicle models by means of stochastic analysis. In Proc.
VDI Congress Berechnung und Simulation im Fahrzeugbau, VDI-Berichte
1846, pages 505–525. 2004.
17