Fast Sprite Decomposition from Animated
Graphics
Tomoyuki Suzuki , Kotaro Kikuchi , and Kota Yamaguchi
CyberAgent
{suzuki_tomoyuki,kikuchi_kotaro_xa,yamaguchi_kota}@cyberagent.co.jp
Abstract. This paper presents an approach to decomposing animated
graphics into sprites, a set of basic elements or layers. Our approach
buildsontheoptimizationofspriteparameterstofittherastervideo.For
efficiency,weassumestatictexturesforspritestoreducethesearchspace
while preventing artifacts using a texture prior model. To further speed
uptheoptimization,weintroducetheinitializationofthespriteparam-
eters utilizing a pre-trained video object segmentation model and user
inputofsingleframeannotations.Forourstudy,weconstructtheCrello
Animation dataset from an online design service and define quantita-
tivemetricstomeasurethequalityoftheextractedsprites.Experiments
showthatourmethodsignificantlyoutperformsbaselinesforsimilarde-
composition tasks in terms of the quality/efficiency tradeoff. 1
Keywords: sprite decomposition · animated graphics · optimization
1 Introduction
Designers build animated graphics such as social media posts or advertisements
using sprites, a basic set of animated objects or layers in video editing. Sprite
allows intuitive manipulation of objects in videos thanks to its compact and
interpretable data representation. However, once a video is composited into a
raster video through a rendering engine, it is almost impossible to manipulate
objects in a video instantly. Still, it is common to observe a situation where one
wishes to edit certain parts of the raster animated graphic video, for example,
when a user attempts to create an original from a reference raster animated
graphic video. This is where the decomposition of a raster video into the sprites
comes into play.
In this work, we tackle the decomposition of raster animated graphics into
sprites for video editing applications. While there has been literature on video
decomposition[10,33,34],wearguethatthedecompositionofanimatedgraphics
poses unique challenges. Compared to the decomposition of natural scenes, ani-
matedgraphicsincludemoreobjectsinasinglevideoandinvolvemanytypesof
1 Project page: https://cyberagentailab.github.io/sprite-decompose
4202
guA
7
]VC.sc[
1v32930.8042:viXra2 T. Suzuki et al.
Input Output: Sprites Applications
Video Fast Edit animation Remove sprite
decom-
position
Foreground boxes
Textures Animation
params. Edit textures
Fig.1: Sprite decomposition from animated graphics. Given a raster video and auxil-
iary bounding box annotations, our method decomposes sprites that consist of static
texturesandanimationparameters.Thedecomposedparametersareeasilyapplicable
to various video-editing applications.
elements,suchasbackgroundimages,illustrations,typographicelements,orem-
bellishments. Each element has different dynamics, which are typically defined
as animation effects (e.g., zoom-in or fade-out) in a video authoring tool. While
the dynamics or texture of the objects are usually simpler than natural scenes,
any artifact resulting from the failure of decomposition is perceptually unac-
ceptable for video editing applications. For ease of manipulation, the animation
parameters should be a compact parametric representation (in our case, affine
transformation) . Also, considering that the users are designers, it is important
to make a decomposition approach fast enough to implement in an interactive
editing tool; e.g., it is not acceptable to spend hours processing a raster video
in a workflow.
Ourspritedecompositionapproachisdesignedtoaddresstheaforementioned
challenges. Fig. 1 illustrates the overview of our decomposition task. We for-
mulate the sprite decomposition as an optimization problem to fit the sprite
parameters to the given raster video. Considering the typical scenario in ani-
mated graphics, we introduce a static sprite assumption that all the textures
are static and only animation parameters change over time, significantly reduc-
ing the parameter space in the optimization process. Under this assumption,
we incorporate an image-prior model to prevent undesirable pixel artifacts. For
an efficient optimization process, we employ a gradient-based optimizer with
an effective initialization procedure that builds on a video object segmentation
model from minimal user annotation of object bounding boxes in a single video
frame. Combining those simple yet effective approaches, we achieve much faster
convergence in the decomposition, which we show in the experiments.
To evaluate the quality and speed of animated graphics decomposition, we
build Crello Animation dataset that consists of high-quality templates of ani-
mated graphic designs, which we collect from an online design service. Also, we
define benchmark metrics to evaluate the quality of decomposition tailored forFast Sprite Decomposition from Animated Graphics 3
animated graphics. In the experiment, we show our approach considerably out-
performssimilardecompositionbaselinesregardingthetrade-offbetweenquality
andefficiency.Finally,wepresentapplicationexamplesofvideoeditingusingde-
composed sprites by our approach. We summarize our contributions as follows:
1. Weproposeasimpleandefficientoptimization-basedmethodfordecompos-
ing sprites from animated graphics.
2. We construct the Crello Animation dataset and benchmark metrics to eval-
uate the quality of sprite decomposition.
3. We empirically show that our method constitutes a strong baseline in terms
of the quality/efficiency trade-off and achieves significantly faster conver-
gence to reach the same decomposition quality.
2 Related work
2.1 Image vectorization and decomposition
Vectorizing or decomposing images is the inverse problem of rasterizing or ren-
dering, i.e., the task of converting an input image into a parametric representa-
tionthatcanberasterizedorrenderedasvisuallyidenticaltotheinput.Motiva-
tions behind this task include editing raster content and obtaining scalable vec-
tor representations. Several studies have been made in the computer vision and
computer graphics communities for representations such as image layers [2,22],
vector graphics [14,16], and text attributes [23]. A common approach is to use
gradient-based optimization to search for parameters that minimize reconstruc-
tion error. While we share the motivation and general approach with the above
studies, our work is differentiated by a new data representation, which we refer
to as animated graphics, and a method designed specifically for this purpose.
2.2 Video decomposition
There have been many attempts to decompose a raster video into a sequence
of layers. Omnimatte [15], Layered Neural Rendering [6], DyStaB [32], Double-
DIP[5]andamodalvideoobjectsegmentation[12,29]aimtodecomposeavideo
into layers (pixel arrays or masks), but they do not aim to parameterize them.
Wang and Adelson [28] proposed a method to represent a layer as a pair of
appearance and parameterized animation in addition to decomposing a video
into layers, and following attempts [1,3,8,18,20] have been made. While the
basic formulation has not changed since these early works, recent studies adopt
machine learning approaches to the decomposition pipeline for better quality.
Layered Neural Atlases (LNA) [10] represent primary objects in a video as 2D
atlases and their dynamics as moving reference points in the 2D atlases. The
coordinate-based multilayer perceptrons (MLPs) [4,24,26] represent the 2D at-
lases and the mapping of reference points. Lee et al. [13] extends LNA to edit
the appearance of the atlas based on text prompts and compensate for changes
indeformationthroughestimatedsemanticcorrespondences.DeformableSprites4 T. Suzuki et al.
Layered Neural Atlases Deformable Sprites Ours
H × W × 3 (RGB) H × W × 4 (RGBA)
(u, v)
Texture MLP
(r, g, b)
(u, v)
Multiply
Deform by opacity
MLP by B-spline Alpha mask
(x, y, t) T × H × W Deform
by affine
Animation
(x, y, t)
MLP Rendered sprite
T × H × W × 4 (RGBA)
alpha
Rendered sprite Rendered sprite
T × H × W × 4 (RGBA) T × H × W × 4 (RGBA)
Fig.2:ComparisonofspriterepresentationsofLayeredNeuralAtlases[10],Deformable
Sprites [33], and ours. Our approach limits parameter space to static texture and
affine transformation, which enables faster convergence while keeping the necessary
representation for animated graphics.
(DS) [33] is similar to LNA but differs in that the 2D atlas (or texture) is sim-
plified to a pixel grid, and deformations are parameterized as B-splines. Sprite-
from-Sprite [34] decomposes a cartoon into sprites, where each sprite is repre-
sented by a homography warping and spatiotemporal pixel grid including all
other information. In this representation, it is difficult to propagate the appear-
ance manipulation temporally, unlike LNA, DS and ours.
We compare our method with LNA [10] and DS [33], which have static tex-
tures in their sprite representation like ours. The detailed differences in sprite
representationaresummarizedinFig.2.Comparedtothesemethods,ourmethod
has a minimal yet sufficient parametrization to cover the typical cases of ani-
matedgraphicsandisparticularlybeneficialwheneditingtextures.Asdiscussed
in [13], LNA requires correction of the mapping between the original and edited
textures. The B-spline transform used in DS is flexible for general video decom-
positionbutcanresultinunwanteddeformationforouranimatedgraphics.Both
methods use only pixel-level alpha masks, which may not capture the temporal
changesinsprite-levelopacityoftenseeninfade-inandfade-outanimations.Our
simplified representation also leads to faster convergence, is further accelerated
byourdedicatedinitializationandgivesapriorforbetterdecompositionquality.
Another line of approach is to decompose and parameterize a video using
auto-encoder-baseddisentangledrepresentationlearning[17,25].However,these
methodsassumethatacertainamountoftrainingdataisavailableinthetarget
domain, while we only require the target video for optimization.Fast Sprite Decomposition from Animated Graphics 5
3 Sprite decomposition
3.1 Data definition
In this work, we define an animated graphic X as a sequence of K sprites, each
consisting of a static texture image x and its animation parameters Θ :
k k
(cid:0) (cid:1) (cid:0) (cid:1)K
X = (x ,Θ ),(x ,Θ ),...,(x ,Θ ) = (x ,Θ ) . (1)
1 1 2 2 K K k k k=1
Here, k is the sprite index, x ∈ [0,1]H×W×4 is a RGBA texture with of size
k
H ×W. In a more general video decomposition, the texture image is dynamic;
i.e., x ∈[0,1]H×W×4×T. However, our main applications of animated graphics
k
often do not include a dynamic texture, so we drop the temporal dynamics
in the formulation in this work. We define the animation parameters Θ by
k
a temporal sequence of tuples of affine warping parameters and a sprite-level
opacity parameter: Θ =(Θt)T =(at,ot)T , where at ∈R6 is affine matrix
k k t=1 k k t=1 k
parameters and ot ∈[0,1] is a sprite-level opacity parameter.
k
We can render the graphic X into a raster video Y =(yt)T . Dropping the
t=1
notation t for simplicity, at a given time, an RGB frame y ∈ [0,1]H×W×3 is
obtained by rendering sprites from the back (k =1) to the front (k =K):
b =D(x ;Θ ), (2)
1 1 1
b =B(D(x ;Θ ),b ), (3)
k k k k−1
y =b , (4)
K
where b is an intermediate backdrop of the rendering and B is the source-over
k
alpha blending function. The function D warps the image by affine transform:
D(x;Θ) = W([x ,x ⊙o];a), where W is an image warping function by a
RGB A
given affine matrix, [·,·] denotes the channel-wise concatenation, ⊙ denotes the
element-wise product, and o∈[0,1]H×W is a 2D array filled with opacity o.
3.2 Problem formulation
WedefinethedecompositionproblemasfindingtheoptimalparameterX∗ that
can be rendered visually identical to the target raster video Y. Let L be the
functiontomeasuredifferencesbetweentwovideos.Thedecompositionproblem
can be expressed by:
(cid:0) (cid:1)
minL R(X),Y , (5)
X
whereR(·)istherenderingfunctionthatappliesEqs.(2)to(4)framebyframe.
Our experiments use the mean squared error for L.
4 Approach
WhileitispossibletoapplyanyoptimizationapproachtotheprobleminEq.(5),
the problem is practically hard to solve due to the complexity of the search6 T. Suzuki et al.
Sprite Animation Sprite Recon. video
Texture params = BG
🔥
🔥
θ
θ
🔥
FG seg.
Sprite Minimize
Animation
🔥 Texture params loss
θ🔥 Original video
θ
FG boxes
inputs 🔥 targets to optimize initialization flow
Fig.3:Ourdecompositionpipeline.Givenarastervideoandboundingboxannotation
forasingleframe,wefirstapplyavideoobjectsegmentationmodeltoinitializetexture
and animation parameters. Then, we apply a gradient-based optimizer to find the
optimal texture codes, animation parameters, and the texture prior parameters.
space. For example, naively applying a gradient-based optimizer often results
in unwanted artifacts with undesired sprite boundaries. We employ the follow-
ing approaches to achieve the good quality/efficiency tradeoff. 1) We introduce
an image prior model [27] and re-formulate texture optimization as a search
for model parameters and codes to prevent undesired artifacts. 2) We slightly
simplify the problem setup by assuming the user provides additional auxiliary
bounding boxes in a single frame, leading to an efficient initialization method
by video object segmentation. 3) We use a robust pre-trained video object seg-
mentation model [31] to identify a good initial solution for the optimal sprites.
Givenarastervideoandauxiliaryboundingboxes,wefirstapplyvideoobject
segmentation for the foreground sprites and initialize the texture codes and
renderingparameters.Then,weapplyastandardgradient-basedoptimizationto
findtheoptimalparameters.Oursimplifiedrepresentationitselfworksasaprior
and eliminates the need for regularization losses and optimization scheduling as
in existing methods [10,33].
4.1 Prior-based formulation
We introduce an image prior model [27] to re-formulate our optimization prob-
lem (Eq. (5)). An image prior model f represents a mapping of texture code
θ
z ∈RH×W×4 toatextureimagexwithmodelparametersθ:x=f (z).Assum-
θ
ing textures are generated from this prior model, we can transform the texture
optimization problem into a search over the code z and the model parameters
(cid:0) (cid:1)K
θ.LetusdefineZ = (z ,Θ ) .WecanrewriteEq.(5)inthefollowing:
k k k=1
minL(cid:0) R′(Z;θ),Y(cid:1)
, (6)
Z,θ
where R′ is the rendering function from Z and the prior model f .
θFast Sprite Decomposition from Animated Graphics 7
Following the existing study [33], we adopt U-Net [21] as a prior model ar-
chitecture and take the output from the code z as the texture. With multiple
sprites, we have an input code z for each sprite and generate texture x for
k k
each using the shared model parameter θ. The introduction of the prior model
couldincreasethenumberofvariablesintheoptimizationproblemandthetime
per iteration. However, we empirically find that the convolutional architecture’s
inductive bias effectively prevents undesirable artifacts, improves the resulting
texture quality, and eventually achieves a good quality/efficiency trade-off.
4.2 Auxiliary input
ThetransformedformulationofEq.(6)isstillahardproblemtofindareasonable
solution. In this work, we slightly change the problem setting and assume an
additional auxiliary user input that specifies the bounding box of the visible
objects in a single video frame. This auxiliary input tells us 1) the number
of sprites to decompose and 2) the rough location of the sprite at time step τ,
whichallowsustoinitializevariableseffectively.Ourauxiliaryinputispractically
effortless to obtain in interactive video editing, where users are only asked to
annotate bounding boxes in a single video frame.
4.3 Segmentation-based initialization
The goal of the initialization step is to derive good initial values for Z given the
auxiliary user input ((τ ,β ))K , where β is a bounding box for the sprite k.
k k k=1 k
In this work, we initialize the prior parameters θ by random values [27], and
opacityparameters o to1inalltimesteps,assumingthatallspritesarevisible
k
throughout the video frames.
For the initialization of the texture codes z and affine parameters at, we
k k
employanoff-the-shelfvideoobjectsegmentationmodel[31].Giventheauxiliary
user input, we apply a tracking model and obtain bounding boxes and segmen-
tation masks for all time steps t ̸= τ . Using the bounding boxes, we initialize
k
the affine parameters at to the box locations with no sheer component.
k
For initializing textures, we first obtain an initial RGB texture image for
each foreground sprite by averaging pixels over time within the bounding box
regions.Similarly,weobtainthealphachannelbyaveragingsegmentationmasks
overtime.Forthebackgroundtexture(k =1),weaveragethevisiblepixelvalues
over time for the RGB at each spatial position and set the alpha to 1. When
there are always occluded pixels, we in-paint the region by average pixel values
of the visible areas. Once we obtain the initial texture image, we naively treat
them as the initial codes z , which empirically yields good performance. After
k
initialization, we apply a standard gradient-based optimization to solve Eq. (6).
Our initialization is not perfect due to errors in various sources, such as
segmentation, spatial misalignment caused by the texture’s deformation, or the
transparency effect, but it is still sufficiently effective as the initial solution.
Also, the inference time for the video object segmentation model is negligible
comparedtothereductionofoptimizationtimethankstothegoodinitialization.8 T. Suzuki et al.
4.4 Sprite ordering
Our initialization approach has another limitation: our model does not know
the order of sprites. If the rendering order is incorrect, the initial solution may
need to swap the order of sprites. Otherwise, the whole process may fall into
a local minimum with a wrong order. To address this issue, we search for the
renderingorderthatminimizesthereconstructionerrorforthefirstN steps
warm
of the optimization and then optimize with the rendering order obtained at the
N -th step.
warm
5 Crello Animation dataset
We construct a new dataset to study animated graphics. Inspired by a dataset
of static design templates [30], we scrape animated templates from the online
design service2 that comes with complete sprite information in each animated
graphic. Our dataset, named the Crello Animation3 dataset, consists of hun-
dreds of visually appealing animated graphics, mostly designed for social media
platforms such as Instagram or TikTok.
In the dataset construction process, we simplify the original templates into
the format described in Sec. 3. We first export the static image textures for
all sprites at the same size. Then, we compute the animation parameters for
each sprite. The original templates come with sprite animations in one of 17
preset types, such as Zoom in/out, Fade in/out, Slide in/out, and Shake.
If an animation is set on the sprite, we generate per-frame affine matrices and
opacities based on their corresponding function. We apply the identity affine
matrix and opacity to all frames if no animation is set. After converting all
the templates, we excluded templates with animated backgrounds as outliers,
duplicated templates, and templates with less than two or more than six layers
toavoidtoocomplexsprites.Intheend,weobtained299samples.Werandomly
split them into 154 / 145 samples for validation and test splits. More details of
the dataset are presented in Appendix.
Compared with natural video datasets such as DAVIS [19] used for video
layer decomposition, our Crello Animation has unique characteristics: videos
contain various types of texture, including natural images, text, or illustrations,
and consist of various numbers of sprites, while the natural scene datasets con-
tain at most two or three objects. Our dataset preserves complete composition
information without artifacts in the background. This allows us to evaluate the
qualityofappearance,includingoccludedareas,whiletheexistingworks[10,33]
on video layer decomposition have evaluated using Intersection-over-Union of
only visible part. Although several studies [7,9,12,29] have proposed datasets
withcompositioninformationlikeours,thesearesynthetic.Ourdataset,sourced
from real-world design templates, enables more proper practical evaluations.
2 https://create.vista.com
3 https://huggingface.co/datasets/cyberagent/crello-animationFast Sprite Decomposition from Animated Graphics 9
6 Experiments
We evaluate the performance of our method in decomposing animated graphics
and conduct a comparative evaluation with existing video decomposition base-
lines that output similar layered representations using our Crello Animation.
6.1 Implementation details
We first tuned the hyperparameters using Crello Animation’s validation split
and then evaluated their performance on the test split with the tuned hyperpa-
rameters. We used Adam [11] as the optimizer with a learning rate of 10−3, set
N to 100 as described in Sec. 4.4 and set the resolution of z and textures
warm k
in our model to 100×100. We conducted all experiments on a workstation with
a single NVIDIA Tesla T4 accelerator. We resized the frame size of videos to
have a short side of 128 while keeping the aspect ratio.
For the initialization step, we adopted TAM [31] as the video object seg-
mentationmodelandusedtheofficialimplementation4.TAMsegmentsatarget
object by first specifying the target with a user prompt at the keyframe. We
used box prompts and simulated them using ground-truth sprites information
in our experiments. Since in Crello Animation it often happens that most or all
of an object’s area becomes invisible due to overlapping objects or fading, we
calculated the visible area for each sprite, selected the frame with the largest
visible area, and generated the rectangle surrounding the visible area in that
frame as the prompt. Our approach to extracting bounding boxes follows the
idea that a user annotates a frame where the object has a large visible area. We
obtained the segmentation of the entire video by applying TAM in the forward
and backward time directions from the keyframe. We study the robustness to
noise of the user prompt in Sec. 6.5.
6.2 Evaluation metrics
WeneedtomeasurethequalityofthedecomposedXˆ comparedtoground-truth
X. An animated graphic X consists of multiple interdependent variables, so the
evaluationmetricsneedtobecarefullydesigned.Wedevelopmultipleevaluation
metrics to evaluate the overall quality of the decomposed result.
Frame error.Wemeasurethereconstructionerrorbetweentherenderedvideo
frames Yˆ =R(Xˆ) and the original video frames Y =R(X):
E (Xˆ,X)=e(R(Xˆ),R(X)), (7)
frame
wheree(·,·)isafunctionthatmeasurestheerrorbetweenframes,andweusethe
meanofpixel-levelL1errororLPIPS[35].SinceYˆ isreconstructedusingallthe
informationofXˆ,thismetricmeasurestheoverallqualityofthedecomposedXˆ.
However, the frame error alone is insufficient because proximity in the rendered
video does not directly reflect the quality of the decomposition result.
4 https://github.com/gaomingqi/Track-Anything10 T. Suzuki et al.
Ours LNA DS
Frame error (L1) Sprite error Alpha (L1) Sprite error RGB (L1)
0.12
0.4
0.10
10 1 0.08 0.3
0.06
0.04 0.2
10 2 0.02
0.1
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
Time (min) Time (min) Time (min)
Fig.4: Comparisonofthetrade-offbetweenthequalityandoptimizationtimeonthe
test split. The solid lines show the average of the samples with four or fewer layers,
and the dashed lines show the average of the samples with five and six layers.
Sprite error. The optimal solution for a sprite (xˆ ,Θˆ ) is not unique. For
k k
example, two sprites can look identical if the texture is shifted by 1 pixel, but
the affine transformation adjusts the shift by −1 pixel. Thus, we render each
sprite and measure the reconstruction error in pixel space. Also, in this metric,
weaimtomeasurethequalityofeachspriteindependently,wemeasuretheerror
after searching for the optimal sprite assignment:
K T
E (Xˆ,X)=min 1 (cid:88)(cid:88) e(D(xˆ ;Θˆt ),D(x ;Θt)), (8)
sprite σ∈SK KT
k=1t=1
σ(k) σ(k) k k
whereS representsthesetofallpossiblepermutationfunctionsforK elements,
K
with the background always fixed (i.e., σ(1)=1). We define the RGB error by
the L1 error weighted by the alpha channel:
(cid:0) (cid:1)
e (yˆ,y)=ϕ ∥yˆ −y ∥ ⊙y , (9)
RGB RGB RGB 1 A
where subscripts represent the channels, respectively, and ϕ is the average oper-
ator over the spatial dimensions. For the alpha channel, we use the L1 error.
6.3 Comparison to prior work
WecompareourmethodwithLNA[10]andDS[33],whichoutputsimilarsprite
representations to ours as described in Sec. 2. We used the official implementa-
tions of both methods, with slight modifications to fit our setting. For LNA, we
used the TAM’s segmentation masks to calculate the mask bootstrapping loss.
ForDS,wesimplifythetransformationastheaffinetransformationandinitialize
the parameters in the same way as our method. We conducted hyperparameter
tuning on the validation split and evaluated the performance on the test split
for all baselines, including ours.
Fig. 4 shows the trade-off curve between optimization time and quality for
eachmethod.OurmethodshowssmallerrorsevenattheearlystagecomparedtoFast Sprite Decomposition from Animated Graphics 11
Table 1: Quantitative comparison on the test split. All values are averages of the
samples. The best and the second best result for each metric are highlighted in bold
and underlined, respectively. * indicates the results after optimization convergence.
Time Frame error ↓ Sprite error ↓
Method # Iter.
(min.) L1 LPIPS RGB L1 Alpha L1
LNA 3k 10.4 0.0339 0.2955 0.2654 0.0370
DS 9k 10.6 0.2446 0.3869 0.2965 0.0499
Ours 11k 10.2 0.0163 0.0670 0.1179 0.0193
LNA* 11k 40.7 0.0163 0.1321 0.2332 0.0336
DS* 16k 22.8 0.0054 0.0224 0.1190 0.0294
Ours* 91k 91.8 0.0101 0.0411 0.0984 0.0179
other methods. DS receives smaller frame errors than ours as the optimization
progresses, but ours is still better in the sprite errors. We suspect this situa-
tion was caused by DS’s too-high degree of representation, which can reduce
the reconstruction error even if it does not decompose a video well. Our static
texture assumption and limited animation parameters effectively regularize the
optimization process and prevent this local minima.
Tab. 1 shows the quantitative comparison on the test split at approximately
the same optimization time (10 minutes) and after convergence. We define the
maximum number of iterations for each method as the iteration where the best
sprite error is not updated for a quarter of the current iteration on the valida-
tion split, and report the errors at the iteration where the loss is minimized as
the converged results on the test split (the results on the validation split is in
Appendix). When the optimization time is 10 minutes, our method shows the
best results across all metrics, and moreover, it achieves better sprite error than
the converged comparative methods. After convergence, ours achieves the best
results in terms of the sprite error. We emphasize that the frame error is an
auxiliary metric that can be low even if the decomposition fails.
Also, we show the qualitative comparison in Fig. 5. In LNA, the reconstruc-
tion results are generally blurred, and the sprite boundaries are rough. We sus-
pect LNA has a bias to generate smooth masks since it represents masks with
an MLP that tends to output smooth value for the input, i.e. coordinate. DS
achieves more precise boundaries than LNA, but DS sometimes fails to group
objects. For example, in the first sample in Fig. 5, the second sprite is partially
included in the first sprite. Our method does not allow a single sprite to have
a complex animation and successfully decomposes this case. We observe a sub-
tle artifact where the foreground remains in the background. We suspect this is
because the foreground alpha of 1 in the ground truth is not exactly optimized
to 1. This causes the occluded area to slightly impact reconstruction, leading to
minor RGB inaccuracies that reduce reconstruction loss. We might be able to
rely on post-processing or manual editing since those artifacts often stand out
in easily fixable homogeneous regions. We provide more qualitative results in
Appendix.12 T. Suzuki et al. Sprite 1 Sprite 2
Frames Sprite 1 (fade-in) Sprite 2 (slide-in from left)
GT
Ours
LNA
DS
Frames Sprite 1(slide-in from left) Sprite 2 (slide-in from right)
GT
Ours
LNA
DS
Fig.5: Qualitative comparison between LNA [10], DS [33], and our method. We put
the description of the animation above each sprite.
6.4 Ablation study
Weablatetheeffectofeachcomponentofourmethodusingthevalidationsplit.
We summarize the results in Tab. 2. We can see that using the texture prior
improves the decomposition quality in all metrics from the comparison betweenFast Sprite Decomposition from Animated Graphics 13
Table 2: Ablation study of our decomposition pipeline. In all settings, the number
of total iterations is 20,000. The best and the second best result for each metric are
highlighted in bold and underlined, respectively.
Texture Frame error ↓ Sprite error ↓
Texture init. Matrix init.
prior model L1 LPIPS RGB L1 Alpha L1
✓ ✓ ✓ 0.0207 0.0793 0.1344 0.0237
✓ ✓ 0.0324 0.1473 0.1995 0.0283
✓ ✓ 0.0336 0.2090 0.2651 0.0324
✓ 0.0318 0.2258 0.3064 0.3384
Frame error (L1) Sprite error RGB (L1) Sprite error Alpha (L1)
1.5
1.0
0.5
0.0
m=1 1 1 1 9 9 9 9 17 17 17 17
rmax=0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3
Fig.6: Resultsofevaluatingtherobustnesstoprompt’snoise.Theverticalaxisshows
the relative error normalized by the one when no noise is added (the leftmost bar).
The gray dashed line indicates where the relative error becomes 1.
the first and second rows. This conforms to the previous report in generative
tasks [27]. Comparing rows 3 and 4, texture initialization significantly improves
the sprite alpha L1. This suggests that with no appropriate texture initializa-
tion, optimization tends to fall into local minima where the reconstruction error
is small, but the sprite is inappropriately decomposed. Initializing both affine
matrices and textures achieves the best results in all metrics.
6.5 Robustness to prompt noise
Although we simulate box prompts by users in our experiments, bounding box
annotation usually contains noise in a real-world application. We verify the ro-
bustness of our method to the annotation noise. We consider two types of noise:
noise in the keyframe selection and noise in the box’s position and size. For
the former, we select the frame with the m-th largest visible area (described in
Sec. 6.1) for each foreground sprite, varying the m. For the latter, we add noise
to a box directly: p′ =p+s×r, where p is top, bottom, left, or right coordinate
of the box, s is the height if p is the top or bottom and the width otherwise,
and r is a random variable sampled from [−r ,r ]. We vary the r and
max max max
evaluate the decomposition quality. We show the results in Fig. 6. The results
confirm that our method does not suffer from critical performance degradation
eveninthepresenceofsubstantialnoise(m=9andr =0.3),indicatingthat
max
usersdonotneedtobenervousabouttheaccuracyofboundingboxannotation.
rorre
evitaleR14 T. Suzuki et al.
Time
(a)
(b)
(c)
Fig.7: Applicationexamplesofourdecompositionresults.Wedecomposedavideoin
the test split (the first row) using our method and applied three types of editing: (a)
sprite removal, (b) texture replacement, and (c) animation (rotation) insertion.
7 Application
We demonstrate a video editing application using our decomposition approach,
shown in Fig. 7. Here, we first decomposed videos and applied three types of
editing:(a)spriteremoval,(b)texturereplacement,and(c)animationinsertion.
In the texture replacement, we replaced the texture of a sprite with a different
one while keeping the animation. In the animation insertion, we added rotation
totheoriginalanimationwhilekeepingthetexture.Wecanobservethatthenew
occlusion and re-appearance caused by the editing are appropriately reflected,
andtheoriginalanimationsarecorrectlytransferredtothenewtextures.Thanks
toouraccuratespritedecomposition,theseexamplesdonothavemajorartifacts,
which is preferable for video editing.
8 Conclusion
We addressed sprite decomposition from animated graphics. Our optimization-
based approach introduces several strategies to make efficient decomposition for
theanimatedgraphics,andtheevaluationinournewlycreatedCrelloAnimation
shows that our method successfully outperforms existing methods in the trade-
off between the quality of the decomposition and the convergence time.
Inthefuture,itwouldbeinterestingtorelaxourstaticspriteassumptionand
represent deformation and opacity as a function of time rather than per-frame
add
(e.g., animation representation via keyframes and their interpolation, which is
animation
common in video editing software). This would allow for applications such as
increasing the temporal resolution and may improve the performance of the
decomposition, functioning as an additional prior. Also, we are interested in
parameterizing videos with more types of animation than motion and opacity,
such as blur change and lighting effects, to support creative video workflow.Fast Sprite Decomposition from Animated Graphics 15
References
1. Agarwal, S., Wills, J., Belongie, S.: What went where. In: CVPR (2003)
2. Aksoy, Y., Aydın, T.O., Smolić, A., Pollefeys, M.: Unmixing-based soft color seg-
mentation for image manipulation. TOG (2017)
3. Brostow, G.J., Essa, I.A.: Motion based decompositing of video. In: ICCV (1999)
4. Chen, Y., Liu, S., Wang, X.: Learning continuous image representation with local
implicit image function. In: CVPR (2021)
5. Gandelsman,Y.,Shocher,A.,Irani,M.:"double-dip":unsupervisedimagedecom-
position via coupled deep-image-priors. In: CVPR (2019)
6. Geng, C., Peng, S., Xu, Z., Bao, H., Zhou, X.: Learning neural volumetric repre-
sentations of dynamic humans in minutes. In: CVPR (2023)
7. Johnson,J.,Hariharan,B.,VanDerMaaten,L.,Fei-Fei,L.,LawrenceZitnick,C.,
Girshick, R.: Clevr: A diagnostic dataset for compositional language and elemen-
tary visual reasoning. In: CVPR (2017)
8. Jojic, N., Frey, B.J.: Learning flexible sprites in video layers. In: CVPR (2001)
9. Kabra,R.,Burgess,C.,Matthey,L.,Kaufman,R.L.,Greff,K.,Reynolds,M.,Ler-
chner, A.: Multi-object datasets. https://github.com/google-deepmind/multi_
object_datasets (2019)
10. Kasten, Y., Ofri, D., Wang, O., Dekel, T.: Layered neural atlases for consistent
video editing. TOG (2021)
11. Kingma,D.,Ba,J.:Adam:Amethodforstochasticoptimization.In:ICLR(2015)
12. Lamdouar, H., Xie, W., Zisserman, A.: Segmenting invisible moving objects. In:
BMVC (2021)
13. Lee,Y.C.,Jang,J.Z.G.,Chen,Y.T.,Qiu,E.,Huang,J.B.:Shape-awaretext-driven
layered video editing. In: CVPR (2023)
14. Li, T.M., Lukáč, M., Michaël, G., Ragan-Kelley, J.: Differentiable vector graphics
rasterization for editing and learning. TOG (2020)
15. Lu, E., Cole, F., Dekel, T., Zisserman, A., Freeman, W.T., Rubinstein, M.: Omn-
imatte: Associating objects and their effects in video. In: CVPR (2021)
16. Ma, X., Zhou, Y., Xu, X., Sun, B., Filev, V., Orlov, N., Fu, Y., Shi, H.: Towards
layer-wise image vectorization. In: CVPR (2022)
17. Monnier, T., Vincent, E., Ponce, J., Aubry, M.: Unsupervised layered image de-
composition into object prototypes. In: ICCV (2021)
18. Pawan Kumar, M., Torr, P.H., Zisserman, A.: Learning layered motion segmenta-
tions of video. IJCV (2008)
19. Perazzi, F., Pont-Tuset, J., McWilliams, B., Van Gool, L., Gross, M., Sorkine-
Hornung, A.: A benchmark dataset and evaluation methodology for video object
segmentation. In: CVPR (2016)
20. Rav-Acha, A., Kohli, P., Rother, C., Fitzgibbon, A.: Unwrap mosaics: A new rep-
resentation for video editing. TOG (2008)
21. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomed-
ical image segmentation. In: MICCAI (2015)
22. Sbai, O., Couprie, C., Aubry, M.: Unsupervised image decomposition in vector
layers. In: ICIP (2020)
23. Shimoda, W., Haraguchi, D., Uchida, S., Yamaguchi, K.: De-rendering stylized
texts. In: ICCV (2021)
24. Sitzmann, V., Martel, J., Bergman, A., Lindell, D., Wetzstein, G.: Implicit neural
representations with periodic activation functions. In: NeurIPS (2020)16 T. Suzuki et al.
25. Smirnov,D.,Gharbi,M.,Fisher,M.,Guizilini,V.,Efros,A.,Solomon,J.M.:Mar-
ionette: Self-supervised sprite learning. In: NeurIPS (2021)
26. Tancik, M., Srinivasan, P., Mildenhall, B., Fridovich-Keil, S., Raghavan, N., Sing-
hal, U., Ramamoorthi, R., Barron, J., Ng, R.: Fourier features let networks learn
high frequency functions in low dimensional domains. In: NeurIPS (2020)
27. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Deep image prior. In: CVPR (2018)
28. Wang, J.Y., Adelson, E.H.: Representing moving images with layers. TIP (1994)
29. Xie, J., Xie, W., Zisserman, A.: Segmenting moving objects via an object-centric
layered representation. In: NeurIPS (2022)
30. Yamaguchi, K.: Canvasvae: Learning to generate vector graphic documents. In:
CVPR (2021)
31. Yang, J., Gao, M., Li, Z., Gao, S., Wang, F., Zheng, F.: Track anything: Segment
anything meets videos. arXiv preprint arXiv:2304.11968 (2023)
32. Yang, Y., Lai, B., Soatto, S.: Dystab: Unsupervised object segmentation via
dynamic-static bootstrapping. In: CVPR (2021)
33. Ye, V., Li, Z., Tucker, R., Kanazawa, A., Snavely, N.: Deformable sprites for un-
supervised video decomposition. In: CVPR (2022)
34. Zhang, L., Wong, T.T., Liu, Y.: Sprite-from-sprite: Cartoon animation decompo-
sition with self-supervised sprite estimation. TOG (2022)
35. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: CVPR (2018)Fast Sprite Decomposition from Animated Graphics 17
A Dataset details
Tab. 3 summarizes the description and the number of each animation type in
the Crello Animation dataset. Each sprite has one of the animation types or
no animation. All animation types can be represented by affine transformation
and opacity changes. In addition to the animation type, each sprite has a delay
parameter,whichspecifiesthestarttimeoftheanimation.Wesetthedurationof
all videos to 5 seconds and adjusted the speed of each animation accordingly, as
intheactualrenderingengine5.Wesettheframerateto10forourexperiments,
butitcanbesettoanyvalueastheoriginalanimationsarecontinuousfunctions
of time.
Fig. 8 shows the histogram of the number of sprites in each video and the
aspect ratio. Though the frame resolution can be set to any value by changing
the target size of the affine matrices, we set the short edge to 128 pixels for our
experiments while keeping the original aspect ratio.
80
Val Val
60
Test 60 Test
40
40
20 20
0 0
2 3 4 5 6 16:9 1:1 9:16
# Sprites Aspect ratio (width:height)
Fig.8: Statistics in Crello Animation.
B Additional results
WeprovideadditionalquantitativeresultsonthevalidationsplitinTab.4.Asin
theresultsonthetestsplitinthemainpaper,ourmethodachievesspriteerrors
comparable to the converged other methods even in 10 minutes, and achieves
even lower sprite errors after convergence.
Wealsoprovideadditionalqualitativeresults.Figs.9to12showthecompar-
ison between Layered Neural Atlases (LNA) [10], Deformable sprites (DS) [33]
and our method. As described in the main paper, our method consistently de-
composes sprites with higher quality than LNA and DS, especially for sprites
with complex contours such as text. Figs. 13 and 14 show more examples of the
decomposition results with textures by our method. The output textures and
groundtruthtexturesmaydifferinthedegreesoffreedomoftheaffinetransfor-
mation,buttheoutputanimationsareadjustedaccordinglysotheyarecorrectly
reproduced as sprites.
5 https://create.vista.com
elpmaS
#
elpmaS
#18 T. Suzuki et al.
Table 3: AnimationtypesintheCrelloAnimationdataset.“None” indicatesthatthe
sprite has no animation.
# Sprites # Sprites
Type Description
(Val.) (Test)
Translation changes continuously. There are three
types: Slide-in, where translation changes from the
start position outside the frame to the base posi-
Slide 244 242
tion; Slide-out, where translation changes inversely;
and Slide-both, where both Slide-in and Slide-out oc-
cur sequentially.
Opacity and scale change continuously. There are
threetypes:Scale-in,whereopacityandscalechange
Scale 165 150 from0to1;Scale-out,whereopacityandscalechange
inversely; and Scale-both, where both Scale-in and
Scale-out occur sequentially.
Opacitychangescontinuously.Therearethreetypes:
Fade-in, where opacity changes from 0 to 1, Fade-
Fade 91 92
out, where opacity changes inversely, and Fade-both,
where both Fade-in and Fade-out occur sequentially.
Scale changes continuously. There are three types:
Zoom-in,wherescalechangesfrom0to1;Zoom-out,
Zoom 58 43
where scale changes inversely; and Zoom-both, where
both Zoom-in and Zoom-out occur sequentially.
Shake 26 12 Translation oscillates horizontally or vertically.
Theobjectrotatesaroundthecenteraxisinthehor-
Spin 6 6
izontal or vertical direction.
Flash 4 5 Opacity oscillates between 0 and 1.
None 210 194 –
Wealsoshowfailurecasesofourmethod.InthefirstexampleinFig.15,the
close sprites with similar animations are difficult to decompose. In the second
example in Fig. 16, the sprite with (almost) no animation tends to be absorbed
into the background. These failure cases are challenging because they result in
smallreconstructionerrors.Ourinitializationshouldfunctionasapriortoavoid
these failures, but further consideration of priors may be necessary.
C Baseline details
We describe the details of the comparison baselines, Layered Neural Atlases
(LNA) [10] and Deformable Sprites (DS) [33].Fast Sprite Decomposition from Animated Graphics 19
Table 4: Quantitativecomparisonwithpriorworksonthevalidationsplit.Allvalues
are averages of the samples. The best and the second best result for each metric are
highlightedinboldandunderlined,respectively.*indicatestheresultsafteroptimiza-
tion convergence.
Time Frame error ↓ Sprite error ↓
Method # Iter.
(min.) L1 LPIPS RGB L1 Alpha L1
LNA 3k 10.8 0.0308 0.2584 0.2422 0.0271
DS 9k 10.3 0.2771 0.4720 0.3497 0.0369
Ours 11k 10.5 0.0123 0.0510 0.1095 0.0116
LNA* 11k 40.6 0.0145 0.1151 0.2003 0.0214
DS* 16k 24.4 0.0052 0.0167 0.1068 0.0146
Ours* 91k 97.9 0.0090 0.0338 0.0926 0.0094
C.1 Layered Neural Atlases
Basedontheofficialcode(updatedversion)6,wemakeaminormodificationand
tunehyperparameters.Forafaircomparison,weutilizethepredictedforeground
segmentation masks, which we also used in our method. Specifically, we add a
binary cross-entropy loss to match the predicted alpha with the segmentation
mask for each sprite, as the alpha bootstrapping loss in the original paper. We
adopt the original paper’s setting except for the weight of the flow alpha loss
(β in their paper) set to 49 and the weight of the rigidity loss (β in their
f−α r
paper) set to 1. We set the weight of the additional binary cross entropy loss to
10,000.
C.2 Deformable Sprites
We adopted the official implementation7 to our problem and adjusted several
hyperparameters for a fair comparison. DS uses an image prior model to repre-
sent texture images, similar to our method ($4.1). We apply the same strategy
to initialize the textures as we do ($4.3). We also simplify the deformation as
theaffinetransformationandinitializeitscorrespondingparametersinamanner
similartoourmethod.Toincorporatethegivensegmentationmasks,weemploy
the binary cross-entropy loss used in LNA to guide the predicted alpha masks.
With a schedule ratio of 1:10 for warm start and main optimization, we set the
weightsoftheaddedalphalossandthedynamicgroupingloss(L intheir
dynamic
paper) to 1.0 for the warm start; we set the weights of the reconstruction loss
(L in their paper) to 1.0, and the alpha and grouping losses to 0.01 for the
recon
mainoptimization.Weomitotherlosses,suchasopticalflowconsistencylosses,
because they do not work effectively in our data domain/problem setting.
6 https://github.com/thiagoambiel/NeuralAtlases
7 https://github.com/vye16/deformable-sprites20 T. Suzuki et al.
Frames Background Sprite 1 (Slide-in from bottom)
GT
Ours
LNA
DS
Sprite 2 (Zoom-in) Sprite 3 (Zoom-in)
GT
Ours
LNA
DS
Fig.9: Qualitative comparison between Layered Neural Atlases (LNA) [10], De-
formable sprites (DS) [33], and our method. We put the description of the animation
above each sprite. Best viewed with zoom and color.Fast Sprite Decomposition from Animated Graphics 21
Frames
GT
Ours
LNA
DS
Background Sprite (Flash)
GT
Ours
LNA
DS
Fig.10: Qualitative comparison between Layered Neural Atlases (LNA) [10], De-
formable sprites (DS) [33], and our method. We put the description of the animation
above each sprite. Best viewed with zoom and color.22 T. Suzuki et al.
Frames Background Sprite 1 (Slide-in from left)
GT
Ours
LNA
DS
Sprite 2 (Zoom-in) Sprite 3 (Slide-in from right) Sprite 4 (Slide-in from left)
GT
Ours
LNA
DS
Fig.11: Qualitative comparison between Layered Neural Atlases (LNA) [10], De-
formable sprites (DS) [33], and our method. We put the description of the animation
above each sprite. Best viewed with zoom and color.Fast Sprite Decomposition from Animated Graphics 23
Frames Background Sprite 1 (Slide-in from right up)
GT
Ours
LNA
DS
Sprite 2 (Slide-in from left up) Sprite 3 (Slide-in from right bottom) Sprite 4 (Slide-in from left bottom)
GT
Ours
LNA
DS
Fig.12: Qualitative comparison between Layered Neural Atlases (LNA) [10], De-
formable sprites (DS) [33], and our method. We put the description of the animation
above each sprite. Best viewed with zoom and color.24 T. Suzuki et al.
Sprite 1 (Slide-in from right)
Background Frames Texture
Sprite 2 (Slide-in from left) Sprite 3 (Slide-in from left)
Texture Texture
Fig.13: Output example of our method. The top-left group shows the background
texture and the reconstructed frame and the others show the foreground sprites. We
put the description of the animation above each sprite. Best viewed with zoom and
color.
Sprite 1 (Fade-in)
Background Frames Texture
Sprite 2 (Slide-in from left)
Texture
Fig.14: Output example of our method. The top-left group shows the background
texture and the reconstructed frame, and the others show the foreground sprites. We
put the description of the animation above each sprite. Best viewed with zoom and
color.
TG
tuptuO
TG
tuptuO
TG
tuptuO
TG
tuptuOFast Sprite Decomposition from Animated Graphics 25
Sprite 1 (Fade-in)
Background Frames Texture
Sprite 2 (Fade-in) Sprite 3 (Zoom-in)
Texture Texture
Sprite 4 (Zoom-in)
Texture
Fig.15: A failure case of our method. Sprites with similar animations and close dis-
tancesaredifficulttodecompose(asshowninSprite1andSprite3).Thetop-leftgroup
shows the background texture and the reconstructed frame, and the others show the
foreground sprites. We put the description of the animation above each sprite. Best
viewed with zoom and color.
TG
tuptuO
TG
tuptuO
TG
tuptuO26 T. Suzuki et al.
Sprite 1 (Static)
Background Frames Texture
Sprite 2 (Slide-in from right up) Sprite 3 (Slide-in from left down)
Texture Texture
Sprite 3 (Zoom-in)
Texture
Fig.16: A failure case of our method. Sprites with (almost) no animation tend to be
absorbed into the background (as shown in Background and Sprite 1). The top-left
groupshowsthebackgroundtextureandthereconstructedframeandtheothersshow
theforegroundsprites.Weputthedescriptionoftheanimationaboveeachsprite.Best
viewed with zoom and color.
TG
tuptuO
TG
tuptuO
TG
tuptuO