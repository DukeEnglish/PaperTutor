ON THE CHOICE OF THE NON-TRAINABLE INTERNAL WEIGHTS IN RANDOM
FEATURE MAPS
Pinak Mandal,∗1 Georg A. Gottwald1
1 The University of Sydney, NSW 2006, Australia
Abstract. Thecomputationallycheapmachinelearningarchitectureofrandomfeaturemapscanbeviewed
as a single-layer feedforward network in which the weights of the hidden layer are random but fixed and
only the outer weights are learned via linear regression. The internal weights are typically chosen from a
prescribed distribution. The choice of the internal weights significantly impacts the accuracy of random
featuremaps. Weaddressherethetaskofhowtobestselecttheinternalweights. Inparticular,weconsider
the forecasting problem whereby random feature maps are used to learn a one-step propagator map for a
dynamicalsystem. Weprovideacomputationallycheaphit-and-runalgorithmtoselectgoodinternalweights
whichleadtogoodforecastingskill. Weshowthatthenumberofgoodfeaturesisthemainfactorcontrolling
theforecastingskillofrandomfeaturemapsandactsasaneffectivefeaturedimension. Lastly,wecompare
random feature maps with single-layer feedforward neural networks in which the internal weights are now
learned using gradient descent. We find that random feature maps have superior forecasting capabilities
whilsthavingseveralordersofmagnitudelowercomputationalcost.
1. Introduction
Estimation and prediction of the state of a dynamical system evolving in time is central to our under-
standing of the natural world and to controlling the engineered world. Often practitioners are tasked with
such problems without the knowledge of the underlying governing dynamical system. In such scenarios a
popularapproachistoreconstructthedynamicalmodelfromobservationsofthesystem[31,1,5]. Predicting
the future state of the system from these reconstructions is particularly challenging for chaotic dynamical
systems. Chaotic dynamical systems cannot be accurately predicted beyond a finite time known as the
predictability time due to their sensitive dependence on the initial conditions.
In recent times machine learning has achieved remarkable progress in learning surrogate models for dy-
namical systems from given data. Recurrent networks such as Long Short-Term Memory networks [39, 41]
and gated recurrent units [8] have been successfully applied in a plethora of time series prediction tasks
[9, 6, 22]. These methods however often contain learnable parameters of the order of O(106), and require
substantial fine tuning of hyperparameters and costly optimization strategies [21]. An attractive alternative
is provided by random feature maps [36, 37, 33] and its extensions such as echo state networks and reservoir
computers [30, 29, 35, 32]. These architectures can be viewed as a single-layer feedforward network in which
the weights and biases of the hidden layer, the so called internal parameters, are randomized before training
and then are kept fixed. This renders the costly nonconvex optimization problem of neural networks to
a simple linear least-square regression for the outer weights. The output of random feature maps and its
extensions is hence a linear combination of a high-dimensional randomized basis. These methods have been
shown to enjoy the universal approximation property, which states that in principle they can approximate
any continuous function arbitrarily close [38, 3, 16, 12].
We focus here on classical random feature maps [36, 37, 33] which have recently been shown to have
excellent forecasting skill for chaotic dynamical systems [15, 14]. The fact that random feature maps enjoy
the universal approximation property does not provide practitioners with information on how to choose the
internal parameters. The internal parameters are typically drawn from some prescribed distribution such
as the uniform distribution on an interval or a Gaussian distribution. The forecasting capability of the
learned surrogate map sensitively depends on the choice of the distribution [15]. To generate good internal
parameterswhichleadtoimprovedperformanceofrandomfeaturemaps,severaldata-independentmethods
such as Monte Carlo and quadrature based algorithms as well as data-dependent methods such as leverage
∗Correspondingauthor: pinak.mandal@sydney.edu.au
1
4202
guA
7
]GL.sc[
1v62630.8042:viXrascore based sampling and kernel learning have been proposed; for a detailed survey see [23]. In recent work
Dunbar et al [34] choose the distribution of the random weights from a parametric family. The parameters
are chosen to optimize a cost function motivated from Empirical Bayes arguments, with the optimization
performed with derivative-free Ensemble Kalman inversion. Here we introduce a computationally cheap,
non-parametric, optimization-free and data-driven method to draw internal parameters which lead to im-
proved forecasting skill. We argue that good features, corresponding to good internal parameters, need to
explore the expressivity of a given activation function. We consider here as an example the tanh activation
function. To allow for good expressivity, good parameters should neither map the training data into the
linear range of the activation function nor into the saturated range in which different inputs cannot be dis-
cerned. This leads us to a definition of good features corresponding to good internal parameters. We show
that the set of good internal parameters is non-convex but can be expressed as a union of convex sets. To
sample from a convex set we employ a hit-and-run algorithm [40, 43]. Hit-and-run algorithms are a class
of Markov chain Monte Carlo samplers known for their fast mixing times in convex regions [26, 28, 20]. In
recent years, hit-and-run algorithms have also been analyzed for sampling nonconvex regions [7, 17, 2].
The hit-and-run algorithms we develop allow us to generate any desired ratio of good features. We show
in numerical experiments that the ratio of good features as defined by our criterion controls the forecasting
capabilities of the learned surrogate map. Moreover, we illustrate the mechanism by which the least-square
solution enhances good features and suppresses bad ones.
A secondary objective of our work is to demonstrate that a random feature map typically achieves superior
forecastingskillwhencomparedtoaneuralnetworkofthesamearchitecture, trainedwithgradientdescent,
while being several orders of magnitude cheaper computationally. We show that the bad performance of the
single-layer feedforward network can be attributed to the optimization procedure not being constrained to
the set of good internal parameters. This can potentially lead to new design and improved training schemes
for more complex networks.
The outline of this paper is as follows. In Section 2 we describe the setup of data-driven surrogate maps
for dynamical systems and how to assess their forecasting capabilities. Section 3 introduces random feature
mapsandillustrateshowthechoiceoftheinternalweightseffectstheforecastingcapabilitiesoftheassociated
trained surrogate maps. Section 4 defines the set of good internal parameters and introduces hit-and-run
algorithms to uniformly sample from this set. Section 5 illustrates the effect of sampling from the good
set of internal parameters on the forecasting skill and how the least-square training learns to distinguish
good features associated with good parameters from those associated with internal parameters drawn from
the complement of the good set. Section 6 compares random feature maps with single-layer feedforward
networks in which the internal parameters are learned using backpropagation, and establishes that random
feature maps with good parameters far outperform the single-layer feedforward neural network. Finally, we
conclude in Section 7 with a summary of our results and possible future extensions.
2. Dynamical setup
Weconsidertheforecastingproblemforchaoticdynamicalsystems. ConsiderthefollowingD-dimensional
continuous-time system,
u˙ =F(u), (1)
withinitialdatau(0)=u ,whichweobserveatdiscretetimest =n∆tforn=1,2,...,N. Weconsiderhere
0 n
the case when the full D-dimensional state is observed and observations are noise-free. For the treatment of
noisyobservationsandpartialobservationssee[15,14]. Weviewthedynamicalsystemoftheseobservations
in terms of a discrete propagator map,
u =Ψ (u ). (2)
n+1 ∆t n
The aim of data-driven modelling is to construct a surrogate map Ψ from the training data given by the
S
observationsthatwellapproximatesthetruepropagatormapΨ of (2). Inthefollowingwedenotevariables
∆t
associated with the surrogate map with a hat, and write the learned surrogate dynamical system as
uˆ =Ψ (uˆ ), (3)
n+1 S n
2with initial data uˆ = u . Throughout this work we use the D = 3-dimensional Lorenz-63 system [24, 25]
0 0
with u=(x,y,z) and
x˙ =10(y−x),
y˙ =x(28−z)−y,
(4)
8
z˙ =xy− z,
3
as the underlying continuous dynamical system (1). The Lorenz-63 system is chaotic with a positive Lya-
punov exponent of λ ≈ 0.91 [42]. We generate independent training and validation data sampled at
max
∆t = 0.02 by randomly selecting initial conditions u . We discard an initial transient dynamics of 40 time
0
units to ensure that the dynamics has settled on the attractor.
To test the predictive capability of a surrogate model, we define the forecast time τ associated with the
f
surrogate model,
(cid:26) ∥uˆvalidation−uvalidation∥2 (cid:27)
τ =inf t λ : n n 2 >θ . (5)
f n max ∥uvalidation∥2
n 2
The forecast time is measured in Lyapunov time units and measures when the prediction of the learned
surrogate map (3), initialized at uˆvalidation = uvalidation, significantly deviates from the true validation
0 0
trajectory uvalidation. We employ here an error threshold of θ =0.05.
n
3. Random feature maps
We consider random feature maps to learn the surrogate map (3) with
Ψ (u)=Wσ(W u+b ), (6)
S in in
whereuistheD-dimensionalstatevector,W
in
∈RDr×Distheinternalweightmatrix,b
in
∈RDr theinternal
bias and W ∈ RD×Dr the outer weight matrix. The nonlinear activation function σ is applied component
wise and we choose here σ = tanh. Random features are characterized by the internal weights (W ,b )
in in
being drawn before training from a prescribed distribution p(w ) and p(b ). The internal weights remain
in in
fixed and are not learned as it would be the case for a single-layer feedforward network which has the same
architecture as in (6). Random feature maps can hence be seen as a linear combination of D -dimensional
r
random features vectors
ϕ=σ(W u+b ). (7)
in in
In the following we refer to components of this feature vector as features.
The matrix W, controlling the linear combinations of the feature vectors (7), is learned from training
data U ∈ RD×N the columns of which are the observations u , n = 1,...,N of the system (2). We do so
n
by solving the following regularized optimization problem,
W∗ =argminL(W;U), (8)
W
with loss function
L(W;U)=∥WΦ(U)−U∥2+β∥W∥2. (9)
Here ∥·∥ denotes the Frobenius norm, β > 0 is a regularization hyperparameter, and Φ(U) is the feature
matrix whose n-th column is given by,
ϕ(u )=tanh(W u +b ). (10)
n−1 in n−1 in
The solution of the optimization problem (8) is given explicitly by linear ridge regression as
W∗ =UΦ(U)⊤(Φ(U)Φ(U)⊤+βI)−1. (11)
The low computational cost of random feature maps makes them a very attractive architecture.
3Figure 1. Contour plots of the mean and standard deviation of the forecast time τ com-
f
puted using W ,b sampled uniformly from intervals of variable size [−w,w] and [−b,b]
in in
respectively. Samples were drawn for grid points (w,b) on a 30×30 regular grid over the
domain (0,0.4) × (0,4.0). Averages are taken over M = 100 realizations per grid-point
(w,b), for a feature dimension D = 300, training data length N = 20,000 and regulariza-
r
tion parameter β =4×10−5, using fixed training and validation data.
3.1. The effect of the internal weights on the performance of random feature maps. Random
featuremapsenjoytheuniversalapproximationproperty[38,3]andhence,inprinciple,forasufficientlyhigh
feature dimension D can approximate continuous functions arbitrarily close. The universal approximation
r
property however does not guide practitioners how to find the internal weights (W ,b ) which allow for
in in
such an approximation. The main objective of this paper is to sample the internal parameters in a way that
increases the forecasting skill of the random feature maps.
Indeed, the forecasting skill of a learned random feature map surrogate model (3) sensitively depends
on the internal weights. To illustrate the effect of the hyperparameters (W ,b ) on the forecast time
in in
τ we uniformly sample W and b from the intervals [−w,w] and [−b,b], respectively, with (w,b) ∈
f in in
(0,w ) × (0,b ). In particular, we use 30 × 30 regular grid points over (0,w ) × (0,b ) with
max max max max
w = 0.4 and b = 4.0, and probe the statistics by generating M = 100 feature maps for each grid
max max
point, while keeping the training data and the validation data fixed for all realizations to focus on the effect
of the internal weights. We fix the feature dimension at D = 300 and the regularization parameter at
r
β =4×10−5. Figure 1 shows a contour plot of the mean and the standard deviation of the forecast time τ
f
overthedomainoftheinternalweights. Wecanclearlyseethatcertainregionsinthehyperparameterspace
are associated with good performance with mean forecast times τ > 4 while other regions produce poor
f
mean forecast times. Moreover, regions in the hyperparameter space corresponding to high mean forecast
times τ may have large variance.
f
Ideally, we would like parameters which have both, high mean forecast time and low variance so that the
performance is not dependent on the particular training data used. It is clear that if the internal weights
(W ,b )arechosensufficientlysmall,theassociatedfeatures(7)areessentiallylinearwithϕ≈W u+b
in in in in
for all input data u. This would render the random feature maps a linear model which are known to be
incapable of modelling nonlinear chaotic dynamical systems [10, 4]. On the other extreme, for sufficiently
large internal weights a tanh-activation function saturates, and one obtains ϕ ≈ ±1 independent of the
inputdatau, severelydecreasingtheexpressivityoftherandomfeaturemap. Thissuggeststhatoneshould
chooseinternalweightswhichsamplethetanh-activationfunctioninitsnonlinearnon-saturatedrange. This
is illustrated in Figure 2. We shall call features linear, if for all data u the argument of the tanh-activation
4function lies within the interval centred around the origin in [−L ,L ]. Those features obtained by the
0 0
tanh-activation function that are approximately ±1 for all input data u, i.e. where the arguments of the
tanh-activationfunctionlieineitheroftwounboundedsets(−∞,−L ],[L ,+∞),welabelsaturated features.
1 1
Those features which for all input data are neither linear nor saturated, i.e. for which the argument of the
tanh-activationfunctionliesineitherofthetwointervals(−L ,−L )or(L ,L ), arelabelledgood features.
1 0 0 1
We use L =0.4 and L =3.5 to define good, linear and saturated features throughout this paper.
0 1
Figure 2. Domain and range of features produced by a tanh-activation function with
L =0.4 and L =3.5, leading to linear, saturated and good features respectively.
0 1
To illustrate the detrimental effect of saturated features on the forecasting skill of random features we
select from the random feature maps which were sampled in Figure 1, those if they fall into two groups,
those that lead to particularly large forecast times τ > 8 and those that lead to particularly low forecast
f
times τ < 0.5. For each of those feature vectors we determine the average fraction F of how much of the
f s
data input u is mapped to the saturated values ±1, by probing for 800 randomly selected data points u .
n
For each group we randomly select 500 samples from the 90,000 random feature maps used in Figure 1.
Figure 3 shows that the histograms of F for these two groups are clearly distinct. The group with low
s
forecast times τ < 0.5 has a significantly higher probability of having more saturated features compared
f
to the group with large forecast times τ > 8. The pronounced peak at F = 0 is a sampling effect: when
f s
sampling uniformly from the grid (0,w )×(0,b ) with w = 0.4 and b = 4.0, it is much more
max max max max
likely to draw parameters which correspond to non-saturated features. Such random feature map samples
are much more likely to have higher forecast times and hence are concentrated entirely in the τ >8 group.
f
In the following Section we develop a computationally cheap algorithm to sample from the set of good
weights and show in Section 5 how this increases the forecasting skill of random feature surrogate maps.
4. How to sample good internal weights
We would like our random feature maps to produce good features ϕ(u) = W u+b by restricting
in in
(W ,b ) to be neither linear nor saturated for all training data u . To that end, we select (W ,b ) such
in in n in in
that
L <|W u +b |<L , ∀n=1,2,...N. (12)
0 in n in 1
The lower bound L controls the linear features and the upper bound L controls the saturated features (cf.
0 1
Figure2). Notethat(12)isavectorinequalityandisequivalenttoD scalarinequalities. Denotingthei-th
r
row of W with win and the i-th entry of b with bin, for each i∈{1,2,...D } we require
in i in i r
L <|win·u +bin|<L , ∀n=1,2,...N. (13)
0 i n i 1
5Figure 3. Empirical histograms of average fraction of saturated features F for random
s
feature maps corresponding to large forecast times with τ > 8 and to low forecast times
f
with τ < 0.5. Each group has 500 samples and the histograms depict the probability of
f
having a certain value of F in each group.
s
Definition 4.1. We call the i-th row (win,bin) of the internal parameters (W ,b ) good if it satisfies (13).
i i in in
Similarly, we call (win,bin) linear if
i i
|win·u +bin|≤L , ∀n=1,2,...N, (14)
i n i 0
and we call (win,bin) saturated if
i i
|win·u +bin|≥L , ∀n=1,2,...N. (15)
i n i 1
For a streamlined discussion we call the i-th column of the outer weight matrix W, good if the associated
i-th row of the matrix of internal weights (W ,b ) is good and so on.
in in
This categorization of rows of the internal parameters is useful for investigating the effects of different
realizations of the random feature map on its forecasting skill. Note that this is not an exhaustive classifica-
tion since there exist rows that satisfy different inequalities for different observations u and do not satisfy
n
(13) for the whole data set U. Although not considered here, such mixed rows may be an interesting topic
for further exploration.
We denote the set of good internal weights satisfying (12) by Ω . The solution set Ω is not convex, but
g g
can be written as the disjoint union of two convex sets with
Ω =S ∪S , (16)
g − +
where
S = {(w,b)∈RD+1 :−L <w·u +b<−L ∀n=1,2,...,N}, (17)
− 1 n 0
S = {(w,b)∈RD+1 :+L <w·u +b<+L ∀n=1,2,...,N}. (18)
+ 0 n 1
6Since the convex subsets are reflections of each other with
S =−S , (19)
− +
it suffices to sample from only one of these convex sets and then uniformly sample the sign of the internal
weights to sample from Ω . Hence, the sampling problem is effectively a convex problem. Analogously, we
g
defineΩ andΩ tobethesolutionsetstotheproblems(14)and(15)respectively,andagainsamplingthese
l s
sets are also convex problems.
We present in the next two subsections algorithms to effectively sample from the sets Ω . A naive
g,l,s
choice of sampling algorithm would be to uniformly sample from the D-dimensional hypercube with the 2D
corners defined by the observed extremal training data points, and checking the inequality (12), if we want
to sample from Ω , let’s say. This, however, is computationally very costly as typically the solution set only
g
occupies a small region within that hypercube. Instead, we begin with a hit-and-run algorithm sampling
from Ω in Section 4.1 and then present a faster more efficient hit-and-run algorithm to sample from an
g
equivalent restricted solution set in Section 4.2.
4.1. Standard hit-and-run sampling of good internal parameters. We now describe a computation-
ally cheap and easy to implement numerical algorithm to uniformly sample from the solution sets Ω .
g,l,s
We shall employ hit-and-run algorithms [40, 43]. To uniformly sample a set Ω with hit-and-run, one starts
from a feasible point inside the set, considers the line through that point in a randomly chosen direction,
and then randomly picks a point on the intersection of that line and the set Ω as a new point. This process
is then repeated to generate further samples. For convex sets the hit-and-run samples converge to uniform
samples in total variation distance. The convergence depends polynomially on the number of iterations and
dimension with the polynomial dependence on dimension being of low order [26, 2, 27]. This and the fast
mixing properties make hit-and-run algorithms an attractive method to uniformly sample from Ω .
g,l,s
We sample the augmented internal weight matrix (W ,b ) row by row. Each sample lies then in a
in in
D+1-dimensionalsearchspacefor (win,bin). Dueto(19) itsufficesto samplefrom S . Inorderto perform
i i +
hit-and-run,givenapoint,weneedtoefficientlydetermineifapointliesinS . Focusingonaconvexconical
+
subset of S , it turns out that we can determine if a point belongs to S by checking just two inequalities.
+ +
Define the convex cone
V(s,b)={(w,b):sgn(w )∈{s ,0} ∀i=1,2,...,D}, (20)
i i
wheresisaD-dimensionalsignvectorwithentries±1labellingthe2D cornersofaD-dimensionalhypercube.
To control the range of the training data set, we further define the vectors x (s)∈RD as
∓

min u , if s =1
 n,i i
x (s)= 1≤n≤N
−,i
 max u n,i, otherwise
1≤n≤N
(21)

max u , if s =1
 n,i i
x (s)= 1≤n≤N
+,i
 min u n,i, otherwise,
1≤n≤N
where u is the i-th entry of the n-th training data point. Now for (w,b)∈V(s,b) we have,
n,i
max (w·u +b)≤w·x (s)+b,
n +
1≤n≤N
and (22)
min (w·u +b)≥w·x (s)+b.
n −
1≤n≤N
Therefore, for (w,b)∈V(sgn(w),b), we have (w,b)∈S if
+
w·x (sgn(w))+b>L ,
− 0
and (23)
w·x (sgn(w))+b<L
+ 1
The feasibility inequalities (23) simply check if the internal weights (w,b) map the training data into the
smallest D-dimensional hypercube that contains the training data.
7To initialize the hit-and-run algorithm with a feasible point we choose (w,b) = (0,b ) ∈ S for b ∈
0 + 0
(L ,L ). To determine the line segments inside S we use bisection together with the feasibility criterion
0 1 +
(23). The hit-and-run algorithm requires a few iterations to ensure that the samples become independent of
the initial feasible weight point (w,b)=(0,b ).
0
We summarize this hit-and-run algorithm for randomly generating uniform samples from Ω in Algo-
g
rithm 1.
Algorithm 1 Standard hit-and-run sampling for a good row
1: Input: training data U.
2: Choose number of decorrelation iterations K ∈ N and L 0,L 1 ∈ R >0. Below π denotes the canonical
projection: π(w,b)=w.
3: Sample b uniformly from (L 0,L 1).
4: w←0.
5: k ←0.
6: while k <K do
7: Randomly select a unit vector d∈RD+1.
8: A←{a∈R:(w+aπ(d))·x −(sgn(w+aπ(d)))+b+ad D+1 >L 0∧(w+aπ(d))·x +(sgn(w+aπ(d)))+
b+ad <L }.
D+1 1
9: a 0 ←infA.
10: a 1 ←supA.
11: Sample a uniformly from (a 0,a 1).
12: (w,b)←(w,b)+ad.
13: k ←k+1.
14: end while
15: Uniformly sample a scalar z from {−1,1} to determine which set to sample from, S − or S +.
16: if z =1 then
17: (w,b) is our final good row sample.
18: else
19: (−w,−b) is our final good row sample.
20: end if
4.2. One-shothit-and-runsampling. Wenowpresentareducedhit-and-runalgorithmwhichoperateson
asmallerD-dimensionalsearchspaceanddoesnotrequirecomputationallycostlybisection. Thisalgorithm,
which we will coin one-shot hit-and-run algorithm, produces independent samples without the need for
sufficiently many iterations to ensure decorrelation from the fixed initial feasible point.
To generate good (or linear or saturated) random feature maps one can restrict the solution spaces
Ω by first sampling b appropriately and then sampling w on a D-dimensional search space. For ease of
g,l,s
presentation we describe the algorithm for sampling from the good set Ω . We sample b uniformly from the
g
interval (L ,L ). The weights w are then sampled from the restricted solution set ΩR =SR∪SR with
0 1 g + −
SR ={(w,b)∈S :−L <b<−L }, (24)
− − 1 0
SR ={(w,b)∈S :+L <b<+L }. (25)
+ + 0 1
Since SR =−SR, sampling from the nonconvex set ΩR can again be done by sampling from the convex set
− + g
SR andthenmultiplyingthesamplewith1or−1uniformlyrandomly. Thisrestrictionallowsustoperform
+
hit-and-run sampling on a D-dimensional random convex set instead of a (D+1)-dimensional convex set.
Note that fixing b is akin to shrinking the search space from S to πSR where π is the canonical projection
+ +
with π(w,b)=w. Note that we can partition πSR according to
+
(cid:71)
πSR = (πSR∩V(s)), (26)
+ +
s∈{−1,1}D
(cid:70)
whereweuse todenotealmostdisjointunion,andwhereV(s)=πV(s,b)areD-dimensionalorthants. Let
us randomly select a sign vector s ∈ {−1,1}D or equivalently pick the random convex subset πSR∩V(s).
+
8Randomly choosing the sign vector or the corresponding convex subset is tantamount to assigning signs
randomly to the entries of w. In order to uniformly sample this conical subset we can a pick a random
direction d in the cone V(s), determine the maximal line segment starting at the origin parallel to d that
is contained in πSR∩V(s) and uniformly sample a point on this line segment. Figure 4 shows a schematic
+
for this one-shot hit-and-run algorithm. Since x (sgn(w)) is constant for all w ∈V(s), we can analytically
±
Figure 4. Schematicoftheone-shothit-and-runalgorithm2. Theweightpoint0isalways
an interior point of πSR and the cone V(s) is a D-dimensional orthant. The set πSR is
+ +
drawn as bounded here, but it can be unbounded depending on the training data u .
n
determine the maximal line segment without having to resort to bisection. Moreover, the special structure
of the cone lets us sample with a single iteration unlike the standard hit-and-run algorithm 1. Thus the
computation of the line segment in the solution set and the final sampling both happen in one shot and
therefore the one-shot hit-and-run is much faster than its standard counterpart given by Algorithm 1.
Algorithm 2 summarizes the one-shot hit-and-run sampling of a good row described above. Note that,
depending on the training data U it is possible for πSR to be unbounded which is why +∞ appears in
+
the algorithm. We can extend the notion of restriction to the coordinates of w as well by restricting the
intervals where we are allowed to sample them from which is akin to regularizing parameters in machine
learning [19, 18, 13] but we do not consider such algorithms here. Obvious modifications of Algorithm 1
and Algorithm 2 let us sample linear and saturated rows which we refrain from describing here to avoid
repetition.
4.3. Performanceofthehit-and-runsampling. Thetwohit-and-runalgorithms1and2aredesignedto
uniformlysamplefromthesetofgoodrows(w,b). Theresultingdistributionsfortheweightsandbiasesare
shown in Figure 5. For the standard hit-and-run algorithm 1 it was found that K = 10 decorrelation steps
were sufficient and results were very similar for K =100 iterations. It is clearly seen that the distributions
arefarfrombeingtheusuallyemployeduniformorGaussiandistribution. Thedistributionsareverysimilar
for both algorithms. In particular, the standard Algorithm 1 exhibits the same lack of biases with small
absolute value, as the one-shot hit-and-run Algorithm 2.
9Algorithm 2 One-shot hit-and-run sampling for a good row
1: Input: training data U.
2: Choose L 0,L 1 ∈R >0.
3: Sample b uniformly from (L 0,L 1).
4: Select the sign vector s by uniformly generating D samples from {−1,1}.
5: Randomly select a unit vector d∈V(s).
6: a 0 ←0.
(cid:16)(cid:110) (cid:111) (cid:17)
7: a 1 ←inf dL ·x0 −− (b s), dL ·x1 +− (b s) ∩(R >0∪{+∞}) with the convention inf∅=+∞.
8: Sample a uniformly from (a 0,a 1).
9: Uniformly sample a scalar z from {−1,1} to determine which set to sample from, S − or S +.
10: if z =1 then
11: (ad,b) is our final good row sample.
12: else
13: (−ad,−b) is our final good row sample.
14: end if
Figure 5. Empirical histograms for samples generated using standard and one-shot hit-
and-runalgorithms1and2,respectively. Theleftpanelshowsthedistributionsoftheentries
of W and the right panel shows the distributions of the entries of b . For each algorithm
in in
500 rows of internal parameters were generated. Algorithm 1 used K = 10 decorrelation
iterations.
Whereas the one-shot hit-and-run algorithm excludes biases with absolute values smaller than L by
0
design, this may seem surprising for the standard hit-and-run algorithm. This can be explained as follows.
For 0 < b < L and (w,b) ∈ S we require that w·u lies in the positive interval (L −b,L −b) for all
0 + 0 1
trainingdatau. Sincethedirectionsofthevectorsuinthetrainingdataaretypicallydistributedoversome
range, w·u is typically not sign-definite for all data points u. This implies that for all parameters in Ω we
g
typicallyhave|b|>L ; asimilarargumentshowsthattypically|b|<L . Hence,fortypicaldatauΩR =Ω
0 1 g g
andthesearchspaceoftheone-shothit-and-runalgorithm2isthesameasthatofthestandardalgorithm1.
We have tested that the forecasting skill is the same for both sampling algorithms.
FortheweightsandbiaseswhichwereobtainedbysamplinguniformlyfromanintervalasinFigure1,we
checkedthattheweightscorrespondingtohighforecastingskillindeedallsatisfyourcriterionofbeinggood
rows (12). This highlights the advantage of our non-parametric sampling over sampling strategies involving
asetofparametrizeddistributions. Inapplicationswehenceusethecomputationallymoreefficientone-shot
hit-and-run algorithm 2.
10Thehit-and-runalgorithms1and2weredesignedtouniformlysamplefromthesetsΩ . Thisdoesnot
g,l,s
imply, however, that w u+b is uniformly distributed in the interval (−L ,−L )∪(L ,L ). To quantify
in in 1 0 0 1
the occupied range of random features we introduce the following notation. A sample (win,bin) produces
i i
outputs the absolute values of which lie in the interval [m ,M ], i.e.
i i
m = min |win·u +bin|,
i i n i
1≤n≤N
(27)
M = max |win·u +bin|.
i i n i
1≤n≤N
The effective range R of a random feature map vector can then be defined as
1
(cid:88)Dr
R= (M −m ). (28)
D i i
r
i=1
Figure 6 shows that the features only occupy a relatively small part of the desired interval and the observed
maximum value of the effective range R ≈ 0.52 is much smaller than the desired range of L −L = 3.1.
1 0
ThisreductionoftherangeRcanbeexplainedbyasimpleapproximatemodel. Considerthecasewhenthe
extremevaluesm andM arei.i.drandomvariables,andM isdrawnuniformlyaccordingtoM ∼U[L ,L ].
i i i i 0 1
The lower bound m is then conditionally distributed according to m ∼U[L ,M ].
i i 0 i
Bythecentrallimittheorem,forD ≫1theeffectiverange(28)isanormallydistributedrandomvariable
r
with mean
(cid:20) (cid:21)
L +M
E[R]≈E[M ]−E[m |M ]= E[M ]−E 0 i
i i i i 2
L −L
= 1 0. (29)
4
Hence the range is approximately E[R]=(L −L )/4 which is significantly reduced from the desired range
1 0
L −L . Similarly, by the central limit theorem, the standard deviation of the effective range converges to
1 0
σ[M −m ]
σ[R]≈ √i i , (30)
D
r
whichimpliesaconcentrationoftheeffectiverangeRaroundthemean(29)forhighfeaturedimensionsD ,
r
consistent with the observations in Figure 6.
OnewouldliketherangeRtobeaslargeaspossible. Indeed,Figure7showsthatRandτ arepositively
f
correlated justifying our assumption that better exploration of the space of good features generally yields
betterforecastingskill. TheobservationofasmalleffectiverangeRsuggeststhattuningL andL tomake
0 1
the interval (L ,L ) larger may be beneficial for the forecasting skill of the random feature map.
0 1
5. Results
In this section we explore how increasing the number of good features improves the forecasting skill of
a surrogate map for the Lorenz-63 system (4), and conversely explore the effect of linear and saturated
features. To do so we define the number of good, linear and saturated features in a random feature vector
of dimension D as
r
N =p D , N =p D , N =p D , (31)
g g r l l r s s r
where the respective fractions satisfy p +p +p = 1. We construct random feature maps with internal
g l s
weights (W ,b ) with specified fractions of good, linear or saturated rows using the one-shot hit-and-run
in in
algorithm 2.
5.1. Effect of the quality of internal weights on the forecast time τ . In this section we investigate
f
how the forecasting skill of a random feature surrogate model (3) improves with increasing the number of
good rows N . We would like to have internal parameters resulting in large mean forecast times τ with
g f
relatively small standard deviations. For chaotic dynamical systems we expect a residual variance of the
forecast time due to the sensitivity to small changes in the model: small changes in the internal parameters
may cause the surrogate models to deviate from each other after some time.
11Figure 6. Empirical histogram of the effective range R of random features with only
good rows for L = 0.4 and L = 3.5. We used 500 independent samples and a feature
0 1
dimension of D =300. The mean is estimated as E[R]=0.44 and the standard deviation
r
as σ[R]=0.03.
Figure 7. Scatter plot of the effective range R and the forecast time τ when only good
f
rowsareusedwithp =1. Eachdotrepresentsadifferentrealizationoftherandomfeature
g
map which was trained and tested on randomized data. We observe a Pearson correlation
coefficientof0.37betweenτ andR. ThefeaturedimensionisD =300andaregularization
f r
parameter of β =4×10−5 is used with training data length N =20,000.
We estimate the mean of the forecast time τ and its coefficient of variation as a function of the fraction
f
ofgoodfeaturesp ,varyingp fromp =0withonlybadfeaturestop =1withonlygoodfeaturespresent.
g g g g
Foreachvalueofp weapproximatelyuniformlydistributetheremaining(1−p )D featuresoverthelinear
g g r
and saturated features with p ≈p ≈(1−p )/2. Note that we cannot always impose perfect equality since
l s g
N = p D , N = p D and N = p D are integers. We use 51 equally spaced values of p in [0,1] and
g g r l l r s s r g
compute averages over 500 realizations for each value of p , differing in the draws of the random internal
g
weights, the training data and the validation data.
12Figure 8 shows the dependence of the mean forecast time E[τ ] and the associated coefficient of variation
f
σ[τ ]/E[τ ] on p for various values of the feature dimensions D and training data lengths N. It is clearly
f f g r
seen that increasing the number of good rows increases the mean forecast time and decreases the coefficient
of variation as desired. As expected, for fixed feature dimension D increasing the training data length N is
r
beneficial. Similarly, forfixedtrainingdatalengthN, increasingthefeaturedimensionD isbeneficial. The
r
observation that, for fixed data length N, the mean forecast time E[τ ] saturates upon increasing p once
f g
a sufficiently large number of good features N = p D are present, suggests that the distribution of the
g g r
forecasttimeτ convergesreflectingaresidualuncertaintyofthechaoticsurrogatemodel. Thisisconfirmed
f
in Figure 9 where we see convergence of the empirical histograms of the forecast time for increasing values
of D in the case when p =1.
r g
Figure 10 shows the dependency of the mean forecast time E[τ ] on the fraction of good rows p for
f g
different values of D . We can clearly see that beyond N = p D = 256 (indicated by the vertical line),
r g g r
the mean forecast time E[τ ] depends only on the number of good rows N = p D and not on the overall
f g g r
feature dimension D . For smaller number of good rows N < 256 the mean forecast time depends on the
r g
featuredimensionD withlargerfeaturedimensionsimplyinglargermeanforecasttimes. Thissuggeststhat
r
the number of good features N constitutes an effective feature dimension D∗, which controls the forecast
g r
skill of the learned surrogate model. This implies that on average the forecast time τ is the same for a
f
random feature surrogate model of dimension D with only good features p = 1 as a surrogate map with
r g
a larger feature dimension αD with α > 1 but only a fraction of 1/α good rows. This is confirmed in
r
Figure 11 which shows the empirical histogram of τ for fixed number of good features N =p D =1,024.
f g g r
We compare the distribution of the forecast times for random feature maps with D =1,024 and p =1 to
r g
those with D =2,048 and p =0.5. We show examples when the remaining bad features are either equally
r g
distributed between linear and saturated features, or only linear or only saturated. The distributions for
all three examples are very similar and match the one with the smaller feature dimension but same number
of good features. This leads us to conclude that the number of good rows is the only determining factor
for the distribution of τ (all other parameters being equal), and that linear and saturated rows are equally
f
ineffective in terms of the forecasting skill.
We briefly discuss the effect of the regularization parameter β on the forecasting skill. We show in
Figure12themeanforecasttimeE[τ ]andcoefficientofvariationσ[τ ]/E[τ ]asafunctionofp forarange
f f f g
of regularization parameters β ∈ [2−25,2−13]. For fixed feature dimension D = 300, we see that β = 2−19
r
is optimal within this range in terms of the mean forecast time (left panel) and the coefficient of variation
(right panel) once sufficiently many good features are present with p >0.33. Note that we had previously
g
employed β =4×10−5 ≈2−14.6.
5.2. Effect of the quality of internal weights on the outer weights W. In this section we explore
how the nature of the internal weights affects the learned solutions of the ridge regression (11) which we
denote simply as W, dropping the star.
WebeginbyrecordingtheFrobeniusnorm∥W∥ofthelearnedouterweightsasafunctionofthefractionof
good features p (bad features are again roughly equally distributed between linear and saturated features).
g
Figure 13 shows the mean of ∥W∥ as a function of p on a log-log scale for the simulations used in Figure 8.
g
It is seen that ∥W∥ is a decreasing function of the number of good features. The solution of the linear
regression problem W minimizes the loss function (10). Once there are sufficiently many good features, the
trainingdatacanbesufficientlywellfit,decreasingthefirsttermofthelossfunction. Increasingthenumber
of good features further then allows to decrease the second regularizing term of the loss function, leading
to a decrease of ∥W∥. Assuming that the true one-step map Ψ in (2) lies in the domain of the random
∆t
feature map (3) with infinitely many features, the first term of the loss function should scale with the usual
√
Monte-Carlo estimate scaling of O(1/D ), suggesting a scaling of the regularization term ∥W∥ ∼ 1/ D .
r r
In the right panel of Figure 13 we show that the mean of ∥W∥ roughly scales as ∥W∥ ∼ 1/D0.54 when all
r
the internal weights correspond to good features with p =1, suggesting that the true one-step map can be
g
well approximated by random features with a tanh-activation function. We remark that the Monte-Carlo
scaling is valid for D >256 only, i.e. provided sufficiently many good features are present.
r
13Figure 8. The top row depicts the mean of the forecast time E[τ ] as a function of the
f
fraction of good features p . The bottom row depicts the coefficient of variation σ[τ ]/E[τ ]
g f f
asafunctionofp . AlongthefirstcolumnthefeaturedimensionD =300iskeptconstant,
g r
andalongthesecondcolumnthelengthofthetrainingdatasetN =20,000iskeptconstant.
Expectationarecomputedover500realizationsoftheinternalparameters,thetrainingdata
and testing data. A regularization parameter of β =4×10−5 is employed.
We now investigate how the decrease in the outer weights W is distributed over the various features.
We will see that the outer weights are learned to suppress the bad features provided there are sufficiently
many good features allowing for a reduction of the loss function. Let us denote the i-th column of W by
Wi. The columns Wi are the weights attributed to the features produced by the i-th row of the internal
weights (w ,b ). We expect the outer weights corresponding to good rows to be significantly larger than
i i
those corresponding to bad rows. We label columns of W that have only small entries with absolute value
smaller than a threshold δ =1 by N0.
TostudythesuppressionofbadfeaturesbysuchsmallcolumnsofthelearnedouterweightsW,wedesign
two sets of numerical experiments: one in which bad features are entirely comprised of linear features and
one in which bad features are entirely comprised of saturated features.
In the first set we initialize a random feature map with D = 300 features consisting of only bad linear
r
features. We then successively replace one linear feature by a good feature, i.e. replacing one inner linear
weight row(win,bin)byagoodrow. Ateach stepwe recordthecorrespondinglinear regressionsolution W.
i i
Figure 14 shows the normalized supremum norm of columns of W after N = 10, N = 50 and N = 150
g g g
14Figure 9. Empirical histogram of τ for different values of D when p =1 for increasing
f r g
feature dimension D . The same 500 realizations are used as in Figure 8 with N =20,000.
r
z
Figure 10. Forecast time mean E[τ ] as a function of good features N = p D . The
f g g r
verticallinedemarcatesN =256. TherangeofN isrestrictedtoN ≤512,corresponding
g g g
top =1forthesmallestvalueofthefeaturedimensionD =512. Thesame500realizations
g r
are used as in Figure 8 with N =20,000.
bad linear features have been replaced by good features. The red dots signify small columns which do not
containanyentrywithabsolutevaluelargerthanδ =1. Itisclearlyseenthatlinearfeaturesaresuppressed
by the columns of W. Note that not all linear features are entirely suppressed.
Inthesecondexperimentwefollowthesameprocedureasbeforeexceptwestartwithonlysaturatedrandom
features. In Figure 15 it is seen that saturated features are suppressed even stronger by the outer weights
than linear features. In contrast to linear features, saturated features are effectively fully suppressed once
the number of good features exceeds N =50.
g
15Figure 11. Empirical histogram of the forecast time τ for D = 1,024 and D = 2,048.
f r r
In each case the number of good rows is N = 1,024. For D = 2,048 we show results
g r
for an equal number of linear and saturated features with p = p = 0.25 (left), for only
l s
linear bad features with p =0.5,p =0 (middle) and for only saturated bad features with
l s
p = 0.5,p = 0 (right) for D = 2,048. We used 500 realizations differing in the random
s l r
draws of the internal parameters, the training data and the validation data. We employed
a regularization parameter of β =4×10−5 and used training data of length N =20,000.
Figure 12. Mean forecast time E[τ ] (left) and coefficient of variation σ[τ ]/E[τ ] (right)
f f f
as a function of p for a range of regularization parameters β.A regularization parameter
g
of β = 2−19 is optimal among the values presented here for p > 0.33 (demarcated by a
g
vertical line). Results are shown for fixed D =300 and N =20,000.
r
6. Comparison with a single-layer feedforward network trained with gradient descent
A natural question is if a single-layer feedforward network of the architecture (6) for which the internal
weights (W ,b ) are learned together with the outer weights W performs better or worse than random
in in
feature maps with fixed good internal weights. In particular, we consider the non-convex optimization
16Figure13. Left: ThemeanoftheFrobeniusnormoftheouterweights,∥W∥,asafunction
ofp onalog-logscale. Right: ThemeanoftheFrobeniusnormoftheouterweights,∥W∥,
g
as a function of the feature dimension D for p = 1. Shown is also a line of best fit with
r g
approximate slope −0.54. Expectations were taken over 500 realizations.
Figure 14. Normalized supremum norm of the columns of W for different numbers of
good features with N = 10, N = 50 and N = 150 and otherwise exclusively linear bad
g g g
features. The x-axis represents column indices. The good and linear columns are indicated
in blue and orange, respectively. The red dots signify columns with supremum norm less
than δ =1. The overall feature dimension is D =300 and the outer weights were obtained
r
from training data of length N =20,000.
17Figure 15. Normalized supremum norm of the columns of W for different numbers of
good features with N = 10, N = 50 and N = 150 and otherwise exclusively saturated
g g g
bad features. The x-axis represents column indices. The good and saturated columns are
indicated in blue and green, respectively. The red dots signify columns with supremum
normlessthanδ =1. TheoverallfeaturedimensionisD =300andtheouterweightswere
r
obtained from training data of length N =20,000.
problem
Θ= argmin L(U), (32)
Win,bin,W
with Θ = (W ,b ,W) and the loss function L defined in (10). To solve the optimization problem (32)
in in
we employ gradient descent. In order to fairly compare with the results from the random feature model,
we fix the width of the internal layer to D = 300 and employ a regularization parameter of β = 4×10−5.
r
We use training data of length N = 20,000. To initialize the network weights we use the standard Glorot
initialization [11]. We use an adaptive learning rate scheduler which is described in Appendix 8.1.
Figure 16 shows the evolution of the mean forecast time E[τ ] and the logarithm of the loss function L
f
during training. The expectation is computed over 500 different validation data sets. Optimization over
all weights clearly allows for a significantly smaller training loss L compared to random feature maps (cf.
Figure 17). The neural network achieves a final value of the loss function of L ≈ 0.09 which is a 95%
improvementwhen compared to a random feature map ofthe same size with onlygood internal parameters,
i.e. p = 1, which has a loss of L ≈ 1.73 on average. However, the situation is very different for the
g
mean forecast time. The mean forecast time E[τ ] is a slowly growing function of the gradient descent steps
f
with the last 105 steps resulting in only about 0.32% improvement. The data are plotted every 104 steps
and therefore the typical fluctuations of gradient descent are not visible. Maybe surprisingly, optimizing
the internal weights via gradient descent does not lead to a better forecasting skill when compared to the
random feature map surrogate model. After 1.5×106 steps the neural network achieves a mean forecast
time of only E[τ ]≈3.75. Random feature maps of the same size with p =1 generate a mean forecast time
f g
of E[τ ] ≈ 4.46 (cf. Figure 8). Furthermore, the training took approximately 8.2×104 seconds on the T4
f
18GPUavailablethroughGoogleColabcloudplatform. Incontrast, initializingandtrainingarandomfeature
map of the same size took less than 1 second in total, i.e almost 100,000 times faster.
Figure16. EvolutionofthemeanforecasttimeE[τ ]andthelogarithmofthelossfunction
f
(10)log(L)duringtrainingofa single-layerfeedforward networkwithgradient descent. For
each step E[τ ] is computed using 500 test trajectories. The network with width D =300
f r
was trained with training data of length N = 20,000 and a regularization parameter β =
4×10−5. Results are shown every 104 gradient descent steps.
Figure 17. Mean loss L for random feature maps as a function of p for different values
g
of the feature dimension D . The data shown here correspond to the experiments shown in
r
Figure 8 with N =20,000 and β =4×10−5.
TheleftpanelofFigure18showsthatthemeanforecasttimeE[τ ]andthelogarithmofthelossfunction
f
log(L) are linearly related. This is a direct manifestation of the exponential sensitivity in chaotic dynamical
systems: in each gradient descent step the loss experiences small changes leading to small changes in the
learned weights and hence in the resulting surrogate model. These small changes in the chaotic surrogate
model lead to an exponential divergence of nearby trajectories. This causes an exponential in time loss of
19predictability, characterized here by the mean of the forecast time (5). The same sensitive dependency on
Figure 18. Left and right panels show the relationship between E[τ ] and the logarithm
f
of the loss function L for a single-layer feedforward network and for a random feature map
with only good internal parameters, i.e. p = 1, respectively. Each dot in the left panel
g
corresponds to a gradient descent step. Each dot in the right panel corresponds to one
realization of a random feature map. The expectation is computed over 500 validation
trajectories. Each descent step and each realization use the same training and testing data.
The black line in the left panel represents the best-fit line with slope −0.79. In the right
paneltheredcrossesdenotetheconditionalmeanE[τ |log(L)]andtheblacklinerepresents
f
the best-fit line with slope −1.01. We use a feature dimension of D = 300, training data
r
length N =20,000 and regularization parameter β =4×10−5 .
small changes of the surrogate model, quantified by small changes of the loss function, is also present in
random feature maps. The right panel of Figure 18 shows the mean forecast time E[τ ] as a function of the
f
logarithm of the loss function for random feature maps. Each dot represents one realization of a random
feature map with feature dimension D = 300, trained on the same data as the single-layer feedforward
r
network. The mean forecast time E[τ ] is computed using the same 500 validation trajectories as the
f
network. Averaged over bins of the logarithm of the loss function, the mean forecast shows the same linear
relationship with respect to the logarithm of the loss function (red crosses in Figure 18). The slopes of the
best-fit lines in Figure 18 show that the forecasting skill of the random feature map improves slightly faster
with decreasing loss when compared to the network.
The discrepancy between the neural network having worse forecasting skill compared to random feature
maps despite achieving smaller loss can be explained as follows. Minimizing the loss function L aims at
learning the single-step surrogate map (6). High forecasting skill, however, requires multiple applications of
the single-step surrogate model which is not explicitly accounted for in the loss function (10). In Section 3.1
we established that the main controlling factor for achieving high forecasting skill is the number of good
features. In random feature maps we can control and maximize this number simply by sampling good
parameters according to our hit-and-run algorithms 1 and 2, respectively. On the other hand, the training
of the single-layer feedforward network is only designed to minimize the loss but not to unable to maximize
thenumberofgoodfeaturestoN =D =300inourcase. Figure19showsthenumberofdifferenttypesof
g r
rows produced during the training instance of Figure 16. We see that only a single good row was produced
in (W ,b ) during the early steps of the optimization, and this good row was then quickly destroyed
in in
during the training process. We checked that even when the network is initialized with only good internal
parameters, i.e. p = 1, training eventually leads to a complete absence of good internal parameters with
g
p = 0 for reasonable learning rates. To understand the absence of good rows in the trained network, note
g
20that for any random feature map Θ = (W ,b ,W) essentially lies on the graph of a continuous function
in in
due to the intricate relationship between the internal and outer weights dictated by (11). So the set of all
possibleΘforrandomfeaturemapshaszeroLebesguemeasureinRDr×(2D+1). Itisthereforehighlyunlikely
that gradient descent finds the lower-dimensional subset of the random feature map weights in its search
space which is the full RDr×(2D+1). It would be interesting to see if the network generates good features
if the loss function is augmented by a penalty term promoting good features. In any case, random feature
maps are significantly cheaper to train.
Figure 19. Evolution of the number (normalized by D ) of learned good, linear and satu-
r
rated rows (win,bin) in the internal parameters during a single training episode of a single-
i i
layer feedforward network. The same neural network is used as in Figure 16. Results are
shown every 104 gradient descent steps.
7. Summary and future work
We established the notion of good features and good internal parameters for random feature maps with
a tanh-activation function. These good internal weights are characterised by affinely mapping the training
data into the nonlinear, non-saturated domain of the tanh-activation function. We established that the
number of good features N ≤ D is the controlling factor in determining the forecasting skill of a learned
g r
surrogate map, rather than the feature dimension D . Interestingly, the forecasting skill was found to be
r
equally deteriorated by linear features as by non-saturated features. The non-convex set of good features
could be written as a union of two convex sets which are mutual reflections of each other. We developed
computationally cheap hit-and-run sampling algorithms to uniformly sample from the set of good internal
parameters. We demonstrated how ridge regression engages with a given number of good and bad features.
It was found that non-saturated features are eliminated almost entirely by the outer weights provided a
sufficient number of good features are present. Once there are sufficiently many good features present to
allow for a significant reduction of the data mismatch term of the loss function, regularization kicks in and
reduces the norm of the outer weights corresponding to good features.
We further showed that a single-layer feedforward network with the same width D trained with gradient
r
descent exhibits inferior forecasting skill compared to a random feature map surrogate map which used only
good internal parameters. The neural network achieves a significantly smaller value of the loss function.
Good forecasting skill, however, requires multiple applications of the surrogate map, and as we showed is
controlledbythenumberofgoodfeatures. Thelowerforecastingskillisduetotheoptimizationprocessnot
finding solutions on the measure-zero set of good parameters. Even when initialized with good parameters,
21the gradient descent quickly reduces the number of good internal parameters.
The proposed optimization-free algorithm to choose internal non-trainable parameters can potentially
lead to new design and computationally cheap training schemes for more complex network architectures.
Our algorithms may be used to further improve the forecasting skill of reservoir computing [35, 10]. The
distinctionintolinear,saturatedandgoodfeaturesreadilytranslatestoothersigmoidalactivationfunctions.
ItwillbeinterestingtotestiflargervaluesofL ,whichweshowedinSection4.3toleadtoalargereffective
0,1
range explored by features, may improve the forecast skill.
We considered here random feature maps as an alternative to single-layer feedforward networks. It will
be interesting to see if the ideas of selecting weights according to the domain of the tanh-activation function
canbeemployedtogeneratedeeprandomfeaturenetworkswhereateachlayerweightsandbiasesaredrawn
using our hit-and-run algorithm. This is planned for future research.
Acknowledgement
The authors would like to acknowledge support from the Australian Research Council under Grant No.
DP220100931. GAG would like to acknowledge discussions with Nicholas Cranch in the early stages of this
research.
8. Appendix
8.1. Adaptive learning rate for the single-layer neural network. We describe in Algorithm 3 the
adaptive learning rate algorithm we used when training the single-layer feedforward network in Section 6.
Essentially our scheduler computes the decay rate of the loss every I steps and modifies the learning rate by
increasing or decreasing it by a constant fraction ξ if necessary. We use an initial rate η = 10−3, update
0
intervalI =100,updatefractionξ =0.1,updatethresholdγ =−10−4 andnumberofgradientdescentsteps
E =1.5×106 in our scheduler.
Algorithm 3 Adaptive learning rate scheduler
1: Input: Chooseinitialrateη 0,updateintervalI,updatefractionξ,updatethresholdγ,numberofgradient
descent steps E.
2: k ←1 (gradient descent step).
3: L 0 ← value of L at gradient descent step k.
4: η ←η 0 (learning rate).
5: while k <E do
6: if k is divisible by I then
7: L 1 ← value of L at gradient descent step k.
8: ∆← L1−L0.
L0
9: if ∆>γ then
10: if ∆>0 then
11: η ←η(1−ξ)
12: else
13: η ←η(1+ξ)
14: end if
15: end if
16: L 0 ←L 1
17: end if
18: k ←k+1
19: end while
We tried several other strategies such as finding an optimal learning rate every few steps using bisection,
random modifications of the learning rate based on the behavior of the loss function, aggressive constant
learning rates and conservative constant learning rates, piecewise linear learning rates etc. We found that
22the simple strategy presented in Algorithm 3 leads to the lowest final value of the loss function for the
same number of gradient descent steps. Figure 20 shows the adaptive learning rate used during the training
instance shown in Figure 16.
Figure 20. Adaptive learning rate η used during the training instance presented in Fig-
ure 16. The associated evolution of the loss function is shown in Figure 16.
References
[1] H.D.Abarbanel,R.Brown,J.J.Sidorowich,andL.S.Tsimring,The analysisof observed chaotic datain physical
systems,ReviewsofModernPhysics,65(1993),p.1331.
[2] Y.Abbasi-Yadkori,P.Bartlett,V.Gabillon,andA.Malek,Hit-and-runforsamplingandplanninginnon-convex
spaces,inArtificialIntelligenceandStatistics,PMLR,2017,pp.888–895.
[3] A. R. Barron, Universal approximation bounds for superpositions of a sigmoidal function, IEEETransactions onInfor-
mationTheory,39(1993),pp.930–945.
[4] E.Bollt,Onexplainingthesurprisingsuccessofreservoircomputingforecasterofchaos? Theuniversalmachinelearning
dynamical system with contrast to VAR and DMD, Chaos: An Interdisciplinary Journal of Nonlinear Science, 31 (2021),
p.013108,https://doi.org/10.1063/5.0024890.
[5] S. L. Brunton, J. L. Proctor, and J. N. Kutz, Discovering governing equations from data by sparse identification
of nonlinear dynamical systems, Proceedings of the National Academy of Sciences, 113 (2016), pp. 3932–3937, https:
//doi.org/10.1073/pnas.1517384113.
[6] J.Cao,Z.Li,andJ.Li,FinancialtimeseriesforecastingmodelbasedonCEEMDANandLSTM,PhysicaA:Statistical
mechanicsanditsapplications,519(2019),pp.127–139.
[7] K.Chandrasekaran,D.Dadush,andS.Vempala,Thinpartitions: Isoperimetricinequalitiesandsamplingalgorithms
for some nonconvex families,arXivpreprintarXiv:0904.0583,(2009).
[8] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio,Empirical evaluation of gated recurrent neural networks on sequence
modeling,arXivpreprintarXiv:1412.3555,(2014).
[9] R. Fu, Z. Zhang, and L. Li, Using LSTM and GRU neural network methods for traffic flow prediction, in 2016 31st
YouthAcademicAnnualConferenceofChineseAssociationofAutomation(YAC),IEEE,2016,pp.324–328.
[10] D. Gauthier, E. Bollt, A. Griffith, and W. Barbosa, Next generation reservoir computing, 2021, https://arxiv.
org/abs/2106.07688.
[11] X.GlorotandY.Bengio,Understandingthedifficultyoftrainingdeepfeedforwardneuralnetworks,inProceedingsofthe
thirteenth international conference on artificial intelligence and statistics, JMLR Workshop and Conference Proceedings,
2010,pp.249–256.
[12] L.GononandJ.-P.Ortega,Fadingmemoryechostatenetworksareuniversal,NeuralNetworks,138(2021),pp.10–13.
[13] I. Goodfellow, Y. Bengio, and A. Courville,Deep learning,MITpress,2016.
[14] G. A. Gottwald and S. Reich,Combining machine learning and data assimilation to forecast dynamical systems from
noisy partial observations, Chaos: An Interdisciplinary Journal of Nonlinear Science, 31 (2021), p. 101103, https:
//doi.org/10.1063/5.0066080, https://doi.org/10.1063/5.0066080, https://arxiv.org/abs/https://pubs.aip.org/
aip/cha/article-pdf/doi/10.1063/5.0066080/19770487/101103_1_online.pdf.
23[15] G. A. Gottwald and S. Reich, Supervised learning from noisy observations: Combining machine-learning techniques
with data assimilation, Physica D: Nonlinear Phenomena, 423 (2021), p. 132911, https://doi.org/10.1016/j.physd.
2021.132911.
[16] L. Grigoryeva and J.-P. Ortega,Echo state networks are universal,NeuralNetworks,108(2018),pp.495–508.
[17] S.Kiatsupaibul,R.L.Smith,andZ.B.Zabinsky,Ananalysisofavariationofhit-and-runforuniformsamplingfrom
general regions,ACMTransactionsonModelingandComputerSimulation(TOMACS),21(2011),pp.1–11.
[18] A. Krogh and J. Hertz,A simple weight decay can improve generalization,AdvancesinNeuralInformationProcessing
Systems,4(1991).
[19] J. Kukacˇka, V. Golkov, and D. Cremers, Regularization for deep learning: A taxonomy, arXiv preprint
arXiv:1710.10686,(2017).
[20] A. Laddha and S. S. Vempala, Convergence of Gibbs sampling: Coordinate Hit-and-Run mixes fast, Discrete & Com-
putationalGeometry,70(2023),pp.406–425.
[21] Y. Li, J. Lu, and A. Mao, Variational training of neural network approximations of solution maps for physical models,
JournalofComputationalPhysics,409(2020),p.109338.
[22] Y.Li,Z.Zhu,D.Kong,H.Han,andY.Zhao,EA-LSTM:Evolutionaryattention-basedLSTMfortimeseriesprediction,
Knowledge-BasedSystems,181(2019),p.104785.
[23] F. Liu, X. Huang, Y. Chen, and J. A. Suykens, Random features for kernel approximation: A survey on algorithms,
theory, and beyond,IEEETransactionsonPatternAnalysisandMachineIntelligence,44(2021),pp.7128–7148.
[24] E. N. Lorenz,Deterministic nonperiodic flow,JournalofAtmosphericSciences,20(1963),pp.130–141.
[25] E. N. Lorenz,Predictability: A problem partly solved,inProc.SeminaronPredictability,vol.1,Reading,1996.
[26] L. Lova´sz,Hit-and-run mixes fast,MathematicalProgramming,86(1999),pp.443–461.
[27] L. Lova´sz and S. Vempala, Hit-and-run from a corner, in Proceedings of the thirty-sixth Annual ACM Symposium on
TheoryofComputing,2004,pp.310–314.
[28] L. Lova´sz and S. Vempala, The geometry of logconcave functions and sampling algorithms, Random Structures &
Algorithms,30(2007),pp.307–358.
[29] M. Lukoˇsevicˇius, A practical guide to applying echo state networks, in Neural Networks: Tricks of the Trade: Second
Edition,Springer,2012,pp.659–686.
[30] M.LukoˇsevicˇiusandH.Jaeger,Reservoircomputingapproachestorecurrentneuralnetworktraining,ComputerScience
Review,3(2009),pp.127–149.
[31] R. Meyer and N. Christensen, Bayesian reconstruction of chaotic dynamical systems, Physical Review E, 62 (2000),
p.3535.
[32] K. Nakajima and I. Fischer,Reservoir computing,Springer,2021.
[33] N.H.NelsenandA.M.Stuart,Therandomfeaturemodelforinput-outputmapsbetweenBanachspaces,SIAMJournal
onScientificComputing,43(2021),pp.A3212–A3243,https://doi.org/10.1137/20M133957X,https://doi.org/10.1137/
20M133957X,https://arxiv.org/abs/https://doi.org/10.1137/20M133957X.
[34] M. M. Oliver R. A. Dunbar, Nicholas H. Nelsen, Hyperparameter optimization for randomized algorithms: A case
study for random features,2024,https://arxiv.org/abs/2407.00584.
[35] J. Pathak, B. Hunt, M. Girvan, Z. Lu, and E. Ott, Model-free prediction of large spatiotemporally chaotic systems
from data: A reservoir computing approach,PhysicalReviewLetters,120(2018),p.024102.
[36] A.RahimiandB.Recht,Randomfeaturesforlarge-scalekernelmachines,inAdvancesinNeuralInformationProcessing
Systems 20, J. C. Platt, D. Koller, Y. Singer, and S. T. Roweis, eds., Curran Associates, Inc., 2008, pp. 1177–1184,
http://papers.nips.cc/paper/3182-random-features-for-large-scale-kernel-machines.pdf.
[37] A.RahimiandB.Recht,Uniformapproximationoffunctionswithrandombases,in200846thAnnualAllertonConfer-
enceonCommunication,Control,andComputing,2008,pp.555–561.
[38] A.RahimiandB.Recht,Uniformapproximationoffunctionswithrandombases,in200846thAnnualAllertonConfer-
enceonCommunication,Control,andComputing,IEEE,2008,pp.555–561.
[39] A.Sherstinsky,Fundamentalsofrecurrentneuralnetwork(RNN)andlongshort-termmemory(LSTM)network,Physica
D:NonlinearPhenomena,404(2020),p.132306.
[40] R.L.Smith,Efficientmontecarloproceduresforgeneratingpointsuniformlydistributedoverboundedregions,Operations
Research,32(1984),pp.1296–1308.
[41] R. C. Staudemeyer and E. R. Morris,Understanding LSTM–a tutorial into long short-term memory recurrent neural
networks,arXivpreprintarXiv:1909.09586,(2019).
[42] W. Tucker, A rigorous ODE solver and Smale’s 14th problem, Foundations of Computational Mathematics, 2 (2002),
pp.53–117.
[43] Z. B. Zabinsky, R. L. Smith, S. Gass, and M. Fu, Hit-and-run methods, Encyclopedia of Operations Research and
ManagementScience,(2013),pp.721–729.
24