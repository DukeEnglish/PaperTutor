CODEXGRAPH: Bridging Large Language Models
and Code Repositories via Code Graph Databases
XiangyanLiu1,2,∗ BoLan3,∗ ZhiyuanHu1 YangLiu2 ZhichengZhang2
WenmengZhou2 FeiWang3 MichaelShieh1
1NationalUniversityofSingapore 2AlibabaGroup 3Xi’anJiaotongUniversity
Abstract
Large Language Models (LLMs) excel in stand-alone code tasks like Hu-
manEval and MBPP, but struggle with handling entire code repositories. This
challenge has prompted research on enhancing LLM-codebase interaction at a
repository scale. Current solutions rely on similarity-based retrieval or man-
ual tools and APIs, each with notable drawbacks. Similarity-based retrieval
often has low recall in complex tasks, while manual tools and APIs are typ-
ically task-specific and require expert knowledge, reducing their generalizabil-
ity across diverse code tasks and real-world applications. To mitigate these
limitations, we introduce CODEXGRAPH, a system that integrates LLM agents
with graph database interfaces extracted from code repositories. By leverag-
ing the structural properties of graph databases and the flexibility of the graph
query language, CODEXGRAPH enables the LLM agent to construct and ex-
ecute queries, allowing for precise, code structure-aware context retrieval and
code navigation. We assess CODEXGRAPH using three benchmarks: Cross-
CodeEval, SWE-bench, andEvoCodeBench. Additionally, wedevelopfivereal-
world coding applications. With a unified graph database schema, CODEX-
GRAPH demonstrates competitive performance and potential in both academic
and real-world environments, showcasing its versatility and efficacy in software
engineering. Ourapplicationdemo: https://github.com/modelscope/
modelscope-agent/tree/master/apps/codexgraph_agent.
1 Introduction
Large Language Models (LLMs) excel in code tasks, impacting automated software engineering
(Chen et al., 2021; Yang et al., 2024b; OpenDevin Team, 2024). Repository-level tasks (Zhang
et al., 2023; Jimenez et al., 2023; Ding et al., 2024; Li et al., 2024b) mimic software engineers’
work with large codebases (Kovrigin et al., 2024). These tasks require models to handle intricate
dependenciesandcomprehendprojectstructure(Jiangetal.,2024;Sunetal.,2024).
Current LLMs struggle with long-context inputs, limiting their effectiveness with large codebases
(Jimenez et al., 2023) and lengthy sequences reasoning (Liu et al., 2024a). Researchers have
proposed methods to enhance LLMs by retrieving task-relevant code snippets and structures,
improvingperformanceincomplexsoftwaredevelopment(Dengetal.,2024;Aroraetal.,2024;Ma
et al., 2024). However, these approaches mainly rely on either similarity-based retrieval (Jimenez
et al., 2023; Cheng et al., 2024; Liu et al., 2024b) or manual tools and APIs (Zhang et al., 2024b;
O¨rwall, 2024). Similarity-based retrieval methods, common in Retrieval-Augmented Generation
(RAG) systems (Lewis et al., 2020), often struggle with complex reasoning for query formulation
(Jimenez et al., 2023) and handling intricate code structures (Phan et al., 2024), leading to low
recall rates. Meanwhile, existing tool/API-based interfaces that connect codebases and LLMs are
∗Equalcontribution.WorkwasdoneduringXiangyan’sinternshipatAlibaba.liu.xiangyan@u.nus.edu
Preprint.Workinprogress.
4202
guA
7
]ES.sc[
1v01930.8042:viXra(a) Illustration of CodexGraph
Schema Schema
LM Agent Code Repository
Code Graph Database
(b) CodexGraph vs. Repository-Level Code Tasks and Applications
CrossCodeEval Code Chat
Code Debugger
EvoCodeBench CodexGraph Code Commenter
Code Genrator
SWE-Bench Code Unit Tester
academic benchmarks real-world applications
Figure1: (a)Usingaunifiedschema,CODEXGRAPHemployscodegraphdatabasesasinterfacesthatallow
LLMagentstointeractseamlesslywithcoderepositories. (b)CODEXGRAPHsupportsthemanagementofa
widerangeoftasks,fromacademic-levelcodebenchmarkstoreal-worldsoftwareengineeringapplications.
typically task-specific and require extensive expert knowledge (O¨rwall, 2024; Chen et al., 2024).
Furthermore, our experimental results in Section 5 indicate that the two selected methods lack
flexibilityandgeneralizabilityfordiverserepository-levelcodetasks.
Recent studies have demonstrated the effectiveness of graph structures in code repositories (Phan
et al., 2024; Cheng et al., 2024). Meanwhile, inspired by recent advances in graph-based RAG
(Edgeetal.,2024;Liuetal.,2024b;Heetal.,2024)andtheapplicationofexecutablecode(suchas
SQL,Cypher,andPython)toconsolidateLLMagentactions(Wangetal.,2024;Lietal.,2024c;Xue
etal.,2023),wepresentCODEXGRAPH,asshowninFigure1(a). CODEXGRAPHalleviatesthelim-
itations of existing approaches by bridging code repositories with LLMs through graph databases.
CODEXGRAPHutilizesstaticanalysistoextractcodegraphsfromrepositoriesusingatask-agnostic
schema that defines the nodes and edges within the code graphs. In these graphs, nodes represent
source code symbols such as MODULE, CLASS, and FUNCTION, and each node is enriched with
relevantmeta-information. Theedgesbetweennodesrepresenttherelationshipsamongthesesym-
bols, such as CONTAINS, INHERITS, and USES (see Figure 2 for an illustrative example). By
leveragingthestructuralpropertiesofgraphdatabases, CODEXGRAPH enhancestheLLMagent’s
comprehensionofcodestructures. CODEXGRAPHleveragesrepositorycodeinformationandgraph
structures for global analysis and multi-hop reasoning, enhancing code task performance. When
usersprovidecode-relatedinputs,theLLMagentanalyzestherequiredinformationfromthecode
graphs,constructsflexiblequeriesusinggraphquerylanguage,andlocatesrelevantnodesoredges.
Thisenablespreciseandefficientretrieval,allowingforeffectivescalingtolargerrepositorytasks.
ToevaluatetheeffectivenessoftheCODEXGRAPH,weassessitsperformanceacrossthreechalleng-
ingandrepresentativerepository-levelbenchmarks:CrossCodeEval(Dingetal.,2024),SWE-bench
(Yang et al., 2024b) and EvoCodeBench (Li et al., 2024b). Our experimental results demonstrate
that, by leveraging a unified graph database schema (Section 3.1) and a simple workflow design
(Section 3.2), the CODEXGRAPH achieves competitive performance across all academic bench-
marks,especiallywhenequippedwithmoreadvancedLLMs. Furthermore,asillustratedinFigure
1(b),toaddressreal-worldsoftwaredevelopmentneeds,weextend CODEXGRAPH tothefeature-
richModelScope-Agent(Lietal.,2023)framework.Section6highlightsfivereal-worldapplication
scenarios,includingcodedebuggingandwritingcodecomments,showcasingtheversatilityandef-
ficacyofCODEXGRAPHinpracticalsoftwareengineeringtasks.
Ourcontributionsarefromthreeperspectives:
• Pioneeringcoderetrievalsystem: Weintroduce CODEXGRAPH,integratingcoderepositories
withLLMsviagraphdatabasesforenhancedcodenavigationandunderstanding.
• Benchmarkperformance:WedemonstrateCODEXGRAPH’scompetitiveperformanceonthree
challengingandrepresentativerepository-levelcodebenchmarks.
• Practical applications: We showcase CODEXGRAPH’s versatility in five real-world software
engineeringscenarios,provingitsvaluebeyondacademicsettings.
22 RelatedWork
2.1 Repository-LevelCodeTasks
Repository-level code tasks have garnered significant attention due to their alignment with real-
worldproductionenvironments(Bairietal.,2023;Luoetal.,2024;CognitionLabs,2024;Kovrigin
etal.,2024).Unliketraditionalstandalonecode-relatedtaskssuchasHumanEval(Chenetal.,2021)
andMBPP(Austinetal.,2021),whichoftenfailtocapturethecomplexitiesofreal-worldsoftware
engineering, repository-level tasks necessitate models to understand cross-file code structures and
performintricatereasoning(Liuetal.,2024b;Maetal.,2024;Sunetal.,2024).Thesesophisticated
tasks can be broadly classified into two lines of work based on their inputs and outputs. The first
line of work involves natural language to code repository tasks, exemplified by benchmarks like
DevBench (Li et al., 2024a) and SketchEval (Zan et al., 2024), where models generate an entire
coderepositoryfromscratchbasedonanaturallanguagedescriptionofinputrequirements. State-
of-the-artsolutionsinthisareaoftenemploymulti-agentframeworkssuchasChatDev(Qianetal.,
2023) and MetaGPT (Hong et al., 2023) to handle the complex process of generating a complete
codebase. The second line of work, which our research focuses on, includes tasks that integrate
both a natural language description and a reference code repository, requiring models to perform
taskslikerepository-levelcodecompletion(Zhangetal.,2023;Shrivastavaetal.,2023;Liuetal.,
2023;Dingetal.,2024;Suetal.,2024),automaticGitHubissueresolution(Jimenezetal.,2023),
andrepository-levelcodegeneration(Lietal.,2024b). Toassesstheversatilityandeffectivenessof
ourproposedsystemCODEXGRAPH,weevaluateitonthreediverseandrepresentativebenchmarks
including CrossCodeEval (Ding et al., 2024) for code completion, SWE-bench (Jimenez et al.,
2023)forGithubissueresolution,andEvoCodeBench(Lietal.,2024b)forcodegeneration.
2.2 Retrieval-AugmentedCodeGeneration
Retrieval-Augmented Generation (RAG) systems primarily aim to retrieve relevant content from
externalknowledgebasestoaddressagivenquestion,therebymaintainingcontextefficiencywhile
reducinghallucinationsinprivatedomains(Lewisetal.,2020;Shusteretal.,2021). Forrepository-
level code tasks, which involve retrieving and manipulating code from repositories with complex
dependencies, RAG systems—referred to here as Retrieval-Augmented Code Generation (RACG)
(Jiang et al., 2024)—are utilized to fetch the necessary code snippets or code structures from the
specializedknowledgebaseofcoderepositories. CurrentRACGmethodologiescanbedividedinto
three main paradigms: the first paradigm involves similarity-based retrieval, which encompasses
term-based sparse retrievers (Robertson & Zaragoza, 2009; Jimenez et al., 2023) and embedding-
baseddenseretrievers(Guoetal.,2022;Zhangetal.,2023),withadvancedapproachesintegrating
structured information into the retrieval process (Phan et al., 2024; Cheng et al., 2024; Liu et al.,
2024b).Thesecondparadigmconsistsofmanuallydesignedcode-specifictoolsorAPIsthatrelyon
expertknowledgetocreateinterfacesforLLMstointeractwithcoderepositoriesforspecifictasks
(Zhangetal.,2024b;Deshpandeetal.,2024;Aroraetal.,2024). Thethirdparadigmcombinesboth
similarity-basedretrievalandcode-specifictoolsorAPIs(O¨rwall,2024), leveragingthereasoning
capabilities of LLMs to enhance context retrieval from code repositories. Apart from the three
paradigms,Agentless(Xiaetal.,2024)preprocessesthecoderepository’sstructureandfileskeleton,
allowing the LLMs to interact with the source code. Our proposed framework, CODEXGRAPH,
aligns most closely with the second paradigm but distinguishes itself by discarding the need for
expertknowledgeandtask-specificdesigns.Byusingcodegraphdatabasesasflexibleanduniversal
interfaces, which also structurally store information to facilitate the code structure understanding
ofLLMs, CODEXGRAPH cannavigatethecoderepositoriesandmanagemultiplerepository-level
codetasks,providingaversatileandpowerfulsolutionforRACG.
3 CODEXGRAPH: EnableLLMstoNavigatetheCodeRepository
CODEXGRAPH is a system that bridges code repositories and large language models (LLMs)
through code graph database interfaces. It indexes input code repositories using static analysis,
storingcodesymbolsandrelationshipsasnodesandedgesinagraphdatabaseaccordingtoapre-
definedschema. Whenpresentedwithacodingquestion,CODEXGRAPHleveragestheLLMagent
togenerategraphqueries,whichareexecutedtoretrieverelevantcodefragmentsorcodestructures
3# math/geometric_shapes.py # example nodes
PI = 3.14159 MODULE: “math.geometric_shapes”
class Shape: CLASS: “Shape”, “Circle”
d e f _ s_ ei ln f.i nt a_ m_( es e =l f n, a mn eame): METHOD: “__init__”, “calculate_area”
FUNCTION: “square_root”
def describe(self):
return f"This is a {self.name}." FIELD: “last_result”, “radius”
class Circle(Shape): GLOBAL_VARIABLE: “PI”
def __init__(self, radius):
super().__init__("Circle") # example edges
self.radius = radius
CONTAINS:
d e f c ra el tc uu rl na Pt Ie _a *r e sa e( ls fe .l rf a) d: ius ** 2 (“math.geometric_shapes”) -> (“Circle”)
INHERITS:
def
calculate_circle_circumference(circle): (“Circle”) -> (“Shape”)
return 2 * PI * circle.radius
HAS_METHOD:
# math/math_utils.py (“Circle”) -> (“calculate_area”)
from geometric_shapes import Circle, PI HAS_FIELD:
(“Circle”) -> (“radius”)
EULER_NUMBER = 2.71828
USES:
c l a s s d eG fe o _m _e it nr iy tC _a _l (c su ell fat )o :r: (“calculate_area”) -> (“PI”)
self.last_result = 0
# meta-info of an CLASS node (“Circle”):
def calculate_shape_area(self, shape):
if isinstance(shape, Circle): name: “Circle”
self.last_result =
shape.calculate_area() file_path: “math/geometric_shapes.py”
else:
self.last_result = 0 signature: “class Circle(Shape)”
return self.last_result code: “class Circle(Shape):
def square_root(number): def __init__....”
return number ** 0.5
(1) source code (2) nodes & edges (3) visualization in graph database
Figure 2: Illustration of the process for indexing source code to generate a code graph based on the given
graphdatabaseschema.Subfigure(3)providesavisualizationexampleoftheresultantcodegraphinNeo4j.
from the database. The detailed processes of constructing the code graph database and the LLM
agent’sinteractionswithitareexplainedinsections3.1and3.2,respectively.
3.1 BuildCodeGraphDatabasefromRepositoryCodebase
Schema. We abstract code repositories into code graphs where nodes represent symbols in the
sourcecode,andedgesrepresentrelationshipsbetweenthesesymbols.Theschemadefinesthetypes
ofnodesandedges,directlydetermininghowcodegraphsarestoredinthegraphdatabase.Different
programming languages typically require different schemas based on their characteristics. In our
project, we focus on Python and have empirically designed a schema tailored to its features, with
nodetypesincludingMODULE,CLASS,METHOD,FUNCTION,FIELD,andGLOBAL VARIABLE,
andedgetypesincludingCONTAINS,INHERITS,HAS METHOD,HAS FIELD,andUSES.
Each node type has corresponding attributes to represent its meta-information. For instance,
METHODnodeshaveattributessuchasname,file path,class,code,andsignature. For
storage efficiency, nodes with a code attribute do not store the code snippet directly in the graph
database but rather an index pointing to the corresponding code fragment. Figure 2 illustrates a
samplecodegraphderivedfromourschema,andAppendixA.1showsthedetailsoftheschema.
Phase1:Shallowindexing. Thecodegraphdatabaseconstructionprocessconsistsoftwophases,
beginningwiththeinputofthecoderepositoryandschema. Thefirstphaseemploysashallowin-
dexingmethod,inspiredbySourcetrail’sstaticanalysisprocess2,toperformasingle-passscanof
the entire repository. During this scan, symbols and relationships are extracted from each Python
file,processedonlyonce,andstoredasnodesandedgesinthegraphdatabase. Concurrently,meta-
information for these elements is recorded. This approach ensures speed and efficiency, capturing
all nodes and their meta-information in one pass. However, the shallow indexing phase has lim-
itations due to its single-pass nature. Some important edges, particularly certain INHERITS and
CONTAINSrelationships,maybeoverlookedastheymightrequirecontextfrommultiplefiles.
Phase 2: Complete the edges. The second phase addresses the limitations of shallow indexing
byfocusingoncross-filerelationships. WeemployDepth-FirstSearch(DFS)totraverseeachcode
file,usingabstractsyntaxtreeparsingtoidentifymodulesandclasses. Thisapproachisparticularly
2https://github.com/CoatiSoftware/Sourcetrail
4Analysis & Natural Langugae Queries
Code Question
# Analysis #
• complete the unfinished code Primay LM <analysis_context>
• resolve the github issue Agent # Natural Language Queries #
• finish the function given the sigan- Retrieve the module where class
ature and comment `LinearClassifier` is defined, along
• .... with the global variables it contains
...
Schema
translation LM Agent
Graph Queriy Translation
•MA AT nC aH
l
y( sic s: &C l Na as tus
r
a{
l
n La am ne g:
u
g' aL
e
i Qn ue ea rr ieC slassifier'})<-[:CONTAINS]-(m:Module)
MATCH (c)-[:CONTAINS]->(f:Function)
RETURN m.name AS module_name, collect(f.name) AS functions
Figure3:TheprimaryLLMagentanalyzesthegivencodequestion,writtingnaturallanguagequeries.These
queriesarethenprocessedbythetranslationLLMagent,whichtranslatesthemintoexecutablegraphqueries.
effective in resolving Python’s re-export issues. We convert relative imports to absolute imports,
enabling accurate establishment of cross-file CONTAINS relationships through graph queries. Si-
multaneously,werecordINHERITSrelationshipsforeachclass. Forcomplexcaseslikemultiple
inheritance,DFSisusedtoestablishedgesforinheritedFIELDandMETHODnodeswithinthegraph
database. Thiscomprehensiveapproachensuresaccuratecaptureofbothintra-fileandcross-filere-
lationships,providingacompleterepresentationofthecodebasestructure.
Summary. Ourcodegraphdatabasedesignoffersfourkeyadvantagesforsubsequentuse. First,
itensuresefficientstoragebystoringcodesnippetsasindexedreferencesratherthandirectlyinthe
graphdatabase. Second,itenablesmulti-granularitysearches,frommodule-leveltovariable-level,
accommodating diverse analytical needs. Third, it facilitates topological analysis of the codebase,
revealing crucial insights into hierarchical and dependency structures. Last, this schema design
supports multiple tasks without requiring modifications, demonstrating its versatility and general
applicability. These features collectively enhance the system’s capability to handle complex code
analysistaskseffectivelyacrossvariousscenarios.
3.2 LargeLanguageModelsInteractionwithCodeGraphDatabase
Codestructure-awaresearch. CODEXGRAPH leveragestheflexibilityofgraphquerylanguage
toconstructcomplexandcompositesearchconditions. Bycombiningthisflexibilitywiththestruc-
turalpropertiesofgraphdatabases, theLLMagentcaneffectivelynavigatethroughvariousnodes
and edges in the code graph. This capability allows for intricate queries such as: “Find classes
under a certain module that contain a specific method”, or “Retrieve the module where a certain
classisdefined,alongwiththefunctionsitcontains”. Thisapproachenablescodestructure-aware
searches,providingalevelofcoderetrievalthatisdifficulttoachievewithsimilarity-basedretrieval
methods (Robertson & Zaragoza, 2009; Guo et al., 2022) or conventional code-specific tools and
APIs(Zhangetal.,2024b;Deshpandeetal.,2024).
Write then translate. LLM agents are powered by LLMs and operate based on user-provided
prompts to break down tasks, utilize tools, and perform reasoning. This design is effective for
handlingspecific, focusedtasks(Gupta&Kembhavi,2022;Yuanetal.,2024), butwhentasksare
complexandmultifaceted,LLMagentsmayunderperform. Thislimitationhasledtothedevelop-
mentofmulti-agentsystems(Hongetal.,2023;Qianetal.,2023;Guoetal.,2024),wheremultiple
LLM agents independently handle parts of the task. Inspired by this approach, CODEXGRAPH
implementsastreamlined“writethentranslate”strategytooptimizeLLM-databaseinteractions.
5AsillustratedinFigure3,theprimaryLLMagentfocusesonunderstandingcontextandgenerating
naturallanguagequeriesbasedontheuser’squestion. Thesequeriesarethenpassedtoaspecialized
translationLLMagent,whichconvertsthemintoformalgraphqueries.Thisdivisionoflaborallows
theprimaryLLMagenttoconcentrateonhigh-levelreasoningwhileensuringsyntacticallycorrect
and optimized graph queries. By separating these tasks, CODEXGRAPH enhances query success
ratesandimprovesthesystem’sabilitytoaccuratelyretrieverelevantcodeinformation.
Iterativepipeline. Insteadofcompletingthecodetaskinasinglestep, CODEXGRAPH employs
an iterative pipeline for interactions between LLM agents and code graph databases, drawing in-
sightsfromexistingagentsystems(Yaoetal.,2023;Yangetal.,2024b).Ineachround,LLMagents
formulatemultiplequeriesbasedontheuser’squestionandpreviouslygatheredinformation.Similar
toMadaanetal.(2023),theagentthenanalyzestheaggregatedresultstodeterminewhethersuffi-
cientcontexthasbeenacquiredorifadditionalroundsarenecessary. Thisiterativeapproachfully
leveragesthereasoningcapabilitiesoftheLLMagent,therebyenhancingproblem-solvingaccuracy.
4 ExperimentalSetting
Benchmarks. We employ three diverse repository-level code benchmarks to evaluate CODEX-
GRAPH: CrossCodeEval(Dingetal.,2024),SWE-bench(Yangetal.,2024b),andEvoCodeBench
(Lietal.,2024b). CrossCodeEvalisamultilingualscopecross-filecompletiondatasetforPython,
Java,TypeScript,andC#.SWE-benchevaluatesamodel’sabilitytosolveGitHubissueswith2,294
Issue-PullRequestpairsfrom12Pythonrepositories. EvoCodeBenchisanevolutionarycodegen-
erationbenchmarkwithcomprehensiveannotationsandevaluationmetrics.
WereportourprimaryresultsontheCrossCodeEvalLite(Python)andSWE-benchLitetestsetsfor
CrossCodeEval and SWE-bench, respectively, and on the full test set for EvoCodeBench. Cross-
CodeEvalLite(Python)andSWE-benchLiterepresentsubsetsoftheirrespectivedatasets. Cross-
CodeEval Lite (Python) consists of 1000 randomly sampled Python instances, while SWE-bench
Liteincludes300instancesrandomlysampledafterfilteringoutthosewithpoorissuedescriptions.
Remark:Duringindexingof43SympysamplesfromtheSWE-benchdataset,wefaceout-of-memory
issuesduetonumerousfilesandcomplexdependencies,leadingtotheirexclusion. Similarly,some
EvoCodeBenchsamplesareomittedduetotestenvironmentconfigurationissues. Thus,SWE-bench
LiteandEvoCodeBenchresultsarebasedon257and212samples,respectively.
Baselines. We evaluate whether CODEXGRAPH is a powerful solution for Retrieval-Augmented
Code Generation (RACG) (Jiang et al., 2024). We specifically assess how effectively code graph
databaseinterfacesaidLLMsinunderstandingcoderepositories,particularlywhenhandlingdiverse
code questions across different benchmarks to test CODEXGRAPH ’s general applicability. To
achievethis,weselectresilientRACGbaselinesthatcanbeadaptedtovarioustasks. Basedonthe
categoriesinSection2.2,wechooseBM25(Robertson&Zaragoza,2009)andAUTOCODEROVER
(Zhangetal.,2024b),whicharewidelyrecognizedincodetasks(Jimenezetal.,2023;Dingetal.,
2024; Kovrigin et al., 2024; Chen et al., 2024), along with a NO-RAG method. Besides, since
our work focuses on RACG methods and their generalizability, we exclude methods that interact
with external websites (OpenDevin Team, 2024; Zhang et al., 2024a) and runtime environments
(Yang et al., 2024b), as well as task-specific methods that are not easily adaptable across multiple
benchmarks(Chengetal.,2024;O¨rwall,2024). Thesemethodsfalloutsidethescopeofourproject.
Especially,althoughZhangetal.(2024b)evaluateAUTOCODEROVERexclusivelyonSWE-bench,
weextenditsimplementationtoCrossCodeEvalandEvoCodeBench,whileretainingitscoresetof
7code-specifictoolsforcoderetrieval.
Large Language Models (LLMs). We evaluate CODEXGRAPH on three advanced and well-
known LLMs with long text processing, tool use, and code generation capabilities: GPT-4o,
DeepSeek-Coder-V2(Zhuetal.,2024),andQwen2-72b-Instruct(Yangetal.,2024a).
3https://github.com/princeton-nlp/SWE-bench/issues/2
6Table 1: Performance comparison of CODEXGRAPH and RACG baselines across three benchmarks using
differentbackbonelargelanguagemodels.TheabsenceofvaluesinSWE-benchLitefortheNORAGmethod
is due to the non-reproducibility of the inference scripts 3provided by the SWE-bench authors. Similarly,
missingvaluesinEvoCodeBenchareattributabletotaskinputsbeingunsuitableforconstructingtherequired
queriesforBM25,astheoriginalpaperdoesnotprovidethecorrespondingimplementation.
CrossCodeEvalLite(Python) SWE-benchLite EvoCodeBench
Model Method
EM ES ID-EM ID-F1 Pass@1 Pass@1 Recall@1
NORAG 8.20 46.16 13.0 36.92 - 19.34 11.34
BM25 15.50 51.74 22.60 45.44 0.00 - -
Qwen2
AUTOCODEROVER 5.21 47.63 10.16 36.54 9.34 16.91 7.86
CODEXGRAPH 5.00 47.99 9.10 36.44 1.95 14.62 8.60
NORAG 11.70 60.73 16.90 47.85 - 25.47 11.04
BM25 21.90 67.52 30.60 59.04 1.17 - -
DS-Coder
AUTOCODEROVER 14.90 59.78 22.30 51.34 15.56 20.28 7.56
CODEXGRAPH 20.20 63.14 28.10 54.88 12.06 27.62 12.01
NORAG 10.80 59.36 16.70 48.22 - 27.83 11.79
BM25 21.20 66.18 30.20 58.71 3.11 - -
GPT-4o
AUTOCODEROVER 21.20 61.92 28.10 54.81 22.96 28.78 11.17
CODEXGRAPH 27.90 67.98 35.60 61.08 22.96 36.02 11.87
• GPT-4o: Developed by OpenAI 4, this model excels in commonsense reasoning, mathematics,
andcode,andisamongthetop-performingmodelsasofJuly20245.
• DeepSeek-Coder-V2 (DS-Coder): A specialized code-specific LLM by DeepSeek 6, it retains
generalcapabilitieswhilebeinghighlyproficientincode-relatedtasks.
• Qwen2-72b-Instruct (Qwen2): Developed by Alibaba 7, thisopen-source model has about 72
billionparametersanda128klongcontext,makingitsuitableforevaluatingexistingmethods.
Forthehyperparametersoftheselectedlargelanguagemodels,weempiricallysetthetemperature
coefficientto0.0forbothGPT-4oandQwen2-72b-Instruct,andto1.0forDeepSeek-Coder-V2. All
otherparametersarekeptattheirdefaultsettings.
Metrics. Inmetricsselection,wefollowtheoriginalpapers’settings(Jimenezetal.,2023;Ding
etal.,2024;Lietal.,2024b). Specifically,forCrossCodeEval,wemeasureperformancewithcode
matchandidentifiermatchmetrics,assessingaccuracywithexactmatch(EM),editsimilarity(ES),
and F1 scores. SWE-bench utilizes % Resolved (Pass@1) to gauge the effectiveness of model-
generated patches based on provided unit tests. EvoCodeBench employs Pass@k, where k repre-
sentsthenumberofgeneratedprograms,forfunctionalcorrectnessandRecall@ktoassesstherecall
ofreferencedependenciesingeneratedprograms. Wesetkto1inourmainexperiments.
Implementation details. Before indexing, we filter the Python repositories for each benchmark
toretainonlyPythonfiles. FortheSWE-benchdataset,wealsoexcludetestfilestoavoidslowing
down the creation of the code graph database. Following the process outlined in Section 3.1, we
construct code graph databases for the indexed repositories, storing the corresponding nodes and
edges. WeselectNeo4jasthegraphdatabaseandCypherasthequerylanguage.
5 Results
5.1 AnalysisofRepository-LevelCodeTasks
RACG is crucial for repository-level code tasks. In Table 1, RACG-based methods—BM25,
AUTOCODEROVER, and CODEXGRAPH—basically outperform the NO-RAG method across all
benchmarksandevaluationmetrics.Forinstance,ontheCrossCodeEvalLite(Python)dataset,using
GPT-4oasthebackboneLLM,RACGmethodsimproveperformanceby10.4%to17.1%ontheEM
4Weusethegpt-4o-2024-05-13version,https://openai.com/api
5https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard
6https://chat.deepseek.com/coder
7https://dashscope.console.aliyun.com/model
7Table2: Averagetokencostcomparisonacrossthreebenchmarks(GPT-4oasthebackboneLLM).
CrossCodeEvalLite(Python) SWE-benchLite EvoCodeBench
BM25 1.47k 14.76k -
AUTOCODEROVER 10.74k 76.01k 21.41k
CODEXGRAPH 22.16k 102.25k 24.49k
metriccomparedto NO-RAG.Thisdemonstratesthatthe NO-RAG approach, whichreliessolely
onin-filecontextandlacksinteractionwiththecoderepository,significantlylimitsperformance.
Existing RACG methods struggle to adapt to various repo-level code tasks. Experimental re-
sults in Table 1 reveal the shortcomings of existing RACG-based methods like BM25 and AU-
TOCODEROVER. While these methods perform well in specific tasks, they often underperform
when applied to other repository-level code tasks. This discrepancy typically arises from their in-
herentcharacteristicsortask-specificoptimizations.
Specifically,AUTOCODEROVERisdesignedwithcodetoolstailoredforSWE-benchtasks,leverag-
ingexpertknowledgeandtheuniquefeaturesofSWE-benchtooptimizetoolselectionanddesign.
ThisoptimizationrefinestheLLMagent’sactionspaces,enablingittogathervaluableinformation
moreefficientlyandboostingitsperformanceonSWE-benchtasks(22.96%). However,thesetask-
specificoptimizationslimititsflexibilityandeffectivenessinothercodingtasks,asevidencedbyits
subparresultsonCrossCodeEvalLite(Python)andEvoCodeBenchcomparedtoothermethods.
Similarly, BM25 faces the same issues. In CrossCodeEval Lite (Python), its similarity-based re-
trievalalignswellwithcodecompletiontasks,enablingittoeasilyretrieverelevantusagereferences
ordirectanswers.Thisresultsinstrongperformance,particularlyintheESmetric.However,BM25
lacksthereasoningcapabilitiesofLLMsduringqueryconstruction,makingitsretrievalprocessless
intelligent. Consequently, when confronted with reasoning-heavy tasks like those in SWE-bench,
BM25oftenfailstoretrieveappropriatecodesnippets,leadingtopoorperformance.
CODEXGRAPH shows versatility and efficacy across diverse benchmarks. Table 1 shows that
CODEXGRAPHachievescompetitiveresultsacrossvariousrepository-levelcodetaskswithgeneral
code graph database interfaces. Specifically, with GPT-4o as the LLM backbone, CODEXGRAPH
outperforms other RACG baselines on CrossCodeEval Lite (Python) and EvoCodeBench, while
also achieving results comparable to AUTOCODEROVER on SWE-bench Lite. This demonstrates
thegeneralityandeffectivenessofthecodegraphdatabaseinterfacedesign.
CODEXGRAPH increases token consumption. CODEXGRAPH uses code graph databases as
interfaces and retrieves information from the code repository by writing graph queries. While
benefiting from larger and more flexible action spaces, it also incurs increased token costs. The
primary reason for this is that the length of the query outcomes is not controllable. Moreover,
CODEXGRAPHsometimesencountersloopswhereitfailstogenerateexecutablegraphqueries. As
demonstratedinTable2,thisleadstoahighertokenusagecomparedtoexistingRACGmethods.
5.2 DeeperAnalysisofCODEXGRAPH
Optimalqueryingstrategiesvaryacrossdif-
Legends: single query multiple queries
ferent benchmarks. There are two strate-
gies for formulating queries in each round
withinCODEXGRAPH: eithergeneratingasin- 67.98 65.45
gle query or producing multiple queries for
code retrieval. Opting for a single query per 25.90 27.90 22.96
round can enhance precision in retrieving rel- 17.90
evant content but may compromise the recall
rate. Conversely, generating multiple queries
CrossCodeEval Lite (Python) SWE-bench Lite
per round can improve recall but may reduce
precision. Experimental results, as illustrated Figure4: Performancecomparisonofdifferentquery-
ing strategies on CrossCodeEval Lite (Python) and
inFigure4,revealthatforCrossCodeEvalLite
SWE-benchLite.
(Python), which involves lower reasoning dif-
ficulty (26.43 vs. 27.90 in the EM metric), the “multiple queries” strategy is more effective. In
8
)ME(
hctaM
tcaxE
)SE(
ytiralimiS
tidE
1@ssaPTable3: AblationstudyaboutthetranslationLLMagentonCrossCodeEvalLite(Python).
CrossCodeEvalLite(Python)
Model Method
EM ES ID-EM ID-F1
CODEXGRAPH 5.00 47.99 9.10 36.44
Qwen2
w/otranslationLLMAgent 0.50(-4.50) 10.45(-37.54) 0.60(-8.50) 2.62(-33.82)
CODEXGRAPH 20.20 63.14 28.10 54.88
DS-Coder
w/otranslationLLMAgent 5.50(-14.70) 53.56(-9.58) 11.20(-16.90) 39.75(-15.13)
CODEXGRAPH 27.90 67.98 35.60 61.08
GPT-4o
w/otranslationLLMAgent 8.30(-19.60) 56.36(-11.62) 14.40(-21.20) 44.08(-17.00)
contrast, forSWE-benchLite, whichpresentshigherreasoningdifficulty, the“single query”strat-
egyyieldsbetteroutcomes(22.96vs. 17.90inthePass@1metric). Thesefindingsprovidevaluable
guidanceforresearchersinselectingthemostappropriatequeryingstrategyforfuturestudies.
“Writethentranslate”easesreasoningload. WhentheassistanceofthetranslationLLMagent
is removed, the primary LLM agent must independently analyze the coding question and directly
formulatethegraphqueryforcoderetrieval. ThisincreasesthereasoningloadontheprimaryLLM
agent, leading to a decline in the syntactic accuracy of the graph queries. Experimental results
in Table 3 highlight the significant negative impact of the removal of the translation LLM agent
on CODEXGRAPH’s performance across all selected LLMs in the CrossCodeEval Lite (Python)
benchmark. Even when GPT-4o is used as the backbone model, performance metrics exhibit a
significantdrop(e.g.,theEMmetricdropsfrom27.90%to8.30%),underscoringthecriticalroleof
thetranslationLLMagentinalleviatingtheprimaryLLMagent’sreasoningburden.
CODEXGRAPH is enhanced when equipped with advanced LLMs. Code graph databases pro-
vide a flexible and general interface, resulting in a broader action space for CODEXGRAPH com-
paredtoexistingmethods. However, iftheunderlyingLLMlackssufficientreasoningandcoding
capabilities,theLLMagentinCODEXGRAPHmaystruggletoformulateappropriategraphqueries.
Thiscanleadtofailuresinretrievingtheexpectedcode,whichinturnhampersfurtherreasoning.
AsshowninTable1,theeffectivenessofCODEXGRAPHimprovessignificantlywithadvancements
inLLMs. Forexample,transitioningfromQwen2-72b-InstructtoDeepSeek-Coder-v2andthento
GPT-4o, the overall performance enhancement across various benchmarks and metrics is notable.
ThisillustratesthatwhileCODEXGRAPHrequireshigh-levelcodingskills,reasoningabilities,and
proficiency in handling complex texts from LLMs, the rapid advancement of these models allows
themtobetterleveragetheflexibleinterfacesprovidedbycodegraphdatabases.
6 Real-WorldApplicationScenario
To highlight the practical value of the CODEXGRAPH in real-world applications, we develop five
code agents using the flexible ModelScope-Agent framework (Li et al., 2023). These agents are
designedtoaddresscommoncodingchallengesinproductionenvironmentsbyintegratingkeycon-
ceptsoftheCODEXGRAPH. CodeChatallowsuserstoinquireaboutacoderepository,providing
insights into code structure and function usage. Code Debugger diagnoses and resolves bugs by
applying iterative reasoning and information retrieval to suggest targeted fixes. Code Unittestor
generates unit tests for specified classes or functions to ensure thorough functionality verification.
CodeGeneratorautomaticallycreatescodetomeetnewrequirements,extendingthefunctionality
ofexistingcodebases. Lastly, CodeCommentorproducescomprehensiveannotations, enhancing
documentation for code segments lacking comments. Examples of these agents are provided in
AppendixA.2tomaintainbrevityinthemaintext.
7 Discussion
Limitations. CODEXGRAPHhasonlybeenevaluatedonasingleprogramminglanguage,Python.
Inthefuture,weplantoextendCODEXGRAPHtomoreprogramminglanguages,suchasJavaand
C++. Secondly,thereisroomforimprovementintheconstructionefficiencyandschemacomplete-
9nessofthecodegraphdatabase. Fasterdatabaseindexingandamorecomprehensiveschema(e.g.,
adding edges related to function calls) will enhance the broader applicability of CODEXGRAPH.
Finally,thedesignofCODEXGRAPH’sworkflowcanfurtherintegratewithexistingadvancedagent
techniques,suchasfiner-grainedmulti-agentcollaboration.
Conclusion. CODEXGRAPH addresses the limitations of existing RACG methods, which often
struggle with flexibility and generalization across different code tasks. By integrating LLMs with
code graph database interfaces, CODEXGRAPH facilitates effective, code structure-aware retrieval
for diverse repository-level code tasks. Our evaluations highlight its competitive performance and
broad applicability on academic benchmarks. Additionally, we provide several code applications
inModelScope-Agent,demonstratingCODEXGRAPH’scapabilitytoenhancetheaccuracyandus-
abilityofautomatedsoftwaredevelopment.
10References
Daman Arora, Atharv Sonwane, Nalin Wadhwa, Abhav Mehrotra, Saiteja Utpala, Ramakrishna
Bairi, Aditya Kanade, and Nagarajan Natarajan. Masai: Modular architecture for software-
engineeringaiagents. arXivpreprintarXiv:2406.11638,2024.
JacobAustin,AugustusOdena,MaxwellNye,MaartenBosma,HenrykMichalewski,DavidDohan,
Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language
models. arXivpreprintarXiv:2108.07732,2021.
RamakrishnaBairi,AtharvSonwane,AdityaKanade,VageeshDC,ArunIyer,SureshParthasarathy,
SriramRajamani,B.Ashok,andShashankShet. Codeplan: Repository-levelcodingusingllms
andplanning,2023. URLhttps://arxiv.org/abs/2309.12499.
DongChen,ShaoxinLin,MuhanZeng,DaoguangZan,Jian-GangWang,AntonCheshkov,JunSun,
Hao Yu, Guoliang Dong, Artem Aliev, Jie Wang, Xiao Cheng, Guangtai Liang, Yuchi Ma, Pan
Bian, Tao Xie, and Qianxiang Wang. Coder: Issue resolving with multi-agent and task graphs,
2024.
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared
Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large
languagemodelstrainedoncode. arXivpreprintarXiv:2107.03374,2021.
Wei Cheng, Yuhan Wu, and Wei Hu. Dataflow-guided retrieval augmentation for repository-level
codecompletion. arXivpreprintarXiv:2405.19782,2024.
Cognition Labs. Devin, AI software engineer. https://www.cognition-labs.com/
introducing-devin,2024.
Ken Deng, Jiaheng Liu, He Zhu, Congnan Liu, Jingxin Li, Jiakai Wang, Peng Zhao, Chenchen
Zhang, Yanan Wu, Xueqiao Yin, et al. R2c2-coder: Enhancing and benchmarking real-
world repository-level code completion abilities of code large language models. arXiv preprint
arXiv:2406.01359,2024.
Ajinkya Deshpande, Anmol Agarwal, Shashank Shet, Arun Iyer, Aditya Kanade, Ramakrishna
Bairi, and Suresh Parthasarathy. Class-level code generation from natural language using it-
erative, tool-enhanced reasoning over repository, 2024. URL https://arxiv.org/abs/
2405.01573.
YangruiboDing, ZijianWang, WasiAhmad, HantianDing, MingTan, NihalJain, MuraliKrishna
Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, et al. Crosscodeeval: A diverse
and multilingual benchmark for cross-file code completion. Advances in Neural Information
ProcessingSystems,36,2024.
DarrenEdge,HaTrinh,NewmanCheng,JoshuaBradley,AlexChao,ApurvaMody,StevenTruitt,
andJonathanLarson. Fromlocaltoglobal: Agraphragapproachtoquery-focusedsummariza-
tion. arXivpreprintarXiv:2404.16130,2024.
Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and Jian Yin. Unixcoder: Unified
cross-modalpre-trainingforcoderepresentation. arXivpreprintarXiv:2203.03850,2022.
TaichengGuo,XiuyingChen,YaqiWang,RuidiChang,ShichaoPei,NiteshV.Chawla,OlafWiest,
and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and
challenges,2024. URLhttps://arxiv.org/abs/2402.01680.
Tanmay Gupta and Aniruddha Kembhavi. Visual programming: Compositional visual reasoning
withouttraining,2022. URLhttps://arxiv.org/abs/2211.11559.
XiaoxinHe,YijunTian,YifeiSun,NiteshVChawla,ThomasLaurent,YannLeCun,XavierBres-
son,andBryanHooi. G-retriever: Retrieval-augmentedgenerationfortextualgraphunderstand-
ingandquestionanswering. arXivpreprintarXiv:2402.07630,2024.
SiruiHong, XiawuZheng, JonathanChen, YuhengCheng, JinlinWang, CeyaoZhang, ZiliWang,
Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-
agentcollaborativeframework. arXivpreprintarXiv:2308.00352,2023.
11JuyongJiang,FanWang,JiasiShen,SungjuKim,andSunghunKim. Asurveyonlargelanguage
modelsforcodegeneration. arXivpreprintarXiv:2406.00515,2024.
Carlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik
Narasimhan. Swe-bench: Canlanguagemodelsresolvereal-worldgithubissues? arXivpreprint
arXiv:2310.06770,2023.
Alexander Kovrigin, Aleksandra Eliseeva, Yaroslav Zharov, and Timofey Bryksin. On the im-
portance of reasoning for context retrieval in repository-level code editing. arXiv preprint
arXiv:2406.04464,2024.
PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,
HeinrichKu¨ttler,MikeLewis,Wen-tauYih,TimRockta¨schel,etal. Retrieval-augmentedgenera-
tionforknowledge-intensivenlptasks. AdvancesinNeuralInformationProcessingSystems,33:
9459–9474,2020.
Bowen Li, Wenhan Wu, Ziwei Tang, Lin Shi, John Yang, Jinyang Li, Shunyu Yao, Chen Qian,
BinyuanHui,QichengZhang,etal. Devbench: Acomprehensivebenchmarkforsoftwaredevel-
opment. arXivpreprintarXiv:2403.08604,2024a.
ChenliangLi,HehongChen,MingYan,WeizhouShen,HaiyangXu,ZhikaiWu,ZhichengZhang,
Wenmeng Zhou, Yingda Chen, Chen Cheng, Hongzhu Shi, Ji Zhang, Fei Huang, and Jingren
Zhou. Modelscope-agent: Buildingyourcustomizableagentsystemwithopen-sourcelargelan-
guagemodels,2023. URLhttps://arxiv.org/abs/2309.00986.
JiaLi,GeLi,XuanmingZhang,YihongDong,andZhiJin. Evocodebench: Anevolvingcodegen-
erationbenchmarkalignedwithreal-worldcoderepositories. arXivpreprintarXiv:2404.00599,
2024b.
Zhuoyang Li, Liran Deng, Hui Liu, Qiaoqiao Liu, and Junzhao Du. Unioqa: A unified frame-
work for knowledge graph question answering with large language models. arXiv preprint
arXiv:2406.02110,2024c.
NelsonFLiu,KevinLin,JohnHewitt,AshwinParanjape,MicheleBevilacqua,FabioPetroni,and
Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the
AssociationforComputationalLinguistics,12:157–173,2024a.
TianyangLiu,CanwenXu,andJulianMcAuley. Repobench: Benchmarkingrepository-levelcode
auto-completionsystems. arXivpreprintarXiv:2306.03091,2023.
Wei Liu, Ailun Yu, Daoguang Zan, Bo Shen, Wei Zhang, Haiyan Zhao, Zhi Jin, and Qianxiang
Wang. Graphcoder: Enhancing repository-level code completion via code context graph-based
retrievalandlanguagemodel. arXivpreprintarXiv:2406.07003,2024b.
Qinyu Luo, Yining Ye, Shihao Liang, Zhong Zhang, Yujia Qin, Yaxi Lu, Yesai Wu, Xin Cong,
Yankai Lin, Yingli Zhang, Xiaoyin Che, Zhiyuan Liu, and Maosong Sun. Repoagent: An llm-
poweredopen-sourceframeworkforrepository-levelcodedocumentationgeneration,2024.URL
https://arxiv.org/abs/2402.16667.
YingweiMa,QingpingYang,RongyuCao,BinhuaLi,FeiHuang,andYongbinLi. Howtounder-
standwholesoftwarerepository? arXivpreprintarXiv:2406.01422,2024.
Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri
Alon,NouhaDziri,ShrimaiPrabhumoye,YimingYang,SeanWelleck,BodhisattwaPrasadMa-
jumder,ShashankGupta,AmirYazdanbakhsh,andPeterClark. Self-refine: Iterativerefinement
withself-feedback,2023.
OpenDevinTeam.OpenDevin:AnOpenPlatformforAISoftwareDevelopersasGeneralistAgents.
https://github.com/OpenDevin/OpenDevin,2024. Accessed: ENTERTHEDATE
YOUACCESSEDTHEPROJECT.
HuyNPhan,HoangNPhan,TienNNguyen,andNghiDQBui.Repohyper:Bettercontextretrieval
isallyouneedforrepository-levelcodecompletion. arXivpreprintarXiv:2403.06095,2024.
12Chen Qian, Xin Cong, Cheng Yang, Weize Chen, Yusheng Su, Juyuan Xu, Zhiyuan Liu,
and Maosong Sun. Communicative agents for software development. arXiv preprint
arXiv:2307.07924,2023.
StephenRobertsonandHugoZaragoza. Theprobabilisticrelevanceframework: Bm25andbeyond.
Found.TrendsInf.Retr., 3(4):333–389, apr2009. ISSN1554-0669. doi: 10.1561/1500000019.
URLhttps://doi.org/10.1561/1500000019.
DishaShrivastava,DenisKocetkov,HarmdeVries,DzmitryBahdanau,andTorstenScholak. Re-
pofusion: Trainingcodemodelstounderstandyourrepository. arXivpreprintarXiv:2306.10998,
2023.
Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, and Jason Weston. Retrieval augmenta-
tion reduces hallucination in conversation, 2021. URL https://arxiv.org/abs/2104.
07567.
HongjinSu,ShuyangJiang,YuhangLai,HaoyuanWu,BoaoShi,CheLiu,QianLiu,andTaoYu.
Arks: Activeretrievalinknowledgesoupforcodegeneration,2024. URLhttps://arxiv.
org/abs/2402.12317.
Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang,
ChengchengHan,RenyuZhu,ShuaiYuan,etal.Asurveyofneuralcodeintelligence:Paradigms,
advancesandbeyond. arXivpreprintarXiv:2403.14734,2024.
XingyaoWang,YangyiChen,LifanYuan,YizheZhang,YunzhuLi,HaoPeng,andHengJi. Exe-
cutablecodeactionselicitbetterllmagents. arXivpreprintarXiv:2402.01030,2024.
Chunqiu Steven Xia, Yinlin Deng, Soren Dunn, and Lingming Zhang. Agentless: Demystifying
llm-basedsoftwareengineeringagents. arXivpreprintarXiv:2407.01489,2024.
Siqiao Xue, Caigao Jiang, Wenhui Shi, Fangyin Cheng, Keting Chen, Hongjun Yang, Zhiping
Zhang, Jianshan He, Hongyang Zhang, Ganglin Wei, et al. Db-gpt: Empowering database in-
teractionswithprivatelargelanguagemodels. arXivpreprintarXiv:2312.17449,2023.
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,
ChengyuanLi,DayihengLiu,FeiHuang,GuantingDong,HaoranWei,HuanLin,JialongTang,
Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jin-
gren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin
Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao,
Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wen-
binGe,XiaodongDeng,XiaohuanZhou,XingzhangRen,XinyuZhang,XipinWei,Xuancheng
Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu,
ZeyuCui, ZhenruZhang, ZhifangGuo, andZhihaoFan. Qwen2technicalreport, 2024a. URL
https://arxiv.org/abs/2407.10671.
JohnYang, CarlosEJimenez, AlexanderWettig, KilianLieret, ShunyuYao, KarthikNarasimhan,
and Ofir Press. Swe-agent: Agent-computer interfaces enable automated software engineering.
arXivpreprintarXiv:2405.15793,2024b.
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.
React: Synergizing reasoning and acting in language models, 2023. URL https://arxiv.
org/abs/2210.03629.
Zhengqing Yuan, Ruoxi Chen, Zhaoxu Li, Haolong Jia, Lifang He, Chi Wang, and Lichao Sun.
Mora: Enablinggeneralistvideogenerationviaamulti-agentframework, 2024. URLhttps:
//arxiv.org/abs/2403.13248.
Daoguang Zan, Ailun Yu, Wei Liu, Dong Chen, Bo Shen, Wei Li, Yafen Yao, Yongshun Gong,
XiaolinChen,BeiGuan,etal. Codes:Naturallanguagetocoderepositoryviamulti-layersketch.
arXivpreprintarXiv:2403.16443,2024.
Fengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang
Lou,andWeizhuChen. Repocoder: Repository-levelcodecompletionthroughiterativeretrieval
andgeneration. arXivpreprintarXiv:2303.12570,2023.
13KechiZhang,JiaLi,GeLi,XianjieShi,andZhiJin. Codeagent: Enhancingcodegenerationwith
tool-integratedagentsystemsforreal-worldrepo-levelcodingchallenges,2024a. URLhttps:
//arxiv.org/abs/2401.07339.
YuntongZhang,HaifengRuan,ZhiyuFan,andAbhikRoychoudhury.Autocoderover:Autonomous
programimprovement. arXivpreprintarXiv:2404.05427,2024b.
Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Y Wu, Yukun Li,
HuazuoGao,ShirongMa,etal.Deepseek-coder-v2:Breakingthebarrierofclosed-sourcemodels
incodeintelligence. arXivpreprintarXiv:2406.11931,2024.
AlbertO¨rwall.Moatlesstools.https://github.com/aorwall/moatless-tools,2024.
14A Appendix
A.1 Detailsofthegraphdatabaseschema
This schema is designed to abstract code repositories into code graphs for Python, where nodes
representsymbolsinthesourcecode,andedgesrepresentrelationshipsbetweenthesesymbols.
A.1.1 NodeTypes
EachnodeinthecodegraphrepresentsadifferentelementwithinPythoncode,andeachnodetype
has a set of attributes that encapsulate its meta-information. The node types and their respective
attributesareasfollows:
GraphDatabaseSchema: Nodes
## Nodes
MODULE:
Attributes:
- name (String): Name of the module (dotted name)
- file_path (String): File path of the module
CLASS:
Attributes:
- name (String): Name of the class
- file_path (String): File path of the class
- signature (String): The signature of the class
- code (String): Full code of the class
FUNCTION:
Attributes:
- name (String): Name of the function
- file_path (String): File path of the function
- code (String): Full code of the function
- signature (String): The signature of the function
FIELD:
Attributes:
- name (String): Name of the field
- file_path (String): File path of the field
- class (String): Name of the class the field belongs to
METHOD:
Attributes:
- name (String): Name of the method
- file_path (String): File path of the method
- class (String): Name of the class the method belongs to
- code (String): Full code of the method
- signature (String): The signature of the method
GLOBAL_VARIABLE:
Attributes:
- name (String): Name of the global variable
- file_path (String): File path of the global variable
- code (String): The code segment in which the global variable
is defined
15A.1.2 EdgeTypes
Edgesinthecodegraphrepresentvariousrelationshipsbetweenthenodes.Theedgetypeswedefine
andtherelationshipstheysignifyareasfollows:
GraphDatabaseSchema: Edges
## Edges
CONTAINS:
Source: MODULE
Target: CLASS or FUNCTION or GLOBAL_VARIABLE
HAS_METHOD:
Source: CLASS
Target: METHOD
HAS_FIELD:
Source: CLASS
Target: FIELD
INHERITS:
Source: CLASS
Target: CLASS (base class)
USES:
Source: FUNCTION or METHOD
Target: GLOBAL_VARIABLE or FIELD
Attributes:
- source_association_type (String): FUNCTION, METHOD
- target_association_type (String): GLOBAL_VARIABLE, FIELD
16A.2 Real-WorldApplication
In this section, we present the WebUI interface for CODEXGRAPH, showcasing its five practical
applications: CodeChat,CodeDebugger,CodeUnittestor,CodeGenerator,andCodeCommentor.
Theinterfaceisdesignedtofacilitateuserinteraction,providingastreamlinedandintuitiveenviron-
mentforvariouscode-relatedtasks. WebuilttheWebUIinterfaceusingStreamlit8,apowerfuland
user-friendlyframeworkthatallowsfortherapiddevelopmentofinteractivewebapplications.
Figure5: WebUIfortheCodeChat,usedforansweringanyquestionsrelatedtocoderepositories.
(a)CodeDebugger (b)CodeUnittestor
(c)CodeGenerator (d)CodeCommentor
Figure6: WebUIforCodeDebugger,CodeUnittestor,CodeGenerator,andCodeCommentor.
To experience our application firsthand, you can visit ModelScope-Agent and navigate to the
CODEXGRAPH9. This repository provides a detailed guide on how to set up and interact with the
variousapplicationswehavedescribed.
8Streamlit:https://streamlit.io/
9https://github.com/modelscope/modelscope-agent/tree/master/apps/
codexgraph_agent
17A.2.1 ExampleofCodeChat
Code Chat allows users to inquire about a code repository, providing insights into code structure
andfunctionusage. Thisfunctionalityisparticularlyusefulforunderstandingcomplexcodebases,
identifyingdependencies,andexploringtheusageofspecificclasses,methods,andfunctions.
HereisanexampleofCodeChat. Theuser’squestionis“Summarizethe‘CodexGraphAgentChat’
class,whathasmethod,andwhatfor”.
Figure 7: Using Cypher queries to retrieve information about the ‘CodexGraphAgentChat’ class,
fromthecoderepository.
Figure 8: Once the necessary information is gathered, Code Chat constructs a comprehensive re-
sponse to the user’s question. This response includes a summary of the ‘CodexGraphAgentChat’
class,alistofitsmethods,andadescriptionofwhateachmethoddoes.
18A.2.2 ExampleofCodeDebugger
The Code Debugger diagnoses and resolves bugs by applying iterative reasoning and information
retrievaltosuggesttargetedfixes. ItutilizesCypherqueriestoanalyzethecoderepository,identify
thecauseoftheissue,andrecommendprecisemodifications.
Here is an example of Code Debugger. The user’s input is a real issue10 where the outcome does
notmatchtheexpectedbehavior. TheCodeDebuggerfirstanalyzestheproblem,thenusesCypher
queriestoretrieverelevantinformationandinferthecauseofthebug. Finally,itprovidesanexpla-
nationofthebugandsuggeststhelocationforthemodification.
Figure 9: User input issue detailing the problem where the outcome does not match the expected
behavior.
Figure10: AnalyzingtheproblemandretrievinginformationusingCypherqueries.
10https://github.com/modelscope/modelscope-agent/pull/549
19Figure11: ExecutingCypherqueriestosearchthecodeforrelevantinformation.
Figure12: Analyzingtheretrievedinformationtoidentifypotentialcausesofthebug.
20Figure13: PerformingadditionalCyphercodesearchestogathermoreinformation.
Figure14: Inferringthecauseofthebugbasedontheanalysisoftheretrievedinformation.
Figure15: Identifyingthepreciselocationofthebuginthecodebase.
21Figure16: Providingadetailedexplanationoftheissueandtheunderlyingcauseofthebug.
Figure17: Suggestingthefirstmodificationtoresolvethebug.
Figure18: Suggestingthesecondmodificationtoensurethebugisresolved.
222024/8/4 00:01 code_unittester
You will format your final output as follows:
{{code_with_annotations}}
Professional Unittest Notes:
1. use import unittest
2. Setup and Teardown Methods: Define setup methods to initialize any resources needed for the tests.
3. Test Case Descriptions: Clearly describe each test case, specifying what functionality it is testing and the expected outcome.
4. Test Methods: Implement individual test methods for each functionality of the class. Use descriptive names for test methods to indicate what is being tested.
5. Assertions: Use assertions to verify that the actual outcomes match the expected results. Cover positive, negative, and edge cases in the assertions.
6. Mocking and Stubbing: Mock dependencies and external services to isolate the functionality of the class being tested.
7. Code Coverage: Ensure that all methods and code paths of the class are covered by the tests. Aim for high code coverage to minimize untested scenarios.
Example of professional unittest for a class:
import unittest
from unittest.mock import Mock, patch
from mymodule import User, Database, InvalidCredentialsError
class TestUser(unittest.TestCase):
def setUp(self):
# Setup resources needed for the tests
self.user = User()
self.mock_db = Mock(spec=Database)
self.user.database = self.mock_db
def test_login_success(self):
# Test successful login
self.mock_db.get_user.return_value = {'username': 'testuser', 'password': 'correctpassword'}
A.2.3 Exa m presullte = soelf.fuseCr.logoin(d'tesetuseUr', 'ncorriecttptaseswosrdt')or
self.assertTrue(result, "Login should succeed with correct credentials")
# Additional test methods covering all functionalities
Hereisane x ..a.mpleofCodeUnittestor. Theuser’sinputis: ”GeneratetestcasesforTaskManager.”
def test_login_empty_username(self):
TheCodexG r a #p Tehst loaging wieth nemptty wuserniamlelfirstretrieveallmethodsandinheritancerelationshipsin‘TaskManager’,
with self.assertRaises(ValueError):
self.user.login('', 'password')
andthengeneratedetailedtestcasecode.
if __name__ == '__main__':
unittest.main()
🤖 import unittest
from unittest.mock import Mock, patch
# Assuming Task and BaseManager are defined elsewhere in the codebase
from mymodule import TaskManager, Task, BaseManager
class TestTaskManager(unittest.TestCase):
def setUp(self):
# Setup resources needed for the tests
self.task_manager = TaskManager()
# Mocking BaseManager's add_item method
self.task_manager.add_item = Mock(spec=BaseManager.add_item)
def test_add_task_success(self):
# Test successful addition of a task
task_id = 1
title = "Test Task"
description = "This is a test task"
user_id = 1
self.task_manager.add_item.return_value = True
result = self.task_manager.add_task(task_id, title, description, user_id)
self.assertTrue(result, "Task addition should succeed")
self.task_manager.add_item.assert_called_once_with(task_id, Task(task_id, title, description, user_id))
def test_add_task_failure(self):
# Test failure of task addition when task_id already exists
task_id = 1
title = "Test Task"
description = "This is a test task"
user_id = 1
self.task_manager.add_item.return_value = False
result = self.task_manager.add_task(task_id, title, description, user_id)
self.assertFalse(result, "Task addition should fail if task_id already exists")
self.task_manager.add_item.assert_called_once_with(task_id, Task(task_id, title, description, user_id))
def test_add_task_invalid_input(self):
# Test addition of a task with invalid input
with self.assertRaises(TypeError):
self.task_manager.add_task("invalid_id", "Test Task", "This is a test task", 1)
def test_init_method(self):
# Test the __init__ method to ensure it correctly initializes the TaskManager
self.assertIsInstance(self.task_manager, TaskManager, "TaskManager instance should be created")
self.assertIsInstance(self.task_manager, BaseManager, "TaskManager should inherit from BaseManager")
if __name__ == '__main__':
unittest.main()
This unittest suite for the TaskManager class includes tests for the add_task method, ensuring it correctly handles successful task addition, failure due to existing task_id, and invalid input. It also tests the __init__ method to ensure proper initialization of the
TaskManager instance. Mocking is used to isolate the add_task method's behavior from the add_item method of the BaseManager class, allowing for focused testing of TaskManager's functionality.
Figure19: Generateddetailedunittestcodeforthe‘TaskManager’class,coveringitsmethodsand
inheritancerelationships.
A.2.4 ExampleofCodeGenerator
The user has requested a function to retrieve the number of input and output tokens of ‘CypherA-
gent’. However,thechallengeisidentifyingthecorrespondingfieldswithin‘CypherAgent’asthis
informationisnotprovidedintheuser’sinput.
Figure20: Thethoughtprocessindetermininghowtoidentifytherelevantfields.
23
localhost:8501/code_unittester 2/2Figure 21: By using Cypher queries, it was discovered that the corresponding fields are ‘in-
put token num’and‘output token num’,whichenablesthegenerationofthecorrectcode.
A.2.5 ExampleofCodeCommentor
TheCodeCommentoranalyzescodetoprovidedetailedcomments,enhancingcodereadabilityand
maintainability.Itleveragesthecodegraphdatabasetounderstandthecode’sstructureandbehavior,
ensuringaccurateandinformativecomments.
Figure22: Thethoughtprocess: Understandthe‘Task’classand‘add item’method.
Figure23:ByusingCypherqueries,thespecificimplementationofthereturnfunctionwasobtained,
andthereturntypewasclarified.
24