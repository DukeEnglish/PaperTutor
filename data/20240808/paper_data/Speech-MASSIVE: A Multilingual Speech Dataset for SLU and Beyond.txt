August,2024
Speech-MASSIVE: A Multilingual Speech Dataset for SLU
and Beyond
BeomseokLee123,IoanCalapodescu2,MarcoGaido3,MatteoNegri3,LaurentBesacier2
1UniversityofTrento,Italy
2NAVERLABSEurope,France
3FondazioneBrunoKessler,Italy
Abstract
We present Speech-MASSIVE, a multilingual Spoken Language Understanding (SLU) dataset com-
prising the speech counterpart for a portion of the MASSIVE textual corpus. Speech-MASSIVE cov-
ers 12 languages from different families and inherits from MASSIVE the annotations for the intent
prediction and slot-filling tasks. Our extension is prompted by the scarcity of massively multilin-
gual SLU datasets and the growing need for versatile speech datasets to assess foundation models
(LLMs, speech encoders) across languages and tasks. We provide a multimodal, multitask, multi-
lingual dataset and report SLU baselines using both cascaded and end-to-end architectures in vari-
ous training scenarios (zero-shot, few-shot, and full fine-tune). Furthermore, we demonstrate the
suitability of Speech-MASSIVE for benchmarking other tasks such as speech transcription, language
identification, and speech translation. The dataset, models, and code are publicly available at:
https://github.com/hlt-mt/Speech-MASSIVE
1. Introduction MASSIVEpubliclyunderCC-BY-NC-SA 4.0license.1
Multilingual speech corpora have limited coverage of
Besides detailing the creation process involving a
speech-related tasks, primarily focusing on automatic
crowdsourcing-based protocol for data collection and
speech recognition (ASR) [1,5,16,23] and speech
quality control, this paper presents baseline SLU re-
translation(ST)[7,10,18,19],whileneglectingspoken
sults on Speech-MASSIVE. Our results with both cas-
language understanding(SLU –thetask ofextracting
cade and end-to-end architectures trained in differ-
semantic information from spoken utterances, which
ent conditions (zero-shot, few-shot, full fine-tune)
typicallyinvolvessubtaskslikeintentdetectionandslot
will enable future comparisons and tracking SLU ad-
filling). Unliketextprocessing,whereextensiveefforts
vancements compared to the more mature field of
in naturallanguage understanding (NLU) have led to
NLU. Lastly, we showcase Speech-MASSIVE’s versatil-
resourcescoveringawiderangeoflanguages[8,13,14,
ity through additional experiments on ASR, LID, and
21],SLUdatasetsaremainlyEnglish-centric[3],with
ST.
fewexceptions[6,11,12].
2. Speech-MASSIVE
OurgoalistobridgethegapinmultilingualSLUdraw-
inginspirationfrom[11]andcollectingspeechrecord- 2.1. Data collection and validation process
ingsinmultiplelanguages. WestartwiththeMASSIVE WecreatedthespeechcounterpartoftextualMASSIVE
NLU(i.e.textual)dataset[8],anidealfoundationdue data by recruiting native speakers through the Pro-
to its size, domain diversity, and broad coverage of lificcrowdsourcingplatform.2 Afirstgroupofworkers
languages, intent, and slot types. Developed by com- was instructed to record the spoken version of MAS-
missioning professional translators to localize the En- SIVEsentenceswithguidelinesemphasizingtheimpor-
glish SLURP dataset [3] into 51 languages, MASSIVE tanceofaccurateandnaturalreading,aswellasproper
comprises1Mlabeledutterancesspanning18domains, recordingconditionsandstrictadherencetothecorre-
with60intentsand55slots. Ourcontribution,Speech- spondingtext. Toensurehighfinaldataquality,asec-
MASSIVE, spans 12 languages from diverse families: ondgroupofnativespeakersvalidatedtherecordedut-
Arabic, German, Spanish, French,Hungarian, Korean, terances.Duringvalidation,participantsweredirected
Dutch,Polish,EuropeanPortuguese,Russian,Turkish, to read the original text, listen to the recording, and
and Vietnamese. It also facilitates evaluation across
various speech tasks beyond SLU, including ASR, ST,
1https://hf.co/datasets/FBK-MT/Speech-MASSIVE
and language identification(LID).We release Speech- 2https://www.prolific.com,Compensated£9perhour.
Correspondingauthor:beomseok.lee@unitn.it
4202
guA
7
]LC.sc[
1v00930.8042:viXraSpeech-MASSIVE:AMultilingualSpeechDatasetforSLUandBeyond
label it as valid or invalid. Those marked as invalid
underwentaseconditerationofthistwo-step(record-
ingandvalidation)process. Aftertheseconditeration,
the process concluded, irrespective of the outcome of
the second validation phase, to avoid potentially end-
lesscycles. Thisdecisionwasalsoinformedbytheob-
servationthat,uponinspecting theinvalid recordings,
wefoundsomeweremarkedassuchnotduetoalack
of adherence of the speech to the text but because of
grammatical errors in the original MASSIVE dataset
text. Correctingthese errorswas beyondthe scope of
Table 1: Speech-MASSIVE’s overall statistics. ‘# hrs’
ourwork.
displaystherecordingdurationforallsamples(includ-
To further enhance the reliability of the collected inginvalid),while‘#spk(Male/Female/Unknown)’in-
dataset, we implemented two additional precautions. dicatesthenumberofspeakersforallthesamples(in-
Duringtherecordingphase,weinstructedparticipants cludinginvalid). Thelast2columns(‘WER’,and‘CER’)
to review their own recordings before proceeding to measuresWhisperASRperformance.
the next sample, allowing them to re-recordif the au-
diowasnotproperlyacquired. Additionally,intheval- total
lang split #sample #valid #hrs WER CER
#spk(M/F/U)
idationstep,fourspeechutteranceswerechosenfrom train-115 115 115 0.14 8(4/4/0) - -
ar
Common Voice [1] and inserted among the samples dev 2033 2027 2.12 36(22/14/0) 31.75 14.43
test 2974 2962 3.23 37(15/17/5) 34.19 15.85
for validation. Out of these four quality control sam- train-115 115 115 0.15 7(3/4/0) - -
de train-full 11514 11201 12.61 117(50/63/4) - -
ples, two intentionally featured audio-transcript mis-
dev 2033 2032 2.33 68(35/32/1) 11.24 3.96
matchestobemarkedasinvalid. Theothertwocases test 2974 2969 3.41 82(36/36/10) 11.84 4.16
train-115 115 115 0.13 7(3/4/0) - -
had perfect audio-transcript alignment to be marked es dev 2033 2024 2.53 109(51/53/5) 7.61 3.00
as valid. Care was takentoselect qualitycontrolsam- test 2974 2948 3.61 85(37/33/15) 8.95 3.76
train-115 115 115 0.12 103(50/52/1) - -
pleswithclearandintelligibleaudio.Validationresults fr train-full 11514 11481 12.42 103(50/52/1) - -
dev 2033 2031 2.20 55(26/26/3) 10.20 4.42
from a Prolific user were retained only if they accu-
test 2974 2972 2.65 75(31/35/9) 11.09 4.71
rately assessed all four quality control samples. Any train-115 115 115 0.12 8(3/4/1) - -
hu
dev 2033 2019 2.27 69(33/33/3) 25.96 10.93
mistakes led to the disregarding of their validations, test 2974 2932 3.30 55(25/24/6) 20.98 6.01
requiringtheentiresetofsamplesfromthatusertobe ko train-115 115 115 0.14 8(4/4/0) - -
dev 2033 2032 2.12 21(8/13/0) 25.29 7.13
re-validatedbyotherparticipants. test 2974 2970 2.66 31(10/18/3) 26.42 8.04
train-115 115 115 0.12 7(3/4/0) - -
nl
dev 2033 2032 2.14 37(17/19/1) 11.03 3.98
2.2. Overall statistics test 2974 2959 3.30 100(48/49/3) 10.52 3.82
train-115 115 115 0.10 7(3/4/0) - -
We chose 12 languages based on various criteria. Ini- pl dev 2033 2024 2.24 105(50/52/3) 9.94 4.88
test 2974 2933 3.21 151(73/71/7) 12.58 6.22
tially,weconsideredthenumberofregistereduserson
train-115 115 115 0.12 8(4/4/0) - -
pt
Prolific,sortingthe51languagescoveredinMASSIVE. dev 2033 2031 2.20 107(51/53/3) 11.73 5.10
test 2974 2967 3.25 102(48/50/4) 12.11 5.13
Languageswithfewerthan200userswereexcludedto
train-115 115 115 0.12 7(3/4/0) - -
ru
ensure sufficient workerparticipationtocomplete the dev 2033 2032 2.25 40(7/31/2) 8.55 4.06
test 2974 2969 3.44 51(25/23/3) 8.99 4.57
entireacquisitionandvalidationprocessinreasonable train-115 115 115 0.11 6(3/3/0) - -
tr
time. Italian was also excluded dueto theavailability dev 2033 2030 2.17 71(36/34/1) 16.65 4.56
test 2974 2950 3.00 42(17/18/7) 18.06 5.05
ofthefulldatasetelsewhere[11]. Finally,withaneye train-115 115 115 0.11 7(2/4/1) - -
vi
dev 2033 1978 2.10 28(13/14/1) 16.65 10.5
atthebalancebetweenbudgetconsiderationsandlin-
test 2974 2954 3.23 30(11/14/5) 14.94 9.77
guistic diversity,fromtheremaining 18languages we
selectedArabic,German,Spanish,French,Hungarian,
Korean, Dutch, Polish, EuropeanPortuguese, Russian,
Turkish,andVietnamese.
WecollectedspeechrecordingsforMASSIVE’sdevelop-
mentandtestsplits. Acquiringthefulltrainingdataset
(11,514 utterances for each of the 12 languages) ex-
ceededourbudget. Inaconcession,ouremphasiswas
placed on acquiring comprehensive training data for
French and German, while we obtained limited few-
2Speech-MASSIVE:AMultilingualSpeechDatasetforSLUandBeyond
shot training data consisting of 115 utterances from a NLU model, serving as an upper bound free from
thetrainingsetfortheremaining10languages(train- ASR errors. Secondly, we build a cascaded SLU sys-
115split). tem(§3.3),inwhichanASRcomponenttranscribesin-
put audio and the NLU model utilizes ASR output for
Columns 1-6 of Table 1 provide statistics for the col-
inference. Thirdly, to complete the inventory of SLU
lecteddataset,including, foreachlanguage,theavail-
baselines, we introduce an end-to-end (E2E) model
able data splits, the number of recordings, hours of
(§3.4). We conclude by showcasing the versatility of
speech, and speakers (total, male, female and un-
Speech-MASSIVE beyond SLU, computing additional
known). The “# valid” column indicates the count
baselines for tasks such as speech translation and lan-
of human-validated utterances for each data split af-
guageidentification(§3.5).
ter the two iterations. As a few speech recordings re-
mainedinvalidatedafterourtworecording-validation
3.1. NLU/SLU training conditions and metrics
cycles, we retained for each utterance the candidate
To simulate different training resource scenarios, we
withthelowestWordErrorRate(WER)astranscribed
reportperformanceinthreedifferentsettings:(a)Zero-
using Whisper [17]. This ensures speech availability
shot: wetrainthemodelonlywithonelanguage data
forallMASSIVEutterances,evenifsomemaynotper-
from the train split (11,514 utterances) and evaluate
fectly align with the reference transcript. Additional
in all other different languages; (b) Few-shot: we em-
information regarding this is included in the corpus
ploy subsets (115 examples) for each of the 12 non-
metadata.
Englishlanguages,aligningwithourtrain-115split. 5
Additionally, we integrate the full zero-shot training
2.3. ASR assessment
split to enrich the multilingual training dataset, total-
To assess Speech-MASSIVE in multilingual ASR, we
ing 12.8k samples for training; (c)Full fine-tune (NLU
used Whisper, since it is one of the recent state-of-
only): 11,514 training examples of all 12 languages
the-art multilingual speech recognition models. We
are pooled (138k samples for training). We assess in-
selected Whisper-large-v3,3 utilizing it without addi-
tent prediction in a given text or speech with intent
tional fine-tuning for our ASR evaluation. Table 1
accuracy6.
shows WER and character error rate (CER) across
languages and data splits. We compared ASR error
3.2. NLU model
rates to those obtained on the FLEURS dataset [5].4
OurNLU system usesthemT5encoder-decoderarchi-
FLEURSgenerallyyieldslowerWERs/CERscompared
tecture [22], selected for its superior performance as
toSpeech-MASSIVE.Thesameobservationwasmade
demonstratedin[8],wherethemT5text-to-textmodel
forItalianin[11],whichfollowedarecordingmethod-
outperformed both the mT5 encoder-only model and
ology similar to ours. This suggests that the higher
theXLM-Rmodel[4]. Weuseapre-trainedmT5-base
WERs are likely due tothe inherent difficulty ofMAS-
model,7andfine-tuneboththeencoderanddecoderin
SIVEutterancescomparedtothoseinFLEURS.Further-
asequence-to-sequencemanner.Wesupplysourceand
more, there are still discrepancies between our Whis-
targettextsasdescribedin[8]andshowninFigure2.
permodel’shypothesesandthereferencesintheMAS-
Forinstance,theFrenchsentence(Fr)“oùpuis-jealler
SIVEdataset(e.g.,numbersreportedinlettersinMAS-
ce soir” is annotatedin slots (Fr-Slots) as ‘OtherOther
SIVEreferences),whichwedidnotaddressasoptimiz-
Othertimeofdaytimeofday” and intent (Intent)as “rec-
ingASRWERwasnotourmaingoal. Finally,wecalcu-
ommendation_events”inMASSIVE.Weadaptthosean-
lated the correlation coefficient between WERs (CER
notationsto create source and target textsto be used
for Korean) on Speech-MASSIVE and FLEURS, result-
in training: for the source text (Fr-Src in [NLU]), we
inginavalueof0.96. ThisshowsthatWhisperconsis-
prepend “Annotate:” to the French sentence (Fr); for
tently performs across both datasets, despite Speech-
thetargettext(Fr-Tgt in[NLU]),weconcatenateslots
MASSIVE being more challenging than FLEURS for
(Fr-Slots)andintent(Intent).
ASR.
3. SLU Baselines and Beyond 5train-115coversall18domains,60intents,and55slots(includ-
ingemptyslots).
Inthissection,weestablishseveralSLUbaselines,eval- 6Duetospacelimitations,wereportonlyintentaccuracyscores.
uating them with different training conditions and However,additionalSLUmetrics(e.g.,micro-averagedslotF1,exact
metrics described in §3.1. Firstly (§3.2), we build matchaccuracy,slot-typeF1,slot-valueCER)exhibitasimilartrend
andareavailableintheGitHubrepository. Wereporttheaverage
3https://hf.co/openai/whisper-large-v3 result(andstandarddeviation)ofthree runswith differentseeds.
4Accessible for our 12 languages except Arabic at Allexperimentswereexecutedon1A10080GBGPU.
https://github.com/openai/whisper/discussions/1762 7https://huggingface.co/google/mt5-base
3Speech-MASSIVE:AMultilingualSpeechDatasetforSLUandBeyond
90 87.81 87.43 86.77 87.7 87.8 87.15 87.53 86.97 86.42 86.13 86.59
67788 50505 8 70 8. .0 86 2 78 80 .9.4 3 85.82 77 58 .. 65 15 77 79 .. 59 61 84.73 7 77 6. .8 22 9 77 89 .. 04 89 85.46 7 75 3. .8 11 2 77 79 .. 27 19 85.24 7 77 5. .1 95 6 77 67 .. 99 62 86.57 7 76 5.6 .71 77 88 .. 08 57 85.87 77 46 .. 563 7 77 67 .. 19 13 85.17 66 58 .. 36 25 77 03 .. 39 29 83.13 68.14 7 62 8.9 .79 81.82
64.55
67 41 .. 72 71 78.17
62.34
67 80 .1.5
1
83.4
65.23
78 12 .. 14 37
60 63.43 60.19 60.93 58.65
55 54.56
50
49.27
nl-NL fr-FR de-DE pt-PT ru-RU es-ES pl-PL tr-TR hu-HU vi-VN ko-KR ar-SA
Languages
NLU zero-shot Cascaded SLU zero-shot NLU few-shot Cascaded SLU few-shot NLU fine-tune Cascaded SLU fine-tune
Figure1: NLUvsCascadedSLU(IntentAccuracy)onourSpeech-MASSIVEDataset.
[Original text in MASSIVE] ASR
En) where can i go tonight [<|startofstranscript|>, <|language_id|>,
En-Annot) where can i go [timeofday : tonight] <|transcribe|>, <|notimestamps|>]
En-Slots) Other Other Other Other timeofday E2E SLU
Fr) où puis-je aller ce soir [<|startofstranscript|>, <|language_id|>,
Fr-Annot) où puis-je aller [timeofday:ce soir] <|transcribe|>, <|startoflm|>, <|notimestamps|>]
Fr-Slots) Other Other Other timeofday timeofday LID
Intent) recommendation_events [<|startofstranscript|>]
[NLU] ST
Fr-Src) Annotate: où puis-je aller ce soir [<|startofstranscript|>, <|language_id|>,
Fr-Tgt) Other Other Other timeofday timeofday <|translate|>, <|notimestamps|>]
recommendation_events
[Cascaded SLU] Figure 3: Varioustask controltokensfedto Whisper’s
Fr-ASR) où puis je aller ce soir
decoder.
Fr-Src) Annotate: où puis je aller ce soir
Fr-Tgt) Other Other Other timeofday timeofday
recommendation_events
[E2E SLU]
Fr-Tgt) où puis-je aller ce soir | Other Other Other
timeofday timeofday | recommendation_events
TheSLUintentaccuracyscoresinFigure1revealthat
Figure 2: Input/Output formatting across NLU/SLU
processing automatically transcribed utterances intro-
tasks. En: originalEnglishtext. Fr: Frenchtranslation
ducesperformancedropsofvaryingmagnitudeacross
in MASSIVE. Annot, Slots and Intent: slot and intent
the different languages and training modes. This is
annotationofMASSIVE.
especially notable for languages with lower ASR qual-
ity(i.e., higher WER), such as Ar, Hu, Ko, Tr, and Vn.
Thissupportsourexpectationsaboutthedifficultyfor
the downstream textual NLU component of the SLU
Figure1displaystheintentaccuracyresultsofourNLU
systemacrossalllanguagesandmodes(zero-shot,few- cascade to handle unrecoverable transcription errors.
As a matter of fact, in zero-shot mode, the distance
shot, full fine-tune), along with those of the cascaded
SLU models discussed in §3.3. Unsurprisingly, NLU with the text-onlyupper-bound NLU system is consid-
performanceincreaseswhenmovingfromzero-shotto erablysmallerforlanguagesfeaturinghigherASRqual-
ity. Similar to what we observed for NLU (§3.2), cas-
fullfine-tuneregimes. Also,asexpected,higherscores
caded SLU performance in few-shot mode improves
are observed for languages (Nl, Fr, De, Pt, Ru, Es and
thanks to the additional multilingual data. The gains
Pl)thatarebetterrepresentedinthemC4multilingual
datasetusedtotrainmT5model[22].Finally,thehigh- are particularly significant for languages with lesser
representationin mT5model, such as Tr, Vn, Ko, and
est results align with those reported in the MASSIVE
Ar. Lastly in full fine-tune mode, leveraging a larger
paper[8],servingasasuitablereferenceupperbound
forcomparisonswiththeSLUmodelsdiscussedinthe multilingual training dataset leads to substantial per-
formanceenhancements. Whilethegainsarevariable,
followingsection.
we observe that: i) for some languages (i.e. De, Ru,
3.3. Cascaded SLU model andEs),thegapwiththehighestresultsofthetextual
NLUupperboundshrinkstolessthantwopoints,while
We develop a cascaded SLU system in which an
ii)foralllanguages,thescoresaresignificantlyhigher
ASR modelbasedonWhisper-large-v3 transcribesthe
than those achieved by the textual NLU models deal-
speech, and the same NLU models of §3.2 (zero-shot,
ing with clean input not only in zero-shot,but also in
few-shot, full fine-tune) predict slots and intent from
few-shotmode.
thetranscribedtexts.
4
)%(ycaruccA
tnetnISpeech-MASSIVE:AMultilingualSpeechDatasetforSLUandBeyond
Table 2: Intent accuracy of cascaded and E2E SLU. Both E2E SLU zero-shot and few-shot models are trained
either with initial English train set of [3] (En) or with French train set of Speech-MASSIVE (Fr). We exclude
French (*) from the average as fr-FR scores are no longer zero/few-shot when French is used as the training
language.
Casc. (En) E2E(En) E2E(Fr) Casc. (En) E2E(En) E2E(Fr)
lang
zero-shot zero-shot zero-shot few-shot few-shot few-shot
ar 49.27±0.90 33.04±4.74 40.00±2.44 54.56±0.73 57.71±1.46 61.22±1.74
de 76.29±0.14 70.68±1.37 73.91±0.73 78.08±0.50 78.64±0.65 78.45±0.64
es 75.70±0.19 73.12±0.75 78.62±0.41 78.05±0.33 79.79±0.66 80.59±0.31
fr 75.61±0.48 68.43±2.30 85.87±0.26* 77.56±0.13 77.11±0.77 85.93±0.35*
hu 63.43±0.92 36.62±1.49 42.28±2.20 68.70±0.80 60.75±2.40 63.93±0.19
ko 60.93±0.84 57.96±2.26 66.09±1.86 68.11±0.04 72.82±0.23 74.09±0.73
nl 78.82±0.45 65.17±0.57 67.24±1.44 78.93±0.34 77.49±0.77 77.37±0.47
pl 74.57±0.37 64.82±1.51 64.38±1.29 76.11±0.39 74.85±0.58 76.88±1.37
pt 73.12±0.49 62.91±1.97 72.60±1.01 77.21±0.65 78.15±1.16 80.02±0.29
ru 75.96±0.19 69.06±1.71 74.75±0.28 76.96±0.08 79.22±0.67 79.51±0.26
tr 65.32±0.61 47.60±3.08 55.08±1.09 70.32±0.48 69.44±1.62 71.14±1.15
vi 60.19±0.39 35.44±1.48 49.67±2.30 64.77±0.98 63.36±1.69 68.71±0.33
avg. 69.10±0.19 57.07±1.82 62.24±0.92 72.45±0.32 72.45±0.53 73.81±0.58
3.4. E2E SLU model inbothzero-shotandfew-shotmodes. Itisworthnot-
TocompletetheinventoryofSLUbaselinesforcompar- ing that the comparison between the two approaches
ison,weintroduceanend-to-end(E2E)SLUmodel: a isfaironlywhenusingtheEnglishtrainset(En),since
direct solution that bypasses intermediate text repre- they utilize the same training utterances albeit in dif-
sentations (ASR transcripts). We utilize Whisper, fol- ferent modalities (written form for cascade and spo-
lowing the approachproposedin [20], which showed kenformforE2E).Inthiscondition(En),forzero-shot
superior performance compared to cascaded systems mode,cascaded SLU outperformsE2E SLU for all lan-
and other speech encoders like wav2vec2.0 [2] and guages. In few-shot mode, we note a different trend,
HuBERT [9]. Model training follows a sequence-to- withcascadedandE2Emodelsexhibitingsimilaraver-
sequence approach, with predictions extended to in- age performance. Employing the French training set
clude transcript, slots, and intent. This allows us from Speech-MASSIVE (Fr), E2E SLU surpasses mod-
to leverage both speech and text information in the els trained on the English dataset from [3] (En) in
model’s predictions. We introduce an additional sep- bothzero-shotandfew-shotmodes.Inzero-shotmode,
arator “|” between the tasks, allowing Whisper’s to- we observe improvements of more than 5 points for
kenizer to tokenize the target text as is, without the 9 out of 11 languages. In few-shot mode, although
need to add slots or intents to the original vocabu- the influence of the training language (En vs Fr) di-
lary. Two specific tokens, “|” and “_”, are removed minishesduetomultilingualtraining,usingFrenchas
from Whisper’s suppressed token list, as they are re- the majority language still yields better performance
quired for predicting SLU outputs as task separators thanusing English. Theseresultshighlight thesignifi-
and in certain intent values. In zero-shot mode, we cantinfluenceofthe‘traininglanguage’ontheperfor-
fine-tune Whisper-large-v3 with either a) the English mance of E2E SLU models in zero/few-shot settings.
train set of [3], or b) the French train set of Speech- Speech-MASSIVEprovidesauniqueopportunitytoex-
MASSIVE.Thesetwoconditions(EnvsFr)allowusto plorethisintriguingobservationfurther.Finally,exam-
investigatetheimpactofthetraininglanguageonzero- iningFrench(Fr)resultsrepresentingthefullfine-tune
shot E2ESLU acrossallotherlanguages. Additionally, modeforthislanguage,E2ESLUachievesintentaccu-
infew-shotmode,wefine-tuneWhisper-large-v3with racyof85.87%,comparedto84.73%forcascadedSLU
the English or French train sets, along with train-115 and87.43%forNLUgiveninFig.1.
splits from other languages. We do not provide a full
3.5. Other baselines
fine-tune E2E SLU mode since only two languages in
Speech-MASSIVEaresupportedbyfulltrainsplits. We conclude our experiments using Whisper-large-v3
without any finetuning to compute other baselines
Table2comparescascadedandE2ESLUperformance
5Speech-MASSIVE:AMultilingualSpeechDatasetforSLUandBeyond
Table3: LIDaccuracyandSTBLEUresultswithWhisper-large-v3onSpeech-MASSIVE.
lang ar de es fr hu ko nl pl pt ru tr vi
split dev test dev test dev test dev test dev test dev test dev test dev test dev test dev test dev test dev test
LIDaccuracy 90.9 89.5 98.9 98.4 99.0 98.6 98.7 98.9 94.6 95.8 99.1 98.7 94.8 94.9 95.3 94.6 95.9 96.0 99.1 98.8 96.1 96.0 90.7 93.2
STBLEU 17.2 16.6 36.7 38.2 38.5 38.2 38.7 40.1 19.4 20.6 19.7 19.5 40.0 38.9 29.9 28.8 32.4 32.3 28.4 28.2 26.7 26.0 18.9 20.2
and demonstrate the versatility of Speech-MASSIVE. References
WeperformLanguageIdentification(LID)andSpeech [1] Rosana Ardila, Megan Branson, Kelly Davis, Michael
Translation(ST)acrossx→enlanguagedirections.Dif-
Kohler,JoshMeyer,MichaelHenretty,ReubenMorais,
ferent types of tokens are fed to Whisper’s decoder Lindsay Saunders, Francis Tyers, and Gregor Weber.
depending on the tasks as shown in Figure 3. Table Common voice: A massively-multilingual speech cor-
3 reports Whisper-large-v3 model’s LID accuracy and pus. In Proceedings of the Twelfth Language Resources
ST BLEU [15] on Speech-MASSIVE. LID is calculated and Evaluation Conference, pages 4218–4222, Mar-
seille,France,2020. 1,2
over all the samples in dev and test splits. For ST, in-
[2] AlexeiBaevski, YuhaoZhou, AbdelrahmanMohamed,
stead, BLEU is computed on subsets of dev and test
andMichaelAuli. wav2vec2.0: Aframeworkforself-
splitsidentifiedusingmetainformationfromMASSIVE
supervisedlearning of speech representations. In Ad-
to exclude samples with localizedtranslation. This fil-
vancesinNeuralInformationProcessingSystems33:An-
tering is necessary to ensure an accurate assessment
nual Conference on Neural Information Processing Sys-
of translation quality, as localized references may in-
tems 2020, NeurIPS 2020, December 6-12, 2020, vir-
troducediscrepanciesinwordchoice(see§1). Besides tual,2020. 5
indicating the versatilityof Speech-MASSIVE for eval- [3] EmanueleBastianelli,AndreaVanzo,PawelSwietojan-
uation purposes, our additional baselines on speech- ski,andVerenaRieser. SLURP:Aspokenlanguageun-
related tasks offer valuable reference scores for cross- derstanding resource package. In Proceedings of the
task comparisonsandforexploringcollaborativesolu- 2020ConferenceonEmpiricalMethodsinNaturalLan-
tionstoleveragepotentialmutualbenefits. guage Processing (EMNLP), pages 7252–7262,Online,
2020.AssociationforComputationalLinguistics. 1,5
4. Conclusion [4] Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
We introduced Speech-MASSIVE, a multilingual SLU
Guzmán, EdouardGrave,MyleOtt,LukeZettlemoyer,
dataset spanning 12 languages for intent prediction
andVeselinStoyanov. Unsupervisedcross-lingualrep-
andslot-fillingtasks. Alongsidedatasetcreation,wees-
resentationlearningatscale.InProceedingsofthe58th
tablishedbaselinesforSLUacrossvariousresourceand
AnnualMeetingoftheAssociationforComputationalLin-
architecture configurations. Additionally, we show- guistics, pages 8440–8451, Online, 2020. Association
cased Speech-MASSIVE’s versatility beyond SLU, ex- forComputationalLinguistics. 3
tending to tasks such as ASR, LID, and ST. With its [5] Alexis Conneau, Min Ma, SimranKhanuja, Yu Zhang,
diverse array of native speakers and recording envi- Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara
ronments,Speech-MASSIVEholdspromiseasabench- Rivera, and Ankur Bapna. Fleurs: Few-shot learn-
mark for multilingual, multimodal, and multi-task ing evaluation of universal representations of speech.
In 2022 IEEE Spoken Language Technology Workshop
speech research. Future research opportunities in-
(SLT),pages798–805,2023. 1,3
clude exploring further the influence of training lan-
[6] Alice Coucke, Alaa Saade, Adrien Ball, Théodore
guagesonzero/few-shotSLUperformance,thoroughly
Bluche, Alexandre Caulier, David Leroy, Clément
comparing cascade and E2E SLU solutions, assess the
Doumouro, Thibault Gisselbrecht, Francesco Calta-
effectofincludingmulti-taskandmultilingualcorpora
girone, Thibaut Lavril, Maël Primet, and Joseph
inthetrainingofspeechfoundationmodels,andpush-
Dureau. Snips voice platform: an embedded spoken
ing the boundaries of E2E multi-task speech systems
language understanding system for private-by-design
beyondourbaselines. voiceinterfaces. CoRR,abs/1805.10190,2018. 1
[7] MattiaA.DiGangi,RoldanoCattoni,LuisaBentivogli,
Acknowledgements
MatteoNegri,andMarcoTurchi. MuST-C:aMultilin-
The speech collection was funded by EU Horizon gualSpeechTranslationCorpus. InProceedingsofthe
Europe (HE) Research and Innovation programme 2019 Conferenceof the NorthAmerican Chapter of the
AssociationforComputationalLinguistics: HumanLan-
grant No 101070631. We also acknowledge the sup-
guageTechnologies,Volume1(LongandShortPapers),
port of the PNRR project FAIR - Future AI Research
pages 2012–2017,Minneapolis, Minnesota, 2019.As-
(PE00000013),undertheNRRPMURprogramfunded
sociationforComputationalLinguistics. 1
bytheNextGenerationEU.
[8] Jack FitzGerald, Christopher Hench, Charith Peris,
6Speech-MASSIVE:AMultilingualSpeechDatasetforSLUandBeyond
Scott Mackie, Kay Rottmann, Ana Sanchez, Aaron meetingoftheAssociationforComputationalLinguistics,
Nash, Liam Urbach, Vishesh Kakarala, Richa Singh, pages311–318,2002. 6
SwethaRanganath,LaurieCrist,MishaBritan,Wouter [16] VineelPratap, Qiantong Xu, AnuroopSriram, Gabriel
Leeuwis, Gokhan Tur, and Prem Natarajan. MAS- Synnaeve, and Ronan Collobert. Mls: A large-scale
SIVE:A1M-examplemultilingualnaturallanguageun- multilingualdatasetforspeechresearch.InInterspeech
derstandingdataset with51typologically-diverselan- 2020.ISCA,2020. 1
guages. In Proceedings of the 61st Annual Meeting of [17] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brock-
the Association for Computational Linguistics (Volume man, Christine McLeavey, and Ilya Sutskever. Robust
1: Long Papers), pages 4277–4302, Toronto, Canada, speechrecognitionvialarge-scaleweaksupervision.In
2023.AssociationforComputationalLinguistics. 1,3, Proceedingsofthe40thInternationalConferenceonMa-
4,8 chineLearning.JMLR.org,2023. 3
[9] Wei-NingHsu,BenjaminBolte,Yao-HungHubertTsai, [18] Elizabeth Salesky, Matthew Wiesner, Jacob Bremer-
KushalLakhotia,RuslanSalakhutdinov,andAbdelrah- man, Roldano Cattoni, Matteo Negri, Marco Turchi,
man Mohamed. Hubert: Self-supervised speech rep- Douglas W. Oard, and Matt Post. The Multilingual
resentation learning by masked prediction of hidden TEDxCorpusfor SpeechRecognition and Translation.
units. IEEE/ACM Transactions on Audio, Speech, and InProc.Interspeech2021,pages3655–3659,2021. 1
LanguageProcessing,29:3451–3460,2021. 5 [19] ChanghanWang,AnneWu,andJuanMiguelPino.Cov-
[10] Javier Iranzo-Sánchez, Joan Albert Silvestre-Cerdà, ost 2: Amassivelymultilingualspeech-to-texttransla-
Javier Jorge, Nahuel Roselló, Adrià Giménez, Albert tioncorpus. CoRR,abs/2007.10310,2020. 1
Sanchis, Jorge Civera, and Alfons Juan. Europarl-st: [20] MinghanWang, YingluLi, JiaxinGuo,XiaosongQiao,
A multilingualcorpus for speech translation of parlia- Zongyao Li, Hengchao Shang, Daimeng Wei, Shimin
mentary debates. In ICASSP 2020 - 2020 IEEE Inter- Tao, Min Zhang, and Hao Yang. Whislu: End-to-end
nationalConferenceonAcoustics,SpeechandSignalPro- spokenlanguageunderstandingwithwhisper. InProc.
cessing(ICASSP),pages8229–8233,2020. 1 Interspeech,pages770–774,2023. 5
[11] Alkis Koudounas, Moreno La Quatra, Lorenzo Vaiani, [21] Weijia Xu, Batool Haider, andSaabMansour. End-to-
LucaColomba,GiuseppeAttanasio,ElianaPastor,Luca end slot alignment and recognition for cross-lingual
Cagliero,andElena Baralis. ITALIC:AnItalian Intent NLU. In Proceedingsofthe 2020ConferenceonEmpir-
Classification Dataset. In Proc. INTERSPEECH 2023, ical Methodsin NaturalLanguage Processing(EMNLP),
pages2153–2157,2023. 1,2,3 pages5052–5063,Online,2020.AssociationforCom-
[12] FabriceLefèvre,DjamelMostefa,LaurentBesacier,Yan- putationalLinguistics. 1
nick Estève, Matthieu Quignard, Nathalie Camelin, [22] Linting Xue, Noah Constant, Adam Roberts, Mihir
Benoît Favre, Bassam Jabaian, and Lina Maria Rojas- Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua,
Barahona.Leveragingstudyofrobustnessandportabil- and Colin Raffel. mT5: A massively multilingualpre-
ity of spoken language understanding systems across trained text-to-text transformer. In Proceedings of the
languagesanddomains: thePORTMEDIAcorpora. In 2021 Conferenceof the NorthAmerican Chapter of the
Proceedings of the Eighth International Conference on AssociationforComputationalLinguistics: HumanLan-
Language ResourcesandEvaluation,LREC2012, Istan- guage Technologies, pages 483–498,Online, 2021.As-
bul, Turkey, May 23-25, 2012, pages 1436–1442.Eu- sociationforComputationalLinguistics. 3,4
ropeanLanguageResourcesAssociation(ELRA),2012. [23] Marcely Zanon Boito, William Havard, Mahault Gar-
1 nerin,ÉricLeFerrand,andLaurentBesacier. MaSS:A
[13] Patrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian largeandcleanmultilingualcorpusofsentence-aligned
Riedel,andHolgerSchwenk. MLQA:Evaluatingcross- spokenutterancesextractedfromtheBible.InProceed-
lingual extractive question answering. In Proceedings ings of the Twelfth Language Resources and Evaluation
of the 58thAnnualMeeting ofthe Association for Com- Conference,pages6486–6493,Marseille,France,2020.
putationalLinguistics,pages7315–7330,Online,2020. EuropeanLanguageResourcesAssociation. 1
AssociationforComputationalLinguistics. 1
[14] Nikita Moghe, Evgeniia Razumovskaia, Liane Guil-
lou, Ivan Vulić, AnnaKorhonen, andAlexandraBirch.
Multi3NLU++: A multilingual, multi-intent, multi-
domaindatasetfornaturallanguageunderstandingin
task-oriented dialogue. In Findings of the Association
forComputationalLinguistics: ACL 2023,pages 3732–
3755,Toronto,Canada,2023.AssociationforCompu-
tationalLinguistics. 1
[15] KishorePapineni,SalimRoukos,ToddWard,andWei-
JingZhu. Bleu: amethodforautomaticevaluationof
machinetranslation. InProceedingsofthe40thannual
7Speech-MASSIVE:AMultilingualSpeechDatasetforSLUandBeyond
A. Appendix
Thehyperparameterusedtotraintheend-to-endSpo-
kenLanguageUnderstandingmodelispresentedinTa-
ble4.
We reportall theevaluationresults Exactmatch accu-
racyinTable5,IntentaccuracyinTable6,Slotmicro-
F1inTable7andbothSlottypeF1andSlotvalueCER
inTable8.
Evaluationcodeusedtocalculate themetricsofExact
match accuracy, Intent accuracy and Slot micro-F1 is
fromMASSIVE[8]implementation. 8 ForSlottypeF1
andslotvalueCERevaluation,weuseS3PRLtoolkit.9
8https://github.com/alexa/massive/blob/main/src/massive/utils/training_utils.py
9https://github.com/s3prl/s3prl/blob/aa3ba844bfe2b5402b7f345cbebd72b33ef6aeff/s3prl/metric/slot_filling.py
8Speech-MASSIVE:AMultilingualSpeechDatasetforSLUandBeyond
Table4: Hyperparametersettingsforend-to-endSLUzero-shotandfew-shotmodels.
adambeta1 0.8
adambeta2 0.999
adamepsilon 0
addseparator ‘|’
freezefeatureencoder FALSE
gradientaccumulationsteps 2
gradientcheckpointing FALSE
labelsmoothingfactor 0
learningrate 0.00001
lrschedulertype linear
20000(zero-shot)
maxsteps
25000(few-shot)
perdeviceevalbatchsize 8
perdevicetrainbatchsize 8
targetformatcontent transcriptslotsintent
task transcribe
tokenstoremovefromsuppress ‘G˙|’,‘_’
2000(zero-shot)
warmupsteps
2500(few-shot)
Table5: ExactMatchAccuracyforallthesettings.
zero-shot few-shot fine-tune
lang NLU CascadedSLU E2ESLU(En) E2ESLU(Fr) NLU CascadedSLU E2ESLU(En) E2ESLU(Fr) NLU CascadedSLU
ar-SA 28.39±1.16 20.93±1.04 17.81±2.47 22.22±1.72 39.48±1.32 27.55±0.78 37.47±0.30 39.76±1.32 64.64±0.58 45.06±0.29
de-DE 51.18±1.02 44.71±1.01 46.71±1.43 49.61±0.87 56.06±1.13 48.28±0.54 56.44±0.62 57.64±0.19 69.70±0.53 60.35±0.34
es-ES 47.71±0.44 44.40±0.60 50.00±0.83 54.55±0.47 52.64±0.39 49.23±0.42 57.42±0.45 59.09±0.36 67.09±0.12 61.84±0.09
fr-FR 45.09±1.11 38.3±0.79 43.67±2.09 65.38±0.43 52.59±1.34 38.88±0.14 53.04±0.55 65.61±0.25 67.44±0.17 46.08±0.29
hu-HU 38.11±1.25 31.46±1.01 19.1±0.83 22.10±1.46 47.00±0.74 37.40±0.48 38.70±2.60 42.57±0.50 68.35±0.46 53.12±0.12
ko-KR 31.97±0.81 30.08±0.81 33.55±2.06 39.70±1.41 43.97±0.20 37.73±0.12 47.81±0.76 49.18±0.95 69.45±0.45 55.34±0.15
nl-NL 52.05±1.10 46.21±1.24 40.51±0.34 42.17±0.79 56.2±0.19 47.89±0.05 53.78±1.16 54.95±0.27 69.72±0.21 59.18±0.17
pl-PL 45.30±0.61 41.32±0.64 39.59±1.68 40.22±0.91 49.22±0.69 44.27±0.63 51.57±0.69 54.75±0.74 65.98±0.40 58.61±0.44
pt-PT 46.35±0.58 39.84±0.74 38.49±1.00 46.55±1.04 52.72±0.87 44.48±0.59 53.64±0.52 56.01±0.26 68.84±0.05 56.92±0.05
ru-RU 48.9±1.06 46.32±1.35 45±1.68 49.86±0.24 53.25±0.34 49.78±0.40 57.79±0.57 58.09±0.07 70.17±0.14 64.52±0.15
tr-TR 37.88±0.25 32.33±0.2 24.56±2.25 30.14±0.74 45.79±0.88 37.56±0.50 44.98±1.71 46.62±0.88 68.91±0.32 55.51±0.47
vi-VN 30.35±0.93 25.96±0.69 13.85±0.39 21.15±1.77 38.25±1.47 31.01±1.26 35.35±1.12 41.35±0.31 65.78±0.24 51.46±0.45
avg 41.94±0.67 36.82±0.65 34.4±1.28 38.02±0.84 48.93±0.71 41.17±0.41 49±0.47 50.91±0.4 68.01±0.18 55.66±0.13
9Speech-MASSIVE:AMultilingualSpeechDatasetforSLUandBeyond
Table6: IntentAccuracyforallthesettings.
zero-shot few-shot fine-tune
lang NLU CascadedSLU E2ESLU(En) E2ESLU(Fr) NLU CascadedSLU E2ESLU(En) E2ESLU(Fr) NLU CascadedSLU
ar-SA 58.65±0.40 49.27±0.90 33.04±4.74 40.00±2.44 65.23±1.23 54.56±0.73 57.71±1.46 61.22±1.74 82.47±0.20 71.13±0.28
de-DE 77.82±0.14 76.29±0.14 70.68±1.37 73.91±0.73 79.49±0.64 78.08±0.50 78.64±0.65 78.45±0.64 86.77±0.21 85.46±0.19
es-ES 76.61±0.27 75.70±0.19 73.12±0.75 78.62±0.41 78.87±0.07 78.05±0.33 79.79±0.66 80.59±0.31 87.15±0.38 85.87±0.27
fr-FR 78.55±0.44 75.61±0.48 68.43±2.30 85.87±0.26 79.91±0.51 77.56±0.13 77.11±0.77 85.93±0.35 87.43±0.41 84.73±0.35
hu-HU 68.14±0.95 63.43±0.92 36.62±1.49 42.28±2.20 72.99±0.61 68.70±0.80 60.75±2.40 63.93±0.19 86.42±0.27 81.82±0.21
ko-KR 62.34±0.93 60.93±0.84 57.96±2.26 66.09±1.86 70.50±0.10 68.11±0.04 72.82±0.23 74.09±0.73 86.59±0.29 83.40±0.30
nl-NL 80.06±0.42 78.82±0.45 65.17±0.57 67.24±1.44 80.4±0.22 78.93±0.34 77.49±0.77 77.37±0.47 87.81±0.30 85.82±0.40
pl-PL 76.63±0.42 74.57±0.37 64.82±1.51 64.38±1.29 77.93±0.28 76.11±0.39 74.85±0.58 76.88±1.37 87.53±0.04 85.17±0.16
pt-PT 75.81±0.05 73.12±0.49 62.91±1.97 72.60±1.01 79.79±0.76 77.21±0.65 78.15±1.16 80.02±0.29 87.70±0.26 85.24±0.24
ru-RU 77.15±0.33 75.96±0.19 69.06±1.71 74.75±0.28 77.92±0.30 76.96±0.08 79.22±0.67 79.51±0.26 87.80±0.22 86.57±0.27
tr-TR 68.65±0.30 65.32±0.61 47.60±3.08 55.08±1.09 73.99±0.37 70.32±0.48 69.44±1.62 71.14±1.15 86.97±0.26 83.13±0.24
vi-VN 64.55±0.80 60.19±0.39 35.44±1.48 49.67±2.30 71.21±1.06 64.77±0.98 63.36±1.69 68.71±0.33 86.13±0.16 78.17±0.59
avg 72.08±0.21 69.10±0.19 57.07±1.82 62.24±0.92 75.69±0.42 72.45±0.32 72.45±0.53 73.81±0.58 86.73±0.13 83.04±0.15
Table7: Micro-avgslotF1forallthesettings.
zero-shot few-shot fine-tune
lang NLU CascadedSLU E2ESLU(En) E2ESLU(Fr) NLU CascadedSLU E2ESLU(En) E2ESLU(Fr) NLU CascadedSLU
ar-SA 36.49±1.89 26.89±1.77 10.23±2.64 16.41±1.33 54.21±1.14 40.20±1.07 36.27±0.69 38.58±0.67 76.50±0.52 54.66±0.27
de-DE 62.49±0.56 55.04±0.21 50.56±1.95 54.12±1.09 69.32±0.76 58.92±0.37 59.56±0.85 62.12±0.54 80.00±0.54 67.59±0.22
es-ES 56.35±1.06 52.09±1.21 47.51±0.55 50.38±0.80 63.35±0.86 58.79±0.82 55.04±2.03 58.39±0.31 76.00±0.10 69.12±0.20
fr-FR 49.57±0.82 37.91±0.45 37.17±2.2 62.30±0.47 62.30±1.81 38.71±0.68 49.93±0.29 62.31±0.22 76.38±0.33 43.19±0.53
hu-HU 44.47±1.03 36.25±1.09 18.54±1.51 22.61±0.67 60.68±0.83 47.16±0.68 42.01±0.24 46.12±0.17 78.70±0.19 59.01±0.07
ko-KR 42.83±1.52 39.29±1.01 23.38±3.55 27.37±1.78 57.20±0.64 47.12±0.09 41.43±0.89 43.30±1.11 80.06±0.60 60.97±0.29
nl-NL 60.92±0.87 52.50±1.39 44.57±0.58 44.88±0.45 68.68±0.34 56.29±0.09 55.90±0.14 57.68±0.30 78.33±0.29 63.33±0.30
pl-PL 52.69±0.96 47.53±1.04 35.93±2.55 40.74±1.07 61.35±0.61 54.69±0.36 51.53±1.29 55.20±0.24 74.14±0.49 65.29±0.35
pt-PT 54.45±0.87 44.50±0.98 32.72±1.31 40.12±2.22 62.49±0.94 50.34±0.65 48.48±1.46 51.13±0.67 77.63±0.19 60.63±0.34
ru-RU 59.73±1.15 55.98±1.00 41.10±3.42 48.70±1.05 64.91±0.71 59.62±0.59 57.40±0.73 59.45±0.47 79.08±0.32 71.60±0.28
tr-TR 46.74±0.96 39.60±0.70 24.44±2.76 29.51±1.09 58.71±0.86 47.94±0.18 45.95±0.88 48.59±0.99 78.58±0.55 60.85±0.51
vi-VN 36.97±1.13 30.24±0.94 12.01±2.04 21.98±3.06 46.02±2.87 37.58±2.45 44.40±1.50 50.26±0.56 75.00±0.52 58.26±0.45
avg 50.31±0.86 43.15±0.81 31.51±1.32 36.07±1.04 60.77±0.93 49.78±0.43 48.99±1.50 51.89±1.50 77.53±0.28 61.21±0.15
Table8: SlottypeF1scoreandslotvalueCERforallthesettings.
SlottypeF1score SlotvalueCER
zero-shot few-shot zero-shot few-shot
lang E2ESLU(En) E2ESLU(Fr) E2ESLU(En) E2ESLU(Fr) E2ESLU(En) E2ESLU(Fr) E2ESLU(En) E2ESLU(Fr)
ar-SA 70.61±1.14 70.70±1.18 78.70±0.50 80.14±0.78 59.39±2.90 48.75±2.52 33.71±0.21 30.81±0.23
de-DE 86.48±0.38 86.65±0.49 89.45±0.67 89.71±0.19 23.23±1.10 20.74±0.84 16.72±0.34 14.59±0.13
es-ES 87.73±0.34 88.81±0.33 89.70±0.24 90.25±0.07 20.05±0.47 15.61±0.39 14.88±0.63 13.42±0.10
fr-FR 86.73±0.82 92.88±0.13 89.62±0.21 92.97±0.04 25.07±1.25 10.99±0.21 17.40±0.24 10.84±0.03
hu-HU 76.20±0.82 77.05±0.45 82.59±0.67 83.75±0.32 43.71±1.25 42.65±0.66 29.12±1.01 25.56±0.05
ko-KR 77.36±2.24 78.62±3.30 84.80±0.37 86.24±0.36 58.56±3.44 50.55±2.92 36.70±1.86 30.89±0.21
nl-NL 86.41±0.28 86.06±0.27 89.11±0.38 89.43±0.20 27.78±0.69 24.65±0.71 17.76±0.29 15.77±0.06
pl-PL 82.83±0.83 83.48±0.34 87.65±0.15 88.30±0.25 28.77±0.93 28.70±0.47 20.58±0.58 17.90±0.25
pt-PT 83.27±0.57 86.23±0.52 89.18±0.30 89.94±0.14 31.32±1.11 25.04±1.63 17.62±0.36 15.78±0.13
ru-RU 84.49±1.00 85.47±0.58 88.76±0.31 89.62±0.20 36.53±2.05 25.28±1.01 17.91±0.66 15.72±0.06
tr-TR 77.86±1.75 78.55±1.93 84.82±0.89 85.78±0.47 37.96±1.59 35.56±1.56 24.43±0.41 21.57±0.44
vi-VN 75.41±0.30 78.67±1.31 83.60±0.91 85.56±0.40 52.96±0.92 41.99±1.96 30.10±0.57 26.57±0.26
avg 81.28±0.78 81.84±0.90 86.50±0.18 87.16±1.50 37.11±0.98 32.68±1.09 23.08±0.21 20.78±1.50
10