[
    {
        "title": "Nadaraya-Watson kernel smoothing as a random energy model",
        "authors": "Jacob A. Zavatone-VethCengiz Pehlevan",
        "links": "http://arxiv.org/abs/2408.03769v1",
        "entry_id": "http://arxiv.org/abs/2408.03769v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03769v1",
        "summary": "We investigate the behavior of the Nadaraya-Watson kernel smoothing estimator\nin high dimensions using its relationship to the random energy model and to\ndense associative memories.",
        "updated": "2024-08-07 13:43:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03769v1"
    },
    {
        "title": "Flexible Bayesian Last Layer Models Using Implicit Priors and Diffusion Posterior Sampling",
        "authors": "Jian XuZhiqi LinShigui LiMin ChenJunmei YangDelu ZengJohn Paisley",
        "links": "http://arxiv.org/abs/2408.03746v1",
        "entry_id": "http://arxiv.org/abs/2408.03746v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03746v1",
        "summary": "Bayesian Last Layer (BLL) models focus solely on uncertainty in the output\nlayer of neural networks, demonstrating comparable performance to more complex\nBayesian models. However, the use of Gaussian priors for last layer weights in\nBayesian Last Layer (BLL) models limits their expressive capacity when faced\nwith non-Gaussian, outlier-rich, or high-dimensional datasets. To address this\nshortfall, we introduce a novel approach that combines diffusion techniques and\nimplicit priors for variational learning of Bayesian last layer weights. This\nmethod leverages implicit distributions for modeling weight priors in BLL,\ncoupled with diffusion samplers for approximating true posterior predictions,\nthereby establishing a comprehensive Bayesian prior and posterior estimation\nstrategy. By delivering an explicit and computationally efficient variational\nlower bound, our method aims to augment the expressive abilities of BLL models,\nenhancing model accuracy, calibration, and out-of-distribution detection\nproficiency. Through detailed exploration and experimental validation, We\nshowcase the method's potential for improving predictive accuracy and\nuncertainty quantification while ensuring computational efficiency.",
        "updated": "2024-08-07 12:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03746v1"
    },
    {
        "title": "Bayes-optimal learning of an extensive-width neural network from quadratically many samples",
        "authors": "Antoine MaillardEmanuele TroianiSimon MartinFlorent KrzakalaLenka Zdeborová",
        "links": "http://arxiv.org/abs/2408.03733v1",
        "entry_id": "http://arxiv.org/abs/2408.03733v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03733v1",
        "summary": "We consider the problem of learning a target function corresponding to a\nsingle hidden layer neural network, with a quadratic activation function after\nthe first layer, and random weights. We consider the asymptotic limit where the\ninput dimension and the network width are proportionally large. Recent work\n[Cui & al '23] established that linear regression provides Bayes-optimal test\nerror to learn such a function when the number of available samples is only\nlinear in the dimension. That work stressed the open challenge of theoretically\nanalyzing the optimal test error in the more interesting regime where the\nnumber of samples is quadratic in the dimension. In this paper, we solve this\nchallenge for quadratic activations and derive a closed-form expression for the\nBayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE,\nwhich combines approximate message passing with rotationally invariant matrix\ndenoising, and that asymptotically achieves the optimal performance.\nTechnically, our result is enabled by establishing a link with recent works on\noptimal denoising of extensive-rank matrices and on the ellipsoid fitting\nproblem. We further show empirically that, in the absence of noise,\nrandomly-initialized gradient descent seems to sample the space of weights,\nleading to zero training loss, and averaging over initialization leads to a\ntest error equal to the Bayes-optimal one.",
        "updated": "2024-08-07 12:41:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03733v1"
    },
    {
        "title": "On the choice of the non-trainable internal weights in random feature maps",
        "authors": "Pinak MandalGeorg A. Gottwald",
        "links": "http://arxiv.org/abs/2408.03626v1",
        "entry_id": "http://arxiv.org/abs/2408.03626v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03626v1",
        "summary": "The computationally cheap machine learning architecture of random feature\nmaps can be viewed as a single-layer feedforward network in which the weights\nof the hidden layer are random but fixed and only the outer weights are learned\nvia linear regression. The internal weights are typically chosen from a\nprescribed distribution. The choice of the internal weights significantly\nimpacts the accuracy of random feature maps. We address here the task of how to\nbest select the internal weights. In particular, we consider the forecasting\nproblem whereby random feature maps are used to learn a one-step propagator map\nfor a dynamical system. We provide a computationally cheap hit-and-run\nalgorithm to select good internal weights which lead to good forecasting skill.\nWe show that the number of good features is the main factor controlling the\nforecasting skill of random feature maps and acts as an effective feature\ndimension. Lastly, we compare random feature maps with single-layer feedforward\nneural networks in which the internal weights are now learned using gradient\ndescent. We find that random feature maps have superior forecasting\ncapabilities whilst having several orders of magnitude lower computational\ncost.",
        "updated": "2024-08-07 08:37:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03626v1"
    },
    {
        "title": "Sensitivity analysis using the Metamodel of Optimal Prognosis",
        "authors": "Thomas MostJohannes Will",
        "links": "http://arxiv.org/abs/2408.03590v1",
        "entry_id": "http://arxiv.org/abs/2408.03590v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03590v1",
        "summary": "In real case applications within the virtual prototyping process, it is not\nalways possible to reduce the complexity of the physical models and to obtain\nnumerical models which can be solved quickly. Usually, every single numerical\nsimulation takes hours or even days. Although the progresses in numerical\nmethods and high performance computing, in such cases, it is not possible to\nexplore various model configurations, hence efficient surrogate models are\nrequired. Generally the available meta-model techniques show several advantages\nand disadvantages depending on the investigated problem. In this paper we\npresent an automatic approach for the selection of the optimal suitable\nmeta-model for the actual problem. Together with an automatic reduction of the\nvariable space using advanced filter techniques an efficient approximation is\nenabled also for high dimensional problems. This filter techniques enable a\nreduction of the high dimensional variable space to a much smaller subspace\nwhere meta-model-based sensitivity analyses are carried out to assess the\ninfluence of important variables and to identify the optimal subspace with\ncorresponding surrogate model which enables the most accurate probabilistic\nanalysis. For this purpose we investigate variance-based and moment-free\nsensitivity measures in combination with advanced meta-models as moving least\nsquares and kriging.",
        "updated": "2024-08-07 07:09:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03590v1"
    }
]