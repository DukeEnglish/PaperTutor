[
    {
        "title": "How Well Can Vision Language Models See Image Details?",
        "authors": "Chenhui GouAbdulwahab FelembanFaizan Farooq KhanDeyao ZhuJianfei CaiHamid RezatofighiMohamed Elhoseiny",
        "links": "http://arxiv.org/abs/2408.03940v1",
        "entry_id": "http://arxiv.org/abs/2408.03940v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03940v1",
        "summary": "Large Language Model-based Vision-Language Models (LLM-based VLMs) have\ndemonstrated impressive results in various vision-language understanding tasks.\nHowever, how well these VLMs can see image detail beyond the semantic level\nremains unclear. In our study, we introduce a pixel value prediction task (PVP)\nto explore \"How Well Can Vision Language Models See Image Details?\" and to\nassist VLMs in perceiving more details. Typically, these models comprise a\nfrozen CLIP visual encoder, a large language model, and a connecting module.\nAfter fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle to\npredict precise pixel values by only fine-tuning the connection module and LLM;\nand 2) prediction precision is significantly improved when the vision encoder\nis also adapted. Additionally, our research reveals that incorporating pixel\nvalue prediction as one of the VLM pre-training tasks and vision encoder\nadaptation markedly boosts VLM performance on downstream image-language\nunderstanding tasks requiring detailed image perception, such as referring\nimage segmentation (with an average +10.19 cIoU improvement) and video game\ndecision making (with average score improvements of +80.34 and +70.54 on two\ngames, respectively).",
        "updated": "2024-08-07 17:59:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03940v1"
    },
    {
        "title": "Fast Sprite Decomposition from Animated Graphics",
        "authors": "Tomoyuki SuzukiKotaro KikuchiKota Yamaguchi",
        "links": "http://arxiv.org/abs/2408.03923v1",
        "entry_id": "http://arxiv.org/abs/2408.03923v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03923v1",
        "summary": "This paper presents an approach to decomposing animated graphics into\nsprites, a set of basic elements or layers. Our approach builds on the\noptimization of sprite parameters to fit the raster video. For efficiency, we\nassume static textures for sprites to reduce the search space while preventing\nartifacts using a texture prior model. To further speed up the optimization, we\nintroduce the initialization of the sprite parameters utilizing a pre-trained\nvideo object segmentation model and user input of single frame annotations. For\nour study, we construct the Crello Animation dataset from an online design\nservice and define quantitative metrics to measure the quality of the extracted\nsprites. Experiments show that our method significantly outperforms baselines\nfor similar decomposition tasks in terms of the quality/efficiency tradeoff.",
        "updated": "2024-08-07 17:30:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03923v1"
    },
    {
        "title": "FMiFood: Multi-modal Contrastive Learning for Food Image Classification",
        "authors": "Xinyue PanJiangpeng HeFengqing Zhu",
        "links": "http://arxiv.org/abs/2408.03922v1",
        "entry_id": "http://arxiv.org/abs/2408.03922v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03922v1",
        "summary": "Food image classification is the fundamental step in image-based dietary\nassessment, which aims to estimate participants' nutrient intake from eating\noccasion images. A common challenge of food images is the intra-class diversity\nand inter-class similarity, which can significantly hinder classification\nperformance. To address this issue, we introduce a novel multi-modal\ncontrastive learning framework called FMiFood, which learns more discriminative\nfeatures by integrating additional contextual information, such as food\ncategory text descriptions, to enhance classification accuracy. Specifically,\nwe propose a flexible matching technique that improves the similarity matching\nbetween text and image embeddings to focus on multiple key information.\nFurthermore, we incorporate the classification objectives into the framework\nand explore the use of GPT-4 to enrich the text descriptions and provide more\ndetailed context. Our method demonstrates improved performance on both the\nUPMC-101 and VFN datasets compared to existing methods.",
        "updated": "2024-08-07 17:29:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03922v1"
    },
    {
        "title": "AdapMTL: Adaptive Pruning Framework for Multitask Learning Model",
        "authors": "Mingcan XiangSteven Jiaxun TangQizheng YangHui GuanTongping Liu",
        "links": "http://dx.doi.org/10.1145/3664647.3681426",
        "entry_id": "http://arxiv.org/abs/2408.03913v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03913v1",
        "summary": "In the domain of multimedia and multimodal processing, the efficient handling\nof diverse data streams such as images, video, and sensor data is paramount.\nModel compression and multitask learning (MTL) are crucial in this field,\noffering the potential to address the resource-intensive demands of processing\nand interpreting multiple forms of media simultaneously. However, effectively\ncompressing a multitask model presents significant challenges due to the\ncomplexities of balancing sparsity allocation and accuracy performance across\nmultiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive\npruning framework for MTL models. AdapMTL leverages multiple learnable soft\nthresholds independently assigned to the shared backbone and the task-specific\nheads to capture the nuances in different components' sensitivity to pruning.\nDuring training, it co-optimizes the soft thresholds and MTL model weights to\nautomatically determine the suitable sparsity level at each component to\nachieve both high task accuracy and high overall sparsity. It further\nincorporates an adaptive weighting mechanism that dynamically adjusts the\nimportance of task-specific losses based on each task's robustness to pruning.\nWe demonstrate the effectiveness of AdapMTL through comprehensive experiments\non popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with different\narchitectures, showcasing superior performance compared to state-of-the-art\npruning methods.",
        "updated": "2024-08-07 17:19:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03913v1"
    },
    {
        "title": "Lightweight Video Denoising Using a Classic Bayesian Backbone",
        "authors": "Clément BledFrançois Pitié",
        "links": "http://arxiv.org/abs/2408.03904v1",
        "entry_id": "http://arxiv.org/abs/2408.03904v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03904v1",
        "summary": "In recent years, state-of-the-art image and video denoising networks have\nbecome increasingly large, requiring millions of trainable parameters to\nachieve best-in-class performance. Improved denoising quality has come at the\ncost of denoising speed, where modern transformer networks are far slower to\nrun than smaller denoising networks such as FastDVDnet and classic Bayesian\ndenoisers such as the Wiener filter.\n  In this paper, we implement a hybrid Wiener filter which leverages small\nancillary networks to increase the original denoiser performance, while\nretaining fast denoising speeds. These networks are used to refine the Wiener\ncoring estimate, optimise windowing functions and estimate the unknown noise\nprofile. Using these methods, we outperform several popular denoisers and\nremain within 0.2 dB, on average, of the popular VRT transformer. Our method\nwas found to be over x10 faster than the transformer method, with a far lower\nparameter cost.",
        "updated": "2024-08-07 17:08:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03904v1"
    }
]