[
    {
        "title": "SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature",
        "authors": "Vinícius Di OliveiraYuri Façanha BezerraLi WeigangPedro Carvalho BromVictor Rafael R. Celestino",
        "links": "http://arxiv.org/abs/2408.03936v1",
        "entry_id": "http://arxiv.org/abs/2408.03936v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03936v1",
        "summary": "Natural language processing (NLP) has seen significant advancements with the\nadvent of large language models (LLMs). However, substantial improvements are\nstill needed for languages other than English, especially for specific domains\nlike the applications of Mercosur Common Nomenclature (NCM), a Brazilian\nHarmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a\nfoundational Portuguese LLM, as an LLM source to implement the NCM application\nprocessing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)\ntechnique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.\nThis approach retains the chain-of-thought (CoT) methodology for prompt\ndevelopment in a more concise and streamlined manner, utilizing brief and\nfocused documents for training. The proposed model demonstrates an efficient\nand cost-effective alternative for fine-tuning smaller LLMs, significantly\noutperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the\nresearch focuses on NCM applications, the methodology can be easily adapted for\nHS applications worldwide.",
        "updated": "2024-08-07 17:54:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03936v1"
    },
    {
        "title": "CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases",
        "authors": "Xiangyan LiuBo LanZhiyuan HuYang LiuZhicheng ZhangWenmeng ZhouFei WangMichael Shieh",
        "links": "http://arxiv.org/abs/2408.03910v1",
        "entry_id": "http://arxiv.org/abs/2408.03910v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03910v1",
        "summary": "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval\nand MBPP, but struggle with handling entire code repositories. This challenge\nhas prompted research on enhancing LLM-codebase interaction at a repository\nscale. Current solutions rely on similarity-based retrieval or manual tools and\nAPIs, each with notable drawbacks. Similarity-based retrieval often has low\nrecall in complex tasks, while manual tools and APIs are typically\ntask-specific and require expert knowledge, reducing their generalizability\nacross diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce \\framework, a system that integrates LLM agents with\ngraph database interfaces extracted from code repositories. By leveraging the\nstructural properties of graph databases and the flexibility of the graph query\nlanguage, \\framework enables the LLM agent to construct and execute queries,\nallowing for precise, code structure-aware context retrieval and code\nnavigation. We assess \\framework using three benchmarks: CrossCodeEval,\nSWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding\napplications. With a unified graph database schema, \\framework demonstrates\ncompetitive performance and potential in both academic and real-world\nenvironments, showcasing its versatility and efficacy in software engineering.\nOur application demo:\nhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.",
        "updated": "2024-08-07 17:13:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03910v1"
    },
    {
        "title": "LaFA: Latent Feature Attacks on Non-negative Matrix Factorization",
        "authors": "Minh VuBen NebgenErik SkauGeigh ZollicofferJuan CastorenaKim RasmussenBoian AlexandrovManish Bhattarai",
        "links": "http://arxiv.org/abs/2408.03909v1",
        "entry_id": "http://arxiv.org/abs/2408.03909v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03909v1",
        "summary": "As Machine Learning (ML) applications rapidly grow, concerns about\nadversarial attacks compromising their reliability have gained significant\nattention. One unsupervised ML method known for its resilience to such attacks\nis Non-negative Matrix Factorization (NMF), an algorithm that decomposes input\ndata into lower-dimensional latent features. However, the introduction of\npowerful computational tools such as Pytorch enables the computation of\ngradients of the latent features with respect to the original data, raising\nconcerns about NMF's reliability. Interestingly, naively deriving the\nadversarial loss for NMF as in the case of ML would result in the\nreconstruction loss, which can be shown theoretically to be an ineffective\nattacking objective. In this work, we introduce a novel class of attacks in NMF\ntermed Latent Feature Attacks (LaFA), which aim to manipulate the latent\nfeatures produced by the NMF process. Our method utilizes the Feature Error\n(FE) loss directly on the latent features. By employing FE loss, we generate\nperturbations in the original data that significantly affect the extracted\nlatent features, revealing vulnerabilities akin to those found in other ML\ntechniques. To handle large peak-memory overhead from gradient back-propagation\nin FE attacks, we develop a method based on implicit differentiation which\nenables their scaling to larger datasets. We validate NMF vulnerabilities and\nFE attacks effectiveness through extensive experiments on synthetic and\nreal-world data.",
        "updated": "2024-08-07 17:13:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03909v1"
    },
    {
        "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
        "authors": "Shachi H KumarSaurav SahaySahisnu MazumderEda OkurRamesh ManuvinakurikeNicole BeckageHsuan SuHung-yi LeeLama Nachman",
        "links": "http://arxiv.org/abs/2408.03907v1",
        "entry_id": "http://arxiv.org/abs/2408.03907v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03907v1",
        "summary": "Large Language Models (LLMs) have excelled at language understanding and\ngenerating human-level text. However, even with supervised training and human\nalignment, these LLMs are susceptible to adversarial attacks where malicious\nusers can prompt the model to generate undesirable text. LLMs also inherently\nencode potential biases that can cause various harmful effects during\ninteractions. Bias evaluation metrics lack standards as well as consensus and\nexisting methods often rely on human-generated templates and annotations which\nare expensive and labor intensive. In this work, we train models to\nautomatically create adversarial prompts to elicit biased responses from target\nLLMs. We present LLM- based bias evaluation metrics and also analyze several\nexisting automatic evaluation methods and metrics. We analyze the various\nnuances of model responses, identify the strengths and weaknesses of model\nfamilies, and assess where evaluation methods fall short. We compare these\nmetrics to human evaluation and validate that the LLM-as-a-Judge metric aligns\nwith human judgement on bias in response generation.",
        "updated": "2024-08-07 17:11:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03907v1"
    },
    {
        "title": "Lightweight Video Denoising Using a Classic Bayesian Backbone",
        "authors": "Clément BledFrançois Pitié",
        "links": "http://arxiv.org/abs/2408.03904v1",
        "entry_id": "http://arxiv.org/abs/2408.03904v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03904v1",
        "summary": "In recent years, state-of-the-art image and video denoising networks have\nbecome increasingly large, requiring millions of trainable parameters to\nachieve best-in-class performance. Improved denoising quality has come at the\ncost of denoising speed, where modern transformer networks are far slower to\nrun than smaller denoising networks such as FastDVDnet and classic Bayesian\ndenoisers such as the Wiener filter.\n  In this paper, we implement a hybrid Wiener filter which leverages small\nancillary networks to increase the original denoiser performance, while\nretaining fast denoising speeds. These networks are used to refine the Wiener\ncoring estimate, optimise windowing functions and estimate the unknown noise\nprofile. Using these methods, we outperform several popular denoisers and\nremain within 0.2 dB, on average, of the popular VRT transformer. Our method\nwas found to be over x10 faster than the transformer method, with a far lower\nparameter cost.",
        "updated": "2024-08-07 17:08:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03904v1"
    }
]