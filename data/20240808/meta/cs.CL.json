[
    {
        "title": "SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature",
        "authors": "Vinícius Di OliveiraYuri Façanha BezerraLi WeigangPedro Carvalho BromVictor Rafael R. Celestino",
        "links": "http://arxiv.org/abs/2408.03936v1",
        "entry_id": "http://arxiv.org/abs/2408.03936v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03936v1",
        "summary": "Natural language processing (NLP) has seen significant advancements with the\nadvent of large language models (LLMs). However, substantial improvements are\nstill needed for languages other than English, especially for specific domains\nlike the applications of Mercosur Common Nomenclature (NCM), a Brazilian\nHarmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a\nfoundational Portuguese LLM, as an LLM source to implement the NCM application\nprocessing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)\ntechnique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.\nThis approach retains the chain-of-thought (CoT) methodology for prompt\ndevelopment in a more concise and streamlined manner, utilizing brief and\nfocused documents for training. The proposed model demonstrates an efficient\nand cost-effective alternative for fine-tuning smaller LLMs, significantly\noutperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the\nresearch focuses on NCM applications, the methodology can be easily adapted for\nHS applications worldwide.",
        "updated": "2024-08-07 17:54:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03936v1"
    },
    {
        "title": "From Words to Worth: Newborn Article Impact Prediction with LLM",
        "authors": "Penghai ZhaoQinghua XingKairan DouJinyu TianYing TaiJian YangMing-Ming ChengXiang Li",
        "links": "http://arxiv.org/abs/2408.03934v1",
        "entry_id": "http://arxiv.org/abs/2408.03934v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03934v1",
        "summary": "As the academic landscape expands, the challenge of efficiently identifying\npotentially high-impact articles among the vast number of newly published works\nbecomes critical. This paper introduces a promising approach, leveraging the\ncapabilities of fine-tuned LLMs to predict the future impact of newborn\narticles solely based on titles and abstracts. Moving beyond traditional\nmethods heavily reliant on external information, the proposed method discerns\nthe shared semantic features of highly impactful papers from a large collection\nof title-abstract and potential impact pairs. These semantic features are\nfurther utilized to regress an improved metric, TNCSI_SP, which has been\nendowed with value, field, and time normalization properties. Additionally, a\ncomprehensive dataset has been constructed and released for fine-tuning the\nLLM, containing over 12,000 entries with corresponding titles, abstracts, and\nTNCSI_SP. The quantitative results, with an NDCG@20 of 0.901, demonstrate that\nthe proposed approach achieves state-of-the-art performance in predicting the\nimpact of newborn articles when compared to competitive counterparts. Finally,\nwe demonstrate a real-world application for predicting the impact of newborn\njournal articles to demonstrate its noteworthy practical value. Overall, our\nfindings challenge existing paradigms and propose a shift towards a more\ncontent-focused prediction of academic impact, offering new insights for\nassessing newborn article impact.",
        "updated": "2024-08-07 17:52:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03934v1"
    },
    {
        "title": "CodexGraph: Bridging Large Language Models and Code Repositories via Code Graph Databases",
        "authors": "Xiangyan LiuBo LanZhiyuan HuYang LiuZhicheng ZhangWenmeng ZhouFei WangMichael Shieh",
        "links": "http://arxiv.org/abs/2408.03910v1",
        "entry_id": "http://arxiv.org/abs/2408.03910v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03910v1",
        "summary": "Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval\nand MBPP, but struggle with handling entire code repositories. This challenge\nhas prompted research on enhancing LLM-codebase interaction at a repository\nscale. Current solutions rely on similarity-based retrieval or manual tools and\nAPIs, each with notable drawbacks. Similarity-based retrieval often has low\nrecall in complex tasks, while manual tools and APIs are typically\ntask-specific and require expert knowledge, reducing their generalizability\nacross diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce \\framework, a system that integrates LLM agents with\ngraph database interfaces extracted from code repositories. By leveraging the\nstructural properties of graph databases and the flexibility of the graph query\nlanguage, \\framework enables the LLM agent to construct and execute queries,\nallowing for precise, code structure-aware context retrieval and code\nnavigation. We assess \\framework using three benchmarks: CrossCodeEval,\nSWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding\napplications. With a unified graph database schema, \\framework demonstrates\ncompetitive performance and potential in both academic and real-world\nenvironments, showcasing its versatility and efficacy in software engineering.\nOur application demo:\nhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.",
        "updated": "2024-08-07 17:13:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03910v1"
    },
    {
        "title": "Decoding Biases: Automated Methods and LLM Judges for Gender Bias Detection in Language Models",
        "authors": "Shachi H KumarSaurav SahaySahisnu MazumderEda OkurRamesh ManuvinakurikeNicole BeckageHsuan SuHung-yi LeeLama Nachman",
        "links": "http://arxiv.org/abs/2408.03907v1",
        "entry_id": "http://arxiv.org/abs/2408.03907v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03907v1",
        "summary": "Large Language Models (LLMs) have excelled at language understanding and\ngenerating human-level text. However, even with supervised training and human\nalignment, these LLMs are susceptible to adversarial attacks where malicious\nusers can prompt the model to generate undesirable text. LLMs also inherently\nencode potential biases that can cause various harmful effects during\ninteractions. Bias evaluation metrics lack standards as well as consensus and\nexisting methods often rely on human-generated templates and annotations which\nare expensive and labor intensive. In this work, we train models to\nautomatically create adversarial prompts to elicit biased responses from target\nLLMs. We present LLM- based bias evaluation metrics and also analyze several\nexisting automatic evaluation methods and metrics. We analyze the various\nnuances of model responses, identify the strengths and weaknesses of model\nfamilies, and assess where evaluation methods fall short. We compare these\nmetrics to human evaluation and validate that the LLM-as-a-Judge metric aligns\nwith human judgement on bias in response generation.",
        "updated": "2024-08-07 17:11:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03907v1"
    },
    {
        "title": "Speech-MASSIVE: A Multilingual Speech Dataset for SLU and Beyond",
        "authors": "Beomseok LeeIoan CalapodescuMarco GaidoMatteo NegriLaurent Besacier",
        "links": "http://arxiv.org/abs/2408.03900v1",
        "entry_id": "http://arxiv.org/abs/2408.03900v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03900v1",
        "summary": "We present Speech-MASSIVE, a multilingual Spoken Language Understanding (SLU)\ndataset comprising the speech counterpart for a portion of the MASSIVE textual\ncorpus. Speech-MASSIVE covers 12 languages from different families and inherits\nfrom MASSIVE the annotations for the intent prediction and slot-filling tasks.\nOur extension is prompted by the scarcity of massively multilingual SLU\ndatasets and the growing need for versatile speech datasets to assess\nfoundation models (LLMs, speech encoders) across languages and tasks. We\nprovide a multimodal, multitask, multilingual dataset and report SLU baselines\nusing both cascaded and end-to-end architectures in various training scenarios\n(zero-shot, few-shot, and full fine-tune). Furthermore, we demonstrate the\nsuitability of Speech-MASSIVE for benchmarking other tasks such as speech\ntranscription, language identification, and speech translation. The dataset,\nmodels, and code are publicly available at:\nhttps://github.com/hlt-mt/Speech-MASSIVE",
        "updated": "2024-08-07 16:55:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03900v1"
    }
]