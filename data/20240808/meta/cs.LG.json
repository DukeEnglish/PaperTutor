[
    {
        "title": "SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic Performance for Mercosur Common Nomenclature",
        "authors": "Vinícius Di OliveiraYuri Façanha BezerraLi WeigangPedro Carvalho BromVictor Rafael R. Celestino",
        "links": "http://arxiv.org/abs/2408.03936v1",
        "entry_id": "http://arxiv.org/abs/2408.03936v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03936v1",
        "summary": "Natural language processing (NLP) has seen significant advancements with the\nadvent of large language models (LLMs). However, substantial improvements are\nstill needed for languages other than English, especially for specific domains\nlike the applications of Mercosur Common Nomenclature (NCM), a Brazilian\nHarmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a\nfoundational Portuguese LLM, as an LLM source to implement the NCM application\nprocessing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)\ntechnique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.\nThis approach retains the chain-of-thought (CoT) methodology for prompt\ndevelopment in a more concise and streamlined manner, utilizing brief and\nfocused documents for training. The proposed model demonstrates an efficient\nand cost-effective alternative for fine-tuning smaller LLMs, significantly\noutperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the\nresearch focuses on NCM applications, the methodology can be easily adapted for\nHS applications worldwide.",
        "updated": "2024-08-07 17:54:21 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03936v1"
    },
    {
        "title": "Hard to Explain: On the Computational Hardness of In-Distribution Model Interpretation",
        "authors": "Guy AmirShahaf BassanGuy Katz",
        "links": "http://arxiv.org/abs/2408.03915v1",
        "entry_id": "http://arxiv.org/abs/2408.03915v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03915v1",
        "summary": "The ability to interpret Machine Learning (ML) models is becoming\nincreasingly essential. However, despite significant progress in the field,\nthere remains a lack of rigorous characterization regarding the innate\ninterpretability of different models. In an attempt to bridge this gap, recent\nwork has demonstrated that it is possible to formally assess interpretability\nby studying the computational complexity of explaining the decisions of various\nmodels. In this setting, if explanations for a particular model can be obtained\nefficiently, the model is considered interpretable (since it can be explained\n``easily''). However, if generating explanations over an ML model is\ncomputationally intractable, it is considered uninterpretable. Prior research\nidentified two key factors that influence the complexity of interpreting an ML\nmodel: (i) the type of the model (e.g., neural networks, decision trees, etc.);\nand (ii) the form of explanation (e.g., contrastive explanations, Shapley\nvalues, etc.). In this work, we claim that a third, important factor must also\nbe considered for this analysis -- the underlying distribution over which the\nexplanation is obtained. Considering the underlying distribution is key in\navoiding explanations that are socially misaligned, i.e., convey information\nthat is biased and unhelpful to users. We demonstrate the significant influence\nof the underlying distribution on the resulting overall interpretation\ncomplexity, in two settings: (i) prediction models paired with an external\nout-of-distribution (OOD) detector; and (ii) prediction models designed to\ninherently generate socially aligned explanations. Our findings prove that the\nexpressiveness of the distribution can significantly influence the overall\ncomplexity of interpretation, and identify essential prerequisites that a model\nmust possess to generate socially aligned explanations.",
        "updated": "2024-08-07 17:20:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03915v1"
    },
    {
        "title": "AdapMTL: Adaptive Pruning Framework for Multitask Learning Model",
        "authors": "Mingcan XiangSteven Jiaxun TangQizheng YangHui GuanTongping Liu",
        "links": "http://dx.doi.org/10.1145/3664647.3681426",
        "entry_id": "http://arxiv.org/abs/2408.03913v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03913v1",
        "summary": "In the domain of multimedia and multimodal processing, the efficient handling\nof diverse data streams such as images, video, and sensor data is paramount.\nModel compression and multitask learning (MTL) are crucial in this field,\noffering the potential to address the resource-intensive demands of processing\nand interpreting multiple forms of media simultaneously. However, effectively\ncompressing a multitask model presents significant challenges due to the\ncomplexities of balancing sparsity allocation and accuracy performance across\nmultiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive\npruning framework for MTL models. AdapMTL leverages multiple learnable soft\nthresholds independently assigned to the shared backbone and the task-specific\nheads to capture the nuances in different components' sensitivity to pruning.\nDuring training, it co-optimizes the soft thresholds and MTL model weights to\nautomatically determine the suitable sparsity level at each component to\nachieve both high task accuracy and high overall sparsity. It further\nincorporates an adaptive weighting mechanism that dynamically adjusts the\nimportance of task-specific losses based on each task's robustness to pruning.\nWe demonstrate the effectiveness of AdapMTL through comprehensive experiments\non popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with different\narchitectures, showcasing superior performance compared to state-of-the-art\npruning methods.",
        "updated": "2024-08-07 17:19:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03913v1"
    },
    {
        "title": "LaFA: Latent Feature Attacks on Non-negative Matrix Factorization",
        "authors": "Minh VuBen NebgenErik SkauGeigh ZollicofferJuan CastorenaKim RasmussenBoian AlexandrovManish Bhattarai",
        "links": "http://arxiv.org/abs/2408.03909v1",
        "entry_id": "http://arxiv.org/abs/2408.03909v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03909v1",
        "summary": "As Machine Learning (ML) applications rapidly grow, concerns about\nadversarial attacks compromising their reliability have gained significant\nattention. One unsupervised ML method known for its resilience to such attacks\nis Non-negative Matrix Factorization (NMF), an algorithm that decomposes input\ndata into lower-dimensional latent features. However, the introduction of\npowerful computational tools such as Pytorch enables the computation of\ngradients of the latent features with respect to the original data, raising\nconcerns about NMF's reliability. Interestingly, naively deriving the\nadversarial loss for NMF as in the case of ML would result in the\nreconstruction loss, which can be shown theoretically to be an ineffective\nattacking objective. In this work, we introduce a novel class of attacks in NMF\ntermed Latent Feature Attacks (LaFA), which aim to manipulate the latent\nfeatures produced by the NMF process. Our method utilizes the Feature Error\n(FE) loss directly on the latent features. By employing FE loss, we generate\nperturbations in the original data that significantly affect the extracted\nlatent features, revealing vulnerabilities akin to those found in other ML\ntechniques. To handle large peak-memory overhead from gradient back-propagation\nin FE attacks, we develop a method based on implicit differentiation which\nenables their scaling to larger datasets. We validate NMF vulnerabilities and\nFE attacks effectiveness through extensive experiments on synthetic and\nreal-world data.",
        "updated": "2024-08-07 17:13:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03909v1"
    },
    {
        "title": "Knowledge Probing for Graph Representation Learning",
        "authors": "Mingyu ZhaoXingyu HuangZiyu LyuYanlin WangLixin CuiLu Bai",
        "links": "http://arxiv.org/abs/2408.03877v1",
        "entry_id": "http://arxiv.org/abs/2408.03877v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03877v1",
        "summary": "Graph learning methods have been extensively applied in diverse application\nareas. However, what kind of inherent graph properties e.g. graph proximity,\ngraph structural information has been encoded into graph representation\nlearning for downstream tasks is still under-explored. In this paper, we\npropose a novel graph probing framework (GraphProbe) to investigate and\ninterpret whether the family of graph learning methods has encoded different\nlevels of knowledge in graph representation learning. Based on the intrinsic\nproperties of graphs, we design three probes to systematically investigate the\ngraph representation learning process from different perspectives, respectively\nthe node-wise level, the path-wise level, and the structural level. We\nconstruct a thorough evaluation benchmark with nine representative graph\nlearning methods from random walk based approaches, basic graph neural networks\nand self-supervised graph methods, and probe them on six benchmark datasets for\nnode classification, link prediction and graph classification. The experimental\nevaluation verify that GraphProbe can estimate the capability of graph\nrepresentation learning. Remaking results have been concluded: GCN and\nWeightedGCN methods are relatively versatile methods achieving better results\nwith respect to different tasks.",
        "updated": "2024-08-07 16:27:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03877v1"
    }
]