[
    {
        "title": "Textoshop: Interactions Inspired by Drawing Software to Facilitate Text Editing",
        "authors": "Damien MassonYoung-Ho KimFanny Chevalier",
        "links": "http://arxiv.org/abs/2409.17088v1",
        "entry_id": "http://arxiv.org/abs/2409.17088v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17088v1",
        "summary": "We explore how interactions inspired by drawing software can help edit text.\nMaking an analogy between visual and text editing, we consider words as pixels,\nsentences as regions, and tones as colours. For instance, direct manipulations\nmove, shorten, expand, and reorder text; tools change number, tense, and\ngrammar; colours map to tones explored along three dimensions in a tone picker;\nand layers help organize and version text. This analogy also leads to new\nworkflows, such as boolean operations on text fragments to construct more\nelaborated text. A study shows participants were more successful at editing\ntext and preferred using the proposed interface over existing solutions.\nBroadly, our work highlights the potential of interaction analogies to rethink\nexisting workflows, while capitalizing on familiar features.",
        "updated": "2024-09-25 16:51:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17088v1"
    },
    {
        "title": "Towards User-Focused Research in Training Data Attribution for Human-Centered Explainable AI",
        "authors": "Elisa NguyenJohannes BertramEvgenii KortukovJean Y. SongSeong Joon Oh",
        "links": "http://arxiv.org/abs/2409.16978v1",
        "entry_id": "http://arxiv.org/abs/2409.16978v1",
        "pdf_url": "http://arxiv.org/pdf/2409.16978v1",
        "summary": "While Explainable AI (XAI) aims to make AI understandable and useful to\nhumans, it has been criticised for relying too much on formalism and\nsolutionism, focusing more on mathematical soundness than user needs. We\npropose an alternative to this bottom-up approach inspired by design thinking:\nthe XAI research community should adopt a top-down, user-focused perspective to\nensure user relevance. We illustrate this with a relatively young subfield of\nXAI, Training Data Attribution (TDA). With the surge in TDA research and\ngrowing competition, the field risks repeating the same patterns of\nsolutionism. We conducted a needfinding study with a diverse group of AI\npractitioners to identify potential user needs related to TDA. Through\ninterviews (N=10) and a systematic survey (N=31), we uncovered new TDA tasks\nthat are currently largely overlooked. We invite the TDA and XAI communities to\nconsider these novel tasks and improve the user relevance of their research\noutcomes.",
        "updated": "2024-09-25 14:40:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.16978v1"
    },
    {
        "title": "Tactile Perception of Electroadhesion: Effect of DC versus AC Stimulation and Finger Moisture",
        "authors": "Easa AliAbbasiMuhammad MuzammilOmer SirinPhilippe LefèvreØrjan Grøttem MartinsenCagatay Basdogan",
        "links": "http://dx.doi.org/10.1109/TOH.2024.3441670",
        "entry_id": "http://arxiv.org/abs/2409.16936v1",
        "pdf_url": "http://arxiv.org/pdf/2409.16936v1",
        "summary": "Electroadhesion has emerged as a viable technique for displaying tactile\nfeedback on touch surfaces, particularly capacitive touchscreens found in\nsmartphones and tablets. This involves applying a voltage signal to the\nconductive layer of the touchscreen to generate tactile sensations on the\nfingerpads of users. In our investigation, we explore the tactile perception of\nelectroadhesion under DC and AC stimulations. Our tactile perception\nexperiments with 10 participants demonstrate a significantly lower voltage\ndetection threshold for AC signals compared to their DC counterparts. This\ndiscrepancy is elucidated by the underlying electro-mechanical interactions\nbetween the finger and the voltage-induced touchscreen and considering the\nresponse of mechanoreceptors in the fingerpad to electrostatic forces generated\nby electroadhesion. Additionally, our study highlights the impact of moisture\non electroadhesive tactile perception. Participants with moist fingers\nexhibited markedly higher threshold levels. Our electrical impedance\nmeasurements show a substantial reduction in impedance magnitude when sweat is\npresent at the finger-touchscreen interface, indicating increased conductivity.\nThese findings not only contribute to our understanding of tactile perception\nunder electroadhesion but also shed light on the underlying physics. In this\nregard, the results of this study extend beyond mobile devices to encompass\nother applications of this technology, including robotics, automation, space\nmissions, and textiles.",
        "updated": "2024-09-25 13:49:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.16936v1"
    },
    {
        "title": "AI-assisted Gaze Detection for Proctoring Online Exams",
        "authors": "Yong-Siang ShihZach ZhaoChenhao NiuBruce IbergJames SharpnackMirza Basim Baig",
        "links": "http://arxiv.org/abs/2409.16923v1",
        "entry_id": "http://arxiv.org/abs/2409.16923v1",
        "pdf_url": "http://arxiv.org/pdf/2409.16923v1",
        "summary": "For high-stakes online exams, it is important to detect potential rule\nviolations to ensure the security of the test. In this study, we investigate\nthe task of detecting whether test takers are looking away from the screen, as\nsuch behavior could be an indication that the test taker is consulting external\nresources. For asynchronous proctoring, the exam videos are recorded and\nreviewed by the proctors. However, when the length of the exam is long, it\ncould be tedious for proctors to watch entire exam videos to determine the\nexact moments when test takers look away. We present an AI-assisted gaze\ndetection system, which allows proctors to navigate between different video\nframes and discover video frames where the test taker is looking in similar\ndirections. The system enables proctors to work more effectively to identify\nsuspicious moments in videos. An evaluation framework is proposed to evaluate\nthe system against human-only and ML-only proctoring, and a user study is\nconducted to gather feedback from proctors, aiming to demonstrate the\neffectiveness of the system.",
        "updated": "2024-09-25 13:31:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.16923v1"
    },
    {
        "title": "Cross-lingual Speech Emotion Recognition: Humans vs. Self-Supervised Models",
        "authors": "Zhichen HanTianqi GengHui FengJiahong YuanKorin RichmondYuanchao Li",
        "links": "http://arxiv.org/abs/2409.16920v1",
        "entry_id": "http://arxiv.org/abs/2409.16920v1",
        "pdf_url": "http://arxiv.org/pdf/2409.16920v1",
        "summary": "Utilizing Self-Supervised Learning (SSL) models for Speech Emotion\nRecognition (SER) has proven effective, yet limited research has explored\ncross-lingual scenarios. This study presents a comparative analysis between\nhuman performance and SSL models, beginning with a layer-wise analysis and an\nexploration of parameter-efficient fine-tuning strategies in monolingual,\ncross-lingual, and transfer learning contexts. We further compare the SER\nability of models and humans at both utterance- and segment-levels.\nAdditionally, we investigate the impact of dialect on cross-lingual SER through\nhuman evaluation. Our findings reveal that models, with appropriate knowledge\ntransfer, can adapt to the target language and achieve performance comparable\nto native speakers. We also demonstrate the significant effect of dialect on\nSER for individuals without prior linguistic and paralinguistic background.\nMoreover, both humans and models exhibit distinct behaviors across different\nemotions. These results offer new insights into the cross-lingual SER\ncapabilities of SSL models, underscoring both their similarities to and\ndifferences from human emotion perception.",
        "updated": "2024-09-25 13:27:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.16920v1"
    }
]