[
    {
        "title": "Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models",
        "authors": "Matt DeitkeChristopher ClarkSangho LeeRohun TripathiYue YangJae Sung ParkMohammadreza SalehiNiklas MuennighoffKyle LoLuca SoldainiJiasen LuTaira AndersonErin BransomKiana EhsaniHuong NgoYenSung ChenAjay PatelMark YatskarChris Callison-BurchAndrew HeadRose HendrixFavyen BastaniEli VanderBiltNathan LambertYvonne ChouArnavi ChhedaJenna SparksSam SkjonsbergMichael SchmitzAaron SarnatByron BischoffPete WalshChris NewellPiper WoltersTanmay GuptaKuo-Hao ZengJon BorchardtDirk GroeneveldJen DumasCrystal NamSophie LebrechtCaitlin WittlifCarissa SchoenickOscar MichelRanjay KrishnaLuca WeihsNoah A. SmithHannaneh HajishirziRoss GirshickAli FarhadiAniruddha Kembhavi",
        "links": "http://arxiv.org/abs/2409.17146v1",
        "entry_id": "http://arxiv.org/abs/2409.17146v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17146v1",
        "summary": "Today's most advanced multimodal models remain proprietary. The strongest\nopen-weight models rely heavily on synthetic data from proprietary VLMs to\nachieve good performance, effectively distilling these closed models into open\nones. As a result, the community is still missing foundational knowledge about\nhow to build performant VLMs from scratch. We present Molmo, a new family of\nVLMs that are state-of-the-art in their class of openness. Our key innovation\nis a novel, highly detailed image caption dataset collected entirely from human\nannotators using speech-based descriptions. To enable a wide array of user\ninteractions, we also introduce a diverse dataset mixture for fine-tuning that\nincludes in-the-wild Q&A and innovative 2D pointing data. The success of our\napproach relies on careful choices for the model architecture details, a\nwell-tuned training pipeline, and, most critically, the quality of our newly\ncollected datasets, all of which will be released. The best-in-class 72B model\nwithin the Molmo family not only outperforms others in the class of open weight\nand data models but also compares favorably against proprietary systems like\nGPT-4o, Claude 3.5, and Gemini 1.5 on both academic benchmarks and human\nevaluation.\n  We will be releasing all of our model weights, captioning and fine-tuning\ndata, and source code in the near future. Select model weights, inference code,\nand demo are available at https://molmo.allenai.org.",
        "updated": "2024-09-25 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17146v1"
    },
    {
        "title": "DreamWaltz-G: Expressive 3D Gaussian Avatars from Skeleton-Guided 2D Diffusion",
        "authors": "Yukun HuangJianan WangAiling ZengZheng-Jun ZhaLei ZhangXihui Liu",
        "links": "http://arxiv.org/abs/2409.17145v1",
        "entry_id": "http://arxiv.org/abs/2409.17145v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17145v1",
        "summary": "Leveraging pretrained 2D diffusion models and score distillation sampling\n(SDS), recent methods have shown promising results for text-to-3D avatar\ngeneration. However, generating high-quality 3D avatars capable of expressive\nanimation remains challenging. In this work, we present DreamWaltz-G, a novel\nlearning framework for animatable 3D avatar generation from text. The core of\nthis framework lies in Skeleton-guided Score Distillation and Hybrid 3D\nGaussian Avatar representation. Specifically, the proposed skeleton-guided\nscore distillation integrates skeleton controls from 3D human templates into 2D\ndiffusion models, enhancing the consistency of SDS supervision in terms of view\nand human pose. This facilitates the generation of high-quality avatars,\nmitigating issues such as multiple faces, extra limbs, and blurring. The\nproposed hybrid 3D Gaussian avatar representation builds on the efficient 3D\nGaussians, combining neural implicit fields and parameterized 3D meshes to\nenable real-time rendering, stable SDS optimization, and expressive animation.\nExtensive experiments demonstrate that DreamWaltz-G is highly effective in\ngenerating and animating 3D avatars, outperforming existing methods in both\nvisual quality and animation expressiveness. Our framework further supports\ndiverse applications, including human video reenactment and multi-subject scene\ncomposition.",
        "updated": "2024-09-25 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17145v1"
    },
    {
        "title": "Attention Prompting on Image for Large Vision-Language Models",
        "authors": "Runpeng YuWeihao YuXinchao Wang",
        "links": "http://arxiv.org/abs/2409.17143v1",
        "entry_id": "http://arxiv.org/abs/2409.17143v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17143v1",
        "summary": "Compared with Large Language Models (LLMs), Large Vision-Language Models\n(LVLMs) can also accept images as input, thus showcasing more interesting\nemergent capabilities and demonstrating impressive performance on various\nvision-language tasks. Motivated by text prompting in LLMs, visual prompting\nhas been explored to enhance LVLMs' capabilities of perceiving visual\ninformation. However, previous visual prompting techniques solely process\nvisual inputs without considering text queries, limiting the models' ability to\nfollow text instructions to complete tasks. To fill this gap, in this work, we\npropose a new prompting technique named Attention Prompting on Image, which\njust simply overlays a text-query-guided attention heatmap on the original\ninput image and effectively enhances LVLM on various tasks. Specifically, we\ngenerate an attention heatmap for the input image dependent on the text query\nwith an auxiliary model like CLIP. Then the heatmap simply multiplies the pixel\nvalues of the original image to obtain the actual input image for the LVLM.\nExtensive experiments on various vison-language benchmarks verify the\neffectiveness of our technique. For example, Attention Prompting on Image\nimproves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild benchmarks,\nrespectively.",
        "updated": "2024-09-25 17:59:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17143v1"
    },
    {
        "title": "PACE: marrying generalization in PArameter-efficient fine-tuning with Consistency rEgularization",
        "authors": "Yao NiShan ZhangPiotr Koniusz",
        "links": "http://arxiv.org/abs/2409.17137v1",
        "entry_id": "http://arxiv.org/abs/2409.17137v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17137v1",
        "summary": "Parameter-Efficient Fine-Tuning (PEFT) effectively adapts pre-trained vision\ntransformers to downstream tasks. However, the optimization for tasks\nperformance often comes at the cost of generalizability in fine-tuned models.\nTo address this issue, we theoretically connect smaller weight gradient norms\nduring training and larger datasets to the improved model generalization.\nMotivated by this connection, we propose reducing gradient norms for enhanced\ngeneralization and aligning fine-tuned model with the pre-trained counterpart\nto retain knowledge from large-scale pre-training data. Yet, naive alignment\ndoes not guarantee gradient reduction and can potentially cause gradient\nexplosion, complicating efforts to manage gradients. To address such issues, we\npropose PACE, marrying generalization of PArameter-efficient fine-tuning with\nConsistency rEgularization. We perturb features learned from the adapter with\nthe multiplicative noise and ensure the fine-tuned model remains consistent for\nsame sample under different perturbations. Theoretical analysis shows that PACE\nnot only implicitly regularizes gradients for enhanced generalization, but also\nimplicitly aligns the fine-tuned and pre-trained models to retain knowledge.\nExperimental evidence supports our theories. PACE outperforms existing PEFT\nmethods in four visual adaptation tasks: VTAB-1k, FGVC, few-shot learning and\ndomain adaptation. Code will be available at\nhttps://github.com/MaxwellYaoNi/PACE",
        "updated": "2024-09-25 17:56:00 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17137v1"
    },
    {
        "title": "Streaming Neural Images",
        "authors": "Marcos V. CondeAndy BigosRadu Timofte",
        "links": "http://arxiv.org/abs/2409.17134v1",
        "entry_id": "http://arxiv.org/abs/2409.17134v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17134v1",
        "summary": "Implicit Neural Representations (INRs) are a novel paradigm for signal\nrepresentation that have attracted considerable interest for image compression.\nINRs offer unprecedented advantages in signal resolution and memory efficiency,\nenabling new possibilities for compression techniques. However, the existing\nlimitations of INRs for image compression have not been sufficiently addressed\nin the literature. In this work, we explore the critical yet overlooked\nlimiting factors of INRs, such as computational cost, unstable performance, and\nrobustness. Through extensive experiments and empirical analysis, we provide a\ndeeper and more nuanced understanding of implicit neural image compression\nmethods such as Fourier Feature Networks and Siren. Our work also offers\nvaluable insights for future research in this area.",
        "updated": "2024-09-25 17:51:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17134v1"
    }
]