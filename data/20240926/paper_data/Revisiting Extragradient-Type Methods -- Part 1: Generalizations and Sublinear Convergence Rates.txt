Noname manuscript No.
(will be inserted by the editor)
Revisiting Extragradient-Type Methods – Part 1:
Generalizations and Sublinear Convergence Rates
Quoc Tran-Dinh · Nghia Nguyen-Trung
Received:date/Accepted:date
Abstract This paper presents a comprehensive analysis of the well-known extragradi-
ent (EG) method for solving both equations and inclusions. First, we unify and generalize
EG for [non]linear equations to a wider class of algorithms, encompassing various existing
schemes and potentially new variants. Next, we analyze both sublinear “best-iterate” and
“last-iterate” convergence rates for the entire class of algorithms, and derive new conver-
genceresultsfortwowell-knowninstances.Second,weextendourEGframeworkaboveto
“monotone”inclusions,introducinganewclassofalgorithmsanditscorrespondingconver-
gence results. Third, we also unify and generalize Tseng’s forward-backward-forward split-
ting (FBFS) method to a broader class of algorithms to solve [non]linear inclusions when
a weak-Minty solution exists, and establish its “best-iterate” convergence rate. Fourth, to
completeourpicture,wealsoinvestigatesublinearratesoftwoothercommonvariantsofEG
using our EG analysis framework developed here: the reflected forward-backward splitting
and the golden ratio methods. Finally, we conduct an extensive numerical experiment to
validate our theoretical findings. Our results demonstrate that several new variants of our
proposedalgorithmsoutperformexistingschemesinthemajorityofexamples.
Keywords Extragradient method weak-Minty solution monotonicity
· · ·
sublinear convergence rate variational inequality minimax problem
· ·
Mathematics Subject Classification (2000) 90C25 90C06 90-08
· ·
1 Introduction
The generalized equation (also called the [non]linear inclusion) provides a
unified template to model various problems in computational mathematics
and related fields such as optimization problems (both unconstrained and
QuocTran-Dinh·NghiaNguyen-Trung
DepartmentofStatisticsandOperationsResearch
TheUniversityofNorthCarolinaatChapelHill
318HanesHall,UNC-ChapelHill,NC27599-3260
Corresponding author: quoctd@email.unc.edu
4202
peS
52
]CO.htam[
1v95861.9042:viXra2 QuocTran-Dinh,NghiaNguyen-Trung
constrained), minimax optimization, variational inequality, complementarity,
game theory, and fixed-point problems, see, e.g., [6,13,33,75,79,81,83]. This
mathematical model has also found various direct applications in operations
research, economics, uncertainty quantification, and transportations, see, e.g.,
[8,35,40,33,45]. Moreover, recent advancements in machine learning and ro-
bustoptimization,particularlyingenerativeadversarialnetworks(GANs)and
adversarial training, have led to a surge of interest in minimax problems, a
specialcaseofgeneralizedequations[3,8,37,47,53,77].Thesemodelshavealso
found applications in online learning and reinforcement learning, see, e.g., [3,
4,9,37,43,47,48,53,77,95].Suchprominentapplicationshavemotivatedanew
research trend for [non]linear inclusions in the last many years.
Problem statement. In this paper, we consider the following generalized
equation (also known as a [composite] [non]linear inclusion):
Find x⋆ dom(Φ) such that: 0 Φx⋆ Fx⋆+Tx⋆, (NI)
∈ ∈ ≡
where F : Rp Rp is a single-valued mapping, T : Rp ⇒ 2Rp is a set-
valued(ormulti→ valued)mappingfromRp to2Rp (thesetofallsubsetsofRp),
Φ := F +T, and dom(Φ) := dom(F) dom(T) is the domain of Φ. Here, we
will focus on the finite-dimensional Eu∩ clidean spaces Rp. However, it is worth
noting that most results presented in this paper can be extended to Hilbert
spaces as demonstrated in the existing literature.
Special cases. We are also interested in various special cases of (NI) as fol-
lows. If T =0, then (NI) reduces to a [non]linear equation:
Find x⋆ dom(F) such that: Fx⋆ =0. (NE)
∈
IfT :=∂g,thesubdifferentialofaproper,closed,andconvexfunctiong,then
(NI) reduces a mixed variational inequality problem (MVIP) of the form:
Find x⋆ Rp such that: Fx⋆,x x⋆ +g(x) g(x⋆) 0, x Rp. (MVIP)
∈ ⟨ − ⟩ − ≥ ∀ ∈
In particular, if T = , the normal cone of a nonempty, closed, and convex
set inRp (i.e.g =δNX ,theindicatorof ),then(MVIP)reducestheclassical
(StaX mpacchia’s) variaXtional inequality prX oblem (VIP):
Find x⋆ such that: Fx⋆,x x⋆ 0, for all x . (VIP)
∈X ⟨ − ⟩≥ ∈X
Another important special case of (NI) is the following minimax problem:
(cid:110) (cid:111)
min max (u,v):=φ(u)+ (u,v) ψ(v) , (1)
u Rmv Rn L H −
∈ ∈
where φ : Rm R + and ψ : Rn R + are often proper,
closed, and conv→ ex fu∪ nc{ tio∞ ns,} and : Rm →Rn ∪ {R∞ is} a bifunction, often
H × →
assumed to be differentiable, but not necessarily convex-concave. If we denote
x := [u,v] as the concatenation of u and v, and define T := [∂φ,∂ψ] and
F :=[ (u,v), (u,v)], then the optimality condition of (1) is exactly
u v
∇ H −∇ H
covered by (NI) as a special case.RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 3
Related work. Theory and solution methods for (NI) and its special cases
havebeenextensivelystudiedfordecades.Numerousmonographsandarticles
have explored the existence of solutions, their properties, and various numeri-
cal methods. Researchers have investigated (NI) under a range of monotonic-
ity assumptions, including monotone, quasi-monotone, pseudo-monotone, hy-
pomonotone, and weakly monotone conditions. This has led to a rich body of
literature, with key references including [6,7,11,12,23,30,33,45,64,78,82,96].
Contemporary solution methods for (NI) and its special cases often rely
on a fundamental assumption: maximal monotonicity of F and T, or of Φ
to guarantee global convergences. These methods essentially generalize exist-
ing optimization algorithms such as gradient, proximal-point, Newton, and
interior-point schemes to (NI) and its special cases [23,33,34,58,70,71,81,90,
94], while leveraging the splitting structure of (NI) to use individual opera-
tors defined on F and T. This approach leads to a class of operator splitting
algorithms for solving (NI) such as forward-backward splitting (FBS) and
Douglas-Rachford (DRS) splitting schemes, as seen in [6,24,28,31,49,50]. Al-
ternatively,otherapproachesfor(NI)anditsspecialcasesrelyonprimal-dual,
dual averaging, and mirror descent techniques, with notable works including
[19,67,68], and many recent works such as [21,22,25,27,32,41,69,88,91,97].
Unlike optimization and minimax problems, convergence analysis of gra-
dient or forward-based methods for solving (NI) faces a fundamental chal-
lenge as (NI) does not have an objective function, which plays a central
role in constructing a Lyapunov or potential function to analyze convergence.
This drawback is even more critical when analyzing algorithms for solving
nonmonotone instances of (NI). Additionally, unlike convex functions where
strong properties such as coerciveness and cyclic monotonicity hold for their
[sub]gradients beyond monotonicity, this is not the case for general monotone
and Lipschitz continuous operators. This lack of a strong property results in
gradient/forward-based methods being non-convergent, which limits their ap-
plicability in practice, see, e.g., [33] for further details.
Toaddressthisissue,theextragradient(EG)methodwasintroducedbyG.
M.Korpelevichin1976[46]andalsobyA.S.Antipinin[2].Thismethodper-
formstwosequentialgradient/forwardstepsateachiteration,makingittwice
as expensive as the standard gradient method, but convergent under only the
monotonicity and the Lipschitz continuity of F. Since then, this method has
been extended and modified in different directions to reduce its per-iteration
complexity, including in certain nonmonotone settings, see [1,16,17,42,44,54,
56,57,62,63,76,84,85,86,92,93] for various representative examples. Among
thesevariantsofEG,thepast-extragradientschemein[76]andTeng’sforward-
backward-forwardsplittingmethodin[93]arethemostnotableones.However,
the results discussed in those are only applicable to the monotone setting of
(NI) and its special cases. Additionally, most of the convergence results dis-
cussed in the literature are asymptotic, leading to sublinear “best-iterate”
convergence rates of the residual term associated with (NI). Under stronger
assumptions such as “strong monotonicity” or error bound conditions, linear
convergenceratescanbeachieved.Suchtypesofconvergenceguaranteeshave4 QuocTran-Dinh,NghiaNguyen-Trung
been widely studied in the literature and are beyond the scope of this pa-
per, see, e.g., [6,33,45] for concrete examples. A non-exhaustive summary of
classical and recent results can be found in Table 1.
Fromanapplicationperspective,motivatedbyrecentmachinelearningand
robust optimization applications, such as Generative Adversarial Networks
(GANs), adversarial training, distributionally robust optimization, reinforce-
mentlearning,andonlinelearning,efficientmethodsforsolvingminimaxprob-
lemshavebecomecriticallyimportantandattractive.Thisisparticularlytrue
in nonconvex-nonconcave, large-scale, and stochastic settings, as evidenced in
works such as [3,4,8,9,37,43,47,48,53,77]. Several researchers have proposed
and revisited EG and its variants, including [10,26,29,73]. A notable work is
due to [29], where the authors proposed an EG-plus (EG+) variant of EG,
capable of handling nonmonotone instances of (NE), known as weak-Minty
solutions. In [73], this method was further extended to (NI), while [10,52]
modified EG+ for Popov’s methods, as well as optimistic gradient variants.
Table1 Summaryofexistingandourresultsinthispaperandthemostrelatedreferences
Methods Assumptions Add.Assumptions ConvergenceRates References
Forsolving(NE)
EG/EG+/FBFS F iswMs None O(cid:0)1/√ k(cid:1)best-iterate [29,73]
F ismono None O(cid:0)1/√ k(cid:1)last-iterate [36,38]
PEG/OG/FRBS/RFBS/GR F iswMs F ischm O(cid:0)1/√ k(cid:1)best-iterate [10,89]
F ismono F is“coherent” O(cid:0)1/√ k(cid:1)last-iterate [59,61]
GEG/GPEG F iswMs F ischm O(cid:0)1/√ k(cid:1)bestandlast Ours∗
Forsolving(NI),(MVIP),and(VIP)
GE EG G/ /E GG P+
EG
Φ Φi is sm mo on no
o
FF isis mm oo nn oo ,, TT is= 3N -cX
m
O O(cid:0) (cid:0)1 1/ /√ √k k(cid:1) (cid:1)b be es st ta an nd dl la as st
t
O[1 u4 r]
s
FBFS ΦiswMs None O(cid:0)1/√ k(cid:1)best-iterate [73]
OG/FRBS Φismono None O(cid:0)1/√ k(cid:1)best-iterate [56]
GFBFS/GFRBS ΦiswMs None O(cid:0)1/√ k(cid:1)best-iterate Ours∗
R RF FB BS
S
F F
F
i i is s sm m mo o on n no o
o
T=N TX T iso =r mT N oX nis omono OO O (cid:0)(cid:0) 1(cid:0)1 1 // / √√ √ kk k (cid:1)(cid:1) (cid:1) bb l eae sss ttt - a- ii t nt ee drr aa lt at e se
t
[1 O[8 1 u, 55 r] s4]
GR F ismono T=∂g O(cid:0)1/√ k(cid:1)best-iterate [55]
GR+ F ismono T is3-cm O(cid:0)1/√ k(cid:1)best-iterate Ours
Abbreviations:EG=extragradient;PEG=pastextragradient;FBFS=
forward-backward-forwardsplitting;OG=optimisticgradient;FRBS=
forward-reflected-backwardsplitting;RFBS=reflected-forward-backwardsplitting;GR
=goldenratio.Inaddition,wMs=weak-Mintysolution;mono=monotone;chm=
co-hypomonotone;and3-cm=3-cyclicallymonotone.
∗Thelast-iterateratesof EG/EG+/FBFS/PEG/OG/FRBS/RFBS/GRfor(NE)un-
dertheco-hypomonotonicityofF werefirstproveninourunpublishedreport[52].
Our approach and contribution. The EG method [46] and its variants for
solving (NI) can be written as xk+1 :=Jˆ (xk ηdk), where η >0 is a given
ηT
−
stepsize,dk isasearchdirection,andJˆ isanapproximationoftheresolvent
ηT
J of ηT. Hitherto, existing works primarily focus on:
ηT
(i) Instantiatingdk toobtainaspecificvariantandthenstudyingconvergence
ofsuchascheme.Forexample,ifwechoosedk :=F(J (xk ηFxk)),then
ηT
−
we get the classical EG algorithm in [46], while if we set dk :=F(yk) with
yk :=J (xk ηFyk 1), then we get Popov’s past-EG scheme in [76].
ηT −
−
(ii) Approximating J by a simpler operator Jˆ . For instance, if T = δ ,
ηT ηT
X
then we can replace J by the projection onto an appropriate half-space.
ηTRevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 5
While (ii) has been intensively studied, e.g., in [16,33,45,57], especially when
T = ,anditisnotourmainfocusinthispaper,(i)coversafewnon-unified
NX
schemessuchas[46,54,55,56,76,93]andisfarfrombeingcomprehensive.The
latter raises the following question:
Can we unify and generalize EG to cover broader classes of algorithms?
In this paper, we attempt to tackle this question by unifying and generalizing
EG to three broader classes of algorithms to solve both (NE) and (NI) as
showninTable1.Specifically,ourcontributioncanbesummarizedasfollows.
(a) First,weunifyandgeneralizeEGtoawiderclassofalgorithmsforsolving
(cid:0) (cid:1)
(NE). We prove both 1/√k -best-iterate and last-iterate convergence
O
rates of this generalized scheme. Then, we specify our results to cover
various existing variants from the literature, where our results apply.
(b) Second, we unify and generalize EG to a broader class of schemes for
(cid:0) (cid:1)
solving (NI), and establish its 1/√k -best-iterate and last-iterate con-
O
vergence rates under a “monotonicity” assumption. Again, we specify our
methodtocover:thestandardEGschemeandPopov’spast-EGalgorithm.
(c) Third, we develop a generalization of Tseng’s FBFS method, and provide
(cid:0) (cid:1)
a new unified 1/√k -best-iterate convergence rate for this scheme. Our
O
result covers both Tseng’s classical FBFS algorithm and the FRBS (also
equivalent to the optimistic gradient) method as its instances.
(d) Fourth,wepresentanewbest-iterateandlast-iterateconvergenceanalysis
forthereflectedforward-backwardsplitting(RFBS)methodstosolve(NI)
under a “monotonicity” assumption. Alternatively, we also analyze the
best-iterate convergence rate of the golden ratio (GR) method in [55] by
extending T to a 3-cyclically monotone operator and the range of the GR
parameter τ from a fixed value τ := 1+√5 to 1<τ <1+√3.
2
(e) Finally, we implement our algorithms and their competitors and perform
an extensive test on different problems. Our results show some promising
improvement of the new methods over existing ones on various examples.
Comparison. Since EG and its common variants in Table 1 are classical,
their convergence analysis is known and can be found in the literature. Here,
we only compare our results with the most recent and related work.
First, our generalized EG (called GEG) scheme for (NE) covers both the
classical EG [46] and Popov’s past-EG [76] methods as instances, and allows
to derive new variants. As we will show in Subsection 3.1, these two existing
instancesalreadycoveralmostknownmethodsintheliterature.Nevertheless,
ourGEGhasaflexibilitytochooseasearchdirection,calleduk,topotentially
improveitsperformanceoverEGorpast-EG(seeVariant3inSubsection3.1).
Our convergence and convergence rate guarantees are new and covers many
known results, including [10,29,36,38,39,52,73]. Note that the best-iterate
rate has recently been obtained when a weak-Minty solution of (NE) exists,
see [29,73] for EG and [10,52] for past-EG. The last-iterate convergence rates
for EG and past-EG have been recently proven in [36,38] for the monotone
equation (NE), and in our unpublished report [52] for the co-hypomonotone6 QuocTran-Dinh,NghiaNguyen-Trung
case (see also [39] for a refined analysis). Our analysis in this paper is rather
elementary, unified, and thus different from those.
Second, existing convergence analysis for the EG method often focuses on
(VIP) and (MVIP). In this paper, we extend the analysis for a class of (NI),
whereT is3-cyclicallymonotone.Thisclassofoperatoristheoreticallybroader
thanT =∂g.Moreover,ourgeneralizationofEGinthissettingcoversdifferent
existing schemes, while also allows us to derive new variants. Our last-iterate
convergencerateanalysisisnewandmuchbroaderthanaspecialcasein[14].
Third, again, our FBFS scheme is also broader and covers both Tseng’s
method [93] and the FRBS method in [56] as specific instances. Our best-
iterateconvergencerateanalysisisforaweak-Mintysolution,whichisbroader
than those in [56,93]. It also covers the results in [52,73].
Fourth, RFBS was proposed in [54] to solve (VIP) and was extended to
solve (NI) in [18]. The best-iterate rates were proven in these works, and the
last-iterate rate of RFBS for solving (VIP) has recently been proven in [15].
Our result here is more general and covers these works as special cases. Our
analysis in both cases is also different from [18] and [15].
Finally,wenotethatafterourunpublishedreport[52]andthefirstdraftof
this paper [87] were online, we find some concurrent and following-up papers
such as [10,39,72,74] that are related to our results presented in this paper,
but we do not including them as existing results in Table 1.
Paperoutline.Therestofthispaperisorganizedasfollows.Section2reviews
basicconceptsandrelatedresultsusedinthispaper.Section3generalizesand
investigates the convergence rates of EG and its variants for solving (NE).
Section 4 focuses on the EG method and its variants for solving (NI). Section
5studiestheFBFSmethodanditsvariantsforsolving(NI).Section6provides
a new convergence rate analysis of FBFS and OG for solving (NI). Section 7
presents several numerical examples to validate our theoretical results.
2 Background and Preliminary Results
We recall several concepts which will be used in this paper. These concepts
and properties are well-known and can be found, e.g., in [6,33,79,80,83].
2.1 Basic Concepts, Monotonicity, and Lipschitz Continuity
We work with finite dimensional Euclidean spaces Rp and Rn equipped with
standard inner product , and Euclidean norm . For a multivalued
mapping T : Rp ⇒ 2Rp ,⟨· do·⟩ m(T) = x Rp :Tx=∥ · ∥ denotes its domain,
ran(T):=(cid:83) Txisitsrange,an{ dg∈ ra(T)= (̸ x,∅ y} ) Rp Rp :y Tx
x dom(T) { ∈ × ∈ }
standsforits∈ graph,where2Rp isthesetofallsubsetsofRp.Theinversemap-
ping of T is defined as T 1y := x Rp : y Tx . For a proper, closed,
−
and convex function f : Rp R { +∈ , dom(∈ f) :=} x Rp : f(x) < +
→ ∪{ ∞} { ∈ ∞}
denotes the domain of f, ∂f denotes the subdifferential of f, and f stands
∇
for the [sub]gradient of f. For a symmetric matrix Q, λ (Q) and λ (Q)
min max
denote the smallest and largest eigenvalues of Q, respectively.RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 7
(a) Monotonicity. For a multivalued mapping T : Rp ⇒ 2Rp and µ R, we
∈
say that T is µ-monotone if
u v,x y µ x y 2, (x,u),(y,v) gra(T).
⟨ − − ⟩≥ ∥ − ∥ ∀ ∈
IfT issingle-valued,thenthisconditionreducesto Tx Ty,x y µ x y 2
⟨ − − ⟩≥ ∥ − ∥
for all x,y dom(T). If µ = 0, then we say that T is monotone. If µ > 0,
∈
then T is µ-strongly monotone (or sometimes called coercive). If µ < 0, then
we say that T is weakly monotone. It is also called µ-hypomonotone, see [7].
| |
If T =∂g, the subdifferential of a proper and convex function, then T is also
monotone.Ifg isµ-stronglyconvex,thenT =∂g isalsoµ-stronglymonotone.
We say that T is ρ-comonotone if there exists ρ R such that
∈
u v,x y ρ u v 2, (x,u),(y,v) gra(T).
⟨ − − ⟩≥ ∥ − ∥ ∀ ∈
Ifρ=0,thenthisconditionreducestothemonotonicityofT.Ifρ>0,thenT
is called ρ-co-coercive. In particular, if ρ = 1, then T is firmly nonexpansive.
If ρ < 0, then T is called ρ-co-hypomonotone, see, e.g., [7,23]. Note that a
| |
co-hypomonotone operator can also be nonmonotone. As a concrete example,
consider a linear mapping Fx = Qx + q for a symmetric matrix Q and a
vector q. If Q is invertible then it is obvious to check that Fx Fy,x y
ρ Fx Fy 2 for all x,y Rp, where ρ:=λ (Q 1). If ρ⟨ <0,− then F− is ⟩ ρ≥ -
min −
∥ − ∥ ∈ | |
co-hypomonotone,butF isnotmonotonesinceQisnotpositivesemidefinite.
We say that T is maximally µ-monotone if gra(T) is not properly con-
tained in the graph of any other µ-monotone operator. If µ = 0, then we
say that T is maximally monotone. These definitions are also extended to
ρ-cohypomonotone operators. For a proper, closed, and convex function f :
|Rp| R + , the subdifferential ∂f of f is maximally monotone.
→ ∪{ ∞}
For a given mapping T such that zer(T) := x dom(T):0 Tx =
{ ∈ ∈ } ̸
, we say that T is star-monotone (respectively, µ-star-monotone or ρ-star-
∅
comonotone (see [51])) if for some x⋆ zer(T), we have u,x x⋆ 0
∈ ⟨ − ⟩ ≥
(respectively, u,x x⋆ µ x x⋆ 2 or u,x x⋆ ρ u 2) for all (x,u)
⟨ − ⟩≥ ∥ − ∥ ⟨ − ⟩≥ ∥ ∥ ∈
gra(T). A solution x⋆ zer(T) satisfying a ρ-star-comonotonicity is called a
∈
weak-Mintysolution.Clearly,ifT ismonotone(respectively,µ-monotoneorρ-
co-monotone), then it is also star-monotone (respectively, µ-star-monotone or
ρ-star-comonotone). However, the reverse statement does not hold in general.
(b)Cyclicmonotonicity.WesaythatamappingT ism-cyclicallymonotone
(m 2) if (cid:80)m ui,xi xi+1 0 for all (xi,ui) gra(T) and x = x
≥ i=1⟨ − ⟩ ≥ ∈ 1 m+1
(see [6]). We say that T is cyclically monotone if it is m-cyclically monotone
for every m 2. If T is m-cyclically monotone, then it is also mˆ-cyclically
≥
monotone for any 2 mˆ m. Since a 2-cyclically monotone operator T is
≤ ≤
monotone, any m-cyclically monotone operator T is 2-cyclically monotone,
and thus is also monotone. An m-cyclically monotone operator T is called
maximally m-cyclically monotone if gra(T) is not properly contained into the
graph of any other m-cyclically monotone operator.
As proven in [6, Theorem 22.18] that T is maximally cyclically monotone
iff T = ∂f, the subdifferential of a proper, closed, and convex function f.8 QuocTran-Dinh,NghiaNguyen-Trung
However, there exist maximally m-cyclically monotone operators (e.g., rota-
tion linear operators) that are not the subdifferential ∂f of a proper, closed,
and convex function f, see, e.g., [6]. Furthermore, as indicated in [5, Exam-
ple 2.16], there exist maximally 3-cyclically monotone operators that are not
maximally monotone.
(c) Lipschitz continuity and contraction. A single-valued mapping F is
called L-Lipschitz continuous if Fx Fy L x y for all x,y dom(F),
∥ − ∥≤ ∥ − ∥ ∈
where L 0 is the Lipschitz constant. If L = 1, then we say that F is
≥
nonexpansive, while if L [0,1), then we say that F is L-contractive.
∈
(d) Normal cone. Given a nonempty, closed, and convex set in Rp, the
normal cone of is defined as (x) := w Rp : w,x y X 0, y
X NX { ∈ ⟨ − ⟩ ≥ ∀ ∈ X}
if x and (x) = , otherwise. If f := δ , the indicator of a convex set
∈ X NX ∅ X
, then we have ∂f = . Moreover, J reduces to the projection onto .
∂f
X NX X
(e) Resolvent and proximal operators. Given a multivalued mapping T,
theoperatorJ x:= y Rp :x y+Ty iscalledtheresolventofT,denoted
T
byJ x=(I+T) 1x{ ,wh∈ ereIist∈ heidenti} tymapping.IfT isρ-monotonewith
T −
ρ > 1, then evaluating J requires solving a strongly monotone inclusion
T
−
0 y x+Ty. Hence, J is well-defined and single-valued. If T = ∂f, the
T
∈ −
subdifferential of proper, closed, and convex function f, then J reduces to
T
the proximal operator of f, denoted by prox , which can be computed as
f
prox (x) := argmin f(y)+(1/2) y x 2 . In particular, if T = , the
f y { ∥ − ∥ } NX
normal cone of a closed and convex set , then J is the projection onto ,
T
denoted by proj . If T is maximally mX onotone, then ran(I+T) = Rp (X by
Minty’s theorem)Xand T is firmly nonexpansive (and thus nonexpansive).
2.2 Best-Iterate and Last-Iterate Convergence Rates
The results presented in this paper are related to two types of sublinear con-
vergence rates: the best-iterate and the last-iterate convergence rates. To
elaborateontheseconcepts,weassumethat isagivenmetric(e.g., Fxk 2
D ∥ ∥
or e(xk)2 defined by (2) below) defined on an iterate sequence xk generated
{ }
bytheunderlyingalgorithmforsolving(NI)oritsspecialcases.Foranyk 0
≥
and a given order ν >0, if
k (cid:18) (cid:19)
1 (cid:88) 1
min (xl) (xl)= ,
0 l kD ≤ k+1 D O kν
≤≤ l=0
then we say that xk has a (1/kν)-best-iterate convergence rate. In this
{ } O
case, we can take xˆ := x with k := argmin (xl) as the “best”
k kmin min 0 ≤l ≤k
D
output of the underlying algorithm.
Ifweinsteadhave (xk)= (cid:0) 1 (cid:1) withxk beingthek-thiterategenerated
D O kν
bythealgorithm,thenwesaythat xk hasa (1/kν)last-iterateconvergence
{ } O
rate. We emphasize that the convergence on the metric of xk does not
D { }
generallyimplytheconvergenceof xk itself,especiallywhenwecharacterize
{ }
the rate of convergence in different metrics.RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 9
2.3 Exact Solutions and Approximate Solutions
Thereare differentmetrics tocharacterizeexactand approximatesolutions of
(NI). The most obvious one is the residual norm of Φ, which is defined as
e(x):= min Fx+ξ , x dom(Φ). (2)
ξ Tx∥ ∥ ∈
∈
Clearly, if e(x⋆) = 0 for some x⋆ dom(Φ), then x⋆ zer(Φ), a solution of
∈ ∈
(NI). If T = 0, then e(x) = Fx . However, if e(xˆ) ϵ for a given tolerance
∥ ∥ ≤
ϵ>0,thenxˆcanbeconsideredasanϵ-approximatesolutionof (NI).Thealgo-
rithms presented in this paper use this metric to characterize an approximate
solution of (NI) and its special cases.
Other metrics often used for monotone (VIP) are gap functions and re-
stricted gap functions [33,45,68], which are respectively defined as follows:
(x):=max Fy,y x and B(x):= max Fy,y x , (3)
G y ⟨ − ⟩ G y B⟨ − ⟩
∈X ∈X∩
whereBisagivennonempty,closed,andboundedconvexset.Notethat,under
the monotonicity, (x) 0 for all x , and (x⋆)=0 iff x⋆ is a solution of
G ≥ ∈X G
(VIP). Thus x˜ is an ϵ-approximate solution of (VIP) if (x˜) ϵ.
For the restricted gap function B, if x⋆ is a solutionG of (V≤ IP) and x⋆ B,
then B(x⋆)=0. Conversely, if B(G x⋆)=0 and x⋆ int(B), the interior o∈ f B,
then G x⋆ is a solution of (VIP) iG n B (see [68, Lemm∈ a 1]). Gap functions have
widely been used in the literature to characterize approximate solutions for
many numerical methods, see, e.g., [22,25,33,45,67,68] as concrete examples.
Note that for nonmonotone cases, gap functions are not applicable in general.
If J is well-defined and single-valued for some η > 0, and F is single-
ηT
valued, then we can use the following forward-backward splitting residual:
x:= 1 (x J (x ηFx)), (4)
Gη η − ηT −
to characterize solutions of (NI). It is clear that x⋆ = 0 iff x⋆ zer(Φ). In
η
G ∈
addition, if J is firmly nonexpansive, then we also have
ηT
x Fx+ξ , (x,ξ) gra(T). (5)
η
∥G ∥≤∥ ∥ ∈
Hence,foragiventoleranceϵ>0,if x˜ ϵ,thenwecansaythatx˜isanϵ-
η
∥G ∥≤
approximatesolutionof (NI).IfT := ,i.e.(NI)reducesto(VIP),then,with
NX
η =1, xreducestotheclassicalnaturalmapΠ x=x proj (x Fx)of
1 F,
(VIP),G
and r (x) := x = Π x is the
correX spondin−
g
natXural−
residual
n 1 F,
∥G ∥ ∥ X ∥
at x, see [33]. From (5), we have r (x) Fx+ξ for any ξ (x).
n
≤∥ ∥ ∈NX
2.4 The Forward-Type or Fixed-Point Methods
Let us briefly recall the gradient/forward or fixed-point scheme for solving
(NE) as follows. Starting from x0 dom(F), at each iteration k, we update
∈
xk+1 :=xk ηFxk, (FW)
−
where η > 0 is a given constant stepsize. The forward scheme is a classical
method to solve (NE). This scheme covers the gradient descent method for10 QuocTran-Dinh,NghiaNguyen-Trung
minimizing f(x) as a special case with F = f. If G := I ηF, then (FW)
∇ −
becomes the well-known fixed-point iteration to approximate a fixed-point
of G. We recall this method here to compare with EG methods.
If F is ρ-co-coercive and 0 < η < ρ, then xk converges to x⋆ zer(F)
{ } ∈
(see, e.g., [33]). Otherwise, if F is only monotone and L-Lipschitz continu-
ous, then there exist examples (e.g., Fx = [x , x ]) showing that (FW) is
2 1
−
divergent for any choice of stepsize η even if F is monotone.
Tosolve(NI),wecaninsteadapplytheforward-backwardsplittingmethod
as follows. Starting from x0 dom(F), at each iteration k 0, we update
∈ ≥
xk+1 :=J (xk ηFxk), (FBS)
ηT
−
whereη >0isagivenconstantstepsize.Similarto(FW),ifF isρ-co-coercive
and T is maximally monotone, then with η (0,ρ), xk generated by (FBS)
∈ { }
converges to x⋆ zer(Φ). If F = f, the gradient of a convex and L-smooth
∈ ∇
function f, then F is co-coercive. However, imposing the co-coerciveness for a
general mapping F is often restrictive. Hence, both (FW) and (FBS) are less
practical in applications due to the absence of the co-coerciveness.
3 A Class of Extragradient-Type Methods for Nonlinear Equations
In this section, we unify and generalize EG to a wider class of algorithms for
solve(NE).Then,weanalyzeitsconvergenceratesandstudyitsspecialcases.
3.1 A Class of Extragradient Methods for Equations
Our class of extragradient methods for solving (NE) is presented as follows.
Startingfromaninitialpointx0 dom(F),ateachiterationk 0,weupdate
∈ ≥
(cid:40) yk := xk ηuk,
− β (GEG)
xk+1 := xk ηFyk,
−
where η > 0 is a given constant stepsize, β (0,1] is a scaling factor, and
uk Rp satisfies the following condition: ∈
∈
Fxk uk 2 κ Fxk Fyk 1 2+κ Fxk Fxk 1 2, (6)
1 − 2 −
∥ − ∥ ≤ ∥ − ∥ ∥ − ∥
for given constants κ 0 and κ 0 and x 1 = y 1 := x0. Under different
1 2 − −
≥ ≥
choices of uk, (GEG) covers a wide range of methods generalizing the extra-
gradient method (EG) in [46]. Note that condition (6) also makes (GEG)
different from the hybrid extragradient method in, e.g., [63,84]. If we define
dk :=F(xk ηuk), then (GEG) becomes xk+1 :=xk ηdk, which falls into a
− β −
generic iterative scheme we have discussed earlier.
Let us consider the following three choices of uk which fulfill (6).
(a) Variant 1: Extragradient method. If we choose uk := Fxk, then we
obtaintheextragradient(EG)scheme[46]from(GEG)forsolving(NE).
Clearly, uk satisfies (6) with κ =κ =0.
1 2RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 11
(b) Variant 2: Past-extragradient method. If we choose uk := Fyk 1,
−
then we obtain the Popov’s past-extragradient method [76] to solve
(NE). This scheme is also equivalent to the optimistic gradient method
in the literature, see also [26,59,61] for more details. Clearly, uk satisfies
(6) with κ =1 and κ =0.
1 2
(c) Variant3:Generalization.Wecanconstructuk :=α Fxk+α Fyk 1+
1 2 −
(1 α α )Fxk 1 as an affine combination of Fxk, Fyk 1 and Fxk 1
1 2 − − −
for− given− constants α ,α R. Then, uk satisfies (6) with κ = (1+c)α2
1 2 ∈ 1 2
and κ = (1+c 1)(1 α α )2 by Young’s inequality for any c > 0.
2 − 1 2
− −
This variant essentially has the same per-iteration complexity as Variant
1, but it covers all candidates as combinations of xk, yk 1, and xk 1.
− −
Figure 1 illustrates the three variants of (GEG) presented above.
<latexit sha1_base64="IsTKdBmyfRGlW6kgP8TflrLdTe4=">AAAB7nicbVBNSwMxEJ31s9avqkcvwSJ4seyKVI9FQTxWsB/QriWbZtuw2WxIsmJZ+iO8eFDEq7/Hm//GtN2Dtj4YeLw3w8y8QHKmjet+O0vLK6tr64WN4ubW9s5uaW+/qZNUEdogCU9UO8CaciZowzDDaVsqiuOA01YQXU/81iNVmiXi3owk9WM8ECxkBBsrtU5vnh6yaNwrld2KOwVaJF5OypCj3it9dfsJSWMqDOFY647nSuNnWBlGOB0Xu6mmEpMID2jHUoFjqv1seu4YHVulj8JE2RIGTdXfExmOtR7Fge2MsRnqeW8i/ud1UhNe+hkTMjVUkNmiMOXIJGjyO+ozRYnhI0swUczeisgQK0yMTahoQ/DmX14kzbOKV61U787Ltas8jgIcwhGcgAcXUINbqEMDCETwDK/w5kjnxXl3PmatS04+cwB/4Hz+ACUjj3c=</latexit> Fxk y<latexit sha1_base64="7GARkq8p7XegHQmjI8Mpd37os+8=">AAAB6nicdVDLSsNAFJ3UV62vqks3g0VwFSahpnVXdOOyon1AG8tkOm2HTiZhZiKE0E9w40IRt36RO//GSVtBRQ9cOJxzL/feE8ScKY3Qh1VYWV1b3yhulra2d3b3yvsHbRUlktAWiXgkuwFWlDNBW5ppTruxpDgMOO0E08vc79xTqVgkbnUaUz/EY8FGjGBtpJv0bjooV5B9XvfcMwciGyGvjrycuLWq60LHKDkqYInmoPzeH0YkCanQhGOleg6KtZ9hqRnhdFbqJ4rGmEzxmPYMFTikys/mp87giVGGcBRJU0LDufp9IsOhUmkYmM4Q64n67eXiX14v0aO6nzERJ5oKslg0SjjUEcz/hkMmKdE8NQQTycytkEywxESbdEomhK9P4f+k7dqOZ3vX1UrjYhlHERyBY3AKHFADDXAFmqAFCBiDB/AEni1uPVov1uuitWAtZw7BD1hvn8oajik=</latexit> k
 <latexit sha1_base64="X15J7cExp2ggziWbxRVehmyZMdI=">AAAB8HicdVBNSwMxEM3Wr1q/qh69BIvgpctuLbW9FQXxWMG2SruWbJptQ5PskmSFZemv8OJBEa/+HG/+G9N2BRV9MPB4b4aZeX7EqNKO82HllpZXVtfy64WNza3tneLuXkeFscSkjUMWyhsfKcKoIG1NNSM3kSSI+4x0/cn5zO/eE6loKK51EhGPo5GgAcVIG+m2fJHcpZOyOx0US47dqDfcehU6tjOHIY0Tt1KpQjdTSiBDa1B87w9DHHMiNGZIqZ7rRNpLkdQUMzIt9GNFIoQnaER6hgrEifLS+cFTeGSUIQxCaUpoOFe/T6SIK5Vw33RypMfqtzcT//J6sQ7qXkpFFGsi8GJREDOoQzj7Hg6pJFizxBCEJTW3QjxGEmFtMiqYEL4+hf+TTsV2a3btqlpqnmVx5MEBOATHwAWnoAkuQQu0AQYcPIAn8GxJ69F6sV4XrTkrm9kHP2C9fQJblZAm</latexit> Fyk  1
 <latexit sha1_base64="IsTKdBmyfRGlW6kgP8TflrLdTe4=">AAAB7nicbVBNSwMxEJ31s9avqkcvwSJ4seyKVI9FQTxWsB/QriWbZtuw2WxIsmJZ+iO8eFDEq7/Hm//GtN2Dtj4YeLw3w8y8QHKmjet+O0vLK6tr64WN4ubW9s5uaW+/qZNUEdogCU9UO8CaciZowzDDaVsqiuOA01YQXU/81iNVmiXi3owk9WM8ECxkBBsrtU5vnh6yaNwrld2KOwVaJF5OypCj3it9dfsJSWMqDOFY647nSuNnWBlGOB0Xu6mmEpMID2jHUoFjqv1seu4YHVulj8JE2RIGTdXfExmOtR7Fge2MsRnqeW8i/ud1UhNe+hkTMjVUkNmiMOXIJGjyO+ozRYnhI0swUczeisgQK0yMTahoQ/DmX14kzbOKV61U787Ltas8jgIcwhGcgAcXUINbqEMDCETwDK/w5kjnxXl3PmatS04+cwB/4Hz+ACUjj3c=</latexit> Fxk  <latexit sha1_base64="X15J7cExp2ggziWbxRVehmyZMdI=">AAAB8HicdVBNSwMxEM3Wr1q/qh69BIvgpctuLbW9FQXxWMG2SruWbJptQ5PskmSFZemv8OJBEa/+HG/+G9N2BRV9MPB4b4aZeX7EqNKO82HllpZXVtfy64WNza3tneLuXkeFscSkjUMWyhsfKcKoIG1NNSM3kSSI+4x0/cn5zO/eE6loKK51EhGPo5GgAcVIG+m2fJHcpZOyOx0US47dqDfcehU6tjOHIY0Tt1KpQjdTSiBDa1B87w9DHHMiNGZIqZ7rRNpLkdQUMzIt9GNFIoQnaER6hgrEifLS+cFTeGSUIQxCaUpoOFe/T6SIK5Vw33RypMfqtzcT//J6sQ7qXkpFFGsi8GJREDOoQzj7Hg6pJFizxBCEJTW3QjxGEmFtMiqYEL4+hf+TTsV2a3btqlpqnmVx5MEBOATHwAWnoAkuQQu0AQYcPIAn8GxJ69F6sV4XrTkrm9kHP2C9fQJblZAm</latexit> Fyk  1
 <latexit sha1_base64="TDtJ8tW1eA+YRdkEdcHgaoMBvGM=">AAAB63icdVDLSsNAFJ3UV62vqks3g0VwY5iEmtZd0Y3LCvYBbSyT6aQdOpOEmYlQQn/BjQtF3PpD7vwbJ20FFT1w4XDOvdx7T5BwpjRCH1ZhZXVtfaO4Wdra3tndK+8ftFWcSkJbJOax7AZYUc4i2tJMc9pNJMUi4LQTTK5yv3NPpWJxdKunCfUFHkUsZATrXDpL7yaDcgXZF3XPPXcgshHy6sjLiVurui50jJKjApZoDsrv/WFMUkEjTThWquegRPsZlpoRTmelfqpogskEj2jP0AgLqvxsfusMnhhlCMNYmoo0nKvfJzIslJqKwHQKrMfqt5eLf3m9VId1P2NRkmoakcWiMOVQxzB/HA6ZpETzqSGYSGZuhWSMJSbaxFMyIXx9Cv8nbdd2PNu7qVYal8s4iuAIHINT4IAaaIBr0AQtQMAYPIAn8GwJ69F6sV4XrQVrOXMIfsB6+wQttI5c</latexit> uk
y<latexit sha1_base64="7GARkq8p7XegHQmjI8Mpd37os+8=">AAAB6nicdVDLSsNAFJ3UV62vqks3g0VwFSahpnVXdOOyon1AG8tkOm2HTiZhZiKE0E9w40IRt36RO//GSVtBRQ9cOJxzL/feE8ScKY3Qh1VYWV1b3yhulra2d3b3yvsHbRUlktAWiXgkuwFWlDNBW5ppTruxpDgMOO0E08vc79xTqVgkbnUaUz/EY8FGjGBtpJv0bjooV5B9XvfcMwciGyGvjrycuLWq60LHKDkqYInmoPzeH0YkCanQhGOleg6KtZ9hqRnhdFbqJ4rGmEzxmPYMFTikys/mp87giVGGcBRJU0LDufp9IsOhUmkYmM4Q64n67eXiX14v0aO6nzERJ5oKslg0SjjUEcz/hkMmKdE8NQQTycytkEywxESbdEomhK9P4f+k7dqOZ3vX1UrjYhlHERyBY3AKHFADDXAFmqAFCBiDB/AEni1uPVov1uuitWAtZw7BD1hvn8oajik=</latexit> k
 
 <latexit sha1_base64="hftxwsmALVINzr4ODYsPV7OD+Lw=">AAAB7HicdVDLSgMxFM3UV62vqks3wSK4ccjUoY9dURCXFZy20I4lk2ba0ExmSDLCUPoNblwo4tYPcuffmD4EFT1w4XDOvdx7T5BwpjRCH1ZuZXVtfSO/Wdja3tndK+4ftFScSkI9EvNYdgKsKGeCepppTjuJpDgKOG0H48uZ376nUrFY3OosoX6Eh4KFjGBtJO/sKrsb94slZKNaBbkIIttB9TKqGVJ3z8tuFTo2mqMElmj2i++9QUzSiApNOFaq66BE+xMsNSOcTgu9VNEEkzEe0q6hAkdU+ZP5sVN4YpQBDGNpSmg4V79PTHCkVBYFpjPCeqR+ezPxL6+b6rDmT5hIUk0FWSwKUw51DGefwwGTlGieGYKJZOZWSEZYYqJNPgUTwten8H/SKttOxa7cuKXGxTKOPDgCx+AUOKAKGuAaNIEHCGDgATyBZ0tYj9aL9bpozVnLmUPwA9bbJ8Fzjq8=</latexit> Fyk
 <latexit sha1_base64="HGX0nrrZLE4sCyMYY19YTcPZQUI=">AAAB8HicdVDLSgMxFM3UV62vqks3wSK46TBTx7bLoiAuK9iHtGPJpGkbmmSGJCOWoV/hxoUibv0cd/6NaTuCih64cDjnXu69J4gYVdpxPqzM0vLK6lp2PbexubW9k9/da6owlpg0cMhC2Q6QIowK0tBUM9KOJEE8YKQVjM9nfuuOSEVDca0nEfE5Ggo6oBhpI90UL+5vk3HRnfbyBcd25oCOXamWTh3PELfkeicedFOrAFLUe/n3bj/EMSdCY4aU6rhOpP0ESU0xI9NcN1YkQniMhqRjqECcKD+ZHzyFR0bpw0EoTQkN5+r3iQRxpSY8MJ0c6ZH67c3Ev7xOrAdVP6EiijUReLFoEDOoQzj7HvapJFiziSEIS2puhXiEJMLaZJQzIXx9Cv8nzZLtlu3ylVeonaVxZMEBOATHwAUVUAOXoA4aAAMOHsATeLak9Wi9WK+L1oyVzuyDH7DePgE9TJAS</latexit> Fxk  1
y<latexit sha1_base64="7GARkq8p7XegHQmjI8Mpd37os+8=">AAAB6nicdVDLSsNAFJ3UV62vqks3g0VwFSahpnVXdOOyon1AG8tkOm2HTiZhZiKE0E9w40IRt36RO//GSVtBRQ9cOJxzL/feE8ScKY3Qh1VYWV1b3yhulra2d3b3yvsHbRUlktAWiXgkuwFWlDNBW5ppTruxpDgMOO0E08vc79xTqVgkbnUaUz/EY8FGjGBtpJv0bjooV5B9XvfcMwciGyGvjrycuLWq60LHKDkqYInmoPzeH0YkCanQhGOleg6KtZ9hqRnhdFbqJ4rGmEzxmPYMFTikys/mp87giVGGcBRJU0LDufp9IsOhUmkYmM4Q64n67eXiX14v0aO6nzERJ5oKslg0SjjUEcz/hkMmKdE8NQQTycytkEywxESbdEomhK9P4f+k7dqOZ3vX1UrjYhlHERyBY3AKHFADDXAFmqAFCBiDB/AEni1uPVov1uuitWAtZw7BD1hvn8oajik=</latexit> k
x<latexit sha1_base64="HjKWIzJR5ffh2H33LNfdixEluAI=">AAAB6nicbVDLTgJBEOzFF+IL9ehlIjHxRHaNQY9ELx4xyiOBlcwODUyYnd3MzBrJhk/w4kFjvPpF3vwbB9iDgpV0UqnqTndXEAuujet+O7mV1bX1jfxmYWt7Z3evuH/Q0FGiGNZZJCLVCqhGwSXWDTcCW7FCGgYCm8Hoeuo3H1FpHsl7M47RD+lA8j5n1Fjp7ulh1C2W3LI7A1kmXkZKkKHWLX51ehFLQpSGCap123Nj46dUGc4ETgqdRGNM2YgOsG2ppCFqP52dOiEnVumRfqRsSUNm6u+JlIZaj8PAdobUDPWiNxX/89qJ6V/6KZdxYlCy+aJ+IoiJyPRv0uMKmRFjSyhT3N5K2JAqyoxNp2BD8BZfXiaNs7JXKVduz0vVqyyOPBzBMZyCBxdQhRuoQR0YDOAZXuHNEc6L8+58zFtzTjZzCH/gfP4AZaWN5A==</latexit> k
 <latexit sha1_base64="hftxwsmALVINzr4ODYsPV7OD+Lw=">AAAB7HicdVDLSgMxFM3UV62vqks3wSK4ccjUoY9dURCXFZy20I4lk2ba0ExmSDLCUPoNblwo4tYPcuffmD4EFT1w4XDOvdx7T5BwpjRCH1ZuZXVtfSO/Wdja3tndK+4ftFScSkI9EvNYdgKsKGeCepppTjuJpDgKOG0H48uZ376nUrFY3OosoX6Eh4KFjGBtJO/sKrsb94slZKNaBbkIIttB9TKqGVJ3z8tuFTo2mqMElmj2i++9QUzSiApNOFaq66BE+xMsNSOcTgu9VNEEkzEe0q6hAkdU+ZP5sVN4YpQBDGNpSmg4V79PTHCkVBYFpjPCeqR+ezPxL6+b6rDmT5hIUk0FWSwKUw51DGefwwGTlGieGYKJZOZWSEZYYqJNPgUTwten8H/SKttOxa7cuKXGxTKOPDgCx+AUOKAKGuAaNIEHCGDgATyBZ0tYj9aL9bpozVnLmUPwA9bbJ8Fzjq8=</latexit> Fyk  <latexit sha1_base64="hftxwsmALVINzr4ODYsPV7OD+Lw=">AAAB7HicdVDLSgMxFM3UV62vqks3wSK4ccjUoY9dURCXFZy20I4lk2ba0ExmSDLCUPoNblwo4tYPcuffmD4EFT1w4XDOvdx7T5BwpjRCH1ZuZXVtfSO/Wdja3tndK+4ftFScSkI9EvNYdgKsKGeCepppTjuJpDgKOG0H48uZ376nUrFY3OosoX6Eh4KFjGBtJO/sKrsb94slZKNaBbkIIttB9TKqGVJ3z8tuFTo2mqMElmj2i++9QUzSiApNOFaq66BE+xMsNSOcTgu9VNEEkzEe0q6hAkdU+ZP5sVN4YpQBDGNpSmg4V79PTHCkVBYFpjPCeqR+ezPxL6+b6rDmT5hIUk0FWSwKUw51DGefwwGTlGieGYKJZOZWSEZYYqJNPgUTwten8H/SKttOxa7cuKXGxTKOPDgCx+AUOKAKGuAaNIEHCGDgATyBZ0tYj9aL9bpozVnLmUPwA9bbJ8Fzjq8=</latexit> Fyk
x<latexit sha1_base64="HjKWIzJR5ffh2H33LNfdixEluAI=">AAAB6nicbVDLTgJBEOzFF+IL9ehlIjHxRHaNQY9ELx4xyiOBlcwODUyYnd3MzBrJhk/w4kFjvPpF3vwbB9iDgpV0UqnqTndXEAuujet+O7mV1bX1jfxmYWt7Z3evuH/Q0FGiGNZZJCLVCqhGwSXWDTcCW7FCGgYCm8Hoeuo3H1FpHsl7M47RD+lA8j5n1Fjp7ulh1C2W3LI7A1kmXkZKkKHWLX51ehFLQpSGCap123Nj46dUGc4ETgqdRGNM2YgOsG2ppCFqP52dOiEnVumRfqRsSUNm6u+JlIZaj8PAdobUDPWiNxX/89qJ6V/6KZdxYlCy+aJ+IoiJyPRv0uMKmRFjSyhT3N5K2JAqyoxNp2BD8BZfXiaNs7JXKVduz0vVqyyOPBzBMZyCBxdQhRuoQR0YDOAZXuHNEc6L8+58zFtzTjZzCH/gfP4AZaWN5A==</latexit> k
x<latexit sha1_base64="HjKWIzJR5ffh2H33LNfdixEluAI=">AAAB6nicbVDLTgJBEOzFF+IL9ehlIjHxRHaNQY9ELx4xyiOBlcwODUyYnd3MzBrJhk/w4kFjvPpF3vwbB9iDgpV0UqnqTndXEAuujet+O7mV1bX1jfxmYWt7Z3evuH/Q0FGiGNZZJCLVCqhGwSXWDTcCW7FCGgYCm8Hoeuo3H1FpHsl7M47RD+lA8j5n1Fjp7ulh1C2W3LI7A1kmXkZKkKHWLX51ehFLQpSGCap123Nj46dUGc4ETgqdRGNM2YgOsG2ppCFqP52dOiEnVumRfqRsSUNm6u+JlIZaj8PAdobUDPWiNxX/89qJ6V/6KZdxYlCy+aJ+IoiJyPRv0uMKmRFjSyhT3N5K2JAqyoxNp2BD8BZfXiaNs7JXKVduz0vVqyyOPBzBMZyCBxdQhRuoQR0YDOAZXuHNEc6L8+58zFtzTjZzCH/gfP4AZaWN5A==</latexit> k x<latexit sha1_base64="hC/HbLKxa/DEvgyBU+SsGODF86M=">AAAB7nicbVBNSwMxEJ3Ur1q/qh69BIsgCGVXpHosevFYwX5Au5Zsmm3DZrNLkhXL0h/hxYMiXv093vw3pu0etPXBwOO9GWbm+Yng2jjONyqsrK6tbxQ3S1vbO7t75f2Dlo5TRVmTxiJWHZ9oJrhkTcONYJ1EMRL5grX98Gbqtx+Z0jyW92acMC8iQ8kDTomxUvvpIQvP3Em/XHGqzgx4mbg5qUCORr/81RvENI2YNFQQrbuukxgvI8pwKtik1Es1SwgNyZB1LZUkYtrLZudO8IlVBjiIlS1p8Ez9PZGRSOtx5NvOiJiRXvSm4n9eNzXBlZdxmaSGSTpfFKQCmxhPf8cDrhg1YmwJoYrbWzEdEUWosQmVbAju4svLpHVedWvV2t1FpX6dx1GEIziGU3DhEupwCw1oAoUQnuEV3lCCXtA7+pi3FlA+cwh/gD5/AAPaj2A=</latexit> k+1 x<latexit sha1_base64="hC/HbLKxa/DEvgyBU+SsGODF86M=">AAAB7nicbVBNSwMxEJ3Ur1q/qh69BIsgCGVXpHosevFYwX5Au5Zsmm3DZrNLkhXL0h/hxYMiXv093vw3pu0etPXBwOO9GWbm+Yng2jjONyqsrK6tbxQ3S1vbO7t75f2Dlo5TRVmTxiJWHZ9oJrhkTcONYJ1EMRL5grX98Gbqtx+Z0jyW92acMC8iQ8kDTomxUvvpIQvP3Em/XHGqzgx4mbg5qUCORr/81RvENI2YNFQQrbuukxgvI8pwKtik1Es1SwgNyZB1LZUkYtrLZudO8IlVBjiIlS1p8Ez9PZGRSOtx5NvOiJiRXvSm4n9eNzXBlZdxmaSGSTpfFKQCmxhPf8cDrhg1YmwJoYrbWzEdEUWosQmVbAju4svLpHVedWvV2t1FpX6dx1GEIziGU3DhEupwCw1oAoUQnuEV3lCCXtA7+pi3FlA+cwh/gD5/AAPaj2A=</latexit> k+1
x<latexit sha1_base64="hC/HbLKxa/DEvgyBU+SsGODF86M=">AAAB7nicbVBNSwMxEJ3Ur1q/qh69BIsgCGVXpHosevFYwX5Au5Zsmm3DZrNLkhXL0h/hxYMiXv093vw3pu0etPXBwOO9GWbm+Yng2jjONyqsrK6tbxQ3S1vbO7t75f2Dlo5TRVmTxiJWHZ9oJrhkTcONYJ1EMRL5grX98Gbqtx+Z0jyW92acMC8iQ8kDTomxUvvpIQvP3Em/XHGqzgx4mbg5qUCORr/81RvENI2YNFQQrbuukxgvI8pwKtik1Es1SwgNyZB1LZUkYtrLZudO8IlVBjiIlS1p8Ez9PZGRSOtx5NvOiJiRXvSm4n9eNzXBlZdxmaSGSTpfFKQCmxhPf8cDrhg1YmwJoYrbWzEdEUWosQmVbAju4svLpHVedWvV2t1FpX6dx1GEIziGU3DhEupwCw1oAoUQnuEV3lCCXtA7+pi3FlA+cwh/gD5/AAPaj2A=</latexit> k+1
E<latexit sha1_base64="NaRwC/LnhoG6g57l5d2cNJ5QDck=">AAAB8nicbVBNSwMxEM3Wr1q/qh69BIvgqeyKVI9FET1WsB+wXUo2zbahyWZJZsWy9Gd48aCIV3+NN/+NabsHbX0w8Hhvhpl5YSK4Adf9dgorq2vrG8XN0tb2zu5eef+gZVSqKWtSJZTuhMQwwWPWBA6CdRLNiAwFa4ej66nffmTacBU/wDhhgSSDmEecErCS3wX2BFpmN7eTXrniVt0Z8DLxclJBORq98le3r2gqWQxUEGN8z00gyIgGTgWblLqpYQmhIzJgvqUxkcwE2ezkCT6xSh9HStuKAc/U3xMZkcaMZWg7JYGhWfSm4n+en0J0GWQ8TlJgMZ0vilKBQeHp/7jPNaMgxpYQqrm9FdMh0YSCTalkQ/AWX14mrbOqV6vW7s8r9as8jiI6QsfoFHnoAtXRHWqgJqJIoWf0it4ccF6cd+dj3lpw8plD9AfO5w+UuZF3</latexit> G P<latexit sha1_base64="Dcg41qAOEfIPVaHNuoNxvpH2ThM=">AAAB+XicbVBNSwMxEM3Wr1q/Vj16CRbBi2VXpHosiuixgv2AdinZNG1Dk+ySzBbL0n/ixYMiXv0n3vw3pu0etPXBwOO9GWbmhbHgBjzv28mtrK6tb+Q3C1vbO7t77v5B3USJpqxGIxHpZkgME1yxGnAQrBlrRmQoWCMc3kz9xohpwyP1COOYBZL0Fe9xSsBKHddtA3sCLdMqMXB2ezfpuEWv5M2Al4mfkSLKUO24X+1uRBPJFFBBjGn5XgxBSjRwKtik0E4Miwkdkj5rWaqIZCZIZ5dP8IlVurgXaVsK8Ez9PZESacxYhrZTEhiYRW8q/ue1EuhdBSlXcQJM0fmiXiIwRHgaA+5yzSiIsSWEam5vxXRANKFgwyrYEPzFl5dJ/bzkl0vlh4ti5TqLI4+O0DE6RT66RBV0j6qohigaoWf0it6c1Hlx3p2PeWvOyWYO0R84nz+OvJOf</latexit> ast-EG G<latexit sha1_base64="LfriCHaTFHTjBQXJx7KjtepQ1OU=">AAAB83icbVBNSwMxEM3Wr1q/qh69BIvgqeyKVI9FkXqsYD+gu5Rsmm1Dk+ySzIpl6d/w4kERr/4Zb/4b03YPWn0w8Hhvhpl5YSK4Adf9cgorq2vrG8XN0tb2zu5eef+gbeJUU9aisYh1NySGCa5YCzgI1k00IzIUrBOOr2d+54Fpw2N1D5OEBZIMFY84JWAl3wf2CFpmjZvGtF+uuFV3DvyXeDmpoBzNfvnTH8Q0lUwBFcSYnucmEGREA6eCTUt+alhC6JgMWc9SRSQzQTa/eYpPrDLAUaxtKcBz9edERqQxExnaTklgZJa9mfif10shugwyrpIUmKKLRVEqMMR4FgAecM0oiIklhGpub8V0RDShYGMq2RC85Zf/kvZZ1atVa3fnlfpVHkcRHaFjdIo8dIHq6BY1UQtRlKAn9IJendR5dt6c90VrwclnDtEvOB/fKayRyA==</latexit> EG u<latexit sha1_base64="D8wXhMXVcLI6gtYlX21kQJA4hFY=">AAACD3icdVDLSgMxFM3UV62vqks3waII0jIztNO6EIpCcVnBPqAvMmnahsk8SDLSMvQP3Pgrblwo4tatO//GTFtBRQ+EnHvOvST32AGjQur6h5ZYWl5ZXUuupzY2t7Z30rt7deGHHJMa9pnPmzYShFGP1CSVjDQDTpBrM9KwncvYb9wSLqjv3chJQDouGnp0QDGSSuqlj8OuA89hZayuU6jnCpVJN3KyxhRmZ9V4XvXSGT13VrLMgqFkXbdKuhUTs5g3TWgoJUYGLFDtpd/bfR+HLvEkZkiIlqEHshMhLilmZJpqh4IECDtoSFqKesglohPN9pnCI6X04cDn6ngSztTvExFyhZi4tup0kRyJ314s/uW1QjkodSLqBaEkHp4/NAgZlD6Mw4F9ygmWbKIIwpyqv0I8QhxhqSJMqRC+NoX/k7qZM6ycdZ3PlC8WcSTBATgEJ8AARVAGV6AKagCDO/AAnsCzdq89ai/a67w1oS1m9sEPaG+f2hKZcQ==</latexit> k=Fxk+0.5Fyk  1  0.5Fxk  1
Fig.1 Anillustrationofthreevariantsof (GEG):Variant1(Left),Variant2(Middle),
andVariant 3withuk:=Fxk+0.5Fyk−1−0.5Fxk−1 forα1=1andα2=0.5(Right).
(i) Discussion of the extragradient method. With uk := Fxk, if β = 1,
thenweobtainexactlytheclassicalEGmethod [46]forsolving(NE).Ifβ <1,
thenwerecovertheextragradient-plus (EG+)schemefrom[29]forsolving
(NE).Ifwecomputexk =yk+ηFxk fromthefirstlineof (GEG)andsubsti-
β
tuteitintothesecondlineof (GEG),thenwegetxk+1 =yk η(Fyk 1Fxk).
− −β
In this case, we obtain a forward-backward-forward splitting variant of
Tseng’s method in [93] from (GEG) as follows:
(cid:40) yk := xk ηFxk,
− β (FBFS)
xk+1 := yk η(Fyk 1Fxk).
− − β
Clearly, if β =1, then we recover exactly Tseng’s method for solving (NE).
(ii) Discussion of the past-extragradient method and its relations to
other methods. With uk :=Fyk 1, we can show that it is equivalent to the
−
following variants. First, we can rewrite (GEG) as
(cid:40)
xk+1 := xk ηFyk
− (PEG)
yk+1 := xk+1 ηFyk.
− β
This form shows us that (PEG) saves one evaluation Fxk of F at each itera-
tion compared to Variant 1. If β = 1, then we obtain exactly the Popov’s
method in [76]. If we rotate up the second line and use β = 1 as yk =
xk ηFyk 1, then we get the past-extragradient method.
−
−12 QuocTran-Dinh,NghiaNguyen-Trung
Now, under this choice of uk, from the first line of (GEG), we have xk =
yk + ηuk = yk + ηFyk 1. Substituting this expression into the first line of
β β −
(PEG), we get xk+1 = yk ηFyk + ηFyk 1. Substituting this relation into
− β −
the second line of (PEG), we can eliminate xk+1 to get the following variant:
yk+1 :=yk η(cid:0) (1+β)Fyk Fyk 1(cid:1) . (FRBS)
− β − −
Thisschemecanbeconsideredasasimplifiedvariantoftheforward-reflected-
backward splitting scheme in [56] for solving (NE) when we set β := 1 as
yk+1 :=yk η(2Fyk Fyk 1).
−
− −
Alternatively,from(PEG),wehavexk 1 xk =ηFyk 1 andβ(xk yk)=
− −
− −
ηFyk 1, leading to xk 1 xk = β(xk yk). Therefore, we get yk = 1((1+
− − − − β
β)xk xk 1). Substituting this expression into the first line of (PEG), we get
−
−
xk+1 :=xk ηF(cid:0)1((1+β)xk xk 1)(cid:1) . (RFB)
− β − −
In particular, if β = 1, then we obtain xk+1 := xk ηF(2xk xk 1), which
−
− −
turns out to be the reflected gradient method in [54] or the reflected
forward-backward splitting scheme in [18] for solving (NE).
Using the relation xk 1 xk = β(xk yk) above, we can compute that
−
− −
xk = 1+β βyk+ 1+1 βxk −1 = (ω ω−1)yk+ ω1xk −1, where ω :=1+β. Combining the
two lines of (GEG), we get yk+1 :=xk+1 ηFyk =xk η(1+β)Fyk. Putting
− β − β
both expressions together, we arrive at
(cid:40) xk := (ω ω−1)yk+ ω1xk −1,
(GR)
yk+1 := xk η(1+β)Fyk.
− β
This method is a simplified variant of the golden ratio method in [55] for
solving (NE). Overall, the template (GEG) covers a class of EG algorithms
with many common instances as discussed above.
(iii)Discussionofthegeneralizationcase.Supposethatuk :=α Fxk+
GEG 1
α Fyk 1+(1 α α )Fxk 1 for fixed α ,α R.
2 − 1 2 − 1 2
− − ∈
Letusdefinevk :=Fyk Fxk,v¯k :=Fyk Fyk 1,andvˆk :=Fyk Fxk 1.
− −
− − −
Then, we can compute Fyk uk 2 = α vk+α v¯k+(1 α α )vˆk 2.
∥ − GEG∥ ∥ 1 2 − 1 − 2 ∥
Now, we compare (GEG) and EG in Variant 1 and PEG in Variant 2.
– For EG, we choose uk := Fxk, and hence, Fyk uk 2 = Fyk
Fxk 2 = vk 2. If therE eG exist α ,α R such th∥ at − EG∥ ∥ −
1 2
∥ ∥ ∥ ∈
vk > α vk+α v¯k+(1 α α )vˆk ,
1 2 1 2
∥ ∥ ∥ − − ∥
then, by Lemma 3.1 below, we can easily show that at each iteration k,
xk+1 x⋆ 2 decreases faster in GEG than in EG.
∥ − ∥
– For PEG, we choose uk :=Fyk 1, and hence, Fyk uk 2 = v¯k 2.
If there exist α ,α
P REG
such
that− ∥ − PEG∥ ∥ ∥
1 2
∈
v¯k > α vk+α v¯k+(1 α α )vˆk ,
1 2 1 2
∥ ∥ ∥ − − ∥
then, again by Lemma 3.1, we can easily show that at each iteration k,
xk+1 x⋆ 2 decreases faster in GEG than in PEG.
∥ − ∥RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 13
Consequently, our (GEG) scheme gives more freedom to improve the perfor-
mance compared to existing EG variants as we will see in Section 7.
3.2 Key Estimates for Convergence Analysis
To analyze the convergence of (GEG), we first prove the following lemmas.
Lemma 3.1 If (xk,yk) is generated by (GEG), then for any γ >0 and any
{ }
xˆ dom(F), we have
∈
xk+1 xˆ 2 xk xˆ 2 β yk xk 2+ η2 Fyk uk 2
∥ − ∥ ≤ ∥ − ∥ − ∥ − ∥ γ ∥ − ∥
2η Fyk,yk xˆ (β γ) xk+1 yk 2 (7)
− ⟨ − ⟩− − ∥ − ∥
(1 β) xk+1 xk 2.
− − ∥ − ∥
Proof First, for any xˆ dom(F), using xk+1 xk = ηFyk from the second
∈ − −
line of (GEG), we have
xk+1 xˆ 2 = xk xˆ 2+2 xk+1 xk,xk+1 xˆ xk+1 xk 2
∥ − ∥ ∥ − ∥ ⟨ − − ⟩−∥ − ∥
= xk xˆ 2 2η Fyk,xk+1 xˆ xk+1 xk 2.
∥ − ∥ − ⟨ − ⟩−∥ − ∥
Next,usingηuk =β(xk yk)fromthefirstlineof (GEG),theCauchy-Schwarz
−
inequality, the identity 2 xk+1 yk,xk yk = xk yk 2+ xk+1 yk 2
⟨ − − ⟩ ∥ − ∥ ∥ − ∥ −
xk+1 xk 2, and an elementary inequality 2 w,z γ w 2 + ∥z ∥2 for any
∥ − ∥ ⟨ ⟩ ≤ ∥ ∥ γ
γ >0 and any vectors w and z, we can derive that
2η Fyk,xk+1 xˆ = 2η Fyk,yk xˆ +2η Fyk uk,xk+1 yk
⟨ − ⟩ ⟨ − ⟩ ⟨ − − ⟩
+ 2η uk,xk+1 yk
⟨ − ⟩
2η Fyk,yk xˆ η2 Fyk uk 2 γ xk+1 yk 2
≥ ⟨ − ⟩− γ ∥ − ∥ − ∥ − ∥
(cid:2) (cid:3)
+ β xk yk 2+ xk+1 yk 2 xk+1 xk 2
∥ − ∥ ∥ − ∥ −∥ − ∥
= 2η Fyk,yk xˆ +β yk xk 2 η2 Fyk uk 2
⟨ − ⟩ ∥ − ∥ − γ ∥ − ∥
+ (β γ) xk+1 yk 2 β xk+1 xk 2.
− ∥ − ∥ − ∥ − ∥
Finally, combining the last two expressions, we obtain (7). □
Lemma 3.2 Suppose that zer(F)= , F is L-Lipschitz continuous, and there
̸ ∅
exist x⋆ zer(F) and ρ 0 such that Fx,x x⋆ ρ Fx 2 for all x
∈ ≥ ⟨ − ⟩ ≥ − ∥ ∥ ∈
dom(F). Let (xk,yk) be generated by (GEG) for a given uk satisfying (6).
{ }
For any γ >0 and r >0, let us introduce the following function:
:= xk x⋆ 2+
κ1(1+r)L2η2
xk yk 1 2+
κ2(1+r)L2η2
xk xk 1 2. (8)
Pk ∥ − ∥ rγ ∥ − − ∥ rγ ∥ − − ∥
Then, for any s>0 and µ [0,1], we have
∈
(cid:16) (cid:17)
β (1+r)L2η2 2µρ(1+s) yk xk 2
Pk+1 ≤ Pk − − γ − sη ∥ − ∥
(cid:16) (cid:17)
β γ κ1(1+r)L2η2 2µρ(1+s) xk+1 yk 2 (9)
− − − rγ − η ∥ − ∥
(cid:0) 1 β κ2(1+r)L2η2 2(1 −µ)ρ(cid:1) xk+1 xk 2.
− − − rγ − η ∥ − ∥14 QuocTran-Dinh,NghiaNguyen-Trung
Proof First, since Fx,x x⋆ ρ Fx 2 for all x dom(F), using this
⟨ − ⟩ ≥ − ∥ ∥ ∈
condition with x := yk, and then utilizing the second line of (GEG) and
Young’s inequality, for any s>0 and µ [0,1], we can derive that
∈
Fyk,yk x⋆ ρ Fyk 2 (G =EG) ρ xk+1 xk 2
⟨ − ⟩ ≥ − ∥ ∥ −η2∥ − ∥
µρ(1+s) xk+1 yk 2 µρ(1+s) yk xk 2 (10)
≥ − η2 ∥ − ∥ − sη2 ∥ − ∥
−
(1
−
η2µ)ρ ∥xk+1 −xk ∥2.
Next,foranyr >0,itfollowsfromYoung’sinequality,(6),andtheL-Lipschitz
continuity of F that
Fyk uk 2 (1+r) Fxk Fyk 2+ (1+r) Fxk uk 2
∥ − ∥ ≤ ∥ − ∥ r ∥ − ∥
(6)
(1+r) Fxk Fyk 2+ κ1(1+r) Fxk Fyk 1 2
≤ ∥ − ∥ r ∥ − − ∥
+ κ2(1+r) Fxk Fxk 1 2 (11)
r ∥ − − ∥
(1+r)L2 xk yk 2+
κ1(1+r)L2
xk yk 1 2
≤ ∥ − ∥ r ∥ − − ∥
+
κ2(1+r)L2
xk xk 1 2.
r ∥ − − ∥
Now,substitutingx⋆ zer(F)forxˆinto(7),andusing (10)and(11),wehave
∈
xk+1 x⋆ 2 xk x⋆ 2+
κ1(1+r)L2η2
xk yk 1 2+
κ2(1+r)L2η2
xk xk 1 2
∥ − ∥ ≤ ∥ − ∥ rγ ∥ − − ∥ rγ ∥ − − ∥
(cid:0)
β
(1+r)L2η2 2µρ(1+s)(cid:1)
yk xk 2
− − γ − sη ∥ − ∥
(cid:0)
β γ
2µρ(1+s)(cid:1)
xk+1 yk 2
− − − η ∥ − ∥
(cid:0) 1 β 2(1 −µ)ρ(cid:1) xk+1 xk 2.
− − − η ∥ − ∥
Finally, by rearranging this inequality, and then using from (8), we get
k
P
(cid:16) (cid:17)
β (1+r)L2η2 2µρ(1+s) yk xk 2
Pk+1 ≤ Pk − − γ − sη ∥ − ∥
(cid:16) (cid:17)
β γ κ1(1+r)L2η2 2µρ(1+s) xk+1 yk 2
− − − rγ − η ∥ − ∥
(cid:0) 1 β κ2(1+r)L2η2 2(1 −µ)ρ(cid:1) xk+1 xk 2.
− − − rγ − η ∥ − ∥
This expression is exactly (9). □
Lemma 3.3 Let F be ρ-co-hypomonotone, i.e. there exists ρ 0 such that
≥
Fx Fy,x y ρ Fx Fy 2 for all x,y dom(F) and F be L-Lipschitz
⟨ − − ⟩≥− ∥ − ∥ ∈
continuous.Let (xk,yk) begeneratedby (GEG).Then,foranys>0,ω 0,
{ } ≥
and ωˆ 0, the following estimate holds:
≥
Fxk+1 2+ω Fxk+1 Fyk 2+ωˆ Fxk+1 Fxk 2 Fxk 2
∥ ∥ ∥ − ∥ ∥ − ∥ ≤∥ ∥
(cid:2)
1
1+s(cid:0)
ωˆ +
2ρ(cid:1)(cid:3)
Fyk Fxk 2 (12)
− − s η ∥ − ∥
+
(cid:2) 1+ω+(1+s)(cid:0)
ωˆ +
2ρ(cid:1)(cid:3)L2η2
βFyk uk 2.
η β2 ∥ − ∥RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 15
Proof Since F is ρ-co-hypomonotone, we have Fxk+1 Fxk,xk+1 xk +
⟨ − − ⟩
ρ Fxk+1 Fxk 2 0. Substituting xk+1 xk = ηFyk from the second line
∥ − ∥ ≥ − −
of (GEG) into this inequality and expanding it, we can show that
0 2 Fxk,Fyk 2 Fxk+1,Fyk + 2ρ Fxk+1 Fxk 2
≤ ⟨ ⟩− ⟨ ⟩ η ∥ − ∥
= Fxk 2 Fyk Fxk 2 Fxk+1 2+ Fxk+1 Fyk 2
∥ ∥ −∥ − ∥ −∥ ∥ ∥ − ∥
+ 2ρ Fxk+1 Fxk 2.
η ∥ − ∥
For any ω 0, ωˆ 0, and s>0, by Young’s inequality, this estimate leads to
≥ ≥
:= Fxk+1 2+ω Fxk+1 Fyk 2+ωˆ Fxk+1 Fxk 2
[k+1]
T ∥ ∥ ∥ − ∥ ∥ − ∥
Fxk 2 Fyk Fxk 2+(1+ω) Fxk+1 Fyk 2
≤ ∥ ∥ −∥ − ∥ ∥ − ∥
+
(cid:0)
ωˆ +
2ρ(cid:1)
Fxk+1 Fxk 2 (13)
η ∥ − ∥
Fxk
2+(cid:2) 1+ω+(1+s)(cid:0)
ωˆ +
2ρ(cid:1)(cid:3)
Fxk+1 Fyk 2
≤ ∥ ∥ η ∥ − ∥
(cid:2)
1
1+s(cid:0)
ωˆ +
2ρ(cid:1)(cid:3)
Fyk Fxk 2.
− − s η ∥ − ∥
Now,bytheL-LipschitzcontinuityofF andxk+1 yk = η(Fyk 1uk)from
− − −β
(GEG), we obtain Fxk+1 Fyk 2 L2 xk+1 yk 2 = L2η2 βFyk uk 2.
∥ − ∥ ≤ ∥ − ∥ β2 ∥ − ∥
Substituting this relation into (13), we arrive at (12). □
3.3 The Sublinear Best-Iterate Convergences Rate of (GEG)
For given constants κ and κ in (6), we denote r :=
κ1+√κ2 1+4κ1
0. First,
1 2 2 ≥
for any β (0,1], we define the following quantity:
∈
∆ := 16(11
+r)
·min(cid:110)(cid:0) 1+ 2κ r2(cid:1) β2, (1 −β)2 κr(
2
2r+2κ2)(cid:111) . (14)
For r given above, let α:= 1+r and µ:= r . We also define
r r+2κ2
 β √β2 16(1+r)µLρ β+√β2 16(1+r)µLρ
η 1:= − 2(−
1+r)L
, η¯ 1:= 2(−
1+r)L
,
η 2:=
1 −β −√(1 −β 2α)2
κ−
2L8α(1 −µ)κ2Lρ
, η¯ 2:=
1 −β+√(1 −β 2α)2
κ−
2L8α(1 −µ)κ2Lρ
,
(15)
η :=max η ,η , η¯ :=min η¯ ,η¯ .
{ 1 2} { 1 2 }
Note that we can only choose β = 1 if κ = 0 in (6). In this case, we have
2
µ = 1, η = η , and η¯= η¯ . Lemma A.1 in the appendix below shows that if
1 1
Lρ ∆, then 0 η η¯, i.e. [η,η¯]= .
≤ ≤ ≤ ̸ ∅
Now, given η [η,η¯], we consider the following quantities:
∈
C 1 :=β −(1+r)Lη
−
4µ ηρ and C 2 :=1 −β −ακ 2Lη
−
2(1 − ηµ)ρ. (16)
Then, we are ready to state the best-iterate convergence rate of (GEG) for
any uk satisfying the condition (6).16 QuocTran-Dinh,NghiaNguyen-Trung
Theorem 3.1 (Best-iterate convergence rate) For Equation (NE), sup-
pose that zer(F)= , F is L-Lipschitz continuous, and there exist x⋆ zer(F)
̸ ∅ ∈
and ρ 0 such that Fx,x x⋆ ρ Fx 2 for all x dom(F).
≥ ⟨ − ⟩≥− ∥ ∥ ∈
Let ∆ be defined by (14), and η and η¯ be defined by (15). Let (xk,yk)
{ }
be generated by (GEG) starting from x0 dom(F) and y 1 =x 1 :=x0 such
− −
∈
that uk satisfies (6). If Lρ ∆ and η is chosen such that η [η,η¯], then C
1
≤ ∈
and C in (16) are nonnegative.
2
Moreover, for any K 0, we have
≥
K
(cid:88)(cid:2) C yk xk 2+C xk+1 yk 2+C xk+1 xk 2(cid:3) x0 x⋆ 2. (17)
1 1 2
∥ − ∥ ∥ − ∥ ∥ − ∥ ≤∥ − ∥
k=0
If C >0, then for Λ:= C1+2C2 and Γ := 2L2 + 2 , we also have
1 2 C1 Λη2
 
0
00
≤≤
m
mm
k
kk
i
ii
≤≤
n
nn
K
KK
∥
∥∥
F
Fuk
y
x∥
k
k2
∥
∥≤
2 2≤
≤K1 +
K
K1
1 1+
+(cid:80)
1 1(cid:80)
(cid:80)K k=
K k
K
k0
=
=∥
0
0u
∥
∥k
F
F∥
y
x2
k
k≤
∥ ∥2
2β C
≤
≤2 1∥ ηx Λ2 Γ∥0 (
x
η− ∥K
0 2
xx
K−
(+ 0K⋆
−x
+1∥ +⋆) x2
1∥
⋆1,
2 ) ∥2,
.
(18)
≤ ≤
Consequently, the following results hold:
min Fxk = (cid:0) 1 (cid:1) , min Fyk = (cid:0) 1 (cid:1) ,
0 k K∥ ∥ O √K 0 k K∥ ∥ O √K
≤ ≤ ≤ ≤ (19)
lim xk yk = lim Fxk = lim Fyk =0.
k ∥ − ∥ k ∥ ∥ k ∥ ∥
→∞ →∞ →∞
In addition, xk x⋆ is bounded and xk converges to x⋆ zer(F).
{∥ − ∥} { } ∈
Proof First, we choose γ := Lη in Lemma 3.2. Next, substituting α := 1+r
r
and s:=1 into the following quantities with r :=
κ1+√κ2 1+4κ1,
we get
2
 C
1
:= β
−
(1+r γ)L2η2
−
2µρ s(1 η+s) = β −(1+r)Lη
−
4µ ηρ,
Cˆ := β γ κ1(1+r)L2η2 2µρ(1+s) = β (1+r)Lη 4µρ,
C1
2 := 1
−−
β
−−
κ2(1+
rr
r
γγ
)L2η2
−−
2(1 −
ηη
µ)ρ = 1
−β−
−ακ 2Lη
−− 2(1η
− ηµ)ρ.
This is exactly the constants defined in (16). By Lemma A.1, for β (0,1],
we have η η¯. In addition, for any η [η,η¯], C =Cˆ 0 and C ∈ 0.
1 1 2
≤ ∈ ≥ ≥
Now,forgivenη andC andC definedby(16),(9)isrewrittenasfollows:
1 2
C yk xk 2+C xk+1 yk 2+C xk+1 xk 2 . (20)
1 1 2 k k+1
∥ − ∥ ∥ − ∥ ∥ − ∥ ≤P −P
Summing up (20) from k =0 to K, and noting that 0, we obtain
k
P ≥
(cid:80)K (cid:2) C xk+1 yk 2+C yk xk 2+C xk+1 xk 2(cid:3) .
k=0 1 ∥ − ∥ 1 ∥ − ∥ 2 ∥ − ∥ ≤P0RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 17
Since we choose y 1 = x 1 = x0, it follows from (8) that = x0 x⋆ 2.
− − 0
P ∥ − ∥
Substituting this fact into the last inequality, we obtain (17).
Next, from the first line of (GEG), we have η2 uk 2 = β2 yk xk 2.
∥ ∥ ∥ − ∥
Combining this relation and (17) we get the first line of (18).
Then, from the second line of (GEG), by Young’s inequality, we have
η2 Fyk 2 = xk+1 xk 2
∥ ∥ ∥ − ∥
≤
a 1(1+a−21) ∥yk −xk ∥2+a 1(1+a 2) ∥xk+1 −yk ∥2
+ (1 a ) xk+1 xk 2,
1
− ∥ − ∥
for any a [0,1] and a > 0. If C > 0, then by setting Λ := C1+2C2,
1 ∈ 2 1 2
a :=1 C2, and a :=1, the last inequality is equivalent to
1 − Λ 2
Λη2 Fyk 2 C yk xk 2+C xk+1 yk 2+C xk+1 xk 2.
1 1 2
∥ ∥ ≤ ∥ − ∥ ∥ − ∥ ∥ − ∥
Combining this inequality and (17) we get the second line of (18).
Again,byYoung’sinequalityandthe L-Lipschitzcontinuityof F,we have
η2 Fxk 2 2η2 Fyk 2+2η2 Fxk Fyk 2 2η2 Fyk 2+2L2η2 xk yk 2.
∥ ∥ ≤ ∥ ∥ ∥ − ∥ ≤ ∥ ∥ ∥ − ∥
Combining this inequality, (cid:80)K 2L2η2 yk xk 2 2L2η2 x0 x⋆ 2, and
k=0 ∥ − ∥ ≤ C1 ∥ − ∥
(cid:80)K 2η2 Fyk 2 2 x0 x⋆ 2, we obtain the third line of (18), where
k=0 ∥ ∥ ≤ Λ∥ − ∥
Γ := 2L2 + 2 .
C1 Λη2
The bounds and limits in (19) are direct consequences of (17) and (18).
From (20), we can see that is non-increasing, and xk x⋆ 2
k k
{P } ∥ − ∥ ≤ P ≤
= x0 x⋆ 2,showingthat xk x⋆ isbounded.Finally,theconvergence
0
P ∥ − ∥ {∥ − ∥}
of xk to x⋆ zer(F) can be proven using standard arguments as in, e.g.,
[54{ ] by} noticing∈ that we are working on a finite-dimensional space Rp. □
Remark 3.1 Ifρ=0inTheorem3.1,i.e.F isstar-monotone(andinparticular,
monotone) and κ = 0, then the choice η [η,η¯] reduces to 0 < η < β .
2 ∈ (1+r)L
For EG with uk :=Fxk, we have κ =0, and hence 0<η < β. For past-EG
1 L
with uk :=Fyk 1, we have κ =1, and hence 0<η < 2β . These choices
− 1 (3+√5)L
are standard in the literature [33,76].
3.4 The Sublinear Last-Iterate Convergence Rate of (GEG)
For simplicity of our presentation, let us first define the following quantities:
m:=[(1+√2)(κ +√2)]1/2, ∆ˆ:= 1 ,
1 16m
(21)
ηˆ:= 1 −√1 −16mLρ, and η¯ˆ:= 1+√1 −16mLρ.
2mL 2mL
Then,forη andη¯definedby(15),byLemmaA.2intheappendix,undergiven
conditions on the parameters specified below, we have [η,η¯] [ηˆ,η¯ˆ]= .
∩ ̸ ∅
Now, we can state the last-iterate convergence rate of (GEG) as follows.18 QuocTran-Dinh,NghiaNguyen-Trung
Theorem 3.2 (Last-iterate convergence rate) For Equation (NE), sup-
posethatzer(F)= ,andF isL-Lipschitzcontinuousandρ-co-hypomonotone.
Let ∆, η and̸ η¯∅ be defined by (14) and (15), respectively, and ∆ˆ, ηˆ, and η¯ˆ
be defined by (21). Let (xk,yk) be generated by (GEG) starting from x0
{ } ∈
dom(F) with y 1 = x 1 := x0 and β := 1 such that uk satisfies (6) with
− −
κ :=0. Assume that Lρ ∆ˆ and η is chosen such that η [η,η¯] [ηˆ,η¯ˆ]= .
2
≤ ∈ ∩ ̸ ∅
Then, for ω :=
2(1+√2)κ1L2η2
0, we have
1 −(1+√2)κ1L2η2 ≥
Fxk+1 2+ω Fxk+1 Fyk 2 Fxk 2+ω Fxk Fyk 1 2,
−
∥ ∥ ∥ − ∥ ≤∥ ∥ ∥ − ∥
(cid:16) (cid:17) (22)
∥FxK ∥2 ≤∥FxK ∥2+ω ∥FxK −FyK −1 ∥2
≤
ω CL 12 +Γ ∥x0 K− +x 1⋆ ∥2 .
(cid:0) (cid:1)
Thus we conclude that FxK = 1/√K on the last-iterate xK.
∥ ∥ O
Proof First, since β = 1 and κ = 0, for any b > 0, it follows from Young’s
2
inequality and (6) that
Fyk uk 2 (1+b) Fxk Fyk 2+ (1+b) Fxk uk 2
∥ − ∥ ≤ ∥ − ∥ b ∥ − ∥ (23)
(1+b) Fxk Fyk 2+ (1+b)κ1 Fxk Fyk 1 2.
≤ ∥ − ∥ b ∥ − − ∥
We can choose ωˆ =0 and substitute β =1 into (12) to get
Fxk+1 2+ω Fxk+1 Fyk 2 Fxk 2
(cid:2)
1
2(1+s)ρ(cid:3)
Fyk Fxk 2
∥ ∥ ∥ − ∥ ≤∥ ∥ − − sη ∥ − ∥
+
L2η2(cid:2)
1+ω+
2(1+s)ρ(cid:3)
Fyk uk 2.
η ∥ − ∥
Let us denote A := 1 2(1+s)ρ and A := (cid:2) 1 + ω + 2(1+s)ρ(cid:3) L2η2. Then,
1 − sη 2 η
utilizing (23), A , and A into the last inequality, we can show that
1 2
Fxk+1 2+ω Fxk+1 Fyk 2 Fxk 2+ κ1(1+b)A2 Fxk Fyk 1 2
∥ ∥ ∥ − ∥ ≤∥ ∥ b ∥ − − ∥ (24)
[A (1+b)A ] Fyk Fxk 2.
1 2
− − ∥ − ∥
Suppose that there exists m > 0 such that 1 2(1+s)ρ mLη 0. This
− sη − ≥
implies that 2(1+s)ρ s and Lη 1. Hence, we can upper bound A as
η ≤ ≤ m 2
A A¯ :=(1+s+ω)L2η2.Ifweimposeω = κ1(1+b)A¯ 2,thenusingA A¯ ,
2 ≤ 2 b 2 ≤ 2
we have κ1(1+b)A2 ω. Clearly, ω := κ1(1+b)(1+s)L2η2 satisfies ω = κ1(1+b)A¯ 2,
provided
thatb
κ
(1≤
+b)L2η2 <b.
b −κ1(1+b)L2η2 b
1
Forthechoiceofωabove,weassumethat(1+b)A¯ mLη.UsingLη 1,
(cid:113) 2 ≤ ≤ m
wecaneasilyshowthatifwechoosem:= (1+b)[κ1+b(1+s)],then(1+b)A¯
b 2 ≤
mLη. As a consequence, we have
A (1+b)A 1 2(1+s)ρ (1+b)A¯ 1 2(1+s)ρ mLη 0.
1 − 2 ≥ − sη − 2 ≥ − sη − ≥
Utilizing A (1+b)A 0 and κ1(1+b)A2 ω proven above, (24) reduces to
1 − 2 ≥ b ≤
Fxk+1 2+ω Fxk+1 Fyk 2 Fxk 2+ω Fxk Fyk 1 2,
−
∥ ∥ ∥ − ∥ ≤∥ ∥ ∥ − ∥RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 19
which proves the first line of (22).
Ontheonehand,ifηˆ:= 1 −√1 −8(1+s)mLρ/s η η¯ˆ:= 1+√1 −8(1+s)mLρ/s ,
2mL ≤ ≤ 2mL
then the condition 1 2(1+s)ρ mLη 0 holds, provided that Lρ s .
− sη − ≥ ≤ 8(1+s)m
On the other hand, for β = 1 and κ = 0, ∆ defined by (14) becomes ∆ =
2
1 . Moreover, η and η¯defined by (15) reduce to η = 1 −√1 −16(1+r)Lρ and
16(1+r) 2(1+r)L
1 √1 16(1+r)Lρ
η¯= − − , respectively.
2(1+r)L
Ifwechooses:=1andb:= 1 ,thenm=[(1+√2)(κ +√2)]1/2 andLρ
√2 1 ≤
s = 1 =:∆ˆ.Inaddition,wecanverifythatω := 2(1+√2)κ1L2η2 ,ηˆ:=
8(1+s)m 16m 1 −(1+√2)κ1L2η2
1 −√1 −16mLρ, and η¯ˆ := 1+√1 −16mLρ. Applying Lemma A.2 in the appendix
2mL 2mL
below, we can easily show that [η,η¯] [ηˆ,η¯ˆ]= .
∩ ̸ ∅
Finally, from (17), the third line of (18), and the L-Lipschitz continuity of
F, it is straightforward to get
Kω +1(cid:80)K k=0∥Fxk −Fyk −1 ∥2
≤
KωL +2 1(cid:80)K k=0∥xk −yk −1 ∥2
≤
ωL C2 1∥ (x K0 − +x 1⋆ )∥2 ,
K1 +1(cid:80)K k=0∥Fxk ∥2
≤
Γ ∥x K0 − +x 1⋆ ∥2 .
Adding both inequalities and using the first line of (22), we can show that
FxK 2 FxK 2+ω FxK FyK 1 2
−
∥ ∥ ≤∥ ∥ ∥ − ∥
1 (cid:80)K [ Fxk 2+ω Fxk Fyk 1 2]
≤ K+1 k=0 ∥ ∥ ∥ − − ∥
(cid:0)ωL2 +Γ(cid:1) ∥x0 −x⋆ ∥2
,
≤ C1 K+1
which proves the second line of (22). □
3.5 The Sublinear Convergences Rates of Two Instances
Now, we specify (GEG) for two instances: Variant 1 – the EG method, and
Variant 2 – the past-EG method. Then, we apply Theorem 3.1 and Theo-
rem 3.2 to derive the following corollaries.
Corollary 3.1 (TheEGmethod)ForEquation (NE),supposethatzer(F)=
̸
, F is L-Lipschitz continuous, and there exist x⋆ zer(F) and ρ 0 such
∅ ∈ ≥
that Fx,x x⋆ ρ Fx 2 for all x dom(F) (the last condition holds if,
⟨ − ⟩≥− ∥ ∥ ∈
in particular, F is ρ-co-hypomonotone).
Let (xk,yk) be generated by (GEG) using uk :=Fxk (Variant 1). Sup-
{ }
pose that Lρ
β2
and η is chosen as
≤ 16
0 β −√β2 −16Lρ <η < β+√β2 −16Lρ β. (25)
≤ 2L 2L ≤ L
Then, with C :=β Lη 4ρ >0, we have
1 − − η
1 (cid:88)K β2 x0 x⋆ 2
min Fxk 2 Fxk 2 ∥ − ∥ . (26)
0 k K∥ ∥ ≤ K+1 ∥ ∥ ≤ C 1η2(K+1)
≤ ≤ k=020 QuocTran-Dinh,NghiaNguyen-Trung
(cid:0) (cid:1) (cid:0) (cid:1)
Thus we conclude that min Fxk = 1/√K showing a 1/√K
0 k K
best-iterate convergence rate≤of≤ xk∥ . ∥ O O
{ }
In addition, xk x⋆ is nonincreasing and xk converges to x⋆
{∥ − ∥} { } ∈
zer(F). Moreover, we have
lim xk yk =lim Fxk =lim Fyk =0. (27)
k k k
→∞∥ − ∥ →∞∥ ∥ →∞∥ ∥
In particular, if F is ρ-co-hypomonotone such that Lρ 1 0.044194,
≤ 16√2 ≈
then for β :=1 and η is chosen such that
1 −√1 −16√2Lρ η 1+√1 −16√2Lρ, (28)
2√2L ≤ ≤ 2√2L
we have
x0 x⋆ 2
Fxk+1 2 Fxk 2 ψ Fxk Fyk 2 and FxK 2 ∥ − ∥ , (29)
∥ ∥ ≤∥ ∥ − ∥ − ∥ ∥ ∥ ≤ C η2(K+1)
1
where ψ := 1 4ρ (cid:0) 1+ 4ρ(cid:1) L2η2 0. The last bound shows that FxK =
− η − η ≥ ∥ ∥
(cid:0) (cid:1)
1/√K on the last-iterate xK.
O
Proof Since we choose uk :=Fxk in (GEG), (6) holds with κ =κ =0. The
1 2
conditionLρ ∆ofTheorem3.1for∆definedby (14)reducestoLρ
β2
as
≤ ≤ 16
westated.Theconditionη [η,η¯]inTheorem3.1becomes(25).Consequently,
∈
(25) holds strictly and we can derive (26) directly from the first line of (18).
ThenexttwostatementsofCorollary3.1arealsoconsequencesofTheorem3.1.
To obtain the last-iterate convergence rates, we set β =1 and κ =κ =0
1 2
in Theorem 3.2. Hence, the condition Lρ ∆ˆ in Theorem 3.2 reduces to
Lρ 1 as in Corollary 3.1. The choice of≤ η [η,η¯] [ηˆ,η¯ˆ] in Theorem 3.2
≤ 16√2 ∈ ∩
becomes (28). As a consequence, (29) is obtained from (22) of Theorem 3.2.
Here, the term ψ Fxk Fyk 2 comes from (24) since ψ A (1+b)A . □
1 2
∥ − ∥ ≤ −
Corollary 3.2 (The past-EG method) For Equation (NE), suppose that
zer(F)= ,F isL-Lipschitzcontinuous,andthereexistx⋆ zer(F)andρ 0
̸ ∅ ∈ ≥
suchthat Fx,x x⋆ ρ Fx 2 forallx dom(F)(thelastconditionholds
⟨ − ⟩≥− ∥ ∥ ∈
if F is ρ-co-hypomonotone).
Let (xk,yk) be generated by (GEG) using uk := Fyk 1 (Variant 2).
−
{ }
For any β (0,1], assume that Lρ
β2
and η is chosen as
∈ ≤ 8(3+√5)
0 β −√β2 −8(3+√5)Lρ <η < β+√β2 −8(3+√5)Lρ 2β . (30)
≤ (3+√5)L (3+√5)L ≤ (3+√5)L
Then, we have
0m kin K∥Fyk ∥2
≤
K1 +1(cid:80)K k=0∥Fyk ∥2
≤
Λ∥x η0 2− (Kx +⋆ ∥ 12 ),
≤ ≤ (31)
0m kin K∥Fxk ∥2
≤
K1 +1(cid:80)K k=0∥Fxk ∥2
≤
Γ ∥x K0 − +x 1⋆ ∥2 ,
≤ ≤RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 21
where Λ:= C1+2C2 and Γ := 2 +2L2 >0 with C :=1 (3+√5)Lη 4ρ >0
2 Λη2 C1 1 − 2 −
(cid:0)
η
(cid:1)
and C = 1 β 0. Consequently, we have min Fxk = 1/√K
2 0 k K
−(cid:0) ≥ (cid:1) ≤ ≤ ∥ ∥ O
showing the 1/√K best-iterate convergence rate of xk . Moreover, xk
O { } { }
converges to x⋆ zer(F), and (27) still holds.
∈
In particular, if F is ρ-co-hypomonotone such that Lρ 1
≤ 16(1+√2) ≈
0.025888, then for β :=1 and η is chosen as in (30), we have
Fxk+1 2+ω Fxk+1 Fyk 2 Fxk 2+ω Fxk Fyk 1 2,
−
∥ ∥ ∥ − ∥ ≤ ∥ ∥ ∥ − ∥ (32)
FxK 2 Cˆ ∥x0 −x⋆ ∥2 ,
∥ ∥ ≤ K+1
where ω := 2(1+√2)L2η2 > 0 and Cˆ := Γ + ωL2. Hence, we conclude that
1 (1+√2)L2η2 C1
(cid:0)− (cid:1)
FxK = 1/√K on the last-iterate xK.
∥ ∥ O
Proof Since we choose uk := Fyk 1 in (GEG), (6) holds with κ = 1 and
− 1
κ =0. In this case, r := κ1+√κ2 1+4κ1 = 1+√5 in Theorem 3.1. The condition
2 2 2
Lρ ∆ of Theorem 3.1 for ∆ defined by (14) reduces to Lρ
β2
as we
≤ ≤ 8(3+√5)
stated. The condition η [η,η¯] in Theorem 3.1 becomes (30). Consequently,
∈
wecanderive(31)directlyfromthesecondandthirdlinesof (18),respectively.
ThenexttwostatementsofCorollary3.2arealsoconsequencesofTheorem3.1.
To obtain the last-iterate convergence rates, we set β = 1, κ = 1, and
1
κ =0 in Theorem 3.2. Hence, the condition Lρ ∆ˆ in Theorem 3.2 reduces
2
to Lρ 1 as in Corollary 3.2. The cho≤ ice of η [η,η¯] [ηˆ,η¯ˆ] in
≤ 16(1+√2) ∈ ∩
Theorem 3.2 becomes (30). As a consequence, (32) is obtained from (22). □
Remark 3.2 If ρ=0, i.e., there exists x⋆ zer(F) such that Fx,x x⋆ 0
forallx dom(F),thentheconditionη
∈
[η,η¯]
[ηˆ,η¯ˆ]inThe⟨ orem3.− 2red⟩ u≥
ces
∈ ∈ ∩
to 0 < η 1 for σ := [(1+√2)(κ +√2)]1/2. In particular, for EG with
≤ σL 1
uk := Fxk, we have κ = 0, leading to 0 < η 1 . Similarly, for past-
1 ≤ √2+√2L
EG with uk := Fyk 1, we have κ = 1, leading to 0 < η 1 . Hence,
− 1 ≤ (1+√2)L
the ranges of stepsize η for the last-iterate convergence rates are smaller than
the ones for the best-iterate convergence rates.
Remark 3.3 ThetwoinstancesinCorollary3.1andCorollary3.2wereproven
in our unpublished report [52], and then were further revised in [39]. The
last-iterate convergence rates for these two instances were proven in previous
works such as [36] for the monotone case but with an additional assumption.
Note that the best-iterate rates for the monotone or the star-monotone case
are classical, which can be found, e.g., in [33,46]. The last-iterate convergence
for the monotone case can be found in recent works such as [36,38]. The best-
iterate rates for the co-hypomonotone or the star-co-hypomonotone case can
be found in [29], while the last-iterate convergence rates were proven in [52].
Nevertheless, in this subsection, we derive Corollary 3.1 and Corollary 3.2 as
special cases of Theorem 3.1 and Theorem 3.2.22 QuocTran-Dinh,NghiaNguyen-Trung
4 A Class of Extragradient Methods for Monotone Inclusions
Inthissection,wegobeyond(NE)togeneralizeEGforaclassof“monotone”
(NI).Ourbest-iterateconvergencerateanalysisgeneralizesclassicalresultsto
abroaderclassofmethodsandallowsT tobemaximally3-cyclicallymonotone
insteadofanormalcone.Ourlast-iterateconvergencerateanalysisisnewand
elementary, compared to, e.g., [39]. We also specify our results to cover two
classical instances similar to our results in Section 3.
4.1 A Class of Extragradient Methods for Solving (NI)
We propose the following generalized EG scheme (called GEG2) for solving
(NI). Starting from x0 dom(Φ), at each iteration k 0, we update
∈ ≥
(cid:40) yk := J βηT(xk
−
βηuk),
(GEG2)
xk+1 := J (xk ηFyk),
ηT
−
where J is the resolvent of ηT, η > 0 is a given stepsize, β > 0 is a scaling
ηT
factor, and uk Rp satisfies the following condition:
∈
Fxk uk 2 κ Fxk Fyk 1 2+κ Fxk Fxk 1 2, (33)
1 − 2 −
∥ − ∥ ≤ ∥ − ∥ ∥ − ∥
for given parameters κ 0 and κ 0 and x 1 = y 1 := x0. Note that (6)
1 2 − −
≥ ≥
alsomakes(GEG2)differentfromthehybridextragradientmethodin[63,84].
Similar to (GEG), we have at least three concrete choices of uk as follows.
(a) Variant 1. If uk := Fxk (i.e. κ = κ = 0 in (33)), then we obtain
1 2
yk := J βηT(xk
−
βηFxk). Clearly, if β = 1, then we get the well-known
extragradientmethod.Ifβ (0,1),thenweobtaintheextragradient-
∈
plus – EG+ in [29] but to solve (NI).
(b) Variant 2. If uk := Fyk 1 (i.e. κ = 1 and κ = 0), then we obtain
− 1 2
yk := J βηT(xk
−
βηFyk −1), leading to the past-extragradient method
(or equivalently, Popov’s method [76]) for solving (NI).
(c) Variant3.Wecanconstructuk :=α Fxk+α Fyk 1+(1 α α )Fxk 1
1 2 − 1 2 −
− −
as an affine combination of Fxk, Fyk 1 and Fxk 1 for given constants
− −
α ,α R. Then, uk satisfies (33) with κ = (1+c)(1 α )2 and κ =
1 2 1 1 2
∈ −
(1+c 1)(1 α α )2 by Young’s inequality for any c>0.
− 1 2
− −
Clearly,whenT = ,thenormalconeofanonempty,closed,andconvexset
,thenJ
=projNX
,theprojectiononto andhence,(GEG2)withVariant
γT
1X reduces to the extXragradient variant forX solving (VIP) widely studied in the
literature [33,45]. In terms of computational complexity, (GEG2) with Vari-
ants 1 and 3 requires two evaluations of F at xk and yk, and two evaluations
of the resolvent J at each iteration. It costs as twice as one iteration of the
ηT
forward-backward splitting method (FBS). However, its does not require the
co-coerciveness of F to guarantee convergence. Again, we use a scaling factor
β as in (GEG), which covers EG+ in [29] as a special case.RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 23
Now, for given ζk Tyk and ξk+1 Txk+1, we denote w˜k := Fxk +ζk,
∈ ∈
and wˆk+1 :=Fyk+ξk+1. Then, we can rewrite (GEG2) equivalently to
(cid:40) yk := xk η(uk+ζk) = xk η(w˜k+uk Fxk),
− β − β − (34)
xk+1 := xk η(Fyk+ξk+1) = xk ηwˆk+1,
− −
where ζk Tyk and ξk+1 Txk+1. This representation makes (GEG2) look
∈ ∈
like (GEG), and it is a key step for our convergence analysis below.
4.2 Key Estimates for Convergence Analysis
We establish convergence rates of (GEG2) under the assumptions that F is
monotone and L-Lipschitz continuous, and T is maximally 3-cyclically mono-
tone. We first define
wk :=Fxk+ξk for some ξk Txk. (35)
∈
Then,thefollowinglemmasarekeystoestablishingtheconvergenceof(GEG2).
Lemma 4.1 Suppose that T is maximally 3-cyclically monotone and x⋆
∈
zer(Φ) exists. Let (xk,yk) be generated by (GEG2) and wk be defined by
{ }
(35). Then, for any γ >0, we have
xk+1 x⋆ 2 xk x⋆ 2 (1 β) xk+1 xk 2 β xk yk 2
∥ − ∥ ≤ ∥ − ∥ − − ∥ − ∥ − ∥ − ∥
(β γ) xk+1 yk 2+ η2 Fyk uk 2 (36)
− − ∥ − ∥ γ ∥ − ∥
2η Fyk Fx⋆,yk x⋆ .
− ⟨ − − ⟩
Proof First, since ξk+1 Txk+1, ζk Tyk, and ξ⋆ = Fx⋆ Tx⋆, by the
∈ ∈ − ∈
maximally 3-cyclic monotonicity of T, we have
ξk+1,xk+1 x⋆ + ξ⋆,x⋆ yk + ζk,yk xk+1 0.
⟨ − ⟩ ⟨ − ⟩ ⟨ − ⟩≥
This inequality leads to
ξk+1 ζk,xk+1 x⋆ ζk ξ⋆,x⋆ yk = Fx⋆+ζk,yk x⋆ .
⟨ − − ⟩≥⟨ − − ⟩ −⟨ − ⟩
Utilizingthisestimateandthesecondlinexk xk+1 =η(Fyk+ξk+1)of (34),
−
for any x⋆ zer(Φ), we can derive that
∈
xk+1 x⋆ 2 = xk x⋆ 2 2 xk xk+1,xk+1 x⋆ xk+1 xk 2
∥ − ∥ ∥ − ∥ − ⟨ − − ⟩−∥ − ∥
= xk x⋆ 2 2η Fyk+ξk+1,xk+1 x⋆ xk+1 xk 2
∥ − ∥ − ⟨ − ⟩−∥ − ∥
= xk x⋆ 2 2η Fyk+ζk,xk+1 x⋆ xk+1 xk 2
∥ − ∥ − ⟨ − ⟩−∥ − ∥ (37)
2η ξk+1 ζk,xk+1 x⋆
− ⟨ − − ⟩
xk x⋆ 2 xk+1 xk 2 2η Fyk+ζk,xk+1 yk
≤ ∥ − ∥ −∥ − ∥ − ⟨ − ⟩
2η Fyk Fx⋆,yk x⋆ .
− ⟨ − − ⟩24 QuocTran-Dinh,NghiaNguyen-Trung
Next,fromthefirstlineof (34),wehaveη(Fyk+ζk)=β(xk yk)+η(Fyk uk).
− −
Therefore, by the Cauchy-Schwarz inequality and Young’s inequality, for any
γ >0, we can prove that
2η Fyk+ζk,xk+1 yk = 2β xk yk,xk+1 yk +2η Fyk uk,xk+1 yk
⟨ − ⟩ ⟨ − − ⟩ ⟨ − − ⟩
(cid:2) (cid:3)
β xk yk 2+ xk+1 yk 2 xk+1 xk 2
≥ ∥ − ∥ ∥ − ∥ −∥ − ∥
2η Fyk uk xk+1 yk
− ∥ − ∥∥ − ∥
β xk yk 2+(β γ) xk+1 yk 2
≥ ∥ − ∥ − ∥ − ∥
β xk+1 xk 2 η2 Fyk uk 2.
− ∥ − ∥ − γ ∥ − ∥
Finally, substituting this estimate into (37), we obtain (36). □
Lemma 4.2 Suppose that F is monotone and T is maximally 3-cyclically
monotone. Let (xk,yk) be generated by (GEG2) and wk be defined by (35)
{ }
Then, for ω 0, γ >0, and s>0, we have
≥
wk+1 2 + ω wk+1 wˆk+1 2 wk 2 (1 γ) wk w˜k 2
∥ ∥ ∥ − ∥ ≤∥ ∥ − − ∥ − ∥
(cid:104) (cid:105)
+ 1 + (1+ω)(1+s)L2η2 Fxk uk 2 (38)
γ s ∥ − ∥
(cid:2) (cid:3)
1 (1+ω)(1+s)L2η2 wˆk+1 w˜k 2.
− − ∥ − ∥
Proof First, using the 3-cyclic monotonicity of T but with ξk Txk, we have
∈
ξk+1,xk+1 xk + ξk,xk yk + ζk,yk xk+1 0.
⟨ − ⟩ ⟨ − ⟩ ⟨ − ⟩≥
Next,bythemonotonicityofF,weget Fxk+1 Fxk,xk+1 xk 0.Summing
⟨ − − ⟩≥
up these inequalities and using wk =Fxk+ξk and w˜k :=Fxk+ζk, we have
wk+1 w˜k,xk+1 xk + wk w˜k,xk yk 0. (39)
⟨ − − ⟩ ⟨ − − ⟩≥
Fromthesecondlineof (34),wehavexk+1 xk = η(Fyk+ξk+1)= ηwˆk+1.
Fromthefirstlineof (34)andβ =1,wealso− havex− k yk = η(w˜k+uk − Fxk)=
− β −
ηw˜k +η(uk Fxk). Substituting these expressions into (39), and using an
−
elementary inequality 2 z,s γ s 2+ ∥z ∥2 for any γ >0 and vectors z and
⟨ ⟩≤ ∥ ∥ γ
s, we can show that
0 2 w˜k,wˆk+1 2 wk+1,wˆk+1 +2 wk,w˜k 2 w˜k 2+2 wk w˜k,uk Fxk
≤ ⟨ ⟩− ⟨ ⟩ ⟨ ⟩− ∥ ∥ ⟨ − − ⟩
= wk 2 wk+1 2+ wk+1 wˆk+1 2 wˆk+1 w˜k 2
∥ ∥ −∥ ∥ ∥ − ∥ −∥ − ∥
wk w˜k 2+2 wk w˜k,uk Fxk
− ∥ − ∥ ⟨ − − ⟩
wk 2 wk+1 2+ wk+1 wˆk+1 2 wˆk+1 w˜k 2
≤ ∥ ∥ −∥ ∥ ∥ − ∥ −∥ − ∥
(1 γ) wk w˜k 2+ 1 Fxk uk 2.
− − ∥ − ∥ γ∥ − ∥
This inequality leads to
wk+1 2 wk 2+ wk+1 wˆk+1 2+ 1 Fxk uk 2
∥ ∥ ≤ ∥ ∥ ∥ − ∥ γ∥ − ∥
(1 γ) wk w˜k 2 wˆk+1 w˜k 2.
− − ∥ − ∥ −∥ − ∥RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 25
Now,bytheL-LipschitzcontinuityofF,xk+1 yk = η(wˆk+1 w˜k) η(Fxk
− − − − −
uk) from (34) with β =1, and Young’s inequality, for any s>0, we have
wk+1 wˆk+1 2 = Fxk+1 Fyk 2 L2 xk+1 yk 2
∥ − ∥ ∥ − ∥ ≤ ∥ − ∥
= L2η2 wˆk+1 w˜k+Fxk uk 2
∥ − − ∥
(1+s)L2η2 wˆk+1 w˜k 2+
(1+s)L2η2
Fxk uk 2.
≤ ∥ − ∥ s ∥ − ∥
Multiplying this inequality by 1+ω 0 and adding the result to the last
inequality above, we obtain (38). ≥ □
4.3 The Sublinear Convergence Rates of (GEG2)
For simplicity of our analysis, we denote r :=
κ1+√κ2 1+4κ1
and define
2
C :=β (1+r)Lη and C :=1 β κ2(1+r)Lη. (40)
1 − 2 − − r
Then, we can prove the convergence of (GEG2) under (33) as follows.
Theorem 4.1 (Best-iterate convergence rate) For Inclusion (NI), sup-
pose that zer(Φ)= , F is L-Lipschitz continuous, T is maximally 3-cyclically
̸ ∅
monotone, and there exists x⋆ zer(Φ) such that Fx Fx⋆,x x⋆ 0 for
∈ ⟨ − − ⟩≥
all x dom(F).
∈
Let (xk,yk) be generated by (GEG2) starting from x0 dom(Φ) and
{ } ∈
y 1 =x 1 :=x0 such that uk satisfies (33). Suppose that η is chosen as
− −
0<η min(cid:110) β , (1 −β)r (cid:111) with r := κ1+√κ2 1+4κ1. (41)
≤ (1+r)L κ2(1+r)L 2
Then, C and C given by (40) are nonnegative, and
1 2
K
(cid:88)(cid:2) C yk xk 2+C xk+1 yk 2+C xk+1 xk 2(cid:3) x0 x⋆ 2. (42)
1 1 2
∥ − ∥ ∥ − ∥ ∥ − ∥ ≤∥ − ∥
k=0
Moreover, if C >0, then we also have
1
  0m kin K∥uk+ζk ∥2
≤
K1 +1(cid:80)K k=0∥uk+ζk ∥2
≤
β C2 1∥ ηx 20 (− Kx +⋆ 1∥ )2 ,
≤ ≤ (43)
 0m kin K∥Fxk+1+ξk+1 ∥2
≤
K1 +1(cid:80)K k=0∥Fxk+1+ξk+1 ∥2
≤
Λ η∥2x (0 K− +x⋆ 1∥ )2 ,
≤ ≤
where Λ :=
3[3C1+2(C1+3C2)L2η2].
Hence, we conclude that min Fxk +
(cid:0)
(cid:1)3C1(C1+3C2) 0 ≤k ≤K ∥
ξk = 1/√K . Furthermore, xk x⋆ is bounded and xk converges to
∥ O {∥ − ∥} { }
x⋆ zer(Φ). In addition, we have
∈
lim xk yk =0 and lim Fxk+ξk =0.
k ∥ − ∥ k ∥ ∥
→∞ →∞26 QuocTran-Dinh,NghiaNguyen-Trung
Proof First, for any r > 0, by Young’s inequality, (33), and the L-Lipschitz
continuity of F, we can show that
Fyk uk 2 (1+r) Fxk Fyk 2+ (1+r) Fxk uk 2
∥ − ∥ ≤ ∥ − ∥ r ∥ − ∥
(1+r)L2 xk yk 2+ (1+r)κ1 Fxk Fyk 1 2
≤ ∥ − ∥ r ∥ − − ∥
+ (1+r)κ2 Fxk Fxk 1 2
r ∥ − − ∥
(1+r)L2 xk yk 2+
(1+r)κ1L2
xk yk 1 2
≤ ∥ − ∥ r ∥ − − ∥
+
(1+r)κ2L2
xk xk 1 2.
r ∥ − − ∥
Utilizing this expression and the condition Fyk Fx⋆,yk x⋆ 0, we can
⟨ − − ⟩≥
show from (36) that
xk+1 x⋆ 2+
κ1(1+r)L2η2
xk+1 yk 2+
κ2(1+r)L2η2
xk+1 xk 2 xk x⋆ 2
∥ − ∥ rγ ∥ − ∥ rγ ∥ − ∥ ≤∥ − ∥
+
κ1(1+r)L2η2
xk yk 1 2+
κ2(1+r)L2η2
xk xk 1 2
rγ ∥ − − ∥ rγ ∥ − − ∥
(cid:0)
β
(1+r)L2η2(cid:1)
xk yk 2
(cid:0)
β γ
κ1(1+r)L2η2(cid:1)
xk+1 yk 2
− − γ ∥ − ∥ − − − rγ ∥ − ∥
(cid:0)
1 β
κ2(1+r)L2η2(cid:1)
xk+1 xk 2.
− − − rγ ∥ − ∥
Now, we define a new potential function:
ˆ := xk x⋆ 2+ κ1(1+r)L2η2 xk yk 1 2+ κ2(1+r)L2η2 xk xk 1 2.
Pk ∥ − ∥ rγ ∥ − − ∥ rγ ∥ − − ∥
We also denote
C :=β (1+r)L2η2 , Cˆ :=β γ κ1(1+r)L2η2 , and C :=1 β κ2(1+r)L2η2 .
1 − γ 1 − − rγ 2 − − rγ
Then, it is obvious that ˆ 0. Moreover, the last inequality is equivalent to
k
P ≥
C yk xk 2+Cˆ xk+1 yk 2+C xk+1 xk 2 ˆ ˆ . (44)
1 1 2 k k+1
∥ − ∥ ∥ − ∥ ∥ − ∥ ≤P −P
Let us choose γ := Lη and r := κ1+√κ2 1+4κ1. Then, we obtain C = Cˆ =
2 1 1
β (1+r)Lη andC =1 β κ2(1+r)Lη asin(40).Furthermore,ifwechoose
− 2 − − r
η as in (41), then C and C are nonnegative.
1 2
Now, we sum up (44) from k = 0 to K, and note that ˆ 0 and
K+1
ˆ := x0 x⋆ 2, we obtain (42). P ≥
0
P ∥ − ∥
Next, from the first line of (34), we get uk+ζk 2 = β2 yk xk 2. Using
∥ ∥ η2∥ − ∥
this relation and (42), we obtain the first line of (43).
From the second line of (34), for any a > 0, a [0,1], and a > 0, by
1 2 3
∈
Young’s inequality and the L-Lipschitz continuity of F, we can show that
η2 Fxk+1+ξk+1 2(3 =4) xk+1 xk η(Fxk+1 Fyk) 2
∥ ∥ ∥ − − − ∥
≤
(1+a 1) ∥xk+1 −xk ∥2+(1+a−11)L2η2 ∥xk+1 −yk ∥2
a (1+a )(1+a ) yk xk 2
2 1 3
≤ ∥ − ∥
+
(cid:2)
a 2(1+a
1)(1+a−31)+(1+a−11)L2η2(cid:3)
∥xk+1 −yk ∥2
+ (1+a )(1 a ) xk+1 xk 2.
1 2
− ∥ − ∥RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 27
Inthiscase,weassumethatΛC
1
=a 2(1+a 1)(1+a 3)=(cid:2) a 2(1+a 1)(1+a−31)+
(1+a−11)L2η2(cid:3)
, and ΛC
2
=(1+a 1)(1 −a 2) for some Λ>0. By appropriately
chosen a , a , and a :=2, we obtain
1 2 3
Λ:=
3[3C1+2(C1+3C2)L2η2].
3C1(C1+3C2)
Hence, the last inequality is equivalent to
η2 Fxk+1+ξk+1 2 C xk yk 2+C xk+1 yk 2+C xk+1 xk 2.
Λ∥ ∥ ≤ 1 ∥ − ∥ 1 ∥ − ∥ 2 ∥ − ∥
Combining this inequality and (42), we obtain the second line of (43). The
remaining statements are proven similar to Theorem 3.1 and we omit. □
Next, we prove the last-iterate convergence rate of (GEG2) when κ =0.
2
Theorem 4.2 (Last-iterate convergence rate) For Inclusion (NI), sup-
pose that zer(Φ) = , F is monotone and L-Lipschitz continuous, and T is
̸ ∅
maximally 3-cyclically monotone.
Let (xk,yk) be generated by (GEG2) starting from x0 dom(Φ) and
{ } ∈
y 1 =x 1 :=x0 such that uk satisfies (33). Suppose that η is chosen as
− −
0<η < β with r := κ1+√κ2 1+4κ1. (45)
(1+r)L 2
Then, for wk := Fxk + ξk with ξk Txk and ω :=
κ1[s+(1+s)L2η2]
with
∈ s −(1+s)κ1L2η2
s:=
(1+r)2 −2κ1−1+√[(1+r)2 −2κ1−1]2 −4κ1(1+κ1)
0, we have
2(1+κ1) ≥
wk+1 2+ω Fxk+1 Fyk 2 wk 2+ω Fxk Fyk 1 2,
−
∥ ∥ ∥ − ∥ ≤∥ ∥ ∥ − ∥
(46)
FxK+1+ξK+1 2 Γ ∥x0 −x⋆ ∥2 ,
∥ ∥ ≤ K+1
where Γ := Λ + L2ω. Hence, we conclude that FxK +ξK = (cid:0) 1/√K(cid:1) on
η2 C1 ∥ ∥ O
the last iterate xK for some ξK TxK.
∈
Proof First,sincewk =Fxk+ξk andwˆk =Fyk 1+ξk forξk Txk,wehave
−
∈
Fxk Fyk 1 =wk wˆk 1.Next,sinceκ =0,(33)reducesto Fxk uk 2
− − 2
− − ∥ − ∥ ≤
κ Fxk Fyk 1 2 = κ wk wˆk 2 due to Fxk Fyk 1 = wk wˆk. Using
1 − 1 −
∥ − ∥ ∥ − ∥ − −
this relation into (38) and choosing γ =1, we get
(cid:104) (cid:105)
wk+1 2+ω wk+1 wˆk+1 2 wk 2+κ 1+
(1+ω)(1+s)L2η2
wk wˆk 2
∥ ∥ ∥ − ∥ ≤∥ ∥ 1 s ∥ − (∥47)
(cid:2) (cid:3)
1 (1+ω)(1+s)L2η2 wˆk+1 w˜k 2.
− − ∥ − ∥
Now, we choose ω =
κ1[s+(1+s)L2η2],
provided that (1+s)κ L2η2 < s. Then,
we have ω =κ
(cid:2)
1+
(1s +− ω( )1 (+ 1+s) sκ )1 LL 22 ηη 22 (cid:3)
.
1
1 s
We also need to guarantee (1+ω)(1+s)L2η2 1, which is equivalent to
≤
L2η2 s .Ifwechooses:= (1+r)2 −2κ1−1+√[(1+r)2 −2κ1−1]2 −4κ1(1+κ1)
≤ (1+s)(κ1s+κ1+s) 2(1+κ1)28 QuocTran-Dinh,NghiaNguyen-Trung
for r := κ1+√κ2 1+4κ1, then we have s = 1 . Furthermore, if
2 (1+s)(κ1s+κ1+s) (1+r)2
we choose η < 1 as in (45), then L2η2 s .
(1+r)L ≤ (1+s)(κ1s+κ1+s)
Next, under (45) and the choice of ω, (47) reduces to
wk+1 2 + ω wk+1 wˆk+1 2 wk 2+ω wk wˆk 2. (48)
∥ ∥ ∥ − ∥ ≤∥ ∥ ∥ − ∥
which proves the first line of (46) due to Fxk Fyk 1 =wk wˆk.
−
− −
Finally, from (42) and the second line of (43), we have
K1 +1(cid:80)K k=0∥Fxk+1 −Fyk ∥2
≤
K1 +1(cid:80)K k=0L2 ∥xk+1 −yk ∥2
≤
L2 C∥ 1x (0 K− +x 1⋆ )∥2 ,
K1 +1(cid:80)K k=0∥Fxk+1+ξk+1 ∥2
≤
Λ η∥2x (0 K− +x⋆ 1∥ )2 .
Multiplying the first line by ω and adding the result to the second line, and
thenusing(48)withwk+1 :=Fxk+1+ξk+1 andwk+1 wˆk+1 =Fxk+1 Fyk,
− −
we obtain the second line of (46), where Γ := L2ω + Λ. □
C1 η2
4.4 The Sublinear Convergence Rates of Two Instances
Now, we specify Theorems 4.1 and 4.2 for two instances: the extragradient
method (Variant 1) and Popov’s past-extragradient method (Variant 2).
Corollary 4.1 (TheEGMethod)ForInclusion (NI),supposethatzer(Φ)=
̸
, F is L-Lipschitz continuous, T is maximally 3-cyclically monotone, and
∅
there exists x⋆ zer(Φ) such that Fx Fx⋆,x x⋆ 0 for all x dom(F).
∈ ⟨ − − ⟩≥ ∈
Let (xk,yk) be generated by (GEG2) starting from x0 dom(Φ) using
uk :={ Fxk (V} ariant 1) and 0<η β. ∈
≤ L
(a) The best-iterate convergence rate of EG. Then, we have
0m kin K∥Fxk+1+ξk+1 ∥2
≤
K1 +1(cid:80)K k=0∥Fxk+1+ξk+1 ∥2
≤
Λ η∥2x (0 K− +x⋆ 1∥ )2 , (49)
≤ ≤
where ξk Txk and Λ is defined in (43).
∈
Furthermore, xk x⋆ is nonincreasing and xk converges to x⋆
{∥ − ∥} { } ∈
zer(Φ). We also have
lim xk yk = lim Fxk+ξk = lim Fyk+ζk =0, (50)
k ∥ − ∥ k ∥ ∥ k ∥ ∥
→∞ →∞ →∞
where ξk Txk and ζk Tyk.
∈ ∈
(b) The last-iterate convergence rate of EG. If, in addition, F is mono-
tone, then Fxk+ξk is monotonically non-increasing and
{∥ ∥}
Λ x0 x⋆ 2
FxK+1+ξK+1 2 ∥ − ∥ . (51)
∥ ∥ ≤ η2(K+1)
(cid:0) (cid:1)
Hence, we conclude that FxK +ξK = 1/√K on the last-iterate xk.
∥ ∥ O
(c) Convergence in xk . For defined by (4), we also have
η η
∥G ∥ G
(cid:16) (cid:17) (cid:16) (cid:17)
0m kin K∥Gηxk ∥=
O
√1
K
and ∥GηxK ∥=
O
√1
K
. (52)
≤ ≤RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 29
Proof For Variant 1 with uk :=Fxk, we have κ =κ =0, leading to r =0.
1 2
Hence, (41) reduces to 0 < η β, and (49) is a direct consequence of the
≤ L
second line of (43). Combining (49) and (42), we obtain (50).
Next, since κ = 0, we have ω = 0 in Theorem 4.2. Therefore, the second
1
estimateof (46)reducesto(51),whereΓ = Λ.Finally,combiningourresults
η2
and (5) we obtain the remaining conclusions. □
Corollary 4.2 (The Past-EG Method) For Inclusion (NI), suppose that
zer(Φ) = , F is L-Lipschitz continuous, T is maximally 3-cyclically mono-
̸ ∅
tone, and there exists x⋆ zer(Φ) such that Fx Fx⋆,x x⋆ 0 for
∈ ⟨ − − ⟩ ≥
all x dom(F). Let (xk,yk) be generated by (GEG2) starting from x0
∈ { } ∈
dom(Φ) and y 1 := x0 using uk := Fyk 1 (Variant 2) and η such that
− −
0<η 2β .
≤ (3+√5)L
(a) The best-iterate convergence rate of Past-EG. Then, we have
0m kin K∥Fxk+1+ξk+1 ∥2
≤
K1 +1(cid:80)K k=0∥Fxk+1+ξk+1 ∥2
≤
Λ η∥2x (0 K− +x⋆ 1∥ )2 , (53)
≤ ≤
where ξk Txk and Λ is defined in (43). Moreover, xk x⋆ is bounded
∈ {∥ − ∥}
and xk converges to x⋆. Furthermore, (50) still holds.
{ }
(b) The last-iterate convergence rate of Past-EG. If, in addition, F is
monotone, then we have
Fxk+1+ξk+1 2+ω Fxk+1 Fyk 2 Fxk+ξk 2+ω Fxk Fyk 1 2
−
∥ ∥ ∥ − ∥ ≤∥ ∥ ∥ − ∥
(54)
∥FxK+1+ξK+1 ∥2
≤
Λ η∥2x (0 K− +x⋆ 1∥ )2 .
(cid:0) (cid:1)
Thus we conclude that FxK +ξK = 1/√K on the last iterate xK.
∥ ∥ O
(c) Convergence in xk . For defined by (4), we also have
η η
∥G ∥ G
(cid:16) (cid:17) (cid:16) (cid:17)
0m kin K∥Gηxk ∥=
O
√1
K
and ∥GηxK ∥=
O
√1
K
. (55)
≤ ≤
Proof For Variant 2 with uk := Fyk 1, we have κ = 1 and κ = 0 leading
− 1 2
to r = 1+√5. Hence, (41) reduces to 0 < η 2β , and (53) is a direct
2 ≤ (3+√5)L
consequence of the second line of (43). Next, since κ = 1, we have ω > 0 in
1
Theorem 4.2. Therefore, the second estimate of (46) reduces to (54). Finally,
combining our results and (5) we obtain the remaining conclusions. □
Remark 4.1 If β = 1, then the condition 0 < η < 1 in Corollary 4.1 is
L
the same as in the classical EG method, see [33]. Similarly, when β = 1,
the condition 0 < η 2 in Corollary 4.2 is slightly relaxed than the
≤ (3+√5)L
condition0<η 1 in[76].Itremainsopentoestablishboththebest-iterate
≤ 3L
and last-iterate convergence rates of (GEG2) under the weak-Minty solution
condition [29] and the co-hypomonotonicity of Φ.
5 A Class of Forward-Backward-Forward Splitting Methods for (NI)
Alternativeto(GEG2),wenowgeneralizetheFBFSmethodin[93]forsolving
(NI), but when a weak-Minty solution exists.30 QuocTran-Dinh,NghiaNguyen-Trung
5.1 A Class of Forward-Backward-Forward Splitting Methods
The forward-backward-forward splitting (FBFS) method was proposed by P.
Tseng in [93] for solving (NI), which is originally called a modified forward-
backward splitting method.
Now, we propose to consider the following generalized FBFS scheme for
solving (NI). Starting from x0 dom(Φ), at each iteration k 0, we update
∈ ≥
(cid:40) yk := J βηT(xk
−
βηuk),
(GFBFS2)
xk+1 := βyk+(1 β)xk η(Fyk uk),
− − −
where J βηT is the resolvent of βηT, η > 0 is a given stepsize, and β > 0 is a
scaling factor, and uk Rp satisfies the following condition:
∈
Fxk uk 2 κ Fxk Fyk 1 2+κ Fxk Fxk 1 2, (56)
1 − 2 −
∥ − ∥ ≤ ∥ − ∥ ∥ − ∥
for given constants κ 0 and κ 0 and x 1 = y 1 := x0. Here, we have
1 2 − −
≥ ≥
ourflexibilitytochooseuk.Letusconsiderthreespecialcasesofuk asbefore.
(a) Variant 1. If we choose uk :=Fxk, then we obtain a variant of Tseng’s
FBFS method. In particular, if β = 1, then we get exactly Tseng’s
FBFSmethodin[93]forsolving(NI).Notethatonecanextend(GFBFS2)
tocoverthecasezer(Φ) = forsomesubset ofRp aspresentedin[93].
Nevertheless, for
simpli∩ citC y,̸ w∅
e assume that
=CRp.
As shown in (FBFS),
C
if T =0, then (GFBFS2) reduces to the extragradient method (GEG) for
(NE). However, if T =0, then (GFBFS2) is different from (GEG2).
̸
(b) Variant 2. If we choose uk :=Fyk 1, where y 1 :=x0, then we obtain a
− −
past-FBFS variant. This variant can also be referred to as a generalized
variant of the optimistic gradient (OG) method, see, e.g., [26,60,61].
Ifβ =1,thenyk+1 =J (xk+1 ηFyk)andxk+1 =yk η(Fyk Fyk 1).
ηT −
− − −
Combining these two expressions, (GFBFS2) reduces to
(cid:0) (cid:1)
yk+1 = J yk η(2Fyk Fyk 1) . (FRBS2)
ηT −
− −
This is exactly the forward-reflected-backward splitting (FRBS)
method proposed in [56].
(c) Variant 3. We can form uk :=α Fxk+α Fyk 1+(1 α α )Fxk 1
1 2 − 1 2 −
− −
as an affine combination of Fxk, Fyk 1 and Fxk 1 for given constants
− −
α ,α R. Then, uk satisfies (56) with κ = (1+c)(1 α )2 and κ =
1 2 1 1 2
∈ −
(1+c 1)(1 α α )2 for some c>0 by Young’s inequality.
− 1 2
− −
Comparedto(GEG2),wedonotrequireT tobemonotonein(GFBFS2).How-
ever, to guarantee the well-definedness of (xk,yk) , we need yk ran(J )
ηT
and yk dom(F). Hence, we can assume t{ hat ran(J} ) dom(F)∈ = Rp and
ηT
dom(J ∈ ) = Rp. This requirement makes (GFBFS2) cov⊆ er a broader class of
ηT
problems than (GEG2), and it obviously holds if T is maximally monotone
andF ispartiallymonotoneandLipschitzcontinuousasin(GEG2).Here,we
assume that J is single-valued, but it can be extended to multivalued J .
ηT ηTRevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 31
In addition, (GFBFS2) only requires one evaluation of J instead of two as
ηT
in (GEG2), reducing the per-iteration complexity when J is expensive.
ηT
Similar to (34), we can rewrite (GFBFS2) equivalently to
(cid:40) yk := xk η(uk+ζk), ζk Tyk,
− β ∈ (57)
xk+1 := xk+β(yk xk) η(Fyk uk)=xk η(Fyk+ζk).
− − − −
This representation is an important step for our convergence analysis below.
5.2 Key Estimates for Convergence Analysis
The following lemma is key to establishing convergence of (GFBFS2).
Lemma 5.1 Suppose that (xk,yk) is generated by (GFBFS2) and T is not
necessary monotone, but ra{ n(J ) } dom(F) = Rp and dom(J ) = Rp.
ηT ηT
⊆
Then, for any γ >0, any x⋆ zer(Φ), we have
∈
xk+1 x⋆ 2 xk x⋆ 2 β xk yk 2+ η2 Fyk uk 2
∥ − ∥ ≤ ∥ − ∥ − ∥ − ∥ γ ∥ − ∥
(β γ) xk+1 yk 2 2η Fyk+ζk,yk x⋆ (58)
− − ∥ − ∥ − ⟨ − ⟩
(1 β) xk+1 xk 2.
− − ∥ − ∥
For any r >0, let us introduce the following potential function:
:= xk x⋆ 2+
κ1(1+r)L2η2
xk yk 1 2+
κ2(1+r)L2η2
xk xk 1 2.(59)
Pk ∥ − ∥ rγ ∥ − − ∥ rγ ∥ − − ∥
Ifuk satisfies (56)andthereexistx⋆ zer(Φ)andρ 0suchthat w,x x⋆
∈ ≥ ⟨ − ⟩≥
ρ w 2 for all (x,w) gra(Φ), then for any s>0 and µ [0,1], we have
− ∥ ∥ ∈ ∈
(cid:16) (cid:17)
β (1+r)L2η2 2µρ(1+s) yk xk 2
Pk+1 ≤ Pk − − γ − sη ∥ − ∥
(cid:16) (cid:17)
β γ κ1(1+r)L2η2 2µρ(1+s) xk+1 yk 2 (60)
− − − rγ − η ∥ − ∥
(cid:0) 1 β κ2(1+r)L2η2 2(1 −µ)ρ(cid:1) xk+1 xk 2.
− − − rγ − η ∥ − ∥
Proof First, combining the first and second lines of (57), we get xk+1 =xk
−
β(xk yk)+η(uk Fyk)=xk η(Fyk+ζk). Using this relation, we have
− − −
xk+1 x⋆ 2 = xk x⋆ 2 2η Fyk+ζk,xk+1 yk xk+1 xk 2
∥ − ∥ ∥ − ∥ − ⟨ − ⟩−∥ − ∥
2η Fyk+ζk,yk x⋆ .
− ⟨ − ⟩
Next,fromthefirstlineof (57),wehaveη(Fyk+ζk)=β(xk yk)+η(Fyk uk).
− −
Hence,bytheCauchy-Schwarz’sandYoung’sinequality,foranyγ >0,wecan
derive that
2η Fyk+ζk,xk+1 yk = 2β xk yk,xk+1 yk +2η Fyk uk,xk+1 yk
⟨ − ⟩ ⟨ − − ⟩ ⟨ − − ⟩
(cid:2) (cid:3)
β xk yk 2+ xk+1 yk 2 xk+1 xk 2
≥ ∥ − ∥ ∥ − ∥ −∥ − ∥
2η Fyk uk xk+1 yk
− ∥ − ∥∥ − ∥
β xk yk 2+(β γ) xk+1 yk 2
≥ ∥ − ∥ − ∥ − ∥
β xk+1 xk 2 η2 Fyk uk 2.
− ∥ − ∥ − γ ∥ − ∥
Finally, combining the last two expressions, we obtain (58). The proof of (60)
is similar to the proof of Lemma 3.2, and we omit. □32 QuocTran-Dinh,NghiaNguyen-Trung
5.3 The Sublinear Best-Iterate Convergence Rate of (GFBFS2)
Now, we are ready to state the best-iterate convergence rate of (GFBFS2) for
any uk satisfying (56).
Theorem 5.1 (Best-iterate convergence rate) For Inclusion (NI), sup-
pose that zer(Φ)= , F is L-Lipschitz continuous, and there exist x⋆ zer(Φ)
̸ ∅ ∈
and ρ 0 such that u,x x⋆ ρ u 2 for all (x,u) gra(Φ). Suppose
further≥ that T is not n⟨ ecess− arily⟩ m≥ on− oto∥ ne∥ , but ran(J ) ∈ dom(F)=Rp and
ηT
dom(J )=Rp. ⊆
ηT
Let ∆ be defined by (14), and η and η¯ be defined by (15). Let (xk,yk)
{ }
be generated by (GFBFS2) starting from x0 dom(Φ) and y 1 = x 1 := x0
− −
∈
such that uk satisfies (56). If Lρ ∆ and η is chosen such that η [η,η¯],
≤ ∈
then C and C in (16) are nonnegative.
1 2
Moreover, for any K 0, we also have
≥
K
(cid:88)(cid:2) C yk xk 2+C xk+1 yk 2+C xk+1 xk 2(cid:3) x0 x⋆ 2. (61)
1 1 2
∥ − ∥ ∥ − ∥ ∥ − ∥ ≤∥ − ∥
k=0
If C >0, the following bounds also hold:
1
  0m kin K∥Fyk+ζk ∥2
≤
K1 +1(cid:80)K k=0∥Fyk+ζk ∥2
≤
Λ∥x η0 2− (Kx +⋆ ∥ 12 ),
≤ ≤ (62)
 0m kin K∥Fxk+ξk ∥2
≤
K1 +1(cid:80)K k=0∥Fxk+ξk ∥2
≤
Γ ∥x K0 − +x 1⋆ ∥2 ,
≤ ≤
where ζk Tyk, ξk Txk, Λ:= C1+2C2, and Γ := 2L2 + 2 . Thus we have
∈ ∈ 2 C1 Λη2
(cid:18) (cid:19) (cid:18) (cid:19)
1 1
min Fxk+ξk = and min Fyk+ζk = .
0 k K∥ ∥ O √K 0 k K∥ ∥ O √K
≤ ≤ ≤ ≤
The sequence xk x⋆ is bounded and xk converges to x⋆ zer(Φ). In
{∥ − ∥} { } ∈
addition, we have
lim xk yk = lim Fxk+ξk = lim Fyk+ζk =0.
k ∥ − ∥ k ∥ ∥ k ∥ ∥
→∞ →∞ →∞
Proof The proof of this theorem is very similar to the proof of Theorem 3.1,
but using Lemma 5.1, and we do not repeat it here. □
5.4 The Sublinear Convergence Rates of Two Instances
Now, we specify Theorem 5.1 for two instances: Tseng’s method (Variant
1) and the forward-reflected-backward splitting method (Variant 2). These
results are very similar to the ones in Subsection 3.5, but we state them here
for completeness without proof.RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 33
Corollary 5.1 (FBFS method) For Inclusion (NI), suppose that zer(Φ)=
̸
, F is L-Lipschitz continuous, and there exist x⋆ zer(Φ) and ρ 0 such
∅ ∈ ≥
that w,x x⋆ ρ w 2 for all (x,w) gra(Φ). Suppose further that T is
not n⟨ ecessa− rily⟩ m≥ on− oto∥ ne∥ , but ran(J ) ∈ dom(F)=Rp and dom(J )=Rp.
ηT ηT
⊆
Let (xk,yk) be generated by (GFBFS2) using uk := Fxk. Assume that
{ }
Lρ
β2
for any 0<β 1, and η is chosen as
≤ 16 ≤
0 β −√β2 −16Lρ <η < β+√β2 −16Lρ β. (63)
≤ 2L 2L ≤ L
Then, for some ξk Txk and Γ given in Theorem 5.1, we have
∈
1 (cid:88)K Γ x0 x⋆ 2
min Fxk+ξk 2 Fxk+ξk 2 ∥ − ∥ . (64)
0 k K∥ ∥ ≤ K+1 ∥ ∥ ≤ K+1
≤ ≤ k=0
Moreover, xk x⋆ is nonincreasing and bounded, and xk converges to
{∥ − ∥} { }
x⋆ zer(Φ). For ξk Txk and ζk Tyk, we also have
∈ ∈ ∈
lim xk yk = lim Fxk+ξk = lim Fyk+ζk =0. (65)
k ∥ − ∥ k ∥ ∥ k ∥ ∥
→∞ →∞ →∞
If J is nonexpansive and x:= 1(x J (x ηFx)) is given by (4), then
ηT Gη η − ηT −
(cid:18) (cid:19) (cid:18) (cid:19)
1 1
min xk = and min yk = . (66)
η η
0 k K∥G ∥ O √K 0 k K∥G ∥ O √K
≤ ≤ ≤ ≤
Corollary 5.2 (Past-FBFS/OGmethod)ForInclusion (NI),supposethat
zer(Φ)= ,F isL-Lipschitzcontinuous,andthereexistx⋆ zer(Φ)andρ 0
̸ ∅ ∈ ≥
suchthat u,x x⋆ ρ u 2 forall(x,u) gra(Φ).SupposefurtherthatT is
not necess⟨ arily− mo⟩ no≥ to− ne,∥ b∥ ut ran(J ) d∈ om(F)=Rp and dom(J )=Rp.
ηT ηT
⊆
Let (xk,yk) be generated by (GFBFS2) starting from x0 dom(Φ) and
{ } ∈
y 1 :=x0 using uk :=Fyk 1. Assume that Lρ β2 and η is chosen as
− − ≤ 8(3+√5)
0 β −√β2 −8(3+√5)Lρ <η < β+√β2 −8(3+√5)Lρ 2β . (67)
≤ (3+√5)L (3+√5)L ≤ (3+√5)L
Then, we have
1 (cid:88)K Γ x0 x⋆ 2
min Fxk+ξk 2 Fxk+ξk 2 ∥ − ∥ , (68)
0 k K∥ ∥ ≤ K+1 ∥ ∥ ≤ K+1
≤ ≤ k=0
where ξk Txk and Γ is given in Theorem 5.1. Moreover, xk x⋆ is
∈ {∥ − ∥}
nonincreasing and bounded, and xk converges to x⋆ zer(Φ). If J is
ηT
{ } ∈
nonexpansive, then (66) remains valid.
Remark 5.1 If ρ = 0, i.e. there exist x⋆ zer(Φ) such that w,x x⋆ 0
∈ ⟨ − ⟩ ≥
for all (x,w) gra(Φ), then the condition on η from Theorem 5.1 reduces to
∈
0<η < β2 as we have seen in Remark 3.1. For FBFS with uk :=Fxk, we
(1+r)L
have 0 < η < β2 , while for past-FBFS with uk := Fyk 1, we have 0 < η <
L −
2β2
. Hitherto, we have only proven the best-iterate convergence rates of
(3+√5)L
(GFBFS2). The last-iterate convergence rate of this method remains open.34 QuocTran-Dinh,NghiaNguyen-Trung
6 Two Other Variants of The Extragradient Method
In this section, we first provide a new convergence analysis for the reflected
forward-backward splitting (RFBS) algorithm [18,54]. Our best-iterate con-
vergencerateanalysisisdifferentfromtheonein[18]formonotoneinclusions.
Ourlast-iterateconvergencerateanalysisisnew,simple,andgeneralthanthe
one in [15], which is only for (VIP). Next, we extend the convergence analy-
sis for the golden ratio (GR) method [55] to a wider range of its parameters.
OurnewrangeshowsapotentialimprovementofGRonconcreteexamplesin
Section 7. Nevertheless, its last-iterate convergence rate is still open.
6.1 New Convergence Analysis of The RFBS Method for (NI)
The RFBS method was proposed by Malitsky in [54] to solve (VIP) and it is
calledtheprojected reflected gradientmethod.Itwasgeneralizedtosolve
monotone (NI) in [18]. The last iterate convergence rate of the projected
reflected gradient method for (VIP) was recently proven in [15]. Here, we
provide a new and full convergence rate analysis for RFBS to solve (NI) by
exploiting our analysis techniques in the previous sections.
6.1.1 The RFBS Method for (NI)
The reflected forward-backward splitting (RFBS) method to approximate a
solution of (NI) is described as follows. Starting from x0 dom(Φ), we set
∈
x 1 :=x0 and at each iteration k 0, we update
−
≥
(cid:40)
yk := 2xk xk 1,
−
− (RFBS2)
xk+1 := J (xk ηFyk),
ηT
−
where η >0 is a given step-size, determined later.
Clearly, if we eliminate yk, then (RFBS2) can be rewritten in one line:
xk+1 :=J (xk ηF(2xk xk 1)).
ηT −
− −
From the second line of (RFBS2), we have ξk+1 := 1(xk ηFyk xk+1)
η − − ∈
Txk+1. As before, if we denote
wk :=Fxk+ξk and wˆk :=Fyk 1+ξk, (69)
−
then we can rewrite (RFBS2) equivalently to
(cid:40)
yk := xk+xk xk 1 = xk ηwˆk,
−
− − (70)
xk+1 := xk ηwˆk+1.
−
This expression leads to xk+1 =yk η(wˆk+1 wˆk).
− −RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 35
6.1.2 Key Estimates for Convergence Analysis
Next, we prove the following lemmas for our analysis.
Lemma 6.1 For Inclusion (NI), assume that zer(Φ) = , T is maximally
̸ ∅
monotoneandF isL-Lipschitzcontinuousandsatisfies Fx Fx⋆,x x⋆ 0
⟨ − − ⟩≥
for all x dom(F) and some x⋆ zer(Φ). Let (xk,yk) be generated by
∈ ∈ { }
(RFBS2) using η >0 and be defined as
k
V
(cid:0) (cid:1)
:= xk x⋆ 2+2 xk xk 1 2+ 1 √2Lη xk yk 1 2
k − −
V ∥ − ∥ ∥ − ∥ − ∥ − ∥ (71)
+ 2η Fyk 1 Fx⋆,xk xk 1 .
− −
⟨ − − ⟩
Then, we have
(cid:2) (cid:3)(cid:2) (cid:3)
+ 1 (1+√2)Lη yk xk 2+ xk yk 1 2 ,
k k+1 −
V ≥ V − ∥ − ∥ ∥ − ∥
(1 Lη) xk x⋆ 2+(1 (1+√2)Lη) xk yk 1 2 (72)
k −
V ≥ − ∥ − ∥ − ∥ − ∥
+ 2(1 Lη) xk xk 1 2.
−
− ∥ − ∥
Proof First,since(RFBS2)isequivalentto(70),wehavexk+1 xk = ηwˆk+1
− −
from line 2 of (70). Using this expression, for any x⋆ zer(Φ), we get
∈
xk+1 x⋆ 2 = xk x⋆ 2 2η wˆk+1,xk+1 x⋆ xk+1 xk 2. (73)
∥ − ∥ ∥ − ∥ − ⟨ − ⟩−∥ − ∥
Next,sinceFx⋆+ξ⋆ =0fromthefactthatx⋆ zer(Φ)andT ismonotone,itis
∈
obvioustoshowthat ξk+1,xk+1 x⋆ ξ⋆,xk+1 x⋆ = Fx⋆,xk+1 x⋆ ,
⟨ − ⟩≥⟨ − ⟩ −⟨ − ⟩
where ξ⋆ Tx⋆. Using this relation, we can prove that
∈
wˆk+1,xk+1 x⋆ = Fyk,xk+1 x⋆ + ξk+1,xk+1 x⋆
⟨ − ⟩ ⟨ − ⟩ ⟨ − ⟩ (74)
Fyk Fx⋆,yk x⋆ Fyk Fx⋆,yk xk+1 .
≥ ⟨ − − ⟩−⟨ − − ⟩
Utilizing yk xk = xk xk 1 from the first line of (RFBS2), we can further
−
− −
expand the term := Fyk Fx⋆,yk xk+1 as
[1]
T ⟨ − − ⟩
:= Fyk Fx⋆,yk xk+1
[1]
T ⟨ − − ⟩
= Fyk 1 Fx⋆,yk xk Fyk Fx⋆,xk+1 xk
−
⟨ − − ⟩−⟨ − − ⟩
+ Fyk Fyk 1,yk xk (75)
−
⟨ − − ⟩
= Fyk 1 Fx⋆,xk xk 1 Fyk Fx⋆,xk+1 xk
− −
⟨ − − ⟩−⟨ − − ⟩
+ Fyk Fyk 1,yk xk .
−
⟨ − − ⟩
Now,fromthesecondlineof (70),wehaveηξk+1 =xk xk+1 ηFyk,leading
− −
toη(ξk+1 ξk)=2xk xk 1 xk+1 η(Fyk Fyk 1)=yk xk+1 η(Fyk
− −
− − − − − − − −
Fyk 1).BythemonotonicityofT,wehave yk xk+1 η(Fyk Fyk 1),xk+1
− −
⟨ − − − −
xk =η ξk+1 ξk,xk+1 xk 0, leading to
⟩ ⟨ − − ⟩≥
2η Fyk Fyk 1,xk+1 xk 2 yk xk+1,xk+1 xk
−
⟨ − − ⟩ ≤ ⟨ − − ⟩
= yk xk 2 xk+1 xk 2 xk+1 yk 2.
∥ − ∥ −∥ − ∥ −∥ − ∥36 QuocTran-Dinh,NghiaNguyen-Trung
BytheCauchy-Schwarzinequality,theLipschitzcontinuityofF,andYoung’s
inequality, we can show that
2η Fyk Fyk 1,yk xk+1 ηL(√2+1) yk xk 2+ηL xk yk 1 2
− −
⟨ − − ⟩ ≤ ∥ − ∥ ∥ − ∥
+ √2ηL xk+1 yk 2.
∥ − ∥
Summing up the last two inequalities, we get
(cid:2) (cid:3)
2η Fyk Fyk 1,yk xk 1+ηL(√2+1) yk xk 2+ηL xk yk 1 2
− −
⟨ − − ⟩ ≤ ∥ − ∥ ∥ − ∥
(1 √2Lη) xk+1 yk 2 xk+1 xk 2.
− − ∥ − ∥ −∥ − ∥
Combining this inequality, (74), and (75) with (73), we can prove that
xk+1 x⋆ 2 xk x⋆ 2+2η Fyk 1 Fx⋆,xk xk 1
− −
∥ − ∥ ≤ ∥ − ∥ ⟨ − − ⟩
(cid:2) (cid:3)
+ 1+Lη(√2+1) yk xk 2 2 xk+1 xk 2
∥ − ∥ − ∥ − ∥
+ Lη xk yk 1 2 (1 √2Lη) xk+1 yk 2
−
∥ − ∥ − − ∥ − ∥
2η Fyk Fx⋆,xk+1 xk 2η Fyk Fx⋆,yk x⋆ .
− ⟨ − − ⟩− ⟨ − − ⟩
Using the definition (71) of , Fyk Fx⋆,yk x⋆ 0, and yk xk =
k
V ⟨ − − ⟩ ≥ −
xk xk 1, this estimate becomes
−
−
:= xk+1 x⋆ 2+2 xk+1 xk 2+(1 √2Lη) xk+1 yk 2
k+1
V ∥ − ∥ ∥ − ∥ − ∥ − ∥
+ 2η Fyk Fx⋆,xk+1 xk
⟨ − − ⟩
(cid:2) (cid:3)
xk x⋆ 2+ 1+(√2+1)Lη yk xk 2+Lη xk yk 1 2
−
≤ ∥ − ∥ ∥ − ∥ ∥ − ∥
+ 2η Fyk 1 Fx⋆,xk xk 1
− −
⟨ − − ⟩
(cid:2) (cid:3)(cid:0) (cid:1)
= 1 (√2+1)Lη yk xk 2+ xk yk 1 2 ,
k −
V − − ∥ − ∥ ∥ − ∥
which proves the first inequality of (72).
Next, using Young’s inequality twice, we can show that
2η Fyk 1 Fx⋆,xk xk 1 Lη yk 1 x⋆ 2 2Lη xk xk 1 2
⟨ − − − − ⟩ ≥ − 2 ∥ − − ∥ − ∥ − − ∥
Lη xk x⋆ 2 Lη xk yk 1 2
−
≥ − ∥ − ∥ − ∥ − ∥
2Lη xk xk 1 2.
−
− ∥ − ∥
Substituting this estimate into (71), we get
(cid:0) (cid:1)
:= xk x⋆ 2+2 xk xk 1 2+ 1 √2Lη xk yk 1 2
k − −
V ∥ − ∥ ∥ − ∥ − ∥ − ∥
+ 2η Fyk 1 Fx⋆,xk xk 1
− −
⟨ − − ⟩
(1 Lη) xk x⋆ 2+(1 (1+√2)Lη) xk yk 1 2
−
≥ − ∥ − ∥ − ∥ − ∥
+ 2(1 Lη) xk xk 1 2,
−
− ∥ − ∥
which proves the second line of (72). □RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 37
Lemma 6.2 For Inclusion (NI), suppose that Φ is maximally monotone and
F is L-Lipschitz continuous. Let (xk,yk) be generated by (RFBS2). Then
{ }
Fxk+ξk 2 5L2η2+3 xk yk 2+ 5L2η2+3 xk yk 1 2. (76)
∥ ∥ ≤ 3η2 ∥ − ∥ 5η2 ∥ − − ∥
Moreover, if √2Lη <1, then with ω := 2L2η2 >0, we also have
1 2L2η2
−
Fxk+1+ξk+1 2+ω Fxk+1 Fyk 2 Fxk+ξk 2+ω Fxk Fyk 1 2
−
∥ ∥ ∥ − ∥ ≤∥ ∥ ∥ − ∥
(cid:16) (cid:17) (77)
−
1 1−4 2L L2 2η η2
2
∥Fyk −Fxk+ξk+1 −ξk ∥2.
−
Proof First, by Young’s inequality, xk yk = η(Fyk 1 +ξk) from (70), and
−
−
the L-Lipschitz continuity of F, we have
Fxk+ξk 2 (cid:0) 1+ 5L2η2(cid:1) Fyk 1+ξk 2+(cid:16) 1+ 3 (cid:17) Fxk Fyk 1 2
∥ ∥ ≤ 3 ∥ − ∥ 5L2η2 ∥ − − ∥
5L2η2+3 xk yk 2+ 5L2η2+3 xk yk 1 2.
≤ 3η2 ∥ − ∥ 5η2 ∥ − − ∥
This estimate is exactly (76).
Next, by the monotonicity of Φ, we have wk+1 wk,xk+1 xk 0.
⟨ − − ⟩ ≥
Substituting xk+1 xk = ηwˆk+1 into this inequality, we get
− −
0 2 wk,wˆk+1 2 wk+1,wˆk+1
≤ ⟨ ⟩− ⟨ ⟩
= wk 2 wk+1 + wk+1 wˆk+1 2 wˆk+1 wk 2.
∥ ∥ −∥ ∥ ∥ − ∥ −∥ − ∥
This inequality can be simplified as
wk+1 2 wk 2+ wk+1 wˆk+1 2 wˆk+1 wk 2.
∥ ∥ ≤ ∥ ∥ ∥ − ∥ −∥ − ∥
By the Lipschitz continuity of F and xk+1 yk = η(wˆk+1 wˆk), we have
− − −
wk+1 wˆk+1 2 = Fxk+1 Fyk 2 L2 xk+1 yk 2
∥ − ∥ ∥ − ∥ ≤ ∥ − ∥
= L2η2 wˆk+1 wˆk 2
∥ − ∥
2L2η2 wˆk+1 wk 2+2L2η2 wk wˆk 2.
≤ ∥ − ∥ ∥ − ∥
Multiplying this expression by ω+1 for ω 0, and adding the result to the
≥
last inequality, we get
wk+1 2+ω wk+1 wˆk+1 2 wk 2+2(ω+1)L2η2 wk wˆk 2
∥ ∥ ∥ − ∥ ≤ ∥ ∥ ∥ − ∥
(1 2(ω+1)L2η2) wˆk+1 wk 2.
− − ∥ − ∥
Finally, let us choose ω 0 such that ω = 2(ω+1)L2η2. If 2L2η2 < 1, then
≥
ω := 2L2η2 satisfiesω =2(ω+1)L2η2.Consequently,thelastestimateleads
1 2L2η2
to (77)−. □38 QuocTran-Dinh,NghiaNguyen-Trung
6.1.3 The Sublinear Convergence Rate Analysis of (RFBS2)
Now, we are ready to establish the convergence rates of (RFBS2).
Theorem 6.1 For Inclusion (NI), suppose that zer(Φ)= , F is L-Lipschitz
̸ ∅
continuous, there exists x⋆ zer(Φ) such that Fx Fx⋆,x x⋆ 0 for all
∈ ⟨ − − ⟩ ≥
x dom(F), and T is maximally monotone. Let (xk,yk) be generated by
(R∈ FBS2) using η (cid:0) 0,√2 1(cid:1) . Then, we have the fo{ llowing } statements.
∈
L−
(cid:0) (cid:1)
(a) The 1/√k best-iterate rate. The following bound holds:
O
K K
1 (cid:88) 1 (cid:88)(cid:2) (cid:3)
Fxk+ξk 2 Fxk+ξk 2+ω Fxk Fyk 1 2
−
K+1 ∥ ∥ ≤ K+1 ∥ ∥ ∥ − ∥
k=0 k=0 (78)
C x0 x⋆ 2
0
∥ − ∥ ,
≤ K+1
where ω :=
2L2η2
>0 and C :=
5L2η2+3
>0.
1 L2η2 0 3η2[1 (1+√2)Lη]
(cid:0) − (cid:1) −
(b) The 1/√k last-iterate rate. If Φ is additionally monotone, then
O
C x0 x⋆ 2
FxK+ξK 2 FxK+ξK 2+ω FxK FyK −1 2 0 ∥ − ∥ , (79)
∥ ∥ ≤∥ ∥ ∥ − ∥ ≤ K+1
whereω := 2L2η2 >0.Hence,weconcludethat xK FxK+ξK =
(cid:16) (cid:17) 1 −2L2η2 ∥Gη ∥≤∥ ∥
1 on the last-iterate xK.
O √K
Proof First, since 0 < η < √2 1, we have 1 (1+ √2)Lη > 0 and ω :=
L−
−
2L2η2 < 2. From (76), we have
1 2L2η2 3
−
Fxk+ξk 2 + ω Fxk Fyk 1 2 5L2η2+3 xk yk 2
∥ ∥ ∥ − − ∥ ≤ 3η2 ∥ − ∥
(cid:16) (cid:17)
+ 5L2η2+3 + 2L2 xk yk 1 2
5η2 3 ∥ − − ∥
5L2η2+3(cid:2)
xk yk 2+ xk yk 1
2(cid:3)
.
≤ 3η2 ∥ − ∥ ∥ − − ∥
Combining this estimate and (72), we get
T[1]
:= 3η2[1 5− L( 21 η+ 2+√ 32)Lη](cid:2) ∥Fxk+ξk ∥2+ω ∥Fxk −Fyk −1 ∥2(cid:3)
(cid:2) (cid:3)
(1 (1+√2)Lη) xk yk 2+ xk yk 1 2
−
≤ − ∥ − ∥ ∥ − ∥
.
k k+1
≤ V −V
Summing this inequality from k = 0 to k = K, and using x 1 = y 1 := x0,
− −
we get
3η2[1 5− L( 21 η+ 2+√ 32)Lη](cid:80)K k=0(cid:2) ∥Fxk+ξk ∥2+ω ∥Fxk −Fyk −1 ∥2(cid:3)
≤
V0 −VK+1
= x0 x⋆ 2,
0
≤ V ∥ − ∥
which leads to (78). Finally, combining (78) and (77), we obtain (79). □RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 39
Remark 6.1 Wecanmodify(RFBS2)tocaptureadaptiveparametersasyk :=
xk+β (xk xk 1)andxk+1 :=J (xk η Fyk),whereη :=β η forsome
k
−
− ηkT
−
k k k k −1
β > 0. Then, by imposing appropriate bounds on η , we can still prove the
k k
convergence of this variant by modifying the proof of Theorem 6.1. The best-
iterateandlast-iterateconvergenceratesof (RFBS2)underweakerconditions
(e.g., a weak-Minty solution and the co-hypomonotonicity) is still open.
6.2 New Golden Ratio Method for (NI)
The golden ratio (GR) method was proposed by Malitsky in [55] to solve
monotone(MVIP),wheretheratioparameterτ := √5+1,leadingtoaso-called
2
golden ratio. Here, we extend it to solve (NI) for the case F is monotone and
L-Lipschitz continuous, and T is maximally 3-cyclically monotone. Moreover,
we extend our analysis for any τ (1,1+√3).
∈
6.2.1 The GR Method for (NI)
Thegoldenratio(GR)methodforsolving(NI)ispresentedasfollows.Starting
from x0 dom(Φ), we set y 1 :=x0, and at each iteration k 0, we update
−
∈ ≥
(cid:40)
yk := τ 1xk+ 1yk 1,
−τ τ − (GR2+)
xk+1 :=J (yk ηFxk),
ηT
−
where J is the resolvent of ηT, τ >1 is given, and η (0, τ ).
ηT ∈ 2L
Let us denote w˘k := Fxk 1+ξk for ξk Txk. Then, we can rewrite the
−
∈
second line of (GR2+) as xk+1 := yk η(Fxk + ξk+1) = yk ηw˘k+1 for
− −
ξk+1 Txk+1. In this case, we have xk = yk 1 η(Fxk 1 +ξk), leading to
− −
∈ −
yk 1 =xk+ηw˘k. Combining this expression and the first line of (GR2+), we
−
have yk = τ 1xk + 1(xk +ηw˘k) = xk + ηw˘k. Consequently, we can rewrite
−τ τ τ
(GR2+) equivalently to the following one:
(cid:40)
yk := xk+ ηw˘k,
τ (80)
xk+1 := yk η(Fxk+ξk+1) = yk ηw˘k+1.
− −
If we eliminate yk, then we obtain
(cid:40) xk+1:=J (cid:0) xk η(cid:0) Fxk 1(Fxk 1+ξk)(cid:1)(cid:1) ,
ξk+1 :=
1η (T
xk
x−
k+1)
(cid:0) F− xkτ 1(−
Fxk
1+ξk)(cid:1)
.
(81)
η − − − τ −
This scheme is another interpretation of (GR2+).
6.2.2 Key Estimates for Convergence Analysis
The convergence of (GR2+) is established based on the following key lemma.
Lemma 6.3 For Inclusion (NI), suppose that zer(Φ) = , T is maximally
̸ ∅
3-cyclically monotone, and F is L-Lipschitz continuous. Let (xk,yk) be gen-
{ }
erated by (GR2+) with τ >1. Then, for any x⋆ zer(Φ), we have
∈
τ yk+1 x⋆ 2+(τ 1)(τ γ) xk+1 xk 2 τ yk x⋆ 2
∥ − ∥ − − ∥ − ∥ ≤ ∥ − ∥
+ (τ −1 γ)L2η2 ∥xk −xk −1 ∥2
−
(τ −1)(1 τ−τ2+τ) ∥xk+1 −yk ∥2 (82)
τ(τ 1) xk yk 2 2η(τ 1) Fxk Fx⋆,xk x⋆ .
− − ∥ − ∥ − − ⟨ − − ⟩40 QuocTran-Dinh,NghiaNguyen-Trung
Proof Since T is 3-cyclically monotone, for ξk+1 Txk+1, ξk Txk, and
∈ ∈
x⋆ Tx⋆, we have
∈
ξk+1,xk+1 x⋆ + ξ⋆,x⋆ xk + ξk,xk xk+1 0. (83)
⟨ − ⟩ ⟨ − ⟩ ⟨ − ⟩≥
From (80), we get ηξk+1 =yk xk+1 ηFxk and ηξk =yk 1 xk ηFxk 1.
− −
− − − −
Moreover,sincex⋆ zer(Φ),weobtainFx⋆+ξ⋆ =0,leadingtoηξ⋆ = ηFx⋆.
∈ −
Substituting these expressions into (83), we can show that
yk xk+1 ηFxk,xk+1 x⋆ + yk 1 xk ηFxk 1,xk xk+1
− −
⟨ − − − ⟩ ⟨ − − − ⟩
Fx⋆,x⋆ xk 0.
− ⟨ − ⟩≥
However, since yk := τ 1xk + 1yk 1 from the first line of (GR2+), we get
−τ τ −
yk 1 xk = τ(yk xk). Substituting this relation into the last inequality,
−
− −
rearranging the result, we obtain
yk xk+1,xk+1 x⋆ +τ xk yk,xk+1 xk
⟨ − − ⟩ ⟨ − − ⟩ (84)
+ η Fxk 1 Fxk,xk+1 xk η Fxk Fx⋆,xk x⋆ 0.
−
⟨ − − ⟩− ⟨ − − ⟩≥
By Young’s inequality and the L-Lipschitz continuity of F, for any γ >0, we
get the first line of the following:
2η Fxk 1 Fxk,xk+1 xk L2η2 xk xk 1 2+γ xk+1 xk 2,
⟨ − − − ⟩ ≤ γ ∥ − − ∥ ∥ − ∥
2 yk xk+1,xk+1 x⋆ = yk x⋆ 2 xk+1 x⋆ 2 xk+1 yk 2,
⟨ − − ⟩ ∥ − ∥ −∥ − ∥ −∥ − ∥
2 xk yk,xk+1 xk = xk+1 yk 2 xk yk 2 xk+1 xk 2.
⟨ − − ⟩ ∥ − ∥ −∥ − ∥ −∥ − ∥
Substitutingtheseexpressionsinto(84),andrearrangingtheresults,weobtain
xk+1 x⋆ 2 yk x⋆ 2+(τ 1) xk+1 yk 2 τ xk yk 2
∥ − ∥ ≤ ∥ − ∥ − ∥ − ∥ − ∥ − ∥
+ L2η2 xk xk 1 2 (τ γ) xk+1 xk 2 (85)
γ ∥ − − ∥ − − ∥ − ∥
2η Fxk Fx⋆,xk x⋆ .
− ⟨ − − ⟩
Now, using (τ 1)xk+1 =τyk+1 yk and τ(yk+1 yk)=(τ 1)(xk+1 yk)
− − − − −
from the first line of (GR2+), we can derive that
(τ 1)2 xk+1 x⋆ 2 = τ(τ 1) yk+1 x⋆ 2 (τ 1) yk x⋆ 2
− ∥ − ∥ − ∥ − ∥ − − ∥ − ∥
+ τ yk+1 yk 2
∥ − ∥
= τ(τ 1) yk+1 x⋆ 2 (τ 1) yk x⋆ 2
− ∥ − ∥ − − ∥ − ∥
+ (τ −1)2 xk+1 yk 2.
τ ∥ − ∥
Simplifyingthisexpressiontoget(τ 1) xk+1 x⋆ 2 =τ yk+1 x⋆ 2 yk
− ∥ − ∥ ∥ − ∥ −∥ −
x⋆ 2+ (τ −1) xk+1 yk 2. Combining it and (85), and rearranging the result,
we∥ obtainτ (8∥ 2). − ∥ □RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 41
6.2.3 The Sublinear Best-Iterate Convergence Rate Analysis
Now, we are ready to state the convergence of (GR2+).
Theorem 6.2 For Inclusion (NI), suppose that zer(Φ) = , T is maximally
̸ ∅
3-cyclically monotone, and F is L-Lipschitz continuous and satisfies Fx
⟨ −
Fx⋆,x x⋆ 0 for all x dom(F) and some x⋆ zer(Φ). Let (xk,yk) be
− ⟩≥ ∈ ∈ { }
generated by (GR2+). Then, the following statements hold.
(a) The best-iterate rate of GR. If 1<τ 1+√5 and η (cid:0) 0, τ (cid:1) , then
≤ 2 ∈ 2L
K K
1 (cid:88) Fxk+ξk 2 C 0 (cid:88) (τ 1)(cid:2) τ xk yk 2+φ xk xk 1 2(cid:3)
−
K+1 ∥ ∥ ≤ K+1 − ∥ − ∥ ∥ − ∥
k=0 k=0 (86)
C x0 x⋆ 2
0
∥ − ∥ ,
≤ K+1
where ξk ∈Txk, φ:= τ2 −24 τL2η2 >0, and C
0
:= (τ2(τ 42 L−22 ηL 2)2 ηη 22 () ττ
1)
>0.
− −
(b) The best-iterate rate of (GR2+). If we choose 1+√5 < τ < 1+√3
2
and 0<η < ψ , then
2L
K K
1 (cid:88) 1 (cid:88) (cid:2) (cid:3)
Fxk+ξk 2 (τ 1) ψ xk yk 2+κ xk yk 1 2
−
K+1 ∥ ∥ ≤ K+1 − ∥ − ∥ ∥ − ∥
k=0 k=0 (87)
Cˆ x0 x⋆ 2
0
∥ − ∥ ,
≤ K+1
where ψ := 2τ+ τ2 −τ2 >0, κ:= ψ2 −24 ψL2η2 , and Cˆ 0 := [ (ψ τ2 − 12 )L (ψ2 2η2( 42 Lτ 22 η−2ψ )η2 2)] ψτ.
− −
Proof First, to guarantee that 1+τ τ2 0 and τ > 1, we need to choose
− ≥
1 < τ √5+1. If 0 < η < τ , then by choosing γ := τ, we have ψ :=
≤ 2 2L 2
(τ −1)(τγ −γ2 −L2η2) = (τ −1)(τ2 −4L2η2) > 0. Using this relation and Fxk
γ 2τ ⟨ −
Fx⋆,xk −x⋆ ⟩≥0, if we define
Vk
:=τ ∥yk −x⋆ ∥2+ τ(τ 2−1) ∥xk −xk −1 ∥2 ≥0,
then we can deduce from (82) that
ψ xk xk 1 2 τ(τ 1) xk yk 2. (88)
k+1 k −
V ≤ V − ·∥ − ∥ − − ∥ − ∥
Next, using yk xk = ηw˘k and w˘k =Fxk 1+ξk, by Young’s inequality, we
− τ −
can prove that
wk 2= Fxk+ξk 2
∥ ∥ ∥ ∥
(cid:16) (cid:17) (cid:16) (cid:17)
≤
1+ L2η2ψ (τ
τ 1)
∥Fxk −Fxk −1 ∥2+ 1+ L2η2 ψ( ττ −1) ∥w˘k ∥2
(cid:16) − (cid:17) (cid:16) (cid:17) (89)
≤
1+ L2η2ψ (τ
τ 1)
L2 ∥xk −xk −1 ∥2+ 1+ L2η2 ψ( ττ −1) τ η22 ∥xk −yk ∥2
−
= L2η ψ2 η( 2τ (− τ1) 1+ )ψτ (cid:2) ψ ·∥xk −xk −1 ∥2+τ(τ −1) ∥xk −yk ∥2(cid:3) .
−42 QuocTran-Dinh,NghiaNguyen-Trung
Combining this estimate and (88), and noting that 0, we can show that
k
V ≥
(cid:80)k l=0∥wl ∥2
≤
L2η ψ2 η( 2τ (− τ1) 1+ )ψτ (cid:80)k l=0(cid:2) ψ ·∥xl −xl −1 ∥2+τ(τ −1) ∥xl −yl ∥2(cid:3)
−
≤
L2η ψ2 η( 2τ (− τ1) 1+ )ψτ [ V0 −Vk+1]
≤
L2η ψ2 η( 2τ (− τ1) 1+ )ψτ ·V0
− −
= (τ2(τ 42 L−22 ηL )2 ηη 22 () ττ
1)
·∥x0 −x⋆ ∥2,
− −
which is exactly (86), where we have used
V0
:= τ ∥y0 −x⋆ ∥2 + τ(τ 2−1) ∥x0
−
x 1 2 =τ x0 x⋆ 2 due to x 1 =y0 =x0.
− −
∥ ∥ − ∥
Next,if1.6180 1+√5 <τ <1+√3 2.7321,thenwehaveτ2 τ 1>0
≈ 2 ≈ − −
andψ :=τ 2(τ2 −τ −1) >0.Inthiscase,using xk+1 yk 2 2 xk+1 xk 2+
− τ ∥ − ∥ ≤ ∥ − ∥
2 yk xk 2 and Fxk Fx⋆,xk x⋆ 0 into (82), rearranging the result,
a∥
nd
u− sing∥
γ :=
ψ,⟨
we
ca−
n derive
t−
hat
⟩ ≥
2
τ ∥yk+1 −x⋆ ∥2+ψ(τ 2−1) ∥xk+1 −xk ∥2 ≤τ ∥yk −x⋆ ∥2+ ψ(τ 2−1) ∥xk −xk −1 ∥2
(90)
−
ψ(τ −1) ∥xk −yk ∥2
−
(τ −1)(ψ 22 ψ−4L2η2) ∥xk −xk −1 ∥2.
xSi km 1ila 2r +to ψthe xkproo yf kof 2( (cid:3)8 .9 C), ow me bh ina iv ne g∥ tw hk is∥2 in≤ equψ a2τ l( i2 ψ t− y2 −2 aL 4 n2 Lη d22 η( (22 9)τ η 02 2 )− ψ ,ψ w2) it(cid:2) hψ2 s− a24 mψL e2η a2 r∥ gx uk -−
−
ment∥ as in t∥ he p− roof∥ of (86), we obtain (87). □
7 Numerical Experiments
We provide an extensive experiment set to verify the algorithms we studied
above for both equations and inclusions under new assumptions and parame-
ters. All the algorithms are implemented in Python running on a single node
of a Linux server (called Longleaf) with the configuration: AMD EPYC 7713
64-Core Processor, 512KB cache, and 4GB RAM.
7.1 Quadratic Minimax Problems
Our first example is the following quadratic minimax problem:
(cid:110) (cid:111)
min max (u,v)=f(u)+ (x,y) g(v) , (91)
u Rp1v Rp2 L H −
∈ ∈
w anh der Be H(x R, py 2) ×p:=
2
a1 2 ru e⊤ sA ymu m+ eb t⊤ riu c+ mu a⊤ trL icv e−
s,
1 2 bv ⊤B Rpv 1−
,
cc ⊤v Rsu p2ch art eha gt ivA en∈ vR ecp t1 o× rp s1
,
andL
∈Rp1×p2 isagivenmatrix.Thefuncti∈
onsf
and∈
g areaddedtopossibly
∈
handle constraints or regularizers associated with u and v, respectively.
First, we denote x := [u,v] Rp for p := p +p , which is the concate-
1 2
nation of the primal variable u∈ Rp1 and its dual variable v Rp2. Next,
we define F := (cid:2) [A,L],[ L ,B]∈(cid:3) as the KKT matrix in Rp p∈ constructed
⊤ ×
from the four blocks A,L− , L , and B, and f := [b,c] Rp. The operator
⊤
F : Rp Rp is then define− d as Fx := Fx+f. When f a∈ nd g are presented,
→
we denote by T := [∂f,∂g] the maximally monotone operator constructed
from the subdifferentials of f and g. Then, the optimality condition of (91)
becomes 0 Fx+Tx covered by (NI). If f and g are absent from (91), then
∈
its optimality condition reduces to Fx=0 as a special case of (NE).RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 43
7.1.1 The unconstrained case as (NE)
First, we consider the unconstrained minimax problem as an instance of (91)
(or in particular, of (NE)), where f = 0 and g = 0. We aim to test different
variants covered by (GEG).
Data generation. In what follows, all random matrices and vectors are
generated randomly from the standard normal distribution. We generate the
matrixA=QDQ ,whereQisanorthonormalmatrixobtainedfromtheQR
⊤
factorization of a random matrix, and D = diag(d ,...,d ) is the diagonal
1 p1
matrix formed from d ,...,d randomly generated and then clipped by a
1 p1
lower bound d, i.e. d := max d ,d . The matrix B is also generated by the
j j
{ }
same way. The matrix L and vectors b and c are randomly generated.
Algorithms.Weimplement5differentinstancesof (GEG)byusingdifferent
choices of uk. These variants consist of EG with uk := Fxk and β = 1, EG+
with uk := Fxk and β := 0.5, PEG with uk := Fyk 1 and β = 1, PEG+
−
with uk :=Fyk 1 and β :=0.5, and GEG with uk =1.35Fxk 0.25Fyk 1
− −
− −
0.1Fxk 1 and β =0.95. The reason we choose different β in these algorithms
−
is to avoid identical performance between EG and EG+, PEG and PEG+.
For GEG, we performed an experiment and found at least one possible pair
(α ,α ):=(1.35, 0.25) that works relatively well.
1 2
−
Parameters. We choose the stepsize η in each algorithm based on a grid
search and then tuned it manually to obtain the best possible performance.
We run all algorithms up to 5000 iterations (or 10000 iterations for PEG and
PEG+duetooneevaluationofF periteration)on10problemsinstances,and
report the average of the relative residual norm Fxk / Fx0 . The starting
∥ ∥ ∥ ∥
point x0 is always chosen as x0 =0.01 ones(p).
·
Experiment setup.Weperformfourdifferentexperiments.InExperiment 1
and Experiment 2, we choose d=0.1 (monotone), and run the five algorithms
on10probleminstancesforeachcase:p=1000andp=2000,respectively.In
Experiment 3 andExperiment 4,wechoose d= 0.1(possiblynonmonotone)
−
and run the five algorithms on 10 problem instances for each case: p = 1000
and p=2000, respectively. Then, we report the mean of the relative operator
norm Fxk / Fx0 against the number of iterations k and the number of
∥ ∥ ∥ ∥
operator evaluations Fxk, respectively over the 10 instances.
Results. The numerical results after 5000 iterations and after 10000 oper-
ator evaluations are reported in Figures 2 and 3, respectively.
From Figures 2 and 3, we observe that GEG outperforms the other algo-
rithms across all experiments, particularly in larger instances (p=2000). The
performancesoftheremainingalgorithmsareconsistentacrossallexperiments
andcanberankedintermsofthenumberofiterationsasfollows:EG>EG+
>PEG>PEG+.Intermsofoperatorevaluations,PEGslightlyoutperforms
EG, while the performances of PEG+ and EG+ become more comparable.
This can be attributed to PEG/PEG+ saving one operator evaluation per it-
eration. Interestingly, GEG continues to deliver the best overall performance
in terms of the number of operator evaluations and is comparable to PEG in
this regard.44 QuocTran-Dinh,NghiaNguyen-Trung
Experiment 1: p = 1000 Experiment 2: p = 2000
100
101
102
103
104
105
106
107 108
EG 109 EG
1010
EG+ EG+
1012 PEG 1011 PEG
1014
P GE EG G+ 1013 P GE EG G+
0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000
Number of iterations Number of iterations
Experiment 1: p = 1000 Experiment 2: p = 2000
11 00 20 E EG
G+
101 E EG
G+
PEG 103 PEG
104 PEG+ PEG+
GEG 105 GEG
106
107 108
109
1010
1012 1011
1014
1013
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Number of operator evaluations Number of operator evaluations
Fig.2 Comparisonofthefivevariantsof (GEG)forsolving(91)asaspecialcaseof (NE).
Twoexperiments,eachison10probleminstancesofthemonotonesetting.
Experiment 3: p = 1000 Experiment 4: p = 2000
101 101
103 103
105 105
107 107
109 EG 109 EG
EG+ EG+
1011 PEG 1011 PEG
1013 PEG+ 1013 PEG+
GEG GEG
0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000
Number of iterations Number of iterations
Experiment 3: p = 1000 Experiment 4: p = 2000
101 E EG
G+
101 E EG
G+
103 PEG 103 PEG
PEG+ PEG+
105 GEG 105 GEG
107 107
109 109
1011 1011
1013 1013
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Number of operator evaluations Number of operator evaluations
Fig.3 Comparisonofthefivevariantsof (GEG)forsolving(91)asaspecialcaseof (NE).
Twoexperiments,eachison10probleminstancesofthepossiblynonmonotonesetting.
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleRRevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 45
7.1.2 The constrained case as (NI)
Next, we consider the minimax problem (91) with the constraints u ∆
a Rn pd 2,v res∈ pe∆ ctv i2 v, elw y.h Ter oe h∆ anp1 dla en td hi∆
s
cp2 ona sr te rat inh te
,
s inta (n 9d 1a )r ,d ws eim up sele fxe (us )in =R δp ∆∈ 1 p1a (n up d1
)
and g(v) = δ (v), where δ is the indicator function of a closed convex
set . The op∆ tp im2 ality conditiX on of (91) becomes 0 Fx+Tx, where T :=
[∂δ X ,∂δ ] from Rp to 2Rp is a maximally monoto∈ ne operator.
∆p1 ∆p2
Experiment setup.Weconsidertwosettingsof (91):Test 1 withmonotone
operatorF andTest 2 with[possibly]nonmonotoneoperatorF.InTest 1,we
generate data as described in unconstrained case. In Test 2, data is generated
very similarly, except that the diagonal elements d of D are sampled from a
j
uniformdistributionU( 10,10).Ineachsetting,weconsidertwoexperiments
−
as before corresponding to p=1000 and p=2000, respectively. Then, we re-
wpo hr et rethem :=ea ηno 1f (t xher Jelat (iv xeop ηe Fr xat )o )r isno dr em fin∥
∥
eG
G
dη ηx x ink 0 ∥∥ (4o )v .er10probleminstances,
η − ηT
G − −
Algorithmsandparameters.InTest1,weimplementthreedifferentvariants
of (GEG2): EG2 with uk = Fxk and β = 1, EG2+ with uk = Fxk and
β = 0.5, GEG2 with uk = 1.35Fxk 0.45Fyk 1+0.1Fxk 1 and β = 0.975,
− −
−
(RFBS2), and two variants of (GR2+): GR2 with τ = 1+√5 and GR2+ with
(cid:16) (cid:17) 2
τ = 1 1+√5 +1+√3 = 3+2√3+√5. Next, in Test 2, we test 3 variants of
2 2 4
(GFBFS2): FBFS2 with uk = Fxk and β = 1, FBFS2+ with uk = Fxk and
β = 0.25, and GFBFS2 with uk = 1.45Fxk 0.45Fyk 1 and β = 1. The
−
−
stepsize of each algorithm is obtained from a grid search and then fine tuned
toobtainthebestpossibleperformance.Thestartingpointsarealwayschosen
as x0 :=0.01 ones(p).
·
Results.ThenumericalresultsofTest1 arereportedinFigure4.Aswecan
see from Figure 4, GEG2 performs better than EG2 and outperforms all the
remainingalgorithmsinbothexperiments.Wealsoseethatthethreevariants
of (GEG2) outperform RFBS2 when they reach the solution accuracy in the
rangeof10 12 to10 14 after500iterationscomparedto10 8 ofRFBS2.How-
− − −
ever,ascompensation,RFBS2savesoneoperatorevaluationandoneresolvent
evaluation per iteration compared to the three variants of (GEG2). Finally,
the performance of (GR2+) has been improved by extending the value of τ
from τ = 1+√5 to τ = 1(1+√5 +1+√3).
2 2 2
ThenumericalresultsofTest 2 arereportedinFigure5.Thisfigureshows
thatGFBFS2providesabetterperformancethanFBFS2andFBFS2+.How-
ever, their performances are comparable and become more stable in a higher
dimensional space.
7.2 Bilinear Matrix Games
Our second example is the following classical bilinear matrix game problem:
(cid:8) (cid:9)
minmax (x,y):= Lu,v , (92)
u v H ⟨ ⟩
∈U ∈V
where L Rp2×p1, Rp1 and Rp2 are closed convex sets.
∈ U ⊂ V ⊂46 QuocTran-Dinh,NghiaNguyen-Trung
Experiment 1: p = 1000 Experiment 2: p = 2000
100 100
102 102
104 104
106 106
108 EG2 108 EG2
1010 EG2+ 1010 EG2+
GEG2 GEG2
1012 RFBS2 1012 RFBS2
1014 G GR R2
2+
1014 G GR R2
2+
0 100 200 300 400 500 0 100 200 300 400 500
Number of iterations Number of iterations
Experiment 1: p = 1000 Experiment 2: p = 2000
100 EG2 100 EG2
102 EG2+ 102 EG2+
GEG2 GEG2
104 RFBS2 104 RFBS2
106 GR2 106 GR2
GR2+ GR2+
108 108
1010 1010
1012 1012
1014 1014
0 200 400 600 800 1000 0 200 400 600 800 1000
Number of operator F evaluations Number of operator F evaluations
Fig.4 Comparisonof6differentalgorithmsforsolving(NI)inTest1.Theplotrevealsthe
meanof10probleminstances.
Experiment 3: p = 1000 Experiment 4: p = 2000
100 FBFS2 100 FBFS2
FBFS2+ FBFS2+
GFBFS2 GFBFS2
101 101
102 102
0 100 200 300 400 500 0 100 200 300 400 500
Number of iterations Number of iterations
Fig.5 Comparisonof3differentalgorithmsforsolving(NI)inTest2.Theplotrevealsthe
meanof10probleminstances.
The optimality condition 0 Fx+Tx of (92) is a special case of (NI),
∈
where we define x := [u,v], Fx := [L v; Lu], Tx := [∂δ ,∂δ ], and δ is
⊤
− U V X
the indicator function of . Clearly, F is skew-symmetric, and thus mono-
X
tone. However, it is well-known that (FBS) for solving this problem may not
converge. Therefore, it is important to apply extragradient-type methods.
Models and data.Wefollowthesuggestionsin[66]togeneratedataforour
test.WeconsiderasymmetricmatrixLofsizeq q,whereq :=p =p ,with
1 2
×
two options:
– L belongs to the first family with L
ij
:=(cid:0)i 2+ qj −11(cid:1)α for 1 ≤i,j ≤q.
– L belongs to the second family with L ij :=(cid:0)− |i 2− qj |+ 11(cid:1)α for 1 ≤i,j ≤q.
−
0x
/kx
mron
rotarepo
evitaleR
0x
/kx
mron
rotarepo
evitaleR
0x
/kx
mron
rotarepo
evitaleR
0x
/kx
mron
rotarepo
evitaleR
0x
/kx
mron
rotarepo
evitaleR
0x
/kx
mron
rotarepo
evitaleRRevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 47
Experiment 1 Experiment 2 Experiment 3
100 EG 100 EG 102 EG+ EG+
PEG PEG
105 101 PEG+ 101 PEG+
GEG GEG
108
102
1011 102
EG
1014 EG+ 103 PEG 103
1017 P GE EG G+
104
0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000
Number of iterations Number of iterations Number of iterations
Experiment 1 Experiment 2 Experiment 3
100 EG 100 EG 102 EG+ EG+
PEG PEG
105 101 PEG+ 101 PEG+
GEG GEG
108
102
1011 102
EG
1014 EG+ 103 PEG 103
1017 P GE EG G+
104
0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000 0 2000 4000 6000 8000 10000
Number of operator evaluations Number of operator evaluations Number of operator evaluations
Fig. 6 The performance of 5 variants of (GEG) for solving Fˆx = 0 resulting from (92)
usingthreedifferentproblems.
Here, α>0 is a given parameter.
In addition to the above two artificial instances, we also follow [65] and
consider the Policeman vs. Burglar problem as follows. There are q houses in
a city, the i-th house is with the wealth w . Every evening, Burglar chooses
i
a house i to attack, and Policeman chooses his post near a house j, 1
≤
i,j q. After Burglary starts, Policeman becomes aware where it happens,
≤
and his probability to catch Burglar is exp θ dist(i,j) , where dist(i,j)
{− · }
is the distance between houses i and j. On the other hand, Burglar seeks
to maximize his expected profit w (1 exp θ dist(i,j) ), the interest of
i
− {− · }
Policeman is completely opposite. This leads to the third family of symmetric
matrices L with L = w (1 exp θ dist(i,j) ) for 1 i,j q. The set
ij i
− {− · } ≤ ≤ U
and are chosen to be the standard simplexes in the respective spaces.
V
In the above three problems, we fix q := 500. Moreover, in the first two
problems, we also take α = 1. In the Burglar and Policeman problem, we
choose dist(i,j) := i j , θ := 0.005, and the vector of wealth w Rq is
| − | ∈
generated randomly from a standard normal distribution and then take the
absolute value so that it is nonnegative.
Experiment with 5 variants of (GEG).Wefirstreformulatetheoptimality
0 Fx+Tx of (92) into (NE) by using Tseng’s FBFS operator as Fˆx :=
∈
x J (x λFx) λ(Fx F(J (x λFx))) for any λ>0. It is clear that
λT λT
so− lving 0 − Fx⋆ +− Fx⋆ is e− quivalent t− o solving Fˆx⋆ = 0. We implement our
(GEG) wi∈ th different choices of uk as in Subsection 7.1 to solve Fˆx⋆ =0 with
λ := 0.5, except that now GEG uses uk = 1.1Fxk 0.1Fyk 1 and β = 0.1.
−
−
The starting point for all experiments is always chosen as x0 :=0.5 ones(p).
·
Foreachalgorithm,thestepsizeη ischosenbyagridsearchsothatitachieves
thebestpossibleperformance.ThenumericalresultsarereportedinFigure6.
As we can observe from Figure 6, our GEG outperforms all the remain-
ing algorithms on all 3 problems in both two criteria, namely the number
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR48 QuocTran-Dinh,NghiaNguyen-Trung
Experiment 4 Experiment 5 Experiment 6
100 EG2 100 EG2 100 EG2
EG2+ EG2+ EG2+
101 G RFE BG S2 2 101 G RFE BG S2 2 101 G RFE BG S2 2
GR2 GR2 GR2 GR2+ GR2+ 102 GR2+
102 102
103
103 103
104
0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000 0 1000 2000 3000 4000 5000
Number of iterations Number of iterations Number of iterations
Fig. 7 The performance of 6 different algorithms for solving 0∈Fx+Tx resulting from
(92)usingthreedifferentproblems.
of iterations and the number of operator evaluations. Among the remaining
algorithms, while EG+ provides better performances than EG, both still out-
perform PEG and PEG+, which have comparable performances in all experi-
ments.ThedifferencesbetweentheperformancesofPEG/PEG+andtheother
algorithmsarediminishedasexpectedwhenwereportthenumberofoperator
evaluations, since PEG and PEG+, each saves one operator evaluation per
iteration compared to EG, EG+, or our new GEG.
Experiment with different variants of (GEG2), (RFBS2), and (GR2+)
Now, we will test different variants of (GEG2), (RFBS2), and (GR2+) de-
scribed in Subsection 7.1 for solving (92) as a special case of (NI). Again, for
eachalgorithm,thestepsizeηisobtainedfromagridsearchandthentuned(if
necessary) so that it produces the best possible performance. Our numerical
results of this test are depicted in Figure 7.
Figure7showsthatthethreevariantsof (GEG2)and(RFBS2)havecom-
parable performance in all three problems, and they also have a better per-
formance than GR2 and GR2+. Unlike the previous tests, in this particular
example, we do not see a clear improvement of GEG2 over EG2, but it is
betterthanEG2+.Wecanalsoverifytheeffectivenessofextendingtherange
of τ in (GR2+) from this set of experiments when GR2+ with a larger value
of τ produces a slightly better performance than GR2.
7.3 Regularized Logistic Regression with Ambiguous Features
Finally,weconsiderastandardregularizedlogisticregressionmodelassociated
with a given dataset (Xˆ ,y ) N , where Xˆ is an i.i.d. sample of a feature
{ i i }i=1 i
vector and y 0,1 is the corresponding label of Xˆ . Unfortunately, Xˆ is
i i i
∈ { }
ambiguous,i.e.itbelongstooneofmpossibleexamples X m .Sincewedo
{ ij }j=1
notknowwhichXˆ toevaluatealoss,weconsidertheworst-caselossf (w):=
i i
max ℓ( X ,w ,y )computedfrommexamples,whereℓ(t,s):=log(1+
1 j m ij i
≤ ≤ ⟨ ⟩
exp(t)) st is the standard logistic loss.
Usin−
gthefactthatmax ℓ ()=max
(cid:80)m
v ℓ (),where∆ is
the standard simplex in
Rm1 ,≤ wj ≤ em canj ·
model
thv is∈∆ rem gulaj r= iz1 edj lj og·
istic
regressm
ion
into the following minimax problem:
(cid:110) 1 (cid:88)N (cid:88)m (cid:111)
min max (w,v):= v ℓ( X ,w ,y )+γR(w) δ (v) , (93)
w Rdv Rm L N
j
⟨
ij
⟩
i
−
∆m
∈ ∈ i=1j=1
0x
/kx
mron
rotarepo
evitaleR
0x
/kx
mron
rotarepo
evitaleR
0x
/kx
mron
rotarepo
evitaleRRevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 49
where R(w) := w is an ℓ -norm regularizer used to introduce sparsity to
∥ ∥1 1
w, γ > 0 is a regularization parameter, and δ is the indicator of ∆ that
∆m m
handles the constraint v ∆ .
m
∈
Let us define x:=[w;v], T(x):=[γ∂R(w);∂δ (v)], and
∆m
(cid:104)(cid:88)m (cid:105)
F (x):= v ℓ( X ,w ,y )X ; ℓ( X ,w ,y ); ; ℓ( X ,w ,y ) ,
i j ′ ij i ij i1 i im i
⟨ ⟩ − ⟨ ⟩ ··· − ⟨ ⟩
j=1
where ℓ(t,s) = exp(t) s. Then, the optimality condition of (93) can be
′ 1+exp(t) −
written as 0 Fx+Tx, where F := 1 (cid:80)N F .
∈ N i=1 i
Data generation. We use the following two real datasets for the experi-
ments: w7a (300 features and 24,692 data points) and duke breast-cancer
(7,129 features and 44 data points) downloaded from LIBSVM [20]. We first
normalizethefeaturevectorXˆ suchthateachsamplehasunitnorm,andadd
i
acolumnofallonestoaddressthebiasterm.Togenerateambiguousfeatures,
wetakethenominalfeaturevectorXˆ andaddarandomnoisegeneratedfrom
i
astandardnormaldistribution.Inourtest,wechooseγ :=5 10 4 andm:=5.
−
·
The starting point for every experiment is chosen as x0 :=0.5 ones(p).
·
Experiment with 5 variants of (GEG).Wefirstreformulatetheoptimality
0 Fx+Tx of (93) into (NE) by using Tseng’s FBFS operator Fˆ as stated
∈
in Subsection 7.2. We implement our (GEG) with different choices of uk as
in Subsection 7.1 to solve Fˆx⋆ = 0, except that GEG now uses the direction
uk =0.7Fxk+0.3Fyk 1 andβ =1.Asbefore,foreachalgorithm,thestepsize
−
ηistunedmanuallytoobtainthebestpossibleperformance.Then,therelative
norm ∥F Fˆ ˆx xk 0∥ against the number of iterations k of 5 variants of (GEG) are
reporte∥d in∥Figure 8.
From Figure 8, we can see that for the w7a dataset, our GEG outperforms
all the remaining variants in both the number of iterations and the number
of operator evaluations. If we consider the number of iterations, then the re-
maining variants can be ranked as EG > EG+ > PEG > PEG+ based on
their performance, which agrees with the observation in Subsection 7.1. For
theduke breast-cancerdataset,wecanseethattherankingofthefirstfour
classical variants of (GEG) based on their performances are still consistent
with those in the w7a. The GEG variant still produces the best performance,
though there is no significant difference from EG. If we consider the number
of operator evaluations, then PEG becomes the second-best algorithm, which
outperforms EG in the experiment with w7a dataset and is comparable to
EG+ in the experiment with the duke breast-cancer. At the same time,
PEG+ also provides a better performance, though still the humblest, thanks
to the saving of one operator evaluation per iteration. It is worth noting that
our GEG still provides the best performance when the number of operator
evaluations is taken into account.
Experiment with different variants of (GEG2), (RFBS2), and (GR2+)
Next, we test different variants of (GEG2), (RFBS2), and (GR2+) described
inSubsection7.1forsolving(93)asaspecialcaseof (NI),butnowGEG2uses50 QuocTran-Dinh,NghiaNguyen-Trung
The w7a Dataset: (n, p) = (22646, 7130) The duke breast-cancer Dataset: (n, p) = (44, 7129)
100 EG 100 EG
EG+ EG+
101 P PE EG
G+
P PE EG
G+
GEG 101 GEG
102
103
102
104
0 250 500 750 1000 1250 1500 1750 2000 0 100 200 300 400 500
Number of iterations Number of iterations
The w7a Dataset: (n, p) = (22646, 7130) The duke breast-cancer Dataset: (n, p) = (44, 7129)
100 EG 100 EG
EG+ EG+
101 P PE EG
G+
P PE EG
G+
GEG 101 GEG
102
103
102
104
0 500 1000 1500 2000 2500 3000 3500 4000 0 200 400 600 800 1000
Number of operator evaluations Number of operator evaluations
Fig. 8 The performance of 5 variants of (GEG) for solving Fˆx = 0 resulting from (93)
usingthetwodatasets:w7aandduke breast-cancerfromLibSVM.
thedirectionuk =0.7Fxk+0.3Fyk 1 andβ =1.Now,foreachalgorithm,the
−
stepsizeη istunedmanuallysothatitprovidesthebestpossibleperformance.
T prh ee sec no tr er des ip no Fn id gi un rg er 9e .lative norm ∥ ∥G Gη ηx xk 0 ∥∥ against the number of iterations is
The w7a Dataset: (n, p) = (22646, 300) The duke breast-cancer dataset: (n, p) = (44, 7129)
100 EG2 100 EG2
EG2+ EG2+
GEG2 GEG2
101 RFBS2 RFBS2
GR2 GR2
GR2+ GR2+
102
101
103
0 20 40 60 80 100 0 20 40 60 80 100
Number of iterations Number of iterations
Fig. 9 The performance of 5 variants of different variants of (GEG2), (RFBS2), and
(GR2+) for solving 0 ∈ Fx+Tx resulting from (93) using the two datasets: w7a and
duke breast-cancerfromLibSVM.
We can see from Figure 9 that for the w7a dataset, GEG2 highly outper-
forms all its competitors and reaches the accuracy of 10 4 in only 100 iter-
−
ations, which is pretty fast. EG2, EG2+, and RFBS2 are the second-fastest
algorithms, and their performance is comparable after the first 40 iterations.
Again, we still see an improvement of GR2+ compared to GR2, which il-
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR
0x
/kx
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR
0xF/kxF
mron
rotarepo
evitaleR
0x
/kx
mron
rotarepo
evitaleRRevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 51
lustrates the effectiveness of extending the range of τ. Finally, for the duke
breast-cancer dataset, the difference between the performances of the algo-
rithms becomes less clear, but GEG2 still shows the best performance overall.
Dataavailability.Thispaperusesbothsyntheticandpubliclyavailabledata.
The procedure of generating synthetic data is clearly described in the paper.
The publicly available data is from LIBSVM [20].
Acknowledgements. This work is partially supported by the National Sci-
ence Foundation (NSF), grant no. NSF-RTG DMS-2134107 and the Office of
Naval Research (ONR), grant No. N00014-20-1-2088 (2020-2023) and grant
No. N00014-23-1-2588 (2023-2026).
A Technical Lemmas
ThefollowingtechnicallemmaswillbeusedtoproveTheorem3.1andTheorem3.2.
Lemma A.1 Suppose that Lρ≤∆ for ∆ defined by (14). Then, η and η¯defined by (15)
are well-defined and η≥0 and η¯≥0.
( (a b)) II ff ww ee cc hh oo oo ss ee 0 r+<
r
κ2β ≤≤ βr+ <r κ2 1,, tt hh ee nn ηη 12 ≤≤ ηη 21 ≤≤ ηη ¯¯ 21 ≤≤ ηη ¯¯ 12 ,, ll ee aa dd ii nn gg tt oo ηη ≤≤ ηη ¯¯ ..
(c) If κ2=0, then we can choose β=1, leading to η=η 1≤η¯=η¯1.
Moreover, in all cases above, C1 and C2 defined by (16) are nonnegative.
Proof First,givenC1 andC2 in(16),toh √aveC1 ≥0,weneedtochooseη √suchthat(1+
r)Lη2−βη+4µρ<0.Thisimpliesη 1:= β− β 22 (− 11 +6 r( )1 L+r)µLρ ≤η≤η¯1:= β+ β 22 (− 11 +6 r( )1 L+r)µLρ,
providedthatLρ≤
β2
=
β2(r+2κ2).Thischoiceofη
leadstothefirstlineof (15).
16(1+r)µ 16(1+r)r
Next,toguaranteeC2≥ √0,weneedtochooseηsuchthatκ2αLη2 √−(1−β)η+2(1−µ)ρ≤
0.Thisimpliesη 2:= 1−β− (1−β 2α)2 κ− 28 Lα(1−µ)κ2Lρ ≤η≤η¯2:= 1−β+ (1−β 2α)2 κ− 28 Lα(1−µ)κ2Lρ,
providedthatLρ≤
(1−β)2
=
(1−β)2(r+2κ2)r.Thisη
leadstothesecondlineof (15).
8α(1−µ)κ2 16(1+r)κ2
2
Now,underthechoiceofαandµabove,wecanshowthatη≤η¯.Indeed,thisisobvious
whenβ=1,proving(c).
Forβ∈(0,1),wedefineφ1(η):=(1+r)Lη2−βη+4µρandφ2(η):=κ2αLη2−(1−
β)η+2(1−µ)ρastwoquadraticfunctionsofη.
Case 1:Ifwechoose0<β≤ r+r κ2,thenwecanevaluateφ2(η 1)as
√ √
φ2(η 1)=κ2αL(cid:16)β− β 22 (− 11 +6 r( )1 L+r)µLρ(cid:17)2 −(1−β)β− β 22 (− 11 +6 r( )1 L+r)µLρ +2(1−µ)ρ
√
= [(1−β)r−κ2β][ β2(r+2κ2)2−16r(1+r)(r+2κ2)Lρ−β(r+2κ2)]
2r(1+r)(r+2κ2)L
≤0,
providedthatLρ≤∆≤
β2(r+2κ2).Thisshowsthatη
≤η .
16r(1+r) 2 1
Alternatively,wecanevaluateφ2(η¯1)as
√ √
φ2(η¯1)=κ2αL(cid:16)β+ β 22 (− 11 +6 r( )1 L+r)µLρ(cid:17)2 −(1−β)β+ β 22 (− 11 +6 r( )1 L+r)µLρ +2(1−µ)ρ
√
= [κ2β−(1−β)r][ β2(r+2κ2)2−16r(1+r)(r+2κ2)Lρ+β(r+2κ2)]
2r(1+r)(r+2κ2)L
≤0,
providedthatLρ≤∆≤ β 12 6( rr (+ 1+2κ r2 )).Thisshowsthatη¯1≤η¯2.52 QuocTran-Dinh,NghiaNguyen-Trung
Combiningbothcases,wehaveη 2≤η 1≤η¯1≤η¯2,andthusη≤η¯.Thisproves(a).
Case 2.Ifwechoose r+r
κ2
≤β<1,thenwecanevaluateφ1(η 2)as
√ √
φ1(η 2)=(1+r)L(cid:16)1−β− (1−β 2α)2 κ− 28 Lα(1−µ)κ2Lρ(cid:17)2 −β1−β− (1−β 2α)2 κ− 28 Lα(1−µ)κ2Lρ +4µρ
(cid:113)
=
[κ2β−r(1−β)][ r2(1−β)2(r+2κ2)2−16r(1+r)(r+2κ2)κ2 2Lρ−r(1−β)(r+2κ2)]
2κ2 2(1+r)(r+2κ2)L
≤0,
providedthatLρ≤∆≤
r(1−β)2(r+2κ2).Thisshowsthatη
≤η .
16(1+r)κ2 1 2
2
Similarly,wecanevaluateφ1(η¯2)as
√ √
φ1(η¯2)=(1+r)L(cid:16)1−β+ (1−β 2α)2 κ− 28 Lα(1−µ)κ2Lρ(cid:17)2 −β1−β+ (1−β 2α)2 κ− 28 Lα(1−µ)κ2Lρ +4µρ
(cid:113)
=
[r(1−β)−κ2β][ r2(1−β)2(r+2κ2)2−16r(1+r)(r+2κ2)κ2 2Lρ+r(1−β)(r+2κ2)]
2κ2 2(1+r)(r+2κ2)L
≤0,
providedthatLρ≤∆≤ r(1 1− 6β (1)2 +( rr )+ κ2 2κ2).Thisshowsthatη¯2≤η¯1.
2
Combining both cases, we can show that η
1
≤ η
2
≤ η¯2 ≤ η¯1, and thus η ≤ η¯. This
proves(b). □
Lemma A.2 Let η and η¯ be defined by (15), and ηˆ and η¯ˆ be defined by (21). Then, we
have [η,η¯]∩[ηˆ,η¯ˆ]̸=∅.
√ √
1− 1−16(1+r)Lρ 1+ 1−16(1+r)Lρ
Proof Whenβ=1,ηandη¯in(15)becomeη= andη¯= ,
2(1+r)L 2(1+r)L
respectively.Then,weconsiderthetwofollowingfunctions
√ √
ϕ1(t):= 1− 1 2− t16ρt and ϕ2(t):= 1+ 1 2− t16ρt, for t∈(cid:0) 0, 161 ρ(cid:1) .
√
Ontheonehand,wecanseethatϕ′(t)= 1−8ρt− ( √1−8ρt)2−64ρ2t2 ≥0forallt∈(cid:0) 0, 1 (cid:1) ,
1 2t2 1−16ρt 16ρ
which implies that ϕ1(t) is an increasing function on (cid:0) 0, 161 ρ(cid:1) . On the other hand, it is
obvious that ϕ2(t) is a decreasing function on (cid:0) 0, 161 ρ(cid:1) as it is the product of two positive
decreasing functions. Thus if 1+r ≤ m, by substituting t := mL and t := (1+r)L into
ϕ1(t) and ϕ2(t), we have η ≤ηˆ≤η¯ˆ≤η¯. Similarly, if 1+r ≥m, we have ηˆ≤η ≤η¯≤η¯ˆ.
Combiningbothcases,weobtain[η,η¯]∩[ηˆ,η¯ˆ]̸=∅. □
References
1. A. Alacaoglu, A. Bo¨hm, and Y. Malitsky. Beyond the golden ratio for variational
inequalityalgorithms. arXiv preprint arXiv:2212.13955,2022.
2. A. S. Antipin. On a method for convex programs using a symmetrical modification of
theLagrangefunction. 12:1164–1173.
3. M.Arjovsky,S.Chintala,andL.Bottou. Wassersteingenerativeadversarialnetworks.
InInternational Conference on Machine Learning,pages214–223,2017.
4. M.G.Azar,I.Osband,andR.Munos. Minimaxregretboundsforreinforcementlearn-
ing. InInternational Conference on Machine Learning,pages263–272.PMLR,2017.
5. S.Bartz,H.H.Bauschke,J.M.Borwein,S.Reich,andX.Wang. Fitzpatrickfunctions,
cyclicmonotonicityandRockafellar’santiderivative.NonlinearAnalysis:Theory,Meth-
ods & Applications,66(5):1198–1223,2007.RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 53
6. H.H.BauschkeandP.Combettes. Convexanalysisandmonotoneoperatorstheoryin
Hilbert spaces. Springer-Verlag,2ndedition,2017.
7. H. H. Bauschke, W. M. Moursi, and X. Wang. Generalized monotone operators and
theiraveragedresolvents. Math. Program.,pages1–20,2020.
8. A.Ben-Tal,L.ElGhaoui,andA.Nemirovski. Robust optimization. PrincetonUniver-
sityPress,2009.
9. K. Bhatia and K. Sridharan. Online learning with dynamics: A minimax perspective.
Advances in Neural Information Processing Systems,33:15020–15030,2020.
10. A. Bo¨hm. Solving nonconvex-nonconcave min-max problems exhibiting weak Minty
solutions. Transactions on Machine Learning Research,2022.
11. J. F. Bonnans and A. Shapiro. Perturbation Analysis of Optimization Problems.
Springer,2000.
12. J.F.Bonnans.LocalAnalysisofNewton-TypeMethodsforVariationalInequalitiesand
NonlinearProgramming. Appl. Math. Optim,29:161–186,1994.
13. R. S. Burachik and A. Iusem. Set-Valued Mappings and Enlargements of Monotone
Operators. NewYork:Springer,2008.
14. Y. Cai, A. Oikonomou, and W. Zheng. Tight last-iterate convergence of the extragra-
dient and the optimistic gradient descent-ascent algorithm for constrained monotone
variationalinequalities. arXiv preprint arXiv:2204.09228,2022.
15. Y. Cai and W. Zheng. Accelerated single-call methods for constrained min-max opti-
mization. arXiv preprint arXiv:2210.03096,2022.
16. Y.Censor,A.Gibali,andS.Reich. Thesubgradientextragradientmethodforsolving
variationalinequalitiesinhilbertspace. J.Optim.TheoryAppl.,148(2):318–335,2011.
17. Y.Censor,A.Gibali,andS.Reich. ExtensionsofKorpelevich’sextragradientmethod
for the variational inequality problem in Euclidean space. Optimization, 61(9):1119–
1132,2012.
18. V. Cevher and B.C. Vu˜. A reflected forward-backward splitting method for mono-
toneinclusionsinvolvingLipschitzianoperators. Set-Valued and Variational Analysis,
29(1):163–174,2021.
19. A. Chambolle and T. Pock. A first-order primal-dual algorithm for convex problems
withapplicationstoimaging. J. Math. Imaging Vis.,40(1):120–145,2011.
20. C.-C. Chang and C.-J. Lin. LIBSVM: A library for Support Vector Machines. ACM
Transactions on Intelligent Systems and Technology,2:27:1–27:27,2011.
21. Y.Chen,G.Lan,andY.Ouyang. Optimalprimal-dualmethodsforaclassofsaddle-
pointproblems. SIAM J. Optim.,24(4):1779–1814,2014.
22. Y. Chen, G. Lan, and Y. Ouyang. Accelerated schemes for a class of variational in-
equalities. Math. Program.,165(1):113–149,2017.
23. P. L. Combettes and T. Pennanen. Proximal methods for cohypomonotone operators.
SIAM J. Control Optim.,43(2):731–742,2004.
24. P.L.CombettesandV.R.Wajs. Signalrecoverybyproximalforward-backwardsplit-
ting. Multiscale Model. Simul.,4:1168–1200,2005.
25. D.D.CongandG.Lan. Ontheconvergencepropertiesofnon-euclideanextragradient
methods for variational inequalities with generalized monotone operators. Comput.
Optim. Appl.,60(2):277–310,2015.
26. C.Daskalakis,A.Ilyas,V.Syrgkanis,andH.Zeng. TrainingGANswithOptimism. In
International Conference on Learning Representations (ICLR 2018),2018.
27. D.Davis. Convergencerateanalysisoftheforward-Douglas-Rachfordsplittingscheme.
SIAM J. Optim.,25(3):1760–1786,2015.
28. D.DavisandW.Yin. Athree-operatorsplittingschemeanditsoptimizationapplica-
tions. Set-Valued Var. Anal.,25(4):829–858,2017.
29. J. Diakonikolas, C. Daskalakis, and M. Jordan. Efficient methods for structured
nonconvex-nonconcave min-max optimization. In International Conference on Arti-
ficial Intelligence and Statistics,pages2746–2754.PMLR,2021.
30. A.L.DontchevandT.R.Rockafellar. Characterizationsofstrongregularityforvaria-
tionalinequalitiesoverpolyhedralconvexsets. SIAMJ.Optim.,6(4):1087–1105.,1996.
31. J. Eckstein and D. P. Bertsekas. On the Douglas—Rachford splitting method and
theproximalpointalgorithmformaximalmonotoneoperators. Math. Program.,55(1-
3):293–318,1992.54 QuocTran-Dinh,NghiaNguyen-Trung
32. J.E.Esser.Primal-dualalgorithmforconvexmodelsandapplicationstoimagerestora-
tion, registration and nonlocal inpainting. PhD Thesis, University of California, Los
Angeles,USA,2010.
33. F. Facchinei and J.-S. Pang. Finite-dimensional variational inequalities and comple-
mentarity problems,volume1-2. Springer-Verlag,2003.
34. M.Fukushima. Applicationofthealternatingdirectionmethodofmultiplierstosepa-
rableconvexprogrammingproblems. Comput. Optim. Appl.,1(1):93–111,1992.
35. F. Giannessi and A. Maugeri. Variational inequalities and network equilibrium prob-
lems. Springer,1995.
36. N.Golowich,S.Pattathil,C.Daskalakis,andA.Ozdaglar. Lastiterateisslowerthan
averagediterateinsmoothconvex-concavesaddlepointproblems.ConferenceonLearn-
ing Theory (PMLR),pages1758–1784,2020.
37. I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A.Courville,andY.Bengio. Generativeadversarialnets. InAdvancesinneuralinfor-
mation processing systems,pages2672–2680,2014.
38. E. Gorbunov, N. Loizou, and G. Gidel. Extragradient method: O(1/k) last-iterate
convergence for monotone variational inequalities and connections with cocoercivity.
In International Conference on Artificial Intelligence and Statistics, pages 366–402.
PMLR,2022.
39. E.Gorbunov,A.Taylor,S.Horva´th,andG.Gidel. Convergenceofproximalpointand
extragradient-basedmethodsbeyondmonotonicity:Thecaseofnegativecomonotonic-
ity. arXiv preprint arXiv:2210.13831,2022.
40. P. T. Harker and J.-S. Pang. Finite-dimensional variational inequality and nonlinear
complementarityproblems:asurveyoftheory,algorithmsandapplications. Mathemat-
ical programming,48(1):161–220,1990.
41. B. He and X. Yuan. Convergence analysis of primal-dual algorithms for saddle-point
problem:fromcontractionperspective. SIAM J. Imaging Sci.,5:119–149,2012.
42. A.N.IusemandB.F.Svaiter.AvariantofKorpelevich’smethodforvariationalinequal-
itieswithanewsearchstrategy. Optimization,42(4):309–321,1997.
43. AbdulJabbar,XiLi,andBourahlaOmar.Asurveyongenerativeadversarialnetworks:
Variants, applications, and training. ACM Computing Surveys (CSUR), 54(8):1–49,
2021.
44. E. N. Khobotov. Modification of the extra-gradient method for solving variational
inequalities and certain optimization problems. USSR Computational Mathematics
and Mathematical Physics,27(5):120–127,1987.
45. I.V.Konnov.Combinedrelaxationmethodsforvariationalinequalities.Springer-Verlag,
2001.
46. G.M.Korpelevich. Theextragradientmethodforfindingsaddlepointsandotherprob-
lems. Matecon,12:747–756,1976.
47. D. Levy, Y. Carmon, J. C. Duchi, and A. Sidford. Large-scale methods for distri-
butionally robust optimization. Advances in Neural Information Processing Systems,
33:8847–8860,2020.
48. F.Lin,X.Fang,andZ.Gao. Distributionallyrobustoptimization:Areviewontheory
andapplications. Numerical Algebra, Control & Optimization,12(1):159,2022.
49. T. Lin, C. Jin, and M. I. Jordan. Near-optimal algorithms for minimax optimization.
InConference on Learning Theory,pages2738–2779.PMLR,2020.
50. P.L.LionsandB.Mercier. Splittingalgorithmsforthesumoftwononlinearoperators.
SIAM J. Num. Anal.,16:964–979,1979.
51. N.Loizou,H.Berard,G.Gidel,I.Mitliagkas,andS.Lacoste-Julien. Stochasticgradi-
entdescent-ascentandconsensusoptimizationforsmoothgames:Convergenceanalysis
under expected co-coercivity. Advances in Neural Information Processing Systems,
34:19095–19108,2021.
52. Y. Luo and Q. Tran-Dinh. Extragradient-type methods for co-monotone root-finding
problems. (UNC-STOR Technical Report),2022.
53. A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learn-
ing models resistant to adversarial attacks. In International Conference on Learning
Representations,2018.
54. Y.Malitsky.Projectedreflectedgradientmethodsformonotonevariationalinequalities.
SIAM J. Optim.,25(1):502–520,2015.RevisitingExtragradient-TypeMethods–Part1:Non-acceleratedMethods 55
55. Y.Malitsky.Goldenratioalgorithmsforvariationalinequalities.Math.Program.,pages
1–28,2019.
56. Y. Malitsky and M. K. Tam. A forward-backward splitting method for monotone in-
clusionswithoutcocoercivity. SIAM J. Optim.,30(2):1451–1472,2020.
57. Y.V.MalitskyandV.V.Semenov.Anextragradientalgorithmformonotonevariational
inequalities. Cybernetics and Systems Analysis,50(2):271–277,2014.
58. B.Martinet.R´egularisationd’in´equationsvariationnellesparapproximationssuccesives.
Rev. Franc¸aise Inf. Rech. Oper.,R-3:154–179,1970.
59. P.Mertikopoulos,B.Lecouat,H.Zenati,C.-S.Foo,V.Chandrasekhar,andG.Piliouras.
Optimistic mirror descent in saddle-point problems: Going the extra (gradient) mile.
InICLR 2019-7th International Conference on Learning Representations,pages1–23,
2019.
60. A.Mokhtari,A.Ozdaglar,andS.Pattathil.Aunifiedanalysisofextra-gradientandop-
timisticgradientmethodsforsaddlepointproblems:Proximalpointapproach.InInter-
national Conference on Artificial Intelligence and Statistics,pages1497–1507.PMLR,
2020.
61. A.Mokhtari,A.E.Ozdaglar,andS.Pattathil.ConvergencerateofO(1/k)foroptimistic
gradientandExtragradientmethodsinsmoothconvex-concavesaddlepointproblems.
SIAM J. Optim.,30(4):3230–3251,2020.
62. R.D.C.MonteiroandB.F.Svaiter. Onthecomplexityofthehybridproximalextragra-
dientmethodfortheinteratesandtheergodicmean.SIAMJ.Optim.,20(6):2755–2787,
2010.
63. R.D.C. Monteiro and B.F. Svaiter. Complexity of variants of Tseng’s modified F-B
splitting and Korpelevich’s methods for hemivariational inequalities with applications
tosaddle-pointandconvexoptimizationproblems. SIAM J. Optim.,21(4):1688–1720,
2011.
64. B. S. Mordukhovich. Variational analysis and generalized differentiation: Volumes I
and II,volume330. SpringerScience&BusinessMedia,2006.
65. A.Nemirovski. Mini-courseonconvexprogrammingalgorithms. Lecture notes,2013.
66. A.Nemirovski,A.Juditsky,G.Lan,andA.Shapiro. Robuststochasticapproximation
approachtostochasticprogramming.SIAMJ.onOptimization,19(4):1574–1609,2009.
67. A.Nemirovskii.Prox-methodwithrateofconvergenceO(1/t)forvariationalinequalities
withLipschitzcontinuousmonotoneoperatorsandsmoothconvex-concavesaddlepoint
problems. SIAM J. Optim.,15(1):229–251,2004.
68. Y.Nesterov. Dualextrapolationanditsapplicationstosolvingvariationalinequalities
andrelatedproblems. Math. Program.,109(2–3):319–344,2007.
69. Y. Nesterov and L. Scrimali. Solving strongly monotone variational and quasi-
variationalinequalities. CORE Discussion Paper,107:1–15,2006.
70. J.-M. Peng and M. Fukushima. A hybrid Newton method for solving the variational
inequalityproblemviatheD-gapfunction. Math. Program.,86(2):367–386,1999.
71. T.Pennanen.Localconvergenceoftheproximalpointalgorithmandmultipliermethods
withoutmonotonicity. Math. Oper. Res.,27(1):170–191,2002.
72. T. Pethick, O. Fercoq, P. Latafat, P. Patrinos, and V. Cevher. Solving stochas-
tic weak Minty variational inequalities without increasing batch size. arXiv preprint
arXiv:2302.09029,2023.
73. T. Pethick, P. Patrinos, O. Fercoq, and V. Cevher. Escaping limit cycles: Global con-
vergence for constrained nonconvex-nonconcave minimax problems. In International
Conference on Learning Representations,2022.
74. T. Pethick, W. Xie, and V. Cevher. Stable nonconvex-nonconcave training via linear
interpolation. Advances in Neural Information Processing Systems,36,2024.
75. R.R.Phelps.Convexfunctions,monotoneoperatorsanddifferentiability,volume1364.
Springer,2009.
76. L.D.Popov. AmodificationoftheArrow-Hurwiczmethodforsearchofsaddlepoints.
Math. notes of the Academy of Sciences of the USSR,28(5):845–848,1980.
77. H. Rahimian and S. Mehrotra. Distributionally robust optimization: A review. arXiv
preprint arXiv:1908.05659,2019.
78. S. M. Robinson. Strongly Regular Generalized Equations. Math. Opers. Res., Vol. 5,
No. 1 (Feb., 1980), pp. 43-62,5:43–62,1980.56 QuocTran-Dinh,NghiaNguyen-Trung
79. R.RockafellarandR.Wets. Variational Analysis,volume317. Springer,2004.
80. R.T.Rockafellar.ConvexAnalysis,volume28ofPrincetonMathematicsSeries.Prince-
tonUniversityPress,1970.
81. R.T. Rockafellar. Monotone operators and the proximal point algorithm. SIAM J.
Control Optim.,14:877–898,1976.
82. R.T.RockafellarandR.J-B.Wets. Variational Analysis. Springer-Verlag,1997.
83. E.K.RyuandS.Boyd. Primeronmonotoneoperatormethods. Appl.Comput.Math,
15(1):3–43,2016.
84. M.V.SolodovandB.F.Svaiter. Ahybridapproximateextragradient–proximalpoint
algorithm using the enlargement of a maximal monotone operator. Set-Valued Var.
Anal.,7(4):323–345,1999.
85. M. V. Solodov and B. F. Svaiter. A new projection method for variational inequality
problems. SIAM J. Control Optim.,37(3):765–776,1999.
86. M.V.SolodovandP.Tseng.Modifiedprojection-typemethodsformonotonevariational
inequalities. SIAM J. Control Optim.,34(5):1814–1830,1996.
87. Q.Tran-Dinh. SublinearConvergenceRatesofExtragradient-TypeMethods:ASurvey
onClassicalandRecentDevelopments. arXiv preprint arXiv:2303.17192,2023.
88. Q.Tran-Dinh,O.Fercoq,andV.Cevher.Asmoothprimal-dualoptimizationframework
fornonsmoothcompositeconvexminimization. SIAM J. Optim.,28(1):96–134,2018.
89. Q.Tran-DinhandY.Luo.Randomizedblock-coordinateoptimisticgradientalgorithms
forroot-findingproblems. arXiv preprint arXiv:2301.03113,2023.
90. Q. Tran-Dinh, T. Sun, and S. Lu. Self-concordant inclusions: A unified framework for
path-followinggeneralizedNewton-typealgorithms.Math.Program.,177(1–2):173–223,
2019.
91. Q.Tran-DinhandY.Zhu.Non-stationaryfirst-orderprimal-dualalgorithmswithfaster
convergencerates. SIAM J. Optim.,30(4):2866–2896,2020.
92. P.Tseng. Furtherapplicationsofasplittingalgorithmtodecompositioninvariational
inequalitiesandconvexprogramming. Math. Program.,48(1-3):249–263,1990.
93. P.Tseng. Amodifiedforward-backwardsplittingmethodformaximalmonotonemap-
pings. SIAM J. Control and Optim.,38(2):431–446,2000.
94. V.PhanTu. Ontheweakconvergenceoftheextragradientmethodforsolvingpseudo-
monotonevariationalinequalities. J. Optim. Theory Appl.,176(2):399–409,2018.
95. C.-Y.Wei,C.-W.Lee,M.Zhang,andH.Luo. Last-iterateconvergenceofdecentralized
optimistic gradient descent/ascent in infinite-horizon competitive Markov games. In
Conference on learning theory,pages4259–4299.PMLR,2021.
96. E.Zeidler. NonlinearFunctionalAnalysisanditsApplicationsIII-VariationalMeth-
ods and Optimization. SpringerVerlag,1984.
97. Y.Zhu,D.Liu,andQ.Tran-Dinh.Newprimal-dualalgorithmsforaclassofnonsmooth
and nonlinear convex-concave minimax problems. SIAM J. Optim., 32(4):2580–2611,
2022.