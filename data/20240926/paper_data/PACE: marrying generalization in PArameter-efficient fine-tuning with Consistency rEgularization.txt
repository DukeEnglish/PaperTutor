PACE: marrying generalization in PArameter-efficient
fine-tuning with Consistency rEgularization
YaoNi† ShanZhang† PiotrKoniusz∗,§,†
†TheAustralianNationalUniversity §Data61 CSIRO
†firstname.lastname@anu.edu.au §piotr.koniusz@data61.csiro.au
Abstract
Parameter-EfficientFine-Tuning(PEFT)effectivelyadaptspre-trainedvisiontrans-
formerstodownstreamtasks. However,theoptimizationfortasksperformance
oftencomesatthecostofgeneralizabilityinfine-tunedmodels. Toaddressthis
issue,wetheoreticallyconnectsmallerweightgradientnormsduringtrainingand
largerdatasetstotheimprovedmodelgeneralization. Motivatedbythisconnection,
weproposereducinggradientnormsforenhancedgeneralizationandaligningfine-
tunedmodelwiththepre-trainedcounterparttoretainknowledgefromlarge-scale
pre-trainingdata. Yet,naivealignmentdoesnotguaranteegradientreductionand
canpotentiallycausegradientexplosion,complicatingeffortstomanagegradients.
Toaddresssuchissues,weproposePACE,marryinggeneralizationofPArameter-
efficientfine-tuningwithConsistencyrEgularization. Weperturbfeatureslearned
from the adapter with the multiplicative noise and ensure the fine-tuned model
remains consistent for same sample under different perturbations. Theoretical
analysisshowsthatPACEnotonlyimplicitlyregularizesgradientsforenhanced
generalization,butalsoimplicitlyalignsthefine-tunedandpre-trainedmodelsto
retainknowledge.Experimentalevidencesupportsourtheories.PACEoutperforms
existingPEFTmethodsinfourvisualadaptationtasks: VTAB-1k,FGVC,few-shot
learninganddomainadaptation. CodewillbeavailableatMaxwellYaoNi/PACE.
1 Introduction
Visiontransformers[12],withtheself-attentionmechanism[2]capturinglong-rangedependenciesin
data,havebeensuccessfulinvariouscomputervisiontasks,includingimageclassification(ViT[12],
Swin[38]),multimodallearning(CLIP[49],BLIP[33]),imagesynthesis(StableDiffusion[51]),and
semanticsegmentation(SAM[27]). Thesuccessofvisiontransformerscanbelargelyattributed
to the availability of abundant data, such as ImageNet [8] and Laion5B [54], which has enabled
researcherstoscaleupthesemodelsbytrainingthemwithanenormousnumberofparameters.
Such huge models, with knowledge from large-scale pre-training [57], have become foundation
models that can be easily adapted to various downstream tasks through full fine-tuning or linear
probing[15],eliminatingtheneedfortask-specificmodeldesign[6]. However,fullfine-tuningis
storage-intensiveandinfeasibleformaintainingseparatemodelweightsasthenumberoftasksgrows,
whilelinearprobing,whichonlytrainsthelastheadlayer,yieldsinferioradaptationperformance.
Toovercometheselimitations,Parameter-EfficientFine-Tuning(PEFT)[18]fine-tunesonlyasmall
subsetofparameters,therebyreducingstoragerequirementswhilesurpassingtheperformanceof
full fine-tuning and linear probing. These advantages have popularized PEFT and inspired the
development of various PEFT methods for computer vision, which can be categorized into two
groups: thoseincreasinginferencecostandcost-efficientones. Thefirstgroupintroducesadditional
∗Thecorrespondingauthor. ThispaperisacceptedbyNeurIPS2024asaspotlight. Thispreliminary
versionwillsoonbeextendedwiththeexperimentsandanalysesfromtherebuttal.
4202
peS
52
]GL.sc[
1v73171.9042:viXralearningbranches,suchasnon-linearadapters[19,6],orconcatenateslearnableparameterswith
inputtokens,e.g.,visualprompts[22,73,46],increasinginferencecost. Thesecondgroup,focuses
oncost-efficiencyinvolvinglower-rankadaptationinlinearlayers[5,20],oraffinetransformations
suchasSSF[36]andRepAdapters[39],whichcanbereparameterizedduringinferenceforefficiency.
Despite the superiority and efficiency of PEFT, prioritizing optimization for downstream tasks
compromisesthegeneralizabilityoffine-tunedmodels,yieldingsuboptimalperformance. Although
some analyses have been conducted on PEFT [57, 21, 14, 64, 34], they fail to fully explain the
generalizationofPEFT,leadingtoineffectivestrategiesforimprovinggeneralization.
ToaddressthisgapinunderstandinggeneralizationinPEFT,weestablishatheoreticalconnection
fromgeneralizationtheory: smallerweightgradientnormsandlargerdatavolumescontributeto
bettergeneralization. Motivatedbythis,weproposereducingweightgradientnormsandaligning
outputspaceofthefine-tunedmodelwiththepre-trainedonetoretainknowledgecapturedfromlarge
pre-trainingdata. Yet,theoreticalanalysesrevealthisnaivealignmentdosenotguaranteegradient
regularizationandcanevencausegradientexplosion,complicatingeffortsforgradientmanagement.
Toaddressthisissue,weproposeperturbingfeatureslearnedfromtheadapterwithmultiplicative
noiseandconstrainingthenetworkoutputtobeconsistentacrossdifferentperturbations.
WecallourmethodPACE.ItmarriesgeneralizationofPArameter-efficientfine-tuningwithConsis-
tencyrEgularization. Thenamereflectsourgoalofkeepingtheoutputbehaviorofthefine-tuned
modelinpacewithpre-trainedone. Despiteitssimplicity,theoreticalanalysisconfirmsthatPACE
notonlyimplicitlyregularizesweightgradientsforbettergeneralizationbutalsoimplicitlyalignsthe
fine-tunedmodelwiththepre-trainedcounterparttoretainknowledgefromlarge-scalepre-training
data. Experimental evidence supports our theories. PACE outperforms existing PEFT methods,
achievingsuperiorresultsacrossfouradaptationbenchmarks. Ourkeycontributionsare:
i. Weestablishatheoryconnectingsmallerweightgradientnormsandlargerdatasetswithen-
hancedgeneralization,motivatinggradientreductionandmodelalignmentforfine-tuning.
ii. WeproposePACE,asimpleyeteffectivemethodperturbingfeaturesfromadapterswithmulti-
plicativenoiseandconstrainingoutputoffine-tunedmodeltobeconsistentacrossperturbations.
iii. OurtheoreticalandempiricalevidenceconfirmsthatPACEimplicitlyregularizesgradientsand
alignsthefine-tunedmodelwiththepre-trainedone. PACEexcelson4visualadaptationtasks.
iv. Weprovidenoveltheoreticalexplanationsforhowgradientpenalizationandconsistencyregu-
larizationbenefitgeneralization,offeringfundamentalinsightsapplicableacrossdeeplearning.
2 Relatedwork
Parameter-Efficient Fine-Tuning (PEFT). LoRA [20] uses low-rank decomposition to reduce
parameters and treats adapters as side paths. SSF [36] proposes affine transformations on latent
features. FacT[24]decomposesandreassemblesparametermatricesinViT.Surgicalfine-tuning[30]
differentnetworkpartsresultsindifferentperformancefordifferentdatasets. FLoRA[66]aimsat
real-timeglobalservice.GLoRA[5]unifiescost-efficientPEFTmethods.NOAH[73]usesparameter
searchonneuralprompts. ARC[10]leveragescross-layerViTsimilarity,parameter-sharingadapter
andscalingfactorsforlowerfine-tuningcost. RLRR[11]incorporatesaresidualtermforflexibility
whilepreservingpre-trainedrepresentation. RepAdapter[39]reparameterizesadaptersforefficient
inference. Res-tuning[23]unbindstunersfromthebackboneformemoryefficiency. Zhaoetal.[74]
showimpressivefine-tuningresultsbytuningonlytheattentionlayernormalization. OFT[48]and
BOFT[37]proposeorthogonalfine-tuningtopreservehypersphereenergybetweenneurons.
Consistency Regularization. Fixmatch [55] applies consistency regularization over augmented
imagesforsemi-supervisedlearning. Openmatch[53]utilizesitonoutlierpredictionsforopen-set
semi-supervisedlearning. R-Drop[67]appliesittotransformers[61]withdropoutforNLPtasks.
CR[70]appliesitoveraugmentedrealandfakeimagesforGANtraining. CAGAN[44]enforces
consistency on discriminators with dropout for GAN training. Despite the empirical success of
consistencyregularizationdemonstratedbypreviousworks,theoreticalanalysisislacking. While
NICE[42]demonstratesthatconsistencyregularizationlowerslatentfeaturegradientsforstable
GANtraining,itfailstorevealreducedweightgradientforenhancedgeneralization. Ourstudygoes
beyondpriorworksbyprovidingatheoreticallinkbetweensmallerweightgradientsandimproved
generalization,effectivelymarryinggeneralizationofPEFTwithconsistencyregularization.
2Generalization of Fine-Tuning. Li et al. [32] constrain the fine-tuned model’s closeness to the
pre-trainedmodelinweightspace. Fuetal.[14]inducesparsityonPEFTmethodsforenhanced
generalization. Wangetal.[64]findsPEFTmethodsimprovegeneralizationonfine-tuninggraph
neuralnetwork. Recentworks,includingVioLET[65],PromptSRC[25],CoPrompt[52],propose
aligning the fine-tuned model with the pre-trained one for enhanced generalization or avoiding
forgetting,whichcanbeseenasournaivealignment. Additionally,L2SP[68],DELTA[35],andFTP
[58]aimtoretainpre-trainedknowledgebyaligningfinetunedmodelswithpre-trainedones,reducing
distanceinweightspace,featurespaceandusingprojectedgradientdescent,respectively. However,
theyfailtoprovideatheoreticalanalysisforthisalignment. Ourstudygoesbeyondunderstanding
generalizationofPEFTbydiscoveringthebenefitsofgradientregularizationandmodelalignment.
WeproposePACEtomatchbothrequirements,pavingacomprehensiveunderstandingforPEFT.
Gradient regularization. Previous studies have empirically shown that gradient regularization
improvesneuralnetworkperformance[60,75,41,43]. However,theyfailedtotheoreticallyestablish
the connection between smaller gradient norms and better generalization [13, 72, 4]. Our work
bridgesthisgapbyestablishingafundamentaltheorybetweenreducedgradientnormsandimproved
generalization,providingasolidfoundationforfutureresearchonenhancinggeneralization.
3 Approach
Webeginwithaunifiedperspectiveoncost-efficientPEFTbasedonGLoRA[5],linkinggeneraliza-
tionwithgradientsandlarge-scaledataandmotivatingthealignmentofthefine-tunedmodelwiththe
pre-trainedmodeltoleverageitsknowledge. Weidentifylimitationsofnaivealignmentingradient
regularizationandintroducePACE,whichimplicitlyenhancesgradientregularizationandmodel
alignment. Weconcludewiththeoreticaljustificationandefficientimplementations.
3.1 Aunifiedperspectiveoncost-efficientPEFTmethods
VisionTransformer(ViT)[12]extendsthesequentialmodelingcapabilitiesoftheTransformer[61],
originallydesignedfornaturallanguageprocessing,tocomputervisiontasks. Itachievesthisby
splittingimagesintonon-overlappingpatchesandextractingfeaturesusingLtransformerblocks.
Eachblockcontainsself-attentionandMLPmodules,primarilycomposedoflinearlayers. These
linearlayersunderpintheself-attentionmechanism,allowingViTtocapturelong-rangedependencies
inimagesandoutperformconvolutionalnetworkswhentrainedonlarge-scaledata.
TheViT,withmassiveparameterspretrainedonlarge-scaledata,servesasafoundationmodelthatcan
befine-tunedfordownstreamtasksusinglimiteddata. However,fullyfine-tuningallViTparameters
forvariousdownstreamtasksrequiressubstantialmemoryandcanleadtheforgettingofpretrained
knowledge. Toalleviatethiswithoutincreasinginferencecost,adapterswithlightweightparameters
areoftenpreferredforfine-tuning. Leth¯ ()beatransformationwithinthepre-trainedViT.Current
0
adapterscanbeunifiedasintroducingares· idualbranch∆h¯ toformanewtransformationh¯:
h¯(a)=h¯ (a)+∆h¯(a). (1)
0
Here,aistheinputandh¯ canrepresentMLPmodules,asinAdapter[19]andAdaptFormer[6],or
0
linearlayersinself-attentionandMLPmodules,asin[20,5,9,28]. InSSF[36],h¯ istheidentity
0
mappingand∆h¯(a)=a (γ 1)+βwithγ andβasaffinetransformationparameters.
⊙ −
Giventhatlinearlayersarekeycomponentsintransformer,tuningthemoffersaflexibleandeffective
waytoadaptmodelstodownstreamtasks. Thisworkfocusesonmethodsthattunethelinearlayer
without increasing inference cost. Let (W ,b ), (∆W,∆b), and (W,b) be the parameters of
0 0
pretrainedmodel,adapterandfinetunedmodel,respectively,whereW 0,∆W,W Rdout×din and
b ,∆b,b Rd ,finetuningalinearlayerinself-attentionorMLPmodulecanbefo∈ rmedas:
0 0 ∈ out
h(a)=Wa+b=(W +∆W)a+(b +∆b)
0 0
=h (a)+∆h(a)=(W a+b )+(∆Wa+∆b). (2)
0 0 0
BasedonGLoRA[5],cost-efficientPEFTmethodsforlinearlayersvaryintheformof∆W,∆b:
LoRA add: ∆W =W dW u,∆b=b lorawhereW
d
Rdout×r,W
u
Rr×din,andristherank.
∈ ∈
LoRA : ∆W=W (W W ),∆b=b b ,includingRepAdapter[39]viareparameterization.
mul 0 d u 0 lora
⊙ ⊙
3VPT add: ∆W iszero,∆b=W 0P,withlearnableP Rdin×1aslayer-wisevisualprompt. Weuse
∈
VPT todifferentiatefromVPT[22],whichconcatenatesP withtokens,increasinginferencecost.
add
3.2 Generalizationofdeepneuralnetworks
Havingestablishedaunifiedperspectiveoncost-efficientPEFT,wenowmotivateourmethodfroma
perspectiveonimprovinggeneralizationofneuralnetworkstoenhanceperformanceonunseendata.
Consideranetworkf :=ϕ(g(x))withllayers,wheregisfeatureextractorandϕistheclassification
head. Letθ := (W(i),b(i)) l betheparametersetwithdimensiondand n := (x ,y ) n
be the training{ set of size n} di r= a1 wn i.i.d. from distribution D, which containD s infin{ ite di atai . } Ti= he1
followinglemmafrom[13]buildsarelationshipbetweentheempiricalandpopulationloss.
Lemma1 (Theorem1from[13])Let Dn(θ)betheempiricallossfunctionoverf ontrainingset
nand D(θ)bethepopulationloss.L Foranyρ>0,withhighprobabilityover n D,wehave
D L D ∼
(cid:16) θ 2 1(cid:17)
LD(θ) ≤∥m ϵ∥2a ≤x ρLDn(θ+ϵ)+R ∥ ρ2∥2,
n
, (3)
whereR:(R +,R +) R +isastrictlyincreasingfunction(undersomeconditionson D(θ)).
→ L
Lemma1boundsthepopulationlossbytheempiricallosswithperturbedweights,indicatingthat
minimalempiricallossincreasefromsmallweightperturbationsimplieslowpopulationloss.
Byobservingthatthemaximumof LDn isachievedatϵ= ∥ρ ∇∇ θθ ∥2,where∇ θ isthegradientof LDn
atθ,andperformingaTaylorexpansionof aroundθ,weformulatethefollowingtheorem:
Dn
L
Theorem1 Denote∇ asthegradientandλH asthelargesteigenvaluesoftheHessianmatrix
θ max
H
θ
of
Dn
atθ. Foranyρ>0,withhighprobabilityovertrainingset n D,wehave
L D ∼
ρ2 (cid:16) θ 2 1(cid:17)
LD(θ) ≤LDn(θ)+ρ ∥∇
θ
∥2+
2
λH max+R ∥ ρ2∥2,
n
. (4)
Here,
higher-ordertermsfromtheTaylorexpansionareincorporatedintoR(cid:16) ∥θ∥2
2,
1(cid:17)
, whichis
ρ2 n
relatedtoweightsnormandinverselyrelatedtothetrainingdatasizen.
Theorem1(proofisin§B.1)outlinesstrategiesforenhancinggeneralization. Theseinvolveregulariz-
ingweightnormsandthelargesteigenvaluesintheHessianmatrix,andcrucially,increasingdatasize
nandreducingtheweightgradientnorms. However,cautionisneededtoavoidexcessivereduction,
asthiscouldimpairnetwork’srepresentationcapacity,yieldinghigherempiricalandpopulationloss.
3.3 Motivationandlimitationofaligningthefine-tunedmodelwiththepre-trainedmodel
Theorem1emphasizesthatlarge-scaledataandsmallergradientmagnitudesareessentialforbetter
generalizationinneuralnetworktraining. Therefore, aligningthefine-tunedmodelwiththepre-
trainedoneiscrucial,asitensuresretentionofknowledgedevelopedfromlarge-scaledata,preserving
generalizability.PEFTmethodsachievethisalignmentbylimitingthenumberoftrainableparameters,
restrictingmodel’scapacitytodeviatefromthepre-trainedoneandoftenoutperformingfullfine-
tuning. However, thetrainingobjectiveprioritizesdownstreamtaskperformance, compromising
alignment with pre-trained knowledge. While sparsity regularization [14] and weight decay on
adapter weights help, they do not ensure alignment, as even smaller weight changes can lead to
significantdivergenceinoutputspace. Therefore,weproposetoachievethealignmentbyreducing
theFP-distance(outputdistancebetweenfine-tunedandpre-trainedmodelsontrainingsamples):
n
1 (cid:88)
Dfp(θ)= f(x ;θ) f(x ;θ ) 2, θ =θ +∆θ, (5)
n ∥ i − i 0 ∥2 0
i=1
whereθ,θ ,∆θ Rdareparametersforthefine-tunedmodel,pre-trainedmodelandtheadapter.
0
∈
WhilereducingFP-distancekeepsthefine-tunedmodelclosetothepre-trainedmodel,thuspreserving
itsknowledge,itdoesnotensurereducedgradientmagnitudes,leadingtosuboptimalgeneralization.
Tounderstandthegradient-relatedlimitationsinthisalignment,weassume∆θissmallenoughfora
Taylorexpansionapproximation. Followingstandardpractices[13,71,1],weperformtheexpansion
4uptothesecond-orderterms. Simplifyingourapproach,weanalyzeaone-dimensionaloutputfora
singlei.i.d.sample,whichleadsustothefollowingproposition.
Proposition1 Assuming∆θissmall,denotef(θ) Rastheone-dimensionaloutputforx,with
∈
∇andH asitsgradientandHessianatθ. FP-distanceoverxcanbedecomposedasfollows:
[f(θ) f(θ )]2 =[f(θ) f(θ ∆θ)]2 (cid:2) f(θ) [f(θ) ∆θT∇+ 1 ∆θTH∆θ](cid:3)2
0
− − − ≈ − − 2
1
[∆θT∇ ∆θTH∆θ]2. (6)
≈ − 2
Prop. 1 establishes the relationship between weight gradients, adapter weights, and FP-distance.
However, itremainsunclearifitregulatesgradients. OurexperimentsshowthatminimizingFP-
distancecansometimesincreasegradientmagnitude,complicatingeffortsformanaginggradient.
3.4 Consistencyregularization
Toachievebettergeneralizationbybothregularizinggradientsandaligningthefine-tunedmodelwith
thepre-trinedmodel,weproposeaconsistencyregularizationlossforf,encouraginginvarianceoff
tothesameinputundervaryingmultiplicativenoiseperturbationsontheadapterweights,asfollows:
n
1 (cid:88)
Dpace(θ)= E f(x ;θ +z ∆θ) f(x ;θ +z ∆θ) 2, (7)
n z1,z2∥ i 0 1 ⊙ − i 0 2 ⊙ ∥2
i=1
wherez ,z (1,σ2I)isthemultiplicativenoiseappliedonadapterweight. Tounderstandthe
1 2
∼N
generalizationbenefitsinthisconsistencyregularization,wesimplifytheanalysisbyfocusingon
one-dimensionaloutputforasinglesample,resultinginthefollowingtheorem.
Theorem2 UsingnotationsfromProp. 1,letf(θ +z ∆θ) Rbetheone-dimensionaloutput
0
⊙ ∈
forx. Define∆θ asj-thelementin∆θ, asthej-thelementin∇andH asthe(j,k)-entryin
j j jk
∇
H. Withz ,z (1,σ2I),theconsistencylossoverxcanbeapproximatedas:
1 2
∼N
E [f(θ +z ∆θ) f(θ +z ∆θ)]2
z1,z2 0 1
⊙ −
0 2
⊙
2σ2(cid:80) ∆θ2 2+σ4(cid:80) ∆θ2∆θ2H2 =2σ2 ∆θ ∇ 2+σ4 (∆θ∆θT) H 2. (8)
≈ j j∇j j,k k j jk ∥ ⊙ ∥2 ∥ ⊙ ∥F
Theorem 2 (Proof is in §B.2) shows that the consistency regularization essentially penalizes the
first-andsecond-ordergradientsoff atθ,withtheregularizationstrengthcontrolledbythenoise
varianceσ2andadaptivelyinfluencedbythemagnitudeofelementsintheadapterweight∆θ. Thus,
minimizingtheconsistencylossimplicitlyregularizesthegradients,improvinggeneralization.
WiththeFP-distanceinProp. 1andconsistencylossinTheorem2,weestablishtheirrelationshipas:
Theorem3 Withdasthedimensionofθ,Eq. 6canbeupperboundedas:
1
[∆θT∇ ∆θTH∆θ]2 2d ∆θ ∇ 2+d2 (∆θ∆θT) H 2. (9)
− 2 ≤ ∥ ⊙ ∥2 ∥ ⊙ ∥F
Theorem3(proofisinB.3)establishestherelationshipbetweenEq. 6andEq. 8,showingthatEq. 6
isupper-boundedbytermsinvolving ∆θ ∇ 2and (∆θ∆θT) H 2 whichappearinEq. 8.
∥ ⊙ ∥2 ∥ ⊙ ∥F
ReducingthesetermsresultsinadecreaseinEq. 6. Thusminimizingtheconsistencylossimplicitly
alignsthefine-tunedwithpre-trainedmodels,preservingknowledgeinpre-trainedmodel.
3.5 EfficientimplementationofPACE
Providing different weight perturbations for each input in a mini-batch increases memory and
computationaldemands. Toavoidthisinefficiency,weperturbfeatureoutputsfromtheadapter∆h,
effectivelysimulatingperturbationthatsharesnoiseacrosseachrowintheweight∆W. Oursimple
pipelineisillustratedinFigure1. ConsiderX RB×T×din asabatchofdatawhereB,T bethe
∈
5Transformerblockwithadapterperturbedbynoise Consistencyregularizationbetweentwooutputsofx
Transformer
MLP ∆ℎ(⋅) Block ×𝐿 head loss
Norm 𝑓 !(𝒙)
ℎ(⋅)
# ℎ(⋅)=ℎ #(⋅)+𝒛⊙Δℎ(⋅) 𝒙 shareweights ||𝑓 𝒙 −𝑓(𝒙)||"
Multi-Head Δ𝑾 𝑾 where𝒛∼𝒩(𝟏,𝜎"𝑰) non-sharednoises ! "
Attention #
∆𝒃 𝒃 # 𝑓 "(𝒙)
Transformer
Norm head
Block
×𝐿
Adapter∆ℎin
Transformer linearlayerℎ
Block
Figure 1: Our pipeline. Adapter ∆h and h from pre-trained model form the linear layer h of
0
Multi-HeadAttentionandMLPinfine-tunedmodel. Weperturb∆hwithmultiplicativenoiseand
ensurethenetworkremainsconsistenttosameinputsundervaryingperturbations.
batchandtokensizes. Thecalculationforthelinearlayerofthefine-tunedmodel,whichutilizes
pre-trainedweightsW ,b andadapterweights∆W,∆b,processesanoutputsizeofd as:
0 0 out
h (X)=W X+b ; ∆h(X)=∆WX+∆b, (10)
0 0 0
h(X)=h (X)+Z ∆h(X). (11)
0
⊙
Here istheelement-wisemultiplicationafterexpandingtheleftmatrixZ RB×dout (1,σ2I)
⊙ ∈ ∼N
intoB T d wheretokenswithinthesameexamplesharesamenoise. Motivatedby[31],theσ
out
× ×
decreaseslinearlyasblockdepthincreases. Letf andf betwonetworkssharesameweightsbut
1 2
non-sharenoises. ThelossfunctionforPACEis:
n
1 (cid:88)
PACE = ℓ(f (x ),y )+λ f (x ) f (x ) 2, (12)
L n 1 i i ∥ 1 i − 2 i ∥2
i=1
whereℓistheclassificationlossandλisahyperparametercontrollingregularizationstrength. During
inference,noiseandregularizationareommitted,∆W,∆bareintegratedwithW ,b forefficiency:
0 0
W =W +∆W; b=b +∆b; h(X)=WX+b. (13)
0 0
4 Experiments
WecombineLoRA andVPT toformastrongbaselineLoRA +VPT ,outperformingother
mul add mul add
combinationsinmostcases. Weevaluateourmethodacrossfourvisualclassificationadaptation
tasks: VTAB-1K[69],few-shotlearning[24],FGVC[22]anddomainadaptation[73].
Datasetsandevluations. VTAB-1Kcomprises19datasetsclusteredinto(i)Naturalimages,(ii)
Specializeddatasets(remotesensing,medical)and(iii)Structureddatasets(scenestructure)domains.
Eachdatasethas1Ktrainingexamples. Following[69,22],weusetheprovided800-200trainsplit
forhyperparameterselection,evaluateusingthefulltrainingsetandreportaverageaccuracyacross
threetrails. Few-shotlearninginvolves5fine-graineddatasets: FGVC-Aircraft[40],Food101[3],
OxfordFlowers102 [45], OxfordPets [47] and StanfordCars [29]. Following [24], we evaluate 1,
2,4,8and16shots,trainontheprovidedtrainingset,tunehyperparametersusingvalidationand
reportaveragetestaccuracyoverthreerandomseeds. FGVCincludes5fine-graineddatasets: CUB-
200-2011[62],NABirds[59],OxfordFlowers[45],StanfordDogs[7]andStanfordCars[29]. We
follow[22]tousevalidationsetforhyperparameterandreporttestresults. Fordomainadaptation,
following[73,5],wetrainonImageNet[8]witha16-shotsetting,usethevalidationsplitby[73]
forhyperparameterselectionandreporttheresultsontheofficialvalidationsetand4out-of-domain
datasets: ImageNet-Sketch[63],ImageNet-V2[50],ImageNet-A[17]andImageNet-R[16].
Pre-trainedbackbones. Weexperimentwithtwovisiontransformers,VisionTransforms(ViT-B/16)
[12]andSwinTransformer(Swin-B)[38]. Thesetwoarepre-trainedonImageNet-21K[8]. Wetest
aViT-B-Laion-IN12Kmodel,pre-trainedonLaion-2B[54]andfine-tunedonImageNet-12K[8].
Implementationdetails. Wefollow[22]forimageprocessing. 224 224resizingforVTAB-1K;
×
randomflipsandcropsto224 224forFGVCandfew-shotlearning;strongeraugmentationfor
×
domainadaptationtask,following[12,73,36]. WeusetheAdamoptimizer[26]withcosinelearning
6Table1: ResultsonVTAB-1KwithViT-B/16. MeanAcc. istheaverageofgroupmeanvalues.
Natural Specialized Structured
Method
Full 68.9 87.7 64.3 97.3 86.9 87.4 38.8 79.7 95.7 84.2 73.9 56.3 58.6 41.7 65.5 57.5 46.7 25.7 29.1 68.9
Linear 64.4 85.0 63.2 97.0 86.3 36.6 51.0 78.5 87.5 68.5 74.0 34.3 30.6 33.2 55.4 12.5 20.0 9.6 19.2 57.6
VPT-Deep 78.8 90.8 65.8 98.0 88.3 78.1 49.6 81.8 96.1 83.4 68.4 68.5 60.0 46.5 72.8 73.6 47.9 32.9 37.8 72.0
Adapter 69.2 90.1 68.0 98.8 89.9 82.8 54.3 84.0 94.9 81.9 75.5 80.9 65.3 48.6 78.3 74.8 48.5 29.9 41.6 73.9
AdaptFormer 70.8 91.2 70.5 99.1 90.9 86.6 54.8 83.0 95.8 84.4 76.3 81.9 64.3 49.3 80.3 76.3 45.7 31.7 41.1 74.7
LoRA 67.1 91.4 69.4 98.8 90.4 85.3 54.0 84.9 95.3 84.4 73.6 82.9 69.2 49.8 78.5 75.7 47.1 31.0 44.0 74.5
NOAH 69.6 92.7 70.2 99.1 90.4 86.1 53.7 84.4 95.4 83.9 75.8 82.8 68.9 49.9 81.7 81.8 48.3 32.8 44.2 74.2
RepAdapter 69.0 92.6 75.1 99.4 91.8 90.2 52.9 87.4 95.9 87.4 75.5 75.9 62.3 53.3 80.6 77.3 54.9 29.5 37.9 76.1
RLRR 75.6 92.4 72.9 99.3 91.5 89.8 57.0 86.8 95.2 85.3 75.9 79.7 64.2 53.9 82.1 83.9 53.7 33.4 43.6 76.7
GLoRA 76.4 92.9 74.6 99.6 92.5 91.5 57.8 87.3 96.8 88.0 76.0 83.1 67.3 54.5 86.2 83.8 52.9 37.0 41.4 78.0
Baseline 74.9 93.3 72.0 99.4 91.0 91.5 54.8 83.2 95.7 86.9 74.2 83.0 70.5 51.9 81.4 77.9 51.7 33.6 44.4 76.4
+PACE 79.0 94.2 73.6 99.4 92.4 93.7 58.0 87.4 96.4 89.3 77.1 84.9 70.9 54.9 84.3 84.7 57.3 39.3 44.8 79.0
Table2: ClassificationaccuracyonFew-shotlearningwithViT-B/16pretrainedonImageNet-21K.
Shot FGVCAircraft Food101 Flowers102
Method 1 2 4 8 16 1 2 4 8 16 1 2 4 8 16
LoRAadd 10.4 15.2 27.2 41.7 59.2 33.9 51.9 59.3 66.0 71.3 93.3 96.4 98.0 98.6 98.7
+PACE 10.7 16.3 28.2 42.1 61.0 40.6 55.9 63.8 70.3 75.2 95.0 98.0 98.9 99.5 99.6
VPTadd 11.2 15.1 23.7 36.3 51.5 34.3 56.6 64.8 71.7 75.4 94.3 97.6 98.2 99.3 99.6
+PACE 11.6 16.2 24.0 37.0 52.4 39.9 57.2 66.7 72.4 76.1 95.3 97.8 98.6 99.4 99.6
LoRAadd+VPTadd 10.5 15.6 28.4 44.8 61.8 35.4 54.3 64.8 72.1 76.4 90.4 97.3 98.4 99.4 99.5
+PACE 12.3 16.8 29.9 45.7 62.5 39.3 57.2 66.7 73.4 77.8 93.4 98.1 99.1 99.5 99.7
OxfordPets StanfordCars Average
LoRAadd 73.2 83.1 87.5 89.2 91.1 8.7 15.3 30.2 55.3 74.5 43.9 52.3 60.4 70.1 78.9
+PACE 75.3 85.0 90.7 90.8 92.4 9.4 16.0 30.9 56.1 75.9 46.2 54.2 62.5 71.7 80.8
VPTadd 75.9 85.6 90.3 90.6 92.3 9.3 15.0 27.8 46.6 65.1 45.0 53.9 60.9 68.9 76.7
+PACE 78.2 87.4 90.3 91.1 92.3 9.9 15.4 27.9 47.0 65.9 46.9 54.8 61.5 69.3 77.2
LoRAadd+VPTadd 69.9 84.1 89.1 91.3 91.9 9.0 16.3 32.7 59.0 76.4 43.0 53.5 62.6 73.2 81.2
+PACE 76.5 88.0 90.3 91.4 92.4 9.7 16.4 33.7 59.8 77.3 46.2 55.3 63.9 73.9 81.9
ratedecayandalinearwarm-upforthefirst10epochs. Modelsarefine-tunedfor300epochson
VTAB-1Kand100epochsonFGVC,few-shotlearninganddomainadaptationtasks,withabatch
sizeof64. AllexperimentswereconductedonanNVIDIAH100GPUwith96GBmemory.
Baseline. Foreachdataset,weidentifiedthebettermethod(LoRA +VPT orLoRA )andtuned
mul add add
therank,learningrate,andweightdecaytoformastrongbaseline. Thedetailedbaselinesettings
foreachtaskandthenumberoftrainableparameters,areprovidedin§D,whereLoRA +VPT
mul add
generallyoutperformedothervariants. BuildingonstrongbaselineLoRA +VPT ,weusegrid
mul add
searchforourhyper-parametersλandσ,followingstrategiesfrompreviousstudies[22,36,20].
4.1 ComparisonwiththeStateoftheArts
Results on VTAB-1K. Table 1 presents the results comparing PACE with recent state-of-the-art
PEFTmethods. PACEimprovesthestrongbaselineby2.6%accuracy,surpassingthepreviousSOTA
GLoRA[5]by1%,whichusestwostageslearningforneuralparametersearch.
Results on Few-shot Learning. Table 2 compares performance w/ and w/o our PACE. PACE
improvesLoRA ,VPT ,LoRA +VPT ,withLoRA +VPT +PACEperformingbestin
add add mul add mul add
mostcases. PACEyieldsnotableimprovement,especiallywhenthenumberofshotissmall.
ResultsonFGVC.Table3showsthatPACEimprovesthestrongLoRA +VPT by0.7%,outper-
mul add
formingSSF[36],ARC[10]andRLRR[11]thatusestronglypre-trainedViTwithaugmentations.
7
001rafiC 101hcetlaC
DTD
201srewolF
steP
NHVS 793nuS noylemaC TASoruE 54csiseR yhtaponiteR tnuoC-rvelC tsiD-rvelC baLMD tsiD-ITTIK coL-rpSd irO-rpSd
mizA-BRONs
elE-BROsN .ccAnaeMTable5: ClassificationresultsondomainadaptationandCIFAR-100inVTAB-1Kbaseddifferent
pretrainedmodels. Src. isshortfor‘source’inTable4.
ViT-B(ImageNet-21K) ViT-B(Laion2B-ImageNet-12K) Swin-B(ImageNet-21K)
Method CIFAR ImageNet-1K CIFAR ImageNet-1K CIFAR ImageNet-1K
-100 Src. -S -V -A -R -100 Src. -S -V -A -R -100 Src. -S -V -A -R
Full 51.6 63.918.552.5 3.2 21.2 51.2 66.029.056.1 8.1 27.9 65.6 71.727.061.110.824.4
Linear 63.4 67.914.460.8 9.4 25.6 61.9 79.243.269.523.440.9 65.0 78.836.768.823.235.9
LoRAadd 71.2 73.827.164.813.625.0 71.3 77.539.867.820.435.6 74.3 76.330.765.716.828.9
VPTadd 73.6 74.327.165.911.526.7 71.8 78.440.468.722.438.4 72.7 76.230.666.217.629.1
LoRAmul 73.4 78.131.268.313.432.7 73.2 78.641.968.822.637.8 73.9 76.130.865.718.128.9
LoRAadd+VPTadd 70.3 76.828.766.613.729.9 71.8 78.041.468.320.636.9 74.5 76.330.765.716.828.9
LoRAmul+VPTadd 74.9 78.330.668.514.132.5 73.8 78.341.568.621.638.2 74.6 76.631.266.518.529.4
+PACE 79.0 79.031.869.416.335.2 78.0 80.145.871.224.643.6 78.9 79.639.270.125.238.0
Table3: ResultsonFGVCwithViT-B/16. Table4: ResultsondomainadaptationwithViT-
*denotesusingaugmentedViTbyAugReg[56]. B/16pretrainedonImageNet-21K.
CUB NA- Oxford Stan. Stan.Mean Source Target Mean
Method Method
-2011BirdsFlowersDogs Cars Acc. ImageNet -Sketch -V2 -A -R Acc.
Full 87.3 82.7 98.8 89.4 84.5 85.9 Full 63.9 18.5 52.5 3.2 21.2 31.8
Linear 85.3 75.9 97.9 86.2 51.3 79.3 Linear 67.9 14.4 60.8 9.4 25.6 35.6
VPT 88.5 84.2 99.0 90.2 83.6 89.1 Adapter 70.5 16.4 59.1 5.5 22.1 34.7
LoRA 88.3 85.6 99.2 91.0 83.2 89.5 VPT 70.5 18.3 58.0 4.6 23.2 34.7
SSF* 89.5 85.7 99.6 89.6 89.2 90.7 LoRA 70.8 20.0 59.3 6.9 23.3 36.0
ARC* 89.3 85.7 99.7 89.1 89.5 90.7 NOAH 71.5 24.8 66.111.928.5 40.5
RLRR* 89.8 85.3 99.6 90.0 90.4 91.0 GLoRA 78.3 30.6 67.513.331.0 44.1
LoRAmul+VPTadd 88.9 87.1 99.4 91.2 87.5 90.8 LoRAmul+VPTadd 78.3 30.6 68.514.132.5 44.8
+PACE 89.8 87.3 99.5 92.2 88.8 91.5 +PACE 79.0 31.8 69.416.335.2 46.3
Resultsondomainadaptation. Table4comparesPACEwithothers. LoRA +VPT outperforms
mul add
GLoRA[5]whichreliesonparametersearch. Meanwhile,PACEimprovesLoRA +VPT by
mul add
1.5%,outperformingotherPEFTmethods,demonstratingsuperiorperformanceondomainadaptation.
Generalizetootherbackbones. WeevaluatePACEonCIFAR-100(VTAB-1K)anddomainadapta-
tionusingSwin-B[38]pretrainedonImageNet-21KandViT-B(pretrainedonLaion2B,thenfine-
tunedonImageNet-12K).Table5showsPACEeffectivelyoutperformsbaselineLoRA +VPT
mul add
andotherPEFTmethodsacrossallbackbones,demonstratingitseffectivegeneralizability.
4.2 Analyses
To verify our theories, we conduct experiments on CIFAR-100 (VTAB-1K) using ViT-B/16 and
Camelyon(VTAB-1K)onSwin-B.Figure2&3plotthegradientnormandFP-distance(Eq. 5)and
thetrain&validationaccuracyduringtrainingforbaselineLoRA +VPT andPACEonvalidation
mul add
set. Figures2a&3ashowthatPACEhasasmallergradientnormthanbaseline,verifyingTheorem
2thatPACEcanimplicitlylowertheweightgradientnormforbettergeneralization. Figures2b&
3bdemonstratethatPACEmaintainsalowerFP-distancethanthebaseline,verifyingTheorem3
thatPACEcanimplicitlyalignthefine-tunedmodelwithpre-trainedmodel,retainingknowledge
developedfromlarge-scalepre-training. Owingtotheadvantagesofthegradientregularizationand
modelalignment,PACEshortenstheperformancegapbetweenseenandunseendata,yieldinghigher
classificationaccuracyontheunseenvalidationset,asshowninFigures2c&3c.
Toclarifywhynaivealignmentisproblematic,wevarytheregularizationstrengthλoverawide
range(1e-3to5e4)forbothFine-tunedPre-trainedmodelAlignment(FPA)byminimizingDfpin
Eq. 5)andPACE.Figure4plotstheaveragedgradientnormovertraining(seealsoFigures7&8
formorevisualizations). PACErobustlylowersgradientnormswithlargerλ,whileFPAexhibits
unpredictablebehavior,evencausinggradientexplosion. ThisverifiesProp. 1thatminimizingDfpis
problematicforgradientregularization,complicatinggradientmanagement.
4.3 Ablationstudies
WeablatePACEbasedonthebaselineLoRA +VPT onCIFAR-100(VTAB-1K)andImageNet-
mul add
1KindomainadaptionasshowninTable6.TheablationsincludeNoise(baselinew/noiseperturbing
adapter),PACE (replacingmultiplicativenoisewithadditivenoise),PACE (perturbinghinstead
add h
of∆hinEq. 11),PACE (replacingGaussiannoisewithdropoutnoise),PACE (alltransformer
drop σ=
8∂f Dfp Acc
∥∂θ∥2
12e3 140 100
Baseline Baseline trainacc Baseline
9e3 100 90
+PACE +PACE valacc +PACE
6e3 60 80
3e3 20 70
epoch= 100 200 300 epoch= 100 200 300 epoch= 100 200 300
(a)GradientNorm. (b)FP-Distance (c)Trainandvalidationaccuracy.
Figure2: AnalysisforPACE.(a)gradientnorm,(b)FP-Distanceand(c)train&valaccuracy,are
evaluatedonvalidationsetofCIFAR-100(VTAB-1K)withbaselineLoRA +VPT onViT-B/16.
mul add
∂f Dfp Acc
∥∂θ∥2
8e3 100 1.00
Baseline trainacc Baseline
6e3 70 0.95
+PACE valacc +PACE
4e3 Baseline 40 0.90
+PACE
2e3 10 0.85
epoch= 100 200 300 epoch= 100 200 300 epoch= 100 200 300
(a)GradientNorm. (b)FP-distance (c)Trainandvalidationaccuracy.
Figure3: AnalysisforPACE.(a)gradientnorm,(b)FP-Distanceand(c)train&valaccuracy,are
evaluatedonvalidationsetofCamelyon(VTAB-1K)withbaselineLoRA +VPT onSwin-B.
mul add
blocks share the same σ), PACE (σ increases linearly with depth), FPA (fine-tuned and pre-
σ↑
trinedalignmentbyminimizingEq. 5),SAM(sharpness-awareminimization[13]),GP(gradient
penalization),ℓ (sparsityregularization). Wegrid-searchhyperparametersandreportthebestresults.
1
Table6presentstheresultsforallvariants. PACEimprovesoverNoise,whichitselfisbetterthan
baseline,justifyingouradapterperturbationandconsistencyregularization. PACE performsworse
add
thanPACE,showingthesuperiorityofmultiplicativenoise.AlthoughPACE canimplicitlyregularize
h
gradients,itunderperformsPACE,verifyingtheadvantagesofperturbingadaptertoimplicitlyalign
models. PACE isworsethanPACE,indicatingdropoutnoiseissuboptimal. PACE andPACE
drop σ= σ↑
performsworse,justifyingourdesignoflinearlydecreasingσ. FPA,SAMandGP,whicheitheronly
alignmodelsoronlyregularizegradients,areoutperformedbyPACE.DespitecombiningFPA+GP,it
stillunderperformsours,suggestingineffectivecombination. ℓ obtainsworseresultsthanPACE,
1
verifyingineffectivenessofsparseregularizationforimprovinggeneralization. PACEregularizes
gradientsforbettergeneralizationandalignmodelstoretainknowledge,surpassingallothervariants.
WefurtherevaluateapplyingPACEacrossmultipleM networksduringtrainingorapplyingitlazily
ateveryN steps. Figure5presentstheresults,showingthatapplyingPACEamongtwonetworksat
everytrainingstepyieldsthebestresults. However,lazyregularizationappliedeveryfewstepscan
stillprovidereasonableresultswhilesavingcomputationtime.
WetestthesensitivityofhyperparameterλandσintroducedinourPACEonOxfordPetsforfew-shot
learningaccross1,2,4,8shots. TheresultspresentedinFigure6demonstratethatwithlessdata,
largerλandσarefavoured,verifyingthattheeffectivenessofPACEinimprovinggeneralization.
5 Conclusions
WehaveintroducedPACE,anovelandeffectivemethodthatcombinesgeneralizationofPArameter-
efficientfine-tuningwithConsistencyrEgularization. Throughrigoroustheoreticalanalyses,wehave
shownPACEreducesweightgradientforimprovedgeneralizationandalignsthefine-tunedmodel
withthepre-trainedmodelforretainingpre-trainingknowledge. Ourexperimentalresultssupport
thetheoreticalanalyses,justifyingthegeneralizationadvantagesofPACEoverotherPEFTmethods.
Withitsdualadvantages,PACEconsistentlyoutperformsothervariantsacrossdifferentbackbones,
firmlyestablishingPACEasapowerfulsolutionforenhancinggeneralizationforPEFTmethods.
Limitationsandborderimpactsarediscussedin§A.
9∂f 79.0
∥∂θ∥2 Baseline
78.5
+FPA
78.0
2e4 +PACE
M=2 3 4 5 6 7 8
79.0
1e4
78.5
78.0
λ=1e-35e-30.010.05 0.1 0.5 1 5 10 50 100 500 1e3 5e3 1e4 5e4
N=1 2 4 6 8 10 12
Figure4: Gradientnormsofmodelsacrosswiderangeofregu- Figure5: Ablationresultsforap-
larizationstrengthsλonCIFAR-100(VTAB-1K)w/ViT-B/16. plyingPACEamongM networks
Lineandshadowrepresentmeanandstdacrosstrainingepochs. andateveryN steps.
Method CIFAR ImageNet-1K λ 1-Shot λ 2-Shot
-100 Source-Sketch -V2 -A -R 0.02 0.02
75 87
LoRAmul+VPTadd 74.9 78.3 30.6 68.514.132.5 0.05 0.05
+Noise 77.4 78.3 31.3 68.614.333.0 0.1 72 0.1 86
+PACE 79.0 79.0 31.8 69.416.335.2 0.2 69 0.2 85
+ +P PA AC CE Ea md ed rge 7 75 5. .7 9 7 78 8. .3 4 3 31 1. .2 2 6 68 8. .7 11 13 3. .7 83 32 2. .7 6 0 σ.5 = 0.1 0.2 0.5 1.0 1.5 66 0 σ.5 = 0.1 0.2 0.5 1.0 1.5 84
+PACEdrop 78.3 78.9 31.2 68.916.034.6
4-Shot 8-Shot
+ +P PA AC CE Eσ σ= ↑ 7 77 7. .9 3 7 78 8. .8 7 3 31 1. .6 3 6 68 8. .3 91 16 4. .6 033 34 .. 67 0.0λ 2 90 0.0λ 2 91
+FPA 76.6 78.8 31.2 68.614.733.5 0.05 0.05
+SAM 75.4 78.4 31.4 68.513.832.9 0.1 89 0.1
+GP 75.8 78.3 31.7 68.414.232.1 0.2 0.2
+FPA+GP 74.9 78.1 31.5 68.113.532.6 0.5 0.5
+ℓ1 75.2 78.2 30.6 68.613.732.8 σ= 0.1 0.2 0.5 1.0 1.5 σ= 0.1 0.2 0.5 1.0 1.5 90
Table6: Accuracyresultsondomainadaptation Figure 6: Results for varied λ and σ as well as
andVTAB-1Kbaseddifferentpretrainedmodels. shotondatasetOxfordPetsinfew-shotlearning.
Acknowledgements
We thank Moyang Liu, Melody Ip, Chenyi Du, and Yinuo Xu for their valuable discussions and
support. YNisfundedbyChinaScholarshipCouncil,whilePKbyCSIRO’sScienceDigital.
References
[1] GuillaumeAlainandYoshuaBengio. Whatregularizedauto-encoderslearnfromthedata-generating
distribution. JMLR,15(110):3743–3773,2014. 4
[2] DzmitryBahdanau,KyunghyunCho,andYoshuaBengio. Neuralmachinetranslationbyjointlylearning
toalignandtranslate. arXivpreprintarXiv:1409.0473,2014. 1
[3] LukasBossard,MatthieuGuillaumin,andLucVanGool. Food-101–miningdiscriminativecomponents
withrandomforests. InComputerVision–ECCV2014:13thEuropeanConference,Zurich,Switzerland,
September6-12,2014,Proceedings,PartVI13,pages446–461.Springer,2014. 6
[4] JunbumCha,SanghyukChun,KyungjaeLee,Han-CheolCho,SeunghyunPark,YunsungLee,andSungrae
Park. Swad:Domaingeneralizationbyseekingflatminima. AdvancesinNeuralInformationProcessing
Systems,34:22405–22418,2021. 3
[5] ArnavChavan,ZhuangLiu,DeepakGupta,EricXing,andZhiqiangShen. One-for-all:Generalizedlora
forparameter-efficientfine-tuning. arXivpreprintarXiv:2306.07967,2023. 2,3,6,7,8,17
[6] ShoufaChen,ChongjianGe,ZhanTong,JiangliuWang,YibingSong,JueWang,andPingLuo. Adapt-
former: Adaptingvisiontransformersforscalablevisualrecognition. AdvancesinNeuralInformation
ProcessingSystems,35:16664–16678,2022. 1,2,3
[7] EDataset. Noveldatasetsforfine-grainedimagecategorization. InFirstWorkshoponFineGrainedVisual
Categorization,CVPR.Citeseer.Citeseer.Citeseer,2011. 6
[8] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,pages248–255.
Ieee,2009. 1,6
[9] TimDettmers,ArtidoroPagnoni,AriHoltzman,andLukeZettlemoyer. Qlora: Efficientfinetuningof
quantizedllms. AdvancesinNeuralInformationProcessingSystems,36,2024. 3
[10] WeiDong,DaweiYan,ZhijunLin,andPengWang. Efficientadaptationoflargevisiontransformervia
adapterre-composing. AdvancesinNeuralInformationProcessingSystems,36,2024. 2,7
10[11] Wei Dong, Xing Zhang, Bihui Chen, Dawei Yan, Zhijun Lin, Qingsen Yan, Peng Wang, and Yang
Yang. Low-rankrescaledvisiontransformerfine-tuning: Aresidualdesignapproach. arXivpreprint
arXiv:2403.19067,2024. 2,7
[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit,
andNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionatscale. In
InternationalConferenceonLearningRepresentations,2021. 1,3,6
[13] PierreForet,ArielKleiner,HosseinMobahi,andBehnamNeyshabur. Sharpness-awareminimizationfor
efficientlyimprovinggeneralization. InInternationalConferenceonLearningRepresentations,2021. 3,4,
9
[14] Zihao Fu, Haoran Yang, Anthony Man-Cho So, Wai Lam, Lidong Bing, and Nigel Collier. On the
effectivenessofparameter-efficientfine-tuning. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume37,pages12799–12807,2023. 2,3,4
[15] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRossGirshick. Momentumcontrastforunsupervised
visualrepresentationlearning.InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pages9729–9738,2020. 1
[16] DanHendrycks,StevenBasart,NormanMu,SauravKadavath,FrankWang,EvanDorundo,RahulDesai,
Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of
out-of-distributiongeneralization. InProceedingsoftheIEEE/CVFinternationalconferenceoncomputer
vision,pages8340–8349,2021. 6
[17] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial
examples. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
15262–15271,2021. 6
[18] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentinDeLaroussilhe,AndreaGes-
mundo,MonaAttariyan,andSylvainGelly. Parameter-efficienttransferlearningfornlp. InInternational
conferenceonmachinelearning,pages2790–2799.PMLR,2019. 1
[19] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentinDeLaroussilhe,AndreaGes-
mundo,MonaAttariyan,andSylvainGelly. Parameter-efficienttransferlearningfornlp. InInternational
conferenceonmachinelearning,pages2790–2799.PMLR,2019. 2,3
[20] EdwardJHu,yelongshen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. LoRA:Low-rankadaptationoflargelanguagemodels. InInternationalConferenceon
LearningRepresentations,2022. 2,3,7
[21] ShengdingHu,ZhenZhang,NingDing,YadaoWang,YashengWang,ZhiyuanLiu,andMaosongSun.
Sparsestructuresearchfordeltatuning.InAliceH.Oh,AlekhAgarwal,DanielleBelgrave,andKyunghyun
Cho,editors,AdvancesinNeuralInformationProcessingSystems,2022. 2
[22] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and
Ser-NamLim. Visualprompttuning. InEuropeanConferenceonComputerVision, pages709–727.
Springer,2022. 2,4,6,7,17
[23] ZeyinziJiang,ChaojieMao,ZiyuanHuang,AoMa,YiliangLv,YujunShen,DeliZhao,andJingrenZhou.
Res-tuning: Aflexibleandefficienttuningparadigmviaunbindingtunerfrombackbone. Advancesin
NeuralInformationProcessingSystems,36,2024. 2
[24] ShiboJieandZhi-HongDeng. Fact:Factor-tuningforlightweightadaptationonvisiontransformer. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume37,pages1060–1068,2023. 2,6
[25] MuhammadUzairKhattak,SyedTalalWasim,MuzammalNaseer,SalmanKhan,Ming-HsuanYang,and
FahadShahbazKhan. Self-regulatingprompts: Foundationalmodeladaptationwithoutforgetting. In
ProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages15190–15200,2023. 3
[26] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014. 6
[27] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,TeteXiao,
SpencerWhitehead,AlexanderCBerg,Wan-YenLo,etal. Segmentanything. InProceedingsofthe
IEEE/CVFInternationalConferenceonComputerVision,pages4015–4026,2023. 1
[28] Dawid Jan Kopiczko, Tijmen Blankevoort, and Yuki M Asano. VeRA: Vector-based random matrix
adaptation. InTheTwelfthInternationalConferenceonLearningRepresentations,2024. 3
[29] JonathanKrause,MichaelStark,JiaDeng,andLiFei-Fei. 3dobjectrepresentationsforfine-grained
categorization. InProceedingsoftheIEEEinternationalconferenceoncomputervisionworkshops,pages
554–561,2013. 6
[30] YoonhoLee,AnnieSChen,FahimTajwar,AnanyaKumar,HuaxiuYao,PercyLiang,andChelseaFinn.
Surgicalfine-tuningimprovesadaptationtodistributionshifts. InTheEleventhInternationalConference
onLearningRepresentations,2023. 2
[31] Bonan Li, Yinhan Hu, Xuecheng Nie, Congying Han, Xiangjian Jiang, Tiande Guo, and Luoqi Liu.
Dropkeyforvisiontransformer. InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pages22700–22709,2023. 6
11[32] Dongyue Li and Hongyang Zhang. Improved regularization and robustness for fine-tuning in neural
networks. AdvancesinNeuralInformationProcessingSystems,34:27249–27262,2021. 3
[33] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip:Bootstrappinglanguage-imagepre-training
forunifiedvision-languageunderstandingandgeneration.InInternationalconferenceonmachinelearning,
pages12888–12900.PMLR,2022. 1
[34] ShengruiLi,XuetingHan,andJingBai. Adaptergnn: Parameter-efficientfine-tuningimprovesgener-
alizationingnns. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume38,pages
13600–13608,2024. 2
[35] XingjianLi,HaoyiXiong,HanchaoWang,YuxuanRao,LipingLiu,ZeyuChen,andJunHuan. Delta:
Deep learning transfer using feature map with attention for convolutional networks. arXiv preprint
arXiv:1901.09229,2019. 3
[36] DongzeLian,DaquanZhou,JiashiFeng,andXinchaoWang. Scaling&shiftingyourfeatures: Anew
baselineforefficientmodeltuning. InAdvancesinNeuralInformationProcessingSystems(NeurIPS),
2022. 2,3,6,7,17
[37] WeiyangLiu, ZejuQiu, YaoFeng, YuliangXiu, YuxuanXue, LonghuiYu, HaiwenFeng, ZhenLiu,
JuyeonHeo,SongyouPeng,YandongWen,MichaelJ.Black,AdrianWeller,andBernhardSchölkopf.
Parameter-efficientorthogonalfinetuningviabutterflyfactorization. InICLR,2024. 2
[38] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo. Swin
transformer: Hierarchicalvisiontransformerusingshiftedwindows. InProceedingsoftheIEEE/CVF
InternationalConferenceonComputerVision(ICCV),2021. 1,6,8
[39] GenLuo,MinglangHuang,YiyiZhou,XiaoshuaiSun,GuannanJiang,ZhiyuWang,andRongrongJi.
Towardsefficientvisualadaptionviastructuralre-parameterization. arXivpreprintarXiv:2302.08106,
2023. 2,3,17
[40] SubhransuMaji,EsaRahtu,JuhoKannala,MatthewBlaschko,andAndreaVedaldi. Fine-grainedvisual
classificationofaircraft. arXivpreprintarXiv:1306.5151,2013. 6
[41] YaoNiandPiotrKoniusz. Chain:Enhancinggeneralizationindata-efficientgansvialipschitzcontinuity
constrainednormalization. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),pages6763–6774,June2024. 3
[42] Yao Niand PiotrKoniusz. Nice: Noise-modulated consistency regularizationfor data-efficientgans.
AdvancesinNeuralInformationProcessingSystems,36,2024. 2,15
[43] YaoNi,PiotrKoniusz,RichardHartley,andRichardNock.Manifoldlearningbenefitsgans.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages11265–11274,2022. 3
[44] Yao Ni, Dandan Song, Xi Zhang, Hao Wu, and Lejian Liao. Cagan: Consistent adversarial training
enhancedgans. InIJCAI,pages2588–2594,2018. 2
[45] Maria-ElenaNilsbackandAndrewZisserman. Avisualvocabularyforflowerclassification. InIEEE
ConferenceonComputerVisionandPatternRecognition,volume2,pages1447–1454,2006. 6
[46] ChangdaeOh,HyejiHwang,Hee-youngLee,YongTaekLim,GeunyoungJung,JiyoungJung,HosikChoi,
andKyungwooSong. Blackvip:Black-boxvisualpromptingforrobusttransferlearning. InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages24224–24235,2023. 2
[47] OmkarM.Parkhi, AndreaVedaldi, AndrewZisserman, andC.V.Jawahar. Catsanddogs. InIEEE
ConferenceonComputerVisionandPatternRecognition,2012. 6
[48] ZejuQiu,WeiyangLiu,HaiwenFeng,YuxuanXue,YaoFeng,ZhenLiu,DanZhang,AdrianWeller,and
BernhardSchölkopf. Controllingtext-to-imagediffusionbyorthogonalfinetuning. InNeurIPS,2023. 2
[49] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,
2021. 1
[50] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers
generalizetoimagenet? InInternationalconferenceonmachinelearning,pages5389–5400.PMLR,2019.
6
[51] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10684–10695,2022. 1
[52] ShuvenduRoyandAliEtemad. Consistency-guidedpromptlearningforvision-languagemodels. InThe
TwelfthInternationalConferenceonLearningRepresentations,2024. 3
[53] KuniakiSaito,DonghyunKim,andKateSaenko. Openmatch:Open-setsemi-supervisedlearningwith
open-setconsistencyregularization.AdvancesinNeuralInformationProcessingSystems,34:25956–25967,
2021. 2
[54] ChristophSchuhmann,RomainBeaumont,RichardVencu,CadeWGordon,RossWightman,Mehdi
Cherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,PatrickSchramowski,SrivatsaR
Kundurthy,KatherineCrowson,LudwigSchmidt,RobertKaczmarczyk,andJeniaJitsev. LAION-5b:An
openlarge-scaledatasetfortrainingnextgenerationimage-textmodels. InThirty-sixthConferenceon
NeuralInformationProcessingSystemsDatasetsandBenchmarksTrack,2022. 1,6
12[55] KihyukSohn,DavidBerthelot,NicholasCarlini,ZizhaoZhang,HanZhang,ColinARaffel,EkinDogus
Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with
consistencyandconfidence. Advancesinneuralinformationprocessingsystems,33:596–608,2020. 2
[56] AndreasPeterSteiner,AlexanderKolesnikov,XiaohuaZhai,RossWightman,JakobUszkoreit,andLucas
Beyer. Howtotrainyourvit?data,augmentation,andregularizationinvisiontransformers. Transactions
onMachineLearningResearch,2022. 8
[57] YushengSu,XiaozhiWang,YujiaQin,Chi-MinChan,YankaiLin,HuadongWang,KaiyueWen,Zhiyuan
Liu,PengLi,JuanziLi,etal. Ontransferabilityofprompttuningfornaturallanguageprocessing. arXiv
preprintarXiv:2111.06719,2021. 1,2
[58] JunjiaoTian,Yen-ChengLiu,JamesSSmith,andZsoltKira.Fasttrainableprojectionforrobustfine-tuning.
AdvancesinNeuralInformationProcessingSystems,36,2024. 3
[59] GrantVanHorn,SteveBranson,RyanFarrell,ScottHaber,JessieBarry,PanosIpeirotis,PietroPerona,
andSergeBelongie. Buildingabirdrecognitionappandlargescaledatasetwithcitizenscientists:The
fineprintinfine-graineddatasetcollection. InProceedingsoftheIEEEconferenceoncomputervisionand
patternrecognition,pages595–604,2015. 6
[60] Dániel Varga, Adrián Csiszárik, and Zsolt Zombori. Gradient regularization improves accuracy of
discriminativemodels. arXivpreprintarXiv:1712.09936,2017. 3
[61] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser, andIlliaPolosukhin. Attentionisallyouneed. InI.Guyon, U.VonLuxburg, S.Bengio, H.
Wallach,R.Fergus,S.Vishwanathan,andR.Garnett,editors,AdvancesinNeuralInformationProcessing
Systems,volume30.CurranAssociates,Inc.,2017. 2,3
[62] C.Wah,S.Branson,P.Welinder,P.Perona,andS.Belongie. Thecaltech-ucsdbirds-200-2011dataset.
TechnicalReportCNS-TR-2011-001,CaliforniaInstituteofTechnology,2011. 6
[63] HaohanWang,SongweiGe,ZacharyLipton,andEricPXing. Learningrobustglobalrepresentationsby
penalizinglocalpredictivepower. AdvancesinNeuralInformationProcessingSystems,32,2019. 6
[64] YihanWang,JatinChauhan,WeiWang,andCho-JuiHsieh. Universalityandlimitationsofprompttuning.
AdvancesinNeuralInformationProcessingSystems,36,2024. 2,3
[65] YaomingWang,YuchenLiu,XiaopengZhang,JinLi,BowenShi,ChenglinLi,WenruiDai,Hongkai
Xiong,andQiTian. Violet:Vision-languageefficienttuningwithcollaborativemulti-modalgradients. In
Proceedingsofthe31stACMInternationalConferenceonMultimedia,pages4595–4605,2023. 3
[66] YemingWenandSwaratChaudhuri. Batchedlow-rankadaptationoffoundationmodels. arXivpreprint
arXiv:2312.05677,2023. 2
[67] Lijun Wu, Juntao Li, Yue Wang, Qi Meng, Tao Qin, Wei Chen, Min Zhang, Tie-Yan Liu, et al. R-
drop: Regularizeddropoutforneuralnetworks. AdvancesinNeuralInformationProcessingSystems,
34:10890–10905,2021. 2
[68] LIXuhong, YvesGrandvalet, andFranckDavoine. Explicitinductivebiasfortransferlearningwith
convolutionalnetworks. InInternationalConferenceonMachineLearning,pages2825–2834.PMLR,
2018. 3
[69] XiaohuaZhai,JoanPuigcerver,AlexanderKolesnikov,PierreRuyssen,CarlosRiquelme,MarioLucic,
JosipDjolonga,AndreSusanoPinto,MaximNeumann,AlexeyDosovitskiy,etal. Alarge-scalestudyof
representationlearningwiththevisualtaskadaptationbenchmark. arXivpreprintarXiv:1910.04867,2019.
6
[70] HanZhang,ZizhaoZhang,AugustusOdena,andHonglakLee. Consistencyregularizationforgenerative
adversarialnetworks. arXivpreprintarXiv:1910.12027,2019. 2
[71] LinjunZhang,ZhunDeng,KenjiKawaguchi,AmirataGhorbani,andJamesZou. Howdoesmixuphelp
withrobustnessandgeneralization? InICLR,2021. 4
[72] ShanZhang,YaoNi,JinhaoDu,YanxiaLiu,andPiotrKoniusz. Semantictransferfromheadtotail:
Enlargingtailmarginforlong-tailedvisualrecognition.InProceedingsoftheIEEE/CVFWinterConference
onApplicationsofComputerVision,pages1350–1360,2024. 3
[73] YuanhanZhang,KaiyangZhou,andZiweiLiu. Neuralpromptsearch. arXivpreprintarXiv:2206.04673,
2022. 2,6
[74] Bingchen Zhao, Haoqin Tu, Chen Wei, Jieru Mei, and Cihang Xie. Tuning layernorm in attention:
Towardsefficientmulti-modalLLMfinetuning. InTheTwelfthInternationalConferenceonLearning
Representations,2024. 2
[75] YangZhao,HaoZhang,andXiuyuanHu.Penalizinggradientnormforefficientlyimprovinggeneralization
indeeplearning. InInternationalConferenceonMachineLearning,pages26982–26992.PMLR,2022. 3
13PACE: marrying generalization of PArameter-efficient
fine-tuning with Consistency rEgularization
(Supplementary Material)
YaoNi† ShanZhang† PiotrKoniusz∗,§,†
†TheAustralianNationalUniversity §Data61 CSIRO
†firstname.lastname@anu.edu.au §piotr.koniusz@data61.csiro.au
A Broaderimpactsandlimitations
A.1 Broaderimpacts
OurworkprovidesapowerfulsolutionforimprovinggeneralizationinParameterEfficientFine-
Tuning(PEFT),allowingforeffectivefine-tuningofpre-trainedmodelswhilereducetheheavily
relianceonpretrainingfromscratchusingmassivedata. OuradvancementinPEFT,supportedby
Theorems1,2and3,offernovelinsightsintogradientregularizationandmodelalignment. These
insightsextendbeyondPEFTandcanbeappliedtootherareassuchascontinuallearningandtransfer
learning,potentiallyenhancingtheperformanceandefficiencyofmodelsinvariousdomains. By
leveragingourfindings,practitionerscandevelopmorerobustandadaptablemodelsthatgeneralize
welltonewtasksandenvironments,leadingtomoreintelligentandversatileAIsystems. Intermsof
negativeimpacts,therobustnessofourfine-tuningmethodcouldpotentiallybemisusedtocreate
moreconvincingdeepfakes,raisingconcernsaboutthespreadofmisinformation,manipulationof
publicopinion,andmaliciousactivitiessuchasfraud,blackmail,orharassment.
A.2 Limitations
Whileourworkeffectivelyimprovesgeneralizationability,itintroducesadditionalcomputational
costsbyrequiringinputsamplestobepassedthroughthenetworktwiceforregularization. However,
thiscanbemitigatedbyusinglazyregularization,wherethenetworkisregularizedeveryN steps,as
showninFigure5. Lazyregularizationyieldsreasonableimprovementscomparedtothebaseline;for
example,with12steps,itachievesanaccuracyof78.0comparedtothebaseline’s74.9. Additionally,
ourmethodintroducesextrahyperparametersλandσ,whichrequirecautionduringhyperparameter
search. Nonetheless, Figure 6 suggests that fewer training data requires larger λ and σ values,
providinginsightforhyperparametertuning.
*Thecorrespondingauthor. ThispaperisacceptedbyNeurIPS2024asaspotlight. Thispreliminary
versionwillsoonbeextendedwiththeexperimentsandanalysesfromtherebuttal.
14B Proofs
B.1 ProofofTheorem1
S the ett hti in gg heϵ r-= ord∥ρ ∇ e∇ rθ tθ ∥ e2 r, mw se frp oe mrfo tr hm eTa as ye lc oo rn ed x- po ard ne sr ioT nay inlo tore Rx (cid:16)pa ∥n θs ∥i
2
2o ,n 1o (cid:17)f ,L wD en da er ro ivu en :dθ.Byincorporating
ρ2 n
(cid:16) ρ∇ (cid:17) (cid:16) θ 2 1(cid:17)
LD(θ) ≤LDn θ+
∇
θ +R ∥ ρ2∥2,
n
θ 2
∥ ∥
ρ2 (cid:16) θ 2 1(cid:17)
≈LDn(θ)+ρ ∥∇
θ
∥2+
2 ∇
2∇T θH θ∇ θ+R ∥ ρ2∥2,
n
(14)
∥ θ ∥2
Assumingthattheapproximationdoesnotaltertheinequalityrelationship,i.e.,itpreservesthe
relationonbothsidesandconsideringthelargesteigenvalueofH asλH ,implyingvTH v ≤
θ max θ ≤
λH v 2foranyv,wefurtherboundEq. 14asfollowsandarriveat:
max∥ ∥2
ρ2 (cid:16) θ 2 1(cid:17)
LD(θ) ≤LDn(θ)+ρ ∥∇
θ
∥2+
2
λH max+R ∥ ρ2∥2,
n
B.2 ProofofTheorem2
Theproofismotivatedfrom[42]. Weincludetheproofprocessforcompleteness. Denotem =
1
z 1,m =z 1thusm ,m (0,σ2)
1 2 2 1 2
− − ∼N
dpace =E [f(θ +z ∆θ) f(θ +z ∆θ)]2
z1,z2 0 1
⊙ −
0 2
⊙
=E [f(θ +∆θ+(z 1) ∆θ) f(θ +∆θ+(z 1) ∆θ)]2
z1,z2 0 1
− ⊙ −
0 2
− ⊙
=E [f(θ+m ∆θ) f(θ+m ∆θ)]2 (15)
m1,m2 1
⊙ −
2
⊙
Definingv := m ∆θ andu := m ∆θ, wherev,u (0,σ2diag(∆θ ∆θ)), wecan
1 2
⊙ ⊙ ∼ N ⊙
rewriteEq. 15asfollows:
E [f(θ+v) f(θ+u)]2
v,u
−
E (cid:2) f(θ)+vT∇+ 1 vTHv f(θ) uT∇ 1 uTHu(cid:3)2
v,u
≈ 2 − − − 2
=E (cid:2) vT∇+ 1 vTHv uT∇ 1 uTHu(cid:3)2
v,u
2 − − 2
=E (cid:2) (v u)T∇+ 1 vTHv 1 uTHu(cid:3)2
v,u
− 2 − 2
=E (cid:2) (v u)T∇(cid:3)2 (16)
v,u
−
+E (cid:2)(cid:0) (v u)T∇(cid:1)(cid:0) vTHv uTHu(cid:1)(cid:3) (17)
v,u
− −
1 1
+ E [vTHv]2+ E [uTHu]2 (18)
v u
4 4
1 E (cid:2) (vTHv)(uTHu)]. (19)
v,u
− 2
Next,wederivethefourterms,Eq. 16,17,18,and19,respectivelyasfollows:
Eq. 16. UsingE [(z z )2]=2σ2forz ,z (0,σ2),wecansimplify(Eq. 16)asfollows,
z1,z2 1
−
2 1 2
∼N
notingthattermsrelatedtodifferentdimensionsarecanceledduetozero-meanindependentGaussian
noise:
E (cid:2) (v u)T∇(cid:3)2 =E (cid:2)(cid:88) (v u )2 2(cid:3) =2σ2(cid:88) ∆θ2 2. (20)
v,u − v,u j − j ∇j j∇k
j j
Eq. 17. UtilizingE[z3]=µ3+3µσ2 forz (µ,σ2),andnotingthatE[z3]=0forµ=0,Eq.
∼N
17isderivedas:
E (cid:2)(cid:0) (v u)T∇(cid:1)(cid:0) vTHv uTHu(cid:1)(cid:3)
v,u
− −
=E (cid:2) (vT∇)(vTHv)]+E (cid:2) (uT∇)(uTHu)] E (cid:2) (vT∇)(uTHu)] E (cid:2) (uT∇)(vTHv)]
v u v,u v,u
− −
=2E (cid:2) (vT∇)(vTHv)]=0. (21)
v
15Eq. 18. WefirstdecomposeEq. 18,thendiscusseachcaseandobtainthefinalresult.
1 E [vTHv]2+ 1 E [uTHu]2 = 1 E [vTHv]2 = 1 E (cid:2) (cid:88) v H v v H v (cid:3) . (22)
v u v v j jk k p pq q
4 4 2 2
j,k,p,q
Giventheindependenceofelementsinv,onlytermswithanelementrepeatedtwoorfourtimes
contributenon-zeroresults,leadingtofourdistinct,non-overlappingcases. UsingE[z2]=σ2+µ2
andE[z4]=µ4+6µ2σ2+3σ4forz (µ,σ2),andsimplifyingtoE[z2]=σ2andE[z4]=3σ4
∼N
whenµ=0,wehave:
Case1: j =k =p=q,giventheindependenceofv andv ,wehave:
j p
̸
E (cid:2)(cid:88)(cid:88) v2H v2H (cid:3) = (cid:88) H H E[v2]E[v2]=σ4(cid:88) H H ∆θ2∆θ2. (23)
v j jj p pp jj pp j p jj kk j k
j p̸=j j,p̸=j j,k̸=j
Case2: Forj =p=k =q,theindependenceofv andv simplifiesourcalculation,leadingto:
j k
̸
E (cid:2)(cid:88)(cid:88) v H v v H v (cid:3) = (cid:88) H2 E[v2]E[v2]=σ4 (cid:88) H2 ∆θ2∆θ2. (24)
v j jk k j jk k jk j k jk j k
j k̸=j j,k̸=j j,k̸=j
Case 3: For j = q = k = p, utilizing the independence of v and v as well as the symmetry
j k
̸
H =H ,weobtain:
jk kj
E (cid:2)(cid:88)(cid:88) v H v v H v (cid:3) = (cid:88) H2 E[v2]E[v2]=σ4 (cid:88) H2 ∆θ2∆θ2. (25)
v j jk k k kj j jk j k jk j k
j k̸=j j,k̸=j j,k̸=j
Case4: Forj =q =k =p,usingE[z4]=3σ4wherez (0,σ2),wehave:
∼N
(cid:104)(cid:88) (cid:105) (cid:88) (cid:88)
E v H v v H v = H2 E[v4]=3σ4 H2 ∆θ4. (26)
v j jj j j jj j jj j jj j
j j j
Combiningabovefourcasestogether,wehavetheresultforEq. 18:
σ4(cid:16)(cid:88) (cid:88) (cid:17)
3H2 ∆θ4+ (H H +2H2 )∆θ2∆θ2 . (27)
2 jj j jj kk jk j k
j j,k̸=j
Eq. 19:
1 E (cid:2) (vTHv)(uTHu)]
v,u
− 2
= 1 E (cid:2) (vTHv)(cid:3)E (cid:2) (uTHu)(cid:3)
v u
− 2
= 1 E (cid:2)(cid:88) H v2(cid:3)E (cid:2)(cid:88) H v2(cid:3)
− 2 v jj j u kk k
j k
1(cid:16)(cid:88) (cid:17)(cid:16)(cid:88) (cid:17)
= H E[v2] H E[v2]
− 2 jj j kk k
j k
σ4(cid:16)(cid:88) (cid:88) (cid:17)
= H2 ∆θ4+ H H ∆θ2∆θ2 . (28)
− 2 jj j jj kk j k
j j,k̸=j
WithresultsofEq. 20,21,27,28,wehavethefinalresults:
(cid:88)
dpace 2σ2 ∆θ2 2+0
≈ j∇j
j
σ4(cid:16)(cid:88) (cid:88) (cid:88) (cid:88) (cid:17)
+ 3H2 ∆θ4+ (H H +2H2 )∆θ2∆θ2 H2 ∆θ4 H H ∆θ2∆θ2
2 jj j jj kk jk j k− jj j − jj kk j k
j j,k̸=j j j,k̸=j
(cid:88) (cid:16)(cid:88) (cid:88) (cid:17)
=2σ2 ∆θ2 2+σ4 H2 ∆θ4+ H2 ∆θ2∆θ2
j∇j jj j jk j k
j j j,k̸=j
(cid:88) (cid:88)
=2σ2 ∆θ2 2 +σ4 H2 ∆θ2∆θ2 =2σ2 ∆θ ∇ 2+σ4 (∆θ∆θT) H 2 (29)
j∇k jk j k ∥ ⊙ ∥2 ∥ ⊙ ∥F
j j,k
16B.3 ProofofTheorem3
TheCauchy-Schwarzinequalitystatesthatforu,v Rd,wehave((cid:80) u v )2 ((cid:80) u2)((cid:80) v2).
Letu=1,itfollowsthat((cid:80) v )2 d v 2. Using∈ thisinequality,wj ethj enj pro≤ vethej foj llowinj g:j
j j ≤ ∥ ∥2
1
[∆θT∇ ∆θTH∆θ]2 2[∆θT∇]2+[∆θTH∆θ]2
− 2 ≤
(cid:16)(cid:88) (cid:17)2
[∆θT∇]2 = ∆θ d ∆θ ∇ 2 (30)
j ∇j ≤ ∥ ⊙ ∥2
j
[∆θTH∆θ]2 =(cid:16)(cid:88) ∆θ j∆θ kH jk(cid:17)2 ≤d2(cid:13) (cid:13)(∆θ∆θT) ⊙H(cid:13) (cid:13)2
F
(31)
j,k
Here,theinequalityisobtainedbytreating∆θ ∆θ H asanelementofavectorwithsizeofd2.
j k jk
Thisleadstothefinalresults.
C AdditionalPlots
Baseline 0.001 0.005 0.01 0.05 0.1 0.5
3e4 3e4
2e4 2e4
1e4 1e4
epoch= 100 200 300 epoch= 100 200 300
(a)FPA (b)PACE
Figure7: Gradientnormsof(a)FPAand(b)PACEwithdifferentregularizationstrengthsλduring
trainingonCIFAR-100(VTAB-1K)w/ViT-B/16. Figure4illustratestheaveragegradientnormover
trainingepochs.
∂f
2 Baseline +FPA +PACE
∥∂θ∥
6e3
3e3
λ= 1e-35e-30.010.05 0.1 0.5 1 5 10 50 100 500 1e3 5e3 1e4 5e4
Figure8: GradientnormsofmodelsacrosswiderangeofregularizationstrengthsλonCamelyon
(VTAB-1K)w/Swin-B.Lineandshadowrepresentmeanandstdovertrainingepochs.Whilegradient
explosionislessfrequentforFPAinthissetting,itexhibitsunpredictablegradientnormwithvaried
regularizationstrengths. Incontrast,PACEreliablylowersgradientnormsasregularizationstrength
λincreases,demonstratingitsrobustnessforeffectivegradientcontrol.
D Hyperparametersettings
Foreachdataset,wefollowstrategiesfrompreviousworks[36,22,5,39]toapplygridsearchon
therank,learningrateandweightdecaytoestablishstrongbaselines. Table7,8,9and10present
17
Θ∂/f∂
2
∥
∥thehyperparametersandnumberoftrainableparametersusedinourstrongbaselineforVTAB-1K,
few-shotlearning,FGVCanddomainadaptationtasks.
With these strong baselines, we apply grid search on λ 0.02,0.05,0.1,0.2,0.5,1 and σ
∈ { } ∈
0.1,0.5,1,1.5,2 forPACEtooptimizeitsperformance.
{ }
Table 7: Hyperparameters for baseline on VTAB-1K with ViT-B/16. A: LoRA +VPT , B:
mul add
LoRA . lr: learningrate. WD:weightdecay.
add
Natural Specialized Structured
Method A A A A A A A A A A B B B A A A A A B
Rank 10 14 12 18 18 14 10 8 8 10 2 2 8 18 4 10 10 22 4
1.81
lr 1e-3 1e-3 1e-3 1e-3 1e-3 1e-2 1e-3 5e-3 5e-3 5e-3 5e-4 5e-4 1e-4 5e-3 5e-3 5e-3 5e-3 1e-2 2e-4
WD 1e-4 1e-4 1e-3 1e-2 1e-3 1e-3 1e-2 1e-2 1e-2 1e-2 1e-4 1e-3 1e-4 1e-3 1e-3 1e-4 1e-2 1e-2 1e-2
Table8: RanksforbaselinesinFew-shotlearning. Weightdecayisfixedat1e-4.
learningrate FGVCAircraft Food101 Flowers102 OxfordPets StanfordCars Mean
Baseline 5e-3 5e-3 5e-3 2e-3 2e-3 Parameter(M)
LoRAadd 4 4 4 4 10 0.93
VPTadd 1 1 1 1 1 0.14
LoRAmul+VPTadd 14 10 18 18 24 2.70
Table9: HyperparametersforthebaselineLoRA +VPT inFGVC.
mul add
Hyperparameter CUB-200-2011 NABirds OxfordFlowers StanfordDogs StanfordCars MeanParameter(M)
learningrate 5e-3 5e-4 5e-3 5e-3 2e-4
weightdecay 1e-2 1e-3 1e-3 1e-2 1e-3 2.80
rank 14 18 18 24 14
Table10: HyperparametersforbaselineLoRA +VPT indomainadaptation.
mul add
Baseline rank learningrate weightdecay Parameter(M)
LoRAmul+VPTadd 10 5e-4 1e-2 2.39
18
retemaraprepyH
001rafiC
101hcetlaC
DTD
201srewolF
steP
NHVS 793nuS noylemaC TASoruE 54csiseR
yhtaponiteR tnuoC-rvelC
tsiD-rvelC baLMD
tsiD-ITTIK
coL-rpSd irO-rpSd
mizA-BRONs
elE-BROsN
)M(retemarapegarevA