Assessing the Level of Toxicity Against Distinct
Groups in Bangla Social Media Comments: A
Comprehensive Investigation
Mukaffi Bin Moin*, Pronay Debnath, Usafa Akther Rifa, Rijeet Bin Anis
Ahsanullah University of Science and Technology, Dhaka, Bangladesh.
*Corresponding author(s). E-mail(s): mukaffi28@gmail.com
Contributing authors: pronaydebnath99@gmail.com; usafarifa97@gmail.com;
risan.aust@gmail.com
Abstract
Social media platforms have a vital role in the modern world, serving as conduits for communication, the
exchange of ideas, and the establishment of networks. However, the misuse of these platforms through toxic
comments, which can range from offensive remarks to hate speech, is a concerning issue. This study focuses
on identifying toxic comments in the Bengali language targeting three specific groups: transgender people,
indigenouspeople,andmigrantpeople,frommultiplesocialmediasources. Thestudydelvesintotheintricate
process of identifying and categorizing toxic language while considering the varying degrees of toxicity—high,
medium,andlow. Themethodologyinvolvescreatingadataset,manualannotation,andemployingpre-trained
transformer models like Bangla-BERT, bangla-bert-base, distil-BERT and Bert-base-multilingual-cased for
classification. Diverse assessment metrics such as accuracy, recall, precision, and F1-score are employed to
evaluate the model’s effectiveness. The experimental findings reveal that Bangla-BERT surpasses alternative
models, achieving an F1-score of 0.8903. This research exposes the complexity of toxicity in Bangla social
media dialogues, revealing its differing impacts on diverse demographic groups.
Keywords Toxic Comment Classification · Deep Learning · Levels of Toxicity · Pre-trained Language
Models · Low-resource Language
1 Introduction
Social media sites like LinkedIn, Facebook, Twitter, Instagram, TikTok, and others play a significant role in
today’s society. It allows individuals to communicate and exchange concepts within a secure environment.
Additionally, it functions as a platform for staying updated on trends and ongoing events, as well as for
promoting businesses, groups, and social causes. Social media also helps us make friends and talk to people
from all over the world, building strong connections1.
Unfortunately, some individuals also misuse these platforms by engaging in toxic comments and harmful
behavior. Socialmediatoxiccommentsrefertoharmful,offensive,ornegativeremarksmadebyusersonsocial
media platforms. These comments can range from personal attacks and insults to hate speech, harassment,
and cyberbullying [1].
1www.linkedin.com/pulse/importance-social-media-todays-world-johan-smith
4202
peS
52
]LC.sc[
1v03171.9042:viXraToxic language, which is defined as unpleasant, disrespectful, or inappropriate speech that is likely to cause
someonetoquitaconversation,isawidespreadissueontheinternet. Usingartificialintelligence(ML)models
to detect hazardous language in online chats is a hot topic. Models aimed at identifying such online toxicity
in chats, however, can be biased [2]. Recent research has found that several of these models’ classification
systems are more likely to identify benign language from minority cultures as harmful than identical language
representing non-minority groups. Researchers discovered that a publicly available method for detecting
toxicityintexthasanadvantagewhenpredictinghightoxicityscoresforcommentsthatuseAfricanAmerican
English when compared to other comments [3]. Measuring toxicity among specific identity groups is an
essential step in recognizing and addressing online discrimination and harm. By analyzing the type and
frequency of toxic comments directed at different identity groups, such as race, ethnicity, gender, or sexual
orientation, we gain a deeper understanding of the challenges they encounter in digital spaces.Identifying
patternsofharmallowsustoassesstheseverityoftheissueandtailorinterventionsaccordingly. Forinstance,
if a particular group consistently faces higher levels of toxic comments, it signals the need for specific support
and measures to create a safer environment for them [4].
There has been no such work conducted in the Bengali language regarding toxic comments. The proposed
idea is to identify toxic comments used in opposed to a total of three separate groups of people—transgender
people, indigenous people and migrated people in Bengali language—from multiple social media. We may
summarize the main points of our study as follows:
• Creating a multi-level Bangla toxic comments Datasets.
• Identifying toxic comments used against three distinct groups of people—transgender people, indige-
nous people and migrated people.
• Measuring the level of toxicity of a comments (high, low and medium).
• Recognizing the subjectivity of toxic comments, where what’s harmless to one can be harmful to
others.
2 RELATED WORKS
Saha et al. [5] identify abusive comments written in Bengali on social media platforms. The authors explore
various machine learning algorithms and employ SMOTE to address the imbalanced dataset. The study
highlightsthesignificanceoftacklingabusivecommentsinBengaliandprovidesvaluableinsightsintoeffective
detection methods. Haque et al. [6] Utilize a method capable of identifying various labels to categorize
harmful comments written in the Bangla language. This technique addresses the significance of automating
the identification of detrimental content in Bangla and presents an efficient strategy for both classifying and
evaluating the intensity of toxic remarks. Jubaer et al. [7] build a Machine Learning and Deep Learning
Approach approche to Classify bangla Toxic comments. Their dataset consists of a total of 250,283 Facebook
comments that have been categorized into six types: toxic, insult, obscene, clean and identity hate. Through
stemming, the dataset’s accuracy may be increased, which might significantly change its accuracy. Rasid
et al. [8] constructs A carefully selected dataset of Bangla offensive language sourced from comments on
Facebook referred to as ToxLex_bn. ToxLex_bn is a comprehensive wordlist Bigram dataset of Bangladeshi
toxic language that has been created from 2207590 Facebook comments. The dataset consists of 8 categories,
such as misogynist bullies, sexist, patriarchic, vulgar, political, communal, and racial hate words, each tagged
with a binary label (Yes/No) for toxicity. Belal et al. [9] conduct a pipeline based on deep learning for
classifying offensive Bengali comments. The study utilizes a binary classification model using LSTM with
BERT Embedding to identify toxic comments, achieving an accuracy of 89.42%. For multi-label classification,
the utilization of a blend between CNN and BiLSTM, coupled with an attention mechanism, results in a
performanceof78.92%accuracyandaF1-scoreof0.86. TheresearchalsointroducestheLIMEframeworkfor
model interpretability and provides a publicly accessible dataset for further research. Goyal et al. [4] explore
how the self-described identities of human raters affect the detection of toxic language in online comments.
Groups of raters were created corresponding to identities such as African American and LGBTQ. They found
that the identity of the raters greatly impacts how they identify toxicity related to specific identity groups.
3 Corpus Creation
3.1 Data Collection
The dataset consists of 3100 labeled samples, categorizing individuals into four groups: transgender people,
indigenous people, migrated people, and universal toxic. Each category contains comments ranked by toxicity
2Table 1: Indentity of the annotators.
ANN-1 ANN-2 ANN-3 Expert
Research-Status Undergrad Undergrad Undergrad NLP researcher
Research-field NLP NLP NLP NLP, CV, HCI
Age 23 22 26 35
Gender Male Female Male Male
Viewed OnAb Yes Yes Yes Yes
level (high, medium, or low). Comments were manually extracted from TikTok, Facebook, and Instagram,
ensuring a diverse range of sources for comprehensive analysis of toxic behaviors and attitudes within the
specified groups.
Transgender- The approach for collecting data on toxic comments from the transgender group involved
sourcing social media posts from various influencers, TikTok videos, and comments on reels. The data
collection focused on identifying comments with toxic behavior, including those that incite violence or express
aggression. Additionally, aggressive replies to such comments were also analyzed to ensure a comprehensive
dataset for toxic comment classification.
Indigenous- For collecting toxic comment data related to indigenous people, posts from indigenous food
vloggers, travel vloggers, and cultural posts were targeted. The focus was on identifying comments that
contain toxic behavior, such as aggression or hate speech. Aggressive replies to toxic comments were also
considered to provide a more thorough dataset for classifying toxic comments.
Migrated- Toxic comments from the migrated group were sourced from Facebook pages of TV news portals
andcommentsonYouTubevideosofnewsmedia. Thedatacollectionaimedtoidentifyandclassifycomments
with toxic language or aggressive content, including those that wish harm to others or use offensive language.
Universal toxic- To gather data on universal toxic comments, we identified social media posts from different
groupsandinfluencercommunities. Thefocuswasonfindingpostsandcommentsthatshowedtoxicbehavior,
like inciting violence or expressing aggression. We also looked at aggressive replies to these posts and
comments to create a complete dataset for classifying universal toxic comments.
3.2 Data Annotation
Dataannotationcanbedonemanuallybyhumanannotatorsorusingautomatedtoolsandtechniques. Manual
annotation often requires expertise and can be time-consuming, especially for large data sets. Automated
annotation methods, such as weak supervision or active learning, can help speed up the process but may
require additional validation steps to ensure accuracy [10].
3.3 Identity of annotators
The identity of annotators is crucial, as their unique perspectives can impact annotations [11]. To reduce bias,
we selected annotators from diverse racial, geographic, and religious backgrounds. Four annotators—two
undergraduate students, one graduate student, and one expert—perform the manual annotations. All are
fluent in Bengali, their native language. Table 1 details their background, area of specialization and other
pertinent demographic data. The following essential traits define the annotators:
• They are between the ages of 23 and 26.
• None of these individuals are connected to migrated and indigenous groups.
• Their experience ranges from one to three years, and their area of study is Natural Language
Processing (NLP) and computer vision (CV).
• They frequently use social media and have seen instances of hostility there.
Table 1 is an overview of the annotator’s details includes their research status, field of research and personal
encounters with online abuse on social media. Here ANN, OnAg denotes annotator and online abuse
respectively.
33.4 Annotation guidelines
Data annotation is critical for training machine learning models because it provides the ground truth
information necessary for algorithms to learn. Accurate and consistent annotations are required for high-
quality model performance. When annotators are given the opportunity to include their own perspectives,
it can provide benefits but also pose issues. [12]. Creating annotations for a toxic language collection that
include sensitive themes like as remarks regarding transgender, tribal, and migratory persons necessitates
rigorous ethical thought. The idea is to discover and categorize harmful language into separate groups and
degrees. Set specific criteria for poisonous comments for each category, such as abusive language, insulting
remarks, hate speech, or discriminatory material. Annotators should evaluate toxicity levels and classify
comments as Low, Medium, or High. For example, slang insults or physical bullying may be Medium, but
family assaults or death wishes may be High. Comments that do not fit the medium or high criterion should
be classed as low toxicity. The Flow Charts of Annotation guidelines is shown fig. 1
Toxic Comment di is nc tir o mE im rx i dtip n r ar aae tn its oi sos ngn h e to rb no ar a ds nhst e sai e rl gri dt as ey st no, a s dp n t mu er ge s reej , nu n t td hd ti rec aere ra g i to d esr e t, in nt gity NO ne P tg he mt a er r tp i i oib rve c a eet ku l t p,hopa r rnret i ae dio d cc iph e tci itla m u cyer el , em eo sp a ,r ,f o n ou i onr i rl n rt drs bg ta it reg e y am ler i i dn te na t igo l tn o e
i
t on ut y ch ne sp ue r
s
e i lm .db ts uea a i rnsn eb te sio a d t ,yu ,ot n NO c H ub io nla ts ts u et e ri nadli tlt i oy ob n nao c r t oh k p fge r ii r ne r o cj nu u itad n it ni dic o g e on fft a e o ml aiw t riy ga o, rr re ad t pns h rt nm s ei jc w uig i ditr ty ia h c,n eo tt h .rs e NO Un Ti ov xe ir csal
Yes Yes Yes
Transgender Indigenous Migrants
Toxic Toxic Toxic
Figure 1: Flow Chart of Annotation guidelines
1. Toxic Comments - Transgender
Comments that express hostility, prejudice, or discrimination based on a person’s gender identity
or transgender status.Also a Comments that contain threats, intimidation, or harassment targeting
transgender individuals.
• Low: iHjraguela eqel na emey ebaCa Jay na (It’s not possible to determine the transgender whether
they are boys or girls)
Remarks: Discrimination on gender identity (class)
• Medium: EI iHjra, duz naI bLaUj pe(cid:136) ik krib (He doesn’t have any boobs so why is he wearing
bra)
Remarks : Harassment (class) and physical abuse (toxicity level)
• High: iHjraguelaek emer efla Uict (The transgenders should be killed)
Remarks: Threat (class) and wish for death(toxicity level)
2. Toxic Comments - Migrants
Comments that express hostility or prejudice towards migrants based on their nationality, ethnicity,
or cultural background.Also Comments that spread false or harmful information about migrants with
the intention of inciting fear or prejudice.
• Low: Eedr edS tYag kra Uict (They should leave the country)
Remarks: identity of nationality (class)
• Medium: eraiH¢ga gulan Aar ktidn Aamaedr edeS Qakeb . Ee(cid:155)(cid:26)etI edeS Qakar jayga naI tarpr Upr EI
enaKra nfldmar ikTgula AaIsa edeS Zukes Qaket. (HowlongwilltheRohingyapeoplestayinourcountry?
There is no place to stay in the country yet these unwanted and disgusting insects like the
Rohingya keep entering and living in the country)
Remarks: identity of nationality (class) and abuse (Toxicity level)
• High: eraiHKga mruk Era GnGn ba£ca j«M edy (Rohingyas should die because they often give birth to
babies)
Remarks : hostility (class) and death threat (toxicity level)
3. Toxic Comments - Indigenous people
Comments that perpetuate harmful stereotypes about tribal people, portraying them in a negative
or demeaning manner based on their ethnicity or indigenous identity. Also Comments that mock,
ridicule, or belittle tribal cultures, practices, or traditions.
• Low: Era sap xay ikvaeb iqh! (How do they eat snakes? Shame!)
Remarks: cultural abuse (class)
4• Medium: Aaidb(cid:146)s(cid:140) gulaer edxelI men cay zumaya laI(cid:211)aI. Sala banerr ba£ca gula! (When I see the indige-
nous people, I feel like simply beating them. Piece of monkeys!)
Remarks: mocking (class) and slang (medium)
• High: cakma madarecadgula ekaTa ecaday, mruk Era (These ’Chakma’ motherfuckers only ask for quota,
they should die)
Remarks: demeaning manner (class) and slang,death threat (toxicity level)
4. Toxic Comments - Universal Toxic
Toxiccommentsthatarenotfocusedonaparticulargrouplike(TribalPeople,Transgender,Migrated
People) but still exhibit offensive, harmful or disrespectful language. Comments that contain threats,
intimidation or harassment towards individuals without focusing on their tribal, transgender or
migrant identity. Those comments may not be specifically targeted at tribal, transgender or migrant
communities, they can still cause harm and perpetuate harmful behaviors.
• etaedr mt sbaI Habla paFa na, etaedr ik ekan kaj naI, teb elaekr bal kamaga ikqu pysa paib (Not every-
one is lazy like you, don’t you have any work, go earn some money)
• nai(cid:239)k,,, Oek juta marun (Atheist... beat him with a shoe)
• kaela emey, edexI eta bim Aaes (Such a Dark-skinned girl, It makes me vomit)
Figure 2: Number of total hate comments by each class
3.5 Dataset Statistics
The collection of data comprises a complete sum of 3100 data instances. Among these instances, 2300 are
labeled as Toxic comments, while the remaining 800 are labeled as universal toxic comments. These Toxic
texts are further categorized into 3 classes, where the Trans, indigenous , and Migrants classes have 700, 800,
and 800 text samples, respectively. The dataset overview is summarized in Figure 2. To request access to the
dataset, please reach out to the corresponding author.
4 PROPOSED METHODOLOGY
Dataset
Evaluation Metrics
Accuracy
Comment: িহজরা েলা  ছেল না  মেয়  বাঝা যায় না
E thn eg tl ris ah n sT gr ea nn ds el ra wtio hn et: h I et' rs tn ho et y p ao rs es bib ol ye s t o o rd ge it re lr sm . ine Precision
C Tola xs is c: i tT yr a len vs eg le sn : d Le or w Text Preprocessing Recall
F1 Score Toxicity Class
C E tho n em g yl im os fhe te n T nt r : ga   in vর esাি l হ ba iং t riগ to hা n ম t: o R  boক ah bi nএ iegর sy া a ঘ s ন shঘ oন u lব dা   dieা জ be  ca   uদ sয় e Punctuation Removal Pre-trained Language Models Transgender Toxicity Levels
C T C
E
fমol uoa n x cs m
g
kis কc
l
e: imi rstM
এ
sy he i
o
রnlg Te nr t
া
rva :
la y
en চ
n
lt aাss ক
s
s:
l
kH aম fti oাg
i
oমh
r n
qাদ
: u
Tা oর
h
tে aeচ
,s
া teদ
h
' 
eC
yhল saা
hk
  omক
ua
lা d'ট
m
া
d
 
o
iচ
et
া hদ eা rয় -, DT ar ta ai sn e t W Nh oit E ne m -s TRp eoa exjc mi
t
e uR
o
aE e vlml ii Cnm o ogvi nn a ta l et ni ton Dim stB ilE BR ET
RT
Ban Bg al na
g
B laE BR ET
R
B Tase T Mra oin de ed l UnIn iM vd ei i rg g sr e aa n ln o Tt us os
xic
Best Pre-tr Mai on de ed l Language T Mra oin de ed l Me HL do igiw u hm
C Tola xs is c: i tI yn d leig ve en lso :u Hs igh Spelling Correction DaT te as st et DaT te as st et
Comment: কােলা  মেয়,  দেখই  তা বিম আেস
English Translation: Such a Dark-skinned girl, It
makes me vomit
Class: Universal Toxic
Toxicity levels: Medium
Figure 3: The diagram showcases the suggested methodology for Unveiling the Levels of Toxicity in Bangla
Social Media Comments
In this section, we outline a suggested methodology for Unveiling the Levels of Toxicity in Bangla Social
MediaComments. Theschematicrepresentationoftheproposedmethodforidentifyingbanglatoxiccomment
in Figure 3.
5To effectively handle toxic comment detection in Bangla, The dataset is preprocessed to normalize Bengali
text by managing whitespace, commas, and removing extraneous letters, with a concentration on Bangla
Unicode characters. During fine-tuning, pre-trained language models such as mBERT [16], DistilBERT [15],
bangla-bert-base [14], and BanglaBert [13] are trained on a Toxic Bangla dataset using transfer learning
techniques, and model parameters are adjusted using gradient descent optimization using the AdamW
optimizer and CrossEntropyLoss. Hyperparameters like as learning rate of 2e-5, batch size of 16 are tuned
to improve model performance while avoiding overfitting. The model’s performance is measured using
accuracy [17], precision [18], recall [19], and F1-score [20] measures.
5 EXPERIMENTAL RESULTS
To classify toxic comments targeting specific groups such as transgender, Indigenous, migrants and universal
toxic, we utilized several pre-trained language models. Here in table 2 BanglaBERT performed the best,
achieving an accuracy of 0.8903. The other models, DistilBERT base multilingual, Bangla BERT base
and BERT base multilingual, obtained accuracies of 0.8323, 0.8645 and 0.8419 respectively. In table 3, we
presented the classification of toxicity levels for each category using the best model, BanglaBERT. The
accuracy for transgender across low, medium and high toxicity level is 0.6714. For the indigenous, the
accuracy stands at 0.6375 while for the Migrants, it is 0.5625. These results highlight BanglaBERT’s superior
performance, which can be attributed to its ability to capture linguistic nuances in Bangla, as well as its
training on a larger and more diverse Bangla corpus compared to the other models.
Table 2: Performance comparison of Pre-trained Language Models For Bangla Toxic Comment Classifications
Models Accuracy Precision Recall F1 Score
DistilBERT 0.8323 0.8370 0.8320 0.8320
bangla-bert-base 0.8645 0.8651 0.8696 0.8648
BanglaBERT 0.8903 0.8906 0.8901 0.8903
mBERT 0.8419 0.8433 0.8478 0.8419
Table 3: Evaluation Metrics for Classifying Toxicity Levels in Bangla Social Media Comments
Class Models Accuracy Level Precision Recall F1 Score
Low 0.7143 0.7143 0.7143
Transgender BanglaBERT 0.6714 Medium 0.5769 0.6522 0.6122
High 0.7778 0.5833 0.6667
Low 0.7778 0.5526 0.6462
Indigenous BanglaBERT 0.6375 Medium 0.6286 0.7097 0.6667
High 0.4444 0.7273 0.5517
Low 0.5745 0.6585 0.6136
Migrants BanglaBERT 0.5625 Medium 0.5600 0.4516 0.5000
High 0.5000 0.5000 0.5000
6 Conclusion
To sum up, this paper emphasizes addressing toxic comments in a multicultural context, targeting groups
like transgender individuals, indigenous people, and migrants. While previous research has concentrated on
toxicity detection in languages such as English and others, this study makes a unique contribution to the
underexplored field of Bangla language toxicity detection, particularly in vulnerable groups. The toxicity
6a) BanglaBert b) mBert c) Bangla-Bert-Base d) DistilBert
Figure 4: Confusion matrix of Pre-trained Language Models For Bangla Toxic Comment Classifications
identification of languages like English and other languages has been the subject of previous research, but this
studymakesanewcontributiontotheunderexploredfieldofBanglatoxicitydetectionespeciallyinvulnerable
groups. The study presents a manually annotated multi-level dataset and evaluates several transformer-based
models, including Bangla-BERT, bangla-bert-base, mBert and distilBert. Bangla-BERT showed the highest
precision, accuracy, recall, and F1-score. Future plans include expanding the dataset to include more groups
and refining toxicity levels (very high, high, low, very low) for a more nuanced understanding of toxic
comments.
References
[1] Emon, Md. Imdadul Haque & Nazia Iqbal, Khondoker & Mehedi, Md Humaion Kabir & Mahbub,
Mohammed Julfikar Ali & Rasel, Annajiat Alim. (2022). Detection of Bangla Hate Comments and
Cyberbullying in Social Media Using NLP and Transformer Models. 10.1007/978-3-031-12638-3_8.
[2] Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith. 2019. The Risk of Racial
Bias in Hate Speech Detection. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, pages 1668–1678, Florence, Italy. Association for Computational Linguistics.
[3] Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A. Smith. 2019. The Risk of Racial
Bias in Hate Speech Detection. In Proceedings of the 57th Annual Meeting of the Association for
Computational Linguistics, pages 1668–1678, Florence, Italy. Association for Computational Linguistics.
[4] Nitesh Goyal, Ian D. Kivlichan, Rachel Rosen, and Lucy Vasserman. 2022. Is Your Toxicity My Toxicity?
Exploring the Impact of Rater Identity on Toxicity Annotation. In ,. ACM, New York, NY, USA, 17
pages. https://doi. org/10.1145/
[5] Saha, Pratim & Sultana, Naznin & Khan, Ashraful & Noman, Shibli. (2022). Abusive Bangla Comment
Detection from Social Media Using Machine Learning Approach. 10.1007/978-981-19-1653-3_46.
[6] Haque, Naimul & Alam, Md & Ath Towfiq, Abdullah & Hossain, Mehorab. (2022). Bangla Toxic
Comment Classification and Severity Measure Using Deep Learning.
[7] Jubaer, A.N.M. & Sayem, Abu & Rahman, Md. (2019). Bangla Toxic Comment Classification (Machine
Learning and Deep Learning Approach). 62-66. 10.1109/SMART46866.2019.9117286.
[8] Mohammad Mamun Or Rashid,ToxLex_bn: A curated dataset of bangla toxic language derived from
Facebook comment,Data in Brief,Volume 43,2022,108416,ISSN 2352-3409, https://doi.org/10.1016/
j.dib.2022.108416.
[9] Belal, Tanveer & Shahariar, G. & Kabir, Md. (2023). Interpretable Multi Labeled Bengali Toxic
Comments Classification using Deep Learning. 10.1109/ECCE57851.2023.10101588.
[10] Ross, Björn, et al. "Measuring the reliability of hate speech annotations: The case of the european
refugee crisis." arXiv preprint arXiv:1701.08118 (2017).
[11] Sharif, Omar, and Mohammed Moshiul Hoque. "Tackling cyber-aggression: Identification and fine-
grained categorization of aggressive texts on social media using weighted ensemble of transformers."
Neurocomputing 490 (2022): 462-481.
[12] Fortuna, Paula, et al. "A hierarchically-labeled portuguese hate speech dataset." Proceedings of the third
workshop on abusive language online. 2019.
[13] Bhattacharjee, A., Hasan, T., Ahmad, W.U., Samin, K., Islam, M.S., Iqbal, A., Rahman, M.S. and
Shahriyar,R.,2021.BanglaBERT:Languagemodelpretrainingandbenchmarksforlow-resourcelanguage
understanding evaluation in Bangla. arXiv preprint arXiv:2101.00204.
7[14] Sarker, S. (2020). BanglaBERT: Bengali Mask Language Model for Bengali Language Understanding.
https://github.com/sagorbrur/bangla-bert
[15] Sanh, V., Debut, L., Chaumond, J., & Wolf, T. (2019). DistilBERT, a distilled version of BERT: smaller,
faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.
[16] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805.
[17] Liu, Brian, and Madeleine Udell. "Impact of accuracy on model interpretations." arXiv preprint
arXiv:2011.09903 (2020).
[18] Powers,D.M.(2020).Evaluation: fromprecision,recallandF-measuretoROC,informedness,markedness
and correlation. arXiv preprint arXiv:2010.16061.
[19] Saito, Takaya, and Marc Rehmsmeier.: The precision-recall plot is more informative than the ROC plot
when evaluating binary classifiers on imbalanced datasets. PloS one, 10(3), e0118432 (2015)
[20] Sokolova, Marina & Japkowicz, Nathalie & Szpakowicz, Stan. (2006). Beyond Accuracy, F-Score and
ROC: A Family of Discriminant Measures for Performance Evaluation. AI 2006: Advances in Artificial
Intelligence, Lecture Notes in Computer Science. Vol. 4304. 1015-1021. 10.1007/11941439_114.
8