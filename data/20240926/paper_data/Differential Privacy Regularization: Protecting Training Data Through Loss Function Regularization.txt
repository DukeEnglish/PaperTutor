Differential Privacy Regularization:
Protecting Training Data Through
Loss Function Regularization
Francisco Aguilera-Mart´ınez Fernando Berzal
Department of Computer Science and Artificial Intelligence Department of Computer Science and Artificial Intelligence
ETSIIT, University of Granada ETSIIT, University of Granada
faguileramartinez@acm.org berzal@acm.org
Abstract—Training machine learning models based on neural DP-SGDintroducesrandomnoiseinthegradientsasadefense
networks requires large datasets, which may contain sensitive mechanism during model training. Our work started from the
information. The models, however, should not expose private
observation that the introduction of Gaussian noise in the
information from these datasets. Differentially private SGD
gradients, as the DP-SGD algorithm does, is not completely
[DP-SGD] requires the modification of the standard stochastic
gradient descent [SGD] algorithm for training new models. In effective in mitigating gradient leakage (GL) attacks.
this short paper, a novel regularization strategy is proposed to In this work, we propose a new method that intends to
achieve the same goal in a more efficient manner. achieve differential privacy through the regularization of the
loss(orerror)functionusedtotrainartificialneuralnetworks.
I. INTRODUCTION
In order to offer protection against gradient leakage (GL)
Wehaverecentlywitnessedthewidespreadadoptionofdeep attacks, our regularization term depends directly on both the
learning models across many different applications. These network parametersand its inputs. Our proposal, called PDP-
modelshavebeenparticularlysuccessfulinanincreasingnum- SGD, is somehow equivalent to the introduction of Gaussian
ber of natural language processing (NLP) tasks [1], including noise that is proportionalto the magnitude of each parameter
text summarization, machine translation, and language gener- inthemodel.However,theexplicitintroductionofnoiseisnot
ation. Large language models (LLMs) have gained significant needed,actually,anditscomputationalcostcanbeavoidedby
attention for their exceptional capabilities to generate and PDP-SGD.
interprettext as humansdo [2]. While LLMs offer significant
advantages, they are not without flaws and remain vulnerable
II. BACKGROUND
to security and privacy attacks [3]. Differential privacy (DP) has emerged as a fundamental
Training and fine-tuning LLMs requires massive quantities technique for safeguarding sensitive information in machine
ofdatasourcedfromtheinternetandproprietarydatasources, learning models, particularly in scenarios involving large
as well as carefully annotated text data to enhance their language models. Differential privacy limits the information
performance for specific tasks. This reliance on extensive that is leaked about specific individuals. In terms of machine
datasetscanincreasetheirsusceptibilitytosecurityandprivacy learningmodels, we try to ensurethat an individualdata does
vulnerabilities. Despite their widespread use, the vulnerabili- notsignificantlyaffecttheoutcomeofacomputation,therefore
ties of LLMs have not been extensively explored on a large providingguaranteesofprivacywhileallowingusefulinsights
scale. tobederivedfromaggregatedata.Inshort,wearefocusedon
Maliciousactorscanexploitdeeplearningmodelstoextract the study of how differential privacy can be used to protect
sensitive information that is used during their training and sensitive information in training data.
is, in some sense, memorized by them. One of the most In the context of deep learning, implementing differential
prominent and dangerous attack methods is called gradient privacy poses unique challenges due to the complexity of the
leakage [3]. This attack attempts to infer whether a specific modelsand the sensitivity of the data oftenused to train deep
datainstancewaspartofthetrainingdatausedtotrain(orfine- learning models. Below, we summarize some key works that
tune) the model under attack. To mitigate the effectiveness of have shaped the field of differential privacy in deep learning.
theseattacks,modeldeveloperssometimesemploydifferential
A. Differential Privacy in Deep Neural Networks
privacy[4]duringthetrainingprocessasaprotectivemeasure.
We investigate how to preserve privacy by implementing Abadi et al. [5] explored the integration of differential
differential privacy regularization for deep learning models, privacy in deep neural networks.
includingLLMs.Ourapproachisinspiredbythedifferentially The core idea of differential privacy is to ensure that the
private stochastic gradient descent (DP-SGD) algorithm [5]. inclusion or exclusion of a single training example does not
4202
peS
52
]GL.sc[
1v44171.9042:viXrasignificantly influence the outcome of a model. Formally, an D. Differential Privacy through Classic Regularization
algorithm A is said to satisfy (ǫ,δ)-differential privacy if, for While DP-SGD is a robust method for ensuring privacy,
any two adjacent datasets D and D′, and for any subset of it often leads to significant performance degradation due to
possible results S, the following holds:
noise used during training. Lomurno et al. [4] compared DP
Pr[A(D) S] eǫPr[A(D′ ) S]+δ techniques and traditional regularization methods, such as
∈ ≤ ∈ dropoutandL2regularization.Accordingtotheirstudy,classic
Here, ǫ represents the privacy budget, which quantifies the
regularization,commonlyusedtopreventoverfitting,provides
level of privacy protection, and δ accounts for a small proba-
similar levels of protection against membership inference and
bility of failure to maintain privacy.
model inversion attacks, which are key privacy concerns in
In practice, implementing differential privacy in deep neu-
machine learning.
ral networks involves techniques like gradient clipping and
Their empirical results suggest that regularization methods
Gaussiannoiseaddition.TheresultingalgorithmiscalledDP-
may offer a more effective trade-off between privacy and
SGD, differentially-privatestochastic gradientdescent. Gradi-
model performance in certain scenarios. Unlike DP-SGD,
entclippinglimitsthegradientnormtoapredefinedthreshold
which incurs in high computational costs and significant
C, reducing sensitivity, while Gaussian noise is added to
accuracy loss, regularization techniques provide privacy pro-
the gradients to obscure individual data contributions. This
tection with a minimal impact on performance. As such,
approach, however, introduces a trade-off, as higher privacy
these methodsmay be more suitable in contextswhere model
levels tend to decrease model accuracy due to the increased
performance and training efficiency are critical.
noise that is introduced during model training.
III. A NEW PERSPECTIVE ONTHE DIFFERENTIALLY
B. Differential Privacy in LLMs: The EW-Tune Framework
PRIVATE SGD ALGORITHM
Differentialprivacyhas also been appliedto largelanguage
DP-SGD [5] offers one way to control the influence of the
models (LLMs), which present additional challenges due to
training data during the training process: At each step of the
their size and complexity. Behnia et al. [6] proposed the
SGD, we compute the gradient (θ) for a random subset
EW-Tune framework to implement DP in LLMs. They intro- ∇θ L
of examples, clip the ℓ2 norm of each gradient, compute the
duced the Edgeworth Accountant, a method for calculating
average, add noise in order to protect privacy,and take a step
preciseprivacyguaranteesin thecontextof finitesamples.By
in the opposite direction of this average noisy gradient:
leveragingthe Edgeworthexpansion,the authorsprovidenon-
asymptoticguarantees,improvingupontraditionalapproaches ǫ(t) (0,σ2 )
∼N
that often rely on asymptotic bounds.
g˜(t)=g(t)+ǫ(t)
The DP-SGD algorithm is used for applying DP in LLMs,
byintroducingGaussiannoiseintothegradientsduringmodel where g(t) is the gradient for the current batch of training
training. The Edgeworth Accountant just refines the noise examples and ǫ(t) is the Gaussian noise we introduce.
addition process, calculating the necessary amount of noise The resulting weight update is, therefore:
based on the given privacy budget. This method balances the
∆θ(t)= η g˜(t)
trade-off between noise and model utility more effectively, − t
allowing for reduced noise and, consequently, better model i.e.
performance without compromising privacy guarantees. θ(t+1)=θ(t) η g˜(t)
t
−
C. User-Level Differential Privacy in LLMs where η is the current learning rate.
t
Building on previous work, Charles et al. [7] examine DP Let us now assume a linear neuron (or a nonlinear neuron
in the context of LLM training by introducing two sampling operating within its linear regime):
approaches:example-levelsampling(ELS)anduser-levelsam- n
pling (ULS). These methodsaim to protectuser-levelprivacy, y =θ x= θ x
i i
·
ensuringthatthe contributionofindividualusersto the model i=0
X
is protected. ELS involves clipping gradients at the example Before the weight update:
level, while ULS operates at the user level, allowing for
y(t)=θ(t) x
gradient aggregation over all examples provided by a single
·
user. Withnoisein thegradients,theoutputafterthe weightupdate
The authors introduce a noveluser-level DP accountantfor is
ELSthatleveragesadivergencemeasureknownasthehockey-
y˜(t+1)=θ(t+1) x
stick divergence. This measure enables the derivation of pre-
·
cise privacy guarantees for ELS. Comparisons between ELS =(θ(t) η g˜(t)) x
t
− ·
and ULS show that, under fixed computational budgets, ULS =(θ(t) η (g(t)+ǫ(t)) x
t
tends to providestronger privacyguaranteesand better model − ·
=(θ(t) η (g(t)) x η ǫ(t) x
performance,particularly when stringent privacy protection is − t · − t ·
=y(t+1) η ǫ(t) x
requiredorwhenlargercomputationalresourcesareavailable. t
− ·Simplifying our notation, we have:
= +
y˜=(θ ηg˜) x LDP L Lnoisygradient
− ·
=y ηǫx Please, compare the above expression with the standard L2
− regularization strategy:
Usingaquadraticerrorfunction =(y t)2forthetraining
L − 2
algorithm using gradient noise: LL2regularization = L+λ θ i
i
E[(y˜ t)2 ]=E[(y˜ t)2 ] X
− − Ofcourse,bothregularizationtermscanbeeasilycombined:
2
=E[((y ηǫx) t) ]
=E[((y− t) η− ǫx)2 ] LDP+L2regularization = L+λ θ i2 +κ x2 i
− −
=E[(y
t)2
] E[2(y
t)ηǫx]+E[(ηǫx)2
]
Xi Xi
− − − As training with input noise is equivalent to weight decay,
Let us recall that the noise ǫ is sampled from a Gaussian alsoknownasTikhonovorL2 regulatization[8],trainingwith
with mean 0 and variance σ2. Hence: noisygradientsissomehowequivalenttoperformingTikhonov
regularization on the input.
E[2(y t)ηǫx]=2(y t)ηE[ǫ]x=0
− − However,itshouldbenotedthattheDPregularizationterm
Therefore is independent from the network parameters θ. Therefore,
its gradient with respect to the network parameters is zero,
2 2 2
E[(y˜ t) ]=E[(y t) ]+E[(ηǫx) ]
i.e. = 0. Hence, the resulting optimization
− − θ noisygradient
∇ L
algorithm is exactly the same for the standard stochastic
The first term is just the traditionalquadraticerrorfunction
=(y t)2, whereas the second term can be interpreted as descent algorithm (SGD) and for its gradient noise variant
L an L2 re− gularization term for the input: (DP-SGD). In other words, we are just introducing some
artificial noise in the training algorithm, which adds to the
2
E[(ηǫx) ] noisy estimate of the gradient computed by the stochastic
=η2 E[(ǫx)2 ] gradient descent algorithm.
2 The above discussion might explain why some researchers
2 have found that, even though DP-SGD “is a popular mecha-
=η E ǫ x
i i
 !  nismfortrainingmachinelearningmodelswithboundedleak-
i
X ageaboutthepresenceofspecificpointsinthetrainingdata[,]
 
[t]he cost of differential privacy is a reduction in the model’s
2 2 2
=η E ǫ x +2 ǫ x ǫ x
 i i i i j j  accuracy” [9]. Moreover, “[a]ccording to the literature, [DP-
Xi Xi<j SGD] has proven to be a successful defence against several
  models’ privacy attacks, but its downside is a substantial
2 2 2
=η

E ǫ ix
i
+2 E[ǫ ix iǫ jx j]

degradation of the models’ performance... and [researchers]
i i<j empirically demonstrate the often superior privacy-preserving
X (cid:2) (cid:3) X
  properties of dropout and l2-regularization” [4].
2 2 2
=η E[ǫ ]E[x ]+2 E[ǫ ]E[x ]E[ǫ ]E[x ]
 i i i i j j  IV. DIFFERENTIALLYPRIVATE REGULARIZATION
i i<j
X X
  In the previous section, we observed that the addition of
2 2 2 Gaussian noise to the gradients in DP-SGD is not really
=η σ x +2 0x 0x
 i i i j  effective, since it just introduces an additional noise to the
i i<j
X X noisy gradient estimate of the conventional SGD, without
2 2 2 
=η σ ix i reallychangingthelossfunctionweareimplicitlyoptimizing.
Xi InthisSection,weproposetheintroductionofnoisepropor-
The error function given the noisy gradients in DP-SGD is tional to each parametervalue, so that the resulting algorithm
finally is not equivalent to SGD in its linear regime.
E[(y˜ t)2 ]=(y t)2 +η2 σ2 x2 OurproportionaldifferentiallyprivatePDP-SGDalgorithm1
− − i i
starts by introducing Gaussian noise as follows:
i
X
If we assume that the gradient noise variance is the same 2
ǫ (t) (0,(θ σ) )
i i
for all the inputs: ∼N
2 2 2 2 2 1Our PDP acronym is intentional, in honor to the Stanford Parallel
E[(y˜ t) ]=(y t) +η σ x
− − i Distributed Processing (PDP)lab led by JayMcClelland, who is known for
i his work on statistical learning, applying connectionist models (i.e. neural
X
networks) to explain cognitive phenomena such as spoken word recognition
=κ x2 and visual word recognition, and the books he edited in the 1980s, which
Lnoisygradient i
spurredthescientific interest inconnectionism [10][11].
i
Xg˜(t)=g (t)+ǫ (t) Theerrorfunctiongiventhe proportionalnoisygradientsin
i i i
PDP-SGD is, therefore,
For each network parameter, θ , we add Gaussian noise
i
whose standard deviation is proportional to the parameter 2 2 2 2 2 2
E[(y˜ t) ]=(y t) +η σ θ x
value(i.e.largerparametersreceivelargernoise).Thegradient − − i i
noise variance is, therefore, σ2 =(θ σ)2 Xi
i i
By definition, in Gaussian noise, the values are identically If we define a proportionaldifferentially private regulariza-
distributed and statistically independent (and hence uncorre- tion term as follows
lated), so E[ǫ ǫ ]=E[ǫ ]E[ǫ ].
i j i j
Using the same reasoning we followed in the previous 2 2
=κ θ x
Section, we have:
Lproportionalnoisygradient i i
i
X
y˜=y ηǫx
− then we have
and
= +
E[(y˜ t)2 ]=E[(y t)2 ] E[2(y t)ηǫx]+E[(ηǫx)2 ] LPDP L Lproportionalnoisygradient
− − − −
Now, the noise ǫ for each gradient is sampled from a Now, our regularization term depends on the network pa-
i
Gaussian with mean 0 and variance σ2, which is different rameters, so its gradient is not zero:
i
for each parameter. Hence:
2
=2κx θ
E[2(y t)ηǫx]=2(y t)ηE[ǫx]
∇θiLproportionalnoisygradient i i
− −
=2(y t)ηE ǫ ix i = +2κx2 θ
− ∇θiLPDPregularization ∇θiL i i
=2(y t)η
hXE[ǫ
x
]i
i i
− Let us finally observe that this term is still different from
=2(y t)ηXE[ǫ i]E[x i] the standard L2-regularization,which does not depend on the
−
inputs:
=2(y t)ηX0E[x ]
i
−
2
=0 X LL2 =λ θ i
i
X
2
E[(ηǫx) ]
=η2 E[(ǫx)2 ] ∇θiLL2 =2λθ i
2
In fact, both regularization techniques can be easily com-
2
=η E ǫ x
 i i !  bined and incorporated into the standard training procedure
Xi for deep neural networks:
 
2 2 2
=η E ǫ x +2 ǫ x ǫ x
 i i i i j j 
Xi Xi<j

LPDP+L2regularization = L+λ θ i2 +κ θ i2 x2 i
i i
2 2 2 X X
=η E ǫ x +2 E[ǫ x ǫ x ]
 i i i i j j 
i i<j
X (cid:2) (cid:3) X 2
  ∇θiLPDP+L2regularization = ∇θiL+2λθ i+2κx iθ i
2 2 2 2
=η

E[ǫ i]E[x i]+2 E[ǫ i]E[x i]E[ǫ j]E[x j]

= ∇θiL+2(λ+κx i)θ i
i i<j
X X
 
2 2 2
=η  σ ix i +2 0x i0x j  Giventhatthegoalofdifferentialprivacyisprotectingtrain-
Xi Xi<j ing data (i.e. the inputsx i), we hypothesizethat the proposed
2 2 2  proportional differentially-private regularization term, PDP,
=η σ x
i i
should be more effective than the popularDP-SGD algorithm
i
2X 2 2 in practice. In addition, it would also be more efficient, since
=η (θ σ) x
i i the introduction of noise when computing the gradients in
i
X SGD is replaced by an extra regularization term in the loss
2 2 2 2
=η σ θ x
i i function used to train the network, which can then be trained
Xi using the standard SGD optimization algorithm of our choice
(e.g. Adam).REFERENCES square of that constant: Var[aX] = a2Var[X]. In general,
the variance of the sum of two random variables is
[1] X. Fu, Z. Gu, W. Han, Y. Qian, and B. Wang, “Exploring Security
Vulnerabilities of Deep Learning Models by Adversarial Attacks,”
Var[X +Y]=Var[X]+Var[Y]+2Cov[X,Y]
Wireless Communications and Mobile Computing, vol. 2021, no. 1,
p.9969867, 2021.
where Cov[X,Y] is the covariance Cov[X,Y] = E[(X
[2] F. Wu, N. Zhang, S. Jha, P. McDaniel, and C. Xiao, “A New Era in
−
LLMSecurity:ExploringSecurityConcernsinReal-WorldLLM-based E[X])(Y E[Y])]=E[XY] E[X]E[Y] using thelinearity
Systems,”arXive-prints, 2024. arXiv:2402.18649. property o− f expectations. −
[3] B. C. Das, M. H. Amini, and Y. Wu, “Security and Privacy Chal-
2) Non-multiplicativity of expectations: If X and Y are
lenges of Large Language Models: A Survey,” arXiv e-prints, 2024.
arXiv:2402.00888. independent,thenE[XY]=E[X] E[Y].However,ingeneral,
[4] E. Lomurno and M. Matteucci, “On the Utility and Protection of the expected value is not multipl· icative, i.e. E[XY] is not
OptimizationwithDifferentialPrivacyandClassicRegularizationTech-
necessarily equal to E[X] E[Y]. In fact, Cov[X,Y] =
niques,” in Machine Learning, Optimization, and Data Science, LOD
·
2022 Revised Selected Papers (G. Nicosia, V. Ojha, E. La Malfa, E[XY] E[X]E[Y].
G. La Malfa, P. Pardalos, G. Di Fatta, G. Giuffrida, and R. Umeton, The v− ariance of the product of two independent random
eds.),LNCS13810,pp.223–238, SpringerNatureSwitzerland, 2023. variables is Var[XY] = E[(XY)2] E[XY]2 =
[5] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan, I. Mironov,
K. Talwar, and L. Zhang, “Deep Learning with Differential Privacy,” E[X2]E[Y2] (E[X]E[Y])2 = Va− r[X]Var[Y] +
in Proceedings of the 2016 ACM SIGSAC Conference on Computer Var[X]E[Y]2− + Var[Y]E[X]2, which can be rewritten
andCommunications Security, CCS’2016, p.308–318, Association for as Var[X]E[Y]2 +Var[Y]E[X]2 +(Cov[X,Y]/ρ[X,Y])2,
ComputingMachinery, 2016.
[6] R. Behnia, M. R. Ebrahimi, J. Pacheco, and B. Padmanabhan, “EW- where ρ[X,Y] is the Pearson correlation coefficient,
Tune:AFrameworkforPrivatelyFine-TuningLargeLanguageModels ρ[X,Y]=Cov[X,Y]/ Var[X]Var[Y].
with Differential Privacy,” in 2022 IEEE International Conference on
DataMiningWorkshops,ICDMW’2022,pp.560–566,IEEE,Nov2022. B. Normal distributionsp
[7] Z. Charles, A. Ganesh, R. McKenna, H. B. McMahan, N. Mitchell,
K. Pillutla, and K. Rush, “Fine-Tuning Large Language Models with AnormalorGaussiandistributionwithmeanµandvariance
User-LevelDifferentialPrivacy,”arXive-prints,2024.arxiv:2407.07737. σ2 is a continuous probability distribution for a real-valued
[8] C. M. Bishop, “Training with noise is equivalent to Tikhonov regular-
ization,” NeuralComputation, vol.7,p.108–116,Jan1995. random variable whose probability density function is
[9] E.Bagdasaryan, O.Poursaeed,andV.Shmatikov,“Differential Privacy
HasDisparateImpactonModelAccuracy,”inProceedings ofthe33rd 1 −1(x−µ)2
P(x)= e 2 σ
International Conference on Neural Information Processing Systems, σ√2π
NeurIPS2019(H.Wallach,H.Larochelle, A.Beygelzimer, F.d'Alche´-
Buc, E. Fox, and R. Garnett, eds.), vol. 32, Curran Associates Inc., If X is distributed normally with mean µ and variance
2019. σ2, then aX + b, for any real numbers a and b, is also
[10] D.E.Rumelhart,J.L.McClelland, andthePDPResearchGroup,eds., normallydistributed,withmeanaµ+bandvariancea2σ2.That
Parallel Distributed Processing: Explorations in the Microstructure of
Cognition -Volume1:Foundations. MITPress,1986. is, the family of normal distributions is closed under linear
[11] D.E.Rumelhart,J.L.McClelland, andthePDPResearchGroup,eds., transformations. Hence, E[kX]=kµ because E[X]=µ.
Parallel Distributed Processing: Explorations in the Microstructure of
1) Product of normal distributions: The distribution of a
Cognition-Volume2:PsychologicalandBiologicalModels.MITPress,
1986. product of two normally distributed random variables X and
Y with zero means and variances σ2 and σ2 is given by
x y
APPENDIX
A. Expectations (and variances) ∞ ∞
P (u)= P (x)P (y)δ(xy u)dxdy
XY X Y
1) Linearity of expectations: The expected value operator −∞ −∞ −
Z Z
(or ”expectation operator”) E[] is linear in the sense that, for 1 u
· = K0 | |
any random variables X and Y, and a constant a: πσ σ σ σ
x y (cid:18) x y(cid:19)
E[X +Y]=E[X]+E[Y] where δ(x) is Dirac’s delta function and K n(z) is a modified
Bessel function of the second kind.
E[aX]=aE[X] ∞
K0(z)= cos(zsinht)dt
This means that the expected value of the sum of any finite 0
Z ∞
numberofrandomvariablesis thesumoftheexpectedvalues cos(zt)
= dt
of the individual random variables, and the expected value 0 √t2+1
Z
scales linearly with a multiplicative constant.
2) Square of normal distributions: For a general normal
The variance of a random variable X is the expected value distributionX (µ,σ2),youcanusethefactthatX =µ+
of the squared deviation from the mean of X: Var[X] = ∼N
σN where N is a standard normal(zero mean, unit variance)
E[(X E[X])2] = E[X2] E[X]2. Therefore, E[X2] =
to get
Var[X− ] E[X]2. − 2 2 2 2
− X =µ +2σµN +σ N
Variance is invariant with respect to changes in a location
parameter, i.e. Var[X + a] = Var[X]. However, when all For a zero-mean normal distribution, X (0,σ2), X2 =
values are scaled by a constant, the variance is scaled by the σ2N2. X2/σ2 follows a Chi-squared d∼ istN ribution with 1degree of freedom, i.e. X2/σ2 χ2 (a non-central Chi-
1
∼
squared distribution in general, when the mean is not zero).
2
X (0,σ )
∼N
2 2 2
X σ χ
1
∼
Since µ=E[X]=0 and Var[X]=E[X2] E[X]2,
−
2 2
E[X ]=Var[X]=σ
Finally,
2 4 2 2
Var[X ]=E[X ] E[X ]
−
4 4
E[X ]=3σ
2 4 2 2 4 4 4
Var[X ]=E[X ] E[X ] =3σ σ =2σ
− −
NOTE: X2 σ2χ2. Since E[χ2]=1 and Var[χ2]=2, then
1 1 1
E[X2]=σ2∼ and Var[X2]=2σ4.