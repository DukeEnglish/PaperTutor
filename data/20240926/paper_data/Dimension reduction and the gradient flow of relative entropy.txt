DIMENSION REDUCTION AND
THE GRADIENT FLOW OF RELATIVE ENTROPY
BEN WEINKOVE
Abstract. Dimensionreduction,widelyusedinscience,mapshigh-dimensionaldata
into low-dimensional space. We investigate a basic mathematical model underlying
the techniques of stochastic neighborhood embedding (SNE) and its popular variant
t-SNE. Distances between points in high dimensions are used to define a probability
distribution on pairs of points, measuring how similar the points are. The aim is
to map these points to low dimensions in an optimal way so that similar points are
closer together. This is carried out by minimizing the relative entropy between two
probability distributions.
We consider the gradient flow of the relative entropy and analyze its long-time
behavior. This is a self-contained mathematical problem about the behavior of a
system of nonlinear ordinary differential equations. We find optimal bounds for the
diameter of the evolving sets as time tends to infinity. In particular, the diameter
may blow up for thet-SNEversion, but remains bounded for SNE.
1. Introduction
Dimension reduction (or dimensionality reduction) refers to a method of represent-
ing high-dimensional data in low-dimensional space. The goal is to retain the essential
features of the dataset, such as clustering, in the low-dimensional space (see [4], for
example). This paper investigates a basic mathematical model underlying two par-
ticular dimension reduction techniques: stochastic neighborhood embedding (SNE) and
the more widely-used variant called t-distributed stochastic neighborhood embedding (t-
SNE). They were introduced by Hinton and Roweis in 2002 [7] and van der Maaten and
Hinton in 2008 [14], respectively. These techniques have been used extensively in sci-
ence, including in medical research, see for example [6, 9, 10]. Given these applications,
it is important to understand rigorously their mathematical foundations. Conversely,
we believe these methods may reveal some mathematical phenomena of interest in their
own right. We describe briefly these methods, pose some questions, and then discuss
our results.
1.1. The SNE and t-SNE algorithms. Let x ,...,x be n points in Rd. Dimension
1 n
reduction should provide a map x y to y ,...,y in Rs. Since we wish to reduce
i i 1 n
7→
the dimension of the space, we assume s is strictly smaller than d. We also assume
that n > s+1, otherwise one could find an s-plane in Rd containing all the points. (In
practical applications, usually n and d are very large and s = 2 or 3).
Write for the discrete set of n pairs representing the directed edges between n
Pn 2
points, (cid:0) (cid:1)
= (i,j) i,j = 1,...,n, i = j .
n
P { | 6 }
Theinformaloutlineof themethodhasthreesteps,each ofwhich requiressomechoices.
Research supported in part byNSFgrant DMS-2348846 and theSimons Foundation.
1
4202
peS
52
]LM.tats[
1v36961.9042:viXra(1) Define a discrete probability distribution (p ) on representing the “simi-
ij i6=j n
P
larity” of the points x ,...,x in Rd. We insist on the symmetry p = p , for
1 n ij ji
simplicity. The larger the probability p , the “more similar” are the points x
ij i
and x .
j
(2) Given any points y ,...,y in Rs, we define a discrete probability distribution
1 n
(q ) on withq = q . Thelarger theprobability q , thecloser thepoints
ij i6=j n ij ji ij
P
y and y .
i j
(3) We look for points y ,...,y so that the probability distributions (p ) and (q )
1 n ij ij
are as close together as possible, by minimizing a cost function.
We describe now the steps more precisely. For (1), define, for positive constants
σ ,...,σ ,
1 n
exp( x x 2/(2σ2))
p = −| i − j | i for i = j.
j|i exp( x x 2/(2σ2)) 6
k6=i −| i − k | i
This represents “the condPitional probability that x would pick x as its neighbor if
i j
neighbors were picked in proportion to their probability density under a Gaussian cen-
tered at x ” [14], where σ is the variance of the Gaussian. The numbers σ are selected
i i i
according to certain criteria; since this is not the focus of the paper refer the reader to
[14]. We define
p +p
i|j j|i
p = ,
ij
2n
and one can check that this is a probability distribution on .
n
P
For step (2), for a smooth function β : [0, ) (0, ), define
∞ → ∞
β(y y 2)
i j
(1.1) q := | − | , for i = j.
ij β(y y 2) 6
k6=ℓ | k − ℓ |
In the case of SNE, we take β(xP ) = e−x, which corresponds to a Gaussian distribution.
For t-SNE, we take
1
β(x) = ,
1+x
which corresponds to a Student’s t-distribution with one degree of freedom (a Cauchy
distribution), from which t-SNE gets its name.
For step (3), we take our cost function to bethe relative entropy (or Kullback-Leibler
divergence),
p
ij
(1.2) (Y):= p log ,
ij
C q
Xi6=j ij
for points Y = (y ,...,y ) with y Rs. The relative entropy is a measure of how
1 n i
∈
the p differ from the q ; it is nonnegative and vanishes if and only if the probability
ij ij
distributions agree. The goal in step (3) is to minimize (Y) over all Y. The standard
algorithms start with random initial data Y in Rs and cC arry out gradient descent for a
0
finite number of steps, adding a “random jitter” at each step. There are also variants
such as “early exaggeration”. We refer the reader to [7, 14] for more details.
The development of dimension reduction methods such as the SNE and t-SNE has
been driven largely by empirical approaches (see [15] for example), with comparatively
little focus on rigorous mathematical foundations. We describe now a few of the recent
theoretical papers on the subject, all of which treat t-SNE. Under specific conditions,
2it has been shown that t-SNE separates clusters in a suitable sense by Shaham and
Steinerberger [12], Arora, Hu and Kothari [1] and Linderman and Steinerberger [11].
Analysis of the early exaggeration phase, mentioned above, was studied by Linderman
and Steinerberger [11] and further developed by Cai and Ma [3]. Steinerberger and
Zhang [13] analyzed t-SNE on a single homogeneous cluster and in this setting recast
the cost function as a classical calculus of variations problem. Auffinger and Fletcher
[2] considered the case when the initial data is given by n independent identically dis-
tributed outputs and showed under certain conditions the convergence of the minima
of the entropy to an equilibrium distribution. Recently, Jeong and Wu [8] gave some
conditions under which the solution of the gradient flow of the relative entropy remains
bounded.
1.2. The gradient flow of relative entropy. Thefocus of this paper is the following
mathematical problem, which can be described in a simple, self-contained way. Let
(p ) beagivenprobabilitydistributionon ,withn > s+1andp > 0foralli= j.
ij i6=j n ij
P 6
Fixasmoothdecreasingfunctionβ : [0, ) (0, )suchthatsup (logβ)′(x) <
∞ → ∞
x∈[0,∞)|
|
. Define probabilities q by (1.1) and let be the relative entropy (1.2). We wish to
ij
∞ C
understandthe minimaof . To doso, we consider thegradient flow of with arbitrary
initial data Y Rs, whichC is given by C
0
⊂
dy
i = 4 (p q )(y y )(logβ)′(y y 2),
(1.3) ij ij i j i j
dt − − | − |
Xj6=i
for β(x) as described in step 2. See Section 2 for a proof of this formula. There exists
a solution Y(t) = (y (t),...,y (t)) for all time t 0 by Gronwall’s inequality, since
1 n
≥
(logβ)′ is bounded by assumption. We can now state our main problem.
Problem. What is the behavior of solutions Y(t) of this flow as t and how does
→ ∞
this relate to the minima of ? Does Y(t) converge (after rescaling, if necessary) to a
set of points Y Rs? HowC does the limit depend on the initial data Y ?
∞ 0
⊂
Remark 1.1.
(i) These questions remain completely open, as far as we know. In this paper, our
main results are to obtain optimal estimates on the diameter of Y(t) as t
→ ∞
when β(x) = (1+x)−1 (t-SNE) and β(x) = e−x (SNE). We are also interested
in more general functions β.
(ii) As in [8], we consider the gradient flow (1.3) rather than a gradient descent
method with a finite number of steps, since we regard it as more natural from a
mathematical point of view. The behavior of Y(t) as t should be closely
→ ∞
related to the behavior of the SNE and t-SNE algorithms.
(iii) We regard the p as fixed at the outset, ignoring the construction in step (1)
ij
above, although this is surely a very important aspect to be studied. On the
other hand, this means that our results are relevant no matter how the p are
ij
chosen. In fact, our problem makes no reference to the high-dimensional space
Rd; rather it is a question about prescribing as close as possible a probability
distribution on arising from a configuration of n points y ,...,y in Rs.
n 1 n
P
31.3. Results. We explore the question of what happens to the diameter of the solution
sets Y(t) of the gradient flow of relative entropy, as t . We find optimal bounds
→ ∞
when β(x) = (1+x)−1 and β(x) = e−x, the t-SNE and SNE cases, respectively. We
observe very different behavior in these two cases.
Write Y(t) for the solution of the flow starting at Y = Y(0). Our first theorem is as
0
follows.
Theorem 1.1. The following diameter bounds hold.
(i) If β(x) = (1+x)−1, then for t 1,
≥
1
(1.4) diamY(t) Ct4.
≤
(ii) If β(x) = e−x, then for t 0,
≥
(1.5) diamY(t) C.
≤
Here and in the sequel, we use C,C′,c,c′ etc to denote “uniform” positive constants,
which means they are independent of t, but may depend on n, s, Y , p and the choice
0 ij
of function β. These constants may differ from line to line.
The bounds of Theorem 1.1 are optimal in terms of t in the following sense.
Theorem 1.2.
(i) Assume β(x) = (1+x)−1. We can find a probability distribution (p ) on some
ij
n, initial data Y 0 Rs and a positive constant c such that diamY(t) ct1 4
P ⊂ ≥
for t 0.
≥
(ii) Assume β(x) = e−x. We can find a probability distribution (p ) on some ,
ij n
initial data Y Rs and a positive constant c such that diamY(t) c for t P 0.
0
⊂ ≥ ≥
The examples in the proof of this result are very simple and have n = 3 and s = 1.
We also give an example (Example 4.2 below) with n = 4 and s = 2, to show that this
is not a phenomenon unique to s = 1.
We now consider the case of more general β. We make the following assumptions:
(A1) γ(x) defined by γ(x) := 1 is a smooth convex function satisfying
β(x)
γ(0) = 1, γ′(x) 0, lim γ(x) = .
≥ x→∞ ∞
(A2) sup x∈R(logγ)′(x) < ∞.
Given(A1), assumption(A2)isequivalenttotheboundednessof(logβ)′. Thefunctions
β(x) = (1+x)−1 and β(x) = e−x satisfy (A1) and (A2).
Under these assumptions, we prove the following.
Theorem 1.3. If diamY(t ) for t then the sequence
i i
→ ∞ → ∞
Y(t )
i
diamY(t )
i
subconverges to n distinct points Y = (y∞,...,y∞) in Rs.
∞ 1 n
In the special case β(x) = (1 + x)−1, Theorem 1.3 is a consequence of a result of
Jeong and Wu [8] (see Proposition 3.1 below).
It is easy to find examples where diamY(t) remains bounded in t, even when β(x) =
(1+x)−1. The following gives rise to a large class of examples.
4Theorem 1.4. If p = p for all j > 2 and y = y in Y then there exists a uniform
1j 2j 1 2 0
constant C such that
diamY(t) C.
≤
In particular, if diamY(t) is unbounded, one can add an extra point to this data
(“doubling up” one of the existing points) and the diameter will remain bounded.
We also give an example (see Example 4.1 below) where diamY(t) 0 as t .
→ → ∞
There are some natural questions to ask regarding the long time behavior of Y(t).
(i) If diamY(t) as t , is this true also when Y is perturbed slightly?
0
→ ∞ → ∞
(ii) Is the limit Y in Theorem 1.3 independent of the choice of subsequence of
∞
times?
(iii) In the case β(x) = (1+x)−1, are there probability distributions (p ) such that
ij
diamY(t) for generic initial data Y ?
0
→ ∞
(iv) In the case β(x) = e−x what can we say about the limits Y(t ) Y ? Under
i ∞
→
what assumptions are the elements of Y distinct?
∞
Given a probability distribution (p ), we expect the space of all possible limits Y ,
ij ∞
with varying initial data Y , to be quite complicated in general.
0
1.4. SNE versus t-SNE. Our results show a marked difference in the behavior of the
flow Y(t) in the SNE versus t-SNE cases. Only t-SNE exhibits divergence of points
at infinity, and our Theorem 1.3 shows that if this occurs one obtains distinct points
in the rescaled limit. We believe this is related to the so-called “crowding problem”
discussed by van der Maaten and Hinton in [14, Section 3.2]. They note that in SNE,
the points y tend to be pushed towards the center, preventing gaps from occurring
i
between clusters. They give heuristic reasons why the heavier tails of the probability
distributionint-SNE may compensatefor thiseffect, pushingpoints apart. Indeed,this
motivated their construction of the t-SNE algorithm.
The outline of the paper is as follows. In Section 2 we compute the formula (1.3)
for the gradient flow. Section 3 is the heart of the paper, in which we prove the main
results, Theorems 1.1 to 1.4. Finally, in Section 4, we give two further examples.
Acknowledgements. The author is very grateful to Antonio Auffinger for spurring
his interest in the mathematical study of t-SNE, and for some helpful comments.
2. The gradient flow of relative entropy
In this section we prove the formula (1.3) for the gradient flow of the entropy . The
C
gradient of was calculated for β = e−x and β(x) = (1+x)−1 in [7, 14] (see also [4,
C
Chapter 16]) and its extension to general β is straightforward. However, we include the
details here for the convenience of the reader. Recalling (1.1), we write
β(y y 2)
(2.1) q = | i − j | , for Z := β(y y 2).
ij k ℓ
Z | − |
Xk6=ℓ
Compute for m = j,
6
2
∇ymq
mj
=
Z2
Zβ′( |y
m
−y
j
|2)(y
m
−y j) −2β( |y
m
−y
j
|2) β′( |y
m
−y
ℓ
|2)(y
m
−y ℓ).
ℓX6=m
 
5On the other hand, if m is not equal to either i or j, we have
4q
q = ij β′(y y 2)(y y ).
∇ym ij
− Z |
m
−
ℓ
|
m
−
ℓ
ℓX6=m
Then
p
ij
= q
∇ymC
− q
∇ym ij
Xi6=j ij
p p
mj ij
= 2 q q
− q
∇ym mj
− q
∇ym ij
jX6=m mj Xi6=j ij
i6=m,j6=m
p
= 4 mj Zβ′(y y 2)(y y )
− Zβ(y y 2)(cid:18) | m − j | m − j
jX6=m | m − j |
2β(y y 2) β′(y y 2)(y y )
m j m ℓ m ℓ
− | − | | − | − (cid:19)
ℓX6=m
p
+4 ij β′(y y 2)(y y ).
m ℓ m ℓ
Z | − | −
Xi6=j ℓX6=m
i6=m,j6=m
But since p = 1 we have
i6=j ij
P
p = 1 2 p ,
ij mj
−
Xi6=j jX6=m
i6=m,j6=m
and hence
p β′(y y 2)
mj m j
= 4 | − | (y y )
∇ymC − β(y y 2) m − j
jX6=m | m − j |
8
+  p mj β′(y m y ℓ 2)(y m y ℓ)
Z | − | −
jX6=m ℓX6=m
 
4
+ (1 2 p ) β′(y y 2)(y y )
mj m ℓ m ℓ
Z − | − | −
jX6=m ℓX6=m
= 4 (p q )(y y )(logβ)′(y y 2),
mℓ mℓ m ℓ m ℓ
− − − | − |
ℓX6=m
and then (1.3) follows.
3. Proofs of the main results
We begin with the most general setting, and then specialize later to the cases of
β = (1 + x)−1 and β(x) = e−x. Assume that the function γ(x) := 1 satisfies
β(x)
conditions (A1) and (A2) as in the introduction.
6As observed in [8], the center of mass of the points y ,...,y does not change in t.
1 n
Indeed,
n
d
y = 4 (p q )(y y )(logβ)′(y y 2) = 0,
i ij ij i j i j
dt − − | − |
Xi=1 Xj6=i
since (y y ) is anti-symmetric in i,j while (p q )(logβ)′(y y 2) is symmetric
i j ij ij i j
− − | − |
in i, j. We may and do assume from now on that the center of mass of Y is the origin.
Define S := n y 2. We note that since the center of mass of Y is the origin,
i=1| i |
(3.1) P C−1diamY √S CdiamY.
≤ ≤
We will later make use of the following, which was already proved by Jeong and Wu
[8] in the case β = (1+x)−1.
Proposition 3.1. There exist uniform constants C and c> 0 such that if S C then
≥
(3.2) y y 2 cS, for i = j.
i j
| − | ≥ 6
We first prove an elementary lemma, which uses assumption (A1).
Lemma 3.1. Given C 1, there exists C depending only on β and C such that if
0
≥
z C and β(w) Cβ(z), then
0
≥ ≤ z
w .
≥ 2C
Proof. Suppose z C . The convexity of γ implies that for any τ [0,1],
0
≥ ∈
γ((1 τ)z) τγ(0)+(1 τ)γ(z).
− ≤ −
Choose τ = 1 1 and choose C large enough so that γ(z) 2C. Then we have
− 2C 0 ≥
z 1 1 1 1
γ < 1+ γ(z) γ(z)+ γ(z) = γ(z).
2C 2C ≤ 2C 2C C
(cid:16) (cid:17)
Hence,
z
β > Cβ(z) β(w),
2C ≥
(cid:16) (cid:17)
wherethesecond inequality follows fromtheassumption. Sinceβ is decreasing, wehave
w z . (cid:3)
≥ 2C
We now prove the proposition.
Proof of Proposition 3.1. It must be true that for some i = j we have y y 2 cS
i j
6 | − | ≥
since the center of mass of the points is the origin. Without loss of generality, assume
that y y 2 cS. To prove (3.2) holds for all i = j we argue as follows. Since the
1 2
| − | ≥ 6
gradient flow decreases the functional
p
ij
= p log ,
ij
C q
Xi6=j ij
we have for i = j,
6
p logq C,
ij ij
− ≤
which implies in particular that
β(y y 2)
(3.3) c′ q = | 1 − 2 | ,
≤ 12 β(y y 2)
i6=j | i − j |
P 7for a uniform positive c′. Recall that by assumption, each p is strictly positive.
ij
For each i = j, we have
6
β(y y 2) Cβ(y y 2).
i j 1 2
| − | ≤ | − |
Applying the lemma we see that if S is sufficiently large then
y y 2 cS
y y 2 | 1 − 2 | ,
i j
| − | ≥ 2C ≥ 2C
as required. (cid:3)
We can now prove Theorem 1.1.
Proof of Theorem 1.1. Compute
n
d
y 2 = 8 (p q )y (y y )(logβ)′(y y 2)
i ij ij i i j i j
dt | | − · − | − |
Xi=1 Xi6=j
= 8 (p q )y (y y )(logβ)′(y y 2)
ij ij i i j i j
− · − | − |
Xi<j
+8 (p q )y (y y )(logβ)′(y y 2)
ij ij j j i i j
− · − | − |
Xi<j
= 8 (p q )y y 2(logβ)′(y y 2).
ij ij i j i j
− | − | | − |
Xi<j
Now for i = j, write
6
A = β(y y 2),
ij i j
| − |
and as in (2.1) above,
A
ij
q = , Z := 2 A .
ij kℓ
Z
Xk<ℓ
Then,
d 8
S = (p Z A ) y y 2(logβ)′(y y 2).
(3.4) ij ij i j i j
dt Z − | − | | − |
Xi<j
In case (i), we compute
x(logβ)′(x) = β(x) 1,
−
and hence
d 8
S = (p Z A )(A 1)
ij ij ij
dt Z − −
Xi<j
8
= (p Z A )A ,
ij ij ij
Z −
Xi<j
since
1
(3.5) (p Z A ) = Z A = 0.
ij ij ij
− 2 −
Xi<j Xi<j
8Hence for S large,
d C A2 1 C
S i6=j ij C′ A = C′ ,
dt ≤ P A ≤ ij 1+ y y 2 ≤ S
i6=j ij Xi6=j Xi6=j | i − j |
P
where we used Proposition 3.1 for the last inequality. Hence
d
S2 C,
dt ≤
giving S2 Ct and the bound (1.4) follows from (3.1).
≤
In case (ii),
x(logβ)′(x) = x = logβ(x),
−
and so from (3.4),
d 8
S = (p Z A )logA
ij ij ij
dt Z −
Xi<j
(3.6)
8 A
ij
= (p Z A )log ,
ij ij
Z − Z
Xi<j
using again (3.5).
Next we claim that there is a universal constant η = η(s) > 0 such that for any n
points y ,...,y Rs with n > s+1,
1 n
∈
(3.7) max y y 2 (1+η)min y y 2.
i j k ℓ
| − | ≥ k6=ℓ | − |
Indeed,withoutlossofgeneralitywemayassumethatmin y y 2 = 1(ifmin y
k6=ℓ k ℓ k6=ℓ k
| − | | −
y 2 = 0 there is nothing to prove). If the claim is false then we can find a sequence
ℓ
|
(y(ℓ) ,...,y(ℓ) )∞ such that 1 y(ℓ) y(ℓ) 2 (1+ℓ−1) for all i,j. Letting ℓ pro-
du1 ces n >n s+ℓ 1=1 points in Rs w≤ hi| chi ar− e aj ll e| qu≤ idistant from each other. This c→ ont∞ radicts
an elementary result that the equilateral dimension of Euclidean space Rs is s+1 (see
[5], for example).
We will now show that there are uniform positive constants c and C such that for S
sufficiently large
d
(3.8) S cS +C.
dt ≤ −
Given this we are done since it implies that S decreases when it is too large, and hence
S must be bounded.
Working with Y(t) = (y (t),...,y (t)) at a fixed t, we may assume without loss of
1 n
generality by the claim (3.7) above that
y y 2 (1+η)min y y 2.
1 2 k ℓ
| − | ≥ k6=ℓ | − |
Then, since S is large, using Proposition 3.1,
A =
e−|y1−y2|2 e−(1+η)min|yk−yℓ|2 e−c′Se−min|yk−yℓ|2 <e−c′SZ.
12
≤ ≤
9In particular, A is very small compared to Z. From (3.6) we have
12
d 8 A 8 A
12 ij
S = (p Z A )log + (p Z A )log
12 12 ij ij
dt Z − Z Z − Z
Xi<j
(i,j)6=(1,2)
4c′ 8 A
ij
p ZS+ (p Z A )log ,
12 ij ij
≤ − Z Z − Z
X
i<j,(i,j)6=(1,2)
pijZ−Aij<0
since we may assume that p Z A p12Z, and noting that log Aij 0 so that we
12 − 12 ≥ 2 Z ≤
can discard terms in the sum with p Z A 0. Then for S large,
ij ij
− ≥
d 8
S = cS + (p Z A )logp
ij ij ij
dt − Z −
X
i<j,(i,j)6=(1,2)
pijZ−Aij<0
cS +C < 0,
≤ −
proving (3.8) as required. (cid:3)
Remark 3.1. It was pointed out to the author by Antonio Auffinger that if one is
interested in theLangevin dynamics instead of thegradient flow, thesame computation
can be used to estimate the expectation of S.
Proof of Theorem 1.2. We provide an example with three points in R. Write p =
12
p = a so that p = 1/2(1 4a) with a (0,1/4) to be determined. Assume that
23 13
− ∈
y (t) = X(t), y (t) = 0 and y (t) = X(t) so that the points are symmetric about the
1 2 3
−
origin, with X(0) > 0. Recalling (2.1), calculate Z = 4β(X2)+2β(4X2) and
d β(X2)
X = 4 a X(logβ)′(X2)
dt (cid:18) − 4β(X2)+2β(4X2)(cid:19)
1 β(4X2)
+4 (1 4a) 2X(logβ)′(4X2)
(cid:18)2 − − 4β(X2)+2β(4X2)(cid:19)
4X γ′(X2)
= (1 4a)β(X2) 2aβ(4X2)
Z − − γ(X2)
(3.9) (cid:0) (cid:1)
8X γ′(4X2)
+ 4aβ(4X2) 2(1 4a)β(X2)
Z − − γ(4X2)
(cid:0) (cid:1)
4X 1
= (1 4a)γ(4X2) 2aγ(X2)
Z (γ(X2))2(γ(4X2))2(cid:20) − −
(cid:0) (cid:1)
γ′(X2)γ(4X2) 4γ′(4X2)γ(X2) .
· − (cid:21)
(cid:0) (cid:1)
For (i) we have γ(x) = 1+x and the term in the square brackets is
(3.10) [ ]= 3 1 6a+(4(1 4a) 2a)X2 .
··· − − − −
(cid:0) (cid:1)
Now choose a (2/9,1/4) so that 1 6a < 0 and 4(1 4a) 2a < c for a uniform
∈ − − − −
c> 0. Then
[ ] cX2.
··· ≥
10It follows that for X large,
d cXX2
X = cX−3,
dt ≥ X−2X4X4
and hence X4 ct. Then for t 1,
≥ ≥
1
diamY(t) = 2X(t) ct4,
≥
giving the example for (i).
For (ii), choose a (1/6,1/4). Then the term in the square brackets in (3.9) is
∈
[ ]=
3e5X2
((1
4a)e4X2 2aeX2
),
··· − − −
which is negative for X large and positive for X small. This implies that the diamY(t)
is bounded from below away from zero, as required. (cid:3)
For Theorem 1.3 we argue as follows.
Proof of Theorem 1.3. The only nontrivial assertion is that points in Y are distinct,
∞
but this follows immediately from Proposition 3.1. (cid:3)
Finally, we prove Theorem 1.4.
Proof of Theorem 1.4. We first show that y (t) = y (t) for all t. Compute
1 2
d
(y y )2 = 8(y y ) (p q )(y y )(logβ)′(y y 2)
1 2 1 2 12 12 1 2 1 2
dt − − (cid:20) − − | − |
(p q )(y y )(logβ)′(y y 2)
12 12 2 1 1 2
− − − | − |
+ (p q )(y y )(logβ)′(y y 2)
1j 1j 1 j 1 j
− − | − |
Xj>2
(p q )(y y )(logβ)′(y y 2) .
2j 2j 2 j 2 j
− − − | − | (cid:21)
Xj>2
Consider Y(t) for t [0,T] where T > 0 is fixed. In what follows, C,C′ will denote
∈
constants that may now depend also on T and Y(t) for t [0,T]. By the Mean Value
∈
Theorem,
(logβ)′(y y 2) (logβ)′(y y 2) C y y 2 y y 2 C′ y y ,
1 j 2 j 1 j 2 j 1 2
| | − | − | − | | ≤ | − | −| − | ≤ | − |
(cid:12) (cid:12)
and similarly (cid:12) (cid:12)
q q C y y .
1j 2j 1 2
| − | ≤ | − |
Hence, using the fact that p = p for all j > 2,
1j 2j
d
(y y )2 C(y y )2,
1 2 1 2
dt − ≤ −
andthuse−Ct(y y )2 isdecreasingon[0,T]andinitially vanishes. Hencey (t) = y (t)
1 2 1 2
−
on [0,T] and since T was arbitrary this proves y (t) = y (t) for all t.
1 2
The theorem then follows immediately from Proposition 3.1. (cid:3)
Remark 3.2. Note that in the example given in the proof of Theorem 1.2 for β(x) =
(1+x)−1, if we instead chose the initial data with y = y then it follows from Theorem
1 3
1.4thatdiamY(t)isbounded. Henceforagiven probabilitydistribution p , whether
ij
{ }
diamY(t) tends to infinity or not may depend on the initial data Y .
0
114. Examples
Example 4.1. This is an example of three points in R which collapse to the origin as t
tends to infinity.
Take β(x) = (1+ x)−1 and take s = 1 and p = p = p = 1/6. Assume that
12 13 23
y (t) = X(t), y (t) = 0 and y (t) = X(t) so that points are symmetric about the
1 2 3
−
origin, and we take X(0) = 1. From (3.9) and (3.10) with a = 1/6 we have
dX 12X 1
= X2.
dt − Z (γ(X2))2(γ(4X2))2
Then we see that 0 X(t) 1 and hence
≤ ≤
dX
CX3.
dt ≤ −
It follows that
d 1
X−2 ,
dt ≥ C
which gives
1
diamY(t) = 2X(t) Ct− 2 0 as t .
≤ → → ∞
If β(x) = e−x then
dX
=
4X 1 e6X2 (e3X2
1).
dt − Z (γ(X2))2(γ(4X2))2 −
It follows that 0 X(t) 1 and since e3X2 1 3X2, we have
≤ ≤ − ≥
dX
CX3
dt ≤ −
and hence
1
diamY(t) = 2X(t) Ct− 2 0 as t .
≤ → → ∞
Example 4.2. This is an example of four points in R2. First assume β(x) = (1+x)−1.
We will see that the flow has similar long time behavior as in the case of 3 points in R
described in the proof of Theorem 1.2. Consider the four points y ,...,y in R2 with
1 4
coordinates
y = (X,0), y = (0,X), y = ( X,0), y = (0, X),
1 2 3 4
− −
for X = X(t), and X(0) > 0. Define
p = p = p = p = a
12 23 34 41
1
p = p = (1 8a),
13 24
4 −
for a (0,1/8) to be determined. We have Z = 8β(2X2)+4β(4X2). Compute
∈
dX 4 β(y y 2)
= 4 p | 1 − j | (y y ) (logβ)′(y y 2),
1j 1 j 1 1 j
dt (cid:18) − Z (cid:19) − | − |
Xj=2
where (y y ) means the x component of y y .
1 j 1 1 1 j
− −
12Compute
dX β(2X2) γ′(2X2) β(4X2) 1 γ′(4X2)
= 8 a X +4 (1 8a) 2X
dt (cid:18) Z − (cid:19) γ(2X2) (cid:18) Z − 4 − (cid:19) γ(4X2)
8X 1
= (1 8a)γ(4X2) 4aγ(2X2)
Z (γ(2X2))2(γ(4X2))2(cid:20) − −
(cid:0) (cid:1)
γ′(2X2)γ(4X2) 2γ′(4X2)γ(2X2) .
· − (cid:21)
(cid:0) (cid:1)
Using now γ(x) = 1+x, the term in the square brackets is
[ ]= 1 12a+(4(1 8a) 8a)X2 .
··· − − − −
(cid:0) (cid:1)
Choose a (1/10,1/8) so that 1 12a < 0 and 4(1 8a) 8a < c for a uniform c > 0.
∈ − − − −
Then
[ ] cX2
··· ≥
and
1
diamY(t) = 2X(t) ct4,
≥
as in the proof of Theorem 1.2.
If β(x) = e−x then by choosing a (1/12,1/8) one can check that the diameter
∈
remains bounded from below away from zero.
References
1. Arora S., Hu, W., Kothari, P. K., An analysis of the t-sne algorithm for data visualization, In
Bubeck,S.,Perchet,V.,Rigollet,P.,editors,Proceedingsofthe31stConferenceOnLearningTheory,
Proceedings of Machine Learning Research 75, pages 1455–1462. PMLR, 06–09 Jul 2018
2. Auffinger,A.,Fletcher,D.,Equilibriumdistributions fort-distributed stochastic neighborhood embed-
ding, preprint,arXiv:2304.03727
3. Cai, T. T., Ma, R., Theoretical foundations of t-sne for visualizing high-dimensional clustered data,
J. Machine Learning Res., 23, no. 301 (2022), 1–54
4. Ghojogh, B., Crowley, M., Karray,F.,Ghodsi, A.Elements of Dimensionality Reduction and Mani-
fold Learning, Springer, 2023
5. Guy, R. K., An olla-podrida of open problems, often oddly posed, American Mathematical Monthly
90, (1983), no. 3, 196–200
6. Hajibabaee, P., Pourkamali-Anaraki, F., Hariri-Ardebili, M.A., An empirical evaluation of the t-
sne algorithm for data visualization in structural engineering , In 2021 20th IEEE International
Conference on Machine Learning and Applications (ICMLA), 2021 1674-1680
7. Hinton,G.E.,Roweis,S.,Stochastic neighbor embedding,Adv.neuralinformationprocessingsys.15
(2002)
8. Jeong,S.,Wu,H.-T.,Convergenceanalysisoft-SNEasagradientflowforpointcloudonamanifold,
preprint,arXiv:2401.17675
9. Kobak,D.,Berens,P.,The art ofusing t-sne forsingle-cell transcriptomicsNatureCommunications
10, (2019), no. 1, 1–14
10. Li, W., Cerise, J.E., Yang, Y., Han, H., Application of t-sne to human genetic data J Bioinform.
Comput. Biol. 15 (2017), no. 4
11. Linderman,G.C., Steinerberger,S.,Clustering with t-sne, provably, SIAMJournalonMathematics
of Data Science, 1, no. 2 (2019), 313–332
12. Shaham, U., Steinerberger, S., Stochastic neighbor embedding separates well-separated clusters,
preprint,arXiv:1702.02670
13. Steinerberger, S., Zhang, Y., t-SNE, forceful colorings, and mean field limits, Res. Math. Sci. 9
(2022), no.3, Paper No. 42, 30 pp
1314. vanderMaaten,L.,Hinton,G.,Visualizing Data using t-SNE, J.MachineLearningRes.9, no.86,
(2008), 2579–2605
15. Wang, Y., Huang, H., Rudin, C., Shaposhnik, Y., Understanding How Dimension Reduction Tools
Work: An Empirical Approach to Deciphering t-SNE, UMAP, TriMap, and PaCMAP for Data
Visualization, J. Machine Learning Res. 22 (2021), no.1, 9129–9201
Departmentof Mathematics, NorthwesternUniversity,2033 SheridanRoad, Evanston,
IL 60208, USA., email: weinkove@math.northwestern.edu
14