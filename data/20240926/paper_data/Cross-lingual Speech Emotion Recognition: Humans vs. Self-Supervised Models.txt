Cross-lingual Speech Emotion Recognition:
Humans vs. Self-Supervised Models
Zhichen Han Tianqi Geng Hui Feng
University of Edinburgh, UK Tianjin University, China Tianjin University, China
Jiahong Yuan Korin Richmond Yuanchao Li†
University of Science and Technology of China, China University of Edinburgh, UK University of Edinburgh, UK
Abstract—UtilizingSelf-SupervisedLearning(SSL)modelsforSpeech 4) Can SSL-based models identify emotionally salient segments
Emotion Recognition (SER) has proven effective, yet limited research similar to human behaviors?
has explored cross-lingual scenarios. This study presents a comparative
To answer the above questions, we conduct a comparative study
analysisbetweenhumanperformanceandSSLmodels,beginningwitha
between humans and SSL models, specifically:
layer-wiseanalysisandanexplorationofparameter-efficientfine-tuning
strategies in monolingual, cross-lingual, and transfer learning contexts. • Weperformalayer-wiseanalysisandinvestigatevariousPEFT
We further compare the SER ability of models and humans at both strategies for SSL models in monolingual, cross-lingual, and
utterance-andsegment-levels.Additionally,weinvestigatetheimpactof
transfer learning settings, comparing SER performance with
dialect on cross-lingual SER through human evaluation. Our findings
reveal that models, with appropriate knowledge transfer, can adapt human performance across emotions.
to the target language and achieve performance comparable to native • We evaluate SER performance on Tianjin speech (a Chinese
speakers. We also demonstrate the significant effect of dialect on SER dialect),exploringtheimpactofdialectonhumanlistenerswith
for individuals without prior linguistic and paralinguistic background.
andwithoutlinguisticandparalinguisticbackgroundknowledge.
Moreover, both humans and models exhibit distinct behaviors across
differentemotions.Theseresultsoffernewinsightsintothecross-lingual • We assess both human and SSL model performance on the
SER capabilities of SSL models, underscoring both their similarities to Speech Emotion Diarization (SED) task (i.e., segment-level
anddifferencesfromhumanemotionperception. SER), aiming to compare their ability to detect prominent
Index Terms—Speech Emotion Recognition, Speech Emotion Diariza- emotion segments.
tion,Cross-lingualEvaluation,Self-SupervisedModels
II. RELATEDWORK
I. INTRODUCTION Onthemodelside,previousstudieshavetypicallyfine-tunedSER
TheadvancementofSelf-SupervisedLearning(SSL)hasledtothe models using target language data, but have observed a significant
development of powerful pre-trained models, such as Wav2vec 2.0 dropinperformancewhenshiftingfrommonolingualtocross-lingual
(W2V2) [1] and WavLM [2], including their multilingual variants. conditions [7], [12]. Additionally, adversarial neural networks in
These models have demonstrated remarkable success across a range unsupervised settings have been explored for cross-lingual adapta-
ofdownstreamspeechtasks,includingSpeechEmotionRecognition tion [13], [14]. More recently, [15] introduced a layer-anchoring
(SER) [3]. To further enhance their adaptability across different mechanism to facilitate emotion transfer, accounting for the task-
languages and datasets for SER, Parameter-Efficient Fine-Tuning specific nature and hierarchical structure of speech models. On the
(PEFT) has been utilized to improve the efficacy of SSL models human side, [10] found that SVM models outperformed humans in
while minimizing fine-tuning requirements [4], [5]. monolingual settings, whereas humans were less affected by cross-
Nevertheless, cross-lingual SER remains a significant challenge lingual challenges. Further research by [16] concluded that human
due to language and cultural differences [6]. Typically, both tradi- cross-lingual capabilities in SER are generally robust.
tionalandSSLmodelsrequiresufficienttrainingdatainthetargetlan- Despite this progress, comparative studies between humans and
guage to achieve satisfactory cross-lingual SER performance, which models remain lacking, leading to an insufficient understanding of
is often infeasible for languages lacking emotional speech datasets human-modelcomparison.Toourknowledge,wearethefirsttocon-
[7], [8]. For humans, however, although cross-lingual barriers exist ductacomparativestudybetweenhumansandSSLmodels,exploring
[9],emotionsinspeechareuniversallydistinguishableashumansare notonlyutterance-levelSERbutalsofine-grainedemotionperception
less affected by cross-lingual differences [10]. (i.e., SED), the impact of dialect, and fine-tuning strategies.
WhilesomeresearchhasexploredtheuseofSSLmodelsforcross- III. MATERIALSANDMETHODOLOGY
lingualandmultilingualSER[11],therehasbeenlittleinvestigation
A. Datasets and Models
into how these models compare to human performance. To this end,
we raise four key questions: As various tasks are investigated in this work, we use multiple
1) Can SSL-based models achieve competitive SER performance datasets and models. For the datasets, four public emotion corpora
to that of humans? and a non-public dialect corpus are used:
2) How to better fine-tune SSL models for SER in cross-lingual • ESD:aMandarinChinese(CN)emotioncorpus[17],containing
scenarios? utterances spoken by ten native CN speakers (five male, five
3) Does dialect have an impact on human perception in cross- female) across five emotion categories.
lingual SER? • PAVOQUE: a German (DE) emotion corpus [18], featuring a
professional male actor with five emotion categories, where
†Correspondingauthor.yuanchao.li@ed.ac.uk neutral comprises over 50% of the dataset.
4202
peS
52
]SA.ssee[
1v02961.9042:viXra• ZED: an English emotion corpus specifically designed for the D. Comparison of SSL Models with Human Evaluation
SED task [19], with speech data annotated by humans at both
Inthistask,weuseallthedatasetsandmodels.ForSER,weuse
utterance and sub-utterance (segment) levels. the emotions: angry, happy, neutral, and sad; while for SED, we
• TJD:anon-publicTianjin(TJ)Chinesedialectcorpuscollected exclude neutral as it does not contain emotion variation to perceive
in our previous work [20]. It was recorded and annotated at
and segment.
Tianjin University by two native Tianjin dialect speakers who
Six native DE speakers (one male, five female) and six native
were university students. It includes three functional categories
CN speakers (two male, four female), with no prior knowledge
(question,negation,expectation),approximatedtoemotionsdue
of each other’s language, are recruited for the human evaluation
to high acoustic similarity. According to annotators, negation
from the Univ. of Edinburgh and Tianjin Univ. All participants have
resembles anger, and expectation resembles happiness. Tianjin
studied English for many years with sufficient skills (e.g., IELTS
dialectisknownforitscomplextonesandhipatternswhilefea-
score ≥ 6.5). The webMUSHRA interface [26] is used to create the
turingasimilarbutslightlydifferenttonesystemthanMandarin
experimental tests.
[21]. Native speakers of the Tianjin dialect convey emotions
For SER, participants listen to speech samples and identify the
moredirectlywithnoticeablesonorousvowelsandfasterspeech
conveyed emotion. We use UA as the evaluation metric, consistent
[22].
with the model performance evaluation. Additionally, to investigate
For the models, we use three W2V2 base models pre-trained on fine-grained speech emotion expression, we perform SED, where
Mandarin CN1, DE2, and EN3, along with a WavLM large model participantsfirstlistentospeechsamplesandlabeltheemotion,asin
trained on EN emotional speech 4. the SER task. Subsequently, they clip the speech and select the seg-
The following tasks are conducted using different models and ment that most prominently expresses the emotion. Following [19],
datasets for specific purposes. we use the Emotion Diarization Error Rate (EDER) as the metric,
B. Layer-wise Analysis of SSL Models whichcalculatestheerrorrateofdiarizationresults,includingmissed
emotions(ME),falsealarms(FA),overlaps(OL),andconfusion(CF):
In this task, we use the datasets: ESD, PAVOQUE, IEMOCAP;
the models: W2V2-CN, -DE, -EN; and the emotions: angry, happy, ME+FA+OL+CF
EDER= (1)
neutral, and sad. Uttrance Duration
SSL models encode speech information across different layers;
For comparison with the SSL models, we compare participants’
specifically, in SER tasks, speech representations from the middle
performance on their native language with the monolingual setting,
layers often yield higher performance [23]. Therefore, we perform
theirperformanceonthenon-nativelanguageswiththecross-lingual
a layer-wise analysis to identify the optimal layer for monolingual
or transfer learning settings. Finally, we explore whether dialect has
and cross-lingual SER. SSL models are used as feature extractors
an impact on human perception of cross-lingual SER.
with all parameters frozen, and Unweighted Accuracy (UA) is used
as the evaluation metric. The analysis is conducted in the following IV. EXPERIMENTS
settings: A. Experimental Settings
• Monolingual(Mono):Themodelisfine-tunedwithbothtraining ForSER,toreducetheeffectofvaryingtrainingdatasizes,weuse
andtestdatafromspeechinthesamelanguageasitspre-training
thesameamountofdataforCN,DE,andEN.Toensureabalanced
language. For example, W2V2-CN is fine-tuned using CN data
emotion distribution, we use an equal number of samples for each
(ESD) as both training and test data.
emotion.Specifically,forESD,PAVOQUE,andIEMOCAP,weapply
• Cross-lingual (Cross): The model is fine-tuned using its pre- 5-foldcross-validationformodeltraining:400utterancesperemotion
traininglanguageastrainingdataandadifferentlanguageastest
category, totaling 1,600 utterances per dataset, are used for training.
data.Forexample,W2V2-CNisfine-tunedusingCNdata(ESD)
Similarly, 200 utterances are randomly selected for validation and
and tested on DE data (PAVOQUE) or EN data (IEMOCAP).
test sets, respectively. Given the difficulty of performing human
• Transfer learning (Trans): The model is fine-tuned and tested evaluation on all the data, for comparison with human evaluation,
on a language different from its pre-training language. For
weselect12sentencesperemotioncategory,totaling144utterances
example, W2V2-CN is fine-tuned and tested on either DE data
for each language. The model settings are as follows:
(PAVOQUE) or EN data (IEMOCAP).
1) Layer-wise analysis: We use a classification head projecting
C. PEFT of SSL Models for Cross-lingual SER fromdimension768to4forSER,withalearningrateof1e-4,epsilon
of1e-8,andweightdecayof1e-5,trainedfor100epochswithabatch
Inthistask,weusethedatasets:ESD,PAVOQUE,IEMOCAP;the
sizeof32.Cross-entropyisusedasthelosscriterion.Trainingstops
models: W2V2-CN, -DE; and the emotions: angry, happy, neutral,
if the validation loss does not decrease for 10 consecutive epochs.
and sad.
2) PEFTstrategies: Weusethesameclassificationheadconfigu-
Afterthelayer-wiseanalysis,thebest-performinglayersarefurther
rationasinthelayer-wiseanalysisforPEFT.FortheLoRAmodule,
fine-tuned using various PEFT strategies to enhance performance.
theattentionheadissetto8,alphaforscalingis16,withadropout
We apply the Low-Rank Adapter (LoRA) [24], Bottleneck Adapter
rateof0.1.FortheBAmodule,thereductionfactoris16.Modelsare
(BA) [25], and Weighted Gating (WG). Additionally, a two-stage
trainedfor100epochswithabatchsizeof16.Thelossandstopping
fine-tuning [5] is performed: the model is first fine-tuned on the
criteria from the layer-wise analysis remain the same.
sourcelanguage,thenonthetargetlanguageoncethefirstfine-tuning
converges. For SED, given the considerable effort required for segmenting
speech, only 8 utterances per emotion are randomly selected from
1https://huggingface.co/TencentGameMate/chinese-wav2vec2-base ZED, totaling 24 utterances, for comparison with human evaluation
2https://huggingface.co/facebook/wav2vec2-base-de-voxpopuli-v2 and model results5.
3https://huggingface.co/facebook/wav2vec2-base-960h
4https://huggingface.co/speechbrain/emotion-diarization-wavlm-large 5Codeavailable:https://github.com/zhan7721/Crosslingual SERTABLEI:Modelperformanceundermonolingual,cross-lingual,and
transfer learning with various PEFT strategies.
PEFTstrategy
Model Setting Source Target UA%
LoRA BA+WG 2-stg
91.4
Mono CN CN ✓ 87.0
✓ ✓ 93.9
62.8
Cross CN DE ✓ 65.3
✓ ✓ 70.7
W2V2
98.5
-CN ✓ 98.8
Trans DE DE ✓ ✓ 98.8
✓ ✓ ✓ 98.9
65.5
Trans EN EN ✓ 66.3
✓ ✓ 67.7
98.9
Mono DE DE ✓ 97.8
✓ ✓ 97.9
52.2
Cross DE CN ✓ 58.5
✓ ✓ 56.0
W2V2
84.2
-DE ✓ 83.6
Trans CN CN ✓ ✓ 87.5
✓ ✓ ✓ 85.8
62.4
Trans EN EN ✓ 65.0
✓ ✓ 66.0
Fig.1:Layer-wiseanalysisofCNandDEmodelsundermonolingual,
TABLEII:Humanperformance(UA%)onalllanguages.Thehigher
cross-lingual and transfer learning settings.
the value, the better the SER performance.
CN DE EN TJ
B. Results and Discussions CN participants 79.5 73.3 63.5 67.5
DE participants 82.6 91.7 73.6 29.2
The results of the layer-wise analysis are presented in Figure 1.
Inthemonolingualsetting,boththeCNandDEmodelsdemonstrate
strongperformanceontheirrespectivesourcelanguages,asexpected, false alarms for sad in neutral DE speech. CN natives, compared to
given that the models are pre-trained on these languages. However, the CN monolingual model, demonstrate lower precision in happy
in the cross-lingual setting, both models show a significant drop andneutral.TheseresultsindicatethatSSLmodelsexhibitexcellent
in accuracy. While this is reasonable due to language differences, monolingual performance on the SER task when provided with
the extent of the drop is beyond our expectations, considering the sufficient training data.
sharedcharacteristicsofemotionalacoustics[27],[28].Onepossible 2) SER: cross-lingual models vs. humans
explanation is that SSL models not only encode low-level acoustic Intermsofoverallaccuracy,asshowninTableIandTableII,both
featuresbutalsotransformthemintohigh-level,linguisticallyrelated humansandmodelsexperienceaperformancedecreaseinthecross-
information, such as word identity and meaning [29]. This process lingualcondition,withcross-lingualmodelsbeingmoresignificantly
creates a linguistic gap across languages, exacerbating the accuracy affected than humans. This aligns with findings from [10], which
decline. Nonetheless, under the transfer learning setting, the models demonstrated that humans are capable of handling cross-lingual
can achieve performance levels comparable to the monolingual set- scenariosbetter.Intermsofperformanceoneveryemotioncategory,
ting, demonstrating the ability of SSL models to adapt to different asshowninTableIII,DEcross-lingualmodelstrugglestorecognize
languagesforSERwithappropriatetechniquesofknowledgetransfer. neutral and sad in CN data, exhibiting low recall. Additionally, the
Thevariationsinthecontoursarerelatedtothetrainingobjectivesof DE model confuses angry and happy more frequently compared to
SSL models, particularly the contrastive masked segment prediction humans in both languages. Conversely, the CN cross-lingual model
(since these patterns align with previous research on layer-wise closely aligns with CN natives when recognizing DE speech, with
analysis of SSL models [23], [29], [30], we omit further detailed both often predicting happy as neutral.
explanation). Moreover, we conduct a two-sided Welch’s t-test on humans’
TheresultsofPEFTundermonolingual,cross-lingual,andtransfer precision, recall, and F1-scores. We notice significant difference
learningsettings,areshowninTableI.HumanperformanceonSER in the recall of happy on DE data between CN and DE speakers
isshowninTableII,andSEDcomparisonispresentedinTableIV. (t(10)=−7.511,p<0.001), as well as in the precision of neutral
From these results, we make the following observations: (t(10)=−5.614,p<0.001). CN speakers also exhibit lower recall
1) SER: monolingual model vs. native speakers forhappyinDEdatathaninCNdata(t(10)=−5.137,p<0.001),
In terms of overall accuracy, as shown in Table I and Table II, suggestingalinguisticandparalinguisticknowledgegapbetweentwo
both models outperform their respective human native speakers. speaker groups. Particularly, significant differences are found in the
For predictions across all emotion categories, Table III presents the recall of sad across CN, DE, and EN data (t(10) = −2.708,p =
confusionmatricesoftheCNandDEmonolingualmodelsalongside 0.022)andintheprecisionofneutral(t(10)=−7.511,p<0.001).
thoseofCNandDEnativesfortheirrespectivelanguages.Compared The precision of neutral is largely impacted by CN speakers’ dif-
to the DE monolingual model, DE natives are more likely to report ficulty in perceiving happy in DE data, indicating that linguisticTABLE III: Confusion matrices of CN and DE speakers and models under monolingual (first row), cross-lingual (second row) and transfer
learning (third row) settings. No humans under the transfer learning setting.
A H N S A H N S A H N S A H N S
A 0.71 0.24 0.06 0.00 A 0.95 0.05 0.00 0.00 A 0.97 0.03 0.00 0.00 A 0.98 0.02 0.00 0.00
H 0.00 0.82 0.18 0.00 H 0.02 0.97 0.02 0.00 H 0.00 0.94 0.05 0.00 H 0.02 0.98 0.00 0.00
N 0.00 0.10 0.86 0.04 N 0.00 0.00 1.00 0.00 N 0.00 0.00 0.83 0.17 N 0.00 0.00 0.93 0.07
S 0.00 0.01 0.18 0.81 S 0.00 0.00 0.03 0.97 S 0.00 0.00 0.04 0.96 S 0.00 0.00 0.00 1.00
(a)Human:CN(mono) (b)Model:CNmono(onCN) (c)Human:DE(mono) (d)Model:DEmono(onDE)
A H N S A H N S A H N S A H N S
A 0.81 0.13 0.06 0.01 A 0.95 0.02 0.03 0.00 A 0.88 0.07 0.06 0.00 A 0.52 0.48 0.00 0.00
H 0.21 0.38 0.42 0.00 H 0.08 0.18 0.51 0.22 H 0.08 0.75 0.15 0.01 H 0.17 0.65 0.17 0.02
N 0.01 0.01 0.93 0.04 N 0.00 0.00 0.95 0.05 N 0.03 0.03 0.76 0.18 N 0.00 0.38 0.38 0.23
S 0.01 0.00 0.15 0.83 S 0.00 0.00 0.00 1.00 S 0.00 0.03 0.06 0.92 S 0.05 0.28 0.30 0.37
(e)Human:CN(crossonDE) (f)Model:CNcross(onDE) (g)Human:DE(crossonCN) (h)Model:DEcross(onCN)
A H N S A H N S A H N S A H N S
A 0.54 0.22 0.13 0.11 A 0.70 0.13 0.10 0.07 A 0.78 0.03 0.04 0.15 A 0.70 0.12 0.17 0.02
H 0.14 0.61 0.19 0.06 H 0.08 0.57 0.35 0.00 H 0.06 0.69 0.22 0.03 H 0.00 0.65 0.33 0.02
N 0.04 0.28 0.58 0.10 N 0.07 0.18 0.75 0.00 N 0.15 0.13 0.60 0.13 N 0.08 0.30 0.58 0.03
S 0.03 0.01 0.15 0.81 S 0.00 0.05 0.02 0.93 S 0.01 0.01 0.10 0.88 S 0.00 0.00 0.25 0.75
(k)Human:CN(L2onEN) (l)Model:CNtrans(onEN) (m)Human:DE(L2onEN) (n)Model:DEtrans(onEN)
and paralinguistic differences affect the perception of sad across effectivelyasinCNdata,confirmingthelinguisticandparalinguistic
languages. impact of dialect.
3) SER: transfer learning models vs. L2 learners 5) SED: models vs. humans in prominent emotion perception
As the transfer learning setting resembles the human learning TheresultsinTableIVindicatethatbothhumangroupsoutperform
processofasecondlanguage(i.e.,fine-tuning≈languagestudy),we the model, with the DE speakers achieving the lowest EDER. The
compare the models with human speakers using EN data. As shown modelperformsbestonhappyandworstonsad.Betweenthehuman
in Table I, SSL models with transfer learning achieve monolingual- groups,CNspeakersareslightlybetteratperceivingangrysegments,
levelperformanceandsurpasshumanaccuracyonCNandDEdata. whileDEspeakersarebetteratidentifyingsadsegments.Thispattern
However,forENdata,DEspeakersexhibithigheraccuracythanCN isconsistentwithSERresultsinTableIII,whereCNspeakersshowa
speakers and all models tested on EN data. Additionally, two-stage higherthresholdforpredictingsad,leadingtohigherrecallbutlower
fine-tuningdoesnotresultinasignificantperformanceboost,which precision.Conversely,DEspeakersdemonstratehigherprecisionbut
was observed in the cross-corpus scenario under the same language lowerrecall.Thedifferenceinsensitivitytosad amongCNspeakers
[5]. These findings suggest that while transfer learning helps SSL results in more false negatives for sad in the SED task.
modelsinadaptingtonewlanguages,performancevariesdepending
on the specific target language dataset. In terms of performance TABLEIV:EDER(%)comparisonofWavLMandhumansonZED
on every emotion category, shown in Table III, CN speakers only data. The lower the score, the better the performance.
outperformthemodelinrecognizinghappy,whereastheCNtransfer
WavLM CN participants DE participants
learning model outperforms humans in the other three emotion
Angry 36.6 25.8 27.5
categories. For DE speakers, humans perform better at predicting
Happy 27.5 31.8 28.7
happy and neutral compared to the DE transfer learning model. In
Sad 50.3 38.6 28.3
addition, an effective PEFT strategy used in monolingual scenarios
Average 38.2 32.1 28.2
is not necessarily useful in cross-lingual or multilingual scenarios.
Moreover,TableIIrevealsthatrecognizingemotioninENismore
challenging than in CN and DE, despite CN and DE speakers being
L2learners.Thisdifficultyislikelyattributedtotheselectionofonly V. CONCLUSION
improvised utterances from IEMOCAP, which are more natural and
real-life emotions, thus making SER more challenging. In this study, we conduct a comparative analysis of cross-lingual
4) SER: linguistic and paralinguistic impact of dialect SERbetweenhumansandSSLmodels,includingbothmodelingand
In addition to the finding in observation 2 that linguistic and par- humanexperiments,andcomparetheirperformanceinmonolingual,
alinguistic differences impact emotion perception across languages, cross-lingual, and transfer learning settings. We perform a layer-
the results on the TJ data in Table II further indicate the existence wise analysis and apply PEFT to the best-performing layers using
of such differences, particularly due to dialect. The SER results multiple strategies to enhance model performance. Additionally,
demonstratethegeneralizabilityofhumanemotionperceptionacross we implement SED for fine-grained detection of salient emotion
languages. However, in the TJD dataset, performance varies signif- segments to evaluate the ability of SSL models to capture segment-
icantly between the two speaker groups. While DE speakers excel level emotion. The results show that humans excel in cross-lingual
withCNspeechdata,theuniqueprosodyoftheTJdialectleadstoa SERandSED,whilemodelscanadapttothetargetlanguagethrough
notableperformancedeclineamongDEspeakers.Thisdiscrepancyis transferlearningtoachievenativespeaker-levelperformance.Wealso
plausiblegiventhatTJprosodyandtonesdiffersignificantlyfromCN revealthelinguisticandparalinguisticimpactofdialectinthecross-
(andlikelymanyothermajorlanguages),makingemotionrecognition lingualsettingthroughhumanevaluations.Ourstudyprovidesnovel
challengingforDEspeakers.Evenwithsomebackgroundknowledge, insights into human emotion perception and the application of SSL
CN speakers also struggle to recognize emotions in TJ data as models for cross-lingual SER.REFERENCES Symposium on Chinese Spoken Language Processing (ISCSLP). IEEE,
2021,pp.1–5.
[1] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael
[15] Shreya G Upadhyay, Carlos Busso, and Chi-Chun Lee, “A layer-
Auli,“wav2vec2.0:Aframeworkforself-supervisedlearningofspeech
anchoringstrategyforenhancingcross-lingualspeechemotionrecogni-
representations,” Advances in neural information processing systems,
tion,”ICASSP2024-2024IEEEInternationalConferenceonAcoustics,
vol.33,pp.12449–12460,2020.
SpeechandSignalProcessing(ICASSP),2024.
[2] Sanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu,
[16] Stefan Werner and Georgii K Petrenko, “Speech emotion recognition:
Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao,
humansvsmachines,” Discourse,vol.5,no.5,pp.136–152,2019.
et al., “Wavlm: Large-scale self-supervised pre-training for full stack
[17] KunZhou,BerrakSisman,RuiLiu,andHaizhouLi, “Emotionalvoice
speechprocessing,” IEEEJournalofSelectedTopicsinSignalProcess-
conversion: Theory, databases and esd,” Speech Communication, vol.
ing,vol.16,no.6,pp.1505–1518,2022.
137,pp.1–18,2022.
[3] YuanchaoLi,PeterBell,andCatherineLai,“FusingASRoutputsinjoint
[18] Ingmar Steiner, Marc Schro¨der, and Annette Klepp, “The PAVOQUE
training for speech emotion recognition,” in ICASSP 2022-2022 IEEE
corpus as a resource for analysis and synthesis of expressive speech,”
International Conference on Acoustics, Speech and Signal Processing
Proc.Phonetik&Phonologie,vol.9,2013.
(ICASSP).IEEE,2022,pp.7362–7366.
[19] Yingzhi Wang, Mirco Ravanelli, and Alya Yacoubi, “Speech emotion
[4] Tiantian Feng and Shrikanth Narayanan, “Peft-ser: On the use of
diarization: Which emotion appears when?,” in 2023 IEEE Automatic
parameter efficient transfer learning approaches for speech emotion
SpeechRecognitionandUnderstandingWorkshop(ASRU).IEEE,2023,
recognitionusingpre-trainedspeechmodels,”in202311thInternational
pp.1–7.
Conference on Affective Computing and Intelligent Interaction (ACII). [20] TianqiGengandHuiFeng, “Formandfunctioninprosodicrepresenta-
IEEE,2023,pp.1–8. tion:Inthecaseof‘ma’intianjinmandarin,” inInterspeech,2024.
[5] NineliLashkarashvili,WenWu,GuangzhiSun,andPhilipCWoodland, [21] QianLi,YiyaChen,andZiyuXiong,“Tianjinmandarin,”Journalofthe
“Parameterefficientfinetuningforspeechemotionrecognitionanddo- InternationalPhoneticAssociation,vol.49,no.1,pp.109–128,2019.
mainadaptation,”inICASSP2024-2024IEEEInternationalConference [22] ShulingQi, AStudyofTianjinDialect’sGrammar, ShanghaiJiaotong
onAcoustics,SpeechandSignalProcessing(ICASSP).IEEE,2024,pp. UniversityPress,2020.
10986–10990. [23] YuanchaoLi,YumnahMohamied,PeterBell,andCatherineLai,“Explo-
[6] MoazzamShoukat,MuhammadUsama,HafizShehbazAli,andSiddique rationofaself-supervisedspeechmodel:Astudyonemotionalcorpora,”
Latif, “Breaking barriers: Can multilingual foundation models bridge in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE,
thegapincross-languagespeechemotionrecognition?,” in2023Tenth 2023,pp.868–875.
InternationalConferenceonSocialNetworksAnalysis,Managementand [24] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Security(SNAMS).IEEE,2023,pp.1–9. Wang, Lu Wang, Weizhu Chen, et al., “LoRA: Low-rank adaptation
[7] SiddiqueLatif,AdnanQayyum,MuhammadUsman,andJunaidQadir, of large language models,” in International Conference on Learning
“Crosslingualspeechemotionrecognition:Urduvs.westernlanguages,” Representations,2021.
in2018Internationalconferenceonfrontiersofinformationtechnology [25] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone,
(FIT).IEEE,2018,pp.88–93. Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and
[8] YoungdoAhn,SungJooLee,andJongWonShin,“Cross-corpusspeech Sylvain Gelly, “Parameter-efficient transfer learning for nlp,” in
emotionrecognitionbasedonfew-shotlearninganddomainadaptation,” Internationalconferenceonmachinelearning.PMLR,2019,pp.2790–
IEEESignalProcessingLetters,vol.28,pp.1190–1194,2021. 2799.
[9] HillaryAngerElfenbeinandNaliniAmbady, “Ontheuniversalityand [26] Michael Schoeffler, Sarah Bartoschek, Fabian-Robert Sto¨ter, Marlene
cultural specificity of emotion recognition: a meta-analysis.,” Psycho- Roess, Susanne Westphal, Bernd Edler, and Ju¨rgen Herre, “web-
logicalbulletin,vol.128,no.2,pp.203,2002. MUSHRA—acomprehensiveframeworkforweb-basedlisteningtests,”
[10] JeHunJeon,DucLe,RuiXia,andYangLiu, “Apreliminarystudyof 2018.
cross-lingualemotionrecognitionfromspeech:automaticclassification [27] Klaus R Scherer, “Vocal communication of emotion: A review of
versushumanperception.,” inInterspeech,2013,pp.2837–2840. researchparadigms,” Speechcommunication,vol.40,no.1-2,pp.227–
[11] AnantSinghandAkshatGupta, “Decodingemotions:Acomprehensive 256,2003.
multilingual study of speech models for speech emotion recognition,” [28] RainerBanseandKlausRScherer, “Acousticprofilesinvocalemotion
arXivpreprintarXiv:2308.08713,2023. expression.,” Journalofpersonalityandsocialpsychology,vol.70,no.
[12] MichaelNeumannetal.,“Cross-lingualandmultilingualspeechemotion 3,pp.614,1996.
recognition on English and French,” in ICASSP 2018-2018 IEEE [29] AnkitaPasad,Ju-ChiehChou,andKarenLivescu, “Layer-wiseanalysis
International Conference on Acoustics, Speech and Signal Processing of a self-supervised speech representation model,” in 2021 IEEE
(ICASSP).IEEE,2018,pp.5769–5773. Automatic Speech Recognition and Understanding Workshop (ASRU).
[13] Siddique Latif, Junaid Qadir, and Muhammad Bilal, “Unsupervised IEEE,2021,pp.914–921.
adversarialdomainadaptationforcross-lingualspeechemotionrecogni- [30] Alexandra Saliba, Yuanchao Li, Ramon Sanabria, and Catherine Lai,
tion,” in20198thinternationalconferenceonaffectivecomputingand “Layer-wise analysis of self-supervised acoustic word embeddings: A
intelligentinteraction(ACII).IEEE,2019,pp.732–737. study on speech emotion recognition,” in 2024 IEEE International
[14] Xiong Cai, Zhiyong Wu, Kuo Zhong, Bin Su, Dongyang Dai, and Conference on Acoustics, Speech, and Signal Processing Workshops
Helen Meng, “Unsupervised cross-lingual speech emotion recognition (ICASSPW).IEEE,2024.
using domain adversarial neural network,” in 2021 12th International