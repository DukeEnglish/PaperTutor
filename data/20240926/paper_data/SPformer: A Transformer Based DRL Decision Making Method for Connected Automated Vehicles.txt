SPformer: A Transformer Based DRL Decision Making Method for
Connected Automated Vehicles
Ye Han, Lijun Zhang‚àó, Dejian Meng, Xingyu Hu, Yixia Lu
Abstract‚ÄîIn mixed autonomy traffic environment, every and interaction of agent. Reinforcement learning algorithms
decision made by an autonomous-driving car may have a allow agents to gain experience through interaction with
great impact on the transportation system. Because of the
the surrounding environment and other traffic participants,
complex interaction between vehicles, it is challenging to make
and continuously improve their decision-making ability.
decisions that can ensure both high traffic efficiency and safety
now and futher. Connected automated vehicles (CAVs) have Connectedautomatedvehiclesusuallyneedtoweighbetween
great potential to improve the quality of decision-making in competition and collaboration to meet their own driving
this continuous, highly dynamic and interactive environment purposes and overall traffic efficiency requirements. By rea-
becauseoftheirstrongersensingandcommunicatingability.For
sonably setting reward functions and formulating exploration
multi-vehicle collaborative decision-making algorithms based
and exploitation schemes, DRL algorithms can help agents
on deep reinforcement learning (DRL), we need to represent
the interactions between vehicles to obtain interactive features. learn high-performance strategies.
The representation in this aspect directly affects the learning This paper introduces SPformer, a multi-vehicle collabora-
efficiency and the quality of the learned policy. To this end, tive decision-making method based on DRL and transformer
we propose a CAV decision-making architecture based on
architecture. The framework adopts transformer encoders as
transformerandreinforcementlearningalgorithms.Alearnable
part of the DRL algorithm. The input of the network is the
policytokenisusedasthelearningmediumofthemulti-vehicle
jointpolicy,thestatesofallvehiclesintheareaofinterestcanbe State sequence of all vehicles in the traffic scenario and the
adaptivelynoticedinordertoextractinteractivefeaturesamong output is the multi-vehicle joint driving Policy. The main
agents.Wealsodesignanintuitivephysicalpositionalencodings, contribution of this paper can be summarized as follows:
the redundant location information of which optimizes the
1) An effective multi-vehicle collaborative decision-making
performance of the network. Simulations show that our model
can make good use of all the state information of vehicles in framework based on deep reinforcement learning is
trafficscenario,soastoobtainhigh-qualitydrivingdecisionsthat proposed. It can effectively solve the lateral-longitudinal
meet efficiency and safety objectives. The comparison shows joint decision making of CAVs from the perspective of
that our method significantly improves existing DRL-based
mesoscopic traffic.
multi-vehicle cooperative decision-making algorithms.
2) A learnable policy token are introduced as the learning
I. INTRODUCTION medium of policy and an intuitive physical positional
Autonomus-driving vehicles are playing an increasingly encoding is designed to improve the algorithm‚Äôs per-
important role in modern transportation systems. At present formance. SPformer can well extract the interactive
and for a long time to come, autonomous and human information between agents, thereby speeding up the
driving vehicles (HDVs) will coexisit both in urban and learning process of DRL algorithm and improving the
highway traffic environment. Multi-vehicle collaborative quality of learned policy. We verified the algorithm in
decision-making will play a crucial role in mixed autonomy on-ramp tasks and compared it with the state of the art
traffics. It has advantages that single-vehicle autonomous multi-vehicle decision making algorithms. The results
driving cannot match in terms of safety, traffic efficiency, show that our methods have better performance than
driving experience, energy conservation, and environmental other deep reinforcement learning algorithms in terms
protection[1].However,duetothedynamicstateinformation of safety and efficiency.
and complex interactions of traffic participants, high-quality
collaborative driving decision-making is very challenging. II. RELATEDWORKS
Therefore, to develop a good collaborative decision-making
Multi-vehicle decision making: Multi-vehicle decision-
algorithm, we should effectively represent the interaction
making aims to provide safer and more efficient driving
between agents and make full use of it in decision making
strategiesforautonomousdrivingsystems.Earlymulti-vehicle
process.
cooperative decision-making researches can be traced back
Deepreinforcementlearningisaneffectivemethodtosolve
to the study of longitudinal platooning such as ACC and
multi-agent decision-making problems. Deep neural networks
CACC [2]. These studies use limited on-board sensors, and
helps modeling and understanding complex environments
the objective is mainly concerned with the string stability in
Ye Han, Lijun Zhang, Dejian Meng, Xingyu Hu, Yixia Lu are one dimention. Optimization-based planning methods such as
with the School of Automotive Studies, Tongji University, Shanghai mixed integer optimization and dynamic priority allocation
201804, China. {hanye leohancnjs, tjedu zhanglijun,
can also solve collaborative decision-making problems to
mengdejian, 2410254, 2051517}@tongji.edu.cn
‚àóCorrespondingauthor:LijunZhang some extent [3]‚Äì[5], but it is difficult to guarantee the speed
4202
peS
32
]IA.sc[
1v50151.9042:viXraScenario Agent State Transformer Blocks
embedding
‚Ä¶
ùë•ùë•1 ùë•ùë•2 ùë•ùë•3 ùë•ùë•ùëÅùëÅ ‚Ä¶ Policy token
‚Ä¶
Transformer block #1
Transformer block #2
Transformer block #L
Physical Positional Encoding
(PPE)
1 MLP head
0.8
‚Ä¶ 0.6 ùëùùëù1 0.4
ùëùùëù2 0.2 Policy
0
-0.2
-0.4
RL Algorithm
-0.6
-0.8
Connected Human Rewards actions
Automated Driving ùëùùëùùëÅùëÅ
Vehicel Vehicle
state transition
Fig.1. Anoverviewofourmulti-vehicledecision-makingframeworkwithSPformer.Givenamixedautonomyscenario,thevehiclestaterepresentation
containingmulti-modalinformationisusedastheinputofSPformer.Thepositionalencodingbasedonthephysicalpositionareaddedwiththeembedded
vehiclestateinformation,thenfedintothetransfomerblocktogetherwiththepolicytoken.Theoutputjointpolicycouldbeprobabilitydistributionor
Q-valuesofactions,dependingontheRLalgorithm.TheRLalgorithmselectsactionsaccordingtothejointpolicyandexecutesthemandupdatethe
networkparameterswithcollectedexperience.
and quality of the solution at the same time in large-scale cessing(NLP),recommendationsystems,timeseriesanalysis,
collaborative driving tasks. etc. all need to properly handle the interaction between
With the development of artificial intelligence, V2X com- sequencial inputs [16]‚Äì[18]. RNN and RNN-based LSTM
munication, and edge computing technologies, CAVs can are often used to construct complex sequence interaction
make more reasonable decisions in a wider spatial dimension models of time series [19]. In terms of spatial sequences,
and a longer time range [6]‚Äì[9]. The application of deep A. Alexandre et al. [20] proposed social LSTM to predict
learning in autonomous driving impels researchers to solve pedestrian trajectory, and designed a convolutional social
multi-vehicle decision-making problems with DL methods. pooling to connect the spatial close LSTMs so that infor-
A. J. M. Muzahid et al. [10] systematically summarized mation can be shared with each other, which represents the
the multi-vehicle cooperative collision avoidance technol- space interaction of agent in complex scenes. Graph neural
ogy of CAVs, and proposed a multi-vehicle cooperative networks (GNN) introduce graphs to represent the structural
perception-communication-decisionframeworkbasedondeep relationships between sequences, which are used by many
reinforcement learning. Y. Zheng et al. [11] modeled the researchers to model the interactions between vehicles in
multi-vehicle decision-making of urban multi-intersections autonomous driving studies. S. Chen et al. [21] proposed
as a predator-pray problem, and used deep reinforcement a DRL model combined with GNN to make efficient and
learning to establish a multi-agent decision-making method safe multi-vehicle cooperative lane change. D. Xu et al. [22]
where the agents show collaborative behavior patterns far established a multi-vehicle GRL algorithm to realize the
beyondhumans.TheDLbasedmulti-vehicledecisionmaking cooperative control of vehicles in highway mixed traffic, the
algorithmscaneffectivelydealwithcomplextrafficsituations, graphattentionmechanismsignificantlyimprovesthedecision
but refined modeling for collaborative interaction is needed efficiency. In GNN, however, the propagation of information
for better performance. isusuallycarriedoutthroughtheadjacencyrelationshiponthe
graph, which makes long-distance information dissemination
In addition, game theory, Monte Carlo Tree Search algo-
difficult.
rithm(MCTS), etc. are also used or combined with deep
learning methods to solve multi-vehicle decision-making Transformerisadeeplearningarchitecturewithmulti-head
problems recently [12]‚Äì[15]. These methods have shown attention mechanism [23]. It has achieved great success in
great potential in solving problems in complex multi-agent the fieldof NLP and hasbeen applied to trajectoryprediction
systems. [24] and decision making [25], [26] problems. H. Liu et
Sequencial interaction modeling: Natural language pro- al. [27] implemented two Transformer blocks for scene
‚Ä¶
‚Ä¶ etadpu
‚Ä¶encoding and vehicle latent feature extraction respectively, destination of the vehicle in the concerned area. The relative
which effectively extract the interaction feature between map states of surrounding vehicles takes into account all of the
andagent.ThefeatureisthenusedbySACalgorithmasinput other vehicle‚Äôs relative information to the current vehicle. In
togenerateautomaticdrivingpolicyindifferenturbandriving this study, we use a multi-modal coupled matrix to represent
scenarios. H. Hu et al. [28] finely designed a transformer the state of the vehicle which consists of the informations
network to integrate multi-modal information of maps and mentioned above.
agents,soastoimprovethetrajectorypredictionanddecision- The backbone of the state matrix is the rasterized road
making for autonomous vehicles. Current transformer-based area. Take the off-ramp scene shown in Fig.2 as an example.
researches on vehicle decision-making use transformer to n is the number of main road lanes and l is the
lanes main
deal with the multi-modal state sequence input of a single length. State matrix of the i-th vehicle S
i
‚ààR(nlanes+1)√ólmain,
vehicle, and in most cases the sequence is in time order. and,
There are few studies implement transformer architecture in
multi-vehicle collaborative decision-making, where the multi- S =S +S +S +S
i position,i speed,i intention,i ‚àíi
(1)
head attention mechanism can properly handle the spatial
=S +S
self,i ‚àíi
interaction between agents.
where S , S , S , S is the ego vehicle‚Äôs
III. PROBLEMSTATEMENT position,i speed,i intention,i -i
position matrix, velocity field matrix, intention matrix, and
This paper aims to solve the collaborative decision-making relative vehicle information matrix respectively. The sum of
problem of connected automated vehicles through DRL the first three is denoted as S .
self,i
algorithms. The scenario is mixed traffic where CAVs and The position matrix S = I ¬∑M , where
position,i ego position,i
HDVs coexist. It is assumed that the CAVs has the ability M is one hot matrix, where occupied by the ego
position,i
of global traffic state perception and information sharing vehicle is 1, and I is the position state factor.
ego
in the area of interest. In fact, it is not difficult to realize
A two-dimensional Gaussian potential field is used for
with the help of roadside facilities and V2X technology. We
velocity representation. In the above scenario, the speed state
model the cooperative driving problem from the perspective
matrix of the red vehicle
of mesoscopic traffic flow, considering the lane change and
logitudinal acceleration of vehicles, but do not discuss about
‚àí(cid:20)(r‚àíxi)2 +(c‚àíyi)2(cid:21)
how these behaviors are realized in terms of vehicle dynamic.
S speed,i(r,c)=I pi otentialv i¬∑e 2œÉx2 2œÉy2 (2)
This work focuses on the development and verification of
where r and c are the row and column index of the velocity
interactive collaborative decision-making algorithms between
state matrix, Ii is the speed state factor, œÉ , œÉ are
vehicles, and currently does not consider factors such as potential x y
the longitudinal and lateral speed state decay factors of the
communication delay and sensing information uncertainty.
vehicle respectively.
A single row matrix S is added to represent the
intention,i
ùëôùëôùëöùëöùëöùëöùëöùëöùëöùëö location of the target ramp. S intention,i is initialized as an all-0
matrix and make xi (r,c) = I , if (x ‚àíint ) <
int intention int range
1 r <x and c=3.whereI istheintentionstatefactor,
int intention
int is the range of the vehicle‚Äôs intention area.
2 Intention area range
Forvehiclei,S istheweighedsumofthestatematrices
3 ‚àíi
CAV ùëñùëñùëôùëôùëñùëñùëüùëüùëöùëöùëöùëöùëüùëüHùëüùëüDV ùë•ùë•ùëöùëöùëöùëöùëñùëñ of all vehicles except itself, i.e.,
(cid:88)
S =w S (3)
‚àíi ‚àí self,j
jÃ∏=i
Fig.2. Skatchofoff-rampscene.Themainroadisrasterizedbythelane
lines and straight lines equidistant along the center line of the road. For So far, the final expression of S is obtained. Fig.3 shows
i
vehiclesintendingtoentertheramp,theintentionareaismarkedwithgreen.
an example state representation for a single vehicle.
IV. APPROACH
A. Deep reinforcement learning problem construction
State representation: The vehicle‚Äôs state consists of
its individual dynamic characteristics, traffic environment,
driving intention, and the relative states of surrounding
vehicles. Specifically, vehicle‚Äô individual dynamic charac-
teristics include the current lateral and longitudinal position,
speed, and acceleration, The traffic environment includes
Fig.3. Anexamplestaterepresentationforasinglevehicle.Theupper
road information, key road element characteristics (locations
halfisthedistributionofvehiclesontheroadsection,andthelowerhalfis
of intersections, ramps, etc.), The driving intention is the thestateheatmapoftheredvehicle.
ùëôùëôùëôùëôùëôùëôùëôùëôùëôùëôActionspace:Weconsiderboththelateralandlongitudinal and multi-layer perceptron(MLP) layer alternately. Layer-
behaviors of the vehicle. Longitudinal actions include accel- norm(LN)isaddedbeforeeachblock,andresidualconnection
erating, speed keeping and decelerating, and lateral actions is performed after each MHA and MLP. The architecture is
consist of left lane changing, lane keeping and right lane shown in the right side of Fig.1 and can be summarized as
changing. Considering that longitudinal and lateral actions follows:
canbeperformedatthesametime,thereare9elementsinthe z =(cid:2) x ;x1E;x2E;¬∑¬∑¬∑ ;xNE(cid:3) +E
0 policy v v v pos
joint action space. A={(a ,a )|a ‚ààA ,a ‚ààA },
lon lat lon lon lat lat z‚Ä≤ =MHA(LN(z ))+z ,‚Ñì=1...L
where A lon ={AC,SK,DC}, and A lat ={LC,LK,RC}. ‚Ñì ‚Ñì‚àí1 ‚Ñì‚àí1 (5)
Reward function: Our work aims at the driving efficiency z ‚Ñì =MLP(LN(z ‚Ñì‚Ä≤))+z ‚Ñì‚Ä≤,‚Ñì=1...L
and safety of the global traffic. The reward function is y
=LN(cid:0) z0(cid:1)
L
designed as shown in Equation 4, the implementation details
where E ‚ààR(H¬∑W)√óD,E ‚ààR(N+1)√óD.
can be found in paper [29]. pos
We use the standard qkv self-attention to caculate the
R=w 1R speed +w 2R intention +w 3P collision +w 4P LC attention matrix. For the input sequence z ‚ààRN√óD,
N
= 1 (w (cid:88) v i +w N +w N +w N ) [q,k,v]=z¬∑U qkv
N 1
i=1
v max 2 onramp 3 collision 4 LC Att(z)=softmax(cid:16) q¬∑k‚ä§/(cid:112)
D
(cid:17)
¬∑v
(6)
(4) head
where N is the number of vehicles in the scene (including
HDVs and CAVs), N is the vehicle passing through
where U
qkv
‚ààRD√ó3Dhead, and Att(z)‚ààRN√óN.
onramp The multi-head self-attention is the expansion of self-
intention area at the previous time step and aiming for the
attention, and k self-attention matrices are calculated simul-
ramp, N is the number of collisions, and N is the
collision LC taneously. The results are concatenated into a multi-head
number of frequently lane-changing vehicles.
attention matrix.
There are two main differences between our work and the
reference work in the calculation of rewards : MHA(z)=[Att (z);Att (z);¬∑¬∑¬∑ ;Att (z)]W (7)
1 2 k O
1) For speed reward, we take into account all vehicles‚Äô where W
O
‚ààRk¬∑Dhead√óD.
speed while most previous work only considered the
It can be seen that the policy token and the embedded
average speed of CAVs. It has been proved that the be-
states are fed into the transformer block together, and the
havior of autonomous vehicles can affect other vehicles
finaloutputresultisusedtoderivethepolicy.Thetransformer
in the traffic environment.
encoder processed a total of N +1 tokens of dimension D,
2) To encourage vehicles to explore more diverse driving
and only the output of the policy token is used to derive the
strategies, we only set intention rewards in a small area
policy. This architecture forces the exchange of information
close to the ramp as shown in Fig.2, and no punishment
between vehicles‚Äô state feature and policy token.
related to intention is set when the vehicles are in other
Physical positional encoding: In NLP studies, word
area.
order is of great importantance. Vaswani et al. give the
classic position encoding method of sine-cosine alternation.
B. Interactive feature extraction method based on Trans-
Researchers have improved the PE method according to
former
different tasks, and the performance of transformer has been
In this paper, the transformer encoder is used to extract the significantly improved [33]‚Äì[36]. For driving tasks, location
interaction features of vehicles. We introduce policy-token information is of natural importance. Although location
as the learning medium of multi-agent joint strategy. The feature is contained in the vehicle states input, it will be
multi-head self-attention mechanism of the transformer helps diluted by other information such as speed, intention, and
to extract the interaction information between vehicles. In ad- maps. To our knowledge, after steps of feature extraction, the
dition,weintegratedphysicalpositionencodingintothebasic superposition of physical location encoding can strengthen
transformer, which makes the network more sensitive to the location features and improve the network performance.
vehicles‚Äô location and effectively improves the performance In this paper, we simply refer to the original transformer,
of the algorithm. and generate PPE in the form of sines and cosines combi-
Transformer encoder with policy-token: Inspired by the nation. The map in the area of interest are discretized into
research in NLP and CV [30]‚Äì[32], we introduce a learnable N physical positions. For the physical position pos ‚àí1,
pos ph
policy token as the policy learning medium. Policy token the PPE is calculated according to Equation 8.
enables the network to have perception of global traffic state,
it has the same dimension as the vehicle‚Äôs state feature. x ‚àà (cid:16) (cid:17)
RH√óW istheinputstatematrixofeachvehicle,itisreshav ped PPE 2k(¬∑, pos ph)=sin pos ph/(2N pos))2k/D
(8)
to 1√óHW and then embedded to 1√óD. x
policy
‚ààR1√óD
PPE (¬∑, pos
)=cos(cid:16)
pos /(2N
))2k/D(cid:17)
2k+1 ph ph pos
is the policy token.
We design the transformer encoder based on ViT [31]. where 2k and 2k+1 are index of PE vector, D is the model
The encoder consists of multi-head attention layer (MHA) dimention.An example of PPE is shown in Fig.4. Compared with the main road until the simulation ends. At the beginning
graph neural networks, PPE enhances the absolute position of the simulation, all vehicles are generated at the initial
informationoftheagentintheenvironment,whichwebelieve position with the initial speed. Episode ends when the first
can achieve better performance in scene-centric tasks. vehicle in the environment reached the end of main road.
Specified parameters of the agent are shown in Table I. The
D
initial position and lane of CAVs are marked by *.
1
0.8
B. RL agent implementation details
ùëùùëù1 0.6
ùëùùëù2 0.4 Weusetheclassicaldeepreinforcementlearningalgorithm
0.2 DQN to verify the performance of the proposed method.
0 DQN is a value-based reinforcement learning algorithm.
-0.2
The Q-Learing algorithm maintains a Q-table, and uses the
-0.4
table to store the return obtained by taking action a under
-0.6
each state s, that is, the state-value function Q(s,a). But in
-0.8
many cases, the state space faced by reinforcement learning
ùëùùëù10
tasks is continuous, and there are infinite states. In this case,
the value function can no longer be stored in the form of
tables.Tosolvethisproblem,wecanuseafunctionQ(s,a;Œ∏Œ∏Œ∏)
Fig.4. ExampleofPPE.Asectionoftheone-waythree-laneroadisrejoint
to approximate the action-value Q(s,a), which is called
bylanesfromlefttoright.ThePPEmapshownintherightiscaculated
according to Equation 8, the red lines mark the position encoding of all Value Function Approximation. We use neural networks to
vehiclesatthistime. generatethisfunctionQ(s,a;Œ∏Œ∏Œ∏),calledDeepQ-network,Œ∏Œ∏Œ∏ is
a parameter for neural network training. DQN introduces the
V. EXPERIMENT neural network in deep learning, and uses the neural network
to fit the Q table in Q-learning, which solves the problem of
We carried out the verification of the algorithm on the
dimension disaster.
Flow platform [37]. Using DQN as the basic reinforcement
For single-agent DQN, we update the neural network
learningalgorithm,theperformanceofdifferentdeeplearning
weights Œ∏Œ∏Œ∏ by minimizing the loss function:
networks is compared.
TABLEI (cid:16) (cid:17)2
L(s,a|Œ∏Œ∏Œ∏)= r+Œ≥maxQ(s‚Ä≤,a‚Ä≤ |Œ∏Œ∏Œ∏)‚àíQ(s,a|Œ∏Œ∏Œ∏) .
AGENTPARAMETERSSETTINGSFORTHEEXPERIMENTS
a
(9)
Parameters Value In our work, we use a single neural network to simultane-
NumberofHDVs 4
ously predict the Q values of multiple agents. The MADQN
NumberofCAVs 2
HDVdeparturespeed 10m/s architecture is discussed in [39]. Since the reward function
CAVdeparturespeed 10m/s is designed to be the mean value of the current state values
Acceleration 3.5m/s2
of all agents, the Q value in this condition should be the
MaxHDVspeed 20m/s
MaxCAVspeed 20m/s discounted sum of the state values of all agents. To this end,
Initialposition [20,30,50,50,30‚àó,0‚àó] we design the following loss function:
Initiallane [1,0,0,2,2‚àó,1‚àó]
Simulationstep 1s
(cid:16)
L(s,a|Œ∏Œ∏Œ∏)= r+Œ≥ 1 (cid:80)NCAV max Q (s‚Ä≤,a‚Ä≤ |Œ∏Œ∏Œ∏)
NCAV i=1 ai i i
(cid:17)2
A. Simulation environment and experiment settings ‚àí 1 (cid:80)NCAV Q (s,a |Œ∏Œ∏Œ∏) .
NCAV i=1 i i
We use Flow to build simulation scenarios and verify (10)
the algorithm. Flow is a computational framework for deep SPformer is applied to reinforcement learning agents. The
RL and control experiments for traffic microsimulation. It overall structure of the network is described by Formula 5,
provides a set of basic traffic control scenarios and tools where each MLP contains two fully connected layers with
for designing custom traffic scenarios. In the simulation, the Gaussian error linear unit(GELU) between, and the input
built-in EIDM model of the framework is implemented as is state vector of size 6√ó1000. The specific parameters of
HDVs [38]. In order to maximize the ability of the algorithm, SPformer are shown in Table II. The implementation details
all active safety detection of the vehicle controlled by the of DQN are shown in Table.III.
reinforcement learning algorithm are removed during the
C. Compared Methods
training process.
The simulation scenario is the on-ramp scenario shown We compare the performance of convolutional neural
in Fig.2. Considering a one-way three-lane main road with network, graph neural network and SPformer in DQN
length of 250m, the exit ramp is 200m away from the start. algorithm. The convolutional neural network has a kernel
The agents in the case include 2 CAVs and 4 HDVs. 2 CAVs with size of 4√ó4, followed by a two-layer fully connected
aretragetedtoentertheramp,andHDVsaresettodrivealong network, and rectified linear unit(ReLU) is added after each
noitisop
elciheV
‚Ä¶layer.Theimplementationdetailsofthegraphneuralnetwork Average velocity (Velo.) : The mean value of average
are shown in paper [40]. velocity(in m/s) of all vehicles per episode.
E. Results and Comparasion
TABLEII
SPFORMERPARAMETERS Duringthetrainingprocess,thecurvesoftheaveragetraffic
state value and the number of collisions are shown in Fig.5
Variable Parameters Value
and Fig.6. We use the rule-based approach as a baseline for
E Inputdimension 6√ó1000
D Modeldimension 192 comparison, which can represent the general level of human
L TransformerblockLayers 2 drivers.
k Numberofheads 6 In the early stage of training, because the agent has a high
D head Dimensionofhead 32 exploration rate and does not have safety-related experience,
- Dropoutrate 0.1
- Outputdimention 1√ó18 the number of collisions is large. This leads to the overall
performance of the DRL algorithm agent worse than the
rule-based method.
TABLEIII
25
DQNPARAMETERS
20
Parameters Value
Trainingepisodes 5000
Discountfactor 1 15
Initialexplorationrate 1
Minimumexplorationrate 0.01 10
Explorationdecayrate 0.996
Learningrate 0.001
5
Batchsize 16
Replaybuffercapacity 4000
Intentionarearange 5 0
Iego 30
CNN
I potential 1 -5 GNN
œÉx,œÉy 5,0.7
SPformer
w‚àí 0.5
-10 baseline
w1,w2,w3,w4 20,6,-0.05,-80
Simulationstep 1s
-15
0 1000 2000 3000 4000
All the methods in the comparison experiment (except
episode
EIDM) performed 5000 episodes of training in the on-
ramp scenario, and each experiment is conducted 6 times
with different random seeds(Two random seeds are assigned
Fig. 5. Average traffic score in the training process. For each method,
to 1).the random action selection of RL algorithm in the weconducted6trainings.ThecurveshowninthefigureisthemeanATS
explorationprocess,2).therandomparametersofSUMObuilt- valueandtheshadowedareashowstheupperandlowerboundof6training
results.
in car-following and lane-changing controller.). The neural
network is trained on a single NVIDIA RTX 3090 GPU
After 2500 episodes of training, the agent has learned a
using PyTorch and Adam optimizer. The training process of
stable strategy. It can be seen in Fig.5 that under the same
a method in a single scenario takes about 3 hours.
exploration strategy, the learning efficiency of SPformer is
D. Evaluation Metric significantly higher than that of CNN and GNN. After 1500
Average traffic score (ATS.) : The superiority of coopera- episodes of training, SPformer has already learned a stable
tive driving is reflected in the efficiency and safety of global driving strategy. This is mainly caused by the additional
traffic flow, although such a strategy is not always optimal location information of PPE. It can be seen from the Fig.7
for a single vehicle. Therefore, we use Average Traffic Score that after removing PPE, the learning speed of SPformer and
(ATS) to evaluate the quality of traffic flow. It is calculated the final stable ATS are almost the same as GNN. It should
as: be noted that we have optimized the GNN network to our
best with reference to paper [40]. The CNN network has
1 T (cid:88)‚àí1 also been designed to achieve its best performance in this
ATS = R . (11)
T t experiment.
t=0 Table IV shows that SPformer achieves a good balance
where T is the simulation steps of an episode. between task completion rate, safety and driving speed.
Success rate (Succ.%) : Percentage of vehicles success- Although it does not have an advantage in average speed, it
fully entering the ramp in all test cases. can lead other algorithms significantly on the comprehensive
Collision number (Coll.) : The average number of index ATS. This shows that SPformer can fully take into
collisions per episode in the test case, which indicates the account all agents in the scene and maximize group interests.
safety of the strategy.
erocs
ciffart
egareva15 vehicle driving strategies and integrates an intuitive physical
CNN
10 positional encoding. Policy token can prompt the network to
5 obtain a global perception of the traffic state, and physical
0 positionalencodingenhancesthevehiclelocationinformation
0 1000 2000 3000 4000 5000
that is crucial to the quality of decision-making. Therefore,
15 SPformer can effectively improve multi-vehicle cooperative
GNN
10 driving strategy learned by DRL algorithms. We tested the
5 performance of SPformer in the on-ramp scenario. Compared
with CNN and GNN networks, SPformer have obvious
0
0 1000 2000 3000 4000 5000
advantages in strategy learning speed and quality.
15 The future work will focus on improving the performance
SPformer
10 of cooperative driving algorithms in large-scale scenarios.
5 Although the current algorithm has good interactive decision-
making performance, it is difficult to achieve excellent per-
0
0 1000 2000 3000 4000 5000
formance in cases with large number of vehicles and random
training episode traffic flow, where more diverse cooperative behavior patterns
canbefound.Inaddition,physicalpositionalembeddingwith
higher dimension, new architectures combined with game
Fig.6. Numberofcollisioninthetrainingprocess theory and MCTS, and more efficient collaborative state
representation methods are to be verified in more complex
25 CAV decision making problems.
20
REFERENCES
15
[1] A. Talebpour and H. S. Mahmassani, ‚ÄúInfluence of connected and
10
autonomousvehiclesontrafficflowstabilityandthroughput,‚ÄùTrans-
portationResearchPartC:EmergingTechnologies,vol.71,pp.143‚Äì
5 163,2016.
[2] K. C. Dey, L. Yan, X. Wang, Y. Wang, H. Shen, M. Chowdhury,
0 L. Yu, C. Qiu, and V. Soundararaj, ‚ÄúA review of communication,
driver characteristics, and controls aspects of cooperative adaptive
-5 SPformer-nonPPE cruisecontrol(cacc),‚ÄùIEEETransactionsonIntelligentTransportation
GNN Systems,vol.17,no.2,pp.491‚Äì509,2015.
SPformer [3] F.FabianiandS.Grammatico,‚ÄúMulti-vehicleautomateddrivingas
-10 a generalized mixed-integer potential game,‚Äù IEEE Transactions on
IntelligentTransportationSystems,vol.21,no.3,pp.1064‚Äì1073,2020.
-15 [4] S.Liu,D.Sun,andC.Zhu,‚ÄúAdynamicprioritybasedpathplanningfor
0 1000 2000 3000 4000 cooperationofmultiplemobilerobotsinformationforming,‚ÄùRobotics
episode andComputer-IntegratedManufacturing,vol.30,no.6,pp.589‚Äì596,
2014.
[5] Y.Ouyang,B.Li,Y.Zhang,T.Acarman,Y.Guo,andT.Zhang,‚ÄúFast
andoptimaltrajectoryplanningformultiplevehiclesinanonconvexand
Fig.7. Averagetrafficscoreinthetrainingprocess.Thisfigshowsthe clutteredenvironment:Benchmarks,methodology,andexperiments,‚Äù
importanceofPPEinSPformer. in2022InternationalConferenceonRoboticsandAutomation(ICRA),
2022,pp.10746‚Äì10752.
[6] K.C.Dey,A.Rayamajhi,M.Chowdhury,P.Bhavsar,andJ.Martin,
TABLEIV
‚ÄúVehicle-to-vehicle(v2v)andvehicle-to-infrastructure(v2i)communi-
METRICSCOMPARATIONOF1000TESTINGEPISODES cationinaheterogeneouswirelessnetwork‚Äìperformanceevaluation,‚Äù
TransportationResearchPartC:EmergingTechnologies,vol.68,pp.
Method ATS. Succ.% Coll. Velo. 168‚Äì184,2016.
EIDM 12.769 100 0 11.200 [7] M.ElZorkany,A.Yasser,andA.I.Galal,‚ÄúVehicletovehicle‚Äúv2v‚Äù
CNN-DQN 9.134 62.8 2.979 111444...777666444 communication:scope,importance,challenges,researchdirectionsand
GNN-DQN 13.265 62.8 1.548 13.958 future,‚ÄùTheOpenTransportationJournal,vol.14,no.1,2020.
SPformer-nonPPE 14.203 72.6 1.407 13.931 [8] H. Ye, G. Y. Li, and B.-H. F. Juang, ‚ÄúDeep reinforcement learning
SPformer 111888...111444222 999777...444 000...222444222 13.757 basedresourceallocationforv2vcommunications,‚ÄùIEEETransactions
onVehicularTechnology,vol.68,no.4,pp.3163‚Äì3173,2019.
[9] A. R. Khan, M. F. Jamlos, N. Osman, M. I. Ishak, F. Dzaharudin,
Y.K.Yeow,andK.A.Khairi,‚ÄúDsrctechnologyinvehicle-to-vehicle
VI. CONCLUSIONANDFUTUREWORK
(v2v) and vehicle-to-infrastructure (v2i) iot system for intelligent
transportationsystem(its):Areview,‚ÄùRecentTrendsinMechatronics
In summary, this research proposed SPformer, a DRL-
TowardsIndustry4.0:SelectedArticlesfromiM3F2020,Malaysia,pp.
based multi-vehicle collaborative decision-making method, 97‚Äì106,2022.
which provides an effective solution to multi-vehicle collabo- [10] A.J.M.Muzahid,S.F.Kamarulzaman,M.A.Rahman,S.A.Murad,
M.A.S.Kamal,andA.H.Alenezi,‚ÄúMultiplevehiclecooperationand
rative lateral and longitudinal joint decision-making problem.
collisionavoidanceinautomatedvehicles:Surveyandanai-enabled
SPformer uses policy-token as a learning medium for multi- conceptualframework,‚ÄùScientificreports,vol.13,no.1,p.603,2023.
snoisilloc
latot
erocs
ciffart
egareva[11] Z.Yuan,T.Wu,Q.Wang,Y.Yang,L.Li,andL.Zhang,‚ÄúT3omvp:A [29] J.Dong,S.Chen,P.Y.J.Ha,Y.Li,andS.Labi,‚ÄúAdrl-basedmultiagent
transformer-basedtimeandteamreinforcementlearningschemefor cooperativecontrolframeworkforcavnetworks:Agraphicconvolution
observation-constrainedmulti-vehiclepursuitinurbanarea,‚ÄùElectron- qnetwork,‚ÄùarXivpreprintarXiv:2010.05437,2020.
ics,vol.11,no.9,2022. [30] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova,‚ÄúBert:Pre-training
[12] P.Huang,H.Ding,Z.Sun,andH.Chen,‚ÄúAgame-basedhierarchical ofdeepbidirectionaltransformersforlanguageunderstanding,‚Äù2019.
model for mandatory lane change of autonomous vehicles,‚Äù IEEE [31] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,
TransactionsonIntelligentTransportationSystems,pp.1‚Äì13,2024. T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gelly,etal.,
[13] P.Hang,C.Huang,Z.Hu,andC.Lv,‚ÄúDecisionmakingforconnected ‚ÄúAnimageisworth16x16words:Transformersforimagerecognition
automated vehicles at urban intersections considering social and atscale,‚ÄùarXivpreprintarXiv:2010.11929,2020.
individualbenefits,‚ÄùIEEETransactionsonIntelligentTransportation [32] B.Liao,S.Chen,Y.Zhang,B.Jiang,Q.Zhang,W.Liu,C.Huang,and
Systems,vol.23,no.11,pp.22549‚Äì22562,2022. X.Wang,‚ÄúMaptrv2:Anend-to-endframeworkforonlinevectorized
[14] Z.Huang,H.Liu,andC.Lv,‚ÄúGameformer:Game-theoreticmodeling hdmapconstruction,‚ÄùarXivpreprintarXiv:2308.05736,2023.
andlearningoftransformer-basedinteractivepredictionandplanning [33] H.Peng,G.Li,Y.Zhao,andZ.Jin,‚ÄúRethinkingpositionalencoding
forautonomousdriving,‚ÄùinProceedingsoftheIEEE/CVFInternational intreetransformerforcoderepresentation,‚ÄùinProceedingsofthe2022
ConferenceonComputerVision,2023,pp.3903‚Äì3913. ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
[15] L. Wen, P. Cai, D. Fu, S. Mao, and Y. Li, ‚ÄúBringing diversity to Y.Goldberg,Z.Kozareva,andY.Zhang,Eds. AbuDhabi,United
autonomousvehicles:Aninterpretablemulti-vehicledecision-making ArabEmirates:AssociationforComputationalLinguistics,Dec.2022,
andplanningframework,‚ÄùarXivpreprintarXiv:2302.06803,2023. pp.3204‚Äì3214.
[16] G.Zhou,X.Zhu,C.Song,Y.Fan,H.Zhu,X.Ma,Y.Yan,J.Jin,H.Li, [34] C.Shu,J.Deng,F.Yu,andY.Liu,‚Äú3dppe:3dpointpositionalencoding
andK.Gai,‚ÄúDeepinterestnetworkforclick-throughrateprediction,‚Äù for multi-camera 3d object detection transformers,‚Äù arXiv preprint
inProceedingsofthe24thACMSIGKDDinternationalconferenceon arXiv:2211.14710,2022.
knowledgediscovery&datamining,2018,pp.1059‚Äì1068. [35] X. Chu, Z. Tian, B. Zhang, X. Wang, and C. Shen, ‚ÄúCondi-
[17] R. Patil, S. Boit, V. Gudivada, and J. Nandigam, ‚ÄúA survey of text tional positional encodings for vision transformers,‚Äù arXiv preprint
representationandembeddingtechniquesinnlp,‚ÄùIEEEAccess,vol.11, arXiv:2102.10882,2021.
pp.36120‚Äì36146,2023. [36] B. Wang, D. Zhao, C. Lioma, Q. Li, P. Zhang, and J. G. Simon-
[18] Z. Mariet and V. Kuznetsov, ‚ÄúFoundations of sequence-to-sequence sen,‚ÄúEncodingwordorderincomplexembeddings,‚ÄùarXivpreprint
modeling for time series,‚Äù in The 22nd international conference on arXiv:1912.12333,2019.
artificialintelligenceandstatistics. PMLR,2019,pp.408‚Äì417. [37] C.Wu,K.Parvate,N.Kheterpal,L.Dickstein,A.Mehta,E.Vinitsky,
[19] A. Sherstinsky, ‚ÄúFundamentals of recurrent neural network (rnn) and A. M. Bayen, ‚ÄúFramework for control and deep reinforcement
andlongshort-termmemory(lstm)network,‚ÄùPhysicaD:Nonlinear learningintraffic,‚Äùin2017IEEE20thInternationalConferenceon
Phenomena,vol.404,p.132306,2020. IntelligentTransportationSystems(ITSC). IEEE,2017,pp.1‚Äì8.
[38] D.Salles,S.Kaufmann,andH.-C.Reuss,‚ÄúExtendingtheintelligent
[20] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and
drivermodelinsumoandverifyingthedriveofftrajectorieswithaerial
S. Savarese, ‚ÄúSocial lstm: Human trajectory prediction in crowded
spaces,‚ÄùinProceedingsoftheIEEEconferenceoncomputervision
measurements,‚ÄùSUMOConferenceProceedings,vol.1,p.1‚Äì25,Jul.
andpatternrecognition,2016,pp.961‚Äì971. 2022.
[39] M. Egorov, ‚ÄúMulti-agent deep reinforcement learning,‚Äù CS231n:
[21] S.Chen,J.Dong,P.Y.J.Ha,Y.Li,andS.Labi,‚ÄúGraphneuralnetwork
convolutionalneuralnetworksforvisualrecognition,pp.1‚Äì8,2016.
andreinforcementlearningformulti-agentcooperativecontrolofcon-
[40] Q.Liu,Z.Li,X.Li,J.Wu,andS.Yuan,‚ÄúGraphconvolution-baseddeep
nectedautonomousvehicles,‚ÄùComputer-AidedCivilandInfrastructure
reinforcementlearningformulti-agentdecision-makingininteractive
Engineering,vol.36,no.7,pp.838‚Äì857,2021.
traffic scenarios,‚Äù in 2022 IEEE 25th International Conference on
[22] D. Xu, P. Liu, H. Li, H. Guo, Z. Xie, and Q. Xuan, ‚ÄúMulti-view
IntelligentTransportationSystems(ITSC),2022,pp.4074‚Äì4081.
graphconvolutionnetworkreinforcementlearningforcavscooperative
controlinhighwaymixedtraffic,‚ÄùIEEETransactionsonIntelligent
Vehicles,vol.9,no.1,pp.2588‚Äì2599,2024.
[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.
Gomez,L.u.Kaiser,andI.Polosukhin,‚ÄúAttentionisallyouneed,‚Äùin
AdvancesinNeuralInformationProcessingSystems,I.Guyon,U.V.
Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and
R.Garnett,Eds.,vol.30. CurranAssociates,Inc.,2017.
[24] Y. Yuan, X. Weng, Y. Ou, and K. M. Kitani, ‚ÄúAgentformer: Agent-
aware transformers for socio-temporal multi-agent forecasting,‚Äù in
ProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision(ICCV),October2021,pp.9813‚Äì9823.
[25] Y.Chebotar,Q.Vuong,K.Hausman,F.Xia,Y.Lu,A.Irpan,A.Kumar,
T.Yu,A.Herzog,K.Pertsch,K.Gopalakrishnan,J.Ibarz,O.Nachum,
S.A.Sontakke,G.Salazar,H.T.Tran,J.Peralta,C.Tan,D.Manjunath,
J.Singh,B.Zitkovich,T.Jackson,K.Rao,C.Finn,andS.Levine,‚ÄúQ-
transformer:Scalableofflinereinforcementlearningviaautoregressive
q-functions,‚ÄùinProceedingsofThe7thConferenceonRobotLearning,
ser.ProceedingsofMachineLearningResearch,J.Tan,M.Toussaint,
andK.Darvish,Eds.,vol.229. PMLR,06‚Äì09Nov2023,pp.3909‚Äì
3928.
[26] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin,
P. Abbeel, A. Srinivas, and I. Mordatch, ‚ÄúDecision transformer:
Reinforcement learning via sequence modeling,‚Äù in Advances in
NeuralInformationProcessingSystems,M.Ranzato,A.Beygelzimer,
Y. Dauphin, P. Liang, and J. W. Vaughan, Eds., vol. 34. Curran
Associates,Inc.,2021,pp.15084‚Äì15097.
[27] H. Liu, Z. Huang, X. Mo, and C. Lv, ‚ÄúAugmenting reinforcement
learning with transformer-based scene representation learning for
decision-making of autonomous driving,‚Äù IEEE Transactions on
IntelligentVehicles,pp.1‚Äì17,2024.
[28] H.Hu,Q.Wang,Z.Zhang,Z.Li,andZ.Gao,‚ÄúHolistictransformer:
Ajointneuralnetworkfortrajectorypredictionanddecision-making
of autonomous vehicles,‚Äù Pattern Recognition, vol. 141, p. 109592,
2023.