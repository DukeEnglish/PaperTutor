Attention Prompting on Image for Large
Vision-Language Models
Runpeng Yu , Weihao Yu† , and Xinchao Wang†
National University of Singapore
{r.yu,weihaoyu}@u.nus.edu, xinchao@nus.edu.sg
Websitehttps://yu-rp.github.io/api-prompting/
Code https://github.com/yu-rp/apiprompting
Space https://huggingface.co/spaces/rp-yu/apiprompting
Question: Which direction is the red bird facing?
Auxiliary LVLM
Question CLIP
×
Original Image Attention Heatmap Attention Prompting Image
Input original image
The red bird is facing away from the camera, as it is
perched on a tree branch with other birds. This could be due
LLaVA to the bird's natural behavior. ×
The red bird in the image is facing to the right. The bird's
Question
eyes and beak are directed toward the right side of the
GPT-4V image, indicating its attention and focus are in that direction.×
Input attention prompting image
The red bird in the image is facing to the left.
√
LLaVA
The red bird in the image is facing to the left. This can be
Question
determined by observing the direction in which its beak and
GPT-4V eyes are pointing. √
Fig.1: Comparison of the proposed Attention Prompting on Image (API) with the
naiveVQA.API provideshintsforLVLMbysimplyoverlyingaheatmapontheimage.
† Corresponding author.
4202
peS
52
]VC.sc[
1v34171.9042:viXra2 Yu et al.
Abstract. ComparedwithLargeLanguageModels(LLMs),LargeVision-
Language Models (LVLMs) can also accept images as input, thus show-
casingmoreinterestingemergentcapabilitiesanddemonstratingimpres-
sive performance on various vision-language tasks. Motivated by text
prompting in LLMs, visual prompting has been explored to enhance
LVLMs’capabilitiesofperceivingvisualinformation.However,previous
visualpromptingtechniquessolelyprocessvisualinputswithoutconsid-
eringtextqueries,limitingthemodels’abilitytofollowtextinstructions
tocompletetasks.Tofillthisgap,inthiswork,weproposeanewprompt-
ing technique named Attention Prompting on Image (API), which just
simply overlays a text-query-guided attention heatmap on the original
inputimageandeffectivelyenhancesLVLMonvarioustasks.Specifically,
wegenerateanattentionheatmapfortheinputimagedependentonthe
textquerywithanauxiliarymodellikeCLIP.Thentheheatmapsimply
multipliesthepixelvaluesoftheoriginalimagetoobtaintheactualinput
image for the LVLM. Extensive experiments on various vison-language
benchmarksverifytheeffectivenessofourtechnique.Forexample,API
improves LLaVA-1.5 by 3.8% and 2.9% on MM-Vet and LLaVA-Wild
benchmarks, respectively.
Keywords: Visual Prompting · Large Vision-Language Model · Large
Multimodal Model
1 Introduction
Benefiting from the great progress of Large Language Models (LLMs) [1,53,54],
LargeVision-LanguageModels(LVLMs)[2,4,9,18,26,32,66,67,81]alsoadvances
rapidly, represented by the seminal works GPT-4V [66] and LLaVA [32].1 They
have been widely applied in tasks that involve understanding both visual and
linguistic information, such as referring segmentation [72,73], localization [72],
captioning[55],openworld2D/3Dunderstanding[52,57,66,82],andimageedit-
ing [63,66].
To enhance the performance of LVLMs, an economical method is to develop
prompting techniques to elicit the models’ potential. Similar to textual prompt-
ing [24,61], visual prompting2 [64,65] is a technique that enhances a model’s
understanding of images by directly adding annotations such as masks, circles,
and marks to the image. This technique provides clear hints for visual percep-
tion by highlighting areas relevant to solving the problem, guiding the model’s
attentiontospecificpartsoftheimage,thusmitigatingissuesarisingfromcom-
plex scenes with distractions. It has been demonstrated that even simple visual
cues like circles [48], arrows [66], or image tiling [30] can improve LVLMs’ abil-
ity to extract the required information correctly. Unlike methods that improve
LVLM performance through adaptation or fine-tuning, visual prompting does
not require the training process, thereby reducing the risks of overfitting and
1 Although also referred to as Multimodal Large Language Model (MLLM) or Large
Multimodal Model (LMM) [32,66], we use Large Vision-Language Model (LVLM)
to refer to the models discussed in this paper, as we primarily utilizes the model’s
vision and language capabilities.
2 Inthiswork,wespecificallyuse“visualprompts”torefertomasks,circles,marks,and
otherannotationsaddedtoimagesanduse“visualprompting”torefertotechnologies
that employ visual prompts to assist in VQA tasks.Attention Prompting on Image for Large Vision-Language Models 3
knowledge forgetting. Moreover, compared to textual prompts, visual prompt-
ing is more direct and precise in guiding the model’s focus to specific areas.
Textual descriptions cannot succinctly describe an irregular area in an image or
accuratelyindicatethelocationofaspecificobject,andtheyalsofaceissueswith
aligning textual coordinates with actual image pixels [66]. However, compared
to research on textual prompts and LVLM fine-tuning, visual prompting is still
underexplored.
Previous visual prompting techniques focused on designing appropriate fine-
grained annotations to the image, aiming to highlight important local areas
without impairing the model’s overall understanding of the image. Remarkably,
FGVP[65]andSoM[64]arebothbasedonsegmentationmasks[64]:Theformer
blurs the image outside the segmentation mask while the latter overlays the
image with a set of including alphanumerics, masks, and boxes. However, all
thesemethodssorelyprocesstheinputimageswithoutconsideringthetextquery
content.Inotherwords,whateverthetextqueryis,animage’svisualprompting
results are the same. This can easily lead to a mismatch between the prompted
image and the text query, as different text queries for the same image require
focus on different areas and necessitate different annotations. This mismatch
may thereby limit the model’s ability to follow instructions accurately.
To address this issue, in this paper, we propose a novel prompting technique
namedAttentionPromptingonImage(API),whichjustsimplyoverlaysatext-
query-guidedattentionheatmapontheoriginalinputimage.Specifically,togen-
eratetext-guidedattentionheatmapforanimage,weutilizeanauxiliaryLVLM
that can accept both image and text as input. For image-text matching type
(like CLIP [40]) as auxiliary model, we devised a heatmap generation technique
basedonthedecompositionofclstokensimilarityscore.Forthevision-language-
input text generation model (like LLaVA [32]), we generate the heatmap based
on attention weights. Extensive experiments on various commonly used vision-
language (VL) datasets verify the effectiveness of API in enhancing the VLM’s
perception of visual information. For example, API improves LLaVA-1.5 by
3.8%, 2.9%, and 2.3% on MM-Vet, LLaVA-Bench and MMMU benchmarks
Our contributions can be summarized as follows:
1. Wefindthatcurrentvisualpromptingtechniquessorelymodifyinputimages
without considering the text query, limiting the model’s capability to follow
instructions accurately.
2. To fill the gap, we propose the API method, exploring how to derive valu-
able attribution maps from various types of VLM models and utilize them
as visual prompts to offer hints for visual perception, thereby boosting per-
formance.
3. Our experiments demonstrate the effectiveness of our method across a wide
range of VLM models on various datasets. Moreover, our approach has also
proven effective in addressing the issue of hallucination.
2 Related Works
2.1 Visual Prompting for LVLM
Originatingfromlanguagemodels[33,34,44],theconceptofpromptinghasbeen
widelyappliedinvisionmodelsandvisionlanguagemodelstoenhancethetrans-
ferlearningandadaptationforvarioustasks(e.g.,classification[21,41,77,79,80],4 Yu et al.
detection [13,20,58], segmentation [42] and generation [62]) and under var-
ious learning settings (e.g., few-shot learning [49], continual learning [59,60,
78], domain adaptation/generalization [14,37], unlearning [74], and long-tailed
learning [11]). It is crucial to distinguish our work from soft prompts gener-
ated through gradient optimization and related prompt-tuning efforts. These
prompts, concatenated in the form of continuous vectors to the token sequence
of the VL model’s transformer layer input [8,19,50], or added to the input im-
age as optimizable pixel patches and paddings [22,45], depend on an additional
learning process. Thus, they are strongly coupled with the model and dataset,
lack generalizability, and are not intuitively interpretable. Moreover, since (part
of) these prompts are incorporated at a shallow layer, their optimization pro-
cessinvolvesgradientpropagationthroughouttheentirebranch,whichiscostly.
Unlike these methods, the visual prompting studied in this paper is manually
designed and automatically generated by extra LVLMs. It is interpretable and
generalizable across different models and tasks.
Visual prompting is a specialized technique in vision models, especially for
segmentationtasks[23,27,39].Basedonanadditionallytrainedpromptencoder,
manuallyannotatedpoints,strokes,boxes,andirregularmaskscanprovidethese
models with extra instructions to assist in controlling segmentation granularity
or in facilitating instance selection. Recently, LVLMs have also been shown to
understand manually added circles and color masks in images in a zero-shot
manner, focusing attention on highlighted areas without relying on additional
encoder components [48,68]. Unlike these works that explore the LVLM’s abil-
ity to understand visual prompts, our method discusses how to use pretrained
LVLMs to automatically generate visual prompts to enhance image readability.
The two methods most related to ours are [64] and [65], which modify
masks generated by segmentation models to construct visual prompts to im-
prove LVLM’s performance in segmentation and grounding tasks. Our method
differs fundamentally from theirs in that we use LVLMs to construct visual
prompts. This leads to two main differences in functionality and applicability.
1) For a single image, the visual prompts generated by [64,65] are invariant,
as these models rely on fixed segmentation models. In contrast, with different
text queries, our method can adapt and generate distinct visual prompts to em-
phasize different areas as required. 2) The visual prompts generated by [64,65]
areessentiallyinstance-specificproposalsforsegmentationandgroundingtasks,
focusing on enhancing the LVLM’s grounding capability. Conversely, our visual
promptsaimtohighlightimportantareasneededtoaddresstextqueries,thereby
improvingtheLVLM’sperformanceingeneralVisualQuestionAnsweringtasks.
2.2 Self-Reflection and Ensemble
Our method involves LVLM at two stages: once for generating visual prompts
andonceforperforminginference.WhenthesameLVLMisusedatbothstages,
ourapproachcanbeseenasamethodtoenhanceLVLMperformanceusingself-
reflection technology. The concept of Self-Reflection originated from LLMs [38,
47] but can be directly transferred to LVLMs. Self-Reflection scheme improves
model performance by repeatedly answering a query and iteratively updating
the answer. The Self-Reflection process involves using self-evaluation [3], self-
checking [36], self-feedback [35], feedback from the external environment [7,46],
and even previous answers themselves [76] as hints to input into the model forAttention Prompting on Image for Large Vision-Language Models 5
it to answer the question again. Unlike these works, where the medium of self-
reflectionistext,ourmethodemploysvisualpromptingtoachieveSelf-Reflection
in the pixel space.
WhendifferentLVLMsareusedattwostages,ourmethodcanbeconsidered
a form of model ensemble, where the knowledge of the first VLM is ensembled
into the second VLM in the form of visual prompts. In tasks with standard out-
puts, deep learning model ensemble involves aggregating outputs from multiple
models[16].However,outputaggregationisinvalidingenerationtasks.InLLMs
and LVLMs, model ensemble is achieved in the form of sequential or stage-wise
use between auxiliary and inference models. The final inference model can en-
hanceitsperformancebyincorporatingoutputsfromotherauxiliarymodelsinto
itsinput.Theauxiliarymodeloutputsusedasinputsfortheinferencemodelcan
be responses from another language model [12] or textualized outputs from vi-
sionmodelsorvision-languagemodels(imagecaptions,categorynames)[28,71].
Unlike these works, our method uses visual rather than textual signals for en-
sembling. Furthermore, our approach does not ensemble the final hard outputs
of auxiliary models but their visual cues used during the inference process. This
soft knowledge ensemble provides valuable auxiliary information and mitigates
error accumulation introduced by mistakes in auxiliary model inference.
3 Method
LargeVisionLanguageModelf takesanimageI ∈RH×W×3andatextqueryTi
asinputs,generatinganoutputtextTo =f(I,Ti).Duringtheinferenceprocess
using API, instead of being directly fed into f, the original image I undergoes
an additional annotation operation A, resulting in an image Ia =A(I,Ti) that
has been overlaid with a heatmap Φ. Subsequently, the annotated image Ia and
the original query are input into the LVLM model f, producing the output
To =f(Ia,Ti). The overall framework of the method is shown in Fig. 1.
In our method, the annotation process comprises two steps. The first step
involves using an auxiliary LVLM model g to establish an initial attribution
map Ψ between the text query Ti and each patch of the image. This attribution
map indicates which patches in the image are more relevant to Ti or which
patches should be paid more attention to for answering Ti. In our method,
there are no additional constraints on the LVLM g; if the inference LVLM f
is accessible and capable of performing the annotation operation A, then the
LVLM g used to generate the attribution map can be the same as f, i.e., g =f.
Alternatively, g could be a different LVLM to introduce knowledge from other
models to enhance f’s functionality, i.e., g ̸= f. Moreover, due to the diversity
ofLVLMmodels,wedonotnecessarilyusetheattentionmapasourattribution
map. For example, for the image-text matching model, experiments have shown
thatusingtheattentionmapastheattributionmaphassuboptimalresults.After
obtaining the attribution map Ψ, the second step in the annotation process is
to convert it into a suitable Φ and apply it to the original image using alpha
blending.
Various LVLM models can be utilized to generate attribution maps. We
discuss two prevalent and representative LVLM models: CLIP [40], exemplify-
ing image-text matching models, and LLaVA [32], representing vision-language-
input text generation models.6 Yu et al.
3.1 Obtaining Attribution Map from CLIP
The CLIP model, g , consists of an image encoder and a text encoder, calcu-
clip
lating the similarity between an image and a text query in the image-language
latent space, sim(Iˆ,Tˆ), where Iˆ = gimg(I) and Tˆ = gtext(T). This similarity
clip clip
measure evaluates the correlation between the entire image and the query. To
obtain the attribution map value from the text query to each image patch, we
decompose the output image-level similarity Iˆand then calculate the similarity
of each patch’s output with the Tˆ.
The decomposition process is as follows. Due to the presence of residual
connections, the final output of the vision tower, Iˆ, actually includes influences
fromeachlayer.Consequently,Iˆcanbeexpressedasalinearcombinationofthe
values at the class token positions from each layer
L L
Iˆ=L((cid:2) Z0 (cid:3) )+(cid:88) L((cid:104) MSAl(Zl−1)(cid:105) )+(cid:88) L((cid:104) MLPl(Zˆl)(cid:105) ), (1)
cls
cls cls
l=1 l=1
where L denotes the number of transformer layers within the vision encoder,
with MSA and MLP representing the Multihead Self-Attention structure and
the Multi-Layer Perceptron structure within the transformer, respectively; L
represents the linear transformation that includes the fully-connected layer and
the normalization operations performed after the transformer structure, before
calculatingthesimilarityscore;Zl signifiestheinputtokensequenceforthel-th
transformerlayer;and[Z] indicatesthevalueoftheclstokenwithinthetoken
cls
sequenceZ.Theseoutputclstokensareaggregatedthroughresidualconnections
to form the output of the vision encoder. As evidenced in [17,32], among these
summation terms, the outputs of the last few layers of MSA play a decisive
role, while the contributions from the outputs of the shallow MSA layers, the
outputsofMLP,andtheZ0 term,whichisindependentoftheinputimage,can
cls
be considered negligible to the final measurement of similarity. Therefore, the
similaritysim(Iˆ,Tˆ)caneffectivelybeapproximatedbycalculatingthesimilarity
between Tˆ and the aggregated outputs of MSAs in the deeper layers :
L
sim(Iˆ,Tˆ)≈sim((cid:88) L((cid:104) MSAl(Zl−1)(cid:105) ),Tˆ), (2)
cls
l=L′
where L′ represents a predefined starting layer index. To further calculate the
attribution of the text query to each patch, inspired by [17], we unfold the
operations of the Multihead Self-Attention, obtaining
H
(cid:104) (cid:105) (cid:88)(cid:104) (cid:105)
MSAl(Zl−1) = A(l,h)V(l,h)W(l,h) +Bl (3)
cls cls
h
T (cid:34) H (cid:35) T
=(cid:88) (cid:88) A(l,h)V(l,h)W(l,h)+ 1 Bl ≜(cid:88) ηl, (4)
cls,t t,: HT t
t=1 h t=1
(cid:124) (cid:123)(cid:122) (cid:125)
TheMSAoutputcorrespondingto
thet-thpatch(token)
where A(l,h), V(l,h) are the attention map and the value matrix in the l-th layer
corresponding to the h-th head, respectively; W(l,h) is the weight matrix in theAttention Prompting on Image for Large Vision-Language Models 7
l-th layer used to merge the multiple attention heads and corresponds to the
h-th head; B(l) is the bias matrix in the l-th layer used to merge the multiple
attentionheads;A(l,h) denotestheattentionvalueoftheclasstokentowardsthe
cls,t
t-th token in A(l,h), and V(l,h) represents the t-th row of V(l,h); H and T are
t,:
the number of attention heads and the number of tokens, respectively; and the
value T equals the number of patches P ×P plus one.
Consequently, by summing across layers and incorporating the final linear
transformation, we obtain ψ ≜(cid:80)L L(ηl), which is the direct influence of the
t l=L′ t
t-th patch to the similarity in Eq. (2), allowing us to calculate the similarity
between text query and the t-th image patch. Accordingly, the attribution map
Ψcls ∈RP×P is defined as
Ψcls ≜sim(ψ ,Tˆ), where t=1+j+P ∗(i−1). (5)
i,j t
By decomposing the cls token, we can identify which patches are more rele-
vanttothequery.Thisapproachisparticularlyeffectivewhenthequerycontains
specific entities, allowing for accurate grounding. However, in complex Visual
Question Answering (VQA) tasks, there are often no explicit entities mentioned
inthequery,orthelogicandanalysisprocessinvolvedinansweringthequestion
may rely on entities that are not explicitly mentioned in the query. To address
this issue, we also define another complementary attribution map Ψcomp using
the CLIP model. This map is designed to capture patches that have potential
or implicit relevance to the query.
Weexperimentallyobservethat,inthevisiontransformerofCLIP,thesimi-
larityscoreofthequeryfeatureTˆandtokensotherthantheclstokeninthefinal
layer can (inversely) select the important regions. Patches corresponding to the
image background or large monochrome areas have a significantly higher simi-
larity score with Tˆ than those tokens representing specific entities (which may
notnecessarilyappearinthequery).Thisphenomenonissimilartoobservations
made in [10]. Drawing on analyses of the transformer’s mechanism in [10,43], a
potential explanation is that these “blank” tokens, lacking valuable information
themselves,aretreatedbythetransformerasregisters.Thetransformerinitially
utilizes them to store information from other informative tokens, subsequently
filtering and aggregating this stored information to the class token via the at-
tention mechanism to formulate the final prediction. Therefore, tokens other
than the class token, with a high similarity score to Tˆ, represent patches with
low information content that can be disregarded. We define the complementary
attribution map as follows
Ψcomp ≜1−sim(L(ZL),Tˆ), where t=1+j+P ∗(i−1), (6)
i,j t
where ZL is the t-th output token from the last transformer layer. The com-
t
plementary attribution map is inversely related to similarity, suggesting that
patches lacking information are ignored, retaining only those with potential rel-
evance.
Thus, we obtain two attribution maps that complement each other: Ψcls ex-
plicitly identifies patches directly related to entities in the query but may miss
some potentially relevant patches. Ψcomp equally identifies all patches with po-
tentialrelevancebutlacksspecificityandcannothighlightthosedirectlyrelated
to entities in the query.8 Yu et al.
Byintegratingthetwoattributionmapsthroughthefollowingoperation,we
obtain the final attribution map for CLIP:
Ψ ≜Ψcls+Ψcomp−Ψcomp∗Ψcls. (7)
i,j i,j i,j i,j i,j
This integration can be considered as a soft OR operation (a detailed mathe-
matical explanation is provided in the Appendix). This ensures that the final
attribution map highlights patches directly related to entities within the query
while retaining those with potential or implicit relevance, merely reducing the
weights of patches that do not contain important information for the query. If
the function of the final attribution map were described as an algorithm, then
thisattributionmapwould,inthefirststep,applyamasktoallnon-informative
patches, making them less considered in subsequent VQA processes while leav-
ing other patches unaffected; and, in the second step, for patches not masked, if
a patch is directly related to the entities in the query, it further highlights this
patch.
3.2 Obtaining Attribution Map from LLaVA
The LLaVA model is an auto-regressive vision-language-input text generation
model that utilizes Multihead Self-Attention to extract information from text
queries and image patches, predicting the following tokens. Given a text token
sequenceoflengthN,Ztext ={Ztext}N ,andanimagetokensequenceoflength
t t=1
P ×P, Zimg = {Zimg}P×P, LLaVA generates a new token sequence of length
t t=1
M, Zout ={Zout}M . We directly use the attention weight between token Zout
t t=1 t
and each image token as Zout’s attribution to that image patch. Similar to the
t
strategy for the CLIP model, we select attention maps from the deeper layer to
extract attention weights. The final attribution map is averaged over the entire
generatedtokensequenceandallattentionheads.Formally,theattributionmap
Ψ is defined as
M H
Ψ ≜ 1 (cid:88) (cid:88) A(L¯,h), where t=j+P ∗(i−1). (8)
i,j MH m,t
m=1h=1
Inthedefinition,A(L¯,h) isagaintheattentionmapintheL¯-thlayercorrespond-
ing to the h-th head, where L¯ is a set to be a hyper-parameter; for notation
simplicity, A(L¯,h) here is a submatrix of the entire attention map and only in-
cludes cross attention between Zout and Zimg;
A(L¯,h)
still denotes the attention
m,t
value from the m-th token to the t-th token.
3.3 From Token Space to Pixel Space
The attribution map Ψ ∈RP×P is generated in the token space. We first resize
it back to the pixel space to obtain the raw heatmap Φˆ≜Resize(Ψ). Due to the
square shape of the patches, the mask pattern in Φˆ also appears rectangular.
To mitigate the issue that the rectangular mask pattern does not align with
the object’s irregular shape, we apply a mean filter to obtain the final heatmap
Φ ≜ Mean (Φˆ), where k is the kernel size of the filter. The final heatmap Φ is
k
thenoverlaidontheoriginalimagebyusingitasthealphachannel,resultingin
the final image after annotation Ia.Attention Prompting on Image for Large Vision-Language Models 9
Table1:Comparisonofourmethodwithprevioustextualandvisualpromptingmeth-
ods for various LVLMs. The best result are marked for each model-dataset pair.
Inference Dataset
PromptingMethod
Model
VisWiz TextVQA MMMU MM-Vet MME LLaVA-Bench
w/oprompt 60.93 48.32 35.15 32.8 85.5 71.9
+Step-by-Step 60.98(+0.1) 48.22(−0.1) 35.40(+0.3) 33.7(+0.9) 84.2(−1.3) 73.5(+1.6)
FGVP(Mask) 56.89(−4.0) 39.38(<-5) 36.14(+1.0) 31.0(−1.8) 75.8(<-5) 57.4(<-5)
LLaVA FGVP(RBM) 61.22(+0.3) 33.91(<-5) 35.00(−0.2) 25.0(<-5) 81.4(−4.1) 57.4(<-5)
SoM 54.16(<-5) 18.81(<-5) 35.57(+0.4) 26.4(<-5) 75.4(<-5) 56.1(<-5)
Ours(CLIP) 61.26(+0.3) 48.78(+0.5) 37.52(+2.4) 35.3(+2.5) 87.2(+1.7) 74.1(+2.2)
Ours(LLaVA) 61.35(+0.4) 48.79(+0.5) 36.95(+1.8) 36.6(+3.8) 86.3(+0.8) 74.8(+2.9)
w/oprompt 53.54 78.41 36.43 49.6 81.8 50.8
+Step-by-Step 28.86(<-5) 42.53(<-5) 29.19(<-5) 48.0(−1.6) 63.0(<-5) 40.7(<-5)
FGVP(Mask) 53.55(+0.0) 63.69(<-5) 35.34(−1.1) 44.1(<-5) 80.4(−1.4) 49.1(−1.7)
CogVLMFGVP(RBM) 53.68(+0.1) 65.51(<-5) 36.55(+0.1) 48.2(−1.4) 82.0(+0.2) 48.1(−2.7)
SoM 51.00(−2.5) 36.64(<-5) 35.55(−0.9) 31.2(<-5) 78.0(−3.8) 38.9(<-5)
Ours(CLIP) 54.01(+0.5) 78.99(+0.6) 37.05(+0.6) 52.5(+2.9) 82.3(+0.5) 53.3(+2.5)
Ours(LLaVA) 54.34(+0.8) 78.85(+0.4) 36.95(+0.5) 52.0(+2.4) 82.7(+0.9) 52.4(+1.6)
w/oprompt 59.40 50.60 50.55 67.00 84.3 102.0
+Step-by-Step 55.75(−3.6) 49.85(−0.7) 48.33(−2.2) 62.50(−4.5) 82.0(−2.3) 102.6(+0.6)
FGVP(Mask) 69.30(+9.9) 45.95(−4.6) 43.88(<−5) 61.00(<−5) 65.0(<-5) 59.2(<-5)
GPT-4V
FGVP(RBM) 69.40(+10.0) 46.15(−4.4) 52.50(+1.9) 60.20(<−5) 79.6(−4.7) 92.5(<-5)
(1106)
SoM 65.30(+5.9) 45.00(<−5) 48.33(−2.22) 58.90(<−5) 65.8(<-5) 56.1(<-5)
Ours(CLIP) 69.50(+10.1) 51.50(+0.9) 50.96(+0.4) 67.70(+0.7)85.3(+1.0) 103.3(+1.3)
Ours(LLaVA) 71.01(+11.6) 50.80(+0.2) 51.38(+0.8) 67.10(+0.1) 84.7(+0.3) 103.6(+1.6)
w/oprompt 50.28 56.68 35.11 59.0 78.6 81.5
+Step-by-Step 22.82(<-5) 21.51(<-5) 36.37(+1.3) 30.6(<-5) 29.8(<-5) 40.5(<-5)
FGVP(Mask) 52.88(+2.6) 40.81(<-5) 34.88(−0.2) 45.8(<-5) 71.0(<-5) 64.2(<-5)
Gemini FGVP(RBM) 53.01(+2.7) 45.67(<-5) 34.08(−1.0) 52.0(<-5) 77.4(−1.2) 82.3(+0.8)
SoM 51.25(+1.0) 27.29(<-5) 34.77(−0.3) 34.4(<-5) 69.8(<-5) 64.5(<-5)
Ours(CLIP) 58.58(+8.3) 59.07(+2.4) 37.71(+2.6) 60.5(+1.5) 80.2(+1.6) 85.2(+3.7)
Ours(LLaVA) 58.17(+7.9) 58.35(+1.7) 38.16(+3.1) 60.1(+1.1) 80.0(+1.4) 82.3(+0.8)
4 Experiments
We show the main experimental results in this section. More experiments and
implementation details are in the appendix.
4.1 Comprehensive VQA Tasks
Datasets.Experimentsareconductedon6datasets:VisWiz[5],TextVQA[51],
MMMU[70],MME[15],MM-Vet[69],andLLaVA-Bench[32].Theperformance
on the first four datasets is evaluated using matching accuracy with the ground
truth response. The performance of the latter two datasets is measured using
the GPT-based evaluation scores.
LVLMs.Experimentsareconductedusingtwoopen-sourcemodels:CogVLM[56]
and LLaVA [31], and two commercial models: GPT-4V [66] and Gemini [52].
DuetoGPT-4V’stokenlimit,followingtheexperimentprotocolintheprevious
work [64] when conducting experiments with GPT-4V, for VisWiz, TextVQA,
and MMMU, we randomly selected 200 images from the dataset to verify our
method. Because, about 50 questions on MM-Vet are categorised as related to
personal identification or brand evaluation due to GPT-4V’s safety policy and
are refused to answers. Therefore, we evaluated our method’s performance only
on the remaining questions.
Comparison. We compare with the following methods: (1) naively feeding the
query and image to the model without any prompt; (2) using “Let’s think step10 Yu et al.
Table 2: Ablation study on the auxiliary VLM Scale. The best result are marked for
each auxiliary model-dataset pair.
Mask Model MMMU MME
w/o prompt 35.15 85.50
CLIP-ViT-B 36.03 (+0.88) 83.50 (−2.00)
CLIP-ViT-L 36.21 (+1.09) 83.50 (−2.00)
CLIP-ViT-L-33637.52 (+2.37)87.16 (+1.66)
LLaVA-7B 35.86 (+0.71) 85.66 (+0.16)
LLaVA-13B 36.95 (+1.80)86.34 (+0.84)
by step” as a prompt to trigger the model’s chain-of-thought process, a method
that has been proven to significantly improve zero-shot reasoning performance
for LLMs [25]; and (3) two visual prompting methods designed for LVLMs,
FGVP [65] and SoM [64]. FGVP is designed to generate diverse visual prompts.
We compared the most straightforward method of using a mask as a visual
prompt and the best-performing method of using a Reverse Blur Mask (RBM)
as a visual prompt. Performance improvements/decrements listed in the table
are calculated relative to the “w/o prompt” method.
The main observations from the experimental results are as follows: (1) Our
method consistently achieves the best performance across all datasets-LVLM
pairsinTab.1.RegardlessofwhethertheCLIPorLLaVAisusedastheauxiliary
model, our method leads to performance improvements. For LLaVA, CogVLM,
GPT-4V, and Gemini, the average improvements relative to “w/o prompt” are
1.94%, 1.38%, 1.76%, and 3.42%, respectively. Our method performs particu-
larly well on Gemini+VisWiz, with an average improvement of 8.1%. Excluding
it, our method appears more effective for open-ended questions, with an aver-
age improvement of 2.20% on MM-Vet and LLaVA-Bench, while the average
accuracy increase on multiple-choice and true-false datasets is 1.18%. (2) The
“let’sthinkstepbystep” approach,whichissignificantlyeffectiveinLLMs,does
not perform well in VQA tasks. We suspect this is because this method can-
notenhancetheLVLM’svisualperceptioncapabilitiesandmayevenexacerbate
LVLM’s hallucination due to its language-oriented prompt nature. (3) Previous
visual prompting methods, lacking the ability to adapt to different queries, do
notperformwellonVQAtasks.Ourmethodisclearlysuperiortothem.Thisin-
dicatesthatindiscriminatelyannotatingobjectsinanimagedoesnoteffectively
assist the model in performing VQA tasks. Visual prompting methods need the
ability to adapt to queries.
4.2 Ablation Studies
We identify three important factors affecting the performance of our method
and conduct ablation studies on them.
The Power of the Auxiliary Model.OntheMMMUandMMEdatasets,we
used CLIP models and LLaVA models of different scales to generate heatmaps,
withLLaVAservingastheinferencemodel,tocompareperformance.Theresults
are shown in Tab. 2. As the scale of the auxiliary model increased, the perfor-
mance of our method also improved. Both increasing the depth of the auxiliaryAttention Prompting on Image for Large Vision-Language Models 11
Table 3: Ablation study on the mean filter kernel size.
Kernel Size MMMU MME
w/o filter
36.09 (+0.94) 83.70 (−1.80)
(kernelsize=1)
3 36.95 (+1.80) 86.20 (+0.70)
7 36.32 (+1.17) 87.14 (+1.74)
w/o prompt
35.15 85.50
(kernelsize≥2W=2H)
Table 4: AblationstudyontheTransformerlayerforattributionmapextraction.The
best result are marked for each auxiliary model-dataset pair.
Mask ModelLayer Index MMMU MME
w/o prompt n/a 35.15 85.50
23 36.32 (+1.17) 87.16 (+1.66)
22 37.52 (+2.37) 84.80 (−0.70)
CLIP
20 37.12 (+1.97) 83.20 (−1.30)
15 36.14 (+0.99) 83.16 (−1.34)
23 36.15 (+1.00) 83.10 (−1.40)
22 36.49 (+1.35) 83.00 (−1.50)
LLaVA
20 36.95 (+1.80)86.34 (+0.84)
15 36.32 (+1.17) 83.16 (−2.34)
model or reducing the patch size to generate attribution map with finer granu-
larity prove to be effective for improving the performance of our method. When
the capability of the auxiliary model is insufficient, the masks generated by it
could even be detrimental.
TheKernelSizeoftheMeanFilter.Tomitigatethelimitationsofrectangu-
larmaskpatternswhenhighlightingirregularlyshapedobjects,weincorporated
ameanfilterintoourmethod.Weconductedablationstudiesondifferentkernel
sizesontheMMMUandMMEdatasetswithLLaVAastheinferencemodel.The
resultsareshowninTab.3.Withoutthemeanfilter,heatmapswithrectangular
patternscouldpotentiallyharmthefinaltask’sperformance.Theoptimalkernel
size varied across datasets, due to the different image complexity and question
complexity.
The Layer for Attribution Map Extraction. Another factor affecting our
method’s performance was the layer used for extracting the attribution map.
Althoughweknewthatdeeperlayers,whichcontainhigher-levelsemanticinfor-
mation, should be used, the specific choice of layer also impacted our method’s
performance. We conduct ablation on MMMU and MMe datasets using LLaVA
asinferencemodel.TheresultsareshowninTab.4.FortheCLIPmodel,thelast
two layers are more effective. However, for LLaVA, directly using the attention
maps from the last two layers do not yield good results; the best performance
occurredwhenamid-to-late layerwas used,such as20-thlayerforLLaVA-13B.12 Yu et al.
Table 5: Thecomparisonbetweenourmethodandtextualself-reflectionmethodand
their combination.
Prompt Method LLaVA-Bench
w/o prompt 71.90
textual self reflection 72.90 (+1.00)
ours (LLaVA) 74.80 (+2.90)
+ reflection via re-emphasize 72.70 (+0.80)
+ reflection via evaluation 76.10 (+4.20)
4.3 Self-Reflection
WhentheauxiliaryLVLMandtheinferenceLVLMarethesame,ourmethodcan
beseenashavingatwo-roundchatwiththeLVLM.Thefirstroundgeneratesan
annotated image, where the highlighted areas represent what the LVLM consid-
ers important, embedding the LVLM’s process of extracting visual information.
The second round conducts inference based on the generated annotated image,
allowing the LVLM to perform Self-Reflection and refine its previous process
of visual information extraction. Unlike previous Self-Reflection methods using
text as a medium in LLMs, under the API framework, all information related
to the first answer is stored in the annotated image, and the text response from
the first round is not provided to the model in the second round.
As a new perspective of Self-Reflection, we explore two questions: (1) Can
visual mediums also achieve effective Self-Reflection? To answer this, we com-
pared text-based Self-Reflection and our method using LLaVA as the inference
modelontheLLaVA-Benchdataset.TheresultsinTab.5showthatourmethod
achieves better performance than text-based Self-Reflection, proving that visual
mediums can effectively facilitate Self-Reflection.
Thesecondquestionis:(2)Canwemoreeffectivelyutilizevisualmediumsfor
Self-Reflection? Generally, Self-Reflection techniques involve two steps int the
second round: first, evaluating the previous answer, and second, combining the
evaluationtore-answerthequestion.However,inourframework,theevaluation
processisnotincluded,andthemodeldirectlyproceedstoinference.Therefore,
we designed a new inference process. We input the annotated image and the
question into the VLM, prompting it to judge whether the highlighted areas in
the image support the answer to the question. If yes, the answer is generated
usingtheannotatedimage;ifnot,theanswerisgeneratedusingtheoriginalim-
age.Theresult(thelastrowofTab.5)showsthatthisstrategyfurtherimproves
our method. Conversely, when we do not allow the model to perform evaluation
andemphasizethattheanswerlieswithinthehighlightedareasoftheannotated
image, performance decreases (second to last row in Tab. 5). This also proves
the importance and effectiveness of the evaluation process when using visual
mediums for Self-Reflection.
4.4 Other Discussion
Hallucination.Wealsoexploreourmethod’sabilitytoassistLVLMinovercom-
inghallucinations.Weconducttwoexperiments.First,onVisWiz,wecalculated
theaccuracywithwhichourmethodandthebaselineidentifytheunanswerableAttention Prompting on Image for Large Vision-Language Models 13
Table 6: The performance of our method on hallucination datasets.
Prompt Method VisWiz-Unanswerable POPE
w/o prompt 81.41 81.00
Ours (CLIP) 83.83 (+2.42) 82.81 (+0.81)
Ours (LLaVA) 85.26 (+3.85) 83.52 (+2.52)
questions. These questions often involve information that does not exist in the
image,thustheresponsestothesequestionsarebasedonhallucination.Second,
we conduct experiments on a subset of a commonly used LVLM hallucination
dataset POPE [29]. The experimental results presented in Tab. 6 demonstrate
that our method also has the ability to mitigate hallucination.
5 Conclusion
In this work, we introduce a novel visual prompting technique called Attention
PromptingonImage(API),whichincorporatesanauxiliaryLVLMtogenerate
an attention heatmap on the image dependent on text query. Our extensive
experiments demonstrate the advantages of our prompting method for different
LVLMs on various benchmarks. Additionally, our approach offers new insights
into using visual signals for LVLM ensembling and LVLM self-reflection.
Acknowledgement
This project is supported by the Ministry of Education, Singapore, under its
Academic Research Fund Tier 2 (Award Number: MOE-T2EP20122-0006).14 Yu et al.
References
1. Achiam,J.,Adler,S.,Agarwal,S.,Ahmad,L.,Akkaya,I.,Aleman,F.L.,Almeida,
D.,Altenschmidt,J.,Altman,S.,Anadkat,S.,etal.:Gpt-4technicalreport.arXiv
preprint arXiv:2303.08774 (2023)
2. Alayrac,J.B.,Donahue,J.,Luc,P.,Miech,A.,Barr,I.,Hasson,Y.,Lenc,K.,Men-
sch, A., Millican, K., Reynolds, M., et al.: Flamingo: a visual language model for
few-shotlearning.AdvancesinNeuralInformationProcessingSystems35,23716–
23736 (2022)
3. Asai, A., Wu, Z., Wang, Y., Sil, A., Hajishirzi, H.: Self-rag: Learning to retrieve,
generate, and critique through self-reflection. CoRR (2023)
4. Awadalla, A., Gao, I., Gardner, J., Hessel, J., Hanafy, Y., Zhu, W., Marathe,
K., Bitton, Y., Gadre, S., Sagawa, S., et al.: Openflamingo: An open-source
frameworkfortraininglargeautoregressivevision-languagemodels.arXivpreprint
arXiv:2308.01390 (2023)
5. Bigham, J.P., Jayant, C., Ji, H., Little, G., Miller, A., Miller, R.C., Miller, R.,
Tatarowicz,A.,White,B.,White,S.,Yeh,T.:Vizwiz:nearlyreal-timeanswersto
visual questions. In: Proceedings of the 23nd Annual ACM Symposium on User
Interface Software and Technology. p. 333–342 (2010)
6. Burns, C., Izmailov, P., Kirchner, J.H., Baker, B., Gao, L., Aschenbrenner, L.,
Chen,Y.,Ecoffet,A.,Joglekar,M.,Leike,J.,Sutskever,I.,Wu,J.:Weak-to-strong
generalization: Eliciting strong capabilities with weak supervision (2023)
7. Chen, X., Lin, M., Schärli, N., Zhou, D.: Teaching large language models to self-
debug. CoRR (2023)
8. Chowdhury,S.,Nag,S.,Manocha,D.:Apollo:Unifiedadapterandpromptlearn-
ing for vision language models. In: Conference on Empirical Methods in Natural
Language Processing, EMNLP (2023)
9. Dai, W., Li, J., Li, D., Tiong, A., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.:
InstructBLIP: Towards general-purpose vision-language models with instruction
tuning. In: Thirty-seventh Conference on Neural Information Processing Systems
(2023), https://openreview.net/forum?id=vvoWPYqZJA
10. Darcet, T., Oquab, M., Mairal, J., Bojanowski, P.: Vision transformers need reg-
isters. CoRR (2023)
11. Dong, B., Zhou, P., Yan, S., Zuo, W.: LPT: long-tailed prompt tuning for image
classification. CoRR (2022)
12. Du, Y., Li, S., Torralba, A., Tenenbaum, J.B., Mordatch, I.: Improving factuality
and reasoning in language models through multiagent debate. CoRR (2023)
13. Du, Y., Wei, F., Zhang, Z., Shi, M., Gao, Y., Li, G.: Learning to prompt for
open-vocabulary object detection with vision-language model. In: IEEE / CVF
Computer Vision and Pattern Recognition Conference (CVPR) (2022)
14. Fahes, M., Vu, T., Bursuc, A., Pérez, P., de Charette, R.: Pøda: Prompt-driven
zero-shot domain adaptation. CoRR (2022)
15. Fu,C.,Chen,P.,Shen,Y.,Qin,Y.,Zhang,M.,Lin,X.,Qiu,Z.,Lin,W.,Yang,J.,
Zheng, X., Li, K., Sun, X., Ji, R.: MME: A comprehensive evaluation benchmark
for multimodal large language models. CoRR abs/2306.13394 (2023)
16. Ganaie,M.A.,Hu,M.,Malik,A.K.,Tanveer,M.,Suganthan,P.N.:Ensembledeep
learning: A review. Eng. Appl. Artif. Intell. 115, 105151 (2022)
17. Gandelsman, Y., Efros, A.A., Steinhardt, J.: Interpreting CLIP’s image repre-
sentation via text-based decomposition. In: International Conference on Learning
Representations (ICLR) (2024)
18. Gao, P., Han, J., Zhang, R., Lin, Z., Geng, S., Zhou, A., Zhang, W., Lu, P., He,
C.,Yue,X.,etal.:Llama-adapterv2:Parameter-efficientvisualinstructionmodel.
arXiv preprint arXiv:2304.15010 (2023)
19. Gao, T., Fisch, A., Chen, D.: Making pre-trained language models better few-
shot learners. In: Annual Meeting of the Association for Computational Linguis-
ticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing,
ACL/IJCNLP (2021)Attention Prompting on Image for Large Vision-Language Models 15
20. Guo, Z., Dong, B., Ji, Z., Bai, J., Guo, Y., Zuo, W.: Texts as images in prompt
tuning for multi-label image recognition. In: IEEE / CVF Computer Vision and
Pattern Recognition Conference (CVPR) (2023)
21. Jia,M.,Tang,L.,Chen,B.,Cardie,C.,Belongie,S.J.,Hariharan,B.,Lim,S.:Vi-
sualprompttuning.In:EuropeanConferenceonComputerVision(ECCV)(2022)
22. Jia,M.,Tang,L.,Chen,B.,Cardie,C.,Belongie,S.J.,Hariharan,B.,Lim,S.:Vi-
sualprompttuning.In:EuropeanConferenceonComputerVision(ECCV)(2022)
23. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment anything.
In: arXiv (2023)
24. Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models
are zero-shot reasoners. Advances in neural information processing systems 35,
22199–22213 (2022)
25. Kojima, T., Gu, S.S., Reid, M., Matsuo, Y., Iwasawa, Y.: Large language models
are zero-shot reasoners. Advances in neural information processing systems 35,
22199–22213 (2022)
26. Li, B., Zhang, Y., Chen, L., Wang, J., Yang, J., Liu, Z.: Otter: A multi-modal
model with in-context instruction tuning. arXiv preprint arXiv:2305.03726 (2023)
27. Li,F.,Jiang,Q.,Zhang,H.,Ren,T.,Liu,S.,Zou,X.,Xu,H.,Li,H.,Li,C.,Yang,
J., Zhang, L., Gao, J.: Visual in-context prompting. CoRR (2023)
28. Li, S., Du, Y., Tenenbaum, J.B., Torralba, A., Mordatch, I.: Composing ensem-
blesofpre-trainedmodelsviaiterativeconsensus.In:InternationalConferenceon
Learning Representations (ICLR) (2023)
29. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.R.: Evaluating object
hallucinationinlargevision-languagemodels.In:The2023ConferenceonEmpir-
ical Methods in Natural Language Processing (2023), https://openreview.net/
forum?id=xozJw0kZXF
30. Lin, Z., Liu, C., Zhang, R., Gao, P., Qiu, L., Xiao, H., Qiu, H., Lin, C., Shao,
W., Chen, K., Han, J., Huang, S., Zhang, Y., He, X., Li, H., Qiao, Y.: SPHINX:
the joint mixing of weights, tasks, and visual embeddings for multi-modal large
language models. CoRR abs/2311.07575 (2023)
31. Liu,H.,Li,C.,Li,Y.,Lee,Y.J.:Improvedbaselineswithvisualinstructiontuning
(2023)
32. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. In: Conference on
Neural Information Processing Systems (NeurlPS) (2023)
33. Liu,P.,Yuan,W.,Fu,J.,Jiang,Z.,Hayashi,H.,Neubig,G.:Pre-train,prompt,and
predict:Asystematicsurveyofpromptingmethodsinnaturallanguageprocessing.
ACM Comput. Surv. 55(9), 195:1–195:35 (2023)
34. Ma, X., Fang, G., Wang, X.: Llm-pruner: On the structural pruning of large lan-
guagemodels.Advancesinneuralinformationprocessingsystems36,21702–21720
(2023)
35. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon,
U., Dziri, N., Prabhumoye, S., Yang, Y., Gupta, S., Majumder, B.P., Hermann,
K.,Welleck,S.,Yazdanbakhsh,A.,Clark,P.:Self-refine:Iterativerefinementwith
self-feedback.In:ConferenceonNeuralInformationProcessingSystems(NeurlPS)
(2023)
36. Miao, N.,Teh,Y.W.,Rainforth,T.:Selfcheck:Usingllmstozero-shotchecktheir
own step-by-step reasoning. CoRR (2023)
37. Niu,H.,Li,H.,Zhao,F.,Li,B.:Domain-unifiedpromptrepresentationsforsource-
free domain generalization. CoRR (2022)
38. Pan, L., Saxon, M., Xu, W., Nathani, D., Wang, X., Wang, W.Y.: Automatically
correctinglargelanguagemodels:Surveyingthelandscapeofdiverseself-correction
strategies. arXiv preprint arXiv:2308.03188 (2023)
39. Pan, T., Tang, L., Wang, X., Shan, S.: Tokenize anything via prompting. CoRR
(2023)16 Yu et al.
40. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell,A.,Mishkin,P.,Clark,J.,Krueger,G.,Sutskever,I.:Learningtransferable
visual models from natural language supervision. In: International Conference on
Machine Learning (ICML) (2021)
41. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell,A.,Mishkin,P.,Clark,J.,Krueger,G.,Sutskever,I.:Learningtransferable
visual models from natural language supervision. In: Meila, M., Zhang, T. (eds.)
International Conference on Machine Learning (ICML) (2021)
42. Rao,Y.,Zhao,W.,Chen,G.,Tang,Y.,Zhu,Z.,Huang,G.,Zhou,J.,Lu,J.:Dense-
clip:Language-guideddensepredictionwithcontext-awareprompting.In:IEEE/
CVF Computer Vision and Pattern Recognition Conference (CVPR) (2022)
43. Reddy,G.:Themechanisticbasisofdatadependenceandabruptlearninginanin-
context classification task. International Conference on Learning Representations
(ICLR) (2023)
44. Sahoo, P., Singh, A.K., Saha, S., Jain, V., Mondal, S., Chadha, A.: A systematic
survey of prompt engineering in large language models: Techniques and applica-
tions. CoRR (2024)
45. Shen, S., Yang, S., Zhang, T., Zhai, B., Gonzalez, J.E., Keutzer, K., Darrell, T.:
Multitask vision-language prompt tuning. CoRR (2022)
46. Shinn,N.,Cassano,F.,Gopinath,A.,Narasimhan,K.,Yao,S.:Reflexion:language
agents with verbal reinforcement learning. In: Conference on Neural Information
Processing Systems (NeurlPS) (2023)
47. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., Yao, S.: Reflexion: Lan-
guage agents with verbal reinforcement learning. Advances in Neural Information
Processing Systems 36 (2024)
48. Shtedritski,A.,Rupprecht,C.,Vedaldi,A.:WhatdoesCLIPknowaboutaredcir-
cle?visualpromptengineeringforvlms.In:InternationalConferenceonComputer
Vision (ICCV) (2023)
49. Shu, M., Nie, W., Huang, D., Yu, Z., Goldstein, T., Anandkumar, A., Xiao, C.:
Test-time prompt tuning for zero-shot generalization in vision-language models.
CoRR (2022)
50. Shu, M., Nie, W., Huang, D., Yu, Z., Goldstein, T., Anandkumar, A., Xiao, C.:
Test-time prompt tuning for zero-shot generalization in vision-language models.
In: Conference on Neural Information Processing Systems 2022, NeurIPS (2022)
51. Singh,A.,Natarjan,V.,Shah,M.,Jiang,Y.,Chen,X.,Parikh,D.,Rohrbach,M.:
Towardsvqamodelsthatcanread.In:IEEE/CVFComputerVisionandPattern
Recognition Conference (CVPR). pp. 8317–8326 (2019)
52. Team,G.,Anil,R.,Borgeaud,S.,Wu,Y.,Alayrac,J.B.,Yu,J.,Soricut,R.,Schalk-
wyk,J.,Dai,A.M.,Hauth,A.,etal.:Gemini:afamilyofhighlycapablemultimodal
models. arXiv preprint arXiv:2312.11805 (2023)
53. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient
foundation language models. arXiv preprint arXiv:2302.13971 (2023)
54. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bash-
lykov, N., Batra, S., Bhargava, P., Bhosale, S., et al.: Llama 2: Open foundation
and fine-tuned chat models. arXiv preprint arXiv:2307.09288 (2023)
55. Wang,T.,Zhang,J.,Fei,J.,Zheng,H.,Tang,Y.,Li,Z.,Gao,M.,Zhao,S.:Caption
anything: Interactive image description with diverse multimodal controls (2023)
56. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao,
L., Song, X., Xu, J., Xu, B., Li, J., Dong, Y., Ding, M., Tang, J.: Cogvlm: Visual
expert for pretrained language models (2023)
57. Wang, W., Ren, Y., Luo, H., Li, T., Yan, C., Chen, Z., Wang, W., Li, Q., Lu,
L., Zhu, X., Qiao, Y., Dai, J.: The all-seeing project v2: Towards general relation
comprehension of the open world (2024)Attention Prompting on Image for Large Vision-Language Models 17
58. Wang,W.,Cao,Y.,Zhang,J.,Tao,D.:FP-DETR:detectiontransformeradvanced
by fully pre-training. In: International Conference on Learning Representations
(ICLR) (2022)
59. Wang,Y.,Huang,Z.,Hong,X.:S-promptslearningwithpre-trainedtransformers:
An occam’s razor for domain incremental learning. CoRR (2022)
60. Wang, Z., Zhang, Z., Lee, C., Zhang, H., Sun, R., Ren, X., Su, G., Perot, V., Dy,
J.G., Pfister, T.: Learning to prompt for continual learning. In: CVPR (2022)
61. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q.V., Zhou,
D., et al.: Chain-of-thought prompting elicits reasoning in large language models.
Advances in Neural Information Processing Systems 35, 24824–24837 (2022)
62. Wu, C.H., Motamed, S., Srivastava, S., la Torre, F.D.: Generative visual prompt:
Unifying distributional control of pre-trained generative models. CoRR (2022)
63. Wu, C., Yin, S., Qi, W., Wang, X., Tang, Z., Duan, N.: Visual chatgpt: Talking,
drawing and editing with visual foundation models. CoRR (2023)
64. Yang, J., Zhang, H., Li, F., Zou, X., Li, C., Gao, J.: Set-of-mark prompting un-
leashes extraordinary visual grounding in GPT-4V. CoRR (2023)
65. Yang,L.,Wang,Y.,Li,X.,Wang,X.,Yang,J.:Fine-grainedvisualprompting.In:
Conference on Neural Information Processing Systems (NeurlPS) (2023)
66. Yang,Z.,Li,L.,Lin,K.,Wang,J.,Lin,C.C.,Liu,Z.,Wang,L.:Thedawnoflmms:
Preliminary explorations with gpt-4v(ision) (2023)
67. Yang, Z., Li, L., Wang, J., Lin, K., Azarnasab, E., Ahmed, F., Liu, Z., Liu, C.,
Zeng, M., Wang, L.: Mm-react: Prompting chatgpt for multimodal reasoning and
action. arXiv preprint arXiv:2303.11381 (2023)
68. Yao, Y., Zhang, A., Zhang, Z., Liu, Z., Chua, T., Sun, M.: CPT: colorful prompt
tuning for pre-trained vision-language models. CoRR (2021)
69. Yu,W.,Yang,Z.,Li,L.,Wang,J.,Lin,K.,Liu,Z.,Wang,X.,Wang,L.:Mm-vet:
Evaluating large multimodal models for integrated capabilities (2023)
70. Yue, X., Ni, Y., Zhang, K., Zheng, T., Liu, R., Zhang, G., Stevens, S., Jiang, D.,
Ren,W.,Sun,Y.,Wei,C.,Yu,B.,Yuan,R.,Sun,R.,Yin,M.,Zheng,B.,Yang,Z.,
Liu,Y.,Huang,W.,Sun,H.,Su,Y.,Chen,W.:Mmmu:Amassivemulti-discipline
multimodalunderstandingandreasoningbenchmarkforexpertagi.arXivpreprint
arXiv:2311.16502 (2023)
71. Zeng, A., Attarian, M., Ichter, B., Choromanski, K.M., Wong, A., Welker, S.,
Tombari,F.,Purohit,A.,Ryoo,M.S.,Sindhwani,V.,Lee,J.,Vanhoucke,V.,Flo-
rence, P.: Socratic models: Composing zero-shot multimodal reasoning with lan-
guage. In: International Conference on Learning Representations (ICLR) (2023)
72. Zhang,A.,Ji,W.,Chua,T.:Next-chat:AnLMMforchat,detectionandsegmen-
tation. CoRR (2023)
73. Zhang,Y.,Ma,Z.,Gao,X.,Shakiah,S.,Gao,Q.,Chai,J.:Groundhog:Grounding
large language models to holistic segmentation (2024)
74. Zhang, Z., Zhou, Y., Zhao, X., Che, T., Lyu, L.: Prompt certified machine un-
learningwithrandomizedgradientsmoothingandquantization.In:Conferenceon
Neural Information Processing Systems (NeurlPS) (2022)
75. Zhao, X., Yang, X., Pang, T., Du, C., Li, L., Wang, Y.X., Wang, W.Y.: Weak-to-
strong jailbreaking on large language models (2024)
76. Zheng, C., Liu, Z., Xie, E., Li, Z., Li, Y.: Progressive-hint prompting improves
reasoning in large language models. CoRR (2023)
77. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional prompt learning for vision-
language models. In: IEEE / CVF Computer Vision and Pattern Recognition
Conference (CVPR) (2022)
78. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Conditional prompt learning for vision-
language models. In: IEEE / CVF Computer Vision and Pattern Recognition
Conference (CVPR) (2022)
79. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language
models. Int. J. Comput. Vis. (2022)18 Yu et al.
80. Zhou, K., Yang, J., Loy, C.C., Liu, Z.: Learning to prompt for vision-language
models. Int. J. Comput. Vis. (2022)
81. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-
language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 (2023)
82. Zhu, X., Zhang, R., He, B., Zeng, Z., Zhang, S., Gao, P.: Pointclip V2: adapting
CLIP for powerful 3d open-world learning. CoRR abs/2211.11682 (2022)Attention Prompting on Image for Large Vision-Language Models 19
Attention Prompting on Image for Large
Vision-Language Models
- Supplementary Material -
6 Examples
Question
Question: What fruit is in the left part of the fridge?
Input Images
+
Original Image Heatmap API-Generated Image
Answers from LVLM
GPT-4V + Original Image:
On the left side of the fridge, there is a clear container filled with
strawberries. Below that container is another one with blueberries.
Both strawberries and blueberries are types of fruit.
GPT-4V + API-Generated Image:
In the left part of the fridge, there are strawberries. They appear to
be stored in a clear, plastic clamshell container, which is quite common
for berry packaging.
Fig.2:Incompleximagesincludingmultipleobjects,ourmethodaccuratelyhighlights
the fruits and masks the other objects, thereby simplifying the scene and facilitating
the LVLM’s inference of spatial relationships.
empty20 Yu et al.
Question
Question: On the right desk, what is to the left of the laptop?
Input Images
+
Original Image Heatmap API-Generated Image
Answers from LVLM
GPT-4V + Original Image:
To the left of the laptop on the right desk, there is a white cup or mug
that seems to be placed on a light-colored coaster or a small dish.
GPT-4V + API-Generated Image:
To the left of the laptop on the right desk, there appears to be a desk
lamp. The lamp is black, and its design suggests it is adjustable, with a
classic desk lamp shape and a directional shade that can be oriented to
direct the light where needed.
Fig.3:Ourmethodidentifiesregionsrelatedtotheobjects,therebyassistingtheLVLM
in spatial reasoning.
emptyAttention Prompting on Image for Large Vision-Language Models 21
Question
Question: What is the index of the step when we need to add all purpose flour?
Input Images
+
Original Image Heatmap API-Generated Image
Answers from LVLM
LLaVA-1.5 + Original Image:
The index of the step when we need to add all purpose flour is step 5.
LLaVA-1.5 + API-Generated Image:
The index of the step when we need to add all purpose flour is step 3.
Fig.4:OurmethodassistsLVLM’srecognitionprocessbyhighlightingthecorrespond-
ing steps in the flowchart.
Question
Question: What are all the scene text in the image?
Input Images
+
Original Image Heatmap API-Generated Image
Answers from LVLM
LLaVA-1.5 + Original Image:
The scene text in the image reads “S33020C”.
LLaVA-1.5 + API-Generated Image:
The scene text in the image reads “S3302CD”.
Fig.5: In this example, our method enhances LVLM’s OCR capability by masking
background areas and highlighting the regions that require OCR.22 Yu et al.
Question
Question: What is unusual about this image?
Input Images
+
Original Image Heatmap API-Generated Image
Answers from LVLM
Gemini + Original Image:
A man is ironing clothes while riding on the back of a moving vehicle.
Gemini + API-Generated Image:
A man is ironing a shirt while riding on the back of a moving vehicle.
Fig.6: In this example, our method highlights related regions and enables the LVLM
to generate more detailed and accurate response.
emptyAttention Prompting on Image for Large Vision-Language Models 23
Question
Question: Is the trash empty or full?
Input Images
+
Original Image Heatmap API-Generated Image
Answers from LVLM
CogVLM + Original Image:
The trash appears to be full, as evidenced by the overflowing white
trash bag.
CogVLM + API-Generated Image:
The trash appears to be empty as there are no visible items inside.
Fig.7: Inthisexample,wherethequestionaskstodeterminewhetherthetrashcanis
full,ourmethodaccuratelyhighlightstheareaaroundthetrashcan’sopening,thereby
guiding the LVLM to make a correct judgment.
empty24 Yu et al.
Question and Ground Truth Answer
Question: What country is one book about? Answer the question using a single
word or phrase.
Ground Truth Answer: China
Input Images
+
Original Image Heatmap API-Generated Image
Answers from LVLM
LLaVA-1.5 + Original Image: France
LLaVA-1.5 + API-Generated Image: China
Fig.8: Inthisexample,wherethequestionisrelatedtobooks,ourmethodaccurately
highlights the area where the books are located in the image.
emptyAttention Prompting on Image for Large Vision-Language Models 25
Question and Ground Truth Answer
Question: What is the largest measurement we can see on this ruler? Answer the
question using a single word or phrase.
Ground Truth Answer: 50
Input Images
+
Original Image Heatmap API-Generated Image
Answers from LVLM
LLaVA-1.5 + Original Image: 40
LLaVA-1.5 + API-Generated Image: 50
Fig.9: In this example, the largest measurement number 50 on the ruler is not fully
displayed, leading to error in the baseline method. In contrast, as seen through the
heatmap,our method emphasizesthe bottom right corner ofthe imagewhere theend
of the ruler is located, thereby guiding the LVLM to provide the correct answer.
empty26 Yu et al.
Question and Ground Truth Answer
Question: Is the baby on the left of the dog in the image? Please answer yes or no.
Ground Truth Answer: No
Input Images
+
Original Image Heatmap API-Generated Image
Answers from LVLM
GPT-4V + Original Image: Yes
GPT-4V + API-Generated Image: No
Fig.10: Our method accurately emphasizes the baby and dog in the image, thereby
facilitating the inference of their spatial relationship.
emptyAttention Prompting on Image for Large Vision-Language Models 27
Question and Ground Truth Answer
Question: Are there any red shoes in the image? Please answer yes or no.
Ground Truth Answer: Yes
Input Images
+
Original Image Heatmap API-Generated Image
Answers from LVLM
LLaVA-1.5 + Original Image: No
LLaVA-1.5 + API-Generated Image: Yes
Fig.11: In this example, the question is related to the shoes, which are small objects
and are difficult to recognize for the model. Our method accurately located the shoes
in the image, leading the LVLM to the correct answer.
7 Notation Table
Although the definitions of all symbols are included within the main text, we
provide a comprehensive notation table in Tabs. 7 and 8 to facilitate easy refer-
ence and a macro-level understanding of the concepts involved in each part of
the method.28 Yu et al.
Table 7: The notations used in the manuscript.
Symbol Definition Mainly used in
f LVLM used for inference Entire Sec. 3
g AuxiliaryLVLMusedforattributionmapextraction Entire Sec. 3
A Annotation function, which is the proposed method Entire Sec. 3
I Original image Entire Sec. 3
Ia Image with annotations, which is obained by visual Entire Sec. 3
prompting method
Ψ Attribution map in the token space, which is ex- Entire Sec. 3
tractedfromtheauxiliaryLVLMandisusedtogen-
erate the heatmap
Φ Heatmap in the pixel space, which will be overlied Entire Sec. 3
on the original image
Ti Input text query Entire Sec. 3
To Output text response Entire Sec. 3
A(l,h) Attention map in the l-th transformer layer corre- Entire Sec. 3
sponding to the h-th head
g CLIP model Sec. 3.1
clip
Iˆ Image feature generated by CLIP, which is able to Sec. 3.1
calculate the similarity
Tˆ TextfeaturegeneratedbyCLIP,whichisabletocal- Sec. 3.1
culate the similarity
L NumberoftransformerlayerswithintheCLIPvision Sec. 3.1
encoder
MSA Multihead Self-Attention structure Sec. 3.1
MLP Multi-Layer Perceptron structure Sec. 3.1
Zl Input token sequence for the l-th transformer layer Sec. 3.1
[Z] Value of the cls token within the token sequence Z. Sec. 3.1
clsAttention Prompting on Image for Large Vision-Language Models 29
Table 8: The notations used in the manuscript.
Symbol Definition Mainly used in
L Linear transformation in the CLIP model, which is Sec. 3.1
performedafterthetransformerstructure,beforecal-
culating the similarity score
L′ In the similarity decomposition of the CLIP model, Sec. 3.1
only the MSA output of last L−L′ layers are con-
sidered. L′ is the starting layer index.
V(l,h) Value matrix in the l-th layer corresponding to the Sec. 3.1
h-th head
W(l,h) Weight matrix in the l-th layer used to merge the Sec. 3.1
multiple attention heads and corresponds to the h-
th head. For each head, after the the multiplication
betweentheattentionmapandthevaluematrix,we
have a matrix with the size of T ×D′. To aggregate
thematricesfromallheads,aweightmatrixwiththe
size of (H×D′)×D is used. W(l,h) is obtained from
splitting this large weight matrix.
B(l) Bias matrix in the l-th layer used to merge the mul- Sec. 3.1
tiple attention heads
A(l,h) Attention value of the class token towards the t-th Sec. 3.1
cls,t
token in A(l,h)
V(l,h) t-th row of V(l,h) Sec. 3.1
t,:
H Number of attention heads Sec. 3.1
T Number of tokens Sec. 3.1
ηl MSA output of the l-th layer corresponding to the Sec. 3.1
t
t-th patch(token)}
ψ ηl summing over the layer index Sec. 3.1
t t
Ψcls Attribution map generated from the CLS token Sec. 3.1
Ψcomp Complementaryattributionmapgeneratedusingthe Sec. 3.1
non-CLS token
Ztext N tokens corresponding to the text query Sec. 3.2
Zimg P ×P tokens corresponding to the image patches Sec. 3.2
Zout M tokens generated by the LLaVA model Sec. 3.2
A(L¯,h) AttentionvalueinA(L¯,h) fromthem-thtokentothe Sec. 3.2
m,t
t-th token
Φˆ Rawheatmap,whichisgeneratedbyresizingtheat- Sec. 3.3
tribution map30 Yu et al.
eg
am
I
lan
ig
irO
Query: “Where is the computer?” Query: “Where is the blanket?”
y
tiralim
iS
n
ek
o
T
S
L
C
Annotated Image Heatmap Annotated Image Heatmap
y
tiralim
iS
n
ek
o
T
S
L
C
n-
o
N Annotated Image Heatmap Annotated Image Heatmap
Fig.12: Comparison between the functionality of CLS token similarity and the Non-
CLS token similarity.
8 Observation and Discussion of API Method
8.1 CLS Token Similarity and Non-CLS Token Similarity
To extract heatmaps from the CLIP model, we designed two complementary
typesofattributionmaps:onebasedonthedecompositionofsimilaritybetween
the feature of the CLS token and text feature, and the other measuring the
similarity between the feature of the Non-CLS tokens and text feature. Fig. 12
compares the differences in functionality between these two types of attribution
maps. The third row in the image shows the heatmap generated solely based
on Ψcls and its resulting annotated image. The fourth row shows the heatmap
obtainedsolelyfromΨcomp.Firstly,wecanobservethatwhenthequerychanges,
Ψcls canhighlightdifferentpartsoftheimagecorrespondingtodifferentqueries.
It selects the areas where the blanket and computer are located based on the
query.However,Ψcomp doesnotshowsignificantdifferencesinresponsepatterns
todifferentqueries.Ontheotherhand,Ψcompcanfilteroutthebackgroundofthe
image,leavingtheobjects,whichpotentiallycanbeusedintheprocessofVQA.
For instance, when the query explicitly mentions “computer”, Ψcls completely
ignores the chair and blanket in the lower left corner, but Ψcomp still assigns
high values to these areas. Therefore, we combine Ψcls and Ψcomp to form a
complete attribution map.Attention Prompting on Image for Large Vision-Language Models 31
8.2 Attribution Map Aggregation for CLIP Model
First,Eq.(7)inthemaintextcanberewrittenas1−(1−Ψcls)(1−Ψcomp),where
since Ψcls and Ψcomp are cosine similarities, both (1−Ψcls) and (1−Ψcomp)
rangebetween0and1.Thus,thefinalmaskisrelatedtotheproductofthetwo
parts, (1−Ψcls) and (1−Ψcomp). If Ψcls and Ψcomp are considered binary, then
(1−Ψcls)(1−Ψcomp)canbeapproximatedasanORoperationbetween(1−Ψcls)
and(1−Ψcomp).Thatis,wheneither(1−Ψcls)or(1−Ψcomp)is0,theequation
will be 1, and only when both are 1, the equation will be 0. This means that for
patch i, as long as either attribution map Ψcls or Ψcomp highlights this patch,
the final attribution map Ψ will also highlight this patch. Only when both Ψcls
and Ψcomp consider patch i unimportant, the final attribution map will ignore
this patch.
Experimentalfindings,asshowninFig.12,indicatethat,ononehand,Ψcomp
can indiscriminately choose all entities, whereas Ψcls selects entities explicitly
mentioned in the query. The highlighted area in Ψcls can be understood as a
subset of the highlighted area in Ψcomp. On the other hand, both Ψcls and
Ψcomp will ignore non-informative parts of the image. Therefore, in actual non-
binarycases, thecomputationof Eq.(7)can be describedasan algorithm:first,
apply a mask to non-informative areas (i.e., instruct the LVLM to ignore these
patches) because these patches will not be selected by either Ψcls or Ψcomp. For
the remaining areas, which are patches with objects directly mentioned in the
query or other entities potentially related to the query, a multiplication of Ψcls
and Ψcomp further highlights the patches with objects appearing in the query
because they have greater weight in Ψcls.
9 More Experimental Results and Implementation
Details
9.1 Ensemble
Table 9: Ensemble of visual prompts generated from different LVLM.
LLaVA-Bench
w/o prompt 102.00
Ours (CLIP) 103.30 (+1.30)
Ours (LLaVA) 103.60 (+1.60)
Ours (CLIP+LLaVA)104.80 (+2.80)
When the auxiliary LVLM and the LVLM used for inference are different,
our approach can be seen as ensembling the knowledge of the auxiliary LVLM
intotheLVLMusedforinferencethroughvisualprompts.Underthisdefinition,
baselinemethodslikeFGVPandSoMcanalsobeconsideredaformofensemble,
not between LVLMs but between a vision model (segmentation model) and an
LVLM.Fromtheexperimentalresults,ourmethodisthefirsteffectiveensemble
method that is based on visual prompting in a VQA context.32 Yu et al.
In traditional ensemble methods that are based on output aggregation, the
numberofmodelstobeensembledcanbemorethan2.However,inourmethod,
we ensemble only two models, namely, an auxiliary LVLM and an LVLM for
inference. To achieve an ensemble of more than two models, we conduct the
following experiment. We use GPT-4V as the inference model and experiment
on the LLaVA-Bench (in-the-wild) dataset, Instead of using a single annotated
image. We input the annotated images generated by both API +CLIP and
CLIP+LLaVA simultaneously into GPT-4V, while keep using the original ques-
tion without additional prompts as the textual query. The experimental results
inTab.9,showthattheensembleofAPI +CLIPandCLIP+LLaVAcanfurther
improve performance.
9.2 Influence on Different VQA Abilities
To thoroughly understand the impact of our method on various capabilities of
LVLMs, we report the performance changes across different specific abilities on
theMM-VetdatasetusingtheCogVLMmodelastheinferencemodelandCLIP
as the mask model. The results are shown in Tab. 10. It is observed that our
method enhances all categories of capabilities in the MM-Vet dataset. Notably,
ourmethodisparticularlybeneficialforOCRandMathabilities.Thesignificant
improvementinOCRcapabilityisattributedtoourmethod’shighlightingofrel-
evantareas,allowingthemodeltofocusonlyonregionsrelatedtoansweringthe
question.ThisnarrowsdownthescopeoftheOCRtask,therebyenhancingOCR
performance. Consequently, the improvement in mathematical ability is closely
linked to the enhancement in OCR capability. Since addressing math-related
questions in images first requires performing OCR tasks, the improvement in
OCR also contributes to the enhancement of mathematical abilities.
Table 10: The influence of our method on various categories of LVLM capabilities.
Capability
Spatial
Recognition OCR Knowledge Generation Math
Relationship
w/o prompt 54.9 42 43.9 42.6 50.1 3.5
Ours 55.3 48.3 45.6 46 51.2 14.6
9.3 Implementation Details
Pre-trained weight and API. During the mask generation phase, we used
the CLIP-ViT-L-336 model [40] released by OpenAI and the LLaVA-1.5-13B
model [31]. In the inference process, we utilized the released weight of LLaVA-
1.5-13B model [31] and cogvlm-chat-v1.1 model [56]. We use the “gpt-4-1106-
vision-preview” and“gemini-pro-vision” modelsforGPT-4V[66]andGemini[52]
API, respectively. All local experiments were deployed on a single A100 GPU.
QueryGPT-4VandGemini.ForGPT-4VandGemini,weusedpythonAPIs
forbatchquerying.Whenencounteringerrorsduetoserverornetworkissues,weAttention Prompting on Image for Large Vision-Language Models 33
pausedforawhileandretriedthequeryonce.Iftheerrorpersisted,werecorded
the response as an empty string. If a query was detected against security policy,
suchaspersonidentification,wedidnotretryanddirectlyrecordedtheresponses
from GPT-4V and Gemini as empty strings.
Baselines. The “w/o prompt” baseline is implemented by directly querying the
LVLM with the question together with the original image. Following [25], the
“Step-by-Step” baselineisimplementedbyinputtingtheoriginalimageandquery
in the format of
[Question] Let’s think step by step.
FortheexperimentswithFGVP[65]andSoM[64],wequerytheLVLMwiththe
correspondingannotatedimageandtheoriginalquestion,whichisalsothesame
whenweimplementourmethod.Theonlydifferenceamongtheexperimentswith
FGVP, SoM and our method is the annotated image. For the FGVP method,
the annotation process is aligned with the default of the released code. For the
SoMmethod,wechooseSAM[23]asthesegmentationmodelandkeepallother
parameters aligned with the default setting in the released code.
Implementation on each dataset. Our implementation on various datasets
adopts the approach from LLaVA [32]. The evaluation process of each dataset
adheres to its official usage protocols or its official template, when it is accessi-
ble. (1) LLaVA-Bench (in-the-Wild) [32] is a dataset comprising real-world
scenes, drawings, memes, and other types of images, along with open-ended
questions. It focuses on testing LVLMs’ capabilities in QA, detailed description,
and complex reasoning. In our implementation, the textual prompt is directly
the question from the dataset. We record the LVLM’s complete answer and use
theGPT-basedevaluationtoolofficiallyreleasedbyLLaVA-Bench(in-the-Wild)
to score the answers. (2) MM-Vet [69] is a comprehensive dataset containing
varioustypesofimages,includingreal-worldscenes,artworks,statisticalgraphs,
memes, etc., along with open-ended questions. Each question involves multiple
aspects of visual and language abilities, such as recognition + spatial aware-
ness or OCR + Math. In our implementation, the textual prompt is directly
the question from the dataset. We record the LVLM’s complete answer and use
the GPT-based evaluation tool officially released by MM-VET to score the an-
swers. (3) MME [15] is a dataset that includes images of real-world scenes, art-
works,logos,etc.,alongwithTrue-Falsequestions.Thisdatasetinvolvesabilities
in commonsense reasoning, numerical calculation, and text translation, among
others. Given its binary response format (yes or no), we add “Please answer
yes or no” as an additional textual prompt to the original question. We eval-
uate the performance by the matching accuracy between LVLM’s answers and
the ground truth. (4) The MMMU [70] dataset encompasses multi-discipline
questionsrequiringcollege-levelexpertiseforresponses.Thequestionsareeither
multiple-choice or can be answered with simple data or phrases. For multiple-
choicequestions,weguidetheLVLMtodirectlyanswerthecorrespondingoption
by adding “Answer with the option’s letter from the given choices directly” af-
ter the original question and options. For other questions, we add “Answer the
question using a single word or phrase.” to the original question. Our experi-
ment is conducted using the validation set of MMMU. Evaluation is based on
the matching accuracy between LVLM’s answers and the ground truth. (5) The
TextVQA [51] dataset contains real-world images with text, where the ques-
tions can be answered with simple words or phrases, mainly testing the LVLM’s34 Yu et al.
OCRandreasoningabilities.Weadd“Answerthequestionusingasinglewordor
phrase” aftertheoriginalquestiontoguidetheLVLMtodirectlyrespondtothe
query without providing additional explanations. Our experiment is conducted
using the validation set of TextVQA. The evaluation score is the matching ac-
curacy between LVLM’s answers and the ground truth. (6) The VisWiz [5]
dataset is collected from questions about real-world images asked by blind peo-
pleandmanuallyannotatedanswers.Thequestionscanbeansweredwithsimple
words or phrases. However, since the questions are from blind individuals, some
questions are unanswerable based on the image alone and thus are marked as
unanswerable. To address this, we concatenate the following prompt after the
original question: “When the provided information is insufficient, respond with
’Unanswerable’. Answer the question using a single word or phrase” Our exper-
iment is conducted using the validation set of VisWiz. Evaluation is based on
the matching accuracy between LVLM’s answers and the ground truth.
Prompts used in the Self-Reflection experiment. For the textual self-
reflection experiment, we use a two-round chat. In the first round, we directly
ask the LVLM to answer the query and record the answer. In the second round,
we use a prompt in the format of
FortheQuestion“[Question]”,Yourpreviousansweris“[Answer in the Round 1]”.
Evaluate the quality of the answer and provide a new answer.
Werecordtheresponseofthesecondroundandextracttheanswerbymanually
delete the sentences related to the quality evaluation of previous answer. The
extracted answer is stored as the final answer. For the “API + reflection via
re-emphasize” setup,weinputtheannotatedimagetogetherwiththepromptin
the format of
[Question](Hint:Theanswerisrelatedtotheunmaskedvisibleregions).
For the “API + reflection via evaluation” setup, we input the annotated image
together with the prompt in the format of
For this image, the question is “[Question]”. Evaluate whether the un-
masked visible regions of the image alone can provide an answer to the
question. If they suffice to answer the question, respond with letter “T”.
If they do not support an answer to the question, reply with the letter
“F”.
If the LVLM responses with “F”, we query it again using the original image and
the question, and then use the response as the final answer. If the LVLM re-
sponseswith“T”,wequeryitagainusingtheannotatedimageandthequestion,
and then use the response as the final answer.
10 Limitation, Future Direction, and Potential Impact
Limitation and future direction.Anessentialcomponentofthisworkisthe
extractionofattributionmapsbasedonanauxiliaryLVLM.Theintroductionof
an auxiliary LVLM enhances the performance of visual prompting methods but
also introduces some limitations and new research opportunities. First, gener-
ating visual prompts based on an LVLM incurs additional computational costs,Attention Prompting on Image for Large Vision-Language Models 35
either from an extra execution of the same LVLM or a forward pass through
another LVLM. Note that this is a limitation, exploring ways to reduce this ad-
ditional overhead, such as using lightweight LVLMs to generate visual prompts
to achieve a weak-to-strong effect [6,75], is a worthwhile research direction.
Secondly, our current selection of auxiliary LVLMs is not adaptive; we cannot
automatically choose a more suitable auxiliary LVLM for different image-query
pairs.Thisisanotherlimitationofourmethodandapotentialresearchdirection
with promise.
Potentialimpact.Thepotentialsocialimpactsofthisworkmainlyincludetwo
aspects.Thefirstaspectisthepotentialaccumulationofbiasandunfairnessdue
to the introduction of an extra LVLM. The bias and unfairness of the auxiliary
LVLM may accumulate through our visual prompts into the final inference pro-
cess. The other aspect is the creation of a new possibility for attacks, namely,
by attacking the auxiliary LVLM to generate harmful visual prompts, thereby
attacking the LVLM. Because the attack is based on the visual prompts in the
pixel space, such attacks might be more covert and difficult to detect.