Blox-Net: Generative Design-for-Robot-Assembly Using VLM
Supervision, Physics Simulation, and a Robot with Reset
Andrew Goldberg1, Kavish Kondap1, Tianshuang Qiu1, Zehan Ma1, Letian Fu1
Justin Kerr1, Huang Huang1, Kaiyuan Chen1, Kuan Fang2, Ken Goldberg1
https://bloxnet.org/
Abstract—Generative AI systems have shown impressive
capabilities in creating text, code, and images. Inspired by the
rich history of research in industrial “Design for Assembly”,
we introduce a novel problem: Generative Design-for-Robot-
Assembly(GDfRA).Thetaskistogenerateanassemblybased
on a natural language prompt (e.g., “giraffe”) and an image
of available physical components, such as 3D-printed blocks.
The output is an assembly, a spatial arrangement of these
components,andinstructionsforarobottobuildthisassembly.
The output must 1) resemble the requested object and 2) be
reliablyassembledbya6DoFrobotarmwithasuctiongripper.
WethenpresentBlox-Net,aGDfRAsystemthatcombinesgen-
erative vision language models with well-established methods
in computer vision, simulation, perturbation analysis, motion
planning,andphysicalrobotexperimentationtosolveaclassof
GDfRA problems with minimal human supervision. Blox-Net
achievedaTop-1accuracyof63.5%inthe“recognizability”of
its designed assemblies (eg, resembling giraffe as judged by a
VLM). These designs, after automated perturbation redesign,
were reliably assembled by a robot, achieving near-perfect
success across 10 consecutive assembly iterations with human
intervention only during reset prior to assembly. Surprisingly,
this entire design process from textual word (“giraffe”) to
Fig. 1: Can a vision-language model generate designs suitable
reliable physical assembly is performed with zero human
for robot assembly? Blox-Net is a GDfRA system that produces
intervention.
3D designs constructible by robots subject to physical material
constraints. (a) Starting with a phrase (e.g., ”giraffe”) and a set
I. INTRODUCTION of blocks, (b) Blox-Net iteratively prompts GPT-4o to generate
designs, using simulation to verify stability. (c) A physical robot
Design-for-Assembly (DfA) has a long history dating
then assembles the design to test stability and constructibility, (d)
back to the start of the Industrial Revolution, where guns,
resulting in the successful assembly of the design.
pocket watches, and clocks were designed with interchange-
potential jamming and wedging conditions (tolerance stack-
able parts to facilitate mass production on human assembly
up) [7].
lines [1]. With the advent of industrial automation in the
AllexistingDfRAsystemsrequirehumandesignersinthe
latterhalfofthe20thcentury,DfAwasexpandedtotakeinto
loop [3, 4, 7]. One factor that is difficult for DfRA systems
accounttheerrortolerancesofmechanicalassemblysystems
to accurately model is the reliability of robot assembly,
driven by mechanical cams and belts, and later for robotic
which depends on the inherent uncertainty in perception,
assembly systems, the latter known as Design-for-Robot-
control, and physics (eg, friction) [8–13]. This can to some
Assembly (DfRA) [2]. DfRA is the process of designing
degree be modeled with simulation, but it is well-known
a product and robot assembly system together to ensure
that 3D simulation systems struggle to accurately model
feasibility, for example designing an injection molded part
minute 3D deformations and collisions that occur during
along with a custom workcell for manipulating it. These de-
robot grasping and effects such as deformations of robot
signsystemswereenhancedbytheemergenceofComputer-
gripperandsuctioncupswhichcanproducesubstantialerrors
Aided Design (CAD) and Computer-Aided-Manufacturing
leading to assembly failures [14–18]. Therefore, physical
(CAM) software that streamlined human visualization and
assembly trials are ideal for evaluation.
evaluation of components and assemblies using Finite Ele-
ment Methods (FEM) and perturbation analysis [3–6]. Such Recent advances in Generative AI systems have demon-
systems help human designers visualize and arrange me- strated remarkable abilities to create novel texts, code, and
chanical components with realistic tolerances, checking for images [19–21]. Researchers are actively exploring “text-to-
video”[22–24]and“text-to-3D”[25–27]systems,wherethe
1TheAUTOLabatUCBerkeley,2CornellUniversity lattergenerates3Dmeshstructuresfromtextualdescriptions
4202
peS
52
]OR.sc[
1v62171.9042:viXraInput VLM Design VLM Design Selection Perturbation Redesign Robot Assembly
“Sofa” 1 4 1. Sample Positions
Describe Updated
Structure
2
Plan Stability
Feedback
3 2. Update Blocks
Order Iterative Design Improvement
Create 10 designs Iterate
Fig. 2: Overview of Blox-Net. We present a multi-stage framework for producing physically constructible models based on a user-
specifiedprompt.TheBlox-NetpipelinebeginswithanaturallanguageinputandJSONdetailingtheavailableblocks.Theseparameters
arepassedintoaseriesofVLMprompts,beginningwithahigh-leveloverview(Describe),followedbyrequestingspecificblockstouse
inconstruction(Plan)andasequencetoplacethemin(Order).Finally,theVLMgeneratestheinitialdesignandentersafeedbackloop,
continuously receiving visual and stability feedback from the simulator. After generating 10 candidate designs, a separate VLM selects
thebeststructurethroughhead-to-headimagecomparisons.Theperturbationredesignphasethenadjuststheselectedstructuretoenhance
its physical constructability before it is assembled by a real robot.
(and there are ongoing research efforts applying Gen AI for 1980s[29],withHitachidevelopingitsAssemblabilityEval-
eCAD design of chips [28]). This suggests that Generative uation Method (AEM) in 1986 [30]. These seminal works
AI may have potential for DfRA, and that if coupled with laid the foundation for systematic approaches that follow
a physical robot, it may be possible in certain cases to fully product design guidelines [31] facilitate facilitate efficient
automate the design cycle. assembly processes. As robotics automation in manufactur-
In this paper, we propose Blox-Net, a fully-implemented ing became prevalent, Design for Robot Assembly (DfRA)
generative DfRA (GDfRA) system that combines the se- emerged as an extension of DfA principles, specifically
mantic and text generation capabilities of large language addressing the unique capabilities and limitations of robotic
models(LLM)withphysicalanalysisfromasimulator.Blox- systems in assembly tasks [32, 33].
Net includes 3 phases: 1) A vision language model (VLM) Design for Robot Assembly (DfRA) [32, 34–36] has
with customized iterative prompting to design a feasible 3D evolved significantly with the advent of Computer-Aided
arrangement of the available components – an assembly – Design (CAD) and Computer-Aided Manufacturing (CAM)
that approximates the shape of the desired object (eg “a software, which expedite design and evaluation of compo-
giraffe”);2)simulationwithperturbationanalysistoevaluate nents and assemblies using Finite Element Methods and
this assembly in terms of physical robot constructability and perturbation analysis [3–7]. While these tools facilitate vi-
to revise the assembly accordingly; 3) Computer vision, sualization and analysis of tolerances, stresses, and forces,
motion planning, and control of a physical robot with a allexistingDfRAsystemsrequireextensivehumaninput[3,
camera to repeatedly, through an automated reset, construct 4, 7]. A persistent challenge in DfRA is accurately mod-
this assembly with the given components to automatically eling assembly reliability, given the inherent uncertainties
evaluate physical assembly reliability. in perception, control, and physics [8–13]. Simulation can
This paper makes the following contributions: partially address this, but struggles to capture 3D deforma-
1) Formulationofanovelproblem,GenerativeDesign-for- tions and collisions crucial to robot grasping, necessitating
Robot-Assembly (GDfRA). iterative real-world testing and redesign [14–18, 37]. Recent
2) Blox-Net, a GDfRA system that combines prompting advancements leverage large language models (LLMs) [20,
of GPT-4o with a physical robot, physics simulation, 38] for various aspects of design, including task planning,
andmotionplanningtoautomaticallyaddressaclassof robot code generation [39, 40], engineering documentation
GDfRAproblemswherethecomponentsare3Dprinted understanding [41], and generating planar layouts or CAD
blocks. models[28,42–44].However,thesemethodsprimarilyfocus
3) Results from experiments suggesting that Blox-Net can on determining assembly sequences for fixed designs. In
produce assemblies – arrangements of given physical contrast, this paper addresses both the design and execu-
blocks – that closely resemble the requested object, tion aspects of robot assembly, aiming to create physically
are stable under gravity throughout the construction feasible designs for robotic assembly with minimal human
process, and can be reliably assembled by a six-axis supervision.
robot arm. Starting from singulated objects, Blox-Net
achieves 99.2% accuracy in autonomous block place- B. Text-to-Shape Generation
ments. Semantic generation of 3D shapes and structures is a
long-standing problem in computer vision and computer
II. RELATEDWORK
graphics [45]. Deep generative models have enabled a wide
A. Design for Robot Assembly
range of approaches that learn to capture the distribution
TheconceptofDesignforAssembly(DfA)waspioneered of realistic 3D shapes, in the format of voxel maps [46],
by Geoffrey Boothroyd and Peter Dewhurst in the early meshes [47], point clouds [48], sign distance functions [49],Tree
Tall Table Long Table
Sheep
Round Table Two Tables
Fig. 3: Diverse Design Generations: Left: Blox-Net generates a diverse set of candidate designs and uses the VLM (GPT 4o) to select
themostsuitableone.Right:Blox-Netaccuratelygeneratesavarietyofstructuraldesigns,adheringtospecificinputconstraints.Asetof
10 designs can be generated in 81 seconds, and the selection of the best design takes an additional 60 seconds.
and implicit representations [50]. A large number of ap- Blox-Net includes three phases. In phase I (Figure 3),
proaches have also been proposed to reconstruct 3D shapes Blox-NetpromptsaVLM(GPT-4o[58])togeneratemultiple
by conditioning on a single or multiple images [51–55]. assembly designs, from which the VLM selects the top
With the advances of aligned text-image representations and candidate based on stability and visual fidelity. In phase
vision-languagemodels,anincreasingnumberofworkshave II (Section IV-B), the chosen assembly design undergoes
aimed to generate semantically meaningful shapes specified an iterative refinement process in a customized physics
by natural language instructions [25, 56, 57]. Unlike these simulator.Thissimulation-basedapproachappliescontrolled
methods, based on the available physical building blocks, perturbations to enhance the design’s constructability while
Blox-Netgenerates3Dshapesby promptinganLLM(Chat- maintaining its core characteristics. In phase III (Section IV-
GPT 4o [58]) and then generates a plan for assembling the C), Blox-Net utilizes a robot arm equipped with a wrist-
blocks to construct the desired shape. mounted stereo camera and suction gripper to construct the
optimized design using 3D printed blocks. The assembly
C. Robot Task Planning with Foundation Models
is constructed on a tilt plate, which the robot actuates to
Recent advancements in large pre-trained models, such as automatically reset the blocks back into a tray.
large language models (LLMs) and vision-language models
A. Phase I: VLM Design and Selection
(VLMs) [20, 59–65], have significantly impacted robotics
task planning by leveraging vast internet-scale data. These Given the language description and a set of blocks with
models enable end-to-end learning through fine-tuning on known sizes and shapes, Blox-Net uses a VLM to gener-
roboticsdatasets[66–71]orallowLLMstodirectlygenerate ate candidate structure designs. Unlike existing text-to-3D
taskormotionplansintextorcode[70,72–79].Ratherthan generation methods that produce unconstrained meshes [25,
focusingonmotionorwaypointplanning,Blox-Netprompts 56],Blox-Netgenerates3Dstructuressubjecttothephysical
the VLM to generate a construction plan by determining constraints imposed by the available blocks. It prompts the
the poses of blocks to form semantically meaningful and VLM to generate an assembly plan that specifies the 3D
physically feasible structures, which are then assembled locations and orientations for placing each block using the
using motion planning and force feedback control. available components (illustrated in Fig. 3 (VLM Design
Prompting)).
III. GDFRAPROBLEM To facilitate high-quality generation, similar to DALL-E
We formally define the problem of Generative Design for 3 [80], Blox-Net first elaborates the prompt. For example,
RoboticAssembly(GDfRA).Weconsiderthedesignofa3D to construct a “giraffe”, the VLM is prompted to give a
structure that can be assembled with an industrial robot arm concise, qualitative textual description that conveys the key
(seeFigure1).Theinputisawordorphrase(e.g.,“bridge”) featuresofagiraffebyhighlightingtheoverallstructureand
and an image of available components for assembly. The proportions.
objective for the GDfRA system is to design a structure After prompt elaboration, the VLM is prompted for the
which is (1) ”recognizable” meaning the structure visually assembly plan. Specifically, the prompt includes the target
resembles the provided text input and (2) ”constructible” object (“giraffe”), the VLM’s elaboration response, and the
meaning the structure can be assembled by a robot. set of available blocks. The set of available blocks is en-
coded as JSON, which provides a structured, flexible format
IV. METHOD
familiar to VLM models. Based on these inputs, the VLM
We present Blox-Net, a system for a class of GDfRA is asked to explain each block’s role in the structure.
that assumes (1) components are cuboids and cylinders and Once a high-level plan is generated, Blox-Net prompts
(2) components are lying in stable poses within a reachable the VLM to produce an assembly plan, specifying the
planar area. rotation, position and color of objects. Instead of usingeach point. The block position is updated to the average
of positions that are stable and free from collision. This
process is applied to all blocks in the structure until no
furtheradjustmentsareneededoreachblockhasbeenvisited
a predefined maximum number of times.
C. Phase III: Robot Assembly and Evaluation
Fig.4:BlockReorientation:Therobotfirstplacestheblockintoa To evaluate constructability, Blox-Net automates physical
90degreeanglebracket.Then,theblockisregraspedonadifferent assemblyandevaluatesthegenerateddesignonarobot.The
face, achieving a 90 degree rotation. robot first moves to a predefined pose and captures a top-
common rotation parametrizations like Euler angles, Blox- down RGBD image of the blocks on a plastic tray. Blox-
Net instructs the VLM to rotate blocks by rearranging their Net uses SAM [81] to segment an RGB image and obtain
dimensions directly, thereby providing a more simple inter- image masks. SAM segmentations include regions that do
face for specifying orientation. Next, Blox-Net prompts the not correspond to blocks. To filter out extraneous masks, we
VLM to output the (x, y) coordinates for block placement. generate a point cloud for each mask by deprojecting the
Limiting the specification to (x, y) coordinates rather than masked area from the depth image obtained from a stereo
(x, y, z) simplifies the action space and avoids potential camera [82]. Blox-Net then discards masks that are outside
issues with blocks being placed inside one another. Blocks the tray, below a certain minimum area, or not circular or
are placed by dropping them in the order. rectangular.
To enhance stability and correct misplaced blocks, Blox- Blox-Net refines each mask to segment the top of each
Net performs iterative, simulation-in-the-loop prompting. blockbyfittingaRANSAC[83]planetothepointcloudand
Each block’s placement is simulated by dropping it in sim- retaining only inliers. The block’s rotation is determined by
ulation from above the structure. After each placement, the fittingthetightestorientedboundingboxtotherefinedmask.
systemcheckstheblockforstability.Ifinstabilityisdetected, The block’s center is the mean of the points in the filtered
details such as the specific block that moved, the direction point cloud, with the x and y dimensions measured from
of movement, and two orthographic views highlighting the the point cloud and the z dimension derived from its height
unstable block are included in a prompt sent back to the relative to the tray base.
VLM for correction. This process continues until all blocks Upon determining the size, shape, position, and dimen-
are stable or a maximum of two iterations is reached. sionsforeachblock,Blox-Netcanobtainanewplanthrough
This full prompting pipeline is run in parallel, generating the design generation and perturbation-based redesign pro-
10 design candidates. For each design, the VLM is queried cess(SectionIV-AandSectionIV-B),orconstructthetarget
with a rendered image from the simulation, and provides a object based on a previously generated plan. Blocks may
ratingfrom1to5basedonhowwellthestructureresembles require rotations about their x or y axis to align with the
the intended design. The top-rated stable designs are then pose used in the plan. This rotation is facilitated by placing
paired in a head-to-head comparison, where two images are the block in a 90-degree angle bracket and regrasping the
shown to the VLM, and it selects the more recognizable block from a different side (Fig. 4). After reorientation, the
design.Thisprocessisrepeatedinaknockoutformat(Fig.3) robotcapturesanewtop-downimageandallblockpositions,
until a final design is chosen. rotations, shapes, and dimensions are recomputed via the
aforementioned pipeline.
B. Phase II: Perturbation-Based Redesign
The assembly process begins after all blocks are properly
In GDfRA, accounting for imprecise state estimation and oriented. Each block is grasped at its centroid, rotated to the
robot control is important to ensure robust assembly. The planned orientation, and placed at the location specified in
design output from the VLM does not account for such the design. Force feedback control is used for both grasping
tolerances, which can result in collisions and misplaced andplacingblocks:duringagrasp,therobotlowersontothe
blocks during assembly. We thus introduce a perturbation- block until a force is detected; similarly, during placement,
based redesign process. it descends and releases the block once a force is sensed.
The redesign process iterates through each of the blocks Toenableefficienttestinganddesignvalidation,wedesign
and determines if adjustments are needed. A block will be an automatic reset. After completing the full assembly, the
perturbed if it violates at least one of the following three robot arm captures an image. Then, the robot presses down
criterion:(1)thesurface-to-surfacedistancetoanotherblock on the tilt plate, dumping the blocks back into the tray. This
islessthanaspecifiedcollisionthresholdandthetwoblocks resets the scene for subsequent trials.
overlap in the gravity-aligned axis (2) the block is already
in collision with another block; or (3) the block is unstable
V. EXPERIMENTS
at some nearby sampled point within a predefined radius. To evaluate how well the generated structures by Blox-
For each block, Blox-Net samples points evenly along Net satisfy the GDfRA objective, we assess both the seman-
regularly spaced, concentric circles centered at the block tic recognizability of the designs (in Section V-A), which
nominal location and checks for stability and collision at refers to how well the designs semantically align with theGiraffe Taj Mahal Shelf Letter U Sofa Table
XZY
Fig. 5: Task
ExecX uZY
tion: We present
BXZ lY
ox-Net VLM generated
dXZY
esigns assembled by
aXZY
robot paired with
simuX lZY
ation renderings
N Top-1Accuracy Avg.Ranking RelativeRanking of placement, and the % of trials where the structure is
5 63.5% 1.7 34.0% fully successfully assembled. These experiments incorporate
10 48.5% 2.92 29.1%
15 46.0% 3.68 24.5% automatedreset,blockreorientation,andassemblyfullyend-
20 41.5% 5.05 25.3% to-end. To assess the system’s autonomy, we track the aver-
TABLE I: VLM-Based Design Recognizability: Top-1 accuracy, age percentage of blocks per trial which require intervention
average ranking, and relative ranking based on GPT-4o responses
during the reset phase, where an intervention is counted for
averagedacrossall200objects.Relativerankingisreportedasthe
each block moved. In failure cases after reset where blocks
average ranking divided by N where N is the number of labels.
occlude each other, are not adequately separated, or fall out
prompts, and their constructability (in Section V-B), which of the tray, blocks are repositioned and placed back in their
refers to how reliably they can be constructed by a real same stable pose. In cases where the robot fails to regrasp
robot. Additionally, we evaluate the effectiveness of the a block during reorientation, the block is placed back in
perturbationredesign(inSectionV-C).Tocreateacandidate the stable pose corresponding to its final stable pose in the
objects list for evaluation, we prompt GPT-4o to generate structure. Human interventions are only performed during
a list of 200 objects spanning categories such as furniture, resetting;thereisnohumaninterventionduringtheassembly
alphabet letters, architecture, and animals. We run Blox- process.
Net’s design generation (Section IV-A) on all objects using
a fixed set of block shapes and dimensions. We evaluate C. Perturbation Redesign Ablation
semantic recognizability on all 200 designs and evaluate We evaluate the effect of perturbation redesign (Sec-
constructability on a representative subset of 11 designs, tion IV-B) on construction success. We conduct experiments
which showcase the capabilities and limitations of Blox- on 5 objects, each assembled 10 times with and without
Net, using a physical robot. Additionally, we evaluate the perturbationredesign.Eachtrialbeginswithallblockssingu-
effectiveness of perturbation redesign on 5 designs using a lated and in their correct stable pose. This isolates the effect
physical robot. of perturbation redesign by eliminating influence from prior
assemblystates.Assemblyisperformedfullyautonomously.
A. Semantic Recognizability
We report the percentage of blocks correctly placed at the
Tomeasurehowwellthegeneratedstructureresemblesthe time of their placement, the average percentage of blocks
requestedlanguagedescription,wedesignanexperimentus- in the correct location at the end of each trial, and the
ingGPT-4oasanevaluatortoassessthesemanticdistinctive- percentage of trials where the structure is fully completed.
ness and accuracy of each design, following methodologies
similar to those used in VLM answer scoring [84–86]. In D. Implementation Details
this experiment, we use a set of N object labels, where N Blox-Net is implemented with the following components:
includesthecorrectlabelalongsideN−1randomlyselected GPT-4o,PyBullet,UR5erobotarm,Robotiqsuctiongripper,
distractor labels from the pool of 200 objects. We provide Zed Mini Stereo Camera, and 3D printed cuboidal and
GPT-4owitharenderedimageofthegeneratedassemblyand cylindrical blocks. We use PyBullet as a simulator and
theN labelsinrandomorder,andtasktheVLMwithranking define simulation parameters as a uniform object density
these labels based on howwell each one matches the image. of 1000kg/m3, lateral friction coefficient of 0.5, spinning
We report the percentage of correct Top-1 predictions, and friction coefficient of 0.2, gravity of −9.81m/s2. A block is
forimperfectguesses,weanalyzetheaveragerankingofthe measured as unstable if after 500 simulation steps at 240Hz
correct label within GPT-4o’s ordered list, (where a ranking the block’s position deviates by more than 1cm or is rotated
of 1 is best). Additionally, we report the average ranking by more than .1 radians from its starting position.
relative to N, with results presented for Top-1 accuracy and During perturbation redesign, Blox-Net samples 8 points
average ranking for N=5, 10, 15, 20. from each of 10 concentric circles with radii from 1mm to
15mm and each block is perturbed a maximum of 10 times.
B. Constructability
Blox-Net filters masks by shape by fitting a minimum area
We measure constructability on a real robot over 10 trials bounding rectangle and minimum bounding circle to each
on 6 designs selected to highlight diversity. For each trial, mask. Masks are discarded if their area is less than 80% of
we record the % of blocks correctly positioned at the time the areas of both bounding shapes.Object %ofBlocksAdjusted %Correct %ofAssemblies
(#ofBlocks) DuringResetPhase BlocksPlaced Completed
FilamentRoll(3) 14% 100% 100%
Giraffe(9) 28% 100% 100%
Lighthouse(7) 18% 100% 100%
Letter-U(3) 7% 100% 100%
Shelf(10) 34% 100% 100%
Table(5) 11% 98% 90%
TABLE II: Robot Assembly and Reset: The table presents the
Fig. 6: VLM Generation Failures Blox-Net’s design generation
robot assembly results for six designs, each assembled by the
occasionally produces designs that: include unavailable blocks
robot over 10 trials following a reset, during which all blocks are
(Rook Chess Piece), incorrectly orient blocks (Bicycle’s Handle-
singulated and reoriented on a plastic tray. Human intervention
bar), or fail to account for gravity (Letter H).
occurred only during the reset phase to de-stack, singulate, and
reorient blocks.
Object %CorrectBlocksPlaced %CorrectinEndState %FullCompletion
✗ ✓ ✗ ✓ ✗ ✓
CeilingFan(7) 66.7% 100% 91.0% 100% 40% 100%
Sandbox(5) 50.0% 96% 50.0% 96% 20% 80%
Shark(6) 75.0% 100% 76.7% 100% 40% 100%
Sofa(4) 72.5% 100% 72.5% 100% 30% 100% Fig. 7: Perturbation Redesign Ablation Failures Omitting per-
TajMahal(10) 71.0% 100% 72.0% 100% 10% 100%
turbationredesignfromtheBlox-Netleadstoasignificantincrease
Average 67.1% 99.2% 72.4% 99.2% 28.0% 96.0%
inphysicalconstructionfailures.Smallinaccuraciesinblockplace-
TABLE III: Perturbation Redesign: Each object is followed by ment result in collisions, fallen blocks, and structural collapses.
the number of blocks in the design enclosed in parenthesis. We
run each design for 10 iterations. ✗ indicates experiments without of correctly placed blocks and the percentage of correctness
perturbationredesignand✓indicatesexperimentswithperturbation in the end state are similar for all objects except the ceiling
redesign. % Correct Blocks Placed: the ratio of blocks that were fan.Whileincorrectblockplacementstypicallyleadtoerrors
placed correctly (determined by a group) to the whole structure. in the final structure, later block placements sometimes
% Correct In End State: the ratio of blocks that remain in their
correct these errors. Perturbation redesign improves the full
correct pose at the end (blocks may be knocked down by later
completion success rate by an average of 4x. Overall, per-
placements). % Full Completion: the ratio of overall success (0
or 1 for the whole structure per run over all 10 runs). We observe turbation redesign significantly enhances the robustness of
a significant performance decrease across all objects in all metrics theassemblyprocessbyaccommodatingslightimprecisions,
whenconstructingwithoutperturbationredesign,demonstratingits leading to more reliable and accurate final structures across
impact on design success.
a variety of designs.
VI. RESULTS
VII. LIMITATIONSANDCONCLUSION
Semantic Recognizability: We present results in Table I.
While Blox-Net shows promising results in constrained
ResultsfromtheevaluationofBloxNet’sdesignsusingGPT-
3D structure generation, it is limited to non-deformable
4oasanevaluatorsuggestthatthegenerateddesignsclosely
cuboid and cylinder blocks, restricting geometric diversity
align with the correct category semantics as recognized by
andreducingBlox-Net’sabilitytorepresentcomplexshapes.
GPT-4o. Notably, with N = 5 labels, the model achieves a
Many assembly designs are still not clearly recognizable,
Top-1 accuracy of 63.5%, demonstrating a consistent corre-
likelydueinparttotheseblocklimitations.Thesystemuses
spondence between the generated designs and the intended
only a suction-based gripper, without accounting for gripper
prompts. Importantly, even with larger label sets, the model
width or slanted surfaces, and sometimes requires human
maintains a reasonable average ranking, with the correct
intervention during the reset process, reducing assembly
label placed consistently near the top. This suggests that the
efficacy.
generated designs remain recognizable, even among a large
ThispaperintroducesBlox-Net,anovelsystemaddressing
pool of designs.
the Generative Design-for-Robot-Assembly problem using a
Constructability: Results are in Table II. All designs are three-phaseapproach:creatingtheinitialdesignsbyprompt-
reliably assembled by the robot without human intervention ing a vision language model, conducting simulation-based
during assembly. Five of six designs achieve a perfect analysisforconstructability,andutilizingaphysicalrobotfor
assembly completion rate, and all designs achieve a 98%+ assembly evaluation. Experiment results suggest that Blox-
placement success rate, highlighting Blox-Net’s ability to Netcanbridgethegapbetweenabstractdesignconceptsand
assemble complex structures. Human interventions, which robot-executable assemblies. Remarkably, five Blox-Net as-
occur only during the reset phase, are sometimes needed semblydesigns,eachusing3to10blocksandscoringhighin
to singulate or reorient blocks. Complex structures, such recognizability, were successfully assembled 10 consecutive
as the Giraffe (9 blocks) or shelf (10 blocks), have more times by the robot without any human intervention.
human reset interventions due to an increasing likelihood of
overlapping, non-singulated, or misoriented blocks. ACKNOWLEDGMENTS
Perturbation Redesign Ablation Results are summarized This research was performed at the AUTOLAB at UC
in Table III. Perturbation redesign greatly improves all three BerkeleyinaffiliationwiththeBerkeleyAIResearch(BAIR)
metrics across all 5 designs to near-perfect. The percentage Lab. The authors were supported in part by donations fromToyota Research Institute, Bosch, Google, Siemens, and [21] L.Ouyangetal.,“Traininglanguagemodelstofollowinstructions
Autodesk and by equipment grants from PhotoNeo, Nvidia, with human feedback,” Advances in neural information processing
systems,vol.35,pp.27730–27744,2022.
and Intuitive Surgical. This material is based upon work
[22] J.Hoetal.,“Imagenvideo:Highdefinitionvideogenerationwith
supported by the National Science Foundation Graduate Re- diffusionmodels,”arXivpreprintarXiv:2210.02303,2022.
search Fellowship Program under Grant No. DGE 2146752. [23] T. Brooks et al., “Generating long videos of dynamic scenes,”
Advances in Neural Information Processing Systems, vol. 35,
Anyopinions,findings,andconclusionsorrecommendations
pp.31769–31781,2022.
expressed in this material are those of the author(s) and [24] L. Castrejon, N. Ballas, and A. Courville, “Improved conditional
do not necessarily reflect the views of the National Science vrnnsforvideoprediction,”inProceedingsoftheIEEE/CVFinter-
nationalconferenceoncomputervision,2019,pp.7608–7617.
Foundation.WethankTimotheKasriel,ChungMinKimand
[25] B. Poole, A. Jain, J. T. Barron, and B. Mildenhall, “Dreamfusion:
Kush Hari for their helpful discussions and feedback. Text-to-3dusing2ddiffusion,”arXiv,2022.
[26] C.-H. Lin et al., “Magic3d: High-resolution text-to-3d content
REFERENCES creation,” in IEEE Conference on Computer Vision and Pattern
Recognition(CVPR),2023.
[1] E. Foner, Give Me Liberty! An American History: Seagull Fourth
[27] H. Wang, X. Du, J. Li, R. A. Yeh, and G. Shakhnarovich, “Score
Edition.WWNorton&Company,2013,vol.1.
jacobianchaining:Liftingpretrained2ddiffusionmodelsfor3dgen-
[2] G.Boothroyd,Assemblyautomationandproductdesign.crcpress,
eration,”inProceedingsoftheIEEE/CVFConferenceonComputer
2005.
VisionandPatternRecognition,2023,pp.12619–12629.
[3] R. W. Kennard and L. A. Stone, “Computer aided design of
[28] M. Liu et al., “Chipnemo: Domain-adapted llms for chip design,”
experiments,”Technometrics,vol.11,no.1,pp.137–148,1969.
arXivpreprintarXiv:2311.00176,2023.
[4] T.-C.Chang,R.A.Wysk,andH.-P.Wang,Computer-aidedmanu-
[29] G.BoothroydandP.Dewhurst,DesignforAssembly-ADesigner’s
facturing.Prentice-Hall,Inc.,1991.
Handbook.Amherst,MA:UniversityofMassachusetts,1983.
[5] K. Millheim, S. Jordan, and C. Ritter, “Bottom-hole assembly
[30] S.Miyakawa,“Thehitachiassemblabilityevaluationmrthod(aem),”
analysis using the finite-element method,” Journal of Petroleum
in Proceedings of 1st Int. Conf. on Product Design for Assembly,
Technology,vol.30,no.02,pp.265–274,1978.
1986,1986.
[6] S. Lee and H. H. Asada, “A perturbation/correlation method for
[31] GeneralElectricCo.,ManufacturingProducibilityHandbook.Sch-
force guided robot assembly,” IEEE Transactions on Robotics and
enectady,NY:ManufacturingServices,GeneralElectricCo.,1960.
Automation,vol.15,no.4,pp.764–773,1999.
[32] S. Y. Nof, W. E. Wilhelm, and H.-J. Warnecke, “Design for as-
[7] Z.BiandX.Wang,Computeraideddesignandmanufacturing.John
sembly,” in Industrial Assembly. Boston, MA: Springer US, 1997,
Wiley&Sons,2020.
pp.84–134.
[8] S. Y. Nof, Handbook of industrial robotics. John Wiley & Sons,
[33] G. Boothroyd, “Design for assembly—the key to design for man-
1999.
ufacture,” The International Journal of Advanced Manufacturing
[9] K.Y.Goldberg,“Orientingpolygonalpartswithoutsensors,”Algo-
Technology,vol.2,pp.3–11,1987.
rithmica,vol.10,no.2,pp.201–225,1993.
[34] T. Ohashi, M. Iwata, S. Arimoto, and S. Miyakawa, “Extended
[10] A. A. Apolinarska et al., “Robotic assembly of timber joints
assemblability evaluation method (aem)(extended quantitative as-
usingreinforcementlearning,”AutomationinConstruction,vol.125,
semblyproducibilityevaluationforassembledpartsandproducts),”
p.103569,2021.
JSMEInternationalJournalSeriesCMechanicalSystems,Machine
[11] Y.Tianetal.,“Assemblethemall:Physics-basedplanningforgener-
ElementsandManufacturing,vol.45,no.2,pp.567–574,2002.
alizableassemblybydisassembly,”ACMTransactionsonGraphics
[35] G. Boothroyd, P. Dewhurst, and W. A. Knight, Product design for
(TOG),vol.41,no.6,pp.1–11,2022.
manufactureandassembly.CRCpress,2010.
[12] J. Luo and H. Li, “A learning approach to robot-agnostic force-
[36] H.K.Rampersad,IntegratedandSimultaneousDesignforRobotic
guided high precision assembly,” in 2021 IEEE/RSJ International
Assembly: Product Development, Planning... John Wiley & Sons,
ConferenceonIntelligentRobotsandSystems(IROS),IEEE,2021,
Inc.,1994.
pp.2151–2157.
[37] Y. Tian et al., Asap: Automated sequence planning for complex
[13] L.Fu,H.Huang,L.Berscheid,H.Li,K.Goldberg,andS.Chitta,
robotic assembly with physical feasibility, 2023. arXiv: 2309.
“Safeself-supervisedlearninginrealofvisuo-tactilefeedbackpoli-
16909[cs.RO].
ciesforindustrialinsertion,”in2023IEEEInternationalConference
[38] H. Touvron et al., “Llama 2: Open foundation and fine-tuned chat
on Robotics and Automation (ICRA), IEEE, 2023, pp. 10380–
models,”arXivpreprintarXiv:2307.09288,2023.
10386.
[39] A. Macaluso, N. Cote, and S. Chitta, “Toward automated pro-
[14] J. Xu, M. Danielczuk, E. Steinbach, and K. Goldberg, “6dfc:
gramming for robotic assembly using chatgpt,” arXiv preprint
Efficiently planning soft non-planar area contact grasps using 6d
arXiv:2405.08216,2024.
frictioncones,”in2020IEEEInternationalConferenceonRobotics
[40] H. You, Y. Ye, T. Zhou, Q. Zhu, and J. Du, “Robot-enabled
andAutomation(ICRA),IEEE,2020,pp.7891–7897.
constructionassemblywithautomatedsequenceplanningbasedon
[15] C. M. Kim, M. Danielczuk, I. Huang, and K. Goldberg, “Ipc-
chatgpt:Robogpt,”Buildings,vol.13,no.7,p.1772,2023.
graspsim:Reducingthesim2realgapforparallel-jawgraspingwith
[41] A. C. Doris, D. Grandi, R. Tomich, M. F. Alam, H. Cheong, and
the incremental potential contact model,” in 2022 International
F.Ahmed,“Designqa:Amultimodalbenchmarkforevaluatinglarge
Conference on Robotics and Automation (ICRA), IEEE, 2022,
language models’ understanding of engineering documentation,”
pp.6180–6187.
arXivpreprintarXiv:2404.07917,2024.
[16] R.M.Murray,Z.Li,andS.S.Sastry,Amathematicalintroduction
[42] A. Gaier, J. Stoddart, L. Villaggi, and S. Sudhakaran, “Generative
toroboticmanipulation.CRCpress,2017.
design through quality-diversity data synthesis and language mod-
[17] H. Huang et al., “Mechanical search on shelves with efficient
els,” in Proceedings of the Genetic and Evolutionary Computation
stackinganddestackingofobjects,”inTheInternationalSymposium
Conference,2024,pp.823–831.
ofRoboticsResearch,Springer,2022,pp.205–221.
[43] A.Badagabettu,S.S.Yarlagadda,andA.B.Farimani,“Query2cad:
[18] J. Mahler, M. Matl, X. Liu, A. Li, D. Gealy, and K. Goldberg,
Generating cad models using natural language queries,” arXiv
“Dex-net 3.0: Computing robust vacuum suction grasp targets in
preprintarXiv:2406.00144,2024.
pointcloudsusinganewanalyticmodelanddeeplearning,”in2018
[44] S.Wuetal.,“Cad-llm:Largelanguagemodelforcadgeneration,”
IEEEInternationalConferenceonroboticsandautomation(ICRA),
inProceedingsoftheneuralinformationprocessingsystemsconfer-
IEEE,2018,pp.5620–5627.
ence.neurIPS,2023.
[19] A.Rameshetal.,“Zero-shottext-to-imagegeneration,”inInterna-
[45] X. Chen, A. Golovinskiy, and T. Funkhouser, “A benchmark for
tional Conference on Machine Learning, PMLR, 2021, pp. 8821–
3D mesh segmentation,” ACM Transactions on Graphics (Proc.
8831.
SIGGRAPH),vol.28,no.3,Aug.2009.
[20] T.Brownetal.,“Languagemodelsarefew-shotlearners,”Advances
[46] J. Wu, C. Zhang, T. Xue, W. T. Freeman, and J. B. Tenen-
inNeuralInformationProcessingSystems,vol.33,pp.1877–1901,
baum, Learning a probabilistic latent space of object shapes via
2020.3d generative-adversarial modeling, 2017. arXiv: 1610.07584 [71] L.Fuetal.,“In-contextimitationlearningvianext-tokenprediction,”
[cs.CV]. arXivpreprintarXiv:2408.15980,2024.
[47] N. Wang, Y. Zhang, Z. Li, Y. Fu, W. Liu, and Y.-G. Jiang, [72] M. Ahn et al., “Do as i can, not as i say: Grounding language in
“Pixel2mesh:Generating3dmeshmodelsfromsinglergbimages,” roboticaffordances,”arXivpreprintarXiv:2204.01691,2022.
inECCV,2018. [73] W. Huang et al., “Inner monologue: Embodied reasoning through
[48] H. Fan, H. Su, and L. Guibas, “A point set generation network planningwithlanguagemodels,”arXivpreprintarXiv:2207.05608,
for 3d object reconstruction from a single image,” in 2017 IEEE 2022.
Conference on Computer Vision and Pattern Recognition (CVPR), [74] W.Huang,P.Abbeel,D.Pathak,andI.Mordatch,“Languagemodels
Los Alamitos, CA, USA: IEEE Computer Society, Jul. 2017, aszero-shotplanners:Extractingactionableknowledgeforembodied
pp.2463–2471. agents,”inInternationalConferenceonMachineLearning,PMLR,
[49] J.J.Park,P.Florence,J.Straub,R.Newcombe,andS.Lovegrove, 2022,pp.9118–9147.
“Deepsdf:Learningcontinuoussigneddistancefunctionsforshape [75] B. Chen et al., “Open-vocabulary queryable scene representations
representation,” in The IEEE Conference on Computer Vision and for real world planning,” in IEEE International Conference on
PatternRecognition(CVPR),Jun.2019. RoboticsandAutomation,IEEE,2023,pp.11509–11522.
[50] L. Mescheder, M. Oechsle, M. Niemeyer, S. Nowozin, and A. [76] J. Liang et al., “Code as policies: Language model programs for
Geiger,“Occupancynetworks:Learning3dreconstructioninfunc- embodied control,” in IEEE International Conference on Robotics
tion space,” in Proceedings IEEE Conf. on Computer Vision and andAutomation,IEEE,2023,pp.9493–9500.
PatternRecognition(CVPR),2019. [77] I. Singh et al., “Progprompt: Generating situated robot task plans
[51] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ra- usinglargelanguagemodels,”inIEEEInternationalConferenceon
mamoorthi, and R. Ng, “Nerf: Representing scenes as neural ra- RoboticsandAutomation,IEEE,2023,pp.11523–11530.
diance fields for view synthesis,” Commun. ACM, vol. 65, no. 1, [78] G.Wangetal.,“Voyager:Anopen-endedembodiedagentwithlarge
pp.99–106,Dec.2021. languagemodels,”arXivpreprintarXiv:2305.16291,2023.
[52] A. Yu, S. Fridovich-Keil, M. Tancik, Q. Chen, B. Recht, and A. [79] S. Mirchandani et al., “Large language models as general pattern
Kanazawa, “Plenoxels: Radiance fields without neural networks,” machines,”inConferenceonRobotLearning(CoRL),2023.
arXivpreprintarXiv:2112.05131,2021. [80] J.Betkeretal.,“Improvingimagegenerationwithbettercaptions,”
[53] B.Kerbl,G.Kopanas,T.Leimku¨hler,andG.Drettakis,“3dgaussian Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf,
splattingforreal-timeradiancefieldrendering,”ACMTransactions vol.2,no.3,p.8,2023.
onGraphics,vol.42,no.4,Jul.2023. [81] A.Kirillovetal.,“Segmentanything,”arXiv:2304.02643,2023.
[54] S. Wang, V. Leroy, Y. Cabon, B. Chidlovskii, and J. Revaud, [82] L.Lipson,Z.Teed,andJ.Deng,“Raft-stereo:Multilevelrecurrent
“Dust3r:Geometric3dvisionmadeeasy,”inCVPR,2024. fieldtransformsforstereomatching,”in2021InternationalConfer-
[55] R. Liu, R. Wu, B. Van Hoorick, P. Tokmakov, S. Zakharov, and enceon3DVision(3DV),IEEE,2021,pp.218–227.
C. Vondrick, “Zero-1-to-3: Zero-shot one image to 3d object,” in [83] M. A. Fischler and R. C. Bolles, “Random sample consensus: A
ProceedingsoftheIEEE/CVFinternationalconferenceoncomputer paradigmformodelfittingwithapplicationstoimageanalysisand
vision,2023,pp.9298–9309. automatedcartography,”CommunicationsoftheACM,vol.24,no.6,
[56] A.Jain,B.Mildenhall,J.T.Barron,P.Abbeel,andB.Poole,“Zero- pp.381–395,1981.
shottext-guidedobjectgenerationwithdreamfields,”CVPR,2022. [84] H.Liu,C.Li,Q.Wu,andY.J.Lee,“Visualinstructiontuning,”in
[57] A.Haque,M.Tancik,A.A.Efros,A.Holynski,andA.Kanazawa, NeurIPS,2023.
“Instruct-nerf2nerf:Editing3dsceneswithinstructions,”inProceed- [85] W.-L. Chiang et al., Vicuna: An open-source chatbot impressing
ingsoftheIEEE/CVFInternationalConferenceonComputerVision, gpt-4with90%*chatgptquality,Mar.2023.
2023,pp.19740–19750. [86] L.Fuetal.,“Atouch,vision,andlanguagedatasetformultimodal
[58] OpenAI, Gpt-4o system card, https://cdn.openai.com/ alignment,” in Forty-first International Conference on Machine
gpt-4o-system-card.pdf,Accessed:2024-09-14,2024. Learning,2024.
[59] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-
trainingofdeepbidirectionaltransformersforlanguageunderstand-
ing,”arXivpreprintarXiv:1810.04805,2018.
[60] A.Radford,K.Narasimhan,T.Salimans,I.Sutskever,etal.,“Im-
provinglanguageunderstandingbygenerativepre-training,”2018.
[61] A.Radford,J.Wu,R.Child,D.Luan,D.Amodei,I.Sutskever,et
al.,“Languagemodelsareunsupervisedmultitasklearners,”OpenAI
Blog,vol.1,no.8,p.9,2019.
[62] A. Chowdhery et al., “Palm: Scaling language modeling with
pathways,”JournalofMachineLearningResearch,vol.24,no.240,
pp.1–113,2023.
[63] J. Achiam et al., “Gpt-4 technical report,” arXiv preprint
arXiv:2303.08774,2023.
[64] A.Radfordetal.,“Learningtransferablevisualmodelsfromnatu-
ral language supervision,” in International conference on machine
learning,PMLR,2021,pp.8748–8763.
[65] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping
language-image pre-training with frozen image encoders and large
languagemodels,”arXivpreprintarXiv:2301.12597,2023.
[66] A.Brohanetal.,“Rt-1:Roboticstransformerforreal-worldcontrol
atscale,”arXivpreprintarXiv:2212.06817,2022.
[67] A.Brohanetal.,“Rt-2:Vision-language-actionmodelstransferweb
knowledge to robotic control,” arXiv preprint arXiv:2307.15818,
2023.
[68] Y.Jiangetal.,“Vima:Generalrobotmanipulationwithmultimodal
prompts,” in Fortieth International Conference on Machine Learn-
ing,2023.
[69] Octo Model Team et al., Octo: An open-source generalist robot
policy,https://octo-models.github.io,2023.
[70] K. Fang, F. Liu, P. Abbeel, and S. Levine, “MOKA: Open-World
Robotic Manipulation through Mark-Based Visual Prompting,” in
Proceedings of Robotics: Science and Systems, Delft, Netherlands,
Jul.2024.