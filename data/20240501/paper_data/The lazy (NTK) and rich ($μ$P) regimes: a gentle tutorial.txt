The lazy (NTK) and rich (µP) regimes:
A gentle tutorial
DhruvaKarkada∗
UCBerkeley
Abstract
Acentralthemeofthemodernmachinelearningparadigmisthatlargerneuralnetworks
achievebetterperformanceonavarietyofmetrics.Theoreticalanalysesoftheseoverparameter-
izedmodelshaverecentlycenteredaroundstudyingverywideneuralnetworks.Inthistutorial,
weprovideanonrigorousbutillustrativederivationofthefollowingfact:inordertotrainwide
networkseffectively,thereisonlyonedegreeoffreedominchoosinghyperparameterssuchas
thelearningrateandthesizeoftheinitialweights.Thisdegreeoffreedomcontrolstherichness
oftrainingbehavior:atminimum,thewidenetworktrainslazilylikeakernelmachine,andat
maximum,itexhibitsfeaturelearningintheso-calledµPregime.Inthispaper,weexplainthis
richnessscale,synthesizerecentresearchresultsintoacoherentwhole,offernewperspectives
andintuitions,andprovideempiricalevidencesupportingourclaims.Indoingso,wehopeto
encouragefurtherstudyoftherichnessscale,asitmaybekeytodevelopingascientifictheory
offeaturelearninginpracticaldeepneuralnetworks.
1 Introduction 2
1.1 Relatedwork . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
1.2 Mathematicalnotation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2 Therichnessscale 4
2.1 Roadmap . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
2.2 Derivingtherichnessscale . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
2.3 Understandingtherichnessscale. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
2.3.1 Weightsupdatetoalignwiththeirinput. . . . . . . . . . . . . . . . . . . . . 11
2.3.2 Weightalignmentdoesnotmagnifygradients.. . . . . . . . . . . . . . . . . 11
2.3.3 Smallinitialoutputsarenecessaryforrepresentationlearning. . . . . . . . 12
2.3.4 Standardparameterizationyieldsunstabletraining. . . . . . . . . . . . . . . 12
2.3.5 Modelstrainlazilyifandonlyiftheyarelinearized. . . . . . . . . . . . . . 13
2.3.6 Donecorrectly,modelrescalingemulatestrainingatanyrichness. . . . . . 14
2.3.7 Ourconclusionsempiricallyholdforpracticalarchitectures. . . . . . . . . . 15
2.3.8 Gradientmultiplierscanandshouldbeusedoverlayerwiselearningrates. . 15
3 Conclusions 16
A Linearizationinwidelinearmodels 18
B Experimentaldetails 21
∗dkarkada@berkeley.edu
4202
rpA
03
]GL.sc[
1v91791.4042:viXra1 Introduction
Themoderneraofmachinelearningischaracterizedprimarilybytheuseoflargemodels.Inpractice,
thesemodelsarebothdeep(i.e.,consistingofmanylayerstransformingthedatainseries)andwide
(i.e.,havinghiddenrepresentationswithhigherdimensionthanthedataitself.)Forexample,the
mostperformantWideResNetlearningCIFAR10consistsof28layers,andjustthesecondhidden
featuremaphasmorethan50timesasmanyfeaturesastheoriginalinput(Zagoruykoetal.2016).
Suchalongseriesofhigh-dimensionaltransformationscaninduceundesirablebehaviorsthathave
nolow-dimensionalanalog,andwemustbecarefultoavoidthem.
Tounderstandthis,considerthattrainingdeepnetworksconsistsofalternatingtwocomplemen-
taryprocesses: afeedforwardinferenceandabackpropagatingupdate. Wewanttoensurethat
these processes remain well-behaved throughout training: feedforward outputs should evolve
appreciablytowardsthelabelsinfinitetime,andbackpropagationshouldinduceupdatesinthe
hiddenrepresentationsthatallowoptimizationtoproceedstablywithoutstallingorexploding.
Howmightwechooseourmodelhyperparameterstoensurethesetwodesiderata?Thistutorial
aimstotransparentlyanswerthisquestion.
Wewillfindthatcodifyingthesedesiderataasquantitativetrainingcriterianaturallyconstrains
ourchoiceofhyperparameters.Furthermore,itisnottoodifficulttosolvethisconstraintproblem,
and in the end we will be left with only one degree of freedom: the size of the updates to the
hiddenrepresentations.Ifwechoosetheupdatestobesmall,werecoverthelazykernelregime,in
whichthemodelislinearizedinitsparametersthroughouttrainingandthehiddenrepresentations
changenegligibly.Iftheupdatesarelarge,werecovertherichµPlimit,inwhichthemodellearns
nontrivial(andoftenbetter-generalizing)features.WedepictthisstructureinFigure1.
Weemphasizethatthisone-dimensionalrichnessscalefollowsnaturallyanduniquelyfromenforcing
ourtrainingcriteria.Inaddition,thoughweanalyzeatoymodel(namely,a3-layerlinearnetwork),
ourderivationfaithfullycapturestheessenceoftherigorousproofsintheliterature.Ontheway,we
willuncoverandjustifyseveralkeyintuitionsabouttrainingwidenetworkswithgradientdescent:
1. Achievingstable,nontrivial,andmaximaltrainingrequirescontrollingtherelativesizesof
thebackpropagatinggradientandthefeedforwardsignal.
2. Lazytrainingcorrespondstoalinearizedkernelregime;representationlearning(i.e.,non-
negligiblefeatureevolution)necessarilydoesnot.
3. Weightsupdatetoalignwithlearnedrepresentations.
4. Smalloutputsatinitializationarenecessarytoachieverichfeature-learningbehavior.
The richness scale offers a useful probe that both theorists and empiricists may tune in order
tostudythephasetransitionbetweenthelazykernelregimeandtherealisticfeature-learning
regime.Inparticular,itmayenablesystematicstudyoftheperformancegapbetweenNTKlearners
andpracticalneuralnetworks.(Afterall,widenetworksintherichregimearelikelytodescribe
practicalneuralnetworksmoreaptlythantheNTKpreciselybecausetheylearnfeaturesevenin
thethermodynamiclimit.) Infact,understandingtherichregimeaswellasweunderstandthe
kernelregimeisarguablythe majoropenproblemindevelopingascientifictheoryofpractical
deeplearning.Ourhopehereistoelucidatetheideasunderlyingthislineofresearch.
2Figure1:Forwell-behavedsufficiently-widemodels,trainingbehaviorischaracterizedbyasingle
richness hyperparameterr prescribinghowthesizeofthehiddenrepresentationupdates∥∆h∥
scaleswithmodelwidthn. Atfinitewidth,modelbehaviorchangessmoothlybetweentheNTK
endpointr=0andtheµPendpointr=1/2,butinthethermodynamiclimit(n→∞)thereisa
discontinuousphasetransitionseparatingrichµPbehaviorfromlazyr<1/2behavior.
1.1 Relatedwork
ThemainresultspresentedinthisworkarestatedandprovedrigorouslyinYangandHu(2021).
Weshowherethatsimplescalingargumentscanbeusedtoarriveatthesameconclusionsnon-
rigorously.WhatwecalltherichnessscaleisdenotedUP intheirFigure5,althoughourrichness
r
hyperparameterrdiffersfromtheirscalingexponentrinpolarity(i.e.,theydefinersuchthatr =0
isµPandr =1/2isNTK).Inaddition,theyrelaxourmaximalitycriterionandallowbothlayerwise
learningratesandgradientmultipliers,arrivingatamuchlargerspaceofpossiblehyperparameter
scalings.However,asweargueinSection2.1andSection2.3.8,wedon’twinmuchbyconsidering
thisgeneralcase,sowerestrictourattentionforconceptualclarity.
ThistutorialborrowsfromandextendsthederivationpresentedinYang,Simon,etal.(2023),which
emphasizescontrollingthespectralnormofweightmatricesinordertoachieveµPtraining. In
contrast,weavoidreferencingparticularmatrixnorms,insteadfocusingourattentiononthesize
oftherepresentationsandgradients;thisallowsustotreatbothfeaturelearningandlazytraining.
Asaresult,wedisambiguatethecriteriathatarenecessaryforwell-behavedtrainingfromthose
thatyieldfeature-learningbehavior. Inaddition,ourframeworkshowsthatµPisnecessary for
maximal,stable,richtraining,whiletheyfocusondemonstratingsufficiency.Finally,weeschew
theirlayerwiselearningratesinfavorofgradientmultipliers,therebyminimizingunnecessary
couplingbetweenthemodelandtheoptimizer.
The lazy NTK regime was proposed by Jacot et al. (2018) and has since been well-studied. For
example, Arora et al. (2019) and Yang (2020) provide the NTK for architectures beyond MLPs;
worksincludingAllen-Zhuetal.(2019)andDuetal.(2019)provideconvergenceguarantees;and
workssuchasBordelon,Canatar,etal.(2020)andSimon,Dickens,etal.(2021)provideclosed-form
estimatesofgeneralizationerrorintermsoftheNTKeigensystem.Ontheotherhand,workssuch
asChizatetal.(2019),Geigeretal.(2020),andBordelonandPehlevan(2023)analyzedwidemodels
awayfromthelazyregimeusinga“modelrescaling”approach.InSection2.3.6wedemonstrate
thatourframeworkaccommodatesthismethodofachievingrichtraining,althoughwearguethat
3modelrescalingisunnatural.Wedemonstratethisbyusingourframeworktorevealtheunderlying
causeoftheundesiredbehaviorsreportedinthoseworks.
RigorousproofsregardingthelinearizationofmodelswithNTKparameterizationwereprovidedin
Lee,Xiao,etal.(2019),Huangetal.(2020),andLiuetal.(2020).Toourknowledge,thenonrigorous
derivationweprovideinAppendixAisthefirsttoestimatethechangeinthetangentkernelasa
functionoftrainingrichness.
TheexperimentsweperformtoprovideempiricalevidenceareinspiredbythoseinYang,Simon,
etal.2023,Vyasetal.2024,andShanetal.2021.
1.2 Mathematicalnotation
Weuselowercaseboldfaceforvectorsanduppercaseboldfaceformatrices. Weuse∥a∥forthe
Euclideanvectornorm,a⊤bfortheEuclideaninnerproduct,anda⊗bfortheouterproduct.We
usesubscriptstoindexthelayersandparenthesizedsuperscriptstodenotetensorindices(e.g.,
W(ij) refers to the i,j component of the ℓth weight matrix). We use the Einstein summation
ℓ
convention,underwhichrepeatedtensorindicesdenoteasumalongthataxis(e.g.,a(i)b(i) =a⊤b
buta(i)b(j) =a⊗b).Weuse∆todenotechangesacrossasingleoptimizationstep(often,between
initialization and the first update). We say a matrix M is “aligned” with a vector v if ∥Mvˆ∥
typicallydominates∥Muˆ∥forsomerandomisotropicunitvectoruˆ.
Throughoutthistutorial,weconsiderhowcertainscalarsasymptoticallyscalewiththenetwork
width. For conciseness, we use binary relation symbols instead of Bachmann-Landau (big-O)
notationtodenoteasymptoticscalings. Specifically,weuse∼tomean“scaleswith”,i.e.,a ∼ b
meansa = Θ(b);a ≳ bmeansa = Ω(b);a ≲ bmeansa = O(b);a ≫ bmeansa = ω(b);and
a≪bmeansa=o(b).
2 The richness scale
2.1 Roadmap
Weareinterestedintrainingwideneuralnetworksonasupervisedlearningtask.Tostudythis,we
willanalyzea3-layerlinearmodel.Thistoymodelisrealisticenoughtocapturethesalientaspects
ofsignalpropagationinpracticalneuralnetworks,yetissimpleenoughthattheoreticalanalysisis
straightforward.Theargumentswewilluseinouranalysisfaithfullycapturetheessenceofthe
rigorousproofsintheliterature.Inaddition,weprovideempiricalevidencethattheconclusions
wedrawareapplicabletopracticalnetworksinSection2.3.7.
Asthenetworkwidthbecomeslarge,itbecomesimportanttocontrolthesizeofthegradientrelative
tothesizeofthefeedforwardrepresentations.Anticipatingthis,wewillfactortheweightmatrices
intoafixedcoefficientg(ascalargradientmultiplier)andatrainablepartW (theremainingmatrix).
InSection2.3.8,weshowthatthisapproachisequivalenttosimplyassigningeachweightmatrix
itsownlearningrate.
43-layerlinearmodel.
Wewilltrainthemodel
def
h (x) = g W g W g W x
3 3 3 2 2 1 1
bygradientdescentwithconstantΘ(1)learningrateonastandardlossfunctionL(y,h (x)).
3
Wewilluseℓtoindexthelayers,1≤ℓ≤3.SeeFigure2.
TheW areweightmatriceswhoseelementsarethetrainableparameters.WeuseGaussian
ℓ
initializationwithscaleσ (i.e.,W(ij) drawnfromN(0,σ2)). EachW pairswithafixed
ℓ ℓ ℓ ℓ
gradientmultiplierg :increasingg whilekeepingg σ fixedincreasesthesizeofthegradient
ℓ ℓ ℓ ℓ
receivedbyW whilemaintainingthesizeofthefeedforwardsignal.
ℓ
Forconvenience,weadditionallydefinetheℓth-layerrepresentationsrecursivelyas
def
h (x) = g W h (x) withbasecase h (x)=x. (1)
ℓ ℓ ℓ ℓ−1 0
Formuchofouranalysisweconsiderasinglefixedx,soweabbreviateh (x)ash .Define
ℓ ℓ
def
n = dimh
ℓ ℓ
tobethedimensionofarepresentation. Thelearningtaskspecifiestheinputandoutput
dimensions,n andn . Wewilltakeawidenetworklimitwithasinglewidthscale,n ∼
0 3
n ∼n ≫n ∼n ∼1,butforclaritywewilldistinguishthen inourderivations.
1 2 0 3 ℓ
Nowwecanjustifyneedingthreelayers:aswe’lllatersee,thesignalpropagationproperties
ofalayerdependonthelayer’sshape(shape = fanin/fanout),andourmodelcantreat
thethreepossibilitiesshape≪1,shape∼1,andshape≫1.
Equation(1)prescribesthefeedforwardsignalonsomechoseninputx.Howdoesthissignalchange
afteragradientdescentstep?Afterupdatingtheweightsby∆W ,thenewrepresentationsare
ℓ
h +∆h =g ((W +∆W )(h +∆h )).
ℓ ℓ ℓ ℓ ℓ ℓ−1 ℓ−1
Subtractingtheoriginalrepresentation,wefindthattherepresentationupdateis
∆h =g ∆W h +g W ∆h +g ∆W ∆h . (2)
ℓ ℓ ℓ ℓ−1 ℓ ℓ ℓ−1 ℓ ℓ ℓ−1
| {z } | {z } | {z }
layer passthrough interaction
Forstable, well-behavedtraining, wewouldliketocontroltheseupdates. Inlieuofthis, letus
understandthetermsinvolved.Thelayercontributionisinducedbytheupdatetothecurrentlayer’s
weights. The passthrough contribution is induced by the update to the previous representation
passingthroughtheoldweights.Theinteractioncontributioncapturestheinteractionbetweenthe
layerupdateandthepreviousrepresentationupdate.
Wenowpositthattrainingwillproceedstablyandquicklyifourmodelsatisfiesthreekeycriteria.
Thesecriteriaunderpinourderivationofthetrainingregimes,soit’sworthunderstandingand
rememberingthem.
5Figure2:Signalpropagationdiagramforourwide3-layerlinearmodel.Thisdiagramvisuallydepicts
thelogicalflowofourmainderivation:weanalyzethefirstforwardpass,thefirstbackwardpass,
andthesecondforwardpasstoenforceourtrainingcriteria(showninpink)andconstrainourinitial
ninedegreesoffreedom(showninblue).Wedepicttheforwardpasssignalsflowingrighttoleftto
matchtheconventionformatrixmultiplication.
Criteriaforwell-behavedtraining.
1. NontrivialityCriterion.
√
∥∆h ∥∼ n (NTC)
3 3
Aftereverygradientstep,theoutputsshouldupdatewithasizecomparabletotheerror
signal.Thisensuresthatthelossdecreasesatawidth-independentrate.
2. UsefulUpdateCriterion.
(cid:12) (cid:12)
(cid:12) ∂L ⊤ (cid:12)
(cid:12) ∆h (cid:12)∼1 forℓ≥1 (UUC)
(cid:12)∂h ℓ(cid:12)
(cid:12) ℓ (cid:12)
Eachrepresentationupdateshouldcontributetooptimizingtheloss.
3. MaximalityCriterion.
∥g ∆W h ∥∼∥∆h ∥ (MAX)
ℓ ℓ ℓ−1 ℓ
Alayer’sweightupdateshouldcontributenon-negligiblytothefollowingrepresentation
update(i.e.,thelayercontributionshouldnotbedominated,c.f.Equation(2)).
6TheNTCisasingleconstraintthatappliestothelastlayer.TheUUCprovidesoneconstraintper
layer,andisinstrumentalinguaranteeingthatthegradientsareappropriately-sized. TheMAX
providesoneconstraintperlayer, althoughitistriviallysatisfiedintheread-inlayersincethe
inputs are fixed. Although the MAX is not strictly necessary for stable training, it is desirable
nonethelesssinceitprevents“dudlayers”(i.e.,effectivelyfrozenlayers.)
Takentogether,theseprovidesixconstraintsforour3-layermodel.Letusnowcountthenumber
ofdegreesoffreedomwehaveinourinitializationscheme.Wearefreetochoosethesixhyperpa-
rametersg andσ .Itturnsoutthatwewillhaveachoiceinthesizeoftherepresentationupdates,
ℓ ℓ
∥∆h ∥. (Theinitialrepresentationsizes,∥h ∥,aredeterminedbyourchoiceofg andσ .) This
ℓ ℓ ℓ ℓ
totalsninedegreesoffreedom. Satisfyingthesixconstraintsleavesthreeremainingdegreesof
freedom.Wewillusetwoofthosedegreesoffreedomtofixtheinitial∥h ∥and∥h ∥sothatthe
2 1
activationsareΘ(1).Theremainingdegreeoffreedomwillcontroltherichnessoftraining,i.e.,the
degreetowhichthemodeltrainsinthekernelregimeorthefeaturelearningregime.1
Toresolvethisconstraintproblem,wewillconsidertrainingourmodelonasingletrainingsample.
Then,wewillmanuallyperformforwardandbackwardpassesuntilwecanapplyallourcriteriato
solveforthehyperparameters.WevisuallydepictthelogicalstructureofourderivationinFigure2.
2.2 Derivingtherichnessscale
Webeginbyenforcingthatthehiddenactivationsh(i) areinitiallyΘ(1).We’llassumetheelements
ℓ
of the input typically have size 1: |x(i)| ∼ 1. By the forward pass equation, Equation (1), we
know h(i) = g w⊤x, where w is the ith row of W . Since w is a Gaussian vector, we use a
1 1 1
√
central-limit-styleargumenttoclaim|h(i)|∼g σ n ∼! 1.Applyingthesamereasoningtothe
1 1 1 0
otherhiddenrepresentation,weobtainthefollowingtwoconstraintequations:
√ √
g σ n ∼! 1 and g σ n ∼! 1. (3)
1 1 0 2 2 1
Ifthesefeedforwardconstraintsaresatisfied,thenduringthefirstforwardpasswehave∥h ∥∼
√ √ 1
n and∥h ∥ ∼ n asdesired. Notethatwedonothavethefreedomtochoosetheoutput
1 2 2
activationstobeΘ(1)withoutrelinquishingthefreedomtochoosebetweenlazyandrichtraining.
ThisisbecausetheoutputlayermustsatisfytheNTC,anextraconstraintfromwhichthehidden
layersarefree.
Nowwewillcalculatewhathappensduringthefirstbackwardspass.Wewillassumethatthelabels
areΘ(1).Aslongasthefirstoutputdoesn’tblowupwithwidth,2theerrorsignalwillhavesize
√
∥y−h ∥∼ n . ThisobservationjustifiestheNTC:wewantouroutputtoupdatewithasize
3 3
comparabletotheerrorsignal.
Now,letusexaminetheUUCforthelastlayer.First,letusassume
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)∂∂ hL ℓ⊤ ∆h ℓ(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)∼(cid:13) (cid:13) (cid:13) (cid:13)∂∂ hL ℓ(cid:13) (cid:13) (cid:13) (cid:13)∥∆h ℓ∥, (4)
1Itisnothardtoextendthisconstraint-countingargumenttomodelsofarbitrarydepth.
2WewillgobackandcheckthisinSection2.3afterwesolveforg3andσ3.
7whichamountstoanassumptionthatthelossderivativeandtherepresentationupdatearealigned.
(This is a natural assumption in lieu of the fact that gradient descent implements this kind of
alignment in parameter space.) Going forward, any time we invoke the UUC we will use this
alignmentassumption.Now,combiningtheUUCforlayer3withtheNTC,weimmediatelyseethat
(cid:13) (cid:13)
(cid:13) ∂L (cid:13) 1
(cid:13) (cid:13)∼ √ .
(cid:13)∂h (cid:13) n
3 3
Backpropagatingtothepreviousrepresentation,weobtain
√
∂∂ hL
(j)
= ∂∂ hL (i)∂∂ hh (( 3 ji)
)
∼ √1
n
3(g 3σ 3) sothat (cid:13) (cid:13) (cid:13) (cid:13)∂∂ hL 2(cid:13) (cid:13) (cid:13) (cid:13)∼ n √2 ng 3 3σ 3 ∼! ∥∆1
h
2∥, (5)
2 3 2
.
whereweusedEquation(1)tocalculate
∂h(i) ∂h(j)
andthenenforcedtheUUCinthelastscaling.
3 2
Backpropagatingfurther,weobtain
√
(cid:13) (cid:13)
(cid:13) (cid:13) ∂L (cid:13) (cid:13)∼ n 1g 2σ 2 ∼! 1 . (6)
(cid:13)∂h (cid:13) ∥∆h ∥ ∥∆h ∥
1 2 1
Butherewecanpluginafeedforwardconstraint(Equation(3))toobtainourfirstmainresult:
∥∆h ∥∼∥∆h ∥. (7)
1 2
Theupdatestothehiddenrepresentationsmustscalewitheachother.Interestingly,thisrelation
doesnotreferencetherepresentationdimension,suggestingageometricasymmetryifn ̸=n .
1 2
Asmentionedbefore,wepresentlyimaginen =n sowemayneglectthisasymmetry.
1 2
Summarizing,wethusfar:a)stipulatedthatthehiddenactivationsareΘ(1),expendingtwoofour
degreesoffreedomupfront;b)usedtheNTCalongwiththeUUCatℓ = 2andℓ = 3toobtain
Equation(5),whichwewilluselater;andc)usedthepreviousresultsandUUCatℓ=1toconclude
∥∆h ∥∼∥∆h ∥.Wearenowpreparedtoanalyzehowtheweightupdatesinfluencethesecond
1 2
forwardpass.
WeshowinSection2.3.1thatgradientdescentcausesweightstoalignwiththeirinputs,so
∂L ∂L
∆W =−g ⊗h andtherefore g ∆W h =−g2 ∥h ∥2. (8)
ℓ ℓ∂h ℓ−1 ℓ ℓ ℓ−1 ℓ∂h ℓ−1
ℓ ℓ
We’llusethisexpressionforthelayercontributionasweproceedthroughthesecondforwardpass
tosatisfytheMAX.Inthefirstlayer,thepassthroughandinteractioncontributionsarezerosince
∆x=0.ThustheMAXistriviallysatisfiedandweobtain
1
∥∆h ∥=∥g ∆W x∥∼g2 n
1 1 1 1∥∆h ∥ 0
1
√
bycombiningEquation(8)withtheUUCandusingourpreviousassumptionthat∥x∥∼ n .We
0
canfinallysolveforthefirstlayerhyperparametersbyusingthefeedforwardconstraintEquation(3):
∥∆h ∥ 1
g ∼ √ 1 and σ ∼ . (9)
1 n 1 ∥∆h ∥
0 1
8WenowfeedforwardtothesecondlayerandapplytheMAX:
1
∥g ∆W h ∥∼g2 n ∼! ∥∆h ∥.
2 2 1 2∥∆h ∥ 1 2
2
Solvingforthesecondlayerhyperparameters,againusingEquation(3),weobtain
∥∆h ∥ 1
g ∼ √ 2 and σ ∼ . (10)
2 n 2 ∥∆h ∥
1 2
Wecouldimmediatelyproceedtothethirdlayer, butitisillustrativetostudythesecondlayer
passthroughcontribution.Itssizeis
√ r n
∥g W ∆h ∥∼ n g σ ∥∆h ∥∼ 2∥∆h ∥≲∥∆h ∥.
2 2 1 2 2 2 1 n 1 2
1
whereweusedthefactthatthesizeofthepassthroughcontributionlowerboundsthesizeofthe
representationupdateinthelastscaling.OurpreviousresultEquation(7)thenimmediatelyimplies
thatsincen ∼ n ,thepassthroughcontributioninfacttightlybounds∥∆h ∥. Ingeneral,for
2 1 2
layerswhosefaninscaleswiththeirfanout, oneseesthattherepresentationupdatereceives
equal-sizecontributionsfromboththepassthroughandthelayer.
Proceedingtothethirdlayerandrepeatingtheanalysis,
1 1 √
∥g ∆W h ∥∼g2 n ∼g2√ n ∼! ∥∆h ∥∼ n
3 3 2 3∥∆h ∥ 2 3 n 2 3 3
3 3
Wecannowsolveforg .Butnotethatwedon’thaveafeedforwardconstraintEquation(3)forthe
3
thirdlayer.Instead,wewillgobackandfinallyuseEquation(5)tosolveforσ :
3
r
n 1
g ∼ 3 and σ ∼ . (11)
3 n 3 ∥∆h ∥
2 2
We’vefinishedsolvingfortheg andσ .Forcompleteness,let’sevaluatethepassthroughcontri-
ℓ ℓ
butioninthefinallayer. Onemightexpectthelayercontributiontodominatethepassthrough
contribution,sinceanalignedprojection3willalmostsurelydominatearandomone.However,this
isnotthecase:thepassthroughisalsoaligned!
(cid:13) !(cid:13)
(cid:13) ∂L 1 (cid:13)
∥g W ∆h ∥∼(cid:13)g W(·j) g W(ij) (cid:13)∼g2n ∼n
3 3 2 (cid:13) (cid:13) 3 3 ∂h( 3i) 3 3 ∥∆h 2∥2 (cid:13) (cid:13) 3 2 3
In words, ∆h is aligned with ∂L/∂h (see Equation (4)), which in turn is aligned with W
2 2 3
(becauseW istheonlymatrixbetweenh andthelosssignal,anditisaprojection).Ingeneral,
3 2
thisalignmentbetweenthedeepesthiddenrepresentationupdateandthereadoutmatrixensures
thatthepassthroughcontributionscaleswiththelayercontributioninthereadoutlayer(despite
thereadoutlayerbeingaprojection).WedemonstratethisempiricallyinFigure6.
Toconcludethederivation,wecompileourresultsinTable1.
3Weuseprojectiontorefertolinearmapsfromveryhighdimensiontolowdimension.
9g σ ∥h ∥ ∥g ∆W h ∥ ∥g W ∆h ∥
ℓ ℓ ℓ ℓ ℓ ℓ−1 ℓ ℓ ℓ−1
∥∆h∥ 1 √
ℓ=1 √ n ∥∆h∥ 0
n ∥∆h∥ 1
0
∥∆h∥ 1 √
ℓ=2 √ n ∥∆h∥ ∥∆h∥
n ∥∆h∥ 2
1
rn 1 n √ n
ℓ=3 3 3 n √3
n ∥∆h∥ ∥∆h∥ 3 n
2 2
Table1: Choosingg andσ accordingtothistablewillresultintrainingthatsatisfiesourthree
ℓ ℓ
trainingcriteria.Thereremainsafreehyperparameter∥∆h∥whichcontrolsthelaziness/richnessof
training.Wealsoreporttherepresentationsizesatinitialization,aswellasthelayerandpassthrough
contributionstothefirstrepresentationupdate.
2.3 Understandingtherichnessscale
Aspromised,wehaveusedalltheconstraintstodeterminethescalingofalltheg andσ interms
ℓ ℓ
of a single degree of freedom, ∥∆h∥. We drop the layer index since by Equation (7) we know
∥∆h ∥and∥∆h ∥scaletogether.Inthisvein,we’llhenceforthstopdistinguishingthedifferent
2 1
n ,insteadabbreviatingn ,n asnandtreatingn ,n as1.
ℓ 1 2 0 3
Whatvaluescan∥∆h∥take?LetusrevisitaclaimmadeatthebeginningofSection2.2:theinitial
outputdoesnotblowupwiththewidth.Thismeans
1
∥h ∥∼ ≲1 implying ∥∆h∥≳1.
3 ∥∆h∥
Ontheotherhand,wedon’twanttherepresentationupdatestobetoolargeeither.Areasonable
√
criterionis∥∆h ∥≲∥h ∥∼ n(seeSection2.3.2).Together,thesedemarcatetherichnessscale,
2 2
whichdefinesacontinuousscaleofpossiblechoicesfortherichness(orlaziness)oftraining:
∥∆h∥∼nr wheretherichnessrsatisfies 0≤r ≤1/2. (12)
AswedepictinFigure1,theendpointsoftherichnessscalecorrespondtowell-studiedtraining
regimes.Specifically,ifr =0thentheupdatesareminimalandwerecoverthelazyNTKtraining
regime;ontheotherhand,ifr =1/2thentheupdatesaremaximalandwerecoverthefeature-
learningµPregime.Noticethatinthe“thermodynamiclimit”n→∞,weobserveadiscontinuous
phasetransitionin∥∆h∥/∥h∥,whichvanisheseverywherealongtherichnessscaleexceptatµP.
Weconcludethatatinfinitewidth,onlyµPavoidsthelazyregimewherehiddenrepresentations
evolvenegligibly.
10We now make several comments which explore useful intuition, address loose ends from our
derivation,provideempiricalevidence,andconnectwithotherresultsintheliterature.
2.3.1 Weightsupdatetoalignwiththeirinput.
Letussolvefortheweightupdate∆W usinggradientdescentwithlearningrateη =1.
ℓ
∂L ∂L ∂h(k) ∂L (cid:16) (cid:17)
∆W(ij) =− =− ℓ =− δ(ik)g h(j)
ℓ ∂W(ij) ∂h(k)∂W(ij) ∂h(k) ℓ ℓ−1
ℓ ℓ ℓ ℓ
∂L
=−g h(j)
ℓ ∂h(i) ℓ−1
ℓ
wheretheKroneckerdeltaδ(ik) indicatesthatthederivative ∂h /∂W isasparsetensor,soits
ℓ ℓ
contractionwith ∂L/∂h resultsinarank-1update.Asexpected,theupdatesizeismodulatedby
ℓ
thegradientmultiplier.Toillustratethattheupdateisalignedwiththeinputh ,wecalculate
ℓ−1
thelayercontribution,yieldingtheexpressionweusedinEquation(8):
(cid:18) (cid:19)
∂L ∂L
g ∆W h =g −g ⊗h h =−g2 ∥h ∥2.
ℓ ℓ ℓ−1 ℓ ℓ∂h ℓ−1 ℓ−1 ℓ∂h ℓ−1
ℓ ℓ
2.3.2 Weightalignmentdoesnotmagnifygradients.
Wemustnowask:aretheconclusionsofourderivationinvalidatedassoonastheweightmatrices
becomecorrelatedwiththeinputs?Inparticular,dothenewlyalignedweightsmagnifythegradients
anddestabilizetraining?Wenowshowthatourupperboundon∥∆h∥preventsexactlythis.First,
let’sexaminethesizeoftheweightupdate
 √
(cid:12) (cid:12)∆W(ij)(cid:12) (cid:12)=(cid:12) (cid:12)
(cid:12)g
∂L
h(j)
(cid:12) (cid:12)
(cid:12)∼
√g ℓh( ℓj −)
1
∼ 11 // nn i if fℓ ℓ= =3
2
(cid:12) ℓ (cid:12) (cid:12) (cid:12) ℓ ∂h( ℓi) ℓ−1(cid:12) (cid:12) n ℓ∥∆h ℓ∥ 1/√
n ifℓ=1
andusethistoenforcetheUUCinlayer2:
(cid:13) (cid:13) (cid:13) (cid:13)∂∂ hL 2(cid:13) (cid:13) (cid:13) (cid:13)∼(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)∂∂ hL (i) (cid:16) g 3(cid:16) W 3(i·)+∆W 3(i·)(cid:17)(cid:17)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)∼√ n√1 n(cid:18) ∥∆1 h∥ + √1 n(cid:19) ∼! ∥∆1 h 2∥.
3
√
Asymptotically, we see that choosing ∥∆h∥ ≲ n prevents the updates from magnifying the
gradient. (Applyingthisreasoningtolayer1yieldsthesameconclusion.) WeconcludethatµP
achievesthelargestpossiblerepresentationupdates;anylarger,andthealignedweightswould
magnifythegradientsignaltoinstability.
Aftertgradientsteps,thescaleofthecumulativeupdateincludesaprefactoroftatmost,andifthe
convergencetimeisΘ(1)w.r.t.width(assuggestedbytheNTC),theconclusionsofourderivation
shouldholdthroughouttraining.However,thisisaheuristicargument,andweemphasizethatour
conclusionsregardingtrainingstabilityarerootedinthebehaviorofthemodelnearinitialization.
Inparticular,oursignalpropagationargumentscannotmakestrongguaranteesaboutconvergence
ratesorthequalityofthefinallearnedrepresentations.
11Figure3: Width-scalingofrepresentationsmatchpredictions. Wereportmeasurementsof
3-layerlinearmodelslearningGaussiandataacrosswidthsandrichnesses. (A)Depictshowwe
measurethescalingexponentofsomescalars,whichwedenotescaling(s).Inthiscase,s=∥∆h ∥;
2
wemeasureitacrosswidthsandfitaline(onlog-scale)whoseslopeisthemeasuredscalingexponent.
Weplottheaverageover20networkinstancesand50trainingsamples. (B)Weverifythatthe
UUCholdsatalllayersandrichnesses.Dottedlinedenotestheoryprediction.(C)Therelativesizes
ofrepresentationupdatesmatchpredictions. Weseethatthehiddenrepresentationsfallonthe
lowerdotteddiagonalaspredicted(lowrichnessyieldssmallupdates).Atinitialization,weseethe
relativesizeoftheoutputupdates(bluetriangles)fallontheupperdiagonal(intherichregime,initial
outputsscaleinverselywithwidth).Afteragradientstep,theoutputsizeshouldmatchtheupdate
size(horizontaldottedline),butourmeasurements(pinktriangles)revealasmalldiscrepancyat
intermediaterichness;thisisduetoourslightlysmallgloballearningrate(seeAppendixBfordetails).
2.3.3 Smallinitialoutputsarenecessaryforrepresentationlearning.
Noticethatsince∥h ∥ ∼ 1/∥∆h∥,inordertoachievefeaturelearninginthen → ∞limit,we
3
mustnecessarilyhavesmalloutputsatinitialization. Conversely,ifwewanttheinitialoutputs
tobeΘ(1),wemustacceptlazytraining.Notethathavingsmalloutputsatinitializationisnota
problem,sinceafterthefirstgradientstep,thereadoutlayerwillalign,andtheoutputswillachieve
thecorrectscale.
2.3.4 Standardparameterizationyieldsunstabletraining.
p
Thedefaultinitializationschemeinmostdeeplearningframeworkssetsg =1andσ ∼ 1/n .
ℓ ℓ ℓ−1
Thisschemedoesnotfallanywhereontherichnessscale.OnecanimagineitsimilartoµP except
thatg istoolargeandg istoosmall. Thissuggeststhatthegradientwillbetoolargeandthe
3 1
outputs will explode after the weights align. Practically, these shortcomings may not become
evidentuntilthenetworkbecomessufficientlywide(seeFigure4).
12Figure4:Trainingoutsidetherichnessscaleyieldsunstabletrainingatlargewidth.Wereport
measurementsofpracticalconvolutionalnetworklearningaminibatchofCIFAR-10.(A)Training
iswell-behavedontherichnessscale(0≤r ≤0.5);outsideintheshadedregions,trainingerror
eitherdivergesorconvergesveryslowly.Thiseffectbecomesmoreprominentaswidthincreases.
(B)Thelossdynamicsofthewidth-1024architecturerevealsthatther >0.5regimeinitiallyhas
reasonably-sizedoutputs, buttrainingisunstable. Ontheotherhand, inther < 0regime, the
initialoutputsblowup.Althoughgradientdescenteventuallycorrectsthis,thecorrectiontimescale
divergeswithwidth.Thehorizontalcross-sectionatthedashedlineistheorangecurveinpanelA.
(C)Here,weretainthesameconvolutionalarchitecturebutusestandardparameterization(i.e.,the
defaultPyTorchinitialization).Atsufficientlylargewidth,trainingdivergesaspredicted.However,
toseethiseffectatpracticalwidths,weusedagloballearningrateη=1(comparedtoη=0.1in
theotherexperiments).Theobservedstabilityatmoderatewidthsandlearningratessuggeststhat
trainingstandardneuralnetworksmaybephenomenologicallysimilartotrainingµPnetworks.See
AppendixBforexperimentaldetails.
2.3.5 Modelstrainlazilyifandonlyiftheyarelinearized.
Somewidenetworksareknowntobewell-approximatedbytheirfirst-orderTaylorapproximation
inparameterspace:
f(x;θ +∆θ)=f(x;θ )+∆θ⊤(∇ f(x;θ ))+ 1 ∆θ⊤(cid:0) ∇2f(x;θ )(cid:1) ∆θ+···
0 0 θ 0 2 θ 0
| {z }
negligibleinthewide+lazylimit.
Suchlinearizedmodelsenjoyseveralnicetheoreticalproperties,notleastofwhichisanequivalence
with(neuraltangent)kernelmachines.Underwhatconditionsdowidenetworksbecomelinearized
kernelmachines?InAppendixAweshowthatif∆θisagradientdescentupdate,thefirst-order
gradienttermscaleslike∆θ⊤(∇ f(x;θ ))∼1,whilethesecond-ordercurvaturetermscaleslike
θ 0
∆θ⊤(cid:0) ∇2f(x;θ )(cid:1) ∆θ ∼
∥∆h∥2
.
θ 0 n
√
Therefore,asn→∞,thecurvaturetermvanishesaslongas∥∆h∥≪ n,i.e.,aslongasweare
notintheµPregime.ThisreflectsthedynamicaldichotomytheoremofYangandHu(2021),which
statesthatthekernelregimeandfeaturelearningaremutuallyexclusive.SeeFigure5forempirical
evidenceofthisscaling.
13Figure5:Modellinearizationmatchespredictions.For3-layerlinearmodelslearningGaussian
dataacrosswidthsandrichnesses,wemeasurethechangeinthegradientacrossthefirstoptimization
step.(A)Thechangeinthegradientdecayswithwidthinthekernelregime.(Here,thesubscriptsin
f andf enumeratetimesteps,notlayers.)(B)ThescalingmatchesthepredictioninEquation(14).
0 1
2.3.6 Donecorrectly,modelrescalingemulatestrainingatanyrichness.
Previous works (Chizat et al. 2019; Geiger et al. 2020; Bordelon and Pehlevan 2023) consider
initializinganetworkintheNTKregimeandtogglingrichtrainingbyrescalingonlythefinal-layer
gradientmultiplierandthegloballearningrate:
optimize L(αf(x),y) withlearningrate η =α−2.
This contrasts our approach of adjusting all the gradient multipliers and initial weight scales
andleavingthegloballearningratearchitecture-independent. However,aswe’llshow,thetwo
approachesareequivalent:onecanexactlyemulatetrainingwithrichnessrbychoosing
α=! n−r.
To see this, let’s implement the global learning rate as gradient multipliers instead, using the
procedurewedescribeinSection2.3.8.Afterthistransformation,wehave
η =1 g =α−1g(NTK) σ =ασ(NTK)
ℓ ℓ ℓ ℓ
sothatthescaleoftheforwardpass,g σ ,isinvariantunderthistransformation.Nowweabsorb
ℓ ℓ
themodelrescalingcoefficientintothemodelarchitecture(i.e.,multiplyingitintog ),givingus
3
g =g(NTK) .ComparingtoTable1,weseethatsettingα=1/∥∆h∥allowstherescaledmodelto
3 3
achievetrainingatanyrichness.(It’snothardtoextendthisargumenttoshowthatonecanstart
withamodelinitializedatanyrichnessr iandsetα=nri−r toachievetrainingwithrichnessr.)
However,wenowemphasizesomeimportanttakeaways.First,therichnessscalemeasureshow
√
∥∆h∥scaleswiththewidthn.Therefore,experimentswhichfixα(orα n)andvarythewidth,as
thoseinGeigeretal.(2020),donotinterrogateasinglerichness,butinsteadunintentionallytraverse
therichnessscalewithincreasingwidth.Second,itisimportanttoapplymodelrescalingtomodels
initializedontherichnessscale;ifonetriestorescalemodelsinthestandardparameterization,
asisdoneinChizatetal.(2019),themodeloutputswillexplodeinthewidelimitandonemust
14resorttohackssuchasmodelcenteringorsymmetrizedinitialization.Third,αcannotbechosen
√
arbitrarilylargeorsmall; onemustensurethat1 ≲ ∥∆h∥ ≲ n. Forexample,asGeigeretal.
(2020)report,ifα≫1(i.e.,r <0)trainingdoesnotconvergeunlessthemodeliscentered,and
√
if α ≪ 1/ n (i.e., r > 1/2) then the model variance explodes. We empirically reproduce this
undesiredbehaviorinFigure4;examiningthetrainingbehaviorformodelsofftherichnessscale
validatesourexplanation.
Insummary, toapplymodelrescaling, oneshouldchooser firstandsetα = n−r, ratherthan
treatingαasawidth-independentnumericalconstant.
2.3.7 Ourconclusionsempiricallyholdforpracticalarchitectures.
Ourtoymodelisunrealisticonseveralcounts. Wetrainonasinglesample,whereasminibatch
SGDisstandard;weneglectnonlinearactivationfunctions;andweuseafully-connectednetwork
despiteitsinferiorinductivebias(comparedto,e.g.,convolutionalnetworksforvisiontasks).In
Figure4andFigure7wepresentempiricalevidencethatourconclusionsholdforrealistictraining
setups.Inparticular,wetrainaMyrtle-5CNN(depth-5,ReLUactivations,globalaveragepooling,
SGD+momentumwithminibatchsize32;describedinShankaretal.2020)tolearnCIFAR10.
2.3.8 Gradientmultiplierscanandshouldbeusedoverlayerwiselearningrates.
Gradientdescentprescribesthattheweightsupdateviaalinearcouplingtothegradientsignal.To
controlthesizeoftheupdate,onecaneitherchangethecouplingstrength(i.e.,thelearningrate)or
thesizeofthegradientsignal.Here,weshowthatthetwoapproachesarefunctionallyequivalent.
Considerupdatingalayer4withoutagradientmultiplierbutwithalayerwiselearningrateη :
ℓ
∂L
h =W˜ h and ∆h =∆W˜ h =−η h .
ℓ ℓ ℓ−1 ℓ ℓ ℓ−1 ℓ ∂W˜ ℓ−1
ℓ
Wenowshowthatwecanalwayschooseagradientmultipliersuchthatwemaysetη =1globally
butnonethelessachievethesameforwardpassandthesameupdate.First,wefactortheweights
intoafixedcoefficientandatrainableremainder,W˜ = g W,sotheforwardpassisunaffected.
ℓ
Settingη =1,thelayercontributionis
∂L
∆h =g ∆W h =−g h
ℓ ℓ ℓ ℓ−1 ℓ∂W ℓ−1
ℓ
∂L ∂W˜ ∂L
=−g ℓh =−g2 h
ℓ ∂W˜ ∂W ℓ−1 ℓ∂W˜ ℓ−1
ℓ ℓ ℓ
Comparingtotheupdateequationwithlayerwiselearningrateη ,weseethatwecanachieve
ℓ
thesameeffectwhenwesetg2 =η .Thuswelosenothingbychoosingtoimplementlayerwise
ℓ ℓ
learning rates as gradient multipliers and simply setting a global learning rate η = 1. In fact,
wearguethatfromasoftwaredesignperspective,it’sdesirabletoisolatearchitecture-dependent
constantsfromtheoptimizerasmuchaspossible.
4Forsimplicitywefreezetheotherlayerssothattherepresentationupdateisjustthelayercontribution,butitiseasyto
extendtheargumentusinginductiontotreatthegeneralcasewithnofrozenlayers.
153 Conclusions
We’veshownhowtousescalingargumentstoderivetherichnessscalefortrainingbehavior;we’ve
discussedseveralconsequencesandrelatedperspectives;and,we’veprovidedempiricalevidence
supportingournonrigorousderivations.Weconcludewithafewfinalcomments.
Theinfinite-widthNTKregimeiswell-studied,andmanyquestionsregardingconvergenceand
generalizationhavebeenanswered. However,understandingtheemergenceoffeaturelearning
remainsanopenproblem.Priorworksperturbativelycomputedfinite-widthcorrectionstotheNTK
andconjecturedthatthesecorrectionswerethesourceoffeaturelearninginpracticalnetworks.
Wenowknowthatrichnessisanotherdistinctmechanismforneuralnetworkstoachievefeature
learning. Aswe’veseen,richerarchitectureshavelargergradientmultipliers,sorichtrainingis
likelyrelatedtotheempiricalphenomenaassociatedwithlargelearningrates.Disentanglingthese
effectsisaripeareaforfutureresearch.
Acoreideainourderivationisthattherepresentationsh playakeyrole.It’snotenoughtostudy
ℓ
theweightmatricesalone;intheend,it’sthehiddenrepresentationsthatformthebridgebetween
theinputdataandthedesiredoutput.Inoursimplederivation,weonlycontrolledthesizeofh
ℓ
and∆h ,buttobetterunderstandtherichregime,itwillbeimportanttoassessandcontrolthe
ℓ
qualityofthelearnedrepresentations.
Aprimarydifficultyincharacterizingthelearnedrepresentationsisthatthestructureofnatural
data distributions is poorly understood. To see why understanding this structure is necessary,
considerthatlearnedrepresentationstypicallyimprovegeneralization(Lee,Schoenholz,etal.2020),
andcharacterizinggeneralizationperformancetypicallyrequires“omniscient”knowledgeofthe
interactionbetweenthedatadistributionandthemodel’sinductivebias(Bordelon,Canatar,etal.
2020;Simon,Karkada,etal.2023).Studyingtheinfinite-widthrichregimemayshedlightonthis
data-modelinteractioninpracticalneuralnetworks,therebyadvancingthescienceofdeeplearning.
Acknowledgements. IamgratefultoJamieSimonforinspiration,guidance,andhelpfulcom-
ments;LibinZhuandJonathanShiforhelpfuldiscussion;myadvisorMichaelDeWeeseforuseful
discussionsandsupport;andJaredHrebenarforloveandencouragement.Thisworkissupported
bytheUCBerkeleyDepartmentofPhysics.
References
Allen-Zhu,Zeyuan,YuanzhiLi,andZhaoSong(2019). “Aconvergencetheoryfordeeplearningvia
over-parameterization”. In:Internationalconferenceonmachinelearning. PMLR, pp.242–252
(cit.onp.3).
Arora,Sanjeevetal.(2019). “Onexactcomputationwithaninfinitelywideneuralnet”. In:Advances
inneuralinformationprocessingsystems32 (cit.onp.3).
Bordelon,Blake,AbdulkadirCanatar,andCengizPehlevan(2020). “Spectrumdependentlearning
curvesinkernelregressionandwideneuralnetworks”. In:InternationalConferenceonMachine
Learning. PMLR, pp.1024–1034 (cit.onpp.3,16).
Bordelon, Blake and Cengiz Pehlevan (2023). “Self-consistent dynamical field theory of kernel
evolutioninwideneuralnetworks”. In:JournalofStatisticalMechanics:TheoryandExperiment
2023.11,p.114009 (cit.onpp.3,14).
16Chizat, Lenaic, Edouard Oyallon, and Francis Bach (2019). “On lazy training in differentiable
programming”. In:Advancesinneuralinformationprocessingsystems32 (cit.onpp.3,14).
Du, Simon et al. (2019). “Gradient descent finds global minima of deep neural networks”. In:
Internationalconferenceonmachinelearning. PMLR, pp.1675–1685 (cit.onp.3).
Geiger,Marioetal.(2020). “Disentanglingfeatureandlazytrainingindeepneuralnetworks”. In:
JournalofStatisticalMechanics:TheoryandExperiment 2020.11,p.113301 (cit.onpp.3,14,15).
Huang,JiaoyangandHorng-TzerYau(2020).“Dynamicsofdeepneuralnetworksandneuraltangent
hierarchy”. In:Internationalconferenceonmachinelearning. PMLR, pp.4542–4551 (cit.onp.4).
Jacot,Arthur,FranckGabriel,andClémentHongler(2018). “Neuraltangentkernel:Convergence
andgeneralizationinneuralnetworks”. In:Advancesinneuralinformationprocessingsystems
31 (cit.onp.3).
Lee,Jaehoon,SamuelSchoenholz,etal.(2020). “Finiteversusinfiniteneuralnetworks:anempirical
study”. In:AdvancesinNeuralInformationProcessingSystems33,pp.15156–15172 (cit.onp.16).
Lee,Jaehoon,LechaoXiao,etal.(2019).“Wideneuralnetworksofanydepthevolveaslinearmodels
undergradientdescent”. In:Advancesinneuralinformationprocessingsystems32 (cit.onp.4).
Liu,Chaoyue,LibinZhu,andMishaBelkin(2020). “Onthelinearityoflargenon-linearmodels:
whenandwhythetangentkernelisconstant”. In:AdvancesinNeuralInformationProcessing
Systems33,pp.15954–15964 (cit.onp.4).
Shan,HaozheandBlakeBordelon(2021). “Atheoryofneuraltangentkernelalignmentandits
influenceontraining”. In:arXivpreprintarXiv:2105.14301 (cit.onp.4).
Shankar,Vaishaaletal.(2020). “Neuralkernelswithouttangents”. In:Internationalconferenceon
machinelearning. PMLR, pp.8614–8623 (cit.onpp.15,21).
Simon,JamesB,MadelineDickens,etal.(2021). “Theeigenlearningframework:Aconservationlaw
perspectiveonkernelregressionandwideneuralnetworks”. In:arXivpreprintarXiv:2110.03922
(cit.onp.3).
Simon, James B, Dhruva Karkada, et al. (2023). “More is better in modern machine learning:
wheninfiniteoverparameterizationisoptimalandoverfittingisobligatory”. In:arXivpreprint
arXiv:2311.14646 (cit.onp.16).
Vyas,Nikhiletal.(2024).“Feature-learningnetworksareconsistentacrosswidthsatrealisticscales”.
In:AdvancesinNeuralInformationProcessingSystems36 (cit.onp.4).
Yang,Greg(2020). “Tensorprogramsii:Neuraltangentkernelforanyarchitecture”. In:arXiv
preprintarXiv:2006.14548 (cit.onp.3).
Yang,GregandEdwardJHu(2021). “Tensorprogramsiv:Featurelearningininfinite-widthneural
networks”. In:InternationalConferenceonMachineLearning. PMLR, pp.11727–11737 (cit.on
pp.3,13).
Yang,Greg,JamesBSimon,andJeremyBernstein(2023). “Aspectralconditionforfeaturelearning”.
In:arXivpreprintarXiv:2310.17813 (cit.onpp.3,4).
Zagoruyko,SergeyandNikosKomodakis(2016). “Wideresidualnetworks”. In:arXivpreprint
arXiv:1605.07146 (cit.onp.2).
17A Linearization in wide linear models
Infinitely-widenetworkstrainedinthelazyregimeareknowntobeequivalenttokernelizedlinear
models,i.e.,modelsoftheformf(x;w)=w⊤ϕ(x)forsomekernelmappingϕ(·). Foraneural
networkf(x;θ)withmodelparametersθ,thishappenswhenthenetworkislinearized,i.e.,when
the second (and higher) order terms of the Taylor expansion of f(x;θ +∆θ) in ∆θ become
0
negligible:
f(x;θ +∆θ)−f(x;θ )≈∆θ⊤∇ f(x;θ ). (13)
0 0 θ 0
Here,weseetheresidualisakernelizedlinearmodelwithϕ(x)=∇ f(x;θ )beingthekernel
θ 0
mapping.OurgoalhereistoshowthatEquation(13)followsfromlazytrainingintheinfinite-width
limit.Todothis,wewillshowthatlazytrainingcausesthecurvatureofthemodelinparameter
spacetovanishwithrespecttothesizeofthegradient:
∆θ⊤H∆θ ≪∆θ⊤∇ f(x;θ ) where H d =ef ∇2f(x;θ )istheHessianmatrix.
θ 0 θ 0
Underthiscondition,themodelreducestoitsfirst-orderTaylorapproximation,andsoourlearning
problemreducestokernelregressionwiththeaforementionedkernelmapping.Itisimportantto
notethatthesufficientconditionforkernellearningislinearizingf(x;θ)inparameterspace;in
general,itmaystillbenonlinearintheinputs.However,forconvenienceinourpresentanalysis,
wewillalsoassumethatourmodelislinearinourinputs(aswedidpreviously).Todisambiguate
goingforward,wewilluselinearizedtorefertomodelsthathavebecomelinearinparameterspace
duetothelazyinfinite-widthlimit,andlinear torefertomodelswhicharelinearintheinputsdue
tolackingnonlinearactivationfunctions.
Wewillconsideradepth-Lscalarlinearmodel
!
Y
f(x)=g w ⊤ g W g w x
L L ℓ ℓ 1 1
1<ℓ<L
onascalarinputx.Wedefinethehiddenrepresentationsh asbefore.Againwetakeawidelimit,
ℓ
withallhiddenrepresentationshavingequalwidthn. Wemustconsiderdeepermodels(L>3)
because,aswewillsee,thescalingofthecurvaturetermdependsprimarilyonthecouplingbetween
squarelayers,sohavingasinglesquarelayerisinsufficientfortreatingthegeneralcase.However,
wewillalwaysassumeL∼1w.r.t.tothewidenetworklimit.
For convenience, we will analyze the curvature term for the first gradient step only, so that
∆θ ∼∇ f(x;θ ).Ifweassumethattrainingconvergesatawidth-independentrate,thenwemay
θ 0
assumethatalackofcurvatureinthefirststepwillimplyflatnessthroughouttraining.
UsingtheUUCandoursolutionforg forintermediatelayers,weknowthatthegradientw.r.t.an
ℓ
intermediatelayerparameterhassize
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) ∂f (cid:12) (cid:12) ∂f (cid:12) g 1
(cid:12) (cid:12)=(cid:12) g h(j) (cid:12)∼ √ ℓ ∼ .
(cid:12) (cid:12)∂W(ij)(cid:12)
(cid:12)
(cid:12) (cid:12)∂h(i) ℓ ℓ−1(cid:12)
(cid:12)
n∥∆h∥ n
ℓ ℓ
Itiseasytoshowthatthereadinandreadoutderivativesarenottoolarge,sowemayestimatethe
gradienttermbyconsideringtheΘ(n2)intermediate-layerparameters:
∆θ⊤∇ f(x;θ )∼∥∇ f(x;θ )∥2 ∼p n2(1/n)2 ∼1.
θ 0 θ 0
18LetuswritetheHessianforourdeeplinearmodel. Ingeneral,thetwoHessianindicesreferto
parameters at possibly different layers, ℓ and ℓ′. Invoking the symmetry of the Hessian, let us
chooseℓ≥ℓ′.Then
∂2f ∂f ∂h(j) ∂f ∂h(j)
=g ℓ−1 =g ℓ−1g h(q)
∂W(ij)∂W(pq) ℓ ∂h(i)∂W(pq) ℓ ∂h(i) ∂h(p) ℓ′ ℓ′−1
ℓ ℓ′ ℓ ℓ′ ℓ ℓ′
WewillfindthatthescaleofthisHessianelementdependssensitivelyonthe“layerconnection
derivative:”

0 ifℓ=ℓ′
∂h(j) 
ℓ−1 ∼ δ(jp) ifℓ−1=ℓ′
∂h( ℓp ′) 1/√
n otherwise
ℓ′−1
whereδ(··)isaKroneckerdelta.ThefirstcasetellsusthattheHessianelementoftwoparameters
belongingtothesamelayervanishes;thisfollowsfromthelinearityofthemodel.Thesecondcase
tellsusthattheHessianelementoftwoparametersfromadjacentlayersvanishesunlesstheyare
connectedtothesameneuron.Thefinalcasecanbeverifiedbydirectcomputation,andfollows
fromourfeedforwardconstraintEquation(3).
Wearenowreadytocomputethecurvaturetermforonegradientstep:
X ∂f ∂2f ∂f
∆θ⊤H∆θ ∼
∂W(ij)∂W(ij)∂W(pq)∂W(pq)
ℓ,ℓ′ ℓ ℓ ℓ′ ℓ′
∼X g ∂f h(j)
!
g ∂f
∂h( ℓj −)
1g h(q)
!
g ∂f h(q)
!
ℓ ∂h(i) ℓ−1 ℓ ∂h(i) ∂h(p) ℓ′ ℓ′−1 ℓ′ ∂h(p) ℓ′−1
ℓ,ℓ′ ℓ ℓ ℓ′ ℓ′
∼X g ℓ2g ℓ2 ′(cid:13) (cid:13) (cid:13) (cid:13)∂∂ hf ℓ(cid:13) (cid:13) (cid:13) (cid:13)2 ∥h ℓ′−1∥2! h( ℓj −) 1∂ ∂h h( ℓ (j − p) )1 ∂∂ hf (p)!
ℓ,ℓ′ ℓ′ ℓ′
aftercontractingindicesiandq.Althoughthesumisoveralllayerpairsℓ,ℓ′,thereadercanverify
thatthetermscomingfromthereadinandreadoutlayersdonotdominate,sowemayhereforth
assumethatℓandℓ′ indexintermediatelayers. Inthiscase,n = n = n ≡ n,sothefirst
ℓ ℓ′ ℓ′−1
parenthesizedfactorinthelastlinesimplifiesandweobtain
∆θ⊤H∆θ
∼X∥∆h∥2
h(j)
∂h( ℓj −)
1
∂f !
.
n ℓ−1 ∂h(p) ∂h(p)
ℓ,ℓ′ ℓ′ ℓ′
Wenowevaluatetheremainingfactoraccordingtothe“layerconnectionderivative”above:

0 ifℓ=ℓ′
∂h(j)
∂f

h(j) ℓ−1 ∼ h ⊤(∂f/∂h )∼1 ifℓ−1=ℓ′(byUUC)
ℓ−1 ∂h( ℓp ′) ∂h( ℓp ′) 1/ℓ ∥′
∆h∥
ℓ′
otherwise(byUUC)
Pluggingin,wefinallyobtain
∥∆h∥2 (cid:18) L2 (cid:19) ∥∆h∥2
∆θ⊤H∆θ ∼ L+ ∼ ,
n ∥∆h∥ n
19whereinthelastscalingweusedthefactthatL∼1and∥∆h∥≳1.5 Comparingthiscurvature
termtoourgradientterm,weseethatthemodelislinearizedif
∥∆h∥2
lim ≪1.
n→∞ n
√
TheonlypointontherichnessscalewhichavoidsthisconditionisµP,∥∆h∥ ∼ n. Infinitely
widemodelsinitializedanywhereelsetraininthekernelregime.
Change in gradient. It’s experimentally easier to simply store the gradient and track how it
changesoveroptimization.Thechangeinthegradientoverasingleoptimizationstepscaleslike
∥∇ f(x;θ )−∇ f(x;θ )∥∼∥H∇ f(x;θ )∥∼∥H∆θ∥.
θ 1 θ 0 θ 0
Bootstrappingthepreviouscalculation,weseethatthetypicalelementofthisgradientchange
vectorscaleslike
H(ij,pq)∆θ(pq) ∼X g ∂f
∂h( ℓj −)
1g h(q)
!
g ∂f h(q)
!
ℓ ∂h(i) ∂h(p) ℓ′ ℓ′−1 ℓ′ ∂h(p) ℓ′−1
ℓ,ℓ′ ℓ ℓ′ ℓ′
∼X
g g2
∂f
∥h
∥2! ∂h( ℓj −)
1
∂f
!
ℓ ℓ′ ∂h(i) ℓ′−1 ∂h(p) ∂h(p)
ℓ,ℓ′ ℓ ℓ′ ℓ′
! !
∼X ∥∆h∥3 ∂f
n
δ(ℓ′,ℓ−1) ∂f
+
∂f
n3/2 ∂h(i) ∂h(j) ∂h(j)
ℓ,ℓ′ ℓ ℓ′ ℓ′
whereagainwerestrictourattentiontotheintermediatelayerssothatn = n = n ≡ n.
ℓ ℓ′ ℓ′−1
Simplifying,wefind
∥∆h∥2 (cid:18) L L2 (cid:19)√ ∥∆h∥
∥H∆θ∥∼ √ + √ n2 ∼ √ . (14)
n ∥∆h∥ n ∥∆h∥ n n
Thefirsttwofactorsaccountforthescalingofasingleelement(above),andthefinalfactoraccounts
forthenormofthegradientchangevector,whosedimensionscalesliken2.Thisisthescalingfor
thegradientchangeweobserveinFigure5.
5Itisinterestingtonotethatcurvatureinthefinite-widthNTKregimereceivesequalcontributionsfromallpairsof
layers,whereascurvatureintheµPregimeisdominatedbyparameterpairsfromadjacentlayers.
20B Experimental details
Weperformscalingexperimentsontwolearningtasks,eachacrossavarietyofwidthsandrichnesses,
usinggradientdescentwithgloballearningrateη =0.1andmomentumβ =0.9.Thelineartaskis
trainingthe3-layerfully-connectedlinearmodeldescribedinthetexttolearnrandomGaussiandata
(n =n =10).ThepracticaltaskistrainingtheMyrtle-5convolutionalarchitecturedescribedin
0 3
Shankaretal.2020tolearnCIFAR-10:
1 def forward(self, x):
2 h1 = F.relu(self.conv1(x))
3 h2 = self.pool(F.relu(self.conv2(h1)))
4 h3 = self.pool(F.relu(self.conv3(h2)))
5 h4 = self.gap(F.relu(self.conv4(h3)))
6 assert h4.shape[1:] == (width, 1, 1)
7 h5 = self.readout(h4.squeeze())
8 return h5
wheretheconvolutionsareall3×3with“same”paddingsothattheconvlayersdonotchangethe
spatialshape,andweexclusivelyuseaveragepooling(2×2forallpoolinglayersexceptthefinal
global-average-pool,whichis8×8).Inbothlearningtasks,allconvlayersandlinearlayersare
customimplementationsusingthegradientmultipliersandinitializationscalepreviouslyderived.
Thewidthofaconvolutionalfeaturemapissimplythenumberofchannels(whichisgenerallynot
thedimensionoftheflattenedfeaturevector).WetrainallmodelsusingPyTorch.
Toestimatescalingexponents,wesimplyfitalinetothelog-scaleddata.Wesometimesrestrictthe
fittomedium/largewidthstomitigateunwantedfinite-sizeeffects.Errorbarsinscalingplotsreport
thebest-fiterrorsfortheslope.Itisimportanttonotethattheerrorbarsdonotreflectstatistical
variationovermultipletrials,nordotheyreflectvariationsduetofinite-widtheffects.
The linear model scaling experiment (Figure 3 and Figure 6) and the linearization experiment
(Figure5)performthelineartask;theformerusesminibatchsize1,andthelatterusesminibatch
size256.Thetrainingstabilityexperiment(Figure4)andtheCNNscalingexperiment(Figure7)
performthepracticaltask;theformerusesminibatchsize128,andthelatterusesminibatchsize32.
Noneofourmainresultsaresensitivetothechoiceofminibatchsize.
Boththelinearmodelscalingexperiment(Figure3)andtheCNNscalingexperiment(Figure7)
exhibitanapparenttheory/experimentdiscrepancyfortherelativesizeoftheoutputupdateafter
weightalignment.Theorypredicts:(a)outputupdatesshouldneverexhibitanywidth-dependence
atanyrichness(c.f.theNTC),and(b)afteralignment,theoutputsh =h +∆h
L,(t=1) L,(t=0) L,(t=0)
shouldbethesamesizeasthelabels(i.e.,width-independent).Ifbothofthesearesatisfied,thenat
t=1thenextrelativeoutputupdateshouldnotscalewithwidth. However,weempiricallysee
thatthereisstillmildscalingatintermediaterichness.Thereasonforthisseemingdiscrepancyis
thatprediction(b)isnotsatisfiedinourexperimentsatintermediaterichness;thegloballearning
rateisslightlytoosmall,andthefirstoutputupdate∆h isnotlargeenoughatintermediate
L,(t=0)
richnessto“overwrite”thewidth-dependenceofh . Contrastthiswiththebehaviornear
L,(t=0)
r =0,wherethereisnowidth-dependencetobeginwith,andthebehaviornearr =1/2,where
theinitialh aresmallenoughthattheyaredominatedbythefirstupdate∆h .The
L,(t=0) L,(t=0)
takeawaysare: thisdiscrepancymaybeeliminatedbyusingaslightlylargerlearningrate(i.e.,
η =0.5);theNTCisalwayssatisfied;and,theexperimentsareinfactconsistentwithourderivation.
21Figure6:Nearinitialization,layerandpassthroughcontributionsdominate.Forthelinear
modelscalingexperiment(Figure3),weprojectthedifferentupdatecontributionsontothetotal
representationupdate(seeEquation(2))andmeasuretherelativelengths.Theseratiosdonotvary
appreciablyoverwidthsandrichnesses.Inparticular,thepassthroughcontributiondoesnotdecay
withwidthinthereadoutlayer,suggestingalignmentbetween∆h andW (discussedattheendof
2 3
Section2.2).Inaddition,thefactthelayercontributionisnon-negligibleacrosswidthsandrichnesses
indicates that the MAX is satisfied. Interestingly, we see that near initialization the interaction
contributionisnegligibleatalllayers;itmaybecomenon-negligiblelaterintraining.
Figure7: Width-scalingofrepresentationsmatchpredictions. Werepeatthelinearmodel
scalingexperimentshowninFigure3withthepracticaltask.Thelargerbatchsizeeffectivelyreduces
thelearningrate,makingtheapparenttheory/experimentdiscrepancymorepronounced(i.e.,pink
triangles stray from horizontal line). We discuss the origin of this apparent discrepancy on the
previouspage.
22