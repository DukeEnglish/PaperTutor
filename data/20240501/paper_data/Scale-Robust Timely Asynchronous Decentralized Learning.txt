Scale-Robust Timely Asynchronous
Decentralized Learning
Purbesh Mitra Sennur Ulukus
Department of Electrical and Computer Engineering
University of Maryland, College Park, MD 20742
pmitra@umd.edu ulukus@umd.edu
Abstract—Weconsideranasynchronousdecentralizedlearning
system, which consists of a network of connected devices trying
to learn a machine learning model without any centralized
parameterserver.Theusersinthenetworkhavetheirownlocal
training data, which is used for learning across all the nodes
in the network. The learning method consists of two processes,
evolving simultaneously without any necessary synchronization.
The first process is the model update, where the users update
their local model via a fixed number of stochastic gradient
descent steps. The second process is model mixing, where the
users communicate with each other via randomized gossiping to
exchange their models and average them to reach consensus. In
thiswork,weinvestigatethestalenesscriteriaforsuchasystem,
whichisasufficientconditionforconvergenceofindividualuser
models.Weshowthatfornetworkscaling,i.e.,whenthenumber
ofuserdevicesnisverylarge,ifthegossipcapacityofindividual
usersscalesasΩ(logn),wecanguaranteetheconvergenceofuser Fig. 1. An example of a small distributed learning network. Each device
is updating their local ML model via SGD and parallelly, asynchronously
models in finite time. Furthermore, we show that the bounded
gossipingandmixingmodelswiththeirneighboringdevices.
stalenesscanonlybeguaranteedbyanydistributedopportunistic
scheme by Ω(n) scaling.
Thecommunicationefficiencyofmodelcommunicationunder
I. INTRODUCTION
channel delay and straggler effects in asynchronous learning
Decentralized learning, also known as gossip-based learn- setting was further improved in the subsequent studies [18]–
ing, is a method for learning a machine learning (ML) model [20]. The analysis in reference [17] shows that linear speedup
with distributed data stored across different users [1], [2]. for convergence of the model can be achieved by completely
This method utilizes two processes: model update and model asynchronous model update and model mixing process. Such
mixing. The model update process is essentially the user asynchronous stochastic device-to-device communication pro-
device performing stochastic gradient descent (SGD) with cess is referred to as randomized gossip algorithms. Gossip
the locally available data for a fixed number of steps. The algorithms offer a low complexity mechanism to disseminate
model mixing process is device-to-device communication, by information quickly in a network [21]. Such mechanisms are
which the local models are exchanged and then averaged for often useful in low latency network applications [22], where
consensus,asshowninFig.1.Decentralizedlearninghasbeen timely information delivery and lower staleness is a sufficient
analyzed in the literature as a viable alternative to federated criterion. In the convergence analysis in [17], the authors
learning [3], [4], which is the current state-of-the-art dis- have assumed bounded staleness of the user models, i.e., the
tributed ML method. With the promise of hyper-connectivity maximum number of training steps a user model update can
in the emergent sixth generation (6G) networks [5], such lag behind the global model is bounded by a finite quan-
gossip-basedmechanismsprovidecheap,reliableandprivacy- tity. This assumption, however, is not immediately obvious
preserving learning with decentralized implementations. when network scaling is considered. In large hyper-connected
Decentralized optimization mechanism was first proved for networks, such as in the emergent 6G communication, the
distributed convex function optimization [6]. In subsequent networksizeincreasesandmaintainingthestalenessbounded,
literature [7]–[9], the convergence guarantee was shown with i.e., O(1), requires carefully designing the communication
various constraints. Similar techniques were used in the de- network parameters. We call such systems, which satisfy the
centralizedlearningsettingforoptimizingMLmodels[1],[2] bounded staleness constraint, scale-robust.
usingSGD.Theworksin[10]–[12]analyzecompressedmodel Reference [22] compares a vast number of works in the
communicationfordecentralizedlearning.Theworksin[13]– literature, which analyze average information staleness (also
[17] showed that the process can be extended even when the referredtoasageofgossip)indifferentnetworksettings.The
asynchronous communication and model update is involved. most relevant works, in the context of this paper, are [23]–
4202
rpA
03
]TI.sc[
1v94791.4042:viXra[26]. [23] shows that for a uniform gossiping scheme with a Algorithm1AsynchronousDecentralizedLearningalgorithm
single source, the average staleness scales as Θ(logn). [24]– 1: Initialize model θ 0 at all the users.
[26] shows that this scaling can be as low as O(1) if the 2: for i∈[n] do
nodes follow an opportunistic gossiping policy. To the best 3: procedure MODELUPDATE(user i)
of our knowledge, the only work, so far, that has considered 4: Wait for availability time ∼Exp(µ i).
scale-robustness for bounded staleness is [27] in the context 5: Calculate gradient ∇L i(θ i(t);D i), taking time c i.
of asynchronous hierarchical federated learning (AHFL) set- 6: Update the model with learning rate α i(t) as
ting. [27] considers a client-edge-cloud based AHFL system,
θ (t+c )←θ (t+c )−α (t)∇L (θ (t);D ).
and shows that when the number of user devices grows i i i i i i i i
very large, the bounded staleness criterion is achieved if the 7: procedure MODELMIXING(user i)
number of edge servers is O(1). The AHFL setting is used 8: procedure MODELTRANSMISSION(from user i)
for circumventing data heterogeneity, i.e., presence of non- 9: Wait for availability time ∼Exp(λ i).
i.i.d. data distributions. This is achieved by clustering devices 10: Randomly select a user j from the set [n]\{i}.
with the same data distributions together under the same edge 11: Transmit model θ i(t) to user j.
devices. In our work, we extend the concept to decentralized
12: procedure MODELRECEIVAL(at user i)
learning,wheremultipleuserdevices,withheterogeneousdata
distributions,areinafullyconnectednetwork.Weshowthatif
13: Receive model θ j(t) from node j ∈[n]\{i}.
14: Choose a random value β ∈(0,1), uniformly.
the gossip capacity of the individual users scales as Ω(logn),
15: Mix with the local model as
then the scale-robustness can be guaranteed.
θ (t)←βθ (t)+(1−β)θ (t).
i i j
II. SYSTEMMODEL
We consider a symmetric fully connected network with n
user devices performing decentralized learning. Each node i
updatesitslocalmodelθ (t)byperformingτ gradientdescent which represents the number of versions user j is lagging
i
steps on their locally available data D i. After calculating behind the source version at user i. Note that S ii(t) = 0.
the gradient ∇L (θ (t);D ) and updating the model with During a gossip communication from node k to node j, the
i i i
deterministic delay c , the user device remains unavailable staleness, therefore, gets modified as follows
i
for a time duration. For the ith user, this time is a shifted Si(t+)=min{Si(t−),Si(t−)}, ∀i∈[n]. (3)
exponential distribution with mean exponential time 1 . For j j k
mixingmodels,thegossipingfromtheithuserischaracµ ti erized To evaluate the convergence criterion, the expected staleness
as an exponentially distributed unavailability time window of insteadystateneedstobefinite[17].Hence,inthecontextof
shifted exponential distribution with deterministic delay of d networkscalingwithlargen,weexpectthefollowingcriterion
i
and mean exponential time 1 , as shown in Fig. 2(a). Since for a model to converge
λi
in the fully connected network, each device is connected to
lim
E(cid:2) Si(t)(cid:3)
=O(1), ∀i∈[n],j ∈[n]. (4)
n−1 neighbors, the mean unavailability window of gossiping t→∞ j
between two devices is d i + n λ− i1 ≈ n λ− i1, for large network In the following lemma, we derive an upper bound for the
size n. Hence, for model mixing process, we ignore d i, staleness of the decentralized learning system.
and essentially consider a Poisson arrival process with rate
nλ −i 1 between a pair of users. The asynchronous decentralized Lemma 1 The expected staleness of a user is bounded as
learning procedure is given in Algorithm 1.
n−1
III. STALENESSGUARANTEE tl →im ∞E(cid:2) S ji(t)(cid:3) ≤
λ
mµ i
in
(cid:88) k1 , (5)
k=1
In this section, we derive the gossip capacity scaling for
bounded staleness guarantee. We denote the version process where λ min =min{λ 1,λ 2,··· ,λ n}.
for the ith user as Ni(t). Whenever user i updates its model
i
by SGD, Ni(t) increases by 1. The model version at user j Proof: Wenotethateventhoughtherearemultiplesourcesin
i
corresponding to user i’s model is denoted as Ni(t), which the decentralized learning network, when a single ith user’s
j
is the latest version of the model of user i, mixed with user model is considered, it essentially becomes a combination of
j. When user k sends a gossip update to user j at time t, the n different source tracking problems, as shown in Fig. 2(b).
updated model becomes the latest version of the two. This is Now, since the inter-arrival times for user self-update process
expressed as is a shifted exponential distribution, it does not have the
memoryless property as an exponential distribution, making
Ni(t+)=max{Ni(t−),Ni(t−)}, ∀i∈[n]. (1)
j j k it difficult to analyze for staleness calculations. Hence, we
The staleness of user j at time t is defined as calculate the staleness of the system considering exponential
distribution with mean 1 only. Since this reduces the arrival
Si(t)=Ni(t)−Ni(t), ∀i∈[n], (2) µi
j i j times of the new self-updates, it makes all the increments in(c 1,µ 1) (c 1,µ 1) µ
1
1 λ 1 1 1
λ 6 λ 1 λ min
(c ,µ ) 6 2
6 6 (c ,µ )
2 2
λ
2 λ λ
λ λ 2 λ min
5 6 2 min 2
6 6
5 3
(c 5,µ 5)
λ (c 3,µ 3) λ 5 5
3
λ min 5
3
3
λ 4 4 4 λ 3 4 λ min
(c ,µ ) λ λ
4 4 4 min
(a) Systemmodelofdecentralizedlearning. (b) Gossipingandtrackingofuser1model. (c) Modifiedgossipnetwork.
Fig.2. Differentrepresentationsofadecentralizedlearningsystem.
N (t) faster, making the resulting staleness of the system an Now,weshowthatthisscale-robustnesscannotbeachieved
i
upper bound for the original system. We denote the staleness byanydistributedopportunisticgossipingschemebyΩ(logn)
of this new system as S˜i(t). Furthermore, the different gossip gossip rate scaling for individual users. Consider the scheme
j
rates {λ ,λ ,··· ,λ } make the network asymmetric. To usedin[24]forexample,whichallowsonlythefreshestnodes
1 2 n
derive the stated result, we replace all the rates with the in the network to transmit with full capacity
(cid:80)n
λ . Such
i=1 i
minimum value λ = min{λ ,λ ,··· ,λ }, as shown in opportunistic scheme is achieved by transmitting some pilot
min 1 2 n
Fig. 2(c). Due to the substitution by lower gossip rate, this signal in the network whenever a user updates its model. This
substitution yields a higher expected staleness value from the alerts all the other users in the network to not transmit any
resultin[23,Thm.2].Thus,weobtainanupperboundforthe updates in the network, thus avoiding any possible collision
averageageforthissymmetricfullyconnectedgossipnetwork or interference in the gossip capacity utilization. The freshest
as userkeepstransmittinguntilitreceivesasignalfromanyother
n−1 fresh user. In Theorem 2, we show that this kind of scheme
lim E(cid:2) Si(t)(cid:3) ≤ µ i (cid:88) 1 . (6) doesnotyieldanyscalinggainfordistributedlearningsetting.
t→∞ j λ min k
k=1
This concludes the proof of Lemma 1. ■
Theorem 2 The gossip capacity scaling of individual users
Using the result of this lemma, we show a sufficient
that guarantees scale-robustness in a fully connected network
condition for gossip capacity scaling of an individual user to
using opportunistic schemes is Ω(n).
meet the scale-robustness condition.
Theorem 1 Ifthegossipcapacityofindividualusersinafully Proof: We prove this result by evaluating the expected stal-
connected network scales as Ω(logn), the scale-robustness eness of a user in the network, where the gossip rates are
condition is guaranteed. replaced by λ max = max{λ 1,λ 2,··· ,λ n}. Following the
average age formulation in [23, Thm. 2], this substitution
yields a lower bound for the expected staleness. The user
Proof: We can write the sum of reciprocals in (5) as
updateratesarealsoreplacedbytheshiftedexponentialdistri-
n−1 (cid:18) (cid:19) bution (c ,µ ), where c =max{c ,c ,··· ,c } and
(cid:88) 1 1 max min max 1 2 n
=log(n−1)+γ+O , (7) µ =min{µ ,µ ,··· ,µ }. The mean inter update time of
k n min 1 2 n
k=1 user i is c + 1 . We denote the gossiping time for the
max µmin
where γ is the Euler–Mascheroni constant. Therefore, if λ ∼ ith user after the kth update as Ti[k]. As there are n users in
i
Ω(logn),∀i∈[n], we can write the system, the mean gossiping time is
(cid:18) (cid:18) (cid:19)(cid:19) (cid:18) (cid:19)
µ i log(n−1)+γ+O 1 =O(1). (8) E(cid:2) Ti[k](cid:3) = 1 c + 1 . (10)
Ω(logn) n n max µ
min
Hence, we obtain We denote the staleness of this modified system at the kth
lim
E(cid:2) Si(t)(cid:3)
=O(1), (9)
updatetimeasSˆ ji[k].Ifthereisnogossipcommunicationfrom
t→∞
j useritoj,inTi[k],Sˆi[k+1]isjustSˆi[k]+1.Theprobability
j j
implying scale-robustness. ■ of this event is e−λmaxTi[k]. Otherwise, with probability 1−1.000 n=10,λ=10 1.000 n=10,λ=10log(log10) 1.000 n=10,λ=10log(10)
0.975 n=50,λ=10 0.975 n=50,λ=10log(log50) 0.975 n=50,λ=10log(50)
n=100,λ=10 n=100,λ=10log(log100) n=100,λ=10log(100)
0.950 0.950 0.950
0.925 0.925 0.925
0.900 0.900 0.900
0.875 0.875 0.875
0.850 0.850 0.850
0.825 0.825 0.825
0.800 0.800 0.800
0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000
time time time
(a) Lossvs.epochsforλi∼Θ(1). (b) Lossvs.epochsforλi∼Θ(loglogn). (c) Lossvs.epochsforλi∼Θ(logn).
Fig.3. Loss-epochplotsforlinearregressiontask;m=1.
e−λmaxTi[k], Sˆi[k+1]=0. Thus, we obtain of w∗ ∈Rd is chosen uniformly from the interval [0,1]. The
j ℓ
E(cid:104)
Sˆ
ji[k+1](cid:105) =E(cid:104)(cid:16)
Sˆ
ji[k]+1(cid:17) e−λmaxTi[k](cid:105)
. (11)
c co or rr re es sp po on nd di sn tg oo tu ht epu int dis exy j o=
f
uf n( iqx uj e,w diℓ∗ s) tr. iF bu ur tit oh nerm pro er se e, nℓ
t
∈ in[ tm he]
dataset. In our simulation m ≤ n. Note that when only one
Since, Sˆ ji[k] and Ti[k] are independent, (11) becomes distributionispresentinthedata,i.e.,m=1,thelossfunction
(cid:104) (cid:105) (cid:16) (cid:104) (cid:105) (cid:17) (cid:104) (cid:105) achieves its minimum at θ =w∗.
E Sˆi[k+1] = E Sˆi[k] +1 E e−λmaxTi[k] . (12) 1
j j Now, we note that this loss function satisfies the conver-
Now, since e−λmaxTi[k] is a convex function of Ti[k], using gence criteria in [17]. First, we consider a linear regression
problem, i.e., f(x,θ) = xTθ, with d = 100. We observe
Jensen’s inequality and substituting from (10), we obtain
that (16) is differentiable, and its gradient can be written as
(cid:104) (cid:105) (cid:16) (cid:104) (cid:105) (cid:17)
E Sˆ ji[k+1] ≥ E Sˆ ji[k] +1 e−λmaxE[Ti[k]] (13) ∇ θL(θ;D)= |D2 |XT(Xθ−y). Since X is normalized data,
thisshowsthatLhasL-Lipschitzgradient.Themixingmatrix
=(cid:16) E(cid:104) Sˆ ji[k](cid:105) +1(cid:17) e−λm nax(cid:16) cmax+ µm1 in(cid:17) . (14) W
k
is doubly stochastic with bounded spectral gap, by our
choiceofformulationinAlgorithm1.Thegradientestimation
Thus,fromtheinitialconditionSˆ ji[0]=0,andusingrecursion
is unbiased because of the addition of zero-mean mixture of
(cid:104) (cid:105) (cid:104) (cid:105) 1 Gaussian noise. The final criterion of bounded variance holds
tl →im ∞E Sˆ ji(t) = kl →im ∞E Sˆ ji[k+1] ≥
1−e− nλ µm ma ix
n
. (15) t vr au re ianb ce ec ,a au nse
d
ha ell nct ehe thein mdi iv xi td uu real ofdi ts ht erib du ist ti ro in bs utih oa nv se isb ao lu sn od se od
.
Clearly, the right hand side of (15) does not yield O(1) if Additionally, we consider a nonlinear regression with d = 2,
λ = Θ(logn). Only O(n) scaling of λ yields O(1) where f(x,θ) = θ x +θ θ x , where, x = [x ,x ]T and
max max 1 1 1 2 2 1 2
lower bound for the expected staleness. Thus, the gossip rate θ = [θ ,θ ]T. Since the formulation in (16) for this case is
1 2
scalingforboundedexpectedstalenessintheoriginalnetwork differentiable, we obtain ∂L = 2(θ x +θ θ x −y)x and
must be O(n). ■ ∂L =2(θ x +θ θ x −y∂ )θ θ1 x .Fo1 llo1 wing1 sim2 il2 arargum1 ents
∂θ1 1 1 1 2 2 1 1
asbefore,thisformulationalsosatisfiestheconditionsin[17].
IV. NUMERICALRESULTS
We simulate the linear regression task and show the loss-
Weshowvalidityofthestalenessboundsvianumericalsim-
epoch plot in Fig. 3. We show the plot for λ ∼Θ(1) scaling
ulations. We consider a simple regression task that minimizes i
in Fig. 3(a), the plot for λ ∼ Θ(loglogn) = o(logn) in
a loss function over distributed users. The loss function is i
Fig. 3(b), and the plot for λ ∼ Θ(logn) in Fig. 3(c). We
L(θ;D)= 1 (cid:88) (f(x ,θ)−y )2 (16) observe that this change in i scaling of the gossip capacity
|D| j j
does not change the loss-epoch profile of the decentralized
j∈[|D|]
= |D i| (cid:88) 1 (cid:88) (f(x ,θ)−y )2 (17) learning setting, as in all the three cases, the loss function
|D| |D | j j is deceasing with epochs and the individual models are con-
i
i∈[n] j∈[|Di|] verging, although with different rates. This can be explained
=
|D i| (cid:88)
L (θ;D ). (18)
by the linearity of the regression task. Since the gradient
|D| i i of the loss function is additive, addition of new users, and
i∈[n]
therebyincreasedstaleness,inthesystemdoesnotdeviatethe
We assume that the data is equally distributed among the individual loss functions much from the overall loss functions
users, and thus, |Di| = 1. We synthetically generate the andtheuserscanstillachievesufficientmodelmixing.Hence,
|D| n
dataset as D = {(x ,y )}, where x ∈ Rd. In our case, the we observe the speedup of model convergence for large
j j j
data points are from the Gaussian mixture distribution x ∼ number of users, as in [17]. We also observe that the linear
j
1N (cid:0)1.5w∗,I (cid:1) +1N (cid:0) −1.5w∗,I (cid:1) , where each component speedup of convergence trend appears for any choice of m.
2 d ℓ d 2 d ℓ d
ssol
egareva
ssol
egareva
ssol
egareva1.00 n=10,λ=1 1.00 n=10,λ=log(log10) 1.00 n=10,λ=log(10)
n=50,λ=1 n=50,λ=log(log50) n=50,λ=log(50)
0.95 n=100,λ=1 0.98 n=100,λ=log(log100) 0.98 n=100,λ=log(100)
0.96 0.96
0.90 0.94 0.94
0.92 0.92
0.85
0.90 0.90
0.80 0.88
0.88
0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000
time time time
(a) Lossvs.epochsforλi∼Θ(1). (b) Lossvs.epochsforλi∼Θ(loglogn). (c) Lossvs.epochsforλi∼Θ(logn).
Fig.4. Loss-epochplotsfornon-linearregressiontask;m∼n.
Next, we plot the loss-epoch profile for non-linear regres- [6] A.NedicandA.Ozdaglar. Distributedsubgradientmethodsformulti-
sion task in Fig. 4. We observe that, both for λ ∼ Θ(1), agentoptimization.IEEETransactionsonAutomaticControl,54(1):48–
i
61,January2009.
and λ ∼ Θ(loglogn) = o(logn), the loss function does
i [7] A. Nedic, A. Ozdaglar, and P. A. Parrilo. Constrained consensus and
not show any speedup of convergence trend for increasing optimizationinmulti-agentnetworks. IEEETransactionsonAutomatic
numberofusersinFig.4(a)andFig.4(b),respectively.Rather, Control,55(4):922–938,April2010.
[8] A.Nedic,A.Olshevsky,A.Ozdaglar,andJ.N.Tsitsiklis.Ondistributed
for n = 50 and n = 100, it deviates from the convergence
averaging algorithms and quantization effects. IEEE Transactions on
trajectory by quite a lot. However, in this setting, we observe AutomaticControl,54(11):2506–2517,November2009.
that for λ ∼ Θ(logn), the convergence for any number of [9] T.Yang,X.Yi,J.Wu,Y.Yuan,D.Wu,Z.Meng,Y.Hong,H.Wang,
i
Z. Lin, and K. H. Johansson. A survey of distributed optimization.
users almost follow the same trajectory in Fig. 4(c). This
AnnualReviewsinControl,47:278–305,January2019.
is consistent with the loss function, which yields non-linear [10] A. Koloskova, T. Lin, S. Stich, and M. Jaggi. Decentralized deep
gradients, thus resulting in higher deviation of the individual learning with arbitrary communication compression. In ICLR, April
2019.
loss functions of the users from the global loss function.
[11] A.Koloskova,S.Stich,andM.Jaggi.Decentralizedstochasticoptimiza-
tionandgossipalgorithmswithcompressedcommunication. InICML,
V. CONCLUSION May2019.
[12] A.Koloskova,N.Loizou,S.Boreiri,M.Jaggi,andS.Stich. Aunified
Weanalyzedthescale-robustnesscriterionforasynchronous theory of decentralized sgd with changing topology and local updates.
decentralized learning systems. In particular, we showed that InICML,November2020.
[13] S. S. Ram, A. Nedic, and V. V. Veeravalli. Asynchronous gossip
for randomized gossip schemes, if the gossip capacity of the
algorithmsforstochasticoptimization. InIEEECDC,December2009.
individualnodesscaleasanyfunctionthatisΩ(logn),thenthe [14] P.H.Jin,Q.Yuan,F.Iandola,andK.Keutzer. Howtoscaledistributed
staleness at the users is guaranteed to be O(1). Additionally, deeplearning? InNeurIPS,November2016.
[15] X. Lian, C. Zhang, H. Zhang, C. Hsieh, W. Zhang, and J. Liu.
we proved that such scaling gain cannot be achieved by any
Candecentralizedalgorithmsoutperformcentralizedalgorithms?acase
opportunisticgossipscheme,asinthecaseofsinglesourcein- study for decentralized parallel stochastic gradient descent. NeurIPS,
formation dissemination. The required gossip capacity scaling December2017.
[16] M. Glasgow and M. Wootters. Asynchronous distributed optimization
for scale-robustness is Ω(n) for such opportunistic schemes.
withstochasticdelays. InAISTAT,May2022.
Furthermore, by numerical simulations, we observed that the [17] X.Lian,W.Zhang,C.Zhang,andJ.Liu. Asynchronousdecentralized
necessity of scale-robustness is much more prominent with parallelstochasticgradientdescent. InICML,July2018.
[18] J.Tu,J.Zhou,andD.Ren. Anasynchronousdistributedtrainingalgo-
non-linear machine learning models.
rithm based on gossip communication and stochastic gradient descent.
ComputerCommunications,195:416–423,November2022.
REFERENCES [19] X. Wang and Y. Wang. Asynchronous hierarchical federated learning.
2022. AvailableatarXiv:2206.00054.
[1] R. Ormandi, I. Hegedus, and M. Jelasity. Gossip learning with linear [20] G.Xiong,G.Yan,S.Wang,andJ.Li. Straggler-resilientdecentralized
models on fully distributed data. Concurrency and Computation: learning via adaptive asynchronous updates. 2023. Available at
PracticeandExperience,25(4):556–571,May2013. arXiv:2306.06559.
[2] I. Hegedus, A. Berta, L. Kocsis, A. Benczur, and M. Jelasity. Robust [21] D. Shah. Gossip algorithms. Foundations and Trends in Networking,
decentralized low-rank matrix decomposition. ACM Transactions on 3(1):1–125,2008.
IntelligentSystemsandTechnology,7(4):1–24,May2016. [22] P.Kaswan,P.Mitra,A.Srivastava,andS.Ulukus. Ageofinformation
[3] I.Hegedus,G.Danner,andM.Jelasity. Gossiplearningasadecentral- ingossipnetworks:Afriendlyintroductionandliteraturesurvey,2023.
ized alternative to federated learning. In Distributed Applications and AvailableatarXiv:2312.16163.
InteroperableSystems,pages74–90.Springer,June2019. [23] R.D.Yates. Theageofgossipinnetworks. InIEEEISIT,July2021.
[4] I.Hegedus,G.Danner,andM.Jelasity. Decentralizedlearningworks: [24] P.MitraandS.Ulukus. ASUMAN:Agesenseupdatingmultipleaccess
An empirical comparison of gossip learning and federated learning. innetworks. InAllertonConference,September2022.
JournalofParallelandDistributedComputing,148:109–124,February [25] P. Mitra and S. Ulukus. Timely opportunistic gossiping in dense
2021. networks. InIEEEInfocom,May2023.
[5] H. Lee, B. Lee, H. Yang, J. Kim, S. Kim, W. Shin, B. Shim, and [26] P. Mitra and S. Ulukus. Age-aware gossiping in network topologies.
H. V. Poor. Towards 6G hyper-connectivity: Vision, challenges, and April2023. AvailableatarXiv:2304.03249.
key enabling technologies. Journal of Communications and Networks, [27] P. Mitra and S. Ulukus. Timely asynchronous hierarchical federated
25(3):344–354,June2023. learning:Ageofconvergence. InWiOptconference,August2023.
ssol
egareva
ssol
egareva
ssol
egareva