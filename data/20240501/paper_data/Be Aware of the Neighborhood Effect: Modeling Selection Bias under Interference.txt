PublishedasaconferencepaperatICLR2024
BE AWARE OF THE NEIGHBORHOOD EFFECT: MODEL-
ING SELECTION BIAS UNDER INTERFERENCE
HaoxuanLi1 ChunyuanZheng1 SihaoDing2 PengWu3,∗ ZhiGeng3
FuliFeng2 XiangnanHe2
1PekingUniversity 2UniversityofScienceandTechnologyofChina
3BeijingTechnologyandBusinessUniversity
hxli@stu.pku.edu.cn dsihao@mail.ustc.edu.cn
{zhengchunyuan99, fulifeng93, xiangnanhe}@gmail.com
{pengwu, zhigeng}@btbu.edu.cn
ABSTRACT
Selectionbiasinrecommendersystemarisesfromtherecommendationprocess
ofsystemfilteringandtheinteractiveprocessofuserselection. Manyprevious
studies have focused on addressing selection bias to achieve unbiased learning
of the prediction model, but ignore the fact that potential outcomes for a given
user-item pair may vary with the treatments assigned to other user-item pairs,
named neighborhood effect. To fill the gap, this paper formally formulates the
neighborhood effect as an interference problem from the perspective of causal
inferenceandintroducesatreatmentrepresentationtocapturetheneighborhood
effect. Onthisbasis,weproposeanovelideallossthatcanbeusedtodealwith
selectionbiasinthepresenceofneighborhoodeffect. Wefurtherdeveloptwonew
estimatorsforestimatingtheproposedidealloss. Wetheoreticallyestablishthe
connectionbetweentheproposedandpreviousdebiasingmethodsignoringthe
neighborhood effect, showing that the proposed methods can achieve unbiased
learningwhenbothselectionbiasandneighborhoodeffectarepresent,whilethe
existingmethodsarebiased. Extensivesemi-syntheticandreal-worldexperiments
areconductedtodemonstratetheeffectivenessoftheproposedmethods.
1 INTRODUCTION
Selectionbiasiswidespreadinrecommendersystem(RS)andchallengesthepredictionofusers’true
preferences(Wuetal.,2022;Chenetal.,2023),whicharisesfromtherecommendationprocessof
systemfilteringandtheinteractiveprocessofuserselection(MarlinandZemel,2009;Huangetal.,
2022). Forexample,intheratingpredictiontask,selectionbiashappensinexplicitfeedbackdata
asusersarefreetochoosewhichitemstorate,sothattheobservedratingsarenotarepresentative
sample of all ratings (Steck, 2010; Wang et al., 2023c). In the post-click conversion rate (CVR)
predictiontask,selectionbiashappensduetoconventionalCVRmodelsaretrainedwithsamples
of clicked impressions while utilized to make inference on the entire space with samples of all
impressions(Maetal.,2018;Zhangetal.,2020;Wangetal.,2022a;Lietal.,2023f).
Inspiredbythecausalinferenceliterature(ImbensandRubin,2015),manystudieshaveproposed
unbiasedestimatorsforeliminatingtheselectionbias,suchasinversepropensityscoring(IPS)(Schn-
abel et al., 2016), self-normalized IPS (SNIPS) (Swaminathan and Joachims, 2015), and doubly
robust(DR)methods(Wangetal.,2019;Chenetal.,2021a;Daietal.,2022;Lietal.,2023d;e).
Giventhefeaturesofauser-itempair,thesemethodsfirstestimatetheprobabilityofobservingthat
userratingorclickingontheitem,calledpropensity. Thentheinverseofthepropensityisusedto
weighttheobservedsamplestoachieveunbiasedestimatesoftheidealloss.
However,thetheoreticalguaranteesofthepreviousmethodsareallestablishedundertheStableUnit
TreatmentValuesAssumption(SUTVA)(Rubin,1980),whichrequiresthatthepotentialoutcomes
foroneuser-itempairdonotvarywiththetreatmentsassignedtootheruser-itempairs(alsoknown
∗Correspondingauthor.
1
4202
rpA
03
]GL.sc[
1v02691.4042:viXraPublishedasaconferencepaperatICLR2024
Figure1: Causaldiagramsoftheexistingdebiasingmethodsundernointerferenceassumption(left),
andtheproposedmethodtakingintoaccountthepresenceofinterference(right),wherex ,o ,
u,i u,i
and r denote the confounder, treatment, and outcome of user-item pair (u,i), respectively. In
u,i
thepresenceofinterference,N andN denotetheotheruser-itempairsaffectingandnot
(u,i) −(u,i)
affecting(u,i),respectively,andg denotesthetreatmentrepresentationtocapturetheinterference.
u,i
asnointerferenceornoneighborhoodeffect),asshowninFigure1(a). Infact,suchanassumption
canhardlybesatisfiedinreal-worldscenarios. Forexample,auser’sratingonanitemcanbeeasily
influencedbyotherusers’ratingsonthatitem,aswellasauser’sclickingonanitemmightfacilitate
otherusers’clickingandpurchasingofthatitem(Chenetal.,2021b;Zhengetal.,2021). Figure1(b)
showsageneralcausaldiagraminthepresenceofinterferenceindebiasedrecommendation.
Tofillthisgap,inthispaper,wefirstformulatethedebiasprobleminFigure1(b)fromtheperspective
ofcausalinferenceandextendthedefinitionofpotentialoutcomestobecompatibleinthepresence
of interference, then introduce a learnable treatment representation to capture such interference.
Basedontheextendedpotentialoutcomeandtreatmentrepresentation,weproposeanovelidealloss
thatcaneffectivelyevaluatetheperformanceofthepredictionmodelwhenbothselectionbiasand
neighborhoodeffectarepresent. Wethenproposetwonewestimatorsforestimatingtheproposed
idealloss,namedneighborhoodinversepropensityscore(N-IPS)andneighborhooddoublyrobust
(N-DR), respectively. Theoretical analysis shows that the proposed N-IPS and N-DR estimators
canachieveunbiasedlearninginthepresenceofbothselectionbiasandneighborhoodeffect,while
thepreviousdebiasingestimatorscannotresultinunbiasedlearningwithoutimposingextrastrong
assumptions. Extensivesemi-syntheticandreal-worldexperimentsareconductedtodemonstratethe
effectivenessoftheproposedmethodsforeliminatingtheselectionbiasunderinterference.
2 PRELIMINARIES: PREVIOUS SELECTION BIAS FORMULATION
Let u ∈ U and i ∈ I be a user and an item, x , o , and r be the feature, treatment (e.g.,
u,i u,i u,i
exposure), and feedback (e.g., conversion) of the user-item pair (u,i), where o equals 1 or 0
u,i
representswhethertheitemiisexposedtouseruornot. LetD ={(u,i)|u∈U,i∈I}bethesetof
alluser-itempairs. Usingthepotentialoutcomeframework(Rubin,1974;Neyman,1990),letr (1)
u,i
bethepotentialfeedbackthatwouldbeobservedifitemihadbeenexposedtouseru(i.e.,o had
u,i
beensetto1). Thepotentialfeedbackr (1)isobservedonlywheno =1,otherwiseitismissing.
u,i u,i
Thenignoringthemissingr (1)andtrainingthepredictionmodeldirectlywiththeexposeddata
u,i
suffersfromselectionbias,sincetheexposureisnotrandomandisaffectedbyvariousfactors.
Intheabsenceofneighborhoodeffect,thepotentialfeedbackr (1)representstheuser’spreference
u,i
by making intervention o = 1. To predict r (1) for all (u,i) ∈ D, let rˆ ≜ f (x ) be a
u,i u,i u,i θ u,i
predictionmodelparameterizedwithθ. DenoteRˆ ∈ R|U|×|I| asthepredictedpotentialfeedback
matrixwitheachelementbeingrˆ . Ifallthepotentialfeedback{r (1)|(u,i)∈D}wereobserved,
u,i u,i
theideallossfortrainingthepredictionmodelrˆ isformallydefinedas
u,i
(cid:88)
L (Rˆ)=|D|−1 δ(rˆ ,r (1)),
ideal u,i u,i
(u,i)∈D
whereδ(·,·)isapre-definedlossfunction,e.g.,thesquaredloss(r (1)−rˆ )2. However,since
u,i u,i
r (1)ismissingwheno =0,theideallosscannotbecomputeddirectlyfromobservationaldata.
u,i u,i
Totacklethisproblem,manydebiasingmethodsaredevelopedtoaddresstheselectionbiasbyestab-
lishingunbiasedestimatorsofL (Rˆ),suchaserrorimputationbased(EIB)method(Hernández-
ideal
Lobatoetal.,2014),inversepropensityscoring(IPS)method(Schnabeletal.,2016),self-normalized
2PublishedasaconferencepaperatICLR2024
IPS(SNIPS)method(SwaminathanandJoachims,2015),anddoublyrobust(DR)methods(Wang
etal.,2019;Chenetal.,2021a;Daietal.,2022;Lietal.,2023e). Wesummarizethecausalparameter
ofinterestandthecorrespondingestimationmethodsinthepreviousstudiesasfollows.
• Forthecausalparameterofinterest,previousstudiesassumethetargeteduserpreference
r (o =1)dependsonlyonthetreatmentstatuso =1. Thentheideallossisdefined
u,i u,i u,i
usingthesampleaverageofδ(rˆ ,r (o =1)).
u,i u,i u,i
• Forthemethodsofestimatingthecausalparameterofinterest,previousstudieshavemade
extensiveeffortstoestimatetheprobabilityP(o =1|x ),calledpropensity,i.e.,the
u,i u,i
probabilityofitemiexposedtouserugiventhefeaturesx . ThentheexistingIPSand
u,i
DRmethodsusetheinverseofthepropensityforweightingtheobservedsamples.
Nevertheless,wearguethatboththecausalparameterandthecorrespondingestimationmethodsin
thepreviousstudiesleadtothefailurewheneliminatingtheselectionbiasunderinterference.
• (Section3)Forthecausalparameterofinterest,asshowninFigure1(b),inthepresenceof
interference,boththetreatmentstatuso andthetreatmentstatuseso wouldaffect
u,i N(u,i)
thetargeteduserpreferencer (o ,o ),insteadofr (o )inthepreviousstudies.
u,i u,i N(u,i) u,i u,i
• (Section 4) For the estimation methods of the causal parameter of interest, as shown in
Figure1(b),whenperformingpropensity-basedreweightingmethods,botho ando
u,i N(u,i)
fromitsneighborsshouldbeconsideredastreatmentsofuser-itempair(u,i). Therefore,the
propensityshouldbemodeledasP(o =1,o |x )insteadofP(o =1|x )in
u,i N(u,i) u,i u,i u,i
previousstudies,whichmotivatesustodesignnewIPSandDRestimatorsunderinterference.
3 MODELING SELECTION BIAS UNDER NEIGHBORHOOD EFFECT
Inthissection,wetaketheneighborhoodeffectinRSasaninterferenceproblemincausalinference
area,andthenintroduceatreatmentrepresentationtocapturetheneighborhoodeffect. Lastly,we
proposeanovelideallosswhenbothselectionbiasandneighborhoodeffectarepresent.
3.1 BEYOND“NOINTERFERENCE”ASSUMPTIONINPREVIOUSSTUDIES
Inthepresenceofneighborhoodeffect,thevalueofr (1)dependsonnotonlytheuser’spreference
u,i
butalsotheneighborhoodeffect,thereforewecannotdistinguishtheinfluenceofuserpreference
andtheneighborhoodeffect,evenifallthepotentialoutcomes{r (1) : (u,i) ∈ D}areknown.
u,i
Conceptually,theneighborhoodeffectwillcausethevalueofr (1)relyingontheexposurestatus
u,i
o andthefeedbackr forsomeotheruser-itempairs(u′,i′)̸=(u,i). Formally,wesaythat
u′,i′ u′,i′
interferenceexistswhenatreatmentononeunithasaneffectontheoutcomeofanotherunit(Ogburn
and VanderWeele, 2014; Forastiere et al., 2021; Sävje et al., 2021), due to the social or physical
interactionamongunits. Previousdebiasingmethodsrelyonthe“nointerference”assumption,which
requiresthepotentialoutcomesofaunitarenotaffectedbythetreatmentstatusoftheotherunits.
Nevertheless,suchanassumptioncanhardlybesatisfiedinreal-worldrecommendationscenarios.
3.2 PROPOSEDCAUSALPARAMETEROFINTERESTUNDERINTERFERENCE
Leto=(o ,...,o )bethevectorofexposuresofalluser-itempairs. Foreach(u,i)∈D,we
1,1 |U|,|I|
defineapartitionofo = (o ,o ,o ),whereN isalltheuser-itempairsaffecting
u,i N(u,i) N−(u,i) (u,i)
(u,i),calledtheneighborsof(u,i),andN isalltheuser-itempairsnotaffecting(u,i). When
−(u,i)
the feedback r is further influenced by the neighborhood exposures o , then the potential
u,i N(u,i)
feedbackof(u,i)shouldbedefinedasr (o ,o )toaccountfortheneighbourhoodeffect.
u,i u,i N(u,i)
However, ifwetake(o ,o )asthenewtreatmentdirectly, itwouldbeahigh-dimensional
u,i N(u,i)
sparsevectorwhenthedimensionofo ishighandthenumberofexposedneighborsislimited.
N(u,i)
Toaddressthisproblemandcapturetheneighborhoodeffecteffectively,wemakeanassumptionon
theinterferencemechanismleveragingtheideaofrepresentationlearning(Johanssonetal.,2016).
Assumption 1 (Neighborhood Treatment Representation). There exists a representation vector
ϕ:{0,1}|N(u,i)| →G,ifϕ(o N(u,i))=ϕ(o′ N(u,i)),thenr u,i(o u,i,o N(u,i))=r u,i(o u,i,o′ N(u,i)).
3PublishedasaconferencepaperatICLR2024
The above assumption implies that the value of r (o ,o ) depends on o through a
u,i u,i N(u,i) N(u,i)
specific treatment representation ϕ(·) that summarizes the neighborhood effect. Denote g =
u,i
ϕ(o ),thenwehaver (o ,o )=r (o ,g )underAssumption1,i.e.,thefeedback
N(u,i) u,i u,i N(u,i) u,i u,i u,i
of(u,i)underindividualexposureo andtreatmentrepresentationg .
u,i u,i
Wenowproposeideallossunderneighborhoodeffectwithtreatmentrepresentationlevelg ∈G as
LN (Rˆ|g)=|D|−1 (cid:88) δ(rˆ ,r (o =1,g)),
ideal u,i u,i u,i
(u,i)∈D
andthefinalideallosssummarizesvariousneighborhoodeffectsg ∈G as
(cid:90)
LN (Rˆ)= LN (Rˆ|g)π(g)dg,
ideal ideal
whereπ(g)isapre-specifiedprobabilitydensityfunctionofg.
TheproposedLN (Rˆ)forcesthepredictionmodelrˆ toperformwellacrossvaryingtreatment
ideal u,i
representationlevelsg ∈ G. Thus,LN (Rˆ)isexpectedtocontroltheextrabiasthatarisesfrom
ideal
theneighborhoodeffect. Incomparison,theselfinterestandneighborhoodeffectareintertwinedin
previouslyusedL (Rˆ),whereasourproposedLN (Rˆ)isveryflexibleduetothefreechoice
ideal ideal
ofπ(g). Thechoiceofπ(g)dependsonthetargetpopulationthatwewanttomakepredictionson.
Consideranextremecaseofnoneighborhoodeffect,thiscorrespondstog =0foralluser-item
u,i
pairs. Insuchacase,wecanwriter (1,0)asr (1)andLN (Rˆ)wouldreducetoL (Rˆ).
u,i u,i ideal ideal
4 UNBIASED ESTIMATION AND LEARNING UNDER INTERFERENCE
Inthissection,wefirstdiscusstheconsequenceofignoringtheneighborhoodeffect,andthenpropose
twonovelestimatorsforestimatingtheideallossLN (Rˆ). Moreover,wetheoreticallyanalyzethe
ideal
bias,variance,optimalbandwidth,andgeneralizationerrorboundsoftheproposedestimators.
Beforepresentingtheproposeddebiasingmethodsunderinterference,webrieflydiscusstheidentifi-
abilityoftheideallossLN (Rˆ). Acausalestimandissaidtobeidentifiableifitcanbewrittenasa
ideal
seriesofquantitiesthatcanbeestimatedfromobserveddata.
Assumption2(ConsistencyunderInterference). r =r (1,g)ifo =1andg =g.
u,i u,i u,i u,i
Assumption3(UnconfoundednessunderInterference). r (1,g)⊥⊥(o ,G )|x .
u,i u,i u,i u,i
These assumptions are common in causal inference to ensure the identifiability of causal effects.
Specifically, Assumption 2 implies that r (1,g) is observed only when o = 1 and g = g.
u,i u,i u,i
Assumption3indicatesthatthereisnounmeasuredconfounderthataffectsbothr and(o ,g ).
u,i u,i u,i
Theorem1(Identifiability). UnderAssumptions1–3,LN (Rˆ|g)andLN (Rˆ)areidentifiable.
ideal ideal
Theorem1ensurestheidentifiabilityoftheproposedideallossLN (Rˆ). LetEdenotetheexpecta-
ideal
tiononthetargetpopulationD,andp(·)denotestheprobabilitydensityfunctionofP.
4.1 EFFECTOFIGNORINGINTERFERENCE
The widely used ideal loss L (Rˆ) under no neighborhood effect is generally different from
ideal
the proposed ideal loss LN (Rˆ) in the presence of neighborhood effect. Next, we establish
ideal
the connection between these two loss functions, to deepen the understanding of the methods of
considering/ignoringneighborhoodeffect. Forbrevity,weletδ (g)=δ(rˆ ,r (1,g))hereafter.
u,i u,i u,i
Theorem2(LinktoSelectionBias). UnderAssumptions1–3,
(a)ifg ⊥⊥o |x ,LN (Rˆ)=L (Rˆ);
u,i u,i u,i ideal ideal
(b)ifg ⊥̸⊥o |x ,LN (Rˆ)−L (Rˆ)isequalto
u,i u,i u,i ideal ideal
(cid:90) (cid:104) (cid:110) (cid:111)(cid:105)
E E{δ (g)|x }· p(g =g|x )−p(g =g|x ,o =1) π(g)dg.
u,i u,i u,i u,i u,i u,i u,i
4PublishedasaconferencepaperatICLR2024
FromTheorem2(a),iftheindividualandneighborhoodexposuresareindependentconditionalonx ,
u,i
thenLN (Rˆ)isequaltoL (Rˆ),whichindicatesthattheexistingdebiasingmethodsneglecting
ideal ideal
neighborhoodeffectarealsounbiasedestimatorofLN (Rˆ). Thisisintuitivelyreasonablesince
ideal
insuchacase,theneighborhoodeffectrandomlyinfluenceso conditionalonx ,andtheeffect
u,i u,i
ofneighborswouldbesmoothedoutinanaveragesense. Theorem2(b)showsthatabiaswould
arisewheng ⊥̸⊥o |x ,andthebiasmainlydependsontheassociationbetweeno andg
u,i u,i u,i u,i u,i
conditionalonx ,i.e.,p(g =g|x =x)−p(g =g|x =x,o =1).
u,i u,i u,i u,i u,i u,i
4.2 PROPOSEDUNBIASEDESTIMATORS
ToderiveanunbiasedestimatorofLN (Rˆ),itsufficestofindanunbiasedestimatorofLN (Rˆ|g).
ideal ideal
MotivatedbySchnabeletal.(2016),anintuitivesolutionistotake(o ,g )asajointtreatment,then
u,i u,i
theIPSestimatorofLN (Rˆ|g)shouldbe|D|−1(cid:80) I{o =1,g =g}·δ (g)/p (g),
ideal (u,i)∈D u,i u,i u,i u,i
where I(·) is an indicator function, p (g) = p(o = 1,g = g|x ) is the propensity score.
u,i u,i u,i u,i
Clearly,thisstrategyworksifg isabinaryormulti-valuedrandomvariable. However,ifg has
u,i u,i
acontinuousprobabilitydensity,theaboveestimatorisnumericallyinfeasibleeveniftheoretically
feasible,sincealmostallI{o =1,g =g}willbezeroinsuchacase.
u,i u,i
To tackle this problem, we propose a novel kernel-smoothing based neighborhood IPS (N-IPS)
estimatorofLN (Rˆ|g),whichisgivenas
ideal
LN (Rˆ|g)=|D|−1 (cid:88) I(o u,i =1)·K((g u,i−g)/h)·δ u,i(g) ,
IPS h·p (g)
u,i
(u,i)∈D
wherehisabandwidth(smoothingparameter)andK(·)isasymmetrickernelfunction(Fanand
(cid:82) (cid:82)
Gijbels,1996;LiandRacine,2023;Wuetal.,2024)thatsatisfies K(t)dt=1and tK(t)dt=1.
For example, Epanechnikov kernel K(t) = 3(1−t2)I{|t| ≤ 1}/4 and Gaussian kernel K(t) =
√
exp(−t2/2)/ 2π fort ∈ R. Foreaseofpresentation,westatetheresultsforascalarg. Similar
conclusionscanbederivedformulti-dimensionalgandweputtheminAppendixC.
Similarly,thekernel-smoothingbasedneighborhoodDR(N-DR)estimatorcanbeconstructedby
LN (Rˆ|g)=|D|−1 (cid:88) (cid:104) δˆ (g)+ I(o u,i =1)·K((g u,i−g)/h)·{δ u,i(g)−δˆ u,i(g)}(cid:105) ,
DR u,i h·p (g)
u,i
(u,i)∈D
whereδˆ (g)=δ(rˆ ,m(x ,ϕ ))istheimputederrorofδ (g),andm(x ,ϕ )isanimputa-
u,i u,i u,i g u,i u,i g
tionmodelofr (1,g). Theimputationmodelistrainedbyminimizingthetrainingloss
u,i
LN−DR(Rˆ)=(cid:90) |D|−1 (cid:88) I(o u,i =1)·K((g u,i−g)/h)·(δ u,i(g)−δˆ u,i(g))2 π(g)dg.
e h·p (g)
u,i
(u,i)∈D
Then,thecorrespondingN-IPSandN-DRestimatorsofLN (Rˆ)aregivenas
ideal
(cid:90) (cid:90)
LN (Rˆ)= LN (Rˆ|g)π(g)dg, LN (Rˆ)= LN (Rˆ|g)π(g)dg. (1)
IPS IPS DR DR
Next,weshowthebiasandvarianceoftheproposedN-IPSandN-DRestimators,whichrelyona
standardassumptioninkernel-smoothingestimation(Härdleetal.,2004;LiandRacine,2023).
Assumption4(RegularityConditionsforKernelSmoothing). (a)h→0as|D|→∞;(b)|D|h→
∞as|D|→∞;(c)p(o =1,g =g |x )istwicedifferentiablewithrespecttog.
u,i u,i u,i
Theorem3(BiasandVarianceofN-IPSandN-DR). UnderAssumptions1–4,
(a)thebiasoftheN-DRestimatorisgivenas
1 (cid:90) (cid:104)∂2p(o =1,g =g|x ) (cid:105)
Bias(LN (Rˆ))= µ E u,i u,i u,i ·{δ (g)−δˆ (g)} π(g)dg·h2+o(h2),
DR 2 2 ∂g2 u,i u,i
whereµ =(cid:82) K(t)t2dt. ThebiasofN-IPSisprovidedinAppendixB.2;
2
5PublishedasaconferencepaperatICLR2024
(b)thevarianceoftheN-DRestimatorisgivenas
1 (cid:90) 1
Var(LN (Rˆ))= ψ(g)π(g)dg+o( ),
DR |D|h |D|h
whereψ(g)=(cid:82) 1 ·K¯(g−g′ )·{δ (g)−δˆ (g)}{δ (g′)−δˆ (g′)}π(g′)dg′isabounded
functionofg,K¯(p ·)u, =i(g (cid:82)′) K(t)Kh (·+t)u d,i
t.
Thevu a, ri ianceou f, Ni -IPSispu r,i
ovidedinAppendixB.2.
FromTheorem3(a),thekernel-smoothingbasedN-DRestimatorhasasmallbiasoforderO(h2),
whichconvergesto0as|D|→∞byAssumption4(a). Theorem3(b)showsthatthevarianceofthe
N-DRestimatorhasaconvergencerateoforderO(1/|D|h). Notably,thebandwidthhplaysakey
roleinthebias-variancetrade-offoftheN-DRestimator: thelargertheh,thelargerthebiasandthe
smallerthevariance. ThefollowingTheorem4givestheoptimalbandwidthforN-IPSandN-DR.
Theorem4(OptimalBandwidthofN-IPSandN-DR). UnderAssumptions1–4,theoptimalband-
widthfortheN-DRestimatorintermsoftheasymptoticmean-squarederroris
 1/5
(cid:82)
ψ(g)π(g)dg
h∗ =  ,
N−DR  4|D|(cid:16)
1µ (cid:82)
E(cid:104)
∂2p(ou,i=1,gu,i=g|xu,i) ·{δ (g)−δˆ
(g)}(cid:105) π(g)dg(cid:17)2
2 2 ∂g2 u,i u,i
whereψ(g)isdefinedinTheorem3. TheoptimalbandwidthforN-IPSisprovidedinAppendixB.3.
Theorem4showsthattheoptimalbandwidthofN-DRisoforderO(|D|−1/5). Insuchacase,
(cid:104) (cid:105)2 1
Bias(LN (Rˆ)) =O(h4)=O(|D|−4/5), Var(LN (Rˆ))=O( )=O(|D|−4/5),
DR DR |D|h
thatis,thesquareofthebiashasthesameconvergencerateasthevariance.
4.3 PROPENSITYESTIMATIONMETHOD
Different from previous debiasing methods in RS, in the presence of neighborhood effect, the
propensityisdefinedforjointtreatmentthatincludesabinaryvariableoandacontinuousvariable
g. Tofillthisgap,weconsideranovelmethodforpropensityestimation. LetPu(g |o=1,x)bea
uniformdistributiononG andequals1/cforallfeaturex. Notethat
1 1 c Pu(g |o=1,x)
= = · ,
p (g) P(o=1|x)P(g |o=1,x) P(o=1|x) P(g |o=1,x)
u,i
whereP(o=1|x)canbeestimatedbyusingtheexistingmethodssuchasnaiveBayesorlogistic
regressionwithorwithoutafewunbiasedratings,respectively(Schnabeletal.,2016). Toestimate
the density ratio Pu(g | o = 1,x)/P(g | o = 1,x), we first label the samples in the exposed
data {(x ,g )} as positive samples (L = 1), then uniformly sample treatments
g′ ∈ Gu t, oi geu n,i era{ t( eu s,i a) m:ou p,i le= s1} (cid:8) (x ,g′ )(cid:9) withnegativelabels(L = 0). Sincethe
u,i u,i u,i {(u,i):ou,i=1}
datageneratingprocessensuresthatPu(x|o=1)=P(x|o=1),wehave
Pu(g |o=1,x) Pu(x,g |o=1) P(x,g |L=0) P(L=1) P(L=0|x,g)
= = = · ,
P(g |o=1,x) P(x,g |o=1) P(x,g |L=1) P(L=0) P(L=1|x,g)
whereP(L=l|x,g)forl=0or1canbeobtainedbymodelingLwith(x,g).
4.4 FURTHERTHEORETICALANALYSIS
WefurthertheoreticallyanalyzethegeneralizationerrorboundsoftheproposedN-IPSandN-DR
estimators. LettingF bethehypothesisspaceofpredictionmatricesRˆ (orpredictionmodelf ),we
θ
definetheRademachercomplexity
(cid:104) 1 (cid:88) (cid:105)
R(F)=E sup σ δ (g) ,
σ∼{−1,+1}|D| |D| u,i u,i
fθ∈F
(u,i)∈D
whereσ ={σ :(u,i)∈D}isaRademachersequence(Mohrietal.,2018).
u,i
6PublishedasaconferencepaperatICLR2024
Assumption5(Boundedness). 1/p (g)≤M ,δ (g)≤M ,and|δ (g)−δˆ (g)|≤M .
u,i p u,i δ u,i u,i |δ−δˆ|
Theorem5givesthegeneralizationerrorboundsofthepredictionmodeltrainedbyminimizingour
proposedN-IPSandN-DRestimators.
Theorem5(GeneralizationErrorBoundsofN-IPSandN-DR). UnderAssumptions1–5andsuppose
thatK(t)≤M ,wehavewithprobabilityatleast1−η,
K
(cid:12)(cid:90) (cid:104)∂2p(o =1,g =g|x )(cid:105) (cid:12)
LN (Rˆ†)≤ minLN (Rˆ)+µ M (cid:12) E u,i u,i u,i π(g)dg(cid:12)·h2
ideal Rˆ∈F ideal 2 |δ−δˆ|(cid:12) ∂g2 (cid:12)
(cid:115)
+ 4M pM KR(F)+ 5M pM KM |δ−δˆ| 2 log(4 )+o(h2),
h h |D| η
whereRˆ† =argmin LN (Rˆ)isthelearnedpredictionmodelbyminimizingtheN-DRestimator.
Rˆ∈F DR
ThegeneralizationerrorboundsoftheN-IPSestimatorisprovidedinAppendixB.4.
5 SEMI-SYNTHETIC EXPERIMENTS
Weconductsemi-syntheticexperimentsusingtheMovieLens100K1(ML-100K)dataset,focusingon
thefollowingtworesearchquestions(RQs):RQ1.Dotheproposedestimatorsresultinmoreaccurate
estimationforideallosscomparedtothepreviousestimatorsinthepresenceofneighborhoodeffect?
RQ2. Howdoestheneighborhoodeffectstrengthaffecttheestimationaccuracy?
ExperimentalSetup2. TheML-100Kdatasetcontains100,000missing-not-at-random(MNAR)
ratings from 943 users to 1,682 movies. Following the previous studies (Schnabel et al., 2016;
Wang et al., 2019; Guo et al., 2021), we first complete the full rating matrix R by Matrix Fac-
torization (MF) (Koren et al., 2009), resulting in r ∈ {1,2,3,4,5}, and then set propensity
u,i
p
u,i
=pαmax(0,4−ru,i)withα=0.5tomodelMNAReffect (Wangetal.,2019;Guoetal.,2021).
Next,tomodeltheneighborhoodeffect,wecomputeg
=I((cid:80)
o ≥c)withvarying
u,i (u′,i′)∈N(u,i) u′,i′
cfor100,000observedMNARratings,whereN = {(u′,i′) ̸= (u,i) | u′ = uori′ = i}. In
(u,i)
(cid:80)
ourexperiment,cischosentobethemedianofall o for(u,i) ∈ D. Thenwe
(u′,i′)∈N(u,i) u′,i′
complete two full rating matrices Rg=0 and Rg=1 with r (1,g) ∈ {1,2,3,4,5} by MF, using
u,i
{(u,i)|o =1,g =0}and{(u,i)|o =1,g =1}respectively.
u,i u,i u,i u,i
ExperimentalDetails. Thecomputationoftheideallossneedsbothaground-truthratingmatrix
andapredictedratingmatrix. Therefore,wegeneratethefollowingsixpredictedmatricesRˆ:
•ONE:ThepredictedratingmatrixRˆ isidenticaltothetrueratingmatrix,exceptthat|{(u,i) |
r =5}|randomlyselectedtrueratingsof1areflippedto5. Thismeanshalfofthepredictedfives
u,i
aretruefive,andhalfaretrueone.
•THREE:SameasONE,butflippingtrueratingof3.
•FOUR:SameasONE,butflippingtrueratingof4.
•ROTATE:Foreachpredictedratingrˆ =r −1whenr ≥2,andrˆ =5whenr =1.
u,i u,i u,i u,i u,i
•SKEW:Predictedrˆ aresampledfromtheGaussiandistributionN(µ=r ,σ =(6−r )/2),
u,i u,i u,i
andclippedtotheinterval[1,5].
•CRS:Setrˆ =2ifr ≤3,otherwise,setrˆ =4.
u,i u,i u,i
Toconsidertheneighborhoodeffect, weassumethateachuser-itempairintheuniformdatahas
an equal probability of having g = 0 and g = 1, that is, π(g) = 0.5 for g ∈ {0,1}. Thus,
u,i u,i
LN (Rˆ) = |D|−1(cid:80) {δ(rˆ ,r (1,g = 0)) + δ(rˆ ,r (1,g = 1)}/2, where δ(·,·)
ideal (u,i)∈D u,i u,i u,i u,i
is the mean absolute error (MAE). We follow the previous studies (Guo et al., 2021; Li et al.,
2023b)toadoptrelativeabsoluteerror(RE)tomeasuretheestimationaccuracy,whichisdefinedas
RE(L )=|LN (Rˆ)−L (Rˆ)|/LN (Rˆ),whereL denotestheideallossestimationbythe
est ideal est ideal est
estimator. ThesmallertheRE,thehighertheestimationaccuracy(seeAppendixEformoredetails).
PerformanceAnalysis.Wetakethreepropensity-basedestimators:IPS,DR,andMRDRasbaselines
(seeSection6forbaselinesintroduction). TheresultsareshowninTable1. First,theREsofour
1https://grouplens.org/datasets/movielens/100k/
2Ourcodesanddatasetsareavailableathttps://github.com/haoxuanli-pku/ICLR24-Interference.
7PublishedasaconferencepaperatICLR2024
Table1: Relativeerroronsixpredictionmetrics. Thebestresultsarebolded.
ONE THREE FOUR ROTATE SKEW CRS
Naive 0.8612±0.0068 1.0011±0.0075 1.0471±0.0077 0.2781±0.0019 0.3538±0.0038 0.3419±0.0030
IPS 0.4766±0.0060 0.5501±0.0056 0.5731±0.0057 0.1434±0.0040 0.1969±0.0046 0.1885±0.0028
N-IPS 0.2383±0.0066 0.2670±0.0069 0.2829±0.0062 0.0417±0.0043 0.1024±0.0051 0.0966±0.0029
DR 0.4247±0.0088 0.4637±0.0093 0.4661±0.0096 0.0571±0.0021 0.1938±0.0043 0.0565±0.0020
N-DR 0.3089±0.0088 0.3533±0.0091 0.3577±0.0092 0.0339±0.0031 0.1219±0.0039 0.0511±0.0026
MRDR 0.2578±0.0070 0.2639±0.0071 0.2611±0.0073 0.1001±0.0025 0.1538±0.0038 0.0156±0.0021
N-MRDR 0.0622±0.0065 0.0520±0.0065 0.0503±0.0064 0.0456±0.0037 0.0672±0.0038 0.0042±0.0022
0.70 0.70
0.60 DR MRDR DR MRDR DR MRDR
N-DR N-MRDR 0.60 N-DR N-MRDR 0.60 N-DR N-MRDR
0.50
0.50 0.50
0.40
0.40 0.40
0.30
0.30 0.30
0.20 0.20 0.20
0.10 0.10 0.10
0.00 0.00 0.00
50 150250350 50 150250350 50 150250350 50 150250350 50 150250350 50 150250350
Interference strength Interference strength Interference strength
(a)RECONE (b)RECTHREE (c)RECFOUR
0.20 DR MRDR 0.20 DR MRDR 0.20 DR MRDR
N-DR N-MRDR N-DR N-MRDR N-DR N-MRDR
0.16 0.16 0.16
0.12 0.12 0.12
0.08 0.08 0.08
0.04 0.04 0.04
0.00 0.00 0.00
50 150250350 50 150250350 50 150250350 50 150250350 50 150250350 50 150250350
Interference strength Interference strength Interference strength
(d)ROTATE (e)SKEW (f)CRS
Figure2: TheeffectofmasknumbersasinterferencestrengthonREonsixpredictionmatrices.
estimatorsaresignificantlylowercomparedtothecorrespondingpreviousestimators,whichindicates
thatourestimatorsareabletoestimatetheideallossaccuratelyinthepresenceofneighborhoodeffect.
Inaddition,toinvestigatehowtheneighborhoodeffectaffectstheestimationerror,werandomlymask
someuserrowsanditemcolumnsbeforesamplingo ,whichresultsinp = 0forthemasked
u,i u,i
user-itempairs. Forunmaskeduser-itempairs,weraisetheirpropensitiessuchthattheexpectedtotal
numberofobservedsamplesremainsthesame,whichincreasestheproportionofobservedsamples
withg =1tostrengthentheneighborhoodeffect. Figure2showstheREoftheestimatorswith
u,i
varyingneighborhoodeffects. Ourmethodsstablyoutperformthepreviousmethodsinallscenarios,
whichverifiesthatourmethodsarerobusttotheincreasedneighborhoodeffect.
6 REAL-WORLD EXPERIMENTS
DatasetandExperimentDetails. Weverifytheeffectivenessoftheproposedestimatorsonthree
real-worlddatasets:Coatcontains6,960MNARratingsand4,640missing-at-random(MAR)ratings.
Yahoo! R3contains311,704MNARratingsand54,000MARratings. KuaiReccontains4,676,570
videowatchingratiorecordsfrom1,411usersfor3,327videos. Wepre-specifythreeneighborhood
choicesforauser-itempairinMNARdata: (1)usinguserhistoricalbehavior,(2)usingthepurchase
historyofanitem,and(3)usingtheinteractionofusersanditems,andletg betheneighborhood
u,i
numberoftheuser-itempair,whichisamulti-valuedrepresentation. Wereportthebestresultofour
methodsamongthethreeneighborhoodchoicesusingMSE,AUC,andNDCG@K astheevaluation
protocols,whereK = 5forCoatandYahoo! R3andK = 50forKuaiRec. Weadoptboththe
Gaussian kernel and Epanechnikov kernel as the kernel function for implementing our proposed
N-IPS,N-DR-JL,andN-MRDR(seeAppendixFformoredetails).
Baselines. WetakeMatrixFactorization(MF)(Korenetal.,2009)asthebasemodelandconsider
thefollowingdebiasingbaselines: IPS(Schnabeletal.,2016;Saitoetal.,2020),SNIPS(Schnabel
8
ER
ER
ER
ER
ER
ERPublishedasaconferencepaperatICLR2024
Table2: PerformanceofMSE,AUC,andNDCG@5onthreereal-worlddatasets. Thebestsixresults
arebolded,andthebestbaselineisunderlined.
Dataset Coat Yahoo!R3 KuaiRec
Method MSE↓ AUC↑ N@5↑ MSE↓ AUC↑ N@5↑ MSE↓ AUC↑ N@50↑
Basemodel(Korenetal.,2009) 0.238 0.710 0.616 0.249 0.682 0.634 0.137 0.754 0.553
+CVIB(Wangetal.,2020) 0.222 0.722 0.635 0.257 0.683 0.645 0.103 0.769 0.563
+DIB(Liuetal.,2021) 0.242 0.726 0.629 0.248 0.687 0.641 0.142 0.754 0.556
+SNIPS(Schnabeletal.,2016) 0.208 0.737 0.636 0.245 0.687 0.656 0.048 0.788 0.576
+ASIPS(Saito,2020a) 0.205 0.722 0.621 0.230 0.678 0.643 0.097 0.753 0.554
+DAMF(SaitoandNomura,2022) 0.218 0.734 0.643 0.245 0.697 0.656 0.097 0.775 0.572
+DR(Saito,2020b) 0.208 0.726 0.634 0.216 0.684 0.658 0.046 0.773 0.564
+DR-BIAS(Daietal.,2022) 0.223 0.717 0.631 0.220 0.689 0.654 0.046 0.771 0.552
+DR-MSE(Daietal.,2022) 0.214 0.720 0.630 0.222 0.689 0.657 0.047 0.769 0.547
+MR(Lietal.,2023a) 0.210 0.730 0.643 0.247 0.693 0.651 0.114 0.780 0.573
+TDR(Lietal.,2023b) 0.229 0.710 0.634 0.234 0.674 0.662 0.134 0.769 0.573
+TDR-JL(Lietal.,2023b) 0.216 0.734 0.639 0.248 0.684 0.654 0.121 0.771 0.560
+SDR(Lietal.,2023e) 0.208 0.736 0.642 0.210 0.690 0.655 0.116 0.775 0.574
+IPS(Schnabeletal.,2016) 0.214 0.718 0.626 0.221 0.681 0.644 0.097 0.752 0.554
+N-IPS[LR,Gaussian] 0.212 0.742 0.678 0.226 0.693 0.664 0.092 0.796 0.585
+N-IPS[LR,Epanechnikov] 0.224 0.746 0.645 0.242 0.703 0.673 0.094 0.794 0.582
+N-IPS[NB,Gaussian] 0.206 0.744 0.648 0.196 0.693 0.658 0.049 0.785 0.579
+N-IPS[NB,Epanechnikov] 0.210 0.753 0.646 0.197 0.685 0.653 0.047 0.755 0.562
+DR-JL(Wangetal.,2019) 0.211 0.721 0.620 0.224 0.682 0.646 0.050 0.764 0.526
+N-DR-JL[LR,Gaussian] 0.231 0.731 0.651 0.247 0.698 0.664 0.113 0.779 0.537
+N-DR-JL[LR,Epanechnikov] 0.235 0.741 0.655 0.251 0.693 0.663 0.108 0.784 0.552
+N-DR-JL[NB,Gaussian] 0.204 0.748 0.650 0.198 0.691 0.653 0.049 0.778 0.574
+N-DR-JL[NB,Epanechnikov] 0.209 0.744 0.648 0.191 0.681 0.637 0.046 0.786 0.570
+MRDR-JL(Guoetal.,2021) 0.214 0.721 0.631 0.215 0.686 0.650 0.047 0.777 0.554
+N-MRDR-JL[LR,Gaussian] 0.217 0.728 0.662 0.252 0.697 0.666 0.107 0.785 0.539
+N-MRDR-JL[LR,Epanechnikov] 0.233 0.734 0.656 0.253 0.695 0.666 0.097 0.791 0.560
+N-MRDR-JL[NB,Gaussian] 0.208 0.742 0.651 0.206 0.694 0.663 0.045 0.793 0.583
+N-MRDR-JL[NB,Epanechnikov] 0.207 0.756 0.635 0.194 0.690 0.644 0.044 0.802 0.587
etal.,2016),DR-JL(Wangetal.,2019),ASIPS(Saito,2020a),CVIB(Wangetal.,2020),DR(Saito,
2020b),MRDR-JL(Guoetal.,2021),DIB(Liuetal.,2021),DAMF(SaitoandNomura,2022),
DR-BIAS(Daietal.,2022),DR-MSE(Daietal.,2022),MR(Lietal.,2023a),Stable-DR(Lietal.,
2023e),TDR(Lietal.,2023b),andTDR-JL(Lietal.,2023b). Followingpreviousstudies(Schnabel
etal.,2016;Wangetal.,2019),forallbaselinemethodsrequiringpropensityestimation,weadopt
naiveBayes(NB)methodusing5%MARratingsfortrainingthepropensitymodel. Forourproposed
methods,wealsoadoptlogisticregression(LR)toestimatethepropensitieswithoutMARratings.
Real-World Debiasing Performance. Table 2 shows the performance of the baselines and our
methodsonthreedatasets. Comparedwiththebasemodel,thedebiasingmethodsachievebetter
performance. Notably, the proposed methods can stably outperform the baseline methods in all
metrics,showingthatourmethodscaneffectivelytaketheneighboreffectintoaccount. Thisalso
providesempiricalevidenceoftheexistenceoftheneighborhoodeffectinreal-worlddatasets. The
proposedmethodsshowcompetitiveperformancewhethertheMARdataareaccessible(NB)ornot
(LR),andperformsimilarlyinthecaseofadoptingtheGaussiankernelorEpanechnikovkernel.
7 CONCLUSION
Inthispaper,westudytheproblemofselectionbiasinthepresenceofneighborhoodeffect. First,
weformulatetheneighborhoodeffectinRSasaninterferenceproblemincausalinference. Next,
aneighborhoodtreatmentrepresentationvectorisintroducedtoreducethedimensionandsparsity
oftheneighborhoodtreatments. Basedonit, wereformulatethepotentialfeedbackandpropose
a novel ideal loss that can be used to deal with selection bias in the presence of neighborhood
effect. Then,weproposetwonovelkernel-smoothingbasedneighborhoodestimatorsfortheideal
loss,whichallowstheneighborhoodtreatmentrepresentationvectortohavecontinuousprobability
density. We systematically analyze the properties of the proposed estimators, including the bias,
variance, optimal bandwidth, and generalization error bounds. In addition, we also theoretically
establishtheconnectionbetweenthedebiasingmethodsconsideringandignoringtheneighborhood
effect. Extensiveexperimentsareconductedonsemi-syntheticandreal-worlddatatodemonstratethe
effectivenessofourapproaches. AlimitationofthisworkisthatthehypothesisspaceG ofgrelies
onpriorknowledge,anditisnotobvioustochooseitinpractice. Weleaveitforourfuturework.
9PublishedasaconferencepaperatICLR2024
8 ACKNOWLEDGEMENT
ThisworkwassupportedinpartbyNationalNaturalScienceFoundationofChina(No. 623B2002,
12301370).
REFERENCES
PeterM.AronowandCyrusSamii. Estimatingaveragecausaleffectsundergeneralinterference,with
applicationtoasocialnetworkexperiment. TheAnnalsofAppliedStatistics,11:1912–1947,2017.
ShuanghaoBai,MinZhang,WanqiZhou,SitengHuang,ZhirongLuan,DonglinWang,andBadong
Chen. Prompt-baseddistributionalignmentforunsuperviseddomainadaptation. InAAAI,2024.
JiaweiChen,HandeDong,YangQiu,XiangnanHe,XinXin,LiangChen,GuliLin,andKeping
Yang. Autodebias: Learningtodebiasforrecommendation. InSIGIR,2021a.
Jiawei Chen, Hande Dong, Xiang Wang, Fuli Feng, Meng Wang, and Xiangnan He. Bias and
debiasinrecommendersystem: Asurveyandfuturedirections. ACMTransactionsonInformation
Systems,41(3):1–39,2023.
MouxiangChen,ChenghaoLiu,JianlingSun,andStevenCHHoi.Adaptinginteractionalobservation
embeddingforcounterfactuallearningtorank. InSIGIR,2021b.
QuanyuDai,HaoxuanLi,PengWu,ZhenhuaDong,Xiao-HuaZhou,RuiZhang,XiuqiangHe,Rui
Zhang, andJieSun. Ageneralizeddoublyrobustlearningframeworkfordebiasingpost-click
conversionrateprediction. InKDD,2022.
SihaoDing,PengWu,FuliFeng,XiangnanHe,YitongWang,YongLiao,andYongdongZhang.
Addressingunmeasuredconfounderforrecommendationwithsensitivityanalysis. InKDD,2022.
JianqingFanandIreneGijbels. LocalPolynomialModellingandItsApplications. Chapmanand
Hall/CRC,1996.
MarcFerracci,GrégoryJolivet,andGerardJ.vandenBerg. Evidenceoftreatmentspilloverswithin
markets. ReviewofEconomicsandStatistics,96:812–823,2014.
LauraForastiere,EdoardoM.Airoldi,andFabriziaMealli. Identificationandestimationoftreatment
andinterferenceeffectsinobservationalstudiesonnetworks. JournaloftheAmericanStatistical
Association,116:901–918,2021.
Chongming Gao, Shijun Li, Wenqiang Lei, Jiawei Chen, Biao Li, Peng Jiang, Xiangnan He, Ji-
axinMao, andTat-SengChua. KuaiRec: Afully-observeddatasetandinsightsforevaluating
recommendersystems. InCIKM,2022.
SiyuanGuo,LixinZou,YidingLiu,WenwenYe,SuqiCheng,ShuaiqiangWang,HechangChen,
DaweiYin,andYiChang. Enhanceddoublyrobustlearningfordebiasingpost-clickconversion
rateestimation. InSIGIR,2021.
JoséMiguelHernández-Lobato,NeilHoulsby,andZoubinGhahramani. Probabilisticmatrixfactor-
izationwithnon-randommissingdata. InICML,2014.
GuangleiHongandStephenW.Raudenbush. valuatingkindergartenretentionpolicy:Acasestudyof
causalinferenceformultilevelobservationaldata. JournaloftheAmericanStatisticalAssociation,
101:901–910,2006.
JinHuang,HarrieOosterhuis,andMaartendeRijke. Itisdifferentwhenitemsareolder: Debiasing
recommendationswhenselectionbiasanduserpreferencesaredynamic. InWSDM,2022.
ShanshanHuang,HaoxuanLi,QingsongLi,ChunyuanZheng,andLiLiu. Paretoinvariantrepresen-
tationlearningformultimediarecommendation. InACM-MM,2023.
MichaelGHudgensandMElizabethHalloran. Towardcausalinferencewithinterference. Journal
oftheAmericanStatisticalAssociation,103:832–842,2008.
10PublishedasaconferencepaperatICLR2024
WolfgangHärdle,AxelWerwatz,MarleneMüller,andStefanSperlich. NonparametricandSemi-
parametricModels. SpringerSeriesinStatistics,2004.
Guido W. Imbens and Donald B. Rubin. Causal Inference For Statistics Social and Biomedical
Science. CambridgeUniversityPress,2015.
FredrikD.Johansson,UriShalit,andDavidSontag. Learningrepresentationsforcounterfactual
inference. InICML,2016.
YehudaKoren,RobertBell,andChrisVolinsky. Matrixfactorizationtechniquesforrecommender
systems. Computer,42(8):30–37,2009.
HaoxuanLi,QuanyuDai,YuruLi,YanLyu,ZhenhuaDong,Xiao-HuaZhou,andPengWu. Multiple
robustlearningforrecommendation. InAAAI,2023a.
HaoxuanLi,YanLyu,ChunyuanZheng,andPengWu.TDR-CL:Targeteddoublyrobustcollaborative
learningfordebiasedrecommendations. InICLR,2023b.
HaoxuanLi,YanghaoXiao,ChunyuanZheng,andPengWu. Balancingunobservedconfounding
withafewunbiasedratingsindebiasedrecommendations. InWWW,2023c.
HaoxuanLi,YanghaoXiao,ChunyuanZheng,PengWu,andPengCui. Propensitymatters: Measur-
ingandenhancingbalancingforrecommendation. InICML,2023d.
Haoxuan Li, Chunyuan Zheng, and Peng Wu. StableDR: Stabilized doubly robust learning for
recommendationondatamissingnotatrandom. InICLR,2023e.
HaoxuanLi,ChunyuanZheng,YanghaoXiao,HaoWang,FuliFeng,XiangnanHe,ZhiGeng,and
Peng Wu. Removing hidden confounding in recommendation: A unified multi-task learning
approach. InNeurIPS,2023f.
HaoxuanLi,ChunyuanZheng,YanghaoXiao,PengWu,ZhiGeng,XuChen,andPengCui.Debiased
collaborativefilteringwithkernel-basedcausalbalancing. InICLR,2024.
Qi Li and Jeffrey Scott Racine. Nonparametric econometrics: theory and practice. Princeton
UniversityPress,2023.
DugangLiu,PengxiangCheng,HongZhu,ZhenhuaDong,XiuqiangHe,WeikePan,andZhong
Ming. Mitigatingconfoundingbiasinrecommendationviainformationbottleneck. InRecSys,
2021.
HuishiLuo,FuzhenZhuang,RuobingXie,HengshuZhu,DeqingWang,ZhulinAn,andYongjunXu.
Asurveyoncausalinferenceforrecommendation. TheInnovation,2024.
ZheqiLv,WenqiaoZhang,ShengyuZhang,KunKuang,FengWang,YongweiWang,ZhengyuChen,
TaoShen,HongxiaYang,BengChinOoi,etal. Duet: Atuning-freedevice-cloudcollaborative
parametersgenerationframeworkforefficientdevicemodelgeneralization. InWWW,2023.
ZheqiLv,WenqiaoZhang,ZhengyuChen,ShengyuZhang,andKunKuang.Intelligentmodelupdate
strategyforsequentialrecommendation. InWWW,2024.
XiaoMa,LiqinZhao,GuanHuang,ZhiWang,ZelinHu,XiaoqiangZhu,andKunGai. Entirespace
multi-taskmodel: Aneffectiveapproachforestimatingpost-clickconversionrate. InSIGIR,2018.
BenjaminMMarlinandRichardSZemel. Collaborativepredictionandrankingwithnon-random
missingdata. InRecSys,2009.
MehryarMohri,AfshinRostamizadeh,andAmeetTalwalkar. FoundationsofMachineLearning.
MITPress,2018.
JerzySplawaNeyman. Ontheapplicationofprobabilitytheorytoagriculturalexperiments.essayon
principles.section9. StatisticalScience,5:465–472,1990.
ElizabethL.OgburnandTylerJ.VanderWeele. Causaldiagramsforinterference. StatisticalScience,
29:559–578,2014.
11PublishedasaconferencepaperatICLR2024
ElizabethL.OgburnandTylerJ.VanderWeele. Vaccines,contagion,andsocialnetworks. TheAnnals
ofAppliedStatistics,11:919–948,2017.
DonaldB.Rubin. Estimatingcausaleffectsoftreatmentsinrandomizedandnonrandomizedstudies.
JournalofEducationalPsychology,66:688–701,1974.
DonaldB.Rubin. Discussionofrandomizationanalysisofexperimentaldatainthefisherrandomiza-
tiontestbybasu. JournaloftheAmericanStatisticalAssociation,75:591–593,1980.
YutaSaito. Asymmetrictri-trainingfordebiasingmissing-not-at-randomexplicitfeedback. InSIGIR,
2020a.
YutaSaito. Doublyrobustestimatorforrankingmetricswithpost-clickconversions. InRecSys,
2020b.
YutaSaitoandMasahiroNomura.Towardsresolvingpropensitycontradictioninofflinerecommender
learning. InIJCAI,2022.
Yuta Saito, Suguru Yaginuma, Yuta Nishino, Hayato Sakata, and Kazuhide Nakata. Unbiased
recommenderlearningfrommissing-not-at-randomimplicitfeedback. InWSDM,2020.
TobiasSchnabel,AdithSwaminathan,AshudeepSingh,NavinChandak,andThorstenJoachims.
Recommendationsastreatments: Debiasinglearningandevaluation. InICML,2016.
Shai Shalev-Shwartz and Shai Ben-David. Understanding Machine Learning: From Theory to
Algorithms. CambridgeUniversityPress,2014.
MichaelESobel. Whatdorandomizedstudiesofhousingmobilitydemonstrate? causalinferencein
thefaceofinterference. JournaloftheAmericanStatisticalAssociation,101:1398–1407,2006.
HaraldSteck. Trainingandtestingofrecommendersystemsondatamissingnotatrandom. InKDD,
2010.
AdithSwaminathanandThorstenJoachims.Theself-normalizedestimatorforcounterfactuallearning.
InNeurIPS,2015.
FredrikSävje,PeterM.Aronow,andPeterM.Aronows. Averagetreatmenteffectsinthepresenceof
unknowninterference. AnnalsofStatistics,49:673–701,2021.
Eric J. Tchetgen Tchetgen and Tyler J. VanderWeele. On causal inference in the presence of
interference. StatisticalMethodsinMedicalResearch,21:55–75,2012.
EricJ.TchetgenTchetgen,IsabelR.Fulcher,andIlyaShpitser. Auto-g-computationofcausaleffects
onanetwork. JournaloftheAmericanStatisticalAssociation,116:833–844,2021.
HaoWang,Tai-WeiChang,TianqiaoLiu,JianminHuang,ZhichaoChen,ChaoYu,RuopengLi,and
WeiChu. ESCM2: Entirespacecounterfactualmulti-taskmodelforpost-clickconversionrate
estimation. InSIGIR,2022a.
HaoWang,JiajunFan,ZhichaoChen,HaoxuanLi,WeimingLiu,TianqiaoLiu,QuanyuDai,Yichao
Wang,ZhenhuaDong,andRuimingTang. Optimaltransportfortreatmenteffectestimation. In
NeurIPS,2023a.
HaotianWang,WenjingYang,LongqiYang,AnpengWu,LiyangXu,JingRen,FeiWu,andKun
Kuang. Estimatingindividualizedcausaleffectwithconfoundedinstruments. InKDD,2022b.
Haotian Wang, Kun Kuang, Haoang Chi, Longqi Yang, Mingyang Geng, Wanrong Huang, and
WenjingYang. Treatmenteffectestimationwithadjustmentfeatureselection. InKDD,2023b.
HaotianWang,KunKuang,LongLan,ZigeWang,WanrongHuang,FeiWu,andWenjingYang.
Out-of-distributiongeneralizationwithcausalfeatureseparation.IEEETransactionsonKnowledge
andDataEngineering,36(4):1758–1772,2024.
12PublishedasaconferencepaperatICLR2024
JunWang,HaoxuanLi,ChiZhang,DongxuLiang,EnyunYu,WenwuOu,andWenjiaWang. Coun-
terCLR:Counterfactualcontrastivelearningwithnon-randommissingdatainrecommendation. In
ICDM,2023c.
WenjieWang,YangZhang,HaoxuanLi,PengWu,FuliFeng,andXiangnanHe. Causalrecommen-
dation: Progressesandfuturedirections. InSIGIR,2023d.
XiaojieWang,RuiZhang,YuSun,andJianzhongQi. Doublyrobustjointlearningforrecommenda-
tionondatamissingnotatrandom. InICML,2019.
XiaojieWang,RuiZhang,YuSun,andJianzhongQi. Combatingselectionbiasesinrecommender
systemswithafewunbiasedratings. InWSDM,2021.
Zifeng Wang, Xi Chen, Rui Wen, Shao-Lun Huang, Ercan E. Kuruoglu, and Yefeng Zheng. In-
formationtheoreticcounterfactuallearningfrommissing-not-at-randomfeedback. InNeurIPS,
2020.
PengWu,HaoxuanLi,YuhaoDeng,WenjieHu,QuanyuDai,ZhenhuaDong,JieSun,RuiZhang,and
Xiao-HuaZhou. Ontheopportunityofcausallearninginrecommendationsystems: Foundation,
estimation,predictionandchallenges. InIJCAI,2022.
PengWu,ShaShaHan,XingweiTong,andRunzeLi.Propensityscoreregressionforcausalinference
withtreatmentheterogeneity. StatisticaSinica,34:747–769,2024.
Min Zhang, Junkun Yuan, Yue He, Wenbin Li, Zhengyu Chen, and Kun Kuang. Map: Towards
balancedgeneralizationofiidandoodthroughmodel-agnosticadapters. InICCV,2023.
Min Zhang, Haoxuan Li, Fei Wu, and Kun Kuang. MetaCoCo: A new few-shot classification
benchmarkwithspuriouscorrelation. InICLR,2024.
Wenhao Zhang, Wentian Bao, Xiao-Yang Liu, Keping Yang, Quan Lin, Hong Wen, and Ramin
Ramezani. Large-scalecausalapproachestodebiasingpost-clickconversionrateestimationwith
multi-tasklearning. InWWW,2020.
YuZheng,ChenGao,XiangLi,XiangnanHe,DepengJin,andYongLi. Disentanglinguserinterest
andconformityforrecommendationwithcausalembedding. InWWW,2021.
HaoZou,HaotianWang,RenzheXu,BoLi,JianPei,YeJunJian,andPengCui. Factualobservation
basedheterogeneitylearningforcounterfactualprediction. InCLeaR,2023.
13PublishedasaconferencepaperatICLR2024
A RELATED WORK
SelectionBiasinRecommenderSystem. Recommendersystem(RS)playsanimportantroleinthe
currenteraofinformationexplosion(Huangetal.,2023;Lvetal.,2023;2024). However,dueto
theself-selectionbytheusers,thedatacollectedbyRSwilloftencontainselectionbias,whichwill
resultinthedistributionoftrainingdata(observedpopulation)differentfromthatoftestdata(target
population),leadingtothechallengesofachievingunbiasedlearning(Wangetal.,2022b;2023b;Zou
etal.,2023;Wangetal.,2023a;2024). Ifwelearnthemodeldirectlyonthetrainingdatawithout
debiasing,itwillresultinasub-optimalperformanceonthetestdata(Wangetal.,2023c;Zhangetal.,
2023;Baietal.,2024;Zhangetal.,2024). Manymethodshavebeendevelopedtoaddressselection
bias(Wuetal.,2022;Wangetal.,2023d;Luoetal.,2024),forinstance,MarlinandZemel(2009);
Steck(2010)discussedtheerror-imputation-basedmethods,Schnabeletal.(2016)recommended
usingtheinversepropensityscoring(IPS)methodforunbiasedlearning. Wangetal.(2019)proposed
a doubly robust (DR) joint learning method and achieved superior performance. Subsequently,
various novel model structures and algorithms are designed to enhance the DR method, such as
Guoetal.(2021);Daietal.(2022);Lietal.(2024),whichproposednewDRmethodsbyfurther
reducingthebiasorvarianceoftheDRestimator. Zhangetal.(2020)proposedmulti-tasklearning
throughparametersharingbetweenthepropensityandpredictionmodel,Chenetal.(2021a);Wang
etal.(2021);Lietal.(2023c)proposedusingasmalluniformdatasettoenhancetheperformance
ofpredictionmodels,andDingetal.(2022)proposedanadversariallearning-basedframeworkto
addresstheunmeasuredconfounders. However,auser’sfeedbackonanitemmayreceiveinfluence
fromtheotheruser-itempairs(Zhengetal.,2021). Tofillthisgap,Chenetal.(2021b)focusesonthe
taskoflearningtorank(LTR),addressingpositionbiasusingimplicitfeedbackdata. Theyconsider
otheruser-iteminteractionsasconfoundersfromacounterfactualperspectiveanduseembeddingasa
proxyconfoundertocapturetheinfluenceofotheruser-iteminteractions. DifferentfromChenetal.
(2021b),ourpaperfocusesontheselectionbiasinthecontextofratingprediction,andregardsother
user-iteminteractionsasanewtreatmentfromtheperspectiveofinterferenceincausalinference. We
formulatetheinfluenceofotheruser-iteminteractionsasaninterferenceproblemincausalinference
andintroduceatreatmentrepresentationtocapturetheinfluence. Onthisbasis,weproposeanovel
ideallossthatcanbeusedtodealwithselectionbiasinthepresenceofinterference.
InterferenceinCausalInference. Interferenceisacommonprobleminobservationalstudies,and
the effects of interference are also called spillover effects in economics or peer effects in social
sciences(Forastiereetal.,2021). Earlyliteraturefocusesonthecaseofpartialinterference(Hong
and Raudenbush, 2006; Sobel, 2006; Hudgens and Halloran, 2008; Tchetgen and VanderWeele,
2012;Ferraccietal.,2014),i.e.,thesamplecanbedividedintomultiplegroups,withinterference
betweenunitsinthesamegroup,whileunitsbetweengroupsareindependent. Recentworkshave
attempted to further relax the partial interference assumption by allowing for a wide variety of
interference patterns (Ogburn and VanderWeele, 2014; Aronow and Samii, 2017), such as direct
interference(Forastiereetal.,2021),interferencebycontagion(OgburnandVanderWeele,2017),
allocationalinterference(OgburnandVanderWeele,2014),ortheirhybrids(Tchetgenetal.,2021).
Thesestudiesdifferfromoursinseveralimportantways: (1)theirgoalistoestimatethemaineffect
andneighborhoodeffect,whileourgoalistoachieveunbiasedlearningofthepredictionmodelthat
ismorechallengingandneedtocarefullydesignthelossaswellasthetrainingalgorithmtomitigate
theneighborhoodeffect;(2)theydonotusetreatmentrepresentationandtheestimationdoesnottake
intoaccountthepossiblecasesofcontinuousandmulti-dimensionalrepresentations.
B PROOFS
B.1 PROOFSOFTHEOREMS1AND2
Recallthatp (g)=P(o =1,g =g|x )andrˆ =f (x )arefunctionsofx ,δ (g)=
u,i u,i u,i u,i u,i θ u,i u,i u,i
δ(rˆ ,r (1,g)),PandEdenotethedistributionandexpectationonthetargetpopulationD,and
u,i u,i
p(·)denotestheprobabilitydensityfunctionofP.
Theorem1(Identifiability). UnderAssumptions1–3,LN (Rˆ|g)andLN (Rˆ)areidentifiable.
ideal ideal
14PublishedasaconferencepaperatICLR2024
ProofofTheorem1. Since
(cid:90)
LN (Rˆ)= LN (Rˆ|g)π(g)dg,
ideal ideal
it suffices to show that LN (Rˆ|g) is identifiable. This follows immediately from the following
ideal
equations
LN (Rˆ|g)=E[δ(rˆ ,r (1,g))]
ideal u,i u,i
(cid:104) (cid:110) (cid:111)(cid:105)
=E E δ(rˆ ,r (1,g))|x (thelawofiteratedexpectations)
u,i u,i u,i
(cid:104) (cid:110) (cid:111)(cid:105)
=E E δ(rˆ ,r (1,g))|x ,o =1,g =g (Assumption3)
u,i u,i u,i u,i u,i
(cid:104) (cid:110) (cid:111)(cid:105)
=E E δ(rˆ ,r )|x ,o =1,g =g (Assumption2)
u,i u,i u,i u,i u,i
(cid:90) (cid:90)
= δ(rˆ ,r )p(r |x ,o =1,g =g)p(x )dr dx .
u,i u,i u,i u,i u,i u,i u,i u,i u,i
Theorem2(LinktoSelectionBias). UnderAssumptions1–3,
(a)ifg ⊥⊥o |x ,LN (Rˆ)=L (Rˆ);
u,i u,i u,i ideal ideal
(b)ifg ⊥̸⊥o |x ,LN (Rˆ)−L (Rˆ)equals
u,i u,i u,i ideal ideal
(cid:90) (cid:104) (cid:110) (cid:111)(cid:105)
E E{δ (g)|x }· p(g =g|x )−p(g =g|x ,o =1) π(g)dg.
u,i u,i u,i u,i u,i u,i u,i
ProofofTheorem2. For previous methods addressing selection bias without taking into account
interference,theideallossL (Rˆ)is
ideal
L (Rˆ)=E[δ(rˆ ,r (1))]
ideal u,i u,i
=E[E{δ(rˆ ,r (1))|x }]
u,i u,i u,i
=E[E{δ(rˆ ,r )|x ,o =1}]
u,i u,i u,i u,i
(cid:104) (cid:110) (cid:111) (cid:105)
=E E δ(rˆ ,r )|x ,o =1,g =g ·p(g =g|x ,o =1)
u,i u,i u,i u,i u,i u,i u,i u,i
(cid:104) (cid:110) (cid:111) (cid:105)
=E E δ (g)|x ,o =1,g =g ·p(g =g|x ,o =1)
u,i u,i u,i u,i u,i u,i u,i
(cid:104) (cid:110) (cid:111) (cid:105)
=E E δ (g)|x ·p(g =g|x ,o =1) .
u,i u,i u,i u,i u,i
Fortheproposedmethodaddressingselectionbiasunderinterference,ournewlydefinedidealloss
LN (Rˆ)is
ideal
(cid:90)
LN (Rˆ)= E[δ(rˆ ,r (1,g))]π(g)dg
ideal u,i u,i
(cid:90)
= E[E{δ (g)|x }]π(g)dg
u,i u,i
(cid:90) (cid:104) (cid:110) (cid:111) (cid:105)
= E E δ (g)|x ,g =g ·p(g =g |x ) π(g)dg
u,i u,i u,i u,i u,i
(cid:90) (cid:104) (cid:110) (cid:111) (cid:105)
= E E δ (g)|x ·p(g =g |x ) π(g)dg.
u,i u,i u,i u,i
Theorem2(b)followsimmediatelyfromthesetworewrittenequations. Wheng ⊥⊥ o | x ,
u,i u,i u,i
wehavep(g = g | x = x,o = 1) = p(g = g | x = x),whichleadstoLN (Rˆ) =
u,i u,i u,i u,i u,i ideal
L (Rˆ). ThiscompletestheproofofTheorem2(a).
ideal
15PublishedasaconferencepaperatICLR2024
B.2 PROOFOFTHEOREM3
Theorem3(BiasandVarianceofN-IPSandN-DR). UnderAssumptions1–4,
(a)thebiasoftheN-DRestimatorisgivenas
1 (cid:90) (cid:104)∂2p(o =1,g =g|x ) (cid:105)
Bias(LN (Rˆ))= µ E u,i u,i u,i ·{δ (g)−δˆ (g)} π(g)dg·h2+o(h2),
DR 2 2 ∂g2 u,i u,i
whereµ =(cid:82) K(t)t2dt. ThebiasoftheN-IPSestimatorisgivenas
2
1 (cid:90) (cid:104)∂2p(o =1,g =g|x ) (cid:105)
Bias(LN (Rˆ))= µ E u,i u,i u,i ·δ (g) π(g)dg·h2+o(h2);
IPS 2 2 ∂g2 u,i
(b)thevarianceoftheN-DRestimatorisgivenas
1 (cid:90) 1
Var(LN (Rˆ))= ψ(g)π(g)dg+o( ),
DR |D|h |D|h
where
(cid:90) 1 g−g′
ψ(g)= ·K¯( )·{δ (g)−δˆ (g)}{δ (g′)−δˆ (g′)}π(g′)dg′
p (g′) h u,i u,i u,i u,i
u,i
is a bounded function of g, K¯(·) = (cid:82) K(t)K(·+t)dt. The variance of the N-IPS estimator is
givenas
1 (cid:90) 1
Var(LN (Rˆ))= φ(g)π(g)dg+o( ),
IPS |D|h |D|h
where
(cid:90) 1 g−g′
φ(g)= ·K¯( )·δ (g)δ (g′)π(g′)dg′
p (g′) h u,i u,i
u,i
isaboundedfunctionofg.
ProofofTheorem3. (a) We first show the biasof the N-IPS estimator, and the bias of theN-DR
estimatorcanbeshownsimilarly. Foragiveng,wehave
(cid:20)I(o
=1)·K((g −g)/h)·δ
(g)(cid:21)
E[LN (Rˆ|g)]=E u,i u,i u,i
IPS h·p (g)
u,i
(cid:20) 1 (cid:110) 1 (cid:18) g −g(cid:19)(cid:12) (cid:111) (cid:21)
=E E o · K u,i (cid:12)x ·E{δ (g)|x }
p (g) u,i h h (cid:12) u,i u,i u,i
u,i
(cid:20) 1 (cid:90) (cid:90) 1 g −g (cid:21)
=E o K( u,i )p(o ,g |x )do dg ·E{δ (g)|x }
p (g) u,ih h u,i u,i u,i u,i u,i u,i u,i
u,i
(cid:20) 1 (cid:90) 1 g −g (cid:21)
=E K( u,i )p(o =1,g |x )dg ·E{δ (g)|x } ,
p (g) h h u,i u,i u,i u,i u,i u,i
u,i
16PublishedasaconferencepaperatICLR2024
wherethesecondequationfollowsfromAssumption3. Lettingt=(g −g)/h,wehaveg =
u,i u,i
g+htanddg =hdt,andthen
u,i
(cid:20) 1 (cid:90) 1 g −g (cid:21)
E K( u,i )p(o =1,g |x )dg ·E{δ (g)|x }
p (g) h h u,i u,i u,i u,i u,i u,i
u,i
(cid:20) 1 (cid:90) (cid:21)
=E K(t)p(o =1,g+ht|x )dt·E{δ (g)|x }
p (g) u,i u,i u,i u,i
u,i
(cid:104) 1 (cid:90) (cid:110) ∂p(o =1,g|x )
=E K(t) p(o =1,g|x )+ u,i u,i ht
p (g) u,i u,i ∂g
u,i
∂2p(o =1,g|x )h2t2 (cid:111) (cid:105)
+ u,i u,i +o(h2) dt·E{δ (g)|x }
∂g2 2 u,i u,i
(cid:20) 1 (cid:90) (cid:21)
=E K(t)dt·p(o =1,g|x )·E{δ (g)|x }
p (g) u,i u,i u,i u,i
u,i
(cid:104) 1 (cid:90) ∂p(o =1,g|x ) (cid:105)
+E K(t)tdt· u,i u,i h·E{δ (g)|x }
p (g) ∂g u,i u,i
u,i
(cid:104) 1 (cid:90) ∂2p(o =1,g|x )h2 (cid:105)
+E K(t)t2dt· u,i u,i ·E{δ (g)|x } +o(h2)
p (g) ∂g2 2 u,i u,i
u,i
1 (cid:104)∂2p(o =1,g|x ) (cid:105)
=E[E{δ (g)|x }]+ µ E u,i u,i ·δ (g) h2+o(h2)
u,i u,i 2 2 ∂g2 u,i
1 (cid:104)∂2p(o =1,g|x ) (cid:105)
=E[δ (g)]+ µ E u,i u,i ·δ (g) h2+o(h2)
u,i 2 2 ∂g2 u,i
1 (cid:104)∂2p(o =1,g|x ) (cid:105)
=LN (Rˆ|g)+ µ E u,i u,i ·δ (g) h2+o(h2),
ideal 2 2 ∂g2 u,i
wherethethirdequationisaTaylorexpansionofp(o =1,g+ht|x )underAssumption4(a).
u,i u,i
Thus,thebiasofLN (Rˆ)is
IPS
E[LN (Rˆ)]−LN (Rˆ)
IPS ideal
(cid:20)(cid:90) (cid:21)
=E (cid:8) LN (Rˆ|g)−LN (Rˆ|g)(cid:9) π(g)dg
IPS ideal
g
(cid:90)
= E(cid:8) LN (Rˆ|g)−LN (Rˆ|g)(cid:9) π(g)dg
IPS ideal
g
1 (cid:90) (cid:104)∂2p(o =1,g|x ) (cid:105)
= µ E u,i u,i ·δ (g) π(g)dg·h2+o(h2).
2 2 ∂g2 u,i
Likewise,foragivengandδˆ (g),byasimilarargumentofproofofthebiasofN-IPSestimator,
u,i
E[LN (Rˆ|g)]
DR
(cid:34) (cid:35)
I(o =1)·K((g −g)/h)·{δ (g)−δˆ (g)}
=E δ (g)+ u,i u,i u,i u,i
u,i h·p (g)
u,i
(cid:20) 1 (cid:110) 1 (cid:18) g −g(cid:19)(cid:12) (cid:111) (cid:21)
=LN (Rˆ|g)+E E o · K u,i (cid:12)x ·E{δ (g)−δˆ (g)|x }
ideal p (g) u,i h h (cid:12) u,i u,i u,i u,i
u,i
1 (cid:104)∂2p(o =1,g|x ) (cid:105)
=LN (Rˆ|g)+ µ E u,i u,i ·{δ (g)−δˆ (g)} h2+o(h2).
ideal 2 2 ∂g2 u,i u,i
Thus,thebiasofLN (Rˆ)isgivenas
DR
1 (cid:90) (cid:104)∂2p(o =1,g|x ) (cid:105)
µ E u,i u,i ·{δ (g)−δˆ (g)} π(g)dg·h2+o(h2).
2 2 ∂g2 u,i u,i
17PublishedasaconferencepaperatICLR2024
(b)Bydefinition,thevarianceofLN (Rˆ)canberepresentedas
IPS
Var(LN (Rˆ))
IPS
(cid:20)(cid:90) (cid:21)
=Var LN (Rˆ|g)π(g)dg
IPS
 
=Var |D1
|
(cid:88) (cid:90) I(o pu,i (= g)1) · h1 K(cid:18) g u,i h−g(cid:19) ·δ u,i(g)π(g)dg
u,i
(u,i)∈D
1 (cid:20)(cid:90) I(o =1) 1 (cid:18) g −g(cid:19) (cid:21)
= Var u,i · K u,i ·δ (g)π(g)dg
|D| p (g) h h u,i
u,i
1 (cid:34) (cid:40)(cid:18)(cid:90) I(o =1) 1 (cid:18) g −g(cid:19) (cid:19)2(cid:41)
= E u,i · K u,i ·δ (g)π(g)dg
|D| p (g) h h u,i
u,i
(cid:26) (cid:18)(cid:90) I(o =1) 1 (cid:18) g −g(cid:19) (cid:19)(cid:27)2(cid:35)
− E u,i · K u,i ·δ (g)π(g)dg . (A.1)
p (g) h h u,i
u,i
AccordingtotheaboveresultofthebiasoftheN-IPSestimator,wehave
(cid:26) (cid:18)(cid:90) I(o =1) 1 (cid:18) g −g(cid:19) (cid:19)(cid:27)2
E u,i · K u,i ·δ (g)π(g)dg
p (g) h h u,i
u,i
(cid:26) 1 (cid:90) (cid:104)∂2p(o =1,g|x ) (cid:105) (cid:27)2
= LN (Rˆ)+ µ E u,i u,i ·δ (g) π(g)dg·h2+o(h2)
ideal 2 2 ∂g2 u,i
=[LN (Rˆ)]2+O(h2). (A.2)
ideal
Then,wefocusonanalyzingthefollowingterm
(cid:40)(cid:18)(cid:90) I(o =1) 1 (cid:18) g −g(cid:19) (cid:19)2(cid:41)
E u,i · K u,i ·δ (g)π(g)dg .
p (g) h h u,i
u,i
Wecanobservethat
(cid:18)(cid:90) I(o =1) 1 (cid:18) g −g(cid:19) (cid:19)2
u,i · K u,i ·δ (g)π(g)dg
p (g) h h u,i
u,i
(cid:18)(cid:90) I(o =1) 1 (cid:18) g −g(cid:19) (cid:19)
= u,i · K u,i ·δ (g)π(g)dg
p (g) h h u,i
u,i
(cid:18)(cid:90) I(o =1) 1 (cid:18) g −g′(cid:19) (cid:19)
· u,i · K u,i ·δ (g′)π(g′)dg′
p (g′) h h u,i
u,i
(cid:90) (cid:90) I(o =1) 1 (cid:18) g −g(cid:19) (cid:18) g −g′(cid:19)
= u,i · K u,i K u,i ·δ (g)δ (g′)π(g)π(g′)dgdg′,
p (g)p (g′) h2 h h u,i u,i
u,i u,i
then,weswaptheorderofintegrationandexpectation,whichleadstothat
(cid:40)(cid:18)(cid:90) I(o =1) 1 (cid:18) g −g(cid:19) (cid:19)2(cid:41)
E u,i · K u,i ·δ (g)π(g)dg
p (g) h h u,i
u,i
(cid:90) (cid:90) (cid:20) I(o =1) 1 (cid:18) g −g(cid:19) (cid:18) g −g′(cid:19) (cid:21)
= E u,i · K u,i K u,i ·δ (g)δ (g′) π(g)π(g′)dgdg′.
p (g)p (g′) h2 h h u,i u,i
u,i u,i
18PublishedasaconferencepaperatICLR2024
Letg =g+ht,theng −g′ =(g−g′)+ht. Wehave
u,i u,i
(cid:20) I(o =1) 1 (cid:18) g −g(cid:19) (cid:18) g −g′(cid:19) (cid:21)
E u,i · K u,i K u,i ·δ (g)δ (g′)
p (g)p (g′) h2 h h u,i u,i
u,i u,i
(cid:20) 1 (cid:26) 1 (cid:18) g −g(cid:19) (cid:18) g −g′(cid:19)(cid:12) (cid:27) (cid:110) (cid:12) (cid:111)(cid:21)
=E ·E I(o =1) K u,i K u,i (cid:12)x ·E δ (g)δ (g′)(cid:12)x
p (g)p (g′) u,i h2 h h (cid:12) u,i u,i u,i (cid:12) u,i
u,i u,i
(cid:20) 1 (cid:90) (cid:26) 1 (cid:18) g−g′ (cid:19) (cid:27) (cid:110) (cid:12) (cid:111)(cid:21)
=E · K(t)K +t p(o =1,g+ht|x ) dt·E δ (g)δ (g′)(cid:12)x
p (g)p (g′) h h u,i u,i u,i u,i (cid:12) u,i
u,i u,i
(cid:20) 1 (cid:90) (cid:26) 1 (cid:18) g−g′ (cid:19) (cid:27) (cid:110) (cid:12) (cid:111)(cid:21)
=E · K(t)K +t p(o =1,g|x )+O(h)t dt·E δ (g)δ (g′)(cid:12)x
p (g)p (g′) h h u,i u,i u,i u,i (cid:12) u,i
u,i u,i
(cid:20) 1 (cid:90) 1 (cid:18) g−g′ (cid:19) (cid:110) (cid:12) (cid:111)(cid:21)
=E · K(t)K +t dt·E δ (g)δ (g′)(cid:12)x ·{1+O(h)}
p (g′) h h u,i u,i (cid:12) u,i
u,i
(cid:20) 1 (cid:90) 1 (cid:18) g−g′ (cid:19) (cid:21)
=E · K(t)K +t dt·δ (g)δ (g′) ·{1+O(h)}.
p (g′) h h u,i u,i
u,i
(cid:16) (cid:17)
Denote(cid:82)
K(t)K
g−g′
+t
dt=K¯(g−g′
),then
h h
(cid:40)(cid:18)(cid:90) I(o =1) 1 (cid:18) g −g(cid:19) (cid:19)2(cid:41)
E u,i · K u,i ·δ (g)π(g)dg
p (g) h h u,i
u,i
(cid:90) (cid:90) (cid:20) 1 1 g−g′ (cid:21)
= E · K¯( )·δ (g)δ (g′) ·{1+O(h)}.π(g)π(g′)dgdg′
p (g′) h h u,i u,i
u,i
(cid:90) (cid:20)(cid:90) 1 1 g−g′ (cid:21)
= E · K¯( )·δ (g)δ (g′)π(g′)dg′ ·{1+O(h)}.π(g)dg
p (g′) h h u,i u,i
u,i
(cid:90) 1
≜ φ(g)π(g)dg +O(1), (A.3)
h
where
(cid:90) 1 g−g′
φ(g)= ·K¯( )·δ (g)δ (g′)π(g′)dg′
p (g′) h u,i u,i
u,i
isaboundedfunctionofg.
Combingequations(A.1),(A.2),and(A.3)givesthat
1
(cid:20)(cid:90)
1
(cid:21)
Var(LN (Rˆ))= φ(g)π(g)dg +O(1)−[LN (Rˆ)]2+O(h2)
IPS |D| h ideal
1 (cid:90) 1
= φ(g)π(g)dg+o( ).
|D|h |D|h
Similarly,thevarianceoftheN-DRestimatorisgivenby
1 (cid:90) 1
Var(LN (Rˆ))= ψ(g)π(g)dg+o( ),
DR |D|h |D|h
where
(cid:90) 1 g−g′
ψ(g)= ·K¯( )·{δ (g)−δˆ (g)}{δ (g′)−δˆ (g′)}π(g′)dg′
p (g′) h u,i u,i u,i u,i
u,i
isaboundedfunctionofg.
B.3 PROOFOFTHEOREM4
Theorem4(OptimalBandwidthofN-IPSandN-DR). UnderAssumptions1–4,
19PublishedasaconferencepaperatICLR2024
(a)theoptimalbandwidthfortheN-IPSestimatorintermsoftheasymptoticmean-squarederroris
 1/5
(cid:82)
φ(g)π(g)dg
h∗ =  ,
N−IPS  4|D|(cid:16)
1µ (cid:82)
E(cid:104)
∂2p(ou,i=1,gu,i=g|xu,i) ·δ
(g)(cid:105) π(g)dg(cid:17)2
2 2 ∂g2 u,i
whereφ(g)isdefinedinTheorem3;
(b)theoptimalbandwidthfortheN-DRestimatorintermsoftheasymptoticmean-squarederroris
 1/5
(cid:82)
ψ(g)π(g)dg
h∗ =  .
N−DR  4|D|(cid:16)
1µ (cid:82)
E(cid:104)
∂2p(ou,i=1,g|xu,i) ·{δ (g)−δˆ
(g)}(cid:105) π(g)dg(cid:17)2
2 2 ∂g2 u,i u,i
ProofofTheorem4. Recallthat
1 (cid:90) (cid:104)∂2p(o =1,g|x ) (cid:105)
Bias(LN (Rˆ))= µ E u,i u,i ·δ (g) π(g)dg·h2+o(h2),
IPS 2 2 ∂g2 u,i
1 (cid:90) 1
Var(LN (Rˆ))= φ(g)π(g)dg+o( ).
IPS |D|h |D|h
Themean-squarederroroftheN-IPSestimatorisgivenas
E(cid:2)(cid:0) LN (Rˆ)−LN (Rˆ)(cid:1)2(cid:3)
IPS ideal
=(Bias(LN (Rˆ)))2+Var(LN (Rˆ))
IPS IPS
(cid:18) 1 (cid:90) (cid:104)∂2p(o =1,g|x ) (cid:105) (cid:19)2
= µ E u,i u,i ·δ (g) π(g)dg ·h4+o(h4)
2 2 ∂g2 u,i
1 (cid:90) 1
+ φ(g)π(g)dg+o( ).
|D|h |D|h
Minimizingtheleadingtermsoftheabovemean-squarederrorwithrespecttohleadstothat
 1/5
(cid:82)
φ(g)π(g)dg
h∗ =  =O(|D|−1/5).
N−IPS  4|D|(cid:16)
1µ (cid:82)
E(cid:104)
∂2p(ou,i=1,gu,i=g|xu,i) ·δ
(g)(cid:105) π(g)dg(cid:17)2
2 2 ∂g2 u,i
Similarly,theoptimalbandwidthfortheN-DRestimatorintermsoftheasymptoticmean-squared
errorcanbeobtained.
B.4 PROOFOFTHEOREM5
Lemma1(McDiarmid’sInequality). LetX ,...,X ∈Xmbeasetofm≥1independentrandom
1 m
variablesandassumethatthereexistc ,...,c >0suchthatf :Xm →Rsatisfiesthefollowing
1 m
conditions:
|f(x ,...,x ,...,x )−f(x ,...,x′,...,x )|≤c ,
1 i m 1 i m i
foralli∈{1,2,...,m}andanypointsx ,...,x ,x′ ∈X. Letf(S)denotef(X ,...,X ),thenfor
1 m i 1 m
allϵ>0,thefollowinginequalitieshold:
(cid:18) 2ϵ2 (cid:19)
P[f(S)−E{f(S)}≥ϵ]≤ exp −
(cid:80)m c2
i=1 i
(cid:18) 2ϵ2 (cid:19)
P[f(S)−E{f(S)}≤−ϵ]≤ exp − .
(cid:80)m c2
i=1 i
ProofofLemma1. TheproofcanbefoundinAppendixD.3ofMohrietal.(2018).
20PublishedasaconferencepaperatICLR2024
Lemma2. UndertheconditionsinLemma1,wehavewithprobabilityatleast1−η,
(cid:115)
(cid:80)m c2 2
|f(S)−E[f(S)]|≤ i=1 i log( ).
2 η
Inparticular,ifc ≤cforalli∈{1,2,...,m},
i
(cid:114)
m 2
|f(S)−E[f(S)]|≤c log( ).
2 η
(cid:16) (cid:17)
ProofofLemma2. Thisconclusionfollowsbylettingη =2exp − 2ϵ2 inLemma1.
(cid:80)m c2
i=1 i
Lemma3(RademacherComparisonLemma). LetX ∈X bearandomvariablewithdistributionP,
X ,...,X beasetofindependentcopiesofX,F beaclassofreal-valuedfunctionsonX. Then
1 m
wehave
(cid:12) (cid:12) 1 (cid:88)m (cid:12) (cid:12) (cid:34) 1 (cid:88)m (cid:35)
Esup(cid:12) f(X )−E(f(X ))(cid:12)≤2E E sup f(X )σ ,
(cid:12)m i i (cid:12) σ∼{−1,+1}m m i i
f∈G(cid:12) (cid:12) f∈G
i=1 i=1
whereσ =(σ ,...,σ )isaRademachersequence.
1 m
ProofofLemma3. SeeLemma26.2ofShalev-ShwartzandBen-David(2014).
Recall that F is the hypothesis space of prediction matrices Rˆ (or prediction model f ), the
θ
Rademachercomplexityis
(cid:104) 1 (cid:88) (cid:105)
R(F)=E sup σ δ (g) .
σ∼{−1,+1}|D| |D| u,i u,i
fθ∈F
(u,i)∈D
Lemma4(UniformTailBoundofN-IPSandN-DR). UnderAssumptions1–5andsupposethat
K(t)≤M ,thenwehavewithprobabilityatleast1−η,
K
(a)
(cid:115)
(cid:12) (cid:12) 2M M 5M M M 2 4
sup (cid:12)LN (Rˆ)−E[LN (Rˆ)](cid:12)≤ p KR(F)+ p K δ log( );
(cid:12) IPS IPS (cid:12) h 2 h |D| η
Rˆ∈F
(b)
(cid:115)
sup (cid:12) (cid:12)LN (Rˆ)−E[LN (Rˆ)](cid:12) (cid:12)≤ 2M pM KR(F)+ 5M pM KM |δ−δˆ| 2 log(4 ).
(cid:12) DR DR (cid:12) h 2 h |D| η
Rˆ∈F
ProofofLemma4. WefirstdiscusstheuniformtailboundofLN (Rˆ),thatis,wewanttoshowthe
IPS
upperboundofsup Rˆ∈F(cid:12) (cid:12)LN IPS(Rˆ)−E{LN IPS(Rˆ)}(cid:12) (cid:12).
Notethat
(cid:12) (cid:12)
sup (cid:12)LN (Rˆ)−E{LN (Rˆ)}(cid:12)
(cid:12) IPS IPS (cid:12)
Rˆ∈F
= sup(cid:12) (cid:12) (cid:12)(cid:90) π(g)(cid:34) 1 (cid:88) (cid:26)I(o u,i =1)·K((g u,i−g)/h)·δ u,i(g)(cid:27)
(cid:12) |D| h·pˆ (g)
Rˆ∈F(cid:12)
(u,i)∈D
u,i
(cid:26)I(o =1)·K((g −g)/h)·δ (g)(cid:27)(cid:35) (cid:12) (cid:12)
−E u,i u,i u,i dg(cid:12)
h·pˆ (g) (cid:12)
u,i (cid:12)
(cid:12)
≤ (cid:90) π(g) sup(cid:12) (cid:12) 1 (cid:88) (cid:26)I(o u,i =1)·K((g u,i−g)/h)·δ u,i(g)(cid:27)
(cid:12)|D| h·pˆ (g)
Rˆ∈F(cid:12)
(u,i)∈D
u,i
(cid:12)
(cid:26)I(o
=1)·K((g −g)/h)·δ
(g)(cid:27)(cid:12)
−E u,i u,i u,i (cid:12)dg.
h·pˆ (g) (cid:12)
u,i (cid:12)
21PublishedasaconferencepaperatICLR2024
ForallpredictionmodelRˆ ∈F andg,wehave
1 (cid:12) (cid:12) (cid:12)I(o u,i =1)·K((g u,i−g)/h)·δ u,i(g) − I(o u′,i′ =1)·K((g u′,i′ −g)/h)·δ u′,i′(g)(cid:12) (cid:12) (cid:12)≤ M pM δM K,
|D|(cid:12) h·pˆ u,i(g) h·pˆ u′,i′(g) (cid:12) h|D|
thenapplyingLemma2yieldsthatwithprobabilityatleast1−η/2
(cid:12)
(cid:12) (cid:26)I(o =1)·K((g −g)/h)·δ (g)(cid:27)
sup(cid:12)E u,i u,i u,i
(cid:12) h·pˆ (g)
Rˆ∈F(cid:12) u,i
(cid:12)
− 1 (cid:88)
(cid:26)I(o
u,i =1)·K((g u,i−g)/h)·δ
u,i(g)(cid:27)(cid:12)
(cid:12)
|D| h·pˆ (g) (cid:12)
u,i (cid:12)
(u,i)∈D
(cid:34) (cid:12) (cid:12) (cid:26)I(o =1)·K((g −g)/h)·δ (g)(cid:27)
≤E sup(cid:12)E u,i u,i u,i
(cid:12) h·pˆ (g)
Rˆ∈F(cid:12) u,i
− 1 (cid:88) (cid:26)I(o u,i =1)·K((g u,i−g)/h)·δ u,i(g)(cid:27)(cid:12) (cid:12) (cid:12)(cid:35) + M pM δM K(cid:115) 2 log(4 )
|D| h·pˆ (g) (cid:12) 2h |D| η
u,i (cid:12)
(u,i)∈D
(cid:115)
M M M M M 2 4
≤ p K2E[R(F)]+ p δ K log( ), (A.4)
h 2h |D| η
wherethelastinequalityholdsbyLemma3and
I(o =1)·K((g −g)/h) M M
u,i u,i ≤ p K.
h·pˆ (g) h
u,i
Recallthat
(cid:104) 1 (cid:88) (cid:105)
R(F)=E sup σ δ (g)
σ∼{−1,+1}|D| |D| u,i u,i
Rˆ∈F
(u,i)∈D
andletf(S)=R(F)andc=2M /|D|inLemmas1and2,byapplyingLemma2again,wehave
δ
withprobabilityatleast1−η/2
(cid:115)
2 4
E[R(F)]−R(F)≤M log( ). (A.5)
δ |D| η
Combininginequalities(A.4)and(A.5)leadstothatwithprobabilityatleast1−η
(cid:12)
sup(cid:12) (cid:12) 1 (cid:88) (cid:26)I(o u,i =1)·K((g u,i−g)/h)·δ u,i(g)(cid:27)
(cid:12)|D| h·pˆ (g)
Rˆ∈F(cid:12)
(u,i)∈D
u,i
(cid:12)
(cid:26)I(o
=1)·K((g −g)/h)·δ
(g)(cid:27)(cid:12)
−E u,i u,i u,i (cid:12)
h·pˆ (g) (cid:12)
u,i (cid:12)
(cid:115)
2M M 5M M M 2 4
≤ p KR(F)+ p K δ log( ),
h 2 h |D| η
(cid:82)
whichimpliesLemma4(a)bynotingthat π(g)dg =1.
Next,weshowtheuniformtailboundofLN (Rˆ). Notethat
DR
I(o =1)·K((g −g)/h)·{δ (g)−δˆ (g)}
LN (Rˆ)=δ (g)+ u,i u,i u,i u,i
DR u,i h·pˆ (g)
u,i
22PublishedasaconferencepaperatICLR2024
and
(cid:12) (cid:12)
sup (cid:12)LN (Rˆ)−E{LN (Rˆ)}(cid:12)
(cid:12) DR DR (cid:12)
Rˆ∈F
(cid:34) (cid:40) (cid:41)
= sup (cid:90) π(g) 1 (cid:88) I(o u,i =1)·K((g u,i−g)/h)·{δ u,i(g)−δˆ u,i(g)}
|D| h·pˆ (g)
Rˆ∈F
(u,i)∈D
u,i
(cid:40) (cid:41)(cid:35)
I(o =1)·K((g −g)/h)·{δ (g)−δˆ (g)}
−E u,i u,i u,i u,i dg,
h·pˆ (g)
u,i
whichhasthesameformassup Rˆ∈F(cid:12) (cid:12)LN IPS(Rˆ)−E{LN IPS(Rˆ)}(cid:12) (cid:12),exceptthatδ u,i(g)isreplacedby
δ (g)−δˆ . ByasimilarargumentoftheproofoftheN-IPSestimator,weobtaintheLemma4(b).
u,i u,i
Let
Rˆ† =argminLN (Rˆ), Rˆ‡ =argminLN (Rˆ).
DR IPS
Rˆ∈F Rˆ∈F
ThefollowingTheorem5showsthegeneralizationerrorboundsoftheN-IPSandN-DRestimators.
Theorem5(GeneralizationErrorBoundsofN-IPSandN-DR). UnderAssumptions1–5andsuppose
thatK(t)≤M ,wehavewithprobabilityatleast1−η,
K
(a)
(cid:90) (cid:104)∂2p(o =1,g|x )(cid:105)
LN (Rˆ†)≤ minLN (Rˆ)+µ M E u,i u,i π(g)dg·h2+o(h2)
ideal Rˆ∈F ideal 2 |δ−δˆ| ∂g2
(cid:115)
+ 4M pM KR(F)+ 5M pM KM |δ−δˆ| 2 log(4 );
h h |D| η
(b)
(cid:90) (cid:104)∂2p(o =1,g|x )(cid:105)
LN (Rˆ‡)≤ minLN (Rˆ)+µ M E u,i u,i π(g)dg·h2+o(h2)
ideal Rˆ∈F ideal 2 δ ∂g2
(cid:115)
4M M 5M M M 2 4
+ p KR(F)+ p K δ log( ).
h h |D| η
ProofofTheorem5. ItsufficestoshowTheorem5(a),sinceTheorem5(b)canbederivedfroma
similarargument. Define
Rˆ∗ =argminLN (Rˆ),
ideal
Rˆ∈F
thenwehave
LN (Rˆ†)− minLN (Rˆ)
ideal ideal
Rˆ∈F
=LN (Rˆ†)−LN (Rˆ∗)
ideal ideal
≤LN (Rˆ†)−LN (Rˆ†)+LN (Rˆ†)−LN (Rˆ∗)+LN (Rˆ∗)−LN (Rˆ∗)
ideal DR DR DR DR ideal
≤LN (Rˆ†)−LN (Rˆ†)+LN (Rˆ∗)−LN (Rˆ∗)
ideal DR DR ideal
:=A+B,
where
A=LN (Rˆ†)−LN (Rˆ†),
ideal DR
B =LN (Rˆ∗)−LN (Rˆ∗).
DR ideal
23PublishedasaconferencepaperatICLR2024
Thefirsttermcanbedecomposedasfollows
A=LN (Rˆ†)−E[LN (Rˆ†)]+E[LN (Rˆ†)]−LN (Rˆ†)
ideal DR DR DR
(cid:12) (cid:12)
= (cid:12)Bias[LN (Rˆ†)](cid:12)+E[LN (Rˆ†)]−LN (Rˆ†)
(cid:12) DR (cid:12) DR DR
(cid:12) (cid:12) (cid:104) (cid:105)
≤ (cid:12)Bias[LN (Rˆ†)](cid:12)+ sup E{LN (Rˆ)}−LN (Rˆ)
(cid:12) DR (cid:12) IPS IPS
Rˆ∈F
= 21 µ 2(cid:12) (cid:12) (cid:12) (cid:12)(cid:90) E(cid:104)∂2p(o u,i ∂= g21,g|x u,i) ·{δ u,i(g)−δˆ u,i(g)}(cid:105) π(g)dg(cid:12) (cid:12) (cid:12) (cid:12)·h2+o(h2)
(cid:104) (cid:105)
+ sup E{LN (Rˆ)}−LN (Rˆ)
IPS IPS
Rˆ∈F
≤ 21 µ 2M |δ−δˆ|(cid:12) (cid:12) (cid:12) (cid:12)(cid:90) E(cid:104)∂2p(o u,i ∂= g21,g|x u,i)(cid:105) π(g)dg(cid:12) (cid:12) (cid:12) (cid:12)·h2+o(h2)
(cid:104) (cid:105)
+ sup E{LN (Rˆ)}−LN (Rˆ) ,
IPS IPS
Rˆ∈F
theupperbounddoesnotdependonRˆ†. Likewise,theupperboundofthesecondtermis
B ≤ 21 µ 2M |δ−δˆ|(cid:12) (cid:12) (cid:12) (cid:12)(cid:90) E(cid:104)∂2p(o u,i ∂= g21,g|x u,i)(cid:105) π(g)dg(cid:12) (cid:12) (cid:12) (cid:12)·h2+o(h2)
(cid:104) (cid:105)
+ sup E{LN (Rˆ)}−LN (Rˆ) .
IPS IPS
Rˆ∈F
ThenbyLemma4,wehavewithprobabilityatleast1−η,
A+B ≤µ 2M |δ−δˆ|(cid:12) (cid:12) (cid:12) (cid:12)(cid:90) E(cid:104)∂2p(o u,i ∂= g21,g|x u,i)(cid:105) π(g)dg(cid:12) (cid:12) (cid:12) (cid:12)·h2+o(h2)
(cid:104) (cid:105)
+2 sup E{LN (Rˆ)}−LN (Rˆ)
IPS IPS
Rˆ∈F
≤µ 2M |δ−δˆ|(cid:12) (cid:12) (cid:12) (cid:12)(cid:90) E(cid:104)∂2p(o u,i ∂= g21,g|x u,i)(cid:105) π(g)dg(cid:12) (cid:12) (cid:12) (cid:12)·h2+o(h2)
(cid:115)
+
4M pM KR(F)+5M pM KM |δ−δˆ| 2 log(4
),
h h |D| η
whichimpliestheconclusionofTheorem5(a).
C EXTENSION: MULTI-DIMENSIONAL TREATMENT REPRESENTATION
Foreaseofpresentation,wefocusonthecaseofunivariateginthemanuscript. Inthissection,we
extendtheunivariatecaseandconsiderthecaseofmulti-dimensionalg.
Supposethatg isaq-dimensionalvectordenotedasg = (g ,...,g ). Inthiscase,thebandwidth
1 q
ish=(h ,...,h )andthekernelfunctionisK((g −g)/h)=(cid:81)q K((gs −g )/h ),where
1 q u,i s=1 u,i s s
g =(g1 ,...,gq )withgs beingitss-thelement,K(·)istheunivariatekernelfunctionsuchas
u,i u,i u,i u,i
EpanechnikovkernelK(t)= 3(1−t2)I{|t|≤1}andGaussiankernelK(t)= √1 ·exp{−t2}for
4 2π 2
t∈R. TheN-IPSandN-DRestimatorsaregivenas
(cid:90)
LN (Rˆ)= LN (Rˆ|g)π(g)dg,
IPS IPS
(cid:90)
LN (Rˆ)= LN (Rˆ|g)π(g)dg,
DR DR
24PublishedasaconferencepaperatICLR2024
where
LN (Rˆ|g)= 1 (cid:88) I(o u,i =1)·K((g u,i−g)/h)·δ u,i(g) ,
IPS |D| (cid:81)q h ·p (g)
(u,i)∈D s=1 s u,i
(cid:34) (cid:35)
LN (Rˆ|g)= 1 (cid:88) δˆ (g)+ I(o u,i =1)·K((g u,i−g)/h)·{δ u,i(g)−δˆ u,i(g)} .
DR |D| u,i (cid:81)q h ·p (g)
(u,i)∈D s=1 s u,i
WeshowthetheoreticalpropertiesoftheproposedN-IPSandN-DRestimators,extendingtheresults
of Theorems 3–5. First, we present the bias of the N-IPS and N-DR estimators in the setting of
multi-dimensiontreatmentrepresentation.
Assumption 6 (Regularity Conditions for Kernel Smoothing). (a) For s = 1,...,q, h → 0 as
s
|D| → ∞; (b)
|D|(cid:81)q
h → ∞ as |D| → ∞; (c) p(o = 1,g = g | x ) is twice
s=1 s u,i u,i u,i
differentiablewithrespecttog =(g ,...,g ).
1 q
Theorem6(BiasandVarianceofN-IPSandN-DR). UnderAssumptions1–3and6,
(a)thebiasoftheN-IPSestimatorisgivenas
1
µ
(cid:34) (cid:88)q h2(cid:90) E(cid:110)∂2p(o
u,i
=1,g
u,i
=g|x u,i)
·δ
(g)(cid:111) π(g)dg(cid:35) +o((cid:88)q
h2),
2 2 s ∂g2 u,i s
s=1 s s=1
whereµ =(cid:82) K(t)t2dt. ThebiasoftheN-DRestimatorisgivenas
2
1 µ (cid:34) (cid:88)q h2(cid:90) E(cid:110)∂2p(o u,i =1,g u,i =g|x u,i) ·(cid:0) δ (g)−δˆ (g)(cid:1)(cid:111) π(g)dg(cid:35) +o((cid:88)q h2);
2 2 s ∂g2 u,i u,i s
s=1 s s=1
(b)thevarianceoftheN-IPSestimatoris
1 (cid:90) 1
Var(LN (Rˆ))= φ(g)π(g)dg+o( ),
IPS |D|(cid:81)q h |D|(cid:81)q h
s=1 s s=1 s
where
(cid:90) 1 g−g′
φ(g)= ·K¯( )·δ (g)δ (g′)π(g′)dg′
p (g′) h u,i u,i
u,i
isaboundedfunctionofg,K¯(g− hg′ ) = (cid:82) (cid:81)q s=1K(t s)K(cid:16) gs h− sg s′ +t s(cid:17) dt 1···dt q. Thevariance
oftheN-DRestimatoris
1 (cid:90) 1
Var(LN (Rˆ))= ψ(g)π(g)dg+o( ),
DR |D|(cid:81)q h |D|(cid:81)q h
s=1 s s=1 s
where
(cid:90) 1 g−g′
ψ(g)= ·K¯( )·{δ (g)−δˆ (g)}{δ (g′)−δˆ (g′)}π(g′)dg′
p (g′) h u,i u,i u,i u,i
u,i
isaboundedfunctionofg.
ProofofTheorem6. WeshowthebiasandvarianceoftheN-IPSestimator,andthebiasandvariance
oftheN-DRestimatorcanbeobtainedsimilarly.
(a)Foragiveng,byasimilarargumentoftheproofofTheorem3(a),
(cid:20)I(o
=1)·K((g −g)/h)·δ
(g)(cid:21)
E[LN (Rˆ|g)]=E u,i u,i u,i
IPS (cid:81)q h ·p (g)
s=1 s u,i
=E(cid:34) 1 (cid:90) (cid:89)q 1 K(g us ,i−g s )p(o =1,g |x )dg ·E{δ (g)|x }(cid:35) .
p (g) h h u,i u,i u,i u,i u,i u,i
u,i s s
s=1
25PublishedasaconferencepaperatICLR2024
Let t = (gs − g )/h for s = 1,...,q, then gs = g + h t , dg = dg1 ···dgq =
s u,i s s u,i s s s u,i u,i u,i
(cid:81)q
h dt ···dt . ByaTaylorexpansionofp(o =1,g +h t ,...,g +h t |x ),wehave
s=1 s 1 q u,i 1 1 1 q q q u,i
E(cid:34) 1 (cid:90) (cid:89)q 1 K(g us ,i−g s )p(o =1,g |x )dg ·E{δ (g)|x }(cid:35)
p (g) h h u,i u,i u,i u,i u,i u,i
u,i s s
s=1
(cid:34) 1 (cid:90) (cid:90) (cid:89)q (cid:35)
=E ··· K(t )p(o =1,g +h t ,...,g +h t |x )dt ···dt ·E{δ (g)|x }
p (g) s u,i 1 1 1 q q q u,i 1 q u,i u,i
u,i
s=1
=E(cid:104) 1 (cid:90) ···(cid:90) (cid:89)q
K(t
)(cid:110)
p(o =1,g|x
)+(cid:88)q ∂p(o
u,i
=1,g|x u,i)
h t
p (g) s u,i u,i ∂g s s
u,i s
s=1 s=1
+(cid:88)q (cid:88)q ∂2p(o
u,i
=1,g|x u,i)h sh s′t st
s′
+o((cid:88)q h2)(cid:111)
dt ···dt ·E{δ (g)|x
}(cid:105)
∂g ∂g 2 s 1 q u,i u,i
s s′
s=1s′=1 s=1
(cid:34) 1 (cid:89)q (cid:90) (cid:35)
=E · K(t )dt ·p(o =1,g|x )·E{δ (g)|x } +0
p (g) s s u,i u,i u,i u,i
u,i
s=1
+E(cid:104) 1 (cid:88)q (cid:90)
K(t )t2dt ·
∂2p(o
u,i
=1,g|x u,i)h2
s ·E{δ (g)|x
}(cid:105) +o((cid:88)q
h2)
p (g) s s s ∂g2 2 u,i u,i s
u,i s=1 s s=1
=E[E{δ (g)|x }]+
1
µ
E(cid:104)(cid:88)q ∂2p(o
u,i
=1,g|x u,i)
·h2·δ
(g)(cid:105) +o((cid:88)q
h2)
u,i u,i 2 2 ∂g2 s u,i s
s=1 s s=1
=LN (Rˆ|g)+
1
µ
E(cid:104)(cid:88)q ∂2p(o
u,i
=1,g|x u,i)
·h2·δ
(g)(cid:105) +o((cid:88)q
h2).
ideal 2 2 ∂g2 s u,i s
s=1 s s=1
Thus,thebiasofLN (Rˆ)is
IPS
(cid:90)
E[LN (Rˆ)]−LN (Rˆ)= E(cid:8) LN (Rˆ|g)−LN (Rˆ|g)(cid:9) π(g)dg
IPS ideal IPS ideal
g
=
1
µ
(cid:34) (cid:88)q h2(cid:90) E(cid:110)∂2p(o
u,i
=1,g|x u,i)
·δ
(g)(cid:111) π(g)dg(cid:35) +o((cid:88)q
h2).
2 2 s ∂g2 u,i s
s=1 s s=1
(b)ThevarianceofLN (Rˆ)canberepresentedas
IPS
1 (cid:20)(cid:90) I(o =1)·K((g −g)/h)·δ (g) (cid:21)
Var{LN (Rˆ)}= Var u,i u,i u,i π(g)dg
IPS |D| (cid:81)q h ·p (g)
s=1 s u,i
1
(cid:34) (cid:40)(cid:18)(cid:90)
I(o =1)·K((g −g)/h)·δ (g)
(cid:19)2(cid:41)
= E u,i u,i u,i π(g)dg
|D|
(cid:81)q
h ·p (g)
s=1 s u,i
(cid:26) (cid:18)(cid:90) I(o =1)·K((g −g)/h)·δ (g) (cid:19)(cid:27)2(cid:35)
− E u,i u,i u,i π(g)dg .
(cid:81)q
h ·p (g)
s=1 s u,i
BythebiasoftheN-IPSestimator,
(cid:26) E(cid:18)(cid:90) I(o
u,i
=1)·K((g u,i−g)/h)·δ u,i(g) π(g)dg(cid:19)(cid:27)2
=[LN
(Rˆ)]2+O((cid:88)q
h2)=O(1).
(cid:81)q h ·p (g) ideal s
s=1 s u,i s=1
Ontheotherhand,notethat
(cid:18)(cid:90) I(o =1)·K((g −g)/h)·δ (g) (cid:19)2
u,i u,i u,i π(g)dg
(cid:81)q
h ·p (g)
s=1 s u,i
(cid:90) (cid:90) I(o =1) 1 (cid:18) g −g(cid:19) (cid:18) g −g′(cid:19)
= u,i · K u,i K u,i ·δ (g)δ (g′)π(g)π(g′)dgdg′,
p (g)p (g′) (cid:81)q h2 h h u,i u,i
u,i u,i s=1 s
26PublishedasaconferencepaperatICLR2024
weswaptheorderofintegrationandexpectation,whichleadstothat
(cid:40)(cid:18)(cid:90)
I(o =1)·K((g −g)/h)·δ (g)
(cid:19)2(cid:41)
E u,i u,i u,i π(g)dg
(cid:81)q
h ·p (g)
s=1 s u,i
(cid:90) (cid:90) (cid:20) I(o =1) 1 (cid:18) g −g(cid:19) (cid:18) g −g′(cid:19) (cid:21)
= E u,i · K u,i K u,i ·δ (g)δ (g′) π(g)π(g′)dgdg′.
p (g)p (g′) (cid:81)q h2 h h u,i u,i
u,i u,i s=1 s
Letg =g+ht,i.e.,gs =g +h t ,theng −g′ =(g−g′)+ht,wehave
u,i u,i s s s u,i
(cid:34) I(o =1) 1 (cid:18) g −g(cid:19) (cid:18) g −g′(cid:19) (cid:35)
E u,i · K u,i K u,i ·δ (g)δ (g′)
p (g)p (g′) (cid:81)q h2 h h u,i u,i
u,i u,i s=1 s
=E(cid:34) 1 ·(cid:90) (cid:40) (cid:89)q 1
K(t
)K(cid:18) g s−g s′
+t
(cid:19)
p(o =1,g+ht|x
)(cid:41)
dt ···dt
p (g)p (g′) h2 s h s u,i u,i 1 q
u,i u,i s=1 s s
(cid:35)
(cid:110) (cid:12) (cid:111)
·E δ (g)δ (g′)(cid:12)x
u,i u,i (cid:12) u,i
=E(cid:34) 1 ·(cid:90) (cid:40) (cid:89)q 1
K(t
)K(cid:18) g s−g s′
+t
(cid:19)
p(o =1,g|x
)+O((cid:88)q
h
)(cid:41)
dt ···dt
p (g)p (g′) h2 s h s u,i u,i s 1 q
u,i u,i s=1 s s s=1
(cid:35)
(cid:110) (cid:12) (cid:111)
·E δ (g)δ (g′)(cid:12)x
u,i u,i (cid:12) u,i
=E(cid:34) 1 ·(cid:90) (cid:89)q 1
K(t
)K(cid:18) g s−g s′
+t
(cid:19)
dt ···dt
·E(cid:110)
δ (g)δ
(g′)(cid:12)
(cid:12)x
(cid:111)(cid:35) ·{1+O((cid:88)q
h )}
p (g′) h s h s 1 q u,i u,i (cid:12) u,i s
u,i s s
s=1 s=1
=E(cid:34) 1 ·(cid:90) (cid:89)q 1
K(t
)K(cid:18) g s−g s′
+t
(cid:19)
dt ···dt ·δ (g)δ
(g′)(cid:35) ·{1+O((cid:88)q
h )}.
p (g′) h s h s 1 q u,i u,i s
u,i s s
s=1 s=1
Thus,defineK¯(g− hg′ )=(cid:82) (cid:81)q s=1K(t s)K(cid:16) gs h− sg s′ +t s(cid:17) dt 1···dt q,then
(cid:40)(cid:18)(cid:90)
I(o =1)·K((g −g)/h)·δ (g)
(cid:19)2(cid:41)
E u,i u,i u,i π(g)dg
(cid:81)q
h ·p (g)
s=1 s u,i
=
(cid:90) E(cid:20)(cid:90) 1
·
1 K¯(g−g′
)·δ (g)δ
(g′)π(g′)dg′(cid:21) ·{1+O((cid:88)q
h )}.π(g)dg
p (g′) (cid:81)q h h u,i u,i s
u,i s=1 s s=1
(cid:90) 1
≜ φ(g)π(g)dg (1+o(1)),
(cid:81)q
h
s=1 s
where
(cid:90) 1 g−g′
φ(g)= ·K¯( )·δ (g)δ (g′)π(g′)dg′
p (g′) h u,i u,i
u,i
isaboundedfunctionofg. Therefore,wehave
1 (cid:90) 1
Var(LN (Rˆ))= φ(g)π(g)dg+o( ).
IPS |D|(cid:81)q h |D|(cid:81)q h
s=1 s s=1 s
Theorem7(OptimalBandwidthofN-IPSandN-DR). UnderAssumptions1–3and6,andassume
thath=h =···=h ,then
1 q
(a)theoptimalbandwidthfortheN-IPSestimatorintermsoftheasymptoticmean-squarederroris
 1/(4+q)
(cid:82)
φ(g)π(g)dg
h∗ =  ,
N−IPS  4|D|(cid:16)
1µ (cid:80)q (cid:82)
E(cid:104)
∂2p(ou,i=1,gu,i=g|xu,i) ·δ
(g)(cid:105) ·π(g)dg(cid:17)2
2 2 s=1 ∂g2 u,i
s
27PublishedasaconferencepaperatICLR2024
whereφ(g)isdefinedinTheorem6;
(b)theoptimalbandwidthfortheN-DRestimatorintermsoftheasymptoticmean-squarederroris
 1/(4+q)
(cid:82)
ψ(g)π(g)dg
h∗ =  .
N−DR  4|D|(cid:16)
1µ (cid:80)q (cid:82)
E(cid:104)
∂2p(ou,i=1,gu,i=g|xu,i) ·{δ (g)−δˆ
(g)}(cid:105) ·π(g)dg(cid:17)2
2 2 s=1 ∂g2 u,i u,i
s
ProofofTheorem7. ThisconclusioncanbederivedsimilarlyfromtheproofofTheorem4.
Then,weobtaintheuniformtailboundoftheN-IPSandN-DRestimatorswithmulti-dimensional
treatmentrepresentation.
Lemma5(UniformTailBoundofN-IPSandN-DR). UnderAssumptions1–3,5,and6,andsuppose
thatK(t)≤M ,thenwehavewithprobabilityatleast1−η,
K
(a)
(cid:115)
(cid:12) (cid:12) 2M (M )q 5M M (M )q 2 4
sup (cid:12)E[LN (Rˆ)]−LN (Rˆ)(cid:12)≤ p K R(F)+ p δ K log( );
(cid:12) IPS IPS (cid:12) (cid:81)q h 2(cid:81)q h |D| η
Rˆ∈F s=1 s s=1 s
(b)
(cid:115)
sup
(cid:12)
(cid:12)E[LN (Rˆ)]−LN
(Rˆ)(cid:12)
(cid:12)≤
2M p(M K)q
R(F)+
5M pM |δ−δˆ|(M K)q 2 log(4
).
(cid:12) DR DR (cid:12) (cid:81)q h 2(cid:81)q h |D| η
Rˆ∈F s=1 s s=1 s
ProofofLemma5. It is sufficient to prove the uniform tail bound of LN (Rˆ), and the result for
IPS
LN (Rˆ)canbederivedbyasimilarargument.
DR
Notethat
(cid:12) (cid:12)
sup (cid:12)LN (Rˆ)−E{LN (Rˆ)}(cid:12)
(cid:12) IPS IPS (cid:12)
Rˆ∈F
(cid:12)
≤ (cid:90) π(g) sup(cid:12) (cid:12) 1 (cid:88) (cid:26)I(o u,i =1)·K((g u,i−g)/h)·δ u,i(g)(cid:27)
(cid:12)|D| (cid:81)q h ·pˆ (g)
Rˆ∈F(cid:12) (u,i)∈D s=1 s u,i
(cid:12)
(cid:26)I(o
=1)·K((g −g)/h)·δ
(g)(cid:27)(cid:12)
−E u,i u,i u,i (cid:12)dg.
(cid:81)q h ·pˆ (g) (cid:12)
s=1 s u,i (cid:12)
ForallpredictionmodelRˆ ∈F andg,wehave
|D1 |(cid:12) (cid:12) (cid:12) (cid:12)I(o u,i =1) (cid:81)·K q s=1(( hg su, ·i pˆ− u,ig () g/ )h)·δ u,i(g) − I(o u′,i′ =1) (cid:81)·K q s=1(( hg su′ ·,i pˆ′ u− ′,i′g () g/ )h)·δ u′,i′(f)(cid:12) (cid:12) (cid:12) (cid:12)≤ M (cid:81)p q sM =1δ( hM s|K D) |q ,
thenapplyingLemma2yieldsthatwithprobabilityatleast1−η/2
(cid:12) (cid:12)
sup(cid:12) (cid:12) (cid:12)E(cid:26)I(o u,i =1) (cid:81)·K
q
(( hg u, ·i pˆ−g () g/ )h)·δ u,i(g)(cid:27) − |D1
|
(cid:88) (cid:26)I(o u,i =1) (cid:81)·K
q
(( hg u, ·i pˆ−g () g/ )h)·δ u,i(g)(cid:27)(cid:12) (cid:12)
(cid:12)
Rˆ∈F(cid:12) s=1 s u,i (u,i)∈D s=1 s u,i (cid:12)
≤E(cid:34) sup(cid:12) (cid:12) (cid:12) (cid:12)E(cid:26)I(o u,i =1) (cid:81)·K
q
(( hg u, ·i pˆ−g () g/ )h)·δ u,i(g)(cid:27) − |D1
|
(cid:88) (cid:26)I(o u,i =1) (cid:81)·K
q
(( hg u, ·i pˆ−g () g/ )h)·δ u,i(g)(cid:27)(cid:12) (cid:12) (cid:12) (cid:12)(cid:35)
Rˆ∈F(cid:12) s=1 s u,i (u,i)∈D s=1 s u,i (cid:12)
(cid:115)
M M (M )q 2 4
+ 2p (cid:81)qδ hK |D|log( η)
s=1 s
(cid:115)
M (M )q M M (M )q 2 4
≤ (cid:81)p
q
K
h
2E[R(F)]+ 2p (cid:81)qδ hK |D|log( η),
s=1 s s=1 s
28PublishedasaconferencepaperatICLR2024
wherethelastinequalityholdsbyLemma3and
I(o =1)·K((g −g)/h) M (M )q
u,i u,i ≤ p K .
(cid:81)q
h ·pˆ (g)
(cid:81)q
h
s=1 s u,i s=1 s
Applying(A.5)yieldsthatwithprobabilityatleast1−η
(cid:12) (cid:12)
sup(cid:12) (cid:12) (cid:12)E(cid:26)I(o u,i =1) (cid:81)·K
q
(( hg u, ·i pˆ−g () g/ )h)·δ u,i(g)(cid:27) − |D1
|
(cid:88) (cid:26)I(o u,i =1) (cid:81)·K
q
(( hg u, ·i pˆ−g () g/ )h)·δ u,i(g)(cid:27)(cid:12) (cid:12)
(cid:12)
Rˆ∈F(cid:12) s=1 s u,i (u,i)∈D s=1 s u,i (cid:12)
(cid:115)
2M (M )q 5M M (M )q 2 4
≤ (cid:81)p
q
hK R(F)+ 2p (cid:81)qδ hK |D|log( η),
s=1 s s=1 s
(cid:82)
whichimpliesLemma5(a)bynotingthat π(g)dg =1.
Let
R† =argminLN (Rˆ), R‡ =argminLN (Rˆ).
IPS DR
Rˆ∈F Rˆ∈F
ThefollowingTheorem8showsthegeneralizationerrorboundsoftheN-IPSandN-DRestimators.
Theorem8(GeneralizationErrorBoundsofN-IPSandN-DR). UnderAssumptions1–3,5,and6,
andsupposethatK(t)≤M ,thenwehavewithprobabilityatleast1−η,
K
(a)
LN (Rˆ†)≤ minLN (Rˆ)+µ
(cid:34) (cid:88)q h2(cid:90) E(cid:110)∂2p(o
u,i
=1,g
u,i
=g|x u,i)
·δ
(g)(cid:111) π(g)dg(cid:35)
ideal Rˆ∈F ideal 2
s=1
s ∂g s2 u,i
+
4M p(M K)q
R(F)+
5M pM δ(M K)q(cid:115) 2 log(4 )+o((cid:88)q
h2);
(cid:81)q h (cid:81)q h |D| η s
s=1 s s=1 s s=1
(b)
LN (Rˆ‡)≤ minLN (Rˆ)+µ (cid:34) (cid:88)q h2(cid:90) E(cid:110)∂2p(o u,i =1,g u,i =g|x u,i) ·(cid:0) δ (g)−δˆ (g)(cid:1)(cid:111) π(g)dg(cid:35)
ideal Rˆ∈F ideal 2
s=1
s ∂g s2 u,i u,i
+
4M p(M K)q
R(F)+
5M pM |δ−δˆ|(M K)q(cid:115) 2 log(4 )+o((cid:88)q
h2).
(cid:81)q h (cid:81)q h |D| η s
s=1 s s=1 s s=1
ProofofTheorem8. ThisconclusioncanbederivedsimilarlyfromtheproofofTheorem5.
D PSEUDO-CODES FOR PROPENSITY LEARNING, N-IPS, N-DR-JL AND
N-MRDR-JL
Tolearnthepropensitymodelp (g),wehavethefollowingholdsthat
u,i
1 c P(L=1) P(L=0|x,g) 1 P(L=0|x,g)
= · · ∝ · .
p (g) P(o=1|x) P(L=0) P(L=1|x,g) P(o=1|x) P(L=1|x,g)
u,i
TheconstantcandP(L=1)/P(L=0)canbeignoredandP(o=1|x)canbeestimatedbyusing
theexistingmethodssuchasnaiveBayesorlogisticregressionwithorwithoutafewunbiasedratings,
respectively. Therefore, we need to train the model that estimates P(L = 1 | x,g), as shown in
Algorithm1. Inaddition,thelearningalgorithmsforN-IPS,N-DR-JLandN-MRDR-JLareshown
in Algorithms 2-4. Specifically, for the N-MRDR-JL learning algorithm, the imputation model
parametersareoptimizedbyminimizingthefollowingloss
LN−MRDR(Rˆ)=(cid:90)
|D|−1
(cid:88) I(o u,i=1)·(1−p u,i(g))·K((g u,i−g)/h)·(δ u,i(g)−δˆ u,i(g))2
π(g)dg.
e h·p2 (g)
(u,i)∈D u,i
ComparedtotheLN−DR(Rˆ),theLN−MRDR(Rˆ)losshasvariancereductionproperty.
e e
29PublishedasaconferencepaperatICLR2024
Algorithm1:TheProposedPropensityLearningAlgorithm
Input: theobservationmatrixOandtherepresentationspaceG.
UsingpreviousmethodssuchaslogisticregressiontotrainamodeltoestimateP(o=1|x);
1
whilestoppingcriteriaisnotsatisfieddo
2
Sampleabatchofuser-itempairs{(u ,i )}J witho =1togeneratesamples
3 j j j=1 u,i
(cid:8) (cid:9)
(x ,g ) withpositivelabel(L=1);
uj,ij uj,ij
Uniformlysampleabatchoftreatments{g′ }K ⊂G togeneratesamples
4
(cid:8) (x ,g′ )(cid:9)
withnegativelabel(L=uk, 0ik );k=1
uj,ij uk,ik
UsinggradientdescenttotrainalogisticregressionmodelthatestimatesP(L=1|x,g)
5
usingthepositivesamplesandnegativesamples;
end
6
Output: thepropensitymodelthatestimatesP(L=1|x,g).
Algorithm2:TheProposedN-IPSLearningAlgorithm
Input: theobservedratingsYo,therepresentationspaceG,thepre-specifiedπ(g),the
pre-specifiedkernelfunctionK(·)andthepropensitymodel.
whilestoppingcriteriaisnotsatisfieddo
1
Sampleabatchofuser-itempairs{(u ,i )}J fromO;
2 j j j=1
Calculatepˆ (g)usingthepropensitymodel;
3 uj,ij
Updateθbydescendingalongthegradient∇ LN (Rˆ);
4 θ IPS
end
5
Output: thedebiasedpredictionmodelf (x).
θ
E SEMI-SYNTHETIC EXPERIMENT DETAILS
Followingthepreviousstudies(Schnabeletal.,2016;Wangetal.,2019;Guoetal.,2021),weset
propensityp
u,i
=pαmax(0,4−ru,i)foreachuser-itempairtointroduceselectionbias. Meanwhile,to
investigatetheeffectoftheneighborhoodeffect,werandomlyblockoffsomeuserrowsanditem
columnstogetthemaskmatrix. Specifically,weletm ∼Bern(p ),∀u∈U,m ∼Bern(p ),∀i∈
u u i i
I andm =m ·m ,whereBern(·)denotestheBernoullidistributionandp andp arethemask
u,i u i u i
ratio for user and item. We set p and p equal to 1 in RQ1, and set p = (|U|−n )/|U| and
u i u u
p =(|I|−n )/|I|inRQ2,where|U|and|I|arethetotaluseranditemnumbers,andn andn
i i u i
aretheuseranditemmasknumbers, respectively. Wetunethen ∈ {50,150,250,350}andlet
u
n
i
=n u·|I|/|U|. Thenweobtainthepropensitieswithp
u,i
←p u,i·m
u,i
=pαmax(0,4−ru,i)·m u,i.
Fordifferentmasknumbers,weadjustpforunmaskeduser-itempairstoensurethetotalobserved
sampleis5%oftheentirematrix(Schnabeletal.,2016). Thedifferentn andn correspondto
u i
thedifferentstrengthsoftheneighborhoodeffect. Next,wefollowWangetal.(2019);Guoetal.
(2021)toaddauniformdistributedvariabletointroducenoisetoobtaintheestimatedpropensities
1 = β + 1−β,whereβ isfromauniformdistributionU(0,1)andp = 1 (cid:80) o .
pˆu,i pu,i po o |D| (u,i)∈D u,i
F REAL-WORLD EXPERIMENT DETAILS
Dataset. Weverifytheeffectivenessoftheproposedestimatorsonthreereal-worlddatasets. Coat3
contains6,960MNARratingsand4,640missing-at-random(MAR)ratings. BothMNARandMAR
ratingsarefrom290usersand300items. Yahoo! R34contains311,704MNARratingsand54,000
MARratings. TheMNARratingsarefrom15,400usersand1,000items,andtheMARratingsare
fromthefirst5,400usersand1,000items. Forbothdatasets,ratingsarebinarizedto1ifr ≥3,
u,i
and0otherwise. KuaiRec5(Gaoetal.,2022)isapubliclarge-scaleindustrialdataset,whichcontains
3https://www.cs.cornell.edu/~schnabts/mnar/
4http://webscope.sandbox.yahoo.com/
5https://github.com/chongminggao/KuaiRec
30PublishedasaconferencepaperatICLR2024
Algorithm3:TheProposedN-DR-JLLearningAlgorithm
Input: theobservedratingsYo,theobservationmatrixO,therepresentationspaceG,the
pre-specifiedπ(g),thepre-specifiedkernelfunctionK(·)andthepropensitymodel.
whilestoppingcriteriaisnotsatisfieddo
fornumberofstepsfortrainingtheimputationmodeldo
Sampleabatchofuser-itempairs{(u ,i )}J fromO;
j j j=1
Calculatepˆ (g)usingthepropensitymodel;
uj,ij
Updateϕ bydescendingalongthegradient∇ LN−DR(Rˆ);
g ϕg e
end
fornumberofstepsfortrainingthepredictionmodeldo
Sampleabatchofuser-itempairs{(u ,i )}K fromD;
k k k=1
Calculatepˆ (g)usingthepropensitymodelforuser-itempairwitho =1;
uk,ik uk,ik
Updateθbydescendingalongthegradient∇ LN (Rˆ);
θ DR
end
end
Output: thedebiasedpredictionmodelf (x).
θ
Algorithm4:TheProposedN-MRDR-JLLearningAlgorithm
Input: theobservedratingsYo,theobservationmatrixO,therepresentationspaceG,the
pre-specifiedπ(g),thepre-specifiedkernelfunctionK(·)andthepropensitymodel.
whilestoppingcriteriaisnotsatisfieddo
fornumberofstepsfortrainingtheimputationmodeldo
Sampleabatchofuser-itempairs{(u ,i )}J fromO;
j j j=1
Calculatepˆ (g)usingthepropensitymodel;
uj,ij
Updateϕ bydescendingalongthegradient∇ LN−MRDR(Rˆ);
g ϕg e
end
fornumberofstepsfortrainingthepredictionmodeldo
Sampleabatchofuser-itempairs{(u ,i )}K fromD;
k k k=1
Calculatepˆ (g)usingthepropensitymodelforuser-itempairwitho =1;
uk,ik uk,ik
Updateθbydescendingalongthegradient∇ LN (Rˆ);
θ DR
end
end
Output: thedebiasedpredictionmodelf (x).
θ
4,676,570videowatchingratiorecordsfrom1,411usersfor3,327videos. Therecordslessthan2
aresetto0andotherwisearesetto1.
ExperimentalDetails. AlltheexperimentsareimplementedonPyTorchwithAdamastheopti-
mizer. Forallexperiments, weuseNVIDIAGeForceRTX3090asthecomputingresource. We
tune the learning rate in {0.005,0.01,0.05,0.1} and weight decay in [1e−6,1e−2]. We tune
bandwidthvaluein{40,45,50,55,60}forCoat, {1000,1500,2000,2500,3000}forYahoo! R3
and{100,150,200,250,300}forKuaiRec.
31