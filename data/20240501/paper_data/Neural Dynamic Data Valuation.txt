Neural Dynamic Data Valuation
ZhangyongLianga,HuanhuanGaob,andJiZhangc
ThismanuscriptwascompiledonMay1,2024
Dataconstitutethefoundationalcomponentofthedataeconomyanditsmarketplaces.Efficient
Significance
andfairdatavaluationhasemergedasatopicofsignificantinterest.Manyapproachesbased
onmarginalcontributionhaveshownpromisingresultsinvariousdownstreamtasks.However, Data is a form of property, and it
theyarewellknowntobecomputationallyexpensiveastheyrequiretrainingalargenumber is the fuel that powers the data
ofutilityfunctions,whichareusedtoevaluatetheusefulnessorvalueofagivendatasetfora economyandmarketplaces. How-
specificpurpose. Asaresult,ithasbeenrecognizedasinfeasibletoapplythesemethodsto ever,efficientlyandfairlyevaluating
adatamarketplaceinvolvinglarge-scaledatasets. Consequently,acriticalissuearises: how data value remains a significant
canthere-trainingoftheutilityfunctionbeavoided?Toaddressthisissue,weproposeanovel challenge, and existing marginal
datavaluationmethodfromtheperspectiveofoptimalcontrol,namedtheneuraldynamicdata contribution-based methods face
valuation(NDDV).Ourmethodhassolidtheoreticalinterpretationstoaccuratelyidentifythedata computational challenges due to
valuationviathesensitivityofthedataoptimalcontrolstate. Inaddition,weimplementadata requiringtrainingnumerousmodels.
re-weightingstrategytocapturetheuniquefeaturesofdatapoints,ensuringfairnessthrough Utilizingtheoptimalcontroltheory,
theinteractionbetweendatapointsandthemean-fieldstates.Notably,ourmethodrequiresonly weviewdatavaluationasadynamic
trainingoncetoestimatethevalueofalldatapoints,significantlyimprovingthecomputatio Tnal optimization process and propose
efficiency. Weconductcomprehensiveexperimentsusingdifferentdatasetsandtasks. The a neural dynamic data valuation
resultsdemonstratethattheproposedNDDVmethodoutperformstheexistingstate-of-the-art method. Our method implements
datavaluationmethodsinaccuratelyidentifyingdatapointswitheitherhighorlowvaluesandis adatare-weightingstrategytocap-
F
morecomputationallyefficient. ture the unique features of data
points, ensuring fairness through
dataeconomy|datamarketplace|datavaluation|marginalcontribution|optimalcontrol theinteractionbetweendatapoints
A
andmean-fieldstates.Furthermore,
Data has become a valuable asset in the modern data-driven economy, and it is the training is required only once
consideredaformofpropertyindatamarketplaces(1,2). Eachdatasetpossesses to obtain the value of all data
an individual value, which helps facilitate data sharingR, exchange, and reuse among points,whichsignificantlyimproves
variousentities,suchasbusinesses,organizations,andresearchers. Thevalueofdatais thecomputationalefficiencyofdata
influenced by a wide range of factors, including the size of the dataset, the dynamic valuationproblems.
changes in the data marketplace, the decision-making processes that rely on the data,
and the inherent noise or errors within the daDta itself. These factors can significantly
impact the usefulness and reliability of the data for different applications.
Quantifying the value of data, which is termed data valuation, is crucial for
establishing a fair and efficient data marketplace. By accurately assessing the value
of datasets, buyers and sellers can engage in informed transactions, leading to the
formation of rational data products. This process of buying and selling data based on
its assessed value is known as data pricing (3). Effective data pricing strategies enable
data owners to receive fair compensation for their data while allowing data consumers
to acquire valuable datasets that align with their needs and budget.
However, accurately and fairly assessing the value of data in real-world scenarios
remains a fundamental challenge. The complex interplay of various factors, such as
marketdynamics,dataquality,andthespecificcontextinwhichthedataisused,makes
it difficult to develop a universal valuation framework. Moreover, ensuring fairness
in data valuation is essential to prevent bias and discrimination against certain data
owners or types of data. Another significant challenge is the computational efficiency
of data valuation methods when dealing with large datasets, which are prevalent in
many real-world applications. Traditional data valuation approaches often struggle to
scale effectively to massive datasets, making it difficult to assess the value of data in a
timelyandcost-effectivemanner. Addressingthesechallengesrequiresthedevelopment Authoraffiliations:aNationalCenterforAppliedMathe-
of sophisticated data valuation methods that can account for the complex nature of matics,TianjinUniversity,Tianjin300072,PRChina;
bSchool of Mechanical and Aerospace Engineer-
data value, promote equitable practices in data marketplaces, and efficiently handle
ing,JilinUniversity,Changchun130025,PRChina;
large-scale datasets. cSchool of Mathematics, Physics and Computing,
Contribution-basedmethodsfordatavaluationaimtoquantifythevalueofindividual UniversityofSouthernQueensland,Australia
datapointsbymeasuringtheirimpactontheperformanceofamachinelearningmodel.
These methods typically assess the marginal contribution of each data point to the
model’s performance, which can be used as a proxy for its value. Theauthorsdeclarenocompetinginterests.
Astandardcontribution-baseddatavaluationmethodinvolves
1Correspondenceandrequestsformaterialsshould
be addressed. Email: gaohuanhuan@jlu.edu.cn,
assessing the impact of adding or removing data points on ji.zhang@unisq.edu.au.
1–8
4202
rpA
03
]LM.tats[
1v75591.4042:viXraA
Utility Function
Static Dynamic
Marginal Value Marginal
Contribution Function Contribution
Stochastic Optimal Control

 
New Utility Function i,1   

Hamilton gradient flow X i,T
1
Train only once
 Φ(X ,μ )
 (x ,y;U)  (x ,y;U ) T T
j n n n n n
Re-train for models T Meta Learning
Static Data Valuation Neural Dynamic Data Valuation
B.1 Half Moons Dataset B.2 Data State TrajecFtories B.3 Data Value Trajectories
A
R
D
C.1 Detecting Corrupted Data C.2 Removing Low/High Value Data C.3 Adding Low/High Value Data
Fig.1.Neuraldynamicdatavaluationschematicandresults.(A)showsacomparisonbetweentheNDDVmethodandexistingdatavaluationmethods.Itisevidentthatthe
NDDVmethodtransformsthestaticcompositecalculationmethodofexistingdatavaluationintoadynamicoptimizationprocess,defininganewutilityfunctionanddynamicmarginal
contribution.Comparedtoexistingdatavaluationmethods,theNDDVmethodrequiresonlyonetrainingsessiontodeterminethevalueofalldatapoints,significantlyenhancing
computationalefficiency.Takingthehalf-moonsdatasetillustratedinPanel(B.1)asanexample,wedemonstratesomeresultsoftheNDDVmethodtoindicateitseffectiveness.Panels
(B.2),(B.3)displaythetrajectoriesofdatastatesandvaluesovertime,revealingtherelationshipbetweenthedynamiccharacteristicsofthedataandtheirintrinsicvalues.Panels
(C.1),(C.2),(C.2)displaytheeffectivenessoftheNDDVmethodinthreetypicaldatavaluationexperiments:detectingcorrupteddata,removinglow/highvaluedata,andaddinglow/high
valuedata.
2 Leadauthorlastname etal.
(
(
(
x
x
x
), y1
1
), y2 2
), yn
n
n


(j
(j
x
x
,1
,2
y ;U1
y ;U2
)
)
( x
( x
( x
1
2
n
,
,
,
)y
1
)y 2
)y
n


(
(
x
x
, y ;U1
1
, y ;U2 2
)1
)2 U i = − X i,T Y i,T Y
X
i,0
i,0
i,0
0
Y
X
i,1
i,1
Y
X
i,2
i,2
i,22
Y
X
i,i
i,i
i,i
i
Y
i,T
i,T
T
( x
( x
( x
), y1
1
), y2 2
), yn nmodel performance by calculating their marginal contributions. captures the essential characteristics and relationships among
One of the earliest methods proposed for computing marginal data points that contribute to their value within the dataset.
contributionsistheleave-one-out(LOO)approach. Thismethod Our method establishes a connection with the marginal
entails removing a specific piece of data from the entire dataset contributioncalculationbasedoncooperativegametheory,which
and using the difference in model performance before and after focuses on the contribution of individual data entities to a
its removal as the marginal contribution (4, 5). However, the coalition of data points in the context of data valuation. Instead
leave-one-out (LOO) method has several limitations. Firstly, of calculating the marginal contribution of each data point
it requires retraining the model for each data point, which separately, the NDDV approach, by leveraging the mean-field
can be computationally expensive, especially for large datasets. approximation and the optimal state of data points, explores
Secondly, the LOO method may not capture the interactions the potential for a more efficient and unified approach to data
betweendatapoints,asitonlyconsiderstheimpactofremovinga valuation. This unified approach aims to compute the overall
singledatapointatatime,potentiallyoverlookingthesynergistic value of the dataset and then redistribute the contributions
or antagonistic effects among data points. among the individual data points, taking into account their
optimal states and interactions with the mean-field.
Another class of methods calculates the Shapley value based
TheproposedNDDVmethodreformulatesthedatavaluation
on cooperative game theory (6), which distributes the average
paradigm from the perspective of mean-field optimal control.
marginal contributions fairly as the value of the data points
Existingdatavaluationmethodsbasedonmarginalcontributions
(7–9). The Shapley-based data valuation methods have been
adopt a discrete combinatorial computation model, which only
widely applied in various domains, such as high-quality data
accounts for the static interactions among data points. In
mining, data marketplace design, and medical image screening,
contrast, under mean-field optimal control, data points undergo
due to their ability to provide a fair and theoretically grounded a continuous opTtimization process to derive mean-field optimal
valuation. However, these methods require the prior setting of
control strategies. This approach facilitates the manifestation
utilityfunctionsandestimatemarginalcontributionsthroughthe
of the value of data points through their interactions with the
exponential combination of training numerous models, leading
mean-fiFeldstate,therebycapturingthedynamicinterplayamong
to high computational costs in the data valuation process. To
data points. By considering the continuous optimization process
addressthisissue,aseriesofimprovedapproximationalgorithms
andtheinteractionsbetweendatapointsandthemean-fieldstate,
have been proposed to reduce the computational burden while
Athe NDDV method provides a more comprehensive and dynamic
maintaining valuation accuracy. The use of k-Nearest Neighbors
representation of data value compared to the static and discrete
(10) or linear models (11) for data valuation has somewhat
nature of traditional marginal contribution-based methods.
enhancedvaluationefficiencybutstruggleswithhigh-dimensional
data. Then,adatavaluationmethodemployingalinearlRysparse
OurContributions..This paper has the following three contribu-
LASSO model has been proposed (12), which improves sampling
tions:
efficiency under the assumption of sparsity. However, it requires
additional training of the LASSO model and performs poorly • We propose a new data valuation method from the perspec-
in identifying low-quality data. Another receDnt development is tiveofstochasticoptimalcontrol,framingthedatavaluation
the out-of-bag (OOB) estimation method (13), introduced for as a continuous-time optimization process.
efficientdatavaluationproblems. Althoughitreducesthenumber
ofmodelstobetrained,itstillrequirestrainingnumerousmodels • We develop a novel marginal contribution metric that
when faced with new data points. capturestheimpactofdataviathesensitivityofitsoptimal
control state.
Recent developments in marginal contribution-based data val-
uationmethodshavefocusedoncapturingtheinteractioncharac- • The NDDV method requires the training only once, avoid-
teristicsamongdatapointsbycalculatingmarginalcontributions ing the repetitive training of the utility function, which
throughtheremovaloradditionofdatapointsandaveragingthe significantly enhances computational efficiency.
model performance differences across all combinations. However,
this approach raises a fundamental question: is every outcome of
RelatedWorks
theexponentiallevelofcombinatorialcalculationstrulyessential?
Given the goal of capturing the average impact of data points
DynamicsandOptimalControlTheory.The dynamical perspec-
tive has received considerable attention recently as it brings new
withinthedataset,itmaybemoreefficienttodirectlymodelthe
insights into deep architectures and training processes. For in-
interactions between the data points and the mean-field.
stance,viewingDeepNeuralNetworks(DNNs)asadiscretization
To address the question raised, we propose Neural Dynamic of continuous-time dynamical systems is proposed in (14). From
Data Valuation (NDDV), a novel data valuation method that this,thepropagatingruleinthedeepresidualnetwork(15)canbe
reformulatestheclassicalvaluationcriteriaintermsofstochastic thoughtofasaone-stepdiscretizationoftheforwardEulerscheme
optimal control in continuous time, as illustrated in Fig. 1. on an ordinary differential equation (ODE). This interpretation
This reformulation involves expressing the traditional valuation has been leveraged to improve residual blocks to achieve more
methods, such as marginal contribution-based approaches, using effectivenumericalapproximation(16). Inthecontinuumlimitof
the framework of stochastic optimal control theory. The NDDV depth, the flow representation of DNNs has made the transport
method obtains the optimal representation of data points in a analysis with Wasserstein geometry possible (17). In algorithms,
latentspace,whichwerefertoastheoptimalstateofdatapoints, pioneering computational methods have been developed, as
throughdynamicinteractionsbetweendatapointsandthemean- detailed in (18, 19), facilitating the direct parameterization of
field. This approach leads to a novel method for calculating stochasticdynamicsusingDNNs. Expandinguponthis,whenthe
marginal contributions, where the optimal state of data points analogybetweenoptimizationalgorithmsandcontrolmechanisms
Leadauthorlastname etal. 3isfurtherexplored,asdiscussedby(20),itbecomesapparentthat games, necessitating repetitive training of a predefined utility
standard supervised learning methodologies can effectively be function, the proposed NDDV method derives control strategies
reinterpretedwithintheframeworkofmean-fieldoptimalcontrol fromacontinuoustimeoptimizationprocess. Thegradientofthe
problems(21). This is particularly beneficial since it enables new Hamiltonianconcerningthecontrolstatesservesasthemarginal
trainingalgorithmsinspiredbyoptimalcontrolliterature(22–24). contribution, representing a novel attempt at data valuation
A similar analysis can be applied to stochastic gradient descent problems. Lastly, the proposed NDDV method transforms the
(SGD) by viewing it as a stochastic dynamical system. Most interactions among data points into interactions between data
previousdiscussionsonimplicitbiasformulateSGDasstochastic pointsandthemean-fieldstates,therebycircumventingtheneed
Langevindynamics(25). Otherstochasticmodeling,suchasL`evy for exponential combinatorial computations.
process, has been recently proposed (26). In parallel, stability
analysis of the Gram matrix dynamics induced by DNNs reveals
global optimality (27, 28). Applying optimal control theory to ProblemFormulationandPreliminaries
SGD dynamics results in optimal adaptive strategies for tuning In this section, we formally describe the data valuation problem.
hyper-parameters, such as the learning rate, momentum, and Then,wereviewtheconceptofmarginalcontribution-baseddata
batch size (29, 30). valuation.
In various downstream tasks, data valuation aims to fairly
DataValuation.Existing data valuation methods, such as LOO
assign model performance scores to each data point, reflecting
(5), DataShapley (7, 31), BetaShapley (32), DataBanzhaf (33),
the contribution of individual data points. Let [N]={1,...,N}
InfluenceFunction (34) and so on, generally require knowledge
denotes a training set of size n. We define the training dataset
of the underlying learning algorithms and are known for their
as D=(x ,y )N , where each pair (x ,y ) consists of an input
substantial computational demands. Notably, the work of (10) i i i=T1 i i
x from the input space X ⊂ Rd and a corresponding label y
has proposed to use the k-Nearest Neighbor classifier as a fri
om the label space Y ⊂ R, pertaining to the i-th data
pointi
.
default proxy model to perform data valuation. While it can be
To measure the contributions of data points, we define a utility
considered a learning-agnostic data valuation method, it is less functioFn U : 2N → R, which takes a subset of the training
effective and efficient than the NDDV method in distinguishing
dataset D as input and outputs the performance score that is
data quality. Alternatively, measuring the utility of a dataset
trained on that subset. In classification tasks, for instance, a
by the volume(35), defined as the square root of the trace of Acommon choice for U is the test classification accuracy of an
thefeaturematrixinnerproduct,providesanalgorithm-agnostic
empirical risk minimizer trained on a subset of D. Formally, we
and straightforward calculation. Nonetheless, its reliance solely
setU(S)=metric(A(S)),whereAdenotesabasemodeltrained
on features does not allow for the detection of poor data due
on the dataset S, and metric represents the metric function for
to labeling errors. In evaluating the contribution of indRividual
evaluating the performance of A, e.g., the accuracy of a finite
data points, it is proposed to resort to the Shapley value, which
hold-outvalidationset. WhenS ={}istheemptyset,U(S)isset
would still be expensive for large datasets. Furthermore, the
to be the performance of the best constant model by convention.
work by (36) introduces a learning-agnostic methodology for
D The utility function is influenced by the selection of learning
data valuation based on the sensitivity of class-wise Wasserstein
algorithms and a specific class. However, this dependency is
distances. However,thismethodisconstrainedbyitsdependence
omitted in our discussion, prioritizing instead the comparative
on a validation set and its limitations in evaluating the fairness
analysis of data value formulations. For a set S, we denote its
of data valuation. Recently, a method based on the out-of-bag
powersetby2S anditscardinalityby|S|. Weset[j]:={1,...,j}
estimate is computationally efficient and outperforms existing
for j ∈N.
methods(13). Despiteitsadvantages,thismethodisconstrained
Astandardmethodforquantifyingdatavaluesisthemarginal
by the sequential dependency of weak learners in boosting,
contribution, which measures the average change in a utility
limiting its direct applicability to downstream tasks. Marginal
function when a particular datum is removed from a subset of
contribution-based methods have been studied and applied to
the entire training dataset. We denote the data value of data
various machine learning problems, including feature attribution
point(x ,y )∈DcomputedfromU asϕ(x ,y ;U). Thefollowing
(37, 38), model explanation (39, 40), and collaborative learning i i i i
sections review well-known notions of data value.
(41,42). Amongthese,theShapleyvalueisoneofthemostwidely
usedmarginalcontribution-basedmethods,andmanyalternative
methods have been studied by relaxing some of the underlying Loo Metric.A simple data value measure is the LOO metric,
whichcalculatesthechangeinmodelperformancewhenthedata
fair division axioms (43, 44). Alternatively, there are methods
that are independent of marginal contributions. For instance, point (x i,y i) is excluded from the training set N:
in the data valuation literature, a data value estimator model
using reinforcement learning is proposed by (45). This method ϕ loo(x i,y i;U)≜U(N)−U(N \(x i,y i)). [1]
combinesdatavaluationwithmodeltrainingusingreinforcement
learning. However, it measures data usage likelihood, not the The LOO method measures the changes when one particular
impact on model quality. datum (x i,y i) is removed from the entire dataset D. LOO
To the best of our knowledge, optimal control has not been includes the Cook’s distance and the approximate empirical
exploited for data valuation problems. The NDDV method influence function.
diverges from existing data valuation methods in several key
aspects. Firstly, we construct a data valuation framework from StaticMarginalContribution.Existing standard data valuation
theperspectiveofoptimalcontrol,markingapioneeringendeavor methodscanbeexpressedasafunctionofthemarginalcontribu-
in this field. Secondly, whereas most data valuation methods tion. For a specific utility function U and j ∈[N], the marginal
rely on calculating marginal contributions within cooperative contribution of (x ,y ) ∈ D with respect to [j] data points is
i i
4 Leadauthorlastname etal.defined as follows points, leading to a static and brute-force computation that
incurs exponential computational costs. While many methods
1 X
∆ j(x i,y i;U)≜ (cid:0)N−1(cid:1) U(S∪(x i,y i))−U(S), [2] have been proposed to reduce computational costs, they still
j−1 S∈D\(xi,yi) require to train extensive utility functions. Data points often
exist in the form of coalitions influenced by collective decisions.
where D\(xi,yi) = {S ⊆ D\(x i,y i) : |S| = j −1}. Eq.2 is a This influence can essentially be considered as a utility function.
combination to calculate the error in adding (x ,y ), which is a The interactions among data points under such conditions tend
i i
static metric. tobedynamicandstochastic(seeFig.2). Datapointsevolveover
time to get optimal control trajectories by maximizing coalition
Shapley Value Metric.The Shapley value metric is considered gains. To get this, we construct the stochastic dynamics of data
the most widely studied data valuation scheme, originating from points as a stochastic optimal control process (more details in SI
cooperative game theory. At a high level, it appraises each point Appendix 1).
based on the average utility change caused by adding the point The following minimization of the stochastic control problem
into different subsets. The Shapley value of a data point i is is considered, involving a cost function with naive partial
defined as information. We define the stochastic optimal control as follows
ˆ
1 XN (cid:20) T (cid:21)
ϕ (x ,y ;U)≜ ∆ (x ,y ;U). [3] L(ψ)≜E R(X ,ψ )dt+Φ(X ) , [5]
Shap i i N j i i t t T
0
j=1
As its extension, Beta Shapley is proposed by is expressed as
where ψ:[0,T]→Ψ⊂Rp is the stochastic control parameters
a weighted mean of marginal contributions and X t = (X 1T,t,...,X N,t) ∈ Rd×N is the data state for all
t ∈ [0,T]. Then, Φ : Rd → R is the terminal cost function,
N corresponding to the loss term in traditional machine learning
X
ϕ (x ,y ;U)≜ β ∆ (x ,y ;U). [4] tasks. R:Rd×Ψ→R is the running cost function, playing a
Beta i i j j i i F
role in regularization. Eq.5 subjects to the following stochastic
j=1
dynamical system
where β =(β ,...,β ) is a predefined weight vector such that
PN j=1β j = 11 and β jn ≥ 0 for all j ∈ [N]. A functional form of A (cid:26)dX t =b(X t,ψ t)dt+σdW t, t∈[0,T], [6]
Eq.4 is also known as semi-values. X =x,
0
The LOO metric is known to be computationally feasible,
but it often assigns erroneous values that are close to zero whereb:Rd×Ψ→Rd isthedriftfunction,primarilyembodying
R
(46). Shapley value metric is empirically more effective than acombinationofalineartransformationfollowedbyanon-linear
the LOO metric in many downstream tasks such as mislabeled function applied element-wise. σ:R→R and W :Rd →Rd are
t
data detection in classification settings (7, 32). However, their the diffusion function and standard Wiener process, respectively.
computationalcomplexityiswellknowntobeDexpensive,making Moreover, σ and dW
t
(the differential of W t) remain identical
it infeasible to apply to large datasets (31, 33, 47). As a result, constant for all t. The control equation of Eq.6 is the Forward
most existing work has focused on small datasets, e.g., n≤1000. Stochastic Differential Equation (FSDE).
The stochastic optimal control problem is often stated by the
Stochastic Maximum Principle (SMP)(48, 49). The SMP finds
NDDV:ADynamicDataValuationNotion
the optimal control strategy by maximizing the Hamiltonian.
Existingdatavaluationmethodsareoftenformulatedasaplayer This optimization process involves deriving a gradient process
valuation problem in cooperative game theory. A fundamental concerning control through the adjoint process of the state
problemincooperativegametheoryistoassignanessentialvector dynamics, thereby obtaining a gradient descent format.
toallplayers.Astandardmethodforquantifyingdatavaluesisto We define the Hamiltonian H :[0,T]×Rd×Rd×Ψ→R as
usethemarginalcontributionasEq.2. Themarginalcontribution-
based method considers the interactions among data points, H(X ,Y ,Z ,ψ )≜b(X ,ψ )·Y +tr(σ⊤Z )−R(X ,ψ ). [7]
t t t t t t t t t t
ensuring the fair distribution of contributions from each data
TheadjointequationisthengivenbytheBackwardStochastic
point. This static combinatorial computation method measures
Differential Equation (BSDE)
the average change in a utility function as data points are added
or removed. This is achieved by iterating through all possible ( dY =−∇ H(cid:0) X ,Y ,ψ )dt+Z dW , t∈[0,T],
combinations among the data points. Moreover, this incurs t x t t t t t [8]
an exponential level of computational cost. Despite a series of Y T =−∇ xΦ(X T),
effortstoreducethecomputationalburdenofcalculatingmarginal
contributions, the challenge of avoiding repetitive training of the where the terminal condition Y T is an optimally controlled path
for Eq.5. Then, Eq.6 and Eq.8 are combined to the framework
utility function remains difficult to overcome.
of Forward-Backward Stochastic Differential Equation (FBSDE)
of the McKean–Vlasov type. The Hamiltonian maximization
Learning Stochastic Dynamics from Data.The marginal
condition is
contribution-based data valuation is a standard paradigm for
evaluating the value of data, focusing on quantifying the average
H(X∗,Y∗,Z∗,ψ∗)≥H(X∗,Y∗,Z∗,ψ), [9]
impactofaddingorremovingaparticulardatapointonautility t t t t t t t
function. This method averages the marginal contributions of The SMP (Eq.6-9) establishes necessary conditions for the
all data points to ascertain the value of a specific data point. optimalsolutionofEq.6. However,itdoesnotprovideanumerical
However,itrequiresexhaustivepairwiseinteractionsamongdata methodforfindingthissolution. Here, weemploythemethodof
Leadauthorlastname etal. 5A B
μk-1(Xk-1) 0 0
Xk
1,i
k(;θ)Xk 1,0
X ik ,0 k(;θ)X ik ,0
Xk
N,0 k(;θ)Xk
X Nk ,i X Nk ,T N,0 T k(;θ)X Nk ,i k(;θ)X Nk ,T
Data State Trajectory Weighted Data State Trajectory
Fig.2.Learningdatastochasticdynamicschematic.(A)Instochasticoptimalcontrol,datapointsgFettheiroptimalstatetrajectoriesviadynamicinteractionswiththemean-field
state.(B)Withinthedatare-weightingstrategy,datapointsarecharacterizedbyheterogeneity.Inthisscenario,datapointsdynamicallyinteractwiththeweightedmean-fieldstate,
therebydeterminingtheiroptimalstatetrajectories.
A
successiveapproximations(MSA)(50,51)asourtrainingstrategy, Data Points Re-weighting Strategy.In data marketplaces, the
which is an iterative method based on alternating propagation data needing valuation is not always vast in volume. Our goal
and optimization steps. We convert Eq.9 into an iterative form is to reflect the value of individual data points, regardless of
of the MSA, as follows R whether the dataset is small, medium, or large. To enhance the
heterogeneity of data points, we employ a strategy of assigning
ψ tk+1 =argmaxH(X tk,Y tk,Z tk,ψ), [10] weights to highlight the importance of individual ones. We
ψ
introduce a novel SDE model for data valuation that considers
TheprocedurefromEq.10isiterateduntilconveDrgenceisachieved.
weighted mean-field states. This is referred to as the weighted
The basic MSA is outlined in Alg.1. However, ψk+1 may
control function because it represents the attention of each data
t
diverge when the arg-max step is excessively abrupt(51), leading
pointondifferentinferenceweights. Theproofsofthissubsection
to a situation where the non-negative penalty terms become
are found in SI Appendix 2.
predominant. A straightforward solution is to incrementally
Here,weemployametare-weightingmethod(52)derivedfrom
adjust the arg-max step in a suitable direction, ensuring these
the Stackelberg game (53). The meta-network of this method,
minor updates remain within the realm of viable solutions. In
acting as the leader, makes decisions and disseminates meta-
other words, if we assume the differentiability concerning ψ, we
weights across individual data points. Imposing meta-weight
may substitute the arg-max step with the steepest ascent step
function V(Φ (ψ);θ) on the i-th data point terminal cost, where
i
ψk+1 =ψk+α∇ H(Xk,Yk,Zk,ψk), [11] θrepresentsthehyper-parametersinmeta-network,thestochastic
t t ψ t t t t
control formulation in Eq.5 is modified as follows
In fact, there is an interesting relationship of Eq.11 with
classical gradient descent for back-propagation (51), as follow
" #
∇ ψL(ψ t)=∇ Xt+1Φ(X T)·∇ ψX t+1+∇ ψR(ψ t) L(ψ;θ)= N1 XN T X−1 R i(ψ t)+V(Φ i(ψ T);θ)Φ i(ψ T) . [15]
=−Y t+1·∇ ψX t+1+∇ ψR(ψ t) [12] i=1 t=0
=−∇ H(X ,Y ,Z ,ψ ),
ψ t t t t
where X
t+1
and X
t+1
is where V(Φ i(ψ);θ) is approximated via a multilayer perceptron
ˆ ˆ (MLP) network with only one hidden layer containing a few

t t nodes. Each hidden node has the ReLU activation function, and
X
t
=X 0+ b(X s,ψ s)ds+ σdW s,
the output has the Sigmoid activation function to guarantee the
ˆ 0 0 ˆ [13]
Y t =Y 0− t ∇ xH(cid:0) X s,Y s,ψ s)ds+ t Z sdW s, wou et pp ru ot vii ds elo tc ha ete pd roi on f t oh fe coi nn vte er rv ga enl co ef f[ o0 r,1 E]. q.I 1n 5.S II tA isp ep ve in dd enix t t2 h.A at,
0 0 the training loss L progressively converges to 0 through gradient
Hence, Eq.11 is simply the gradient descent step descent.
ψk+1 =ψk−α∇ L(ψk). [14] Generally,themeta-datasetformeta-networkrequiresasmall,
t t ψ t
unbiaseddatasetwithcleanlabelsandbalanceddatadistribution.
6 Leadauthorlastname etal.
μ (k-10 X )k-10
X k1,0
μ
μ
( )k-1 k-1Xi i
( )k kX =t t
kX i,i
1N Nj=1 X kj,t
X ki,T
(k-1μ T
kX
1,T
X )k-1T
( )k-1 k-1μ Xi i
N1( ) k kμ X =t t N
j=1
k(;θ )X
k k(;θ )X i,i
k(;θ
k1,i
)X kj,t
(k-1μ XT
k k(;θ )X
1,T
k k(;θ )X i,T
)k-1TThe meta loss is We consider a class of stochastic dynamics where the inter-
action between the data points is given in terms of functions of
1 XM averagecharacteristicsoftheindividualstate. Inourformulation,
ℓ(ψ(θ))= ℓ (ψ(θ)), [16]
M i the data state X whose N components X can be interpreted
t i,t
i=1 as the individual state of the data point (x ,y ). A typical
i i
wherethegradientofthemetalossℓconvergestoasmallnumber, interactioncapturingthesymmetryofinterestisdepictedthrough
meeting the convergence criteria (see SI Appendix 2.Bfor a more models. In these models, the dynamics of the individual state
detailed explanation). X unfold.Thosearedescribedbycoupledstochasticdifferential
i,t
Theoptimalparameterψ∗andθ∗arecalculatedbyminimizing equations of the following form
the following
 ψ∗(θ)=argmin L(ψ;θ), 1 XN
 ψ [17] dX i,t = N b(X i,t,X j,t,ψ i,t)dt+σdW i,t, [20]
θ∗ =argmin ℓ(ψ∗(θ)), j=1
θ
For a given admissible control ψ , we write X for the unique
Calculatingtheoptimalψ∗andθ∗requirestwonestedloopsof i,t i,t
solution to Eq.20, which exists under the usual growth and
optimization. Here,weadoptanonlinestrategytoupdateψ∗and
Lipchitzconditionsonthefunctionb. Theproblemistooptimally
θ∗ through a single optimization loop, respectively, to guarantee
control this process to minimize the expectation.
the efficiency of the algorithm. The bi-optimization proceeds in
Forthesakeofsimplicity,weassumethateachprocessX is
three sequential steps to update parameters. Representing the i,t
univariate. Otherwise,thenotationsbecomemoreinvolvedwhile
gradient descent for back-propagation through the gradient of
the results remain essentially the same. The present discussion
the Hamiltonian concerning weights T
canaccommodatemodelswherethevolatilityσisafunctionwith
  ψ θˆ kk +=
1
=ψk θk+ −Nα βiP =N
P1
M∇ ∇ψH ℓi( (ψ ψˆ, kV (θ(Φ ))i |( kψ ,Tk);θ))| ψk,
[18]
t t
l
seh h
tv
re i aess
l
t.a l eem gWFv iee eel ss .ut or sf Nu ec g ott e tu hn icr eee er naa tols hi tt at ayh ttie ot tof
n
hu en k ψc e stt tei
=
opo cn h(t ψhb a. e
s1
t,W
t
in
,
co ·e
·
dtr a
·
ye t
,
nf ir ψo aa n
N
min s
, it
c)f tr soo
fo
Em a
r
qtc .r
h
2o e 0n eas s
c
ci od
o
ane
n
nar ti b
r
bn olg e
el
M θ i θ

ψk+1 =ψk+
Nαi= PN1
∇ ψH i(ψ,V(Φ i(ψ Tk);θk+1))| ψk,
Arewritten in th de Xform
=b(X ,µ ,ψ )dt+σdW , [21]
i=1 i,t i,t t i,t i,t
where α and β are the step size. The parameters ψˆk, θk+1 and If the function b, which encompasses time, an individual state, a
ψk+1 are updated as delineated in Eq.18, where the flowRform of probabilitydistributionoverindividualstatesforthedatapoints,
these is and a control, is defined as follows
. ˆ
Φ(ψ Tk) V(;θ) θk θk+1 ψˆk ℓ(ψˆk(θk+1)) b(X t,µ t,ψ t)= b(X t,X t′,ψ t)dµ t(X t′), [22]
D R
where the measure µ is defined as the empirical distribution of
ψ Tk ψˆk ψk+1 L(ψk+1;θk+1) the data points statet s, i.e.
Updatingmetaparametersθk+1 ofEq.18bybackpropagation
can be rewritten as µ ≜
1 XN
δ , [23]
t N Xi,t
!
αβ Xn 1 Xm ∂V(L (ψk);θ)(cid:12) i=1
θk+1 =θk+ G j (cid:12) , [19]
n m ij ∂θ (cid:12) θt Interactions given by functions of the form Eq.22 are called
j=1 i=1 linear (order 1). We employ the function b of Eq.20, giving the
where G =
∂ℓi(ψˆ)(cid:12) (cid:12)T ∂ℓj(ψ)(cid:12)
(cid:12) . The derivation of G can be
interaction between the private states is of the form
f jo -tu hnd grain di ij S enI tA tep r∂ p mψ eˆ n md(cid:12) eψ iˆ x at s2 u.∂ rC eψ s. tT(cid:12) hψ h et e sic mo ie lffi arc iti yen bt etm1 weP enm i= a1 dG ai tij j apon oint th ’e s N1 2 XN b(X i,t,X j,t,X k,t,ψ i,t), [24]
j,k=1
gradient from training loss and the average gradient from meta
loss in the mini-batch. If aligned closely, the data point’s weight where the function b could be rewritten in the form
ˆ
is likely to increase, showing its effectiveness; otherwise, it may
decrease. b(X t,µ t,ψ t)= b(X t,X t′,X t′′,ψ t)dψ(X t′)dµ t(X t′′). [25]
R
Mean-fieldInteractions.Motivated by cooperative game theory, Interactions of the form shown in Eq.25 are called quadratic.
existing data valuation methods provide mathematical justifica- Given the frozen flow of measures µ, the stochastic optimization
tions that are seemingly solid. However, the fair division axioms problem is a standard stochastic control problem, and as such,
used in the Shapley value have not been statistically examined, its solution can be solved via the SMP.
anditraisesafundamentalquestionabouttheappropriatenessof Generally, it is understood that stochastic control exhibits
theseaxiomsinmachinelearningproblems(44,54). Moreover,it mean-field interactions when the dynamics of an individual
isnotnecessaryfordatapointstoiteratethroughallcombinations state,asdescribedbythecoefficientsofitsstochasticdifferential
toengageinthegame. Infact,eachdatapointhasanindividual equation, are influenced solely by the empirical distribution of
optimalcontrolstrategy,whichisobtainedandoptimizedthrough other individual states. This implies that the interaction is
dynamic interactions among others. entirely nonlinear according to the definition provided.
Leadauthorlastname etal. 7To summarize the aforementioned issue, the mean-field In Eq.32, the drift term within the context of the mean-field
interactions consist of minimizing simultaneously costs of the linear quadratic control can be simplified to
following form
(cid:20)ˆ
T
(cid:21) b(X t,µ t,ψ t)=a(µ t−X t)+ψ t, [33]
E R(X ,µ ,ψ )dt+V(Φ(ψ );θ)Φ(X ,µ ) , [26]
t t t T T T
0 where a is the positive constant. At this stage, the marginal
distribution embodies the empirical average of the data state.
It’s important to highlight that, due to considerations of
Consequently, Eq.23 can be succinctly reformulated as follows
symmetry, we select R and Φ to be the same for all data points.
For clarity, the focus is restricted to equilibriums given by
Markovian strategies in the closed-loop feedback form 1 XN
µ = X . [34]
t N i,t
ψ t =(ψ1(X t),··· ,ψN(X t)), t∈[0,T], [27] i=1
forsomedeterministicfunctionsψ ,···,ψ oftimeandthestate where Eq.34 does not account for the heterogeneity among data
1 N
of the system. Further, an assumption is made. The strategies points, as shown in Fig.2.(A). To emphasize the contribution of
are categorized as distributed. This categorization means the individual data points, the weighted of µ should be considered
t
strategyfunctionψ ,associatedwiththedatapoint(x ,y ),relies
i i i
solely on X i,t. Here, X i,t represents the individual state of the 1 XN
data point (x i,y i). It is independent of the overall system state µ t = N V(Φ(X i,T;µ T))X i,t. [35]
X t. In other words, we request that. i=1
T
ψ =ψ(X ), i=1,··· ,N, [28] Fig.2.(B) shows data points dynamically interact with the
i,t i,t
weightedmean-fieldstateofEq.35. Itisevidentthattheweighted
Furthermore, due to the symmetry of the setup, the search for
optimalstatetrajectoriescapturethecharacteristicsofindividual
equilibriums is limited to scenarios where all data points use the F
data points’ states.
same feedback strategy function, as the following
In the following, we interact the control states of the data
points with the control states endowed with authority, explicitly
ψ (X )=···=ψ (X )=ψ(X ), t∈[0,T], [29] A
1 1,t N N,t t formulating the drift term of the control equation as
Here, ψ is identified as a deterministic function. With the
involvement of a finite number of data points, it is assumed that dX =[a(µ −X )+ψ ]dt+σdW , [36]
t t t t t
eachoneanticipatesotherparticipantswhohavealreadyRselected
their optimal control strategies. That means ψ =ψ(X ), ··· Applying Eq.36 into Eq.5, the discrete-time analogue of the
1,t 1,t
, ψ =ψ(X ), ψ =ψ(X ), ··· , ψ =ψ(X ), weighted stochastic control problem is
i−1,t i−1,t i+1,t i+1,t N,t N,t
and under this assumption, solving the optimization problem
D
" #
ψ i =argminL(ψ(X i,t)), [30] min 1 XN T X−1 R (ψ )+V(Φ (ψ );θ)Φ (ψ ) ,
ψ ψ,θ N i t i T i T
i=1 t=0
Additionally, it is important to note that the assumption of
s.t.X =X +[a(µ −X )+ψ ]∆t+σ∆W. [37]
symmetry leads to the restriction to optimal strategies where i,t+1 i,t t i,t t
ψ = ψ = ··· = ψ . Due to the system’s exchangeability
1 2 N where ∆t and ∆W represent the uniform partitioning of the
property, the individual characteristics of data points are not
interval [0,T].
easily highlighted. To emphasize the non-exchangeability of
Based on this, we define the discrete mean-field Hamiltonian
optimal control strategies, the data points re-weighting is
in Eq.7 as
introduced. So, one solves the weighted stochastic control
problem
H (X ,Y ,Z ,µ ,ψ )=[a(µ −X )+ψ ]·Y +σ⊤Z
ˆ i i,t i,t i,t t t t i,t t i,t i,t
(cid:20) (cid:21)
L(ψ)=E T R(X ,µ ,ψ )dt+V(Φ(ψ );θ)Φ(X ,µ ) , −R(X i,t,µ t,ψ t). [38]
t t t T T T
0
[31] Then, we transform Eq.37 into the discrete-time format of the
subject to the following mean-field dynamical system SMP to maximize the mean-field Hamiltonian H in Eq.38. The
i
necessary conditions can be written as follows
dX =b(X ,µ ,ψ )dt+σdW . [32]
t t t t t
a
foR re mvis oit fin ”g avE erq a.2 g, e”t .he Tm oa org bi tn aa inl c to hn etr oib pu tt imio an lp cr oim nta rr oi lly str re afl te ec gt ys   YX ∗i∗ ,t+1 == YX ∗i∗ ,t −+ ∇(cid:2) a H(µ∗ t (X− ∗X ,i∗ Y,t) ∗+ ,Zψ ∗t∗ ,(cid:3) µ∆ ∗t ,ψ+ ∗σ )∆∆ tW +,
Z∗ ∆W,
i,t+1 i,t x i i,t i,t i,t t t i,t
for data points, complex mean-field models require a certain
d de isg tr re ibe uo tf ios nim µp tli tfi oca rt ei po rn e. senIn
t
tfa hc et, avt eh re aga ebi sli tt ay teof ofth de atm aa prg oi in nta sl  EX (cid:2)0∗ H= i(x X,
i∗ ,t,Y
iY
∗
,tT ,∗ Z=
i∗
,t− ,µ∇
∗
tx ,( ψV t∗( )Φ (cid:3)i ≥(ψ ET (cid:2)); Hθ i) (Φ Xi(
i∗
,X t,i∗ Y,T
i∗
,, tµ ,Z∗ T i∗) ,) t,
,µ∗
t,ψ)(cid:3)
.
during the interactions can achieve an approximation of the [39]
marginal contribution in mean-field interactions. Inspired by the where Eq.39 describes the trajectory of a random variable that
mean-field linear quadratic control theory, we have reformulated is completely determined by random variables.
the marginal contribution format based cooperative games into The proof for the error estimate of the weighted mean-field
a state control marginal contribution. MSA is provided in SI Appendix 3.
8 Leadauthorlastname etal.DataStateUtilityFunction.To perform data valuation from the Eq.42 can be interpreted as a measure of the data point’s
perspectiveofstochasticdynamics,anaturalchoiceistoredefine contribution to the terminal cost. ϕ(x ,y ;U ) can be either
i i i
the utility function U(S). Similar to the existing marginal positive or negative, indicating that the evolution of the data
contribution-baseddatavaluationmethods,U(S)issettobethe state towards this point would increase or decrease the terminal
model’s optimal performance. The NDDV method reconstructs cost, respectively.
model training through the continuous dynamical system to The dynamic data valuation metric satisfies the common
capture the dynamic characteristics of data points. Based on axioms1ofdatavaluationproblems. Fortheproofprocess,seeSI
this, we establish a connection between the dynamic states of Appendix 4.C for a more detailed explanation. To illustrate the
data points and marginal contributions, constructing a novel effectivenessofthedynamicdatavaluationmetric,weapplythis
method for data valuation. Under optimal control, the data metric for data value ranking and compare it with the ground
points get their respective optimal control strategies, with the truth (see SI Appendix 4.C).
valueofeachdatapointbeingdeterminedbyitssensitivitytothe
coalition’s revenue. For stochastic optimal control problems, the
Experiments
optimality conditions from SMP establish a connection between
the sensitivities of a functional cost and the adjoint variables. In this section, we conduct a thorough evaluation of the
This relationship is exploited in continuous sensitivity analysis, NDDV method’s effectiveness through three sets of experiments
which estimates the influence of parameters in the solution of commonly performed in prior studies (7, 36, 45): elapsed time
differential equation systems (55, 56). comparison, mislabeled data detection, and data points re-
Inspired by recent work on propagating importance scores moval/additionexperiments.Theseexperimentsaimtoassessthe
(57),wedefinetheindividualdatastateutilityfunctionU (S)as computationalefficiency,accuracyinidentifyingmislabeleddata,
i
follow and the impactTof data point selection on model training. We
demonstrate that the NDDV method is computationally efficient
∂L(ψ)
U (S)=X · and highly effective in identifying mislabeled data. Furthermore,
i i,T X
i,T compared to state-of-the-art data valuation methods, NDDV
F
=X i,T ·∇ xΦ(X i,T,µ T) can identify more accurately which data points are beneficial or
=−X ·Y , [40] detrimental for model training. A detailed description of these
i,T i,T
experiments is provided in SI Appendix 5.
where Eq.40 highlights the maximization of utility scores under A
theconditionofminimalcontrolstatedifferences,seeSIAppendix
ExperimentalSetup.We use six classification datasets that are
4.A for further details. Moreover, it becomes evident that a
publicly available in OpenDataVal (58), many of which were
single training suffices to obtain all data state utility functions
R used in previous data valuation papers (7, 32). We compare
U(S)=(U (S),U (S),··· ,U (S)).
1 2 n NDDV with the following eight data valuation methods: LOO
(5), DataShapley (7), BetaShapley (32), DataBanzhaf (33),
DynamicMarginalContribution.Under optimal control, the data
InfluenceFunction(34),KNNShapley(10),AME(12),Data-OOB
points to be evaluated obtain their respective optimal control
D
(13). Existing data valuation methods, except for Data-OOB,
strategies, with the value of each data point being determined
requireadditionalvalidationdatatoevaluatetheutilityfunction
by its sensitivity to the coalition’s revenue.
We can compute the value of a data point based on the
U(S). To make our comparison fair, we use the same number or
a greater number of utility functions for existing data valuation
calibrated sensitivity of the coalition’s cost to the final state of
methods compared to the proposed NDDV method.
the data point
A standard normalization procedure is applied to ensure
∆(x ,y ;U )=U (S)− X U j(S) , [41] consistency and comparability across the dataset. Each feature
i i i i N −1 is normalized to have zero mean and unit standard deviation.
j∈{1,...,N}\i
After this preprocessing, we split the dataset into training,
Like Eq.2, Eq.41 also reflects the changes in the utility function
validation, and test subsets. We evaluate the value of data
upon removing or adding data points. Notably, it distinguishes
in the training subset and use the validation subset to evaluate
itself by representing the average change in the utility function,
the utility function. Note that the NDDV method does not
as captured in Eq.2, by comparing individual data point state
use this validation subset and essentially uses a smaller training
utility scores against the average utility scores of other data
subset. The test subset is used for point removal experiments
points.
only when evaluating the test accuracy. The training subset size
Eq.41 emphasizes the stochastic dynamic properties of data
n is either 1,000, 10,000, or the full dataset, and the validation
points, herein named the dynamic marginal contribution. The
size is fixed to 10% of the training sample size. The size of the
erroranalysisbetweenEq.41andEq.2ispresentedinSIAppendix
test subset is fixed at 30% of the training sample size, and the
4.B.
size of the meta subset is fixed at 10% of the training sample
size. The hyperparameters for all methods are provided in SI
Dynamic Data Valuation Metric.In learning the stochastic dy-
Appendix 5.A.
namics from data, effective interactions occur among the data
points, leading to the identification of each data point’s optimal
control strategy. Therefore, calculating a data point’s value does
ElapsedTimeComparison.To intuitively compare the runtime,
several data valuation methods known for their computational
not necessitate iterating over all other data points to average
efficiency, such as InfluenceFunction, KNNShapley, AME, Data-
marginal contributions. The value of a data point (x ,y ) is
i i OOB, and NDDV, are compared. In contrast, other methods
equivalent to its marginal contribution, which is expressed as
like LOO, DataShapley, BetaShapley, and DataBanzhaf exhibit
ϕ(x ,y ;U )=∆(x ,y ;U ). [42] lowercomputationalefficiencyand areevaluated underscenarios
i i i i i i
Leadauthorlastname etal. 930 K N N S h ap ley 60 K N N S h ap ley 100 K N N S h ap ley
A M E A M E A M E
25 D ata-O O B 50 D ata-O O B D ata-O O B
In flu en ceF u n ction In flu en ceF u n ction 80 In flu en ceF u n ction
N D D V N D D V N D D V
20 40
60 15 30
40
10 20
5 10 20
0 0.286 0 0.289 0 0.328
0 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9 10 0 1 2 3 4 5 6 7 8 9 10
N u m b er o f d a ta p o in ts (× 1 0 5) N u m b er o f d a ta p o in ts (× 1 0 5) N u m b er o f d a ta p o in ts (× 1 0 5)
Fig.3.ElapsedtimecomparisonbetweenKNNShapley,AME,Data-OOB,InfluenceFunction,andNDDV.Weemployasyntheticbinaryclassificationdataset,illustratedwith
featuredimensionsof(left)d=5,(middle)d=50and(right)d=500.Asthenumberofdatapointsincreases,theNDDVmethodexhibitssignificantlylowerelapsedtimethanother
methods.
with fewer data points (details in SI Appendix 5.F). Firstly, Then,werandomlyselecta10%labelnoiserateandflipthedata
we synthesized a binary classification dataset under a standard labels accordingly. After conducting data valuation, we use the
T
Gaussian distribution and sampled five sets of data points. k-meansclusteringalgorithmtodividethevalueofdataintotwo
Secondly,asetofdatapointssizesnis{1.0×104,5.0×104,1.0× clusters based on the mean. The cluster with the lower mean is
105,5.0 × 105,1.0 × 106}. Finally, we set four data feature considered to be the identification of corrupted data points.
dimensions d as {5,50,500}, corresponding to four groups of As sFhown in Tab.1, the F1-scores for various data valuation
time comparison experiments. To expedite the running time, we methods are presented, showcasing performance under the
set the batch size to 1000. The influence of batch size on the influence of mislabeled data points across six datasets. Overall,
running time of all methods is presented in SI Appendix 5.F. AtheNDDVmethodoutperformstheexistingmethods. Moreover,
Fig.3 shows the runtime curves of different data valuation Fig.4 shows the capability to achieve optimal results in this
methods with increasing data points in various d settings, high- task. The NDDV method demonstrates superior performance in
lighting the NDDV method’s superior computational efficiency. converging towards optimal results compared to the currently
R
AME and Data-OOB require training in numerous base models, high-performing KNNShapley, AME, Data-OOB, and Influence-
resulting in lower computational efficiency. Concurrently, the Function. It is evident that the NDDV method is better able
KNNShapley method exhibits high computational efficiency to detect corrupted data. In SI Appendix 5.G, we explore the
when n is small, benefiting from its natureDas a closed-form impact of introducing various levels of noise and observe that
expression. However,asnincreases,thecomputationalefficiency the NDDV method also maintains superior performance amidst
ofthis methodrapidly decreases. Moreover, themethod requires these conditions. This fact notably underscores its effectiveness.
substantialcomputationalmemory,especiallywhenn≥1.0×105.
This decline is attributed to the requirement to sort data points RemovingandAddingDatawithHigh/LowValue.In this section,
for each validation data point. six datasets from Section S1 are selected to conduct the data
As for the algorithmic complexity, the computational com- points removal and addition experiments as described in (7, 13).
plexity of NDDV is O(kn/b) where k is the number of training Theseexperimentsaimtoidentifydatapointsthatarebeneficial
the meta-learning model, n is the number of data points, and b ordetrimentaltomodelperformance. Forthedatapointsremoval
is the batch size for training. For KNNShapley is O(n2log(n)) experiment,datapointsaresequentiallyremovedfromhighestto
whenthenumberofdatapointsinthevalidationdatasetisO(n); lowestvalue,focusingontheimpactofremovinghigh-valuedata
AME is O(kn/b); Data-OOB is O(Bdnlog(n)) where B is the points on model performance. Conversely, in the data points
number of trees, d is the number of features. Therefore, It can additionexperiment,datapointsareaddedfromlowesttohighest
be observed that NDDV exhibits linear complexity. Overall, value, focusing on the effects of adding low-value data on model
NDDV is highly efficient, and it takes less than half an hour performance. After removing or adding data points, we re-train
when (n,d)=(106,500). the base model to evaluate its test accuracy. In all experiments,
wefixthenumberoftrainingdatapointsat1000,validationdata
points at 100, and test data points at 300, with 100 metadata
CorruptedDataDetection.In the real world, training datasets
points. The test accuracy results are evaluated on the fixed
often contain mislabeled and noisy data points. Since these
holdout dataset.
data points frequently negatively affect the model performance,
it is desirable to assign low values. In this section, we focus on Fig.S3 (first column) shows the per-
DataRemovalExperiment.
detectingmislabeleddatapointsandshowtheresultsofdetecting formance changes of different data valuation methods after
noise data points in the appendix. We synthesize label noise on removing the most valuable data points. It can be observed
six training datasets and introduced perturbations to compare thatthetestaccuracycurvecorrespondingtotheNDDVmethod
theabilityofdifferentdatavaluationmethodstodetectcorrupted effectivelydecreasesinalldatasets. Then,Fig.S3(secondcolumn)
data. We set the training sample size to n∈{1000,10000}, but illustrates that the impact of removing the least valuable data
LOO,DataShapley,BetaShapley,andDataBanzhafarecomputed points on performance by different data valuation methods is
only when n = 1000 due to their low computational efficiency. less significant than that of removing the most valuable data
10 Leadauthorlastname etal.
)sr
u o h ni(
e
mi
T
)sr
u o h ni(
e
mi
T
)sr
u o h ni(
e
mi
TTable1. F1-scoreofdifferentdatavaluationmethodsonthesixdatasetswhen(left)n = 1000and(right)n = 10000. Themeanandstandard
deviationoftheF1-scorearederivedfrom5independentexperiments.Thehighestandsecond-highestresultsarehighlightedinboldandunderlined,
respectively.
n=1000 n=10000
Dataset Data Beta Data Influence KNN Data KNN Data
LOO AME NDDV AME NDDV
Shapley Shapley Banzhaf Function Shapley -OOB Shapley -OOB
0.17± 0.17± 0.19± 0.18± 0.21± 0.34± 0.18± 0.60± 0.67± 0.37± 0.010± 0.71± 0.79±
2dplanes
0.003 0.005 0.003 0.009 0.005 0.007 0.009 0.007 0.005 0.004 0.012 0.002 0.005
0.18± 0.20± 0.20± 0.35± 0.17± 0.24± 0.18± 0.35± 0.39± 0.32± 0.01± 0.38± 0.44±
electricity
0.004 0.004 0.006 0.002 0.003 0.006 0.010 0.002 0.002 0.001 0.009 0.003 0.002
0.19± 0.16± 0.18± 0.14± 0.16± 0.33± 0.18± 0.78± 0.90± 0.52± 0.01± 0.73± 0.85±
bbc
0.004 0.004 0.003 0.005 0.002 0.008 0.009 0.004 0.002 0.005 0.010 0.002 0.006
0.18± 0.16± 0.17± 0.18± 0.16± 0.21± 0.18± 0.42± 0.26± 0.29± 0.18± 0.48± 0.52±
IMDB
0.002 0.004 0.003 0.002 0.009 0.008 0.011 0.005 0.007 0.002 0.012 0.002 0.003
0.14± 0.17± 0.16± 0.17± 0.18± 0.25± 0.01± 0.53± 0.85± 0.16± 0.01± 0.77 0.91±
STL10
0.006 0.004 0.002 0.005 0.009 0.007 0.009 0.003 0.008 0.009 0.012 0.002 0.003
0.18± 0.19± 0.20± 0.17± 0.18± 0.24± 0.01± 0.40± 0.59± 0.27± 0.01± 0.46± 0.58±
CIFAR10
0.004 0.003 0.005 0.002 0.007 0.004 0.008 0.004 0.004 0.009 0.010 0.001 0.004
T
points. It is evident that the effectiveness of the NDDV method we formulate data valuation as an optimization process of
in removing the least valuable data points aligns with that optimal control, thereby eliminating the need for re-training
F
of most existing data valuation methods. For existing data utility functions. In addition, we propose a novel marginal
valuationmethods,AMEtendstohavearandombaselineintest
contribution metric that captures the impact of data points
accuracyduetotheLASSOmodel. KNNShapleyperformsworse
Avia the sensitivity of their optimal control state. Through
inremovinghigh-valuedatapointsduetoitslocalcharacteristic,
comprehensive experiments, we demonstrate that the NDDV
which fails to capture the margin from a classification boundary.
method significantly improves computational efficiency while
Data-OOB shows the worst performance in removing low-value
ensuring the accurate identification of high and low-value data
data points, indicating a lack of sensitivity towards lowR-quality
points for model training.
data. Overall, the result shows the fastest model performance
decreaseinremovinghigh-valuedatapointsandtheslowestmodel Our work represents a novel attempt with several research
performance decline in adding low-value data points, reflecting avenues worthy of further exploration:
the NDDV method’s superiority in data valuaDtion.
• Extending the NDDV method to handle data valuation in
Data Addition Experiment. Similar to the data removal exper- dynamic and uncertain data marketplaces is an important
iment, as shown in Fig.S3 (third and fourth column), it researchdirection,asitcanprovidemorerobustandadaptive
demonstratesthetestaccuracycurvesviadifferentdatavaluation valuation techniques in real-world scenarios.
methods in adding data points experiment. The experiments
demonstrate that adding high-value data points boosts test • Data transactions frequently occur across data markets,
accuracy above the random baseline, whereas adding low-value and applying the NDDV method to datasets originating
points leads to a decrease. This illustrates their effectiveness in from multiple sources or incorporating the method into a
adding high and low-value data points. Among existing works, frameworkthatcanhandlemultipledatasetssimultaneously
Data-OOBshowsthebestperformanceinidentifyingtheimpact represents an exciting direction.
of adding high and low-value data points. LOO shows a slightly
better performance than the random baseline. Compared to • Adapting the NDDV method to analyze multimodal data,
Data-OOB, the NDDV method shows similar performance in whichinvolvesintegratingdatafromvariousmodalities,such
most cases. as text, images, and audio, presents a valuable application
Comprehensively, the NDDV method achieves superior test of interest.
accuracy compared to other baseline models under various
settings. The presented experiment results support these
theoretical findings. The NDDV method dynamically optimizes Data,Materials,andSoftwareAvailability
all data points, enhancing the ability to capture high and low-
The code used for conducting experiments can be available at
value data points via the sensitivity of their states.
https://github.com/liangzhangyong/NDDV
Conclusion
ACKNOWLEDGMENTS. Pleaseincludeyouracknowledgmentshere,
In this paper, we propose a novel data valuation method from setinasingleparagraph. Pleasedonotincludeanyacknowledgments
a new perspective of optimal control. Unlike previous works, intheSupportingInformation,oranywhereelseinthemanuscript.
1.AAgarwal,MDahleh,TSarkar,Amarketplacefordata:AnalgorithmicsolutioninProceedingsof
the2019ACMConferenceonEconomicsandComputation.pp.701–726(2019).
Leadauthorlastname etal. 11N D D V D ata-O O B A M E K N N S h ap ley In flu en ceF u n ction L O O D ataS h ap ley B etaS h ap ley D ataB an zh af O p tim al R an d om
D a ta set: 2 d p la n es D a ta set: electricity D a ta set: b b c
1.0 1.0 1.0
0.8 0.8 0.8
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
F ra ctio n o f d a ta in sp ected F ra ctio n o f d a ta in sp ected F ra ctio n o f d a ta in sp ected
D a ta set: IM D B D a ta set: S T L 1 0 D a ta set: C IF A R 1 0
1.0 1.0 1.0
0.8 0.8 0.8
T
0.6 0.6 0.6
0.4 0.4 0.4
F
0.2 0.2 0.2
A
0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
F ra ctio n o f d a ta in sp ected F ra ctio n o f d a ta in sp ected F ra ctio n o f d a ta in sp ected
Fig.4.Discoveringcorrupteddatainsixdatasetswith10%labelnoisyrRate.The’Optimal’curvedenotesassigningthelowestdatavaluescorestodatapointswithlabelnoise
witha10%labelnoiseratetodetectcorrupteddataeffectively.The’Random’curvedenotesanabsenceofdistinctionbetweencleanandnoisylabels,leadingtoaproportional
relationshipbetweenthedetectionofcorrupteddataandtheextentofinspectionperformed.
D
2.ZTian,etal.,Privatedatavaluationandfairpaymentindatamarketplaces.arXivpreprint 10.RJia,etal.,Efficienttask-specificdatavaluationfornearestneighboralgorithms.Proc.VLDB
arXiv:2210.08723(2022). Endow.12,1610–1623(2019).
3.JPei,Asurveyondatapricing:fromeconomicstodatascience.IEEETransactionson 11.YKwon,MARivas,JZou,Efficientcomputationandanalysisofdistributionalshapleyvaluesin
knowledgeDataEng.34,4586–4608(2020). InternationalConferenceonArtificialIntelligenceandStatistics.(PMLR),pp.793–801(2021).
4.RDCook,SWeisberg,Residualsandinfluenceinregression.(NewYork:ChapmanandHall), 12.JLin,etal.,Measuringtheeffectoftrainingdataondeeplearningpredictionsviarandomized
(1982). experimentsinInternationalConferenceonMachineLearning.(PMLR),pp.13468–13504(2022).
5.PWKoh,PLiang,Understandingblack-boxpredictionsviainfluencefunctionsinInternational 13.YKwon,JZou,Data-oob:Out-of-bagestimateasasimpleandefficientdatavaluein
ConferenceonMachineLearning.(PMLR),pp.1885–1894(2017). Internationalconferenceonmachinelearning.(PMLR),(2023).
6.LSShapley,Avalueforn-persongames.ContributionstoTheoryGames2,307–317(1953). 14.EWeinan,Aproposalonmachinelearningviadynamicalsystems.Commun.Math.Stat.1,1–11
7.AGhorbani,JZou,Datashapley:EquitablevaluationofdataformachinelearninginInternational (2017).
ConferenceonMachineLearning.pp.2242–2251(2019). 15.KHe,XZhang,SRen,JSun,DeepresiduallearningforimagerecognitioninProceedingsofthe
8.AGhorbani,MKim,JZou,AdistributionalframeworkfordatavaluationinInternational IEEEconferenceoncomputervisionandpatternrecognition.pp.770–778(2016).
ConferenceonMachineLearning.(PMLR),pp.3535–3544(2020). 16.YLu,AZhong,QLi,BDong,Beyondfinitelayerneuralnetworks:Bridgingdeeparchitectures
9.SSchoch,HXu,YJi,CS-shapley:Class-wiseshapleyvaluesfordatavaluationinclassificationin andnumericaldifferentialequations.arXivpreprintarXiv:1710.10121(2017).
AdvancesinNeuralInformationProcessingSystems,eds.AHOh,AAgarwal,DBelgrave,KCho. 17.SSonoda,NMurata,Transportanalysisofinfinitelydeepneuralnetwork.J.Mach.Learn.Res.
(2022). 20,1–52(2019).
18.RTChen,YRubanova,JBettencourt,DKDuvenaud,Neuralordinarydifferentialequations.Adv.
neuralinformationprocessingsystems31(2018).
19.RTQChen,DDuvenaud,Neuralnetworkswithcheapdifferentialoperatorsin2019ICML
WorkshoponInvertibleNeuralNetsandNormalizingFlows(INNF).(2019).
20.BHu,LLessard,Controlinterpretationsforfirst-orderoptimizationmethodsin2017American
ControlConference(ACC).(IEEE),pp.3114–3119(2017).
21.EWeinan,JHan,QLi,Amean-fieldoptimalcontrolformulationofdeeplearning.arXivpreprint
arXiv:1807.01083(2018).
22.QLi,LChen,CTai,EWeinan,Maximumprinciplebasedalgorithmsfordeeplearning.TheJ.
Mach.Learn.Res.18,5998–6026(2017).
23.QLi,SHao,Anoptimalcontrolapproachtodeeplearningandapplicationstodiscrete-weight
neuralnetworks.arXivpreprintarXiv:1803.01299(2018).
24.DZhang,TZhang,YLu,ZZhu,BDong,Youonlypropagateonce:Acceleratingadversarial
trainingviamaximalprinciple.arXivpreprintarXiv:1905.00877(2019).
25.GAPavliotis,Stochasticprocessesandapplications:diffusionprocesses,theFokker-Planckand
Langevinequations.(Springer)Vol.60,(2014).
26.USimsekli,LSagun,MGurbuzbalaban,Atail-indexanalysisofstochasticgradientnoiseindeep
neuralnetworks.arXivpreprintarXiv:1901.06053(2019).
27.SSDu,XZhai,BPoczos,ASingh,Gradientdescentprovablyoptimizesover-parameterized
neuralnetworks.arXivpreprintarXiv:1810.02054(2018).
12 Leadauthorlastname etal.
at
a d det
p urr
oc
detcete
d f o
n oitc
ar
F
at
a
d
det
p urr
oc
detcete
d
f
o
n
oitc
ar
F
at
a d det
p urr
oc
detcete
d f o
n oitc
ar
F
at
a
d
det
p urr
oc
detcete
d
f
o
n
oitc
ar
F
at
a d det
p urr
oc
detcete
d f o
n oitc
ar
F
at
a
d
det
p urr
oc
detcete
d
f
o
n
oitc
ar
F28.SSDu,JDLee,HLi,LWang,XZhai,Gradientdescentfindsglobalminimaofdeepneural
networks.arXivpreprintarXiv:1811.03804(2018).
29.QLi,CTai,WE,Stochasticmodifiedequationsandadaptivestochasticgradientalgorithmsin
Proceedingsofthe34thInternationalConferenceonMachineLearning-Volume70.(JMLR.org),
pp.2101–2110(2017).
30.JAn,JLu,LYing,Stochasticmodifiedequationsfortheasynchronousstochasticgradient
descent.arXivpreprintarXiv:1805.08244(2018).
31.RJia,etal.,TowardsefficientdatavaluationbasedontheshapleyvalueinThe22ndInternational
ConferenceonArtificialIntelligenceandStatistics.pp.1167–1176(2019).
32.YKwon,JZou,Betashapley:aunifiedandnoise-reduceddatavaluationframeworkformachine
learninginInternationalConferenceonArtificialIntelligenceandStatistics.(PMLR),pp.
8780–8802(2022).
33.TWang,RJia,Databanzhaf:Adatavaluationframeworkwithmaximalrobustnesstolearning
stochasticity.arXivpreprintarXiv:2205.15466(2022).
34.VFeldman,CZhang,Whatneuralnetworksmemorizeandwhy:Discoveringthelongtailvia
influenceestimation.Adv.NeuralInf.Process.Syst.33,2881–2891(2020).
35.XXu,ZWu,CSFoo,BKHLow,Validationfreeandreplicationrobustvolume-baseddata
valuation.Adv.NeuralInf.Process.Syst.34,10837–10848(2021).
36.HAJust,etal.,LAVA:Datavaluationwithoutpre-specifiedlearningalgorithmsinTheEleventh
InternationalConferenceonLearningRepresentations.(2023).
37.SMLundberg,SILee,AunifiedapproachtointerpretingmodelpredictionsinProceedingsofthe
31stinternationalconferenceonneuralinformationprocessingsystems.pp.4768–4777(2017).
38.ICovert,SLundberg,SILee,Explainingbyremoving:Aunifiedframeworkformodelexplanation.
J.Mach.Learn.Res.22,1–90(2021).
39.JStier,GGianini,MGranitzer,KZiegler,Analysingneuralnetworktopologies:agametheoretic
approach.ProcediaComput.Sci.126,234–243(2018).
40.AGhorbani,JYZou,Neuronshapley:Discoveringtheresponsibleneurons.Adv.NeuralInf.
T
Process.Syst.33,5922–5932(2020).
41.RHLSim,YZhang,MCChan,BKHLow,Collaborativemachinelearningwithincentive-aware
modelrewardsinInternationalConferenceonMachineLearning.(PMLR),pp.8927–8936(2020).
42.XXu,etal.,Gradientdrivenrewardstoguaranteefairnessincollaborativemachinelearning.Adv.
NeuralInf.Process.Syst.34,16104–16117(2021). F
43.TYan,ADProcaccia,Ifyoulikeshapleythenyou’lllovethecoreinProceedingsoftheAAAI
ConferenceonArtificialIntelligence.Vol.35,pp.5751–5759(2021).
44.BRozemberczki,etal.,Theshapleyvalueinmachinelearning.arXivpreprintarXiv:2202.05594
(2022). A
45.JYoon,SArik,TPfister,DatavaluationusingreinforcementlearninginInternationalConference
onMachineLearning.(PMLR),pp.10842–10851(2020).
46.SBasu,PPope,SFeizi,InfluencefunctionsindeeplearningarefragileinInternational
ConferenceonLearningRepresentations.(2020).
47.YBachrach,etal.,Approximatingpowerindices:theoreticalandempiricalanalysisR.Auton.
AgentsMulti-AgentSyst.20,105–122(2010).
48.HJKushner,Onthestochasticmaximumprinciple:Fixedtimeofcontrol.J.Math.AnalysisAppl.
11,78–92(1965).
49.HKushner,Necessaryconditionsforcontinuousparameterstochasticoptimizationproblems.
SIAMJ.onControl.10,550–565(1972). D
50.FLChernousko,ALyubushin,Methodofsuccessiveapproximationsforsolutionofoptimalcontrol
problems.Optim.Control.Appl.Methods3,101–114(1982).
51.QLi,LChen,CTai,EWeinan,Maximumprinciplebasedalgorithmsfordeeplearning.J.Mach.
Learn.Res.18,1–29(2018).
52.JShu,etal.,Meta-weight-net:LearninganexplicitmappingforsampleweightinginNeurIPS.
(2019).
53.HVonStackelberg,SHVon,Thetheoryofthemarketeconomy.(OxfordUniversityPress),(1952).
54.IEKumar,SVenkatasubramanian,CScheidegger,SFriedler,Problemswithshapley-value-based
explanationsasfeatureimportancemeasuresinInternationalConferenceonMachineLearning.
(PMLR),pp.5491–5500(2020).
55.RSerban,ACHindmarsh,CVODES:Thesensitivity-enabledODEsolverinSUNDIALSinVolume
6:5thInternationalConferenceonMultibodySystems,NonlinearDynamics,andControl,PartsA,
B,andC.(ASMEDC),pp.257–269(2005).
56.JBJorgensen,Adjointsensitivityresultsforpredictivecontrol,state-andparameter-estimation
withnonlinearmodelsin2007EuropeanControlConference(ECC).(IEEE,Kos),pp.3649–3656
(2007).
57.AShrikumar,PGreenside,AKundaje,Learningimportantfeaturesthroughpropagatingactivation
differencesinInternationalconferenceonmachinelearning.(PMLR),pp.3145–3153(2017).
58.KJiang,WLiang,JYZou,YKwon,Opendataval:aunifiedbenchmarkfordatavaluation.Adv.
NeuralInf.Process.Syst.36(2023).
59.MFeurer,etal.,Openml-python:anextensiblepythonapiforopenml.J.Mach.Learn.Res.22,
1–5(2021).
60.JGama,PMedas,GCastillo,PRodrigues,LearningwithdriftdetectioninAdvancesinArtificial
Intelligence–SBIA2004:17thBrazilianSymposiumonArtificialIntelligence,SaoLuis,Maranhao,
Brazil,September29-Ocotber1,2004.Proceedings17.(Springer),pp.286–295(2004).
61.DGreene,PCunningham,Practicalsolutionstotheproblemofdiagonaldominanceinkernel
documentclusteringinProceedingsofthe23rdinternationalconferenceonMachinelearning.pp.
377–384(2006).
62.AMaas,etal.,LearningwordvectorsforsentimentanalysisinProceedingsofthe49thannual
meetingoftheassociationforcomputationallinguistics:Humanlanguagetechnologies.pp.
142–150(2011).
63.ACoates,ANg,HLee,Ananalysisofsingle-layernetworksinunsupervisedfeaturelearningin
Proceedingsofthefourteenthinternationalconferenceonartificialintelligenceandstatistics.
(JMLRWorkshopandConferenceProceedings),pp.215–223(2011).
64.AKrizhevsky,,etal.,Learningmultiplelayersoffeaturesfromtinyimages.(Citeseer),(2009).
Leadauthorlastname etal. 13SupportingInformation
1. TheAxiomsofExistingDataValuationMethod
ThepopularityoftheShapleyvalueisattributabletothefactthatitistheuniquedatavaluenotionsatisfyingthefollowingfivecommon
axioms(38),asfollow
• Efficiency: Thevaluesadduptothedifferenceinvaluebetweenthegrandcoalitionandtheemptycoalition: P i∈Nϕ(xi,yi;U)=
U(N)−U(∅);
• Symmetry: ifU(S∪(xi,yi))=U(S∪(xj,yj))forallS∈N\{(xi,yi),(xj,yj)},thenϕ(xi,yi;U)=ϕ(xj,yj;U);
• Dummy: ifU(S∪(xi,yi))=U(S)forallS∈N\(xi,yi),thenitmakes0marginalcontribution,sothevalueofthedatapoint
(xi,yi)shouldbeϕ(xi,yi;U)=0;
• Additivity: FortwoutilityfunctionsU1,U2 andarbitraryα1,α2∈R,thetotalcontributionofadatapoint(xi,yi)isequaltothe
sumofitscontributionswhencombined: ϕ((xi,yi);α1U1(S)+α2U2(S))=α1ϕ(xi,yi;U1(S))+α2ϕ(xi,yi;U2(S));
• Marginalism: FortwoutilityfunctionsU1,U2,ifeachdatapointhastheidenticalmarginalimpact,theyreceivesamevaluation:
U1(S∪(xi,yi))−U1(S)=U2(S∪(xi,yi))−U2(S)holdsforall((xi,yi);S),thenitholdsthatϕ(xi,yi;U1)=ϕ(xi,yi;U2).
2. DetailsofLearningStochasticDynamic
A. BasicMSA.Inthissubsection,weillustratetheiterativeupdateprocessofthebasicMSAinAlg.1,comprisingthestateequation,the
costateequation,andthemaximizationoftheHamiltonian.
T
Algorithm 1 Basic MSA
1: Initializeψ0={ψ0∈Ψ:t=0...,T −1}.
t
2: fork=0toK do
3: SolvetheforwardSDE: F
dX tk=b(X tk,ψ tk)dt+σdWt, X 0k=x,
4: SolvethebackwardSDE: A
(cid:0)
dY tk=−∇xH X tk,Y tk,ψ tk)dt+Z tkdWt, Y Tk=−∇xΦ(X Tk),
5: Foreacht∈[0,T −1],updatethestatecontrolψk+1:
t
R
ψk+1=argmaxH(Xk,Yk,Zk,ψ).
t t t t
ψ∈Ψ
D
3. DerivationofRe-weightingStrategy
A. ConvergenceProofofTheTrainingLoss.
Lemma 1. Let (an) n≤1,(bn)
n≤1
be two non-negative real sequences such that the series P∞ i=1an diverges, the series P∞ i=1anbn
converges, and there exists K>0 such that |bn+1−bn|≤Kan. Then the sequences (bn)
n≤1
converges to 0.
Theorem 1. Assume the training loss function L is Lipschitz smooth with constant L, and V(·) is differential with a δ-bounded gradient
and twice differential with its Hessian bounded by B, and L have ρ-bounded gradients concerning training/metadata points. Let the
learning rate αk satisfies αk = min{1, k}, for some k > 0, such that k < 1, and βk,1 ≤ k ≤ K is a monotone descent sequence,
βk=min{1, √c } for some c>0, suchT that σ√ T ≥L and P∞ βk≤T ∞,P∞ (βk)2≤∞.Then
L σ T c k=1 k=1
lim E[∥∇L(ψk;θk+1)∥2]=0. [43]
2
k→∞
Proof. Itiseasytoconcludethatαk satisfyP∞ αk=∞,P∞ (αk)2<∞. Recalltheupdateofψ ineachiterationasfollows
k=0 k=0
N
α X
ψk+1=ψk+
N
∇ ψHi(ψ,V(Φ i(ψ Tk);θk+1))| ψk. [44]
i=1
Itcanbewrittenas
ψk+1=ψk+αk∇H(ψk;θk+1)| , [45]
υk
where∇H(ψk;θ)=−∇L(ψk;θ). Sincethemini-batchυk isdrawnuniformlyatrandom,wecanrewritetheupdateequationas:
(cid:2) (cid:3)
ψk+1=ψk+αk ∇H(ψk;θk+1)+υk , [46]
whereυk=∇H(ψk;θk+1)| −∇H(ψk;θk+1). Notethatυk isi.i.d. randomvariablewithfinitevariancesinceΥk aredrawni.i.d. with
Υk
afinitenumberofsamples. Furthermore,E[υk]=0,sincesamplesaredrawnuniformlyatrandom,andE[∥υk∥2]≤σ2.
2
AntonioSclocchiandMatthieuWyart 1of15TheHamiltonianH(ψ;θ)definedinEq.38canbeeasilycheckedtobeLipschitz-smoothwithconstantL,andhaveρ-boundedgradients
concerningtrainingdata. Observethat
H(ψk+1;θk+2)−H(ψk;θk+1)
[47]
(cid:8) (cid:9) (cid:8) (cid:9)
= H(ψk+1;θk+2)−H(ψk+1;θk+1) + H(ψk+1;θk+1)−H(ψk;θk+1) .
Forthefirstterm,
H(ψk+1;θk+2)−H(ψk+1;θk+1)
N T−1
1 XX(cid:8)(cid:2) (cid:3)(cid:2) (cid:3) (cid:9)
=
N
V(Φ i(ψ Tk);θk+2)−V(Φ i(ψ Tk);θk+1) b(ψ tk)Yt(ψ tk)+σZt(ψ tk) −Ri(ψ tk)
i=1 t=0
≤ N1 XN T X−1(cid:26)(cid:20) ⟨∂V(Φ i ∂( θψ Tk);θ)(cid:12) (cid:12)
(cid:12)
θk,θk+1−θk⟩+ 2δ ∥θk+1−θk∥2 2(cid:21) (cid:2) b(ψ tk)Yt(ψ tk)+σZt(ψ tk)(cid:3) −Ri(ψ tk)(cid:27)
i=1 t=0
= N1 XN T X−1(cid:26)(cid:20) ⟨∂V(Φ i ∂( θψ Tk);θ)(cid:12) (cid:12)
(cid:12)
θk,−βk(cid:2) ∇ℓ(ψˆk(θk))+ξk(cid:3) ⟩+ δβ 2k2 ∥∇ℓ(ψˆk(θk))+ξk∥2 2(cid:21) (cid:2) b(ψ tk)Yt(ψ tk)+σZt(ψ tk)(cid:3) −Ri(ψ tk)(cid:27) [48]
i=1 t=0
= N1 XN T X−1(cid:26)(cid:20) ⟨∂V(Φ i ∂( θψ Tk);θ)(cid:12) (cid:12)
(cid:12)
θk,−βk(cid:2) ∇ℓ(ψˆk(θk))+ξk(cid:3) ⟩+ δβ 2k2 ∥∇ℓ(ψˆk(θk))+ξk∥2 2(cid:21) (cid:2) b(ψ tk)Yt(ψ tk)+σZt(ψ tk)(cid:3) −Ri(ψ tk)(cid:27)
i=1 t=0
=1 XN T X−1(cid:26)(cid:20) ⟨∂V(Φ i(ψ Tk);θ)(cid:12) (cid:12) ,−βk(cid:2) ∇ℓ(ψˆk(θk))+ξk(cid:3) ⟩+ δ(βk)2 (cid:0) ∥∇ℓ(ψˆk(θkT))∥2+∥ξk∥2
N ∂θ (cid:12) θk 2 2 2
i=1 t=0
+2(cid:10) ∇ℓ(ψˆk(θk)),ξk(cid:11)(cid:1)(cid:3)(cid:2)
b(ψ tk)Yt(ψ tk)+σZt(ψ
tk)(cid:3)
−Ri(ψ
tk)(cid:9)
Forthesecondterm, F
H(ψk+1;θk+1)−H(ψk;θk+1)
L
≤⟨∇H(ψk;θk+1),ψk+1−ψk⟩+ ∥ψk+1−Aψk∥2
2 2
L(αk)2
=⟨∇H(ψk;θk+1),−αk[∇L(ψk;θk+1)+υk]⟩+ ∥∇H(ψk;θk+1)+υk∥2
2 2
L(αk)2 RL(αk)2
=−(αk− )∥∇H(ψk;θk+1)∥2+ ∥υk∥2−(α −L(αk)2)⟨∇H(ψk;θk+1),υk⟩. [49]
2 2 2 2 k
Therefore,wehave:
H(ψk+1;θk+2)−H(ψk;D θk+1)≤ 1 XN (cid:26) ⟨∂V(Φ i(ψk);θ)(cid:12) (cid:12) ,−βk(cid:2) ∇ℓ(ψˆk(θk))+ξk(cid:3) ⟩+
N ∂θ (cid:12) θk
i=1
(cid:27)
+δ(β 2K)2
(∥∇ℓ(ψˆk(θk))∥2 2+∥ξk∥2 2+2⟨∇ℓ(ψˆk(θk)),ξk⟩) Φ i(ψk)
[50]
L(αk)2 L(αk)2
−(αk− )∥∇H(ψk;θk+1)∥2+ ∥υk∥2−(αk−L(αk)2)⟨∇H(ψk;θk+1),υk⟩.
2 2 2 2
Takingexpectationofbothsidesof(49)andsinceE[ξk]=0,E[υk]=0,wehave
E(cid:2) H(ψk+1;θk+2)(cid:3) −E(cid:2) H(ψk;θk+1)(cid:3) ≤E N1 XN Hi(ψk)(cid:26) ⟨∂V(Φ i ∂( θψk);θ)(cid:12) (cid:12)
(cid:12)
θk,−βk(cid:2) ∇ℓ(ψˆk(θk))(cid:3) ⟩+
i=1
(cid:27)
+δ(βk)2 (∥∇ℓ(ψˆk(θk))∥2+∥ξk∥2) −αkE(cid:2) ∥∇H(ψk;θk+1)∥2(cid:3) + L(αk)2 (cid:8) E(cid:2) ∥∇H(ψk;θk+1)∥2(cid:3) +E(cid:2) ∥υk∥2(cid:3)(cid:9)
2 2 2 2 2 2 2
Summinguptheaboveinequalitiesoverk=1,...,∞inbothsides,weobtain
X∞ αkE(cid:2) ∥∇H(ψk;θk+1)∥2 2(cid:3) +X∞ βkE N1 XN ∥Hi(ψk)∥∥∂V(Φ i ∂( θψk);θ)(cid:12) (cid:12)
(cid:12)
θk∥·∥∇ℓ(ψˆk(θk))∥
k=1 k=1 i=1
∞
XL(αk)2(cid:8) (cid:2) (cid:3) (cid:2) (cid:3)(cid:9) (cid:2) (cid:3) (cid:2) (cid:3)
≤ E ∥∇H(ψk;θk+1)∥2 +E ∥υk∥2 +E H(ψ1;θ2) −limE H(ψk+1;θk+2)
2 2 2
T→∞
k=1
( )
∞ N
+Xδ(β 2k)2 n1 X ∥Hi(ψk)∥(E∥∇ℓ(ψˆk(θk))∥2 2+E∥ξk∥2
2
k=1 i=1
∞ ∞
XL(αk)2 (cid:2) (cid:3) Xδ(βk)2 (cid:8) (cid:9)
≤ {ρ2+σ2}+E Htr(ψ1;θ2) + M(ρ2+σ2) ≤∞.
2 2
k=1 k=1
2of15 AntonioSclocchiandMatthieuWyartThelastinequalityholdssinceP∞ k=0(αk)2<∞,P∞ k=0(βk)2<∞,and N1 PN i=1∥Hi(ψk)∥≤M forlimitednumberofdatapoints’loss
isbounded. Thuswehave
X∞
αkE[∥∇H(ψk;θk+1)∥2
2]+X∞
βkE
n1 Xn ∥Hi(ψk)∥∥∂V(Φ
i
∂( θψk);θ)(cid:12)
(cid:12)
(cid:12)
θk∥·∥∇ℓ(ψˆk(θk))∥≤∞. [51]
k=1 k=1 i=1
Since
X∞
βkE
N1 XN ∥Hi(ψk)∥∥∂V(Φ
i
∂( θψk);θ)(cid:12)
(cid:12)
(cid:12)
θk∥·∥∇ℓ(ψˆk(θk))∥
k=1 i=1 [52]
∞
X
≤Mδρ βk≤∞,
k=1
whichimpliesthatP∞ k=1αkE[∥∇H(ψk;θk+1)∥2 2]<∞. ByLemma1,tosubstantiatelim t→∞E[∇H(ψk;θk+1)∥2 2]=0,sinceP∞ k=0αk=
∞,itonlyneedstoprove
(cid:12) (cid:12)
(cid:12)E[∇H(ψk+1;θk+2)∥2]−E[∇H(ψk;θk+1)∥2](cid:12)≤Cαk, [53]
2 2
forsomeconstantC. Basedontheinequality
|(∥a∥+∥b∥)(∥a∥−∥b∥)|≤∥a+b∥∥a−b∥,
wethenhave
(cid:12) (cid:2) (cid:3)(cid:12)
(cid:12)E[∥∇H(ψk+1;θk+2)∥2]−E ∥∇H(ψk;θk+1)∥2 (cid:12)
2 2 T
(cid:12) (cid:2) (cid:3)(cid:12)
=(cid:12)E (∥∇H(ψk+1;θk+2)∥2+∥∇H(ψk;θk+1)∥2)(∥∇H(ψk+1;θk+2)∥2−∥∇H(ψk;θk+1)∥2) (cid:12)
(cid:2)(cid:12) (cid:12)(cid:12) (cid:12)(cid:3)
≤E (cid:12)∥∇H(ψk+1;θk+1)∥2+∥∇H(ψk;θk)∥2)(cid:12)(cid:12)(∥∇H(ψk+1;θk+2)∥2−∥∇H(ψk;θk+1)∥2)(cid:12)
(cid:2)(cid:13) (cid:13) (cid:13) F (cid:13) (cid:3)
≤E (cid:13)∇H(ψk+1;θk+2)+∇H(ψk;θk+1)(cid:13) (cid:13)∇H(ψk+1;θk+2)−∇H(ψk;θt+1)(cid:13)
2 2
(cid:2) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:3)
≤E ((cid:13)∇H(ψk+1;θk+2)(cid:13) +(cid:13)∇H(ψk;θk+1)(cid:13) )(cid:13)∇H(ψk+1;θk+2)−∇H(ψk;θk+1)(cid:13)
2 2 2
(cid:2) (cid:3) A
≤2LρE ∥(ψk+1,θk+2)−(ψk,θk+1)∥2
(cid:2)(cid:13)(cid:0) (cid:1)(cid:13) (cid:3)
≤2Lρα kβ kE (cid:13) ∇H(ψk;θk+1)+υk,∇ℓ(θk+1)+ξk+1 (cid:13) [54]
2
hp p i
≤2Lρα β E ∥∇H(ψk;θk+1)+υRk∥2+ ∥∇ℓ(θk+1)+ξk+1∥2
k k 2 2
q
(cid:2) (cid:3) (cid:2) (cid:3)
≤2Lρα β E ∥∇H(ψk;θk+1)+υk∥2 +E ∥∇ℓ(θk+1)+ξk+1∥2
k k 2 2
q (cid:2) D (cid:3) (cid:2) (cid:3) (cid:2) (cid:3) (cid:2) (cid:3)
≤2Lρα β E ∥∇H(ψk;θk+1)∥2 +E ∥υk∥2 +E ∥ξk+1∥2 +E ∥∇ℓ(θk+1)∥2
k k 2 2 2 2
p
≤2Lραkβk 2σ2+2ρ2
p
≤2 2(σ2+ρ2)Lρβ1αk.
Accordingtotheaboveinequality,wecanconcludethatouralgorithmcanachieve
(cid:2) (cid:3)
lim E ∥∇H(ψk;θk+1)∥2 =0. [55]
2
k→∞
Theproofiscompleted.
B. ConvergenceProofofTheMetaLoss.AssumethatwehaveasmallamountofmetadatasetwithM datapoints{(x′,y′),1≤i≤M}
i i
withcleanlabels,andthemetalossisEq.16. Let’ssupposewehaveanotherN trainingdatapoints,{(xi,yi),1≤i≤N},whereM ≪N,
andthetraininglossisEq.15.
Lemma 2. Assume the meta loss function ℓ is Lipschitz smooth with constant L, and V(·) is differential with a δ-bounded gradient
and twice differential with its Hessian bounded by B. The loss function has ρ-bounded gradients concerning metadata points. Then, the
gradient of θ concerning meta loss ℓ is Lipschitz continuous.
Proof. Thegradientofθ withrespecttometalossℓi canbewrittenas:
(cid:12)
∇
θℓi(ψˆk(θ))(cid:12)
(cid:12)
θk
=−αXn (cid:18) ∂ℓi(ψˆ)(cid:12) (cid:12)T ∂Φ j(ψ)(cid:12)
(cid:12)
(cid:19) ∂V(Φ j(ψk);θ)(cid:12)
(cid:12) . [56]
n ∂ψˆ (cid:12) ψˆk ∂ψ (cid:12) ψk ∂θ (cid:12) θk
j=1
LetVj(θ)=V(Φ j(ψk);θ)andGij beingdefinedinEq.75. TakinggradientofθinbothsidesofEq.56,wehave
∇2 θ2ℓi(ψˆk(θ))(cid:12) (cid:12)
θk
= − nαXn h ∂∂ θ(Gij)(cid:12) (cid:12) θk∂V ∂j θ(θ)(cid:12) (cid:12) θk+(Gij)∂2 ∂V θj 2(θ)(cid:12) (cid:12) θki .
j=1
AntonioSclocchiandMatthieuWyart 3of15Forthefirsttermontheright-handside,wehavethat
(cid:13) (cid:13) (cid:13)∂∂ θ(Gij)(cid:12) (cid:12) θk∂V ∂j θ(θ)(cid:12) (cid:12) θk(cid:13) (cid:13) (cid:13)≤δ(cid:13) (cid:13) (cid:13) (cid:13)∂∂ ψˆ(cid:18) ∂ℓ ∂i( θψˆ)(cid:12) (cid:12) θk(cid:19) (cid:12) (cid:12)T ψˆk∂L ∂j ψ(ψ)(cid:12) (cid:12) ψk(cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13) ! (cid:13)
=δ(cid:13) (cid:13) (cid:13) (cid:13)∂∂ ψˆ ∂ℓ ∂i ψ( ˆψˆ)(cid:12) (cid:12) ψˆk− nαXn ∂L ∂m ψ(ψ)(cid:12) (cid:12) ψk∂V ∂k θ(θ)(cid:12) (cid:12) θk (cid:12) (cid:12)T ψˆk∂L ∂j ψ(ψ)(cid:12) (cid:12) ψk(cid:13) (cid:13) (cid:13)
(cid:13)
m=1
(cid:13) ! (cid:13)
=δ(cid:13) (cid:13) (cid:13)
(cid:13)
∂2 ∂ℓ ψi ˆ( 2ψˆ)(cid:12) (cid:12) ψˆk− nαXn ∂L ∂m ψ(ψ)(cid:12) (cid:12) ψk∂V ∂m θ(θ)(cid:12) (cid:12) θk (cid:12) (cid:12)T ψˆk∂L ∂j ψ(ψ)(cid:13) (cid:13) (cid:13) (cid:13)≤αLρ2δ2, [57]
m=1
since(cid:13) (cid:13) (cid:13)∂2 ∂ℓ ψi ˆ( 2ψˆ)(cid:12) (cid:12) ψˆk(cid:13) (cid:13) (cid:13)≤L,(cid:13) (cid:13) (cid:13)∂Φ ∂j ψ(ψ)(cid:12) (cid:12) ψk(cid:13) (cid:13) (cid:13)≤ρ,(cid:13) (cid:13) (cid:13)∂V ∂j θ(θ)(cid:12) (cid:12) θk(cid:13) (cid:13) (cid:13)≤δ. Forthesecondterm,wehave
(cid:13) (cid:13) (cid:13)(Gij)∂2 ∂V θj 2(θ)(cid:12) (cid:12) θk(cid:13) (cid:13) (cid:13)=(cid:13) (cid:13) (cid:13) (cid:13)∂ℓ ∂i ψ( ˆψˆ)(cid:12) (cid:12)T ψˆk∂L ∂j ψ(ψ)(cid:12) (cid:12) ψk∂2 ∂V θj 2(θ)(cid:12) (cid:12) θk(cid:13) (cid:13) (cid:13) (cid:13)≤Bρ2, [58]
since(cid:13) (cid:13) (cid:13)∂ℓ ∂i ψ( ˆψˆ)(cid:12) (cid:12)T ψˆk(cid:13) (cid:13) (cid:13)≤ρ,(cid:13) (cid:13) (cid:13)∂2 ∂V θj 2(θ)(cid:12) (cid:12) θk(cid:13) (cid:13) (cid:13)≤B. CombiningtheabovetwoinequalitiesEq.57and58,wehave
(cid:13) (cid:13) (cid:13)∇2 θ2ℓi(ψˆk(θ))(cid:12) (cid:12) θk(cid:13) (cid:13) (cid:13)≤αρ2(αLδ2+B). [59]
DefineLV =αρ2(αLδ2+B),basedonLagrangemeanvaluetheorem,wehave:
∥∇ℓ(ψˆk(θ1))−∇ℓ(ψˆk(θ2))∥≤LV∥θ1−θ2∥, forarbTitrary θ1,θ2, [60]
(cid:12)
where∇ℓ(ψˆk(θ1))=∇θℓi(ψˆk(θ))(cid:12) .
θ1
Theorem 2. AssumethemetalossfunctionℓisLipschitzsmoothwithconstaFntL,andV(·)isdifferentialwithaδ-boundedgradientand
twice differential with its Hessian bounded by B, and ℓ have ρ-bounded gradients concerning metadata points. Let the learning rate αk
satisfies αk=min{1, k}, for some k>0, such that k <1, and βk,1≤k≤K is a monotone descent sequence, β1=min{1, √c } for
some c>0, such thatT σ√ cT ≥L and P∞ t=1βt≤∞,T P∞ t=1β t2≤∞A. Then meta network can achieve E[∥∇ℓ(ψˆk(θk))∥2 2]≤L ϵ inσ OT (1/ϵ2)
steps. More specifically,
min
E[∥∇ℓ(ψˆk(θk))∥2]≤O(√C
), [61]
2
0≤k≤KR T
where C is some constant independent of the convergence process, σ is the variance of randomly drawing uniformly mini-batch data
points.
Proof. Theupdateofθ ink-thiterationisasDfollows
θk+1=θk−
mβ Xm
∇
θℓi(ψˆk(θ))(cid:12)
(cid:12)
(cid:12)
θk. [62]
i=1
Thiscanbewrittenas:
θk+1=θk−βk∇ℓ(ψˆk(θk))(cid:12)
(cid:12) . [63]
Ξk
Sincethemini-batchΞk isdrawnuniformlyfromtheentiredataset,wecanrewritetheupdateequationas
θk+1=θk−βk[∇ℓ(ψˆk(θk))+ξk], [64]
whereξk =∇ℓ(ψˆk(θk))(cid:12) (cid:12) −∇ℓ(ψˆk(θk)). Notethatξk arei.i.drandomvariableswithfinitevariancesinceΞk aredrawni.i.dwitha
Ξk
finitenumberofsamples. Furthermore,E[ξk]=0,sincesamplesaredrawnuniformlyatrandom. Observethat
ℓ(ψˆk+1(θk+1))−ℓ(ψˆk(θk))
[65]
=(cid:8) ℓ(ψˆk+1(θk+1))−ℓ(ψˆk(θk+1))(cid:9) +(cid:8) ℓ(ψˆk(θk+1))−ℓ(ψˆk(θk))(cid:9)
.
ByLipschitzsmoothnessofmetalossfunctionℓ,wehave
ℓ(ψˆk+1(θk+1))−ℓ(ψˆk(θk+1))
≤⟨∇ℓ(ψˆk(θk+1)),ψˆk+1(θk+1)−ψˆk(θk+1)⟩+ L ∥ψˆk+1(θk+1)−ψˆk(θk+1)∥2
2 2
(cid:12)
Sinceψˆk+1(θk+1)−ψˆk(θk+1)=−α nk Pn i=1V(Φ i(ψk+1);θk+1)∇ ψΦ i(ψ)(cid:12)
(cid:12)
accordingtoEq.18,wehave
ψk+1
∥ℓ(ψˆk+1(θk+1))−ℓ(ψˆk(θk+1))∥≤αkρ2+
L(αk)2
ρ2=αkρ2(1+
αkL
) [66]
2 2
Since(cid:13) (cid:13) (cid:13) (cid:13)∂Φ ∂j ψ(ψ)(cid:12) (cid:12) (cid:12) ψk(cid:13) (cid:13) (cid:13) (cid:13)≤ρ,(cid:13) (cid:13) (cid:13) (cid:13)∂ℓ ∂i ψ( ˆψˆ)(cid:12) (cid:12) (cid:12)T ψˆk(cid:13) (cid:13) (cid:13) (cid:13)≤ρ.
4of15 AntonioSclocchiandMatthieuWyartByLipschitzcontinuityof∇ℓ(ψˆk(θ))accordingtoLemma2,wecanobtainthefollowing
ℓ(ψˆk(θk+1))−ℓ(ψˆk(θk))≤⟨∇ℓ(ψˆk(θk)),θk+1−θk⟩+ L ∥θk+1−θk∥2
2 2
=⟨∇ℓ(ψˆk(θk)),−βk[∇ℓ(ψˆk(θk))+ξk]⟩+
L(βk)2
∥∇ℓ(ψˆk(θk))+ξk∥2
2 2
=−(βk−
L(βk)2
)∥∇ℓ(ψˆk(θk))∥2+
L(βk)2
∥ξk∥2−(βk−L(βk)2)⟨∇ℓ(ψˆk(θk)),ξk⟩. [67]
2 2 2 2
ThusEq.65satisfies
ℓ(ψˆk+1(θk+1))−ℓ(ψˆk(θk))≤αkρ2(1+
αkL
)−(βk−
L(βk)2
)
2 2
∥∇ℓ(ψˆk(θk))∥2+
L(βk)2
∥ξk∥2−(βk−L(βk)2)⟨∇ℓ(ψˆk(θk)),ξk⟩. [68]
2 2 2
Rearrangingtheterms,wecanobtain
(βk−
L(βk)2
)∥∇ℓ(ψˆk(θk))∥2≤αkρ2(1+
αkL
)+ℓ(ψˆk(θk))−ℓ(ψˆk+1(θk+1))
2 2 2
+
L(βk)2
∥ξk∥2−(βk−L(βk)2)⟨∇ℓ(ψˆk(θk)),ξk⟩. [69]
2 2
Summinguptheaboveinequalitiesandrearrangingtheterms,wecanobtain
XK (βk− L(βk)2 )∥∇ℓ(ψˆk(θk))∥2≤ℓ(ψˆ1)(θ1)−ℓ(ψˆK+1(θK+1))
2 2
k=1 T
K K K
+X
αkρ2(1+
αkL )−X (βk−L(βk)2)⟨∇ℓ(ψˆk(θk)),ξk⟩+LX
(β )2∥ξk∥2
2 2 k 2
k=1 k=1 k=1
F
K K K
≤ℓ(ψˆ1(θ1))+X
αkρ2(1+
αkL )−X (βk−L(βk)2)⟨∇ℓ(ψˆk(θk)),ξk⟩+LX
(βk)2∥ξk∥2, [70]
2 2 2
k=1 k=1 k=1
A
TakingexpectationswithrespecttoξK onbothsidesofEq.70,wecanthenobtain:
K K K
X
(βk−
L(βk)2
)E
∥∇ℓ(ψˆk(θk))∥2≤ℓ(ψˆ1(θ1))+X
αkρ2(1+
αkL
)+
Lσ2 X
(βk)2, [71]
2 ξK 2 2 2
R
k=1 k=1 k=1
sinceE ⟨∇ℓ(θk),ξk⟩=0andE[∥ξk∥2]≤σ2,whereσ2 isthevarianceofξk. Furthermore,wecandeducethat
ξK 2
minE(cid:2) ∥∇ℓ(ψˆk(θk))∥2(cid:3)
2
k D
PK (βk− L(βk)2)E ∥∇ℓ(ψˆk(θk))∥2
≤ k=1 2 ξK 2
PK (βk− L(βk)2)
k=1 2
" #
K K
≤
1 2ℓ(ψˆ1(θ1))+X αkρ2(2+αkL)+Lσ2X
(βk)2
PK (2βk−L(βk)2)
k=1 k=1 k=1
" #
K K
≤
1 2ℓ(ψˆ1(θ1))+X αkρ2(2+αkL)+Lσ2X
(βk)2
PK
βk
k=1 k=1 k=1
" #
K
≤
1 2ℓ(ψˆ1(θ1))+α1ρ2T(2+L)+Lσ2X
(βk)2
Kβk
k=1
2ℓ(ψˆ1(θ1)) 1 2α1ρ2(2+L) Lσ2 XK
= + + βk
K βk βk K
k=1
2ℓ(ψˆ1(θ1)) 1 2α1ρ2(2+L)
≤ + +Lσ2βk
K βk βk
√ √
ℓ(ψˆ1(θ1)) σ K k σ T 1 c
= max{L, }+min{1, }max{L, }ρ2(2+L)+Lσ2min{ , √ }
K c K c L σ T
σℓ(ψˆ1(θ1) kσρ2(2+L) Lσc
≤ √ + √ + √
c K c K K
1
=O(√ ). [72]
K
The third inequality holds for PK (2βk−L(βk)2)≥PK βk. Therefore, we can conclude that our algorithm can always achieve
k=1 k=1
min 0≤k≤KE[∥∇ℓ(θk)∥2 2]≤O(√1 K)inK steps,andthisfinishesourproofofTheorem2.
AntonioSclocchiandMatthieuWyart 5of15C. DerivationofTheUpdateDetailsforMetaParameters.Recalltheupdatedequationoftheparametersofdatapointsre-weightingstrategy
asfollows
θk+1=θk−β
m1 Xm
∇
θℓi(ψˆk(θ))(cid:12)
(cid:12)
(cid:12)
θk. [73]
i=1
ThecomputationofEq.73inthepaperbybackpropagationcanbeunderstoodbythefollowingderivation
m1 Xm
∇
θℓi(ψˆk(θ))(cid:12)
(cid:12)
(cid:12)
θk
i=1
=1 Xm ∂ℓi(ψˆ)(cid:12)
(cid:12)
Xn ∂ψˆk(θ) ∂V(Φ j(ψk);θ)(cid:12)
(cid:12)
m ∂ψˆ(θ)(cid:12) ψˆk ∂V(Φ j(ψk);θ) ∂θ (cid:12) θk
i=1 j=1
[74]
=
−α Xm ∂ℓi(ψˆ)(cid:12)
(cid:12)
Xn ∂Φ j(ψ)(cid:12)
(cid:12)
∂V(Φ j(ψk);θ)(cid:12)
(cid:12)
n∗m ∂ψˆ (cid:12) ψˆk ∂ψ (cid:12) ψk ∂θ (cid:12) θk
i=1 j=1
!
=−αXn 1 Xm ∂ℓi(ψˆ)(cid:12) (cid:12)T ∂Φ j(ψ)(cid:12)
(cid:12)
∂V(Φ j(ψk);θ)(cid:12)
(cid:12) .
n m ∂ψˆ (cid:12) ψˆk ∂ψ (cid:12) ψk ∂θ (cid:12) θk
j=1 i=1
Let
Gij = ∂ℓ ∂i ψ( ˆψˆ)(cid:12) (cid:12) (cid:12)T ψˆk∂Φ ∂j ψ(ψ)(cid:12) (cid:12)
(cid:12)
ψk, T [75]
bysubstitutingEq.74intoEq. (73),wecanget:
!F
θk+1=θk+ α nβ Xn m1 Xm Gij ∂V(Φ j ∂( θψk);θ)(cid:12) (cid:12)
(cid:12)
θk. [76]
j=1 i=1
A
4. DetailedDescriptionoftheWeightedMean-FieldMSA
A. WeightedMean-fieldMSA.In this subsection, we illustrate the iterative update process of the weighted mean-field MSA in Alg.2,
R
comprisingthestateequation,thecostateequation,andthemaximizationoftheHamiltonian.
Algorithm 2 Weighted Mean-field MSA
D
1: Initializeψ0={ψ0∈Ψ:t=0...,T −1}.
t
2: fork=0toK do
3: SolvetheforwardSDE:
(cid:2) (cid:3)
dX tk= a(µt−X tk)+ψ tk dt+σdWt, X 0k=x,
4: SolvethebackwardSDE:
(cid:0)
dY tk=−∇xH X tk,Y tk,ψ tk)dt+Z tkdWt, Y Tk=−V(Φ(ψ Tk);θ)∇xΦ(X Tk),
5: Foreacht∈[0,T −1],updatethestatecontrolψk+1:
t
ψk+1=argmaxH(Xk,Yk,Zk,ψ).
t t t t
ψ∈Ψ
B. ErrorEstimatefortheWeightedMean-FieldMSA.Inthissection,wederivearigorouserrorestimatefortheweightedmean-fieldMSA,
aidingincomprehendingitsstochasticdynamic. Letusnowmakethefollowingassumptions:
Assumption 1. The terminal cost function Φ in Eq.37 is twice continuously differentiable, with Φ and ∇Φ satisfying a Lipschitz
condition. There is a constant K≥0 such that ∀XT,X T′ ∈R,∀ψT ∈Ψ
|Φ(XT,µT)−Φ(X T′ ,µT)|+∥∇Φ(XT,µT)−∇Φ(X T′ ,µT)∥≤K(cid:13) (cid:13) (cid:13)VX (ΦT (ψ− TX );T′ θ)(cid:13) (cid:13)
(cid:13)
Assumption 2. From Eq.37, The running cost function R and the drift function b are jointly continuous in t and twice continuously
differentiable in Xt, with b,∇xb,R,∇xR satisfying Lipschitz conditions in Xt uniformly in t and ψt,. Since σ is constant, it has no
effect on the error estimate. There exists K≥0 such that ∀x∈Rd,∀ψt∈Ψ,∀t∈[0,T −1]
∥b(Xt,ϕt)−b(X t′,ϕt)∥+∥∇xb(Xt,ϕt)−∇xb(X t′,ϕt)∥+|R(Xt,ϕt)−R(X t′,ϕt)|+∥∇xR(Xt,ϕt)−∇xR(X t′,ϕt)∥≤K∥Xt−X t′∥
Withtheseassumptions,weprovidethefollowingerrorestimateforweightedmean-fieldMSA.
6of15 AntonioSclocchiandMatthieuWyartLemma 3. Suppose Assumptions2 and 1 hold. Then for arbitrary admissible controls ψ and ψ′ there exists a constant C>0 such that
N T−1
XX(cid:2) (cid:3)
L(ψ)−L(ψ′)≤− H(Xi,t,Yi,t,Pi,t,ψt)−H(Xi,t,Yi,t,Pi,t,ψ t′)
i=1 t=0
N T−1
C XX
+
N
∥b(Xi,t,Yi,t,Pi,t,ψt)−b(Xi,t,Yi,t,Pi,t,ψ t′)∥2
i=1 t=0
N T−1
C XX
+
N
∥∇xb(Xi,t,Yi,t,Pi,t,ψt)−∇xb(Xi,t,Yi,t,Pi,t,ψ t′)∥2,
i=1 t=0
N T−1
C XX
+
N
∥∇xR(Xi,t,Yi,t,Pi,t,ψt)−∇xR(Xi,t,Yi,t,Pi,t,ψ t′)∥2, [77]
i=1 t=0
TheprooffollowsadiscreteGronwall’slemma.
Lemma 4. Let K≥0 and ut, wt, be non-negative real valued sequences satisfying
ut+1≤Kut+wt,
for t=0,...,T −1. Then, we have for all t=0,...,T,
!
T−1
X
ut≤max(1,KT) u0+ ws .
T
s=0
Proof. Weprovebyinductiontheinequality
!
t−1
X
ut≤max(1,Kt) u0+ Fws , [78]
s=0
fromwhichthelemmafollowsimmediately. Thecaset=0istrivial. Supposetheaboveistrueforsomet,wehave
ut+1≤Kut+wt A
!
t−1
X
≤Kmax(1,Kt) u0+ ws +wt
R s=0
!
t−1
X
≤max(1,Kt+1) u0+ ws +max(1,Kt+1)wt
s=0
D !
t
X
=max(1,Kt+1) u0+ ws .
s=0
ThisprovesEq.(78)andhencethelemma.
LetusnowcommencetheproofofapreliminarylemmathatestimatesthemagnitudeofYt forarbitraryψ∈Ψ. Hereafter,C willbe
standforarbitrarygenericconstantthatdoesnotdependonψ,ψ′ andS (batchsize)butmaydependonotherfixedquantitiessuchasT
andtheLipschitzconstantsK inAssumption1-2. Also,thevalueofC isallowedtochangetoanotherconstantvaluewiththesame
dependenciesfromlinetolinetoreducenotationalclutter.
Lemma 5. There exists a constant C>0 such that for each t=0,...,T and ψ∈Ψ, we have
C
∥Yi,t∥≤ .
N
for all i=1,...,N.
Proof. First,noticethatYi,t=− N1∇Φ(Xi,t)andsobyAssumption1,wehave
1 K
∥Yi,t∥= ∥∇Φ(Xi,t)∥≤ .
N N
Now,foreach0≤t<T,wehavebyAssumption2inthemaintext,
∥Yi,t∥=∥∇xH(Xi,t,Yi,t+1,ψ t′)∥
1
≤∥∇xb(Xi,t,ψ t′)TYi,t+1∥+ N∇x∥R(Xi,t,ψt)∥
K
≤K∥Yi,t+1∥+
N
UsingLemma4witht→T −t,weget
K TK C
∥Yi,t∥≤max(1,KT)( + )= .
N N N
AntonioSclocchiandMatthieuWyart 7of15WearenowreadytoproveTheorem3.
Proof. RecalltheHamiltoniandefinition
Hi(Xt,Yt,Zt,µt,ψt)=[a(µt−Xt)+ψt]·Yt+σ⊤Zt−R(Xt,µt,ψt).
Letusdefinethequantity
T−1
X
I(Xt,Yt,ψ):= [Yt+1·Xt+1−H(Xt,Yt+1,ψt)−R(Xt,ψt)]
t=0
Then,weconsiderthefollowinglinearizedform
b(Xt+1,ψt+1)=b(Xt,ψt)+∇xb(Xt,ψt)(ψt−Xt)
=[a(µt−Xt)+ψt]+∇x[a(µt−Xt)+ψt](ψt−Xt), [79]
FromEq.79,weknowthatI(Xt,Yt,ψ)=0forarbitraryi=1,...,N andψ∈Ψ. Letusnowfixsomesampleiandobtaincorresponding
estimates. Wehave T
0=I(Xt,Yt,ψ)−I(X t′,Y t′,ψ′)
N T−1 F
XX(cid:2) (cid:3)
= Yi,t+1·Xi,t+1−Y i′ ,t+1·X i′
,t+1
i=1 t=0
N T−1 A
XX(cid:2) (cid:3)
− Hi(Xi,t,Yi,t+1,ψt)−Hi(X i′ ,t,Y i′ ,t+1,ψ t′)
i=1 t=0
N T−1
1 XXR(cid:2) (cid:3)
−
N
Ri(Xi,t,ψt)−Ri(X i′ ,t,ψ t′)
i=1 t=0
[80]
D
Wecanrewritethefirsttermontheright-handsideas
N T−1
XX(cid:2) (cid:3)
Yi,t+1·Xi,t+1−Y i′ ,t+1·X i′
,t+1
i=1 t=0
N T−1
XX
= [Yi,t+1·∆Xi,t+1+Xi,t+1·∆Yi,t+1−∆Xi,t+1·∆Yi,t+1], [81]
i=1 t=0
wherewehavedefined∆Xi,t:=Xi,t−X i′
,t
and∆Yi,t:=Yi,t−Y i′ ,t. Wemaysimplifyfurtherbyobservingthat∆Xi,0=0,andso
N T−1
XX
[Yi,t+1·∆Xi,t+1+Xi,t+1·∆Yi,t+1]
i=1 t=0
( )
N T−1
X X
= Yi,T ·∆Xi,T + [Yi,t·∆Xi,t+Xi,t+1·∆Yi,t+1]
i=1 t=0
( )
N T−1
X X(cid:2) (cid:3)
= Yi,T ·∆Xi,T + ∇xHi(Xi,t,Yi,t+1,ψ t′)·∆Xi,t+∇yHi(Xi,t,Yi,t+1,ψ t′)·∆Yi,t+1
i=1 t=0
BydefiningtheextendedvectorPi,t:=(Xi,t,Yi,t+1)andP i′ ,t:=(X i′ ,t,Y i′ ,t+1),wecanrewritethisas
( )
N T−1 N T−1
XX X X
[Yi,t+1·∆Xi,t+1+Xi,t+1·∆Yi,t+1]= Yi,T ·∆Xi,T + ∇pHi(Pi,t,ψt)·∆Pi,t [82]
i=1 t=0 i=1 t=0
8of15 AntonioSclocchiandMatthieuWyartSimilarly,wealsohave
N T−1
XX
∆Xi,t+1·∆Yi,t+1
i=1 t=0
N T−1
1XX
=
2
[∆Xi,t+1·∆Yi,t+1+∆Xi,t+1·∆Yi,t+1]
i=1 t=0
( )
N T−1
1X 1X(cid:2) (cid:3)
=
2
∆Xi,T ·∆Yi,T +
2
∇pHi(Pi,t,ψt)−∇pHi(P i′ ,t,ψ t′) ·∆Pi,t
i=1 t=0
( )
N T−1 T−1
1X 1X(cid:2) (cid:3) 1X
=
2
∆Xi,T ·∆Yi,T +
2
∇pH(Pi,t,ψt)−∇zH(Pi,t,ψ t′) ·∆Pi,t+
2
∆Pi,t·∇2 zH(Pi,t+r1(t)∆Pi,t,ψt)∆Pi,t [83]
i=1 t=0 t=0
whereinthelastline,weusedTaylor’stheoremwithr1(t)∈[0,1]foreacht. Now,wecanrewritetheterminaltermsinEq.82andEq.83
asfollows:
1
(Yi,T + 2∆Yi,T)·∆Xi,T
1 1 (cid:2) (cid:3) T
=− N∇Φ(Xi,T)·∆Xi,T −
2N
∇Φ(Xi,T)−∇Φ(Xi,T) ·∆Xi,T
1 1
=− N∇Φ(Xi,T)·∆Xi,T − 2N∆Xi,T ·∇2Φ(Xi,T +r2∆Xi,T)∆Xi,T
F
1 1
=− N(Φ(XT)−Φ(xT))− 2N∆Xi,T ·[∇2Φ(Xi,T +r2∆Xi,T)+∇2Φ(Xi,T +r3∆Xi,T)]∆Xi,T, [84]
A
forsomer2,r3∈[0,1]. Lastly,foreacht=0,1,...,T −1wehave
H(Pi,t,ψt)− RH(P i′ ,t,ψ t′)
(cid:2) (cid:3)
= H(Pi,t,ψt)−H(Pi,t,ψ t′)
+∇pH(Pi,t,ψt)·∆Pi,t
1
+D 2∆Pi,t·∇2 pH(Pi,t+r4(t)∆Pi,t,ψt)∆Pi,t [85]
wherer4(t)∈[0,1].
SubstitutingEq.81-85intoEq.80yields
" # " #
N T−1 N T−1
1 X X 1 X X
N
Ri(Xi,t,ψt)+V(Φ i(ψT);θ)Φ i(Xi,T) −
N
Ri(X i′ ,t,ψ t′)+V(Φ i(ψ T′ );θ)Φ i(X i′ ,T)
i=1 t=0 i=1 t=0
N T−1
XX(cid:2) (cid:3)
=− Hi(Xt,Yt+1,ψt)−Hi(Xt,Yt+1,ψ t′)
i=1 t=0
N
1 X(cid:8) (cid:2) (cid:3) (cid:9)
+
2N
∆Xi,T ·V(Φ i(ψT);θ) ∇2Φ i(Xi,T +r2∆Xi,T)+∇2Φ i(Xi,T +r3∆Xi,T) ∆Xi,T
i=1
N T−1
1XX(cid:2) (cid:3)
+
2
∇pHi(Pi,t,ψt)−∇pHi(Pi,t,ψ t′) ·∆Pi,t
i=1 t=0
N T−1
1XX (cid:2) (cid:3)
+
2
∆Pi,t· ∇2 pHi(Pi,t+r1(t)∆Pi,t,ψt)−∇2 pHi(Pi,t+r4(t)∆Pi,t,ψt) ∆Pi,t [86]
i=1 t=0
Note that by summing over all s, the left hand side is simply L(ψ)−L(ψ′). Let us further simplify the right hand side. First, by
Assumption1,wehave
(cid:2) (cid:3)
∆Xi,T ·V(Φ i(ψT);θ) ∇2Φ(Xi,T +r2∆Xi,T)+∇2Φ(Xi,T +r3∆Xi,T) ∆Xi,T ≤K∥∆Xi,T∥2. [87]
AntonioSclocchiandMatthieuWyart 9of15Next,
(cid:2) (cid:3)
∇zH(Pi,t,ψt)−∇zH(Pi,t,ψ t′) ·∆Pi,t
≤∥∇xH(Xi,t,Yi,t+1,ψt)−∇xH(Xi,t,Yi,t+1,ψ t′)∥∥∆Xi,t∥
+∥∇yH(Xi,t,Yi,t+1,ψt)−∇yH(Xi,t,Yi,t+1,ψ t′)∥∥∆Yi,t+1∥
1 N
≤ 2N∥∆Xi,t∥2+ 2∥∇xH(Xi,t,Yi,t+1,ψt)−∇xH(Xi,t,Yi,t+1,ψ t′)∥2
N 1
+ 2∥∆Yi,t∥2+ 2N∥∇yH(Xi,t,Yi,t+1,ψt)−∇yH(Xi,t,Yi,t+1,ψ t′)∥2
1 C2
≤ 2N∥∆Xi,t∥2+ 2N∥∇xb(Xi,t,ψt)−∇xb(Xi,t,ψ t′)∥2
1
+ 2N∥∇xR(Xi,t,ψt)−∇xR(Xi,t,ψ t′)∥2
N 1
+ 2∥∆Yi,t∥2+ 2N∥b(Xi,t,ψt)−b(Xi,t,ψ t′)∥2, [88]
whereinthelastline,wehaveusedLemma5. Similarly,wecansimplifythelastterminEq.86. NoticethatthesecondderivativeofH
concerningY vanishessinceitislinear. Hence,asinEq.87andusingLemma5,wehave
(cid:2) (cid:3)
∆Pi,t· ∇2 zH(Pi,t+r1(t)∆Pi,t,ψt)−∇2 zH(Pi,t+r4(t)∆Pi,t,ψt) ∆Pi,t
2KC
≤ ∥∆Xi,t∥2+4K∥∆Xi,t∥∥∆Yi,t+1∥
N T
2KC 2K
≤ ∥∆Xi,t∥2+ ∥∆Xi,t∥2+2KN∥∆Yi,t+1∥2 [89]
N N
SubstitutingEq.87-89intoEq.86,asfollow
F
" # " #
T−1 T−1
1 X 1 X
N
Ri(Xi,t,ψt)+V(Φ i(ψT);θ)Φ i(Xi,T) A−
N
Ri(X i′ ,t,ψ t′)+V(Φ i(ψ T′ );θ)Φ i(X i′ ,T)
t=0 t=0
N T−1
XX(cid:2) (cid:3)
=− Hi(Xt,Yt+1,ϕt)−Hi(Xt,Yt+1,ψ t′)
i=1 t=0 R
N T N T−1
C XX XX
+ ∥∆Xi,t∥2+CN ∥∆Yi,t+1∥2
N
i=1 t=0 i=1 t=0
D
N T−1
C XX
+
N
∥b(Xi,t,ψt)−b(Xi,t,ψ t′)∥2
i=1 t=0
N T−1
C XX
+
N
∥∇xb(Xi,t,ψt)−∇xb(Xi,t,ψ t′)∥2
i=1 t=0
N T−1
C XX
+
N
∥∇xRi(Xi,t,ψt)−∇xRi(Xi,t,ψ t′)∥2 [90]
i=1 t=0
Itremainstoestimatethemagnitudesof∆Xi,t and∆Yi,t. Observethat∆Xi,0=0,hencewehaveforeacht=0,...,T −1
∥∆Xi,t+1∥≤∥b(Xi,t,ψt)−b(X i′ ,t,ϕt)∥+∥b(Xi,t,ψt)−b(X i′ ,t,ψ t′)∥
≤K∥∆Xi,t∥+∥b(Xi,t,ψt)−b(Xi,t,ψ t′)∥
UsingLemma4,wehave
N T−1
XX
∥∆Xi,t∥≤C ∥b(Xi,t,ψt)−b(Xi,t,ψ t′)∥ [91]
i=1 t=0
Similarly,
∥∆Yi,t∥≤∥∇xHi(Xi,t,Yi,t+1,ψt)−∇xHi(X i′ ,t,Y i′ ,t+1,ψ t′)∥
C
≤2K∥∆Yi,t+1∥+ ∥∆Xi,t∥
N
C
+ N∥∇xb(Xi,t,ψt)−∇xb(Xi,t,ψ t′)∥
C
+ N∥∇xRi(Xi,t,ψt)−∇xRi(Xi,t,ψ t′)∥,
10of15 AntonioSclocchiandMatthieuWyartandsobyLemma4,Eq.91andthefactthat∥∆Yi,T∥≤ K N∥∆Xi,T∥byAssumption1,wehave
N T−1
C XX
∥∆Yi,t∥≤
N
∥b(Xi,t,ψt)−b(Xi,t,ψ t′)∥
i=1 t=0
N T−1
C XX
+
N
∥∇xb(Xi,t,ψt)−∇xb(Xi,t,ψ t′)∥
i=1 t=0
N T−1
C XX
+
N
∥∇xR(Xi,t,ψt)−∇xR(Xi,t,ψ t′)∥. [92]
i=1 t=0
Finally,weconcludetheproofofTheorem3bysubstitutingestimatesEq.91andEq.92intoEq.90.
5. ProofofOurDataValuationMethod
A. ProofoftheDataStateUtilityFunction.Inthissubsection,weprovidedetailedproofofthedatastateutilityfunctionfromtheoptimal
statesensitivity.
Theorem 3. Assume that a set of data points undergoes a single iteration of model training, yielding an optimal control strategy, the
form of the utility function corresponding to these data points is then as follows
Ui(S)=−Xi,T ·Yi,T, T
Proof. F
B. ProofofDynamicMarginalContribution.Inthissubsection,weprovideadetailedproofofdynamicmarginalcontributioninProposition1.
A
Proposition1. Forarbitrarytwodatapoints(xi,yi)and(xj,yj),i̸=j∈[N],ifUi(S)>Uj(S),then∆(xi,yi;Ui(S))>∆(xj,yj;Uj(S)).
Proof. Fori̸=j,ifUi(S)>Uj(S),wehave R
∆(xi,yi;Ui(S))−∆(xj,yj;Uj(S))
   
=Ui(S)− DX U Ni′ −(S 1) −Uj(S)− X U Nj′ −(S 1) 
i′∈{1,...,N}\i j′∈{1,...,N}\j
 
1 X X
=[Ui(S)−Uj(S)]+ N−1 U j′(S)− U i′(S)
j′∈{1,...,N}\j i′∈{1,...,N}\i
1
=[Ui(S)−Uj(S)]+ N−1[Ui(S)−Uj(S)]
N
= N−1[Ui(S)−Uj(S)]
>0. [93]
Then,theproofiscomplete.
C. ProofofDynamicDataValuation.Inthissubsection,weprovideadetailedproofprocessdemonstratinghowourproposeddynamicdata
valuationmetricsatisfiesthecommonaxioms.
Fortheefficiencyproperty,Iftherearenodatapointswithinthecoalition,thennodatastatesarepresented. Inthiscase,itstill
holdsthat
X X
ϕ(xi,yi;Ui)= ∆(xi,yi;Ui)
i∈N i∈N
 
=X
Ui(S)−
X NUj −(S 1)

i∈N j∈{1,...,N}\i
=−Xt·V(Φ(Xt;µT))YT −0
=U(N)−U(∅), [94]
AntonioSclocchiandMatthieuWyart 11of15Forthesymmetryproperty,thevaluetodatapoint(x′ i,y i′)isϕ((x′ i,y i′);U i′). ForarbitraryS∈N\{(xi,yi),(x′ i,y i′)},ifU(S∪(xi,yi))=
U(S∪(x′,y′)),theproofofthispropertyasfollow
i i
ϕ(x′,y′;U′(S))=∆(x′,y′;U′(S))
i i i i i i
=U′(S)−
X Uj(S)
i N−1
j∈{1,...,N}\{i′,i}
=Ui(S)−
X NUj −(S 1)
j∈{1,...,N}\{i,i′}
=ϕ(xi,yi;Ui(S)), [95]
Forthedummyproperty,sinceU(S∪(xi,yi))=U(S)forallS∈N\(xi,yi)alwaysholds,thevaluetothedatapoint(xi,yi)is
ϕ(xi,yi;Ui(S))=∆(xi,yi;Ui(S))
=Ui(S)−
X NUj −(S 1)
j∈{1,...,N}\i
=0, [96]
Fortheadditivityproperty,wechoosethetwoutilityfunctionsU1 andU2 together,impactingthedatapoint(xi,yi). Thevaluetoitfor
arbitraryα1,α2∈Ris
ϕ(xi,yi;α1U1,i(S)+α2U2,i(S))=∆(xi,yi;α1U1,i(S)+α2U2,i(S))
=[α1U1,i(S)+α2U2,i(S)]−
X Tα1U1,j(S N)+ −α 12U2,j(S)
j∈{1,...,N}\i
   
=α1U1,i(S)− X U NF 1,j −(S 1) +α2U2,i(S)− X U N2,j −(S 1) 
j∈{1,...,N}\i j∈{1,...,N}\i
=α1ϕ((xi,yi);U1,i(S)A)+α2ϕ((xi,yi);U2,i(S)), [97]
For the marginalism property, if each data point has the identical marginal impact in two utility functions U1,U2, satisfies U1(S∪
(xi,yi))−U1(S)=U2(S∪(xi,yi))−U2(S). Theproofofthispropertyisasfollows
R
ϕ(xi,yi;U1,i)=∆(xi,yi,U1,i(S))
=U1,i(S)− X U N1,j −(S 1)
D j∈{1,...,N}\i
=U1(S∪(xi,yi))−U1(S)
=U2(S∪(xi,yi))−U2(S)
=U2,i(S)− X U N2,j −(S 1)
j∈{1,...,N}\i
=ϕ(xi,yi;U2,i). [98]
Then,theproofofthefivecommonaxiomsiscomplete.
Itisevidentthatourproposeddynamicdatavaluationmetricsatisfiesthecommonaxioms. Then,weprovidethefollowingProposition2
toidentifyimportantdatapointsamongtwoarbitrarilydifferentdatapoints.
Proposition2. Forarbitrarytwodatapoints(xi,yi)and(xj,yj),i̸=j∈[N],ifUi(S)>Uj(S),thenϕ(xi,yi;Ui(S))>ϕ(xj,yj;Uj(S)).
Proof. Fori̸=j,ifUi(S)>Uj(S),wehave
ϕ(xi,yi;Ui(S))−ϕ(xj,yj;Uj(S))=∆(xi,yi;Ui(S))−∆(xj,yj;Uj(S))
   
=Ui(S)− X U Ni′ −(S 1) −Uj(S)− X U Nj′ −(S 1) 
i′∈{1,...,N}\i j′∈{1,...,N}\j
 
1 X X
=[Ui(S)−Uj(S)]+ N−1 U j′(S)− U i′(S)
j′∈{1,...,N}\j i′∈{1,...,N}\i
1
=[Ui(S)−Uj(S)]+ N−1[Ui(S)−Uj(S)]
N
= N−1[Ui(S)−Uj(S)]
>0. [99]
12of15 AntonioSclocchiandMatthieuWyartThen,theproofiscomplete.
Moreover,thefollowingProposition3showsthatourproposeddynamicdatavaluationmetricconvergestotheLOOmetric.
Proposition 3. For arbitrary i∈[n], if ∥Ui(S)−U(S∪(xi,yi))∥≤ε, then ∥NDDV−LOO∥≤2ε.
Proof. Forarbitrarypairdatapoints(xi,yi)and(xj,yj),i̸=j∈[N]. TheLOOmetricdifferencebetweenthosedatapointsis
ϕloo(xi,yi;U(S))−ϕloo(xj,yj;U(S))
=[U(S∪(xi,yi))−U(S)]−[U(S∪(xj,yj))−U(S)]
=U(S∪(xi,yi))−U(S∪(xj,yj)), [100]
Then,callforEq.93,wehave
N
ϕnddv(xi,yi;Ui(S))−ϕnddv(xj,yj;Uj(S))= N−1[Ui(S)−Uj(S)] [101]
Inaddition,theL2 errorboundbetweentheNDDVandLOOmetricis
∥NDDV−LOO∥=∥[ϕnddv((xi,yi),Ui(S))−ϕnddv((xj,yj),Uj(S))]−[ϕloo((xi,yi),U(S))−ϕloo((xj,yj),U(S))]∥
N
=∥ N−1[Ui(S)−Uj(S)]−[U(S∪(xi,yi))−U(S∪(xj,yj))]∥
1
=∥[Ui(S)−U(S∪(xi,yi))]−[Uj(S)−U(S∪(xj,yj))]+ N−1[Ui(S)−Uj(S)]∥
1
≤∥Ui(S)−U(S∪(xi,yi))∥+∥Uj(S)−U(S∪(xj,yj))∥+T N−1∥Ui(S)−Uj(S)∥
=2ε, N →∞. [102]
Concludestheproof. F
Finally,wealsoprovidetheL2 errorboundbetweenourproposeddynamicdatavaluationmetricandShapleyvaluemetricinthe
followingProposition4.
A
Proposition 4. For arbitrary i∈[N], if ∥Ui(S)−U(S∪(xi,yi))∥≤ε, then ∥NDDV−Shap∥≤ N2ε −N 1.
Proof. Forarbitrarypairdatapoints(xi,yi)and(xj,yj),i̸=j∈[N]. TheShapleyvaluemetricdifferentbetweenthosedatapointsis
R
ϕshap(xi,yi;U(S))−ϕshap(xi,yi;U(S))
N N
1 X 1 X
=
N
∆ i′(xi,yi;U(S))−
N
D∆ j′(xi,yi;U(S))
i′=1 j′=1
X |S|!(N−|S|−1)! X |S|!(N−|S|−1)!
=
N!
[U(S∪(xi,yi))−U(S)]−
N!
[U(S∪(xj,yj))−U(S)]
S∈D\(xi,yi) S∈D\(xj,yj)
X |S|!(N−|S|−1)!
=
N!
[U(S∪(xi,yi))−U(S∪(xj,yj))]
S∈D\(xi,yi)
X |S|!(N−|S|−1)!
+
N!
[U(S∪(xi,yi))−U(S)]
S∈{T|T∈D,(xi,yi)∈/T,(xj,yj)∈T}
X |S|!(N−|S|−1)!
−
N!
[U(S∪(xj,yj))−U(S)]
S∈{T|T∈D,(xi,yi)∈T,(xj,yj)∈/T}
X |S|!(N−|S|−1)!
=
N!
[U(S∪(xi,yi))−U(S∪(xj,yj))]
S∈D\{(xi,yi),(xj,yj)}
X (|S′|+1)!(N−|S′|−2)!(cid:2) (cid:3)
+
N!
U(S′∪(xi,yi))−U(S′∪(xj,yj))
S′∈D\{(xi,yi),(xj,yj)}
X h|S|!(N−|S|−1)! (|S|+1)!(N−|S|−2)!i
=
N!
+
N!
[U(S∪(xi,yi))−U(S∪(xj,yj))]
S∈D\{(xi,yi),(xj,yj)}
1 X 1
=
N−1 C|S|
[U(S∪(xi,yi))−U(S∪(xj,yj))]. [103]
S∈D\{(xi,yi),(xj,yj)} N−2
Then,callforEq.93,wehave
N
ϕnddv(xi,yi;Ui(S))−ϕnddv(xj,yj;Uj(S))= N−1[Ui(S)−Uj(S)] [104]
AntonioSclocchiandMatthieuWyart 13of15Inaddition,theL2 errorboundbetweentheNDDVmethodandShapleyvaluemetricis
(cid:2) (cid:3)
∥NDDV−Shap∥=∥[ϕnddv(xi,yi;Ui(S))−ϕnddv(xj,yj;Uj(S))]− ϕshap(xi,yi;U(S))−ϕshap(xj,yj;U(S)) ∥
(cid:13) N 1 X 1 (cid:13)
=(cid:13) (cid:13)N−1[Ui(S)−Uj(S)]−
N−1 C|S|
[U(S∪(xi,yi))−U(S∪(xj,yj))](cid:13)
(cid:13)
S∈D\{(xi,yi),(xj,yj)} N−2
(cid:13) N 1 N X−2 1 (cid:13)
=(cid:13) (cid:13)N−1[Ui(S)−Uj(S)]−
N−1
C kN−2
C|S|
[U(S∪(xi,yi))−U(S∪(xj,yj))](cid:13)
(cid:13)
k=0 N−2
( )
N (cid:13) N X−2 1 (cid:13)
≤ N−1(cid:13) (cid:13)[Ui(S)−Uj(S)]−min C kN−2
NC|S|
[U(S∪(xi,yi))−U(S∪(xj,yj))](cid:13)
(cid:13)
k=0 N−2
N (cid:13) 1 (cid:13)
= N−1(cid:13) (cid:13)[Ui(S)−Uj(S)]−2N−2
N−2
[U(S∪(xi,yi))−U(S∪(xj,yj))](cid:13)
(cid:13)
NC 2
N−2
N
≤ N−1∥Ui(S)−U(S∪(xi,yi))∥+∥Uj(S)−U(S∪(xj,yj))∥
2εN
= . [105]
N−1
Concludestheproof.
6. ExperimentsDetails T
Inthissection,weprovidedetailsoftheexperiment. OurPython-basedimplementationcodesarepubliclyavailableathttps://github.com/
liangzhangyong.
F
A. Datasets.Inthesubsection,sixdistinctdatasetswereutilized,detailedinTab.S1,encompassingapairofdatasetsacrossthreecategories:
tabular,textual,andimage.
A
TableS1.Asummaryofsixclassificationdatasetsusedinourexperiments.
Dataset SampleSize InputDimension NumberofClasses MinorClassProportion DataType Source
R
pol 15000 48 2 0.336 Tabular (59)
electricity 38474 6 2 0.5 Tabular (60)
2dplanes 40768 10 2 0.499 Tabular (59)
bbc 2225 768 5 0.17 Text (61)
IMDB 50000 D768 2 0.5 Text (62)
STL10 5000 96 10 0.01 Image (63)
CIFAR10 50000 2048 10 0.1 Image (64)
B. HyperparametersforDataValuationMethods.Inthissubsection,weexploretheimpactofhyperparametersformarginalcontribution-based
datavaluationmethods.
• ForLOO(5),wedonotneedtosetarbitraryparameters,maintainingthedefaultparametervalues. Fortheutilityfunction,we
choosethetestaccuracyofabasemodeltrainedonthetrainingsubset.
• ForDataShapley(7)andBetaShapley(32),weuseaMonteCarlomethodtoapproximatethevalueofdatapoints. Specifically,the
processbeginsbyestimatingthemarginalcontributionsofdatapoints. Followingthis,theShapleyvalueiscomputedbasedonthese
marginalcontributions,servingasthedata’sassignedvalue. Accordingly,weconfiguretheindependentMonteCarlochainsto10,
thestoppingthresholdto1.05,thesamplingepochto100,andtheminimumtrainingsetcardinalityto5.
• ForDataBanzhaf(33),wesetthenumberofutilityfunctionstobe1000. Moreover,Weuseatwo-layerMLPwith256neuronsinthe
hiddenlayerforlargerdatasetsand100neuronsforsmallerones.
• ForInfluenceFunction(34),wealsosetthenumberofutilityfunctionstobe1000. Subsequently,thecardinalityofeachsubsetisset
to0.7,indicatingthat70%ofitconsistsofdatapointstobeevaluated.
• ForKNNShapley(10),wesetthenumberofnearestneighborstobeequaltothesizeofthevalidationset. Thisistheonlyparameter
thatrequiressetting.
• ForAME(12),wesetthenumberofutilityfunctionstobe1000. Weconsiderthesameuniformdistributionforconstructingsubsets.
Foreachp∈{0.2,0.4,0.6,0.8},werandomlygenerate250subsetssuchthattheprobabilitythatadatumisincludedinthesubsetisp.
AsfortheLassomodel,weoptimizetheregularizationparameterusing‘LassoCV’in‘scikit-learn’withitsdefaultparametervalues.
• ForData-OOB(13),wesetthenumberofweakclassifiersto1000,correspondingtoutilityfunction. Theseweakclassifiersarea
randomforestmodelwithdecisiontrees,andtheparametersaresettothedefaultvaluesin’scikit-learn’.
14of15 AntonioSclocchiandMatthieuWyart• Ourmethod,wesetthesizeofthemeta-datasettobeequaltothesizeofthevalidationset. Themetanetwork’shiddenlayersize
issetat10%ofthemeta-datasize. Forthetrainingparameters,wesetthemaxepochsto50. Bothbaseoptimizationandmeta
optimizationuseAdamoptimizer,withtheinitiallearningrateforout-optimizationsetat0.01andforin-optimizationat0.001. For
baseoptimization,uponreaching60%ofthemaximumepochs,thelearningratedecaysto10%ofitsinitialvalue,andat80%ofthe
maximumepochs,itfurtherdecaysby10%. Formetaoptimization,onlyoneepochoftrainingisrequired. Tomaintaintheoriginal
trainingstepsize,weightsofallexamplesinatrainingbatcharenormalizedtosumuptoone,enforcingtheconstraint|V(ℓ;θ)|=1,
andthenormalizedweight
η ik= P jVk(Φ j;θV )k +(Φ δi (; Pθ) iVk(Φ j;θ)),
wherethefunctionδ(a),settoτ >0ifa=0and0otherwise,preventsdegenerationofVk(Φ j;θ)tozerosinamini-batch,stabilizing
themetaweightlearningratewhenusedwithbatchnormalization.
C. PseudoAlgorithm.WeprovideapseudoalgorithminAlg3.
Algorithm 3 Pseudo-code of NDDV training
Input: TrainingdataD,meta-datasetD′,batchsizen,m,maxiterationsT.
Output: Thevalueofdatapoints: ϕ(xi,yi;U(S)).
1: InitializeThebaseoptimizationparameterψ0 andthemetaoptimizationparameterθ0.
2: fort=0toT −1do
T
3: {x,y}←SampleMiniBatch(D,n).
4:
{x′,y′}←SampleMiniBatch(D′,m).
5: Formulatethebasetrainingfunctionψˆk(θ)byEq.18. F
6: Updatethebaseoptimizationparametersθk+1 byEq.18.
7: Updatethemetaoptimizationparametersψk+1 byEq.18. A
8: Updatetheweightedmean-fieldstateµk byEq.34.
t
9: ComputethedatastateutilityfunctionUi(S)byEq.40.
R
10: Computethedynamicmarginalcontribution∆(xi,yi;Ui)byEq.41.
11: Computethevalueofdatapointsϕ(xi,yi;U(S))byEq.42.
D
D. ConvergenceVerificationforTheTrainingLoss.TovalidatetheconvergenceresultsobtainedinTheorem1and2inthepaper,weplot
thechangingtendencycurvesoftrainingandmetalosseswiththenumberofepochsinourexperiments,asshowninFig.S1-S2. The
convergencetendencycanbeeasilyobservedinthefigures,substantiatingthepropernessofthetheoreticalresultsinproposedtheorems.
E. DataPointsRemovalandAdditionExperiment.Inthissection,weconsideranalyzingtheevolutionofdatapointsremovalandaddition
insixdatasets.
F. ResultsonDataStateTrajectories.Inthissection,weconsideranalyzingtheevolutionofdatastatetrajectoriesinsixdatasets.
G. ResultsonDataValueTrajectories.Inthissection,weconsideranalyzingtheevolutionofdatastatetrajectoriesinsixdatasets.
H. AdditionalResultsonElapsedTimeComparison.In this section, we present comparing the elapsed time of our method with LOO,
DataShapley,BetaShapley,andDataBanzhafinsmalldatasetsizes. Asetofdatasizesnis{5.0×102,7.0×102,1.0×103,3.0×103,5.0×103}.
I. AdditionalResultsonTheImpactofAddingVariouslevelsofLabelNoise.Inthissection,weexploretheimpactofaddingvariouslevelsof
labelnoise. Here,weconsiderthesixdifferentlevelsoflabelnoiseratepnoise∈{5%,10%,20%,30%,40%,45%}.
J. AdditionalResultsonTheImpactofAddingVariouslevelsofFeatureNoise.Inthissection,weexploretheimpactofaddingvariouslevels
offeaturenoise. Here,weconsiderthesixdifferentlevelsoffeaturenoiseratepnoise∈{5%,10%,20%,30%,40%,45%}.
K. AblationStudy.WeperformanablationstudyonthehyperparametersintheproposedNDDVmethod,whereweprovideinsightson
theimpactofsettingchanges. Weusethemislabeleddetectionusecaseandtheplcdatasetasanexamplesettingfortheablationstudy.
Foralltheexperimentsinthemaintext,theimpactofre-weightingdatapointsontheNDDVmethodisinitiallyexplored. Asshown
inFig.S11,theNDDVmethodenhancedwithdatapointre-weightingdemonstratessuperiorperformanceindetectingmislabeleddataand
removingoraddingdatapoints. Additionally,inthesameexperiments,Fig.S12showstheimpactofmean-fieldinteractionsparametera
at1,3,5,10. Itisobservedthatwhena=10,theNDDVmethodexhibitspoorermodelperformance,whereas,fortheothervalues,it
showssimilarperformance. Furthermore,weexploretheimpactofthediffusionconstantσ ontheNDDVmethod. Specifically,wesetσ
AntonioSclocchiandMatthieuWyart 15of15× 10(cid:2)(cid:1) D a ta set: 2 d p la n es × 10(cid:2)(cid:1) D a ta set: electricity × 10(cid:2)(cid:1) D a ta set: b b c
1.2 0.4 0.0
0.0 0.0 (cid:7)(cid:2)(cid:1)(cid:6)
(cid:9)(cid:3)(cid:1)(cid:4) (cid:8)(cid:2)(cid:1)(cid:5) (cid:7)(cid:3)(cid:1)(cid:2)
(cid:9)(cid:4)(cid:1)(cid:6) (cid:8)(cid:2)(cid:1)(cid:7) (cid:7)(cid:3)(cid:1)(cid:6)
(cid:9)(cid:5)(cid:1)(cid:7) (cid:8)(cid:3)(cid:1)(cid:4) (cid:7)(cid:4)(cid:1)(cid:2)
(cid:9)(cid:6)(cid:1)(cid:8) (cid:8)(cid:3)(cid:1)(cid:6) (cid:7)(cid:4)(cid:1)(cid:6)
(cid:9)(cid:7)(cid:1)(cid:2) (cid:8)(cid:4)(cid:1)(cid:2) (cid:7)(cid:5)(cid:1)(cid:2)
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
E p och s E p och s E p och s
× 10(cid:2)(cid:1) D a ta set: IM D B × 10(cid:2)(cid:1) D a ta set: S T L 1 0 × 10(cid:2)(cid:1) D a ta set: C IF A R 1 0
2
0.0 0.0
1 (cid:8)(cid:2)(cid:1)(cid:5)
(cid:6)(cid:2)(cid:1)(cid:5)
(cid:8)(cid:2)(cid:1)(cid:7) 0 (cid:6)(cid:3)(cid:1)(cid:2)
(cid:8)(cid:3)(cid:1)(cid:4)
(cid:4)(cid:1) T(cid:6)(cid:3)(cid:1)(cid:5)
(cid:8)(cid:3)(cid:1)(cid:6)
(cid:4)(cid:2) (cid:6)(cid:4)(cid:1)(cid:2)
(cid:8)(cid:4)(cid:1)(cid:2)
(cid:4)(cid:3) (cid:8)(cid:4)(cid:1)(cid:5) F (cid:6)(cid:4)(cid:1)(cid:5)
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
E p o ch s E p och s E p och s
A
Fig.S1.Traininglosstendencycurvesonsixdatasets.
× 10(cid:2)(cid:1) D a ta set: 2 d p la n es × 10(cid:2)(cid:1) D a ta set: electricity × 10(cid:2)(cid:1) D a ta set: b b c
R
(cid:10)(cid:2)(cid:1)(cid:9) 0.0 0.0
(cid:10)(cid:3)(cid:1)(cid:8) (cid:7)(cid:2)(cid:1)(cid:6)
(cid:6)(cid:2)(cid:1)(cid:5)
(cid:10)(cid:4)(cid:1)(cid:6) D (cid:7)(cid:3)(cid:1)(cid:2)
(cid:10)(cid:5)(cid:1)(cid:4) (cid:6)(cid:3)(cid:1)(cid:2) (cid:7)(cid:3)(cid:1)(cid:6)
(cid:10)(cid:6)(cid:1)(cid:2) (cid:7)(cid:4)(cid:1)(cid:2)
(cid:6)(cid:3)(cid:1)(cid:5)
(cid:10)(cid:6)(cid:1)(cid:9) (cid:7)(cid:4)(cid:1)(cid:6)
(cid:10)(cid:7)(cid:1)(cid:8) (cid:6)(cid:4)(cid:1)(cid:2) (cid:7)(cid:5)(cid:1)(cid:2)
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
E p och s E p och s E p och s
× 10(cid:2)(cid:1) D a ta set: IM D B × 10(cid:2)(cid:1) D a ta set: S T L 1 0 × 10(cid:2)(cid:1) D a ta set: C IF A R 1 0
0.0 0.0
0.00
(cid:10)(cid:2)(cid:1)(cid:9) (cid:9)(cid:2)(cid:1)(cid:7)
(cid:9)(cid:2)(cid:1)(cid:3)(cid:6) (cid:10)(cid:3)(cid:1)(cid:8)
(cid:9)(cid:3)(cid:1)(cid:4)
(cid:10)(cid:4)(cid:1)(cid:6)
(cid:9)(cid:2)(cid:1)(cid:4)(cid:2) (cid:9)(cid:3)(cid:1)(cid:8)
(cid:10)(cid:5)(cid:1)(cid:4)
(cid:9)(cid:4)(cid:1)(cid:6)
(cid:9)(cid:2)(cid:1)(cid:5)(cid:6) (cid:10)(cid:6)(cid:1)(cid:2)
(cid:9)(cid:5)(cid:1)(cid:2)
(cid:10)(cid:6)(cid:1)(cid:9)
(cid:9)(cid:2)(cid:1)(cid:7)(cid:2)
(cid:10)(cid:7)(cid:1)(cid:8) (cid:9)(cid:5)(cid:1)(cid:7)
(cid:9)(cid:2)(cid:1)(cid:8)(cid:6) (cid:10)(cid:8)(cid:1)(cid:6) (cid:9)(cid:6)(cid:1)(cid:4)
0 10 20 30 40 50 0 10 20 30 40 50 0 10 20 30 40 50
E p och s E p och s E p och s
Fig.S2.Metalosstendencycurvesonsixdatasets.
at0.001,0.01,0.1,1.0tocomparemodelperformance. AsshowninFig.S13,themodelperformancesignificantlydeterioratesatσ=1.0. It
canbeobservedthatexcessivelyhighvaluesofσ increasetherandomdynamicsofthedatapoints,causingthemodelperformanceto
16of15 AntonioSclocchiandMatthieuWyart
ssol
g ni
niar
T
ss ol
g ni
ni ar
T
ssol
ate
M
ssol
ate
M
ssol
g ni
niar
T
ssol
g ni
niar
T
ssol
ate
M
ssol
ate
M
ssol
g ni
niar
T
ssol
g ni
niar
T
ssol
ate
M
ssol
ate
Mrandomness. Then,weusethemeta-datasetofsizeidenticaltothesizeofthevalidationset. Naturally,wewanttoexaminetheeffectof
thesizeofthemetadatasetonthedetectionrateofmislabeleddata. Weillustratetheperformanceofthedetectionratewithdifferent
metadatasizes: 10,100,and300. Fig.S14showsthatvariousmetadatasizeshaveaminimalimpactonmodelperformance. Finally,we
analyzedtheimpactofthemetahiddenpointsonmodelperformance. AsshowninFig.S15,itisaneventinwhichthemetahidden
pointsaresetto5,andthereisanotabledeclineintheperformanceoftheNDDVmethod. Infact,smallermetahiddenpointstendto
leadtounderfitting,makingitchallengingforthemodeltoachieveoptimalperformance.
T
TableS2.F1-scoreofdifferentdatavaluationmethodsonthedifferentlabelnoiserates.ThemeanandstandarddeviationoftheF1-scoreare
F
derivedfrom5independentexperiments.Thehighestandsecond-highestresultsarehighlightedinboldandunderlined,respectively.
Data Beta Data Influence KNN Data
NoiseRate LOO AME NDDV
Shapley Shapley Banzhaf AFunction Shapley -OOB
0.09± 0.12± 0.11± 0.09± 0.11± 0.17± 0.01± 0.62± 0.74±
5%
0.003 0.007 0.008 0.004 0.003 0.003 0.009 0.002 0.003
0.16± 0.19± 0.19± R0.18± 0.18± 0.30± 0.18± 0.74± 0.76±
10%
0.007 0.010 0.009 0.005 0.003 0.003 0.010 0.002 0.003
0.30± 0.25± 0.25± 0.31± 0.31± 0.45± 0.010± 0.79± 0.77±
20%
0.005 0.008 0.008 0.002 0.002 0.004 0.009 0.001 0.001
D
0.39± 0.52± 0.51± 0.42± 0.42± 0.55± 0.46± 0.80± 0.78±
30%
0.003 0.012 0.010 0.002 0.008 0.002 0.011 0.001 0.004
0.54± 0.55± 0.56± 0.48± 0.46± 0.60± 0.58± 0.73± 0.74±
40%
0.008 0.008 0.008 0.003 0.004 0.002 0.010 0.001 0.002
0.55± 0.55± 0.62± 0.48± 0.48± 0.56± 0.27± 0.63± 0.67±
45%
0.007 0.008 0.009 0.003 0.001 0.004 0.009 0.001 0.004
TableS3.F1-scoreofdifferentdatavaluationmethodsonthedifferentfeaturenoiserate.ThemeanandstandarddeviationoftheF1-scoreare
derivedfrom5independentexperiments.Thehighestandsecond-highestresultsarehighlightedinboldandunderlined,respectively.
Data Beta Data Influence KNN Data
NoiseRate LOO AME NDDV
Shapley Shapley Banzhaf Function Shapley -OOB
0.09± 0.10± 0.10± 0.07± 0.10± 0.17± 0.09± 0.15± 0.30±
5%
0.007 0.009 0.007 0.004 0.003 0.003 0.012 0.002 0.006
0.18± 0.18± 0.18± 0.15± 0.15± 0.15± 0.18± 0.21± 0.46±
10%
0.007 0.010 0.009 0.005 0.003 0.003 0.010 0.002 0.003
0.33± 0.01± 0.01± 0.28± 0.30± 0.27± 0.01± 0.32± 0.34±
20%
0.005 0.008 0.008 0.002 0.002 0.002 0.010 0.001 0.003
0.43± 0.01± 0.01± 0.33± 0.35± 0.35± 0.01± 0.37± 0.45±
30%
0.008 0.012 0.010 0.002 0.008 0.002 0.012 0.001 0.005
0.51± 0.01± 0.01± 0.01± 0.37± 0.40± 0.01± 0.43± 0.57±
40%
0.008 0.010 0.008 0.003 0.004 0.002 0.010 0.001 0.004
0.53± 0.01± 0.01± 0.50± 0.47± 0.39± 0.01± 0.46± 0.62±
45%
0.007 0.011 0.009 0.003 0.001 0.002 0.012 0.001 0.006
AntonioSclocchiandMatthieuWyart 17of15NDDV Data-O O B AM E K NNShapley InfluenceFunction LO O DataShapley BetaShapley DataBanzhaf Random
R em oving high value data R em oving low value data A dding high value data A dding low value data
0.90 1.0 1.0 1.0
0.75 0.8 0.8 0.8
0.60 0.6 0.6 0.6
0.45 0.4 0.4 0.4
0.30 0.2 0.2 0.2
0.15 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
1.0 1.0 1.0
0.90
0.75 0.8 0.8 0.8
0.60 0.6 0.6 0.6
0.45 0.4 0.4 0.4
0.30 0.2 0.2 0.2
0.15 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
1.0 1.0 T 1.0
0.90
0.75 0.8 0.8 0.8
0.60 0.6 0.6 F 0.6
0.45 0.4 0.4 0.4
0.30 0.2 0.2 0.2
A
0.15 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
0.90 1.0 R 1.0 1.0
0.75 0.8 0.8 0.8
0.60 0.6 0.6 0.6
D
0.45 0.4 0.4 0.4
0.30 0.2 0.2 0.2
0.15 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
1.0 1.0 1.0
0.90
0.75 0.8 0.8 0.8
0.60 0.6 0.6 0.6
0.45
0.4 0.4 0.4
0.30
0.2 0.2 0.2
0.15
0.00 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
0.75 0.8 1.0 0.8
0.60 0.8
0.6 0.6
0.45 0.6
0.4 0.4
0.30 0.4
0.2 0.2 0.15 0.2
0.00 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
Fig.S3.Datapointsremovalandadditionexperimentonsixdatasets.(Firstcolumn)Removinghighvaluedataexperiment.Testaccuracycurvesshowthetrendafter
removingthemostvaluabledatapoints.(Secondcolumn)Removinglowvaluedataexperiment.Testaccuracycurvesshowthetrendafterremovingtheleastvaluabledata
points.(Thirdcolumn)Addinghighvaluedataexperiment.Testaccuracycurvesshowthetrendafteraddingthemostvaluabledatapoints.(Fourthcolumn)Addinglowvalue
dataexperiment.Testaccuracycurvesshowthetrendafterremovingtheleastvaluabledatapoints.
18of15 AntonioSclocchiandMatthieuWyart
senalpd2
:tesata
D
yticirtcele
:tesata
D
cbb :tesata
D
B D MI
:tesata
D
01
L
TS
:tesata
D
01
R AFI
C :tesata
D
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseTDataset: 2dplanes
Dataset: electricity Dataset: bbc
2.5 0.5
2
2.0
0.4
1 1.5
0.3
T
1.0
0 0.2
0.5
F 0.1
1 0.0
0.0
0.5
2 A
1.0 0.1
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
t t t
Dataset: IMDB R Dataset: STL10 Dataset: CIFAR10
0.30
0.30
0.04
0.25
0.25
D
0.02
0.20
0.20
0.00 0.15
0.15
0.02
0.10
0.10
0.04
0.05
0.05
0.06
0.00
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
t t t
Fig.S4.DataStateTrajectoriesonsixdatasets.
AntonioSclocchiandMatthieuWyart 19of15
tX
tX
tX
tX
tX
tXDataset: 2dplanes Dataset: electricity
Dataset: bbc
0.00
0.00
0.02 0.0000
0.04 0.01
0.0005
0.06 T
0.08 0.02
0.0010
0.10
F
0.03
0.12 0.0015
0.14
0.04 A
0.0020
0.16
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
t t t
0.005 Dataset: IMDB R Dataset: STL10 Dataset: CIFAR10
0.0002 0.0002
0.000
0.0000 0.0000
0.005 D
0.0002 0.0002
0.010
0.0004 0.0004
0.015
0.0006 0.0006
0.020
0.0008 0.0008
0.025
0.0010
0.0010
0.030
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
t t t
Fig.S5.DatavalueTrajectoriesonsixdatasets.
20of15 AntonioSclocchiandMatthieuWyart
t
t
t
tX
t
tT
0.8 L O O 1.0 L O O F 2.1 L O O
D ataS h ap ley D ataS h ap ley D ataS h ap ley
B etaS h ap ley B etaS h ap ley 1.8 B etaS h ap ley
D ataB an zh af 0.8 D ataB an zh af D ataB an zh af
0.6 N D D V N D D V A 1.5 N D D V
0.6 1.2
0.4
0.9
0.4
R 0.6
0.2
0.2
0.3
0.0 0.00389 0.0 0.00417 0.0 0.00722
D
0 1 2 3 4 5 0 1 2 3 4 5 0 1 2 3 4 5
N u m b er of d ata p oin ts (× 103) N u m b er of d ata p oin ts (× 103) N u m b er of d ata p oin ts (× 103)
Fig.S6.ElapsedtimecomparisonbetweenLOO,DataShapley,BetaShapley,DataBanzhaf,andNDDV.Weemployasyntheticbinaryclassificationdataset,illustrated
withfeaturedimensionsof(left)d=5,(middle)d=50and(right)d=500.Asthenumberofdatapointsincreases,ourmethodexhibitssignificantlylowerelapsedtime
comparedtoothermethods.
AntonioSclocchiandMatthieuWyart 21of15
)sr
uo
h ni(
e
mi
T
)sr
uo
h ni(
e
mi
T
)sr
uo
h ni(
e
mi
TN D D V D ata-O O B A M E K N N S h ap ley In flu en ceF u n ction L O O D ataS h ap ley B etaS h ap ley D ataB an zh af O p tim al R an d om
L a b el N o ise R a te: 5 % L a b el N o ise R a te: 1 0 % L a b el N o ise R a te: 2 0 %
1.0 1.0 1.0
0.8 0.8 0.8
T
0.6 0.6 0.6
0.4 0.4 0.4
F
0.2 0.2 0.2
A
0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
F ra ctio n o f d a ta in sp ected F raction of d ata in sp ected F raction of d ata in sp ected
L a b el N o ise R a te: 3 0 % RL a b el N o ise R a te: 4 0 % L a b el N o ise R a te: 4 5 %
1.0 1.0 1.0
0.8 0.8 0.8
D
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
F ra ctio n o f d a ta in sp ected F ra ctio n o f d a ta in sp ected F ra ctio n o f d a ta in sp ected
Fig.S7.Discoveringcorrupteddatainthesixdifferentlevelsoflabelnoiserate.
22of15 AntonioSclocchiandMatthieuWyart
at
a
d det
p urr
oc
detcete
d f o
n oitc
ar
F
at
a
d
det
p
urr
oc
detcete
d
f o
n oitc
ar
F
ata
d det
p urroc
detcete
d fo
noitcar
F
at
a
d
det
p
urr
oc
detcete
d
f o
n oitc
ar
F
ata
d det
p urroc
detcete
d fo
noitcar
F
at
a
d
det
p
urr
oc
detcete
d
f o
n oitc
ar
FNDDV Data-O O B AM E K NNShapley InfluenceFunction LO O DataShapley BetaShapley DataBanzhaf Random
R em oving high value data R em oving low value data A dding high value data A dding low value data
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
T
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 F 0.6
0.4 0.4 0.4 0.4
0.2 0.2 A0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
R
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
D
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
Fig.S8.Datapointsremovalandadditionexperimentonsixdifferentlevelsoflabelnoiserate.(Firstcolumn)Removinghighvaluedataexperiment.(Secondcolumn)
Removinglowvaluedataexperiment.(Thirdcolumn)Addinghighvaluedataexperiment.(Fourthcolumn)Addinglowvaluedataexperiment.
AntonioSclocchiandMatthieuWyart 23of15
%5
:eta
R
esio
N leba
L
%01
:eta
R
esio
N leba
L
%02
:eta
R
esio
N leba
L
%03
:eta
R
esio
N leba
L
%04
:eta
R
esio
N leba
L
%54
:eta
R
esio
N leba
L
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseTN D D V D ata-O O B A M E K N N S h ap ley In flu en ceF u n ction L O O D ataS h ap ley B etaS h ap ley D ataB an zh af O p tim al R an d om
F ea tu re N o ise R a te: 5 % F ea tu re N o ise R a te: 1 0 % F ea tu re N o ise R a te: 2 0 %
1.0 1.0 1.0
0.8 0.8 0.8
T
0.6 0.6 0.6
0.4 0.4 0.4
F
0.2 0.2 0.2
A
0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
F ra ctio n o f d a ta in sp ected F raction of d ata in sp ected F raction of d ata in sp ected
F ea tu re N o ise R a te: 3 0 % RF ea tu re N o ise R a te: 4 0 % F ea tu re N o ise R a te: 4 5 %
1.0 1.0 1.0
0.8 0.8 0.8
D
0.6 0.6 0.6
0.4 0.4 0.4
0.2 0.2 0.2
0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
F ra ctio n o f d a ta in sp ected F ra ctio n o f d a ta in sp ected F ra ctio n o f d a ta in sp ected
Fig.S9.Discoveringcorrupteddatainthesixdifferentlevelsoffeaturenoiserate.
24of15 AntonioSclocchiandMatthieuWyart
at
a
d det
p urr
oc
detcete
d f o
n oitc
ar
F
at
a
d
det
p
urr
oc
detcete
d
f o
n oitc
ar
F
ata
d det
p urroc
detcete
d fo
noitcar
F
at
a
d
det
p
urr
oc
detcete
d
f o
n oitc
ar
F
ata
d det
p urroc
detcete
d fo
noitcar
F
at
a
d
det
p
urr
oc
detcete
d
f o
n oitc
ar
FNDDV Data-O O B AM E K NNShapley InfluenceFunction LO O DataShapley BetaShapley DataBanzhaf Random
R em oving high value data R em oving low value data A dding high value data A dding low value data
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
T
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 F 0.6
0.4 0.4 0.4 0.4
0.2 0.2 A0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
R
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
D
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
1.0 1.0 1.0 1.0
0.8 0.8 0.8 0.8
0.6 0.6 0.6 0.6
0.4 0.4 0.4 0.4
0.2 0.2 0.2 0.2
0.0 0.0 0.0 0.0
0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90 0.00 0.15 0.30 0.45 0.60 0.75 0.90
Fraction of data rem oved Fraction of data rem oved Fraction of data added Fraction of data added
Fig.S10.Datapointsremovalandadditionexperimentonsixdifferentlevelsoffeaturenoiserate.(Firstcolumn)Removinghighvaluedataexperiment.(Second
column)Removinglowvaluedataexperiment.(Thirdcolumn)Addinghighvaluedataexperiment.(Fourthcolumn)Addinglowvaluedataexperiment.
AntonioSclocchiandMatthieuWyart 25of15
%5
:eta
R
esio
N erutaeF
%01
:eta
R
esio
N
erutaeF
%02
:eta
R
esio
N
erutaeF
%03
:eta
R esio
N
erutaeF
%04
:eta
R
esio
N
erutaeF
%54
:eta
R
esio
N
erutaeF
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseT
ycarucca
tseTT
1.0 1.0 F 1.0
0.8 0.8 0.8
A
0.6 0.6 0.6
0.4 0.4 0.4
R
0.2 N D D V 0.2 R em oving high value data 0.2 A dding low value data
N D D V w ith ou t re-w eigh t R em oving low value data A dding high value data
O p tim al R em oving high value data w ithout re-w eight A ddving low value data w ithout re-w eight
0.0 R an d om 0.0 R em oving low value data w ithout re-w eight 0.0 A dding high value data w ithout re-w eight
0.0 0.2 0.4 0.6 0.8 1.0 D 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
F ra ctio n o f d a ta in sp ected F ra ctio n o f d a ta rem o v ed F ra ctio n o f d a ta a d d ed
Fig.S11.Impactofdatapointsre-weighting.
26of15 AntonioSclocchiandMatthieuWyart
at
a
d det
p
urr
oc
g nitcete
d f o
n oitc
ar
F
yc ar
ucc
a
tse
T
yc ar
ucc
a
tse
TT
1.0 1.0 F 1.0
0.8 0.8 0.8
A
0.6 0.6 0.6
0.4 0.4 0.4
N
N
D
D
D
D
V
V
w
w
i it th
h
a a=
=
1
3
R R
R
e e em m
m
o o ov v vi i in n nRg g
g
h l hoi iwg gh
h
v v vaa alul lu uee
e
d d daa atat ta
a
w w witi iht th
h
a a a== =11
3
A A
A
d d dd d di i in n ng g
g
l h lo oiw wgh v
v
va aal lu ulue ee d
d
da aat ta ata w
w
wi it tih hth a
a
a= ==1 31
0.2 N D D V w ith a= 5 0.2 R em oving low value data w ith a=3 0.2 A dding high value data w ith a=3
N D D V w ith a= 10 R em oving high value data w ith a=5 A dding low value data w ith a=5 R em oving low value data w ith a=5 A dding high value data w ith a=5
R an d om R em oving high value data w ith a=10 A dding low value data w ith a=10
0.0 O p tim al 0.0 R em oving low value data w ith a=10 0.0 A dding high value data w ith a=10
0.0 0.2 0.4 0.6 0.8 1.0 D0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
F ra ctio n o f d a ta in sp ected F ra ctio n o f d a ta rem o v ed F ra ctio n o f d a ta a d d ed
Fig.S12.Impactofthemean-fieldinteractions.
AntonioSclocchiandMatthieuWyart 27of15
at
a
d
det
p
urr
oc
detcete
d f
o n oitc
ar
F
yc ar
ucc
a
tse
T
yc ar
ucc
a
tse
TT
1.0 1.0 F 1.0
0.8 0.8 0.8
A
0.6 0.6 0.6
0.4 0.4 0.4
N N D D D D V V w w i it th h (cid:1) (cid:1)= = 0 0. .0 00 11 R R R e e em m m o o ov v vRi i in n ng g g h l hoi iwg gh h v v vaa alul lu uee e d d daa atat ta a w w witi iht th h (cid:1) (cid:1) (cid:1)== =00 0.0. .0 000 111 A A A d d dd d di i in n ng g g l h lo oiw wgh v v va aal lu ulue ee d d da aat ta ata w w wi it tih hth (cid:1) (cid:1) (cid:1)= ==0 00. .0 0.00 101 1 0.2 N D D V w ith (cid:1)= 0.1 0.2 R em oving low value data w ith (cid:1)=0.01 0.2 A dding high value data w ith (cid:1)=0.01
N D D V w ith (cid:1)= 1.0 R em oving high value data w ith (cid:1)=0.1 A dding low value data w ith (cid:1)=0.1 R an d om R R e em m o ov vi in ng g l ho iw gh v va alu lue e d da ata ta w wit ih th (cid:1) (cid:1)= =0 1.1 .0 A A d dd di in ng g h loi wgh v v aa lulu ee d d aa tata w w iti hth (cid:1) (cid:1) == 10 .0.1
0.0 O p tim al 0.0 R em oving low value data w ith (cid:1)=1.0 0.0 A dding high value data w ith (cid:1)=1.0
0.0 0.2 0.4 0.6 0.8 1.0 D 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
F ra ctio n o f d a ta in sp ected F ra ctio n o f d a ta rem o v ed F ra ctio n o f d a ta a d d ed
Fig.S13.Impactofthediffusionconstant.
28of15 AntonioSclocchiandMatthieuWyart
at
a
d
det
p
urr
oc
detcete
d f o n oitc
ar F
yc ar
ucc
a
tse
T
yc ar
ucc
a
tse
TT
1.0 1.0 F 1.0
0.8 0.8 0.8
A
0.6 0.6 0.6
0.4 0.4 0.4
N D D V w ith M = 10 R em ovinRg high value data w ith M =10 A dding low value data w ith M =10
0.2 N D D V w ith M = 100 0.2 R em oving low value data w ith M =10 0.2 A dding high value data w ith M =10
N D D V w ith M = 300 R em oving high value data w ith M =100 A dding low value data w ith M =100 R em oving low value data w ith M =100 A dding high value data w ith M =100
R an d om R em oving high value data w ith M =300 A dding low value data w ith M =300
0.0 O p tim al 0.0 R em oving low value data w ith M =300 0.0 A dding high value data w ith M =300
0.0 0.2 0.4 0.6 0.8 1.0 D0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
F ra ctio n o f d a ta in sp ected F ra ctio n o f d a ta rem o v ed F ra ctio n o f d a ta a d d ed
Fig.S14.Impactofthemetadatasizes.
AntonioSclocchiandMatthieuWyart 29of15
at
a
d
det
p
urr
oc
detcete
d f o
n oitc
ar
F
yc ar
ucc
a
tse
T
yc ar
ucc
a
tse
TT
1.0 1.0 F 1.0
0.8 0.8 0.8
A
0.6 0.6 0.6
0.4 0.4 0.4
N D D V w ith h = 5 R em oRving high value data w ith h=5 A dding low value data w ith h=5
0.2 N D D V w ith h = 10 0.2 R em oving low value data w ith h=5 0.2 A dding high value data w ith h=5
N D D V w ith h = 30 R em oving high value data w ith h=10 A dding low value data w ith h=10 R em oving low value data w ith h=10 A dding high value data w ith h=10
R an d om R em oving high value data w ith h=30 A dding low value data w ith h=30
0.0 O p tim al 0.0 R em oving low value data w ith h=30 0.0 A dding high value data w ith h=30
0.0 0.2 0.4 0.6 0.8 1.0 D 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
F ra ctio n o f d a ta in sp ected F ra ctio n o f d a ta rem o v ed F ra ctio n o f d a ta a d d ed
Fig.S15.Impactofthemetahiddenpoints.
30of15 AntonioSclocchiandMatthieuWyart
at
a
d
det
p
urr
oc
detcete
d f o
n oitc
ar
F
yc ar
ucc
a
tse
T
yc ar
ucc
a
tse
T