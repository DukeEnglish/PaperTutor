KAN: Kolmogorov–Arnold Networks
ZimingLiu1,4∗ YixuanWang2 SachinVaidya1 FabianRuehle3,4
JamesHalverson3,4 MarinSoljacˇic´1,4 ThomasY.Hou2 MaxTegmark1,4
1MassachusettsInstituteofTechnology
2CaliforniaInstituteofTechnology
3NortheasternUniversity
4TheNSFInstituteforArtificialIntelligenceandFundamentalInteractions
Abstract
Inspired by the Kolmogorov-Arnold representation theorem, we propose Kolmogorov-
Arnold Networks (KANs) as promising alternatives to Multi-Layer Perceptrons (MLPs).
WhileMLPshavefixedactivationfunctionsonnodes(“neurons”),KANshavelearnable
activation functions on edges (“weights”). KANs have no linear weights at all – every
weightparameterisreplacedbyaunivariatefunctionparametrizedasaspline. Weshow
that this seemingly simple change makes KANs outperform MLPs in terms of accuracy
andinterpretability. Foraccuracy,muchsmallerKANscanachievecomparableorbetter
accuracythanmuchlargerMLPsindatafittingandPDEsolving. Theoreticallyandem-
pirically,KANspossessfasterneuralscalinglawsthanMLPs. Forinterpretability,KANs
canbeintuitivelyvisualizedandcaneasilyinteractwithhumanusers. Throughtwoex-
amplesinmathematicsandphysics,KANsareshowntobeuseful“collaborators”helping
scientists(re)discovermathematicalandphysicallaws. Insummary,KANsarepromising
alternativesforMLPs,openingopportunitiesforfurtherimprovingtoday’sdeeplearning
modelswhichrelyheavilyonMLPs.
Model Multi-Layer Perceptron (MLP) Kolmogorov-Arnold Network (KAN)
Theorem Universal Approximation Theorem Kolmogorov-Arnold Representation Theorem
(F So hr am llou wla )
f(x)≈∑N(ϵ)
a iσ(wi⋅x+b i)
f(x)=2 ∑n+1
Φq
∑n
ϕ q,p(x p)
i=1 q=1 p=1
(a) fixed activation functions (b) learnable activation functions
on nodes on edges
Model
(Shallow) sum operation on nodes
learnable weights
on edges
F (o Dr em eu pl )a MLP(x)=(W3∘σ 2∘W2∘σ 1∘W1)(x) KAN(x)=(Φ3∘Φ2∘Φ1)(x)
(c) MLP(x) (d) KAN(x)
W3 Φ3
Model σ nonlinear,
(Deep) 2 fixed nonlinear,
W2 Φ2 learnable
σ
1 linear,
W1 learnable Φ1
x x
Figure0.1:Multi-LayerPerceptrons(MLPs)vs.Kolmogorov-ArnoldNetworks(KANs)
∗zmliu@mit.edu
Preprint.Underreview.
4202
rpA
03
]GL.sc[
1v65791.4042:viXra1 Introduction
Multi-layer perceptrons (MLPs) [1, 2, 3], also known as fully-connected feedforward neural net-
works,arefoundationalbuildingblocksoftoday’sdeeplearningmodels. TheimportanceofMLPs
can never be overstated, since they are the default models in machine learning for approximating
nonlinearfunctions,duetotheirexpressivepowerguaranteedbytheuniversalapproximationtheo-
rem[3].However,areMLPsthebestnonlinearregressorswecanbuild?Despitetheprevalentuseof
MLPs,theyhavesignificantdrawbacks. Intransformers[4]forexample,MLPsconsumealmostall
non-embeddingparametersandaretypicallylessinterpretable(relativetoattentionlayers)without
post-analysistools[5].
We propose a promising alternative to MLPs, called Kolmogorov-Arnold Networks (KANs).
Whereas MLPs are inspired by the universal approximation theorem, KANs are inspired by the
Kolmogorov-Arnoldrepresentationtheorem[6,7]. LikeMLPs,KANshavefully-connectedstruc-
tures. However, while MLPs place fixed activation functions on nodes (“neurons”), KANs place
learnableactivationfunctionsonedges(“weights”),asillustratedinFigure0.1. Asaresult,KANs
havenolinearweightmatricesatall: instead,eachweightparameterisreplacedbyalearnable1D
functionparametrizedasaspline. KANs’nodessimplysumincomingsignalswithoutapplyingany
non-linearities. One might worry that KANs are hopelessly expensive, since each MLP’s weight
parameterbecomesKAN’ssplinefunction. Fortunately,KANsusuallyallowmuchsmallercompu-
tation graphs than MLPs. For example, we show that for PDE solving, a 2-Layer width-10 KAN
is 100 times more accurate than a 4-Layer width-100 MLP (10−7 vs 10−5 MSE) and 100 times
moreparameterefficient(102vs104parameters).
Unsurprisingly,thepossibilityofusingKolmogorov-Arnoldrepresentationtheoremtobuildneural
networks has been studied [8, 9, 10, 11, 12, 13]. However, most work has stuck with the origi-
nal depth-2 width-(2n+1) representation, and did not have the chance to leverage more modern
techniques(e.g., backpropagation)totrainthenetworks. Ourcontributionliesingeneralizingthe
original Kolmogorov-Arnold representation to arbitrary widths and depths, revitalizing and con-
textualizing it in today’s deep learning world, as well as using extensive empirical experiments to
highlight its potential role as a foundation model for AI + Science due to its accuracy and inter-
pretability.
Despite their elegant mathematical interpretation, KANs are nothing more than combinations of
splines and MLPs, leveraging their respective strengths and avoiding their respective weaknesses.
Splinesareaccurateforlow-dimensionalfunctions,easytoadjustlocally,andabletoswitchbetween
different resolutions. However, splines have a serious curse of dimensionality (COD) problem,
becauseoftheirinabilitytoexploitcompositionalstructures. MLPs,Ontheotherhand,sufferless
from COD thanks to their feature learning, but are less accurate than splines in low dimensions,
becauseoftheirinabilitytooptimizeunivariatefunctions. Tolearnafunctionaccurately, amodel
should not only learn the compositional structure (external degrees of freedom), but should also
approximate well the univariate functions (internal degrees of freedom). KANs are such models
sincetheyhaveMLPsontheoutsideandsplinesontheinside. Asaresult,KANscannotonlylearn
features(thankstotheirexternalsimilaritytoMLPs),butcanalsooptimizetheselearnedfeaturesto
greataccuracy(thankstotheirinternalsimilaritytosplines). Forexample,givenahighdimensional
function
(cid:32) N (cid:33)
1 (cid:88)
f(x ,··· ,x )=exp sin2(x ) , (1.1)
1 N N i
i=1
2Figure2.1:OurproposedKolmogorov-Arnoldnetworksareinhonoroftwogreatlatemathematicians,Andrey
KolmogorovandVladimirArnold.KANsaremathematicallysound,accurateandinterpretable.
splineswouldfailforlargeN duetoCOD;MLPscanpotentiallylearnthethegeneralizedadditive
structure,buttheyareveryinefficientforapproximatingtheexponentialandsinefunctionswithsay,
ReLUactivations. Incontrast,KANscanlearnboththecompositionalstructureandtheunivariate
functionsquitewell,henceoutperformingMLPsbyalargemargin(seeFigure3.1).
Throughout this paper, we will use extensive numerical experiments to show that KANs can lead
toremarkableaccuracyandinterpretabilityimprovementoverMLPs. Theorganizationofthepaper
is illustrated in Figure 2.1. In Section 2, we introduce the KAN architecture and its mathematical
foundation,introducenetworksimplificationtechniquestomakeKANsinterpretable,andintroduce
agridextensiontechniquetomakeKANsincreasinglymoreaccurate. InSection3, weshowthat
KANsaremoreaccuratethanMLPsfordatafittingandPDEsolving: KANscanbeatthecurseof
dimensionalitywhenthereisacompositionalstructureindata,achievingmuchbetterscalinglaws
thanMLPs.InSection4,weshowthatKANsareinterpretableandcanbeusedforscientificdiscov-
eries. Weusetwoexamplesfrommathematics(knottheory)andphysics(Andersonlocalization)to
demonstratethatKANscanbehelpful“collaborators”forscientiststo(re)discovermathandphys-
icallaws. Section5summarizesrelatedworks. InSection6,weconcludebydiscussingbroadim-
pactsandfuturedirections. Codesareavailableathttps://github.com/KindXiaoming/pykan
andcanalsobeinstalledviapip install pykan.
2 Kolmogorov–ArnoldNetworks(KAN)
Multi-LayerPerceptrons(MLPs)areinspiredbytheuniversalapproximationtheorem. Weinstead
focus on the Kolmogorov-Arnold representation theorem, which can be realized by a new type of
neural network called Kolmogorov-Arnold networks (KAN). We review the Kolmogorov-Arnold
theorem in Section 2.1, to inspire the design of Kolmogorov-Arnold Networks in Section 2.2. In
Section 2.3, we provide theoretical guarantees for the expressive power of KANs and their neural
scaling laws. In Section 2.4, we propose a grid extension technique to make KANs increasingly
moreaccurate. InSection2.5,weproposesimplificationtechniquestomakeKANsinterpretable.
2.1 Kolmogorov-ArnoldRepresentationtheorem
VladimirArnoldandAndreyKolmogorovestablishedthatiff isamultivariatecontinuousfunction
on a bounded domain, then f can be written as a finite composition of continuous functions of a
3Figure 2.2: Left: Notations of activations that flow through the network. Right: an activation function is
parameterizedasaB-spline,whichallowsswitchingbetweencoarse-grainedandfine-grainedgrids.
singlevariableandthebinaryoperationofaddition.Morespecifically,forasmoothf :[0,1]n →R,
2n+1 (cid:32) n (cid:33)
(cid:88) (cid:88)
f(x)=f(x ,··· ,x )= Φ ϕ (x ) , (2.1)
1 n q q,p p
q=1 p=1
whereϕ : [0,1] → RandΦ : R → R. Inasense,theyshowedthattheonlytruemultivariate
q,p q
functionis addition, sinceevery otherfunction canbe writtenusingunivariate functionsand sum.
Onemightnaivelyconsiderthisgreatnewsformachinelearning:learningahigh-dimensionalfunc-
tion boils down to learning a polynomial number of 1D functions. However, these 1D functions
canbenon-smoothandevenfractal,sotheymaynotbelearnableinpractice[14]. Becauseofthis
pathological behavior, the Kolmogorov-Arnold representation theorem was basically sentenced to
deathinmachinelearning,regardedastheoreticallysoundbutpracticallyuseless[14].
However,wearemoreoptimisticabouttheusefulnessoftheKolmogorov-Arnoldtheoremforma-
chinelearning. Firstofall,weneednotsticktotheoriginalEq.(2.1)whichhasonlytwo-layernon-
linearitiesandasmallnumberofterms(2n+1)inthehiddenlayer: wewillgeneralizethenetwork
toarbitrarywidthsanddepths. Secondly,mostfunctionsinscienceanddailylifeareoftensmooth
and have sparse compositional structures, potentially facilitating smooth Kolmogorov-Arnold rep-
resentations. Thephilosophyhereisclosetothemindsetofphysicists,whooftencaremoreabout
typicalcasesratherthanworstcases. Afterall,ourphysicalworldandmachinelearningtasksmust
havestructurestomakephysicsandmachinelearningusefulorgeneralizableatall[15].
2.2 KANarchitecture
Supposewehaveasupervisedlearningtaskconsistingofinput-outputpairs{x ,y },wherewewant
i i
tofindf suchthaty ≈ f(x )foralldatapoints. Eq.(2.1)impliesthatwearedoneifwecanfind
i i
appropriate univariate functions ϕ and Φ . This inspires us to design a neural network which
q,p q
explicitlyparametrizesEq.(2.1). Sinceallfunctionstobelearnedareunivariatefunctions,wecan
parametrizeeach1DfunctionasaB-splinecurve,withlearnablecoefficientsoflocalB-splinebasis
functions (see Figure 2.2 right). Now we have a prototype of KAN, whose computation graph is
exactly specified by Eq. (2.1) and illustrated in Figure 0.1 (b) (with the input dimension n = 2),
appearingasatwo-layerneuralnetworkwithactivationfunctionsplacedonedgesinsteadofnodes
(simplesummationisperformedonnodes),andwithwidth2n+1inthemiddlelayer.
4As mentioned, such a network is known to be too simple to approximate any function arbitrarily
well in practice with smooth splines! We therefore generalize our KAN to be wider and deeper.
It is not immediately clear how to make KANs deeper, since Kolmogorov-Arnold representations
correspond to two-layer KANs. To the best of our knowledge, there is not yet a “generalized”
versionofthetheoremthatcorrespondstodeeperKANs.
ThebreakthroughoccurswhenwenoticetheanalogybetweenMLPsandKANs. InMLPs,oncewe
definealayer(whichiscomposedofalineartransformationandnonlinearties),wecanstackmore
layerstomakethenetworkdeeper. TobuilddeepKANs,weshouldfirstanswer: “whatisaKAN
layer?” ItturnsoutthataKANlayerwithn -dimensionalinputsandn -dimensionaloutputscan
in out
bedefinedasamatrixof1Dfunctions
Φ={ϕ }, p=1,2,··· ,n , q =1,2··· ,n , (2.2)
q,p in out
wherethefunctionsϕ havetrainableparameters,asdetaildbelow. IntheKolmogov-Arnoldtheo-
q,p
rem,theinnerfunctionsformaKANlayerwithn =nandn =2n+1,andtheouterfunctions
in out
formaKANlayerwithn =2n+1andn =1. SotheKolmogorov-Arnoldrepresentationsin
in out
Eq.(2.1)aresimplycompositionsoftwoKANlayers. Nowitbecomesclearwhatitmeanstohave
deeperKolmogorov-Arnoldrepresentations: simplystackmoreKANlayers!
Letusintroducesomenotation. Thisparagraphwillbeabittechnical,butreaderscanrefertoFig-
ure2.2(left)foraconcreteexampleandintuitiveunderstanding.TheshapeofaKANisrepresented
byanintegerarray
[n ,n ,··· ,n ], (2.3)
0 1 L
where n is the number of nodes in the ith layer of the computational graph. We denote the ith
i
neuroninthelthlayerby(l,i),andtheactivationvalueofthe(l,i)-neuronbyx . Betweenlayerl
l,i
andlayerl+1,therearen n activationfunctions:theactivationfunctionthatconnects(l,j)and
l l+1
(l+1,i)isdenotedby
ϕ , l=0,··· ,L−1, i=1,··· ,n , j =1,··· ,n . (2.4)
l,i,j l+1 l
The pre-activation of ϕ is simply x ; the post-activation of ϕ is denoted by x˜ ≡
l,i,j l,i l,i,j l,i,j
ϕ (x ). The activation value of the (l+1,j) neuron is simply the sum of all incoming post-
l,i,j l,i
activations:
(cid:88)nl (cid:88)nl
x = x˜ = ϕ (x ), j =1,··· ,n . (2.5)
l+1,j l,i,j l,i,j l,i l+1
i=1 i=1
Inmatrixform,thisreads
 
ϕ (·) ϕ (·) ··· ϕ (·)
l,1,1 l,1,2 l,1,nl
 ϕ (·) ϕ (·) ··· ϕ (·) 
x l+1
=
 
l,2,
. .
.1 l,2,
. .
.2 l,2,
. .
.nl 
 x l, (2.6)
 
ϕ (·) ϕ (·) ··· ϕ (·)
l,nl+1,1 l,nl+1,2 l,nl+1,nl
(cid:124) (cid:123)(cid:122) (cid:125)
Φl
whereΦ isthefunctionmatrixcorrespondingtothelth KANlayer. AgeneralKANnetworkisa
l
compositionofLlayers: givenaninputvectorx
0
∈Rn0,theoutputofKANis
KAN(x)=(Φ ◦Φ ◦···◦Φ ◦Φ )x. (2.7)
L−1 L−2 1 0
5We can also rewrite the above equation to make it more analogous to Eq. (2.1), assuming output
dimensionn =1,anddefinef(x)≡KAN(x):
L
n (cid:88)L−1  n (cid:88)L−2 (cid:32) (cid:88)n2 (cid:32) (cid:88)n1 (cid:32) (cid:88)n0 (cid:33)(cid:33)(cid:33) 
f(x)= ϕ L−1,iL,iL−1 ··· ϕ 2,i3,i2 ϕ 1,i2,i1 ϕ 0,i1,i0(x i0) ···,
iL−1=1 iL−2=1 i2=1 i1=1 i0=1
(2.8)
whichisquitecumbersome. Incontrast,ourabstractionofKANlayersandtheirvisualizationsare
cleaner and intuitive. The original Kolmogorov-Arnold representation Eq. (2.1) corresponds to a
2-LayerKANwithshape[n,2n+1,1]. Noticethatalltheoperationsaredifferentiable,sowecan
trainKANswithbackpropagation. Forcomparison,anMLPcanbewrittenasinterleavingofaffine
transformationsWandnon-linearitiesσ:
MLP(x)=(W ◦σ◦W ◦σ◦···◦W ◦σ◦W )x. (2.9)
L−1 L−2 1 0
It is clear that MLPs treat linear transformations and nonlinearities separately as W and σ, while
KANstreatthemalltogetherinΦ. InFigure0.1(c)and(d),wevisualizeathree-layerMLPanda
three-layerKAN,toclarifytheirdifferences.
Implementationdetails. AlthoughaKANlayerEq.(2.5)looksextremelysimple,itisnon-trivial
tomakeitwelloptimizable. Thekeytricksare:
(1) Residual activation functions. We include a basis function b(x) (similar to residual connec-
tions)suchthattheactivationfunctionϕ(x)isthesumofthebasisfunctionb(x)andthespline
function:
ϕ(x)=w(b(x)+spline(x)). (2.10)
Weset
b(x)=silu(x)=x/(1+e−x) (2.11)
inmostcases. spline(x)isparametrizedasalinearcombinationofB-splinessuchthat
(cid:88)
spline(x)= c B (x) (2.12)
i i
i
where c s are trainable. In principle w is redundant since it can be absorbed into b(x) and
i
spline(x). However,westillincludethiswfactortobettercontroltheoverallmagnitudeofthe
activationfunction.
(2) Initialization scales. Each activation function is initialized to have spline(x) ≈ 0 2. w is
initializedaccordingtotheXavierinitialization,whichhasbeenusedtoinitializelinearlayers
inMLPs.
(3) Update of spline grids. We update each grid on the fly according to its input activations, to
address the issue that splines are defined on bounded regions but activation values can evolve
outofthefixedregionduringtraining3.
Parametercount. Forsimplicity,letusassumeanetwork
(1) ofdepthL,
2ThisisdonebydrawingB-splinecoefficientsc ∼N(0,σ2)withasmallσ,typicallywesetσ=0.1.
i
3Otherpossibilitiesare:(a)thegridislearnablewithgradientdescent,e.g.,[16];(b)usenormalizationsuch
thattheinputrangeisfixed.Wetried(b)atfirstbutitsperformanceisinferiortoourcurrentapproach.
6Paper Idea Scalingexponentα
Sharma&Kaplan[17] Intrinsicdimensionality (k+1)/d
Michaudetal.[18] maximumarity (k+1)/2
Poggioetal.[14] compositionalsparsity m/2
Ours K-Arepresentation k+1
Table 1: Scaling exponents from different theories ℓ ∝ N−α. ℓ: test RMSE loss, N: number of model
parameters,d: inputintrinsicdimension,k: orderofpiecewisepolynomial,m: derivativeorderasinfunction
classW .
m
(2) withlayersofequalwidthn =n =···=n =N,
0 1 L
(3) witheachsplineoforderk(usuallyk =3)onGintervals(forG+1gridpoints).
ThenthereareintotalO(N2L(G+k)) ∼ O(N2LG)parameters. Incontrast,anMLPwithdepth
L and width N only needs O(N2L) parameters, which appears to be more efficient than KAN.
Fortunately, KANsusuallyrequiremuchsmallerN thanMLPs, whichnotonlysavesparameters,
butalsoachievesbettergeneralization(seee.g., Figure3.1and3.3)andfacilitatesinterpretability.
WecharacterizethegeneralizationbehaviorofKANswithatheorembelow.
2.3 KAN’sApproximationAbilitiesandScalingLaws
Recall that in Eq. (2.1), the 2-Layer width-(2n + 1) representation may be non-smooth. How-
ever, deeper representations may bring the advantages of smoother activations. For example, the
4-variablefunction
f(x ,x ,x ,x
)=exp(cid:0) sin(x2+x2)+sin(x2+x2)(cid:1)
(2.13)
1 2 3 4 1 2 3 4
can be smoothly represented by a [4,2,1,1] KAN which is 3-Layer, but may not admit a 2-Layer
KANwithsmoothactivations. Tofacilitateanapproximationanalysis,westillassumesmoothness
of activations, but allow the representations to be arbitrarily wide and deep, as in Eq. (2.7). To
emphasizethedependenceofourKANonthefinitesetofgridpoints,weuseΦG andΦG below
l l,i,j
toreplacethenotationΦ andΦ usedinEq.(2.5)and(2.6).
l l,i,j
Theorem 2.1 (Approximation theory, KAT). Let x = (x ,x ,··· ,x ). Suppose that a function
1 2 n
f(x)admitsarepresentation
f =(Φ ◦Φ ◦···◦Φ ◦Φ )x, (2.14)
L−1 L−2 1 0
as in Eq. (2.7), where each one of the Φ are (k +1)-times continuously differentiable. Then
l,i,j
there exists a constant C depending on f and its representation, such that we have the following
approximation bound in terms of the grid size G: there exist k-th order B-spline functions ΦG
l,i,j
suchthatforany0≤m≤k,wehavethebound
∥f −(ΦG ◦ΦG ◦···◦ΦG◦ΦG)x∥ ≤CG−k−1+m. (2.15)
L−1 L−2 1 0 Cm
HereweadoptthenotationofCm-normmeasuringthemagnitudeofderivativesuptoorderm:
∥g∥
Cm
= max sup
(cid:12) (cid:12)Dβg(x)(cid:12)
(cid:12).
|β|≤mx∈[0,1]n
Proof. Bytheclassical1DB-splinetheory[19]andthefactthatΦ ascontinuousfunctionscan
l,i,j
beuniformlyboundedonaboundeddomain,weknowthatthereexistfinite-gridB-splinefunctions
7ΦG suchthatforany0≤m≤k,
l,i,j
∥(Φ ◦Φ ◦Φ ◦···◦Φ ◦Φ )x−(ΦG ◦Φ ◦Φ ◦···◦Φ ◦Φ )x∥ ≤CG−k−1+m,
l,i,j l−1 l−2 1 0 l,i,j l−1 l−2 1 0 Cm
withaconstantC independentofG. WefixthoseB-splineapproximations. Thereforewehavethat
theresidueR definedvia
l
R :=(ΦG ◦···◦ΦG ◦Φ ◦Φ ◦···◦Φ )x−(ΦG ◦···◦ΦG ◦ΦG◦Φ ◦···◦Φ )x
l L−1 l+1 l l−1 0 L−1 l+1 l l−1 0
satisfies
∥R ∥ ≤CG−k−1+m,
l Cm
withaconstantindependentofG. Finallynoticethat
f −(ΦG ◦ΦG ◦···◦ΦG◦ΦG)x=R +R +···+R +R ,
L−1 L−2 1 0 L−1 L−2 1 0
weknowthat(2.15)holds.
We know that asymptotically, provided that the assumption in Theorem 2.1 holds, KANs with fi-
nitegridsizecanapproximatethefunctionwellwitharesiduerateindependentofthedimension,
hencebeatingcurseofdimensionality! Thiscomesnaturallysinceweonlyusesplinestoapprox-
imate1Dfunctions. Inparticular,form = 0,werecovertheaccuracyinL∞ norm,whichinturn
providesaboundofRMSEonthefinitedomain,whichgivesascalingexponentk+1. Ofcourse,
theconstantC isdependentontherepresentation;henceitwilldependonthedimension. Wewill
leavethediscussionofthedependenceoftheconstantonthedimensionasafuturework.
We remark that although the Kolmogorov-Arnold theorem Eq. (2.1) corresponds to a KAN repre-
sentationwithshape[d,2d+1,1],itsfunctionsarenotnecessarilysmooth. Ontheotherhand,ifwe
areabletoidentifyasmoothrepresentation(maybeatthecostofextralayersormakingtheKAN
widerthanthetheoryprescribes),thenTheorem2.1indicatesthatwecanbeatthecurseofdimen-
sionality(COD).Thisshouldnotcomeasasurprisesincewecaninherentlylearnthestructureof
thefunctionandmakeourfinite-sampleKANapproximationinterpretable.
Neural scaling laws: comparison to other theories. Neural scaling laws are the phenomenon
wheretestlossdecreaseswithmoremodelparameters,i.e.,ℓ ∝ N−α whereℓistestRMSE,N is
thenumberofparameters, andα isthescalingexponent. Alargerα promisesmoreimprovement
by simply scaling up the model. Different theories have been proposed to predict α. Sharma &
Kaplan[17]suggestthatαcomesfromdatafittingonaninputmanifoldofintrinsicdimensionality
d. If the model function class is piecewise polynomials of order k (k = 1 for ReLU), then the
standardapproximationtheoryimpliesα = (k+1)/dfromtheapproximationtheory. Thisbound
suffers from the curse of dimensionality, so people have sought other bounds independent of d by
leveraging compositional structures. In particular, Michaud et al. [18] considered computational
graphs that only involve unary (e.g., squared, sine, exp) and binary (+ and ×) operations, finding
α=(k+1)/d∗ =(k+1)/2,whered∗ =2isthemaximumarity. Poggioetal.[14]leveragedthe
ideaofcompositionalsparsityandprovedthatgivenfunctionclassW (functionwhosederivatives
m
arecontinuousuptom-thorder),oneneedsN =O(ϵ− m2 )numberofparameterstoachieveerrorϵ,
whichisequivalenttoα= m. Ourapproach,whichassumestheexistenceofsmoothKolmogorov-
2
Arnoldrepresentations,decomposesthehigh-dimensionalfunctionintoseveral1Dfunctions,giving
α=k+1(wherekisthepiecewisepolynomialorderofthesplines).Wechoosek =3cubicsplines
soα = 4whichisthelargestandbestscalingexponentcomparedtootherworks. Wewillshowin
Section3.1thatthisboundα = 4caninfactbeachievedempiricallywithKANs, whileprevious
8Fitting f(x,y)=exp(sin( x)+y2)
KAN [2,5,1] KAN [2,1,1]
101 101
train train
101 test 101 test
103 103
105 105
107 grid=3 grid=5 grid=10 grid=20 grid=50 grid=100 grid=200 grid=500 grid=1000 107 grid=3 grid=5 grid=10 grid=20 grid=50 grid=100 grid=200 grid=500 grid=1000
109 109
0 200 400 600 800 1000 1200 1400 1600 1800 0 200 400 600 800 1000 1200 1400 1600 1800
step step
102 KAN [2,5,1]
KAN [2,1,1]
103
G 3
104 G 2 100
105
106 K KA AN N [ [2 2, ,5 1, ,1 1] ] s sq qr rt t( (m me ea an n o of f s sq qu ua ar re ed d) ) G 4
107 KAN [2,1,1] sqrt(median of squared) 101
101 102 101 102 103
grid size G grid size G
Figure2.3: WecanmakeKANsmoreaccuratebygridextension(fine-grainingsplinegrids). Topleft(right):
trainingdynamicsofa[2,5,1]([2,1,1])KAN.Bothmodelsdisplaystaircasesintheirlosscurves, i.e., loss
suddentlydropsthenplateausaftergridextension. Bottomleft: testRMSEfollowsscalinglawsagainstgrid
sizeG.Bottomright:trainingtimescalesfavorablywithgridsizeG.
work [18] reported that MLPs have problems even saturating slower bounds (e.g., α = 1) and
plateauquickly. Ofcourse,wecanincreasektomatchthesmoothnessoffunctions,buttoohighk
mightbetoooscillatory,leadingtooptimizationissues.
ComparisonbetweenKATandUAT.Thepoweroffully-connectedneuralnetworksisjustifiedby
theuniversalapproximationtheorem(UAT),whichstatesthatgivenafunctionanderrortolerance
ϵ > 0, a two-layer network with k > N(ϵ) neurons can approximate the function within error ϵ.
However,theUATguaranteesnoboundforhowN(ϵ)scaleswithϵ.Indeed,itsuffersfromtheCOD,
and N has been shown to grow exponentially with d in some cases [15]. The difference between
KAT and UAT is a consequence that KANs take advantage of the intrinsically low-dimensional
representation of the function while MLPs do not. Indeed, we will show that KANs are nicely
alignedwithsymbolicfunctionswhileMLPsarenot.
2.4 Foraccuracy: GridExtension
In principle, a spline can be made arbitrarily accurate to a target function as the grid can be made
arbitrarily fine-grained. This good feature is inherited by KANs. By contrast, MLPs do not have
the notion of “fine-graining”. Admittedly, increasing the width and depth of MLPs can lead to
improvementinperformance(“neuralscalinglaws”). However,theseneuralscalinglawsareslow
(discussedinthelastsection). Theyarealsoexpensivetoobtain, becausemodelsofvaryingsizes
aretrainedindependently. Bycontrast,forKANs,onecanfirsttrainaKANwithfewerparameters
andthenextendittoaKANwithmoreparametersbysimplymakingitssplinegridsfiner,without
theneedtoretrainingthelargermodelfromscratch.
We next describe how to perform grid extension (illustrated in Figure 2.2 right), which is basi-
cally fitting a new fine-grained spline to an old coarse-grained spline. Suppose we want to ap-
proximate a 1D function f in a bounded region [a,b] with B-splines of order k. A coarse-grained
grid with G intervals has grid points at {t = a,t ,t ,··· ,t = b}, which is augmented to
1 0 1 2 G1
9
ESMR
ssol
tset
dlohserht
noitalopretni ESMR
)pets/sdnoces(
emit
gniniart
dlohserht
noitalopretni{t ,··· ,t ,t ,··· ,t ,t ,··· ,t }. There are G +k B-spline basis functions, with
−k −1 0 G1 G1+1 G1+k 1
the ith B-spline B (x) being non-zero only on [t ,t ] (i = 0,··· ,G + k − 1). Then f
i −k+i i+1 1
on the coarse grid is expressed in terms of linear combination of these B-splines basis functions
f (x) =
(cid:80)G1+k−1c
B (x). Given a finer grid with G intervals, f on the fine grid is cor-
coarse i=0 i i 2
respondingly f (x) = (cid:80)G2+k−1c′B′(x). The parameters c′s can be initialized from the pa-
fine j=0 j j j
rameters c by minimizing the distance between f (x) to f (x) (over some distribution of
i fine coarse
x):
 2
G2(cid:88)+k−1 G1(cid:88)+k−1
{c′ j}=argmin E  c′ jB j′(x)− c iB i(x) , (2.16)
{c′ j} x∼p(x) j=0 i=0
whichcanbeimplementedbytheleastsquaresalgorithm. Weperformgridextensionforallsplines
inaKANindependently.
Toyexample: staricase-likelosscurves. Weuseatoyexamplef(x,y) = exp(sin(πx)+y2)to
demonstratetheeffectofgridextension.InFigure2.3(topleft),weshowthetrainandtestRMSEfor
a[2,5,1]KAN.Thenumberofgridpointsstartsas3,increasestoahighervalueevery200LBFGS
steps,endingupwith1000gridpoints. Itisclearthateverytimefinegraininghappens,thetraining
lossdropsfasterthanbefore(exceptforthefinestgridwith1000points,whereoptimizationceases
to work probably due to bad loss landscapes). However, the test losses first go down then go up,
displayingaU-shape,duetothebias-variancetradeoff(underfittingvs. overfitting). Weconjecture
thattheoptimaltestlossisachievedattheinterpolationthresholdwhenthenumberofparameters
matchthenumberofdatapoints. Sinceourtrainingsamplesare1000andthetotalparametersofa
[2,5,1]KANis15G(Gisthenumberofgridintervals),weexpecttheinterpolationthresholdtobe
G=1000/15≈67,whichroughlyagreeswithourexperimentallyobservedvalueG∼50.
SmallKANsgeneralizebetter. Isthisthebesttestperformancewecanachieve? Noticethatthe
synthetictaskcanberepresentedexactlybya[2,1,1]KAN,sowetraina[2,1,1]KANandpresent
the training dynamics in Figure 2.3 top right. Interestingly, it can achieve even lower test losses
than the [2,5,1] KAN, with clearer staircase structures and the interpolation threshold is delayed
to a larger grid size as a result of fewer parameters. This highlights a subtlety of choosing KAN
architectures. If we do not know the problem structure, how can we determine the minimal KAN
shape? InSection2.5,wewillproposeamethodtoauto-discoversuchminimalKANarchitecture
viaregularizationandpruning.
Scalinglaws: comparisonwiththeory. Wearealsointerestedinhowthetestlossdecreasesasthe
numberofgridparametersincreases. InFigure2.3(bottomleft), a[2,1,1]KANscalesroughlyas
testRMSE∝G−3.However,accordingtotheTheorem2.1,wewouldexpecttestRMSE∝G−4.
We found that the errors across samples are not uniform. This is probably attributed to boundary
effects[18]. Infact,thereareafewsamplesthathavesignificantlylargererrorsthanothers,making
theoverallscalingslowdown. Ifweplotthesquarerootofthemedian(notmean)ofthesquared
losses, we get a scaling closer to G−4. Despite this suboptimality (probably due to optimization),
KANsstillhavemuchbetterscalinglawsthanMLPs,fordatafitting(Figure3.1)andPDEsolving
(Figure3.3). Inaddition,thetrainingtimescalesfavorablywiththenumberofgridpointsG,shown
inFigure2.3bottomright4.
External vs Internal degrees of freedom. A new concept that KANs highlights is a distinction
betweenexternalversusinternaldegreesoffreedom(parameters). Thecomputationalgraphofhow
4WhenG = 1000,trainingbecomessignificantlyslower,whichisspecifictotheuseoftheLBFGSopti-
mizerwithlinesearch. WeconjecturethatthelosslandscapebecomesbadforG=1000,solinesearchwith
tryingtofindanoptimalstepsizewithinmaximaliterationswithoutearlystopping.
10nodes are connected represents external degrees of freedom (“dofs”), while the grid points inside
an activation function are internal degrees of freedom. KANs benefit from the fact that they have
both external dofs and internal dofs. External dofs (that MLPs also have but splines do not) are
responsible for learning compositional structures of multiple variables. Internal dofs (that splines
alsohavebutMLPsdonot)areresponsibleforlearningunivariatefunctions.
2.5 ForInterpretability: SimplifyingKANsandMakingtheminteractive
OnelooseendfromthelastsubsectionisthatwedonotknowhowtochoosetheKANshapethat
bestmatchesthestructureofadataset. Forexample,ifweknowthatthedatasetisgeneratedviathe
symbolicformulaf(x,y)=exp(sin(πx)+y2),thenweknowthata[2,1,1]KANisabletoexpress
thisfunction. However,inpracticewedonotknowtheinformationapriori,soitwouldbeniceto
haveapproachestodeterminethisshapeautomatically.TheideaistostartfromalargeenoughKAN
andtrainitwithsparsityregularizationfollowedbypruning. WewillshowthattheseprunedKANs
are much more interpretable than non-pruned ones. To make KANs maximally interpretable, we
proposeafewsimplificationtechniquesinSection2.5.1,andanexampleofhowuserscaninteract
withKANstomakethemmoreinterpretableinSection2.5.2.
2.5.1 Simplificationtechniques
1. Sparsification. For MLPs, L1regularization of linear weights is usedto favor sparsity. KANs
canadaptthishigh-levelidea,butneedtwomodifications:
(1) Thereisnolinear“weight”inKANs. Linearweightsarereplacedbylearnableactivationfunc-
tions,soweshoulddefinetheL1normoftheseactivationfunctions.
(2) WefindL1tobeinsufficientforsparsificationofKANs;insteadanadditionalentropyregular-
izationisnecessary(seeAppendixCformoredetails).
WedefinetheL1normofanactivationfunctionϕtobeitsaveragemagnitudeoveritsN inputs,
p
i.e.,
1
(cid:88)Np
(cid:12) (cid:12)
|ϕ| ≡ (cid:12)ϕ(x(s))(cid:12). (2.17)
1 N (cid:12) (cid:12)
p
s=1
Then for a KAN layer Φ with n inputs and n outputs, we define the L1 norm of Φ to be the
in out
sumofL1normsofallactivationfunctions,i.e.,
(cid:88)nin n (cid:88)out
|Φ| ≡ |ϕ | . (2.18)
1 i,j 1
i=1 j=1
Inaddition,wedefinetheentropyofΦtobe
S(Φ)≡−(cid:88)nin n (cid:88)out |ϕ i,j| 1log(cid:18)|ϕ i,j| 1(cid:19)
. (2.19)
|Φ| |Φ|
i=1 j=1 1 1
Thetotaltrainingobjectiveℓ isthepredictionlossℓ plusL1andentropyregularizationof
total pred
allKANlayers:
(cid:32) L−1 L−1 (cid:33)
(cid:88) (cid:88)
ℓ =ℓ +λ µ |Φ | +µ S(Φ ) , (2.20)
total pred 1 l 1 2 l
l=0 l=0
whereµ ,µ arerelativemagnitudesusuallysettoµ =µ =1,andλcontrolsoverallregulariza-
1 2 1 2
tionmagnitude.
11Figure2.4:AnexampleofhowtodosymbolicregressionwithKAN.
2. Visualization. WhenwevisualizeaKAN,togetasenseofmagnitudes,wesetthetransparency
ofanactivationfunctionϕ proportionaltotanh(βA )whereβ = 3. Hence,functionswith
l,i,j l,i,j
smallmagnitudeappearfadedouttoallowustofocusonimportantones.
3. Pruning. Aftertrainingwithsparsificationpenalty,wemayalsowanttoprunethenetworktoa
smallersubnetwork. WesparsifyKANsonthenodelevel(ratherthanontheedgelevel). Foreach
node(saytheithneuroninthelthlayer),wedefineitsincomingandoutgoingscoreas
I =max(|ϕ | ), O =max(|ϕ | ), (2.21)
l,i k l−1,k,i 1 l,i j l+1,j,i 1
andconsideranodetobeimportantifbothincomingandoutgoingscoresaregreaterthanathreshold
hyperparameterθ =10−2bydefault. Allunimportantneuronsarepruned.
4. Symbolification. In cases where we suspect that some activation functions are in fact sym-
bolic (e.g., cos or log), we provide an interface to set them to be a specified symbolic form,
fix_symbolic(l,i,j,f) can set the (l,i,j) activation to be f. However, we cannot simply set
theactivationfunctiontobetheexactsymbolicformula,sinceitsinputsandoutputsmayhaveshifts
andscalings. So, weobtainpreactivationsxandpostactivationsy fromsamples, andfitaffinepa-
rameters(a,b,c,d)suchthaty ≈cf(ax+b)+d. Thefittingisdonebyiterativegridsearchofa,b
andlinearregression.
Besides these techniques, we provide additional tools that allow users to apply more fine-grained
controltoKANs,listedinAppendixA.
2.5.2 Atoyexample: howhumanscaninteractwithKANs
Above we have proposed a number of simplification techniques for KANs. We can view these
simplificationchoicesasbuttonsonecanclickon. Auserinteractingwiththesebuttonscandecide
whichbuttonismostpromisingtoclicknexttomakeKANsmoreinterpretable. Weuseanexample
belowtoshowcasehowausercouldinteractwithaKANtoobtainmaximallyinterpretableresults.
Letusagainconsidertheregressiontask
f(x,y)=exp(cid:0) sin(πx)+y2(cid:1)
. (2.22)
12Given data points (x ,y ,f ), i = 1,2,··· ,N , a hypothetical user Alice is interested in figuring
i i i p
out the symbolic formula. The steps of Alice’s interaction with the KANs are described below
(illustratedinFigure2.4):
Step1: Trainingwithsparsification. Startingfromafully-connected[2,5,1]KAN,trainingwith
sparsificationregularizationcanmakeitquitesparse. 4outof5neuronsinthehiddenlayerappear
useless,hencewewanttoprunethemaway.
Step 2: Pruning. Automatic pruning is seen to discard all hidden neurons except the last one,
leavinga[2,1,1]KAN.Theactivationfunctionsappeartobeknownsymbolicfunctions.
Step 3: Setting symbolic functions. Assuming that the user can correctly guess these symbolic
formulasfromstaringattheKANplot,theycanset
fix_symbolic(0,0,0,‘sin’)
fix_symbolic(0,1,0,‘xˆ2’) (2.23)
fix_symbolic(1,0,0,‘exp’).
In case the user has no domain knowledge or no idea which symbolic functions these activation
functionsmightbe,weprovideafunctionsuggest_symbolictosuggestsymboliccandidates.
Step4: Furthertraining. Aftersymbolifyingalltheactivationfunctionsinthenetwork,theonly
remainingparametersaretheaffineparameters. Wecontinuetrainingtheseaffineparameters, and
whenweseethelossdroppingtomachineprecision,weknowthatwehavefoundthecorrectsym-
bolicexpression.
Step 5: Output the symbolic formula. Sympy is used to compute the symbolic formula of the
outputnode. Theuserobtains1.0e1.0y2+1.0sin(3.14x), whichisthetrueanswer(weonlydisplayed
twodecimalsforπ).
Remark: Whynotsymbolicregression(SR)?Itisreasonabletousesymbolicregressionforthis
example. However, symbolic regression methods are in general brittle and hard to debug. They
either return a success or a failure in the end without outputting interpretable intermediate results.
In contrast, KANs do continuous search (with gradient descent) in function space, so their results
are more continuous and hence more robust. Moreover, users have more control over KANs as
comparedtoSRduetoKANs’transparency. ThewaywevisualizeKANsislikedisplayingKANs’
“brain” to users, and users can perform “surgery” (debugging) on KANs. This level of control is
typicallyunavailableforSR.WewillshowexamplesofthisinSection4.4.Moregenerally,whenthe
targetfunctionisnotsymbolic,symbolicregressionwillfailbutKANscanstillprovidesomething
meaningful. For example, a special function (e.g., a Bessel function) is impossible to SR to learn
unless it is provided in advance, but KANs can use splines to approximate it numerically anyway
(seeFigure4.1(d)).
3 KANsareaccurate
Inthissection,wedemonstratethatKANsaremoreeffectiveatrepresentingfunctionsthanMLPs
invarioustasks(regressionandPDEsolving). Whencomparingtwofamiliesofmodels,itisfairto
compareboththeiraccuracy(loss)andtheircomplexity(numberofparameters). Wewillshowthat
KANsdisplaymorefavorableParetoFrontiersthanMLPs. Moreover,inSection3.5,weshowthat
KANscannaturallyworkincontinuallearningwithoutcatastrophicforgetting.
13f(x)=J0(20x) f(x,y)=exp(sin( x)+y2)
102
f(x,y)=xy f(x1, ,x100)=exp(1100( i1 =00 1sin2(2xi))) f(x1,x2,x3,x4)=exp(sin(x12+x22)+sin(x32+x42))
101 101 103 101 101
102 102 104 102 N0.04 102 1111 0000 6543 N4 N4 K M
M M
MA L
L L
LN P
P P P
(
( (
(( d
d d
dd e
e e
ee p
p p
pp t
t t
tt h
h h
hh 2
3 4
52 )
) )
)) 1111 0000 6543 N4 N2 K M
M M
MA L
L L
LN P
P P P
(
( (
(( d
d d
dd e
e e
ee p
p p
pp t
t t
tt h
h h
hh 2
3 4
52 )
) )
)) 111 000 765 N4 N2 K M
M M
MA L
L L
LN P
P P P
(
( (
(( d
d d
dd e
e e
ee p
p p
pp t
t t
tt h
h h
hh 2
3 4
52 )
) )
)) 11 00 43
N4
K M
M M
MA L
L L
LN P
P P P
(
( (
(( d
d d
dd e
e e
ee p
p p
pp t
t t
tt h
h h
hh 2
3 4
52 )
) )
)) 1111 0000 6543
N4
N1 K K M
M M
MA A L
L L
LN N P
P P P
(
( (
(( ( d
d d
dd d e
e e
ee e p
p p
pp p t
t t
tt t h
h h
hh h 2
3 4
53 2 )
) )
)) )
107 T Th he eo or ry y ( (K IDA )N) 107 T Th he eo or ry y ( (K IDA )N) 108 T Th he eo or ry y ( (K IDA )N) 105 T Th he eo or ry y ( (K IDA )N) 107 T Th he eo or ry y ( (K IDA )N)
101 102 103 104 105 101 102 103 104 105 101 102 103 104 105 103 104 105 102 103 104
Number of parameters Number of parameters Number of parameters Number of parameters Number of parameters
Figure3.1: CompareKANstoMLPsonfivetoyexamples. KANscanalmostsaturatethefastestscalinglaw
predictedbyourtheory(α=4),whileMLPsscalesslowlyandplateauquickly.
3.1 Toydatasets
InSection2.3,ourtheorysuggestedthattestRMSElossℓscalesasℓ∝N−4withmodelparameters
N.However,thisreliesontheexistenceofaKolmogorov-Arnoldrepresentation.Asasanitycheck,
weconstructfiveexamplesweknowhavesmoothKArepresentations:
(1) f(x) = J (20x), which is the Bessel function. Since it is a univariate function, it can be
0
representedbyaspline,whichisa[1,1]KAN.
(2) f(x,y)=exp(sin(πx)+y2). Weknowthatitcanbeexactlyrepresentedbya[2,1,1]KAN.
(3) f(x,y)=xy. WeknowfromFigure4.1thatitcanbeexactlyrepresentedbya[2,2,1]KAN.
(4) Ahigh-dimensionalexamplef(x 1,··· ,x 100) = exp( 101 0(cid:80)1 i=00 1sin2(π 2xi))whichcanberep-
resentedbya[100,1,1]KAN.
(5) Afour-dimensionalexamplef(x ,x ,x ,x )=exp(1(sin(π(x2+x2))+sin(π(x2+x2))))
1 2 3 4 2 1 2 3 4
whichcanberepresentedbya[4,4,2,1]KAN.
We train these KANs by increasing grid points every 200 steps, in total covering G =
{3,5,10,20,50,100,200,500,1000}. We train MLPs with different depths and widths as base-
lines. BothMLPsandKANsaretrainedwithLBFGSfor1800stepsintotal. WeplottestRMSEas
afunctionofthenumberofparametersforKANsandMLPsinFigure3.1,showingthatKANshave
betterscalingcurvesthanMLPs,especiallyforthehigh-dimensionalexample. Forcomparison,we
plotthelinespredictedfromourKANtheoryasreddashed(α=k+1=4),andthelinespredicted
fromSharma&Kaplan[17]asblack-dashed(α=(k+1)/d=4/d).KANscanalmostsaturatethe
steeperredlines,whileMLPsstruggletoconvergeevenasfastastheslowerblacklinesandplateau
quickly. Wealsonotethatforthelastexample,the2-LayerKAN[4,9,1]behavesmuchworsethan
the3-LayerKAN(shape[4,2,2,1]). ThishighlightsthegreaterexpressivepowerofdeeperKANs,
whichisthesameforMLPs: deeperMLPshavemoreexpressivepowerthanshallowerones.
3.2 Specialfunctions
Onecaveatfortheaboveresultsisthatweassumeknowledgeofthe“true”KANshape. Inpractice,
wedonotknowtheexistenceofKArepresentations. EvenwhenwearepromisedthatsuchaKA
representation exists, we do not know the KAN shape a priori. Special functions in more than
onevariablesaresuchcases,becauseitwouldbe(mathematically)surprisingifmultivariatespecial
functions(e.g.,aBesselfunctionf(ν,x)=J (x))couldbewritteninKArepresenations,involving
ν
onlyunivariatefunctionsandsums). Weshowbelowthat:
(1) Finding (approximate) compact KA representations of special functions is possible, revealing
novelmathematicalpropertiesofspecialfunctionsfromtheperspectiveofKolmogorov-Arnold
representations.
14
ESMR tset101 ellipj ellipkinc ellipeinc jv yv
103
KAN train
105 KAN test
MLP train
107 MLP test
101 kv iv lpmv_m_0 lpmv_m_1 lpmv_m_2
103
105
107
101 sph_harm_m_0_n_1 sph_harm_m_1_n_1 sph_harm_m_0_n_2 sph_harm_m_1_n_2 sph_harm_m_2_n_2
103
105
107
101 102 103 104101 102 103 104101 102 103 104101 102 103 104101 102 103 104
number of parameters number of parameters number of parameters number of parameters number of parameters
Figure3.2:Fittingspecialfunctions.WeshowtheParetoFrontierofKANsandMLPsintheplanespannedby
thenumberofmodelparametersandRMSEloss.Consistentlyaccrossallspecialfunctions,KANshavebetter
ParetoFrontiersthanMLPs.ThedefinitionsofthesespecialfunctionsareinTable2.
(2) KANsaremoreefficientandaccurateinrepresentingspecialfunctionsthanMLPs.
Wecollect15specialfunctionscommoninmathandphysics, summarizedinTable2. Wechoose
MLPs with fixed width 5 or 100 and depths swept in {2,3,4,5,6}. We run KANs both with and
without pruning. KANs without pruning: We fix the shape of KAN, whose width are set to 5 and
depthsaresweptin{2,3,4,5,6}. KANwithpruning. Weusethesparsification(λ = 10−2 or10−3)
andpruning techniqueinSection 2.5.1toobtain asmallerKANpruned fromafixed-shape KAN.
EachKANisinitializedtohaveG=3,trainedwithLBFGS,withincreasingnumberofgridpoints
every 200 steps to cover G = {3,5,10,20,50,100,200}. For each hyperparameter combination,
werun3randomseeds.
For each dataset and each model family (KANs or MLPs), we plot the Pareto frontier 5, in the
(number of parameters, RMSE) plane, shown in Figure 3.2. KANs’ performance is shown to be
consistentlybetterthanMLPs, i.e., KANscanachievelowertraining/testlossesthanMLPs, given
thesamenumberofparameters. Moreover,wereportthe(surprisinglycompact)shapesofourauto-
discovered KANs for special functions in Table 2. On one hand, it is interesting to interpret what
thesecompactrepresentationsmeanmathematically(weincludetheKANillustrationsinFigureF.1
andF.2inAppendixF).Ontheotherhand, thesecompactrepresentationsimplythepossibilityof
breakingdownahigh-dimensionallookuptableintoseveral1Dlookuptables,whichcanpotentially
savealotofmemory,withthe(almostnegligible)overheadtoperformafewadditionsatinference
time.
3.3 Feynmandatasets
ThesetupinSection3.1iswhenweclearlyknow“true”KANshapes. ThesetupinSection3.2is
whenweclearlydonotknow“true”KANshapes. Thispartinvestigatesasetuplyinginthemiddle:
Giventhestructureofthedataset,wemayconstructKANsbyhand,butwearenotsureiftheyare
optimal. Inthisregime,itisinterestingtocomparehuman-constructedKANsandauto-discovered
KANsviapruning(techniquesinSection2.5.1).
5Paretofrontierisdefinedasfitsthatareoptimalinthesenseofnootherfitbeingbothsimplerandmore
accurate.
15
ESMR
ESMR
ESMRName scipy.specialAPI M tei sn tim RMal SK EA <N 1s 0h −ap 2e MinimalKANtestRMSE BestKANshape BestKANtestRMSE MLPtestRMSE
Jacobianellipticfunctions ellipj(x,y) [2,2,1] 7.29×10−3 [2,3,2,1,1,1] 1.33×10−4 6.48×10−4
Incompleteellipticintegralofthefirstkind ellipkinc(x,y) [2,2,1,1] 1.00×10−3 [2,2,1,1,1] 1.24×10−4 5.52×10−4
Incompleteellipticintegralofthesecondkind ellipeinc(x,y) [2,2,1,1] 8.36×10−5 [2,2,1,1] 8.26×10−5 3.04×10−4
Besselfunctionofthefirstkind jv(x,y) [2,2,1] 4.93×10−3 [2,3,1,1,1] 1.64×10−3 5.52×10−3
Besselfunctionofthesecondkind yv(x,y) [2,3,1] 1.89×10−3 [2,2,2,1] 1.49×10−5 3.45×10−4
ModifiedBesselfunctionofthesecondkind kv(x,y) [2,1,1] 4.89×10−3 [2,2,1] 2.52×10−5 1.67×10−4
ModifiedBesselfunctionofthefirstkind iv(x,y) [2,4,3,2,1,1] 9.28×10−3 [2,4,3,2,1,1] 9.28×10−3 1.07×10−2
AssociatedLegendrefunction(m=0) lpmv(0,x,y) [2,2,1] 5.25×10−5 [2,2,1] 5.25×10−5 1.74×10−2
AssociatedLegendrefunction(m=1) lpmv(1,x,y) [2,4,1] 6.90×10−4 [2,4,1] 6.90×10−4 1.50×10−3
AssociatedLegendrefunction(m=2) lpmv(2,x,y) [2,2,1] 4.88×10−3 [2,3,2,1] 2.26×10−4 9.43×10−4
sphericalharmonics(m=0,n=1) sph_harm(0,1,x,y) [2,1,1] 2.21×10−7 [2,1,1] 2.21×10−7 1.25×10−6
sphericalharmonics(m=1,n=1) sph_harm(1,1,x,y) [2,2,1] 7.86×10−4 [2,3,2,1] 1.22×10−4 6.70×10−4
sphericalharmonics(m=0,n=2) sph_harm(0,2,x,y) [2,1,1] 1.95×10−7 [2,1,1] 1.95×10−7 2.85×10−6
sphericalharmonics(m=1,n=2) sph_harm(1,2,x,y) [2,2,1] 4.70×10−4 [2,2,1,1] 1.50×10−5 1.84×10−3
sphericalharmonics(m=2,n=2) sph_harm(2,2,x,y) [2,2,1] 1.12×10−3 [2,2,3,2,1] 9.45×10−5 6.21×10−4
Table2:Specialfunctions
Pruned
FeynmanEq. OriginalFormula Dimensionlessformula Variables Hum Ka An- Nco sn hs at pru ected (s tmK haA a tlN l ae cs hs th is ea h vp a ee p se (K loA wP Nr eu sn s the ld oap sse
)
H (lou wm eKa sn tA- tc N eo sn tlos Rt sr Msuc St Eed
)
(loweK sP tAr tNu esn tle o Rd s Ms
SE)
(loweU K stn Ap tN er su tln o Re sd Ms
SE)
(lowestM l to eL ss tsP
RMSE)
RMSE<10−2)
II .6.6 .2.2
b
exe px (p −(− (θ−2 2θσ σθ2 2 21)) 2/ )√ /√2π 2σ π2
σ2
exe px (p −(− (θ−2 2θσ σθ2 2 21)) 2/ )√ /√2π 2σ π2
σ2
θ,θ θ, 1σ
,σ
[3[2 ,2,2 ,2,1 ,1,1 ,1]
]
[ [2 3, ,2 4, ,1 1]
]
[3[2 ,2,2 ,2,1 ,1,1 ,1]
]
7 1. .6 26 2× ×1 10 0− −5
3
2 4. .8 46 5× ×1 10 0− −5
4
4 1. .6 20 5× ×1 10 0− −5
3
1 7. .4 45 0× ×1 10 0− −4
4
I.9.18 (x2−x1)2+(Gy2m−1ym 1)22+(z2−z1)2 (b−1)2+(c−ad)2+(e−f)2 a,b,c,d,e,f [6,4,2,1,1] [6,4,1,1] [6,4,1,1] 1.48×10−3 8.62×10−3 6.56×10−3 1.59×10−3
I.12.11 q(Ef+Bvsinθ) 1+asinθ a,θ [2,2,2,1] [2,2,1] [2,2,1] 2.07×10−3 1.39×10−3 9.13×10−4 6.71×10−4
I.13.12 Gm1m2(r1 2−r1 1) a(1b−1) a,b [2,2,1] [2,2,1] [2,2,1] 7.22×10−3 4.81×10−3 2.72×10−3 1.42×10−3
I.15.3x √ 1x −− (u uct )2 √1 1− −a b2 a,b [2,2,1,1] [2,1,1] [2,2,1,1,1] 7.35×10−3 1.58×10−3 1.14×10−3 8.54×10−4
I.16.6 1u++ucv 2v 1a++abb a,b [2,2,2,2,2,1] [2,2,1] [2,2,1] 1.06×10−3 1.19×10−3 1.53×10−3 6.20×10−4
I.18.4 m1mr11++mm22r2 11++aab a,b [2,2,2,1,1] [2,2,1] [2,2,1] 3.92×10−4 1.50×10−4 1.32×10−3 3.68×10−4
I.26.2 arcsin(nsinθ2) arcsin(nsinθ2) n,θ2 [2,2,2,1,1] [2,2,1] [2,2,2,1,1] 1.22×10−1 7.90×10−4 8.63×10−4 1.24×10−3
II .. 22 97 .. 16
6
(cid:112)x21+x22−d 211 x+1 1xdn2
2cos(θ1−θ2)
(cid:112)1+a2−1 2+1 aa cb
os(θ1−θ2)
a,a θ1, ,b
θ2
[3,2[2 ,2,2 ,3,1 ,2,1 ,1]
,1]
[3[2 ,2,1 ,2,1 ,1]
]
[3[2 ,2,1 ,3,1 ,1]
]
2 2. .2 32 6× ×1 10 0− −4
1
1 3. .9 94 9×× 11 00 −− 34 32 .. 21 04× ×1 10 0− −4
3
2 4. .4 66 4× ×1 10 0− −4
3
I.30.3 I∗,0s si in n2 2( (n θ2 2θ )) s si in n2 2( (n θ2 2θ )) n,θ [2,3,2,2,1,1] [2,4,3,1] [2,3,2,3,1,1] 3.85×10−1 1.03×10−3 1.11×10−2 1.50×10−2
I I. .3 30 7. .5
4
I∗=I1+arc Is 2i +n( 2nλ √d)
I1I2cosδ
1+a arc +si 2n √(na a)
cosδ
a a, ,n
δ
[2[2 ,3,1 ,2,1 ,1]
]
[ [2 2, ,1 2, ,1 1]
]
[2,1 [2,1 ,2,1 ,1,1 ],1] 2 7. .2 53 7× ×1 10 0− −4
5
3 4. .4 99 1× ×1 10 0− −5
6
6 3. .9 42 1× ×1 10 0− −5
4
9 5. .4 65 7× ×1 10 0− −5
4
I.40.1 n0exp(−m kbg Tx) n0e−a n0,a [2,1,1] [2,2,1] [2,2,1,1,1,2,1] 3.45×10−3 5.01×10−4 3.12×10−4 3.99×10−4
I.44.4 nkbTln(VV21) nlna n,a [2,2,1] [2,2,1] [2,2,1] 2.30×10−5 2.43×10−5 1.10×10−4 3.99×10−4
I.50.26 x1(cos(ωt)+αcos2(wt)) cosa+αcos2a a,α [2,2,3,1] [2,3,1] [2,3,2,1] 1.52×10−4 5.82×10−4 4.90×10−4 1.53×10−3
II II .. 62 .. 14 52
a
4π3ϵk
p
rd( 5T z2 (cid:112)− dT x1 2)A
+y2
41π( ca √− a21 +)b
b2
aa ,, b,b
c
[3[ ,22 ,, 22 ,, 21 ,]
1]
[3[2 ,2,2 ,1,1 ,1]
]
[ [2 3, ,2 2, ,2 1, ,1 1]
]
8 2. .5 64 1× ×1 10 0− −4
3
7 3. .2 22 8× ×1 10 0− −4
3
1 1. .2 32 5× ×1 10 0− −3
3
1 5. .8 91 2× ×1 10 0− −4
4
II.11.7 n0(1+pdE kbfTcosθ) n0(1+acosθ) n0,a,θ [3,3,3,2,2,1] [3,3,1,1] [3,3,1,1] 7.10×10−3 8.52×10−3 5.03×10−3 5.92×10−4
II.11.27 1−nαn3αϵEf 1−nαn3α n,α [2,2,1,2,1] [2,1,1] [2,2,1] 2.67×10−5 4.40×10−5 1.43×10−5 7.18×10−5
I II I. .3 35 6. .1 38 8 exp( µµ kk mm bb TT BB) ++n e0x µ ϵcp m2( kα− bMµ TkmbTB) exp( aa) ++ne0 αxp b(−a) an ,0 α, ,a b [ [2 3, ,1 3, ,1 1] ] [ [2 3, ,1 2, ,1 1] ] [2 [3,1 ,2,1 ,1,1 ]] 4 2. .1 83 5× ×1 10 0− −4 3 11 .. 15 58× ×1 10 0− −4 3 7 3. .7 01 3×× 11 00 −− 35 7 2. .9 12 5× ×1 10 0− −5 3
II.38.3 YdAx ab a,b [2,1,1] [2,1,1] [2,2,1,1,1] 1.47×10−4 8.78×10−5 6.43×10−4 5.26×10−4
II II II .1.9 0. .5 12
9
µp mdhE (cid:113)f Bsi (n x2(2 ω( +−(ω ω B− 0 yω ) 2t0/ +) 2t )/ B22)
z2
√a 1s +i (n b2 a−2( 2cb− )2 +2c)
b2
a a, ,b, bc [3 [,2 2, ,3 1, ,1 1, ]1] [3 [2,3 ,1,2 ,1,1 ]] [3 [,3 2, ,2 1, ,1 2, ,1 1, ]1] 4 2. .4 53 4× ×1 10 0− −2
3
3 1. .9 10 8× ×1 10 0− −3
3
2 8. .1 11 6× ×1 10 0− −2
4
9 1. .0 67 7× ×1 10 0− −4
4
III.17.37 β(1+αcosθ) β(1+αcosθ) α,β,θ [3,3,3,2,2,1] [3,3,1] [3,3,1] 1.10×10−3 5.03×10−4 4.12×10−4 6.80×10−4
Table3:Feynmandataset
Feynman dataset. The Feynman dataset collects many physics equations from Feynman’s text-
books[20,21]. Forourpurpose,weareinterestedinproblemsintheFeynman_no_unitsdataset
that have at least 2 variables, since univariate problems are trivial for KANs (they simplify to 1D
splines). AsampleequationfromtheFeynmandatasetistherelativisicvelocityadditionformula
f(u,v)=(u+v)/(1+uv). (3.1)
The dataset can be constructed by randomly drawing u ∈ (−1,1), v ∈ (−1,1), and computing
i i
f = f(u ,v ). Given many tuples (u ,v ,f ), a neural network is trained and aims to predict f
i i i i i i
fromuandv. Weareinterestedin(1)howwellaneuralnetworkcanperformontestsamples;(2)
howmuchwecanlearnaboutthestructureoftheproblemfromneuralnetworks.
Wecomparefourkindsofneuralnetworks:
(1) Human-constructued KAN. Given a symbolic formula, we rewrite it in Kolmogorov-Arnold
representations. Forexample,tomultiplytwonumbersxandy,wecanusetheidentityxy =
16(x+y)2
−
(x−y)2
, which corresponds to a [2,2,1] KAN. The constructued shapes are listed in
4 4
the“Human-constructedKANshape”inTable3.
(2) KANswithoutpruning.WefixtheKANshapetowidth5anddepthsaresweptover{2,3,4,5,6}.
(3) KANwithpruning. Weusethesparsification(λ = 10−2 or10−3)andthepruningtechnique
fromSection2.5.1toobtainasmallerKANfromafixed-shapeKANfrom(2).
(4) MLPs with fixed width 20, depths swept in {2,3,4,5,6}, and activations chosen from
{Tanh,ReLU,SiLU}.
EachKANisinitializedtohaveG=3,trainedwithLBFGS,withincreasingnumberofgridpoints
every 200 steps to cover G = {3,5,10,20,50,100,200}. For each hyperparameter combination,
we try 3 random seeds. For each dataset (equation) and each method, we report the results of the
bestmodel(minimalKANshape,orlowesttestloss)overrandomseedsanddepthsinTable3. We
findthatMLPsandKANsbehavecomparablyonaverage. Foreachdatasetandeachmodelfamily
(KANs or MLPs), we plot the Pareto frontier in the plane spanned by the number of parameters
and RMSE losses, shown in Figure D.1 in Appendix D. We conjecture that the Feynman datasets
are too simple to let KANs make further improvements, in the sense that variable dependence is
usually smooth or monotonic, which is in contrast to the complexity of special functions which
oftendemonstrateoscillatorybehavior.
Auto-discoveredKANsaresmallerthanhuman-constructedones. WereporttheprunedKAN
shapeintwocolumnsofTable3;onecolumnisfortheminimalprunedKANshapethatcanachieve
reasonable loss (i.e., test RMSE smaller than 10−2); the other column is for the pruned KAN that
achieveslowesttestloss. Forcompleteness,wevisualizeall54prunedKANsinAppendixD(Fig-
ure D.2 and D.3). It is interesting to observe that auto-discovered KAN shapes (for both minimal
andbest)areusuallysmallerthanourhumanconstructions.ThismeansthatKArepresentationscan
bemoreefficientthanweimagine. Atthesametime,thismaymakeinterpretabilitysubtlebecause
informationisbeingsquashedintoasmallerspacethanwhatwearecomfortablewith.
Consider the relativistic velocity composition f(u,v) = u+v , for example. Our construction is
1+uv
quitedeepbecausewewereassumingthatmultiplicationofu,vwouldusetwolayers(seeFigure4.1
(a)), inversion of 1+uv would use one layer, and multiplication of u+v and 1/(1+uv) would
useanothertwolayers6,resultingatotalof5layers. However,theauto-discoveredKANsareonly2
layersdeep! Inhindsight,thisisactuallyexpectedifwerecalltherapiditytrickinrelativity: define
thetwo“rapidities”a ≡ arctanhuandb ≡ arctanhv. Therelativisticcompositionofvelocities
are simple additions in rapidity space, i.e., u+v = tanh(arctanh u+arctanh v), which can be
1+uv
realizedbyatwo-layerKAN.Pretendingwedonotknowthenotionofrapidityinphysics,wecould
potentiallydiscoverthisconceptrightfromKANswithouttrial-and-errorsymbolicmanipulations.
TheinterpretabilityofKANswhichcanfacilitatescientificdiscoveryisthemaintopicinSection4.
3.4 Solvingpartialdifferentialequations
WeconsideraPoissonequationwithzeroDirichletboundarydata. ForΩ = [−1,1]2,considerthe
PDE
u +u =f in Ω,
xx yy
(3.2)
u=0 on ∂Ω.
Weconsiderthe dataf = −π2(1+4y2)sin(πx)sin(πy2)+2πsin(πx)cos(πy2) forwhich u =
sin(πx)sin(πy2)isthetruesolution. Weusetheframeworkofphysics-informedneuralnetworks
6Notethatwecannotusethelogarithmicconstructionfordivision,becauseuandvheremightbenegative
numbers.
17111 000 321 K M MA L LN P P [ [[ 2 22 , ,, 1 11 0 00 , 0, 11 ,] 1] 00,100,1] 11 00 10 K M MA L LN P P [ [[ 2 22 , ,, 1 11 0 00 , 0, 11 ,] 1] 00,100,1] 111 000 321 K K K
M M
M
MA A A
L L
L
LN N N
P P
P
P
( (
(
([ [ [
d d
d
d2 2 2
e e
e
e, , ,5 7 1
p p
p
p, , 0
t t
t
t1 1 ,
h h
h
h] ] 1 ]
2 3
4
5) )
)
)
11 00 10 K K K
M M
M
MA A A
L L
L
LN N N
P P
P
P
( (
(
([ [ [
d d
d
d2 2 2
e e
e
e, , ,5 7 1
p p
p
p, , 0
t t
t
t1 1 ,
h h
h
h] ] 1 ]
2 3
4
5) )
)
)
N4 N4
104 102 104 102
105 103 105 103
106 104 106 104
107 107
0 50 100 150 200 250 0 50 100 150 200 250 101 102 103 104 105 101 102 103 104 105
step step Number of parameters Number of parameters
Figure3.3: ThePDEexample. WeplotL2squaredandH1squaredlossesbetweenthepredictedsolutionand
groundtruthsolution. Firstandsecond: trainingdynamicsoflosses. Thirdandfourth: scalinglawsoflosses
againstthenumberofparameters. KANsconvergefaster,achievelowerlosses,andhavesteeperscalinglaws
thanMLPs.
Phase 1 Phase 2 Phase 3 Phase 4 Phase 5
1
0
1
0
1
0
-1 0 1 -1 0 1 -1 0 1 -1 0 1 -1 0 1
Figure3.4: Atoycontinuallearningproblem. Thedatasetisa1Dregressiontaskwith5Gaussianpeaks(top
row). Data around each peak is presented sequentially (instead of all at once) to KANs and MLPs. KANs
(middlerow)canperfectlyavoidcatastrophicforgetting,whileMLPs(bottomrow)displayseverecatastrophic
forgetting.
(PINNs)[22,23]tosolvethisPDE,withthelossfunctiongivenby
1
(cid:88)ni
1
(cid:88)nb
loss =αloss +loss :=α |u (z )+u (z )−f(z )|2+ u2,
pde i b n xx i yy i i n
i b
i=1 i=1
whereweuseloss todenotetheinteriorloss,discretizedandevaluatedbyauniformsamplingofn
i i
pointsz = (x ,y )insidethedomain,andsimilarlyweuseloss todenotetheboundaryloss,dis-
i i i b
cretizedandevaluatedbyauniformsamplingofn pointsontheboundary. αisthehyperparameter
b
balancingtheeffectofthetwoterms.
WecomparetheKANarchitecturewiththatofMLPsusingthesamehyperparametersn =10000,
i
n =800,andα=0.01. WemeasureboththeerrorintheL2 normandenergy(H1)normandsee
b
thatKANachievesamuchbetterscalinglawwithasmallererror,usingsmallernetworksandfewer
parameters;seeFigure3.3. ThereforewespeculatethatKANsmighthavethepotentialofserving
asagoodneuralnetworkrepresentationformodelreductionofPDEs.
3.5 ContinualLearning
Catastrophicforgettingisaseriousproblemincurrentmachinelearning[24]. Whenahumanmas-
tersataskandswitchestoanothertask, theydonotforgethowtoperformthefirsttask. Unfortu-
nately,thisisnotthecaseforneuralnetworks. Whenaneuralnetworkistrainedontask1andthen
shiftedtobeingtrainedontask2,thenetworkwillsoonforgetabouthowtoperformtask1. Akey
18
derauqs
rorre
2L
ataD
NAK
PLM
derauqs
rorre
1H
derauqs
rorre
2L
derauqs
rorre
1Hdifferencebetweenartificialneuralnetworksandhumanbrainsisthathumanbrainshavefunction-
allydistinctmodulesplacedlocallyinspace. Whenanewtaskislearned,structurere-organization
onlyoccursinlocalregionsresponsibleforrelevantskills[25,26],leavingotherregionsintact.Most
artificialneuralnetworks,includingMLPs,donothavethisnotionoflocality,whichisprobablythe
reasonforcatastrophicforgetting.
We show that KANs have local plasticity and can avoid catastrophic forgetting by leveraging the
locality of splines. The idea is simple: since spline bases are local, a sample will only affect a
few nearby spline coefficients, leaving far-away coefficients intact (which is desirable since far-
away regions may have already stored information that we want to preserve). By contrast, since
MLPs usually use global activations, e.g., ReLU/Tanh/SiLU etc., any local change may propagate
uncontrollablytoregionsfaraway,destroyingtheinformationbeingstoredthere.
Weuseatoyexampletovalidatethisintuition. The1Dregressiontaskiscomposedof5Gaussian
peaks. Dataaroundeachpeakispresentedsequentially(insteadofallatonce)toKANsandMLPs,
asshowninFigure3.4toprow. KANandMLPpredictionsaftereachtrainingphaseareshownin
the middle and bottom rows. As expected, KAN only remodels regions where data is present on
in the current phase, leaving previous regions unchanged. By contrast, MLPs remodels the whole
regionafterseeingnewdatasamples,leadingtocatastrophicforgetting.
Here we simply present our preliminary results on an extremely simple example, to demonstrate
how one could possibly leverage locality in KANs (thanks to spline parametrizations) to reduce
catastrophic forgetting. However, it remains unclear whether our method can generalize to more
realisticsetups,whichweleaveforfuturework. Wewouldalsoliketostudyhowourmethodcan
beconnectedtoandcombinedwithSOTAmethodsincontinuallearning[27,28].
4 KANsareinterpretable
In this section, we show that KANs are interpretable and interactive thanks to the techniques we
developedinSection2.5. WewanttotesttheuseofKANsnotonlyonsynthetictasks(Section4.1
and4.2), butalsoinreal-lifescientificresearch. WedemonstratethatKANscan(re)discoverboth
highlynon-trivialrelationsinknottheory(Section4.3)andphasetransitionboundariesincondensed
matterphysics(Section4.4). KANscouldpotentiallybethefoundationmodelforAI+Sciencedue
totheiraccuracy(lastsection)andinterpretability(thissection).
4.1 Supervisedtoydatasets
We first examine KANs’ ability to reveal the compositional structures in symbolic formulas. Six
examplesarelistedbelowandtheirKANsarevisualizedinFigure4.1. KANsareabletorevealthe
compositionalstructurespresentintheseformulas,aswellaslearnthecorrectunivariatefunctions.
(a) Multiplication f(x,y) = xy. A [2,5,1] KAN is pruned to a [2,2,1] KAN. The learned acti-
vationfunctionsarelinearandquadratic. Fromthecomputationgraph, weseethatthewayit
computesxyisleveraging2xy =(x+y)2−(x2+y2).
(b) Division of positive numbers f(x,y) = x/y. A [2,5,1] KAN is pruned to a [2,1,1] KAN.
The learned activation functions are logarithmic and exponential functions, and the KAN is
computingx/ybyleveragingtheidentityx/y =exp(logx−logy).
(c) Numericaltocategorical.Thetaskistoconvertarealnumberin[0,1]toitsfirstdecimaldigit(as
one hots), e.g., 0.0618 → [1,0,0,0,0,···], 0.314 → [0,0,0,1,0,···]. Notice that activation
functionsarelearnedtobespikeslocatedaroundthecorrespondingdecimaldigits.
19Figure4.1:KANsareinterepretableforsimplesymbolictasks
(d) Specialfunctionf(x,y)=exp(J (20x)+y2). Onelimitationofsymbolicregressionisthatit
0
willneverfindthecorrectformulaofaspecialfunctionifthespecialfunctionisnotprovidedas
priorknowledge.KANscanlearnspecialfunctions–thehighlywigglyBesselfunctionJ (20x)
0
islearned(numerically)byKAN.
(e) Phase transition f(x ,x ,x ) = tanh(5(x4 +x4 +x4 −1)). Phase transitions are of great
1 2 3 1 2 3
interestinphysics, sowewantKANstobeabletodetectphasetransitionsandtoidentifythe
correct order parameters. We use the tanh function to simulate the phase transition behavior,
andtheorderparameteristhecombinationofthequartictermsofx ,x ,x . Boththequartic
1 2 3
dependence and tanh dependence emerge after KAN training. This is a simplified case of a
localizationphasetransitiondiscussedinSection4.4.
(cid:112)
(f) Deeper compositions f(x ,x ,x ,x ) = (x −x )2+(x −x )2. To compute this, we
1 2 3 4 1 2 3 4
would need the identity function, squared function, and square root, which requires at least
a three-layer KAN. Indeed, we find that a [4,3,3,1] KAN can be auto-pruned to a [4,2,1,1]
KAN,whichexactlycorrespondstothecomputationgraphwewouldexpect.
More examples from the Feynman dataset and the special function dataset are visualized in Fig-
ureD.2,D.3,F.1,F.2inAppendicesDandF.
4.2 Unsupervisedtoydataset
Often, scientificdiscoveriesareformulatedassupervisedlearningproblems, i.e., giveninputvari-
ablesx ,x ,··· ,x andoutputvariable(s)y,wewanttofindaninterpretablefunctionf suchthat
1 2 d
y ≈ f(x ,x ,··· ,x ). However, another type of scientific discovery can be formulated as unsu-
1 2 d
pervised learning, i.e., given a set of variables (x ,x ,··· ,x ), we want to discover a structural
1 2 d
relationshipbetweenthevariables. Specifically,wewanttofindanon-zerof suchthat
f(x ,x ,··· ,x )≈0. (4.1)
1 2 d
20Figure 4.2: Unsupervised learning of a toy task. KANs can identify groups of dependent variables, i.e.,
(x ,x ,x )and(x ,x )inthiscase.
1 2 3 4 5
Forexample,considerasetoffeatures(x ,x ,x )thatsatisfiesx =exp(sin(πx )+x2). Thena
1 2 3 3 1 2
validf isf(x ,x ,x ) = sin(πx )+x2−log(x ) = 0,implyingthatpointsof(x ,x ,x )form
1 2 3 1 2 3 1 2 3
a2Dsubmanifoldspecifiedbyf =0insteadoffillingthewhole3Dspace.
Ifanalgorithmforsolvingtheunsupervisedproblemcanbedevised,ithasaconsiderableadvantage
overthesupervisedproblem,sinceitrequiresonlythesetsoffeaturesS = (x ,x ,··· ,x ). The
1 2 d
supervisedproblem,ontheotherhand,triestopredictsubsetsoffeaturesintermsoftheothers,i.e.
itsplitsS =S ∪S intoinputandoutputfeaturesofthefunctiontobelearned. Withoutdomain
in out
expertise to advise the splitting, there are 2d −2 possibilities such that |S | > 0 and |S | > 0.
in out
This exponentially large space of supervised problems can be avoided by using the unsupervised
approach. ThisunsupervisedlearningapproachwillbevaluabletotheknotdatasetinSection4.3.
A Google Deepmind team [29] manually chose signature to be the target variable, otherwise they
would face this combinatorial problem described above. This raises the question whether we can
insteadtackletheunsupervisedlearningdirectly. Wepresentourmethodandatoyexamplebelow.
We tackle the unsupervised learning problem by turning it into a supervised learning problem on
all of the d features, without requiring the choice of a splitting. The essential idea is to learn a
function f(x ,...,x ) = 0 such that f is not the 0-function. To do this, similar to contrastive
1 d
learning, wedefinepositivesamplesandnegativesamples: positivesamplesarefeaturevectorsof
realdata. Negativesamplesareconstructedbyfeaturecorruption. Toensurethattheoverallfeature
distributionforeachtopologicalinvariantstaysthesame,weperformfeaturecorruptionbyrandom
permutation of each feature across the entire training set. Now we want to train a network g such
thatg(x ) = 1andg(x ) = 0whichturnstheproblemintoasupervisedproblem. However,
real fake
rememberthatweoriginallywantf(x ) = 0andf(x ) ̸= 0. Wecanachievethisbyhaving
real fake
g = σ◦f where σ(x) = exp(− x2 ) is a Gaussian function with a small width w, which can be
2w2
convenientlyrealizedbyaKANwithshape[...,1,1]whoselastactivationissettobetheGaussian
functionσandallpreviouslayersformf.Exceptforthemodificationsmentionedabove,everything
elseisthesameforsupervisedtraining.
Nowwedemonstratethattheunsupervisedparadigmworksforasyntheticexample.Letusconsider
a6Ddataset,where(x ,x ,x )aredependentvariablessuchthatx =exp(sin(x )+x2);(x ,x )
1 2 3 3 1 2 4 5
aredependentvariableswithx = x3; x isindependentoftheothervariables. InFigure4.2, we
5 4 6
showthatforseed=0, KANrevealsthefunctionaldependenceamongx ,x , andx ; foranother
1 2 3
seed=2024, KANrevealsthefunctionaldependencebetweenx andx . Ourpreliminaryresults
4 5
rely on randomness (different seeds) to discover different relations; in the future we would like to
investigateamoresystematicandmorecontrolledwaytodiscoveracompletesetofrelations. Even
21so,ourtoolinitscurrentstatuscanprovideinsightsforscientifictasks. Wepresentourresultswith
theknotdatasetinSection4.3.
4.3 ApplicationtoMathematics: KnotTheory
Knottheoryisasubjectinlow-dimensionaltopologythatshedslightontopologicalaspectsofthree-
manifoldsandfour-manifoldsandhasavarietyofapplications,includinginbiologyandtopological
quantumcomputing. Mathematically,aknotK isanembeddingofS1 intoS3. TwoknotsK and
K′aretopologicallyequivalentifonecanbedeformedintotheotherviadeformationoftheambient
space S3, in which case we write [K] = [K′]. Some knots are topologically trivial, meaning that
theycanbesmoothlydeformedtoastandardcircle. Knotshaveavarietyofdeformation-invariant
featuresf calledtopologicalinvariants,whichmaybeusedtoshowthattwoknotsaretopologically
inequivalent,[K] ̸= [K′]iff(K) ̸= f(K′). Insomecasesthetopologicalinvariantsaregeometric
innature. Forinstance,ahyperbolicknotK hasaknotcomplementS3\K thatadmitsacanonical
hyperbolic metric g such that vol (K) is a topological invariant known as the hyperbolic volume.
g
Othertopologicalinvariantsarealgebraicinnature,suchastheJonespolynomial.
Given the fundamental nature of knots in mathematics and the importance of its applications, it is
interestingtostudywhetherMLcanleadtonewresults.Forinstance,in[30]reinforcementlearning
wasutilizedtoestablishribbonnessofcertainknots,whichruledoutmanypotentialcounterexam-
plestothesmooth4dPoincaréconjecture.
SupervisedlearningIn[29],supervisedlearningandhumandomainexpertswereutilizedtoarrive
at a new theorem relating algebraic and geometric knot invariants. In this case, gradient saliency
identifiedkeyinvariantsforthesupervisedproblem, whichledthedomainexpertstomakeacon-
jecture that was subsequently refined and proven. We study whether a KAN can achieve good
interpretableresultsonthesameproblem,whichpredictsthesignatureofaknot. Theirmainresults
fromstudyingtheknottheorydatasetare:
(1) Theyusenetworkattributionmethodstofindthatthesignatureσismostlydependentonmerid-
inaldistanceµ(realµ ,imagµ )andlongitudinaldistanceλ.
r i
(2) Humanscientistslateridentifiedthatσ hashighcorrelationwiththeslope ≡ Re(λ) = λµr
µ µ2+µ2
r i
andderivedaboundfor|2σ−slope|.
WeshowbelowthatKANsnotonlyrediscovertheseresultswithmuchsmallernetworksandmuch
moreautomation,butalsopresentsomeinterestingnewresultsandinsights.
To investigate (1), we treat 17 knot invariants as inputs and signature as outputs. Similar to the
setupin[29],signatures(whichareevennumbers)areencodedasone-hotvectorsandnetworksare
trainedwithcross-entropyloss. Wefindthatanextremelysmall[17,1,14]KANisabletoachieve
81.6%testaccuracy(whileDeepmind’s4-layerwidth-300MLPachieves78%testaccuracy). The
[17,1,14]KAN(G = 3,k = 3)has≈ 200parameters,whiletheMLPhas≈ 3×105 parameters,
showninTable4. ItisremarkablethatKANscanbebothmoreaccurateandmuchmoreparameter
efficientthanMLPsatthesametime. Intermsofinterpretability,wescalethetransparencyofeach
activation according to its magnitude, so it becomes immediately clear which input variables are
importantwithouttheneedforfeatureattribution(seeFigure4.3left):signatureismostlydependent
onµ ,andslightlydependentonµ andλ,whiledependenceonothervariablesissmall. Wethen
r i
traina[3,1,14]KANonthethreeimportantvariables,obtainingtestaccuracy78.2%. Ourresults
haveonesubtledifferencefromresultsin[29]: theyfindthatsignatureismostlydependentonµ ,
i
while we find that signature is mostly dependent on µ . This difference could be due to subtle
r
algorithmicchoices,buthasledustocarryoutthefollowingexperiments: (a)ablationstudies. We
22Figure4.3: Knotdataset, supervisedmode. WithKANs, werediscoverDeepmind’sresultsthatsignatureis
mainlydependentonmeridinaltranslation(realandimaginaryparts).
Method Architecture ParameterCount Accuracy
Deepmind’sMLP 4layer,width-300 3×105 78.0%
KANs 2layer,[17,1,14](G=3,k =3) 2×102 81.6%
Table4:KANscanachievebetteraccuracythanMLPswithmuchfewerparametersinthesignatureclassifica-
tionproblem.
show that µ contributes more to accuracy than µ (see Figure 4.3): for example, µ alone can
r i r
achieve65.0%accuracy,whileµ alonecanonlyachieve43.8%accuracy. (b)Wefindasymbolic
i
formula(inTable5)whichonlyinvolvesµ andλ,butcanachieve77.8%testaccuracy.
r
To investigate (2), i.e., obtain the symbolic form of σ, we formulate the problem as a regression
task. Using auto-symbolic regression introduced in Section 2.5.1, we can convert a trained KAN
intosymbolicformulas. WetrainKANswithshapes[3,1], [3,1,1], [3,2,1],whosecorresponding
symbolic formulas are displayed in Table 5 B-D. It is clear that by having a larger KAN, both
accuracyandcomplexityincrease.SoKANsprovidenotjustasinglesymbolicformula,butawhole
Pareto frontier of formulas, trading off simplicity and accuracy. However, KANs need additional
inductivebiasestofurthersimplifytheseequationstorediscovertheformulafrom[29](Table5A).
We have tested two scenarios: (1) in the first scenario, we assume the ground truth formula has a
multi-variatePaderepresentation(divisionoftwomulti-variateTaylorseries). Wefirsttrain[3,2,1]
andthenfitittoaPaderepresentation. WecanobtainFormulaEinTable5,whichbearssimilarity
withDeepmind’sformula. (2)WehypothesizethatthedivisionisnotveryinterpretableforKANs,
sowe traintwo KANs(one forthe numeratorand theotherfor thedenominator) anddivide them
manually. Surprisingly, we end up with the formula F (in Table 5) which only involves µ and λ,
r
althoughµ isalsoprovidedbutignoredbyKANs.
i
So far, we have rediscovered the main results from [29]. It is remarkable to see that KANs made
this discovery very intuitive and convenient. Instead of using feature attribution methods (which
are great methods), one can instead simply stare at visualizations of KANs. Moreover, automatic
symbolicregressionalsomakesthediscoveryofsymbolicformulasmucheasier.
In the next part, we propose a new paradigm of “AI for Math” not included in the Deepmind pa-
per, where we aim to use KANs’ unsupervised learning mode to discover more relations (besides
signature)inknotinvariants.
23test r2with r2withDM
Id Formula Discoveredby
acc Signature formula
A λµr Human(DM) 83.1% 0.946 1
(µ2 r+µ2 i)
B −0.02sin(4.98µ i+0.85)+0.08|4.02µ r+6.28|−0.52− [3,1]KAN 62.6% 0.837 0.897
0.04e−0.88(1−0.45λ)2
C
0.17tan(−1.51+0.1e−1.43(1−0.4µi)2+0.09e−0.06(1−0.21λ)2
+ [3,1,1]KAN 71.9% 0.871 0.934
1.32e−3.18(1−0.43µr)2)
D −0.09+1.04exp(−9.59(−0.62sin(0.61µ r +7.26))− [3,2,1]KAN 84.0% 0.947 0.997
0.32tan(0.03λ−6.59)+1−0.11e−1.77(0.31−µi)2)2 −
1.09e−7.6(0.65(1−0.01λ)3 + 0.27atan(0.53µ − 0.6) +
i
0.09+exp(−2.58(1−0.36µ )2))
r
E 4.76λµr [3,2,1]KAN 82.8% 0.946 0.997
3.09µi+6.05µ2 r+3.54µ2
i
+Padeapprox
F 2.94−2.92(1−0.10µr)2 [3,1]KAN/[3,1]KAN 77.8% 0.925 0.977
0.32(0.18−µr)2+5.36(1−0.04λ)2+0.50
Table5: Symbolicformulasofsignatureasafunctionofmeridinaltranslationµ(realµ ,imagµ )andlon-
r i
gitudinal translation λ. In [29], formula A was discovered by human scientists inspired by neural network
attributionresults. FormulasB-Fareauto-discoveredbyKANs. KANscantrade-offbetweensimplicityand
accuracy (B, C, D). By adding more inductive biases, KAN is able to discover formula E which is not too
dissimilarfromformulaA.KANsalsodiscoveredaformulaFwhichonlyinvolvestwovariables(µ andλ)
r
insteadofallthreevariables,withlittlesacrificeinaccuracy.
Figure4.4: Knotdataset,unsupervisedmode. WithKANs,werediscoverthreemathematicalrelationsinthe
knotdataset.
UnsupervisedlearningAswementionedinSection4.2,unsupervisedlearningisthesetupthatis
morepromisingsinceitavoidsmanualpartitionofinputandoutputvariableswhichhavecombina-
toriallymanypossibilities. Intheunsupervisedlearningmode,wetreatall18variables(including
signature) as inputs such that they are on the same footing. Knot data are positive samples, and
we randomly shuffle features to obtain negative samples. An [18,1,1] KAN is trained to classify
whetheragivenfeaturevectorbelongstoapositivesample(1)oranegativesample(0). Weman-
ually set the second layer activation to be the Gaussian function with a peak one centered at zero,
so positive samples will have activations at (around) zero, implicitly giving a relation among knot
invariants
(cid:80)18
g (x ) = 0 where x stands for a feature (invariant), and g is the corresponding
i=1 i i i i
24activation function which can be readily read off from KAN diagrams. We train the KANs with
λ = {10−2,10−3}tofavorsparsecombinationofinputs,andseed = {0,1,··· ,99}. All200net-
workscanbegroupedintothreeclusters,withrepresentativeKANsdisplayedinFigure4.4. These
threegroupsofdependentvariablesare:
(1) The first group of dependent variables is signature, real part of meridinal distance, and longi-
tudinal distance (plus two other variables which can be removed because of (3)). This is the
signaturedependencestudiedabove,soitisveryinterestingtoseethatthisdependencerelation
isrediscoveredagainintheunsupervisedmode.
(2) ThesecondgroupofvariablesinvolvecuspvolumeV,realpartofmeridinaltranslationµ and
r
longitudinal translation λ. Their activations all look like logarithmic functions (which can be
verified by the implied symbolic functionality in Section 2.5.1). So the relation is −logV +
logµ +logλ=0whichisequivalenttoV =µ λ,whichistruebydefinition. Itis,however,
r r
reassuringthatwediscoverthisrelationwithoutanypriorknowledge.
(3) The third group of variables includes the real part of short geodesic g and injectivity radius.
r
Theiractivationslookqualitativelythesamebutdifferbyaminussign,soitisconjecturedthat
thesetwovariableshavealinearcorrelation. Weplot2Dscatters,findingthat2rupperbounds
g ,whichisalsoawell-knownrelation[31].
r
It is interesting that KANs’ unsupervised mode can rediscover several known mathematical rela-
tions. The good news is that the results discovered by KANs are probably reliable; the bad news
isthatwehavenotdiscoveredanythingnewyet. Itisworthnotingthatwehavechosenashallow
KANforsimplevisualization,butdeeperKANscanprobablyfindmorerelationsiftheyexist. We
would like to investigate how to discover more complicated relations with deeper KANs in future
work.
4.4 ApplicationtoPhysics: Andersonlocalization
Andersonlocalizationisthefundamentalphenomenoninwhichdisorderinaquantumsystemleads
tothelocalizationofelectronicwavefunctions,causingalltransporttobeceased[32]. Inoneand
twodimensions, scalingargumentsshowthatallelectroniceigenstatesareexponentiallylocalized
foraninfinitesimalamountofrandomdisorder[33,34]. Incontrast,inthreedimensions,acritical
energyformsaphaseboundarythatseparatestheextendedstatesfromthelocalizedstates,known
as a mobility edge. The understanding of these mobility edges is crucial for explaining various
fundamentalphenomenasuchasthemetal-insulatortransitioninsolids[35],aswellaslocalization
effectsoflightinphotonicdevices[36,37,38,39,40]. Itisthereforenecessarytodevelopmicro-
scopicmodelsthatexhibitmobilityedgestoenabledetailedinvestigations.Developingsuchmodels
isoftenmorepracticalinlowerdimensions,whereintroducingquasiperiodicityinsteadofrandom
disordercanalsoresultinmobilityedgesthatseparatelocalizedandextendedphases. Furthermore,
experimentalrealizationsofanalyticalmobilityedgescanhelpresolvethedebateonlocalizationin
interactingsystems[41,42]. Indeed,severalrecentstudieshavefocusedonidentifyingsuchmodels
andderivingexactanalyticexpressionsfortheirmobilityedges[43,44,45,46,47,48,49].
Here, weapplyKANstonumericaldatageneratedfromquasiperiodictight-bindingmodelstoex-
tract their mobility edges. In particular, we examine three classes of models: the Mosaic model
(MM) [47], the generalized Aubry-André model (GAAM) [46] and the modified Aubry-André
model (MAAM) [44]. For the MM, we testify KAN’s ability to accurately extract mobility edge
asa1Dfunctionofenergy. FortheGAAM,wefindthattheformulaobtainedfromaKANclosely
matches the ground truth. For the more complicated MAAM, we demonstrate yet another exam-
pleofthesymbolicinterpretabilityofthisframework. Ausercansimplifythecomplexexpression
25obtainedfromKANs(andcorrespondingsymbolicformulas)bymeansofa“collaboration”where
thehumangenerateshypothesestoobtainabettermatch(e.g.,makinganassumptionoftheformof
certainactivationfunction),afterwhichKANscancarryoutquickhypothesestesting.
To quantify the localization of states in these models, the inverse participation ratio (IPR) is com-
monlyused. TheIPRforthektheigenstate,ψ(k),isgivenby
(cid:80) |ψ(k)|4
IPR = n n (4.2)
k (cid:16) (cid:17)2
(cid:80) |ψ(k)|2
n n
wherethesumrunsoverthesiteindex. Here,weusetherelatedmeasureoflocalization–thefractal
dimensionofthestates,givenby
log(IPR )
D =− k (4.3)
k log(N)
whereN isthesystemsize. D =0(1)indicateslocalized(extended)states.
k
Mosaic Model (MM) We first consider a class of tight-binding models defined by the Hamilto-
nian[47]
(cid:88)(cid:16) (cid:17) (cid:88)
H =t c† c +H.c. + V (λ,ϕ)c†c , (4.4)
n+1 n n n n
n n
wheretisthenearest-neighborcoupling,c (c†)istheannihilation(creation)operatoratsitenand
n n
thepotentialenergyV isgivenby
n
(cid:40)
λcos(2πnb+ϕ) j =mκ
V (λ,ϕ)= (4.5)
n
0, otherwise,
Tointroducequasiperiodicity, wesetbtobeirrational(inparticular, wechoosebtobethegolden
√
ratio 1+ 5). κ is an integer and the quasiperiodic potential occurs with interval κ. The energy
2
(E) spectrum for this model generically contains extended and localized regimes separated by a
mobilityedge. Interestingly,auniquefeaturefoundhereisthatthemobilityedgesarepresentforan
arbitrarilystrongquasiperiodicpotential(i.e. therearealwaysextendedstatespresentinthesystem
thatco-existwithlocalizedones).
The mobility edge can be described by g(λ,E) ≡ λ−|f (E)| = 0. g(λ,E) > 0 and g(λ,E) <
κ
0correspondtolocalizedandextendedphases, respectively. Learningthemobilityedgetherefore
hingesonlearningthe“orderparameter”g(λ,E). Admittedly,thisproblemcanbetackledbymany
othertheoreticalmethodsforthisclassofmodels[47],butwewilldemonstratebelowthatourKAN
frameworkisreadyandconvenienttotakeinassumptionsandinductivebiasesfromhumanusers.
Let us assume a hypothetical user Alice, who is a new PhD student in condensed matter physics,
and she is provided with a [2,1] KAN as an assistant for the task. Firstly, she understands that
this is a classification task, so it is wise to set the activation function in the second layer to be
sigmoid by using the fix_symbolic functionality. Secondly, she realizes that learning the whole
2Dfunctiong(λ,E)isunnecessarybecauseintheendsheonlycaresaboutλ = λ(E)determined
byg(λ,E)=0. Insodoing,itisreasonabletoassumeg(λ,E)=λ−h(E)=0. Alicesimplysets
theactivationfunctionofλtobelinearbyagainusingthefix_symbolicfunctionality. NowAlice
trainstheKANnetworkandconvenientlyobtainsthemobilityedge,asshowninFigure4.5. Alice
can get both intuitive qualitative understanding (bottom) and quantitative results (middle), which
wellmatchthegroundtruth(top).
26Figure4.5: ResultsfortheMosaicModel. Top: phasediagram. MiddleandBottom: KANscanobtainboth
√
qualitativeintuition(bottom)andextractquantitativeresults(middle).φ= 1+ 5 isthegoldenratio.
2
GeneralizedAndre-AubryModel(GAAM)Wenextconsideraclassoftight-bindingmodelsde-
finedbytheHamiltonian[46]
(cid:88)(cid:16) (cid:17) (cid:88)
H =t c† c +H.c. + V (α,λ,ϕ)c†c , (4.6)
n+1 n n n n
n n
wheretisthenearest-neighborcoupling,c (c†)istheannihilation(creation)operatoratsitenand
n n
thepotentialenergyV isgivenby
n
cos(2πnb+ϕ)
V (α,λ,ϕ)=2λ , (4.7)
n 1−αcos(2πnb+ϕ)
whichissmoothforα ∈ (−1,1). Tointroducequasiperiodicity,weagainsetbtobeirrational(in
particular,wechoosebtobethegoldenratio). Asbefore,wewouldliketoobtainanexpressionfor
themobilityedge. Forthesemodels,themobilityedgeisgivenbytheclosedformexpression[46,
48],
αE =2(t−λ). (4.8)
Werandomlysamplethemodelparameters:ϕ,αandλ(settingtheenergyscalet=1)andcalculate
the energy eigenvalues as well as the fractal dimension of the corresponding eigenstates, which
formsourtrainingdataset.
Here the “order parameter” to be learned is g(α,E,λ,ϕ) = αE + 2(λ − 1) and mobility edge
corresponds to g = 0. Let us again assume that Alice wants to figure out the mobility edge but
only has access to IPR or fractal dimension data, so she decides to use KAN to help her with the
task. Alicewantsthemodeltobeassmallaspossible,soshecouldeitherstartfromalargemodel
and use auto-pruning to get a small model, or she could guess a reasonable small model based on
her understanding of the complexity of the given problem. Either way, let us assume she arrives
ata[4,2,1,1]KAN.First, shesetsthelastactivationtobesigmoidbecausethisisaclassification
27System Origin MobilityEdgeFormula Accuracy
Theory αE+2λ−2=0 99.2%
GAAM
KANauto
(cid:24)1.5(cid:24) 2E(cid:24)(cid:24) 2+21.06αE+(cid:24)0.6(cid:24) 6(cid:24) E+(cid:24)3.5(cid:24) 5α(cid:24)(cid:24) 2+(cid:24)0.9(cid:24) 1(cid:24)
α+45.13λ−54.45=0 99.0%
Theory E+exp(p)−λcoshp=0 98.6%
KANauto 13.99sin(0.28sin(0.87λ+2.22)−0.84arctan(0.58E−0.26)+0.85arctan(0.94p+ 97.1%
0.13)−8.14)−16.74+43.08exp(−0.93(0.06(0.13−p)2−0.27tanh(0.65E+0.25)+
0.63arctan(0.54λ−0.62)+1)2)=0
MAAM
KANman(step2)+auto 4.19(0.28sin(0.97λ+2.17)−0.77arctan(0.83E−0.19)+arctan(0.97p+0.15)− 97.7%
0.35)2−28.93+39.27exp(−0.6(0.28cosh2(0.49p−0.16)−0.34arctan(0.65E+
0.51)+0.83arctan(0.54λ−0.62)+1)2)=0
KANman(step3)+auto −4.63E − 10.25(−0.94sin(0.97λ − 6.81) + tanh(0.8p − 0.45) + 0.09)2 + 97.7%
11.78sin(0.76p−1.41)+22.49arctan(1.08λ−1.32)+31.72=0
KANman(step4A) 6.92E−6.23(−0.92λ−1)2+2572.45(−0.05λ+0.95cosh(0.11p+0.4)−1)2− 96.6%
12.96cosh2(0.53p+0.16)+19.89=0
KANman(step4B) 7.25E−8.81(−0.83λ−1)2−4.08(−p−0.04)2+12.71(−0.71λ+(0.3p+1)2− 95.4%
0.86)2+10.29=0
Table 6: Symbolic formulas for two systems GAAM and MAAM, ground truth ones and KAN-discovered
ones.
problem.ShetrainsherKANwithsomesparsityregularizationtoaccuracy98.7%andvisualizesthe
trainedKANinFigure4.6(a)step1.Sheobservesthatϕisnotpickeduponatall,whichmakesher
realizethatthemobilityedgeisindependentofϕ(agreeingwithEq.(4.8)).Inaddition,sheobserves
thatalmostallotheractivationfunctionsarelinearorquadratic,sosheturnsonautomaticsymbolic
snapping,constrainingthelibrarytobeonlylinearorquadratic. Afterthat,sheimmediatelygetsa
networkwhichisalreadysymbolic(showninFigure4.6(a)step2),withcomparable(evenslightly
better) accuracy 98.9%. By using symbolic_formula functionality, Alice conveniently gets the
symbolicformofg,showninTable6GAAM-KANauto(rowthree).Perhapsshewantstocrossout
somesmalltermsandsnapcoefficienttosmallintegers,whichtakesherclosetothetrueanswer.
This hypothetical story for Alice would be completely different if she is using a symbolic regres-
sion method. If she is lucky, SR can return the exact correct formula. However, the vast majority
of the time SR does not return useful results and it is impossible for Alice to “debug” or inter-
act with the underlying process of symbolic regression. Furthermore, Alice may feel uncomfort-
able/inexperienced to provide a library of symbolic terms as prior knowledge to SR before SR is
run. ByconstrastinKANs,AlicedoesnotneedtoputanypriorinformationtoKANs. Shecanfirst
getsomecluesbystaringatatrainedKANandonlythenitisherjobtodecidewhichhypothesis
shewantstomake(e.g., “allactivationsarelinearorquadratic”)andimplementherhypothesisin
KANs. Although it is not likely for KANs to return the correct answer immediately, KANs will
alwaysreturnsomethinguseful,andAlicecancollaboratewithittorefinetheresults.
Modified Andre-Aubry Model (MAAM) The last class of models we consider is defined by the
Hamiltonian[44]
H = (cid:88) te−p|n−n′|(cid:0) c†c +H.c.(cid:1) +(cid:88) V (λ,ϕ)c†c , (4.9)
n n′ n n n
n̸=n′ n
where t is the strength of the exponentially decaying coupling in space, c (c†) is the annihilation
n n
(creation)operatoratsitenandthepotentialenergyV isgivenby
n
V (λ,ϕ)=λcos(2πnb+ϕ), (4.10)
n
28Figure4.6:Human-KANcollaborationtodiscovermobilityedgesofGAAMandMAAM.Thehumanusercan
choosetobelazy(usingtheautomode)ormoreinvolved(usingthemanualmode).Moredetailsintext.
Asbefore,tointroducequasiperiodicity,wesetbtobeirrational(thegoldenratio).Forthesemodels,
themobilityedgeisgivenbytheclosedformexpression[44],
λcosh(p)=E+t=E+t exp(p) (4.11)
1
wherewedefinet ≡texp(−p)asthenearestneighborhoppingstrength,andwesett =1below.
1 1
LetusassumeAlicewantstofigureoutthemobilityedgeforMAAM.Thistaskismorecomplicated
andrequiresmorehumanwisdom. Asinthelastexample,Alicestartsfroma[4,2,1,1]KANand
trainsitbutgetsanaccuracyaround75%whichislessthanacceptable. Shethenchoosesalarger
[4,3,1,1] KAN and successfully gets 98.4% which is acceptable (Figure 4.6 (b) step 1). Alice
noticesthatϕisnotpickeduponbyKANs,whichmeansthatthemobilityedgeisindependentof
thephasefactorϕ(agreeingwithEq.(4.11)). IfAliceturnsontheautomaticsymbolicregression
(usingalargelibraryconsistingofexp,tanhetc.),shewouldgetacomplicatedformulainTabel6-
MAAM-KANauto,whichhas97.1%accuracy. However,ifAlicewantstofindasimplersymbolic
formula, she will want to use the manual mode where she does the symbolic snapping by herself.
Beforethatshefindsthatthe[4,3,1,1]KANaftertrainingcanthenbeprunedtobe[4,2,1,1],while
maintaining 97.7% accuracy (Figure 4.6 (b)). Alice may think that all activation functions except
thosedependentonparelinearorquadraticandsnapthemtobeeitherlinearorquadraticmanually
byusingfix_symbolic.Aftersnappingandretraining,theupdatedKANisshowninFigure4.6(c)
step3,maintaining97.7%accuracy. Fromnowon,Alicemaymaketwodifferentchoicesbasedon
herpriorknowledge. Inonecase,Alicemayhaveguessedthatthedependenceonpiscosh,soshe
setstheactivationsofptobecoshfunction. SheretrainsKANandgets96.9%accuracy(Figure4.6
(c)Step4A).Inanothercase,Alicedoesnotknowthecoshpdependence,soshepursuessimplicity
29and again assumes the functions of p to be quadratic. She retrains KAN and gets 95.4% accuracy
(Figure4.6(c)Step4B).Ifshetriedboth,shewouldrealizethatcoshisbetterintermsofaccuracy,
whilequadraticisbetterintermsofsimplicity. Theformulascorrespondingtothesestepsarelisted
inTable6. ItisclearthatthemoremanualoperationsaredonebyAlice,thesimplerthesymbolic
formulais(whichslightsacrificeinaccuracy). KANshavea“knob"thatausercantunetotrade-off
between simplicity and accuracy (sometimes simplicity can even lead to better accuracy, as in the
GAAMcase).
5 Relatedworks
Kolmogorov-Arnold theorem and neural networks. The connection between the Kolmogorov-
Arnoldtheorem(KAT)andneuralnetworksisnotnewintheliterature [50,51,8,9,10,11,12,13],
but the pathological behavior of inner functions makes KAT appear unpromising in practice [50].
Most of these prior works stick to the original 2-layer width-(2n+1) networks, which were lim-
ited in expressive power and many of them are even predating back-propagation. Therefore, most
studieswerebuiltontheorieswithratherlimitedorartificialtoyexperiments.Ourcontributionlies
ingeneralizingthenetworktoarbitrarywidthsanddepths,revitalizingandcontexualizingthemin
today’sdeeplearningstream,aswellashighlightingitspotentialroleasafoundationmodelforAI
+Science.
Neural Scaling Laws (NSLs). NSLs are the phenomena where test losses behave as power laws
against model size, data, compute etc [52, 53, 54, 55, 17, 56, 57, 58]. The origin of NSLs still
remainsmysterious, butcompetitivetheoriesincludeintrinsicdimensionality[52], quantizationof
tasks [57], resource theory [58], random features [56], compositional sparsity [50], and maximu
arity [18]. This paper contributes to this space by showing that a high-dimensional function can
surprisingly scale as a 1D function (which is the best possible bound one can hope for) if it has
a smooth Kolmogorov-Arnold representation. Our paper brings fresh optimism to neural scaling
laws, since it promises the fastest scaling exponent ever. We have shown in our experiments that
this fast neural scaling law can be achieved on synthetic datasets, but future research is required
to address the question whether this fast scaling is achievable for more complicated tasks (e.g.,
language modeling): Do KA representations exist for general tasks? If so, does our training find
theserepresentationsinpractice?
MechanisticInterpretability(MI).MIisanemergingfieldthataimstomechanisticallyunderstand
theinnerworkingsofneuralnetworks[59,60,61,62,63,64,65,66,5]. MIresearchcanberoughly
dividedintopassiveandactiveMIresearch. MostMIresearchispassiveinfocusingonunderstand-
ingexistingneuralnetworkstrainedwithstandardmethods. ActiveMIresearchattemptstoachieve
interpretabilitybydesigningintrinsicallyinterpretablearchitecturesordevelopingtrainingmethods
to explicitly encourage interpretability [65, 66]. Our work lies in the second category, where the
modelandtrainingmethodarebydesigninterpretable.
Learnable activations. The idea of learnable activations in neural networks is not new in ma-
chine learning. Trainable activations functions are learned in a differentiable way [67, 13, 68, 69]
or searched in a discrete way [70]. Activation function are parametrized as polynomials [67],
splines [13, 71, 72], sigmoid linear unit [68], or neural networks [69]. KANs use B-splines to
parametrizetheiractivationfunctions. Wealsopresentourpreliminaryresultsonlearnableactiva-
tionnetworks(LANs),whosepropertiesliebetweenKANsandMLPsandtheirresultsaredeferred
toAppendixBtofocusonKANsinthemainpaper.
30SymbolicRegression. Therearemanyoff-the-shelfsymbolicregressionmethodsbasedongenetic
algorithms (Eureka [73], GPLearn [74], PySR [75]), neural-network based methods (EQL [76],
OccamNet[77]),physics-inspiredmethod(AIFeynman[20,21]),andreinforcementlearning-based
methods [78]. KANs are most similar to neural network-based methods, but differ from previous
worksinthatouractivationfunctionsarecontinuouslylearnedbeforesymbolicsnappingratherthan
manuallyfixed[73,77].
Physics-Informed Neural Networks (PINNs) and Physics-Informed Neural Operators
(PINOs). InSubsection 3.4, wedemonstratethat KANscanreplacethe paradigmofusing MLPs
for imposing PDE loss when solving PDEs. We refer to Deep Ritz Method [79], PINNs [22, 23]
forPDEsolving,andFourierNeuraloperator[80],PINOs[81,82,83],DeepONet[84]foroperator
learningmethodslearningthesolutionmap. ThereispotentialtoreplaceMLPswithKANsinall
theaforementionednetworks.
AIforMathematics.AswesawinSubsection4.3,AIhasrecentlybeenappliedtoseveralproblems
inKnottheory,includingdetectingwhetheraknotistheunknot[85,86]oraribbonknot[30],and
predictingknotinvariantsanduncoveringrelationsamongthem[87,88,89,29]. Forasummaryof
datascienceapplicationstodatasetsinmathematicsandtheoreticalphysicsseee.g.[90,91],andfor
ideashowtoobtainrigorousresultsfromMLtechniquesinthesefields,see[92].
6 Discussion
Inthissection, wediscussKANs’limitationsandfuturedirectionsfromtheperspectiveofmathe-
maticalfoundation,algorithmsandapplications.
Mathematicalaspects: Althoughwe havepresentedpreliminary mathematicalanalysisof KANs
(Theorem 2.1), our mathematical understanding of them is still very limited. The Kolmogorov-
Arnoldrepresentationtheoremhasbeenstudiedthoroughlyinmathematics,butthetheoremcorre-
spondstoKANswithshape[n,2n+1,1],whichisaveryrestrictedsubclassofKANs. Doesour
empirical success with deeper KANs imply something fundamental in mathematics? An appeal-
inggeneralizedKolmogorov-Arnoldtheoremcoulddefine“deeper”Kolmogorov-Arnoldrepresen-
tations beyond depth-2 compositions, and potentially relate smoothness of activation functions to
depth. Hypothetically, there exist functions which cannot be represented smoothly in the original
(depth-2)Kolmogorov-Arnoldrepresentations, butmightbesmoothlyrepresentedwithdepth-3or
beyond. Canweusethisnotionof“Kolmogorov-Arnolddepth”tocharacterizefunctionclasses?
Algorithmicaspects: Wediscussthefollowing:
(1) Accuracy. Multiple choices in architecture design and training are not fully investigated so
alternativescanpotentiallyfurtherimproveaccuracy. Forexample, splineactivationfunctions
mightbereplacedbyradialbasisfunctionsorotherlocalkernels. Adaptivegridstrategiescan
beused.
(2) Efficiency. One major reason why KANs run slowly is because different activation functions
cannot leverage batch computation (large data through the same function). Actually, one can
interpolate between activation functions being all the same (MLPs) and all different (KANs),
bygroupingactivationfunctionsintomultiplegroups(“multi-head”),wherememberswithina
groupsharethesameactivationfunction.
(3) HybridofKANsandMLPs. KANshavetwomajordifferencescomparedtoMLPs:
(i) activationfunctionsareonedgesinsteadofonnodes,
31(ii) activationfunctionsarelearnableinsteadoffixed.
WhichchangeismoreessentialtoexplainKAN’sadvantage?Wepresentourpreliminaryresults
in Appendix B where we study a model which has (ii), i.e., activation functions are learnable
(likeKANs),butnot(i),i.e.,activationfunctionsareonnodes(likeMLPs). Moreover,onecan
alsoconstructanothermodelwithfixedactivations(likeMLPs)butonedges(likeKANs).
(4) Adaptivity.Thankstotheintrinsiclocalityofsplinebasisfunctions,wecanintroduceadaptivity
in the design and training of KANs to enhance both accuracy and efficiency: see the idea of
multi-leveltraininglikemultigridmethodsasin[93,94],ordomain-dependentbasisfunctions
likemultiscalemethodsasin[95].
Applicationaspects: WehavepresentedsomepreliminaryevidencesthatKANsaremoreeffective
thanMLPsinscience-relatedtasks,e.g.,fittingphysicalequationsandPDEsolving. Weexpectthat
KANsmayalsobepromisingforsolvingNavier-Stokesequations,densityfunctionaltheory,orany
othertasksthatcanbeformulatedasregressionorPDEsolving. WewouldalsoliketoapplyKANs
tomachine-learning-relatedtasks,whichwouldrequireintegratingKANsintocurrentarchitectures,
e.g.,transformers–onemaypropose“kansformers”whichreplaceMLPsbyKANsintransformers.
KAN as a “language model” for AI + Science The reason why large language models are so
transformativeisbecausetheyareusefultoanyonewhocanspeaknaturallanguage. Thelanguage
ofscienceisfunctions. KANsarecomposedofinterpretablefunctions,sowhenahumanuserstares
at a KAN, it is like communicating with it using the language of functions. This paragraph aims
to promote the AI-Scientist-Collaboration paradigm rather than our specific tool KANs. Just like
peopleusedifferentlanguagestocommunicate,weexpectthatinthefutureKANswillbejustone
ofthelanguagesforAI+Science,althoughKANswillbeoneoftheveryfirstlanguagesthatwould
enableAIandhumantocommunicate. However,enabledbyKANs,theAI-Scientist-Collaboration
paradigmhasneverbeenthiseasyandconvenient, whichleadsustorethinktheparadigmofhow
wewanttoapproachAI+Science:DowewantAIscientists,ordowewantAIthathelpsscientists?
Theintrinsicdifficultyof(fullyautomated)AIscientistsisthatitishardtomakehumanpreferences
quantitative,whichwouldcodifyhumanpreferencesintoAIobjectives.Infact,scientistsindifferent
fieldsmayfeeldifferentlyaboutwhichfunctionsaresimpleorinterpretable. Asaresult,itismore
desirable for scientists to have an AI that can speak the scientific language (functions) and can
convenientlyinteractwithinductivebiasesofindividualscientist(s)toadapttoaspecificscientific
domain.
Finaltakeaway: ShouldIuseKANsorMLPs?
Currently, the biggest bottleneck of KANs lies in its slow training. KANs are usually 10x slower
than MLPs, given the same number of parameters. We should be honest that we did not try hard
to optimize KANs’ efficiency though, so we deem KANs’ slow training more as an engineering
problem to be improved in the future rather than a fundamental limitation. If one wants to train a
modelfast, oneshoulduseMLPs. Inothercases, however, KANsshouldbecomparableorbetter
thanMLPs,whichmakesthemworthtrying. ThedecisiontreeinFigure6.1canhelpdecidewhen
touseaKAN.Inshort,ifyoucareaboutinterpretabilityand/oraccuracy,andslowtrainingisnota
majorconcern,wesuggesttryingKANs.
Acknowledgement
We would like to thank Mikail Khona, Tomaso Poggio, Pingchuan Ma, Rui Wang, Di Luo, Sara
Beery, Catherine Liang and Matthieu Darcy for fruitful discussion and constructive suggestions.
Z.L., F.R., J.H., M.S. and M.T. are supported by IAIFI through NSF grant PHY-2019786. The
32Figure6.1:ShouldIuseKANsorMLPs?
work of FR is in addition supported by the NSF grant PHY-2210333 and by startup funding from
NortheasternUniversity. Y.WandT.HaresupportedbytheNSFGrantDMS-2205590andtheChoi
Family Gift Fund. S. V. and M. S. acknowledge support from the U.S. Office of Naval Research
(ONR) Multidisciplinary University Research Initiative (MURI) under Grant No. N00014-20-1-
2325onRobustPhotonicMaterialswithHigher-OrderTopologicalProtection.
References
[1] SimonHaykin. Neuralnetworks: acomprehensivefoundation. PrenticeHallPTR,1994.
[2] GeorgeCybenko. Approximationbysuperpositionsofasigmoidalfunction. Mathematicsof
control,signalsandsystems,2(4):303–314,1989.
[3] KurtHornik,MaxwellStinchcombe,andHalbertWhite. Multilayerfeedforwardnetworksare
universalapproximators. Neuralnetworks,2(5):359–366,1989.
[4] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin.Attentionisallyouneed.Advancesinneuralinformation
processingsystems,30,2017.
[5] Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. Sparse
autoencoders find highly interpretable features in language models. arXiv preprint
arXiv:2309.08600,2023.
[6] A.N.Kolmogorov. Ontherepresentationofcontinuousfunctionsofseveralvariablesassuper-
positionsofcontinuousfunctionsofasmallernumberofvariables. Dokl.Akad.Nauk,108(2),
1956.
[7] Jürgen Braun and Michael Griebel. On a constructive proof of kolmogorov’s superposition
theorem. Constructiveapproximation,30:653–675,2009.
[8] David A Sprecher and Sorin Draghici. Space-filling curves and kolmogorov superposition-
basedneuralnetworks. NeuralNetworks,15(1):57–67,2002.
[9] Mario Köppen. On the training of a kolmogorov network. In Artificial Neural Net-
works—ICANN2002: InternationalConferenceMadrid,Spain,August28–30,2002Proceed-
ings12,pages474–479.Springer,2002.
33[10] Ji-NanLinandRolfUnbehauen. Ontherealizationofakolmogorovnetwork. NeuralCom-
putation,5(1):18–20,1993.
[11] Ming-Jun Lai and Zhaiming Shen. The kolmogorov superposition theorem can break the
curse of dimensionality when approximating high dimensional functions. arXiv preprint
arXiv:2112.09963,2021.
[12] Pierre-EmmanuelLeni,YohanDFougerolle,andFrédéricTruchetet. Thekolmogorovspline
network for image processing. In Image Processing: Concepts, Methodologies, Tools, and
Applications,pages54–78.IGIGlobal,2013.
[13] DanieleFakhoury,EmanueleFakhoury,andHendrikSpeleers.Exsplinet:Aninterpretableand
expressivespline-basedneuralnetwork. NeuralNetworks,152:332–346,2022.
[14] Tomaso Poggio, Andrzej Banburski, and Qianli Liao. Theoretical issues in deep networks.
ProceedingsoftheNationalAcademyofSciences,117(48):30039–30045,2020.
[15] HenryWLin,MaxTegmark,andDavidRolnick. Whydoesdeepandcheaplearningworkso
well? JournalofStatisticalPhysics,168:1223–1247,2017.
[16] Hongyi Xu, Funshing Sin, Yufeng Zhu, and Jernej Barbicˇ. Nonlinear material design using
principalstretches. ACMTransactionsonGraphics(TOG),34(4):1–11,2015.
[17] UtkarshSharmaandJaredKaplan. Aneuralscalinglawfromthedimensionofthedatamani-
fold. arXivpreprintarXiv:2004.10802,2020.
[18] Eric J Michaud, Ziming Liu, and Max Tegmark. Precision machine learning. Entropy,
25(1):175,2023.
[19] CarlDeBoorandCarlDeBoor. Apracticalguidetosplines,volume27. springer-verlagNew
York,1978.
[20] Silviu-MarianUdrescuandMaxTegmark. Aifeynman: Aphysics-inspiredmethodforsym-
bolicregression. ScienceAdvances,6(16):eaay2631,2020.
[21] Silviu-Marian Udrescu, Andrew Tan, Jiahai Feng, Orisvaldo Neto, Tailin Wu, and Max
Tegmark. Ai feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity.
AdvancesinNeuralInformationProcessingSystems,33:4860–4871,2020.
[22] MaziarRaissi,ParisPerdikaris,andGeorgeEKarniadakis.Physics-informedneuralnetworks:
Adeeplearningframeworkforsolvingforwardandinverseproblemsinvolvingnonlinearpar-
tialdifferentialequations. JournalofComputationalphysics,378:686–707,2019.
[23] GeorgeEmKarniadakis,IoannisGKevrekidis,LuLu,ParisPerdikaris,SifanWang,andLiu
Yang. Physics-informedmachinelearning. NatureReviewsPhysics,3(6):422–440,2021.
[24] RonaldKemker,MarcMcClure,AngelinaAbitino,TylerHayes,andChristopherKanan.Mea-
suringcatastrophic forgettinginneural networks. InProceedingsof theAAAIconference on
artificialintelligence,volume32,2018.
[25] BryanKolbandIanQWhishaw. Brainplasticityandbehavior. Annualreviewofpsychology,
49(1):43–64,1998.
[26] DavidMeunier,RenaudLambiotte,andEdwardTBullmore.Modularandhierarchicallymod-
ularorganizationofbrainnetworks. Frontiersinneuroscience,4:7572,2010.
34[27] JamesKirkpatrick,RazvanPascanu,NeilRabinowitz,JoelVeness,GuillaumeDesjardins,An-
dreiARusu,KieranMilan,JohnQuan,TiagoRamalho,AgnieszkaGrabska-Barwinska,etal.
Overcomingcatastrophicforgettinginneuralnetworks. Proceedingsofthenationalacademy
ofsciences,114(13):3521–3526,2017.
[28] AojunLu,TaoFeng,HangjieYuan,XiaotianSong,andYananSun.Revisitingneuralnetworks
forcontinuallearning: Anarchitecturalperspective,2024.
[29] AlexDavies,PetarVelicˇkovic´,LarsBuesing,SamBlackwell,DanielZheng,NenadTomašev,
RichardTanburn, PeterBattaglia, CharlesBlundell, AndrásJuhász, etal. Advancingmathe-
maticsbyguidinghumanintuitionwithai. Nature,600(7887):70–74,2021.
[30] SergeiGukov,JamesHalverson,CiprianManolescu,andFabianRuehle.Searchingforribbons
withmachinelearning,2023.
[31] P.Petersen.RiemannianGeometry.GraduateTextsinMathematics.SpringerNewYork,2006.
[32] Philip W Anderson. Absence of diffusion in certain random lattices. Physical review,
109(5):1492,1958.
[33] David J Thouless. A relation between the density of states and range of localization for one
dimensionalrandomsystems. JournalofPhysicsC:SolidStatePhysics,5(1):77,1972.
[34] Elihu Abrahams, PW Anderson, DC Licciardello, and TV Ramakrishnan. Scaling theory
of localization: Absence of quantum diffusion in two dimensions. Physical Review Letters,
42(10):673,1979.
[35] AdLagendijk,BartvanTiggelen,andDiederikSWiersma. Fiftyyearsofandersonlocaliza-
tion. Physicstoday,62(8):24–29,2009.
[36] MordechaiSegev,YaronSilberberg,andDemetriosNChristodoulides. Andersonlocalization
oflight. NaturePhotonics,7(3):197–204,2013.
[37] Z Valy Vardeny, Ajay Nahata, and Amit Agrawal. Optics of photonic quasicrystals. Nature
photonics,7(3):177–187,2013.
[38] SajeevJohn.Stronglocalizationofphotonsincertaindisordereddielectricsuperlattices.Phys-
icalreviewletters,58(23):2486,1987.
[39] Yoav Lahini, Rami Pugatch, Francesca Pozzi, Marc Sorel, Roberto Morandotti, Nir David-
son, andYaronSilberberg. Observationofalocalizationtransitioninquasiperiodicphotonic
lattices. Physicalreviewletters,103(1):013901,2009.
[40] Sachin Vaidya, Christina Jörg, Kyle Linn, Megan Goh, and Mikael CRechtsman. Reentrant
delocalizationtransitioninone-dimensionalphotonicquasicrystals.PhysicalReviewResearch,
5(3):033170,2023.
[41] Wojciech De Roeck, Francois Huveneers, Markus Müller, and Mauro Schiulaz. Absence of
many-bodymobilityedges. PhysicalReviewB,93(1):014203,2016.
[42] Xiaopeng Li, Sriram Ganeshan, JH Pixley, and S Das Sarma. Many-body localization and
quantumnonergodicityinamodelwithasingle-particlemobilityedge.Physicalreviewletters,
115(18):186601,2015.
[43] FangzhaoAlexAn,KarmelaPadavic´,EricJMeier,SurajHegde,SriramGaneshan,JHPixley,
Smitha Vishveshwara, and Bryce Gadway. Interactions and mobility edges: Observing the
generalizedaubry-andrémodel. Physicalreviewletters,126(4):040603,2021.
35[44] J Biddle and S Das Sarma. Predicted mobility edges in one-dimensional incommensurate
optical lattices: An exactly solvable model of anderson localization. Physical review letters,
104(7):070601,2010.
[45] AlexanderDuthie,SthitadhiRoy,andDavidELogan.Self-consistenttheoryofmobilityedges
inquasiperiodicchains. PhysicalReviewB,103(6):L060201,2021.
[46] SriramGaneshan,JHPixley,andSDasSarma. Nearestneighbortightbindingmodelswithan
exactmobilityedgeinonedimension. Physicalreviewletters,114(14):146601,2015.
[47] Yucheng Wang, Xu Xia, Long Zhang, Hepeng Yao, Shu Chen, Jiangong You, Qi Zhou, and
Xiong-JunLiu.One-dimensionalquasiperiodicmosaiclatticewithexactmobilityedges.Phys-
icalReviewLetters,125(19):196604,2020.
[48] Yucheng Wang, Xu Xia, Yongjian Wang, Zuohuan Zheng, and Xiong-Jun Liu. Duality be-
tween two generalized aubry-andré models with exact mobility edges. Physical Review B,
103(17):174205,2021.
[49] Xin-Chi Zhou, Yongjian Wang, Ting-Fung Jeffrey Poon, Qi Zhou, and Xiong-Jun Liu. Ex-
act new mobility edges between critical and localized states. Physical Review Letters,
131(17):176401,2023.
[50] Tomaso Poggio. How deep sparse networks avoid the curse of dimensionality: Efficiently
computablefunctionsarecompositionallysparse. CBMMMemo,10:2022,2022.
[51] JohannesSchmidt-Hieber. Thekolmogorov–arnoldrepresentationtheoremrevisited. Neural
networks,137:119–126,2021.
[52] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon
Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural
languagemodels. arXivpreprintarXiv:2001.08361,2020.
[53] TomHenighan,JaredKaplan,MorKatz,MarkChen,ChristopherHesse,JacobJackson,Hee-
wooJun,TomBBrown,PrafullaDhariwal,ScottGray,etal. Scalinglawsforautoregressive
generativemodeling. arXivpreprintarXiv:2010.14701,2020.
[54] MitchellAGordon,KevinDuh,andJaredKaplan. Dataandparameterscalinglawsforneural
machinetranslation. InACLRollingReview-May2021,2021.
[55] JoelHestness,SharanNarang,NewshaArdalani,GregoryDiamos,HeewooJun,HassanKia-
ninejad, Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is
predictable,empirically. arXivpreprintarXiv:1712.00409,2017.
[56] Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining
neuralscalinglaws. arXivpreprintarXiv:2102.06701,2021.
[57] EricJMichaud,ZimingLiu,UzayGirit,andMaxTegmark. Thequantizationmodelofneural
scaling. InThirty-seventhConferenceonNeuralInformationProcessingSystems,2023.
[58] JinyeopSong,ZimingLiu,MaxTegmark,andJeffGore. Aresourcemodelforneuralscaling
law. arXivpreprintarXiv:2402.05164,2024.
[59] Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom
Henighan, Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, et al. In-context learning
andinductionheads. arXivpreprintarXiv:2209.11895,2022.
36[60] KevinMeng,DavidBau,AlexAndonian,andYonatanBelinkov. Locatingandeditingfactual
associations in gpt. Advances in Neural Information Processing Systems, 35:17359–17372,
2022.
[61] KevinRoWang,AlexandreVariengien,ArthurConmy,BuckShlegeris,andJacobSteinhardt.
Interpretabilityinthewild: acircuitforindirectobjectidentificationinGPT-2small. InThe
EleventhInternationalConferenceonLearningRepresentations,2023.
[62] Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna
Kravec,ZacHatfield-Dodds,RobertLasenby,DawnDrain,CarolChen,etal. Toymodelsof
superposition. arXivpreprintarXiv:2209.10652,2022.
[63] NeelNanda,LawrenceChan,TomLieberum,JessSmith,andJacobSteinhardt. Progressmea-
suresforgrokkingviamechanisticinterpretability. InTheEleventhInternationalConference
onLearningRepresentations,2023.
[64] ZiqianZhong,ZimingLiu,MaxTegmark,andJacobAndreas. Theclockandthepizza: Two
storiesinmechanisticexplanationofneuralnetworks. InThirty-seventhConferenceonNeural
InformationProcessingSystems,2023.
[65] ZimingLiu,EricGan,andMaxTegmark.Seeingisbelieving:Brain-inspiredmodulartraining
formechanisticinterpretability. Entropy,26(1):41,2023.
[66] NelsonElhage,TristanHume,CatherineOlsson,NeelNanda,TomHenighan,ScottJohnston,
Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda
Askell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli,
Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav
Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom
Brown,SamMcCandlish,DarioAmodei,andChristopherOlah. Softmaxlinearunits. Trans-
formerCircuitsThread,2022. https://transformer-circuits.pub/2022/solu/index.html.
[67] MohitGoyal,RajanGoyal,andBrejeshLall. Learningactivationfunctions: Anewparadigm
forunderstandingneuralnetworks. arXivpreprintarXiv:1906.09529,2019.
[68] PrajitRamachandran,BarretZoph,andQuocVLe. Searchingforactivationfunctions. arXiv
preprintarXiv:1710.05941,2017.
[69] Shijun Zhang, Zuowei Shen, and Haizhao Yang. Neural network architecture beyond width
anddepth. AdvancesinNeuralInformationProcessingSystems,35:5669–5681,2022.
[70] GarrettBinghamandRistoMiikkulainen.Discoveringparametricactivationfunctions.Neural
Networks,148:48–65,2022.
[71] Pakshal Bohra, Joaquim Campos, Harshit Gupta, Shayan Aziznejad, and Michael Unser.
Learning activation functions in deep (spline) neural networks. IEEE Open Journal of Sig-
nalProcessing,1:295–309,2020.
[72] Shayan Aziznejad and Michael Unser. Deep spline networks with control of lipschitz regu-
larity. InICASSP2019-2019IEEEInternationalConferenceonAcoustics,SpeechandSignal
Processing(ICASSP),pages3242–3246.IEEE,2019.
[73] RenátaDubcáková. Eureqa: softwarereview. GeneticProgrammingandEvolvableMachines,
12:173–178,2011.
[74] Gplearn. https://github.com/trevorstephens/gplearn. Accessed: 2024-04-19.
37[75] MilesCranmer. Interpretablemachinelearningforsciencewithpysrandsymbolicregression.
jl. arXivpreprintarXiv:2305.01582,2023.
[76] GeorgMartiusandChristophHLampert.Extrapolationandlearningequations.arXivpreprint
arXiv:1610.02995,2016.
[77] OwenDugan,RumenDangovski,AllanCosta,SamuelKim,PawanGoyal,JosephJacobson,
and Marin Soljacˇic´. Occamnet: A fast neural model for symbolic regression at scale. arXiv
preprintarXiv:2007.10784,2020.
[78] TerrellN.Mundhenk,MikelLandajuela,RubenGlatt,ClaudioP.Santiago,Danielfaissol,and
BrendenK.Petersen. Symbolicregressionviadeepreinforcementlearningenhancedgenetic
programming seeding. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan,
editors,AdvancesinNeuralInformationProcessingSystems,2021.
[79] Bing Yu etal. The deep ritzmethod: a deep learning-basednumerical algorithm for solving
variationalproblems. CommunicationsinMathematicsandStatistics,6(1):1–12,2018.
[80] ZongyiLi,NikolaKovachki,KamyarAzizzadenesheli,BurigedeLiu,KaushikBhattacharya,
AndrewStuart,andAnimaAnandkumar. Fourierneuraloperatorforparametricpartialdiffer-
entialequations. arXivpreprintarXiv:2010.08895,2020.
[81] ZongyiLi,HongkaiZheng,NikolaKovachki,DavidJin,HaoxuanChen,BurigedeLiu,Kam-
yarAzizzadenesheli,andAnimaAnandkumar. Physics-informedneuraloperatorforlearning
partialdifferentialequations. ACM/JMSJournalofDataScience,2021.
[82] NikolaKovachki,ZongyiLi,BurigedeLiu,KamyarAzizzadenesheli,KaushikBhattacharya,
Andrew Stuart, and Anima Anandkumar. Neural operator: Learning maps between function
spaceswithapplicationstopdes. JournalofMachineLearningResearch,24(89):1–97,2023.
[83] HaydnMaust,ZongyiLi,YixuanWang,DanielLeibovici,OscarBruno,ThomasHou,andAn-
imaAnandkumar. Fouriercontinuationforexactderivativecomputationinphysics-informed
neuraloperators. arXivpreprintarXiv:2211.15960,2022.
[84] LuLu,PengzhanJin,GuofeiPang,ZhongqiangZhang,andGeorgeEmKarniadakis.Learning
nonlinearoperatorsviadeeponetbasedontheuniversalapproximationtheoremofoperators.
Naturemachineintelligence,3(3):218–229,2021.
[85] Sergei Gukov, James Halverson, Fabian Ruehle, and Piotr Sułkowski. Learning to Unknot.
Mach.Learn.Sci.Tech.,2(2):025035,2021.
[86] L.H.Kauffman,N.E.Russkikh,andI.A.Taimanov. Rectangularknotdiagramsclassification
withdeeplearning,2020.
[87] Mark C Hughes. A neural network approach to predicting and computing knot invariants.
JournalofKnotTheoryandItsRamifications,29(03):2050005,2020.
[88] JessicaCraven,VishnuJejjala,andArjunKar. Disentanglingadeeplearnedvolumeformula.
JHEP,06:040,2021.
[89] Jessica Craven, Mark Hughes, Vishnu Jejjala, and Arjun Kar. Illuminating new and known
relationsbetweenknotinvariants. 112022.
[90] FabianRuehle. Datascienceapplicationstostringtheory. Phys.Rept.,839:1–117,2020.
[91] Y.H. He. Machine Learning in Pure Mathematics and Theoretical Physics. G - Refer-
ence,InformationandInterdisciplinarySubjectsSeries.WorldScientific,2023.
38[92] SergeiGukov,JamesHalverson,andFabianRuehle. Rigorwithmachinelearningfromfield
theorytothepoincaréconjecture. NatureReviewsPhysics,2024.
[93] Shumao Zhang, Pengchuan Zhang, and Thomas Y Hou. Multiscale invertible generative
networks for high-dimensional bayesian inference. In International Conference on Machine
Learning,pages12632–12641.PMLR,2021.
[94] JinchaoXuandLudmilZikatanov.Algebraicmultigridmethods.ActaNumerica,26:591–721,
2017.
[95] Yifan Chen, Thomas Y Hou, and Yixuan Wang. Exponentially convergent multiscale finite
element method. Communications on Applied Mathematics and Computation, pages 1–17,
2023.
[96] VincentSitzmann,JulienMartel,AlexanderBergman,DavidLindell,andGordonWetzstein.
Implicit neural representations with periodic activation functions. Advances in neural infor-
mationprocessingsystems,33:7462–7473,2020.
39Functionality Descriptions
model.train(dataset) trainingmodelondataset
model.plot() plotting
model.prune() pruning
fixtheactivationfunctionϕ
model.fix_symbolic(l,i,j,fun) l,i,j
tobethesymbolicfunctionfun
suggestsymbolicfunctionsthatmatch
model.suggest_symbolic(l,i,j)
thenumericalvalueofϕ
l,i,j
usetop1symbolicsuggestionsfromsuggest_symbolic
model.auto_symbolic()
toreplaceallactivationfunctions
model.symbolic_formula() returnthesymbolicformula
Table7:KANfunctionalities
Appendix
A KANFunctionalities
Table7includescommonfunctionalitiesthatusersmayfinduseful.
B Learnableactivationnetworks(LANs)
B.1 Architecture
Besides KAN, we also proposed another type of learnable activation networks (LAN), which are
almostMLPsbutwithlearnableactivationfunctionsparametrizedassplines. KANshavetwomain
changes to standard MLPs: (1) the activation functions become learnable rather than being fixed;
(2)theactivationfunctionsareplacedonedgesratherthannodes. Todisentanglethesetwofactors,
wealsoproposelearnableactivationnetworks(LAN)whichonlyhaslearnableactivationsbutstill
onnodes,illustratedinFigureB.1.
ForaLANwithwidthN,depthL,andgridpointnumberG,thenumberofparametersisN2L+
NLG where N2L is the number of parameters for weight matrices and NLG is the number of
parameters for spline activations, which causes little overhead in addition to MLP since usually
G ≪ N so NLG ≪ N2L. LANs are similar to MLPs so they can be initialized from pretrained
MLPs and fine-tuned by allowing learnable activation functions. An example is to use LAN to
improveSIREN,presentedinSection B.3.
ComparisonofLANandKAN.ProsofLANs:
(1) LANsareconceptuallysimplerthanKANs. TheyareclosertostandardMLPs(theonlychange
isthatactivationfunctionsbecomelearnable).
(2) LANsscalebetterthanKANs.LANs/KANshavelearnableactivationfunctionsonnodes/edges,
respectively. SoactivationparametersinLANs/KANsscaleasN/N2,whereN ismodelwidth.
ConsofLANs:
(1) LANsseemtobelessinterpretable(weightmatricesarehardtointerpret,justlikeinMLPs);
(2) LANsalsoseemtobelessaccuratethanKANs,butstillmoreaccuratethanMLPs. LikeKANs,
LANsalsoadmitgridextensioniftheLANs’activationfunctionsareparametrizedbysplines.
40FigureB.1:Trainingofalearnableactivationnetwork(LAN)onthetoyexamplef(x,y)=exp(sin(πx)+y2).
FigureB.2:LANsonsyntheticexamples.LANsdonotappeartobeveryinterpretable.Weconjecturethatthe
weightmatricesleavetoomanydegreeoffreedoms.
B.2 LANinterpretabilityresults
We present preliminary interpretabilty results of LANs in Figure B.2. With the same examples in
Figure4.1forwhichKANsareperfectlyinterpretable, LANsseemmuchlessinterpretabledueto
the existence of weight matrices. First, weight matrices are less readily interpretable than learn-
ableactivationfunctions. Second, weightmatricesbringintoomanydegreesoffreedom, making
learnableactivationfunctionstoounconstrained. OurpreliminaryresultswithLANsseemtoimply
that getting rid of linear weight matrices (by having learnable activations on edges, like KANs) is
necessaryforinterpretability.
B.3 FittingImages(LAN)
Implicit neural representations view images as 2D functions f(x,y), where the pixel value f is a
function of two coordinates of the pixel x and y. To compress an image, such an implicit neu-
ralrepresentation(f isaneuralnetwork)canachieveimpressivecompressionofparameterswhile
maintaining almost original image quality. SIREN [96] proposed to use MLPs with periodic acti-
vationfunctionstofitthefunctionf. Itisnaturaltoconsiderotheractivationfunctions,whichare
41FigureB.3: ASIRENnetwork(fixedsineactivations)canbeadaptedtoLANs(learnableactivations)toim-
proveimagerepresentations.
allowedinLANs. However,sinceweinitializeLANactivationstobesmoothbutSIRENrequires
high-frequency features, LAN does not work immediately. Note that each activation function in
LANsisasumofthebasefunctionandthesplinefunction,i.e.,ϕ(x) = b(x)+spline(x),weset
b(x) to sine functions, the same setup as in SIREN but let spline(x) be trainable. For both MLP
and LAN, the shape is [2,128,128,128,128,128,1]. We train them with the Adam optimizer, batch
size4096,for5000stepswithlearningrate10−3and5000stepswithlearningrate10−4. Asshown
inFigureB.3, theLAN(orange)canachievehigherPSNRthantheMLP(blue)duetotheLAN’s
flexibilitytofinetuneactivationfunctions. WeshowthatitisalsopossibletoinitializeaLANfrom
an MLP and further fine tune the LAN (green) for better PSNR. We have chosen G = 5 in our
experiments,sotheadditionalparameterincreaseisroughlyG/N =5/128≈4%overtheoriginal
parameters.
C Dependenceonhyperparameters
Weshowtheeffectsofhyperparamtersonthef(x,y)=exp(sin(πx)+y2)caseinFigureC.1. To
getaninterpretablegraph,wewantthenumberofactiveactivationfunctionstobeassmall(ideally
3)aspossible.
(1) Weneedentropypenaltytoreducethenumberofactiveactivationfunctions. Withoutentropy
penalty,therearemanyduplicatefunctions.
(2) Results can depend on random seeds. With some unlucky seed, the pruned network could be
largerthanneeded.
(3) Theoverallpenaltystrengthλeffectivelycontrolsthesparsity.
(4) The grid number G also has a subtle effect on interpretability. When G is too small, because
eachoneofactivationfunctionisnotveryexpressive,thenetworktendstousetheensembling
strategy,makinginterpretationharder.
(5) The piecewise polynomial order k only has a subtle effect on interpretability. However, it be-
havesabitliketherandomseedswhichdonotdisplayanyvisiblepatterninthistoyexample.
D FeynmanKANs
Weincludemore resultsontheFeynman dataset (Section3.3). FigureD.1showsthepareto fron-
tiersofKANsandMLPsforeachFeynmandataset. FigureD.3andD.2visualizeminimalKANs
42FigureC.1:Effectsofhyperparametersoninterpretabilityresults.
(undertheconstrainttestRMSE<10−2)andbestKANs(withthelowesttestRMSEloss)foreach
Feynmanequationfittingtask.
E Remarkongridsize
ForbothPDEandregressiontasks,whenwechoosethetrainingdataonuniformgrids,wewitness
asuddenincreaseintrainingloss(i.e.,suddendropinperformance)whenthegridsizeisupdatedto
alargelevel,comparabletothedifferenttrainingpointsinonespatialdirection. Thiscouldbedue
toimplementationofB-splineinhigherdimensionsandneedsfurtherinvestigation.
F KANsforspecialfunctions
Weincludemoreresultsonthespecialfunctiondataset(Section3.2). FigureF.2andF.1visualize
minimal KANs (under the constraint test RMSE < 10−2) and best KANs (with the lowest test
RMSEloss)foreachspecialfunctionfittingtask.
43101 I.6.2 I.6.2b I.9.18 I.12.11
KAN train
103 KAN test
ReLU MLP train
ReLU MLP test
105 Tanh MLP train
Tanh MLP test
107 S Si iL LU U M ML LP P t tr ea si tn
101 I.13.12 I.15.3x I.16.6 I.18.4
103
105
107
101 I.26.2 I.27.6 I.29.16 I.30.3
103
105
107
101 I.30.5 I.37.4 I.40.1 I.44.4
103
105
107
101 I.50.26 II.2.42 II.6.15a II.11.7
103
105
107
101 II.11.27 II.35.18 II.36.38 II.38.3
103
105
107
101 III.9.52 III.10.19 III.17.37 101 102III.17.13037 104
number of parameters
103
105
107
101 102 103 104 101 102 103 104 101 102 103 104 101 102 103 104
number of parameters number of parameters number of parameters number of parameters
FigureD.1:TheParetoFrontiersofKANsandMLPsforFeynmandatasets.
44
ESMR
ESMR
ESMR
ESMR
ESMR
ESMR
ESMRFigureD.2:BestFeynmanKANs
45FigureD.3:MinimalFeynmanKANs
46FigureF.1:BestspecialKANs
47FigureF.2:MinimalspecialKANs
48