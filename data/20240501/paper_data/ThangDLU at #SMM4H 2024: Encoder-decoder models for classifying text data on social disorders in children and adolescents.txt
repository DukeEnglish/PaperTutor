ThangDLU at #SMM4H 2024: Encoder-decoder models for classifying text
data on social disorders in children and adolescents
Hoang-ThangTa AbuBakarSiddiqurRahman
DalatUniversity UniversityofNebraskaatOmaha
thangth@dlu.edu.vn abubakarsiddiqurra@unomaha.edu
LotfollahNajjar AlexanderGelbukh
UniversityofNebraskaatOmaha InstitutoPolitécnicoNacional(IPN),Mexico
lnajjar@unomaha.edu gelbukh@cic.ipn.mx
Abstract passed. Asatextclassificationproblem,themost
advancedandpopularmethodsofsocialdisorder
ThispaperdescribesourparticipationinTask identificationusedeeplearningnetworkssuchas
3andTask5ofthe#SMM4H(SocialMedia
BERT(Devlinetal.,2018),RoBERTa(Liuetal.,
MiningforHealth)2024Workshop,explicitly
2019),Transformermodels(Vaswanietal.,2017),
targeting the classification challenges within
andtheirvariants.
tweetdata. Task3isamulti-classclassification
taskcenteredontweetsdiscussingtheimpact In participating in the #SMM4H 2024 work-
ofoutdoorenvironmentsonsymptomsofsocial shop(Xuetal.,2024),weapplytransferlearning
anxiety. Task5involvesabinaryclassification fromtwopre-trainedmodels(BART-base(Lewis
taskfocusingontweetsreportingmedicaldis- et al., 2019) and T5-small (Raffel et al., 2020)),
ordersinchildren. Weappliedtransferlearning
whichfollowthearchitectureofTransformerand
frompre-trainedencoder-decodermodelssuch
sequence-to-sequence. Additionally,weexploited
asBART-baseandT5-smalltoidentifythela-
two data augmentation methods — (1) false in-
belsofasetofgiventweets. Wealsopresented
ferreddataand(2)paraphraseddataextractedfrom
somedataaugmentationmethodstoseetheir
impactonthemodelperformance. Finally,the ChatGPT—tosupplementthetrainingsetandsee
systemsobtainedthebestF1scoreof0.627in theirimpactonmodelperformance. Becausethe
Task3andthebestF1scoreof0.841inTask5. organizershidethefinalrankingtable,wecanonly
presentourresultscomparedtothemeanandme-
1 Introduction
dian values by metrics (F1, precision, recall, and
accuracy)theyprovided.
Social disorders are significantly influencing a
large proportion of young people globally. So-
2 Methodology
cialanxietydisorder(SAD)typicallyemergesdur-
ing early adolescence and is characterized by ex- After conducting several initial experiments on
cessive anxiety in social situations (Rao et al., trainingthedatasetwithandwithoutpreprocessing
2007). Althoughspendingtimeoutdoorsingreen steps,weobservedthattrainingwithoutanyprepro-
or blue environments has been shown to allevi- cessingyieldedbetterperformance. Itisassumed
ate symptoms of various anxiety disorders, lim- thateverytokenwithinthedatacanpositivelyim-
ited research has explored its impact specifically pactthemodel’sperformance. Therefore,wefed
on SAD. Meanwhile, numerous children receive raw data directly into the model in the training
diagnoses of conditions that can significantly af- process.
fect their daily functioning and persist into adult- We applied transfer learning from two pre-
hood. The commonly diagnosed childhood dis- trained models based on the architecture of the
ordersareattention-deficit/hyperactivitydisorder Transformer and sequence-to-sequence: BART-
(ADHD)(Kidd,2000),autismspectrumdisorders base (Vaswani et al., 2017) and T5-small (Raffel
(ASD) (Matson et al., 2009), speech delay, and et al., 2020) for Task 3 and Task 5, respectively.
asthma. They are encoder-decoder language models, pro-
Datasets related to these disorders are usually ducingoutputsthatcanbeunknownlabels. From
extractedfromtweetsoruserpostsonsocialplat- agiventext,themodelsmustdetectitslabel,and
formssuchasTwitterandReddit. Usersdescribe anyout-of-scopelabelwillbesetautomaticallyto
their disorders daily and receive feedback from a default label, which we choose as "0" for both
the community or comment on what they have tasks. Thebestmodelsweresavedbasedontheir
4202
rpA
03
]LC.sc[
1v41791.4042:viXraperformanceonthevalidationsetbyF1-macrofor over3differenttrainingdatasets.
Task3andF1-microforTask5.
• Training: The model was trained over the
Two data augmentation methods complement
originaltrainingsetofferedbytheorganizer.
newdatatoimprovemodeperformance. First,the
falseinferreddatainthevalidationsetwereutilized
• Training + Paraphrased: Weextractedthe
when inferring them in a trained model with the
paraphraseddatabasedonthevalidationset
defaulttrainingdata. Second,paraphraseddataby
byChatGPT.Then,weaddedthisnewdatato
ChatGPTistakenfromthetrainingandvalidation
thetrainingset.
setsataninsignificantcost.
• Training + Validation: We added a vali-
3 Tasks&Datasets
dationsettothetrainingsetandthenusedthis
newsetfortrainingthemodel.
3.1 Task3
This task involves classifying Reddit posts men- Thetraininghasthesameparametersforallmod-
tioningpredeterminedkeywordsrelatedtooutdoor els,includingepochs = 10,batch_size = 4,and
spaces into one of four categories: ("1") positive max_source_length = 768. For any input with
effect,("2")neutral/noeffect,("3")negativeeffect, alengthover768tokens,weprocessittotakeits
and("4")unrelated1. Thedatasetcomprises3,000 first256tokensanditslast512tokens.
annotated posts from the r/socialanxiety sub- Table 1 shows the results of our team and the
reddit,filteredforusersaged12-25,andkeywords meanandmedianperformanceofallteamsbymet-
related to green or blue spaces. 80% of the data rics: F1,precision,recall,andaccuracy. Itisclear
will be used for training/validation, and 20% for our models outperformed the mean and median
evaluation. Evaluationwillbebasedonthemacro- overall metrics. Our best model was trained on
averagedF1-scoreacrossallcategories. Datawill theTraining + Validationdataandobtainedan
beprovidedinCSVformatwithfields: post_id, F1valueof0.627,whilethemodelwithTraining
keyword, text, and label. The distribution of + Paraphrased data takes slightly lower perfor-
subsetsfollowsaratio6:2:2,inwhichtraining,val- mance. Whileparaphraseddatahelpsimprovethe
idation, and testing sets take 1800, 600, and 600 model, it is better to collect actual data to obtain
postscorrespondingly. However,theorganizerpro- thebestperformance. ThelowF1valueindicates
vided a test set with 1200 posts to hide the real thetask’sdifficultyandtheneedforaddingmore
ones. trainingdatatoimprovethemodelperformance.
3.2 Task5 4.2 Task5
Thistaskinvolvesautomaticallyclassifyingtweets The task limits each team to only 2 submissions.
from users who reported pregnancy on Twitter. Therefore,wepick2trainedT5-smallmodelson
It distinguishes tweets reporting children with twotrainingsetsforparticipation. Inthepost-eval
ADHD, autism, delayed speech, or asthma ("1") phase, we also trained the other 2 models with
from those merely mentioning a disorder ("0")2. differenttrainingsets. Finally,wehave4models
The goal is to enable large-scale epidemiologic with4trainingsets,whichare:
studies and explore parents’ experiences for tar-
• Training: The model was trained over the
getedsupportinterventions. Thedatasetincludes
originaltrainingsetofferedbytheorganizer.
7398 training tweets, 389 validation tweets, and
1947testtweets. LikeTask3,theorganizergavea
• Training + Validation: We added a vali-
newtestwith10000tweetstohidetheactualdata.
dationsettothetrainingsetandthenusedthis
newsetfortrainingthemodel.
4 Experiments
• Training + False inferred: First,weused
4.1 Task3
themodeltrainedontheoriginaltrainingset
Theorganizerlimitedeachteamto3submissions.
to infer the labels of inputs in the validation
Therefore, weusedBART-basetotrain3models
set. Then,wecollectfalseinferredtextswith
1https://codalab.lisn.upsaclay.fr/competitions/18305 theirlabels(41examples)andaddthemtothe
2https://codalab.lisn.upsaclay.fr/competitions/17310 originaltrainingsettoformanewone.#Submission Data F1 Precision Recall Accuracy
1 Training 0.595 0.589 0.615 0.631
2 Training+Paraphrased 0.601 0.592 0.622 0.640
3 Training+Validation 0.627 0.620 0.644 0.670
Comparedtootherteams
- Mean 0.518 0.564 0.537 0.574
- Median 0.579 0.630 0.588 0.627
Table1: F1-macro,Precison-macro,andRecall-macrovaluesofBART-basemodelsonTask3,whichweretrained
overdifferentdatacombinations.
#Submission Data F1 Precision Recall
1 Training+Falseinferred+Paraphrased 0.841 0.844 0.839
2 Training+Falseinferred 0.829 0.803 0.856
3* Training+Validation 0.870 0.869 0.867
4* Training 0.820 0.809 0.831
Comparedtootherteams
- Mean 0.822 0.818 0.838
- Median 0.901 0.885 0.917
Otherworks
- RoBERTa-Large(Kleinetal.,2024) 0.930 - -
*Ourextraparticipationinthepost-evalphase.
Table2: ThemetricsofT5-smallmodelsonTask5,whichweretrainedoverdifferentdatacombinations.
• Training + False inferred + itivelytothemodelperformanceeventhoughwith
Paraphrased: Similar to Training + a subset. Unfortunately, we can not experiment
False inferred,weaddmoreparaphrased with training on more paraphrased data based on
data to the training set. First, we used thefullvalidationandthetrainingsets,butweex-
BM25(Robertsonetal.,1995)totakesimilar pectthemodelwillbemuchbetter. Ourbestmodel
textsinthetrainingsetbasedonthevalidation wastrainedontheTraining + Validationwith
set. Not that the new set’s size equals the anF1valueof0.87,indicatingthatthemoredata,
validationset’ssize(389examples). Then,we thebettermodelperformance.
usedChatGPTAPIs3 toextractparagraphed
5 Conclusion
textsandaddthemtothetrainingset.
Thispaperintroducedourapproach,utilizingpre-
Thetraininghasthesameparametersforallmod-
trainedencoder-decodermodelswithtwodataaug-
els,includingepochs = 20,batch_size = 4,and
mentationmethodstoaddressTask3andTask5of
max_source_length = 128. Table 2 shows the
the#SMM4H2024workshop. Ourfindingsunder-
resultsofourteamandthemeanandmedianper-
score the advantages of encoder-decoder models
formance of all teams by metrics: F1, precision,
in text classification problems when they offer a
andrecall. Allourmodelshavemetricvaluesthat
strong baseline performance. Furthermore, it is
arebetterthanthemeanbutlowerthanthemedian.
beneficialtoexploitdataaugmentationmethodsto
Especially,ourmetricvaluesaresignificantlylower
enhancethemodel’sperformancebycomplement-
thanthebenchmarkF1 (Kleinetal.,2024)when
ingparaphrasedtextsfromChatGPT.Accordingto
using RoBERTa-Large. It can be explained that
the organizers, weofficially achieved the highest
we only use a small-scale pre-trained model like
F1 score of 0.627 for Task 3 and 0.841 for Task
T5-smallfortheclassificationtask.
5. In the future, we will investigate further how
Duetothesmallsizeoffalseinferreddata,the
largelanguagemodels’outputslikeChatGPTcan
model performance is not much better. However,
positivelyimpactdownstreamclassificationtasks’
werealizethattheparaphraseddatacontributespos-
performance.
3https://platform.openai.com/docs/overviewReferences Rodriguez-Esteban,JuanBanda,IvanFloresAmaro,
Sai Tharuni Samineni, and Graciela Gonzalez-
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Hernandez.2024. Overviewofthe9thsocialmedia
KristinaToutanova.2018. Bert: Pre-trainingofdeep
mining for health applications (#SMM4H) shared
bidirectionaltransformersforlanguageunderstand-
tasks at ACL 2024. In Proceedings of the 9th So-
ing. arXivpreprintarXiv:1810.04805.
cialMediaMiningforHealthApplicationsWorkshop
andSharedTask,Bangkok,Thailand.Associationfor
ParrisMKidd.2000. Attentiondeficit/hyperactivitydis-
ComputationalLinguistics.
order(adhd)inchildren: rationaleforitsintegrative
management. Alternativemedicinereview,5(5):402–
428.
Ari Z Klein, José Agustín Gutiérrez Gómez, Lisa D
Levine,andGracielaGonzalez-Hernandez.2024. Us-
inglongitudinaltwitterdatafordigitalepidemiology
ofchildhoodhealthoutcomes: anannotateddataset
anddeepneuralnetworkclassifiers. JournalofMedi-
calInternetResearch,26:e50652.
Mike Lewis, Yinhan Liu, Naman Goyal, Marjan
Ghazvininejad,AbdelrahmanMohamed,OmerLevy,
VesStoyanov,andLukeZettlemoyer.2019. Bart:De-
noisingsequence-to-sequencepre-trainingfornatural
languagegeneration,translation,andcomprehension.
arXivpreprintarXiv:1910.13461.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Luke Zettlemoyer, and Veselin Stoyanov. 2019.
Roberta: A robustly optimized bert pretraining ap-
proach. arXivpreprintarXiv:1907.11692.
MichaelLMatson,SaraMahan,andJohnnyLMatson.
2009. Parenttraining: Areviewofmethodsforchil-
dren with autism spectrum disorders. Research in
AutismSpectrumDisorders,3(4):868–875.
ColinRaffel,NoamShazeer,AdamRoberts,Katherine
Lee,SharanNarang,MichaelMatena,YanqiZhou,
Wei Li, and Peter J Liu. 2020. Exploring the lim-
its of transfer learning with a unified text-to-text
transformer. Journalofmachinelearningresearch,
21(140):1–67.
PatriciaARao,DeborahCBeidel,SamuelMTurner,
RobertTAmmerman,LoriECrosby,andFloydR
Sallee.2007. Socialanxietydisorderinchildhood
andadolescence: Descriptivepsychopathology. Be-
haviourResearchandTherapy,45(6):1181–1191.
Stephen E Robertson, Steve Walker, Susan Jones,
MichelineMHancock-Beaulieu,MikeGatford,etal.
1995. Okapiattrec-3. NistSpecialPublicationSp,
109:109.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser,andIlliaPolosukhin.2017. Attentionisall
youneed. Advancesinneuralinformationprocessing
systems,30.
DongfangXu, GuillermoLopezGarcia, LisaRaithel,
Rolland Roller, Philippe Thomas, Eiji Aramaki,
Shuntaro Yada, Pierre Zweigenbaum, Karen
O’Connor, Yao Ge, Sudeshna Das, Abeed Sarker,
Ari Klein, Lucia Schmidt, Vishakha Sharma, Raul