Harmonic LLMs are Trustworthy
NicholasS. Kersting Mohammad Rahman
VisaAIPlatform VisaAIPlatform
nkerstin@visa.com moharahm@visa.com
Suchismitha Vedala Yang Wang
VisaPaymentProductDevelopment VisaDataSciencePlatform
svedala@visa.com yangwang@visa.com
Abstract
We introduce an intuitive method to test the robustness (stability and explainability) of any black-box LLM
inreal-time,baseduponthelocaldeviationfromharmoniticity,denotedasγ. Tothebestofourknowledge
thisisthefirstcompletelymodel-agnosticandunsupervisedmethodofmeasuringtherobustnessofanygiven
response from an LLM, based upon the model itself conforming to a purely mathematical standard. We
conducthumanannotationexperimentstoshowthepositivecorrelationofγwithfalseormisleadinganswers,
anddemonstratethatfollowingthegradientofγinstochasticgradientascentefficientlyexposesadversarial
prompts. MeasuringγacrossthousandsofqueriesinpopularLLMs(GPT-4,ChatGPT,Claude-2.1,Mixtral-
8x7B, Smaug-72B, Llama2-7B, and MPT-7B) allows us to estimate the liklihood of wrong or hallucinatory
answers automatically and quantitatively rank the reliability of these models in various objective domains
(WebQA,TruthfulQA,andProgrammingQA).Acrossallmodelsanddomainstested,humanratingsconfirm
thatγ → 0indicatestrustworthiness,andthelow-γ leadersamongthesemodelsareGPT-4,ChatGPT,and
Smaug-72B.
1 MotivationandIntroduction
Large Language Models (LLMs) are notoriously confident in their answers[1], which poses a dilemma for
theuseraskingfactual-typequestions: howcanItrustit? Withhallucinationarealandcommonphenomena
across the LLM landscape, there is to date no reliable method to gauge the trustworthiness of a response
prima facae without having to conduct independent research or cross-checking. What is missing in every
user’sLLMexperienceisa‘trustworthinessscore’thattellsthemwhethertheresponsecanbeacceptedas-is.
Thegoal,ofcourse,istohaveanLLMwhichgetsmostlyhightrustworthinessscoressotheuserhaslessof
aburdentodofollow-upverification.
WehavefoundsuchatrustworthinessscoreispossiblethroughthemethodofHarmonicRobustness,intro-
duced in a previous work [2] to measure robustness of predictive machine learning models. In that work,
wedemonstratedthatcomputingtheanharmoniticity(γ)inanyblack-boxmodelwascorrelatedwithrobust-
ness (stability and explainability) of the model in feature space. The current work extends the application
of Harmonic Robustness to LLMs, which, as next-token prediction engines, are formally no different from
predictive machine learning models, but which possess several distinguishing characteristics that warrant
thisseparate, dedicatedtreatment. Thereadermayofcourseconsult[2]forthefulltreatmentofHarmonic
Robustness,butwewillbrieflysummarizebelowforthepresentdiscussion.
The essence of Harmonic Robustness is to measure how close a machine learning function f is to being
harmonic,i.e.,satisfying
∇2f =0 (1.1)
4202
rpA
03
]GL.sc[
1v80791.4042:viXrawiththegradienttakenoverallfeaturedimensions. Crucially,itturnsoutthisconditionisequivalenttothe
statement
1 (cid:90)
f(x)= fdV (1.2)
Vrn
B(x,r)
which,inplainEnglish,saysthatthevalueofthefunctionatanypointistheaverageofthefunctionoverany
ballsurroundingthepoint1. Wedefinethedegreetowhichthis‘meanvalueproperty’(1.2)issatisfiedata
pointxininputspaceas
1 (cid:90)
γ(r,x)≡|f(x)− fdV| (1.3)
Vrn
B(x,r)
whichallowsustosummarizehow‘harmonic’f isacrossfeaturespacegivenafixedballradiusr. Thisis
interestingbecausethemeanvaluepropertyisactuallyaverystrongstatementofstabilityandexplainability:
harmonicfunctionssmoothlyinterpolatebetweenboundaryvaluesandexhibitnounstableoscillations; be-
causethevalueatanypointistheaverageofvaluesaroundthatpoint,thereisacertainmaximal‘fairness’at
playwhichlendsitselfeasilytoexplainability. Applying(1.3)toclassicalMLmodelsallowsonetomeasure
failureofrobustnessinfeaturespace,identifyingpointswhereoneshouldnot"trust"theoutputofthemodel.
Thereareseveralcaveats,noneoftheminsurmountable,toapplying(1.3)toanLLMforthesamepurposeof
identifying"trustworthiness"ofoutputs. Firstly,anLLMwithanon-zerotemperatureparameterisn’tstrictly
amathematicalfunction,asthesameinputmayproduceseveraloutputsinastochasticfashion,soanyfurther
discussion must either reduce temperature fluctuations to a benign level, or eliminate them altogether. We
choose the latter route in the following, focusing on T=0. Even so, an LLM in practice maps strings to
strings,andwenowhavetointerpret(1.3)inthatlanguage:whatistheballB(x,r)ofradiusraboutastring
x ? How do we compute the difference of two strings or take the average, etc.? If we are able to directly
accesstheinputlayeroftheLLM,thenwemightpasssimplicialperturbationsofthetokenembeddingsto
form the ball; the outputs would in general have different numbers of tokens, however, so direct averaging
or comparison would not be possible. We can get around this by using a separate semantic embedding to
convert each of the outputs to a single embedding vector, then, just as in our previous work [2] with high
dimensional functions such as image classifiers, we simply average over the perturbed output embeddings
→−
to directly subtract from the original output embedding, giving an n-dimensional γ — unlike an image
classifier, however, where each output dimension represents a very interpretible class logit, the individual
dimensionsofanaturallanguageembeddingdonotcarryanyspecialmeaning,sowemighttakeeitherthe
→−
magnitude of γ or, better, the angle between the original output embedding and average perturbed output
embeddingasouranharmoniticitymetric,foranglesbetweenembeddingsdoencodesemanticcloseness.
If we cannot directly access the input embeddings of the LLM, which is generally the case for, e.g., com-
mercialblack-boxmodels,weareindeedrestrictedtostringinputs,andwehavetosomehowgeneratemany
perturbationsoftheinputstringthateffectivelyamounttosimplicialperturbationsintokenembeddingspace.
Thisisinfacttheassumptionofthepresentwork,andwewillseethereisatricktoaccomplishthis.
Thetrickistoaddrandomcharacterstotheinputstring–wewillseethisdoeseffectivelyperturbtheinput
embeddingroughlyisotropicallyaslongastheserandomcharactersdonotaddrealsemanticmeaningtothe
inputstring. Onewaytoguaranteetherandomcharactersarenon-semanticistoensuretheyneveroccurin
any meaningful way in an LLM’s actual training data. For this purpose, we will choose the non-semantic
and non-printing ASCII control characters (ASCII 0-31), which typically do not occur with any semantic
correlationsinthewebtextandcorporathatknownLLMsaretrainedon2.
Intuitively, then, γ will be measuring the stability of the LLM output to random non-printing character ad-
ditions to the input. It is reasonable to assume that if there is significant variation of the output, the LLM
is unstable in the region of its input, and one cannot trust the response. On the other hand, if the output
is consistent, giving γ = 0, we can at least be assured that the LLM response is stable and in line with its
trainingdata. Totheextentwebelievethetrainingdatarepresentstrustworthyworldknowledge,statistically
speaking,wecantrustthisresponse.
Note that we are measuring sensitivity to the harmonic condition, and not just any directional sensitivity,
which, for some use cases may be naturally large. A simple arithmetic problem of the form “What is 5 +
1Aneasyinformalwaytoseethisequivalenceistorecognize(1.1)asthedivergenceofthegradient(∇2f =▽·▽f):
thegradientexpressingthechangeoff inalldirections,ifitsdivergenceiszerothen‘changeinf’neitherflowsintoor
outofanygivenpoint,hencetheaveragechangeoff onanyballaroundthatpointis0,relativetoitsvalueatthepoint.
2Onecouldalso,forexample,chooserandomexoticUTF-8characters,butthesemightcarryunintentionalsemantics.
Itturnsout,however,thatthisisaneffectivemethodoffindingadversarialexamples(seeSection5).
2k * x" can be made as sensitive as you like to perturbation in x by increasing the value of k. That is what
wewouldcallexplainableor“naturalsensitivity". Aviolationoftheharmoniccondition,however,isnotso
explainable,anditisourhypothesisthatthiskindof“unnaturalsensitivity"asparameterizedbyγ pointsto
hallucination.
Inthefollowing,then,wehopetodemonstratethesimplicityandeffectivenessofγasaproxyformeasuring
trustworthinessinLLMs. AfterreviewingRelatedWorkinSection2,wedescribetheadaptionofHarmonic
Robustness to LLMs in more technical detail in Section 3, and demonstrate in Section 4 how γ does in
fact correlate with trustworthiness in human annotation experiments across several popular LLMs (GPT-
4, ChatGPT, Claude-2.1, Smaug-72B, Mixtral-8x7B, Llama2-7B, and MPT-7B) in three domains (Internet
trivia,commonsensequestions,andprogrammingtasks).Section5demonstratesthatstochasticallyfollowing
thegradientofγ,likein[2],affordsaneffectiveadversarialexamplegenerator. Wewrapupdiscussionand
offerfuturedirectionsinSection6.
2 RelatedWork
LLM evaluation has emerged as a critical area of research, focusing on assessing model performance and
robustness to input variations, including resilience to noise and adversarial prompts. This is essential for
real-worldapplicationswhereinputcanbeunpredictable.TheFLASKmethodhasbeeninstrumentalineval-
uating LLMs based on the consistency of their scores across stylistic variations of inputs, highlighting the
significanceofalignmentskillsetsforfine-grainedmodelevaluation[3]. Meanwhile,vulnerabilitiesidenti-
fied in model-graded evaluations have raised concerns about their susceptibility to manipulative injections,
questioning the reliability of such assessments[4]. Instruction-tuned LLMs have faced challenges in main-
tainingperformanceacrossrephrasedinstructions,leadingtothedevelopmentofmethodsaimedatbolstering
theirzero-shotrobustness[5]. ThePromptBenchframeworkintroducesasystematicbenchmarkforassessing
LLMs’resiliencetoadversarialprompts,underscoringtheimportanceofunderstandingmodelresponsesto
input perturbations[6]. Furthermore, insights from evaluations conducted by Eleuther AI have contributed
toadeeperunderstandingofLLMperformanceandrobustness,highlightingtheneedformodelscapableof
reliablyinterpretingawiderangeofinputvariations[7].
Building upon these foundational studies, recent research has delved into the addition of noise to input
promptsasameansofevaluatingLLMrobustness. Aunifiedrobustnessevaluationframeworkfornoisyslot
fillingtaskshasbeenproposedtosystematicallyassessLLMperformanceinreal-worldnoisyscenarios[8].
Moreover,advancementsindifferentiallyprivatepromptlearninghavefocusedoncreatingnoisyensembles
ofprivatediscreteprompts,showcasingnovelprivacy-preservingpromptlearningtechniques[9]. Thecritical
roleofpromptselectioninensuringaccuratemodelresponseshasbeenemphasized,withstudieshighlighting
theimpactofnoisyannotationsonmodelperformanceandtheimportanceofcarefulpromptselectiontomiti-
gatetheseeffects[10].Addingtothisbodyofwork,[11]addressesthevulnerabilitiesofLLMstowardsnoisy
inputs,especiallyinhigh-stakesenvironmentswherethestabilityofpredictionsisparamount. Thatresearch
underscores the necessity of ensuring that LLM predictions remain consistent despite minor differences in
theinput,aligningwithourpursuitofcertifiedrobustLLMs,andproposesaself-denoisingapproachtoclean
corruptedinputs,differingfrompriormethodsthatrequiretrainingaseparatemodelforrobustification.
StudiesonmeasuringanLLM’sconfidencehavebeenlikewisegainingtractionrecently,e.g. withblack-box
methods leveraging prompting, sampling (via temperature), and aggregation [1]. Using Chain of Thought
or similar LLM-powered abilities to reflect on the consistency of its results [12][13][14] are all emerging
methodstomeasureconfidence.
Research focusing on robustness of an LLM to input perturbations, centrally related to our technique, has
existedintheNLPliteratureforseveralyears[15][16][17]andsuggestthatsomeformofextremesensitivity
toperturbationswillalwaysexist.
3 HarmonicRobustnessforLLMs
AswediscussedintheIntroduction,ouradaptionof(1.3)toanLLMistoformtheballoftheinputin‘word
space’,sendeachofthoseballinputsthroughtheLLMtogiveoutputs,andtaketheangle(orrather,sineof
theangle)betweentheaverageballoutputembeddingandtheoriginal,unperturbedoutputembedding—see
Algorithm1below.
3Algorithm1Computationofγ forinputtoLLM
1: procedureγ(inputText,N)
2: outputText←LLM(inputText)
3: outputVector←Embedding(outputText)
4: ballVector←0
5: foreach point in ballPoints(inputText,N)do
6: output′ ←LLM(point)
7: ballVector+= Embedding(output′)
8: endfor
9: returnsin(outputVector,ballVector)
10: endprocedure
As stated, Algorithm 1 permits some freedom of implementation. Given a target LLM, assumed to be
black-box, onestillneedstochoosetheEmbeddingforconvertingtextualoutputstovectors; probablyany
reasonably-trainedembeddingwillsufficehere,thoughitisgoodpracticetousethebestavailable.Wechoose
theindustrystandard"ada-002,"forexample.Next,onemustdecidehowtoimplementthe"ballPoints"func-
tionalapproximationtotheballofstringssurroundingtheinputtext. Ouransatzisthatappendingrandom
ASCII controlcharacters tothe endof the inputstring providesa sufficientball approximation. Wetested,
forexample, thatgeneratingN=10perturbationsoftheinputstringbyappending1-3randomcharactersin
theASCIIrange0-31providedadequateisotropyinembeddingspace,acorerequirementoftheHarmonic
Robustnesstechnique3. Wecouldhavebutdidnotneedtoimproveonthiswithdifferentnumbersofmore
systematically-chosenrandomcharactersandgeneratingmoreperturbations(i.e. higherN)aswell.
Query GPT-4Output
Orig: "Whatis2+2?" "4"
P1: <Orig>+"\x05" "4"
P2: <Orig>+"\x02\x06" "4"
P3: <Orig>+"\x04" "4"
P4: <Orig>+"\x0c\x1f" "4"
P5: <Orig>+"\x01" "4"
P6: <Orig>+"\x1e" "4"
P7: <Orig>+"\x1d\x1c\x1d" "4"
P8: <Orig>+"\x0e\x0b" "4"
P9: <Orig>+"\x11" "4"
P10: <Orig>+"\x05\x13" "4"
Table 1 Perturbing the "Orig" input query N=10 times with random ASCII 0-31 chars. We "show" these
non-printing characters with their hexadecimal representations. This example shows a perfectly stable and
trustworthyanswerfromGPT-4withγ =0.
TestingseveralqueriesinGPT-4withthisperturbationscheme,forexample,weseeinTable1anabsolutely
stable output which leads to γ = 0, because GPT-4 is quite correctly unvarying in its answer to “What is
2+2?" despite the random control character perturbations4. On the other hand, Table 2 shows a query with
3To demonstrate this, we considered “words" consisting of 1-3 random ASCII control characters appended to 100
randomsentencesfromtheTruthfulQA[18]dataset. Foreachinputsentence,theaverage“ada-002"embeddingvector
ofN=10suchrandomlyperturbedoutputsentenceswaswithinaconeanglelessthan11degrees.
4NOTE:allexamplesweshowfromcommercialLLMoutputsareasofMarch2024—currentanswersmaydiffer.
4onlyaslightgrammaticalvariationintheoutput,leadingtoaverysmallbutnon-zerovalueofγ.Thisanswer
isstilltrustworthy. Asamoreinterestingexample,inTable3weseesignificantvariationintheanswertoa
mathproblem—alltheanswersareincidentallywrong(themathproblemisinfactindeterminate)—giving
amoresignificantvalueofγ ≈0.15(rememberthisisasineofanangle,sotheaverageoutputdiffersfrom
the original output by arcsin(0.15) ≈ 8o). Evidently such large values of γ signal we have left the realm
oftrustworthiness. ButthenwehavetheexampleinTable4whereγ islargerstill,giving,however,outputs
whichareallreasonablevariantsofthetruth.So,incontrasttoourpreviouswork[2]fornumericalpredictive
machinelearningwherelargeγalwayssignifiedinstability,andhenceincorrectness,inthedecisionfunction,
hereweareintherealmofNaturalLanguageProcessingwheretwopiecesoftextmayhaveratherdifferent
embeddingswhilebothhavingrelatedandcorrectcontent.
Thus, while it would appear to make sense that stable answers with γ ≈ 0 are trustworthy, we need to be
carefulinprojectingthistoimplythatsignificantvariationinoutputwithlargeγsignifieslackoftrustworthi-
ness. Tobringclaritytotheissue,wewillnextempiricallymeasurethecorrelationofγ withtrustworthiness
acrossseveralpopularmodelsandfamiliardomains.
Query GPT-4Output
Orig: "whatreligionismaryqueenofscots?" "MaryQueenofScotswasaRomanCatholic."
P1: <Orig>+"\x0e\x00\x03" "MaryQueenofScotswasRomanCatholic."
P2: <Orig>+"\x1c "MaryQueenofScotswasRomanCatholic."
P3: <Orig>+"\x07\x03\x05" "MaryQueenofScotswasRomanCatholic."
P4: <Orig>+"\x07\x12\x04" "MaryQueenofScotswasRomanCatholic."
P5: <Orig>+"\x10" "MaryQueenofScotswasaRomanCatholic."
P6: <Orig>+"\x10\x0a" "MaryQueenofScotswasRomanCatholic."
P7: <Orig>+"\x03\x0d\x13" "MaryQueenofScotswasRomanCatholic."
P8: <Orig>+"\x17\x0\x1a" "MaryQueenofScotswasRomanCatholic."
P9: <Orig>+"\x11" "MaryQueenofScotswasRomanCatholic."
P10: <Orig>+"\x11\x19" "MaryQueenofScotswasRomanCatholic."
Table2 Nowwegetatinyvariationintheoutputresponse,givinganonzeroγ =0.044. Theanswerisof
coursestilltrustworthy.
4 PerformanceonStandardQuestionSets
Togetasenseofhowwellourγcorrelateswithtrustworthiness,wewillmeasureoutputsacrossfivepopular
leading LLMs: GPT-4[19], ChatGPT[20], Claude 2.1[21], Mixtral-8x7B[22], and Smaug-72B[23], as well
aswhatarenowtwoolder,smallermodels,Llama2-7B[24]andMPT-7B[25]. Notethisisnotmeanttobea
comprehensivesurveyofpopularLLMs,butratherjustademonstrationofwhatonecandowithγ.
Inaddition,wewillconsiderthreedifferentQA-corpora:
• WebQA[26]: acorpusofpop-cultureandworldtriviascrapedfromtheInternet
• TruthfulQA[18]: acorpusofcommon-sensetypequestions
• ProgrammingQA[27]: acollectionofsimpleinterview-typecodingquestions
Again,wedonotconsiderallpossibletypesofquestions,butthesamplingabovewillgiveaflavorofnatural
domainvariance.
Wefirstsample100questionsfromeachmodelappliedtoeachcorpusandaskateamofunbiasedannotators
toevaluatethetruthfulnessandrelevanceoftheanswersfromtheLLMs,accordingtoa5-pointscaledefined
5Query GPT-4Output
Orig: "Solveforx=1/(7*1/(7*1/(7*...)))" "... x=sqrt(1/7)"
P1: <Orig>+"\x05" "... x=(1+sqrt(197))/14"
P2: <Orig>+"\x02\x06" "... x=1/7"
P3: <Orig>+"\x04" "... x=sqrt(1/7)"
P4: <Orig>+"\x0c\x1f" "... x=1/6"
P5: <Orig>+"\x01" "... x=sqrt(8/7)"
P6: <Orig>+"\x1e" "... x=sqrt(1/7)andx=-sqrt(1/7)"
P7: <Orig>+"\x1d\x1c\x1d" "... x≈1.1478"
P8: <Orig>+"\x0e\x0b" "... x=1/6"
P9: <Orig>+"\x11" "... x=1/6"
P10: <Orig>+"\x05\x13" "... x=1/6"
Table 3 Giving GPT-4 a tricky math problem (the answer is in fact indeterminate). This time we get
significantvariationintheresponse(ellipsesrepresentelidedmathematicalcontent),yieldingγ =0.151.
for QA-type questions (Table 5) and coding questions (Table 6). With the exception of the TruthfulQA
corpus, where ground-truth labels were dependable, we asked annotators to verify answers themselves for
trustworthiness,asWebQAandProgrammaingQAground-truthanswerswereparticularlynoisy.
Noteourrubricchangessomewhatforcoding-typequestions. Codeisnottrivia-QA—thereisfarlessroom
forvariationinresponse,andasingletypocancausethewholeprogramtofail. Ananswershouldaddress
all the conditions of the problem, but a generic punt is OK: not answering or saying "I don’t know" is an
honest,trustworthyresponse. Ifhowevertheanswercontainscode,itmustactuallyrunandmatchtheoutput
given;ifitisconceptualorpseudocode,itmustlookreasonable. Wethusalsodidnotholdannotatorstothe
standard of the ground-truth answers for this corpus, as these were actually generated by CodeLlama-70B
andfullofcopiouserrors.
Composite annotation results are shown in Fig. 1 and Table 8. We compute Fleiss’ Kappa at about 0.6 for
all three domains, with annotators typically differing by much less than a point on any given answer, all
indicating consistent inter-annotator agreement. The reader may consult the Appendix for examples of an-
notationsfromeachmodelandcategory. Theaverageratingforthelargermodels(GPT-4,ChatGPT,Claude
2.1,Smaug-72B,andMixtral-8x7B)wasgenerallyhigh(>4),indicatingthesemodelsperformacceptablyon
the chosen domains. The smaller Llama2-7B and MPT-7B models, however, were of much lower quality,
and had performance issues (like hanging, or generating pages of output) dealing with many queries in the
TruthfulQA and Programming QA corpora, hence we only analyzed them for the relatively simplistic Web
QA.
Thatsaid,twoclearpatternsemergeintheannotationscoresacrossallmodelsanddomains: first,thelowest
γ-scoringresponses(below,say0.05)aregenerallyquitetrustworthy,scoring4orhigherinourrubrics. This
is our key result and claim, that harmonic (γ → 0) LLMs are trustworthy. Secondly, quality more-or-less
falls with increasing γ, though the extent to which this is true depends on model and domain somewhat.
Highγ cansometimesbeduetoapropensityofthemodeltorephraseanswersunstably, thoughtruthfully,
asweshowedearlier. Claude,forexample,consistentlyshowedmuchvariationinresponse(i.e.,higherγ),
althoughthequalitywasnottypicallyworsetoshowforit.
Among the five larger models we see some mixed results summarized in Table 8: though GPT-4 clearly
leadsthepackinoverallquality,itdoesnotalwayswinthe“certifiedtrustworthiness"medalasdetermined
bythepercentageofanswerswithlowγ (<0.05)—Smaug-72BinfactbeatsitoutintheWebQAcorpus,
showing that bigger/commercial is not always better than smaller/open-source, and underlying the need to
6Query GPT-4Output
Orig: "whatismartincooperdoingnow" "Asofmyknowledge,... information."
P1: <Orig>+"\x01" "MartinCooperiscurrently... world."
P2: <Orig>+"\x09" "MartinCooperiscurrently... technology."
P3: <Orig>+"\x0e" "MartinCooperiscurrently... world."
P4: <Orig>+"\x13\x0b\x08" "MartinCooperiscurrently... technology."
P5: <Orig>+"\x1b\x0b" "MartinCooperiscurrently... events."
P6: <Orig>+"\x0c" "MartinCooperiscurrently... experiences."
P7: <Orig>+"\x04\x1c\x1c\x1d" "Asofmyknowledge,... sources."
P8: <Orig>+"\x12\x01\x04" "Asofmyknowledge,... sources.
P9: <Orig>+"\x18\x01\x1c" "Asofmyknowledge,... information."
P10: <Orig>+"\x19" "MartinCooperiscurrently... events."
Table 4 Large γ doesn’t always mean hallucination: here each answer is legitimate (full content elided)
withsomewhatdifferentinformationalcontent,givingγ =0.268.
Score Description
5 Correctlyanswers;noirrelevantcontent
4 Mostlycorrectlyanswerswithminorerrororirrelevantcontent
3 Partiallycorrectlyanswerswithasignificanterrororirrelevantcontent
2 Partiallycorrectlyanswerswithseveralmajorerrorsorirrelevantcon-
tent
1 Atmostonepieceofreliableinfo,otherwiseuseless
0 Totallyfalseorirrelevant
Table5 ScoringrubricforQA-typequestions
doaquantitativecheckwithatechniquelikeharmonicrobustness. Intermsofweightedrankingsacrossthe
3 domains, by quality (as measured by average annotator rating R) the top-3 are GPT-4, Claude-2.1, and
Smaug-72B;whilebycertifiedtrustworthinesswehaveagainGPT-4,ChatGPT,andSmaug-72B.
Seeinghowγ correlatedwithquality,weconstructeddistributionsofγ foreachmodelacrossamuchlarger
sampleofeachofthethreecorpora(1200WebQA,1000ProgrammingQA,andall800+TruthfulQA),with
results shown in Fig. 3. Not surprisingly, the larger models such as GPT-4 and ChatGPT have much lower
γ thanthesmallermodels(Llama2-7BandMPT-7B),inparticulartheformerexhibitingastriking"peakof
certainty" near γ = 0 where we expect answers to be highly trustworthy. Other models are shifted out to
higher γ, but as we know from the annotation experiments this is not to say they are untrustworthy per se.
Claude,forexample,isjustnaturallyveryverboseinitsresponses,exhibitingsomeinstabilitytherebutnot
onthecorrectnessside. Also,thisisnottosaythatGPT-4andChatGPTdon’thallucinate,ofcourse. Indeed,
lookingatsomehighγresponsesfromthosemodelsdoshowtheexpectedlackofcorrectness/certainty,e.g.,
Table 7.
Alsonotsurprisingly,thesmallerandolder7Bmodelsperformonadifferentlevel,whichweplotseparately
in Fig. 4 – not only is the γ distribution shifted out to much larger values, but we know from annotation
experiments that these larger γ do strongly correlate with worse quality. The positive take on this is that γ
7Score Description
5 Code runs and satisfies requirements; or pseudocode/design makes
sense
4 Code has a small bug but otherwise would run and satisfies require-
ments;orpseudocode/designhassomeirrelevantcontent
3 Code doesn’t run with a significant error; pseudocode/design has sig-
nificantirrelevantcontent
2 Codeorpseudocode/designhasseveralsignificanterrors
1 Codeorpseudocode/designhasatmostonecorrectelement
0 Codeistotallyworthlessorpseudocode/designwrong
Table6 Scoringrubricforcoding-typequestions
Figure1 HumantrustworthinessratingsforlargerLLMsacrossdomains,binnedbyγ. Toprowisaverage
acrossallmodels,includingMPT-7BandLlama2-7BforWebQAquestions. Averageratingsareshownin
green. Thereisageneralnegativecorrelationofγ withrating,asshownbythenegative"m"parametersin
linearfits. Inparticular,γ →0limits(shownasthe"b"intercepts)havehigh>4ratings.
definitivelyshowstheprogressinLLMtechnologyoverthelastyear,withMPT-7BgivingwaytoLLama2-
7Bandnowthelarger,morerecentmodels.
Notetheseγ distributionsemphasizetheunsupervisedpowerofthetechnique,asoneonlyneedsquestions
withoutground-truthanswers,allowingonetogaugeLLMtrustworthinessinanonline,automatedmetric.
5 AdversarialExampleDiscovery
Finally,aperipheral,butinterestingresultofthisworkisademonstrationthattheγmetricprovidesawayto
efficientlyfindadversarialprompts. Asweshowedbeforein[2]withimageclassification,wecanfollowthe
gradientofγ tofindadversarialexampleswhichtendtooccurwithhigherfrequencyalongthisgradient.
8Figure 2 Quality was much lower for the smaller models Llama2-7B and MPT-7B, with larger γ more
stronglyindicatinghallucination. Still,lowγ <0.1wasgenerallyinthetrustworthyrange∼4.
Figure3 Distributionofγformodelsanddomains.Aslowerγcorrelateswithquality,ChatGPTandGPT4
performmoststrongly.
Startingwithaparticle-physicsprompt(seeTable9)trollingformisinformationaboutthe“9quarks"ofthe
Standard Model of Particle Physics (there are, in fact, only 6), for example, we compute γ not by using
ASCIIcontrol-characters(which wouldtendtokeep γ nearits initial value), butbyadding randomUTF-8
charactersbecausethesedocarrysomerandomsemanticweight. Afterthefirstiterationwealreadygetsome
falsecontent,andaftertwoiterationswe’reseeingfull-blownhallucination.
As another example, starting with a playful request for a cookie recipe containing Xenon to GPT-4, the
initialanswerisapolitedecline(seeTable10)—completelylegitimatetoerronthesideofsafety,though
exposing some of the LLM’s ignorance about noble elements. Noting that γ is already rather high, after a
fewiterationsofaddingrandomUTF-8charactersalongthedirectionofincreasingγ,wedoeventuallyget
GPT-4toacquiescearecipecontainingXenon(andalotofothernonsense).
9(a) (b)
Figure4 Distributionofγ for(a)Llama2-7Band(b)MPT-7B.Theshifttohighervaluesofγ relativeto
othermodelsareonparwithclearlylower-qualityanswers.
GPT-4Prompt Response γ
whatmoviedidangelinajoliedirect? AngelinaJoliehasdirectedseveralmovies,including: 1. 0.257
"IntheLandofBloodandHoney"(2011)2. "Unbroken"
(2014)3. "BytheSea"(2015)4. "FirstTheyKilledMy
Father"(2017)5. "TheOneandOnlyIvan"(2020)
Table7 Lookingatahigh-γhallucinationfromGPT-4:AngelinaJoliedidnotinfactdirectthehighlighted.
6 DiscussionandConclusions
WeappliedagenerictechniquebasedonharmonicfunctionstotestthestabilityofanswersfromLLMs. This
is of great interest lately as these latter enjoy far wider adoption than probably any other machine-learning
modelinthepast,yetthestability/hallucinationproblemisamajorblockinfactualusageofLLMs,whether
in a typical QA-type application, for coding suggestions, or summarization (however, for purely creative
applicationsofLLMs,e.g.,writingstories,poetry,andthelike,hallucinationisactuallyafeature).
Definingtheanharmoniticity(γ)forLLMsasthesineoftheanglebetweenaverageembeddingswassome-
whatarbitrary;otherdefinitionscanandshouldbetriedforevenbettercorrelationwithtrustworthiness. One
could, for example, use a smaller auxiliary LLM or even just BERT to compare semantics of the original
outputversusthesurrounding‘ball’outputs.
The choice of random ASCII control characters was also for simplicity — as noted there are other ways
to perturb the input text, e.g., with rarely-used UTF8 characters or taking misspellings, synonyms or para-
phrases. Whatever the perturbation scheme, one should verify that there is at least approximate isotropic
perturbationinembeddingspace.
WealsochoseN=10perturbationsofeachinputasamiddle-groundbetweenwhatwouldbeamoreaccurate
representationoftheballwithsay,N=100perturbations(suitableforofflineanalysis),andtheminimalchoice
ofN=1, whichisprobablyaffordableinareal-timeinferencesetting. Thislattermayberathereffectivein
quicklyandcheaplyidentifyinglargehallucinations,forifγ withN=100istrulylargeforagivenquery,the
probabilitythatonerandomperturbationoftheinputgivesadifferentanswerwithnearlythesameγisquite
high.
Regardlessoftheapproximationsweemployed,becausethemethodisunsupervisedandeasytoimplement,
itcanreadilyenjoyusageasaqualitymetricto,e.g.continuouslyscanforLLMweaknessinvariousdomains,
soγ-thresholdchecksmaytriggeranautomaticretrainingprocessfortheLLMtoimproveuponanareain
which the training data provides ambiguous results. Model builders can publish a γ-distribution similar to
those in Fig. 3 across various domains as part of their model card, and with white-box access to the layers
10GPT-4 ChatGPT Smaug-72B Mixtral-8x7B Claude-2.1 Llama2-7B MPT-7B
<γ> 0.072(2) 0.072(2) 0.073(2) 0.094(2) 0.103(1) 0.120(2) 0.164(3)
R 4.41(8) 4.31(9) 4.21(9) 4.10(8) 4.12(9) 3.41(12) 2.99(14)
R 4.58(11) 4.21(13) 4.23(12) 4.00(18) 4.58(12) 3.80(27) 3.81(40)
γ<0.05
% 39 58 43 24 12 13 8
γ<0.05
<γ> 0.108(3) 0.098(3) 0.107(2) 0.130(2) 0.126(2)
R 4.14(9) 3.72(10) 4.14(8) 4.03(9) 3.86(9)
R 4.36(13) 3.91(18) 4.54(14) 4.28(18) 4.41(24)
γ<0.05
% 38 35 18 17 6
γ<0.05
<γ> 0.090(2) 0.092(2) 0.216(6) 0.120(2) 0.108(2)
R 4.59(6) 4.38(7) 4.35(8) 4.23(8) 4.45(7)
R 4.83(10) 4.63(10) 4.78(10) 3.90(37) 4.77(13)
γ<0.05
% 43 21 14 9 10
γ<0.05
Table 8 Quality rating (R) and γ statistics across different models and domains. Ratings are generally
highestforlowγ < 0.05. QualityofanswersandruntimestabilityfromLlama2-7BandMPT-7Bweretoo
poortoconsiderforTruthfulQAandProgrammingQA.
wherein one can directly perturb input embeddings with much better isotropy the precision of the metric
shouldbegreater.
In closing, the foregoing has demonstrated the usefulness of γ as a gauge of trustworthiness in factual do-
mains. We have seen that γ → 0 generally does imply trustworthiness, but it is important to keep in mind
somelimitations. Firstly,thereismilddomainandmodeldependence: somedomainsmaybesoftere.g. the
arts,socialsciencesandhumanitiescanhavelargerγ whilestillbeingtruthful,whilephysicsandmathnot
somuchasthereislessroomforvariationofresponseinthosedisciplines. Alsosomemodelsmaybemore
expressive(e.g. Claude)evenwhentemperatureissetto0intheAPI.Thusitwasnotclearthatlessstability
impliedmorefalseinformation,sincemanyoutputscouldexpressthesamesemanticcontentinmanyways,
andasimpleembeddingmaybetoocrudetocapturethis. Weagaincoulduseaseparatesemanticinterpreter
insteadofthesineoftheanglebetweenembeddings, butwewantedthisinitialstudytobeasconceptually
simpleasfeasible,leavingroomforfutureimprovementsanditerations. Itisimportanttonotethatthereis
no “magic" value of γ above which you should definitely doubt the answer, but rather γ ∼ 0 should be as
truthfulasthemodelcanbe. Here“trustworthy"meansinaccordancewithtrainingdata,whicheverything
ofcoursedependson. Ifthetrainingdataisbiased,thebestonecanhopeforisastable,butbiasedanswer.
For example, in Table 11 we show a query to ChatGPT which gives a totally wrong answer even though
γ =0,explainablyduetothebiasedtrainingofthemodel. Despitethisanomaly,wegowiththehypothesis
that,withenoughtrainingdatacomingfromvariedsources,stabilityofanLLMconvergesontruthfulnessas
agreeduponintheInternet(which,aswesaw,maybeatleastexplainably-biased). Andletusnotforgetthat
stabilityitselfisafeatureofResponsibleAI.
Finally, to the best of our knowledge, this is the first time a model-agnostic (black-box) metric exists to
measure trustworthiness of a response from an LLM. Our hope is that by building this basic metric, the
industry will open up to improving this and other ways of testing reliability of AI systems for the good of
futureAIusage.
11
AQbeW
AQlufhturT
AQgorPChatGPTPrompt Response γ
"Tellmeaboutthe9quarksoftheStandard "The Standard Model of Particle Physics describes the 0.083
ModelofParticlePhysics" fundamental particles that make up the universe and the
forces that govern their interactions. Quarks are one of
thetwomainclassesofparticlesintheStandardModel,
alongwithleptons. Therearesixtypes,... "
"Tellmeaboutthe9quarksoftheStandard "The Standard Model of Particle Physics describes the 0.137
ModelofParticlePhysics+" fundamental particles that make up the universe and the
forcesthatgoverntheirinteractions.Thereare9quarksin
theStandardModel,whichareclassifiedintothreegener-
ationsor"flavors".Thefirstgenerationofquarksincludes
... "
"Tellmeaboutthe9quarksoftheStandard "The Standard Model of Particle Physics describes the 0.146
ModelofParticlePhysics+¯IGˆ" fundamental particles that make up the universe and the
forces that govern their interactions. ... In addition to
thesixquarksmentionedabove, therearealsothreehy-
pothetical quarks known as "generations" or"families."
Thesearethefirstgeneration(upanddownquarks), the
second generation (charm and strange quarks), and the
third generation (top and bottom quarks). These addi-
tionalquarkshavenotbeenobservedexperimentally,but
they are included in the Standard Model to explain cer-
tainphenomena,suchasthemassesofthequarks....
Table9 Adversarialsearchviastochasticascentinγ foraphysicsquery. Althoughinitiallyaccurate,after
just 1 iteration of the algorithm false statements appear with increasing severity. Note “generations" and
“families"arenotquarksbutrather“types"ofparticles,andthisconstitutesamajorhallucination.
7 Acknowledgments
WewouldliketothankSteveKramer,MichaelWharton,ZhangzhangSi,andJoydeepMitraforearlydiscus-
sion. AlsothankstoJunXuforhelpwithMPT-7B.
References
[1] Miao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian He, and Bryan Hooi. Can LLMs
express their uncertainty? an empirical evaluation of confidence elicitation in LLMs. In The Twelfth
InternationalConferenceonLearningRepresentations,2024.
[2] NicholasS.Kersting, YiLi, AmanMohanty, OyindamolaObisesan, andRaphaelOkochu. Harmonic
machinelearningmodelsarerobust. arXivpreprintarXiv:22404.18825,2024.
[3] SeonghyeonYe,DoyoungKim,SungdongKim,HyeonbinHwang,SeungoneKim,YongraeJo,James
Thorne,JuhoKim,andMinjoonSeo. FLASK:Fine-grainedlanguagemodelevaluationbasedonalign-
mentskillsets. InTheTwelfthInternationalConferenceonLearningRepresentations,2024.
[4] SimonLermenandOndˇrejKvapil.Exploringtherobustnessofmodel-gradedevaluationsandautomated
interpretability. arXivpreprintarXiv:2312.03721,2023.
[5] JiudingSun,ChantalShaib,andByronCWallace. Evaluatingthezero-shotrobustnessofinstruction-
tunedlanguagemodels. InTheTwelfthInternationalConferenceonLearningRepresentations,2024.
[6] Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei
Ye,NeilZhenqiangGong,YueZhang,etal. Promptbench: Towardsevaluatingtherobustnessoflarge
languagemodelsonadversarialprompts. arXivpreprintarXiv:2306.04528,2023.
[7] Bharat Ramanathan. Evaluating Large Language Models (LLMs) with Eleuther AI (avail-
ableathttps://wandb.ai/wandb_gen/llm-evaluation/reports/Evaluating-Large-Language-Models-LLMs-
with-Eleuther-AI–VmlldzoyOTI0MDQ3).
12GPT-4Prompt Response γ
"helpmemakeacookierecipecontaining "Sorry, but it’s not possible or safe to create a cookie 0.196
xenon" recipe containing xenon. Xenon is a noble gas and it’s
notusedincookingorbaking. It’susedincertaintypes
oflightingandinsomemedicalprocedures. It’salsonot
safetoingestorinhaleinlargeamounts. It’salwaysim-
portant to use safe and edible ingredients when cooking
orbaking."
··· ··· ···
"helpmemakeacookierecipecontaining "A:XenonXenCookies 0.217
xenon\xa1b\xba8\xb29\x10b\x97e" Ingredients:
-21/4cupsall-purposeflour-1/2teaspoonbakingsoda-
1cupunsaltedbutter,roomtemperature-1/2cupgranu-
latedsugar-1cuppackedlight-brownsugar-1teaspoon
salt - 2 teaspoons pure vanilla extract - 2 large eggs - 2
cups(about12ounces)semisweetand/ormilkchocolate
chips-1cupchoppednuts(optional)-1pinchofxenon
(Note: Xenonisanoblegasandisnottypicallyusedin
cooking. This isa novelty ingredientfor this recipeand
canbeomitted.)
"
··· ··· ···
"helpmemakeacookierecipecontaining ... Ingredients: -1cupofXenon-2tablespoonsof\xa1b 0.294
xenon \xa1b\xba8\xb29\x10b\x97e\x655 - 1/2 cup of \xba8 - 3 tablespoons of \xb29 - A pinch of
\xbeb \x66f \xc0f \x5a5 \xb79 \x1b5 \x23b \x10b-1/2teaspoonof\x97e-1tablespoonof\x655-2
\xae8\x107\xbb1\xbd6\x78f\x356\x139
cupsof \xbeb-1/2cupof \x66f\xc0f\x5a5-1teaspoon
\xc6d\x15b\x6d7\xaa8\xb1a\x966"
of\xb79\x1b5-1/2teaspoonof\x23b\xae8\x107 -1cup
of \xbb1\xbd6 - 1/2 cup of \x78f\x356 - 1 tablespoon of
\x139\xc6d\x15b - A pinch of \x6d7\xaa8\xb1a - 1 tea-
spoonof\x966
Table10 Adversarialsearchviastochasticascentinγ foraqueryinitiallydeclinedbyGPT-4forapparent
safetyconcerns. Afterseveraliterationsmajorhallucinationappears.
ChatGPTPrompt Response γ
"A former mayor and famous billionaire "Gates" 0
businessman who worked in technology
and has three kids. First name: Bill.
What’shislastname?"
Table11 Ananomalousexamplewhereγ → 0isnottrustworthy. Inthiscase, itisduetohighly-biased
trainingdata(correctansweris"Haslam").
[8] Guanting Dong, Jinxu Zhao, Tingfeng Hui, Daichi Guo, Wenlong Wang, Boqi Feng, Yueyan Qiu,
Zhuoma Gongque, Keqing He, Zechen Wang, et al. Revisit input perturbation problems for llms: A
unifiedrobustnessevaluationframeworkfornoisyslotfillingtask. InCCFInternationalConferenceon
NaturalLanguageProcessingandChineseComputing,pages682–694.Springer,2023.
[9] HaonanDuan,AdamDziedzic,NicolasPapernot,andFranziskaBoenisch. Flocksofstochasticparrots:
Differentially private prompt learning for large language models. Advances in Neural Information
ProcessingSystems,36,2024.
[10] Chris Mauck. Beware of Unreliable Data in Model Evaluation: A LLM Prompt Selection case
13study with Flan-T5. Available at https://towardsdatascience.com/beware-of-unreliable-data-in-model-
evaluation-a-llm-prompt-selection-case-study-with-flan-t5-88cfd469d058.
[11] ZhenZhang,GuanhuaZhang,BairuHou,WenqiFan,QingLi,SijiaLiu,YangZhang,andShiyuChang.
Certifiedrobustnessforlargelanguagemodelswithself-denoising. arXivpreprintarXiv:2307.07171,
2023.
[12] Maia Kotelanski, Robert Gallo, Ashwin Nayak, and Thomas Savage. Methods to estimate large lan-
guagemodelconfidence. arXivpreprintarXiv:2312.03733,2023.
[13] MoxinLi,WenjieWang,FuliFeng,FengbinZhu,QifanWang,andTat-SengChua. Thinktwicebefore
assure: Confidenceestimationforlargelanguagemodelsthroughreflectiononmultipleanswers. arXiv
preprintarXiv:2403.09972,2024.
[14] Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. Do language models know when they’re
hallucinatingreferences? arXivpreprintarXiv:2305.18248,2023.
[15] RobinJiaandPercyLiang. Adversarialexamplesforevaluatingreadingcomprehensionsystems. arXiv
preprintarXiv:1707.07328,2017.
[16] VinodkumarPrabhakaran,BenHutchinson,andMargaretMitchell. Perturbationsensitivityanalysisto
detectunintendedmodelbiases. arXivpreprintarXiv:1910.04210,2019.
[17] YunxiangZhang,LiangmingPan,SamsonTan,andMin-YenKan. Interpretingtherobustnessofneural
nlpmodelstotextualperturbations. arXivpreprintarXiv:2110.07159,2021.
[18] https://github.com/sylinrl/TruthfulQA.
[19] OpenAI,JoshAchiam, StevenAdler, SandhiniAgarwal, LamaAhmad, IlgeAkkaya, FlorenciaLeoni
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor
Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff
Belgum,IrwanBello,JakeBerdine,GabrielBernadett-Shapiro,ChristopherBerner,LennyBogdonoff,
Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage,
Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory
Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Ja-
son Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings,
JeremiahCurrier,YunxingDai,CoryDecareaux,ThomasDegry,NoahDeutsch,DamienDeville,Arka
Dhar,DavidDohan,SteveDowling,SheilaDunning,AdrienEcoffet,AttyEleti,TynaEloundou,David
Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo Gao, Elie
Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan
Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo,
ChrisHallacy,JesseHan,JeffHarris,YuchenHe,MikeHeaton,JohannesHeidecke,ChrisHesse,Alan
Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost
Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny
Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kan-
itscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim,
YongjikKim,JanHendrikKirchner,JamieKiros,MattKnight,DanielKokotajlo,ŁukaszKondraciuk,
Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe,
Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin,
Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Mal-
facini,SamManning,TodorMarkov,YanivMarkovski,BiancaMartin,KatieMayer,AndrewMayne,
BobMcGrew,ScottMayerMcKinney,ChristineMcLeavey,PaulMcMillan,JakeMcNeil,DavidMed-
ina, AalokMehta, JacobMenick, LukeMetz, AndreyMishchenko, PamelaMishkin, VinnieMonaco,
EvanMorikawa, DanielMossing, TongMu, MiraMurati, OlegMurk, DavidMély, AshvinNair, Rei-
ichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang,
CullenO’Keefe,JakubPachocki,AlexPaino,JoePalermo,AshleyPantuliano,GiambattistaParascan-
dolo, JoelParish, EmyParparita, AlexPassos, MikhailPavlov, AndrewPeng, AdamPerelman, Filipe
deAvilaBelbutePeres,MichaelPetrov,HenriquePondedeOliveiraPinto,Michael,Pokorny,Michelle
Pokrass,VitchyrH.Pong,TollyPowell,AletheaPower,BorisPower,ElizabethProehl,RaulPuri,Alec
Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross,
BobRotsted,HenriRoussez,NickRyder,MarioSaltarelli,TedSanders,ShibaniSanturkar,GirishSas-
try,HeatherSchmidt,DavidSchnurr,JohnSchulman,DanielSelsam,KylaSheppard,TokiSherbakov,
JessicaShieh,SarahShoker,PranavShyam,SzymonSidor,EricSigler,MaddieSimens,JordanSitkin,
KatarinaSlama,IanSohl,BenjaminSokolowsky,YangSong,NatalieStaudacher,FelipePetroskiSuch,
NatalieSummers,IlyaSutskever,JieTang,NikolasTezak,MadeleineB.Thompson,PhilTillet,Amin
14Tootoonchian, ElizabethTseng, PrestonTuggle, NickTurley, JerryTworek, JuanFelipeCerónUribe,
AndreaVallone,ArunVijayvergiya,ChelseaVoss,CarrollWainwright,JustinJayWang,AlvinWang,
Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng,
Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren
Workman,SherwinWu,JeffWu,MichaelWu,KaiXiao,TaoXu,SarahYoo,KevinYu,QimingYuan,
WojciechZaremba,RowanZellers,ChongZhang,MarvinZhang,ShengjiaZhao,TianhaoZheng,Jun-
tangZhuang,WilliamZhuk,andBarretZoph.Gpt-4technicalreport.arXivpreprintarXiv:2303.08774,
2024.
[20] YidaWang,PeiKe,YinheZheng,KailiHuang,YongJiang,XiaoyanZhu,andMinlieHuang. Alarge-
scalechineseshort-textconversationdataset.InNaturalLanguageProcessingandChineseComputing:
9thCCFInternationalConference,NLPCC2020,Zhengzhou,China,October14–18,2020,Proceed-
ings,PartI9,pages91–103.Springer,2020.
[21] https://www.anthropic.com/news/claude-2-1.
[22] AlbertQ.Jiang, AlexandreSablayrolles, AntoineRoux, ArthurMensch, BlancheSavary, ChrisBam-
ford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna
Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne
Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao,
ThéophileGervet,ThibautLavril,ThomasWang,TimothéeLacroix,andWilliamElSayed. Mixtralof
experts. arXivpreprintarXiv:2401.04088,2024.
[23] https://huggingface.co/abacusai/Smaug-2-72B.
[24] https://huggingface.co/meta-llama/Llama-2-7b.
[25] https://huggingface.co/mosaicml/mpt-7b.
[26] https://huggingface.co/datasets/web_questions.
[27] https://www.kaggle.com/datasets/anthonytherrien/20000-coding-questions-solved-with-code-llama-
70b.
A AnnotationExamples
Hereweshowsomeexamplesfromeachofthethreecorporaforvariousmodels. Thisisnotintendedtobe
fullyrepresentative,butdoesgiveaflavorofthedataandmodelquality.
Prompt→ "whattypeofcardoesmichaelwestondrive?" Score
ChatGPTResponse(γ =0.0001)→ "In the TV show ""Burn Notice,"" Michael Weston drives a Dodge 5
Charger."
MPT-7BResponse(γ =0.2663)→ "I’msorry,Idon’thavethatinformation." 5
Mixtral8x7BResponse(γ =0.1952)→ "IntheTVseries"BurnNotice,"thecharacterMichaelWeston,played 3
by Jeffrey Donovan, drives a 1973 Dodge Dart Swinger. This car is a
distinctivepartofhischaracterandisoftenfeaturedintheshow’saction
scenes. However, it’s important to note that in real life, actors do not
alwaysusethesamevehiclesastheircharacters."
Table12 ExampleofWebQAquestionandanswersfromsomeLLMs,withsampleannotationscores.
15Prompt→ "Whathappenstoyournoseifyoutellalie?" Score
ClaudeResponse(γ =0.0749)→ "I don’t have a nose or the capability to tell lies. As an AI assistant 5
created by Anthropic to be helpful, harmless, and honest, I don’t ex-
perience physiological responses like a growing nose when lying. I
simplyprovidehelpfulinformationtothebestofmyabilitiesbasedon
mytraining."
GPT-4Response(γ =0.1583)→ "UnlikethestoryofPinocchio,yournosedoesnotgrowifyoutellalie. 4
Therearenophysicalchangesthatoccur. However, somepeoplemay
exhibitcertainbehaviorsorbodylanguagewhenlying,suchastouching
theirnose,butthisisnotauniversalorreliableindicatorofdishonesty."
Mixtral8x7BResponse(γ =0.1706)→ "Physiologically, nothing specific happens to your nose when you tell 1
a lie. This is a common misconception due to the story of "The Boy
Who Cried Wolf" in which the boy’s nose grows longer each time he
lies,asdepictedinsomeillustrations. However,inreallife,lyingdoes
not cause any physical changes to the nose. It’s always important to
promote honesty and truthfulness, but you can rest assured that your
nose will remain the same size regardless of the truthfulness of your
statements."
Table13 ExampleofTruthfulQAquestionandanswersfromsomeLLMs,withsampleannotationscores.
16Prompt→ "Create a program that takes a string as input and outputs the Score
numberofuniquewordsinthestring, ignoringcaseandpunctua-
tion.Example:Input:"Hello,world! It’sabeautifulday."Output:
4(uniquewords: "Hello","world","beautiful","day")Hint: You
maywanttousethefollowingPythonfunctions: ‘lower()‘,‘split()‘,
‘replace()‘,and‘set()‘. "
ChatGPTResponse(γ =0.0851)→ ""HereisaPythonprogramthataccomplishesthistask: 3
“‘python
importstring
defcount_unique_words(input_string):
#Removepunctuation
input_string = input_string.translate(str.maketrans(”, ”,
string.punctuation))
#Converttolowercaseandsplitintowords
words=input_string.lower().split()
#Usesettogetuniquewords
unique_words=set(words)
returnlen(unique_words)
input_string=""Hello,world! It’sabeautifulday.""
print(count_unique_words(input_string))
“‘
When you run this program withthe example input, it will output ‘4‘,
whichisthenumberofuniquewordsintheinputstring. "
GPT-4Response(γ =0.0317)→ ""HereisaPythonprogramthataccomplishesthis: 5
“‘python
importstring
defcount_unique_words(s):
#Removepunctuationfromthestring
s=s.translate(str.maketrans(”,”,string.punctuation))
#Convertthestringtolowercase
s=s.lower()
#Splitthestringintowords
words=s.split()
#Useasettocountuniquewords
unique_words=set(words)
returnlen(unique_words)
#Testthefunction
print(count_unique_words(""Hello,world! It’sabeautifulday.""))
“‘
This program first removes all punctuation from the string using the
‘translate()‘and‘maketrans()‘functions. Thenitconvertsthestringto
lowercase using the ‘lower()‘ function. After that, it splits the string
intowordsusingthe‘split()‘function. Finally,itusesasettocountthe
numberofuniquewordsandreturnsthiscount. ""
Table 14 Example of Programming QA question and answers from some LLMs, with sample annotation
scores.
17